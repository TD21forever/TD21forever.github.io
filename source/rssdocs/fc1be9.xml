<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735674459418194188#m</id>
            <title>R to @op7418: 这是作者的介绍推：
https://x.com/CeyuanY/status/1735657553504477517?s=20</title>
            <link>https://nitter.cz/op7418/status/1735674459418194188#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735674459418194188#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 14:53:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是作者的介绍推：<br />
<a href="https://x.com/CeyuanY/status/1735657553504477517?s=20">x.com/CeyuanY/status/1735657…</a></p>
<p><a href="https://nitter.cz/CeyuanY/status/1735657553504477517#m">nitter.cz/CeyuanY/status/1735657553504477517#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735673644309795061#m</id>
            <title>过年了！#animatediff 今天更新V3版本的运动模型，同时更新了作者前几天发布了视频控制模型SparseCtrl。

SparseCtrl可以理解为转为视频优化过的Contorlnet，可以通过输入关键帧的深度或者涂鸦图像控制视频按照指定的方式运动和过度。
这个项目一定程度上解决了现在Animatediff生成视频过程中无法控制的问题。
目前发布了两个SparseCtrl模型分别是深度和涂鸦控制。
已经在下载模型测试了。下面是演示视频。

项目地址https://github.com/guoyww/AnimateDiff</title>
            <link>https://nitter.cz/op7418/status/1735673644309795061#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735673644309795061#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 14:50:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>过年了！<a href="https://nitter.cz/search?q=%23animatediff">#animatediff</a> 今天更新V3版本的运动模型，同时更新了作者前几天发布了视频控制模型SparseCtrl。<br />
<br />
SparseCtrl可以理解为转为视频优化过的Contorlnet，可以通过输入关键帧的深度或者涂鸦图像控制视频按照指定的方式运动和过度。<br />
这个项目一定程度上解决了现在Animatediff生成视频过程中无法控制的问题。<br />
目前发布了两个SparseCtrl模型分别是深度和涂鸦控制。<br />
已经在下载模型测试了。下面是演示视频。<br />
<br />
项目地址<a href="https://github.com/guoyww/AnimateDiff">github.com/guoyww/AnimateDif…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU2NzM1MTc2MTY1Mzc2MDAvcHUvaW1nL2dQZnp3Y1o3NDRJNUgwaHQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/9hills/status/1735637926493786167#m</id>
            <title>RT by @op7418: mistral 还是厉害，现在lmsys elo 比较高的7B模型，包括openchat、starling、openhermes、zephyr 都是以 mistral 为基座模型。

而且实测用中文prompt 进行sft，也有良好的表现。

目前我们新的7B模型的base，已经全部切换为mistral。</title>
            <link>https://nitter.cz/9hills/status/1735637926493786167#m</link>
            <guid isPermaLink="false">https://nitter.cz/9hills/status/1735637926493786167#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 12:28:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>mistral 还是厉害，现在lmsys elo 比较高的7B模型，包括openchat、starling、openhermes、zephyr 都是以 mistral 为基座模型。<br />
<br />
而且实测用中文prompt 进行sft，也有良好的表现。<br />
<br />
目前我们新的7B模型的base，已经全部切换为mistral。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735633083599294545#m</id>
            <title>这个Animatediff效果好，之前他发过工作流。太稳定了。</title>
            <link>https://nitter.cz/op7418/status/1735633083599294545#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735633083599294545#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 12:09:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个Animatediff效果好，之前他发过工作流。太稳定了。</p>
<p><a href="https://nitter.cz/Morning_wood17/status/1735614493965840793#m">nitter.cz/Morning_wood17/status/1735614493965840793#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735612535909532028#m</id>
            <title>卧槽，这个 LLM 算法的可视化演示太强了，之前我们看到的都是 2D 的，这个是 3D 的。
而且他完整的展示了整个 LLM不同模块内部的运作机制和各模块之间的联系。
你还可以看 GPT-3 和 GPT-2 这种不同规模的 LLM 在架构和模块上的区别。

来玩玩看。https://bbycroft.net/llm</title>
            <link>https://nitter.cz/op7418/status/1735612535909532028#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735612535909532028#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 10:47:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽，这个 LLM 算法的可视化演示太强了，之前我们看到的都是 2D 的，这个是 3D 的。<br />
而且他完整的展示了整个 LLM不同模块内部的运作机制和各模块之间的联系。<br />
你还可以看 GPT-3 和 GPT-2 这种不同规模的 LLM 在架构和模块上的区别。<br />
<br />
来玩玩看。<a href="https://bbycroft.net/llm">bbycroft.net/llm</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU2MTI0NTI3NTc1MTYyODgvcHUvaW1nLzIzRVNEWVR0cHdoNFVDV1AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735600276349092076#m</id>
            <title>Runway 上线文字生成语音功能，免费使用。
发现前几天说的文字生成语音的功能，Runway 悄悄的上了。尝试了一下英文的效果真的很好，感情很丰富自然。
中文还是老问题，有外国人口音，这块可能不能指望外国公司了。
这个功能可以选的语音模型非常多，可以都试试。目前是消耗右上角点数生成可以用好久。下面是我的测试录音。
可以来试试：https://app.runwayml.com</title>
            <link>https://nitter.cz/op7418/status/1735600276349092076#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735600276349092076#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 09:58:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 上线文字生成语音功能，免费使用。<br />
发现前几天说的文字生成语音的功能，Runway 悄悄的上了。尝试了一下英文的效果真的很好，感情很丰富自然。<br />
中文还是老问题，有外国人口音，这块可能不能指望外国公司了。<br />
这个功能可以选的语音模型非常多，可以都试试。目前是消耗右上角点数生成可以用好久。下面是我的测试录音。<br />
可以来试试：<a href="https://app.runwayml.com">app.runwayml.com</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU1OTkxMjA0MzIzOTQyNDAvcHUvaW1nL3M0Wmhwa1ZUcnlZOVlrYmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735591304778887295#m</id>
            <title>R to @op7418: 另一个 LCM 配合 3D 渲染的例子，不过这个是3D 雕刻的实时渲染，可能加上了 Contorlnet 还原度非常好。
https://x.com/MartinNebelong/status/1735574652825567597?s=20</title>
            <link>https://nitter.cz/op7418/status/1735591304778887295#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735591304778887295#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 09:23:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个 LCM 配合 3D 渲染的例子，不过这个是3D 雕刻的实时渲染，可能加上了 Contorlnet 还原度非常好。<br />
<a href="https://x.com/MartinNebelong/status/1735574652825567597?s=20">x.com/MartinNebelong/status/…</a></p>
<p><a href="https://nitter.cz/MartinNebelong/status/1735574652825567597#m">nitter.cz/MartinNebelong/status/1735574652825567597#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735586639945380110#m</id>
            <title>R to @op7418: 补充一下第二个的链接：https://www.reddit.com/r/StableDiffusion/comments/18i56hz/a1111_sd15lcm_checkpoints_animatediff_controlnet/</title>
            <link>https://nitter.cz/op7418/status/1735586639945380110#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735586639945380110#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 09:04:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补充一下第二个的链接：<a href="https://teddit.net/r/StableDiffusion/comments/18i56hz/a1111_sd15lcm_checkpoints_animatediff_controlnet/">teddit.net/r/StableDiffusion…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNTU4NjY0NDc5ODExOTkzNi9Udkt5c2NQRj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735586524224450787#m</id>
            <title>R to @op7418: 另一个LCM 加上Animatediff 和 Contorlnet渲染 Blender 内容的测试他还是用的 A1111。
用了 64 帧的 Blender 素材，然后生成了一张静态图片，又用了IP-Adapter Plus和 Animatediff 生成视频。</title>
            <link>https://nitter.cz/op7418/status/1735586524224450787#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735586524224450787#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 09:04:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个LCM 加上Animatediff 和 Contorlnet渲染 Blender 内容的测试他还是用的 A1111。<br />
用了 64 帧的 Blender 素材，然后生成了一张静态图片，又用了IP-Adapter Plus和 Animatediff 生成视频。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU1ODUxODE3Nzg3MTg3MjAvcHUvaW1nL2VIcExOcTdGb3U0ZlV2bl8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735585111658389656#m</id>
            <title>最近很多人用 LCM 加上Animatediff 和 Contorlnet 代替 Blender 的渲染。
比如这个就是用了右边的视频在 Blender 上面随便加了点胡子和眼镜然后用 LCM 生成的视频。
他用 LCM 的时候的参数是 8 步，CFG 2，采样器是Euler A。
我之前一直用 ComfyUI 那个 lcm 的采样器，生成的 Animatediff 的视频质量确实不好，看了这个帖子换了Euler A确实稳定多了。

原贴地址：https://www.reddit.com/r/StableDiffusion/comments/18isc5q/the_captain_30_seconds_temporal_consistency/</title>
            <link>https://nitter.cz/op7418/status/1735585111658389656#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735585111658389656#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 08:58:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近很多人用 LCM 加上Animatediff 和 Contorlnet 代替 Blender 的渲染。<br />
比如这个就是用了右边的视频在 Blender 上面随便加了点胡子和眼镜然后用 LCM 生成的视频。<br />
他用 LCM 的时候的参数是 8 步，CFG 2，采样器是Euler A。<br />
我之前一直用 ComfyUI 那个 lcm 的采样器，生成的 Animatediff 的视频质量确实不好，看了这个帖子换了Euler A确实稳定多了。<br />
<br />
原贴地址：<a href="https://teddit.net/r/StableDiffusion/comments/18isc5q/the_captain_30_seconds_temporal_consistency/">teddit.net/r/StableDiffusion…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU1ODAzNDgzODg2Mzg3MjAvcHUvaW1nL2pGVUJJWlVtRDNRcmVtcmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735574474685284700#m</id>
            <title>Open AI推特拆分行动，现在又多了一个面向开发者的Developers。
可以先关注一下。配图很好看。</title>
            <link>https://nitter.cz/op7418/status/1735574474685284700#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735574474685284700#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 08:16:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Open AI推特拆分行动，现在又多了一个面向开发者的Developers。<br />
可以先关注一下。配图很好看。</p>
<p><a href="https://nitter.cz/OpenAIDevs/status/1735363447326687443#m">nitter.cz/OpenAIDevs/status/1735363447326687443#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735566656246809041#m</id>
            <title>Perplexity 现在会根据你的搜索内容和结果生成一张图片。

刚才 Perplexity CEO 说他们会上图像生成服务 。@Yayoi_no_yume 说已经有了，我去试了一下果然已经有了。
这下自己写东西更方便了搜索完内容整理一下用生成的图片当头图就可以直接发布了。

应该用的是 DALL-E3，生成的图片右下角会有个 AI 的标记。
搜索完成后在右下角会有个Generate Image的按钮，点击之后会让你选风格，有绘画、照片、插画、图表四种。
下面第一张图是刺客信条：黑旗搜索结果生成的。</title>
            <link>https://nitter.cz/op7418/status/1735566656246809041#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735566656246809041#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 07:45:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Perplexity 现在会根据你的搜索内容和结果生成一张图片。<br />
<br />
刚才 Perplexity CEO 说他们会上图像生成服务 。<a href="https://nitter.cz/Yayoi_no_yume" title="◂Ⓘ▸YAYOI の 夢">@Yayoi_no_yume</a> 说已经有了，我去试了一下果然已经有了。<br />
这下自己写东西更方便了搜索完内容整理一下用生成的图片当头图就可以直接发布了。<br />
<br />
应该用的是 DALL-E3，生成的图片右下角会有个 AI 的标记。<br />
搜索完成后在右下角会有个Generate Image的按钮，点击之后会让你选风格，有绘画、照片、插画、图表四种。<br />
下面第一张图是刺客信条：黑旗搜索结果生成的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JYNFlzZWFJQUFuTERLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JYNGJOdWFrQUFwZkdhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1735405409941287340#m</id>
            <title>RT by @op7418: 推特上有两个用户Futuristflower 和 Jimmy Apples，他们经常会有些爆料，很多时候还是准确的。

Jimmy Apples 曾透露 OpenAI 12月份要发布 GPT-4.5 

Futuristflower 也确认了 Jimmy Apples 的爆料，称 OpenAI 计划在12月推出 GPT-4.5。

Futuristflower 还透露了一份似乎来自谷歌内部的泄露文件，文件显示由于 GPT-4.5 即将问世，谷歌正在加快推进其 Gemini API 的发布。

此外，Futuristflower 分享了一份关于 GPT-4.5 定价的文件，并指出：“虽然截图内容似乎大致准确，但目前还没有消息来源能确认这是否是一份真实的草案”。</title>
            <link>https://nitter.cz/dotey/status/1735405409941287340#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1735405409941287340#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 21:04:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推特上有两个用户Futuristflower 和 Jimmy Apples，他们经常会有些爆料，很多时候还是准确的。<br />
<br />
Jimmy Apples 曾透露 OpenAI 12月份要发布 GPT-4.5 <br />
<br />
Futuristflower 也确认了 Jimmy Apples 的爆料，称 OpenAI 计划在12月推出 GPT-4.5。<br />
<br />
Futuristflower 还透露了一份似乎来自谷歌内部的泄露文件，文件显示由于 GPT-4.5 即将问世，谷歌正在加快推进其 Gemini API 的发布。<br />
<br />
此外，Futuristflower 分享了一份关于 GPT-4.5 定价的文件，并指出：“虽然截图内容似乎大致准确，但目前还没有消息来源能确认这是否是一份真实的草案”。</p>
<p><a href="https://nitter.cz/AISafetyMemes/status/1735282033926996449#m">nitter.cz/AISafetyMemes/status/1735282033926996449#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735561613368008790#m</id>
            <title>R to @op7418: 顺便提一嘴沉浸式翻译在arxiv右边加的arXiv Vanity入口真是方便。
可以一键查看论文网页版本了，这样翻译的时候排版也不会乱窜，读论文方便多了。</title>
            <link>https://nitter.cz/op7418/status/1735561613368008790#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735561613368008790#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 07:25:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>顺便提一嘴沉浸式翻译在arxiv右边加的arXiv Vanity入口真是方便。<br />
可以一键查看论文网页版本了，这样翻译的时候排版也不会乱窜，读论文方便多了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JYMERRX2J3QUFxenZHLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735561407180239175#m</id>
            <title>视频 LCM 项目，通过四个采样步骤即可实现高保真、流畅的视频合成。

其实这个我没看懂，现有的 LCM 在采样器前接入加上视频生成模型也可以实现类似的事情。他们在论文里没有说明他们的研究比现在利用 LCM 生成视频的优势在哪里。

论文地址：https://arxiv.org/abs/2312.09109</title>
            <link>https://nitter.cz/op7418/status/1735561407180239175#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735561407180239175#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 07:24:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频 LCM 项目，通过四个采样步骤即可实现高保真、流畅的视频合成。<br />
<br />
其实这个我没看懂，现有的 LCM 在采样器前接入加上视频生成模型也可以实现类似的事情。他们在论文里没有说明他们的研究比现在利用 LCM 生成视频的优势在哪里。<br />
<br />
论文地址：<a href="https://arxiv.org/abs/2312.09109">arxiv.org/abs/2312.09109</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JYeTUyZGJZQUFpc29oLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735558737505796252#m</id>
            <title>Perplexity 将会推出图像生成服务，不知道啥时候能看到具体的东西。</title>
            <link>https://nitter.cz/op7418/status/1735558737505796252#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735558737505796252#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 07:13:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Perplexity 将会推出图像生成服务，不知道啥时候能看到具体的东西。</p>
<p><a href="https://nitter.cz/AravSrinivas/status/1735544253978468820#m">nitter.cz/AravSrinivas/status/1735544253978468820#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JYeFVhRmE4QUFYbmdYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735555974491173338#m</id>
            <title>哈哈哈哈 我也想搞几张当圣诞节礼物了。其实挺好实现的。脑洞真大啊。</title>
            <link>https://nitter.cz/op7418/status/1735555974491173338#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735555974491173338#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 07:02:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈哈哈 我也想搞几张当圣诞节礼物了。其实挺好实现的。脑洞真大啊。</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1735538480019828925#m">nitter.cz/xiaohuggg/status/1735538480019828925#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Tisoga/status/1735497861918523756#m</id>
            <title>RT by @op7418: http://devv.ai 发布重大更新 🚀

这个版本迎来了非常多的更新，主要包含：

- 增加了多行编辑，使用 Shift + Enter 换行（现在可以编辑 &amp; 修改代码块了）
- 增加了预设模式，可以选择默认输出的编程语言

http://devv.ai 致力于把搜索这件小事做好，目标是取代开发过程中使用 Google / StackOverflow / 文档的场景，目前收到越来越多的反馈表示已经把 http://devv.ai 作为默认的搜索引擎了。

另外如果大家觉得 http://devv.ai 好用，欢迎推荐给身边的同事和朋友！</title>
            <link>https://nitter.cz/Tisoga/status/1735497861918523756#m</link>
            <guid isPermaLink="false">https://nitter.cz/Tisoga/status/1735497861918523756#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 03:11:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="http://devv.ai">devv.ai</a> 发布重大更新 🚀<br />
<br />
这个版本迎来了非常多的更新，主要包含：<br />
<br />
- 增加了多行编辑，使用 Shift + Enter 换行（现在可以编辑 & 修改代码块了）<br />
- 增加了预设模式，可以选择默认输出的编程语言<br />
<br />
<a href="http://devv.ai">devv.ai</a> 致力于把搜索这件小事做好，目标是取代开发过程中使用 Google / StackOverflow / 文档的场景，目前收到越来越多的反馈表示已经把 <a href="http://devv.ai">devv.ai</a> 作为默认的搜索引擎了。<br />
<br />
另外如果大家觉得 <a href="http://devv.ai">devv.ai</a> 好用，欢迎推荐给身边的同事和朋友！</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU0OTY3ODc1NzI3NDQxOTMvcHUvaW1nL2dpMXZaNk5JcmJyNTVObXguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735144562967159169#m</id>
            <title>RT by @op7418: 终于出现完全产品化的为个人炼制模型并提供服务的产品了。Delphi 这个应用可以将你所有的视频、播客、PDF、博客文章等信息训练为一个你的分身，并且你可以用你的分身对外提供咨询服务。
支持文字、语音甚至视频沟通。
你的分身会用你的语气和你上传内容的知识跟你的粉丝对话，同时还支持对话内容的数据分析帮你优化分身跟粉丝的交流。
看了一下价格最便宜的套餐每个月 25，不过需要跟他们 CEO 视频获得引导才能创建，感觉这个会议也不是真人只是他们 CEO 的分身，来炫耀技术的。

网站：https://www.delphi.ai/</title>
            <link>https://nitter.cz/op7418/status/1735144562967159169#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735144562967159169#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 03:47:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于出现完全产品化的为个人炼制模型并提供服务的产品了。Delphi 这个应用可以将你所有的视频、播客、PDF、博客文章等信息训练为一个你的分身，并且你可以用你的分身对外提供咨询服务。<br />
支持文字、语音甚至视频沟通。<br />
你的分身会用你的语气和你上传内容的知识跟你的粉丝对话，同时还支持对话内容的数据分析帮你优化分身跟粉丝的交流。<br />
看了一下价格最便宜的套餐每个月 25，不过需要跟他们 CEO 视频获得引导才能创建，感觉这个会议也不是真人只是他们 CEO 的分身，来炫耀技术的。<br />
<br />
网站：<a href="https://www.delphi.ai/">delphi.ai/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDIyNDkxNzUwODA5NjAvcHUvaW1nLzgzS19WMmY5cjhObm1JQVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735553173325304291#m</id>
            <title>恭喜啊 LobeChat 登上了今天Github Trending的第一。</title>
            <link>https://nitter.cz/op7418/status/1735553173325304291#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735553173325304291#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 06:51:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>恭喜啊 LobeChat 登上了今天Github Trending的第一。</p>
<p><a href="https://nitter.cz/lobehub/status/1735541345241157751#m">nitter.cz/lobehub/status/1735541345241157751#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>