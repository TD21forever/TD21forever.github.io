<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752018965700935918#m</id>
            <title>Meta发布了Code Llama 70B，是之前Code Llama的升级版本，包括三个模型。
CodeLlama-70B-Instruct在 HumanEval 上获得了 67.8 分，使其成为当今性能最高的开放模型之一。</title>
            <link>https://nitter.cz/op7418/status/1752018965700935918#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752018965700935918#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 17:20:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta发布了Code Llama 70B，是之前Code Llama的升级版本，包括三个模型。<br />
CodeLlama-70B-Instruct在 HumanEval 上获得了 67.8 分，使其成为当今性能最高的开放模型之一。</p>
<p><a href="https://nitter.cz/AIatMeta/status/1752013879532782075#m">nitter.cz/AIatMeta/status/1752013879532782075#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752009159321772089#m</id>
            <title>R to @op7418: 动起来以后确实有那个味道了</title>
            <link>https://nitter.cz/op7418/status/1752009159321772089#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752009159321772089#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 16:41:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>动起来以后确实有那个味道了</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTIwMDg1NTA2NDg2MjcyMDAvcHUvaW1nL2NicXA5SWFfQ0VrM2xWbjIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751849029624123765#m</id>
            <title>RT by @op7418: Secret Leaders播客的主理人Dan，介绍了一些基于心理学的通过待办事项提高工作效率的技巧，这条推有 300 万曝光，我精简并润色了一下：

拥有清单的人们能以比其他人快 40% 的速度完成工作。但是，有 99% 的人在制作他们的清单时出错。以下是正确做法：（运用心理学）

1️⃣进行头脑大扫除。将脑中所有的任务写下来。所有的项目、目标和待办事项。这样可以减轻认知负担。

2️⃣分离任务。将任务简化为每天的 3-5 个大任务。使用艾森豪威尔矩阵（Eisenhower Matrix）按重要性分离任务。→ 紧急且重要 → 不紧急但重要 → 紧急但不重要 → 不紧急也不重要

3️⃣制定早晨例行程序列表。每天从一个简单的 5-15 分钟的例行程序开始。例如：→ 喝水 → 早餐 → 伸展 → 平板支撑 → 冥想。这有助于大脑进入生产力状态。

4️⃣时间分块。为每项任务安排时间。工作 90 分钟后休息。使用番茄工作法（Pomodoro Technique）。25 分钟的工作冲刺，伴随 5 分钟的休息。

5️⃣将任务与目标联系起来。对每个任务提问“为什么？”了解每个任务背后的原因可以将其与目标联系起来。目标 = 动力 = 更高的生产力。

6️⃣停止超负荷你的列表。忙碌并不意味着生产力。3-5 个主要任务和 1-2 个次要任务足够了。这样可以防止过劳并保持动力。

7️⃣在一天结束时回顾你的列表。回顾已完成的任务。庆祝胜利。分析错过了什么。这使得下一天的列表更好。

生产力是关于专注，而不是任务的数量。使用这些技巧来制作有效的清单：→ 头脑大扫除 → 分离任务 → 制定早晨例行程序 → 时间分块 → 将任务与目标联系起来 → 停止超负荷你的列表 → 一天结束时回顾</title>
            <link>https://nitter.cz/op7418/status/1751849029624123765#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751849029624123765#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 06:05:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Secret Leaders播客的主理人Dan，介绍了一些基于心理学的通过待办事项提高工作效率的技巧，这条推有 300 万曝光，我精简并润色了一下：<br />
<br />
拥有清单的人们能以比其他人快 40% 的速度完成工作。但是，有 99% 的人在制作他们的清单时出错。以下是正确做法：（运用心理学）<br />
<br />
1️⃣进行头脑大扫除。将脑中所有的任务写下来。所有的项目、目标和待办事项。这样可以减轻认知负担。<br />
<br />
2️⃣分离任务。将任务简化为每天的 3-5 个大任务。使用艾森豪威尔矩阵（Eisenhower Matrix）按重要性分离任务。→ 紧急且重要 → 不紧急但重要 → 紧急但不重要 → 不紧急也不重要<br />
<br />
3️⃣制定早晨例行程序列表。每天从一个简单的 5-15 分钟的例行程序开始。例如：→ 喝水 → 早餐 → 伸展 → 平板支撑 → 冥想。这有助于大脑进入生产力状态。<br />
<br />
4️⃣时间分块。为每项任务安排时间。工作 90 分钟后休息。使用番茄工作法（Pomodoro Technique）。25 分钟的工作冲刺，伴随 5 分钟的休息。<br />
<br />
5️⃣将任务与目标联系起来。对每个任务提问“为什么？”了解每个任务背后的原因可以将其与目标联系起来。目标 = 动力 = 更高的生产力。<br />
<br />
6️⃣停止超负荷你的列表。忙碌并不意味着生产力。3-5 个主要任务和 1-2 个次要任务足够了。这样可以防止过劳并保持动力。<br />
<br />
7️⃣在一天结束时回顾你的列表。回顾已完成的任务。庆祝胜利。分析错过了什么。这使得下一天的列表更好。<br />
<br />
生产力是关于专注，而不是任务的数量。使用这些技巧来制作有效的清单：→ 头脑大扫除 → 分离任务 → 制定早晨例行程序 → 时间分块 → 将任务与目标联系起来 → 停止超负荷你的列表 → 一天结束时回顾</p>
<p><a href="https://nitter.cz/danmurrayserter/status/1748722134187151511#m">nitter.cz/danmurrayserter/status/1748722134187151511#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VfTzFEbWFrQUF1RjNiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752002899729911921#m</id>
            <title>🧪一组带有法术光芒的武侠或者仙侠写实肖像，做类似的仙侠视频或者设定应该可以用的上。
主要需要介绍的就是这些法术光芒是怎么搞出来的。

主要的词有“holographic particle glowing”、“surrounded by halo”、“golden threads of light”、“smoke”等这些词都产生类似法术光芒的氛围，同时图片的比例对成像效果也有影响。

提示词：
full body shot, holographic Hanfu particle glowing, dragon entangled Asian male model, white and light silver and light blue, weapon, Ethereal, golden threads of light, dramatic lighting, cinematic color, style of Bill Sienkiewicz, smoke, Minimalism --ar 9:16 --style raw --stylize 700 --v 6

full body shot, artwork by Romulus Royo and Luis Royo, Land of the Lustrous, wonderland, Zen, surrounded by halo, ethereal artistic conception, aesthetic, Hanfu boy, dreamy, beautiful, sword, Ethereal, beautiful woman, long box braids, golden threads of light, dramatic lighting, cinematic color, style of Bill Sienkiewicz, smoke, Minimalism --ar 9:16 --style raw --stylize 700 --v 6

#晚安提示词 #Midjourney #Catjourney</title>
            <link>https://nitter.cz/op7418/status/1752002899729911921#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752002899729911921#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 16:16:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪一组带有法术光芒的武侠或者仙侠写实肖像，做类似的仙侠视频或者设定应该可以用的上。<br />
主要需要介绍的就是这些法术光芒是怎么搞出来的。<br />
<br />
主要的词有“holographic particle glowing”、“surrounded by halo”、“golden threads of light”、“smoke”等这些词都产生类似法术光芒的氛围，同时图片的比例对成像效果也有影响。<br />
<br />
提示词：<br />
full body shot, holographic Hanfu particle glowing, dragon entangled Asian male model, white and light silver and light blue, weapon, Ethereal, golden threads of light, dramatic lighting, cinematic color, style of Bill Sienkiewicz, smoke, Minimalism --ar 9:16 --style raw --stylize 700 --v 6<br />
<br />
full body shot, artwork by Romulus Royo and Luis Royo, Land of the Lustrous, wonderland, Zen, surrounded by halo, ethereal artistic conception, aesthetic, Hanfu boy, dreamy, beautiful, sword, Ethereal, beautiful woman, long box braids, golden threads of light, dramatic lighting, cinematic color, style of Bill Sienkiewicz, smoke, Minimalism --ar 9:16 --style raw --stylize 700 --v 6<br />
<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> <a href="https://nitter.cz/search?q=%23Catjourney">#Catjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZCZFZhemIwQUFYVURlLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751811283941466586#m</id>
            <title>RT by @op7418: AIGC 周刊第 57 期更新了，从这期开始会尝试在微信公众号更新，很多人还是没有看邮件的习惯。

还有Quail 用微软系邮箱订阅的朋友有可能这期会收不到，可以直接去下面的链接查看。

AIGC Weekly 57：https://quail.ink/op7418/p/aigc-weekly-57</title>
            <link>https://nitter.cz/op7418/status/1751811283941466586#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751811283941466586#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 03:35:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AIGC 周刊第 57 期更新了，从这期开始会尝试在微信公众号更新，很多人还是没有看邮件的习惯。<br />
<br />
还有Quail 用微软系邮箱订阅的朋友有可能这期会收不到，可以直接去下面的链接查看。<br />
<br />
AIGC Weekly 57：<a href="https://quail.ink/op7418/p/aigc-weekly-57">quail.ink/op7418/p/aigc-week…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0UtdWppWWEwQUFQS0N6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751960861869228112#m</id>
            <title>R to @op7418: 海螺问问的图像识别和搜索调用逻辑真不错。以后没有 GPT-4V 的话用这个也还行。

刚才我突发奇想拿之前InstructGPT 论文中LLM 训练步骤的图片让他识别和解释他基本上把图片上的训练步骤都说清楚了。

不过有的说的不够细，于是我就让他更详细的解释每个步骤，这时候他自动启动了搜索详细解释了图中的每个名词。</title>
            <link>https://nitter.cz/op7418/status/1751960861869228112#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751960861869228112#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 13:29:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>海螺问问的图像识别和搜索调用逻辑真不错。以后没有 GPT-4V 的话用这个也还行。<br />
<br />
刚才我突发奇想拿之前InstructGPT 论文中LLM 训练步骤的图片让他识别和解释他基本上把图片上的训练步骤都说清楚了。<br />
<br />
不过有的说的不够细，于是我就让他更详细的解释每个步骤，这时候他自动启动了搜索详细解释了图中的每个名词。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZBM0hUUWF3QUF4TjNDLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZBM0hUU2FvQUFLSDB6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751931754351677722#m</id>
            <title>Minimax的AI对话机器人问答产品海螺问问上线了，产品的语音对话能力、模型能力以及体验打磨都非常的不错。

体验好的一方面是海螺问问有非常自然的语音模型接入，支持语音回答问题。
还支持跟软件进行类似 ChatGPT 的实时的 AI 语音对话，但是没有 ChatGPT 那股外国腔，比如视频第一段的演示。

语音能力的另外一个杀手锏是，可以在海螺问问里面训练自己的语音模型，而且这个过程非常简单，只需要阅读屏幕上显示的一段话就可以。比如20 秒以后的视频配音就是我自己的模型。

然后就是模型能力，海螺问问接入的是 Minimax 的 Moe 模型，所以在很多特殊任务的处理上会比同规模的其他不是 Moe架构的模型要强很多。

它也支持图像的多模态识别，比如我这里在出租车的时候拍了一张照片让它识别内容，它识别出了座位下面的报纸和前面的车座，同时还进行了一定的推理，判断出这个车可能是用来拉人的不是自用的。

最后是海螺问问的整个UI细节和交互逻辑都是经过精细打磨过的，比如输入框为了支持长内容输入是可以展开的，而且支持按住Command 加上回车换行，从Web到移动端非常一致且有温度的UI界面样式。

另外海螺问问现在也是免费的，尤其是语音能力的加持让他的使用场景变得更多了，如果你不方便使用或者嫌 ChatGPT 慢的话，可以试试跟海螺问问聊聊。

这里使用：https://hailuoai.com?origin=op7418</title>
            <link>https://nitter.cz/op7418/status/1751931754351677722#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751931754351677722#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 11:34:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Minimax的AI对话机器人问答产品海螺问问上线了，产品的语音对话能力、模型能力以及体验打磨都非常的不错。<br />
<br />
体验好的一方面是海螺问问有非常自然的语音模型接入，支持语音回答问题。<br />
还支持跟软件进行类似 ChatGPT 的实时的 AI 语音对话，但是没有 ChatGPT 那股外国腔，比如视频第一段的演示。<br />
<br />
语音能力的另外一个杀手锏是，可以在海螺问问里面训练自己的语音模型，而且这个过程非常简单，只需要阅读屏幕上显示的一段话就可以。比如20 秒以后的视频配音就是我自己的模型。<br />
<br />
然后就是模型能力，海螺问问接入的是 Minimax 的 Moe 模型，所以在很多特殊任务的处理上会比同规模的其他不是 Moe架构的模型要强很多。<br />
<br />
它也支持图像的多模态识别，比如我这里在出租车的时候拍了一张照片让它识别内容，它识别出了座位下面的报纸和前面的车座，同时还进行了一定的推理，判断出这个车可能是用来拉人的不是自用的。<br />
<br />
最后是海螺问问的整个UI细节和交互逻辑都是经过精细打磨过的，比如输入框为了支持长内容输入是可以展开的，而且支持按住Command 加上回车换行，从Web到移动端非常一致且有温度的UI界面样式。<br />
<br />
另外海螺问问现在也是免费的，尤其是语音能力的加持让他的使用场景变得更多了，如果你不方便使用或者嫌 ChatGPT 慢的话，可以试试跟海螺问问聊聊。<br />
<br />
这里使用：<a href="https://hailuoai.com?origin=op7418">hailuoai.com?origin=op7418</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTE5MzE1ODc4OTIyNjQ5NjAvcHUvaW1nL1dxTjZHbmtlVlRzMU9vQ2ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751910626405044557#m</id>
            <title>这个SDXL模型有点意思同时支持中文和英文的提示词，做国内应用的可以用这个试试，看起来效果还行。

训练过程：

Taiyi-Diffusion-XL文生图模型训练主要包括了3个阶段。首先，我们制作了一个高质量的图文对数据集，每张图片都配有详细的描述性文本。

为了克服网络爬取数据的局限性，我们使用先进的视觉-语言大模型生成准确描述图片的caption。这种方法丰富了我们的数据集，确保了相关性和细节。

然后，我们从预训练的英文CLIP模型开始，为了更好地支持中文和长文本我们扩展了模型的词表和位置编码，通过大规模双语数据集扩展其双语能力。训练涉及对比损失函数和内存高效的方法。

最后，我们基于Stable-Diffusion-XL，替换了第二阶段获得的text encoder，在第一阶段获得的数据集上进行扩散模型的多分辨率、多宽高比训练。

模型下载：https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B</title>
            <link>https://nitter.cz/op7418/status/1751910626405044557#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751910626405044557#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 10:10:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个SDXL模型有点意思同时支持中文和英文的提示词，做国内应用的可以用这个试试，看起来效果还行。<br />
<br />
训练过程：<br />
<br />
Taiyi-Diffusion-XL文生图模型训练主要包括了3个阶段。首先，我们制作了一个高质量的图文对数据集，每张图片都配有详细的描述性文本。<br />
<br />
为了克服网络爬取数据的局限性，我们使用先进的视觉-语言大模型生成准确描述图片的caption。这种方法丰富了我们的数据集，确保了相关性和细节。<br />
<br />
然后，我们从预训练的英文CLIP模型开始，为了更好地支持中文和长文本我们扩展了模型的词表和位置编码，通过大规模双语数据集扩展其双语能力。训练涉及对比损失函数和内存高效的方法。<br />
<br />
最后，我们基于Stable-Diffusion-XL，替换了第二阶段获得的text encoder，在第一阶段获得的数据集上进行扩散模型的多分辨率、多宽高比训练。<br />
<br />
模型下载：<a href="https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B">huggingface.co/IDEA-CCNL/Tai…</a></p>
<p><a href="https://nitter.cz/_akhaliq/status/1751795754639769937#m">nitter.cz/_akhaliq/status/1751795754639769937#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZBSldqRGJnQUEwQzV0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751880548048650575#m</id>
            <title>Every这篇内容非常好，主要的论点是在人工智能的时代，每个执行者都将转变为协调者。

文章详细的讨论了这个转变的逻辑和理由，并且提出了分配经济下的员工需要具有哪些能力，还有这种转变会带来的影响。

下面大概总结和翻译了一下文章内容，全文翻译看最后的链接。在这个新经济中，你的价值不再取决于你掌握多少知识，而是取决于你如何有效分配和管理资源（模型）以完成工作。

以前初级员工是不需要具有管理能力的，但是在新的分配经济中即使是初几员工也需要具有管理能力，这个管理不是管理真人而是管理模型，但是管理需要的能力与管理人的时候是类似的。

未来需要的模型管理能力主要有：

1️⃣清晰连贯的愿景：现代管理者需要制定一个既明确又具体、简洁且目标明晰的愿景。模型管理者可能需要为了自己的利益和投入工作而确定一个明确的目的。

2️⃣明确的品味：优秀的管理者知道他们想要什么，并且能够清楚地表达出来。模型管理者也会面临同样的挑战。他们的品味定义得越明确，语言模型就能更好地帮助他们创造出连贯的成果。

3️⃣评估人才的能力：每位管理者都明白，招聘对于成功至关重要。未来的模型管理者也需要了解哪些 AI 模型适用于哪些任务。他们还需要能够迅速评估新模型是否足够好。

4️⃣知道何时深入细节：好的管理者知道何时深入细节，何时让团队自主行动。他们懂得提出正确的问题，知道何时进行检查，何时放手。

翻译及原文链接：https://quail.ink/op7418/p/farewell-knowledge-economy-welcome-distribution-economy</title>
            <link>https://nitter.cz/op7418/status/1751880548048650575#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751880548048650575#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 08:10:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Every这篇内容非常好，主要的论点是在人工智能的时代，每个执行者都将转变为协调者。<br />
<br />
文章详细的讨论了这个转变的逻辑和理由，并且提出了分配经济下的员工需要具有哪些能力，还有这种转变会带来的影响。<br />
<br />
下面大概总结和翻译了一下文章内容，全文翻译看最后的链接。在这个新经济中，你的价值不再取决于你掌握多少知识，而是取决于你如何有效分配和管理资源（模型）以完成工作。<br />
<br />
以前初级员工是不需要具有管理能力的，但是在新的分配经济中即使是初几员工也需要具有管理能力，这个管理不是管理真人而是管理模型，但是管理需要的能力与管理人的时候是类似的。<br />
<br />
未来需要的模型管理能力主要有：<br />
<br />
1️⃣清晰连贯的愿景：现代管理者需要制定一个既明确又具体、简洁且目标明晰的愿景。模型管理者可能需要为了自己的利益和投入工作而确定一个明确的目的。<br />
<br />
2️⃣明确的品味：优秀的管理者知道他们想要什么，并且能够清楚地表达出来。模型管理者也会面临同样的挑战。他们的品味定义得越明确，语言模型就能更好地帮助他们创造出连贯的成果。<br />
<br />
3️⃣评估人才的能力：每位管理者都明白，招聘对于成功至关重要。未来的模型管理者也需要了解哪些 AI 模型适用于哪些任务。他们还需要能够迅速评估新模型是否足够好。<br />
<br />
4️⃣知道何时深入细节：好的管理者知道何时深入细节，何时让团队自主行动。他们懂得提出正确的问题，知道何时进行检查，何时放手。<br />
<br />
翻译及原文链接：<a href="https://quail.ink/op7418/p/farewell-knowledge-economy-welcome-distribution-economy">quail.ink/op7418/p/farewell-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VfdURjS2JBQUFkWDA5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751607245698437446#m</id>
            <title>RT by @op7418: 🧪今天继续整点抽象的壁纸，想复刻一下之前那种3D渲染出来的抽象玻璃质感的图片。
整了半天发现只需要前面加上“extreme macro photo of clean polished glass”清洁抛光玻璃的极微距照片，这段话就行。

然后后面的词就是常规的颜色以及背景颜色，还有模糊之类的内容了，这个提示词想产生好看的图，主要的就是配色的描述，不能只用纯色的颜色描述，得用比如海军蓝、靛蓝等多种颜色混合。

提示词：
extreme macro photo of clean polished glass, edges with light from four different colors, blurred, warmcore, light red and indigo, grey background, natural colors, depth of field --ar 16:9 --style raw --stylize 0 --v 6

#晚安提示词 #Midjourney #Catjourney</title>
            <link>https://nitter.cz/op7418/status/1751607245698437446#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751607245698437446#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 14:04:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪今天继续整点抽象的壁纸，想复刻一下之前那种3D渲染出来的抽象玻璃质感的图片。<br />
整了半天发现只需要前面加上“extreme macro photo of clean polished glass”清洁抛光玻璃的极微距照片，这段话就行。<br />
<br />
然后后面的词就是常规的颜色以及背景颜色，还有模糊之类的内容了，这个提示词想产生好看的图，主要的就是配色的描述，不能只用纯色的颜色描述，得用比如海军蓝、靛蓝等多种颜色混合。<br />
<br />
提示词：<br />
extreme macro photo of clean polished glass, edges with light from four different colors, blurred, warmcore, light red and indigo, grey background, natural colors, depth of field --ar 16:9 --style raw --stylize 0 --v 6<br />
<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> <a href="https://nitter.cz/search?q=%23Catjourney">#Catjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2cWE0QUE5MWthLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2Z2FVQUFnOTVCLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2dGEwQUF5dWZrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2OWFvQUFmdGZzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751602173845348373#m</id>
            <title>RT by @op7418: Huggingface上的一篇内容，非常详细的介绍了如何从零开始实现一个MoE架构的语言模型。

文章详细解释了模型的实施过程，包括采用稀疏混合专家取代传统的前馈神经网络，实现 top-k 门控和带噪声的 top-k 门控，以及采用 Kaiming He 初始化技术。

作者还说明了从 makemore 架构保持不变的元素，比如数据集处理、分词预处理和语言建模任务。

最后还提供了一个 GitHub 仓库链接，用于实现模型的整个过程。

内容地址：https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch</title>
            <link>https://nitter.cz/op7418/status/1751602173845348373#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751602173845348373#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 13:44:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Huggingface上的一篇内容，非常详细的介绍了如何从零开始实现一个MoE架构的语言模型。<br />
<br />
文章详细解释了模型的实施过程，包括采用稀疏混合专家取代传统的前馈神经网络，实现 top-k 门控和带噪声的 top-k 门控，以及采用 Kaiming He 初始化技术。<br />
<br />
作者还说明了从 makemore 架构保持不变的元素，比如数据集处理、分词预处理和语言建模任务。<br />
<br />
最后还提供了一个 GitHub 仓库链接，用于实现模型的整个过程。<br />
<br />
内容地址：<a href="https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch">huggingface.co/blog/AviSoori…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3d2E2Y2FJQUFTMUN5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751791023599280309#m</id>
            <title>Arc 发布了新的移动端浏览器 Arc Search 他们认为移动端主要是用浏览器的场景是搜索。目前国内 App store 可以下载。

新浏览器专注于提高搜索效率，内置了帮你浏览的功能会自动帮你总结前六个搜索结果，帮助你选择。

同时他们会推出一个新的标签同步服务来同步Windows、iOS 和 Mac 的浏览器标签。</title>
            <link>https://nitter.cz/op7418/status/1751791023599280309#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751791023599280309#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 02:15:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Arc 发布了新的移动端浏览器 Arc Search 他们认为移动端主要是用浏览器的场景是搜索。目前国内 App store 可以下载。<br />
<br />
新浏览器专注于提高搜索效率，内置了帮你浏览的功能会自动帮你总结前六个搜索结果，帮助你选择。<br />
<br />
同时他们会推出一个新的标签同步服务来同步Windows、iOS 和 Mac 的浏览器标签。</p>
<p><a href="https://nitter.cz/joshm/status/1751734580124787028#m">nitter.cz/joshm/status/1751734580124787028#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751600800668004750#m</id>
            <title>Dolphin-Mixtral这个模型可以让Json格式文件的输出正确率从Mixtral原模型的60%提高到90%。</title>
            <link>https://nitter.cz/op7418/status/1751600800668004750#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751600800668004750#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 13:39:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Dolphin-Mixtral这个模型可以让Json格式文件的输出正确率从Mixtral原模型的60%提高到90%。</p>
<p><a href="https://nitter.cz/DrTBehrens/status/1751523382196117675#m">nitter.cz/DrTBehrens/status/1751523382196117675#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1751388813237141542#m</id>
            <title>RT by @op7418: 来自浙江大学、腾讯 AI 实验室和西湖大学的新论文：《WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models》

这篇论文详细的讲解了如何借助 GPT-4V 这样的多模态模型，与开放网络中的网站交互，完成用户的各项指令。

如果你有做过类似的事情的话，会发现其实还是很有挑战的，因为让 AI 遵循指令操作网页，特定的网站相对容易，因为网页元素和路径比较固定，但是开放环境的话，每个网站都不一样，交互方式也千差万别，再加上浮动广告、弹出窗口和网页内容实时更新等等。

具体在实现层面，要先理解当前网页的内容，然后根据用户指令，在网页上选择正确的操作，根据操作的结果再继续下一步操作，直到完成任务。

举例来说，我们要去苹果官网查询，附近的哪个苹果店能买到特定型号的 iPad 保护壳（Smart Folio）。如果是人操作的话，要打开官网，找到配件页面，搜索关键字，找到配件查看详情，从详情页选择弹出位置搜索界面，输入邮编，找到最近的苹果店。但这系列操作对于 AI 来说还是很有挑战的。

那么 WebVoyager 是怎么来做的呢？

一、AI 如何浏览操作网页？

首先，WebVoyager 不是用的普通浏览器，而是基于 Selenium，这是一个自动化网页测试工具，可以方便的截图，可以自动化操作网页浏览器。

但是要让 GPT-4V 能识别和操作网页元素，还需要对网页上的可以操作的元素进行标记，WebVoyager 开发了一个叫 GPT-4-ACT4 的 JavaScript 工具，它能够根据网页元素的类型自动识别交互元素，并在这些元素上覆盖带有数字标记的黑色边框。

此外，GPT-4-ACT4 还能向智能体提供了一些辅助文本，如交互元素内的文字内容、元素类型以及 aria-label 属性中可能的注释文本，以简化观察过程。

如果你有些自动化测试代码的经验的话，可以知道我们可以用 JavaScript 或者 Python 脚本灵活的操作网页做任意操作，但是对于 AI 来说，如果让它直接写代码可能会出错率比较高，所以 WebVoyager 将常用的网页操作进行了归类，提供了有限的几种操作，例如：点击、输入、滚动、等待、返回上一页等等。

这样 AI 就不需要写代码，而是直接基于这几种操作给出清晰的指令，根据 AI 的指令，WebVoyager 将指令翻译成操作 Selenium 的代码操作网页。

这些为后面 GPT-4V 识别和提供后续指令提供了基础，否则 GPT-4V 无法清晰的描述出下一步要采取的操作。

二、如何让 AI 清晰的给出网页操作的指令？

然后就是 Prompt，Prompt 就是和 AI 交互的指令。要让 GPT-4V 帮助我们完成任务，光有截图还不够，还需要让 AI 能根据截图和任务，清晰的说明下一步如何操作，才能去相应的网站，借助外部工具进行交互。

WebVoyager 采用的是 ReAct 的 Prompt 框架，让 AI 能够根据目标任务和当前状态，推理出下一步的行动，每一步都采用：思考（Thought）、行动（Action）和观察（Observe）的结构，思考推理出行动，行动完成后观察行动后的结果，根据观察的结果进一步思考，思考推理出下一步的行动，这样一步步，直到完成任务。

举例来说要查询附近哪个苹果店可以买到特定型号的 iPad 保护壳，基于 ReAct 的框架是这样做的：

思考 1：我要找哪个苹果店可以买到特定型号的 iPad 保护壳，我需要打开苹果官网
行动 1：打开苹果官网
观察 1：苹果官网已经打开，上面有 Mac、iPhone、iPad、配件……导航

思考 2：iPad 保护壳属于配件，我已经打开配件页面
行动 2：点击打开配件页面
观察 2：配件页面打开，有导航，有推荐配件，有搜索框……

思考 3：我应该使用搜索框输入 Smart Folio 搜索
行动 3：在搜索框中输入 Smart Folio，点击搜索按钮
观察 3：列出了所有 Smart Folio 相关产品，第一项是 Smart Folio for iPad Pro 11，第二项是 Smart Folio for iPad……

思考 4：第一个搜索结果就是我想要的，我需要点击进入详情页面
行动 4：打开第一个 Smart Folio 详情页
观察 4：标题…介绍…图片…苹果商店……

思考 5：点击苹果商店链接查看有哪些商店
行动 5：点击苹果商店链接
观察 5：弹出对话框，有苹果商店列表 1,2,3,4…，有位置搜索框

思考 6：这些苹果商店离我太远，需要按照我的邮编寻找最近的
行动 6：输入邮编到位置输入框，搜索
观察 6：列出了新的苹果商店列表 1,2,3,4…

思考 7：第一个苹果商店就是离我最近的苹果店，任务完成

三、效果如何？

根据论文上的结果显示，WebVoyager 在任务成功率上达到了 55.7%。这个结果显然还达不到替代人类操作的效果，但是作为现阶段来说，已经算是个不错的成绩。未来随着 AI 能力的增强，成功率应该可以做到更高。

目前 WebVoyager 任务失败的原因主要有：

1. 导航失败
a) 如果智能体的搜索查询不够精确和明确，它会被海量无关搜索结果淹没。在这种情况下，智能体可能倾向于浏览这些不相关的结果，而不是纠正之前的错误；b) 当只有屏幕的一小部分可滚动时，智能体可能找不到正确的滚动区域，反复进行无效的滚动操作；c) 有时在网页中部，智能体难以决定是向上滚动还是向下滚动。
2. 视觉识别不足
a) 智能体无法正确理解一些不常见的模式，比如误将代表发音的字符或数学公式理解错了；b) 智能体没有识别出两次观察之间的微妙差异，误以为操作失败了；c) 由于元素之间位置接近，智能体有时会选错了操作对象。比如，它可能会将相邻的元素混淆，或者把日历上的数字误认为是数值标签。有时，文本信息对于区分密集的网页元素至关重要。
3. 幻觉
理解和遵循复杂的提示对智能体来说是一个重大挑战。此外，长时间的操作路径可能导致上下文过于冗长，从而妨碍了有效的指令执行。

总的来说，WebVoyager 是一个很不错的尝试，期待未来 AI 能真正的帮助我们操作网页，解放双手。

完整的论文翻译：https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models</title>
            <link>https://nitter.cz/dotey/status/1751388813237141542#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1751388813237141542#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 23:36:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来自浙江大学、腾讯 AI 实验室和西湖大学的新论文：《WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models》<br />
<br />
这篇论文详细的讲解了如何借助 GPT-4V 这样的多模态模型，与开放网络中的网站交互，完成用户的各项指令。<br />
<br />
如果你有做过类似的事情的话，会发现其实还是很有挑战的，因为让 AI 遵循指令操作网页，特定的网站相对容易，因为网页元素和路径比较固定，但是开放环境的话，每个网站都不一样，交互方式也千差万别，再加上浮动广告、弹出窗口和网页内容实时更新等等。<br />
<br />
具体在实现层面，要先理解当前网页的内容，然后根据用户指令，在网页上选择正确的操作，根据操作的结果再继续下一步操作，直到完成任务。<br />
<br />
举例来说，我们要去苹果官网查询，附近的哪个苹果店能买到特定型号的 iPad 保护壳（Smart Folio）。如果是人操作的话，要打开官网，找到配件页面，搜索关键字，找到配件查看详情，从详情页选择弹出位置搜索界面，输入邮编，找到最近的苹果店。但这系列操作对于 AI 来说还是很有挑战的。<br />
<br />
那么 WebVoyager 是怎么来做的呢？<br />
<br />
一、AI 如何浏览操作网页？<br />
<br />
首先，WebVoyager 不是用的普通浏览器，而是基于 Selenium，这是一个自动化网页测试工具，可以方便的截图，可以自动化操作网页浏览器。<br />
<br />
但是要让 GPT-4V 能识别和操作网页元素，还需要对网页上的可以操作的元素进行标记，WebVoyager 开发了一个叫 GPT-4-ACT4 的 JavaScript 工具，它能够根据网页元素的类型自动识别交互元素，并在这些元素上覆盖带有数字标记的黑色边框。<br />
<br />
此外，GPT-4-ACT4 还能向智能体提供了一些辅助文本，如交互元素内的文字内容、元素类型以及 aria-label 属性中可能的注释文本，以简化观察过程。<br />
<br />
如果你有些自动化测试代码的经验的话，可以知道我们可以用 JavaScript 或者 Python 脚本灵活的操作网页做任意操作，但是对于 AI 来说，如果让它直接写代码可能会出错率比较高，所以 WebVoyager 将常用的网页操作进行了归类，提供了有限的几种操作，例如：点击、输入、滚动、等待、返回上一页等等。<br />
<br />
这样 AI 就不需要写代码，而是直接基于这几种操作给出清晰的指令，根据 AI 的指令，WebVoyager 将指令翻译成操作 Selenium 的代码操作网页。<br />
<br />
这些为后面 GPT-4V 识别和提供后续指令提供了基础，否则 GPT-4V 无法清晰的描述出下一步要采取的操作。<br />
<br />
二、如何让 AI 清晰的给出网页操作的指令？<br />
<br />
然后就是 Prompt，Prompt 就是和 AI 交互的指令。要让 GPT-4V 帮助我们完成任务，光有截图还不够，还需要让 AI 能根据截图和任务，清晰的说明下一步如何操作，才能去相应的网站，借助外部工具进行交互。<br />
<br />
WebVoyager 采用的是 ReAct 的 Prompt 框架，让 AI 能够根据目标任务和当前状态，推理出下一步的行动，每一步都采用：思考（Thought）、行动（Action）和观察（Observe）的结构，思考推理出行动，行动完成后观察行动后的结果，根据观察的结果进一步思考，思考推理出下一步的行动，这样一步步，直到完成任务。<br />
<br />
举例来说要查询附近哪个苹果店可以买到特定型号的 iPad 保护壳，基于 ReAct 的框架是这样做的：<br />
<br />
思考 1：我要找哪个苹果店可以买到特定型号的 iPad 保护壳，我需要打开苹果官网<br />
行动 1：打开苹果官网<br />
观察 1：苹果官网已经打开，上面有 Mac、iPhone、iPad、配件……导航<br />
<br />
思考 2：iPad 保护壳属于配件，我已经打开配件页面<br />
行动 2：点击打开配件页面<br />
观察 2：配件页面打开，有导航，有推荐配件，有搜索框……<br />
<br />
思考 3：我应该使用搜索框输入 Smart Folio 搜索<br />
行动 3：在搜索框中输入 Smart Folio，点击搜索按钮<br />
观察 3：列出了所有 Smart Folio 相关产品，第一项是 Smart Folio for iPad Pro 11，第二项是 Smart Folio for iPad……<br />
<br />
思考 4：第一个搜索结果就是我想要的，我需要点击进入详情页面<br />
行动 4：打开第一个 Smart Folio 详情页<br />
观察 4：标题…介绍…图片…苹果商店……<br />
<br />
思考 5：点击苹果商店链接查看有哪些商店<br />
行动 5：点击苹果商店链接<br />
观察 5：弹出对话框，有苹果商店列表 1,2,3,4…，有位置搜索框<br />
<br />
思考 6：这些苹果商店离我太远，需要按照我的邮编寻找最近的<br />
行动 6：输入邮编到位置输入框，搜索<br />
观察 6：列出了新的苹果商店列表 1,2,3,4…<br />
<br />
思考 7：第一个苹果商店就是离我最近的苹果店，任务完成<br />
<br />
三、效果如何？<br />
<br />
根据论文上的结果显示，WebVoyager 在任务成功率上达到了 55.7%。这个结果显然还达不到替代人类操作的效果，但是作为现阶段来说，已经算是个不错的成绩。未来随着 AI 能力的增强，成功率应该可以做到更高。<br />
<br />
目前 WebVoyager 任务失败的原因主要有：<br />
<br />
1. 导航失败<br />
a) 如果智能体的搜索查询不够精确和明确，它会被海量无关搜索结果淹没。在这种情况下，智能体可能倾向于浏览这些不相关的结果，而不是纠正之前的错误；b) 当只有屏幕的一小部分可滚动时，智能体可能找不到正确的滚动区域，反复进行无效的滚动操作；c) 有时在网页中部，智能体难以决定是向上滚动还是向下滚动。<br />
2. 视觉识别不足<br />
a) 智能体无法正确理解一些不常见的模式，比如误将代表发音的字符或数学公式理解错了；b) 智能体没有识别出两次观察之间的微妙差异，误以为操作失败了；c) 由于元素之间位置接近，智能体有时会选错了操作对象。比如，它可能会将相邻的元素混淆，或者把日历上的数字误认为是数值标签。有时，文本信息对于区分密集的网页元素至关重要。<br />
3. 幻觉<br />
理解和遵循复杂的提示对智能体来说是一个重大挑战。此外，长时间的操作路径可能导致上下文过于冗长，从而妨碍了有效的指令执行。<br />
<br />
总的来说，WebVoyager 是一个很不错的尝试，期待未来 AI 能真正的帮助我们操作网页，解放双手。<br />
<br />
完整的论文翻译：<a href="https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models">baoyu.io/translations/ai-pap…</a></p>
<p><a href="https://nitter.cz/wyu_nd/status/1750743389287669940#m">nitter.cz/wyu_nd/status/1750743389287669940#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U0dVhsRVcwQUFfQnNXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U0dWdpVVhJQUUxZkp1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U0dXNNTVcwQUFCTDVJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>