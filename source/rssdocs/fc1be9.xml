<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733730238809665549#m</id>
            <title>Mixtral 8x7B 已经可以在Poe上体验了，他们不缺卡，这次速度应该很快，哈哈。</title>
            <link>https://nitter.cz/op7418/status/1733730238809665549#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733730238809665549#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 06:07:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral 8x7B 已经可以在Poe上体验了，他们不缺卡，这次速度应该很快，哈哈。</p>
<p><a href="https://nitter.cz/poe_platform/status/1733727569965621286#m">nitter.cz/poe_platform/status/1733727569965621286#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733729669550407780#m</id>
            <title>每次Comfyui连线的时候，我感觉我的工作流跟图吧大佬的电脑是一样的，主打一个能跑就行。</title>
            <link>https://nitter.cz/op7418/status/1733729669550407780#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733729669550407780#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 06:05:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>每次Comfyui连线的时候，我感觉我的工作流跟图吧大佬的电脑是一样的，主打一个能跑就行。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E5eDhEdmJJQUFOakxuLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E5eDhEeWFRQUFoc2ZuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733702267478704265#m</id>
            <title>R to @op7418: 同时最近刷抖音频繁刷到视频转视频做的美女擦边视频，几乎都可以稳定起量。
所以即使不产品化，把Animatediff的一个流程吃透也可以稳定的产生收益了。</title>
            <link>https://nitter.cz/op7418/status/1733702267478704265#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733702267478704265#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 04:16:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>同时最近刷抖音频繁刷到视频转视频做的美女擦边视频，几乎都可以稳定起量。<br />
所以即使不产品化，把Animatediff的一个流程吃透也可以稳定的产生收益了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733701614689771957#m</id>
            <title>最近在做Animatediff产品化的一些事情，遇到的问题都是类似的。不能完全自动化必须依赖人工辅助，这样就不可能面向C端用户。
所以仅仅是Animatediff得这些小问题如果解决了依托开源社区的方案进行优化也可以有比较好的产品了。
如果没有深厚的科研背景又想做视频生成产品的话这个路子其实也是可行的。</title>
            <link>https://nitter.cz/op7418/status/1733701614689771957#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733701614689771957#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 04:14:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近在做Animatediff产品化的一些事情，遇到的问题都是类似的。不能完全自动化必须依赖人工辅助，这样就不可能面向C端用户。<br />
所以仅仅是Animatediff得这些小问题如果解决了依托开源社区的方案进行优化也可以有比较好的产品了。<br />
如果没有深厚的科研背景又想做视频生成产品的话这个路子其实也是可行的。</p>
<p><a href="https://nitter.cz/hylarucoder/status/1733667272731009186#m">nitter.cz/hylarucoder/status/1733667272731009186#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733690660081856734#m</id>
            <title>R to @op7418: 这个教程比较好的是还带了练习用的动画原视频，这样练习的时候就不用自己找素材了。
练习用的素材地址：
https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2F7mxuph93xp3c1.gif%3Fformat%3Dmp4%26s%3D8b3ab1a6c52d1c1d76fea469507418eef50db451</title>
            <link>https://nitter.cz/op7418/status/1733690660081856734#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733690660081856734#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 03:30:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个教程比较好的是还带了练习用的动画原视频，这样练习的时候就不用自己找素材了。<br />
练习用的素材地址：<br />
<a href="https://teddit.net/media?url=https%3A%2F%2Fpreview.redd.it%2F7mxuph93xp3c1.gif%3Fformat%3Dmp4%26s%3D8b3ab1a6c52d1c1d76fea469507418eef50db451">teddit.net/media?url=https%3…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733684523593068782#m</id>
            <title>Civitai最近有非常多的用Animatediff做出来的好内容，而且很多都有教程，所以开个帖子推荐一下这些视频和内容。
首先是the_marconi用QRcode Monster做的产品徽标动画，首先先找到基础动画，然后用Comfyui转视频，最后到AE里最后调整。

教程和原视频地址：https://civitai.com/articles/3172/nature-powers-netflix-seasons-workflow-and-details</title>
            <link>https://nitter.cz/op7418/status/1733684523593068782#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733684523593068782#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 03:06:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Civitai最近有非常多的用Animatediff做出来的好内容，而且很多都有教程，所以开个帖子推荐一下这些视频和内容。<br />
首先是the_marconi用QRcode Monster做的产品徽标动画，首先先找到基础动画，然后用Comfyui转视频，最后到AE里最后调整。<br />
<br />
教程和原视频地址：<a href="https://civitai.com/articles/3172/nature-powers-netflix-seasons-workflow-and-details">civitai.com/articles/3172/na…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzM2ODQzOTI0MjAzNDc5MDQvcHUvaW1nL1JtLWY3OHFHNFhHcTVLNVEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733541708917616924#m</id>
            <title>RT by @op7418: 好久没发 #晚安提示词 了，用Niji微调了一个风格，主要特点是人像生成非常柔和，对白色加上光芒提示词响应非常好，背景会出现很好看的炫光，同时服装也会有彩色光泽。
图片都是4K分辨率，喜欢的可以直接下载，提示词和风格代码在ALT里面。</title>
            <link>https://nitter.cz/op7418/status/1733541708917616924#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733541708917616924#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 17:38:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>好久没发 <a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> 了，用Niji微调了一个风格，主要特点是人像生成非常柔和，对白色加上光芒提示词响应非常好，背景会出现很好看的炫光，同时服装也会有彩色光泽。<br />
图片都是4K分辨率，喜欢的可以直接下载，提示词和风格代码在ALT里面。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3R2RSWGFzQUFhekJXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3R1djRmE0QUFBeS1zLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3R2FHaWE0QUFPLWIzLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3RzlCOWFvQUFGN0RxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733503266141704195#m</id>
            <title>RT by @op7418: 这个老哥带来了首个开源MoE架构模型Mixtral-8x7b比较详细的介绍，还有MoE模型是什么，以及和GPT-4 MoE架构的区别。可以看看比我上午自己搜的强不少：

◆该模型以 87 GB 的种子文件形式发布
◆看似是 GPT-4 的精简版
◆在 X 平台发布，没有配套的新闻发布会，且对更多细节守口如瓶。

谷歌凭借其精心编排的演示视频令 AI 社区敬畏，但现在这段视频正受到广泛批评。  
另一方面，开源 AI 创业公司 Mistral AI 发布了一个包含 8 个 7B 级别专家的 MoE 模型。 

专家混合（MoE）是什么？  
专家混合（MoE）是用于提高大语言模型效率和准确度的技术。这种方法将复杂任务划分为更小、更易管理的子任务，每个子任务由专门的小型模型或“专家”负责。
以下是简要说明：  
1. 专家层：这些是在特定领域训练有素的小型神经网络。每个专家以其独特专长的方式处理相同的输入。
2. 门控网络：这是 MoE 架构的决策核心。它判断哪个专家最适合处理特定输入。网络为输入数据与每个专家的兼容性打分，然后根据这些得分确定每个专家在任务中的角色。 
这些组成部分共同确保正确的专家处理正确的任务。门控网络有效地将输入引导至最合适的专家，而专家则专注于他们擅长的领域。这种合作培训使得整体模型更加多才多艺、能力更强。
关于 Mistral 新 MoE 的详情（来自 Reddit）  在对每个 Token 进行推理时，只有 2 个专家被使用。
这一信息可以从模型的元数据中获得： 
 {"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}

与 GPT-4 的比较Mistral 的 8x7B 模型采用了与 GPT-4 相似的架构，但规模更小：
◆总共 8 个专家模型，而不是 16 个（减少了一半）
◆每个专家拥有 7B 参数，而不是 166B（减少了 24 倍） 
◆总共约 42B 参数，而非 1.8T（减少了 42 倍）
◆与原版 GPT-4 相同的 32K 上下文限制</title>
            <link>https://nitter.cz/op7418/status/1733503266141704195#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733503266141704195#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:06:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个老哥带来了首个开源MoE架构模型Mixtral-8x7b比较详细的介绍，还有MoE模型是什么，以及和GPT-4 MoE架构的区别。可以看看比我上午自己搜的强不少：<br />
<br />
◆该模型以 87 GB 的种子文件形式发布<br />
◆看似是 GPT-4 的精简版<br />
◆在 X 平台发布，没有配套的新闻发布会，且对更多细节守口如瓶。<br />
<br />
谷歌凭借其精心编排的演示视频令 AI 社区敬畏，但现在这段视频正受到广泛批评。  <br />
另一方面，开源 AI 创业公司 Mistral AI 发布了一个包含 8 个 7B 级别专家的 MoE 模型。 <br />
<br />
专家混合（MoE）是什么？  <br />
专家混合（MoE）是用于提高大语言模型效率和准确度的技术。这种方法将复杂任务划分为更小、更易管理的子任务，每个子任务由专门的小型模型或“专家”负责。<br />
以下是简要说明：  <br />
1. 专家层：这些是在特定领域训练有素的小型神经网络。每个专家以其独特专长的方式处理相同的输入。<br />
2. 门控网络：这是 MoE 架构的决策核心。它判断哪个专家最适合处理特定输入。网络为输入数据与每个专家的兼容性打分，然后根据这些得分确定每个专家在任务中的角色。 <br />
这些组成部分共同确保正确的专家处理正确的任务。门控网络有效地将输入引导至最合适的专家，而专家则专注于他们擅长的领域。这种合作培训使得整体模型更加多才多艺、能力更强。<br />
关于 Mistral 新 MoE 的详情（来自 Reddit）  在对每个 Token 进行推理时，只有 2 个专家被使用。<br />
这一信息可以从模型的元数据中获得： <br />
 {"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}<br />
<br />
与 GPT-4 的比较Mistral 的 8x7B 模型采用了与 GPT-4 相似的架构，但规模更小：<br />
◆总共 8 个专家模型，而不是 16 个（减少了一半）<br />
◆每个专家拥有 7B 参数，而不是 166B（减少了 24 倍） <br />
◆总共约 42B 参数，而非 1.8T（减少了 42 倍）<br />
◆与原版 GPT-4 相同的 32K 上下文限制</p>
<p><a href="https://nitter.cz/Saboo_Shubham_/status/1733364854973456561#m">nitter.cz/Saboo_Shubham_/status/1733364854973456561#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733554168533786629#m</id>
            <title>学到了，公众号的确实也很需要这个能力。</title>
            <link>https://nitter.cz/op7418/status/1733554168533786629#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733554168533786629#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 18:28:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学到了，公众号的确实也很需要这个能力。</p>
<p><a href="https://nitter.cz/HzaoHzao/status/1733337729436618997#m">nitter.cz/HzaoHzao/status/1733337729436618997#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733544630418121044#m</id>
            <title>R to @op7418: 还有几张图也发一下吧 #midjourney</title>
            <link>https://nitter.cz/op7418/status/1733544630418121044#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733544630418121044#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 17:50:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还有几张图也发一下吧 <a href="https://nitter.cz/search?q=%23midjourney">#midjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3SmQyOWJRQUFzNmo3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3SmZjMWJRQUFwZnB3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3SmlVYWFJQUFXbm1RLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3SmthSGFBQUFXU1U3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733542707145093604#m</id>
            <title>R to @op7418: 照例推荐一下和莱森搞得Midjourney风格收集网站，里面有各种风格和提示词，有需要的可以看看:
https://catjourney.framer.website/</title>
            <link>https://nitter.cz/op7418/status/1733542707145093604#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733542707145093604#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 17:42:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>照例推荐一下和莱森搞得Midjourney风格收集网站，里面有各种风格和提示词，有需要的可以看看:<br />
<a href="https://catjourney.framer.website/">catjourney.framer.website/</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzIwNzE3NDk4NDQ1MDA0OS85LVFDVkRWcT9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733515976212451706#m</id>
            <title>R to @op7418: 我前几天还想试试转一下洛基第二季他坐上时间线王座的那段来着。</title>
            <link>https://nitter.cz/op7418/status/1733515976212451706#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733515976212451706#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:56:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我前几天还想试试转一下洛基第二季他坐上时间线王座的那段来着。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733515802408824908#m</id>
            <title>最近用Animatediff将传统经典电影转成动漫风格的流程看起来很成熟了。这个就是黑客帝国经典的打斗转的。</title>
            <link>https://nitter.cz/op7418/status/1733515802408824908#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733515802408824908#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:55:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近用Animatediff将传统经典电影转成动漫风格的流程看起来很成熟了。这个就是黑客帝国经典的打斗转的。</p>
<p><a href="https://nitter.cz/Mrboofyy/status/1733157260770083058#m">nitter.cz/Mrboofyy/status/1733157260770083058#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733513341816221758#m</id>
            <title>哈哈 最近生病了卷不动了，一个支原体让我瘫痪了二十天。</title>
            <link>https://nitter.cz/op7418/status/1733513341816221758#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733513341816221758#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:46:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈 最近生病了卷不动了，一个支原体让我瘫痪了二十天。</p>
<p><a href="https://nitter.cz/vista8/status/1733509609959379094#m">nitter.cz/vista8/status/1733509609959379094#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733498433166840116#m</id>
            <title>最近也把之前积累的零碎搬到Heptabase上了。</title>
            <link>https://nitter.cz/op7418/status/1733498433166840116#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733498433166840116#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 14:46:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近也把之前积累的零碎搬到Heptabase上了。</p>
<p><a href="https://nitter.cz/lyson_ober/status/1733496688504140029#m">nitter.cz/lyson_ober/status/1733496688504140029#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733482321368752182#m</id>
            <title>Mixtral-8x7b更详细的一些信息：
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。
◆这些专家模型都采用了 Mistral-7B 的架构。</title>
            <link>https://nitter.cz/op7418/status/1733482321368752182#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733482321368752182#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:42:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b更详细的一些信息：<br />
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。<br />
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。<br />
◆这些专家模型都采用了 Mistral-7B 的架构。</p>
<p><a href="https://nitter.cz/carrigmat/status/1733159362028257353#m">nitter.cz/carrigmat/status/1733159362028257353#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733480631966072878#m</id>
            <title>青龙之前写的MagicAnimate本地版本现在支持把视频或者gif转换为对应的Openpose或者其他ContorlNet动画了，这下用Animatediff做视频的时候也可以用了。
可以搞一个自己的动作库。做视频的时候直接用就行。</title>
            <link>https://nitter.cz/op7418/status/1733480631966072878#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733480631966072878#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:36:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>青龙之前写的MagicAnimate本地版本现在支持把视频或者gif转换为对应的Openpose或者其他ContorlNet动画了，这下用Animatediff做视频的时候也可以用了。<br />
可以搞一个自己的动作库。做视频的时候直接用就行。</p>
<p><a href="https://nitter.cz/bdsqlsz/status/1733478639692562792#m">nitter.cz/bdsqlsz/status/1733478639692562792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>