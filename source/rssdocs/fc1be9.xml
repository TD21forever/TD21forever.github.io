<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748363331344453680#m</id>
            <title>RT by @op7418: Midjoirney V6 的风格微调有点意思啊，本质上就是风格转换功能，上传照片之后把图片转成提示词描述的风格，感觉玩法挺多的。</title>
            <link>https://nitter.cz/op7418/status/1748363331344453680#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748363331344453680#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 15:14:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjoirney V6 的风格微调有点意思啊，本质上就是风格转换功能，上传照片之后把图片转成提示词描述的风格，感觉玩法挺多的。</p>
<p><a href="https://nitter.cz/nickfloats/status/1748039999264584016#m">nitter.cz/nickfloats/status/1748039999264584016#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748688816242855964#m</id>
            <title>🧪Midjourney真是每天都能给我带来惊喜，看到一个玉剑的设定图，就想试试能不能用MJ还原出来，结果真还原出来了，提高风格化之后还有了第二张图。

提示词：
a lotustailed jade sword made in china, in the style of kris knight, luminous 3d objects, nature-inspired art nouveau, flower and nature motifs, cambodian art, balanced symmetry, precisionist style --ar 9:16 --v 6.0

#晚安提示词 #midjourney #catjourney</title>
            <link>https://nitter.cz/op7418/status/1748688816242855964#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748688816242855964#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:47:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪Midjourney真是每天都能给我带来惊喜，看到一个玉剑的设定图，就想试试能不能用MJ还原出来，结果真还原出来了，提高风格化之后还有了第二张图。<br />
<br />
提示词：<br />
a lotustailed jade sword made in china, in the style of kris knight, luminous 3d objects, nature-inspired art nouveau, flower and nature motifs, cambodian art, balanced symmetry, precisionist style --ar 9:16 --v 6.0<br />
<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23midjourney">#midjourney</a> <a href="https://nitter.cz/search?q=%23catjourney">#catjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTWEh4Y2JJQUFnS0x1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTWEh4Y2FrQUFmbGFWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748681893761995194#m</id>
            <title>昨晚东西挺多啊,新版本的SD图像模型？饱和度还是有点过高，写实的图像比现在的XL要强。

第一张图可能需要更强的提示词理解能力才能实现，所以新模型可能也会带来更强的提示词理解能力。</title>
            <link>https://nitter.cz/op7418/status/1748681893761995194#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748681893761995194#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:20:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚东西挺多啊,新版本的SD图像模型？饱和度还是有点过高，写实的图像比现在的XL要强。<br />
<br />
第一张图可能需要更强的提示词理解能力才能实现，所以新模型可能也会带来更强的提示词理解能力。</p>
<p><a href="https://nitter.cz/EMostaque/status/1748472853802942945#m">nitter.cz/EMostaque/status/1748472853802942945#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748681025730081094#m</id>
            <title>最近又刷到了一些Domo AI制作的视频，感觉他们的流程又进行了优化呢，稳定性现在高的离谱，转场和一些特效都可以完美重绘掉。

国内抖音上也看到很多用这个做效果的内容，用来补充和增强氛围挺好的。

突然想起来我还是付费用户所以就去Discord看了一下，发现还更新了两个新的风格，剪纸和油画，就找了两个视频试了。

我自己用Animatediff生成的视频有时也会去Domo AI过一下，转换风格的同时稳定性也会高一些，最后一段是原始视频和重绘视频，前两段是真实视频重绘的。

Domo AI现在依然有免费试用，进入Discord服务器之后，选择一个Use Domo分类下的频道输入/video回车就可以转换视频了。

使用Domo：https://discord.com/invite/BnkYWZr3na</title>
            <link>https://nitter.cz/op7418/status/1748681025730081094#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748681025730081094#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:17:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近又刷到了一些Domo AI制作的视频，感觉他们的流程又进行了优化呢，稳定性现在高的离谱，转场和一些特效都可以完美重绘掉。<br />
<br />
国内抖音上也看到很多用这个做效果的内容，用来补充和增强氛围挺好的。<br />
<br />
突然想起来我还是付费用户所以就去Discord看了一下，发现还更新了两个新的风格，剪纸和油画，就找了两个视频试了。<br />
<br />
我自己用Animatediff生成的视频有时也会去Domo AI过一下，转换风格的同时稳定性也会高一些，最后一段是原始视频和重绘视频，前两段是真实视频重绘的。<br />
<br />
Domo AI现在依然有免费试用，进入Discord服务器之后，选择一个Use Domo分类下的频道输入/video回车就可以转换视频了。<br />
<br />
使用Domo：<a href="https://discord.com/invite/BnkYWZr3na">discord.com/invite/BnkYWZr3n…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg2ODA5NDE3NDc0NDU3NjAvcHUvaW1nL3FBTFpoUTRBTWd3anV3dkcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748381779290198296#m</id>
            <title>RT by @op7418: Nicolas介绍了一下如何根据画面内容的深度信息，设置Runway多运动笔刷的参数。
让整个画面内容的运动更加自然。思路非常清晰，可以学习一下👇：

教程：利用多动作刷进行深度运动制作

昨天，@runwayml 向所有用户推出了多动作刷功能。这个功能在增强创作者对作品控制力方面，标志着一大步进。

我最喜欢的应用之一，就是在不同深度层面上添加逼真的动作。

现在，让我们深入了解我是如何实现这一点的！

🔍 识别物体深度（图 1）

为不同深度层面的物体添加动作的第一步，是确定它们与相机的距离。通常这个过程不需要太专业的观察能力。在这个例子中，以下是我想要用不同动作强度来制作动画的不同深度层面：

1️⃣ 街道上的阴影 
2️⃣ 正在远去的汽车 
3️⃣ 靠近的植物 
4️⃣ 附近的植物 
5️⃣ 远处的植物

🗺️ 可选：利用深度图（图 2）

为了更精确，或者处理复杂素材时，你可以使用深度图来更准确地判断物体的远近。Runway 实际上提供了一个“提取深度”的工具，你可以在工具概览中找到它！
如果你之前没用过深度图，一个基本原则是：区域越亮，表示它越靠近相机。

🖌️ 应用多动作刷（图 3）

识别出不同的深度层面后，我们就可以开始用不同的动作刷来突出它们。

我通常从最近的深度层面开始，给它设定一个动作值 5。接着我会按层递减，离相机越远，动作值就越小。在下面的例子中，我选择了以下值：

阴影：Ambient 5 近处植物：Ambient 3 汽车：Proximity -2.5 + Ambient 2 附近植物：Ambient 1 远处植物：Ambient 0.5

🎬 制作具有逼真深度运动的视频（视频）

只要动作值设置得当，你就能制作出动作更加逼真的视频。比如，相机近处的对象将比远处的物体移动得更快。

正如往常，略微的动作值差异，往往能让整体构图更加逼真。

如果你尝试了这种技术，欢迎在下面的评论区分享你的成果！👇🏼</title>
            <link>https://nitter.cz/op7418/status/1748381779290198296#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748381779290198296#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 16:27:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nicolas介绍了一下如何根据画面内容的深度信息，设置Runway多运动笔刷的参数。<br />
让整个画面内容的运动更加自然。思路非常清晰，可以学习一下👇：<br />
<br />
教程：利用多动作刷进行深度运动制作<br />
<br />
昨天，<a href="https://nitter.cz/runwayml" title="Runway">@runwayml</a> 向所有用户推出了多动作刷功能。这个功能在增强创作者对作品控制力方面，标志着一大步进。<br />
<br />
我最喜欢的应用之一，就是在不同深度层面上添加逼真的动作。<br />
<br />
现在，让我们深入了解我是如何实现这一点的！<br />
<br />
🔍 识别物体深度（图 1）<br />
<br />
为不同深度层面的物体添加动作的第一步，是确定它们与相机的距离。通常这个过程不需要太专业的观察能力。在这个例子中，以下是我想要用不同动作强度来制作动画的不同深度层面：<br />
<br />
1️⃣ 街道上的阴影 <br />
2️⃣ 正在远去的汽车 <br />
3️⃣ 靠近的植物 <br />
4️⃣ 附近的植物 <br />
5️⃣ 远处的植物<br />
<br />
🗺️ 可选：利用深度图（图 2）<br />
<br />
为了更精确，或者处理复杂素材时，你可以使用深度图来更准确地判断物体的远近。Runway 实际上提供了一个“提取深度”的工具，你可以在工具概览中找到它！<br />
如果你之前没用过深度图，一个基本原则是：区域越亮，表示它越靠近相机。<br />
<br />
🖌️ 应用多动作刷（图 3）<br />
<br />
识别出不同的深度层面后，我们就可以开始用不同的动作刷来突出它们。<br />
<br />
我通常从最近的深度层面开始，给它设定一个动作值 5。接着我会按层递减，离相机越远，动作值就越小。在下面的例子中，我选择了以下值：<br />
<br />
阴影：Ambient 5 近处植物：Ambient 3 汽车：Proximity -2.5 + Ambient 2 附近植物：Ambient 1 远处植物：Ambient 0.5<br />
<br />
🎬 制作具有逼真深度运动的视频（视频）<br />
<br />
只要动作值设置得当，你就能制作出动作更加逼真的视频。比如，相机近处的对象将比远处的物体移动得更快。<br />
<br />
正如往常，略微的动作值差异，往往能让整体构图更加逼真。<br />
<br />
如果你尝试了这种技术，欢迎在下面的评论区分享你的成果！👇🏼</p>
<p><a href="https://nitter.cz/iamneubert/status/1748339127613841587#m">nitter.cz/iamneubert/status/1748339127613841587#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748665904928268714#m</id>
            <title>R to @op7418: 补一个完整的四段视频</title>
            <link>https://nitter.cz/op7418/status/1748665904928268714#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748665904928268714#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 11:16:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补一个完整的四段视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg2NjU4NDM1NjE3MDk1NjgvcHUvaW1nL014eHZhNmpBTFhnd0ExSXguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748663306775371965#m</id>
            <title>牛皮，padphone用SVD做的新片子，巨兽和机甲，更期待SVD的新模型了。</title>
            <link>https://nitter.cz/op7418/status/1748663306775371965#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748663306775371965#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 11:06:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>牛皮，padphone用SVD做的新片子，巨兽和机甲，更期待SVD的新模型了。</p>
<p><a href="https://nitter.cz/lepadphone/status/1748645137692148076#m">nitter.cz/lepadphone/status/1748645137692148076#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748661489555378226#m</id>
            <title>SVD新版本的模型？感觉Stability AI要靠这个翻身了。这个清晰度和流畅度，别人还搞鸡毛。</title>
            <link>https://nitter.cz/op7418/status/1748661489555378226#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748661489555378226#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 10:59:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SVD新版本的模型？感觉Stability AI要靠这个翻身了。这个清晰度和流畅度，别人还搞鸡毛。</p>
<p><a href="https://nitter.cz/EMostaque/status/1748405750907457548#m">nitter.cz/EMostaque/status/1748405750907457548#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748184369976713484#m</id>
            <title>RT by @op7418: 谷歌推出了用于用于增强大语言模型的选择性预测框架 ASPIRE。

这个预测框架可以在 LLM 输出的时候给出一个可靠性评分，让用户更好的判断 LLM 输出内容的可靠性。

选择性预测会让大语言模型在给出答案的同时，提供一个选择性得分，用以表示答案正确的可能性。

训练方式：
ASPIRE 的工作方式是对大语言模型进行针对性的微调，使其更擅长于问题回答任务，并训练模型自行评估其生成答案的正确性。该框架包括三个阶段：特定任务的调优、答案采样和自我评估学习（self-evaluation learning）。在特定任务调优阶段，ASPIRE 对预先训练好的大语言模型进行微调，以提升其预测表现。答案采样阶段则是针对每个训练问题生成多种答案，进而构建自我评估学习的数据集。到了自我评估学习阶段，ASPIRE 引入可调整参数，并对这些参数进行微调，以培养模型的自我评估能力。

结论：
ASPIRE 的出现标志着大语言模型领域的一次变革，它强调了模型容量并非其性能的唯一决定因素。实际上，通过策略性的调整，即便是小型模型也能实现更精确、更有信心的预测，从而显著提升模型的有效性。

内容来源：https://blog.research.google/2024/01/introducing-aspire-for-selective.html</title>
            <link>https://nitter.cz/op7418/status/1748184369976713484#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748184369976713484#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:23:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌推出了用于用于增强大语言模型的选择性预测框架 ASPIRE。<br />
<br />
这个预测框架可以在 LLM 输出的时候给出一个可靠性评分，让用户更好的判断 LLM 输出内容的可靠性。<br />
<br />
选择性预测会让大语言模型在给出答案的同时，提供一个选择性得分，用以表示答案正确的可能性。<br />
<br />
训练方式：<br />
ASPIRE 的工作方式是对大语言模型进行针对性的微调，使其更擅长于问题回答任务，并训练模型自行评估其生成答案的正确性。该框架包括三个阶段：特定任务的调优、答案采样和自我评估学习（self-evaluation learning）。在特定任务调优阶段，ASPIRE 对预先训练好的大语言模型进行微调，以提升其预测表现。答案采样阶段则是针对每个训练问题生成多种答案，进而构建自我评估学习的数据集。到了自我评估学习阶段，ASPIRE 引入可调整参数，并对这些参数进行微调，以培养模型的自我评估能力。<br />
<br />
结论：<br />
ASPIRE 的出现标志着大语言模型领域的一次变革，它强调了模型容量并非其性能的唯一决定因素。实际上，通过策略性的调整，即便是小型模型也能实现更精确、更有信心的预测，从而显著提升模型的有效性。<br />
<br />
内容来源：<a href="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">blog.research.google/2024/01…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0VMTVNneGE4QUFSdXNLLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dFTE1TZ3hhOEFBUnVzSy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748189182307303606#m</id>
            <title>RT by @op7418: 小扎刚发言完，Meta 就出王炸？推出了可以进行自我奖励的 LLM。
简单来说就是语言模型可以自我判断模型质量，从而实现一定程度上的自我进化。

使用这个方法微调的 Llama 2 70B 模型，优于 AlpacaEval 2.0 排行榜上的Claude 2、Gemini Pro 和 GPT-4 0613等模型。

实现方式：

自奖励语言模型，这类智能体具备双重功能：一方面（i）它们能够作为遵循指令的模型，针对给定的提示生成回应；另一方面（ii）它们还能创造并评估新的指令遵循示例，并将这些示例加入到自己的训练集中。

我们采用了与 Xu 等人（2023年）最近提出的类似的迭代式动态评价优化（Iterative DPO）框架来训练这些模型。从一个基础模型出发，在每一轮迭代中，模型都会经历一个自我指令生成的过程，在这个过程中，模型针对新创造的提示生成候选回应，并由模型自身对这些回应进行奖励评分。

这个评分过程是通过让大语言模型扮演评判员（LLM-as-a-Judge）的方式来实现的，这本身也是一种遵循指令的任务。然后，根据这些生成的数据构建一个偏好数据集，并利用动态评价优化方法来训练下一轮的模型。

论文地址：https://arxiv.org/html/2401.10020v1</title>
            <link>https://nitter.cz/op7418/status/1748189182307303606#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748189182307303606#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:42:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小扎刚发言完，Meta 就出王炸？推出了可以进行自我奖励的 LLM。<br />
简单来说就是语言模型可以自我判断模型质量，从而实现一定程度上的自我进化。<br />
<br />
使用这个方法微调的 Llama 2 70B 模型，优于 AlpacaEval 2.0 排行榜上的Claude 2、Gemini Pro 和 GPT-4 0613等模型。<br />
<br />
实现方式：<br />
<br />
自奖励语言模型，这类智能体具备双重功能：一方面（i）它们能够作为遵循指令的模型，针对给定的提示生成回应；另一方面（ii）它们还能创造并评估新的指令遵循示例，并将这些示例加入到自己的训练集中。<br />
<br />
我们采用了与 Xu 等人（2023年）最近提出的类似的迭代式动态评价优化（Iterative DPO）框架来训练这些模型。从一个基础模型出发，在每一轮迭代中，模型都会经历一个自我指令生成的过程，在这个过程中，模型针对新创造的提示生成候选回应，并由模型自身对这些回应进行奖励评分。<br />
<br />
这个评分过程是通过让大语言模型扮演评判员（LLM-as-a-Judge）的方式来实现的，这本身也是一种遵循指令的任务。然后，根据这些生成的数据构建一个偏好数据集，并利用动态评价优化方法来训练下一轮的模型。<br />
<br />
论文地址：<a href="https://arxiv.org/html/2401.10020v1">arxiv.org/html/2401.10020v1</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VMUXhvNGFJQUFrS05FLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748191803965378723#m</id>
            <title>RT by @op7418: 另一个 ComfyUI 上的AnimateAnyone实现，看起来比其他的稳定一些，面部崩的不是很厉害。

也是基于摩尔线程的AnimateAnyone方案，不过这个的文档写的非常详细，而且提供了不同采样器的效果预览。

项目地址：https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved</title>
            <link>https://nitter.cz/op7418/status/1748191803965378723#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748191803965378723#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:53:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个 ComfyUI 上的AnimateAnyone实现，看起来比其他的稳定一些，面部崩的不是很厉害。<br />
<br />
也是基于摩尔线程的AnimateAnyone方案，不过这个的文档写的非常详细，而且提供了不同采样器的效果预览。<br />
<br />
项目地址：<a href="https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved">github.com/MrForExample/Comf…</a></p>
<p><a href="https://nitter.cz/MrForExample/status/1748080976104734858#m">nitter.cz/MrForExample/status/1748080976104734858#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgxOTE3NzM1NDQwMzAyMDgvcHUvaW1nL3JmVVR0Q3dwa3lwNEtXaVUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748180416840995281#m</id>
            <title>RT by @op7418: 刚发现摩尔线程前几天复原了阿里的单图跳舞项目并且已经开源训练代码，你可以训练自己的AnimateAnyone模型。
有个基于摩尔线程开源的版本制作了 ComfyUI 节点，并且提供了基础的工作流。
现在可以在ComfyUI中非常简单的让单图跳舞了。

节点地址：https://github.com/chaojie/ComfyUI-Moore-AnimateAnyone?tab=readme-ov-file</title>
            <link>https://nitter.cz/op7418/status/1748180416840995281#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748180416840995281#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:07:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚发现摩尔线程前几天复原了阿里的单图跳舞项目并且已经开源训练代码，你可以训练自己的AnimateAnyone模型。<br />
有个基于摩尔线程开源的版本制作了 ComfyUI 节点，并且提供了基础的工作流。<br />
现在可以在ComfyUI中非常简单的让单图跳舞了。<br />
<br />
节点地址：<a href="https://github.com/chaojie/ComfyUI-Moore-AnimateAnyone?tab=readme-ov-file">github.com/chaojie/ComfyUI-M…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VMSHlmZWIwQUF3TmNlLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748293897237991794#m</id>
            <title>帮快手的朋友招两个设计，主要在探索UI 和运营设计的自动生成，需要在 AIGC 领域有过设计实践，懂 SD，有前端能力更好。
邮件标题或者备注可以加上在歸藏这里看到的。

下面是完整的 JD：
快手研发设计中心团队，负责面向公司内部的产品交互设计。我们正在进行前沿的 AIGC 设计探索，并把探索的成果变成产品，实现 UI 和运营设计的自动生成，希望找到志同道合的朋友一起同行:

1. 希望你是 2024 年 6/7 月毕业的本科/研究生,专业不限
2. 有 B 端 UI 设计 / 工具类 UI 设计能力
3. 希望你在通用设计能力上至少有一个亮点（良好的表达 / 优秀的审美 / 出色的逻辑性 / 良好的同理心…）
4. 希望你对新鲜事物感兴趣，并且有一定自己的探索和尝试
5. 在 AIGC 方向有过学习实践，熟悉 SD，或有一定前端代码能力优先
6. 坐标北京

如果你对我们的工作感兴趣，欢迎投递简历和作品集，我的微信：ding_zu，邮箱 dzwangyihan@gmail.com</title>
            <link>https://nitter.cz/op7418/status/1748293897237991794#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748293897237991794#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 10:38:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>帮快手的朋友招两个设计，主要在探索UI 和运营设计的自动生成，需要在 AIGC 领域有过设计实践，懂 SD，有前端能力更好。<br />
邮件标题或者备注可以加上在歸藏这里看到的。<br />
<br />
下面是完整的 JD：<br />
快手研发设计中心团队，负责面向公司内部的产品交互设计。我们正在进行前沿的 AIGC 设计探索，并把探索的成果变成产品，实现 UI 和运营设计的自动生成，希望找到志同道合的朋友一起同行:<br />
<br />
1. 希望你是 2024 年 6/7 月毕业的本科/研究生,专业不限<br />
2. 有 B 端 UI 设计 / 工具类 UI 设计能力<br />
3. 希望你在通用设计能力上至少有一个亮点（良好的表达 / 优秀的审美 / 出色的逻辑性 / 良好的同理心…）<br />
4. 希望你对新鲜事物感兴趣，并且有一定自己的探索和尝试<br />
5. 在 AIGC 方向有过学习实践，熟悉 SD，或有一定前端代码能力优先<br />
6. 坐标北京<br />
<br />
如果你对我们的工作感兴趣，欢迎投递简历和作品集，我的微信：ding_zu，邮箱 dzwangyihan@gmail.com</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VNd0JDRWJ3QUFDTTZVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748289276708855904#m</id>
            <title>Nick这张用 Midjourney 生成的手帐照片有点意思，可以用来表达观点和配图，比如他表达意思就是：

在X平台上成长的唯一秘诀就是发布优质内容，而“关于如何在X平台上成长”的内容很少能算作优质内容。

我也试了一下居然真的可以，后面加上-- s 0 会生成的更稳定。

完整提示词：
a note that reads "The only secret to growing on X is putting out good content and "content about how to grow on X is very rarely good content" --ar 4:5 --style raw --v 6.0 --s 0</title>
            <link>https://nitter.cz/op7418/status/1748289276708855904#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748289276708855904#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 10:20:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nick这张用 Midjourney 生成的手帐照片有点意思，可以用来表达观点和配图，比如他表达意思就是：<br />
<br />
在X平台上成长的唯一秘诀就是发布优质内容，而“关于如何在X平台上成长”的内容很少能算作优质内容。<br />
<br />
我也试了一下居然真的可以，后面加上-- s 0 会生成的更稳定。<br />
<br />
完整提示词：<br />
a note that reads "The only secret to growing on X is putting out good content and "content about how to grow on X is very rarely good content" --ar 4:5 --style raw --v 6.0 --s 0</p>
<p><a href="https://nitter.cz/nickfloats/status/1748285824339153407#m">nitter.cz/nickfloats/status/1748285824339153407#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VNcm1rZWFZQUFDc2FsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748249758723150303#m</id>
            <title>专注于 Animatediff 动画的赛博菩萨Jerry Davos发布了一套用 IPapadter 和遮罩给跳舞视频转视频，并且可以更换背景的工作流。
老样子里面除了工作流之外还有非常详细的教程，还有涉及到的模型。

工作流下载：https://www.patreon.com/posts/v3-0-bg-changer-96735652</title>
            <link>https://nitter.cz/op7418/status/1748249758723150303#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748249758723150303#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 07:43:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>专注于 Animatediff 动画的赛博菩萨Jerry Davos发布了一套用 IPapadter 和遮罩给跳舞视频转视频，并且可以更换背景的工作流。<br />
老样子里面除了工作流之外还有非常详细的教程，还有涉及到的模型。<br />
<br />
工作流下载：<a href="https://www.patreon.com/posts/v3-0-bg-changer-96735652">patreon.com/posts/v3-0-bg-ch…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgyMzkwNjkxNTc3MDc3NzYvcHUvaW1nLzFKLXlWd21WUjZybkpCbDEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748233784582144147#m</id>
            <title>剪过的完整版本</title>
            <link>https://nitter.cz/op7418/status/1748233784582144147#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748233784582144147#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 06:39:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>剪过的完整版本</p>
<p><a href="https://nitter.cz/indigo11/status/1748194964016889990#m">nitter.cz/indigo11/status/1748194964016889990#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748194254667829407#m</id>
            <title>R to @op7418: 补一个摩尔线程的项目地址：https://github.com/MooreThreads/Moore-AnimateAnyone</title>
            <link>https://nitter.cz/op7418/status/1748194254667829407#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748194254667829407#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 04:02:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补一个摩尔线程的项目地址：<a href="https://github.com/MooreThreads/Moore-AnimateAnyone">github.com/MooreThreads/Moor…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0ODMxNDg4MDQ1NTcyMDk2MC9QbkdZSkVtMT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748012298038653237#m</id>
            <title>RT by @op7418: 今天无意间刷到《恋与深空》的宣传片，才发现女性向游戏已经做到这种地步了，NPC的表情动作和配音都非常自然生动。
场景也非常漂亮，麻了，男性向没有这么高质量的恋爱游戏啊。</title>
            <link>https://nitter.cz/op7418/status/1748012298038653237#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748012298038653237#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 15:59:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天无意间刷到《恋与深空》的宣传片，才发现女性向游戏已经做到这种地步了，NPC的表情动作和配音都非常自然生动。<br />
场景也非常漂亮，麻了，男性向没有这么高质量的恋爱游戏啊。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgwMTE4MjAzNDkzNzQ0NjQvcHUvaW1nL2Q5LVo2VkNSc0Izc0dyUnUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>