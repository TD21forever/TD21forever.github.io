<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742477352448544847#m</id>
            <title>这里有一个非常简单的ComfyUI 自定义节点开发指南，只需要 5 分钟，你甚至不需要会代码，跟着也能写一个自定义节点。

ComfyUI 之所以现在开始流行有一个很重要的原因是他的插件和节点开发成本比 WebUI 低很多。
但是他本身的开发文档写的很乱，导致入门看起来很困难。

Reddit 一个老哥写了一个自定义节点的开发指南，我跟着走了一遍发现真的简单，写的也很详细。
所以就翻译了一下由于比较长，就扔在周刊里面了。

教程和原文地址：https://quail.ink/op7418/p/create-custom-node-in-5-minutes-comfyui-custom-node-getting-started-guide</title>
            <link>https://nitter.cz/op7418/status/1742477352448544847#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742477352448544847#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:25:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这里有一个非常简单的ComfyUI 自定义节点开发指南，只需要 5 分钟，你甚至不需要会代码，跟着也能写一个自定义节点。<br />
<br />
ComfyUI 之所以现在开始流行有一个很重要的原因是他的插件和节点开发成本比 WebUI 低很多。<br />
但是他本身的开发文档写的很乱，导致入门看起来很困难。<br />
<br />
Reddit 一个老哥写了一个自定义节点的开发指南，我跟着走了一遍发现真的简单，写的也很详细。<br />
所以就翻译了一下由于比较长，就扔在周刊里面了。<br />
<br />
教程和原文地址：<a href="https://quail.ink/op7418/p/create-custom-node-in-5-minutes-comfyui-custom-node-getting-started-guide">quail.ink/op7418/p/create-cu…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M2RjFCcmJFQUFGQm5sLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742188908832870455#m</id>
            <title>RT by @op7418: 卧槽，老哥牛皮，这清晰度和运动幅度，还得是SVD，质量确实是现在最好的了，就是门槛有点高。

Stability AI抱着金砖要饭啊，赶紧把SVD搞搞啥融资没有。</title>
            <link>https://nitter.cz/op7418/status/1742188908832870455#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742188908832870455#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 14:19:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽，老哥牛皮，这清晰度和运动幅度，还得是SVD，质量确实是现在最好的了，就是门槛有点高。<br />
<br />
Stability AI抱着金砖要饭啊，赶紧把SVD搞搞啥融资没有。</p>
<p><a href="https://nitter.cz/lepadphone/status/1742039954719666681#m">nitter.cz/lepadphone/status/1742039954719666681#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742224037890429163#m</id>
            <title>RT by @op7418: 这个EMBEDDING模型有点意思，将色板放在了EMBEDDING里面。
使用的时候SD生成的图像就会只有色板里面的颜色，而且是搭配好的。一个文件一个色板。

使用方式：
把触发词写在正向提示词就行，强度0.1 to 1.5，可以把色板中的某个颜色写在反向提示词降低颜色强度。

模型下载：https://civitai.com/models/240967/color-palettes</title>
            <link>https://nitter.cz/op7418/status/1742224037890429163#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742224037890429163#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 16:39:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个EMBEDDING模型有点意思，将色板放在了EMBEDDING里面。<br />
使用的时候SD生成的图像就会只有色板里面的颜色，而且是搭配好的。一个文件一个色板。<br />
<br />
使用方式：<br />
把触发词写在正向提示词就行，强度0.1 to 1.5，可以把色板中的某个颜色写在反向提示词降低颜色强度。<br />
<br />
模型下载：<a href="https://civitai.com/models/240967/color-palettes">civitai.com/models/240967/co…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyZmM4WWIwQUFxeklTLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyZmVKeGJrQUF2NEgxLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyZmZsU2FvQUFZelRGLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyZmhINmJ3QUFtVElWLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742192659375256019#m</id>
            <title>RT by @op7418: 之前介绍的AI小说生成工具MidReal AI，更新了Beta版本，模型更新的同时增加了很多新功能。
顺便给大家要了点福利，可以看最后。

下面是具体更新内容：

模型更新：新模型生成内容更有逻辑，更连贯。
小说展厅：官网上线了推荐小说的页面，比在Discord里面看着更舒服。
新功能：比如/start_private这个命令可以创建完全私密的内容，不用怕你发的提示词导致自己社死了。

关注我的账号和下面MidReal官推，评论这条推文，并加入Discord体验beta版本。
就可以参与抽取3名用户送出三个月的无限量生成会员。

这里尝试：https://discord.gg/ReKvgchE3P</title>
            <link>https://nitter.cz/op7418/status/1742192659375256019#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742192659375256019#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 14:34:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前介绍的AI小说生成工具MidReal AI，更新了Beta版本，模型更新的同时增加了很多新功能。<br />
顺便给大家要了点福利，可以看最后。<br />
<br />
下面是具体更新内容：<br />
<br />
模型更新：新模型生成内容更有逻辑，更连贯。<br />
小说展厅：官网上线了推荐小说的页面，比在Discord里面看着更舒服。<br />
新功能：比如/start_private这个命令可以创建完全私密的内容，不用怕你发的提示词导致自己社死了。<br />
<br />
关注我的账号和下面MidReal官推，评论这条推文，并加入Discord体验beta版本。<br />
就可以参与抽取3名用户送出三个月的无限量生成会员。<br />
<br />
这里尝试：<a href="https://discord.gg/ReKvgchE3P">discord.gg/ReKvgchE3P</a></p>
<p><a href="https://nitter.cz/midreal_ai/status/1742184410047471651#m">nitter.cz/midreal_ai/status/1742184410047471651#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyQzFieWJJQUVFbHNBLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742208505204109729#m</id>
            <title>RT by @op7418: 一份提示工程最佳实践，内容比较基础，各位应该都看过很多次了。
不过她说这也是和人沟通的最佳实践有点意思。仔细想了一下确实是这样的，这些内容在跟人沟通的时候也很有用。

> 如何编写清晰/具体的说明 
> 给模型时间思考 
> 多次提示 
> 指导模型 
> 分解提示 
> 使用外部工具

全文链接：https://mphr.notion.site/Prompt-Engineering-Best-Practices-0839585d4bce4c6abb0b551b2107a92a</title>
            <link>https://nitter.cz/op7418/status/1742208505204109729#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742208505204109729#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 15:37:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一份提示工程最佳实践，内容比较基础，各位应该都看过很多次了。<br />
不过她说这也是和人沟通的最佳实践有点意思。仔细想了一下确实是这样的，这些内容在跟人沟通的时候也很有用。<br />
<br />
> 如何编写清晰/具体的说明 <br />
> 给模型时间思考 <br />
> 多次提示 <br />
> 指导模型 <br />
> 分解提示 <br />
> 使用外部工具<br />
<br />
全文链接：<a href="https://mphr.notion.site/Prompt-Engineering-Best-Practices-0839585d4bce4c6abb0b551b2107a92a">mphr.notion.site/Prompt-Engi…</a></p>
<p><a href="https://nitter.cz/SarahChieng/status/1741926266087870784#m">nitter.cz/SarahChieng/status/1741926266087870784#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742230668447490483#m</id>
            <title>破釜沉舟了也是，不过现在各国也没有明确的判决，可能吸引不了多少公司。</title>
            <link>https://nitter.cz/op7418/status/1742230668447490483#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742230668447490483#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 17:05:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>破釜沉舟了也是，不过现在各国也没有明确的判决，可能吸引不了多少公司。</p>
<p><a href="https://nitter.cz/dotey/status/1742229424425037947#m">nitter.cz/dotey/status/1742229424425037947#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742226452035391999#m</id>
            <title>阿里这个图像中生成文字的Anytext对中文的支持确实还行，有时候会多笔画或者少笔画，不过已经很可以了。
中文生成确实还是只能指望中国公司做，没办法。

这里体验：https://www.modelscope.cn/studios/damo/studio_anytext/summary</title>
            <link>https://nitter.cz/op7418/status/1742226452035391999#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742226452035391999#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 16:48:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里这个图像中生成文字的Anytext对中文的支持确实还行，有时候会多笔画或者少笔画，不过已经很可以了。<br />
中文生成确实还是只能指望中国公司做，没办法。<br />
<br />
这里体验：<a href="https://www.modelscope.cn/studios/damo/studio_anytext/summary">modelscope.cn/studios/damo/s…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyaFBnSWJFQUF6Q0Y5LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742218585853096081#m</id>
            <title>腾讯 M 2 UGen 这个多模态音乐生成模型有点意思。
不止可以从文字生成音乐，还支持图像、视频和音频生成音乐，而且可以编辑已有的音乐。

项目简介：
模型利用 MERT 等编码器进行音乐理解、ViT 进行图像理解和 ViViT 进行视频理解，并使用 MusicGen/AudioLDM2 模型作为音乐生成模型（音乐解码器），再加上适配器和 LLaMA 2 模型，使该模型能够多种能力。

项目地址：https://crypto-code.github.io/M2UGen-Demo/</title>
            <link>https://nitter.cz/op7418/status/1742218585853096081#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742218585853096081#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 16:17:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>腾讯 M 2 UGen 这个多模态音乐生成模型有点意思。<br />
不止可以从文字生成音乐，还支持图像、视频和音频生成音乐，而且可以编辑已有的音乐。<br />
<br />
项目简介：<br />
模型利用 MERT 等编码器进行音乐理解、ViT 进行图像理解和 ViViT 进行视频理解，并使用 MusicGen/AudioLDM2 模型作为音乐生成模型（音乐解码器），再加上适配器和 LLaMA 2 模型，使该模型能够多种能力。<br />
<br />
项目地址：<a href="https://crypto-code.github.io/M2UGen-Demo/">crypto-code.github.io/M2UGen…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDIyMTg1MzIyNTgyMDU2OTYvcHUvaW1nLzV0RWZqZXpFUDRkSnRTaFEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742212461779169377#m</id>
            <title>Text2Immersion：可以通过文本直接生成3D场景，不过看演示能动的角度比较有限，可能再转就会穿帮，不过也很有意思了。

项目简介：
Text2Immersion，这是一种优雅的方法，可以从文本提示生成高质量的3D沉浸式场景。
我们提出的流程首先通过预训练的2D扩散和深度估计模型逐步生成高斯云。然后在高斯云上进行细化阶段，插值和细化以增强生成场景的细节。
与主流方法不同，这些方法通常专注于单个对象或室内场景，或者采用缩小轨迹，我们的方法生成具有各种对象的不同场景，甚至扩展到虚构场景的创建。
因此，Text2Immersion可能对虚拟现实、游戏开发和自动化内容创作等各种应用产生广泛影响。

项目地址：https://ken-ouyang.github.io/text2immersion/index.html</title>
            <link>https://nitter.cz/op7418/status/1742212461779169377#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742212461779169377#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 15:53:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Text2Immersion：可以通过文本直接生成3D场景，不过看演示能动的角度比较有限，可能再转就会穿帮，不过也很有意思了。<br />
<br />
项目简介：<br />
Text2Immersion，这是一种优雅的方法，可以从文本提示生成高质量的3D沉浸式场景。<br />
我们提出的流程首先通过预训练的2D扩散和深度估计模型逐步生成高斯云。然后在高斯云上进行细化阶段，插值和细化以增强生成场景的细节。<br />
与主流方法不同，这些方法通常专注于单个对象或室内场景，或者采用缩小轨迹，我们的方法生成具有各种对象的不同场景，甚至扩展到虚构场景的创建。<br />
因此，Text2Immersion可能对虚拟现实、游戏开发和自动化内容创作等各种应用产生广泛影响。<br />
<br />
项目地址：<a href="https://ken-ouyang.github.io/text2immersion/index.html">ken-ouyang.github.io/text2im…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDIyMTIwNzk2NzM5MDkyNDgvcHUvaW1nLzV4ak11TVZHaUlBLUFwNk4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742210816466886742#m</id>
            <title>liblib和Civitai，现在都是类似的模式。
不过liblib给的是真钱，Civitai给的是算力代币。都是在自己补贴，现在还是圈地阶段不敢大规模收费。</title>
            <link>https://nitter.cz/op7418/status/1742210816466886742#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742210816466886742#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 15:46:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>liblib和Civitai，现在都是类似的模式。<br />
不过liblib给的是真钱，Civitai给的是算力代币。都是在自己补贴，现在还是圈地阶段不敢大规模收费。</p>
<p><a href="https://nitter.cz/Gorden_Sun/status/1741985433649926332#m">nitter.cz/Gorden_Sun/status/1741985433649926332#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742205534244331744#m</id>
            <title>这个场景确实好，而且收费的话用户的付费意愿应该也挺强的。</title>
            <link>https://nitter.cz/op7418/status/1742205534244331744#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742205534244331744#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 15:25:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个场景确实好，而且收费的话用户的付费意愿应该也挺强的。</p>
<p><a href="https://nitter.cz/heroooooh/status/1742149598259638730#m">nitter.cz/heroooooh/status/1742149598259638730#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742204838010896411#m</id>
            <title>图像生成中手部修复项目Hand Refiner的使用方法。

从下面链接里面下载模型和图像处理器，然后会自动找到需要修复图像的手部位置生成新的深度图和对应遮罩。

模型下载：https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/tree/main</title>
            <link>https://nitter.cz/op7418/status/1742204838010896411#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742204838010896411#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 15:22:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>图像生成中手部修复项目Hand Refiner的使用方法。<br />
<br />
从下面链接里面下载模型和图像处理器，然后会自动找到需要修复图像的手部位置生成新的深度图和对应遮罩。<br />
<br />
模型下载：<a href="https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/tree/main">huggingface.co/hr16/ControlN…</a></p>
<p><a href="https://nitter.cz/toyxyz3/status/1742183610952884695#m">nitter.cz/toyxyz3/status/1742183610952884695#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MyT0JBdGFVQUFqS1pJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742203595662192943#m</id>
            <title>阿里可以让人物照片说话的项目DreamTalk，开源了。
支持包括歌曲、多种语言的语音、嘈杂的音频在内的各种声音匹配。

这里下载模型：https://huggingface.co/damo-vilab/dreamtalk</title>
            <link>https://nitter.cz/op7418/status/1742203595662192943#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742203595662192943#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 15:18:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里可以让人物照片说话的项目DreamTalk，开源了。<br />
支持包括歌曲、多种语言的语音、嘈杂的音频在内的各种声音匹配。<br />
<br />
这里下载模型：<a href="https://huggingface.co/damo-vilab/dreamtalk">huggingface.co/damo-vilab/dr…</a></p>
<p><a href="https://nitter.cz/_akhaliq/status/1742192199800864769#m">nitter.cz/_akhaliq/status/1742192199800864769#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MjA5NjU1NjY2MzQyNzA3Mi9KZGpXNHduRj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742160019674841187#m</id>
            <title>🧪今天继续来玩抽象，这是一个可玩性比较强的抽象图片模版，通过更改不同位置的内容，可以生成完全不同的抽象图案。

版本更新对抽象图片生成的影响最大。因为本来就不好描述，所以模型版本一更新用旧的提示词就会生成出完全不同的东西。

下面提示词模板里面的材质、色彩以及是否是 3D 渲染还有图像比例，只要稍微一做改动，整个画面变化就会非常大。

整个图像的底子就是第一部分的“抽象光效背景照片”，后面的内容可以理解为在上面加材质，比如你选择玻璃，它就会生成比较坚硬笔直的线条，如果选择流动的树脂，生成的内容就会是流体，选择丝绸或者其他布料，生成的内容又会比较轻盈。

在搭配上主次四种颜色，还有是否为渲染，图像的变化也会比较明显，即使所有提示词都一致比例不同画面也会很不相同。抽卡模版，哈哈。

提示词模板：
abstract vector abstract light effect background stock photo | clip art, [材质], [主要色彩], in the style of atelier olschinsky, lo-fi aesthetics, [3D rendering], [次要色彩], 35mm film, ian davenport, cross-processed film [比例]

提示词示例：
abstract vector abstract light effect background stock photo | clip art, glass, colorful rainbow, in the style of atelier olschinsky, lo-fi aesthetics, 3D rendering, light silver and bronze,  35mm film, ian davenport, cross-processed film --ar 16:9 --v 6.0 --style raw
#晚安提示词 #midjourneyV6</title>
            <link>https://nitter.cz/op7418/status/1742160019674841187#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742160019674841187#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 12:24:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪今天继续来玩抽象，这是一个可玩性比较强的抽象图片模版，通过更改不同位置的内容，可以生成完全不同的抽象图案。<br />
<br />
版本更新对抽象图片生成的影响最大。因为本来就不好描述，所以模型版本一更新用旧的提示词就会生成出完全不同的东西。<br />
<br />
下面提示词模板里面的材质、色彩以及是否是 3D 渲染还有图像比例，只要稍微一做改动，整个画面变化就会非常大。<br />
<br />
整个图像的底子就是第一部分的“抽象光效背景照片”，后面的内容可以理解为在上面加材质，比如你选择玻璃，它就会生成比较坚硬笔直的线条，如果选择流动的树脂，生成的内容就会是流体，选择丝绸或者其他布料，生成的内容又会比较轻盈。<br />
<br />
在搭配上主次四种颜色，还有是否为渲染，图像的变化也会比较明显，即使所有提示词都一致比例不同画面也会很不相同。抽卡模版，哈哈。<br />
<br />
提示词模板：<br />
abstract vector abstract light effect background stock photo | clip art, [材质], [主要色彩], in the style of atelier olschinsky, lo-fi aesthetics, [3D rendering], [次要色彩], 35mm film, ian davenport, cross-processed film [比例]<br />
<br />
提示词示例：<br />
abstract vector abstract light effect background stock photo | clip art, glass, colorful rainbow, in the style of atelier olschinsky, lo-fi aesthetics, 3D rendering, light silver and bronze,  35mm film, ian davenport, cross-processed film --ar 16:9 --v 6.0 --style raw<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23midjourneyV6">#midjourneyV6</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MxbFFxVWE0QUF3bFprLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MxbFFySmFBQUFPWWNvLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MxbFF0Y2JNQUE1Zk5ZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MxbFF3bGEwQUFGeDBoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>