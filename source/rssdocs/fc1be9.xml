<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753111393505800376#m</id>
            <title>卧槽ARC整了个不得了的功能出来</title>
            <link>https://nitter.cz/op7418/status/1753111393505800376#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753111393505800376#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 17:41:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽ARC整了个不得了的功能出来</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753104697026904445#m</id>
            <title>在一些下沉平台，品味确实是流量的敌人</title>
            <link>https://nitter.cz/op7418/status/1753104697026904445#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753104697026904445#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 17:15:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在一些下沉平台，品味确实是流量的敌人</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752957869727911995#m</id>
            <title>RT by @op7418: a16z的一篇文章，详细盘点了现在人工智能视频生成领域的现状，看完就可以对这个领域有个大概的了解，感兴趣可以看看。

他们列出了 2023 视频生成产品的时间表以及对应产品的详细信息。同时对视频生成目前需要解决的问题以及视频生成领域的 ChatGPT 时刻到来需要具备的条件进行了探讨。

下面是主要内容，也可以去链接看全文翻译：

AI 视频领域目前需要解决的核心问题？

控制：你能否控制场景中的事件以及“摄像机”的运动？对于后者，一些产品增加了可以让你进行缩放或平移摄像机，甚至添加特效的功能。至于前者 — 即动作是否如所描述的那样 — 这个问题更加棘手。这是一个关于基础模型质量的问题（模型是否能理解并执行你的提示）。

时间连贯性：如何确保在视频的不同帧之间，角色、物体和背景的一致性，防止它们在画面中突变或扭曲？这是目前所有公开的模型普遍面临的问题。

视频长度：如何制作时长超过几秒的视频片段？这个问题与时间连贯性密切相关。因为保持视频在几秒钟后仍具有一致性存在难度，许多公司限制了用户能生成的视频长度。

AI 视频领域的 ChatGPT 时刻何时到来，需要回答的几个问题？

当前的扩散架构是否适合视频制作？
目前的视频模型是基于扩散技术的：它们主要通过生成连续的帧并尝试创建时间上连贯的动画（采用多种策略实现）。这些模型没有对三维空间及物体间互动的内在理解，这就是扭曲或变形的原因。

高质量的训练数据将从何而来？
训练视频模型比训练其他内容模态更加困难，主要原因是缺乏足够的高质量、有标签的训练数据。

这些用例将如何在不同平台或模型间区分开来？
我们在几乎所有内容模态中观察到的现象是，没有一个模型能在所有用例中独占鳌头。

谁将主导视频制作的工作流程？
在目前的情况下，除了视频本身的生成，制作一段优质的视频或电影通常还需要进行编辑。我们预计视频生成平台将开始引入视频编辑需要的附加功能。

翻译及原文链接：https://quail.ink/op7418/p/why-2023-ai-video-breakthrough-2024-outlook</title>
            <link>https://nitter.cz/op7418/status/1752957869727911995#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752957869727911995#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 07:31:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>a16z的一篇文章，详细盘点了现在人工智能视频生成领域的现状，看完就可以对这个领域有个大概的了解，感兴趣可以看看。<br />
<br />
他们列出了 2023 视频生成产品的时间表以及对应产品的详细信息。同时对视频生成目前需要解决的问题以及视频生成领域的 ChatGPT 时刻到来需要具备的条件进行了探讨。<br />
<br />
下面是主要内容，也可以去链接看全文翻译：<br />
<br />
AI 视频领域目前需要解决的核心问题？<br />
<br />
控制：你能否控制场景中的事件以及“摄像机”的运动？对于后者，一些产品增加了可以让你进行缩放或平移摄像机，甚至添加特效的功能。至于前者 — 即动作是否如所描述的那样 — 这个问题更加棘手。这是一个关于基础模型质量的问题（模型是否能理解并执行你的提示）。<br />
<br />
时间连贯性：如何确保在视频的不同帧之间，角色、物体和背景的一致性，防止它们在画面中突变或扭曲？这是目前所有公开的模型普遍面临的问题。<br />
<br />
视频长度：如何制作时长超过几秒的视频片段？这个问题与时间连贯性密切相关。因为保持视频在几秒钟后仍具有一致性存在难度，许多公司限制了用户能生成的视频长度。<br />
<br />
AI 视频领域的 ChatGPT 时刻何时到来，需要回答的几个问题？<br />
<br />
当前的扩散架构是否适合视频制作？<br />
目前的视频模型是基于扩散技术的：它们主要通过生成连续的帧并尝试创建时间上连贯的动画（采用多种策略实现）。这些模型没有对三维空间及物体间互动的内在理解，这就是扭曲或变形的原因。<br />
<br />
高质量的训练数据将从何而来？<br />
训练视频模型比训练其他内容模态更加困难，主要原因是缺乏足够的高质量、有标签的训练数据。<br />
<br />
这些用例将如何在不同平台或模型间区分开来？<br />
我们在几乎所有内容模态中观察到的现象是，没有一个模型能在所有用例中独占鳌头。<br />
<br />
谁将主导视频制作的工作流程？<br />
在目前的情况下，除了视频本身的生成，制作一段优质的视频或电影通常还需要进行编辑。我们预计视频生成平台将开始引入视频编辑需要的附加功能。<br />
<br />
翻译及原文链接：<a href="https://quail.ink/op7418/p/why-2023-ai-video-breakthrough-2024-outlook">quail.ink/op7418/p/why-2023-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQQjEwaGFjQUF2OEpMLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQQjNzZGJjQUFCaC14LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753102879098122442#m</id>
            <title>R to @op7418: 补一张4K的图，高清的更帅了</title>
            <link>https://nitter.cz/op7418/status/1753102879098122442#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753102879098122442#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 17:07:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补一张4K的图，高清的更帅了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZSRnRGbGFvQUVPV3ZRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753101367043072399#m</id>
            <title>Midjourney整了个好玩的出来，这龙也太潮了，哈哈哈哈</title>
            <link>https://nitter.cz/op7418/status/1753101367043072399#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753101367043072399#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 17:01:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney整了个好玩的出来，这龙也太潮了，哈哈哈哈</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZSRVNTa2JVQUFUYThELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753097719294611824#m</id>
            <title>Anthropic这么长时间就只给Claude更新了一个深色模型，太离谱了这个产能。</title>
            <link>https://nitter.cz/op7418/status/1753097719294611824#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753097719294611824#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 16:47:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic这么长时间就只给Claude更新了一个深色模型，太离谱了这个产能。</p>
<p><a href="https://nitter.cz/AnthropicAI/status/1753093341322080314#m">nitter.cz/AnthropicAI/status/1753093341322080314#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753097193119166843#m</id>
            <title>Allen 人工智能研究所推出了完全开源的LLM OLMo，提供了模型的数据、训练代码、模型以及评估代码。

首次发布的内容包括四个参数规模达到 70 亿的语言模型，这些模型具有不同的架构、优化器和训练硬件，另外还有一个参数规模为 10 亿的模型。所有这些模型都在至少 2 万亿个词元（token）上接受了训练。

每个模型都包含完整的训练数据、模型权重、训练和推理代码、训练日志和性能指标。在多种任务中，OLMo 7B 模型显示出了强大的性能，可以与 Llama 2 等模型相媲美。

公告链接：https://blog.allenai.org/olmo-open-language-model-87ccfc95f580</title>
            <link>https://nitter.cz/op7418/status/1753097193119166843#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753097193119166843#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 16:45:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Allen 人工智能研究所推出了完全开源的LLM OLMo，提供了模型的数据、训练代码、模型以及评估代码。<br />
<br />
首次发布的内容包括四个参数规模达到 70 亿的语言模型，这些模型具有不同的架构、优化器和训练硬件，另外还有一个参数规模为 10 亿的模型。所有这些模型都在至少 2 万亿个词元（token）上接受了训练。<br />
<br />
每个模型都包含完整的训练数据、模型权重、训练和推理代码、训练日志和性能指标。在多种任务中，OLMo 7B 模型显示出了强大的性能，可以与 Llama 2 等模型相媲美。<br />
<br />
公告链接：<a href="https://blog.allenai.org/olmo-open-language-model-87ccfc95f580">blog.allenai.org/olmo-open-l…</a></p>
<p><a href="https://nitter.cz/allen_ai/status/1753072249459081599#m">nitter.cz/allen_ai/status/1753072249459081599#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753095061519348000#m</id>
            <title>Bard又获得了更新，现在可以生成图片了。
图像生成能力由DeepMind的Imagen 2图像生成模型提供支持。我试了一下我自己的账号还不行。

同时Bard已扩展到 230 多个国家/地区的 40 多种语言。Bard现在已经完全由Gemini Pro模型来提供支持。

Bard生成的图像都将由 SynthID 进行标记，SynthID 是 Google DeepMind 开发的一种工具，可将数字水印直接添加到我们生成的图像的像素中。 
SynthID 水印人眼无法察觉，但可检测以进行识别。

Imagen 2介绍：https://blog.google/technology/ai/google-imagen-2/</title>
            <link>https://nitter.cz/op7418/status/1753095061519348000#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753095061519348000#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 16:36:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Bard又获得了更新，现在可以生成图片了。<br />
图像生成能力由DeepMind的Imagen 2图像生成模型提供支持。我试了一下我自己的账号还不行。<br />
<br />
同时Bard已扩展到 230 多个国家/地区的 40 多种语言。Bard现在已经完全由Gemini Pro模型来提供支持。<br />
<br />
Bard生成的图像都将由 SynthID 进行标记，SynthID 是 Google DeepMind 开发的一种工具，可将数字水印直接添加到我们生成的图像的像素中。 <br />
SynthID 水印人眼无法察觉，但可检测以进行识别。<br />
<br />
Imagen 2介绍：<a href="https://blog.google/technology/ai/google-imagen-2/">blog.google/technology/ai/go…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZROVNoZGFjQUEtcU5ELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752907695739510924#m</id>
            <title>RT by @op7418: 昨晚 Midjourney Office Time 的一些功能预告：

V6 的 Beta 版本模型将会在本周末推出，新的Describe 正在测试，可以帮助用户更好的书写提示词。

Alpha版本的网站将会开放给生成了 1000 张图片的用户。

Niji6 下周会更新局部修补和放大功能。

目前的重点是角色一致性功能。</title>
            <link>https://nitter.cz/op7418/status/1752907695739510924#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752907695739510924#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 04:12:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚 Midjourney Office Time 的一些功能预告：<br />
<br />
V6 的 Beta 版本模型将会在本周末推出，新的Describe 正在测试，可以帮助用户更好的书写提示词。<br />
<br />
Alpha版本的网站将会开放给生成了 1000 张图片的用户。<br />
<br />
Niji6 下周会更新局部修补和放大功能。<br />
<br />
目前的重点是角色一致性功能。</p>
<p><a href="https://nitter.cz/aliejulesai/status/1752804253083607518#m">nitter.cz/aliejulesai/status/1752804253083607518#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752888375995220142#m</id>
            <title>RT by @op7418: 终于大概摸清楚一些 Niji V6的提示词逻辑了，这次的 Niji V6 是真离谱，V5 的提示词几乎完全不可用。</title>
            <link>https://nitter.cz/op7418/status/1752888375995220142#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752888375995220142#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 02:55:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于大概摸清楚一些 Niji V6的提示词逻辑了，这次的 Niji V6 是真离谱，V5 的提示词几乎完全不可用。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZPQ2Z3TmFZQUFCQzNFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZPQ2Z3SWFNQUFaVDh3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752872343939187011#m</id>
            <title>RT by @op7418: Midjourney这下真出类似Lora的风格调整器了，先放出的风格一致性工具，后面还会有角色一致性工具。

可以从多张图片学习对应的风格然后结合提示词生成图片,下面4张为输入图片。

下面是使用方法：

“风格参照”。这项功能与图像提示类似，你可以提供一个或多个图像的链接，用以描述你想要的统一风格。

风格参照的使用方法在你的提示语后输入 --sref，然后加上一个或多个图像链接，例如 --sref urlA urlB urlC。 

图像模型会将这些图像链接作为“风格参照”，尝试创作出与其美学风格相匹配的作品。

 可以这样设置不同风格的权重 --sref urlA::2 urlB::3 urlC::5。 

通过 --sw 100 来调整整体风格化的强度（默认值是 100，0 表示关闭，最大值是 1000）。 

常规图像提示应放在 --sref 前面，例如 /imagine cat ninja ImagePrompt1 ImagePrompt2 --sref stylePrompt1 stylePrompt2。 该功能支持 V6 和 Niji V6 版本（不支持 V5 等旧版本）。

请注意：

我们可能会在未来几周对这一功能进行更新（可能会有所改变，因此在测试阶段请谨慎使用）。 

如果你的提示倾向于真实感风格，但你希望加入与之冲突的风格，比如插画风，可能还需要在提示语中额外说明。

风格参照不会直接影响图像提示的效果，只会作用于包含至少一个文本提示的作业。 我们计划在未来增加一个“一致性角色”特性，其工作原理类似，将使用 --cref 参数。</title>
            <link>https://nitter.cz/op7418/status/1752872343939187011#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752872343939187011#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 01:51:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney这下真出类似Lora的风格调整器了，先放出的风格一致性工具，后面还会有角色一致性工具。<br />
<br />
可以从多张图片学习对应的风格然后结合提示词生成图片,下面4张为输入图片。<br />
<br />
下面是使用方法：<br />
<br />
“风格参照”。这项功能与图像提示类似，你可以提供一个或多个图像的链接，用以描述你想要的统一风格。<br />
<br />
风格参照的使用方法在你的提示语后输入 --sref，然后加上一个或多个图像链接，例如 --sref urlA urlB urlC。 <br />
<br />
图像模型会将这些图像链接作为“风格参照”，尝试创作出与其美学风格相匹配的作品。<br />
<br />
 可以这样设置不同风格的权重 --sref urlA::2 urlB::3 urlC::5。 <br />
<br />
通过 --sw 100 来调整整体风格化的强度（默认值是 100，0 表示关闭，最大值是 1000）。 <br />
<br />
常规图像提示应放在 --sref 前面，例如 /imagine cat ninja ImagePrompt1 ImagePrompt2 --sref stylePrompt1 stylePrompt2。 该功能支持 V6 和 Niji V6 版本（不支持 V5 等旧版本）。<br />
<br />
请注意：<br />
<br />
我们可能会在未来几周对这一功能进行更新（可能会有所改变，因此在测试阶段请谨慎使用）。 <br />
<br />
如果你的提示倾向于真实感风格，但你希望加入与之冲突的风格，比如插画风，可能还需要在提示语中额外说明。<br />
<br />
风格参照不会直接影响图像提示的效果，只会作用于包含至少一个文本提示的作业。 我们计划在未来增加一个“一致性角色”特性，其工作原理类似，将使用 --cref 参数。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZOei1NcGJNQUFxQ1MwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752903151651614899#m</id>
            <title>RT by @op7418: 谷歌昨晚发布了类似 LCM 的图片快速生成模型，主要为了在移动设备上本地运行。
只需要 0.5 秒就可以生成一张512*512 的图片，模型大小仅为 520M 。

详细介绍：

一个专为移动设备上快速的文本转图像生成而设计的模型。这个模型特别强调了高效的设备端操作，能在不到一秒内将文本提示转换成高质量的图像。

模型的架构——包括文本编码器、扩散 UNet 和图像解码器——都经过了为移动端使用而特别优化，参数量相对较小，仅有520M。利用 DiffusionGAN 技术实现的一步采样进一步提升了模型的效率。测试结果显示，

MobileDiffusion 能在半秒内生成一张512x512分辨率的高质量图像，非常适合移动设备上的各种应用场景。

“MobileDiffusion”模型采用了一种新颖的方法，能在移动设备上迅速实现文本到图像的转换，核心是一个专为设备内运行设计的高效潜在扩散模型。

它通过优化模型架构和应用 DiffusionGAN 进行一步采样，解决了传统文本到图像扩散模型中的高计算成本和大型模型尺寸问题。

这个模型的性能和效率使其成为移动设备上实现文本到图像转换的一个有潜力的选择，可能在多个领域有广泛应用。

原文链接：https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</title>
            <link>https://nitter.cz/op7418/status/1752903151651614899#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752903151651614899#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 03:54:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌昨晚发布了类似 LCM 的图片快速生成模型，主要为了在移动设备上本地运行。<br />
只需要 0.5 秒就可以生成一张512*512 的图片，模型大小仅为 520M 。<br />
<br />
详细介绍：<br />
<br />
一个专为移动设备上快速的文本转图像生成而设计的模型。这个模型特别强调了高效的设备端操作，能在不到一秒内将文本提示转换成高质量的图像。<br />
<br />
模型的架构——包括文本编码器、扩散 UNet 和图像解码器——都经过了为移动端使用而特别优化，参数量相对较小，仅有520M。利用 DiffusionGAN 技术实现的一步采样进一步提升了模型的效率。测试结果显示，<br />
<br />
MobileDiffusion 能在半秒内生成一张512x512分辨率的高质量图像，非常适合移动设备上的各种应用场景。<br />
<br />
“MobileDiffusion”模型采用了一种新颖的方法，能在移动设备上迅速实现文本到图像的转换，核心是一个专为设备内运行设计的高效潜在扩散模型。<br />
<br />
它通过优化模型架构和应用 DiffusionGAN 进行一步采样，解决了传统文本到图像扩散模型中的高计算成本和大型模型尺寸问题。<br />
<br />
这个模型的性能和效率使其成为移动设备上实现文本到图像转换的一个有潜力的选择，可能在多个领域有广泛应用。<br />
<br />
原文链接：<a href="https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html">blog.research.google/2024/01…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTI5MDMxMDMwNTIyMDE5ODQvcHUvaW1nL3NjWW11ekpNOUFJc1BxMGwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753047609961590855#m</id>
            <title>牛批 加的特效很出彩</title>
            <link>https://nitter.cz/op7418/status/1753047609961590855#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753047609961590855#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 13:28:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>牛批 加的特效很出彩</p>
<p><a href="https://nitter.cz/lepadphone/status/1753038933922132369#m">nitter.cz/lepadphone/status/1753038933922132369#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753035024574923080#m</id>
            <title>SVD 最擅长的项目，致郁系海洋。#AIvideo</title>
            <link>https://nitter.cz/op7418/status/1753035024574923080#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753035024574923080#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 12:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SVD 最擅长的项目，致郁系海洋。<a href="https://nitter.cz/search?q=%23AIvideo">#AIvideo</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMwMzQ3Njk2MTU3MTYzNTIvcHUvaW1nL2pPNkdOSVlRYzlrY3hDT3QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753009048755273890#m</id>
            <title>R to @op7418: 素材图在这里：</title>
            <link>https://nitter.cz/op7418/status/1753009048755273890#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753009048755273890#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 10:55:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>素材图在这里：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQd2JveGJrQUFtd2F6LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQd2JvemJBQUFFOWlXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQd2JvdmE0QUE0UDlPLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQd2JveGFzQUFMaGR3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753008944451145930#m</id>
            <title>Midjourney 今天发布了风格参照功能，还是挺复杂的很多朋友估计看了早上的也不知道怎么用。

录了一个简单的视频教大家用一下，提示词和素材在下面的推里面。</title>
            <link>https://nitter.cz/op7418/status/1753008944451145930#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753008944451145930#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 10:54:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney 今天发布了风格参照功能，还是挺复杂的很多朋友估计看了早上的也不知道怎么用。<br />
<br />
录了一个简单的视频教大家用一下，提示词和素材在下面的推里面。</p>
<p><a href="https://nitter.cz/op7418/status/1753005357478396206#m">nitter.cz/op7418/status/1753005357478396206#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMwMDgzNDgxNDI5NDQyNTYvcHUvaW1nL3lLR2k4THB0ZUVGY1FtNDkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1753005357478396206#m</id>
            <title>🧪今天 Midjoureny 发的风格参照功能在一些比较难描述的风格上效果真的好。就是做半截感觉又在给 MJ 付费打工了。

比如下面这非常柔和的颜色混合渐变，还带一些噪点，我之前怎么都还原不出来，用了风格参照非常简单就还原了。

还有就是我们上传的风格参照图片和搭配的提示词都是非常好的人工挑选的训练素材，估计会直接进到训练流程里，又花钱打工了家人们。

提示词：the lightest wave of color over the blue background of a blurred figure, in the style of jo ann callis, light black and violet, asymmetrical forms, ultrafine detail, mike winkelmann --ar 9:16
#晚安提示词 #midjourny #catjoureny</title>
            <link>https://nitter.cz/op7418/status/1753005357478396206#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1753005357478396206#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 10:40:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪今天 Midjoureny 发的风格参照功能在一些比较难描述的风格上效果真的好。就是做半截感觉又在给 MJ 付费打工了。<br />
<br />
比如下面这非常柔和的颜色混合渐变，还带一些噪点，我之前怎么都还原不出来，用了风格参照非常简单就还原了。<br />
<br />
还有就是我们上传的风格参照图片和搭配的提示词都是非常好的人工挑选的训练素材，估计会直接进到训练流程里，又花钱打工了家人们。<br />
<br />
提示词：the lightest wave of color over the blue background of a blurred figure, in the style of jo ann callis, light black and violet, asymmetrical forms, ultrafine detail, mike winkelmann --ar 9:16<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23midjourny">#midjourny</a> <a href="https://nitter.cz/search?q=%23catjoureny">#catjoureny</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQczlpemFZQUFROVF5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752977747667046764#m</id>
            <title>一个帮助你更好的使用 ComfyUI 作为后端服务的项目。可以自动下载所有必要的节点和模型并执行提供的工作流程。具体功能有：

◆自动安装缺失的节点；
◆自动下载工作流程模型。 （检查数据文件夹以了解支持的型号）；
◆执行工作流而不启动 UI 服务器；
◆如果工作流程中未找到类似模型，则建议类似模型；
◆可以提供自定义节点和模型的链接以进行安装/设置；

项目地址：https://github.com/piyushK52/comfy-runner</title>
            <link>https://nitter.cz/op7418/status/1752977747667046764#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752977747667046764#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 08:50:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个帮助你更好的使用 ComfyUI 作为后端服务的项目。可以自动下载所有必要的节点和模型并执行提供的工作流程。具体功能有：<br />
<br />
◆自动安装缺失的节点；<br />
◆自动下载工作流程模型。 （检查数据文件夹以了解支持的型号）；<br />
◆执行工作流而不启动 UI 服务器；<br />
◆如果工作流程中未找到类似模型，则建议类似模型；<br />
◆可以提供自定义节点和模型的链接以进行安装/设置；<br />
<br />
项目地址：<a href="https://github.com/piyushK52/comfy-runner">github.com/piyushK52/comfy-r…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTI5NzY5MjE1NzM1ODg5OTIvcHUvaW1nL0trZ0N6TXcxUUs3LW1xdnguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1752958636404683061#m</id>
            <title>R to @op7418: 原文链接在这里：
https://a16z.com/why-2023-was-ai-videos-breakout-year-and-what-to-expect-in-2024/</title>
            <link>https://nitter.cz/op7418/status/1752958636404683061#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1752958636404683061#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 07:34:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原文链接在这里：<br />
<a href="https://a16z.com/why-2023-was-ai-videos-breakout-year-and-what-to-expect-in-2024/">a16z.com/why-2023-was-ai-vid…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MjczOTE3ODE4ODgzMjc3My9SM2VmYlpCaj9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>