<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740686124736262583#m</id>
            <title>一个开源的多模态 LLM Unified-IO 2。
比较离谱的是它可以实现语音理解和动作理解还有图像标记这种任务，还可以理解空间关系。真正的 All in one 。
甚至还可以驱动机器人做对应的操作。

项目简介：
Unified-IO 2，这是第一个能够理解和生成图像、文本、音频和动作的自回归多模态模型。
为了统一不同的模态，我们将输入和输出（图像、文本、音频、动作、框等）进行分词，并将它们置于一个共享的语义空间中，然后使用单个编码器-解码器变换器模型进行处理。由于使用多样的模态进行训练非常困难，我们提出了各种架构改进来稳定模型。
我们从头开始在来自不同来源的大型多模态预训练语料库上训练我们的模型，并采用多模态混合去噪目标。为了学习一系列广泛的技能，比如遵循多模态指令，我们构建并微调了一个包含120个现有数据集的集合，并进行了提示和增强。
通过一个统一的模型，Unified-IO 2在GRIT基准测试中达到了最先进的性能，并在30多个基准测试中取得了强大的结果，包括图像生成和理解、文本理解、视频和音频理解以及机器人操作。我们将所有的模型都发布给研究界。

项目地址：https://unified-io-2.allenai.org/</title>
            <link>https://nitter.cz/op7418/status/1740686124736262583#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740686124736262583#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 10:48:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个开源的多模态 LLM Unified-IO 2。<br />
比较离谱的是它可以实现语音理解和动作理解还有图像标记这种任务，还可以理解空间关系。真正的 All in one 。<br />
甚至还可以驱动机器人做对应的操作。<br />
<br />
项目简介：<br />
Unified-IO 2，这是第一个能够理解和生成图像、文本、音频和动作的自回归多模态模型。<br />
为了统一不同的模态，我们将输入和输出（图像、文本、音频、动作、框等）进行分词，并将它们置于一个共享的语义空间中，然后使用单个编码器-解码器变换器模型进行处理。由于使用多样的模态进行训练非常困难，我们提出了各种架构改进来稳定模型。<br />
我们从头开始在来自不同来源的大型多模态预训练语料库上训练我们的模型，并采用多模态混合去噪目标。为了学习一系列广泛的技能，比如遵循多模态指令，我们构建并微调了一个包含120个现有数据集的集合，并进行了提示和增强。<br />
通过一个统一的模型，Unified-IO 2在GRIT基准测试中达到了最先进的性能，并在30多个基准测试中取得了强大的结果，包括图像生成和理解、文本理解、视频和音频理解以及机器人操作。我们将所有的模型都发布给研究界。<br />
<br />
项目地址：<a href="https://unified-io-2.allenai.org/">unified-io-2.allenai.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nnb1ladWFvQUFFNjZnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nnb2hRWWFBQUVZczl1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nnb3B0RGJ3QUEzWXVFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nnb3g2SWFFQUFDLThILmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740681583496245656#m</id>
            <title>小红书新发布的一个编码器SSR-Encoder，可以实现从一张图片种提取不同的主题（人物、物体等）特征生成图像。与 SD 现有的能力都能结合，Animatediff 也可以。

这个厉害。类似于加强版的 IPadapter？而且说了会开源，这几个以图像内容消费为主的国内公司都在图像生成模型上发力。

项目简介：
最近在主题驱动的图像生成领域的进展已经实现了零样本生成，然而精确选择和关注关键主题表示仍然是一个挑战。
为了解决这个问题，我们引入了SSR-Encoder，这是一种新架构，旨在从单个或多个参考图像中选择性地捕获任何主题。它响应各种查询模式，包括文本和掩模，而无需在测试时进行微调。
SSR-Encoder结合了一个Token-to-Patch Aligner，用于将查询输入与图像块对齐，以及一个细节保留的主题编码器，用于提取和保留主题的精细特征，从而生成主题嵌入。
这些嵌入与原始文本嵌入结合使用，条件化生成过程。SSR-Encoder以其模型泛化性和效率为特点，适应于一系列自定义模型和控制模块。通过Embedding Consistency Regularization Loss进行增强，以改进训练。
我们广泛的实验表明其在多样化和高质量的图像生成中的有效性，表明其广泛的适用性。

项目地址：https://ssr-encoder.github.io/</title>
            <link>https://nitter.cz/op7418/status/1740681583496245656#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740681583496245656#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 10:30:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小红书新发布的一个编码器SSR-Encoder，可以实现从一张图片种提取不同的主题（人物、物体等）特征生成图像。与 SD 现有的能力都能结合，Animatediff 也可以。<br />
<br />
这个厉害。类似于加强版的 IPadapter？而且说了会开源，这几个以图像内容消费为主的国内公司都在图像生成模型上发力。<br />
<br />
项目简介：<br />
最近在主题驱动的图像生成领域的进展已经实现了零样本生成，然而精确选择和关注关键主题表示仍然是一个挑战。<br />
为了解决这个问题，我们引入了SSR-Encoder，这是一种新架构，旨在从单个或多个参考图像中选择性地捕获任何主题。它响应各种查询模式，包括文本和掩模，而无需在测试时进行微调。<br />
SSR-Encoder结合了一个Token-to-Patch Aligner，用于将查询输入与图像块对齐，以及一个细节保留的主题编码器，用于提取和保留主题的精细特征，从而生成主题嵌入。<br />
这些嵌入与原始文本嵌入结合使用，条件化生成过程。SSR-Encoder以其模型泛化性和效率为特点，适应于一系列自定义模型和控制模块。通过Embedding Consistency Regularization Loss进行增强，以改进训练。<br />
我们广泛的实验表明其在多样化和高质量的图像生成中的有效性，表明其广泛的适用性。<br />
<br />
项目地址：<a href="https://ssr-encoder.github.io/">ssr-encoder.github.io/</a></p>
<p><a href="https://nitter.cz/_akhaliq/status/1740616525478781168#m">nitter.cz/_akhaliq/status/1740616525478781168#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nna1ItbWFVQUFITlNiLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nna1dmUGJ3QUFzdHVYLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nna2t1OWJFQUFfTUJ1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Nna3BmS2JzQUVxd3hCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740677173856625055#m</id>
            <title>发现了一个可以在手机上跑的多模态模型UForm，有点意思。

支持图像识别，图像问答和多模态聊天，参数只有 1.5B 非常袖珍。下面是模型的主要特点：

吞吐量：由于尺寸小，推理速度比竞争对手快2-4倍。
微型嵌入：256维向量的搜索速度比类似CLIP模型的向量快2-3倍。
量化感知：将嵌入从 f32 降级到 i8 ，而不会损失太多召回率。
多语种：在平衡的数据集上训练，召回率在20多种语言中表现出色。
硬件友好：无论是苹果的CoreML还是ONNX，我们都可以满足需求。

项目地址：https://github.com/unum-cloud/uform</title>
            <link>https://nitter.cz/op7418/status/1740677173856625055#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740677173856625055#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 10:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发现了一个可以在手机上跑的多模态模型UForm，有点意思。<br />
<br />
支持图像识别，图像问答和多模态聊天，参数只有 1.5B 非常袖珍。下面是模型的主要特点：<br />
<br />
吞吐量：由于尺寸小，推理速度比竞争对手快2-4倍。<br />
微型嵌入：256维向量的搜索速度比类似CLIP模型的向量快2-3倍。<br />
量化感知：将嵌入从 f32 降级到 i8 ，而不会损失太多召回率。<br />
多语种：在平衡的数据集上训练，召回率在20多种语言中表现出色。<br />
硬件友好：无论是苹果的CoreML还是ONNX，我们都可以满足需求。<br />
<br />
项目地址：<a href="https://github.com/unum-cloud/uform">github.com/unum-cloud/uform</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NnYXJrc2FNQUFHNW10LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740676192003604750#m</id>
            <title>快手也发布了一个视频生成模型 I2V-Adapter。
主要用于从图片获取信息直接生成视频，这个项目可以与已有的 SD 生态比如ContorlNet等结合。又有可以玩的了

完整简介：
提出了一种新的解决方案——I2V-Adapter，旨在解决这些限制。
保留了T2I模型的结构完整性和它们内在的运动模块。
I2V-Adapter通过并行处理带有噪声的视频帧和输入图像，使用了一个轻量级的适配器模块。这个模块就像一座桥梁，高效地将输入与模型的自注意力机制连接起来，从而在不需要对T2I模型进行结构性改动的情况下保持空间细节。
此外，I2V-Adapter所需的参数量只是传统模型的一小部分，并且确保了与现有的社区驱动的T2I模型和控制工具的兼容性。
我们的实验结果表明，I2V-Adapter能够生成高质量的视频输出。

论文网页版本地址：https://browse.arxiv.org/html/2312.16693v1</title>
            <link>https://nitter.cz/op7418/status/1740676192003604750#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740676192003604750#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 10:08:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>快手也发布了一个视频生成模型 I2V-Adapter。<br />
主要用于从图片获取信息直接生成视频，这个项目可以与已有的 SD 生态比如ContorlNet等结合。又有可以玩的了<br />
<br />
完整简介：<br />
提出了一种新的解决方案——I2V-Adapter，旨在解决这些限制。<br />
保留了T2I模型的结构完整性和它们内在的运动模块。<br />
I2V-Adapter通过并行处理带有噪声的视频帧和输入图像，使用了一个轻量级的适配器模块。这个模块就像一座桥梁，高效地将输入与模型的自注意力机制连接起来，从而在不需要对T2I模型进行结构性改动的情况下保持空间细节。<br />
此外，I2V-Adapter所需的参数量只是传统模型的一小部分，并且确保了与现有的社区驱动的T2I模型和控制工具的兼容性。<br />
我们的实验结果表明，I2V-Adapter能够生成高质量的视频输出。<br />
<br />
论文网页版本地址：<a href="https://browse.arxiv.org/html/2312.16693v1">browse.arxiv.org/html/2312.1…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NnZVZhRmFJQUF2UDdRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740667560809611747#m</id>
            <title>帮转一下，真的很好，看的我都想试试了。
主要英语实在拉跨，哈哈。</title>
            <link>https://nitter.cz/op7418/status/1740667560809611747#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740667560809611747#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 09:34:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>帮转一下，真的很好，看的我都想试试了。<br />
主要英语实在拉跨，哈哈。</p>
<p><a href="https://nitter.cz/Yuugumo_ichi/status/1740641259436569079#m">nitter.cz/Yuugumo_ichi/status/1740641259436569079#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740666920653963359#m</id>
            <title>这个项目优化了NeRF大场景渲染的速度和资源消耗。

通过将整个场景划分为可管理的块来表示整个场景，每个块都有自己的细节级别，实现了使用 RTX 3060 GPU 在 1080P 分辨率下 32FPS的渲染速度。</title>
            <link>https://nitter.cz/op7418/status/1740666920653963359#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740666920653963359#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 09:31:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个项目优化了NeRF大场景渲染的速度和资源消耗。<br />
<br />
通过将整个场景划分为可管理的块来表示整个场景，每个块都有自己的细节级别，实现了使用 RTX 3060 GPU 在 1080P 分辨率下 32FPS的渲染速度。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1740589735024975967#m">nitter.cz/_akhaliq/status/1740589735024975967#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740651265309467069#m</id>
            <title>🧪 #晚安提示词 前几天早上发了一张图，很多朋友都很喜欢，其实 V5 的时候就发过类似的，但是 V5 的提示词在 V6 已经很不稳定了。所以又重新优化了提示词。

这套词会生成出带特定颜色的流体效果，同时会有金色或者银色光斑，非常吸睛，很适合用在封面或者无意义的内容配图上面。

将提示词对应位置的颜色替换为自己需要的就行，不建议超过三种，颜色选择尽量以相邻色和对比色为主，这样可以保证美观度，比如红色和蓝色，蓝色和橙色等。

提示词：
blue and orange abstract with lights flowing across it., in the style of vray tracing, glittery and shiny, sinuous lines, light cyan and gold, rendered in unreal engine, long exposure, selective focus --ar 3:2 --v 6.0 --style raw</title>
            <link>https://nitter.cz/op7418/status/1740651265309467069#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740651265309467069#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 08:29:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪 <a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> 前几天早上发了一张图，很多朋友都很喜欢，其实 V5 的时候就发过类似的，但是 V5 的提示词在 V6 已经很不稳定了。所以又重新优化了提示词。<br />
<br />
这套词会生成出带特定颜色的流体效果，同时会有金色或者银色光斑，非常吸睛，很适合用在封面或者无意义的内容配图上面。<br />
<br />
将提示词对应位置的颜色替换为自己需要的就行，不建议超过三种，颜色选择尽量以相邻色和对比色为主，这样可以保证美观度，比如红色和蓝色，蓝色和橙色等。<br />
<br />
提示词：<br />
blue and orange abstract with lights flowing across it., in the style of vray tracing, glittery and shiny, sinuous lines, light cyan and gold, rendered in unreal engine, long exposure, selective focus --ar 3:2 --v 6.0 --style raw</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NnRExaYmJBQUF5ZDUwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740581492932354411#m</id>
            <title>LLM 相关的基础文章很多人整理，AI 图像生成的比较少，刚好发现了一个，我又自己筛选了一下，一共 15 篇内容，都在这里了，如果想深入研究 SD，建议看看。

主要包括三个部分：图片生成简介、图像生成基础模型的精细调整和构建 AI 生成服务，。

都放在下面的这个个链接里，打开一个就都有了。元旦我也会找时间把这些内容翻译一下，然后发出来，可以收藏这条回来看。等不及可以直接看，下面是具体的分类和文章目录：

图像生成技术简介：

◆ 人工智能和艺术：机器学习如何改变创造性工作（报告）
◆ 平面设计师如何创建他们自己的AI工具（博客文章）
◆ AI图像生成器的工作原理以及扩散是什么（视频）
◆ 什么是Diffusion模型？（视频）
◆ Diffusion模型的工作原理（1小时课程）
◆ 初学者指南：Stable diffusion（指南）

基础模型的精细调整：

◆ SD1.5通用目的模型的全面精细调整指南（博客文章）
◆ SD模型的基准测试与混合（博客文章）
◆ 解耦文本编码器和UNET学习率（博客文章）
◆ D适应：再见学习率困扰？（博客文章）
◆ 自己精细调整通用稳定扩散模型指南（博客文章）

构建 AI 生成服务的后端：

◆ 如何构建一个生成AI服务的后端
◆ Stable diffusion提示：权威指南
◆ SD提示的精细调整：修改Stable diffusion提示的GPT 3.5（博客文章）
◆ SD提示的目录（目录）

所有内容链接：https://arc.net/folder/10431A09-4798-4002-B99A-2769BD9131FF</title>
            <link>https://nitter.cz/op7418/status/1740581492932354411#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740581492932354411#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 03:52:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LLM 相关的基础文章很多人整理，AI 图像生成的比较少，刚好发现了一个，我又自己筛选了一下，一共 15 篇内容，都在这里了，如果想深入研究 SD，建议看看。<br />
<br />
主要包括三个部分：图片生成简介、图像生成基础模型的精细调整和构建 AI 生成服务，。<br />
<br />
都放在下面的这个个链接里，打开一个就都有了。元旦我也会找时间把这些内容翻译一下，然后发出来，可以收藏这条回来看。等不及可以直接看，下面是具体的分类和文章目录：<br />
<br />
图像生成技术简介：<br />
<br />
◆ 人工智能和艺术：机器学习如何改变创造性工作（报告）<br />
◆ 平面设计师如何创建他们自己的AI工具（博客文章）<br />
◆ AI图像生成器的工作原理以及扩散是什么（视频）<br />
◆ 什么是Diffusion模型？（视频）<br />
◆ Diffusion模型的工作原理（1小时课程）<br />
◆ 初学者指南：Stable diffusion（指南）<br />
<br />
基础模型的精细调整：<br />
<br />
◆ SD1.5通用目的模型的全面精细调整指南（博客文章）<br />
◆ SD模型的基准测试与混合（博客文章）<br />
◆ 解耦文本编码器和UNET学习率（博客文章）<br />
◆ D适应：再见学习率困扰？（博客文章）<br />
◆ 自己精细调整通用稳定扩散模型指南（博客文章）<br />
<br />
构建 AI 生成服务的后端：<br />
<br />
◆ 如何构建一个生成AI服务的后端<br />
◆ Stable diffusion提示：权威指南<br />
◆ SD提示的精细调整：修改Stable diffusion提示的GPT 3.5（博客文章）<br />
◆ SD提示的目录（目录）<br />
<br />
所有内容链接：<a href="https://arc.net/folder/10431A09-4798-4002-B99A-2769BD9131FF">arc.net/folder/10431A09-4798…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NmSm5qbGE0QUFTdGxZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740204677713789401#m</id>
            <title>RT by @op7418: Midjourney 昨晚办公时间的一些消息，他们要开始训练视频模型了，V6 会有重大版本更新，感觉视频生成领域又会有一个有力竞争者啊。

详细的内容：

➜他们计划从一月开始着手训练视频模型 👀  
➜下周将迎来 v6 版本的一次重要更新。  
➜文本处理方面将有显著的进步。  
➜内容的连贯性会有所提高。  
➜对于提示的准确性也将得到改善。  
➜总的来说，各方面都会有所提升。  

➜Inpainting 功能的升级将是 v6 版本的第一个重点改进。  ➜目前 Niji v6 正在训练过程中。 
➜v6 版本的风格调整器可能会带来全新且不同的体验。  
➜角色和风格的一致性是他们正在重点研究的另一个领域。  

➜V7 版本似乎将是一次重大更新，待一切准备就绪后便会推出。</title>
            <link>https://nitter.cz/op7418/status/1740204677713789401#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740204677713789401#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 02:55:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney 昨晚办公时间的一些消息，他们要开始训练视频模型了，V6 会有重大版本更新，感觉视频生成领域又会有一个有力竞争者啊。<br />
<br />
详细的内容：<br />
<br />
➜他们计划从一月开始着手训练视频模型 👀  <br />
➜下周将迎来 v6 版本的一次重要更新。  <br />
➜文本处理方面将有显著的进步。  <br />
➜内容的连贯性会有所提高。  <br />
➜对于提示的准确性也将得到改善。  <br />
➜总的来说，各方面都会有所提升。  <br />
<br />
➜Inpainting 功能的升级将是 v6 版本的第一个重点改进。  ➜目前 Niji v6 正在训练过程中。 <br />
➜v6 版本的风格调整器可能会带来全新且不同的体验。  <br />
➜角色和风格的一致性是他们正在重点研究的另一个领域。  <br />
<br />
➜V7 版本似乎将是一次重大更新，待一切准备就绪后便会推出。</p>
<p><a href="https://nitter.cz/nickfloats/status/1740105219009130954#m">nitter.cz/nickfloats/status/1740105219009130954#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740336745554845811#m</id>
            <title>RT by @op7418: 昨天腾讯那个可以自定义视频生成中的镜头运动路径的项目MotionCtrl，camenduru也做了 colab 的脚本，运行就可以尝试了，官方 demo 没办法尝试可以利用这个。
https://colab.research.google.com/github/camenduru/MotionCtrl-colab/blob/main/MotionCtrl_colab.ipynb</title>
            <link>https://nitter.cz/op7418/status/1740336745554845811#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740336745554845811#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 11:39:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨天腾讯那个可以自定义视频生成中的镜头运动路径的项目MotionCtrl，camenduru也做了 colab 的脚本，运行就可以尝试了，官方 demo 没办法尝试可以利用这个。<br />
<a href="https://colab.research.google.com/github/camenduru/MotionCtrl-colab/blob/main/MotionCtrl_colab.ipynb">colab.research.google.com/gi…</a></p>
<p><a href="https://nitter.cz/camenduru/status/1740214606377726374#m">nitter.cz/camenduru/status/1740214606377726374#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczOTM3NDM3NTQ0NTY2Mzc0NC9qYU5uZ19Gcz9mb3JtYXQ9cG5nJm5hbWU9MjgweDI4MF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740289987860713746#m</id>
            <title>RT by @op7418: #晚安提示词 今天这几个 Midjourney V6 做的图有点帅了，非常科幻的效果

受到Tatiana Tsiguleva这几天玩光线的启发，看看能不能尝试一下用强折射效果做出科幻感和未来感。

因为这种强折射的彩色光线在我们的印象里会有类似时间旅行或者空间破碎的这种暗示。比如游戏Control和量子破碎的演出效果那样。

居然真让我搞出来了，需要注意的是，这个提示词要尽量避免对人物面部的描写，同时纵向画面的效果最好。

提示词：
a woman standing behind a rainbow light beam, in the style of textured minimalist abstractions, liquid emulsion printing, vhs, conceptual installation art, darktable processing, aquamarine and amber, close-up intensity --ar 3:4 --v 6.0</title>
            <link>https://nitter.cz/op7418/status/1740289987860713746#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740289987860713746#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 08:34:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> 今天这几个 Midjourney V6 做的图有点帅了，非常科幻的效果<br />
<br />
受到Tatiana Tsiguleva这几天玩光线的启发，看看能不能尝试一下用强折射效果做出科幻感和未来感。<br />
<br />
因为这种强折射的彩色光线在我们的印象里会有类似时间旅行或者空间破碎的这种暗示。比如游戏Control和量子破碎的演出效果那样。<br />
<br />
居然真让我搞出来了，需要注意的是，这个提示词要尽量避免对人物面部的描写，同时纵向画面的效果最好。<br />
<br />
提示词：<br />
a woman standing behind a rainbow light beam, in the style of textured minimalist abstractions, liquid emulsion printing, vhs, conceptual installation art, darktable processing, aquamarine and amber, close-up intensity --ar 3:4 --v 6.0</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiQU94YWE0QUE0QWZhLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiQU94YmJNQUE5bzVoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiQU94aWJvQUF1WVh4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiQU94YmJJQUFJa3dsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740421701928989016#m</id>
            <title>RT by @op7418: 麻了 没想到，一直懒得搜的Mac截图快捷键在一个AI博主这里看到了，终于不用天天去找图标了。

- Command+Option+Shift+4 选择屏幕的一小部分并将其复制到剪贴板中作为图像。
- Command+Shift+4 做同样的事情，但将其保存为png文件放在桌面上。</title>
            <link>https://nitter.cz/op7418/status/1740421701928989016#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740421701928989016#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 17:17:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>麻了 没想到，一直懒得搜的Mac截图快捷键在一个AI博主这里看到了，终于不用天天去找图标了。<br />
<br />
- Command+Option+Shift+4 选择屏幕的一小部分并将其复制到剪贴板中作为图像。<br />
- Command+Shift+4 做同样的事情，但将其保存为png文件放在桌面上。</p>
<p><a href="https://nitter.cz/karpathy/status/1740097030729683381#m">nitter.cz/karpathy/status/1740097030729683381#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740405945778532793#m</id>
            <title>RT by @op7418: 我们做Catjourney主要还是想帮助大家的找图和找提示词需求，只是好看用不到工作里面没有任何意义。
所以我们会紧跟最近的营销节点，不断上线可以用在项目和工作里的提示词和图片。

最近更新了很多春节国风红色主题的图，还有很多贺卡之类的样机模板，可以直接把文字或图片叠在上面。有需要的可以自行取用。

另外还上线了搜索功能，点击左边的搜索图标，输入相关单词就可以用了，现在还有点糙会继续优化。

这里使用：https://catjourney.life/</title>
            <link>https://nitter.cz/op7418/status/1740405945778532793#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740405945778532793#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 16:14:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们做Catjourney主要还是想帮助大家的找图和找提示词需求，只是好看用不到工作里面没有任何意义。<br />
所以我们会紧跟最近的营销节点，不断上线可以用在项目和工作里的提示词和图片。<br />
<br />
最近更新了很多春节国风红色主题的图，还有很多贺卡之类的样机模板，可以直接把文字或图片叠在上面。有需要的可以自行取用。<br />
<br />
另外还上线了搜索功能，点击左边的搜索图标，输入相关单词就可以用了，现在还有点糙会继续优化。<br />
<br />
这里使用：<a href="https://catjourney.life/">catjourney.life/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Njb3U1UWJjQUEzdkYtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740352582906949730#m</id>
            <title>RT by @op7418: 之前提过一嘴国内没办法上 ChatGPT 的朋友我一般会推荐用月之暗面的 Kimi Chat。
我最近也开始用的比较多了，尤其在一些我的日常任务上面，它比 ChatGPT 还方便一些，尤其是不用担心网络问题打开就用，最重要的是目前所有功能都是免费的。

Kimi 的表现从侧面上说明一个 AI 产品模型本身能力是一方面，对整个 LLM 生态其他能力的打磨和探索也很重要。

下面从我日常用的最多的两件事情来举例子：

首先是论文的总结和阅读，之前 PDF 格式的论文文件总结起来很费劲因为涉及到很多 OCR 的事情即使是 GPT-4 完成的也不太好，总结的不够详细总是忽略关键信息。

刚好最近arxiv默认支持了网页格式的论文，理论上应该更好总结了，直接丢网页版本链接就行，Kimi 在这件事情上做的非常好，比如昨天字节的这个很复杂的论文，非常详细而且结构很清晰，我没有特别优化提示词。

反观 GPT-4，直接告诉我没办法访问这个链接，太离谱了，之前 PDF 的时候虽然不输出关键信息，起码还会写，这下直接不访问了。

我日常最多用的第二个事情是翻译，所以第二个测试是用宝玉的翻译提示词分别让 ChatGPT 和 Kimi 翻译同一段比较复杂的 LLM 论文的简介，首先翻译内容上两者没有出现幻觉和丢失的情况。

ChatGPT出现问题的方面主要是体验和行文，Code 组件无论多长的内容都不会换行不管输出的内容是不是代码，所以导致我根本无法在界面上完整预览，输出结果。 Kimi 则会在大段文本内容输出的时候正确的换行，起码我可以看全。

另外在意译结果上，我感觉 Kimi 更符合中文的书写和阅读直觉，看起来更加流畅，可能还是因为 ChatGPT 主要照顾的还是英文用户，所以中文语料相对较少，导致写的中文即使通过提示词优化过也还是有翻译腔。各位也可以对比一下。

这里使用 Kimi，现在是免费的：https://kimi.moonshot.cn/?utm_campaign=TR_0pmOqbGz&amp;utm_content=&amp;utm_medium=Twitter&amp;utm_source=CH_2NsfSkVO&amp;utm_term=</title>
            <link>https://nitter.cz/op7418/status/1740352582906949730#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740352582906949730#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 12:42:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前提过一嘴国内没办法上 ChatGPT 的朋友我一般会推荐用月之暗面的 Kimi Chat。<br />
我最近也开始用的比较多了，尤其在一些我的日常任务上面，它比 ChatGPT 还方便一些，尤其是不用担心网络问题打开就用，最重要的是目前所有功能都是免费的。<br />
<br />
Kimi 的表现从侧面上说明一个 AI 产品模型本身能力是一方面，对整个 LLM 生态其他能力的打磨和探索也很重要。<br />
<br />
下面从我日常用的最多的两件事情来举例子：<br />
<br />
首先是论文的总结和阅读，之前 PDF 格式的论文文件总结起来很费劲因为涉及到很多 OCR 的事情即使是 GPT-4 完成的也不太好，总结的不够详细总是忽略关键信息。<br />
<br />
刚好最近arxiv默认支持了网页格式的论文，理论上应该更好总结了，直接丢网页版本链接就行，Kimi 在这件事情上做的非常好，比如昨天字节的这个很复杂的论文，非常详细而且结构很清晰，我没有特别优化提示词。<br />
<br />
反观 GPT-4，直接告诉我没办法访问这个链接，太离谱了，之前 PDF 的时候虽然不输出关键信息，起码还会写，这下直接不访问了。<br />
<br />
我日常最多用的第二个事情是翻译，所以第二个测试是用宝玉的翻译提示词分别让 ChatGPT 和 Kimi 翻译同一段比较复杂的 LLM 论文的简介，首先翻译内容上两者没有出现幻觉和丢失的情况。<br />
<br />
ChatGPT出现问题的方面主要是体验和行文，Code 组件无论多长的内容都不会换行不管输出的内容是不是代码，所以导致我根本无法在界面上完整预览，输出结果。 Kimi 则会在大段文本内容输出的时候正确的换行，起码我可以看全。<br />
<br />
另外在意译结果上，我感觉 Kimi 更符合中文的书写和阅读直觉，看起来更加流畅，可能还是因为 ChatGPT 主要照顾的还是英文用户，所以中文语料相对较少，导致写的中文即使通过提示词优化过也还是有翻译腔。各位也可以对比一下。<br />
<br />
这里使用 Kimi，现在是免费的：<a href="https://kimi.moonshot.cn/?utm_campaign=TR_0pmOqbGz&amp;utm_content=&amp;utm_medium=Twitter&amp;utm_source=CH_2NsfSkVO&amp;utm_term=">kimi.moonshot.cn/?utm_campai…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiNVJnTmJzQUFnaEF0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiNVZWWWJJQUFZbDlhLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiNVhpcmJvQUFQUVN4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiNVpnbmIwQUFVME8zLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740441933443637598#m</id>
            <title>太漂亮了</title>
            <link>https://nitter.cz/op7418/status/1740441933443637598#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740441933443637598#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 18:37:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>太漂亮了</p>
<p><a href="https://nitter.cz/coffee2hai/status/1740125430546657459#m">nitter.cz/coffee2hai/status/1740125430546657459#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740383223501476174#m</id>
            <title>阿里DreaMoving的Demo发布到Huggingface上了，但是依然需要排队很久，还有没有放出开源代码，不试了。</title>
            <link>https://nitter.cz/op7418/status/1740383223501476174#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740383223501476174#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 14:44:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里DreaMoving的Demo发布到Huggingface上了，但是依然需要排队很久，还有没有放出开源代码，不试了。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1740380744726594003#m">nitter.cz/_akhaliq/status/1740380744726594003#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740354727953776744#m</id>
            <title>R to @op7418: 最近我自己的提示词和图片都已经上传到 CatJourney，还有莱森做的一些图，我们还收集了很多其他优秀的提示词和图片。

欢迎来看看：https://catjourney.life/</title>
            <link>https://nitter.cz/op7418/status/1740354727953776744#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740354727953776744#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 12:51:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近我自己的提示词和图片都已经上传到 CatJourney，还有莱森做的一些图，我们还收集了很多其他优秀的提示词和图片。<br />
<br />
欢迎来看看：<a href="https://catjourney.life/">catjourney.life/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiN1VueGFFQUE1cF9fLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>