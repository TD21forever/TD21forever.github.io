<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757431514214932736#m</id>
            <title>R to @op7418: 这个是Stable Cascade的演示吗？为啥和 Demo 差距这么大。
https://x.com/EMostaque/status/1757431180201242791?s=20</title>
            <link>https://nitter.cz/op7418/status/1757431514214932736#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757431514214932736#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:48:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是Stable Cascade的演示吗？为啥和 Demo 差距这么大。<br />
<a href="https://x.com/EMostaque/status/1757431180201242791?s=20">x.com/EMostaque/status/17574…</a></p>
<p><a href="https://nitter.cz/EMostaque/status/1757431180201242791#m">nitter.cz/EMostaque/status/1757431180201242791#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757427744085065891#m</id>
            <title>R to @op7418: 尝试了一下Stable Cascade，可能是 Demo 的资源问题，现在生成的图片细节非常的差，基本没有纹理，和介绍页面的差距较大。

提示词响应还可以，文字生成真的不错。

这里尝试：https://huggingface.co/spaces/multimodalart/stable-cascade</title>
            <link>https://nitter.cz/op7418/status/1757427744085065891#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757427744085065891#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:33:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>尝试了一下Stable Cascade，可能是 Demo 的资源问题，现在生成的图片细节非常的差，基本没有纹理，和介绍页面的差距较大。<br />
<br />
提示词响应还可以，文字生成真的不错。<br />
<br />
这里尝试：<a href="https://huggingface.co/spaces/multimodalart/stable-cascade">huggingface.co/spaces/multim…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPakk4QmE4QUFGUDlxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPakk4QmFZQUEtNUx0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPakk4QmFBQUU2a191LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757424908970451136#m</id>
            <title>Stable Cascade的模型已经正式放出了，A、B 、C 三个阶段的模型都在里面。

模型页面也有一些新的图片，看起来效果确实比 SDXL 要好一些，尤其是文字部分。

在这里下载：https://huggingface.co/stabilityai/stable-cascade</title>
            <link>https://nitter.cz/op7418/status/1757424908970451136#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757424908970451136#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:22:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable Cascade的模型已经正式放出了，A、B 、C 三个阶段的模型都在里面。<br />
<br />
模型页面也有一些新的图片，看起来效果确实比 SDXL 要好一些，尤其是文字部分。<br />
<br />
在这里下载：<a href="https://huggingface.co/stabilityai/stable-cascade">huggingface.co/stabilityai/s…</a></p>
<p><a href="https://nitter.cz/op7418/status/1757318955713384723#m">nitter.cz/op7418/status/1757318955713384723#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPZ1hUaGJvQUE5VUpSLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757222182894424254#m</id>
            <title>RT by @op7418: 黄仁勋最近在迪拜的世界政府峰会上呼吁所有国家开发属于自己的“国家级人工智能”（Sovereign AI）。

换言之，数据就像新时代的黄金，每个国家都需要自主掌控自己的人工智能技术开发。

他强调，“这件事不能让其他人代劳。”</title>
            <link>https://nitter.cz/op7418/status/1757222182894424254#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757222182894424254#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 01:56:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>黄仁勋最近在迪拜的世界政府峰会上呼吁所有国家开发属于自己的“国家级人工智能”（Sovereign AI）。<br />
<br />
换言之，数据就像新时代的黄金，每个国家都需要自主掌控自己的人工智能技术开发。<br />
<br />
他强调，“这件事不能让其他人代劳。”</p>
<p><a href="https://nitter.cz/rowancheung/status/1757080008135258258#m">nitter.cz/rowancheung/status/1757080008135258258#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757215817664692711#m</id>
            <title>RT by @op7418: Gemini的更新速度可以啊。

已经在很多图像生成和编码案例中降低了对非实时URL输入的拒绝率。

正专注于进一步降低拒绝率，同时确保项目的正确对齐。</title>
            <link>https://nitter.cz/op7418/status/1757215817664692711#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757215817664692711#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 01:31:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini的更新速度可以啊。<br />
<br />
已经在很多图像生成和编码案例中降低了对非实时URL输入的拒绝率。<br />
<br />
正专注于进一步降低拒绝率，同时确保项目的正确对齐。</p>
<p><a href="https://nitter.cz/JackK/status/1757204678524424539#m">nitter.cz/JackK/status/1757204678524424539#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757198559433183324#m</id>
            <title>RT by @op7418: 还有这种玩法，先用SVD生成然后用Web UI跑一次Animatediff，SVD 的噪点被优化了非常多。细看也没问题。</title>
            <link>https://nitter.cz/op7418/status/1757198559433183324#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757198559433183324#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 00:22:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还有这种玩法，先用SVD生成然后用Web UI跑一次Animatediff，SVD 的噪点被优化了非常多。细看也没问题。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcwNjMyNjI1MDY4MTk1ODQvcHUvaW1nL3dUb0FJSFV6cWt4bUswLXIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757192885165056157#m</id>
            <title>RT by @op7418: 配音演员新出路？

ElevenLabs 现在可以在语音库中分享自己的声音模型，并获得收益。

只需要 30 分钟的音频用于训练。工作原理如下：

前往 VoiceLab 并上传 30 分钟以上的音频

命名并描述你的声音

设置价格和使用参数

添加付款详细信息

了解更多：https://elevenlabs.io/voice-actors</title>
            <link>https://nitter.cz/op7418/status/1757192885165056157#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757192885165056157#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 00:00:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>配音演员新出路？<br />
<br />
ElevenLabs 现在可以在语音库中分享自己的声音模型，并获得收益。<br />
<br />
只需要 30 分钟的音频用于训练。工作原理如下：<br />
<br />
前往 VoiceLab 并上传 30 分钟以上的音频<br />
<br />
命名并描述你的声音<br />
<br />
设置价格和使用参数<br />
<br />
添加付款详细信息<br />
<br />
了解更多：<a href="https://elevenlabs.io/voice-actors">elevenlabs.io/voice-actors</a></p>
<p><a href="https://nitter.cz/elevenlabsio/status/1757087275131748639#m">nitter.cz/elevenlabsio/status/1757087275131748639#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1NTgyNjE4NzM4Nzk4NTkyMC9GZ3R6dzYtZj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757395823003152660#m</id>
            <title>确实，AI时代就是什么都懂一点的人的胜利。</title>
            <link>https://nitter.cz/op7418/status/1757395823003152660#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757395823003152660#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 13:26:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>确实，AI时代就是什么都懂一点的人的胜利。</p>
<p><a href="https://nitter.cz/geekplux/status/1757242916689338752#m">nitter.cz/geekplux/status/1757242916689338752#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757355160760881285#m</id>
            <title>好东西，讲的非常清晰易懂</title>
            <link>https://nitter.cz/op7418/status/1757355160760881285#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757355160760881285#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 10:44:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>好东西，讲的非常清晰易懂</p>
<p><a href="https://nitter.cz/MoonAtCloud/status/1756838592348127460#m">nitter.cz/MoonAtCloud/status/1756838592348127460#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757345123816616365#m</id>
            <title>R to @op7418: 试了一下确实还行</title>
            <link>https://nitter.cz/op7418/status/1757345123816616365#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757345123816616365#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 10:05:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>试了一下确实还行</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dOWUNwZWFJQUE0cGJkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757340944402440239#m</id>
            <title>KREA 发布了他们图像放大功能的 2.0 版本，从演示来看似乎跟 Magnific AI 不相上下了。

可以免费试用，但是需要排队，时长有点离谱，不过整体来看还是比Magnific AI便宜。

这里尝试：https://www.krea.ai/apps/image/enhancer</title>
            <link>https://nitter.cz/op7418/status/1757340944402440239#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757340944402440239#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 09:48:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>KREA 发布了他们图像放大功能的 2.0 版本，从演示来看似乎跟 Magnific AI 不相上下了。<br />
<br />
可以免费试用，但是需要排队，时长有点离谱，不过整体来看还是比Magnific AI便宜。<br />
<br />
这里尝试：<a href="https://www.krea.ai/apps/image/enhancer">krea.ai/apps/image/enhancer</a></p>
<p><a href="https://nitter.cz/krea_ai/status/1757338152933662855#m">nitter.cz/krea_ai/status/1757338152933662855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757327426747764996#m</id>
            <title>ComfyUI ProPost一个非常有意思的节点，可以为你生成的图片添加各种丰富的效果，比如胶片颗粒、暗角、模糊等等，帮助生成图片的质感获得极大的提升。

你还可以对这些效果组合使用，比如下面这张图。

支持的效果有：

胶片颗粒：它可以创建不同的噪声类型和图案，并且可用于创建各种胶片颗粒外观。

晕影效果：使屏幕边缘变暗。

径向模糊：让你模糊图像的边缘。

深度图模糊：允许根据深度图模糊图像。可以将其与现有的深度图节点结合使用。

应用 LUT 滤镜：允许将 3D LUT 应用到图像。目前它仅支持 CUBE 格式的 3D LUT。

项目地址：https://github.com/digitaljohn/comfyui-propost#depth-map-blur</title>
            <link>https://nitter.cz/op7418/status/1757327426747764996#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757327426747764996#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:54:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI ProPost一个非常有意思的节点，可以为你生成的图片添加各种丰富的效果，比如胶片颗粒、暗角、模糊等等，帮助生成图片的质感获得极大的提升。<br />
<br />
你还可以对这些效果组合使用，比如下面这张图。<br />
<br />
支持的效果有：<br />
<br />
胶片颗粒：它可以创建不同的噪声类型和图案，并且可用于创建各种胶片颗粒外观。<br />
<br />
晕影效果：使屏幕边缘变暗。<br />
<br />
径向模糊：让你模糊图像的边缘。<br />
<br />
深度图模糊：允许根据深度图模糊图像。可以将其与现有的深度图节点结合使用。<br />
<br />
应用 LUT 滤镜：允许将 3D LUT 应用到图像。目前它仅支持 CUBE 格式的 3D LUT。<br />
<br />
项目地址：<a href="https://github.com/digitaljohn/comfyui-propost#depth-map-blur">github.com/digitaljohn/comfy…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dOSDV0WmE0QUFSUkFyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757325374860787911#m</id>
            <title>试了一下Leiapix 这个可以提取图片深度信息，然后生成运镜视频的产品。

效果还挺好的，自定义选项也很丰富，一些简单的场景运镜视频不用视频生成工具用这个也挺好。

直接上传图片然后再右侧调整选项就行。

这里尝试：https://www.leiapix.com/</title>
            <link>https://nitter.cz/op7418/status/1757325374860787911#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757325374860787911#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:46:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>试了一下Leiapix 这个可以提取图片深度信息，然后生成运镜视频的产品。<br />
<br />
效果还挺好的，自定义选项也很丰富，一些简单的场景运镜视频不用视频生成工具用这个也挺好。<br />
<br />
直接上传图片然后再右侧调整选项就行。<br />
<br />
这里尝试：<a href="https://www.leiapix.com/">leiapix.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTczMjUzMjkyMTgyNzMyODAvcHUvaW1nL29oeGpuZ19DeklvTkY1eS0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757321176320524688#m</id>
            <title>R to @op7418: 比较奇怪的是这个是由日本部门发布的，英语官网没有，这里是原推：
https://x.com/StabilityAI_JP/status/1757197090089058548?s=20</title>
            <link>https://nitter.cz/op7418/status/1757321176320524688#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757321176320524688#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:29:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>比较奇怪的是这个是由日本部门发布的，英语官网没有，这里是原推：<br />
<a href="https://x.com/StabilityAI_JP/status/1757197090089058548?s=20">x.com/StabilityAI_JP/status/…</a></p>
<p><a href="https://nitter.cz/StabilityAI_JP/status/1757197090089058548#m">nitter.cz/StabilityAI_JP/status/1757197090089058548#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757320624966631719#m</id>
            <title>R to @op7418: 讲话影响立竿见影，英伟达的市值历史上首次超过谷歌和亚马逊。 $NVDA目前是全球第四大最有价值的上市公司，价值 1.84 万亿美元。
https://x.com/KobeissiLetter/status/1757082230571094069?s=20</title>
            <link>https://nitter.cz/op7418/status/1757320624966631719#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757320624966631719#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:27:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>讲话影响立竿见影，英伟达的市值历史上首次超过谷歌和亚马逊。 $NVDA目前是全球第四大最有价值的上市公司，价值 1.84 万亿美元。<br />
<a href="https://x.com/KobeissiLetter/status/1757082230571094069?s=20">x.com/KobeissiLetter/status/…</a></p>
<p><a href="https://nitter.cz/KobeissiLetter/status/1757082230571094069#m">nitter.cz/KobeissiLetter/status/1757082230571094069#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757319254830129234#m</id>
            <title>R to @op7418: 这里是Würstchen架构简介：

我们推出了一种创新的文本到图像合成技术，名为Würstchen。这一技术不仅在性能上与众不同，还在成本效率上达到了新的高度，特别适用于处理大规模的文本到图像转换任务。

我们的关键创新之一是开发了一种称为“潜在扩散”的技术。在这项技术中，我们创造了一种既详尽又极为紧凑的图像表示方法，用以有效指导图像生成的扩散过程。这种高度压缩的图像表示方式比传统的基于语言的表示方法提供了更加精确的指引，从而大幅降低了达到顶尖水平所需的计算量。

我们的方法不仅提高了根据文本生成图像的质量，还通过用户偏好研究得到了验证。与传统的方法如Stable Diffusion 2.1相比，Würstchen在训练时只需要24,602个A100-GPU小时，而传统方法则需要高达200,000个GPU小时。此外，我们的方法在获取这些成果时需要的训练数据更少。更重要的是，我们紧凑的潜在表示使得推理过程的速度提高了一倍多，这不仅显著降低了成本和碳排放，还保持了卓越的性能。

在与其他顶尖模型的比较中，Würstchen在效率方面表现出显著的优势，并在图像质量上也有良好的表现。我们相信，这项工作将进一步促进人们对于性能和计算可访问性的重视，推动这一领域的发展。

论文地址：https://openreview.net/forum?id=gU58d5QeGv</title>
            <link>https://nitter.cz/op7418/status/1757319254830129234#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757319254830129234#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:22:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这里是Würstchen架构简介：<br />
<br />
我们推出了一种创新的文本到图像合成技术，名为Würstchen。这一技术不仅在性能上与众不同，还在成本效率上达到了新的高度，特别适用于处理大规模的文本到图像转换任务。<br />
<br />
我们的关键创新之一是开发了一种称为“潜在扩散”的技术。在这项技术中，我们创造了一种既详尽又极为紧凑的图像表示方法，用以有效指导图像生成的扩散过程。这种高度压缩的图像表示方式比传统的基于语言的表示方法提供了更加精确的指引，从而大幅降低了达到顶尖水平所需的计算量。<br />
<br />
我们的方法不仅提高了根据文本生成图像的质量，还通过用户偏好研究得到了验证。与传统的方法如Stable Diffusion 2.1相比，Würstchen在训练时只需要24,602个A100-GPU小时，而传统方法则需要高达200,000个GPU小时。此外，我们的方法在获取这些成果时需要的训练数据更少。更重要的是，我们紧凑的潜在表示使得推理过程的速度提高了一倍多，这不仅显著降低了成本和碳排放，还保持了卓越的性能。<br />
<br />
在与其他顶尖模型的比较中，Würstchen在效率方面表现出显著的优势，并在图像质量上也有良好的表现。我们相信，这项工作将进一步促进人们对于性能和计算可访问性的重视，推动这一领域的发展。<br />
<br />
论文地址：<a href="https://openreview.net/forum?id=gU58d5QeGv">openreview.net/forum?id=gU58…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757318955713384723#m</id>
            <title>Stability AI产量很高啊，推出了一个新的 AI 图像生成模型Stable Cascade，还会发布对应的微调、ControlNet 和 LoRA 训练的脚本。

这个模型基于Würstchen架构，可以显著降低模型训练的算力成本，比 SD2.1 的算力成本降低了 10 倍左右。另外推理速度会比现有的 SD 模型快一倍左右。

更多功能：

除了标准的文本到图像生成之外，Stable Cascade 还可以执行图像变化和图像到图像生成。

会跟随模型一起发布的 Controlnet：

局部重绘：输入与文本提示附带的蒙版配对的图像。该模型根据提供的文本提示填充图像的遮罩部分。

Canny Edge：通过跟踪输入到模型的现有图像的边缘来生成新图像。该测试也可以从草图进行扩展。

2x超分辨率：也可用于C阶段生成的潜在空间。

了解更多：https://ja.stability.ai/blog/stable-cascade</title>
            <link>https://nitter.cz/op7418/status/1757318955713384723#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757318955713384723#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:21:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI产量很高啊，推出了一个新的 AI 图像生成模型Stable Cascade，还会发布对应的微调、ControlNet 和 LoRA 训练的脚本。<br />
<br />
这个模型基于Würstchen架构，可以显著降低模型训练的算力成本，比 SD2.1 的算力成本降低了 10 倍左右。另外推理速度会比现有的 SD 模型快一倍左右。<br />
<br />
更多功能：<br />
<br />
除了标准的文本到图像生成之外，Stable Cascade 还可以执行图像变化和图像到图像生成。<br />
<br />
会跟随模型一起发布的 Controlnet：<br />
<br />
局部重绘：输入与文本提示附带的蒙版配对的图像。该模型根据提供的文本提示填充图像的遮罩部分。<br />
<br />
Canny Edge：通过跟踪输入到模型的现有图像的边缘来生成新图像。该测试也可以从草图进行扩展。<br />
<br />
2x超分辨率：也可用于C阶段生成的潜在空间。<br />
<br />
了解更多：<a href="https://ja.stability.ai/blog/stable-cascade">ja.stability.ai/blog/stable-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dNLU0tZGIwQUF1S0FqLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1757315629210890378#m</id>
            <title>卷起来了，AI 视频加特效和排版</title>
            <link>https://nitter.cz/op7418/status/1757315629210890378#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1757315629210890378#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:07:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卷起来了，AI 视频加特效和排版</p>
<p><a href="https://nitter.cz/lepadphone/status/1757302076185682425#m">nitter.cz/lepadphone/status/1757302076185682425#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>