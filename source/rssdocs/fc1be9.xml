<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748293897237991794#m</id>
            <title>帮快手的朋友招两个设计，主要在探索UI 和运营设计的自动生成，需要在 AIGC 领域有过设计实践，懂 SD，有前端能力更好。
邮件标题或者备注可以加上在歸藏这里看到的。

下面是完整的 JD：
快手研发设计中心团队，负责面向公司内部的产品交互设计。我们正在进行前沿的 AIGC 设计探索，并把探索的成果变成产品，实现 UI 和运营设计的自动生成，希望找到志同道合的朋友一起同行:

1. 希望你是 2024 年 6/7 月毕业的本科/研究生,专业不限
2. 有 B 端 UI 设计 / 工具类 UI 设计能力
3. 希望你在通用设计能力上至少有一个亮点（良好的表达 / 优秀的审美 / 出色的逻辑性 / 良好的同理心…）
4. 希望你对新鲜事物感兴趣，并且有一定自己的探索和尝试
5. 在 AIGC 方向有过学习实践，熟悉 SD，或有一定前端代码能力优先
6. 坐标北京

如果你对我们的工作感兴趣，欢迎投递简历和作品集，我的微信：ding_zu，邮箱 dzwangyihan@gmail.com</title>
            <link>https://nitter.cz/op7418/status/1748293897237991794#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748293897237991794#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 10:38:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>帮快手的朋友招两个设计，主要在探索UI 和运营设计的自动生成，需要在 AIGC 领域有过设计实践，懂 SD，有前端能力更好。<br />
邮件标题或者备注可以加上在歸藏这里看到的。<br />
<br />
下面是完整的 JD：<br />
快手研发设计中心团队，负责面向公司内部的产品交互设计。我们正在进行前沿的 AIGC 设计探索，并把探索的成果变成产品，实现 UI 和运营设计的自动生成，希望找到志同道合的朋友一起同行:<br />
<br />
1. 希望你是 2024 年 6/7 月毕业的本科/研究生,专业不限<br />
2. 有 B 端 UI 设计 / 工具类 UI 设计能力<br />
3. 希望你在通用设计能力上至少有一个亮点（良好的表达 / 优秀的审美 / 出色的逻辑性 / 良好的同理心…）<br />
4. 希望你对新鲜事物感兴趣，并且有一定自己的探索和尝试<br />
5. 在 AIGC 方向有过学习实践，熟悉 SD，或有一定前端代码能力优先<br />
6. 坐标北京<br />
<br />
如果你对我们的工作感兴趣，欢迎投递简历和作品集，我的微信：ding_zu，邮箱 dzwangyihan@gmail.com</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VNd0JDRWJ3QUFDTTZVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748289276708855904#m</id>
            <title>Nick这张用 Midjourney 生成的手帐照片有点意思，可以用来表达观点和配图，比如他表达意思就是：

在X平台上成长的唯一秘诀就是发布优质内容，而“关于如何在X平台上成长”的内容很少能算作优质内容。

我也试了一下居然真的可以，后面加上-- s 0 会生成的更稳定。

完整提示词：
a note that reads "The only secret to growing on X is putting out good content and "content about how to grow on X is very rarely good content" --ar 4:5 --style raw --v 6.0 --s 0</title>
            <link>https://nitter.cz/op7418/status/1748289276708855904#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748289276708855904#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 10:20:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nick这张用 Midjourney 生成的手帐照片有点意思，可以用来表达观点和配图，比如他表达意思就是：<br />
<br />
在X平台上成长的唯一秘诀就是发布优质内容，而“关于如何在X平台上成长”的内容很少能算作优质内容。<br />
<br />
我也试了一下居然真的可以，后面加上-- s 0 会生成的更稳定。<br />
<br />
完整提示词：<br />
a note that reads "The only secret to growing on X is putting out good content and "content about how to grow on X is very rarely good content" --ar 4:5 --style raw --v 6.0 --s 0</p>
<p><a href="https://nitter.cz/nickfloats/status/1748285824339153407#m">nitter.cz/nickfloats/status/1748285824339153407#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VNcm1rZWFZQUFDc2FsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748249758723150303#m</id>
            <title>专注于 Animatediff 动画的赛博菩萨Jerry Davos发布了一套用 IPapadter 和遮罩给跳舞视频转视频，并且可以更换背景的工作流。
老样子里面除了工作流之外还有非常详细的教程，还有涉及到的模型。

工作流下载：https://www.patreon.com/posts/v3-0-bg-changer-96735652</title>
            <link>https://nitter.cz/op7418/status/1748249758723150303#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748249758723150303#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 07:43:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>专注于 Animatediff 动画的赛博菩萨Jerry Davos发布了一套用 IPapadter 和遮罩给跳舞视频转视频，并且可以更换背景的工作流。<br />
老样子里面除了工作流之外还有非常详细的教程，还有涉及到的模型。<br />
<br />
工作流下载：<a href="https://www.patreon.com/posts/v3-0-bg-changer-96735652">patreon.com/posts/v3-0-bg-ch…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgyMzkwNjkxNTc3MDc3NzYvcHUvaW1nLzFKLXlWd21WUjZybkpCbDEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748233784582144147#m</id>
            <title>剪过的完整版本</title>
            <link>https://nitter.cz/op7418/status/1748233784582144147#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748233784582144147#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 06:39:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>剪过的完整版本</p>
<p><a href="https://nitter.cz/indigo11/status/1748194964016889990#m">nitter.cz/indigo11/status/1748194964016889990#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748194254667829407#m</id>
            <title>R to @op7418: 补一个摩尔线程的项目地址：https://github.com/MooreThreads/Moore-AnimateAnyone</title>
            <link>https://nitter.cz/op7418/status/1748194254667829407#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748194254667829407#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 04:02:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补一个摩尔线程的项目地址：<a href="https://github.com/MooreThreads/Moore-AnimateAnyone">github.com/MooreThreads/Moor…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0NTc3ODEyODE2NzgyNTQwOS9JbzVSM1lRMj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748012298038653237#m</id>
            <title>RT by @op7418: 今天无意间刷到《恋与深空》的宣传片，才发现女性向游戏已经做到这种地步了，NPC的表情动作和配音都非常自然生动。
场景也非常漂亮，麻了，男性向没有这么高质量的恋爱游戏啊。</title>
            <link>https://nitter.cz/op7418/status/1748012298038653237#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748012298038653237#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 15:59:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天无意间刷到《恋与深空》的宣传片，才发现女性向游戏已经做到这种地步了，NPC的表情动作和配音都非常自然生动。<br />
场景也非常漂亮，麻了，男性向没有这么高质量的恋爱游戏啊。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgwMTE4MjAzNDkzNzQ0NjQvcHUvaW1nL2Q5LVo2VkNSc0Izc0dyUnUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748191803965378723#m</id>
            <title>另一个 ComfyUI 上的AnimateAnyone实现，看起来比其他的稳定一些，面部崩的不是很厉害。

也是基于摩尔线程的AnimateAnyone方案，不过这个的文档写的非常详细，而且提供了不同采样器的效果预览。

项目地址：https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved</title>
            <link>https://nitter.cz/op7418/status/1748191803965378723#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748191803965378723#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:53:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个 ComfyUI 上的AnimateAnyone实现，看起来比其他的稳定一些，面部崩的不是很厉害。<br />
<br />
也是基于摩尔线程的AnimateAnyone方案，不过这个的文档写的非常详细，而且提供了不同采样器的效果预览。<br />
<br />
项目地址：<a href="https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved">github.com/MrForExample/Comf…</a></p>
<p><a href="https://nitter.cz/MrForExample/status/1748080976104734858#m">nitter.cz/MrForExample/status/1748080976104734858#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgxOTE3NzM1NDQwMzAyMDgvcHUvaW1nL3JmVVR0Q3dwa3lwNEtXaVUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748120766103609715#m</id>
            <title>RT by @op7418: Meta 正在开发开源的通用人工智能（AGI），马克·扎克伯格宣布。

今天，扎克伯格意外的在一则 Instagram Reel 中透露，Meta 正致力于开发开源的通用人工智能（AGI）。为了实现这一目标，公司正将其两大 AI 研究团队 FAIR 和 GenAI 进行更紧密的整合，以构建完整的通用智能，并尽可能地开源。

扎克伯格在视频中谈道：“我们的长期愿景是开发通用智能，并以负责任的方式进行开源，让每个人都能广泛受益。”他在视频中表示：“我们清楚地认识到，下一代服务的需求是构建完整的通用智能，包括最优秀的 AI 助手、创意工作者用 AI、企业用 AI 等，这需要在 AI 的各个领域，包括推理、规划、编程、记忆和其他认知能力上取得进步。”

在谈到 Llama 3、基础设施和元宇宙方面，扎克伯格也表达了兴奋之情。他强调，公司目前正在培训 Llama 3 模型，并正在建设大规模的计算基础设施，包括到今年年底部署 350,000 个 Nvidia H100s。

扎克伯格还对元宇宙和 Meta 的 Ray-Ban 智能眼镜表现出极大的热情。“人们将需要新型 AI 设备，而这将逐渐将 AI 与元宇宙融合。”他说，“我认为我们很多人将会经常通过对话与 AI 互动。而且我相信，很多人会通过佩戴智能眼镜来实现这一点。这些眼镜是让 AI 观察你所见、听到你所闻的理想载体，因此 AI 可以随时随地协助你。”

这一声明是在 OpenAI 首席执行官 Sam Altman 在瑞士达沃斯的世界经济论坛上对 AGI 发表评论，并在他 2023 年 11 月重新上任两个月后对 AGI 存在风险的态度有所缓和之后发布的。尽管 Meta 的首席科学家 Yann LeCun 一直对 AGI 的即时到来持怀疑态度，认为至少在未来五年内不会实现，但这一声明依然发布了。

最后，Meta 计划将其未来的 AGI 开源的消息是在 VentureBeat 称 lama 和开源 AI “赢得了” 2023 年仅几个月后发布的。这一声明无疑将引发关于开源与闭源 AI 的进一步讨论，尤其是在 Anthropic 发布论文称开源模型可能隐藏着具有破坏性的“沉睡代理”之后。</title>
            <link>https://nitter.cz/dotey/status/1748120766103609715#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748120766103609715#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 23:10:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta 正在开发开源的通用人工智能（AGI），马克·扎克伯格宣布。<br />
<br />
今天，扎克伯格意外的在一则 Instagram Reel 中透露，Meta 正致力于开发开源的通用人工智能（AGI）。为了实现这一目标，公司正将其两大 AI 研究团队 FAIR 和 GenAI 进行更紧密的整合，以构建完整的通用智能，并尽可能地开源。<br />
<br />
扎克伯格在视频中谈道：“我们的长期愿景是开发通用智能，并以负责任的方式进行开源，让每个人都能广泛受益。”他在视频中表示：“我们清楚地认识到，下一代服务的需求是构建完整的通用智能，包括最优秀的 AI 助手、创意工作者用 AI、企业用 AI 等，这需要在 AI 的各个领域，包括推理、规划、编程、记忆和其他认知能力上取得进步。”<br />
<br />
在谈到 Llama 3、基础设施和元宇宙方面，扎克伯格也表达了兴奋之情。他强调，公司目前正在培训 Llama 3 模型，并正在建设大规模的计算基础设施，包括到今年年底部署 350,000 个 Nvidia H100s。<br />
<br />
扎克伯格还对元宇宙和 Meta 的 Ray-Ban 智能眼镜表现出极大的热情。“人们将需要新型 AI 设备，而这将逐渐将 AI 与元宇宙融合。”他说，“我认为我们很多人将会经常通过对话与 AI 互动。而且我相信，很多人会通过佩戴智能眼镜来实现这一点。这些眼镜是让 AI 观察你所见、听到你所闻的理想载体，因此 AI 可以随时随地协助你。”<br />
<br />
这一声明是在 OpenAI 首席执行官 Sam Altman 在瑞士达沃斯的世界经济论坛上对 AGI 发表评论，并在他 2023 年 11 月重新上任两个月后对 AGI 存在风险的态度有所缓和之后发布的。尽管 Meta 的首席科学家 Yann LeCun 一直对 AGI 的即时到来持怀疑态度，认为至少在未来五年内不会实现，但这一声明依然发布了。<br />
<br />
最后，Meta 计划将其未来的 AGI 开源的消息是在 VentureBeat 称 lama 和开源 AI “赢得了” 2023 年仅几个月后发布的。这一声明无疑将引发关于开源与闭源 AI 的进一步讨论，尤其是在 Anthropic 发布论文称开源模型可能隐藏着具有破坏性的“沉睡代理”之后。</p>
<p><a href="https://nitter.cz/VentureBeat/status/1748046430193746189#m">nitter.cz/VentureBeat/status/1748046430193746189#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgxMjAzMDk1MjAwMzU4NDAvcHUvaW1nLzV4Y0RRd1hjM2ZraGgyY1kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748190345769484715#m</id>
            <title>KREA AI现在可以直接通过文字生成图片，支持抠图和橡皮擦，这个演示有点搞的，把马和蛤蟆组合起来了。</title>
            <link>https://nitter.cz/op7418/status/1748190345769484715#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748190345769484715#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:47:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>KREA AI现在可以直接通过文字生成图片，支持抠图和橡皮擦，这个演示有点搞的，把马和蛤蟆组合起来了。</p>
<p><a href="https://nitter.cz/krea_ai/status/1748045292405195198#m">nitter.cz/krea_ai/status/1748045292405195198#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748189182307303606#m</id>
            <title>小扎刚发言完，Meta 就出王炸？推出了可以进行自我奖励的 LLM。
简单来说就是语言模型可以自我判断模型质量，从而实现一定程度上的自我进化。

使用这个方法微调的 Llama 2 70B 模型，优于 AlpacaEval 2.0 排行榜上的Claude 2、Gemini Pro 和 GPT-4 0613等模型。

实现方式：

自奖励语言模型，这类智能体具备双重功能：一方面（i）它们能够作为遵循指令的模型，针对给定的提示生成回应；另一方面（ii）它们还能创造并评估新的指令遵循示例，并将这些示例加入到自己的训练集中。

我们采用了与 Xu 等人（2023年）最近提出的类似的迭代式动态评价优化（Iterative DPO）框架来训练这些模型。从一个基础模型出发，在每一轮迭代中，模型都会经历一个自我指令生成的过程，在这个过程中，模型针对新创造的提示生成候选回应，并由模型自身对这些回应进行奖励评分。

这个评分过程是通过让大语言模型扮演评判员（LLM-as-a-Judge）的方式来实现的，这本身也是一种遵循指令的任务。然后，根据这些生成的数据构建一个偏好数据集，并利用动态评价优化方法来训练下一轮的模型。

论文地址：https://arxiv.org/html/2401.10020v1</title>
            <link>https://nitter.cz/op7418/status/1748189182307303606#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748189182307303606#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:42:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小扎刚发言完，Meta 就出王炸？推出了可以进行自我奖励的 LLM。<br />
简单来说就是语言模型可以自我判断模型质量，从而实现一定程度上的自我进化。<br />
<br />
使用这个方法微调的 Llama 2 70B 模型，优于 AlpacaEval 2.0 排行榜上的Claude 2、Gemini Pro 和 GPT-4 0613等模型。<br />
<br />
实现方式：<br />
<br />
自奖励语言模型，这类智能体具备双重功能：一方面（i）它们能够作为遵循指令的模型，针对给定的提示生成回应；另一方面（ii）它们还能创造并评估新的指令遵循示例，并将这些示例加入到自己的训练集中。<br />
<br />
我们采用了与 Xu 等人（2023年）最近提出的类似的迭代式动态评价优化（Iterative DPO）框架来训练这些模型。从一个基础模型出发，在每一轮迭代中，模型都会经历一个自我指令生成的过程，在这个过程中，模型针对新创造的提示生成候选回应，并由模型自身对这些回应进行奖励评分。<br />
<br />
这个评分过程是通过让大语言模型扮演评判员（LLM-as-a-Judge）的方式来实现的，这本身也是一种遵循指令的任务。然后，根据这些生成的数据构建一个偏好数据集，并利用动态评价优化方法来训练下一轮的模型。<br />
<br />
论文地址：<a href="https://arxiv.org/html/2401.10020v1">arxiv.org/html/2401.10020v1</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VMUXhvNGFJQUFrS05FLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748185756211028086#m</id>
            <title>前几天微软DragNUWA的一个应用，用非常简陋的 3D 白模控制视频生成中内容的运动方向和速度。

生成的视频看起来非常稳定，一个非常好的取巧方法。</title>
            <link>https://nitter.cz/op7418/status/1748185756211028086#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748185756211028086#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:28:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>前几天微软DragNUWA的一个应用，用非常简陋的 3D 白模控制视频生成中内容的运动方向和速度。<br />
<br />
生成的视频看起来非常稳定，一个非常好的取巧方法。</p>
<p><a href="https://nitter.cz/TDS_95514874/status/1747753162226675844#m">nitter.cz/TDS_95514874/status/1747753162226675844#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748184369976713484#m</id>
            <title>谷歌推出了用于用于增强大语言模型的选择性预测框架 ASPIRE。

这个预测框架可以在 LLM 输出的时候给出一个可靠性评分，让用户更好的判断 LLM 输出内容的可靠性。

选择性预测会让大语言模型在给出答案的同时，提供一个选择性得分，用以表示答案正确的可能性。

训练方式：
ASPIRE 的工作方式是对大语言模型进行针对性的微调，使其更擅长于问题回答任务，并训练模型自行评估其生成答案的正确性。该框架包括三个阶段：特定任务的调优、答案采样和自我评估学习（self-evaluation learning）。在特定任务调优阶段，ASPIRE 对预先训练好的大语言模型进行微调，以提升其预测表现。答案采样阶段则是针对每个训练问题生成多种答案，进而构建自我评估学习的数据集。到了自我评估学习阶段，ASPIRE 引入可调整参数，并对这些参数进行微调，以培养模型的自我评估能力。

结论：
ASPIRE 的出现标志着大语言模型领域的一次变革，它强调了模型容量并非其性能的唯一决定因素。实际上，通过策略性的调整，即便是小型模型也能实现更精确、更有信心的预测，从而显著提升模型的有效性。

内容来源：https://blog.research.google/2024/01/introducing-aspire-for-selective.html</title>
            <link>https://nitter.cz/op7418/status/1748184369976713484#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748184369976713484#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:23:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌推出了用于用于增强大语言模型的选择性预测框架 ASPIRE。<br />
<br />
这个预测框架可以在 LLM 输出的时候给出一个可靠性评分，让用户更好的判断 LLM 输出内容的可靠性。<br />
<br />
选择性预测会让大语言模型在给出答案的同时，提供一个选择性得分，用以表示答案正确的可能性。<br />
<br />
训练方式：<br />
ASPIRE 的工作方式是对大语言模型进行针对性的微调，使其更擅长于问题回答任务，并训练模型自行评估其生成答案的正确性。该框架包括三个阶段：特定任务的调优、答案采样和自我评估学习（self-evaluation learning）。在特定任务调优阶段，ASPIRE 对预先训练好的大语言模型进行微调，以提升其预测表现。答案采样阶段则是针对每个训练问题生成多种答案，进而构建自我评估学习的数据集。到了自我评估学习阶段，ASPIRE 引入可调整参数，并对这些参数进行微调，以培养模型的自我评估能力。<br />
<br />
结论：<br />
ASPIRE 的出现标志着大语言模型领域的一次变革，它强调了模型容量并非其性能的唯一决定因素。实际上，通过策略性的调整，即便是小型模型也能实现更精确、更有信心的预测，从而显著提升模型的有效性。<br />
<br />
内容来源：<a href="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">blog.research.google/2024/01…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0VMTVNneGE4QUFSdXNLLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dFTE1TZ3hhOEFBUnVzSy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748180416840995281#m</id>
            <title>刚发现摩尔线程前几天复原了阿里的单图跳舞项目并且已经开源训练代码，你可以训练自己的AnimateAnyone模型。
有个基于摩尔线程开源的版本制作了 ComfyUI 节点，并且提供了基础的工作流。
现在可以在ComfyUI中非常简单的让单图跳舞了。

节点地址：https://github.com/chaojie/ComfyUI-Moore-AnimateAnyone?tab=readme-ov-file</title>
            <link>https://nitter.cz/op7418/status/1748180416840995281#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748180416840995281#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:07:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚发现摩尔线程前几天复原了阿里的单图跳舞项目并且已经开源训练代码，你可以训练自己的AnimateAnyone模型。<br />
有个基于摩尔线程开源的版本制作了 ComfyUI 节点，并且提供了基础的工作流。<br />
现在可以在ComfyUI中非常简单的让单图跳舞了。<br />
<br />
节点地址：<a href="https://github.com/chaojie/ComfyUI-Moore-AnimateAnyone?tab=readme-ov-file">github.com/chaojie/ComfyUI-M…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VMSHlmZWIwQUF3TmNlLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748154256493072737#m</id>
            <title>啊？这都行？</title>
            <link>https://nitter.cz/op7418/status/1748154256493072737#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748154256493072737#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 01:23:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>啊？这都行？</p>
<p><a href="https://nitter.cz/satoshi/status/1748111687482323232#m">nitter.cz/satoshi/status/1748111687482323232#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1747937127520968737#m</id>
            <title>RT by @op7418: 整了个类似的动物走秀视频</title>
            <link>https://nitter.cz/op7418/status/1747937127520968737#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1747937127520968737#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 11:01:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整了个类似的动物走秀视频</p>
<p><a href="https://nitter.cz/op7418/status/1747930933343498259#m">nitter.cz/op7418/status/1747930933343498259#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDc5MzcwNDk1Mjc5MTA0MDAvcHUvaW1nLy1QSXhIUHJxVlozenBZaG4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1747947995319128497#m</id>
            <title>RT by @op7418: 卷就完事了，百度也推出了自己的视频生成模型 UniVG。

这个模型的特点是，区分了高自由度和低自由度两种任务，分别使用了不同的方式生成视频。
根据运动幅度的不同使用不同的方案确实是现在没办法平衡两者的一个办法。

简介：
视频生成技术，特别是基于“扩散”原理的方法，近来在学术和产业界引起了广泛关注，并已取得显著成就。目前，这一领域的研究和实践主要聚焦于单一目标或单一任务的视频生成，例如根据文本、图片或它们的组合来生成视频。但这样的方法并不能完全满足真实世界多变的应用需求。在实际应用中，用户往往需要更灵活的输入方式，比如单独使用图像或文本，或者将二者结合起来。

针对这一问题，我们提出了一种新的视频生成系统——“统一模态视频生成系统” (UNIfied-modal Video Generation system)。这个系统能够处理各种文本和图像的组合输入。为了实现这一目标，我们重新定义了视频生成模型中的多项任务，将它们分为“高自由度生成”和“低自由度生成”两大类。在高自由度视频生成方面，我们运用了“多条件交叉注意力” (multi-condition cross attention) 技术，以此生成与输入的图像或文本语义高度一致的视频。而在低自由度视频生成方面，我们引入了“偏置高斯噪声” (Biased Gaussian Noise)，这种方法比传统的完全随机高斯噪声更能有效地保留输入条件的原始内容。

在技术性能方面，我们的方法在 MSR-VTT 视频数据库上获得了最低的帧间视频差异性度量 (Frame Video Distance, FVD)，这一成绩不仅超越了当前的开源方法，而且与业界领先的闭源方法 Gen2 不相上下，显示出了卓越的实用价值和技术优势。

项目地址：https://univg-baidu.github.io/</title>
            <link>https://nitter.cz/op7418/status/1747947995319128497#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1747947995319128497#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 11:44:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卷就完事了，百度也推出了自己的视频生成模型 UniVG。<br />
<br />
这个模型的特点是，区分了高自由度和低自由度两种任务，分别使用了不同的方式生成视频。<br />
根据运动幅度的不同使用不同的方案确实是现在没办法平衡两者的一个办法。<br />
<br />
简介：<br />
视频生成技术，特别是基于“扩散”原理的方法，近来在学术和产业界引起了广泛关注，并已取得显著成就。目前，这一领域的研究和实践主要聚焦于单一目标或单一任务的视频生成，例如根据文本、图片或它们的组合来生成视频。但这样的方法并不能完全满足真实世界多变的应用需求。在实际应用中，用户往往需要更灵活的输入方式，比如单独使用图像或文本，或者将二者结合起来。<br />
<br />
针对这一问题，我们提出了一种新的视频生成系统——“统一模态视频生成系统” (UNIfied-modal Video Generation system)。这个系统能够处理各种文本和图像的组合输入。为了实现这一目标，我们重新定义了视频生成模型中的多项任务，将它们分为“高自由度生成”和“低自由度生成”两大类。在高自由度视频生成方面，我们运用了“多条件交叉注意力” (multi-condition cross attention) 技术，以此生成与输入的图像或文本语义高度一致的视频。而在低自由度视频生成方面，我们引入了“偏置高斯噪声” (Biased Gaussian Noise)，这种方法比传统的完全随机高斯噪声更能有效地保留输入条件的原始内容。<br />
<br />
在技术性能方面，我们的方法在 MSR-VTT 视频数据库上获得了最低的帧间视频差异性度量 (Frame Video Distance, FVD)，这一成绩不仅超越了当前的开源方法，而且与业界领先的闭源方法 Gen2 不相上下，显示出了卓越的实用价值和技术优势。<br />
<br />
项目地址：<a href="https://univg-baidu.github.io/">univg-baidu.github.io/</a></p>
<p><a href="https://nitter.cz/_akhaliq/status/1747860016449679534#m">nitter.cz/_akhaliq/status/1747860016449679534#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDc5NDc2MzAyNzIwOTAxMTIvcHUvaW1nL1pTZnNfZW5YaVNPS1dfdzQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748008690777747706#m</id>
            <title>RT by @op7418: 测试了一下Runway的多运动笔刷功能，真的强大，非常适合画面丰富的场景。

可以完全由自己控制每个物品的运动方向和强度，直接将视频控制带向了新的高度。

AI时代光有强大的模型还不行，还需要有搭配模型交互和工具，这样才能发挥出模型的价值。

如何使用：直接在原来的运动笔刷的位置打开就行，已经替换为了新的多运动笔刷。切换笔刷分类分别设置运动就行。

立刻体验：https://app.runwayml.com/video-tools</title>
            <link>https://nitter.cz/op7418/status/1748008690777747706#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748008690777747706#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 15:45:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试了一下Runway的多运动笔刷功能，真的强大，非常适合画面丰富的场景。<br />
<br />
可以完全由自己控制每个物品的运动方向和强度，直接将视频控制带向了新的高度。<br />
<br />
AI时代光有强大的模型还不行，还需要有搭配模型交互和工具，这样才能发挥出模型的价值。<br />
<br />
如何使用：直接在原来的运动笔刷的位置打开就行，已经替换为了新的多运动笔刷。切换笔刷分类分别设置运动就行。<br />
<br />
立刻体验：<a href="https://app.runwayml.com/video-tools">app.runwayml.com/video-tools</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgwMDg0ODQ2NzM4MzA5MTIvcHUvaW1nL3NLWEI5a2RnbDNYcy1NbVAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1747930933343498259#m</id>
            <title>RT by @op7418: 🧪前几天一个老哥做这种动物时装走秀视频在抖音火了，我看了一下每条视频几乎都有几万攒，百万播放是有的。

刚好前几天看到个类似的就想复原一下这个提示词，整了几个宠物的时尚走秀图片。这个可以做好多主题，比如宠物、十二生肖还能以服装品牌为主题。

主要提示词的内容为”拟人化的{动物名称}”加上“{动物名称}穿着{品牌名}时尚服装”。

提示词：
Anthropomorphic fox, Fashion runway, Full body, fox wearing Dior fashion clothes, anthropomorphic, high-end design style, Cold and beautiful, A slender and slender figure, Milan Fashion Show, Full body, Dynamic capture of runway shows --ar 9:16 --v 6.0 --style raw

#晚安提示词 #Midjourney #Catjourney</title>
            <link>https://nitter.cz/op7418/status/1747930933343498259#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1747930933343498259#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 10:36:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪前几天一个老哥做这种动物时装走秀视频在抖音火了，我看了一下每条视频几乎都有几万攒，百万播放是有的。<br />
<br />
刚好前几天看到个类似的就想复原一下这个提示词，整了几个宠物的时尚走秀图片。这个可以做好多主题，比如宠物、十二生肖还能以服装品牌为主题。<br />
<br />
主要提示词的内容为”拟人化的{动物名称}”加上“{动物名称}穿着{品牌名}时尚服装”。<br />
<br />
提示词：<br />
Anthropomorphic fox, Fashion runway, Full body, fox wearing Dior fashion clothes, anthropomorphic, high-end design style, Cold and beautiful, A slender and slender figure, Milan Fashion Show, Full body, Dynamic capture of runway shows --ar 9:16 --v 6.0 --style raw<br />
<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> <a href="https://nitter.cz/search?q=%23Catjourney">#Catjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VIbDVHR2JNQUFNM0NBLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748020871883825300#m</id>
            <title>UncleLee做的一个电商图片AI工具，功能还挺全的，AI换装、换背景、抠图、更换服装颜色，该有的都有。有需求的可以试试。</title>
            <link>https://nitter.cz/op7418/status/1748020871883825300#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748020871883825300#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 16:33:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>UncleLee做的一个电商图片AI工具，功能还挺全的，AI换装、换背景、抠图、更换服装颜色，该有的都有。有需求的可以试试。</p>
<p><a href="https://nitter.cz/UncleLee_404/status/1748018468564439376#m">nitter.cz/UncleLee_404/status/1748018468564439376#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748012698913439775#m</id>
            <title>R to @op7418: 怪不得把百面千相砍了，那个哪有这玩意赚钱。</title>
            <link>https://nitter.cz/op7418/status/1748012698913439775#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748012698913439775#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 16:01:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>怪不得把百面千相砍了，那个哪有这玩意赚钱。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>