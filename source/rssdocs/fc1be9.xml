<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749418314705846764#m</id>
            <title>最近发售短短三天就达到 400 万销量和 120 万同时在线的游戏幻兽帕鲁，使用 AI 探索和重绘宝可梦来设计游戏内的帕鲁宠物。</title>
            <link>https://nitter.cz/op7418/status/1749418314705846764#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749418314705846764#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 13:06:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近发售短短三天就达到 400 万销量和 120 万同时在线的游戏幻兽帕鲁，使用 AI 探索和重绘宝可梦来设计游戏内的帕鲁宠物。</p>
<p><a href="https://nitter.cz/invert_x/status/1748495038898942380#m">nitter.cz/invert_x/status/1748495038898942380#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1749375379901669564#m</id>
            <title>RT by @op7418: 零一万物发布多模态模型 Yi-VL
额外引入视觉模块实现的多模态，分 6B 和 34B 两个版本，算不上新鲜，不过 Yi 的中文能力不错，需要多模态的情形可以试试。
Yi-VL-6B：https://huggingface.co/01-ai/Yi-VL-6B
Yi-VL-34B：https://huggingface.co/01-ai/Yi-VL-34B</title>
            <link>https://nitter.cz/Gorden_Sun/status/1749375379901669564#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1749375379901669564#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 10:16:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>零一万物发布多模态模型 Yi-VL<br />
额外引入视觉模块实现的多模态，分 6B 和 34B 两个版本，算不上新鲜，不过 Yi 的中文能力不错，需要多模态的情形可以试试。<br />
Yi-VL-6B：<a href="https://huggingface.co/01-ai/Yi-VL-6B">huggingface.co/01-ai/Yi-VL-6…</a><br />
Yi-VL-34B：<a href="https://huggingface.co/01-ai/Yi-VL-34B">huggingface.co/01-ai/Yi-VL-3…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0OTI3OTQ3NTYxODE4MTEyMS85SmhNeEVMbD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749388995799445971#m</id>
            <title>尝试了一下可以对 SVD 进行精细化控制的DragNUWA，确实很爽，SVD 的模型素质加上可控性完全可以吊打现在所有的视频生成模型。

没中不足还是现在的DragNUWA帧数和分辨率太低了，如果可以达到 SVD-XT 那个水平就好了。希望可以搞快点。</title>
            <link>https://nitter.cz/op7418/status/1749388995799445971#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749388995799445971#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 11:10:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>尝试了一下可以对 SVD 进行精细化控制的DragNUWA，确实很爽，SVD 的模型素质加上可控性完全可以吊打现在所有的视频生成模型。<br />
<br />
没中不足还是现在的DragNUWA帧数和分辨率太低了，如果可以达到 SVD-XT 那个水平就好了。希望可以搞快点。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkzODgwMjE0NzEwOTY4MzMvcHUvaW1nL1N4TWZoZjZuOWI0aGR6TDAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749372641046339938#m</id>
            <title>发现了一篇介绍 SD 不同采样器区别的好文章，顺手翻译了一下。

详细介绍和对比了不同采样器的区别和原理，同时还介绍了不同的采样器之间的继承关系。最后给出了一些采样器选择建议。

如果你之前也不是很了解 SD 中每个采样器之间的区别，不知道如何选择的话可以看一下。

内容介绍：
有些算法能够迅速收敛，非常适用于快速验证创意和想法。而其他一些算法可能需要更长的时间或更多的步骤才能收敛，但它们通常能够提供更高质量的结果。还有一些算法由于没有设定极限，因此永远不会收敛，这样就为创新和创造性提供了更多空间。

通过这篇文章，你将能够更好地理解这些术语及不同方法的应用场景，而无需深入探讨过于复杂的技术细节。

原文及翻译：https://quail.ink/op7418/p/science-popularization-stable-diffusion-sampler-operation-guide</title>
            <link>https://nitter.cz/op7418/status/1749372641046339938#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749372641046339938#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 10:05:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发现了一篇介绍 SD 不同采样器区别的好文章，顺手翻译了一下。<br />
<br />
详细介绍和对比了不同采样器的区别和原理，同时还介绍了不同的采样器之间的继承关系。最后给出了一些采样器选择建议。<br />
<br />
如果你之前也不是很了解 SD 中每个采样器之间的区别，不知道如何选择的话可以看一下。<br />
<br />
内容介绍：<br />
有些算法能够迅速收敛，非常适用于快速验证创意和想法。而其他一些算法可能需要更长的时间或更多的步骤才能收敛，但它们通常能够提供更高质量的结果。还有一些算法由于没有设定极限，因此永远不会收敛，这样就为创新和创造性提供了更多空间。<br />
<br />
通过这篇文章，你将能够更好地理解这些术语及不同方法的应用场景，而无需深入探讨过于复杂的技术细节。<br />
<br />
原文及翻译：<a href="https://quail.ink/op7418/p/science-popularization-stable-diffusion-sampler-operation-guide">quail.ink/op7418/p/science-p…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjRXFkR2JjQUFZRDdnLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749320420782964923#m</id>
            <title>Adobe发布了一个可以将视频主体分割后更换背景的模型 ActAnywhere，这个对视频制作挺有用的。

主要的能力是把原视频主体分割后加上一张静态图片从而生成新的视频，并保证原有视频内容和图片的融合度。

详细介绍：
我们推出了 ActAnywhere，这是一个能够自动完成这一过程的生成模型，它省去了过去那些繁复的手工操作。

我们的模型借助了强大的大规模视频扩散模型（large-scale video diffusion models）技术，并专门为此任务进行了定制。ActAnywhere 以一系列的前景主体分割图作为输入，并以描述所需场景的图片作为条件，从而创造出一个既真实又连贯，且前景与背景交互自然的视频，同时忠实于设定的条件帧。

我们在一个涵盖丰富的人类与场景互动视频的大型数据集上训练了我们的模型。

经过广泛的评估，我们的模型展现出了卓越的性能，远远超过了基准水平。更令人兴奋的是，我们还证明了 ActAnywhere 能够适应多种不同的、甚至是偏离常规分布的样本，包括非人类主体。

项目地址：https://actanywhere.github.io/</title>
            <link>https://nitter.cz/op7418/status/1749320420782964923#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749320420782964923#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:37:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Adobe发布了一个可以将视频主体分割后更换背景的模型 ActAnywhere，这个对视频制作挺有用的。<br />
<br />
主要的能力是把原视频主体分割后加上一张静态图片从而生成新的视频，并保证原有视频内容和图片的融合度。<br />
<br />
详细介绍：<br />
我们推出了 ActAnywhere，这是一个能够自动完成这一过程的生成模型，它省去了过去那些繁复的手工操作。<br />
<br />
我们的模型借助了强大的大规模视频扩散模型（large-scale video diffusion models）技术，并专门为此任务进行了定制。ActAnywhere 以一系列的前景主体分割图作为输入，并以描述所需场景的图片作为条件，从而创造出一个既真实又连贯，且前景与背景交互自然的视频，同时忠实于设定的条件帧。<br />
<br />
我们在一个涵盖丰富的人类与场景互动视频的大型数据集上训练了我们的模型。<br />
<br />
经过广泛的评估，我们的模型展现出了卓越的性能，远远超过了基准水平。更令人兴奋的是，我们还证明了 ActAnywhere 能够适应多种不同的、甚至是偏离常规分布的样本，包括非人类主体。<br />
<br />
项目地址：<a href="https://actanywhere.github.io/">actanywhere.github.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkzMTkzNzc2MDQ5NzY2NDAvcHUvaW1nL0prdnBfMGRVaGVHbVVsWTkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749313797914075535#m</id>
            <title>devv 上不了的朋友可以加到规则里试试</title>
            <link>https://nitter.cz/op7418/status/1749313797914075535#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749313797914075535#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:11:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>devv 上不了的朋友可以加到规则里试试</p>
<p><a href="https://nitter.cz/Tisoga/status/1749279312111640647#m">nitter.cz/Tisoga/status/1749279312111640647#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749274754773864648#m</id>
            <title>傅盛的猎户星空发布了Orion-14B系列 LLM 模型，模型页面的测试挺全面的，主要特点有：

模型整体多语言能力强，比如日语和韩语。
支持超过 200k 的上下文。
量化版本模型大小缩小70%，推理速度提升30%，性能损失小于1%。

Orion-14B系列一共包含了 7 个不同的模型：
基座模型、对话模型、长上下文模型、 RAG 模型、插件模型、两个量化模型。

模型介绍页面：https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4/blob/main/README_zh.md</title>
            <link>https://nitter.cz/op7418/status/1749274754773864648#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749274754773864648#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 03:36:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>傅盛的猎户星空发布了Orion-14B系列 LLM 模型，模型页面的测试挺全面的，主要特点有：<br />
<br />
模型整体多语言能力强，比如日语和韩语。<br />
支持超过 200k 的上下文。<br />
量化版本模型大小缩小70%，推理速度提升30%，性能损失小于1%。<br />
<br />
Orion-14B系列一共包含了 7 个不同的模型：<br />
基座模型、对话模型、长上下文模型、 RAG 模型、插件模型、两个量化模型。<br />
<br />
模型介绍页面：<a href="https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4/blob/main/README_zh.md">huggingface.co/OrionStarAI/O…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VhcjdJZGJrQUF3djVwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749104495748075653#m</id>
            <title>RT by @op7418: 加州大学伯克利分校的全栈深度学习课程。包括深度学习基础到模型训练和部署的所有流程。
https://www.youtube.com/playlist?list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv</title>
            <link>https://nitter.cz/op7418/status/1749104495748075653#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749104495748075653#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 16:19:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>加州大学伯克利分校的全栈深度学习课程。包括深度学习基础到模型训练和部署的所有流程。<br />
<a href="https://www.youtube.com/playlist?list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv">youtube.com/playlist?list=PL…</a></p>
<p><a href="https://nitter.cz/DanKornas/status/1749056779185504325#m">nitter.cz/DanKornas/status/1749056779185504325#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0ODc0MjcwMjk2NTkxNTY0OC9BRTBVdEItbj9mb3JtYXQ9anBnJm5hbWU9MjgweDI4MF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749051648364990576#m</id>
            <title>RT by @op7418: 看到indigo这条下了一个研究了一下。Studio Photo这个有点意思，本质上还是妙鸭那套Lora方案，有想复刻的可以参考EasyPhoto这个开源项目。

不过他们很专注主打这种传记风格照片。非常适合在海外传播，收费也很离谱一个Lora 加上30张照片20美元。

EasyPhoto：https://github.com/aigc-apps/sd-webui-EasyPhoto</title>
            <link>https://nitter.cz/op7418/status/1749051648364990576#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749051648364990576#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 12:49:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看到indigo这条下了一个研究了一下。Studio Photo这个有点意思，本质上还是妙鸭那套Lora方案，有想复刻的可以参考EasyPhoto这个开源项目。<br />
<br />
不过他们很专注主打这种传记风格照片。非常适合在海外传播，收费也很离谱一个Lora 加上30张照片20美元。<br />
<br />
EasyPhoto：<a href="https://github.com/aigc-apps/sd-webui-EasyPhoto">github.com/aigc-apps/sd-webu…</a></p>
<p><a href="https://nitter.cz/indigo11/status/1748880863881052472#m">nitter.cz/indigo11/status/1748880863881052472#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VYZ0psQWFBQUFoRC1ILmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749255265118216463#m</id>
            <title>zho 用 GragNUWA 复刻的 runway 多运动笔刷效果，开原生态真是啥都能复刻。</title>
            <link>https://nitter.cz/op7418/status/1749255265118216463#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749255265118216463#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 02:18:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>zho 用 GragNUWA 复刻的 runway 多运动笔刷效果，开原生态真是啥都能复刻。</p>
<p><a href="https://nitter.cz/ZHOZHO672070/status/1749003377810522416#m">nitter.cz/ZHOZHO672070/status/1749003377810522416#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748827229109334363#m</id>
            <title>RT by @op7418: 昨天推荐的这篇《AlphaCodium：引领代码生成新境界，从提示工程到流程工程》里面提到了 6 个使用LLM代码生成的最佳实践：

1. 使用 YAML 结构输出而不是 JSON 格式输出

有两个原因：
1) YAML 格式容错率更高，JSON 很容易导致格式错误不能解析
2) YAML 格式的内容消耗的 Token 更少

2. 要点列表（Bullet points）分析 

当让大语言模型 (LLM) 分析问题时，通常以要点列表（Bullet points）格式要求输出会获得更好的结果。要点促进了对问题的深入理解，并迫使模型将输出划分为逻辑上的语义区域，从而提高了结果的质量。例如，以要点自我反思问题（见图 2），每个要点代表了对问题不同部分的语义理解——一般描述、目标与规则、输入结构、输出结构。

3. 大语言模型在生成模块化代码方面更加出色

让大语言模型（LLM）去编写一个长篇的单个函数时，常常会遇到问题：代码中经常出现错误或逻辑漏洞。更严重的是，这种庞大而单一的代码块会影响到后续的迭代修复工作。即便提供了错误信息，模型也很难准确地定位和解决问题。但如果我们明确指导模型：“把生成的代码分割成多个小的子功能模块，并给它们起上有意义的名称”，结果会好得多，生成的代码错误更少，且在迭代修复的阶段有更高的成功率。

部分Prompt参考图3

4. 灵活决策和双重验证

大语言模型在处理那些需要深思熟虑、合理推断和做出严肃、非常规决策的代码任务时，往往会遇到困难。例如，在生成问题的附加测试时，模型生成的测试常常存在错误。

为了解决这个问题，可以引入了双重验证的过程。在这个过程中，模型在生成了初始输出之后，会被要求再次生成相同的输出，并在必要时进行修正。比如，模型在接收到它自己生成的 AI 测试作为输入后，需要重新生成这些测试，并及时纠正其中的错误（如果有的话）。这种双重验证的步骤，不仅促使模型进行批判性思考和推理，而且比直接提出“这个测试正确吗？”这样的是/否问题更为有效。

5. 延迟做决策，避免直接提问，给予探索空间

当我们直接向模型询问复杂问题时，经常会得到错误或不切实际的答案。因此，我们采取了类似 Karpathy 在下面的推文中所述的方法，逐步积累数据，从简单任务逐渐过渡到复杂任务：

- 首先从最简单的任务开始，即对问题进行自我反思和关于公开测试用例的推理。
- 然后转向生成附加的 AI 测试和可能的问题解决方案。
- 只有在得到模型对上述任务的回答后，我们才进入实际的代码生成和运行修复的迭代过程。

再比如，不是选择一个单一的算法解决方案，而是评估并排序多个可能的解决方案，优先考虑排名靠前的方案进行初始代码编写。由于模型可能会出错，我们更倾向于避免做出不可逆的决定，而是留出空间进行探索，以及尝试不同可能解决方案的代码迭代。

6. 流程导向的监督方式

在解决复杂问题时，不寄希望于一步解决问题，而是设计一个科学的流程，在流程的每一步中逐步积累数据，再每一个阶段都加入新的数据。

以文中解决 CodeContests 编程竞赛问题为例，设计了一个两个阶段的若干步骤的流程，每一个步骤都会引入新的数据，比如说第一步是对题目反思得到反思后的数据，第二步是分析公开测试用例得到测相关的数据，第三步生成可能解决方案得到解决方案的数据等等。

对于每一步的数据，采用验证、选择等方式来确保数据的质量和准确，每一步都是下一步的基础。但即使如此也无法保证每一步数据的正确性，所以在第二个阶段还引入了迭代的模式，这样在遇到数据错误，可以回到前面的步骤对数据进行修正。

以上就是用大语言模型生成代码的 6 个最佳实践，最后简单总结以下：
1. YAML 格式化输出要求：
模型需要能够以 YAML（一种数据表示格式）的方式输出数据，这种输出应与 Pydantic（一种 Python 数据模型库）所定义的类结构相匹配。

2. 逻辑性强的语义要点分析：
鼓励使用 YAML 格式来组织和分析关键信息，通过这种方式可以更加逻辑清晰地划分内容段落，有助于深入理解复杂的概念。

3. 编写模块化代码的推荐：
推荐将代码分解成多个小型的子功能模块，并为每个模块赋予清晰、具有描述性的名称，这样不仅使代码更易于管理，也更便于理解其功能。

4. 灵活决策与双重验证：
当模型生成了一个输出后，再次让模型生成同样的输出，同时在必要时对其进行修正。

5. 保留探索的可能性：
考虑到模型可能会犯错，应避免作出不可逆转的决策，并为寻找多种可能的解决方案提供空间。

6. 流程导向的监督方式：
支持逐步积累数据的流动方式，并在流程的不同阶段考虑加入新的数据。

完整译文参考：https://baoyu.io/translations/prompt-engineering/alphacodium-state-of-the-art-code-generation-for-code-contests</title>
            <link>https://nitter.cz/dotey/status/1748827229109334363#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748827229109334363#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 21:57:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨天推荐的这篇《AlphaCodium：引领代码生成新境界，从提示工程到流程工程》里面提到了 6 个使用LLM代码生成的最佳实践：<br />
<br />
1. 使用 YAML 结构输出而不是 JSON 格式输出<br />
<br />
有两个原因：<br />
1) YAML 格式容错率更高，JSON 很容易导致格式错误不能解析<br />
2) YAML 格式的内容消耗的 Token 更少<br />
<br />
2. 要点列表（Bullet points）分析 <br />
<br />
当让大语言模型 (LLM) 分析问题时，通常以要点列表（Bullet points）格式要求输出会获得更好的结果。要点促进了对问题的深入理解，并迫使模型将输出划分为逻辑上的语义区域，从而提高了结果的质量。例如，以要点自我反思问题（见图 2），每个要点代表了对问题不同部分的语义理解——一般描述、目标与规则、输入结构、输出结构。<br />
<br />
3. 大语言模型在生成模块化代码方面更加出色<br />
<br />
让大语言模型（LLM）去编写一个长篇的单个函数时，常常会遇到问题：代码中经常出现错误或逻辑漏洞。更严重的是，这种庞大而单一的代码块会影响到后续的迭代修复工作。即便提供了错误信息，模型也很难准确地定位和解决问题。但如果我们明确指导模型：“把生成的代码分割成多个小的子功能模块，并给它们起上有意义的名称”，结果会好得多，生成的代码错误更少，且在迭代修复的阶段有更高的成功率。<br />
<br />
部分Prompt参考图3<br />
<br />
4. 灵活决策和双重验证<br />
<br />
大语言模型在处理那些需要深思熟虑、合理推断和做出严肃、非常规决策的代码任务时，往往会遇到困难。例如，在生成问题的附加测试时，模型生成的测试常常存在错误。<br />
<br />
为了解决这个问题，可以引入了双重验证的过程。在这个过程中，模型在生成了初始输出之后，会被要求再次生成相同的输出，并在必要时进行修正。比如，模型在接收到它自己生成的 AI 测试作为输入后，需要重新生成这些测试，并及时纠正其中的错误（如果有的话）。这种双重验证的步骤，不仅促使模型进行批判性思考和推理，而且比直接提出“这个测试正确吗？”这样的是/否问题更为有效。<br />
<br />
5. 延迟做决策，避免直接提问，给予探索空间<br />
<br />
当我们直接向模型询问复杂问题时，经常会得到错误或不切实际的答案。因此，我们采取了类似 Karpathy 在下面的推文中所述的方法，逐步积累数据，从简单任务逐渐过渡到复杂任务：<br />
<br />
- 首先从最简单的任务开始，即对问题进行自我反思和关于公开测试用例的推理。<br />
- 然后转向生成附加的 AI 测试和可能的问题解决方案。<br />
- 只有在得到模型对上述任务的回答后，我们才进入实际的代码生成和运行修复的迭代过程。<br />
<br />
再比如，不是选择一个单一的算法解决方案，而是评估并排序多个可能的解决方案，优先考虑排名靠前的方案进行初始代码编写。由于模型可能会出错，我们更倾向于避免做出不可逆的决定，而是留出空间进行探索，以及尝试不同可能解决方案的代码迭代。<br />
<br />
6. 流程导向的监督方式<br />
<br />
在解决复杂问题时，不寄希望于一步解决问题，而是设计一个科学的流程，在流程的每一步中逐步积累数据，再每一个阶段都加入新的数据。<br />
<br />
以文中解决 CodeContests 编程竞赛问题为例，设计了一个两个阶段的若干步骤的流程，每一个步骤都会引入新的数据，比如说第一步是对题目反思得到反思后的数据，第二步是分析公开测试用例得到测相关的数据，第三步生成可能解决方案得到解决方案的数据等等。<br />
<br />
对于每一步的数据，采用验证、选择等方式来确保数据的质量和准确，每一步都是下一步的基础。但即使如此也无法保证每一步数据的正确性，所以在第二个阶段还引入了迭代的模式，这样在遇到数据错误，可以回到前面的步骤对数据进行修正。<br />
<br />
以上就是用大语言模型生成代码的 6 个最佳实践，最后简单总结以下：<br />
1. YAML 格式化输出要求：<br />
模型需要能够以 YAML（一种数据表示格式）的方式输出数据，这种输出应与 Pydantic（一种 Python 数据模型库）所定义的类结构相匹配。<br />
<br />
2. 逻辑性强的语义要点分析：<br />
鼓励使用 YAML 格式来组织和分析关键信息，通过这种方式可以更加逻辑清晰地划分内容段落，有助于深入理解复杂的概念。<br />
<br />
3. 编写模块化代码的推荐：<br />
推荐将代码分解成多个小型的子功能模块，并为每个模块赋予清晰、具有描述性的名称，这样不仅使代码更易于管理，也更便于理解其功能。<br />
<br />
4. 灵活决策与双重验证：<br />
当模型生成了一个输出后，再次让模型生成同样的输出，同时在必要时对其进行修正。<br />
<br />
5. 保留探索的可能性：<br />
考虑到模型可能会犯错，应避免作出不可逆转的决策，并为寻找多种可能的解决方案提供空间。<br />
<br />
6. 流程导向的监督方式：<br />
支持逐步积累数据的流动方式，并在流程的不同阶段考虑加入新的数据。<br />
<br />
完整译文参考：<a href="https://baoyu.io/translations/prompt-engineering/alphacodium-state-of-the-art-code-generation-for-code-contests">baoyu.io/translations/prompt…</a></p>
<p><a href="https://nitter.cz/dotey/status/1748503587682967775#m">nitter.cz/dotey/status/1748503587682967775#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VVVXpJcldvQUFZR3dRLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VVVTFJS1hNQUFpMm41LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VVVTdKTFgwQUV2dXd3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VVVkJxUlc4QUF1SUFjLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749092946664956012#m</id>
            <title>Topaz质量拉满确实猛，这1800花的不亏。</title>
            <link>https://nitter.cz/op7418/status/1749092946664956012#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749092946664956012#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 15:33:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Topaz质量拉满确实猛，这1800花的不亏。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkwOTI3MTc2OTM3MzA4MTYvcHUvaW1nL2stMFBiMjIyWEI4a002ZlMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749088364224229584#m</id>
            <title>orange用最朴素简单的语言教你如何提升模型能力，把这件比较复杂的事情说的非常浅显易懂。</title>
            <link>https://nitter.cz/op7418/status/1749088364224229584#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749088364224229584#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 15:15:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>orange用最朴素简单的语言教你如何提升模型能力，把这件比较复杂的事情说的非常浅显易懂。</p>
<p><a href="https://nitter.cz/oran_ge/status/1749070700353351746#m">nitter.cz/oran_ge/status/1749070700353351746#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748661489555378226#m</id>
            <title>RT by @op7418: SVD新版本的模型？感觉Stability AI要靠这个翻身了。这个清晰度和流畅度，别人还搞鸡毛。</title>
            <link>https://nitter.cz/op7418/status/1748661489555378226#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748661489555378226#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 10:59:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SVD新版本的模型？感觉Stability AI要靠这个翻身了。这个清晰度和流畅度，别人还搞鸡毛。</p>
<p><a href="https://nitter.cz/EMostaque/status/1748405750907457548#m">nitter.cz/EMostaque/status/1748405750907457548#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748681025730081094#m</id>
            <title>RT by @op7418: 最近又刷到了一些Domo AI制作的视频，感觉他们的流程又进行了优化呢，稳定性现在高的离谱，转场和一些特效都可以完美重绘掉。

国内抖音上也看到很多用这个做效果的内容，用来补充和增强氛围挺好的。

突然想起来我还是付费用户所以就去Discord看了一下，发现还更新了两个新的风格，剪纸和油画，就找了两个视频试了。

我自己用Animatediff生成的视频有时也会去Domo AI过一下，转换风格的同时稳定性也会高一些，最后一段是原始视频和重绘视频，前两段是真实视频重绘的。

Domo AI现在依然有免费试用，进入Discord服务器之后，选择一个Use Domo分类下的频道输入/video回车就可以转换视频了。

使用Domo：https://discord.com/invite/BnkYWZr3na</title>
            <link>https://nitter.cz/op7418/status/1748681025730081094#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748681025730081094#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:17:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近又刷到了一些Domo AI制作的视频，感觉他们的流程又进行了优化呢，稳定性现在高的离谱，转场和一些特效都可以完美重绘掉。<br />
<br />
国内抖音上也看到很多用这个做效果的内容，用来补充和增强氛围挺好的。<br />
<br />
突然想起来我还是付费用户所以就去Discord看了一下，发现还更新了两个新的风格，剪纸和油画，就找了两个视频试了。<br />
<br />
我自己用Animatediff生成的视频有时也会去Domo AI过一下，转换风格的同时稳定性也会高一些，最后一段是原始视频和重绘视频，前两段是真实视频重绘的。<br />
<br />
Domo AI现在依然有免费试用，进入Discord服务器之后，选择一个Use Domo分类下的频道输入/video回车就可以转换视频了。<br />
<br />
使用Domo：<a href="https://discord.com/invite/BnkYWZr3na">discord.com/invite/BnkYWZr3n…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg2ODA5NDE3NDc0NDU3NjAvcHUvaW1nL3FBTFpoUTRBTWd3anV3dkcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748688816242855964#m</id>
            <title>RT by @op7418: 🧪Midjourney真是每天都能给我带来惊喜，看到一个玉剑的设定图，就想试试能不能用MJ还原出来，结果真还原出来了，提高风格化之后还有了第二张图。

提示词：
a lotustailed jade sword made in china, in the style of kris knight, luminous 3d objects, nature-inspired art nouveau, flower and nature motifs, cambodian art, balanced symmetry, precisionist style --ar 9:16 --v 6.0

#晚安提示词 #midjourney #catjourney</title>
            <link>https://nitter.cz/op7418/status/1748688816242855964#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748688816242855964#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:47:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪Midjourney真是每天都能给我带来惊喜，看到一个玉剑的设定图，就想试试能不能用MJ还原出来，结果真还原出来了，提高风格化之后还有了第二张图。<br />
<br />
提示词：<br />
a lotustailed jade sword made in china, in the style of kris knight, luminous 3d objects, nature-inspired art nouveau, flower and nature motifs, cambodian art, balanced symmetry, precisionist style --ar 9:16 --v 6.0<br />
<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23midjourney">#midjourney</a> <a href="https://nitter.cz/search?q=%23catjourney">#catjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTWEh4Y2JJQUFnS0x1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTWEh4Y2FrQUFmbGFWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748750293201023145#m</id>
            <title>RT by @op7418: 简单的光学效果混合：
Long exposure trails, Zoom burst, background gradient, orange, green and white background, in the style of dark indigo and amber, curves, light orange and crimson, blurred, Multiple large circular Gaussian blurs, Ambient light, Sketch style, mystical ambiance, wallpaper, Cinematic Lighting --chaos 20 --ar 16:9 --v 6.0 --style raw</title>
            <link>https://nitter.cz/op7418/status/1748750293201023145#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748750293201023145#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 16:52:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>简单的光学效果混合：<br />
Long exposure trails, Zoom burst, background gradient, orange, green and white background, in the style of dark indigo and amber, curves, light orange and crimson, blurred, Multiple large circular Gaussian blurs, Ambient light, Sketch style, mystical ambiance, wallpaper, Cinematic Lighting --chaos 20 --ar 16:9 --v 6.0 --style raw</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VUT3h6dWJNQUFrV1QtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VUTzBfcWFRQUEtclcwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VUTzVqcGFVQUE2eWRjLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VUUEF0amIwQUFfZ1NYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>