<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734513142829555751#m</id>
            <title>开源本地模型的兴起正在取代大规模（且昂贵）的基于云的封闭模型。</title>
            <link>https://nitter.cz/op7418/status/1734513142829555751#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734513142829555751#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:58:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开源本地模型的兴起正在取代大规模（且昂贵）的基于云的封闭模型。</p>
<p><a href="https://nitter.cz/BrianRoemmele/status/1734333713381753165#m">nitter.cz/BrianRoemmele/status/1734333713381753165#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734509545039524199#m</id>
            <title>选对方向真的赚啊。AI研究人员在 OpenAI、Anthropic、Inflection、亚马逊、特斯拉等 大公司的收入，具体包括了基础、奖金、权益等。</title>
            <link>https://nitter.cz/op7418/status/1734509545039524199#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734509545039524199#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:44:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>选对方向真的赚啊。AI研究人员在 OpenAI、Anthropic、Inflection、亚马逊、特斯拉等 大公司的收入，具体包括了基础、奖金、权益等。</p>
<p><a href="https://nitter.cz/chiefaioffice/status/1734329284821672270#m">nitter.cz/chiefaioffice/status/1734329284821672270#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734504939098103960#m</id>
            <title>刚看到腾讯这个视频生成模型AnimateZero，感觉是 Animatediff 的继任者，效果比 Animatediff 好很多。而且可以更现在SD 的生态进行兼容，演示的时候也用的社区 SD 模型。
支持文本生成视频，搭配 Contorlnet 进行视频编辑，多张照片之间的插帧，循环视频生成。下面是具体介绍：

目前的视频生成问题：
黑盒子：生成过程仍然是一个黑盒子。
低效且难以控制：获得满意的结果，需要大量的试错。
域差：受训练过程中使用的视频数据集的领域限制所限。

AnimateZero的解决办法：
解耦：视频生成过程被分解为外观（T2I）和动作（I2V）。
高效可控：与T2V相比，T2I生成更可控和高效，能够在执行I2V生成视频之前获得令人满意的图像。
减轻领域差异问题：T2I模型的领域可以进行微调，以与实际领域对齐，这比调整整个视频模型更高效。

项目地址：https://vvictoryuki.github.io/animatezero.github.io/</title>
            <link>https://nitter.cz/op7418/status/1734504939098103960#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734504939098103960#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:26:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚看到腾讯这个视频生成模型AnimateZero，感觉是 Animatediff 的继任者，效果比 Animatediff 好很多。而且可以更现在SD 的生态进行兼容，演示的时候也用的社区 SD 模型。<br />
支持文本生成视频，搭配 Contorlnet 进行视频编辑，多张照片之间的插帧，循环视频生成。下面是具体介绍：<br />
<br />
目前的视频生成问题：<br />
黑盒子：生成过程仍然是一个黑盒子。<br />
低效且难以控制：获得满意的结果，需要大量的试错。<br />
域差：受训练过程中使用的视频数据集的领域限制所限。<br />
<br />
AnimateZero的解决办法：<br />
解耦：视频生成过程被分解为外观（T2I）和动作（I2V）。<br />
高效可控：与T2V相比，T2I生成更可控和高效，能够在执行I2V生成视频之前获得令人满意的图像。<br />
减轻领域差异问题：T2I模型的领域可以进行微调，以与实际领域对齐，这比调整整个视频模型更高效。<br />
<br />
项目地址：<a href="https://vvictoryuki.github.io/animatezero.github.io/">vvictoryuki.github.io/animat…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1MDM2OTAzOTkyODkzNDQvcHUvaW1nL21XbmV0QzA2aUtHR0c1aTMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734497364474560820#m</id>
            <title>R to @op7418: 作者的推特：
https://x.com/ccloy/status/1734468279123775859?s=20</title>
            <link>https://nitter.cz/op7418/status/1734497364474560820#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734497364474560820#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:56:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者的推特：<br />
<a href="https://x.com/ccloy/status/1734468279123775859?s=20">x.com/ccloy/status/173446827…</a></p>
<p><a href="https://nitter.cz/ccloy/status/1734468279123775859#m">nitter.cz/ccloy/status/1734468279123775859#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734497196953985354#m</id>
            <title>南洋理工发布了一个 AI 视频放大算法 Upscale-A-Video，视频生成真的全方位的卷起来了。下面是演示和介绍：

简介：
Upscale-A-Video的文本引导潜在扩散框架，用于视频放大。该框架通过两个关键机制确保时间上的一致性：在局部上，它将时间层集成到U-Net和VAE-Decoder中，保持短序列的一致性；
在全局上，引入了一个基于流引导的经常性潜在传播模块，通过在整个序列中传播和融合潜在来增强整体视频的稳定性。
由于扩散范式，模型还通过允许文本提示来引导纹理创建和可调噪声水平来平衡恢复和生成，从而在保真度和质量之间实现权衡。

方法：
高级视频使用本地和全局策略处理长视频，以保持时间上的连贯性。它将视频分成片段，并使用具有时间层的U-Net来处理它们，以实现片段内的一致性。在用户指定的全局细化扩散步骤中，使用循环潜在传播模块来增强片段间的一致性。最后，经过微调的VAE-Decoder减少剩余的闪烁伪影，以实现低级一致性。

结果：
广泛的实验表明，Upscale-A-Video在合成和真实世界的基准测试中超过了现有的方法，以及在人工智能生成的视频中展示出令人印象深刻的视觉逼真和时间一致性。

项目地址：https://shangchenzhou.com/projects/upscale-a-video/</title>
            <link>https://nitter.cz/op7418/status/1734497196953985354#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734497196953985354#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:55:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>南洋理工发布了一个 AI 视频放大算法 Upscale-A-Video，视频生成真的全方位的卷起来了。下面是演示和介绍：<br />
<br />
简介：<br />
Upscale-A-Video的文本引导潜在扩散框架，用于视频放大。该框架通过两个关键机制确保时间上的一致性：在局部上，它将时间层集成到U-Net和VAE-Decoder中，保持短序列的一致性；<br />
在全局上，引入了一个基于流引导的经常性潜在传播模块，通过在整个序列中传播和融合潜在来增强整体视频的稳定性。<br />
由于扩散范式，模型还通过允许文本提示来引导纹理创建和可调噪声水平来平衡恢复和生成，从而在保真度和质量之间实现权衡。<br />
<br />
方法：<br />
高级视频使用本地和全局策略处理长视频，以保持时间上的连贯性。它将视频分成片段，并使用具有时间层的U-Net来处理它们，以实现片段内的一致性。在用户指定的全局细化扩散步骤中，使用循环潜在传播模块来增强片段间的一致性。最后，经过微调的VAE-Decoder减少剩余的闪烁伪影，以实现低级一致性。<br />
<br />
结果：<br />
广泛的实验表明，Upscale-A-Video在合成和真实世界的基准测试中超过了现有的方法，以及在人工智能生成的视频中展示出令人印象深刻的视觉逼真和时间一致性。<br />
<br />
项目地址：<a href="https://shangchenzhou.com/projects/upscale-a-video/">shangchenzhou.com/projects/u…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTY0MzU4NTk3OTU5NjkvcHUvaW1nL1ZZLXdpMmk0ZkNPYUVLSGYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734493911786414444#m</id>
            <title>学到了，开发的时候可能需要注意。</title>
            <link>https://nitter.cz/op7418/status/1734493911786414444#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734493911786414444#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:42:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学到了，开发的时候可能需要注意。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492930239524990#m</id>
            <title>R to @op7418: 小教程搞完了，里面有所有需要的内容的链接和操作过程：
https://x.com/op7418/status/1734492326599467291?s=20</title>
            <link>https://nitter.cz/op7418/status/1734492930239524990#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492930239524990#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:38:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小教程搞完了，里面有所有需要的内容的链接和操作过程：<br />
<a href="https://x.com/op7418/status/1734492326599467291?s=20">x.com/op7418/status/17344923…</a></p>
<p><a href="https://nitter.cz/op7418/status/1734492326599467291#m">nitter.cz/op7418/status/1734492326599467291#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492452231491801#m</id>
            <title>R to @op7418: OK 教程到这里就结束了，Ollama的Github 页面还有很多其他的插件和客户端可以玩，可以找感兴趣的试试。</title>
            <link>https://nitter.cz/op7418/status/1734492452231491801#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492452231491801#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OK 教程到这里就结束了，Ollama的Github 页面还有很多其他的插件和客户端可以玩，可以找感兴趣的试试。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492447819100574#m</id>
            <title>R to @op7418: 然后就是使用了，回到你的文档用快捷键 Commad+P 打开命令栏，搜索刚才的命令名称比如Explain selection-mistral，直接回车，就会看到出现了一个笔的图标，稍微等一会就可以看到输出的内容。</title>
            <link>https://nitter.cz/op7418/status/1734492447819100574#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492447819100574#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>然后就是使用了，回到你的文档用快捷键 Commad+P 打开命令栏，搜索刚才的命令名称比如Explain selection-mistral，直接回车，就会看到出现了一个笔的图标，稍微等一会就可以看到输出的内容。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbnJxYWJFQUF6ZlB5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492431146738007#m</id>
            <title>R to @op7418: 安装之后在设置里就会出现Ollama的插件设置，这里可以自定义操作，他这里预置的都是 Llama2 的命令，我们需要自定义成我们的mistral模型。
就是复制他下面的命令，然后在新增命令这里起个别的名字填上就行，比如Explain selection-mistral，然后模型名称那里写上你现在在运行的模型就行，比如mistral。</title>
            <link>https://nitter.cz/op7418/status/1734492431146738007#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492431146738007#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>安装之后在设置里就会出现Ollama的插件设置，这里可以自定义操作，他这里预置的都是 Llama2 的命令，我们需要自定义成我们的mistral模型。<br />
就是复制他下面的命令，然后在新增命令这里起个别的名字填上就行，比如Explain selection-mistral，然后模型名称那里写上你现在在运行的模型就行，比如mistral。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbnFyUWJnQUFCdzNsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492413782278375#m</id>
            <title>R to @op7418: 现在模型已经跑起来了我们需要去 Obsidian 装个Ollama 的插件，进到设置里面，选择第三方插件，浏览社区插件市场，搜索Ollama然后安装就行。</title>
            <link>https://nitter.cz/op7418/status/1734492413782278375#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492413782278375#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>现在模型已经跑起来了我们需要去 Obsidian 装个Ollama 的插件，进到设置里面，选择第三方插件，浏览社区插件市场，搜索Ollama然后安装就行。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbnBucmJNQUFoXzlSLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492395549712520#m</id>
            <title>R to @op7418: 之后就能看到开始有进度条在走了，我们需要等待模型下载完成。
模型下载完成之后就会自动加载，当出现下面的内容的时候就可以开始在终端跟模型对话了。</title>
            <link>https://nitter.cz/op7418/status/1734492395549712520#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492395549712520#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之后就能看到开始有进度条在走了，我们需要等待模型下载完成。<br />
模型下载完成之后就会自动加载，当出现下面的内容的时候就可以开始在终端跟模型对话了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbm5wcWJZQUFhVW5pLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbm9la2JnQUE3cmhCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492361752039504#m</id>
            <title>R to @op7418: 之后我们可以在他的 Github 上找到所有的命令，选择一个你需要的复制那个命令，找到你电脑里终端这个软件，然后粘贴刚才的命令，然后回车。比如：ollama run mistral
Github 地址：https://github.com/jmorganca/ollama</title>
            <link>https://nitter.cz/op7418/status/1734492361752039504#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492361752039504#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之后我们可以在他的 Github 上找到所有的命令，选择一个你需要的复制那个命令，找到你电脑里终端这个软件，然后粘贴刚才的命令，然后回车。比如：ollama run mistral<br />
Github 地址：<a href="https://github.com/jmorganca/ollama">github.com/jmorganca/ollama</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbm1hMmFRQUE2dmdiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492341363458556#m</id>
            <title>R to @op7418: 首先需要去Ollama的网站下载安装包，https://ollama.ai/ 下载完之后直接安装然后打开就行。</title>
            <link>https://nitter.cz/op7418/status/1734492341363458556#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492341363458556#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>首先需要去Ollama的网站下载安装包，<a href="https://ollama.ai/">ollama.ai/</a> 下载完之后直接安装然后打开就行。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJbmxvTWEwQUFBeTl5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492326599467291#m</id>
            <title>写个如何用 Ollama 在 Mac 本地跑 LLM，并且用在 Obsidian 上处理自己的笔记和内容的小教程。视频是具体的演示，我把等待时间剪掉了。
我们开始具体的教程🧵：</title>
            <link>https://nitter.cz/op7418/status/1734492326599467291#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492326599467291#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>写个如何用 Ollama 在 Mac 本地跑 LLM，并且用在 Obsidian 上处理自己的笔记和内容的小教程。视频是具体的演示，我把等待时间剪掉了。<br />
我们开始具体的教程🧵：</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTIyNTkzOTgzMjgzMjAvcHUvaW1nL1RTOTNndlFHLTBvQU5MWmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734472947828703379#m</id>
            <title>R to @op7418: 感觉可以搞一个小教程，我一会发一下</title>
            <link>https://nitter.cz/op7418/status/1734472947828703379#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734472947828703379#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 07:19:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感觉可以搞一个小教程，我一会发一下</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734469157423648991#m</id>
            <title>刚注意到李飞飞团队的这个视频生成模型W.A.L.T，这效果也太好了，感觉比 Pika 1.0 还要好的多。
清晰度和动作都非常好，特别是光剑打斗的那个视频。可惜不开源。
我补充了一些项目页面的演示视频。

项目简介：
方法有两个关键的设计决策。首先，我们使用因果编码器在统一的潜在空间内联合压缩图像和视频，从而实现跨模态的训练和生成。其次，为了提高记忆和训练效率，我们使用专为联合空间和时空生成建模而定制的窗口注意架构。总而言之，这些设计决策使我们能够在已建立的视频（UCF-101 和 Kinetics-600）和图像（ImageNet）生成基准上实现最先进的性能，而无需使用无分类器指导。
最后，我们还训练了三个用于文本到视频生成任务的级联模型，其中包括一个基本潜在视频扩散模型和两个视频超分辨率扩散模型，以每秒 8 帧的速度生成 512 x 896 分辨率的视频。

项目地址：https://walt-video-diffusion.github.io/</title>
            <link>https://nitter.cz/op7418/status/1734469157423648991#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734469157423648991#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 07:04:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚注意到李飞飞团队的这个视频生成模型W.A.L.T，这效果也太好了，感觉比 Pika 1.0 还要好的多。<br />
清晰度和动作都非常好，特别是光剑打斗的那个视频。可惜不开源。<br />
我补充了一些项目页面的演示视频。<br />
<br />
项目简介：<br />
方法有两个关键的设计决策。首先，我们使用因果编码器在统一的潜在空间内联合压缩图像和视频，从而实现跨模态的训练和生成。其次，为了提高记忆和训练效率，我们使用专为联合空间和时空生成建模而定制的窗口注意架构。总而言之，这些设计决策使我们能够在已建立的视频（UCF-101 和 Kinetics-600）和图像（ImageNet）生成基准上实现最先进的性能，而无需使用无分类器指导。<br />
最后，我们还训练了三个用于文本到视频生成任务的级联模型，其中包括一个基本潜在视频扩散模型和两个视频超分辨率扩散模型，以每秒 8 帧的速度生成 512 x 896 分辨率的视频。<br />
<br />
项目地址：<a href="https://walt-video-diffusion.github.io/">walt-video-diffusion.github.…</a></p>
<p><a href="https://nitter.cz/agrimgupta92/status/1734253883076063426#m">nitter.cz/agrimgupta92/status/1734253883076063426#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0NjgyNzY4NDIwMTY3NjkvcHUvaW1nL1BCS0ZHeHFMVmpZeTdEdkcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734465788659253379#m</id>
            <title>为了推动 LLM 更加的开放和开源，推出了 LLM360 计划（看见 360 PTSD 了），他们会开源 LLM 模型训练过程中的所有内容包括训练代码、数据、中间检查点和分析结果，不止是训练权重和推理代码。
目前他们开源了一个英语 7B 模型和一个编码 7B 模型，未来还会开源更大规模的模型。
这种确实很好，可以让想学习的人对整个训练过程的了解更为清晰。
资料这里下载：https://www.llm360.ai/</title>
            <link>https://nitter.cz/op7418/status/1734465788659253379#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734465788659253379#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:50:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>为了推动 LLM 更加的开放和开源，推出了 LLM360 计划（看见 360 PTSD 了），他们会开源 LLM 模型训练过程中的所有内容包括训练代码、数据、中间检查点和分析结果，不止是训练权重和推理代码。<br />
目前他们开源了一个英语 7B 模型和一个编码 7B 模型，未来还会开源更大规模的模型。<br />
这种确实很好，可以让想学习的人对整个训练过程的了解更为清晰。<br />
资料这里下载：<a href="https://www.llm360.ai/">llm360.ai/</a></p>
<p><a href="https://nitter.cz/llm360/status/1734227314773495816#m">nitter.cz/llm360/status/1734227314773495816#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734463448854237262#m</id>
            <title>R to @op7418: 急需Ollama的 Windows 版本，英伟达推理的速度更快，用法也就多了。</title>
            <link>https://nitter.cz/op7418/status/1734463448854237262#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734463448854237262#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:41:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>急需Ollama的 Windows 版本，英伟达推理的速度更快，用法也就多了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734459302130430322#m</id>
            <title>Ollama真的让本地部署 LLM 的成本低了好多啊，今天尝试着部署了一下mistral-7B，非常傻瓜一步到位。
顺便实现了 Obsidian 用本地 LLM 来帮助总结和解释内容，如果硬件好的话本地的 LLM 完成一些基础工作也挺快的，视频加速了 1 倍。
感觉可玩性挺强的，我再试试别的。</title>
            <link>https://nitter.cz/op7418/status/1734459302130430322#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734459302130430322#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:24:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ollama真的让本地部署 LLM 的成本低了好多啊，今天尝试着部署了一下mistral-7B，非常傻瓜一步到位。<br />
顺便实现了 Obsidian 用本地 LLM 来帮助总结和解释内容，如果硬件好的话本地的 LLM 完成一些基础工作也挺快的，视频加速了 1 倍。<br />
感觉可玩性挺强的，我再试试别的。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0NTkyMTU4MTE2NzgyMDkvcHUvaW1nL3pDUV84ODZZUTdXbF92UkouanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>