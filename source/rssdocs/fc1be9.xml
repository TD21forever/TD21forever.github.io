<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749699876492029960#m</id>
            <title>这个开源项目有点意思，可以利用 LLM 让 SD复现 DALL-E3 对于复杂提示词描述的理解能力。

比如下面的几个例子，在增加了这个能力之后的SD对提示词的理解获得了巨大的提升，非常精准的还原出了提示词描述的空间关系和画面细节。

而且项目还支持和 ComtrolNet 一起使用进一步增加对图像的控制能力。

项目介绍：

本文介绍了一种创新的无需额外训练的文本到图像生成与编辑框架——重述、规划和生成（RPG），它利用多模态大语言模型的链式思维推理能力，提升了文本到图像扩散模型的组合能力。

我们使用MLLM作为全局规划器，将复杂图像的生成过程拆分为子区域内的多个简单任务。我们还引入了互补区域扩散技术，以实现区域内的组合式生成。

此外，RPG框架通过闭环方式整合了文本引导的图像生成和编辑，进一步提升了模型的泛化能力。

大量实验证明，RPG在处理多类别对象组合和文本与图像语义对齐方面，表现优于当前顶尖的文本到图像扩散模型，如DALL-E 3和SDXL。

论文地址：https://arxiv.org/abs/2401.11708</title>
            <link>https://nitter.cz/op7418/status/1749699876492029960#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749699876492029960#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 07:45:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个开源项目有点意思，可以利用 LLM 让 SD复现 DALL-E3 对于复杂提示词描述的理解能力。<br />
<br />
比如下面的几个例子，在增加了这个能力之后的SD对提示词的理解获得了巨大的提升，非常精准的还原出了提示词描述的空间关系和画面细节。<br />
<br />
而且项目还支持和 ComtrolNet 一起使用进一步增加对图像的控制能力。<br />
<br />
项目介绍：<br />
<br />
本文介绍了一种创新的无需额外训练的文本到图像生成与编辑框架——重述、规划和生成（RPG），它利用多模态大语言模型的链式思维推理能力，提升了文本到图像扩散模型的组合能力。<br />
<br />
我们使用MLLM作为全局规划器，将复杂图像的生成过程拆分为子区域内的多个简单任务。我们还引入了互补区域扩散技术，以实现区域内的组合式生成。<br />
<br />
此外，RPG框架通过闭环方式整合了文本引导的图像生成和编辑，进一步提升了模型的泛化能力。<br />
<br />
大量实验证明，RPG在处理多类别对象组合和文本与图像语义对齐方面，表现优于当前顶尖的文本到图像扩散模型，如DALL-E 3和SDXL。<br />
<br />
论文地址：<a href="https://arxiv.org/abs/2401.11708">arxiv.org/abs/2401.11708</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VndHZzVWJVQUFnWi1oLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VndDRvbWJNQUF2cXZtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VndDc5QWJVQUE0OGFiLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VndF9DM2JzQUFHNEFYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749372641046339938#m</id>
            <title>RT by @op7418: 发现了一篇介绍 SD 不同采样器区别的好文章，顺手翻译了一下。

详细介绍和对比了不同采样器的区别和原理，同时还介绍了不同的采样器之间的继承关系。最后给出了一些采样器选择建议。

如果你之前也不是很了解 SD 中每个采样器之间的区别，不知道如何选择的话可以看一下。

内容介绍：
有些算法能够迅速收敛，非常适用于快速验证创意和想法。而其他一些算法可能需要更长的时间或更多的步骤才能收敛，但它们通常能够提供更高质量的结果。还有一些算法由于没有设定极限，因此永远不会收敛，这样就为创新和创造性提供了更多空间。

通过这篇文章，你将能够更好地理解这些术语及不同方法的应用场景，而无需深入探讨过于复杂的技术细节。

原文及翻译：https://quail.ink/op7418/p/science-popularization-stable-diffusion-sampler-operation-guide</title>
            <link>https://nitter.cz/op7418/status/1749372641046339938#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749372641046339938#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 10:05:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发现了一篇介绍 SD 不同采样器区别的好文章，顺手翻译了一下。<br />
<br />
详细介绍和对比了不同采样器的区别和原理，同时还介绍了不同的采样器之间的继承关系。最后给出了一些采样器选择建议。<br />
<br />
如果你之前也不是很了解 SD 中每个采样器之间的区别，不知道如何选择的话可以看一下。<br />
<br />
内容介绍：<br />
有些算法能够迅速收敛，非常适用于快速验证创意和想法。而其他一些算法可能需要更长的时间或更多的步骤才能收敛，但它们通常能够提供更高质量的结果。还有一些算法由于没有设定极限，因此永远不会收敛，这样就为创新和创造性提供了更多空间。<br />
<br />
通过这篇文章，你将能够更好地理解这些术语及不同方法的应用场景，而无需深入探讨过于复杂的技术细节。<br />
<br />
原文及翻译：<a href="https://quail.ink/op7418/p/science-popularization-stable-diffusion-sampler-operation-guide">quail.ink/op7418/p/science-p…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjRXFkR2JjQUFZRDdnLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749388995799445971#m</id>
            <title>RT by @op7418: 尝试了一下可以对 SVD 进行精细化控制的DragNUWA，确实很爽，SVD 的模型素质加上可控性完全可以吊打现在所有的视频生成模型。

没中不足还是现在的DragNUWA帧数和分辨率太低了，如果可以达到 SVD-XT 那个水平就好了。希望可以搞快点。</title>
            <link>https://nitter.cz/op7418/status/1749388995799445971#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749388995799445971#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 11:10:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>尝试了一下可以对 SVD 进行精细化控制的DragNUWA，确实很爽，SVD 的模型素质加上可控性完全可以吊打现在所有的视频生成模型。<br />
<br />
没中不足还是现在的DragNUWA帧数和分辨率太低了，如果可以达到 SVD-XT 那个水平就好了。希望可以搞快点。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkzODgwMjE0NzEwOTY4MzMvcHUvaW1nL1N4TWZoZjZuOWI0aGR6TDAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749424594170392982#m</id>
            <title>RT by @op7418: 前几天发布的另一个类似IPadapter的人脸身份保持项目InstantID的模型也发布了。

这个项目与PhotoMaker和IP-Adapter-FaceID相比实现了更好的保真度并保留了良好的文本可编辑性（面孔和样式更好地融合）。

模型下载：https://huggingface.co/InstantX/InstantID</title>
            <link>https://nitter.cz/op7418/status/1749424594170392982#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749424594170392982#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 13:31:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>前几天发布的另一个类似IPadapter的人脸身份保持项目InstantID的模型也发布了。<br />
<br />
这个项目与PhotoMaker和IP-Adapter-FaceID相比实现了更好的保真度并保留了良好的文本可编辑性（面孔和样式更好地融合）。<br />
<br />
模型下载：<a href="https://huggingface.co/InstantX/InstantID">huggingface.co/InstantX/Inst…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjenlWdmJVQUFJOVJ6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749476128203645328#m</id>
            <title>RT by @op7418: PRISMA一个计算摄影流程，可以将视频进行多层次的分析处理。转化为多维数据，这些数据可以用于 3D 重建或实时的后期处理操作。宣传视频也太酷了。

如果对象是视频的话，最后一步将尝试执行稀疏重建，该重建可用于 NeRF（如 NVidia 的 Instant-ngp）或高斯泼溅训练。

项目地址：https://github.com/patriciogonzalezvivo/prisma</title>
            <link>https://nitter.cz/op7418/status/1749476128203645328#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749476128203645328#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 16:56:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PRISMA一个计算摄影流程，可以将视频进行多层次的分析处理。转化为多维数据，这些数据可以用于 3D 重建或实时的后期处理操作。宣传视频也太酷了。<br />
<br />
如果对象是视频的话，最后一步将尝试执行稀疏重建，该重建可用于 NeRF（如 NVidia 的 Instant-ngp）或高斯泼溅训练。<br />
<br />
项目地址：<a href="https://github.com/patriciogonzalezvivo/prisma">github.com/patriciogonzalezv…</a></p>
<p><a href="https://nitter.cz/patriciogv/status/1749420539477672079#m">nitter.cz/patriciogv/status/1749420539477672079#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749482949970653286#m</id>
            <title>RT by @op7418: 非常直观和有意思的测试，让你清楚地知道 runway运动笔刷每一个参数不同的值表现在视频上大概运动多远。</title>
            <link>https://nitter.cz/op7418/status/1749482949970653286#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749482949970653286#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 17:23:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>非常直观和有意思的测试，让你清楚地知道 runway运动笔刷每一个参数不同的值表现在视频上大概运动多远。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk0NDAxNTAxMzQ3NjM1MjEvcHUvaW1nL3pmbkk5TXc1ZU96Z2g3bWUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749477053127365023#m</id>
            <title>HayGen这个实时进行这么高质量的数字人渲染也太离谱了，不过所有的生成时间都被剪辑掉了，估计等待时间短不了。</title>
            <link>https://nitter.cz/op7418/status/1749477053127365023#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749477053127365023#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 17:00:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HayGen这个实时进行这么高质量的数字人渲染也太离谱了，不过所有的生成时间都被剪辑掉了，估计等待时间短不了。</p>
<p><a href="https://nitter.cz/CoffeeVectors/status/1749308520636231824#m">nitter.cz/CoffeeVectors/status/1749308520636231824#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749320420782964923#m</id>
            <title>RT by @op7418: Adobe发布了一个可以将视频主体分割后更换背景的模型 ActAnywhere，这个对视频制作挺有用的。

主要的能力是把原视频主体分割后加上一张静态图片从而生成新的视频，并保证原有视频内容和图片的融合度。

详细介绍：
我们推出了 ActAnywhere，这是一个能够自动完成这一过程的生成模型，它省去了过去那些繁复的手工操作。

我们的模型借助了强大的大规模视频扩散模型（large-scale video diffusion models）技术，并专门为此任务进行了定制。ActAnywhere 以一系列的前景主体分割图作为输入，并以描述所需场景的图片作为条件，从而创造出一个既真实又连贯，且前景与背景交互自然的视频，同时忠实于设定的条件帧。

我们在一个涵盖丰富的人类与场景互动视频的大型数据集上训练了我们的模型。

经过广泛的评估，我们的模型展现出了卓越的性能，远远超过了基准水平。更令人兴奋的是，我们还证明了 ActAnywhere 能够适应多种不同的、甚至是偏离常规分布的样本，包括非人类主体。

项目地址：https://actanywhere.github.io/</title>
            <link>https://nitter.cz/op7418/status/1749320420782964923#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749320420782964923#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:37:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Adobe发布了一个可以将视频主体分割后更换背景的模型 ActAnywhere，这个对视频制作挺有用的。<br />
<br />
主要的能力是把原视频主体分割后加上一张静态图片从而生成新的视频，并保证原有视频内容和图片的融合度。<br />
<br />
详细介绍：<br />
我们推出了 ActAnywhere，这是一个能够自动完成这一过程的生成模型，它省去了过去那些繁复的手工操作。<br />
<br />
我们的模型借助了强大的大规模视频扩散模型（large-scale video diffusion models）技术，并专门为此任务进行了定制。ActAnywhere 以一系列的前景主体分割图作为输入，并以描述所需场景的图片作为条件，从而创造出一个既真实又连贯，且前景与背景交互自然的视频，同时忠实于设定的条件帧。<br />
<br />
我们在一个涵盖丰富的人类与场景互动视频的大型数据集上训练了我们的模型。<br />
<br />
经过广泛的评估，我们的模型展现出了卓越的性能，远远超过了基准水平。更令人兴奋的是，我们还证明了 ActAnywhere 能够适应多种不同的、甚至是偏离常规分布的样本，包括非人类主体。<br />
<br />
项目地址：<a href="https://actanywhere.github.io/">actanywhere.github.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkzMTkzNzc2MDQ5NzY2NDAvcHUvaW1nL0prdnBfMGRVaGVHbVVsWTkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749461559477403870#m</id>
            <title>零一万物推出开源视觉语言模型Yi-VL-34B</title>
            <link>https://nitter.cz/op7418/status/1749461559477403870#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749461559477403870#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 15:58:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>零一万物推出开源视觉语言模型Yi-VL-34B</p>
<p><a href="https://nitter.cz/01AI_Yi/status/1749446827294769540#m">nitter.cz/01AI_Yi/status/1749446827294769540#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749092946664956012#m</id>
            <title>RT by @op7418: Topaz质量拉满确实猛，这1800花的不亏。</title>
            <link>https://nitter.cz/op7418/status/1749092946664956012#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749092946664956012#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 15:33:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Topaz质量拉满确实猛，这1800花的不亏。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkwOTI3MTc2OTM3MzA4MTYvcHUvaW1nL2stMFBiMjIyWEI4a002ZlMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749431051540111838#m</id>
            <title>用Colab快速尝试了一下InstantID。
对模版图像的语义还原较差，但是人脸保持的还行，模版图像的保持看来还得搞一个IPadapter。</title>
            <link>https://nitter.cz/op7418/status/1749431051540111838#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749431051540111838#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 13:57:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>用Colab快速尝试了一下InstantID。<br />
对模版图像的语义还原较差，但是人脸保持的还行，模版图像的保持看来还得搞一个IPadapter。</p>
<p><a href="https://nitter.cz/op7418/status/1749424594170392982#m">nitter.cz/op7418/status/1749424594170392982#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjNTBsWGFJQUFhelJrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjNkwtSWE4QUFiR3hXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749418314705846764#m</id>
            <title>最近发售短短三天就达到 400 万销量和 120 万同时在线的游戏幻兽帕鲁，使用 AI 探索和重绘宝可梦来设计游戏内的帕鲁宠物。</title>
            <link>https://nitter.cz/op7418/status/1749418314705846764#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749418314705846764#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 13:06:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近发售短短三天就达到 400 万销量和 120 万同时在线的游戏幻兽帕鲁，使用 AI 探索和重绘宝可梦来设计游戏内的帕鲁宠物。</p>
<p><a href="https://nitter.cz/invert_x/status/1748495038898942380#m">nitter.cz/invert_x/status/1748495038898942380#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1749375379901669564#m</id>
            <title>RT by @op7418: 零一万物发布多模态模型 Yi-VL
额外引入视觉模块实现的多模态，分 6B 和 34B 两个版本，算不上新鲜，不过 Yi 的中文能力不错，需要多模态的情形可以试试。
Yi-VL-6B：https://huggingface.co/01-ai/Yi-VL-6B
Yi-VL-34B：https://huggingface.co/01-ai/Yi-VL-34B</title>
            <link>https://nitter.cz/Gorden_Sun/status/1749375379901669564#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1749375379901669564#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 10:16:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>零一万物发布多模态模型 Yi-VL<br />
额外引入视觉模块实现的多模态，分 6B 和 34B 两个版本，算不上新鲜，不过 Yi 的中文能力不错，需要多模态的情形可以试试。<br />
Yi-VL-6B：<a href="https://huggingface.co/01-ai/Yi-VL-6B">huggingface.co/01-ai/Yi-VL-6…</a><br />
Yi-VL-34B：<a href="https://huggingface.co/01-ai/Yi-VL-34B">huggingface.co/01-ai/Yi-VL-3…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0OTI3OTQ3NTYxODE4MTEyMS85SmhNeEVMbD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1749313797914075535#m</id>
            <title>devv 上不了的朋友可以加到规则里试试</title>
            <link>https://nitter.cz/op7418/status/1749313797914075535#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1749313797914075535#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:11:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>devv 上不了的朋友可以加到规则里试试</p>
<p><a href="https://nitter.cz/Tisoga/status/1749279312111640647#m">nitter.cz/Tisoga/status/1749279312111640647#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>