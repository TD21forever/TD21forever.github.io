<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733482321368752182#m</id>
            <title>Mixtral-8x7b更详细的一些信息：
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。
◆这些专家模型都采用了 Mistral-7B 的架构。</title>
            <link>https://nitter.cz/op7418/status/1733482321368752182#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733482321368752182#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:42:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b更详细的一些信息：<br />
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。<br />
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。<br />
◆这些专家模型都采用了 Mistral-7B 的架构。</p>
<p><a href="https://nitter.cz/carrigmat/status/1733159362028257353#m">nitter.cz/carrigmat/status/1733159362028257353#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733480631966072878#m</id>
            <title>青龙之前写的MagicAnimate本地版本现在支持把视频或者gif转换为对应的Openpose或者其他ContorlNet动画了，这下用Animatediff做视频的时候也可以用了。
可以搞一个自己的动作库。做视频的时候直接用就行。</title>
            <link>https://nitter.cz/op7418/status/1733480631966072878#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733480631966072878#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:36:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>青龙之前写的MagicAnimate本地版本现在支持把视频或者gif转换为对应的Openpose或者其他ContorlNet动画了，这下用Animatediff做视频的时候也可以用了。<br />
可以搞一个自己的动作库。做视频的时候直接用就行。</p>
<p><a href="https://nitter.cz/bdsqlsz/status/1733478639692562792#m">nitter.cz/bdsqlsz/status/1733478639692562792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733478737017237910#m</id>
            <title>Twitter正式推出了Expanded Bios功能，你可以理解为更详细的个人简介。比如可以放你更多媒体的链接以及你的详细简历，我把我之前写的成体系的教程和做的东西放进去了，感兴趣可以来看看。
点击简介下面的查看更多就可以看到，或者可以点击下面的链接。
创建Expanded Bios的话可以点击编辑个人资料按钮拉到最下面编辑扩展简介就可以了。赶紧设置一下自己的吧。

我的Expanded Bios：https://twitter.com/op7418/bio</title>
            <link>https://nitter.cz/op7418/status/1733478737017237910#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733478737017237910#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:28:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Twitter正式推出了Expanded Bios功能，你可以理解为更详细的个人简介。比如可以放你更多媒体的链接以及你的详细简历，我把我之前写的成体系的教程和做的东西放进去了，感兴趣可以来看看。<br />
点击简介下面的查看更多就可以看到，或者可以点击下面的链接。<br />
创建Expanded Bios的话可以点击编辑个人资料按钮拉到最下面编辑扩展简介就可以了。赶紧设置一下自己的吧。<br />
<br />
我的Expanded Bios：<a href="https://nitter.cz/op7418/bio">nitter.cz/op7418/bio</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2TnR0c2JzQUE3RTdhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733469580545429860#m</id>
            <title>Mixtral-8x7b的Gradio demo，但是不能运行，因为老哥没有对应算力，需要自己本地跑 4xA10G或者3xA100。</title>
            <link>https://nitter.cz/op7418/status/1733469580545429860#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733469580545429860#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:52:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b的Gradio demo，但是不能运行，因为老哥没有对应算力，需要自己本地跑 4xA10G或者3xA100。</p>
<p><a href="https://nitter.cz/realmrfakename/status/1733293237274882150#m">nitter.cz/realmrfakename/status/1733293237274882150#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733468983599476751#m</id>
            <title>R to @op7418: 哈哈 复制的时候模型名字写错了应该是 mixtral-8x7b 哈</title>
            <link>https://nitter.cz/op7418/status/1733468983599476751#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733468983599476751#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:49:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈 复制的时候模型名字写错了应该是 mixtral-8x7b 哈</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733466929518760218#m</id>
            <title>现在去https://publish.twitter.com粘贴媒体的地址就可以把对应视频和播客的内容嵌入到网站了，试了一下推特地址还不行需要点开复制具体媒体的地址。</title>
            <link>https://nitter.cz/op7418/status/1733466929518760218#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733466929518760218#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:41:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>现在去<a href="https://publish.twitter.com">publish.twitter.com</a>粘贴媒体的地址就可以把对应视频和播客的内容嵌入到网站了，试了一下推特地址还不行需要点开复制具体媒体的地址。</p>
<p><a href="https://nitter.cz/Live/status/1733197678706852095#m">nitter.cz/Live/status/1733197678706852095#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2QzlqRmJBQUFCdlRjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733466068881191135#m</id>
            <title>Twitter的媒体Tab改成这种网格形式了，确实清晰多了，对那些经常发视频和好看图片的博主来说这个改动挺好的</title>
            <link>https://nitter.cz/op7418/status/1733466068881191135#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733466068881191135#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:38:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Twitter的媒体Tab改成这种网格形式了，确实清晰多了，对那些经常发视频和好看图片的博主来说这个改动挺好的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2Qnpyd2FZQUFBS2o1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733462829951545664#m</id>
            <title>R to @op7418: Pika制作的动画：
https://x.com/vonkleppski/status/1733197903315825116?s=20</title>
            <link>https://nitter.cz/op7418/status/1733462829951545664#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733462829951545664#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:25:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pika制作的动画：<br />
<a href="https://x.com/vonkleppski/status/1733197903315825116?s=20">x.com/vonkleppski/status/173…</a></p>
<p><a href="https://nitter.cz/vonkleppski/status/1733197903315825116#m">nitter.cz/vonkleppski/status/1733197903315825116#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733348315885138195#m</id>
            <title>RT by @op7418: 这个通过提示词局部编辑图片的项目也不错，比如你可以让图片的人物衣服换色和改变背景不改变原始人物。
相较于其他之前类似的项目，这个项目的理解更加准确对原图影响更小，同时由于利用了LCM所以速度更快。

与基于反转的方法的比较：图像编辑性能：DDCM 匹配或超过其他算法，LCM 和 UAC 带来进一步改进。值得注意的是，它的运行速度快了大约一个数量级。定性示例：InfEdit 与先前方法的比较。 InfEdit 实现了与源图像最佳一致性的编辑目标。

实现方法：尝试消除反演过程，并引入去噪扩散一致性模型（DDCM），这是一种支持虚拟反演的采样策略。 DDCM 利用扩散过程显着增强整个图像生成阶段的一致性，确保转换和细化视觉内容的保真度和速度。 还提出了统一注意力控制（UAC），用于通过自然语言进行免调整图像编辑，将交叉注意力和自注意力控制集成在统一框架内。

论文地址：https://sihanxu.github.io/InfEdit/docs/infedit.pdf</title>
            <link>https://nitter.cz/op7418/status/1733348315885138195#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733348315885138195#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:50:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个通过提示词局部编辑图片的项目也不错，比如你可以让图片的人物衣服换色和改变背景不改变原始人物。<br />
相较于其他之前类似的项目，这个项目的理解更加准确对原图影响更小，同时由于利用了LCM所以速度更快。<br />
<br />
与基于反转的方法的比较：图像编辑性能：DDCM 匹配或超过其他算法，LCM 和 UAC 带来进一步改进。值得注意的是，它的运行速度快了大约一个数量级。定性示例：InfEdit 与先前方法的比较。 InfEdit 实现了与源图像最佳一致性的编辑目标。<br />
<br />
实现方法：尝试消除反演过程，并引入去噪扩散一致性模型（DDCM），这是一种支持虚拟反演的采样策略。 DDCM 利用扩散过程显着增强整个图像生成阶段的一致性，确保转换和细化视觉内容的保真度和速度。 还提出了统一注意力控制（UAC），用于通过自然语言进行免调整图像编辑，将交叉注意力和自注意力控制集成在统一框架内。<br />
<br />
论文地址：<a href="https://sihanxu.github.io/InfEdit/docs/infedit.pdf">sihanxu.github.io/InfEdit/do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WEZnUGJrQUFFSmRTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733344034822000755#m</id>
            <title>RT by @op7418: WonderJourney这个项目有点厉害啊，只需要1张图片就可以创建3D场景动画，从用户提供的任何位置（通过文本描述或图像）开始，并通过一系列不同但连贯的 3D 场景生成一个旅程。
从演示效果来看非常流畅，3D游戏或者影视的场景创建要变简单了。而且这还是最近罕见的谷歌会开源的研究。

主要能力：
◆ 从任意位置（由文本或图像指定）开始，WonderJourney 沿着相机轨迹生成一系列多样化但连贯连接的 3D 场景。
◆ 从同一个地点开始，WonderJourney 可以生成一组不同的“奇妙旅程”，并在不同的目的地结束。使用相机姿势的轨迹渲染下面的每个视频。
◆ WonderJourney 还可以根据一系列文本描述（例如诗歌、俳句和故事摘要）生成受控的奇妙旅程。

WonderJourney的方法论：
场景描述生成：详细说明了为随后的场景生成文本描述的过程。
视觉场景生成：解释了系统如何根据文本描述和当前场景的3D表示来生成下一个3D场景。
视觉验证：讨论了系统如何验证所生成的场景是否存在不良视觉效果，并根据需要进行调整。

实验：展示了用于评估的数据集和基准，生成旅程的定性演示，额外的评估，以及人类偏好评估，突出了WonderJourney在生成多样且连贯的3D场景方面的有效性。

论文地址：https://arxiv.org/pdf/2312.03884.pdf</title>
            <link>https://nitter.cz/op7418/status/1733344034822000755#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733344034822000755#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:33:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WonderJourney这个项目有点厉害啊，只需要1张图片就可以创建3D场景动画，从用户提供的任何位置（通过文本描述或图像）开始，并通过一系列不同但连贯的 3D 场景生成一个旅程。<br />
从演示效果来看非常流畅，3D游戏或者影视的场景创建要变简单了。而且这还是最近罕见的谷歌会开源的研究。<br />
<br />
主要能力：<br />
◆ 从任意位置（由文本或图像指定）开始，WonderJourney 沿着相机轨迹生成一系列多样化但连贯连接的 3D 场景。<br />
◆ 从同一个地点开始，WonderJourney 可以生成一组不同的“奇妙旅程”，并在不同的目的地结束。使用相机姿势的轨迹渲染下面的每个视频。<br />
◆ WonderJourney 还可以根据一系列文本描述（例如诗歌、俳句和故事摘要）生成受控的奇妙旅程。<br />
<br />
WonderJourney的方法论：<br />
场景描述生成：详细说明了为随后的场景生成文本描述的过程。<br />
视觉场景生成：解释了系统如何根据文本描述和当前场景的3D表示来生成下一个3D场景。<br />
视觉验证：讨论了系统如何验证所生成的场景是否存在不良视觉效果，并根据需要进行调整。<br />
<br />
实验：展示了用于评估的数据集和基准，生成旅程的定性演示，额外的评估，以及人类偏好评估，突出了WonderJourney在生成多样且连贯的3D场景方面的有效性。<br />
<br />
论文地址：<a href="https://arxiv.org/pdf/2312.03884.pdf">arxiv.org/pdf/2312.03884.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMzNDMzNzc1OTEzMjg3NjgvcHUvaW1nL01PVmE0VjJSZm1jOEJLYjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733459876075180471#m</id>
            <title>R to @op7418: 贴一下RealJosephus截图的源推：
https://x.com/RealJosephus/status/1733321066104430936?s=20</title>
            <link>https://nitter.cz/op7418/status/1733459876075180471#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733459876075180471#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>贴一下RealJosephus截图的源推：<br />
<a href="https://x.com/RealJosephus/status/1733321066104430936?s=20">x.com/RealJosephus/status/17…</a></p>
<p><a href="https://nitter.cz/RealJosephus/status/1733321066104430936#m">nitter.cz/RealJosephus/status/1733321066104430936#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733459395672244503#m</id>
            <title>重新发一下MoE 8x7B的介绍原来的删掉了，由于没有在HF模型排行上找到我就直接复制了@RealJosephus的HF截图，这里向他道歉。一般只要是推特的原推有的我都会尽量转推MoE 8x7B由于官方没有说明很多内容都是拼凑的就没有把参考的内容都粘过来。
我并不是专门研究LLM的所以很多事情肯定说的不一定严谨，如果有问题欢迎指出。能改的我一般都会改，改不了的我会在下面贴上。但是我依然觉得不应该上来就骂人。

昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。
之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。
MoE 8x7B测试分数来源于第一个链接。

下面是MoE架构LLM的简单介绍：
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。
MoE架构为LLMs提供了几个优势：
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数。这使得能够开发更强大的模型，而无需成比例地增加计算要求。
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B。
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域。
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。  MoE的模型也有两个问题： MoE 模型比普通密集模型更难微调； MoE 模型会消耗大量显存。

模型下载：https://huggingface.co/DiscoResearch/mixtral-7b-8expert
在线试用：https://replicate.com/nateraw/mixtral-8x7b-32kseqlen</title>
            <link>https://nitter.cz/op7418/status/1733459395672244503#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733459395672244503#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:11:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>重新发一下MoE 8x7B的介绍原来的删掉了，由于没有在HF模型排行上找到我就直接复制了<a href="https://nitter.cz/RealJosephus" title="Joseph Cheung">@RealJosephus</a>的HF截图，这里向他道歉。一般只要是推特的原推有的我都会尽量转推MoE 8x7B由于官方没有说明很多内容都是拼凑的就没有把参考的内容都粘过来。<br />
我并不是专门研究LLM的所以很多事情肯定说的不一定严谨，如果有问题欢迎指出。能改的我一般都会改，改不了的我会在下面贴上。但是我依然觉得不应该上来就骂人。<br />
<br />
昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。<br />
之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。<br />
MoE 8x7B测试分数来源于第一个链接。<br />
<br />
下面是MoE架构LLM的简单介绍：<br />
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。<br />
MoE架构为LLMs提供了几个优势：<br />
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数。这使得能够开发更强大的模型，而无需成比例地增加计算要求。<br />
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B。<br />
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域。<br />
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。  MoE的模型也有两个问题： MoE 模型比普通密集模型更难微调； MoE 模型会消耗大量显存。<br />
<br />
模型下载：<a href="https://huggingface.co/DiscoResearch/mixtral-7b-8expert">huggingface.co/DiscoResearch…</a><br />
在线试用：<a href="https://replicate.com/nateraw/mixtral-8x7b-32kseqlen">replicate.com/nateraw/mixtra…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E1N3FrVGFVQUVwMGE3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733451932528992494#m</id>
            <title>这是你的源推，可能我理解的意思有误，我也去HF的榜单找了没找到，就没有自己截图复制了你的，从得分上来看确实比其他的强一些，可能我表述不够严谨，另外这个确实不是7B的模型是多个7B模型的组合。
模型底层的了解我确实不够专业，如果有问题我觉得可以理性讨论或者指出能改的我都会改掉，不能改的一般也会在推下补充。人身攻击没什么意思了。如果你不希望我用你的截图我会删掉https://x.com/RealJosephus/status/1733321066104430936?s=20</title>
            <link>https://nitter.cz/op7418/status/1733451932528992494#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733451932528992494#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 11:42:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是你的源推，可能我理解的意思有误，我也去HF的榜单找了没找到，就没有自己截图复制了你的，从得分上来看确实比其他的强一些，可能我表述不够严谨，另外这个确实不是7B的模型是多个7B模型的组合。<br />
模型底层的了解我确实不够专业，如果有问题我觉得可以理性讨论或者指出能改的我都会改掉，不能改的一般也会在推下补充。人身攻击没什么意思了。如果你不希望我用你的截图我会删掉<a href="https://x.com/RealJosephus/status/1733321066104430936?s=20">x.com/RealJosephus/status/17…</a></p>
<p><a href="https://nitter.cz/RealJosephus/status/1733437510532120626#m">nitter.cz/RealJosephus/status/1733437510532120626#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733350885873611224#m</id>
            <title>Animatediff做的雷神电影转动漫效果，稳定性很好。</title>
            <link>https://nitter.cz/op7418/status/1733350885873611224#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733350885873611224#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 05:00:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Animatediff做的雷神电影转动漫效果，稳定性很好。</p>
<p><a href="https://nitter.cz/InnerRefle11312/status/1733202194424386019#m">nitter.cz/InnerRefle11312/status/1733202194424386019#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733348384285802692#m</id>
            <title>R to @op7418: 作者原推：
https://x.com/ziqiao_ma/status/1733224975207628938?s=20</title>
            <link>https://nitter.cz/op7418/status/1733348384285802692#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733348384285802692#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:50:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者原推：<br />
<a href="https://x.com/ziqiao_ma/status/1733224975207628938?s=20">x.com/ziqiao_ma/status/17332…</a></p>
<p><a href="https://nitter.cz/ziqiao_ma/status/1733224975207628938#m">nitter.cz/ziqiao_ma/status/1733224975207628938#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733204647337476202#m</id>
            <title>RT by @op7418: 尝试了一下DemoFusion这个图片放大项目，这玩意的资源消耗真是恐怖，2048*2048的图片A100居然跑了两分半的时间。
前两张是动漫图片，后两张是写实照片，不过效果确实好。可能是实现问题部分细节有一些崩坏的纹理。
动漫的图片会被识别成写实的照片，可能是提示词问题。
原图放在下面的推里面了可以对比一下生成前后的效果。

想体验的可以用这个Colab链接尝试，不过不是Pro的就算了，T4得跑好久：https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_img2img_colab.ipynb</title>
            <link>https://nitter.cz/op7418/status/1733204647337476202#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733204647337476202#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 19:19:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>尝试了一下DemoFusion这个图片放大项目，这玩意的资源消耗真是恐怖，2048*2048的图片A100居然跑了两分半的时间。<br />
前两张是动漫图片，后两张是写实照片，不过效果确实好。可能是实现问题部分细节有一些崩坏的纹理。<br />
动漫的图片会被识别成写实的照片，可能是提示词问题。<br />
原图放在下面的推里面了可以对比一下生成前后的效果。<br />
<br />
想体验的可以用这个Colab链接尝试，不过不是Pro的就算了，T4得跑好久：<a href="https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_img2img_colab.ipynb">colab.research.google.com/gi…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyRzFMQ2JRQUF3ZnlxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EySUJ1VWJRQUFEWXdxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyUjZNQWJRQVlfQmZpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVWJWT2JRQUVjdGY1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733205116709405140#m</id>
            <title>R to @op7418: 动漫的图</title>
            <link>https://nitter.cz/op7418/status/1733205116709405140#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733205116709405140#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 19:21:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>动漫的图</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVXloV2JRQUVEQ2t3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVXpZWGJRQUFCN0JpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVTJlQ2JRQUFic0lOLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVTNTcGJRQUF4aW5sLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733204922328629571#m</id>
            <title>R to @op7418: 写实的图</title>
            <link>https://nitter.cz/op7418/status/1733204922328629571#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733204922328629571#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 19:20:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>写实的图</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVW5qN2JRQUVmOTRtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVW5rVGJRQUFNUXNQLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVXFXQWJRQUFYc3NmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyVXJOTmFnQUFKa29RLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>