<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751300318464487521#m</id>
            <title>昨晚谷歌还发布了一种叫“仅权重量化”（weight-only quantization）的技术。这项技术使用了 8 位整数（S8）来表示权重，同时使用 16 位浮点数（F16）来表示输入数据，以此进行矩阵乘法运算。这种方法在保证一定精度的同时，提高了运算效率。

详细内容：https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html</title>
            <link>https://nitter.cz/op7418/status/1751300318464487521#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751300318464487521#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:45:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚谷歌还发布了一种叫“仅权重量化”（weight-only quantization）的技术。这项技术使用了 8 位整数（S8）来表示权重，同时使用 16 位浮点数（F16）来表示输入数据，以此进行矩阵乘法运算。这种方法在保证一定精度的同时，提高了运算效率。<br />
<br />
详细内容：<a href="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">blog.research.google/2024/01…</a></p>
<p><a href="https://nitter.cz/GoogleAI/status/1750977073009881469#m">nitter.cz/GoogleAI/status/1750977073009881469#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MDk3NzAyOTQ2NDYwODc2OC9mZnpKT21kQj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751298793130065978#m</id>
            <title>虚幻引擎5生成的一个演示视频，非常真实，如果不是偶尔的画面撕裂我都无法分辨出来这是渲染的。

前几天虚幻引擎更新5.3之后才基本把之前发布时说的所有能力补齐，确实牛批。</title>
            <link>https://nitter.cz/op7418/status/1751298793130065978#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751298793130065978#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:39:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>虚幻引擎5生成的一个演示视频，非常真实，如果不是偶尔的画面撕裂我都无法分辨出来这是渲染的。<br />
<br />
前几天虚幻引擎更新5.3之后才基本把之前发布时说的所有能力补齐，确实牛批。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwODkwMDE5MzcwMjc0ODE2L2ltZy80bllIZE01SEp3QTNfVmM0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751296580836377017#m</id>
            <title>今天是InstructGPT发布两周年的纪念日，它是现代大语言模型的开山鼻祖。

Jim Fan介绍了InstructGPT的重要性并且说了几条关于InstructGPT非常有意思的点。还展示了InstructGPT中非常经典的三步LLM训练方法的图片，我也顺便让GPT-4解释了一下也顺便放在下面：

InstructGPT 开创了一个经典的模型训练方法：先进行预训练，然后是监督式微调，最后是基于强化学习的人类反馈（Reinforcement Learning from Human Feedback, RLHF）。
这个策略至今仍被广泛采用，虽然有些许变化，比如 DPO 策略。

InstructGPT 可能是 OpenAI 最后一次详细介绍他们如何训练尖端模型的论文。
回顾这两年，Jim Fan认为它标志着大语言模型从学术研究（GPT-3）走向实际应用（ChatGPT）的关键转折点。

一些有趣的事实：

1️⃣InstructGPT 并非 RLHF 的发明者。实际上，它的博客链接指向了 OpenAI 团队在 2017 年完成的最早的 RLHF 研究。RLHF 的初衷是解决模拟机器人领域中难以定义的任务。它通过要求人类注释者给出 900 个简单的是非选择，帮助一个名为“跳跃机器人”的机器人在模拟环境中学会了后空翻：https://openai.com/research/learning-from-human-preferences

2️⃣InstructGPT 在 2022 年的 NeurIPS 会议上亮相，地点是新奥尔良。当时Jim Fan正在会上展示他的项目 MineDojo，看到 OpenAI 的展示非常惊喜。

3️⃣这些模型有三种规模：1.3B、6B 和 175B。与老式的、需要复杂提示的 GPT-3-175B 相比，标注者更加青睐 Instruct-1.3B。而且，Microsoft Phi-1，作为众所周知的小型优秀语言模型之一，也是 1.3B。

4️⃣InstructGPT 在展示研究方面堪称典范。它的三步骤图解清晰易懂，成为 AI 领域最具标志性的视觉之一。它的引言部分直奔主题，用粗体突出了八个主要观点。对于模型局限性和偏见问题的讨论，也是实事求是和坦诚的。

InstructGPT模型三步训练方法示意图介绍：

1️⃣第1步：收集示范数据，并训练一个监督策略。

从我们的提示数据集中抽取一个提示，例如“向一个6岁的孩子解释月球登陆”。
标注者提供期望的输出行为，例如“一些人去了月球”。
这些数据被用来通过监督学习细调GPT-3模型。

2️⃣第2步：收集比较数据，并训练一个奖励模型。

抽取一个提示和几个模型输出样本，例如同样是“向一个6岁的孩子解释月球登陆”。
标注者会对这些输出进行排名，从最好到最差。
这些数据被用来训练我们的奖励模型。

3️⃣第3步：使用强化学习对奖励模型优化策略。

从数据集中抽取一个新的提示，例如“写一个关于青蛙的故事”。
策略生成一个输出，比如开始写“从前有一次…”。
奖励模型计算一个奖励值给这个输出。
这个奖励值被用来通过PPO（比例策略优化）更新策略。
整个过程展示了从收集数据到通过人类标注者的反馈来训练和优化人工智能模型的步骤。这种方法结合了监督学习和强化学习来改进模型的性能。

OpenAI原文：https://openai.com/research/instruction-following</title>
            <link>https://nitter.cz/op7418/status/1751296580836377017#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751296580836377017#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:30:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天是InstructGPT发布两周年的纪念日，它是现代大语言模型的开山鼻祖。<br />
<br />
Jim Fan介绍了InstructGPT的重要性并且说了几条关于InstructGPT非常有意思的点。还展示了InstructGPT中非常经典的三步LLM训练方法的图片，我也顺便让GPT-4解释了一下也顺便放在下面：<br />
<br />
InstructGPT 开创了一个经典的模型训练方法：先进行预训练，然后是监督式微调，最后是基于强化学习的人类反馈（Reinforcement Learning from Human Feedback, RLHF）。<br />
这个策略至今仍被广泛采用，虽然有些许变化，比如 DPO 策略。<br />
<br />
InstructGPT 可能是 OpenAI 最后一次详细介绍他们如何训练尖端模型的论文。<br />
回顾这两年，Jim Fan认为它标志着大语言模型从学术研究（GPT-3）走向实际应用（ChatGPT）的关键转折点。<br />
<br />
一些有趣的事实：<br />
<br />
1️⃣InstructGPT 并非 RLHF 的发明者。实际上，它的博客链接指向了 OpenAI 团队在 2017 年完成的最早的 RLHF 研究。RLHF 的初衷是解决模拟机器人领域中难以定义的任务。它通过要求人类注释者给出 900 个简单的是非选择，帮助一个名为“跳跃机器人”的机器人在模拟环境中学会了后空翻：<a href="https://openai.com/research/learning-from-human-preferences">openai.com/research/learning…</a><br />
<br />
2️⃣InstructGPT 在 2022 年的 NeurIPS 会议上亮相，地点是新奥尔良。当时Jim Fan正在会上展示他的项目 MineDojo，看到 OpenAI 的展示非常惊喜。<br />
<br />
3️⃣这些模型有三种规模：1.3B、6B 和 175B。与老式的、需要复杂提示的 GPT-3-175B 相比，标注者更加青睐 Instruct-1.3B。而且，Microsoft Phi-1，作为众所周知的小型优秀语言模型之一，也是 1.3B。<br />
<br />
4️⃣InstructGPT 在展示研究方面堪称典范。它的三步骤图解清晰易懂，成为 AI 领域最具标志性的视觉之一。它的引言部分直奔主题，用粗体突出了八个主要观点。对于模型局限性和偏见问题的讨论，也是实事求是和坦诚的。<br />
<br />
InstructGPT模型三步训练方法示意图介绍：<br />
<br />
1️⃣第1步：收集示范数据，并训练一个监督策略。<br />
<br />
从我们的提示数据集中抽取一个提示，例如“向一个6岁的孩子解释月球登陆”。<br />
标注者提供期望的输出行为，例如“一些人去了月球”。<br />
这些数据被用来通过监督学习细调GPT-3模型。<br />
<br />
2️⃣第2步：收集比较数据，并训练一个奖励模型。<br />
<br />
抽取一个提示和几个模型输出样本，例如同样是“向一个6岁的孩子解释月球登陆”。<br />
标注者会对这些输出进行排名，从最好到最差。<br />
这些数据被用来训练我们的奖励模型。<br />
<br />
3️⃣第3步：使用强化学习对奖励模型优化策略。<br />
<br />
从数据集中抽取一个新的提示，例如“写一个关于青蛙的故事”。<br />
策略生成一个输出，比如开始写“从前有一次…”。<br />
奖励模型计算一个奖励值给这个输出。<br />
这个奖励值被用来通过PPO（比例策略优化）更新策略。<br />
整个过程展示了从收集数据到通过人类标注者的反馈来训练和优化人工智能模型的步骤。这种方法结合了监督学习和强化学习来改进模型的性能。<br />
<br />
OpenAI原文：<a href="https://openai.com/research/instruction-following">openai.com/research/instruct…</a></p>
<p><a href="https://nitter.cz/DrJimFan/status/1751285761364906476#m">nitter.cz/DrJimFan/status/1751285761364906476#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0UzYWM2QWFVQUEyQWFOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751293373074510210#m</id>
            <title>另一个GPTs调用能力的牛批演示，在一个聊天中先使用Grimoire构建一个网页。

然后直接调用DesignerGPT对上面Grimoire生成的网页进行部署。

这样通过两个GPTs的协作，就直接获得了一个部署好的网站。</title>
            <link>https://nitter.cz/op7418/status/1751293373074510210#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751293373074510210#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:17:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个GPTs调用能力的牛批演示，在一个聊天中先使用Grimoire构建一个网页。<br />
<br />
然后直接调用DesignerGPT对上面Grimoire生成的网页进行部署。<br />
<br />
这样通过两个GPTs的协作，就直接获得了一个部署好的网站。</p>
<p><a href="https://nitter.cz/skirano/status/1751092906964787406#m">nitter.cz/skirano/status/1751092906964787406#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751292605781205337#m</id>
            <title>R to @op7418: 我自己的体验在这里：
https://x.com/op7418/status/1751292333268893851?s=20</title>
            <link>https://nitter.cz/op7418/status/1751292605781205337#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751292605781205337#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我自己的体验在这里：<br />
<a href="https://x.com/op7418/status/1751292333268893851?s=20">x.com/op7418/status/17512923…</a></p>
<p><a href="https://nitter.cz/op7418/status/1751292333268893851#m">nitter.cz/op7418/status/1751292333268893851#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751292333268893851#m</id>
            <title>我刚才发现自己有了ChatGPT昨天发布的GPTs联动的功能，试了一下确实强大。

下面用一个例子看一下如何使用和具体能力，还有一个对简中用户比较重要的操作。

我自己看论文会先下载下来让ChatGPT总结，然后会用宝玉的科技文章翻译GPTs对总结的内容进行精细的翻译，以前完成这两步起码需要我切两个窗口，然后复制粘贴一次。

现在我只需要在总结完成后输入“@”找到需要的GPTs，输入要求就可以在一次对话中完成，甚至我还可以再搞一个自动发推的GPTs在ChatGPT直接将翻译内容发到推特上。

最后一个对简中用户不太友好的BUG：拉起GPTs选择的时候你输入的“@”符号需要在英文输入法下才能调用这个功能，中文输入法下面没办法拉起GPTs选择，OpenAI做产品是真的糙。</title>
            <link>https://nitter.cz/op7418/status/1751292333268893851#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751292333268893851#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:13:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我刚才发现自己有了ChatGPT昨天发布的GPTs联动的功能，试了一下确实强大。<br />
<br />
下面用一个例子看一下如何使用和具体能力，还有一个对简中用户比较重要的操作。<br />
<br />
我自己看论文会先下载下来让ChatGPT总结，然后会用宝玉的科技文章翻译GPTs对总结的内容进行精细的翻译，以前完成这两步起码需要我切两个窗口，然后复制粘贴一次。<br />
<br />
现在我只需要在总结完成后输入“@”找到需要的GPTs，输入要求就可以在一次对话中完成，甚至我还可以再搞一个自动发推的GPTs在ChatGPT直接将翻译内容发到推特上。<br />
<br />
最后一个对简中用户不太友好的BUG：拉起GPTs选择的时候你输入的“@”符号需要在英文输入法下才能调用这个功能，中文输入法下面没办法拉起GPTs选择，OpenAI做产品是真的糙。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTEyOTIyNTcwODc2ODg3MDQvcHUvaW1nL0dFdHpDX2JJcWhsNmY3S3YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751187683400105984#m</id>
            <title>昨晚ChatGPT推出了通过在正常的聊天中@其他GPTs协同处理任务的能力，这个能力非常强大。

他让GPTs的可能性多了非常多，我没有被灰度到，Dan Shipper这个演示非常详细和完整。

他演示了如何将对话结果利用GPTs直接保存到Notion中，我顺便翻译了这个视频，如果你也没有被灰度到可以看看这个视频。</title>
            <link>https://nitter.cz/op7418/status/1751187683400105984#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751187683400105984#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 10:17:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚ChatGPT推出了通过在正常的聊天中@其他GPTs协同处理任务的能力，这个能力非常强大。<br />
<br />
他让GPTs的可能性多了非常多，我没有被灰度到，Dan Shipper这个演示非常详细和完整。<br />
<br />
他演示了如何将对话结果利用GPTs直接保存到Notion中，我顺便翻译了这个视频，如果你也没有被灰度到可以看看这个视频。</p>
<p><a href="https://nitter.cz/danshipper/status/1751017376143794415#m">nitter.cz/danshipper/status/1751017376143794415#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTExODc1ODgxMDk3OTUzMjgvcHUvaW1nLzJGQkdTdjNtUnFSTjdJZEsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751170799082643524#m</id>
            <title>Stability ai即将发布图像放大产品或者模型，用的演示还是Magnific AI曾经用过的劳拉图片。</title>
            <link>https://nitter.cz/op7418/status/1751170799082643524#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751170799082643524#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 09:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability ai即将发布图像放大产品或者模型，用的演示还是Magnific AI曾经用过的劳拉图片。</p>
<p><a href="https://nitter.cz/EMostaque/status/1751061406382735633#m">nitter.cz/EMostaque/status/1751061406382735633#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Uxb0xLZWFJQUEwa2FZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750929802461388902#m</id>
            <title>这两个确实挺合适合作的，现在可以将Perplexity设置为Arc的默认搜索引擎。</title>
            <link>https://nitter.cz/op7418/status/1750929802461388902#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750929802461388902#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 17:12:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这两个确实挺合适合作的，现在可以将Perplexity设置为Arc的默认搜索引擎。</p>
<p><a href="https://nitter.cz/joshm/status/1750888638324560289#m">nitter.cz/joshm/status/1750888638324560289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750912814376730954#m</id>
            <title>找到一篇介绍图像识别的好内容《图像识别基础知识》，顺手翻译学习了一下。

就跟LLM训练的时候训练数据集很重要一样，图像模型的训练也需要高质量的图片素材。

同时模型训练结束之后如何对模型产出的图片进行评价也是一个重要的内容，这些都需要一些图像识别的项目来完成比如图像标记、图像分割。

这个篇文章就对图像识别的分类、历史和工作原理都做了详细的介绍，感兴趣可以看看。

原文及翻译链接：https://quail.ink/op7418/p/image-recognition-basics-visual-model-portal</title>
            <link>https://nitter.cz/op7418/status/1750912814376730954#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750912814376730954#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 16:05:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>找到一篇介绍图像识别的好内容《图像识别基础知识》，顺手翻译学习了一下。<br />
<br />
就跟LLM训练的时候训练数据集很重要一样，图像模型的训练也需要高质量的图片素材。<br />
<br />
同时模型训练结束之后如何对模型产出的图片进行评价也是一个重要的内容，这些都需要一些图像识别的项目来完成比如图像标记、图像分割。<br />
<br />
这个篇文章就对图像识别的分类、历史和工作原理都做了详细的介绍，感兴趣可以看看。<br />
<br />
原文及翻译链接：<a href="https://quail.ink/op7418/p/image-recognition-basics-visual-model-portal">quail.ink/op7418/p/image-rec…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V4OTJXLWEwQUFpU0dlLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750776302096060817#m</id>
            <title>Comfy Textures这个项目可以啊，用 ComfyUI 为虚幻引擎中的模型创建贴图，就是部署看起来比较麻烦。

主要功能包括：

单个视角下的纹理投影：从一个固定视角对纹理进行映射和调整。

多个视角下的纹理投影（目前正在开发中）：允许从多个视角对纹理进行更全面的映射和调整。

透视相机：以透视的方式捕捉场景，提供更为真实的视觉效果。

正交相机：采用正交视图来捕捉场景，适用于需要准确比例和结构的设计。

图像修复（Inpainting）：能够修复图像中的缺陷或不完整部分，提高纹理的整体质量。

图像到图像转换：将一幅图像转换成不同风格或质感的新图像。

目前支持 Unreal Engine 5.x 版本。如果要在 4.x 版本上使用，仅需做少量的代码调整即可。

项目地址：https://github.com/AlexanderDzhoganov/ComfyTextures</title>
            <link>https://nitter.cz/op7418/status/1750776302096060817#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750776302096060817#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 07:02:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfy Textures这个项目可以啊，用 ComfyUI 为虚幻引擎中的模型创建贴图，就是部署看起来比较麻烦。<br />
<br />
主要功能包括：<br />
<br />
单个视角下的纹理投影：从一个固定视角对纹理进行映射和调整。<br />
<br />
多个视角下的纹理投影（目前正在开发中）：允许从多个视角对纹理进行更全面的映射和调整。<br />
<br />
透视相机：以透视的方式捕捉场景，提供更为真实的视觉效果。<br />
<br />
正交相机：采用正交视图来捕捉场景，适用于需要准确比例和结构的设计。<br />
<br />
图像修复（Inpainting）：能够修复图像中的缺陷或不完整部分，提高纹理的整体质量。<br />
<br />
图像到图像转换：将一幅图像转换成不同风格或质感的新图像。<br />
<br />
目前支持 Unreal Engine 5.x 版本。如果要在 4.x 版本上使用，仅需做少量的代码调整即可。<br />
<br />
项目地址：<a href="https://github.com/AlexanderDzhoganov/ComfyTextures">github.com/AlexanderDzhogano…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA3NzU0MjI5NjE0OTE5NjgvcHUvaW1nL0dwVVpna3hBMEhjekFQSnIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750719622540841045#m</id>
            <title>R to @op7418: 4️⃣ SVD 和 Animatediff 等开源视频模型 这里投票❤️</title>
            <link>https://nitter.cz/op7418/status/1750719622540841045#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750719622540841045#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 03:17:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>4️⃣ SVD 和 Animatediff 等开源视频模型 这里投票❤️</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750719544946229411#m</id>
            <title>R to @op7418: 3️⃣ Pixverse 等其他 AI 视频工具  这里投票❤️</title>
            <link>https://nitter.cz/op7418/status/1750719544946229411#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750719544946229411#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 03:17:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3️⃣ Pixverse 等其他 AI 视频工具  这里投票❤️</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750719453334294875#m</id>
            <title>R to @op7418: 2️⃣ Pika 这里投票❤️</title>
            <link>https://nitter.cz/op7418/status/1750719453334294875#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750719453334294875#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 03:16:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2️⃣ Pika 这里投票❤️</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750719399202533653#m</id>
            <title>R to @op7418: 1️⃣ Runway 这里投票❤️</title>
            <link>https://nitter.cz/op7418/status/1750719399202533653#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750719399202533653#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 03:16:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>1️⃣ Runway 这里投票❤️</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750719328713114051#m</id>
            <title>刚好来整一个调研（骗赞），你们平时更喜欢用哪种 AI 视频生成工具。
可以点开这条推在下面对应的选项点赞投票：

1️⃣ Runway
2️⃣ Pika
3️⃣ Pixverse 等其他 AI 视频工具
4️⃣ SVD 和 Animatediff 等开源视频模型</title>
            <link>https://nitter.cz/op7418/status/1750719328713114051#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750719328713114051#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 03:16:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚好来整一个调研（骗赞），你们平时更喜欢用哪种 AI 视频生成工具。<br />
可以点开这条推在下面对应的选项点赞投票：<br />
<br />
1️⃣ Runway<br />
2️⃣ Pika<br />
3️⃣ Pixverse 等其他 AI 视频工具<br />
4️⃣ SVD 和 Animatediff 等开源视频模型</p>
<p><a href="https://nitter.cz/op7418/status/1750497734849917017#m">nitter.cz/op7418/status/1750497734849917017#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750714273104556197#m</id>
            <title>ChatGPT 增加了多语言适配，居然支持了中文。

进去以后会询问是不是要在你的语言中使用 ChatGPT，加入测试后界面就变中文了。

这下舒服了，不知道西班牙语日语之类的是不是也有类似的提示。</title>
            <link>https://nitter.cz/op7418/status/1750714273104556197#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750714273104556197#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:56:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT 增加了多语言适配，居然支持了中文。<br />
<br />
进去以后会询问是不是要在你的语言中使用 ChatGPT，加入测试后界面就变中文了。<br />
<br />
这下舒服了，不知道西班牙语日语之类的是不是也有类似的提示。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V2SXRtY2JnQUVEdXBfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750706638993158341#m</id>
            <title>收购失败之后Figma终于停止了摆烂势头，开始处理这个卖的巨贵的半成品Dev mode了。</title>
            <link>https://nitter.cz/op7418/status/1750706638993158341#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750706638993158341#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:26:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>收购失败之后Figma终于停止了摆烂势头，开始处理这个卖的巨贵的半成品Dev mode了。</p>
<p><a href="https://nitter.cz/miggi/status/1750622410817061242#m">nitter.cz/miggi/status/1750622410817061242#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750698767509327988#m</id>
            <title>昨晚Open AI这零碎更新挺多啊： 

1）有两个新的嵌入模型。

2）新的GPT-4 Turbo和GPT-3.5 Turbo模型，3.5还降价。

3）还有可以单独控制API访问的权限，这下不担心API被盗。

4）最后还更新了最新的审核模型text-moderation-007。

完整更新：https://openai.com/blog/new-embedding-models-and-api-updates</title>
            <link>https://nitter.cz/op7418/status/1750698767509327988#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750698767509327988#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 01:54:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚Open AI这零碎更新挺多啊： <br />
<br />
1）有两个新的嵌入模型。<br />
<br />
2）新的GPT-4 Turbo和GPT-3.5 Turbo模型，3.5还降价。<br />
<br />
3）还有可以单独控制API访问的权限，这下不担心API被盗。<br />
<br />
4）最后还更新了最新的审核模型text-moderation-007。<br />
<br />
完整更新：<a href="https://openai.com/blog/new-embedding-models-and-api-updates">openai.com/blog/new-embeddin…</a></p>
<p><a href="https://nitter.cz/OpenAIDevs/status/1750589083397820841#m">nitter.cz/OpenAIDevs/status/1750589083397820841#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MDU4Nzc2MTE4NTE4OTg4OC9jb2FMVWQwcz9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750423935584461121#m</id>
            <title>RT by @op7418: 舒服了，才注意到 Midjourney V6 更新了一堆新的能力。

同时alpha版本的网站只需要生成过 5000 张图片就能进入。快点击下面链接看看自己有没有权限吧。

具体更新内容有：

1）V6 版本现在支持平移、缩放以及变换特定区域的功能。 

这些新功能已经添加到了 v6 版本的机器人图像放大功能中，同时也出现在 alpha 网站上（网络版除了区域变换外都有）。 

现在的平移功能更类似于缩放，使用起来更加方便，而且能够带来更高质量的图像（图像更连贯，重复的元素更少）。 

平移功能如今可以和图像放大、区域变换和创意重组等功能配合使用，但不会再无限制地提高图像分辨率了。

2）alpha版本midjourney 网站（支持图像创作功能）现在对 5000 俱乐部的成员开放。 

5000 俱乐部包括那些在 Midjourney 平台上至少创作了 5000 张图片的用户。 

你可以通过 /info 命令来查询你制作的图片数量。 访问以下链接即可进入该网站：http://alpha.midjourney.com/

3）新增了 /feedback 功能。 这个功能允许你提交给 Midjourney 团队，告诉我们你最希望我们优先开发和完善的功能或项目。</title>
            <link>https://nitter.cz/op7418/status/1750423935584461121#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750423935584461121#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 07:42:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>舒服了，才注意到 Midjourney V6 更新了一堆新的能力。<br />
<br />
同时alpha版本的网站只需要生成过 5000 张图片就能进入。快点击下面链接看看自己有没有权限吧。<br />
<br />
具体更新内容有：<br />
<br />
1）V6 版本现在支持平移、缩放以及变换特定区域的功能。 <br />
<br />
这些新功能已经添加到了 v6 版本的机器人图像放大功能中，同时也出现在 alpha 网站上（网络版除了区域变换外都有）。 <br />
<br />
现在的平移功能更类似于缩放，使用起来更加方便，而且能够带来更高质量的图像（图像更连贯，重复的元素更少）。 <br />
<br />
平移功能如今可以和图像放大、区域变换和创意重组等功能配合使用，但不会再无限制地提高图像分辨率了。<br />
<br />
2）alpha版本midjourney 网站（支持图像创作功能）现在对 5000 俱乐部的成员开放。 <br />
<br />
5000 俱乐部包括那些在 Midjourney 平台上至少创作了 5000 张图片的用户。 <br />
<br />
你可以通过 /info 命令来查询你制作的图片数量。 访问以下链接即可进入该网站：<a href="http://alpha.midjourney.com/">alpha.midjourney.com/</a><br />
<br />
3）新增了 /feedback 功能。 这个功能允许你提交给 Midjourney 团队，告诉我们你最希望我们优先开发和完善的功能或项目。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA0MjM4MDY1ODQ1MTI1MTIvcHUvaW1nL2IxWWNYTndnb1dobUVPNUMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>