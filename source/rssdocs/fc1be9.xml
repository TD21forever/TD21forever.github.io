<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733515976212451706#m</id>
            <title>R to @op7418: 我前几天还想试试转一下洛基第二季他坐上时间线王座的那段来着。</title>
            <link>https://nitter.cz/op7418/status/1733515976212451706#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733515976212451706#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:56:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我前几天还想试试转一下洛基第二季他坐上时间线王座的那段来着。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733515802408824908#m</id>
            <title>最近用Animatediff将传统经典电影转成动漫风格的流程看起来很成熟了。这个就是黑客帝国经典的打斗转的。</title>
            <link>https://nitter.cz/op7418/status/1733515802408824908#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733515802408824908#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:55:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近用Animatediff将传统经典电影转成动漫风格的流程看起来很成熟了。这个就是黑客帝国经典的打斗转的。</p>
<p><a href="https://nitter.cz/Mrboofyy/status/1733157260770083058#m">nitter.cz/Mrboofyy/status/1733157260770083058#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733513341816221758#m</id>
            <title>哈哈 最近生病了卷不动了，一个支原体让我瘫痪了二十天。</title>
            <link>https://nitter.cz/op7418/status/1733513341816221758#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733513341816221758#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:46:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈 最近生病了卷不动了，一个支原体让我瘫痪了二十天。</p>
<p><a href="https://nitter.cz/vista8/status/1733509609959379094#m">nitter.cz/vista8/status/1733509609959379094#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733503266141704195#m</id>
            <title>这个老哥带来了首个开源MoE架构模型Mixtral-8x7b比较详细的介绍，还有MoE模型是什么，以及和GPT-4 MoE架构的区别。可以看看比我上午自己搜的强不少：

◆该模型以 87 GB 的种子文件形式发布
◆看似是 GPT-4 的精简版
◆在 X 平台发布，没有配套的新闻发布会，且对更多细节守口如瓶。

谷歌凭借其精心编排的演示视频令 AI 社区敬畏，但现在这段视频正受到广泛批评。  
另一方面，开源 AI 创业公司 Mistral AI 发布了一个包含 8 个 7B 级别专家的 MoE 模型。 

专家混合（MoE）是什么？  
专家混合（MoE）是用于提高大语言模型效率和准确度的技术。这种方法将复杂任务划分为更小、更易管理的子任务，每个子任务由专门的小型模型或“专家”负责。
以下是简要说明：  
1. 专家层：这些是在特定领域训练有素的小型神经网络。每个专家以其独特专长的方式处理相同的输入。
2. 门控网络：这是 MoE 架构的决策核心。它判断哪个专家最适合处理特定输入。网络为输入数据与每个专家的兼容性打分，然后根据这些得分确定每个专家在任务中的角色。 
这些组成部分共同确保正确的专家处理正确的任务。门控网络有效地将输入引导至最合适的专家，而专家则专注于他们擅长的领域。这种合作培训使得整体模型更加多才多艺、能力更强。
关于 Mistral 新 MoE 的详情（来自 Reddit）  在对每个 Token 进行推理时，只有 2 个专家被使用。
这一信息可以从模型的元数据中获得： 
 {"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}

与 GPT-4 的比较Mistral 的 8x7B 模型采用了与 GPT-4 相似的架构，但规模更小：
◆总共 8 个专家模型，而不是 16 个（减少了一半）
◆每个专家拥有 7B 参数，而不是 166B（减少了 24 倍） 
◆总共约 42B 参数，而非 1.8T（减少了 42 倍）
◆与原版 GPT-4 相同的 32K 上下文限制</title>
            <link>https://nitter.cz/op7418/status/1733503266141704195#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733503266141704195#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:06:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个老哥带来了首个开源MoE架构模型Mixtral-8x7b比较详细的介绍，还有MoE模型是什么，以及和GPT-4 MoE架构的区别。可以看看比我上午自己搜的强不少：<br />
<br />
◆该模型以 87 GB 的种子文件形式发布<br />
◆看似是 GPT-4 的精简版<br />
◆在 X 平台发布，没有配套的新闻发布会，且对更多细节守口如瓶。<br />
<br />
谷歌凭借其精心编排的演示视频令 AI 社区敬畏，但现在这段视频正受到广泛批评。  <br />
另一方面，开源 AI 创业公司 Mistral AI 发布了一个包含 8 个 7B 级别专家的 MoE 模型。 <br />
<br />
专家混合（MoE）是什么？  <br />
专家混合（MoE）是用于提高大语言模型效率和准确度的技术。这种方法将复杂任务划分为更小、更易管理的子任务，每个子任务由专门的小型模型或“专家”负责。<br />
以下是简要说明：  <br />
1. 专家层：这些是在特定领域训练有素的小型神经网络。每个专家以其独特专长的方式处理相同的输入。<br />
2. 门控网络：这是 MoE 架构的决策核心。它判断哪个专家最适合处理特定输入。网络为输入数据与每个专家的兼容性打分，然后根据这些得分确定每个专家在任务中的角色。 <br />
这些组成部分共同确保正确的专家处理正确的任务。门控网络有效地将输入引导至最合适的专家，而专家则专注于他们擅长的领域。这种合作培训使得整体模型更加多才多艺、能力更强。<br />
关于 Mistral 新 MoE 的详情（来自 Reddit）  在对每个 Token 进行推理时，只有 2 个专家被使用。<br />
这一信息可以从模型的元数据中获得： <br />
 {"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}<br />
<br />
与 GPT-4 的比较Mistral 的 8x7B 模型采用了与 GPT-4 相似的架构，但规模更小：<br />
◆总共 8 个专家模型，而不是 16 个（减少了一半）<br />
◆每个专家拥有 7B 参数，而不是 166B（减少了 24 倍） <br />
◆总共约 42B 参数，而非 1.8T（减少了 42 倍）<br />
◆与原版 GPT-4 相同的 32K 上下文限制</p>
<p><a href="https://nitter.cz/Saboo_Shubham_/status/1733364854973456561#m">nitter.cz/Saboo_Shubham_/status/1733364854973456561#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733498433166840116#m</id>
            <title>最近也把之前积累的零碎搬到Heptabase上了。</title>
            <link>https://nitter.cz/op7418/status/1733498433166840116#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733498433166840116#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 14:46:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近也把之前积累的零碎搬到Heptabase上了。</p>
<p><a href="https://nitter.cz/lyson_ober/status/1733496688504140029#m">nitter.cz/lyson_ober/status/1733496688504140029#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733482321368752182#m</id>
            <title>Mixtral-8x7b更详细的一些信息：
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。
◆这些专家模型都采用了 Mistral-7B 的架构。</title>
            <link>https://nitter.cz/op7418/status/1733482321368752182#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733482321368752182#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:42:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b更详细的一些信息：<br />
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。<br />
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。<br />
◆这些专家模型都采用了 Mistral-7B 的架构。</p>
<p><a href="https://nitter.cz/carrigmat/status/1733159362028257353#m">nitter.cz/carrigmat/status/1733159362028257353#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733480631966072878#m</id>
            <title>青龙之前写的MagicAnimate本地版本现在支持把视频或者gif转换为对应的Openpose或者其他ContorlNet动画了，这下用Animatediff做视频的时候也可以用了。
可以搞一个自己的动作库。做视频的时候直接用就行。</title>
            <link>https://nitter.cz/op7418/status/1733480631966072878#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733480631966072878#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:36:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>青龙之前写的MagicAnimate本地版本现在支持把视频或者gif转换为对应的Openpose或者其他ContorlNet动画了，这下用Animatediff做视频的时候也可以用了。<br />
可以搞一个自己的动作库。做视频的时候直接用就行。</p>
<p><a href="https://nitter.cz/bdsqlsz/status/1733478639692562792#m">nitter.cz/bdsqlsz/status/1733478639692562792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733478737017237910#m</id>
            <title>Twitter正式推出了Expanded Bios功能，你可以理解为更详细的个人简介。比如可以放你更多媒体的链接以及你的详细简历，我把我之前写的成体系的教程和做的东西放进去了，感兴趣可以来看看。
点击简介下面的查看更多就可以看到，或者可以点击下面的链接。
创建Expanded Bios的话可以点击编辑个人资料按钮拉到最下面编辑扩展简介就可以了。赶紧设置一下自己的吧。

我的Expanded Bios：https://twitter.com/op7418/bio</title>
            <link>https://nitter.cz/op7418/status/1733478737017237910#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733478737017237910#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:28:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Twitter正式推出了Expanded Bios功能，你可以理解为更详细的个人简介。比如可以放你更多媒体的链接以及你的详细简历，我把我之前写的成体系的教程和做的东西放进去了，感兴趣可以来看看。<br />
点击简介下面的查看更多就可以看到，或者可以点击下面的链接。<br />
创建Expanded Bios的话可以点击编辑个人资料按钮拉到最下面编辑扩展简介就可以了。赶紧设置一下自己的吧。<br />
<br />
我的Expanded Bios：<a href="https://nitter.cz/op7418/bio">nitter.cz/op7418/bio</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2TnR0c2JzQUE3RTdhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733469580545429860#m</id>
            <title>Mixtral-8x7b的Gradio demo，但是不能运行，因为老哥没有对应算力，需要自己本地跑 4xA10G或者3xA100。</title>
            <link>https://nitter.cz/op7418/status/1733469580545429860#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733469580545429860#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:52:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b的Gradio demo，但是不能运行，因为老哥没有对应算力，需要自己本地跑 4xA10G或者3xA100。</p>
<p><a href="https://nitter.cz/realmrfakename/status/1733293237274882150#m">nitter.cz/realmrfakename/status/1733293237274882150#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733468983599476751#m</id>
            <title>R to @op7418: 哈哈 复制的时候模型名字写错了应该是 mixtral-8x7b 哈</title>
            <link>https://nitter.cz/op7418/status/1733468983599476751#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733468983599476751#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:49:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈 复制的时候模型名字写错了应该是 mixtral-8x7b 哈</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733466929518760218#m</id>
            <title>现在去https://publish.twitter.com粘贴媒体的地址就可以把对应视频和播客的内容嵌入到网站了，试了一下推特地址还不行需要点开复制具体媒体的地址。</title>
            <link>https://nitter.cz/op7418/status/1733466929518760218#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733466929518760218#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:41:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>现在去<a href="https://publish.twitter.com">publish.twitter.com</a>粘贴媒体的地址就可以把对应视频和播客的内容嵌入到网站了，试了一下推特地址还不行需要点开复制具体媒体的地址。</p>
<p><a href="https://nitter.cz/Live/status/1733197678706852095#m">nitter.cz/Live/status/1733197678706852095#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2QzlqRmJBQUFCdlRjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733466068881191135#m</id>
            <title>Twitter的媒体Tab改成这种网格形式了，确实清晰多了，对那些经常发视频和好看图片的博主来说这个改动挺好的</title>
            <link>https://nitter.cz/op7418/status/1733466068881191135#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733466068881191135#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:38:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Twitter的媒体Tab改成这种网格形式了，确实清晰多了，对那些经常发视频和好看图片的博主来说这个改动挺好的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2Qnpyd2FZQUFBS2o1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733462829951545664#m</id>
            <title>R to @op7418: Pika制作的动画：
https://x.com/vonkleppski/status/1733197903315825116?s=20</title>
            <link>https://nitter.cz/op7418/status/1733462829951545664#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733462829951545664#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:25:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pika制作的动画：<br />
<a href="https://x.com/vonkleppski/status/1733197903315825116?s=20">x.com/vonkleppski/status/173…</a></p>
<p><a href="https://nitter.cz/vonkleppski/status/1733197903315825116#m">nitter.cz/vonkleppski/status/1733197903315825116#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733348315885138195#m</id>
            <title>RT by @op7418: 这个通过提示词局部编辑图片的项目也不错，比如你可以让图片的人物衣服换色和改变背景不改变原始人物。
相较于其他之前类似的项目，这个项目的理解更加准确对原图影响更小，同时由于利用了LCM所以速度更快。

与基于反转的方法的比较：图像编辑性能：DDCM 匹配或超过其他算法，LCM 和 UAC 带来进一步改进。值得注意的是，它的运行速度快了大约一个数量级。定性示例：InfEdit 与先前方法的比较。 InfEdit 实现了与源图像最佳一致性的编辑目标。

实现方法：尝试消除反演过程，并引入去噪扩散一致性模型（DDCM），这是一种支持虚拟反演的采样策略。 DDCM 利用扩散过程显着增强整个图像生成阶段的一致性，确保转换和细化视觉内容的保真度和速度。 还提出了统一注意力控制（UAC），用于通过自然语言进行免调整图像编辑，将交叉注意力和自注意力控制集成在统一框架内。

论文地址：https://sihanxu.github.io/InfEdit/docs/infedit.pdf</title>
            <link>https://nitter.cz/op7418/status/1733348315885138195#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733348315885138195#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:50:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个通过提示词局部编辑图片的项目也不错，比如你可以让图片的人物衣服换色和改变背景不改变原始人物。<br />
相较于其他之前类似的项目，这个项目的理解更加准确对原图影响更小，同时由于利用了LCM所以速度更快。<br />
<br />
与基于反转的方法的比较：图像编辑性能：DDCM 匹配或超过其他算法，LCM 和 UAC 带来进一步改进。值得注意的是，它的运行速度快了大约一个数量级。定性示例：InfEdit 与先前方法的比较。 InfEdit 实现了与源图像最佳一致性的编辑目标。<br />
<br />
实现方法：尝试消除反演过程，并引入去噪扩散一致性模型（DDCM），这是一种支持虚拟反演的采样策略。 DDCM 利用扩散过程显着增强整个图像生成阶段的一致性，确保转换和细化视觉内容的保真度和速度。 还提出了统一注意力控制（UAC），用于通过自然语言进行免调整图像编辑，将交叉注意力和自注意力控制集成在统一框架内。<br />
<br />
论文地址：<a href="https://sihanxu.github.io/InfEdit/docs/infedit.pdf">sihanxu.github.io/InfEdit/do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WEZnUGJrQUFFSmRTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733344034822000755#m</id>
            <title>RT by @op7418: WonderJourney这个项目有点厉害啊，只需要1张图片就可以创建3D场景动画，从用户提供的任何位置（通过文本描述或图像）开始，并通过一系列不同但连贯的 3D 场景生成一个旅程。
从演示效果来看非常流畅，3D游戏或者影视的场景创建要变简单了。而且这还是最近罕见的谷歌会开源的研究。

主要能力：
◆ 从任意位置（由文本或图像指定）开始，WonderJourney 沿着相机轨迹生成一系列多样化但连贯连接的 3D 场景。
◆ 从同一个地点开始，WonderJourney 可以生成一组不同的“奇妙旅程”，并在不同的目的地结束。使用相机姿势的轨迹渲染下面的每个视频。
◆ WonderJourney 还可以根据一系列文本描述（例如诗歌、俳句和故事摘要）生成受控的奇妙旅程。

WonderJourney的方法论：
场景描述生成：详细说明了为随后的场景生成文本描述的过程。
视觉场景生成：解释了系统如何根据文本描述和当前场景的3D表示来生成下一个3D场景。
视觉验证：讨论了系统如何验证所生成的场景是否存在不良视觉效果，并根据需要进行调整。

实验：展示了用于评估的数据集和基准，生成旅程的定性演示，额外的评估，以及人类偏好评估，突出了WonderJourney在生成多样且连贯的3D场景方面的有效性。

论文地址：https://arxiv.org/pdf/2312.03884.pdf</title>
            <link>https://nitter.cz/op7418/status/1733344034822000755#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733344034822000755#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:33:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WonderJourney这个项目有点厉害啊，只需要1张图片就可以创建3D场景动画，从用户提供的任何位置（通过文本描述或图像）开始，并通过一系列不同但连贯的 3D 场景生成一个旅程。<br />
从演示效果来看非常流畅，3D游戏或者影视的场景创建要变简单了。而且这还是最近罕见的谷歌会开源的研究。<br />
<br />
主要能力：<br />
◆ 从任意位置（由文本或图像指定）开始，WonderJourney 沿着相机轨迹生成一系列多样化但连贯连接的 3D 场景。<br />
◆ 从同一个地点开始，WonderJourney 可以生成一组不同的“奇妙旅程”，并在不同的目的地结束。使用相机姿势的轨迹渲染下面的每个视频。<br />
◆ WonderJourney 还可以根据一系列文本描述（例如诗歌、俳句和故事摘要）生成受控的奇妙旅程。<br />
<br />
WonderJourney的方法论：<br />
场景描述生成：详细说明了为随后的场景生成文本描述的过程。<br />
视觉场景生成：解释了系统如何根据文本描述和当前场景的3D表示来生成下一个3D场景。<br />
视觉验证：讨论了系统如何验证所生成的场景是否存在不良视觉效果，并根据需要进行调整。<br />
<br />
实验：展示了用于评估的数据集和基准，生成旅程的定性演示，额外的评估，以及人类偏好评估，突出了WonderJourney在生成多样且连贯的3D场景方面的有效性。<br />
<br />
论文地址：<a href="https://arxiv.org/pdf/2312.03884.pdf">arxiv.org/pdf/2312.03884.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMzNDMzNzc1OTEzMjg3NjgvcHUvaW1nL01PVmE0VjJSZm1jOEJLYjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733459876075180471#m</id>
            <title>R to @op7418: 贴一下RealJosephus截图的源推：
https://x.com/RealJosephus/status/1733321066104430936?s=20</title>
            <link>https://nitter.cz/op7418/status/1733459876075180471#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733459876075180471#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>贴一下RealJosephus截图的源推：<br />
<a href="https://x.com/RealJosephus/status/1733321066104430936?s=20">x.com/RealJosephus/status/17…</a></p>
<p><a href="https://nitter.cz/RealJosephus/status/1733321066104430936#m">nitter.cz/RealJosephus/status/1733321066104430936#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733459395672244503#m</id>
            <title>重新发一下MoE 8x7B的介绍原来的删掉了，由于没有在HF模型排行上找到我就直接复制了@RealJosephus的HF截图，这里向他道歉。一般只要是推特的原推有的我都会尽量转推MoE 8x7B由于官方没有说明很多内容都是拼凑的就没有把参考的内容都粘过来。
我并不是专门研究LLM的所以很多事情肯定说的不一定严谨，如果有问题欢迎指出。能改的我一般都会改，改不了的我会在下面贴上。但是我依然觉得不应该上来就骂人。

昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。
之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。
MoE 8x7B测试分数来源于第一个链接。

下面是MoE架构LLM的简单介绍：
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。
MoE架构为LLMs提供了几个优势：
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数。这使得能够开发更强大的模型，而无需成比例地增加计算要求。
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B。
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域。
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。  MoE的模型也有两个问题： MoE 模型比普通密集模型更难微调； MoE 模型会消耗大量显存。

模型下载：https://huggingface.co/DiscoResearch/mixtral-7b-8expert
在线试用：https://replicate.com/nateraw/mixtral-8x7b-32kseqlen</title>
            <link>https://nitter.cz/op7418/status/1733459395672244503#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733459395672244503#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:11:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>重新发一下MoE 8x7B的介绍原来的删掉了，由于没有在HF模型排行上找到我就直接复制了<a href="https://nitter.cz/RealJosephus" title="Joseph Cheung">@RealJosephus</a>的HF截图，这里向他道歉。一般只要是推特的原推有的我都会尽量转推MoE 8x7B由于官方没有说明很多内容都是拼凑的就没有把参考的内容都粘过来。<br />
我并不是专门研究LLM的所以很多事情肯定说的不一定严谨，如果有问题欢迎指出。能改的我一般都会改，改不了的我会在下面贴上。但是我依然觉得不应该上来就骂人。<br />
<br />
昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。<br />
之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。<br />
MoE 8x7B测试分数来源于第一个链接。<br />
<br />
下面是MoE架构LLM的简单介绍：<br />
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。<br />
MoE架构为LLMs提供了几个优势：<br />
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数。这使得能够开发更强大的模型，而无需成比例地增加计算要求。<br />
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B。<br />
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域。<br />
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。  MoE的模型也有两个问题： MoE 模型比普通密集模型更难微调； MoE 模型会消耗大量显存。<br />
<br />
模型下载：<a href="https://huggingface.co/DiscoResearch/mixtral-7b-8expert">huggingface.co/DiscoResearch…</a><br />
在线试用：<a href="https://replicate.com/nateraw/mixtral-8x7b-32kseqlen">replicate.com/nateraw/mixtra…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E1N3FrVGFVQUVwMGE3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733451932528992494#m</id>
            <title>这是你的源推，可能我理解的意思有误，我也去HF的榜单找了没找到，就没有自己截图复制了你的，从得分上来看确实比其他的强一些，可能我表述不够严谨，另外这个确实不是7B的模型是多个7B模型的组合。
模型底层的了解我确实不够专业，如果有问题我觉得可以理性讨论或者指出能改的我都会改掉，不能改的一般也会在推下补充。人身攻击没什么意思了。如果你不希望我用你的截图我会删掉https://x.com/RealJosephus/status/1733321066104430936?s=20</title>
            <link>https://nitter.cz/op7418/status/1733451932528992494#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733451932528992494#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 11:42:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是你的源推，可能我理解的意思有误，我也去HF的榜单找了没找到，就没有自己截图复制了你的，从得分上来看确实比其他的强一些，可能我表述不够严谨，另外这个确实不是7B的模型是多个7B模型的组合。<br />
模型底层的了解我确实不够专业，如果有问题我觉得可以理性讨论或者指出能改的我都会改掉，不能改的一般也会在推下补充。人身攻击没什么意思了。如果你不希望我用你的截图我会删掉<a href="https://x.com/RealJosephus/status/1733321066104430936?s=20">x.com/RealJosephus/status/17…</a></p>
<p><a href="https://nitter.cz/RealJosephus/status/1733437510532120626#m">nitter.cz/RealJosephus/status/1733437510532120626#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733350885873611224#m</id>
            <title>Animatediff做的雷神电影转动漫效果，稳定性很好。</title>
            <link>https://nitter.cz/op7418/status/1733350885873611224#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733350885873611224#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 05:00:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Animatediff做的雷神电影转动漫效果，稳定性很好。</p>
<p><a href="https://nitter.cz/InnerRefle11312/status/1733202194424386019#m">nitter.cz/InnerRefle11312/status/1733202194424386019#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>