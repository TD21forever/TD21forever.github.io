<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735585111658389656#m</id>
            <title>RT by @op7418: 最近很多人用 LCM 加上Animatediff 和 Contorlnet 代替 Blender 的渲染。
比如这个就是用了右边的视频在 Blender 上面随便加了点胡子和眼镜然后用 LCM 生成的视频。
他用 LCM 的时候的参数是 8 步，CFG 2，采样器是Euler A。
我之前一直用 ComfyUI 那个 lcm 的采样器，生成的 Animatediff 的视频质量确实不好，看了这个帖子换了Euler A确实稳定多了。

原贴地址：https://www.reddit.com/r/StableDiffusion/comments/18isc5q/the_captain_30_seconds_temporal_consistency/</title>
            <link>https://nitter.cz/op7418/status/1735585111658389656#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735585111658389656#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 08:58:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近很多人用 LCM 加上Animatediff 和 Contorlnet 代替 Blender 的渲染。<br />
比如这个就是用了右边的视频在 Blender 上面随便加了点胡子和眼镜然后用 LCM 生成的视频。<br />
他用 LCM 的时候的参数是 8 步，CFG 2，采样器是Euler A。<br />
我之前一直用 ComfyUI 那个 lcm 的采样器，生成的 Animatediff 的视频质量确实不好，看了这个帖子换了Euler A确实稳定多了。<br />
<br />
原贴地址：<a href="https://teddit.net/r/StableDiffusion/comments/18isc5q/the_captain_30_seconds_temporal_consistency/">teddit.net/r/StableDiffusion…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU1ODAzNDgzODg2Mzg3MjAvcHUvaW1nL2pGVUJJWlVtRDNRcmVtcmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735600276349092076#m</id>
            <title>RT by @op7418: Runway 上线文字生成语音功能，免费使用。
发现前几天说的文字生成语音的功能，Runway 悄悄的上了。尝试了一下英文的效果真的很好，感情很丰富自然。
中文还是老问题，有外国人口音，这块可能不能指望外国公司了。
这个功能可以选的语音模型非常多，可以都试试。目前是消耗右上角点数生成可以用好久。下面是我的测试录音。
可以来试试：https://app.runwayml.com</title>
            <link>https://nitter.cz/op7418/status/1735600276349092076#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735600276349092076#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 09:58:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 上线文字生成语音功能，免费使用。<br />
发现前几天说的文字生成语音的功能，Runway 悄悄的上了。尝试了一下英文的效果真的很好，感情很丰富自然。<br />
中文还是老问题，有外国人口音，这块可能不能指望外国公司了。<br />
这个功能可以选的语音模型非常多，可以都试试。目前是消耗右上角点数生成可以用好久。下面是我的测试录音。<br />
可以来试试：<a href="https://app.runwayml.com">app.runwayml.com</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU1OTkxMjA0MzIzOTQyNDAvcHUvaW1nL3M0Wmhwa1ZUcnlZOVlrYmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735612535909532028#m</id>
            <title>RT by @op7418: 卧槽，这个 LLM 算法的可视化演示太强了，之前我们看到的都是 2D 的，这个是 3D 的。
而且他完整的展示了整个 LLM不同模块内部的运作机制和各模块之间的联系。
你还可以看 GPT-3 和 GPT-2 这种不同规模的 LLM 在架构和模块上的区别。

来玩玩看。https://bbycroft.net/llm</title>
            <link>https://nitter.cz/op7418/status/1735612535909532028#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735612535909532028#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 10:47:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽，这个 LLM 算法的可视化演示太强了，之前我们看到的都是 2D 的，这个是 3D 的。<br />
而且他完整的展示了整个 LLM不同模块内部的运作机制和各模块之间的联系。<br />
你还可以看 GPT-3 和 GPT-2 这种不同规模的 LLM 在架构和模块上的区别。<br />
<br />
来玩玩看。<a href="https://bbycroft.net/llm">bbycroft.net/llm</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU2MTI0NTI3NTc1MTYyODgvcHUvaW1nLzIzRVNEWVR0cHdoNFVDV1AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735683910703722625#m</id>
            <title>RT by @op7418: 来了。一个视频展示 #animatediff V2模型、V3模型、V3模型+Adapter LoRA的效果。
说一下我的观察，V3的画面表现力和细节确实比V2高很多，加上Adapter LoRA之后视频的连贯性和画面丰富度更高了，但是画面风格发生了变化。接下来我会测一下不同权重的影响。
这条推下面会有个简单的教程，顺便发一下ComfyUI工作流，各位可以自己试一下。</title>
            <link>https://nitter.cz/op7418/status/1735683910703722625#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735683910703722625#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:31:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来了。一个视频展示 <a href="https://nitter.cz/search?q=%23animatediff">#animatediff</a> V2模型、V3模型、V3模型+Adapter LoRA的效果。<br />
说一下我的观察，V3的画面表现力和细节确实比V2高很多，加上Adapter LoRA之后视频的连贯性和画面丰富度更高了，但是画面风格发生了变化。接下来我会测一下不同权重的影响。<br />
这条推下面会有个简单的教程，顺便发一下ComfyUI工作流，各位可以自己试一下。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU2ODI5NDEzNTM5NDMwNDAvcHUvaW1nL2Utckx1UnpUZ0F3VkRsZWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735736967487459833#m</id>
            <title>RT by @op7418: 还能这样？在跟Chatgpt语音聊天的时候也可以调用DALL-E3</title>
            <link>https://nitter.cz/op7418/status/1735736967487459833#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735736967487459833#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 19:01:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还能这样？在跟Chatgpt语音聊天的时候也可以调用DALL-E3</p>
<p><a href="https://nitter.cz/gopatrik/status/1735479721134313934#m">nitter.cz/gopatrik/status/1735479721134313934#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735548269391196518#m</id>
            <title>RT by @op7418: a16z昨天公布了他们的开源 AI 资助计划第二期，开个帖子记录一下这几个项目。
这个计划主要关注两个领域：包括用于训练、托管和评估语言模型的工具；以及围绕视觉人工智能构建的模型和社区。
这个资助不是投资，所以是不需要回报的，感觉像是赠予。
第二期总共 7 个项目他们分别是🧵：</title>
            <link>https://nitter.cz/op7418/status/1735548269391196518#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735548269391196518#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 06:32:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>a16z昨天公布了他们的开源 AI 资助计划第二期，开个帖子记录一下这几个项目。<br />
这个计划主要关注两个领域：包括用于训练、托管和评估语言模型的工具；以及围绕视觉人工智能构建的模型和社区。<br />
这个资助不是投资，所以是不需要回报的，感觉像是赠予。<br />
第二期总共 7 个项目他们分别是🧵：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JYbjh4X2JZQUFBZkVxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735709055065395489#m</id>
            <title>上岸了啊，AI艺术创作者Nicolas加入了Runway</title>
            <link>https://nitter.cz/op7418/status/1735709055065395489#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735709055065395489#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 17:11:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>上岸了啊，AI艺术创作者Nicolas加入了Runway</p>
<p><a href="https://nitter.cz/iamneubert/status/1735706312829231354#m">nitter.cz/iamneubert/status/1735706312829231354#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735696564872708205#m</id>
            <title>Framer免费用户的CMS配额提高了十倍，可以白嫖爽了。</title>
            <link>https://nitter.cz/op7418/status/1735696564872708205#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735696564872708205#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 16:21:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Framer免费用户的CMS配额提高了十倍，可以白嫖爽了。</p>
<p><a href="https://nitter.cz/framer/status/1735661692967891323#m">nitter.cz/framer/status/1735661692967891323#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735692744063148373#m</id>
            <title>R to @op7418: 测试了一下V3和V2的对比：
https://x.com/op7418/status/1735683910703722625?s=20</title>
            <link>https://nitter.cz/op7418/status/1735692744063148373#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735692744063148373#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 16:06:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试了一下V3和V2的对比：<br />
<a href="https://x.com/op7418/status/1735683910703722625?s=20">x.com/op7418/status/17356839…</a></p>
<p><a href="https://nitter.cz/op7418/status/1735683910703722625#m">nitter.cz/op7418/status/1735683910703722625#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735690952092881313#m</id>
            <title>R to @op7418: 补充一个将Domain Adapter LoRA用正常Lora加载器加载的工作流，各位可以自己尝试。</title>
            <link>https://nitter.cz/op7418/status/1735690952092881313#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735690952092881313#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:59:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补充一个将Domain Adapter LoRA用正常Lora加载器加载的工作流，各位可以自己尝试。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JacHNYR2JVQUF0RmdWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735689072432324640#m</id>
            <title>R to @op7418: 补充一个视频，作者解释是用 Domain Adapter LoRA 进行图像模型微调，所以有可能是放在正常的lora加载器上的，上面的视频是放在正常Lora加载器上的效果。
这个视频的lora是放在Animatediff Lora加载器上的效果。</title>
            <link>https://nitter.cz/op7418/status/1735689072432324640#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735689072432324640#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:51:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补充一个视频，作者解释是用 Domain Adapter LoRA 进行图像模型微调，所以有可能是放在正常的lora加载器上的，上面的视频是放在正常Lora加载器上的效果。<br />
这个视频的lora是放在Animatediff Lora加载器上的效果。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU2ODg1ODc3ODQ2OTU4MDgvcHUvaW1nLzNzZWtwLWRLdjFvWGhDU2EuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735688278177956151#m</id>
            <title>R to @op7418: 这个是工作流，就改了一下模型。保存图片拖到Comfyui里面就行。</title>
            <link>https://nitter.cz/op7418/status/1735688278177956151#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735688278177956151#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:48:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是工作流，就改了一下模型。保存图片拖到Comfyui里面就行。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JabktzOWJJQUFlZ1RLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735687929811575079#m</id>
            <title>R to @op7418: 注意：我现在是将v3_adapter_sd_v15.ckpt放在Animatediff节点上的，但是作者没细说这个怎么用所以，我的用法有可能是错的，有大神的话可以指点一下。</title>
            <link>https://nitter.cz/op7418/status/1735687929811575079#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735687929811575079#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:47:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>注意：我现在是将v3_adapter_sd_v15.ckpt放在Animatediff节点上的，但是作者没细说这个怎么用所以，我的用法有可能是错的，有大神的话可以指点一下。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JabXYwMWJFQUF3WVp4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735687561354543226#m</id>
            <title>R to @op7418: 然后我们在Animatediff节点将模型更换为v3_sd15_mm.ckpt.ckpt</title>
            <link>https://nitter.cz/op7418/status/1735687561354543226#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735687561354543226#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:45:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>然后我们在Animatediff节点将模型更换为v3_sd15_mm.ckpt.ckpt</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JabVRCYmFzQUFYSHFCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735685500919542227#m</id>
            <title>R to @op7418: 这里下载模型：https://huggingface.co/guoyww/animatediff/tree/main</title>
            <link>https://nitter.cz/op7418/status/1735685500919542227#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735685500919542227#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:37:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这里下载模型：<a href="https://huggingface.co/guoyww/animatediff/tree/main">huggingface.co/guoyww/animat…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDA1MzYwODc2Nzg2NDgzMi9VdVI3TklOdD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735685422288900598#m</id>
            <title>R to @op7418: 简单说一下V3模型怎么用，首先需要在Huggingface下载新的模型。
包括v3_adapter_sd_v15.ckpt和v3_sd15_mm.ckpt.ckpt，那两个SparseCtrl模型现在还不知道怎么用。
v3_sd15_mm.ckpt.ckpt放在插件的模型文件夹。v3_adapter_sd_v15.ckpt放在Lora文件夹就行。</title>
            <link>https://nitter.cz/op7418/status/1735685422288900598#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735685422288900598#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:37:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>简单说一下V3模型怎么用，首先需要在Huggingface下载新的模型。<br />
包括v3_adapter_sd_v15.ckpt和v3_sd15_mm.ckpt.ckpt，那两个SparseCtrl模型现在还不知道怎么用。<br />
v3_sd15_mm.ckpt.ckpt放在插件的模型文件夹。v3_adapter_sd_v15.ckpt放在Lora文件夹就行。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Jaa25JNmIwQUFYZGxrLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735674459418194188#m</id>
            <title>R to @op7418: 这是作者的介绍推：
https://x.com/CeyuanY/status/1735657553504477517?s=20</title>
            <link>https://nitter.cz/op7418/status/1735674459418194188#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735674459418194188#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 14:53:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是作者的介绍推：<br />
<a href="https://x.com/CeyuanY/status/1735657553504477517?s=20">x.com/CeyuanY/status/1735657…</a></p>
<p><a href="https://nitter.cz/CeyuanY/status/1735657553504477517#m">nitter.cz/CeyuanY/status/1735657553504477517#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1735673644309795061#m</id>
            <title>过年了！#animatediff 今天更新V3版本的运动模型，同时更新了作者前几天发布了视频控制模型SparseCtrl。

SparseCtrl可以理解为转为视频优化过的Contorlnet，可以通过输入关键帧的深度或者涂鸦图像控制视频按照指定的方式运动和过度。
这个项目一定程度上解决了现在Animatediff生成视频过程中无法控制的问题。
目前发布了两个SparseCtrl模型分别是深度和涂鸦控制。
已经在下载模型测试了。下面是演示视频。

项目地址https://github.com/guoyww/AnimateDiff</title>
            <link>https://nitter.cz/op7418/status/1735673644309795061#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1735673644309795061#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 14:50:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>过年了！<a href="https://nitter.cz/search?q=%23animatediff">#animatediff</a> 今天更新V3版本的运动模型，同时更新了作者前几天发布了视频控制模型SparseCtrl。<br />
<br />
SparseCtrl可以理解为转为视频优化过的Contorlnet，可以通过输入关键帧的深度或者涂鸦图像控制视频按照指定的方式运动和过度。<br />
这个项目一定程度上解决了现在Animatediff生成视频过程中无法控制的问题。<br />
目前发布了两个SparseCtrl模型分别是深度和涂鸦控制。<br />
已经在下载模型测试了。下面是演示视频。<br />
<br />
项目地址<a href="https://github.com/guoyww/AnimateDiff">github.com/guoyww/AnimateDif…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU2NzM1MTc2MTY1Mzc2MDAvcHUvaW1nL2dQZnp3Y1o3NDRJNUgwaHQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>