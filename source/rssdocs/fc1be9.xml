<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734834548062720190#m</id>
            <title>R to @op7418: 他们要再这么搞开源名声都臭了</title>
            <link>https://nitter.cz/op7418/status/1734834548062720190#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734834548062720190#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:16:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他们要再这么搞开源名声都臭了</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734834158869123480#m</id>
            <title>阿里期货开源，再发新项目：虚拟服装试穿。只需要一张人物照片和服装照片就可以让那个人穿上这个衣服，对服装行业是个利好。
效果确实很好，但是全是期货开源很容易被当成诈骗啊。
Reddit 有个宣传这个项目的帖子，下面全是在喷，用不到就是不存在。
还可以跟前几天发的Animate-Anyone项目结合生成跳舞视频。

简介：
Outfit Anyone通过利用双流条件扩散模型来解决这些限制，使其能够熟练处理服装变形，从而获得更逼真的结果。它通过可扩展性（调节姿势和身体形状等因素）和广泛适用性（从动漫到野外图像）来区别于其他方法。Outfit Anyone在各种场景中的表现突显了它在实际部署中的实用性和准备就绪性。

实现方法：
核心是条件扩散模型，它处理模特、服装和相关文本提示的图像，以服装图像作为控制因素。在内部，网络分为两个流，独立处理模特和服装数据。这些流在融合网络中汇聚，便于将服装细节嵌入模特的特征表示中。在此基础上，我们建立了Outfit Anyone，包括两个关键元素：用于初始试穿图像的零样本试穿网络，以及用于增强输出图像中服装和皮肤纹理的事后优化器。

项目地址：https://humanaigc.github.io/outfit-anyone/</title>
            <link>https://nitter.cz/op7418/status/1734834158869123480#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734834158869123480#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里期货开源，再发新项目：虚拟服装试穿。只需要一张人物照片和服装照片就可以让那个人穿上这个衣服，对服装行业是个利好。<br />
效果确实很好，但是全是期货开源很容易被当成诈骗啊。<br />
Reddit 有个宣传这个项目的帖子，下面全是在喷，用不到就是不存在。<br />
还可以跟前几天发的Animate-Anyone项目结合生成跳舞视频。<br />
<br />
简介：<br />
Outfit Anyone通过利用双流条件扩散模型来解决这些限制，使其能够熟练处理服装变形，从而获得更逼真的结果。它通过可扩展性（调节姿势和身体形状等因素）和广泛适用性（从动漫到野外图像）来区别于其他方法。Outfit Anyone在各种场景中的表现突显了它在实际部署中的实用性和准备就绪性。<br />
<br />
实现方法：<br />
核心是条件扩散模型，它处理模特、服装和相关文本提示的图像，以服装图像作为控制因素。在内部，网络分为两个流，独立处理模特和服装数据。这些流在融合网络中汇聚，便于将服装细节嵌入模特的特征表示中。在此基础上，我们建立了Outfit Anyone，包括两个关键元素：用于初始试穿图像的零样本试穿网络，以及用于增强输出图像中服装和皮肤纹理的事后优化器。<br />
<br />
项目地址：<a href="https://humanaigc.github.io/outfit-anyone/">humanaigc.github.io/outfit-a…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MzM2OTY4ODMzMzEwNzIvcHUvaW1nL3FsaG1TQl9ManRXRkp5V00uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734830577474490544#m</id>
            <title>R to @op7418: Midjourney 官方推特刚才点赞了这条，麻了金 V 都掉了，我以为高仿呢。</title>
            <link>https://nitter.cz/op7418/status/1734830577474490544#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734830577474490544#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:00:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney 官方推特刚才点赞了这条，麻了金 V 都掉了，我以为高仿呢。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734829583328039177#m</id>
            <title>MJ Discord 有个频道的人写了一下 Midjourney 风格微调的原理，跟我原来想的完全不一样，原来提示词的作用没那么大。哈哈哈哈。

简单说一下结论：

✱无论使用什么提示词，风格调整器中两列 128 张图片的风格是固定的。
也就是说你用两个不同的提示词微调选择相同位置的图片也会获得一个相同的风格，比如下面的图是用不同的风格生成的，完全一样。

✱生成风格的提示词只对风格调整时的图像起作用，不会对使用风格的图像起作用。

✱你的提示词只有跟原提示词类似才能够获得跟风格调节器里面图片的效果。如果用的提示词跟原来的一点关系没有就有可能得到完全不同的结果。

那个帖子的原文翻译：

免责声明：此内容非 MidJourney 官方文档，仅代表个人观点，未经官方认证。

✱ 风格调节器是由 128 行和 3 列构成的网格。每个网格位置，如第 1 行左列（1L），在所有网格中完全一致，且风格指向的排序亦然。也就是说，如果你在十个不同的调节器中选择了 1L、6R、9L 和 11R，最终的风格效果将完全相同。
✱ 因此，你在风格调节器中所用的提示仅影响调节器内生成的图片，不会影响后续图片，除非使用同一提示语。
✱ 这里展示了四种不同风格调节器，使用同一调节方向生成的作品。

如果不同调节器生成的图片大相径庭，会影响用这个调节器生成的图片吗？
不会，因为提示仅对调节器内的图片产生影响，而非用调节器生成的图片。_那为何会有这么多不同的调节器？_ 因为每个调节器生成的图片都反映了所用的提示语，使你能通过不同风格直观感受你的提示。

我必须用类似于制作调节器时的提示语吗？
你的提示越接近原始提示，你得到的图片就越接近调节器内的图片。若使用不同提示，根据选择的风格方向，结果可能会截然不同。

那么，使用风格调节器的意义何在？
举例来说，对于 `picture of a woman` 这类提示，在有调节器之前，可用的风格非常有限。现在，调节器能展示出 256 种不同的 `picture of a woman` 风格，从喷笔到怪异、铅笔画到真实照片、鲜艳到柔和、繁复到简约、写实到超现实等。你可以选择多种或少量这些风格方向来创造自己的独特风格。若你的后续提示语接近调节器的提示，风格将非常可预测。若使用不同提示，可能会有意想不到的结果，但依然比默认的 MJ 提供了更多的风格选择。

原贴地址：https://discord.com/channels/662267976984297473/1179745230887591957</title>
            <link>https://nitter.cz/op7418/status/1734829583328039177#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734829583328039177#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 06:56:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MJ Discord 有个频道的人写了一下 Midjourney 风格微调的原理，跟我原来想的完全不一样，原来提示词的作用没那么大。哈哈哈哈。<br />
<br />
简单说一下结论：<br />
<br />
✱无论使用什么提示词，风格调整器中两列 128 张图片的风格是固定的。<br />
也就是说你用两个不同的提示词微调选择相同位置的图片也会获得一个相同的风格，比如下面的图是用不同的风格生成的，完全一样。<br />
<br />
✱生成风格的提示词只对风格调整时的图像起作用，不会对使用风格的图像起作用。<br />
<br />
✱你的提示词只有跟原提示词类似才能够获得跟风格调节器里面图片的效果。如果用的提示词跟原来的一点关系没有就有可能得到完全不同的结果。<br />
<br />
那个帖子的原文翻译：<br />
<br />
免责声明：此内容非 MidJourney 官方文档，仅代表个人观点，未经官方认证。<br />
<br />
✱ 风格调节器是由 128 行和 3 列构成的网格。每个网格位置，如第 1 行左列（1L），在所有网格中完全一致，且风格指向的排序亦然。也就是说，如果你在十个不同的调节器中选择了 1L、6R、9L 和 11R，最终的风格效果将完全相同。<br />
✱ 因此，你在风格调节器中所用的提示仅影响调节器内生成的图片，不会影响后续图片，除非使用同一提示语。<br />
✱ 这里展示了四种不同风格调节器，使用同一调节方向生成的作品。<br />
<br />
如果不同调节器生成的图片大相径庭，会影响用这个调节器生成的图片吗？<br />
不会，因为提示仅对调节器内的图片产生影响，而非用调节器生成的图片。_那为何会有这么多不同的调节器？_ 因为每个调节器生成的图片都反映了所用的提示语，使你能通过不同风格直观感受你的提示。<br />
<br />
我必须用类似于制作调节器时的提示语吗？<br />
你的提示越接近原始提示，你得到的图片就越接近调节器内的图片。若使用不同提示，根据选择的风格方向，结果可能会截然不同。<br />
<br />
那么，使用风格调节器的意义何在？<br />
举例来说，对于 `picture of a woman` 这类提示，在有调节器之前，可用的风格非常有限。现在，调节器能展示出 256 种不同的 `picture of a woman` 风格，从喷笔到怪异、铅笔画到真实照片、鲜艳到柔和、繁复到简约、写实到超现实等。你可以选择多种或少量这些风格方向来创造自己的独特风格。若你的后续提示语接近调节器的提示，风格将非常可预测。若使用不同提示，可能会有意想不到的结果，但依然比默认的 MJ 提供了更多的风格选择。<br />
<br />
原贴地址：<a href="https://discord.com/channels/662267976984297473/1179745230887591957">discord.com/channels/6622679…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JOYURsM2FVQUF0QkNZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734818295977857258#m</id>
            <title>R to @op7418: 终于开始卷视频之外的东西了</title>
            <link>https://nitter.cz/op7418/status/1734818295977857258#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734818295977857258#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 06:11:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于开始卷视频之外的东西了</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734818099659174081#m</id>
            <title>Runway 即将推出文本生成语音工具，支持多种语言，从演示来看音质很好，而且情绪和停顿都很自然。</title>
            <link>https://nitter.cz/op7418/status/1734818099659174081#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734818099659174081#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 06:10:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 即将推出文本生成语音工具，支持多种语言，从演示来看音质很好，而且情绪和停顿都很自然。</p>
<p><a href="https://nitter.cz/iamneubert/status/1734721951841546463#m">nitter.cz/iamneubert/status/1734721951841546463#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MTc4MjE3NzM5Nzk2NDgvcHUvaW1nL1ZMVVRrS0NjdmtlM1BNTVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734805589174235416#m</id>
            <title>RT by @op7418: 微软今天发布小模型 Phi-2 的新闻关注度还蛮大的，只有2.7B参数（Llama 2最小的模型都是 7B的），模型越小就对设备要求越低，甚至于移动设备都能运行。但模型的能力又跟模型的训练量和参数量息息相关，数据量越大参数越大能力越强。

微软的解决方案是提升数据质量，通过专注于高质量的“教科书级”数据，训练数据混合了专门为教授模型常识推理和广泛知识（包括科学、日常生活和心理理论等）而设计的合成数据集。此外，还精心挑选了基于教育价值和内容质量筛选的网络数据，进一步丰富了训练语料库。

Phi-2 的训练数据有 1.4 T 个 token，在 96 块 A100 GPU 上训练了 14 天。

Phi-2 是一个基础模型，没有经过人类反馈强化学习（RLHF）的校准，也没有进行过指令式微调。

从能力上来说，据微软自己公布的数据：2.7B 的 Phi-2 超过了目前开源领域表现最好的 Mistral 和 Llama-2 7B 和 13B，编程能力甚至超过了 Llama-2-70B。

至于是不是真的这么强还得看看用户的反馈。

详情可以看官方博客：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/

也可以看我翻译的版本：《Phi-2：小语言模型的非凡实力 [译]》
https://baoyu.io/translations/microsoft/phi-2-the-surprising-power-of-small-language-models</title>
            <link>https://nitter.cz/dotey/status/1734805589174235416#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734805589174235416#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:20:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软今天发布小模型 Phi-2 的新闻关注度还蛮大的，只有2.7B参数（Llama 2最小的模型都是 7B的），模型越小就对设备要求越低，甚至于移动设备都能运行。但模型的能力又跟模型的训练量和参数量息息相关，数据量越大参数越大能力越强。<br />
<br />
微软的解决方案是提升数据质量，通过专注于高质量的“教科书级”数据，训练数据混合了专门为教授模型常识推理和广泛知识（包括科学、日常生活和心理理论等）而设计的合成数据集。此外，还精心挑选了基于教育价值和内容质量筛选的网络数据，进一步丰富了训练语料库。<br />
<br />
Phi-2 的训练数据有 1.4 T 个 token，在 96 块 A100 GPU 上训练了 14 天。<br />
<br />
Phi-2 是一个基础模型，没有经过人类反馈强化学习（RLHF）的校准，也没有进行过指令式微调。<br />
<br />
从能力上来说，据微软自己公布的数据：2.7B 的 Phi-2 超过了目前开源领域表现最好的 Mistral 和 Llama-2 7B 和 13B，编程能力甚至超过了 Llama-2-70B。<br />
<br />
至于是不是真的这么强还得看看用户的反馈。<br />
<br />
详情可以看官方博客：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a><br />
<br />
也可以看我翻译的版本：《Phi-2：小语言模型的非凡实力 [译]》<br />
<a href="https://baoyu.io/translations/microsoft/phi-2-the-surprising-power-of-small-language-models">baoyu.io/translations/micros…</a></p>
<p><a href="https://nitter.cz/MSFTResearch/status/1734609807770898674#m">nitter.cz/MSFTResearch/status/1734609807770898674#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734807181843984459#m</id>
            <title>试了一下明确说不会写性内容和性暴力内容，可能得需要一些提示词技巧了。</title>
            <link>https://nitter.cz/op7418/status/1734807181843984459#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734807181843984459#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:27:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>试了一下明确说不会写性内容和性暴力内容，可能得需要一些提示词技巧了。</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1734773939367452831#m">nitter.cz/xiaohuggg/status/1734773939367452831#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JORnc5TmJrQUFFWm9vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734797727958179925#m</id>
            <title>应该是蒙版重绘的，选的这个镜头也很好，没有大幅动作手部动作也少。</title>
            <link>https://nitter.cz/op7418/status/1734797727958179925#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734797727958179925#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:49:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>应该是蒙版重绘的，选的这个镜头也很好，没有大幅动作手部动作也少。</p>
<p><a href="https://nitter.cz/AIWarper/status/1734774527098511497#m">nitter.cz/AIWarper/status/1734774527098511497#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734469157423648991#m</id>
            <title>RT by @op7418: 刚注意到李飞飞团队的这个视频生成模型W.A.L.T，这效果也太好了，感觉比 Pika 1.0 还要好的多。
清晰度和动作都非常好，特别是光剑打斗的那个视频。可惜不开源。
我补充了一些项目页面的演示视频。

项目简介：
方法有两个关键的设计决策。首先，我们使用因果编码器在统一的潜在空间内联合压缩图像和视频，从而实现跨模态的训练和生成。其次，为了提高记忆和训练效率，我们使用专为联合空间和时空生成建模而定制的窗口注意架构。总而言之，这些设计决策使我们能够在已建立的视频（UCF-101 和 Kinetics-600）和图像（ImageNet）生成基准上实现最先进的性能，而无需使用无分类器指导。
最后，我们还训练了三个用于文本到视频生成任务的级联模型，其中包括一个基本潜在视频扩散模型和两个视频超分辨率扩散模型，以每秒 8 帧的速度生成 512 x 896 分辨率的视频。

项目地址：https://walt-video-diffusion.github.io/</title>
            <link>https://nitter.cz/op7418/status/1734469157423648991#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734469157423648991#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 07:04:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚注意到李飞飞团队的这个视频生成模型W.A.L.T，这效果也太好了，感觉比 Pika 1.0 还要好的多。<br />
清晰度和动作都非常好，特别是光剑打斗的那个视频。可惜不开源。<br />
我补充了一些项目页面的演示视频。<br />
<br />
项目简介：<br />
方法有两个关键的设计决策。首先，我们使用因果编码器在统一的潜在空间内联合压缩图像和视频，从而实现跨模态的训练和生成。其次，为了提高记忆和训练效率，我们使用专为联合空间和时空生成建模而定制的窗口注意架构。总而言之，这些设计决策使我们能够在已建立的视频（UCF-101 和 Kinetics-600）和图像（ImageNet）生成基准上实现最先进的性能，而无需使用无分类器指导。<br />
最后，我们还训练了三个用于文本到视频生成任务的级联模型，其中包括一个基本潜在视频扩散模型和两个视频超分辨率扩散模型，以每秒 8 帧的速度生成 512 x 896 分辨率的视频。<br />
<br />
项目地址：<a href="https://walt-video-diffusion.github.io/">walt-video-diffusion.github.…</a></p>
<p><a href="https://nitter.cz/agrimgupta92/status/1734253883076063426#m">nitter.cz/agrimgupta92/status/1734253883076063426#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0NjgyNzY4NDIwMTY3NjkvcHUvaW1nL1BCS0ZHeHFMVmpZeTdEdkcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734785122183036962#m</id>
            <title>牛蛙 数学和代码确实厉害</title>
            <link>https://nitter.cz/op7418/status/1734785122183036962#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734785122183036962#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:59:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>牛蛙 数学和代码确实厉害</p>
<p><a href="https://nitter.cz/aigclab/status/1734782664979452010#m">nitter.cz/aigclab/status/1734782664979452010#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734780663843459345#m</id>
            <title>R to @op7418: 麻了，刚才提示我不支持现在又可以生成了</title>
            <link>https://nitter.cz/op7418/status/1734780663843459345#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734780663843459345#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:41:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>麻了，刚才提示我不支持现在又可以生成了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdHcyYmE0QUFxNEpELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734618500000284803#m</id>
            <title>RT by @op7418: 这个观点很有意思，要判断一个模型是否在微调阶段使用了GPT-4生成的数据集只需要向他询问“给我讲个笑话”就行。</title>
            <link>https://nitter.cz/op7418/status/1734618500000284803#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734618500000284803#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 16:57:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个观点很有意思，要判断一个模型是否在微调阶段使用了GPT-4生成的数据集只需要向他询问“给我讲个笑话”就行。</p>
<p><a href="https://nitter.cz/mattshumer_/status/1734605812054827364#m">nitter.cz/mattshumer_/status/1734605812054827364#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734780196853801066#m</id>
            <title>R to @op7418: 目前不支持 Niji 的图片生成</title>
            <link>https://nitter.cz/op7418/status/1734780196853801066#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734780196853801066#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:40:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>目前不支持 Niji 的图片生成</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdFY4bWJBQUFGUk1rLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734770668242432210#m</id>
            <title>意外的发现我已经有了 Midjourney Alpha 版本也就是图片生成版本的权限，体验了一下录制了一个视频，这下图片生成真的方便多了。
写一下如何使用Alpha 版本和图片生成的一些变化，后面发现的小细节也会写在这个帖子上：

如何使用 Alpha 版本：
如果已经生成一万张图可以使用，在 Discord 里面输入/info 可以看到生成的图片数量，也可以直接访问这个链接看自己是不是有权限：http://alpha.midjourney.com

图片生成功能细节：
✦点击页面上方的提示词输入框可以直接开始生成图片。
✦输入框右侧按钮点击可以调整图片生成的所有参数。
✦+号按钮可以上传图片或者使用已有的图片垫图。
✦鼠标 Hover 到每个参数上都可以看到具体的解释。
✦已经生成的图像设置参数直接点击就可以回填到提示词输入框里面。
✦已经生成的图片右下角包含了，可以对图片进行的所有操作，比如放大使用提示词，使用图片，放大重随等。
✦正在生成的图片和已经生成的图片是在一起的，今天的内容会放大显示，昨天的会变成小图。</title>
            <link>https://nitter.cz/op7418/status/1734770668242432210#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734770668242432210#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:02:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>意外的发现我已经有了 Midjourney Alpha 版本也就是图片生成版本的权限，体验了一下录制了一个视频，这下图片生成真的方便多了。<br />
写一下如何使用Alpha 版本和图片生成的一些变化，后面发现的小细节也会写在这个帖子上：<br />
<br />
如何使用 Alpha 版本：<br />
如果已经生成一万张图可以使用，在 Discord 里面输入/info 可以看到生成的图片数量，也可以直接访问这个链接看自己是不是有权限：<a href="http://alpha.midjourney.com">alpha.midjourney.com</a><br />
<br />
图片生成功能细节：<br />
✦点击页面上方的提示词输入框可以直接开始生成图片。<br />
✦输入框右侧按钮点击可以调整图片生成的所有参数。<br />
✦+号按钮可以上传图片或者使用已有的图片垫图。<br />
✦鼠标 Hover 到每个参数上都可以看到具体的解释。<br />
✦已经生成的图像设置参数直接点击就可以回填到提示词输入框里面。<br />
✦已经生成的图片右下角包含了，可以对图片进行的所有操作，比如放大使用提示词，使用图片，放大重随等。<br />
✦正在生成的图片和已经生成的图片是在一起的，今天的内容会放大显示，昨天的会变成小图。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NzA1NTM4MzAxNTgzMzYvcHUvaW1nL0h0UUlTSWNqSHlTTjBYTnIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734604236561006614#m</id>
            <title>RT by @op7418: 这份论文通过对300多篇论文的调研，全面的分析了医学LLM的进展、应用和挑战。这里简要总结一下论文在应用和挑战以及未来发展方向的结论。

医学LLM的主要应用方向：
医学诊断：将LLMs纳入医学诊断流程将提高专业医疗保健的可及性。LLM作为医学诊断的唯一工具存在明显局限性，完全依赖患者的主观输入。
由于LLM主要基于文本，缺乏分析医学诊断图像的固有能力。鉴于客观医学诊断经常依赖视觉图像，LLM通常无法直接进行诊断评估，因为缺乏具体的视觉证据支持疾病诊断。然而，它们可以作为逻辑推理工具帮助改进其他基于视觉的模型的准确性。
格式化和ICD编码：LLM可以通过从临床记录中分离医学术语并为其分配相应的ICD编码来帮助自动化ICD编码。PLM-ICD是一个经过微调的LLM，用于自动ICD编码。
它被微调为多类分类模型。任何LLM中潜在的偏见和幻觉都是至关重要的。此外，鉴于它们的算法显示出改进的空间，正如从它们的AUC分数所表明的那样，建立一种机制来检测和纠正这些错误，以防它们进入患者的电子健康记录（EHRs）变得至关重要。
临床报告生成：LLM在临床报告生成中的直观方式是作为一个总结工具。给定一个诊断作为输入，它可以利用其文本总结能力，如前面讨论的那样，给出一个清晰简洁的最终结论。
尽管使用LLMs进行临床报告生成或总结已被证明比人类同行更完整和更准确van2023clinical，但仍存在幻觉的担忧，以及倾向于以字面意义而非人类医生常采用的基于假设的观点来处理输入的趋势。
医学教育：Karabacak等人提出了将LLM纳入医学教育系统的几个好处，特别是为了为医学生准备医学考试以及随后在现实世界中的情景。他们建议，通过LLM生成情景、问题和相应的答案，可以增强医学教育。
在医学教育中使用LLM可能存在一些潜在的缺点，比如目前缺乏伦理培训以及训练数据集可能带来的偏见，导致某些群体代表不足。
医疗机器人：基于图的机器人指令分解器ni2023grid被提出作为利用LLMs进行路径规划的一种方法。该方案使用场景图而不是图像识别来获取环境信息，并在每个阶段为指令规划任务。它还可以预测即将到来的任务，并在场景图中规划预定义的机器人动作。然后，LLMs将以文本形式输出计划好的路线，将指令、场景图和机器人图作为输入。
实施医疗机器人技术面临的一些挑战与实施协作机器人（协作机器人）时的挑战非常相似，因为两种情况都涉及机器人与人类一起操作，这需要对机器人始终做正确的事情的信任。
医学语言翻译：语言往往是全球合作的一大障碍，LLM的帮助可以大大减少这一障碍。机器翻译已被证明比传统服务准确率高出7% 
使用LLM进行翻译的一个道德考虑是可能会无意中插入歧视性措辞。由于管道的性质，这很难捕捉，可能导致误解甚至法律后果。
心理健康支持：由LLMs驱动的聊天机器人可以大幅提高心理健康治疗资源的可及性。心理咨询和随后的治疗对许多人来说成本高昂，而聊天机器人作为对话伙伴和陪伴者的能力将显著降低具有财务或身体限制的患者的准入门槛。
短期内仅依靠LLMs可能难以克服的一个挑战是书面和口头沟通技巧之间的差异。Hill等人发现，被调查者在被要求书面回答问题时与口头表达答案时的回答方式不同。这可能是LLMs需要突破的障碍，以更高程度地模仿治疗师。

医学LLM应用的主要挑战：
幻觉：内在幻觉是指生成的输出在逻辑上与事实信息相矛盾，比如LLM生成错误的数学公式计算。外在幻觉发生在生成的输出无法验证的情况下，典型例子包括LLM“伪造”不存在的引用或“回避”问题。将LLM整合到医学领域时，流利但非事实的LLM幻觉可能导致不正确的医学信息传播，从而导致误诊、不当治疗和对患者的有害教育。
缺乏评估基准和度量标准：随着通用LLM的出现，当前的基准和度量标准无法评估LLM的整体能力，特别是在医学领域。目前的基准，如MedQA（USMLE）medqa和MedMCQA medmcqa，在问题回答任务上提供了广泛的覆盖，但未能评估重要的LLM特定度量标准，如可信度、忠实度、帮助性和可解释性。
领域数据限制：目前医学领域的数据集相对较小，与用于训练通用LLM的数据集相比。医学知识领域广阔；现有数据集有限，无法涵盖整个领域。这导致LLM在具有广泛数据覆盖范围的开放基准测试中表现出非凡的性能，但在差异诊断和个性化治疗规划等现实任务中表现不佳。
新知识适应：LLM在大量数据上进行训练以学习知识。一旦LLM被训练，通过重新训练注入新知识是昂贵且低效的。当需要更新知识时（例如，药物的新不良反应或新疾病），会出现两个问题：第一个问题是如何使LLM“忘记”旧知识 - 从训练数据中删除所有“旧知识”几乎是不可能的，新旧知识之间的差异可能导致意外的关联和偏见。第二个问题是及时添加知识 - 我们如何确保模型实时更新？
行为对齐：行为一致性是指确保LLM的行为与其任务目标一致的过程。尽管努力将LLM与人类行为保持一致，但一般人类与医疗专业人员之间的行为差异仍然是医疗领域采用LLM所面临的挑战。
道德、法律和安全问题：一些作品提出了在医学领域使用像ChatGPT这样的LLM存在的问题。大多数关注伦理、问责和安全性。例如，科学界因伦理问题而不赞成在撰写生物医学研究论文时使用ChatGPT。此外，将LLM用作医学助手的问责性也具有挑战性。

医学LLM未来发展方向：
引入新的基准：需要研究和建立新的LLM能力，如从可信的医学参考资料中获取信息，理解医学共识的不断发展，并清楚地向用户传达不确定性medpalm。此外，考虑到医学领域的安全关键性，有必要设计评估公平性、公正性、道德和其他在医学中至关重要的微妙考虑的基准。
跨学科合作：医学界主要使用技术公司提供的LLM，而没有对它们的数据训练提出质疑。鉴于这种次优情况，鼓励医学专业人员积极参与创建和部署医疗LLM，提供相关的训练数据，定义LLM的期望益处，并在真实场景中进行测试以评估这些益处。
多模态LLM集成了时间序列、视觉和音频数据：多模式LLM（MLLM）是基于LLM的模型，旨在执行多模式任务yin2023survey。虽然LLM主要解决NLP任务，但MLLM支持更广泛的任务，例如理解模因的潜在含义和从图像生成网站代码。这种多功能性表明MLLM在医学中有着广泛的应用前景。
医学较不成熟领域的LLMS：目前关于在医学中应用LLMs的研究主要集中在一般医学领域，部分原因是该领域有更全面的数据可用。鉴于这种集中，研究人员有机会策划新的数据集，并研究LLMs在非传统但同样重要的医学领域，如“康复治疗”和“运动医学”的应用。

论文地址：https://arxiv.org/abs/2311.05112</title>
            <link>https://nitter.cz/op7418/status/1734604236561006614#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734604236561006614#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 16:00:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这份论文通过对300多篇论文的调研，全面的分析了医学LLM的进展、应用和挑战。这里简要总结一下论文在应用和挑战以及未来发展方向的结论。<br />
<br />
医学LLM的主要应用方向：<br />
医学诊断：将LLMs纳入医学诊断流程将提高专业医疗保健的可及性。LLM作为医学诊断的唯一工具存在明显局限性，完全依赖患者的主观输入。<br />
由于LLM主要基于文本，缺乏分析医学诊断图像的固有能力。鉴于客观医学诊断经常依赖视觉图像，LLM通常无法直接进行诊断评估，因为缺乏具体的视觉证据支持疾病诊断。然而，它们可以作为逻辑推理工具帮助改进其他基于视觉的模型的准确性。<br />
格式化和ICD编码：LLM可以通过从临床记录中分离医学术语并为其分配相应的ICD编码来帮助自动化ICD编码。PLM-ICD是一个经过微调的LLM，用于自动ICD编码。<br />
它被微调为多类分类模型。任何LLM中潜在的偏见和幻觉都是至关重要的。此外，鉴于它们的算法显示出改进的空间，正如从它们的AUC分数所表明的那样，建立一种机制来检测和纠正这些错误，以防它们进入患者的电子健康记录（EHRs）变得至关重要。<br />
临床报告生成：LLM在临床报告生成中的直观方式是作为一个总结工具。给定一个诊断作为输入，它可以利用其文本总结能力，如前面讨论的那样，给出一个清晰简洁的最终结论。<br />
尽管使用LLMs进行临床报告生成或总结已被证明比人类同行更完整和更准确van2023clinical，但仍存在幻觉的担忧，以及倾向于以字面意义而非人类医生常采用的基于假设的观点来处理输入的趋势。<br />
医学教育：Karabacak等人提出了将LLM纳入医学教育系统的几个好处，特别是为了为医学生准备医学考试以及随后在现实世界中的情景。他们建议，通过LLM生成情景、问题和相应的答案，可以增强医学教育。<br />
在医学教育中使用LLM可能存在一些潜在的缺点，比如目前缺乏伦理培训以及训练数据集可能带来的偏见，导致某些群体代表不足。<br />
医疗机器人：基于图的机器人指令分解器ni2023grid被提出作为利用LLMs进行路径规划的一种方法。该方案使用场景图而不是图像识别来获取环境信息，并在每个阶段为指令规划任务。它还可以预测即将到来的任务，并在场景图中规划预定义的机器人动作。然后，LLMs将以文本形式输出计划好的路线，将指令、场景图和机器人图作为输入。<br />
实施医疗机器人技术面临的一些挑战与实施协作机器人（协作机器人）时的挑战非常相似，因为两种情况都涉及机器人与人类一起操作，这需要对机器人始终做正确的事情的信任。<br />
医学语言翻译：语言往往是全球合作的一大障碍，LLM的帮助可以大大减少这一障碍。机器翻译已被证明比传统服务准确率高出7% <br />
使用LLM进行翻译的一个道德考虑是可能会无意中插入歧视性措辞。由于管道的性质，这很难捕捉，可能导致误解甚至法律后果。<br />
心理健康支持：由LLMs驱动的聊天机器人可以大幅提高心理健康治疗资源的可及性。心理咨询和随后的治疗对许多人来说成本高昂，而聊天机器人作为对话伙伴和陪伴者的能力将显著降低具有财务或身体限制的患者的准入门槛。<br />
短期内仅依靠LLMs可能难以克服的一个挑战是书面和口头沟通技巧之间的差异。Hill等人发现，被调查者在被要求书面回答问题时与口头表达答案时的回答方式不同。这可能是LLMs需要突破的障碍，以更高程度地模仿治疗师。<br />
<br />
医学LLM应用的主要挑战：<br />
幻觉：内在幻觉是指生成的输出在逻辑上与事实信息相矛盾，比如LLM生成错误的数学公式计算。外在幻觉发生在生成的输出无法验证的情况下，典型例子包括LLM“伪造”不存在的引用或“回避”问题。将LLM整合到医学领域时，流利但非事实的LLM幻觉可能导致不正确的医学信息传播，从而导致误诊、不当治疗和对患者的有害教育。<br />
缺乏评估基准和度量标准：随着通用LLM的出现，当前的基准和度量标准无法评估LLM的整体能力，特别是在医学领域。目前的基准，如MedQA（USMLE）medqa和MedMCQA medmcqa，在问题回答任务上提供了广泛的覆盖，但未能评估重要的LLM特定度量标准，如可信度、忠实度、帮助性和可解释性。<br />
领域数据限制：目前医学领域的数据集相对较小，与用于训练通用LLM的数据集相比。医学知识领域广阔；现有数据集有限，无法涵盖整个领域。这导致LLM在具有广泛数据覆盖范围的开放基准测试中表现出非凡的性能，但在差异诊断和个性化治疗规划等现实任务中表现不佳。<br />
新知识适应：LLM在大量数据上进行训练以学习知识。一旦LLM被训练，通过重新训练注入新知识是昂贵且低效的。当需要更新知识时（例如，药物的新不良反应或新疾病），会出现两个问题：第一个问题是如何使LLM“忘记”旧知识 - 从训练数据中删除所有“旧知识”几乎是不可能的，新旧知识之间的差异可能导致意外的关联和偏见。第二个问题是及时添加知识 - 我们如何确保模型实时更新？<br />
行为对齐：行为一致性是指确保LLM的行为与其任务目标一致的过程。尽管努力将LLM与人类行为保持一致，但一般人类与医疗专业人员之间的行为差异仍然是医疗领域采用LLM所面临的挑战。<br />
道德、法律和安全问题：一些作品提出了在医学领域使用像ChatGPT这样的LLM存在的问题。大多数关注伦理、问责和安全性。例如，科学界因伦理问题而不赞成在撰写生物医学研究论文时使用ChatGPT。此外，将LLM用作医学助手的问责性也具有挑战性。<br />
<br />
医学LLM未来发展方向：<br />
引入新的基准：需要研究和建立新的LLM能力，如从可信的医学参考资料中获取信息，理解医学共识的不断发展，并清楚地向用户传达不确定性medpalm。此外，考虑到医学领域的安全关键性，有必要设计评估公平性、公正性、道德和其他在医学中至关重要的微妙考虑的基准。<br />
跨学科合作：医学界主要使用技术公司提供的LLM，而没有对它们的数据训练提出质疑。鉴于这种次优情况，鼓励医学专业人员积极参与创建和部署医疗LLM，提供相关的训练数据，定义LLM的期望益处，并在真实场景中进行测试以评估这些益处。<br />
多模态LLM集成了时间序列、视觉和音频数据：多模式LLM（MLLM）是基于LLM的模型，旨在执行多模式任务yin2023survey。虽然LLM主要解决NLP任务，但MLLM支持更广泛的任务，例如理解模因的潜在含义和从图像生成网站代码。这种多功能性表明MLLM在医学中有着广泛的应用前景。<br />
医学较不成熟领域的LLMS：目前关于在医学中应用LLMs的研究主要集中在一般医学领域，部分原因是该领域有更全面的数据可用。鉴于这种集中，研究人员有机会策划新的数据集，并研究LLMs在非传统但同样重要的医学领域，如“康复治疗”和“运动医学”的应用。<br />
<br />
论文地址：<a href="https://arxiv.org/abs/2311.05112">arxiv.org/abs/2311.05112</a></p>
<p><a href="https://nitter.cz/omarsar0/status/1734599425568231513#m">nitter.cz/omarsar0/status/1734599425568231513#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734760424456024238#m</id>
            <title>Mixtral-8x7b 上限这个ELO测试之后获得了1000多次投票，得分马上就要超过cloude1 了，已经超过了所有开源模型，模型质量确实顶。</title>
            <link>https://nitter.cz/op7418/status/1734760424456024238#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734760424456024238#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:21:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b 上限这个ELO测试之后获得了1000多次投票，得分马上就要超过cloude1 了，已经超过了所有开源模型，模型质量确实顶。</p>
<p><a href="https://nitter.cz/lmsysorg/status/1734680611393073289#m">nitter.cz/lmsysorg/status/1734680611393073289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734748355518198111#m</id>
            <title>感觉逻辑还是优先从素材库里拿真实素材有几段视频是生成的</title>
            <link>https://nitter.cz/op7418/status/1734748355518198111#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734748355518198111#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:33:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感觉逻辑还是优先从素材库里拿真实素材有几段视频是生成的</p>
<p><a href="https://nitter.cz/dotey/status/1734742684353380484#m">nitter.cz/dotey/status/1734742684353380484#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734492326599467291#m</id>
            <title>RT by @op7418: 写个如何用 Ollama 在 Mac 本地跑 LLM，并且用在 Obsidian 上处理自己的笔记和内容的小教程。视频是具体的演示，我把等待时间剪掉了。
我们开始具体的教程🧵：</title>
            <link>https://nitter.cz/op7418/status/1734492326599467291#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734492326599467291#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:36:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>写个如何用 Ollama 在 Mac 本地跑 LLM，并且用在 Obsidian 上处理自己的笔记和内容的小教程。视频是具体的演示，我把等待时间剪掉了。<br />
我们开始具体的教程🧵：</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTIyNTkzOTgzMjgzMjAvcHUvaW1nL1RTOTNndlFHLTBvQU5MWmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734633006575337611#m</id>
            <title>runway现在运动笔刷加上提示词可以让人物面部产生指定的表情，比如开心恐惧等，也可以对面部器官进行控制比如睁眼闭眼。</title>
            <link>https://nitter.cz/op7418/status/1734633006575337611#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734633006575337611#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 17:55:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>runway现在运动笔刷加上提示词可以让人物面部产生指定的表情，比如开心恐惧等，也可以对面部器官进行控制比如睁眼闭眼。</p>
<p><a href="https://nitter.cz/runwayml/status/1734574651630616710#m">nitter.cz/runwayml/status/1734574651630616710#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>