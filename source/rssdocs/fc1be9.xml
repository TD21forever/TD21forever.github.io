<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748381779290198296#m</id>
            <title>Nicolas介绍了一下如何根据画面内容的深度信息，设置Runway多运动笔刷的参数。
让整个画面内容的运动更加自然。思路非常清晰，可以学习一下👇：

教程：利用多动作刷进行深度运动制作

昨天，@runwayml 向所有用户推出了多动作刷功能。这个功能在增强创作者对作品控制力方面，标志着一大步进。

我最喜欢的应用之一，就是在不同深度层面上添加逼真的动作。

现在，让我们深入了解我是如何实现这一点的！

🔍 识别物体深度（图 1）

为不同深度层面的物体添加动作的第一步，是确定它们与相机的距离。通常这个过程不需要太专业的观察能力。在这个例子中，以下是我想要用不同动作强度来制作动画的不同深度层面：

1️⃣ 街道上的阴影 
2️⃣ 正在远去的汽车 
3️⃣ 靠近的植物 
4️⃣ 附近的植物 
5️⃣ 远处的植物

🗺️ 可选：利用深度图（图 2）

为了更精确，或者处理复杂素材时，你可以使用深度图来更准确地判断物体的远近。Runway 实际上提供了一个“提取深度”的工具，你可以在工具概览中找到它！
如果你之前没用过深度图，一个基本原则是：区域越亮，表示它越靠近相机。

🖌️ 应用多动作刷（图 3）

识别出不同的深度层面后，我们就可以开始用不同的动作刷来突出它们。

我通常从最近的深度层面开始，给它设定一个动作值 5。接着我会按层递减，离相机越远，动作值就越小。在下面的例子中，我选择了以下值：

阴影：Ambient 5 近处植物：Ambient 3 汽车：Proximity -2.5 + Ambient 2 附近植物：Ambient 1 远处植物：Ambient 0.5

🎬 制作具有逼真深度运动的视频（视频）

只要动作值设置得当，你就能制作出动作更加逼真的视频。比如，相机近处的对象将比远处的物体移动得更快。

正如往常，略微的动作值差异，往往能让整体构图更加逼真。

如果你尝试了这种技术，欢迎在下面的评论区分享你的成果！👇🏼</title>
            <link>https://nitter.cz/op7418/status/1748381779290198296#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748381779290198296#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 16:27:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nicolas介绍了一下如何根据画面内容的深度信息，设置Runway多运动笔刷的参数。<br />
让整个画面内容的运动更加自然。思路非常清晰，可以学习一下👇：<br />
<br />
教程：利用多动作刷进行深度运动制作<br />
<br />
昨天，<a href="https://nitter.cz/runwayml" title="Runway">@runwayml</a> 向所有用户推出了多动作刷功能。这个功能在增强创作者对作品控制力方面，标志着一大步进。<br />
<br />
我最喜欢的应用之一，就是在不同深度层面上添加逼真的动作。<br />
<br />
现在，让我们深入了解我是如何实现这一点的！<br />
<br />
🔍 识别物体深度（图 1）<br />
<br />
为不同深度层面的物体添加动作的第一步，是确定它们与相机的距离。通常这个过程不需要太专业的观察能力。在这个例子中，以下是我想要用不同动作强度来制作动画的不同深度层面：<br />
<br />
1️⃣ 街道上的阴影 <br />
2️⃣ 正在远去的汽车 <br />
3️⃣ 靠近的植物 <br />
4️⃣ 附近的植物 <br />
5️⃣ 远处的植物<br />
<br />
🗺️ 可选：利用深度图（图 2）<br />
<br />
为了更精确，或者处理复杂素材时，你可以使用深度图来更准确地判断物体的远近。Runway 实际上提供了一个“提取深度”的工具，你可以在工具概览中找到它！<br />
如果你之前没用过深度图，一个基本原则是：区域越亮，表示它越靠近相机。<br />
<br />
🖌️ 应用多动作刷（图 3）<br />
<br />
识别出不同的深度层面后，我们就可以开始用不同的动作刷来突出它们。<br />
<br />
我通常从最近的深度层面开始，给它设定一个动作值 5。接着我会按层递减，离相机越远，动作值就越小。在下面的例子中，我选择了以下值：<br />
<br />
阴影：Ambient 5 近处植物：Ambient 3 汽车：Proximity -2.5 + Ambient 2 附近植物：Ambient 1 远处植物：Ambient 0.5<br />
<br />
🎬 制作具有逼真深度运动的视频（视频）<br />
<br />
只要动作值设置得当，你就能制作出动作更加逼真的视频。比如，相机近处的对象将比远处的物体移动得更快。<br />
<br />
正如往常，略微的动作值差异，往往能让整体构图更加逼真。<br />
<br />
如果你尝试了这种技术，欢迎在下面的评论区分享你的成果！👇🏼</p>
<p><a href="https://nitter.cz/iamneubert/status/1748339127613841587#m">nitter.cz/iamneubert/status/1748339127613841587#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748363331344453680#m</id>
            <title>Midjoirney V6 的风格微调有点意思啊，本质上就是风格转换功能，上传照片之后把图片转成提示词描述的风格，感觉玩法挺多的。</title>
            <link>https://nitter.cz/op7418/status/1748363331344453680#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748363331344453680#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 15:14:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjoirney V6 的风格微调有点意思啊，本质上就是风格转换功能，上传照片之后把图片转成提示词描述的风格，感觉玩法挺多的。</p>
<p><a href="https://nitter.cz/nickfloats/status/1748039999264584016#m">nitter.cz/nickfloats/status/1748039999264584016#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748184369976713484#m</id>
            <title>RT by @op7418: 谷歌推出了用于用于增强大语言模型的选择性预测框架 ASPIRE。

这个预测框架可以在 LLM 输出的时候给出一个可靠性评分，让用户更好的判断 LLM 输出内容的可靠性。

选择性预测会让大语言模型在给出答案的同时，提供一个选择性得分，用以表示答案正确的可能性。

训练方式：
ASPIRE 的工作方式是对大语言模型进行针对性的微调，使其更擅长于问题回答任务，并训练模型自行评估其生成答案的正确性。该框架包括三个阶段：特定任务的调优、答案采样和自我评估学习（self-evaluation learning）。在特定任务调优阶段，ASPIRE 对预先训练好的大语言模型进行微调，以提升其预测表现。答案采样阶段则是针对每个训练问题生成多种答案，进而构建自我评估学习的数据集。到了自我评估学习阶段，ASPIRE 引入可调整参数，并对这些参数进行微调，以培养模型的自我评估能力。

结论：
ASPIRE 的出现标志着大语言模型领域的一次变革，它强调了模型容量并非其性能的唯一决定因素。实际上，通过策略性的调整，即便是小型模型也能实现更精确、更有信心的预测，从而显著提升模型的有效性。

内容来源：https://blog.research.google/2024/01/introducing-aspire-for-selective.html</title>
            <link>https://nitter.cz/op7418/status/1748184369976713484#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748184369976713484#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:23:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌推出了用于用于增强大语言模型的选择性预测框架 ASPIRE。<br />
<br />
这个预测框架可以在 LLM 输出的时候给出一个可靠性评分，让用户更好的判断 LLM 输出内容的可靠性。<br />
<br />
选择性预测会让大语言模型在给出答案的同时，提供一个选择性得分，用以表示答案正确的可能性。<br />
<br />
训练方式：<br />
ASPIRE 的工作方式是对大语言模型进行针对性的微调，使其更擅长于问题回答任务，并训练模型自行评估其生成答案的正确性。该框架包括三个阶段：特定任务的调优、答案采样和自我评估学习（self-evaluation learning）。在特定任务调优阶段，ASPIRE 对预先训练好的大语言模型进行微调，以提升其预测表现。答案采样阶段则是针对每个训练问题生成多种答案，进而构建自我评估学习的数据集。到了自我评估学习阶段，ASPIRE 引入可调整参数，并对这些参数进行微调，以培养模型的自我评估能力。<br />
<br />
结论：<br />
ASPIRE 的出现标志着大语言模型领域的一次变革，它强调了模型容量并非其性能的唯一决定因素。实际上，通过策略性的调整，即便是小型模型也能实现更精确、更有信心的预测，从而显著提升模型的有效性。<br />
<br />
内容来源：<a href="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">blog.research.google/2024/01…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0VMTVNneGE4QUFSdXNLLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dFTE1TZ3hhOEFBUnVzSy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748189182307303606#m</id>
            <title>RT by @op7418: 小扎刚发言完，Meta 就出王炸？推出了可以进行自我奖励的 LLM。
简单来说就是语言模型可以自我判断模型质量，从而实现一定程度上的自我进化。

使用这个方法微调的 Llama 2 70B 模型，优于 AlpacaEval 2.0 排行榜上的Claude 2、Gemini Pro 和 GPT-4 0613等模型。

实现方式：

自奖励语言模型，这类智能体具备双重功能：一方面（i）它们能够作为遵循指令的模型，针对给定的提示生成回应；另一方面（ii）它们还能创造并评估新的指令遵循示例，并将这些示例加入到自己的训练集中。

我们采用了与 Xu 等人（2023年）最近提出的类似的迭代式动态评价优化（Iterative DPO）框架来训练这些模型。从一个基础模型出发，在每一轮迭代中，模型都会经历一个自我指令生成的过程，在这个过程中，模型针对新创造的提示生成候选回应，并由模型自身对这些回应进行奖励评分。

这个评分过程是通过让大语言模型扮演评判员（LLM-as-a-Judge）的方式来实现的，这本身也是一种遵循指令的任务。然后，根据这些生成的数据构建一个偏好数据集，并利用动态评价优化方法来训练下一轮的模型。

论文地址：https://arxiv.org/html/2401.10020v1</title>
            <link>https://nitter.cz/op7418/status/1748189182307303606#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748189182307303606#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:42:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小扎刚发言完，Meta 就出王炸？推出了可以进行自我奖励的 LLM。<br />
简单来说就是语言模型可以自我判断模型质量，从而实现一定程度上的自我进化。<br />
<br />
使用这个方法微调的 Llama 2 70B 模型，优于 AlpacaEval 2.0 排行榜上的Claude 2、Gemini Pro 和 GPT-4 0613等模型。<br />
<br />
实现方式：<br />
<br />
自奖励语言模型，这类智能体具备双重功能：一方面（i）它们能够作为遵循指令的模型，针对给定的提示生成回应；另一方面（ii）它们还能创造并评估新的指令遵循示例，并将这些示例加入到自己的训练集中。<br />
<br />
我们采用了与 Xu 等人（2023年）最近提出的类似的迭代式动态评价优化（Iterative DPO）框架来训练这些模型。从一个基础模型出发，在每一轮迭代中，模型都会经历一个自我指令生成的过程，在这个过程中，模型针对新创造的提示生成候选回应，并由模型自身对这些回应进行奖励评分。<br />
<br />
这个评分过程是通过让大语言模型扮演评判员（LLM-as-a-Judge）的方式来实现的，这本身也是一种遵循指令的任务。然后，根据这些生成的数据构建一个偏好数据集，并利用动态评价优化方法来训练下一轮的模型。<br />
<br />
论文地址：<a href="https://arxiv.org/html/2401.10020v1">arxiv.org/html/2401.10020v1</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VMUXhvNGFJQUFrS05FLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748191803965378723#m</id>
            <title>RT by @op7418: 另一个 ComfyUI 上的AnimateAnyone实现，看起来比其他的稳定一些，面部崩的不是很厉害。

也是基于摩尔线程的AnimateAnyone方案，不过这个的文档写的非常详细，而且提供了不同采样器的效果预览。

项目地址：https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved</title>
            <link>https://nitter.cz/op7418/status/1748191803965378723#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748191803965378723#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:53:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个 ComfyUI 上的AnimateAnyone实现，看起来比其他的稳定一些，面部崩的不是很厉害。<br />
<br />
也是基于摩尔线程的AnimateAnyone方案，不过这个的文档写的非常详细，而且提供了不同采样器的效果预览。<br />
<br />
项目地址：<a href="https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved">github.com/MrForExample/Comf…</a></p>
<p><a href="https://nitter.cz/MrForExample/status/1748080976104734858#m">nitter.cz/MrForExample/status/1748080976104734858#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgxOTE3NzM1NDQwMzAyMDgvcHUvaW1nL3JmVVR0Q3dwa3lwNEtXaVUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748180416840995281#m</id>
            <title>RT by @op7418: 刚发现摩尔线程前几天复原了阿里的单图跳舞项目并且已经开源训练代码，你可以训练自己的AnimateAnyone模型。
有个基于摩尔线程开源的版本制作了 ComfyUI 节点，并且提供了基础的工作流。
现在可以在ComfyUI中非常简单的让单图跳舞了。

节点地址：https://github.com/chaojie/ComfyUI-Moore-AnimateAnyone?tab=readme-ov-file</title>
            <link>https://nitter.cz/op7418/status/1748180416840995281#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748180416840995281#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:07:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚发现摩尔线程前几天复原了阿里的单图跳舞项目并且已经开源训练代码，你可以训练自己的AnimateAnyone模型。<br />
有个基于摩尔线程开源的版本制作了 ComfyUI 节点，并且提供了基础的工作流。<br />
现在可以在ComfyUI中非常简单的让单图跳舞了。<br />
<br />
节点地址：<a href="https://github.com/chaojie/ComfyUI-Moore-AnimateAnyone?tab=readme-ov-file">github.com/chaojie/ComfyUI-M…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VMSHlmZWIwQUF3TmNlLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748293897237991794#m</id>
            <title>帮快手的朋友招两个设计，主要在探索UI 和运营设计的自动生成，需要在 AIGC 领域有过设计实践，懂 SD，有前端能力更好。
邮件标题或者备注可以加上在歸藏这里看到的。

下面是完整的 JD：
快手研发设计中心团队，负责面向公司内部的产品交互设计。我们正在进行前沿的 AIGC 设计探索，并把探索的成果变成产品，实现 UI 和运营设计的自动生成，希望找到志同道合的朋友一起同行:

1. 希望你是 2024 年 6/7 月毕业的本科/研究生,专业不限
2. 有 B 端 UI 设计 / 工具类 UI 设计能力
3. 希望你在通用设计能力上至少有一个亮点（良好的表达 / 优秀的审美 / 出色的逻辑性 / 良好的同理心…）
4. 希望你对新鲜事物感兴趣，并且有一定自己的探索和尝试
5. 在 AIGC 方向有过学习实践，熟悉 SD，或有一定前端代码能力优先
6. 坐标北京

如果你对我们的工作感兴趣，欢迎投递简历和作品集，我的微信：ding_zu，邮箱 dzwangyihan@gmail.com</title>
            <link>https://nitter.cz/op7418/status/1748293897237991794#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748293897237991794#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 10:38:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>帮快手的朋友招两个设计，主要在探索UI 和运营设计的自动生成，需要在 AIGC 领域有过设计实践，懂 SD，有前端能力更好。<br />
邮件标题或者备注可以加上在歸藏这里看到的。<br />
<br />
下面是完整的 JD：<br />
快手研发设计中心团队，负责面向公司内部的产品交互设计。我们正在进行前沿的 AIGC 设计探索，并把探索的成果变成产品，实现 UI 和运营设计的自动生成，希望找到志同道合的朋友一起同行:<br />
<br />
1. 希望你是 2024 年 6/7 月毕业的本科/研究生,专业不限<br />
2. 有 B 端 UI 设计 / 工具类 UI 设计能力<br />
3. 希望你在通用设计能力上至少有一个亮点（良好的表达 / 优秀的审美 / 出色的逻辑性 / 良好的同理心…）<br />
4. 希望你对新鲜事物感兴趣，并且有一定自己的探索和尝试<br />
5. 在 AIGC 方向有过学习实践，熟悉 SD，或有一定前端代码能力优先<br />
6. 坐标北京<br />
<br />
如果你对我们的工作感兴趣，欢迎投递简历和作品集，我的微信：ding_zu，邮箱 dzwangyihan@gmail.com</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VNd0JDRWJ3QUFDTTZVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748289276708855904#m</id>
            <title>Nick这张用 Midjourney 生成的手帐照片有点意思，可以用来表达观点和配图，比如他表达意思就是：

在X平台上成长的唯一秘诀就是发布优质内容，而“关于如何在X平台上成长”的内容很少能算作优质内容。

我也试了一下居然真的可以，后面加上-- s 0 会生成的更稳定。

完整提示词：
a note that reads "The only secret to growing on X is putting out good content and "content about how to grow on X is very rarely good content" --ar 4:5 --style raw --v 6.0 --s 0</title>
            <link>https://nitter.cz/op7418/status/1748289276708855904#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748289276708855904#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 10:20:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nick这张用 Midjourney 生成的手帐照片有点意思，可以用来表达观点和配图，比如他表达意思就是：<br />
<br />
在X平台上成长的唯一秘诀就是发布优质内容，而“关于如何在X平台上成长”的内容很少能算作优质内容。<br />
<br />
我也试了一下居然真的可以，后面加上-- s 0 会生成的更稳定。<br />
<br />
完整提示词：<br />
a note that reads "The only secret to growing on X is putting out good content and "content about how to grow on X is very rarely good content" --ar 4:5 --style raw --v 6.0 --s 0</p>
<p><a href="https://nitter.cz/nickfloats/status/1748285824339153407#m">nitter.cz/nickfloats/status/1748285824339153407#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VNcm1rZWFZQUFDc2FsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748249758723150303#m</id>
            <title>专注于 Animatediff 动画的赛博菩萨Jerry Davos发布了一套用 IPapadter 和遮罩给跳舞视频转视频，并且可以更换背景的工作流。
老样子里面除了工作流之外还有非常详细的教程，还有涉及到的模型。

工作流下载：https://www.patreon.com/posts/v3-0-bg-changer-96735652</title>
            <link>https://nitter.cz/op7418/status/1748249758723150303#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748249758723150303#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 07:43:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>专注于 Animatediff 动画的赛博菩萨Jerry Davos发布了一套用 IPapadter 和遮罩给跳舞视频转视频，并且可以更换背景的工作流。<br />
老样子里面除了工作流之外还有非常详细的教程，还有涉及到的模型。<br />
<br />
工作流下载：<a href="https://www.patreon.com/posts/v3-0-bg-changer-96735652">patreon.com/posts/v3-0-bg-ch…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgyMzkwNjkxNTc3MDc3NzYvcHUvaW1nLzFKLXlWd21WUjZybkpCbDEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748233784582144147#m</id>
            <title>剪过的完整版本</title>
            <link>https://nitter.cz/op7418/status/1748233784582144147#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748233784582144147#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 06:39:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>剪过的完整版本</p>
<p><a href="https://nitter.cz/indigo11/status/1748194964016889990#m">nitter.cz/indigo11/status/1748194964016889990#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748194254667829407#m</id>
            <title>R to @op7418: 补一个摩尔线程的项目地址：https://github.com/MooreThreads/Moore-AnimateAnyone</title>
            <link>https://nitter.cz/op7418/status/1748194254667829407#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748194254667829407#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 04:02:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补一个摩尔线程的项目地址：<a href="https://github.com/MooreThreads/Moore-AnimateAnyone">github.com/MooreThreads/Moor…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0ODMxNDg4MDQ1NTcyMDk2MC9QbkdZSkVtMT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748012298038653237#m</id>
            <title>RT by @op7418: 今天无意间刷到《恋与深空》的宣传片，才发现女性向游戏已经做到这种地步了，NPC的表情动作和配音都非常自然生动。
场景也非常漂亮，麻了，男性向没有这么高质量的恋爱游戏啊。</title>
            <link>https://nitter.cz/op7418/status/1748012298038653237#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748012298038653237#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 15:59:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天无意间刷到《恋与深空》的宣传片，才发现女性向游戏已经做到这种地步了，NPC的表情动作和配音都非常自然生动。<br />
场景也非常漂亮，麻了，男性向没有这么高质量的恋爱游戏啊。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgwMTE4MjAzNDkzNzQ0NjQvcHUvaW1nL2Q5LVo2VkNSc0Izc0dyUnUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748120766103609715#m</id>
            <title>RT by @op7418: Meta 正在开发开源的通用人工智能（AGI），马克·扎克伯格宣布。

今天，扎克伯格意外的在一则 Instagram Reel 中透露，Meta 正致力于开发开源的通用人工智能（AGI）。为了实现这一目标，公司正将其两大 AI 研究团队 FAIR 和 GenAI 进行更紧密的整合，以构建完整的通用智能，并尽可能地开源。

扎克伯格在视频中谈道：“我们的长期愿景是开发通用智能，并以负责任的方式进行开源，让每个人都能广泛受益。”他在视频中表示：“我们清楚地认识到，下一代服务的需求是构建完整的通用智能，包括最优秀的 AI 助手、创意工作者用 AI、企业用 AI 等，这需要在 AI 的各个领域，包括推理、规划、编程、记忆和其他认知能力上取得进步。”

在谈到 Llama 3、基础设施和元宇宙方面，扎克伯格也表达了兴奋之情。他强调，公司目前正在培训 Llama 3 模型，并正在建设大规模的计算基础设施，包括到今年年底部署 350,000 个 Nvidia H100s。

扎克伯格还对元宇宙和 Meta 的 Ray-Ban 智能眼镜表现出极大的热情。“人们将需要新型 AI 设备，而这将逐渐将 AI 与元宇宙融合。”他说，“我认为我们很多人将会经常通过对话与 AI 互动。而且我相信，很多人会通过佩戴智能眼镜来实现这一点。这些眼镜是让 AI 观察你所见、听到你所闻的理想载体，因此 AI 可以随时随地协助你。”

这一声明是在 OpenAI 首席执行官 Sam Altman 在瑞士达沃斯的世界经济论坛上对 AGI 发表评论，并在他 2023 年 11 月重新上任两个月后对 AGI 存在风险的态度有所缓和之后发布的。尽管 Meta 的首席科学家 Yann LeCun 一直对 AGI 的即时到来持怀疑态度，认为至少在未来五年内不会实现，但这一声明依然发布了。

最后，Meta 计划将其未来的 AGI 开源的消息是在 VentureBeat 称 lama 和开源 AI “赢得了” 2023 年仅几个月后发布的。这一声明无疑将引发关于开源与闭源 AI 的进一步讨论，尤其是在 Anthropic 发布论文称开源模型可能隐藏着具有破坏性的“沉睡代理”之后。</title>
            <link>https://nitter.cz/dotey/status/1748120766103609715#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748120766103609715#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 23:10:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta 正在开发开源的通用人工智能（AGI），马克·扎克伯格宣布。<br />
<br />
今天，扎克伯格意外的在一则 Instagram Reel 中透露，Meta 正致力于开发开源的通用人工智能（AGI）。为了实现这一目标，公司正将其两大 AI 研究团队 FAIR 和 GenAI 进行更紧密的整合，以构建完整的通用智能，并尽可能地开源。<br />
<br />
扎克伯格在视频中谈道：“我们的长期愿景是开发通用智能，并以负责任的方式进行开源，让每个人都能广泛受益。”他在视频中表示：“我们清楚地认识到，下一代服务的需求是构建完整的通用智能，包括最优秀的 AI 助手、创意工作者用 AI、企业用 AI 等，这需要在 AI 的各个领域，包括推理、规划、编程、记忆和其他认知能力上取得进步。”<br />
<br />
在谈到 Llama 3、基础设施和元宇宙方面，扎克伯格也表达了兴奋之情。他强调，公司目前正在培训 Llama 3 模型，并正在建设大规模的计算基础设施，包括到今年年底部署 350,000 个 Nvidia H100s。<br />
<br />
扎克伯格还对元宇宙和 Meta 的 Ray-Ban 智能眼镜表现出极大的热情。“人们将需要新型 AI 设备，而这将逐渐将 AI 与元宇宙融合。”他说，“我认为我们很多人将会经常通过对话与 AI 互动。而且我相信，很多人会通过佩戴智能眼镜来实现这一点。这些眼镜是让 AI 观察你所见、听到你所闻的理想载体，因此 AI 可以随时随地协助你。”<br />
<br />
这一声明是在 OpenAI 首席执行官 Sam Altman 在瑞士达沃斯的世界经济论坛上对 AGI 发表评论，并在他 2023 年 11 月重新上任两个月后对 AGI 存在风险的态度有所缓和之后发布的。尽管 Meta 的首席科学家 Yann LeCun 一直对 AGI 的即时到来持怀疑态度，认为至少在未来五年内不会实现，但这一声明依然发布了。<br />
<br />
最后，Meta 计划将其未来的 AGI 开源的消息是在 VentureBeat 称 lama 和开源 AI “赢得了” 2023 年仅几个月后发布的。这一声明无疑将引发关于开源与闭源 AI 的进一步讨论，尤其是在 Anthropic 发布论文称开源模型可能隐藏着具有破坏性的“沉睡代理”之后。</p>
<p><a href="https://nitter.cz/VentureBeat/status/1748046430193746189#m">nitter.cz/VentureBeat/status/1748046430193746189#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgxMjAzMDk1MjAwMzU4NDAvcHUvaW1nLzV4Y0RRd1hjM2ZraGgyY1kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748190345769484715#m</id>
            <title>KREA AI现在可以直接通过文字生成图片，支持抠图和橡皮擦，这个演示有点搞的，把马和蛤蟆组合起来了。</title>
            <link>https://nitter.cz/op7418/status/1748190345769484715#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748190345769484715#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:47:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>KREA AI现在可以直接通过文字生成图片，支持抠图和橡皮擦，这个演示有点搞的，把马和蛤蟆组合起来了。</p>
<p><a href="https://nitter.cz/krea_ai/status/1748045292405195198#m">nitter.cz/krea_ai/status/1748045292405195198#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748185756211028086#m</id>
            <title>前几天微软DragNUWA的一个应用，用非常简陋的 3D 白模控制视频生成中内容的运动方向和速度。

生成的视频看起来非常稳定，一个非常好的取巧方法。</title>
            <link>https://nitter.cz/op7418/status/1748185756211028086#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748185756211028086#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 03:28:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>前几天微软DragNUWA的一个应用，用非常简陋的 3D 白模控制视频生成中内容的运动方向和速度。<br />
<br />
生成的视频看起来非常稳定，一个非常好的取巧方法。</p>
<p><a href="https://nitter.cz/TDS_95514874/status/1747753162226675844#m">nitter.cz/TDS_95514874/status/1747753162226675844#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1748154256493072737#m</id>
            <title>啊？这都行？</title>
            <link>https://nitter.cz/op7418/status/1748154256493072737#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1748154256493072737#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 01:23:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>啊？这都行？</p>
<p><a href="https://nitter.cz/satoshi/status/1748111687482323232#m">nitter.cz/satoshi/status/1748111687482323232#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>