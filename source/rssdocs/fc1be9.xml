<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751607245698437446#m</id>
            <title>🧪今天继续整点抽象的壁纸，想复刻一下之前那种3D渲染出来的抽象玻璃质感的图片。
整了半天发现只需要前面加上“extreme macro photo of clean polished glass”清洁抛光玻璃的极微距照片，这段话就行。

然后后面的词就是常规的颜色以及背景颜色，还有模糊之类的内容了，这个提示词想产生好看的图，主要的就是配色的描述，不能只用纯色的颜色描述，得用比如海军蓝、靛蓝等多种颜色混合。

提示词：
extreme macro photo of clean polished glass, edges with light from four different colors, blurred, warmcore, light red and indigo, grey background, natural colors, depth of field --ar 16:9 --style raw --stylize 0 --v 6

#晚安提示词 #Midjourney #Catjourney</title>
            <link>https://nitter.cz/op7418/status/1751607245698437446#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751607245698437446#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 14:04:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪今天继续整点抽象的壁纸，想复刻一下之前那种3D渲染出来的抽象玻璃质感的图片。<br />
整了半天发现只需要前面加上“extreme macro photo of clean polished glass”清洁抛光玻璃的极微距照片，这段话就行。<br />
<br />
然后后面的词就是常规的颜色以及背景颜色，还有模糊之类的内容了，这个提示词想产生好看的图，主要的就是配色的描述，不能只用纯色的颜色描述，得用比如海军蓝、靛蓝等多种颜色混合。<br />
<br />
提示词：<br />
extreme macro photo of clean polished glass, edges with light from four different colors, blurred, warmcore, light red and indigo, grey background, natural colors, depth of field --ar 16:9 --style raw --stylize 0 --v 6<br />
<br />
<a href="https://nitter.cz/search?q=%23晚安提示词">#晚安提示词</a> <a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> <a href="https://nitter.cz/search?q=%23Catjourney">#Catjourney</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2cWE0QUE5MWthLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2Z2FVQUFnOTVCLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2dGEwQUF5dWZrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3MWU2OWFvQUFmdGZzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751602173845348373#m</id>
            <title>Huggingface上的一篇内容，非常详细的介绍了如何从零开始实现一个MoE架构的语言模型。

文章详细解释了模型的实施过程，包括采用稀疏混合专家取代传统的前馈神经网络，实现 top-k 门控和带噪声的 top-k 门控，以及采用 Kaiming He 初始化技术。

作者还说明了从 makemore 架构保持不变的元素，比如数据集处理、分词预处理和语言建模任务。

最后还提供了一个 GitHub 仓库链接，用于实现模型的整个过程。

内容地址：https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch</title>
            <link>https://nitter.cz/op7418/status/1751602173845348373#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751602173845348373#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 13:44:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Huggingface上的一篇内容，非常详细的介绍了如何从零开始实现一个MoE架构的语言模型。<br />
<br />
文章详细解释了模型的实施过程，包括采用稀疏混合专家取代传统的前馈神经网络，实现 top-k 门控和带噪声的 top-k 门控，以及采用 Kaiming He 初始化技术。<br />
<br />
作者还说明了从 makemore 架构保持不变的元素，比如数据集处理、分词预处理和语言建模任务。<br />
<br />
最后还提供了一个 GitHub 仓库链接，用于实现模型的整个过程。<br />
<br />
内容地址：<a href="https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch">huggingface.co/blog/AviSoori…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3d2E2Y2FJQUFTMUN5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751600800668004750#m</id>
            <title>Dolphin-Mixtral这个模型可以让Json格式文件的输出正确率从Mixtral原模型的60%提高到90%。</title>
            <link>https://nitter.cz/op7418/status/1751600800668004750#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751600800668004750#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 13:39:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Dolphin-Mixtral这个模型可以让Json格式文件的输出正确率从Mixtral原模型的60%提高到90%。</p>
<p><a href="https://nitter.cz/DrTBehrens/status/1751523382196117675#m">nitter.cz/DrTBehrens/status/1751523382196117675#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1751388813237141542#m</id>
            <title>RT by @op7418: 来自浙江大学、腾讯 AI 实验室和西湖大学的新论文：《WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models》

这篇论文详细的讲解了如何借助 GPT-4V 这样的多模态模型，与开放网络中的网站交互，完成用户的各项指令。

如果你有做过类似的事情的话，会发现其实还是很有挑战的，因为让 AI 遵循指令操作网页，特定的网站相对容易，因为网页元素和路径比较固定，但是开放环境的话，每个网站都不一样，交互方式也千差万别，再加上浮动广告、弹出窗口和网页内容实时更新等等。

具体在实现层面，要先理解当前网页的内容，然后根据用户指令，在网页上选择正确的操作，根据操作的结果再继续下一步操作，直到完成任务。

举例来说，我们要去苹果官网查询，附近的哪个苹果店能买到特定型号的 iPad 保护壳（Smart Folio）。如果是人操作的话，要打开官网，找到配件页面，搜索关键字，找到配件查看详情，从详情页选择弹出位置搜索界面，输入邮编，找到最近的苹果店。但这系列操作对于 AI 来说还是很有挑战的。

那么 WebVoyager 是怎么来做的呢？

一、AI 如何浏览操作网页？

首先，WebVoyager 不是用的普通浏览器，而是基于 Selenium，这是一个自动化网页测试工具，可以方便的截图，可以自动化操作网页浏览器。

但是要让 GPT-4V 能识别和操作网页元素，还需要对网页上的可以操作的元素进行标记，WebVoyager 开发了一个叫 GPT-4-ACT4 的 JavaScript 工具，它能够根据网页元素的类型自动识别交互元素，并在这些元素上覆盖带有数字标记的黑色边框。

此外，GPT-4-ACT4 还能向智能体提供了一些辅助文本，如交互元素内的文字内容、元素类型以及 aria-label 属性中可能的注释文本，以简化观察过程。

如果你有些自动化测试代码的经验的话，可以知道我们可以用 JavaScript 或者 Python 脚本灵活的操作网页做任意操作，但是对于 AI 来说，如果让它直接写代码可能会出错率比较高，所以 WebVoyager 将常用的网页操作进行了归类，提供了有限的几种操作，例如：点击、输入、滚动、等待、返回上一页等等。

这样 AI 就不需要写代码，而是直接基于这几种操作给出清晰的指令，根据 AI 的指令，WebVoyager 将指令翻译成操作 Selenium 的代码操作网页。

这些为后面 GPT-4V 识别和提供后续指令提供了基础，否则 GPT-4V 无法清晰的描述出下一步要采取的操作。

二、如何让 AI 清晰的给出网页操作的指令？

然后就是 Prompt，Prompt 就是和 AI 交互的指令。要让 GPT-4V 帮助我们完成任务，光有截图还不够，还需要让 AI 能根据截图和任务，清晰的说明下一步如何操作，才能去相应的网站，借助外部工具进行交互。

WebVoyager 采用的是 ReAct 的 Prompt 框架，让 AI 能够根据目标任务和当前状态，推理出下一步的行动，每一步都采用：思考（Thought）、行动（Action）和观察（Observe）的结构，思考推理出行动，行动完成后观察行动后的结果，根据观察的结果进一步思考，思考推理出下一步的行动，这样一步步，直到完成任务。

举例来说要查询附近哪个苹果店可以买到特定型号的 iPad 保护壳，基于 ReAct 的框架是这样做的：

思考 1：我要找哪个苹果店可以买到特定型号的 iPad 保护壳，我需要打开苹果官网
行动 1：打开苹果官网
观察 1：苹果官网已经打开，上面有 Mac、iPhone、iPad、配件……导航

思考 2：iPad 保护壳属于配件，我已经打开配件页面
行动 2：点击打开配件页面
观察 2：配件页面打开，有导航，有推荐配件，有搜索框……

思考 3：我应该使用搜索框输入 Smart Folio 搜索
行动 3：在搜索框中输入 Smart Folio，点击搜索按钮
观察 3：列出了所有 Smart Folio 相关产品，第一项是 Smart Folio for iPad Pro 11，第二项是 Smart Folio for iPad……

思考 4：第一个搜索结果就是我想要的，我需要点击进入详情页面
行动 4：打开第一个 Smart Folio 详情页
观察 4：标题…介绍…图片…苹果商店……

思考 5：点击苹果商店链接查看有哪些商店
行动 5：点击苹果商店链接
观察 5：弹出对话框，有苹果商店列表 1,2,3,4…，有位置搜索框

思考 6：这些苹果商店离我太远，需要按照我的邮编寻找最近的
行动 6：输入邮编到位置输入框，搜索
观察 6：列出了新的苹果商店列表 1,2,3,4…

思考 7：第一个苹果商店就是离我最近的苹果店，任务完成

三、效果如何？

根据论文上的结果显示，WebVoyager 在任务成功率上达到了 55.7%。这个结果显然还达不到替代人类操作的效果，但是作为现阶段来说，已经算是个不错的成绩。未来随着 AI 能力的增强，成功率应该可以做到更高。

目前 WebVoyager 任务失败的原因主要有：

1. 导航失败
a) 如果智能体的搜索查询不够精确和明确，它会被海量无关搜索结果淹没。在这种情况下，智能体可能倾向于浏览这些不相关的结果，而不是纠正之前的错误；b) 当只有屏幕的一小部分可滚动时，智能体可能找不到正确的滚动区域，反复进行无效的滚动操作；c) 有时在网页中部，智能体难以决定是向上滚动还是向下滚动。
2. 视觉识别不足
a) 智能体无法正确理解一些不常见的模式，比如误将代表发音的字符或数学公式理解错了；b) 智能体没有识别出两次观察之间的微妙差异，误以为操作失败了；c) 由于元素之间位置接近，智能体有时会选错了操作对象。比如，它可能会将相邻的元素混淆，或者把日历上的数字误认为是数值标签。有时，文本信息对于区分密集的网页元素至关重要。
3. 幻觉
理解和遵循复杂的提示对智能体来说是一个重大挑战。此外，长时间的操作路径可能导致上下文过于冗长，从而妨碍了有效的指令执行。

总的来说，WebVoyager 是一个很不错的尝试，期待未来 AI 能真正的帮助我们操作网页，解放双手。

完整的论文翻译：https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models</title>
            <link>https://nitter.cz/dotey/status/1751388813237141542#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1751388813237141542#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 23:36:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来自浙江大学、腾讯 AI 实验室和西湖大学的新论文：《WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models》<br />
<br />
这篇论文详细的讲解了如何借助 GPT-4V 这样的多模态模型，与开放网络中的网站交互，完成用户的各项指令。<br />
<br />
如果你有做过类似的事情的话，会发现其实还是很有挑战的，因为让 AI 遵循指令操作网页，特定的网站相对容易，因为网页元素和路径比较固定，但是开放环境的话，每个网站都不一样，交互方式也千差万别，再加上浮动广告、弹出窗口和网页内容实时更新等等。<br />
<br />
具体在实现层面，要先理解当前网页的内容，然后根据用户指令，在网页上选择正确的操作，根据操作的结果再继续下一步操作，直到完成任务。<br />
<br />
举例来说，我们要去苹果官网查询，附近的哪个苹果店能买到特定型号的 iPad 保护壳（Smart Folio）。如果是人操作的话，要打开官网，找到配件页面，搜索关键字，找到配件查看详情，从详情页选择弹出位置搜索界面，输入邮编，找到最近的苹果店。但这系列操作对于 AI 来说还是很有挑战的。<br />
<br />
那么 WebVoyager 是怎么来做的呢？<br />
<br />
一、AI 如何浏览操作网页？<br />
<br />
首先，WebVoyager 不是用的普通浏览器，而是基于 Selenium，这是一个自动化网页测试工具，可以方便的截图，可以自动化操作网页浏览器。<br />
<br />
但是要让 GPT-4V 能识别和操作网页元素，还需要对网页上的可以操作的元素进行标记，WebVoyager 开发了一个叫 GPT-4-ACT4 的 JavaScript 工具，它能够根据网页元素的类型自动识别交互元素，并在这些元素上覆盖带有数字标记的黑色边框。<br />
<br />
此外，GPT-4-ACT4 还能向智能体提供了一些辅助文本，如交互元素内的文字内容、元素类型以及 aria-label 属性中可能的注释文本，以简化观察过程。<br />
<br />
如果你有些自动化测试代码的经验的话，可以知道我们可以用 JavaScript 或者 Python 脚本灵活的操作网页做任意操作，但是对于 AI 来说，如果让它直接写代码可能会出错率比较高，所以 WebVoyager 将常用的网页操作进行了归类，提供了有限的几种操作，例如：点击、输入、滚动、等待、返回上一页等等。<br />
<br />
这样 AI 就不需要写代码，而是直接基于这几种操作给出清晰的指令，根据 AI 的指令，WebVoyager 将指令翻译成操作 Selenium 的代码操作网页。<br />
<br />
这些为后面 GPT-4V 识别和提供后续指令提供了基础，否则 GPT-4V 无法清晰的描述出下一步要采取的操作。<br />
<br />
二、如何让 AI 清晰的给出网页操作的指令？<br />
<br />
然后就是 Prompt，Prompt 就是和 AI 交互的指令。要让 GPT-4V 帮助我们完成任务，光有截图还不够，还需要让 AI 能根据截图和任务，清晰的说明下一步如何操作，才能去相应的网站，借助外部工具进行交互。<br />
<br />
WebVoyager 采用的是 ReAct 的 Prompt 框架，让 AI 能够根据目标任务和当前状态，推理出下一步的行动，每一步都采用：思考（Thought）、行动（Action）和观察（Observe）的结构，思考推理出行动，行动完成后观察行动后的结果，根据观察的结果进一步思考，思考推理出下一步的行动，这样一步步，直到完成任务。<br />
<br />
举例来说要查询附近哪个苹果店可以买到特定型号的 iPad 保护壳，基于 ReAct 的框架是这样做的：<br />
<br />
思考 1：我要找哪个苹果店可以买到特定型号的 iPad 保护壳，我需要打开苹果官网<br />
行动 1：打开苹果官网<br />
观察 1：苹果官网已经打开，上面有 Mac、iPhone、iPad、配件……导航<br />
<br />
思考 2：iPad 保护壳属于配件，我已经打开配件页面<br />
行动 2：点击打开配件页面<br />
观察 2：配件页面打开，有导航，有推荐配件，有搜索框……<br />
<br />
思考 3：我应该使用搜索框输入 Smart Folio 搜索<br />
行动 3：在搜索框中输入 Smart Folio，点击搜索按钮<br />
观察 3：列出了所有 Smart Folio 相关产品，第一项是 Smart Folio for iPad Pro 11，第二项是 Smart Folio for iPad……<br />
<br />
思考 4：第一个搜索结果就是我想要的，我需要点击进入详情页面<br />
行动 4：打开第一个 Smart Folio 详情页<br />
观察 4：标题…介绍…图片…苹果商店……<br />
<br />
思考 5：点击苹果商店链接查看有哪些商店<br />
行动 5：点击苹果商店链接<br />
观察 5：弹出对话框，有苹果商店列表 1,2,3,4…，有位置搜索框<br />
<br />
思考 6：这些苹果商店离我太远，需要按照我的邮编寻找最近的<br />
行动 6：输入邮编到位置输入框，搜索<br />
观察 6：列出了新的苹果商店列表 1,2,3,4…<br />
<br />
思考 7：第一个苹果商店就是离我最近的苹果店，任务完成<br />
<br />
三、效果如何？<br />
<br />
根据论文上的结果显示，WebVoyager 在任务成功率上达到了 55.7%。这个结果显然还达不到替代人类操作的效果，但是作为现阶段来说，已经算是个不错的成绩。未来随着 AI 能力的增强，成功率应该可以做到更高。<br />
<br />
目前 WebVoyager 任务失败的原因主要有：<br />
<br />
1. 导航失败<br />
a) 如果智能体的搜索查询不够精确和明确，它会被海量无关搜索结果淹没。在这种情况下，智能体可能倾向于浏览这些不相关的结果，而不是纠正之前的错误；b) 当只有屏幕的一小部分可滚动时，智能体可能找不到正确的滚动区域，反复进行无效的滚动操作；c) 有时在网页中部，智能体难以决定是向上滚动还是向下滚动。<br />
2. 视觉识别不足<br />
a) 智能体无法正确理解一些不常见的模式，比如误将代表发音的字符或数学公式理解错了；b) 智能体没有识别出两次观察之间的微妙差异，误以为操作失败了；c) 由于元素之间位置接近，智能体有时会选错了操作对象。比如，它可能会将相邻的元素混淆，或者把日历上的数字误认为是数值标签。有时，文本信息对于区分密集的网页元素至关重要。<br />
3. 幻觉<br />
理解和遵循复杂的提示对智能体来说是一个重大挑战。此外，长时间的操作路径可能导致上下文过于冗长，从而妨碍了有效的指令执行。<br />
<br />
总的来说，WebVoyager 是一个很不错的尝试，期待未来 AI 能真正的帮助我们操作网页，解放双手。<br />
<br />
完整的论文翻译：<a href="https://baoyu.io/translations/ai-paper/2401.13919-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models">baoyu.io/translations/ai-pap…</a></p>
<p><a href="https://nitter.cz/wyu_nd/status/1750743389287669940#m">nitter.cz/wyu_nd/status/1750743389287669940#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U0dVhsRVcwQUFfQnNXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U0dWdpVVhJQUUxZkp1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U0dXNNTVcwQUFCTDVJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751170799082643524#m</id>
            <title>RT by @op7418: Stability ai即将发布图像放大产品或者模型，用的演示还是Magnific AI曾经用过的劳拉图片。</title>
            <link>https://nitter.cz/op7418/status/1751170799082643524#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751170799082643524#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 09:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability ai即将发布图像放大产品或者模型，用的演示还是Magnific AI曾经用过的劳拉图片。</p>
<p><a href="https://nitter.cz/EMostaque/status/1751061406382735633#m">nitter.cz/EMostaque/status/1751061406382735633#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Uxb0xLZWFJQUEwa2FZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751292333268893851#m</id>
            <title>RT by @op7418: 我刚才发现自己有了ChatGPT昨天发布的GPTs联动的功能，试了一下确实强大。

下面用一个例子看一下如何使用和具体能力，还有一个对简中用户比较重要的操作。

我自己看论文会先下载下来让ChatGPT总结，然后会用宝玉的科技文章翻译GPTs对总结的内容进行精细的翻译，以前完成这两步起码需要我切两个窗口，然后复制粘贴一次。

现在我只需要在总结完成后输入“@”找到需要的GPTs，输入要求就可以在一次对话中完成，甚至我还可以再搞一个自动发推的GPTs在ChatGPT直接将翻译内容发到推特上。

最后一个对简中用户不太友好的BUG：拉起GPTs选择的时候你输入的“@”符号需要在英文输入法下才能调用这个功能，中文输入法下面没办法拉起GPTs选择，OpenAI做产品是真的糙。</title>
            <link>https://nitter.cz/op7418/status/1751292333268893851#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751292333268893851#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:13:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我刚才发现自己有了ChatGPT昨天发布的GPTs联动的功能，试了一下确实强大。<br />
<br />
下面用一个例子看一下如何使用和具体能力，还有一个对简中用户比较重要的操作。<br />
<br />
我自己看论文会先下载下来让ChatGPT总结，然后会用宝玉的科技文章翻译GPTs对总结的内容进行精细的翻译，以前完成这两步起码需要我切两个窗口，然后复制粘贴一次。<br />
<br />
现在我只需要在总结完成后输入“@”找到需要的GPTs，输入要求就可以在一次对话中完成，甚至我还可以再搞一个自动发推的GPTs在ChatGPT直接将翻译内容发到推特上。<br />
<br />
最后一个对简中用户不太友好的BUG：拉起GPTs选择的时候你输入的“@”符号需要在英文输入法下才能调用这个功能，中文输入法下面没办法拉起GPTs选择，OpenAI做产品是真的糙。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTEyOTIyNTcwODc2ODg3MDQvcHUvaW1nL0dFdHpDX2JJcWhsNmY3S3YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751187683400105984#m</id>
            <title>RT by @op7418: 昨晚ChatGPT推出了通过在正常的聊天中@其他GPTs协同处理任务的能力，这个能力非常强大。

他让GPTs的可能性多了非常多，我没有被灰度到，Dan Shipper这个演示非常详细和完整。

他演示了如何将对话结果利用GPTs直接保存到Notion中，我顺便翻译了这个视频，如果你也没有被灰度到可以看看这个视频。</title>
            <link>https://nitter.cz/op7418/status/1751187683400105984#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751187683400105984#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 10:17:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚ChatGPT推出了通过在正常的聊天中@其他GPTs协同处理任务的能力，这个能力非常强大。<br />
<br />
他让GPTs的可能性多了非常多，我没有被灰度到，Dan Shipper这个演示非常详细和完整。<br />
<br />
他演示了如何将对话结果利用GPTs直接保存到Notion中，我顺便翻译了这个视频，如果你也没有被灰度到可以看看这个视频。</p>
<p><a href="https://nitter.cz/danshipper/status/1751017376143794415#m">nitter.cz/danshipper/status/1751017376143794415#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTExODc1ODgxMDk3OTUzMjgvcHUvaW1nLzJGQkdTdjNtUnFSTjdJZEsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751293373074510210#m</id>
            <title>RT by @op7418: 另一个GPTs调用能力的牛批演示，在一个聊天中先使用Grimoire构建一个网页。

然后直接调用DesignerGPT对上面Grimoire生成的网页进行部署。

这样通过两个GPTs的协作，就直接获得了一个部署好的网站。</title>
            <link>https://nitter.cz/op7418/status/1751293373074510210#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751293373074510210#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:17:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个GPTs调用能力的牛批演示，在一个聊天中先使用Grimoire构建一个网页。<br />
<br />
然后直接调用DesignerGPT对上面Grimoire生成的网页进行部署。<br />
<br />
这样通过两个GPTs的协作，就直接获得了一个部署好的网站。</p>
<p><a href="https://nitter.cz/skirano/status/1751092906964787406#m">nitter.cz/skirano/status/1751092906964787406#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751296580836377017#m</id>
            <title>RT by @op7418: 今天是InstructGPT发布两周年的纪念日，它是现代大语言模型的开山鼻祖。

Jim Fan介绍了InstructGPT的重要性并且说了几条关于InstructGPT非常有意思的点。还展示了InstructGPT中非常经典的三步LLM训练方法的图片，我也顺便让GPT-4解释了一下也顺便放在下面：

InstructGPT 开创了一个经典的模型训练方法：先进行预训练，然后是监督式微调，最后是基于强化学习的人类反馈（Reinforcement Learning from Human Feedback, RLHF）。
这个策略至今仍被广泛采用，虽然有些许变化，比如 DPO 策略。

InstructGPT 可能是 OpenAI 最后一次详细介绍他们如何训练尖端模型的论文。
回顾这两年，Jim Fan认为它标志着大语言模型从学术研究（GPT-3）走向实际应用（ChatGPT）的关键转折点。

一些有趣的事实：

1️⃣InstructGPT 并非 RLHF 的发明者。实际上，它的博客链接指向了 OpenAI 团队在 2017 年完成的最早的 RLHF 研究。RLHF 的初衷是解决模拟机器人领域中难以定义的任务。它通过要求人类注释者给出 900 个简单的是非选择，帮助一个名为“跳跃机器人”的机器人在模拟环境中学会了后空翻：https://openai.com/research/learning-from-human-preferences

2️⃣InstructGPT 在 2022 年的 NeurIPS 会议上亮相，地点是新奥尔良。当时Jim Fan正在会上展示他的项目 MineDojo，看到 OpenAI 的展示非常惊喜。

3️⃣这些模型有三种规模：1.3B、6B 和 175B。与老式的、需要复杂提示的 GPT-3-175B 相比，标注者更加青睐 Instruct-1.3B。而且，Microsoft Phi-1，作为众所周知的小型优秀语言模型之一，也是 1.3B。

4️⃣InstructGPT 在展示研究方面堪称典范。它的三步骤图解清晰易懂，成为 AI 领域最具标志性的视觉之一。它的引言部分直奔主题，用粗体突出了八个主要观点。对于模型局限性和偏见问题的讨论，也是实事求是和坦诚的。

InstructGPT模型三步训练方法示意图介绍：

1️⃣第1步：收集示范数据，并训练一个监督策略。

从我们的提示数据集中抽取一个提示，例如“向一个6岁的孩子解释月球登陆”。
标注者提供期望的输出行为，例如“一些人去了月球”。
这些数据被用来通过监督学习细调GPT-3模型。

2️⃣第2步：收集比较数据，并训练一个奖励模型。

抽取一个提示和几个模型输出样本，例如同样是“向一个6岁的孩子解释月球登陆”。
标注者会对这些输出进行排名，从最好到最差。
这些数据被用来训练我们的奖励模型。

3️⃣第3步：使用强化学习对奖励模型优化策略。

从数据集中抽取一个新的提示，例如“写一个关于青蛙的故事”。
策略生成一个输出，比如开始写“从前有一次…”。
奖励模型计算一个奖励值给这个输出。
这个奖励值被用来通过PPO（比例策略优化）更新策略。
整个过程展示了从收集数据到通过人类标注者的反馈来训练和优化人工智能模型的步骤。这种方法结合了监督学习和强化学习来改进模型的性能。

OpenAI原文：https://openai.com/research/instruction-following</title>
            <link>https://nitter.cz/op7418/status/1751296580836377017#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751296580836377017#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:30:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天是InstructGPT发布两周年的纪念日，它是现代大语言模型的开山鼻祖。<br />
<br />
Jim Fan介绍了InstructGPT的重要性并且说了几条关于InstructGPT非常有意思的点。还展示了InstructGPT中非常经典的三步LLM训练方法的图片，我也顺便让GPT-4解释了一下也顺便放在下面：<br />
<br />
InstructGPT 开创了一个经典的模型训练方法：先进行预训练，然后是监督式微调，最后是基于强化学习的人类反馈（Reinforcement Learning from Human Feedback, RLHF）。<br />
这个策略至今仍被广泛采用，虽然有些许变化，比如 DPO 策略。<br />
<br />
InstructGPT 可能是 OpenAI 最后一次详细介绍他们如何训练尖端模型的论文。<br />
回顾这两年，Jim Fan认为它标志着大语言模型从学术研究（GPT-3）走向实际应用（ChatGPT）的关键转折点。<br />
<br />
一些有趣的事实：<br />
<br />
1️⃣InstructGPT 并非 RLHF 的发明者。实际上，它的博客链接指向了 OpenAI 团队在 2017 年完成的最早的 RLHF 研究。RLHF 的初衷是解决模拟机器人领域中难以定义的任务。它通过要求人类注释者给出 900 个简单的是非选择，帮助一个名为“跳跃机器人”的机器人在模拟环境中学会了后空翻：<a href="https://openai.com/research/learning-from-human-preferences">openai.com/research/learning…</a><br />
<br />
2️⃣InstructGPT 在 2022 年的 NeurIPS 会议上亮相，地点是新奥尔良。当时Jim Fan正在会上展示他的项目 MineDojo，看到 OpenAI 的展示非常惊喜。<br />
<br />
3️⃣这些模型有三种规模：1.3B、6B 和 175B。与老式的、需要复杂提示的 GPT-3-175B 相比，标注者更加青睐 Instruct-1.3B。而且，Microsoft Phi-1，作为众所周知的小型优秀语言模型之一，也是 1.3B。<br />
<br />
4️⃣InstructGPT 在展示研究方面堪称典范。它的三步骤图解清晰易懂，成为 AI 领域最具标志性的视觉之一。它的引言部分直奔主题，用粗体突出了八个主要观点。对于模型局限性和偏见问题的讨论，也是实事求是和坦诚的。<br />
<br />
InstructGPT模型三步训练方法示意图介绍：<br />
<br />
1️⃣第1步：收集示范数据，并训练一个监督策略。<br />
<br />
从我们的提示数据集中抽取一个提示，例如“向一个6岁的孩子解释月球登陆”。<br />
标注者提供期望的输出行为，例如“一些人去了月球”。<br />
这些数据被用来通过监督学习细调GPT-3模型。<br />
<br />
2️⃣第2步：收集比较数据，并训练一个奖励模型。<br />
<br />
抽取一个提示和几个模型输出样本，例如同样是“向一个6岁的孩子解释月球登陆”。<br />
标注者会对这些输出进行排名，从最好到最差。<br />
这些数据被用来训练我们的奖励模型。<br />
<br />
3️⃣第3步：使用强化学习对奖励模型优化策略。<br />
<br />
从数据集中抽取一个新的提示，例如“写一个关于青蛙的故事”。<br />
策略生成一个输出，比如开始写“从前有一次…”。<br />
奖励模型计算一个奖励值给这个输出。<br />
这个奖励值被用来通过PPO（比例策略优化）更新策略。<br />
整个过程展示了从收集数据到通过人类标注者的反馈来训练和优化人工智能模型的步骤。这种方法结合了监督学习和强化学习来改进模型的性能。<br />
<br />
OpenAI原文：<a href="https://openai.com/research/instruction-following">openai.com/research/instruct…</a></p>
<p><a href="https://nitter.cz/DrJimFan/status/1751285761364906476#m">nitter.cz/DrJimFan/status/1751285761364906476#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0UzYWM2QWFVQUEyQWFOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751298793130065978#m</id>
            <title>RT by @op7418: 虚幻引擎5生成的一个演示视频，非常真实，如果不是偶尔的画面撕裂我都无法分辨出来这是渲染的。

前几天虚幻引擎更新5.3之后才基本把之前发布时说的所有能力补齐，确实牛批。</title>
            <link>https://nitter.cz/op7418/status/1751298793130065978#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751298793130065978#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:39:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>虚幻引擎5生成的一个演示视频，非常真实，如果不是偶尔的画面撕裂我都无法分辨出来这是渲染的。<br />
<br />
前几天虚幻引擎更新5.3之后才基本把之前发布时说的所有能力补齐，确实牛批。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwODkwMDE5MzcwMjc0ODE2L2ltZy80bllIZE01SEp3QTNfVmM0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751300318464487521#m</id>
            <title>RT by @op7418: 昨晚谷歌还发布了一种叫“仅权重量化”（weight-only quantization）的技术。这项技术使用了 8 位整数（S8）来表示权重，同时使用 16 位浮点数（F16）来表示输入数据，以此进行矩阵乘法运算。这种方法在保证一定精度的同时，提高了运算效率。

详细内容：https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html</title>
            <link>https://nitter.cz/op7418/status/1751300318464487521#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751300318464487521#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:45:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚谷歌还发布了一种叫“仅权重量化”（weight-only quantization）的技术。这项技术使用了 8 位整数（S8）来表示权重，同时使用 16 位浮点数（F16）来表示输入数据，以此进行矩阵乘法运算。这种方法在保证一定精度的同时，提高了运算效率。<br />
<br />
详细内容：<a href="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">blog.research.google/2024/01…</a></p>
<p><a href="https://nitter.cz/GoogleAI/status/1750977073009881469#m">nitter.cz/GoogleAI/status/1750977073009881469#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MDk3NzAyOTQ2NDYwODc2OC9mZnpKT21kQj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1751292605781205337#m</id>
            <title>R to @op7418: 我自己的体验在这里：
https://x.com/op7418/status/1751292333268893851?s=20</title>
            <link>https://nitter.cz/op7418/status/1751292605781205337#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1751292605781205337#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我自己的体验在这里：<br />
<a href="https://x.com/op7418/status/1751292333268893851?s=20">x.com/op7418/status/17512923…</a></p>
<p><a href="https://nitter.cz/op7418/status/1751292333268893851#m">nitter.cz/op7418/status/1751292333268893851#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750929802461388902#m</id>
            <title>这两个确实挺合适合作的，现在可以将Perplexity设置为Arc的默认搜索引擎。</title>
            <link>https://nitter.cz/op7418/status/1750929802461388902#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750929802461388902#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 17:12:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这两个确实挺合适合作的，现在可以将Perplexity设置为Arc的默认搜索引擎。</p>
<p><a href="https://nitter.cz/joshm/status/1750888638324560289#m">nitter.cz/joshm/status/1750888638324560289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>