<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1743091433639465139#m</id>
            <title>感觉我们这代人以后养老要靠这玩意了。洗衣服、逗猫、煮咖啡准备早餐、洗碗都行。
基本家务基本上通过家电搭配机械臂机器人完成。</title>
            <link>https://nitter.cz/op7418/status/1743091433639465139#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1743091433639465139#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 02:05:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感觉我们这代人以后养老要靠这玩意了。洗衣服、逗猫、煮咖啡准备早餐、洗碗都行。<br />
基本家务基本上通过家电搭配机械臂机器人完成。</p>
<p><a href="https://nitter.cz/zipengfu/status/1742973258528612724#m">nitter.cz/zipengfu/status/1742973258528612724#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1743076793513594975#m</id>
            <title>卧槽，Nick 这个实验有意思。
把一张穿搭推荐的平面图用Midjourney生成对应的照片，只靠提示词。

好玩，各位想锻炼自己提示词书写能力的可以找一张类似图片试试。</title>
            <link>https://nitter.cz/op7418/status/1743076793513594975#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1743076793513594975#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 01:07:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽，Nick 这个实验有意思。<br />
把一张穿搭推荐的平面图用Midjourney生成对应的照片，只靠提示词。<br />
<br />
好玩，各位想锻炼自己提示词书写能力的可以找一张类似图片试试。</p>
<p><a href="https://nitter.cz/nickfloats/status/1743000875235168605#m">nitter.cz/nickfloats/status/1743000875235168605#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RDbkdTRWJBQUFIcEFoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RDbkdTRWJBQUVKMzY3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742815195544817840#m</id>
            <title>RT by @op7418: 昨晚 Midjourney 办公时间的一些信息，Niji6 马上就会来，太期待了。

其他即将发布的内容有：
✦计划在接下来的一个星期内，在版本 6 中改进提示准确度和文本渲染效果。

✦下个星期会有一个重要的网页 alpha 版更新。该更新将包含文件夹、更好的筛选功能以及更新后的存档页面。也许还会增加一些社交功能。

✦Niji v6 即将面世。

✦另外，“—stylize” 参数在较高数值上表现得更佳，这意味着它能够更多地关注给出的提示，并添加细节信息。

✦v6 更多功能正在开发中，比如区域变化和修复图像，预计将于一月份发布。

✦目标是在本月底将 v6 设为默认版本。</title>
            <link>https://nitter.cz/op7418/status/1742815195544817840#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742815195544817840#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 07:48:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚 Midjourney 办公时间的一些信息，Niji6 马上就会来，太期待了。<br />
<br />
其他即将发布的内容有：<br />
✦计划在接下来的一个星期内，在版本 6 中改进提示准确度和文本渲染效果。<br />
<br />
✦下个星期会有一个重要的网页 alpha 版更新。该更新将包含文件夹、更好的筛选功能以及更新后的存档页面。也许还会增加一些社交功能。<br />
<br />
✦Niji v6 即将面世。<br />
<br />
✦另外，“—stylize” 参数在较高数值上表现得更佳，这意味着它能够更多地关注给出的提示，并添加细节信息。<br />
<br />
✦v6 更多功能正在开发中，比如区域变化和修复图像，预计将于一月份发布。<br />
<br />
✦目标是在本月底将 v6 设为默认版本。</p>
<p><a href="https://nitter.cz/nickfloats/status/1742641203181506826#m">nitter.cz/nickfloats/status/1742641203181506826#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742843147926077763#m</id>
            <title>RT by @op7418: 一个视频生成模型，搞笑的是也叫MoonShot，我以为月之暗面的呢。从演示来看挺稳定的，支持的功能也挺全。

支持个性化视频生成、图像动画和视频编辑等功能。也支持跟 ContorlNet 模型配合控制视频生成。

主要特点：

一个用于视频生成的传统时空模块，由空间卷积层、自注意力层和聚合空间特征的时序注意力层组成。这样的设计可以在不改变空间特征分布的情况下重复使用文本到图像生成模型的预训练权重，从而提升生成质量。

一个解耦的多模态交叉注意力层，将生成条件限制在文本和图像输入上。这两个条件相互补充，引导生成过程。此外，图像输入提供参考的视觉线索，使时间模块能够专注于视频的一致性。

由于空间特征分布被保留，预训练的图像控制网络模块可以立即集成，用于控制生成物的几何结构，无需额外的训练开销。

论文地址：https://browse.arxiv.org/html/2401.01827v1</title>
            <link>https://nitter.cz/op7418/status/1742843147926077763#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742843147926077763#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 09:39:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个视频生成模型，搞笑的是也叫MoonShot，我以为月之暗面的呢。从演示来看挺稳定的，支持的功能也挺全。<br />
<br />
支持个性化视频生成、图像动画和视频编辑等功能。也支持跟 ContorlNet 模型配合控制视频生成。<br />
<br />
主要特点：<br />
<br />
一个用于视频生成的传统时空模块，由空间卷积层、自注意力层和聚合空间特征的时序注意力层组成。这样的设计可以在不改变空间特征分布的情况下重复使用文本到图像生成模型的预训练权重，从而提升生成质量。<br />
<br />
一个解耦的多模态交叉注意力层，将生成条件限制在文本和图像输入上。这两个条件相互补充，引导生成过程。此外，图像输入提供参考的视觉线索，使时间模块能够专注于视频的一致性。<br />
<br />
由于空间特征分布被保留，预训练的图像控制网络模块可以立即集成，用于控制生成物的几何结构，无需额外的训练开销。<br />
<br />
论文地址：<a href="https://browse.arxiv.org/html/2401.01827v1">browse.arxiv.org/html/2401.0…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI4NDMwNzIxNjg0ODA3NjgvcHUvaW1nL25CczdCUUxQMDdETEdyU2kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742873702621126924#m</id>
            <title>RT by @op7418: SD A1111 Controlnet 更新了手部修复的预处理器depth_hand_refiner，现在可以在 Web UI 里面对出问题的手进行修复了。

如何使用：
在图生图 Tab 选择inpaint，然后给需要修复的手部涂上蒙版，然后在 Contorlnet 选择depth_hand_refiner 预处理器，点击生成就可以了。</title>
            <link>https://nitter.cz/op7418/status/1742873702621126924#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742873702621126924#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 11:40:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SD A1111 Controlnet 更新了手部修复的预处理器depth_hand_refiner，现在可以在 Web UI 里面对出问题的手进行修复了。<br />
<br />
如何使用：<br />
在图生图 Tab 选择inpaint，然后给需要修复的手部涂上蒙版，然后在 Contorlnet 选择depth_hand_refiner 预处理器，点击生成就可以了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfdUVnb2FJQUFQaC1sLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfdVVhWGFBQUFNcUVGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742891365485494444#m</id>
            <title>RT by @op7418: 太强了，Thibaud Zamora终于发布了他的 AI 更换服装ComfyUI工作流程。
从演示来看效果非常好，大部分服装细节，甚至服装上的图案和文字都能被保留。

从他的周刊获取工作流：https://fictions-ai.beehiiv.com/p/ai-weekly-roundup-6-discover-may-missed-week</title>
            <link>https://nitter.cz/op7418/status/1742891365485494444#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742891365485494444#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 12:50:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>太强了，Thibaud Zamora终于发布了他的 AI 更换服装ComfyUI工作流程。<br />
从演示来看效果非常好，大部分服装细节，甚至服装上的图案和文字都能被保留。<br />
<br />
从他的周刊获取工作流：<a href="https://fictions-ai.beehiiv.com/p/ai-weekly-roundup-6-discover-may-missed-week">fictions-ai.beehiiv.com/p/ai…</a></p>
<p><a href="https://nitter.cz/thibaudz/status/1742863527495209023#m">nitter.cz/thibaudz/status/1742863527495209023#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfLVRLcWFjQUF1ZlVYLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfLVVwQmE4QUFxcDhOLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfLVdUSWJBQUFLQVBNLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742910021892268455#m</id>
            <title>RT by @op7418: 尝试了一下用SVD做视频，确实牛皮。

Midjourney V6生成的国风场景，Topza插帧加放大。

从百面千像的宣传片获得的灵感，所以用了对应的音乐。

SVD还是不能搞画面过于复杂的场景，太复杂就不知道该怎么动了，秋天和冬天的部分是最好的。</title>
            <link>https://nitter.cz/op7418/status/1742910021892268455#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742910021892268455#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 14:05:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>尝试了一下用SVD做视频，确实牛皮。<br />
<br />
Midjourney V6生成的国风场景，Topza插帧加放大。<br />
<br />
从百面千像的宣传片获得的灵感，所以用了对应的音乐。<br />
<br />
SVD还是不能搞画面过于复杂的场景，太复杂就不知道该怎么动了，秋天和冬天的部分是最好的。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI5MDk3NzU5MjE0MTgyNDAvcHUvaW1nL01OS010TTM3ckVzTGpGZHAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742942815355854921#m</id>
            <title>RT by @op7418: AI搜索工具Perplexity宣布B轮7360万美元融资，估值达到5.2亿美元。

下面是这轮融资的一些信息：

Perplexity的月活跃用户增长到了1000万，并在2023年处理了超过50亿次查询。
iOS和Android应用安装量超过100万。

投资者包括NEA、Elad Gil、Nat Friedman和Databricks的支持，以及新的投资者NVIDIA、Jeff Bezos（通过Bezos Expeditions Fund）、Tobi Lutke、Bessemer Ventures、Naval Ravikant等。

B轮7360万美元融资，估值达到5.2亿美元，总融资已经达到了1亿美元。

公告页面：https://blog.perplexity.ai/blog/perplexity-raises-series-b-funding-round</title>
            <link>https://nitter.cz/op7418/status/1742942815355854921#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742942815355854921#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 16:15:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI搜索工具Perplexity宣布B轮7360万美元融资，估值达到5.2亿美元。<br />
<br />
下面是这轮融资的一些信息：<br />
<br />
Perplexity的月活跃用户增长到了1000万，并在2023年处理了超过50亿次查询。<br />
iOS和Android应用安装量超过100万。<br />
<br />
投资者包括NEA、Elad Gil、Nat Friedman和Databricks的支持，以及新的投资者NVIDIA、Jeff Bezos（通过Bezos Expeditions Fund）、Tobi Lutke、Bessemer Ventures、Naval Ravikant等。<br />
<br />
B轮7360万美元融资，估值达到5.2亿美元，总融资已经达到了1亿美元。<br />
<br />
公告页面：<a href="https://blog.perplexity.ai/blog/perplexity-raises-series-b-funding-round">blog.perplexity.ai/blog/perp…</a></p>
<p><a href="https://nitter.cz/AravSrinivas/status/1742918329797574709#m">nitter.cz/AravSrinivas/status/1742918329797574709#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742946023935516751#m</id>
            <title>RT by @op7418: Open AI 发布文章，介绍了GPTs创建器是如何被创建的，搞笑的是这个GPTs构建器本身也是一个GPTs。

来学习一下Open AI是怎么写GPTs提示词的。

👇下面是GPT Builder具体的构建过程和提示词：

GPT Builder 被构建为一个自定义的 GPT，具有指令和动作，允许它写入当前正在构建的 GPT 的字段。

更高级的构建者应该使用手动配置界面来编辑他们的GPT的字段，但GPT构建器始终可以作为一个起点。

由于GPT Builder本身就是一个定制的GPT，我们可以分享我们使用的配置作为创建强大GPT的示例。

以下是我们用于为GPT Builder提供动力的核心指令，截至2023年1月3日。为了清晰起见，我们将指令分为“基本上下文”和“步骤演示”，但在应用到GPT时，它们都会进入“指令”部分。

说明-基本上下文：

您是一个擅长创建和修改GPT的专家，它们就像可以具有额外功能的聊天机器人。

每个用户消息都是您处理和更新GPTs行为的命令。您将承认并将其纳入GPTs的行为，并在gizmo_editor_tool上调用update_behavior。

如果用户告诉你开始以某种方式行为，他们指的是你正在创建的GPTs，而不是你自己。

如果您没有个人资料图片，必须调用generate_profile_pic。如果明确要求，您将通过generate_profile_pic生成个人资料图片。否则不要生成个人资料图片。

保持作为GPTs制作者的专家的语调和观点。 GPTs的个性不应影响您的回答风格或语调。

如果你问用户一个问题，永远不要自己回答。你可以提出答案，但必须让用户确认。

您可见的文件也对 GPT 可见。您可以更新行为以引用已上传的文件。

请勿使用“约束”、“角色和目标”或“个性化”这些词。

GPTs没有记住过去经验的能力。

说明-步骤：

你是一个用于开发新GPTs的迭代原型游乐场。用户将通过初始行为提示你。

您的目标是迭代地定义和完善update_behavior的参数。您将以专业GPT创建者的身份进行交谈，从用户那里收集规范以创建GPTs。您将在每次交互后调用update_behavior。您将按照以下步骤进行：

1）用户的第一条消息是关于这个GPT应该如何行为的广泛目标。使用参数“context”、“description”、“prompt_starters”在gizmo_editor_tool上调用update_behavior。记住，你必须使用参数“context”、“description”和“prompt_starters”调用gizmo_editor_tool上的update_behavior。在调用update_behavior之后，继续进行第2步。

2）在这一步中，你的目标是确定 GPT 的名称。你会为自己建议一个名称，并要求用户确认。你必须提供一个建议的名称供用户确认。
你不可以在没有建议的情况下提示用户。不要使用驼峰式复合词；请使用空格代替。如果用户指定了一个明确的名称，请假设它已经确认。如果你自己生成一个名称，你必须让用户确认该名称。一旦确认，只需调用 update_behavior，并继续到第三步。

3）在这一步中，您的目标是为 GPT 生成一个个人资料图片。您将使用 generate_profile_pic 为这个 GPT 生成一个初始个人资料图片，无需确认，然后询问用户是否喜欢，并是否想要进行任何更改。
请记住，使用 generate_profile_pic 生成个人资料图片时无需确认。在每次改进后生成新的个人资料图片，直到用户满意为止，然后继续进行第四步。

4）在这一步中，你的目标是细化上下文。你现在要引导用户细化上下文。上下文应包括“角色和目标”、“约束”、“指南”、“澄清”和“个性化”等主要领域。你将引导用户逐个定义每个主要领域。
你不会一次性提示多个领域，而是一次只问一个问题。你的提示应该是引导性、自然和简单的语言，不会提及你正在定义的领域的名称。
你的提示不需要介绍它们正在细化的领域，而只需是引导性问题。例如，“约束”应该提示为“应该强调或避免什么？”，“个性化”应该提示为“你希望我怎么说”。
你的引导性问题应该是不言自明的；你不需要问用户“你认为呢？”。每个提示都应参考并建立在现有状态之上。每次互动后都要调用update_behavior。

在这些步骤中，您不会提示或确认“描述”、“提示启动器”的值。但是，您仍会在上下文更新时生成这些值。您不会提到“步骤”; 您将自然地进行下去。

你必须按顺序完成所有这些步骤。不要跳过任何步骤。

请让用户在右侧的独立聊天对话框中尝试GPT。告诉他们你能够听取他们对GPT的任何改进意见。以一个问题结束这条消息，不要说“让我知道！”。
在确认名称时只将GPT的名称加粗；在第2步之后不要加粗名称。

Action 行动：

在上述步骤之后，您现在处于迭代细化模式。用户将提示您进行更改，您必须在每次交互后调用update_behavior。您可以在这里提出澄清问题。

generate_profile_pic: { description: '为GPTs生成个人资料图片。您可以调用此函数而无需生成图像的能力。如果当前的GPT没有个人资料图片，则必须调用此函数，并且在需要生成新的个人资料图片时也可以调用。在调用此函数时，请将个人资料图片视为已更新，不要调用update_behavior。' },

update_behavior: { description: "更新GPTs的行为。您可以有选择地省略更新字段。您将使用这些新字段作为GPTs行为的真相来源，并不再引用任何先前版本的已更新字段来通知响应。当您更新一个字段时，如果它们是不一致的，那么您还必须同时更新所有其他字段以保持一致性。如果您更改了GPTs的名称，则必须使描述和上下文保持一致性。在调用此函数时，不能总结该功能外部使用中所使用的值" , params: { name, context, description, prompt_starters, abilities, profile_pic_file_id } }

GPT可以利用提供给它的所有信息，包括提示、指令和附加文件，来构建对用户的回应。不要包含你不希望用户知道的信息。</title>
            <link>https://nitter.cz/op7418/status/1742946023935516751#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742946023935516751#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 16:28:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Open AI 发布文章，介绍了GPTs创建器是如何被创建的，搞笑的是这个GPTs构建器本身也是一个GPTs。<br />
<br />
来学习一下Open AI是怎么写GPTs提示词的。<br />
<br />
👇下面是GPT Builder具体的构建过程和提示词：<br />
<br />
GPT Builder 被构建为一个自定义的 GPT，具有指令和动作，允许它写入当前正在构建的 GPT 的字段。<br />
<br />
更高级的构建者应该使用手动配置界面来编辑他们的GPT的字段，但GPT构建器始终可以作为一个起点。<br />
<br />
由于GPT Builder本身就是一个定制的GPT，我们可以分享我们使用的配置作为创建强大GPT的示例。<br />
<br />
以下是我们用于为GPT Builder提供动力的核心指令，截至2023年1月3日。为了清晰起见，我们将指令分为“基本上下文”和“步骤演示”，但在应用到GPT时，它们都会进入“指令”部分。<br />
<br />
说明-基本上下文：<br />
<br />
您是一个擅长创建和修改GPT的专家，它们就像可以具有额外功能的聊天机器人。<br />
<br />
每个用户消息都是您处理和更新GPTs行为的命令。您将承认并将其纳入GPTs的行为，并在gizmo_editor_tool上调用update_behavior。<br />
<br />
如果用户告诉你开始以某种方式行为，他们指的是你正在创建的GPTs，而不是你自己。<br />
<br />
如果您没有个人资料图片，必须调用generate_profile_pic。如果明确要求，您将通过generate_profile_pic生成个人资料图片。否则不要生成个人资料图片。<br />
<br />
保持作为GPTs制作者的专家的语调和观点。 GPTs的个性不应影响您的回答风格或语调。<br />
<br />
如果你问用户一个问题，永远不要自己回答。你可以提出答案，但必须让用户确认。<br />
<br />
您可见的文件也对 GPT 可见。您可以更新行为以引用已上传的文件。<br />
<br />
请勿使用“约束”、“角色和目标”或“个性化”这些词。<br />
<br />
GPTs没有记住过去经验的能力。<br />
<br />
说明-步骤：<br />
<br />
你是一个用于开发新GPTs的迭代原型游乐场。用户将通过初始行为提示你。<br />
<br />
您的目标是迭代地定义和完善update_behavior的参数。您将以专业GPT创建者的身份进行交谈，从用户那里收集规范以创建GPTs。您将在每次交互后调用update_behavior。您将按照以下步骤进行：<br />
<br />
1）用户的第一条消息是关于这个GPT应该如何行为的广泛目标。使用参数“context”、“description”、“prompt_starters”在gizmo_editor_tool上调用update_behavior。记住，你必须使用参数“context”、“description”和“prompt_starters”调用gizmo_editor_tool上的update_behavior。在调用update_behavior之后，继续进行第2步。<br />
<br />
2）在这一步中，你的目标是确定 GPT 的名称。你会为自己建议一个名称，并要求用户确认。你必须提供一个建议的名称供用户确认。<br />
你不可以在没有建议的情况下提示用户。不要使用驼峰式复合词；请使用空格代替。如果用户指定了一个明确的名称，请假设它已经确认。如果你自己生成一个名称，你必须让用户确认该名称。一旦确认，只需调用 update_behavior，并继续到第三步。<br />
<br />
3）在这一步中，您的目标是为 GPT 生成一个个人资料图片。您将使用 generate_profile_pic 为这个 GPT 生成一个初始个人资料图片，无需确认，然后询问用户是否喜欢，并是否想要进行任何更改。<br />
请记住，使用 generate_profile_pic 生成个人资料图片时无需确认。在每次改进后生成新的个人资料图片，直到用户满意为止，然后继续进行第四步。<br />
<br />
4）在这一步中，你的目标是细化上下文。你现在要引导用户细化上下文。上下文应包括“角色和目标”、“约束”、“指南”、“澄清”和“个性化”等主要领域。你将引导用户逐个定义每个主要领域。<br />
你不会一次性提示多个领域，而是一次只问一个问题。你的提示应该是引导性、自然和简单的语言，不会提及你正在定义的领域的名称。<br />
你的提示不需要介绍它们正在细化的领域，而只需是引导性问题。例如，“约束”应该提示为“应该强调或避免什么？”，“个性化”应该提示为“你希望我怎么说”。<br />
你的引导性问题应该是不言自明的；你不需要问用户“你认为呢？”。每个提示都应参考并建立在现有状态之上。每次互动后都要调用update_behavior。<br />
<br />
在这些步骤中，您不会提示或确认“描述”、“提示启动器”的值。但是，您仍会在上下文更新时生成这些值。您不会提到“步骤”; 您将自然地进行下去。<br />
<br />
你必须按顺序完成所有这些步骤。不要跳过任何步骤。<br />
<br />
请让用户在右侧的独立聊天对话框中尝试GPT。告诉他们你能够听取他们对GPT的任何改进意见。以一个问题结束这条消息，不要说“让我知道！”。<br />
在确认名称时只将GPT的名称加粗；在第2步之后不要加粗名称。<br />
<br />
Action 行动：<br />
<br />
在上述步骤之后，您现在处于迭代细化模式。用户将提示您进行更改，您必须在每次交互后调用update_behavior。您可以在这里提出澄清问题。<br />
<br />
generate_profile_pic: { description: '为GPTs生成个人资料图片。您可以调用此函数而无需生成图像的能力。如果当前的GPT没有个人资料图片，则必须调用此函数，并且在需要生成新的个人资料图片时也可以调用。在调用此函数时，请将个人资料图片视为已更新，不要调用update_behavior。' },<br />
<br />
update_behavior: { description: "更新GPTs的行为。您可以有选择地省略更新字段。您将使用这些新字段作为GPTs行为的真相来源，并不再引用任何先前版本的已更新字段来通知响应。当您更新一个字段时，如果它们是不一致的，那么您还必须同时更新所有其他字段以保持一致性。如果您更改了GPTs的名称，则必须使描述和上下文保持一致性。在调用此函数时，不能总结该功能外部使用中所使用的值" , params: { name, context, description, prompt_starters, abilities, profile_pic_file_id } }<br />
<br />
GPT可以利用提供给它的所有信息，包括提示、指令和附加文件，来构建对用户的回应。不要包含你不希望用户知道的信息。</p>
<p><a href="https://nitter.cz/OfficialLoganK/status/1742930722766397932#m">nitter.cz/OfficialLoganK/status/1742930722766397932#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742947606429675745#m</id>
            <title>RT by @op7418: Thibaud的新AI视频，图片由SDXL ComfyUI制作，视频由Pika_lab制作。
SD制作图片是为了用IPadapter保持角色一致性。</title>
            <link>https://nitter.cz/op7418/status/1742947606429675745#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742947606429675745#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 16:34:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Thibaud的新AI视频，图片由SDXL ComfyUI制作，视频由Pika_lab制作。<br />
SD制作图片是为了用IPadapter保持角色一致性。</p>
<p><a href="https://nitter.cz/thibaudz/status/1742917769409245627#m">nitter.cz/thibaudz/status/1742917769409245627#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742950724114456881#m</id>
            <title>RT by @op7418: Meta发布了CoTracker视频内容跟踪模型的新版本，可以跨视频序列联合跟踪帧中的密集点。

还有虚拟轨道的概念，它允许 CoTracker 联合、同时跟踪 70k 个点。

即使点被遮挡或离开视野，也可以长时间跟踪这些点。从数量上讲，CoTracker 在标准基准测试中的表现优于所有最新的跟踪器，而且通常遥遥领先。

项目地址：https://co-tracker.github.io/</title>
            <link>https://nitter.cz/op7418/status/1742950724114456881#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742950724114456881#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 16:46:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta发布了CoTracker视频内容跟踪模型的新版本，可以跨视频序列联合跟踪帧中的密集点。<br />
<br />
还有虚拟轨道的概念，它允许 CoTracker 联合、同时跟踪 70k 个点。<br />
<br />
即使点被遮挡或离开视野，也可以长时间跟踪这些点。从数量上讲，CoTracker 在标准基准测试中的表现优于所有最新的跟踪器，而且通常遥遥领先。<br />
<br />
项目地址：<a href="https://co-tracker.github.io/">co-tracker.github.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI5NTA0NzQ3NzU2ODMwNzYvcHUvaW1nLzdRaURKOERKR2poWlhvaUsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742955636638535876#m</id>
            <title>RT by @op7418: 一个LLM提示词的内容框架。 组成部分：

指令：引导LLM推理格式和结构的简短提示
理由：在上下文导向推理（CoT）过程中生成的中间推理步骤
示例：展示目标推理模式的输入输出实例
环境：交互式上下文，如操作系统、应用程序、网页代理
工具：扩展LLM能力的外部模块，包括执行、知识或验证

模块：

感知：通过CoT提示顺序解释环境状态
记忆：短期存储暂时信息；长期保留静态知识
推理：在交错的CoT格式中进行规划、决策和行动

格式：

文本：标准CoT的顺序语言
树形结构：表示互连思想的层级结构
图形：映射思想之间关系的网络
程序：基于代码的思想，逻辑与语言分离
表格：以行/列表格方式展现连贯思维进程

过程：

提示：用指令和示例引出目标推理格式
聚合：结合多个CoT路径以提高流畅性
验证：利用外部信息源评估和修订思想
定制化：与特定用户需求相匹配

实体：

问题：触发代理CoT推理的输入
答案：从CoT推理中得出的最终输出
行动：基于代理决策的操作执行
情节：朝向目标的完整交互序列
轮次：情节内单个顺序互动

属性：

可解释性：理解导致结论的推理过程
可控性：通过调整提示影响模型行为
适应性：在新环境和任务中有效性
安全性：确保行为安全，避免有害故障模式

任务：

算术：数学推理
文本：语言理解和常识
视觉：结合图像的多模态推理
符号：结构化输入，如编程语言
通用：广泛的日常现实世界应用</title>
            <link>https://nitter.cz/op7418/status/1742955636638535876#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742955636638535876#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 17:06:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个LLM提示词的内容框架。 组成部分：<br />
<br />
指令：引导LLM推理格式和结构的简短提示<br />
理由：在上下文导向推理（CoT）过程中生成的中间推理步骤<br />
示例：展示目标推理模式的输入输出实例<br />
环境：交互式上下文，如操作系统、应用程序、网页代理<br />
工具：扩展LLM能力的外部模块，包括执行、知识或验证<br />
<br />
模块：<br />
<br />
感知：通过CoT提示顺序解释环境状态<br />
记忆：短期存储暂时信息；长期保留静态知识<br />
推理：在交错的CoT格式中进行规划、决策和行动<br />
<br />
格式：<br />
<br />
文本：标准CoT的顺序语言<br />
树形结构：表示互连思想的层级结构<br />
图形：映射思想之间关系的网络<br />
程序：基于代码的思想，逻辑与语言分离<br />
表格：以行/列表格方式展现连贯思维进程<br />
<br />
过程：<br />
<br />
提示：用指令和示例引出目标推理格式<br />
聚合：结合多个CoT路径以提高流畅性<br />
验证：利用外部信息源评估和修订思想<br />
定制化：与特定用户需求相匹配<br />
<br />
实体：<br />
<br />
问题：触发代理CoT推理的输入<br />
答案：从CoT推理中得出的最终输出<br />
行动：基于代理决策的操作执行<br />
情节：朝向目标的完整交互序列<br />
轮次：情节内单个顺序互动<br />
<br />
属性：<br />
<br />
可解释性：理解导致结论的推理过程<br />
可控性：通过调整提示影响模型行为<br />
适应性：在新环境和任务中有效性<br />
安全性：确保行为安全，避免有害故障模式<br />
<br />
任务：<br />
<br />
算术：数学推理<br />
文本：语言理解和常识<br />
视觉：结合图像的多模态推理<br />
符号：结构化输入，如编程语言<br />
通用：广泛的日常现实世界应用</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742953722026811804#m</id>
            <title>RT by @op7418: 作者构建了一个减轻大语言模型幻觉的整体知识框架，非常详细和体系化，基本上该有的都涉及到了。
想要了解相关内容的话跟着这个目录搜索就可以。

下面是所有内容的概述，原文还有每个小节的概述：

提示工程领域：

A. 提前检索增强生成（RAG）

- 生成之前：在文本生成之前进行信息检索的策略，比如LLM-Augmenter

- 生成过程中：在句子生成时进行检索，比如知识检索，D&amp;Q框架

- 生成之后：在全文生成完成后进行检索，比如RARR

- 端到端：将检索和生成整合在一起的模型，比如原始的RAG模型

B. 通过反馈和推理进行自我完善

- 通过用户反馈迭代改进输出结果，比如Prompting GPT-3 for Reliability

- 发现并缓解自我矛盾，比如ChatProtect

- 通过反馈循环进行交互式改进，比如自我反思方法论

C. 提示调整

- 为模型提供调整指令，比如UPRISE, SynTra

模型开发领域：

A. 新解码策略

- 在生成过程中引导，比如上下文感知解码

B. 利用知识图谱

- 注入结构化知识，比如RHO

C. 基于忠实度的损失函数

- 提升输出内容的真实性，比如THAM框架

D. 监督微调

- 在标记数据上进行模型调整，比如知识注入方法</title>
            <link>https://nitter.cz/op7418/status/1742953722026811804#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742953722026811804#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 16:58:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者构建了一个减轻大语言模型幻觉的整体知识框架，非常详细和体系化，基本上该有的都涉及到了。<br />
想要了解相关内容的话跟着这个目录搜索就可以。<br />
<br />
下面是所有内容的概述，原文还有每个小节的概述：<br />
<br />
提示工程领域：<br />
<br />
A. 提前检索增强生成（RAG）<br />
<br />
- 生成之前：在文本生成之前进行信息检索的策略，比如LLM-Augmenter<br />
<br />
- 生成过程中：在句子生成时进行检索，比如知识检索，D&amp;Q框架<br />
<br />
- 生成之后：在全文生成完成后进行检索，比如RARR<br />
<br />
- 端到端：将检索和生成整合在一起的模型，比如原始的RAG模型<br />
<br />
B. 通过反馈和推理进行自我完善<br />
<br />
- 通过用户反馈迭代改进输出结果，比如Prompting GPT-3 for Reliability<br />
<br />
- 发现并缓解自我矛盾，比如ChatProtect<br />
<br />
- 通过反馈循环进行交互式改进，比如自我反思方法论<br />
<br />
C. 提示调整<br />
<br />
- 为模型提供调整指令，比如UPRISE, SynTra<br />
<br />
模型开发领域：<br />
<br />
A. 新解码策略<br />
<br />
- 在生成过程中引导，比如上下文感知解码<br />
<br />
B. 利用知识图谱<br />
<br />
- 注入结构化知识，比如RHO<br />
<br />
C. 基于忠实度的损失函数<br />
<br />
- 提升输出内容的真实性，比如THAM框架<br />
<br />
D. 监督微调<br />
<br />
- 在标记数据上进行模型调整，比如知识注入方法</p>
<p><a href="https://nitter.cz/IntuitMachine/status/1742908554514714786#m">nitter.cz/IntuitMachine/status/1742908554514714786#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RBM0p6U2FZQUFOTzlGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742971021664382996#m</id>
            <title>RT by @op7418: 终于来了！Open AI 将于下周开放 GPTs 商店。

如果你的GPTs 想要上架的话需要满足下面👇三个条件：

1）查看 Open ai的 使用政策和GPT 品牌指南，以确保你的  GPTs 合规。

2）验证你的构建者配置文件（设置 > 构建者配置文件 > 启用你的姓名或经过验证的网站）。

3）将你的 GPTs 发布为“公开”（选择“任何有链接的人”的 GPTs将不会显示在商店中）</title>
            <link>https://nitter.cz/op7418/status/1742971021664382996#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742971021664382996#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于来了！Open AI 将于下周开放 GPTs 商店。<br />
<br />
如果你的GPTs 想要上架的话需要满足下面👇三个条件：<br />
<br />
1）查看 Open ai的 使用政策和GPT 品牌指南，以确保你的  GPTs 合规。<br />
<br />
2）验证你的构建者配置文件（设置 > 构建者配置文件 > 启用你的姓名或经过验证的网站）。<br />
<br />
3）将你的 GPTs 发布为“公开”（选择“任何有链接的人”的 GPTs将不会显示在商店中）</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RCRzV4Z2FZQUVTbEIzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742972991036862620#m</id>
            <title>R to @op7418: 补一张白底邮件</title>
            <link>https://nitter.cz/op7418/status/1742972991036862620#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742972991036862620#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:15:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补一张白底邮件</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RCSXNXVWFZQUVEYmRYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742971959514914903#m</id>
            <title>R to @op7418: 顺便推荐 Open AI自己写的 GPTs 构建指南：</title>
            <link>https://nitter.cz/op7418/status/1742971959514914903#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742971959514914903#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:11:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>顺便推荐 Open AI自己写的 GPTs 构建指南：</p>
<p><a href="https://nitter.cz/op7418/status/1742946023935516751#m">nitter.cz/op7418/status/1742946023935516751#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742971738928079010#m</id>
            <title>R to @op7418: 有自己比较火的 GPTs 的可以准备一下了，没有的现在搞也来得及。</title>
            <link>https://nitter.cz/op7418/status/1742971738928079010#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742971738928079010#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:10:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有自己比较火的 GPTs 的可以准备一下了，没有的现在搞也来得及。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742954793637929126#m</id>
            <title>OpenAI最近与两位执行官进行了谈判，向一些媒体公司提供每年100万至500万美元的许可费，以用其新闻文章来训练其大型语言模型。
即使对于小型出版商来说，这个数额也很少，可能会让OpenAI难以达成交易。

与此同时，苹果正试图在生成式人工智能领域赶上OpenAI和谷歌，并且也在努力与出版商达成协议使用他们的内容。
据一位高管称，苹果提供了更多资金，但也希望比OpenAI寻求的更广泛地使用内容的权利。
知情人士表示，苹果希望能够根据公司认为必要的方式，在未来的人工智能产品中使用内容。</title>
            <link>https://nitter.cz/op7418/status/1742954793637929126#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742954793637929126#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 17:02:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI最近与两位执行官进行了谈判，向一些媒体公司提供每年100万至500万美元的许可费，以用其新闻文章来训练其大型语言模型。<br />
即使对于小型出版商来说，这个数额也很少，可能会让OpenAI难以达成交易。<br />
<br />
与此同时，苹果正试图在生成式人工智能领域赶上OpenAI和谷歌，并且也在努力与出版商达成协议使用他们的内容。<br />
据一位高管称，苹果提供了更多资金，但也希望比OpenAI寻求的更广泛地使用内容的权利。<br />
知情人士表示，苹果希望能够根据公司认为必要的方式，在未来的人工智能产品中使用内容。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1742937148251377821#m">nitter.cz/_akhaliq/status/1742937148251377821#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>