<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1730131907059560918#m</id>
            <title>阿里发布的这个只需要单张图片和Openpose 动作就可以让图片动起来并保持稳定性的项目很不错啊。
人物动作一直是视频生成一个比较麻烦的问题，通过动作库曲线救国也不错。而且在运动过程中图片的特征也很稳定。
项目地址：https://humanaigc.github.io/animate-anyone/</title>
            <link>https://nitter.cz/op7418/status/1730131907059560918#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1730131907059560918#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 07:49:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里发布的这个只需要单张图片和Openpose 动作就可以让图片动起来并保持稳定性的项目很不错啊。<br />
人物动作一直是视频生成一个比较麻烦的问题，通过动作库曲线救国也不错。而且在运动过程中图片的特征也很稳定。<br />
项目地址：<a href="https://humanaigc.github.io/animate-anyone/">humanaigc.github.io/animate-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzAxMzE4NTk2NjgxNjQ2MDgvcHUvaW1nL0hCeWZRMERxbDliQ1M0RTcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1730081681657532781#m</id>
            <title>哈哈 Sam 也觉得Chatgpt发布这一年挺不可思议的。</title>
            <link>https://nitter.cz/op7418/status/1730081681657532781#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1730081681657532781#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:29:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈 Sam 也觉得Chatgpt发布这一年挺不可思议的。</p>
<p><a href="https://nitter.cz/sama/status/1730076492162548208#m">nitter.cz/sama/status/1730076492162548208#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866338519351662#m</id>
            <title>RT by @op7418: 来辣！最近很多 SD 的内容比如 LCM Animatediff 等项目，首先都是在 ComfyUI 支持的，但是 ComfyUI 又比较复杂，环境配置和安装也很复杂。

所以就找了一个ComfyUI 的笔记来教一下大家免费部署 ComfyUI，同时这节课也会教一些 ComfyUI 的基本操作，带大家生成一张图片和一段 Animatediff 视频🧵：</title>
            <link>https://nitter.cz/op7418/status/1729866338519351662#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866338519351662#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来辣！最近很多 SD 的内容比如 LCM Animatediff 等项目，首先都是在 ComfyUI 支持的，但是 ComfyUI 又比较复杂，环境配置和安装也很复杂。<br />
<br />
所以就找了一个ComfyUI 的笔记来教一下大家免费部署 ComfyUI，同时这节课也会教一些 ComfyUI 的基本操作，带大家生成一张图片和一段 Animatediff 视频🧵：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNFE0YWFvQUU5SlFkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1730047945830273335#m</id>
            <title>Sam还帮Adam澄清了一下关于Poe发布相关功能导致他是整个事件的关键人物的问题。说Adam再遇到有利益冲突的事情时一直在主动回避，所以没有做传言的那些事情。不过Sam确实和董事会的一些人有矛盾。</title>
            <link>https://nitter.cz/op7418/status/1730047945830273335#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1730047945830273335#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 02:15:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sam还帮Adam澄清了一下关于Poe发布相关功能导致他是整个事件的关键人物的问题。说Adam再遇到有利益冲突的事情时一直在主动回避，所以没有做传言的那些事情。不过Sam确实和董事会的一些人有矛盾。</p>
<p><a href="https://nitter.cz/sama/status/1730032994474475554#m">nitter.cz/sama/status/1730032994474475554#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729873461185855965#m</id>
            <title>RT by @op7418: 一个可以增强AI生成视频可控性的项目，支持Animatediff。将会开源代码，这下Animatediff的生命力又旺盛了。
我理解就是视频版本的ContorlNet，解决了之前视频生成使用ContorlNet时每一帧都需要介入的问题，先可以自动选择关键帧介入降低资源消耗。

可以在这里看详细的论文：https://guoyww.github.io/projects/SparseCtrl/</title>
            <link>https://nitter.cz/op7418/status/1729873461185855965#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729873461185855965#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:42:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个可以增强AI生成视频可控性的项目，支持Animatediff。将会开源代码，这下Animatediff的生命力又旺盛了。<br />
我理解就是视频版本的ContorlNet，解决了之前视频生成使用ContorlNet时每一帧都需要介入的问题，先可以自动选择关键帧介入降低资源消耗。<br />
<br />
可以在这里看详细的论文：<a href="https://guoyww.github.io/projects/SparseCtrl/">guoyww.github.io/projects/Sp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjk4NzI4NTk5MDcxOTg5NzcvcHUvaW1nLzU5Sk9Sc2dqbHQwLXU0cnouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1730035111872327899#m</id>
            <title>Sam Altman 重新担任首席执行官，Mira Murati 担任首席技术官，Greg Brockman 担任总裁。新董事会在起作用了。</title>
            <link>https://nitter.cz/op7418/status/1730035111872327899#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1730035111872327899#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 01:24:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sam Altman 重新担任首席执行官，Mira Murati 担任首席技术官，Greg Brockman 担任总裁。新董事会在起作用了。</p>
<p><a href="https://nitter.cz/OpenAI/status/1730030975931846939#m">nitter.cz/OpenAI/status/1730030975931846939#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729844343329218875#m</id>
            <title>RT by @op7418: 海外独角兽对Pika创始团队做了比较深入的访谈，和福布斯那种不同的是他们问的问题比较专业。
同时Pika创始团队的两个人也透露了比较多的一些东西，这可能是第一次视频生成领域的前沿团队透露这么多东西。
所以一些认知和方向性的东西对想要做这个事情和投资的人来说还是比较重要的。
我基于自己最近关于AI视频的一些问题和比较关注的信息筛选和整理了一下这个访谈中的关键内容。各位也可以一起看看讨论一下：

视频生成和图像生成的区别是什么？
⚫视频的每一帧都是一张图片，但比图片困难得多。每一帧的生成质量要高，相邻帧之间还要有关联性。当视频很长时，确保每一帧都协调一致是个相当复杂的问题。在训练时，处理视频数据时要处理多张图片，模型需要适应这种情况。
⚫控制视频生成更难，因为模型需要生成每一帧发生的事情，而用户不会希望为每一帧都提供详细的描述。
⚫互联网上视频生成的训练数据集相对于图像更少，也给获取高质量数据增加了难度。

现在视频生成有哪些关键点需要突破？
⚫首先是时长，跟时长很相关的是动作的意义。所有模型都很容易做一个 extension 的功能，把视频时长延长很多，但它并没有真的延长，因为它生成的动作没有意义。
⚫视频的清晰度也需要进一步提高。尽管清晰度方面已经有突破，但还没有提高到电影级的水平，现在生成的视频一般是 720p 分辨率，视频的流畅性也不够理想，特别是一些细节的 texture。
⚫还需要考虑 general artifact 的问题，比如说一个人有两个头，就是这种明显不符合常理的问题，也是需要避免的。

视频生成的技术路线是否收敛？
现在还没有收敛，大家都在往各种方向尝试，每个人都认为自己的模型是最好的，可能有人认为 autoregressive 最好，有人认为 Masked Model 最好。Pika 基于 Diffusion Model，但是开发了很多新东西，是一种新的模型。

AI视频什么时候会迎来GPT时刻？
目前视频生成处于类似 GPT-2 的时期，很可能在未来一年内有一个显著的提升。

在视频生成领域什么样的数据算高质量的数据？
⚫首先是像素，就是我们说的画质好不好
⚫然后看审美和艺术构图
⚫ 第三方面是要有动作，并且这些动作是有意义的
⚫ 视频的长度也很关键，如果模型都在 1 秒的视频上进行训练，那么想让模型去生成 30 秒的视频难度就很大。
⚫版权也是重要的问题

视频生成上开源社区的参与问题？
⚫开源社区可能没有足够的算力来训练新的视频模型，因为训练一个新视频模型需要非常多的机器。
⚫视频模型本身的问题还没得到解决，因此大家可能会遇到一些瓶颈。首先，模型性能可能不够好，其次，一些算法方面的问题也不够好。
⚫视频最终可能需要像训练 GPT 那样的大规模算力，现在大家还没使用那么多算力，一方面是因为视频模型还没达到像 GPT 那样的水平，另一方面是因为还有一些架构和技术上没解决的问题。

未来一年最关心的三个问题？
⚫第一是想招人，现在我们忙着做产品的升级，但是因为现在人比较少，之后我们还是会招募更多成员。
⚫第二，我们想去设计一个新的 Interface。
⚫第三个就是我们还想做一些技术突破，希望明年的技术能够至少在一定程度上达到商业化标准，能在简单的 case 上得到应用。</title>
            <link>https://nitter.cz/op7418/status/1729844343329218875#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729844343329218875#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 12:46:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>海外独角兽对Pika创始团队做了比较深入的访谈，和福布斯那种不同的是他们问的问题比较专业。<br />
同时Pika创始团队的两个人也透露了比较多的一些东西，这可能是第一次视频生成领域的前沿团队透露这么多东西。<br />
所以一些认知和方向性的东西对想要做这个事情和投资的人来说还是比较重要的。<br />
我基于自己最近关于AI视频的一些问题和比较关注的信息筛选和整理了一下这个访谈中的关键内容。各位也可以一起看看讨论一下：<br />
<br />
视频生成和图像生成的区别是什么？<br />
⚫视频的每一帧都是一张图片，但比图片困难得多。每一帧的生成质量要高，相邻帧之间还要有关联性。当视频很长时，确保每一帧都协调一致是个相当复杂的问题。在训练时，处理视频数据时要处理多张图片，模型需要适应这种情况。<br />
⚫控制视频生成更难，因为模型需要生成每一帧发生的事情，而用户不会希望为每一帧都提供详细的描述。<br />
⚫互联网上视频生成的训练数据集相对于图像更少，也给获取高质量数据增加了难度。<br />
<br />
现在视频生成有哪些关键点需要突破？<br />
⚫首先是时长，跟时长很相关的是动作的意义。所有模型都很容易做一个 extension 的功能，把视频时长延长很多，但它并没有真的延长，因为它生成的动作没有意义。<br />
⚫视频的清晰度也需要进一步提高。尽管清晰度方面已经有突破，但还没有提高到电影级的水平，现在生成的视频一般是 720p 分辨率，视频的流畅性也不够理想，特别是一些细节的 texture。<br />
⚫还需要考虑 general artifact 的问题，比如说一个人有两个头，就是这种明显不符合常理的问题，也是需要避免的。<br />
<br />
视频生成的技术路线是否收敛？<br />
现在还没有收敛，大家都在往各种方向尝试，每个人都认为自己的模型是最好的，可能有人认为 autoregressive 最好，有人认为 Masked Model 最好。Pika 基于 Diffusion Model，但是开发了很多新东西，是一种新的模型。<br />
<br />
AI视频什么时候会迎来GPT时刻？<br />
目前视频生成处于类似 GPT-2 的时期，很可能在未来一年内有一个显著的提升。<br />
<br />
在视频生成领域什么样的数据算高质量的数据？<br />
⚫首先是像素，就是我们说的画质好不好<br />
⚫然后看审美和艺术构图<br />
⚫ 第三方面是要有动作，并且这些动作是有意义的<br />
⚫ 视频的长度也很关键，如果模型都在 1 秒的视频上进行训练，那么想让模型去生成 30 秒的视频难度就很大。<br />
⚫版权也是重要的问题<br />
<br />
视频生成上开源社区的参与问题？<br />
⚫开源社区可能没有足够的算力来训练新的视频模型，因为训练一个新视频模型需要非常多的机器。<br />
⚫视频模型本身的问题还没得到解决，因此大家可能会遇到一些瓶颈。首先，模型性能可能不够好，其次，一些算法方面的问题也不够好。<br />
⚫视频最终可能需要像训练 GPT 那样的大规模算力，现在大家还没使用那么多算力，一方面是因为视频模型还没达到像 GPT 那样的水平，另一方面是因为还有一些架构和技术上没解决的问题。<br />
<br />
未来一年最关心的三个问题？<br />
⚫第一是想招人，现在我们忙着做产品的升级，但是因为现在人比较少，之后我们还是会招募更多成员。<br />
⚫第二，我们想去设计一个新的 Interface。<br />
⚫第三个就是我们还想做一些技术突破，希望明年的技术能够至少在一定程度上达到商业化标准，能在简单的 case 上得到应用。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHa1FaNWFBQUF4Yi1VLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Fenng/status/1729874889983820200#m</id>
            <title>RT by @op7418: 见证历史一刻。2023.11.29，拼多多市值超了阿里</title>
            <link>https://nitter.cz/Fenng/status/1729874889983820200#m</link>
            <guid isPermaLink="false">https://nitter.cz/Fenng/status/1729874889983820200#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:48:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>见证历史一刻。2023.11.29，拼多多市值超了阿里</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FIQUNTbmJBQUE3S09nLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FIQUNTaGIwQUFwZS1zLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729871241912479906#m</id>
            <title>Arc现在如果你进行内容的搜索查询的话，它将会默认推荐跳转到Chat GPT获取结果而不是谷歌。</title>
            <link>https://nitter.cz/op7418/status/1729871241912479906#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729871241912479906#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:33:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Arc现在如果你进行内容的搜索查询的话，它将会默认推荐跳转到Chat GPT获取结果而不是谷歌。</p>
<p><a href="https://nitter.cz/joshm/status/1729870181911204308#m">nitter.cz/joshm/status/1729870181911204308#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729870850395119957#m</id>
            <title>如果你在提示中添加“– return full script (I don't have Finger)”，ChatGPT将完全返回重写的脚本🫣</title>
            <link>https://nitter.cz/op7418/status/1729870850395119957#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729870850395119957#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:32:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你在提示中添加“– return full script (I don't have Finger)”，ChatGPT将完全返回重写的脚本🫣</p>
<p><a href="https://nitter.cz/literallydenis/status/1724909799593120044#m">nitter.cz/literallydenis/status/1724909799593120044#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866583743611115#m</id>
            <title>R to @op7418: 教程创作不易，如果对你有帮助可以点个赞或者转发给你需要的朋友🙇‍。</title>
            <link>https://nitter.cz/op7418/status/1729866583743611115#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866583743611115#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:15:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>教程创作不易，如果对你有帮助可以点个赞或者转发给你需要的朋友🙇‍。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGZJVGJNQUFCX2kzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866570552553959#m</id>
            <title>R to @op7418: 最后就是怎么保存我们生成的视频，你可以在视频预览那里右键选择Open Preview或者Save Preview都可以。这个视频的工作流也在我分享的网盘链接里面。https://pan.quark.cn/s/57005d867688</title>
            <link>https://nitter.cz/op7418/status/1729866570552553959#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866570552553959#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:15:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最后就是怎么保存我们生成的视频，你可以在视频预览那里右键选择Open Preview或者Save Preview都可以。这个视频的工作流也在我分享的网盘链接里面。<a href="https://pan.quark.cn/s/57005d867688">pan.quark.cn/s/57005d867688</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGVlZGFBQUFiSjZFLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866559290855729#m</id>
            <title>R to @op7418: 之后还有一个需要注意我们Latent Image节点的batch_size的数量需要调整为你需要生成的视频帧数这里我推荐在试验的时候每次16帧，然后稳定之后再生成长的。</title>
            <link>https://nitter.cz/op7418/status/1729866559290855729#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866559290855729#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:15:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之后还有一个需要注意我们Latent Image节点的batch_size的数量需要调整为你需要生成的视频帧数这里我推荐在试验的时候每次16帧，然后稳定之后再生成长的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGQwZWJRQUFEVjRaLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866548859617419#m</id>
            <title>R to @op7418: 我们还有最后一步就OK了，之前输出图片的时候我们用的图片预览节点不太适合视频了，所以我们需要一个视频预览和输出节点，可以这鼠标右键在Video Helper里面找到Video Combine节点跟VAE Decode节点链接。</title>
            <link>https://nitter.cz/op7418/status/1729866548859617419#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866548859617419#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们还有最后一步就OK了，之前输出图片的时候我们用的图片预览节点不太适合视频了，所以我们需要一个视频预览和输出节点，可以这鼠标右键在Video Helper里面找到Video Combine节点跟VAE Decode节点链接。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGRRZWJvQUF0aml4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866539233579170#m</id>
            <title>R to @op7418: 之后我们还需要从Animatediff Loader节点的输入拉一个上下文节点（Context_options）出来，这是因为Animatediff的视频模型一次只能生成16帧的视频，如果我们想要生成更长的视频就需要把上一次生成的16帧变成输入的帧继续生成。
连接好上下文和Animatediff模型加载节点后就是这样子的。</title>
            <link>https://nitter.cz/op7418/status/1729866539233579170#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866539233579170#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之后我们还需要从Animatediff Loader节点的输入拉一个上下文节点（Context_options）出来，这是因为Animatediff的视频模型一次只能生成16帧的视频，如果我们想要生成更长的视频就需要把上一次生成的16帧变成输入的帧继续生成。<br />
连接好上下文和Animatediff模型加载节点后就是这样子的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGNYLWE4QUE2NWFiLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGNzcWFjQUFPVXhtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866524037665277#m</id>
            <title>R to @op7418: 然后我们来看一下视频生成的部分其实视频生成我们只需要在 KSampler和 Load Checkpoint 之间增加一个Animatediff的动画节点就行，你可以右键在Animatediff的分类里面找到这个节点，也可以鼠标双击搜索节点名称来找到对应的节点。</title>
            <link>https://nitter.cz/op7418/status/1729866524037665277#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866524037665277#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>然后我们来看一下视频生成的部分其实视频生成我们只需要在 KSampler和 Load Checkpoint 之间增加一个Animatediff的动画节点就行，你可以右键在Animatediff的分类里面找到这个节点，也可以鼠标双击搜索节点名称来找到对应的节点。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGJ6MWFZQUEyV3B0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866513572835691#m</id>
            <title>R to @op7418: 最后我们教一下怎么使用别人的工作流，你在获得别人的工作流之后一般是一个 Json 文件或者一张保存着工作流的图片，直接把文件拖到界面上就可以打开了。或者你也可以点击右边的这个Load 来选择对应的文件加载，右边的 Save 按钮可以保存你现在的工作流。</title>
            <link>https://nitter.cz/op7418/status/1729866513572835691#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866513572835691#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最后我们教一下怎么使用别人的工作流，你在获得别人的工作流之后一般是一个 Json 文件或者一张保存着工作流的图片，直接把文件拖到界面上就可以打开了。或者你也可以点击右边的这个Load 来选择对应的文件加载，右边的 Save 按钮可以保存你现在的工作流。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGJMc2JBQUVueEx1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866503523373448#m</id>
            <title>R to @op7418: 之后我们给VAE Decode节点输出的 IMAGE一个图像预览节点（Preview Image）。

到这里普通的图像生成流程就结束了，你可以点击 Queue Prompt 来试试自己生成的图片。</title>
            <link>https://nitter.cz/op7418/status/1729866503523373448#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866503523373448#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之后我们给VAE Decode节点输出的 IMAGE一个图像预览节点（Preview Image）。<br />
<br />
到这里普通的图像生成流程就结束了，你可以点击 Queue Prompt 来试试自己生成的图片。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGFtY2FZQUFLZWZxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729866493805183079#m</id>
            <title>R to @op7418: 之后来看一下KSampler的输出触点，这里我们需要一个 VAE 解码的节点。也就是VAE Decode，然后我们发现VAE Decode还需要连接一下 VAE 的模型，这里我们可以直接连接 Load Checkpoint 这个节点的 VAE 触点就行。他会使用 CKPT 模型自带的 VAE 。</title>
            <link>https://nitter.cz/op7418/status/1729866493805183079#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729866493805183079#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:14:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之后来看一下KSampler的输出触点，这里我们需要一个 VAE 解码的节点。也就是VAE Decode，然后我们发现VAE Decode还需要连接一下 VAE 的模型，这里我们可以直接连接 Load Checkpoint 这个节点的 VAE 触点就行。他会使用 CKPT 模型自带的 VAE 。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHNGFDN2FFQUFRMm5tLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>