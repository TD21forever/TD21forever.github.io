<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734954527462420822#m</id>
            <title>Notdiamond-0001这个项目挺屌的，可以自动帮你选择将用户的问题发送给GPT-4还是GPT-3.5，从而大幅降低调用模型的成本提高回答的准确性。
以后还会推出Gemini、Mistral、Claude 和 Llama这几个模型的自动选择。

下面是几个重点功能：
◇ 在用作路由器时，Notdiamond-0001的性能比GPT-4高出1.51倍。
◇ 确定要调用哪个模型在<10毫秒内完成。
◇ 可通过API免费获得或者在HF上使用，还会全天候持续监控OpenAI是否中断，并重新路由到你选择的备用模型。

未来的规划：很快将发布动态路由到 Gemini、Claude、Mistral、Llama、Cohere 和更多模型的功能，以及你自己的微调模型和自定义工作流程、代理、RAG 应用程序和链。

这里使用：https://huggingface.co/notdiamond/notdiamond-0001</title>
            <link>https://nitter.cz/op7418/status/1734954527462420822#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734954527462420822#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 15:12:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Notdiamond-0001这个项目挺屌的，可以自动帮你选择将用户的问题发送给GPT-4还是GPT-3.5，从而大幅降低调用模型的成本提高回答的准确性。<br />
以后还会推出Gemini、Mistral、Claude 和 Llama这几个模型的自动选择。<br />
<br />
下面是几个重点功能：<br />
◇ 在用作路由器时，Notdiamond-0001的性能比GPT-4高出1.51倍。<br />
◇ 确定要调用哪个模型在<10毫秒内完成。<br />
◇ 可通过API免费获得或者在HF上使用，还会全天候持续监控OpenAI是否中断，并重新路由到你选择的备用模型。<br />
<br />
未来的规划：很快将发布动态路由到 Gemini、Claude、Mistral、Llama、Cohere 和更多模型的功能，以及你自己的微调模型和自定义工作流程、代理、RAG 应用程序和链。<br />
<br />
这里使用：<a href="https://huggingface.co/notdiamond/notdiamond-0001">huggingface.co/notdiamond/no…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JQTHVoLWJNQUFmY1BRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734760424456024238#m</id>
            <title>RT by @op7418: Mixtral-8x7b 上限这个ELO测试之后获得了1000多次投票，得分马上就要超过cloude1 了，已经超过了所有开源模型，模型质量确实顶。</title>
            <link>https://nitter.cz/op7418/status/1734760424456024238#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734760424456024238#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:21:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b 上限这个ELO测试之后获得了1000多次投票，得分马上就要超过cloude1 了，已经超过了所有开源模型，模型质量确实顶。</p>
<p><a href="https://nitter.cz/lmsysorg/status/1734680611393073289#m">nitter.cz/lmsysorg/status/1734680611393073289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734829583328039177#m</id>
            <title>RT by @op7418: MJ Discord 有个频道的人写了一下 Midjourney 风格微调的原理，跟我原来想的完全不一样，原来提示词的作用没那么大。哈哈哈哈。

简单说一下结论：

✱无论使用什么提示词，风格调整器中两列 128 张图片的风格是固定的。
也就是说你用两个不同的提示词微调选择相同位置的图片也会获得一个相同的风格，比如下面的图是用不同的风格生成的，完全一样。

✱生成风格的提示词只对风格调整时的图像起作用，不会对使用风格的图像起作用。

✱你的提示词只有跟原提示词类似才能够获得跟风格调节器里面图片的效果。如果用的提示词跟原来的一点关系没有就有可能得到完全不同的结果。

那个帖子的原文翻译：

免责声明：此内容非 MidJourney 官方文档，仅代表个人观点，未经官方认证。

✱ 风格调节器是由 128 行和 3 列构成的网格。每个网格位置，如第 1 行左列（1L），在所有网格中完全一致，且风格指向的排序亦然。也就是说，如果你在十个不同的调节器中选择了 1L、6R、9L 和 11R，最终的风格效果将完全相同。
✱ 因此，你在风格调节器中所用的提示仅影响调节器内生成的图片，不会影响后续图片，除非使用同一提示语。
✱ 这里展示了四种不同风格调节器，使用同一调节方向生成的作品。

如果不同调节器生成的图片大相径庭，会影响用这个调节器生成的图片吗？
不会，因为提示仅对调节器内的图片产生影响，而非用调节器生成的图片。_那为何会有这么多不同的调节器？_ 因为每个调节器生成的图片都反映了所用的提示语，使你能通过不同风格直观感受你的提示。

我必须用类似于制作调节器时的提示语吗？
你的提示越接近原始提示，你得到的图片就越接近调节器内的图片。若使用不同提示，根据选择的风格方向，结果可能会截然不同。

那么，使用风格调节器的意义何在？
举例来说，对于 `picture of a woman` 这类提示，在有调节器之前，可用的风格非常有限。现在，调节器能展示出 256 种不同的 `picture of a woman` 风格，从喷笔到怪异、铅笔画到真实照片、鲜艳到柔和、繁复到简约、写实到超现实等。你可以选择多种或少量这些风格方向来创造自己的独特风格。若你的后续提示语接近调节器的提示，风格将非常可预测。若使用不同提示，可能会有意想不到的结果，但依然比默认的 MJ 提供了更多的风格选择。

原贴地址：https://discord.com/channels/662267976984297473/1179745230887591957</title>
            <link>https://nitter.cz/op7418/status/1734829583328039177#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734829583328039177#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 06:56:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MJ Discord 有个频道的人写了一下 Midjourney 风格微调的原理，跟我原来想的完全不一样，原来提示词的作用没那么大。哈哈哈哈。<br />
<br />
简单说一下结论：<br />
<br />
✱无论使用什么提示词，风格调整器中两列 128 张图片的风格是固定的。<br />
也就是说你用两个不同的提示词微调选择相同位置的图片也会获得一个相同的风格，比如下面的图是用不同的风格生成的，完全一样。<br />
<br />
✱生成风格的提示词只对风格调整时的图像起作用，不会对使用风格的图像起作用。<br />
<br />
✱你的提示词只有跟原提示词类似才能够获得跟风格调节器里面图片的效果。如果用的提示词跟原来的一点关系没有就有可能得到完全不同的结果。<br />
<br />
那个帖子的原文翻译：<br />
<br />
免责声明：此内容非 MidJourney 官方文档，仅代表个人观点，未经官方认证。<br />
<br />
✱ 风格调节器是由 128 行和 3 列构成的网格。每个网格位置，如第 1 行左列（1L），在所有网格中完全一致，且风格指向的排序亦然。也就是说，如果你在十个不同的调节器中选择了 1L、6R、9L 和 11R，最终的风格效果将完全相同。<br />
✱ 因此，你在风格调节器中所用的提示仅影响调节器内生成的图片，不会影响后续图片，除非使用同一提示语。<br />
✱ 这里展示了四种不同风格调节器，使用同一调节方向生成的作品。<br />
<br />
如果不同调节器生成的图片大相径庭，会影响用这个调节器生成的图片吗？<br />
不会，因为提示仅对调节器内的图片产生影响，而非用调节器生成的图片。_那为何会有这么多不同的调节器？_ 因为每个调节器生成的图片都反映了所用的提示语，使你能通过不同风格直观感受你的提示。<br />
<br />
我必须用类似于制作调节器时的提示语吗？<br />
你的提示越接近原始提示，你得到的图片就越接近调节器内的图片。若使用不同提示，根据选择的风格方向，结果可能会截然不同。<br />
<br />
那么，使用风格调节器的意义何在？<br />
举例来说，对于 `picture of a woman` 这类提示，在有调节器之前，可用的风格非常有限。现在，调节器能展示出 256 种不同的 `picture of a woman` 风格，从喷笔到怪异、铅笔画到真实照片、鲜艳到柔和、繁复到简约、写实到超现实等。你可以选择多种或少量这些风格方向来创造自己的独特风格。若你的后续提示语接近调节器的提示，风格将非常可预测。若使用不同提示，可能会有意想不到的结果，但依然比默认的 MJ 提供了更多的风格选择。<br />
<br />
原贴地址：<a href="https://discord.com/channels/662267976984297473/1179745230887591957">discord.com/channels/6622679…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JOYURsM2FVQUF0QkNZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734834158869123480#m</id>
            <title>RT by @op7418: 阿里期货开源，再发新项目：虚拟服装试穿。只需要一张人物照片和服装照片就可以让那个人穿上这个衣服，对服装行业是个利好。
效果确实很好，但是全是期货开源很容易被当成诈骗啊。
Reddit 有个宣传这个项目的帖子，下面全是在喷，用不到就是不存在。
还可以跟前几天发的Animate-Anyone项目结合生成跳舞视频。

简介：
Outfit Anyone通过利用双流条件扩散模型来解决这些限制，使其能够熟练处理服装变形，从而获得更逼真的结果。它通过可扩展性（调节姿势和身体形状等因素）和广泛适用性（从动漫到野外图像）来区别于其他方法。Outfit Anyone在各种场景中的表现突显了它在实际部署中的实用性和准备就绪性。

实现方法：
核心是条件扩散模型，它处理模特、服装和相关文本提示的图像，以服装图像作为控制因素。在内部，网络分为两个流，独立处理模特和服装数据。这些流在融合网络中汇聚，便于将服装细节嵌入模特的特征表示中。在此基础上，我们建立了Outfit Anyone，包括两个关键元素：用于初始试穿图像的零样本试穿网络，以及用于增强输出图像中服装和皮肤纹理的事后优化器。

项目地址：https://humanaigc.github.io/outfit-anyone/</title>
            <link>https://nitter.cz/op7418/status/1734834158869123480#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734834158869123480#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里期货开源，再发新项目：虚拟服装试穿。只需要一张人物照片和服装照片就可以让那个人穿上这个衣服，对服装行业是个利好。<br />
效果确实很好，但是全是期货开源很容易被当成诈骗啊。<br />
Reddit 有个宣传这个项目的帖子，下面全是在喷，用不到就是不存在。<br />
还可以跟前几天发的Animate-Anyone项目结合生成跳舞视频。<br />
<br />
简介：<br />
Outfit Anyone通过利用双流条件扩散模型来解决这些限制，使其能够熟练处理服装变形，从而获得更逼真的结果。它通过可扩展性（调节姿势和身体形状等因素）和广泛适用性（从动漫到野外图像）来区别于其他方法。Outfit Anyone在各种场景中的表现突显了它在实际部署中的实用性和准备就绪性。<br />
<br />
实现方法：<br />
核心是条件扩散模型，它处理模特、服装和相关文本提示的图像，以服装图像作为控制因素。在内部，网络分为两个流，独立处理模特和服装数据。这些流在融合网络中汇聚，便于将服装细节嵌入模特的特征表示中。在此基础上，我们建立了Outfit Anyone，包括两个关键元素：用于初始试穿图像的零样本试穿网络，以及用于增强输出图像中服装和皮肤纹理的事后优化器。<br />
<br />
项目地址：<a href="https://humanaigc.github.io/outfit-anyone/">humanaigc.github.io/outfit-a…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MzM2OTY4ODMzMzEwNzIvcHUvaW1nL3FsaG1TQl9ManRXRkp5V00uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734851634373820808#m</id>
            <title>RT by @op7418: 海德堡大学开源了一个优化版本的 ContorlNet 模型：ConTorlNetXS。这个架构的 ContorlNet 模型精简了原来的模型推理和训练的时间都提高了两倍，同时生成图片的质量更高，控制更加精准。

概述：
这个项目中，我们研究了用于控制基于稳定扩散模型的图像生成过程的ControlNet的大小和架构设计。我们展示了一个新的架构，其参数仅为基础模型的1%，实现了最先进的结果，并在FID分数方面表现出比ControlNet更好的性能。因此，我们称之为ControlNet-XS。

原理：
原始的ControlNet是StableDiffusion基础模型中U-Net编码器的副本，因此接收与基础模型相同的输入，并附加一个类似边缘图的引导信号。经过训练的ControlNet的中间输出然后添加到基础模型的解码器层的输入中。在ControlNet的训练过程中，基础模型的权重保持冻结。我们发现这种方法存在几个概念上的问题，导致ControlNet过于庞大，并且生成图像的质量显著降低。
我们通过将连接从编码器基础模型添加到控制编码器来解决延迟反馈的第一个问题。通过这种方式，纠正可以更快地适应基础模型的生成过程。然而，它并没有完全消除延迟，因为基础模型的编码器仍然没有受到引导。因此，我们从ControlNet-XS直接将额外的连接添加到基础模型的编码器中，直接影响整个生成过程。

大小和FID分数比较：
我们评估了三种变体（A、B、C）相对于原始的ControlNet在COCO2017的验证集上的FID分数表现。我们所有的变体都取得了显著的改进，同时只使用了原始ControlNet参数的一小部分。

论文地址：https://arxiv.org/abs/2312.06573</title>
            <link>https://nitter.cz/op7418/status/1734851634373820808#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734851634373820808#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 08:23:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>海德堡大学开源了一个优化版本的 ContorlNet 模型：ConTorlNetXS。这个架构的 ContorlNet 模型精简了原来的模型推理和训练的时间都提高了两倍，同时生成图片的质量更高，控制更加精准。<br />
<br />
概述：<br />
这个项目中，我们研究了用于控制基于稳定扩散模型的图像生成过程的ControlNet的大小和架构设计。我们展示了一个新的架构，其参数仅为基础模型的1%，实现了最先进的结果，并在FID分数方面表现出比ControlNet更好的性能。因此，我们称之为ControlNet-XS。<br />
<br />
原理：<br />
原始的ControlNet是StableDiffusion基础模型中U-Net编码器的副本，因此接收与基础模型相同的输入，并附加一个类似边缘图的引导信号。经过训练的ControlNet的中间输出然后添加到基础模型的解码器层的输入中。在ControlNet的训练过程中，基础模型的权重保持冻结。我们发现这种方法存在几个概念上的问题，导致ControlNet过于庞大，并且生成图像的质量显著降低。<br />
我们通过将连接从编码器基础模型添加到控制编码器来解决延迟反馈的第一个问题。通过这种方式，纠正可以更快地适应基础模型的生成过程。然而，它并没有完全消除延迟，因为基础模型的编码器仍然没有受到引导。因此，我们从ControlNet-XS直接将额外的连接添加到基础模型的编码器中，直接影响整个生成过程。<br />
<br />
大小和FID分数比较：<br />
我们评估了三种变体（A、B、C）相对于原始的ControlNet在COCO2017的验证集上的FID分数表现。我们所有的变体都取得了显著的改进，同时只使用了原始ControlNet参数的一小部分。<br />
<br />
论文地址：<a href="https://arxiv.org/abs/2312.06573">arxiv.org/abs/2312.06573</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4NTA3ODIxNTQxMDg5MjgvcHUvaW1nLzdqaUpfTFdwaGhQVFhHZ2ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734818099659174081#m</id>
            <title>RT by @op7418: Runway 即将推出文本生成语音工具，支持多种语言，从演示来看音质很好，而且情绪和停顿都很自然。</title>
            <link>https://nitter.cz/op7418/status/1734818099659174081#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734818099659174081#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 06:10:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 即将推出文本生成语音工具，支持多种语言，从演示来看音质很好，而且情绪和停顿都很自然。</p>
<p><a href="https://nitter.cz/iamneubert/status/1734721951841546463#m">nitter.cz/iamneubert/status/1734721951841546463#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MTc4MjE3NzM5Nzk2NDgvcHUvaW1nL1ZMVVRrS0NjdmtlM1BNTVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734869378724950074#m</id>
            <title>RT by @op7418: Krea现在开始向所有人开放，不需要邀请了，没有资格的可以试试了。</title>
            <link>https://nitter.cz/op7418/status/1734869378724950074#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734869378724950074#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 09:34:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Krea现在开始向所有人开放，不需要邀请了，没有资格的可以试试了。</p>
<p><a href="https://nitter.cz/krea_ai/status/1734866368489722035#m">nitter.cz/krea_ai/status/1734866368489722035#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734770668242432210#m</id>
            <title>RT by @op7418: 意外的发现我已经有了 Midjourney Alpha 版本也就是图片生成版本的权限，体验了一下录制了一个视频，这下图片生成真的方便多了。
写一下如何使用Alpha 版本和图片生成的一些变化，后面发现的小细节也会写在这个帖子上：

如何使用 Alpha 版本：
如果已经生成一万张图可以使用，在 Discord 里面输入/info 可以看到生成的图片数量，也可以直接访问这个链接看自己是不是有权限：http://alpha.midjourney.com

图片生成功能细节：
✦点击页面上方的提示词输入框可以直接开始生成图片。
✦输入框右侧按钮点击可以调整图片生成的所有参数。
✦+号按钮可以上传图片或者使用已有的图片垫图。
✦鼠标 Hover 到每个参数上都可以看到具体的解释。
✦已经生成的图像设置参数直接点击就可以回填到提示词输入框里面。
✦已经生成的图片右下角包含了，可以对图片进行的所有操作，比如放大使用提示词，使用图片，放大重随等。
✦正在生成的图片和已经生成的图片是在一起的，今天的内容会放大显示，昨天的会变成小图。</title>
            <link>https://nitter.cz/op7418/status/1734770668242432210#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734770668242432210#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:02:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>意外的发现我已经有了 Midjourney Alpha 版本也就是图片生成版本的权限，体验了一下录制了一个视频，这下图片生成真的方便多了。<br />
写一下如何使用Alpha 版本和图片生成的一些变化，后面发现的小细节也会写在这个帖子上：<br />
<br />
如何使用 Alpha 版本：<br />
如果已经生成一万张图可以使用，在 Discord 里面输入/info 可以看到生成的图片数量，也可以直接访问这个链接看自己是不是有权限：<a href="http://alpha.midjourney.com">alpha.midjourney.com</a><br />
<br />
图片生成功能细节：<br />
✦点击页面上方的提示词输入框可以直接开始生成图片。<br />
✦输入框右侧按钮点击可以调整图片生成的所有参数。<br />
✦+号按钮可以上传图片或者使用已有的图片垫图。<br />
✦鼠标 Hover 到每个参数上都可以看到具体的解释。<br />
✦已经生成的图像设置参数直接点击就可以回填到提示词输入框里面。<br />
✦已经生成的图片右下角包含了，可以对图片进行的所有操作，比如放大使用提示词，使用图片，放大重随等。<br />
✦正在生成的图片和已经生成的图片是在一起的，今天的内容会放大显示，昨天的会变成小图。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NzA1NTM4MzAxNTgzMzYvcHUvaW1nL0h0UUlTSWNqSHlTTjBYTnIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734925508956352513#m</id>
            <title>这游戏画风不暴力操作挺暴力啊，妄图挑战迪斯尼法务部。</title>
            <link>https://nitter.cz/op7418/status/1734925508956352513#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734925508956352513#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 13:17:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这游戏画风不暴力操作挺暴力啊，妄图挑战迪斯尼法务部。</p>
<p><a href="https://nitter.cz/LinusEkenstam/status/1734728826146443374#m">nitter.cz/LinusEkenstam/status/1734728826146443374#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734869967764685086#m</id>
            <title>R to @op7418: 地址：https://www.krea.ai/home</title>
            <link>https://nitter.cz/op7418/status/1734869967764685086#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734869967764685086#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 09:36:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>地址：<a href="https://www.krea.ai/home">krea.ai/home</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734865889483489747#m</id>
            <title>R to @op7418: 他测试了中文，不太行，口音太重了
https://x.com/iamneubert/status/1734731970712604788?s=20</title>
            <link>https://nitter.cz/op7418/status/1734865889483489747#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734865889483489747#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 09:20:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他测试了中文，不太行，口音太重了<br />
<a href="https://x.com/iamneubert/status/1734731970712604788?s=20">x.com/iamneubert/status/1734…</a></p>
<p><a href="https://nitter.cz/iamneubert/status/1734731970712604788#m">nitter.cz/iamneubert/status/1734731970712604788#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734834548062720190#m</id>
            <title>R to @op7418: 他们要再这么搞开源名声都臭了</title>
            <link>https://nitter.cz/op7418/status/1734834548062720190#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734834548062720190#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:16:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他们要再这么搞开源名声都臭了</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734830577474490544#m</id>
            <title>R to @op7418: Midjourney 官方推特刚才点赞了这条，麻了金 V 都掉了，我以为高仿呢。</title>
            <link>https://nitter.cz/op7418/status/1734830577474490544#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734830577474490544#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:00:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney 官方推特刚才点赞了这条，麻了金 V 都掉了，我以为高仿呢。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734818295977857258#m</id>
            <title>R to @op7418: 终于开始卷视频之外的东西了</title>
            <link>https://nitter.cz/op7418/status/1734818295977857258#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734818295977857258#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 06:11:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于开始卷视频之外的东西了</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>