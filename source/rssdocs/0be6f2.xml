<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Gorden Sun / @Gorden_Sun</title>
        <link>https://nitter.cz/Gorden_Sun</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742314322414502023#m</id>
            <title>RT by @Gorden_Sun: 拾象科技CEO李广密的这个采访给出的信息和判断太多了，节选一下，顺序有一定打乱，强烈建议大家看原文或者去听播客，链接：http://t.cn/A6lmccU9

- OpenAI 是在一年前做出的 GPT-4， Anthropic 是半年前做出来的，Google 是下个月才能真正推出 GPT-4，全球其他团队可能还需要 6- 12 个月才能做出来。从 GPT-3 到 GPT-3.5，很多公司有机会达到，但是从 GPT-3.5 到 GPT-4 难度会增加 5-10 倍，只有极少数公司能到。

- 模型公司的壁垒很像台积电或者 SpaceX，有很强的先发效应和规模效应的，但现在还不知道能不能像互联网范式一样有网络效应。

- 全球第一梯队的模型，如果没有 100 亿美金的储备、而且有机会转化成 GPU，是没有办法待在全球第一梯队的，这是一个硬标准。

- 2024 年可能基本上会决定大概的格局。窗口就是未来 12 个月，如果未来 12 个月追不上去，后面再翻转其实是很难了。模型竞争很残酷，很像造芯片或者做 SpaceX，最理想化的格局是很可能只剩一家，最领先的模型又最便宜，没有理由用第二家，但因为有抗衡微软跟 OpenAI 联盟的阵营在，所以会有不同的阵营，这样推演下来可能大概率是 2-3 家。

- 2024 年的叙事肯定是多模态，Google Gemini 就是打了一个新的开端。

- OpenAI 一年做到 10 多亿美元的 ARR（编注：最新的报道是16亿美元），明年可能是五六十亿美元的 ARR，它可能是历史上增长最快的公司。但整个市场上其他的大模型 native 的产品所有的 ARR 加在一起是不到 10 亿美金的。

- 硅谷 VC 几乎都错过了大模型的投资，也同样都错过了对 SpaceX 和 Tesla 的投资，这几件事都是典型的重投入、早期看不到商业模式、风险很大，不符合硅谷 VC 的典型投资偏好。

- 三个头部厂商：微软和 OpenAI，其次是亚马逊和 Google 支持的 Anthropic，第三个是 Google，它自成一派，Apple 跟 Tesla 是潜在的关键变量。

有三个大生意和大模型最相关，首先是芯片，英伟达在这一波就很激进，第二波是公有云，微软的云和亚马逊的云是两个是最大的，可能未来模型都是要跑在云上，所以云厂商拿未来每年营收的 3- 5 个点去投模型公司也很合理。第三个大生意是终端，手机和车，所以 Apple 和 Tesla 未来会是更关键的阵营。

- http://X.ai 现在是晚了 6- 12 个月的，未来有大于 50% 的概率追上；开源模型有可能未来就等于 Meta。

- 关于全球对大模型的投入，OpenAI 未来训练模型可能还需要至少得200-300 亿美金， Google 也不能低于这个数，Anthropic 大概也需要 100-200 亿美金，未来几年，3-5 年至少要花 1000 亿美金赌下去。
- 关于大模型产品：1.大模型是最核心的，没有模型可能是没有所谓的 AI native 应用。2.智能是最关键的变量，过去的产品经验可能在今天是一种包袱，只是模型之上怼很多的功能、UI、 UX 有可能是徒劳的，更本质的是要理解模型的能力是什么。
- 那些复刻GPT-4的厂商是如何做到的：

1.全球范围真的对大模型能有实际大贡献的天才 researcher 可能就两三百个人，天才科学家的聚集效应是很强的，这种人和这种 research 文化其实是非常重要的，不是所有巨头都具备这样的条件。

2.GPT-4 的短期壁垒是数据，尤其是 pre-training 和 post-training 阶段的数据，全球范围真正有 GPT-4 数据 know-how 的只有两三百个人，而且几乎都在目前头部的三家模型公司，其他公司想搞清楚这件事至少得经过几百次、甚至几千次充足的实验，小几万张卡是一个必要条件。

3.训练成本，如果 Claude-3 和 GPT-4.5 训练成本可能 2 到 3 亿美元，那再往后的 25、26 年，更下一代的模型训练成本至少可能是 10 亿美元，甚至说 30、50 亿美元。

4.另外一个核心变量可能还是取决于大家是不是信仰 scaling law，以及能不能做到、能不能继续 scaling 下去，这件事可能是长期的唯一关键变量，只有极少数的科学家是很信的，比如说 http://Character.AI 的 Noam， Anthropic 的 Dario，还有  OpenAI 的 Ilya，他们三个对 scaling law 的贡献也是最大的，同时也是信仰最强的。

- 关于大模型成本：

训练成本其实是分两个部分，一部分是实验成本，一部分是最终大规模训练的成本。可以理解为一年当中是有 9 个月要做实验的，实验就是用小尺寸的模型做训练，做足训练之后，2-3 个月做一次大的训练，这一次就像一次大的火箭的发射，所以简单按时间来分，3/ 4 的成本用在做实验， 1/ 4 用在大的训练，也就是“发射”。

模型参数量在 700 亿是一个分界点， 700 亿以下能容忍非常多的错误，模型不会在训练过程中崩掉，700 亿参数以上每往上扩大一个级别，遇到的训练的难度是指数级提升的，模型越大越容易出错。

OpenAI 的成本优化能力是很强的，比如说他们训练完 GPT-4 以后，因为具备了这个训练能力了，可以再重新训练一个 GPT-3.5、把 3.5 的推理成本降得非常低。

现在共识是下一代就是多模态模型，各种模态的数据要从头 pre-train 进去，而不是用现在的 Flamingo 挂起来，视频数据的 pre-train 其实比文本的 token 整个更复杂，要高出一个量级的 GPU 资源。如果参数量又扩大一倍，又是一个多模态的模型，它需要的 GPU 资源可能是之前的 10-20 倍以上的，而且还包含了优化能力。

下一代模型实际算力可能是当年 GPT-4 的 16- 32 倍的提升，如果这样算下去，到 2025 年训练一个大的模型，他估计可能花费要 10- 30 亿美元之间。

- 开源与闭源之争：

开源模型是追不上闭源模型的，而且差距肯定会越来越大，大模型很像芯片或者 Space X，因为大模型它不是一个传统意义的软件开源，模型不可编码，不可解释，大家没办法一起做贡献，包括 GPU 要在单一一个集训练起来训练才更高效。

开源模型的使命不是做最聪明的模型，而是承接先进模型溢出的很多能力，做民主化。因为未来很多用户和企业的需求是分层的，可能有相当大比例的需求是通过一定能力的模型就可以覆盖的，很多企业和大规模的用户优先考虑的是成本问题，这部分是开源的优势。

开源模型在 2024 年追齐 GPT-4 还是挺挑战的。

下一个开源模型重要的方向是端侧，端侧意味着很多推理成本可以放到端侧，会让 AI 公司的成本结构发生很大的变化。

- 一个模型公司最重要的是至少有一个天才的科学家。上半场可能不一定是 CEO，但科学家一定是最重要的，以及团队的科学家文化，能够持续不断的探索、做实验是最重要的，下半场有可能是商业和应用。

- Sam Altman 跟乔布斯和马斯克好像不太像一类人，乔布斯和马斯克在硅谷几乎没有朋友，但 Sam 在硅谷所有人都是朋友，Sam 更像是一个政客。

以上整理的要点转载自微博用户 i陆三金： https://weibo.com/1706699904/NzIFhs3w7</title>
            <link>https://nitter.cz/dotey/status/1742314322414502023#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742314322414502023#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 22:37:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>拾象科技CEO李广密的这个采访给出的信息和判断太多了，节选一下，顺序有一定打乱，强烈建议大家看原文或者去听播客，链接：<a href="http://t.cn/A6lmccU9">t.cn/A6lmccU9</a><br />
<br />
- OpenAI 是在一年前做出的 GPT-4， Anthropic 是半年前做出来的，Google 是下个月才能真正推出 GPT-4，全球其他团队可能还需要 6- 12 个月才能做出来。从 GPT-3 到 GPT-3.5，很多公司有机会达到，但是从 GPT-3.5 到 GPT-4 难度会增加 5-10 倍，只有极少数公司能到。<br />
<br />
- 模型公司的壁垒很像台积电或者 SpaceX，有很强的先发效应和规模效应的，但现在还不知道能不能像互联网范式一样有网络效应。<br />
<br />
- 全球第一梯队的模型，如果没有 100 亿美金的储备、而且有机会转化成 GPU，是没有办法待在全球第一梯队的，这是一个硬标准。<br />
<br />
- 2024 年可能基本上会决定大概的格局。窗口就是未来 12 个月，如果未来 12 个月追不上去，后面再翻转其实是很难了。模型竞争很残酷，很像造芯片或者做 SpaceX，最理想化的格局是很可能只剩一家，最领先的模型又最便宜，没有理由用第二家，但因为有抗衡微软跟 OpenAI 联盟的阵营在，所以会有不同的阵营，这样推演下来可能大概率是 2-3 家。<br />
<br />
- 2024 年的叙事肯定是多模态，Google Gemini 就是打了一个新的开端。<br />
<br />
- OpenAI 一年做到 10 多亿美元的 ARR（编注：最新的报道是16亿美元），明年可能是五六十亿美元的 ARR，它可能是历史上增长最快的公司。但整个市场上其他的大模型 native 的产品所有的 ARR 加在一起是不到 10 亿美金的。<br />
<br />
- 硅谷 VC 几乎都错过了大模型的投资，也同样都错过了对 SpaceX 和 Tesla 的投资，这几件事都是典型的重投入、早期看不到商业模式、风险很大，不符合硅谷 VC 的典型投资偏好。<br />
<br />
- 三个头部厂商：微软和 OpenAI，其次是亚马逊和 Google 支持的 Anthropic，第三个是 Google，它自成一派，Apple 跟 Tesla 是潜在的关键变量。<br />
<br />
有三个大生意和大模型最相关，首先是芯片，英伟达在这一波就很激进，第二波是公有云，微软的云和亚马逊的云是两个是最大的，可能未来模型都是要跑在云上，所以云厂商拿未来每年营收的 3- 5 个点去投模型公司也很合理。第三个大生意是终端，手机和车，所以 Apple 和 Tesla 未来会是更关键的阵营。<br />
<br />
- <a href="http://X.ai">X.ai</a> 现在是晚了 6- 12 个月的，未来有大于 50% 的概率追上；开源模型有可能未来就等于 Meta。<br />
<br />
- 关于全球对大模型的投入，OpenAI 未来训练模型可能还需要至少得200-300 亿美金， Google 也不能低于这个数，Anthropic 大概也需要 100-200 亿美金，未来几年，3-5 年至少要花 1000 亿美金赌下去。<br />
- 关于大模型产品：1.大模型是最核心的，没有模型可能是没有所谓的 AI native 应用。2.智能是最关键的变量，过去的产品经验可能在今天是一种包袱，只是模型之上怼很多的功能、UI、 UX 有可能是徒劳的，更本质的是要理解模型的能力是什么。<br />
- 那些复刻GPT-4的厂商是如何做到的：<br />
<br />
1.全球范围真的对大模型能有实际大贡献的天才 researcher 可能就两三百个人，天才科学家的聚集效应是很强的，这种人和这种 research 文化其实是非常重要的，不是所有巨头都具备这样的条件。<br />
<br />
2.GPT-4 的短期壁垒是数据，尤其是 pre-training 和 post-training 阶段的数据，全球范围真正有 GPT-4 数据 know-how 的只有两三百个人，而且几乎都在目前头部的三家模型公司，其他公司想搞清楚这件事至少得经过几百次、甚至几千次充足的实验，小几万张卡是一个必要条件。<br />
<br />
3.训练成本，如果 Claude-3 和 GPT-4.5 训练成本可能 2 到 3 亿美元，那再往后的 25、26 年，更下一代的模型训练成本至少可能是 10 亿美元，甚至说 30、50 亿美元。<br />
<br />
4.另外一个核心变量可能还是取决于大家是不是信仰 scaling law，以及能不能做到、能不能继续 scaling 下去，这件事可能是长期的唯一关键变量，只有极少数的科学家是很信的，比如说 <a href="http://Character.AI">Character.AI</a> 的 Noam， Anthropic 的 Dario，还有  OpenAI 的 Ilya，他们三个对 scaling law 的贡献也是最大的，同时也是信仰最强的。<br />
<br />
- 关于大模型成本：<br />
<br />
训练成本其实是分两个部分，一部分是实验成本，一部分是最终大规模训练的成本。可以理解为一年当中是有 9 个月要做实验的，实验就是用小尺寸的模型做训练，做足训练之后，2-3 个月做一次大的训练，这一次就像一次大的火箭的发射，所以简单按时间来分，3/ 4 的成本用在做实验， 1/ 4 用在大的训练，也就是“发射”。<br />
<br />
模型参数量在 700 亿是一个分界点， 700 亿以下能容忍非常多的错误，模型不会在训练过程中崩掉，700 亿参数以上每往上扩大一个级别，遇到的训练的难度是指数级提升的，模型越大越容易出错。<br />
<br />
OpenAI 的成本优化能力是很强的，比如说他们训练完 GPT-4 以后，因为具备了这个训练能力了，可以再重新训练一个 GPT-3.5、把 3.5 的推理成本降得非常低。<br />
<br />
现在共识是下一代就是多模态模型，各种模态的数据要从头 pre-train 进去，而不是用现在的 Flamingo 挂起来，视频数据的 pre-train 其实比文本的 token 整个更复杂，要高出一个量级的 GPU 资源。如果参数量又扩大一倍，又是一个多模态的模型，它需要的 GPU 资源可能是之前的 10-20 倍以上的，而且还包含了优化能力。<br />
<br />
下一代模型实际算力可能是当年 GPT-4 的 16- 32 倍的提升，如果这样算下去，到 2025 年训练一个大的模型，他估计可能花费要 10- 30 亿美元之间。<br />
<br />
- 开源与闭源之争：<br />
<br />
开源模型是追不上闭源模型的，而且差距肯定会越来越大，大模型很像芯片或者 Space X，因为大模型它不是一个传统意义的软件开源，模型不可编码，不可解释，大家没办法一起做贡献，包括 GPU 要在单一一个集训练起来训练才更高效。<br />
<br />
开源模型的使命不是做最聪明的模型，而是承接先进模型溢出的很多能力，做民主化。因为未来很多用户和企业的需求是分层的，可能有相当大比例的需求是通过一定能力的模型就可以覆盖的，很多企业和大规模的用户优先考虑的是成本问题，这部分是开源的优势。<br />
<br />
开源模型在 2024 年追齐 GPT-4 还是挺挑战的。<br />
<br />
下一个开源模型重要的方向是端侧，端侧意味着很多推理成本可以放到端侧，会让 AI 公司的成本结构发生很大的变化。<br />
<br />
- 一个模型公司最重要的是至少有一个天才的科学家。上半场可能不一定是 CEO，但科学家一定是最重要的，以及团队的科学家文化，能够持续不断的探索、做实验是最重要的，下半场有可能是商业和应用。<br />
<br />
- Sam Altman 跟乔布斯和马斯克好像不太像一类人，乔布斯和马斯克在硅谷几乎没有朋友，但 Sam 在硅谷所有人都是朋友，Sam 更像是一个政客。<br />
<br />
以上整理的要点转载自微博用户 i陆三金： <a href="https://weibo.com/1706699904/NzIFhs3w7">weibo.com/1706699904/NzIFhs3…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MzeGJSUFh3QUFMa3Z5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742121849541329321#m</id>
            <title>AI资讯日报，1月2日：https://gorden-sun.notion.site/1-2-AI-e9b709afdf4e46d9880fbaaa5f00977f?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742121849541329321#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742121849541329321#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 09:53:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，1月2日：<a href="https://gorden-sun.notion.site/1-2-AI-e9b709afdf4e46d9880fbaaa5f00977f?pvs=4">gorden-sun.notion.site/1-2-A…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MxQ2xBTGFzQUFjVFZzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742092294948594002#m</id>
            <title>GPTs从简单到复杂可以划分为4种：
1）只通过文字限定角色和功能的GPTs
2）在1的基础上，附加知识库
3）调用标准OpenAPI的GPTs
4）调用自定义API的GPTs
GPTs的潜力非常大，用第4种GPTs甚至可以实现独立的小应用。
这篇教程完整介绍了每种GPTs的创建流程：https://mojju.com/blog/how-to-create-complex-gpts-with-api-actions-and-a-node-js-backend</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742092294948594002#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742092294948594002#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 07:55:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs从简单到复杂可以划分为4种：<br />
1）只通过文字限定角色和功能的GPTs<br />
2）在1的基础上，附加知识库<br />
3）调用标准OpenAPI的GPTs<br />
4）调用自定义API的GPTs<br />
GPTs的潜力非常大，用第4种GPTs甚至可以实现独立的小应用。<br />
这篇教程完整介绍了每种GPTs的创建流程：<a href="https://mojju.com/blog/how-to-create-complex-gpts-with-api-actions-and-a-node-js-backend">mojju.com/blog/how-to-create…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MwbmlVQ2JvQUFTZ1FOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741985433649926332#m</id>
            <title>有没有这样的GPU平台：
我选好SD版本，设置好checkpoint模型和ControlNet模型，设置好默认填充的提示词和参数。其他用户来了开箱即用，并且根据使用量我能赚到收益。

这样一来，基于SD的应用都可以被取代，艺术字、二维码、换脸、上色等等。</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741985433649926332#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741985433649926332#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 00:51:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有没有这样的GPU平台：<br />
我选好SD版本，设置好checkpoint模型和ControlNet模型，设置好默认填充的提示词和参数。其他用户来了开箱即用，并且根据使用量我能赚到收益。<br />
<br />
这样一来，基于SD的应用都可以被取代，艺术字、二维码、换脸、上色等等。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741794568818053539#m</id>
            <title>AI资讯日报，1月1日：https://gorden-sun.notion.site/1-1-AI-4814d23b1304458eb4644a82ca8b9628?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741794568818053539#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741794568818053539#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jan 2024 12:12:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，1月1日：<a href="https://gorden-sun.notion.site/1-1-AI-4814d23b1304458eb4644a82ca8b9628?pvs=4">gorden-sun.notion.site/1-1-A…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0N3WTZwT2JZQUFSWl9YLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741793485106319657#m</id>
            <title>LLaVA-3b：基于 Dolphin 2.6 Phi（微软Phi的微调版本）的多模态模型，图片模型使用的是 SigLIP 400M。
模型地址：https://huggingface.co/visheratin/LLaVA-3b</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741793485106319657#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741793485106319657#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jan 2024 12:08:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LLaVA-3b：基于 Dolphin 2.6 Phi（微软Phi的微调版本）的多模态模型，图片模型使用的是 SigLIP 400M。<br />
模型地址：<a href="https://huggingface.co/visheratin/LLaVA-3b">huggingface.co/visheratin/LL…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MTU0MzI0ODAzNDY2NDQ0OS9fX3prYzdOTT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741791400927326548#m</id>
            <title>FlowVid：Meta推出的一致性视频生成视频模型
保持主体一致，可以修改视频风格或替换视频主体。
项目地址：https://jeff-liangf.github.io/projects/flowvid/
Github（暂未发布代码）：https://github.com/Jeff-LiangF/FlowVid
论文：https://arxiv.org/abs/2312.17681</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741791400927326548#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741791400927326548#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jan 2024 12:00:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>FlowVid：Meta推出的一致性视频生成视频模型<br />
保持主体一致，可以修改视频风格或替换视频主体。<br />
项目地址：<a href="https://jeff-liangf.github.io/projects/flowvid/">jeff-liangf.github.io/projec…</a><br />
Github（暂未发布代码）：<a href="https://github.com/Jeff-LiangF/FlowVid">github.com/Jeff-LiangF/FlowV…</a><br />
论文：<a href="https://arxiv.org/abs/2312.17681">arxiv.org/abs/2312.17681</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDE3OTA4MTI0NDk3MDU5ODQvcHUvaW1nL2lzRVo0U0daN01fRkhEM3guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741788126165799330#m</id>
            <title>TinyLlama 发布1.0版本，1.1B 参数，基于3万亿 tokens 训练，与 LLaMa 2 完全相同的架构和分词器。
从他们的Github能看到完整的训练过程。
模型地址：https://huggingface.co/TinyLlama
Github：https://github.com/jzhang38/TinyLlama</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741788126165799330#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741788126165799330#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jan 2024 11:47:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TinyLlama 发布1.0版本，1.1B 参数，基于3万亿 tokens 训练，与 LLaMa 2 完全相同的架构和分词器。<br />
从他们的Github能看到完整的训练过程。<br />
模型地址：<a href="https://huggingface.co/TinyLlama">huggingface.co/TinyLlama</a><br />
Github：<a href="https://github.com/jzhang38/TinyLlama">github.com/jzhang38/TinyLlam…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MTc3MjMzNjI5NzM1NzMxMi9uM3puMVBVaD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741419996994269542#m</id>
            <title>AI资讯日报，12月31日：https://gorden-sun.notion.site/12-31-AI-682e263f6f39467d997695cc721bee3e?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741419996994269542#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741419996994269542#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 11:24:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，12月31日：<a href="https://gorden-sun.notion.site/12-31-AI-682e263f6f39467d997695cc721bee3e?pvs=4">gorden-sun.notion.site/12-31…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NyRVBmSmF3QUFfZFlrLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741410171153486176#m</id>
            <title>HandRefiner：修复AI画的异常的手部
11月发布的论文，12月29号发布的代码。识别出图片中的手部，并局部重绘为正常的手部。
示例的使用方法是python命令行，估计ControlNet里引入模型也可以正常使用。
Github：https://github.com/wenquanlu/HandRefiner/
论文：https://arxiv.org/abs/2311.17957
模型：https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/tree/main</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741410171153486176#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741410171153486176#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 10:45:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HandRefiner：修复AI画的异常的手部<br />
11月发布的论文，12月29号发布的代码。识别出图片中的手部，并局部重绘为正常的手部。<br />
示例的使用方法是python命令行，估计ControlNet里引入模型也可以正常使用。<br />
Github：<a href="https://github.com/wenquanlu/HandRefiner/">github.com/wenquanlu/HandRef…</a><br />
论文：<a href="https://arxiv.org/abs/2311.17957">arxiv.org/abs/2311.17957</a><br />
模型：<a href="https://huggingface.co/hr16/ControlNet-HandRefiner-pruned/tree/main">huggingface.co/hr16/ControlN…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NxN1N1NmJNQUFtZ3B0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741087804975972811#m</id>
            <title>AI生成视频工具Assistive Video有免费试用了</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741087804975972811#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741087804975972811#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 13:24:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI生成视频工具Assistive Video有免费试用了</p>
<p><a href="https://nitter.cz/assistiveapp/status/1741086196917477587#m">nitter.cz/assistiveapp/status/1741086196917477587#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1741067085877485691#m</id>
            <title>AI资讯日报，12月30日：https://gorden-sun.notion.site/12-30-AI-f514ecc668b8488aae330c3da4a0ba0b?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1741067085877485691#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1741067085877485691#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 12:01:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，12月30日：<a href="https://gorden-sun.notion.site/12-30-AI-f514ecc668b8488aae330c3da4a0ba0b?pvs=4">gorden-sun.notion.site/12-30…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NtRFJxZWJJQUFad2txLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740930168393466246#m</id>
            <title>微软的Copilot出iOS版本了，基本跟之前的Bing Chat差不多，不买ChatGPT Plus的可以考虑用这个。</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740930168393466246#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740930168393466246#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 02:57:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软的Copilot出iOS版本了，基本跟之前的Bing Chat差不多，不买ChatGPT Plus的可以考虑用这个。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NrR3dWQmJNQUFMX29ILmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NrR3dVX2FrQUF4RFNpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NrR3dVX2FrQUUzTXJtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/ZHOZHO672070/status/1740811505413787708#m</id>
            <title>RT by @Gorden_Sun: 👀有小伙伴问除了聊天和生成提示词以外，ComfyUI中的Gemini模型还可以用来干嘛，其实用处有很多，相当于通过ComfyUI这个平台将LLM与图像/视频模型结合起来，实现一种“散装”的多模态
🤖具体来说，最直接最实用的一个用法就是将pro vision模型用于批量打标，关键是免费哈哈哈哈
🤣期待大家创造更多玩法</title>
            <link>https://nitter.cz/ZHOZHO672070/status/1740811505413787708#m</link>
            <guid isPermaLink="false">https://nitter.cz/ZHOZHO672070/status/1740811505413787708#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 19:06:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>👀有小伙伴问除了聊天和生成提示词以外，ComfyUI中的Gemini模型还可以用来干嘛，其实用处有很多，相当于通过ComfyUI这个平台将LLM与图像/视频模型结合起来，实现一种“散装”的多模态<br />
🤖具体来说，最直接最实用的一个用法就是将pro vision模型用于批量打标，关键是免费哈哈哈哈<br />
🤣期待大家创造更多玩法</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDA4MTEzMTI0MDg0NzM2MDAvcHUvaW1nL2p1eWg0aEZsYWRJQ2hwZFkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740668733180510345#m</id>
            <title>AI资讯日报，12月29日：https://gorden-sun.notion.site/12-29-AI-fac4f646715143afbbf8f3f150771e28?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740668733180510345#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740668733180510345#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 09:39:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，12月29日：<a href="https://gorden-sun.notion.site/12-29-AI-fac4f646715143afbbf8f3f150771e28?pvs=4">gorden-sun.notion.site/12-29…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NnWS1ZcWJZQUEyZEVBLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740665370988658868#m</id>
            <title>Deita：微调LLM的高质量数据和工具包
目前开源了6K和10K大小高质量数据，用10K数据微调的Mistral 7B模型，MT-Bench得分7.55，AlpacaEval得分90（超过openchat-3.5）
Github：https://github.com/hkust-nlp/deita
论文：https://arxiv.org/abs/2312.15685</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740665370988658868#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740665370988658868#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 09:25:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Deita：微调LLM的高质量数据和工具包<br />
目前开源了6K和10K大小高质量数据，用10K数据微调的Mistral 7B模型，MT-Bench得分7.55，AlpacaEval得分90（超过openchat-3.5）<br />
Github：<a href="https://github.com/hkust-nlp/deita">github.com/hkust-nlp/deita</a><br />
论文：<a href="https://arxiv.org/abs/2312.15685">arxiv.org/abs/2312.15685</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NnVjZTb2FVQUFoLVphLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740661879624872078#m</id>
            <title>阿里放出了DreaMoving的Demo，但是这个空间实在是拉胯，排队要几个小时，复制空间又报错😅</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740661879624872078#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740661879624872078#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 09:11:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里放出了DreaMoving的Demo，但是这个空间实在是拉胯，排队要几个小时，复制空间又报错😅</p>
<p><a href="https://nitter.cz/_akhaliq/status/1740380744726594003#m">nitter.cz/_akhaliq/status/1740380744726594003#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740651315066544251#m</id>
            <title>UForm：仅 1.5B 大小的多模态模型
能读图和聊天，由 llama-1.3B 和 ViT-B/16 组成。
Github：https://github.com/unum-cloud/uform</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740651315066544251#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740651315066544251#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 08:29:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>UForm：仅 1.5B 大小的多模态模型<br />
能读图和聊天，由 llama-1.3B 和 ViT-B/16 组成。<br />
Github：<a href="https://github.com/unum-cloud/uform">github.com/unum-cloud/uform</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MTIzNDkyODE0NTA1MTY0OC9nNTlfcER4Yj9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740646486302748961#m</id>
            <title>WhatOnEarth：搜索+ChatGPT
与Perplexity的区别是，直接把界面做成了搜索引擎的样式，而不是聊天的样式。支持中文输入，但只能英文回复。
地址：https://www.whatonearth.ai/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740646486302748961#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740646486302748961#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 08:10:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WhatOnEarth：搜索+ChatGPT<br />
与Perplexity的区别是，直接把界面做成了搜索引擎的样式，而不是聊天的样式。支持中文输入，但只能英文回复。<br />
地址：<a href="https://www.whatonearth.ai/">whatonearth.ai/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NnRXVsd2JFQUFIaDNFLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1740585793255862636#m</id>
            <title>今年4月份看到过一句关于AI发展现状的讨论：
Stuff happening faster than you can read up on what stuff is happening.

现在回头看，真是太形象了。看不完，完全看不完。</title>
            <link>https://nitter.cz/Gorden_Sun/status/1740585793255862636#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1740585793255862636#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 04:09:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今年4月份看到过一句关于AI发展现状的讨论：<br />
Stuff happening faster than you can read up on what stuff is happening.<br />
<br />
现在回头看，真是太形象了。看不完，完全看不完。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>