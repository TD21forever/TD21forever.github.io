<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Gorden Sun / @Gorden_Sun</title>
        <link>https://nitter.cz/Gorden_Sun</link>
        
        <item>
            <id>https://nitter.cz/ZHOZHO672070/status/1742884117191004644#m</id>
            <title>RT by @Gorden_Sun: 🔥ComfyUI ArtGallery 1.0正式版 来啦！
🤣Prompt Visualization 提示词可视化：对于很多不熟悉艺术的小伙伴来说，面对众多陌生的的选项时，没有参考和预览是一件是非头疼的事，并且还只能等生成之后才能做出判断
🌟现在只需选择喜欢的图像，滑动选择权重，节点会自动输出提示词，助你自由探索艺术世界</title>
            <link>https://nitter.cz/ZHOZHO672070/status/1742884117191004644#m</link>
            <guid isPermaLink="false">https://nitter.cz/ZHOZHO672070/status/1742884117191004644#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 12:22:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔥ComfyUI ArtGallery 1.0正式版 来啦！<br />
🤣Prompt Visualization 提示词可视化：对于很多不熟悉艺术的小伙伴来说，面对众多陌生的的选项时，没有参考和预览是一件是非头疼的事，并且还只能等生成之后才能做出判断<br />
🌟现在只需选择喜欢的图像，滑动选择权重，节点会自动输出提示词，助你自由探索艺术世界</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI4ODM4MTEwNDAzNjY1OTMvcHUvaW1nL0JnLWc0VThORGcyRXNZZmguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742904738281320698#m</id>
            <title>ComfyUI工作流的下载链接：https://drive.google.com/file/d/1ysc1C5E4qdNvtKKEDAbHyUYLc7SnbPUX/view?pli=1</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742904738281320698#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742904738281320698#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 13:44:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI工作流的下载链接：<a href="https://drive.google.com/file/d/1ysc1C5E4qdNvtKKEDAbHyUYLc7SnbPUX/view?pli=1">drive.google.com/file/d/1ysc…</a></p>
<p><a href="https://nitter.cz/op7418/status/1742891365485494444#m">nitter.cz/op7418/status/1742891365485494444#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742873702621126924#m</id>
            <title>RT by @Gorden_Sun: SD A1111 Controlnet 更新了手部修复的预处理器depth_hand_refiner，现在可以在 Web UI 里面对出问题的手进行修复了。

如何使用：
在图生图 Tab 选择inpaint，然后给需要修复的手部涂上蒙版，然后在 Contorlnet 选择depth_hand_refiner 预处理器，点击生成就可以了。</title>
            <link>https://nitter.cz/op7418/status/1742873702621126924#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742873702621126924#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 11:40:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SD A1111 Controlnet 更新了手部修复的预处理器depth_hand_refiner，现在可以在 Web UI 里面对出问题的手进行修复了。<br />
<br />
如何使用：<br />
在图生图 Tab 选择inpaint，然后给需要修复的手部涂上蒙版，然后在 Contorlnet 选择depth_hand_refiner 预处理器，点击生成就可以了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfdUVnb2FJQUFQaC1sLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfdVVhWGFBQUFNcUVGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742859637467255213#m</id>
            <title>微软这是要把键盘左侧的Windows键换成Copilot键？</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742859637467255213#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742859637467255213#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 10:44:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软这是要把键盘左侧的Windows键换成Copilot键？</p>
<p><a href="https://nitter.cz/yusuf_i_mehdi/status/1742819477681582198#m">nitter.cz/yusuf_i_mehdi/status/1742819477681582198#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742840316485566730#m</id>
            <title>AI资讯日报，1月4日：https://gorden-sun.notion.site/1-4-AI-b95e3dbd45ff4091ac10d211395e1cb2?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742840316485566730#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742840316485566730#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 09:28:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，1月4日：<a href="https://gorden-sun.notion.site/1-4-AI-b95e3dbd45ff4091ac10d211395e1cb2?pvs=4">gorden-sun.notion.site/1-4-A…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfUUJUN2F3QUFfa0M1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742833536368029842#m</id>
            <title>Self-Extend：无需训练和微调，仅通过几行代码，即可提升LLM context的有效长度
不是提升LLM的上下文长度，是减少了上下文长度的退化。方法看起来异常的简单，只是后移了token的位置（在更远的地方建索引）
论文：https://arxiv.org/abs/2401.01325</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742833536368029842#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742833536368029842#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 09:01:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Self-Extend：无需训练和微调，仅通过几行代码，即可提升LLM context的有效长度<br />
不是提升LLM的上下文长度，是减少了上下文长度的退化。方法看起来异常的简单，只是后移了token的位置（在更远的地方建索引）<br />
论文：<a href="https://arxiv.org/abs/2401.01325">arxiv.org/abs/2401.01325</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfSnk0emJjQUFtQmpBLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NfSjJaY2JvQUFzMkZuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742821932695990288#m</id>
            <title>这篇论文总结了减少LLM幻觉的32种方法，包括RAG、微调模型，提示词工程等。
论文：https://arxiv.org/abs/2401.01313</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742821932695990288#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742821932695990288#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 08:15:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇论文总结了减少LLM幻觉的32种方法，包括RAG、微调模型，提示词工程等。<br />
论文：<a href="https://arxiv.org/abs/2401.01313">arxiv.org/abs/2401.01313</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MtX0hxaWFnQUFULV9OLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742819283477139946#m</id>
            <title>E5-mistral-7b-instruct：使用合成数据训练的Embedding模型
亮点是仅使用LLM生成的数据即可实现不错的效果，使用合成数据+标记数据训练可以实现SOTA。
大小比其他模型大10倍，性能也仅是微弱提升。更大的意义是实践了合成数据的作用。
论文：https://arxiv.org/abs/2401.00368
模型：https://huggingface.co/intfloat/e5-mistral-7b-instruct</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742819283477139946#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742819283477139946#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 08:04:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>E5-mistral-7b-instruct：使用合成数据训练的Embedding模型<br />
亮点是仅使用LLM生成的数据即可实现不错的效果，使用合成数据+标记数据训练可以实现SOTA。<br />
大小比其他模型大10倍，性能也仅是微弱提升。更大的意义是实践了合成数据的作用。<br />
论文：<a href="https://arxiv.org/abs/2401.00368">arxiv.org/abs/2401.00368</a><br />
模型：<a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct">huggingface.co/intfloat/e5-m…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MtODVDWWEwQUFpdWdwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742813091170308211#m</id>
            <title>之前的辐射渲染，可以合成高质量的图片和3D场景，但是训练成本高，渲染时间长。
新兴的高斯泼溅，可以实时渲染，但是存在图片模糊的情况。
Deblurring-3D-Gaussian-Splatting提出了新的实时去模糊框架，可以实现高质量实时渲染。
项目地址：https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/
论文：https://arxiv.org/abs/2401.00834</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742813091170308211#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742813091170308211#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 07:39:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前的辐射渲染，可以合成高质量的图片和3D场景，但是训练成本高，渲染时间长。<br />
新兴的高斯泼溅，可以实时渲染，但是存在图片模糊的情况。<br />
Deblurring-3D-Gaussian-Splatting提出了新的实时去模糊框架，可以实现高质量实时渲染。<br />
项目地址：<a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">benhenryl.github.io/Deblurri…</a><br />
论文：<a href="https://arxiv.org/abs/2401.00834">arxiv.org/abs/2401.00834</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI4MTMwNjU3NTcwMzY1NDQvcHUvaW1nL051M2VLU3FSUDQtYjNiQVcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/tonyzzhao/status/1742603121682153852#m</id>
            <title>RT by @Gorden_Sun: Introducing 𝐌𝐨𝐛𝐢𝐥𝐞 𝐀𝐋𝐎𝐇𝐀🏄 -- Hardware!
A low-cost, open-source, mobile manipulator.

One of the most high-effort projects in my past 5yrs! Not possible without co-lead @zipengfu and @chelseabfinn.

At the end, what's better than cooking yourself a meal with the 🤖🧑‍🍳</title>
            <link>https://nitter.cz/tonyzzhao/status/1742603121682153852#m</link>
            <guid isPermaLink="false">https://nitter.cz/tonyzzhao/status/1742603121682153852#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 17:45:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Introducing 𝐌𝐨𝐛𝐢𝐥𝐞 𝐀𝐋𝐎𝐇𝐀🏄 -- Hardware!<br />
A low-cost, open-source, mobile manipulator.<br />
<br />
One of the most high-effort projects in my past 5yrs! Not possible without co-lead <a href="https://nitter.cz/zipengfu" title="Zipeng Fu">@zipengfu</a> and <a href="https://nitter.cz/chelseabfinn" title="Chelsea Finn">@chelseabfinn</a>.<br />
<br />
At the end, what's better than cooking yourself a meal with the 🤖🧑‍🍳</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI1OTQzNzA4MjQ3NTMxNTMvcHUvaW1nL2d3X0JnTG4tZzhNMlk3WkguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742484602193920101#m</id>
            <title>AI资讯日报，1月3日：https://gorden-sun.notion.site/1-3-AI-c2a34f21403543ee8b7e15507df3d1e1?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742484602193920101#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742484602193920101#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:54:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，1月3日：<a href="https://gorden-sun.notion.site/1-3-AI-c2a34f21403543ee8b7e15507df3d1e1?pvs=4">gorden-sun.notion.site/1-3-A…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M2TWdETWF3QUFPVndWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742479464452563229#m</id>
            <title>DiffusionLight：检测图片中的光源
使用ControlNet的Depth模型，通过在图片中画一个反光金属球，实现了目前效果最好的光源检测。
应用场景：可以根据光源，把物体插入到图片中，遵循相同的光影效果。
项目地址：https://diffusionlight.github.io/
Github：https://github.com/DiffusionLight/DiffusionLight</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742479464452563229#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742479464452563229#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:34:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DiffusionLight：检测图片中的光源<br />
使用ControlNet的Depth模型，通过在图片中画一个反光金属球，实现了目前效果最好的光源检测。<br />
应用场景：可以根据光源，把物体插入到图片中，遵循相同的光影效果。<br />
项目地址：<a href="https://diffusionlight.github.io/">diffusionlight.github.io/</a><br />
Github：<a href="https://github.com/DiffusionLight/DiffusionLight">github.com/DiffusionLight/Di…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0Nzk0MDk1OTkzOTM3OTIvcHUvaW1nLzNZTDJfLVdiVkw0ZkJ3S3cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742475555885474203#m</id>
            <title>通过文本、图片、音频、视频都能生成音乐，听起来很厉害，实际对于seq2seq来说是“基操勿六”。
seq2seq是信息的万物互连，没有中介。例如英文语音翻译成中文语音，人脑里至少要经过英文语音->中文文字->中文语音的过程，但对于seq2seq来说，只需要英文语音->中文语音。
seq2seq应该叫anything2anything</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742475555885474203#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742475555885474203#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:18:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过文本、图片、音频、视频都能生成音乐，听起来很厉害，实际对于seq2seq来说是“基操勿六”。<br />
seq2seq是信息的万物互连，没有中介。例如英文语音翻译成中文语音，人脑里至少要经过英文语音-&gt;中文文字-&gt;中文语音的过程，但对于seq2seq来说，只需要英文语音-&gt;中文语音。<br />
seq2seq应该叫anything2anything</p>
<p><a href="https://nitter.cz/_akhaliq/status/1742209475283423556#m">nitter.cz/_akhaliq/status/1742209475283423556#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742472101100224720#m</id>
            <title>MathPile：用于提升LLM数学能力的语料库
包含10亿token的语料，包括教科书、arXiv、维基百科等多种来源的数学内容，目标是提升LLM的数学推理能力。
项目地址：https://gair-nlp.github.io/MathPile/
Github：https://github.com/GAIR-NLP/MathPile/
语料库：https://huggingface.co/datasets/GAIR/MathPile</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742472101100224720#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742472101100224720#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:04:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MathPile：用于提升LLM数学能力的语料库<br />
包含10亿token的语料，包括教科书、arXiv、维基百科等多种来源的数学内容，目标是提升LLM的数学推理能力。<br />
项目地址：<a href="https://gair-nlp.github.io/MathPile/">gair-nlp.github.io/MathPile/</a><br />
Github：<a href="https://github.com/GAIR-NLP/MathPile/">github.com/GAIR-NLP/MathPile…</a><br />
语料库：<a href="https://huggingface.co/datasets/GAIR/MathPile">huggingface.co/datasets/GAIR…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M2QklZV2JVQUFyX2cxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742466974264025390#m</id>
            <title>Reddit上有个网友根据Mistral的API定价，推测Mistral-Medium是8x30B的混合专家模型。
原贴地址：https://www.reddit.com/r/LocalLLaMA/comments/18m2t0z/deducing_mistral_medium_size_from_pricing_is_it_a/
他的计算过程：https://docs.google.com/spreadsheets/d/1ud5zma-9SghdDuck3BTjWSE6psGWbH2aUPPuTGoyNBI/edit#gid=1579842251
不过也有人提到，Mistral之前开发了70B大小的模型，也可能是基于这个模型研发的。
之前提到70B模型的新闻：https://techcrunch.com/2023/11/09/theres-something-going-on-with-ai-startups-in-france/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742466974264025390#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742466974264025390#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 08:44:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Reddit上有个网友根据Mistral的API定价，推测Mistral-Medium是8x30B的混合专家模型。<br />
原贴地址：<a href="https://teddit.net/r/LocalLLaMA/comments/18m2t0z/deducing_mistral_medium_size_from_pricing_is_it_a/">teddit.net/r/LocalLLaMA/comm…</a><br />
他的计算过程：<a href="https://docs.google.com/spreadsheets/d/1ud5zma-9SghdDuck3BTjWSE6psGWbH2aUPPuTGoyNBI/edit#gid=1579842251">docs.google.com/spreadsheets…</a><br />
不过也有人提到，Mistral之前开发了70B大小的模型，也可能是基于这个模型研发的。<br />
之前提到70B模型的新闻：<a href="https://techcrunch.com/2023/11/09/theres-something-going-on-with-ai-startups-in-france/">techcrunch.com/2023/11/09/th…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M1OGR5SmJVQUExcE44LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M1OGR1bGFVQUFFYXhQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742462517920899550#m</id>
            <title>VideoDrafter：多场景视频生成
相比之前的生成视频模型，最大的特点是能在主体不变的基础上，一次性生成多个场景的视频。没有开源。
项目地址：https://videodrafter.github.io/
论文：https://arxiv.org/abs/2401.01256</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742462517920899550#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742462517920899550#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 08:26:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VideoDrafter：多场景视频生成<br />
相比之前的生成视频模型，最大的特点是能在主体不变的基础上，一次性生成多个场景的视频。没有开源。<br />
项目地址：<a href="https://videodrafter.github.io/">videodrafter.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2401.01256">arxiv.org/abs/2401.01256</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0NjI0Mzg3ODY5Mjg2NDAvcHUvaW1nLy1pUTloMkh4S3RXUW5vbFYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742459896577290287#m</id>
            <title>三星即将在1月17日推出Galaxy AI，手机上的AI。暂时没有更多信息，不知用的是哪家模型。</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742459896577290287#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742459896577290287#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 08:16:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>三星即将在1月17日推出Galaxy AI，手机上的AI。暂时没有更多信息，不知用的是哪家模型。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0NTk1NzgwMTE1NDE1MDQvcHUvaW1nL1hBdnVQbEQ0MHNmM1VtVWcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742314322414502023#m</id>
            <title>RT by @Gorden_Sun: 拾象科技CEO李广密的这个采访给出的信息和判断太多了，节选一下，顺序有一定打乱，强烈建议大家看原文或者去听播客，链接：http://t.cn/A6lmccU9

- OpenAI 是在一年前做出的 GPT-4， Anthropic 是半年前做出来的，Google 是下个月才能真正推出 GPT-4，全球其他团队可能还需要 6- 12 个月才能做出来。从 GPT-3 到 GPT-3.5，很多公司有机会达到，但是从 GPT-3.5 到 GPT-4 难度会增加 5-10 倍，只有极少数公司能到。

- 模型公司的壁垒很像台积电或者 SpaceX，有很强的先发效应和规模效应的，但现在还不知道能不能像互联网范式一样有网络效应。

- 全球第一梯队的模型，如果没有 100 亿美金的储备、而且有机会转化成 GPU，是没有办法待在全球第一梯队的，这是一个硬标准。

- 2024 年可能基本上会决定大概的格局。窗口就是未来 12 个月，如果未来 12 个月追不上去，后面再翻转其实是很难了。模型竞争很残酷，很像造芯片或者做 SpaceX，最理想化的格局是很可能只剩一家，最领先的模型又最便宜，没有理由用第二家，但因为有抗衡微软跟 OpenAI 联盟的阵营在，所以会有不同的阵营，这样推演下来可能大概率是 2-3 家。

- 2024 年的叙事肯定是多模态，Google Gemini 就是打了一个新的开端。

- OpenAI 一年做到 10 多亿美元的 ARR（编注：最新的报道是16亿美元），明年可能是五六十亿美元的 ARR，它可能是历史上增长最快的公司。但整个市场上其他的大模型 native 的产品所有的 ARR 加在一起是不到 10 亿美金的。

- 硅谷 VC 几乎都错过了大模型的投资，也同样都错过了对 SpaceX 和 Tesla 的投资，这几件事都是典型的重投入、早期看不到商业模式、风险很大，不符合硅谷 VC 的典型投资偏好。

- 三个头部厂商：微软和 OpenAI，其次是亚马逊和 Google 支持的 Anthropic，第三个是 Google，它自成一派，Apple 跟 Tesla 是潜在的关键变量。

有三个大生意和大模型最相关，首先是芯片，英伟达在这一波就很激进，第二波是公有云，微软的云和亚马逊的云是两个是最大的，可能未来模型都是要跑在云上，所以云厂商拿未来每年营收的 3- 5 个点去投模型公司也很合理。第三个大生意是终端，手机和车，所以 Apple 和 Tesla 未来会是更关键的阵营。

- http://X.ai 现在是晚了 6- 12 个月的，未来有大于 50% 的概率追上；开源模型有可能未来就等于 Meta。

- 关于全球对大模型的投入，OpenAI 未来训练模型可能还需要至少得200-300 亿美金， Google 也不能低于这个数，Anthropic 大概也需要 100-200 亿美金，未来几年，3-5 年至少要花 1000 亿美金赌下去。
- 关于大模型产品：1.大模型是最核心的，没有模型可能是没有所谓的 AI native 应用。2.智能是最关键的变量，过去的产品经验可能在今天是一种包袱，只是模型之上怼很多的功能、UI、 UX 有可能是徒劳的，更本质的是要理解模型的能力是什么。
- 那些复刻GPT-4的厂商是如何做到的：

1.全球范围真的对大模型能有实际大贡献的天才 researcher 可能就两三百个人，天才科学家的聚集效应是很强的，这种人和这种 research 文化其实是非常重要的，不是所有巨头都具备这样的条件。

2.GPT-4 的短期壁垒是数据，尤其是 pre-training 和 post-training 阶段的数据，全球范围真正有 GPT-4 数据 know-how 的只有两三百个人，而且几乎都在目前头部的三家模型公司，其他公司想搞清楚这件事至少得经过几百次、甚至几千次充足的实验，小几万张卡是一个必要条件。

3.训练成本，如果 Claude-3 和 GPT-4.5 训练成本可能 2 到 3 亿美元，那再往后的 25、26 年，更下一代的模型训练成本至少可能是 10 亿美元，甚至说 30、50 亿美元。

4.另外一个核心变量可能还是取决于大家是不是信仰 scaling law，以及能不能做到、能不能继续 scaling 下去，这件事可能是长期的唯一关键变量，只有极少数的科学家是很信的，比如说 http://Character.AI 的 Noam， Anthropic 的 Dario，还有  OpenAI 的 Ilya，他们三个对 scaling law 的贡献也是最大的，同时也是信仰最强的。

- 关于大模型成本：

训练成本其实是分两个部分，一部分是实验成本，一部分是最终大规模训练的成本。可以理解为一年当中是有 9 个月要做实验的，实验就是用小尺寸的模型做训练，做足训练之后，2-3 个月做一次大的训练，这一次就像一次大的火箭的发射，所以简单按时间来分，3/ 4 的成本用在做实验， 1/ 4 用在大的训练，也就是“发射”。

模型参数量在 700 亿是一个分界点， 700 亿以下能容忍非常多的错误，模型不会在训练过程中崩掉，700 亿参数以上每往上扩大一个级别，遇到的训练的难度是指数级提升的，模型越大越容易出错。

OpenAI 的成本优化能力是很强的，比如说他们训练完 GPT-4 以后，因为具备了这个训练能力了，可以再重新训练一个 GPT-3.5、把 3.5 的推理成本降得非常低。

现在共识是下一代就是多模态模型，各种模态的数据要从头 pre-train 进去，而不是用现在的 Flamingo 挂起来，视频数据的 pre-train 其实比文本的 token 整个更复杂，要高出一个量级的 GPU 资源。如果参数量又扩大一倍，又是一个多模态的模型，它需要的 GPU 资源可能是之前的 10-20 倍以上的，而且还包含了优化能力。

下一代模型实际算力可能是当年 GPT-4 的 16- 32 倍的提升，如果这样算下去，到 2025 年训练一个大的模型，他估计可能花费要 10- 30 亿美元之间。

- 开源与闭源之争：

开源模型是追不上闭源模型的，而且差距肯定会越来越大，大模型很像芯片或者 Space X，因为大模型它不是一个传统意义的软件开源，模型不可编码，不可解释，大家没办法一起做贡献，包括 GPU 要在单一一个集训练起来训练才更高效。

开源模型的使命不是做最聪明的模型，而是承接先进模型溢出的很多能力，做民主化。因为未来很多用户和企业的需求是分层的，可能有相当大比例的需求是通过一定能力的模型就可以覆盖的，很多企业和大规模的用户优先考虑的是成本问题，这部分是开源的优势。

开源模型在 2024 年追齐 GPT-4 还是挺挑战的。

下一个开源模型重要的方向是端侧，端侧意味着很多推理成本可以放到端侧，会让 AI 公司的成本结构发生很大的变化。

- 一个模型公司最重要的是至少有一个天才的科学家。上半场可能不一定是 CEO，但科学家一定是最重要的，以及团队的科学家文化，能够持续不断的探索、做实验是最重要的，下半场有可能是商业和应用。

- Sam Altman 跟乔布斯和马斯克好像不太像一类人，乔布斯和马斯克在硅谷几乎没有朋友，但 Sam 在硅谷所有人都是朋友，Sam 更像是一个政客。

以上整理的要点转载自微博用户 i陆三金： https://weibo.com/1706699904/NzIFhs3w7</title>
            <link>https://nitter.cz/dotey/status/1742314322414502023#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742314322414502023#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 22:37:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>拾象科技CEO李广密的这个采访给出的信息和判断太多了，节选一下，顺序有一定打乱，强烈建议大家看原文或者去听播客，链接：<a href="http://t.cn/A6lmccU9">t.cn/A6lmccU9</a><br />
<br />
- OpenAI 是在一年前做出的 GPT-4， Anthropic 是半年前做出来的，Google 是下个月才能真正推出 GPT-4，全球其他团队可能还需要 6- 12 个月才能做出来。从 GPT-3 到 GPT-3.5，很多公司有机会达到，但是从 GPT-3.5 到 GPT-4 难度会增加 5-10 倍，只有极少数公司能到。<br />
<br />
- 模型公司的壁垒很像台积电或者 SpaceX，有很强的先发效应和规模效应的，但现在还不知道能不能像互联网范式一样有网络效应。<br />
<br />
- 全球第一梯队的模型，如果没有 100 亿美金的储备、而且有机会转化成 GPU，是没有办法待在全球第一梯队的，这是一个硬标准。<br />
<br />
- 2024 年可能基本上会决定大概的格局。窗口就是未来 12 个月，如果未来 12 个月追不上去，后面再翻转其实是很难了。模型竞争很残酷，很像造芯片或者做 SpaceX，最理想化的格局是很可能只剩一家，最领先的模型又最便宜，没有理由用第二家，但因为有抗衡微软跟 OpenAI 联盟的阵营在，所以会有不同的阵营，这样推演下来可能大概率是 2-3 家。<br />
<br />
- 2024 年的叙事肯定是多模态，Google Gemini 就是打了一个新的开端。<br />
<br />
- OpenAI 一年做到 10 多亿美元的 ARR（编注：最新的报道是16亿美元），明年可能是五六十亿美元的 ARR，它可能是历史上增长最快的公司。但整个市场上其他的大模型 native 的产品所有的 ARR 加在一起是不到 10 亿美金的。<br />
<br />
- 硅谷 VC 几乎都错过了大模型的投资，也同样都错过了对 SpaceX 和 Tesla 的投资，这几件事都是典型的重投入、早期看不到商业模式、风险很大，不符合硅谷 VC 的典型投资偏好。<br />
<br />
- 三个头部厂商：微软和 OpenAI，其次是亚马逊和 Google 支持的 Anthropic，第三个是 Google，它自成一派，Apple 跟 Tesla 是潜在的关键变量。<br />
<br />
有三个大生意和大模型最相关，首先是芯片，英伟达在这一波就很激进，第二波是公有云，微软的云和亚马逊的云是两个是最大的，可能未来模型都是要跑在云上，所以云厂商拿未来每年营收的 3- 5 个点去投模型公司也很合理。第三个大生意是终端，手机和车，所以 Apple 和 Tesla 未来会是更关键的阵营。<br />
<br />
- <a href="http://X.ai">X.ai</a> 现在是晚了 6- 12 个月的，未来有大于 50% 的概率追上；开源模型有可能未来就等于 Meta。<br />
<br />
- 关于全球对大模型的投入，OpenAI 未来训练模型可能还需要至少得200-300 亿美金， Google 也不能低于这个数，Anthropic 大概也需要 100-200 亿美金，未来几年，3-5 年至少要花 1000 亿美金赌下去。<br />
- 关于大模型产品：1.大模型是最核心的，没有模型可能是没有所谓的 AI native 应用。2.智能是最关键的变量，过去的产品经验可能在今天是一种包袱，只是模型之上怼很多的功能、UI、 UX 有可能是徒劳的，更本质的是要理解模型的能力是什么。<br />
- 那些复刻GPT-4的厂商是如何做到的：<br />
<br />
1.全球范围真的对大模型能有实际大贡献的天才 researcher 可能就两三百个人，天才科学家的聚集效应是很强的，这种人和这种 research 文化其实是非常重要的，不是所有巨头都具备这样的条件。<br />
<br />
2.GPT-4 的短期壁垒是数据，尤其是 pre-training 和 post-training 阶段的数据，全球范围真正有 GPT-4 数据 know-how 的只有两三百个人，而且几乎都在目前头部的三家模型公司，其他公司想搞清楚这件事至少得经过几百次、甚至几千次充足的实验，小几万张卡是一个必要条件。<br />
<br />
3.训练成本，如果 Claude-3 和 GPT-4.5 训练成本可能 2 到 3 亿美元，那再往后的 25、26 年，更下一代的模型训练成本至少可能是 10 亿美元，甚至说 30、50 亿美元。<br />
<br />
4.另外一个核心变量可能还是取决于大家是不是信仰 scaling law，以及能不能做到、能不能继续 scaling 下去，这件事可能是长期的唯一关键变量，只有极少数的科学家是很信的，比如说 <a href="http://Character.AI">Character.AI</a> 的 Noam， Anthropic 的 Dario，还有  OpenAI 的 Ilya，他们三个对 scaling law 的贡献也是最大的，同时也是信仰最强的。<br />
<br />
- 关于大模型成本：<br />
<br />
训练成本其实是分两个部分，一部分是实验成本，一部分是最终大规模训练的成本。可以理解为一年当中是有 9 个月要做实验的，实验就是用小尺寸的模型做训练，做足训练之后，2-3 个月做一次大的训练，这一次就像一次大的火箭的发射，所以简单按时间来分，3/ 4 的成本用在做实验， 1/ 4 用在大的训练，也就是“发射”。<br />
<br />
模型参数量在 700 亿是一个分界点， 700 亿以下能容忍非常多的错误，模型不会在训练过程中崩掉，700 亿参数以上每往上扩大一个级别，遇到的训练的难度是指数级提升的，模型越大越容易出错。<br />
<br />
OpenAI 的成本优化能力是很强的，比如说他们训练完 GPT-4 以后，因为具备了这个训练能力了，可以再重新训练一个 GPT-3.5、把 3.5 的推理成本降得非常低。<br />
<br />
现在共识是下一代就是多模态模型，各种模态的数据要从头 pre-train 进去，而不是用现在的 Flamingo 挂起来，视频数据的 pre-train 其实比文本的 token 整个更复杂，要高出一个量级的 GPU 资源。如果参数量又扩大一倍，又是一个多模态的模型，它需要的 GPU 资源可能是之前的 10-20 倍以上的，而且还包含了优化能力。<br />
<br />
下一代模型实际算力可能是当年 GPT-4 的 16- 32 倍的提升，如果这样算下去，到 2025 年训练一个大的模型，他估计可能花费要 10- 30 亿美元之间。<br />
<br />
- 开源与闭源之争：<br />
<br />
开源模型是追不上闭源模型的，而且差距肯定会越来越大，大模型很像芯片或者 Space X，因为大模型它不是一个传统意义的软件开源，模型不可编码，不可解释，大家没办法一起做贡献，包括 GPU 要在单一一个集训练起来训练才更高效。<br />
<br />
开源模型的使命不是做最聪明的模型，而是承接先进模型溢出的很多能力，做民主化。因为未来很多用户和企业的需求是分层的，可能有相当大比例的需求是通过一定能力的模型就可以覆盖的，很多企业和大规模的用户优先考虑的是成本问题，这部分是开源的优势。<br />
<br />
开源模型在 2024 年追齐 GPT-4 还是挺挑战的。<br />
<br />
下一个开源模型重要的方向是端侧，端侧意味着很多推理成本可以放到端侧，会让 AI 公司的成本结构发生很大的变化。<br />
<br />
- 一个模型公司最重要的是至少有一个天才的科学家。上半场可能不一定是 CEO，但科学家一定是最重要的，以及团队的科学家文化，能够持续不断的探索、做实验是最重要的，下半场有可能是商业和应用。<br />
<br />
- Sam Altman 跟乔布斯和马斯克好像不太像一类人，乔布斯和马斯克在硅谷几乎没有朋友，但 Sam 在硅谷所有人都是朋友，Sam 更像是一个政客。<br />
<br />
以上整理的要点转载自微博用户 i陆三金： <a href="https://weibo.com/1706699904/NzIFhs3w7">weibo.com/1706699904/NzIFhs3…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MzeGJSUFh3QUFMa3Z5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742121849541329321#m</id>
            <title>AI资讯日报，1月2日：https://gorden-sun.notion.site/1-2-AI-e9b709afdf4e46d9880fbaaa5f00977f?pvs=4</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742121849541329321#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742121849541329321#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 09:53:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI资讯日报，1月2日：<a href="https://gorden-sun.notion.site/1-2-AI-e9b709afdf4e46d9880fbaaa5f00977f?pvs=4">gorden-sun.notion.site/1-2-A…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MxQ2xBTGFzQUFjVFZzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1742092294948594002#m</id>
            <title>GPTs从简单到复杂可以划分为4种：
1）只通过文字限定角色和功能的GPTs
2）在1的基础上，附加知识库
3）调用标准OpenAPI的GPTs
4）调用自定义API的GPTs
GPTs的潜力非常大，用第4种GPTs甚至可以实现独立的小应用。
这篇教程完整介绍了每种GPTs的创建流程：https://mojju.com/blog/how-to-create-complex-gpts-with-api-actions-and-a-node-js-backend</title>
            <link>https://nitter.cz/Gorden_Sun/status/1742092294948594002#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1742092294948594002#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jan 2024 07:55:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs从简单到复杂可以划分为4种：<br />
1）只通过文字限定角色和功能的GPTs<br />
2）在1的基础上，附加知识库<br />
3）调用标准OpenAPI的GPTs<br />
4）调用自定义API的GPTs<br />
GPTs的潜力非常大，用第4种GPTs甚至可以实现独立的小应用。<br />
这篇教程完整介绍了每种GPTs的创建流程：<a href="https://mojju.com/blog/how-to-create-complex-gpts-with-api-actions-and-a-node-js-backend">mojju.com/blog/how-to-create…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MwbmlVQ2JvQUFTZ1FOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>