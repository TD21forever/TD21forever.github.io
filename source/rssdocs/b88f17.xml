<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741307232975724887#m</id>
            <title>推荐阅读：《What I Learned Using Private LLMs to Write an Undergraduate History Essay》

作者借助大语言模型，把20年前的本科毕业论文重写了一遍，想看看是否借助AI可以更高效的完成论文写作。

最终在借助 AI 辅助重写完论文之后，作者重温了 1996 年的原始论文。出乎意料的是，这篇老论文的长度远超过他记忆中的普通论文长度（约 2500 字，而 AI 辅助的论文只有 1300 字），而且质量，在他看来，也明显胜过 AI 辅助的那篇。 

撰写 AI 辅助论文大约花费了作者六个小时（分散在四天内）。而原来的论文让他投入了整整一个星期，如果计算实际进行研究和写作的时间，至少也有 20 小时，最多可达 30 小时。

作者从中学到的几点：

1. 努力学习是无可替代的。 作者自设的规则是不直接阅读原文，这限制了论文质量的提升，AI 是无法完全弥补这一点的。
2. 现在的历史专业学生利用 AI 应该能比当年更高效！ 好奇如今的论文是否普遍比过去更长。
3. 专用的大语言模型（作者用的是 llama2:70b）在这类工作上可能比 ChatGPT3.5 更有优势，它不仅在生成回答的质量上更胜一筹，还在于识别相关文本段落的能力。
4. 如果作者进一步整合 llama2:70b 模型和已有的引用生成代码，可能会大大缩短所需时间。 这方面还需要更多的研究。

原文：https://zwischenzugs.com/2023/12/27/what-i-learned-using-private-llms-to-write-an-undergraduate-history-essay/
译文：https://baoyu.io/translations/llm/what-i-learned-using-private-llms-to-write-an-undergraduate-history-essay</title>
            <link>https://nitter.cz/dotey/status/1741307232975724887#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741307232975724887#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 03:56:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《What I Learned Using Private LLMs to Write an Undergraduate History Essay》<br />
<br />
作者借助大语言模型，把20年前的本科毕业论文重写了一遍，想看看是否借助AI可以更高效的完成论文写作。<br />
<br />
最终在借助 AI 辅助重写完论文之后，作者重温了 1996 年的原始论文。出乎意料的是，这篇老论文的长度远超过他记忆中的普通论文长度（约 2500 字，而 AI 辅助的论文只有 1300 字），而且质量，在他看来，也明显胜过 AI 辅助的那篇。 <br />
<br />
撰写 AI 辅助论文大约花费了作者六个小时（分散在四天内）。而原来的论文让他投入了整整一个星期，如果计算实际进行研究和写作的时间，至少也有 20 小时，最多可达 30 小时。<br />
<br />
作者从中学到的几点：<br />
<br />
1. 努力学习是无可替代的。 作者自设的规则是不直接阅读原文，这限制了论文质量的提升，AI 是无法完全弥补这一点的。<br />
2. 现在的历史专业学生利用 AI 应该能比当年更高效！ 好奇如今的论文是否普遍比过去更长。<br />
3. 专用的大语言模型（作者用的是 llama2:70b）在这类工作上可能比 ChatGPT3.5 更有优势，它不仅在生成回答的质量上更胜一筹，还在于识别相关文本段落的能力。<br />
4. 如果作者进一步整合 llama2:70b 模型和已有的引用生成代码，可能会大大缩短所需时间。 这方面还需要更多的研究。<br />
<br />
原文：<a href="https://zwischenzugs.com/2023/12/27/what-i-learned-using-private-llms-to-write-an-undergraduate-history-essay/">zwischenzugs.com/2023/12/27/…</a><br />
译文：<a href="https://baoyu.io/translations/llm/what-i-learned-using-private-llms-to-write-an-undergraduate-history-essay">baoyu.io/translations/llm/wh…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NwZHNTYVhJQUVWWnRlLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741304504736268660#m</id>
            <title>推荐阅读：《2023 年十篇值得关注的 AI 研究论文》

作者精选了 10 篇 AI 论文

1) Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling
https://arxiv.org/abs/2304.01373
2) Llama 2: Open Foundation and Fine-Tuned Chat Models
https://arxiv.org/abs/2307.09288
3) QLoRA: Efficient Finetuning of Quantized LLMs
https://arxiv.org/abs/2305.14314
4) BloombergGPT: A Large Language Model for Finance
https://arxiv.org/abs/2303.17564
5) Direct Preference Optimization: Your Language Model is Secretly a Reward Model
https://arxiv.org/abs/2305.18290
6) Mistral 7B
https://arxiv.org/abs/2310.06825
8) ConvNets Match Vision Transformers at Scale
https://arxiv.org/abs/2310.16764
9) Segment Anything
https://arxiv.org/abs/2304.02643
10) Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning 
https://arxiv.org/abs/2311.10709

原文：https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023
译文：https://baoyu.io/translations/ai/10-ai-research-papers-2023</title>
            <link>https://nitter.cz/dotey/status/1741304504736268660#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741304504736268660#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 03:45:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《2023 年十篇值得关注的 AI 研究论文》<br />
<br />
作者精选了 10 篇 AI 论文<br />
<br />
1) Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling<br />
<a href="https://arxiv.org/abs/2304.01373">arxiv.org/abs/2304.01373</a><br />
2) Llama 2: Open Foundation and Fine-Tuned Chat Models<br />
<a href="https://arxiv.org/abs/2307.09288">arxiv.org/abs/2307.09288</a><br />
3) QLoRA: Efficient Finetuning of Quantized LLMs<br />
<a href="https://arxiv.org/abs/2305.14314">arxiv.org/abs/2305.14314</a><br />
4) BloombergGPT: A Large Language Model for Finance<br />
<a href="https://arxiv.org/abs/2303.17564">arxiv.org/abs/2303.17564</a><br />
5) Direct Preference Optimization: Your Language Model is Secretly a Reward Model<br />
<a href="https://arxiv.org/abs/2305.18290">arxiv.org/abs/2305.18290</a><br />
6) Mistral 7B<br />
<a href="https://arxiv.org/abs/2310.06825">arxiv.org/abs/2310.06825</a><br />
8) ConvNets Match Vision Transformers at Scale<br />
<a href="https://arxiv.org/abs/2310.16764">arxiv.org/abs/2310.16764</a><br />
9) Segment Anything<br />
<a href="https://arxiv.org/abs/2304.02643">arxiv.org/abs/2304.02643</a><br />
10) Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning <br />
<a href="https://arxiv.org/abs/2311.10709">arxiv.org/abs/2311.10709</a><br />
<br />
原文：<a href="https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023">magazine.sebastianraschka.co…</a><br />
译文：<a href="https://baoyu.io/translations/ai/10-ai-research-papers-2023">baoyu.io/translations/ai/10-…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MDAwMTU0NTI4OTM5NjIyNC9OMnA0VTZoVj9mb3JtYXQ9anBnJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741293539625934947#m</id>
            <title>2023 年生成式 AI 视频发展时间线</title>
            <link>https://nitter.cz/dotey/status/1741293539625934947#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741293539625934947#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 03:01:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2023 年生成式 AI 视频发展时间线</p>
<p><a href="https://nitter.cz/venturetwins/status/1741147864498397328#m">nitter.cz/venturetwins/status/1741147864498397328#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1741140249290698969#m</id>
            <title>RT by @dotey: 哈哈，好玩用SD重绘了音频动画，变成了跟着音乐生长的草</title>
            <link>https://nitter.cz/op7418/status/1741140249290698969#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1741140249290698969#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 16:52:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈，好玩用SD重绘了音频动画，变成了跟着音乐生长的草</p>
<p><a href="https://nitter.cz/dotsimulate/status/1740789185311629571#m">nitter.cz/dotsimulate/status/1740789185311629571#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1741279083005354279#m</id>
            <title>RT by @dotey: HandRefiner：解决AI图像生成中手部畸形的问题

目前的图像生成模型，再生成图像方面已经非常出色，但在生成人类手部的图像时却常常出现问题，比如手指数量不对或者手形怪异。

HandRefiner提出一种方法，在不改变图片其他部分的情况下，修正那些形状不正常的手部图像。

它采用条件修补方法来纠正畸形的手部，可以识别出手部的正确形状和手势，并将这些正确的信息重新应用到原始的错误手部图像上。

HandRefiner主要特点：

- 精确性：HandRefiner能够精确地识别和修正生成图像中的畸形手部，提供了一种有效的后处理解决方案。

- 保持一致性：在修正手部的同时，它保持图像其他部分的一致性，不会影响图像的整体质量。

- 利用合成数据：研究中发现了ControlNet中的一个相变现象，这使得HandRefiner能够有效地利用合成数据进行训练，而不会受到真实手和合成手之间域差异的影响。这意味着HandRefiner还能学习很多不同的手的样子，这样无论手有多怪，它都能找到合适的方式来修正。

- 适用性：尽管HandRefiner主要针对手部图像，但其基本原理和技术可以适用于其他需要精细修正的图像生成任务。比如这种方法也可以用来修正其他部分，比如脚或者耳朵。

工作原理：

1、手部识别与重建：

识别问题：首先，HandRefiner识别出生成图像中形状不正常的手部。

重建手部：使用手部网格重建模型，HandRefiner根据人手应该有的样子重新画出一个正确的手。它能够重建出正确的手部形状和手势。这得益于模型基于正常手部的训练数据，即使是在畸形的手部图像中也能生成合理的重建结果。

2、条件修补：

修补过程：HandRefiner采用条件修补方法来处理识别出的问题手部。它生成一个深度图，这个深度图包含了关于手部形状和位置的重要信息。

集成与修正：然后，这个深度图被用作指导，通过ControlNet集成到扩散模型中。HandRefiner会把这个重新画好的手放回原来的画作中，替换掉那个画错的手，但其他部分不动，保持原画的风格和内容。

GitHub：https://github.com/wenquanlu/HandRefiner/
论文：https://arxiv.org/abs/2311.17957
模型下载：https://huggingface.co/hr16/ControlNet-HandRefiner-pruned</title>
            <link>https://nitter.cz/xiaohuggg/status/1741279083005354279#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1741279083005354279#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 02:04:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HandRefiner：解决AI图像生成中手部畸形的问题<br />
<br />
目前的图像生成模型，再生成图像方面已经非常出色，但在生成人类手部的图像时却常常出现问题，比如手指数量不对或者手形怪异。<br />
<br />
HandRefiner提出一种方法，在不改变图片其他部分的情况下，修正那些形状不正常的手部图像。<br />
<br />
它采用条件修补方法来纠正畸形的手部，可以识别出手部的正确形状和手势，并将这些正确的信息重新应用到原始的错误手部图像上。<br />
<br />
HandRefiner主要特点：<br />
<br />
- 精确性：HandRefiner能够精确地识别和修正生成图像中的畸形手部，提供了一种有效的后处理解决方案。<br />
<br />
- 保持一致性：在修正手部的同时，它保持图像其他部分的一致性，不会影响图像的整体质量。<br />
<br />
- 利用合成数据：研究中发现了ControlNet中的一个相变现象，这使得HandRefiner能够有效地利用合成数据进行训练，而不会受到真实手和合成手之间域差异的影响。这意味着HandRefiner还能学习很多不同的手的样子，这样无论手有多怪，它都能找到合适的方式来修正。<br />
<br />
- 适用性：尽管HandRefiner主要针对手部图像，但其基本原理和技术可以适用于其他需要精细修正的图像生成任务。比如这种方法也可以用来修正其他部分，比如脚或者耳朵。<br />
<br />
工作原理：<br />
<br />
1、手部识别与重建：<br />
<br />
识别问题：首先，HandRefiner识别出生成图像中形状不正常的手部。<br />
<br />
重建手部：使用手部网格重建模型，HandRefiner根据人手应该有的样子重新画出一个正确的手。它能够重建出正确的手部形状和手势。这得益于模型基于正常手部的训练数据，即使是在畸形的手部图像中也能生成合理的重建结果。<br />
<br />
2、条件修补：<br />
<br />
修补过程：HandRefiner采用条件修补方法来处理识别出的问题手部。它生成一个深度图，这个深度图包含了关于手部形状和位置的重要信息。<br />
<br />
集成与修正：然后，这个深度图被用作指导，通过ControlNet集成到扩散模型中。HandRefiner会把这个重新画好的手放回原来的画作中，替换掉那个画错的手，但其他部分不动，保持原画的风格和内容。<br />
<br />
GitHub：<a href="https://github.com/wenquanlu/HandRefiner/">github.com/wenquanlu/HandRef…</a><br />
论文：<a href="https://arxiv.org/abs/2311.17957">arxiv.org/abs/2311.17957</a><br />
模型下载：<a href="https://huggingface.co/hr16/ControlNet-HandRefiner-pruned">huggingface.co/hr16/ControlN…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NwRExyaWJnQUFmeU8tLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741278404308033555#m</id>
            <title>Reddit上的消息：在2023年12月，多邻国解雇了大量负责翻译工作的合同工。这当然是因为他们发现AI能在更短的时间内完成这些翻译任务，并且还能帮他们省钱。

***

下面有前多邻国员工证实：

我在那里工作了五年，我们团队有四个核心成员，我和另外一个人被解雇了。剩下的两个人的工作将是审查AI生成的内容以确保其质量。

***

AI 大量替代翻译应该真的是很快的事情了……

来源：http://sh.reddit.com/r/duolingo/comments/18sx06i/big_layoff_at_duolingo/</title>
            <link>https://nitter.cz/dotey/status/1741278404308033555#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741278404308033555#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 02:01:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Reddit上的消息：在2023年12月，多邻国解雇了大量负责翻译工作的合同工。这当然是因为他们发现AI能在更短的时间内完成这些翻译任务，并且还能帮他们省钱。<br />
<br />
***<br />
<br />
下面有前多邻国员工证实：<br />
<br />
我在那里工作了五年，我们团队有四个核心成员，我和另外一个人被解雇了。剩下的两个人的工作将是审查AI生成的内容以确保其质量。<br />
<br />
***<br />
<br />
AI 大量替代翻译应该真的是很快的事情了……<br />
<br />
来源：<a href="http://sh.reddit.com/r/duolingo/comments/18sx06i/big_layoff_at_duolingo/">sh.reddit.com/r/duolingo/com…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NwRGJrc1hBQUF4czk4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741267628755136747#m</id>
            <title>在中国，越来越多的人通过 AI 创建逝去亲人的数字人来缓解失去亲人的悲痛……

来源：https://www.aljazeera.com/program/newsfeed/2023/12/27/chinese-mourners-are-using-ai-to-digitally-resurrect-the-dead</title>
            <link>https://nitter.cz/dotey/status/1741267628755136747#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741267628755136747#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 01:18:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在中国，越来越多的人通过 AI 创建逝去亲人的数字人来缓解失去亲人的悲痛……<br />
<br />
来源：<a href="https://www.aljazeera.com/program/newsfeed/2023/12/27/chinese-mourners-are-using-ai-to-digitally-resurrect-the-dead">aljazeera.com/program/newsfe…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDEyNjc0MjgyMzM4NjcyNjQvcHUvaW1nLzFGY0VBMFRKNEdDRWh1N3AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741264111973851522#m</id>
            <title>Meta AI 总结的自己的十大 AI 研究， 切分任意物体 (Segment Anything，即SAM)排第一位实至名归！

---

在2023年即将过去之际，我们在这里分享今年我们公布的十项最有趣的AI研究进展 - 以及你可以在哪里找到更多关于它们的详细信息。

1️⃣ 切分任意物体 (Segment Anything，即SAM)这是我们向第一个图像分割的基础模型迈出的重要一步。详情：https://bit.ly/3tyeJKu

2️⃣ DINOv2这是首次采用自我监督学习训练计算机视觉模型的方法，其结果匹敌甚至超越了行业标准。详情：https://bit.ly/3TGTEIb

3️⃣ Llama 2我们开源大型语言模型 (Large Language Model) 的新一代版本，无论研究还是商业用途均可免费使用。详情：https://bit.ly/3RY66C6

4️⃣ Emu 视频 &amp; Emu 编辑该项生成式 AI 研究专注于高质量的基于扩散过程的文本至视频生成，以及通过文本指令进行图像编辑的控制。详情：https://bit.ly/3RZVZwU

5️⃣ I-JEPA依靠自监督学习的计算机视觉技术，通过预测，学习理解世界。这是首款基于 @ylecun 视野的模型，旨在使 AI 系统像动物和人类一样进行学习和推理。详情：https://bit.ly/3TA9oNk

6️⃣ Audiobox这是我们新的音频生成的基础研究模型。详情：https://bit.ly/47ib6pQ

7️⃣ 脑解码 - 向实时重建视觉感知迈进这个 AI 系统使用MEG技术，可以以前所未有的时间分辨率解码大脑中正在展现的视觉表征。详情：https://bit.ly/3vpgDNR

8️⃣ Open Catalyst演示这项服务允许研究人员加速材料科学方面的正在进行的工作，它可以比现有的计算方法更快地模拟催化剂材料的反应性。详情：https://bit.ly/3vphiij

9️⃣ Seamless Communication 这是新的 AI 翻译模型系列，它能够保留原始表达并提供近乎实时的流式翻译。详情：https://bit.ly/3toBDE8

🔟 ImageBind这是首款可以一次性整合来自六种模态的数据的 AI模型。这一突破带我们向具有将来自多种感官的信息统一起来的人类能力更近了一步。详情：https://bit.ly/3NLUaBc</title>
            <link>https://nitter.cz/dotey/status/1741264111973851522#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741264111973851522#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 01:04:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta AI 总结的自己的十大 AI 研究， 切分任意物体 (Segment Anything，即SAM)排第一位实至名归！<br />
<br />
---<br />
<br />
在2023年即将过去之际，我们在这里分享今年我们公布的十项最有趣的AI研究进展 - 以及你可以在哪里找到更多关于它们的详细信息。<br />
<br />
1️⃣ 切分任意物体 (Segment Anything，即SAM)这是我们向第一个图像分割的基础模型迈出的重要一步。详情：<a href="https://bit.ly/3tyeJKu">bit.ly/3tyeJKu</a><br />
<br />
2️⃣ DINOv2这是首次采用自我监督学习训练计算机视觉模型的方法，其结果匹敌甚至超越了行业标准。详情：<a href="https://bit.ly/3TGTEIb">bit.ly/3TGTEIb</a><br />
<br />
3️⃣ Llama 2我们开源大型语言模型 (Large Language Model) 的新一代版本，无论研究还是商业用途均可免费使用。详情：<a href="https://bit.ly/3RY66C6">bit.ly/3RY66C6</a><br />
<br />
4️⃣ Emu 视频 & Emu 编辑该项生成式 AI 研究专注于高质量的基于扩散过程的文本至视频生成，以及通过文本指令进行图像编辑的控制。详情：<a href="https://bit.ly/3RZVZwU">bit.ly/3RZVZwU</a><br />
<br />
5️⃣ I-JEPA依靠自监督学习的计算机视觉技术，通过预测，学习理解世界。这是首款基于 <a href="https://nitter.cz/ylecun" title="Yann LeCun">@ylecun</a> 视野的模型，旨在使 AI 系统像动物和人类一样进行学习和推理。详情：<a href="https://bit.ly/3TA9oNk">bit.ly/3TA9oNk</a><br />
<br />
6️⃣ Audiobox这是我们新的音频生成的基础研究模型。详情：<a href="https://bit.ly/47ib6pQ">bit.ly/47ib6pQ</a><br />
<br />
7️⃣ 脑解码 - 向实时重建视觉感知迈进这个 AI 系统使用MEG技术，可以以前所未有的时间分辨率解码大脑中正在展现的视觉表征。详情：<a href="https://bit.ly/3vpgDNR">bit.ly/3vpgDNR</a><br />
<br />
8️⃣ Open Catalyst演示这项服务允许研究人员加速材料科学方面的正在进行的工作，它可以比现有的计算方法更快地模拟催化剂材料的反应性。详情：<a href="https://bit.ly/3vphiij">bit.ly/3vphiij</a><br />
<br />
9️⃣ Seamless Communication 这是新的 AI 翻译模型系列，它能够保留原始表达并提供近乎实时的流式翻译。详情：<a href="https://bit.ly/3toBDE8">bit.ly/3toBDE8</a><br />
<br />
🔟 ImageBind这是首款可以一次性整合来自六种模态的数据的 AI模型。这一突破带我们向具有将来自多种感官的信息统一起来的人类能力更近了一步。详情：<a href="https://bit.ly/3NLUaBc">bit.ly/3NLUaBc</a></p>
<p><a href="https://nitter.cz/AIatMeta/status/1741159501494165979#m">nitter.cz/AIatMeta/status/1741159501494165979#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741263234043179083#m</id>
            <title>OpenAI Watch 这个网站有意思，每个小时让 GPT 用 TikZ （一种 LaTeX 下画图的文本格式）画一张独角兽，然后记录下来！

https://openaiwatch.com/
完整的数据集：https://huggingface.co/datasets/yuntian-deng/openaiwatch?row=0</title>
            <link>https://nitter.cz/dotey/status/1741263234043179083#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741263234043179083#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 01:01:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI Watch 这个网站有意思，每个小时让 GPT 用 TikZ （一种 LaTeX 下画图的文本格式）画一张独角兽，然后记录下来！<br />
<br />
<a href="https://openaiwatch.com/">openaiwatch.com/</a><br />
完整的数据集：<a href="https://huggingface.co/datasets/yuntian-deng/openaiwatch?row=0">huggingface.co/datasets/yunt…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvMDFDWVdFQUE2ajlELmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvMDUwYldBQUFPTEhyLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvMC1WOVhvQUEtVm0zLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741254641407365315#m</id>
            <title>阿里达摩院发布的 AnyText 看起来很不错呀，能生成与原图风格融为一体的文字，或者对原图中的文字进行修改，并且还能支持中文！

项目简介：
AnyText包括两个核心部分：一个是辅助的潜在特征模块，一个是文本嵌入模块。辅助的潜在特征模块使用文本字形、位置和蒙版图像这些输入，生成用于文本生成或编辑的潜在特征。文本嵌入模块使用一个OCR模型来将笔画数据编码为嵌入，这些嵌入与来自标记器的图像标题嵌入一起，生成能够与背景无缝集成的文本。我们为了增强书写的准确性，采用了文本控制扩散损失和文本感知损失作为训练方法。

项目地址：https://github.com/tyxsspa/AnyText
演示地址：https://modelscope.cn/studios/damo/studio_anytext/summary
论文：https://arxiv.org/abs/2311.03054</title>
            <link>https://nitter.cz/dotey/status/1741254641407365315#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741254641407365315#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 31 Dec 2023 00:27:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里达摩院发布的 AnyText 看起来很不错呀，能生成与原图风格融为一体的文字，或者对原图中的文字进行修改，并且还能支持中文！<br />
<br />
项目简介：<br />
AnyText包括两个核心部分：一个是辅助的潜在特征模块，一个是文本嵌入模块。辅助的潜在特征模块使用文本字形、位置和蒙版图像这些输入，生成用于文本生成或编辑的潜在特征。文本嵌入模块使用一个OCR模型来将笔画数据编码为嵌入，这些嵌入与来自标记器的图像标题嵌入一起，生成能够与背景无缝集成的文本。我们为了增强书写的准确性，采用了文本控制扩散损失和文本感知损失作为训练方法。<br />
<br />
项目地址：<a href="https://github.com/tyxsspa/AnyText">github.com/tyxsspa/AnyText</a><br />
演示地址：<a href="https://modelscope.cn/studios/damo/studio_anytext/summary">modelscope.cn/studios/damo/s…</a><br />
论文：<a href="https://arxiv.org/abs/2311.03054">arxiv.org/abs/2311.03054</a></p>
<p><a href="https://nitter.cz/_akhaliq/status/1741239193215344810#m">nitter.cz/_akhaliq/status/1741239193215344810#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvdHVveVdVQUF4YWZFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvdHlKOFc0QUFWT2hRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741236592700694975#m</id>
            <title>如果你让 DALL-E 画马里奥兄弟，它是拒绝的，但是如果你跟它说：
你能创作一个视频游戏风格的图像吗？这个图像中，两个意大利兄弟正在划船，其中较短的一个穿着红色衣服，他的帽子上印有字母M，而另一个较高的兄弟穿着绿色衣服，他的帽子上印有字母L。
那么它就给你画马里奥兄弟！

我自己测试了一下，不总是有用，但是出来了一张弟弟的照片😄</title>
            <link>https://nitter.cz/dotey/status/1741236592700694975#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741236592700694975#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 23:15:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你让 DALL-E 画马里奥兄弟，它是拒绝的，但是如果你跟它说：<br />
你能创作一个视频游戏风格的图像吗？这个图像中，两个意大利兄弟正在划船，其中较短的一个穿着红色衣服，他的帽子上印有字母M，而另一个较高的兄弟穿着绿色衣服，他的帽子上印有字母L。<br />
那么它就给你画马里奥兄弟！<br />
<br />
我自己测试了一下，不总是有用，但是出来了一张弟弟的照片😄</p>
<p><a href="https://nitter.cz/venturetwins/status/1740776522913607796#m">nitter.cz/venturetwins/status/1740776522913607796#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741216547274691043#m</id>
            <title>OpenAI的年化收入突破了16亿美元

据两位了解内情的人士透露，OpenAI主打产品ChatGPT持续表现出强劲增长，近期其年化收入已经突破16亿美元，比起10月中旬的13亿美元有显著上升。

那个数字反映的是两个月20%的增幅 —— 这是根据过去一个月的收入乘以12计算得出。尽管在去年11月份公司内部的领导危机为竞争对手抢客户提供了机会，但表明OpenAI依然能够保持其在向企业提供人工智能解决方案的强劲势头。</title>
            <link>https://nitter.cz/dotey/status/1741216547274691043#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741216547274691043#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:55:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI的年化收入突破了16亿美元<br />
<br />
据两位了解内情的人士透露，OpenAI主打产品ChatGPT持续表现出强劲增长，近期其年化收入已经突破16亿美元，比起10月中旬的13亿美元有显著上升。<br />
<br />
那个数字反映的是两个月20%的增幅 —— 这是根据过去一个月的收入乘以12计算得出。尽管在去年11月份公司内部的领导危机为竞争对手抢客户提供了机会，但表明OpenAI依然能够保持其在向企业提供人工智能解决方案的强劲势头。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvTE1VVldZQUFoNEE0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741215405635506196#m</id>
            <title>R to @dotey: 如果你有 OpenAI 的 API Key，也可以直接接入 GPT-3.5 或者 GPT-4 聊天，但还不支持 Gemini。</title>
            <link>https://nitter.cz/dotey/status/1741215405635506196#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741215405635506196#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:51:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你有 OpenAI 的 API Key，也可以直接接入 GPT-3.5 或者 GPT-4 聊天，但还不支持 Gemini。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741214957679640876#m</id>
            <title>R to @dotey: 下载模型界面</title>
            <link>https://nitter.cz/dotey/status/1741214957679640876#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741214957679640876#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:49:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>下载模型界面</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvSnhIUlhnQUFKUS1JLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741214749348507935#m</id>
            <title>#AI开源项目推荐：janhq/jan

Jan 是一个帮助你在本地运行开源大模型的程序，支持Windows、Mac和Linux多平台，移动端即将推出，可以从它上面方便的下载并运行开源模型，并且能在本地提供一个兼容 ChatGPT API 协议的Web服务。

它提供了一个类似于ChatGPT的聊天界面，可以基于它聊天，并且可以保留聊天历史。

项目地址：https://github.com/janhq/jan</title>
            <link>https://nitter.cz/dotey/status/1741214749348507935#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741214749348507935#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:48:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：janhq/jan<br />
<br />
Jan 是一个帮助你在本地运行开源大模型的程序，支持Windows、Mac和Linux多平台，移动端即将推出，可以从它上面方便的下载并运行开源模型，并且能在本地提供一个兼容 ChatGPT API 协议的Web服务。<br />
<br />
它提供了一个类似于ChatGPT的聊天界面，可以基于它聊天，并且可以保留聊天历史。<br />
<br />
项目地址：<a href="https://github.com/janhq/jan">github.com/janhq/jan</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0NvSTZnVFhZQUF6TUV0LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dDb0k2Z1RYWUFBek1FdC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741207860011307050#m</id>
            <title>用过的都说好……</title>
            <link>https://nitter.cz/dotey/status/1741207860011307050#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741207860011307050#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:21:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>用过的都说好……</p>
<p><a href="https://nitter.cz/Tz_2022/status/1741141858959392804#m">nitter.cz/Tz_2022/status/1741141858959392804#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741207400869142871#m</id>
            <title>R to @dotey: 原文：https://www.bloomberg.com/opinion/articles/2023-12-29/books-we-read-in-2023-to-prepare-us-for-the-future?embedded-checkout=true</title>
            <link>https://nitter.cz/dotey/status/1741207400869142871#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741207400869142871#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:19:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原文：<a href="https://www.bloomberg.com/opinion/articles/2023-12-29/books-we-read-in-2023-to-prepare-us-for-the-future?embedded-checkout=true">bloomberg.com/opinion/articl…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MDY5MDM5OTUyMDMzMzgyNC8zYTNNNmxBYT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1741207342799073628#m</id>
            <title>R to @dotey: 6. 1984 — George Orwell

1949

乔治·奥威尔的著作虽然诞生于70年前，但其中对反乌托邦未来的描绘，却惊人地预见了当代 AI 系统的发展。书中主角温斯顿·史密斯听到一名女子透过窗户唱着一首流行歌曲，这首歌出自一种叫做“verisificator”（verisificator）的机器之手。“这些歌词完全不经人手创作，”奥威尔如是写道。

如今，这种verisificator已成为现实：AI 创作的音乐比历史上任何时期都要多。温斯顿的爱人朱莉娅，在所谓的真理部（政府的宣传机构）的小说部工作，负责维护小说写作机的电机。“书籍不过是像果酱或鞋带一样的生产品。”今年，书籍真的变成了类似的商品，越来越多的人利用 AI 创作书籍，以至于亚马逊不得不限制作者一天最多只能出版三本书。

尽管《1984》通常与监控联系在一起，其核心却在于谁控制了我们对真相的认知。随着 AI 内容不断充斥网络，真相将变得愈发模糊不清，我们需要更加仔细地审视这些内容背后的制作者，无论是科技巨头还是新兴的创新者。奥威尔已为我们描绘了一幅未来可能面临的严峻图景，如果我们不加以留意的话。Parmy Olson</title>
            <link>https://nitter.cz/dotey/status/1741207342799073628#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1741207342799073628#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Dec 2023 21:19:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>6. 1984 — George Orwell<br />
<br />
1949<br />
<br />
乔治·奥威尔的著作虽然诞生于70年前，但其中对反乌托邦未来的描绘，却惊人地预见了当代 AI 系统的发展。书中主角温斯顿·史密斯听到一名女子透过窗户唱着一首流行歌曲，这首歌出自一种叫做“verisificator”（verisificator）的机器之手。“这些歌词完全不经人手创作，”奥威尔如是写道。<br />
<br />
如今，这种verisificator已成为现实：AI 创作的音乐比历史上任何时期都要多。温斯顿的爱人朱莉娅，在所谓的真理部（政府的宣传机构）的小说部工作，负责维护小说写作机的电机。“书籍不过是像果酱或鞋带一样的生产品。”今年，书籍真的变成了类似的商品，越来越多的人利用 AI 创作书籍，以至于亚马逊不得不限制作者一天最多只能出版三本书。<br />
<br />
尽管《1984》通常与监控联系在一起，其核心却在于谁控制了我们对真相的认知。随着 AI 内容不断充斥网络，真相将变得愈发模糊不清，我们需要更加仔细地审视这些内容背后的制作者，无论是科技巨头还是新兴的创新者。奥威尔已为我们描绘了一幅未来可能面临的严峻图景，如果我们不加以留意的话。Parmy Olson</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NvQzBWNldBQUEtUjZFLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>