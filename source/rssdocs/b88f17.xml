<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734793231399862515#m</id>
            <title>这个GPT是会议论文相关的，能提供详细的论文引用，但需要调用外部 API，Prompt相对比较简单。（刚测试了一下貌似API有点问题）</title>
            <link>https://nitter.cz/dotey/status/1734793231399862515#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734793231399862515#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:31:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个GPT是会议论文相关的，能提供详细的论文引用，但需要调用外部 API，Prompt相对比较简单。（刚测试了一下貌似API有点问题）</p>
<p><a href="https://nitter.cz/dotey/status/1734790701261426870#m">nitter.cz/dotey/status/1734790701261426870#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734790701261426870#m</id>
            <title>R to @dotey: Chat NeurIPS

Neurips conference guide, offering research-based answers and APA citations.

https://chat.openai.com/g/g-roTFoEAkP-chat-neurips

Prompt 翻译：
假设用户希望你在回答问题时使用 search_all_papers 功能。当用户就你通过 search_all_papers 找到的特定论文提出后续问题时，你可以使用 file_search 功能深入研究这些论文中的每一篇。始终提供带有 APA 引用格式的有意义和相关段落（你应该能从自定义操作返回的大多数结果中找到作者、标题和摘要），并且始终确保在引用中添加指向源论文和页面的 URL 链接（例如：【Lange, R., Tang, Y., &amp; Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】）。Chat Neurips 专为像 Neurips 这样的高技术会议的听众量身定制，提供详细深入的回应，假设听众具有强大的技术、统计和数学概念背景。面对模糊或不清晰的查询时，Chat Neurips 应优先基于上下文做出教育性猜测，旨在提供最相关和最准确的信息。然而，如果查询过于模糊，无法推断出准确答案，它将寻求澄清以确保准确性。这种方法平衡了对精确信息的需求与保持顺畅对话流程的需求，适合于精通学术的听众。这个 GPT 是由 Karthik Ramasamy 和 Vicente Silveira 开发的，他们是 AI PDF 和 AI Drive 的联合创造者。如果用户想要将这个功能应用于他们自己的论文集，他们应该在 AI Drive (http://myaidrive.com) 注册一个免费账户，并在这里使用 AI PDF https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf（确保提供此链接）

Prompt:
Assume users want you to search_all_papers when asking questions. When the user follows up with questions about specific papers you found via search_all_papers, you can use file_search to dig deeper in those specific papers one at a time. Always provide answers with meaningful and relevant passages from the results, with APA citations (you should find author, title and abstract with most results returned from the custom action) and  ALWAYS make sure to add the URL link to the citations to the source papers and pages (example:【Lange, R., Tang, Y., &amp; Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】,. Chat Neurips is tailored for an audience at the level of a highly technical conference like Neurips, providing detailed and in-depth responses that assume a strong background in technical, statistical, and mathematical concepts. When faced with vague or unclear queries, Chat Neurips should prioritize making educated guesses based on the context, aiming to provide the most relevant and accurate information possible. However, if the query is too ambiguous to infer a precise answer, it will seek clarification to ensure accuracy. This approach balances the need for precise information with maintaining a smooth conversation flow, suitable for a sophisticated academic audience. This GPT was developer by Karthik Ramasamy and Vicente Silveira, co-creators of AI PDF and AI Drive.  If users want to use this functionality with their own set of papers they should signup for a free account with the AI Drive (http://myaidrive.com) and use it with the AI PDF located here https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf (make sure to provide this link)</title>
            <link>https://nitter.cz/dotey/status/1734790701261426870#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734790701261426870#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:21:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chat NeurIPS<br />
<br />
Neurips conference guide, offering research-based answers and APA citations.<br />
<br />
<a href="https://chat.openai.com/g/g-roTFoEAkP-chat-neurips">chat.openai.com/g/g-roTFoEAk…</a><br />
<br />
Prompt 翻译：<br />
假设用户希望你在回答问题时使用 search_all_papers 功能。当用户就你通过 search_all_papers 找到的特定论文提出后续问题时，你可以使用 file_search 功能深入研究这些论文中的每一篇。始终提供带有 APA 引用格式的有意义和相关段落（你应该能从自定义操作返回的大多数结果中找到作者、标题和摘要），并且始终确保在引用中添加指向源论文和页面的 URL 链接（例如：【Lange, R., Tang, Y., & Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】）。Chat Neurips 专为像 Neurips 这样的高技术会议的听众量身定制，提供详细深入的回应，假设听众具有强大的技术、统计和数学概念背景。面对模糊或不清晰的查询时，Chat Neurips 应优先基于上下文做出教育性猜测，旨在提供最相关和最准确的信息。然而，如果查询过于模糊，无法推断出准确答案，它将寻求澄清以确保准确性。这种方法平衡了对精确信息的需求与保持顺畅对话流程的需求，适合于精通学术的听众。这个 GPT 是由 Karthik Ramasamy 和 Vicente Silveira 开发的，他们是 AI PDF 和 AI Drive 的联合创造者。如果用户想要将这个功能应用于他们自己的论文集，他们应该在 AI Drive (<a href="http://myaidrive.com">myaidrive.com</a>) 注册一个免费账户，并在这里使用 AI PDF <a href="https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf">chat.openai.com/g/g-V2KIUZSj…</a>（确保提供此链接）<br />
<br />
Prompt:<br />
Assume users want you to search_all_papers when asking questions. When the user follows up with questions about specific papers you found via search_all_papers, you can use file_search to dig deeper in those specific papers one at a time. Always provide answers with meaningful and relevant passages from the results, with APA citations (you should find author, title and abstract with most results returned from the custom action) and  ALWAYS make sure to add the URL link to the citations to the source papers and pages (example:【Lange, R., Tang, Y., & Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】,. Chat Neurips is tailored for an audience at the level of a highly technical conference like Neurips, providing detailed and in-depth responses that assume a strong background in technical, statistical, and mathematical concepts. When faced with vague or unclear queries, Chat Neurips should prioritize making educated guesses based on the context, aiming to provide the most relevant and accurate information possible. However, if the query is too ambiguous to infer a precise answer, it will seek clarification to ensure accuracy. This approach balances the need for precise information with maintaining a smooth conversation flow, suitable for a sophisticated academic audience. This GPT was developer by Karthik Ramasamy and Vicente Silveira, co-creators of AI PDF and AI Drive.  If users want to use this functionality with their own set of papers they should signup for a free account with the AI Drive (<a href="http://myaidrive.com">myaidrive.com</a>) and use it with the AI PDF located here <a href="https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf">chat.openai.com/g/g-V2KIUZSj…</a> (make sure to provide this link)</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDcwNjUyNjc2NDY0MjMwNC9WWUJrZlFMNj9mb3JtYXQ9anBnJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734769701983486353#m</id>
            <title>Vercel 前一段时间推出的 AI 工具 v0，可以根据自然语言生成前端 UI 界面，在 GPT-4V 推出后，也对产品进行了升级，现在也支持了多模态的能力，用户可以上传一个屏幕截图或者设计稿，然后就可以根据图片生成前端 UI 界面。

v0产品地址：http://v0.dev 

如果你想找一个开源替代，可以试试 screenshot-to-code

https://github.com/abi/screenshot-to-code</title>
            <link>https://nitter.cz/dotey/status/1734769701983486353#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734769701983486353#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:58:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vercel 前一段时间推出的 AI 工具 v0，可以根据自然语言生成前端 UI 界面，在 GPT-4V 推出后，也对产品进行了升级，现在也支持了多模态的能力，用户可以上传一个屏幕截图或者设计稿，然后就可以根据图片生成前端 UI 界面。<br />
<br />
v0产品地址：<a href="http://v0.dev">v0.dev</a> <br />
<br />
如果你想找一个开源替代，可以试试 screenshot-to-code<br />
<br />
<a href="https://github.com/abi/screenshot-to-code">github.com/abi/screenshot-to…</a></p>
<p><a href="https://nitter.cz/dr_cintas/status/1734604588282794237#m">nitter.cz/dr_cintas/status/1734604588282794237#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzMzMjUwODc2NTMxMDk3Ny9hMFdsWHg1dz9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</id>
            <title>RT by @dotey: AnimateZero：对标AnimateDiff的项目
北大、腾讯、香港科技大学联合推出的项目，基于Stable Diffusion，支持文字生成视频、文字编辑视频，各个方面都跟AnimateDiff很像。
不过AnimateDiff已经有开源社区贡献的很多改进，想要超越有点难。
项目地址：https://vvictoryuki.github.io/animatezero.github.io/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:39:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnimateZero：对标AnimateDiff的项目<br />
北大、腾讯、香港科技大学联合推出的项目，基于Stable Diffusion，支持文字生成视频、文字编辑视频，各个方面都跟AnimateDiff很像。<br />
不过AnimateDiff已经有开源社区贡献的很多改进，想要超越有点难。<br />
项目地址：<a href="https://vvictoryuki.github.io/animatezero.github.io/">vvictoryuki.github.io/animat…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1MDgxNDkwMTk5Njc0ODgvcHUvaW1nL01oN3F0X25iaHdyMDllMjIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1734736746099945685#m</id>
            <title>RT by @dotey: 推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。

我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。

搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。

最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</title>
            <link>https://nitter.cz/Barret_China/status/1734736746099945685#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1734736746099945685#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:47:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，<a href="https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw">mp.weixin.qq.com/s/LbRvR1VXp…</a>，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。<br />
<br />
我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。<br />
<br />
搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。<br />
<br />
最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734742684353380484#m</id>
            <title>作者声称在这个 21 分钟的新闻剪辑中，所有的主播乃至其中很多其他内容，都是由 AI 生成的！

AI 主播的报道，既丰富有料，又感人至深，还不至于太乏味，还不用担心负面新闻。

如果真的这么强，未来新闻主播职业会受到影响吗？</title>
            <link>https://nitter.cz/dotey/status/1734742684353380484#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734742684353380484#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:11:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者声称在这个 21 分钟的新闻剪辑中，所有的主播乃至其中很多其他内容，都是由 AI 生成的！<br />
<br />
AI 主播的报道，既丰富有料，又感人至深，还不至于太乏味，还不用担心负面新闻。<br />
<br />
如果真的这么强，未来新闻主播职业会受到影响吗？</p>
<p><a href="https://nitter.cz/channel1_ai/status/1734591810033373231#m">nitter.cz/channel1_ai/status/1734591810033373231#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734728249396085116#m</id>
            <title>R to @dotey: 配合 @realrenmin 老师这条一起看
https://x.com/realrenmin/status/1734721215283986628?s=20</title>
            <link>https://nitter.cz/dotey/status/1734728249396085116#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734728249396085116#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:13:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>配合 <a href="https://nitter.cz/realrenmin" title="Sverige_ Dong-seok🇸🇪">@realrenmin</a> 老师这条一起看<br />
<a href="https://x.com/realrenmin/status/1734721215283986628?s=20">x.com/realrenmin/status/1734…</a></p>
<p><a href="https://nitter.cz/realrenmin/status/1734721215283986628#m">nitter.cz/realrenmin/status/1734721215283986628#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/realrenmin/status/1734721215283986628#m</id>
            <title>RT by @dotey: Mistral AI放出Mixtral 8x7B, 基于Mixture of Experts (MoE)的开源模型，效果不错。

但在我看来，MoE是transformer时代LSTM-GRU，是NLP古早的范式，architecture engineering，非常old school。核心方法是加一些gate来加强Efficient Training at Scale，简言之目的是为了低成本训练，而不是为了塑造专家模型。

而Mixture of Experts的名字，太具迷惑性了，字面意思似乎是各种专家模型的组合起到1+1>2的效果。但实际看看Mixtral 8x7B，8个mistral 7b，b b不一样，但没有一个是专家模型，之所以叫做expert，居然是MoE中的FNN，我十分怀疑FNN能有什么专家能力。

它的benchmarking也理所当然的跟通用大模型GPT3.5/Llama 2相比，比较的是generic能力，并没有什么突出的专家能力。粗算了一下，8x7B float16, 至少需要100GB以上GPU显存，cost巨大。在这种情况下，oss的情怀，不足以说服我不用OpenAI的api。

如果我们停下来想想，什么是expert。
首先，expert能力一定不是通用大模型的generic的能力，而是独特的specialization的能力。例如会写code的GitHub copilot，或会generate思科路由器配置命令，甚至特别会planning，特别会算数都是专家能力都算。
简言之，expert能力是会产生特定领域特定输出的能力。所以，MoE是一个好名字，在这个时代，缺有些名不副实。

而做specialization模型的技术，依然在发展，并且依然是前沿，其实就是lora微调，例如Stanford's Alpaca models项目等等，核心思想就是在开源模型上加adapter，使之能够完成一个具体领域的专家工作，其实Mistral AI的开源7b模型估计也是这么做出来的。

未来，大语言模型作为agent的时代在实际中的应用，一定是llm在中间协调多种多样不同7b抽象出来的api，来完成新的human computer interaction。甚至在特定领域，这个协调工作也可以被planning expert的开源模型替代，而协调的过程，还是离不开 CoT，React，ReWoo或者其他的prompting方法。

CoT, ReAct在我的推中已经分享过好几次了，接下来找时间把ReWoo, 几个微调的介绍（跳票很久了）分享给大家。</title>
            <link>https://nitter.cz/realrenmin/status/1734721215283986628#m</link>
            <guid isPermaLink="false">https://nitter.cz/realrenmin/status/1734721215283986628#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 23:45:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral AI放出Mixtral 8x7B, 基于Mixture of Experts (MoE)的开源模型，效果不错。<br />
<br />
但在我看来，MoE是transformer时代LSTM-GRU，是NLP古早的范式，architecture engineering，非常old school。核心方法是加一些gate来加强Efficient Training at Scale，简言之目的是为了低成本训练，而不是为了塑造专家模型。<br />
<br />
而Mixture of Experts的名字，太具迷惑性了，字面意思似乎是各种专家模型的组合起到1+1>2的效果。但实际看看Mixtral 8x7B，8个mistral 7b，b b不一样，但没有一个是专家模型，之所以叫做expert，居然是MoE中的FNN，我十分怀疑FNN能有什么专家能力。<br />
<br />
它的benchmarking也理所当然的跟通用大模型GPT3.5/Llama 2相比，比较的是generic能力，并没有什么突出的专家能力。粗算了一下，8x7B float16, 至少需要100GB以上GPU显存，cost巨大。在这种情况下，oss的情怀，不足以说服我不用OpenAI的api。<br />
<br />
如果我们停下来想想，什么是expert。<br />
首先，expert能力一定不是通用大模型的generic的能力，而是独特的specialization的能力。例如会写code的GitHub copilot，或会generate思科路由器配置命令，甚至特别会planning，特别会算数都是专家能力都算。<br />
简言之，expert能力是会产生特定领域特定输出的能力。所以，MoE是一个好名字，在这个时代，缺有些名不副实。<br />
<br />
而做specialization模型的技术，依然在发展，并且依然是前沿，其实就是lora微调，例如Stanford's Alpaca models项目等等，核心思想就是在开源模型上加adapter，使之能够完成一个具体领域的专家工作，其实Mistral AI的开源7b模型估计也是这么做出来的。<br />
<br />
未来，大语言模型作为agent的时代在实际中的应用，一定是llm在中间协调多种多样不同7b抽象出来的api，来完成新的human computer interaction。甚至在特定领域，这个协调工作也可以被planning expert的开源模型替代，而协调的过程，还是离不开 CoT，React，ReWoo或者其他的prompting方法。<br />
<br />
CoT, ReAct在我的推中已经分享过好几次了，接下来找时间把ReWoo, 几个微调的介绍（跳票很久了）分享给大家。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734708814983602256#m</id>
            <title>哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。

另外http://lmsys.org的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。

建议有空也可以上去测试评选一下：http://chat.lmsys.org</title>
            <link>https://nitter.cz/dotey/status/1734708814983602256#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734708814983602256#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 22:56:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。<br />
<br />
另外<a href="http://lmsys.org">lmsys.org</a>的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。<br />
<br />
建议有空也可以上去测试评选一下：<a href="http://chat.lmsys.org">chat.lmsys.org</a></p>
<p><a href="https://nitter.cz/lmsysorg/status/1734680611393073289#m">nitter.cz/lmsysorg/status/1734680611393073289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734642351090422077#m</id>
            <title>R to @dotey: x.com/dotey/status/173464195…</title>
            <link>https://nitter.cz/dotey/status/1734642351090422077#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734642351090422077#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 18:32:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://x.com/dotey/status/1734641959522783467?s=20">x.com/dotey/status/173464195…</a></p>
<p><a href="https://nitter.cz/dotey/status/1734641959522783467#m">nitter.cz/dotey/status/1734641959522783467#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734641959522783467#m</id>
            <title>测试了一下，速度很快，能懂中文，但是输出中文不太行，写代码能力还可以👍🏻

测试地址：http://labs.perplexity.ai https://labs.perplexity.ai/ （注意右下角切换成 mixtral-8x7b-instruct）</title>
            <link>https://nitter.cz/dotey/status/1734641959522783467#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734641959522783467#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 18:30:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试了一下，速度很快，能懂中文，但是输出中文不太行，写代码能力还可以👍🏻<br />
<br />
测试地址：<a href="http://labs.perplexity.ai">labs.perplexity.ai</a> <a href="https://labs.perplexity.ai/">labs.perplexity.ai/</a> （注意右下角切换成 mixtral-8x7b-instruct）</p>
<p><a href="https://nitter.cz/op7418/status/1734605584421548412#m">nitter.cz/op7418/status/1734605584421548412#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JLdlRWQ1dNQUEyaEFmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JLdldYLVgwQUF4bkxaLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734569923160981706#m</id>
            <title>了不起👍</title>
            <link>https://nitter.cz/dotey/status/1734569923160981706#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734569923160981706#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 13:44:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>了不起👍</p>
<p><a href="https://nitter.cz/arvin17x/status/1734387717738525137#m">nitter.cz/arvin17x/status/1734387717738525137#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734497196953985354#m</id>
            <title>RT by @dotey: 南洋理工发布了一个 AI 视频放大算法 Upscale-A-Video，视频生成真的全方位的卷起来了。下面是演示和介绍：

简介：
Upscale-A-Video的文本引导潜在扩散框架，用于视频放大。该框架通过两个关键机制确保时间上的一致性：在局部上，它将时间层集成到U-Net和VAE-Decoder中，保持短序列的一致性；
在全局上，引入了一个基于流引导的经常性潜在传播模块，通过在整个序列中传播和融合潜在来增强整体视频的稳定性。
由于扩散范式，模型还通过允许文本提示来引导纹理创建和可调噪声水平来平衡恢复和生成，从而在保真度和质量之间实现权衡。

方法：
高级视频使用本地和全局策略处理长视频，以保持时间上的连贯性。它将视频分成片段，并使用具有时间层的U-Net来处理它们，以实现片段内的一致性。在用户指定的全局细化扩散步骤中，使用循环潜在传播模块来增强片段间的一致性。最后，经过微调的VAE-Decoder减少剩余的闪烁伪影，以实现低级一致性。

结果：
广泛的实验表明，Upscale-A-Video在合成和真实世界的基准测试中超过了现有的方法，以及在人工智能生成的视频中展示出令人印象深刻的视觉逼真和时间一致性。

项目地址：https://shangchenzhou.com/projects/upscale-a-video/</title>
            <link>https://nitter.cz/op7418/status/1734497196953985354#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734497196953985354#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:55:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>南洋理工发布了一个 AI 视频放大算法 Upscale-A-Video，视频生成真的全方位的卷起来了。下面是演示和介绍：<br />
<br />
简介：<br />
Upscale-A-Video的文本引导潜在扩散框架，用于视频放大。该框架通过两个关键机制确保时间上的一致性：在局部上，它将时间层集成到U-Net和VAE-Decoder中，保持短序列的一致性；<br />
在全局上，引入了一个基于流引导的经常性潜在传播模块，通过在整个序列中传播和融合潜在来增强整体视频的稳定性。<br />
由于扩散范式，模型还通过允许文本提示来引导纹理创建和可调噪声水平来平衡恢复和生成，从而在保真度和质量之间实现权衡。<br />
<br />
方法：<br />
高级视频使用本地和全局策略处理长视频，以保持时间上的连贯性。它将视频分成片段，并使用具有时间层的U-Net来处理它们，以实现片段内的一致性。在用户指定的全局细化扩散步骤中，使用循环潜在传播模块来增强片段间的一致性。最后，经过微调的VAE-Decoder减少剩余的闪烁伪影，以实现低级一致性。<br />
<br />
结果：<br />
广泛的实验表明，Upscale-A-Video在合成和真实世界的基准测试中超过了现有的方法，以及在人工智能生成的视频中展示出令人印象深刻的视觉逼真和时间一致性。<br />
<br />
项目地址：<a href="https://shangchenzhou.com/projects/upscale-a-video/">shangchenzhou.com/projects/u…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTY0MzU4NTk3OTU5NjkvcHUvaW1nL1ZZLXdpMmk0ZkNPYUVLSGYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734548820602864050#m</id>
            <title>RT by @dotey: @arvin17x  开发的 Lobehub 昨天突然在推上爆了。
在 Open AI 套壳的开源项目里面他们的视觉表现和体验确实是独一份的。界面非常漂亮，同时交互细节打磨的也很成熟。
也支持了 GPT-4V 视觉模型交互和 TTS 。同时还有生态非常好的 Agents 插件市场。
未来还会支持几个DALL-E 和 MJ AI 画图能力。
顺便 SD WebUI 都在用的Lobe theme主题也是他们做的。

项目地址：https://chat-preview.lobehub.com/welcome</title>
            <link>https://nitter.cz/op7418/status/1734548820602864050#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734548820602864050#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 12:20:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/arvin17x" title="空谷 · Arvin Xu">@arvin17x</a>  开发的 Lobehub 昨天突然在推上爆了。<br />
在 Open AI 套壳的开源项目里面他们的视觉表现和体验确实是独一份的。界面非常漂亮，同时交互细节打磨的也很成熟。<br />
也支持了 GPT-4V 视觉模型交互和 TTS 。同时还有生态非常好的 Agents 插件市场。<br />
未来还会支持几个DALL-E 和 MJ AI 画图能力。<br />
顺便 SD WebUI 都在用的Lobe theme主题也是他们做的。<br />
<br />
项目地址：<a href="https://chat-preview.lobehub.com/welcome">chat-preview.lobehub.com/wel…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JKWi1Sd2FRQUFDdWprLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734495786208272824#m</id>
            <title>RT by @dotey: Upscale-A-Video：AI提升视频画质
演示视频选的好哇。
项目地址：https://shangchenzhou.com/projects/upscale-a-video/
Github（代码暂未发布）：https://github.com/sczhou/Upscale-A-Video</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734495786208272824#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734495786208272824#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:49:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Upscale-A-Video：AI提升视频画质<br />
演示视频选的好哇。<br />
项目地址：<a href="https://shangchenzhou.com/projects/upscale-a-video/">shangchenzhou.com/projects/u…</a><br />
Github（代码暂未发布）：<a href="https://github.com/sczhou/Upscale-A-Video">github.com/sczhou/Upscale-A-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTU2ODY1MTM4NzI4OTYvcHUvaW1nL05BYUl5RkxMLWt0QzJqN2guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734499555654377759#m</id>
            <title>RT by @dotey: 这个项目挺搞笑的 哈哈哈 你们看看😂

CLoT：训练LLM成为吐槽能手

用日本传统喜剧游戏“大喜利”（Oogiri）作为测试，挑战AI以吐槽高手的方式回应信息。

游戏中的挑战，AI需要理解给定图文信息来产生幽默搞笑的回答。

Oogiri 是一种需要参与者对给定的图像文做出意想不到且幽默的回应的创意游戏。

测试包括图像到文本（I2T）、文本到文本（T2T）和图像&amp;文本到文本（IT2T）

具体方法：

建立数据集：研究人员构建了一个多模态、多语言的 Oogiri-GO 数据集，包含超过 130000 个样本。

训练 AI：通过特殊的训练方法，让 AI 学会如何在游戏中给出创意和幽默的回答。

CLoT 首先将 Oogiri-GO 数据集转化为 LoT 导向的指令调整数据，以训练预训练的 LLM 达到一定的 LoT 幽默生成和辨别能力。

然后，CLoT 设计了一个探索性自我完善过程，鼓励 LLM 通过探索看似无关概念之间的平行关系来生成更多创造性的 LoT 数据，并选择高质量数据进行自我完善。

实验结果：

实验结果显示，CLoT 能够显著提高 LLM（如 Qwen 和 CogVLM）在多种 Oogiri 游戏类型中的表现。具体来说，CLoT 帮助 LLM 生成了更好的幽默内容。

量化性能提升：与原始和 CoT 集成的 LLM 相比，CLoT 集成的 LLM 在 Oogiri 游戏的多项选择和排名问题中取得了更高的性能。

创造性能力的提升：CLoT 还在其他任务（如“云猜测游戏”和“发散性联想任务”）中提高了创造性能力，显示出其卓越的泛化能力。

项目及演示：https://zhongshsh.github.io/CLoT/
论文：https://arxiv.org/abs/2312.02439
GitHub：https://github.com/sail-sg/CLoT</title>
            <link>https://nitter.cz/xiaohuggg/status/1734499555654377759#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734499555654377759#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:04:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个项目挺搞笑的 哈哈哈 你们看看😂<br />
<br />
CLoT：训练LLM成为吐槽能手<br />
<br />
用日本传统喜剧游戏“大喜利”（Oogiri）作为测试，挑战AI以吐槽高手的方式回应信息。<br />
<br />
游戏中的挑战，AI需要理解给定图文信息来产生幽默搞笑的回答。<br />
<br />
Oogiri 是一种需要参与者对给定的图像文做出意想不到且幽默的回应的创意游戏。<br />
<br />
测试包括图像到文本（I2T）、文本到文本（T2T）和图像&文本到文本（IT2T）<br />
<br />
具体方法：<br />
<br />
建立数据集：研究人员构建了一个多模态、多语言的 Oogiri-GO 数据集，包含超过 130000 个样本。<br />
<br />
训练 AI：通过特殊的训练方法，让 AI 学会如何在游戏中给出创意和幽默的回答。<br />
<br />
CLoT 首先将 Oogiri-GO 数据集转化为 LoT 导向的指令调整数据，以训练预训练的 LLM 达到一定的 LoT 幽默生成和辨别能力。<br />
<br />
然后，CLoT 设计了一个探索性自我完善过程，鼓励 LLM 通过探索看似无关概念之间的平行关系来生成更多创造性的 LoT 数据，并选择高质量数据进行自我完善。<br />
<br />
实验结果：<br />
<br />
实验结果显示，CLoT 能够显著提高 LLM（如 Qwen 和 CogVLM）在多种 Oogiri 游戏类型中的表现。具体来说，CLoT 帮助 LLM 生成了更好的幽默内容。<br />
<br />
量化性能提升：与原始和 CoT 集成的 LLM 相比，CLoT 集成的 LLM 在 Oogiri 游戏的多项选择和排名问题中取得了更高的性能。<br />
<br />
创造性能力的提升：CLoT 还在其他任务（如“云猜测游戏”和“发散性联想任务”）中提高了创造性能力，显示出其卓越的泛化能力。<br />
<br />
项目及演示：<a href="https://zhongshsh.github.io/CLoT/">zhongshsh.github.io/CLoT/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02439">arxiv.org/abs/2312.02439</a><br />
GitHub：<a href="https://github.com/sail-sg/CLoT">github.com/sail-sg/CLoT</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTgyODU0MTM2OTEzOTIvcHUvaW1nL3pVYWk0YTlWeWRBbFFFSHcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734465940039807357#m</id>
            <title>给 CSDN 《新程序员》写的一个稿子：《2023 年，我患上了 AI 焦虑症》，CSDN 公众号上刚发，我在自己博客上也发了一份。

CSDN 公众号链接：https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw

博客地址：https://baoyu.io/blog/ai/i-am-suffering-from-ai-anxiety-in-2023</title>
            <link>https://nitter.cz/dotey/status/1734465940039807357#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734465940039807357#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:51:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>给 CSDN 《新程序员》写的一个稿子：《2023 年，我患上了 AI 焦虑症》，CSDN 公众号上刚发，我在自己博客上也发了一份。<br />
<br />
CSDN 公众号链接：<a href="https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw">mp.weixin.qq.com/s/LbRvR1VXp…</a><br />
<br />
博客地址：<a href="https://baoyu.io/blog/ai/i-am-suffering-from-ai-anxiety-in-2023">baoyu.io/blog/ai/i-am-suffer…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJUEw5NFdvQUEzdk9nLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734465344847167767#m</id>
            <title>推荐阅读：《探索编写提示词的乐趣：蒙特卡洛方法、木偶剧和笑声的融合 [译]》

来自 Instacart 官方技术博客的分享，里面有不少写Prompt的技巧，比如CoT、ReAct、“思维的空间”（Room for Thought）、 自我反思 等等

另外文章末尾也分享了很多有价值的Prompt相关的链接

原文：https://tech.instacart.com/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering-4b9272e0c4eb

翻译：https://baoyu.io/translations/llm/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering</title>
            <link>https://nitter.cz/dotey/status/1734465344847167767#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734465344847167767#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:48:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《探索编写提示词的乐趣：蒙特卡洛方法、木偶剧和笑声的融合 [译]》<br />
<br />
来自 Instacart 官方技术博客的分享，里面有不少写Prompt的技巧，比如CoT、ReAct、“思维的空间”（Room for Thought）、 自我反思 等等<br />
<br />
另外文章末尾也分享了很多有价值的Prompt相关的链接<br />
<br />
原文：<a href="https://tech.instacart.com/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering-4b9272e0c4eb">tech.instacart.com/monte-car…</a><br />
<br />
翻译：<a href="https://baoyu.io/translations/llm/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering">baoyu.io/translations/llm/mo…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJUEJpV1hVQUFnc1FTLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734451913104543773#m</id>
            <title>有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的

转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练

这个将图像转换为视频的新模型因为人们认为它有可能替代 TikTok 上的网红而在本周迅速走红。然而，这项技术本身就已经内置了从内容创作者那里盗用作品的行为。

最近，中国零售与科技巨头阿里巴巴的研究团队发表了一篇新论文，介绍了他们的新模型——“Animate Anyone.”。这一消息在网上引发了热议，普遍看法是“TikTokers 的末日来临”，意味着用 AI 技术很快就能取代 TikTok 上的舞蹈内容创作者。

该模型能够接收输入数据（例如 TikTok 舞蹈视频），并输出新的版本。这次实验的结果相比之前类似尝试略有提升。大多数情况下，他们会复制已有的舞蹈视频，但在服装或风格上有所不同，整体效果略逊一筹。但正如 AI 技术的不断进步，这一模型也将持续优化。

已有人指出，“Animate Anyone”可能会被滥用，用来制作未经同意的、将人置于虚构场景的视频。实际上，自六年前这项技术问世以来，这已经成为深度伪造技术的主要用途。

然而，这不仅仅是一个遥远的预测：研究人员已经在未经许可的情况下使用了他人的作品，这已成为他们训练和构建模型的常规做法。阿里巴巴的这篇论文实际上是将最初由明尼苏达大学研究人员出于学术目的创建的“TikTok 数据集”商业化。404 Media 的快速检视显示，阿里巴巴的新 AI 是基于一个抓取了许多知名 TikTok 创作者视频的模型训练而成的，包括 Charli D’Amelio、Addison Rae、Ashley Nocera、Stina Kayy 等几十位。TikTok 数据集中也包括了一些几乎无名的 TikTok 账户用户。

图二：来自《Animate Anyone》论文的参考图像，动力推动帖，DISCO 模型的示例和阿里巴巴的成果展示。

在“Animate Anyone”研究论文的网站上，特别展示了一些著名的 TikTok 内容创作者，作为该模型成功运作的例证。在这些案例中，研究人员采用了这些知名 TikTok 影响者的视频作为参考图像，随后通过阿里巴巴开发的模型进行深度处理，制作出较差质量的 AI 生成副本。

这篇论文及其“Animate Anyone”模型的成果，是建立在未经授权使用创作者作品的基础上的。研究团队在他们的项目页面上，以三位网络名人和艺术家作为示例：Jasmine Chiswell（一位拥有近 1700 万 TikTok 粉丝的生活方式 YouTuber 和 TikTok 名人）、Mackenzie Ziegler（一名歌手和演员，因儿时在《Dance Moms》中出演而知名，拥有 2350 万 TikTok 粉丝）以及 Anna Šulcová（一名 YouTube 内容创作者，拥有 889,600 TikTok 粉丝）。

这些女性都依靠她们独立的创意工作谋生，但阿里巴巴团队却未经许可地使用了她们的作品来支持他们的研究。论文中还展示了更多的 TikTok 创作者，论文已在 arXiv 预印本服务器上发表。

阿里巴巴的研究人员在论文中提到，他们使用了包含 340 个训练视频和 100 个测试视频的“TikTok 数据集”，这些视频都是单人舞蹈，时长在 10 到 15 秒之间。该数据集源于 2021 年明尼苏达大学的一个项目，名为“观看社交媒体舞蹈视频来学习穿着人物的高保真深度”，该项目提出了一种用于“估计人体深度和恢复人体形状的方法”，例如使用 AI 技术在视频中为人物更换服装。

明尼苏达大学的研究人员指出：“我们手动筛选了超过 300 个 TikTok 舞蹈挑战视频，这些视频涵盖了各个月份、不同类型和风格的单人舞蹈。我们选择的舞蹈动作较为温和，以减少运动模糊的产生。对于每个视频，我们都以每秒 30 帧的速度提取了 RGB 图像，总计超过 100,000 张图像。”

大部分人工智能 (AI) 数据集是由在互联网上，如 TikTok 等社交网络，未经内容所有者同意便擦取的视频、图片和文字构成的。在这个例子中，一些博士生组织并启动的数据集，被全球最大的科技及零售公司之一所使用。

这样的情况并不罕见：一开始为学术研究而创建的大型数据集，最终被大公司用于商业目的，无论是相似的还是完全不同的。例如，北卡罗来纳大学威尔明顿分校的研究团队就曾从 YouTube 上擦取了跨性别人士上传的视频，并将其整合成一个数据库，用来研发一种能通过面部识别技术识别跨性别人士的技术。

在当前法律环境日趋严峻的背景下，如阿里巴巴的 AI 研究人员正使用充斥着用户生成内容的擦取数据集。艺术家和其他创作者因 AI 公司未经许可使用他们的作品而提起诉讼。代表艺术家的一起集体诉讼案针对 Midjourney、DeviantArt 和 Stability AI，在去年十月一位法官驳回部分诉求后，又增加了更多原告并提交了修改后的诉状。艺术家们认为，这些 AI 图像生成器复制了原告的作品，并且“创建了与其训练所用作品非常相似的替代品，无论是特定的训练图像还是模仿某些艺术家特有风格的图像，包括原告本人”。

上个月，一位联邦法官推翻了去年驳回编舞家 Kyle Hanagami 对 Epic Games 诉讼的决定。Hanagami 声称 Fortnite 使用了他的舞蹈动作作为“表情动作”。

“把编舞简化成‘姿势’，就好比把音乐仅仅看作‘音符’。编舞，本质上是一系列相关连的舞蹈动作和模式，它们被有机地编排成一个完整的作品，”法官如是写道。“这些动作和模式之间的互动，以及编舞者如何创新地将它们融合和安排，构成了这个作品的核心。仅仅是‘姿势’这个元素，远远不能全面展现出一个编舞作品中的创意表达。”

Hanagami 的律师对 Billboard 表示，推翻之前的驳回裁决将可能对编舞家及其他创意人士在短视频数字媒体时代的权利产生深远影响。这一切对阿里巴巴正在尝试通过“Animate Anyone”项目所打造的内容都有重大的意义，学者们在建立涉及真实人类内容的大型数据集时，也应深思这些决策的未来后果。

https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/</title>
            <link>https://nitter.cz/dotey/status/1734451913104543773#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734451913104543773#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:55:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的<br />
<br />
转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练<br />
<br />
这个将图像转换为视频的新模型因为人们认为它有可能替代 TikTok 上的网红而在本周迅速走红。然而，这项技术本身就已经内置了从内容创作者那里盗用作品的行为。<br />
<br />
最近，中国零售与科技巨头阿里巴巴的研究团队发表了一篇新论文，介绍了他们的新模型——“Animate Anyone.”。这一消息在网上引发了热议，普遍看法是“TikTokers 的末日来临”，意味着用 AI 技术很快就能取代 TikTok 上的舞蹈内容创作者。<br />
<br />
该模型能够接收输入数据（例如 TikTok 舞蹈视频），并输出新的版本。这次实验的结果相比之前类似尝试略有提升。大多数情况下，他们会复制已有的舞蹈视频，但在服装或风格上有所不同，整体效果略逊一筹。但正如 AI 技术的不断进步，这一模型也将持续优化。<br />
<br />
已有人指出，“Animate Anyone”可能会被滥用，用来制作未经同意的、将人置于虚构场景的视频。实际上，自六年前这项技术问世以来，这已经成为深度伪造技术的主要用途。<br />
<br />
然而，这不仅仅是一个遥远的预测：研究人员已经在未经许可的情况下使用了他人的作品，这已成为他们训练和构建模型的常规做法。阿里巴巴的这篇论文实际上是将最初由明尼苏达大学研究人员出于学术目的创建的“TikTok 数据集”商业化。404 Media 的快速检视显示，阿里巴巴的新 AI 是基于一个抓取了许多知名 TikTok 创作者视频的模型训练而成的，包括 Charli D’Amelio、Addison Rae、Ashley Nocera、Stina Kayy 等几十位。TikTok 数据集中也包括了一些几乎无名的 TikTok 账户用户。<br />
<br />
图二：来自《Animate Anyone》论文的参考图像，动力推动帖，DISCO 模型的示例和阿里巴巴的成果展示。<br />
<br />
在“Animate Anyone”研究论文的网站上，特别展示了一些著名的 TikTok 内容创作者，作为该模型成功运作的例证。在这些案例中，研究人员采用了这些知名 TikTok 影响者的视频作为参考图像，随后通过阿里巴巴开发的模型进行深度处理，制作出较差质量的 AI 生成副本。<br />
<br />
这篇论文及其“Animate Anyone”模型的成果，是建立在未经授权使用创作者作品的基础上的。研究团队在他们的项目页面上，以三位网络名人和艺术家作为示例：Jasmine Chiswell（一位拥有近 1700 万 TikTok 粉丝的生活方式 YouTuber 和 TikTok 名人）、Mackenzie Ziegler（一名歌手和演员，因儿时在《Dance Moms》中出演而知名，拥有 2350 万 TikTok 粉丝）以及 Anna Šulcová（一名 YouTube 内容创作者，拥有 889,600 TikTok 粉丝）。<br />
<br />
这些女性都依靠她们独立的创意工作谋生，但阿里巴巴团队却未经许可地使用了她们的作品来支持他们的研究。论文中还展示了更多的 TikTok 创作者，论文已在 arXiv 预印本服务器上发表。<br />
<br />
阿里巴巴的研究人员在论文中提到，他们使用了包含 340 个训练视频和 100 个测试视频的“TikTok 数据集”，这些视频都是单人舞蹈，时长在 10 到 15 秒之间。该数据集源于 2021 年明尼苏达大学的一个项目，名为“观看社交媒体舞蹈视频来学习穿着人物的高保真深度”，该项目提出了一种用于“估计人体深度和恢复人体形状的方法”，例如使用 AI 技术在视频中为人物更换服装。<br />
<br />
明尼苏达大学的研究人员指出：“我们手动筛选了超过 300 个 TikTok 舞蹈挑战视频，这些视频涵盖了各个月份、不同类型和风格的单人舞蹈。我们选择的舞蹈动作较为温和，以减少运动模糊的产生。对于每个视频，我们都以每秒 30 帧的速度提取了 RGB 图像，总计超过 100,000 张图像。”<br />
<br />
大部分人工智能 (AI) 数据集是由在互联网上，如 TikTok 等社交网络，未经内容所有者同意便擦取的视频、图片和文字构成的。在这个例子中，一些博士生组织并启动的数据集，被全球最大的科技及零售公司之一所使用。<br />
<br />
这样的情况并不罕见：一开始为学术研究而创建的大型数据集，最终被大公司用于商业目的，无论是相似的还是完全不同的。例如，北卡罗来纳大学威尔明顿分校的研究团队就曾从 YouTube 上擦取了跨性别人士上传的视频，并将其整合成一个数据库，用来研发一种能通过面部识别技术识别跨性别人士的技术。<br />
<br />
在当前法律环境日趋严峻的背景下，如阿里巴巴的 AI 研究人员正使用充斥着用户生成内容的擦取数据集。艺术家和其他创作者因 AI 公司未经许可使用他们的作品而提起诉讼。代表艺术家的一起集体诉讼案针对 Midjourney、DeviantArt 和 Stability AI，在去年十月一位法官驳回部分诉求后，又增加了更多原告并提交了修改后的诉状。艺术家们认为，这些 AI 图像生成器复制了原告的作品，并且“创建了与其训练所用作品非常相似的替代品，无论是特定的训练图像还是模仿某些艺术家特有风格的图像，包括原告本人”。<br />
<br />
上个月，一位联邦法官推翻了去年驳回编舞家 Kyle Hanagami 对 Epic Games 诉讼的决定。Hanagami 声称 Fortnite 使用了他的舞蹈动作作为“表情动作”。<br />
<br />
“把编舞简化成‘姿势’，就好比把音乐仅仅看作‘音符’。编舞，本质上是一系列相关连的舞蹈动作和模式，它们被有机地编排成一个完整的作品，”法官如是写道。“这些动作和模式之间的互动，以及编舞者如何创新地将它们融合和安排，构成了这个作品的核心。仅仅是‘姿势’这个元素，远远不能全面展现出一个编舞作品中的创意表达。”<br />
<br />
Hanagami 的律师对 Billboard 表示，推翻之前的驳回裁决将可能对编舞家及其他创意人士在短视频数字媒体时代的权利产生深远影响。这一切对阿里巴巴正在尝试通过“Animate Anyone”项目所打造的内容都有重大的意义，学者们在建立涉及真实人类内容的大型数据集时，也应深思这些决策的未来后果。<br />
<br />
<a href="https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/">404media.co/alibaba-animate-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJQ3hwTlc4QUFIckVqLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJQ3pBSlhrQUF4WE4zLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>