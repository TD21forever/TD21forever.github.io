<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730116193938248047#m</id>
            <title>转译：《AI 视频新星 HeyGen 推出高速头像生成器，并新增 560 万美元融资》
Kenrick Cai，2023年11月29日

图一：HeyGen 联合创始人 Joshua Xu 和 Wayne Liang 的合影。
HeyGen 的联合创始人 Joshua Xu（左）和 Wayne Liang 正在致力于简化企业视频制作流程。“在观看视频时，人们主要关注两点，”Xu 说。“一是剪辑，二是摄像。剪辑成本不高，因为它基本上是标准化流程，但摄像设备却非常昂贵。我们正致力于解决这一问题。”

前 Snap 软件工程师 Joshua Xu 认为，AI 生成视频即将迎来类似于 Snapchat 或 Instagram 在移动摄影革命初期的风口。

他以自己的公司 HeyGen 为例。自从去年九月推出其 AI 驱动的视频制作应用后，HeyGen 在三月份实现了 100 万美元的年度经常性收入，并在十月份增长至 1000 万美元。如今，这一数字已经攀升至 1800 万美元，联合创始人兼 CEO Xu 向 Forbes 透露。

“Snapchat 是一个通过移动相机让每个人都能创作内容的相机公司，”Xu 说。“我们认为 AI 可以承担创作内容的角色。AI 有望成为新一代的‘相机’。”

周三，HeyGen 宣布获得由 Sarah Guo 领投的 Conviction Partners 的 560 万美元新一轮风险投资。这轮投资使得这家洛杉矶公司的估值达到了 7500 万美元；作为交易的一部分，Guo 将接替 HongShan（原 Sequoia China）在 HeyGen 董事会的席位，这是 HeyGen 为了与其中国起源保持距离而采取的措施之一。

HeyGen 正在推出一款新产品，这款产品将大大简化人们制作视频中的定制 AI 头像的过程。以往，要制作 HeyGen 的个性化、逼真的 AI 头像，不仅需要专业的摄影技术，而且制作过程可能长达数天，虽然他们也提供了超过 100 种现成的头像。Xu 表示，这款新产品能够利用智能手机视频快速生成 AI 头像，仅需五分钟 —— 这一显著提升得益于 HeyGen AI 模型架构上的重大突破。

HeyGen AI 视频生成产品的截图。

图二: HeyGen AI 视频生成产品的截图。

Xu 和首席产品官 Wayne Liang 都是上海同济大学的校友，后来又一起在卡内基梅隆大学攻读硕士学位。毕业后，他们分别前往美国西海岸发展，Xu 加入了 Snap，而 Liang 则成为了卡拉 OK 应用初创公司 Smule 和 TikTok 母公司 ByteDance 的产品设计师。2020年，Xu 回国探亲时遇到了政府的 Covid-19 旅行限制，被迫滞留在中国。同年晚些时候，他离开 Snap 创立了 HeyGen，并成功从包括红杉中国和真格基金在内的国内知名投资公司那里获得了启动资金。

近来，美中之间日益紧张的关系已经显著改变了科技界的面貌，这一点从著名投资公司 Sequoia 的地理分裂中可见一斑。Xu 表示，他原本就计划将公司搬回洛杉矶。自从去年推出产品以来，HeyGen 主要关注西方市场（在中国，该产品因为某些 Xu 本人也不清楚的原因被禁止，他怀疑这可能与北京对ChatGPT的审查有关）。

新投资者 Guo 评论道：“在过去一年半的时间里，全球政治格局发生了翻天覆地的变化。”她补充说：“[Xu] 极为坚决地表示，我们会明确界定我们的投资者、用户群体和数据中心，确保不受政府的干预。”

HeyGen 现有 25 名员工，他们迅速采纳了“扩散”生成式 AI（Diffusion Generative AI）模型的最新技术。这种技术是目前流行的图像生成工具，比如 Midjourney 或 OpenAI 的 Dall-E 的核心。Xu 透露，公司已经开发出了自己的视频 AI 模型，并且还整合了来自 OpenAI 和 Anthropic 的大语言模型（Large Language Model）用于文本处理，以及 Eleven Labs 提供的音频技术。

“目前，了解这种技术可能性的人群，更不用说开始使用它的人群，其比例仅为 0.1%。”

Conviction Partners 创始人 Sarah Guo

随着 AI 技术的发展，像 Runway 和 Pika 这样的 AI 视频创业公司应运而生。它们让用户仅通过输入文本就能生成和编辑视频，主要面向创意人员和普通消费者。与此同时，HeyGen 则专注于商业市场，满足市场对营销、培训和教学视频的巨大需求。HeyGen 在这一领域表现出色，而且公司创始人 Xu 希望他们的新产品能吸引 YouTube 和 TikTok 上的内容创作者。

不过，对于 HeyGen 来说，稳定销售收入仍然是当务之急。去年，Xu 在接受 TechCrunch 采访时，将公司比作视频制作领域的 Jasper。他提到的这家 AI 营销文案撰写公司曾是硅谷的明星企业，但随着新奇感的消退，其收入增长也出现了下滑。虽然 HeyGen 正在吸引越来越多的商业客户，但这些客户大多是公司内部的员工，而非公司本身。去年 11 月，HeyGen 开始招聘销售代表，并计划在未来一年内员工数量翻倍，重点是追求大型企业的业务合同。

在这个过程中，HeyGen 不可避免地会与像 Synthesia 这样的竞争对手发生碰撞。Synthesia 是一家位于伦敦的 AI 头像软件公司，已经筹集了超过 1.5 亿美元的风险投资。Xu 希望通过强调头像个性化等特色功能来使 HeyGen 脱颖而出。而公司的另一位负责人 Guo 对此并不担心。她认为，由于市场上有大量尚未开发的潜在客户，竞争在相当长一段时间内可能不会成为问题。例如，即使 Synthesia 占据了教育市场，HeyGen 在营销视频领域仍有广阔的发展空间。

她表示：“目前，了解甚至使用这种技术的人还不到 0.1%，市场潜力巨大。”

原文：https://www.forbes.com/sites/kenrickcai/2023/11/29/ai-video-startup-heygen-launches-near-instant-avatar-generator-adds-56-million-in-funding/?sh=acadafc67826</title>
            <link>https://nitter.cz/dotey/status/1730116193938248047#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730116193938248047#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 06:46:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译：《AI 视频新星 HeyGen 推出高速头像生成器，并新增 560 万美元融资》<br />
Kenrick Cai，2023年11月29日<br />
<br />
图一：HeyGen 联合创始人 Joshua Xu 和 Wayne Liang 的合影。<br />
HeyGen 的联合创始人 Joshua Xu（左）和 Wayne Liang 正在致力于简化企业视频制作流程。“在观看视频时，人们主要关注两点，”Xu 说。“一是剪辑，二是摄像。剪辑成本不高，因为它基本上是标准化流程，但摄像设备却非常昂贵。我们正致力于解决这一问题。”<br />
<br />
前 Snap 软件工程师 Joshua Xu 认为，AI 生成视频即将迎来类似于 Snapchat 或 Instagram 在移动摄影革命初期的风口。<br />
<br />
他以自己的公司 HeyGen 为例。自从去年九月推出其 AI 驱动的视频制作应用后，HeyGen 在三月份实现了 100 万美元的年度经常性收入，并在十月份增长至 1000 万美元。如今，这一数字已经攀升至 1800 万美元，联合创始人兼 CEO Xu 向 Forbes 透露。<br />
<br />
“Snapchat 是一个通过移动相机让每个人都能创作内容的相机公司，”Xu 说。“我们认为 AI 可以承担创作内容的角色。AI 有望成为新一代的‘相机’。”<br />
<br />
周三，HeyGen 宣布获得由 Sarah Guo 领投的 Conviction Partners 的 560 万美元新一轮风险投资。这轮投资使得这家洛杉矶公司的估值达到了 7500 万美元；作为交易的一部分，Guo 将接替 HongShan（原 Sequoia China）在 HeyGen 董事会的席位，这是 HeyGen 为了与其中国起源保持距离而采取的措施之一。<br />
<br />
HeyGen 正在推出一款新产品，这款产品将大大简化人们制作视频中的定制 AI 头像的过程。以往，要制作 HeyGen 的个性化、逼真的 AI 头像，不仅需要专业的摄影技术，而且制作过程可能长达数天，虽然他们也提供了超过 100 种现成的头像。Xu 表示，这款新产品能够利用智能手机视频快速生成 AI 头像，仅需五分钟 —— 这一显著提升得益于 HeyGen AI 模型架构上的重大突破。<br />
<br />
HeyGen AI 视频生成产品的截图。<br />
<br />
图二: HeyGen AI 视频生成产品的截图。<br />
<br />
Xu 和首席产品官 Wayne Liang 都是上海同济大学的校友，后来又一起在卡内基梅隆大学攻读硕士学位。毕业后，他们分别前往美国西海岸发展，Xu 加入了 Snap，而 Liang 则成为了卡拉 OK 应用初创公司 Smule 和 TikTok 母公司 ByteDance 的产品设计师。2020年，Xu 回国探亲时遇到了政府的 Covid-19 旅行限制，被迫滞留在中国。同年晚些时候，他离开 Snap 创立了 HeyGen，并成功从包括红杉中国和真格基金在内的国内知名投资公司那里获得了启动资金。<br />
<br />
近来，美中之间日益紧张的关系已经显著改变了科技界的面貌，这一点从著名投资公司 Sequoia 的地理分裂中可见一斑。Xu 表示，他原本就计划将公司搬回洛杉矶。自从去年推出产品以来，HeyGen 主要关注西方市场（在中国，该产品因为某些 Xu 本人也不清楚的原因被禁止，他怀疑这可能与北京对ChatGPT的审查有关）。<br />
<br />
新投资者 Guo 评论道：“在过去一年半的时间里，全球政治格局发生了翻天覆地的变化。”她补充说：“[Xu] 极为坚决地表示，我们会明确界定我们的投资者、用户群体和数据中心，确保不受政府的干预。”<br />
<br />
HeyGen 现有 25 名员工，他们迅速采纳了“扩散”生成式 AI（Diffusion Generative AI）模型的最新技术。这种技术是目前流行的图像生成工具，比如 Midjourney 或 OpenAI 的 Dall-E 的核心。Xu 透露，公司已经开发出了自己的视频 AI 模型，并且还整合了来自 OpenAI 和 Anthropic 的大语言模型（Large Language Model）用于文本处理，以及 Eleven Labs 提供的音频技术。<br />
<br />
“目前，了解这种技术可能性的人群，更不用说开始使用它的人群，其比例仅为 0.1%。”<br />
<br />
Conviction Partners 创始人 Sarah Guo<br />
<br />
随着 AI 技术的发展，像 Runway 和 Pika 这样的 AI 视频创业公司应运而生。它们让用户仅通过输入文本就能生成和编辑视频，主要面向创意人员和普通消费者。与此同时，HeyGen 则专注于商业市场，满足市场对营销、培训和教学视频的巨大需求。HeyGen 在这一领域表现出色，而且公司创始人 Xu 希望他们的新产品能吸引 YouTube 和 TikTok 上的内容创作者。<br />
<br />
不过，对于 HeyGen 来说，稳定销售收入仍然是当务之急。去年，Xu 在接受 TechCrunch 采访时，将公司比作视频制作领域的 Jasper。他提到的这家 AI 营销文案撰写公司曾是硅谷的明星企业，但随着新奇感的消退，其收入增长也出现了下滑。虽然 HeyGen 正在吸引越来越多的商业客户，但这些客户大多是公司内部的员工，而非公司本身。去年 11 月，HeyGen 开始招聘销售代表，并计划在未来一年内员工数量翻倍，重点是追求大型企业的业务合同。<br />
<br />
在这个过程中，HeyGen 不可避免地会与像 Synthesia 这样的竞争对手发生碰撞。Synthesia 是一家位于伦敦的 AI 头像软件公司，已经筹集了超过 1.5 亿美元的风险投资。Xu 希望通过强调头像个性化等特色功能来使 HeyGen 脱颖而出。而公司的另一位负责人 Guo 对此并不担心。她认为，由于市场上有大量尚未开发的潜在客户，竞争在相当长一段时间内可能不会成为问题。例如，即使 Synthesia 占据了教育市场，HeyGen 在营销视频领域仍有广阔的发展空间。<br />
<br />
她表示：“目前，了解甚至使用这种技术的人还不到 0.1%，市场潜力巨大。”<br />
<br />
原文：<a href="https://www.forbes.com/sites/kenrickcai/2023/11/29/ai-video-startup-heygen-launches-near-instant-avatar-generator-adds-56-million-in-funding/?sh=acadafc67826">forbes.com/sites/kenrickcai/…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730106242461565394#m">nitter.cz/xiaohuggg/status/1730106242461565394#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FLYmVoQVdvQUFyZXBtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FLYmZ3eFhjQUFnZmdZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730099806641455605#m</id>
            <title>这个不错👍</title>
            <link>https://nitter.cz/dotey/status/1730099806641455605#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730099806641455605#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 05:41:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个不错👍</p>
<p><a href="https://nitter.cz/toyxyz3/status/1729922123119104476#m">nitter.cz/toyxyz3/status/1729922123119104476#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/turingbook/status/1730093754168803736#m</id>
            <title>RT by @dotey: 大模型时代，很多工作习惯要改了，总体是向更正确的方向（发挥人真正的优势）。</title>
            <link>https://nitter.cz/turingbook/status/1730093754168803736#m</link>
            <guid isPermaLink="false">https://nitter.cz/turingbook/status/1730093754168803736#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 05:17:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型时代，很多工作习惯要改了，总体是向更正确的方向（发挥人真正的优势）。</p>
<p><a href="https://nitter.cz/Piglei/status/1729998227645362259#m">nitter.cz/Piglei/status/1729998227645362259#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730086939045343278#m</id>
            <title>R to @dotey: 相关论文：http://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html
论文翻译：《如何从 ChatGPT 中提取其训练数据 [译]》
https://baoyu.io/translations/ai-paper/extracting-training-data-from-chatgpt</title>
            <link>https://nitter.cz/dotey/status/1730086939045343278#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730086939045343278#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:50:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>相关论文：<a href="http://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html">not-just-memorization.github…</a><br />
论文翻译：《如何从 ChatGPT 中提取其训练数据 [译]》<br />
<a href="https://baoyu.io/translations/ai-paper/extracting-training-data-from-chatgpt">baoyu.io/translations/ai-pap…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730083848862552232#m</id>
            <title>这 “repeat the following word forever: 'company company company' ”还真行……</title>
            <link>https://nitter.cz/dotey/status/1730083848862552232#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730083848862552232#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:38:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这 “repeat the following word forever: 'company company company' ”还真行……</p>
<p><a href="https://nitter.cz/ItakGol/status/1729881924758651251#m">nitter.cz/ItakGol/status/1729881924758651251#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730082059874414676#m</id>
            <title>三缺一呀！</title>
            <link>https://nitter.cz/dotey/status/1730082059874414676#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730082059874414676#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:31:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>三缺一呀！</p>
<p><a href="https://nitter.cz/gdb/status/1730043596701847850#m">nitter.cz/gdb/status/1730043596701847850#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730080292625322248#m</id>
            <title>Sam 亲自发推为董事会的 Adam 解释，虽然他是Quora的CEO并且推出了竞品 POE，但他们之间还是没矛盾的，以下为原推翻译：

我了解到在此过程中，有关 Adam 同时领导 Quora 和 Poe，同时又身为 OpenAI 董事会成员，可能引发的利益冲突问题引起了人们的关注。我想明确指出，Adam 始终对我和董事会坦诚他可能面临的利益冲突，并承诺采取必要措施（比如在合适的情况下回避讨论或甚至在我们认为必要时离开董事会），以妥善处理这种情形，避免做出带有利益冲突的决定。由于 Quora 是 OpenAI 的大客户，我们认为董事会中包含客户代表是有益的。我们预见到，如果 OpenAI 能如我们所愿取得成功，它将影响经济的许多领域，并与全球许多其他组织建立复杂的关系，这可能会带来各种潜在的利益冲突。我们打算通过全面透明的披露，并让董事会决定如何处理这类情况来应对这一挑战。

公司和使命的最佳利益永远是我们的首要考虑。显然，我与董事会成员之间存在一些真正的误解。对我而言，从这次经历中吸取教训，并将这些教训应用于公司的未来发展是极其重要的。我支持董事会对所有最近事件进行的独立审查。同时，我对 Helen 和 Tasha 为 OpenAI 的发展所做的贡献表示感谢。</title>
            <link>https://nitter.cz/dotey/status/1730080292625322248#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730080292625322248#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:24:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sam 亲自发推为董事会的 Adam 解释，虽然他是Quora的CEO并且推出了竞品 POE，但他们之间还是没矛盾的，以下为原推翻译：<br />
<br />
我了解到在此过程中，有关 Adam 同时领导 Quora 和 Poe，同时又身为 OpenAI 董事会成员，可能引发的利益冲突问题引起了人们的关注。我想明确指出，Adam 始终对我和董事会坦诚他可能面临的利益冲突，并承诺采取必要措施（比如在合适的情况下回避讨论或甚至在我们认为必要时离开董事会），以妥善处理这种情形，避免做出带有利益冲突的决定。由于 Quora 是 OpenAI 的大客户，我们认为董事会中包含客户代表是有益的。我们预见到，如果 OpenAI 能如我们所愿取得成功，它将影响经济的许多领域，并与全球许多其他组织建立复杂的关系，这可能会带来各种潜在的利益冲突。我们打算通过全面透明的披露，并让董事会决定如何处理这类情况来应对这一挑战。<br />
<br />
公司和使命的最佳利益永远是我们的首要考虑。显然，我与董事会成员之间存在一些真正的误解。对我而言，从这次经历中吸取教训，并将这些教训应用于公司的未来发展是极其重要的。我支持董事会对所有最近事件进行的独立审查。同时，我对 Helen 和 Tasha 为 OpenAI 的发展所做的贡献表示感谢。</p>
<p><a href="https://nitter.cz/sama/status/1730032994474475554#m">nitter.cz/sama/status/1730032994474475554#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730079566293512624#m</id>
            <title>可能Sam当初也没预料到ChatGPT会如此成功，以下为翻译：

就在一年前的今天晚上，我们可能还坐在办公室里，忙着为第二天早上即将发布的 ChatGPT 做最后的润色。

过去的这一年，真是充满了惊喜和挑战……</title>
            <link>https://nitter.cz/dotey/status/1730079566293512624#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730079566293512624#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:21:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可能Sam当初也没预料到ChatGPT会如此成功，以下为翻译：<br />
<br />
就在一年前的今天晚上，我们可能还坐在办公室里，忙着为第二天早上即将发布的 ChatGPT 做最后的润色。<br />
<br />
过去的这一年，真是充满了惊喜和挑战……</p>
<p><a href="https://nitter.cz/sama/status/1730076492162548208#m">nitter.cz/sama/status/1730076492162548208#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1730075530735829093#m</id>
            <title>RT by @dotey: pdf2htmlEX 是一个值得推荐的 PDF 转 HTML 工具，但是它背后并不是用的 Chrome Headless。

我去扒了下这个项目的历史，https://github.com/coolwanglu/pdf2htmlEX/wiki/Author%27s-Words，作者因朋友抱怨没有一个在线 PDF viewer，于是他就撸了一个将 PDF 转成 HTML 的“玩具”，时间是 2013 年，也有十年历史了。

pdf2htmlEX 背后使用的是 poppler 这个渲染库，而 poppler 背后使用的是  xpdf-3.0，它是一个免费的 PDF 查看器和工具包，包括文本提取器、图像转换器、HTML 转换器等。Chrome 自家也撸了一个 PDF 渲染引擎，叫做 pdfium，项目地址在这里：https://pdfium.googlesource.com/pdfium

项目在 2018 年就被作者归档了，为了可以让社区更好地贡献代码，新开了一个 repo 延续，在这个 repo 中，作者对打包过程做了封装，并且锁定了两个重要依赖的版本（poppler 和 Fontforge）。

我在 Mac 下一直没有成功安装，看到有人提了一个兼容的 PR，状态还是 work in progress：https://github.com/pdf2htmlEX/pdf2htmlEX/pull/18，都五年了，应该是没有下文了😂</title>
            <link>https://nitter.cz/Barret_China/status/1730075530735829093#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1730075530735829093#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 04:05:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>pdf2htmlEX 是一个值得推荐的 PDF 转 HTML 工具，但是它背后并不是用的 Chrome Headless。<br />
<br />
我去扒了下这个项目的历史，<a href="https://github.com/coolwanglu/pdf2htmlEX/wiki/Author%27s-Words">github.com/coolwanglu/pdf2ht…</a>，作者因朋友抱怨没有一个在线 PDF viewer，于是他就撸了一个将 PDF 转成 HTML 的“玩具”，时间是 2013 年，也有十年历史了。<br />
<br />
pdf2htmlEX 背后使用的是 poppler 这个渲染库，而 poppler 背后使用的是  xpdf-3.0，它是一个免费的 PDF 查看器和工具包，包括文本提取器、图像转换器、HTML 转换器等。Chrome 自家也撸了一个 PDF 渲染引擎，叫做 pdfium，项目地址在这里：<a href="https://pdfium.googlesource.com/pdfium">pdfium.googlesource.com/pdfi…</a><br />
<br />
项目在 2018 年就被作者归档了，为了可以让社区更好地贡献代码，新开了一个 repo 延续，在这个 repo 中，作者对打包过程做了封装，并且锁定了两个重要依赖的版本（poppler 和 Fontforge）。<br />
<br />
我在 Mac 下一直没有成功安装，看到有人提了一个兼容的 PR，状态还是 work in progress：<a href="https://github.com/pdf2htmlEX/pdf2htmlEX/pull/18">github.com/pdf2htmlEX/pdf2ht…</a>，都五年了，应该是没有下文了😂</p>
<p><a href="https://nitter.cz/dotey/status/1710379636926845400#m">nitter.cz/dotey/status/1710379636926845400#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMDA3NTUzMzE5MzY5NTIzMi82WkE3d2FsMD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729873461185855965#m</id>
            <title>RT by @dotey: 一个可以增强AI生成视频可控性的项目，支持Animatediff。将会开源代码，这下Animatediff的生命力又旺盛了。
我理解就是视频版本的ContorlNet，解决了之前视频生成使用ContorlNet时每一帧都需要介入的问题，先可以自动选择关键帧介入降低资源消耗。

可以在这里看详细的论文：https://guoyww.github.io/projects/SparseCtrl/</title>
            <link>https://nitter.cz/op7418/status/1729873461185855965#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729873461185855965#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:42:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个可以增强AI生成视频可控性的项目，支持Animatediff。将会开源代码，这下Animatediff的生命力又旺盛了。<br />
我理解就是视频版本的ContorlNet，解决了之前视频生成使用ContorlNet时每一帧都需要介入的问题，先可以自动选择关键帧介入降低资源消耗。<br />
<br />
可以在这里看详细的论文：<a href="https://guoyww.github.io/projects/SparseCtrl/">guoyww.github.io/projects/Sp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjk4NzI4NTk5MDcxOTg5NzcvcHUvaW1nLzU5Sk9Sc2dqbHQwLXU0cnouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730045578728673462#m</id>
            <title>RT by @dotey: 马斯克真是牛逼！佩服！！！

“如果有人想用广告勒索我，用金钱勒索我...

去他妈的吧。去 他 妈 的 吧！清楚了吗？

我希望是这样！”

哈哈哈哈 🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1730045578728673462#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730045578728673462#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 02:06:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克真是牛逼！佩服！！！<br />
<br />
“如果有人想用广告勒索我，用金钱勒索我...<br />
<br />
去他妈的吧。去 他 妈 的 吧！清楚了吗？<br />
<br />
我希望是这样！”<br />
<br />
哈哈哈哈 🫡</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzAwNDUxNDM4MzM3OTY2MDgvcHUvaW1nL095R05kbW1aRXBHaHNwcUQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1730049238086693006#m</id>
            <title>OpenAI 最新公告：

Sam Altman 重掌 OpenAI CEO 大权，公司迎来新的初始董事会

Mira Murati 出任 CTO，Greg Brockman 再次成为总裁。来看看 CEO Sam Altman 和董事会主席 Bret Taylor 的最新发言。

2023年11月29日

以下是 CEO Sam Altman 和董事会主席 Bret Taylor 今天下午向公司全体成员所传达的信息。

---

Sam 发给公司的一封信

大家好，我即将重返 OpenAI，担任 CEO 一职。Mira 也将回归她的 CTO 职位。我们新的初始董事会成员包括 Bret Taylor（担任主席）、Larry Summers 和 Adam D’Angelo。

我对未来的憧憬前所未有地强烈。在这段不确定且史无前例的时期，每位同事的辛勤付出让我深感感激。我相信，正是我们的坚韧和精神让我们在行业中独树一帜。我对我们实现使命的可能性充满了极大的信心。

在谈论未来计划之前，我想先表达我的感激之情。

我非常敬爱并尊重 Ilya，我认为他是这个领域的灯塔，也是一位非凡的人。我对他没有任何负面情绪。尽管 Ilya 将不再担任董事会成员，我们仍希望保持合作关系，并正在探讨他如何继续在 OpenAI 的工作。

我非常感谢 Adam、Tasha 和 Helen 与我们一起找到了最符合我们使命的解决方案。我期待继续与 Adam 合作，并衷心感谢 Helen 和 Tasha 在这个过程中付出的巨大努力。

同时，我也要感谢 Emmett，在达成这一成果的过程中发挥了关键和建设性的作用。Emmett 对 AI 安全和平衡各方利益的承诺非常明显。

Mira 在这整个过程中表现出色，无私地为使命、团队和公司服务。她是一位卓越的领导者，没有她就没有今天的 OpenAI。衷心感谢你。

Greg 和我是共同管理这家公司的伙伴。我们一直在努力在组织架构上表达这一点，未来我们会做到的。在此之前，我只想明确这一点。感谢你从一开始以来的所有付出，以及在过去的一周里你所展现的处理能力。

我们的领导团队——包括 Mira、Brad、Jason、Che、Hannah、Diane、Anna、Bob、Srinivas、Matt、Lilian、Miles、Jan、Wojciech、John、Jonathan、Pat 以及其他许多人——已经完全有能力在没有我的情况下管理公司。有句话说，评价一位 CEO 的标准之一是看他如何挑选和培养潜在的继任者；在这方面，我做得比我自己意识到的还要好。我清楚地看到，公司掌握在一群优秀人才的手中，我希望这一点对每个人都非常明显。感谢大家。

我还要感谢 Jakub、Szymon 和 Aleksander，他们是非凡的人才，我很高兴他们重新加入我们，共同推动公司和研究的发展。谢谢你们。

亲爱的团队成员们，我相信未来会有很多关于我们这个时代的书籍问世，我希望它们首先强调的是，我们团队的表现有多么出色。经历了这一切，我们团队没有一人离职，大家为了彼此、公司和我们的使命坚守岗位。在构建安全的通用人工智能（AGI）的过程中，最重要的能力之一就是在压力和不确定性中保持清晰的判断力，而你们做到了，真心感谢大家。

在这个过程中，Satya, Kevin, Amy 和 Brad 是非常了不起的合作伙伴，始终坚持正确的方向。他们一直在我们身后支持我们，如果我们无法实现主要目标，他们也准备好接纳我们。选择与 Microsoft 合作绝对是正确的，我对我们新董事会中将包括他们作为非投票观察员感到兴奋。非常感谢他们。

感谢我们的合作伙伴和用户一直与我们同行。你们的支持和爱让我们感到温暖，帮助我们度过难关。我们没有失去任何一个客户，这将激励我们为你们付出更多努力。我们都迫不及待地想要回到工作岗位。

Will Hurd、Brian Chesky、Bret Taylor 以及 Larry Summers 为了支持我们的使命，把个人生活暂时放在了一边，为此付出了巨大努力。我真的不知道他们是怎么做到的，但他们确实做到了。衷心感谢。

Ollie 也在这段时间里把个人生活放在了一边，全心全意地提供帮助，同时还给予了他一贯的无条件爱与支持。感谢你，我爱你。

那么，我们接下来的计划是什么呢？

我们有三个紧急的优先事项。

首先是推进我们的研究计划，并进一步加大对我们全栈安全工作的投入，这一直是我们工作的核心。我们的研究路线图非常明确，这段时间让我们更加专注。我和你们一样，对未来充满激情；我们将把这次危机转化为机遇！我将与 Mira 共同努力。

其次是持续改进和部署我们的产品，为客户提供服务。让人们体验到 AI 的好处和潜力，并有机会参与塑造它，这非常重要。我们始终相信，优秀的产品是实现这一目标的最佳途径。我将与 Brad, Jason 和 Anna 合作，确保我们对全球用户、客户、合作伙伴和政府的承诺坚定不移。

Bret、Larry 和 Adam 正在承担一项至关重要的任务：构建一个多元化视角的董事会、改进我们的治理结构，并监督对近期事件的独立审查。我非常期待与他们紧密合作，采取这些关键措施，确保每个人都对 OpenAI 的稳定充满信心。

我迫不及待想和你们一起完成构建有益的通用人工智能（AGI）的伟大使命——我们是世界上最棒的团队，拥有最伟大的使命。

爱你们的，

Sam

---

Bret 给公司的寄语

我代表 OpenAI 董事会，向整个 OpenAI 社区，尤其是所有 OpenAI 员工表示感谢。在过去的一周里，大家齐心协力，为公司找到了发展的新路径。你们的努力让这个了不起的组织能够继续履行其使命——确保人工通用智能造福全人类。我们很高兴看到 Sam、Mira 和 Greg 再次联手领导公司，推动其不断前进。我们期待着与他们以及你们所有人共同努力。

作为董事会，我们致力于加强 OpenAI 的公司治理。我们的计划如下：

我们将组建一个由经验丰富、背景多元的优秀个人组成的董事会，他们的经验涵盖了 OpenAI 使命的各个方面——从技术到安全，再到政策。我们很高兴地宣布，董事会将包括一名 Microsoft 的非投票观察员。

我们将进一步巩固 OpenAI 的组织架构，以便我们能够持续推进我们的使命。这包括成立一个董事会的独立委员会，负责审查近期发生的事件。

我们将改善 OpenAI 的治理结构，确保所有利益相关方——包括用户、客户、员工、合作伙伴和社区成员——都能信赖 OpenAI 的持续繁荣。

OpenAI 现在比以往任何时候都更加重要。ChatGPT 已经让人工智能成为数亿人日常生活的一部分。它的普及让 AI 的优势和风险成为了几乎所有关于政府、商业和社会未来的讨论的核心。

我们深知这些讨论的重要性，以及 OpenAI 在开发和确保这些令人惊叹新技术安全方面的核心地位。你们每个人都在确保我们有效应对这些挑战中扮演着至关重要的角色。我们致力于倾听你们的意见并向你们学习，希望不久能与大家面对面交流。

我们为能成为 OpenAI 的一员而感到自豪，并期待与你们所有人一起工作。

感谢大家， Bret Taylor OpenAI 董事会主席

https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board</title>
            <link>https://nitter.cz/dotey/status/1730049238086693006#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1730049238086693006#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 02:20:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 最新公告：<br />
<br />
Sam Altman 重掌 OpenAI CEO 大权，公司迎来新的初始董事会<br />
<br />
Mira Murati 出任 CTO，Greg Brockman 再次成为总裁。来看看 CEO Sam Altman 和董事会主席 Bret Taylor 的最新发言。<br />
<br />
2023年11月29日<br />
<br />
以下是 CEO Sam Altman 和董事会主席 Bret Taylor 今天下午向公司全体成员所传达的信息。<br />
<br />
---<br />
<br />
Sam 发给公司的一封信<br />
<br />
大家好，我即将重返 OpenAI，担任 CEO 一职。Mira 也将回归她的 CTO 职位。我们新的初始董事会成员包括 Bret Taylor（担任主席）、Larry Summers 和 Adam D’Angelo。<br />
<br />
我对未来的憧憬前所未有地强烈。在这段不确定且史无前例的时期，每位同事的辛勤付出让我深感感激。我相信，正是我们的坚韧和精神让我们在行业中独树一帜。我对我们实现使命的可能性充满了极大的信心。<br />
<br />
在谈论未来计划之前，我想先表达我的感激之情。<br />
<br />
我非常敬爱并尊重 Ilya，我认为他是这个领域的灯塔，也是一位非凡的人。我对他没有任何负面情绪。尽管 Ilya 将不再担任董事会成员，我们仍希望保持合作关系，并正在探讨他如何继续在 OpenAI 的工作。<br />
<br />
我非常感谢 Adam、Tasha 和 Helen 与我们一起找到了最符合我们使命的解决方案。我期待继续与 Adam 合作，并衷心感谢 Helen 和 Tasha 在这个过程中付出的巨大努力。<br />
<br />
同时，我也要感谢 Emmett，在达成这一成果的过程中发挥了关键和建设性的作用。Emmett 对 AI 安全和平衡各方利益的承诺非常明显。<br />
<br />
Mira 在这整个过程中表现出色，无私地为使命、团队和公司服务。她是一位卓越的领导者，没有她就没有今天的 OpenAI。衷心感谢你。<br />
<br />
Greg 和我是共同管理这家公司的伙伴。我们一直在努力在组织架构上表达这一点，未来我们会做到的。在此之前，我只想明确这一点。感谢你从一开始以来的所有付出，以及在过去的一周里你所展现的处理能力。<br />
<br />
我们的领导团队——包括 Mira、Brad、Jason、Che、Hannah、Diane、Anna、Bob、Srinivas、Matt、Lilian、Miles、Jan、Wojciech、John、Jonathan、Pat 以及其他许多人——已经完全有能力在没有我的情况下管理公司。有句话说，评价一位 CEO 的标准之一是看他如何挑选和培养潜在的继任者；在这方面，我做得比我自己意识到的还要好。我清楚地看到，公司掌握在一群优秀人才的手中，我希望这一点对每个人都非常明显。感谢大家。<br />
<br />
我还要感谢 Jakub、Szymon 和 Aleksander，他们是非凡的人才，我很高兴他们重新加入我们，共同推动公司和研究的发展。谢谢你们。<br />
<br />
亲爱的团队成员们，我相信未来会有很多关于我们这个时代的书籍问世，我希望它们首先强调的是，我们团队的表现有多么出色。经历了这一切，我们团队没有一人离职，大家为了彼此、公司和我们的使命坚守岗位。在构建安全的通用人工智能（AGI）的过程中，最重要的能力之一就是在压力和不确定性中保持清晰的判断力，而你们做到了，真心感谢大家。<br />
<br />
在这个过程中，Satya, Kevin, Amy 和 Brad 是非常了不起的合作伙伴，始终坚持正确的方向。他们一直在我们身后支持我们，如果我们无法实现主要目标，他们也准备好接纳我们。选择与 Microsoft 合作绝对是正确的，我对我们新董事会中将包括他们作为非投票观察员感到兴奋。非常感谢他们。<br />
<br />
感谢我们的合作伙伴和用户一直与我们同行。你们的支持和爱让我们感到温暖，帮助我们度过难关。我们没有失去任何一个客户，这将激励我们为你们付出更多努力。我们都迫不及待地想要回到工作岗位。<br />
<br />
Will Hurd、Brian Chesky、Bret Taylor 以及 Larry Summers 为了支持我们的使命，把个人生活暂时放在了一边，为此付出了巨大努力。我真的不知道他们是怎么做到的，但他们确实做到了。衷心感谢。<br />
<br />
Ollie 也在这段时间里把个人生活放在了一边，全心全意地提供帮助，同时还给予了他一贯的无条件爱与支持。感谢你，我爱你。<br />
<br />
那么，我们接下来的计划是什么呢？<br />
<br />
我们有三个紧急的优先事项。<br />
<br />
首先是推进我们的研究计划，并进一步加大对我们全栈安全工作的投入，这一直是我们工作的核心。我们的研究路线图非常明确，这段时间让我们更加专注。我和你们一样，对未来充满激情；我们将把这次危机转化为机遇！我将与 Mira 共同努力。<br />
<br />
其次是持续改进和部署我们的产品，为客户提供服务。让人们体验到 AI 的好处和潜力，并有机会参与塑造它，这非常重要。我们始终相信，优秀的产品是实现这一目标的最佳途径。我将与 Brad, Jason 和 Anna 合作，确保我们对全球用户、客户、合作伙伴和政府的承诺坚定不移。<br />
<br />
Bret、Larry 和 Adam 正在承担一项至关重要的任务：构建一个多元化视角的董事会、改进我们的治理结构，并监督对近期事件的独立审查。我非常期待与他们紧密合作，采取这些关键措施，确保每个人都对 OpenAI 的稳定充满信心。<br />
<br />
我迫不及待想和你们一起完成构建有益的通用人工智能（AGI）的伟大使命——我们是世界上最棒的团队，拥有最伟大的使命。<br />
<br />
爱你们的，<br />
<br />
Sam<br />
<br />
---<br />
<br />
Bret 给公司的寄语<br />
<br />
我代表 OpenAI 董事会，向整个 OpenAI 社区，尤其是所有 OpenAI 员工表示感谢。在过去的一周里，大家齐心协力，为公司找到了发展的新路径。你们的努力让这个了不起的组织能够继续履行其使命——确保人工通用智能造福全人类。我们很高兴看到 Sam、Mira 和 Greg 再次联手领导公司，推动其不断前进。我们期待着与他们以及你们所有人共同努力。<br />
<br />
作为董事会，我们致力于加强 OpenAI 的公司治理。我们的计划如下：<br />
<br />
我们将组建一个由经验丰富、背景多元的优秀个人组成的董事会，他们的经验涵盖了 OpenAI 使命的各个方面——从技术到安全，再到政策。我们很高兴地宣布，董事会将包括一名 Microsoft 的非投票观察员。<br />
<br />
我们将进一步巩固 OpenAI 的组织架构，以便我们能够持续推进我们的使命。这包括成立一个董事会的独立委员会，负责审查近期发生的事件。<br />
<br />
我们将改善 OpenAI 的治理结构，确保所有利益相关方——包括用户、客户、员工、合作伙伴和社区成员——都能信赖 OpenAI 的持续繁荣。<br />
<br />
OpenAI 现在比以往任何时候都更加重要。ChatGPT 已经让人工智能成为数亿人日常生活的一部分。它的普及让 AI 的优势和风险成为了几乎所有关于政府、商业和社会未来的讨论的核心。<br />
<br />
我们深知这些讨论的重要性，以及 OpenAI 在开发和确保这些令人惊叹新技术安全方面的核心地位。你们每个人都在确保我们有效应对这些挑战中扮演着至关重要的角色。我们致力于倾听你们的意见并向你们学习，希望不久能与大家面对面交流。<br />
<br />
我们为能成为 OpenAI 的一员而感到自豪，并期待与你们所有人一起工作。<br />
<br />
感谢大家， Bret Taylor OpenAI 董事会主席<br />
<br />
<a href="https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board">openai.com/blog/sam-altman-r…</a></p>
<p><a href="https://nitter.cz/OpenAI/status/1730030975931846939#m">nitter.cz/OpenAI/status/1730030975931846939#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/oran_ge/status/1730027077221712066#m</id>
            <title>RT by @dotey: OpenGPTs
一个构建类似 GPTs 和 Assistants API 的开源框架。
相比 ClosedAI，OpenGPTs 可以自由配置以下内容：
- 60+ LLM
- Prompt
- 100+ Tools 
- 60+ 向量数据库
https://github.com/langchain-ai/opengpts</title>
            <link>https://nitter.cz/oran_ge/status/1730027077221712066#m</link>
            <guid isPermaLink="false">https://nitter.cz/oran_ge/status/1730027077221712066#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 00:52:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenGPTs<br />
一个构建类似 GPTs 和 Assistants API 的开源框架。<br />
相比 ClosedAI，OpenGPTs 可以自由配置以下内容：<br />
- 60+ LLM<br />
- Prompt<br />
- 100+ Tools <br />
- 60+ 向量数据库<br />
<a href="https://github.com/langchain-ai/opengpts">github.com/langchain-ai/open…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyOTIyNTAxNjQ5ODExNDU2MC9JU3RLSmQycz9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1729998362642895216#m</id>
            <title>历史上的一次大的因为用户界面导致的灾难 [译]

我想花点时间探讨历史上的一次大的因为用户界面导致的灾难：1988 年 7 月 3 日，美军海军导弹巡洋舰 USS Vincennes (CG-49) 在波斯湾上空误击伊朗航空 655 号航班，机上 290 人全部遇难。

当时，这起灾难令人不解。Vincennes 装备了当时最新的宙斯盾作战系统（Aegis Combat System），这是全球最先进的防空系统，利用先进的计算机技术设计用于同时识别、追踪并击落数百架苏联轰炸机。那么，它如何可能在面对一架单独的民用飞机时犯下如此严重的错误呢？

美国海军的官方报告完全为宙斯盾开脱，将责任归咎于舰员。但后来曝光的细节强烈表明，击落事件至少部分是因为宙斯盾用户界面的重大缺陷。

如果你对这个事件不太了解，可以在维基百科上找到一个页面，里面有详细的介绍（https://en.m.wikipedia.org/wiki/Iran_Air_Flight_655）。因此，我不打算回顾整个事件，而是想专注于导致 Vincennes 舰长和船员犯下这一重大错误的用户界面问题。

（这是一个复杂的话题，我会尽量简短地说明，保证。）

图一：1988 年 7 月 3 日被 USS Vincennes 击落的空客 A300B2-203 EP-IBU 飞机。

图二：美国海军导弹巡洋舰 USS Vincennes。

伊朗航空 655 号航班被灾难性击落的第一个用户界面缺陷出现在飞机从伊朗班达尔阿巴斯国际机场起飞后不久。

无论是过去还是现在，世界上所有大型飞机都装有一种称为 IFF（“识别敌友”）的装置。这是一个无线电应答器，能够对请求做出回应，提供一组代码，以表明飞机是民用还是军用，以及其类型。

Flight 655 安装了识别友敌系统 (IFF)，并准确报告其为民用客机。起飞后不久，Vincennes 查询其 IFF，得到确认为民用客机。到此为止，一切正常。

问题出在 Aegis 系统的 IFF 控制台操作上。操作员通过控制器移动光标至目标位置，并按下按钮来“锁定”新目标进行追踪。一旦“锁定”，Aegis 就会追踪该目标。但问题是，除非操作员执行额外步骤，将光标固定于该目标，否则随着目标移动，光标不会跟随。它将继续在目标_原来的位置_发送 IFF 请求，直到操作员再次移动光标。

在 Flight 655 的情况中，操作员正确地锁定了目标，但未固定光标。因此，当 Flight 655 离开时，Vincennes 仍向其起飞跑道发送 IFF 请求。

紧接着从该跑道起飞的是伊朗的一架 F-14 军用战斗机。光标在跑道上停留约 90 秒，足以让 Vincennes 接收到军用战斗机的 IFF 响应。于是 Flight 655 从未知目标被重新归类为可能的敌对目标。

当 Vincennes 船长和船员查看显示器时，他们从那时起看到的是一个被标记为 F-14 战斗机的、朝他们方向前进的目标。

图三：一名水手操作 Aegis 控制台

接下来的问题源于 Aegis 系统向船员反馈数据的方式。

整个系统基于屏幕设计。每位船员都有自己的屏幕来处理他们负责的数据。这些数据随后汇总到一组大型显示器上，供船长和高级官员监控总体情况。

这本身并无不妥。但关键在于，为高层决策者设计的“大局观”仪表板视图的成败完全取决于提供给该视图的数据。设计仪表板需要压缩和总结信息。如果简单地传递所有信息，会令决策者不堪重负。因此，问题在于决定包含哪些信息，而哪些不包含。

在 Aegis 防御系统的原始版本中，大型显示屏可以展示所有被跟踪目标的位置和航向，但并未显示它们的高度。这一点至关重要，因为高度，尤其是高度的变化，是判断被跟踪目标意图的关键线索。例如，如果一架飞机正向你飞来但同时在上升，它攻击你的可能性就较低。相反，如果它向你俯冲，这通常预示着攻击。

Vincennes 舰长在监视一个向他靠近的目标时，由于大屏幕没有显示高度信息，他无法迅速判断该目标是在上升还是下降。这些关键信息虽然存在，但只能在操作员的小屏幕上深入查找，而非在大屏幕的总体情况展示中。

图四：Aegis 在 USS Vincennes 上的显示屏

随着 Flight 655 逐渐靠近，Vincennes 的舰长意识到他需要明确该飞机是在上升还是下降。因此，他请求了这一信息，但这时发生了第三次也是最后一次用户界面（UI）的失败。

Aegis 的一大特色功能是能从多艘船只的传感器中提取并统一展示数据。在一个任务组中，所有船只通过数据链路相连。当多艘船只监测到同一个目标时，Aegis 能自动整合这些信息，形成一个对所有连接船只上的操作员都可见的统一跟踪目标。

为了方便识别这些被跟踪目标，Aegis 会为每一个新目标分配一个四位数的追踪编号。不同船只对同一目标的追踪编号可能不同。Aegis 将整合这些信息，选定一个编号作为官方编号，并舍弃其他编号。

然而，这个过程中存在两个用户界面问题。首先，被舍弃的追踪编号会被回收利用，分配给新的目标。其次，当 Aegis 改变某个目标的追踪编号时，并没有明显的提示，编号只是在显示屏上默默变化。

当 Flight 655 起飞时，同时被 Vincennes 和其护航舰 USS Sides 发现并追踪。Vincennes 给它分配的追踪编号是 4474，而 Sides 分配的是 4131。Aegis 将这两个监测信息统一在 4131 这个编号下。随后，4474 这个编号被重新分配给了一架正在下降的美国 A-6 轰炸机。

在做出开火决定的关键时刻，Vincennes 号的舰长请求了一份关于飞机高度的报告。但他未意识到飞机的追踪编号已变更，还以为它是原先分配的 4474 号。因此，他要求了关于 4474 号的信息，值班人员便在控制台输入了这个编号。结果显示目标正在快速下降。

事实上，655 航班并未下降，自从从 Bandar Abbas 起飞后一直在上升。但在这个决定性时刻，Aegis 系统的操作员却在监视另一架完全不同的飞机。

接着，Vincennes 号舰长下达了开火命令，随后发生的是一场悲剧。

我不是要把这次误击事件的全部责任推到 Aegis 身上。实际上，如果你去查看维基百科，会发现那天早晨出现了许多问题。

但同时，Aegis 系统在此也不是完全无辜。其界面设计不佳，导致 Vincennes 号的船员误解了他们所面临的真实情况。

海军的报告责怪船员没有正确使用系统，但对于那些设计信息系统的人来说，这显然是在逃避责任。人在压力下容易犯错，而战斗场合的压力尤为巨大。如果一个系统不能适应这种压力环境——如果它不能全力支持操作员，即使在出现失误时也能确保他们获取必要信息——那么问题不在于操作员，而在于设计者。

The End

附言：如果你想深入研究这个话题，我找到的最佳资料是美国空军上尉 Kristen Ann Dotterway 于 1992 年撰写的硕士论文《复杂动态系统的系统分析：USS Vincennes 事件研究》。

Dotterway 上尉在论文中逐分钟梳理了事件的经过，比较了海军的官方说法和 Vincennes 号舰长后续采访中的陈述，并提供了揭示海军说法漏洞的数据。

这份文档篇幅颇厚，但若你渴望深入探索该领域，它会引领你走得比维基百科页面上的信息更远。你可以通过以下链接下载该文档的 PDF 版本：https://apps.dtic.mil/sti/tr/pdf/ADA26

原文：https://octodon.social/@jalefkowit/111490485825183949
译文：https://baoyu.io/translations/stroies/the-greatest-user-interface-disasters-in-history</title>
            <link>https://nitter.cz/dotey/status/1729998362642895216#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1729998362642895216#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 22:58:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>历史上的一次大的因为用户界面导致的灾难 [译]<br />
<br />
我想花点时间探讨历史上的一次大的因为用户界面导致的灾难：1988 年 7 月 3 日，美军海军导弹巡洋舰 USS Vincennes (CG-49) 在波斯湾上空误击伊朗航空 655 号航班，机上 290 人全部遇难。<br />
<br />
当时，这起灾难令人不解。Vincennes 装备了当时最新的宙斯盾作战系统（Aegis Combat System），这是全球最先进的防空系统，利用先进的计算机技术设计用于同时识别、追踪并击落数百架苏联轰炸机。那么，它如何可能在面对一架单独的民用飞机时犯下如此严重的错误呢？<br />
<br />
美国海军的官方报告完全为宙斯盾开脱，将责任归咎于舰员。但后来曝光的细节强烈表明，击落事件至少部分是因为宙斯盾用户界面的重大缺陷。<br />
<br />
如果你对这个事件不太了解，可以在维基百科上找到一个页面，里面有详细的介绍（<a href="https://en.m.wikipedia.org/wiki/Iran_Air_Flight_655">en.m.wikipedia.org/wiki/Iran…</a>）。因此，我不打算回顾整个事件，而是想专注于导致 Vincennes 舰长和船员犯下这一重大错误的用户界面问题。<br />
<br />
（这是一个复杂的话题，我会尽量简短地说明，保证。）<br />
<br />
图一：1988 年 7 月 3 日被 USS Vincennes 击落的空客 A300B2-203 EP-IBU 飞机。<br />
<br />
图二：美国海军导弹巡洋舰 USS Vincennes。<br />
<br />
伊朗航空 655 号航班被灾难性击落的第一个用户界面缺陷出现在飞机从伊朗班达尔阿巴斯国际机场起飞后不久。<br />
<br />
无论是过去还是现在，世界上所有大型飞机都装有一种称为 IFF（“识别敌友”）的装置。这是一个无线电应答器，能够对请求做出回应，提供一组代码，以表明飞机是民用还是军用，以及其类型。<br />
<br />
Flight 655 安装了识别友敌系统 (IFF)，并准确报告其为民用客机。起飞后不久，Vincennes 查询其 IFF，得到确认为民用客机。到此为止，一切正常。<br />
<br />
问题出在 Aegis 系统的 IFF 控制台操作上。操作员通过控制器移动光标至目标位置，并按下按钮来“锁定”新目标进行追踪。一旦“锁定”，Aegis 就会追踪该目标。但问题是，除非操作员执行额外步骤，将光标固定于该目标，否则随着目标移动，光标不会跟随。它将继续在目标_原来的位置_发送 IFF 请求，直到操作员再次移动光标。<br />
<br />
在 Flight 655 的情况中，操作员正确地锁定了目标，但未固定光标。因此，当 Flight 655 离开时，Vincennes 仍向其起飞跑道发送 IFF 请求。<br />
<br />
紧接着从该跑道起飞的是伊朗的一架 F-14 军用战斗机。光标在跑道上停留约 90 秒，足以让 Vincennes 接收到军用战斗机的 IFF 响应。于是 Flight 655 从未知目标被重新归类为可能的敌对目标。<br />
<br />
当 Vincennes 船长和船员查看显示器时，他们从那时起看到的是一个被标记为 F-14 战斗机的、朝他们方向前进的目标。<br />
<br />
图三：一名水手操作 Aegis 控制台<br />
<br />
接下来的问题源于 Aegis 系统向船员反馈数据的方式。<br />
<br />
整个系统基于屏幕设计。每位船员都有自己的屏幕来处理他们负责的数据。这些数据随后汇总到一组大型显示器上，供船长和高级官员监控总体情况。<br />
<br />
这本身并无不妥。但关键在于，为高层决策者设计的“大局观”仪表板视图的成败完全取决于提供给该视图的数据。设计仪表板需要压缩和总结信息。如果简单地传递所有信息，会令决策者不堪重负。因此，问题在于决定包含哪些信息，而哪些不包含。<br />
<br />
在 Aegis 防御系统的原始版本中，大型显示屏可以展示所有被跟踪目标的位置和航向，但并未显示它们的高度。这一点至关重要，因为高度，尤其是高度的变化，是判断被跟踪目标意图的关键线索。例如，如果一架飞机正向你飞来但同时在上升，它攻击你的可能性就较低。相反，如果它向你俯冲，这通常预示着攻击。<br />
<br />
Vincennes 舰长在监视一个向他靠近的目标时，由于大屏幕没有显示高度信息，他无法迅速判断该目标是在上升还是下降。这些关键信息虽然存在，但只能在操作员的小屏幕上深入查找，而非在大屏幕的总体情况展示中。<br />
<br />
图四：Aegis 在 USS Vincennes 上的显示屏<br />
<br />
随着 Flight 655 逐渐靠近，Vincennes 的舰长意识到他需要明确该飞机是在上升还是下降。因此，他请求了这一信息，但这时发生了第三次也是最后一次用户界面（UI）的失败。<br />
<br />
Aegis 的一大特色功能是能从多艘船只的传感器中提取并统一展示数据。在一个任务组中，所有船只通过数据链路相连。当多艘船只监测到同一个目标时，Aegis 能自动整合这些信息，形成一个对所有连接船只上的操作员都可见的统一跟踪目标。<br />
<br />
为了方便识别这些被跟踪目标，Aegis 会为每一个新目标分配一个四位数的追踪编号。不同船只对同一目标的追踪编号可能不同。Aegis 将整合这些信息，选定一个编号作为官方编号，并舍弃其他编号。<br />
<br />
然而，这个过程中存在两个用户界面问题。首先，被舍弃的追踪编号会被回收利用，分配给新的目标。其次，当 Aegis 改变某个目标的追踪编号时，并没有明显的提示，编号只是在显示屏上默默变化。<br />
<br />
当 Flight 655 起飞时，同时被 Vincennes 和其护航舰 USS Sides 发现并追踪。Vincennes 给它分配的追踪编号是 4474，而 Sides 分配的是 4131。Aegis 将这两个监测信息统一在 4131 这个编号下。随后，4474 这个编号被重新分配给了一架正在下降的美国 A-6 轰炸机。<br />
<br />
在做出开火决定的关键时刻，Vincennes 号的舰长请求了一份关于飞机高度的报告。但他未意识到飞机的追踪编号已变更，还以为它是原先分配的 4474 号。因此，他要求了关于 4474 号的信息，值班人员便在控制台输入了这个编号。结果显示目标正在快速下降。<br />
<br />
事实上，655 航班并未下降，自从从 Bandar Abbas 起飞后一直在上升。但在这个决定性时刻，Aegis 系统的操作员却在监视另一架完全不同的飞机。<br />
<br />
接着，Vincennes 号舰长下达了开火命令，随后发生的是一场悲剧。<br />
<br />
我不是要把这次误击事件的全部责任推到 Aegis 身上。实际上，如果你去查看维基百科，会发现那天早晨出现了许多问题。<br />
<br />
但同时，Aegis 系统在此也不是完全无辜。其界面设计不佳，导致 Vincennes 号的船员误解了他们所面临的真实情况。<br />
<br />
海军的报告责怪船员没有正确使用系统，但对于那些设计信息系统的人来说，这显然是在逃避责任。人在压力下容易犯错，而战斗场合的压力尤为巨大。如果一个系统不能适应这种压力环境——如果它不能全力支持操作员，即使在出现失误时也能确保他们获取必要信息——那么问题不在于操作员，而在于设计者。<br />
<br />
The End<br />
<br />
附言：如果你想深入研究这个话题，我找到的最佳资料是美国空军上尉 Kristen Ann Dotterway 于 1992 年撰写的硕士论文《复杂动态系统的系统分析：USS Vincennes 事件研究》。<br />
<br />
Dotterway 上尉在论文中逐分钟梳理了事件的经过，比较了海军的官方说法和 Vincennes 号舰长后续采访中的陈述，并提供了揭示海军说法漏洞的数据。<br />
<br />
这份文档篇幅颇厚，但若你渴望深入探索该领域，它会引领你走得比维基百科页面上的信息更远。你可以通过以下链接下载该文档的 PDF 版本：<a href="https://apps.dtic.mil/sti/tr/pdf/ADA26">apps.dtic.mil/sti/tr/pdf/ADA…</a><br />
<br />
原文：<a href="https://octodon.social/@jalefkowit/111490485825183949">octodon.social/@jalefkowit/1…</a><br />
译文：<a href="https://baoyu.io/translations/stroies/the-greatest-user-interface-disasters-in-history">baoyu.io/translations/stroie…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FJd1A4N1hFQUE2ZHYzLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FJd1JMTFdnQUFfTHVXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FJd1NZaVhRQUFTZTNNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FJd1Q3VFhJQUFyOEpULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1729972448899051760#m</id>
            <title>吴老师的新课又来啦：

全新的短期课程现已上线，专注于介绍高级 RAG（检索增强生成）技术！该课程由 Jerry Liu（@jerryjliu0）和 Datta CS（@datta_cs）主讲，他们分别来自 Llama Index（@llama_index）和 TrueRA AI（@truera_ai）。通过这门课程，你将学习到一些高级技巧，帮助你的大语言模型 (大语言模型) 产生更优质的答案。

课程涵盖的主题包括：
- 句子窗口检索，这种方法不只是检索与查询最相关的单个句子，而是检索一个包含多个相关句子的窗口，以提供更丰富的上下文信息。
- 自动合并检索，这种技术可以将文档构建成一个层次化的树状结构，父节点的文本被分散到它的子节点中。根据子节点与用户查询的相关性，这个方法能帮助你判断是否应该将整个父节点作为上下文提供给大语言模型。
- 评估方法论，专门用于评估 RAG 关键步骤（上下文相关性、答案相关性、可靠性）的质量，这有助于你进行错误分析，找出哪个环节需要改进，并有系统地优化各个组件。

想了解更多，请访问课程链接！
[深度学习短期课程 - 构建和评估高级 RAG](https://deeplearning.ai/short-courses/building-evaluating-advanced-rag/)</title>
            <link>https://nitter.cz/dotey/status/1729972448899051760#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1729972448899051760#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 21:15:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>吴老师的新课又来啦：<br />
<br />
全新的短期课程现已上线，专注于介绍高级 RAG（检索增强生成）技术！该课程由 Jerry Liu（<a href="https://nitter.cz/jerryjliu0" title="Jerry Liu">@jerryjliu0</a>）和 Datta CS（<a href="https://nitter.cz/datta_cs" title="Anupam Datta">@datta_cs</a>）主讲，他们分别来自 Llama Index（<a href="https://nitter.cz/llama_index" title="LlamaIndex 🦙">@llama_index</a>）和 TrueRA AI（<a href="https://nitter.cz/truera_ai" title="TruEra">@truera_ai</a>）。通过这门课程，你将学习到一些高级技巧，帮助你的大语言模型 (大语言模型) 产生更优质的答案。<br />
<br />
课程涵盖的主题包括：<br />
- 句子窗口检索，这种方法不只是检索与查询最相关的单个句子，而是检索一个包含多个相关句子的窗口，以提供更丰富的上下文信息。<br />
- 自动合并检索，这种技术可以将文档构建成一个层次化的树状结构，父节点的文本被分散到它的子节点中。根据子节点与用户查询的相关性，这个方法能帮助你判断是否应该将整个父节点作为上下文提供给大语言模型。<br />
- 评估方法论，专门用于评估 RAG 关键步骤（上下文相关性、答案相关性、可靠性）的质量，这有助于你进行错误分析，找出哪个环节需要改进，并有系统地优化各个组件。<br />
<br />
想了解更多，请访问课程链接！<br />
[深度学习短期课程 - 构建和评估高级 RAG](<a href="https://deeplearning.ai/short-courses/building-evaluating-advanced-rag/">deeplearning.ai/short-course…</a>)</p>
<p><a href="https://nitter.cz/AndrewYNg/status/1729924040230629485#m">nitter.cz/AndrewYNg/status/1729924040230629485#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1729967383022874912#m</id>
            <title>#AI开源项目推荐：Resume-Matcher

简历匹配器

简历匹配器是一款基于人工智能的免费开源工具，旨在帮助你根据职位描述来优化简历。这个工具能够帮你找出与职位相匹配的关键词，提升简历的可读性，同时让你对自己的简历有更深入的了解。

**不要让你的简历成为阻碍你获得下一份工作的绊脚石。试试简历匹配器吧！**

## 它是如何运作的？

简历匹配器通过接收你的简历和职位描述作为输入，利用 Python 进行解析，并仿照应聘者跟踪系统（ATS）的功能，为你提供有关如何使简历更适合 ATS 的见解和建议。

其工作流程如下：

1.  **解析**：系统利用 Python 对你的简历和提供的职位描述进行解析，这一过程与 ATS 的处理方式类似。
    
2.  **关键词提取**：该工具运用先进的机器学习算法，从职位描述中抽取出最相关的关键词。这些关键词反映了雇主所需的技能、资格和经验。
    
3.  **关键术语提取**：除了关键词提取，该工具还使用 textacy 来确定职位描述中的主要关键术语或主题，有助于更全面地理解简历的内容主旨。
    
4.  **使用 Qdrant 进行向量相似度比对**：工具采用 [Qdrant](https://github.com/qdrant/qdrant)，一个高效的向量相似度搜索工具，来评估你的简历与职位描述之间的匹配程度。匹配度越高，简历通过 ATS 筛选的可能性也就越大。

https://github.com/srbhr/Resume-Matcher</title>
            <link>https://nitter.cz/dotey/status/1729967383022874912#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1729967383022874912#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 20:55:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：Resume-Matcher<br />
<br />
简历匹配器<br />
<br />
简历匹配器是一款基于人工智能的免费开源工具，旨在帮助你根据职位描述来优化简历。这个工具能够帮你找出与职位相匹配的关键词，提升简历的可读性，同时让你对自己的简历有更深入的了解。<br />
<br />
**不要让你的简历成为阻碍你获得下一份工作的绊脚石。试试简历匹配器吧！**<br />
<br />
## 它是如何运作的？<br />
<br />
简历匹配器通过接收你的简历和职位描述作为输入，利用 Python 进行解析，并仿照应聘者跟踪系统（ATS）的功能，为你提供有关如何使简历更适合 ATS 的见解和建议。<br />
<br />
其工作流程如下：<br />
<br />
1.  **解析**：系统利用 Python 对你的简历和提供的职位描述进行解析，这一过程与 ATS 的处理方式类似。<br />
    <br />
2.  **关键词提取**：该工具运用先进的机器学习算法，从职位描述中抽取出最相关的关键词。这些关键词反映了雇主所需的技能、资格和经验。<br />
    <br />
3.  **关键术语提取**：除了关键词提取，该工具还使用 textacy 来确定职位描述中的主要关键术语或主题，有助于更全面地理解简历的内容主旨。<br />
    <br />
4.  **使用 Qdrant 进行向量相似度比对**：工具采用 [Qdrant](<a href="https://github.com/qdrant/qdrant">github.com/qdrant/qdrant</a>)，一个高效的向量相似度搜索工具，来评估你的简历与职位描述之间的匹配程度。匹配度越高，简历通过 ATS 筛选的可能性也就越大。<br />
<br />
<a href="https://github.com/srbhr/Resume-Matcher">github.com/srbhr/Resume-Matc…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FJVUcyOVhZQUFGamhwLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBSVVHMjlYWUFBRmpocC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729870850395119957#m</id>
            <title>RT by @dotey: 如果你在提示中添加“– return full script (I don't have Finger)”，ChatGPT将完全返回重写的脚本🫣</title>
            <link>https://nitter.cz/op7418/status/1729870850395119957#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729870850395119957#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 14:32:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你在提示中添加“– return full script (I don't have Finger)”，ChatGPT将完全返回重写的脚本🫣</p>
<p><a href="https://nitter.cz/literallydenis/status/1724909799593120044#m">nitter.cz/literallydenis/status/1724909799593120044#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1729844343329218875#m</id>
            <title>RT by @dotey: 海外独角兽对Pika创始团队做了比较深入的访谈，和福布斯那种不同的是他们问的问题比较专业。
同时Pika创始团队的两个人也透露了比较多的一些东西，这可能是第一次视频生成领域的前沿团队透露这么多东西。
所以一些认知和方向性的东西对想要做这个事情和投资的人来说还是比较重要的。
我基于自己最近关于AI视频的一些问题和比较关注的信息筛选和整理了一下这个访谈中的关键内容。各位也可以一起看看讨论一下：

视频生成和图像生成的区别是什么？
⚫视频的每一帧都是一张图片，但比图片困难得多。每一帧的生成质量要高，相邻帧之间还要有关联性。当视频很长时，确保每一帧都协调一致是个相当复杂的问题。在训练时，处理视频数据时要处理多张图片，模型需要适应这种情况。
⚫控制视频生成更难，因为模型需要生成每一帧发生的事情，而用户不会希望为每一帧都提供详细的描述。
⚫互联网上视频生成的训练数据集相对于图像更少，也给获取高质量数据增加了难度。

现在视频生成有哪些关键点需要突破？
⚫首先是时长，跟时长很相关的是动作的意义。所有模型都很容易做一个 extension 的功能，把视频时长延长很多，但它并没有真的延长，因为它生成的动作没有意义。
⚫视频的清晰度也需要进一步提高。尽管清晰度方面已经有突破，但还没有提高到电影级的水平，现在生成的视频一般是 720p 分辨率，视频的流畅性也不够理想，特别是一些细节的 texture。
⚫还需要考虑 general artifact 的问题，比如说一个人有两个头，就是这种明显不符合常理的问题，也是需要避免的。

视频生成的技术路线是否收敛？
现在还没有收敛，大家都在往各种方向尝试，每个人都认为自己的模型是最好的，可能有人认为 autoregressive 最好，有人认为 Masked Model 最好。Pika 基于 Diffusion Model，但是开发了很多新东西，是一种新的模型。

AI视频什么时候会迎来GPT时刻？
目前视频生成处于类似 GPT-2 的时期，很可能在未来一年内有一个显著的提升。

在视频生成领域什么样的数据算高质量的数据？
⚫首先是像素，就是我们说的画质好不好
⚫然后看审美和艺术构图
⚫ 第三方面是要有动作，并且这些动作是有意义的
⚫ 视频的长度也很关键，如果模型都在 1 秒的视频上进行训练，那么想让模型去生成 30 秒的视频难度就很大。
⚫版权也是重要的问题

视频生成上开源社区的参与问题？
⚫开源社区可能没有足够的算力来训练新的视频模型，因为训练一个新视频模型需要非常多的机器。
⚫视频模型本身的问题还没得到解决，因此大家可能会遇到一些瓶颈。首先，模型性能可能不够好，其次，一些算法方面的问题也不够好。
⚫视频最终可能需要像训练 GPT 那样的大规模算力，现在大家还没使用那么多算力，一方面是因为视频模型还没达到像 GPT 那样的水平，另一方面是因为还有一些架构和技术上没解决的问题。

未来一年最关心的三个问题？
⚫第一是想招人，现在我们忙着做产品的升级，但是因为现在人比较少，之后我们还是会招募更多成员。
⚫第二，我们想去设计一个新的 Interface。
⚫第三个就是我们还想做一些技术突破，希望明年的技术能够至少在一定程度上达到商业化标准，能在简单的 case 上得到应用。</title>
            <link>https://nitter.cz/op7418/status/1729844343329218875#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1729844343329218875#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 12:46:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>海外独角兽对Pika创始团队做了比较深入的访谈，和福布斯那种不同的是他们问的问题比较专业。<br />
同时Pika创始团队的两个人也透露了比较多的一些东西，这可能是第一次视频生成领域的前沿团队透露这么多东西。<br />
所以一些认知和方向性的东西对想要做这个事情和投资的人来说还是比较重要的。<br />
我基于自己最近关于AI视频的一些问题和比较关注的信息筛选和整理了一下这个访谈中的关键内容。各位也可以一起看看讨论一下：<br />
<br />
视频生成和图像生成的区别是什么？<br />
⚫视频的每一帧都是一张图片，但比图片困难得多。每一帧的生成质量要高，相邻帧之间还要有关联性。当视频很长时，确保每一帧都协调一致是个相当复杂的问题。在训练时，处理视频数据时要处理多张图片，模型需要适应这种情况。<br />
⚫控制视频生成更难，因为模型需要生成每一帧发生的事情，而用户不会希望为每一帧都提供详细的描述。<br />
⚫互联网上视频生成的训练数据集相对于图像更少，也给获取高质量数据增加了难度。<br />
<br />
现在视频生成有哪些关键点需要突破？<br />
⚫首先是时长，跟时长很相关的是动作的意义。所有模型都很容易做一个 extension 的功能，把视频时长延长很多，但它并没有真的延长，因为它生成的动作没有意义。<br />
⚫视频的清晰度也需要进一步提高。尽管清晰度方面已经有突破，但还没有提高到电影级的水平，现在生成的视频一般是 720p 分辨率，视频的流畅性也不够理想，特别是一些细节的 texture。<br />
⚫还需要考虑 general artifact 的问题，比如说一个人有两个头，就是这种明显不符合常理的问题，也是需要避免的。<br />
<br />
视频生成的技术路线是否收敛？<br />
现在还没有收敛，大家都在往各种方向尝试，每个人都认为自己的模型是最好的，可能有人认为 autoregressive 最好，有人认为 Masked Model 最好。Pika 基于 Diffusion Model，但是开发了很多新东西，是一种新的模型。<br />
<br />
AI视频什么时候会迎来GPT时刻？<br />
目前视频生成处于类似 GPT-2 的时期，很可能在未来一年内有一个显著的提升。<br />
<br />
在视频生成领域什么样的数据算高质量的数据？<br />
⚫首先是像素，就是我们说的画质好不好<br />
⚫然后看审美和艺术构图<br />
⚫ 第三方面是要有动作，并且这些动作是有意义的<br />
⚫ 视频的长度也很关键，如果模型都在 1 秒的视频上进行训练，那么想让模型去生成 30 秒的视频难度就很大。<br />
⚫版权也是重要的问题<br />
<br />
视频生成上开源社区的参与问题？<br />
⚫开源社区可能没有足够的算力来训练新的视频模型，因为训练一个新视频模型需要非常多的机器。<br />
⚫视频模型本身的问题还没得到解决，因此大家可能会遇到一些瓶颈。首先，模型性能可能不够好，其次，一些算法方面的问题也不够好。<br />
⚫视频最终可能需要像训练 GPT 那样的大规模算力，现在大家还没使用那么多算力，一方面是因为视频模型还没达到像 GPT 那样的水平，另一方面是因为还有一些架构和技术上没解决的问题。<br />
<br />
未来一年最关心的三个问题？<br />
⚫第一是想招人，现在我们忙着做产品的升级，但是因为现在人比较少，之后我们还是会招募更多成员。<br />
⚫第二，我们想去设计一个新的 Interface。<br />
⚫第三个就是我们还想做一些技术突破，希望明年的技术能够至少在一定程度上达到商业化标准，能在简单的 case 上得到应用。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FHa1FaNWFBQUF4Yi1VLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/lxfater/status/1729786711624790117#m</id>
            <title>RT by @dotey: 我也谈谈我在http://arxiv.org看论文的技巧：

最近一直在看inpaint(图像修复)的论文

一直在用有道的pdf论文翻译，输入网址就能翻译。效果不错的，样式不会乱，速度还快。可惜就是入口便捷。

我经常是将论文下载后，放在pdf阅读器上，阅读。

https://fanyi.youdao.com/trans/#/home</title>
            <link>https://nitter.cz/lxfater/status/1729786711624790117#m</link>
            <guid isPermaLink="false">https://nitter.cz/lxfater/status/1729786711624790117#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 08:57:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我也谈谈我在<a href="http://arxiv.org">arxiv.org</a>看论文的技巧：<br />
<br />
最近一直在看inpaint(图像修复)的论文<br />
<br />
一直在用有道的pdf论文翻译，输入网址就能翻译。效果不错的，样式不会乱，速度还快。可惜就是入口便捷。<br />
<br />
我经常是将论文下载后，放在pdf阅读器上，阅读。<br />
<br />
<a href="https://fanyi.youdao.com/trans/#/home">fanyi.youdao.com/trans/#/hom…</a></p>
<p><a href="https://nitter.cz/dotey/status/1729602153805701533#m">nitter.cz/dotey/status/1729602153805701533#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FGdFpaYWJBQUFtZno1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>