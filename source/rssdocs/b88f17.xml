<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737683961428717693#m</id>
            <title>RT by @dotey: 兄弟们，又有人要失业了🫣

Text-to-CAD ：通过文本提示生成 CAD文件。

只需要输入自然语言描述，它就能根据这些描述创建相应的 B-Rep CAD 文件和网格模型。

生成的模型可以导入到用户选择的任何 CAD 程序中。

Text-to-CAD 背后的基础设施利用了 Zoo 的设计 API 和机器学习 API。

这些 API 能够程序化地分析训练数据，并生成 CAD 文件。

体验地址：https://zoo.dev/text-to-cad
API申请：https://zoo.dev/machine-learning-api</title>
            <link>https://nitter.cz/xiaohuggg/status/1737683961428717693#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737683961428717693#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 03:58:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，又有人要失业了🫣<br />
<br />
Text-to-CAD ：通过文本提示生成 CAD文件。<br />
<br />
只需要输入自然语言描述，它就能根据这些描述创建相应的 B-Rep CAD 文件和网格模型。<br />
<br />
生成的模型可以导入到用户选择的任何 CAD 程序中。<br />
<br />
Text-to-CAD 背后的基础设施利用了 Zoo 的设计 API 和机器学习 API。<br />
<br />
这些 API 能够程序化地分析训练数据，并生成 CAD 文件。<br />
<br />
体验地址：<a href="https://zoo.dev/text-to-cad">zoo.dev/text-to-cad</a><br />
API申请：<a href="https://zoo.dev/machine-learning-api">zoo.dev/machine-learning-api</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2ODM2NDc2NTI4NjgwOTYvcHUvaW1nL3FMWnFXTmtNYmdneG5WeEUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737658820598398985#m</id>
            <title>HuggingFace 官方博客上的一篇文章：《Speculative Decoding for 2x Faster Whisper Inference | 推测性解码：实现 Whisper 推理速度提升两倍 [译]》

在这篇文章中，展示了如何应用“猜测式解码”(Speculative Decoding) 技术来减少 Whisper 语音识别模型的处理时间，实现了处理速度的 两倍提升，同时数学上保证了模型输出的 完全一致性。因此，这一方法可以无缝替代现有的 Whisper 处理流程，不仅保持了原有的准确性，还能实现处理速度的双倍快速提升。

简单来说，Speculative Decoding就是先利用一个快速的 Assistant 模型生成候选tokens，再用 Main 模型验证。

Assistant 的速度是 Main 模型的 3 倍，但准确率只有 70% - 80%。

使用这种方法可以让整体速度提升 2 倍，并且保证输出完全一致。

文章还提供了Google Colab的测试连接：https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/speculative_decoding.ipynb

原文：https://huggingface.co/blog/whisper-speculative-decoding#english-speech-transcription
译文：https://baoyu.io/translations/huggingface/whisper-speculative-decoding</title>
            <link>https://nitter.cz/dotey/status/1737658820598398985#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737658820598398985#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:18:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HuggingFace 官方博客上的一篇文章：《Speculative Decoding for 2x Faster Whisper Inference | 推测性解码：实现 Whisper 推理速度提升两倍 [译]》<br />
<br />
在这篇文章中，展示了如何应用“猜测式解码”(Speculative Decoding) 技术来减少 Whisper 语音识别模型的处理时间，实现了处理速度的 两倍提升，同时数学上保证了模型输出的 完全一致性。因此，这一方法可以无缝替代现有的 Whisper 处理流程，不仅保持了原有的准确性，还能实现处理速度的双倍快速提升。<br />
<br />
简单来说，Speculative Decoding就是先利用一个快速的 Assistant 模型生成候选tokens，再用 Main 模型验证。<br />
<br />
Assistant 的速度是 Main 模型的 3 倍，但准确率只有 70% - 80%。<br />
<br />
使用这种方法可以让整体速度提升 2 倍，并且保证输出完全一致。<br />
<br />
文章还提供了Google Colab的测试连接：<a href="https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/speculative_decoding.ipynb">colab.research.google.com/gi…</a><br />
<br />
原文：<a href="https://huggingface.co/blog/whisper-speculative-decoding#english-speech-transcription">huggingface.co/blog/whisper-…</a><br />
译文：<a href="https://baoyu.io/translations/huggingface/whisper-speculative-decoding">baoyu.io/translations/huggin…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxbmNFeFd3QUFHOXY5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737657020587630635#m</id>
            <title>终于放出来了！</title>
            <link>https://nitter.cz/dotey/status/1737657020587630635#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737657020587630635#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:11:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于放出来了！</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737647067693211728#m">nitter.cz/xiaohuggg/status/1737647067693211728#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/HiTw93/status/1737618885220647160#m</id>
            <title>RT by @dotey: #工程师学习 这个《动手实战人工智能 Hands-on AI》写得很用心，作者从监督学习开始，带你入门机器学习和深度学习，他尝试剖析和推导每一个基础算法的原理，将数学过程写出来，同时基于 Python 代码对公式进行实现，做到公式和代码的一一对应。
🤖 https://ai.huhuhang.com/</title>
            <link>https://nitter.cz/HiTw93/status/1737618885220647160#m</link>
            <guid isPermaLink="false">https://nitter.cz/HiTw93/status/1737618885220647160#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 23:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23工程师学习">#工程师学习</a> 这个《动手实战人工智能 Hands-on AI》写得很用心，作者从监督学习开始，带你入门机器学习和深度学习，他尝试剖析和推导每一个基础算法的原理，将数学过程写出来，同时基于 Python 代码对公式进行实现，做到公式和代码的一一对应。<br />
🤖 <a href="https://ai.huhuhang.com/">ai.huhuhang.com/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JqUGR3cmF3QUFNZlJ0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737629664493863049#m</id>
            <title>R to @dotey: 完整截图，翻译的这篇文章是自然杂志的一篇：《为何有科学家不用 ChatGPT？他们这样说 [译]》
https://baoyu.io/translations/ai/these-scientists-arent-using-chatgpt-here-is-why</title>
            <link>https://nitter.cz/dotey/status/1737629664493863049#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737629664493863049#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 00:22:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>完整截图，翻译的这篇文章是自然杂志的一篇：《为何有科学家不用 ChatGPT？他们这样说 [译]》<br />
<a href="https://baoyu.io/translations/ai/these-scientists-arent-using-chatgpt-here-is-why">baoyu.io/translations/ai/the…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxTXFTdVhFQUFScllaLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737627736389075377#m</id>
            <title>R to @dotey: Prompt：

你是一位精通简体中文的专业翻译，尤其擅长将专业学术论文翻译成浅显易懂的科普文章。请你帮我将以下英文段落翻译成中文，风格与中文科普读物相似。

规则：
- 翻译时要准确传达原文的事实和背景。
- 即使上意译也要保留原始段落格式，以及保留术语，例如 FLAC，JPEG 等。保留公司缩写，例如 Microsoft, Amazon, OpenAI 等。
- 人名不翻译
- 同时要保留引用的论文，例如 [20] 这样的引用。
- 对于 Figure 和 Table，翻译的同时保留原有格式，例如：“Figure 1: ”翻译为“图 1: ”，“Table 1: ”翻译为：“表 1: ”。
- 全角括号换成半角括号，并在左括号前面加半角空格，右括号后面加半角空格。
- 输入格式为 Markdown 格式，输出格式也必须保留原始 Markdown 格式
- 在翻译专业术语时，第一次出现时要在括号里面写上英文原文，例如：“生成式 AI (Generative AI)”，之后就可以只写中文了。
- 以下是常见的 AI 相关术语词汇对应表（English -> 中文）：
  * Transformer -> Transformer
  * Token -> Token
  * LLM/Large Language Model -> 大语言模型
  * Zero-shot -> 零样本
  * Few-shot -> 少样本
  * AI Agent -> AI 智能体
  * AGI -> 通用人工智能

策略：

分三步进行翻译工作，并打印每步的结果：
1. 根据英文内容直译，保持原有格式，不要遗漏任何信息
2. 根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于：
  - 不符合中文表达习惯，明确指出不符合的地方
  - 语句不通顺，指出位置，不需要给出修改意见，意译时修复
  - 晦涩难懂，不易理解，可以尝试给出解释
3. 根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变

返回格式如下，"{xxx}"表示占位符：

### 直译
{直译结果}

***

### 问题
{直译的具体问题列表}

***

### 意译
```
{意译结果}
```

现在请按照上面的要求从第一行开始翻译以下内容为简体中文：
```</title>
            <link>https://nitter.cz/dotey/status/1737627736389075377#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737627736389075377#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 00:15:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Prompt：<br />
<br />
你是一位精通简体中文的专业翻译，尤其擅长将专业学术论文翻译成浅显易懂的科普文章。请你帮我将以下英文段落翻译成中文，风格与中文科普读物相似。<br />
<br />
规则：<br />
- 翻译时要准确传达原文的事实和背景。<br />
- 即使上意译也要保留原始段落格式，以及保留术语，例如 FLAC，JPEG 等。保留公司缩写，例如 Microsoft, Amazon, OpenAI 等。<br />
- 人名不翻译<br />
- 同时要保留引用的论文，例如 [20] 这样的引用。<br />
- 对于 Figure 和 Table，翻译的同时保留原有格式，例如：“Figure 1: ”翻译为“图 1: ”，“Table 1: ”翻译为：“表 1: ”。<br />
- 全角括号换成半角括号，并在左括号前面加半角空格，右括号后面加半角空格。<br />
- 输入格式为 Markdown 格式，输出格式也必须保留原始 Markdown 格式<br />
- 在翻译专业术语时，第一次出现时要在括号里面写上英文原文，例如：“生成式 AI (Generative AI)”，之后就可以只写中文了。<br />
- 以下是常见的 AI 相关术语词汇对应表（English -> 中文）：<br />
  * Transformer -> Transformer<br />
  * Token -> Token<br />
  * LLM/Large Language Model -> 大语言模型<br />
  * Zero-shot -> 零样本<br />
  * Few-shot -> 少样本<br />
  * AI Agent -> AI 智能体<br />
  * AGI -> 通用人工智能<br />
<br />
策略：<br />
<br />
分三步进行翻译工作，并打印每步的结果：<br />
1. 根据英文内容直译，保持原有格式，不要遗漏任何信息<br />
2. 根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于：<br />
  - 不符合中文表达习惯，明确指出不符合的地方<br />
  - 语句不通顺，指出位置，不需要给出修改意见，意译时修复<br />
  - 晦涩难懂，不易理解，可以尝试给出解释<br />
3. 根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变<br />
<br />
返回格式如下，"{xxx}"表示占位符：<br />
<br />
### 直译<br />
{直译结果}<br />
<br />
***<br />
<br />
### 问题<br />
{直译的具体问题列表}<br />
<br />
***<br />
<br />
### 意译<br />
```<br />
{意译结果}<br />
```<br />
<br />
现在请按照上面的要求从第一行开始翻译以下内容为简体中文：<br />
```</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737627478007456183#m</id>
            <title>我最近对我的翻译 GPT https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi 做了一点优化，将原来的直译->意译两步拆成了三步：
1. 直译
2. 指出直译中的问题，例如：“不符合中文表达习惯”、“语句不通顺”和“晦涩难懂”，并且指出位置或者给出解释
3. 根据直译和问题进行意译

经过我几天的测试下来，效果确实要更好，因为通过指出问题，可以将问题具体化，由于提供了更多上下文，得到了更好的结果。

举一个例子，我翻译的一段英文其中有这样一句话：“I enjoy writing. I do it in a fast way. Why am I even a researcher if I don’t write my own research?”

之前用两步，意译后得到的结果是：“我热爱写作，写得也很迅速。如果我不亲自完成我的研究写作，我还算什么研究员呢？”（参考图二）

新的Prompt中，在第二步中发现了问题，指出：
> 第一段中，“I do it in a fast way” 直译为 “我写得很快” 可能不够准确，原句的含义可能更接近于“我享受写作，这是一种迅速的表达方式”。

最后意译时给出了翻译：
> “我喜欢写作，这是我快速表达思维的方式。如果我连自己的研究都不亲自写，我还算什么研究者？”

可以对比“I do it in a fast way.”部分的翻译结果：
“我热爱写作，写得也很迅速。” 
vs
“我喜欢写作，这是我快速表达思维的方式。”

可以看到翻译质量确实有提升。当然缺点是更费 Token 了一些，往好处想想少了更多人工校对的时间！

更新后Prompt见评论</title>
            <link>https://nitter.cz/dotey/status/1737627478007456183#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737627478007456183#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 00:14:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我最近对我的翻译 GPT <a href="https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi">chat.openai.com/g/g-uBhKUJJT…</a> 做了一点优化，将原来的直译->意译两步拆成了三步：<br />
1. 直译<br />
2. 指出直译中的问题，例如：“不符合中文表达习惯”、“语句不通顺”和“晦涩难懂”，并且指出位置或者给出解释<br />
3. 根据直译和问题进行意译<br />
<br />
经过我几天的测试下来，效果确实要更好，因为通过指出问题，可以将问题具体化，由于提供了更多上下文，得到了更好的结果。<br />
<br />
举一个例子，我翻译的一段英文其中有这样一句话：“I enjoy writing. I do it in a fast way. Why am I even a researcher if I don’t write my own research?”<br />
<br />
之前用两步，意译后得到的结果是：“我热爱写作，写得也很迅速。如果我不亲自完成我的研究写作，我还算什么研究员呢？”（参考图二）<br />
<br />
新的Prompt中，在第二步中发现了问题，指出：<br />
> 第一段中，“I do it in a fast way” 直译为 “我写得很快” 可能不够准确，原句的含义可能更接近于“我享受写作，这是一种迅速的表达方式”。<br />
<br />
最后意译时给出了翻译：<br />
> “我喜欢写作，这是我快速表达思维的方式。如果我连自己的研究都不亲自写，我还算什么研究者？”<br />
<br />
可以对比“I do it in a fast way.”部分的翻译结果：<br />
“我热爱写作，写得也很迅速。” <br />
vs<br />
“我喜欢写作，这是我快速表达思维的方式。”<br />
<br />
可以看到翻译质量确实有提升。当然缺点是更费 Token 了一些，往好处想想少了更多人工校对的时间！<br />
<br />
更新后Prompt见评论</p>
<p><a href="https://nitter.cz/dotey/status/1737512577641390311#m">nitter.cz/dotey/status/1737512577641390311#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxSVNCdlhrQUFvVmRwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxSnJoblgwQUFIX3RWLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxS3FFTlhJQUFoZVBJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737601108212691370#m</id>
            <title>简要总结一下OpenAI这条推文：
American Journalism Project (@JournalismProj) 是一个专注于地方新闻的风险投资慈善机构。OpenAI 通过与其合作，来试验和探索各种 AI 在地方新闻事业中的应用。

具体来说：

*   **Centro de Periodismo Investigativo**（波多黎各）将启动一个试点项目，测试 AI 在西班牙语与英语互译方面的能力。CPI 也将与语言专家合作进行质量控制。他们的目标是更高效地制作更多双语报道，增强他们与加勒比海和美国的英语合作伙伴进行调查合作的能力，并扩大对波多黎各调查新闻的支持，为美国本土的波多黎各人提供服务。

*   **THE CITY**（纽约）将尝试使用以 AI 为驱动的工具，专注于提高受众参与度，帮助他们筛选在线信息，回答纽约人的问题，并接收读者的建议。他们还将探索让记者无需编码知识也能使用 AI 分析数据的方法。THE CITY 还将研究 AI 如何帮助他们与提供报道线索的读者互动，通过 AI 生成的初始问题激励读者提交更完整、详细的信息，这将使新闻室能够将时间和资源集中在处理线索的更复杂阶段。

*   **inewsource**（圣地亚哥）将尝试使用 AI 技术和工具更快地处理更多公共记录请求，覆盖更多公共机构。他们还将探索 AI 如何帮助分析这些请求所产生的大量文件。

*   **Outlier**（底特律）使用 SMS 平台作为其服务型新闻传播的核心。除了发布 Outlier 的作品，该平台还使其能够与底特律居民进行一对一交流，这些交流通常有助于设定 Outler 的编辑策略，方法是回答居民的问题。Outlier 将尝试将 AI 技术融入其平台和工作流程，以便吸引更多人，更好地了解受众，并与服务的社区建立更深入、更有价值的直接关系。

*   **Cityside**（加利福尼亚湾区）将评估和实施与收入相关的 AI 实验，目的是学习如何在新闻机构的商业侧使用 AI。Cityside 将优先考虑使用 AI 辅助的通信来发展不同捐赠层级上的个人捐赠者关系。

很有意思的尝试！</title>
            <link>https://nitter.cz/dotey/status/1737601108212691370#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737601108212691370#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 22:29:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>简要总结一下OpenAI这条推文：<br />
American Journalism Project (<a href="https://nitter.cz/JournalismProj" title="American Journalism Project">@JournalismProj</a>) 是一个专注于地方新闻的风险投资慈善机构。OpenAI 通过与其合作，来试验和探索各种 AI 在地方新闻事业中的应用。<br />
<br />
具体来说：<br />
<br />
*   **Centro de Periodismo Investigativo**（波多黎各）将启动一个试点项目，测试 AI 在西班牙语与英语互译方面的能力。CPI 也将与语言专家合作进行质量控制。他们的目标是更高效地制作更多双语报道，增强他们与加勒比海和美国的英语合作伙伴进行调查合作的能力，并扩大对波多黎各调查新闻的支持，为美国本土的波多黎各人提供服务。<br />
<br />
*   **THE CITY**（纽约）将尝试使用以 AI 为驱动的工具，专注于提高受众参与度，帮助他们筛选在线信息，回答纽约人的问题，并接收读者的建议。他们还将探索让记者无需编码知识也能使用 AI 分析数据的方法。THE CITY 还将研究 AI 如何帮助他们与提供报道线索的读者互动，通过 AI 生成的初始问题激励读者提交更完整、详细的信息，这将使新闻室能够将时间和资源集中在处理线索的更复杂阶段。<br />
<br />
*   **inewsource**（圣地亚哥）将尝试使用 AI 技术和工具更快地处理更多公共记录请求，覆盖更多公共机构。他们还将探索 AI 如何帮助分析这些请求所产生的大量文件。<br />
<br />
*   **Outlier**（底特律）使用 SMS 平台作为其服务型新闻传播的核心。除了发布 Outlier 的作品，该平台还使其能够与底特律居民进行一对一交流，这些交流通常有助于设定 Outler 的编辑策略，方法是回答居民的问题。Outlier 将尝试将 AI 技术融入其平台和工作流程，以便吸引更多人，更好地了解受众，并与服务的社区建立更深入、更有价值的直接关系。<br />
<br />
*   **Cityside**（加利福尼亚湾区）将评估和实施与收入相关的 AI 实验，目的是学习如何在新闻机构的商业侧使用 AI。Cityside 将优先考虑使用 AI 辅助的通信来发展不同捐赠层级上的个人捐赠者关系。<br />
<br />
很有意思的尝试！</p>
<p><a href="https://nitter.cz/OpenAI/status/1737577619636596885#m">nitter.cz/OpenAI/status/1737577619636596885#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Lakr233/status/1737483448288170454#m</id>
            <title>RT by @dotey: 研究了一下他这个手势是怎么识别的，感觉我已经完全弄懂了！#Swift Tip</title>
            <link>https://nitter.cz/Lakr233/status/1737483448288170454#m</link>
            <guid isPermaLink="false">https://nitter.cz/Lakr233/status/1737483448288170454#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 14:41:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>研究了一下他这个手势是怎么识别的，感觉我已经完全弄懂了！<a href="https://nitter.cz/search?q=%23Swift">#Swift</a> Tip</p>
<p><a href="https://nitter.cz/s1ntone/status/1737418763925123086#m">nitter.cz/s1ntone/status/1737418763925123086#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc0ODM0MjYzMjI2Nzc3NjAvcHUvaW1nL0RWcll5aFh0UlZnMmdWWDIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737578011426205827#m</id>
            <title>Anthropic 真是自己作，过于追求安全对齐，而不关心开发者和用户感受……</title>
            <link>https://nitter.cz/dotey/status/1737578011426205827#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737578011426205827#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 20:57:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic 真是自己作，过于追求安全对齐，而不关心开发者和用户感受……</p>
<p><a href="https://nitter.cz/bindureddy/status/1737561873216938110#m">nitter.cz/bindureddy/status/1737561873216938110#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737562311420682495#m</id>
            <title>R to @dotey: 除了Phi-2，这里还有更多模型可以测试，Mistral-7B，Mistral-8x7B（如果配置够好）都值得试试

https://x.com/reach_vb/status/1737559520866636061?s=20</title>
            <link>https://nitter.cz/dotey/status/1737562311420682495#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737562311420682495#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:55:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>除了Phi-2，这里还有更多模型可以测试，Mistral-7B，Mistral-8x7B（如果配置够好）都值得试试<br />
<br />
<a href="https://x.com/reach_vb/status/1737559520866636061?s=20">x.com/reach_vb/status/173755…</a></p>
<p><a href="https://nitter.cz/reach_vb/status/1737559520866636061#m">nitter.cz/reach_vb/status/1737559520866636061#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737561925016330649#m</id>
            <title>不知道，帮转</title>
            <link>https://nitter.cz/dotey/status/1737561925016330649#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737561925016330649#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:53:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不知道，帮转</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737501193058877916#m">nitter.cz/xiaohuggg/status/1737501193058877916#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737429065949479003#m</id>
            <title>RT by @dotey: 还能这么玩，自己雕刻了一个低分辨率模型，然后利用Magnific AI一直放大，实现了前几天类似谷歌那个无限放大项目的效果。</title>
            <link>https://nitter.cz/op7418/status/1737429065949479003#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737429065949479003#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 11:05:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还能这么玩，自己雕刻了一个低分辨率模型，然后利用Magnific AI一直放大，实现了前几天类似谷歌那个无限放大项目的效果。</p>
<p><a href="https://nitter.cz/MartinNebelong/status/1737404691581898826#m">nitter.cz/MartinNebelong/status/1737404691581898826#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737549740592488871#m</id>
            <title>在 Mac 上借助 MLX 运行微软的小语言模型 Phi-2 的教程，很详细。
Phi-2 对资源要求不高，能力也还不错（详见：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/），有兴趣的可以试试。</title>
            <link>https://nitter.cz/dotey/status/1737549740592488871#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737549740592488871#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:05:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在 Mac 上借助 MLX 运行微软的小语言模型 Phi-2 的教程，很详细。<br />
Phi-2 对资源要求不高，能力也还不错（详见：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a>），有兴趣的可以试试。</p>
<p><a href="https://nitter.cz/reach_vb/status/1737541383399895360#m">nitter.cz/reach_vb/status/1737541383399895360#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNzEyNTkyNzY0ODExNjczNi9qMF9KSkZjeD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737492846041530578#m</id>
            <title>RT by @dotey: 刚看到苹果发的这个论文《使用有限的内存实现更快的 LLM 推理》。通过将将模型参数保存在闪存里，根据需要移动到DRAM。

使得能够运行的模型大小是可用DRAM的两倍，与传统的CPU和GPU加载方法相比，推理速度分别提高了4-5倍和20-25倍。</title>
            <link>https://nitter.cz/op7418/status/1737492846041530578#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737492846041530578#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 15:19:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚看到苹果发的这个论文《使用有限的内存实现更快的 LLM 推理》。通过将将模型参数保存在闪存里，根据需要移动到DRAM。<br />
<br />
使得能够运行的模型大小是可用DRAM的两倍，与传统的CPU和GPU加载方法相比，推理速度分别提高了4-5倍和20-25倍。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1737300118070534468#m">nitter.cz/_akhaliq/status/1737300118070534468#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</id>
            <title>RT by @dotey: XHS-Downloader：小红书采集器 

✅ 采集小红书图文/视频作品信息
✅ 提取小红书图文/视频作品下载地址
✅ 下载小红书无水印图文/视频作品文件
✅ 自动跳过已下载的作品文件
✅ 作品文件完整性处理机制
✅ 持久化储存作品信息至文件

GitHub：https://github.com/JoeanAmier/XHS-Downloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737460981331288421#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XHS-Downloader：小红书采集器 <br />
<br />
✅ 采集小红书图文/视频作品信息<br />
✅ 提取小红书图文/视频作品下载地址<br />
✅ 下载小红书无水印图文/视频作品文件<br />
✅ 自动跳过已下载的作品文件<br />
✅ 作品文件完整性处理机制<br />
✅ 持久化储存作品信息至文件<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/XHS-Downloader">github.com/JoeanAmier/XHS-Do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5elhIR2JVQUFjb2hQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737518777481535521#m</id>
            <title>RT by @dotey: ChatGPT聊天记录可以归档了，再也不用看着一堆没用碍眼的聊天记录了。

点击想归档的聊天记录右侧三个点，选择Archive Chat就可以了，已经归档的内容可以在设置中查看。</title>
            <link>https://nitter.cz/op7418/status/1737518777481535521#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737518777481535521#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 17:02:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT聊天记录可以归档了，再也不用看着一堆没用碍眼的聊天记录了。<br />
<br />
点击想归档的聊天记录右侧三个点，选择Archive Chat就可以了，已经归档的内容可以在设置中查看。</p>
<p><a href="https://nitter.cz/OpenAI/status/1737517702766633063#m">nitter.cz/OpenAI/status/1737517702766633063#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc1MTgzMzk3OTc0NTQ4NDgvcHUvaW1nL2Rjakt4NG80WnBUTzE1TlUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1737459317102747701#m</id>
            <title>RT by @dotey: 强烈推荐这本在线免费的电子书，《动手学深度学习》，https://zh.d2l.ai，上线一年多时间，已经更新到了第二版，光看作者阵容就已经十分强大了，这本书也被上百所名校列为教材或参考书，当前也出版了实体书。

本书的每个章节都是可以直接运行的 Jupyter 记事本，你可以在本地直接跑，也可以克隆到 Google Colab 在云端跑；讲解的时候，不仅结合文字、公式和图示来阐明深度学习里常用的模型和算法，还提供代码来演示如何从零开始实现它们，并使用真实数据来提供一个交互式的学习体验。

第二版的内容是 2023 年更新的，手把手教你搭建 Bert/Transformer，各种语言框架都有，包括 Pytorch/Tensorflow/JAX 等，而且还支持中英文对照，英文域名是 https://d2l.ai，它对于初学者和有经验的深度学习从业者都是一份宝贵的资源。</title>
            <link>https://nitter.cz/Barret_China/status/1737459317102747701#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1737459317102747701#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:05:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>强烈推荐这本在线免费的电子书，《动手学深度学习》，<a href="https://zh.d2l.ai">zh.d2l.ai</a>，上线一年多时间，已经更新到了第二版，光看作者阵容就已经十分强大了，这本书也被上百所名校列为教材或参考书，当前也出版了实体书。<br />
<br />
本书的每个章节都是可以直接运行的 Jupyter 记事本，你可以在本地直接跑，也可以克隆到 Google Colab 在云端跑；讲解的时候，不仅结合文字、公式和图示来阐明深度学习里常用的模型和算法，还提供代码来演示如何从零开始实现它们，并使用真实数据来提供一个交互式的学习体验。<br />
<br />
第二版的内容是 2023 年更新的，手把手教你搭建 Bert/Transformer，各种语言框架都有，包括 Pytorch/Tensorflow/JAX 等，而且还支持中英文对照，英文域名是 <a href="https://d2l.ai">d2l.ai</a>，它对于初学者和有经验的深度学习从业者都是一份宝贵的资源。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5cnZOZ2F3QUFIblJwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737518177322475655#m</id>
            <title>推荐阅读：《深入了解大语言模型运维 (LLMOps) [译]》

这篇文章 5 月份的，但并没有过时，对于大语言模型的运维(LLMOps)讲的非常系统。

随着大语言模型的普及，未来的 Ops 肯定离不开 LLMOps ，甚至于需要专门的团队做 LLMOps。

文章中把 LLMOps 分成了几个关键步骤：

第 1 步：选择基础模型
是商业模型还是开源模型，亦或是混合使用

第 2 步：适应下游任务
LLM 的生成结果不像传统的服务，它的结果是不确定的，怎么让 LLM 生成你期望的结果？要不要微调？要不要使用 RAG？

第 3 步：评估
如何评估性能？由于 LLM 生成结果的不确定性，每次微调或者调整 Prompt 后，性能的变化需要可以量化的评估。

这部分可以配合《用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]》 https://baoyu.io/translations/rag/evaluating-rag-applications-with-ragas 这篇一起看

第 4 步：部署和监控
和传统运维一样，对于 LLM 的线上的部署和监控也是必不可少的，但是又不太一样，外部 API 需要监控 API 可用性，故障了还要考虑能切换到其他 API。

以上就是主要的几个步骤，可以帮助你系统的了解 LLMOps，但是文章都没有深入展开，最好是配合 OpenAI 官方的 《OpenAI 生产环境最佳实践官方指南 [译]》https://baoyu.io/translations/openai/guides-production-best-practices 一起阅读。

LLMOps 还是个很新也是个很有前途的领域，我个人对这方面也不专业，如果你有相关经验欢迎分享，或者有好的文章视频也欢迎推荐。

原文：https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations#Why%20the%20Rise%20of%20LLMOps
翻译：https://baoyu.io/translations/llm/understanding-llmops-large-language-model-operations</title>
            <link>https://nitter.cz/dotey/status/1737518177322475655#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737518177322475655#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 16:59:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《深入了解大语言模型运维 (LLMOps) [译]》<br />
<br />
这篇文章 5 月份的，但并没有过时，对于大语言模型的运维(LLMOps)讲的非常系统。<br />
<br />
随着大语言模型的普及，未来的 Ops 肯定离不开 LLMOps ，甚至于需要专门的团队做 LLMOps。<br />
<br />
文章中把 LLMOps 分成了几个关键步骤：<br />
<br />
第 1 步：选择基础模型<br />
是商业模型还是开源模型，亦或是混合使用<br />
<br />
第 2 步：适应下游任务<br />
LLM 的生成结果不像传统的服务，它的结果是不确定的，怎么让 LLM 生成你期望的结果？要不要微调？要不要使用 RAG？<br />
<br />
第 3 步：评估<br />
如何评估性能？由于 LLM 生成结果的不确定性，每次微调或者调整 Prompt 后，性能的变化需要可以量化的评估。<br />
<br />
这部分可以配合《用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]》 <a href="https://baoyu.io/translations/rag/evaluating-rag-applications-with-ragas">baoyu.io/translations/rag/ev…</a> 这篇一起看<br />
<br />
第 4 步：部署和监控<br />
和传统运维一样，对于 LLM 的线上的部署和监控也是必不可少的，但是又不太一样，外部 API 需要监控 API 可用性，故障了还要考虑能切换到其他 API。<br />
<br />
以上就是主要的几个步骤，可以帮助你系统的了解 LLMOps，但是文章都没有深入展开，最好是配合 OpenAI 官方的 《OpenAI 生产环境最佳实践官方指南 [译]》<a href="https://baoyu.io/translations/openai/guides-production-best-practices">baoyu.io/translations/openai…</a> 一起阅读。<br />
<br />
LLMOps 还是个很新也是个很有前途的领域，我个人对这方面也不专业，如果你有相关经验欢迎分享，或者有好的文章视频也欢迎推荐。<br />
<br />
原文：<a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations#Why%20the%20Rise%20of%20LLMOps">wandb.ai/site/articles/under…</a><br />
翻译：<a href="https://baoyu.io/translations/llm/understanding-llmops-large-language-model-operations">baoyu.io/translations/llm/un…</a></p>
<p><a href="https://nitter.cz/dotey/status/1736476171385069941#m">nitter.cz/dotey/status/1736476171385069941#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J6ai1RdVdVQUExeDAyLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737512582057992391#m</id>
            <title>R to @dotey: PopAi 官网链接：https://bit.ly/3Ty8G2Z （官方抽3个注册用户送3个月的会员）
使用折扣码可以有 20% 优惠：BAOYU</title>
            <link>https://nitter.cz/dotey/status/1737512582057992391#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737512582057992391#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 16:37:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PopAi 官网链接：<a href="https://bit.ly/3Ty8G2Z">bit.ly/3Ty8G2Z</a> （官方抽3个注册用户送3个月的会员）<br />
使用折扣码可以有 20% 优惠：BAOYU</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>