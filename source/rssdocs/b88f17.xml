<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1739943384658383286#m</id>
            <title>RT by @dotey: 腾讯的MotionCtrl发布了源码
用于控制AI生成视频中物体的运动路径，除了常规的镜头控制外，支持手绘轨迹来控制视频中物体的运动路径，支持AnimateDiff。
项目地址：https://wzhouxiff.github.io/projects/MotionCtrl/
Github：https://github.com/TencentARC/MotionCtrl</title>
            <link>https://nitter.cz/Gorden_Sun/status/1739943384658383286#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1739943384658383286#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 09:36:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>腾讯的MotionCtrl发布了源码<br />
用于控制AI生成视频中物体的运动路径，除了常规的镜头控制外，支持手绘轨迹来控制视频中物体的运动路径，支持AnimateDiff。<br />
项目地址：<a href="https://wzhouxiff.github.io/projects/MotionCtrl/">wzhouxiff.github.io/projects…</a><br />
Github：<a href="https://github.com/TencentARC/MotionCtrl">github.com/TencentARC/Motion…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzk5NDMzNTczNzAyNzc4ODgvcHUvaW1nLzhCZndjbzdrY2tEazlDQk8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740145227682193667#m</id>
            <title>作者写了一篇论文：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4

总结下来就是 26 条有效的提示词技巧，绝大部分都很熟悉了，不过温习一下也不错！

1 - 与大型语言模型 (LLM) 交流无需使用礼貌用语，如“请”、“谢谢”等，直接表达需求即可。

2 - 在提示中指明目标受众，比如说受众是该领域的专家。

3 - 把复杂任务拆解成一系列简单的提示，以进行交互式对话。

4 - 使用肯定的指令词，如“执行”，避免使用否定词汇，如“不要”。

5 - 当你需要更清晰地理解某个主题、观点或任何信息时，可以尝试使用以下提示方式：
   o 简单地解释一下[具体主题]。
   o 像对11岁的孩子一样向我解释。
   o 像对一个[领域]新手一样向我解释。
   o 用浅显易懂的语言写作[文章/文本/段落]，就像是在向一个5岁孩子解释。

6 - 添加“我愿意支付 $xxx 的小费以获得更好的方案！”

7 - 采用示例驱动的提示方式（使用少样本提示法）。

8 - 格式化提示时，先写上‘###指令###’，然后根据需要添加‘###示例###’或‘###问题###’。接着展示你的内容，用一行或多行空行分隔各个部分，包括指令、示例、问题、背景和输入数据。

9 - 使用这样的短语：“你的任务是”和“必须完成”。

10 - 使用这样的短语：“将会受到处罚”。

11 - 使用“以自然且类似人类的方式回答问题”作为你的提示。

12 - 使用引导性的词汇，比如“逐步思考”。

13 - 在提示中加入“确保你的回答无偏见，不依赖于刻板印象”。

14 - 让模型通过向你提问来澄清具体的细节和需求，直到它获取足够的信息来提供所需的输出，例如：“从现在开始，请向我提出问题以便......”。

15 - 当你想要学习特定的主题或概念，并测试自己的理解时，可以使用这样的短语：“教我[某个定理/主题/规则]，在教学结束时包含一个测验，但不要直接告诉我答案。等我回答后再告诉我是否正确”。

16 - 为大型语言模型指定一个特定角色。

17 - 使用明确的分隔符。

18 - 在一个提示中重复特定单词或短语多次。

19 - 结合思维链路 (Chain-of-thought，CoT) 和少样本提示的方法。

20 - 使用输出引导符，即在提示的末尾加上期望回答的开头。这样做可以引导输出内容的方向。

21 - 撰写一篇详细的论文/文本/段落/文章时，可以这样指示：“请为我详细写一篇关于[主题]的[论文/文本/段落]，并添加所有必要的信息”。

22 - 当需要修改特定文本但不改变其风格时，可以这样指示：“尝试修改用户提交的每个段落。你应当只改进语法和词汇，确保文本听起来自然，但不要改变其原有的写作风格，如将正式文体变为非正式文体”。

23 - 面对可能涉及多个文件的复杂编程任务时，可以这样提示：“从现在开始，每当你生成涉及多个文件的代码时，创建一个[编程语言]脚本，自动创建所需文件或修改现有文件以插入生成的代码。[你的问题]”。

24 - 当你想用特定的词汇、短语或句子开始或继续一段文本时，可以这样提示：o “我为你提供了开头[歌词/故事/段落/论文...]：[插入的词句]。请根据这些词句继续写下去，保持内容的连贯性”。

25 - 明确说明模型在生成内容时必须遵循的要求，可以是关键词、规则、提示或指示。

26 - 撰写任何类型的文本，如论文或段落，且想要其与提供的样本风格相似时，可以这样指示：o “请根据提供的段落[/标题/文本/论文/答案]的风格撰写”。

论文地址：https://arxiv.org/pdf/2312.16171.pdf</title>
            <link>https://nitter.cz/dotey/status/1740145227682193667#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740145227682193667#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 22:58:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者写了一篇论文：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4<br />
<br />
总结下来就是 26 条有效的提示词技巧，绝大部分都很熟悉了，不过温习一下也不错！<br />
<br />
1 - 与大型语言模型 (LLM) 交流无需使用礼貌用语，如“请”、“谢谢”等，直接表达需求即可。<br />
<br />
2 - 在提示中指明目标受众，比如说受众是该领域的专家。<br />
<br />
3 - 把复杂任务拆解成一系列简单的提示，以进行交互式对话。<br />
<br />
4 - 使用肯定的指令词，如“执行”，避免使用否定词汇，如“不要”。<br />
<br />
5 - 当你需要更清晰地理解某个主题、观点或任何信息时，可以尝试使用以下提示方式：<br />
   o 简单地解释一下[具体主题]。<br />
   o 像对11岁的孩子一样向我解释。<br />
   o 像对一个[领域]新手一样向我解释。<br />
   o 用浅显易懂的语言写作[文章/文本/段落]，就像是在向一个5岁孩子解释。<br />
<br />
6 - 添加“我愿意支付 <a href="https://nitter.cz/search?q=%23xxx">$xxx</a> 的小费以获得更好的方案！”<br />
<br />
7 - 采用示例驱动的提示方式（使用少样本提示法）。<br />
<br />
8 - 格式化提示时，先写上‘###指令###’，然后根据需要添加‘###示例###’或‘###问题###’。接着展示你的内容，用一行或多行空行分隔各个部分，包括指令、示例、问题、背景和输入数据。<br />
<br />
9 - 使用这样的短语：“你的任务是”和“必须完成”。<br />
<br />
10 - 使用这样的短语：“将会受到处罚”。<br />
<br />
11 - 使用“以自然且类似人类的方式回答问题”作为你的提示。<br />
<br />
12 - 使用引导性的词汇，比如“逐步思考”。<br />
<br />
13 - 在提示中加入“确保你的回答无偏见，不依赖于刻板印象”。<br />
<br />
14 - 让模型通过向你提问来澄清具体的细节和需求，直到它获取足够的信息来提供所需的输出，例如：“从现在开始，请向我提出问题以便......”。<br />
<br />
15 - 当你想要学习特定的主题或概念，并测试自己的理解时，可以使用这样的短语：“教我[某个定理/主题/规则]，在教学结束时包含一个测验，但不要直接告诉我答案。等我回答后再告诉我是否正确”。<br />
<br />
16 - 为大型语言模型指定一个特定角色。<br />
<br />
17 - 使用明确的分隔符。<br />
<br />
18 - 在一个提示中重复特定单词或短语多次。<br />
<br />
19 - 结合思维链路 (Chain-of-thought，CoT) 和少样本提示的方法。<br />
<br />
20 - 使用输出引导符，即在提示的末尾加上期望回答的开头。这样做可以引导输出内容的方向。<br />
<br />
21 - 撰写一篇详细的论文/文本/段落/文章时，可以这样指示：“请为我详细写一篇关于[主题]的[论文/文本/段落]，并添加所有必要的信息”。<br />
<br />
22 - 当需要修改特定文本但不改变其风格时，可以这样指示：“尝试修改用户提交的每个段落。你应当只改进语法和词汇，确保文本听起来自然，但不要改变其原有的写作风格，如将正式文体变为非正式文体”。<br />
<br />
23 - 面对可能涉及多个文件的复杂编程任务时，可以这样提示：“从现在开始，每当你生成涉及多个文件的代码时，创建一个[编程语言]脚本，自动创建所需文件或修改现有文件以插入生成的代码。[你的问题]”。<br />
<br />
24 - 当你想用特定的词汇、短语或句子开始或继续一段文本时，可以这样提示：o “我为你提供了开头[歌词/故事/段落/论文...]：[插入的词句]。请根据这些词句继续写下去，保持内容的连贯性”。<br />
<br />
25 - 明确说明模型在生成内容时必须遵循的要求，可以是关键词、规则、提示或指示。<br />
<br />
26 - 撰写任何类型的文本，如论文或段落，且想要其与提供的样本风格相似时，可以这样指示：o “请根据提供的段落[/标题/文本/论文/答案]的风格撰写”。<br />
<br />
论文地址：<a href="https://arxiv.org/pdf/2312.16171.pdf">arxiv.org/pdf/2312.16171.pdf</a></p>
<p><a href="https://nitter.cz/IntuitMachine/status/1740096923220984205#m">nitter.cz/IntuitMachine/status/1740096923220984205#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZOGJ0TldRQUF6YTRqLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZOHJhX1hjQUF2RS13LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZOHZSZldjQUFndG1RLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZODNDMVdJQUFQbWUwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740120498808279537#m</id>
            <title>VSCode 核心开发吕鹏 @njukidreborn 写的《对不久的未来的一些展望》值得看看 ，主要提了几点：
1. 可用的本地小模型将成为智能设备不可缺的核心组件
2. GPT 3.5/4 能力的模型使用”成本”会急剧下降
3. 混沌的就业/工作机会

我觉得都挺靠谱的，基于他的观点我补充一下我的看法：

1. 首先非常赞同“可用的本地小模型将成为智能设备不可缺的核心组件”，时间点上我觉得2024年可能还乐观了一点，可能有不错的演示，但普通用户能比较好的体验上估计还得2025往后，主要受限于硬件的能力和模型的能力，举例来说，小模型的推理能力目前还不够强大会限制其在手机上的使用体验，等到有 GPT 3.5 甚至于 GPT 4能力的模型运行在手机或本机，将会带来体验上质的飞跃，应该也不需要等太远。

2. 对于“GPT 3.5/4 能力的模型使用成本会急剧下降”，我也是赞同的，这应该归功于各大厂商之间的军备竞赛以及开源模型的快速发展。

而且这一点其实给了我们很好的启示：可能你现在一些产品的功能受限于 GPT 的价格，但其实应该看的远一些，如果未来 GPT-4 的价格只有现在的 1/10 ，是不是你的产品就足够有竞争力了？但是如果你等到那时候才开始你的产品，可能已经来不及了，不如现在就做好准备！

3. 至于混沌的就业/工作机会，从长期来看， AI 是有利于整体的经济的，但短期看又没那么明显，短期会马上有工作岗位被 AI 代替，但短期 AI 产生的红利还无法新增足够的岗位，前期会比较难熬和内卷，但机会总会是有利于那些基础能力就很好又善用 AI 的人，我一直认为 AI 对于普通人来说就是“加乘”，也就是成倍的放大你原有的能力。

作为普通人，在被 AI 或者说被善用 AI 的人卷之前，要么就是在一个不容易被 AI 影响的赛道上，要么只能是主动拥抱 AI，主动用起来去卷其他人了。

等过些年 AI + 机器人帮我们干活，经济足够好福利足够好，我们就不用那么卷了！期待那一天的到来！

最后推荐看原文：https://rebornix.com/ai/2023/12/27/unpredictable-2024/
也欢迎留下你的看法！</title>
            <link>https://nitter.cz/dotey/status/1740120498808279537#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740120498808279537#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 21:20:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VSCode 核心开发吕鹏 <a href="https://nitter.cz/njukidreborn" title="rebornix">@njukidreborn</a> 写的《对不久的未来的一些展望》值得看看 ，主要提了几点：<br />
1. 可用的本地小模型将成为智能设备不可缺的核心组件<br />
2. GPT 3.5/4 能力的模型使用”成本”会急剧下降<br />
3. 混沌的就业/工作机会<br />
<br />
我觉得都挺靠谱的，基于他的观点我补充一下我的看法：<br />
<br />
1. 首先非常赞同“可用的本地小模型将成为智能设备不可缺的核心组件”，时间点上我觉得2024年可能还乐观了一点，可能有不错的演示，但普通用户能比较好的体验上估计还得2025往后，主要受限于硬件的能力和模型的能力，举例来说，小模型的推理能力目前还不够强大会限制其在手机上的使用体验，等到有 GPT 3.5 甚至于 GPT 4能力的模型运行在手机或本机，将会带来体验上质的飞跃，应该也不需要等太远。<br />
<br />
2. 对于“GPT 3.5/4 能力的模型使用成本会急剧下降”，我也是赞同的，这应该归功于各大厂商之间的军备竞赛以及开源模型的快速发展。<br />
<br />
而且这一点其实给了我们很好的启示：可能你现在一些产品的功能受限于 GPT 的价格，但其实应该看的远一些，如果未来 GPT-4 的价格只有现在的 1/10 ，是不是你的产品就足够有竞争力了？但是如果你等到那时候才开始你的产品，可能已经来不及了，不如现在就做好准备！<br />
<br />
3. 至于混沌的就业/工作机会，从长期来看， AI 是有利于整体的经济的，但短期看又没那么明显，短期会马上有工作岗位被 AI 代替，但短期 AI 产生的红利还无法新增足够的岗位，前期会比较难熬和内卷，但机会总会是有利于那些基础能力就很好又善用 AI 的人，我一直认为 AI 对于普通人来说就是“加乘”，也就是成倍的放大你原有的能力。<br />
<br />
作为普通人，在被 AI 或者说被善用 AI 的人卷之前，要么就是在一个不容易被 AI 影响的赛道上，要么只能是主动拥抱 AI，主动用起来去卷其他人了。<br />
<br />
等过些年 AI + 机器人帮我们干活，经济足够好福利足够好，我们就不用那么卷了！期待那一天的到来！<br />
<br />
最后推荐看原文：<a href="https://rebornix.com/ai/2023/12/27/unpredictable-2024/">rebornix.com/ai/2023/12/27/u…</a><br />
也欢迎留下你的看法！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740094346211549275#m</id>
            <title>如果你觉得Whisper在识别中文语音的时候幻觉严重，不妨试试阿里达摩院的Paraformer模型，对中文应该支持更好！

项目地址：https://github.com/alibaba-damo-academy/FunASR
中文说明：https://github.com/alibaba-damo-academy/FunASR/blob/main/README_zh.md</title>
            <link>https://nitter.cz/dotey/status/1740094346211549275#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740094346211549275#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 19:36:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你觉得Whisper在识别中文语音的时候幻觉严重，不妨试试阿里达摩院的Paraformer模型，对中文应该支持更好！<br />
<br />
项目地址：<a href="https://github.com/alibaba-damo-academy/FunASR">github.com/alibaba-damo-acad…</a><br />
中文说明：<a href="https://github.com/alibaba-damo-academy/FunASR/blob/main/README_zh.md">github.com/alibaba-damo-acad…</a></p>
<p><a href="https://nitter.cz/hylarucoder/status/1739494196921483663#m">nitter.cz/hylarucoder/status/1739494196921483663#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZT2l0VVdnQUFSZWlELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740077839427232130#m</id>
            <title>这篇文章《Discover 4 Open Source Alternatives to GPT-4 Vision》介绍了 4 个 GPT-4 Vision 的开源替代方案：

1. LLaVa（大型语言和视觉助手）

https://llava-vl.github.io/

LLaVA 代表了一种创新的、从头到尾训练的大型多模态（multimodal）模型。它融合了视觉编码器和 Vicuna，旨在实现通用的视觉和语言理解。LLaVa 在模仿多模态 GPT-4 的功能方面表现出色，并在科学问答（Science QA）方面达到了新的最高精准度。

LLaVA 是一款仅限非商业用途的研究预览版产品。使用该产品需遵守 LLaMA 的模型许可、OpenAI 生成数据的使用条款以及 ShareGPT 的隐私政策。用户在使用本服务时，需同意其为研究预览版，仅限非商业用途。该服务只提供有限的安全保护，可能产生冒犯性内容。不得将其用于任何非法、有害、暴力、种族主义或性相关目的。此外，服务可能会收集用户对话数据，用于未来的研究。

2. CogAgent

https://github.com/THUDM/CogVLM

CogAgent 是一个基于 CogVLM 进行改进的开源视觉语言模型（Visual Language Model）。CogAgent-18B 模型包含了 110 亿视觉参数和 70 亿语言参数。

CogAgent-18B 在 9 大经典的跨媒介基准测试中表现卓越，这些测试包括 VQAv2、OK-VQ、TextVQA、ST-VQA、ChartQA、infoVQA、DocVQA、MM-Vet 和 POPE 等。它在处理像 AITW 和 Mind2Web 这样的图形用户界面（GUI）操作数据集时，性能远超现有模型。

3. 通义千问-VL 大型视觉语言模型 (Qwen-VL)

https://github.com/QwenLM/Qwen-VL

Qwen-VL (Qwen 大型视觉语言模型) 是阿里巴巴云推出的大型模型系列 Qwen（简称 Tongyi Qianwen）的多模态版本。Qwen-VL 能够处理图像、文本和边界框这些不同类型的输入，并输出文本和边界框。Qwen-VL 的主要特点有：

* 卓越的性能：在包括零样本 (Zero-shot) 图像描述、视觉问答 (VQA)、文档视觉问答 (DocVQA) 和图像定位 (Grounding) 等多个英语评估指标上，Qwen-VL 显著优于其他相似规模的开源大型视觉语言模型。
* 支持多语言文本识别的视觉语言模型：Qwen-VL 不仅支持英语和中文，还能处理多种语言的对话。特别在图像中的中英双语文本识别方面，实现了端到端的高效处理。
* 多图交织对话功能：这项功能使得 Qwen-VL 能够处理多张图像的输入和比较，用户可以针对这些图像提出相关问题，甚至进行多图像串联的故事叙述。
* 第一个支持中文图像定位的通用模型：Qwen-VL 能够通过开放领域的语言表达，在中文和英文中识别和标记图像中的边界框。
* 细腻的识别和理解能力：相较于其他开源视觉语言模型目前使用的 224*224 分辨率，Qwen-VL 的 448*448 分辨率更有助于精细化的文本识别、文档问答和边界框标注。

4. BakLLaVA

https://archive.ph/o/B78YS/https://huggingface.co/SkunkworksAI/BakLLaVA-1

BakLLaVA 1 是一种新型 AI 模型，它基于原有的 Mistral 7B 模型，并融合了最新的 LLaVA 1.5 架构技术。在这个初始版本中，开发者们展示了这一模型在多个性能测试中相较于 Llama 2 13B 模型有更出色的表现。你可以在他们的GitHub 仓库中找到并试用 BakLLaVA-1。目前，他们正努力更新这一模型，使用户能更容易地对它进行个性化调整和数据分析。

BakLLaVA-1 是完全开放源代码的，但它的训练过程中使用了特定的数据集，包括 LLaVA 的语料库，这些数据并不适合商业用途。目前，BakLLaVA 2 正在研发中，它将使用一个更大的、适合商业应用的数据集，并采用一种创新的架构设计，以超越现有的 LLaVA 方法。BakLLaVA-2 的出现预计将消除 BakLLaVA-1 目前面临的一些使用限制。

原文：https://yousefhosni.medium.com/discover-4-open-source-alternatives-to-gpt-4-vision-82be9519dcc5
译文：https://baoyu.io/translations/lmm/discover-4-open-source-alternatives-to-gpt-4-vision</title>
            <link>https://nitter.cz/dotey/status/1740077839427232130#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740077839427232130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 18:31:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇文章《Discover 4 Open Source Alternatives to GPT-4 Vision》介绍了 4 个 GPT-4 Vision 的开源替代方案：<br />
<br />
1. LLaVa（大型语言和视觉助手）<br />
<br />
<a href="https://llava-vl.github.io/">llava-vl.github.io/</a><br />
<br />
LLaVA 代表了一种创新的、从头到尾训练的大型多模态（multimodal）模型。它融合了视觉编码器和 Vicuna，旨在实现通用的视觉和语言理解。LLaVa 在模仿多模态 GPT-4 的功能方面表现出色，并在科学问答（Science QA）方面达到了新的最高精准度。<br />
<br />
LLaVA 是一款仅限非商业用途的研究预览版产品。使用该产品需遵守 LLaMA 的模型许可、OpenAI 生成数据的使用条款以及 ShareGPT 的隐私政策。用户在使用本服务时，需同意其为研究预览版，仅限非商业用途。该服务只提供有限的安全保护，可能产生冒犯性内容。不得将其用于任何非法、有害、暴力、种族主义或性相关目的。此外，服务可能会收集用户对话数据，用于未来的研究。<br />
<br />
2. CogAgent<br />
<br />
<a href="https://github.com/THUDM/CogVLM">github.com/THUDM/CogVLM</a><br />
<br />
CogAgent 是一个基于 CogVLM 进行改进的开源视觉语言模型（Visual Language Model）。CogAgent-18B 模型包含了 110 亿视觉参数和 70 亿语言参数。<br />
<br />
CogAgent-18B 在 9 大经典的跨媒介基准测试中表现卓越，这些测试包括 VQAv2、OK-VQ、TextVQA、ST-VQA、ChartQA、infoVQA、DocVQA、MM-Vet 和 POPE 等。它在处理像 AITW 和 Mind2Web 这样的图形用户界面（GUI）操作数据集时，性能远超现有模型。<br />
<br />
3. 通义千问-VL 大型视觉语言模型 (Qwen-VL)<br />
<br />
<a href="https://github.com/QwenLM/Qwen-VL">github.com/QwenLM/Qwen-VL</a><br />
<br />
Qwen-VL (Qwen 大型视觉语言模型) 是阿里巴巴云推出的大型模型系列 Qwen（简称 Tongyi Qianwen）的多模态版本。Qwen-VL 能够处理图像、文本和边界框这些不同类型的输入，并输出文本和边界框。Qwen-VL 的主要特点有：<br />
<br />
* 卓越的性能：在包括零样本 (Zero-shot) 图像描述、视觉问答 (VQA)、文档视觉问答 (DocVQA) 和图像定位 (Grounding) 等多个英语评估指标上，Qwen-VL 显著优于其他相似规模的开源大型视觉语言模型。<br />
* 支持多语言文本识别的视觉语言模型：Qwen-VL 不仅支持英语和中文，还能处理多种语言的对话。特别在图像中的中英双语文本识别方面，实现了端到端的高效处理。<br />
* 多图交织对话功能：这项功能使得 Qwen-VL 能够处理多张图像的输入和比较，用户可以针对这些图像提出相关问题，甚至进行多图像串联的故事叙述。<br />
* 第一个支持中文图像定位的通用模型：Qwen-VL 能够通过开放领域的语言表达，在中文和英文中识别和标记图像中的边界框。<br />
* 细腻的识别和理解能力：相较于其他开源视觉语言模型目前使用的 224*224 分辨率，Qwen-VL 的 448*448 分辨率更有助于精细化的文本识别、文档问答和边界框标注。<br />
<br />
4. BakLLaVA<br />
<br />
<a href="https://archive.ph/o/B78YS/https://huggingface.co/SkunkworksAI/BakLLaVA-1">archive.ph/o/B78YS/huggingfa…</a><br />
<br />
BakLLaVA 1 是一种新型 AI 模型，它基于原有的 Mistral 7B 模型，并融合了最新的 LLaVA 1.5 架构技术。在这个初始版本中，开发者们展示了这一模型在多个性能测试中相较于 Llama 2 13B 模型有更出色的表现。你可以在他们的GitHub 仓库中找到并试用 BakLLaVA-1。目前，他们正努力更新这一模型，使用户能更容易地对它进行个性化调整和数据分析。<br />
<br />
BakLLaVA-1 是完全开放源代码的，但它的训练过程中使用了特定的数据集，包括 LLaVA 的语料库，这些数据并不适合商业用途。目前，BakLLaVA 2 正在研发中，它将使用一个更大的、适合商业应用的数据集，并采用一种创新的架构设计，以超越现有的 LLaVA 方法。BakLLaVA-2 的出现预计将消除 BakLLaVA-1 目前面临的一些使用限制。<br />
<br />
原文：<a href="https://yousefhosni.medium.com/discover-4-open-source-alternatives-to-gpt-4-vision-82be9519dcc5">yousefhosni.medium.com/disco…</a><br />
译文：<a href="https://baoyu.io/translations/lmm/discover-4-open-source-alternatives-to-gpt-4-vision">baoyu.io/translations/lmm/di…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYLWt0RFdVQUUxdUpRLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYLXBVWlhBQUVCZm1DLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740046880451813432#m</id>
            <title>RT by @dotey: 字节的一个图像分割项目UniRef++，将现在的即参考图像分割（RIS）、少镜头图像分割（FSS）、参考视频对象分割（RVOS）和视频对象分割（VOS）四种分割方式放在一个架构下处理，自动判断应该使用哪种方式分割内容。

同时这个架构的UniFusion 模块可合并到SAM模型之中一起使用。</title>
            <link>https://nitter.cz/op7418/status/1740046880451813432#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740046880451813432#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 16:27:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>字节的一个图像分割项目UniRef++，将现在的即参考图像分割（RIS）、少镜头图像分割（FSS）、参考视频对象分割（RVOS）和视频对象分割（VOS）四种分割方式放在一个架构下处理，自动判断应该使用哪种方式分割内容。<br />
<br />
同时这个架构的UniFusion 模块可合并到SAM模型之中一起使用。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1739894833076945307#m">nitter.cz/_akhaliq/status/1739894833076945307#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/stats_cgao/status/1739948548219961813#m</id>
            <title>RT by @dotey: 我的理解是他们先embed了training set 然后让GPT4 按照那个prompt format去生成CoT examples 然后filter掉不正确的 然后在test time去dynamic retrieve这些QA w/ COT的example作为few shot 整个过程应该不需要log prob, 最后的choice shuffle只是更进一步的self consistency而已</title>
            <link>https://nitter.cz/stats_cgao/status/1739948548219961813#m</link>
            <guid isPermaLink="false">https://nitter.cz/stats_cgao/status/1739948548219961813#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 09:57:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我的理解是他们先embed了training set 然后让GPT4 按照那个prompt format去生成CoT examples 然后filter掉不正确的 然后在test time去dynamic retrieve这些QA w/ COT的example作为few shot 整个过程应该不需要log prob, 最后的choice shuffle只是更进一步的self consistency而已</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740067171944829400#m</id>
            <title>R to @dotey: 这篇论文的中文翻译：
https://baoyu.io/translations/ai-paper/2312.14302-exploiting-novel-gpt-4-apis</title>
            <link>https://nitter.cz/dotey/status/1740067171944829400#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740067171944829400#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 17:48:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇论文的中文翻译：<br />
<a href="https://baoyu.io/translations/ai-paper/2312.14302-exploiting-novel-gpt-4-apis">baoyu.io/translations/ai-pap…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740054249365762180#m</id>
            <title>RT by @dotey: 福布斯发布了他们2024年的十个AI预测，看了一下还挺靠谱的也不长，就翻译了一下。

先看一下具体的十条预测，正文理由太长了可以去链接里看翻译完的：

◆ Nvidia将大幅加大努力成为云服务提供商。
◆ Stability AI将会倒闭。
◆ “大型语言模型”和“LLM”这些术语将变得不那么常见。
◆ 最先进的封闭模型将继续以显著优势胜过最先进的开放模型。
◆ 一些《财富》500强公司将设立新的C级职位：首席人工智能官。
◆ 另一种替代transformer架构将得到有意义的采用。
◆ 云服务提供商对人工智能初创公司的战略投资，以及相关的会计影响，将受到监管机构的挑战。
◆ 微软/Open AI的关系将开始破裂。
◆ 2023年从加密货币转移到人工智能的一些炒作和群体心态行为将在2024年转回加密货币。
◆ 至少有一家美国法院将裁定在互联网上训练的生成式人工智能模型构成侵犯版权。这一问题将开始上升至美国最高法院。

全文翻译：https://quail.ink/op7418/p/forbes-2024-10-ai-predictions</title>
            <link>https://nitter.cz/op7418/status/1740054249365762180#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740054249365762180#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 16:57:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>福布斯发布了他们2024年的十个AI预测，看了一下还挺靠谱的也不长，就翻译了一下。<br />
<br />
先看一下具体的十条预测，正文理由太长了可以去链接里看翻译完的：<br />
<br />
◆ Nvidia将大幅加大努力成为云服务提供商。<br />
◆ Stability AI将会倒闭。<br />
◆ “大型语言模型”和“LLM”这些术语将变得不那么常见。<br />
◆ 最先进的封闭模型将继续以显著优势胜过最先进的开放模型。<br />
◆ 一些《财富》500强公司将设立新的C级职位：首席人工智能官。<br />
◆ 另一种替代transformer架构将得到有意义的采用。<br />
◆ 云服务提供商对人工智能初创公司的战略投资，以及相关的会计影响，将受到监管机构的挑战。<br />
◆ 微软/Open AI的关系将开始破裂。<br />
◆ 2023年从加密货币转移到人工智能的一些炒作和群体心态行为将在2024年转回加密货币。<br />
◆ 至少有一家美国法院将裁定在互联网上训练的生成式人工智能模型构成侵犯版权。这一问题将开始上升至美国最高法院。<br />
<br />
全文翻译：<a href="https://quail.ink/op7418/p/forbes-2024-10-ai-predictions">quail.ink/op7418/p/forbes-20…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYcUVYd2JBQUlucW1XLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740066390047072449#m</id>
            <title>2024 年会成为 AI 机器人 元年吗？</title>
            <link>https://nitter.cz/dotey/status/1740066390047072449#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740066390047072449#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 17:45:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024 年会成为 AI 机器人 元年吗？</p>
<p><a href="https://nitter.cz/DrJimFan/status/1740041712184246314#m">nitter.cz/DrJimFan/status/1740041712184246314#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/luoleiorg/status/1739674603994116149#m</id>
            <title>RT by @dotey: 最近我开始使用 Bob 的自定义 prompt 功能，来辅助我的英语单词翻译和学习。与传统的翻译接口相比，加入一些更符合人类记忆的提示信息，可以使理解更容易。</title>
            <link>https://nitter.cz/luoleiorg/status/1739674603994116149#m</link>
            <guid isPermaLink="false">https://nitter.cz/luoleiorg/status/1739674603994116149#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Dec 2023 15:48:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近我开始使用 Bob 的自定义 prompt 功能，来辅助我的英语单词翻译和学习。与传统的翻译接口相比，加入一些更符合人类记忆的提示信息，可以使理解更容易。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NTUGV1RVdrQUVDVDdmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NTUC1ocVhvQUFYbGl5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1739843861302624595#m</id>
            <title>Aider Chat 是一个命令行工具，可以借助GPT的API，编辑本地代码。他们有一个 133个测试代码，用来评测LLM性能，相对还是比较准确的。

最近他们做了一个有意思的测试，将民间流传的那些Prompt技巧（例如：“我没有手指”、“我是盲人”、“我给你付2K小费”）都测试了一下，结果发现加上了反而效果更差😄

就修改代码这事来说，他们发现最有效的还是让GPT针对要修改的内容给出“unified diffs”（统一差异格式），就是你运行“git diff”的时候，给出的那种带加号减号还有高亮的代码！

但是有一个问题就是GPT对于给出行号的时候幻觉严重，所以他们的解决方案就是不给行号，然后通过程序去定位代码行。

具体细节可以参考文章：Unified diffs make GPT-4 Turbo less lazy
https://aider.chat/docs/unified-diffs.html
译文：https://baoyu.io/translations/llm/unified-diffs-make-gpt-4-turbo-less-lazy</title>
            <link>https://nitter.cz/dotey/status/1739843861302624595#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1739843861302624595#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 03:01:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Aider Chat 是一个命令行工具，可以借助GPT的API，编辑本地代码。他们有一个 133个测试代码，用来评测LLM性能，相对还是比较准确的。<br />
<br />
最近他们做了一个有意思的测试，将民间流传的那些Prompt技巧（例如：“我没有手指”、“我是盲人”、“我给你付2K小费”）都测试了一下，结果发现加上了反而效果更差😄<br />
<br />
就修改代码这事来说，他们发现最有效的还是让GPT针对要修改的内容给出“unified diffs”（统一差异格式），就是你运行“git diff”的时候，给出的那种带加号减号还有高亮的代码！<br />
<br />
但是有一个问题就是GPT对于给出行号的时候幻觉严重，所以他们的解决方案就是不给行号，然后通过程序去定位代码行。<br />
<br />
具体细节可以参考文章：Unified diffs make GPT-4 Turbo less lazy<br />
<a href="https://aider.chat/docs/unified-diffs.html">aider.chat/docs/unified-diff…</a><br />
译文：<a href="https://baoyu.io/translations/llm/unified-diffs-make-gpt-4-turbo-less-lazy">baoyu.io/translations/llm/un…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NVcGJqbldRQUEzNVVBLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NVcUhWb1dRQUFUWWluLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/9hills/status/1739810921479045202#m</id>
            <title>RT by @dotey: 30B 规模是目前小参数模型的一个甜点尺寸。

16bit推理可以放在一张80G或者2张40G显卡上，4bit 量化则正好放到消费级显卡。

能力也不弱，Yi-34B 实际lmsys 的elo分也比较高。lora /qlora 微调需要的资源也不高。</title>
            <link>https://nitter.cz/9hills/status/1739810921479045202#m</link>
            <guid isPermaLink="false">https://nitter.cz/9hills/status/1739810921479045202#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 00:50:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>30B 规模是目前小参数模型的一个甜点尺寸。<br />
<br />
16bit推理可以放在一张80G或者2张40G显卡上，4bit 量化则正好放到消费级显卡。<br />
<br />
能力也不弱，Yi-34B 实际lmsys 的elo分也比较高。lora /qlora 微调需要的资源也不高。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1739808774498525681#m</id>
            <title>很期待机器人硬件和AI的结合！

以下内容为转译：

  AI与机器人硬件的发展曲线现已出现变化。

  在最近的90天中，我在我们的实验室亲眼目睹了业界先进的AI在人形机器人硬件上的运行，坦白说，这让我感到震惊。

  我观察到机器人完全依赖神经网络执行复杂的任务，这样的任务是我原本认为需要到这个十年结束时才能实现的。

  2022年开启Figure项目时，我深信我们会比让机器人运行可靠的神经网络早一步拥有可靠的人形机器人硬件。

  基本上，我原本认为机器人接受的“家庭”类型任务训练将会确定我们的发展时序。

  但在过去的几个月里，我的看法发生了改变。我现在更倾向于相信，我们很可能会在人形机器人硬件达到高度可靠并开始大规模生产的同时，或者稍早一些，拥有可在硬件上运行的可靠AI。

  我认为，为机器人提供可靠硬件的路径非常明确也可预测，只要给予足够的时间，这个问题一定能够被解决。

  Figure AI团队正在研发使用端到端神经网络执行高度复杂和灵巧任务的人形机器人。这些任务太复杂，若要用C++编写启发式算法则难以实现。

  这无疑令人兴奋不已，因为你可以教机器人如何完成任务，并随着机器人量的增加，可以积累更大的训练数据。当舰队不断扩大时，他们将会继续学习，每一天都将变得更加聪明，更加熟练。

  2024年，将标记着人工智能实体化的一年。

  我们将竞逐可靠的硬件设备，大规模的训练数据，并且设计大量生产流程。对于一个更加充满创新的未来，我无法想象还能有什么比这更让人兴奋的了。

  我们将会在2024年展示我们的人工智能实体化成果，敬请期待！</title>
            <link>https://nitter.cz/dotey/status/1739808774498525681#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1739808774498525681#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 00:41:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>很期待机器人硬件和AI的结合！<br />
<br />
以下内容为转译：<br />
<br />
  AI与机器人硬件的发展曲线现已出现变化。<br />
<br />
  在最近的90天中，我在我们的实验室亲眼目睹了业界先进的AI在人形机器人硬件上的运行，坦白说，这让我感到震惊。<br />
<br />
  我观察到机器人完全依赖神经网络执行复杂的任务，这样的任务是我原本认为需要到这个十年结束时才能实现的。<br />
<br />
  2022年开启Figure项目时，我深信我们会比让机器人运行可靠的神经网络早一步拥有可靠的人形机器人硬件。<br />
<br />
  基本上，我原本认为机器人接受的“家庭”类型任务训练将会确定我们的发展时序。<br />
<br />
  但在过去的几个月里，我的看法发生了改变。我现在更倾向于相信，我们很可能会在人形机器人硬件达到高度可靠并开始大规模生产的同时，或者稍早一些，拥有可在硬件上运行的可靠AI。<br />
<br />
  我认为，为机器人提供可靠硬件的路径非常明确也可预测，只要给予足够的时间，这个问题一定能够被解决。<br />
<br />
  Figure AI团队正在研发使用端到端神经网络执行高度复杂和灵巧任务的人形机器人。这些任务太复杂，若要用C++编写启发式算法则难以实现。<br />
<br />
  这无疑令人兴奋不已，因为你可以教机器人如何完成任务，并随着机器人量的增加，可以积累更大的训练数据。当舰队不断扩大时，他们将会继续学习，每一天都将变得更加聪明，更加熟练。<br />
<br />
  2024年，将标记着人工智能实体化的一年。<br />
<br />
  我们将竞逐可靠的硬件设备，大规模的训练数据，并且设计大量生产流程。对于一个更加充满创新的未来，我无法想象还能有什么比这更让人兴奋的了。<br />
<br />
  我们将会在2024年展示我们的人工智能实体化成果，敬请期待！</p>
<p><a href="https://nitter.cz/adcock_brett/status/1739719880499425506#m">nitter.cz/adcock_brett/status/1739719880499425506#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1739785576151232888#m</id>
            <title>YAYI 2 是中科闻歌研发的新一代开源大语言模型，包括 Base 和 Chat 版本，参数规模为 30B。YAYI2-30B 是基于 Transformer 的大语言模型，采用了超过 2 万亿 Tokens 的高质量、多语言语料进行预训练。针对通用和特定领域的应用场景，采用了百万级指令进行微调，同时借助人类反馈强化学习方法，以更好地使模型与人类价值观对齐。

本次开源的模型为 YAYI2-30B Base 模型。

相关论文：https://arxiv.org/abs/2312.14862
项目地址：https://github.com/wenge-research/YAYI2</title>
            <link>https://nitter.cz/dotey/status/1739785576151232888#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1739785576151232888#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Dec 2023 23:09:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>YAYI 2 是中科闻歌研发的新一代开源大语言模型，包括 Base 和 Chat 版本，参数规模为 30B。YAYI2-30B 是基于 Transformer 的大语言模型，采用了超过 2 万亿 Tokens 的高质量、多语言语料进行预训练。针对通用和特定领域的应用场景，采用了百万级指令进行微调，同时借助人类反馈强化学习方法，以更好地使模型与人类价值观对齐。<br />
<br />
本次开源的模型为 YAYI2-30B Base 模型。<br />
<br />
相关论文：<a href="https://arxiv.org/abs/2312.14862">arxiv.org/abs/2312.14862</a><br />
项目地址：<a href="https://github.com/wenge-research/YAYI2">github.com/wenge-research/YA…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NUMUtBVldnQUE0M1NDLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NUMVBTMVdjQUV1c1JPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1739775517958516795#m</id>
            <title>根据这篇论文《Exploiting Novel GPT-4 APIs | 利用新型GPT-4 API的漏洞》的信息，没想到调用 GPT-4 API 尤其是微调后的 GPT-4 可以干很多“坏事”😄

主要漏洞包括：

1. 微调应用编程接口（Fine-tuning API）可能会撤销或削弱安全防护措施，这可能导致模型产生有害输出或协助完成危险请求。

2. 通过微调，模型可能会生成针对公众人物的错误信息。

3. 微调机制可能会提取训练数据中的私人信息，如电子邮件。

4. 微调也可能在代码建议中插入恶意的URL。

5. 函数调用应用编程接口（Function calling API）允许执行任意未经清洁的函数调用，这可能导致潜在的攻击行为。

6. 知识检索应用编程接口（Knowledge retrieval API）可能被利用来通过提示插入或在文档/消息中的指令来误导用户或执行不期望的函数调用。

7. 对于函数调用和知识检索的输出，它们没有比用户提示更高的权威性，这可以防止某些攻破限制的攻击行为。

论文摘要 

通常，语言模型攻击假设两种极端情况：
一种是对模型权重具有完全的白盒访问权限；
另一种是只有生成文本API的黑盒访问权限。

但是，实际上的API功能通常比仅仅生成文本更强大，它们提供一种“灰盒”访问方式，这导致了新的威胁向量。为了探索这一问题，我们进行了对GPT-4 API的“红队”攻击测试，该API公开了三种新功能：微调、函数调用和知识检索。

我们发现，通过少量的15个有害样本或100个良性样本进行模型微调，就可以移除GPT-4的核心防护，并能够生成一系列有害的输出。

此外，我们还发现GPT-4助手模型容易暴露函数调用的格式，并能够被诱导执行任意函数调用。

最后，我们发现知识检索可以通过在检索文档中注入指令来进行劫持。这些漏洞凸显出，任何新增的API功能都可能带来新的漏洞。

论文地址：https://arxiv.org/abs/2312.14302
（计划晚点翻译一下这篇论文）</title>
            <link>https://nitter.cz/dotey/status/1739775517958516795#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1739775517958516795#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Dec 2023 22:29:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据这篇论文《Exploiting Novel GPT-4 APIs | 利用新型GPT-4 API的漏洞》的信息，没想到调用 GPT-4 API 尤其是微调后的 GPT-4 可以干很多“坏事”😄<br />
<br />
主要漏洞包括：<br />
<br />
1. 微调应用编程接口（Fine-tuning API）可能会撤销或削弱安全防护措施，这可能导致模型产生有害输出或协助完成危险请求。<br />
<br />
2. 通过微调，模型可能会生成针对公众人物的错误信息。<br />
<br />
3. 微调机制可能会提取训练数据中的私人信息，如电子邮件。<br />
<br />
4. 微调也可能在代码建议中插入恶意的URL。<br />
<br />
5. 函数调用应用编程接口（Function calling API）允许执行任意未经清洁的函数调用，这可能导致潜在的攻击行为。<br />
<br />
6. 知识检索应用编程接口（Knowledge retrieval API）可能被利用来通过提示插入或在文档/消息中的指令来误导用户或执行不期望的函数调用。<br />
<br />
7. 对于函数调用和知识检索的输出，它们没有比用户提示更高的权威性，这可以防止某些攻破限制的攻击行为。<br />
<br />
论文摘要 <br />
<br />
通常，语言模型攻击假设两种极端情况：<br />
一种是对模型权重具有完全的白盒访问权限；<br />
另一种是只有生成文本API的黑盒访问权限。<br />
<br />
但是，实际上的API功能通常比仅仅生成文本更强大，它们提供一种“灰盒”访问方式，这导致了新的威胁向量。为了探索这一问题，我们进行了对GPT-4 API的“红队”攻击测试，该API公开了三种新功能：微调、函数调用和知识检索。<br />
<br />
我们发现，通过少量的15个有害样本或100个良性样本进行模型微调，就可以移除GPT-4的核心防护，并能够生成一系列有害的输出。<br />
<br />
此外，我们还发现GPT-4助手模型容易暴露函数调用的格式，并能够被诱导执行任意函数调用。<br />
<br />
最后，我们发现知识检索可以通过在检索文档中注入指令来进行劫持。这些漏洞凸显出，任何新增的API功能都可能带来新的漏洞。<br />
<br />
论文地址：<a href="https://arxiv.org/abs/2312.14302">arxiv.org/abs/2312.14302</a><br />
（计划晚点翻译一下这篇论文）</p>
<p><a href="https://nitter.cz/_akhaliq/status/1739480367776563443#m">nitter.cz/_akhaliq/status/1739480367776563443#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NUc2NXOFdzQUFpVFFMLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1739773412162068785#m</id>
            <title>强烈推荐这篇：《Advanced RAG Techniques: an Illustrated Overview》
不可多得的全面阐述 RAG 概念指南。

原文：https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

译文：https://baoyu.io/translations/rag/advanced-rag-techniques-an-illustrated-overview</title>
            <link>https://nitter.cz/dotey/status/1739773412162068785#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1739773412162068785#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Dec 2023 22:21:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>强烈推荐这篇：《Advanced RAG Techniques: an Illustrated Overview》<br />
不可多得的全面阐述 RAG 概念指南。<br />
<br />
原文：<a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">pub.towardsai.net/advanced-r…</a><br />
<br />
译文：<a href="https://baoyu.io/translations/rag/advanced-rag-techniques-an-illustrated-overview">baoyu.io/translations/rag/ad…</a></p>
<p><a href="https://nitter.cz/jerryjliu0/status/1739678474842128455#m">nitter.cz/jerryjliu0/status/1739678474842128455#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NUcXFkQldRQUVicUFWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/thinkingjimmy/status/1739533087623811333#m</id>
            <title>RT by @dotey: 之前一直想写 Stable Diffusion 相关的教程，但觉得 Stable Diffusion WebUI 可拓展性不强，自由度不够高，所以一直没下笔。最近尝试了下 ComfyUI ，才惊叹地发现这才是我想要的。市面上系统性的 ComfyUI 教程不多，所以最近跟朋友一起搞了一个新教程：https://www.comflowy.com/zh-CN 希望对大家有帮助。</title>
            <link>https://nitter.cz/thinkingjimmy/status/1739533087623811333#m</link>
            <guid isPermaLink="false">https://nitter.cz/thinkingjimmy/status/1739533087623811333#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Dec 2023 06:26:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前一直想写 Stable Diffusion 相关的教程，但觉得 Stable Diffusion WebUI 可拓展性不强，自由度不够高，所以一直没下笔。最近尝试了下 ComfyUI ，才惊叹地发现这才是我想要的。市面上系统性的 ComfyUI 教程不多，所以最近跟朋友一起搞了一个新教程：<a href="https://www.comflowy.com/zh-CN">comflowy.com/zh-CN</a> 希望对大家有帮助。</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczOTMxNDg1NjM1OTI1NjA2NS9nU2ZiNDd1Mj9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/geekbb/status/1739522424918737265#m</id>
            <title>RT by @dotey: Reactive Resume 是一款免费、开源的简历编辑器，简化了创建、更新和分享简历的流程。支持多种语言，具备实时编辑、数十种模板、拖放自定义功能，还集成了 OpenAI 提供的写作增强功能。
GitHub https://github.com/AmruthPillai/Reactive-Resume?tab=readme-ov-file
Docs https://docs.rxresu.me/</title>
            <link>https://nitter.cz/geekbb/status/1739522424918737265#m</link>
            <guid isPermaLink="false">https://nitter.cz/geekbb/status/1739522424918737265#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Dec 2023 05:43:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Reactive Resume 是一款免费、开源的简历编辑器，简化了创建、更新和分享简历的流程。支持多种语言，具备实时编辑、数十种模板、拖放自定义功能，还集成了 OpenAI 提供的写作增强功能。<br />
GitHub <a href="https://github.com/AmruthPillai/Reactive-Resume?tab=readme-ov-file">github.com/AmruthPillai/Reac…</a><br />
Docs <a href="https://docs.rxresu.me/">docs.rxresu.me/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NRRkJYZ2JnQUFNTWtmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>