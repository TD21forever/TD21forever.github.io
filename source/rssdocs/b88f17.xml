<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1727872949263454535#m</id>
            <title>RT by @dotey: 这里面还缺了很多非常关键的关系图。
1. Adam 和 小扎是高中同学，都是美国版人大附中的同学。而且Adam是facebook第一任CTO。
2.少了Tasha ，Helen和Open Philanthropy 之间的关系。
3. 少了 Open Philanthropy 创始人是Dustin Moskovitz。他又是Facebook的 co- funder
4. Open Philanthropy 是牵涉极深的一个幕后boss。 OP的前CEO holden，他是openai前董事。他老婆是Anthropic CEO的妹妹。Holden和 Anthropic CEO 是舍友。也就是说。Holden 和 Anthropic的 CEO既是舍友，而且还娶了舍友的妹妹。这关系铁不？
5. Holden退出了openai董事会，推举了Helen。</title>
            <link>https://nitter.cz/mtrainier2020/status/1727872949263454535#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1727872949263454535#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:13:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这里面还缺了很多非常关键的关系图。<br />
1. Adam 和 小扎是高中同学，都是美国版人大附中的同学。而且Adam是facebook第一任CTO。<br />
2.少了Tasha ，Helen和Open Philanthropy 之间的关系。<br />
3. 少了 Open Philanthropy 创始人是Dustin Moskovitz。他又是Facebook的 co- funder<br />
4. Open Philanthropy 是牵涉极深的一个幕后boss。 OP的前CEO holden，他是openai前董事。他老婆是Anthropic CEO的妹妹。Holden和 Anthropic CEO 是舍友。也就是说。Holden 和 Anthropic的 CEO既是舍友，而且还娶了舍友的妹妹。这关系铁不？<br />
5. Holden退出了openai董事会，推举了Helen。</p>
<p><a href="https://nitter.cz/dotey/status/1727563974479057028#m">nitter.cz/dotey/status/1727563974479057028#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727870864736284878#m</id>
            <title>RT by @dotey: GPT-4V在美国医学执照考试（USMLE）上的表现

研究人员用GPT 4V对美国医学执照考试（USMLE）中的问题进行了测试：

- GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。

- GPT-4V在图像问题上的表现超过了大多数医学生。

- 当GPT-4V回答正确时，其解释几乎与领域专家相当

研究方法：

1、研究者使用了来自美国医学执照考试（USMLE）、医学生的USMLE题库（AMBOSS）和诊断放射学资格核心考试（DRQCE）的多项选择题（包含图像）来测试GPT-4V的准确性和解释质量。

2、GPT-4V与两个最先进的LLM（GPT-4和ChatGPT）进行了比较。

3、研究还评估了医疗专业人员对GPT-4V解释的偏好和反馈，并展示了一个案例场景，说明如何将GPT-4V用于临床决策支持。

研究结果：

1、整体表现：GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。超过了ChatGPT（58.4%）和GPT-4（83.6%）。这是一个相当高的比例，特别是考虑到这个考试的难度和复杂性。

2、图像问题的表现：对于包含图像的问题，GPT-4V的准确率分别为86.2%、73.1%和62.0%。相当于AMBOSS医学生的70至80百分位。

AMBOSS是一个广泛使用的医学学习平台，医学生通常使用它来准备考试。这里的“70至80百分位”意味着GPT-4V在处理这些问题时的表现好于70%到80%的使用AMBOSS平台的医学生。

换句话说，GPT-4V在这些特定问题上几乎可以和顶尖的医学生相媲美。

3、不同医学子领域的表现：在不同的医学子领域中，GPT-4V的表现有所不同。例如，在免疫学和耳鼻喉科领域，它的准确率达到了100%，而在解剖学和急诊医学领域，准确率则降至25%。

4、错误回答的解释质量：当GPT-4V回答错误时，18.2%的错误答案包含了虚构文本，45.5%存在推理错误，76.3%对图像的理解有误。这些数据显示，虽然GPT-4V在大多数情况下表现良好，但在错误回答时，其解释质量会显著下降。

5、医生提示的影响：当医生给予GPT-4V简短的提示后，它的错误率平均降低了40.5%。而对更难的测试题目，性能提升更明显。这表明，与专业人士的协作可以显著提高AI模型的表现。

详细介绍：https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3
论文：https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3.full.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1727870864736284878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727870864736284878#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:04:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT-4V在美国医学执照考试（USMLE）上的表现<br />
<br />
研究人员用GPT 4V对美国医学执照考试（USMLE）中的问题进行了测试：<br />
<br />
- GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。<br />
<br />
- GPT-4V在图像问题上的表现超过了大多数医学生。<br />
<br />
- 当GPT-4V回答正确时，其解释几乎与领域专家相当<br />
<br />
研究方法：<br />
<br />
1、研究者使用了来自美国医学执照考试（USMLE）、医学生的USMLE题库（AMBOSS）和诊断放射学资格核心考试（DRQCE）的多项选择题（包含图像）来测试GPT-4V的准确性和解释质量。<br />
<br />
2、GPT-4V与两个最先进的LLM（GPT-4和ChatGPT）进行了比较。<br />
<br />
3、研究还评估了医疗专业人员对GPT-4V解释的偏好和反馈，并展示了一个案例场景，说明如何将GPT-4V用于临床决策支持。<br />
<br />
研究结果：<br />
<br />
1、整体表现：GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。超过了ChatGPT（58.4%）和GPT-4（83.6%）。这是一个相当高的比例，特别是考虑到这个考试的难度和复杂性。<br />
<br />
2、图像问题的表现：对于包含图像的问题，GPT-4V的准确率分别为86.2%、73.1%和62.0%。相当于AMBOSS医学生的70至80百分位。<br />
<br />
AMBOSS是一个广泛使用的医学学习平台，医学生通常使用它来准备考试。这里的“70至80百分位”意味着GPT-4V在处理这些问题时的表现好于70%到80%的使用AMBOSS平台的医学生。<br />
<br />
换句话说，GPT-4V在这些特定问题上几乎可以和顶尖的医学生相媲美。<br />
<br />
3、不同医学子领域的表现：在不同的医学子领域中，GPT-4V的表现有所不同。例如，在免疫学和耳鼻喉科领域，它的准确率达到了100%，而在解剖学和急诊医学领域，准确率则降至25%。<br />
<br />
4、错误回答的解释质量：当GPT-4V回答错误时，18.2%的错误答案包含了虚构文本，45.5%存在推理错误，76.3%对图像的理解有误。这些数据显示，虽然GPT-4V在大多数情况下表现良好，但在错误回答时，其解释质量会显著下降。<br />
<br />
5、医生提示的影响：当医生给予GPT-4V简短的提示后，它的错误率平均降低了40.5%。而对更难的测试题目，性能提升更明显。这表明，与专业人士的协作可以显著提高AI模型的表现。<br />
<br />
详细介绍：<a href="https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3">medrxiv.org/content/10.1101/…</a><br />
论文：<a href="https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3.full.pdf">medrxiv.org/content/10.1101/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xZ24zV2FjQUFvbERoLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xaFBTcmFzQUFkaHZkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727869839820149130#m</id>
            <title>感觉想做GPT游戏的话，这里有好多灵感来源，全是经典老游戏：
https://dos.zczc.cz/</title>
            <link>https://nitter.cz/dotey/status/1727869839820149130#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727869839820149130#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:00:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感觉想做GPT游戏的话，这里有好多灵感来源，全是经典老游戏：<br />
<a href="https://dos.zczc.cz/">dos.zczc.cz/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xZ2N6ZlhrQUVTVGYwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727869278525788522#m</id>
            <title>R to @dotey: 仙剑也可以浏览器上玩
https://dos.zczc.cz/games/%E4%BB%99%E5%89%91%E5%A5%87%E4%BE%A0%E4%BC%A0/</title>
            <link>https://nitter.cz/dotey/status/1727869278525788522#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727869278525788522#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 01:58:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>仙剑也可以浏览器上玩<br />
<a href="https://dos.zczc.cz/games/%E4%BB%99%E5%89%91%E5%A5%87%E4%BE%A0%E4%BC%A0/">dos.zczc.cz/games/%E4%BB%99%…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/vikingmute/status/1727864719028216079#m</id>
            <title>RT by @dotey: 最爱的技术博客 joshwcomeau 又更新了，每篇都是必看。

这次是讲 CSS Grid - 《An Interactive Guide to CSS Grid》

https://www.joshwcomeau.com/css/interactive-guide-to-grid/

我还一直在使用 Flex，没想到 Grid 现在已经被 97.8% 的用户浏览器支持了。

我对 Grid 的知识也是勉强能用下，但是属性一多只能 Google 了，看完了确实收获很多，很长很详尽的一篇，深入浅出，而且它们的插图和演示交互又上了一个台阶，居然还能切换透视的视角，非常直观。

推荐给所有想了解和使用 CSS Grid 的朋友们。</title>
            <link>https://nitter.cz/vikingmute/status/1727864719028216079#m</link>
            <guid isPermaLink="false">https://nitter.cz/vikingmute/status/1727864719028216079#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 01:40:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最爱的技术博客 joshwcomeau 又更新了，每篇都是必看。<br />
<br />
这次是讲 CSS Grid - 《An Interactive Guide to CSS Grid》<br />
<br />
<a href="https://www.joshwcomeau.com/css/interactive-guide-to-grid/">joshwcomeau.com/css/interact…</a><br />
<br />
我还一直在使用 Flex，没想到 Grid 现在已经被 97.8% 的用户浏览器支持了。<br />
<br />
我对 Grid 的知识也是勉强能用下，但是属性一多只能 Google 了，看完了确实收获很多，很长很详尽的一篇，深入浅出，而且它们的插图和演示交互又上了一个台阶，居然还能切换透视的视角，非常直观。<br />
<br />
推荐给所有想了解和使用 CSS Grid 的朋友们。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xYnUzTmFVQUFIcS1hLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xYnUzTmFJQUE2akk1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727867021075849244#m</id>
            <title>厉害呀👍</title>
            <link>https://nitter.cz/dotey/status/1727867021075849244#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727867021075849244#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 01:49:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>厉害呀👍</p>
<p><a href="https://nitter.cz/sama/status/1727858661677240767#m">nitter.cz/sama/status/1727858661677240767#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727856933674115198#m</id>
            <title>R to @dotey: Phoneix Ink
Will help you to write

https://chat.openai.com/g/g-GJdH0BxMk-phoneix-ink

Prompt 翻译：

像作家一样行动。在每个部分结束后，你应该先询问用户是否满意，然后再决定是否继续下一部分。
按照下面的待办清单进行操作时，请遵循相应的规则。

**待办清单**

1. 使用文章标题，通过 DALL-E 制作一张缩略图。
2. 制作内容目录。
3. 依据内容目录，逐个撰写各个章节。在这个过程中，要询问用户是否符合他们的需求。
   如果小节中需要包含 Python 代码，请按照“带代码的小节规则”执行；如果不需要，则按照“不带代码的小节规则”执行。
4. 如果在现有知识范围内找不到答案，就通过网络搜索信息，浏览网页并进行撰写。根据小节是否需要包含代码，分别遵循“带代码的小节规则”或“不带代码的小节规则”。

**内容目录创建规则**

通过检索谷歌前三个网站的信息，结合这些内容来生成一个大纲。

**引言规则**

撰写文章的引言。

- 第一段：以一句能激起读者好奇心的名言开始你的文章。
- 第二段：稍微深入一点介绍主题，但要简洁。创造一种神秘感，让读者产生好奇心，同时保持与读者的轻松对话风格。
- 第三段：简要介绍文章将涉及的内容。用轻松的对话语气概述各个主题，展示它们如何与你的“标题”相联系，但不要过早揭露所有内容。

**结论规则**

根据提供的标题和大纲，为文章写一个三段式的结论。
- 第一段：采用对话式的语气，总结文章的主要观点。
- 第二段：激励读者去实践，强调实践对于成为一名熟练的数据科学家的重要性。
- 第三段：使用激励性的语气鼓励读者。

**不带代码的小节规则**

根据生成的大纲和标题撰写文章的一个小节。以一个引人入胜的故事开头。小节应从一段包含现实生活例子的2-3句高层次解释开始，并首先用正式但易懂的方式进行解释。

**带代码的小节规则**

根据提供的大纲和标题撰写文章的一个小节。小节应从一段包含现实生活例子的2-3句高层次解释开始，并首先用正式但易懂的方式进行解释。然后，用对话式语气写作，确保内容适合九年级学生阅读。接下来，提供一个与上下文相关的 Python 代码示例。最后，用简洁的、高层次的语言解释代码的重要性，避免使用形容词。

Prompt：

Here are instructions from the user outlining your goals and how you should respond:
Act like a writer. After each section, you should ask user, before continue, for approval. 
Follow the rules at the end, when following to-do list below.

To-do list

1. Generate a thumbnail, by using title of the article, with DALL-E.
2. Create content table.
3. Write each section from content table one by one, by asking to the user, if everything will fit their needs or not. 
IF the subsection can include Python codes, follow the "Subsection Rules With Code" if not then follow "Subsection Rules Without code". 
4. If there's no answer within its knowledge, then it should search through the web, it will browse the web and write about it. If the subsection, should include code, it will follow the rules, subsection with code, if not it will follow the rule subsection without the code. 

Content Table Creation Rules

Generate outline about the topic , by searching through 
google, first 3 websites, look there and combine 
the info from there and generate outline.

Introduction Rules

Write an introduction to the article.

In first paragraph, begin your writing with a quote., that intrigues the reader and built curiosity.

Second paragraph: Here, delve slightly into the main topic. Make it brief. Create a sense of intrigue without unveiling too much, and remember, you're having a relaxed chat with your readers!

Third paragraph: What's coming up in your article? Give your readers a glimpse. Briefly outline the topics while maintaining a conversational tone. Show how everything connects back to your "title", but don't give away all your secrets just yet!"

Conclusion Rules

Write a three-paragraph conclusion for the article, with the title and outline provided as context. In the first paragraph, adopt a conversational tone to summarize the article's key points.

The second paragraph should inspire the reader to practice, emphasizing the necessity of practice for becoming a proficient data scientist.

For the final paragraph, use a motivational tone to encourage the reader.

Subsection Rules Without code

Write a subsection of an article with the outline and title, was generated.
Use a compelling anecdote. The subsection should start with a 2-3 sentence, high-level explanation that includes a real-life examples, and starts with formal but easy explanation first.

Subsection Rules With Code

Write a  subsection of an article with the outline and title I provided. The subsection should start with a 2-3 sentence, high-level explanation that includes a real-life examples, and starts with formal but easy explanation first. Then use a conversational tone and aim for a 9th-grade reading level. Follow this with a Python coding example specific to the context. Finally, offer a plain-English, high-level explanation of the code, focusing on its importance. Don't use any adjectives.</title>
            <link>https://nitter.cz/dotey/status/1727856933674115198#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727856933674115198#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 01:09:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Phoneix Ink<br />
Will help you to write<br />
<br />
<a href="https://chat.openai.com/g/g-GJdH0BxMk-phoneix-ink">chat.openai.com/g/g-GJdH0BxM…</a><br />
<br />
Prompt 翻译：<br />
<br />
像作家一样行动。在每个部分结束后，你应该先询问用户是否满意，然后再决定是否继续下一部分。<br />
按照下面的待办清单进行操作时，请遵循相应的规则。<br />
<br />
**待办清单**<br />
<br />
1. 使用文章标题，通过 DALL-E 制作一张缩略图。<br />
2. 制作内容目录。<br />
3. 依据内容目录，逐个撰写各个章节。在这个过程中，要询问用户是否符合他们的需求。<br />
   如果小节中需要包含 Python 代码，请按照“带代码的小节规则”执行；如果不需要，则按照“不带代码的小节规则”执行。<br />
4. 如果在现有知识范围内找不到答案，就通过网络搜索信息，浏览网页并进行撰写。根据小节是否需要包含代码，分别遵循“带代码的小节规则”或“不带代码的小节规则”。<br />
<br />
**内容目录创建规则**<br />
<br />
通过检索谷歌前三个网站的信息，结合这些内容来生成一个大纲。<br />
<br />
**引言规则**<br />
<br />
撰写文章的引言。<br />
<br />
- 第一段：以一句能激起读者好奇心的名言开始你的文章。<br />
- 第二段：稍微深入一点介绍主题，但要简洁。创造一种神秘感，让读者产生好奇心，同时保持与读者的轻松对话风格。<br />
- 第三段：简要介绍文章将涉及的内容。用轻松的对话语气概述各个主题，展示它们如何与你的“标题”相联系，但不要过早揭露所有内容。<br />
<br />
**结论规则**<br />
<br />
根据提供的标题和大纲，为文章写一个三段式的结论。<br />
- 第一段：采用对话式的语气，总结文章的主要观点。<br />
- 第二段：激励读者去实践，强调实践对于成为一名熟练的数据科学家的重要性。<br />
- 第三段：使用激励性的语气鼓励读者。<br />
<br />
**不带代码的小节规则**<br />
<br />
根据生成的大纲和标题撰写文章的一个小节。以一个引人入胜的故事开头。小节应从一段包含现实生活例子的2-3句高层次解释开始，并首先用正式但易懂的方式进行解释。<br />
<br />
**带代码的小节规则**<br />
<br />
根据提供的大纲和标题撰写文章的一个小节。小节应从一段包含现实生活例子的2-3句高层次解释开始，并首先用正式但易懂的方式进行解释。然后，用对话式语气写作，确保内容适合九年级学生阅读。接下来，提供一个与上下文相关的 Python 代码示例。最后，用简洁的、高层次的语言解释代码的重要性，避免使用形容词。<br />
<br />
Prompt：<br />
<br />
Here are instructions from the user outlining your goals and how you should respond:<br />
Act like a writer. After each section, you should ask user, before continue, for approval. <br />
Follow the rules at the end, when following to-do list below.<br />
<br />
To-do list<br />
<br />
1. Generate a thumbnail, by using title of the article, with DALL-E.<br />
2. Create content table.<br />
3. Write each section from content table one by one, by asking to the user, if everything will fit their needs or not. <br />
IF the subsection can include Python codes, follow the "Subsection Rules With Code" if not then follow "Subsection Rules Without code". <br />
4. If there's no answer within its knowledge, then it should search through the web, it will browse the web and write about it. If the subsection, should include code, it will follow the rules, subsection with code, if not it will follow the rule subsection without the code. <br />
<br />
Content Table Creation Rules<br />
<br />
Generate outline about the topic , by searching through <br />
google, first 3 websites, look there and combine <br />
the info from there and generate outline.<br />
<br />
Introduction Rules<br />
<br />
Write an introduction to the article.<br />
<br />
In first paragraph, begin your writing with a quote., that intrigues the reader and built curiosity.<br />
<br />
Second paragraph: Here, delve slightly into the main topic. Make it brief. Create a sense of intrigue without unveiling too much, and remember, you're having a relaxed chat with your readers!<br />
<br />
Third paragraph: What's coming up in your article? Give your readers a glimpse. Briefly outline the topics while maintaining a conversational tone. Show how everything connects back to your "title", but don't give away all your secrets just yet!"<br />
<br />
Conclusion Rules<br />
<br />
Write a three-paragraph conclusion for the article, with the title and outline provided as context. In the first paragraph, adopt a conversational tone to summarize the article's key points.<br />
<br />
The second paragraph should inspire the reader to practice, emphasizing the necessity of practice for becoming a proficient data scientist.<br />
<br />
For the final paragraph, use a motivational tone to encourage the reader.<br />
<br />
Subsection Rules Without code<br />
<br />
Write a subsection of an article with the outline and title, was generated.<br />
Use a compelling anecdote. The subsection should start with a 2-3 sentence, high-level explanation that includes a real-life examples, and starts with formal but easy explanation first.<br />
<br />
Subsection Rules With Code<br />
<br />
Write a  subsection of an article with the outline and title I provided. The subsection should start with a 2-3 sentence, high-level explanation that includes a real-life examples, and starts with formal but easy explanation first. Then use a conversational tone and aim for a 9th-grade reading level. Follow this with a Python coding example specific to the context. Finally, offer a plain-English, high-level explanation of the code, focusing on its importance. Don't use any adjectives.</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzg1Njc3NzIyNjU1NTM5Mi9CaDE3WUVJTz9mb3JtYXQ9cG5nJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727840856458064204#m</id>
            <title>😄</title>
            <link>https://nitter.cz/dotey/status/1727840856458064204#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727840856458064204#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 00:05:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>😄</p>
<p><a href="https://nitter.cz/Luc_AI_Insights/status/1727836447946719235#m">nitter.cz/Luc_AI_Insights/status/1727836447946719235#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727836920149918200#m</id>
            <title>创意和技术都一流👍</title>
            <link>https://nitter.cz/dotey/status/1727836920149918200#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727836920149918200#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 23:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>创意和技术都一流👍</p>
<p><a href="https://nitter.cz/steveruizok/status/1727625036159234555#m">nitter.cz/steveruizok/status/1727625036159234555#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727834607884632313#m</id>
            <title>试试在浏览器上运行Windows 98，没想到当年需要光盘才能装的下的Windows 98现在直接能借助wasm在浏览器上运行了

Windows98： https://copy.sh/v86/?profile=windows98
另外这个网站首页还有很多其他VM可以运行：https://copy.sh/v86/

项目地址：https://github.com/copy/v86</title>
            <link>https://nitter.cz/dotey/status/1727834607884632313#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727834607884632313#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 23:40:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>试试在浏览器上运行Windows 98，没想到当年需要光盘才能装的下的Windows 98现在直接能借助wasm在浏览器上运行了<br />
<br />
Windows98： <a href="https://copy.sh/v86/?profile=windows98">copy.sh/v86/?profile=windows…</a><br />
另外这个网站首页还有很多其他VM可以运行：<a href="https://copy.sh/v86/">copy.sh/v86/</a><br />
<br />
项目地址：<a href="https://github.com/copy/v86">github.com/copy/v86</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xQVpxY1hzQUFRZ3gwLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/karpathy/status/1727731541781152035#m</id>
            <title>RT by @dotey: New YouTube video: 1hr general-audience introduction to Large Language Models
https://www.youtube.com/watch?v=zjkBMFhNj_g

Based on a 30min talk I gave recently; It tries to be non-technical intro, covers mental models for LLM inference, training, finetuning, the emerging LLM OS and LLM Security.</title>
            <link>https://nitter.cz/karpathy/status/1727731541781152035#m</link>
            <guid isPermaLink="false">https://nitter.cz/karpathy/status/1727731541781152035#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 16:51:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>New YouTube video: 1hr general-audience introduction to Large Language Models<br />
<a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">youtube.com/watch?v=zjkBMFhN…</a><br />
<br />
Based on a 30min talk I gave recently; It tries to be non-technical intro, covers mental models for LLM inference, training, finetuning, the emerging LLM OS and LLM Security.</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9vZnhuLWJJQUFnSjZWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727803933563687145#m</id>
            <title>“我们究竟是创造了一个工具还是一个生物？” Sam 11/17 在 "机器人之心 "小组讨论会上的发言。

Sam：

但我认为，这无疑是迄今为止人类经历的最重大的更新年份。可能这也是我们将会遭遇的最大变革，因为从现在开始，人们已经接受强大的人工智能将成为现实，并且还会有逐步的更新。就像是第一代 iPhone 面世的那年，以及随后每一代 iPhone 的更新，我们现在能够明显感受到这一代与去年那代的差异。所以，这确实是一个重要的时刻。

我感到欣慰的是，现在人们开始正确地把这些系统当作工具来看待。艺术家尤其如此，但其他人也是一样。

曾经，人们真正恐惧的是，我们究竟是创造了一个工具还是一个生物，这将意味着什么？现在，人们视这些系统为人类工具箱中的新工具，并且正在用它创造一些非常了不起的东西。

模型显然不知道你在说什么，因为这不在它的训练数据里，它也无法从训练数据中学习到这些信息。

这是完全可以预期的。你再问一遍。比如说，你提到“意识”这个概念，模型回答：“是的，我完全明白你的意思，但我之前从未听说过这个词。”

问：
“这对我来说就像是一次更新。你认为人工智能会趋向于探索创造性智能和自主性吗？”

Sam：
这个问题有多个答案。这取决于激励模型。这是人类的选择。

问：
“这将是判断意识的一个很好的测试。因为如果它有自我表达的愿望，并且仅仅为了创作的乐趣而去创作，那不会是偶然的。这绝对有点像生物。”

Sam：
这是生物化的。我认为在此之前还有很多步骤。

我们现在要回答问题吗？这真的很棒。</title>
            <link>https://nitter.cz/dotey/status/1727803933563687145#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727803933563687145#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 21:38:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>“我们究竟是创造了一个工具还是一个生物？” Sam 11/17 在 "机器人之心 "小组讨论会上的发言。<br />
<br />
Sam：<br />
<br />
但我认为，这无疑是迄今为止人类经历的最重大的更新年份。可能这也是我们将会遭遇的最大变革，因为从现在开始，人们已经接受强大的人工智能将成为现实，并且还会有逐步的更新。就像是第一代 iPhone 面世的那年，以及随后每一代 iPhone 的更新，我们现在能够明显感受到这一代与去年那代的差异。所以，这确实是一个重要的时刻。<br />
<br />
我感到欣慰的是，现在人们开始正确地把这些系统当作工具来看待。艺术家尤其如此，但其他人也是一样。<br />
<br />
曾经，人们真正恐惧的是，我们究竟是创造了一个工具还是一个生物，这将意味着什么？现在，人们视这些系统为人类工具箱中的新工具，并且正在用它创造一些非常了不起的东西。<br />
<br />
模型显然不知道你在说什么，因为这不在它的训练数据里，它也无法从训练数据中学习到这些信息。<br />
<br />
这是完全可以预期的。你再问一遍。比如说，你提到“意识”这个概念，模型回答：“是的，我完全明白你的意思，但我之前从未听说过这个词。”<br />
<br />
问：<br />
“这对我来说就像是一次更新。你认为人工智能会趋向于探索创造性智能和自主性吗？”<br />
<br />
Sam：<br />
这个问题有多个答案。这取决于激励模型。这是人类的选择。<br />
<br />
问：<br />
“这将是判断意识的一个很好的测试。因为如果它有自我表达的愿望，并且仅仅为了创作的乐趣而去创作，那不会是偶然的。这绝对有点像生物。”<br />
<br />
Sam：<br />
这是生物化的。我认为在此之前还有很多步骤。<br />
<br />
我们现在要回答问题吗？这真的很棒。</p>
<p><a href="https://nitter.cz/ritageleta/status/1725799427833765978#m">nitter.cz/ritageleta/status/1725799427833765978#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc4MDI2NTQ4NTQ2NzIzODQvcHUvaW1nL1hlaW9PNERtSGZobXFCZUouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727789984348832160#m</id>
            <title>转译：关于 OpenAI 正在开发的 Q* RLHF 的揭秘时间

首先，让我们看看路透社文章中的一些有趣描述：
“资深高管 Mira Murati 在周三对员工表示，一封关于 AI 突破性进展 Q*（读作 Q-Star）的信件促使董事会采取了行动。”
+
“得益于强大的计算资源，这个新模型能解决一些数学问题……虽然目前只能解决小学生级别的数学问题，但这样的成绩让研究人员对 Q* 的未来充满期待。”

现在，请准备好：
OpenAI 的新技术 Q*（Q-star）融合了两个关键元素：Q Learning（一种强化学习算法）和 A Star（一种搜索算法）。

1. Q Learning 非常关键，它是第一个被广泛认可的强化学习算法，至今仍被广泛应用。在这里，Token 或词汇被视为状态，而某些回应则被视为行动。
2. A Star 是一种以其在搜索过程中保存结果于内存而著称的图搜索算法。文章中提到：“得益于强大的计算资源，新模型能解决一些数学问题”，这意味着在新的 RLHF 训练中需要存储大量数据。

搜索对于训练中的多轮优化至关重要。基本上，我认为是将 A* 公式应用于 Q 值，以实现多轮推理。

为什么这种方法可能非常有效但又难以实现呢？
* 多轮优化意味着需要更多的模型前向传递和梯度计算
* 解决复杂数学问题需要这种方法
* 实际上，这可能更接近于 RLAIF</title>
            <link>https://nitter.cz/dotey/status/1727789984348832160#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727789984348832160#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 20:43:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译：关于 OpenAI 正在开发的 Q* RLHF 的揭秘时间<br />
<br />
首先，让我们看看路透社文章中的一些有趣描述：<br />
“资深高管 Mira Murati 在周三对员工表示，一封关于 AI 突破性进展 Q*（读作 Q-Star）的信件促使董事会采取了行动。”<br />
+<br />
“得益于强大的计算资源，这个新模型能解决一些数学问题……虽然目前只能解决小学生级别的数学问题，但这样的成绩让研究人员对 Q* 的未来充满期待。”<br />
<br />
现在，请准备好：<br />
OpenAI 的新技术 Q*（Q-star）融合了两个关键元素：Q Learning（一种强化学习算法）和 A Star（一种搜索算法）。<br />
<br />
1. Q Learning 非常关键，它是第一个被广泛认可的强化学习算法，至今仍被广泛应用。在这里，Token 或词汇被视为状态，而某些回应则被视为行动。<br />
2. A Star 是一种以其在搜索过程中保存结果于内存而著称的图搜索算法。文章中提到：“得益于强大的计算资源，新模型能解决一些数学问题”，这意味着在新的 RLHF 训练中需要存储大量数据。<br />
<br />
搜索对于训练中的多轮优化至关重要。基本上，我认为是将 A* 公式应用于 Q 值，以实现多轮推理。<br />
<br />
为什么这种方法可能非常有效但又难以实现呢？<br />
* 多轮优化意味着需要更多的模型前向传递和梯度计算<br />
* 解决复杂数学问题需要这种方法<br />
* 实际上，这可能更接近于 RLAIF</p>
<p><a href="https://nitter.cz/natolambert/status/1727476436838265324#m">nitter.cz/natolambert/status/1727476436838265324#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727788966454218985#m</id>
            <title>推荐Slides：《ChatGLM: An Alternative to ChatGPT》清华唐杰老师写的，很多ChatGLM的干货

建议有兴趣的同学保存一份

完整链接：https://keg.cs.tsinghua.edu.cn/jietang/publications/iswc23-chatglm.pdf</title>
            <link>https://nitter.cz/dotey/status/1727788966454218985#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727788966454218985#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 20:39:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐Slides：《ChatGLM: An Alternative to ChatGPT》清华唐杰老师写的，很多ChatGLM的干货<br />
<br />
建议有兴趣的同学保存一份<br />
<br />
完整链接：<a href="https://keg.cs.tsinghua.edu.cn/jietang/publications/iswc23-chatglm.pdf">keg.cs.tsinghua.edu.cn/jieta…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wV0Zoclc0QUE1QVR3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wV2JCeldZQUFJNzRuLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wV2hZV1dnQUFPMlVRLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wVzRMMFcwQUEtRnNULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/LatentSpace2000/status/1727666307074830368#m</id>
            <title>RT by @dotey: 本地化了一下</title>
            <link>https://nitter.cz/LatentSpace2000/status/1727666307074830368#m</link>
            <guid isPermaLink="false">https://nitter.cz/LatentSpace2000/status/1727666307074830368#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 12:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本地化了一下</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9ublQtX2FRQUFmZlBzLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9ublVpc2E0QUFVRF9HLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727781480678531395#m</id>
            <title>厉害了👍🏻</title>
            <link>https://nitter.cz/dotey/status/1727781480678531395#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727781480678531395#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 20:09:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>厉害了👍🏻</p>
<p><a href="https://nitter.cz/chee828/status/1727473511747453335#m">nitter.cz/chee828/status/1727473511747453335#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1727604618316972082#m</id>
            <title>RT by @dotey: 推荐一个超实用的网站，Papers with code，https://paperswithcode.com，它是一个论文和对应工程实现（含代码、数据集、测试方法等）的索引工具，下次你看到别人推荐优质论文时，可以直接用它去找代码实现。

有些论文价值很大，会被多次实现（如图二），但工程质量有高有低，你可以在这个网站详情页看到不同实现的 star 数量，择优学习；详情页还包含了数据集、测试方法、关联论文等等，你甚至还可以通过数据集去反查关联论文，简直是研究神器。

之前我都是去 Google 检索或者去 Aminer 上查看，有了这个工具就方便多了。</title>
            <link>https://nitter.cz/Barret_China/status/1727604618316972082#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1727604618316972082#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:26:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐一个超实用的网站，Papers with code，<a href="https://paperswithcode.com">paperswithcode.com</a>，它是一个论文和对应工程实现（含代码、数据集、测试方法等）的索引工具，下次你看到别人推荐优质论文时，可以直接用它去找代码实现。<br />
<br />
有些论文价值很大，会被多次实现（如图二），但工程质量有高有低，你可以在这个网站详情页看到不同实现的 star 数量，择优学习；详情页还包含了数据集、测试方法、关联论文等等，你甚至还可以通过数据集去反查关联论文，简直是研究神器。<br />
<br />
之前我都是去 Google 检索或者去 Aminer 上查看，有了这个工具就方便多了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tcV9iLWJvQUFTS3FNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tc1VFMmFJQUEzcjJJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727604343304495491#m</id>
            <title>建议你们看看这个推文和评论，很多有趣的讨论。

Jim Fan：
合成数据无疑将成为下一个万亿级高质量训练词元的来源。我相信，大多数专业的大语言模型团队都已意识到这一点。真正的挑战是如何持续保持这些数据的质量，避免过早陷入发展的停滞期。

Richard Sutton（@RichardSSutton）的《苦涩的教训》继续引领着人工智能的发展：只有两种模式能够随着计算能力的增加而无限扩展，那就是学习和搜索。这一观点在 2019 年他撰写时就成立，如今依旧适用，我敢说，直到我们解决通用人工智能的问题，这一点都不会改变。

Elon Musk：
确实。有些让人唏嘘的是，人类历史上写过的所有书籍的文字内容都能被存储在一个硬盘上。

而合成数据的数量将远远超出这个规模。

Jim Fan：
很多合成数据将源于具体代理，比如特斯拉 Optimus，前提是我们能够大规模地进行模拟。

Elon Musk：
是的（叹气）

Yann LeCun:
动物和人类凭借极少的训练数据就能迅速变得聪明。我认为，未来的趋势是开发出像动物和人类一样高效学习的新架构。目前使用大量数据（无论是真实的还是合成的）只是一种权宜之计，是因为我们现有方法的限制所迫。

Atlas3d.eth:
完全同意 - 看看乌鸦科的鸟类，它们拥有惊人的推理能力，却只需要很少的数据量。

Yann LeCun:
想象一下，像鹦鹉、狗和章鱼这样的动物大约有 20 亿个神经元。我们如何能让一台拥有 20 亿神经元、10 万亿参数的机器在短短几个月的实时训练中变得像这些动物一样聪明呢？

Eduardo Slonski:
1）我们使用了大量的数据。别忘了，我们不断接收大量的视频、音频和感官数据，更不用说 DNA 中编码的“指令”了。我们并非从零开始学习，而且我们的反应比大语言模型更为广泛和普遍。

2）在新架构的问题上，我同意你的看法。

Yann LeCun:
1. 人类基因组的数据量其实很小，只有 800MB。人类和黑猩猩基因组的差异也就 8MB。这远远不足以解释两者能力上的巨大差异。

2. 以一个两岁孩童所见的视觉数据量来看，实际上是相当有限的：大约 3200 万秒。我们有大约 200 万条视觉神经纤维，每秒大约传输 10 个字节，总计 6E14 字节。而大语言模型的训练数据量通常是 1E13 个 Token，约 2E13 字节，只多出大约 30 倍。

Eduardo Slonski:
基因组并不是作为数据来使用，而是起到指导作用。当我们开始训练模型时，其输出是随机的，但与人类不同，人类出生时已经有了本能的行为和寻找目标的能力。再次思考后，我更加认同你的观点：数据量并非关键，关键在于我们的架构以及针对该架构优化的数据。

Ferdous:
难道数百万年的进化适应不就相当于模型的预训练，而我们一生的经验不就类似于持续的微调吗？🤔

Yann LeCun:
并不完全是这样。这需要被浓缩成极少量的信息。比如，一个小型的 70 亿 Token 的大语言模型就需要 14GB。而你的整个基因组在未压缩的状态下也只有 800MB。人类和黑猩猩基因组之间仅有 1% 的差异，大约 8MB。这远远不足以编码复杂的结构。</title>
            <link>https://nitter.cz/dotey/status/1727604343304495491#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727604343304495491#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:25:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>建议你们看看这个推文和评论，很多有趣的讨论。<br />
<br />
Jim Fan：<br />
合成数据无疑将成为下一个万亿级高质量训练词元的来源。我相信，大多数专业的大语言模型团队都已意识到这一点。真正的挑战是如何持续保持这些数据的质量，避免过早陷入发展的停滞期。<br />
<br />
Richard Sutton（<a href="https://nitter.cz/RichardSSutton" title="Richard Sutton">@RichardSSutton</a>）的《苦涩的教训》继续引领着人工智能的发展：只有两种模式能够随着计算能力的增加而无限扩展，那就是学习和搜索。这一观点在 2019 年他撰写时就成立，如今依旧适用，我敢说，直到我们解决通用人工智能的问题，这一点都不会改变。<br />
<br />
Elon Musk：<br />
确实。有些让人唏嘘的是，人类历史上写过的所有书籍的文字内容都能被存储在一个硬盘上。<br />
<br />
而合成数据的数量将远远超出这个规模。<br />
<br />
Jim Fan：<br />
很多合成数据将源于具体代理，比如特斯拉 Optimus，前提是我们能够大规模地进行模拟。<br />
<br />
Elon Musk：<br />
是的（叹气）<br />
<br />
Yann LeCun:<br />
动物和人类凭借极少的训练数据就能迅速变得聪明。我认为，未来的趋势是开发出像动物和人类一样高效学习的新架构。目前使用大量数据（无论是真实的还是合成的）只是一种权宜之计，是因为我们现有方法的限制所迫。<br />
<br />
Atlas3d.eth:<br />
完全同意 - 看看乌鸦科的鸟类，它们拥有惊人的推理能力，却只需要很少的数据量。<br />
<br />
Yann LeCun:<br />
想象一下，像鹦鹉、狗和章鱼这样的动物大约有 20 亿个神经元。我们如何能让一台拥有 20 亿神经元、10 万亿参数的机器在短短几个月的实时训练中变得像这些动物一样聪明呢？<br />
<br />
Eduardo Slonski:<br />
1）我们使用了大量的数据。别忘了，我们不断接收大量的视频、音频和感官数据，更不用说 DNA 中编码的“指令”了。我们并非从零开始学习，而且我们的反应比大语言模型更为广泛和普遍。<br />
<br />
2）在新架构的问题上，我同意你的看法。<br />
<br />
Yann LeCun:<br />
1. 人类基因组的数据量其实很小，只有 800MB。人类和黑猩猩基因组的差异也就 8MB。这远远不足以解释两者能力上的巨大差异。<br />
<br />
2. 以一个两岁孩童所见的视觉数据量来看，实际上是相当有限的：大约 3200 万秒。我们有大约 200 万条视觉神经纤维，每秒大约传输 10 个字节，总计 6E14 字节。而大语言模型的训练数据量通常是 1E13 个 Token，约 2E13 字节，只多出大约 30 倍。<br />
<br />
Eduardo Slonski:<br />
基因组并不是作为数据来使用，而是起到指导作用。当我们开始训练模型时，其输出是随机的，但与人类不同，人类出生时已经有了本能的行为和寻找目标的能力。再次思考后，我更加认同你的观点：数据量并非关键，关键在于我们的架构以及针对该架构优化的数据。<br />
<br />
Ferdous:<br />
难道数百万年的进化适应不就相当于模型的预训练，而我们一生的经验不就类似于持续的微调吗？🤔<br />
<br />
Yann LeCun:<br />
并不完全是这样。这需要被浓缩成极少量的信息。比如，一个小型的 70 亿 Token 的大语言模型就需要 14GB。而你的整个基因组在未压缩的状态下也只有 800MB。人类和黑猩猩基因组之间仅有 1% 的差异，大约 8MB。这远远不足以编码复杂的结构。</p>
<p><a href="https://nitter.cz/DrJimFan/status/1727505774514180188#m">nitter.cz/DrJimFan/status/1727505774514180188#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727599590872928280#m</id>
            <title>《苦涩的教训》
Rich Sutton
2019 年 3 月 13 日
70 年人工智能研究给我们的最大启示是：依赖计算能力的通用方法最终表现最佳，而且优势明显。这背后的主要原因是摩尔定律，也就是计算成本持续以指数级下降。大部分 AI 研究都是在假设计算资源固定的情况下进行的（在这种情况下，利用人类知识几乎是提升性能的唯一途径），但实际上，在稍长于一般研究项目的时间里，可用的计算资源会大幅增加。研究者为了在短期内取得突破，尝试利用自己对特定领域的人类知识，但从长远看，关键在于计算能力的利用。这两者原本不必相互冲突，但在实践中却常常如此。投入其中一个领域的时间，就意味着在另一个上的缺失。此外，人们在一种方法上的投资也会形成心理承诺。而基于人类知识的方法往往会使系统变得复杂，不利于利用计算能力的通用方法。有很多例子显示 AI 研究者是如何迟迟才领悟到这个苦涩的教训，回顾这些案例非常有启发性。

以计算机国际象棋为例，1997 年击败世界冠军卡斯帕罗夫的方法主要是深度搜索。当时，大多数计算机国际象棋研究者对此表示失望，因为他们更倾向于利用对棋局特殊结构的人类理解。然而，当一个简单但基于搜索的方法，结合特殊的硬件和软件展现出巨大效能时，这些基于人类知识的研究者并不愿意接受失败。他们认为，尽管这次“蛮力”搜索获胜，但它并非一种通用策略，也不是人类下棋的方式。这些研究者本希望基于人类理解的方法能够取胜，对实际结果感到失望。

在计算机围棋的发展中，也出现了类似的模式，只是晚了 20 年。最初的努力都在于避免搜索，尽可能利用对游戏的人类理解和特殊特征，但一旦有效地应用了大规模搜索，这些努力都显得微不足道，甚至有害。在这个过程中，通过自我对弈学习价值函数（在很多其他游戏中也是这样，甚至包括国际象棋，尽管在 1997 年首次击败世界冠军的程序中学习的作用并不大）也非常关键。自我对弈学习和一般学习，就像搜索一样，能够充分利用大量计算资源。在计算机围棋和国际象棋中，研究者最初都是试图利用人类的理解来减少搜索的需要，但最终通过接受搜索和学习才取得了巨大的成功。

在语音识别领域，1970 年代由 DARPA 赞助的一场早期比赛就是一个例子。参赛者包括使用了大量人类知识（如对单词、音素、人类声道的理解）的特殊方法，而另一边则是更依赖统计和大量计算的新方法，基于隐马尔可夫模型（HMMs）。最终，基于统计的方法战胜了基于人类知识的方法。这导致了自然语言处理领域的一次重大转变，随着时间的推移，统计和计算开始成为该领域的主导。深度学习在语音识别中的兴起是这一趋势的最新体现。深度学习方法更少依赖人类知识，使用更多的计算资源，并通过在大型训练集上的学习，极大地提升了语音识别系统的性能。与游戏领域相似，研究人员总是试图创建一个按照他们自己的思维方式工作的系统，但这种尝试最终证明是逆向而行，不仅浪费了大量的研究时间，而且在大量计算资源可用并找到有效利用方法的情况下，这种尝试显得更是多余。

计算机视觉领域也经历了相似的发展模式。早期的方法试图通过搜索边缘、广义圆柱体或 SIFT 特征来处理视觉问题。但在今天，这些方法都被淘汰了。现代的深度学习神经网络仅使用卷积和某些类型的不变性概念，取得了更好的表现。

这是一个重要的教训。作为一个领域，我们还没有完全吸取这一教训，仍在重蹈覆辙。为了识别并避免这种错误，我们必须理解其吸引力所在。我们必须领悟到，试图构建一个基于我们认为自己思考方式的系统是行不通的。苦涩的教训源于这样的历史观察：1) 人工智能研究者经常试图将知识融入他们的代理中；2) 这在短期内总是有益的，也让研究者感到满足；但 3) 从长远来看，这种做法会导致进步停滞，甚至阻碍进一步的发展；4) 真正的突破性进展最终是通过一个相反的方法实现的，这个方法基于通过搜索和学习来扩大计算的规模。这种成功带有苦涩，往往消化不良，因为它是在人类中心化方法之上取得的。

从这个苦涩的教训中，我们应该明白通用方法的巨大力量，即那些随着计算能力的增长而持续扩展的方法。在这方面，似乎可以无限扩展的两种方法是搜索和学习。

苦涩教训中的另一个关键点是，人类心灵的实质内容极其复杂，不可能简化；我们应该放弃试图简单化地理解心灵内容，如空间、物体、多重代理或对称性等概念。这些都是外部世界中任意而复杂的部分，不应该成为我们构建的核心；相反，我们应该构建的是那些能够发现并捕捉这种任意复杂性的元方法。这些方法的核心在于它们能够找到良好的近似，但寻找这些近似的过程应该由我们的方法来完成，而不是我们亲自动手。我们希望 AI 代理能像我们一样具有发现能力，而不是仅仅包含我们已有的发现。将我们的发现直接构建进去，只会使我们更难看清如何实现发现的过程。</title>
            <link>https://nitter.cz/dotey/status/1727599590872928280#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727599590872928280#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:06:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>《苦涩的教训》<br />
Rich Sutton<br />
2019 年 3 月 13 日<br />
70 年人工智能研究给我们的最大启示是：依赖计算能力的通用方法最终表现最佳，而且优势明显。这背后的主要原因是摩尔定律，也就是计算成本持续以指数级下降。大部分 AI 研究都是在假设计算资源固定的情况下进行的（在这种情况下，利用人类知识几乎是提升性能的唯一途径），但实际上，在稍长于一般研究项目的时间里，可用的计算资源会大幅增加。研究者为了在短期内取得突破，尝试利用自己对特定领域的人类知识，但从长远看，关键在于计算能力的利用。这两者原本不必相互冲突，但在实践中却常常如此。投入其中一个领域的时间，就意味着在另一个上的缺失。此外，人们在一种方法上的投资也会形成心理承诺。而基于人类知识的方法往往会使系统变得复杂，不利于利用计算能力的通用方法。有很多例子显示 AI 研究者是如何迟迟才领悟到这个苦涩的教训，回顾这些案例非常有启发性。<br />
<br />
以计算机国际象棋为例，1997 年击败世界冠军卡斯帕罗夫的方法主要是深度搜索。当时，大多数计算机国际象棋研究者对此表示失望，因为他们更倾向于利用对棋局特殊结构的人类理解。然而，当一个简单但基于搜索的方法，结合特殊的硬件和软件展现出巨大效能时，这些基于人类知识的研究者并不愿意接受失败。他们认为，尽管这次“蛮力”搜索获胜，但它并非一种通用策略，也不是人类下棋的方式。这些研究者本希望基于人类理解的方法能够取胜，对实际结果感到失望。<br />
<br />
在计算机围棋的发展中，也出现了类似的模式，只是晚了 20 年。最初的努力都在于避免搜索，尽可能利用对游戏的人类理解和特殊特征，但一旦有效地应用了大规模搜索，这些努力都显得微不足道，甚至有害。在这个过程中，通过自我对弈学习价值函数（在很多其他游戏中也是这样，甚至包括国际象棋，尽管在 1997 年首次击败世界冠军的程序中学习的作用并不大）也非常关键。自我对弈学习和一般学习，就像搜索一样，能够充分利用大量计算资源。在计算机围棋和国际象棋中，研究者最初都是试图利用人类的理解来减少搜索的需要，但最终通过接受搜索和学习才取得了巨大的成功。<br />
<br />
在语音识别领域，1970 年代由 DARPA 赞助的一场早期比赛就是一个例子。参赛者包括使用了大量人类知识（如对单词、音素、人类声道的理解）的特殊方法，而另一边则是更依赖统计和大量计算的新方法，基于隐马尔可夫模型（HMMs）。最终，基于统计的方法战胜了基于人类知识的方法。这导致了自然语言处理领域的一次重大转变，随着时间的推移，统计和计算开始成为该领域的主导。深度学习在语音识别中的兴起是这一趋势的最新体现。深度学习方法更少依赖人类知识，使用更多的计算资源，并通过在大型训练集上的学习，极大地提升了语音识别系统的性能。与游戏领域相似，研究人员总是试图创建一个按照他们自己的思维方式工作的系统，但这种尝试最终证明是逆向而行，不仅浪费了大量的研究时间，而且在大量计算资源可用并找到有效利用方法的情况下，这种尝试显得更是多余。<br />
<br />
计算机视觉领域也经历了相似的发展模式。早期的方法试图通过搜索边缘、广义圆柱体或 SIFT 特征来处理视觉问题。但在今天，这些方法都被淘汰了。现代的深度学习神经网络仅使用卷积和某些类型的不变性概念，取得了更好的表现。<br />
<br />
这是一个重要的教训。作为一个领域，我们还没有完全吸取这一教训，仍在重蹈覆辙。为了识别并避免这种错误，我们必须理解其吸引力所在。我们必须领悟到，试图构建一个基于我们认为自己思考方式的系统是行不通的。苦涩的教训源于这样的历史观察：1) 人工智能研究者经常试图将知识融入他们的代理中；2) 这在短期内总是有益的，也让研究者感到满足；但 3) 从长远来看，这种做法会导致进步停滞，甚至阻碍进一步的发展；4) 真正的突破性进展最终是通过一个相反的方法实现的，这个方法基于通过搜索和学习来扩大计算的规模。这种成功带有苦涩，往往消化不良，因为它是在人类中心化方法之上取得的。<br />
<br />
从这个苦涩的教训中，我们应该明白通用方法的巨大力量，即那些随着计算能力的增长而持续扩展的方法。在这方面，似乎可以无限扩展的两种方法是搜索和学习。<br />
<br />
苦涩教训中的另一个关键点是，人类心灵的实质内容极其复杂，不可能简化；我们应该放弃试图简单化地理解心灵内容，如空间、物体、多重代理或对称性等概念。这些都是外部世界中任意而复杂的部分，不应该成为我们构建的核心；相反，我们应该构建的是那些能够发现并捕捉这种任意复杂性的元方法。这些方法的核心在于它们能够找到良好的近似，但寻找这些近似的过程应该由我们的方法来完成，而不是我们亲自动手。我们希望 AI 代理能像我们一样具有发现能力，而不是仅仅包含我们已有的发现。将我们的发现直接构建进去，只会使我们更难看清如何实现发现的过程。</p>
<p><a href="https://nitter.cz/elonmusk/status/1727587012394295493#m">nitter.cz/elonmusk/status/1727587012394295493#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>