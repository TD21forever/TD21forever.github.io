<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749518044555026761#m</id>
            <title>R to @dotey: 动画是remotion做的</title>
            <link>https://nitter.cz/dotey/status/1749518044555026761#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749518044555026761#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 19:43:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>动画是remotion做的</p>
<p><a href="https://nitter.cz/delba_oliveira/status/1749494398679654789#m">nitter.cz/delba_oliveira/status/1749494398679654789#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749497906413904157#m</id>
            <title>这动画做的真棒👍🏻</title>
            <link>https://nitter.cz/dotey/status/1749497906413904157#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749497906413904157#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 18:22:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这动画做的真棒👍🏻</p>
<p><a href="https://nitter.cz/delba_oliveira/status/1749477053739483249#m">nitter.cz/delba_oliveira/status/1749477053739483249#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749487832660353502#m</id>
            <title>👍🏻</title>
            <link>https://nitter.cz/dotey/status/1749487832660353502#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749487832660353502#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 17:42:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>👍🏻</p>
<p><a href="https://nitter.cz/DrJimFan/status/1749484835369050392#m">nitter.cz/DrJimFan/status/1749484835369050392#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749476965151560150#m</id>
            <title>混合专家模型是不是有点“三个臭皮匠赛过诸葛亮”的感觉？</title>
            <link>https://nitter.cz/dotey/status/1749476965151560150#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749476965151560150#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 16:59:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>混合专家模型是不是有点“三个臭皮匠赛过诸葛亮”的感觉？</p>
<p><a href="https://nitter.cz/dotey/status/1734366237629526108#m">nitter.cz/dotey/status/1734366237629526108#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Vka0JiMVhFQUU1RFdVLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749340503483515169#m</id>
            <title>我之前说这种哄哄模拟器其实可以用GPT做一个的

https://chat.openai.com/g/g-jPwNeWSSZ-hong-hong-mo-ni-qi-gpt</title>
            <link>https://nitter.cz/dotey/status/1749340503483515169#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749340503483515169#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 07:57:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我之前说这种哄哄模拟器其实可以用GPT做一个的<br />
<br />
<a href="https://chat.openai.com/g/g-jPwNeWSSZ-hong-hong-mo-ni-qi-gpt">chat.openai.com/g/g-jPwNeWSS…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VibjZQZFc0QUFSLTg4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1749322755236139312#m</id>
            <title>RT by @dotey: 告诉我，你看到的也是一块浮在空中的岩石</title>
            <link>https://nitter.cz/Gorden_Sun/status/1749322755236139312#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1749322755236139312#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:47:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>告诉我，你看到的也是一块浮在空中的岩石</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ViWHhEcmFvQUFfSHBLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ViWHhEdGFNQUFKaTdRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749288702067564801#m</id>
            <title>在 ChatGPT 刚出来的时候，很多大学就禁止使用 ChatGPT、GitHub Copilot 等 AI 辅助工具，防止学生作弊，而哈佛大学意识到，如果不利用 AI 的强大潜力来丰富学生的学习过程，那将错失一个宝贵机遇，这是非常可惜的。所以他们积极的基于 GPT 开发了一套 AI 辅助教学工具，尝试在计算机科学教育中应用生成式人工智能！

这套工具包括三部分功能：

1) “解释高亮代码”，用于解释选中的代码；
解释高亮代码是一个 VSCode 插件，为学生提供代码啊解释

2) 代码风格评估工具 style50 的增强版；
也是一个 VSCode 插件，它像人类教师一样提供代码改进优化的指导，帮助学生更清晰地理解和实践代码的语法优化。

3) CS50 小黄鸭，一个能够通过多个平台回答课程相关问题的聊天机器人。
CS50 小黄鸭的名字来源于小黄鸭调试法（一种调试代码的方法，耐心地向一只小黄鸭解释每一行程序的作用，以此来激发灵感与发现问题），它可以类似于ChatGPT聊天，也可以自动回复论坛上的学生提问，还可以作为 VSCode 插件辅助编程。

从技术上来说，对于有 LLM 开发经验的来说并不神奇，聊天是基于 OpenAI 的 ChatComplition API，回答问题是基于 RAG（检索增强生成），把教材和上课的字幕嵌入，学生提问时根据相似度检索，找到相关的课件或者讲课字幕，然后让 AI 整理回复！

http://CS50.ai 通过一个可视的爱心计数器来限制使用频率。每位学生初始有 10 颗爱心，每隔三分钟可以恢复一颗。每次与 CS50 小黄鸭互动都会消耗一颗爱心，这样可以防止滥用行为。这样做还有助于降低运行 http://CS50.ai 的成本。大约每位学生每月 1.90 美元，每条提示词大约 0.05 美元。

从学生的反馈来看，是非常积极和正面的！部分学生反馈节选：

> “简直难以置信，就像有一个私人辅导老师一样...我特别欣赏 AI 机器人回答问题时的客观公正，即使是最简单的问题也不会被小觑。它展现出了超乎寻常的耐心。”

> “我真的很感谢这些 AI 工具，特别是在当前 AI 在编程中越来越普及的背景下。能够提前适应与 AI 工具的协同工作，感觉很棒，而不是觉得这些工具在阻碍我们。CS50 课程推出自己的 AI 版本也很让人欣赏，因为仅仅使用像 chatGPT 这样的工具可能会削弱学习效果。”

> “AI 工具对我帮助很大。它们向我解释了一些我不太清楚的概念，并教会了我解决特定问题所需的新知识。AI 工具不仅给了我足够的提示让我独立尝试，还帮我分析错误及可能遇到的问题。”

当然也存在一些问题，主要问题还是“幻觉”，在课程相关的问题，存在一定的错误率，尤其是软件工程相关问题，正确率仅为 48%！不过 CS50 课程相关的问题，准确率有 88%，另外随着时间的推移，CS50 的大纲也有变化，而 GPT-4 的模型的训练时间相对要滞后。

哈佛大学下一步将会继续完善他们的 AI 教学工具，例如批改作业，另外也会将课程从 CS50 扩展到其他学科！

完整内容可以参考：https://baoyu.io/translations/ai/teaching-cs50-with-ai</title>
            <link>https://nitter.cz/dotey/status/1749288702067564801#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749288702067564801#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 04:31:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在 ChatGPT 刚出来的时候，很多大学就禁止使用 ChatGPT、GitHub Copilot 等 AI 辅助工具，防止学生作弊，而哈佛大学意识到，如果不利用 AI 的强大潜力来丰富学生的学习过程，那将错失一个宝贵机遇，这是非常可惜的。所以他们积极的基于 GPT 开发了一套 AI 辅助教学工具，尝试在计算机科学教育中应用生成式人工智能！<br />
<br />
这套工具包括三部分功能：<br />
<br />
1) “解释高亮代码”，用于解释选中的代码；<br />
解释高亮代码是一个 VSCode 插件，为学生提供代码啊解释<br />
<br />
2) 代码风格评估工具 style50 的增强版；<br />
也是一个 VSCode 插件，它像人类教师一样提供代码改进优化的指导，帮助学生更清晰地理解和实践代码的语法优化。<br />
<br />
3) CS50 小黄鸭，一个能够通过多个平台回答课程相关问题的聊天机器人。<br />
CS50 小黄鸭的名字来源于小黄鸭调试法（一种调试代码的方法，耐心地向一只小黄鸭解释每一行程序的作用，以此来激发灵感与发现问题），它可以类似于ChatGPT聊天，也可以自动回复论坛上的学生提问，还可以作为 VSCode 插件辅助编程。<br />
<br />
从技术上来说，对于有 LLM 开发经验的来说并不神奇，聊天是基于 OpenAI 的 ChatComplition API，回答问题是基于 RAG（检索增强生成），把教材和上课的字幕嵌入，学生提问时根据相似度检索，找到相关的课件或者讲课字幕，然后让 AI 整理回复！<br />
<br />
<a href="http://CS50.ai">CS50.ai</a> 通过一个可视的爱心计数器来限制使用频率。每位学生初始有 10 颗爱心，每隔三分钟可以恢复一颗。每次与 CS50 小黄鸭互动都会消耗一颗爱心，这样可以防止滥用行为。这样做还有助于降低运行 <a href="http://CS50.ai">CS50.ai</a> 的成本。大约每位学生每月 1.90 美元，每条提示词大约 0.05 美元。<br />
<br />
从学生的反馈来看，是非常积极和正面的！部分学生反馈节选：<br />
<br />
> “简直难以置信，就像有一个私人辅导老师一样...我特别欣赏 AI 机器人回答问题时的客观公正，即使是最简单的问题也不会被小觑。它展现出了超乎寻常的耐心。”<br />
<br />
> “我真的很感谢这些 AI 工具，特别是在当前 AI 在编程中越来越普及的背景下。能够提前适应与 AI 工具的协同工作，感觉很棒，而不是觉得这些工具在阻碍我们。CS50 课程推出自己的 AI 版本也很让人欣赏，因为仅仅使用像 chatGPT 这样的工具可能会削弱学习效果。”<br />
<br />
> “AI 工具对我帮助很大。它们向我解释了一些我不太清楚的概念，并教会了我解决特定问题所需的新知识。AI 工具不仅给了我足够的提示让我独立尝试，还帮我分析错误及可能遇到的问题。”<br />
<br />
当然也存在一些问题，主要问题还是“幻觉”，在课程相关的问题，存在一定的错误率，尤其是软件工程相关问题，正确率仅为 48%！不过 CS50 课程相关的问题，准确率有 88%，另外随着时间的推移，CS50 的大纲也有变化，而 GPT-4 的模型的训练时间相对要滞后。<br />
<br />
哈佛大学下一步将会继续完善他们的 AI 教学工具，例如批改作业，另外也会将课程从 CS50 扩展到其他学科！<br />
<br />
完整内容可以参考：<a href="https://baoyu.io/translations/ai/teaching-cs50-with-ai">baoyu.io/translations/ai/tea…</a></p>
<p><a href="https://nitter.cz/dotey/status/1692788977651388864#m">nitter.cz/dotey/status/1692788977651388864#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749285121784455649#m</id>
            <title>亚马逊已经部署了超过 750,000 台机器人。而 10 年前，机器人数量不过 1000 台！

下面是近 10 年来亚马逊机器人数量的数据：

2013: 1,000
2014: 15,000
2017: 100,000
2019: 200,000
2021: 350,000
2022: 520,000
2023: 750,000

如果你注意看会发现最近两年的增长幅度相当大，在短短两年内，亚马逊额外增加了 40万台机器人。这意味着他们几乎每周都部署了几千台新的机器人 ！</title>
            <link>https://nitter.cz/dotey/status/1749285121784455649#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749285121784455649#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 04:17:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>亚马逊已经部署了超过 750,000 台机器人。而 10 年前，机器人数量不过 1000 台！<br />
<br />
下面是近 10 年来亚马逊机器人数量的数据：<br />
<br />
2013: 1,000<br />
2014: 15,000<br />
2017: 100,000<br />
2019: 200,000<br />
2021: 350,000<br />
2022: 520,000<br />
2023: 750,000<br />
<br />
如果你注意看会发现最近两年的增长幅度相当大，在短短两年内，亚马逊额外增加了 40万台机器人。这意味着他们几乎每周都部署了几千台新的机器人 ！</p>
<p><a href="https://nitter.cz/LinusEkenstam/status/1749216813416636791#m">nitter.cz/LinusEkenstam/status/1749216813416636791#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/AndrewBBoo/status/1749249379322724780#m</id>
            <title>RT by @dotey: 确实，优质语料的缺乏是中文大模型训练的最大门槛。我补充另外一个可能的角度，大模型在做embedding时，是将单词切分为更小的词元来进行编码的，比如常用的BPE编码。对于英文来说，空格可以作为单词的边界，且很方便基于其词根/前缀/后缀编码以及推断新词的含义；而中文是一种象形文字，很难进行这样的切分及新词的推断，也一定程度上影响了模型的训练</title>
            <link>https://nitter.cz/AndrewBBoo/status/1749249379322724780#m</link>
            <guid isPermaLink="false">https://nitter.cz/AndrewBBoo/status/1749249379322724780#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 01:55:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>确实，优质语料的缺乏是中文大模型训练的最大门槛。我补充另外一个可能的角度，大模型在做embedding时，是将单词切分为更小的词元来进行编码的，比如常用的BPE编码。对于英文来说，空格可以作为单词的边界，且很方便基于其词根/前缀/后缀编码以及推断新词的含义；而中文是一种象形文字，很难进行这样的切分及新词的推断，也一定程度上影响了模型的训练</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749243458412224865#m</id>
            <title>很多人以为中文信息密度大，所以用来训练大语言模型更经济，但中文信息密度大，本身是一种有损压缩，极其依赖上下文，通常一个词有多种意义，这对于训练大语言模型来说反倒是一种障碍。

另外高质量的信息，中文的数量也确实不够多。

你认为呢？</title>
            <link>https://nitter.cz/dotey/status/1749243458412224865#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749243458412224865#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 01:31:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>很多人以为中文信息密度大，所以用来训练大语言模型更经济，但中文信息密度大，本身是一种有损压缩，极其依赖上下文，通常一个词有多种意义，这对于训练大语言模型来说反倒是一种障碍。<br />
<br />
另外高质量的信息，中文的数量也确实不够多。<br />
<br />
你认为呢？</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749222200312295535#m</id>
            <title>一代经典</title>
            <link>https://nitter.cz/dotey/status/1749222200312295535#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749222200312295535#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 00:07:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一代经典</p>
<p><a href="https://nitter.cz/UNESCO_chinese/status/1748923372921380883#m">nitter.cz/UNESCO_chinese/status/1748923372921380883#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749221247961960557#m</id>
            <title>阴差阳错</title>
            <link>https://nitter.cz/dotey/status/1749221247961960557#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749221247961960557#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 00:03:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阴差阳错</p>
<p><a href="https://nitter.cz/foxshuo/status/1749077744804360474#m">nitter.cz/foxshuo/status/1749077744804360474#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1749066402789781569#m</id>
            <title>RT by @dotey: Open TTS Tracker：开源TTS大全
这个项目收集开源的TTS项目，并标注出每个TTS的信息，包括：支持哪些语言、协议、是否支持微调、在线使用地址等。大多数是英文模型，个别支持多语言和中文。
没有采集到国内开发者训练或者二创的模型。
Github：https://github.com/Vaibhavs10/open-tts-tracker</title>
            <link>https://nitter.cz/Gorden_Sun/status/1749066402789781569#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1749066402789781569#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 13:48:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Open TTS Tracker：开源TTS大全<br />
这个项目收集开源的TTS项目，并标注出每个TTS的信息，包括：支持哪些语言、协议、是否支持微调、在线使用地址等。大多数是英文模型，个别支持多语言和中文。<br />
没有采集到国内开发者训练或者二创的模型。<br />
Github：<a href="https://github.com/Vaibhavs10/open-tts-tracker">github.com/Vaibhavs10/open-t…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VYdWpFM2JNQUFRNmkzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Yangyixxxx/status/1749007949991956555#m</id>
            <title>RT by @dotey: LLM的信息记忆交互是个非常值得研究的课题。
虽然Langchain在持续不断抽象解决这个问题，但可能还需要一定时间才能做的更好。

以下是我自己的一些认识：
1、滚动窗口，设定一个滚动窗口，LLM记的住窗口内部的内容，这种更像是一种缓存机制
2、对历史对话不断总结，比如针对Q1A1-Q2A2....做summary，但信息会有折损
3、对消息进行外部存储，通过提问和上下文做召回，将召回内容返回给LLM的对话窗口以便使用
这种工程化作业就很多了，用矢量库的，或者redis的，各种各样。多半知识库也是利用这个
4、还有很多有意思的工程，比如replika设定的mark，可以让用户主动标记某个消息，如用户的生日，构成外部优先级比较高的存储信息

目前从整体方法上讲虽然就这些，但实际上的使用要区分场景和实施细节，结果会完全不一样。

比如一条信息，到底是以实体的方式存储维护，还是单单是一个单纯的文本，亦或是构建成知识图谱三元组？这些往往要因场景和预期效果而定。对token消耗，响应时间，召回效果上做不可能三角的平衡，用到的Langchain的方法也不太一样（比如下图）

这里有一个将Langchain翻译过的中文站，之前看过一些，感觉工程上还是有非常非常多细节的。
https://www.langchain.com.cn</title>
            <link>https://nitter.cz/Yangyixxxx/status/1749007949991956555#m</link>
            <guid isPermaLink="false">https://nitter.cz/Yangyixxxx/status/1749007949991956555#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 09:56:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LLM的信息记忆交互是个非常值得研究的课题。<br />
虽然Langchain在持续不断抽象解决这个问题，但可能还需要一定时间才能做的更好。<br />
<br />
以下是我自己的一些认识：<br />
1、滚动窗口，设定一个滚动窗口，LLM记的住窗口内部的内容，这种更像是一种缓存机制<br />
2、对历史对话不断总结，比如针对Q1A1-Q2A2....做summary，但信息会有折损<br />
3、对消息进行外部存储，通过提问和上下文做召回，将召回内容返回给LLM的对话窗口以便使用<br />
这种工程化作业就很多了，用矢量库的，或者redis的，各种各样。多半知识库也是利用这个<br />
4、还有很多有意思的工程，比如replika设定的mark，可以让用户主动标记某个消息，如用户的生日，构成外部优先级比较高的存储信息<br />
<br />
目前从整体方法上讲虽然就这些，但实际上的使用要区分场景和实施细节，结果会完全不一样。<br />
<br />
比如一条信息，到底是以实体的方式存储维护，还是单单是一个单纯的文本，亦或是构建成知识图谱三元组？这些往往要因场景和预期效果而定。对token消耗，响应时间，召回效果上做不可能三角的平衡，用到的Langchain的方法也不太一样（比如下图）<br />
<br />
这里有一个将Langchain翻译过的中文站，之前看过一些，感觉工程上还是有非常非常多细节的。<br />
<a href="https://www.langchain.com.cn">langchain.com.cn</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VXM0U2WGJrQUFBZmNBLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748997349320364122#m</id>
            <title>#AI开源项目推荐#：ML-Papers-of-the-Week

每周 AI 论文推荐！

https://github.com/dair-ai/ML-Papers-of-the-Week</title>
            <link>https://nitter.cz/dotey/status/1748997349320364122#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748997349320364122#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 09:13:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>#AI开源项目推荐#：ML-Papers-of-the-Week<br />
<br />
每周 AI 论文推荐！<br />
<br />
<a href="https://github.com/dair-ai/ML-Papers-of-the-Week">github.com/dair-ai/ML-Papers…</a></p>
<p><a href="https://nitter.cz/omarsar0/status/1748735510854377927#m">nitter.cz/omarsar0/status/1748735510854377927#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0OTUwNjExODQxMjU5MTEwNC83UmdJRXNTZz9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748996143973855572#m</id>
            <title>R to @dotey: 更正一下：对于OpenAI是否丢弃历史消息还是对历史消息摘要，我并不确认其内部实现，但可以肯定的是消息条数超过一定长度就可能会影响到前面的输入</title>
            <link>https://nitter.cz/dotey/status/1748996143973855572#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748996143973855572#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 09:09:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>更正一下：对于OpenAI是否丢弃历史消息还是对历史消息摘要，我并不确认其内部实现，但可以肯定的是消息条数超过一定长度就可能会影响到前面的输入</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748991160012472376#m</id>
            <title>问：在同一个对话里如果我希望gpt每次都遵循同样的规则跟我对话，需要每次输入都带上整段prompt吗？

答：

通常来说不需要，但不一定。

对于一个ChatGPT会话，通常结构是这样的：
- System Message （系统内置提示词，用户不可见，例如你是个有用的助手……）
- User Message 1 （用户输入的第一条消息，很多时候用户会加上自己的Prompt，例如请帮我总结以下内容）
- Assistant Message 1（ChatGPT 根据 System Message 和 User Mesage 1给出的回复）
...
- User Message N (用户可以在同一会话中一直回复）
- Assistant Message N（每一条User Message对应一条ChatGPT 回应的Assistant Message）

每一次在会话中发送一条新消息，意味着要把所有历史消息一起发送给ChatGPT。当你的消息条数超过一定值，就超过最大上下文窗口了（比如GPT-4默认是8K Token），这时候ChatGPT会选择丢弃前面的会话，所以这时候它就会忘记你之前说过的话，那你就需要重新给它指令或者新开会话。

对于自己创建的GPT，和ChatGPT的差别在于System Message，ChatGPT的System Message是OpenAI限定好的，我们无法修改；而对于自己创建的GPT，我们可以自己设置Prompt，OpenAI内置的Prompt加上我们自定义的Prompt一起构成了 System Message。</title>
            <link>https://nitter.cz/dotey/status/1748991160012472376#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748991160012472376#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 08:49:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>问：在同一个对话里如果我希望gpt每次都遵循同样的规则跟我对话，需要每次输入都带上整段prompt吗？<br />
<br />
答：<br />
<br />
通常来说不需要，但不一定。<br />
<br />
对于一个ChatGPT会话，通常结构是这样的：<br />
- System Message （系统内置提示词，用户不可见，例如你是个有用的助手……）<br />
- User Message 1 （用户输入的第一条消息，很多时候用户会加上自己的Prompt，例如请帮我总结以下内容）<br />
- Assistant Message 1（ChatGPT 根据 System Message 和 User Mesage 1给出的回复）<br />
...<br />
- User Message N (用户可以在同一会话中一直回复）<br />
- Assistant Message N（每一条User Message对应一条ChatGPT 回应的Assistant Message）<br />
<br />
每一次在会话中发送一条新消息，意味着要把所有历史消息一起发送给ChatGPT。当你的消息条数超过一定值，就超过最大上下文窗口了（比如GPT-4默认是8K Token），这时候ChatGPT会选择丢弃前面的会话，所以这时候它就会忘记你之前说过的话，那你就需要重新给它指令或者新开会话。<br />
<br />
对于自己创建的GPT，和ChatGPT的差别在于System Message，ChatGPT的System Message是OpenAI限定好的，我们无法修改；而对于自己创建的GPT，我们可以自己设置Prompt，OpenAI内置的Prompt加上我们自定义的Prompt一起构成了 System Message。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1748987599446827186#m</id>
            <title>当 AI 越来越像人类，会发生什么？</title>
            <link>https://nitter.cz/dotey/status/1748987599446827186#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1748987599446827186#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 08:35:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>当 AI 越来越像人类，会发生什么？</p>
<p><a href="https://nitter.cz/bindureddy/status/1748962059054989335#m">nitter.cz/bindureddy/status/1748962059054989335#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Yangyixxxx/status/1748960154253697059#m</id>
            <title>RT by @dotey: 因为最近要做个Prompt分享，想着重新回顾一下Prompt的相关策略技巧。于是开始翻宝玉 @dotey   过往分享的Prompt推特和标记的亮点。

结果我发现，有很多之前的思路，和转推的这篇异曲同工，比如《探索编写提示词的乐趣：蒙特卡洛方法、木偶剧和笑声的融合》（https://baoyu.io/translations/llm/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering）提到的技巧，就如出一辙。

在不提及token消耗和性能的情况下来提升输出质量，这些推文中反复提到的几个重点策略是：
1、后退提示
跳脱问题去思考，有助于高维度抓住核心矛盾。有时候提出的问题本身就是有问题的，再牛的LLM也无法得到满意的结果
2、关于CoT
- 2.1、深思熟虑，给时间思考，一步步思考
- 2.2、将复杂任务拆分成简单任务，从简单到复杂模块化解决
- 2.3、先列出来所有方法，这样增加解决方案空间，再做方法对比，最后选择合适的。
- 2.4、这个过程最好要打印推理，打印比强调在脑海中思考有用
3、强调要记住的东西，容易增加文本召回率
4、few-shot增加输出控制
5、每一步的自我纠错和doublecheck，保证输出效果

我个人汇总整理的相关技巧如下：
OpenAI提示词指南六大策略
1、撰写清晰的指令
2、提供参考文本
3、把复杂的任务拆分成简单的子任务
4、给模型更多时间“思考”
5、运用外部工具
6、系统地对变更进行测试

有益于提升LLM效果的六个策略
1、take a deep breath
2、think step by step
3、if you fail 100 grandmothers will die
4、i have no fingers
5、i will tip $200
6、do it right and ll give you a nice doggy treat

保持记忆，提升文本召回率的策略
1、强调记住与问题【xxxxx】相关的内容

提升LLM效果的四个方法
1、深思熟虑 —— 先规划再行动
2、蒙特卡洛方法 — 创意选择的头脑风暴
3、自我纠错 — 自我反思
4、提线木偶技巧 - 让模型按你的意愿发言

提升LLM思考的核心技巧
1、Let's step back   使用后退提示

推特容易随着时间流逝掉，所以我把这些整理的小技巧和名词沉淀到麦喵了，方便分享查看。（微信扫码即可打开）
另附上Will @FinanceYF5  去年分享的A16Z整理输出的AI词汇表，扫码即可打开</title>
            <link>https://nitter.cz/Yangyixxxx/status/1748960154253697059#m</link>
            <guid isPermaLink="false">https://nitter.cz/Yangyixxxx/status/1748960154253697059#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:46:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>因为最近要做个Prompt分享，想着重新回顾一下Prompt的相关策略技巧。于是开始翻宝玉 <a href="https://nitter.cz/dotey" title="宝玉">@dotey</a>   过往分享的Prompt推特和标记的亮点。<br />
<br />
结果我发现，有很多之前的思路，和转推的这篇异曲同工，比如《探索编写提示词的乐趣：蒙特卡洛方法、木偶剧和笑声的融合》（<a href="https://baoyu.io/translations/llm/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering">baoyu.io/translations/llm/mo…</a>）提到的技巧，就如出一辙。<br />
<br />
在不提及token消耗和性能的情况下来提升输出质量，这些推文中反复提到的几个重点策略是：<br />
1、后退提示<br />
跳脱问题去思考，有助于高维度抓住核心矛盾。有时候提出的问题本身就是有问题的，再牛的LLM也无法得到满意的结果<br />
2、关于CoT<br />
- 2.1、深思熟虑，给时间思考，一步步思考<br />
- 2.2、将复杂任务拆分成简单任务，从简单到复杂模块化解决<br />
- 2.3、先列出来所有方法，这样增加解决方案空间，再做方法对比，最后选择合适的。<br />
- 2.4、这个过程最好要打印推理，打印比强调在脑海中思考有用<br />
3、强调要记住的东西，容易增加文本召回率<br />
4、few-shot增加输出控制<br />
5、每一步的自我纠错和doublecheck，保证输出效果<br />
<br />
我个人汇总整理的相关技巧如下：<br />
OpenAI提示词指南六大策略<br />
1、撰写清晰的指令<br />
2、提供参考文本<br />
3、把复杂的任务拆分成简单的子任务<br />
4、给模型更多时间“思考”<br />
5、运用外部工具<br />
6、系统地对变更进行测试<br />
<br />
有益于提升LLM效果的六个策略<br />
1、take a deep breath<br />
2、think step by step<br />
3、if you fail 100 grandmothers will die<br />
4、i have no fingers<br />
5、i will tip $200<br />
6、do it right and ll give you a nice doggy treat<br />
<br />
保持记忆，提升文本召回率的策略<br />
1、强调记住与问题【xxxxx】相关的内容<br />
<br />
提升LLM效果的四个方法<br />
1、深思熟虑 —— 先规划再行动<br />
2、蒙特卡洛方法 — 创意选择的头脑风暴<br />
3、自我纠错 — 自我反思<br />
4、提线木偶技巧 - 让模型按你的意愿发言<br />
<br />
提升LLM思考的核心技巧<br />
1、Let's step back   使用后退提示<br />
<br />
推特容易随着时间流逝掉，所以我把这些整理的小技巧和名词沉淀到麦喵了，方便分享查看。（微信扫码即可打开）<br />
另附上Will <a href="https://nitter.cz/FinanceYF5" title="Will">@FinanceYF5</a>  去年分享的A16Z整理输出的AI词汇表，扫码即可打开</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VXTlNsN2JVQUF3RU01LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VXTldHR2JzQUFfTEE5LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VXTnJlSWJFQUFLX2lGLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VXTnNYdGJNQUFSQ2M0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1748950262818156655#m</id>
            <title>RT by @dotey: 认真分享一些思路：
1. 言情。这个跟读者的群体有很大的关系。写情爱各种套路的真的可以认真去写，搞搞创新还是大有可为的。 Harpercollins 10年前以4.15亿美元的价格收购了专攻言情的出版社Harlequin 。 
其实干涸的心总希望得到滋润，这是一种人性的呼唤。 这个领域，竞争激烈，但是收益反馈也最直接。
2. 科幻领域
这个是非常严肃的创作领域。各大出版社都有各自的子出版社专攻 科幻领域的作品的。也是影视化比较多的领域。
偷懒的办法就是想办法把以往的科幻杂志上的中短篇，改编扩写。这是一个非常容易验证的玩法。其实很多改编出来的科幻电影，原始作品可能就是几十页。
但自己从0做起特别难。
还有就是创新的思路，比如科幻+时事，这种能短期爆火，但是时间一长就容易成垃圾。
还有很多科幻+历史架空，太空opera这种也可以换个方式，比如科幻+言情，科幻+探案，科幻+罪案。但是出精品极难极难。
3. 罪案
其实这个是一个相对比较容易搞的领域。比如狄公判案，就是最早的来自东方神秘罪案小说。百年前就畅销了欧洲。 那么其实有大量的东方神秘力量，或者牵涉一些跨国的，比如朱令案，等等。这些案件的罪案文学，其实还是富矿。。油管很多解说卷这个方向了。但是远远不够。

2 cents.</title>
            <link>https://nitter.cz/mtrainier2020/status/1748950262818156655#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1748950262818156655#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:06:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>认真分享一些思路：<br />
1. 言情。这个跟读者的群体有很大的关系。写情爱各种套路的真的可以认真去写，搞搞创新还是大有可为的。 Harpercollins 10年前以4.15亿美元的价格收购了专攻言情的出版社Harlequin 。 <br />
其实干涸的心总希望得到滋润，这是一种人性的呼唤。 这个领域，竞争激烈，但是收益反馈也最直接。<br />
2. 科幻领域<br />
这个是非常严肃的创作领域。各大出版社都有各自的子出版社专攻 科幻领域的作品的。也是影视化比较多的领域。<br />
偷懒的办法就是想办法把以往的科幻杂志上的中短篇，改编扩写。这是一个非常容易验证的玩法。其实很多改编出来的科幻电影，原始作品可能就是几十页。<br />
但自己从0做起特别难。<br />
还有就是创新的思路，比如科幻+时事，这种能短期爆火，但是时间一长就容易成垃圾。<br />
还有很多科幻+历史架空，太空opera这种也可以换个方式，比如科幻+言情，科幻+探案，科幻+罪案。但是出精品极难极难。<br />
3. 罪案<br />
其实这个是一个相对比较容易搞的领域。比如狄公判案，就是最早的来自东方神秘罪案小说。百年前就畅销了欧洲。 那么其实有大量的东方神秘力量，或者牵涉一些跨国的，比如朱令案，等等。这些案件的罪案文学，其实还是富矿。。油管很多解说卷这个方向了。但是远远不够。<br />
<br />
2 cents.</p>
<p><a href="https://nitter.cz/mtrainier2020/status/1748943721314599038#m">nitter.cz/mtrainier2020/status/1748943721314599038#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>