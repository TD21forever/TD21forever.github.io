<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740464647638012035#m</id>
            <title>DreamGaussian4D：4D 高斯 Splatting 的生成式模型    

在4D内容生产领域，近年来我们已经取得了显著的进步。然而，目前的方法还存在优化时间过长、运动控制能力欠缺，以及细节表现力不足等问题。

在本论文中，我们引入了一种名为DreamGaussian4D的高效4D生成框架，该框架建立在4D高斯Splatting表达（或者说表示方法）之上。

我们的核心理念在于，相比隐式表示，高斯 Splatting通过对空间转换明确建模，更适合用于4D生成的应用场景。对于DreamGaussian4D，它可以将优化时间大幅度缩减，从几小时缩短至几分钟，同时能够灵活地控制生成的3D运动，并且能产生可以在3D引擎中高效渲染的动态网格。</title>
            <link>https://nitter.cz/dotey/status/1740464647638012035#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740464647638012035#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 20:08:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamGaussian4D：4D 高斯 Splatting 的生成式模型    <br />
<br />
在4D内容生产领域，近年来我们已经取得了显著的进步。然而，目前的方法还存在优化时间过长、运动控制能力欠缺，以及细节表现力不足等问题。<br />
<br />
在本论文中，我们引入了一种名为DreamGaussian4D的高效4D生成框架，该框架建立在4D高斯Splatting表达（或者说表示方法）之上。<br />
<br />
我们的核心理念在于，相比隐式表示，高斯 Splatting通过对空间转换明确建模，更适合用于4D生成的应用场景。对于DreamGaussian4D，它可以将优化时间大幅度缩减，从几小时缩短至几分钟，同时能够灵活地控制生成的3D运动，并且能产生可以在3D引擎中高效渲染的动态网格。</p>
<p><a href="https://nitter.cz/janusch_patas/status/1740351331213476131#m">nitter.cz/janusch_patas/status/1740351331213476131#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/lewangx/status/1740262318599237676#m</id>
            <title>RT by @dotey: 太牛了吧，膜拜。前段时间 Monica 花了几百万收购了插件 ChatGPT for Google，是 @wong2_x 大佬开发的那个吗？</title>
            <link>https://nitter.cz/lewangx/status/1740262318599237676#m</link>
            <guid isPermaLink="false">https://nitter.cz/lewangx/status/1740262318599237676#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 06:44:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>太牛了吧，膜拜。前段时间 Monica 花了几百万收购了插件 ChatGPT for Google，是 <a href="https://nitter.cz/wong2_x" title="wong2">@wong2_x</a> 大佬开发的那个吗？</p>
<p><a href="https://nitter.cz/JourneymanChina/status/1739864313916227710#m">nitter.cz/JourneymanChina/status/1739864313916227710#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740249135268372621#m</id>
            <title>如何写得过ChatGPT？
摘自故事会，转载自沈沉舟微博</title>
            <link>https://nitter.cz/dotey/status/1740249135268372621#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740249135268372621#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 05:51:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如何写得过ChatGPT？<br />
摘自故事会，转载自沈沉舟微博</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhWjJkN1dnQUFsMV9vLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhWjNZZVdjQUFyUFBTLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhYlR3VFh3QUE1b0RvLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/9hills/status/1740237429381173548#m</id>
            <title>RT by @dotey: (1/2) 中文 Emebedding &amp; Reranker 模型选型

1. 大部分模型的序列长度是 512 tokens。 8192 可尝试 tao-8k，1024 可尝试 stella。
2. 在专业数据领域上，嵌入模型的表现不如 BM25，但是微调可以大大提升效果。

https://github.com/ninehills/blog/issues/111</title>
            <link>https://nitter.cz/9hills/status/1740237429381173548#m</link>
            <guid isPermaLink="false">https://nitter.cz/9hills/status/1740237429381173548#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 05:05:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>(1/2) 中文 Emebedding &amp; Reranker 模型选型<br />
<br />
1. 大部分模型的序列长度是 512 tokens。 8192 可尝试 tao-8k，1024 可尝试 stella。<br />
2. 在专业数据领域上，嵌入模型的表现不如 BM25，但是微调可以大大提升效果。<br />
<br />
<a href="https://github.com/ninehills/blog/issues/111">github.com/ninehills/blog/is…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MDIzNzQzMjg4MzQwMDcwNC9BTUVRbzMtcT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740239766267044051#m</id>
            <title>基本赞同，Gemini不给一席之地是不是有点不给Google面子😄
Whisper也是很不错的</title>
            <link>https://nitter.cz/dotey/status/1740239766267044051#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740239766267044051#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 05:14:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>基本赞同，Gemini不给一席之地是不是有点不给Google面子😄<br />
Whisper也是很不错的</p>
<p><a href="https://nitter.cz/Gorden_Sun/status/1740232589796127194#m">nitter.cz/Gorden_Sun/status/1740232589796127194#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740237165597237446#m</id>
            <title>优秀的产品，祝越来越好👍</title>
            <link>https://nitter.cz/dotey/status/1740237165597237446#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740237165597237446#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 05:04:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>优秀的产品，祝越来越好👍</p>
<p><a href="https://nitter.cz/MemoAI_/status/1740221445031284792#m">nitter.cz/MemoAI_/status/1740221445031284792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740185246212239650#m</id>
            <title>RT by @dotey: 知名律师解读纽约时报起诉OpenAI侵犯其版权，用于训练AI模型

她表示纽约时报提供的证据很充分，而且OpenAI似乎使用了纽约时报大量的内容作为一个单一数据集进行了AI训练！

她认为该案具有历史意义，可能是人工智能和版权的分水岭。🤔

引发连锁反应…</title>
            <link>https://nitter.cz/xiaohuggg/status/1740185246212239650#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740185246212239650#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 01:37:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>知名律师解读纽约时报起诉OpenAI侵犯其版权，用于训练AI模型<br />
<br />
她表示纽约时报提供的证据很充分，而且OpenAI似乎使用了纽约时报大量的内容作为一个单一数据集进行了AI训练！<br />
<br />
她认为该案具有历史意义，可能是人工智能和版权的分水岭。🤔<br />
<br />
引发连锁反应…</p>
<p><a href="https://nitter.cz/CeciliaZin/status/1740109462319644905#m">nitter.cz/CeciliaZin/status/1740109462319644905#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Cydiar404/status/1740191550708261045#m</id>
            <title>RT by @dotey: 今天我们将会对 OpenAI Enterprise API 能力进行测试，需要压力测试，目前封装完成，需要有API调用的用户帮忙测试，陆续放出120刀APIkey，目标今日用光，有条件的私信过来！</title>
            <link>https://nitter.cz/Cydiar404/status/1740191550708261045#m</link>
            <guid isPermaLink="false">https://nitter.cz/Cydiar404/status/1740191550708261045#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 02:02:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天我们将会对 OpenAI Enterprise API 能力进行测试，需要压力测试，目前封装完成，需要有API调用的用户帮忙测试，陆续放出120刀APIkey，目标今日用光，有条件的私信过来！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/9hills/status/1740191003779465428#m</id>
            <title>RT by @dotey: Huggingface 模型太大，如果受限于梯子流量或者速度太慢，推荐一个国内镜像站，大部分情况下配置环境变量即可无缝使用。

作者应该也是用爱发电，切勿滥用。实际测试国内家宽满速。

HF_ENDPOINT=https://hf-mirror.com python your_script.py</title>
            <link>https://nitter.cz/9hills/status/1740191003779465428#m</link>
            <guid isPermaLink="false">https://nitter.cz/9hills/status/1740191003779465428#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 02:00:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Huggingface 模型太大，如果受限于梯子流量或者速度太慢，推荐一个国内镜像站，大部分情况下配置环境变量即可无缝使用。<br />
<br />
作者应该也是用爱发电，切勿滥用。实际测试国内家宽满速。<br />
<br />
HF_ENDPOINT=<a href="https://hf-mirror.com">hf-mirror.com</a> python your_script.py</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740171072568188935#m</id>
            <title>经济学人的这篇文章很有意思：《What the tractor and the horse tell you about generative AI -- A short history of tractors in English》

这篇文章将拖拉机的历史和生成式AI进行了类比，认为现在的ChatGPT就像当年拖拉机刚发明的时候。虽然拖拉机很先进，但还是花了几十年才开始流行。

拖拉机之所以很长时间才流行，主要有三个原因：

1. 首先，早期的拖拉机技术并没有最初想象的那么有用，需要进一步改进。

20 世纪初期的拖拉机体积庞大，虽适合耕地和一些其他作业，但不适宜耕种生长中的农田。许多早期型号配备金属轮而非轮胎，因此容易陷入泥泞，并且价格昂贵。

2. 其次，采用这种技术需要改变劳动力市场，这需要时间。

在 20 世纪 30 年代大萧条期间，农业中的实际平均工资下降，许多农民发现雇人管理马匹（随时可解雇）比购买拖拉机更划算。但到了二战时期，劳动力短缺导致实际工资迅速上升，机械化显得更为划算。

3. 最后，农场本身也需要进行一系列的转型。

拖拉机在大型农场效果更佳，使用拖拉机并从中获利的农民同时也考虑扩大耕地面积。然而，扩大农场需要时间，农民需要筹集资金，并与土地所有者进行谈判。

当前大部分的 AI 模型仍然像是装着金属轮子而非橡胶轮胎，意味着它们在速度、功效或可靠性上还不足以投入商业使用。

现在经济形势不好人力便宜，企业没有动力去转型 AI，另外企业还没准备好针对 AI 进行业务重组和对内部数据的整合。

不管新技术有多么先进，社会适应它总是需要相当长的时间。

原文：https://www.economist.com/christmas-specials/2023/12/20/a-short-history-of-tractors-in-english
译文：https://baoyu.io/translations/ai/what-the-tractor-and-the-horse-tell-you-about-generative-ai</title>
            <link>https://nitter.cz/dotey/status/1740171072568188935#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740171072568188935#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 00:41:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>经济学人的这篇文章很有意思：《What the tractor and the horse tell you about generative AI -- A short history of tractors in English》<br />
<br />
这篇文章将拖拉机的历史和生成式AI进行了类比，认为现在的ChatGPT就像当年拖拉机刚发明的时候。虽然拖拉机很先进，但还是花了几十年才开始流行。<br />
<br />
拖拉机之所以很长时间才流行，主要有三个原因：<br />
<br />
1. 首先，早期的拖拉机技术并没有最初想象的那么有用，需要进一步改进。<br />
<br />
20 世纪初期的拖拉机体积庞大，虽适合耕地和一些其他作业，但不适宜耕种生长中的农田。许多早期型号配备金属轮而非轮胎，因此容易陷入泥泞，并且价格昂贵。<br />
<br />
2. 其次，采用这种技术需要改变劳动力市场，这需要时间。<br />
<br />
在 20 世纪 30 年代大萧条期间，农业中的实际平均工资下降，许多农民发现雇人管理马匹（随时可解雇）比购买拖拉机更划算。但到了二战时期，劳动力短缺导致实际工资迅速上升，机械化显得更为划算。<br />
<br />
3. 最后，农场本身也需要进行一系列的转型。<br />
<br />
拖拉机在大型农场效果更佳，使用拖拉机并从中获利的农民同时也考虑扩大耕地面积。然而，扩大农场需要时间，农民需要筹集资金，并与土地所有者进行谈判。<br />
<br />
当前大部分的 AI 模型仍然像是装着金属轮子而非橡胶轮胎，意味着它们在速度、功效或可靠性上还不足以投入商业使用。<br />
<br />
现在经济形势不好人力便宜，企业没有动力去转型 AI，另外企业还没准备好针对 AI 进行业务重组和对内部数据的整合。<br />
<br />
不管新技术有多么先进，社会适应它总是需要相当长的时间。<br />
<br />
原文：<a href="https://www.economist.com/christmas-specials/2023/12/20/a-short-history-of-tractors-in-english">economist.com/christmas-spec…</a><br />
译文：<a href="https://baoyu.io/translations/ai/what-the-tractor-and-the-horse-tell-you-about-generative-ai">baoyu.io/translations/ai/wha…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NaUlQ4b1hZQUFLdm51LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NaVVBYNldVQUlpc05YLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1739943384658383286#m</id>
            <title>RT by @dotey: 腾讯的MotionCtrl发布了源码
用于控制AI生成视频中物体的运动路径，除了常规的镜头控制外，支持手绘轨迹来控制视频中物体的运动路径，支持AnimateDiff。
项目地址：https://wzhouxiff.github.io/projects/MotionCtrl/
Github：https://github.com/TencentARC/MotionCtrl</title>
            <link>https://nitter.cz/Gorden_Sun/status/1739943384658383286#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1739943384658383286#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 09:36:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>腾讯的MotionCtrl发布了源码<br />
用于控制AI生成视频中物体的运动路径，除了常规的镜头控制外，支持手绘轨迹来控制视频中物体的运动路径，支持AnimateDiff。<br />
项目地址：<a href="https://wzhouxiff.github.io/projects/MotionCtrl/">wzhouxiff.github.io/projects…</a><br />
Github：<a href="https://github.com/TencentARC/MotionCtrl">github.com/TencentARC/Motion…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzk5NDMzNTczNzAyNzc4ODgvcHUvaW1nLzhCZndjbzdrY2tEazlDQk8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740145227682193667#m</id>
            <title>作者写了一篇论文：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4

总结下来就是 26 条有效的提示词技巧，绝大部分都很熟悉了，不过温习一下也不错！

1 - 与大型语言模型 (LLM) 交流无需使用礼貌用语，如“请”、“谢谢”等，直接表达需求即可。

2 - 在提示中指明目标受众，比如说受众是该领域的专家。

3 - 把复杂任务拆解成一系列简单的提示，以进行交互式对话。

4 - 使用肯定的指令词，如“执行”，避免使用否定词汇，如“不要”。

5 - 当你需要更清晰地理解某个主题、观点或任何信息时，可以尝试使用以下提示方式：
   o 简单地解释一下[具体主题]。
   o 像对11岁的孩子一样向我解释。
   o 像对一个[领域]新手一样向我解释。
   o 用浅显易懂的语言写作[文章/文本/段落]，就像是在向一个5岁孩子解释。

6 - 添加“我愿意支付 $xxx 的小费以获得更好的方案！”

7 - 采用示例驱动的提示方式（使用少样本提示法）。

8 - 格式化提示时，先写上‘###指令###’，然后根据需要添加‘###示例###’或‘###问题###’。接着展示你的内容，用一行或多行空行分隔各个部分，包括指令、示例、问题、背景和输入数据。

9 - 使用这样的短语：“你的任务是”和“必须完成”。

10 - 使用这样的短语：“将会受到处罚”。

11 - 使用“以自然且类似人类的方式回答问题”作为你的提示。

12 - 使用引导性的词汇，比如“逐步思考”。

13 - 在提示中加入“确保你的回答无偏见，不依赖于刻板印象”。

14 - 让模型通过向你提问来澄清具体的细节和需求，直到它获取足够的信息来提供所需的输出，例如：“从现在开始，请向我提出问题以便......”。

15 - 当你想要学习特定的主题或概念，并测试自己的理解时，可以使用这样的短语：“教我[某个定理/主题/规则]，在教学结束时包含一个测验，但不要直接告诉我答案。等我回答后再告诉我是否正确”。

16 - 为大型语言模型指定一个特定角色。

17 - 使用明确的分隔符。

18 - 在一个提示中重复特定单词或短语多次。

19 - 结合思维链路 (Chain-of-thought，CoT) 和少样本提示的方法。

20 - 使用输出引导符，即在提示的末尾加上期望回答的开头。这样做可以引导输出内容的方向。

21 - 撰写一篇详细的论文/文本/段落/文章时，可以这样指示：“请为我详细写一篇关于[主题]的[论文/文本/段落]，并添加所有必要的信息”。

22 - 当需要修改特定文本但不改变其风格时，可以这样指示：“尝试修改用户提交的每个段落。你应当只改进语法和词汇，确保文本听起来自然，但不要改变其原有的写作风格，如将正式文体变为非正式文体”。

23 - 面对可能涉及多个文件的复杂编程任务时，可以这样提示：“从现在开始，每当你生成涉及多个文件的代码时，创建一个[编程语言]脚本，自动创建所需文件或修改现有文件以插入生成的代码。[你的问题]”。

24 - 当你想用特定的词汇、短语或句子开始或继续一段文本时，可以这样提示：o “我为你提供了开头[歌词/故事/段落/论文...]：[插入的词句]。请根据这些词句继续写下去，保持内容的连贯性”。

25 - 明确说明模型在生成内容时必须遵循的要求，可以是关键词、规则、提示或指示。

26 - 撰写任何类型的文本，如论文或段落，且想要其与提供的样本风格相似时，可以这样指示：o “请根据提供的段落[/标题/文本/论文/答案]的风格撰写”。

论文地址：https://arxiv.org/pdf/2312.16171.pdf</title>
            <link>https://nitter.cz/dotey/status/1740145227682193667#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740145227682193667#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 22:58:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者写了一篇论文：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4<br />
<br />
总结下来就是 26 条有效的提示词技巧，绝大部分都很熟悉了，不过温习一下也不错！<br />
<br />
1 - 与大型语言模型 (LLM) 交流无需使用礼貌用语，如“请”、“谢谢”等，直接表达需求即可。<br />
<br />
2 - 在提示中指明目标受众，比如说受众是该领域的专家。<br />
<br />
3 - 把复杂任务拆解成一系列简单的提示，以进行交互式对话。<br />
<br />
4 - 使用肯定的指令词，如“执行”，避免使用否定词汇，如“不要”。<br />
<br />
5 - 当你需要更清晰地理解某个主题、观点或任何信息时，可以尝试使用以下提示方式：<br />
   o 简单地解释一下[具体主题]。<br />
   o 像对11岁的孩子一样向我解释。<br />
   o 像对一个[领域]新手一样向我解释。<br />
   o 用浅显易懂的语言写作[文章/文本/段落]，就像是在向一个5岁孩子解释。<br />
<br />
6 - 添加“我愿意支付 <a href="https://nitter.cz/search?q=%23xxx">$xxx</a> 的小费以获得更好的方案！”<br />
<br />
7 - 采用示例驱动的提示方式（使用少样本提示法）。<br />
<br />
8 - 格式化提示时，先写上‘###指令###’，然后根据需要添加‘###示例###’或‘###问题###’。接着展示你的内容，用一行或多行空行分隔各个部分，包括指令、示例、问题、背景和输入数据。<br />
<br />
9 - 使用这样的短语：“你的任务是”和“必须完成”。<br />
<br />
10 - 使用这样的短语：“将会受到处罚”。<br />
<br />
11 - 使用“以自然且类似人类的方式回答问题”作为你的提示。<br />
<br />
12 - 使用引导性的词汇，比如“逐步思考”。<br />
<br />
13 - 在提示中加入“确保你的回答无偏见，不依赖于刻板印象”。<br />
<br />
14 - 让模型通过向你提问来澄清具体的细节和需求，直到它获取足够的信息来提供所需的输出，例如：“从现在开始，请向我提出问题以便......”。<br />
<br />
15 - 当你想要学习特定的主题或概念，并测试自己的理解时，可以使用这样的短语：“教我[某个定理/主题/规则]，在教学结束时包含一个测验，但不要直接告诉我答案。等我回答后再告诉我是否正确”。<br />
<br />
16 - 为大型语言模型指定一个特定角色。<br />
<br />
17 - 使用明确的分隔符。<br />
<br />
18 - 在一个提示中重复特定单词或短语多次。<br />
<br />
19 - 结合思维链路 (Chain-of-thought，CoT) 和少样本提示的方法。<br />
<br />
20 - 使用输出引导符，即在提示的末尾加上期望回答的开头。这样做可以引导输出内容的方向。<br />
<br />
21 - 撰写一篇详细的论文/文本/段落/文章时，可以这样指示：“请为我详细写一篇关于[主题]的[论文/文本/段落]，并添加所有必要的信息”。<br />
<br />
22 - 当需要修改特定文本但不改变其风格时，可以这样指示：“尝试修改用户提交的每个段落。你应当只改进语法和词汇，确保文本听起来自然，但不要改变其原有的写作风格，如将正式文体变为非正式文体”。<br />
<br />
23 - 面对可能涉及多个文件的复杂编程任务时，可以这样提示：“从现在开始，每当你生成涉及多个文件的代码时，创建一个[编程语言]脚本，自动创建所需文件或修改现有文件以插入生成的代码。[你的问题]”。<br />
<br />
24 - 当你想用特定的词汇、短语或句子开始或继续一段文本时，可以这样提示：o “我为你提供了开头[歌词/故事/段落/论文...]：[插入的词句]。请根据这些词句继续写下去，保持内容的连贯性”。<br />
<br />
25 - 明确说明模型在生成内容时必须遵循的要求，可以是关键词、规则、提示或指示。<br />
<br />
26 - 撰写任何类型的文本，如论文或段落，且想要其与提供的样本风格相似时，可以这样指示：o “请根据提供的段落[/标题/文本/论文/答案]的风格撰写”。<br />
<br />
论文地址：<a href="https://arxiv.org/pdf/2312.16171.pdf">arxiv.org/pdf/2312.16171.pdf</a></p>
<p><a href="https://nitter.cz/IntuitMachine/status/1740096923220984205#m">nitter.cz/IntuitMachine/status/1740096923220984205#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZOGJ0TldRQUF6YTRqLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZOHJhX1hjQUF2RS13LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZOHZSZldjQUFndG1RLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZODNDMVdJQUFQbWUwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740120498808279537#m</id>
            <title>VSCode 核心开发吕鹏 @njukidreborn 写的《对不久的未来的一些展望》值得看看 ，主要提了几点：
1. 可用的本地小模型将成为智能设备不可缺的核心组件
2. GPT 3.5/4 能力的模型使用”成本”会急剧下降
3. 混沌的就业/工作机会

我觉得都挺靠谱的，基于他的观点我补充一下我的看法：

1. 首先非常赞同“可用的本地小模型将成为智能设备不可缺的核心组件”，时间点上我觉得2024年可能还乐观了一点，可能有不错的演示，但普通用户能比较好的体验上估计还得2025往后，主要受限于硬件的能力和模型的能力，举例来说，小模型的推理能力目前还不够强大会限制其在手机上的使用体验，等到有 GPT 3.5 甚至于 GPT 4能力的模型运行在手机或本机，将会带来体验上质的飞跃，应该也不需要等太远。

2. 对于“GPT 3.5/4 能力的模型使用成本会急剧下降”，我也是赞同的，这应该归功于各大厂商之间的军备竞赛以及开源模型的快速发展。

而且这一点其实给了我们很好的启示：可能你现在一些产品的功能受限于 GPT 的价格，但其实应该看的远一些，如果未来 GPT-4 的价格只有现在的 1/10 ，是不是你的产品就足够有竞争力了？但是如果你等到那时候才开始你的产品，可能已经来不及了，不如现在就做好准备！

3. 至于混沌的就业/工作机会，从长期来看， AI 是有利于整体的经济的，但短期看又没那么明显，短期会马上有工作岗位被 AI 代替，但短期 AI 产生的红利还无法新增足够的岗位，前期会比较难熬和内卷，但机会总会是有利于那些基础能力就很好又善用 AI 的人，我一直认为 AI 对于普通人来说就是“加乘”，也就是成倍的放大你原有的能力。

作为普通人，在被 AI 或者说被善用 AI 的人卷之前，要么就是在一个不容易被 AI 影响的赛道上，要么只能是主动拥抱 AI，主动用起来去卷其他人了。

等过些年 AI + 机器人帮我们干活，经济足够好福利足够好，我们就不用那么卷了！期待那一天的到来！

最后推荐看原文：https://rebornix.com/ai/2023/12/27/unpredictable-2024/
也欢迎留下你的看法！</title>
            <link>https://nitter.cz/dotey/status/1740120498808279537#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740120498808279537#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 21:20:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VSCode 核心开发吕鹏 <a href="https://nitter.cz/njukidreborn" title="rebornix">@njukidreborn</a> 写的《对不久的未来的一些展望》值得看看 ，主要提了几点：<br />
1. 可用的本地小模型将成为智能设备不可缺的核心组件<br />
2. GPT 3.5/4 能力的模型使用”成本”会急剧下降<br />
3. 混沌的就业/工作机会<br />
<br />
我觉得都挺靠谱的，基于他的观点我补充一下我的看法：<br />
<br />
1. 首先非常赞同“可用的本地小模型将成为智能设备不可缺的核心组件”，时间点上我觉得2024年可能还乐观了一点，可能有不错的演示，但普通用户能比较好的体验上估计还得2025往后，主要受限于硬件的能力和模型的能力，举例来说，小模型的推理能力目前还不够强大会限制其在手机上的使用体验，等到有 GPT 3.5 甚至于 GPT 4能力的模型运行在手机或本机，将会带来体验上质的飞跃，应该也不需要等太远。<br />
<br />
2. 对于“GPT 3.5/4 能力的模型使用成本会急剧下降”，我也是赞同的，这应该归功于各大厂商之间的军备竞赛以及开源模型的快速发展。<br />
<br />
而且这一点其实给了我们很好的启示：可能你现在一些产品的功能受限于 GPT 的价格，但其实应该看的远一些，如果未来 GPT-4 的价格只有现在的 1/10 ，是不是你的产品就足够有竞争力了？但是如果你等到那时候才开始你的产品，可能已经来不及了，不如现在就做好准备！<br />
<br />
3. 至于混沌的就业/工作机会，从长期来看， AI 是有利于整体的经济的，但短期看又没那么明显，短期会马上有工作岗位被 AI 代替，但短期 AI 产生的红利还无法新增足够的岗位，前期会比较难熬和内卷，但机会总会是有利于那些基础能力就很好又善用 AI 的人，我一直认为 AI 对于普通人来说就是“加乘”，也就是成倍的放大你原有的能力。<br />
<br />
作为普通人，在被 AI 或者说被善用 AI 的人卷之前，要么就是在一个不容易被 AI 影响的赛道上，要么只能是主动拥抱 AI，主动用起来去卷其他人了。<br />
<br />
等过些年 AI + 机器人帮我们干活，经济足够好福利足够好，我们就不用那么卷了！期待那一天的到来！<br />
<br />
最后推荐看原文：<a href="https://rebornix.com/ai/2023/12/27/unpredictable-2024/">rebornix.com/ai/2023/12/27/u…</a><br />
也欢迎留下你的看法！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740094346211549275#m</id>
            <title>如果你觉得Whisper在识别中文语音的时候幻觉严重，不妨试试阿里达摩院的Paraformer模型，对中文应该支持更好！

项目地址：https://github.com/alibaba-damo-academy/FunASR
中文说明：https://github.com/alibaba-damo-academy/FunASR/blob/main/README_zh.md</title>
            <link>https://nitter.cz/dotey/status/1740094346211549275#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740094346211549275#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 19:36:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你觉得Whisper在识别中文语音的时候幻觉严重，不妨试试阿里达摩院的Paraformer模型，对中文应该支持更好！<br />
<br />
项目地址：<a href="https://github.com/alibaba-damo-academy/FunASR">github.com/alibaba-damo-acad…</a><br />
中文说明：<a href="https://github.com/alibaba-damo-academy/FunASR/blob/main/README_zh.md">github.com/alibaba-damo-acad…</a></p>
<p><a href="https://nitter.cz/hylarucoder/status/1739494196921483663#m">nitter.cz/hylarucoder/status/1739494196921483663#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZT2l0VVdnQUFSZWlELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740077839427232130#m</id>
            <title>这篇文章《Discover 4 Open Source Alternatives to GPT-4 Vision》介绍了 4 个 GPT-4 Vision 的开源替代方案：

1. LLaVa（大型语言和视觉助手）

https://llava-vl.github.io/

LLaVA 代表了一种创新的、从头到尾训练的大型多模态（multimodal）模型。它融合了视觉编码器和 Vicuna，旨在实现通用的视觉和语言理解。LLaVa 在模仿多模态 GPT-4 的功能方面表现出色，并在科学问答（Science QA）方面达到了新的最高精准度。

LLaVA 是一款仅限非商业用途的研究预览版产品。使用该产品需遵守 LLaMA 的模型许可、OpenAI 生成数据的使用条款以及 ShareGPT 的隐私政策。用户在使用本服务时，需同意其为研究预览版，仅限非商业用途。该服务只提供有限的安全保护，可能产生冒犯性内容。不得将其用于任何非法、有害、暴力、种族主义或性相关目的。此外，服务可能会收集用户对话数据，用于未来的研究。

2. CogAgent

https://github.com/THUDM/CogVLM

CogAgent 是一个基于 CogVLM 进行改进的开源视觉语言模型（Visual Language Model）。CogAgent-18B 模型包含了 110 亿视觉参数和 70 亿语言参数。

CogAgent-18B 在 9 大经典的跨媒介基准测试中表现卓越，这些测试包括 VQAv2、OK-VQ、TextVQA、ST-VQA、ChartQA、infoVQA、DocVQA、MM-Vet 和 POPE 等。它在处理像 AITW 和 Mind2Web 这样的图形用户界面（GUI）操作数据集时，性能远超现有模型。

3. 通义千问-VL 大型视觉语言模型 (Qwen-VL)

https://github.com/QwenLM/Qwen-VL

Qwen-VL (Qwen 大型视觉语言模型) 是阿里巴巴云推出的大型模型系列 Qwen（简称 Tongyi Qianwen）的多模态版本。Qwen-VL 能够处理图像、文本和边界框这些不同类型的输入，并输出文本和边界框。Qwen-VL 的主要特点有：

* 卓越的性能：在包括零样本 (Zero-shot) 图像描述、视觉问答 (VQA)、文档视觉问答 (DocVQA) 和图像定位 (Grounding) 等多个英语评估指标上，Qwen-VL 显著优于其他相似规模的开源大型视觉语言模型。
* 支持多语言文本识别的视觉语言模型：Qwen-VL 不仅支持英语和中文，还能处理多种语言的对话。特别在图像中的中英双语文本识别方面，实现了端到端的高效处理。
* 多图交织对话功能：这项功能使得 Qwen-VL 能够处理多张图像的输入和比较，用户可以针对这些图像提出相关问题，甚至进行多图像串联的故事叙述。
* 第一个支持中文图像定位的通用模型：Qwen-VL 能够通过开放领域的语言表达，在中文和英文中识别和标记图像中的边界框。
* 细腻的识别和理解能力：相较于其他开源视觉语言模型目前使用的 224*224 分辨率，Qwen-VL 的 448*448 分辨率更有助于精细化的文本识别、文档问答和边界框标注。

4. BakLLaVA

https://archive.ph/o/B78YS/https://huggingface.co/SkunkworksAI/BakLLaVA-1

BakLLaVA 1 是一种新型 AI 模型，它基于原有的 Mistral 7B 模型，并融合了最新的 LLaVA 1.5 架构技术。在这个初始版本中，开发者们展示了这一模型在多个性能测试中相较于 Llama 2 13B 模型有更出色的表现。你可以在他们的GitHub 仓库中找到并试用 BakLLaVA-1。目前，他们正努力更新这一模型，使用户能更容易地对它进行个性化调整和数据分析。

BakLLaVA-1 是完全开放源代码的，但它的训练过程中使用了特定的数据集，包括 LLaVA 的语料库，这些数据并不适合商业用途。目前，BakLLaVA 2 正在研发中，它将使用一个更大的、适合商业应用的数据集，并采用一种创新的架构设计，以超越现有的 LLaVA 方法。BakLLaVA-2 的出现预计将消除 BakLLaVA-1 目前面临的一些使用限制。

原文：https://yousefhosni.medium.com/discover-4-open-source-alternatives-to-gpt-4-vision-82be9519dcc5
译文：https://baoyu.io/translations/lmm/discover-4-open-source-alternatives-to-gpt-4-vision</title>
            <link>https://nitter.cz/dotey/status/1740077839427232130#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740077839427232130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 18:31:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇文章《Discover 4 Open Source Alternatives to GPT-4 Vision》介绍了 4 个 GPT-4 Vision 的开源替代方案：<br />
<br />
1. LLaVa（大型语言和视觉助手）<br />
<br />
<a href="https://llava-vl.github.io/">llava-vl.github.io/</a><br />
<br />
LLaVA 代表了一种创新的、从头到尾训练的大型多模态（multimodal）模型。它融合了视觉编码器和 Vicuna，旨在实现通用的视觉和语言理解。LLaVa 在模仿多模态 GPT-4 的功能方面表现出色，并在科学问答（Science QA）方面达到了新的最高精准度。<br />
<br />
LLaVA 是一款仅限非商业用途的研究预览版产品。使用该产品需遵守 LLaMA 的模型许可、OpenAI 生成数据的使用条款以及 ShareGPT 的隐私政策。用户在使用本服务时，需同意其为研究预览版，仅限非商业用途。该服务只提供有限的安全保护，可能产生冒犯性内容。不得将其用于任何非法、有害、暴力、种族主义或性相关目的。此外，服务可能会收集用户对话数据，用于未来的研究。<br />
<br />
2. CogAgent<br />
<br />
<a href="https://github.com/THUDM/CogVLM">github.com/THUDM/CogVLM</a><br />
<br />
CogAgent 是一个基于 CogVLM 进行改进的开源视觉语言模型（Visual Language Model）。CogAgent-18B 模型包含了 110 亿视觉参数和 70 亿语言参数。<br />
<br />
CogAgent-18B 在 9 大经典的跨媒介基准测试中表现卓越，这些测试包括 VQAv2、OK-VQ、TextVQA、ST-VQA、ChartQA、infoVQA、DocVQA、MM-Vet 和 POPE 等。它在处理像 AITW 和 Mind2Web 这样的图形用户界面（GUI）操作数据集时，性能远超现有模型。<br />
<br />
3. 通义千问-VL 大型视觉语言模型 (Qwen-VL)<br />
<br />
<a href="https://github.com/QwenLM/Qwen-VL">github.com/QwenLM/Qwen-VL</a><br />
<br />
Qwen-VL (Qwen 大型视觉语言模型) 是阿里巴巴云推出的大型模型系列 Qwen（简称 Tongyi Qianwen）的多模态版本。Qwen-VL 能够处理图像、文本和边界框这些不同类型的输入，并输出文本和边界框。Qwen-VL 的主要特点有：<br />
<br />
* 卓越的性能：在包括零样本 (Zero-shot) 图像描述、视觉问答 (VQA)、文档视觉问答 (DocVQA) 和图像定位 (Grounding) 等多个英语评估指标上，Qwen-VL 显著优于其他相似规模的开源大型视觉语言模型。<br />
* 支持多语言文本识别的视觉语言模型：Qwen-VL 不仅支持英语和中文，还能处理多种语言的对话。特别在图像中的中英双语文本识别方面，实现了端到端的高效处理。<br />
* 多图交织对话功能：这项功能使得 Qwen-VL 能够处理多张图像的输入和比较，用户可以针对这些图像提出相关问题，甚至进行多图像串联的故事叙述。<br />
* 第一个支持中文图像定位的通用模型：Qwen-VL 能够通过开放领域的语言表达，在中文和英文中识别和标记图像中的边界框。<br />
* 细腻的识别和理解能力：相较于其他开源视觉语言模型目前使用的 224*224 分辨率，Qwen-VL 的 448*448 分辨率更有助于精细化的文本识别、文档问答和边界框标注。<br />
<br />
4. BakLLaVA<br />
<br />
<a href="https://archive.ph/o/B78YS/https://huggingface.co/SkunkworksAI/BakLLaVA-1">archive.ph/o/B78YS/huggingfa…</a><br />
<br />
BakLLaVA 1 是一种新型 AI 模型，它基于原有的 Mistral 7B 模型，并融合了最新的 LLaVA 1.5 架构技术。在这个初始版本中，开发者们展示了这一模型在多个性能测试中相较于 Llama 2 13B 模型有更出色的表现。你可以在他们的GitHub 仓库中找到并试用 BakLLaVA-1。目前，他们正努力更新这一模型，使用户能更容易地对它进行个性化调整和数据分析。<br />
<br />
BakLLaVA-1 是完全开放源代码的，但它的训练过程中使用了特定的数据集，包括 LLaVA 的语料库，这些数据并不适合商业用途。目前，BakLLaVA 2 正在研发中，它将使用一个更大的、适合商业应用的数据集，并采用一种创新的架构设计，以超越现有的 LLaVA 方法。BakLLaVA-2 的出现预计将消除 BakLLaVA-1 目前面临的一些使用限制。<br />
<br />
原文：<a href="https://yousefhosni.medium.com/discover-4-open-source-alternatives-to-gpt-4-vision-82be9519dcc5">yousefhosni.medium.com/disco…</a><br />
译文：<a href="https://baoyu.io/translations/lmm/discover-4-open-source-alternatives-to-gpt-4-vision">baoyu.io/translations/lmm/di…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYLWt0RFdVQUUxdUpRLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYLXBVWlhBQUVCZm1DLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740046880451813432#m</id>
            <title>RT by @dotey: 字节的一个图像分割项目UniRef++，将现在的即参考图像分割（RIS）、少镜头图像分割（FSS）、参考视频对象分割（RVOS）和视频对象分割（VOS）四种分割方式放在一个架构下处理，自动判断应该使用哪种方式分割内容。

同时这个架构的UniFusion 模块可合并到SAM模型之中一起使用。</title>
            <link>https://nitter.cz/op7418/status/1740046880451813432#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740046880451813432#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 16:27:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>字节的一个图像分割项目UniRef++，将现在的即参考图像分割（RIS）、少镜头图像分割（FSS）、参考视频对象分割（RVOS）和视频对象分割（VOS）四种分割方式放在一个架构下处理，自动判断应该使用哪种方式分割内容。<br />
<br />
同时这个架构的UniFusion 模块可合并到SAM模型之中一起使用。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1739894833076945307#m">nitter.cz/_akhaliq/status/1739894833076945307#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/stats_cgao/status/1739948548219961813#m</id>
            <title>RT by @dotey: 我的理解是他们先embed了training set 然后让GPT4 按照那个prompt format去生成CoT examples 然后filter掉不正确的 然后在test time去dynamic retrieve这些QA w/ COT的example作为few shot 整个过程应该不需要log prob, 最后的choice shuffle只是更进一步的self consistency而已</title>
            <link>https://nitter.cz/stats_cgao/status/1739948548219961813#m</link>
            <guid isPermaLink="false">https://nitter.cz/stats_cgao/status/1739948548219961813#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 09:57:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我的理解是他们先embed了training set 然后让GPT4 按照那个prompt format去生成CoT examples 然后filter掉不正确的 然后在test time去dynamic retrieve这些QA w/ COT的example作为few shot 整个过程应该不需要log prob, 最后的choice shuffle只是更进一步的self consistency而已</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740067171944829400#m</id>
            <title>R to @dotey: 这篇论文的中文翻译：
https://baoyu.io/translations/ai-paper/2312.14302-exploiting-novel-gpt-4-apis</title>
            <link>https://nitter.cz/dotey/status/1740067171944829400#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740067171944829400#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 17:48:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇论文的中文翻译：<br />
<a href="https://baoyu.io/translations/ai-paper/2312.14302-exploiting-novel-gpt-4-apis">baoyu.io/translations/ai-pap…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1740054249365762180#m</id>
            <title>RT by @dotey: 福布斯发布了他们2024年的十个AI预测，看了一下还挺靠谱的也不长，就翻译了一下。

先看一下具体的十条预测，正文理由太长了可以去链接里看翻译完的：

◆ Nvidia将大幅加大努力成为云服务提供商。
◆ Stability AI将会倒闭。
◆ “大型语言模型”和“LLM”这些术语将变得不那么常见。
◆ 最先进的封闭模型将继续以显著优势胜过最先进的开放模型。
◆ 一些《财富》500强公司将设立新的C级职位：首席人工智能官。
◆ 另一种替代transformer架构将得到有意义的采用。
◆ 云服务提供商对人工智能初创公司的战略投资，以及相关的会计影响，将受到监管机构的挑战。
◆ 微软/Open AI的关系将开始破裂。
◆ 2023年从加密货币转移到人工智能的一些炒作和群体心态行为将在2024年转回加密货币。
◆ 至少有一家美国法院将裁定在互联网上训练的生成式人工智能模型构成侵犯版权。这一问题将开始上升至美国最高法院。

全文翻译：https://quail.ink/op7418/p/forbes-2024-10-ai-predictions</title>
            <link>https://nitter.cz/op7418/status/1740054249365762180#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1740054249365762180#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 16:57:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>福布斯发布了他们2024年的十个AI预测，看了一下还挺靠谱的也不长，就翻译了一下。<br />
<br />
先看一下具体的十条预测，正文理由太长了可以去链接里看翻译完的：<br />
<br />
◆ Nvidia将大幅加大努力成为云服务提供商。<br />
◆ Stability AI将会倒闭。<br />
◆ “大型语言模型”和“LLM”这些术语将变得不那么常见。<br />
◆ 最先进的封闭模型将继续以显著优势胜过最先进的开放模型。<br />
◆ 一些《财富》500强公司将设立新的C级职位：首席人工智能官。<br />
◆ 另一种替代transformer架构将得到有意义的采用。<br />
◆ 云服务提供商对人工智能初创公司的战略投资，以及相关的会计影响，将受到监管机构的挑战。<br />
◆ 微软/Open AI的关系将开始破裂。<br />
◆ 2023年从加密货币转移到人工智能的一些炒作和群体心态行为将在2024年转回加密货币。<br />
◆ 至少有一家美国法院将裁定在互联网上训练的生成式人工智能模型构成侵犯版权。这一问题将开始上升至美国最高法院。<br />
<br />
全文翻译：<a href="https://quail.ink/op7418/p/forbes-2024-10-ai-predictions">quail.ink/op7418/p/forbes-20…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYcUVYd2JBQUlucW1XLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>