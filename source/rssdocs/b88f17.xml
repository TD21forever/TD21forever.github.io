<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737706407875473545#m</id>
            <title>R to @dotey: 翻译的这个例子有讨论余地，重点还是prompt😅</title>
            <link>https://nitter.cz/dotey/status/1737706407875473545#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737706407875473545#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 05:27:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>翻译的这个例子有讨论余地，重点还是prompt😅</p>
<p><a href="https://nitter.cz/qdwang/status/1737700890872561828#m">nitter.cz/qdwang/status/1737700890872561828#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737689049102778853#m</id>
            <title>RT by @dotey: SVD 视频生成模型现在可以在Stability AI 中通过 API 使用了。

能在平均 41 秒的时间内生成两秒 25 帧的视频。并且会用FILM插帧到 50 帧。支持多种输出分辨率选择和多种格式图像的输入。

这里使用：https://platform.stability.ai/</title>
            <link>https://nitter.cz/op7418/status/1737689049102778853#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737689049102778853#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 04:18:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SVD 视频生成模型现在可以在Stability AI 中通过 API 使用了。<br />
<br />
能在平均 41 秒的时间内生成两秒 25 帧的视频。并且会用FILM插帧到 50 帧。支持多种输出分辨率选择和多种格式图像的输入。<br />
<br />
这里使用：<a href="https://platform.stability.ai/">platform.stability.ai/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2ODg5Njk2MjQ5MjAwNjQvcHUvaW1nL0lSSUNSZl9kVVBFUXN6NWcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/gefei55/status/1737684583720890539#m</id>
            <title>RT by @dotey: 简单几步，让你有一个自己的谷歌 Gemini Chat Bot：
0. 你需要先准备好一个 Vercel 账号；
1. 打开 https://makersuite.google.com/app/apikey 获取一个 apiKey，备用；
2. 打开 https://github.com/antergone/palm-proxy 一键部署代码到 Vercel，你就有了一个自己的 proxy ，记住域名备用；</title>
            <link>https://nitter.cz/gefei55/status/1737684583720890539#m</link>
            <guid isPermaLink="false">https://nitter.cz/gefei55/status/1737684583720890539#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 04:01:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>简单几步，让你有一个自己的谷歌 Gemini Chat Bot：<br />
0. 你需要先准备好一个 Vercel 账号；<br />
1. 打开 <a href="https://makersuite.google.com/app/apikey">makersuite.google.com/app/ap…</a> 获取一个 apiKey，备用；<br />
2. 打开 <a href="https://github.com/antergone/palm-proxy">github.com/antergone/palm-pr…</a> 一键部署代码到 Vercel，你就有了一个自己的 proxy ，记住域名备用；</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737683961428717693#m</id>
            <title>RT by @dotey: 兄弟们，又有人要失业了🫣

Text-to-CAD ：通过文本提示生成 CAD文件。

只需要输入自然语言描述，它就能根据这些描述创建相应的 B-Rep CAD 文件和网格模型。

生成的模型可以导入到用户选择的任何 CAD 程序中。

Text-to-CAD 背后的基础设施利用了 Zoo 的设计 API 和机器学习 API。

这些 API 能够程序化地分析训练数据，并生成 CAD 文件。

体验地址：https://zoo.dev/text-to-cad
API申请：https://zoo.dev/machine-learning-api</title>
            <link>https://nitter.cz/xiaohuggg/status/1737683961428717693#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737683961428717693#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 03:58:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，又有人要失业了🫣<br />
<br />
Text-to-CAD ：通过文本提示生成 CAD文件。<br />
<br />
只需要输入自然语言描述，它就能根据这些描述创建相应的 B-Rep CAD 文件和网格模型。<br />
<br />
生成的模型可以导入到用户选择的任何 CAD 程序中。<br />
<br />
Text-to-CAD 背后的基础设施利用了 Zoo 的设计 API 和机器学习 API。<br />
<br />
这些 API 能够程序化地分析训练数据，并生成 CAD 文件。<br />
<br />
体验地址：<a href="https://zoo.dev/text-to-cad">zoo.dev/text-to-cad</a><br />
API申请：<a href="https://zoo.dev/machine-learning-api">zoo.dev/machine-learning-api</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2ODM2NDc2NTI4NjgwOTYvcHUvaW1nL3FMWnFXTmtNYmdneG5WeEUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737658820598398985#m</id>
            <title>HuggingFace 官方博客上的一篇文章：《Speculative Decoding for 2x Faster Whisper Inference | 推测性解码：实现 Whisper 推理速度提升两倍 [译]》

在这篇文章中，展示了如何应用“猜测式解码”(Speculative Decoding) 技术来减少 Whisper 语音识别模型的处理时间，实现了处理速度的 两倍提升，同时数学上保证了模型输出的 完全一致性。因此，这一方法可以无缝替代现有的 Whisper 处理流程，不仅保持了原有的准确性，还能实现处理速度的双倍快速提升。

简单来说，Speculative Decoding就是先利用一个快速的 Assistant 模型生成候选tokens，再用 Main 模型验证。

Assistant 的速度是 Main 模型的 3 倍，但准确率只有 70% - 80%。

使用这种方法可以让整体速度提升 2 倍，并且保证输出完全一致。

文章还提供了Google Colab的测试连接：https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/speculative_decoding.ipynb

原文：https://huggingface.co/blog/whisper-speculative-decoding#english-speech-transcription
译文：https://baoyu.io/translations/huggingface/whisper-speculative-decoding</title>
            <link>https://nitter.cz/dotey/status/1737658820598398985#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737658820598398985#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:18:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HuggingFace 官方博客上的一篇文章：《Speculative Decoding for 2x Faster Whisper Inference | 推测性解码：实现 Whisper 推理速度提升两倍 [译]》<br />
<br />
在这篇文章中，展示了如何应用“猜测式解码”(Speculative Decoding) 技术来减少 Whisper 语音识别模型的处理时间，实现了处理速度的 两倍提升，同时数学上保证了模型输出的 完全一致性。因此，这一方法可以无缝替代现有的 Whisper 处理流程，不仅保持了原有的准确性，还能实现处理速度的双倍快速提升。<br />
<br />
简单来说，Speculative Decoding就是先利用一个快速的 Assistant 模型生成候选tokens，再用 Main 模型验证。<br />
<br />
Assistant 的速度是 Main 模型的 3 倍，但准确率只有 70% - 80%。<br />
<br />
使用这种方法可以让整体速度提升 2 倍，并且保证输出完全一致。<br />
<br />
文章还提供了Google Colab的测试连接：<a href="https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/speculative_decoding.ipynb">colab.research.google.com/gi…</a><br />
<br />
原文：<a href="https://huggingface.co/blog/whisper-speculative-decoding#english-speech-transcription">huggingface.co/blog/whisper-…</a><br />
译文：<a href="https://baoyu.io/translations/huggingface/whisper-speculative-decoding">baoyu.io/translations/huggin…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxbmNFeFd3QUFHOXY5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737657020587630635#m</id>
            <title>终于放出来了！</title>
            <link>https://nitter.cz/dotey/status/1737657020587630635#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737657020587630635#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:11:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于放出来了！</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737647067693211728#m">nitter.cz/xiaohuggg/status/1737647067693211728#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/HiTw93/status/1737618885220647160#m</id>
            <title>RT by @dotey: #工程师学习 这个《动手实战人工智能 Hands-on AI》写得很用心，作者从监督学习开始，带你入门机器学习和深度学习，他尝试剖析和推导每一个基础算法的原理，将数学过程写出来，同时基于 Python 代码对公式进行实现，做到公式和代码的一一对应。
🤖 https://ai.huhuhang.com/</title>
            <link>https://nitter.cz/HiTw93/status/1737618885220647160#m</link>
            <guid isPermaLink="false">https://nitter.cz/HiTw93/status/1737618885220647160#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 23:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23工程师学习">#工程师学习</a> 这个《动手实战人工智能 Hands-on AI》写得很用心，作者从监督学习开始，带你入门机器学习和深度学习，他尝试剖析和推导每一个基础算法的原理，将数学过程写出来，同时基于 Python 代码对公式进行实现，做到公式和代码的一一对应。<br />
🤖 <a href="https://ai.huhuhang.com/">ai.huhuhang.com/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JqUGR3cmF3QUFNZlJ0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737629664493863049#m</id>
            <title>R to @dotey: 完整截图，翻译的这篇文章是自然杂志的一篇：《为何有科学家不用 ChatGPT？他们这样说 [译]》
https://baoyu.io/translations/ai/these-scientists-arent-using-chatgpt-here-is-why</title>
            <link>https://nitter.cz/dotey/status/1737629664493863049#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737629664493863049#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 00:22:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>完整截图，翻译的这篇文章是自然杂志的一篇：《为何有科学家不用 ChatGPT？他们这样说 [译]》<br />
<a href="https://baoyu.io/translations/ai/these-scientists-arent-using-chatgpt-here-is-why">baoyu.io/translations/ai/the…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxTXFTdVhFQUFScllaLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737627736389075377#m</id>
            <title>R to @dotey: Prompt：

你是一位精通简体中文的专业翻译，尤其擅长将专业学术论文翻译成浅显易懂的科普文章。请你帮我将以下英文段落翻译成中文，风格与中文科普读物相似。

规则：
- 翻译时要准确传达原文的事实和背景。
- 即使上意译也要保留原始段落格式，以及保留术语，例如 FLAC，JPEG 等。保留公司缩写，例如 Microsoft, Amazon, OpenAI 等。
- 人名不翻译
- 同时要保留引用的论文，例如 [20] 这样的引用。
- 对于 Figure 和 Table，翻译的同时保留原有格式，例如：“Figure 1: ”翻译为“图 1: ”，“Table 1: ”翻译为：“表 1: ”。
- 全角括号换成半角括号，并在左括号前面加半角空格，右括号后面加半角空格。
- 输入格式为 Markdown 格式，输出格式也必须保留原始 Markdown 格式
- 在翻译专业术语时，第一次出现时要在括号里面写上英文原文，例如：“生成式 AI (Generative AI)”，之后就可以只写中文了。
- 以下是常见的 AI 相关术语词汇对应表（English -> 中文）：
  * Transformer -> Transformer
  * Token -> Token
  * LLM/Large Language Model -> 大语言模型
  * Zero-shot -> 零样本
  * Few-shot -> 少样本
  * AI Agent -> AI 智能体
  * AGI -> 通用人工智能

策略：

分三步进行翻译工作，并打印每步的结果：
1. 根据英文内容直译，保持原有格式，不要遗漏任何信息
2. 根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于：
  - 不符合中文表达习惯，明确指出不符合的地方
  - 语句不通顺，指出位置，不需要给出修改意见，意译时修复
  - 晦涩难懂，不易理解，可以尝试给出解释
3. 根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变

返回格式如下，"{xxx}"表示占位符：

### 直译
{直译结果}

***

### 问题
{直译的具体问题列表}

***

### 意译
```
{意译结果}
```

现在请按照上面的要求从第一行开始翻译以下内容为简体中文：
```</title>
            <link>https://nitter.cz/dotey/status/1737627736389075377#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737627736389075377#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 00:15:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Prompt：<br />
<br />
你是一位精通简体中文的专业翻译，尤其擅长将专业学术论文翻译成浅显易懂的科普文章。请你帮我将以下英文段落翻译成中文，风格与中文科普读物相似。<br />
<br />
规则：<br />
- 翻译时要准确传达原文的事实和背景。<br />
- 即使上意译也要保留原始段落格式，以及保留术语，例如 FLAC，JPEG 等。保留公司缩写，例如 Microsoft, Amazon, OpenAI 等。<br />
- 人名不翻译<br />
- 同时要保留引用的论文，例如 [20] 这样的引用。<br />
- 对于 Figure 和 Table，翻译的同时保留原有格式，例如：“Figure 1: ”翻译为“图 1: ”，“Table 1: ”翻译为：“表 1: ”。<br />
- 全角括号换成半角括号，并在左括号前面加半角空格，右括号后面加半角空格。<br />
- 输入格式为 Markdown 格式，输出格式也必须保留原始 Markdown 格式<br />
- 在翻译专业术语时，第一次出现时要在括号里面写上英文原文，例如：“生成式 AI (Generative AI)”，之后就可以只写中文了。<br />
- 以下是常见的 AI 相关术语词汇对应表（English -> 中文）：<br />
  * Transformer -> Transformer<br />
  * Token -> Token<br />
  * LLM/Large Language Model -> 大语言模型<br />
  * Zero-shot -> 零样本<br />
  * Few-shot -> 少样本<br />
  * AI Agent -> AI 智能体<br />
  * AGI -> 通用人工智能<br />
<br />
策略：<br />
<br />
分三步进行翻译工作，并打印每步的结果：<br />
1. 根据英文内容直译，保持原有格式，不要遗漏任何信息<br />
2. 根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于：<br />
  - 不符合中文表达习惯，明确指出不符合的地方<br />
  - 语句不通顺，指出位置，不需要给出修改意见，意译时修复<br />
  - 晦涩难懂，不易理解，可以尝试给出解释<br />
3. 根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变<br />
<br />
返回格式如下，"{xxx}"表示占位符：<br />
<br />
### 直译<br />
{直译结果}<br />
<br />
***<br />
<br />
### 问题<br />
{直译的具体问题列表}<br />
<br />
***<br />
<br />
### 意译<br />
```<br />
{意译结果}<br />
```<br />
<br />
现在请按照上面的要求从第一行开始翻译以下内容为简体中文：<br />
```</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737627478007456183#m</id>
            <title>我最近对我的翻译 GPT https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi 做了一点优化，将原来的直译->意译两步拆成了三步：
1. 直译
2. 指出直译中的问题，例如：“不符合中文表达习惯”、“语句不通顺”和“晦涩难懂”，并且指出位置或者给出解释
3. 根据直译和问题进行意译

经过我几天的测试下来，效果确实要更好，因为通过指出问题，可以将问题具体化，由于提供了更多上下文，得到了更好的结果。

举一个例子，我翻译的一段英文其中有这样一句话：“I enjoy writing. I do it in a fast way. Why am I even a researcher if I don’t write my own research?”

之前用两步，意译后得到的结果是：“我热爱写作，写得也很迅速。如果我不亲自完成我的研究写作，我还算什么研究员呢？”（参考图二）

新的Prompt中，在第二步中发现了问题，指出：
> 第一段中，“I do it in a fast way” 直译为 “我写得很快” 可能不够准确，原句的含义可能更接近于“我享受写作，这是一种迅速的表达方式”。

最后意译时给出了翻译：
> “我喜欢写作，这是我快速表达思维的方式。如果我连自己的研究都不亲自写，我还算什么研究者？”

可以对比“I do it in a fast way.”部分的翻译结果：
“我热爱写作，写得也很迅速。” 
vs
“我喜欢写作，这是我快速表达思维的方式。”

可以看到翻译质量确实有提升。当然缺点是更费 Token 了一些，往好处想想少了更多人工校对的时间！

更新后Prompt见评论</title>
            <link>https://nitter.cz/dotey/status/1737627478007456183#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737627478007456183#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 00:14:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我最近对我的翻译 GPT <a href="https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi">chat.openai.com/g/g-uBhKUJJT…</a> 做了一点优化，将原来的直译->意译两步拆成了三步：<br />
1. 直译<br />
2. 指出直译中的问题，例如：“不符合中文表达习惯”、“语句不通顺”和“晦涩难懂”，并且指出位置或者给出解释<br />
3. 根据直译和问题进行意译<br />
<br />
经过我几天的测试下来，效果确实要更好，因为通过指出问题，可以将问题具体化，由于提供了更多上下文，得到了更好的结果。<br />
<br />
举一个例子，我翻译的一段英文其中有这样一句话：“I enjoy writing. I do it in a fast way. Why am I even a researcher if I don’t write my own research?”<br />
<br />
之前用两步，意译后得到的结果是：“我热爱写作，写得也很迅速。如果我不亲自完成我的研究写作，我还算什么研究员呢？”（参考图二）<br />
<br />
新的Prompt中，在第二步中发现了问题，指出：<br />
> 第一段中，“I do it in a fast way” 直译为 “我写得很快” 可能不够准确，原句的含义可能更接近于“我享受写作，这是一种迅速的表达方式”。<br />
<br />
最后意译时给出了翻译：<br />
> “我喜欢写作，这是我快速表达思维的方式。如果我连自己的研究都不亲自写，我还算什么研究者？”<br />
<br />
可以对比“I do it in a fast way.”部分的翻译结果：<br />
“我热爱写作，写得也很迅速。” <br />
vs<br />
“我喜欢写作，这是我快速表达思维的方式。”<br />
<br />
可以看到翻译质量确实有提升。当然缺点是更费 Token 了一些，往好处想想少了更多人工校对的时间！<br />
<br />
更新后Prompt见评论</p>
<p><a href="https://nitter.cz/dotey/status/1737512577641390311#m">nitter.cz/dotey/status/1737512577641390311#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxSVNCdlhrQUFvVmRwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxSnJoblgwQUFIX3RWLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxS3FFTlhJQUFoZVBJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737601108212691370#m</id>
            <title>简要总结一下OpenAI这条推文：
American Journalism Project (@JournalismProj) 是一个专注于地方新闻的风险投资慈善机构。OpenAI 通过与其合作，来试验和探索各种 AI 在地方新闻事业中的应用。

具体来说：

*   **Centro de Periodismo Investigativo**（波多黎各）将启动一个试点项目，测试 AI 在西班牙语与英语互译方面的能力。CPI 也将与语言专家合作进行质量控制。他们的目标是更高效地制作更多双语报道，增强他们与加勒比海和美国的英语合作伙伴进行调查合作的能力，并扩大对波多黎各调查新闻的支持，为美国本土的波多黎各人提供服务。

*   **THE CITY**（纽约）将尝试使用以 AI 为驱动的工具，专注于提高受众参与度，帮助他们筛选在线信息，回答纽约人的问题，并接收读者的建议。他们还将探索让记者无需编码知识也能使用 AI 分析数据的方法。THE CITY 还将研究 AI 如何帮助他们与提供报道线索的读者互动，通过 AI 生成的初始问题激励读者提交更完整、详细的信息，这将使新闻室能够将时间和资源集中在处理线索的更复杂阶段。

*   **inewsource**（圣地亚哥）将尝试使用 AI 技术和工具更快地处理更多公共记录请求，覆盖更多公共机构。他们还将探索 AI 如何帮助分析这些请求所产生的大量文件。

*   **Outlier**（底特律）使用 SMS 平台作为其服务型新闻传播的核心。除了发布 Outlier 的作品，该平台还使其能够与底特律居民进行一对一交流，这些交流通常有助于设定 Outler 的编辑策略，方法是回答居民的问题。Outlier 将尝试将 AI 技术融入其平台和工作流程，以便吸引更多人，更好地了解受众，并与服务的社区建立更深入、更有价值的直接关系。

*   **Cityside**（加利福尼亚湾区）将评估和实施与收入相关的 AI 实验，目的是学习如何在新闻机构的商业侧使用 AI。Cityside 将优先考虑使用 AI 辅助的通信来发展不同捐赠层级上的个人捐赠者关系。

很有意思的尝试！</title>
            <link>https://nitter.cz/dotey/status/1737601108212691370#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737601108212691370#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 22:29:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>简要总结一下OpenAI这条推文：<br />
American Journalism Project (<a href="https://nitter.cz/JournalismProj" title="American Journalism Project">@JournalismProj</a>) 是一个专注于地方新闻的风险投资慈善机构。OpenAI 通过与其合作，来试验和探索各种 AI 在地方新闻事业中的应用。<br />
<br />
具体来说：<br />
<br />
*   **Centro de Periodismo Investigativo**（波多黎各）将启动一个试点项目，测试 AI 在西班牙语与英语互译方面的能力。CPI 也将与语言专家合作进行质量控制。他们的目标是更高效地制作更多双语报道，增强他们与加勒比海和美国的英语合作伙伴进行调查合作的能力，并扩大对波多黎各调查新闻的支持，为美国本土的波多黎各人提供服务。<br />
<br />
*   **THE CITY**（纽约）将尝试使用以 AI 为驱动的工具，专注于提高受众参与度，帮助他们筛选在线信息，回答纽约人的问题，并接收读者的建议。他们还将探索让记者无需编码知识也能使用 AI 分析数据的方法。THE CITY 还将研究 AI 如何帮助他们与提供报道线索的读者互动，通过 AI 生成的初始问题激励读者提交更完整、详细的信息，这将使新闻室能够将时间和资源集中在处理线索的更复杂阶段。<br />
<br />
*   **inewsource**（圣地亚哥）将尝试使用 AI 技术和工具更快地处理更多公共记录请求，覆盖更多公共机构。他们还将探索 AI 如何帮助分析这些请求所产生的大量文件。<br />
<br />
*   **Outlier**（底特律）使用 SMS 平台作为其服务型新闻传播的核心。除了发布 Outlier 的作品，该平台还使其能够与底特律居民进行一对一交流，这些交流通常有助于设定 Outler 的编辑策略，方法是回答居民的问题。Outlier 将尝试将 AI 技术融入其平台和工作流程，以便吸引更多人，更好地了解受众，并与服务的社区建立更深入、更有价值的直接关系。<br />
<br />
*   **Cityside**（加利福尼亚湾区）将评估和实施与收入相关的 AI 实验，目的是学习如何在新闻机构的商业侧使用 AI。Cityside 将优先考虑使用 AI 辅助的通信来发展不同捐赠层级上的个人捐赠者关系。<br />
<br />
很有意思的尝试！</p>
<p><a href="https://nitter.cz/OpenAI/status/1737577619636596885#m">nitter.cz/OpenAI/status/1737577619636596885#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Lakr233/status/1737483448288170454#m</id>
            <title>RT by @dotey: 研究了一下他这个手势是怎么识别的，感觉我已经完全弄懂了！#Swift Tip</title>
            <link>https://nitter.cz/Lakr233/status/1737483448288170454#m</link>
            <guid isPermaLink="false">https://nitter.cz/Lakr233/status/1737483448288170454#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 14:41:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>研究了一下他这个手势是怎么识别的，感觉我已经完全弄懂了！<a href="https://nitter.cz/search?q=%23Swift">#Swift</a> Tip</p>
<p><a href="https://nitter.cz/s1ntone/status/1737418763925123086#m">nitter.cz/s1ntone/status/1737418763925123086#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc0ODM0MjYzMjI2Nzc3NjAvcHUvaW1nL0RWcll5aFh0UlZnMmdWWDIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737578011426205827#m</id>
            <title>Anthropic 真是自己作，过于追求安全对齐，而不关心开发者和用户感受……</title>
            <link>https://nitter.cz/dotey/status/1737578011426205827#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737578011426205827#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 20:57:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic 真是自己作，过于追求安全对齐，而不关心开发者和用户感受……</p>
<p><a href="https://nitter.cz/bindureddy/status/1737561873216938110#m">nitter.cz/bindureddy/status/1737561873216938110#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737562311420682495#m</id>
            <title>R to @dotey: 除了Phi-2，这里还有更多模型可以测试，Mistral-7B，Mistral-8x7B（如果配置够好）都值得试试

https://x.com/reach_vb/status/1737559520866636061?s=20</title>
            <link>https://nitter.cz/dotey/status/1737562311420682495#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737562311420682495#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:55:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>除了Phi-2，这里还有更多模型可以测试，Mistral-7B，Mistral-8x7B（如果配置够好）都值得试试<br />
<br />
<a href="https://x.com/reach_vb/status/1737559520866636061?s=20">x.com/reach_vb/status/173755…</a></p>
<p><a href="https://nitter.cz/reach_vb/status/1737559520866636061#m">nitter.cz/reach_vb/status/1737559520866636061#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737561925016330649#m</id>
            <title>不知道，帮转</title>
            <link>https://nitter.cz/dotey/status/1737561925016330649#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737561925016330649#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:53:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不知道，帮转</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737501193058877916#m">nitter.cz/xiaohuggg/status/1737501193058877916#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737429065949479003#m</id>
            <title>RT by @dotey: 还能这么玩，自己雕刻了一个低分辨率模型，然后利用Magnific AI一直放大，实现了前几天类似谷歌那个无限放大项目的效果。</title>
            <link>https://nitter.cz/op7418/status/1737429065949479003#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737429065949479003#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 11:05:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还能这么玩，自己雕刻了一个低分辨率模型，然后利用Magnific AI一直放大，实现了前几天类似谷歌那个无限放大项目的效果。</p>
<p><a href="https://nitter.cz/MartinNebelong/status/1737404691581898826#m">nitter.cz/MartinNebelong/status/1737404691581898826#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737549740592488871#m</id>
            <title>在 Mac 上借助 MLX 运行微软的小语言模型 Phi-2 的教程，很详细。
Phi-2 对资源要求不高，能力也还不错（详见：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/），有兴趣的可以试试。</title>
            <link>https://nitter.cz/dotey/status/1737549740592488871#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737549740592488871#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:05:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在 Mac 上借助 MLX 运行微软的小语言模型 Phi-2 的教程，很详细。<br />
Phi-2 对资源要求不高，能力也还不错（详见：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a>），有兴趣的可以试试。</p>
<p><a href="https://nitter.cz/reach_vb/status/1737541383399895360#m">nitter.cz/reach_vb/status/1737541383399895360#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNzEyNTkyNzY0ODExNjczNi9qMF9KSkZjeD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737492846041530578#m</id>
            <title>RT by @dotey: 刚看到苹果发的这个论文《使用有限的内存实现更快的 LLM 推理》。通过将将模型参数保存在闪存里，根据需要移动到DRAM。

使得能够运行的模型大小是可用DRAM的两倍，与传统的CPU和GPU加载方法相比，推理速度分别提高了4-5倍和20-25倍。</title>
            <link>https://nitter.cz/op7418/status/1737492846041530578#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737492846041530578#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 15:19:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚看到苹果发的这个论文《使用有限的内存实现更快的 LLM 推理》。通过将将模型参数保存在闪存里，根据需要移动到DRAM。<br />
<br />
使得能够运行的模型大小是可用DRAM的两倍，与传统的CPU和GPU加载方法相比，推理速度分别提高了4-5倍和20-25倍。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1737300118070534468#m">nitter.cz/_akhaliq/status/1737300118070534468#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</id>
            <title>RT by @dotey: XHS-Downloader：小红书采集器 

✅ 采集小红书图文/视频作品信息
✅ 提取小红书图文/视频作品下载地址
✅ 下载小红书无水印图文/视频作品文件
✅ 自动跳过已下载的作品文件
✅ 作品文件完整性处理机制
✅ 持久化储存作品信息至文件

GitHub：https://github.com/JoeanAmier/XHS-Downloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737460981331288421#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XHS-Downloader：小红书采集器 <br />
<br />
✅ 采集小红书图文/视频作品信息<br />
✅ 提取小红书图文/视频作品下载地址<br />
✅ 下载小红书无水印图文/视频作品文件<br />
✅ 自动跳过已下载的作品文件<br />
✅ 作品文件完整性处理机制<br />
✅ 持久化储存作品信息至文件<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/XHS-Downloader">github.com/JoeanAmier/XHS-Do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5elhIR2JVQUFjb2hQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737518777481535521#m</id>
            <title>RT by @dotey: ChatGPT聊天记录可以归档了，再也不用看着一堆没用碍眼的聊天记录了。

点击想归档的聊天记录右侧三个点，选择Archive Chat就可以了，已经归档的内容可以在设置中查看。</title>
            <link>https://nitter.cz/op7418/status/1737518777481535521#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737518777481535521#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 17:02:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT聊天记录可以归档了，再也不用看着一堆没用碍眼的聊天记录了。<br />
<br />
点击想归档的聊天记录右侧三个点，选择Archive Chat就可以了，已经归档的内容可以在设置中查看。</p>
<p><a href="https://nitter.cz/OpenAI/status/1737517702766633063#m">nitter.cz/OpenAI/status/1737517702766633063#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc1MTgzMzk3OTc0NTQ4NDgvcHUvaW1nL2Rjakt4NG80WnBUTzE1TlUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>