<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727789984348832160#m</id>
            <title>转译：关于 OpenAI 正在开发的 Q* RLHF 的揭秘时间

首先，让我们看看路透社文章中的一些有趣描述：
“资深高管 Mira Murati 在周三对员工表示，一封关于 AI 突破性进展 Q*（读作 Q-Star）的信件促使董事会采取了行动。”
+
“得益于强大的计算资源，这个新模型能解决一些数学问题……虽然目前只能解决小学生级别的数学问题，但这样的成绩让研究人员对 Q* 的未来充满期待。”

现在，请准备好：
OpenAI 的新技术 Q*（Q-star）融合了两个关键元素：Q Learning（一种强化学习算法）和 A Star（一种搜索算法）。

1. Q Learning 非常关键，它是第一个被广泛认可的强化学习算法，至今仍被广泛应用。在这里，Token 或词汇被视为状态，而某些回应则被视为行动。
2. A Star 是一种以其在搜索过程中保存结果于内存而著称的图搜索算法。文章中提到：“得益于强大的计算资源，新模型能解决一些数学问题”，这意味着在新的 RLHF 训练中需要存储大量数据。

搜索对于训练中的多轮优化至关重要。基本上，我认为是将 A* 公式应用于 Q 值，以实现多轮推理。

为什么这种方法可能非常有效但又难以实现呢？
* 多轮优化意味着需要更多的模型前向传递和梯度计算
* 解决复杂数学问题需要这种方法
* 实际上，这可能更接近于 RLAIF</title>
            <link>https://nitter.cz/dotey/status/1727789984348832160#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727789984348832160#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 20:43:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译：关于 OpenAI 正在开发的 Q* RLHF 的揭秘时间<br />
<br />
首先，让我们看看路透社文章中的一些有趣描述：<br />
“资深高管 Mira Murati 在周三对员工表示，一封关于 AI 突破性进展 Q*（读作 Q-Star）的信件促使董事会采取了行动。”<br />
+<br />
“得益于强大的计算资源，这个新模型能解决一些数学问题……虽然目前只能解决小学生级别的数学问题，但这样的成绩让研究人员对 Q* 的未来充满期待。”<br />
<br />
现在，请准备好：<br />
OpenAI 的新技术 Q*（Q-star）融合了两个关键元素：Q Learning（一种强化学习算法）和 A Star（一种搜索算法）。<br />
<br />
1. Q Learning 非常关键，它是第一个被广泛认可的强化学习算法，至今仍被广泛应用。在这里，Token 或词汇被视为状态，而某些回应则被视为行动。<br />
2. A Star 是一种以其在搜索过程中保存结果于内存而著称的图搜索算法。文章中提到：“得益于强大的计算资源，新模型能解决一些数学问题”，这意味着在新的 RLHF 训练中需要存储大量数据。<br />
<br />
搜索对于训练中的多轮优化至关重要。基本上，我认为是将 A* 公式应用于 Q 值，以实现多轮推理。<br />
<br />
为什么这种方法可能非常有效但又难以实现呢？<br />
* 多轮优化意味着需要更多的模型前向传递和梯度计算<br />
* 解决复杂数学问题需要这种方法<br />
* 实际上，这可能更接近于 RLAIF</p>
<p><a href="https://nitter.cz/natolambert/status/1727476436838265324#m">nitter.cz/natolambert/status/1727476436838265324#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727788966454218985#m</id>
            <title>推荐Slides：《ChatGLM: An Alternative to ChatGPT》清华唐杰老师写的，很多ChatGLM的干货

建议有兴趣的同学保存一份

完整链接：https://keg.cs.tsinghua.edu.cn/jietang/publications/iswc23-chatglm.pdf</title>
            <link>https://nitter.cz/dotey/status/1727788966454218985#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727788966454218985#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 20:39:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐Slides：《ChatGLM: An Alternative to ChatGPT》清华唐杰老师写的，很多ChatGLM的干货<br />
<br />
建议有兴趣的同学保存一份<br />
<br />
完整链接：<a href="https://keg.cs.tsinghua.edu.cn/jietang/publications/iswc23-chatglm.pdf">keg.cs.tsinghua.edu.cn/jieta…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wV0Zoclc0QUE1QVR3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wV2JCeldZQUFJNzRuLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wV2hZV1dnQUFPMlVRLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9wVzRMMFcwQUEtRnNULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/LatentSpace2000/status/1727666307074830368#m</id>
            <title>RT by @dotey: 本地化了一下</title>
            <link>https://nitter.cz/LatentSpace2000/status/1727666307074830368#m</link>
            <guid isPermaLink="false">https://nitter.cz/LatentSpace2000/status/1727666307074830368#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 12:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本地化了一下</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9ublQtX2FRQUFmZlBzLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9ublVpc2E0QUFVRF9HLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727781480678531395#m</id>
            <title>厉害了👍🏻</title>
            <link>https://nitter.cz/dotey/status/1727781480678531395#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727781480678531395#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 20:09:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>厉害了👍🏻</p>
<p><a href="https://nitter.cz/chee828/status/1727473511747453335#m">nitter.cz/chee828/status/1727473511747453335#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1727604618316972082#m</id>
            <title>RT by @dotey: 推荐一个超实用的网站，Papers with code，https://paperswithcode.com，它是一个论文和对应工程实现（含代码、数据集、测试方法等）的索引工具，下次你看到别人推荐优质论文时，可以直接用它去找代码实现。

有些论文价值很大，会被多次实现（如图二），但工程质量有高有低，你可以在这个网站详情页看到不同实现的 star 数量，择优学习；详情页还包含了数据集、测试方法、关联论文等等，你甚至还可以通过数据集去反查关联论文，简直是研究神器。

之前我都是去 Google 检索或者去 Aminer 上查看，有了这个工具就方便多了。</title>
            <link>https://nitter.cz/Barret_China/status/1727604618316972082#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1727604618316972082#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:26:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐一个超实用的网站，Papers with code，<a href="https://paperswithcode.com">paperswithcode.com</a>，它是一个论文和对应工程实现（含代码、数据集、测试方法等）的索引工具，下次你看到别人推荐优质论文时，可以直接用它去找代码实现。<br />
<br />
有些论文价值很大，会被多次实现（如图二），但工程质量有高有低，你可以在这个网站详情页看到不同实现的 star 数量，择优学习；详情页还包含了数据集、测试方法、关联论文等等，你甚至还可以通过数据集去反查关联论文，简直是研究神器。<br />
<br />
之前我都是去 Google 检索或者去 Aminer 上查看，有了这个工具就方便多了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tcV9iLWJvQUFTS3FNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tc1VFMmFJQUEzcjJJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727604343304495491#m</id>
            <title>建议你们看看这个推文和评论，很多有趣的讨论。

Jim Fan：
合成数据无疑将成为下一个万亿级高质量训练词元的来源。我相信，大多数专业的大语言模型团队都已意识到这一点。真正的挑战是如何持续保持这些数据的质量，避免过早陷入发展的停滞期。

Richard Sutton（@RichardSSutton）的《苦涩的教训》继续引领着人工智能的发展：只有两种模式能够随着计算能力的增加而无限扩展，那就是学习和搜索。这一观点在 2019 年他撰写时就成立，如今依旧适用，我敢说，直到我们解决通用人工智能的问题，这一点都不会改变。

Elon Musk：
确实。有些让人唏嘘的是，人类历史上写过的所有书籍的文字内容都能被存储在一个硬盘上。

而合成数据的数量将远远超出这个规模。

Jim Fan：
很多合成数据将源于具体代理，比如特斯拉 Optimus，前提是我们能够大规模地进行模拟。

Elon Musk：
是的（叹气）

Yann LeCun:
动物和人类凭借极少的训练数据就能迅速变得聪明。我认为，未来的趋势是开发出像动物和人类一样高效学习的新架构。目前使用大量数据（无论是真实的还是合成的）只是一种权宜之计，是因为我们现有方法的限制所迫。

Atlas3d.eth:
完全同意 - 看看乌鸦科的鸟类，它们拥有惊人的推理能力，却只需要很少的数据量。

Yann LeCun:
想象一下，像鹦鹉、狗和章鱼这样的动物大约有 20 亿个神经元。我们如何能让一台拥有 20 亿神经元、10 万亿参数的机器在短短几个月的实时训练中变得像这些动物一样聪明呢？

Eduardo Slonski:
1）我们使用了大量的数据。别忘了，我们不断接收大量的视频、音频和感官数据，更不用说 DNA 中编码的“指令”了。我们并非从零开始学习，而且我们的反应比大语言模型更为广泛和普遍。

2）在新架构的问题上，我同意你的看法。

Yann LeCun:
1. 人类基因组的数据量其实很小，只有 800MB。人类和黑猩猩基因组的差异也就 8MB。这远远不足以解释两者能力上的巨大差异。

2. 以一个两岁孩童所见的视觉数据量来看，实际上是相当有限的：大约 3200 万秒。我们有大约 200 万条视觉神经纤维，每秒大约传输 10 个字节，总计 6E14 字节。而大语言模型的训练数据量通常是 1E13 个 Token，约 2E13 字节，只多出大约 30 倍。

Eduardo Slonski:
基因组并不是作为数据来使用，而是起到指导作用。当我们开始训练模型时，其输出是随机的，但与人类不同，人类出生时已经有了本能的行为和寻找目标的能力。再次思考后，我更加认同你的观点：数据量并非关键，关键在于我们的架构以及针对该架构优化的数据。

Ferdous:
难道数百万年的进化适应不就相当于模型的预训练，而我们一生的经验不就类似于持续的微调吗？🤔

Yann LeCun:
并不完全是这样。这需要被浓缩成极少量的信息。比如，一个小型的 70 亿 Token 的大语言模型就需要 14GB。而你的整个基因组在未压缩的状态下也只有 800MB。人类和黑猩猩基因组之间仅有 1% 的差异，大约 8MB。这远远不足以编码复杂的结构。</title>
            <link>https://nitter.cz/dotey/status/1727604343304495491#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727604343304495491#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:25:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>建议你们看看这个推文和评论，很多有趣的讨论。<br />
<br />
Jim Fan：<br />
合成数据无疑将成为下一个万亿级高质量训练词元的来源。我相信，大多数专业的大语言模型团队都已意识到这一点。真正的挑战是如何持续保持这些数据的质量，避免过早陷入发展的停滞期。<br />
<br />
Richard Sutton（<a href="https://nitter.cz/RichardSSutton" title="Richard Sutton">@RichardSSutton</a>）的《苦涩的教训》继续引领着人工智能的发展：只有两种模式能够随着计算能力的增加而无限扩展，那就是学习和搜索。这一观点在 2019 年他撰写时就成立，如今依旧适用，我敢说，直到我们解决通用人工智能的问题，这一点都不会改变。<br />
<br />
Elon Musk：<br />
确实。有些让人唏嘘的是，人类历史上写过的所有书籍的文字内容都能被存储在一个硬盘上。<br />
<br />
而合成数据的数量将远远超出这个规模。<br />
<br />
Jim Fan：<br />
很多合成数据将源于具体代理，比如特斯拉 Optimus，前提是我们能够大规模地进行模拟。<br />
<br />
Elon Musk：<br />
是的（叹气）<br />
<br />
Yann LeCun:<br />
动物和人类凭借极少的训练数据就能迅速变得聪明。我认为，未来的趋势是开发出像动物和人类一样高效学习的新架构。目前使用大量数据（无论是真实的还是合成的）只是一种权宜之计，是因为我们现有方法的限制所迫。<br />
<br />
Atlas3d.eth:<br />
完全同意 - 看看乌鸦科的鸟类，它们拥有惊人的推理能力，却只需要很少的数据量。<br />
<br />
Yann LeCun:<br />
想象一下，像鹦鹉、狗和章鱼这样的动物大约有 20 亿个神经元。我们如何能让一台拥有 20 亿神经元、10 万亿参数的机器在短短几个月的实时训练中变得像这些动物一样聪明呢？<br />
<br />
Eduardo Slonski:<br />
1）我们使用了大量的数据。别忘了，我们不断接收大量的视频、音频和感官数据，更不用说 DNA 中编码的“指令”了。我们并非从零开始学习，而且我们的反应比大语言模型更为广泛和普遍。<br />
<br />
2）在新架构的问题上，我同意你的看法。<br />
<br />
Yann LeCun:<br />
1. 人类基因组的数据量其实很小，只有 800MB。人类和黑猩猩基因组的差异也就 8MB。这远远不足以解释两者能力上的巨大差异。<br />
<br />
2. 以一个两岁孩童所见的视觉数据量来看，实际上是相当有限的：大约 3200 万秒。我们有大约 200 万条视觉神经纤维，每秒大约传输 10 个字节，总计 6E14 字节。而大语言模型的训练数据量通常是 1E13 个 Token，约 2E13 字节，只多出大约 30 倍。<br />
<br />
Eduardo Slonski:<br />
基因组并不是作为数据来使用，而是起到指导作用。当我们开始训练模型时，其输出是随机的，但与人类不同，人类出生时已经有了本能的行为和寻找目标的能力。再次思考后，我更加认同你的观点：数据量并非关键，关键在于我们的架构以及针对该架构优化的数据。<br />
<br />
Ferdous:<br />
难道数百万年的进化适应不就相当于模型的预训练，而我们一生的经验不就类似于持续的微调吗？🤔<br />
<br />
Yann LeCun:<br />
并不完全是这样。这需要被浓缩成极少量的信息。比如，一个小型的 70 亿 Token 的大语言模型就需要 14GB。而你的整个基因组在未压缩的状态下也只有 800MB。人类和黑猩猩基因组之间仅有 1% 的差异，大约 8MB。这远远不足以编码复杂的结构。</p>
<p><a href="https://nitter.cz/DrJimFan/status/1727505774514180188#m">nitter.cz/DrJimFan/status/1727505774514180188#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727599590872928280#m</id>
            <title>《苦涩的教训》
Rich Sutton
2019 年 3 月 13 日
70 年人工智能研究给我们的最大启示是：依赖计算能力的通用方法最终表现最佳，而且优势明显。这背后的主要原因是摩尔定律，也就是计算成本持续以指数级下降。大部分 AI 研究都是在假设计算资源固定的情况下进行的（在这种情况下，利用人类知识几乎是提升性能的唯一途径），但实际上，在稍长于一般研究项目的时间里，可用的计算资源会大幅增加。研究者为了在短期内取得突破，尝试利用自己对特定领域的人类知识，但从长远看，关键在于计算能力的利用。这两者原本不必相互冲突，但在实践中却常常如此。投入其中一个领域的时间，就意味着在另一个上的缺失。此外，人们在一种方法上的投资也会形成心理承诺。而基于人类知识的方法往往会使系统变得复杂，不利于利用计算能力的通用方法。有很多例子显示 AI 研究者是如何迟迟才领悟到这个苦涩的教训，回顾这些案例非常有启发性。

以计算机国际象棋为例，1997 年击败世界冠军卡斯帕罗夫的方法主要是深度搜索。当时，大多数计算机国际象棋研究者对此表示失望，因为他们更倾向于利用对棋局特殊结构的人类理解。然而，当一个简单但基于搜索的方法，结合特殊的硬件和软件展现出巨大效能时，这些基于人类知识的研究者并不愿意接受失败。他们认为，尽管这次“蛮力”搜索获胜，但它并非一种通用策略，也不是人类下棋的方式。这些研究者本希望基于人类理解的方法能够取胜，对实际结果感到失望。

在计算机围棋的发展中，也出现了类似的模式，只是晚了 20 年。最初的努力都在于避免搜索，尽可能利用对游戏的人类理解和特殊特征，但一旦有效地应用了大规模搜索，这些努力都显得微不足道，甚至有害。在这个过程中，通过自我对弈学习价值函数（在很多其他游戏中也是这样，甚至包括国际象棋，尽管在 1997 年首次击败世界冠军的程序中学习的作用并不大）也非常关键。自我对弈学习和一般学习，就像搜索一样，能够充分利用大量计算资源。在计算机围棋和国际象棋中，研究者最初都是试图利用人类的理解来减少搜索的需要，但最终通过接受搜索和学习才取得了巨大的成功。

在语音识别领域，1970 年代由 DARPA 赞助的一场早期比赛就是一个例子。参赛者包括使用了大量人类知识（如对单词、音素、人类声道的理解）的特殊方法，而另一边则是更依赖统计和大量计算的新方法，基于隐马尔可夫模型（HMMs）。最终，基于统计的方法战胜了基于人类知识的方法。这导致了自然语言处理领域的一次重大转变，随着时间的推移，统计和计算开始成为该领域的主导。深度学习在语音识别中的兴起是这一趋势的最新体现。深度学习方法更少依赖人类知识，使用更多的计算资源，并通过在大型训练集上的学习，极大地提升了语音识别系统的性能。与游戏领域相似，研究人员总是试图创建一个按照他们自己的思维方式工作的系统，但这种尝试最终证明是逆向而行，不仅浪费了大量的研究时间，而且在大量计算资源可用并找到有效利用方法的情况下，这种尝试显得更是多余。

计算机视觉领域也经历了相似的发展模式。早期的方法试图通过搜索边缘、广义圆柱体或 SIFT 特征来处理视觉问题。但在今天，这些方法都被淘汰了。现代的深度学习神经网络仅使用卷积和某些类型的不变性概念，取得了更好的表现。

这是一个重要的教训。作为一个领域，我们还没有完全吸取这一教训，仍在重蹈覆辙。为了识别并避免这种错误，我们必须理解其吸引力所在。我们必须领悟到，试图构建一个基于我们认为自己思考方式的系统是行不通的。苦涩的教训源于这样的历史观察：1) 人工智能研究者经常试图将知识融入他们的代理中；2) 这在短期内总是有益的，也让研究者感到满足；但 3) 从长远来看，这种做法会导致进步停滞，甚至阻碍进一步的发展；4) 真正的突破性进展最终是通过一个相反的方法实现的，这个方法基于通过搜索和学习来扩大计算的规模。这种成功带有苦涩，往往消化不良，因为它是在人类中心化方法之上取得的。

从这个苦涩的教训中，我们应该明白通用方法的巨大力量，即那些随着计算能力的增长而持续扩展的方法。在这方面，似乎可以无限扩展的两种方法是搜索和学习。

苦涩教训中的另一个关键点是，人类心灵的实质内容极其复杂，不可能简化；我们应该放弃试图简单化地理解心灵内容，如空间、物体、多重代理或对称性等概念。这些都是外部世界中任意而复杂的部分，不应该成为我们构建的核心；相反，我们应该构建的是那些能够发现并捕捉这种任意复杂性的元方法。这些方法的核心在于它们能够找到良好的近似，但寻找这些近似的过程应该由我们的方法来完成，而不是我们亲自动手。我们希望 AI 代理能像我们一样具有发现能力，而不是仅仅包含我们已有的发现。将我们的发现直接构建进去，只会使我们更难看清如何实现发现的过程。</title>
            <link>https://nitter.cz/dotey/status/1727599590872928280#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727599590872928280#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:06:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>《苦涩的教训》<br />
Rich Sutton<br />
2019 年 3 月 13 日<br />
70 年人工智能研究给我们的最大启示是：依赖计算能力的通用方法最终表现最佳，而且优势明显。这背后的主要原因是摩尔定律，也就是计算成本持续以指数级下降。大部分 AI 研究都是在假设计算资源固定的情况下进行的（在这种情况下，利用人类知识几乎是提升性能的唯一途径），但实际上，在稍长于一般研究项目的时间里，可用的计算资源会大幅增加。研究者为了在短期内取得突破，尝试利用自己对特定领域的人类知识，但从长远看，关键在于计算能力的利用。这两者原本不必相互冲突，但在实践中却常常如此。投入其中一个领域的时间，就意味着在另一个上的缺失。此外，人们在一种方法上的投资也会形成心理承诺。而基于人类知识的方法往往会使系统变得复杂，不利于利用计算能力的通用方法。有很多例子显示 AI 研究者是如何迟迟才领悟到这个苦涩的教训，回顾这些案例非常有启发性。<br />
<br />
以计算机国际象棋为例，1997 年击败世界冠军卡斯帕罗夫的方法主要是深度搜索。当时，大多数计算机国际象棋研究者对此表示失望，因为他们更倾向于利用对棋局特殊结构的人类理解。然而，当一个简单但基于搜索的方法，结合特殊的硬件和软件展现出巨大效能时，这些基于人类知识的研究者并不愿意接受失败。他们认为，尽管这次“蛮力”搜索获胜，但它并非一种通用策略，也不是人类下棋的方式。这些研究者本希望基于人类理解的方法能够取胜，对实际结果感到失望。<br />
<br />
在计算机围棋的发展中，也出现了类似的模式，只是晚了 20 年。最初的努力都在于避免搜索，尽可能利用对游戏的人类理解和特殊特征，但一旦有效地应用了大规模搜索，这些努力都显得微不足道，甚至有害。在这个过程中，通过自我对弈学习价值函数（在很多其他游戏中也是这样，甚至包括国际象棋，尽管在 1997 年首次击败世界冠军的程序中学习的作用并不大）也非常关键。自我对弈学习和一般学习，就像搜索一样，能够充分利用大量计算资源。在计算机围棋和国际象棋中，研究者最初都是试图利用人类的理解来减少搜索的需要，但最终通过接受搜索和学习才取得了巨大的成功。<br />
<br />
在语音识别领域，1970 年代由 DARPA 赞助的一场早期比赛就是一个例子。参赛者包括使用了大量人类知识（如对单词、音素、人类声道的理解）的特殊方法，而另一边则是更依赖统计和大量计算的新方法，基于隐马尔可夫模型（HMMs）。最终，基于统计的方法战胜了基于人类知识的方法。这导致了自然语言处理领域的一次重大转变，随着时间的推移，统计和计算开始成为该领域的主导。深度学习在语音识别中的兴起是这一趋势的最新体现。深度学习方法更少依赖人类知识，使用更多的计算资源，并通过在大型训练集上的学习，极大地提升了语音识别系统的性能。与游戏领域相似，研究人员总是试图创建一个按照他们自己的思维方式工作的系统，但这种尝试最终证明是逆向而行，不仅浪费了大量的研究时间，而且在大量计算资源可用并找到有效利用方法的情况下，这种尝试显得更是多余。<br />
<br />
计算机视觉领域也经历了相似的发展模式。早期的方法试图通过搜索边缘、广义圆柱体或 SIFT 特征来处理视觉问题。但在今天，这些方法都被淘汰了。现代的深度学习神经网络仅使用卷积和某些类型的不变性概念，取得了更好的表现。<br />
<br />
这是一个重要的教训。作为一个领域，我们还没有完全吸取这一教训，仍在重蹈覆辙。为了识别并避免这种错误，我们必须理解其吸引力所在。我们必须领悟到，试图构建一个基于我们认为自己思考方式的系统是行不通的。苦涩的教训源于这样的历史观察：1) 人工智能研究者经常试图将知识融入他们的代理中；2) 这在短期内总是有益的，也让研究者感到满足；但 3) 从长远来看，这种做法会导致进步停滞，甚至阻碍进一步的发展；4) 真正的突破性进展最终是通过一个相反的方法实现的，这个方法基于通过搜索和学习来扩大计算的规模。这种成功带有苦涩，往往消化不良，因为它是在人类中心化方法之上取得的。<br />
<br />
从这个苦涩的教训中，我们应该明白通用方法的巨大力量，即那些随着计算能力的增长而持续扩展的方法。在这方面，似乎可以无限扩展的两种方法是搜索和学习。<br />
<br />
苦涩教训中的另一个关键点是，人类心灵的实质内容极其复杂，不可能简化；我们应该放弃试图简单化地理解心灵内容，如空间、物体、多重代理或对称性等概念。这些都是外部世界中任意而复杂的部分，不应该成为我们构建的核心；相反，我们应该构建的是那些能够发现并捕捉这种任意复杂性的元方法。这些方法的核心在于它们能够找到良好的近似，但寻找这些近似的过程应该由我们的方法来完成，而不是我们亲自动手。我们希望 AI 代理能像我们一样具有发现能力，而不是仅仅包含我们已有的发现。将我们的发现直接构建进去，只会使我们更难看清如何实现发现的过程。</p>
<p><a href="https://nitter.cz/elonmusk/status/1727587012394295493#m">nitter.cz/elonmusk/status/1727587012394295493#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727594974366060978#m</id>
            <title>Wikipedia 上现在有了 Q* 的词条
https://en.wikipedia.org/wiki/Q*</title>
            <link>https://nitter.cz/dotey/status/1727594974366060978#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727594974366060978#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:48:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wikipedia 上现在有了 Q* 的词条<br />
<a href="https://en.wikipedia.org/wiki/Q">en.wikipedia.org/wiki/Q</a>*</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tbUpLOFh3QUFNbDBRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727585886949282272#m</id>
            <title>Andrej Karpathy 的最新教程：[1hr Talk] Intro to Large Language Models ，排在我的待翻译列表中。

最近吃瓜耽误了不少事，得先补补🥲

https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=3s</title>
            <link>https://nitter.cz/dotey/status/1727585886949282272#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727585886949282272#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:12:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Andrej Karpathy 的最新教程：[1hr Talk] Intro to Large Language Models ，排在我的待翻译列表中。<br />
<br />
最近吃瓜耽误了不少事，得先补补🥲<br />
<br />
<a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=3s">youtube.com/watch?v=zjkBMFhN…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzUxNDg0MDE4MzMyMDU3Ni95MEpYZkF3aD9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727568964065382411#m</id>
            <title>RT by @dotey: 更多Q*（Q-Star）信息爆料和猜测：

- Q*可能具备自主学习和自我改进的能力。

- Q*模型可进行自主决策，可能已具备轻微自我意识。

- GPT-Zero项目解决了数据问题，自己”生产“数据。

- OpenAI可能正在利用计算机合成数据进行训练。

据Reddit用户爆料和猜测：Q*可能是一种非常先进的具有“可怕数学能力”的模型，已经具备自主学习和自我改进的能力。

该模型能够通过评估其行为的长期后果，在广泛的场景中做出复杂的决策。

Q*可能与强化学习中的Q-learning算法有关，这是一种评估在特定情境下采取特定行动的好坏的方法。还提到了Q-Value和Bellman方程，这些都是强化学习中的重要概念，用于指导AI在不同情境下做出最优决策。

简而言之，Q*似乎是一个高级的人工智能模型，能够在多种情境中学习和做出最优决策，具有自主学习和自我改进的能力。

Reddit原帖：https://www.reddit.com/r/OpenAI/comments/181n8am/what_is_q/

而根据@theinformation 今天的报道，OpenAI的首席科学家 Ilya Sutskever 领衔的一个名叫 GPT-Zero 的项目实现了巨大突破。他们克服了训练数据限制的困难，可以自己合成训练数据。

 据悉，Ilya Sutskever的 GPT-Zero 的项目，帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。

此前Ilya在一次采访中说到：

“Without going into details I'll just say the Data Limit can be overcome..."  

“无需详细说明，我只是说数据限制是可以克服的......”  

GPT-Zero项目研究的主要使用计算机生成的数据来训练模型，而不是从互联网上提取的真实世界数据，因为OpenAI已经获得了互联网上能获得的几乎所有的真实文本数据，已经无法再获得足够的数据来进行下一阶段训练。

OpenAI 研究团队利用GPT-Zero这一创新成果，构建了能解决基础数学问题的模型，这一直是现有 AI 模型的难题，无法进行复杂的推理能力。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Ilya Sutskever 的研究成果，开发出了这个名为 Q*（Q-Star）的模型。

Theinformation报道：https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern

而本月奥特曼在接受金融时报采访的时候曾表达：开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。

他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识的能力。

种种迹象表明Q*（Q-Star）的模型确实是超越了GPT 4非常多的模型，可以说是直接跨越了几代，具有自主学习和自我改进的能力，甚至就像之前Ilya表达的可能已经表现出了轻微的自我意识能力。

以上内容为综合报道、爆料的可能性猜测总结，不一定准确，请注意分辨！</title>
            <link>https://nitter.cz/xiaohuggg/status/1727568964065382411#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727568964065382411#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 06:05:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>更多Q*（Q-Star）信息爆料和猜测：<br />
<br />
- Q*可能具备自主学习和自我改进的能力。<br />
<br />
- Q*模型可进行自主决策，可能已具备轻微自我意识。<br />
<br />
- GPT-Zero项目解决了数据问题，自己”生产“数据。<br />
<br />
- OpenAI可能正在利用计算机合成数据进行训练。<br />
<br />
据Reddit用户爆料和猜测：Q*可能是一种非常先进的具有“可怕数学能力”的模型，已经具备自主学习和自我改进的能力。<br />
<br />
该模型能够通过评估其行为的长期后果，在广泛的场景中做出复杂的决策。<br />
<br />
Q*可能与强化学习中的Q-learning算法有关，这是一种评估在特定情境下采取特定行动的好坏的方法。还提到了Q-Value和Bellman方程，这些都是强化学习中的重要概念，用于指导AI在不同情境下做出最优决策。<br />
<br />
简而言之，Q*似乎是一个高级的人工智能模型，能够在多种情境中学习和做出最优决策，具有自主学习和自我改进的能力。<br />
<br />
Reddit原帖：<a href="https://teddit.net/r/OpenAI/comments/181n8am/what_is_q/">teddit.net/r/OpenAI/comments…</a><br />
<br />
而根据<a href="https://nitter.cz/theinformation" title="The Information">@theinformation</a> 今天的报道，OpenAI的首席科学家 Ilya Sutskever 领衔的一个名叫 GPT-Zero 的项目实现了巨大突破。他们克服了训练数据限制的困难，可以自己合成训练数据。<br />
<br />
 据悉，Ilya Sutskever的 GPT-Zero 的项目，帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。<br />
<br />
此前Ilya在一次采访中说到：<br />
<br />
“Without going into details I'll just say the Data Limit can be overcome..."  <br />
<br />
“无需详细说明，我只是说数据限制是可以克服的......”  <br />
<br />
GPT-Zero项目研究的主要使用计算机生成的数据来训练模型，而不是从互联网上提取的真实世界数据，因为OpenAI已经获得了互联网上能获得的几乎所有的真实文本数据，已经无法再获得足够的数据来进行下一阶段训练。<br />
<br />
OpenAI 研究团队利用GPT-Zero这一创新成果，构建了能解决基础数学问题的模型，这一直是现有 AI 模型的难题，无法进行复杂的推理能力。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Ilya Sutskever 的研究成果，开发出了这个名为 Q*（Q-Star）的模型。<br />
<br />
Theinformation报道：<a href="https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern">theinformation.com/articles/…</a><br />
<br />
而本月奥特曼在接受金融时报采访的时候曾表达：开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。<br />
<br />
他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识的能力。<br />
<br />
种种迹象表明Q*（Q-Star）的模型确实是超越了GPT 4非常多的模型，可以说是直接跨越了几代，具有自主学习和自我改进的能力，甚至就像之前Ilya表达的可能已经表现出了轻微的自我意识能力。<br />
<br />
以上内容为综合报道、爆料的可能性猜测总结，不一定准确，请注意分辨！</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tS2RGYWE4QUFQRVdTLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tT1BPdmJnQUE5Q3UxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/huangyun_122/status/1727360100283896056#m</id>
            <title>RT by @dotey: 这哥们，凭一己之力，硬把 Stable Diffusion 的玩法，都开源了出来

https://stable-diffusion-art.com/sdxl-model/#Download_and_install_SDXL_10_models

实在心疼自己的笔记本，烧到 86°后，最终还是移到 Google Colab 上玩

但，如果不是 G-Colab Pro, 时不时会被 G-Colab 掐掉，导致每次都要重连。

还是付费香，办了会员，把 runtime 拉到最大，效率起来了</title>
            <link>https://nitter.cz/huangyun_122/status/1727360100283896056#m</link>
            <guid isPermaLink="false">https://nitter.cz/huangyun_122/status/1727360100283896056#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 16:15:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这哥们，凭一己之力，硬把 Stable Diffusion 的玩法，都开源了出来<br />
<br />
<a href="https://stable-diffusion-art.com/sdxl-model/#Download_and_install_SDXL_10_models">stable-diffusion-art.com/sdx…</a><br />
<br />
实在心疼自己的笔记本，烧到 86°后，最终还是移到 Google Colab 上玩<br />
<br />
但，如果不是 G-Colab Pro, 时不时会被 G-Colab 掐掉，导致每次都要重连。<br />
<br />
还是付费香，办了会员，把 runtime 拉到最大，效率起来了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9qUC1VX1hBQUFFU2sxLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9qUXpoOFhvQUFpb3A4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727563974479057028#m</id>
            <title>OpenAI “宫斗”大戏人物关系图</title>
            <link>https://nitter.cz/dotey/status/1727563974479057028#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727563974479057028#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 05:45:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI “宫斗”大戏人物关系图</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tS1JrRVc0QUFCdGMyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727556370507972669#m</id>
            <title>R to @dotey: 斧正：“然而，就在第二天，董事会宣布解雇了 Altman。”之后的内容不属于原文，不小心添加进去无法删除，请注意过滤。</title>
            <link>https://nitter.cz/dotey/status/1727556370507972669#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727556370507972669#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 05:15:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>斧正：“然而，就在第二天，董事会宣布解雇了 Altman。”之后的内容不属于原文，不小心添加进去无法删除，请注意过滤。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727549427278086281#m</id>
            <title>R to @dotey: reddit.com/r/singularity/com…</title>
            <link>https://nitter.cz/dotey/status/1727549427278086281#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727549427278086281#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:47:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://teddit.net/r/singularity/comments/181oe7i/openai_made_an_ai_breakthrough_before_altman/">teddit.net/r/singularity/com…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzUxMDM3MzU3NjkzNzQ3Mi9PR2NTcDZkUD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727549316779094080#m</id>
            <title>R to @dotey: Reddit上的讨论帖：https://www.reddit.com/r/OpenAI/comments/181n8am/what_is_q/</title>
            <link>https://nitter.cz/dotey/status/1727549316779094080#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727549316779094080#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:47:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Reddit上的讨论帖：<a href="https://teddit.net/r/OpenAI/comments/181n8am/what_is_q/">teddit.net/r/OpenAI/comments…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzUwMDA3MDcwNTA0MTQwOC92UERLNlA3Zj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727548654108475558#m</id>
            <title>不知道你们还记不记得当年 Google 的 Alpha Go，就是下围棋超级牛的人工智能，早已经打赢了人类。而且值得一提的是，最开始它是用人类的棋谱训练的，后来在人类的棋谱训练完了后，它开始自己和自己下棋，也就是说是自己生成棋谱自己训练自己。

在GPT-4训练的时候，几乎用到了所有互联网上能用到的文本，很多人都担心GPT-4训练完了后在没有高质量的文本供训练了。

在前面《独家：OpenAI 研究员在 CEO 被撤职前向董事会发出 AI 突破警告 — 据知情人士透露》https://twitter.com/dotey/status/1727510593970561466 那篇文章中，提到了OpenAI的一个神秘项目叫Q-Star，而这个项目背后的一些秘密正在被一点点的揭开神秘面纱。

根据 theinformation《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》的这篇报道中，提到了一个细节：“他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。”

这意味着他们也像AlphaGo一样，实现了让计算机生成无限的高质量数据，而不需要去互联网上抓取数据！！！

现在还不知道这篇报道的准确性如何，但是结合这些信息，可能性还是相当大的！很期待不久能看到GPT-5的发布！

《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》部分内容翻译：

----

OpenAI 在 Sam Altman 被解雇前夕实现重大 AI 突破，引发期待与忧虑

在 Sam Altman 上周被 OpenAI 董事会解雇的前一天，他曾在 APEC 首席执行官峰会上暗示该公司最近取得了一项技术进展。他表示，这一进展让公司能够揭开无知的面纱，推进知识的边界。这番神秘的言论在公司陷入混乱之际并未引起太多关注。

据知情人士透露，部分 OpenAI 员工认为 Altman 的这番话是在提及公司研究人员今年早些时候的一项创新。这项创新可能使他们能够开发出更强大的人工智能模型。然而，由首席科学家 Ilya Sutskever 领衔的这一技术突破，也引发了一些员工的担忧，他们认为公司在商业化这些高级 AI 模型时可能缺乏适当的安全措施。

据悉，Sutskever 的这一突破帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。这一直是开发下一代模型的主要障碍。他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。

多年来，Sutskever 一直致力于研究如何让像 GPT-4 这样的语言模型解决涉及推理的任务，例如数学或科学问题。2021 年，他启动了一个名为 GPT-Zero 的项目，以向 DeepMind 的 AlphaZero 程序致敬，后者能够下国际象棋、围棋和将棋。

OpenAI 研究团队利用这一创新成果，构建了能解决基础数学问题的系统，这一直是现有 AI 模型的难题。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Sutskever 的研究成果，开发出了一个名为 Q*（“Q-Star”）的模型。</title>
            <link>https://nitter.cz/dotey/status/1727548654108475558#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727548654108475558#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:44:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不知道你们还记不记得当年 Google 的 Alpha Go，就是下围棋超级牛的人工智能，早已经打赢了人类。而且值得一提的是，最开始它是用人类的棋谱训练的，后来在人类的棋谱训练完了后，它开始自己和自己下棋，也就是说是自己生成棋谱自己训练自己。<br />
<br />
在GPT-4训练的时候，几乎用到了所有互联网上能用到的文本，很多人都担心GPT-4训练完了后在没有高质量的文本供训练了。<br />
<br />
在前面《独家：OpenAI 研究员在 CEO 被撤职前向董事会发出 AI 突破警告 — 据知情人士透露》<a href="https://nitter.cz/dotey/status/1727510593970561466">nitter.cz/dotey/status/172…</a> 那篇文章中，提到了OpenAI的一个神秘项目叫Q-Star，而这个项目背后的一些秘密正在被一点点的揭开神秘面纱。<br />
<br />
根据 theinformation《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》的这篇报道中，提到了一个细节：“他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。”<br />
<br />
这意味着他们也像AlphaGo一样，实现了让计算机生成无限的高质量数据，而不需要去互联网上抓取数据！！！<br />
<br />
现在还不知道这篇报道的准确性如何，但是结合这些信息，可能性还是相当大的！很期待不久能看到GPT-5的发布！<br />
<br />
《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》部分内容翻译：<br />
<br />
----<br />
<br />
OpenAI 在 Sam Altman 被解雇前夕实现重大 AI 突破，引发期待与忧虑<br />
<br />
在 Sam Altman 上周被 OpenAI 董事会解雇的前一天，他曾在 APEC 首席执行官峰会上暗示该公司最近取得了一项技术进展。他表示，这一进展让公司能够揭开无知的面纱，推进知识的边界。这番神秘的言论在公司陷入混乱之际并未引起太多关注。<br />
<br />
据知情人士透露，部分 OpenAI 员工认为 Altman 的这番话是在提及公司研究人员今年早些时候的一项创新。这项创新可能使他们能够开发出更强大的人工智能模型。然而，由首席科学家 Ilya Sutskever 领衔的这一技术突破，也引发了一些员工的担忧，他们认为公司在商业化这些高级 AI 模型时可能缺乏适当的安全措施。<br />
<br />
据悉，Sutskever 的这一突破帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。这一直是开发下一代模型的主要障碍。他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。<br />
<br />
多年来，Sutskever 一直致力于研究如何让像 GPT-4 这样的语言模型解决涉及推理的任务，例如数学或科学问题。2021 年，他启动了一个名为 GPT-Zero 的项目，以向 DeepMind 的 AlphaZero 程序致敬，后者能够下国际象棋、围棋和将棋。<br />
<br />
OpenAI 研究团队利用这一创新成果，构建了能解决基础数学问题的系统，这一直是现有 AI 模型的难题。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Sutskever 的研究成果，开发出了一个名为 Q*（“Q-Star”）的模型。</p>
<p><a href="https://nitter.cz/bindureddy/status/1727479646315237784#m">nitter.cz/bindureddy/status/1727479646315237784#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sOERaR1cwQUFBVUZnLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sOEtuQ1dVQUF3UDRDLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727530341202563280#m</id>
            <title>刚发现Ilya也转发了，看起来状态还不错，希望他们能继续合作</title>
            <link>https://nitter.cz/dotey/status/1727530341202563280#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727530341202563280#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 03:31:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚发现Ilya也转发了，看起来状态还不错，希望他们能继续合作</p>
<p><a href="https://nitter.cz/ilyasut/status/1727434066411286557#m">nitter.cz/ilyasut/status/1727434066411286557#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727517538722914365#m</id>
            <title>训练过的不算多牛，出个没训练过的就知道了</title>
            <link>https://nitter.cz/dotey/status/1727517538722914365#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727517538722914365#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:40:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>训练过的不算多牛，出个没训练过的就知道了</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/waylybaye/status/1727510389087433103#m</id>
            <title>RT by @dotey: OpenCat 1.8.3 更新啦，将生成图片的功能带到了 iOS 上 🥳</title>
            <link>https://nitter.cz/waylybaye/status/1727510389087433103#m</link>
            <guid isPermaLink="false">https://nitter.cz/waylybaye/status/1727510389087433103#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:12:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenCat 1.8.3 更新啦，将生成图片的功能带到了 iOS 上 🥳</p>
<p><a href="https://nitter.cz/waylybaye/status/1722413580317073702#m">nitter.cz/waylybaye/status/1722413580317073702#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sWmlvUmE4QUExOTl4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sWmlvU2JJQUFGT0w4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>