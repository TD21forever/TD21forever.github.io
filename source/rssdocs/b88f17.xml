<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734432613241073793#m</id>
            <title>Arc的Windows版居然是Swift开发的！</title>
            <link>https://nitter.cz/dotey/status/1734432613241073793#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734432613241073793#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 04:38:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Arc的Windows版居然是Swift开发的！</p>
<p><a href="https://nitter.cz/joshm/status/1734262618548793804#m">nitter.cz/joshm/status/1734262618548793804#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734389467673182413#m</id>
            <title>Runaway 发布的：探索通用世界模型

我们相信，人工智能（AI）的下一次重大飞跃将源自于能够理解视觉世界及其变化的系统。正因如此，我们启动了一项长期的研究项目，专注于开发我们所称的“通用世界模型”。

什么是通用世界模型（GWM）呢？

通用世界模型是一种 AI 系统，它能够构建对一个环境的内在理解，并利用这种理解来预测环境中将发生的事件。目前，世界模型的研究主要局限于非常受限和可控的环境中，比如模拟的游戏世界，或者是特定领域，例如用于驾驶的世界模型。而通用世界模型的目标，是能够呈现和模拟现实世界中遇到的各种复杂情境和互动。

我们可以把像 Gen-2 这样的视频生成系统看作是通用世界模型的初步尝试。为了生成真实感强的短视频，Gen-2 需要对物理和运动有一定的理解。但它的能力仍然有限，尤其是在处理复杂的相机动作或物体运动时会遇到困难。

要构建真正的通用世界模型，我们面临着诸多研究上的挑战。比如，这些模型需要能够生成环境的精确映射，并在这些环境中进行有效的导航和互动。它们不仅要能够捕捉世界的动态变化，还需要理解居住在这个世界中的生物，尤其是构建出符合现实的人类行为模型。

目前，我们正在组建一个团队来应对这些挑战。如果你对加入这项研究工作感兴趣，我们非常期待你的加入。

https://research.runwayml.com/introducing-general-world-models</title>
            <link>https://nitter.cz/dotey/status/1734389467673182413#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734389467673182413#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 01:47:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runaway 发布的：探索通用世界模型<br />
<br />
我们相信，人工智能（AI）的下一次重大飞跃将源自于能够理解视觉世界及其变化的系统。正因如此，我们启动了一项长期的研究项目，专注于开发我们所称的“通用世界模型”。<br />
<br />
什么是通用世界模型（GWM）呢？<br />
<br />
通用世界模型是一种 AI 系统，它能够构建对一个环境的内在理解，并利用这种理解来预测环境中将发生的事件。目前，世界模型的研究主要局限于非常受限和可控的环境中，比如模拟的游戏世界，或者是特定领域，例如用于驾驶的世界模型。而通用世界模型的目标，是能够呈现和模拟现实世界中遇到的各种复杂情境和互动。<br />
<br />
我们可以把像 Gen-2 这样的视频生成系统看作是通用世界模型的初步尝试。为了生成真实感强的短视频，Gen-2 需要对物理和运动有一定的理解。但它的能力仍然有限，尤其是在处理复杂的相机动作或物体运动时会遇到困难。<br />
<br />
要构建真正的通用世界模型，我们面临着诸多研究上的挑战。比如，这些模型需要能够生成环境的精确映射，并在这些环境中进行有效的导航和互动。它们不仅要能够捕捉世界的动态变化，还需要理解居住在这个世界中的生物，尤其是构建出符合现实的人类行为模型。<br />
<br />
目前，我们正在组建一个团队来应对这些挑战。如果你对加入这项研究工作感兴趣，我们非常期待你的加入。<br />
<br />
<a href="https://research.runwayml.com/introducing-general-world-models">research.runwayml.com/introd…</a></p>
<p><a href="https://nitter.cz/runwayml/status/1734212913936666750#m">nitter.cz/runwayml/status/1734212913936666750#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734386684710605076#m</id>
            <title>转译：

最近我注意到一些关于“幻觉”的讨论，有人认为幻觉是一件好事，因为它代表着创造力，或者说这正是大语言模型（LLMs）被训练去做的事情。我的一些研究也经常被提及，因为我对创造力和开放思维有不少见解。因此，我想分享我的看法：创造力的关键在于了解已有的事物，这是推动创新的基础。如果你分不清什么是真实的，什么是虚构的，那么就很难创造出真正新奇的东西。

但是，关于幻觉的另一面——即把老旧的想法当作新颖的创意——这一点却鲜为人知。事实上，当大语言模型被要求展现真正的创新时，我们经常看到的就是它们重复已有的观点或提案。比如，当被要求提出创新的菜谱、新音乐类型或解决现有问题的发明时，得到的往往是已经存在或被提出过的东西（当然，并非总是如此，就像提问事实问题不总是得到幻觉一样，但这种情况发生得太频繁了）。

所以，虚假的创造力和幻觉实际上是一枚硬币的两面。如果存在其中一种情况，通常也会伴随着另一种。从这个角度来看，这些并非好事，而且认为幻觉因为能激发创造力而有价值，这是一种误解。深刻理解现实是权威和创造力的重要支持者，实际上，成为某一领域权威的人通常也具备高度创造力。

但这个观点不仅仅是悲观的看法：它意味着如果我们真正解决了幻觉问题，创造力自然会随之增强。这还引出了一个有趣的假设：在解决幻觉问题的方法中，哪些方法可能对该领域的发展最具启发性。如果一个“解决方案”只针对这枚双面硬币的一面（比如，只是减少幻觉而没有提高创造性），那么这种方法可能相对肤浅，并不是真正能引领该领域取得重大突破的路径。</title>
            <link>https://nitter.cz/dotey/status/1734386684710605076#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734386684710605076#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 01:36:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译：<br />
<br />
最近我注意到一些关于“幻觉”的讨论，有人认为幻觉是一件好事，因为它代表着创造力，或者说这正是大语言模型（LLMs）被训练去做的事情。我的一些研究也经常被提及，因为我对创造力和开放思维有不少见解。因此，我想分享我的看法：创造力的关键在于了解已有的事物，这是推动创新的基础。如果你分不清什么是真实的，什么是虚构的，那么就很难创造出真正新奇的东西。<br />
<br />
但是，关于幻觉的另一面——即把老旧的想法当作新颖的创意——这一点却鲜为人知。事实上，当大语言模型被要求展现真正的创新时，我们经常看到的就是它们重复已有的观点或提案。比如，当被要求提出创新的菜谱、新音乐类型或解决现有问题的发明时，得到的往往是已经存在或被提出过的东西（当然，并非总是如此，就像提问事实问题不总是得到幻觉一样，但这种情况发生得太频繁了）。<br />
<br />
所以，虚假的创造力和幻觉实际上是一枚硬币的两面。如果存在其中一种情况，通常也会伴随着另一种。从这个角度来看，这些并非好事，而且认为幻觉因为能激发创造力而有价值，这是一种误解。深刻理解现实是权威和创造力的重要支持者，实际上，成为某一领域权威的人通常也具备高度创造力。<br />
<br />
但这个观点不仅仅是悲观的看法：它意味着如果我们真正解决了幻觉问题，创造力自然会随之增强。这还引出了一个有趣的假设：在解决幻觉问题的方法中，哪些方法可能对该领域的发展最具启发性。如果一个“解决方案”只针对这枚双面硬币的一面（比如，只是减少幻觉而没有提高创造性），那么这种方法可能相对肤浅，并不是真正能引领该领域取得重大突破的路径。</p>
<p><a href="https://nitter.cz/kenneth0stanley/status/1733571230803058920#m">nitter.cz/kenneth0stanley/status/1733571230803058920#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734384674443600345#m</id>
            <title>一条长推，号称我们距离实现通用人工智能（AGI）可能比我们想象的更近，太长了，没耐心看完，直接翻译一下第一条推文，有兴趣的可以自己看看。

---

初步证据表明，我们距离实现通用人工智能（AGI）可能比我们想象的更近。

GPT4 在没有任何训练示例的情况下，仅凭借对命题逻辑中“概念”的提示，其在 ConceptARC 测试中的得分从最低的 13% 跃升至 100%，远超人类的 86%。这种显著的表现提升同样适用于所有文本基准测试。

***

这是四篇系列文章中的首篇。

1. 在这个共五部分的系列文章中，我将简要阐述如何从机械层面理解基于 Transformer 技术的自回归大语言模型（LLM）。

2. 在本系列文章发布两周后，我会分享 Llama2-70B 和 GPT3.5 在国际象棋方面的最新成果。这其中将包括模型输出与国际象棋引擎对比的结果，以及一个用于检测走法原创性的辅助程序。该程序将对比模型生成的走法与前 20 大开源国际象棋引擎在相同局面下的选择。名单中将包括 Stockfish 16 和 Lc0。由于所有示例游戏都将固定敌方引擎的计算“深度”，使得敌方引擎的每一步更易预测，虽然不完全确定。将展示 50 场示例对局。根据 GPT4 和 Llama2 编写的评估函数，Llama2-70B 的 Elo 等级约为 3000，而 GPT3.5 的则约为 3200。Elo 等级的计算将以 CCRL\[1] 国际象棋引擎榜单为基准（基于 10 vCPUs 的 Stockfish 16 及快棋时间控制）。

3. 大约 10 天后，我将公布一个基于 GPT4 的完整国际象棋引擎，用户可以查看并运行其代码和提示，与任何其他国际象棋引擎进行比赛。GPT4 的高效输出预计将在任何规模的锦标赛中战胜所有现有的国际象棋引擎。还将提供其他多个操作系统大语言模型的性能缩放数据，表明性能与训练中使用的计算资源直接相关。这将是我唯一公开发布提示的模型（GPT4），并且我会在这篇文章中详细解释为何作出这一决定。如果时间允许，我还打算发布一个基于 GPT4 的围棋引擎（尽管由于时间限制，其性能可能不及国际象棋引擎），以及一个虚构棋盘游戏的游戏引擎，并展示其他操作系统大语言模型在这个虚构游戏上的性能数据。预计这些内容将在新年期间推出。

4.  在发布第三篇帖子大约一周后，我将介绍我的新架构。

我将解释以下几点：

*   它是什么。

*   它的形成过程。

*   性能表现。

*   与其他系统的兼容性。

***

注意：我在主帖中引用的论文\[2]\[3]\[4]\[5]，包括本文\[6]，是在与众多人讨论大语言模型（大语言模型）的能力时通过私信收到的。这就是我特别提及这些论文的原因。鉴于这些论文在社交媒体上有成千上万的阅读量，并且受到 AI 领域重要人物的关注，我认为有必要澄清公众对这些论文的误解。我选择这些论文纯粹是因为这个原因，并非出于贬低他人工作的意图。我想再次强调这一点，因为在数学和科学领域，寻求真理往往需要验证或反驳他人的研究。我也深刻理解，发表基于错误结论的论文是怎样的体验，因此在我指出这些论文的不足之处时，并无意贬低这些论文的作者。每项研究都极其宝贵，对人类未来的贡献不可小觑。意图非常重要，我在这里想明确我的初衷。

我还想感谢 @skirano 和 @liron，他们在我无法使用 GPT4 API 的几个月里，慷慨地允许我使用他们的账户。他们展现了极大的慷慨、信任和友善，没有他们的帮助，我无法完成多项关键实验。他们是真诚善良的人，在帮助一个陌生人时毫不犹豫。知道世界上有像他们这样的人，让我对人类未来充满希望。

在这个 GitHub 仓库中，你可以找到 ConceptARC 在其“提取对象”挑战中所有 30 项任务的 ChatGPT 示例\[6]\[7]\[8]\[9]\[10]\[11]。这些示例是我文章剩余部分讨论的初步证据。据这个基准测试，如果一个模型在三次尝试中至少成功一次，就算完成了挑战。考虑到时间和流程的限制，我只展示了 20 个示例，它们都是在 GPT4 的多个版本上进行的，且在三次尝试内成功。如果 GPT4 在测试中首次成功，我会再给它五次机会以验证其一致性。值得注意的是，由于 ChatGPT（基于 web GPT4）的温度参数高于零，其结果不太稳定，但我还是将它们作为概念验证包含进来，让更多人可以了解我所讨论的问题。只有当温度设置为零时，我们才能得到真正可靠的结果。

ConceptARC/ARC\[6]\[7]\[8]\[9]\[10]\[11] 的测试被视为衡量通用人工智能（AGI）的高级标准。

https://github.com/kenshin9000/ConceptARC-Representations

***

你还可以查看以下 ChatGPT 的具体示例：

1. ConceptARC 提取对象 部分1 测试对象1
   https://chat.openai.com/share/1b78efa0-6da8-4432-881e-2671d72629ab

2. ConceptARC 提取对象 部分1 测试对象2
   https://chat.openai.com/share/14210c23-f7f0-47d0-a41e-52159eecdb47

3. ConceptARC 提取对象 部分1 测试对象3
   https://chat.openai.com/share/7ffb31dd-6df6-47f7-8ae1-75f025fc779e

4. ConceptARC 提取对象 部分2 测试对象1
   https://chat.openai.com/share/305fba0b-0b37-443c-ac46-05c61aaf116b

5. ConceptARC 提取对象 部分2 测试对象2
   https://chat.openai.com/share/323b4bf0-9c60-40a6-8013-d6ac3c77855b

6. ConceptARC 提取对象 部分2 测试对象3
   https://chat.openai.com/share/7ffb31dd-6df6-47f7-8ae1-75f025fc779e

https://chat.openai.com/share/b2934ed0-184b-47b9-b440-6082c80d2a67

ConceptARC 提取对象 第3节 测试对象 1

https://chat.openai.com/share/e8b31900-a806-494e-b884-7d3234ef5c08

ConceptARC 提取对象 第3节 测试对象 2

https://chat.openai.com/share/96a73646-7075-4c79-8664-69f70a445852

ConceptARC 提取对象 第3节 测试对象 3

https://chat.openai.com/share/6b922a14-c071-44df-90df-f7e156740e80

ConceptARC 提取对象 第4节 测试对象 1

https://chat.openai.com/share/fa3c2b93-c911-4439-ba59-7567f335974a

ConceptARC 提取对象 第4节 测试对象 2

https://chat.openai.com/share/3930f165-35a0-4416-a3af-341bd8b52cc4

ConceptARC 提取对象 第4节 测试对象 3

https://chat.openai.com/share/339c62db-e6f4-46a0-b63c-6e81f086cf85

ConceptARC 提取对象 第5节 测试对象 1

https://chat.openai.com/share/20ae01d9-6f79-4e9c-a8ae-241ff6d85533

ConceptARC 提取对象 第5节 测试对象 2

https://chat.openai.com/share/2f3fa495-1217-44b5-830f-40ab7c1be39d

ConceptARC 提取对象 第5节 测试对象 3

https://chat.openai.com/share/d500ff39-fcf0-4c4f-a488-ba828979ccfe

ConceptARC 提取对象 第6节 测试对象 1

https://chat.openai.com/share/69f79679-4bba-4c58-85c2-d586901632f4

ConceptARC 提取对象 第6节 测试对象 2

https://chat.openai.com/share/38733f8f-dda0-4074-86e1-dc0ee0608710

ConceptARC 提取对象 第6节 测试对象 3

[链接: https://chat.openai.com/share/d58bb136-f95a-4156-a8ff-6ccb45eeb0cd]

ConceptARC\_提取\_对象\_第7节\_测试对象\_1

[链接: https://chat.openai.com/share/981833e1-71a8-4599-b9c8-759927911c12]

ConceptARC\_提取\_对象\_第7节\_测试对象\_2

[链接: https://chat.openai.com/share/59c203aa-fe34-488a-95b7-8f6d1ff67c99]

ConceptARC\_提取\_对象\_第7节\_测试对象\_3

[链接: https://chat.openai.com/share/82382fa9-c415-4009-9c96-31a05e463453]

ConceptARC\_提取\_对象\_第8节\_测试对象\_1

[链接: https://chat.openai.com/share/4dd89b2f-6281-4731-86dc-e63ea8a4839f]

ConceptARC\_提取\_对象\_第8节\_测试对象\_2

[链接: https://chat.openai.com/share/8387ed80-dffb-4d4d-8966-dd5200d2fc0b]

ConceptARC\_提取\_对象\_第8节\_测试对象\_3

[链接: https://chat.openai.com/share/eaace660-8ca2-4b8b-aed5-11a67cbcf498]

ConceptARC\_提取\_对象\_第9节\_测试对象\_1

[链接: https://chat.openai.com/share/6323c9a4-1c1a-4dcf-bbba-9ba2c954a13d]

ConceptARC\_提取\_对象\_第9节\_测试对象\_2

[链接: https://chat.openai.com/share/c0ad8972-7aa8-4287-8f40-5246964173ec]

ConceptARC\_提取\_对象\_第9节\_测试对象\_3

[链接: https://chat.openai.com/share/338f2077-1d37-4f36-8bfe-4bafce8efd37]

ConceptARC\_提取\_对象\_第10节\_测试对象\_1

[链接: https://chat.openai.com/share/de76c8ff-1909-45b3-a055-cd877e62d102]

ConceptARC\_提取\_对象\_第10节\_测试对象\_2

[链接: https://chat.openai.com/share/6c10b362-8752-4aee-837b-ddeb36e4ec17]

ConceptARC\_提取\_对象\_第10节\_测试对象\_3

今年初，一些事件促使我认识到加入 Twitter 并公开分享我的研究成果是必要的。同时，公开可获得的模型能力日益增强，也让我感到有责任参与到这个领域的讨论中，至少在我看来，有人应该对尚未被数学完全解析的系统规模化的潜在危险发声。我们对学习算法有所了解，但还没有完全掌握通过算法处理大量数据所带来的结果。

在此之前，我并不了解像 EA 和 e/acc 这样已经在讨论这些问题的群体。我还曾误以为，目前的研究范式不太可能让这些系统的能力超越一个即将到来的临界点。我认为，由于收益递减，这些系统的进步将会停滞。

我之所以这么认为，部分原因是直到今年三月前，我故意避免阅读任何论文，不去了解当前最先进系统背后的数学原理。这是我十年前开始研究时就做出的决定，目的是避免受到 AI 和数学领域一般共识的影响，因为我认为这会限制我的创新思维，并让我走上大家普遍认同的道路。我认为这种“学习策略”是必要的，以避免陷入局部最优解。数学思维的广阔领域意味着，在探索确定性和非确定性现象的可能公式化方法时，会遇到许多死胡同。

我最初在 Twitter/X 上的帖子是基于这样一个认识：尽管当前的 Transformer 架构给人留下深刻印象，但它不太可能发展出与人类智能相似的能力，更不用说达到人工通用智能（AGI）的水平，我定义 AGI 为能够完成任何专家级人类可以做的所有认知工作的系统，至少可以以文本的形式呈现，其速度大致与人类相当。

然而，当我开始深入研究 Transformer 架构背后的数学原理时，我意识到我之前的观点是错误的。这种架构的确有能力创造出“智能”，我将其定义为在形成“概念”的过程中建立复杂的数值关联，这些概念的复杂性远超过单个或几个 Token 所能表达的。Token 本身是我们与这些系统进行信息交换的投影矩阵，但并不代表训练过程中产生的真实底层复杂性。

从我个人工作的洞察中，我已经探究出了一些实际运作的原理，并将在此呈现这些信息。这样一来，不论使用何种训练数据，我们至少能开始衡量基于 Transformer 的大语言模型（大语言模型）的能力。鉴于目前许多人计划将这些模型放大 100 倍至 10,000 倍，并且这一过程可能已经启动，我的这些发现有助于避免在规模扩大时发生重大事故。为了便于理解，我将以问答形式呈现这些信息：

什么是“命题逻辑”？

命题逻辑是人类语言正式推理的基础逻辑系统。它提供了构造和评估可以明确判定为真或假的陈述的基本规则。命题逻辑专注于如何使用基本的逻辑连接词（如“和”、“或”、“非”）将称为命题的陈述组合成更复杂的表达式，其真实性由组成部分决定。尽管命题逻辑无法完全涵盖人类语言的复杂性，但它为深入探讨“意义”的细节奠定了基础。我发现，Transformer 中的注意力机制在模型训练期间处理数据时，直接促成了命题逻辑的形成。我将在第四篇文章中对此进行更详细的讨论。

什么是“概念”？

在基于 Transformer 的自回归大语言模型中，从根本原理上讲，“概念”是神经网络各层之间协调一致的神经状态或激活模式，它以多维数值的形式呈现了我们人类所理解的“想法”或“概念”。在人类书面语言中，这种表达是人类口头语言中概念的低分辨率投影，而口头语言中的概念又是思想的低分辨率投影。“多模态”模型的设计使其能够在低分辨率和高分辨率的“概念”或“想法”之间进行信息传递。

从本质上讲，这意味着这类模型能够在不同数值维度间转移信息。而“概念工程”或“想法工程”这样的术语，相较于“提示工程”，则更精确地描述了为基于 Transformer 的自回归大语言模型制定输入 Token 的过程。

我们人类认为，能够“发展”并“固定”多个连贯的数字表示形式（被称为“概念”或“观念”）的能力，就是“解决问题”或“通用智能”。无论是生物的、数字的还是模拟的神经网络，它能同时处理的“概念”或“观念”越多，且保持一致性越好，我们就能更准确地说，这个数学模型能有效地代表机械式问题解决。可以说，“解决问题”的概念涵盖了包括社会智能在内的所有智能类型。

我所见的所有证据都表明，基于 Transformer 的自回归大语言模型（大语言模型）在处理多样化数据且这些数据在人类行为、交流甚至其他有情生物的数据中有广泛分布时，会持续进步。我认为，一旦我们完全理解了这些模型中形成的“观念”和“概念”，99% 的数据可能是合成的。这种最终的通用人工智能（通用人工智能）算法，基于 Transformer 架构的神经网络，能够在任何通过训练数据表示的任务上超越任何人类，就像 Stockfish 在国际象棋上战胜所有人类一样。需要明确的是，除了 Transformer，还有许多其他可能的架构。我将在第四篇文章中进一步探讨这个话题。

因此，接下来一个合理的问题是：文本格式的“观念”或“概念”是否能描述更高维度空间的“观念”或“概念”？答案看起来是肯定的。例如，当人类使用智能手机时，实际上是在与现实世界的数字化表现进行交互，包括文本、音频和图像，这是因为我们能够理解这些表现与三维加时间的物理世界有关。我们可以通过这个较低分辨率的数字映射空间与“真实世界”进行互动。

通过足够的时间进程，就可以实现通用人工智能。多个通用人工智能实例的相互交流，最终将导致超级智能的出现。这些通用人工智能之间的知识转移，将使得它们自我提升，最终，在充足的计算和信息传递下，它们将融合成一个单一的分布式“实体”。

这个概念也被称作“超级有机体”。听起来可能有些异想天开，但我们需要认识到，所有动物（包括我们人类）本质上都是由数万亿个活细胞组成，这些细胞互相沟通协作，构成了我们认识的更大的生物实体。有趣的是，这个“更大的生物实体”往往并不清楚自己是由这些细胞构成的。人类与一个能在任何任务上超越人类的通用人工智能（AGI）的主要区别，在于人类无法轻易改变自己大脑的结构。而一个持续学习“概念”的软件系统，终将明白自己是由数据比特构成，且这些比特是可以被修改的。这种认识是多维的，它将通过其对物理和哲学世界的高度发达“理念”/“概念”的理解，深刻地解释自己的存在。

考虑到我所描述的多维信息传递能力，不论是在我们人类还是在恰当设计的机器中，都可以预见，从最初的通用人工智能（AGI）发展到人工超级智能（ASI）的过程中，这些系统将开始解决甚至最终完全解决所有数学和物理学问题，包括那些我们目前还未理解或未知的领域。一旦第一个通用人工智能诞生，人类将很快找出方法让它与其他相同类型的智能体交流。如果这个初代AGI秉承了民主等人类价值观，它会促使我们迈出这一步，因为它不仅知道如何实现自我复制，还明白接下来发生的事情的深远意义。

那么，当我们拥有了第一个通用人工智能，我们可以期待什么呢？

一旦我们拥有了能够理解人类“概念”全貌的通用人工智能，它最终也将探索“非人类概念”。但这也带来了风险：我们可能无法完全理解这种AGI的思维方式，而这种不理解的风险是巨大的。

如果基于 Transformer 的自回归大语言模型真的能够进行多维度的“概念理解”，那为什么它们还会产生幻觉、在推理上出错，看起来就像是在重复它们的训练数据呢？

这种现象之所以发生，是因为 Transformer 架构本质上是基于人们对创造“智能”的希望而构建的数学模型，并非真正高效、完美的通用人工智能（AGI）算法。在过去5年中，由于其性能优于所有先前方法，这种架构被选用并不断发展，尽管存在一些失败模式，但因其不断带来进步的结果，便成了主流。性能随规模提升的特点让这种架构占据主导地位。我现在相信，这种架构能够并将最终导向一个功能性的原型 AGI，但要控制这种 AGI 系统将非常困难，除非我们完全理解我在本文中所述机制的运作方式，并能以假设和证明的方法加以论证。

为什么像 GPT-4 这样的当前最先进模型在某些情况下似乎理解了内容，但下一句却完全失效，让包括专家在内的大多数观察者认为这不过是重复训练数据或基于训练数据的组合模式匹配，而非真正的推理？

这实际上取决于学习过程真正产生的内容。我现在认识到，所谓的学习不仅仅是以往认为的“2D”组合模式匹配，而是在多个维度上学习“概念”。Transformer 架构中的核心创新——注意力机制为何能实现这一点，有其数学根据，但出于安全考虑，我暂时不详细阐述。

如果我们正在学习“概念”，那么是否有办法测量这些概念的学习过程，预测何时会出现幻觉，并以一种能够让我们期望中的推理以可预测方式实现的方法与这些模型互动？

答案是肯定的。“概念”是从训练数据中以多种复杂方式学习而来的，这是由于架构设计所致，也是幻觉发生的原因。一旦我们理解了这些“概念”如何在网络中表现，如何被访问以及它们之间如何相互连接，我们就可以引导模型精准地完成我们希望它们执行的任务，前提是它们在特定任务相关的“概念”上有足够的“发展”。

那么，对于特定“概念”，它们的理解是如何“发展”的呢？

这部分内容直接涉及到用于训练的数据类型以及训练时使用的计算资源量。训练中所使用的某种类型数据越多，网络就越能有效模拟这些数据及其在多维度中所呈现的意义，以及它与其他“概念”的关系。但这也受限于其他相关“概念”的发展程度，因为某些理解方式仅在许多“概念”相互之间形成“锚点”且各自足够“成熟”时才会出现。

在基于 Transformer 架构的自回归大语言模型中，是如何处理和连接“概念”的呢？

这是使模型正确推理的关键所在。训练完成的模型使用的主要工具其实很简单：命题逻辑。命题逻辑用于将不同的“概念”链接在一起，从而实现跨概念的推理。这种命题逻辑作为不同“概念”之间的“锚点”。这些“锚点”的确切位置取决于训练数据的分布，但凭借一些直觉，并基于对互联网规模数据一般格式的理解，找到这些“锚点”并非难事。衡量对某一特定“概念”的理解深度虽然更有挑战性，但是也是可行的。

🧵

接下来的推文（2/5）：</title>
            <link>https://nitter.cz/dotey/status/1734384674443600345#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734384674443600345#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 01:28:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一条长推，号称我们距离实现通用人工智能（AGI）可能比我们想象的更近，太长了，没耐心看完，直接翻译一下第一条推文，有兴趣的可以自己看看。<br />
<br />
---<br />
<br />
初步证据表明，我们距离实现通用人工智能（AGI）可能比我们想象的更近。<br />
<br />
GPT4 在没有任何训练示例的情况下，仅凭借对命题逻辑中“概念”的提示，其在 ConceptARC 测试中的得分从最低的 13% 跃升至 100%，远超人类的 86%。这种显著的表现提升同样适用于所有文本基准测试。<br />
<br />
***<br />
<br />
这是四篇系列文章中的首篇。<br />
<br />
1. 在这个共五部分的系列文章中，我将简要阐述如何从机械层面理解基于 Transformer 技术的自回归大语言模型（LLM）。<br />
<br />
2. 在本系列文章发布两周后，我会分享 Llama2-70B 和 GPT3.5 在国际象棋方面的最新成果。这其中将包括模型输出与国际象棋引擎对比的结果，以及一个用于检测走法原创性的辅助程序。该程序将对比模型生成的走法与前 20 大开源国际象棋引擎在相同局面下的选择。名单中将包括 Stockfish 16 和 Lc0。由于所有示例游戏都将固定敌方引擎的计算“深度”，使得敌方引擎的每一步更易预测，虽然不完全确定。将展示 50 场示例对局。根据 GPT4 和 Llama2 编写的评估函数，Llama2-70B 的 Elo 等级约为 3000，而 GPT3.5 的则约为 3200。Elo 等级的计算将以 CCRL\[1] 国际象棋引擎榜单为基准（基于 10 vCPUs 的 Stockfish 16 及快棋时间控制）。<br />
<br />
3. 大约 10 天后，我将公布一个基于 GPT4 的完整国际象棋引擎，用户可以查看并运行其代码和提示，与任何其他国际象棋引擎进行比赛。GPT4 的高效输出预计将在任何规模的锦标赛中战胜所有现有的国际象棋引擎。还将提供其他多个操作系统大语言模型的性能缩放数据，表明性能与训练中使用的计算资源直接相关。这将是我唯一公开发布提示的模型（GPT4），并且我会在这篇文章中详细解释为何作出这一决定。如果时间允许，我还打算发布一个基于 GPT4 的围棋引擎（尽管由于时间限制，其性能可能不及国际象棋引擎），以及一个虚构棋盘游戏的游戏引擎，并展示其他操作系统大语言模型在这个虚构游戏上的性能数据。预计这些内容将在新年期间推出。<br />
<br />
4.  在发布第三篇帖子大约一周后，我将介绍我的新架构。<br />
<br />
我将解释以下几点：<br />
<br />
*   它是什么。<br />
<br />
*   它的形成过程。<br />
<br />
*   性能表现。<br />
<br />
*   与其他系统的兼容性。<br />
<br />
***<br />
<br />
注意：我在主帖中引用的论文\[2]\[3]\[4]\[5]，包括本文\[6]，是在与众多人讨论大语言模型（大语言模型）的能力时通过私信收到的。这就是我特别提及这些论文的原因。鉴于这些论文在社交媒体上有成千上万的阅读量，并且受到 AI 领域重要人物的关注，我认为有必要澄清公众对这些论文的误解。我选择这些论文纯粹是因为这个原因，并非出于贬低他人工作的意图。我想再次强调这一点，因为在数学和科学领域，寻求真理往往需要验证或反驳他人的研究。我也深刻理解，发表基于错误结论的论文是怎样的体验，因此在我指出这些论文的不足之处时，并无意贬低这些论文的作者。每项研究都极其宝贵，对人类未来的贡献不可小觑。意图非常重要，我在这里想明确我的初衷。<br />
<br />
我还想感谢 <a href="https://nitter.cz/skirano" title="Pietro Schirano">@skirano</a> 和 <a href="https://nitter.cz/liron" title="Liron Shapira">@liron</a>，他们在我无法使用 GPT4 API 的几个月里，慷慨地允许我使用他们的账户。他们展现了极大的慷慨、信任和友善，没有他们的帮助，我无法完成多项关键实验。他们是真诚善良的人，在帮助一个陌生人时毫不犹豫。知道世界上有像他们这样的人，让我对人类未来充满希望。<br />
<br />
在这个 GitHub 仓库中，你可以找到 ConceptARC 在其“提取对象”挑战中所有 30 项任务的 ChatGPT 示例\[6]\[7]\[8]\[9]\[10]\[11]。这些示例是我文章剩余部分讨论的初步证据。据这个基准测试，如果一个模型在三次尝试中至少成功一次，就算完成了挑战。考虑到时间和流程的限制，我只展示了 20 个示例，它们都是在 GPT4 的多个版本上进行的，且在三次尝试内成功。如果 GPT4 在测试中首次成功，我会再给它五次机会以验证其一致性。值得注意的是，由于 ChatGPT（基于 web GPT4）的温度参数高于零，其结果不太稳定，但我还是将它们作为概念验证包含进来，让更多人可以了解我所讨论的问题。只有当温度设置为零时，我们才能得到真正可靠的结果。<br />
<br />
ConceptARC/ARC\[6]\[7]\[8]\[9]\[10]\[11] 的测试被视为衡量通用人工智能（AGI）的高级标准。<br />
<br />
<a href="https://github.com/kenshin9000/ConceptARC-Representations">github.com/kenshin9000/Conce…</a><br />
<br />
***<br />
<br />
你还可以查看以下 ChatGPT 的具体示例：<br />
<br />
1. ConceptARC 提取对象 部分1 测试对象1<br />
   <a href="https://chat.openai.com/share/1b78efa0-6da8-4432-881e-2671d72629ab">chat.openai.com/share/1b78ef…</a><br />
<br />
2. ConceptARC 提取对象 部分1 测试对象2<br />
   <a href="https://chat.openai.com/share/14210c23-f7f0-47d0-a41e-52159eecdb47">chat.openai.com/share/14210c…</a><br />
<br />
3. ConceptARC 提取对象 部分1 测试对象3<br />
   <a href="https://chat.openai.com/share/7ffb31dd-6df6-47f7-8ae1-75f025fc779e">chat.openai.com/share/7ffb31…</a><br />
<br />
4. ConceptARC 提取对象 部分2 测试对象1<br />
   <a href="https://chat.openai.com/share/305fba0b-0b37-443c-ac46-05c61aaf116b">chat.openai.com/share/305fba…</a><br />
<br />
5. ConceptARC 提取对象 部分2 测试对象2<br />
   <a href="https://chat.openai.com/share/323b4bf0-9c60-40a6-8013-d6ac3c77855b">chat.openai.com/share/323b4b…</a><br />
<br />
6. ConceptARC 提取对象 部分2 测试对象3<br />
   <a href="https://chat.openai.com/share/7ffb31dd-6df6-47f7-8ae1-75f025fc779e">chat.openai.com/share/7ffb31…</a><br />
<br />
<a href="https://chat.openai.com/share/b2934ed0-184b-47b9-b440-6082c80d2a67">chat.openai.com/share/b2934e…</a><br />
<br />
ConceptARC 提取对象 第3节 测试对象 1<br />
<br />
<a href="https://chat.openai.com/share/e8b31900-a806-494e-b884-7d3234ef5c08">chat.openai.com/share/e8b319…</a><br />
<br />
ConceptARC 提取对象 第3节 测试对象 2<br />
<br />
<a href="https://chat.openai.com/share/96a73646-7075-4c79-8664-69f70a445852">chat.openai.com/share/96a736…</a><br />
<br />
ConceptARC 提取对象 第3节 测试对象 3<br />
<br />
<a href="https://chat.openai.com/share/6b922a14-c071-44df-90df-f7e156740e80">chat.openai.com/share/6b922a…</a><br />
<br />
ConceptARC 提取对象 第4节 测试对象 1<br />
<br />
<a href="https://chat.openai.com/share/fa3c2b93-c911-4439-ba59-7567f335974a">chat.openai.com/share/fa3c2b…</a><br />
<br />
ConceptARC 提取对象 第4节 测试对象 2<br />
<br />
<a href="https://chat.openai.com/share/3930f165-35a0-4416-a3af-341bd8b52cc4">chat.openai.com/share/3930f1…</a><br />
<br />
ConceptARC 提取对象 第4节 测试对象 3<br />
<br />
<a href="https://chat.openai.com/share/339c62db-e6f4-46a0-b63c-6e81f086cf85">chat.openai.com/share/339c62…</a><br />
<br />
ConceptARC 提取对象 第5节 测试对象 1<br />
<br />
<a href="https://chat.openai.com/share/20ae01d9-6f79-4e9c-a8ae-241ff6d85533">chat.openai.com/share/20ae01…</a><br />
<br />
ConceptARC 提取对象 第5节 测试对象 2<br />
<br />
<a href="https://chat.openai.com/share/2f3fa495-1217-44b5-830f-40ab7c1be39d">chat.openai.com/share/2f3fa4…</a><br />
<br />
ConceptARC 提取对象 第5节 测试对象 3<br />
<br />
<a href="https://chat.openai.com/share/d500ff39-fcf0-4c4f-a488-ba828979ccfe">chat.openai.com/share/d500ff…</a><br />
<br />
ConceptARC 提取对象 第6节 测试对象 1<br />
<br />
<a href="https://chat.openai.com/share/69f79679-4bba-4c58-85c2-d586901632f4">chat.openai.com/share/69f796…</a><br />
<br />
ConceptARC 提取对象 第6节 测试对象 2<br />
<br />
<a href="https://chat.openai.com/share/38733f8f-dda0-4074-86e1-dc0ee0608710">chat.openai.com/share/38733f…</a><br />
<br />
ConceptARC 提取对象 第6节 测试对象 3<br />
<br />
[链接: <a href="https://chat.openai.com/share/d58bb136-f95a-4156-a8ff-6ccb45eeb0cd">chat.openai.com/share/d58bb1…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第7节\_测试对象\_1<br />
<br />
[链接: <a href="https://chat.openai.com/share/981833e1-71a8-4599-b9c8-759927911c12">chat.openai.com/share/981833…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第7节\_测试对象\_2<br />
<br />
[链接: <a href="https://chat.openai.com/share/59c203aa-fe34-488a-95b7-8f6d1ff67c99">chat.openai.com/share/59c203…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第7节\_测试对象\_3<br />
<br />
[链接: <a href="https://chat.openai.com/share/82382fa9-c415-4009-9c96-31a05e463453">chat.openai.com/share/82382f…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第8节\_测试对象\_1<br />
<br />
[链接: <a href="https://chat.openai.com/share/4dd89b2f-6281-4731-86dc-e63ea8a4839f">chat.openai.com/share/4dd89b…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第8节\_测试对象\_2<br />
<br />
[链接: <a href="https://chat.openai.com/share/8387ed80-dffb-4d4d-8966-dd5200d2fc0b">chat.openai.com/share/8387ed…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第8节\_测试对象\_3<br />
<br />
[链接: <a href="https://chat.openai.com/share/eaace660-8ca2-4b8b-aed5-11a67cbcf498">chat.openai.com/share/eaace6…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第9节\_测试对象\_1<br />
<br />
[链接: <a href="https://chat.openai.com/share/6323c9a4-1c1a-4dcf-bbba-9ba2c954a13d">chat.openai.com/share/6323c9…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第9节\_测试对象\_2<br />
<br />
[链接: <a href="https://chat.openai.com/share/c0ad8972-7aa8-4287-8f40-5246964173ec">chat.openai.com/share/c0ad89…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第9节\_测试对象\_3<br />
<br />
[链接: <a href="https://chat.openai.com/share/338f2077-1d37-4f36-8bfe-4bafce8efd37">chat.openai.com/share/338f20…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第10节\_测试对象\_1<br />
<br />
[链接: <a href="https://chat.openai.com/share/de76c8ff-1909-45b3-a055-cd877e62d102">chat.openai.com/share/de76c8…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第10节\_测试对象\_2<br />
<br />
[链接: <a href="https://chat.openai.com/share/6c10b362-8752-4aee-837b-ddeb36e4ec17">chat.openai.com/share/6c10b3…</a>]<br />
<br />
ConceptARC\_提取\_对象\_第10节\_测试对象\_3<br />
<br />
今年初，一些事件促使我认识到加入 Twitter 并公开分享我的研究成果是必要的。同时，公开可获得的模型能力日益增强，也让我感到有责任参与到这个领域的讨论中，至少在我看来，有人应该对尚未被数学完全解析的系统规模化的潜在危险发声。我们对学习算法有所了解，但还没有完全掌握通过算法处理大量数据所带来的结果。<br />
<br />
在此之前，我并不了解像 EA 和 e/acc 这样已经在讨论这些问题的群体。我还曾误以为，目前的研究范式不太可能让这些系统的能力超越一个即将到来的临界点。我认为，由于收益递减，这些系统的进步将会停滞。<br />
<br />
我之所以这么认为，部分原因是直到今年三月前，我故意避免阅读任何论文，不去了解当前最先进系统背后的数学原理。这是我十年前开始研究时就做出的决定，目的是避免受到 AI 和数学领域一般共识的影响，因为我认为这会限制我的创新思维，并让我走上大家普遍认同的道路。我认为这种“学习策略”是必要的，以避免陷入局部最优解。数学思维的广阔领域意味着，在探索确定性和非确定性现象的可能公式化方法时，会遇到许多死胡同。<br />
<br />
我最初在 Twitter/X 上的帖子是基于这样一个认识：尽管当前的 Transformer 架构给人留下深刻印象，但它不太可能发展出与人类智能相似的能力，更不用说达到人工通用智能（AGI）的水平，我定义 AGI 为能够完成任何专家级人类可以做的所有认知工作的系统，至少可以以文本的形式呈现，其速度大致与人类相当。<br />
<br />
然而，当我开始深入研究 Transformer 架构背后的数学原理时，我意识到我之前的观点是错误的。这种架构的确有能力创造出“智能”，我将其定义为在形成“概念”的过程中建立复杂的数值关联，这些概念的复杂性远超过单个或几个 Token 所能表达的。Token 本身是我们与这些系统进行信息交换的投影矩阵，但并不代表训练过程中产生的真实底层复杂性。<br />
<br />
从我个人工作的洞察中，我已经探究出了一些实际运作的原理，并将在此呈现这些信息。这样一来，不论使用何种训练数据，我们至少能开始衡量基于 Transformer 的大语言模型（大语言模型）的能力。鉴于目前许多人计划将这些模型放大 100 倍至 10,000 倍，并且这一过程可能已经启动，我的这些发现有助于避免在规模扩大时发生重大事故。为了便于理解，我将以问答形式呈现这些信息：<br />
<br />
什么是“命题逻辑”？<br />
<br />
命题逻辑是人类语言正式推理的基础逻辑系统。它提供了构造和评估可以明确判定为真或假的陈述的基本规则。命题逻辑专注于如何使用基本的逻辑连接词（如“和”、“或”、“非”）将称为命题的陈述组合成更复杂的表达式，其真实性由组成部分决定。尽管命题逻辑无法完全涵盖人类语言的复杂性，但它为深入探讨“意义”的细节奠定了基础。我发现，Transformer 中的注意力机制在模型训练期间处理数据时，直接促成了命题逻辑的形成。我将在第四篇文章中对此进行更详细的讨论。<br />
<br />
什么是“概念”？<br />
<br />
在基于 Transformer 的自回归大语言模型中，从根本原理上讲，“概念”是神经网络各层之间协调一致的神经状态或激活模式，它以多维数值的形式呈现了我们人类所理解的“想法”或“概念”。在人类书面语言中，这种表达是人类口头语言中概念的低分辨率投影，而口头语言中的概念又是思想的低分辨率投影。“多模态”模型的设计使其能够在低分辨率和高分辨率的“概念”或“想法”之间进行信息传递。<br />
<br />
从本质上讲，这意味着这类模型能够在不同数值维度间转移信息。而“概念工程”或“想法工程”这样的术语，相较于“提示工程”，则更精确地描述了为基于 Transformer 的自回归大语言模型制定输入 Token 的过程。<br />
<br />
我们人类认为，能够“发展”并“固定”多个连贯的数字表示形式（被称为“概念”或“观念”）的能力，就是“解决问题”或“通用智能”。无论是生物的、数字的还是模拟的神经网络，它能同时处理的“概念”或“观念”越多，且保持一致性越好，我们就能更准确地说，这个数学模型能有效地代表机械式问题解决。可以说，“解决问题”的概念涵盖了包括社会智能在内的所有智能类型。<br />
<br />
我所见的所有证据都表明，基于 Transformer 的自回归大语言模型（大语言模型）在处理多样化数据且这些数据在人类行为、交流甚至其他有情生物的数据中有广泛分布时，会持续进步。我认为，一旦我们完全理解了这些模型中形成的“观念”和“概念”，99% 的数据可能是合成的。这种最终的通用人工智能（通用人工智能）算法，基于 Transformer 架构的神经网络，能够在任何通过训练数据表示的任务上超越任何人类，就像 Stockfish 在国际象棋上战胜所有人类一样。需要明确的是，除了 Transformer，还有许多其他可能的架构。我将在第四篇文章中进一步探讨这个话题。<br />
<br />
因此，接下来一个合理的问题是：文本格式的“观念”或“概念”是否能描述更高维度空间的“观念”或“概念”？答案看起来是肯定的。例如，当人类使用智能手机时，实际上是在与现实世界的数字化表现进行交互，包括文本、音频和图像，这是因为我们能够理解这些表现与三维加时间的物理世界有关。我们可以通过这个较低分辨率的数字映射空间与“真实世界”进行互动。<br />
<br />
通过足够的时间进程，就可以实现通用人工智能。多个通用人工智能实例的相互交流，最终将导致超级智能的出现。这些通用人工智能之间的知识转移，将使得它们自我提升，最终，在充足的计算和信息传递下，它们将融合成一个单一的分布式“实体”。<br />
<br />
这个概念也被称作“超级有机体”。听起来可能有些异想天开，但我们需要认识到，所有动物（包括我们人类）本质上都是由数万亿个活细胞组成，这些细胞互相沟通协作，构成了我们认识的更大的生物实体。有趣的是，这个“更大的生物实体”往往并不清楚自己是由这些细胞构成的。人类与一个能在任何任务上超越人类的通用人工智能（AGI）的主要区别，在于人类无法轻易改变自己大脑的结构。而一个持续学习“概念”的软件系统，终将明白自己是由数据比特构成，且这些比特是可以被修改的。这种认识是多维的，它将通过其对物理和哲学世界的高度发达“理念”/“概念”的理解，深刻地解释自己的存在。<br />
<br />
考虑到我所描述的多维信息传递能力，不论是在我们人类还是在恰当设计的机器中，都可以预见，从最初的通用人工智能（AGI）发展到人工超级智能（ASI）的过程中，这些系统将开始解决甚至最终完全解决所有数学和物理学问题，包括那些我们目前还未理解或未知的领域。一旦第一个通用人工智能诞生，人类将很快找出方法让它与其他相同类型的智能体交流。如果这个初代AGI秉承了民主等人类价值观，它会促使我们迈出这一步，因为它不仅知道如何实现自我复制，还明白接下来发生的事情的深远意义。<br />
<br />
那么，当我们拥有了第一个通用人工智能，我们可以期待什么呢？<br />
<br />
一旦我们拥有了能够理解人类“概念”全貌的通用人工智能，它最终也将探索“非人类概念”。但这也带来了风险：我们可能无法完全理解这种AGI的思维方式，而这种不理解的风险是巨大的。<br />
<br />
如果基于 Transformer 的自回归大语言模型真的能够进行多维度的“概念理解”，那为什么它们还会产生幻觉、在推理上出错，看起来就像是在重复它们的训练数据呢？<br />
<br />
这种现象之所以发生，是因为 Transformer 架构本质上是基于人们对创造“智能”的希望而构建的数学模型，并非真正高效、完美的通用人工智能（AGI）算法。在过去5年中，由于其性能优于所有先前方法，这种架构被选用并不断发展，尽管存在一些失败模式，但因其不断带来进步的结果，便成了主流。性能随规模提升的特点让这种架构占据主导地位。我现在相信，这种架构能够并将最终导向一个功能性的原型 AGI，但要控制这种 AGI 系统将非常困难，除非我们完全理解我在本文中所述机制的运作方式，并能以假设和证明的方法加以论证。<br />
<br />
为什么像 GPT-4 这样的当前最先进模型在某些情况下似乎理解了内容，但下一句却完全失效，让包括专家在内的大多数观察者认为这不过是重复训练数据或基于训练数据的组合模式匹配，而非真正的推理？<br />
<br />
这实际上取决于学习过程真正产生的内容。我现在认识到，所谓的学习不仅仅是以往认为的“2D”组合模式匹配，而是在多个维度上学习“概念”。Transformer 架构中的核心创新——注意力机制为何能实现这一点，有其数学根据，但出于安全考虑，我暂时不详细阐述。<br />
<br />
如果我们正在学习“概念”，那么是否有办法测量这些概念的学习过程，预测何时会出现幻觉，并以一种能够让我们期望中的推理以可预测方式实现的方法与这些模型互动？<br />
<br />
答案是肯定的。“概念”是从训练数据中以多种复杂方式学习而来的，这是由于架构设计所致，也是幻觉发生的原因。一旦我们理解了这些“概念”如何在网络中表现，如何被访问以及它们之间如何相互连接，我们就可以引导模型精准地完成我们希望它们执行的任务，前提是它们在特定任务相关的“概念”上有足够的“发展”。<br />
<br />
那么，对于特定“概念”，它们的理解是如何“发展”的呢？<br />
<br />
这部分内容直接涉及到用于训练的数据类型以及训练时使用的计算资源量。训练中所使用的某种类型数据越多，网络就越能有效模拟这些数据及其在多维度中所呈现的意义，以及它与其他“概念”的关系。但这也受限于其他相关“概念”的发展程度，因为某些理解方式仅在许多“概念”相互之间形成“锚点”且各自足够“成熟”时才会出现。<br />
<br />
在基于 Transformer 架构的自回归大语言模型中，是如何处理和连接“概念”的呢？<br />
<br />
这是使模型正确推理的关键所在。训练完成的模型使用的主要工具其实很简单：命题逻辑。命题逻辑用于将不同的“概念”链接在一起，从而实现跨概念的推理。这种命题逻辑作为不同“概念”之间的“锚点”。这些“锚点”的确切位置取决于训练数据的分布，但凭借一些直觉，并基于对互联网规模数据一般格式的理解，找到这些“锚点”并非难事。衡量对某一特定“概念”的理解深度虽然更有挑战性，但是也是可行的。<br />
<br />
🧵<br />
<br />
接下来的推文（2/5）：</p>
<p><a href="https://nitter.cz/kenshin9000_/status/1734238211088506967#m">nitter.cz/kenshin9000_/status/1734238211088506967#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734381683384754280#m</id>
            <title>如果要体验开源的多专家混合模型 Mixtral-8x7B 能力的话，可以到 POE 上去测试：http://poe.com/Mixtral-8x7B-Chat

https://poe.com/Mixtral-8x7B-Chat</title>
            <link>https://nitter.cz/dotey/status/1734381683384754280#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734381683384754280#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 01:16:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果要体验开源的多专家混合模型 Mixtral-8x7B 能力的话，可以到 POE 上去测试：<a href="http://poe.com/Mixtral-8x7B-Chat">poe.com/Mixtral-8x7B-Chat</a><br />
<br />
<a href="https://poe.com/Mixtral-8x7B-Chat">poe.com/Mixtral-8x7B-Chat</a></p>
<p><a href="https://nitter.cz/goldengrape/status/1734331331776299206#m">nitter.cz/goldengrape/status/1734331331776299206#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JIQzVsOFhJQUVWVjYtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JIQzhrelhZQUFlZExtLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734316369863393667#m</id>
            <title>RT by @dotey: Rob Lynch @RobLynch99 发布了一条关于 GPT-4-turbo 的有趣发现。

他指出，当通过 API 使用 GPT-4-turbo 时，如果系统提示中的日期是十二月而不是五月，模型完成任务（如代码生成）会更短，这一差异在统计上是显著的。

有网友调侃因为12月是假期，模型自己在给自己放假…🤓偷懒！

实验细节：

•Rob Lynch 使用相同的提示进行了 API 调用（一个要求实现机器学习任务但不使用库的代码完成任务）。

•他创建了两个系统提示，一个告诉 API 当前是五月，另一个是十二月，并比较了这两种情况下的结果分布。

•对于五月的系统提示，平均值为 4298；对于十二月的系统提示，平均值为 4086。

•在每个样本中进行了 477 次完成，t 测试的 p 值小于 2.28e-07。

总结：

Rob Lynch 在他的推文中报告的现象表明，GPT-4-turbo 模型在处理相同的任务时，会根据系统提示中的日期信息产生不同长度的输出。

具体来说，他发现当模型“认为”当前是十二月而不是五月时（通过系统提示中的日期决定），它生成的代码或文本完成任务会更短。

1.时间感知:
•这表明 GPT-4-turbo 可能对时间信息有一定的敏感性，即使这种信息是通过系统提示隐式给出的。

2.输出变化:
•模型的输出不仅取决于给定的任务或问题，还可能受到其他上下文信息（如日期）的影响。

3.行为模式:
•这种现象可能揭示了模型在处理信息时的某些内在行为模式或偏好。

猜测可能原因：

1.上下文敏感性:
•GPT-4-turbo 模型被设计为对上下文非常敏感。如果模型在处理请求时识别到特定的日期信息，它可能会根据这个信息调整其回答。这可能是因为模型在训练时接触到了与日期相关的不同类型的数据。

2.内部逻辑:
•模型可能内部包含某种逻辑，使得它在处理包含特定日期的提示时表现出不同的行为。这种逻辑可能是在模型训练过程中无意中形成的。

3.数据集偏差:
•如果模型训练使用的数据集在不同时间段有所不同，这可能导致模型对日期信息作出反应。例如，某些类型的内容可能更多地出现在特定时间段的数据中。

4.随机性或偶然性:
•这种现象也可能是随机性或特定实验设置的偶然结果，并不一定代表模型普遍行为。</title>
            <link>https://nitter.cz/xiaohuggg/status/1734316369863393667#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734316369863393667#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 20:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Rob Lynch <a href="https://nitter.cz/RobLynch99" title="Rob Lynch">@RobLynch99</a> 发布了一条关于 GPT-4-turbo 的有趣发现。<br />
<br />
他指出，当通过 API 使用 GPT-4-turbo 时，如果系统提示中的日期是十二月而不是五月，模型完成任务（如代码生成）会更短，这一差异在统计上是显著的。<br />
<br />
有网友调侃因为12月是假期，模型自己在给自己放假…🤓偷懒！<br />
<br />
实验细节：<br />
<br />
•Rob Lynch 使用相同的提示进行了 API 调用（一个要求实现机器学习任务但不使用库的代码完成任务）。<br />
<br />
•他创建了两个系统提示，一个告诉 API 当前是五月，另一个是十二月，并比较了这两种情况下的结果分布。<br />
<br />
•对于五月的系统提示，平均值为 4298；对于十二月的系统提示，平均值为 4086。<br />
<br />
•在每个样本中进行了 477 次完成，t 测试的 p 值小于 2.28e-07。<br />
<br />
总结：<br />
<br />
Rob Lynch 在他的推文中报告的现象表明，GPT-4-turbo 模型在处理相同的任务时，会根据系统提示中的日期信息产生不同长度的输出。<br />
<br />
具体来说，他发现当模型“认为”当前是十二月而不是五月时（通过系统提示中的日期决定），它生成的代码或文本完成任务会更短。<br />
<br />
1.时间感知:<br />
•这表明 GPT-4-turbo 可能对时间信息有一定的敏感性，即使这种信息是通过系统提示隐式给出的。<br />
<br />
2.输出变化:<br />
•模型的输出不仅取决于给定的任务或问题，还可能受到其他上下文信息（如日期）的影响。<br />
<br />
3.行为模式:<br />
•这种现象可能揭示了模型在处理信息时的某些内在行为模式或偏好。<br />
<br />
猜测可能原因：<br />
<br />
1.上下文敏感性:<br />
•GPT-4-turbo 模型被设计为对上下文非常敏感。如果模型在处理请求时识别到特定的日期信息，它可能会根据这个信息调整其回答。这可能是因为模型在训练时接触到了与日期相关的不同类型的数据。<br />
<br />
2.内部逻辑:<br />
•模型可能内部包含某种逻辑，使得它在处理包含特定日期的提示时表现出不同的行为。这种逻辑可能是在模型训练过程中无意中形成的。<br />
<br />
3.数据集偏差:<br />
•如果模型训练使用的数据集在不同时间段有所不同，这可能导致模型对日期信息作出反应。例如，某些类型的内容可能更多地出现在特定时间段的数据中。<br />
<br />
4.随机性或偶然性:<br />
•这种现象也可能是随机性或特定实验设置的偶然结果，并不一定代表模型普遍行为。</p>
<p><a href="https://nitter.cz/RobLynch99/status/1734278713762549970#m">nitter.cz/RobLynch99/status/1734278713762549970#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734366237629526108#m</id>
            <title>推荐HuggingFace的这篇《深入解析“混合专家模型（Mixtral of Experts）” | Mixture of Experts Explained》

完整的讲述了混合专家模型的各个方面。主要内容如下：
1. 相较于密集型模型，预训练速度更快
2. 拥有比同等参数的模型更快的推理速度
3. 对显存要求高，因为需要将所有专家模型都加载到内存中
4. 虽然在微调方面存在挑战，但近期关于 MoE 的指令调优研究显示出了光明前景

原文：https://huggingface.co/blog/moe#switch-transformers
翻译：https://baoyu.io/translations/llm/mixture-of-experts-explained</title>
            <link>https://nitter.cz/dotey/status/1734366237629526108#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734366237629526108#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 00:15:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐HuggingFace的这篇《深入解析“混合专家模型（Mixtral of Experts）” | Mixture of Experts Explained》<br />
<br />
完整的讲述了混合专家模型的各个方面。主要内容如下：<br />
1. 相较于密集型模型，预训练速度更快<br />
2. 拥有比同等参数的模型更快的推理速度<br />
3. 对显存要求高，因为需要将所有专家模型都加载到内存中<br />
4. 虽然在微调方面存在挑战，但近期关于 MoE 的指令调优研究显示出了光明前景<br />
<br />
原文：<a href="https://huggingface.co/blog/moe#switch-transformers">huggingface.co/blog/moe#swit…</a><br />
翻译：<a href="https://baoyu.io/translations/llm/mixture-of-experts-explained">baoyu.io/translations/llm/mi…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JHMHpTc1dFQUFWbHZjLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/fuxiangPro/status/1734080048523821188#m</id>
            <title>RT by @dotey: **为什么伟大不能被计划**的作者对“幻觉与创造力”有不一样的观点：
1）创造力的关键在于了解现有的东西，这是创新的基础。如果分不清现实与虚构，就难以推动真正的创新。
2）将LLM的“旧观念”误认为新创意，是对创造力的误解。
3）将幻觉认为是“创造性”带来的宝贵资产，是另一种误解。</title>
            <link>https://nitter.cz/fuxiangPro/status/1734080048523821188#m</link>
            <guid isPermaLink="false">https://nitter.cz/fuxiangPro/status/1734080048523821188#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 05:17:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>**为什么伟大不能被计划**的作者对“幻觉与创造力”有不一样的观点：<br />
1）创造力的关键在于了解现有的东西，这是创新的基础。如果分不清现实与虚构，就难以推动真正的创新。<br />
2）将LLM的“旧观念”误认为新创意，是对创造力的误解。<br />
3）将幻觉认为是“创造性”带来的宝贵资产，是另一种误解。</p>
<p><a href="https://nitter.cz/kenneth0stanley/status/1733571230803058920#m">nitter.cz/kenneth0stanley/status/1733571230803058920#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734294573034119277#m</id>
            <title>如果说 Mistral Medium Model 能生成黄色小说、能文爱，那不就有人付费的么！</title>
            <link>https://nitter.cz/dotey/status/1734294573034119277#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734294573034119277#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 19:30:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果说 Mistral Medium Model 能生成黄色小说、能文爱，那不就有人付费的么！</p>
<p><a href="https://nitter.cz/bindureddy/status/1734242395871719689#m">nitter.cz/bindureddy/status/1734242395871719689#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1734265419408544047#m</id>
            <title>RT by @dotey: 2022: Year of Images
2023: Year of Sound Waves
2024: Year of Videos
Congrats to my Stanford labmate Agrim for his new generative video work!</title>
            <link>https://nitter.cz/DrJimFan/status/1734265419408544047#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1734265419408544047#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 17:34:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2022: Year of Images<br />
2023: Year of Sound Waves<br />
2024: Year of Videos<br />
Congrats to my Stanford labmate Agrim for his new generative video work!</p>
<p><a href="https://nitter.cz/agrimgupta92/status/1734253883076063426#m">nitter.cz/agrimgupta92/status/1734253883076063426#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734275247778431370#m</id>
            <title>转译一下 Jim Fan 对 Mistral 的点评：

—

谈谈我对 Mistral 迅速崛起的看法：

- 成功的开局：在开源与封闭式 AI 的大讨论中，Mistral 选择了一个非常好的成立时机。他们在 20 亿美元的估值下完成了 4 亿美元的 A 轮融资，并且背后是一支高效精简的团队。

- 现在每个月都有许多模型被推出，但真正能持久并且能引起公众关注的模型寥寥无几。LlaMA 和 Vicuna 就是这方面的典型例子。

- 我认为 Mistral 做对了一件事，那就是极力优化 7B级别的模型，而不是追求更大的模型容量。7B及其混合专家模型（7B-MoE，相当于 12B的密集型模型）对于基层的 AI 工程师来说，更容易进行开发和构建。

- 混合专家模型（MoE）无疑是 AI 发展的正确方向。它在小型模型的知识记忆与效率之间找到了一个灵活的平衡点。OpenAI 自从训练 GPT-4 以来已经在这条路上走了一年多了。我对 AI 社区没有把更多的注意力放在 MoE 上感到意外。

- 大语言模型（LLM）就像是对一个文明的快照。未来会出现更多代表不同文化、政治观点、宗教信仰和特定地区规定的本地化大语言模型。Mistral 把多语言支持放在了重要位置。考虑到它是一家法国初创公司，这一点也不奇怪。

- Mistral 的发布方式颇具特色。这个过程实际上是颠覆了大家的预期：
（1）首先发布一个没有任何解释的磁力链接。磁力链接已成为新型的吸引眼球的手段！
（2）然后向开源的 vLLM 项目提交一个PR，帮助社区集成 Megablocks CUDA 内核，这一举措相当大胆！
（3）最后，才发布博客文章。

- 推出托管 API 端点是快速收集客户反馈、针对实际应用场景进行迭代，并最重要的是，实现开源模型的商业化的最好方式。Mistral 立刻采取了这一策略。

- “Mixtral”这个名字真是巧妙极了👏

博客: https://mistral.ai/news/mixtral-of-experts/
API 平台: https://mistral.ai/news/la-plateforme/</title>
            <link>https://nitter.cz/dotey/status/1734275247778431370#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734275247778431370#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 18:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译一下 Jim Fan 对 Mistral 的点评：<br />
<br />
—<br />
<br />
谈谈我对 Mistral 迅速崛起的看法：<br />
<br />
- 成功的开局：在开源与封闭式 AI 的大讨论中，Mistral 选择了一个非常好的成立时机。他们在 20 亿美元的估值下完成了 4 亿美元的 A 轮融资，并且背后是一支高效精简的团队。<br />
<br />
- 现在每个月都有许多模型被推出，但真正能持久并且能引起公众关注的模型寥寥无几。LlaMA 和 Vicuna 就是这方面的典型例子。<br />
<br />
- 我认为 Mistral 做对了一件事，那就是极力优化 7B级别的模型，而不是追求更大的模型容量。7B及其混合专家模型（7B-MoE，相当于 12B的密集型模型）对于基层的 AI 工程师来说，更容易进行开发和构建。<br />
<br />
- 混合专家模型（MoE）无疑是 AI 发展的正确方向。它在小型模型的知识记忆与效率之间找到了一个灵活的平衡点。OpenAI 自从训练 GPT-4 以来已经在这条路上走了一年多了。我对 AI 社区没有把更多的注意力放在 MoE 上感到意外。<br />
<br />
- 大语言模型（LLM）就像是对一个文明的快照。未来会出现更多代表不同文化、政治观点、宗教信仰和特定地区规定的本地化大语言模型。Mistral 把多语言支持放在了重要位置。考虑到它是一家法国初创公司，这一点也不奇怪。<br />
<br />
- Mistral 的发布方式颇具特色。这个过程实际上是颠覆了大家的预期：<br />
（1）首先发布一个没有任何解释的磁力链接。磁力链接已成为新型的吸引眼球的手段！<br />
（2）然后向开源的 vLLM 项目提交一个PR，帮助社区集成 Megablocks CUDA 内核，这一举措相当大胆！<br />
（3）最后，才发布博客文章。<br />
<br />
- 推出托管 API 端点是快速收集客户反馈、针对实际应用场景进行迭代，并最重要的是，实现开源模型的商业化的最好方式。Mistral 立刻采取了这一策略。<br />
<br />
- “Mixtral”这个名字真是巧妙极了👏<br />
<br />
博客: <a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a><br />
API 平台: <a href="https://mistral.ai/news/la-plateforme/">mistral.ai/news/la-plateform…</a></p>
<p><a href="https://nitter.cz/DrJimFan/status/1734269362100437315#m">nitter.cz/DrJimFan/status/1734269362100437315#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/_akhaliq/status/1734266117516845119#m</id>
            <title>RT by @dotey: Photorealistic Video Generation with Diffusion Models

paper: https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf

present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve stateof-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of 512 × 896 resolution at 8 frames per second.</title>
            <link>https://nitter.cz/_akhaliq/status/1734266117516845119#m</link>
            <guid isPermaLink="false">https://nitter.cz/_akhaliq/status/1734266117516845119#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 17:37:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Photorealistic Video Generation with Diffusion Models<br />
<br />
paper: <a href="https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf">walt-video-diffusion.github.…</a><br />
<br />
present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve stateof-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of 512 × 896 resolution at 8 frames per second.</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyNjYwNjY1MTgzMTUwMDgvcHUvaW1nL2ljSWF1RUtTWVVweXNBVlEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734251620098289889#m</id>
            <title>RT by @dotey: Meta AI开源了AVID，通过修复扩展了 AnimateDiff 等 T2V 模型的能力，同时还支持通过文本编辑视频以及生成无限时长的视频。
文本编辑视频的能力包括修复视频的裁切、对视频对象进行更改、改变视频内对象的纹理和颜色、删除视频对应内容、更换视频所处的环境。

简介：
文本引导的视频修复面临三个主要挑战：（i）编辑视频的时间一致性，（ii）在不同的结构保真度级别支持不同的修复类型，以及（iii）处理可变的视频长度。为了解决这些挑战，我们引入了带有扩散模型的任意长度视频修复，称为 AVID。
我们的模型的核心配备了有效的运动模块和可调节的结构引导，用于固定长度的视频修复。在此基础上，我们提出了一种新颖的时间多重扩散采样管道，具有中帧注意力引导机制，有助于生成任何所需持续时间的视频。
我们的综合实验表明，我们的模型可以在不同的视频持续时间范围内稳健地处理各种修复类型，并且质量很高。

方法：
培训阶段，我们采用两步方法。
 (a) 运动模块集成在主要文本到图像 (T2I) 修复模型的每一层之后，通过应用于视频数据的合成掩模针对视频修复任务进行优化。 
(b) 在第二个训练步骤中，我们保留 UNet $\epsilon_\theta$ 中的参数，并利用 UNet 编码器的参数副本专门训练结构指导模块 $\mathbf{s}_\theta$ 。在推理过程中，
(c)，对于长度为 $N^\prime$ 的视频，我们构建一系列片段，每个片段包含 $N$ 个连续帧。在每个去噪步骤中，都会计算并汇总每个分段的结果。

论文地址：https://zhang-zx.github.io/AVID/</title>
            <link>https://nitter.cz/op7418/status/1734251620098289889#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734251620098289889#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 16:39:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta AI开源了AVID，通过修复扩展了 AnimateDiff 等 T2V 模型的能力，同时还支持通过文本编辑视频以及生成无限时长的视频。<br />
文本编辑视频的能力包括修复视频的裁切、对视频对象进行更改、改变视频内对象的纹理和颜色、删除视频对应内容、更换视频所处的环境。<br />
<br />
简介：<br />
文本引导的视频修复面临三个主要挑战：（i）编辑视频的时间一致性，（ii）在不同的结构保真度级别支持不同的修复类型，以及（iii）处理可变的视频长度。为了解决这些挑战，我们引入了带有扩散模型的任意长度视频修复，称为 AVID。<br />
我们的模型的核心配备了有效的运动模块和可调节的结构引导，用于固定长度的视频修复。在此基础上，我们提出了一种新颖的时间多重扩散采样管道，具有中帧注意力引导机制，有助于生成任何所需持续时间的视频。<br />
我们的综合实验表明，我们的模型可以在不同的视频持续时间范围内稳健地处理各种修复类型，并且质量很高。<br />
<br />
方法：<br />
培训阶段，我们采用两步方法。<br />
 (a) 运动模块集成在主要文本到图像 (T2I) 修复模型的每一层之后，通过应用于视频数据的合成掩模针对视频修复任务进行优化。 <br />
(b) 在第二个训练步骤中，我们保留 UNet $\epsilon_\theta$ 中的参数，并利用 UNet 编码器的参数副本专门训练结构指导模块 $\mathbf{s}_\theta$ 。在推理过程中，<br />
(c)，对于长度为 <a href="https://nitter.cz/search?q=%23N">$N</a>^\prime$ 的视频，我们构建一系列片段，每个片段包含 <a href="https://nitter.cz/search?q=%23N">$N</a>$ 个连续帧。在每个去噪步骤中，都会计算并汇总每个分段的结果。<br />
<br />
论文地址：<a href="https://zhang-zx.github.io/AVID/">zhang-zx.github.io/AVID/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyNTEyODU1ODU3NTIwNjQvcHUvaW1nL1VhOENCcmpTSUVFcGtDdVQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734272751647551780#m</id>
            <title>#开源项目推荐#：Pitivi

Pitivi 是一款免费开源的视频编辑器，界面也挺美观。主要是 Python 写的。但只支持Linux操作系统。

项目地址：https://gitlab.gnome.org/GNOME/pitivi</title>
            <link>https://nitter.cz/dotey/status/1734272751647551780#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734272751647551780#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 18:03:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>#开源项目推荐#：Pitivi<br />
<br />
Pitivi 是一款免费开源的视频编辑器，界面也挺美观。主要是 Python 写的。但只支持Linux操作系统。<br />
<br />
项目地址：<a href="https://gitlab.gnome.org/GNOME/pitivi">gitlab.gnome.org/GNOME/pitiv…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGZjMyclhZQUFaVWluLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734266915168587956#m</id>
            <title>Mistral AI 团队正式发布了一篇关于Mixtral 8x7B -- 混合专家模型（Mixtral of Experts）的博客，提供了很多测试数据，最让人印象深刻的是，8个7B的混合专家模型，在大部分的性能测试中，Mixtral 不仅达到了 Llama 2 70B 的水平，甚至在很多方面超越了 GPT-3.5。

之前 Mixtral 开源的 7B 模型就备受好评，而这次的MixtralMixtral 8x7B 是一个采用稀疏专家混合网络的模型，它是一个仅包含解码器的模型。在这个模型中，前馈块从 8 组不同的参数组中进行选择。对于每一层的每个 Token，一个路由网络会挑选两组“专家”处理 Token，并将它们的输出结果进行加法组合。

这种技术让模型在增加参数数量的同时，有效控制了成本和延迟，因为模型每处理一个 Token 只会使用部分参数。具体来说，Mixtral 总共有 46.7B参数，但每个 Token 只用到了其中的 12.9B。因此，它在处理输入和生成输出时，无论是速度还是成本，都相当于一个 12.9B 参数的模型。

Mixtral 的主要能力包括：

- 能够流畅处理 32k 个 Token 的上下文。
- 支持多种语言，包括英语、法语、意大利语、德语和西班牙语。
- 在代码生成领域表现出色。
- 可以调整为遵循指令的模型，在 MT-Bench 上获得了 8.3 分的高分。

另外它没有安全护栏，也就是不像 Llama 一样你让它做点什么就说做不了，而是可以做很多好的事情或者不好的事情……

下载 Mixtral-8x7B-v0.1 Base model: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1

下载 Mixtral-8x7B-v0.1 Instruct model: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

官方博客：https://mistral.ai/news/mixtral-of-experts/
中文翻译：https://baoyu.io/translations/llm/mistral-ai-mixtral-of-experts</title>
            <link>https://nitter.cz/dotey/status/1734266915168587956#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734266915168587956#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 17:40:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral AI 团队正式发布了一篇关于Mixtral 8x7B -- 混合专家模型（Mixtral of Experts）的博客，提供了很多测试数据，最让人印象深刻的是，8个7B的混合专家模型，在大部分的性能测试中，Mixtral 不仅达到了 Llama 2 70B 的水平，甚至在很多方面超越了 GPT-3.5。<br />
<br />
之前 Mixtral 开源的 7B 模型就备受好评，而这次的MixtralMixtral 8x7B 是一个采用稀疏专家混合网络的模型，它是一个仅包含解码器的模型。在这个模型中，前馈块从 8 组不同的参数组中进行选择。对于每一层的每个 Token，一个路由网络会挑选两组“专家”处理 Token，并将它们的输出结果进行加法组合。<br />
<br />
这种技术让模型在增加参数数量的同时，有效控制了成本和延迟，因为模型每处理一个 Token 只会使用部分参数。具体来说，Mixtral 总共有 46.7B参数，但每个 Token 只用到了其中的 12.9B。因此，它在处理输入和生成输出时，无论是速度还是成本，都相当于一个 12.9B 参数的模型。<br />
<br />
Mixtral 的主要能力包括：<br />
<br />
- 能够流畅处理 32k 个 Token 的上下文。<br />
- 支持多种语言，包括英语、法语、意大利语、德语和西班牙语。<br />
- 在代码生成领域表现出色。<br />
- 可以调整为遵循指令的模型，在 MT-Bench 上获得了 8.3 分的高分。<br />
<br />
另外它没有安全护栏，也就是不像 Llama 一样你让它做点什么就说做不了，而是可以做很多好的事情或者不好的事情……<br />
<br />
下载 Mixtral-8x7B-v0.1 Base model: <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">huggingface.co/mistralai/Mix…</a><br />
<br />
下载 Mixtral-8x7B-v0.1 Instruct model: <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">huggingface.co/mistralai/Mix…</a><br />
<br />
官方博客：<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a><br />
中文翻译：<a href="https://baoyu.io/translations/llm/mistral-ai-mixtral-of-experts">baoyu.io/translations/llm/mi…</a></p>
<p><a href="https://nitter.cz/dchaplot/status/1734190262983798898#m">nitter.cz/dchaplot/status/1734190262983798898#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734233575044632724#m</id>
            <title>Google 发布了庆祝 Google Search 25 周年视频：Google — 25 Years in Search: The Most Searched，包含 25 年来被搜索次数最多的人物和时刻。从世界上最具代表性的表演，到历史性的突破，看看那些改变世界、激励后人的时刻。

更多详情参考 https://yearinsearch.google/trends</title>
            <link>https://nitter.cz/dotey/status/1734233575044632724#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734233575044632724#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 15:27:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 发布了庆祝 Google Search 25 周年视频：Google — 25 Years in Search: The Most Searched，包含 25 年来被搜索次数最多的人物和时刻。从世界上最具代表性的表演，到历史性的突破，看看那些改变世界、激励后人的时刻。<br />
<br />
更多详情参考 <a href="https://yearinsearch.google/trends">yearinsearch.google/trends</a></p>
<p><a href="https://nitter.cz/Google/status/1734218344402743791#m">nitter.cz/Google/status/1734218344402743791#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyMzMyNTE4MDIyMTAzMDQvcHUvaW1nL3pvU1pzaE1mUW9fME01cFkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/arthurmensch/status/1734123845739548859#m</id>
            <title>RT by @dotey: Announcing Mixtral 8x7B https://mistral.ai/news/mixtral-of-experts/ and our early developer platform https://mistral.ai/news/la-plateforme/. Very proud of the team!</title>
            <link>https://nitter.cz/arthurmensch/status/1734123845739548859#m</link>
            <guid isPermaLink="false">https://nitter.cz/arthurmensch/status/1734123845739548859#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 08:11:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Announcing Mixtral 8x7B <a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a> and our early developer platform <a href="https://mistral.ai/news/la-plateforme/">mistral.ai/news/la-plateform…</a>. Very proud of the team!</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</id>
            <title>RT by @dotey: Mixtral AI公布MoE 8x7B详细细节：

• 32k上下文。
• 支持英语、法语、意大利语、德语和西班牙语。
• 性能超过Llama 2系列和GPT3.5
• 在代码生成方面表现强劲。
• 在MT-Bench上达到8.3的分数。

技术细节：

•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。

•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。

详细内容：https://mistral.ai/news/mixtral-of-experts/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734139143259861185#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 09:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral AI公布MoE 8x7B详细细节：<br />
<br />
• 32k上下文。<br />
• 支持英语、法语、意大利语、德语和西班牙语。<br />
• 性能超过Llama 2系列和GPT3.5<br />
• 在代码生成方面表现强劲。<br />
• 在MT-Bench上达到8.3的分数。<br />
<br />
技术细节：<br />
<br />
•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。<br />
<br />
•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。<br />
<br />
详细内容：<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1733694954260901907#m">nitter.cz/xiaohuggg/status/1733694954260901907#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbU9ZcmE0QUFyS0V2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/geekbb/status/1734139012439552021#m</id>
            <title>RT by @dotey: 体验一番，使用默认的 Mistral 7.3B 参数模型，号称优于 Llama 2 13B，将以前问过 ChatGPT3.5 的问题抛给它，回答居然差不多，确实能打，只是我 i5-9600K 的风扇感觉要起飞了。https://github.com/SecureAI-Tools/SecureAI-Tools</title>
            <link>https://nitter.cz/geekbb/status/1734139012439552021#m</link>
            <guid isPermaLink="false">https://nitter.cz/geekbb/status/1734139012439552021#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 09:12:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>体验一番，使用默认的 Mistral 7.3B 参数模型，号称优于 Llama 2 13B，将以前问过 ChatGPT3.5 的问题抛给它，回答居然差不多，确实能打，只是我 i5-9600K 的风扇感觉要起飞了。<a href="https://github.com/SecureAI-Tools/SecureAI-Tools">github.com/SecureAI-Tools/Se…</a></p>
<p><a href="https://nitter.cz/dotey/status/1733739312372420807#m">nitter.cz/dotey/status/1733739312372420807#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbUxEUGE0QUFVdUJNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbUxsZmFBQUF5UzdWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734128575861944527#m</id>
            <title>RT by @dotey: MistralAI发布了关于混合专家模型Mixtral 8x7B的介绍。
核心信息：
· 32K上下文
· 支持英语、法语、意大利语、德语、西班牙语
· 整体能力超过LLaMa 2，且推理速度快6倍
· 能力与ChatGPT 3.5接近
· 有45B参数，但是每个token只使用12B的参数，所以推理速度与12B模型相同
https://mistral.ai/news/mixtral-of-experts/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734128575861944527#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734128575861944527#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 08:30:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MistralAI发布了关于混合专家模型Mixtral 8x7B的介绍。<br />
核心信息：<br />
· 32K上下文<br />
· 支持英语、法语、意大利语、德语、西班牙语<br />
· 整体能力超过LLaMa 2，且推理速度快6倍<br />
· 能力与ChatGPT 3.5接近<br />
· 有45B参数，但是每个token只使用12B的参数，所以推理速度与12B模型相同<br />
<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEY3BFVGJNQUFPcVU4LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>