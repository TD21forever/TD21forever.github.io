<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737601108212691370#m</id>
            <title>简要总结一下OpenAI这条推文：
American Journalism Project (@JournalismProj) 是一个专注于地方新闻的风险投资慈善机构。OpenAI 通过与其合作，来试验和探索各种 AI 在地方新闻事业中的应用。

具体来说：

*   **Centro de Periodismo Investigativo**（波多黎各）将启动一个试点项目，测试 AI 在西班牙语与英语互译方面的能力。CPI 也将与语言专家合作进行质量控制。他们的目标是更高效地制作更多双语报道，增强他们与加勒比海和美国的英语合作伙伴进行调查合作的能力，并扩大对波多黎各调查新闻的支持，为美国本土的波多黎各人提供服务。

*   **THE CITY**（纽约）将尝试使用以 AI 为驱动的工具，专注于提高受众参与度，帮助他们筛选在线信息，回答纽约人的问题，并接收读者的建议。他们还将探索让记者无需编码知识也能使用 AI 分析数据的方法。THE CITY 还将研究 AI 如何帮助他们与提供报道线索的读者互动，通过 AI 生成的初始问题激励读者提交更完整、详细的信息，这将使新闻室能够将时间和资源集中在处理线索的更复杂阶段。

*   **inewsource**（圣地亚哥）将尝试使用 AI 技术和工具更快地处理更多公共记录请求，覆盖更多公共机构。他们还将探索 AI 如何帮助分析这些请求所产生的大量文件。

*   **Outlier**（底特律）使用 SMS 平台作为其服务型新闻传播的核心。除了发布 Outlier 的作品，该平台还使其能够与底特律居民进行一对一交流，这些交流通常有助于设定 Outler 的编辑策略，方法是回答居民的问题。Outlier 将尝试将 AI 技术融入其平台和工作流程，以便吸引更多人，更好地了解受众，并与服务的社区建立更深入、更有价值的直接关系。

*   **Cityside**（加利福尼亚湾区）将评估和实施与收入相关的 AI 实验，目的是学习如何在新闻机构的商业侧使用 AI。Cityside 将优先考虑使用 AI 辅助的通信来发展不同捐赠层级上的个人捐赠者关系。

很有意思的尝试！</title>
            <link>https://nitter.cz/dotey/status/1737601108212691370#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737601108212691370#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 22:29:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>简要总结一下OpenAI这条推文：<br />
American Journalism Project (<a href="https://nitter.cz/JournalismProj" title="American Journalism Project">@JournalismProj</a>) 是一个专注于地方新闻的风险投资慈善机构。OpenAI 通过与其合作，来试验和探索各种 AI 在地方新闻事业中的应用。<br />
<br />
具体来说：<br />
<br />
*   **Centro de Periodismo Investigativo**（波多黎各）将启动一个试点项目，测试 AI 在西班牙语与英语互译方面的能力。CPI 也将与语言专家合作进行质量控制。他们的目标是更高效地制作更多双语报道，增强他们与加勒比海和美国的英语合作伙伴进行调查合作的能力，并扩大对波多黎各调查新闻的支持，为美国本土的波多黎各人提供服务。<br />
<br />
*   **THE CITY**（纽约）将尝试使用以 AI 为驱动的工具，专注于提高受众参与度，帮助他们筛选在线信息，回答纽约人的问题，并接收读者的建议。他们还将探索让记者无需编码知识也能使用 AI 分析数据的方法。THE CITY 还将研究 AI 如何帮助他们与提供报道线索的读者互动，通过 AI 生成的初始问题激励读者提交更完整、详细的信息，这将使新闻室能够将时间和资源集中在处理线索的更复杂阶段。<br />
<br />
*   **inewsource**（圣地亚哥）将尝试使用 AI 技术和工具更快地处理更多公共记录请求，覆盖更多公共机构。他们还将探索 AI 如何帮助分析这些请求所产生的大量文件。<br />
<br />
*   **Outlier**（底特律）使用 SMS 平台作为其服务型新闻传播的核心。除了发布 Outlier 的作品，该平台还使其能够与底特律居民进行一对一交流，这些交流通常有助于设定 Outler 的编辑策略，方法是回答居民的问题。Outlier 将尝试将 AI 技术融入其平台和工作流程，以便吸引更多人，更好地了解受众，并与服务的社区建立更深入、更有价值的直接关系。<br />
<br />
*   **Cityside**（加利福尼亚湾区）将评估和实施与收入相关的 AI 实验，目的是学习如何在新闻机构的商业侧使用 AI。Cityside 将优先考虑使用 AI 辅助的通信来发展不同捐赠层级上的个人捐赠者关系。<br />
<br />
很有意思的尝试！</p>
<p><a href="https://nitter.cz/OpenAI/status/1737577619636596885#m">nitter.cz/OpenAI/status/1737577619636596885#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Lakr233/status/1737483448288170454#m</id>
            <title>RT by @dotey: 研究了一下他这个手势是怎么识别的，感觉我已经完全弄懂了！#Swift Tip</title>
            <link>https://nitter.cz/Lakr233/status/1737483448288170454#m</link>
            <guid isPermaLink="false">https://nitter.cz/Lakr233/status/1737483448288170454#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 14:41:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>研究了一下他这个手势是怎么识别的，感觉我已经完全弄懂了！<a href="https://nitter.cz/search?q=%23Swift">#Swift</a> Tip</p>
<p><a href="https://nitter.cz/s1ntone/status/1737418763925123086#m">nitter.cz/s1ntone/status/1737418763925123086#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc0ODM0MjYzMjI2Nzc3NjAvcHUvaW1nL0RWcll5aFh0UlZnMmdWWDIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737578011426205827#m</id>
            <title>Anthropic 真是自己作，过于追求安全对齐，而不关心开发者和用户感受……</title>
            <link>https://nitter.cz/dotey/status/1737578011426205827#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737578011426205827#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 20:57:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic 真是自己作，过于追求安全对齐，而不关心开发者和用户感受……</p>
<p><a href="https://nitter.cz/bindureddy/status/1737561873216938110#m">nitter.cz/bindureddy/status/1737561873216938110#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737562311420682495#m</id>
            <title>R to @dotey: 除了Phi-2，这里还有更多模型可以测试，Mistral-7B，Mistral-8x7B（如果配置够好）都值得试试

https://x.com/reach_vb/status/1737559520866636061?s=20</title>
            <link>https://nitter.cz/dotey/status/1737562311420682495#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737562311420682495#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:55:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>除了Phi-2，这里还有更多模型可以测试，Mistral-7B，Mistral-8x7B（如果配置够好）都值得试试<br />
<br />
<a href="https://x.com/reach_vb/status/1737559520866636061?s=20">x.com/reach_vb/status/173755…</a></p>
<p><a href="https://nitter.cz/reach_vb/status/1737559520866636061#m">nitter.cz/reach_vb/status/1737559520866636061#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737561925016330649#m</id>
            <title>不知道，帮转</title>
            <link>https://nitter.cz/dotey/status/1737561925016330649#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737561925016330649#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:53:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不知道，帮转</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737501193058877916#m">nitter.cz/xiaohuggg/status/1737501193058877916#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737429065949479003#m</id>
            <title>RT by @dotey: 还能这么玩，自己雕刻了一个低分辨率模型，然后利用Magnific AI一直放大，实现了前几天类似谷歌那个无限放大项目的效果。</title>
            <link>https://nitter.cz/op7418/status/1737429065949479003#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737429065949479003#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 11:05:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还能这么玩，自己雕刻了一个低分辨率模型，然后利用Magnific AI一直放大，实现了前几天类似谷歌那个无限放大项目的效果。</p>
<p><a href="https://nitter.cz/MartinNebelong/status/1737404691581898826#m">nitter.cz/MartinNebelong/status/1737404691581898826#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737549740592488871#m</id>
            <title>在 Mac 上借助 MLX 运行微软的小语言模型 Phi-2 的教程，很详细。
Phi-2 对资源要求不高，能力也还不错（详见：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/），有兴趣的可以试试。</title>
            <link>https://nitter.cz/dotey/status/1737549740592488871#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737549740592488871#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 19:05:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在 Mac 上借助 MLX 运行微软的小语言模型 Phi-2 的教程，很详细。<br />
Phi-2 对资源要求不高，能力也还不错（详见：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a>），有兴趣的可以试试。</p>
<p><a href="https://nitter.cz/reach_vb/status/1737541383399895360#m">nitter.cz/reach_vb/status/1737541383399895360#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNzEyNTkyNzY0ODExNjczNi9qMF9KSkZjeD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737492846041530578#m</id>
            <title>RT by @dotey: 刚看到苹果发的这个论文《使用有限的内存实现更快的 LLM 推理》。通过将将模型参数保存在闪存里，根据需要移动到DRAM。

使得能够运行的模型大小是可用DRAM的两倍，与传统的CPU和GPU加载方法相比，推理速度分别提高了4-5倍和20-25倍。</title>
            <link>https://nitter.cz/op7418/status/1737492846041530578#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737492846041530578#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 15:19:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚看到苹果发的这个论文《使用有限的内存实现更快的 LLM 推理》。通过将将模型参数保存在闪存里，根据需要移动到DRAM。<br />
<br />
使得能够运行的模型大小是可用DRAM的两倍，与传统的CPU和GPU加载方法相比，推理速度分别提高了4-5倍和20-25倍。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1737300118070534468#m">nitter.cz/_akhaliq/status/1737300118070534468#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</id>
            <title>RT by @dotey: XHS-Downloader：小红书采集器 

✅ 采集小红书图文/视频作品信息
✅ 提取小红书图文/视频作品下载地址
✅ 下载小红书无水印图文/视频作品文件
✅ 自动跳过已下载的作品文件
✅ 作品文件完整性处理机制
✅ 持久化储存作品信息至文件

GitHub：https://github.com/JoeanAmier/XHS-Downloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737460981331288421#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XHS-Downloader：小红书采集器 <br />
<br />
✅ 采集小红书图文/视频作品信息<br />
✅ 提取小红书图文/视频作品下载地址<br />
✅ 下载小红书无水印图文/视频作品文件<br />
✅ 自动跳过已下载的作品文件<br />
✅ 作品文件完整性处理机制<br />
✅ 持久化储存作品信息至文件<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/XHS-Downloader">github.com/JoeanAmier/XHS-Do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5elhIR2JVQUFjb2hQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737518777481535521#m</id>
            <title>RT by @dotey: ChatGPT聊天记录可以归档了，再也不用看着一堆没用碍眼的聊天记录了。

点击想归档的聊天记录右侧三个点，选择Archive Chat就可以了，已经归档的内容可以在设置中查看。</title>
            <link>https://nitter.cz/op7418/status/1737518777481535521#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737518777481535521#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 17:02:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT聊天记录可以归档了，再也不用看着一堆没用碍眼的聊天记录了。<br />
<br />
点击想归档的聊天记录右侧三个点，选择Archive Chat就可以了，已经归档的内容可以在设置中查看。</p>
<p><a href="https://nitter.cz/OpenAI/status/1737517702766633063#m">nitter.cz/OpenAI/status/1737517702766633063#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc1MTgzMzk3OTc0NTQ4NDgvcHUvaW1nL2Rjakt4NG80WnBUTzE1TlUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1737459317102747701#m</id>
            <title>RT by @dotey: 强烈推荐这本在线免费的电子书，《动手学深度学习》，https://zh.d2l.ai，上线一年多时间，已经更新到了第二版，光看作者阵容就已经十分强大了，这本书也被上百所名校列为教材或参考书，当前也出版了实体书。

本书的每个章节都是可以直接运行的 Jupyter 记事本，你可以在本地直接跑，也可以克隆到 Google Colab 在云端跑；讲解的时候，不仅结合文字、公式和图示来阐明深度学习里常用的模型和算法，还提供代码来演示如何从零开始实现它们，并使用真实数据来提供一个交互式的学习体验。

第二版的内容是 2023 年更新的，手把手教你搭建 Bert/Transformer，各种语言框架都有，包括 Pytorch/Tensorflow/JAX 等，而且还支持中英文对照，英文域名是 https://d2l.ai，它对于初学者和有经验的深度学习从业者都是一份宝贵的资源。</title>
            <link>https://nitter.cz/Barret_China/status/1737459317102747701#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1737459317102747701#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:05:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>强烈推荐这本在线免费的电子书，《动手学深度学习》，<a href="https://zh.d2l.ai">zh.d2l.ai</a>，上线一年多时间，已经更新到了第二版，光看作者阵容就已经十分强大了，这本书也被上百所名校列为教材或参考书，当前也出版了实体书。<br />
<br />
本书的每个章节都是可以直接运行的 Jupyter 记事本，你可以在本地直接跑，也可以克隆到 Google Colab 在云端跑；讲解的时候，不仅结合文字、公式和图示来阐明深度学习里常用的模型和算法，还提供代码来演示如何从零开始实现它们，并使用真实数据来提供一个交互式的学习体验。<br />
<br />
第二版的内容是 2023 年更新的，手把手教你搭建 Bert/Transformer，各种语言框架都有，包括 Pytorch/Tensorflow/JAX 等，而且还支持中英文对照，英文域名是 <a href="https://d2l.ai">d2l.ai</a>，它对于初学者和有经验的深度学习从业者都是一份宝贵的资源。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5cnZOZ2F3QUFIblJwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737518177322475655#m</id>
            <title>推荐阅读：《深入了解大语言模型运维 (LLMOps) [译]》

这篇文章 5 月份的，但并没有过时，对于大语言模型的运维(LLMOps)讲的非常系统。

随着大语言模型的普及，未来的 Ops 肯定离不开 LLMOps ，甚至于需要专门的团队做 LLMOps。

文章中把 LLMOps 分成了几个关键步骤：

第 1 步：选择基础模型
是商业模型还是开源模型，亦或是混合使用

第 2 步：适应下游任务
LLM 的生成结果不像传统的服务，它的结果是不确定的，怎么让 LLM 生成你期望的结果？要不要微调？要不要使用 RAG？

第 3 步：评估
如何评估性能？由于 LLM 生成结果的不确定性，每次微调或者调整 Prompt 后，性能的变化需要可以量化的评估。

这部分可以配合《用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]》 https://baoyu.io/translations/rag/evaluating-rag-applications-with-ragas 这篇一起看

第 4 步：部署和监控
和传统运维一样，对于 LLM 的线上的部署和监控也是必不可少的，但是又不太一样，外部 API 需要监控 API 可用性，故障了还要考虑能切换到其他 API。

以上就是主要的几个步骤，可以帮助你系统的了解 LLMOps，但是文章都没有深入展开，最好是配合 OpenAI 官方的 《OpenAI 生产环境最佳实践官方指南 [译]》https://baoyu.io/translations/openai/guides-production-best-practices 一起阅读。

LLMOps 还是个很新也是个很有前途的领域，我个人对这方面也不专业，如果你有相关经验欢迎分享，或者有好的文章视频也欢迎推荐。

原文：https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations#Why%20the%20Rise%20of%20LLMOps
翻译：https://baoyu.io/translations/llm/understanding-llmops-large-language-model-operations</title>
            <link>https://nitter.cz/dotey/status/1737518177322475655#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737518177322475655#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 16:59:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《深入了解大语言模型运维 (LLMOps) [译]》<br />
<br />
这篇文章 5 月份的，但并没有过时，对于大语言模型的运维(LLMOps)讲的非常系统。<br />
<br />
随着大语言模型的普及，未来的 Ops 肯定离不开 LLMOps ，甚至于需要专门的团队做 LLMOps。<br />
<br />
文章中把 LLMOps 分成了几个关键步骤：<br />
<br />
第 1 步：选择基础模型<br />
是商业模型还是开源模型，亦或是混合使用<br />
<br />
第 2 步：适应下游任务<br />
LLM 的生成结果不像传统的服务，它的结果是不确定的，怎么让 LLM 生成你期望的结果？要不要微调？要不要使用 RAG？<br />
<br />
第 3 步：评估<br />
如何评估性能？由于 LLM 生成结果的不确定性，每次微调或者调整 Prompt 后，性能的变化需要可以量化的评估。<br />
<br />
这部分可以配合《用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]》 <a href="https://baoyu.io/translations/rag/evaluating-rag-applications-with-ragas">baoyu.io/translations/rag/ev…</a> 这篇一起看<br />
<br />
第 4 步：部署和监控<br />
和传统运维一样，对于 LLM 的线上的部署和监控也是必不可少的，但是又不太一样，外部 API 需要监控 API 可用性，故障了还要考虑能切换到其他 API。<br />
<br />
以上就是主要的几个步骤，可以帮助你系统的了解 LLMOps，但是文章都没有深入展开，最好是配合 OpenAI 官方的 《OpenAI 生产环境最佳实践官方指南 [译]》<a href="https://baoyu.io/translations/openai/guides-production-best-practices">baoyu.io/translations/openai…</a> 一起阅读。<br />
<br />
LLMOps 还是个很新也是个很有前途的领域，我个人对这方面也不专业，如果你有相关经验欢迎分享，或者有好的文章视频也欢迎推荐。<br />
<br />
原文：<a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations#Why%20the%20Rise%20of%20LLMOps">wandb.ai/site/articles/under…</a><br />
翻译：<a href="https://baoyu.io/translations/llm/understanding-llmops-large-language-model-operations">baoyu.io/translations/llm/un…</a></p>
<p><a href="https://nitter.cz/dotey/status/1736476171385069941#m">nitter.cz/dotey/status/1736476171385069941#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J6ai1RdVdVQUExeDAyLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737512582057992391#m</id>
            <title>R to @dotey: PopAi 官网链接：https://bit.ly/3Ty8G2Z （官方抽3个注册用户送3个月的会员）
使用折扣码可以有 20% 优惠：BAOYU</title>
            <link>https://nitter.cz/dotey/status/1737512582057992391#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737512582057992391#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 16:37:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PopAi 官网链接：<a href="https://bit.ly/3Ty8G2Z">bit.ly/3Ty8G2Z</a> （官方抽3个注册用户送3个月的会员）<br />
使用折扣码可以有 20% 优惠：BAOYU</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737512580690649455#m</id>
            <title>R to @dotey: 我拿我的翻译 Prompt （PopAi 中已经内置成了模板，并且支持多种语言翻译）实际测试后效果挺不错，这个产品免费的每天有条数限制，但 Unlimited 档的话可以无限使用 GPT-4，不用担心受 ChatGPT Plus 3 小时 40 条的限制了。</title>
            <link>https://nitter.cz/dotey/status/1737512580690649455#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737512580690649455#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 16:37:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我拿我的翻译 Prompt （PopAi 中已经内置成了模板，并且支持多种语言翻译）实际测试后效果挺不错，这个产品免费的每天有条数限制，但 Unlimited 档的话可以无限使用 GPT-4，不用担心受 ChatGPT Plus 3 小时 40 条的限制了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737512577641390311#m</id>
            <title>之前我发翻译的 Prompt 的或者 GPTs 时候，很多同学无法注册 ChatGPT Plus 或无法获取到 GPT-4 的 API，没机会用上，所以这里推荐一个 ChatGPT 的替代产品 PopAi @popaiinone，内置有 GPT-4V 多模态模型，所以不仅是能用上高质量的翻译能力，还能支持图像识别，PDF 文档对话等能力。</title>
            <link>https://nitter.cz/dotey/status/1737512577641390311#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737512577641390311#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 16:37:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前我发翻译的 Prompt 的或者 GPTs 时候，很多同学无法注册 ChatGPT Plus 或无法获取到 GPT-4 的 API，没机会用上，所以这里推荐一个 ChatGPT 的替代产品 PopAi <a href="https://nitter.cz/popaiinone" title="PopAi">@popaiinone</a>，内置有 GPT-4V 多模态模型，所以不仅是能用上高质量的翻译能力，还能支持图像识别，PDF 文档对话等能力。</p>
<p><a href="https://nitter.cz/dotey/status/1707478347553395105#m">nitter.cz/dotey/status/1707478347553395105#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J6aVFBLVdvQjhENXR4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J6aVFBLVc4QUFreU1DLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J6aVFBX1dvQUFvdVlxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J6aVRieVdvQVlob1RLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1737398651830276156#m</id>
            <title>RT by @dotey: 我初步读了一下论文，这篇论文的基本思路其实是利用了推理过程中的局部性。现在现在推理性能的一个瓶颈就是GPU的内存。它们的思路就是联合CPU和GPU做联合推理。尽可能把active的neuron 信息load到GPU中，充分利用局部性。这样大大提高了GPU推理的效率。
你不能拿单独的CPU推理或者单独的GPU推理来比。两者的指导思想都不一样。
这样的效果非常牛逼。

在推理速度上，在 4090 上是未经优化的llama.cpp 11 倍。相当于用一块4090（2000刀左右）在推理上取得了比a100 （2万刀左右）仅仅慢18%的成绩。

这个结果，更大的意义就是证明，充分考虑并利用推理的局部性有极大的潜力，CPU+GPU 联合推理有很强的优越性。
类似的，那么在训练当面是不是也可以采用这种思路呢？</title>
            <link>https://nitter.cz/mtrainier2020/status/1737398651830276156#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1737398651830276156#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 09:04:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我初步读了一下论文，这篇论文的基本思路其实是利用了推理过程中的局部性。现在现在推理性能的一个瓶颈就是GPU的内存。它们的思路就是联合CPU和GPU做联合推理。尽可能把active的neuron 信息load到GPU中，充分利用局部性。这样大大提高了GPU推理的效率。<br />
你不能拿单独的CPU推理或者单独的GPU推理来比。两者的指导思想都不一样。<br />
这样的效果非常牛逼。<br />
<br />
在推理速度上，在 4090 上是未经优化的llama.cpp 11 倍。相当于用一块4090（2000刀左右）在推理上取得了比a100 （2万刀左右）仅仅慢18%的成绩。<br />
<br />
这个结果，更大的意义就是证明，充分考虑并利用推理的局部性有极大的潜力，CPU+GPU 联合推理有很强的优越性。<br />
类似的，那么在训练当面是不是也可以采用这种思路呢？</p>
<p><a href="https://nitter.cz/engineer_bob_sz/status/1737297706736144476#m">nitter.cz/engineer_bob_sz/status/1737297706736144476#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J4NjNKeVdJQUFDZlp5LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J4NjNKdlc0QUFGblJfLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J4NjNKeFdrQUFNMEM5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1737408280090280396#m</id>
            <title>RT by @dotey: 前几天发布的大幅提高视频生成流畅性的项目FreeInit，已经可以用了。

试了一下，真的离谱，Animatediff 的稳定性确实得到了大幅提高，但是同时变化幅度也有一定下降，不过这个是值得的。

下面是对比视频，可以用camenduru做的这个 Colab 链接部署测试：https://colab.research.google.com/github/camenduru/FreeInit-colab/blob/main/FreeInit_gradio_colab.ipynb</title>
            <link>https://nitter.cz/op7418/status/1737408280090280396#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1737408280090280396#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 09:43:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>前几天发布的大幅提高视频生成流畅性的项目FreeInit，已经可以用了。<br />
<br />
试了一下，真的离谱，Animatediff 的稳定性确实得到了大幅提高，但是同时变化幅度也有一定下降，不过这个是值得的。<br />
<br />
下面是对比视频，可以用camenduru做的这个 Colab 链接部署测试：<a href="https://colab.research.google.com/github/camenduru/FreeInit-colab/blob/main/FreeInit_gradio_colab.ipynb">colab.research.google.com/gi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc0MDgwNjY4MzA5NzkwNzIvcHUvaW1nL19IdndIQ2FpQUJ0VGQwN1kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737389024300429498#m</id>
            <title>🤣 React Server Component 没那么夸张吧
RSC 虐我千百遍，我待 RSC 如初恋！</title>
            <link>https://nitter.cz/dotey/status/1737389024300429498#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737389024300429498#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:26:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🤣 React Server Component 没那么夸张吧<br />
RSC 虐我千百遍，我待 RSC 如初恋！</p>
<p><a href="https://nitter.cz/sebastienlorber/status/1737380479383277636#m">nitter.cz/sebastienlorber/status/1737380479383277636#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1737388243006480895#m</id>
            <title>推荐阅读：《2023 in Review: Recapping the Post-ChatGPT Era and What to Expect for 2024 | 2023 年回顾：聚焦 ChatGPT 时代之后的发展及 2024 年展望 [译]》

原文：https://towardsdatascience.com/2023-in-review-recapping-the-post-chatgpt-era-and-what-to-expect-for-2024-bb4357a4e827
译文：https://baoyu.io/translations/ai/2023-in-review-recapping-the-post-chatgpt-era-and-what-to-expect-for-2024</title>
            <link>https://nitter.cz/dotey/status/1737388243006480895#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1737388243006480895#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:23:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《2023 in Review: Recapping the Post-ChatGPT Era and What to Expect for 2024 | 2023 年回顾：聚焦 ChatGPT 时代之后的发展及 2024 年展望 [译]》<br />
<br />
原文：<a href="https://towardsdatascience.com/2023-in-review-recapping-the-post-chatgpt-era-and-what-to-expect-for-2024-bb4357a4e827">towardsdatascience.com/2023-…</a><br />
译文：<a href="https://baoyu.io/translations/ai/2023-in-review-recapping-the-post-chatgpt-era-and-what-to-expect-for-2024">baoyu.io/translations/ai/202…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J4eFpXdlhnQUE3SHhULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>