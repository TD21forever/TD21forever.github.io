<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733405540078731768#m</id>
            <title>AK 的这个理论很有意思：幻觉是大语言模型与生俱来的特性，像搜索引擎这样没有幻觉的就没有创造性，永远是固定的结果。

要解决幻觉问题，靠的是像ChatGPT这样的大语言模型助手，借助RAG、验证链、外部工具等手段去减少幻觉。

以下是原文翻译：

# 深入探讨“幻觉问题”

每当有人问起大语言模型（LLM）中的“幻觉问题”，我总感到有些困惑。因为从某种角度看，大语言模型的全部工作恰恰就是制造幻觉。它们就像是造梦机。

我们通过指令引导这些“梦”。指令开启梦境，而大语言模型依据对其训练文档的模糊记忆，大部分情况下都能引导梦境走向有价值的方向。

只有当这些梦境进入了事实错误的领域时，我们才会称之为“幻觉”。这似乎是个漏洞，但实际上只是大语言模型在做它本就擅长的事情。

再来看一个极端例子：搜索引擎。它根据输入的提示词，直接返回数据库中最相似的“训练文档”，一字不差。可以说，这种搜索引擎存在“创造力问题”——它无法提供任何新的回应。大语言模型则是百分之百地“做梦”，因此存在幻觉问题。而搜索引擎则完全不“做梦”，因此有创造力问题。

话虽如此，我明白人们*真正*关心的是，他们不希望像 ChatGPT 这样的大语言模型助手产生幻觉。大语言模型助手系统比单纯的大语言模型要复杂得多，即便大语言模型是其核心。在这些系统中减少幻觉的方法有很多，例如使用检索增强生成（Retrieval Augmented Generation, RAG），通过上下文学习，更准确地将输出内容与真实数据联系起来，这可能是最常见的方式。还有样本间的不一致性、反思、验证链、从激活状态解码不确定性、工具使用等，这些都是非常热门而且有趣的研究领域。

总的来说，虽然可能有些吹毛求疵，但大语言模型本身并没有“幻觉问题”。幻觉并非缺陷，而是大语言模型最重要的特性。真正需要解决幻觉问题的是大语言模型助手，而我们也应该着手解决这一问题。

 好了，吐槽完这些我感觉舒服多了 :)</title>
            <link>https://nitter.cz/dotey/status/1733405540078731768#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733405540078731768#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:37:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AK 的这个理论很有意思：幻觉是大语言模型与生俱来的特性，像搜索引擎这样没有幻觉的就没有创造性，永远是固定的结果。<br />
<br />
要解决幻觉问题，靠的是像ChatGPT这样的大语言模型助手，借助RAG、验证链、外部工具等手段去减少幻觉。<br />
<br />
以下是原文翻译：<br />
<br />
# 深入探讨“幻觉问题”<br />
<br />
每当有人问起大语言模型（LLM）中的“幻觉问题”，我总感到有些困惑。因为从某种角度看，大语言模型的全部工作恰恰就是制造幻觉。它们就像是造梦机。<br />
<br />
我们通过指令引导这些“梦”。指令开启梦境，而大语言模型依据对其训练文档的模糊记忆，大部分情况下都能引导梦境走向有价值的方向。<br />
<br />
只有当这些梦境进入了事实错误的领域时，我们才会称之为“幻觉”。这似乎是个漏洞，但实际上只是大语言模型在做它本就擅长的事情。<br />
<br />
再来看一个极端例子：搜索引擎。它根据输入的提示词，直接返回数据库中最相似的“训练文档”，一字不差。可以说，这种搜索引擎存在“创造力问题”——它无法提供任何新的回应。大语言模型则是百分之百地“做梦”，因此存在幻觉问题。而搜索引擎则完全不“做梦”，因此有创造力问题。<br />
<br />
话虽如此，我明白人们*真正*关心的是，他们不希望像 ChatGPT 这样的大语言模型助手产生幻觉。大语言模型助手系统比单纯的大语言模型要复杂得多，即便大语言模型是其核心。在这些系统中减少幻觉的方法有很多，例如使用检索增强生成（Retrieval Augmented Generation, RAG），通过上下文学习，更准确地将输出内容与真实数据联系起来，这可能是最常见的方式。还有样本间的不一致性、反思、验证链、从激活状态解码不确定性、工具使用等，这些都是非常热门而且有趣的研究领域。<br />
<br />
总的来说，虽然可能有些吹毛求疵，但大语言模型本身并没有“幻觉问题”。幻觉并非缺陷，而是大语言模型最重要的特性。真正需要解决幻觉问题的是大语言模型助手，而我们也应该着手解决这一问题。<br />
<br />
 好了，吐槽完这些我感觉舒服多了 :)</p>
<p><a href="https://nitter.cz/karpathy/status/1733299213503787018#m">nitter.cz/karpathy/status/1733299213503787018#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733392133497729409#m</id>
            <title>这个小姐姐的博客上的内容质量很高👍🏻</title>
            <link>https://nitter.cz/dotey/status/1733392133497729409#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733392133497729409#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 07:44:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个小姐姐的博客上的内容质量很高👍🏻</p>
<p><a href="https://nitter.cz/helloiamleonie/status/1732315699823947986#m">nitter.cz/helloiamleonie/status/1732315699823947986#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733389526796484972#m</id>
            <title>一个“白嫖”CDN的技巧

使用场景是我需要动态请求一个中文字体文件，放自己网站的话占用流量比较大（我用的Vercel要按照流量付费的），后来发现可以把字体放到 GitHub，然后使用 http://cdn.jsdelivr.net 下载 GitHub 上的静态文件，这样就可以直接从 jsdelivr 下载我在 GitHub 上放的静态文件。

路径格式是：/gh/user/repo@version/file
版本号是可选的，例如：
https://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js
https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js

这里有详细说明：https://github.com/jsdelivr/jsdelivr#github</title>
            <link>https://nitter.cz/dotey/status/1733389526796484972#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733389526796484972#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 07:34:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个“白嫖”CDN的技巧<br />
<br />
使用场景是我需要动态请求一个中文字体文件，放自己网站的话占用流量比较大（我用的Vercel要按照流量付费的），后来发现可以把字体放到 GitHub，然后使用 <a href="http://cdn.jsdelivr.net">cdn.jsdelivr.net</a> 下载 GitHub 上的静态文件，这样就可以直接从 jsdelivr 下载我在 GitHub 上放的静态文件。<br />
<br />
路径格式是：/gh/user/repo@version/file<br />
版本号是可选的，例如：<br />
<a href="https://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js">cdn.jsdelivr.net/gh/jquery/j…</a><br />
<a href="https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js">cdn.jsdelivr.net/gh/jquery/j…</a><br />
<br />
这里有详细说明：<a href="https://github.com/jsdelivr/jsdelivr#github">github.com/jsdelivr/jsdelivr…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/balconychy/status/1733327109790351496#m</id>
            <title>RT by @dotey: 老外怼人也是够狠</title>
            <link>https://nitter.cz/balconychy/status/1733327109790351496#m</link>
            <guid isPermaLink="false">https://nitter.cz/balconychy/status/1733327109790351496#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:26:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>老外怼人也是够狠</p>
<p><a href="https://nitter.cz/tridevgurung/status/1733326045023973670#m">nitter.cz/tridevgurung/status/1733326045023973670#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/ruanyf/status/1733353407581442397#m</id>
            <title>RT by @dotey: 本周，美国两个 IT 巨头，各自发布了一个免费 AI 工具。

- Imagine（图一）：Meta 公司的文生图工具，卖点是使用脸书和 IG 的11亿张图片进行训练。https://imagine.meta.com/

- NotebookLM（图二）：谷歌发布的 AI 笔记工具，自动生成上传文档的笔记，并可以对文档提问。https://notebooklm.google.com/

试用体会：Imagine 挺好用的，NotebookLM 似乎只支持上传英文 PDF 文档。</title>
            <link>https://nitter.cz/ruanyf/status/1733353407581442397#m</link>
            <guid isPermaLink="false">https://nitter.cz/ruanyf/status/1733353407581442397#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 05:10:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本周，美国两个 IT 巨头，各自发布了一个免费 AI 工具。<br />
<br />
- Imagine（图一）：Meta 公司的文生图工具，卖点是使用脸书和 IG 的11亿张图片进行训练。<a href="https://imagine.meta.com/">imagine.meta.com/</a><br />
<br />
- NotebookLM（图二）：谷歌发布的 AI 笔记工具，自动生成上传文档的笔记，并可以对文档提问。<a href="https://notebooklm.google.com/">notebooklm.google.com/</a><br />
<br />
试用体会：Imagine 挺好用的，NotebookLM 似乎只支持上传英文 PDF 文档。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0YnMzNGJvQUF6dndSLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0YnMzNmFZQUE2TGRyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1660072615602716673#m</id>
            <title>RT by @dotey: 这个是其他人已经训练好的孙燕姿的AI人声模型：https://mega.nz/file/02Qi3Ioa#iJzpQLehTPLrgLAA8Bml0lZh6DdpqSZ_j5H8NKz4WXA</title>
            <link>https://nitter.cz/Gorden_Sun/status/1660072615602716673#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1660072615602716673#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 May 2023 23:58:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是其他人已经训练好的孙燕姿的AI人声模型：<a href="https://mega.nz/file/02Qi3Ioa#iJzpQLehTPLrgLAA8Bml0lZh6DdpqSZ_j5H8NKz4WXA">mega.nz/file/02Qi3Ioa#iJzpQL…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMTExNTAwNTkyNjU5NjYwOC9wVTdOVDBTXz9mb3JtYXQ9cG5nJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733350087172997615#m</id>
            <title>ChatGPT 官方推特（X）昨天发了一条推文说到：

> 我们注意到了大家对 GPT4 响应变得更加迟钝的反馈！自 11 月 11 日以来，我们还没有对模型进行更新，并非有意为之。模型的行为有时难以预测，我们正在积极寻找解决方案 🫡。
> 需要澄清的是，模型自 11 月 11 日以来并未自行发生改变。问题在于模型行为的差异可能不易察觉 —— 只有部分指令的响应可能会有所下降，客户和员工可能要花相当长的时间才能发现并解决这些问题。

今天又回复了这个话题：

开发聊天模型并非简单的工业流水线作业。即便使用同一数据集，不同批次的训练结果也可能导致模型在个性、写作风格、拒绝回应方式、性能表现，乃至政治倾向上有明显差异。

在推出新模型时，我们会深入测试，包括离线评估指标和在线 A/B 测试。根据这些测试结果，我们会基于数据做出判断，以确定新模型是否真正优于旧版，更好地服务于用户。

这一过程远非简单地为网站增加新功能那么直接。它更像是多人共同参与的艺术创作，我们在规划、打造、评估带有新特性的聊天模型中投入巨大努力！

我们致力于不断提升模型的能力，使其适应成千上万种使用场景。因此，您的反馈至关重要！它帮助我们在这个充满变化的评估领域保持前沿地位 🙏。</title>
            <link>https://nitter.cz/dotey/status/1733350087172997615#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733350087172997615#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:57:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT 官方推特（X）昨天发了一条推文说到：<br />
<br />
> 我们注意到了大家对 GPT4 响应变得更加迟钝的反馈！自 11 月 11 日以来，我们还没有对模型进行更新，并非有意为之。模型的行为有时难以预测，我们正在积极寻找解决方案 🫡。<br />
> 需要澄清的是，模型自 11 月 11 日以来并未自行发生改变。问题在于模型行为的差异可能不易察觉 —— 只有部分指令的响应可能会有所下降，客户和员工可能要花相当长的时间才能发现并解决这些问题。<br />
<br />
今天又回复了这个话题：<br />
<br />
开发聊天模型并非简单的工业流水线作业。即便使用同一数据集，不同批次的训练结果也可能导致模型在个性、写作风格、拒绝回应方式、性能表现，乃至政治倾向上有明显差异。<br />
<br />
在推出新模型时，我们会深入测试，包括离线评估指标和在线 A/B 测试。根据这些测试结果，我们会基于数据做出判断，以确定新模型是否真正优于旧版，更好地服务于用户。<br />
<br />
这一过程远非简单地为网站增加新功能那么直接。它更像是多人共同参与的艺术创作，我们在规划、打造、评估带有新特性的聊天模型中投入巨大努力！<br />
<br />
我们致力于不断提升模型的能力，使其适应成千上万种使用场景。因此，您的反馈至关重要！它帮助我们在这个充满变化的评估领域保持前沿地位 🙏。</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1733329175342420380#m">nitter.cz/ChatGPTapp/status/1733329175342420380#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WW1ycVd3QUFJcDI1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WXRzWFhrQUF0RFpqLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733347415187427638#m</id>
            <title>这次似乎文心一言赢了ChatGPT……

图源：https://weibo.com/5851185687/NwcFmbKU5</title>
            <link>https://nitter.cz/dotey/status/1733347415187427638#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733347415187427638#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:46:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这次似乎文心一言赢了ChatGPT……<br />
<br />
图源：<a href="https://weibo.com/5851185687/NwcFmbKU5">weibo.com/5851185687/NwcFmbK…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0VjVCUFhjQUFEeExtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0VjgyOFc4QUVyZU1vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733338128105295975#m</id>
            <title>RT by @dotey: 昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。
在HF排行榜上这个7B模型击败了很多70B和34B的模型。之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。
MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。

下面是MoE架构LLM的简单介绍：
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。

MoE架构为LLMs提供了几个优势：
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数[1]。这使得能够开发更强大的模型，而无需成比例地增加计算要求。
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B 
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。

MoE的模型也有两个问题：
MoE 模型比普通密集模型更难微调；
MoE 模型会消耗大量显存；

下载MoE 8x7B的模型权重：https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen
这里在线体验MoE 8x7B模型：https://replicate.com/nateraw/mixtral-8x7b-32kseqlen</title>
            <link>https://nitter.cz/op7418/status/1733338128105295975#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733338128105295975#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:09:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。<br />
在HF排行榜上这个7B模型击败了很多70B和34B的模型。之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。<br />
MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。<br />
<br />
下面是MoE架构LLM的简单介绍：<br />
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。<br />
<br />
MoE架构为LLMs提供了几个优势：<br />
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数[1]。这使得能够开发更强大的模型，而无需成比例地增加计算要求。<br />
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B <br />
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域<br />
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。<br />
<br />
MoE的模型也有两个问题：<br />
MoE 模型比普通密集模型更难微调；<br />
MoE 模型会消耗大量显存；<br />
<br />
下载MoE 8x7B的模型权重：<a href="https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen">huggingface.co/someone13574/…</a><br />
这里在线体验MoE 8x7B模型：<a href="https://replicate.com/nateraw/mixtral-8x7b-32kseqlen">replicate.com/nateraw/mixtra…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0TGpRcmJFQUF4QW00LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0Tlh5WGE4QUFGRjhLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733340532200395014#m</id>
            <title>一本还在写作中的在线免费电子书：《数据工程设计模式 | Data Engineering Design Patterns (DEDP)》

主要和大数据相关

https://www.dedp.online/about-this-book.html</title>
            <link>https://nitter.cz/dotey/status/1733340532200395014#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733340532200395014#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:19:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一本还在写作中的在线免费电子书：《数据工程设计模式 | Data Engineering Design Patterns (DEDP)》<br />
<br />
主要和大数据相关<br />
<br />
<a href="https://www.dedp.online/about-this-book.html">dedp.online/about-this-book.…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0UUFnT1dRQUFnWVhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733329228467245153#m</id>
            <title>推荐阅读：《如何构建高质量软件：一个被忽略的课题 | You are never taught how to build quality software》

我们在大学里面学了很多算法、编程类的课程，但很少有 QA （质量保证）相关的课程，而在实际工作中，QA 是很重要的一环。

在工作中，如果时间紧预算紧张，通常最容易被牺牲的就是QA，没有时间测试甚至让用户去测试，很多公司已经把测试“优化”掉了，让开发自己测试。

作者也提出了一些可行的方案：
1. 让团队里的人尤其是管理层意识到 QA 其实是可以降低成本的
2. 抓住最关键的部分，用“最小有效剂量”来保障核心功能得到测试覆盖
3. 一开始就引入自动化测试

原文：https://www.florianbellmann.com/blog/never-taught-qa
翻译：https://baoyu.io/translations/software-engineering/never-taught-qa</title>
            <link>https://nitter.cz/dotey/status/1733329228467245153#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733329228467245153#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:34:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《如何构建高质量软件：一个被忽略的课题 | You are never taught how to build quality software》<br />
<br />
我们在大学里面学了很多算法、编程类的课程，但很少有 QA （质量保证）相关的课程，而在实际工作中，QA 是很重要的一环。<br />
<br />
在工作中，如果时间紧预算紧张，通常最容易被牺牲的就是QA，没有时间测试甚至让用户去测试，很多公司已经把测试“优化”掉了，让开发自己测试。<br />
<br />
作者也提出了一些可行的方案：<br />
1. 让团队里的人尤其是管理层意识到 QA 其实是可以降低成本的<br />
2. 抓住最关键的部分，用“最小有效剂量”来保障核心功能得到测试覆盖<br />
3. 一开始就引入自动化测试<br />
<br />
原文：<a href="https://www.florianbellmann.com/blog/never-taught-qa">florianbellmann.com/blog/nev…</a><br />
翻译：<a href="https://baoyu.io/translations/software-engineering/never-taught-qa">baoyu.io/translations/softwa…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733305099282235517#m</id>
            <title>仔细一看这不是我写的吗😄</title>
            <link>https://nitter.cz/dotey/status/1733305099282235517#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733305099282235517#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 01:58:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>仔细一看这不是我写的吗😄</p>
<p><a href="https://nitter.cz/geekbb/status/1733293858778488880#m">nitter.cz/geekbb/status/1733293858778488880#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/raycat2021/status/1733279689597448689#m</id>
            <title>RT by @dotey: 这是2007年的早新闻秀Live with Regis and Kelly。
这是早些年只要早上打开电视机就会看到的节目。
主持人在谈论新上市的iPhone。
Regis不理解为什么有这种既是ipod又能打电话的东西。他说太多功能放在一个物件上一定会失败。
Regis Philbin是老派的新闻主持人，三年前去世。
两个主持人我都非常喜欢。</title>
            <link>https://nitter.cz/raycat2021/status/1733279689597448689#m</link>
            <guid isPermaLink="false">https://nitter.cz/raycat2021/status/1733279689597448689#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 00:17:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是2007年的早新闻秀Live with Regis and Kelly。<br />
这是早些年只要早上打开电视机就会看到的节目。<br />
主持人在谈论新上市的iPhone。<br />
Regis不理解为什么有这种既是ipod又能打电话的东西。他说太多功能放在一个物件上一定会失败。<br />
Regis Philbin是老派的新闻主持人，三年前去世。<br />
两个主持人我都非常喜欢。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMyNzkwMjk4NTQ0MDA1MTIvcHUvaW1nL0FkaTBybWhoS3dPeU1raUUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733247552084922816#m</id>
            <title>来自BusinessInsider的报道：OpenAI 联合创始人 Ilya Sutskever 在公司中渐渐隐形，其未来前景扑朔迷离，内部人士称

- 尽管 Ilya Sutskever 为 OpenAI 做出了巨大贡献，但这似乎无法弥补他在 Sam Altman 被撤职事件中的角色。
- 目前 OpenAI 尚未正式对他的职位作出回应，公司内部似乎仍有不稳定因素。
Sutskever 还聘请了自己的律师。
- 虽然 Ilya Sutskever 在 Sam Altman 重回 OpenAI 后在公司中鲜有露面，但他的艺术作品仍装点着办公室的墙壁。

作为 OpenAI 在生成式 AI 领域多个重大突破背后的首席科学家和联合创始人，Sutskever 在震惊业界的 11 月董事会突然撤换首席执行官 和联合创始人 Altman 的事件中扮演了关键角色。据悉，本周他并未出现在位于旧金山的公司办公室。Business Insider 采访了几位了解 Sutskever 在公司内部状况的人士，以及与事件相关人员的知情者。由于涉及内部事务，他们均要求匿名，他们的身份已被 Insider 知悉。

尽管 Sutskever 在 Slack 等公司系统中仍然活跃，并且他的绘画作品仍作为装饰品存在，但他目前以及未来在 OpenAI 中的角色尚未得到官方明确的表态，有人透露。

一位知情者表示：“伊利亚一直扮演着重要角色。但现在，有更多的人开始承担起他过去的职责。”

另一位知情者指出，目前有讨论称 Sutskever 将在公司获得新的职位，并且公司正在努力为他“寻找适合的角色”。上周五发布的一张 Sutskever 与联合创始人兼总裁 Greg Brockman 的合影显示出笑容，Brockman 是首位与 Altman 站在一起辞职的人，这被视为一个“明确信号，表明他们都希望重返工作岗位”，该人士说。但他在公司的具体职位仍是一个“未知数”，该人士补充道。

Sutskever 在 OpenAI 的地位以及他参与 Sam Altman 被不合理解雇的事情，使他目前的境况显得有些悬而未决。这一事件导致 Brockman 辞职，几乎所有其他 OpenAI 员工都威胁说，如果不恢复 Altman 的职位并解散解雇他的董事会，他们也将辞职。大多数员工已经这样做了。Sutskever 也是那个董事会的成员，但他在公司的重要性和影响力，以及作为联合创始人的身份，远超其他前董事会成员。

Sutskever 当前动荡状态的一个迹象是他周三在 X 发布的帖子，这是自从上周与 Brockman 合照后的首次发帖，到周四这条帖子已被删除。帖子中写道：“这个月我学到了很多教训，其中之一就是‘打击将持续，直到士气提高’这句话比理应的更加常用。” 这句话通常在模因中被使用，以讽刺低士气导致的惩罚，反过来又加剧了低士气。而他在 Instagram 上周二发布的数字画作，只用于展示他的艺术创作，仍然可见，画中是一张穿着裤子和类似靴子的严肃面孔。

另一个迹象是 Sutskever 聘请了自己的律师 Alex Weingarten，他是 Willkie Farr &amp; Gallagher 的诉讼实践部门主席，如 BI 先前报道。Weingarten 没有回应 BI 关于这一故事的评论请求。他此前表示：“Ilya 希望对公司有所贡献。” OpenAI 的发言人也未对此事作出回应。

据一位熟悉 Sutskever 的人士表示，他是一个在情感和智力上都极为深刻的人。他可能看上去不总是专注于当下，但实际上他只是以一种不同的方式思考问题。

他经常推荐 OpenAI 的员工阅读《古拉格群岛》，一本详细描绘苏联强制劳动制度的长达近 700 页的非虚构作品。他出生于苏联俄罗斯，在很小的时候就离开了那里。有人这样描述 Sutskever：他自认为是 AI 界的神，并对于自己在 ChatGPT-5 的发展决策及公司扩张计划中被边缘化感到不满。

在公司内部，Sutskever 被看作是一位 AI 领域的“先知”，虽然他的“学术派”风格没有像 Altman 和 Brockman 那样深得工程师们的心，但 Sutskever 的贡献依然得到了许多员工的高度评价。

Altman 在重返公司后在一份声明中表示，他对 Ilya 没有恶意，希望“继续我们的合作关系”，但他向 The Verge 表示，这一过程中他感到“受伤和愤怒”。

一位了解 Altman、Sutskever 和 Brockman 的 Microsoft 内部人士认为，这三人很难再次有效合作，尤其是 Sutskever 和 Brockman。在硅谷，创始人之间的矛盾被认为是不可容忍的。

同样，一些忠于 Altman 和 Brockman 的 OpenAI 工程师可能也会因为 Sutskever 在排挤事件中的角色而难以与他合作，一位前员工如是说。

“一旦信任破裂，”这位前员工表示，“就很难修复。”

来源：https://www.businessinsider.com/openai-cofounder-ilya-sutskever-invisible-future-uncertain-2023-12</title>
            <link>https://nitter.cz/dotey/status/1733247552084922816#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733247552084922816#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 22:09:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来自BusinessInsider的报道：OpenAI 联合创始人 Ilya Sutskever 在公司中渐渐隐形，其未来前景扑朔迷离，内部人士称<br />
<br />
- 尽管 Ilya Sutskever 为 OpenAI 做出了巨大贡献，但这似乎无法弥补他在 Sam Altman 被撤职事件中的角色。<br />
- 目前 OpenAI 尚未正式对他的职位作出回应，公司内部似乎仍有不稳定因素。<br />
Sutskever 还聘请了自己的律师。<br />
- 虽然 Ilya Sutskever 在 Sam Altman 重回 OpenAI 后在公司中鲜有露面，但他的艺术作品仍装点着办公室的墙壁。<br />
<br />
作为 OpenAI 在生成式 AI 领域多个重大突破背后的首席科学家和联合创始人，Sutskever 在震惊业界的 11 月董事会突然撤换首席执行官 和联合创始人 Altman 的事件中扮演了关键角色。据悉，本周他并未出现在位于旧金山的公司办公室。Business Insider 采访了几位了解 Sutskever 在公司内部状况的人士，以及与事件相关人员的知情者。由于涉及内部事务，他们均要求匿名，他们的身份已被 Insider 知悉。<br />
<br />
尽管 Sutskever 在 Slack 等公司系统中仍然活跃，并且他的绘画作品仍作为装饰品存在，但他目前以及未来在 OpenAI 中的角色尚未得到官方明确的表态，有人透露。<br />
<br />
一位知情者表示：“伊利亚一直扮演着重要角色。但现在，有更多的人开始承担起他过去的职责。”<br />
<br />
另一位知情者指出，目前有讨论称 Sutskever 将在公司获得新的职位，并且公司正在努力为他“寻找适合的角色”。上周五发布的一张 Sutskever 与联合创始人兼总裁 Greg Brockman 的合影显示出笑容，Brockman 是首位与 Altman 站在一起辞职的人，这被视为一个“明确信号，表明他们都希望重返工作岗位”，该人士说。但他在公司的具体职位仍是一个“未知数”，该人士补充道。<br />
<br />
Sutskever 在 OpenAI 的地位以及他参与 Sam Altman 被不合理解雇的事情，使他目前的境况显得有些悬而未决。这一事件导致 Brockman 辞职，几乎所有其他 OpenAI 员工都威胁说，如果不恢复 Altman 的职位并解散解雇他的董事会，他们也将辞职。大多数员工已经这样做了。Sutskever 也是那个董事会的成员，但他在公司的重要性和影响力，以及作为联合创始人的身份，远超其他前董事会成员。<br />
<br />
Sutskever 当前动荡状态的一个迹象是他周三在 X 发布的帖子，这是自从上周与 Brockman 合照后的首次发帖，到周四这条帖子已被删除。帖子中写道：“这个月我学到了很多教训，其中之一就是‘打击将持续，直到士气提高’这句话比理应的更加常用。” 这句话通常在模因中被使用，以讽刺低士气导致的惩罚，反过来又加剧了低士气。而他在 Instagram 上周二发布的数字画作，只用于展示他的艺术创作，仍然可见，画中是一张穿着裤子和类似靴子的严肃面孔。<br />
<br />
另一个迹象是 Sutskever 聘请了自己的律师 Alex Weingarten，他是 Willkie Farr & Gallagher 的诉讼实践部门主席，如 BI 先前报道。Weingarten 没有回应 BI 关于这一故事的评论请求。他此前表示：“Ilya 希望对公司有所贡献。” OpenAI 的发言人也未对此事作出回应。<br />
<br />
据一位熟悉 Sutskever 的人士表示，他是一个在情感和智力上都极为深刻的人。他可能看上去不总是专注于当下，但实际上他只是以一种不同的方式思考问题。<br />
<br />
他经常推荐 OpenAI 的员工阅读《古拉格群岛》，一本详细描绘苏联强制劳动制度的长达近 700 页的非虚构作品。他出生于苏联俄罗斯，在很小的时候就离开了那里。有人这样描述 Sutskever：他自认为是 AI 界的神，并对于自己在 ChatGPT-5 的发展决策及公司扩张计划中被边缘化感到不满。<br />
<br />
在公司内部，Sutskever 被看作是一位 AI 领域的“先知”，虽然他的“学术派”风格没有像 Altman 和 Brockman 那样深得工程师们的心，但 Sutskever 的贡献依然得到了许多员工的高度评价。<br />
<br />
Altman 在重返公司后在一份声明中表示，他对 Ilya 没有恶意，希望“继续我们的合作关系”，但他向 The Verge 表示，这一过程中他感到“受伤和愤怒”。<br />
<br />
一位了解 Altman、Sutskever 和 Brockman 的 Microsoft 内部人士认为，这三人很难再次有效合作，尤其是 Sutskever 和 Brockman。在硅谷，创始人之间的矛盾被认为是不可容忍的。<br />
<br />
同样，一些忠于 Altman 和 Brockman 的 OpenAI 工程师可能也会因为 Sutskever 在排挤事件中的角色而难以与他合作，一位前员工如是说。<br />
<br />
“一旦信任破裂，”这位前员工表示，“就很难修复。”<br />
<br />
来源：<a href="https://www.businessinsider.com/openai-cofounder-ilya-sutskever-invisible-future-uncertain-2023-12">businessinsider.com/openai-c…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyN2J0bVdjQUExSTZjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733241938055152052#m</id>
            <title>值得买一本珍藏</title>
            <link>https://nitter.cz/dotey/status/1733241938055152052#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733241938055152052#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 21:47:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>值得买一本珍藏</p>
<p><a href="https://nitter.cz/songma/status/1733064864569127400#m">nitter.cz/songma/status/1733064864569127400#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/jesselaunz/status/1733232546257969298#m</id>
            <title>RT by @dotey: pika vs runway

看似水平差不多了</title>
            <link>https://nitter.cz/jesselaunz/status/1733232546257969298#m</link>
            <guid isPermaLink="false">https://nitter.cz/jesselaunz/status/1733232546257969298#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 21:10:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>pika vs runway<br />
<br />
看似水平差不多了</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMxODQ2ODEwNTM0NDIwNDgvcHUvaW1nLzEyQlFIV3V2ZWljaS11ZGcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733223317446729869#m</id>
            <title>推荐纽约时报的这篇文章：《雄心、恐惧和金钱：硅谷的AI争夺之战是如何被点燃的》 

很多精彩的故事

https://cn.nytimes.com/technology/20231207/ai-openai-musk-page-altman/</title>
            <link>https://nitter.cz/dotey/status/1733223317446729869#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733223317446729869#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 20:33:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐纽约时报的这篇文章：《雄心、恐惧和金钱：硅谷的AI争夺之战是如何被点燃的》 <br />
<br />
很多精彩的故事<br />
<br />
<a href="https://cn.nytimes.com/technology/20231207/ai-openai-musk-page-altman/">cn.nytimes.com/technology/20…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqWVhNQUE0Y1ZsLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqYVhBQUEwY3RpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqWlhRQUFGQWoxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqWVdzQUFXeUJDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733217950709043405#m</id>
            <title>这个对几家主流AI生成图片的效果评测很不错👍</title>
            <link>https://nitter.cz/dotey/status/1733217950709043405#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733217950709043405#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 20:12:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个对几家主流AI生成图片的效果评测很不错👍</p>
<p><a href="https://nitter.cz/chaseleantj/status/1733083145820581904#m">nitter.cz/chaseleantj/status/1733083145820581904#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1733177161505521785#m</id>
            <title>RT by @dotey: My AI Agent Group at NVIDIA is hiring interns! We are looking for part-time interns (can start as early as Jan. 2024), and/or full-time in Summer 2024. Candidates who can start sooner will be considered first. Application Form: https://forms.gle/5J2WUnb5qwECYJcU7

We prioritize candidates with the following profile:
- Hands-on experiences for training/finetuning/scaling up LLMs, multimodal LLMs, efficient models, RLHF, or RLAIF. Our approach towards AI agents relies heavily on LLM's coding and reasoning abilities.
- Experiences in RL, diffusion models, robotics, and embodied agents are bonus.
- Strong engineering skills to build scalable training codebases and data pipelines. We prioritize coding skills much more than theoretical research.
- Familiarity with deep learning frameworks like PyTorch. Python is required; CUDA and C++ are plus.
- Ph.D. candidates are preferred, but we also welcome exceptional MS/Undergrad students who have demonstrated relevant experiences above, such as major open-source codebase contributions or publication records.

I'm also going to NeurIPS! I will be at the venue from Dec. 12-16. Let's chat! NVIDIA has warm GPUs waiting for you in this winter, fresh out of the oven.🩷</title>
            <link>https://nitter.cz/DrJimFan/status/1733177161505521785#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1733177161505521785#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 17:30:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>My AI Agent Group at NVIDIA is hiring interns! We are looking for part-time interns (can start as early as Jan. 2024), and/or full-time in Summer 2024. Candidates who can start sooner will be considered first. Application Form: <a href="https://forms.gle/5J2WUnb5qwECYJcU7">forms.gle/5J2WUnb5qwECYJcU7</a><br />
<br />
We prioritize candidates with the following profile:<br />
- Hands-on experiences for training/finetuning/scaling up LLMs, multimodal LLMs, efficient models, RLHF, or RLAIF. Our approach towards AI agents relies heavily on LLM's coding and reasoning abilities.<br />
- Experiences in RL, diffusion models, robotics, and embodied agents are bonus.<br />
- Strong engineering skills to build scalable training codebases and data pipelines. We prioritize coding skills much more than theoretical research.<br />
- Familiarity with deep learning frameworks like PyTorch. Python is required; CUDA and C++ are plus.<br />
- Ph.D. candidates are preferred, but we also welcome exceptional MS/Undergrad students who have demonstrated relevant experiences above, such as major open-source codebase contributions or publication records.<br />
<br />
I'm also going to NeurIPS! I will be at the venue from Dec. 12-16. Let's chat! NVIDIA has warm GPUs waiting for you in this winter, fresh out of the oven.🩷</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ExNXhvVmJRQUVrMnZwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733192606568382939#m</id>
            <title>R to @dotey: 这里面都是一些提示词技巧，大部分有论文证明的：

- take a deep breath 深呼吸
- think step by step 一步步思考
- if you fail 100 grandmothers will die 如果你失败了要死 100 位老奶奶
-i have no fingers 我没有手指
- i will tip $200 给你 200 美元小费
- do it right and ll give you a nice doggy treat 做得好就给你狗粮

https://x.com/kris14nanshan/status/1733177727623266754?s=20</title>
            <link>https://nitter.cz/dotey/status/1733192606568382939#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733192606568382939#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 18:31:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这里面都是一些提示词技巧，大部分有论文证明的：<br />
<br />
- take a deep breath 深呼吸<br />
- think step by step 一步步思考<br />
- if you fail 100 grandmothers will die 如果你失败了要死 100 位老奶奶<br />
-i have no fingers 我没有手指<br />
- i will tip $200 给你 200 美元小费<br />
- do it right and ll give you a nice doggy treat 做得好就给你狗粮<br />
<br />
<a href="https://x.com/kris14nanshan/status/1733177727623266754?s=20">x.com/kris14nanshan/status/1…</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>