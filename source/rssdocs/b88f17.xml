<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732448002076185046#m</id>
            <title>R to @dotey: 谢谢各位指正，我上面对于 llama.cpp 的描述不正确，但无法编辑原文，请无视</title>
            <link>https://nitter.cz/dotey/status/1732448002076185046#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732448002076185046#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 17:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谢谢各位指正，我上面对于 llama.cpp 的描述不正确，但无法编辑原文，请无视</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1732420600428425362#m</id>
            <title>RT by @dotey: Google终于发布了传言中的强大多模态LLM Gemini，他们说这是迄今为止最强大的AI模型。从描述来看确实非常强大。
Google CEO的介绍视频也翻译好了，下面模型是具体的介绍：

◆Gemini 是多模态的，意味着它可以理解、操作和结合不同类型的信息，包括文本、代码、音频、图像和视频。
◆它还非常灵活，能够高效地运行在从数据中心到移动设备上的各种环境中。Gemini 的第一个版本，Gemini 1.0，针对三种不同的大小进行了优化：Gemini Ultra 用于高度复杂的任务，Gemini Pro 适用于广泛的任务，Gemini Nano 用于设备上的任务。

◆Gemini Ultra 在 32 个广泛使用的学术基准测试中的 30 个上超越了当前的最新成果，这些基准测试用于大型语言模型的研究和开发。它是第一个在 MMLU（大规模多任务语言理解）上超越人类专家的模型，MMLU 测试了世界知识和在 57 个科目（如数学、物理、历史、法律、医学和伦理）中的解决问题能力。

◆Gemini 1.0 被训练用于同时识别和理解文本、图像、音频等，使其在解释数学和物理等复杂科目的推理方面表现出色。它还可以理解、解释和生成流行编程语言（如 Python、Java、C++ 和 Go）中的高质量代码。

◆Google 使用其针对 AI 优化的基础设施和自家设计的 Tensor Processing Units (TPUs) v4 和 v5e 来训练 Gemini 1.0。公司还宣布了迄今为止最强大、最高效、最可扩展的 TPU 系统——Cloud TPU v5p，专为训练尖端 AI 模型而设计。

◆Gemini 1.0 现在正在逐步应用于各种产品和平台。它将用于 Google 的产品，如 Bard 和 Pixel，开发者和企业客户可以从 12 月 13 日起通过 Google AI Studio 或 Google Cloud Vertex AI 中的 Gemini API 访问 Gemini Pro。安卓开发者也将能够通过 AICore，在安卓 14 上使用 Gemini Nano 开发，该功能将从 Pixel 8 Pro 设备开始提供。

了解详情：https://blog.google/technology/ai/google-gemini-ai/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=GDMGemini#performance</title>
            <link>https://nitter.cz/op7418/status/1732420600428425362#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1732420600428425362#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 15:23:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google终于发布了传言中的强大多模态LLM Gemini，他们说这是迄今为止最强大的AI模型。从描述来看确实非常强大。<br />
Google CEO的介绍视频也翻译好了，下面模型是具体的介绍：<br />
<br />
◆Gemini 是多模态的，意味着它可以理解、操作和结合不同类型的信息，包括文本、代码、音频、图像和视频。<br />
◆它还非常灵活，能够高效地运行在从数据中心到移动设备上的各种环境中。Gemini 的第一个版本，Gemini 1.0，针对三种不同的大小进行了优化：Gemini Ultra 用于高度复杂的任务，Gemini Pro 适用于广泛的任务，Gemini Nano 用于设备上的任务。<br />
<br />
◆Gemini Ultra 在 32 个广泛使用的学术基准测试中的 30 个上超越了当前的最新成果，这些基准测试用于大型语言模型的研究和开发。它是第一个在 MMLU（大规模多任务语言理解）上超越人类专家的模型，MMLU 测试了世界知识和在 57 个科目（如数学、物理、历史、法律、医学和伦理）中的解决问题能力。<br />
<br />
◆Gemini 1.0 被训练用于同时识别和理解文本、图像、音频等，使其在解释数学和物理等复杂科目的推理方面表现出色。它还可以理解、解释和生成流行编程语言（如 Python、Java、C++ 和 Go）中的高质量代码。<br />
<br />
◆Google 使用其针对 AI 优化的基础设施和自家设计的 Tensor Processing Units (TPUs) v4 和 v5e 来训练 Gemini 1.0。公司还宣布了迄今为止最强大、最高效、最可扩展的 TPU 系统——Cloud TPU v5p，专为训练尖端 AI 模型而设计。<br />
<br />
◆Gemini 1.0 现在正在逐步应用于各种产品和平台。它将用于 Google 的产品，如 Bard 和 Pixel，开发者和企业客户可以从 12 月 13 日起通过 Google AI Studio 或 Google Cloud Vertex AI 中的 Gemini API 访问 Gemini Pro。安卓开发者也将能够通过 AICore，在安卓 14 上使用 Gemini Nano 开发，该功能将从 Pixel 8 Pro 设备开始提供。<br />
<br />
了解详情：<a href="https://blog.google/technology/ai/google-gemini-ai/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=GDMGemini#performance">blog.google/technology/ai/go…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI0MjAyOTg3NDg5NTY2NzIvcHUvaW1nL2J6cmJYYWNaVF91NG5sa1AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732435176720908458#m</id>
            <title>厉害了👍</title>
            <link>https://nitter.cz/dotey/status/1732435176720908458#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732435176720908458#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 16:21:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>厉害了👍</p>
<p><a href="https://nitter.cz/op7418/status/1732377907795013749#m">nitter.cz/op7418/status/1732377907795013749#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/IlllIllllIIl_/status/1732387641784762615#m</id>
            <title>RT by @dotey: 官方的比whisper.cpp慢五倍。。。不愧是苹果</title>
            <link>https://nitter.cz/IlllIllllIIl_/status/1732387641784762615#m</link>
            <guid isPermaLink="false">https://nitter.cz/IlllIllllIIl_/status/1732387641784762615#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 13:12:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>官方的比whisper.cpp慢五倍。。。不愧是苹果</p>
<p><a href="https://nitter.cz/dotey/status/1732285963647254663#m">nitter.cz/dotey/status/1732285963647254663#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FxdFJxOGJzQUVNQ3BrLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FxdFNJbWFRQUFkOUMxLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/indigo11/status/1732303417878643068#m</id>
            <title>RT by @dotey: Stripe Press 把芒格的《穷查理宝典》做了一个超级酷的电子版，可看、可听、可动还全免费！https://www.stripe.press/poor-charlies-almanack

一家做支付 API 的公司，如何让自己显得有品位？那就是干出版 http://press.stripe.com 的选品相当不错📖 记得以前 Ping++ 总结过 Stripe 成功的五个基本条件：

1. 开发者作为支付服务采购者阶层；
2. 大量中长尾客群的存在；
3. 基于 Visa 和 MasterCard 营造的高利润支付生态；
4. 没有工程师红利的欧美社会；
5. 贸易文明主宰下的企业协同文化；</title>
            <link>https://nitter.cz/indigo11/status/1732303417878643068#m</link>
            <guid isPermaLink="false">https://nitter.cz/indigo11/status/1732303417878643068#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 07:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stripe Press 把芒格的《穷查理宝典》做了一个超级酷的电子版，可看、可听、可动还全免费！<a href="https://www.stripe.press/poor-charlies-almanack">stripe.press/poor-charlies-a…</a><br />
<br />
一家做支付 API 的公司，如何让自己显得有品位？那就是干出版 <a href="http://press.stripe.com">press.stripe.com</a> 的选品相当不错📖 记得以前 Ping++ 总结过 Stripe 成功的五个基本条件：<br />
<br />
1. 开发者作为支付服务采购者阶层；<br />
2. 大量中长尾客群的存在；<br />
3. 基于 Visa 和 MasterCard 营造的高利润支付生态；<br />
4. 没有工程师红利的欧美社会；<br />
5. 贸易文明主宰下的企业协同文化；</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIzMDMzMjI5OTA5NTI0NDgvcHUvaW1nL0xiM204Zm1scWkzQ0ROTXUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732289154652791077#m</id>
            <title>这曹操还挺神似电视剧版的👍🏻</title>
            <link>https://nitter.cz/dotey/status/1732289154652791077#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732289154652791077#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 06:41:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这曹操还挺神似电视剧版的👍🏻</p>
<p><a href="https://nitter.cz/hanqing_me/status/1732265674624700693#m">nitter.cz/hanqing_me/status/1732265674624700693#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732288614157943151#m</id>
            <title>R to @dotey: 没有Swift版本吗？🥲</title>
            <link>https://nitter.cz/dotey/status/1732288614157943151#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732288614157943151#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 06:39:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>没有Swift版本吗？🥲</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732285963647254663#m</id>
            <title>#AI开源项目推荐：MLX

苹果刚刚发布了一个专门为苹果芯片定制的神经网络框架 MLX，类似于 PyTorch ，但是是针对苹果 M 系列芯片定制的。

MLX 的亮点在于设计了一个对于深度学习用户易于上手的API，并包含一些经典案例，比如 Llama、LoRa、Stable Diffusion 和 Whisper！

看来不需要再用 whisper.cpp、llama.cpp 这种靠 CPU 运行的框架了。

MLX 源代码：http://github.com/ml-explore/mlx</title>
            <link>https://nitter.cz/dotey/status/1732285963647254663#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732285963647254663#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 06:28:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：MLX<br />
<br />
苹果刚刚发布了一个专门为苹果芯片定制的神经网络框架 MLX，类似于 PyTorch ，但是是针对苹果 M 系列芯片定制的。<br />
<br />
MLX 的亮点在于设计了一个对于深度学习用户易于上手的API，并包含一些经典案例，比如 Llama、LoRa、Stable Diffusion 和 Whisper！<br />
<br />
看来不需要再用 whisper.cpp、llama.cpp 这种靠 CPU 运行的框架了。<br />
<br />
MLX 源代码：<a href="http://github.com/ml-explore/mlx">github.com/ml-explore/mlx</a></p>
<p><a href="https://nitter.cz/DrJimFan/status/1732261026815770912#m">nitter.cz/DrJimFan/status/1732261026815770912#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</id>
            <title>RT by @dotey: Generative Powers of Ten：基于文本的多尺度图像生成技术

是一种图像无限缩放技术，而且质量非常高清！

它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。

可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。

该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。

这项技术的关键特点包括：

- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。

- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。

- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。

- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。

- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。

多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。

该项目基于一种联合采样算法：

联合采样算法的核心特点

并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。

迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。

优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。

它使用了以下几个关键步骤和技术：

1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。

2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。

3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。

4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。

5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。

项目及演示：https://powers-of-10.github.io/
论文：https://arxiv.org/abs/2312.02149</title>
            <link>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732247057237500130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:54:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Generative Powers of Ten：基于文本的多尺度图像生成技术<br />
<br />
是一种图像无限缩放技术，而且质量非常高清！<br />
<br />
它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。<br />
<br />
可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。<br />
<br />
该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。<br />
<br />
这项技术的关键特点包括：<br />
<br />
- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。<br />
<br />
- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。<br />
<br />
- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。<br />
<br />
- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。<br />
<br />
- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。<br />
<br />
多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。<br />
<br />
该项目基于一种联合采样算法：<br />
<br />
联合采样算法的核心特点<br />
<br />
并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。<br />
<br />
迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。<br />
<br />
优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。<br />
<br />
它使用了以下几个关键步骤和技术：<br />
<br />
1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。<br />
<br />
2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。<br />
<br />
3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。<br />
<br />
4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。<br />
<br />
5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。<br />
<br />
项目及演示：<a href="https://powers-of-10.github.io/">powers-of-10.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02149">arxiv.org/abs/2312.02149</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIyNDMyNzc2NDUzMzY1NzYvcHUvaW1nL2ZwZ2pVYmN2Zkd2OV9WWFEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1732247223499641077#m</id>
            <title>RT by @dotey: 复旦大学张奇教授团队写了一本在线免费的电子书，《大规模语言模型：从理论到实践》，https://intro-llm.github.io，大概有 300 页篇幅，将大模型从理论到实战的每个阶段都描述的较为清楚。

全文在线阅读地址：https://intro-llm.github.io/chapter/LLM-TAP.pdf</title>
            <link>https://nitter.cz/Barret_China/status/1732247223499641077#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1732247223499641077#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:54:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>复旦大学张奇教授团队写了一本在线免费的电子书，《大规模语言模型：从理论到实践》，<a href="https://intro-llm.github.io">intro-llm.github.io</a>，大概有 300 页篇幅，将大模型从理论到实战的每个阶段都描述的较为清楚。<br />
<br />
全文在线阅读地址：<a href="https://intro-llm.github.io/chapter/LLM-TAP.pdf">intro-llm.github.io/chapter/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvczhYdGFJQUF0YVo0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvdG1DX2FrQUF4NXRsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/0xthefool/status/1731896006608838834#m</id>
            <title>RT by @dotey: 不得不承认O Reilly是技术书籍出版的大哥大，已经整理出来了一本 《用GPT-4与ChatGPT完成应用开发》的书籍，Amazon上面可以几十刀买一份实体书或者电子书，也可以在下面官网上注册账号免费读10天。

https://www.oreilly.com/library/view/developing-apps-with/9781098152475/</title>
            <link>https://nitter.cz/0xthefool/status/1731896006608838834#m</link>
            <guid isPermaLink="false">https://nitter.cz/0xthefool/status/1731896006608838834#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 04:39:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不得不承认O Reilly是技术书籍出版的大哥大，已经整理出来了一本 《用GPT-4与ChatGPT完成应用开发》的书籍，Amazon上面可以几十刀买一份实体书或者电子书，也可以在下面官网上注册账号免费读10天。<br />
<br />
<a href="https://www.oreilly.com/library/view/developing-apps-with/9781098152475/">oreilly.com/library/view/dev…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FqdUNSMGE0QUE4WU1WLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732219459526340887#m</id>
            <title>高斯头像 (GaussianAvatars)，可以高逼真还原演员表情、姿势到3D虚拟人头像上的技术

相关论文：高斯头像 (GaussianAvatars): 用 3D 高斯技术打造的逼真头部虚拟形象

摘要
我们推出了一种名为高斯头像 (GaussianAvatars) 的创新技术，用以制作不仅逼真而且可以完全操控表情、姿势和观看角度的头部虚拟形象。这一技术的核心在于一个基于 3D 高斯点的动态三维表示法，这些高斯点被配置在一个可参数化、可塑形的面部模型上。这种结合不仅实现了逼真的渲染效果，而且通过底层的参数化模型，实现了精确的动画控制。例如，可以通过从视频序列中传递表情或手动调整可塑形模型的参数来控制动画。我们利用三角形的局部坐标系对每个高斯点进行参数设置，并通过优化明确的位移偏移来实现更精准的几何表达。在头像重建过程中，我们采用端到端的方式同时优化可塑形模型参数和高斯点参数。我们还展示了这种逼真头像在多个具有挑战性的场景中的动画表现力。例如，在驱动视频的重现场景中，我们的方法相比现有技术有了显著的提升。

项目首页：https://shenhanqian.github.io/gaussian-avatars
论文：http://arxiv.org/abs/2312.02069</title>
            <link>https://nitter.cz/dotey/status/1732219459526340887#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732219459526340887#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 02:04:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>高斯头像 (GaussianAvatars)，可以高逼真还原演员表情、姿势到3D虚拟人头像上的技术<br />
<br />
相关论文：高斯头像 (GaussianAvatars): 用 3D 高斯技术打造的逼真头部虚拟形象<br />
<br />
摘要<br />
我们推出了一种名为高斯头像 (GaussianAvatars) 的创新技术，用以制作不仅逼真而且可以完全操控表情、姿势和观看角度的头部虚拟形象。这一技术的核心在于一个基于 3D 高斯点的动态三维表示法，这些高斯点被配置在一个可参数化、可塑形的面部模型上。这种结合不仅实现了逼真的渲染效果，而且通过底层的参数化模型，实现了精确的动画控制。例如，可以通过从视频序列中传递表情或手动调整可塑形模型的参数来控制动画。我们利用三角形的局部坐标系对每个高斯点进行参数设置，并通过优化明确的位移偏移来实现更精准的几何表达。在头像重建过程中，我们采用端到端的方式同时优化可塑形模型参数和高斯点参数。我们还展示了这种逼真头像在多个具有挑战性的场景中的动画表现力。例如，在驱动视频的重现场景中，我们的方法相比现有技术有了显著的提升。<br />
<br />
项目首页：<a href="https://shenhanqian.github.io/gaussian-avatars">shenhanqian.github.io/gaussi…</a><br />
论文：<a href="http://arxiv.org/abs/2312.02069">arxiv.org/abs/2312.02069</a></p>
<p><a href="https://nitter.cz/MattNiessner/status/1731799570177228975#m">nitter.cz/MattNiessner/status/1731799570177228975#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732205057989308825#m</id>
            <title>都7K多star了👍</title>
            <link>https://nitter.cz/dotey/status/1732205057989308825#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732205057989308825#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 01:07:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>都7K多star了👍</p>
<p><a href="https://nitter.cz/HiTw93/status/1732187345234075972#m">nitter.cz/HiTw93/status/1732187345234075972#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199373579203059#m</id>
            <title>R to @dotey: 原文：https://blogs.bing.com/search-quality-insights/december-2023/Continued-AI-Innovation-in-Copilot</title>
            <link>https://nitter.cz/dotey/status/1732199373579203059#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199373579203059#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:44:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原文：<a href="https://blogs.bing.com/search-quality-insights/december-2023/Continued-AI-Innovation-in-Copilot">blogs.bing.com/search-qualit…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199370928337134#m</id>
            <title>R to @dotey: 视频理解与问答 - Copilot in Edge。现在，您可以在 Edge 浏览器观看视频时，对视频进行总结或提问。例如，观看 Satya 的最新 Ignite 主题演讲的 YouTube 视频时，您可以让 Copilot 为您进行总结，如下面的截图所示。</title>
            <link>https://nitter.cz/dotey/status/1732199370928337134#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199370928337134#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:44:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频理解与问答 - Copilot in Edge。现在，您可以在 Edge 浏览器观看视频时，对视频进行总结或提问。例如，观看 Satya 的最新 Ignite 主题演讲的 YouTube 视频时，您可以让 Copilot 为您进行总结，如下面的截图所示。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQ0h3Y1dFQUFSQzJhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199255597531484#m</id>
            <title>R to @dotey: 代码解释器 - 我们正在研发新功能，帮助您完成更精确的计算、编码、数据分析、可视化、数学等复杂任务。我们正在向一批选定用户收集反馈，计划不久后广泛推出。

Copilot 将为您复杂的自然语言请求编写代码，在一个安全的沙盒环境中运行代码，并利用结果提供更高质量的回答。您还可以向 Copilot 上传和下载文件，这样您就能利用自己的数据和代码，结合 Bing 搜索结果进行工作。

Copilot 强大的 Python 环境运行在基于 Azure 容器应用构建的安全沙盒中。它提供快速、隔离的用户环境，预装了多种流行的数据科学工具和库，如 pandas、numpy、matplotlib、sklearn、flask 等，以解决复杂问题。您可以将 Copilot 通过 Bing 搜索和网页获取的数据与上传的文件数据结合使用，以获得深入的洞察和精美的交互式输出。</title>
            <link>https://nitter.cz/dotey/status/1732199255597531484#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199255597531484#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:44:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>代码解释器 - 我们正在研发新功能，帮助您完成更精确的计算、编码、数据分析、可视化、数学等复杂任务。我们正在向一批选定用户收集反馈，计划不久后广泛推出。<br />
<br />
Copilot 将为您复杂的自然语言请求编写代码，在一个安全的沙盒环境中运行代码，并利用结果提供更高质量的回答。您还可以向 Copilot 上传和下载文件，这样您就能利用自己的数据和代码，结合 Bing 搜索结果进行工作。<br />
<br />
Copilot 强大的 Python 环境运行在基于 Azure 容器应用构建的安全沙盒中。它提供快速、隔离的用户环境，预装了多种流行的数据科学工具和库，如 pandas、numpy、matplotlib、sklearn、flask 等，以解决复杂问题。您可以将 Copilot 通过 Bing 搜索和网页获取的数据与上传的文件数据结合使用，以获得深入的洞察和精美的交互式输出。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FvQ0JoY1d3QUFKS3FwLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBb0NCaGNXd0FBSktxcC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199152430293201#m</id>
            <title>R to @dotey: 效果确实令人印象深刻。如下例所示，传统的多模态系统只能大致描述图片内容，但有了搜索基础，我们能够精准识别出具体的航天飞机及其发射日期。</title>
            <link>https://nitter.cz/dotey/status/1732199152430293201#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199152430293201#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>效果确实令人印象深刻。如下例所示，传统的多模态系统只能大致描述图片内容，但有了搜索基础，我们能够精准识别出具体的航天飞机及其发射日期。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FvQjcxMlgwQUFZVm9vLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBb0I3MTJYMEFBWVZvby5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199040182374578#m</id>
            <title>R to @dotey: 多模态与搜索基础 – 我们结合了 GPT-4 的强大能力和视觉技术，通过 Bing 图像搜索和网络搜索数据，提升了对您查询内容的图像理解。这项新技能不久后就会推出。

看看我们如何升级 Prometheus 以实现多模态：</title>
            <link>https://nitter.cz/dotey/status/1732199040182374578#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199040182374578#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多模态与搜索基础 – 我们结合了 GPT-4 的强大能力和视觉技术，通过 Bing 图像搜索和网络搜索数据，提升了对您查询内容的图像理解。这项新技能不久后就会推出。<br />
<br />
看看我们如何升级 Prometheus 以实现多模态：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQjE1UVhFQUE0YlhYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732198994145591383#m</id>
            <title>Bing 官方博客发布的 Copilot 更新：

Copilot的持续AI革新

今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。

GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。

全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 http://bing.com/create 或指令 Copilot 制作图片，即刻体验这些功能。

欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</title>
            <link>https://nitter.cz/dotey/status/1732198994145591383#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732198994145591383#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Bing 官方博客发布的 Copilot 更新：<br />
<br />
Copilot的持续AI革新<br />
<br />
今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。<br />
<br />
GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。<br />
<br />
全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 <a href="http://bing.com/create">bing.com/create</a> 或指令 Copilot 制作图片，即刻体验这些功能。<br />
<br />
欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQndwLVhBQUFLak1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732168397989789747#m</id>
            <title>这篇关于提示工程、RAGs 与微调对比的推文不错，它这个坐标图分成了两个坐标轴：
1. 所需要外部知识多少
2. 所需要的对模型定制的多少

原推文翻译如下：

提示工程、RAGs 与微调的对比：

这是每位搭建基于大语言模型（LLM）应用的 AI 工程师都面临的关键选择。

要理解这个决策的指导原则，我们首先得明白这些术语的含义。

1️⃣ 提示工程：

所谓提示，指的是你输入的文本，大语言模型就根据这个输入来生成回应。

这实际上是一种精确的输入方法，旨在引导模型产生相应的输出。

模型的输出将基于其已有的知识。

2️⃣ RAGs（检索增强生成）：

当你将提示工程与数据库查询结合，以获得含丰富上下文的答案时，这就是所谓的 RAG。

生成的输出将基于数据库中现有的知识。

3️⃣ 微调：

微调是指使用特定任务的数据调整大语言模型的参数，使其在某一领域内专业化。

比如，一个语言模型可以在医学文献上进行微调，从而更擅长回答健康护理相关的问题。

这就好比对一位已经技艺娴熟的工人进行额外培训，让他们在特定领域成为专家。

那么，我们如何决定采取哪种方法呢？

（阅读下文时请参考下面的图片）

❗️有两个关键的指导参数，一个是对外部知识的需求，另一个是模型适应性的需求。

❗️尽管前者的含义较为明确，模型适应性则意味着改变模型的行为、词汇、写作风格等。

例如，一个预训练的大语言模型可能在总结公司会议记录时遇到挑战，因为会议中可能穿插了一些特定的内部术语。

🔹因此，微调更多的是关于改变结构（行为）而非知识，而对于 RAGs 则正好相反。

🔸当你需要生成基于定制知识库的输出，同时保持大语言模型的词汇和写作风格不变时，你可以选择使用 RAGs。

🔹如果你不需要上述任一功能，那么提示工程就是你的选择。

🔸如果你的应用既需要定制知识又需要改变模型的行为，那么采用混合方案（RAGs + 微调）将是更佳选择。</title>
            <link>https://nitter.cz/dotey/status/1732168397989789747#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732168397989789747#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 22:41:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇关于提示工程、RAGs 与微调对比的推文不错，它这个坐标图分成了两个坐标轴：<br />
1. 所需要外部知识多少<br />
2. 所需要的对模型定制的多少<br />
<br />
原推文翻译如下：<br />
<br />
提示工程、RAGs 与微调的对比：<br />
<br />
这是每位搭建基于大语言模型（LLM）应用的 AI 工程师都面临的关键选择。<br />
<br />
要理解这个决策的指导原则，我们首先得明白这些术语的含义。<br />
<br />
1️⃣ 提示工程：<br />
<br />
所谓提示，指的是你输入的文本，大语言模型就根据这个输入来生成回应。<br />
<br />
这实际上是一种精确的输入方法，旨在引导模型产生相应的输出。<br />
<br />
模型的输出将基于其已有的知识。<br />
<br />
2️⃣ RAGs（检索增强生成）：<br />
<br />
当你将提示工程与数据库查询结合，以获得含丰富上下文的答案时，这就是所谓的 RAG。<br />
<br />
生成的输出将基于数据库中现有的知识。<br />
<br />
3️⃣ 微调：<br />
<br />
微调是指使用特定任务的数据调整大语言模型的参数，使其在某一领域内专业化。<br />
<br />
比如，一个语言模型可以在医学文献上进行微调，从而更擅长回答健康护理相关的问题。<br />
<br />
这就好比对一位已经技艺娴熟的工人进行额外培训，让他们在特定领域成为专家。<br />
<br />
那么，我们如何决定采取哪种方法呢？<br />
<br />
（阅读下文时请参考下面的图片）<br />
<br />
❗️有两个关键的指导参数，一个是对外部知识的需求，另一个是模型适应性的需求。<br />
<br />
❗️尽管前者的含义较为明确，模型适应性则意味着改变模型的行为、词汇、写作风格等。<br />
<br />
例如，一个预训练的大语言模型可能在总结公司会议记录时遇到挑战，因为会议中可能穿插了一些特定的内部术语。<br />
<br />
🔹因此，微调更多的是关于改变结构（行为）而非知识，而对于 RAGs 则正好相反。<br />
<br />
🔸当你需要生成基于定制知识库的输出，同时保持大语言模型的词汇和写作风格不变时，你可以选择使用 RAGs。<br />
<br />
🔹如果你不需要上述任一功能，那么提示工程就是你的选择。<br />
<br />
🔸如果你的应用既需要定制知识又需要改变模型的行为，那么采用混合方案（RAGs + 微调）将是更佳选择。</p>
<p><a href="https://nitter.cz/akshay_pachaar/status/1732014719794585684#m">nitter.cz/akshay_pachaar/status/1732014719794585684#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>