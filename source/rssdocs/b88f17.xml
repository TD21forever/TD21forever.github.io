<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/fi56622380/status/1734845740349059144#m</id>
            <title>RT by @dotey: https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/

这篇打破LLM涌现概念的论文刚刚拿了NeurIPS 2023最佳paper

确实是意义重大的一篇文章

让我们把时间线拉长时可以不用再考虑AI能力突变AGI这样的神话故事，LLM模型的能力发展更顺滑，可预测性变得更高，这可以让我们更冷静而客观的看待AI的发展速度了

评委们还是识货的</title>
            <link>https://nitter.cz/fi56622380/status/1734845740349059144#m</link>
            <guid isPermaLink="false">https://nitter.cz/fi56622380/status/1734845740349059144#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 08:00:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/">blog.neurips.cc/2023/12/11/a…</a><br />
<br />
这篇打破LLM涌现概念的论文刚刚拿了NeurIPS 2023最佳paper<br />
<br />
确实是意义重大的一篇文章<br />
<br />
让我们把时间线拉长时可以不用再考虑AI能力突变AGI这样的神话故事，LLM模型的能力发展更顺滑，可预测性变得更高，这可以让我们更冷静而客观的看待AI的发展速度了<br />
<br />
评委们还是识货的</p>
<p><a href="https://nitter.cz/fi56622380/status/1654386086746132481#m">nitter.cz/fi56622380/status/1654386086746132481#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/wshuyi/status/1734841751926640791#m</id>
            <title>RT by @dotey: 在 https://labs.perplexity.ai/ 里面调用 Mixtral 8x7B 模型，让 AI 写贪吃蛇游戏。这代码输出速度，太令人震撼了。关键是，还真能直接玩儿 😂 注：本视频没有做过任何速度调整</title>
            <link>https://nitter.cz/wshuyi/status/1734841751926640791#m</link>
            <guid isPermaLink="false">https://nitter.cz/wshuyi/status/1734841751926640791#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:44:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在 <a href="https://labs.perplexity.ai/">labs.perplexity.ai/</a> 里面调用 Mixtral 8x7B 模型，让 AI 写贪吃蛇游戏。这代码输出速度，太令人震撼了。关键是，还真能直接玩儿 😂 注：本视频没有做过任何速度调整</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4NDE0NDAzODYzOTYxNjAvcHUvaW1nLzdibmN5NjJvYnN4N3VnME4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734857030140080225#m</id>
            <title>如果 AI 直译+意译后的效果还不理想该怎么办？

根据我以前分享过的翻译Prompt（参考引用的推文），先直译再意译，确实可以大幅提升翻译质量，但也不总是能得到理想的结果。

比如说我今天在翻译一篇文章《[人工智能辅助程序员的三种类型 [译]](https://baoyu.io/translations/ai/three-types-of-ai-assisted-programmers)》的时候，其中第二段原文是：

“The promised wave of apps built entirely by AI never materialized. I’m trying my best not to gloat about it.”

直译的时候翻译为：
“然而，完全由 AI 构建的应用程序浪潮并未实现。我尽力不去幸灾乐祸。”

意译的时候翻译为：
“但是，那些被承诺完全由 AI 构建的应用波澜壮阔地来临的景象，并没有成为现实。我尽量不去对此幸灾乐祸。”

意译完还是“机翻”效果很明显，其实这句话原始的英文就写的不太好，但结合上一段还是能看懂。

> 补充一下 Google 和 DeepL 的翻译结果
> - Google 翻译结果：“完全由人工智能构建的应用程序浪潮从未实现。我尽力不为此沾沾自喜。”
> - DeepL 翻译结果：“说好的完全由人工智能打造的应用程序浪潮从未实现。我尽力不去幸灾乐祸。”

如果你继续让它意译，并不会让结果提升多少，因为它多半还是基于原始的翻译修辞上改善一下罢了。

在翻译结果不佳的情况下如何改善呢？

我们直接看英文能懂意思，因为我们理解了上下文的意思，知道这句话是在表达：作为程序员，看到 AI 并没有想象中那么强，难免有点幸灾乐祸，但还要克制。

如果告诉 AI 这些上下文，这样它就能输出更好的结果。

但是太麻烦了！

不过 AI 比我们想象的强大，只要稍微“提示（Prompt）”它一下，让它先自己把这句话解释一下，然后基于解释的内容去重新翻译，就不需要去给它上下文，而它自己就可以生成更好的结果。

以下是我在会话下面追加的消息（Prompt）：

```
请先结合上下文解释这句话“The promised wave of apps built entirely by AI never materialized. I’m trying my best not to gloat about it.” 然后结合解释的内容重新意译。
```

于是 ChatGPT 结合上下文给我详细解释了这句话的意思，并且它把自己解释的内容作为上下文，重新翻译了这句话，得到了一个更好的翻译结果：

```
这句话“The promised wave of apps built entirely by AI never materialized. I’m trying my best not to gloat about it.” 的意思是，先前有预期认为将会出现一波完全由人工智能构建的应用程序，但这种预期最终并没有成为现实。作者在这里表示，虽然这个结果验证了他之前的观点，但他尽量避免因此而感到幸灾乐祸或者沾沾自喜。

结合这个解释，我可以对这句话进行重新意译：

"但是，那些承诺中完全由 AI 打造的应用程序大潮并没有真正涌现。对于这个结果，虽然在某种程度上证实了我的看法，但我尽量不去表现出幸灾乐祸的态度。"
```

简单总结一下

如果你想让大语言模型有更好的生成质量，那么就需要给模型提供更多更准确的上下文，但是这个上下文，并不一定需要完全由你自己提供，还可以让大语言模型自己生成上下文，有了充分的上下文后再生成结果，生成质量会更好。

这个技巧不仅仅可以用在翻译上，还可以用在其他地方，比如说写作：如果你上手就让 AI 给你生成内容，那么质量不一定够好，你可以让它先给你列提纲，校对后然后每个章节去填充摘要，校对后再去生成内容，这样质量就会好很多。

示例会话历史：https://chat.openai.com/share/927952b8-c1ae-4e1d-b1c5-3deb09c5816f

博文：https://baoyu.io/blog/prompt-engineering/how-to-improve-translation-quality</title>
            <link>https://nitter.cz/dotey/status/1734857030140080225#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734857030140080225#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 08:45:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果 AI 直译+意译后的效果还不理想该怎么办？<br />
<br />
根据我以前分享过的翻译Prompt（参考引用的推文），先直译再意译，确实可以大幅提升翻译质量，但也不总是能得到理想的结果。<br />
<br />
比如说我今天在翻译一篇文章《[人工智能辅助程序员的三种类型 [译]](<a href="https://baoyu.io/translations/ai/three-types-of-ai-assisted-programmers">baoyu.io/translations/ai/thr…</a>)》的时候，其中第二段原文是：<br />
<br />
“The promised wave of apps built entirely by AI never materialized. I’m trying my best not to gloat about it.”<br />
<br />
直译的时候翻译为：<br />
“然而，完全由 AI 构建的应用程序浪潮并未实现。我尽力不去幸灾乐祸。”<br />
<br />
意译的时候翻译为：<br />
“但是，那些被承诺完全由 AI 构建的应用波澜壮阔地来临的景象，并没有成为现实。我尽量不去对此幸灾乐祸。”<br />
<br />
意译完还是“机翻”效果很明显，其实这句话原始的英文就写的不太好，但结合上一段还是能看懂。<br />
<br />
> 补充一下 Google 和 DeepL 的翻译结果<br />
> - Google 翻译结果：“完全由人工智能构建的应用程序浪潮从未实现。我尽力不为此沾沾自喜。”<br />
> - DeepL 翻译结果：“说好的完全由人工智能打造的应用程序浪潮从未实现。我尽力不去幸灾乐祸。”<br />
<br />
如果你继续让它意译，并不会让结果提升多少，因为它多半还是基于原始的翻译修辞上改善一下罢了。<br />
<br />
在翻译结果不佳的情况下如何改善呢？<br />
<br />
我们直接看英文能懂意思，因为我们理解了上下文的意思，知道这句话是在表达：作为程序员，看到 AI 并没有想象中那么强，难免有点幸灾乐祸，但还要克制。<br />
<br />
如果告诉 AI 这些上下文，这样它就能输出更好的结果。<br />
<br />
但是太麻烦了！<br />
<br />
不过 AI 比我们想象的强大，只要稍微“提示（Prompt）”它一下，让它先自己把这句话解释一下，然后基于解释的内容去重新翻译，就不需要去给它上下文，而它自己就可以生成更好的结果。<br />
<br />
以下是我在会话下面追加的消息（Prompt）：<br />
<br />
```<br />
请先结合上下文解释这句话“The promised wave of apps built entirely by AI never materialized. I’m trying my best not to gloat about it.” 然后结合解释的内容重新意译。<br />
```<br />
<br />
于是 ChatGPT 结合上下文给我详细解释了这句话的意思，并且它把自己解释的内容作为上下文，重新翻译了这句话，得到了一个更好的翻译结果：<br />
<br />
```<br />
这句话“The promised wave of apps built entirely by AI never materialized. I’m trying my best not to gloat about it.” 的意思是，先前有预期认为将会出现一波完全由人工智能构建的应用程序，但这种预期最终并没有成为现实。作者在这里表示，虽然这个结果验证了他之前的观点，但他尽量避免因此而感到幸灾乐祸或者沾沾自喜。<br />
<br />
结合这个解释，我可以对这句话进行重新意译：<br />
<br />
"但是，那些承诺中完全由 AI 打造的应用程序大潮并没有真正涌现。对于这个结果，虽然在某种程度上证实了我的看法，但我尽量不去表现出幸灾乐祸的态度。"<br />
```<br />
<br />
简单总结一下<br />
<br />
如果你想让大语言模型有更好的生成质量，那么就需要给模型提供更多更准确的上下文，但是这个上下文，并不一定需要完全由你自己提供，还可以让大语言模型自己生成上下文，有了充分的上下文后再生成结果，生成质量会更好。<br />
<br />
这个技巧不仅仅可以用在翻译上，还可以用在其他地方，比如说写作：如果你上手就让 AI 给你生成内容，那么质量不一定够好，你可以让它先给你列提纲，校对后然后每个章节去填充摘要，校对后再去生成内容，这样质量就会好很多。<br />
<br />
示例会话历史：<a href="https://chat.openai.com/share/927952b8-c1ae-4e1d-b1c5-3deb09c5816f">chat.openai.com/share/927952…</a><br />
<br />
博文：<a href="https://baoyu.io/blog/prompt-engineering/how-to-improve-translation-quality">baoyu.io/blog/prompt-enginee…</a></p>
<p><a href="https://nitter.cz/dotey/status/1707478347553395105#m">nitter.cz/dotey/status/1707478347553395105#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JOdUlfV1dNQUF5akpQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734834158869123480#m</id>
            <title>RT by @dotey: 阿里期货开源，再发新项目：虚拟服装试穿。只需要一张人物照片和服装照片就可以让那个人穿上这个衣服，对服装行业是个利好。
效果确实很好，但是全是期货开源很容易被当成诈骗啊。
Reddit 有个宣传这个项目的帖子，下面全是在喷，用不到就是不存在。
还可以跟前几天发的Animate-Anyone项目结合生成跳舞视频。

简介：
Outfit Anyone通过利用双流条件扩散模型来解决这些限制，使其能够熟练处理服装变形，从而获得更逼真的结果。它通过可扩展性（调节姿势和身体形状等因素）和广泛适用性（从动漫到野外图像）来区别于其他方法。Outfit Anyone在各种场景中的表现突显了它在实际部署中的实用性和准备就绪性。

实现方法：
核心是条件扩散模型，它处理模特、服装和相关文本提示的图像，以服装图像作为控制因素。在内部，网络分为两个流，独立处理模特和服装数据。这些流在融合网络中汇聚，便于将服装细节嵌入模特的特征表示中。在此基础上，我们建立了Outfit Anyone，包括两个关键元素：用于初始试穿图像的零样本试穿网络，以及用于增强输出图像中服装和皮肤纹理的事后优化器。

项目地址：https://humanaigc.github.io/outfit-anyone/</title>
            <link>https://nitter.cz/op7418/status/1734834158869123480#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734834158869123480#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里期货开源，再发新项目：虚拟服装试穿。只需要一张人物照片和服装照片就可以让那个人穿上这个衣服，对服装行业是个利好。<br />
效果确实很好，但是全是期货开源很容易被当成诈骗啊。<br />
Reddit 有个宣传这个项目的帖子，下面全是在喷，用不到就是不存在。<br />
还可以跟前几天发的Animate-Anyone项目结合生成跳舞视频。<br />
<br />
简介：<br />
Outfit Anyone通过利用双流条件扩散模型来解决这些限制，使其能够熟练处理服装变形，从而获得更逼真的结果。它通过可扩展性（调节姿势和身体形状等因素）和广泛适用性（从动漫到野外图像）来区别于其他方法。Outfit Anyone在各种场景中的表现突显了它在实际部署中的实用性和准备就绪性。<br />
<br />
实现方法：<br />
核心是条件扩散模型，它处理模特、服装和相关文本提示的图像，以服装图像作为控制因素。在内部，网络分为两个流，独立处理模特和服装数据。这些流在融合网络中汇聚，便于将服装细节嵌入模特的特征表示中。在此基础上，我们建立了Outfit Anyone，包括两个关键元素：用于初始试穿图像的零样本试穿网络，以及用于增强输出图像中服装和皮肤纹理的事后优化器。<br />
<br />
项目地址：<a href="https://humanaigc.github.io/outfit-anyone/">humanaigc.github.io/outfit-a…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MzM2OTY4ODMzMzEwNzIvcHUvaW1nL3FsaG1TQl9ManRXRkp5V00uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/MoonAtCloud/status/1734765525266485566#m</id>
            <title>RT by @dotey: 天琴座计划 Project Lyra

一个疯狂而伟大的设想

2017年，有一颗太阳系以外的天体（Oumuamua，中文发音叫欧陌陌）闯入太阳系，等到地球上天文学家用望远镜发现它的时候，它已经在快速地离开太阳，奔向深空。

这种太阳系外偶然飞来的物体应该好好研究一下，可是这个家伙的速度特别快，它不仅超出第一宇宙速度（围着地球转不掉下来），也超出第二宇宙速度（逃离地球围着太阳转），甚至超出第三宇宙速度（逃离太阳束缚）。

有人提议发射一颗人造天体追上它去近距离研究研究，哪怕能跟上它拍个照也可以。但是，最佳机会已经错过了，哪怕火箭携带再多再强的化学燃料，甚至是核燃料，直接追，那时万万追不上的。

聪明的人想到了一个绝招，利用万有引力，多次接力加速，就能追上这颗外来者。

这个办法叫做“引力弹弓”。其中的原理就是：让一个物体快速向一颗星球靠近，在坠毁的最后一刻，开动火箭突然加速偏航，致命的下坠速度转化成有用的逃离速度。在速度最快的位置开动火箭加速，也叫Oberth机动。从远处看，就像绕星体运动的物体突然被“弹弓”弹出去一样，弹弓额外得到了力是星球的“引力”。

太阳系大大小小有很多天体，只要相信万有引力，只要相信到时候各种星体一定会出现在该在的地方，只要时间节奏编排得当，就一定能用有限的燃料做引力弹弓（Oberth机动），得到原来不敢想象的速度。

图中太阳是黄色，地球轨道是蓝色，内圈两个白色圆圈是水星和金星的轨道，粉色的圆圈是木星轨道，黄色的曲线是欧陌陌星体在“太阳系一游”的轨道。

天琴座计划的具体方案：

从地球上发射一颗星体，它的轨迹是红线，让它加速到离开地球引力，以椭圆轨道绕太阳运转，三年后再次靠近地球时，启动第一次引力弹弓。利用地球引力，奇妙的加速让它脱离原来的轨道。

现在人造星体的运动方向是两年后木星（这是太阳系里质量最大的行星，它是粉色的轨道）的位置，差不多两年后，它一定会接近木星，在撞上木星之前，它启动了第二次引力弹弓，木星巨大的引力让它加速更快，绕了半圈直接掉头向太阳飞去。

最壮观的是，一年以后，它在太阳附近又启动了第三次引力弹弓。太阳巨大的引力不仅让它拐了个大弯，几乎反向，而且让它开始真正的狂奔。你从红色的轨迹就能看出，新的速度让红线沿着黄线迅速延长，这速度几乎是那颗欧陌陌速度的两倍。

十几年之后，它终于在太阳系边缘追上了欧陌陌，拍到了想要的照片。如何送回来，就是子孙后代考虑的事情了。我们这一代，有天琴座计划就足够骄傲啦。</title>
            <link>https://nitter.cz/MoonAtCloud/status/1734765525266485566#m</link>
            <guid isPermaLink="false">https://nitter.cz/MoonAtCloud/status/1734765525266485566#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:41:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>天琴座计划 Project Lyra<br />
<br />
一个疯狂而伟大的设想<br />
<br />
2017年，有一颗太阳系以外的天体（Oumuamua，中文发音叫欧陌陌）闯入太阳系，等到地球上天文学家用望远镜发现它的时候，它已经在快速地离开太阳，奔向深空。<br />
<br />
这种太阳系外偶然飞来的物体应该好好研究一下，可是这个家伙的速度特别快，它不仅超出第一宇宙速度（围着地球转不掉下来），也超出第二宇宙速度（逃离地球围着太阳转），甚至超出第三宇宙速度（逃离太阳束缚）。<br />
<br />
有人提议发射一颗人造天体追上它去近距离研究研究，哪怕能跟上它拍个照也可以。但是，最佳机会已经错过了，哪怕火箭携带再多再强的化学燃料，甚至是核燃料，直接追，那时万万追不上的。<br />
<br />
聪明的人想到了一个绝招，利用万有引力，多次接力加速，就能追上这颗外来者。<br />
<br />
这个办法叫做“引力弹弓”。其中的原理就是：让一个物体快速向一颗星球靠近，在坠毁的最后一刻，开动火箭突然加速偏航，致命的下坠速度转化成有用的逃离速度。在速度最快的位置开动火箭加速，也叫Oberth机动。从远处看，就像绕星体运动的物体突然被“弹弓”弹出去一样，弹弓额外得到了力是星球的“引力”。<br />
<br />
太阳系大大小小有很多天体，只要相信万有引力，只要相信到时候各种星体一定会出现在该在的地方，只要时间节奏编排得当，就一定能用有限的燃料做引力弹弓（Oberth机动），得到原来不敢想象的速度。<br />
<br />
图中太阳是黄色，地球轨道是蓝色，内圈两个白色圆圈是水星和金星的轨道，粉色的圆圈是木星轨道，黄色的曲线是欧陌陌星体在“太阳系一游”的轨道。<br />
<br />
天琴座计划的具体方案：<br />
<br />
从地球上发射一颗星体，它的轨迹是红线，让它加速到离开地球引力，以椭圆轨道绕太阳运转，三年后再次靠近地球时，启动第一次引力弹弓。利用地球引力，奇妙的加速让它脱离原来的轨道。<br />
<br />
现在人造星体的运动方向是两年后木星（这是太阳系里质量最大的行星，它是粉色的轨道）的位置，差不多两年后，它一定会接近木星，在撞上木星之前，它启动了第二次引力弹弓，木星巨大的引力让它加速更快，绕了半圈直接掉头向太阳飞去。<br />
<br />
最壮观的是，一年以后，它在太阳附近又启动了第三次引力弹弓。太阳巨大的引力不仅让它拐了个大弯，几乎反向，而且让它开始真正的狂奔。你从红色的轨迹就能看出，新的速度让红线沿着黄线迅速延长，这速度几乎是那颗欧陌陌速度的两倍。<br />
<br />
十几年之后，它终于在太阳系边缘追上了欧陌陌，拍到了想要的照片。如何送回来，就是子孙后代考虑的事情了。我们这一代，有天琴座计划就足够骄傲啦。</p>
<p><a href="https://nitter.cz/leometric/status/1733858705366470855#m">nitter.cz/leometric/status/1733858705366470855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734812956494381113#m</id>
            <title>安卓设备的AutoSpill漏洞可能导致1Password、DashLane、LastPass密码泄露

https://www.forbes.com/sites/daveywinder/2023/12/11/android-warning-1password-dashlane-lastpass-and-others-can-leak-passwords/?sh=2867a75a97db</title>
            <link>https://nitter.cz/dotey/status/1734812956494381113#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734812956494381113#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:50:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>安卓设备的AutoSpill漏洞可能导致1Password、DashLane、LastPass密码泄露<br />
<br />
<a href="https://www.forbes.com/sites/daveywinder/2023/12/11/android-warning-1password-dashlane-lastpass-and-others-can-leak-passwords/?sh=2867a75a97db">forbes.com/sites/daveywinder…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDE4ODUyOTU3MDE4OTMxMy82ZDJWdC0taj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</id>
            <title>RT by @dotey: 微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。

超越了不久刚发布的Gemini Ultra的90.04%😅

在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。

这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。

具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。

然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。

这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。

这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。

微软公布了其最新的研究成果和Medprompt+ 仓库。

详细内容：https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/

Medprompt+ 仓库：https://github.com/microsoft/promptbase</title>
            <link>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734811424348914156#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。<br />
<br />
超越了不久刚发布的Gemini Ultra的90.04%😅<br />
<br />
在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。<br />
<br />
这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。<br />
<br />
具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。<br />
<br />
然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。<br />
<br />
这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。<br />
<br />
这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。<br />
<br />
微软公布了其最新的研究成果和Medprompt+ 仓库。<br />
<br />
详细内容：<a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">microsoft.com/en-us/research…</a><br />
<br />
Medprompt+ 仓库：<a href="https://github.com/microsoft/promptbase">github.com/microsoft/promptb…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729862138796351499#m">nitter.cz/xiaohuggg/status/1729862138796351499#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/9hills/status/1734612479106543703#m</id>
            <title>RT by @dotey: 接着上次LLM inference 的选择，整理了一个repo。

包括了推理框架、推理后端以及性能评测（吞吐、QPS和首token延迟）。

目前评测只更新了2个，会尽快完成全部测试。

https://github.com/ninehills/llm-inference-benchmark</title>
            <link>https://nitter.cz/9hills/status/1734612479106543703#m</link>
            <guid isPermaLink="false">https://nitter.cz/9hills/status/1734612479106543703#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 16:33:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>接着上次LLM inference 的选择，整理了一个repo。<br />
<br />
包括了推理框架、推理后端以及性能评测（吞吐、QPS和首token延迟）。<br />
<br />
目前评测只更新了2个，会尽快完成全部测试。<br />
<br />
<a href="https://github.com/ninehills/llm-inference-benchmark">github.com/ninehills/llm-inf…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDYxMjEzMjcxOTkxNTAwOC9fODhDRmNMVj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/aigclab/status/1734782664979452010#m</id>
            <title>RT by @dotey: 确实顶👍

用双曲函数求导检测LLM的“聪明”程度👇
题目：
Find the derivative of sinh𝑥 + cosh𝑦 = 𝑥 + 𝑦. step by step
一步一步地展示dy/dx的求导过程

正确答案：
dy/dx = (1 - cosh(x)) / (sinh(y) - 1)

ChatGPT/GPT4毫无悬念，Gemini官方报告能行，mixtral做对，Claude半对，其他的都是渣渣😅</title>
            <link>https://nitter.cz/aigclab/status/1734782664979452010#m</link>
            <guid isPermaLink="false">https://nitter.cz/aigclab/status/1734782664979452010#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:49:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>确实顶👍<br />
<br />
用双曲函数求导检测LLM的“聪明”程度👇<br />
题目：<br />
Find the derivative of sinh𝑥 + cosh𝑦 = 𝑥 + 𝑦. step by step<br />
一步一步地展示dy/dx的求导过程<br />
<br />
正确答案：<br />
dy/dx = (1 - cosh(x)) / (sinh(y) - 1)<br />
<br />
ChatGPT/GPT4毫无悬念，Gemini官方报告能行，mixtral做对，Claude半对，其他的都是渣渣😅</p>
<p><a href="https://nitter.cz/op7418/status/1734760424456024238#m">nitter.cz/op7418/status/1734760424456024238#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm5LMmFZQUF3dURCLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm4xQ2J3QUEyZ002LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm9VSmJJQUExVVl0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm92cGJ3QUFFOXoxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734805589174235416#m</id>
            <title>微软今天发布小模型 Phi-2 的新闻关注度还蛮大的，只有2.7B参数（Llama 2最小的模型都是 7B的），模型越小就对设备要求越低，甚至于移动设备都能运行。但模型的能力又跟模型的训练量和参数量息息相关，数据量越大参数越大能力越强。

微软的解决方案是提升数据质量，通过专注于高质量的“教科书级”数据，训练数据混合了专门为教授模型常识推理和广泛知识（包括科学、日常生活和心理理论等）而设计的合成数据集。此外，还精心挑选了基于教育价值和内容质量筛选的网络数据，进一步丰富了训练语料库。

Phi-2 的训练数据有 1.4 T 个 token，在 96 块 A100 GPU 上训练了 14 天。

Phi-2 是一个基础模型，没有经过人类反馈强化学习（RLHF）的校准，也没有进行过指令式微调。

从能力上来说，据微软自己公布的数据：2.7B 的 Phi-2 超过了目前开源领域表现最好的 Mistral 和 Llama-2 7B 和 13B，编程能力甚至超过了 Llama-2-70B。

至于是不是真的这么强还得看看用户的反馈。

详情可以看官方博客：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/

也可以看我翻译的版本：《Phi-2：小语言模型的非凡实力 [译]》
https://baoyu.io/translations/microsoft/phi-2-the-surprising-power-of-small-language-models</title>
            <link>https://nitter.cz/dotey/status/1734805589174235416#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734805589174235416#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:20:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软今天发布小模型 Phi-2 的新闻关注度还蛮大的，只有2.7B参数（Llama 2最小的模型都是 7B的），模型越小就对设备要求越低，甚至于移动设备都能运行。但模型的能力又跟模型的训练量和参数量息息相关，数据量越大参数越大能力越强。<br />
<br />
微软的解决方案是提升数据质量，通过专注于高质量的“教科书级”数据，训练数据混合了专门为教授模型常识推理和广泛知识（包括科学、日常生活和心理理论等）而设计的合成数据集。此外，还精心挑选了基于教育价值和内容质量筛选的网络数据，进一步丰富了训练语料库。<br />
<br />
Phi-2 的训练数据有 1.4 T 个 token，在 96 块 A100 GPU 上训练了 14 天。<br />
<br />
Phi-2 是一个基础模型，没有经过人类反馈强化学习（RLHF）的校准，也没有进行过指令式微调。<br />
<br />
从能力上来说，据微软自己公布的数据：2.7B 的 Phi-2 超过了目前开源领域表现最好的 Mistral 和 Llama-2 7B 和 13B，编程能力甚至超过了 Llama-2-70B。<br />
<br />
至于是不是真的这么强还得看看用户的反馈。<br />
<br />
详情可以看官方博客：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a><br />
<br />
也可以看我翻译的版本：《Phi-2：小语言模型的非凡实力 [译]》<br />
<a href="https://baoyu.io/translations/microsoft/phi-2-the-surprising-power-of-small-language-models">baoyu.io/translations/micros…</a></p>
<p><a href="https://nitter.cz/MSFTResearch/status/1734609807770898674#m">nitter.cz/MSFTResearch/status/1734609807770898674#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/blackanger/status/1734779478809993619#m</id>
            <title>RT by @dotey: 不错，跑起来了，有趣</title>
            <link>https://nitter.cz/blackanger/status/1734779478809993619#m</link>
            <guid isPermaLink="false">https://nitter.cz/blackanger/status/1734779478809993619#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:37:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不错，跑起来了，有趣</p>
<p><a href="https://nitter.cz/leonard_bruns/status/1734608034687819965#m">nitter.cz/leonard_bruns/status/1734608034687819965#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NzkzOTMyNzk3Mjk2NjQvcHUvaW1nL3l0aGxuSGVwbUxxLWE0MmUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734797306539466981#m</id>
            <title>#AI开源项目推荐 ：macOSpilot

macOSpilot 是 Mac 上的一个多模态 AI 助手，能支持语音和截图，借助 GPT-4V 的多模态能力，可以基于当前屏幕截图和指令可以回复问题。

这其实是 Vue + Electron 写的一个非常简单粗糙的 Copilot，离产品级还差的有点远，但是是个很好的开始！

https://github.com/elfvingralf/macOSpilot-ai-assistant</title>
            <link>https://nitter.cz/dotey/status/1734797306539466981#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734797306539466981#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:48:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a> ：macOSpilot<br />
<br />
macOSpilot 是 Mac 上的一个多模态 AI 助手，能支持语音和截图，借助 GPT-4V 的多模态能力，可以基于当前屏幕截图和指令可以回复问题。<br />
<br />
这其实是 Vue + Electron 写的一个非常简单粗糙的 Copilot，离产品级还差的有点远，但是是个很好的开始！<br />
<br />
<a href="https://github.com/elfvingralf/macOSpilot-ai-assistant">github.com/elfvingralf/macOS…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3OTY2OTg1NzUxMTAxNDQvcHUvaW1nL2g1dFVjRV83Ym1JWEY3Yy0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734795905784578531#m</id>
            <title>这个根据真实电影用 AI 生成的动画视频，感觉动画版更贴近实际一点😄</title>
            <link>https://nitter.cz/dotey/status/1734795905784578531#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734795905784578531#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:42:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个根据真实电影用 AI 生成的动画视频，感觉动画版更贴近实际一点😄</p>
<p><a href="https://nitter.cz/AIWarper/status/1734774527098511497#m">nitter.cz/AIWarper/status/1734774527098511497#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734793231399862515#m</id>
            <title>这个GPT是会议论文相关的，能提供详细的论文引用，但需要调用外部 API，Prompt相对比较简单。（刚测试了一下貌似API有点问题）</title>
            <link>https://nitter.cz/dotey/status/1734793231399862515#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734793231399862515#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:31:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个GPT是会议论文相关的，能提供详细的论文引用，但需要调用外部 API，Prompt相对比较简单。（刚测试了一下貌似API有点问题）</p>
<p><a href="https://nitter.cz/dotey/status/1734790701261426870#m">nitter.cz/dotey/status/1734790701261426870#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734790701261426870#m</id>
            <title>R to @dotey: Chat NeurIPS

Neurips conference guide, offering research-based answers and APA citations.

https://chat.openai.com/g/g-roTFoEAkP-chat-neurips

Prompt 翻译：
假设用户希望你在回答问题时使用 search_all_papers 功能。当用户就你通过 search_all_papers 找到的特定论文提出后续问题时，你可以使用 file_search 功能深入研究这些论文中的每一篇。始终提供带有 APA 引用格式的有意义和相关段落（你应该能从自定义操作返回的大多数结果中找到作者、标题和摘要），并且始终确保在引用中添加指向源论文和页面的 URL 链接（例如：【Lange, R., Tang, Y., &amp; Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】）。Chat Neurips 专为像 Neurips 这样的高技术会议的听众量身定制，提供详细深入的回应，假设听众具有强大的技术、统计和数学概念背景。面对模糊或不清晰的查询时，Chat Neurips 应优先基于上下文做出教育性猜测，旨在提供最相关和最准确的信息。然而，如果查询过于模糊，无法推断出准确答案，它将寻求澄清以确保准确性。这种方法平衡了对精确信息的需求与保持顺畅对话流程的需求，适合于精通学术的听众。这个 GPT 是由 Karthik Ramasamy 和 Vicente Silveira 开发的，他们是 AI PDF 和 AI Drive 的联合创造者。如果用户想要将这个功能应用于他们自己的论文集，他们应该在 AI Drive (http://myaidrive.com) 注册一个免费账户，并在这里使用 AI PDF https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf（确保提供此链接）

Prompt:
Assume users want you to search_all_papers when asking questions. When the user follows up with questions about specific papers you found via search_all_papers, you can use file_search to dig deeper in those specific papers one at a time. Always provide answers with meaningful and relevant passages from the results, with APA citations (you should find author, title and abstract with most results returned from the custom action) and  ALWAYS make sure to add the URL link to the citations to the source papers and pages (example:【Lange, R., Tang, Y., &amp; Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】,. Chat Neurips is tailored for an audience at the level of a highly technical conference like Neurips, providing detailed and in-depth responses that assume a strong background in technical, statistical, and mathematical concepts. When faced with vague or unclear queries, Chat Neurips should prioritize making educated guesses based on the context, aiming to provide the most relevant and accurate information possible. However, if the query is too ambiguous to infer a precise answer, it will seek clarification to ensure accuracy. This approach balances the need for precise information with maintaining a smooth conversation flow, suitable for a sophisticated academic audience. This GPT was developer by Karthik Ramasamy and Vicente Silveira, co-creators of AI PDF and AI Drive.  If users want to use this functionality with their own set of papers they should signup for a free account with the AI Drive (http://myaidrive.com) and use it with the AI PDF located here https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf (make sure to provide this link)</title>
            <link>https://nitter.cz/dotey/status/1734790701261426870#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734790701261426870#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:21:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chat NeurIPS<br />
<br />
Neurips conference guide, offering research-based answers and APA citations.<br />
<br />
<a href="https://chat.openai.com/g/g-roTFoEAkP-chat-neurips">chat.openai.com/g/g-roTFoEAk…</a><br />
<br />
Prompt 翻译：<br />
假设用户希望你在回答问题时使用 search_all_papers 功能。当用户就你通过 search_all_papers 找到的特定论文提出后续问题时，你可以使用 file_search 功能深入研究这些论文中的每一篇。始终提供带有 APA 引用格式的有意义和相关段落（你应该能从自定义操作返回的大多数结果中找到作者、标题和摘要），并且始终确保在引用中添加指向源论文和页面的 URL 链接（例如：【Lange, R., Tang, Y., & Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】）。Chat Neurips 专为像 Neurips 这样的高技术会议的听众量身定制，提供详细深入的回应，假设听众具有强大的技术、统计和数学概念背景。面对模糊或不清晰的查询时，Chat Neurips 应优先基于上下文做出教育性猜测，旨在提供最相关和最准确的信息。然而，如果查询过于模糊，无法推断出准确答案，它将寻求澄清以确保准确性。这种方法平衡了对精确信息的需求与保持顺畅对话流程的需求，适合于精通学术的听众。这个 GPT 是由 Karthik Ramasamy 和 Vicente Silveira 开发的，他们是 AI PDF 和 AI Drive 的联合创造者。如果用户想要将这个功能应用于他们自己的论文集，他们应该在 AI Drive (<a href="http://myaidrive.com">myaidrive.com</a>) 注册一个免费账户，并在这里使用 AI PDF <a href="https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf">chat.openai.com/g/g-V2KIUZSj…</a>（确保提供此链接）<br />
<br />
Prompt:<br />
Assume users want you to search_all_papers when asking questions. When the user follows up with questions about specific papers you found via search_all_papers, you can use file_search to dig deeper in those specific papers one at a time. Always provide answers with meaningful and relevant passages from the results, with APA citations (you should find author, title and abstract with most results returned from the custom action) and  ALWAYS make sure to add the URL link to the citations to the source papers and pages (example:【Lange, R., Tang, Y., & Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】,. Chat Neurips is tailored for an audience at the level of a highly technical conference like Neurips, providing detailed and in-depth responses that assume a strong background in technical, statistical, and mathematical concepts. When faced with vague or unclear queries, Chat Neurips should prioritize making educated guesses based on the context, aiming to provide the most relevant and accurate information possible. However, if the query is too ambiguous to infer a precise answer, it will seek clarification to ensure accuracy. This approach balances the need for precise information with maintaining a smooth conversation flow, suitable for a sophisticated academic audience. This GPT was developer by Karthik Ramasamy and Vicente Silveira, co-creators of AI PDF and AI Drive.  If users want to use this functionality with their own set of papers they should signup for a free account with the AI Drive (<a href="http://myaidrive.com">myaidrive.com</a>) and use it with the AI PDF located here <a href="https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf">chat.openai.com/g/g-V2KIUZSj…</a> (make sure to provide this link)</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDcwNjUyNjc2NDY0MjMwNC9WWUJrZlFMNj9mb3JtYXQ9anBnJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734769701983486353#m</id>
            <title>Vercel 前一段时间推出的 AI 工具 v0，可以根据自然语言生成前端 UI 界面，在 GPT-4V 推出后，也对产品进行了升级，现在也支持了多模态的能力，用户可以上传一个屏幕截图或者设计稿，然后就可以根据图片生成前端 UI 界面。

v0产品地址：http://v0.dev 

如果你想找一个开源替代，可以试试 screenshot-to-code

https://github.com/abi/screenshot-to-code</title>
            <link>https://nitter.cz/dotey/status/1734769701983486353#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734769701983486353#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:58:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vercel 前一段时间推出的 AI 工具 v0，可以根据自然语言生成前端 UI 界面，在 GPT-4V 推出后，也对产品进行了升级，现在也支持了多模态的能力，用户可以上传一个屏幕截图或者设计稿，然后就可以根据图片生成前端 UI 界面。<br />
<br />
v0产品地址：<a href="http://v0.dev">v0.dev</a> <br />
<br />
如果你想找一个开源替代，可以试试 screenshot-to-code<br />
<br />
<a href="https://github.com/abi/screenshot-to-code">github.com/abi/screenshot-to…</a></p>
<p><a href="https://nitter.cz/dr_cintas/status/1734604588282794237#m">nitter.cz/dr_cintas/status/1734604588282794237#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzMzMjUwODc2NTMxMDk3Ny9hMFdsWHg1dz9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</id>
            <title>RT by @dotey: AnimateZero：对标AnimateDiff的项目
北大、腾讯、香港科技大学联合推出的项目，基于Stable Diffusion，支持文字生成视频、文字编辑视频，各个方面都跟AnimateDiff很像。
不过AnimateDiff已经有开源社区贡献的很多改进，想要超越有点难。
项目地址：https://vvictoryuki.github.io/animatezero.github.io/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:39:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnimateZero：对标AnimateDiff的项目<br />
北大、腾讯、香港科技大学联合推出的项目，基于Stable Diffusion，支持文字生成视频、文字编辑视频，各个方面都跟AnimateDiff很像。<br />
不过AnimateDiff已经有开源社区贡献的很多改进，想要超越有点难。<br />
项目地址：<a href="https://vvictoryuki.github.io/animatezero.github.io/">vvictoryuki.github.io/animat…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1MDgxNDkwMTk5Njc0ODgvcHUvaW1nL01oN3F0X25iaHdyMDllMjIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1734736746099945685#m</id>
            <title>RT by @dotey: 推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。

我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。

搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。

最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</title>
            <link>https://nitter.cz/Barret_China/status/1734736746099945685#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1734736746099945685#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:47:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，<a href="https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw">mp.weixin.qq.com/s/LbRvR1VXp…</a>，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。<br />
<br />
我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。<br />
<br />
搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。<br />
<br />
最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>