<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</id>
            <title>RT by @dotey: Generative Powers of Ten：基于文本的多尺度图像生成技术

是一种图像无限缩放技术，而且质量非常高清！

它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。

可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。

该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。

这项技术的关键特点包括：

- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。

- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。

- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。

- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。

- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。

多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。

该项目基于一种联合采样算法：

联合采样算法的核心特点

并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。

迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。

优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。

它使用了以下几个关键步骤和技术：

1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。

2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。

3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。

4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。

5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。

项目及演示：https://powers-of-10.github.io/
论文：https://arxiv.org/abs/2312.02149</title>
            <link>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732247057237500130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:54:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Generative Powers of Ten：基于文本的多尺度图像生成技术<br />
<br />
是一种图像无限缩放技术，而且质量非常高清！<br />
<br />
它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。<br />
<br />
可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。<br />
<br />
该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。<br />
<br />
这项技术的关键特点包括：<br />
<br />
- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。<br />
<br />
- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。<br />
<br />
- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。<br />
<br />
- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。<br />
<br />
- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。<br />
<br />
多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。<br />
<br />
该项目基于一种联合采样算法：<br />
<br />
联合采样算法的核心特点<br />
<br />
并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。<br />
<br />
迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。<br />
<br />
优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。<br />
<br />
它使用了以下几个关键步骤和技术：<br />
<br />
1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。<br />
<br />
2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。<br />
<br />
3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。<br />
<br />
4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。<br />
<br />
5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。<br />
<br />
项目及演示：<a href="https://powers-of-10.github.io/">powers-of-10.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02149">arxiv.org/abs/2312.02149</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIyNDMyNzc2NDUzMzY1NzYvcHUvaW1nL2ZwZ2pVYmN2Zkd2OV9WWFEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1732247223499641077#m</id>
            <title>RT by @dotey: 复旦大学张奇教授团队写了一本在线免费的电子书，《大规模语言模型：从理论到实践》，https://intro-llm.github.io，大概有 300 页篇幅，将大模型从理论到实战的每个阶段都描述的较为清楚。

全文在线阅读地址：https://intro-llm.github.io/chapter/LLM-TAP.pdf</title>
            <link>https://nitter.cz/Barret_China/status/1732247223499641077#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1732247223499641077#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:54:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>复旦大学张奇教授团队写了一本在线免费的电子书，《大规模语言模型：从理论到实践》，<a href="https://intro-llm.github.io">intro-llm.github.io</a>，大概有 300 页篇幅，将大模型从理论到实战的每个阶段都描述的较为清楚。<br />
<br />
全文在线阅读地址：<a href="https://intro-llm.github.io/chapter/LLM-TAP.pdf">intro-llm.github.io/chapter/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvczhYdGFJQUF0YVo0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvdG1DX2FrQUF4NXRsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/0xthefool/status/1731896006608838834#m</id>
            <title>RT by @dotey: 不得不承认O Reilly是技术书籍出版的大哥大，已经整理出来了一本 《用GPT-4与ChatGPT完成应用开发》的书籍，Amazon上面可以几十刀买一份实体书或者电子书，也可以在下面官网上注册账号免费读10天。

https://www.oreilly.com/library/view/developing-apps-with/9781098152475/</title>
            <link>https://nitter.cz/0xthefool/status/1731896006608838834#m</link>
            <guid isPermaLink="false">https://nitter.cz/0xthefool/status/1731896006608838834#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 04:39:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不得不承认O Reilly是技术书籍出版的大哥大，已经整理出来了一本 《用GPT-4与ChatGPT完成应用开发》的书籍，Amazon上面可以几十刀买一份实体书或者电子书，也可以在下面官网上注册账号免费读10天。<br />
<br />
<a href="https://www.oreilly.com/library/view/developing-apps-with/9781098152475/">oreilly.com/library/view/dev…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FqdUNSMGE0QUE4WU1WLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732219459526340887#m</id>
            <title>高斯头像 (GaussianAvatars)，可以高逼真还原演员表情、姿势到3D虚拟人头像上的技术

相关论文：高斯头像 (GaussianAvatars): 用 3D 高斯技术打造的逼真头部虚拟形象

摘要
我们推出了一种名为高斯头像 (GaussianAvatars) 的创新技术，用以制作不仅逼真而且可以完全操控表情、姿势和观看角度的头部虚拟形象。这一技术的核心在于一个基于 3D 高斯点的动态三维表示法，这些高斯点被配置在一个可参数化、可塑形的面部模型上。这种结合不仅实现了逼真的渲染效果，而且通过底层的参数化模型，实现了精确的动画控制。例如，可以通过从视频序列中传递表情或手动调整可塑形模型的参数来控制动画。我们利用三角形的局部坐标系对每个高斯点进行参数设置，并通过优化明确的位移偏移来实现更精准的几何表达。在头像重建过程中，我们采用端到端的方式同时优化可塑形模型参数和高斯点参数。我们还展示了这种逼真头像在多个具有挑战性的场景中的动画表现力。例如，在驱动视频的重现场景中，我们的方法相比现有技术有了显著的提升。

项目首页：https://shenhanqian.github.io/gaussian-avatars
论文：http://arxiv.org/abs/2312.02069</title>
            <link>https://nitter.cz/dotey/status/1732219459526340887#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732219459526340887#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 02:04:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>高斯头像 (GaussianAvatars)，可以高逼真还原演员表情、姿势到3D虚拟人头像上的技术<br />
<br />
相关论文：高斯头像 (GaussianAvatars): 用 3D 高斯技术打造的逼真头部虚拟形象<br />
<br />
摘要<br />
我们推出了一种名为高斯头像 (GaussianAvatars) 的创新技术，用以制作不仅逼真而且可以完全操控表情、姿势和观看角度的头部虚拟形象。这一技术的核心在于一个基于 3D 高斯点的动态三维表示法，这些高斯点被配置在一个可参数化、可塑形的面部模型上。这种结合不仅实现了逼真的渲染效果，而且通过底层的参数化模型，实现了精确的动画控制。例如，可以通过从视频序列中传递表情或手动调整可塑形模型的参数来控制动画。我们利用三角形的局部坐标系对每个高斯点进行参数设置，并通过优化明确的位移偏移来实现更精准的几何表达。在头像重建过程中，我们采用端到端的方式同时优化可塑形模型参数和高斯点参数。我们还展示了这种逼真头像在多个具有挑战性的场景中的动画表现力。例如，在驱动视频的重现场景中，我们的方法相比现有技术有了显著的提升。<br />
<br />
项目首页：<a href="https://shenhanqian.github.io/gaussian-avatars">shenhanqian.github.io/gaussi…</a><br />
论文：<a href="http://arxiv.org/abs/2312.02069">arxiv.org/abs/2312.02069</a></p>
<p><a href="https://nitter.cz/MattNiessner/status/1731799570177228975#m">nitter.cz/MattNiessner/status/1731799570177228975#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732205057989308825#m</id>
            <title>都7K多star了👍</title>
            <link>https://nitter.cz/dotey/status/1732205057989308825#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732205057989308825#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 01:07:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>都7K多star了👍</p>
<p><a href="https://nitter.cz/HiTw93/status/1732187345234075972#m">nitter.cz/HiTw93/status/1732187345234075972#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199373579203059#m</id>
            <title>R to @dotey: 原文：https://blogs.bing.com/search-quality-insights/december-2023/Continued-AI-Innovation-in-Copilot</title>
            <link>https://nitter.cz/dotey/status/1732199373579203059#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199373579203059#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:44:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原文：<a href="https://blogs.bing.com/search-quality-insights/december-2023/Continued-AI-Innovation-in-Copilot">blogs.bing.com/search-qualit…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199370928337134#m</id>
            <title>R to @dotey: 视频理解与问答 - Copilot in Edge。现在，您可以在 Edge 浏览器观看视频时，对视频进行总结或提问。例如，观看 Satya 的最新 Ignite 主题演讲的 YouTube 视频时，您可以让 Copilot 为您进行总结，如下面的截图所示。</title>
            <link>https://nitter.cz/dotey/status/1732199370928337134#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199370928337134#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:44:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频理解与问答 - Copilot in Edge。现在，您可以在 Edge 浏览器观看视频时，对视频进行总结或提问。例如，观看 Satya 的最新 Ignite 主题演讲的 YouTube 视频时，您可以让 Copilot 为您进行总结，如下面的截图所示。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQ0h3Y1dFQUFSQzJhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199255597531484#m</id>
            <title>R to @dotey: 代码解释器 - 我们正在研发新功能，帮助您完成更精确的计算、编码、数据分析、可视化、数学等复杂任务。我们正在向一批选定用户收集反馈，计划不久后广泛推出。

Copilot 将为您复杂的自然语言请求编写代码，在一个安全的沙盒环境中运行代码，并利用结果提供更高质量的回答。您还可以向 Copilot 上传和下载文件，这样您就能利用自己的数据和代码，结合 Bing 搜索结果进行工作。

Copilot 强大的 Python 环境运行在基于 Azure 容器应用构建的安全沙盒中。它提供快速、隔离的用户环境，预装了多种流行的数据科学工具和库，如 pandas、numpy、matplotlib、sklearn、flask 等，以解决复杂问题。您可以将 Copilot 通过 Bing 搜索和网页获取的数据与上传的文件数据结合使用，以获得深入的洞察和精美的交互式输出。</title>
            <link>https://nitter.cz/dotey/status/1732199255597531484#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199255597531484#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:44:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>代码解释器 - 我们正在研发新功能，帮助您完成更精确的计算、编码、数据分析、可视化、数学等复杂任务。我们正在向一批选定用户收集反馈，计划不久后广泛推出。<br />
<br />
Copilot 将为您复杂的自然语言请求编写代码，在一个安全的沙盒环境中运行代码，并利用结果提供更高质量的回答。您还可以向 Copilot 上传和下载文件，这样您就能利用自己的数据和代码，结合 Bing 搜索结果进行工作。<br />
<br />
Copilot 强大的 Python 环境运行在基于 Azure 容器应用构建的安全沙盒中。它提供快速、隔离的用户环境，预装了多种流行的数据科学工具和库，如 pandas、numpy、matplotlib、sklearn、flask 等，以解决复杂问题。您可以将 Copilot 通过 Bing 搜索和网页获取的数据与上传的文件数据结合使用，以获得深入的洞察和精美的交互式输出。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FvQ0JoY1d3QUFKS3FwLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBb0NCaGNXd0FBSktxcC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199152430293201#m</id>
            <title>R to @dotey: 效果确实令人印象深刻。如下例所示，传统的多模态系统只能大致描述图片内容，但有了搜索基础，我们能够精准识别出具体的航天飞机及其发射日期。</title>
            <link>https://nitter.cz/dotey/status/1732199152430293201#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199152430293201#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>效果确实令人印象深刻。如下例所示，传统的多模态系统只能大致描述图片内容，但有了搜索基础，我们能够精准识别出具体的航天飞机及其发射日期。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FvQjcxMlgwQUFZVm9vLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBb0I3MTJYMEFBWVZvby5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732199040182374578#m</id>
            <title>R to @dotey: 多模态与搜索基础 – 我们结合了 GPT-4 的强大能力和视觉技术，通过 Bing 图像搜索和网络搜索数据，提升了对您查询内容的图像理解。这项新技能不久后就会推出。

看看我们如何升级 Prometheus 以实现多模态：</title>
            <link>https://nitter.cz/dotey/status/1732199040182374578#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732199040182374578#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多模态与搜索基础 – 我们结合了 GPT-4 的强大能力和视觉技术，通过 Bing 图像搜索和网络搜索数据，提升了对您查询内容的图像理解。这项新技能不久后就会推出。<br />
<br />
看看我们如何升级 Prometheus 以实现多模态：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQjE1UVhFQUE0YlhYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732198994145591383#m</id>
            <title>Bing 官方博客发布的 Copilot 更新：

Copilot的持续AI革新

今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。

GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。

全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 http://bing.com/create 或指令 Copilot 制作图片，即刻体验这些功能。

欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</title>
            <link>https://nitter.cz/dotey/status/1732198994145591383#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732198994145591383#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Bing 官方博客发布的 Copilot 更新：<br />
<br />
Copilot的持续AI革新<br />
<br />
今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。<br />
<br />
GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。<br />
<br />
全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 <a href="http://bing.com/create">bing.com/create</a> 或指令 Copilot 制作图片，即刻体验这些功能。<br />
<br />
欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQndwLVhBQUFLak1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732168397989789747#m</id>
            <title>这篇关于提示工程、RAGs 与微调对比的推文不错，它这个坐标图分成了两个坐标轴：
1. 所需要外部知识多少
2. 所需要的对模型定制的多少

原推文翻译如下：

提示工程、RAGs 与微调的对比：

这是每位搭建基于大语言模型（LLM）应用的 AI 工程师都面临的关键选择。

要理解这个决策的指导原则，我们首先得明白这些术语的含义。

1️⃣ 提示工程：

所谓提示，指的是你输入的文本，大语言模型就根据这个输入来生成回应。

这实际上是一种精确的输入方法，旨在引导模型产生相应的输出。

模型的输出将基于其已有的知识。

2️⃣ RAGs（检索增强生成）：

当你将提示工程与数据库查询结合，以获得含丰富上下文的答案时，这就是所谓的 RAG。

生成的输出将基于数据库中现有的知识。

3️⃣ 微调：

微调是指使用特定任务的数据调整大语言模型的参数，使其在某一领域内专业化。

比如，一个语言模型可以在医学文献上进行微调，从而更擅长回答健康护理相关的问题。

这就好比对一位已经技艺娴熟的工人进行额外培训，让他们在特定领域成为专家。

那么，我们如何决定采取哪种方法呢？

（阅读下文时请参考下面的图片）

❗️有两个关键的指导参数，一个是对外部知识的需求，另一个是模型适应性的需求。

❗️尽管前者的含义较为明确，模型适应性则意味着改变模型的行为、词汇、写作风格等。

例如，一个预训练的大语言模型可能在总结公司会议记录时遇到挑战，因为会议中可能穿插了一些特定的内部术语。

🔹因此，微调更多的是关于改变结构（行为）而非知识，而对于 RAGs 则正好相反。

🔸当你需要生成基于定制知识库的输出，同时保持大语言模型的词汇和写作风格不变时，你可以选择使用 RAGs。

🔹如果你不需要上述任一功能，那么提示工程就是你的选择。

🔸如果你的应用既需要定制知识又需要改变模型的行为，那么采用混合方案（RAGs + 微调）将是更佳选择。</title>
            <link>https://nitter.cz/dotey/status/1732168397989789747#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732168397989789747#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 22:41:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这篇关于提示工程、RAGs 与微调对比的推文不错，它这个坐标图分成了两个坐标轴：<br />
1. 所需要外部知识多少<br />
2. 所需要的对模型定制的多少<br />
<br />
原推文翻译如下：<br />
<br />
提示工程、RAGs 与微调的对比：<br />
<br />
这是每位搭建基于大语言模型（LLM）应用的 AI 工程师都面临的关键选择。<br />
<br />
要理解这个决策的指导原则，我们首先得明白这些术语的含义。<br />
<br />
1️⃣ 提示工程：<br />
<br />
所谓提示，指的是你输入的文本，大语言模型就根据这个输入来生成回应。<br />
<br />
这实际上是一种精确的输入方法，旨在引导模型产生相应的输出。<br />
<br />
模型的输出将基于其已有的知识。<br />
<br />
2️⃣ RAGs（检索增强生成）：<br />
<br />
当你将提示工程与数据库查询结合，以获得含丰富上下文的答案时，这就是所谓的 RAG。<br />
<br />
生成的输出将基于数据库中现有的知识。<br />
<br />
3️⃣ 微调：<br />
<br />
微调是指使用特定任务的数据调整大语言模型的参数，使其在某一领域内专业化。<br />
<br />
比如，一个语言模型可以在医学文献上进行微调，从而更擅长回答健康护理相关的问题。<br />
<br />
这就好比对一位已经技艺娴熟的工人进行额外培训，让他们在特定领域成为专家。<br />
<br />
那么，我们如何决定采取哪种方法呢？<br />
<br />
（阅读下文时请参考下面的图片）<br />
<br />
❗️有两个关键的指导参数，一个是对外部知识的需求，另一个是模型适应性的需求。<br />
<br />
❗️尽管前者的含义较为明确，模型适应性则意味着改变模型的行为、词汇、写作风格等。<br />
<br />
例如，一个预训练的大语言模型可能在总结公司会议记录时遇到挑战，因为会议中可能穿插了一些特定的内部术语。<br />
<br />
🔹因此，微调更多的是关于改变结构（行为）而非知识，而对于 RAGs 则正好相反。<br />
<br />
🔸当你需要生成基于定制知识库的输出，同时保持大语言模型的词汇和写作风格不变时，你可以选择使用 RAGs。<br />
<br />
🔹如果你不需要上述任一功能，那么提示工程就是你的选择。<br />
<br />
🔸如果你的应用既需要定制知识又需要改变模型的行为，那么采用混合方案（RAGs + 微调）将是更佳选择。</p>
<p><a href="https://nitter.cz/akshay_pachaar/status/1732014719794585684#m">nitter.cz/akshay_pachaar/status/1732014719794585684#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732152449014571157#m</id>
            <title>多少年了……</title>
            <link>https://nitter.cz/dotey/status/1732152449014571157#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732152449014571157#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 21:38:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多少年了……</p>
<p><a href="https://nitter.cz/naman34/status/1732140580342509622#m">nitter.cz/naman34/status/1732140580342509622#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732151792102691277#m</id>
            <title>哈哈，到底Arxiv上的新论文有多少是发现了GPT-4的新Prompt呢？</title>
            <link>https://nitter.cz/dotey/status/1732151792102691277#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732151792102691277#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 21:35:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈，到底Arxiv上的新论文有多少是发现了GPT-4的新Prompt呢？</p>
<p><a href="https://nitter.cz/hbouammar/status/1731970658278469714#m">nitter.cz/hbouammar/status/1731970658278469714#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732092883883139313#m</id>
            <title>Meta 和 IBM正联合 40 多家公司和机构，共同成立一个专注于开源人工智能的行业组织，目的在于共享技术并减少相关风险。

这个名为 AI Alliance（AI 联盟）的组织，将着重于负责任地发展 AI 技术，其中包括安全性和防护工具，据周二发布的声明所述。该联盟还计划增加开源 AI 模型的数量，与此同时减少某些公司偏好的专有系统，发展新硬件，并与学术研究人员进行合作。

开源 AI 技术的倡导者认为，由开发者公开并供他人使用的技术，是培养高度复杂系统的一种更高效的方式。在过去几个月里，Meta 已经发布了其大语言模型（Large Language Model，LLM）的开源版本，这些模型是 AI 聊天机器人的核心基础。

Meta 全球事务总裁 Nick Clegg 在声明中表示：“我们相信，公开开发 AI 会更好——这样更多人能够享受到其带来的好处，创造创新产品，并致力于确保安全。”

该联盟将最终成立一个管理委员会和技术监督委员会。参与单位包括 Oracle Corp.（甲骨文公司）、Advanced Micro Devices Inc.（AMD）、Intel Corp.（英特尔公司）和 Stability AI，还有像圣母大学（University of Notre Dame）和马萨诸塞开放云联盟（Mass Open Cloud Alliance）这样的学术研究机构。

最近，OpenAI——即 ChatGPT 的开发者——因解雇并重新聘请其知名首席执行官而引发了一场全球性的辩论，关注点在于企业在开发强大的 AI 技术时应保持多大的透明度。值得注意的是，OpenAI 并未列为 AI 联盟的成员之一。

AI Alliance 官网：https://thealliance.ai/

新闻来源：https://www.bloomberg.com/news/articles/2023-12-05/nvidia-plans-network-of-chip-plants-in-japan-to-meet-ai-demand</title>
            <link>https://nitter.cz/dotey/status/1732092883883139313#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732092883883139313#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 17:41:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta 和 IBM正联合 40 多家公司和机构，共同成立一个专注于开源人工智能的行业组织，目的在于共享技术并减少相关风险。<br />
<br />
这个名为 AI Alliance（AI 联盟）的组织，将着重于负责任地发展 AI 技术，其中包括安全性和防护工具，据周二发布的声明所述。该联盟还计划增加开源 AI 模型的数量，与此同时减少某些公司偏好的专有系统，发展新硬件，并与学术研究人员进行合作。<br />
<br />
开源 AI 技术的倡导者认为，由开发者公开并供他人使用的技术，是培养高度复杂系统的一种更高效的方式。在过去几个月里，Meta 已经发布了其大语言模型（Large Language Model，LLM）的开源版本，这些模型是 AI 聊天机器人的核心基础。<br />
<br />
Meta 全球事务总裁 Nick Clegg 在声明中表示：“我们相信，公开开发 AI 会更好——这样更多人能够享受到其带来的好处，创造创新产品，并致力于确保安全。”<br />
<br />
该联盟将最终成立一个管理委员会和技术监督委员会。参与单位包括 Oracle Corp.（甲骨文公司）、Advanced Micro Devices Inc.（AMD）、Intel Corp.（英特尔公司）和 Stability AI，还有像圣母大学（University of Notre Dame）和马萨诸塞开放云联盟（Mass Open Cloud Alliance）这样的学术研究机构。<br />
<br />
最近，OpenAI——即 ChatGPT 的开发者——因解雇并重新聘请其知名首席执行官而引发了一场全球性的辩论，关注点在于企业在开发强大的 AI 技术时应保持多大的透明度。值得注意的是，OpenAI 并未列为 AI 联盟的成员之一。<br />
<br />
AI Alliance 官网：<a href="https://thealliance.ai/">thealliance.ai/</a><br />
<br />
新闻来源：<a href="https://www.bloomberg.com/news/articles/2023-12-05/nvidia-plans-network-of-chip-plants-in-japan-to-meet-ai-demand">bloomberg.com/news/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FtaEItYlhFQUF0QWV0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FtaE90bVdFQUE1V0J3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732085476977148310#m</id>
            <title>其实GPTs可以做到的，只是都是用自然语言（Prompt）描述工作流和分支，另外它可以根据不同情况调用不同的Action调用外部工具。

当然准确性稳定性目前还不如 if else</title>
            <link>https://nitter.cz/dotey/status/1732085476977148310#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732085476977148310#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 17:12:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>其实GPTs可以做到的，只是都是用自然语言（Prompt）描述工作流和分支，另外它可以根据不同情况调用不同的Action调用外部工具。<br />
<br />
当然准确性稳定性目前还不如 if else</p>
<p><a href="https://nitter.cz/buaaxhm/status/1731851320275984539#m">nitter.cz/buaaxhm/status/1731851320275984539#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/lxfater/status/1731344168524394681#m</id>
            <title>RT by @dotey: 纯浏览器端的segment-anythin 的性能还不错。

不过这样估计也开启新的功能，比如一键抠取图像中的物品，还有点击就能inpaint，甚至使用prompt来inpaint和抠图。

后续会这个功能，加入这个项目，欢迎大家关注，更新了也提醒大家。
https://github.com/lxfater/inpaint-web</title>
            <link>https://nitter.cz/lxfater/status/1731344168524394681#m</link>
            <guid isPermaLink="false">https://nitter.cz/lxfater/status/1731344168524394681#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 03 Dec 2023 16:06:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>纯浏览器端的segment-anythin 的性能还不错。<br />
<br />
不过这样估计也开启新的功能，比如一键抠取图像中的物品，还有点击就能inpaint，甚至使用prompt来inpaint和抠图。<br />
<br />
后续会这个功能，加入这个项目，欢迎大家关注，更新了也提醒大家。<br />
<a href="https://github.com/lxfater/inpaint-web">github.com/lxfater/inpaint-w…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzEzNDIzNTczMDY0MzM1MzYvcHUvaW1nL0UtTnc1OFJ5OURhZjRudlkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732076418903789628#m</id>
            <title>看到一篇有意思的文章：《人工智能是伟大的平衡器》——研究表明，ChatGPT 等工具的兴起对于工作表现不佳的员工来说是个好消息

作者先是以为 AI 会帮助新手能快速达到比较高的水平，然后拉平与高手之间的差距。但结果发现似乎现实并非如此。

> 事实可能并非如此简单。AI 减少工资不平等的另一个可能方式是，它可能会降低顶尖收入者的薪酬，而对底层工作者的工资提升不大。随着生产力的提升，企业主可能会选择将利润归于己有，降低薪资上限而非提高薪资底线。在这种情况下，尽管借助 AI 减少了收入不平等，但大家的总体收入可能都会有所下降。

> 就像 AI 通过商品化顶级插画师的才华而降低他们的薪酬一样，这与机械化织布机在工业革命期间摧毁手工织布工的生计如出一辙。

> 最近的人工智能（AI）研究似乎在暗示，雇主们应该聪明地选择低薪雇佣新手，同时淘汰那些高薪的资深大腕，这种策略类似于针对 ChatGPT 时代的“Moneyball”套利手段。然而，事实并非如此简单。过去一年中，我与多位高管进行了交流，他们在重新考虑团队配置时，没有一位提到要放弃他们的高薪员工。相反，他们中的许多人私下表示，他们计划采取完全相反的策略。由于 AI 越来越能够处理那些直接、明确的任务，他们打算减少刚毕业的初级员工的招聘，转而加大对能处理复杂难题的专家的投入。

最后引用作者在另一篇文章中写到的：中世纪时，农耕变得更加富有成效，但新技术带来的收益却很少惠及农民。
Farming became more productive in medieval times, but the gains from new technology rarely benefited peasants. 

如果有兴趣可以看原文：
AI is the great equalizer https://www.businessinsider.com/ai-productivity-boost-job-performance-inequality-economics-2023-11

中文翻译：https://baoyu.io/translations/ai/ai-productivity-boost-job-performance-inequality-economics</title>
            <link>https://nitter.cz/dotey/status/1732076418903789628#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732076418903789628#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 16:36:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看到一篇有意思的文章：《人工智能是伟大的平衡器》——研究表明，ChatGPT 等工具的兴起对于工作表现不佳的员工来说是个好消息<br />
<br />
作者先是以为 AI 会帮助新手能快速达到比较高的水平，然后拉平与高手之间的差距。但结果发现似乎现实并非如此。<br />
<br />
> 事实可能并非如此简单。AI 减少工资不平等的另一个可能方式是，它可能会降低顶尖收入者的薪酬，而对底层工作者的工资提升不大。随着生产力的提升，企业主可能会选择将利润归于己有，降低薪资上限而非提高薪资底线。在这种情况下，尽管借助 AI 减少了收入不平等，但大家的总体收入可能都会有所下降。<br />
<br />
> 就像 AI 通过商品化顶级插画师的才华而降低他们的薪酬一样，这与机械化织布机在工业革命期间摧毁手工织布工的生计如出一辙。<br />
<br />
> 最近的人工智能（AI）研究似乎在暗示，雇主们应该聪明地选择低薪雇佣新手，同时淘汰那些高薪的资深大腕，这种策略类似于针对 ChatGPT 时代的“Moneyball”套利手段。然而，事实并非如此简单。过去一年中，我与多位高管进行了交流，他们在重新考虑团队配置时，没有一位提到要放弃他们的高薪员工。相反，他们中的许多人私下表示，他们计划采取完全相反的策略。由于 AI 越来越能够处理那些直接、明确的任务，他们打算减少刚毕业的初级员工的招聘，转而加大对能处理复杂难题的专家的投入。<br />
<br />
最后引用作者在另一篇文章中写到的：中世纪时，农耕变得更加富有成效，但新技术带来的收益却很少惠及农民。<br />
Farming became more productive in medieval times, but the gains from new technology rarely benefited peasants. <br />
<br />
如果有兴趣可以看原文：<br />
AI is the great equalizer <a href="https://www.businessinsider.com/ai-productivity-boost-job-performance-inequality-economics-2023-11">businessinsider.com/ai-produ…</a><br />
<br />
中文翻译：<a href="https://baoyu.io/translations/ai/ai-productivity-boost-job-performance-inequality-economics">baoyu.io/translations/ai/ai-…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1731938685296513460#m</id>
            <title>RT by @dotey: 能够让大模型推理结果变得更好的基础优化手段已经非常多了，我梳理了常见的技术手段和对应的论文：

- Zero-shot：https://arxiv.org/abs/2109.01652
- Few-shot：https://arxiv.org/abs/2005.14165
- CoT：https://arxiv.org/abs/2201.11903
- ToT：https://arxiv.org/abs/2305.10601
- GoT：https://arxiv.org/abs/2308.09687
- SC：https://arxiv.org/abs/2203.11171
- Multi Persona：https://arxiv.org/abs/2307.05300
- Least to Most：https://arxiv.org/abs/2205.10625
- Step Back：https://arxiv.org/abs/2310.06117
- ART：https://arxiv.org/abs/2303.09014
- ReAct：https://arxiv.org/abs/2210.03629
- Reflection：https://arxiv.org/abs/2303.11366
- RAG：https://arxiv.org/abs/2005.11401

以上内容在之前的分享中均详细提到过，有一些只需要在 Prompt 上做简单优化便可看到效果；有一些则需要进行框架设计，如对任务进行规划、分解、组合等，包括与外界环境的交互、让人参与交互，存在一定的设计成本，市面上很多 XXXGPT 也是对这些基础手段组合后的工程实践。

学习这些知识的原理有助于帮助我们打开 LLM 推理黑盒，感兴趣的朋友不妨花点时间研究下，也欢迎留言补充更多有趣的技术和论文。</title>
            <link>https://nitter.cz/Barret_China/status/1731938685296513460#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1731938685296513460#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 07:28:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>能够让大模型推理结果变得更好的基础优化手段已经非常多了，我梳理了常见的技术手段和对应的论文：<br />
<br />
- Zero-shot：<a href="https://arxiv.org/abs/2109.01652">arxiv.org/abs/2109.01652</a><br />
- Few-shot：<a href="https://arxiv.org/abs/2005.14165">arxiv.org/abs/2005.14165</a><br />
- CoT：<a href="https://arxiv.org/abs/2201.11903">arxiv.org/abs/2201.11903</a><br />
- ToT：<a href="https://arxiv.org/abs/2305.10601">arxiv.org/abs/2305.10601</a><br />
- GoT：<a href="https://arxiv.org/abs/2308.09687">arxiv.org/abs/2308.09687</a><br />
- SC：<a href="https://arxiv.org/abs/2203.11171">arxiv.org/abs/2203.11171</a><br />
- Multi Persona：<a href="https://arxiv.org/abs/2307.05300">arxiv.org/abs/2307.05300</a><br />
- Least to Most：<a href="https://arxiv.org/abs/2205.10625">arxiv.org/abs/2205.10625</a><br />
- Step Back：<a href="https://arxiv.org/abs/2310.06117">arxiv.org/abs/2310.06117</a><br />
- ART：<a href="https://arxiv.org/abs/2303.09014">arxiv.org/abs/2303.09014</a><br />
- ReAct：<a href="https://arxiv.org/abs/2210.03629">arxiv.org/abs/2210.03629</a><br />
- Reflection：<a href="https://arxiv.org/abs/2303.11366">arxiv.org/abs/2303.11366</a><br />
- RAG：<a href="https://arxiv.org/abs/2005.11401">arxiv.org/abs/2005.11401</a><br />
<br />
以上内容在之前的分享中均详细提到过，有一些只需要在 Prompt 上做简单优化便可看到效果；有一些则需要进行框架设计，如对任务进行规划、分解、组合等，包括与外界环境的交互、让人参与交互，存在一定的设计成本，市面上很多 XXXGPT 也是对这些基础手段组合后的工程实践。<br />
<br />
学习这些知识的原理有助于帮助我们打开 LLM 推理黑盒，感兴趣的朋友不妨花点时间研究下，也欢迎留言补充更多有趣的技术和论文。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>