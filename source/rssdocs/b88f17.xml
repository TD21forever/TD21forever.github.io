<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727604343304495491#m</id>
            <title>建议你们看看这个推文和评论，很多有趣的讨论。

Jim Fan：
合成数据无疑将成为下一个万亿级高质量训练词元的来源。我相信，大多数专业的大语言模型团队都已意识到这一点。真正的挑战是如何持续保持这些数据的质量，避免过早陷入发展的停滞期。

Richard Sutton（@RichardSSutton）的《苦涩的教训》继续引领着人工智能的发展：只有两种模式能够随着计算能力的增加而无限扩展，那就是学习和搜索。这一观点在 2019 年他撰写时就成立，如今依旧适用，我敢说，直到我们解决通用人工智能的问题，这一点都不会改变。

Elon Musk：
确实。有些让人唏嘘的是，人类历史上写过的所有书籍的文字内容都能被存储在一个硬盘上。

而合成数据的数量将远远超出这个规模。

Jim Fan：
很多合成数据将源于具体代理，比如特斯拉 Optimus，前提是我们能够大规模地进行模拟。

Elon Musk：
是的（叹气）

Yann LeCun:
动物和人类凭借极少的训练数据就能迅速变得聪明。我认为，未来的趋势是开发出像动物和人类一样高效学习的新架构。目前使用大量数据（无论是真实的还是合成的）只是一种权宜之计，是因为我们现有方法的限制所迫。

Atlas3d.eth:
完全同意 - 看看乌鸦科的鸟类，它们拥有惊人的推理能力，却只需要很少的数据量。

Yann LeCun:
想象一下，像鹦鹉、狗和章鱼这样的动物大约有 20 亿个神经元。我们如何能让一台拥有 20 亿神经元、10 万亿参数的机器在短短几个月的实时训练中变得像这些动物一样聪明呢？

Eduardo Slonski:
1）我们使用了大量的数据。别忘了，我们不断接收大量的视频、音频和感官数据，更不用说 DNA 中编码的“指令”了。我们并非从零开始学习，而且我们的反应比大语言模型更为广泛和普遍。

2）在新架构的问题上，我同意你的看法。

Yann LeCun:
1. 人类基因组的数据量其实很小，只有 800MB。人类和黑猩猩基因组的差异也就 8MB。这远远不足以解释两者能力上的巨大差异。

2. 以一个两岁孩童所见的视觉数据量来看，实际上是相当有限的：大约 3200 万秒。我们有大约 200 万条视觉神经纤维，每秒大约传输 10 个字节，总计 6E14 字节。而大语言模型的训练数据量通常是 1E13 个 Token，约 2E13 字节，只多出大约 30 倍。

Eduardo Slonski:
基因组并不是作为数据来使用，而是起到指导作用。当我们开始训练模型时，其输出是随机的，但与人类不同，人类出生时已经有了本能的行为和寻找目标的能力。再次思考后，我更加认同你的观点：数据量并非关键，关键在于我们的架构以及针对该架构优化的数据。

Ferdous:
难道数百万年的进化适应不就相当于模型的预训练，而我们一生的经验不就类似于持续的微调吗？🤔

Yann LeCun:
并不完全是这样。这需要被浓缩成极少量的信息。比如，一个小型的 70 亿 Token 的大语言模型就需要 14GB。而你的整个基因组在未压缩的状态下也只有 800MB。人类和黑猩猩基因组之间仅有 1% 的差异，大约 8MB。这远远不足以编码复杂的结构。</title>
            <link>https://nitter.cz/dotey/status/1727604343304495491#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727604343304495491#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:25:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>建议你们看看这个推文和评论，很多有趣的讨论。<br />
<br />
Jim Fan：<br />
合成数据无疑将成为下一个万亿级高质量训练词元的来源。我相信，大多数专业的大语言模型团队都已意识到这一点。真正的挑战是如何持续保持这些数据的质量，避免过早陷入发展的停滞期。<br />
<br />
Richard Sutton（<a href="https://nitter.cz/RichardSSutton" title="Richard Sutton">@RichardSSutton</a>）的《苦涩的教训》继续引领着人工智能的发展：只有两种模式能够随着计算能力的增加而无限扩展，那就是学习和搜索。这一观点在 2019 年他撰写时就成立，如今依旧适用，我敢说，直到我们解决通用人工智能的问题，这一点都不会改变。<br />
<br />
Elon Musk：<br />
确实。有些让人唏嘘的是，人类历史上写过的所有书籍的文字内容都能被存储在一个硬盘上。<br />
<br />
而合成数据的数量将远远超出这个规模。<br />
<br />
Jim Fan：<br />
很多合成数据将源于具体代理，比如特斯拉 Optimus，前提是我们能够大规模地进行模拟。<br />
<br />
Elon Musk：<br />
是的（叹气）<br />
<br />
Yann LeCun:<br />
动物和人类凭借极少的训练数据就能迅速变得聪明。我认为，未来的趋势是开发出像动物和人类一样高效学习的新架构。目前使用大量数据（无论是真实的还是合成的）只是一种权宜之计，是因为我们现有方法的限制所迫。<br />
<br />
Atlas3d.eth:<br />
完全同意 - 看看乌鸦科的鸟类，它们拥有惊人的推理能力，却只需要很少的数据量。<br />
<br />
Yann LeCun:<br />
想象一下，像鹦鹉、狗和章鱼这样的动物大约有 20 亿个神经元。我们如何能让一台拥有 20 亿神经元、10 万亿参数的机器在短短几个月的实时训练中变得像这些动物一样聪明呢？<br />
<br />
Eduardo Slonski:<br />
1）我们使用了大量的数据。别忘了，我们不断接收大量的视频、音频和感官数据，更不用说 DNA 中编码的“指令”了。我们并非从零开始学习，而且我们的反应比大语言模型更为广泛和普遍。<br />
<br />
2）在新架构的问题上，我同意你的看法。<br />
<br />
Yann LeCun:<br />
1. 人类基因组的数据量其实很小，只有 800MB。人类和黑猩猩基因组的差异也就 8MB。这远远不足以解释两者能力上的巨大差异。<br />
<br />
2. 以一个两岁孩童所见的视觉数据量来看，实际上是相当有限的：大约 3200 万秒。我们有大约 200 万条视觉神经纤维，每秒大约传输 10 个字节，总计 6E14 字节。而大语言模型的训练数据量通常是 1E13 个 Token，约 2E13 字节，只多出大约 30 倍。<br />
<br />
Eduardo Slonski:<br />
基因组并不是作为数据来使用，而是起到指导作用。当我们开始训练模型时，其输出是随机的，但与人类不同，人类出生时已经有了本能的行为和寻找目标的能力。再次思考后，我更加认同你的观点：数据量并非关键，关键在于我们的架构以及针对该架构优化的数据。<br />
<br />
Ferdous:<br />
难道数百万年的进化适应不就相当于模型的预训练，而我们一生的经验不就类似于持续的微调吗？🤔<br />
<br />
Yann LeCun:<br />
并不完全是这样。这需要被浓缩成极少量的信息。比如，一个小型的 70 亿 Token 的大语言模型就需要 14GB。而你的整个基因组在未压缩的状态下也只有 800MB。人类和黑猩猩基因组之间仅有 1% 的差异，大约 8MB。这远远不足以编码复杂的结构。</p>
<p><a href="https://nitter.cz/DrJimFan/status/1727505774514180188#m">nitter.cz/DrJimFan/status/1727505774514180188#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727599590872928280#m</id>
            <title>《苦涩的教训》
Rich Sutton
2019 年 3 月 13 日
70 年人工智能研究给我们的最大启示是：依赖计算能力的通用方法最终表现最佳，而且优势明显。这背后的主要原因是摩尔定律，也就是计算成本持续以指数级下降。大部分 AI 研究都是在假设计算资源固定的情况下进行的（在这种情况下，利用人类知识几乎是提升性能的唯一途径），但实际上，在稍长于一般研究项目的时间里，可用的计算资源会大幅增加。研究者为了在短期内取得突破，尝试利用自己对特定领域的人类知识，但从长远看，关键在于计算能力的利用。这两者原本不必相互冲突，但在实践中却常常如此。投入其中一个领域的时间，就意味着在另一个上的缺失。此外，人们在一种方法上的投资也会形成心理承诺。而基于人类知识的方法往往会使系统变得复杂，不利于利用计算能力的通用方法。有很多例子显示 AI 研究者是如何迟迟才领悟到这个苦涩的教训，回顾这些案例非常有启发性。

以计算机国际象棋为例，1997 年击败世界冠军卡斯帕罗夫的方法主要是深度搜索。当时，大多数计算机国际象棋研究者对此表示失望，因为他们更倾向于利用对棋局特殊结构的人类理解。然而，当一个简单但基于搜索的方法，结合特殊的硬件和软件展现出巨大效能时，这些基于人类知识的研究者并不愿意接受失败。他们认为，尽管这次“蛮力”搜索获胜，但它并非一种通用策略，也不是人类下棋的方式。这些研究者本希望基于人类理解的方法能够取胜，对实际结果感到失望。

在计算机围棋的发展中，也出现了类似的模式，只是晚了 20 年。最初的努力都在于避免搜索，尽可能利用对游戏的人类理解和特殊特征，但一旦有效地应用了大规模搜索，这些努力都显得微不足道，甚至有害。在这个过程中，通过自我对弈学习价值函数（在很多其他游戏中也是这样，甚至包括国际象棋，尽管在 1997 年首次击败世界冠军的程序中学习的作用并不大）也非常关键。自我对弈学习和一般学习，就像搜索一样，能够充分利用大量计算资源。在计算机围棋和国际象棋中，研究者最初都是试图利用人类的理解来减少搜索的需要，但最终通过接受搜索和学习才取得了巨大的成功。

在语音识别领域，1970 年代由 DARPA 赞助的一场早期比赛就是一个例子。参赛者包括使用了大量人类知识（如对单词、音素、人类声道的理解）的特殊方法，而另一边则是更依赖统计和大量计算的新方法，基于隐马尔可夫模型（HMMs）。最终，基于统计的方法战胜了基于人类知识的方法。这导致了自然语言处理领域的一次重大转变，随着时间的推移，统计和计算开始成为该领域的主导。深度学习在语音识别中的兴起是这一趋势的最新体现。深度学习方法更少依赖人类知识，使用更多的计算资源，并通过在大型训练集上的学习，极大地提升了语音识别系统的性能。与游戏领域相似，研究人员总是试图创建一个按照他们自己的思维方式工作的系统，但这种尝试最终证明是逆向而行，不仅浪费了大量的研究时间，而且在大量计算资源可用并找到有效利用方法的情况下，这种尝试显得更是多余。

计算机视觉领域也经历了相似的发展模式。早期的方法试图通过搜索边缘、广义圆柱体或 SIFT 特征来处理视觉问题。但在今天，这些方法都被淘汰了。现代的深度学习神经网络仅使用卷积和某些类型的不变性概念，取得了更好的表现。

这是一个重要的教训。作为一个领域，我们还没有完全吸取这一教训，仍在重蹈覆辙。为了识别并避免这种错误，我们必须理解其吸引力所在。我们必须领悟到，试图构建一个基于我们认为自己思考方式的系统是行不通的。苦涩的教训源于这样的历史观察：1) 人工智能研究者经常试图将知识融入他们的代理中；2) 这在短期内总是有益的，也让研究者感到满足；但 3) 从长远来看，这种做法会导致进步停滞，甚至阻碍进一步的发展；4) 真正的突破性进展最终是通过一个相反的方法实现的，这个方法基于通过搜索和学习来扩大计算的规模。这种成功带有苦涩，往往消化不良，因为它是在人类中心化方法之上取得的。

从这个苦涩的教训中，我们应该明白通用方法的巨大力量，即那些随着计算能力的增长而持续扩展的方法。在这方面，似乎可以无限扩展的两种方法是搜索和学习。

苦涩教训中的另一个关键点是，人类心灵的实质内容极其复杂，不可能简化；我们应该放弃试图简单化地理解心灵内容，如空间、物体、多重代理或对称性等概念。这些都是外部世界中任意而复杂的部分，不应该成为我们构建的核心；相反，我们应该构建的是那些能够发现并捕捉这种任意复杂性的元方法。这些方法的核心在于它们能够找到良好的近似，但寻找这些近似的过程应该由我们的方法来完成，而不是我们亲自动手。我们希望 AI 代理能像我们一样具有发现能力，而不是仅仅包含我们已有的发现。将我们的发现直接构建进去，只会使我们更难看清如何实现发现的过程。</title>
            <link>https://nitter.cz/dotey/status/1727599590872928280#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727599590872928280#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:06:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>《苦涩的教训》<br />
Rich Sutton<br />
2019 年 3 月 13 日<br />
70 年人工智能研究给我们的最大启示是：依赖计算能力的通用方法最终表现最佳，而且优势明显。这背后的主要原因是摩尔定律，也就是计算成本持续以指数级下降。大部分 AI 研究都是在假设计算资源固定的情况下进行的（在这种情况下，利用人类知识几乎是提升性能的唯一途径），但实际上，在稍长于一般研究项目的时间里，可用的计算资源会大幅增加。研究者为了在短期内取得突破，尝试利用自己对特定领域的人类知识，但从长远看，关键在于计算能力的利用。这两者原本不必相互冲突，但在实践中却常常如此。投入其中一个领域的时间，就意味着在另一个上的缺失。此外，人们在一种方法上的投资也会形成心理承诺。而基于人类知识的方法往往会使系统变得复杂，不利于利用计算能力的通用方法。有很多例子显示 AI 研究者是如何迟迟才领悟到这个苦涩的教训，回顾这些案例非常有启发性。<br />
<br />
以计算机国际象棋为例，1997 年击败世界冠军卡斯帕罗夫的方法主要是深度搜索。当时，大多数计算机国际象棋研究者对此表示失望，因为他们更倾向于利用对棋局特殊结构的人类理解。然而，当一个简单但基于搜索的方法，结合特殊的硬件和软件展现出巨大效能时，这些基于人类知识的研究者并不愿意接受失败。他们认为，尽管这次“蛮力”搜索获胜，但它并非一种通用策略，也不是人类下棋的方式。这些研究者本希望基于人类理解的方法能够取胜，对实际结果感到失望。<br />
<br />
在计算机围棋的发展中，也出现了类似的模式，只是晚了 20 年。最初的努力都在于避免搜索，尽可能利用对游戏的人类理解和特殊特征，但一旦有效地应用了大规模搜索，这些努力都显得微不足道，甚至有害。在这个过程中，通过自我对弈学习价值函数（在很多其他游戏中也是这样，甚至包括国际象棋，尽管在 1997 年首次击败世界冠军的程序中学习的作用并不大）也非常关键。自我对弈学习和一般学习，就像搜索一样，能够充分利用大量计算资源。在计算机围棋和国际象棋中，研究者最初都是试图利用人类的理解来减少搜索的需要，但最终通过接受搜索和学习才取得了巨大的成功。<br />
<br />
在语音识别领域，1970 年代由 DARPA 赞助的一场早期比赛就是一个例子。参赛者包括使用了大量人类知识（如对单词、音素、人类声道的理解）的特殊方法，而另一边则是更依赖统计和大量计算的新方法，基于隐马尔可夫模型（HMMs）。最终，基于统计的方法战胜了基于人类知识的方法。这导致了自然语言处理领域的一次重大转变，随着时间的推移，统计和计算开始成为该领域的主导。深度学习在语音识别中的兴起是这一趋势的最新体现。深度学习方法更少依赖人类知识，使用更多的计算资源，并通过在大型训练集上的学习，极大地提升了语音识别系统的性能。与游戏领域相似，研究人员总是试图创建一个按照他们自己的思维方式工作的系统，但这种尝试最终证明是逆向而行，不仅浪费了大量的研究时间，而且在大量计算资源可用并找到有效利用方法的情况下，这种尝试显得更是多余。<br />
<br />
计算机视觉领域也经历了相似的发展模式。早期的方法试图通过搜索边缘、广义圆柱体或 SIFT 特征来处理视觉问题。但在今天，这些方法都被淘汰了。现代的深度学习神经网络仅使用卷积和某些类型的不变性概念，取得了更好的表现。<br />
<br />
这是一个重要的教训。作为一个领域，我们还没有完全吸取这一教训，仍在重蹈覆辙。为了识别并避免这种错误，我们必须理解其吸引力所在。我们必须领悟到，试图构建一个基于我们认为自己思考方式的系统是行不通的。苦涩的教训源于这样的历史观察：1) 人工智能研究者经常试图将知识融入他们的代理中；2) 这在短期内总是有益的，也让研究者感到满足；但 3) 从长远来看，这种做法会导致进步停滞，甚至阻碍进一步的发展；4) 真正的突破性进展最终是通过一个相反的方法实现的，这个方法基于通过搜索和学习来扩大计算的规模。这种成功带有苦涩，往往消化不良，因为它是在人类中心化方法之上取得的。<br />
<br />
从这个苦涩的教训中，我们应该明白通用方法的巨大力量，即那些随着计算能力的增长而持续扩展的方法。在这方面，似乎可以无限扩展的两种方法是搜索和学习。<br />
<br />
苦涩教训中的另一个关键点是，人类心灵的实质内容极其复杂，不可能简化；我们应该放弃试图简单化地理解心灵内容，如空间、物体、多重代理或对称性等概念。这些都是外部世界中任意而复杂的部分，不应该成为我们构建的核心；相反，我们应该构建的是那些能够发现并捕捉这种任意复杂性的元方法。这些方法的核心在于它们能够找到良好的近似，但寻找这些近似的过程应该由我们的方法来完成，而不是我们亲自动手。我们希望 AI 代理能像我们一样具有发现能力，而不是仅仅包含我们已有的发现。将我们的发现直接构建进去，只会使我们更难看清如何实现发现的过程。</p>
<p><a href="https://nitter.cz/elonmusk/status/1727587012394295493#m">nitter.cz/elonmusk/status/1727587012394295493#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727594974366060978#m</id>
            <title>Wikipedia 上现在有了 Q* 的词条
https://en.wikipedia.org/wiki/Q*</title>
            <link>https://nitter.cz/dotey/status/1727594974366060978#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727594974366060978#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:48:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wikipedia 上现在有了 Q* 的词条<br />
<a href="https://en.wikipedia.org/wiki/Q">en.wikipedia.org/wiki/Q</a>*</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tbUpLOFh3QUFNbDBRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727585886949282272#m</id>
            <title>Andrej Karpathy 的最新教程：[1hr Talk] Intro to Large Language Models ，排在我的待翻译列表中。

最近吃瓜耽误了不少事，得先补补🥲

https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=3s</title>
            <link>https://nitter.cz/dotey/status/1727585886949282272#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727585886949282272#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:12:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Andrej Karpathy 的最新教程：[1hr Talk] Intro to Large Language Models ，排在我的待翻译列表中。<br />
<br />
最近吃瓜耽误了不少事，得先补补🥲<br />
<br />
<a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=3s">youtube.com/watch?v=zjkBMFhN…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzUxNDg0MDE4MzMyMDU3Ni95MEpYZkF3aD9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727568964065382411#m</id>
            <title>RT by @dotey: 更多Q*（Q-Star）信息爆料和猜测：

- Q*可能具备自主学习和自我改进的能力。

- Q*模型可进行自主决策，可能已具备轻微自我意识。

- GPT-Zero项目解决了数据问题，自己”生产“数据。

- OpenAI可能正在利用计算机合成数据进行训练。

据Reddit用户爆料和猜测：Q*可能是一种非常先进的具有“可怕数学能力”的模型，已经具备自主学习和自我改进的能力。

该模型能够通过评估其行为的长期后果，在广泛的场景中做出复杂的决策。

Q*可能与强化学习中的Q-learning算法有关，这是一种评估在特定情境下采取特定行动的好坏的方法。还提到了Q-Value和Bellman方程，这些都是强化学习中的重要概念，用于指导AI在不同情境下做出最优决策。

简而言之，Q*似乎是一个高级的人工智能模型，能够在多种情境中学习和做出最优决策，具有自主学习和自我改进的能力。

Reddit原帖：https://www.reddit.com/r/OpenAI/comments/181n8am/what_is_q/

而根据@theinformation 今天的报道，OpenAI的首席科学家 Ilya Sutskever 领衔的一个名叫 GPT-Zero 的项目实现了巨大突破。他们克服了训练数据限制的困难，可以自己合成训练数据。

 据悉，Ilya Sutskever的 GPT-Zero 的项目，帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。

此前Ilya在一次采访中说到：

“Without going into details I'll just say the Data Limit can be overcome..."  

“无需详细说明，我只是说数据限制是可以克服的......”  

GPT-Zero项目研究的主要使用计算机生成的数据来训练模型，而不是从互联网上提取的真实世界数据，因为OpenAI已经获得了互联网上能获得的几乎所有的真实文本数据，已经无法再获得足够的数据来进行下一阶段训练。

OpenAI 研究团队利用GPT-Zero这一创新成果，构建了能解决基础数学问题的模型，这一直是现有 AI 模型的难题，无法进行复杂的推理能力。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Ilya Sutskever 的研究成果，开发出了这个名为 Q*（Q-Star）的模型。

Theinformation报道：https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern

而本月奥特曼在接受金融时报采访的时候曾表达：开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。

他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识的能力。

种种迹象表明Q*（Q-Star）的模型确实是超越了GPT 4非常多的模型，可以说是直接跨越了几代，具有自主学习和自我改进的能力，甚至就像之前Ilya表达的可能已经表现出了轻微的自我意识能力。

以上内容为综合报道、爆料的可能性猜测总结，不一定准确，请注意分辨！</title>
            <link>https://nitter.cz/xiaohuggg/status/1727568964065382411#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727568964065382411#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 06:05:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>更多Q*（Q-Star）信息爆料和猜测：<br />
<br />
- Q*可能具备自主学习和自我改进的能力。<br />
<br />
- Q*模型可进行自主决策，可能已具备轻微自我意识。<br />
<br />
- GPT-Zero项目解决了数据问题，自己”生产“数据。<br />
<br />
- OpenAI可能正在利用计算机合成数据进行训练。<br />
<br />
据Reddit用户爆料和猜测：Q*可能是一种非常先进的具有“可怕数学能力”的模型，已经具备自主学习和自我改进的能力。<br />
<br />
该模型能够通过评估其行为的长期后果，在广泛的场景中做出复杂的决策。<br />
<br />
Q*可能与强化学习中的Q-learning算法有关，这是一种评估在特定情境下采取特定行动的好坏的方法。还提到了Q-Value和Bellman方程，这些都是强化学习中的重要概念，用于指导AI在不同情境下做出最优决策。<br />
<br />
简而言之，Q*似乎是一个高级的人工智能模型，能够在多种情境中学习和做出最优决策，具有自主学习和自我改进的能力。<br />
<br />
Reddit原帖：<a href="https://teddit.net/r/OpenAI/comments/181n8am/what_is_q/">teddit.net/r/OpenAI/comments…</a><br />
<br />
而根据<a href="https://nitter.cz/theinformation" title="The Information">@theinformation</a> 今天的报道，OpenAI的首席科学家 Ilya Sutskever 领衔的一个名叫 GPT-Zero 的项目实现了巨大突破。他们克服了训练数据限制的困难，可以自己合成训练数据。<br />
<br />
 据悉，Ilya Sutskever的 GPT-Zero 的项目，帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。<br />
<br />
此前Ilya在一次采访中说到：<br />
<br />
“Without going into details I'll just say the Data Limit can be overcome..."  <br />
<br />
“无需详细说明，我只是说数据限制是可以克服的......”  <br />
<br />
GPT-Zero项目研究的主要使用计算机生成的数据来训练模型，而不是从互联网上提取的真实世界数据，因为OpenAI已经获得了互联网上能获得的几乎所有的真实文本数据，已经无法再获得足够的数据来进行下一阶段训练。<br />
<br />
OpenAI 研究团队利用GPT-Zero这一创新成果，构建了能解决基础数学问题的模型，这一直是现有 AI 模型的难题，无法进行复杂的推理能力。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Ilya Sutskever 的研究成果，开发出了这个名为 Q*（Q-Star）的模型。<br />
<br />
Theinformation报道：<a href="https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern">theinformation.com/articles/…</a><br />
<br />
而本月奥特曼在接受金融时报采访的时候曾表达：开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。<br />
<br />
他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识的能力。<br />
<br />
种种迹象表明Q*（Q-Star）的模型确实是超越了GPT 4非常多的模型，可以说是直接跨越了几代，具有自主学习和自我改进的能力，甚至就像之前Ilya表达的可能已经表现出了轻微的自我意识能力。<br />
<br />
以上内容为综合报道、爆料的可能性猜测总结，不一定准确，请注意分辨！</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tS2RGYWE4QUFQRVdTLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tT1BPdmJnQUE5Q3UxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/huangyun_122/status/1727360100283896056#m</id>
            <title>RT by @dotey: 这哥们，凭一己之力，硬把 Stable Diffusion 的玩法，都开源了出来

https://stable-diffusion-art.com/sdxl-model/#Download_and_install_SDXL_10_models

实在心疼自己的笔记本，烧到 86°后，最终还是移到 Google Colab 上玩

但，如果不是 G-Colab Pro, 时不时会被 G-Colab 掐掉，导致每次都要重连。

还是付费香，办了会员，把 runtime 拉到最大，效率起来了</title>
            <link>https://nitter.cz/huangyun_122/status/1727360100283896056#m</link>
            <guid isPermaLink="false">https://nitter.cz/huangyun_122/status/1727360100283896056#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 16:15:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这哥们，凭一己之力，硬把 Stable Diffusion 的玩法，都开源了出来<br />
<br />
<a href="https://stable-diffusion-art.com/sdxl-model/#Download_and_install_SDXL_10_models">stable-diffusion-art.com/sdx…</a><br />
<br />
实在心疼自己的笔记本，烧到 86°后，最终还是移到 Google Colab 上玩<br />
<br />
但，如果不是 G-Colab Pro, 时不时会被 G-Colab 掐掉，导致每次都要重连。<br />
<br />
还是付费香，办了会员，把 runtime 拉到最大，效率起来了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9qUC1VX1hBQUFFU2sxLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9qUXpoOFhvQUFpb3A4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727563974479057028#m</id>
            <title>OpenAI “宫斗”大戏人物关系图</title>
            <link>https://nitter.cz/dotey/status/1727563974479057028#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727563974479057028#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 05:45:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI “宫斗”大戏人物关系图</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tS1JrRVc0QUFCdGMyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727556370507972669#m</id>
            <title>R to @dotey: 斧正：“然而，就在第二天，董事会宣布解雇了 Altman。”之后的内容不属于原文，不小心添加进去无法删除，请注意过滤。</title>
            <link>https://nitter.cz/dotey/status/1727556370507972669#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727556370507972669#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 05:15:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>斧正：“然而，就在第二天，董事会宣布解雇了 Altman。”之后的内容不属于原文，不小心添加进去无法删除，请注意过滤。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727549427278086281#m</id>
            <title>R to @dotey: reddit.com/r/singularity/com…</title>
            <link>https://nitter.cz/dotey/status/1727549427278086281#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727549427278086281#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:47:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://teddit.net/r/singularity/comments/181oe7i/openai_made_an_ai_breakthrough_before_altman/">teddit.net/r/singularity/com…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzUxMDM3MzU3NjkzNzQ3Mi9PR2NTcDZkUD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727549316779094080#m</id>
            <title>R to @dotey: Reddit上的讨论帖：https://www.reddit.com/r/OpenAI/comments/181n8am/what_is_q/</title>
            <link>https://nitter.cz/dotey/status/1727549316779094080#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727549316779094080#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:47:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Reddit上的讨论帖：<a href="https://teddit.net/r/OpenAI/comments/181n8am/what_is_q/">teddit.net/r/OpenAI/comments…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzUwMDA3MDcwNTA0MTQwOC92UERLNlA3Zj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727548654108475558#m</id>
            <title>不知道你们还记不记得当年 Google 的 Alpha Go，就是下围棋超级牛的人工智能，早已经打赢了人类。而且值得一提的是，最开始它是用人类的棋谱训练的，后来在人类的棋谱训练完了后，它开始自己和自己下棋，也就是说是自己生成棋谱自己训练自己。

在GPT-4训练的时候，几乎用到了所有互联网上能用到的文本，很多人都担心GPT-4训练完了后在没有高质量的文本供训练了。

在前面《独家：OpenAI 研究员在 CEO 被撤职前向董事会发出 AI 突破警告 — 据知情人士透露》https://twitter.com/dotey/status/1727510593970561466 那篇文章中，提到了OpenAI的一个神秘项目叫Q-Star，而这个项目背后的一些秘密正在被一点点的揭开神秘面纱。

根据 theinformation《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》的这篇报道中，提到了一个细节：“他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。”

这意味着他们也像AlphaGo一样，实现了让计算机生成无限的高质量数据，而不需要去互联网上抓取数据！！！

现在还不知道这篇报道的准确性如何，但是结合这些信息，可能性还是相当大的！很期待不久能看到GPT-5的发布！

《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》部分内容翻译：

----

OpenAI 在 Sam Altman 被解雇前夕实现重大 AI 突破，引发期待与忧虑

在 Sam Altman 上周被 OpenAI 董事会解雇的前一天，他曾在 APEC 首席执行官峰会上暗示该公司最近取得了一项技术进展。他表示，这一进展让公司能够揭开无知的面纱，推进知识的边界。这番神秘的言论在公司陷入混乱之际并未引起太多关注。

据知情人士透露，部分 OpenAI 员工认为 Altman 的这番话是在提及公司研究人员今年早些时候的一项创新。这项创新可能使他们能够开发出更强大的人工智能模型。然而，由首席科学家 Ilya Sutskever 领衔的这一技术突破，也引发了一些员工的担忧，他们认为公司在商业化这些高级 AI 模型时可能缺乏适当的安全措施。

据悉，Sutskever 的这一突破帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。这一直是开发下一代模型的主要障碍。他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。

多年来，Sutskever 一直致力于研究如何让像 GPT-4 这样的语言模型解决涉及推理的任务，例如数学或科学问题。2021 年，他启动了一个名为 GPT-Zero 的项目，以向 DeepMind 的 AlphaZero 程序致敬，后者能够下国际象棋、围棋和将棋。

OpenAI 研究团队利用这一创新成果，构建了能解决基础数学问题的系统，这一直是现有 AI 模型的难题。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Sutskever 的研究成果，开发出了一个名为 Q*（“Q-Star”）的模型。</title>
            <link>https://nitter.cz/dotey/status/1727548654108475558#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727548654108475558#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:44:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不知道你们还记不记得当年 Google 的 Alpha Go，就是下围棋超级牛的人工智能，早已经打赢了人类。而且值得一提的是，最开始它是用人类的棋谱训练的，后来在人类的棋谱训练完了后，它开始自己和自己下棋，也就是说是自己生成棋谱自己训练自己。<br />
<br />
在GPT-4训练的时候，几乎用到了所有互联网上能用到的文本，很多人都担心GPT-4训练完了后在没有高质量的文本供训练了。<br />
<br />
在前面《独家：OpenAI 研究员在 CEO 被撤职前向董事会发出 AI 突破警告 — 据知情人士透露》<a href="https://nitter.cz/dotey/status/1727510593970561466">nitter.cz/dotey/status/172…</a> 那篇文章中，提到了OpenAI的一个神秘项目叫Q-Star，而这个项目背后的一些秘密正在被一点点的揭开神秘面纱。<br />
<br />
根据 theinformation《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》的这篇报道中，提到了一个细节：“他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。”<br />
<br />
这意味着他们也像AlphaGo一样，实现了让计算机生成无限的高质量数据，而不需要去互联网上抓取数据！！！<br />
<br />
现在还不知道这篇报道的准确性如何，但是结合这些信息，可能性还是相当大的！很期待不久能看到GPT-5的发布！<br />
<br />
《OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern》部分内容翻译：<br />
<br />
----<br />
<br />
OpenAI 在 Sam Altman 被解雇前夕实现重大 AI 突破，引发期待与忧虑<br />
<br />
在 Sam Altman 上周被 OpenAI 董事会解雇的前一天，他曾在 APEC 首席执行官峰会上暗示该公司最近取得了一项技术进展。他表示，这一进展让公司能够揭开无知的面纱，推进知识的边界。这番神秘的言论在公司陷入混乱之际并未引起太多关注。<br />
<br />
据知情人士透露，部分 OpenAI 员工认为 Altman 的这番话是在提及公司研究人员今年早些时候的一项创新。这项创新可能使他们能够开发出更强大的人工智能模型。然而，由首席科学家 Ilya Sutskever 领衔的这一技术突破，也引发了一些员工的担忧，他们认为公司在商业化这些高级 AI 模型时可能缺乏适当的安全措施。<br />
<br />
据悉，Sutskever 的这一突破帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。这一直是开发下一代模型的主要障碍。他们的研究主要使用计算机生成的数据，而不是从互联网上提取的真实世界数据，如文本或图像来训练新模型。<br />
<br />
多年来，Sutskever 一直致力于研究如何让像 GPT-4 这样的语言模型解决涉及推理的任务，例如数学或科学问题。2021 年，他启动了一个名为 GPT-Zero 的项目，以向 DeepMind 的 AlphaZero 程序致敬，后者能够下国际象棋、围棋和将棋。<br />
<br />
OpenAI 研究团队利用这一创新成果，构建了能解决基础数学问题的系统，这一直是现有 AI 模型的难题。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Sutskever 的研究成果，开发出了一个名为 Q*（“Q-Star”）的模型。</p>
<p><a href="https://nitter.cz/bindureddy/status/1727479646315237784#m">nitter.cz/bindureddy/status/1727479646315237784#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sOERaR1cwQUFBVUZnLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sOEtuQ1dVQUF3UDRDLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727530341202563280#m</id>
            <title>刚发现Ilya也转发了，看起来状态还不错，希望他们能继续合作</title>
            <link>https://nitter.cz/dotey/status/1727530341202563280#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727530341202563280#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 03:31:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚发现Ilya也转发了，看起来状态还不错，希望他们能继续合作</p>
<p><a href="https://nitter.cz/ilyasut/status/1727434066411286557#m">nitter.cz/ilyasut/status/1727434066411286557#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727517538722914365#m</id>
            <title>训练过的不算多牛，出个没训练过的就知道了</title>
            <link>https://nitter.cz/dotey/status/1727517538722914365#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727517538722914365#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:40:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>训练过的不算多牛，出个没训练过的就知道了</p>
<p><a href="https://nitter.cz/66567848787A/status/1727368263032799512#m">nitter.cz/66567848787A/status/1727368263032799512#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/waylybaye/status/1727510389087433103#m</id>
            <title>RT by @dotey: OpenCat 1.8.3 更新啦，将生成图片的功能带到了 iOS 上 🥳</title>
            <link>https://nitter.cz/waylybaye/status/1727510389087433103#m</link>
            <guid isPermaLink="false">https://nitter.cz/waylybaye/status/1727510389087433103#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:12:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenCat 1.8.3 更新啦，将生成图片的功能带到了 iOS 上 🥳</p>
<p><a href="https://nitter.cz/waylybaye/status/1722413580317073702#m">nitter.cz/waylybaye/status/1722413580317073702#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sWmlvUmE4QUExOTl4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sWmlvU2JJQUFGT0w4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727510593970561466#m</id>
            <title>转译：**独家：OpenAI 研究员在 CEO 被撤职前向董事会发出 AI 突破警告 — 据知情人士透露**

11 月 22 日（路透社）- 在 OpenAI 首席执行官 Sam Altman 被迫离职前的四天，几位公司研究员向董事会发送了一封紧急信函，警告他们说，他们发现了一种强大的人工智能技术，这种技术可能对人类构成威胁。这一消息是由两位了解内情的人士向路透社透露的。

这封此前未曾报道过的信件和相关的 AI 算法，是董事会决定撤换 Altman 的关键前奏。两位消息来源表示，Altman 被视为生成式 AI 的标志性人物。在他周二晚些时候的华丽归来之前，超过 700 名员工威胁要辞职，并表示将与被解雇的领导者一起支持投资者微软 (MSFT.O)。

这些消息来源指出，这封信只是导致 Altman 被解雇的众多不满之一。路透社未能查看这封信的副本。写信的研究人员并未立即回应置评请求。

据其中一位知情人士透露，资深高管 Mira Murati 上周三向员工提及了一个名为 Q* 的项目，并表示在周末发生的事件之前，已向董事会发送过一封信。

在报道发布后，一位 OpenAI 发言人表示，Murati 告诉员工媒体即将报道的内容，但她并未对报道的准确性发表评论。

制造 ChatGPT 的公司在 Q*（发音为 Q-Star）项目上取得了进展，一些内部人士认为这可能是该初创公司在追求超级智能（也称为人工通用智能，AGI）方向上的重大突破，一位人士告诉路透社。OpenAI 将 AGI 定义为智力超过人类的 AI 系统。

该人士表示，凭借庞大的计算资源，这个新模型能够解决一些数学问题。这位不愿透露姓名的人士说，尽管 Q* 目前的数学运算能力仅限于小学生水平，但它在这些测试中的出色表现让研究人员对其未来的成功充满期望。

路透社无法独立核实研究人员对 Q* 能力的说法。

**超级智能的挑战**
研究人员认为，数学是生成式 AI 发展的新前沿。目前，生成式 AI 在通过统计方法预测下一个词以进行写作和语言翻译方面表现良好，但面对同一个问题可能会有截然不同的答案。然而，掌握了数学运算——在这里只有一个正确答案——意味着 AI 将具备更接近人类的推理能力。AI 研究者相信，这一技术可以应用于创新的科学研究。

与只能执行有限运算的计算器不同，AGI 能够进行泛化学习和理解。

研究人员在给董事会的信中提到了 AI 的强大能力和潜在危险，但消息来源没有具体说明信中提到的安全问题。长期以来，计算机科学家一直在讨论超级智能机器可能带来的风险，比如它们可能会认为摧毁人类符合自身利益。

在这种背景下，Altman 领导着使 ChatGPT 成为史上增长最快的软件应用之一的努力，并从微软那里获得了投资和必要的计算资源，以更接近于实现超级智能或 AGI。

在本月早些时候展示了一系列新工具之后，上周 Altman 在旧金山举行的世界领袖聚会上暗示，他认为 AGI 已经近在眼前。

“在 OpenAI 的历史中，最近一次是在过去几周，我已经四次有幸亲身经历推动认知界限的拓展和发现的前沿，能够参与其中是我职业生涯中的极大荣誉，”他在亚太经济合作组织峰会上说。

然而，就在第二天，董事会宣布解雇了 Altman。

在 AI 安全和人工通用智能（AGI）定义上，存在激烈争议。Microsoft 对 OpenAI 投入巨资，但后者的条款明确规定，不得利用 AGI 追求利益。

OpenAI 宪章指出：无论是与 Microsoft 还是其他商业和知识产权许可协议，AGI 都被明确排除在外。

Sam Altman 意识到，即使是现阶段的原型 AGI，也足以推动公司业绩飙升，带来巨额利润和更多投资。因此，他们推出了 Dev Day、GPT 商店和收入分成模式。

这一做法引起了 OAI 董事会的不满，部分董事仍坚持初衷，认为 AGI 应致力于人类福祉，而非商业利益。他们担忧此举可能导致不安全模型的使用。

目前 AGI 的定义并不明确。若实现重大突破（如 Sam 所述），是否将其归为 AGI 取决于董事会的投票。若获得足够票数宣布其为 AGI，Microsoft 和 OpenAI 将失去数十亿潜在许可收入；反之，他们可以通过授权类似 AGI 技术获利。

几周或几个月前，OpenAI 团队取得重大进展，接近实现 AGI（由此产生的玩笑、泄露、氛围变化等）。然而，Sam 和 Brockman 对此隐瞒了董事会的非员工成员。Ilyas 对此感到不满，认为这应视为 AGI，不应授权给任何人，包括 Microsoft。当 AGI 状态的投票提上日程时，董事们因被蒙在鼓里而愤怒，最终决定开除 Sam 并迫使 Brockman 辞职。

Ilyas 近期声称，现有架构已足够实现 AGI，而 Sam 则坚持认为需要新的突破。在这种情况下，Sam 更倾向于将 AGI 商业化，而 Ilyas 则认为我们已经实现了 AGI。

Sam Altman 想要推迟宣布这一技术为 AGI，因为这样做可以延长盈利时间。相反，Ilya 希望尽快将其定性为 AGI，以便按照公司最初的原则使用，而非追求利润。最终，Ilya 在这场权力争斗中胜出。在 Microsoft 还未来得及干预之前，事情已经尘埃落定，因为 Microsoft 表示他们对这一进展毫不知情，而他们显然会倾向于延迟 AGI 的宣布。

更早宣布 AGI 意味着该技术不能被授权给任何方（因此其带来的利润将更具社会公平性，并迫使研究人员专注于一致性和安全），同时还意味着更多监管。可以想象，“人工通用智能已被发明”的新闻头条将在 /r/WorldNews 上引发轰动，引起全球范围内的恐慌，迫使各国政府召开紧急会议，确保不发生 Skynet 式的灾难，安全派对此表示欢迎。

如果不是这样，情况会有所不同。我们可能会继续推进当前的前沿模型和代理共享计划，而不将其定性为 AGI，这样 OAI 和 Microsoft 将因此获得巨大利益。对于关注安全的群体而言，这意味着 AGI 的发展将受到更少监管，同时被加州原则融入到 ChatGPT 和 DALL-E 的输出中，从而让 OAI 可以宣称“我们确实关心安全！”

Ilya 可能并非有意赶走 Sam，但当收入分享方案被提出，而 Sam 辩称 OAI 所拥有的技术并非 AGI 或类似之物时，这可能促使 Ilya 决定发起政变。OpenAI 当前可能计划很快宣布他们已经拥有 AGI，可能在接下来的 6 到 8 个月内，也许是通过部署 GPT-4.5 或比预期更早发布 GPT-5。或许甚至更早。

这并非源于技术突破；而是基于他们已有的技术。这只是一个围绕是否为了利润而将该技术称为 AGI 的争议和冲突。

https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/</title>
            <link>https://nitter.cz/dotey/status/1727510593970561466#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727510593970561466#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:13:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译：**独家：OpenAI 研究员在 CEO 被撤职前向董事会发出 AI 突破警告 — 据知情人士透露**<br />
<br />
11 月 22 日（路透社）- 在 OpenAI 首席执行官 Sam Altman 被迫离职前的四天，几位公司研究员向董事会发送了一封紧急信函，警告他们说，他们发现了一种强大的人工智能技术，这种技术可能对人类构成威胁。这一消息是由两位了解内情的人士向路透社透露的。<br />
<br />
这封此前未曾报道过的信件和相关的 AI 算法，是董事会决定撤换 Altman 的关键前奏。两位消息来源表示，Altman 被视为生成式 AI 的标志性人物。在他周二晚些时候的华丽归来之前，超过 700 名员工威胁要辞职，并表示将与被解雇的领导者一起支持投资者微软 (MSFT.O)。<br />
<br />
这些消息来源指出，这封信只是导致 Altman 被解雇的众多不满之一。路透社未能查看这封信的副本。写信的研究人员并未立即回应置评请求。<br />
<br />
据其中一位知情人士透露，资深高管 Mira Murati 上周三向员工提及了一个名为 Q* 的项目，并表示在周末发生的事件之前，已向董事会发送过一封信。<br />
<br />
在报道发布后，一位 OpenAI 发言人表示，Murati 告诉员工媒体即将报道的内容，但她并未对报道的准确性发表评论。<br />
<br />
制造 ChatGPT 的公司在 Q*（发音为 Q-Star）项目上取得了进展，一些内部人士认为这可能是该初创公司在追求超级智能（也称为人工通用智能，AGI）方向上的重大突破，一位人士告诉路透社。OpenAI 将 AGI 定义为智力超过人类的 AI 系统。<br />
<br />
该人士表示，凭借庞大的计算资源，这个新模型能够解决一些数学问题。这位不愿透露姓名的人士说，尽管 Q* 目前的数学运算能力仅限于小学生水平，但它在这些测试中的出色表现让研究人员对其未来的成功充满期望。<br />
<br />
路透社无法独立核实研究人员对 Q* 能力的说法。<br />
<br />
**超级智能的挑战**<br />
研究人员认为，数学是生成式 AI 发展的新前沿。目前，生成式 AI 在通过统计方法预测下一个词以进行写作和语言翻译方面表现良好，但面对同一个问题可能会有截然不同的答案。然而，掌握了数学运算——在这里只有一个正确答案——意味着 AI 将具备更接近人类的推理能力。AI 研究者相信，这一技术可以应用于创新的科学研究。<br />
<br />
与只能执行有限运算的计算器不同，AGI 能够进行泛化学习和理解。<br />
<br />
研究人员在给董事会的信中提到了 AI 的强大能力和潜在危险，但消息来源没有具体说明信中提到的安全问题。长期以来，计算机科学家一直在讨论超级智能机器可能带来的风险，比如它们可能会认为摧毁人类符合自身利益。<br />
<br />
在这种背景下，Altman 领导着使 ChatGPT 成为史上增长最快的软件应用之一的努力，并从微软那里获得了投资和必要的计算资源，以更接近于实现超级智能或 AGI。<br />
<br />
在本月早些时候展示了一系列新工具之后，上周 Altman 在旧金山举行的世界领袖聚会上暗示，他认为 AGI 已经近在眼前。<br />
<br />
“在 OpenAI 的历史中，最近一次是在过去几周，我已经四次有幸亲身经历推动认知界限的拓展和发现的前沿，能够参与其中是我职业生涯中的极大荣誉，”他在亚太经济合作组织峰会上说。<br />
<br />
然而，就在第二天，董事会宣布解雇了 Altman。<br />
<br />
在 AI 安全和人工通用智能（AGI）定义上，存在激烈争议。Microsoft 对 OpenAI 投入巨资，但后者的条款明确规定，不得利用 AGI 追求利益。<br />
<br />
OpenAI 宪章指出：无论是与 Microsoft 还是其他商业和知识产权许可协议，AGI 都被明确排除在外。<br />
<br />
Sam Altman 意识到，即使是现阶段的原型 AGI，也足以推动公司业绩飙升，带来巨额利润和更多投资。因此，他们推出了 Dev Day、GPT 商店和收入分成模式。<br />
<br />
这一做法引起了 OAI 董事会的不满，部分董事仍坚持初衷，认为 AGI 应致力于人类福祉，而非商业利益。他们担忧此举可能导致不安全模型的使用。<br />
<br />
目前 AGI 的定义并不明确。若实现重大突破（如 Sam 所述），是否将其归为 AGI 取决于董事会的投票。若获得足够票数宣布其为 AGI，Microsoft 和 OpenAI 将失去数十亿潜在许可收入；反之，他们可以通过授权类似 AGI 技术获利。<br />
<br />
几周或几个月前，OpenAI 团队取得重大进展，接近实现 AGI（由此产生的玩笑、泄露、氛围变化等）。然而，Sam 和 Brockman 对此隐瞒了董事会的非员工成员。Ilyas 对此感到不满，认为这应视为 AGI，不应授权给任何人，包括 Microsoft。当 AGI 状态的投票提上日程时，董事们因被蒙在鼓里而愤怒，最终决定开除 Sam 并迫使 Brockman 辞职。<br />
<br />
Ilyas 近期声称，现有架构已足够实现 AGI，而 Sam 则坚持认为需要新的突破。在这种情况下，Sam 更倾向于将 AGI 商业化，而 Ilyas 则认为我们已经实现了 AGI。<br />
<br />
Sam Altman 想要推迟宣布这一技术为 AGI，因为这样做可以延长盈利时间。相反，Ilya 希望尽快将其定性为 AGI，以便按照公司最初的原则使用，而非追求利润。最终，Ilya 在这场权力争斗中胜出。在 Microsoft 还未来得及干预之前，事情已经尘埃落定，因为 Microsoft 表示他们对这一进展毫不知情，而他们显然会倾向于延迟 AGI 的宣布。<br />
<br />
更早宣布 AGI 意味着该技术不能被授权给任何方（因此其带来的利润将更具社会公平性，并迫使研究人员专注于一致性和安全），同时还意味着更多监管。可以想象，“人工通用智能已被发明”的新闻头条将在 /r/WorldNews 上引发轰动，引起全球范围内的恐慌，迫使各国政府召开紧急会议，确保不发生 Skynet 式的灾难，安全派对此表示欢迎。<br />
<br />
如果不是这样，情况会有所不同。我们可能会继续推进当前的前沿模型和代理共享计划，而不将其定性为 AGI，这样 OAI 和 Microsoft 将因此获得巨大利益。对于关注安全的群体而言，这意味着 AGI 的发展将受到更少监管，同时被加州原则融入到 ChatGPT 和 DALL-E 的输出中，从而让 OAI 可以宣称“我们确实关心安全！”<br />
<br />
Ilya 可能并非有意赶走 Sam，但当收入分享方案被提出，而 Sam 辩称 OAI 所拥有的技术并非 AGI 或类似之物时，这可能促使 Ilya 决定发起政变。OpenAI 当前可能计划很快宣布他们已经拥有 AGI，可能在接下来的 6 到 8 个月内，也许是通过部署 GPT-4.5 或比预期更早发布 GPT-5。或许甚至更早。<br />
<br />
这并非源于技术突破；而是基于他们已有的技术。这只是一个围绕是否为了利润而将该技术称为 AGI 的争议和冲突。<br />
<br />
<a href="https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/">reuters.com/technology/sam-a…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727489479156318487#m</id>
            <title>1. 你自己要捋清楚其中的成本、收益和风险，也许没必要

2. 要让manager清楚收益远大于成本和风险

3. 要有清晰的计划，让Manager知道不会影响当前项目进展，例如：
-将要重构的模块列出来，加上T恤size
-先补充测试代码不会让重构影响质量
-将所有任务用任务管理跟踪工具跟踪起来
-每个Sprint完成一部分而不需要专门停下来做这事</title>
            <link>https://nitter.cz/dotey/status/1727489479156318487#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727489479156318487#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 00:49:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>1. 你自己要捋清楚其中的成本、收益和风险，也许没必要<br />
<br />
2. 要让manager清楚收益远大于成本和风险<br />
<br />
3. 要有清晰的计划，让Manager知道不会影响当前项目进展，例如：<br />
-将要重构的模块列出来，加上T恤size<br />
-先补充测试代码不会让重构影响质量<br />
-将所有任务用任务管理跟踪工具跟踪起来<br />
-每个Sprint完成一部分而不需要专门停下来做这事</p>
<p><a href="https://nitter.cz/beihuo/status/1727449107005067545#m">nitter.cz/beihuo/status/1727449107005067545#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727468251154620804#m</id>
            <title>#AI开源项目推荐：vscode-ui-sketcher

UI Sketcher是一个VSCode插件，借助GPT-4V的多模态能力，在插件中画出界面草图，就能生成一个基于ReactNative的UI界面。

需要OpenAI的API Key

https://github.com/pAIrprogio/vscode-ui-sketcher</title>
            <link>https://nitter.cz/dotey/status/1727468251154620804#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727468251154620804#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 23:25:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：vscode-ui-sketcher<br />
<br />
UI Sketcher是一个VSCode插件，借助GPT-4V的多模态能力，在插件中画出界面草图，就能生成一个基于ReactNative的UI界面。<br />
<br />
需要OpenAI的API Key<br />
<br />
<a href="https://github.com/pAIrprogio/vscode-ui-sketcher">github.com/pAIrprogio/vscode…</a></p>
<p><a href="https://nitter.cz/taishiyadeee/status/1727175786963300531#m">nitter.cz/taishiyadeee/status/1727175786963300531#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNjg5NjAxNTE4NDE1MDUyOC9sYUJZWlJCcj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/yumimao/status/1727240174994874805#m</id>
            <title>RT by @dotey: https://www.fal.ai/dynamic 一个简单的在线体验网站</title>
            <link>https://nitter.cz/yumimao/status/1727240174994874805#m</link>
            <guid isPermaLink="false">https://nitter.cz/yumimao/status/1727240174994874805#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 08:18:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://www.fal.ai/dynamic">fal.ai/dynamic</a> 一个简单的在线体验网站</p>
<p><a href="https://nitter.cz/RioJot/status/1727026140802670857#m">nitter.cz/RioJot/status/1727026140802670857#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9oanJpbGJzQUE0TFJ6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727456845646733767#m</id>
            <title>R to @dotey: 我让GPT-4V帮我分析了一下两张图：

----

由于我无法同时访问两张图片，我将分别对每张图片进行解读，然后基于我的解读进行对比分析。

对于第一张图片：

这是一个关于机器学习模型（称为Claude 2.1）的性能测试图。测试的目的是评估模型在不同文档深度和上下文长度下检索信息的准确性。图中的矩阵表格显示了不同的上下文长度（标记为“Context Length (# Tokens)”）和文档深度百分比（从顶部到底部）。颜色代码表示检索准确性，从绿色（更高准确性）到红色（较低准确性）。注释指出，随着上下文长度的增加，Claude 2.1的检索准确性逐渐下降。这表明模型在处理大量数据时可能会遇到困难。

对于第二张图片：

这张图与第一张相似，但针对的是不同的机器学习模型，即GPT-4。测试的目的是评估GPT-4在不同文档深度和上下文长度下的信息检索准确性。这张图同样显示了一个由不同上下文长度和文档深度组成的矩阵，并用颜色代码表示检索准确性。注释指出，当事实被放置在文档深度的10%-50%之间时，GPT-4的检索准确性开始下降，尤其是在大的上下文长度时。

对比分析：

两张图展示了两种不同的模型在不同条件下的性能测试。主要的比较点在于模型处理大量信息时的准确性。Claude 2.1的测试覆盖了更宽的上下文长度范围，而GPT-4的测试似乎集中在更小的数据集上，但进行了更多的样本测试以增加结果的稳定性。两张图表的颜色渐变表明，两个模型都在较大的上下文长度下表现出检索准确性下降的趋势，尽管具体的准确性水平和下降趋势在每个模型中是不同的。这些测试对于理解和改进机器学习模型在处理大规模数据时的性能至关重要。</title>
            <link>https://nitter.cz/dotey/status/1727456845646733767#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727456845646733767#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 22:39:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我让GPT-4V帮我分析了一下两张图：<br />
<br />
----<br />
<br />
由于我无法同时访问两张图片，我将分别对每张图片进行解读，然后基于我的解读进行对比分析。<br />
<br />
对于第一张图片：<br />
<br />
这是一个关于机器学习模型（称为Claude 2.1）的性能测试图。测试的目的是评估模型在不同文档深度和上下文长度下检索信息的准确性。图中的矩阵表格显示了不同的上下文长度（标记为“Context Length (# Tokens)”）和文档深度百分比（从顶部到底部）。颜色代码表示检索准确性，从绿色（更高准确性）到红色（较低准确性）。注释指出，随着上下文长度的增加，Claude 2.1的检索准确性逐渐下降。这表明模型在处理大量数据时可能会遇到困难。<br />
<br />
对于第二张图片：<br />
<br />
这张图与第一张相似，但针对的是不同的机器学习模型，即GPT-4。测试的目的是评估GPT-4在不同文档深度和上下文长度下的信息检索准确性。这张图同样显示了一个由不同上下文长度和文档深度组成的矩阵，并用颜色代码表示检索准确性。注释指出，当事实被放置在文档深度的10%-50%之间时，GPT-4的检索准确性开始下降，尤其是在大的上下文长度时。<br />
<br />
对比分析：<br />
<br />
两张图展示了两种不同的模型在不同条件下的性能测试。主要的比较点在于模型处理大量信息时的准确性。Claude 2.1的测试覆盖了更宽的上下文长度范围，而GPT-4的测试似乎集中在更小的数据集上，但进行了更多的样本测试以增加结果的稳定性。两张图表的颜色渐变表明，两个模型都在较大的上下文长度下表现出检索准确性下降的趋势，尽管具体的准确性水平和下降趋势在每个模型中是不同的。这些测试对于理解和改进机器学习模型在处理大规模数据时的性能至关重要。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9rbzFYT1c4QUVUVVl5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727454708627808261#m</id>
            <title>将Greg Kamradt的两次实验：
- GPT-4-128K https://twitter.com/dotey/status/1727445313261039717
- Claude 2.1 200K：https://twitter.com/dotey/status/1727437625194136060 

两次结果放一起对比，可以看到：

GPT-4-128K
1. 在上下文长度小于 73K Tokens时，不会有信息遗忘
2. 在上下文长度超出 73K 后，位于中间7%-50% 区间的信息可能会被遗忘

Claude-2.1-200K
1. 上下文长度小于24K时效果最好，几乎不会有信息遗忘
2. 上下文长度超过24K就开始变差，90K后，效果很差，一大半信息会遗忘
3. 最头部和最顶部的1%效果最好，不会被遗忘

我的总结
1. GPT-4还是最强的
2. 尽量少一些上下文，长文还是需要手动或者自动分割，GPT-4 的话，不要超过73K（输入输出加一起）
3. System Prompt 放在最前或者最后都可以，放在后面更好
4. 如果System Prompt 放在前面，对于GPT-4来说，不要超过5%，对于Claude来说不要超过1%
4. GPTs的Prompt只能放在最前面，所以不要超过不要超过5%也就是128*0.05 = 6.4 K Tokens比较好</title>
            <link>https://nitter.cz/dotey/status/1727454708627808261#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727454708627808261#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 22:31:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将Greg Kamradt的两次实验：<br />
- GPT-4-128K <a href="https://nitter.cz/dotey/status/1727445313261039717">nitter.cz/dotey/status/172…</a><br />
- Claude 2.1 200K：<a href="https://nitter.cz/dotey/status/1727437625194136060">nitter.cz/dotey/status/172…</a> <br />
<br />
两次结果放一起对比，可以看到：<br />
<br />
GPT-4-128K<br />
1. 在上下文长度小于 73K Tokens时，不会有信息遗忘<br />
2. 在上下文长度超出 73K 后，位于中间7%-50% 区间的信息可能会被遗忘<br />
<br />
Claude-2.1-200K<br />
1. 上下文长度小于24K时效果最好，几乎不会有信息遗忘<br />
2. 上下文长度超过24K就开始变差，90K后，效果很差，一大半信息会遗忘<br />
3. 最头部和最顶部的1%效果最好，不会被遗忘<br />
<br />
我的总结<br />
1. GPT-4还是最强的<br />
2. 尽量少一些上下文，长文还是需要手动或者自动分割，GPT-4 的话，不要超过73K（输入输出加一起）<br />
3. System Prompt 放在最前或者最后都可以，放在后面更好<br />
4. 如果System Prompt 放在前面，对于GPT-4来说，不要超过5%，对于Claude来说不要超过1%<br />
4. GPTs的Prompt只能放在最前面，所以不要超过不要超过5%也就是128*0.05 = 6.4 K Tokens比较好</p>
<p><a href="https://nitter.cz/dotey/status/1727437625194136060#m">nitter.cz/dotey/status/1727437625194136060#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9rbC1fR1hrQUFLemNmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9rbUE3U1hJQUFqSUY5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>