<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1734736746099945685#m</id>
            <title>RT by @dotey: 推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。

我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。

搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。

最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</title>
            <link>https://nitter.cz/Barret_China/status/1734736746099945685#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1734736746099945685#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:47:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，<a href="https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw">mp.weixin.qq.com/s/LbRvR1VXp…</a>，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。<br />
<br />
我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。<br />
<br />
搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。<br />
<br />
最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734742684353380484#m</id>
            <title>作者声称在这个 21 分钟的新闻剪辑中，所有的主播乃至其中很多其他内容，都是由 AI 生成的！

AI 主播的报道，既丰富有料，又感人至深，还不至于太乏味，还不用担心负面新闻。

如果真的这么强，未来新闻主播职业会受到影响吗？</title>
            <link>https://nitter.cz/dotey/status/1734742684353380484#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734742684353380484#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:11:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者声称在这个 21 分钟的新闻剪辑中，所有的主播乃至其中很多其他内容，都是由 AI 生成的！<br />
<br />
AI 主播的报道，既丰富有料，又感人至深，还不至于太乏味，还不用担心负面新闻。<br />
<br />
如果真的这么强，未来新闻主播职业会受到影响吗？</p>
<p><a href="https://nitter.cz/channel1_ai/status/1734591810033373231#m">nitter.cz/channel1_ai/status/1734591810033373231#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734728249396085116#m</id>
            <title>R to @dotey: 配合 @realrenmin 老师这条一起看
https://x.com/realrenmin/status/1734721215283986628?s=20</title>
            <link>https://nitter.cz/dotey/status/1734728249396085116#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734728249396085116#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:13:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>配合 <a href="https://nitter.cz/realrenmin" title="Sverige_ Dong-seok🇸🇪">@realrenmin</a> 老师这条一起看<br />
<a href="https://x.com/realrenmin/status/1734721215283986628?s=20">x.com/realrenmin/status/1734…</a></p>
<p><a href="https://nitter.cz/realrenmin/status/1734721215283986628#m">nitter.cz/realrenmin/status/1734721215283986628#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/realrenmin/status/1734721215283986628#m</id>
            <title>RT by @dotey: Mistral AI放出Mixtral 8x7B, 基于Mixture of Experts (MoE)的开源模型，效果不错。

但在我看来，MoE是transformer时代LSTM-GRU，是NLP古早的范式，architecture engineering，非常old school。核心方法是加一些gate来加强Efficient Training at Scale，简言之目的是为了低成本训练，而不是为了塑造专家模型。

而Mixture of Experts的名字，太具迷惑性了，字面意思似乎是各种专家模型的组合起到1+1>2的效果。但实际看看Mixtral 8x7B，8个mistral 7b，b b不一样，但没有一个是专家模型，之所以叫做expert，居然是MoE中的FNN，我十分怀疑FNN能有什么专家能力。

它的benchmarking也理所当然的跟通用大模型GPT3.5/Llama 2相比，比较的是generic能力，并没有什么突出的专家能力。粗算了一下，8x7B float16, 至少需要100GB以上GPU显存，cost巨大。在这种情况下，oss的情怀，不足以说服我不用OpenAI的api。

如果我们停下来想想，什么是expert。
首先，expert能力一定不是通用大模型的generic的能力，而是独特的specialization的能力。例如会写code的GitHub copilot，或会generate思科路由器配置命令，甚至特别会planning，特别会算数都是专家能力都算。
简言之，expert能力是会产生特定领域特定输出的能力。所以，MoE是一个好名字，在这个时代，缺有些名不副实。

而做specialization模型的技术，依然在发展，并且依然是前沿，其实就是lora微调，例如Stanford's Alpaca models项目等等，核心思想就是在开源模型上加adapter，使之能够完成一个具体领域的专家工作，其实Mistral AI的开源7b模型估计也是这么做出来的。

未来，大语言模型作为agent的时代在实际中的应用，一定是llm在中间协调多种多样不同7b抽象出来的api，来完成新的human computer interaction。甚至在特定领域，这个协调工作也可以被planning expert的开源模型替代，而协调的过程，还是离不开 CoT，React，ReWoo或者其他的prompting方法。

CoT, ReAct在我的推中已经分享过好几次了，接下来找时间把ReWoo, 几个微调的介绍（跳票很久了）分享给大家。</title>
            <link>https://nitter.cz/realrenmin/status/1734721215283986628#m</link>
            <guid isPermaLink="false">https://nitter.cz/realrenmin/status/1734721215283986628#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 23:45:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral AI放出Mixtral 8x7B, 基于Mixture of Experts (MoE)的开源模型，效果不错。<br />
<br />
但在我看来，MoE是transformer时代LSTM-GRU，是NLP古早的范式，architecture engineering，非常old school。核心方法是加一些gate来加强Efficient Training at Scale，简言之目的是为了低成本训练，而不是为了塑造专家模型。<br />
<br />
而Mixture of Experts的名字，太具迷惑性了，字面意思似乎是各种专家模型的组合起到1+1>2的效果。但实际看看Mixtral 8x7B，8个mistral 7b，b b不一样，但没有一个是专家模型，之所以叫做expert，居然是MoE中的FNN，我十分怀疑FNN能有什么专家能力。<br />
<br />
它的benchmarking也理所当然的跟通用大模型GPT3.5/Llama 2相比，比较的是generic能力，并没有什么突出的专家能力。粗算了一下，8x7B float16, 至少需要100GB以上GPU显存，cost巨大。在这种情况下，oss的情怀，不足以说服我不用OpenAI的api。<br />
<br />
如果我们停下来想想，什么是expert。<br />
首先，expert能力一定不是通用大模型的generic的能力，而是独特的specialization的能力。例如会写code的GitHub copilot，或会generate思科路由器配置命令，甚至特别会planning，特别会算数都是专家能力都算。<br />
简言之，expert能力是会产生特定领域特定输出的能力。所以，MoE是一个好名字，在这个时代，缺有些名不副实。<br />
<br />
而做specialization模型的技术，依然在发展，并且依然是前沿，其实就是lora微调，例如Stanford's Alpaca models项目等等，核心思想就是在开源模型上加adapter，使之能够完成一个具体领域的专家工作，其实Mistral AI的开源7b模型估计也是这么做出来的。<br />
<br />
未来，大语言模型作为agent的时代在实际中的应用，一定是llm在中间协调多种多样不同7b抽象出来的api，来完成新的human computer interaction。甚至在特定领域，这个协调工作也可以被planning expert的开源模型替代，而协调的过程，还是离不开 CoT，React，ReWoo或者其他的prompting方法。<br />
<br />
CoT, ReAct在我的推中已经分享过好几次了，接下来找时间把ReWoo, 几个微调的介绍（跳票很久了）分享给大家。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1734671070144446927#m</id>
            <title>RT by @dotey: 讲一个小案例，一家走完上市准备流程的公司，死在上市前的故事。中间有很多可以参考的地方。
这家公司是欧洲某国的物流+电商企业。做了很久了，在当地也算是地头蛇了。 营收都很健康。
随着海淘的浪潮，他们公司发展特别快。很快就被巨头给盯上了。
第一个出手的是顺丰旗下的公司，他们的应对方法就是，你只要在一个地方设点，我就永远低于你价格10%，揽件。对于地头蛇来讲属于生死之战，可以拿其他地方的利润来补。但是对于顺丰来讲，除非公司层面有决心亏三年，一定战略占领这个地方，否则，大公司有大公司的难处。所以顺丰在该国业务开展的并不恨顺利。
ps，这也是非常常见的打法。

第二就是，老奸巨猾的某东了。

首先，某东首先表示收购意愿。
然后，强哥邀请核心创始人去家里吃饭。强哥亲自做饭。
结果就是，该司将该公司的各种商业窍门，以及上上下下，里里外外啥都不剩，给人讲了一遍。 这是该公司犯的第一个错误。
好了。
过了一段时间，某东就说，给你两个选择，要么帮我们建一套跟你们一样的系统。要么我去找你竞争对手去建。
然后收购这事呢，也一直缓慢推进。吊着你。
这个公司，犯的第二个错误。他们知道被当枪使，还是去帮某东去建一套系统。
果不其然，收购这事，最后也黄了。

当然，还有一些事情纠葛在一起，这个公司今年黄的。

lesson learned：
1. 不要怕打架。
2. 见到大佬不要慌，不要把核心机密全盘托出。 不见兔子不撒鹰。
3. 打破囚徒困境的办法找到利益共同点搞联盟，搞交叉持股。</title>
            <link>https://nitter.cz/mtrainier2020/status/1734671070144446927#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1734671070144446927#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 20:26:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>讲一个小案例，一家走完上市准备流程的公司，死在上市前的故事。中间有很多可以参考的地方。<br />
这家公司是欧洲某国的物流+电商企业。做了很久了，在当地也算是地头蛇了。 营收都很健康。<br />
随着海淘的浪潮，他们公司发展特别快。很快就被巨头给盯上了。<br />
第一个出手的是顺丰旗下的公司，他们的应对方法就是，你只要在一个地方设点，我就永远低于你价格10%，揽件。对于地头蛇来讲属于生死之战，可以拿其他地方的利润来补。但是对于顺丰来讲，除非公司层面有决心亏三年，一定战略占领这个地方，否则，大公司有大公司的难处。所以顺丰在该国业务开展的并不恨顺利。<br />
ps，这也是非常常见的打法。<br />
<br />
第二就是，老奸巨猾的某东了。<br />
<br />
首先，某东首先表示收购意愿。<br />
然后，强哥邀请核心创始人去家里吃饭。强哥亲自做饭。<br />
结果就是，该司将该公司的各种商业窍门，以及上上下下，里里外外啥都不剩，给人讲了一遍。 这是该公司犯的第一个错误。<br />
好了。<br />
过了一段时间，某东就说，给你两个选择，要么帮我们建一套跟你们一样的系统。要么我去找你竞争对手去建。<br />
然后收购这事呢，也一直缓慢推进。吊着你。<br />
这个公司，犯的第二个错误。他们知道被当枪使，还是去帮某东去建一套系统。<br />
果不其然，收购这事，最后也黄了。<br />
<br />
当然，还有一些事情纠葛在一起，这个公司今年黄的。<br />
<br />
lesson learned：<br />
1. 不要怕打架。<br />
2. 见到大佬不要慌，不要把核心机密全盘托出。 不见兔子不撒鹰。<br />
3. 打破囚徒困境的办法找到利益共同点搞联盟，搞交叉持股。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734708814983602256#m</id>
            <title>哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。

另外http://lmsys.org的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。

建议有空也可以上去测试评选一下：http://chat.lmsys.org</title>
            <link>https://nitter.cz/dotey/status/1734708814983602256#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734708814983602256#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 22:56:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。<br />
<br />
另外<a href="http://lmsys.org">lmsys.org</a>的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。<br />
<br />
建议有空也可以上去测试评选一下：<a href="http://chat.lmsys.org">chat.lmsys.org</a></p>
<p><a href="https://nitter.cz/lmsysorg/status/1734680611393073289#m">nitter.cz/lmsysorg/status/1734680611393073289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734642351090422077#m</id>
            <title>R to @dotey: x.com/dotey/status/173464195…</title>
            <link>https://nitter.cz/dotey/status/1734642351090422077#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734642351090422077#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 18:32:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://x.com/dotey/status/1734641959522783467?s=20">x.com/dotey/status/173464195…</a></p>
<p><a href="https://nitter.cz/dotey/status/1734641959522783467#m">nitter.cz/dotey/status/1734641959522783467#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734641959522783467#m</id>
            <title>测试了一下，速度很快，能懂中文，但是输出中文不太行，写代码能力还可以👍🏻

测试地址：http://labs.perplexity.ai https://labs.perplexity.ai/ （注意右下角切换成 mixtral-8x7b-instruct）</title>
            <link>https://nitter.cz/dotey/status/1734641959522783467#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734641959522783467#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 18:30:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试了一下，速度很快，能懂中文，但是输出中文不太行，写代码能力还可以👍🏻<br />
<br />
测试地址：<a href="http://labs.perplexity.ai">labs.perplexity.ai</a> <a href="https://labs.perplexity.ai/">labs.perplexity.ai/</a> （注意右下角切换成 mixtral-8x7b-instruct）</p>
<p><a href="https://nitter.cz/op7418/status/1734605584421548412#m">nitter.cz/op7418/status/1734605584421548412#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JLdlRWQ1dNQUEyaEFmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JLdldYLVgwQUF4bkxaLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734569923160981706#m</id>
            <title>了不起👍</title>
            <link>https://nitter.cz/dotey/status/1734569923160981706#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734569923160981706#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 13:44:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>了不起👍</p>
<p><a href="https://nitter.cz/arvin17x/status/1734387717738525137#m">nitter.cz/arvin17x/status/1734387717738525137#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734497196953985354#m</id>
            <title>RT by @dotey: 南洋理工发布了一个 AI 视频放大算法 Upscale-A-Video，视频生成真的全方位的卷起来了。下面是演示和介绍：

简介：
Upscale-A-Video的文本引导潜在扩散框架，用于视频放大。该框架通过两个关键机制确保时间上的一致性：在局部上，它将时间层集成到U-Net和VAE-Decoder中，保持短序列的一致性；
在全局上，引入了一个基于流引导的经常性潜在传播模块，通过在整个序列中传播和融合潜在来增强整体视频的稳定性。
由于扩散范式，模型还通过允许文本提示来引导纹理创建和可调噪声水平来平衡恢复和生成，从而在保真度和质量之间实现权衡。

方法：
高级视频使用本地和全局策略处理长视频，以保持时间上的连贯性。它将视频分成片段，并使用具有时间层的U-Net来处理它们，以实现片段内的一致性。在用户指定的全局细化扩散步骤中，使用循环潜在传播模块来增强片段间的一致性。最后，经过微调的VAE-Decoder减少剩余的闪烁伪影，以实现低级一致性。

结果：
广泛的实验表明，Upscale-A-Video在合成和真实世界的基准测试中超过了现有的方法，以及在人工智能生成的视频中展示出令人印象深刻的视觉逼真和时间一致性。

项目地址：https://shangchenzhou.com/projects/upscale-a-video/</title>
            <link>https://nitter.cz/op7418/status/1734497196953985354#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734497196953985354#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:55:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>南洋理工发布了一个 AI 视频放大算法 Upscale-A-Video，视频生成真的全方位的卷起来了。下面是演示和介绍：<br />
<br />
简介：<br />
Upscale-A-Video的文本引导潜在扩散框架，用于视频放大。该框架通过两个关键机制确保时间上的一致性：在局部上，它将时间层集成到U-Net和VAE-Decoder中，保持短序列的一致性；<br />
在全局上，引入了一个基于流引导的经常性潜在传播模块，通过在整个序列中传播和融合潜在来增强整体视频的稳定性。<br />
由于扩散范式，模型还通过允许文本提示来引导纹理创建和可调噪声水平来平衡恢复和生成，从而在保真度和质量之间实现权衡。<br />
<br />
方法：<br />
高级视频使用本地和全局策略处理长视频，以保持时间上的连贯性。它将视频分成片段，并使用具有时间层的U-Net来处理它们，以实现片段内的一致性。在用户指定的全局细化扩散步骤中，使用循环潜在传播模块来增强片段间的一致性。最后，经过微调的VAE-Decoder减少剩余的闪烁伪影，以实现低级一致性。<br />
<br />
结果：<br />
广泛的实验表明，Upscale-A-Video在合成和真实世界的基准测试中超过了现有的方法，以及在人工智能生成的视频中展示出令人印象深刻的视觉逼真和时间一致性。<br />
<br />
项目地址：<a href="https://shangchenzhou.com/projects/upscale-a-video/">shangchenzhou.com/projects/u…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTY0MzU4NTk3OTU5NjkvcHUvaW1nL1ZZLXdpMmk0ZkNPYUVLSGYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734548820602864050#m</id>
            <title>RT by @dotey: @arvin17x  开发的 Lobehub 昨天突然在推上爆了。
在 Open AI 套壳的开源项目里面他们的视觉表现和体验确实是独一份的。界面非常漂亮，同时交互细节打磨的也很成熟。
也支持了 GPT-4V 视觉模型交互和 TTS 。同时还有生态非常好的 Agents 插件市场。
未来还会支持几个DALL-E 和 MJ AI 画图能力。
顺便 SD WebUI 都在用的Lobe theme主题也是他们做的。

项目地址：https://chat-preview.lobehub.com/welcome</title>
            <link>https://nitter.cz/op7418/status/1734548820602864050#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734548820602864050#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 12:20:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/arvin17x" title="空谷 · Arvin Xu">@arvin17x</a>  开发的 Lobehub 昨天突然在推上爆了。<br />
在 Open AI 套壳的开源项目里面他们的视觉表现和体验确实是独一份的。界面非常漂亮，同时交互细节打磨的也很成熟。<br />
也支持了 GPT-4V 视觉模型交互和 TTS 。同时还有生态非常好的 Agents 插件市场。<br />
未来还会支持几个DALL-E 和 MJ AI 画图能力。<br />
顺便 SD WebUI 都在用的Lobe theme主题也是他们做的。<br />
<br />
项目地址：<a href="https://chat-preview.lobehub.com/welcome">chat-preview.lobehub.com/wel…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JKWi1Sd2FRQUFDdWprLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734495786208272824#m</id>
            <title>RT by @dotey: Upscale-A-Video：AI提升视频画质
演示视频选的好哇。
项目地址：https://shangchenzhou.com/projects/upscale-a-video/
Github（代码暂未发布）：https://github.com/sczhou/Upscale-A-Video</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734495786208272824#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734495786208272824#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:49:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Upscale-A-Video：AI提升视频画质<br />
演示视频选的好哇。<br />
项目地址：<a href="https://shangchenzhou.com/projects/upscale-a-video/">shangchenzhou.com/projects/u…</a><br />
Github（代码暂未发布）：<a href="https://github.com/sczhou/Upscale-A-Video">github.com/sczhou/Upscale-A-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTU2ODY1MTM4NzI4OTYvcHUvaW1nL05BYUl5RkxMLWt0QzJqN2guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734499555654377759#m</id>
            <title>RT by @dotey: 这个项目挺搞笑的 哈哈哈 你们看看😂

CLoT：训练LLM成为吐槽能手

用日本传统喜剧游戏“大喜利”（Oogiri）作为测试，挑战AI以吐槽高手的方式回应信息。

游戏中的挑战，AI需要理解给定图文信息来产生幽默搞笑的回答。

Oogiri 是一种需要参与者对给定的图像文做出意想不到且幽默的回应的创意游戏。

测试包括图像到文本（I2T）、文本到文本（T2T）和图像&amp;文本到文本（IT2T）

具体方法：

建立数据集：研究人员构建了一个多模态、多语言的 Oogiri-GO 数据集，包含超过 130000 个样本。

训练 AI：通过特殊的训练方法，让 AI 学会如何在游戏中给出创意和幽默的回答。

CLoT 首先将 Oogiri-GO 数据集转化为 LoT 导向的指令调整数据，以训练预训练的 LLM 达到一定的 LoT 幽默生成和辨别能力。

然后，CLoT 设计了一个探索性自我完善过程，鼓励 LLM 通过探索看似无关概念之间的平行关系来生成更多创造性的 LoT 数据，并选择高质量数据进行自我完善。

实验结果：

实验结果显示，CLoT 能够显著提高 LLM（如 Qwen 和 CogVLM）在多种 Oogiri 游戏类型中的表现。具体来说，CLoT 帮助 LLM 生成了更好的幽默内容。

量化性能提升：与原始和 CoT 集成的 LLM 相比，CLoT 集成的 LLM 在 Oogiri 游戏的多项选择和排名问题中取得了更高的性能。

创造性能力的提升：CLoT 还在其他任务（如“云猜测游戏”和“发散性联想任务”）中提高了创造性能力，显示出其卓越的泛化能力。

项目及演示：https://zhongshsh.github.io/CLoT/
论文：https://arxiv.org/abs/2312.02439
GitHub：https://github.com/sail-sg/CLoT</title>
            <link>https://nitter.cz/xiaohuggg/status/1734499555654377759#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734499555654377759#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:04:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个项目挺搞笑的 哈哈哈 你们看看😂<br />
<br />
CLoT：训练LLM成为吐槽能手<br />
<br />
用日本传统喜剧游戏“大喜利”（Oogiri）作为测试，挑战AI以吐槽高手的方式回应信息。<br />
<br />
游戏中的挑战，AI需要理解给定图文信息来产生幽默搞笑的回答。<br />
<br />
Oogiri 是一种需要参与者对给定的图像文做出意想不到且幽默的回应的创意游戏。<br />
<br />
测试包括图像到文本（I2T）、文本到文本（T2T）和图像&文本到文本（IT2T）<br />
<br />
具体方法：<br />
<br />
建立数据集：研究人员构建了一个多模态、多语言的 Oogiri-GO 数据集，包含超过 130000 个样本。<br />
<br />
训练 AI：通过特殊的训练方法，让 AI 学会如何在游戏中给出创意和幽默的回答。<br />
<br />
CLoT 首先将 Oogiri-GO 数据集转化为 LoT 导向的指令调整数据，以训练预训练的 LLM 达到一定的 LoT 幽默生成和辨别能力。<br />
<br />
然后，CLoT 设计了一个探索性自我完善过程，鼓励 LLM 通过探索看似无关概念之间的平行关系来生成更多创造性的 LoT 数据，并选择高质量数据进行自我完善。<br />
<br />
实验结果：<br />
<br />
实验结果显示，CLoT 能够显著提高 LLM（如 Qwen 和 CogVLM）在多种 Oogiri 游戏类型中的表现。具体来说，CLoT 帮助 LLM 生成了更好的幽默内容。<br />
<br />
量化性能提升：与原始和 CoT 集成的 LLM 相比，CLoT 集成的 LLM 在 Oogiri 游戏的多项选择和排名问题中取得了更高的性能。<br />
<br />
创造性能力的提升：CLoT 还在其他任务（如“云猜测游戏”和“发散性联想任务”）中提高了创造性能力，显示出其卓越的泛化能力。<br />
<br />
项目及演示：<a href="https://zhongshsh.github.io/CLoT/">zhongshsh.github.io/CLoT/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02439">arxiv.org/abs/2312.02439</a><br />
GitHub：<a href="https://github.com/sail-sg/CLoT">github.com/sail-sg/CLoT</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTgyODU0MTM2OTEzOTIvcHUvaW1nL3pVYWk0YTlWeWRBbFFFSHcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734465940039807357#m</id>
            <title>给 CSDN 《新程序员》写的一个稿子：《2023 年，我患上了 AI 焦虑症》，CSDN 公众号上刚发，我在自己博客上也发了一份。

CSDN 公众号链接：https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw

博客地址：https://baoyu.io/blog/ai/i-am-suffering-from-ai-anxiety-in-2023</title>
            <link>https://nitter.cz/dotey/status/1734465940039807357#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734465940039807357#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:51:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>给 CSDN 《新程序员》写的一个稿子：《2023 年，我患上了 AI 焦虑症》，CSDN 公众号上刚发，我在自己博客上也发了一份。<br />
<br />
CSDN 公众号链接：<a href="https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw">mp.weixin.qq.com/s/LbRvR1VXp…</a><br />
<br />
博客地址：<a href="https://baoyu.io/blog/ai/i-am-suffering-from-ai-anxiety-in-2023">baoyu.io/blog/ai/i-am-suffer…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJUEw5NFdvQUEzdk9nLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734465344847167767#m</id>
            <title>推荐阅读：《探索编写提示词的乐趣：蒙特卡洛方法、木偶剧和笑声的融合 [译]》

来自 Instacart 官方技术博客的分享，里面有不少写Prompt的技巧，比如CoT、ReAct、“思维的空间”（Room for Thought）、 自我反思 等等

另外文章末尾也分享了很多有价值的Prompt相关的链接

原文：https://tech.instacart.com/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering-4b9272e0c4eb

翻译：https://baoyu.io/translations/llm/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering</title>
            <link>https://nitter.cz/dotey/status/1734465344847167767#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734465344847167767#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:48:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《探索编写提示词的乐趣：蒙特卡洛方法、木偶剧和笑声的融合 [译]》<br />
<br />
来自 Instacart 官方技术博客的分享，里面有不少写Prompt的技巧，比如CoT、ReAct、“思维的空间”（Room for Thought）、 自我反思 等等<br />
<br />
另外文章末尾也分享了很多有价值的Prompt相关的链接<br />
<br />
原文：<a href="https://tech.instacart.com/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering-4b9272e0c4eb">tech.instacart.com/monte-car…</a><br />
<br />
翻译：<a href="https://baoyu.io/translations/llm/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering">baoyu.io/translations/llm/mo…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJUEJpV1hVQUFnc1FTLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734451913104543773#m</id>
            <title>有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的

转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练

这个将图像转换为视频的新模型因为人们认为它有可能替代 TikTok 上的网红而在本周迅速走红。然而，这项技术本身就已经内置了从内容创作者那里盗用作品的行为。

最近，中国零售与科技巨头阿里巴巴的研究团队发表了一篇新论文，介绍了他们的新模型——“Animate Anyone.”。这一消息在网上引发了热议，普遍看法是“TikTokers 的末日来临”，意味着用 AI 技术很快就能取代 TikTok 上的舞蹈内容创作者。

该模型能够接收输入数据（例如 TikTok 舞蹈视频），并输出新的版本。这次实验的结果相比之前类似尝试略有提升。大多数情况下，他们会复制已有的舞蹈视频，但在服装或风格上有所不同，整体效果略逊一筹。但正如 AI 技术的不断进步，这一模型也将持续优化。

已有人指出，“Animate Anyone”可能会被滥用，用来制作未经同意的、将人置于虚构场景的视频。实际上，自六年前这项技术问世以来，这已经成为深度伪造技术的主要用途。

然而，这不仅仅是一个遥远的预测：研究人员已经在未经许可的情况下使用了他人的作品，这已成为他们训练和构建模型的常规做法。阿里巴巴的这篇论文实际上是将最初由明尼苏达大学研究人员出于学术目的创建的“TikTok 数据集”商业化。404 Media 的快速检视显示，阿里巴巴的新 AI 是基于一个抓取了许多知名 TikTok 创作者视频的模型训练而成的，包括 Charli D’Amelio、Addison Rae、Ashley Nocera、Stina Kayy 等几十位。TikTok 数据集中也包括了一些几乎无名的 TikTok 账户用户。

图二：来自《Animate Anyone》论文的参考图像，动力推动帖，DISCO 模型的示例和阿里巴巴的成果展示。

在“Animate Anyone”研究论文的网站上，特别展示了一些著名的 TikTok 内容创作者，作为该模型成功运作的例证。在这些案例中，研究人员采用了这些知名 TikTok 影响者的视频作为参考图像，随后通过阿里巴巴开发的模型进行深度处理，制作出较差质量的 AI 生成副本。

这篇论文及其“Animate Anyone”模型的成果，是建立在未经授权使用创作者作品的基础上的。研究团队在他们的项目页面上，以三位网络名人和艺术家作为示例：Jasmine Chiswell（一位拥有近 1700 万 TikTok 粉丝的生活方式 YouTuber 和 TikTok 名人）、Mackenzie Ziegler（一名歌手和演员，因儿时在《Dance Moms》中出演而知名，拥有 2350 万 TikTok 粉丝）以及 Anna Šulcová（一名 YouTube 内容创作者，拥有 889,600 TikTok 粉丝）。

这些女性都依靠她们独立的创意工作谋生，但阿里巴巴团队却未经许可地使用了她们的作品来支持他们的研究。论文中还展示了更多的 TikTok 创作者，论文已在 arXiv 预印本服务器上发表。

阿里巴巴的研究人员在论文中提到，他们使用了包含 340 个训练视频和 100 个测试视频的“TikTok 数据集”，这些视频都是单人舞蹈，时长在 10 到 15 秒之间。该数据集源于 2021 年明尼苏达大学的一个项目，名为“观看社交媒体舞蹈视频来学习穿着人物的高保真深度”，该项目提出了一种用于“估计人体深度和恢复人体形状的方法”，例如使用 AI 技术在视频中为人物更换服装。

明尼苏达大学的研究人员指出：“我们手动筛选了超过 300 个 TikTok 舞蹈挑战视频，这些视频涵盖了各个月份、不同类型和风格的单人舞蹈。我们选择的舞蹈动作较为温和，以减少运动模糊的产生。对于每个视频，我们都以每秒 30 帧的速度提取了 RGB 图像，总计超过 100,000 张图像。”

大部分人工智能 (AI) 数据集是由在互联网上，如 TikTok 等社交网络，未经内容所有者同意便擦取的视频、图片和文字构成的。在这个例子中，一些博士生组织并启动的数据集，被全球最大的科技及零售公司之一所使用。

这样的情况并不罕见：一开始为学术研究而创建的大型数据集，最终被大公司用于商业目的，无论是相似的还是完全不同的。例如，北卡罗来纳大学威尔明顿分校的研究团队就曾从 YouTube 上擦取了跨性别人士上传的视频，并将其整合成一个数据库，用来研发一种能通过面部识别技术识别跨性别人士的技术。

在当前法律环境日趋严峻的背景下，如阿里巴巴的 AI 研究人员正使用充斥着用户生成内容的擦取数据集。艺术家和其他创作者因 AI 公司未经许可使用他们的作品而提起诉讼。代表艺术家的一起集体诉讼案针对 Midjourney、DeviantArt 和 Stability AI，在去年十月一位法官驳回部分诉求后，又增加了更多原告并提交了修改后的诉状。艺术家们认为，这些 AI 图像生成器复制了原告的作品，并且“创建了与其训练所用作品非常相似的替代品，无论是特定的训练图像还是模仿某些艺术家特有风格的图像，包括原告本人”。

上个月，一位联邦法官推翻了去年驳回编舞家 Kyle Hanagami 对 Epic Games 诉讼的决定。Hanagami 声称 Fortnite 使用了他的舞蹈动作作为“表情动作”。

“把编舞简化成‘姿势’，就好比把音乐仅仅看作‘音符’。编舞，本质上是一系列相关连的舞蹈动作和模式，它们被有机地编排成一个完整的作品，”法官如是写道。“这些动作和模式之间的互动，以及编舞者如何创新地将它们融合和安排，构成了这个作品的核心。仅仅是‘姿势’这个元素，远远不能全面展现出一个编舞作品中的创意表达。”

Hanagami 的律师对 Billboard 表示，推翻之前的驳回裁决将可能对编舞家及其他创意人士在短视频数字媒体时代的权利产生深远影响。这一切对阿里巴巴正在尝试通过“Animate Anyone”项目所打造的内容都有重大的意义，学者们在建立涉及真实人类内容的大型数据集时，也应深思这些决策的未来后果。

https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/</title>
            <link>https://nitter.cz/dotey/status/1734451913104543773#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734451913104543773#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:55:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的<br />
<br />
转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练<br />
<br />
这个将图像转换为视频的新模型因为人们认为它有可能替代 TikTok 上的网红而在本周迅速走红。然而，这项技术本身就已经内置了从内容创作者那里盗用作品的行为。<br />
<br />
最近，中国零售与科技巨头阿里巴巴的研究团队发表了一篇新论文，介绍了他们的新模型——“Animate Anyone.”。这一消息在网上引发了热议，普遍看法是“TikTokers 的末日来临”，意味着用 AI 技术很快就能取代 TikTok 上的舞蹈内容创作者。<br />
<br />
该模型能够接收输入数据（例如 TikTok 舞蹈视频），并输出新的版本。这次实验的结果相比之前类似尝试略有提升。大多数情况下，他们会复制已有的舞蹈视频，但在服装或风格上有所不同，整体效果略逊一筹。但正如 AI 技术的不断进步，这一模型也将持续优化。<br />
<br />
已有人指出，“Animate Anyone”可能会被滥用，用来制作未经同意的、将人置于虚构场景的视频。实际上，自六年前这项技术问世以来，这已经成为深度伪造技术的主要用途。<br />
<br />
然而，这不仅仅是一个遥远的预测：研究人员已经在未经许可的情况下使用了他人的作品，这已成为他们训练和构建模型的常规做法。阿里巴巴的这篇论文实际上是将最初由明尼苏达大学研究人员出于学术目的创建的“TikTok 数据集”商业化。404 Media 的快速检视显示，阿里巴巴的新 AI 是基于一个抓取了许多知名 TikTok 创作者视频的模型训练而成的，包括 Charli D’Amelio、Addison Rae、Ashley Nocera、Stina Kayy 等几十位。TikTok 数据集中也包括了一些几乎无名的 TikTok 账户用户。<br />
<br />
图二：来自《Animate Anyone》论文的参考图像，动力推动帖，DISCO 模型的示例和阿里巴巴的成果展示。<br />
<br />
在“Animate Anyone”研究论文的网站上，特别展示了一些著名的 TikTok 内容创作者，作为该模型成功运作的例证。在这些案例中，研究人员采用了这些知名 TikTok 影响者的视频作为参考图像，随后通过阿里巴巴开发的模型进行深度处理，制作出较差质量的 AI 生成副本。<br />
<br />
这篇论文及其“Animate Anyone”模型的成果，是建立在未经授权使用创作者作品的基础上的。研究团队在他们的项目页面上，以三位网络名人和艺术家作为示例：Jasmine Chiswell（一位拥有近 1700 万 TikTok 粉丝的生活方式 YouTuber 和 TikTok 名人）、Mackenzie Ziegler（一名歌手和演员，因儿时在《Dance Moms》中出演而知名，拥有 2350 万 TikTok 粉丝）以及 Anna Šulcová（一名 YouTube 内容创作者，拥有 889,600 TikTok 粉丝）。<br />
<br />
这些女性都依靠她们独立的创意工作谋生，但阿里巴巴团队却未经许可地使用了她们的作品来支持他们的研究。论文中还展示了更多的 TikTok 创作者，论文已在 arXiv 预印本服务器上发表。<br />
<br />
阿里巴巴的研究人员在论文中提到，他们使用了包含 340 个训练视频和 100 个测试视频的“TikTok 数据集”，这些视频都是单人舞蹈，时长在 10 到 15 秒之间。该数据集源于 2021 年明尼苏达大学的一个项目，名为“观看社交媒体舞蹈视频来学习穿着人物的高保真深度”，该项目提出了一种用于“估计人体深度和恢复人体形状的方法”，例如使用 AI 技术在视频中为人物更换服装。<br />
<br />
明尼苏达大学的研究人员指出：“我们手动筛选了超过 300 个 TikTok 舞蹈挑战视频，这些视频涵盖了各个月份、不同类型和风格的单人舞蹈。我们选择的舞蹈动作较为温和，以减少运动模糊的产生。对于每个视频，我们都以每秒 30 帧的速度提取了 RGB 图像，总计超过 100,000 张图像。”<br />
<br />
大部分人工智能 (AI) 数据集是由在互联网上，如 TikTok 等社交网络，未经内容所有者同意便擦取的视频、图片和文字构成的。在这个例子中，一些博士生组织并启动的数据集，被全球最大的科技及零售公司之一所使用。<br />
<br />
这样的情况并不罕见：一开始为学术研究而创建的大型数据集，最终被大公司用于商业目的，无论是相似的还是完全不同的。例如，北卡罗来纳大学威尔明顿分校的研究团队就曾从 YouTube 上擦取了跨性别人士上传的视频，并将其整合成一个数据库，用来研发一种能通过面部识别技术识别跨性别人士的技术。<br />
<br />
在当前法律环境日趋严峻的背景下，如阿里巴巴的 AI 研究人员正使用充斥着用户生成内容的擦取数据集。艺术家和其他创作者因 AI 公司未经许可使用他们的作品而提起诉讼。代表艺术家的一起集体诉讼案针对 Midjourney、DeviantArt 和 Stability AI，在去年十月一位法官驳回部分诉求后，又增加了更多原告并提交了修改后的诉状。艺术家们认为，这些 AI 图像生成器复制了原告的作品，并且“创建了与其训练所用作品非常相似的替代品，无论是特定的训练图像还是模仿某些艺术家特有风格的图像，包括原告本人”。<br />
<br />
上个月，一位联邦法官推翻了去年驳回编舞家 Kyle Hanagami 对 Epic Games 诉讼的决定。Hanagami 声称 Fortnite 使用了他的舞蹈动作作为“表情动作”。<br />
<br />
“把编舞简化成‘姿势’，就好比把音乐仅仅看作‘音符’。编舞，本质上是一系列相关连的舞蹈动作和模式，它们被有机地编排成一个完整的作品，”法官如是写道。“这些动作和模式之间的互动，以及编舞者如何创新地将它们融合和安排，构成了这个作品的核心。仅仅是‘姿势’这个元素，远远不能全面展现出一个编舞作品中的创意表达。”<br />
<br />
Hanagami 的律师对 Billboard 表示，推翻之前的驳回裁决将可能对编舞家及其他创意人士在短视频数字媒体时代的权利产生深远影响。这一切对阿里巴巴正在尝试通过“Animate Anyone”项目所打造的内容都有重大的意义，学者们在建立涉及真实人类内容的大型数据集时，也应深思这些决策的未来后果。<br />
<br />
<a href="https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/">404media.co/alibaba-animate-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJQ3hwTlc4QUFIckVqLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJQ3pBSlhrQUF4WE4zLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734446460458955228#m</id>
            <title>用自然语言进行无反转图像编辑 InfEdit

项目地址：https://sled-group.github.io/InfEdit/
在线测试：http://sled-snowbird.eecs.umich.edu/
论文地址：https://arxiv.org/abs/2312.04965

摘要

我们推出了一种名为无反转编辑（InfEdit）的新方法。这种方法能在图像的语义和空间层面上做出细致且一致的编辑，既能适应复杂的修改需求，又能保持图像的完整性，避免了直接反转图像的步骤。通过大量实验，我们发现 InfEdit 在处理复杂的编辑任务时表现卓越，且工作流程高效流畅（在一台 A40 设备上不到 3 秒），显示出在实时应用方面的巨大潜力。

方法介绍
我们的目标是去除图像编辑中的反转过程。为此，我们引入了一种名为去噪扩散一致模型（DDCM）的采样策略，它能实现所谓的“虚拟反转”。DDCM 利用了扩散过程，这大幅提升了图像生成各阶段的一致性，确保了视觉内容在变换和精细化过程中的忠实度和速度。

此外，我们还提出了统一注意力控制（UAC）机制。通过自然语言，它能实现无需手动调整的图像编辑，将交叉注意力和自我注意力控制融合进一个统一的框架中。</title>
            <link>https://nitter.cz/dotey/status/1734446460458955228#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734446460458955228#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:33:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>用自然语言进行无反转图像编辑 InfEdit<br />
<br />
项目地址：<a href="https://sled-group.github.io/InfEdit/">sled-group.github.io/InfEdit…</a><br />
在线测试：<a href="http://sled-snowbird.eecs.umich.edu/">sled-snowbird.eecs.umich.edu…</a><br />
论文地址：<a href="https://arxiv.org/abs/2312.04965">arxiv.org/abs/2312.04965</a><br />
<br />
摘要<br />
<br />
我们推出了一种名为无反转编辑（InfEdit）的新方法。这种方法能在图像的语义和空间层面上做出细致且一致的编辑，既能适应复杂的修改需求，又能保持图像的完整性，避免了直接反转图像的步骤。通过大量实验，我们发现 InfEdit 在处理复杂的编辑任务时表现卓越，且工作流程高效流畅（在一台 A40 设备上不到 3 秒），显示出在实时应用方面的巨大潜力。<br />
<br />
方法介绍<br />
我们的目标是去除图像编辑中的反转过程。为此，我们引入了一种名为去噪扩散一致模型（DDCM）的采样策略，它能实现所谓的“虚拟反转”。DDCM 利用了扩散过程，这大幅提升了图像生成各阶段的一致性，确保了视觉内容在变换和精细化过程中的忠实度和速度。<br />
<br />
此外，我们还提出了统一注意力控制（UAC）机制。通过自然语言，它能实现无需手动调整的图像编辑，将交叉注意力和自我注意力控制融合进一个统一的框架中。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0JIOW43dFdrQUE4YlpULmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dCSDluN3RXa0FBOGJaVC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734442517100187689#m</id>
            <title>#AI开源项目推荐：Vary

一套视觉感知上限极高的通用多模态框架：Vary

有多模态能力，并且可以做 OCR 和公式识别。

底层是基于LLaVA 和 Qwen（通义千问）

测试了一下，中英文 OCR 识别准确率相当高，公式识别也很不错，测试了一篇文章里的公式，只有一个地方把 g 认成了 σ ，其他都对了。

在线演示：http://region-31.seetacloud.com:22701/
项目地址：https://github.com/Ucas-HaoranWei/Vary
论文：https://arxiv.org/abs/2312.06109
知乎上的介绍：https://zhuanlan.zhihu.com/p/671420712</title>
            <link>https://nitter.cz/dotey/status/1734442517100187689#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734442517100187689#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:18:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：Vary<br />
<br />
一套视觉感知上限极高的通用多模态框架：Vary<br />
<br />
有多模态能力，并且可以做 OCR 和公式识别。<br />
<br />
底层是基于LLaVA 和 Qwen（通义千问）<br />
<br />
测试了一下，中英文 OCR 识别准确率相当高，公式识别也很不错，测试了一篇文章里的公式，只有一个地方把 g 认成了 σ ，其他都对了。<br />
<br />
在线演示：<a href="http://region-31.seetacloud.com:22701/">region-31.seetacloud.com:227…</a><br />
项目地址：<a href="https://github.com/Ucas-HaoranWei/Vary">github.com/Ucas-HaoranWei/Va…</a><br />
论文：<a href="https://arxiv.org/abs/2312.06109">arxiv.org/abs/2312.06109</a><br />
知乎上的介绍：<a href="https://zhuanlan.zhihu.com/p/671420712">zhuanlan.zhihu.com/p/6714207…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JINEJmMVhnQUFOZy1QLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JINVVkM1dFQUF6MmhzLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JINW9ObldNQUFTNXhYLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JINXJRVlcwQUVuYjlMLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734432613241073793#m</id>
            <title>Arc的Windows版居然是Swift开发的！</title>
            <link>https://nitter.cz/dotey/status/1734432613241073793#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734432613241073793#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 04:38:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Arc的Windows版居然是Swift开发的！</p>
<p><a href="https://nitter.cz/joshm/status/1734262618548793804#m">nitter.cz/joshm/status/1734262618548793804#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>