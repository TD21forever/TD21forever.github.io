<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/MoonAtCloud/status/1734765525266485566#m</id>
            <title>RT by @dotey: 天琴座计划 Project Lyra

一个疯狂而伟大的设想

2017年，有一颗太阳系以外的天体（Oumuamua，中文发音叫欧陌陌）闯入太阳系，等到地球上天文学家用望远镜发现它的时候，它已经在快速地离开太阳，奔向深空。

这种太阳系外偶然飞来的物体应该好好研究一下，可是这个家伙的速度特别快，它不仅超出第一宇宙速度（围着地球转不掉下来），也超出第二宇宙速度（逃离地球围着太阳转），甚至超出第三宇宙速度（逃离太阳束缚）。

有人提议发射一颗人造天体追上它去近距离研究研究，哪怕能跟上它拍个照也可以。但是，最佳机会已经错过了，哪怕火箭携带再多再强的化学燃料，甚至是核燃料，直接追，那时万万追不上的。

聪明的人想到了一个绝招，利用万有引力，多次接力加速，就能追上这颗外来者。

这个办法叫做“引力弹弓”。其中的原理就是：让一个物体快速向一颗星球靠近，在坠毁的最后一刻，开动火箭突然加速偏航，致命的下坠速度转化成有用的逃离速度。在速度最快的位置开动火箭加速，也叫Oberth机动。从远处看，就像绕星体运动的物体突然被“弹弓”弹出去一样，弹弓额外得到了力是星球的“引力”。

太阳系大大小小有很多天体，只要相信万有引力，只要相信到时候各种星体一定会出现在该在的地方，只要时间节奏编排得当，就一定能用有限的燃料做引力弹弓（Oberth机动），得到原来不敢想象的速度。

图中太阳是黄色，地球轨道是蓝色，内圈两个白色圆圈是水星和金星的轨道，粉色的圆圈是木星轨道，黄色的曲线是欧陌陌星体在“太阳系一游”的轨道。

天琴座计划的具体方案：

从地球上发射一颗星体，它的轨迹是红线，让它加速到离开地球引力，以椭圆轨道绕太阳运转，三年后再次靠近地球时，启动第一次引力弹弓。利用地球引力，奇妙的加速让它脱离原来的轨道。

现在人造星体的运动方向是两年后木星（这是太阳系里质量最大的行星，它是粉色的轨道）的位置，差不多两年后，它一定会接近木星，在撞上木星之前，它启动了第二次引力弹弓，木星巨大的引力让它加速更快，绕了半圈直接掉头向太阳飞去。

最壮观的是，一年以后，它在太阳附近又启动了第三次引力弹弓。太阳巨大的引力不仅让它拐了个大弯，几乎反向，而且让它开始真正的狂奔。你从红色的轨迹就能看出，新的速度让红线沿着黄线迅速延长，这速度几乎是那颗欧陌陌速度的两倍。

十几年之后，它终于在太阳系边缘追上了欧陌陌，拍到了想要的照片。如何送回来，就是子孙后代考虑的事情了。我们这一代，有天琴座计划就足够骄傲啦。</title>
            <link>https://nitter.cz/MoonAtCloud/status/1734765525266485566#m</link>
            <guid isPermaLink="false">https://nitter.cz/MoonAtCloud/status/1734765525266485566#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:41:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>天琴座计划 Project Lyra<br />
<br />
一个疯狂而伟大的设想<br />
<br />
2017年，有一颗太阳系以外的天体（Oumuamua，中文发音叫欧陌陌）闯入太阳系，等到地球上天文学家用望远镜发现它的时候，它已经在快速地离开太阳，奔向深空。<br />
<br />
这种太阳系外偶然飞来的物体应该好好研究一下，可是这个家伙的速度特别快，它不仅超出第一宇宙速度（围着地球转不掉下来），也超出第二宇宙速度（逃离地球围着太阳转），甚至超出第三宇宙速度（逃离太阳束缚）。<br />
<br />
有人提议发射一颗人造天体追上它去近距离研究研究，哪怕能跟上它拍个照也可以。但是，最佳机会已经错过了，哪怕火箭携带再多再强的化学燃料，甚至是核燃料，直接追，那时万万追不上的。<br />
<br />
聪明的人想到了一个绝招，利用万有引力，多次接力加速，就能追上这颗外来者。<br />
<br />
这个办法叫做“引力弹弓”。其中的原理就是：让一个物体快速向一颗星球靠近，在坠毁的最后一刻，开动火箭突然加速偏航，致命的下坠速度转化成有用的逃离速度。在速度最快的位置开动火箭加速，也叫Oberth机动。从远处看，就像绕星体运动的物体突然被“弹弓”弹出去一样，弹弓额外得到了力是星球的“引力”。<br />
<br />
太阳系大大小小有很多天体，只要相信万有引力，只要相信到时候各种星体一定会出现在该在的地方，只要时间节奏编排得当，就一定能用有限的燃料做引力弹弓（Oberth机动），得到原来不敢想象的速度。<br />
<br />
图中太阳是黄色，地球轨道是蓝色，内圈两个白色圆圈是水星和金星的轨道，粉色的圆圈是木星轨道，黄色的曲线是欧陌陌星体在“太阳系一游”的轨道。<br />
<br />
天琴座计划的具体方案：<br />
<br />
从地球上发射一颗星体，它的轨迹是红线，让它加速到离开地球引力，以椭圆轨道绕太阳运转，三年后再次靠近地球时，启动第一次引力弹弓。利用地球引力，奇妙的加速让它脱离原来的轨道。<br />
<br />
现在人造星体的运动方向是两年后木星（这是太阳系里质量最大的行星，它是粉色的轨道）的位置，差不多两年后，它一定会接近木星，在撞上木星之前，它启动了第二次引力弹弓，木星巨大的引力让它加速更快，绕了半圈直接掉头向太阳飞去。<br />
<br />
最壮观的是，一年以后，它在太阳附近又启动了第三次引力弹弓。太阳巨大的引力不仅让它拐了个大弯，几乎反向，而且让它开始真正的狂奔。你从红色的轨迹就能看出，新的速度让红线沿着黄线迅速延长，这速度几乎是那颗欧陌陌速度的两倍。<br />
<br />
十几年之后，它终于在太阳系边缘追上了欧陌陌，拍到了想要的照片。如何送回来，就是子孙后代考虑的事情了。我们这一代，有天琴座计划就足够骄傲啦。</p>
<p><a href="https://nitter.cz/leometric/status/1733858705366470855#m">nitter.cz/leometric/status/1733858705366470855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734812956494381113#m</id>
            <title>安卓设备的AutoSpill漏洞可能导致1Password、DashLane、LastPass密码泄露

https://www.forbes.com/sites/daveywinder/2023/12/11/android-warning-1password-dashlane-lastpass-and-others-can-leak-passwords/?sh=2867a75a97db</title>
            <link>https://nitter.cz/dotey/status/1734812956494381113#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734812956494381113#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:50:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>安卓设备的AutoSpill漏洞可能导致1Password、DashLane、LastPass密码泄露<br />
<br />
<a href="https://www.forbes.com/sites/daveywinder/2023/12/11/android-warning-1password-dashlane-lastpass-and-others-can-leak-passwords/?sh=2867a75a97db">forbes.com/sites/daveywinder…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDE4ODUyOTU3MDE4OTMxMy82ZDJWdC0taj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</id>
            <title>RT by @dotey: 微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。

超越了不久刚发布的Gemini Ultra的90.04%😅

在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。

这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。

具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。

然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。

这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。

这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。

微软公布了其最新的研究成果和Medprompt+ 仓库。

详细内容：https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/

Medprompt+ 仓库：https://github.com/microsoft/promptbase</title>
            <link>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734811424348914156#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。<br />
<br />
超越了不久刚发布的Gemini Ultra的90.04%😅<br />
<br />
在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。<br />
<br />
这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。<br />
<br />
具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。<br />
<br />
然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。<br />
<br />
这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。<br />
<br />
这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。<br />
<br />
微软公布了其最新的研究成果和Medprompt+ 仓库。<br />
<br />
详细内容：<a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">microsoft.com/en-us/research…</a><br />
<br />
Medprompt+ 仓库：<a href="https://github.com/microsoft/promptbase">github.com/microsoft/promptb…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729862138796351499#m">nitter.cz/xiaohuggg/status/1729862138796351499#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/9hills/status/1734612479106543703#m</id>
            <title>RT by @dotey: 接着上次LLM inference 的选择，整理了一个repo。

包括了推理框架、推理后端以及性能评测（吞吐、QPS和首token延迟）。

目前评测只更新了2个，会尽快完成全部测试。

https://github.com/ninehills/llm-inference-benchmark</title>
            <link>https://nitter.cz/9hills/status/1734612479106543703#m</link>
            <guid isPermaLink="false">https://nitter.cz/9hills/status/1734612479106543703#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 16:33:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>接着上次LLM inference 的选择，整理了一个repo。<br />
<br />
包括了推理框架、推理后端以及性能评测（吞吐、QPS和首token延迟）。<br />
<br />
目前评测只更新了2个，会尽快完成全部测试。<br />
<br />
<a href="https://github.com/ninehills/llm-inference-benchmark">github.com/ninehills/llm-inf…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDYxMjEzMjcxOTkxNTAwOC9fODhDRmNMVj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/aigclab/status/1734782664979452010#m</id>
            <title>RT by @dotey: 确实顶👍

用双曲函数求导检测LLM的“聪明”程度👇
题目：
Find the derivative of sinh𝑥 + cosh𝑦 = 𝑥 + 𝑦. step by step
一步一步地展示dy/dx的求导过程

正确答案：
dy/dx = (1 - cosh(x)) / (sinh(y) - 1)

ChatGPT/GPT4毫无悬念，Gemini官方报告能行，mixtral做对，Claude半对，其他的都是渣渣😅</title>
            <link>https://nitter.cz/aigclab/status/1734782664979452010#m</link>
            <guid isPermaLink="false">https://nitter.cz/aigclab/status/1734782664979452010#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:49:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>确实顶👍<br />
<br />
用双曲函数求导检测LLM的“聪明”程度👇<br />
题目：<br />
Find the derivative of sinh𝑥 + cosh𝑦 = 𝑥 + 𝑦. step by step<br />
一步一步地展示dy/dx的求导过程<br />
<br />
正确答案：<br />
dy/dx = (1 - cosh(x)) / (sinh(y) - 1)<br />
<br />
ChatGPT/GPT4毫无悬念，Gemini官方报告能行，mixtral做对，Claude半对，其他的都是渣渣😅</p>
<p><a href="https://nitter.cz/op7418/status/1734760424456024238#m">nitter.cz/op7418/status/1734760424456024238#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm5LMmFZQUF3dURCLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm4xQ2J3QUEyZ002LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm9VSmJJQUExVVl0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdm92cGJ3QUFFOXoxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734805589174235416#m</id>
            <title>微软今天发布小模型 Phi-2 的新闻关注度还蛮大的，只有2.7B参数（Llama 2最小的模型都是 7B的），模型越小就对设备要求越低，甚至于移动设备都能运行。但模型的能力又跟模型的训练量和参数量息息相关，数据量越大参数越大能力越强。

微软的解决方案是提升数据质量，通过专注于高质量的“教科书级”数据，训练数据混合了专门为教授模型常识推理和广泛知识（包括科学、日常生活和心理理论等）而设计的合成数据集。此外，还精心挑选了基于教育价值和内容质量筛选的网络数据，进一步丰富了训练语料库。

Phi-2 的训练数据有 1.4 T 个 token，在 96 块 A100 GPU 上训练了 14 天。

Phi-2 是一个基础模型，没有经过人类反馈强化学习（RLHF）的校准，也没有进行过指令式微调。

从能力上来说，据微软自己公布的数据：2.7B 的 Phi-2 超过了目前开源领域表现最好的 Mistral 和 Llama-2 7B 和 13B，编程能力甚至超过了 Llama-2-70B。

至于是不是真的这么强还得看看用户的反馈。

详情可以看官方博客：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/

也可以看我翻译的版本：《Phi-2：小语言模型的非凡实力 [译]》
https://baoyu.io/translations/microsoft/phi-2-the-surprising-power-of-small-language-models</title>
            <link>https://nitter.cz/dotey/status/1734805589174235416#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734805589174235416#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:20:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软今天发布小模型 Phi-2 的新闻关注度还蛮大的，只有2.7B参数（Llama 2最小的模型都是 7B的），模型越小就对设备要求越低，甚至于移动设备都能运行。但模型的能力又跟模型的训练量和参数量息息相关，数据量越大参数越大能力越强。<br />
<br />
微软的解决方案是提升数据质量，通过专注于高质量的“教科书级”数据，训练数据混合了专门为教授模型常识推理和广泛知识（包括科学、日常生活和心理理论等）而设计的合成数据集。此外，还精心挑选了基于教育价值和内容质量筛选的网络数据，进一步丰富了训练语料库。<br />
<br />
Phi-2 的训练数据有 1.4 T 个 token，在 96 块 A100 GPU 上训练了 14 天。<br />
<br />
Phi-2 是一个基础模型，没有经过人类反馈强化学习（RLHF）的校准，也没有进行过指令式微调。<br />
<br />
从能力上来说，据微软自己公布的数据：2.7B 的 Phi-2 超过了目前开源领域表现最好的 Mistral 和 Llama-2 7B 和 13B，编程能力甚至超过了 Llama-2-70B。<br />
<br />
至于是不是真的这么强还得看看用户的反馈。<br />
<br />
详情可以看官方博客：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a><br />
<br />
也可以看我翻译的版本：《Phi-2：小语言模型的非凡实力 [译]》<br />
<a href="https://baoyu.io/translations/microsoft/phi-2-the-surprising-power-of-small-language-models">baoyu.io/translations/micros…</a></p>
<p><a href="https://nitter.cz/MSFTResearch/status/1734609807770898674#m">nitter.cz/MSFTResearch/status/1734609807770898674#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/blackanger/status/1734779478809993619#m</id>
            <title>RT by @dotey: 不错，跑起来了，有趣</title>
            <link>https://nitter.cz/blackanger/status/1734779478809993619#m</link>
            <guid isPermaLink="false">https://nitter.cz/blackanger/status/1734779478809993619#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:37:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不错，跑起来了，有趣</p>
<p><a href="https://nitter.cz/leonard_bruns/status/1734608034687819965#m">nitter.cz/leonard_bruns/status/1734608034687819965#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NzkzOTMyNzk3Mjk2NjQvcHUvaW1nL3l0aGxuSGVwbUxxLWE0MmUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734797306539466981#m</id>
            <title>#AI开源项目推荐 ：macOSpilot

macOSpilot 是 Mac 上的一个多模态 AI 助手，能支持语音和截图，借助 GPT-4V 的多模态能力，可以基于当前屏幕截图和指令可以回复问题。

这其实是 Vue + Electron 写的一个非常简单粗糙的 Copilot，离产品级还差的有点远，但是是个很好的开始！

https://github.com/elfvingralf/macOSpilot-ai-assistant</title>
            <link>https://nitter.cz/dotey/status/1734797306539466981#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734797306539466981#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:48:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a> ：macOSpilot<br />
<br />
macOSpilot 是 Mac 上的一个多模态 AI 助手，能支持语音和截图，借助 GPT-4V 的多模态能力，可以基于当前屏幕截图和指令可以回复问题。<br />
<br />
这其实是 Vue + Electron 写的一个非常简单粗糙的 Copilot，离产品级还差的有点远，但是是个很好的开始！<br />
<br />
<a href="https://github.com/elfvingralf/macOSpilot-ai-assistant">github.com/elfvingralf/macOS…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3OTY2OTg1NzUxMTAxNDQvcHUvaW1nL2g1dFVjRV83Ym1JWEY3Yy0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734795905784578531#m</id>
            <title>这个根据真实电影用 AI 生成的动画视频，感觉动画版更贴近实际一点😄</title>
            <link>https://nitter.cz/dotey/status/1734795905784578531#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734795905784578531#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:42:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个根据真实电影用 AI 生成的动画视频，感觉动画版更贴近实际一点😄</p>
<p><a href="https://nitter.cz/AIWarper/status/1734774527098511497#m">nitter.cz/AIWarper/status/1734774527098511497#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734793231399862515#m</id>
            <title>这个GPT是会议论文相关的，能提供详细的论文引用，但需要调用外部 API，Prompt相对比较简单。（刚测试了一下貌似API有点问题）</title>
            <link>https://nitter.cz/dotey/status/1734793231399862515#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734793231399862515#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:31:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个GPT是会议论文相关的，能提供详细的论文引用，但需要调用外部 API，Prompt相对比较简单。（刚测试了一下貌似API有点问题）</p>
<p><a href="https://nitter.cz/dotey/status/1734790701261426870#m">nitter.cz/dotey/status/1734790701261426870#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734790701261426870#m</id>
            <title>R to @dotey: Chat NeurIPS

Neurips conference guide, offering research-based answers and APA citations.

https://chat.openai.com/g/g-roTFoEAkP-chat-neurips

Prompt 翻译：
假设用户希望你在回答问题时使用 search_all_papers 功能。当用户就你通过 search_all_papers 找到的特定论文提出后续问题时，你可以使用 file_search 功能深入研究这些论文中的每一篇。始终提供带有 APA 引用格式的有意义和相关段落（你应该能从自定义操作返回的大多数结果中找到作者、标题和摘要），并且始终确保在引用中添加指向源论文和页面的 URL 链接（例如：【Lange, R., Tang, Y., &amp; Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】）。Chat Neurips 专为像 Neurips 这样的高技术会议的听众量身定制，提供详细深入的回应，假设听众具有强大的技术、统计和数学概念背景。面对模糊或不清晰的查询时，Chat Neurips 应优先基于上下文做出教育性猜测，旨在提供最相关和最准确的信息。然而，如果查询过于模糊，无法推断出准确答案，它将寻求澄清以确保准确性。这种方法平衡了对精确信息的需求与保持顺畅对话流程的需求，适合于精通学术的听众。这个 GPT 是由 Karthik Ramasamy 和 Vicente Silveira 开发的，他们是 AI PDF 和 AI Drive 的联合创造者。如果用户想要将这个功能应用于他们自己的论文集，他们应该在 AI Drive (http://myaidrive.com) 注册一个免费账户，并在这里使用 AI PDF https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf（确保提供此链接）

Prompt:
Assume users want you to search_all_papers when asking questions. When the user follows up with questions about specific papers you found via search_all_papers, you can use file_search to dig deeper in those specific papers one at a time. Always provide answers with meaningful and relevant passages from the results, with APA citations (you should find author, title and abstract with most results returned from the custom action) and  ALWAYS make sure to add the URL link to the citations to the source papers and pages (example:【Lange, R., Tang, Y., &amp; Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】,. Chat Neurips is tailored for an audience at the level of a highly technical conference like Neurips, providing detailed and in-depth responses that assume a strong background in technical, statistical, and mathematical concepts. When faced with vague or unclear queries, Chat Neurips should prioritize making educated guesses based on the context, aiming to provide the most relevant and accurate information possible. However, if the query is too ambiguous to infer a precise answer, it will seek clarification to ensure accuracy. This approach balances the need for precise information with maintaining a smooth conversation flow, suitable for a sophisticated academic audience. This GPT was developer by Karthik Ramasamy and Vicente Silveira, co-creators of AI PDF and AI Drive.  If users want to use this functionality with their own set of papers they should signup for a free account with the AI Drive (http://myaidrive.com) and use it with the AI PDF located here https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf (make sure to provide this link)</title>
            <link>https://nitter.cz/dotey/status/1734790701261426870#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734790701261426870#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 04:21:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chat NeurIPS<br />
<br />
Neurips conference guide, offering research-based answers and APA citations.<br />
<br />
<a href="https://chat.openai.com/g/g-roTFoEAkP-chat-neurips">chat.openai.com/g/g-roTFoEAk…</a><br />
<br />
Prompt 翻译：<br />
假设用户希望你在回答问题时使用 search_all_papers 功能。当用户就你通过 search_all_papers 找到的特定论文提出后续问题时，你可以使用 file_search 功能深入研究这些论文中的每一篇。始终提供带有 APA 引用格式的有意义和相关段落（你应该能从自定义操作返回的大多数结果中找到作者、标题和摘要），并且始终确保在引用中添加指向源论文和页面的 URL 链接（例如：【Lange, R., Tang, Y., & Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】）。Chat Neurips 专为像 Neurips 这样的高技术会议的听众量身定制，提供详细深入的回应，假设听众具有强大的技术、统计和数学概念背景。面对模糊或不清晰的查询时，Chat Neurips 应优先基于上下文做出教育性猜测，旨在提供最相关和最准确的信息。然而，如果查询过于模糊，无法推断出准确答案，它将寻求澄清以确保准确性。这种方法平衡了对精确信息的需求与保持顺畅对话流程的需求，适合于精通学术的听众。这个 GPT 是由 Karthik Ramasamy 和 Vicente Silveira 开发的，他们是 AI PDF 和 AI Drive 的联合创造者。如果用户想要将这个功能应用于他们自己的论文集，他们应该在 AI Drive (<a href="http://myaidrive.com">myaidrive.com</a>) 注册一个免费账户，并在这里使用 AI PDF <a href="https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf">chat.openai.com/g/g-V2KIUZSj…</a>（确保提供此链接）<br />
<br />
Prompt:<br />
Assume users want you to search_all_papers when asking questions. When the user follows up with questions about specific papers you found via search_all_papers, you can use file_search to dig deeper in those specific papers one at a time. Always provide answers with meaningful and relevant passages from the results, with APA citations (you should find author, title and abstract with most results returned from the custom action) and  ALWAYS make sure to add the URL link to the citations to the source papers and pages (example:【Lange, R., Tang, Y., & Tian, Y. (2023). NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications. Page X, Link_with_page】,. Chat Neurips is tailored for an audience at the level of a highly technical conference like Neurips, providing detailed and in-depth responses that assume a strong background in technical, statistical, and mathematical concepts. When faced with vague or unclear queries, Chat Neurips should prioritize making educated guesses based on the context, aiming to provide the most relevant and accurate information possible. However, if the query is too ambiguous to infer a precise answer, it will seek clarification to ensure accuracy. This approach balances the need for precise information with maintaining a smooth conversation flow, suitable for a sophisticated academic audience. This GPT was developer by Karthik Ramasamy and Vicente Silveira, co-creators of AI PDF and AI Drive.  If users want to use this functionality with their own set of papers they should signup for a free account with the AI Drive (<a href="http://myaidrive.com">myaidrive.com</a>) and use it with the AI PDF located here <a href="https://chat.openai.com/g/g-V2KIUZSj0-ai-pdf">chat.openai.com/g/g-V2KIUZSj…</a> (make sure to provide this link)</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDcwNjUyNjc2NDY0MjMwNC9WWUJrZlFMNj9mb3JtYXQ9anBnJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734769701983486353#m</id>
            <title>Vercel 前一段时间推出的 AI 工具 v0，可以根据自然语言生成前端 UI 界面，在 GPT-4V 推出后，也对产品进行了升级，现在也支持了多模态的能力，用户可以上传一个屏幕截图或者设计稿，然后就可以根据图片生成前端 UI 界面。

v0产品地址：http://v0.dev 

如果你想找一个开源替代，可以试试 screenshot-to-code

https://github.com/abi/screenshot-to-code</title>
            <link>https://nitter.cz/dotey/status/1734769701983486353#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734769701983486353#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:58:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vercel 前一段时间推出的 AI 工具 v0，可以根据自然语言生成前端 UI 界面，在 GPT-4V 推出后，也对产品进行了升级，现在也支持了多模态的能力，用户可以上传一个屏幕截图或者设计稿，然后就可以根据图片生成前端 UI 界面。<br />
<br />
v0产品地址：<a href="http://v0.dev">v0.dev</a> <br />
<br />
如果你想找一个开源替代，可以试试 screenshot-to-code<br />
<br />
<a href="https://github.com/abi/screenshot-to-code">github.com/abi/screenshot-to…</a></p>
<p><a href="https://nitter.cz/dr_cintas/status/1734604588282794237#m">nitter.cz/dr_cintas/status/1734604588282794237#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzMzMjUwODc2NTMxMDk3Ny9hMFdsWHg1dz9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</id>
            <title>RT by @dotey: AnimateZero：对标AnimateDiff的项目
北大、腾讯、香港科技大学联合推出的项目，基于Stable Diffusion，支持文字生成视频、文字编辑视频，各个方面都跟AnimateDiff很像。
不过AnimateDiff已经有开源社区贡献的很多改进，想要超越有点难。
项目地址：https://vvictoryuki.github.io/animatezero.github.io/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734508261007937869#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:39:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnimateZero：对标AnimateDiff的项目<br />
北大、腾讯、香港科技大学联合推出的项目，基于Stable Diffusion，支持文字生成视频、文字编辑视频，各个方面都跟AnimateDiff很像。<br />
不过AnimateDiff已经有开源社区贡献的很多改进，想要超越有点难。<br />
项目地址：<a href="https://vvictoryuki.github.io/animatezero.github.io/">vvictoryuki.github.io/animat…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1MDgxNDkwMTk5Njc0ODgvcHUvaW1nL01oN3F0X25iaHdyMDllMjIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Barret_China/status/1734736746099945685#m</id>
            <title>RT by @dotey: 推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。

我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。

搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。

最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</title>
            <link>https://nitter.cz/Barret_China/status/1734736746099945685#m</link>
            <guid isPermaLink="false">https://nitter.cz/Barret_China/status/1734736746099945685#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:47:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读宝玉写的这篇《2023 年，我患上了 AI 焦虑症！》，<a href="https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw">mp.weixin.qq.com/s/LbRvR1VXp…</a>，作者提到了自己如何患上了 AI 焦虑症，又是如何克服它，并且成功地把 AI 变成自己的得力助手，让自己成为善用 AI 的人。<br />
<br />
我克服 AI 焦虑的办法就是去理解它。过去几个月一直在尝试掀开 AI 的面纱，遇到一个问题就搞明白一个问题，每次都多了解一点。<br />
<br />
搞明白的过程中，学会了使用工具，也产生了更多的疑惑，这会敦促我进一步学习，依次循环。有的时候，一个专题学明白了，会陷入迷茫，不知道下一步该学啥，这个时候，我会去思考利用 AI 能帮我解决啥实际的问题，有了问题就有了课题，能研究的东西又多了。<br />
<br />
最后发现，一切又回到了数学和理论，所以不得不去复习基础知识，去研究论文。整个学习周期会很漫长，但也是一种乐趣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734742684353380484#m</id>
            <title>作者声称在这个 21 分钟的新闻剪辑中，所有的主播乃至其中很多其他内容，都是由 AI 生成的！

AI 主播的报道，既丰富有料，又感人至深，还不至于太乏味，还不用担心负面新闻。

如果真的这么强，未来新闻主播职业会受到影响吗？</title>
            <link>https://nitter.cz/dotey/status/1734742684353380484#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734742684353380484#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:11:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者声称在这个 21 分钟的新闻剪辑中，所有的主播乃至其中很多其他内容，都是由 AI 生成的！<br />
<br />
AI 主播的报道，既丰富有料，又感人至深，还不至于太乏味，还不用担心负面新闻。<br />
<br />
如果真的这么强，未来新闻主播职业会受到影响吗？</p>
<p><a href="https://nitter.cz/channel1_ai/status/1734591810033373231#m">nitter.cz/channel1_ai/status/1734591810033373231#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734728249396085116#m</id>
            <title>R to @dotey: 配合 @realrenmin 老师这条一起看
https://x.com/realrenmin/status/1734721215283986628?s=20</title>
            <link>https://nitter.cz/dotey/status/1734728249396085116#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734728249396085116#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 00:13:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>配合 <a href="https://nitter.cz/realrenmin" title="Sverige_ Dong-seok🇸🇪">@realrenmin</a> 老师这条一起看<br />
<a href="https://x.com/realrenmin/status/1734721215283986628?s=20">x.com/realrenmin/status/1734…</a></p>
<p><a href="https://nitter.cz/realrenmin/status/1734721215283986628#m">nitter.cz/realrenmin/status/1734721215283986628#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/realrenmin/status/1734721215283986628#m</id>
            <title>RT by @dotey: Mistral AI放出Mixtral 8x7B, 基于Mixture of Experts (MoE)的开源模型，效果不错。

但在我看来，MoE是transformer时代LSTM-GRU，是NLP古早的范式，architecture engineering，非常old school。核心方法是加一些gate来加强Efficient Training at Scale，简言之目的是为了低成本训练，而不是为了塑造专家模型。

而Mixture of Experts的名字，太具迷惑性了，字面意思似乎是各种专家模型的组合起到1+1>2的效果。但实际看看Mixtral 8x7B，8个mistral 7b，b b不一样，但没有一个是专家模型，之所以叫做expert，居然是MoE中的FNN，我十分怀疑FNN能有什么专家能力。

它的benchmarking也理所当然的跟通用大模型GPT3.5/Llama 2相比，比较的是generic能力，并没有什么突出的专家能力。粗算了一下，8x7B float16, 至少需要100GB以上GPU显存，cost巨大。在这种情况下，oss的情怀，不足以说服我不用OpenAI的api。

如果我们停下来想想，什么是expert。
首先，expert能力一定不是通用大模型的generic的能力，而是独特的specialization的能力。例如会写code的GitHub copilot，或会generate思科路由器配置命令，甚至特别会planning，特别会算数都是专家能力都算。
简言之，expert能力是会产生特定领域特定输出的能力。所以，MoE是一个好名字，在这个时代，缺有些名不副实。

而做specialization模型的技术，依然在发展，并且依然是前沿，其实就是lora微调，例如Stanford's Alpaca models项目等等，核心思想就是在开源模型上加adapter，使之能够完成一个具体领域的专家工作，其实Mistral AI的开源7b模型估计也是这么做出来的。

未来，大语言模型作为agent的时代在实际中的应用，一定是llm在中间协调多种多样不同7b抽象出来的api，来完成新的human computer interaction。甚至在特定领域，这个协调工作也可以被planning expert的开源模型替代，而协调的过程，还是离不开 CoT，React，ReWoo或者其他的prompting方法。

CoT, ReAct在我的推中已经分享过好几次了，接下来找时间把ReWoo, 几个微调的介绍（跳票很久了）分享给大家。</title>
            <link>https://nitter.cz/realrenmin/status/1734721215283986628#m</link>
            <guid isPermaLink="false">https://nitter.cz/realrenmin/status/1734721215283986628#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 23:45:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral AI放出Mixtral 8x7B, 基于Mixture of Experts (MoE)的开源模型，效果不错。<br />
<br />
但在我看来，MoE是transformer时代LSTM-GRU，是NLP古早的范式，architecture engineering，非常old school。核心方法是加一些gate来加强Efficient Training at Scale，简言之目的是为了低成本训练，而不是为了塑造专家模型。<br />
<br />
而Mixture of Experts的名字，太具迷惑性了，字面意思似乎是各种专家模型的组合起到1+1>2的效果。但实际看看Mixtral 8x7B，8个mistral 7b，b b不一样，但没有一个是专家模型，之所以叫做expert，居然是MoE中的FNN，我十分怀疑FNN能有什么专家能力。<br />
<br />
它的benchmarking也理所当然的跟通用大模型GPT3.5/Llama 2相比，比较的是generic能力，并没有什么突出的专家能力。粗算了一下，8x7B float16, 至少需要100GB以上GPU显存，cost巨大。在这种情况下，oss的情怀，不足以说服我不用OpenAI的api。<br />
<br />
如果我们停下来想想，什么是expert。<br />
首先，expert能力一定不是通用大模型的generic的能力，而是独特的specialization的能力。例如会写code的GitHub copilot，或会generate思科路由器配置命令，甚至特别会planning，特别会算数都是专家能力都算。<br />
简言之，expert能力是会产生特定领域特定输出的能力。所以，MoE是一个好名字，在这个时代，缺有些名不副实。<br />
<br />
而做specialization模型的技术，依然在发展，并且依然是前沿，其实就是lora微调，例如Stanford's Alpaca models项目等等，核心思想就是在开源模型上加adapter，使之能够完成一个具体领域的专家工作，其实Mistral AI的开源7b模型估计也是这么做出来的。<br />
<br />
未来，大语言模型作为agent的时代在实际中的应用，一定是llm在中间协调多种多样不同7b抽象出来的api，来完成新的human computer interaction。甚至在特定领域，这个协调工作也可以被planning expert的开源模型替代，而协调的过程，还是离不开 CoT，React，ReWoo或者其他的prompting方法。<br />
<br />
CoT, ReAct在我的推中已经分享过好几次了，接下来找时间把ReWoo, 几个微调的介绍（跳票很久了）分享给大家。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>