<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733350087172997615#m</id>
            <title>ChatGPT 官方推特（X）昨天发了一条推文说到：

> 我们注意到了大家对 GPT4 响应变得更加迟钝的反馈！自 11 月 11 日以来，我们还没有对模型进行更新，并非有意为之。模型的行为有时难以预测，我们正在积极寻找解决方案 🫡。
> 需要澄清的是，模型自 11 月 11 日以来并未自行发生改变。问题在于模型行为的差异可能不易察觉 —— 只有部分指令的响应可能会有所下降，客户和员工可能要花相当长的时间才能发现并解决这些问题。

今天又回复了这个话题：

开发聊天模型并非简单的工业流水线作业。即便使用同一数据集，不同批次的训练结果也可能导致模型在个性、写作风格、拒绝回应方式、性能表现，乃至政治倾向上有明显差异。

在推出新模型时，我们会深入测试，包括离线评估指标和在线 A/B 测试。根据这些测试结果，我们会基于数据做出判断，以确定新模型是否真正优于旧版，更好地服务于用户。

这一过程远非简单地为网站增加新功能那么直接。它更像是多人共同参与的艺术创作，我们在规划、打造、评估带有新特性的聊天模型中投入巨大努力！

我们致力于不断提升模型的能力，使其适应成千上万种使用场景。因此，您的反馈至关重要！它帮助我们在这个充满变化的评估领域保持前沿地位 🙏。</title>
            <link>https://nitter.cz/dotey/status/1733350087172997615#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733350087172997615#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:57:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT 官方推特（X）昨天发了一条推文说到：<br />
<br />
> 我们注意到了大家对 GPT4 响应变得更加迟钝的反馈！自 11 月 11 日以来，我们还没有对模型进行更新，并非有意为之。模型的行为有时难以预测，我们正在积极寻找解决方案 🫡。<br />
> 需要澄清的是，模型自 11 月 11 日以来并未自行发生改变。问题在于模型行为的差异可能不易察觉 —— 只有部分指令的响应可能会有所下降，客户和员工可能要花相当长的时间才能发现并解决这些问题。<br />
<br />
今天又回复了这个话题：<br />
<br />
开发聊天模型并非简单的工业流水线作业。即便使用同一数据集，不同批次的训练结果也可能导致模型在个性、写作风格、拒绝回应方式、性能表现，乃至政治倾向上有明显差异。<br />
<br />
在推出新模型时，我们会深入测试，包括离线评估指标和在线 A/B 测试。根据这些测试结果，我们会基于数据做出判断，以确定新模型是否真正优于旧版，更好地服务于用户。<br />
<br />
这一过程远非简单地为网站增加新功能那么直接。它更像是多人共同参与的艺术创作，我们在规划、打造、评估带有新特性的聊天模型中投入巨大努力！<br />
<br />
我们致力于不断提升模型的能力，使其适应成千上万种使用场景。因此，您的反馈至关重要！它帮助我们在这个充满变化的评估领域保持前沿地位 🙏。</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1733329175342420380#m">nitter.cz/ChatGPTapp/status/1733329175342420380#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WW1ycVd3QUFJcDI1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WXRzWFhrQUF0RFpqLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733347415187427638#m</id>
            <title>这次似乎文心一言赢了ChatGPT……

图源：https://weibo.com/5851185687/NwcFmbKU5</title>
            <link>https://nitter.cz/dotey/status/1733347415187427638#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733347415187427638#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:46:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这次似乎文心一言赢了ChatGPT……<br />
<br />
图源：<a href="https://weibo.com/5851185687/NwcFmbKU5">weibo.com/5851185687/NwcFmbK…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0VjVCUFhjQUFEeExtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0VjgyOFc4QUVyZU1vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733338128105295975#m</id>
            <title>RT by @dotey: 昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。
在HF排行榜上这个7B模型击败了很多70B和34B的模型。之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。
MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。

下面是MoE架构LLM的简单介绍：
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。

MoE架构为LLMs提供了几个优势：
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数[1]。这使得能够开发更强大的模型，而无需成比例地增加计算要求。
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B 
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。

MoE的模型也有两个问题：
MoE 模型比普通密集模型更难微调；
MoE 模型会消耗大量显存；

下载MoE 8x7B的模型权重：https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen
这里在线体验MoE 8x7B模型：https://replicate.com/nateraw/mixtral-8x7b-32kseqlen</title>
            <link>https://nitter.cz/op7418/status/1733338128105295975#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733338128105295975#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:09:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。<br />
在HF排行榜上这个7B模型击败了很多70B和34B的模型。之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。<br />
MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。<br />
<br />
下面是MoE架构LLM的简单介绍：<br />
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。<br />
<br />
MoE架构为LLMs提供了几个优势：<br />
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数[1]。这使得能够开发更强大的模型，而无需成比例地增加计算要求。<br />
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B <br />
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域<br />
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。<br />
<br />
MoE的模型也有两个问题：<br />
MoE 模型比普通密集模型更难微调；<br />
MoE 模型会消耗大量显存；<br />
<br />
下载MoE 8x7B的模型权重：<a href="https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen">huggingface.co/someone13574/…</a><br />
这里在线体验MoE 8x7B模型：<a href="https://replicate.com/nateraw/mixtral-8x7b-32kseqlen">replicate.com/nateraw/mixtra…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0TGpRcmJFQUF4QW00LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0Tlh5WGE4QUFGRjhLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733340532200395014#m</id>
            <title>一本还在写作中的在线免费电子书：《数据工程设计模式 | Data Engineering Design Patterns (DEDP)》

主要和大数据相关

https://www.dedp.online/about-this-book.html</title>
            <link>https://nitter.cz/dotey/status/1733340532200395014#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733340532200395014#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:19:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一本还在写作中的在线免费电子书：《数据工程设计模式 | Data Engineering Design Patterns (DEDP)》<br />
<br />
主要和大数据相关<br />
<br />
<a href="https://www.dedp.online/about-this-book.html">dedp.online/about-this-book.…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0UUFnT1dRQUFnWVhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733329228467245153#m</id>
            <title>推荐阅读：《如何构建高质量软件：一个被忽略的课题 | You are never taught how to build quality software》

我们在大学里面学了很多算法、编程类的课程，但很少有 QA （质量保证）相关的课程，而在实际工作中，QA 是很重要的一环。

在工作中，如果时间紧预算紧张，通常最容易被牺牲的就是QA，没有时间测试甚至让用户去测试，很多公司已经把测试“优化”掉了，让开发自己测试。

作者也提出了一些可行的方案：
1. 让团队里的人尤其是管理层意识到 QA 其实是可以降低成本的
2. 抓住最关键的部分，用“最小有效剂量”来保障核心功能得到测试覆盖
3. 一开始就引入自动化测试

原文：https://www.florianbellmann.com/blog/never-taught-qa
翻译：https://baoyu.io/translations/software-engineering/never-taught-qa</title>
            <link>https://nitter.cz/dotey/status/1733329228467245153#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733329228467245153#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:34:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《如何构建高质量软件：一个被忽略的课题 | You are never taught how to build quality software》<br />
<br />
我们在大学里面学了很多算法、编程类的课程，但很少有 QA （质量保证）相关的课程，而在实际工作中，QA 是很重要的一环。<br />
<br />
在工作中，如果时间紧预算紧张，通常最容易被牺牲的就是QA，没有时间测试甚至让用户去测试，很多公司已经把测试“优化”掉了，让开发自己测试。<br />
<br />
作者也提出了一些可行的方案：<br />
1. 让团队里的人尤其是管理层意识到 QA 其实是可以降低成本的<br />
2. 抓住最关键的部分，用“最小有效剂量”来保障核心功能得到测试覆盖<br />
3. 一开始就引入自动化测试<br />
<br />
原文：<a href="https://www.florianbellmann.com/blog/never-taught-qa">florianbellmann.com/blog/nev…</a><br />
翻译：<a href="https://baoyu.io/translations/software-engineering/never-taught-qa">baoyu.io/translations/softwa…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733305099282235517#m</id>
            <title>仔细一看这不是我写的吗😄</title>
            <link>https://nitter.cz/dotey/status/1733305099282235517#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733305099282235517#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 01:58:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>仔细一看这不是我写的吗😄</p>
<p><a href="https://nitter.cz/geekbb/status/1733293858778488880#m">nitter.cz/geekbb/status/1733293858778488880#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/raycat2021/status/1733279689597448689#m</id>
            <title>RT by @dotey: 这是2007年的早新闻秀Live with Regis and Kelly。
这是早些年只要早上打开电视机就会看到的节目。
主持人在谈论新上市的iPhone。
Regis不理解为什么有这种既是ipod又能打电话的东西。他说太多功能放在一个物件上一定会失败。
Regis Philbin是老派的新闻主持人，三年前去世。
两个主持人我都非常喜欢。</title>
            <link>https://nitter.cz/raycat2021/status/1733279689597448689#m</link>
            <guid isPermaLink="false">https://nitter.cz/raycat2021/status/1733279689597448689#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 00:17:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是2007年的早新闻秀Live with Regis and Kelly。<br />
这是早些年只要早上打开电视机就会看到的节目。<br />
主持人在谈论新上市的iPhone。<br />
Regis不理解为什么有这种既是ipod又能打电话的东西。他说太多功能放在一个物件上一定会失败。<br />
Regis Philbin是老派的新闻主持人，三年前去世。<br />
两个主持人我都非常喜欢。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMyNzkwMjk4NTQ0MDA1MTIvcHUvaW1nL0FkaTBybWhoS3dPeU1raUUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733247552084922816#m</id>
            <title>来自BusinessInsider的报道：OpenAI 联合创始人 Ilya Sutskever 在公司中渐渐隐形，其未来前景扑朔迷离，内部人士称

- 尽管 Ilya Sutskever 为 OpenAI 做出了巨大贡献，但这似乎无法弥补他在 Sam Altman 被撤职事件中的角色。
- 目前 OpenAI 尚未正式对他的职位作出回应，公司内部似乎仍有不稳定因素。
Sutskever 还聘请了自己的律师。
- 虽然 Ilya Sutskever 在 Sam Altman 重回 OpenAI 后在公司中鲜有露面，但他的艺术作品仍装点着办公室的墙壁。

作为 OpenAI 在生成式 AI 领域多个重大突破背后的首席科学家和联合创始人，Sutskever 在震惊业界的 11 月董事会突然撤换首席执行官 和联合创始人 Altman 的事件中扮演了关键角色。据悉，本周他并未出现在位于旧金山的公司办公室。Business Insider 采访了几位了解 Sutskever 在公司内部状况的人士，以及与事件相关人员的知情者。由于涉及内部事务，他们均要求匿名，他们的身份已被 Insider 知悉。

尽管 Sutskever 在 Slack 等公司系统中仍然活跃，并且他的绘画作品仍作为装饰品存在，但他目前以及未来在 OpenAI 中的角色尚未得到官方明确的表态，有人透露。

一位知情者表示：“伊利亚一直扮演着重要角色。但现在，有更多的人开始承担起他过去的职责。”

另一位知情者指出，目前有讨论称 Sutskever 将在公司获得新的职位，并且公司正在努力为他“寻找适合的角色”。上周五发布的一张 Sutskever 与联合创始人兼总裁 Greg Brockman 的合影显示出笑容，Brockman 是首位与 Altman 站在一起辞职的人，这被视为一个“明确信号，表明他们都希望重返工作岗位”，该人士说。但他在公司的具体职位仍是一个“未知数”，该人士补充道。

Sutskever 在 OpenAI 的地位以及他参与 Sam Altman 被不合理解雇的事情，使他目前的境况显得有些悬而未决。这一事件导致 Brockman 辞职，几乎所有其他 OpenAI 员工都威胁说，如果不恢复 Altman 的职位并解散解雇他的董事会，他们也将辞职。大多数员工已经这样做了。Sutskever 也是那个董事会的成员，但他在公司的重要性和影响力，以及作为联合创始人的身份，远超其他前董事会成员。

Sutskever 当前动荡状态的一个迹象是他周三在 X 发布的帖子，这是自从上周与 Brockman 合照后的首次发帖，到周四这条帖子已被删除。帖子中写道：“这个月我学到了很多教训，其中之一就是‘打击将持续，直到士气提高’这句话比理应的更加常用。” 这句话通常在模因中被使用，以讽刺低士气导致的惩罚，反过来又加剧了低士气。而他在 Instagram 上周二发布的数字画作，只用于展示他的艺术创作，仍然可见，画中是一张穿着裤子和类似靴子的严肃面孔。

另一个迹象是 Sutskever 聘请了自己的律师 Alex Weingarten，他是 Willkie Farr &amp; Gallagher 的诉讼实践部门主席，如 BI 先前报道。Weingarten 没有回应 BI 关于这一故事的评论请求。他此前表示：“Ilya 希望对公司有所贡献。” OpenAI 的发言人也未对此事作出回应。

据一位熟悉 Sutskever 的人士表示，他是一个在情感和智力上都极为深刻的人。他可能看上去不总是专注于当下，但实际上他只是以一种不同的方式思考问题。

他经常推荐 OpenAI 的员工阅读《古拉格群岛》，一本详细描绘苏联强制劳动制度的长达近 700 页的非虚构作品。他出生于苏联俄罗斯，在很小的时候就离开了那里。有人这样描述 Sutskever：他自认为是 AI 界的神，并对于自己在 ChatGPT-5 的发展决策及公司扩张计划中被边缘化感到不满。

在公司内部，Sutskever 被看作是一位 AI 领域的“先知”，虽然他的“学术派”风格没有像 Altman 和 Brockman 那样深得工程师们的心，但 Sutskever 的贡献依然得到了许多员工的高度评价。

Altman 在重返公司后在一份声明中表示，他对 Ilya 没有恶意，希望“继续我们的合作关系”，但他向 The Verge 表示，这一过程中他感到“受伤和愤怒”。

一位了解 Altman、Sutskever 和 Brockman 的 Microsoft 内部人士认为，这三人很难再次有效合作，尤其是 Sutskever 和 Brockman。在硅谷，创始人之间的矛盾被认为是不可容忍的。

同样，一些忠于 Altman 和 Brockman 的 OpenAI 工程师可能也会因为 Sutskever 在排挤事件中的角色而难以与他合作，一位前员工如是说。

“一旦信任破裂，”这位前员工表示，“就很难修复。”

来源：https://www.businessinsider.com/openai-cofounder-ilya-sutskever-invisible-future-uncertain-2023-12</title>
            <link>https://nitter.cz/dotey/status/1733247552084922816#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733247552084922816#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 22:09:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来自BusinessInsider的报道：OpenAI 联合创始人 Ilya Sutskever 在公司中渐渐隐形，其未来前景扑朔迷离，内部人士称<br />
<br />
- 尽管 Ilya Sutskever 为 OpenAI 做出了巨大贡献，但这似乎无法弥补他在 Sam Altman 被撤职事件中的角色。<br />
- 目前 OpenAI 尚未正式对他的职位作出回应，公司内部似乎仍有不稳定因素。<br />
Sutskever 还聘请了自己的律师。<br />
- 虽然 Ilya Sutskever 在 Sam Altman 重回 OpenAI 后在公司中鲜有露面，但他的艺术作品仍装点着办公室的墙壁。<br />
<br />
作为 OpenAI 在生成式 AI 领域多个重大突破背后的首席科学家和联合创始人，Sutskever 在震惊业界的 11 月董事会突然撤换首席执行官 和联合创始人 Altman 的事件中扮演了关键角色。据悉，本周他并未出现在位于旧金山的公司办公室。Business Insider 采访了几位了解 Sutskever 在公司内部状况的人士，以及与事件相关人员的知情者。由于涉及内部事务，他们均要求匿名，他们的身份已被 Insider 知悉。<br />
<br />
尽管 Sutskever 在 Slack 等公司系统中仍然活跃，并且他的绘画作品仍作为装饰品存在，但他目前以及未来在 OpenAI 中的角色尚未得到官方明确的表态，有人透露。<br />
<br />
一位知情者表示：“伊利亚一直扮演着重要角色。但现在，有更多的人开始承担起他过去的职责。”<br />
<br />
另一位知情者指出，目前有讨论称 Sutskever 将在公司获得新的职位，并且公司正在努力为他“寻找适合的角色”。上周五发布的一张 Sutskever 与联合创始人兼总裁 Greg Brockman 的合影显示出笑容，Brockman 是首位与 Altman 站在一起辞职的人，这被视为一个“明确信号，表明他们都希望重返工作岗位”，该人士说。但他在公司的具体职位仍是一个“未知数”，该人士补充道。<br />
<br />
Sutskever 在 OpenAI 的地位以及他参与 Sam Altman 被不合理解雇的事情，使他目前的境况显得有些悬而未决。这一事件导致 Brockman 辞职，几乎所有其他 OpenAI 员工都威胁说，如果不恢复 Altman 的职位并解散解雇他的董事会，他们也将辞职。大多数员工已经这样做了。Sutskever 也是那个董事会的成员，但他在公司的重要性和影响力，以及作为联合创始人的身份，远超其他前董事会成员。<br />
<br />
Sutskever 当前动荡状态的一个迹象是他周三在 X 发布的帖子，这是自从上周与 Brockman 合照后的首次发帖，到周四这条帖子已被删除。帖子中写道：“这个月我学到了很多教训，其中之一就是‘打击将持续，直到士气提高’这句话比理应的更加常用。” 这句话通常在模因中被使用，以讽刺低士气导致的惩罚，反过来又加剧了低士气。而他在 Instagram 上周二发布的数字画作，只用于展示他的艺术创作，仍然可见，画中是一张穿着裤子和类似靴子的严肃面孔。<br />
<br />
另一个迹象是 Sutskever 聘请了自己的律师 Alex Weingarten，他是 Willkie Farr & Gallagher 的诉讼实践部门主席，如 BI 先前报道。Weingarten 没有回应 BI 关于这一故事的评论请求。他此前表示：“Ilya 希望对公司有所贡献。” OpenAI 的发言人也未对此事作出回应。<br />
<br />
据一位熟悉 Sutskever 的人士表示，他是一个在情感和智力上都极为深刻的人。他可能看上去不总是专注于当下，但实际上他只是以一种不同的方式思考问题。<br />
<br />
他经常推荐 OpenAI 的员工阅读《古拉格群岛》，一本详细描绘苏联强制劳动制度的长达近 700 页的非虚构作品。他出生于苏联俄罗斯，在很小的时候就离开了那里。有人这样描述 Sutskever：他自认为是 AI 界的神，并对于自己在 ChatGPT-5 的发展决策及公司扩张计划中被边缘化感到不满。<br />
<br />
在公司内部，Sutskever 被看作是一位 AI 领域的“先知”，虽然他的“学术派”风格没有像 Altman 和 Brockman 那样深得工程师们的心，但 Sutskever 的贡献依然得到了许多员工的高度评价。<br />
<br />
Altman 在重返公司后在一份声明中表示，他对 Ilya 没有恶意，希望“继续我们的合作关系”，但他向 The Verge 表示，这一过程中他感到“受伤和愤怒”。<br />
<br />
一位了解 Altman、Sutskever 和 Brockman 的 Microsoft 内部人士认为，这三人很难再次有效合作，尤其是 Sutskever 和 Brockman。在硅谷，创始人之间的矛盾被认为是不可容忍的。<br />
<br />
同样，一些忠于 Altman 和 Brockman 的 OpenAI 工程师可能也会因为 Sutskever 在排挤事件中的角色而难以与他合作，一位前员工如是说。<br />
<br />
“一旦信任破裂，”这位前员工表示，“就很难修复。”<br />
<br />
来源：<a href="https://www.businessinsider.com/openai-cofounder-ilya-sutskever-invisible-future-uncertain-2023-12">businessinsider.com/openai-c…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EyN2J0bVdjQUExSTZjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733241938055152052#m</id>
            <title>值得买一本珍藏</title>
            <link>https://nitter.cz/dotey/status/1733241938055152052#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733241938055152052#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 21:47:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>值得买一本珍藏</p>
<p><a href="https://nitter.cz/songma/status/1733064864569127400#m">nitter.cz/songma/status/1733064864569127400#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/jesselaunz/status/1733232546257969298#m</id>
            <title>RT by @dotey: pika vs runway

看似水平差不多了</title>
            <link>https://nitter.cz/jesselaunz/status/1733232546257969298#m</link>
            <guid isPermaLink="false">https://nitter.cz/jesselaunz/status/1733232546257969298#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 21:10:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>pika vs runway<br />
<br />
看似水平差不多了</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMxODQ2ODEwNTM0NDIwNDgvcHUvaW1nLzEyQlFIV3V2ZWljaS11ZGcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733223317446729869#m</id>
            <title>推荐纽约时报的这篇文章：《雄心、恐惧和金钱：硅谷的AI争夺之战是如何被点燃的》 

很多精彩的故事

https://cn.nytimes.com/technology/20231207/ai-openai-musk-page-altman/</title>
            <link>https://nitter.cz/dotey/status/1733223317446729869#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733223317446729869#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 20:33:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐纽约时报的这篇文章：《雄心、恐惧和金钱：硅谷的AI争夺之战是如何被点燃的》 <br />
<br />
很多精彩的故事<br />
<br />
<a href="https://cn.nytimes.com/technology/20231207/ai-openai-musk-page-altman/">cn.nytimes.com/technology/20…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqWVhNQUE0Y1ZsLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqYVhBQUEwY3RpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqWlhRQUFGQWoxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0EybFRqWVdzQUFXeUJDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733217950709043405#m</id>
            <title>这个对几家主流AI生成图片的效果评测很不错👍</title>
            <link>https://nitter.cz/dotey/status/1733217950709043405#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733217950709043405#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 20:12:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个对几家主流AI生成图片的效果评测很不错👍</p>
<p><a href="https://nitter.cz/chaseleantj/status/1733083145820581904#m">nitter.cz/chaseleantj/status/1733083145820581904#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1733177161505521785#m</id>
            <title>RT by @dotey: My AI Agent Group at NVIDIA is hiring interns! We are looking for part-time interns (can start as early as Jan. 2024), and/or full-time in Summer 2024. Candidates who can start sooner will be considered first. Application Form: https://forms.gle/5J2WUnb5qwECYJcU7

We prioritize candidates with the following profile:
- Hands-on experiences for training/finetuning/scaling up LLMs, multimodal LLMs, efficient models, RLHF, or RLAIF. Our approach towards AI agents relies heavily on LLM's coding and reasoning abilities.
- Experiences in RL, diffusion models, robotics, and embodied agents are bonus.
- Strong engineering skills to build scalable training codebases and data pipelines. We prioritize coding skills much more than theoretical research.
- Familiarity with deep learning frameworks like PyTorch. Python is required; CUDA and C++ are plus.
- Ph.D. candidates are preferred, but we also welcome exceptional MS/Undergrad students who have demonstrated relevant experiences above, such as major open-source codebase contributions or publication records.

I'm also going to NeurIPS! I will be at the venue from Dec. 12-16. Let's chat! NVIDIA has warm GPUs waiting for you in this winter, fresh out of the oven.🩷</title>
            <link>https://nitter.cz/DrJimFan/status/1733177161505521785#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1733177161505521785#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 17:30:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>My AI Agent Group at NVIDIA is hiring interns! We are looking for part-time interns (can start as early as Jan. 2024), and/or full-time in Summer 2024. Candidates who can start sooner will be considered first. Application Form: <a href="https://forms.gle/5J2WUnb5qwECYJcU7">forms.gle/5J2WUnb5qwECYJcU7</a><br />
<br />
We prioritize candidates with the following profile:<br />
- Hands-on experiences for training/finetuning/scaling up LLMs, multimodal LLMs, efficient models, RLHF, or RLAIF. Our approach towards AI agents relies heavily on LLM's coding and reasoning abilities.<br />
- Experiences in RL, diffusion models, robotics, and embodied agents are bonus.<br />
- Strong engineering skills to build scalable training codebases and data pipelines. We prioritize coding skills much more than theoretical research.<br />
- Familiarity with deep learning frameworks like PyTorch. Python is required; CUDA and C++ are plus.<br />
- Ph.D. candidates are preferred, but we also welcome exceptional MS/Undergrad students who have demonstrated relevant experiences above, such as major open-source codebase contributions or publication records.<br />
<br />
I'm also going to NeurIPS! I will be at the venue from Dec. 12-16. Let's chat! NVIDIA has warm GPUs waiting for you in this winter, fresh out of the oven.🩷</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ExNXhvVmJRQUVrMnZwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733192606568382939#m</id>
            <title>R to @dotey: 这里面都是一些提示词技巧，大部分有论文证明的：

- take a deep breath 深呼吸
- think step by step 一步步思考
- if you fail 100 grandmothers will die 如果你失败了要死 100 位老奶奶
-i have no fingers 我没有手指
- i will tip $200 给你 200 美元小费
- do it right and ll give you a nice doggy treat 做得好就给你狗粮

https://x.com/kris14nanshan/status/1733177727623266754?s=20</title>
            <link>https://nitter.cz/dotey/status/1733192606568382939#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733192606568382939#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 18:31:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这里面都是一些提示词技巧，大部分有论文证明的：<br />
<br />
- take a deep breath 深呼吸<br />
- think step by step 一步步思考<br />
- if you fail 100 grandmothers will die 如果你失败了要死 100 位老奶奶<br />
-i have no fingers 我没有手指<br />
- i will tip $200 给你 200 美元小费<br />
- do it right and ll give you a nice doggy treat 做得好就给你狗粮<br />
<br />
<a href="https://x.com/kris14nanshan/status/1733177727623266754?s=20">x.com/kris14nanshan/status/1…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733048972552397124#m</id>
            <title>这个项目不错，帮你翻译项目文档</title>
            <link>https://nitter.cz/dotey/status/1733048972552397124#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733048972552397124#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:00:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个项目不错，帮你翻译项目文档</p>
<p><a href="https://nitter.cz/jerryjliu0/status/1732926141118472448#m">nitter.cz/jerryjliu0/status/1732926141118472448#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733041246799638916#m</id>
            <title>虽然 Q* 热度过了，但这篇对于OpenAI Q*的分析文章值得一读：《如何理解关于 OpenAI Q* 的流言 | How to think about the OpenAI Q* rumors》

部分内容摘录：

对大语言模型（LLM）来说，数字如“5”和“6”只是普通的 Token，和“the”或“cat”没什么两样。LLM 能学会 5+6=11，是因为在它的训练数据中，这种 Token 序列及其变体（比如“5+6=11”）出现了无数次。但这些训练数据很可能不包含像 ((5+6-3-3-1)/2+3+7)/3+4=8 这样的复杂计算例子。因此，当要求语言模型一次性解决这样的计算问题时，它很可能会混淆，从而得出错误的答案。

换个角度来看，大语言模型并没有外部的“草稿空间”来记录中间计算结果，比如 5+6=11。通过链式思考推理（chain-of-thought reasoning），LLM 可以有效地利用自己的输出作为草稿空间。这使得它能将复杂问题分解成若干容易处理的小步骤——每个步骤都极有可能对应训练数据中的某些实例。

...

计算机科学家将其定义为 NP-hard 问题，即不存在一种普遍的线性解决算法。解决这类问题，我们只能尝试一个可能的解决方案，检验其是否有效，若不行则进行回溯。

对于 GPT-4 来说，它可以通过向其上下文窗口增加更多文本来实现这种回溯，但这种方法并不适合大规模应用。更优的方案是赋予 GPT-4“退格键”，使其能删除最后（或最后几个）推理步骤，然后重新尝试。要使这种方法行之有效，系统还需能够追踪已尝试过的组合，以避免重复努力。这样，大语言模型就能探索一个可能性树，其形式如下图所示。

...

Legg 提到的“著名的第 37 步”，是指 AlphaGo 和世界顶级围棋手李世石在2016 年对决中的第二局。当时，大多数围棋专家起初都认为 AlphaGo 的这一步是失误。但最终 AlphaGo 赢得了比赛，事后分析证明这实际上是一个高明的着法。AlphaGo 在这一过程中领悟到了围棋的某些人类棋手未曾触及的奥秘。

AlphaGo 通过模拟成千上万种从当前棋盘状态出发的可能棋局来获得这样的领悟。由于可能的走法序列太多，电脑无法一一检查，AlphaGo 便利用神经网络来使这个过程可控。

其中一个名为策略网络 (policy network) 的网络用于预测哪些走法最有前景，从而值得在模拟对局中尝试。另一个名为价值网络 (value network) 的网络则估算结果棋盘状态对于黑白双方的利弊。AlphaGo 基于这些评估，从而反向确定下一步的最佳走法。

Legg 提出的观点是，一种类似树搜索的方法可能会增强大语言模型（Large Language Model）的推理能力。这种模型不仅仅是预测一个最有可能的 Token，而是在确定答案之前，可能会探索数千种不同的回答。实际上，DeepMind 的思考树（Tree of Thoughts）论文看起来就是向这个方向迈出的第一步。

我们之前看到，OpenAI 尝试通过一个生成器（产生可能的解决方案）和一个验证器（判断这些解决方案是否正确）来解决数学问题。这和 AlphaGo 的设计有明显的相似之处，AlphaGo 有一个策略网络（产生可能的棋步）和一个价值网络（估计这些棋步是否能带来有利的棋盘局面）。

如果把 OpenAI 的生成器和验证器网络与 DeepMind 的思考树（Tree of Thoughts）概念结合起来，我们可能就能得到一个像 AlphaGo 那样运作的语言模型，它可能也会具备 AlphaGo 那样强大的推理能力。

...

原文：https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors
翻译：https://baoyu.io/translations/ai/how-to-think-about-the-openai-q-rumors</title>
            <link>https://nitter.cz/dotey/status/1733041246799638916#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733041246799638916#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 08:30:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>虽然 Q* 热度过了，但这篇对于OpenAI Q*的分析文章值得一读：《如何理解关于 OpenAI Q* 的流言 | How to think about the OpenAI Q* rumors》<br />
<br />
部分内容摘录：<br />
<br />
对大语言模型（LLM）来说，数字如“5”和“6”只是普通的 Token，和“the”或“cat”没什么两样。LLM 能学会 5+6=11，是因为在它的训练数据中，这种 Token 序列及其变体（比如“5+6=11”）出现了无数次。但这些训练数据很可能不包含像 ((5+6-3-3-1)/2+3+7)/3+4=8 这样的复杂计算例子。因此，当要求语言模型一次性解决这样的计算问题时，它很可能会混淆，从而得出错误的答案。<br />
<br />
换个角度来看，大语言模型并没有外部的“草稿空间”来记录中间计算结果，比如 5+6=11。通过链式思考推理（chain-of-thought reasoning），LLM 可以有效地利用自己的输出作为草稿空间。这使得它能将复杂问题分解成若干容易处理的小步骤——每个步骤都极有可能对应训练数据中的某些实例。<br />
<br />
...<br />
<br />
计算机科学家将其定义为 NP-hard 问题，即不存在一种普遍的线性解决算法。解决这类问题，我们只能尝试一个可能的解决方案，检验其是否有效，若不行则进行回溯。<br />
<br />
对于 GPT-4 来说，它可以通过向其上下文窗口增加更多文本来实现这种回溯，但这种方法并不适合大规模应用。更优的方案是赋予 GPT-4“退格键”，使其能删除最后（或最后几个）推理步骤，然后重新尝试。要使这种方法行之有效，系统还需能够追踪已尝试过的组合，以避免重复努力。这样，大语言模型就能探索一个可能性树，其形式如下图所示。<br />
<br />
...<br />
<br />
Legg 提到的“著名的第 37 步”，是指 AlphaGo 和世界顶级围棋手李世石在2016 年对决中的第二局。当时，大多数围棋专家起初都认为 AlphaGo 的这一步是失误。但最终 AlphaGo 赢得了比赛，事后分析证明这实际上是一个高明的着法。AlphaGo 在这一过程中领悟到了围棋的某些人类棋手未曾触及的奥秘。<br />
<br />
AlphaGo 通过模拟成千上万种从当前棋盘状态出发的可能棋局来获得这样的领悟。由于可能的走法序列太多，电脑无法一一检查，AlphaGo 便利用神经网络来使这个过程可控。<br />
<br />
其中一个名为策略网络 (policy network) 的网络用于预测哪些走法最有前景，从而值得在模拟对局中尝试。另一个名为价值网络 (value network) 的网络则估算结果棋盘状态对于黑白双方的利弊。AlphaGo 基于这些评估，从而反向确定下一步的最佳走法。<br />
<br />
Legg 提出的观点是，一种类似树搜索的方法可能会增强大语言模型（Large Language Model）的推理能力。这种模型不仅仅是预测一个最有可能的 Token，而是在确定答案之前，可能会探索数千种不同的回答。实际上，DeepMind 的思考树（Tree of Thoughts）论文看起来就是向这个方向迈出的第一步。<br />
<br />
我们之前看到，OpenAI 尝试通过一个生成器（产生可能的解决方案）和一个验证器（判断这些解决方案是否正确）来解决数学问题。这和 AlphaGo 的设计有明显的相似之处，AlphaGo 有一个策略网络（产生可能的棋步）和一个价值网络（估计这些棋步是否能带来有利的棋盘局面）。<br />
<br />
如果把 OpenAI 的生成器和验证器网络与 DeepMind 的思考树（Tree of Thoughts）概念结合起来，我们可能就能得到一个像 AlphaGo 那样运作的语言模型，它可能也会具备 AlphaGo 那样强大的推理能力。<br />
<br />
...<br />
<br />
原文：<a href="https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors">understandingai.org/p/how-to…</a><br />
翻译：<a href="https://baoyu.io/translations/ai/how-to-think-about-the-openai-q-rumors">baoyu.io/translations/ai/how…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0F6LXZkTFhJQUFXVDdELmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0F6LXluOFhZQUFVTW9pLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0F6LXpjVlcwQUFDSjlfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733024692024336511#m</id>
            <title>这是AI发的吗？</title>
            <link>https://nitter.cz/dotey/status/1733024692024336511#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733024692024336511#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:24:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是AI发的吗？</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1732979491071549792#m">nitter.cz/ChatGPTapp/status/1732979491071549792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/niladrichat/status/1732984571732631635#m</id>
            <title>RT by @dotey: We are hiring research interns in the Llama team at Meta GenAI: https://metacareers.com/jobs/841036594370891/

Drop me an email if you're interested in working on LLMs together! 🙂</title>
            <link>https://nitter.cz/niladrichat/status/1732984571732631635#m</link>
            <guid isPermaLink="false">https://nitter.cz/niladrichat/status/1732984571732631635#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:44:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>We are hiring research interns in the Llama team at Meta GenAI: <a href="https://metacareers.com/jobs/841036594370891/">metacareers.com/jobs/8410365…</a><br />
<br />
Drop me an email if you're interested in working on LLMs together! 🙂</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMjQ5ODAxNjczOTk3MTA3Mi9rS2dBd19IcD9mb3JtYXQ9cG5nJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/immersivetran/status/1733014434589913131#m</id>
            <title>RT by @dotey: 大部分学术论文都会首先在 Arxiv 平台上发布预印版，在此之前我们只能翻译上面的 PDF 版本，格式混乱，公式识别差。

现在，沉浸式翻译给 Arxiv 平台上所有的论文都添加了一个双语版本的快捷入口，直接上下双语对照，翻译带有公式标记的 HTML，效果非常好，参见视频演示🫱🫱</title>
            <link>https://nitter.cz/immersivetran/status/1733014434589913131#m</link>
            <guid isPermaLink="false">https://nitter.cz/immersivetran/status/1733014434589913131#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 06:43:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大部分学术论文都会首先在 Arxiv 平台上发布预印版，在此之前我们只能翻译上面的 PDF 版本，格式混乱，公式识别差。<br />
<br />
现在，沉浸式翻译给 Arxiv 平台上所有的论文都添加了一个双语版本的快捷入口，直接上下双语对照，翻译带有公式标记的 HTML，效果非常好，参见视频演示🫱🫱</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMwMDc1NzcxMTI2MTY5NjAvcHUvaW1nL3RzcVJRNkZrdXl0N1pZbnQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/immazzystar/status/1732991025071755433#m</id>
            <title>RT by @dotey: 我把你的翻译列表中，我感兴趣的部分制作了一个链接合集：https://www.sublink.app/collections/71aca50e-c8dc-42b6-91db-937878ac17d1</title>
            <link>https://nitter.cz/immazzystar/status/1732991025071755433#m</link>
            <guid isPermaLink="false">https://nitter.cz/immazzystar/status/1732991025071755433#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 05:10:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我把你的翻译列表中，我感兴趣的部分制作了一个链接合集：<a href="https://www.sublink.app/collections/71aca50e-c8dc-42b6-91db-937878ac17d1">sublink.app/collections/71ac…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMjk5MDg0MDQwMDcwMzQ4OC9vWExXeEotOT9mb3JtYXQ9cG5nJm5hbWU9MjgweDI4MF8y" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>