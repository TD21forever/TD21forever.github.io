<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734316369863393667#m</id>
            <title>RT by @dotey: Rob Lynch @RobLynch99 发布了一条关于 GPT-4-turbo 的有趣发现。

他指出，当通过 API 使用 GPT-4-turbo 时，如果系统提示中的日期是十二月而不是五月，模型完成任务（如代码生成）会更短，这一差异在统计上是显著的。

有网友调侃因为12月是假期，模型自己在给自己放假…🤓偷懒！

实验细节：

•Rob Lynch 使用相同的提示进行了 API 调用（一个要求实现机器学习任务但不使用库的代码完成任务）。

•他创建了两个系统提示，一个告诉 API 当前是五月，另一个是十二月，并比较了这两种情况下的结果分布。

•对于五月的系统提示，平均值为 4298；对于十二月的系统提示，平均值为 4086。

•在每个样本中进行了 477 次完成，t 测试的 p 值小于 2.28e-07。

总结：

Rob Lynch 在他的推文中报告的现象表明，GPT-4-turbo 模型在处理相同的任务时，会根据系统提示中的日期信息产生不同长度的输出。

具体来说，他发现当模型“认为”当前是十二月而不是五月时（通过系统提示中的日期决定），它生成的代码或文本完成任务会更短。

1.时间感知:
•这表明 GPT-4-turbo 可能对时间信息有一定的敏感性，即使这种信息是通过系统提示隐式给出的。

2.输出变化:
•模型的输出不仅取决于给定的任务或问题，还可能受到其他上下文信息（如日期）的影响。

3.行为模式:
•这种现象可能揭示了模型在处理信息时的某些内在行为模式或偏好。

猜测可能原因：

1.上下文敏感性:
•GPT-4-turbo 模型被设计为对上下文非常敏感。如果模型在处理请求时识别到特定的日期信息，它可能会根据这个信息调整其回答。这可能是因为模型在训练时接触到了与日期相关的不同类型的数据。

2.内部逻辑:
•模型可能内部包含某种逻辑，使得它在处理包含特定日期的提示时表现出不同的行为。这种逻辑可能是在模型训练过程中无意中形成的。

3.数据集偏差:
•如果模型训练使用的数据集在不同时间段有所不同，这可能导致模型对日期信息作出反应。例如，某些类型的内容可能更多地出现在特定时间段的数据中。

4.随机性或偶然性:
•这种现象也可能是随机性或特定实验设置的偶然结果，并不一定代表模型普遍行为。</title>
            <link>https://nitter.cz/xiaohuggg/status/1734316369863393667#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734316369863393667#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 20:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Rob Lynch <a href="https://nitter.cz/RobLynch99" title="Rob Lynch">@RobLynch99</a> 发布了一条关于 GPT-4-turbo 的有趣发现。<br />
<br />
他指出，当通过 API 使用 GPT-4-turbo 时，如果系统提示中的日期是十二月而不是五月，模型完成任务（如代码生成）会更短，这一差异在统计上是显著的。<br />
<br />
有网友调侃因为12月是假期，模型自己在给自己放假…🤓偷懒！<br />
<br />
实验细节：<br />
<br />
•Rob Lynch 使用相同的提示进行了 API 调用（一个要求实现机器学习任务但不使用库的代码完成任务）。<br />
<br />
•他创建了两个系统提示，一个告诉 API 当前是五月，另一个是十二月，并比较了这两种情况下的结果分布。<br />
<br />
•对于五月的系统提示，平均值为 4298；对于十二月的系统提示，平均值为 4086。<br />
<br />
•在每个样本中进行了 477 次完成，t 测试的 p 值小于 2.28e-07。<br />
<br />
总结：<br />
<br />
Rob Lynch 在他的推文中报告的现象表明，GPT-4-turbo 模型在处理相同的任务时，会根据系统提示中的日期信息产生不同长度的输出。<br />
<br />
具体来说，他发现当模型“认为”当前是十二月而不是五月时（通过系统提示中的日期决定），它生成的代码或文本完成任务会更短。<br />
<br />
1.时间感知:<br />
•这表明 GPT-4-turbo 可能对时间信息有一定的敏感性，即使这种信息是通过系统提示隐式给出的。<br />
<br />
2.输出变化:<br />
•模型的输出不仅取决于给定的任务或问题，还可能受到其他上下文信息（如日期）的影响。<br />
<br />
3.行为模式:<br />
•这种现象可能揭示了模型在处理信息时的某些内在行为模式或偏好。<br />
<br />
猜测可能原因：<br />
<br />
1.上下文敏感性:<br />
•GPT-4-turbo 模型被设计为对上下文非常敏感。如果模型在处理请求时识别到特定的日期信息，它可能会根据这个信息调整其回答。这可能是因为模型在训练时接触到了与日期相关的不同类型的数据。<br />
<br />
2.内部逻辑:<br />
•模型可能内部包含某种逻辑，使得它在处理包含特定日期的提示时表现出不同的行为。这种逻辑可能是在模型训练过程中无意中形成的。<br />
<br />
3.数据集偏差:<br />
•如果模型训练使用的数据集在不同时间段有所不同，这可能导致模型对日期信息作出反应。例如，某些类型的内容可能更多地出现在特定时间段的数据中。<br />
<br />
4.随机性或偶然性:<br />
•这种现象也可能是随机性或特定实验设置的偶然结果，并不一定代表模型普遍行为。</p>
<p><a href="https://nitter.cz/RobLynch99/status/1734278713762549970#m">nitter.cz/RobLynch99/status/1734278713762549970#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734366237629526108#m</id>
            <title>推荐HuggingFace的这篇《深入解析“混合专家模型（Mixtral of Experts）” | Mixture of Experts Explained》

完整的讲述了混合专家模型的各个方面。主要内容如下：
1. 相较于密集型模型，预训练速度更快
2. 拥有比同等参数的模型更快的推理速度
3. 对显存要求高，因为需要将所有专家模型都加载到内存中
4. 虽然在微调方面存在挑战，但近期关于 MoE 的指令调优研究显示出了光明前景

原文：https://huggingface.co/blog/moe#switch-transformers
翻译：https://baoyu.io/translations/llm/mixture-of-experts-explained</title>
            <link>https://nitter.cz/dotey/status/1734366237629526108#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734366237629526108#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 00:15:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐HuggingFace的这篇《深入解析“混合专家模型（Mixtral of Experts）” | Mixture of Experts Explained》<br />
<br />
完整的讲述了混合专家模型的各个方面。主要内容如下：<br />
1. 相较于密集型模型，预训练速度更快<br />
2. 拥有比同等参数的模型更快的推理速度<br />
3. 对显存要求高，因为需要将所有专家模型都加载到内存中<br />
4. 虽然在微调方面存在挑战，但近期关于 MoE 的指令调优研究显示出了光明前景<br />
<br />
原文：<a href="https://huggingface.co/blog/moe#switch-transformers">huggingface.co/blog/moe#swit…</a><br />
翻译：<a href="https://baoyu.io/translations/llm/mixture-of-experts-explained">baoyu.io/translations/llm/mi…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JHMHpTc1dFQUFWbHZjLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/fuxiangPro/status/1734080048523821188#m</id>
            <title>RT by @dotey: **为什么伟大不能被计划**的作者对“幻觉与创造力”有不一样的观点：
1）创造力的关键在于了解现有的东西，这是创新的基础。如果分不清现实与虚构，就难以推动真正的创新。
2）将LLM的“旧观念”误认为新创意，是对创造力的误解。
3）将幻觉认为是“创造性”带来的宝贵资产，是另一种误解。</title>
            <link>https://nitter.cz/fuxiangPro/status/1734080048523821188#m</link>
            <guid isPermaLink="false">https://nitter.cz/fuxiangPro/status/1734080048523821188#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 05:17:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>**为什么伟大不能被计划**的作者对“幻觉与创造力”有不一样的观点：<br />
1）创造力的关键在于了解现有的东西，这是创新的基础。如果分不清现实与虚构，就难以推动真正的创新。<br />
2）将LLM的“旧观念”误认为新创意，是对创造力的误解。<br />
3）将幻觉认为是“创造性”带来的宝贵资产，是另一种误解。</p>
<p><a href="https://nitter.cz/kenneth0stanley/status/1733571230803058920#m">nitter.cz/kenneth0stanley/status/1733571230803058920#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734294573034119277#m</id>
            <title>如果说 Mistral Medium Model 能生成黄色小说、能文爱，那不就有人付费的么！</title>
            <link>https://nitter.cz/dotey/status/1734294573034119277#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734294573034119277#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 19:30:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果说 Mistral Medium Model 能生成黄色小说、能文爱，那不就有人付费的么！</p>
<p><a href="https://nitter.cz/bindureddy/status/1734242395871719689#m">nitter.cz/bindureddy/status/1734242395871719689#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1734265419408544047#m</id>
            <title>RT by @dotey: 2022: Year of Images
2023: Year of Sound Waves
2024: Year of Videos
Congrats to my Stanford labmate Agrim for his new generative video work!</title>
            <link>https://nitter.cz/DrJimFan/status/1734265419408544047#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1734265419408544047#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 17:34:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2022: Year of Images<br />
2023: Year of Sound Waves<br />
2024: Year of Videos<br />
Congrats to my Stanford labmate Agrim for his new generative video work!</p>
<p><a href="https://nitter.cz/agrimgupta92/status/1734253883076063426#m">nitter.cz/agrimgupta92/status/1734253883076063426#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734275247778431370#m</id>
            <title>转译一下 Jim Fan 对 Mistral 的点评：

—

谈谈我对 Mistral 迅速崛起的看法：

- 成功的开局：在开源与封闭式 AI 的大讨论中，Mistral 选择了一个非常好的成立时机。他们在 20 亿美元的估值下完成了 4 亿美元的 A 轮融资，并且背后是一支高效精简的团队。

- 现在每个月都有许多模型被推出，但真正能持久并且能引起公众关注的模型寥寥无几。LlaMA 和 Vicuna 就是这方面的典型例子。

- 我认为 Mistral 做对了一件事，那就是极力优化 7B级别的模型，而不是追求更大的模型容量。7B及其混合专家模型（7B-MoE，相当于 12B的密集型模型）对于基层的 AI 工程师来说，更容易进行开发和构建。

- 混合专家模型（MoE）无疑是 AI 发展的正确方向。它在小型模型的知识记忆与效率之间找到了一个灵活的平衡点。OpenAI 自从训练 GPT-4 以来已经在这条路上走了一年多了。我对 AI 社区没有把更多的注意力放在 MoE 上感到意外。

- 大语言模型（LLM）就像是对一个文明的快照。未来会出现更多代表不同文化、政治观点、宗教信仰和特定地区规定的本地化大语言模型。Mistral 把多语言支持放在了重要位置。考虑到它是一家法国初创公司，这一点也不奇怪。

- Mistral 的发布方式颇具特色。这个过程实际上是颠覆了大家的预期：
（1）首先发布一个没有任何解释的磁力链接。磁力链接已成为新型的吸引眼球的手段！
（2）然后向开源的 vLLM 项目提交一个PR，帮助社区集成 Megablocks CUDA 内核，这一举措相当大胆！
（3）最后，才发布博客文章。

- 推出托管 API 端点是快速收集客户反馈、针对实际应用场景进行迭代，并最重要的是，实现开源模型的商业化的最好方式。Mistral 立刻采取了这一策略。

- “Mixtral”这个名字真是巧妙极了👏

博客: https://mistral.ai/news/mixtral-of-experts/
API 平台: https://mistral.ai/news/la-plateforme/</title>
            <link>https://nitter.cz/dotey/status/1734275247778431370#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734275247778431370#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 18:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译一下 Jim Fan 对 Mistral 的点评：<br />
<br />
—<br />
<br />
谈谈我对 Mistral 迅速崛起的看法：<br />
<br />
- 成功的开局：在开源与封闭式 AI 的大讨论中，Mistral 选择了一个非常好的成立时机。他们在 20 亿美元的估值下完成了 4 亿美元的 A 轮融资，并且背后是一支高效精简的团队。<br />
<br />
- 现在每个月都有许多模型被推出，但真正能持久并且能引起公众关注的模型寥寥无几。LlaMA 和 Vicuna 就是这方面的典型例子。<br />
<br />
- 我认为 Mistral 做对了一件事，那就是极力优化 7B级别的模型，而不是追求更大的模型容量。7B及其混合专家模型（7B-MoE，相当于 12B的密集型模型）对于基层的 AI 工程师来说，更容易进行开发和构建。<br />
<br />
- 混合专家模型（MoE）无疑是 AI 发展的正确方向。它在小型模型的知识记忆与效率之间找到了一个灵活的平衡点。OpenAI 自从训练 GPT-4 以来已经在这条路上走了一年多了。我对 AI 社区没有把更多的注意力放在 MoE 上感到意外。<br />
<br />
- 大语言模型（LLM）就像是对一个文明的快照。未来会出现更多代表不同文化、政治观点、宗教信仰和特定地区规定的本地化大语言模型。Mistral 把多语言支持放在了重要位置。考虑到它是一家法国初创公司，这一点也不奇怪。<br />
<br />
- Mistral 的发布方式颇具特色。这个过程实际上是颠覆了大家的预期：<br />
（1）首先发布一个没有任何解释的磁力链接。磁力链接已成为新型的吸引眼球的手段！<br />
（2）然后向开源的 vLLM 项目提交一个PR，帮助社区集成 Megablocks CUDA 内核，这一举措相当大胆！<br />
（3）最后，才发布博客文章。<br />
<br />
- 推出托管 API 端点是快速收集客户反馈、针对实际应用场景进行迭代，并最重要的是，实现开源模型的商业化的最好方式。Mistral 立刻采取了这一策略。<br />
<br />
- “Mixtral”这个名字真是巧妙极了👏<br />
<br />
博客: <a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a><br />
API 平台: <a href="https://mistral.ai/news/la-plateforme/">mistral.ai/news/la-plateform…</a></p>
<p><a href="https://nitter.cz/DrJimFan/status/1734269362100437315#m">nitter.cz/DrJimFan/status/1734269362100437315#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/_akhaliq/status/1734266117516845119#m</id>
            <title>RT by @dotey: Photorealistic Video Generation with Diffusion Models

paper: https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf

present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve stateof-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of 512 × 896 resolution at 8 frames per second.</title>
            <link>https://nitter.cz/_akhaliq/status/1734266117516845119#m</link>
            <guid isPermaLink="false">https://nitter.cz/_akhaliq/status/1734266117516845119#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 17:37:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Photorealistic Video Generation with Diffusion Models<br />
<br />
paper: <a href="https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf">walt-video-diffusion.github.…</a><br />
<br />
present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve stateof-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of 512 × 896 resolution at 8 frames per second.</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyNjYwNjY1MTgzMTUwMDgvcHUvaW1nL2ljSWF1RUtTWVVweXNBVlEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1734251620098289889#m</id>
            <title>RT by @dotey: Meta AI开源了AVID，通过修复扩展了 AnimateDiff 等 T2V 模型的能力，同时还支持通过文本编辑视频以及生成无限时长的视频。
文本编辑视频的能力包括修复视频的裁切、对视频对象进行更改、改变视频内对象的纹理和颜色、删除视频对应内容、更换视频所处的环境。

简介：
文本引导的视频修复面临三个主要挑战：（i）编辑视频的时间一致性，（ii）在不同的结构保真度级别支持不同的修复类型，以及（iii）处理可变的视频长度。为了解决这些挑战，我们引入了带有扩散模型的任意长度视频修复，称为 AVID。
我们的模型的核心配备了有效的运动模块和可调节的结构引导，用于固定长度的视频修复。在此基础上，我们提出了一种新颖的时间多重扩散采样管道，具有中帧注意力引导机制，有助于生成任何所需持续时间的视频。
我们的综合实验表明，我们的模型可以在不同的视频持续时间范围内稳健地处理各种修复类型，并且质量很高。

方法：
培训阶段，我们采用两步方法。
 (a) 运动模块集成在主要文本到图像 (T2I) 修复模型的每一层之后，通过应用于视频数据的合成掩模针对视频修复任务进行优化。 
(b) 在第二个训练步骤中，我们保留 UNet $\epsilon_\theta$ 中的参数，并利用 UNet 编码器的参数副本专门训练结构指导模块 $\mathbf{s}_\theta$ 。在推理过程中，
(c)，对于长度为 $N^\prime$ 的视频，我们构建一系列片段，每个片段包含 $N$ 个连续帧。在每个去噪步骤中，都会计算并汇总每个分段的结果。

论文地址：https://zhang-zx.github.io/AVID/</title>
            <link>https://nitter.cz/op7418/status/1734251620098289889#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1734251620098289889#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 16:39:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta AI开源了AVID，通过修复扩展了 AnimateDiff 等 T2V 模型的能力，同时还支持通过文本编辑视频以及生成无限时长的视频。<br />
文本编辑视频的能力包括修复视频的裁切、对视频对象进行更改、改变视频内对象的纹理和颜色、删除视频对应内容、更换视频所处的环境。<br />
<br />
简介：<br />
文本引导的视频修复面临三个主要挑战：（i）编辑视频的时间一致性，（ii）在不同的结构保真度级别支持不同的修复类型，以及（iii）处理可变的视频长度。为了解决这些挑战，我们引入了带有扩散模型的任意长度视频修复，称为 AVID。<br />
我们的模型的核心配备了有效的运动模块和可调节的结构引导，用于固定长度的视频修复。在此基础上，我们提出了一种新颖的时间多重扩散采样管道，具有中帧注意力引导机制，有助于生成任何所需持续时间的视频。<br />
我们的综合实验表明，我们的模型可以在不同的视频持续时间范围内稳健地处理各种修复类型，并且质量很高。<br />
<br />
方法：<br />
培训阶段，我们采用两步方法。<br />
 (a) 运动模块集成在主要文本到图像 (T2I) 修复模型的每一层之后，通过应用于视频数据的合成掩模针对视频修复任务进行优化。 <br />
(b) 在第二个训练步骤中，我们保留 UNet $\epsilon_\theta$ 中的参数，并利用 UNet 编码器的参数副本专门训练结构指导模块 $\mathbf{s}_\theta$ 。在推理过程中，<br />
(c)，对于长度为 <a href="https://nitter.cz/search?q=%23N">$N</a>^\prime$ 的视频，我们构建一系列片段，每个片段包含 <a href="https://nitter.cz/search?q=%23N">$N</a>$ 个连续帧。在每个去噪步骤中，都会计算并汇总每个分段的结果。<br />
<br />
论文地址：<a href="https://zhang-zx.github.io/AVID/">zhang-zx.github.io/AVID/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyNTEyODU1ODU3NTIwNjQvcHUvaW1nL1VhOENCcmpTSUVFcGtDdVQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734272751647551780#m</id>
            <title>#开源项目推荐#：Pitivi

Pitivi 是一款免费开源的视频编辑器，界面也挺美观。主要是 Python 写的。但只支持Linux操作系统。

项目地址：https://gitlab.gnome.org/GNOME/pitivi</title>
            <link>https://nitter.cz/dotey/status/1734272751647551780#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734272751647551780#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 18:03:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>#开源项目推荐#：Pitivi<br />
<br />
Pitivi 是一款免费开源的视频编辑器，界面也挺美观。主要是 Python 写的。但只支持Linux操作系统。<br />
<br />
项目地址：<a href="https://gitlab.gnome.org/GNOME/pitivi">gitlab.gnome.org/GNOME/pitiv…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGZjMyclhZQUFaVWluLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734266915168587956#m</id>
            <title>Mistral AI 团队正式发布了一篇关于Mixtral 8x7B -- 混合专家模型（Mixtral of Experts）的博客，提供了很多测试数据，最让人印象深刻的是，8个7B的混合专家模型，在大部分的性能测试中，Mixtral 不仅达到了 Llama 2 70B 的水平，甚至在很多方面超越了 GPT-3.5。

之前 Mixtral 开源的 7B 模型就备受好评，而这次的MixtralMixtral 8x7B 是一个采用稀疏专家混合网络的模型，它是一个仅包含解码器的模型。在这个模型中，前馈块从 8 组不同的参数组中进行选择。对于每一层的每个 Token，一个路由网络会挑选两组“专家”处理 Token，并将它们的输出结果进行加法组合。

这种技术让模型在增加参数数量的同时，有效控制了成本和延迟，因为模型每处理一个 Token 只会使用部分参数。具体来说，Mixtral 总共有 46.7B参数，但每个 Token 只用到了其中的 12.9B。因此，它在处理输入和生成输出时，无论是速度还是成本，都相当于一个 12.9B 参数的模型。

Mixtral 的主要能力包括：

- 能够流畅处理 32k 个 Token 的上下文。
- 支持多种语言，包括英语、法语、意大利语、德语和西班牙语。
- 在代码生成领域表现出色。
- 可以调整为遵循指令的模型，在 MT-Bench 上获得了 8.3 分的高分。

另外它没有安全护栏，也就是不像 Llama 一样你让它做点什么就说做不了，而是可以做很多好的事情或者不好的事情……

下载 Mixtral-8x7B-v0.1 Base model: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1

下载 Mixtral-8x7B-v0.1 Instruct model: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

官方博客：https://mistral.ai/news/mixtral-of-experts/
中文翻译：https://baoyu.io/translations/llm/mistral-ai-mixtral-of-experts</title>
            <link>https://nitter.cz/dotey/status/1734266915168587956#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734266915168587956#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 17:40:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral AI 团队正式发布了一篇关于Mixtral 8x7B -- 混合专家模型（Mixtral of Experts）的博客，提供了很多测试数据，最让人印象深刻的是，8个7B的混合专家模型，在大部分的性能测试中，Mixtral 不仅达到了 Llama 2 70B 的水平，甚至在很多方面超越了 GPT-3.5。<br />
<br />
之前 Mixtral 开源的 7B 模型就备受好评，而这次的MixtralMixtral 8x7B 是一个采用稀疏专家混合网络的模型，它是一个仅包含解码器的模型。在这个模型中，前馈块从 8 组不同的参数组中进行选择。对于每一层的每个 Token，一个路由网络会挑选两组“专家”处理 Token，并将它们的输出结果进行加法组合。<br />
<br />
这种技术让模型在增加参数数量的同时，有效控制了成本和延迟，因为模型每处理一个 Token 只会使用部分参数。具体来说，Mixtral 总共有 46.7B参数，但每个 Token 只用到了其中的 12.9B。因此，它在处理输入和生成输出时，无论是速度还是成本，都相当于一个 12.9B 参数的模型。<br />
<br />
Mixtral 的主要能力包括：<br />
<br />
- 能够流畅处理 32k 个 Token 的上下文。<br />
- 支持多种语言，包括英语、法语、意大利语、德语和西班牙语。<br />
- 在代码生成领域表现出色。<br />
- 可以调整为遵循指令的模型，在 MT-Bench 上获得了 8.3 分的高分。<br />
<br />
另外它没有安全护栏，也就是不像 Llama 一样你让它做点什么就说做不了，而是可以做很多好的事情或者不好的事情……<br />
<br />
下载 Mixtral-8x7B-v0.1 Base model: <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">huggingface.co/mistralai/Mix…</a><br />
<br />
下载 Mixtral-8x7B-v0.1 Instruct model: <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">huggingface.co/mistralai/Mix…</a><br />
<br />
官方博客：<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a><br />
中文翻译：<a href="https://baoyu.io/translations/llm/mistral-ai-mixtral-of-experts">baoyu.io/translations/llm/mi…</a></p>
<p><a href="https://nitter.cz/dchaplot/status/1734190262983798898#m">nitter.cz/dchaplot/status/1734190262983798898#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734233575044632724#m</id>
            <title>Google 发布了庆祝 Google Search 25 周年视频：Google — 25 Years in Search: The Most Searched，包含 25 年来被搜索次数最多的人物和时刻。从世界上最具代表性的表演，到历史性的突破，看看那些改变世界、激励后人的时刻。

更多详情参考 https://yearinsearch.google/trends</title>
            <link>https://nitter.cz/dotey/status/1734233575044632724#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734233575044632724#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 15:27:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 发布了庆祝 Google Search 25 周年视频：Google — 25 Years in Search: The Most Searched，包含 25 年来被搜索次数最多的人物和时刻。从世界上最具代表性的表演，到历史性的突破，看看那些改变世界、激励后人的时刻。<br />
<br />
更多详情参考 <a href="https://yearinsearch.google/trends">yearinsearch.google/trends</a></p>
<p><a href="https://nitter.cz/Google/status/1734218344402743791#m">nitter.cz/Google/status/1734218344402743791#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyMzMyNTE4MDIyMTAzMDQvcHUvaW1nL3pvU1pzaE1mUW9fME01cFkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/arthurmensch/status/1734123845739548859#m</id>
            <title>RT by @dotey: Announcing Mixtral 8x7B https://mistral.ai/news/mixtral-of-experts/ and our early developer platform https://mistral.ai/news/la-plateforme/. Very proud of the team!</title>
            <link>https://nitter.cz/arthurmensch/status/1734123845739548859#m</link>
            <guid isPermaLink="false">https://nitter.cz/arthurmensch/status/1734123845739548859#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 08:11:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Announcing Mixtral 8x7B <a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a> and our early developer platform <a href="https://mistral.ai/news/la-plateforme/">mistral.ai/news/la-plateform…</a>. Very proud of the team!</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</id>
            <title>RT by @dotey: Mixtral AI公布MoE 8x7B详细细节：

• 32k上下文。
• 支持英语、法语、意大利语、德语和西班牙语。
• 性能超过Llama 2系列和GPT3.5
• 在代码生成方面表现强劲。
• 在MT-Bench上达到8.3的分数。

技术细节：

•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。

•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。

详细内容：https://mistral.ai/news/mixtral-of-experts/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734139143259861185#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 09:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral AI公布MoE 8x7B详细细节：<br />
<br />
• 32k上下文。<br />
• 支持英语、法语、意大利语、德语和西班牙语。<br />
• 性能超过Llama 2系列和GPT3.5<br />
• 在代码生成方面表现强劲。<br />
• 在MT-Bench上达到8.3的分数。<br />
<br />
技术细节：<br />
<br />
•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。<br />
<br />
•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。<br />
<br />
详细内容：<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1733694954260901907#m">nitter.cz/xiaohuggg/status/1733694954260901907#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbU9ZcmE0QUFyS0V2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/geekbb/status/1734139012439552021#m</id>
            <title>RT by @dotey: 体验一番，使用默认的 Mistral 7.3B 参数模型，号称优于 Llama 2 13B，将以前问过 ChatGPT3.5 的问题抛给它，回答居然差不多，确实能打，只是我 i5-9600K 的风扇感觉要起飞了。https://github.com/SecureAI-Tools/SecureAI-Tools</title>
            <link>https://nitter.cz/geekbb/status/1734139012439552021#m</link>
            <guid isPermaLink="false">https://nitter.cz/geekbb/status/1734139012439552021#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 09:12:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>体验一番，使用默认的 Mistral 7.3B 参数模型，号称优于 Llama 2 13B，将以前问过 ChatGPT3.5 的问题抛给它，回答居然差不多，确实能打，只是我 i5-9600K 的风扇感觉要起飞了。<a href="https://github.com/SecureAI-Tools/SecureAI-Tools">github.com/SecureAI-Tools/Se…</a></p>
<p><a href="https://nitter.cz/dotey/status/1733739312372420807#m">nitter.cz/dotey/status/1733739312372420807#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbUxEUGE0QUFVdUJNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbUxsZmFBQUF5UzdWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1734128575861944527#m</id>
            <title>RT by @dotey: MistralAI发布了关于混合专家模型Mixtral 8x7B的介绍。
核心信息：
· 32K上下文
· 支持英语、法语、意大利语、德语、西班牙语
· 整体能力超过LLaMa 2，且推理速度快6倍
· 能力与ChatGPT 3.5接近
· 有45B参数，但是每个token只使用12B的参数，所以推理速度与12B模型相同
https://mistral.ai/news/mixtral-of-experts/</title>
            <link>https://nitter.cz/Gorden_Sun/status/1734128575861944527#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1734128575861944527#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 08:30:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MistralAI发布了关于混合专家模型Mixtral 8x7B的介绍。<br />
核心信息：<br />
· 32K上下文<br />
· 支持英语、法语、意大利语、德语、西班牙语<br />
· 整体能力超过LLaMa 2，且推理速度快6倍<br />
· 能力与ChatGPT 3.5接近<br />
· 有45B参数，但是每个token只使用12B的参数，所以推理速度与12B模型相同<br />
<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEY3BFVGJNQUFPcVU4LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734129116167729596#m</id>
            <title>强烈推荐这篇文章：《深入探索：AI 驱动的 PDF 布局检测引擎源代码解析 [译]》

系统的分析了最近很火的 PDF 转 Markdown 开源程序 Marker https://github.com/VikParuchuri/marker 的工作原理，比我想象的要复杂不少，用了好几个开源库。Marker 主要通过以下六个阶段来工作：

1. 准备阶段： 利用 PyMuPDF 工具，可以把各种格式的文档转换成 PDF 文件。

2. 文本识别（OCR）： 使用 Tesseract 或 OCRMyPDF 进行文字识别；也可以选择用 PyMuPDF 进行基本的文字抓取。

3. 布局识别： 运用专门定制的 LayoutLMv3 模型 来识别文档中的表格、图表、标题、图说、页眉和页脚。

4. 列的检测和排序： 再用一个定制的 LayoutLMv3 模型来识别文档中的列，并按照正确的顺序（上到下，左到右）进行排列。

5. 公式/代码处理： 通过 Nougat 工具，把公式图片转换成对应的 latex 代码，并利用启发式方法准确识别和调整代码及表格内容。

6. 文本清理与优化： 使用定制的 T5ForTextClassification 模型进行文本清理，比如去掉不必要的空格和奇怪的字符，确保以一种保守且保留原意的方式进行优化。

借助这六个阶段，Marker 能够把任何文档转化为格式整洁的 Markdown 文件。

原文：Inside Marker: A Guided Source Code Tour for an AI-powered PDF Layout Detection Engine https://journal.hexmos.com/marker-pdf-document-ai/
翻译：https://baoyu.io/translations/ai/marker-pdf-document-ai</title>
            <link>https://nitter.cz/dotey/status/1734129116167729596#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734129116167729596#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 08:32:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>强烈推荐这篇文章：《深入探索：AI 驱动的 PDF 布局检测引擎源代码解析 [译]》<br />
<br />
系统的分析了最近很火的 PDF 转 Markdown 开源程序 Marker <a href="https://github.com/VikParuchuri/marker">github.com/VikParuchuri/mark…</a> 的工作原理，比我想象的要复杂不少，用了好几个开源库。Marker 主要通过以下六个阶段来工作：<br />
<br />
1. 准备阶段： 利用 PyMuPDF 工具，可以把各种格式的文档转换成 PDF 文件。<br />
<br />
2. 文本识别（OCR）： 使用 Tesseract 或 OCRMyPDF 进行文字识别；也可以选择用 PyMuPDF 进行基本的文字抓取。<br />
<br />
3. 布局识别： 运用专门定制的 LayoutLMv3 模型 来识别文档中的表格、图表、标题、图说、页眉和页脚。<br />
<br />
4. 列的检测和排序： 再用一个定制的 LayoutLMv3 模型来识别文档中的列，并按照正确的顺序（上到下，左到右）进行排列。<br />
<br />
5. 公式/代码处理： 通过 Nougat 工具，把公式图片转换成对应的 latex 代码，并利用启发式方法准确识别和调整代码及表格内容。<br />
<br />
6. 文本清理与优化： 使用定制的 T5ForTextClassification 模型进行文本清理，比如去掉不必要的空格和奇怪的字符，确保以一种保守且保留原意的方式进行优化。<br />
<br />
借助这六个阶段，Marker 能够把任何文档转化为格式整洁的 Markdown 文件。<br />
<br />
原文：Inside Marker: A Guided Source Code Tour for an AI-powered PDF Layout Detection Engine <a href="https://journal.hexmos.com/marker-pdf-document-ai/">journal.hexmos.com/marker-pd…</a><br />
翻译：<a href="https://baoyu.io/translations/ai/marker-pdf-document-ai">baoyu.io/translations/ai/mar…</a></p>
<p><a href="https://nitter.cz/dotey/status/1730418206991409407#m">nitter.cz/dotey/status/1730418206991409407#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEY285LVhrQUFGVG9wLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEYzE5ZFhnQUFtSkpBLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEYzNwTVhBQUFkcFc4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734105617982456270#m</id>
            <title>RT by @dotey: 阿里又整活了

DreaMoving：能够生成目标身份在任何地方跳舞的视频。

你可以指定一个特定的人物（如名人、朋友或任何特定形象），让他正在任意背景或场景（可以是真实的地点如海滩、城市街道或任何虚构的场景）下跳舞。

更牛的是仅靠脸部照片或文字提示也能生成跳舞视频...

还可指定人物动作和姿势

DreaMoving是一个基于扩散模型的人类舞蹈视频生成框架。能够根据指导序列和简单的内容描述（仅文本提示、仅图像提示或文本和图像提示）生成高质量、高保真度的视频。

用户可以通过下几种输入内容来生成和控制视频：

1、文本提示：对视频内容的简单描述，比如场景设置、人物动作或任何特定的主题。例如，“一个女孩在海滩上跳舞”。

文本提示也可以用于描述视频中的背景环境。

2、参考图像：这些图像用于指定视频中人物的外观。例如，用户可以上传一个人脸图像来确保视频中的人物具有相同或类似的面部特征。

如果需要，还可以包括衣着或身体特征的图像。

3、姿势或深度序列：这是定义视频中人物动作的关键输入。姿势序列指定了人物在视频中的具体动作和姿态。
用户可以通过提供特定的舞蹈动作序列来控制视频中的舞蹈风格和动作。

4、可选的衣物图像：如果用户想要在视频中指定人物的服装样式，可以上传相关的衣物图像。

技术细节：

架构: DreaMoving 基于 Stable-Diffusion 模型构建，包括去噪 U-Net、视频控制网（Video ControlNet）和内容引导器（Content Guider）。

数据收集与预处理: 收集了约1000个高质量的人类舞蹈视频，经过剪辑和处理，得到约6000个短视频片段。

运动块: 为了提高时间一致性和运动保真度，将运动块集成到去噪 U-Net 和控制网中。

内容引导器: 设计用于控制生成视频的内容，包括人物外观和背景。使用图像提示精确引导人物外观，文本提示生成背景。

模型训练: 包括内容引导器训练、长帧预训练、视频控制网训练和表情微调。

模型推理: 输入包括文本提示、参考图像和姿势或深度序列。通过调整控制网和内容引导器中的参数来控制视频内容。

项目地址：https://dreamoving.github.io/dreamoving/

论文：https://arxiv.org/abs/2312.05107

代码还没有，是个很基础的演示，估计是为了抢流量...🤣</title>
            <link>https://nitter.cz/xiaohuggg/status/1734105617982456270#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734105617982456270#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:59:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里又整活了<br />
<br />
DreaMoving：能够生成目标身份在任何地方跳舞的视频。<br />
<br />
你可以指定一个特定的人物（如名人、朋友或任何特定形象），让他正在任意背景或场景（可以是真实的地点如海滩、城市街道或任何虚构的场景）下跳舞。<br />
<br />
更牛的是仅靠脸部照片或文字提示也能生成跳舞视频...<br />
<br />
还可指定人物动作和姿势<br />
<br />
DreaMoving是一个基于扩散模型的人类舞蹈视频生成框架。能够根据指导序列和简单的内容描述（仅文本提示、仅图像提示或文本和图像提示）生成高质量、高保真度的视频。<br />
<br />
用户可以通过下几种输入内容来生成和控制视频：<br />
<br />
1、文本提示：对视频内容的简单描述，比如场景设置、人物动作或任何特定的主题。例如，“一个女孩在海滩上跳舞”。<br />
<br />
文本提示也可以用于描述视频中的背景环境。<br />
<br />
2、参考图像：这些图像用于指定视频中人物的外观。例如，用户可以上传一个人脸图像来确保视频中的人物具有相同或类似的面部特征。<br />
<br />
如果需要，还可以包括衣着或身体特征的图像。<br />
<br />
3、姿势或深度序列：这是定义视频中人物动作的关键输入。姿势序列指定了人物在视频中的具体动作和姿态。<br />
用户可以通过提供特定的舞蹈动作序列来控制视频中的舞蹈风格和动作。<br />
<br />
4、可选的衣物图像：如果用户想要在视频中指定人物的服装样式，可以上传相关的衣物图像。<br />
<br />
技术细节：<br />
<br />
架构: DreaMoving 基于 Stable-Diffusion 模型构建，包括去噪 U-Net、视频控制网（Video ControlNet）和内容引导器（Content Guider）。<br />
<br />
数据收集与预处理: 收集了约1000个高质量的人类舞蹈视频，经过剪辑和处理，得到约6000个短视频片段。<br />
<br />
运动块: 为了提高时间一致性和运动保真度，将运动块集成到去噪 U-Net 和控制网中。<br />
<br />
内容引导器: 设计用于控制生成视频的内容，包括人物外观和背景。使用图像提示精确引导人物外观，文本提示生成背景。<br />
<br />
模型训练: 包括内容引导器训练、长帧预训练、视频控制网训练和表情微调。<br />
<br />
模型推理: 输入包括文本提示、参考图像和姿势或深度序列。通过调整控制网和内容引导器中的参数来控制视频内容。<br />
<br />
项目地址：<a href="https://dreamoving.github.io/dreamoving/">dreamoving.github.io/dreamov…</a><br />
<br />
论文：<a href="https://arxiv.org/abs/2312.05107">arxiv.org/abs/2312.05107</a><br />
<br />
代码还没有，是个很基础的演示，估计是为了抢流量...🤣</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwOTk1ODQyNjY0MDc5MzYvcHUvaW1nL2RyejRqMWpJQ3hLT1VQWkUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734104397003518410#m</id>
            <title>给臧师傅点赞👍🏻</title>
            <link>https://nitter.cz/dotey/status/1734104397003518410#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734104397003518410#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:54:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>给臧师傅点赞👍🏻</p>
<p><a href="https://nitter.cz/op7418/status/1734093582913708368#m">nitter.cz/op7418/status/1734093582913708368#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734104301155262545#m</id>
            <title>上次Google的Gemini演示视频是后期剪辑后结果，于是有网友拿GPT-4V试验了部分场景，发现基本上能还原Gemini的演示效果，并且他把代码开源了：https://github.com/gregsadetsky/sagittarius

原始视频：http://www.youtube.com/watch?v=__nL7Vc0OCg</title>
            <link>https://nitter.cz/dotey/status/1734104301155262545#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734104301155262545#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:54:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>上次Google的Gemini演示视频是后期剪辑后结果，于是有网友拿GPT-4V试验了部分场景，发现基本上能还原Gemini的演示效果，并且他把代码开源了：<a href="https://github.com/gregsadetsky/sagittarius">github.com/gregsadetsky/sagi…</a><br />
<br />
原始视频：<a href="http://www.youtube.com/watch?v=__nL7Vc0OCg">youtube.com/watch?v=__nL7Vc0…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQxMDM5ODgyMjI1MDkwNTYvcHUvaW1nL2o5OUNGWmxoaU15Qng1bE0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734081217069211836#m</id>
            <title>推荐阅读：把大语言模型封装成桌面应用

作者使用Electron+Transformer.js，把大语言模型打包成桌面应用，这样就可以本地运行大语言模型。

另外作者开发了一个框架叫ollama（https://github.com/jmorganca/ollama），可以支持多种开源模型，例如Llama 2、Mistral等，但目前只支持Mac和Linux，还不支持Windows。

文章链接：https://www.bmacd.xyz/packaging-language-models-into-desktop-apps/
中文翻译：https://baoyu.io/translations/llm/packaging-language-models-into-desktop-apps</title>
            <link>https://nitter.cz/dotey/status/1734081217069211836#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734081217069211836#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 05:22:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：把大语言模型封装成桌面应用<br />
<br />
作者使用Electron+Transformer.js，把大语言模型打包成桌面应用，这样就可以本地运行大语言模型。<br />
<br />
另外作者开发了一个框架叫ollama（<a href="https://github.com/jmorganca/ollama">github.com/jmorganca/ollama</a>），可以支持多种开源模型，例如Llama 2、Mistral等，但目前只支持Mac和Linux，还不支持Windows。<br />
<br />
文章链接：<a href="https://www.bmacd.xyz/packaging-language-models-into-desktop-apps/">bmacd.xyz/packaging-language…</a><br />
中文翻译：<a href="https://baoyu.io/translations/llm/packaging-language-models-into-desktop-apps">baoyu.io/translations/llm/pa…</a></p>
<p><a href="https://nitter.cz/_bmacd/status/1733891071384625438#m">nitter.cz/_bmacd/status/1733891071384625438#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JDd3E0alhZQUFZT0o4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JDd3U4WVhvQUFxMWNHLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>