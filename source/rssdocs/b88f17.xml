<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743094484357837136#m</id>
            <title>Alter3：一个能够执行 ChatGPT 指令的仿人机器人

东京大学的研究人员成功将仿人机器人 Alter3 与 GPT-4 连接。他们利用指令让这个机器人完成了一系列的人类行为，例如弹吉他、自拍、扮演鬼魂角色，甚至在电影院偷吃别人的爆米花。

这一过程可以看作是一场现代化的“哑剧游戏”：大语言模型 (Large Language Model) 将书面指令转换为可执行的代码，从而让机器人能够模仿出多种人类的动作。

项目地址：https://tnoinkwms.github.io/ALTER-LLM/</title>
            <link>https://nitter.cz/dotey/status/1743094484357837136#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743094484357837136#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 02:18:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Alter3：一个能够执行 ChatGPT 指令的仿人机器人<br />
<br />
东京大学的研究人员成功将仿人机器人 Alter3 与 GPT-4 连接。他们利用指令让这个机器人完成了一系列的人类行为，例如弹吉他、自拍、扮演鬼魂角色，甚至在电影院偷吃别人的爆米花。<br />
<br />
这一过程可以看作是一场现代化的“哑剧游戏”：大语言模型 (Large Language Model) 将书面指令转换为可执行的代码，从而让机器人能够模仿出多种人类的动作。<br />
<br />
项目地址：<a href="https://tnoinkwms.github.io/ALTER-LLM/">tnoinkwms.github.io/ALTER-LL…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDMwOTQzNTQxNTgyMjMzNjAvcHUvaW1nL2EyY3RKd3FGTV9hQ2tKcGcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743041592712167540#m</id>
            <title>这个绿幕技巧不错</title>
            <link>https://nitter.cz/dotey/status/1743041592712167540#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743041592712167540#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 22:47:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个绿幕技巧不错</p>
<p><a href="https://nitter.cz/hugovntr/status/1742968850444894217#m">nitter.cz/hugovntr/status/1742968850444894217#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1743011913980850633#m</id>
            <title>RT by @dotey: 嗯。研究了一下tiktok shop。在这过去的一年内，真的改进非常非常非常大，从原来业务流程都磕磕碰碰，到现在已经开始成型的生态，今年必须要玩玩tiktok了。

另外分享一个码农们挣外快的机会。https://partner.tiktokshop.com/docv2
建议研读tiktok shop的api，做一些me too就能挣钱。
我自己暂时可能不会去做这块的应用所以我愿意分享出来。至于哪些可以做的，就看每个人自己的理解了。</title>
            <link>https://nitter.cz/mtrainier2020/status/1743011913980850633#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1743011913980850633#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 20:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>嗯。研究了一下tiktok shop。在这过去的一年内，真的改进非常非常非常大，从原来业务流程都磕磕碰碰，到现在已经开始成型的生态，今年必须要玩玩tiktok了。<br />
<br />
另外分享一个码农们挣外快的机会。<a href="https://partner.tiktokshop.com/docv2">partner.tiktokshop.com/docv2</a><br />
建议研读tiktok shop的api，做一些me too就能挣钱。<br />
我自己暂时可能不会去做这块的应用所以我愿意分享出来。至于哪些可以做的，就看每个人自己的理解了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1742979646864646550#m</id>
            <title>RT by @dotey: 其实这种方法说起来也很简单，我经常拿来去查一些公司的真实情况。比如有的公司曾经吹的非常厉害。我只要查数据，我就知道他厂家在哪里，到底销量怎么样，到底盈利状况怎么样，到底掺了多少水。很多数据你是很难隐藏的。
当然聪明的公司，会做好隐藏。他会这么做，本土供应商—本土proxy -境外proxy -境外销售商。你看到的只是proxy对proxy，这就很难溯源对方的供应链，但是能估算其规模。 但是你很难保证他只有一个proxy。等等。这也是一个攻防。</title>
            <link>https://nitter.cz/mtrainier2020/status/1742979646864646550#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1742979646864646550#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:41:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>其实这种方法说起来也很简单，我经常拿来去查一些公司的真实情况。比如有的公司曾经吹的非常厉害。我只要查数据，我就知道他厂家在哪里，到底销量怎么样，到底盈利状况怎么样，到底掺了多少水。很多数据你是很难隐藏的。<br />
当然聪明的公司，会做好隐藏。他会这么做，本土供应商—本土proxy -境外proxy -境外销售商。你看到的只是proxy对proxy，这就很难溯源对方的供应链，但是能估算其规模。 但是你很难保证他只有一个proxy。等等。这也是一个攻防。</p>
<p><a href="https://nitter.cz/mtrainier2020/status/1742975445736513636#m">nitter.cz/mtrainier2020/status/1742975445736513636#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742988095631597917#m</id>
            <title>新的Mobile ALOHA演示视频，它可以做到：

洗衣服👔👖
自己充电⚡️
用吸尘器洗地
给植物浇水🌳
将碗放到洗碗机或者拿出来
用咖啡机冲咖啡☕️
从冰箱里取出饮料，开瓶啤酒🍺
开门🚪
陪宠物玩🐱
扔垃圾
开关灯💡</title>
            <link>https://nitter.cz/dotey/status/1742988095631597917#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742988095631597917#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 19:15:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>新的Mobile ALOHA演示视频，它可以做到：<br />
<br />
洗衣服👔👖<br />
自己充电⚡️<br />
用吸尘器洗地<br />
给植物浇水🌳<br />
将碗放到洗碗机或者拿出来<br />
用咖啡机冲咖啡☕️<br />
从冰箱里取出饮料，开瓶啤酒🍺<br />
开门🚪<br />
陪宠物玩🐱<br />
扔垃圾<br />
开关灯💡</p>
<p><a href="https://nitter.cz/zipengfu/status/1742973258528612724#m">nitter.cz/zipengfu/status/1742973258528612724#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Rey100001/status/1698208045460001054#m</id>
            <title>RT by @dotey: 我们这一代人的机会是什么？

1998年前后，搜狐新浪网易、BAT相继成立，中国互联网时代开启，伴随而来的还有风险投资；

1998年事业单位停止福利分房，开启房地产市场化时代；

1998年亚洲金融危机，中国增发特别国债加强基建，开始大规模基础设施建设时代，拉动经济增长的第一辆马车开启；

2001年中国加入世贸组织，开启大出口时代，拉动经济的第二辆马车开启；

这四匹马奔跑了二十年，造就了3亿中产阶级，上千万的富豪，无数的创业机会，梦想是这个时代的主旋律，2010~2020这十年更是人类历史上最繁荣的十年，没有非典与新冠，也没有伊拉克战争与俄乌战争，全球化越来越紧密，移动互联网吞噬一切，中国的房地产以每年20%的速度在增值。

但是今天，互联网不增长了、伴随的VC没地方投钱了、甚至由于中美贸易战，美国VC的钱都撤走了，房子卖不动了、该建设的基建都建设完了、出口也被G7制裁了，这四匹马都瘸了。至于消费这匹马，没有前面这四匹马赚钱，口袋空瘪瘪的，未来预期也不可控，谁还敢消费呢？

这就是我们这一代人的时代背景，我们不禁要问，我们这一代人的机会在哪里？特别是年轻人的机会在哪里？因为年轻人还不能躺平，也没有资本躺平呀。我分析了一些机会点：

1、短视频依然有红利，但那不属于产品人

2020年的短视频会像2010年的移动互联网一样吞噬一切，只要一个领域还没有知名的IP，那个领域就还有机会，说明机会窗口还没有关闭，比如我要在海外办公司，其实我并不知道找谁，因为我不知道谁靠谱，这存在着很大的信息差，我需要一个可信任的人操办这些事情，如果你的短视频能做到让我认识你，我的订单就是你的；再比如我要买保险，我也不知道应该信任谁。比起一个机构，我们更愿意相信一个活生生的人，这是IP的独特价值，如果你有做IP的能力，短视频是一个好的选择。

不过，短视频全是流量运营的事情，里面没有做产品、做技术的机会，2021年我们在抖音做MCN的时候就发现了这一点。现在抖音上唯一可以产品化的就是数据参谋类产品，比如蝉妈妈、考古加。

2、生物科技是硬核科学，也不存在产品定义的机会

都说21世纪是生物科技的时代，我认为是对的，其一是刚需，健康与排除孤独，是人类一直追求，但一直都没有办法解决的问题；其二是AI、大数据与生物科技的结合，提供了一条新的解题路径，很多生物问题可以“计算”出来。

但是，生物科技是硬核科学，他需要的是技术，不是产品定义的能力，产品经理的能力模型跟苹果公司类似，从来不发明任何新技术，只根据用户需求整合需要的技术。

当一个领域在比拼技术的时候，更需要的还是数学家、科学家、工程师，而不是产品经理，我们能把产品做得更高效、更简单、价格更低、体验更好，但我们没有办法将语音识别率从97%提升到99%，将OCR的能力从99.0%提升到99.7%，生物科技同理。

3、如果你不做AI，可能就真的没有机会了

提交YC的项目，70%已经与AI相关，这反映了全球创业的趋势。这还是要回归到用户需求，1994年是世界互联网的元年，2011年是移动互联网的元年，这么多年过去了，该被满足的需求都被满足得差不多了。在未来的日子里，非AI的领域，可能只有社交与游戏还能产生千万DAU、百万DAU的可能性，其他领域都不太可能了。

AI对我们来说，只是一种工具，区块链也是工具，技术其实都是工具，我们武器库里的工具越多，我们解决某个问题就越彻底，可能很多以前没有办法解决的问题，通过AI就能解决得很好，比如妙鸭相机，他是通过AI解决海马体拍证件照很麻烦、价格很贵的问题，如果没有AI，这个问题是解决不了的。

AI的技术已经可用了，目前需要的是洞察，看一些以前没有AI的时候，没法解决的问题，现在能不能通过AI解决，能不能解决得更好。

4、做一个5人小团队，永远都有机会

做小而美的事情，永远都是有机会的，放弃过去的宏大叙事，回归个体户的思维，找到一个极度细分的领域做好、做精、能守得住，当营收多一些，能Cover多一个人工资的时候，那就多增加一个人，不行的时候，就裁掉一个人，永远保持精干的小团队。

优势永远都是一点一滴积累的，打磨一个小而美的产品永远不是一件简单的事情，这样的事情是不会有VC支持的，这就意味着要靠自己，自负盈亏，这就需要你极度有耐心，要做好3年、5年没有正向收入的可能性，最好还是先业余做，等看到希望以后再辞职全力去做，这样的路径是最顺畅的。

5、出海，到竞争还不充分的地方去

“未来已来，只是空间分布不均匀”，美国、中国、其他地方，这是不同的世界，地区与地区之间存在着时间差，移动互联网在中美竞争很充分，但不意味着世界其他地方竞争也很充分，这里就是机会。

我的选择是什么，5＞3＞4。</title>
            <link>https://nitter.cz/Rey100001/status/1698208045460001054#m</link>
            <guid isPermaLink="false">https://nitter.cz/Rey100001/status/1698208045460001054#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 03 Sep 2023 05:35:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们这一代人的机会是什么？<br />
<br />
1998年前后，搜狐新浪网易、BAT相继成立，中国互联网时代开启，伴随而来的还有风险投资；<br />
<br />
1998年事业单位停止福利分房，开启房地产市场化时代；<br />
<br />
1998年亚洲金融危机，中国增发特别国债加强基建，开始大规模基础设施建设时代，拉动经济增长的第一辆马车开启；<br />
<br />
2001年中国加入世贸组织，开启大出口时代，拉动经济的第二辆马车开启；<br />
<br />
这四匹马奔跑了二十年，造就了3亿中产阶级，上千万的富豪，无数的创业机会，梦想是这个时代的主旋律，2010~2020这十年更是人类历史上最繁荣的十年，没有非典与新冠，也没有伊拉克战争与俄乌战争，全球化越来越紧密，移动互联网吞噬一切，中国的房地产以每年20%的速度在增值。<br />
<br />
但是今天，互联网不增长了、伴随的VC没地方投钱了、甚至由于中美贸易战，美国VC的钱都撤走了，房子卖不动了、该建设的基建都建设完了、出口也被G7制裁了，这四匹马都瘸了。至于消费这匹马，没有前面这四匹马赚钱，口袋空瘪瘪的，未来预期也不可控，谁还敢消费呢？<br />
<br />
这就是我们这一代人的时代背景，我们不禁要问，我们这一代人的机会在哪里？特别是年轻人的机会在哪里？因为年轻人还不能躺平，也没有资本躺平呀。我分析了一些机会点：<br />
<br />
1、短视频依然有红利，但那不属于产品人<br />
<br />
2020年的短视频会像2010年的移动互联网一样吞噬一切，只要一个领域还没有知名的IP，那个领域就还有机会，说明机会窗口还没有关闭，比如我要在海外办公司，其实我并不知道找谁，因为我不知道谁靠谱，这存在着很大的信息差，我需要一个可信任的人操办这些事情，如果你的短视频能做到让我认识你，我的订单就是你的；再比如我要买保险，我也不知道应该信任谁。比起一个机构，我们更愿意相信一个活生生的人，这是IP的独特价值，如果你有做IP的能力，短视频是一个好的选择。<br />
<br />
不过，短视频全是流量运营的事情，里面没有做产品、做技术的机会，2021年我们在抖音做MCN的时候就发现了这一点。现在抖音上唯一可以产品化的就是数据参谋类产品，比如蝉妈妈、考古加。<br />
<br />
2、生物科技是硬核科学，也不存在产品定义的机会<br />
<br />
都说21世纪是生物科技的时代，我认为是对的，其一是刚需，健康与排除孤独，是人类一直追求，但一直都没有办法解决的问题；其二是AI、大数据与生物科技的结合，提供了一条新的解题路径，很多生物问题可以“计算”出来。<br />
<br />
但是，生物科技是硬核科学，他需要的是技术，不是产品定义的能力，产品经理的能力模型跟苹果公司类似，从来不发明任何新技术，只根据用户需求整合需要的技术。<br />
<br />
当一个领域在比拼技术的时候，更需要的还是数学家、科学家、工程师，而不是产品经理，我们能把产品做得更高效、更简单、价格更低、体验更好，但我们没有办法将语音识别率从97%提升到99%，将OCR的能力从99.0%提升到99.7%，生物科技同理。<br />
<br />
3、如果你不做AI，可能就真的没有机会了<br />
<br />
提交YC的项目，70%已经与AI相关，这反映了全球创业的趋势。这还是要回归到用户需求，1994年是世界互联网的元年，2011年是移动互联网的元年，这么多年过去了，该被满足的需求都被满足得差不多了。在未来的日子里，非AI的领域，可能只有社交与游戏还能产生千万DAU、百万DAU的可能性，其他领域都不太可能了。<br />
<br />
AI对我们来说，只是一种工具，区块链也是工具，技术其实都是工具，我们武器库里的工具越多，我们解决某个问题就越彻底，可能很多以前没有办法解决的问题，通过AI就能解决得很好，比如妙鸭相机，他是通过AI解决海马体拍证件照很麻烦、价格很贵的问题，如果没有AI，这个问题是解决不了的。<br />
<br />
AI的技术已经可用了，目前需要的是洞察，看一些以前没有AI的时候，没法解决的问题，现在能不能通过AI解决，能不能解决得更好。<br />
<br />
4、做一个5人小团队，永远都有机会<br />
<br />
做小而美的事情，永远都是有机会的，放弃过去的宏大叙事，回归个体户的思维，找到一个极度细分的领域做好、做精、能守得住，当营收多一些，能Cover多一个人工资的时候，那就多增加一个人，不行的时候，就裁掉一个人，永远保持精干的小团队。<br />
<br />
优势永远都是一点一滴积累的，打磨一个小而美的产品永远不是一件简单的事情，这样的事情是不会有VC支持的，这就意味着要靠自己，自负盈亏，这就需要你极度有耐心，要做好3年、5年没有正向收入的可能性，最好还是先业余做，等看到希望以后再辞职全力去做，这样的路径是最顺畅的。<br />
<br />
5、出海，到竞争还不充分的地方去<br />
<br />
“未来已来，只是空间分布不均匀”，美国、中国、其他地方，这是不同的世界，地区与地区之间存在着时间差，移动互联网在中美竞争很充分，但不意味着世界其他地方竞争也很充分，这里就是机会。<br />
<br />
我的选择是什么，5＞3＞4。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRjVFX05YZmJJQUFvVi05LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742979986041282593#m</id>
            <title>R to @dotey: 视频来源：https://www.youtube.com/watch?v=ltrPvggcx4c</title>
            <link>https://nitter.cz/dotey/status/1742979986041282593#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742979986041282593#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:43:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频来源：<a href="https://www.youtube.com/watch?v=ltrPvggcx4c">youtube.com/watch?v=ltrPvggc…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0Mjk3OTk4MTI1NTI4MjY4OC81SDhIRjM1Tz9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742979829467627937#m</id>
            <title>[深度访谈] Ddog： 世界上首款脑控四足机器人

Ddog项目的特点集合了Boston Dynamics的Spot机器人以及由AttentivU提供的脑电脑接口（BCI）系统。这个系统是一副能测量人的脑电（EEG - 脑活动）和眼电（EOG - 眼部活动）信号的无线眼镜。Ddog项目是Brain Switch应用的升级版，这是一个实时的闭环BCI系统，允许用户以非语言的方式实时传达简单需求。Brain Switch的目标是帮助那些有身体挑战（如ALS，CP，SCI）的人满足基本的交流需求。Ddog项目是在Brain Switch的技术架构和基础设施基础上构建的。

Ddog的最大特点是其行动能力：这是第一个完全自主，由大脑驱动，无线的系统，包含了Spot机器人，在两部iPhone上运行，而且不需要黏贴的电极和计算用的背包装置。
Ddog的设计初衷是帮助进行物体操作，例如Spot的手臂被用于: 送货上门，移动椅子，携带书籍或拿玩具等。

这个Ddog项目负责人Nataliya Kosmyna博士的访谈涵盖了以下话题：为什么要创建Ddog项目？为什么选择使用Spot而不是其他机器人？Ddog背后的技术架构是怎样的？为什么认为Ddog项目需要无线且可携带的大脑感应解决方案是重要的？以及一些疯狂的应用案例，对于Ddog项目的未来展望，STEM和Ddog的联系等等！

项目网页：
https://media.mit.edu/projects/ddog
https://braini.io/ddog

以下为采访内容

采访者：请介绍一下自己。

Dr. Kosmyna：大家好，我是 Nataliya Kosmyna，麻省理工学院媒体实验室的研究科学家。在过去的 15 年里，我主要研究脑机接口技术。

采访者：今天我们的讨论主题是什么？

Dr. Kosmyna：今天我们将讨论我们最新的应用案例，名为 Ddog。Ddog 结合了波士顿动力公司的 Spot 机器人和我们自主研发的脑机接口。这款脑机接口采用了眼镜的形态设计，我们称之为 AttentivU。

采访者：可以进行演示了吗？

Dr. Kosmyna：是的，我们有新的项目要展示，这个项目名为 Ddog。它包括了波士顿动力公司的 Spot 机器人和 AttentivU 眼镜。我们的这款产品是可穿戴的、无线的，能够捕捉使用者的大脑活动和眼动。我们内部将其称为“想即得”。如果你没见过 Spot，现在可以在我的背后看到，它是一种类似狗的机器人。你可以通过它在空间内移动，执行各种任务，比如拿椅子、购物或打开门等。

采访者：能否介绍一下不同的大脑感应技术？

Dr. Kosmyna：当然。从屏幕上可以看到，我们的技术是如何从最初的阶段发展过来的。最初的阶段看上去像章鱼一样。不过，脑机接口技术（BCI）已经取得了巨大的进步。我们现在有更为简约和微型化的设备。但说实话，即使是头带式的，也不适合日常使用。在运动场或办公室里或许还行，但在日常生活中人们不太可能随身携带。大脑感应技术面临的挑战之一是它需要收集大量的数据，而这些数据的获取成本很高，且不容易获得。这和计算机视觉不同，计算机视觉所需的数据随处可得，易于获取。如果我们想要将脑机接口技术从理论走向实际应用，关键在于数据的收集。它不仅仅是关于一些杀手级应用或者你可能听说过的 XR 和其他头戴设备。目前而言，数据是成功的关键。因此，尽管实验室里的“章鱼式”应用非常适合研究，但现实世界中，我们更加看重的是可穿戴和便携式的设备。这里我特别强调那些可以每天多小时佩戴的设备，它们可以在驾车、办公或者录制视频等各种场合中使用，让用户几乎感觉不到它们的存在。

采访者：关于大脑感应眼镜的想法。

Dr. Kosmyna：我们谈论的就是眼镜形式的大脑感应设备。我们每天都在佩戴各种头戴物品，比如耳机、眼镜、太阳镜、口罩和各种帽子等。我们已经习惯了在头部携带这些物品，所以只需要在这些已有的形式中加入数据收集功能。

采访者：对 Ddog 项目感到兴奋的原因是什么？

Dr. Kosmyna：Ddog 是一个概念证明项目，显然非常令人兴奋，因为它能够实现利用大脑活动与大型工业机器人进行互动。据我们所知，这是首次将此类应用与波士顿动力公司的 Spot 机器人结合起来。因此，Ddog 是面向消费者级别应用的首个版本。更重要的是，这能够提高人们对于这类机器人在日常生活中应用的认识和需求，比如在家庭和医院环境中的应用。虽然 Spot 本身是一种工业级别的机器人，但我们认为这些类型的机器人在民用领域也有重要的应用价值。

采访者：为什么开发 Ddog？

Dr. Kosmyna：其中一个项目名为 Brain Switch，它是一个为患有晚期肌萎缩侧索硬化症（ALS）的人士提供沟通支持的工具。我已经在这个项目上工作了大约 10 年。我的研究不仅涵盖了大量文献，还与多个非营利组织和顾问合作，覆盖了两个大洲、三个国家。我们的工作涉及许多患者和照顾者，他们已经在使用我们的 AttentivU 系统。考虑到这些用户的需要，我们进行了进一步的开发。ALS 是一种非常不幸的神经退行性疾病，目前没有治愈方法。我们的系统也不能提供治愈。在疾病的早期阶段，有许多辅助设备可用，如眼动跟踪和语音识别。然而，在疾病的晚期，患者将无法使用眼睛和声音进行交流，唯一剩下的是大脑活动。在这一阶段，有 93% 的美国人拒绝使用呼吸机。但他们仍然可以通过大脑活动进行基本的沟通。目前，我们正在使用的 Brain Switch 初版，为患者提供了基本的是与否沟通方式。而 Ddog 项目则是在我们已建立的基础设施和生态系统上的进一步发展，意在未来发展智能家居控制和机器人支持。我们已经为这些患者实现了智能家居控制，例如，如果他们想开电视或微波炉，可以通过大脑活动来控制。因此，Ddog 是这一发展路线的自然延伸，它是一种能够理解用户需求并为用户提供帮助的助手，例如打开音乐或在需要紧急帮助时通知他人。

采访者：Ddog 项目的技术架构是怎样的？

Dr. Kosmyna：首先，我们将发布一篇论文，不仅是视频。对于那些感兴趣或希望复制这一系统的人来说，你们将能够详细了解技术细节。我们在论文中描述了所谓的技术架构。简而言之，这个系统是设计出来的，同时考虑到了终端用户的需求，这里指的是机器人部分。正如我提到的，我们已经有了大脑电脑接口（BCI）部分，它背后有强大的人工智能支撑。这是一种深度学习技术，能够分析患者的大脑数据，并训练以识别特定患者的大脑模式。因此，当用户想要表达“是”或“否”的时候，系统可以识别并给出响应。机器人本身则是另一整套系统。如你所想，机器人配备了摄像头，需要了解自身在空间中的位置并进行导航。这部分我们称之为映射，属于机器人的功能范畴。我们得到了 Reactive Lions 公司的帮助，在机器人的操作和计算机视觉方面提供了大力支持。手臂部分是关键，它需要能够识别物体并拿起它们。这是一个非常复杂的任务，对于那些不熟悉机器人技术的人来说，拿起一个物体并将其移动是非常有挑战性的。你可以在视频的最后看到我们尝试拿起一个玩具或者试图带来一个轮式椅子。拿起物品并将其放入篮子或直接交给用户，识别用户所在的位置，从而有效地提供支持。

采访者：能否分享更多关于 Ddog 项目基础设施的信息？

Dr. Kosmyna：我们的一些选择是由我们现有的生态系统基础设施预先决定的。这里我指的是之前提到的硬件部分，也就是大脑电脑接口。这已经是一个经过验证的界面形式因素。我们的用户大多都有手机或移动设备，例如 iPhone。作为我们的 Brain Switch 试验的一部分，我们为用户提供了 iPhone，因为不是每个人都有这样的设备，为了统一我们所有用户的体验，我们为他们提供了相同的设备。因此，他们已经拥有了 iPhone，并在其上运行 Brain Switch 应用。所以在这个情况下，我们保持了这部分技术堆栈的基础设施不变。我们增加了第二部分，即 Reactive Lions 公司提供的帮助，他们在操控技术和计算机视觉方面为机器人提供了支持。实际上，为了让整个系统更加简单，这部分也是在另一部 iPhone 上运行。最终，我们得到了一个完全便携、移动、无线的系统，仅依赖于两部 iPhone 和一副眼镜。无需任何背包或重型设备，完全自主运行。

采访者：Ddog 项目能使用哪些其他机器人？为什么选择 Spot？

Dr. Kosmyna：如我之前提到的，国防和军事用途是 Spot 最为人所知的应用场景。虽然有许多成功案例值得关注，但我认为，还没有足够多的案例来满足那些实际上可以从这些系统中获益的用户的需求。我们选择 Spot，因为目前市场上没有其他类似的产品。当然，市场上有操控臂，但它们通常是固定的，不能移动。还有类似人形的机器人，但它们目前还不适合部署，而且对于我们的特定用途来说，可能有些过于复杂。此外，还有外骨骼，这是一个非常强大的工具。但在我来自的法国，Clinatec 实验室在外骨骼的实际应用研究方面取得了出色的成果，尽管如此，它们只进行侵入性研究，并且在过去八年里只有五名患者实际上植入了系统。因此，所有这些因素让我们再次选择了 Spot，因为在工业级别的机器人中，我们没有找到更好的选择。

采访者：能否评论一下 Spot 在国防和军事领域的应用？

Dr. Kosmyna：在国防、警察和军事领域，Spot 的应用确实引起了广泛的关注。例如，我记得最近的一个案例是澳大利亚国防部使用 HoloLens 和电极来控制 Spot。这些应用面临着一些挑战，比如 HoloLens 作为一种头部装置，相当重，且电池寿命仅为两小时，这对最终用户来说并不方便。另一个问题是，在视频中可以看到，他们使用的电极看起来像是粘在耳朵后面的，位置与我们的 AttentivU 设备类似，但它们是有线的且需要粘贴。

采访者：民用用例。

Dr. Kosmyna：对于民用用例来说，用户不太可能会选择这样的设备。例如，经历了长时间工作的母亲回到家后，不太可能选择这样的设备。因此，从我十年的经验来看，无线和舒适的设备更受欢迎。

采访者：伦理考虑。

Dr. Kosmyna：伦理方面的考虑当然也很重要。军事和国防应用确实非常重要，因为它们可以支持那些可能无法或不应该置身于危险中的人。例如，我们已经看到 Spot 被用于核设施等危险环境。然而，也有一些争议，比如纽约警方尝试使用 Spot 进行监控，但效果并不理想。因此，在这些系统真正投入实际应用之前，需要进行更多的讨论和审慎考虑。就像我们的项目一样，我们发布了视频和论文，并继续进行研究，但系统还没有准备好进行广泛部署。除了机器人本身是工业级别的之外，还需要更多的立法和公众讨论。对于大脑感应技术来说，这些讨论尤为重要，因为它处理的是人类最私密的数据：我们的思维。

采访者：Ddog 未来的发展方向是什么？

Dr. Kosmyna：我们即将发布一篇论文和视频，对于感兴趣的人来说，你们可以通过这些资料了解更多技术细节，并可能与我们合作或尝试复制一些部分。我们的项目有两个主要发展方向。第一个是操纵技术，特别是手臂的操纵非常关键。我们希望进一步探索这个领域，并希望与最终用户共同进行探索。目前为止，我们的录像是在实验室严格控制的条件下进行的。

采访者：你想实现的任何疯狂用例。

Dr. Kosmyna：例如，一个可以感知用户恐惧或压力水平的保护版本。在录制视频时，环境是实验室，而且是晚上。如果有人进入并表现出攻击性，机器狗可能已经察觉到了这一点，并可能呼叫帮助......

采访者：Ddog 和 STEM 教育。

Dr. Kosmyna：第二个方向是我们受到马萨诸塞州发生的事情的启发。在波士顿和大波士顿地区的学校中，有一系列的研讨会，让孩子们通过与 Spot 这样的编程机器人互动来引入 STEM 教育，这个项目进行得非常成功。

采访者：联系信息。

Dr. Kosmyna：你现在可以在我身后的图像上看到所有的联系信息，包括网站和电子邮件。你可以通过 nkosmyna [at] http://mit.edu 发邮件给我，询问任何问题。你也可以访问 MediaLab 的网站或我的个人网站 http://braini.io http://BRAINI.IO，了解更多关于 Ddog 以及我们提到的其他项目，如 Thinking Cap、Brain Switch 和 AttentivU，还有更多内容。欢迎随时联系我们。谢谢大家的时间。</title>
            <link>https://nitter.cz/dotey/status/1742979829467627937#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742979829467627937#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:42:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>[深度访谈] Ddog： 世界上首款脑控四足机器人<br />
<br />
Ddog项目的特点集合了Boston Dynamics的Spot机器人以及由AttentivU提供的脑电脑接口（BCI）系统。这个系统是一副能测量人的脑电（EEG - 脑活动）和眼电（EOG - 眼部活动）信号的无线眼镜。Ddog项目是Brain Switch应用的升级版，这是一个实时的闭环BCI系统，允许用户以非语言的方式实时传达简单需求。Brain Switch的目标是帮助那些有身体挑战（如ALS，CP，SCI）的人满足基本的交流需求。Ddog项目是在Brain Switch的技术架构和基础设施基础上构建的。<br />
<br />
Ddog的最大特点是其行动能力：这是第一个完全自主，由大脑驱动，无线的系统，包含了Spot机器人，在两部iPhone上运行，而且不需要黏贴的电极和计算用的背包装置。<br />
Ddog的设计初衷是帮助进行物体操作，例如Spot的手臂被用于: 送货上门，移动椅子，携带书籍或拿玩具等。<br />
<br />
这个Ddog项目负责人Nataliya Kosmyna博士的访谈涵盖了以下话题：为什么要创建Ddog项目？为什么选择使用Spot而不是其他机器人？Ddog背后的技术架构是怎样的？为什么认为Ddog项目需要无线且可携带的大脑感应解决方案是重要的？以及一些疯狂的应用案例，对于Ddog项目的未来展望，STEM和Ddog的联系等等！<br />
<br />
项目网页：<br />
<a href="https://media.mit.edu/projects/ddog">media.mit.edu/projects/ddog</a><br />
<a href="https://braini.io/ddog">braini.io/ddog</a><br />
<br />
以下为采访内容<br />
<br />
采访者：请介绍一下自己。<br />
<br />
Dr. Kosmyna：大家好，我是 Nataliya Kosmyna，麻省理工学院媒体实验室的研究科学家。在过去的 15 年里，我主要研究脑机接口技术。<br />
<br />
采访者：今天我们的讨论主题是什么？<br />
<br />
Dr. Kosmyna：今天我们将讨论我们最新的应用案例，名为 Ddog。Ddog 结合了波士顿动力公司的 Spot 机器人和我们自主研发的脑机接口。这款脑机接口采用了眼镜的形态设计，我们称之为 AttentivU。<br />
<br />
采访者：可以进行演示了吗？<br />
<br />
Dr. Kosmyna：是的，我们有新的项目要展示，这个项目名为 Ddog。它包括了波士顿动力公司的 Spot 机器人和 AttentivU 眼镜。我们的这款产品是可穿戴的、无线的，能够捕捉使用者的大脑活动和眼动。我们内部将其称为“想即得”。如果你没见过 Spot，现在可以在我的背后看到，它是一种类似狗的机器人。你可以通过它在空间内移动，执行各种任务，比如拿椅子、购物或打开门等。<br />
<br />
采访者：能否介绍一下不同的大脑感应技术？<br />
<br />
Dr. Kosmyna：当然。从屏幕上可以看到，我们的技术是如何从最初的阶段发展过来的。最初的阶段看上去像章鱼一样。不过，脑机接口技术（BCI）已经取得了巨大的进步。我们现在有更为简约和微型化的设备。但说实话，即使是头带式的，也不适合日常使用。在运动场或办公室里或许还行，但在日常生活中人们不太可能随身携带。大脑感应技术面临的挑战之一是它需要收集大量的数据，而这些数据的获取成本很高，且不容易获得。这和计算机视觉不同，计算机视觉所需的数据随处可得，易于获取。如果我们想要将脑机接口技术从理论走向实际应用，关键在于数据的收集。它不仅仅是关于一些杀手级应用或者你可能听说过的 XR 和其他头戴设备。目前而言，数据是成功的关键。因此，尽管实验室里的“章鱼式”应用非常适合研究，但现实世界中，我们更加看重的是可穿戴和便携式的设备。这里我特别强调那些可以每天多小时佩戴的设备，它们可以在驾车、办公或者录制视频等各种场合中使用，让用户几乎感觉不到它们的存在。<br />
<br />
采访者：关于大脑感应眼镜的想法。<br />
<br />
Dr. Kosmyna：我们谈论的就是眼镜形式的大脑感应设备。我们每天都在佩戴各种头戴物品，比如耳机、眼镜、太阳镜、口罩和各种帽子等。我们已经习惯了在头部携带这些物品，所以只需要在这些已有的形式中加入数据收集功能。<br />
<br />
采访者：对 Ddog 项目感到兴奋的原因是什么？<br />
<br />
Dr. Kosmyna：Ddog 是一个概念证明项目，显然非常令人兴奋，因为它能够实现利用大脑活动与大型工业机器人进行互动。据我们所知，这是首次将此类应用与波士顿动力公司的 Spot 机器人结合起来。因此，Ddog 是面向消费者级别应用的首个版本。更重要的是，这能够提高人们对于这类机器人在日常生活中应用的认识和需求，比如在家庭和医院环境中的应用。虽然 Spot 本身是一种工业级别的机器人，但我们认为这些类型的机器人在民用领域也有重要的应用价值。<br />
<br />
采访者：为什么开发 Ddog？<br />
<br />
Dr. Kosmyna：其中一个项目名为 Brain Switch，它是一个为患有晚期肌萎缩侧索硬化症（ALS）的人士提供沟通支持的工具。我已经在这个项目上工作了大约 10 年。我的研究不仅涵盖了大量文献，还与多个非营利组织和顾问合作，覆盖了两个大洲、三个国家。我们的工作涉及许多患者和照顾者，他们已经在使用我们的 AttentivU 系统。考虑到这些用户的需要，我们进行了进一步的开发。ALS 是一种非常不幸的神经退行性疾病，目前没有治愈方法。我们的系统也不能提供治愈。在疾病的早期阶段，有许多辅助设备可用，如眼动跟踪和语音识别。然而，在疾病的晚期，患者将无法使用眼睛和声音进行交流，唯一剩下的是大脑活动。在这一阶段，有 93% 的美国人拒绝使用呼吸机。但他们仍然可以通过大脑活动进行基本的沟通。目前，我们正在使用的 Brain Switch 初版，为患者提供了基本的是与否沟通方式。而 Ddog 项目则是在我们已建立的基础设施和生态系统上的进一步发展，意在未来发展智能家居控制和机器人支持。我们已经为这些患者实现了智能家居控制，例如，如果他们想开电视或微波炉，可以通过大脑活动来控制。因此，Ddog 是这一发展路线的自然延伸，它是一种能够理解用户需求并为用户提供帮助的助手，例如打开音乐或在需要紧急帮助时通知他人。<br />
<br />
采访者：Ddog 项目的技术架构是怎样的？<br />
<br />
Dr. Kosmyna：首先，我们将发布一篇论文，不仅是视频。对于那些感兴趣或希望复制这一系统的人来说，你们将能够详细了解技术细节。我们在论文中描述了所谓的技术架构。简而言之，这个系统是设计出来的，同时考虑到了终端用户的需求，这里指的是机器人部分。正如我提到的，我们已经有了大脑电脑接口（BCI）部分，它背后有强大的人工智能支撑。这是一种深度学习技术，能够分析患者的大脑数据，并训练以识别特定患者的大脑模式。因此，当用户想要表达“是”或“否”的时候，系统可以识别并给出响应。机器人本身则是另一整套系统。如你所想，机器人配备了摄像头，需要了解自身在空间中的位置并进行导航。这部分我们称之为映射，属于机器人的功能范畴。我们得到了 Reactive Lions 公司的帮助，在机器人的操作和计算机视觉方面提供了大力支持。手臂部分是关键，它需要能够识别物体并拿起它们。这是一个非常复杂的任务，对于那些不熟悉机器人技术的人来说，拿起一个物体并将其移动是非常有挑战性的。你可以在视频的最后看到我们尝试拿起一个玩具或者试图带来一个轮式椅子。拿起物品并将其放入篮子或直接交给用户，识别用户所在的位置，从而有效地提供支持。<br />
<br />
采访者：能否分享更多关于 Ddog 项目基础设施的信息？<br />
<br />
Dr. Kosmyna：我们的一些选择是由我们现有的生态系统基础设施预先决定的。这里我指的是之前提到的硬件部分，也就是大脑电脑接口。这已经是一个经过验证的界面形式因素。我们的用户大多都有手机或移动设备，例如 iPhone。作为我们的 Brain Switch 试验的一部分，我们为用户提供了 iPhone，因为不是每个人都有这样的设备，为了统一我们所有用户的体验，我们为他们提供了相同的设备。因此，他们已经拥有了 iPhone，并在其上运行 Brain Switch 应用。所以在这个情况下，我们保持了这部分技术堆栈的基础设施不变。我们增加了第二部分，即 Reactive Lions 公司提供的帮助，他们在操控技术和计算机视觉方面为机器人提供了支持。实际上，为了让整个系统更加简单，这部分也是在另一部 iPhone 上运行。最终，我们得到了一个完全便携、移动、无线的系统，仅依赖于两部 iPhone 和一副眼镜。无需任何背包或重型设备，完全自主运行。<br />
<br />
采访者：Ddog 项目能使用哪些其他机器人？为什么选择 Spot？<br />
<br />
Dr. Kosmyna：如我之前提到的，国防和军事用途是 Spot 最为人所知的应用场景。虽然有许多成功案例值得关注，但我认为，还没有足够多的案例来满足那些实际上可以从这些系统中获益的用户的需求。我们选择 Spot，因为目前市场上没有其他类似的产品。当然，市场上有操控臂，但它们通常是固定的，不能移动。还有类似人形的机器人，但它们目前还不适合部署，而且对于我们的特定用途来说，可能有些过于复杂。此外，还有外骨骼，这是一个非常强大的工具。但在我来自的法国，Clinatec 实验室在外骨骼的实际应用研究方面取得了出色的成果，尽管如此，它们只进行侵入性研究，并且在过去八年里只有五名患者实际上植入了系统。因此，所有这些因素让我们再次选择了 Spot，因为在工业级别的机器人中，我们没有找到更好的选择。<br />
<br />
采访者：能否评论一下 Spot 在国防和军事领域的应用？<br />
<br />
Dr. Kosmyna：在国防、警察和军事领域，Spot 的应用确实引起了广泛的关注。例如，我记得最近的一个案例是澳大利亚国防部使用 HoloLens 和电极来控制 Spot。这些应用面临着一些挑战，比如 HoloLens 作为一种头部装置，相当重，且电池寿命仅为两小时，这对最终用户来说并不方便。另一个问题是，在视频中可以看到，他们使用的电极看起来像是粘在耳朵后面的，位置与我们的 AttentivU 设备类似，但它们是有线的且需要粘贴。<br />
<br />
采访者：民用用例。<br />
<br />
Dr. Kosmyna：对于民用用例来说，用户不太可能会选择这样的设备。例如，经历了长时间工作的母亲回到家后，不太可能选择这样的设备。因此，从我十年的经验来看，无线和舒适的设备更受欢迎。<br />
<br />
采访者：伦理考虑。<br />
<br />
Dr. Kosmyna：伦理方面的考虑当然也很重要。军事和国防应用确实非常重要，因为它们可以支持那些可能无法或不应该置身于危险中的人。例如，我们已经看到 Spot 被用于核设施等危险环境。然而，也有一些争议，比如纽约警方尝试使用 Spot 进行监控，但效果并不理想。因此，在这些系统真正投入实际应用之前，需要进行更多的讨论和审慎考虑。就像我们的项目一样，我们发布了视频和论文，并继续进行研究，但系统还没有准备好进行广泛部署。除了机器人本身是工业级别的之外，还需要更多的立法和公众讨论。对于大脑感应技术来说，这些讨论尤为重要，因为它处理的是人类最私密的数据：我们的思维。<br />
<br />
采访者：Ddog 未来的发展方向是什么？<br />
<br />
Dr. Kosmyna：我们即将发布一篇论文和视频，对于感兴趣的人来说，你们可以通过这些资料了解更多技术细节，并可能与我们合作或尝试复制一些部分。我们的项目有两个主要发展方向。第一个是操纵技术，特别是手臂的操纵非常关键。我们希望进一步探索这个领域，并希望与最终用户共同进行探索。目前为止，我们的录像是在实验室严格控制的条件下进行的。<br />
<br />
采访者：你想实现的任何疯狂用例。<br />
<br />
Dr. Kosmyna：例如，一个可以感知用户恐惧或压力水平的保护版本。在录制视频时，环境是实验室，而且是晚上。如果有人进入并表现出攻击性，机器狗可能已经察觉到了这一点，并可能呼叫帮助......<br />
<br />
采访者：Ddog 和 STEM 教育。<br />
<br />
Dr. Kosmyna：第二个方向是我们受到马萨诸塞州发生的事情的启发。在波士顿和大波士顿地区的学校中，有一系列的研讨会，让孩子们通过与 Spot 这样的编程机器人互动来引入 STEM 教育，这个项目进行得非常成功。<br />
<br />
采访者：联系信息。<br />
<br />
Dr. Kosmyna：你现在可以在我身后的图像上看到所有的联系信息，包括网站和电子邮件。你可以通过 nkosmyna [at] <a href="http://mit.edu">mit.edu</a> 发邮件给我，询问任何问题。你也可以访问 MediaLab 的网站或我的个人网站 <a href="http://braini.io">braini.io</a> <a href="http://BRAINI.IO">BRAINI.IO</a>，了解更多关于 Ddog 以及我们提到的其他项目，如 Thinking Cap、Brain Switch 和 AttentivU，还有更多内容。欢迎随时联系我们。谢谢大家的时间。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQyOTc4NDE0NDg1NTczNjMyL2ltZy9DQXFTQk9RVHBSVWNMV2luLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742971021664382996#m</id>
            <title>RT by @dotey: 终于来了！Open AI 将于下周开放 GPTs 商店。

如果你的GPTs 想要上架的话需要满足下面👇三个条件：

1）查看 Open ai的 使用政策和GPT 品牌指南，以确保你的  GPTs 合规。

2）验证你的构建者配置文件（设置 > 构建者配置文件 > 启用你的姓名或经过验证的网站）。

3）将你的 GPTs 发布为“公开”（选择“任何有链接的人”的 GPTs将不会显示在商店中）</title>
            <link>https://nitter.cz/op7418/status/1742971021664382996#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742971021664382996#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>终于来了！Open AI 将于下周开放 GPTs 商店。<br />
<br />
如果你的GPTs 想要上架的话需要满足下面👇三个条件：<br />
<br />
1）查看 Open ai的 使用政策和GPT 品牌指南，以确保你的  GPTs 合规。<br />
<br />
2）验证你的构建者配置文件（设置 > 构建者配置文件 > 启用你的姓名或经过验证的网站）。<br />
<br />
3）将你的 GPTs 发布为“公开”（选择“任何有链接的人”的 GPTs将不会显示在商店中）</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RCRzV4Z2FZQUVTbEIzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742976652487823463#m</id>
            <title>高端挖墙脚，不过感觉确实不难，只要有心</title>
            <link>https://nitter.cz/dotey/status/1742976652487823463#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742976652487823463#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:29:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>高端挖墙脚，不过感觉确实不难，只要有心</p>
<p><a href="https://nitter.cz/mtrainier2020/status/1742975445736513636#m">nitter.cz/mtrainier2020/status/1742975445736513636#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742976011933831632#m</id>
            <title>这周被机器人的新闻刷屏了，Google DeepMind 的机器人团队也刚发布了一篇博客，展示了他们最新的研究进展，不过只有一篇博客，没有代码。

他们开发了一种名为AutoRT的新技术，这是一个将大型基础模型（比如 大语言模型 (LLM) 或视觉语言模型 (VLM)）与机器人控制模型（RT-1或RT-2）相结合的系统。这个系统使得机器人能够在全新的环境中收集训练数据。良好的感知模型配合能够生成运动控制系统指令的大语言模型 (LLM)，将在机器人领域站在潮头。

博客地址：https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/
译文：https://baoyu.io/translations/google/shaping-the-future-of-advanced-robotics</title>
            <link>https://nitter.cz/dotey/status/1742976011933831632#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742976011933831632#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 18:27:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这周被机器人的新闻刷屏了，Google DeepMind 的机器人团队也刚发布了一篇博客，展示了他们最新的研究进展，不过只有一篇博客，没有代码。<br />
<br />
他们开发了一种名为AutoRT的新技术，这是一个将大型基础模型（比如 大语言模型 (LLM) 或视觉语言模型 (VLM)）与机器人控制模型（RT-1或RT-2）相结合的系统。这个系统使得机器人能够在全新的环境中收集训练数据。良好的感知模型配合能够生成运动控制系统指令的大语言模型 (LLM)，将在机器人领域站在潮头。<br />
<br />
博客地址：<a href="https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/">deepmind.google/discover/blo…</a><br />
译文：<a href="https://baoyu.io/translations/google/shaping-the-future-of-advanced-robotics">baoyu.io/translations/google…</a></p>
<p><a href="https://nitter.cz/bindureddy/status/1742962891903623524#m">nitter.cz/bindureddy/status/1742962891903623524#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1742942815355854921#m</id>
            <title>RT by @dotey: AI搜索工具Perplexity宣布B轮7360万美元融资，估值达到5.2亿美元。

下面是这轮融资的一些信息：

Perplexity的月活跃用户增长到了1000万，并在2023年处理了超过50亿次查询。
iOS和Android应用安装量超过100万。

投资者包括NEA、Elad Gil、Nat Friedman和Databricks的支持，以及新的投资者NVIDIA、Jeff Bezos（通过Bezos Expeditions Fund）、Tobi Lutke、Bessemer Ventures、Naval Ravikant等。

B轮7360万美元融资，估值达到5.2亿美元，总融资已经达到了1亿美元。

公告页面：https://blog.perplexity.ai/blog/perplexity-raises-series-b-funding-round</title>
            <link>https://nitter.cz/op7418/status/1742942815355854921#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1742942815355854921#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 16:15:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI搜索工具Perplexity宣布B轮7360万美元融资，估值达到5.2亿美元。<br />
<br />
下面是这轮融资的一些信息：<br />
<br />
Perplexity的月活跃用户增长到了1000万，并在2023年处理了超过50亿次查询。<br />
iOS和Android应用安装量超过100万。<br />
<br />
投资者包括NEA、Elad Gil、Nat Friedman和Databricks的支持，以及新的投资者NVIDIA、Jeff Bezos（通过Bezos Expeditions Fund）、Tobi Lutke、Bessemer Ventures、Naval Ravikant等。<br />
<br />
B轮7360万美元融资，估值达到5.2亿美元，总融资已经达到了1亿美元。<br />
<br />
公告页面：<a href="https://blog.perplexity.ai/blog/perplexity-raises-series-b-funding-round">blog.perplexity.ai/blog/perp…</a></p>
<p><a href="https://nitter.cz/AravSrinivas/status/1742918329797574709#m">nitter.cz/AravSrinivas/status/1742918329797574709#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742965587918164211#m</id>
            <title>供北美找工作或者找远程工作的参考：
Hacknews who is hiring
https://bernawil.github.io/hn-who-is-hiring/</title>
            <link>https://nitter.cz/dotey/status/1742965587918164211#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742965587918164211#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 17:45:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>供北美找工作或者找远程工作的参考：<br />
Hacknews who is hiring<br />
<a href="https://bernawil.github.io/hn-who-is-hiring/">bernawil.github.io/hn-who-is…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742960543772602621#m</id>
            <title>MIT的Ddog项目成功将脑机接口和波士顿动力机器狗进行链连接，脑瘫人士也能操作机器狗

在Nataliya Kos’myna的领导下，麻省理工学院的一支研究团队最近发表了他们的研究成果，一项名为Ddog的项目。

该项目致力于将波士顿动力学公司的Spot四足机器人改造为能够为患有肌萎缩侧索硬化症（ALS）、脑瘫和脊髓损伤等疾病的人士提供基本沟通工具。

该项目 使用了一套包括AttentivU的脑-电脑界面（脑机接口，BCI）系统。这种从技术上实现了脑-电脑直接接口的技术，以一副内嵌传感器的无线眼镜的形式进行应用。这些传感器不仅可以测量人脑活动的脑电图（EEG），也可以追踪他们的眼动。

该研究以大学 的Brain Switch 作为基础，这是一个实时闭环型的脑机接口，它允许用户与看护者进行无言的、实时的交流。如今，Kos’myna正在借助与Brain Switch相同的技术所开展的Ddog项目，进一步扩大该技术的应用范围。

Spot可以为用户取物

根据全国罕见疾病协会报告，目前在美国有3万ALS患者，并且每年有5000个新病例被确诊。另一方面，据脑瘫指南所报告，大约有一百万的美国人生活在脑瘫之中。

许多患者已经或将会失去他们的行走、穿衣、说话、写字甚至呼吸的能力。目前虽然存在沟通辅助工具，但多数是靠视线跟踪的设备，允许患者通过计算机来交流。然而目前还没有太多的系统可以让患者与他们周围的世界进行互动。

Ddog的最大优势在于其移动性。Spot机器人可以进行完全自主的移动，这意味着在给定简单指令后，它可以在无需人工干预的情况下进行操作。

Spot也具有出色的移动性。凭借四足的结构，Spot几乎可以走向人类可以到达的任何地方，包括上下斜坡和楼梯。机器人的手臂配件还可以执行类似送货、移动椅子或者给用户送来书或玩具等任务。

MIT的这套系统只需要两部iPhone和一副眼镜就可以运行。它无需复杂的电极设备或背包，使其比其它现有的辅助设备更方便日常使用，这是研究团队的观点。

Ddog的工作原理

作为与新用户在新环境中工作的初步步骤，Spot首先需要建立工作环境的3D地图。然后，第一部iPhone将询问用户接下来想做些什么，用户只需想象他们想要的东西来做出回答。

第二部iPhone则会运行本地导航地图，并控制Spot的手臂，同时利用iPhone的雷达数据来提升Spot的雷达性能。这两部iPhone可以彼此通信，以跟踪Spot完成任务的进度。

MIT的研究团队设计了一个可以完全在线或离线工作的系统。在线版本则具有一套更为先进的机器学习模型，以及更好的微调模型。

来源：https://www.therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/</title>
            <link>https://nitter.cz/dotey/status/1742960543772602621#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742960543772602621#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 17:25:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MIT的Ddog项目成功将脑机接口和波士顿动力机器狗进行链连接，脑瘫人士也能操作机器狗<br />
<br />
在Nataliya Kos’myna的领导下，麻省理工学院的一支研究团队最近发表了他们的研究成果，一项名为Ddog的项目。<br />
<br />
该项目致力于将波士顿动力学公司的Spot四足机器人改造为能够为患有肌萎缩侧索硬化症（ALS）、脑瘫和脊髓损伤等疾病的人士提供基本沟通工具。<br />
<br />
该项目 使用了一套包括AttentivU的脑-电脑界面（脑机接口，BCI）系统。这种从技术上实现了脑-电脑直接接口的技术，以一副内嵌传感器的无线眼镜的形式进行应用。这些传感器不仅可以测量人脑活动的脑电图（EEG），也可以追踪他们的眼动。<br />
<br />
该研究以大学 的Brain Switch 作为基础，这是一个实时闭环型的脑机接口，它允许用户与看护者进行无言的、实时的交流。如今，Kos’myna正在借助与Brain Switch相同的技术所开展的Ddog项目，进一步扩大该技术的应用范围。<br />
<br />
Spot可以为用户取物<br />
<br />
根据全国罕见疾病协会报告，目前在美国有3万ALS患者，并且每年有5000个新病例被确诊。另一方面，据脑瘫指南所报告，大约有一百万的美国人生活在脑瘫之中。<br />
<br />
许多患者已经或将会失去他们的行走、穿衣、说话、写字甚至呼吸的能力。目前虽然存在沟通辅助工具，但多数是靠视线跟踪的设备，允许患者通过计算机来交流。然而目前还没有太多的系统可以让患者与他们周围的世界进行互动。<br />
<br />
Ddog的最大优势在于其移动性。Spot机器人可以进行完全自主的移动，这意味着在给定简单指令后，它可以在无需人工干预的情况下进行操作。<br />
<br />
Spot也具有出色的移动性。凭借四足的结构，Spot几乎可以走向人类可以到达的任何地方，包括上下斜坡和楼梯。机器人的手臂配件还可以执行类似送货、移动椅子或者给用户送来书或玩具等任务。<br />
<br />
MIT的这套系统只需要两部iPhone和一副眼镜就可以运行。它无需复杂的电极设备或背包，使其比其它现有的辅助设备更方便日常使用，这是研究团队的观点。<br />
<br />
Ddog的工作原理<br />
<br />
作为与新用户在新环境中工作的初步步骤，Spot首先需要建立工作环境的3D地图。然后，第一部iPhone将询问用户接下来想做些什么，用户只需想象他们想要的东西来做出回答。<br />
<br />
第二部iPhone则会运行本地导航地图，并控制Spot的手臂，同时利用iPhone的雷达数据来提升Spot的雷达性能。这两部iPhone可以彼此通信，以跟踪Spot完成任务的进度。<br />
<br />
MIT的研究团队设计了一个可以完全在线或离线工作的系统。在线版本则具有一套更为先进的机器学习模型，以及更好的微调模型。<br />
<br />
来源：<a href="https://www.therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/">therobotreport.com/ddog-mit-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI5NTY0MTE4NjMwNjQ1NzYvcHUvaW1nL2RGcWVQb0daR19wMElhWHouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742928955848679812#m</id>
            <title>有意思的文章

译文：https://baoyu.io/translations/people/elon-musk-is-not-understood</title>
            <link>https://nitter.cz/dotey/status/1742928955848679812#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742928955848679812#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 15:20:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有意思的文章<br />
<br />
译文：<a href="https://baoyu.io/translations/people/elon-musk-is-not-understood">baoyu.io/translations/people…</a></p>
<p><a href="https://nitter.cz/Danielw19410/status/1742561488299323422#m">nitter.cz/Danielw19410/status/1742561488299323422#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MjkyODk1OTk0NjU0NzIwMC9ZNmpNSVJ4UD9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742839505412137338#m</id>
            <title>RT by @dotey: 兄弟们炸裂了

Meta AI又发布了一个炸裂的东西：从音频生成全身逼真的虚拟人物形象。

它可以从多人对话中语音中生成与对话相对应的逼真面部表情、完整身体和手势动作。

这些生成的虚拟人物不仅在视觉上很逼真，而且能够准确地反映出对话中的手势和表情细节，如指点、手腕抖动、耸肩、微笑、嘲笑等。

工作原理：

该项目结合了向量量化的样本多样性和通过扩散获得的高频细节的优势，以生成更具动态性和表现力的动作。

1、数据集捕获：首先捕获了一组丰富的双人对话数据集，这些数据集允许进行逼真的重建。

2、运动模型构建：项目构建了一个包括面部运动模型、引导姿势预测器和身体运动模型的复合运动模型。

3、面部运动生成：使用预训练的唇部回归器处理音频，提取面部运动相关的特征。
利用条件扩散模型根据这些特征生成面部运动。

4、身体运动生成：以音频为输入，自回归地输出每秒1帧的向量量化（VQ）引导姿势。将音频和引导姿势一起输入到扩散模型中，以30帧/秒的速度生成高频身体运动。

5、虚拟人物渲染：将生成的面部和身体运动传入训练好的虚拟人物渲染器，生成逼真的虚拟人物。

6、结果展示：最终展示的是根据音频生成的全身逼真虚拟人物，这些虚拟人物能够表现出对话中的细微表情和手势动作。

项目及演示：https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/
论文：https://arxiv.org/pdf/2401.01885.pdf
GitHub：https://github.com/facebookresearch/audio2photoreal/
Demo：https://colab.research.google.com/drive/1lnX3d-3T3LaO3nlN6R8s6pPvVNAk5mdK?usp=sharing</title>
            <link>https://nitter.cz/xiaohuggg/status/1742839505412137338#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742839505412137338#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 09:24:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们炸裂了<br />
<br />
Meta AI又发布了一个炸裂的东西：从音频生成全身逼真的虚拟人物形象。<br />
<br />
它可以从多人对话中语音中生成与对话相对应的逼真面部表情、完整身体和手势动作。<br />
<br />
这些生成的虚拟人物不仅在视觉上很逼真，而且能够准确地反映出对话中的手势和表情细节，如指点、手腕抖动、耸肩、微笑、嘲笑等。<br />
<br />
工作原理：<br />
<br />
该项目结合了向量量化的样本多样性和通过扩散获得的高频细节的优势，以生成更具动态性和表现力的动作。<br />
<br />
1、数据集捕获：首先捕获了一组丰富的双人对话数据集，这些数据集允许进行逼真的重建。<br />
<br />
2、运动模型构建：项目构建了一个包括面部运动模型、引导姿势预测器和身体运动模型的复合运动模型。<br />
<br />
3、面部运动生成：使用预训练的唇部回归器处理音频，提取面部运动相关的特征。<br />
利用条件扩散模型根据这些特征生成面部运动。<br />
<br />
4、身体运动生成：以音频为输入，自回归地输出每秒1帧的向量量化（VQ）引导姿势。将音频和引导姿势一起输入到扩散模型中，以30帧/秒的速度生成高频身体运动。<br />
<br />
5、虚拟人物渲染：将生成的面部和身体运动传入训练好的虚拟人物渲染器，生成逼真的虚拟人物。<br />
<br />
6、结果展示：最终展示的是根据音频生成的全身逼真虚拟人物，这些虚拟人物能够表现出对话中的细微表情和手势动作。<br />
<br />
项目及演示：<a href="https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/">people.eecs.berkeley.edu/~ev…</a><br />
论文：<a href="https://arxiv.org/pdf/2401.01885.pdf">arxiv.org/pdf/2401.01885.pdf</a><br />
GitHub：<a href="https://github.com/facebookresearch/audio2photoreal/">github.com/facebookresearch/…</a><br />
Demo：<a href="https://colab.research.google.com/drive/1lnX3d-3T3LaO3nlN6R8s6pPvVNAk5mdK?usp=sharing">colab.research.google.com/dr…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI4MzQxMDEyOTA0ODM3MTIvcHUvaW1nLzJTa3l3blA5SU1BTUdTTEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742813384725205300#m</id>
            <title>超导又狼来了</title>
            <link>https://nitter.cz/dotey/status/1742813384725205300#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742813384725205300#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 07:41:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>超导又狼来了</p>
<p><a href="https://nitter.cz/pronounced_kyle/status/1742588127628361809#m">nitter.cz/pronounced_kyle/status/1742588127628361809#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1742798404936241242#m</id>
            <title>一位有视觉障碍的计算机专业学生谈 ChatGPT 对他们学业上的影响：

我是一位视障的计算机科学学生。在接触到ChatGPT之前，我的学习进展是非常困难的，因为我的一些教授是视觉学习者，他们老是强调，如果我们无法看到或画出我们正在处理的问题，我们将难以通过他们的课程。这使我感到非常压力，即使我知道有很多优秀的视障软件开发者。这种压力让我在进行编码项目时感到非常焦虑，以至于我经常在思维的纷争中陷入僵局，无法真正写出代码。

然而，有了ChatGPT的帮助，它强大的能力使我从这种困境中走了出来。虽然我现在还会有那么几天，焦虑让我无法进行我想要完成的代码工作，但大多数时候我已经摆脱了这种状态。

ChatGPT还为我提供了有关教授课堂示范的代码示例，使我能更好地理解他们的指导。如果没有ChatGPT的帮助，我真不知道我今天的学习会是什么样子。

我非常感谢有这样的工具，使我的学习之路变得更加明亮。</title>
            <link>https://nitter.cz/dotey/status/1742798404936241242#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1742798404936241242#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 06:41:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一位有视觉障碍的计算机专业学生谈 ChatGPT 对他们学业上的影响：<br />
<br />
我是一位视障的计算机科学学生。在接触到ChatGPT之前，我的学习进展是非常困难的，因为我的一些教授是视觉学习者，他们老是强调，如果我们无法看到或画出我们正在处理的问题，我们将难以通过他们的课程。这使我感到非常压力，即使我知道有很多优秀的视障软件开发者。这种压力让我在进行编码项目时感到非常焦虑，以至于我经常在思维的纷争中陷入僵局，无法真正写出代码。<br />
<br />
然而，有了ChatGPT的帮助，它强大的能力使我从这种困境中走了出来。虽然我现在还会有那么几天，焦虑让我无法进行我想要完成的代码工作，但大多数时候我已经摆脱了这种状态。<br />
<br />
ChatGPT还为我提供了有关教授课堂示范的代码示例，使我能更好地理解他们的指导。如果没有ChatGPT的帮助，我真不知道我今天的学习会是什么样子。<br />
<br />
我非常感谢有这样的工具，使我的学习之路变得更加明亮。</p>
<p><a href="https://nitter.cz/aaron_stormerr/status/1742596896475185504#m">nitter.cz/aaron_stormerr/status/1742596896475185504#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0MtbzF4QlhZQUFLRkg1LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>