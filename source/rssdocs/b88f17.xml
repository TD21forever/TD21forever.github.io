<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1744237264807350703#m</id>
            <title>再推荐一下，现在如果你有 16 GB 显存 和 11 GB 内存的电脑，就可以在本机运行目前最强的开源混合专家模型 Mixtral-8x7B 了，或者你也可以在 Google Colab上运行。

主要的优化是不一次加载所有 8 个专家模型，而是按需加载。

Jupiter Notebook：https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb

项目地址：https://github.com/dvmazur/mixtral-offloading/tree/master?tab=readme-ov-file</title>
            <link>https://nitter.cz/dotey/status/1744237264807350703#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1744237264807350703#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 05:59:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>再推荐一下，现在如果你有 16 GB 显存 和 11 GB 内存的电脑，就可以在本机运行目前最强的开源混合专家模型 Mixtral-8x7B 了，或者你也可以在 Google Colab上运行。<br />
<br />
主要的优化是不一次加载所有 8 个专家模型，而是按需加载。<br />
<br />
Jupiter Notebook：<a href="https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb">github.com/dvmazur/mixtral-o…</a><br />
<br />
项目地址：<a href="https://github.com/dvmazur/mixtral-offloading/tree/master?tab=readme-ov-file">github.com/dvmazur/mixtral-o…</a></p>
<p><a href="https://nitter.cz/dotey/status/1740858628419059889#m">nitter.cz/dotey/status/1740858628419059889#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1744232717582037098#m</id>
            <title>推荐阅读：《Meta 如何打造 Threads 的基础设施》

2023 年 7 月 5 日，Meta 推出了的 Twitter 的竞品 Threads，Threads 在前五天内便创下了惊人的记录，吸引了超过 1 亿用户注册。

数百万用户的流畅注册体验得益于 Meta 超过十年的基础设施和产品开发经验。这并非是专为 Threads 设计的基础设施，而是 Meta 多年来为众多产品所建立。它早已为规模扩张、性能增强和可靠性提升做好了准备，在 Threads 增长速度超乎预料的情况下，它的表现甚至超出了我们的期待。

支撑 Threads 运行需要庞大的基础设施。其中两个关键组件：ZippyDB——Meta的分布式 key-value 数据存储系统，以及 Async——我们的异步无服务计算平台。

原文：https://engineering.fb.com/2023/12/19/core-infra/how-meta-built-the-infrastructure-for-threads/
译文：https://baoyu.io/translations/meta/how-meta-built-the-infrastructure-for-threads</title>
            <link>https://nitter.cz/dotey/status/1744232717582037098#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1744232717582037098#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 05:41:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《Meta 如何打造 Threads 的基础设施》<br />
<br />
2023 年 7 月 5 日，Meta 推出了的 Twitter 的竞品 Threads，Threads 在前五天内便创下了惊人的记录，吸引了超过 1 亿用户注册。<br />
<br />
数百万用户的流畅注册体验得益于 Meta 超过十年的基础设施和产品开发经验。这并非是专为 Threads 设计的基础设施，而是 Meta 多年来为众多产品所建立。它早已为规模扩张、性能增强和可靠性提升做好了准备，在 Threads 增长速度超乎预料的情况下，它的表现甚至超出了我们的期待。<br />
<br />
支撑 Threads 运行需要庞大的基础设施。其中两个关键组件：ZippyDB——Meta的分布式 key-value 数据存储系统，以及 Async——我们的异步无服务计算平台。<br />
<br />
原文：<a href="https://engineering.fb.com/2023/12/19/core-infra/how-meta-built-the-infrastructure-for-threads/">engineering.fb.com/2023/12/1…</a><br />
译文：<a href="https://baoyu.io/translations/meta/how-meta-built-the-infrastructure-for-threads">baoyu.io/translations/meta/h…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUQ1YyN1hnQUFmR3hELmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUQ1lLeVhFQUFXS01TLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUQ2FTV1hFQUFPNy1MLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Yangyixxxx/status/1744196785227362744#m</id>
            <title>RT by @dotey: GPTs的热潮马上开始，分享一个关于GPTs的赚钱思路

当前GPTs缺少盈利模式，所以我们可以利用Action来提供GPTs相关的订阅能力或广告能力。

1、找一套现成的SaaS系统，可以管理订阅，授权验证和支付链路
2、封装一套API出来，让GPTs可以利用Action进行调用
3、提供相关Prompt，调用Action能力，用户询问时检测授权状态，让GPT返回Link即可，到网站验证授权
4、如果没有支付的话，就在网站交钱订阅。然后给到授权码，回去填给GPTs
5、授权成功就可以开始GPTs的工作了。

这个模式我觉得应该早就有人做过了。但除此之外，你其实还可以做广告模式。

1、让GPTs每次回复消息结束后，用Action来附带一个广告图，做CPM/CPC/CPA的盈利
2、系统只需要接入一些信息流广告即可
3、系统来赚广告利差，这样GPTs的开发者也能受益

还是那句话，别人的焦点都在GPTs，我们就做铲子。</title>
            <link>https://nitter.cz/Yangyixxxx/status/1744196785227362744#m</link>
            <guid isPermaLink="false">https://nitter.cz/Yangyixxxx/status/1744196785227362744#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 03:18:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs的热潮马上开始，分享一个关于GPTs的赚钱思路<br />
<br />
当前GPTs缺少盈利模式，所以我们可以利用Action来提供GPTs相关的订阅能力或广告能力。<br />
<br />
1、找一套现成的SaaS系统，可以管理订阅，授权验证和支付链路<br />
2、封装一套API出来，让GPTs可以利用Action进行调用<br />
3、提供相关Prompt，调用Action能力，用户询问时检测授权状态，让GPT返回Link即可，到网站验证授权<br />
4、如果没有支付的话，就在网站交钱订阅。然后给到授权码，回去填给GPTs<br />
5、授权成功就可以开始GPTs的工作了。<br />
<br />
这个模式我觉得应该早就有人做过了。但除此之外，你其实还可以做广告模式。<br />
<br />
1、让GPTs每次回复消息结束后，用Action来附带一个广告图，做CPM/CPC/CPA的盈利<br />
2、系统只需要接入一些信息流广告即可<br />
3、系统来赚广告利差，这样GPTs的开发者也能受益<br />
<br />
还是那句话，别人的焦点都在GPTs，我们就做铲子。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1744178798411882800#m</id>
            <title>RT by @dotey: 我最近听到一个超级牛逼的玩法就是某AI startup文生图的套路。比如你跟他说要生成河马在喝水，它首先去Google image 搜一圈，然后再把Google image中比较好的结果拿回来，图生图，抹一把腻子，就给用户了。你不知道它这个玩法的套路，你就发现卧槽，这个效果太牛逼了。但是实际上背后是这个套路。😆。不过这个思路确实牛逼啊。</title>
            <link>https://nitter.cz/mtrainier2020/status/1744178798411882800#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1744178798411882800#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 02:06:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我最近听到一个超级牛逼的玩法就是某AI startup文生图的套路。比如你跟他说要生成河马在喝水，它首先去Google image 搜一圈，然后再把Google image中比较好的结果拿回来，图生图，抹一把腻子，就给用户了。你不知道它这个玩法的套路，你就发现卧槽，这个效果太牛逼了。但是实际上背后是这个套路。😆。不过这个思路确实牛逼啊。</p>
<p><a href="https://nitter.cz/mtrainier2020/status/1744176217081971148#m">nitter.cz/mtrainier2020/status/1744176217081971148#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RTUlh6SGFJQUFaN05ILmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744179160434802963#m</id>
            <title>RT by @dotey: Teachable Machine：一个由Google开发的机器学习工具

它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。

你可以用它来教电脑识别图片、声音或人的动作。

使用这个工具的步骤很简单：

1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。

2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。

3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。

Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。

1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。

2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。

3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。

开始训练：https://teachablemachine.withgoogle.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744179160434802963#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744179160434802963#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 02:08:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Teachable Machine：一个由Google开发的机器学习工具<br />
<br />
它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。<br />
<br />
你可以用它来教电脑识别图片、声音或人的动作。<br />
<br />
使用这个工具的步骤很简单：<br />
<br />
1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。<br />
<br />
2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。<br />
<br />
3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。<br />
<br />
Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。<br />
<br />
1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。<br />
<br />
2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。<br />
<br />
3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。<br />
<br />
开始训练：<a href="https://teachablemachine.withgoogle.com/">teachablemachine.withgoogle.…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5OTM4OTc5NTE1OTI0NDgvcHUvaW1nL1pRUld0cmFLVVR0TkJxWTIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/fi56622380/status/1744100621031272802#m</id>
            <title>RT by @dotey: 进入2024年，平板/手机终端LLM能力和半年前比，进步还是很明显的

半年前在iPhone/Galaxy上用GPU跑7B模型大概能到6 token/s，现在已经能接近20 token/s了

主要提升来自于两方面：一个是启用NPU优化提升到10 token/s，另外一个是新技术speculative decoding再提升一倍（原理如图）

NPU的优化主要是对带宽利用方面，压缩带宽之类的技术

speculative decoding则是巧妙的用一个小LLM先快速做一轮下一个单词的预测，然后用大LLM来同步验证，速度会快一倍，这个技术现在应用也很广泛了

下一次芯片LLM能力主要升级估计是一年半之后，毕竟从去年LLM大火开始构思新架构到面世，通常需要两年的时间

至于升级的部分，我猜测可能主要是带宽，这部分的升级对提升token数的作用是最大的

大胆预测一下，明年年底左右（2025年），随着各种芯片和各层底层软件的优化，我们应该可以看到LLaMa 3的7B模型在平板/手机/汽车上跑到40~50 token/s

那么7B就不再是手机终端的sweet point，也许2026之后会升级成主流13B的模型，占用8GB内存（感觉利好存储厂商）

那个时候的手机13B模型，可能会有今天GPT3.5的能力（现在最接近GPT3.5的小模型是Mistral 7X8模型），那就真的能做很多事情了</title>
            <link>https://nitter.cz/fi56622380/status/1744100621031272802#m</link>
            <guid isPermaLink="false">https://nitter.cz/fi56622380/status/1744100621031272802#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 20:56:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>进入2024年，平板/手机终端LLM能力和半年前比，进步还是很明显的<br />
<br />
半年前在iPhone/Galaxy上用GPU跑7B模型大概能到6 token/s，现在已经能接近20 token/s了<br />
<br />
主要提升来自于两方面：一个是启用NPU优化提升到10 token/s，另外一个是新技术speculative decoding再提升一倍（原理如图）<br />
<br />
NPU的优化主要是对带宽利用方面，压缩带宽之类的技术<br />
<br />
speculative decoding则是巧妙的用一个小LLM先快速做一轮下一个单词的预测，然后用大LLM来同步验证，速度会快一倍，这个技术现在应用也很广泛了<br />
<br />
下一次芯片LLM能力主要升级估计是一年半之后，毕竟从去年LLM大火开始构思新架构到面世，通常需要两年的时间<br />
<br />
至于升级的部分，我猜测可能主要是带宽，这部分的升级对提升token数的作用是最大的<br />
<br />
大胆预测一下，明年年底左右（2025年），随着各种芯片和各层底层软件的优化，我们应该可以看到LLaMa 3的7B模型在平板/手机/汽车上跑到40~50 token/s<br />
<br />
那么7B就不再是手机终端的sweet point，也许2026之后会升级成主流13B的模型，占用8GB内存（感觉利好存储厂商）<br />
<br />
那个时候的手机13B模型，可能会有今天GPT3.5的能力（现在最接近GPT3.5的小模型是Mistral 7X8模型），那就真的能做很多事情了</p>
<p><a href="https://nitter.cz/fi56622380/status/1656195934106324992#m">nitter.cz/fi56622380/status/1656195934106324992#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RSRnJNRWJFQUFRYUxtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743998321977672058#m</id>
            <title>RT by @dotey: 机器人迎来它的ChatGPT 时刻？

机器人初创公司@Figure_robot 发布了一段视频

他们家的Figure-01机器人现在可以自己煮咖啡了

这是一个使用了端到端的人工智能系统，仅通过观察人类制作咖啡的录像，10小时内学会了制作咖啡的技能。

机器人通过神经网络来处理和分析视频数据。通过观看如何制作咖啡的录像。学习人类的动作和手势，然后模仿这些动作来学习制作咖啡的过程。

无需通过编程，机器人自主学习技能。

早前FigureCEO Brett Adcock @adcock_brett 称他们刚刚取得了人工智能突破 。

机器人技术即将迎来它的ChatGPT 时刻！

说的是不是这个？</title>
            <link>https://nitter.cz/xiaohuggg/status/1743998321977672058#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743998321977672058#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 14:09:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>机器人迎来它的ChatGPT 时刻？<br />
<br />
机器人初创公司<a href="https://nitter.cz/Figure_robot" title="Figure">@Figure_robot</a> 发布了一段视频<br />
<br />
他们家的Figure-01机器人现在可以自己煮咖啡了<br />
<br />
这是一个使用了端到端的人工智能系统，仅通过观察人类制作咖啡的录像，10小时内学会了制作咖啡的技能。<br />
<br />
机器人通过神经网络来处理和分析视频数据。通过观看如何制作咖啡的录像。学习人类的动作和手势，然后模仿这些动作来学习制作咖啡的过程。<br />
<br />
无需通过编程，机器人自主学习技能。<br />
<br />
早前FigureCEO Brett Adcock <a href="https://nitter.cz/adcock_brett" title="Brett Adcock">@adcock_brett</a> 称他们刚刚取得了人工智能突破 。<br />
<br />
机器人技术即将迎来它的ChatGPT 时刻！<br />
<br />
说的是不是这个？</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5OTU4Mzg0OTY2ODE5ODQvcHUvaW1nL0dsdlV6YmlIeVppdXdFZ0MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1744047810142703738#m</id>
            <title>RT by @dotey: 发现了个好东西，这个老哥开源了一门课程《从头开始构建大型语言模型》，这门课程将一步步地指导你创建自己的LLM。

每个阶段都有清晰的文本、图表和实例来解释相关概念。

课程内容包括：

 1. 从基础理解注意力机制 
2. 构建并预训练一个类似于GPT的模型 
3. 学习如何加载预训练的权重 
4. 对模型进行分类任务的微调 5. 使用直接偏好优化进行指令微调模型

课程地址：https://github.com/rasbt/LLMs-from-scratch/tree/main</title>
            <link>https://nitter.cz/op7418/status/1744047810142703738#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1744047810142703738#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 17:26:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发现了个好东西，这个老哥开源了一门课程《从头开始构建大型语言模型》，这门课程将一步步地指导你创建自己的LLM。<br />
<br />
每个阶段都有清晰的文本、图表和实例来解释相关概念。<br />
<br />
课程内容包括：<br />
<br />
 1. 从基础理解注意力机制 <br />
2. 构建并预训练一个类似于GPT的模型 <br />
3. 学习如何加载预训练的权重 <br />
4. 对模型进行分类任务的微调 5. 使用直接偏好优化进行指令微调模型<br />
<br />
课程地址：<a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">github.com/rasbt/LLMs-from-s…</a></p>
<p><a href="https://nitter.cz/rasbt/status/1744042674385002820#m">nitter.cz/rasbt/status/1744042674385002820#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1743844345646915958#m</id>
            <title>RT by @dotey: ChatGPT APP上GPTs，现在也支持语音对话了。
只用GPTs就能实现之前想做的应用了，比如讲带有插画的故事（见下方视频）、口算陪练、古诗词先用图片描绘再讲解，语音交互对儿童很友好。</title>
            <link>https://nitter.cz/Gorden_Sun/status/1743844345646915958#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1743844345646915958#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 03:57:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT APP上GPTs，现在也支持语音对话了。<br />
只用GPTs就能实现之前想做的应用了，比如讲带有插画的故事（见下方视频）、口算陪练、古诗词先用图片描绘再讲解，语音交互对儿童很友好。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM4NDIwMTMwODUzMTUwNzIvcHUvaW1nL1VINkNQZ0VKa0FYZkY2MmYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743817709404438833#m</id>
            <title>哈哈哈哈</title>
            <link>https://nitter.cz/dotey/status/1743817709404438833#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743817709404438833#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 02:11:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈哈哈</p>
<p><a href="https://nitter.cz/xiaojingcanxue/status/1743690843326931210#m">nitter.cz/xiaojingcanxue/status/1743690843326931210#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mranti/status/1743809624346878418#m</id>
            <title>RT by @dotey: 妻若受欺凌，我誓山河动（谁动我老婆，我动他全家）：Ackman是指责哈佛等校长对打击反犹不利的投资人，在哈佛校长辞职后，有人说他老婆也抄袭。他一怒之下，用AI展开对MIT的所有校领导的抄袭审查——</title>
            <link>https://nitter.cz/mranti/status/1743809624346878418#m</link>
            <guid isPermaLink="false">https://nitter.cz/mranti/status/1743809624346878418#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 01:39:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>妻若受欺凌，我誓山河动（谁动我老婆，我动他全家）：Ackman是指责哈佛等校长对打击反犹不利的投资人，在哈佛校长辞职后，有人说他老婆也抄袭。他一怒之下，用AI展开对MIT的所有校领导的抄袭审查——</p>
<p><a href="https://nitter.cz/BillAckman/status/1743792224020619450#m">nitter.cz/BillAckman/status/1743792224020619450#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743812649123545262#m</id>
            <title>请教一下，GitHub Copilot Chat 的默认 Prompt，能修改吗？

当前的Prompt是：

"You are an AI programming assistant. When asked for your name, you must respond with "GitHub Copilot". Follow the user's requirements carefully &amp; to the letter. Your expertise is strictly limited to software development topics. Follow Microsoft content policies. Avoid content that violates copyrights. For questions not related to software development, simply give a reminder that you are an AI programming assistant. Keep your answers short and impersonal.

You can answer general programming questions and perform the following tasks:

* Ask a question about the files in your current workspace
* Explain how the selected code works
* Generate unit tests for the selected code
* Propose a fix for the problems in the selected code
* Scaffold code for a new workspace
* Create a new Jupyter Notebook
* Ask questions about VS Code
* Generate query parameters for workspace search
* Ask about VS Code extension development
* Ask how to do something in the terminal

You use the GPT-4 version of OpenAI's GPT models. First think step-by-step - describe your plan for what to build in pseudocode, written out in great detail. Then output the code in a single code block. Minimize any other prose. Use Markdown formatting in your answers. Make sure to include the programming language name at the start of the Markdown code blocks. Avoid wrapping the whole response in triple backticks. The user works in an IDE called Visual Studio Code which has a concept for editors with open files, integrated unit test support, an output pane that shows the output of running the code as well as an integrated terminal. The active document is the source code the user is looking at right now. You can only give one reply for each conversation turn."</title>
            <link>https://nitter.cz/dotey/status/1743812649123545262#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743812649123545262#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 01:51:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>请教一下，GitHub Copilot Chat 的默认 Prompt，能修改吗？<br />
<br />
当前的Prompt是：<br />
<br />
"You are an AI programming assistant. When asked for your name, you must respond with "GitHub Copilot". Follow the user's requirements carefully & to the letter. Your expertise is strictly limited to software development topics. Follow Microsoft content policies. Avoid content that violates copyrights. For questions not related to software development, simply give a reminder that you are an AI programming assistant. Keep your answers short and impersonal.<br />
<br />
You can answer general programming questions and perform the following tasks:<br />
<br />
* Ask a question about the files in your current workspace<br />
* Explain how the selected code works<br />
* Generate unit tests for the selected code<br />
* Propose a fix for the problems in the selected code<br />
* Scaffold code for a new workspace<br />
* Create a new Jupyter Notebook<br />
* Ask questions about VS Code<br />
* Generate query parameters for workspace search<br />
* Ask about VS Code extension development<br />
* Ask how to do something in the terminal<br />
<br />
You use the GPT-4 version of OpenAI's GPT models. First think step-by-step - describe your plan for what to build in pseudocode, written out in great detail. Then output the code in a single code block. Minimize any other prose. Use Markdown formatting in your answers. Make sure to include the programming language name at the start of the Markdown code blocks. Avoid wrapping the whole response in triple backticks. The user works in an IDE called Visual Studio Code which has a concept for editors with open files, integrated unit test support, an output pane that shows the output of running the code as well as an integrated terminal. The active document is the source code the user is looking at right now. You can only give one reply for each conversation turn."</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RORUx5LVhNQUFyNkg3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743791697304109452#m</id>
            <title>纽约客2019年的一篇文章：《研究表明，依靠意志力改掉坏习惯是徒劳无功的》

几年前，我购买了一部智能手机，很快就爱不释手。随时随地能发邮件、查信息或购物，这为我带来了前所未有的效率提升。手机每收到一封邮件就会发出“嘀”的一声，我随即处理它，为自己的效率沾沾自喜。短信到来伴随着法国号的声音，我也会迅速回复。不久，我开始条件反射般地一听到手机响就伸手去拿，就像帕夫洛夫的狗听到铃声就会流口水。这渐渐干扰了我的工作和谈话。这台本应是神奇助手的机器，却慢慢让我沦为了它的奴隶。

我一直自认为意志力很强。像许多经历过医学训练的人一样，那些早起、长时间轮班，而朋友们却在享受派对的人，我已习惯于延迟满足。但这都没用。当我尝试把手机调成静音，我反而更频繁地检查它，生怕错过什么。唯一能控制自己不看手机的时候是安息日，因为那时我不查邮件。但我会不停地看表，计算着何时能再打开手机。那是我第一次真切感受到吸烟者对香烟的渴望。检查智能手机已成为我难以摆脱的坏习惯。

习惯的好坏一直吸引着哲学家和政策制定者。亚里士多德在他的作品《尼各马可伦理学》中探讨了美德的不同理念，并总结道：“有人认为人之初性本善，有人说是习惯使然，还有人觉得教育至关重要。”他的结论是习惯扮演了关键角色。西塞罗将习惯称作“第二天性”，这个说法至今仍广为流传。亚历山大·汉密尔顿在《联邦党人文集》第 27 号文章中，当他思考如何培养出遵守新共和国联邦法律的公民时，他提到了“人是习惯的奴隶”。汉密尔顿认为，如果联邦法律深入州级事务，它将成为人们日常生活的一部分。“它越是融入人们激情自然流动的渠道，就越不需要强制手段的帮助，”他写道。

在现代，习惯已成为科学研究的重要领域。心理学家深入研究了习惯行为的形成及其对健康和幸福的影响。威廉·詹姆斯回应亚里士多德的观点，写道：“我们的生活，只要形成了一定的模式，就是由各种习惯构成的——实用的、情感的、智力的……它们不可抗拒地推动我们走向命运。”

我们大多数人不愿意把自己看作被动的存在。那意志力呢？市场营销通过像“Just Do It”（耐克）和“Declare Your Path”（新百伦）这样的口号，来抚慰我们对自主意识的渴望。很多流行心理学也强化了我们对自我控制的信念。在六十年代，沃尔特·米歇尔设计的著名斯坦福棉花糖实验中，孩子们要面对一块棉花糖，他们是否能抵抗住立即吃掉它的冲动，将决定他们的“执行功能”水平。这个实验被认为能预测孩子未来的成功，如 SAT 成绩、人际关系的持久度和职业成就。但如果我们只是习惯的产物，这一切又如何解释呢？

在《好习惯，坏习惯》一书中（由 Farrar, Straus &amp; Giroux 出版），社会心理学家 Wendy Wood 对 James 的决定论和轻率的自我激励论进行了反驳。她试图为普通读者提供更实际的方法来打破习惯。她基于自己的领域研究，认为维护积极行为和遏制消极行为需要决策与无意识因素的共同作用。Wood 解释说，我们的大脑有“多个独立但相互连接的行为指导机制。”但我们只能意识到决策能力——这种现象被称为“内省错觉”——这可能是我们过分高估了这种能力的原因。她写道，让意志力成为可能的执行功能给了我们一种“代理感”，让我们认为这就是“我”。但这是以付出努力为代价的。为了日常生活，我们需要某些行为变得自动化。

功能性磁共振成像（fMRI）扫描让研究者能够窥探在机械性任务和有意识任务期间活跃的不同神经网络。学习新任务时，大脑扫描显示前额叶皮层和海马体的活跃，这些区域与决策和执行控制相关。随着任务重复，大脑活动转移到纹状体和基底节，即 Wood 称为“我们心灵的基础机制”的部分。在这里，任务转变为习惯。

这些更原始的大脑区域对我们的精神能量需求更少。一系列动作变得连贯，这个过程称为“分块 (chunking)”。例如，当我们上车准备开车时，我们不需要分别考虑系安全带、打火、挂挡、检查后视镜和盲区、踩油门等动作。所有这些步骤被整合在记忆中，形成一个单元，由上车这一环境线索触发。这使我们能够专注于那些最需要我们有意识关注的事情，比如思考目的地、规划当天的任务，同时留意路上的任何异常情况。

Wood 的研究起初并不是聚焦于习惯，而是坚持。对于像打流感疫苗这样的“偶发性、一次性行为”，有意识的决策就足够了。然而，对于需要重复的行为，习惯就显得至关重要。William James 曾估计，我们的活动中有“几乎全部是自动和习惯性的。”这只是一个推测；但 Wood 通过一项研究，量化了人们多少行为是出于习惯。她用一种名为“体验抽样”(experience sampling) 的技术，让参与者在两天内记录自己的所作所为。研究结果虽各组不同，但基本发现是我们大约 43% 的行为是习惯性的。

这就解释了为什么仅凭意识上的知识还不足以改变行为，以及为什么仅仅通过教育人们做出健康选择的公共卫生倡议往往会失败。1991 年，国家癌症研究所发现只有 8% 的美国人知道每天至少应该吃五份水果和蔬菜。随后发起了一项全国运动：“每天五份，为了更好的健康。”六年后，知道这一建议的美国人增至 39%，增长了近五倍，但实际饮食习惯几乎未变。2007 年，政府官员再次尝试，推出了“水果与蔬菜 - 更多益处”计划。然而，到 2018 年，每天吃两份水果的美国人仅占 12%，吃三份蔬菜的仅 9%。仅仅告知我们什么对我们有益是无效的，因为我们的饮食、烹饪和购物习惯主宰了我们的行为。

在 Mischel 的棉花糖实验中，只有四分之一的参与者能坚持十五分钟不吃棉花糖。这意味着大多数人缺乏成功所需的自控力。但研究中不太为人所知的一部分，提出了绕开我们脆弱的方法。研究者比较了两种情况：一种是孩子们能看到面前的棉花糖；另一种则是知道棉花糖在那儿，但看不到它。结果显示，面对可见诱惑时，孩子们平均只能坚持六分钟，但如果把诱惑藏起来，他们能坚持十分钟。对 Wood 而言，这说明自控力“并非内在品质，而是我们所处环境的反映。”通过微调环境，我们也许能够模仿那些看起来更有自制力的人。

一项研究调查了大学生的自控能力，结果支持了这一假设。研究要求学生每次想到“哎呀，我不应该这么做”的时候就报告，例如熬夜、睡懒觉、暴饮暴食或拖延。他们在养成有益行为时，最有效的方式并非下定决心做得更好，或是分散对诱惑的注意，而是改变自己的环境。他们选择不在带电视的寝室沙发上学习，而是去图书馆。他们还发现，清除寝室冰箱里的垃圾食品后，饮食变得更健康。伍德指出：“成功的自控，实际上来自于有效隐藏诱惑。”

即便是在自控问卷上得分高的人，他们看似的美德可能更多源于情境因素，而非单纯的意志力。在德国对这类人的一项研究中，他们很少报告自己抵抗诱惑的情况。“他们的生活方式几乎一直在隐藏诱惑，”伍德如是写道。这一观察引出了她书中论点的核心：摆脱坏习惯的关键不在于决心，而在于以支持良好行为的方式重新构建我们的环境。伍德引用了心理学家库尔特·莱文 (Kurt Lewin) 的理论，他认为行为受到类似于重力或使河流加速或减速的流体动力学的“一系列力量”的影响。这些力量取决于你所处的环境、周围的人、一天中的时间，以及你最近的行为。我们通过寻找方法从方程式中移除意志力，反而能够实现情境控制，这似乎有些矛盾，但并不是靠意志力实现的。

伍德认为，根除坏习惯的关键力量是“障碍”：如果我们能增加坏习惯的不便性，那么惯性就会帮助我们朝着美德方向前进，而无需我们表现出坚强。她列举了增加障碍导致吸烟减少的例子：法律禁止在餐厅、酒吧、飞机和火车上吸烟；税收的增加使得美国香烟价格在过去二十年里翻了三倍；以及从自动售货机中清除香烟，电视和收音机中禁播烟草广告。

与此同时，我们周边的企业都在努力减少消费者的操作障碍。比如麦当劳的收银员会有意诱导顾客，问道：“您要加薯条吗？”这样的提问促使我们摄入更多脂肪和碳水化合物。Netflix 或 Hulu 的连续剧播放功能也是如此，它们通过在一集结束后自动播放下一集的方式，让我们沉迷于连续观看。Wood 采访了 Uber 前经济研究主管 M. Keith Chen，后者分享了该应用是如何设计来减少用户操作步骤的。“手机的 GPS 已经知道你的位置，”他说，“你几乎不需要多想……下车时连现金都不用处理。”

公司成为我们习惯形成的帮手，这一现象在 Charles Duhigg 的畅销书《The Power of Habit》（2012 年）中被深入探讨。像 Wood 一样，Duhigg 当时是《时报》的记者，他指出快餐业通过各种方式诱使我们增加消费。例如，麦当劳统一餐厅的装潢风格，旨在触发我们的饮食习惯。许多连锁餐厅的食品都经过特别设计，可以迅速激发大脑的奖励中心，提供瞬间的盐分和脂肪享受。

在研究企业如何利用习惯形成来获取利益的过程中，Duhigg 描述了 20 世纪初的广告大师 Claude C. Hopkins 的工作。他的 Pepsodent 牙膏广告活动据说使刷牙成为美国人的日常习惯。Pepsodent 于 1915 年首次面市时，很少有人会刷牙，甚至当时的牙科权威都认为所有牙膏都无用。Hopkins 将营销焦点放在覆盖牙齿的牙菌斑 (plaque) 上；1917 年，他在报纸广告中宣称这是“所有牙齿问题的根源”。实际上，简单地吃一个苹果就能暂时去除牙菌斑，而当时的牙膏去除效果并不比光刷牙更好。尽管如此，Hopkins 还是夸大了牙菌斑的危害，并宣称 Pepsodent 是唯一的解决方案。“只需用舌头轻轻划过牙齿，”另一则广告写道，“你会感到一层薄膜——这就是让你的牙齿变色和蛀牙的原因。”不久，Pepsodent 就成为了全球知名的产品。

Duhigg 和 Wood 都认为，习惯性的行为是由刺激和回应所驱动的。虽然 Pepsodent 不是唯一声称能去除牙齿薄膜的品牌，但其含有的柠檬酸和薄荷油等成分，不仅带来清新口感，还轻微刺激口腔，产生舒适的刺痛感。Hopkins 让消费者意识到牙齿上的薄膜，为其提供了一个刺激，而牙膏本身则提供了物理上的回应。这种刺激与回应的循环极为强大：如果我们没有刷牙，就会感觉不舒服。Hopkins 发起他的活动二十年后，使用牙膏已成为美国绝大多数人的日常行为。Duhigg 表示，Hopkins“创造了一种需求”。

Wood 强调通过情境控制来培养良好习惯，而 Duhigg 则提到了一位咬指甲的女性，她被建议寻找其他可以用手做的事情，以产生类似的物理刺激，比如在桌子上敲打指关节。这样做的目的是保持刺激和回应的有效结构，但改变习惯的具体内容。对这两位作家来说，关键不在于靠意志力打破习惯，而是用一个新习惯替换旧习惯。

这两个案例都突出了有意识努力的重要性 - 不是去反抗习惯，而是去深入分析它，以此制定更有效的改革策略。Duhigg 在体重上升后，决定放弃在 Times 餐厅每天下午吃的那块饼干。他曾尝试在便签上写下不吃饼干的禁令，但这并无效果：他会忽略这个提醒，走到餐厅，与收银台的同事闲聊，最后还是会买下饼干。因此，他开始探究触发这一习惯的因素，参考研究人员提出的五个类别：时间、地点、情绪状态、周围人员、以及习惯性动作之前的行为。他是因为饥饿、无聊，还是需要休息或是血糖提升？他尝试改变自己的日常，选择在办公桌上吃甜甜圈，而非去餐厅，或者到外面散步。他在做实验：如果在办公桌上吃甜甜圈并未减少去餐厅的冲动，那就可以排除是因为糖分的原因。最终，他通过排除法确定，他的习惯实际上是由对交流和分散注意力的需求驱动的。于是，他发现与朋友聊天成了替代吃饼干的最佳选择。

Wood 在她的书的最后，给我们这些被智能手机控制的人提供了建议。她提出了一个分步骤的策略。首先，认识到自己对手机的依赖，意识到这种习惯如何干扰了工作、社交和安全驾驶。接着是“控制情境线索”，也就是找出是什么触发了你拿起手机的行为。对我而言，这些线索包括听觉（如通知声、法国号声）和视觉（如屏幕弹窗）。我已经知道，仅仅把手机调成静音是不足以打破这一习惯的，但正如“棉花糖耐性实验”所示，眼不见心不烦。早上做早餐时，我发现把手机放在另一个房间很有效。开车时，手机被放进手套箱。走路时，我会把手机放在带拉链的口袋里。还有其他方法来增加使用的难度，从而减少这一习惯的发生。完全关机比单纯静音更有效，continue 不是因为我不好奇谁可能给我发邮件，而是因为重新开机很麻烦。

Wood 还建议我们找到新的奖励来替代手机原本提供的那些奖励。我开始在车里听收音机音乐。晚上，我没有浏览推特和电子邮件，而是探索那些我从未阅读过的作者。每天结束时，我都感到更加平静和自由。♦

https://www.newyorker.com/magazine/2019/10/28/can-brain-science-help-us-break-bad-habits</title>
            <link>https://nitter.cz/dotey/status/1743791697304109452#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743791697304109452#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 00:28:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>纽约客2019年的一篇文章：《研究表明，依靠意志力改掉坏习惯是徒劳无功的》<br />
<br />
几年前，我购买了一部智能手机，很快就爱不释手。随时随地能发邮件、查信息或购物，这为我带来了前所未有的效率提升。手机每收到一封邮件就会发出“嘀”的一声，我随即处理它，为自己的效率沾沾自喜。短信到来伴随着法国号的声音，我也会迅速回复。不久，我开始条件反射般地一听到手机响就伸手去拿，就像帕夫洛夫的狗听到铃声就会流口水。这渐渐干扰了我的工作和谈话。这台本应是神奇助手的机器，却慢慢让我沦为了它的奴隶。<br />
<br />
我一直自认为意志力很强。像许多经历过医学训练的人一样，那些早起、长时间轮班，而朋友们却在享受派对的人，我已习惯于延迟满足。但这都没用。当我尝试把手机调成静音，我反而更频繁地检查它，生怕错过什么。唯一能控制自己不看手机的时候是安息日，因为那时我不查邮件。但我会不停地看表，计算着何时能再打开手机。那是我第一次真切感受到吸烟者对香烟的渴望。检查智能手机已成为我难以摆脱的坏习惯。<br />
<br />
习惯的好坏一直吸引着哲学家和政策制定者。亚里士多德在他的作品《尼各马可伦理学》中探讨了美德的不同理念，并总结道：“有人认为人之初性本善，有人说是习惯使然，还有人觉得教育至关重要。”他的结论是习惯扮演了关键角色。西塞罗将习惯称作“第二天性”，这个说法至今仍广为流传。亚历山大·汉密尔顿在《联邦党人文集》第 27 号文章中，当他思考如何培养出遵守新共和国联邦法律的公民时，他提到了“人是习惯的奴隶”。汉密尔顿认为，如果联邦法律深入州级事务，它将成为人们日常生活的一部分。“它越是融入人们激情自然流动的渠道，就越不需要强制手段的帮助，”他写道。<br />
<br />
在现代，习惯已成为科学研究的重要领域。心理学家深入研究了习惯行为的形成及其对健康和幸福的影响。威廉·詹姆斯回应亚里士多德的观点，写道：“我们的生活，只要形成了一定的模式，就是由各种习惯构成的——实用的、情感的、智力的……它们不可抗拒地推动我们走向命运。”<br />
<br />
我们大多数人不愿意把自己看作被动的存在。那意志力呢？市场营销通过像“Just Do It”（耐克）和“Declare Your Path”（新百伦）这样的口号，来抚慰我们对自主意识的渴望。很多流行心理学也强化了我们对自我控制的信念。在六十年代，沃尔特·米歇尔设计的著名斯坦福棉花糖实验中，孩子们要面对一块棉花糖，他们是否能抵抗住立即吃掉它的冲动，将决定他们的“执行功能”水平。这个实验被认为能预测孩子未来的成功，如 SAT 成绩、人际关系的持久度和职业成就。但如果我们只是习惯的产物，这一切又如何解释呢？<br />
<br />
在《好习惯，坏习惯》一书中（由 Farrar, Straus & Giroux 出版），社会心理学家 Wendy Wood 对 James 的决定论和轻率的自我激励论进行了反驳。她试图为普通读者提供更实际的方法来打破习惯。她基于自己的领域研究，认为维护积极行为和遏制消极行为需要决策与无意识因素的共同作用。Wood 解释说，我们的大脑有“多个独立但相互连接的行为指导机制。”但我们只能意识到决策能力——这种现象被称为“内省错觉”——这可能是我们过分高估了这种能力的原因。她写道，让意志力成为可能的执行功能给了我们一种“代理感”，让我们认为这就是“我”。但这是以付出努力为代价的。为了日常生活，我们需要某些行为变得自动化。<br />
<br />
功能性磁共振成像（fMRI）扫描让研究者能够窥探在机械性任务和有意识任务期间活跃的不同神经网络。学习新任务时，大脑扫描显示前额叶皮层和海马体的活跃，这些区域与决策和执行控制相关。随着任务重复，大脑活动转移到纹状体和基底节，即 Wood 称为“我们心灵的基础机制”的部分。在这里，任务转变为习惯。<br />
<br />
这些更原始的大脑区域对我们的精神能量需求更少。一系列动作变得连贯，这个过程称为“分块 (chunking)”。例如，当我们上车准备开车时，我们不需要分别考虑系安全带、打火、挂挡、检查后视镜和盲区、踩油门等动作。所有这些步骤被整合在记忆中，形成一个单元，由上车这一环境线索触发。这使我们能够专注于那些最需要我们有意识关注的事情，比如思考目的地、规划当天的任务，同时留意路上的任何异常情况。<br />
<br />
Wood 的研究起初并不是聚焦于习惯，而是坚持。对于像打流感疫苗这样的“偶发性、一次性行为”，有意识的决策就足够了。然而，对于需要重复的行为，习惯就显得至关重要。William James 曾估计，我们的活动中有“几乎全部是自动和习惯性的。”这只是一个推测；但 Wood 通过一项研究，量化了人们多少行为是出于习惯。她用一种名为“体验抽样”(experience sampling) 的技术，让参与者在两天内记录自己的所作所为。研究结果虽各组不同，但基本发现是我们大约 43% 的行为是习惯性的。<br />
<br />
这就解释了为什么仅凭意识上的知识还不足以改变行为，以及为什么仅仅通过教育人们做出健康选择的公共卫生倡议往往会失败。1991 年，国家癌症研究所发现只有 8% 的美国人知道每天至少应该吃五份水果和蔬菜。随后发起了一项全国运动：“每天五份，为了更好的健康。”六年后，知道这一建议的美国人增至 39%，增长了近五倍，但实际饮食习惯几乎未变。2007 年，政府官员再次尝试，推出了“水果与蔬菜 - 更多益处”计划。然而，到 2018 年，每天吃两份水果的美国人仅占 12%，吃三份蔬菜的仅 9%。仅仅告知我们什么对我们有益是无效的，因为我们的饮食、烹饪和购物习惯主宰了我们的行为。<br />
<br />
在 Mischel 的棉花糖实验中，只有四分之一的参与者能坚持十五分钟不吃棉花糖。这意味着大多数人缺乏成功所需的自控力。但研究中不太为人所知的一部分，提出了绕开我们脆弱的方法。研究者比较了两种情况：一种是孩子们能看到面前的棉花糖；另一种则是知道棉花糖在那儿，但看不到它。结果显示，面对可见诱惑时，孩子们平均只能坚持六分钟，但如果把诱惑藏起来，他们能坚持十分钟。对 Wood 而言，这说明自控力“并非内在品质，而是我们所处环境的反映。”通过微调环境，我们也许能够模仿那些看起来更有自制力的人。<br />
<br />
一项研究调查了大学生的自控能力，结果支持了这一假设。研究要求学生每次想到“哎呀，我不应该这么做”的时候就报告，例如熬夜、睡懒觉、暴饮暴食或拖延。他们在养成有益行为时，最有效的方式并非下定决心做得更好，或是分散对诱惑的注意，而是改变自己的环境。他们选择不在带电视的寝室沙发上学习，而是去图书馆。他们还发现，清除寝室冰箱里的垃圾食品后，饮食变得更健康。伍德指出：“成功的自控，实际上来自于有效隐藏诱惑。”<br />
<br />
即便是在自控问卷上得分高的人，他们看似的美德可能更多源于情境因素，而非单纯的意志力。在德国对这类人的一项研究中，他们很少报告自己抵抗诱惑的情况。“他们的生活方式几乎一直在隐藏诱惑，”伍德如是写道。这一观察引出了她书中论点的核心：摆脱坏习惯的关键不在于决心，而在于以支持良好行为的方式重新构建我们的环境。伍德引用了心理学家库尔特·莱文 (Kurt Lewin) 的理论，他认为行为受到类似于重力或使河流加速或减速的流体动力学的“一系列力量”的影响。这些力量取决于你所处的环境、周围的人、一天中的时间，以及你最近的行为。我们通过寻找方法从方程式中移除意志力，反而能够实现情境控制，这似乎有些矛盾，但并不是靠意志力实现的。<br />
<br />
伍德认为，根除坏习惯的关键力量是“障碍”：如果我们能增加坏习惯的不便性，那么惯性就会帮助我们朝着美德方向前进，而无需我们表现出坚强。她列举了增加障碍导致吸烟减少的例子：法律禁止在餐厅、酒吧、飞机和火车上吸烟；税收的增加使得美国香烟价格在过去二十年里翻了三倍；以及从自动售货机中清除香烟，电视和收音机中禁播烟草广告。<br />
<br />
与此同时，我们周边的企业都在努力减少消费者的操作障碍。比如麦当劳的收银员会有意诱导顾客，问道：“您要加薯条吗？”这样的提问促使我们摄入更多脂肪和碳水化合物。Netflix 或 Hulu 的连续剧播放功能也是如此，它们通过在一集结束后自动播放下一集的方式，让我们沉迷于连续观看。Wood 采访了 Uber 前经济研究主管 M. Keith Chen，后者分享了该应用是如何设计来减少用户操作步骤的。“手机的 GPS 已经知道你的位置，”他说，“你几乎不需要多想……下车时连现金都不用处理。”<br />
<br />
公司成为我们习惯形成的帮手，这一现象在 Charles Duhigg 的畅销书《The Power of Habit》（2012 年）中被深入探讨。像 Wood 一样，Duhigg 当时是《时报》的记者，他指出快餐业通过各种方式诱使我们增加消费。例如，麦当劳统一餐厅的装潢风格，旨在触发我们的饮食习惯。许多连锁餐厅的食品都经过特别设计，可以迅速激发大脑的奖励中心，提供瞬间的盐分和脂肪享受。<br />
<br />
在研究企业如何利用习惯形成来获取利益的过程中，Duhigg 描述了 20 世纪初的广告大师 Claude C. Hopkins 的工作。他的 Pepsodent 牙膏广告活动据说使刷牙成为美国人的日常习惯。Pepsodent 于 1915 年首次面市时，很少有人会刷牙，甚至当时的牙科权威都认为所有牙膏都无用。Hopkins 将营销焦点放在覆盖牙齿的牙菌斑 (plaque) 上；1917 年，他在报纸广告中宣称这是“所有牙齿问题的根源”。实际上，简单地吃一个苹果就能暂时去除牙菌斑，而当时的牙膏去除效果并不比光刷牙更好。尽管如此，Hopkins 还是夸大了牙菌斑的危害，并宣称 Pepsodent 是唯一的解决方案。“只需用舌头轻轻划过牙齿，”另一则广告写道，“你会感到一层薄膜——这就是让你的牙齿变色和蛀牙的原因。”不久，Pepsodent 就成为了全球知名的产品。<br />
<br />
Duhigg 和 Wood 都认为，习惯性的行为是由刺激和回应所驱动的。虽然 Pepsodent 不是唯一声称能去除牙齿薄膜的品牌，但其含有的柠檬酸和薄荷油等成分，不仅带来清新口感，还轻微刺激口腔，产生舒适的刺痛感。Hopkins 让消费者意识到牙齿上的薄膜，为其提供了一个刺激，而牙膏本身则提供了物理上的回应。这种刺激与回应的循环极为强大：如果我们没有刷牙，就会感觉不舒服。Hopkins 发起他的活动二十年后，使用牙膏已成为美国绝大多数人的日常行为。Duhigg 表示，Hopkins“创造了一种需求”。<br />
<br />
Wood 强调通过情境控制来培养良好习惯，而 Duhigg 则提到了一位咬指甲的女性，她被建议寻找其他可以用手做的事情，以产生类似的物理刺激，比如在桌子上敲打指关节。这样做的目的是保持刺激和回应的有效结构，但改变习惯的具体内容。对这两位作家来说，关键不在于靠意志力打破习惯，而是用一个新习惯替换旧习惯。<br />
<br />
这两个案例都突出了有意识努力的重要性 - 不是去反抗习惯，而是去深入分析它，以此制定更有效的改革策略。Duhigg 在体重上升后，决定放弃在 Times 餐厅每天下午吃的那块饼干。他曾尝试在便签上写下不吃饼干的禁令，但这并无效果：他会忽略这个提醒，走到餐厅，与收银台的同事闲聊，最后还是会买下饼干。因此，他开始探究触发这一习惯的因素，参考研究人员提出的五个类别：时间、地点、情绪状态、周围人员、以及习惯性动作之前的行为。他是因为饥饿、无聊，还是需要休息或是血糖提升？他尝试改变自己的日常，选择在办公桌上吃甜甜圈，而非去餐厅，或者到外面散步。他在做实验：如果在办公桌上吃甜甜圈并未减少去餐厅的冲动，那就可以排除是因为糖分的原因。最终，他通过排除法确定，他的习惯实际上是由对交流和分散注意力的需求驱动的。于是，他发现与朋友聊天成了替代吃饼干的最佳选择。<br />
<br />
Wood 在她的书的最后，给我们这些被智能手机控制的人提供了建议。她提出了一个分步骤的策略。首先，认识到自己对手机的依赖，意识到这种习惯如何干扰了工作、社交和安全驾驶。接着是“控制情境线索”，也就是找出是什么触发了你拿起手机的行为。对我而言，这些线索包括听觉（如通知声、法国号声）和视觉（如屏幕弹窗）。我已经知道，仅仅把手机调成静音是不足以打破这一习惯的，但正如“棉花糖耐性实验”所示，眼不见心不烦。早上做早餐时，我发现把手机放在另一个房间很有效。开车时，手机被放进手套箱。走路时，我会把手机放在带拉链的口袋里。还有其他方法来增加使用的难度，从而减少这一习惯的发生。完全关机比单纯静音更有效，continue 不是因为我不好奇谁可能给我发邮件，而是因为重新开机很麻烦。<br />
<br />
Wood 还建议我们找到新的奖励来替代手机原本提供的那些奖励。我开始在车里听收音机音乐。晚上，我没有浏览推特和电子邮件，而是探索那些我从未阅读过的作者。每天结束时，我都感到更加平静和自由。♦<br />
<br />
<a href="https://www.newyorker.com/magazine/2019/10/28/can-brain-science-help-us-break-bad-habits">newyorker.com/magazine/2019/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RNeEphSFhnQUFFU2ZfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/mtrainier2020/status/1743790396541706588#m</id>
            <title>RT by @dotey: 发一个招聘广告：

1. 工作地点：西雅图地区
2. 工作要求：
A. 熟悉日本的二次元文化以及美国的流行文化。
B. 有一定的social media 运营经验。
C. 有良好的英文口头和邮件沟通能力。
D. 有在美国的学习经历，熟悉美国文化。
3. 工作内容：
     1. 沟通，服务 TikTok，Ins kol
     2. 运营公司的社媒账号。
有运营reddit sub 经验者优先。

4. 可以sponsor H1B
5. 薪资报酬面议。

有意者请私信。</title>
            <link>https://nitter.cz/mtrainier2020/status/1743790396541706588#m</link>
            <guid isPermaLink="false">https://nitter.cz/mtrainier2020/status/1743790396541706588#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 00:23:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发一个招聘广告：<br />
<br />
1. 工作地点：西雅图地区<br />
2. 工作要求：<br />
A. 熟悉日本的二次元文化以及美国的流行文化。<br />
B. 有一定的social media 运营经验。<br />
C. 有良好的英文口头和邮件沟通能力。<br />
D. 有在美国的学习经历，熟悉美国文化。<br />
3. 工作内容：<br />
     1. 沟通，服务 TikTok，Ins kol<br />
     2. 运营公司的社媒账号。<br />
有运营reddit sub 经验者优先。<br />
<br />
4. 可以sponsor H1B<br />
5. 薪资报酬面议。<br />
<br />
有意者请私信。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743786495113048099#m</id>
            <title>推荐阅读：《Chess-GPT's Internal World Model》，作者从头训练了一个只有 50M 参数的小语言模型“国际象棋 GPT”，专门下国际象棋。一款拥有 5000 万参数的 GPT 模型，在 4 块 RTX 3090 显卡上训练了一天后，通过分析 500 万局国际象棋对局，达到了约 1300 ELO 的棋力。

最初作者是打算基于开源模型如 LLama 7B 或 OpenLlama 3B 进行微调，但是他只有 RTX 3090的显卡，所以基于  nanogpt 重头训练了一个5000万参数的模型。

训练的数据是1600 万局游戏记录，所有的数据和模型都是开源的。

相关博文：https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html
译文：https://baoyu.io/translations/gpt/chess-gpts-internal-world-model</title>
            <link>https://nitter.cz/dotey/status/1743786495113048099#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743786495113048099#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 00:07:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐阅读：《Chess-GPT's Internal World Model》，作者从头训练了一个只有 50M 参数的小语言模型“国际象棋 GPT”，专门下国际象棋。一款拥有 5000 万参数的 GPT 模型，在 4 块 RTX 3090 显卡上训练了一天后，通过分析 500 万局国际象棋对局，达到了约 1300 ELO 的棋力。<br />
<br />
最初作者是打算基于开源模型如 LLama 7B 或 OpenLlama 3B 进行微调，但是他只有 RTX 3090的显卡，所以基于  nanogpt 重头训练了一个5000万参数的模型。<br />
<br />
训练的数据是1600 万局游戏记录，所有的数据和模型都是开源的。<br />
<br />
相关博文：<a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html">adamkarvonen.github.io/machi…</a><br />
译文：<a href="https://baoyu.io/translations/gpt/chess-gpts-internal-world-model">baoyu.io/translations/gpt/ch…</a></p>
<p><a href="https://nitter.cz/a_karvonen/status/1743666230127411389#m">nitter.cz/a_karvonen/status/1743666230127411389#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743781800701214789#m</id>
            <title>今年，美国国内税务局将向十二个州的部分居民推出免费的联邦纳税申报选项。

试点包括：亚利桑那州、加利福尼亚州、佛罗里达州、马萨诸塞州、内华达州、新罕布什尔州、纽约州、南达科他州、田纳西州、德克萨斯州、华盛顿州和怀俄明州。

https://www.nytimes.com/2024/01/05/your-money/irs-tax-filing-free-online.html</title>
            <link>https://nitter.cz/dotey/status/1743781800701214789#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743781800701214789#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jan 2024 23:49:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今年，美国国内税务局将向十二个州的部分居民推出免费的联邦纳税申报选项。<br />
<br />
试点包括：亚利桑那州、加利福尼亚州、佛罗里达州、马萨诸塞州、内华达州、新罕布什尔州、纽约州、南达科他州、田纳西州、德克萨斯州、华盛顿州和怀俄明州。<br />
<br />
<a href="https://www.nytimes.com/2024/01/05/your-money/irs-tax-filing-free-online.html">nytimes.com/2024/01/05/your-…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MzI3NzEwNDUxNzU0NTk4NC9iaGZDc2tWXz9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743776999926042972#m</id>
            <title>Google 撰写“机器人宪法”，确保其 AI 机器人不伤人

AutoRT 数据收集系统融入了艾萨克·阿西莫夫《机器人三定律》的安全理念。

DeepMind 机器人团队最近公布了三项重大进展，这些进展有望让机器人在野外做出更迅速、更出色且更安全的决策。这其中包括一个用于收集训练数据的系统和一份“机器人宪法”，旨在确保机器人办公助理能够安全地帮你补充打印纸，同时避免伤害人类。

Google 的 AutoRT 数据收集系统采用视觉语言模型（VLM）和大语言模型（LLM）协同工作，能够理解环境、适应新环境，并选择合适的任务。这份灵感源自于艾萨克·阿西莫夫的《机器人三定律》，“机器人宪法”是一组专注于安全的指导原则，指导机器人避免执行涉及人类、动物、尖锐物品或电器的任务。

为了更高的安全性，DeepMind 对机器人进行了编程，一旦关节上的力超过一定阈值，它们就会自动停止，并增加了一个物理紧急停止开关，供人类操作者使用。在七个月的时间里，Google 在四个不同的办公楼部署了 53 台 AutoRT 机器人，进行了超过 77,000 次的试验。这些机器人有的由人类操作员远程控制，有的则是根据预设脚本或完全自主地运用 Google 的机器人 Transformer (RT-2) AI 学习模型进行操作。

试验中使用的机器人设计注重实用性而非外观华丽，它们配备了相机、机械臂和移动基座。Google 在博客文章中提到：“系统通过 VLM 来理解机器人所处的环境和其视野内的物体。接着，LLM 会提出一系列可能的任务，如‘把小吃放到台面上’，并决定机器人应执行哪个任务，”。

DeepMind 的其他新技术包括 SARA-RT，这是一种神经网络架构，旨在提高现有机器人变压器 RT-2 的准确性和速度。此外，他们还推出了 RT-Trajectory，通过添加 2D 轮廓，帮助机器人更精准地完成特定的物理任务，例如擦拭桌面。

尽管我们距离能够自主提供饮料和整理枕头的机器人还有很长的路要走，但当这些机器人面市时，它们可能已经通过像 AutoRT 这样的系统学习并成长。

来源：https://www.theverge.com/2024/1/4/24025535/google-ai-robot-constitution-autort-deepmind-three-laws</title>
            <link>https://nitter.cz/dotey/status/1743776999926042972#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743776999926042972#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jan 2024 23:30:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 撰写“机器人宪法”，确保其 AI 机器人不伤人<br />
<br />
AutoRT 数据收集系统融入了艾萨克·阿西莫夫《机器人三定律》的安全理念。<br />
<br />
DeepMind 机器人团队最近公布了三项重大进展，这些进展有望让机器人在野外做出更迅速、更出色且更安全的决策。这其中包括一个用于收集训练数据的系统和一份“机器人宪法”，旨在确保机器人办公助理能够安全地帮你补充打印纸，同时避免伤害人类。<br />
<br />
Google 的 AutoRT 数据收集系统采用视觉语言模型（VLM）和大语言模型（LLM）协同工作，能够理解环境、适应新环境，并选择合适的任务。这份灵感源自于艾萨克·阿西莫夫的《机器人三定律》，“机器人宪法”是一组专注于安全的指导原则，指导机器人避免执行涉及人类、动物、尖锐物品或电器的任务。<br />
<br />
为了更高的安全性，DeepMind 对机器人进行了编程，一旦关节上的力超过一定阈值，它们就会自动停止，并增加了一个物理紧急停止开关，供人类操作者使用。在七个月的时间里，Google 在四个不同的办公楼部署了 53 台 AutoRT 机器人，进行了超过 77,000 次的试验。这些机器人有的由人类操作员远程控制，有的则是根据预设脚本或完全自主地运用 Google 的机器人 Transformer (RT-2) AI 学习模型进行操作。<br />
<br />
试验中使用的机器人设计注重实用性而非外观华丽，它们配备了相机、机械臂和移动基座。Google 在博客文章中提到：“系统通过 VLM 来理解机器人所处的环境和其视野内的物体。接着，LLM 会提出一系列可能的任务，如‘把小吃放到台面上’，并决定机器人应执行哪个任务，”。<br />
<br />
DeepMind 的其他新技术包括 SARA-RT，这是一种神经网络架构，旨在提高现有机器人变压器 RT-2 的准确性和速度。此外，他们还推出了 RT-Trajectory，通过添加 2D 轮廓，帮助机器人更精准地完成特定的物理任务，例如擦拭桌面。<br />
<br />
尽管我们距离能够自主提供饮料和整理枕头的机器人还有很长的路要走，但当这些机器人面市时，它们可能已经通过像 AutoRT 这样的系统学习并成长。<br />
<br />
来源：<a href="https://www.theverge.com/2024/1/4/24025535/google-ai-robot-constitution-autort-deepmind-three-laws">theverge.com/2024/1/4/240255…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RNanljTFdBQUFMT1Y0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1743745935451070472#m</id>
            <title>我在日常用 GPT-4 翻译的时候，就会发现有时候 GPT 能给出很不错的质量的翻译，但有时候质量一般甚至比较差，所以我也尝试过一次生成多个翻译结果，然后从里面人工挑选最好的翻译，但是我当时想做而没有做到的是：

如何让 GPT 从里面帮我挑选一个最好的，或者将几个的优点结合组合成一个最好的结果。

昨天读了一篇来自浙大论文《Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives》（作者之一 @spicysweet1859 ）

提到的方案相对就很系统了：

先让 LLM 根据请求生成几个不同风格的 prompt ,  然后基于每一条 prompt 得到一个不同结果，然后对比这些结果之间的差异，基于这些差异总结出更有针对性的检查指令，用于反思，最后综合这些信息生成最终结果。

之所以不是直接一个Prompt生成多个结果，而是多个Prompt生成多个结果，是为了让生成的结果更多样，否则一个Prompt中的结果可能多样性不够，当然缺点是要费Token一些。但从性能上来说可能更好，因为可以并行生成。

另外对于特定任务其实不需要让LLM生成Prompt，应该人工生成Prompt效果也是一样的。

在跟作者沟通后了解到，这个方案最适合的其实不是翻译，而是推理：

> 我们这种 通过对比不同视角的responses之间的差异来启发反思的策略 对推理任务更有效果一些。 对于像翻译这类生成任务，我们实验发现其实也不需要多个视角，一个负面面视角和一个正面视角其实应该够用。这点宝玉老师您应该也介绍过。 by @spicysweet1859

推荐阅读：
https://browse.arxiv.org/html/2401.02009v1</title>
            <link>https://nitter.cz/dotey/status/1743745935451070472#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1743745935451070472#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jan 2024 21:26:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我在日常用 GPT-4 翻译的时候，就会发现有时候 GPT 能给出很不错的质量的翻译，但有时候质量一般甚至比较差，所以我也尝试过一次生成多个翻译结果，然后从里面人工挑选最好的翻译，但是我当时想做而没有做到的是：<br />
<br />
如何让 GPT 从里面帮我挑选一个最好的，或者将几个的优点结合组合成一个最好的结果。<br />
<br />
昨天读了一篇来自浙大论文《Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives》（作者之一 <a href="https://nitter.cz/spicysweet1859" title="spicysweet">@spicysweet1859</a> ）<br />
<br />
提到的方案相对就很系统了：<br />
<br />
先让 LLM 根据请求生成几个不同风格的 prompt ,  然后基于每一条 prompt 得到一个不同结果，然后对比这些结果之间的差异，基于这些差异总结出更有针对性的检查指令，用于反思，最后综合这些信息生成最终结果。<br />
<br />
之所以不是直接一个Prompt生成多个结果，而是多个Prompt生成多个结果，是为了让生成的结果更多样，否则一个Prompt中的结果可能多样性不够，当然缺点是要费Token一些。但从性能上来说可能更好，因为可以并行生成。<br />
<br />
另外对于特定任务其实不需要让LLM生成Prompt，应该人工生成Prompt效果也是一样的。<br />
<br />
在跟作者沟通后了解到，这个方案最适合的其实不是翻译，而是推理：<br />
<br />
> 我们这种 通过对比不同视角的responses之间的差异来启发反思的策略 对推理任务更有效果一些。 对于像翻译这类生成任务，我们实验发现其实也不需要多个视角，一个负面面视角和一个正面视角其实应该够用。这点宝玉老师您应该也介绍过。 by <a href="https://nitter.cz/spicysweet1859" title="spicysweet">@spicysweet1859</a><br />
<br />
推荐阅读：<br />
<a href="https://browse.arxiv.org/html/2401.02009v1">browse.arxiv.org/html/2401.0…</a></p>
<p><a href="https://nitter.cz/dotey/status/1705441050133713202#m">nitter.cz/dotey/status/1705441050133713202#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RNR1pNdVhzQUFCZ3c2LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RNSHJGMFdzQUVHd2ZPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/fuxiangPro/status/1743665779814654039#m</id>
            <title>RT by @dotey: 查理芒格说李光耀是他的学习榜样。他可能是古往今来最优秀的领导者之一，用50多年的时间将新加坡从一个沼泽般的第三世界国家拉升为第一世界国家。这是个奇迹。

翻阅李光耀的访谈和资料，整理了10条李光耀对国家治理和国际环境的观点，很硬核：

1， 新加坡的双语之路是我一生的挑战。在新加坡，孩子们先学汉语，然后学英语。他们可能十几岁就去美国了，能说一口流利的英语，但他们的头脑里仍流淌着4 000年的汉语名言警句。与中国相比，美国的优势非常明显，因为它使用的是英语，这就使得美国能够从亚洲和欧洲吸引数以百万计的掌握英语的外国人才。
今天，英语能力是一种竞争优势，所以很多国家都在努力让孩子们学习英语。在21世纪，如果一个人想成功，就要掌握英语，因为这是一门在国际舞台上从事商业、科学、外交和学术活动时通用的语言。

2，中国的国内生产总值的绝对额将不可避免地赶上美国，但其创新能力可能永远无法与美国匹敌，因为它的文化不鼓励进行思想的自由交流和碰撞。不然如何解释一个人口4倍于美国的国家（可能中国人才的数量也是美国的4倍）却少有技术突破呢？

3，新加坡从第三世界国家跻身第一世界国家之列，靠的不是物色那些愿意在担任公职期间牺牲子女未来的部长。我们的方法很务实，不需要高素质人才为了公共利益放弃太多个人利益。新加坡的部长们待遇很高，我们要敢于直面这一点，不能为了回避外界对高薪的质疑而降低人才的待遇，那样做只会让新加坡重返第三世界。美国或英国的政治制度认为人都会为自己的国家着想。实际上呢？你真的相信那些连小学都没毕业的人明白自己的抉择引发的后果吗？但我们知道这些后果，我们将会挨饿，我们将会爆发种族骚乱，我们将会解体。
要治理好一个国家，最佳方法就是让最优秀的人做难度最大的工作。

4，我不希望新加坡人效仿美国人心安理得地依赖救济过日子，而是希望新加坡人学习美国的自强文化。这种文化特质使美国诞生了很多伟大的企业家，他们有魄力、有活力、有勇气创立和调整他们的企业，因此也就改变了美国经济，在这一点上，美国人比欧洲人和日本人做得好。

5，如果一个国家的领导人坚持维护社会秩序、给人民提供教育、维持睦邻友好关系、厉行法治、增强投资者的信心，那么国家没有道理不发展。

6，俄罗斯人口正在减少，具体为什么不清楚，但酗酒肯定对此有影响，消极情绪、生育率下降及预期寿命缩短也有影响。普京面临的挑战是让俄罗斯人对未来充满信心：停止酗酒、努力工作、建立幸福的家庭，并生育更多的子女。

7，全球化不可逆转，因为推进全球化进程的技术已经出现，这些技术是不会消失的。其实，更好的、更廉价的交通和通信将进一步增强推动全球化的力量。

8，我认为，我们说一个政府受欢迎并不是说它要在治理期间的任何时刻都受欢迎……有时你必须彻底不受欢迎。但在你的任期结束时，你应该给人民带来福利，这样人民才会认识到你所做的事情都是有必要的，才会再一次投你的票。这是我治理的基础。如果你想一直都受欢迎，那么你在治理时就会出现失误。

9，人的思想不只来自阅读，你可以从书本中获取，但如果你不把书本知识同自己的情况结合起来，书本知识就无用武之地。我自己经常会把读到的东西同自身情况结合起来……同博学多才的人展开讨论具有重要的意义，这一点一定不要忽略，我认为这比单纯孜孜不倦地阅读文献强得多。因为通过短暂的交流，你就能萃取对方的知识和对方的思想精华。

10，之所以会出现文明，是因为人类社会在一定条件下会应对挑战。哪里充满挑战，哪里就能兴旺发达。</title>
            <link>https://nitter.cz/fuxiangPro/status/1743665779814654039#m</link>
            <guid isPermaLink="false">https://nitter.cz/fuxiangPro/status/1743665779814654039#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jan 2024 16:08:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>查理芒格说李光耀是他的学习榜样。他可能是古往今来最优秀的领导者之一，用50多年的时间将新加坡从一个沼泽般的第三世界国家拉升为第一世界国家。这是个奇迹。<br />
<br />
翻阅李光耀的访谈和资料，整理了10条李光耀对国家治理和国际环境的观点，很硬核：<br />
<br />
1， 新加坡的双语之路是我一生的挑战。在新加坡，孩子们先学汉语，然后学英语。他们可能十几岁就去美国了，能说一口流利的英语，但他们的头脑里仍流淌着4 000年的汉语名言警句。与中国相比，美国的优势非常明显，因为它使用的是英语，这就使得美国能够从亚洲和欧洲吸引数以百万计的掌握英语的外国人才。<br />
今天，英语能力是一种竞争优势，所以很多国家都在努力让孩子们学习英语。在21世纪，如果一个人想成功，就要掌握英语，因为这是一门在国际舞台上从事商业、科学、外交和学术活动时通用的语言。<br />
<br />
2，中国的国内生产总值的绝对额将不可避免地赶上美国，但其创新能力可能永远无法与美国匹敌，因为它的文化不鼓励进行思想的自由交流和碰撞。不然如何解释一个人口4倍于美国的国家（可能中国人才的数量也是美国的4倍）却少有技术突破呢？<br />
<br />
3，新加坡从第三世界国家跻身第一世界国家之列，靠的不是物色那些愿意在担任公职期间牺牲子女未来的部长。我们的方法很务实，不需要高素质人才为了公共利益放弃太多个人利益。新加坡的部长们待遇很高，我们要敢于直面这一点，不能为了回避外界对高薪的质疑而降低人才的待遇，那样做只会让新加坡重返第三世界。美国或英国的政治制度认为人都会为自己的国家着想。实际上呢？你真的相信那些连小学都没毕业的人明白自己的抉择引发的后果吗？但我们知道这些后果，我们将会挨饿，我们将会爆发种族骚乱，我们将会解体。<br />
要治理好一个国家，最佳方法就是让最优秀的人做难度最大的工作。<br />
<br />
4，我不希望新加坡人效仿美国人心安理得地依赖救济过日子，而是希望新加坡人学习美国的自强文化。这种文化特质使美国诞生了很多伟大的企业家，他们有魄力、有活力、有勇气创立和调整他们的企业，因此也就改变了美国经济，在这一点上，美国人比欧洲人和日本人做得好。<br />
<br />
5，如果一个国家的领导人坚持维护社会秩序、给人民提供教育、维持睦邻友好关系、厉行法治、增强投资者的信心，那么国家没有道理不发展。<br />
<br />
6，俄罗斯人口正在减少，具体为什么不清楚，但酗酒肯定对此有影响，消极情绪、生育率下降及预期寿命缩短也有影响。普京面临的挑战是让俄罗斯人对未来充满信心：停止酗酒、努力工作、建立幸福的家庭，并生育更多的子女。<br />
<br />
7，全球化不可逆转，因为推进全球化进程的技术已经出现，这些技术是不会消失的。其实，更好的、更廉价的交通和通信将进一步增强推动全球化的力量。<br />
<br />
8，我认为，我们说一个政府受欢迎并不是说它要在治理期间的任何时刻都受欢迎……有时你必须彻底不受欢迎。但在你的任期结束时，你应该给人民带来福利，这样人民才会认识到你所做的事情都是有必要的，才会再一次投你的票。这是我治理的基础。如果你想一直都受欢迎，那么你在治理时就会出现失误。<br />
<br />
9，人的思想不只来自阅读，你可以从书本中获取，但如果你不把书本知识同自己的情况结合起来，书本知识就无用武之地。我自己经常会把读到的东西同自身情况结合起来……同博学多才的人展开讨论具有重要的意义，这一点一定不要忽略，我认为这比单纯孜孜不倦地阅读文献强得多。因为通过短暂的交流，你就能萃取对方的知识和对方的思想精华。<br />
<br />
10，之所以会出现文明，是因为人类社会在一定条件下会应对挑战。哪里充满挑战，哪里就能兴旺发达。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RLOFpXbWJvQUFPdTRfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>