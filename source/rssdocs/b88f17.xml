<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>宝玉 / @dotey</title>
        <link>https://nitter.cz/dotey</link>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733622056153972793#m</id>
            <title>R to @dotey: xAI的Grok系统在设计时就考虑到了幽默感</title>
            <link>https://nitter.cz/dotey/status/1733622056153972793#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733622056153972793#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 22:58:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>xAI的Grok系统在设计时就考虑到了幽默感</p>
<p><a href="https://nitter.cz/elonmusk/status/1720635518289908042#m">nitter.cz/elonmusk/status/1720635518289908042#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733621976411881735#m</id>
            <title>转译自TheVerge的一篇文章：马斯克的 Grok 聊天机器人为何缺乏幽默感？

那好，我们来讨论一下 xAI，这个项目已经获得了高达 10亿美元的投资。

根据一些评论家的看法，xAI是 埃隆·马斯克为“拯救X”（即Twitter）所做的尝试。马斯克可能在吸引广告商这方面失败了，也未能通过增加订阅来弥补这一缺口。但他似乎能够利用一款新的 AI 产品的炒作来筹集资金。这款产品名为Grok，据说是一个具有幽默感的类似ChatGPT的回答机器人。这引起了很多问题，尤其是当 AI 聊天机器人仍然是一个盈利前景不明的资金黑洞时。但最让人好奇的是：为什么Grok这么缺乏幽默感？

xAI的网站上的介绍让人觉得Grok的推出有些尴尬：“Grok旨在带着点智慧和叛逆精神回答问题，所以如果你不喜欢幽默，请别使用它！”从一开始，就给人一种监管者的感觉。

通常，我并不指望工程师有意制造幽默（但还是要感谢他们的付出）。我更看重的是他们的实用性。然而，问题在于，Grok的主打卖点就是幽默。除了一些关于xAI如何利用推文进行训练的闲谈，马斯克承诺的是，Grok比其他现有的、功能更全面、价格更便宜的产品更“酷”和“有趣”。好吧，我们来看看马斯克觉得什么是真正的笑料。

我翻看了马斯克的Twitter历史，寻找他发布或转发的Grok的回答。我认为马斯克会突出他认为特别出色的回答，作为宣传这项服务的手段。毕竟，在他拥有Twitter之前，他的推文就已经是特斯拉极其重要的宣传工具。那么，Grok的推广效果如何？</title>
            <link>https://nitter.cz/dotey/status/1733621976411881735#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733621976411881735#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 22:57:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转译自TheVerge的一篇文章：马斯克的 Grok 聊天机器人为何缺乏幽默感？<br />
<br />
那好，我们来讨论一下 xAI，这个项目已经获得了高达 10亿美元的投资。<br />
<br />
根据一些评论家的看法，xAI是 埃隆·马斯克为“拯救X”（即Twitter）所做的尝试。马斯克可能在吸引广告商这方面失败了，也未能通过增加订阅来弥补这一缺口。但他似乎能够利用一款新的 AI 产品的炒作来筹集资金。这款产品名为Grok，据说是一个具有幽默感的类似ChatGPT的回答机器人。这引起了很多问题，尤其是当 AI 聊天机器人仍然是一个盈利前景不明的资金黑洞时。但最让人好奇的是：为什么Grok这么缺乏幽默感？<br />
<br />
xAI的网站上的介绍让人觉得Grok的推出有些尴尬：“Grok旨在带着点智慧和叛逆精神回答问题，所以如果你不喜欢幽默，请别使用它！”从一开始，就给人一种监管者的感觉。<br />
<br />
通常，我并不指望工程师有意制造幽默（但还是要感谢他们的付出）。我更看重的是他们的实用性。然而，问题在于，Grok的主打卖点就是幽默。除了一些关于xAI如何利用推文进行训练的闲谈，马斯克承诺的是，Grok比其他现有的、功能更全面、价格更便宜的产品更“酷”和“有趣”。好吧，我们来看看马斯克觉得什么是真正的笑料。<br />
<br />
我翻看了马斯克的Twitter历史，寻找他发布或转发的Grok的回答。我认为马斯克会突出他认为特别出色的回答，作为宣传这项服务的手段。毕竟，在他拥有Twitter之前，他的推文就已经是特斯拉极其重要的宣传工具。那么，Grok的推广效果如何？</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733478737017237910#m</id>
            <title>RT by @dotey: Twitter正式推出了Expanded Bios功能，你可以理解为更详细的个人简介。比如可以放你更多媒体的链接以及你的详细简历，我把我之前写的成体系的教程和做的东西放进去了，感兴趣可以来看看。
点击简介下面的查看更多就可以看到，或者可以点击下面的链接。
创建Expanded Bios的话可以点击编辑个人资料按钮拉到最下面编辑扩展简介就可以了。赶紧设置一下自己的吧。

我的Expanded Bios：https://twitter.com/op7418/bio</title>
            <link>https://nitter.cz/op7418/status/1733478737017237910#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733478737017237910#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:28:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Twitter正式推出了Expanded Bios功能，你可以理解为更详细的个人简介。比如可以放你更多媒体的链接以及你的详细简历，我把我之前写的成体系的教程和做的东西放进去了，感兴趣可以来看看。<br />
点击简介下面的查看更多就可以看到，或者可以点击下面的链接。<br />
创建Expanded Bios的话可以点击编辑个人资料按钮拉到最下面编辑扩展简介就可以了。赶紧设置一下自己的吧。<br />
<br />
我的Expanded Bios：<a href="https://nitter.cz/op7418/bio">nitter.cz/op7418/bio</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E2TnR0c2JzQUE3RTdhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733482321368752182#m</id>
            <title>RT by @dotey: Mixtral-8x7b更详细的一些信息：
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。
◆这些专家模型都采用了 Mistral-7B 的架构。</title>
            <link>https://nitter.cz/op7418/status/1733482321368752182#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733482321368752182#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 13:42:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral-8x7b更详细的一些信息：<br />
◆当前只提供了状态字典，没有相应代码，所以现在还无法执行。<br />
◆从状态字典来看，这个模型采用了专家混合（MoE）的方法，每次运算过程中会有 2 个专家模型参与，总共有 8 个专家模型。<br />
◆这些专家模型都采用了 Mistral-7B 的架构。</p>
<p><a href="https://nitter.cz/carrigmat/status/1733159362028257353#m">nitter.cz/carrigmat/status/1733159362028257353#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733609848472772900#m</id>
            <title>R to @dotey: 视频来源：https://www.wsj.com/video/robot-dogs-and-ai-is-this-the-future-of-ev-factories/8023CE2D-A13E-404F-8C3C-BA461B356F82</title>
            <link>https://nitter.cz/dotey/status/1733609848472772900#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733609848472772900#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 22:09:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频来源：<a href="https://www.wsj.com/video/robot-dogs-and-ai-is-this-the-future-of-ev-factories/8023CE2D-A13E-404F-8C3C-BA461B356F82">wsj.com/video/robot-dogs-and…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzA4NjYwMzA2NzcwMzI5Ny9mNVM0Qmd2NT9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733609163136069994#m</id>
            <title>新加坡微型工厂：汽车制造业的未来

在新加坡，一家看似普通的汽车工厂实际上是领先科技的产物：一个由自主机器狗和 AI 监控的机械臂运营的全新微型工厂。这个工厂旨在改变汽车的生产方式，它位于全球最小和最昂贵的国家之一。

在这里，与传统汽车制造商在密集城市建厂的常规不同，机器人负责大部分的制造工作，采用更加灵活的模式。微型工厂通过自动化大部分生产流程，并在单独的单元中而不是沿传统生产线组装汽车，彻底改变了汽车的制造方式。

这种微型工厂模型可能是我们未来几年将广泛讨论的趋势。作为汽车制造业数十年来最重大的变革之一，这一模式若被验证有效，将可能塑造整个行业的未来。在这样的工厂中，传统的汽车装配流程被重新设计，以适应更小的空间。机器人在完成汽车的最后装饰工作，这在传统的制造过程中大多由人手完成。

在传统汽车工厂中，汽车沿着线性路径移动，并在装配线上安装零件。虽然这个程序越来越自动化，但其基本流程并未发生重大改变。然而，在这家新型工厂中，情况大为不同。这里采用的是基于单元的模型，工厂地面分布着不同的单元，每个单元专注于不同的任务，这大大增加了自动化的可能性。

每个单元里的机器人被安排安装汽车的几个部分，然后汽车移动到下一个区域。单元可以根据不同的指令进行调整，以适应不同的汽车设计和组装需求。现代汽车公司表示，在其新加坡微型工厂，超过 50% 的制造过程由机器人完成。自动地面车辆负责在工厂内部转移汽车，而装有 AI 摄像头的四足机器人则监控并分析装配过程。机器狗发现的问题随后由人类工人解决。

尽管这是一家规模不大的工厂，它却能像大规模制造商那样生产汽车。这得益于灵活的自动化技术和大量的系统智能，用以指挥制造过程。由于生产高度自动化，该工厂每年生产 30,000 辆汽车的产能只需要大约 100 名工人，意味着微型工厂的工人能够生产出比传统工厂工人多两到三倍的汽车。

现代汽车公司在新加坡的这家工厂是该国 40 多年来的第一家汽车工厂。尽管微型工厂可能使较富裕的国家能够重建制造业，但走向更大规模的自动化将意味着工人手工作业的减少。在这种微型工厂模式下，自动化任务的比例显著高于典型的装配厂。在物流方面，现代汽车已经实现了超过 60% 的自动化，这在其他工厂通常是非常低的比例，基本上依赖手工操作。

微型工厂对于汽车行业而言几乎是未经测试的新领域。EV 创业公司 Arrival 承诺在微型工厂中生产它们的车辆。然而，该公司在英国的工厂只交付了少量预期的车辆，并推迟了在美国计划的第二个微型工厂。目前，现代汽车是唯一一个开始在微型工厂中生产车辆的主要汽车制造商。

现代汽车公司对未来移动性的设想是，如果在市场测试中发现这是正确的方向，可能会在许多不同的城市环境中看到智能城市移动中心。尽管微型工厂的概念在大规模层面上仍未得到验证，但它们提供了一个展望，显示了随着行业迈向更自动化的未来，汽车制造可能会呈现的样子。</title>
            <link>https://nitter.cz/dotey/status/1733609163136069994#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733609163136069994#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 22:06:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>新加坡微型工厂：汽车制造业的未来<br />
<br />
在新加坡，一家看似普通的汽车工厂实际上是领先科技的产物：一个由自主机器狗和 AI 监控的机械臂运营的全新微型工厂。这个工厂旨在改变汽车的生产方式，它位于全球最小和最昂贵的国家之一。<br />
<br />
在这里，与传统汽车制造商在密集城市建厂的常规不同，机器人负责大部分的制造工作，采用更加灵活的模式。微型工厂通过自动化大部分生产流程，并在单独的单元中而不是沿传统生产线组装汽车，彻底改变了汽车的制造方式。<br />
<br />
这种微型工厂模型可能是我们未来几年将广泛讨论的趋势。作为汽车制造业数十年来最重大的变革之一，这一模式若被验证有效，将可能塑造整个行业的未来。在这样的工厂中，传统的汽车装配流程被重新设计，以适应更小的空间。机器人在完成汽车的最后装饰工作，这在传统的制造过程中大多由人手完成。<br />
<br />
在传统汽车工厂中，汽车沿着线性路径移动，并在装配线上安装零件。虽然这个程序越来越自动化，但其基本流程并未发生重大改变。然而，在这家新型工厂中，情况大为不同。这里采用的是基于单元的模型，工厂地面分布着不同的单元，每个单元专注于不同的任务，这大大增加了自动化的可能性。<br />
<br />
每个单元里的机器人被安排安装汽车的几个部分，然后汽车移动到下一个区域。单元可以根据不同的指令进行调整，以适应不同的汽车设计和组装需求。现代汽车公司表示，在其新加坡微型工厂，超过 50% 的制造过程由机器人完成。自动地面车辆负责在工厂内部转移汽车，而装有 AI 摄像头的四足机器人则监控并分析装配过程。机器狗发现的问题随后由人类工人解决。<br />
<br />
尽管这是一家规模不大的工厂，它却能像大规模制造商那样生产汽车。这得益于灵活的自动化技术和大量的系统智能，用以指挥制造过程。由于生产高度自动化，该工厂每年生产 30,000 辆汽车的产能只需要大约 100 名工人，意味着微型工厂的工人能够生产出比传统工厂工人多两到三倍的汽车。<br />
<br />
现代汽车公司在新加坡的这家工厂是该国 40 多年来的第一家汽车工厂。尽管微型工厂可能使较富裕的国家能够重建制造业，但走向更大规模的自动化将意味着工人手工作业的减少。在这种微型工厂模式下，自动化任务的比例显著高于典型的装配厂。在物流方面，现代汽车已经实现了超过 60% 的自动化，这在其他工厂通常是非常低的比例，基本上依赖手工操作。<br />
<br />
微型工厂对于汽车行业而言几乎是未经测试的新领域。EV 创业公司 Arrival 承诺在微型工厂中生产它们的车辆。然而，该公司在英国的工厂只交付了少量预期的车辆，并推迟了在美国计划的第二个微型工厂。目前，现代汽车是唯一一个开始在微型工厂中生产车辆的主要汽车制造商。<br />
<br />
现代汽车公司对未来移动性的设想是，如果在市场测试中发现这是正确的方向，可能会在许多不同的城市环境中看到智能城市移动中心。尽管微型工厂的概念在大规模层面上仍未得到验证，但它们提供了一个展望，显示了随着行业迈向更自动化的未来，汽车制造可能会呈现的样子。</p>
<p><a href="https://nitter.cz/raycat2021/status/1733466566803808358#m">nitter.cz/raycat2021/status/1733466566803808358#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzM2MDg0NzIwOTUxMTMyMTYvcHUvaW1nL2E5VjVSOF9VSEkydkVuMTYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/yetone/status/1733564208917438838#m</id>
            <title>RT by @dotey: 在截图的 ALT 中分享一下多语言版本的专业翻译 prompts，支持切换语言</title>
            <link>https://nitter.cz/yetone/status/1733564208917438838#m</link>
            <guid isPermaLink="false">https://nitter.cz/yetone/status/1733564208917438838#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 19:08:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在截图的 ALT 中分享一下多语言版本的专业翻译 prompts，支持切换语言</p>
<p><a href="https://nitter.cz/yetone/status/1732962308467589473#m">nitter.cz/yetone/status/1732962308467589473#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3YlF6emJZQUFGOWpSLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3YkhpbmFBQUFsYTdoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E3YlkwZWJ3QUFQU3VnLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733583693736767918#m</id>
            <title>R to @dotey: 刚看到页面上有一行小字：书本收益均归耗子叔家人 👍🏻</title>
            <link>https://nitter.cz/dotey/status/1733583693736767918#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733583693736767918#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 20:25:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚看到页面上有一行小字：书本收益均归耗子叔家人 👍🏻</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1733459395672244503#m</id>
            <title>RT by @dotey: 重新发一下MoE 8x7B的介绍原来的删掉了，由于没有在HF模型排行上找到我就直接复制了@RealJosephus的HF截图，这里向他道歉。一般只要是推特的原推有的我都会尽量转推MoE 8x7B由于官方没有说明很多内容都是拼凑的就没有把参考的内容都粘过来。
我并不是专门研究LLM的所以很多事情肯定说的不一定严谨，如果有问题欢迎指出。能改的我一般都会改，改不了的我会在下面贴上。但是我依然觉得不应该上来就骂人。

昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。
之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。
MoE 8x7B测试分数来源于第一个链接。

下面是MoE架构LLM的简单介绍：
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。
MoE架构为LLMs提供了几个优势：
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数。这使得能够开发更强大的模型，而无需成比例地增加计算要求。
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B。
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域。
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。  MoE的模型也有两个问题： MoE 模型比普通密集模型更难微调； MoE 模型会消耗大量显存。

模型下载：https://huggingface.co/DiscoResearch/mixtral-7b-8expert
在线试用：https://replicate.com/nateraw/mixtral-8x7b-32kseqlen</title>
            <link>https://nitter.cz/op7418/status/1733459395672244503#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1733459395672244503#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 12:11:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>重新发一下MoE 8x7B的介绍原来的删掉了，由于没有在HF模型排行上找到我就直接复制了<a href="https://nitter.cz/RealJosephus" title="Joseph Cheung">@RealJosephus</a>的HF截图，这里向他道歉。一般只要是推特的原推有的我都会尽量转推MoE 8x7B由于官方没有说明很多内容都是拼凑的就没有把参考的内容都粘过来。<br />
我并不是专门研究LLM的所以很多事情肯定说的不一定严谨，如果有问题欢迎指出。能改的我一般都会改，改不了的我会在下面贴上。但是我依然觉得不应该上来就骂人。<br />
<br />
昨晚圈子被一个叫MoE 8x7B模型刷屏了，这应该是第个一个开源权重的MoE架构LLM。<br />
之前猜测GPT-4的架构的时候很多人就觉得GPT-4用了MoEt架构。MoE可以与使用两倍FLOPs的密集模型相媲美。例如，使用相同的数据和 FLOP，LLaMA 7B 的 MoE 版本应该与 LLaMA 13B 相当。<br />
MoE 8x7B测试分数来源于第一个链接。<br />
<br />
下面是MoE架构LLM的简单介绍：<br />
Moe（混合专家模型）架构的LLM（大型语言模型）指的是一种神经架构设计，它将稀疏混合专家技术整合进来，以增加可学习参数到大型语言模型中而不增加推理成本。<br />
MoE架构为LLMs提供了几个优势：<br />
◆增加参数效率：MoE允许在不显著增加推理成本的情况下向LLMs添加可学习参数。这使得能够开发更强大的模型，而无需成比例地增加计算要求。<br />
◆通过指导调整改善性能：研究表明，MoE模型比密集模型更容易受益于指导调整。例如，FLAN-MOE-32B 模型在使用仅三分之一的 FLOPs 的情况下，在四项基准任务上优于 FLAN-PALM-62B。<br />
◆适应多样化数据：MoE架构可以处理现代数据集的增加复杂性和规模，这些数据集通常包含具有截然不同特征与标签关系的不同区域。<br />
◆潜力更高的参数效率：SaMoE 架构是 MoE 的一个变体，通过减少总参数达到了最多 5.2 倍，并且相较于基线取得了卓越的预训练和零-shot泛化结果。  MoE的模型也有两个问题： MoE 模型比普通密集模型更难微调； MoE 模型会消耗大量显存。<br />
<br />
模型下载：<a href="https://huggingface.co/DiscoResearch/mixtral-7b-8expert">huggingface.co/DiscoResearch…</a><br />
在线试用：<a href="https://replicate.com/nateraw/mixtral-8x7b-32kseqlen">replicate.com/nateraw/mixtra…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E1N3FrVGFVQUVwMGE3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/shark_louis/status/1733553573697310740#m</id>
            <title>RT by @dotey: 分享一个集中索引大量优质设计系统的网站 因为最近在做类似的事，要是能早点发现能省很多事</title>
            <link>https://nitter.cz/shark_louis/status/1733553573697310740#m</link>
            <guid isPermaLink="false">https://nitter.cz/shark_louis/status/1733553573697310740#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 18:25:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>分享一个集中索引大量优质设计系统的网站 因为最近在做类似的事，要是能早点发现能省很多事</p>
<p><a href="https://nitter.cz/101babich/status/1733476715760906332#m">nitter.cz/101babich/status/1733476715760906332#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/vista8/status/1733509609959379094#m</id>
            <title>RT by @dotey: 推荐中文Twitter（X）圈，AI领域的优秀账号：

@dotey 宝玉老师，博客：https://baoyu.io/  专注于LLM精品课程翻译，Prompt学习，AI行业第一手资讯。

@op7418 专注于AI绘画、AI视频生成，每周都会更新AIGC周刊，信息密度很高。https://quail.ink/op7418

@xiaohuggg 小互，自嘲为微博 500w 粉丝过气博主，今年转战Twitter
，每天更新大量AI新闻和工具资讯，往期推荐：https://waytoagi.feishu.cn/wiki/T2fUwmHBSiHU6Ukq9Imcu7n7nMh

@xicilion 西祠胡同创始人响马，经常分享LLM实战经验，干货很多，会定期删除Twitter帖子，收藏帖子要备份。

@WaytoAGI 最好的中文AI知识库，让人开眼界的AI的资讯整理大师。

@hanqing_me 汗青，AI Talk创始人，AI短视频制作先行者，探索AI技术和艺术结合的天花板。

@jesselaunz  遁一子，关注AI资讯和实际应用，经常分享Prompt探索实践，很有启发。

@lewangx 王乐，独立创业者，探索AI与硬件玩具结合，比如火火兔改造。

@JefferyTatsuya 日本华人AI创业典范，公司作品有Glarity、Felo等，都是口碑相很赞的AI工具。

@OwenYoungZh Chrome神级插件“沉浸式翻译”作者，博客地址：https://www.owenyoung.com/

@FinanceYF5 Will，AI资讯分享者，对数据敏感，有整理不少有价值的AI行业数据信息。

@thinkingjimmy 随意搜寻小报童和 http://learningprompt.wiki 作者，产品经理和创造者，思考分享非常多。

@oran_ge 橘子，AI行业从业者，优秀产品经理，非常坦诚有趣的人。

@99aico Youtuber，AI资讯分享，体验并实践大量AI数字人类产品。

@XDash 范冰，《增长黑客》作者，经常推荐好书、好工具，Twitter内容很有趣。

最后，推荐我的两个好友，我们共同主持Fingerfly访谈节目。（虽然断更两个月😄）

@GlocalTerapy 七娘，科幻迷，顶级婚恋产品经理，对AI在游戏和社交领域的应用感兴趣。

@fuxiangPro 个人见过最自律的人之一，专注于AI和机器人的结合探索。

以上推荐，除前三名“卷王”外，排名不分先后。

求留言推荐你眼中的更多优秀账号~~~</title>
            <link>https://nitter.cz/vista8/status/1733509609959379094#m</link>
            <guid isPermaLink="false">https://nitter.cz/vista8/status/1733509609959379094#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 15:31:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>推荐中文Twitter（X）圈，AI领域的优秀账号：<br />
<br />
<a href="https://nitter.cz/dotey" title="宝玉">@dotey</a> 宝玉老师，博客：<a href="https://baoyu.io/">baoyu.io/</a>  专注于LLM精品课程翻译，Prompt学习，AI行业第一手资讯。<br />
<br />
<a href="https://nitter.cz/op7418" title="歸藏">@op7418</a> 专注于AI绘画、AI视频生成，每周都会更新AIGC周刊，信息密度很高。<a href="https://quail.ink/op7418">quail.ink/op7418</a><br />
<br />
<a href="https://nitter.cz/xiaohuggg" title="小互">@xiaohuggg</a> 小互，自嘲为微博 500w 粉丝过气博主，今年转战Twitter<br />
，每天更新大量AI新闻和工具资讯，往期推荐：<a href="https://waytoagi.feishu.cn/wiki/T2fUwmHBSiHU6Ukq9Imcu7n7nMh">waytoagi.feishu.cn/wiki/T2fU…</a><br />
<br />
<a href="https://nitter.cz/xicilion" title="响马">@xicilion</a> 西祠胡同创始人响马，经常分享LLM实战经验，干货很多，会定期删除Twitter帖子，收藏帖子要备份。<br />
<br />
<a href="https://nitter.cz/WaytoAGI" title="通往AGI之路">@WaytoAGI</a> 最好的中文AI知识库，让人开眼界的AI的资讯整理大师。<br />
<br />
<a href="https://nitter.cz/hanqing_me" title="汗青 HQ">@hanqing_me</a> 汗青，AI Talk创始人，AI短视频制作先行者，探索AI技术和艺术结合的天花板。<br />
<br />
<a href="https://nitter.cz/jesselaunz" title="遁一子">@jesselaunz</a>  遁一子，关注AI资讯和实际应用，经常分享Prompt探索实践，很有启发。<br />
<br />
<a href="https://nitter.cz/lewangx" title="lewang🍥">@lewangx</a> 王乐，独立创业者，探索AI与硬件玩具结合，比如火火兔改造。<br />
<br />
<a href="https://nitter.cz/JefferyTatsuya" title="Jeffery Kaneda　金田達也">@JefferyTatsuya</a> 日本华人AI创业典范，公司作品有Glarity、Felo等，都是口碑相很赞的AI工具。<br />
<br />
<a href="https://nitter.cz/OwenYoungZh" title="Owen">@OwenYoungZh</a> Chrome神级插件“沉浸式翻译”作者，博客地址：<a href="https://www.owenyoung.com/">owenyoung.com/</a><br />
<br />
<a href="https://nitter.cz/FinanceYF5" title="Will">@FinanceYF5</a> Will，AI资讯分享者，对数据敏感，有整理不少有价值的AI行业数据信息。<br />
<br />
<a href="https://nitter.cz/thinkingjimmy" title="JimmyWong">@thinkingjimmy</a> 随意搜寻小报童和 <a href="http://learningprompt.wiki">learningprompt.wiki</a> 作者，产品经理和创造者，思考分享非常多。<br />
<br />
<a href="https://nitter.cz/oran_ge" title="orange.ai">@oran_ge</a> 橘子，AI行业从业者，优秀产品经理，非常坦诚有趣的人。<br />
<br />
<a href="https://nitter.cz/99aico" title="JoJo 99AI">@99aico</a> Youtuber，AI资讯分享，体验并实践大量AI数字人类产品。<br />
<br />
<a href="https://nitter.cz/XDash" title="XDash">@XDash</a> 范冰，《增长黑客》作者，经常推荐好书、好工具，Twitter内容很有趣。<br />
<br />
最后，推荐我的两个好友，我们共同主持Fingerfly访谈节目。（虽然断更两个月😄）<br />
<br />
<a href="https://nitter.cz/GlocalTerapy" title="Dolores">@GlocalTerapy</a> 七娘，科幻迷，顶级婚恋产品经理，对AI在游戏和社交领域的应用感兴趣。<br />
<br />
<a href="https://nitter.cz/fuxiangPro" title="fuxiang">@fuxiangPro</a> 个人见过最自律的人之一，专注于AI和机器人的结合探索。<br />
<br />
以上推荐，除前三名“卷王”外，排名不分先后。<br />
<br />
求留言推荐你眼中的更多优秀账号~~~</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMjM4ODAzNzg2MTIwODA2NC81REpqQlc5cD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733405540078731768#m</id>
            <title>AK 的这个理论很有意思：幻觉是大语言模型与生俱来的特性，像搜索引擎这样没有幻觉的就没有创造性，永远是固定的结果。

要解决幻觉问题，靠的是像ChatGPT这样的大语言模型助手，借助RAG、验证链、外部工具等手段去减少幻觉。

以下是原文翻译：

# 深入探讨“幻觉问题”

每当有人问起大语言模型（LLM）中的“幻觉问题”，我总感到有些困惑。因为从某种角度看，大语言模型的全部工作恰恰就是制造幻觉。它们就像是造梦机。

我们通过指令引导这些“梦”。指令开启梦境，而大语言模型依据对其训练文档的模糊记忆，大部分情况下都能引导梦境走向有价值的方向。

只有当这些梦境进入了事实错误的领域时，我们才会称之为“幻觉”。这似乎是个漏洞，但实际上只是大语言模型在做它本就擅长的事情。

再来看一个极端例子：搜索引擎。它根据输入的提示词，直接返回数据库中最相似的“训练文档”，一字不差。可以说，这种搜索引擎存在“创造力问题”——它无法提供任何新的回应。大语言模型则是百分之百地“做梦”，因此存在幻觉问题。而搜索引擎则完全不“做梦”，因此有创造力问题。

话虽如此，我明白人们*真正*关心的是，他们不希望像 ChatGPT 这样的大语言模型助手产生幻觉。大语言模型助手系统比单纯的大语言模型要复杂得多，即便大语言模型是其核心。在这些系统中减少幻觉的方法有很多，例如使用检索增强生成（Retrieval Augmented Generation, RAG），通过上下文学习，更准确地将输出内容与真实数据联系起来，这可能是最常见的方式。还有样本间的不一致性、反思、验证链、从激活状态解码不确定性、工具使用等，这些都是非常热门而且有趣的研究领域。

总的来说，虽然可能有些吹毛求疵，但大语言模型本身并没有“幻觉问题”。幻觉并非缺陷，而是大语言模型最重要的特性。真正需要解决幻觉问题的是大语言模型助手，而我们也应该着手解决这一问题。

 好了，吐槽完这些我感觉舒服多了 :)</title>
            <link>https://nitter.cz/dotey/status/1733405540078731768#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733405540078731768#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:37:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AK 的这个理论很有意思：幻觉是大语言模型与生俱来的特性，像搜索引擎这样没有幻觉的就没有创造性，永远是固定的结果。<br />
<br />
要解决幻觉问题，靠的是像ChatGPT这样的大语言模型助手，借助RAG、验证链、外部工具等手段去减少幻觉。<br />
<br />
以下是原文翻译：<br />
<br />
# 深入探讨“幻觉问题”<br />
<br />
每当有人问起大语言模型（LLM）中的“幻觉问题”，我总感到有些困惑。因为从某种角度看，大语言模型的全部工作恰恰就是制造幻觉。它们就像是造梦机。<br />
<br />
我们通过指令引导这些“梦”。指令开启梦境，而大语言模型依据对其训练文档的模糊记忆，大部分情况下都能引导梦境走向有价值的方向。<br />
<br />
只有当这些梦境进入了事实错误的领域时，我们才会称之为“幻觉”。这似乎是个漏洞，但实际上只是大语言模型在做它本就擅长的事情。<br />
<br />
再来看一个极端例子：搜索引擎。它根据输入的提示词，直接返回数据库中最相似的“训练文档”，一字不差。可以说，这种搜索引擎存在“创造力问题”——它无法提供任何新的回应。大语言模型则是百分之百地“做梦”，因此存在幻觉问题。而搜索引擎则完全不“做梦”，因此有创造力问题。<br />
<br />
话虽如此，我明白人们*真正*关心的是，他们不希望像 ChatGPT 这样的大语言模型助手产生幻觉。大语言模型助手系统比单纯的大语言模型要复杂得多，即便大语言模型是其核心。在这些系统中减少幻觉的方法有很多，例如使用检索增强生成（Retrieval Augmented Generation, RAG），通过上下文学习，更准确地将输出内容与真实数据联系起来，这可能是最常见的方式。还有样本间的不一致性、反思、验证链、从激活状态解码不确定性、工具使用等，这些都是非常热门而且有趣的研究领域。<br />
<br />
总的来说，虽然可能有些吹毛求疵，但大语言模型本身并没有“幻觉问题”。幻觉并非缺陷，而是大语言模型最重要的特性。真正需要解决幻觉问题的是大语言模型助手，而我们也应该着手解决这一问题。<br />
<br />
 好了，吐槽完这些我感觉舒服多了 :)</p>
<p><a href="https://nitter.cz/karpathy/status/1733299213503787018#m">nitter.cz/karpathy/status/1733299213503787018#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733392133497729409#m</id>
            <title>这个小姐姐的博客上的内容质量很高👍🏻</title>
            <link>https://nitter.cz/dotey/status/1733392133497729409#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733392133497729409#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 07:44:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个小姐姐的博客上的内容质量很高👍🏻</p>
<p><a href="https://nitter.cz/helloiamleonie/status/1732315699823947986#m">nitter.cz/helloiamleonie/status/1732315699823947986#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733389526796484972#m</id>
            <title>一个“白嫖”CDN的技巧

使用场景是我需要动态请求一个中文字体文件，放自己网站的话占用流量比较大（我用的Vercel要按照流量付费的），后来发现可以把字体放到 GitHub，然后使用 http://cdn.jsdelivr.net 下载 GitHub 上的静态文件，这样就可以直接从 jsdelivr 下载我在 GitHub 上放的静态文件。

路径格式是：/gh/user/repo@version/file
版本号是可选的，例如：
https://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js
https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js

这里有详细说明：https://github.com/jsdelivr/jsdelivr#github</title>
            <link>https://nitter.cz/dotey/status/1733389526796484972#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733389526796484972#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 07:34:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个“白嫖”CDN的技巧<br />
<br />
使用场景是我需要动态请求一个中文字体文件，放自己网站的话占用流量比较大（我用的Vercel要按照流量付费的），后来发现可以把字体放到 GitHub，然后使用 <a href="http://cdn.jsdelivr.net">cdn.jsdelivr.net</a> 下载 GitHub 上的静态文件，这样就可以直接从 jsdelivr 下载我在 GitHub 上放的静态文件。<br />
<br />
路径格式是：/gh/user/repo@version/file<br />
版本号是可选的，例如：<br />
<a href="https://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js">cdn.jsdelivr.net/gh/jquery/j…</a><br />
<a href="https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js">cdn.jsdelivr.net/gh/jquery/j…</a><br />
<br />
这里有详细说明：<a href="https://github.com/jsdelivr/jsdelivr#github">github.com/jsdelivr/jsdelivr…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/balconychy/status/1733327109790351496#m</id>
            <title>RT by @dotey: 老外怼人也是够狠</title>
            <link>https://nitter.cz/balconychy/status/1733327109790351496#m</link>
            <guid isPermaLink="false">https://nitter.cz/balconychy/status/1733327109790351496#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:26:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>老外怼人也是够狠</p>
<p><a href="https://nitter.cz/tridevgurung/status/1733326045023973670#m">nitter.cz/tridevgurung/status/1733326045023973670#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/ruanyf/status/1733353407581442397#m</id>
            <title>RT by @dotey: 本周，美国两个 IT 巨头，各自发布了一个免费 AI 工具。

- Imagine（图一）：Meta 公司的文生图工具，卖点是使用脸书和 IG 的11亿张图片进行训练。https://imagine.meta.com/

- NotebookLM（图二）：谷歌发布的 AI 笔记工具，自动生成上传文档的笔记，并可以对文档提问。https://notebooklm.google.com/

试用体会：Imagine 挺好用的，NotebookLM 似乎只支持上传英文 PDF 文档。</title>
            <link>https://nitter.cz/ruanyf/status/1733353407581442397#m</link>
            <guid isPermaLink="false">https://nitter.cz/ruanyf/status/1733353407581442397#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 05:10:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本周，美国两个 IT 巨头，各自发布了一个免费 AI 工具。<br />
<br />
- Imagine（图一）：Meta 公司的文生图工具，卖点是使用脸书和 IG 的11亿张图片进行训练。<a href="https://imagine.meta.com/">imagine.meta.com/</a><br />
<br />
- NotebookLM（图二）：谷歌发布的 AI 笔记工具，自动生成上传文档的笔记，并可以对文档提问。<a href="https://notebooklm.google.com/">notebooklm.google.com/</a><br />
<br />
试用体会：Imagine 挺好用的，NotebookLM 似乎只支持上传英文 PDF 文档。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0YnMzNGJvQUF6dndSLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0YnMzNmFZQUE2TGRyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/Gorden_Sun/status/1660072615602716673#m</id>
            <title>RT by @dotey: 这个是其他人已经训练好的孙燕姿的AI人声模型：https://mega.nz/file/02Qi3Ioa#iJzpQLehTPLrgLAA8Bml0lZh6DdpqSZ_j5H8NKz4WXA</title>
            <link>https://nitter.cz/Gorden_Sun/status/1660072615602716673#m</link>
            <guid isPermaLink="false">https://nitter.cz/Gorden_Sun/status/1660072615602716673#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 May 2023 23:58:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是其他人已经训练好的孙燕姿的AI人声模型：<a href="https://mega.nz/file/02Qi3Ioa#iJzpQLehTPLrgLAA8Bml0lZh6DdpqSZ_j5H8NKz4WXA">mega.nz/file/02Qi3Ioa#iJzpQL…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMTExNTAwNTkyNjU5NjYwOC9wVTdOVDBTXz9mb3JtYXQ9cG5nJm5hbWU9NDIweDQyMF8y" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733350087172997615#m</id>
            <title>ChatGPT 官方推特（X）昨天发了一条推文说到：

> 我们注意到了大家对 GPT4 响应变得更加迟钝的反馈！自 11 月 11 日以来，我们还没有对模型进行更新，并非有意为之。模型的行为有时难以预测，我们正在积极寻找解决方案 🫡。
> 需要澄清的是，模型自 11 月 11 日以来并未自行发生改变。问题在于模型行为的差异可能不易察觉 —— 只有部分指令的响应可能会有所下降，客户和员工可能要花相当长的时间才能发现并解决这些问题。

今天又回复了这个话题：

开发聊天模型并非简单的工业流水线作业。即便使用同一数据集，不同批次的训练结果也可能导致模型在个性、写作风格、拒绝回应方式、性能表现，乃至政治倾向上有明显差异。

在推出新模型时，我们会深入测试，包括离线评估指标和在线 A/B 测试。根据这些测试结果，我们会基于数据做出判断，以确定新模型是否真正优于旧版，更好地服务于用户。

这一过程远非简单地为网站增加新功能那么直接。它更像是多人共同参与的艺术创作，我们在规划、打造、评估带有新特性的聊天模型中投入巨大努力！

我们致力于不断提升模型的能力，使其适应成千上万种使用场景。因此，您的反馈至关重要！它帮助我们在这个充满变化的评估领域保持前沿地位 🙏。</title>
            <link>https://nitter.cz/dotey/status/1733350087172997615#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733350087172997615#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:57:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT 官方推特（X）昨天发了一条推文说到：<br />
<br />
> 我们注意到了大家对 GPT4 响应变得更加迟钝的反馈！自 11 月 11 日以来，我们还没有对模型进行更新，并非有意为之。模型的行为有时难以预测，我们正在积极寻找解决方案 🫡。<br />
> 需要澄清的是，模型自 11 月 11 日以来并未自行发生改变。问题在于模型行为的差异可能不易察觉 —— 只有部分指令的响应可能会有所下降，客户和员工可能要花相当长的时间才能发现并解决这些问题。<br />
<br />
今天又回复了这个话题：<br />
<br />
开发聊天模型并非简单的工业流水线作业。即便使用同一数据集，不同批次的训练结果也可能导致模型在个性、写作风格、拒绝回应方式、性能表现，乃至政治倾向上有明显差异。<br />
<br />
在推出新模型时，我们会深入测试，包括离线评估指标和在线 A/B 测试。根据这些测试结果，我们会基于数据做出判断，以确定新模型是否真正优于旧版，更好地服务于用户。<br />
<br />
这一过程远非简单地为网站增加新功能那么直接。它更像是多人共同参与的艺术创作，我们在规划、打造、评估带有新特性的聊天模型中投入巨大努力！<br />
<br />
我们致力于不断提升模型的能力，使其适应成千上万种使用场景。因此，您的反馈至关重要！它帮助我们在这个充满变化的评估领域保持前沿地位 🙏。</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1733329175342420380#m">nitter.cz/ChatGPTapp/status/1733329175342420380#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WW1ycVd3QUFJcDI1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0WXRzWFhrQUF0RFpqLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733347415187427638#m</id>
            <title>这次似乎文心一言赢了ChatGPT……

图源：https://weibo.com/5851185687/NwcFmbKU5</title>
            <link>https://nitter.cz/dotey/status/1733347415187427638#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733347415187427638#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:46:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这次似乎文心一言赢了ChatGPT……<br />
<br />
图源：<a href="https://weibo.com/5851185687/NwcFmbKU5">weibo.com/5851185687/NwcFmbK…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0VjVCUFhjQUFEeExtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0VjgyOFc4QUVyZU1vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1733340532200395014#m</id>
            <title>一本还在写作中的在线免费电子书：《数据工程设计模式 | Data Engineering Design Patterns (DEDP)》

主要和大数据相关

https://www.dedp.online/about-this-book.html</title>
            <link>https://nitter.cz/dotey/status/1733340532200395014#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1733340532200395014#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:19:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一本还在写作中的在线免费电子书：《数据工程设计模式 | Data Engineering Design Patterns (DEDP)》<br />
<br />
主要和大数据相关<br />
<br />
<a href="https://www.dedp.online/about-this-book.html">dedp.online/about-this-book.…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E0UUFnT1dRQUFnWVhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>