<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>GPTDAOCN-e/acc / @GPTDAOCN</title>
        <link>https://nitter.cz/GPTDAOCN</link>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1755411603078762633#m</id>
            <title>最硬核最大的生成式AI大会</title>
            <link>https://nitter.cz/GPTDAOCN/status/1755411603078762633#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1755411603078762633#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 02:01:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最硬核最大的生成式AI大会</p>
<p><a href="https://nitter.cz/FinanceYF5/status/1755408818446680229#m">nitter.cz/FinanceYF5/status/1755408818446680229#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752584540827345384#m</id>
            <title>GenAI SF 2024峰会每日嘉宾介绍</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752584540827345384#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752584540827345384#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jan 2024 06:48:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GenAI SF 2024峰会每日嘉宾介绍</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZKdVdlT2FvQUFxbGNVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752447609560809535#m</id>
            <title>R to @GPTDAOCN: 想象一下，你有一本旧的百科全书（预训练的大型语言模型），它里面的信息是固定的，不能更新。如果你想要这本书包含最新的信息，你有两个选择：

1. 微调：就像是给这本书加上一些新的章节。你继续用同样的方式写书，只是内容是最新的信息。但问题是，这本书可能不会很好地吸收和整合这些新章节。

2. RAG（检索增强生成）：这种方法更像是在你查阅书的时候，有一个助手去图书馆找到最新的信息，并把它摘录下来插入到旧书的对应部分。这样，当你查阅旧书的时候，你同时也能看到最新的内容。

研究发现，第二种方法（RAG）通常更有效，因为这本旧书（模型）不太擅长通过传统的微调来学习新内容。

再比如假设你是个厨师，想要学习做一道最新流行的菜肴，你有两个选择：

1. 微调：你根据一些零散的新食谱尝试做这道菜，反复试验直到你能熟练掌握。这可能需要很多时间和材料，而且结果可能也不是很完美。

2. RAG：你在做菜时旁边有一个电子屏幕，上面实时显示着从互联网上检索到的最新食谱和大厨们的烹饪技巧。你可以边做边学，立即获取最新的指导，让你更快地学会这道菜。

在这个例子中，RAG就像是你的智能辅助屏幕，帮你做得更好、更快。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752447609560809535#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752447609560809535#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 21:44:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>想象一下，你有一本旧的百科全书（预训练的大型语言模型），它里面的信息是固定的，不能更新。如果你想要这本书包含最新的信息，你有两个选择：<br />
<br />
1. 微调：就像是给这本书加上一些新的章节。你继续用同样的方式写书，只是内容是最新的信息。但问题是，这本书可能不会很好地吸收和整合这些新章节。<br />
<br />
2. RAG（检索增强生成）：这种方法更像是在你查阅书的时候，有一个助手去图书馆找到最新的信息，并把它摘录下来插入到旧书的对应部分。这样，当你查阅旧书的时候，你同时也能看到最新的内容。<br />
<br />
研究发现，第二种方法（RAG）通常更有效，因为这本旧书（模型）不太擅长通过传统的微调来学习新内容。<br />
<br />
再比如假设你是个厨师，想要学习做一道最新流行的菜肴，你有两个选择：<br />
<br />
1. 微调：你根据一些零散的新食谱尝试做这道菜，反复试验直到你能熟练掌握。这可能需要很多时间和材料，而且结果可能也不是很完美。<br />
<br />
2. RAG：你在做菜时旁边有一个电子屏幕，上面实时显示着从互联网上检索到的最新食谱和大厨们的烹饪技巧。你可以边做边学，立即获取最新的指导，让你更快地学会这道菜。<br />
<br />
在这个例子中，RAG就像是你的智能辅助屏幕，帮你做得更好、更快。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752446699916988466#m</id>
            <title>研究表明，与持续的预训练或微调相比，检索增强生成（Retrieval Augmented Generation, RAG）方法在向大语言模型（LLM）中注入新知识方面更为有效和易于实施。

预训练知识：大型语言模型通过预训练过程内嵌了大量事实信息，但这些模型所拥有的知识高度依赖于其预训练数据的特性。遗憾的是，这意味着在当前的LLM范式下，这些模型的知识库是静态的（例如，ChatGPT具有知识截止日期）并且可能缺乏详细信息。

知识注入：给定一个预训练的LLM，我们可以使用两种后处理技术将新数据注入模型的知识库：

- 微调：继续在一个较小的、领域专业化的新信息语料库上进行模型的预训练过程。
- 检索增强生成（RAG）：通过检索相关信息并将其包含在模型的输入查询中，从而使模型通过上下文学习来生成更加扎实/事实的输出。

上述的微调变体是一种持续预训练风格的微调，在这种方法中，使用下一个词预测目标来对预训练模型在专业化文本语料库上进行进一步训练。相比之下，SFT和RLHF强调模型响应的质量，而不是提高LLM的知识广度。

“给定一些以文本语料库形式存在的知识库，如何最好地将这些知识教给一个预训练模型？” - 来自文献[1]

最新研究：在文献[1]中，作者比较了RAG和微调，以确定更优的知识注入方法。RAG设置使用向量搜索来检索相关文档块以包含在模型的提示中。给定一个信息语料库，我们可以：

1. 将这个语料库分成文本块。
2. 使用嵌入模型（例如bge-large-en）为每个文本块生成一个密集向量。
3. 通过嵌入模型的输入并执行向量搜索来搜索相关的文本块。
4. 将相关文本块加入模型的提示中。

我们学到了什么？虽然微调确实提高了模型的性能，但RAG在注入新知识和之前遇到的知识方面始终优于微调。简而言之，LLM通过微调学习新信息存在困难。尽管微调确实提高了与基础模型相比的性能，但RAG相对于微调有显著优势。尽管在某些情况下将RAG与微调结合使用是有效的，但这并不一致地提高性能。

通过释义微调：我们可以通过在多种不同的释义上训练模型来提高微调注入知识的性能。为了通过微调向LLM教授新信息，我们必须以多种方式重复这些信息。

——
[1] Ovadia, Oded, et al. "微调还是检索？比较LLM中的知识注入。" arXiv预印本arXiv:2312.05934（2023）。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752446699916988466#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752446699916988466#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 21:40:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>研究表明，与持续的预训练或微调相比，检索增强生成（Retrieval Augmented Generation, RAG）方法在向大语言模型（LLM）中注入新知识方面更为有效和易于实施。<br />
<br />
预训练知识：大型语言模型通过预训练过程内嵌了大量事实信息，但这些模型所拥有的知识高度依赖于其预训练数据的特性。遗憾的是，这意味着在当前的LLM范式下，这些模型的知识库是静态的（例如，ChatGPT具有知识截止日期）并且可能缺乏详细信息。<br />
<br />
知识注入：给定一个预训练的LLM，我们可以使用两种后处理技术将新数据注入模型的知识库：<br />
<br />
- 微调：继续在一个较小的、领域专业化的新信息语料库上进行模型的预训练过程。<br />
- 检索增强生成（RAG）：通过检索相关信息并将其包含在模型的输入查询中，从而使模型通过上下文学习来生成更加扎实/事实的输出。<br />
<br />
上述的微调变体是一种持续预训练风格的微调，在这种方法中，使用下一个词预测目标来对预训练模型在专业化文本语料库上进行进一步训练。相比之下，SFT和RLHF强调模型响应的质量，而不是提高LLM的知识广度。<br />
<br />
“给定一些以文本语料库形式存在的知识库，如何最好地将这些知识教给一个预训练模型？” - 来自文献[1]<br />
<br />
最新研究：在文献[1]中，作者比较了RAG和微调，以确定更优的知识注入方法。RAG设置使用向量搜索来检索相关文档块以包含在模型的提示中。给定一个信息语料库，我们可以：<br />
<br />
1. 将这个语料库分成文本块。<br />
2. 使用嵌入模型（例如bge-large-en）为每个文本块生成一个密集向量。<br />
3. 通过嵌入模型的输入并执行向量搜索来搜索相关的文本块。<br />
4. 将相关文本块加入模型的提示中。<br />
<br />
我们学到了什么？虽然微调确实提高了模型的性能，但RAG在注入新知识和之前遇到的知识方面始终优于微调。简而言之，LLM通过微调学习新信息存在困难。尽管微调确实提高了与基础模型相比的性能，但RAG相对于微调有显著优势。尽管在某些情况下将RAG与微调结合使用是有效的，但这并不一致地提高性能。<br />
<br />
通过释义微调：我们可以通过在多种不同的释义上训练模型来提高微调注入知识的性能。为了通过微调向LLM教授新信息，我们必须以多种方式重复这些信息。<br />
<br />
——<br />
[1] Ovadia, Oded, et al. "微调还是检索？比较LLM中的知识注入。" arXiv预印本arXiv:2312.05934（2023）。</p>
<p><a href="https://nitter.cz/cwolferesearch/status/1752369105221333061#m">nitter.cz/cwolferesearch/status/1752369105221333061#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752444712873869426#m</id>
            <title>OpenAI提到的“将GPTs带入任何ChatGPT对话”的功能指的是在一个聊天会话中，用户可以通过使用@符号调用不同的GPT模型来参与对话。

这意味着用户可以在一次对话中利用多个GPT模型的能力，每个模型可能针对不同的任务或数据集进行了优化。

这个功能指用户能够在一个对话中召唤多个模型来回答问题或执行任务。

对未来的好处可能包括：

1. 更高效的信息检索：用户可以同时访问多个专门化的GPT模型，以获得更准确或详细的信息。
2. 任务特定对话：某些GPT模型可能更适合处理特定类型的任务，用户可以根据需要调用不同的模型。
3. 上下文保持：在同一对话中使用不同的GPT模型，可以保持对话上下文的连贯性，提高用户体验。

潜在的新产品可能包括：

1. 专业化的聊天应用：用户可以在一个界面中与多个专业化的GPT模型交流，每个模型服务于不同的领域，如医疗、法律或教育咨询。
2. 集成工作流平台：在工作流程中集成多个GPT模型，使得用户可以在一个平台上完成多项任务，如数据分析、文本生成和自动编程。
3. 个性化学习助手：针对用户的学习需求，调用不同的GPT模型来提供个性化的教育内容和指导。

这个功能强调了模型的互操作性和集成，可能会推动开发一系列创新的应用程序和服务，使得用户能够更有效地利用AI的能力。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752444712873869426#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752444712873869426#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 21:32:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI提到的“将GPTs带入任何ChatGPT对话”的功能指的是在一个聊天会话中，用户可以通过使用@符号调用不同的GPT模型来参与对话。<br />
<br />
这意味着用户可以在一次对话中利用多个GPT模型的能力，每个模型可能针对不同的任务或数据集进行了优化。<br />
<br />
这个功能指用户能够在一个对话中召唤多个模型来回答问题或执行任务。<br />
<br />
对未来的好处可能包括：<br />
<br />
1. 更高效的信息检索：用户可以同时访问多个专门化的GPT模型，以获得更准确或详细的信息。<br />
2. 任务特定对话：某些GPT模型可能更适合处理特定类型的任务，用户可以根据需要调用不同的模型。<br />
3. 上下文保持：在同一对话中使用不同的GPT模型，可以保持对话上下文的连贯性，提高用户体验。<br />
<br />
潜在的新产品可能包括：<br />
<br />
1. 专业化的聊天应用：用户可以在一个界面中与多个专业化的GPT模型交流，每个模型服务于不同的领域，如医疗、法律或教育咨询。<br />
2. 集成工作流平台：在工作流程中集成多个GPT模型，使得用户可以在一个平台上完成多项任务，如数据分析、文本生成和自动编程。<br />
3. 个性化学习助手：针对用户的学习需求，调用不同的GPT模型来提供个性化的教育内容和指导。<br />
<br />
这个功能强调了模型的互操作性和集成，可能会推动开发一系列创新的应用程序和服务，使得用户能够更有效地利用AI的能力。</p>
<p><a href="https://nitter.cz/OpenAI/status/1752391522081980855#m">nitter.cz/OpenAI/status/1752391522081980855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752185300514955659#m</id>
            <title>这个图显示了六家大型科技公司的收入构成。每个公司的总收入以及收入的分布通过环形图的不同颜色区段表示。环形图中心标注了各自公司的总收入额。

1. Amazon：总收入514亿美元，第三方卖家服务是最大的收入来源，占42.8%。
2. Apple：总收入338亿美元，iPhone销售占总收入的52.3%，是最大单一来源。
3. Alphabet：总收入283亿美元，广告收入占比最高，达到57.4%。
4. Microsoft：总收入212亿美元，其云服务（包括Azure）是最大的收入来源，占比37.7%。
5. Meta** (Facebook) ：总收入117亿美元，几乎全部收入（98.1%）来自于广告。
6. NVIDIA：总收入27亿美元，其收入主要来自游戏和数据中心，分别占55.9%和40.1%。

从这些数据中可以观察到一些关键的行业趋势：
- 收入集中度：大部分公司有一个或两个主导收入来源，如Apple的iPhone和Meta的广告业务。
- 广告的力量：对于Alphabet和Meta来说，广告是其主要的收入来源，显示了在线广告在科技行业中的重要性。
- 云服务的增长：Microsoft显示出其云服务是重要的收入来源，反映了云计算的增长趋势。
- 技术多样性：NVIDIA的游戏和数据中心收入显示了其在不同技术领域的强势地位。

整体来看，尽管这些公司在不同的技术细分市场中运营，但它们的收入集中度揭示了它们各自在特定领域的市场支配地位。同时，这也反映了市场对于某些产品和服务的高需求。此外，这种收入分布也可能暴露这些公司在市场变化面前的脆弱性，因为对少数收入来源的依赖可能在市场动荡时带来风险。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752185300514955659#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752185300514955659#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 04:21:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个图显示了六家大型科技公司的收入构成。每个公司的总收入以及收入的分布通过环形图的不同颜色区段表示。环形图中心标注了各自公司的总收入额。<br />
<br />
1. Amazon：总收入514亿美元，第三方卖家服务是最大的收入来源，占42.8%。<br />
2. Apple：总收入338亿美元，iPhone销售占总收入的52.3%，是最大单一来源。<br />
3. Alphabet：总收入283亿美元，广告收入占比最高，达到57.4%。<br />
4. Microsoft：总收入212亿美元，其云服务（包括Azure）是最大的收入来源，占比37.7%。<br />
5. Meta** (Facebook) ：总收入117亿美元，几乎全部收入（98.1%）来自于广告。<br />
6. NVIDIA：总收入27亿美元，其收入主要来自游戏和数据中心，分别占55.9%和40.1%。<br />
<br />
从这些数据中可以观察到一些关键的行业趋势：<br />
- 收入集中度：大部分公司有一个或两个主导收入来源，如Apple的iPhone和Meta的广告业务。<br />
- 广告的力量：对于Alphabet和Meta来说，广告是其主要的收入来源，显示了在线广告在科技行业中的重要性。<br />
- 云服务的增长：Microsoft显示出其云服务是重要的收入来源，反映了云计算的增长趋势。<br />
- 技术多样性：NVIDIA的游戏和数据中心收入显示了其在不同技术领域的强势地位。<br />
<br />
整体来看，尽管这些公司在不同的技术细分市场中运营，但它们的收入集中度揭示了它们各自在特定领域的市场支配地位。同时，这也反映了市场对于某些产品和服务的高需求。此外，这种收入分布也可能暴露这些公司在市场变化面前的脆弱性，因为对少数收入来源的依赖可能在市场动荡时带来风险。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZFRFBxNmFVQUFSZE83LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752123939646845226#m</id>
            <title>R to @GPTDAOCN: 心灵感应终于来了</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752123939646845226#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752123939646845226#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 00:17:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>心灵感应终于来了</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752120098884919769#m</id>
            <title>马斯克宣布其Neuralink的第一个产品名为“Telepathy”，这个名字本身就暗示了产品的目标和功能——实现思维到思维的直接通信。在传统意义上，传心术是指不通过言语或肢体语言，而是通过思想直接交流的能力。Neuralink的技术若能实现类似功能，它将是一个划时代的突破，可能会彻底改变人们之间的沟通方式，使信息交换更为迅速和直接。

这个名字也意味着Neuralink的产品可能会探索大脑接口技术的边界，尝试连接和翻译人类的神经活动，允许用户传递复杂的思想和概念而无需说话。这不仅对于普通沟通来说是革命性的，而且对于有沟通障碍的人，比如言语受限者，可能是一个巨大的福音。

此外，“Telepathy”这个名字对长生不老的追求也可能有深远的含义。如果未来的技术能够捕捉、翻译甚至存储人的思想，那么这可能是对人类意识进行数字化和保存的第一步，这是长生不老梦想的关键组成部分

"Telepathy"这个词源于希腊语中的“tele”（远）和“pathos”（感觉），直译为“远感”，通常被解释为心灵感应或心灵传输。在通常的理解中，它指的是不通过传统感官或物理交流手段，而是通过某种形式的超自然方式进行思想和情感的交流。

这个概念最早在19世纪晚期成为心灵研究的一个主题，并且常常出现在科幻和奇幻文学中。心灵感应通常被描绘为一种神秘的能力，使人能够跨越物理距离，分享思想和信息。马斯克将Neuralink的产品命名为“Telepathy”，那么很可能是在引用这个概念，以此来传达他的这项技术可以使人类实现类似的直接思想交流的愿景。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752120098884919769#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752120098884919769#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 00:02:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克宣布其Neuralink的第一个产品名为“Telepathy”，这个名字本身就暗示了产品的目标和功能——实现思维到思维的直接通信。在传统意义上，传心术是指不通过言语或肢体语言，而是通过思想直接交流的能力。Neuralink的技术若能实现类似功能，它将是一个划时代的突破，可能会彻底改变人们之间的沟通方式，使信息交换更为迅速和直接。<br />
<br />
这个名字也意味着Neuralink的产品可能会探索大脑接口技术的边界，尝试连接和翻译人类的神经活动，允许用户传递复杂的思想和概念而无需说话。这不仅对于普通沟通来说是革命性的，而且对于有沟通障碍的人，比如言语受限者，可能是一个巨大的福音。<br />
<br />
此外，“Telepathy”这个名字对长生不老的追求也可能有深远的含义。如果未来的技术能够捕捉、翻译甚至存储人的思想，那么这可能是对人类意识进行数字化和保存的第一步，这是长生不老梦想的关键组成部分<br />
<br />
"Telepathy"这个词源于希腊语中的“tele”（远）和“pathos”（感觉），直译为“远感”，通常被解释为心灵感应或心灵传输。在通常的理解中，它指的是不通过传统感官或物理交流手段，而是通过某种形式的超自然方式进行思想和情感的交流。<br />
<br />
这个概念最早在19世纪晚期成为心灵研究的一个主题，并且常常出现在科幻和奇幻文学中。心灵感应通常被描绘为一种神秘的能力，使人能够跨越物理距离，分享思想和信息。马斯克将Neuralink的产品命名为“Telepathy”，那么很可能是在引用这个概念，以此来传达他的这项技术可以使人类实现类似的直接思想交流的愿景。</p>
<p><a href="https://nitter.cz/elonmusk/status/1752118131579867417#m">nitter.cz/elonmusk/status/1752118131579867417#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752118477949936010#m</id>
            <title>马斯克说的Neuralink植入人体的首次尝试，预示着人类正迈入与机器深度整合的新时代。这样的技术进步意味着我们可能很快就能治疗先前无法治疗的神经系统疾病，如帕金森病、癫痫以及可能的大脑损伤。长期来看，如果这类技术能够进一步发展，它们不仅可能帮助我们恢复失去的感官功能，如视觉和听力，还可能增强我们现有的感官和认知能力。

对于长生不老的概念，Neuralink这类技术可能是一个重要的跳板。通过改善大脑的健康和功能，它有潜力延长健康寿命，甚至有一天可能帮助我们在大脑中上传和存储记忆与意识。虽然实现这些目标仍然面临巨大的技术和伦理挑战，但Neuralink的这一步是开启这一可能性的关键先行。此外，能够检测神经元尖峰活动的初步成功显示了这种接口可能对于理解和解码大脑功能具有重大意义。这些进展不仅有助于医学和健康，也可能在未来对于提高人类寿命和质量有所贡献。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752118477949936010#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752118477949936010#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 23:56:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克说的Neuralink植入人体的首次尝试，预示着人类正迈入与机器深度整合的新时代。这样的技术进步意味着我们可能很快就能治疗先前无法治疗的神经系统疾病，如帕金森病、癫痫以及可能的大脑损伤。长期来看，如果这类技术能够进一步发展，它们不仅可能帮助我们恢复失去的感官功能，如视觉和听力，还可能增强我们现有的感官和认知能力。<br />
<br />
对于长生不老的概念，Neuralink这类技术可能是一个重要的跳板。通过改善大脑的健康和功能，它有潜力延长健康寿命，甚至有一天可能帮助我们在大脑中上传和存储记忆与意识。虽然实现这些目标仍然面临巨大的技术和伦理挑战，但Neuralink的这一步是开启这一可能性的关键先行。此外，能够检测神经元尖峰活动的初步成功显示了这种接口可能对于理解和解码大脑功能具有重大意义。这些进展不仅有助于医学和健康，也可能在未来对于提高人类寿命和质量有所贡献。</p>
<p><a href="https://nitter.cz/elonmusk/status/1752098683024220632#m">nitter.cz/elonmusk/status/1752098683024220632#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752117481131688088#m</id>
            <title>谷歌提出的“学习通用预测器”研究，探索了将所罗门诺夫归纳（Solomonoff Induction, SI）—一种强大的通用预测器—通过极致的元学习（meta-learning）整合进神经网络的潜力。所罗门诺夫归纳是一种理论上的最优预测方法，可以从数据中推断出模式和规律。在这项工作中，使用通用图灵机（UTMs）生成训练数据，让网络接触广泛的模式，提供了对UTM数据生成过程和元训练协议的理论分析，并在神经架构（如LSTMs, Transformers）上进行了广泛实验。

在金融行业，这种通用预测器可以有多方面的应用。例如，在量化交易中，这种预测器可以训练神经网络使用历史市场数据来学习市场行为的普遍模式，从而快速适应并预测未来市场趋势。这可以帮助制定交易策略，实时调整投资组合，甚至进行风险管理和欺诈检测。

以风险管理为例，如果能训练出一个能够预测金融市场波动和异常行为的模型，这将对于银行、投资基金和保险公司来说是极其有价值的。它们可以利用这种模型来预测和缓解潜在的市场风险，例如通过识别可能导致市场崩溃的金融泡沫或不稳定的投资行为。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752117481131688088#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752117481131688088#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 23:52:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌提出的“学习通用预测器”研究，探索了将所罗门诺夫归纳（Solomonoff Induction, SI）—一种强大的通用预测器—通过极致的元学习（meta-learning）整合进神经网络的潜力。所罗门诺夫归纳是一种理论上的最优预测方法，可以从数据中推断出模式和规律。在这项工作中，使用通用图灵机（UTMs）生成训练数据，让网络接触广泛的模式，提供了对UTM数据生成过程和元训练协议的理论分析，并在神经架构（如LSTMs, Transformers）上进行了广泛实验。<br />
<br />
在金融行业，这种通用预测器可以有多方面的应用。例如，在量化交易中，这种预测器可以训练神经网络使用历史市场数据来学习市场行为的普遍模式，从而快速适应并预测未来市场趋势。这可以帮助制定交易策略，实时调整投资组合，甚至进行风险管理和欺诈检测。<br />
<br />
以风险管理为例，如果能训练出一个能够预测金融市场波动和异常行为的模型，这将对于银行、投资基金和保险公司来说是极其有价值的。它们可以利用这种模型来预测和缓解潜在的市场风险，例如通过识别可能导致市场崩溃的金融泡沫或不稳定的投资行为。</p>
<p><a href="https://nitter.cz/_akhaliq/status/1751801040066719953#m">nitter.cz/_akhaliq/status/1751801040066719953#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752115543371657428#m</id>
            <title>构建自主沟通的AI代理：DSPy与未来智能应用编程的变革

这条推文对未来的AI发展预示着重大进步。通过DSPy这样的新型编程模型，可以利用类似PyTorch的方式来精细控制基于LLM构建的程序。DSPy的核心创新在于它的编程模型和编译器。编程模型允许开发者通过“签名”抽象来整理和简化与LLM的交互，使得代码库更加清晰。而编译器能够对LLM程序的各个组成部分进行联合优化，包括为任务寻找示例。

这种技术意味着未来的AI Agent将能够以更加复杂和强大的方式相互交流和交互。LLM调用链的概念使得一个AI的输出可以成为另一个AI的输入，从而形成一个连贯的决策和处理流程。这模拟了人类在处理问题时的逻辑链条：一个解决方案的步骤可以依赖于前一个步骤的结果。

简而言之，这不仅是AI Agent之间交互的未来方式，而且这种方法将允许开发者构建出更加智能、自适应和功能丰富的AI系统。这些系统能够处理更加复杂的任务，实现更深层次的自动化和决策制定。DSPy的出现可能会在AI领域引起一次创新浪潮，带来更加智能的应用程序。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752115543371657428#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752115543371657428#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 23:44:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>构建自主沟通的AI代理：DSPy与未来智能应用编程的变革<br />
<br />
这条推文对未来的AI发展预示着重大进步。通过DSPy这样的新型编程模型，可以利用类似PyTorch的方式来精细控制基于LLM构建的程序。DSPy的核心创新在于它的编程模型和编译器。编程模型允许开发者通过“签名”抽象来整理和简化与LLM的交互，使得代码库更加清晰。而编译器能够对LLM程序的各个组成部分进行联合优化，包括为任务寻找示例。<br />
<br />
这种技术意味着未来的AI Agent将能够以更加复杂和强大的方式相互交流和交互。LLM调用链的概念使得一个AI的输出可以成为另一个AI的输入，从而形成一个连贯的决策和处理流程。这模拟了人类在处理问题时的逻辑链条：一个解决方案的步骤可以依赖于前一个步骤的结果。<br />
<br />
简而言之，这不仅是AI Agent之间交互的未来方式，而且这种方法将允许开发者构建出更加智能、自适应和功能丰富的AI系统。这些系统能够处理更加复杂的任务，实现更深层次的自动化和决策制定。DSPy的出现可能会在AI领域引起一次创新浪潮，带来更加智能的应用程序。</p>
<p><a href="https://nitter.cz/CShorten30/status/1751991624140222516#m">nitter.cz/CShorten30/status/1751991624140222516#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1752114296719569066#m</id>
            <title>这个图展示了不同组织和国家拥有的卫星数量。每个圆圈代表10颗卫星，它们环绕地球展开，显示了SpaceX拥有最多卫星，共计3,395颗。其次是OneWeb、中国、亚马逊的Project Kuiper等。

图中提到，SpaceX的Starlink互联网卫星网络占了在轨活跃卫星总数的一半。还指出，亚马逊计划到2029年部署3,236颗卫星来竞争SpaceX的网络，第一批卫星最早可能在2024年发射。图表下方列出了具体的组织和国家及其拥有的卫星数量，还说明了近7,000颗卫星在地球轨道上执行通信、导航和科学研究等重要功能。图表来源是担心的科学家联盟，由Visual Capitalist制作。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1752114296719569066#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1752114296719569066#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 23:39:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个图展示了不同组织和国家拥有的卫星数量。每个圆圈代表10颗卫星，它们环绕地球展开，显示了SpaceX拥有最多卫星，共计3,395颗。其次是OneWeb、中国、亚马逊的Project Kuiper等。<br />
<br />
图中提到，SpaceX的Starlink互联网卫星网络占了在轨活跃卫星总数的一半。还指出，亚马逊计划到2029年部署3,236颗卫星来竞争SpaceX的网络，第一批卫星最早可能在2024年发射。图表下方列出了具体的组织和国家及其拥有的卫星数量，还说明了近7,000颗卫星在地球轨道上执行通信、导航和科学研究等重要功能。图表来源是担心的科学家联盟，由Visual Capitalist制作。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEQ3FyZWJJQUE3ZUpPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1750714273104556197#m</id>
            <title>RT by @GPTDAOCN: ChatGPT 增加了多语言适配，居然支持了中文。

进去以后会询问是不是要在你的语言中使用 ChatGPT，加入测试后界面就变中文了。

这下舒服了，不知道西班牙语日语之类的是不是也有类似的提示。</title>
            <link>https://nitter.cz/op7418/status/1750714273104556197#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1750714273104556197#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:56:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT 增加了多语言适配，居然支持了中文。<br />
<br />
进去以后会询问是不是要在你的语言中使用 ChatGPT，加入测试后界面就变中文了。<br />
<br />
这下舒服了，不知道西班牙语日语之类的是不是也有类似的提示。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V2SXRtY2JnQUVEdXBfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1750645950090236075#m</id>
            <title>新的GPT Turbor解决了3个问题：

1.  提高了代码生成等任务的完成质量，使得新的GPT-4 Turbo预览模型比之前的版本能更彻底地完成任务。
2.  减少了模型在执行任务时表现出的“懒惰”现象，即避免了模型没有完全完成任务的情况。
3.  修复了影响非英语UTF-8文本生成的错误。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1750645950090236075#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1750645950090236075#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 22:24:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>新的GPT Turbor解决了3个问题：<br />
<br />
1.  提高了代码生成等任务的完成质量，使得新的GPT-4 Turbo预览模型比之前的版本能更彻底地完成任务。<br />
2.  减少了模型在执行任务时表现出的“懒惰”现象，即避免了模型没有完全完成任务的情况。<br />
3.  修复了影响非英语UTF-8文本生成的错误。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V1TE5zWWEwQUVMNlBOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1750643084436885676#m</id>
            <title>R to @GPTDAOCN: 首先，开发者现在可以从API密钥页面为API密钥分配权限。例如，可以为一个密钥分配只读权限，以供内部跟踪仪表板使用，或限制其仅访问某些端点。

其次，在开启追踪后，使用仪表板和使用导出功能现在可以按API密钥级别暴露指标。这使得通过为每个功能、团队、产品或项目使用单独的API密钥，简单地查看每个特性、团队、产品或项目级别的使用情况变得简单。

在未来几个月内，我们计划进一步改进开发者查看其API使用情况和管理API密钥的能力，特别是在更大的组织中。

要获取OpenAI API的最新更新，请在X上关注我们@OpenAIDevs。

这段文字是OpenAI发布的更新通告的翻译，包括了新的嵌入模型的介绍、GPT-3.5 Turbo和GPT-4 Turbo的更新及价格调整、新的文本审核模型的发布，以及关于API使用和管理的新功能。该通告旨在告知开发者他们现在可以使用更高效、成本更低的模型，并提供了新的工具来更好地管理和理解API的使用情况。这些更新预计将对AI开发者社区产生积极影响，特别是在降低成本和提高性能</title>
            <link>https://nitter.cz/GPTDAOCN/status/1750643084436885676#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1750643084436885676#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 22:13:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>首先，开发者现在可以从API密钥页面为API密钥分配权限。例如，可以为一个密钥分配只读权限，以供内部跟踪仪表板使用，或限制其仅访问某些端点。<br />
<br />
其次，在开启追踪后，使用仪表板和使用导出功能现在可以按API密钥级别暴露指标。这使得通过为每个功能、团队、产品或项目使用单独的API密钥，简单地查看每个特性、团队、产品或项目级别的使用情况变得简单。<br />
<br />
在未来几个月内，我们计划进一步改进开发者查看其API使用情况和管理API密钥的能力，特别是在更大的组织中。<br />
<br />
要获取OpenAI API的最新更新，请在X上关注我们<a href="https://nitter.cz/OpenAIDevs" title="OpenAI Developers">@OpenAIDevs</a>。<br />
<br />
这段文字是OpenAI发布的更新通告的翻译，包括了新的嵌入模型的介绍、GPT-3.5 Turbo和GPT-4 Turbo的更新及价格调整、新的文本审核模型的发布，以及关于API使用和管理的新功能。该通告旨在告知开发者他们现在可以使用更高效、成本更低的模型，并提供了新的工具来更好地管理和理解API的使用情况。这些更新预计将对AI开发者社区产生积极影响，特别是在降低成本和提高性能</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1750643082553643137#m</id>
            <title>OpenAI公告产品更新（全文）

我们正在发布新的模型，降低GPT-3.5 Turbo的价格，并引入新的方法供开发者管理API密钥和理解API使用情况。新模型包括：
- 两个新的嵌入模型
- 更新版的GPT-4 Turbo预览模型
- 更新版的GPT-3.5 Turbo模型
- 更新版的文本审核模型
默认情况下，发送到OpenAI API的数据将不会被用于训练或改善OpenAI模型。

新的嵌入模型和更低的价格
我们正在引入两个新的嵌入模型：一个体积小且效率高的text-embedding-3-small模型，和一个更大更强大的text-embedding-3-large模型。
嵌入是一系列数字，代表如自然语言或代码内容中的概念。嵌入让机器学习模型和其他算法容易理解内容之间的关系，执行如聚类或检索等任务。它们支持如ChatGPT和Assistants API中的知识检索，以及许多检索增强型生成（RAG）开发工具的应用。

新的小型文本嵌入模型
text-embedding-3-small是我们新的高效嵌入模型，与其前代模型text-embedding-ada-002（2022年12月发布）相比提供了显著的升级。
- 更强的性能。在多语言检索（MIRACL）常用基准测试中，text-embedding-ada-002与text-embedding-3-small的平均得分从31.4%提高到了44.0%，而在英语任务（MTEB）常用基准测试中，从61.0%提高到了62.3%。
- 降低的价格。text-embedding-3-small比我们之前的一代模型text-embedding-ada-002更高效。因此，text-embedding-3-small的价格比text-embedding-ada-002降低了5倍，从每1k令牌的$0.0001降低到$0.00002。
我们不会淘汰text-embedding-ada-002，所以虽然我们推荐新模型，客户仍然可以继续使用前代模型。

新的大型文本嵌入模型：text-embedding-3-large
text-embedding-3-large是我们新的下一代大型嵌入模型，能创建多达3072维度的嵌入。
- 更强的性能。text-embedding-3-large是我们性能最好的模型。在MIRACL上，与text-embedding-ada-002相比，平均得分从31.4%提高到了54.9%，而在MTEB上，平均得分从61.0%提高到了64.6%。
- text-embedding-3-large的价格是每1k令牌$0.00013。
您可以在我们的嵌入指南中了解更多关于使用新嵌入模型的信息。

对短嵌入的原生支持
使用较大的嵌入，例如将它们存储在向量存储中以便检索，通常比使用较小的嵌入更昂贵，消耗更多的计算、内存和存储资源。
我们的两个新嵌入模型都是用一种技术训练的，允许开发者在使用嵌入时权衡性能和成本。具体来说，开发者可以通过在dimensions API参数中传递来缩短嵌入（即从序列的末尾移除一些数字），而不会失去其代表概念的属性。例如，在MTEB基准测试上，一个text-embedding-3-large嵌入可以缩短到256的大小，仍然表现优于未缩短的text-embedding-ada-002嵌入，大小为1536。

这使得使用非常灵活。例如，当使用一个只支持最长1024维度的向量数据存储时，开发者现在仍然可以使用我们最好的嵌入模型text-embedding-3-large，并为dimensions API参数指定一个值为1024，这将把嵌入从3072维度缩短，以交换较小的向量大小来获得一些准确性。

其他新模型和降价
更新的GPT-3.5 Turbo模型和降价
下周我们将推出新的GPT-3.5 Turbo模型，gpt-3.5-turbo-0125，并在过去一年中第三次降低GPT-3.5 Turbo的价格，以帮助我们的客户扩展规模。新模型的输入价格降低了50%，至每1000个令牌$0.0005，输出价格降低了25%，至每1000个令牌$0.0015。该模型还包括各种改进，包括在请求的格式中响应的准确性更高，以及修复了导致非英语语言函数调用文本编码问题的bug。

使用固定的gpt-3.5-turbo模型别名的客户将在该模型发布两周后自动从gpt-3.5-turbo-0613升级到gpt-3.5-turbo-0125。

更新的GPT-4 Turbo预览
自GPT-4 Turbo发布以来，超过70%的GPT-4 API客户请求已经转移到GPT-4 Turbo，因为开发者利用了其更新的知识截止日期、更大的128k上下文窗口和更低的价格。

今天，我们正在发布更新的GPT-4 Turbo预览模型，gpt-4-0125-preview。与之前的预览模型相比，这个模型可以更彻底地完成诸如代码生成之类的任务，旨在减少模型未完成任务的“懒惰”情况。新模型还包括修复影响非英语UTF-8生成的bug。

对于希望自动升级到新GPT-4 Turbo预览版本的用户，我们还推出了一个新的gpt-4-turbo-preview模型名称别名，它将始终指向我们最新的GPT-4 Turbo预览模型。

我们计划在未来几个月内推出具备视觉功能的GPT-4 Turbo。

更新的审查模型
免费的审核API允许开发者识别可能有害的文本。作为我们持续的安全工作的一部分，我们正在发布text-moderation-007，我们迄今为止最健壮的审核模型。text-moderation-latest和text-moderation-stable别名已更新，指向它。您可以通过我们的安全最佳实践指南了解更多关于构建安全AI系统的信息。

理解API使用情况和管理API密钥的新方法
我们正在推出两项平台改进，为开发者提供更多的使用情况可见性和对API密钥的控制。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1750643082553643137#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1750643082553643137#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 22:13:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI公告产品更新（全文）<br />
<br />
我们正在发布新的模型，降低GPT-3.5 Turbo的价格，并引入新的方法供开发者管理API密钥和理解API使用情况。新模型包括：<br />
- 两个新的嵌入模型<br />
- 更新版的GPT-4 Turbo预览模型<br />
- 更新版的GPT-3.5 Turbo模型<br />
- 更新版的文本审核模型<br />
默认情况下，发送到OpenAI API的数据将不会被用于训练或改善OpenAI模型。<br />
<br />
新的嵌入模型和更低的价格<br />
我们正在引入两个新的嵌入模型：一个体积小且效率高的text-embedding-3-small模型，和一个更大更强大的text-embedding-3-large模型。<br />
嵌入是一系列数字，代表如自然语言或代码内容中的概念。嵌入让机器学习模型和其他算法容易理解内容之间的关系，执行如聚类或检索等任务。它们支持如ChatGPT和Assistants API中的知识检索，以及许多检索增强型生成（RAG）开发工具的应用。<br />
<br />
新的小型文本嵌入模型<br />
text-embedding-3-small是我们新的高效嵌入模型，与其前代模型text-embedding-ada-002（2022年12月发布）相比提供了显著的升级。<br />
- 更强的性能。在多语言检索（MIRACL）常用基准测试中，text-embedding-ada-002与text-embedding-3-small的平均得分从31.4%提高到了44.0%，而在英语任务（MTEB）常用基准测试中，从61.0%提高到了62.3%。<br />
- 降低的价格。text-embedding-3-small比我们之前的一代模型text-embedding-ada-002更高效。因此，text-embedding-3-small的价格比text-embedding-ada-002降低了5倍，从每1k令牌的$0.0001降低到$0.00002。<br />
我们不会淘汰text-embedding-ada-002，所以虽然我们推荐新模型，客户仍然可以继续使用前代模型。<br />
<br />
新的大型文本嵌入模型：text-embedding-3-large<br />
text-embedding-3-large是我们新的下一代大型嵌入模型，能创建多达3072维度的嵌入。<br />
- 更强的性能。text-embedding-3-large是我们性能最好的模型。在MIRACL上，与text-embedding-ada-002相比，平均得分从31.4%提高到了54.9%，而在MTEB上，平均得分从61.0%提高到了64.6%。<br />
- text-embedding-3-large的价格是每1k令牌$0.00013。<br />
您可以在我们的嵌入指南中了解更多关于使用新嵌入模型的信息。<br />
<br />
对短嵌入的原生支持<br />
使用较大的嵌入，例如将它们存储在向量存储中以便检索，通常比使用较小的嵌入更昂贵，消耗更多的计算、内存和存储资源。<br />
我们的两个新嵌入模型都是用一种技术训练的，允许开发者在使用嵌入时权衡性能和成本。具体来说，开发者可以通过在dimensions API参数中传递来缩短嵌入（即从序列的末尾移除一些数字），而不会失去其代表概念的属性。例如，在MTEB基准测试上，一个text-embedding-3-large嵌入可以缩短到256的大小，仍然表现优于未缩短的text-embedding-ada-002嵌入，大小为1536。<br />
<br />
这使得使用非常灵活。例如，当使用一个只支持最长1024维度的向量数据存储时，开发者现在仍然可以使用我们最好的嵌入模型text-embedding-3-large，并为dimensions API参数指定一个值为1024，这将把嵌入从3072维度缩短，以交换较小的向量大小来获得一些准确性。<br />
<br />
其他新模型和降价<br />
更新的GPT-3.5 Turbo模型和降价<br />
下周我们将推出新的GPT-3.5 Turbo模型，gpt-3.5-turbo-0125，并在过去一年中第三次降低GPT-3.5 Turbo的价格，以帮助我们的客户扩展规模。新模型的输入价格降低了50%，至每1000个令牌$0.0005，输出价格降低了25%，至每1000个令牌$0.0015。该模型还包括各种改进，包括在请求的格式中响应的准确性更高，以及修复了导致非英语语言函数调用文本编码问题的bug。<br />
<br />
使用固定的gpt-3.5-turbo模型别名的客户将在该模型发布两周后自动从gpt-3.5-turbo-0613升级到gpt-3.5-turbo-0125。<br />
<br />
更新的GPT-4 Turbo预览<br />
自GPT-4 Turbo发布以来，超过70%的GPT-4 API客户请求已经转移到GPT-4 Turbo，因为开发者利用了其更新的知识截止日期、更大的128k上下文窗口和更低的价格。<br />
<br />
今天，我们正在发布更新的GPT-4 Turbo预览模型，gpt-4-0125-preview。与之前的预览模型相比，这个模型可以更彻底地完成诸如代码生成之类的任务，旨在减少模型未完成任务的“懒惰”情况。新模型还包括修复影响非英语UTF-8生成的bug。<br />
<br />
对于希望自动升级到新GPT-4 Turbo预览版本的用户，我们还推出了一个新的gpt-4-turbo-preview模型名称别名，它将始终指向我们最新的GPT-4 Turbo预览模型。<br />
<br />
我们计划在未来几个月内推出具备视觉功能的GPT-4 Turbo。<br />
<br />
更新的审查模型<br />
免费的审核API允许开发者识别可能有害的文本。作为我们持续的安全工作的一部分，我们正在发布text-moderation-007，我们迄今为止最健壮的审核模型。text-moderation-latest和text-moderation-stable别名已更新，指向它。您可以通过我们的安全最佳实践指南了解更多关于构建安全AI系统的信息。<br />
<br />
理解API使用情况和管理API密钥的新方法<br />
我们正在推出两项平台改进，为开发者提供更多的使用情况可见性和对API密钥的控制。</p>
<p><a href="https://nitter.cz/OpenAI/status/1750636119321120942#m">nitter.cz/OpenAI/status/1750636119321120942#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1750618853581967550#m</id>
            <title>OpenAI Developers宣布了两项模型更新：
1、GPT-4 Turbo的更新预览，声称改进了代码生成等任务的完成效果，并修复了与UTF-8编码相关的一个错误；
2、新的GPT-3.5 Turbo将在下周推出，输入价格降低了50%，输出价格降低了25%。

OpenAI正在改进他们的人工智能模型，使它们更好、更便宜地完成任务，比如写代码。他们还在解决一些技术问题，让AI处理文本的能力更强。

这样的更新可能会让AI更加普及和可用，因为成本降低了，而且效率和准确性都有所提高。这可能会激励更多的公司和开发者使用AI来创新和优化他们的产品和服务。

长远来看，这可能加速各行各业的自动化进程，增加对AI技能的需求，并可能引发对AI伦理和就业影响的进一步讨论。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1750618853581967550#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1750618853581967550#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 20:37:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI Developers宣布了两项模型更新：<br />
1、GPT-4 Turbo的更新预览，声称改进了代码生成等任务的完成效果，并修复了与UTF-8编码相关的一个错误；<br />
2、新的GPT-3.5 Turbo将在下周推出，输入价格降低了50%，输出价格降低了25%。<br />
<br />
OpenAI正在改进他们的人工智能模型，使它们更好、更便宜地完成任务，比如写代码。他们还在解决一些技术问题，让AI处理文本的能力更强。<br />
<br />
这样的更新可能会让AI更加普及和可用，因为成本降低了，而且效率和准确性都有所提高。这可能会激励更多的公司和开发者使用AI来创新和优化他们的产品和服务。<br />
<br />
长远来看，这可能加速各行各业的自动化进程，增加对AI技能的需求，并可能引发对AI伦理和就业影响的进一步讨论。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V0eWtheGFvQUF3NUhiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1750616785039315375#m</id>
            <title>OpenAI 宣布了新一代嵌入模型：

- text-embedding-3-small：与上一代相比，成本降低了5倍，性能有显著提升。
- text-embedding-3-large：我们表现最佳的模型，可以创建最高达3072维的嵌入。

在机器学习和人工智能中，"3072维嵌入"指的是模型生成的一个3072维的向量，用于表示文本数据。"维度"在这里指的是向量中的元素数量，每个元素都是文本数据的一个特征。

这样高维的向量可以更丰富和详细地捕捉到文本的语义和语法特点，从而在进行机器学习任务时，比如文本分类、搜索、相似性比较时能够提供更好的性能。高维嵌入通常能够捕捉更多的细微差别和语境信息。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1750616785039315375#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1750616785039315375#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 20:29:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 宣布了新一代嵌入模型：<br />
<br />
- text-embedding-3-small：与上一代相比，成本降低了5倍，性能有显著提升。<br />
- text-embedding-3-large：我们表现最佳的模型，可以创建最高达3072维的嵌入。<br />
<br />
在机器学习和人工智能中，"3072维嵌入"指的是模型生成的一个3072维的向量，用于表示文本数据。"维度"在这里指的是向量中的元素数量，每个元素都是文本数据的一个特征。<br />
<br />
这样高维的向量可以更丰富和详细地捕捉到文本的语义和语法特点，从而在进行机器学习任务时，比如文本分类、搜索、相似性比较时能够提供更好的性能。高维嵌入通常能够捕捉更多的细微差别和语境信息。</p>
<p><a href="https://nitter.cz/OpenAIDevs/status/1750589056923373834#m">nitter.cz/OpenAIDevs/status/1750589056923373834#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GPTDAOCN/status/1750608854206042297#m</id>
            <title>AI应该彻底颠覆我们的教育体系

现代社会要求我们在进入职场之前接受近16年的“教育和培训”。

我们在学校和大学里学到的80%以上的内容在实际生活中从未应用，对大多数孩子来说似乎毫无意义。

随着AI的出现，几乎所有问题都可以在几分钟内查找或解决，这简直是严重的过度杀伤力。

我们的教育体系需要严肃的改革。重点应该是教授基础知识，然后教会学生如何应用AI工具。

当前对竞争性考试的关注相当无用。大部分测试都是学术性质的，而且毫无意义的记忆测试。

我们不需要像现在这样浪费太多时间和资源来为我们的工作做准备。相反，让“大学”在15岁结束并让学生做一系列的长期全职实习，让他们处理现实世界的问题和应用AI，会更有意义。实习可以顺利过渡到全职职位。

AI的出现对传统教育体系构成了挑战，揭示了它的不足之处。教育体系在内容和方法上的僵化不利于学生准备进入一个由技术快速驱动的工作环境。学生花费多年学习的内容与实际工作需求脱节，导致了资源的浪费和实践技能的缺失。

因此，教育体系需要重视培养学生的实际解决问题能力，特别是在利用AI等先进工具方面的能力。结合实习和项目式学习，可以更好地使学生适应未来的工作要求，并平滑过渡到职业角色。这样的教育改革不仅关注知识的传授，更关注能力的培养和创新思维的激发。</title>
            <link>https://nitter.cz/GPTDAOCN/status/1750608854206042297#m</link>
            <guid isPermaLink="false">https://nitter.cz/GPTDAOCN/status/1750608854206042297#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 19:57:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI应该彻底颠覆我们的教育体系<br />
<br />
现代社会要求我们在进入职场之前接受近16年的“教育和培训”。<br />
<br />
我们在学校和大学里学到的80%以上的内容在实际生活中从未应用，对大多数孩子来说似乎毫无意义。<br />
<br />
随着AI的出现，几乎所有问题都可以在几分钟内查找或解决，这简直是严重的过度杀伤力。<br />
<br />
我们的教育体系需要严肃的改革。重点应该是教授基础知识，然后教会学生如何应用AI工具。<br />
<br />
当前对竞争性考试的关注相当无用。大部分测试都是学术性质的，而且毫无意义的记忆测试。<br />
<br />
我们不需要像现在这样浪费太多时间和资源来为我们的工作做准备。相反，让“大学”在15岁结束并让学生做一系列的长期全职实习，让他们处理现实世界的问题和应用AI，会更有意义。实习可以顺利过渡到全职职位。<br />
<br />
AI的出现对传统教育体系构成了挑战，揭示了它的不足之处。教育体系在内容和方法上的僵化不利于学生准备进入一个由技术快速驱动的工作环境。学生花费多年学习的内容与实际工作需求脱节，导致了资源的浪费和实践技能的缺失。<br />
<br />
因此，教育体系需要重视培养学生的实际解决问题能力，特别是在利用AI等先进工具方面的能力。结合实习和项目式学习，可以更好地使学生适应未来的工作要求，并平滑过渡到职业角色。这样的教育改革不仅关注知识的传授，更关注能力的培养和创新思维的激发。</p>
<p><a href="https://nitter.cz/bindureddy/status/1750231247018975469#m">nitter.cz/bindureddy/status/1750231247018975469#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>