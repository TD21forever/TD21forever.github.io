<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/3095112547268736</id>
            <title>苹果为何一直拒用英伟达？</title>
            <link>https://www.36kr.com/p/3095112547268736</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3095112547268736</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Dec 2024 04:18:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 英伟达, 苹果, GPU  
<br><br>  
总结: 苹果与英伟达的关系经历了从合作到对立的转变，最初的“蜜月期”因技术争议和质量问题而破裂。苹果追求自主研发和完整生态系统，减少对外部供应商的依赖，尤其是在AI芯片领域。尽管英伟达的GPU性能强大，但其高功耗和发热量与苹果的设计理念相悖。苹果通过租赁GPU、与AMD合作以及开发自有AI服务器芯片等方式，努力摆脱对英伟达的依赖。尽管如此，短期内双方的竞合关系仍将持续。 </div>
                        <hr>
                    
                    <p>AI爆发的时代，英伟达凭借其强大的GPU芯片几乎垄断了AI芯片市场，成为众多科技巨头竞相追逐的合作伙伴。</p>
  <p>然而，苹果公司却始终与英伟达保持着微妙的距离，甚至可以说是在刻意回避。事实上，2000年代时，双方曾有过短暂的“蜜月期”，但随着时间推移，双方的矛盾不断加剧。</p>
  <p>这不禁引发人们的好奇：苹果为何一直拒用英伟达？这背后究竟隐藏着怎样的“恩怨情仇”和战略考量？</p>
  <p>苹果一直力求打造完整的生态系统，而大量采购英伟达的GPU，无疑会削弱苹果在AI领域的主导权。为了摆脱对英伟达的依赖，苹果采取了多种策略。</p>
  <p>但随着AI角力的深入，苹果面临着训练更大更好的模型的压力，这将需要更多的高端GPU。短期内，双方的竞合关系可能仍将存在。</p>
  <h2><strong>历史恩怨：从“蜜月期”到“冰河期”</strong></h2>
  <p>苹果与英伟达的合作并非一开始就充满敌意。早在2001年，苹果就曾在其Mac电脑中采用英伟达的芯片，以提升图形处理能力。当时，双方关系良好，甚至可以用“蜜月期”来形容。</p>
  <p>然而，这段蜜月期并没有持续太久。</p>
  <p>双方关系出现裂痕的第一个重要事件，发生在2000年代中期。当时，史蒂夫·乔布斯曾公开指责英伟达窃取皮克斯动画工作室（当时乔布斯是其主要股东）的技术，这无疑给双方关系蒙上了一层阴影。</p>
  <p>2008年，双方紧张关系进一步加剧。当时，英伟达生产的一批存在缺陷的GPU芯片被应用在包括苹果MacBook Pro在内的多款笔记本电脑中，引发了大规模的质量问题，被称为“bumpgate”事件。</p>
  <p>英伟达最初拒绝承担全部责任和赔偿，这激怒了苹果，也直接导致了双方合作关系的破裂。苹果不得不延长受影响MacBook的保修期，并承受了巨大的经济和声誉损失。</p>
  <p>据The Information援引苹果内部人士透露，英伟达高管长期以来将苹果视为一个“要求苛刻”且“利润微薄”的客户，不愿为其投入过多资源。而苹果在iPod成功后，也变得更加强势，认为英伟达难以合作。此外，英伟达试图对苹果移动设备使用的图形芯片收取许可费，也进一步激化了矛盾。</p>
  <h2><strong>商业与技术策略的博弈</strong></h2>
  <p>除了历史恩怨，苹果拒用英伟达也与其一贯的商业策略密切相关。</p>
  <p>苹果一直强调对产品软硬件的全面掌控，力求打造完整的生态系统。为了实现这一目标，苹果不断加强自主研发能力，减少对外部供应商的依赖。</p>
  <p>在芯片领域，苹果更是走在了行业前列。从iPhone的A系列芯片，到Mac的M系列芯片，苹果不断推出性能卓越的自研芯片，逐渐摆脱了对英特尔等传统芯片巨头的依赖。在这种背景下，苹果自然也不愿意在AI芯片领域受制于英伟达。</p>
  <p>苹果希望拥有对关键技术的完全控制权，以确保产品性能的优化和差异化竞争优势。而大量采购英伟达的GPU，无疑会削弱苹果在AI领域的主导权，使其在产品创新和技术路线上受到限制。</p>
  <p>此外，英伟达的GPU虽然性能强大，但也存在功耗高、发热量大的问题，这对于追求轻薄便携的苹果产品来说是一个不小的挑战。苹果一直致力于将产品做得更轻、更薄、更高效，而英伟达的GPU在一定程度上与其设计理念相悖。</p>
  <p>苹果曾多次要求英伟达为其MacBook定制低功耗、低发热的GPU芯片，但未能如愿。这促使苹果转向了AMD，并与其合作开发定制的图形芯片。虽然AMD的芯片在性能上略逊于英伟达，但其在功耗和散热方面的表现更符合苹果的需求。</p>
  <h2><strong>AI浪潮下的新挑战</strong></h2>
  <p>近年来，人工智能技术的爆发式发展给苹果带来了新的挑战。为了在AI领域保持竞争力，苹果需要训练更大规模、更复杂的AI模型，这无疑需要更强大的计算能力和更多的GPU资源。</p>
  <p>为了摆脱对英伟达的依赖，苹果采取了多管齐下的策略。</p>
  <p>首先，苹果主要通过亚马逊和微软等云服务提供商租赁英伟达的GPU，而不是大量购买。这种方式可以避免大量资金投入和长期依赖。</p>
  <p>其次，苹果曾使用AMD的图形芯片，并与谷歌合作使用其TPU（张量处理单元）进行AI模型训练。</p>
  <p>此外，苹果正在与博通合作开发自己的AI服务器芯片，代号为“Baltra”，预计到2026年可投入量产。这款芯片不仅用于推理，还有可能用于训练AI模型。</p>
  <p>尽管苹果一直努力摆脱对英伟达的依赖，但在短期内，双方的竞合关系可能仍将长期存在。掌握核心技术，才能在激烈的市场竞争中立于不败之地。</p>
  <p>本文不构成个人投资建议，不代表平台观点，市场有风险，投资需谨慎，请独立判断和决策。</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/BpDKEb4jmy_IcrSCe2hrUw" rel="noopener noreferrer nofollow" target="_blank">“华尔街见闻”</a>，作者：张雅琦，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3095158054336643</id>
            <title>微软开源视频Tokenizer新SOTA，显著优于Cosmos Tokenizer和Open-Sora</title>
            <link>https://www.36kr.com/p/3095158054336643</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3095158054336643</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Dec 2024 04:17:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <VidTok, Tokenizer, 视频生成, 开源>
<br>
<br>
总结: 微软开源了一款名为VidTok的视频Tokenizer，旨在将高维视频数据转换为紧凑的视觉Token，以提高视频生成模型的性能。VidTok在多种设定下的性能显著优于现有的SOTA模型，支持多样化的隐空间和灵活的压缩率，适用于不同的使用需求。该模型采用创新的混合架构设计、先进的量化技术和增强的训练策略，显著降低了计算成本，同时保持了高质量的重建效果。VidTok的开源为视频生成和世界模型领域提供了新的工具，支持后续微调以优化特定领域的性能。 </div>
                        <hr>
                    
                    <p>Sora、Genie等模型会都用到的Tokenizer，微软下手了——</p>
  <p>开源了一套全能的Video Tokenizer，名为<strong>VidTok</strong>。</p>
  <p>Sora等视频生成模型工作中，都会利用Tokenizer将原始的高维视频数据（如图像和视频帧）转换为更为紧凑的视觉Token，再以视觉Token为目标训练生成模型。</p>
  <p>而最新的VidTok，<strong>在连续和离散、不同压缩率等多种设定下</strong>，各项指标均显著优于SOTA模型。</p>
  <p>以下是涵盖PSNR、SSIM、FVD、LPIPS指标的性能比较雷达图，面积越大表示性能越好。</p>
  <p>从图中可以看出对于离散Tokenizer，VidTok显著优于英伟达Cosmos Tokenizer；对于连续Tokenizer，VidTok也比Open-Sora、CogVideoX有更高的性能。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_7482cfc7605947738300d5a9b12e9f00@5888275_oswg250852oswg1080oswg346_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>这项研究由来自微软亚研院、上海交通大学、北京大学的研究人员共同完成。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_f0a309fabda34fc59c9c0615f453534b@5888275_oswg41757oswg1080oswg402_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>目前，VidTok代码不仅开源了，还支持用户在自定义数据集上的微调，为研究者和开发者提供了一个高性能、易用的工具平台。</p>
  <h2><strong>性能全面领先，适用各种场景</strong></h2>
  <p>近年来，视频生成以及基于此的世界模型已经成为人工智能领域的热门研究方向，这两者的核心在于对视频内容的高效建模。</p>
  <p>视频中蕴含了丰富的视觉信息，不仅能够提供真实的视觉体验，更能作为具身场景中模型理解世界的中间媒介。</p>
  <p>然而，由于视频像素级表示信息高度冗余，如何通过Tokenizer对视频数据进行高效压缩和表示成为关键课题。</p>
  <p>当下很多工作如Sora，Genie等都会通过Tokenizer将原始的高维视频数据（如图像和视频帧）转换为更为紧凑的视觉Token，再以视觉Token为目标训练生成模型。</p>
  <p>可以说，视觉Token的表示能力对于最终的效果至关重要，甚至决定了模型能力的上限。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_a102ad6eb27349b0880ab4668d500794@5888275_oswg189342oswg1080oswg334_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>Tokenizer的主要作用是将高维的原始数据转换为隐空间中高效的压缩表示，使得信息的生成和处理可以在该隐空间中进行。上图展示了一个视频的Token化过程，通过转换为Token建模，能够有效降低模型训练和推理时的计算需求。</p>
  <p>根据不同的使用需求，视频Tokenizer通常有如下分类：</p>
  <ul>
   <li><strong>连续型和离散型。</strong>根据隐空间的数值分布，Tokenizer可以分为连续型和离散型，分别适用于从连续分布中采样的模型（如扩散模型等）和从离散分布中采样的模型（如语言模型等）。</li>
   <li><strong>因果型和非因果型。</strong>因果结构使得模型只依赖历史帧来对当前帧进行Tokenization，这与真实世界系统的因果性质保持一致。非因果模型则可以同时根据历史帧和未来帧对当前帧进行Tokenization，通常具有更优的重建质量。</li>
   <li><strong>不同的压缩率模型。</strong>Sora等众多工作采用了如4x8x8的视频压缩率（时间压缩4倍、空间压缩8倍），实现更高的视频压缩率而保持高质量的视频重建是目前的研究趋势。</li>
  </ul>
  <p>目前业界领先的视频模型多为闭源状态，而<strong>开源的视频Tokenizer大多受限于单一的模型设定或欠佳的重建质量</strong>，导致可用性较差。</p>
  <p>由此，来自微软亚研院、上海交通大学和北京大学的研究人员最近正式发布了开源视频Tokenizer——VidTok。</p>
  <p>在测试中，VidTok性能全面领先，适用各种场景。</p>
  <p>如下表所示，VidTok<strong>支持多样化的隐空间且具有灵活的压缩率</strong>，同时<strong>支持因果和非因果模型</strong>，以适应不同的使用需求。</p>
  <ul>
   <li>对于连续型Tokenizer，支持不同的视频压缩率、不同的隐空间通道数，同时支持因果和非因果模型。</li>
   <li>对于离散型Tokenizer，支持不同的视频压缩率、不同的码本大小，同时支持因果和非因果模型。</li>
  </ul>
  <p>更多模型在持续更新中。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_01c2177417024f79b8a785ec76814d68@5888275_oswg114486oswg985oswg361_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>为了全面评估VidTok在各个设定下的重建性能，作者将VidTok与最先进的连续和离散视频Tokenizer分别进行了对齐设定下的比较。所有模型均为4x8x8倍视频压缩率的因果模型，主要包含以下三种设定：</p>
  <ul>
   <li><strong>VidTok-FSQ：</strong>离散型，码本大小各异。基线方法包括MAGVIT-v2，OmniTokenizer，Cosmos-DV等。</li>
   <li><strong>VidTok-KL-4chn：</strong>连续型，隐空间通道数为4。基线方法包括CV-VAE，Open-Sora-v1.2，Open-Sora-Plan-v1.2等。</li>
   <li><strong>VidTok-KL-16chn：</strong>连续型，隐空间通道数为16。基线方法包括CogVideoX，Cosmos-CV等。</li>
  </ul>
  <p>定量实验结果表明，VidTok在上述三种设定下均达到了SOTA性能，在常见的视频质量评估指标PSNR、SSIM、FVD、LPIPS上具有全面的优势。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_5bb1cc21f4554d97b344d22e3fcf8718@5888275_oswg119220oswg960oswg394_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>与现有的离散Tokenizer相比，VidTok即使在使用更小的码本大小时（例如32,768），也展现出了更优的重建性能。</p>
  <p>在连续Tokenizer的设定下，无论隐空间通道数是4还是16，VidTok在所有评估指标上相比基线方法均取得了全面的提升。值得注意的是，这些提升是在没有模型大小优势的情况下达成的。</p>
  <p>除此之外，团队还进行了定性分析。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_f299cb6573e3408897a781f4d3458061@5888275_oswg864878oswg1080oswg531_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>上图中展示了以上三种设定下的与基线方法的对比结果。</p>
  <p>从视频帧的重建质量可以看出，与现有的方法相比，VidTok在各种模型设定下，均展现出了最优的重建细节保真度和主观视觉质量。证明了VidTok作为多功能视频Tokenizer的有效性。</p>
  <p>所以VidTok是如何做到的？</p>
  <h2><strong>VidTok的技术亮点解析</strong></h2>
  <p>相对于现有的视频Tokenizer，VidTok在模型架构、量化技术、训练策略上分别做了创新。</p>
  <p><strong>高效的混合模型架构设计</strong></p>
  <p>VidTok采用经典的3D编码器-解码器结构，同时创新性地结合了3D、2D和1D卷积，有效地解耦空间和时间采样。</p>
  <p>在现有研究中普遍认为，尽管计算成本较高，完全的3D架构提供了更优的重建质量。然而，VidTok发现将部分3D卷积替换为2D和1D卷积的组合，可以有效地解耦空间和时间采样，在降低计算需求的同时，保持了高水平的重建质量。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_1d01fb3ddb5943b0a70f2bc65580d4ac@5888275_oswg165962oswg1080oswg447_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>详细的网络架构如上图所示。VidTok分别处理空间采样和时间采样模块，并在时间采样模块中引入了AlphaBlender操作符。其余组件，包括输入/输出层和瓶颈层，则利用3D卷积来促进信息融合。此外，整个架构中引入了层归一化以增强稳定性和性能。实验证明该架构在重建质量和计算量之间取得了平衡。</p>
  <h3><strong>先进的量化技术</strong></h3>
  <p>VidTok引入了有限标量量化（FSQ）技术，无需显式学习码本，显著提高了模型的训练稳定性和重建性能。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_cd622b7954f44b99932d573204192573@5888275_oswg100006oswg1080oswg229_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>有限标量量化（FSQ）由「Finite scalar quantization: Vq-vae made simple」提出，其核心原理是，在隐空间表征中，每个标量条目通过四舍五入独立量化到最近的预定义标量值。</p>
  <p>与传统VQ相比，FSQ无需学习显式的码本，从而提高了训练的稳定性。实验表明，FSQ在码本利用率、重建质量和训练稳定性方面具有显著优势，作为一种先进的量化技术，有效提升了离散Tokenizer的性能。</p>
  <h3><strong>增强的训练策略</strong></h3>
  <p>VidTok采用分阶段训练策略，训练时间减少了50%，而重建质量不受影响。</p>
  <p>视频Tokenizer的训练通常是计算密集的，要求大量计算资源（例如对于256x256分辨率的视频需要3,072GPU小时的训练时长）。这就需要开发有效的策略来降低计算成本，同时保持模型性能。</p>
  <p>VidTok采用一种两阶段训练方法来应对这一挑战：首先在低分辨率视频上对完整模型进行预训练，然后仅在高分辨率视频上微调解码器。这种训练策略显著降低了计算成本——训练时间减少了一半（从3,072GPU小时降至1,536GPU 小时），而保持重建视频质量不变。</p>
  <p>该两阶段训练的另一优势是，由于第二阶段只会微调解码器，因此模型可以快速适应到新的领域数据中，而不会影响隐空间数据分布。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_204f73472cfb417a976ca62a39caffd9@5888275_oswg37285oswg859oswg127_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>此外，由于视频Tokenizer旨在建模输入视频的运动动态，因此在模型中有效表示这些动态至关重要。VidTok使用较低帧率的数据进行训练，显著增强了模型捕捉和表示运动动态的能力，获得了更好的重建质量。</p>
  <p>VidTok的开源为视频生成、世界模型领域提供了新的工具，特别是在当前业内许多领先模型仍未开源的背景下。</p>
  <p>团队表示，VidTok支持后续微调也为其他应用提供了更广阔的使用空间，研究者可轻松将VidTok应用于特定领域数据集，为目标场景优化性能。</p>
  <p>更多细节内容感兴趣的童鞋可参阅原论文。</p>
  <p>论文地址：https://arxiv.org/abs/2412.13061</p>
  <p>项目地址：https://github.com/microsoft/vidtok</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/-b5yhTWX9KUBlOpFJT39ZA" rel="noopener noreferrer nofollow" target="_blank">“量子位”</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>