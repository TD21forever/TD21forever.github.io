<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/2990340432210689</id>
            <title>苏姿丰2小时激情演讲，发布AMD最强AI芯片，旗舰CPU单颗10万，OpenAI微软都来站台</title>
            <link>https://www.36kr.com/p/2990340432210689</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2990340432210689</guid>
            <pubDate></pubDate>
            <updated>Sun, 13 Oct 2024 02:55:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>猛，实在是猛！就在今日，老牌芯片巨头AMD交出了一份令人印象深刻的AI答卷。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_338ec62092b342fe80fae403ee4da56e@5595930_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>智东西美国旧金山10月10日现场报道，酷热的天气刚刚过去，旧金山正值秋意凉爽，今日举行的AMD Advancing AI 2024盛会却格外火热。&nbsp;</p>
  <p>AMD倾囊倒出了一系列AI杀手锏，发布<strong>全新旗舰AI芯片、服务器CPU、AI网卡、DPU和AI PC移动处理器</strong>，将AI计算的战火烧得更旺。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_229115412108488d9d036a519686e9e9@5595930_oswg77120oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>这家芯片巨头还大秀AI朋友圈，现场演讲集齐了<strong>谷歌、OpenAI、微软、Meta、xAI、Cohere、RekaAI</strong>等重量级AI生态伙伴。&nbsp;</p>
  <p>备受期待的旗舰AI芯片<strong>AMD Instinct MI325X GPU</strong>首次启用<strong>HBM3E</strong>高带宽内存，8卡AI峰值算力达到<strong>21PFLOPS</strong>，并与去年发布的、同样采用HBM3E的<strong>英伟达H200 GPU用数据掰手腕</strong>：<strong>内存容量是H200的1.8倍，内存带宽、FP16和FP8峰值理论算力都是H200的1.3倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_bbfa69cbbbbd4479947d96905af2d8a8@5595930_oswg300688oswg1000oswg561_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>AMD还披露了最新的AI芯片路线图，采用<strong>CDNA 4架构</strong>的<strong>MI350系列明年上市</strong>，其中8卡MI355X的AI峰值算力达到<strong>74PFLOPS</strong>，MI400系列将采用<strong>更先进的CDNA架构</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_4de3bf1edb804ea7a10d5d6141c4e0a1@5595930_oswg84741oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>更高的数据中心算力，离不开先进的网络解决方案。对此，AMD发布了<strong>业界首款支持UEC超以太网联盟的AI网卡Pensando Pollara 400</strong>和性能翻倍提升的<strong>Pensando Salina 400 DPU</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_017638c2554d4627948ee32d80f40405@5595930_oswg99846oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>另一款重磅新品是<strong>第五代EPYC服务器CPU</strong>，被AMD称为“面向云计算、企业级和AI的全球最好CPU”，采用台积电3/4nm制程工艺，最多支持<strong>192核</strong>、<strong>384个线程</strong>。其中顶配EPYC 9965默认热设计功耗500W，以1000颗起订的单价为14813美元（约合人民币10万元）。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_7267524cbdb748d7b0d8eb6bfa21945c@5595930_oswg657796oswg1000oswg907_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>与第五代英特尔至强铂金8592+处理器相比，AMD EPYC 9575F处理器的SPEC CPU性能提高多达<strong>2.7倍</strong>，企业级性能提高多达<strong>4.0倍</strong>，HPC（高性能计算）性能提高多达<strong>3.9倍</strong>，基于CPU的AI加速提高多达<strong>3.8倍</strong>，GPU主机节点提升多达<strong>1.2倍</strong>。&nbsp;</p>
  <p>自2017年重回数据中心市场后，AMD一路势头强劲：其数据中心CPU收入市占率在2018年还只有<strong>2%</strong>，今年上半年已攀爬到<strong>34%</strong>，在全球覆盖超过950个云实例和超过350个OxM平台。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_4e609b0d9eff43e8a15c9f9562300b82@5595930_oswg63373oswg1000oswg610_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>AMD是唯一一家能够提供全套CPU、GPU和网络解决方案来满足现代数据中心所有需求的公司。&nbsp;</p>
  <p>AI PC芯片也迎来了新成员——AMD第三代商用AI移动处理器<strong>锐龙AI PRO 300系列</strong>。它被AMD称作“为下一代企业级AI PC打造的全球最好处理器”，预计到2025年将有超过<strong>100款</strong>锐龙AI PRO PC上市。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_b5eafded530d441e827e214c64ec333d@5595930_oswg292641oswg1000oswg464_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>01.</strong></p>
  <p><strong>旗舰AI芯片三代同堂：</strong></p>
  <p><strong>内存容量带宽暴涨，峰值算力冲9.2PF</strong></p>
  <p>AI芯片，正成为AMD业务增长的重头戏。&nbsp;</p>
  <p>AMD去年12月发布的Instinct MI300X加速器，已经成为<strong>AMD历史上增长最快的产品</strong>，<strong>不到两个季度</strong>销售额就超过了<strong>10亿美元</strong>。&nbsp;</p>
  <p>今年6月，AMD公布全新年度AI GPU路线图，最新一步便是今日发布的<strong>Instinct MI325X</strong>。在7月公布季度财报时，AMD董事会主席兼CEO苏姿丰博士透露，AMD预计其今年数据中心GPU收入将超过<strong>45亿美元</strong>。&nbsp;</p>
  <p><strong>微软、OpenAI、Meta、Cohere、Stability AI、Lepton AI（贾扬清创办）、World Labs（李飞飞创办）</strong>等公司的很多主流生成式AI解决方案均已采用MI300系列AI芯片。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_a3d4468853ce4700ab3535e1b043d3f7@5595930_oswg77381oswg1000oswg775_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>微软董事长兼CEO萨提亚·纳德拉对MI300赞誉有加，称这款AI加速器在微软Azure工作负载的GPT-4推理上提供了领先的价格/性能。&nbsp;</p>
  <p>基于Llama 3.1 405B运行<strong>对话式AI、内容生成、AI Agent及聊天机器人、总结摘要</strong>等任务时，MI300的推理速度最多达到英伟达H100的<strong>1.3倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_438e7c784f0e4ef09bfa5abc834113fc@5595930_oswg88264oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>新推出的<strong>MI325X</strong>进一步抬高性能，跑Mixtral 8x7B、Mistral 7B、Llama 3.1 70B等大模型的推理性能，比英伟达H200快<strong>20%~40%</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_3d313ce3945b438fba544cc43d7af8c7@5595930_oswg78178oswg1000oswg621_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>MI325X拥有<strong>1530亿颗晶体管</strong>，采用CDNA 3架构、<strong>256GB HBM3E</strong>内存，内存带宽达<strong>6TB/s</strong>，FP8峰值性能达到<strong>2.6PFLOPS</strong>，FP16峰值性能达到<strong>1.3PFLOPS</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_5550accc46604291bb001251fb3ec5ff@5595930_oswg263173oswg1000oswg555_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>由8张MI325X组成的服务器平台有<strong>2TB HBM3E</strong>内存；内存带宽达到<strong>48TB/s</strong>；Infinity Fabric总线带宽为<strong>896GB/s</strong>；FP8性能最高达<strong>20.8PFLOPS</strong>，FP16性能最高达<strong>10.4PFLOPS</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_cfb15c2579ee445cb312e3f8fd7242c5@5595930_oswg118584oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>相比英伟达H200 HGX，MI325X服务器平台在跑Llama 3.1 405B时，推理性能可提高<strong>40%</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_85aa0be4953241f8a353f9f4f91ca403@5595930_oswg90689oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>从训练性能来看，单张MI325X训练Llama 2 7B的速度<strong>超过单张H200</strong>，8张MI325X训练Llama 2 70B的性能<strong>比肩H200 HGX</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_9c8d962d9e384db2875beffd811ba11d@5595930_oswg78151oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>AMD Instinct MI325X加速器或将于<strong>今年第四季度</strong>投产，将从<strong>明年第一季度</strong>起为平台供应商提供。&nbsp;</p>
  <p>下一代<strong>MI350</strong>系列采用<strong>3nm制程工艺</strong>、新一代<strong>CDNA 4架构</strong>、<strong>288GB HBM3E</strong>内存，新增对<strong>FP4/FP6</strong>数据类型的支持，推理性能相比基于CDNA 3的加速器有高达<strong>35倍</strong>的提升，有望在<strong>2025年下半年</strong>上市。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_7e694ab2a5cb4da5be449e37e03a2907@5595930_oswg88544oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>MI355X</strong>加速器的FP8和FP16性能相比MI325X提升了<strong>80%</strong>，FP16峰值性能达到<strong>2.3PFLOPS</strong>，FP8峰值性能达到<strong>4.6PFLOPS</strong>，FP6和FP4峰值性能达到<strong>9.2PFLOPS</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_37db7e4a8daa49a29b4898020e996f51@5595930_oswg82370oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>8张MI355X共有<strong>2.3TB HBM3E</strong>内存，内存带宽达到<strong>64TB/s</strong>，FP16峰值性能达到<strong>18.5PFLOPS</strong>，FP8峰值性能达到<strong>37PFLOPS</strong>，新增FP6和FP4的峰值性能为<strong>74PFLOPS</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_5af06a8868d941aabc551d36c75720a7@5595930_oswg203607oswg1000oswg496_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>三代GPU的配置显著升级：相比8卡MI300X，8卡MI355X的AI峰值算力提升多达<strong>7.4倍</strong>、HBM内存提高多达<strong>1.5倍</strong>、支持的模型参数量提升幅度接近<strong>6倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_ead80c9b6cc146169981534a632ed072@5595930_oswg100497oswg1000oswg531_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>AMD持续投资软件和开放生态系统，在<strong>AMD ROCm</strong>开放软件栈中提供新特性和功能，可原生支持主流AI框架及工具，具备开箱即用特性，搭配AMD Instinct加速器支持主流生成式AI模型及Hugging Face上的超过<strong>100万款</strong>模型。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_d5921c62040a4270ac1349a726323ee4@5595930_oswg96227oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>ROCm 6.2现包括对关键AI功能的支持，如FP8数据类型、Flash Attention、内核融合等，可将AI大模型的推理性能、训练性能分别提升至ROCm 6.0的<strong>2.4倍</strong>、<strong>1.8倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_89ca70d928da4e76a374372e80f5f72a@5595930_oswg80540oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>此前AMD收购了欧洲最大的私人AI实验室<strong>Silo AI</strong>，以解决消费级AI最后一英里问题，加快AMD硬件上AI模型的开发和部署。欧洲最快的超级计算机LUMI便采用AMD Instinct加速器来训练欧洲语言版的大语言模型。&nbsp;</p>
  <p><strong>02.</strong></p>
  <p><strong>下一代AI网络：后端引入业界首款支持UEC的AI网卡，前端上新400G可编程DPU</strong></p>
  <p>网络是实现最佳系统性能的基础。AI模型平均有<strong>30%</strong>的训练周期时间都花在网络等待上。在训练和分布式推理模型中，通信占了<strong>40%-75%</strong>的时间。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_70c8592c5ba640958b761cbc076ed332@5595930_oswg63662oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>AI网络</strong>分为前端和后端：<strong>前端</strong>向AI集群提供数据和信息，可编程DPU不断发展；<strong>后端</strong>管理加速器与集群间的数据传输，关键在于获得最大利用率。&nbsp;</p>
  <p>为了有效管理这两个网络，并推动整个系统的性能、可扩展性和效率提升，AMD今日发布了应用于前端网络的<strong>Pensando Salina 400 DPU</strong>和应用于后端网络的<strong>Pensando Pollara 400网卡</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_4fa2de0f2b24493d94d0f8412c9b910d@5595930_oswg169333oswg1000oswg419_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>Salina 400</strong>是AMD第三代可编程DPU，被AMD称作“前端网络最佳DPU”，其性能、带宽和规模均提高至上一代DPU的<strong>两倍</strong>；<strong>Pollara 400</strong>是<strong>业界首款</strong>支持超以太网联盟（UEC）的AI网卡。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_424f0b4e128c4711afd6e52c7f943ed3@5595930_oswg64839oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>Salina 400支持<strong>400G</strong>吞吐量，可实现快速数据传输速率，可为数据驱动的AI应用优化性能、效率、安全性和可扩展性。&nbsp;</p>
  <p>Pollara 400 采用 <strong>AMD P4可编程引擎</strong> ，支持下一代RDMA软件，并以开放的网络生态系统为后盾，对于在后端网络中提供加速器到加速器通信的领先性能、可扩展性和效率至关重要。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_48fb908f6acd481cb734218bd7bb9ba3@5595930_oswg61731oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>UEC Ready RDMA支持智能数据包喷发和有序消息传递、避免拥塞、选择性重传和快速损失恢复。这种传输方式的消息完成速度是RoCEv2的<strong>6倍</strong>，整体完成速度是RoCEv2的<strong>5倍</strong>。&nbsp;</p>
  <p>在后端网络，相比InfiniBand，<strong>以太网RoCEv2</strong>是更好的选择，具有低成本、高度可扩展的优势，可将TCO节省超过<strong>50%</strong>，能够扩展<strong>100万张</strong>GPU。而InfiniBand至多能扩展<strong>48000张</strong>GPU。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_3bd390bfc867430f8162a8acb4bbc56a@5595930_oswg71517oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>03.</strong></p>
  <p><strong>服务器CPU：</strong></p>
  <p><strong>3/4nm制程，最多192核/384线程</strong></p>
  <p>今年7月公布财报时，苏姿丰提到今年上半年，有超过<strong>1/3</strong>的企业服务器订单来自首次在其数据中心部署EPYC服务器CPU的企业。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_5d7cebb3bbaa4534b128e22b13621ad1@5595930_oswg88916oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>第五代EPYC处理器9005系列（代号“Turin”）</strong>专为现代数据中心设计。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_b3700f75efdc4a14b1c8ba5ed83adb8b@5595930_oswg508309oswg1000oswg553_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>该处理器在<strong>计算、内存、IO与平台、安全</strong>四大层面全面升级。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_c707d98d86dd4db7a82133217ac9fe53@5595930_oswg245183oswg1000oswg436_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>第五代EPYC拥有<strong>1500亿颗晶体管</strong>，采用<strong>台积电3/4nm 制程</strong>、全新<strong>“Zen 5”</strong> 及<strong>“Zen 5c”</strong>核心兼容广泛部署的SP5平台，最多支持<strong>192核</strong>、<strong>384个线程</strong>，8~192核的功耗范畴为<strong>155W~500W</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_a4200f73f65d467cae08b9867ccc51b1@5595930_oswg120745oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>它支持AVX-512全宽512位数据路径、128 PCIe 5.0/CXL 2.0、DDR5-6400MT/s内存速率，提升频率高达<strong>5GHz</strong>，机密计算的可信I/O和FIPS认证正在进行中。&nbsp;</p>
  <p>与“Zen 4”相比，“Zen 5”核心架构为企业和云计算工作负载提供了提升<strong>17%</strong>的IPC（每时钟指令数），为AI和HPC提供了提升<strong>37%</strong>的IPC。&nbsp;</p>
  <p>在SPEC CPU 2017基准测试中，192核EPYC 9965的整数吞吐量是64核至强8592+的<strong>2.7倍</strong>，32核EPYC 9355的每核心性能是32核6548Y+的<strong>1.4倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_7f8b5e3cb32547b9addbfb8450752212@5595930_oswg82112oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>跑<strong>视频转码、商用App、开源数据库、图像渲染</strong>等商用工作负载时，192核EPYC 9965的性能达到64核至强8592+性能的<strong>3~4倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_e2a140a7f8c74e938fcd3c383486a41f@5595930_oswg135152oswg1000oswg479_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>在处理开源的HPC密集线性求解器、建模和仿真任务时，EPYC 9965的性能可达到至强8592+性能的<strong>2.1~3.9倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_37ee42f87b63425295d549c543ae11c0@5595930_oswg80335oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>达到相同性能，第五代EPYC所需的服务器数量更少，有助于降低数据中心的TCO（总拥有成本）以及节省空间和能源。&nbsp;</p>
  <p>例如，要达到总共391000个单位的SPECrate 2017_int_base性能得分，相比<strong>1000台</strong>搭载英特尔至强铂金8280的服务器，现在<strong>131台</strong>搭载AMD EPYC 9965的现代服务器就能实现，<strong>功耗、3年TCO均显著减少</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_c47bead4a6a941d98b7c86a1b657dab4@5595930_oswg101316oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>通过优化的CPU+GPU解决方案，AMD EPYC CPU不仅能处理传统通用目的的计算，而且能胜任AI推理，还能作为AI主机处理器。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_6254f330f4ce467e817e1c0cad994232@5595930_oswg141103oswg1000oswg519_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>相比64核至强8592+，192核EPYC 9965在运行<strong>机器学习、端到端AI、相似搜索、大语言模型</strong>等工作负载时，推理性能提升多达<strong>1.9~3.8倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_f34cf690f55a444e98923c869fb7950b@5595930_oswg84889oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>AMD EPYC 9005系列的新产品是64核EPYC 9575F，专为需要终极主机CPU能力的GPU驱动AI解决方案量身定制。&nbsp;</p>
  <p>与竞争对手的3.8GHz处理器相比，专用AI主机的CPU EPYC 9575F提供了高达<strong>5GHz</strong>的提升，可将GPU编排任务的处理速度提高<strong>28%</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_e1eb4c04498d4ac4906c1ceced92efe3@5595930_oswg61383oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>面向企业级HPC工作负载，64核EPYC 9575F的FEA仿真和CFD仿真&amp;建模的性能，可提升至64核至强8592的<strong>1.6倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_2ba04df7c2b04ac1a9d2881d7079b184@5595930_oswg127388oswg1000oswg457_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>EPYC 9575F可使用其5GHz的最大频率提升来助力1000个节点的AI集群每秒驱动多达<strong>70万</strong>个推理token。同样搭配MI300X GPU，与64核至强8592+相比，EPYC 9575F将GPU系统训练Stable Diffusion XL v2文生图模型的性能提升<strong>20%</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_be75313bed484f5e97c18c530b751ff4@5595930_oswg62811oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>搭配Instinct系列GPU的AMD EPYC AI主机CPU型号如下：</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_e3e89a3134db40d58ac1d6056d04f314@5595930_oswg191356oswg1000oswg466_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>同样搭配英伟达H100，EPYC 9575F可将GPU系统的推理性能、训练性能分别相比至强8592+提升<strong>20%</strong>、<strong>15%</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_0aa9c65c7cd4438380e0e55b649aefb9@5595930_oswg140850oswg1000oswg481_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>与英伟达GPU系统适配的AMD EPYC AI主机CPU型号如下：</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_0dfb941dac6e4ba094427123d15cea07@5595930_oswg216119oswg1000oswg513_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>将EPYC用于计算与AI混合工作负载时，相比至强铂金8592+，EPYC 9654+2张Instinct MI210在处理50%通用计算+50% AI的混合任务时，每美元性能可提升多达<strong>2倍</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_27df1260bc304c128482e4dc3a577c16@5595930_oswg141232oswg1000oswg503_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p><strong>04.</strong></p>
  <p><strong>企业级AI PC处理器：</strong></p>
  <p><strong>升级“Zen 5”架构，AI算力最高55TOPS</strong></p>
  <p>AI PC给企业生产力、身临其境的远程协作、创作与编辑、个人AI助理都带来了全新转型体验。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_80d7e67ef8a24f9ebc88ce6ad6af3865@5595930_oswg87707oswg1000oswg750_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>继今年6月推出第三代AI移动处理器锐龙AI 300系列处理器（代号“Strix Point”）后，今日AMD宣布推出<strong>锐龙AI PRO 300系列</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_99e130f3f9b740d0acd31d48c0a43218@5595930_oswg248379oswg1000oswg586_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>该处理器专为提高企业生产力而设计，采用<strong>4nm工艺</strong>、<strong>“Zen 5”</strong> CPU架构（最多12核、24个线程）、<strong>RDNA 3.5</strong> GPU架构（最多16个计算单元），支持<strong>Copilot+</strong>功能，包括电话会议实时字幕、语言翻译、AI图像生成等。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_dfbf44738d484561ab56a2964fd9bea8@5595930_oswg415225oswg1000oswg584_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>其内置NPU可提供<strong>50-55TOPS</strong>的AI处理能力。&nbsp;</p>
  <p><strong>40TOPS</strong>是微软Copilot+ AI PC的基准要求。相比之下，苹果M4、AMD锐龙PRO 8040系列、英特尔酷睿Ultra 100系列的NPU算力分别为38TOPS、16TOPS、11TOPS。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_1eee5a696de84250aecfabe4cf975c46@5595930_oswg243779oswg1000oswg503_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>与英特尔酷睿Ultra 7 165H相比，旗舰锐龙AI 9 HX PRO 375的多线程性能提高了<strong>40%</strong>，办公生产力提高了<strong>14%</strong>，支持<strong>更长续航</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_5a04bcafc37d42f0b939aedbe8ef4c7b@5595930_oswg279437oswg1000oswg532_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>锐龙AI PRO 300系列采用<strong>AMD PRO</strong>技术，提供世界级领先的安全性和可管理性，旨在简化IT运营及部署并确保企业获得卓越的投资回报率。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_11647a21abaa4e5f9b8865fa51a3c3ce@5595930_oswg91784oswg1000oswg767_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>由搭载锐龙AI PRO 300系列的OEM系统预计将于<strong>今年晚些时候</strong>上市。&nbsp;</p>
  <p>AMD也扩展了其PRO技术阵容，具有新的安全性和可管理性功能。配备AMD PRO技术的移动商用处理器现有云裸机恢复的标准配置，支持IT团队通过云无缝恢复系统，确保平稳和持续的操作；提供一个新的供应链安全功能，实现整个供应链的可追溯性；看门狗定时器，提供额外的检测和恢复过程，为系统提供弹性支持。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_ac9303f4874a4106a36cf7f207c79831@5595930_oswg382251oswg1000oswg529_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>通过AMD PRO技术，还能实现额外的基于AI的恶意软件检测。这些全新的安全特性利用集成的NPU来运行基于AI的安全工作负载，不会影响日常性能。&nbsp;</p>
  <p><strong>05.</strong></p>
  <p><strong>结语：AMD正在数据中心市场攻势凶猛</strong></p>
  <p>AMD正沿着路线图，加速将AI基础设施所需的各种高性能AI解决方案推向市场，并不断证明它能够提供满足数据中心需求的多元化解决方案。&nbsp;</p>
  <p>AI已经成为AMD战略布局的焦点。今日新发布的Instinct加速器、EPYC服务器CPU、Pensando网卡&amp;DPU、锐龙AI PRO 300系列处理器，与持续增长的开放软件生态系统形成了组合拳，有望进一步增强AMD在AI基础设施竞赛中的综合竞争力。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241013/v2_9d0de2d6c0a04ee5964d86b8727417a4@5595930_oswg433522oswg1000oswg512_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p class="img-desc">&nbsp;</p>
  <p>无论是蚕食服务器CPU市场，还是新款AI芯片半年揽金逾10亿美元，都展现出这家老牌芯片巨头在数据中心领域的冲劲。紧锣密鼓的AI芯片产品迭代、快速扩张的全栈软硬件版图，都令人愈发期待AMD在AI计算市场创造出惊喜。&nbsp;</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/glLHm6f__IEESiM-lWBgnQ" rel="noopener noreferrer nofollow" target="_blank">“智东西”</a>，作者：ZeR0，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2990196901784581</id>
            <title>苹果发文质疑：大语言模型根本无法进行逻辑推理</title>
            <link>https://www.36kr.com/p/2990196901784581</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2990196901784581</guid>
            <pubDate></pubDate>
            <updated>Sun, 13 Oct 2024 02:41:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><span>大语言模型（LLM）是真的会数学推理？还是只是在“套路”解题？</span><span style="letter-spacing: 0px;">&nbsp; &nbsp;&nbsp;</span><br /></p>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">近年来，大语言模型在各种任务中的表现引起广泛关注。一个核心问题逐渐浮现：<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>这些模型是否真正具备逻辑推理能力，还是仅仅通过复杂的模式匹配来应对看似推理的问题？</strong></span>尤其是在数学推理任务中，模型的表现究竟是在模拟人类思维，还是仅仅通过数据模式匹配得出答案？</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">日前，来自苹果公司的 Iman Mirzadeh 及其研究团队提出了一个名为 GSM-Symbolic 的新基准，针对多个开源模型（如 Llama、Phi、Gemma、Mistral）和闭源模型（如 GPT-4o、o1 系列）进行了大规模评估。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">结果显示，<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>当问题中的数值或名字变化时，模型的会表现出显著的波动</strong></span>。此外，随着问题难度的提升（如增加更多子句），模型的表现迅速下降，这表明这些模型在推理复杂问题时非常脆弱。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">研究团队认为，这种表现下降并非偶然，而是因为<strong><span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;">当前的大语言模型缺乏真正的逻辑推理能力</span></strong>，更多是在基于训练数据中的模式进行匹配，而非像人类一样进行符号和逻辑推导。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">即使是简单的变化，如调整问题中的数值，也能导致模型准确率<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>下降 10%</strong></span>。而当问题增加一个额外但无关的子句时，性能下降幅度甚至<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>高达 65%</strong></span>。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_7f2980a8a5244938956f4fc1eed68922@000000_oswg24104oswg1062oswg328_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp; &nbsp;
   <span style="font-size: 12px; color: #A0A0A0; letter-spacing: 0.5px;">论文链接：https://arxiv.org/abs/2410.05229</span> &nbsp;
  </section>
  <p><br /></p>
  <h3 style="margin: 0px; line-height: 1.6em;"><span style="color: #1ABC9C; font-size: 24px; letter-spacing: 0.5px;"><strong>大模型不具备形式推理能力？5 大证据来了</strong></span></h3>
  <p><br /></p>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">三年前，OpenAI 发布了 GSM8K 数据集（目前常用的一种小学数学推理基准数据集），测试 GPT-3（175B参数）在数学题上的表现，那时 GPT-3 的得分仅为 35%。如今，拥有约 30 亿参数的模型已能够在 GSM8K 测试中取得超过 85% 的得分，参数更大的模型甚至超过 95%。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_6e24293f1865469c82461cc1b9eb5df1@000000_oswg402839oswg1080oswg601_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">然而，随着准确率的提升，疑问也随之而来：这些模型的推理能力是否真的进步了？它们的表现是否真的体现了逻辑或符号推理能力，抑或是简单的模式识别，数据污染，甚至过拟合的结果？</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">为进一步探索这一问题，研究团队此发了 GSM-Symbolic，用于测试大语言模型在数学推理中的极限。GSM-Symbolic 基于 GSM8K 数据集，通过符号模板生成多样化的问题实例，允许更可控的实验设计。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">为了更清晰地观察模型在面对这些变体问题时的表现，他们生成了 50 个独特的 GSM-Symbolic 集合，这些问题与 GSM8K 问题类似，但更改了其中的数值和名称。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">基于 <span style="font-size: 16px; letter-spacing: 0.5px;">GSM-Symbolic，</span>他们从 5 个方面说明了为何他们认为大语言模型不具备形式推理能力：</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="color: #3498DB; letter-spacing: 0.5px;"><strong><span style="color: #3498DB; font-size: 16px;">1. GSM8K 的当前准确率并不可靠</span></strong></span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">通过对多个开源模型（如 Llama 8B、Phi-3）和闭源模型（如 GPT-4o 和 o1 系列）的大规模评估，他们发现模型在 GSM8K 上的表现存在显著波动。例如，Llama 8B 的准确率在 70%-80% 之间波动，而 Phi-3 的表现则在 75%-90% 之间浮动。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">这也表明，模型在处理相似问题时表现并不稳定，GSM8K 上的高分并不能证明它们具备真正的推理能力。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_507dfba803b14a63a46a3ea9159dcc3a@000000_oswg128985oswg974oswg538_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp; &nbsp;
   <span style="font-size: 12px; color: #A0A0A0; letter-spacing: 0.5px;">图｜由 GSM-Symbolic 模板生成的 50 套 8-shot 思想链（CoT）性能分布，显示了所有 SOTA 模型之间准确性的显著差异性。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">对于大多数型号来说，GSM-Symbolic 的平均性能低于 GSM8K（图中由虚线表示）。有趣的是，GSM8K 的性能落在分布的右侧，从统计学上讲，这应该非常低的可能性，因为 GSM8K 基本上只是 GSM-Symbolic 的一次单一抽样。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="color: #3498DB; letter-spacing: 0.5px;"><strong><span style="color: #3498DB; font-size: 16px;">2. 对名称和数字变动的敏感性</span></strong></span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">研究还发现，当前的大语言模型对问题中的专有名称（如人名、食物、物品）的变化仍然很敏感，当数字发生变化时，大语言模型就会更加敏感。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">例如，仅仅改变问题中的名字，就可能导致模型的准确率变化高达 10%。如果将这种情况类比到小学数学测试中，仅仅因为改变了人名而导致分数下降 10% ，是非常不可思议的。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_7d18f2ad03954445a9f062a39198d2af@000000_oswg131057oswg975oswg468_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 12px; color: #A0A0A0; letter-spacing: 0.5px;">图｜当只更改名称、专有编号或同时更改名称和编号时，大语言模型的敏感性如何？总体而言，即使只更改名称，模型也有明显的性能变化，但当更改编号或合并这些变化时，性能差异更大。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="color: #3498DB; letter-spacing: 0.5px;"><strong><span style="color: #3498DB; font-size: 16px;">3. 问题难度的增加导致表现急剧下降</span></strong></span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">研究团队通过引入三种新的 GSM-Symbolic 变体（GSM-M1、GSM-P1、GSM-P2），通过删除一个分句（GSM-M1）、增加一个分句（GSM-P1）或增加两个分句（GSM-P2），来调整问题难度。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_d5521ec812e54c02806d009f5c190933@000000_oswg87892oswg962oswg365_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp; &nbsp;
   <span style="color: #A0A0A0; font-size: 12px; letter-spacing: 0.5px;">图｜通过修改条款数量来修改 GSM-Symbolic 的难度级别</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_ca2ae3c9874d42479dc7c199b2d06a00@000000_oswg135347oswg929oswg463_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="height: auto !important;" /> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span>图｜增加条款数量对性能的影响：随着GSM-M1→GSM-Symb→GSM-P1→GSM-P2的难度增加，性能分布向左移动（即准确性下降），方差增加。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span>结果发现，随着问题难度的增加（GSM-M1 → <span style="font-size: 16px; letter-spacing: 0.5px;">GSM-</span>Symb → <span style="font-size: 16px; letter-spacing: 0.5px;">GSM-</span>P1 → <span style="font-size: 16px; letter-spacing: 0.5px;">GSM-</span>P2），模型的表现不仅下降显著，且表现波动也变得更加剧烈。面对更复杂的问题时，模型的推理能力变得更加不可靠。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="color: #3498DB; letter-spacing: 0.5px;"><strong><span style="color: #3498DB; font-size: 16px;">4. 添加无关子句对性能的巨大影响</span></strong></span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">为进一步测试模型的推理能力，研究团队设计了 GSM_NoOp 实验，在原有问题中添加一个似乎相关但实际无关的子句 (hence "no-op")。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">结果显示，<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>所有模型的表现都显著下降，包括性能较好的 o1 模型在内</strong></span>。这种现象进一步说明，模型并没有真正理解数学概念，而是通过模式匹配来得出答案。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;"><img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_1f6d062a00f84bdc97e60abeb2e9dafa@000000_oswg190561oswg1080oswg566_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="height: auto !important;" /></span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 12px; color: #A0A0A0; letter-spacing: 0.5px;">图｜在 GSM-NoOp 上，模型的性能明显下降，较新的模型比旧的模型下降更大。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="color: #3498DB; letter-spacing: 0.5px;"><strong><span style="color: #3498DB; font-size: 16px;">5. 扩展规模和计算能力并不能解决根本问题</span></strong></span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">此外，他们还探讨了通过扩大数据、模型规模或计算能力是否能够解决推理能力不足的问题。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">Mehrdad Farajtabar 表示，尽管 OpenAI 的 o1 系列在性能上有一定改善，但它们也会出现这样的愚蠢错误，要么是它不明白“现在”是什么意思，要么是它不明白“去年”是什么意思，还有一种更可能的解释是，更大的训练数据具有这种模式，所以它又沿用了这种模式。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_c48d4ce9082649f0aadba5cc1cf27b55@000000_oswg117457oswg916oswg581_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp; &nbsp;
   <span style="font-size: 12px; color: #A0A0A0; letter-spacing: 0.5px;">图｜o1-mini 和 o1-preview 的结果：这两个模型大多遵循我们在正文中介绍的相同趋势。然而，o1-preview 在所有难度级别上都显示出非常强大的结果，因为所有分布都彼此接近。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">他认为，理解大语言模型的真正推理能力对于在现实世界中的应用至关重要，尤其是在 AI 安全、教育、医疗保健和决策系统等对准确性和一致性要求极高的领域。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">研究结果表明，<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>当前大语言模型的表现，更像是高级的模式匹配器，而非具备形式推理能力的系统</strong></span>。为了在这些领域安全、可靠地部署大语言模型，开发更为鲁棒和适应性强的评估方法显得尤为重要。</span> &nbsp;
  </section>
  <p><br /></p>
  <h3 style="margin: 0px; line-height: 1.6em;"><span style="font-size: 24px; color: #1ABC9C; letter-spacing: 0.5px;"><strong>逻辑推理：大语言模型的真正挑战</strong></span></h3>
  <p><br /></p>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">研究人员表示，总体而言</span> &nbsp; &nbsp;
   <span style="font-size: 16px; letter-spacing: 0.5px;">，这项研究没有发现大语言模型具备正式的逻辑推理能力，无论是开源模型，还是闭源模型。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">它们的行为更像是复杂的模式匹配，甚至很脆弱，以至于简单改变名字就能导致结果变化约 10%。尽管可以通过增加数据量、参数规模或计算能力，或者为 Phi-4、Llama-4、GPT-5 提供更好的训练数据来提高表现，但<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>他们认为这只会带来“更好的模式匹配者”，而不是“更好的推理者”</strong></span>。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">有读者对 10% 的表现波动提出了疑问。对此，Farajtabar 回应道：</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">“如果你指的是 Llama 3 8B，它确实是一个先进的模型，并且假设已经通过大量精心设计的数据进行了训练，然而即便如此，10% 的偏差对我来说还是太大了。对于较旧的模型来说，这种波动更为明显。<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>真正令人担忧的问题在于，当问题难度稍微提升（例如通过增加一个子句）时，偏差会迅速增加到 16%。</strong></span>是的，或许我们可以通过收集更多类似数据来缩小这些差异，但<span style="font-size: 16px; letter-spacing: 0.5px; color: #3498DB;"><strong>如果问题难度继续上升，这种偏差很可能会呈指数级增长</strong></span>。”</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <img class="rich_pages wxw-img" src="https://img.36krcdn.com/hsossms/20241013/v2_748536a5509440948ab60978c2a5c7a8@000000_oswg458536oswg1080oswg987_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" style="width: 100%; height: auto !important;" /> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">随着大语言模型在各类应用场景中被广泛采用，如何确保它们能够处理更复杂、更多样化的问题，已成为 AI 研究领域面临的下一个重大挑战。</span> &nbsp;
  </section>
  <section style="margin: 0px; line-height: 1.6em;">
   <span style="font-size: 16px; letter-spacing: 0.5px;">未来，大语言模型需要突破模式匹配，真正实现逻辑推理，才能应对不断变化的现实需求。这也是 AI 社区共同努力的方向。</span> &nbsp;
  </section>
  <p><span style="letter-spacing: 0.544px;">&nbsp;</span><span style="letter-spacing: 0px;">本文来自微信公众号 &nbsp;</span><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MDE3OTA5NA==&amp;mid=2247592268&amp;idx=1&amp;sn=6b766c6240211c69bcc3b7eb873d04e6&amp;chksm=cef6be73da72512afdb6edd06d6a44e9bc9e5f6730f715f1217d187db076ab8a28e6080316e7&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" style="letter-spacing: 0px; color: rgb(38, 38, 38); border-color: rgb(153, 153, 153); border-image: none; padding-bottom: 1px;">“学术头条”（ID：SciTouTiao）</a><span style="letter-spacing: 0px;">，作者：田小婷，36氪经授权发布。</span></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2989581682859648</id>
            <title>美团忙着出海，京东开始送外卖</title>
            <link>https://www.36kr.com/p/2989581682859648</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2989581682859648</guid>
            <pubDate></pubDate>
            <updated>Sun, 13 Oct 2024 00:51:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>王兴在海外开拓新战场的时候，美团国内的大本营又迎来了新对手。&nbsp;</p>
  <p>10 月 9 日，美团的海外平台 Keeta 在沙特利雅得上线，这是继 2023 年 5 月香港，2024 年 9 月沙特阿尔卡吉之后， Keeta 在境外落地的第三座城市。&nbsp;</p>
  <p>Keeta 落地沙特利雅得并不意外，今年 4 月彭博社就报道过美团的该项计划。并且在今年 2 月，美团所做出的重大架构调整早已为出海计划进一步铺路，除了将到店、到家以及两大平台等业务交由王莆中负责之外，<strong>无人机及境外业务则是由王兴亲自带队。</strong></p>
  <p>经历过 2023 年抖音的搅局，美团和抖音也逐渐在本地生活板块决出相对差异化的定位，美团今年的股价也从低点一路回暖。如今 Keeta 正式上线沙特利雅得，王兴带队的美团海外业务正是风生水起时。&nbsp;</p>
  <p>但就在这个当口，在大中华区，美团的大本营，业务根基外卖，再度迎来新的挑战——京东秒送，叠加抖音小时达加码，王莆中的守擂是否真的能让王兴放心开拓海外业务？这还要从一杯咖啡开始说起。&nbsp;</p>
  <h3 label="二级标题"><strong>京东秒送，从一杯咖啡开始的“质变”</strong></h3>
  <p>5 月，京东将自己的“小时达”业务改名为“秒送”。但不管是小时达还是秒送，在当时的舆论看来京东还是在即时零售这个概念里范围打转，最多算是进一步提高了时效性。&nbsp;</p>
  <p>就商业模式来说，即时零售大致可以分为商超自营模式和平台模式。但如果从支撑这两个模式的运力来看，相比起一些商超自营模式独占自身配送服务（如盒马、山姆等），<strong>平台模式也仅有美团对自身的配送服务独占性较强</strong>，其余平台模式的运力则相对较为开放。如京东使用的达达配送，抖音同城配送使用的第三方配送顺达闪，淘宝小时达则是在 7 月与饿了么达成深度合作，尽管淘宝饿了么同属于阿里，但终归还是不同平台的业务。&nbsp;</p>
  <p>而在此前 2022 年，饿了么就以小程序的方式和抖音展开合作，加之抖音的第三方配送服务商也有达达，<strong>也就是说，对于抖音、京东、淘宝而言，自身能够调动的基础服务运力已然算不上核心竞争力，仅能算作一个必要的基础服务。</strong></p>
  <p>如果继续跟抖音天猫在原有的生鲜商超及快消领域竞争，京东优势并不明显。毕竟美团的自有骑手大军，以及商家高渗透率，不管在外卖领域还是范围更广的即时零售领域都占据着较高市场份额。所以<strong>哪怕 5 月京东已经将“小时达”改为看起来时效性更快的“秒送”，当时的讨论也都是围绕着京东将如何改变即时零售格局，鲜有人提到其对美团的冲击。</strong></p>
  <p><strong>但这一切都从京东可以 9.9 元瑞幸送到家开始，有了质的改变。</strong></p>
  <p>京东秒送 9.9 瑞幸送到家这件事，最早可以追溯到 5 月的官方信息，但由于当时用户的感知并不强，舆论开始讨论起京东将对美团会有怎样的冲击，就是几个月后的事情了。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241012/v2_d427e10a8ffa43fca50bbc29697f6e51@5603302_oswg105447oswg640oswg1082_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p>毕竟 9.9 送到家是连瑞幸自己的官方 App 或小程序也做不到的，并且目前不止瑞幸，更加划算的是瑞幸的老对头库迪 8.8 送到家，有时还可以叠加平台优惠券。显然京东为了铺开市场培养心智，正在自己承担这些品牌的配送费用，但不得不说这样的方法十分奏效。加之霸王茶姬，挪瓦，汉堡王等其他连锁餐饮品牌，<strong>用免费的配送服务来换取用户对于京东新的消费心智和更多的流量倒也划算。</strong></p>
  <p>而从生鲜商超快消品类，拓展到咖啡奶茶汉堡这种即食快餐品类，已经算得上用户认知里的餐饮外卖范畴了。美团就是仗着其外卖业务，使得美团其他的到家业务也获得更高的转化。由于外卖配送的商品具有极强的即食性，对于配送运力的要求更高，能做外卖的运力一定能做即时零售，但能做即时零售的运力却不一定能做外卖。<strong>现如今京东也能点茶饮快餐外卖的心智逐渐加强，对于美团来说确实不是一个很好的信号。</strong></p>
  <p>而至于今后 9.9 的标签是否会尾大不掉，亦或者京东本意就是打算将 9.9 送到家作为成为长期流量入口，则还需要时间进一步观察。&nbsp;</p>
  <h3 label="二级标题"><strong>京东抖音，做即时零售路径各不相同</strong></h3>
  <p>在这之前，不管京东、淘宝再怎么出招，只要不做外卖，就没人会觉得这些玩家会威胁到美团的核心业务，也正是因为去年抖音高调做起了外卖，叠加到店酒旅等本地生活服务，所以舆论才会纷纷认为，抖音正在向美团的腹地发起进攻。&nbsp;</p>
  <p>但正如开头所说，经过了 2023 年，美团和抖音也逐渐在本地生活板块决出相对差异化的定位，美团今年的股价也从低点一路回暖。而在这时，京东则悄悄揽过了抖音的大旗，成为下一个舆论口中“向美团腹地”进攻的角色。&nbsp;</p>
  <p><strong>目前看来，京东做茶饮快餐外卖这条路，似乎走得比抖音此前尝试做外卖更顺。</strong></p>
  <p><strong>首当其冲就是，京东的“外卖”定位，做得比抖音外卖更加符合外卖用户心智。</strong></p>
  <p>毕竟京东从未高调宣称自己要做“外卖”，但因为一个 9.9 送到家，用户开始自发在京东平台点起了外卖，由这些性价比高的快餐外卖心智再去渗透范围更广的即时零售业务是个极具想象力的业务逻辑。众所周知，美团最大的流量入口就是客单价较低的工作餐。&nbsp;</p>
  <p>而反观抖音此前的外卖主要是面向了高客单价的团餐，符合抖音本身具有较高的到店型商家渗透率的特性，但却不太符合大部分用户点外卖的初衷——方便，快捷，一人食，性价比高。也许正因如此，去年 6 月，抖音曾传出放弃过外卖 1000 亿元 GMV 目标，但并未放弃外卖业务本身。&nbsp;</p>
  <p>另外，此前《新立场》的文章就有提到，<strong>抖音对于自己做外卖的定位本就显得有些“摇摆不定”。</strong></p>
  <p>2024 年 4 月，抖音外卖从抖音本地生活业务线调整至抖音电商业务线。到了 8 月又有消息称，抖音外卖业务重新回到抖音本地生活业务线。而京东则完全没有想要做到店团购业务的困扰，倒显得更加坚定。&nbsp;</p>
  <p>当时《新立场》推测，这样在电商和本地生活之间的摇摆，可能是抖音又想做外卖跟到店团购的结合，又想从电商的逻辑去切入超市业务，抖音电商副总裁木青此前就称：“在平台服务、物流时效可以满足一些特定用户的需求，对我们现有平台、商家形成一个有效补充”。&nbsp;</p>
  <p>如果我们点开属于抖音电商业务的“商城”板块，会发现目前小时达和抖音超市都在这一界面主菜单栏中；而外卖功能则属于本地生活范畴的团购板块之下。外卖跟商超即时零售业务如此割裂，也就意味着抖音的外卖几乎不能够为其商超小时达带来转化。&nbsp;</p>
  <p><strong>还有一个可能的因素则是，京东物流的品牌形象也在间接促进着京东的即时零售业务，但抖音本身却没有这种物流品牌心智。</strong></p>
  <p>正如上述所说，抖音和京东其实都在使用达达配送，只不过京东是京东到家结合达达配送，而抖音则是依靠顺达闪等第三方服务。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241012/v2_72d54593c07847dfae9629956cb33353@5603302_oswg65685oswg1080oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p>据悉，达达 2020 年就已在纳斯达克 IPO 挂牌上市，兴起于外卖，算得上即时零售第一股。其背后股东有京东、沃尔玛、红杉中国等。在今年 9 月，京东已完成收购沃尔玛所持的达达股份，京东对达达的持股比例超过 60% 。另一边，闪送 10 月 4 日也已于纳斯达克挂牌上市，只不过目前（美东时间10月11日）股价已跌至首日开盘价的 3/4，而达达的股价也已经是当初刚上市时的 1/10 左右了。&nbsp;</p>
  <p>倘若京东或抖音的即时零售模式能做起来，这些第三方配送服务公司的股价也将因此受益。&nbsp;</p>
  <h3 label="二级标题"><strong>王莆中守擂，王兴放心飞</strong></h3>
  <p>开头说到目前王兴主要的注意力在海外业务和无人机上，而国内的到店、到家以及两大平台等业务都交由王莆中来负责。&nbsp;</p>
  <p>王普中上任后，美团继续发力“拼好饭”、“神会员”，美团今年第二季度利润率也较去年上涨 6.8 个百分点，订单量峰值达到了 9800 万。7 月份美团又提出“秒提”，企图打通到店到家两个场景，美团本身的业务融合正在进一步加强。&nbsp;</p>
  <p>反观京东，正在将其同城配送业务同时押注在自己和达达配送上，据知情人士透露，京东正在进一步招募达达骑手，进入京东的秒送页面，也会以抽奖的方式赠送达达快送专用优惠券。&nbsp;</p>
  <p>而据另一个目前正在密集使用京东秒送的用户称：“京东自己的配送没有任何问题，但是有一次在秒送上点到了达达配送的店，给我送丢了，不过后来平台给我退款了。”尽管这只是个例并不能以偏概全，但我们也能从这件事窥得，现在京东跟达达的合作正在进入新一轮的磨合。&nbsp;</p>
  <p>因此面对来自京东的冲击，美团倒也不必过于杯弓蛇影。&nbsp;</p>
  <p>*题图及文中配图来源于网络。&nbsp;</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/OF-K9vegwRo4Fk6wMu_nVg" rel="noopener noreferrer nofollow" target="_blank">“新立场Pro”</a>，作者：X X，编辑：王威，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>