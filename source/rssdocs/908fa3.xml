<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/3194685475028353</id>
            <title>万字解构“幻觉陷阱”：大模型犯的错，会摧毁互联网吗？</title>
            <link>https://www.36kr.com/p/3194685475028353</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3194685475028353</guid>
            <pubDate></pubDate>
            <updated>Thu, 06 Mar 2025 12:04:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AGI, 大模型幻觉, 技术治理, 人工智能错误

<br>
<br>
总结: 本文探讨了AGI（通用人工智能）发展中的技术、商业和治理问题，特别是大模型“幻觉”现象。大模型的幻觉是其基于概率预测的固有特性，无法完全消除。人类自身的偏见和后真相时代的传播环境加剧了这一问题。专家权威的下降和大模型的普及使得虚假信息更容易传播。尽管大模型开发者有能力减少幻觉，但完全消除风险是不可能的。文章还讨论了如何应对大模型幻觉，建议用户保持怀疑态度，并通过多模型对比和详细输入来提高准确性。 </div>
                        <hr>
                    
                    <p>人类实现AGI之前，在技术、商业、治理方面仍然存在诸多问题——“人与AI能否共处” “算力叙事是否依然奏效” “开源有多大商业价值”等，腾讯科技策划《AGI之路》系列直播，联合合作伙伴，特邀专家、学者直播解读相关议题，对齐AGI共识，探寻AGI可行之路。</p>
  <p>当DeepSeek以惊人的速度攀升至日下载量500万、DAU逼近ChatGPT的23%之际，大模型正以前所未有的速度走进普通人的生活。然而，在这场AI与人类的密切接触中，我们遇到了一个独特的悖论：这些超级AI既能给出令人惊叹的答案，却也会编织出巧妙的"美丽谎言"。它们的回答往往逻辑自洽、言之凿凿，却可能完全是虚构的。&nbsp;</p>
  <p>这种被业界称为"幻觉"的现象，并非简单的技术缺陷，而是刻在大模型基因中的固有特征。&nbsp;</p>
  <p>面对这样一个既强大又"不可完全信任"的AI伙伴，我们人类该如何自处？&nbsp;</p>
  <p>2月18日，腾讯科技特别策划《AGI 之路》系列直播第三期：《面对AI生成的“幻觉陷阱”，我们该怎么办？》，邀请北京大学新闻与传播学院教授、《后人类的后真相》作者胡泳，清华大学长聘副教授、清华大学科技发展与治理研究中心主任助理陈天昊，出门问问大模型团队前工程副总裁、NETBASE前首席科学家李维，共同从技术、传播、治理等多个维度，揭开大模型幻觉的神秘面纱。并在直播最后，给出了普通人如何应对“大模型幻觉”的最实用建议。&nbsp;</p>
  <p>核心观点：&nbsp;</p>
  <p>幻觉刻在大模型基因里：大模型本质上是语言概率模型，用户给定前文条件，模型预测下文，这就是它的训练目标。大模型训练是对大数据的压缩，捕捉数据蕴含的大大小小的知识点和规律性。大模型不是死记硬背的数据库，训练数据中的“长尾”事实与噪音无异，被压缩剔除。面对这些缺失信息，模型会自动编造看上去“合理”的细节填补，形成AI幻觉。</p>
  <p>当我们今天讨论人工智能的错误时，本质上也是人类自身的错误：人类天生是带有偏见和刻板印象的动物，这使得在后真相时代，大模型造成的错误信息也更容易被接受、被传播。</p>
  <p>“专家之死论”潜藏着巨大危险，一定程度助长了“大模型幻觉内容”的传播：许多人认为像ChatGPT这样的技术是“专家终结者”，大量并不精通某一领域专业知识的普通人现在可以通过复制粘贴的方式给出看似令人印象深刻的内容。这加剧了普通人之间的竞争，同时使得真正的专业知识竞争相对减弱。这意味着那些具有表面知识而缺乏深入理解的人，可能会在某些领域获得更多的影响力和话语权。</p>
  <p>大模型生产方有能力减少模型幻觉程度：尽管我们承认语言模型无法完全克服幻觉或虚构症等现象，但研究也表明，大模型开发者实际上有能力逐渐减少幻觉的发生。</p>
  <p>如果将所有由虚构症或错误输出引发的责任完全归咎于企业，其实很不利于产业发展。任何新兴技术的诞生都是逐步完善的过程，不可能从一开始就做到完美，也不可能完全消除潜在的风险。</p>
  <p>以下是本期直播全部精华内容总结，在不改变原意的情况下有删减和调整：&nbsp;</p>
  <h2><strong>什么是大模型的“幻觉”？</strong></h2>
  <p>腾讯科技：首先请李维老师为大家科普一下，为什么说“大模型的幻觉是刻在基因里的”？&nbsp;</p>
  <p>李维：大模型的基本原理是基于上下文的预测，也就是在给定前文的情况下通过概率模型预测下文。预测的准确性取决于大模型在训练时如何处理和消化这些信息。大模型不是数据库，它压缩消化的是知识体系，包括常识或百科知识，但天然排除缺乏信息冗余度的长尾事实及细节。&nbsp;</p>
  <p>统计上，“长尾”事实与噪音无异。只有冗余信息和常识性内容最终嵌入模型参数中。当模型需要预测下一个词时，如果此处需要模型没有“记住”的长尾事实，它只能“编造”细节继续生成，这就是幻觉的来源。&nbsp;</p>
  <p>腾讯科技：您提到的编造事实”是一个很拟人化的说法，稍后我们可以深入探讨这个问题。接下来请胡老师分享一下，作为中国互联网发展的深度见证者和观察者，您认为AI时代大模型生成的内容与互联网时代的传播有什么不同？&nbsp;</p>
  <p>胡泳：这是一个相当大的话题，可以说差异非常多。我今天主要集中在虚假信息上。因为大模型自身的幻觉是虚假信息的重要来源之一。通常我们将大模型产生的虚假信息分为两类：&nbsp;</p>
  <p>第一类虚假信息来源于训练数据集的原材料不准确。大模型通过网络内容进行训练，而这些网络内容本身往往包含错误信息。人类在传播信息时会带有偏差，这些错误信息会被纳入训练集，进而影响模型输出。&nbsp;</p>
  <p>第二类虚假信息则是由模型推断出来的。在某些情况下，模型并未掌握某些事实，但为了自圆其说，它会进行推断，从而产生幻觉。&nbsp;</p>
  <p>正如李维老师所提到的，幻觉内生于大模型，且无法完全克服。即便数据量庞大，模型也只能捕捉到其中一小部分信息。导致很多事实性数据缺失。&nbsp;</p>
  <p>为了弥补这些缺失，大模型通过学习概念之间的关系来进行推断，但这种弥补方式就像是一个记忆缺陷的人凭直觉做事；模型在不知道答案的情况下仍然会给出一个“最好的猜测”，但这个猜测往往就是虚假信息。&nbsp;</p>
  <p>腾讯科技：那么为什么我们人类很难识别这种“最好但虚假的猜测”，导致它会在网络上被迅速传播？&nbsp;</p>
  <p>胡泳：当我们今天讨论人工智能的错误时，本质上都是人类自身的错误。人类天生是带有偏见和刻板印象的动物，这使得在后真相时代，错误信息也更容易被接受。&nbsp;</p>
  <p>过去我们可能还有一种共识，虽然人们存在不同的价值观，但事实是唯一的。如今，人们对于事实的认定也已经变得模糊。在后真相时代，不仅存在价值观的差异，对“事实”本身也有了不同的理解。&nbsp;</p>
  <p>此外，理性思维的败退也是一个重要因素。我认为基本有三个源流：&nbsp;</p>
  <p>1.很多人质疑推理本身，认为所有推理都只是合理化的过程。&nbsp;</p>
  <p>2.有人认为科学也只是一种“信仰”，是主观建构的。&nbsp;</p>
  <p>3.还有人相信客观性是一种幻觉，不存在客观真理。&nbsp;</p>
  <p>这三种怀疑主义的源流彼此支撑，导致理性讨论逐渐让位于情感驱动和直觉判断，使得事实难以统一，虚假信息更加广泛流行。&nbsp;</p>
  <p>腾讯科技：可以说，AI大模型制造的幻觉与人类自身的幻觉有一定的相似性吗？&nbsp;</p>
  <p>胡泳：是直接相关的。“幻觉”这个词本身就是一种拟人化的表现。因为我们常常将人类的特征投射到人工智能上，其实我个人认为“幻觉”这个命名不太好，应该换一种命名，但这个问题可以稍后再讨论。&nbsp;</p>
  <p>腾讯科技：请问李维老师，您认为在什么样的场景下，大模型容易产生幻觉？比如在提供论文信息时，它为什么能够清楚地提供一个虚假的论文标题和作者？&nbsp;</p>
  <p>李维：大模型在涉及具体实体（如人名、地名、书名、标题、时间、地点等）时最容易出错，这其实和人脑有相似之处，我们也往往记不住所有细节。大模型在消化数据时采用的是一种抽象过程，它试图从大量数据中找出各种规律，而不是记录所有细节。习惯性说谎者除外，人类记不住事实的时候，就说自己忘了或者添加“好像、可能”等不确定的语气。而现在的语言大模型与此不同，它“记不住”事实的时候，就会编造读起来似乎最顺畅的细节。&nbsp;</p>
  <p>胡泳：长期以来，我一直关注知识生产的过程，尤其是当前知识的生产方式。随着技术发展，专家的权威正在逐步下降。尤其是在中国，专家的角色常常被批评和质疑，甚至有时被讽刺为“砖家”。&nbsp;</p>
  <p>这种“专家之死”的观念在全球范围内流行了很久。许多人认为像ChatGPT这样的技术是“专家终结者”，因为它能够为各行各业提供看似专业的内容。很多人因此认为专家的作用在大模型出现后变得不再重要。但这种现象潜藏着巨大的危险。&nbsp;</p>
  <p>李维老师提到，像ChatGPT这样的模型容易误导人，因为它试图展现一种“准权威”的风格，实际上却无法避免错误和偏见。它的危险之处在于，当无法区分真伪时，它会自信地给出错误的答案，看似可信，实际上却可能误导了用户。因此，在使用这些大模型时，第一法则应该是怀疑而非盲信。大模型的开发者也已经意识到这一点，并提醒用户对其结果保持警觉。&nbsp;</p>
  <p>回到刚才的观点，虽然大模型降低了专家的门槛，但它实际上提高了真正成为专家的门槛。大量并不精通某一领域专业知识的普通人现在可以通过复制粘贴的方式给出看似令人印象深刻的内容。这加剧了普通人之间的竞争，同时使得真正的专业知识竞争相对减弱。这意味着那些具有表面知识而缺乏深入理解的人，可能会在某些领域获得更多的影响力和话语权。&nbsp;</p>
  <p>李维：但从另一个方面来看，大模型的“幻觉”其实是它抽象能力的体现。也可以理解为一种类似想象力的表现。&nbsp;</p>
  <p>例如，新闻记者撰写报道时，如果提供了虚假的信息，意味着不诚实；但小说家创作故事时，所有人物、时间和地点都可以是虚构的，这是创作的自由。大模型的情况类似于小说家，它编造“事实”其实是它学到的想象力的产物。</p>
  <h2><strong>如何应对大模型的“幻觉”？</strong></h2>
  <p>腾讯科技：所以大模型既像“新闻记者”，也像“小说家”；它既要遵循客观事实，又具备一定想象力。各位认为作为“非专家”应该怎样识别大模型在何时扮演“记者”的角色，何时又充当“小说家”的角色？尤其是现在推理模型能够在短时间内给出大量答案，并引用数十个甚至上百个来源。作为非专家，如何理性怀疑这些结果？</p>
  <p>胡泳：不管它引用多少个来源，给出多么雄辩的数据，依然要把怀疑放在首位。&nbsp;</p>
  <p>李维：我认为这是一个平衡的问题。许多人刚接触大模型时容易被它流畅的表达和广博的知识面所迷惑。特别是当你对某一领域不熟悉时，很容易被误导。因此，从大众的角度来说，怀疑的态度、保持警惕并核对信息是必要的。不过也需要找到一种平衡。如果你始终保持全盘怀疑，就无法最大化利用大模型的价值。&nbsp;</p>
  <p>对于深入使用大模型的专业人士来说，他们会发现大模型的确有其独到之处，可以迅速融会贯通大量知识，如果持怀疑一切的态度，可能会错失具有启发性的观点。我认为一个人随着使用大模型逐渐深入，能逐渐找到辨别真伪的感觉。一般来说，大模型的整体框架和逻辑通常更合理；但涉及某个具体事实的论述时则要保持警惕。&nbsp;</p>
  <p>腾讯科技：很多学生，甚至一些小朋友开始使用大模型来获取知识或帮助写作，天昊怎么看待这种现象？&nbsp;</p>
  <p>陈天昊：大家的共识是，大模型本质上是一个语言模型。虽然因为出色的自然语言处理能力，AI大模型已经扩展到包括法律、医学在内的许多领域，但它仍然只是一个“语文特别好”的工具，不能完全代替专业人士。所以本质上的问题是，我们的期待与现实是错配的。&nbsp;</p>
  <p>腾讯科技：那么像儿童、老年人这样特殊群体，或在法律、医学这样的严肃场景下，是否需要限制其使用？&nbsp;</p>
  <p>陈天昊：其实现在已经有相关的工作了。例如法律这样的垂直领域，有些企业会使用自身长期积累的法律数据库与更强大的底层大模型，从而提高输出的准确性。&nbsp;</p>
  <p>至于儿童的使用，情况就更复杂了。针对未成年人肯定需要更严格的内容筛查与引导。这更多是产品侧的问题。</p>
  <h2><strong>大模型的“幻觉”，不应该被称之为“幻觉”？</strong></h2>
  <p>腾讯科技：您之前提到认为“幻觉”这个词不太好，如果不叫它大模型幻觉，应该怎么称呼？</p>
  <p>胡泳：在人工智能中，有一个参数叫“温度”，与创造力的设置有关。创造力高时，模型就容易做出较为天马行空的猜测；设置较低时，则会根据数据集提供更准确的回答。这也是大语言模型使用起来有趣的地方。所以平衡创造力与准确性，实际上是使用大模型时的一大挑战。&nbsp;</p>
  <p>所以我个人一直认为，大模型幻觉这种现象不能一概而论。对于事实性问题，幻觉应该被摒弃，但如果是涉及想象力的领域，特别是娱乐性内容，幻觉则可以成为一个有用的提升创造力的工具。&nbsp;</p>
  <p>李维：能够编造故事、虚构事实，这也是人类智慧的一部分，而且是很关键的能力。赫拉利在《人类简史》中提到，人类的文明发展正是依赖于“讲故事”的能力，能够编造神话、宗教、理想，甚至情怀，这些形而上的东西。正是这种能力，才使得人类能够组织庞大的群体合作，战胜所有动物，成为地球的主宰。&nbsp;</p>
  <p>腾讯科技：虽然我们称之为“大模型幻觉”，但事实上，大模型并不能真正理解人类的语言。我们有时是否高估了大模型的能力，并且对其赋予了过多的拟人化评价？&nbsp;</p>
  <p>李维：的确，我们关于大模型的所有用词都是在拟人化的基础上进行的。人工智能实质是机器智能，只是在模拟人类智能。AI的一切行为，无论翻译、摘要、创作、解题、问答、聊天、自动驾驶，这些用词都是拟人的，只是电路和模型在运行。大模型的智能表现和回应，本质是基于概率模型进行的。但大模型爆发以来，我们都看到了，它的拟人的智能表现非常出色，以至于从行为上看，已经真假莫辨。这就是业界常说的，现代的大模型已经通过了“图灵测试”。&nbsp;</p>
  <p>腾讯科技：从人类的角度来看，主动说谎与无意识的错误是有本质区别的。而目前的大模型其实并不具备主动说谎的能力，是否可以这么理解？&nbsp;</p>
  <p>李维：是的，大模型本质上是一个概率模型，它的输出基于数据的统计概率，而非主动的意图，因此谈不上“主动说谎”。如果问题笼统，模型的回答可能也会很笼统，充满平庸或虚假的内容。而当你提供更多详细、具体的信息与它交流，等于是改变了概率模型的前文条件，这就会压缩它“说谎”的空间，模型的回答会更准确，也会更精彩。&nbsp;</p>
  <p>陈天昊：因为在我们讨论“主动说谎”时，预设了大模型具有主体意识，但当前的研究并没有达成这样的共识。我自己的理解是，大模型的本质是通过最大化条件概率来预测序列中的下一个token。在训练过程中，模型学习捕捉语言的统计规律和语义模式，从而逐渐形成对语言的表示学习，甚至有一些能力的涌现。但是它在本质上依旧是一个语言模型，尚不具备意识。&nbsp;</p>
  <p>腾讯科技：如果它传播了错误信息，这种负面影响是否与人类主动说谎造成的影响相同？&nbsp;</p>
  <p>胡泳：这里就可以说为什么“幻觉”这个词是有问题的，因为它其实存在拟人化的问题。当我们把人工智能做出的不符合训练数据的反应称为“幻觉”时，实际上是在用人的心理现象去解释机器的行为。过度拟人化会导致我们错误地认为大模型是有意识的，甚至是具备情感的。&nbsp;</p>
  <p>此外，过度使用“幻觉”一词，可能也是为产出大模型的公司提供了一个借口：输出错误的内容是模型的问题，而不是开发者的责任。&nbsp;</p>
  <p>因此，我主张用“虚构症”来描述这一现象。这个词来源于心理学，指当人的记忆出现空白时，往往会在无意中用合乎逻辑的理由来填补这些空白，也就是说人类的记忆并不可靠。这与大模型生成内容时的方式非常相似。&nbsp;</p>
  <p>腾讯科技：请问天昊老师，目前您看到的AI幻觉的典型危害案例有哪些？我们已经采取了哪些措施来应对这些问题？&nbsp;</p>
  <p>陈天昊：前两位老师的观点我非常认同。的确，过去我们可能只是价值观上存在分歧，但现在在事实层面也出现了严重分歧，这是一个巨大的挑战。尤其是随着DeepSeek的崛起，我们已经深刻感受到了ChatGPT带来的冲击，它正在迅速渗透到生活的方方面面。&nbsp;</p>
  <p>大模型带来的幻觉问题，早期主要出现在美国和欧洲。比如2019年有一宗涉及航空公司侵权的案件，在2023年进入法院审理。某方律师利用ChatGPT撰写法律简报时，引用了大量法院过去的判例。但法院审查起诉文书时发现，这些判例完全不真实。&nbsp;</p>
  <p>这是一个非常典型的例子，ChatGPT在找不到合适的资料时，会“编造”一些内容，以尽量满足用户的需求。刚好在这个判决里，法官亲自对这份文书进行了审查；如果他也同样使用ChatGPT或其他大模型，这些不真实的内容可能会被遗漏。&nbsp;</p>
  <p>另一个常见问题是，学术论文中的引用有时也会出现伪造。前不久，我向大模型查询法国法律的最新进展，得到的回答头头是道，但我查证后发现，它的引用完全是伪造的。&nbsp;</p>
  <p>这再次提示了两大风险：其一是，非专业人士因为有了大模型的支持，会逐渐对专业领域进行祛魅；其二则是，专业领域内部传统的依靠同行评审保证学术严谨性的自我审查机制也在逐渐退化，现在很多时候大模型也已经在发挥重要作用。&nbsp;</p>
  <p>腾讯科技：还有一个更复杂的问题。如果一个人脑补或虚构内容，最终犯错，责任应当由这个人承担。但如果大模型犯错，责任应该归咎于开发该大模型的公司，还是归咎于使用大模型生成内容的人？&nbsp;</p>
  <p>陈天昊：这是最棘手的问题，也是我们做这行的时候经常讨论的话题。我认为首先需要明确一个前提，尽管我们承认语言模型无法完全克服幻觉或虚构症等现象，这是语言模型的固有特性，但文献中也表明，企业实际上有能力逐渐减少幻觉的发生。&nbsp;</p>
  <p>过去谈到GPT模型时，提到过预训练（pre-training）。事实上预训练之后，还有一个后训练过程（post-training），后训练时会基于人类的反馈对模型进行有监督的微调，确保模型的输出更加符合预期。在微调过程中有一个很重要的点，就是特别强调模型不能造成伤害，尽可能避免产生负面影响。因此，许多基准测试（benchmark）也在衡量这一点。&nbsp;</p>
  <p>所以企业其实有很大的空间，可以投入资源逐步改进模型的表现，当然，同时也可能在一定程度上带来模型的“降智”，我们称之为对齐税（alignment tax），这是大模型由实验研发转变为运营产品所必须完成的一项工作。&nbsp;</p>
  <p>如果将所有由虚构症或错误输出引发的责任完全归咎于企业，其实很不利于产业发展。任何新兴技术的诞生都是逐步完善的过程，不可能从一开始就做到完美，也不可能完全消除潜在的风险。&nbsp;</p>
  <p>所以做产业政策时，通常需要权衡一个产业发展和对社会潜在伤害的最小化，尽量鼓励这些有巨大潜力来改善每个人福祉的产业发展。对于早期的负面影响，可以采取一些配套性的补偿措施尽可能地弥补这些伤害。&nbsp;</p>
  <p>比如，美国互联网产业初期的法律框架中就有“安全港”条款，规定平台企业对其上发布的信息不必承担全部法律责任；而如果平台在受到追责时及时删除相关信息，就可以免于连带责任。这对美国互联网产业的发展起到了很大的推动作用。</p>
  <h2><strong>大模型能力越强，越容易出现“幻觉”吗？</strong></h2>
  <p>腾讯科技：随着大模型技术的发展，它们的规模和迭代速度也在不断提升。在DeepSeek R1&nbsp;发布后，我们发现它的幻觉程度明显高于其基础模型V3及OpenAI的GPT-4等模型。这是否意味着推理能力越强，幻觉就会越严重？</p>
  <p>李维：行业内过去普遍认为，模型规模越大，尤其是在后训练充分、推理能力增强后，幻觉应该减少。然而至少在本次测试中， R1 的幻觉程度明显高于V3。说明这种关系并非简单的正相关或负相关，而是也受到其他因素的影响。&nbsp;</p>
  <p>但总体而言，随着模型规模扩大，训练数据也随之增多，信息冗余度自然提高，更多的事实和知识点能够被更有效地吸收到模型的参数中，从而降低幻觉的发生概率。此外，推理能力增强能够架起信息间的思维链“桥梁”，使模型更容易推导出正确结论，也有助于减少幻觉。例如，以前的非推理大模型面对一个复杂数学题做不出来，它就会编造答案。到了R1这种推理大模型，由于任务分解等思维过程的加入，答案正确的可能性大幅度提高，这显然是减少了幻觉编造。但上面提到的业界标准幻觉测量反映不出来这种进步，因为它们选择了一个单一的文摘任务来测量。这样的测量不能反映全貌。&nbsp;</p>
  <p>我注意到一个对比，Claude是一个非推理的业界顶尖大模型，它的幻觉程度按照相同标准评估甚至高于推理大模型R1。因此不能简单地认为推理能力的增强带来了更多幻觉。&nbsp;</p>
  <p>从 R1 的情况来看，它在所测量的摘要任务上，确实比自己的非推理的基座大模型V3增加了不少幻觉。我的理解是，R1在想象力和风格化表达上“用力过猛”，导致了它在摘要和事实性任务上的表现受损，而他们也没有对摘要类简单任务做特别优化。这是完全可能的，因为摘要这些常规任务，非推理大模型已经做得很好了，这时候，推理模型所加持的长思维想象力，虽然在创作类任务表现亮眼，反而可能在简单文摘类任务上带来副作用。其实，摘要任务根本就不必调用推理模型，V3就足够好了，而且会秒回。&nbsp;</p>
  <p>腾讯科技：但我们也观察到，推理模型在生成数据量和信息量方面，显著超过了原始基础模型。例如，现代大模型的生成能力与基础模型相比，其信息生成量远超移动互联网时代的水平。&nbsp;</p>
  <p>胡泳：结合刚才两位老师的意见可以得出一些结论。一方面，李老师提到的注意力问题非常关键。模型的关注点决定了它的输出特点。模型的设计的攻关方向与幻觉现象密切相关。&nbsp;</p>
  <p>另一方面，我们肯定大模型取得的突破性成就的同时，也不能忽视它的问题。例如，安全性不充分、幻觉频发、隐私保护不足等隐患。这些问题如果不解决，会影响它未来的发展。&nbsp;</p>
  <p>总的来说，我更倾向于用“虚构”这个词来代替“幻觉”，虽然大模型总有“撒谎”的可能性，但它对“虚构”存在一定抵抗能力，因此幻觉问题会随着时间的推移得到逐渐改善。但我们不能期待这个过程自主发生，而是需要社会和政府施加压力，推动企业在调整模型时投入更多对齐成本，以减少幻觉出现，降低对人类社会的负面影响。&nbsp;</p>
  <p>至于信息量的问题，过去我们曾担心数据存储的瓶颈会限制模型的训练。有预测指出，到了2026年，用于训练的数据将会枯竭。因此，许多机构开始限制数据的开放，纽约时报、Reddit等大型平台也开始要求付费使用数据。&nbsp;</p>
  <p>然而，合成数据的出现为这一问题提供了新的解决方案，如今数据的使用不再受限于传统的网络抓取方式。可以预见，数据的供给量不会很快枯竭，信息量将继续成倍增长，毫无悬念。</p>
  <h2><strong>如何与大模型的幻觉共处？</strong></h2>
  <p>腾讯科技：企业或模型开发方应在什么时候主动加大投入以防止幻觉带来的负面影响？现在是否已经到了该加强这类工作的时刻？</p>
  <p>陈天昊：头部大模型企业在这方面还是比较注重的，像腾讯这样的大企业就非常关注合规问题。在社科领域有一个理论，企业越大，面临的规范性压力也就越大。大企业往往会受到更多的监管和关注，因此在规范化方面的压力也较大。&nbsp;</p>
  <p>但我们不能要求每个企业都具备这种意识，更重要的是竞争压力。当同行之间展开竞争时，企业才会感受到来自市场的压力，被迫在对齐方面做得更好。竞争促使所有企业努力解决问题，我认为这比政府监管更为有效。&nbsp;</p>
  <p>尽管政府也有相关监管政策，要求AI生成的内容不能包含虚假或有害信息，但如何检验和实施这些政策、如何以更低的成本实现这些要求，最终还是需要企业研发团队和工程团队的紧密合作，尽可能在成本和对齐间寻求一个平衡。&nbsp;</p>
  <p>腾讯科技：目前大模型的技术水平下，我们能预见到的最大风险是什么？应该不会是什么“AI灭绝人类”这样的科幻情节，但从社会和传播层面来看，最严重的情况可能是怎样？&nbsp;</p>
  <p>陈天昊：虚假信息显然是最直观的影响。现在网络平台上传播的大量内容已经由AI生成。当我们验证一个事实时，习惯上会打开搜索引擎。但检索到的内容可能恰恰是AI生成的，从而存在幻觉，影响我们对事实的判断。&nbsp;</p>
  <p>腾讯科技：请问胡老师，从传播的角度来看，当AI生成的内容与人类创作的内容交织在一起时，这种现象可能会对社会产生什么样的影响？&nbsp;</p>
  <p>胡泳：这种就像所谓的“衔尾蛇”模式，最终所有的数据都将是合成的，分不清哪些是人类创造、哪些是AI生成。这会引发一系列严重的后果。特别是，我们会高估人工智能系统的智力，从而对它产生过度信任。这样一旦人工智能系统出现错误，后果就会相当危险。&nbsp;</p>
  <p>我们可以通过一个思想实验来进行预判。谷歌的拉里·佩奇曾承诺说，未来每个人可能都会拥有一枚植入物，使得人们能通过思维联网即时获取答案。如果几代人都使用这种植入物后，我们就会彻底习惯这种技术，而忘记了通过观察、询问、推理获取知识的能力。最终我们会发现，我们对世界的认识将完全依赖于这些技术，而作为“我”的个人意识则不存在了。&nbsp;</p>
  <p>腾讯科技：我们之前提到，在大模型的时代应对幻觉需要个人有更高的辨别能力。胡老师曾提出过互联网平权的概念，您认为AI是带来了平权的机会，还是加剧了技术使用的鸿沟？&nbsp;</p>
  <p>胡泳：关于提高人工智能素养(AI literacy)的讨论，确实每个人都应该对自己的行为负责，但我们需要思考：为什么只强调用户端的责任？为什么不要求AI公司承担应尽的责任，从源头上降低误用风险？&nbsp;</p>
  <p>以2023年美国一个著名的访谈节目采访谷歌CEO桑达尔·皮查伊的对话为例。皮查伊承认AI存在"黑盒"问题——我们往往无法解释AI为什么会出错。他表示随着技术进步，这些问题会逐渐得到解决。这个说法表面上似乎无懈可击，但主持人一针见血地质疑：既然连你们都不了解AI如何运作，为什么还要将其推向全社会？&nbsp;</p>
  <p>皮查伊的回应是，我们正处于AI技术革命时期，需要以"谦逊的态度"来对待这项技术。但这实际上可能反映了某些大型AI公司的功利主义思维：明知存在风险，却选择先发布产品，期待在未来的使用过程中持续去改进，那谁来承担风险呢？&nbsp;</p>
  <p>所谓对AI保持"谦逊态度"，不应该变成将未经充分测试、未经充分对齐的系统贸然推向市场，期望社会自行消化其带来的问题。相反，AI公司在开发和发布产品时，就应该充分考虑用户需求和经验，研发团队应该与监管机构、用户群体共同努力，找到负责任且合乎伦理的AI应用方式。这个问题值得我们严肃对待。&nbsp;</p>
  <p>陈天昊：我完全支持胡老师的观点。其实从ChatGPT刚发布时起，我们就意识到这是一个非常危险的产品，它将一项能力巨大的技术投入世界，而全球社会尚未做好准备。&nbsp;</p>
  <p>对齐问题非常复杂，因为在人类内部就存在巨大的分歧，具体和谁对齐是一个严肃的问题，无法简单回答。各个大模型企业只能尽力根据其技术手段，选择代表性的人群和数据来进行训练。&nbsp;</p>
  <p>至于平权的问题，与其把它当作一种伤害，不如视为一个机会。因为大模型确实打破了许多知识壁垒，使我们能以低成本地接触到最前沿的知识。尽管其中也有虚假信息，但我们不能因此放弃这片大海。虽然其中有些人可能会无辜暴露于风险之中，但我们也别无选择。既然技术已经被释放出来，我们只能接受现实，尽量做好应对准备。&nbsp;</p>
  <p>当然，企业应承担更多社会责任，法律和规章制度也会提出要求。我相信在竞争压力下，企业会尽力去做好这些工作。我认为这方面在产品端还有很多工作可以做。&nbsp;</p>
  <p>腾讯科技：最后希望三位老师能提供一些实用性建议，普通人如何应对幻觉？&nbsp;</p>
  <p>李维：首先，“搜索”按钮是对付幻觉非常重要的武器。它能够集中互联网上的相关话题，信息密度高，从而提高回答的真实性和准确性，压缩幻觉冒头的机会。&nbsp;</p>
  <p>其次，如果从事创作类工作，可以使用“推理”功能，发挥它强大的想象力，生成意想不到的漂亮文章，甚至在某些方面超越传统写作的限制。&nbsp;</p>
  <p>最后，如果直接要求大模型做摘要这类重事实的简单任务，调用推理大模型结果可能存在失真，简单的办法就是不要用推理模型（R1界面下，不要按 deepthink 的按钮即可）。如果用了推理模型，可以尝试增加一个提示词，比如“请务必忠实于原文进行摘要”，这样对后续的生成可能有所约束，从而降低犯错的机会。&nbsp;</p>
  <p>胡泳：第一，可以尽量使用多个大模型。每个模型都有自己的优势，在使用不同的模型后，可以逐渐得出自己的心得体会， 获得更好的结果。&nbsp;</p>
  <p>第二，对于某一领域的专业人士，建议使用基于特定行业语料库训练的垂类模型。这些模型通常能更好地服务行业需求，帮助专业成长。&nbsp;</p>
  <p>陈天昊：首先，与大模型互动时，尽可能详细地说明自己的需求。输入的信息越充分，输出的准确性和对齐度就越高。&nbsp;</p>
  <p>其次，尽量使用多个大模型来进行对比验证。&nbsp;</p>
  <p>最后，认识一些人类专家并与他们多交流，他们拥有一些现在技术阶段的大模型尚未覆盖到的知识，可以提供更加可靠的意见。当然，更重要的，其实是提升我们自己的认知能力和批判能力。&nbsp;</p>
  <p>技术的狂飙终须与人类的智慧同行，构建怀疑与信任相平衡的“人机关系”，或许我们才能守住“真实”的底线。</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/zsK0XmDIZOsmBXLHnYJZrQ" rel="noopener noreferrer nofollow" target="_blank">“腾讯科技”</a>，作者：腾讯科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3194641155166345</id>
            <title>DeepSeek走下神坛？阿里QwQ模型32B参数吊打671B</title>
            <link>https://www.36kr.com/p/3194641155166345</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3194641155166345</guid>
            <pubDate></pubDate>
            <updated>Thu, 06 Mar 2025 12:01:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, QwQ-32B, 推理模型, C端市场

总结: 阿里云发布了最新的推理模型QwQ-32B，尽管参数规模较小，但其性能与更大的模型DeepSeek-R1相当。QwQ-32B集成了Agent相关能力，提升了模型的适应性与智能性。该模型在多项评测中表现优异，尤其在数学和代码能力方面。阿里希望通过QwQ-32B推动AI技术在C端市场的普及，满足用户多样化的需求，并在内容创作、教育等领域发挥重要作用。未来，QwQ-32B有望进一步提升阿里在C端市场的影响力。 </div>
                        <hr>
                    
                    <p>3月6日，阿里云通义千问官方宣布推出最新推理模型QwQ-32B，这一模型仅有32B参数，但在效果上与拥有671B参数的DeepSeek-R1相媲美。如果你自己部署DeepSeek-R1但资源不够的话，又多了一个新的选择。</p>
  <p>QwQ-32B的独特之处不仅在于其参数规模和效果表现，还集成了与Agent相关的能力。这使得模型在使用工具时能够进行批判性思考，并依据环境反馈灵活调整推理过程，极大提升了模型的适应性与智能性。</p>
  <p>那么，降低了部署难度的QwQ-32B会让大模型本地化更加普及吗?阿里的大模型会继续在C端越走越远吗?</p>
  <h2><strong>QwQ-32B性能如何?</strong></h2>
  <p>今日凌晨3点30，阿里巴巴正式发布通义千问最新开源模型QwQ-32B，它比DeepSeek有更小的尺寸，性能比肩全球最强开源推理模型。</p>
  <p>根据官方披露的测试结果，QwQ-32B在多项关键评测中表现非常出色：</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250306/v2_dc995a1db1db4fcd9cce209edbc98c97@813924438_oswg87334oswg665oswg382_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>1、在测试数学能力的AIME24评测集上，以及评估代码能力的LiveCodeBench中，千问QwQ-32B表现与DeepSeek-R1相当，远胜于o1-mini及相同尺寸的R1蒸馏模型;</p>
  <p>2、在评估代码能力的LiveCodeBench中，表现同样与DeepSeek-R1相当;</p>
  <p>3、在由Meta首席科学家杨立昆领衔的“最难LLMs评测榜”LiveBench、谷歌等提出的指令遵循能力IFEval评测集、由加州大学伯克利分校等提出的评估准确调用函数或工具方面的BFCL测试中，千问QwQ-32B的得分均超越了DeepSeek-R1;</p>
  <p>4、在谷歌等提出的指令遵循能力IFEval评测集中，成绩优于DeepSeek-R1;</p>
  <p>5、在加州大学伯克利分校等提出的评估准确调用函数或工具的BFCL测试中，同样超越DeepSeek-R1。</p>
  <p>据通义千问Qwen团队介绍，近期的研究表明，强化学习可以显著提高模型的推理能力。例如，DeepSeek-R1通过整合冷启动数据和多阶段训练，实现了最先进的性能，使其能够进行深度思考和复杂推理。</p>
  <p>而且，我们还看到，QwQ-32B在开源后，获得用户和业界积极反馈，表明其在C端市场有巨大潜力，有望吸引更多用户使用及开发者参与，推动阿里在C端市场的进一步发展。</p>
  <h2><strong>阿里大模型在C端越走越远</strong></h2>
  <p>随着QwQ-32B大模型的发布，阿里通义千问正以独特的方式面向C端用户，从而希望AI技术更加普惠。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250306/v2_d5103cc53f8f4931b011e45f43a85892@813924438_oswg49211oswg692oswg372_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>比如，从知识获取的角度来看，通义千问打破了传统知识传播的壁垒。以往，人们获取知识往往依赖于书籍、学校教育等相对固定的渠道，存在一定的局限性和时效性。</p>
  <p>而通义千问凭借其强大的数据整合与分析能力，能够迅速汇聚海量信息，并以通俗易懂且准确的形式呈现给用户。无论是学生在学习中遇到的复杂历史事件、物理难题，还是职场人士需要了解的行业前沿动态、专业技能培训等内容，通义千问都能精准地提供相应知识解析。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250306/v2_a61fb01dee4d4436af385edc167eb7d4@813924438_oswg300781oswg690oswg378_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>据相关数据显示，在学习类查询中，超过80%的用户表示通过通义千问能够更快地理解知识点，且知识记忆的准确性平均提升了30%左右。例如，一位初中生在使用通义千问询问物理中的电磁感应现象时，不仅得到了详细的原理解释，还获取了多个相关的实验视频链接以及在生活中的实际应用案例，这种全方位的知识展示方式远胜于单一的教材讲解，极大地激发了学生的学习兴趣与探索欲望。</p>
  <p>在日常生活的应用场景中，通义千问成为人们贴心的生活助手。它能够提供生活小窍门、旅游攻略制定、美食推荐等服务。</p>
  <p>以旅游为例，当用户计划前往一个陌生城市旅行时，在通义千问输入目的地，即可迅速生成包含景点推荐、行程安排、住宿选择以及当地特色美食介绍等一整套旅游攻略。根据平台统计，使用通义千问制定旅游计划的用户，对旅行满意度的评价相比传统自行规划旅行的用户高出25%。而且在出行过程中，如遇到突发情况如天气变化需要调整行程，通义千问也能及时根据最新信息提供应对方案，确保旅行的顺利进行，让普通用户在日常生活中感受到科技带来的便捷与高效。</p>
  <p>对于内容创作群体，通义千问更是发挥着重要作用。无论是自媒体创作者、文案撰写人员还是艺术设计者，都能从中汲取灵感。它可以帮助创作者进行选题策划，提供当下热门话题趋势分析;在文案创作时，辅助生成多样化的写作思路与风格示范;甚至在艺术创作方面，通过提供不同艺术流派的特点介绍以及经典作品赏析，激发创作者的创意灵感。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250306/v2_8989f2bead1f4b3c9b669ccf04f75ffe@813924438_oswg523216oswg692oswg519_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>有数据显示，在使用通义千问辅助创作后，内容创作者的创作效率平均提升约40%，作品的创新性与受欢迎程度也显著提高。比如一位自媒体博主在撰写关于环保主题的文章时，借助通义千问找到了最新的全球环保数据、不同国家的环保政策创新案例以及读者关注度较高的环保细分领域等信息，从而使文章发布后获得了远超以往的阅读量与互动量。</p>
  <p>然而，通义千问面向C也面临一些挑战。如在信息准确性方面，由于网络信息繁杂，尽管其有一套严谨的数据筛选机制，但偶尔仍可能出现信息更新不及时或存在偏差的情况。</p>
  <p>再者，部分用户过于依赖通义千问，可能导致自身独立思考能力的弱化。但总体而言，通义千问以其丰富的功能、便捷的操作以及对知识传播、生活服务和内容创作等多方面的积极影响，正在逐步改变着C端用户的生活方式与思维模式，成为普通大众在数字时代不可或缺的重要工具，持续推动着个人成长与社会进步的车轮滚滚向前，其在未来的发展中也必将发挥更加卓越的作用，为C端用户创造更多可能与价值。</p>
  <h2><strong>写在最后</strong></h2>
  <p>整体来看，阿里发布的QwQ-32B模型，凭借其高性能、低成本、易部署等优势，使其在C端市场的拓展上迈出了重要一步。它不仅降低了使用门槛，满足了广大C端用户多样化的需求，还在内容创作、教育普及等多个领域发挥着重要作用。</p>
  <p>未来，随着阿里在C端应用的不断深入与拓展，QwQ-32B模型有望进一步推动阿里在C端市场的影响力与竞争力提升。</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/uN1X4u5v0vDgD6AfO5lHMg" rel="noopener noreferrer nofollow" target="_blank">“科技旋涡”</a>，作者：科技旋涡，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3194629614373254</id>
            <title>互联网上到处都是“辣眼睛”的“AI写真”</title>
            <link>https://www.36kr.com/p/3194629614373254</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3194629614373254</guid>
            <pubDate></pubDate>
            <updated>Thu, 06 Mar 2025 10:52:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI写真, 社交平台, 内容监管, 擦边内容

<br>
<br>
总结: 文章讨论了AI写真在社交平台上的滥用现象，尤其是“擦边”内容的泛滥。用户对平台内容生态的失望和质疑，认为平台监管不力，导致不良内容传播。文章指出，平台算法和流量机制助长了此类内容的传播，尽管监管部门已采取措施，但效果有限。用户呼吁平台加强监管，维护纯净的内容生态。 </div>
                        <hr>
                    
                    <p>“AI很好很强大，但如果在使用的过程中变味，那就值得深思了。”</p>
  <p>最近，沉迷AI写真的90后林姐颇为感叹，她直言，如今，社交平台的各种AI写真，颇为“辣眼睛”，“现在，我只要打开那些社交平台，整个人都难受。”</p>
  <p>“这种难受，是直接的视觉冲击。”林姐直言，“直接跳出来的是各种AI美女，或行走在‘擦边’上的AI帅哥，这和日常想象和见到的AI写真相差甚远。一点也不正常，‘辣眼睛’。”</p>
  <p>而更普遍的是，打开小红书、抖音等社交平台，运营该类账号的博主，发布的“职业装小姐姐系列”、“泳装系列”等，有的还在内容上打上“无不良引导”的标签。</p>
  <p>这在很多用户看来，简直“指鹿为马”。一位网友甚至表示，“在社交平台天天高喊严监管，积极构建优质内容生态的今天，自己却仿佛回到了曾经那个看成人网站的年代。”</p>
  <p>还有网友表示了强烈的质疑，“平台对成人内容的放开，是否想过对未成年人的荼毒？”</p>
  <p>而于更多的网友而言，他们不止一次对平台感到失望。</p>
  <p>小红书重度用户小然就表示，“每一次内容的扩容，小红书似乎与‘擦边’都会扯上关系。从旅游搭子事件到名媛培训班，再到AI应用，形形色色不良引导的内容就没干净过。”</p>
  <p>显然，企业目标和内容操作的悖论，这很难让用户理解。</p>
  <p>无论平台审核标准是否精细，还是创作者利用了平台机制的灰色地带，亦或整个监管还存在漏洞，在“万物AI”的时代，用户需要的仅仅是一个干净且高质量的内容生态。</p>
  <p>实际上，监管部门对此高度重视。2025年3月5日，在十四届全国人大三次会议首场“部长通道”上，国家市场监督管理总局表示，今年重点整治“大数据杀熟”、“主播恶意炒作”等内容。</p>
  <p>或许，平台是时候真正高度重视那些利用AI恶意制作的不良内容了。</p>
  <h2><strong>-01- “辣眼睛”的AI内容</strong></h2>
  <p>林姐是一名90后，日常的工作就是新媒体运营，也会在社交平台运营相关内容。在看到平台很多人用AI拍写真让“老照片复活”，假装在夏威夷群岛旅游，嵌入多套造型和发型，实现百变造型“拍照片”后，林姐也想玩一玩AI。</p>
  <p>按照经验，林姐打开社交平台先搜索AI拍写真的教程，没想到输入“AI写真”关键字后，跳出来的页面居然大多数是“AI美女”和“AI帅哥”。</p>
  <p>但让林姐震惊的是，这些左下角标识“由AI生成”的照片，并不是传统意义上的写真。这些内容虽是AI合成，但无论男性女性，都衣着暴露，或展示身材，或做不雅举动，整体给人的感觉就是“辣眼睛”。</p>
  <p>让林姐尤其不能理解的是，她是初次在社交平台搜“AI写真”，平时也对这类照片完全不感兴趣，何以被大数据推送至该内容？</p>
  <p>作为一位赛博原住民，林姐经历了早期互联网内容的野蛮时代。最早，林姐上网，网站随处可见成人内容页面，但她没想到的是，“都2025年了，这些擦边内容依然活跃在大众面前。刷短视频，就像在看那些成人网页。”</p>
  <p>习惯逛小红书的小然，对此也深恶痛绝。</p>
  <p>小然是一名00后，平日喜欢美妆和旅行，由此成为小红书的常客。小然也是一名潮流新科技应用爱好者，在知晓“AI写真”后，便在小红书率先浏览关于“AI写真”的博主使用和拍摄心得。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250306/v2_e07dc53d3a15429889e2cf806b4a89b9@5951134_oswg165102oswg1077oswg931_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">图：社交平台关于”AI写真“的内容，来源：小红书《左》、抖音（右）&nbsp;《听筒Tech》截图</p>
  <p>“不看不知道，一看吓一跳。”搜索“AI写真”，小然的小红书页面瞬间被显露身材的“美女”和“帅哥”霸占。</p>
  <p>“他们是非典型性写真。有的图显示‘疑似包含AI创作信息’，有‘浴袍’、‘健身OOTD’、‘跑马的汉子’等等系列，但内容都直指‘擦边’。”小然感慨，“这是小Hong书，还是小Huang书？”</p>
  <p>曾经一度，在“旅行搭子”盛行的时候，小红书就爆出存在“不良引导”新闻。小然也曾在小红书找旅游搭子的时候，发现类似案例。</p>
  <p>令小然没想到的是，一直将高质量内容作为平台标签的小红书，依然保持原来的样子。尤其是，此前Tiktok因受封事件，小红书一度在美登顶下载量第一，一大波海外红人也入驻小红书，连小然都引以为傲地表示，“不得不说，中国社交应用也算是在海外出息了。”</p>
  <p>但如今，一个小小的体验，再次让小然感到失望，“有些无语，也有些看不上。如果一直保持这样的内容生态水准，没准自己会慢慢逃离这个平台。”</p>
  <p>据小然透露，实际上，不仅仅是小红书，在抖音、快手、微信视频号等社交平台，同样存在大量类似的内容。</p>
  <p>小然表示，自从在抖音上搜索过“AI 写真”后，他的推荐内容便“画风大变”。如今，小然直呼，他的抖音账号已经完全无法直视，“我是硬生生地将抖音刷成了青楼。”</p>
  <h2><strong>-02- 快速起号，急于吸金</strong></h2>
  <p>“AI疯狂秀肌肉”，“真人越来越假，AI越来越真”，似乎慢慢成为最贴切的形容。</p>
  <p>而运营这些“AI美女”账号的博主，正在利用擦边吸引力，快速起号并掘金。</p>
  <p>在抖音，一些“AI美女”穿着暴露，在平台不断发布跳舞视频，短短半年就涨粉100多万。</p>
  <p>据媒体报道，去年，抖音还有一名为“小姨妹”的博主，每条视频都带不同产品，价格一般在10元以内，但封面图却是AI生成的美女加上产品介绍，一度收获超110万粉丝。</p>
  <p>甚至还有博主，将此视为下一个“捞偏门”的掘金赛道。在抖音，有不少博主一本正经讲解，该“如何成为一个AI擦边博主”。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250306/v2_ddf9fef571974a19bc523f00305f2209@5951134_oswg43760oswg1080oswg323_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">图：社交平台关于”如何成为AI擦边博主“的内容，来源：抖音&nbsp;《听筒Tech》截图</p>
  <p>实际上，在小红书，从2024年开始，平台就出现了妆容精致、身材火辣的AI写真美女快速起号。这些博主经常发布穿着泳衣在海边度假，或者在时髦的街道拍照的美女照片，点赞量超过几千。</p>
  <p>以坐标浙江的某博主为例，其认证“设计服务公司”，服务涉及“数字视频和数字写真”，从2024年3月开始发布第一条笔记，内容多为擦边，涉及“纯欲风”、“御姐风”等。截至目前，其粉丝超3.6万，获赞与收藏超20万。</p>
  <p>实际上，在平台早期，很多人通过简单的“拍一拍、写一写”就能收获不错的流量。但随着平台内容生态的逐渐成熟，内容分发机制越来越依赖算法逻辑，而非单纯的运气。</p>
  <p>一位小红书运动博主亦向《听筒Tech（ID:Tingtongtech）》透露，她的内容如果“稍微擦点边”，流量便比正常发布的内容要好。</p>
  <p>在该运动博主看来，虽然平台并未承认过“擦边内容能够吸流”，但经过她的多次尝试，至少在她看来，“擦边的内容，流量远比正常发布的内容要高。”</p>
  <p>该运动博主表示，不仅仅是小红书，在抖音、快手、微信视频号等短视频平台，同样存在类似的问题，“我在这几个短视频平台都尝试过发泳装AI短视频，流量非常好。”但考虑到她的运动IP形象，她最终没有走该风格。</p>
  <p>一名在社交平台担任运营工作的内部人士也向《听筒Tech》表示，一般来说，内部生态会给到特定内容的流量支持，在平台的扶持下的快速起号。</p>
  <p>上述内部人士表示，“流量支持的参考标准，包括关键词匹配、兴趣推荐，以及互动和停留时间等不同参考维度。比如，用户的点赞、收藏、评论，以及在笔记上的停留时间，直接决定了内容是否会被推荐给更多人。”</p>
  <p>这也造成这样一个误区， 很多人以为爆款内容是“随机的”，但实际上，爆款背后都有清晰的流量逻辑。当然，上述内部人士亦表示，“刷流”也是重要的推流手段。‍‍</p>
  <p>针对“上述野蛮生长的AI擦边写真，是否存在一定意义上的扶持”，上述人士表示，“这并不好说，但平台的算法问题肯定存在。”</p>
  <h2><strong>-03- 是时候高度重视了</strong></h2>
  <p>无论是大量擦边AI写真内容的存在，还是兜售AI擦边生意的卖课行为，这背后都直指一个问题，平台监管存在漏洞。</p>
  <p>而这已不是第一次出现。</p>
  <p>以小红书为例，在过去的几年里，平台涉黄、违规交易等都曾掀起过舆论焦点。</p>
  <p>2020年11月，小红书被曝出推送大尺度美女裸露照片和视频，内容露骨且包含暗示性。2021年9月，央视曝光小红书存在大量泄露未成年人身体隐私的视频。</p>
  <p>更广为人知的，2023年8月，一篇题为《小红书“旅游搭子费用全包”涉黄了》的文章在网上引发轩然大波，称小红书上的旅游搭子笔记，打着”找搭子“旗号，实则涉及性交易。</p>
  <p>尽管，每次小红书都表示将进行调查和处理，但实际效果却并不理想。</p>
  <p>即便时间走到2025年，面对TikTok“面临封禁”期间，海量美国用户投向小红书，几乎在一夜之间，小红书平台就出现了大量海外博主擦边信息和照片。</p>
  <p>这再次暴露了小红书内容生态监管的薄弱。而彼时，有业内人士表示，小红书真的需要进一步细化审核标准了，以适应不同法律环境下的内容监管要求，避免因内容违规而引发的法律风险。</p>
  <p>小红书也不是没有行动。今年2月13日，小红书官方账号“薯管家”发布公告，称从去年12月至今，治理团队对美妆个护、食品、3C数码、生活服务等15个行业、1695个存在违规营销行为品牌进行了处罚。</p>
  <p>但从目前的情况来看，小红书似乎有点“心有余而力不足”。</p>
  <p>而抖音方面，尽管也进行了多次对“涉黄”等相关内容进行监管，但截至目前，依然无法有效改善平台的生态。</p>
  <p>实际上，这种反复出现的问题不仅损害了平台的形象和声誉，更让用户对其信任度大打折扣。尤其是，在监管日益严格的今天，平台如何有效遏制不良内容的传播，仍成为亟待解决的问题。</p>
  <p>不过，一位算法工程师告诉《听筒Tech》，“实际上，各大主流App，也都推出了AI标识功能，而且也在加强对AI图片、视频的审核。但从目前的情况来看，效果一般。尤其是面对越来越先进的AI工具，有时连平台也无法真正判别出来。”</p>
  <p>当然，对此，国家方面也有了一定的动作。2024年9月，网信办发布了关于《人工智能生成合成内容标识办法（征求意见稿）》征求意见通知，要求平台和服务提供者，对AI生成内容明确标识，比如添加水印。</p>
  <p>但这仍需要平台端的大力支持。</p>
  <p>此前，中国社会科学院大学法学院副教授、互联网法治研究中心主任刘晓春表示，“为防止平台利用AI涉及暴力、侮辱性内容的输出，可采取不同的技术手段。在输出端，服务商应进行筛选和再次审查。”</p>
  <p>不过，面对用户的种种质疑，平台的监管力度明显仍有欠缺。至少目前来说，这无法改变用户对平台和整个生态的不信任。</p>
  <p>“你美得像AI”，对女生来说，这是一种让人难以抗拒的赞美。AI科技的进步不可逆转，相关应用也确实让人们的生活更美好。</p>
  <p>“但仅从擦边写真方面来看，如果我们或者我们的下一代，仍被各种随处可见的色情内容和低级趣味所喂养，那大概是人类为科技进步和平台的逐利行为，所付出的惨痛的代价。”小然直言。</p>
  <p>“我们仍期待AI向善，不要被恶意利用。”在小然和林姐看来，“我们不是批评平台，我们更希望平台能够加强监管，还网友一个真正纯净的空间。”</p>
  <p>而这些，都建立在平台、技术、监管部门和整个社会对内容生产保持基本的尊重和敬畏的基础之上。</p>
  <p>（文中均为化名。）</p>
  <p>（声明：本文仅作为信息交流，不构成任何投资参考建议。）</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/WwFZ4oy7Wqzxr4lFm3UoNA" rel="noopener noreferrer nofollow" target="_blank">“听筒Tech”</a>，作者：小听‍，编辑：饶言 ，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>