<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/3095287739252488</id>
            <title>即梦对阵可灵，重演抖音反超快手？</title>
            <link>https://www.36kr.com/p/3095287739252488</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3095287739252488</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Dec 2024 07:13:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <抖音, 快手, AI, 视频生成>
<br>
<br>
总结: 本文讨论了抖音的即梦与快手的可灵在AI视频生成领域的竞争。即梦被定义为“想象力世界”的相机，而抖音则是“真实世界”的相机。尽管即梦在推出时引发了热度，但其用户互动量在一个月内呈现高开低走的趋势，反观可灵则表现出低开高走的态势。快手可灵凭借稳定的产品能力和成功的营销策略，吸引了更多用户。未来，抖音即梦与快手可灵将在多个市场展开正面竞争，尤其是在电商广告和短剧等领域。字节跳动的AI发展将重点关注多模态模型和世界大模型的转型。 </div>
                        <hr>
                    
                    <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_186f77cb2717489e9bbd865dec22d2d6@65858_oswg355958oswg900oswg500_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p><strong>快手可灵，只是抖音即梦的下一个目标罢了。</strong></p>
  <p>年初从抖音 &nbsp;CEO &nbsp;位置上辞职的张楠，应该度过了一个充实的 2024 年。</p>
  <p>在中央美术学院设计学院四层阶梯空间里，张楠这位曾经的艺术生坐在 C 位，微笑着与一群大学生合影留念，她是评委之一。这是一场 AI 交互创新大赛的分享交流会，主办方就是张楠如今负责的业务重点：剪映旗下的 AI 产品即梦，这被视作一款可能再造抖音的产品。</p>
  <p>几天之后，张楠公开把即梦定义为「想象力世界」的相机，而抖音是一个「真实世界」的相机。</p>
  <p>要打造想象力的世界，创新的来源更多是年轻的头脑。抖音的成功离不开年轻的高校学生，他们曾是抖音早期运营团队关注的重点人群，各种经典的挑战类活动，都是靠着这些年轻的创作者们踊跃参与而不断传播开来，但与早年间抖音运营没什么钱、靠打感情牌不同，如今，即梦 AI 交互创新大赛一等奖是 10 万元的奖金。</p>
  <p>得奖人赵纯想并不是学生，而是一个年轻的独立开发者，此前凭借一款饮食记录的产品「胃之书」崭露头角。而这次获奖的作品，是一个 AI 视频生成镜头精细控制 UIUX 方案，在展示的 2 分钟 Demo 中，导入一张《天堂电影院》的经典场景图，用户就可以生成一段视频，实现推拉镜头、特写、窗外鲜花盛开等 AI 创作。</p>
  <p>而在即梦面世前三个月，快手可灵已经在国内率先上线了视频生成大模型，它们同样没有忽视高校里的年轻人，快手可灵与中国美术学院等高校联合举办了 AI 创作大赛，获得一等奖的三个作品，聚焦生活、广告和自由发挥主题，分别拿到了奖金 36666 元。</p>
  <p><strong>Sora 打开了为真实世界建模的 AI 大门，即梦和可灵则顺着抖音和快手走过的路，用更饱和式投入，以大力出奇迹。</strong></p>
  <p>更大的竞争就在不远的 2025 年，网传一份对字节跳动 AI 视频生成产品的调研纪要显示，「字节希望在其生态内使用 AI 能力，认为明年各生态将形成竞争闭环。明年五一之前，扣子智能体平台、豆包、抖音和 B 端的能力会形成关联的生态网，文生视频将会有更多的体现和使用场景。」</p>
  <p>2025 年，在电商广告、短剧等多个市场，抖音即梦或许将与快手可灵正面交锋。</p>
  <h2><strong>高开难高走</strong></h2>
  <p>「高开低走」与「低开高走」，是现在抖音即梦与快手可灵最大的差别。</p>
  <p>QuestMobile 最新数据显示，即梦上线当天即在抖音平台引发了高涨的讨论热度，可灵 AI 通过热度的持续积累也在快手站内出现热度峰值。但一个明显的不同是，即梦的内容互动量在一个月高开低走，而可灵的内容互动量在一个月内低开高走。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_8e75530549814e5fa34018b2ddc2f313@65858_oswg100489oswg552oswg335_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p>
  <p>这是推广策略、用户体验、市场竞争等多种因素共同作用的结果，但一个最直接的原因，可能就是即梦的产品发布较晚，体验不如预期，用户深入使用之后，很容易区分与同类产品在生成内容上的质量和稳定性，即便初始阶段获得了大量预热，目前即梦在热度上仍不及可灵。</p>
  <p>这并非意味着即梦的绝对落后，有 AI 产品深度使用的用户认为，当下国产 AI 产品想要用好，就不能单用一个，尤其是在文生视频创作中，用户选择的做法往往是用即梦来做前期的文生图，然后再用可灵来制作图生视频，因为「即梦的 AI 生图比较优秀」。</p>
  <p>字节调研纪要同样显示即梦和可灵之间差距很大，即梦日活用户在 20 万-22 万之间，其中 70% 为个人或小型 MCN 工作室，大型企业较少，付费用户 2.5 万人左右，月订阅付费均价在 50 元左右；而同期可灵服务用户超 500 万，累计付费用户超 200 万，付费金额累计约千万级别。</p>
  <p>这种调研纪要难辨虚实，因为二级市场爆火的「豆包概念股」，字节跳动官方曾对投资者们做出风险提示，以免遭受不必要的投资损失。</p>
  <p>而快手可灵之所以「低开高走」，一方面是视频生成大模型本身的能力更稳定，占据了先发优势，另外一面则是营销策略的成功。「蓝洞商业」在<a href="https://mp.weixin.qq.com/s?__biz=MzU2ODc5ODE0OQ==&amp;mid=2247493190&amp;idx=1&amp;sn=d1b8ab0f4b6406a5fe00f4e87db2fec8&amp;scene=21#wechat_redirect" rel="noopener noreferrer nofollow" target="_blank">《快手可灵，把压力给到了抖音剪映》</a>中已经提到，当时快手通过海外科技大 V 测试可灵生成的案例，成功营造出其在海外市场火爆的氛围，进而「出口转内销」。</p>
  <p>半年过去，目前可灵在海外市场的关注度仍远高于即梦，社交媒体 X 的关注人数是即梦的 67 倍。</p>
  <p>就在张楠出现在火山引擎大会上，发布即梦最新消息的当天，快手又升级了可灵大模型，宣称在内部评测中比此前 1.5 模型整体效果提升 195% 。而此前一个月的三季度财报会上，创始人程一笑也提到对可灵的憧憬：可灵 AI 的商业化单月流水超千万人民币，有信心在明年实现收入规模的快速增长。</p>
  <p>「高开低走」的即梦希望成为一种新的创造和体验方式，根据调研纪要，明年即梦没有明确的商业回报目标，但要有商业模式，「盈利要往后放」，明年即梦的重点将是与媒体、影视制作合作等产品形态的落地。</p>
  <h2><strong>快手又将陷入包围战</strong></h2>
  <p>抖音不是第一个做短视频的，却在 2018 年开始超越快手，后来居上成为短视频赛道的第一。张楠曾总结过崛起的四个关键因素：全屏高清、音乐、特效滤镜、个性化算法推荐技术。</p>
  <p>如今，即梦对阵可灵，能否重演抖音对阵快手的故事？</p>
  <p>即梦目前只是字节跳动在多模态大模型应用层的一款产品，隶属于抖音旗下的剪映团队，背后支撑服务的是字节跳动云服务的火山引擎。在火山引擎官网的模型广场上，字节跳动提供 20 个大模型产品，遍布文本、语音和视觉多个类型，此外火山方舟还提供月之暗面和智谱 AI 的产品。</p>
  <p>AI 可能成为字节跳动下一个核心业务支柱，相比之下，快手磁力引擎的官方网站上，很难找到相关大模型商业应用的展示位。</p>
  <p>字节跳动在大模型领域的激进姿态，今年早已经通过 C 端的产品豆包展现出来。今年 9 月，移动数据调研机构 Sensor Tower 曾发布全球 AI 应用报告，其显示了 ChatGPT 是 1-8 月全球下载量最多的 AI 应用，谷歌的 Gemini 排在第四，字节跳动的豆包排名第五，而且是榜单上唯一的中国产品。</p>
  <p>这离不开抖音丰沛的流量广告和投放支持，豆包和 Kimi 在今年的广告市场上投放竞争激烈，根据广告情报分析平台 AppGrowing 统计，豆包智能助手 4 月、5 月的投放金额接近 1800 万元，等到 6 月上旬，投放金额飙升至 1.24 亿元，而且在抖音站内限制了 Kimi 在内的大模型广告投放。</p>
  <p><strong>「营销预算方面，即梦在 12 月预算开始提升，明年第一季度，尤其是春节前后将提到亿级别的投入。」</strong>字节调研纪要提到。而除了营销，字节跳动在芯片底层储备上也不可小觑。英国《金融时报》报道称，字节跳动采购了约 23 万片英伟达芯片，已成为英伟达人工智能芯片的最大中国买家；The Information 也在 9 月份报道称，字节跳动今年订购了超过20 万台 Nvidia H20。</p>
  <p>所以基于豆包在国内大模型市场取得的领先地位，<strong>未来的看点是，抖音和豆包如何联动即梦，这也意味着，快手一枝独秀的可灵，将陷入字节跳动大模型的包围战。</strong></p>
  <p>今年 9 月，对标 OpenAI 的 Sora，字节跳动发布了两款对标文生视频工具 PixelDance 和 Seaweed，即梦 AI 已经接入了豆包，其中支撑即梦的就包括能力更优秀的 PixelDance，官方介绍称，能够生成高质量的长达两分钟的 1080p 分辨率视频，擅长描绘复杂的运动和物体之间的互动。</p>
  <p>目前看，不论是抖音还是快手，AI 生成视频最主要的落地场景都是趋同的，除了 C 端用户收费之外，B 端场景一个是服务于短剧等影视制作和后期市场，另外一个则是服务于广告和电商内容营销，比如商品素材展示上生成不同的图片。</p>
  <p>在火山引擎大会上，张楠曾展示过两个即梦创作者的 AI 短片，其中一个就是今年 7 月份上线的科幻短剧《觉醒》，当时在抖音单日点赞破 40 万；而同一时期，快手的可灵也打造过一部《山海奇镜之劈波斩浪》，同样都属于试水制作。</p>
  <p>但实际上，AI 生成视频对影视制作只是辅助性的，目前阶段仍然是小规模制作，要完成大批量的影视后期制作，即梦和可灵都是顺着 Sora 类似的 Dit 架构（一种结合了 Transformer 架构的扩散模型，用于图像和视频生成任务）在前进，都有很长一段路要走，商业化也尚为时过早。</p>
  <h2><strong>当谷歌击败Sora</strong></h2>
  <p>OpenAI 的 Sora 开放使用之后，一系列生成视频的表现并不符合外界的期待，而谷歌在近期发布的视频生成器 Veo2，通过一系列测试表明，其有超过 Sora 的更惊艳表现。</p>
  <p>尤其是一个最著名的切西红柿的镜头，谷歌的 Veo2 刀子干净利落地切开西红柿，避开了手指，而 Sora 视频中的刀子却切开了手，这让 Sora 再次成为群嘲的对象，也让行业人士认为，Sora 更偏向于运动，而 Veo2 则更注重物理的准确性。</p>
  <p>有 AI 行业人士认为，谷歌之所以能超越 Sora，不光是发现了 Sora 物理准确性的弱点，更在于其利用 YouTube 来训练其人工智能模型。</p>
  <p>字节跳动的技术团队并非没有发现 Sora 在物理准确性上的弱点。11 月，豆包大模型团队曾发布过一篇论文，名为《从世界模型的角度来看，视频生成与之相距多远：基于物理定律的视角》（《How Far is Video Generation from World Model: A Physical Law Perspective》），探究了视频生成模型能否观察事物间的相互关系，并从中提炼出一套稳定的物理规律。</p>
  <p>「视觉模糊性会导致在细粒度物理建模方面出现显著的误差，单纯依赖视频表示不足以进行精确的物理建模。」该论文认为，视频生成模型要成为准确的世界模型还面临挑战。</p>
  <p>而研究这个方向的两位作者都非常年轻，一位是 95 后，一位是 00 后。如同即梦和可灵，都需要年轻的艺术学院的学生参与打造想象力的世界一样，为这个&nbsp;AI&nbsp;想象力世界奠定技术基础的，同样来自年轻的头脑。两位作者耗费&nbsp;8&nbsp;个月时间，就是为了找到通往世界模型的一扇大门。</p>
  <p><strong>找到瓶颈需要 8 个月，而打破瓶颈可能需要耗费更长的时间。</strong></p>
  <p>抖音何时真正即梦？在那份调研纪要中，字节跳动明年的 AI 发展有三条主要路径，一是豆包大家族生态；二是抖音等产品的全面 AI 化；三是包括即梦在内的多模态模型和世界大模型，而且多模态这一条是重点，「无限支持和投入，因为这是转型的重要节点，可以接受较大亏损」。</p>
  <p>当谷歌击败 Sora，预示着 OpenAI 创造的模型神话将被打破；而快手可灵，只是抖音即梦的下一个目标罢了。</p>
  <p>本文来自微信公众号<a href="https://mp.weixin.qq.com/s?__biz=MzU2ODc5ODE0OQ==&amp;mid=2247493688&amp;idx=1&amp;sn=70880ef9c66b83024cbae1a70bc600eb&amp;chksm=fdd5abb90b355466c1a4b2fecc0d0f970a7ddad85d448017b5aefb4a55db18c052e7cda2c166&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“蓝洞商业”（ID：value_creation）</a>，作者：赵卫卫，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3095112547268736</id>
            <title>苹果为何一直拒用英伟达？</title>
            <link>https://www.36kr.com/p/3095112547268736</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3095112547268736</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Dec 2024 04:18:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 英伟达, 苹果, GPU  
<br><br>  
总结: 苹果与英伟达的关系经历了从合作到对立的转变，最初的“蜜月期”因技术争议和质量问题而破裂。苹果追求自主研发和完整生态系统，减少对外部供应商的依赖，尤其是在AI芯片领域。尽管英伟达的GPU性能强大，但其高功耗和发热量与苹果的设计理念相悖。苹果通过租赁GPU、与AMD合作以及开发自有AI服务器芯片等方式，努力摆脱对英伟达的依赖。尽管如此，短期内双方的竞合关系仍将持续。 </div>
                        <hr>
                    
                    <p>AI爆发的时代，英伟达凭借其强大的GPU芯片几乎垄断了AI芯片市场，成为众多科技巨头竞相追逐的合作伙伴。</p>
  <p>然而，苹果公司却始终与英伟达保持着微妙的距离，甚至可以说是在刻意回避。事实上，2000年代时，双方曾有过短暂的“蜜月期”，但随着时间推移，双方的矛盾不断加剧。</p>
  <p>这不禁引发人们的好奇：苹果为何一直拒用英伟达？这背后究竟隐藏着怎样的“恩怨情仇”和战略考量？</p>
  <p>苹果一直力求打造完整的生态系统，而大量采购英伟达的GPU，无疑会削弱苹果在AI领域的主导权。为了摆脱对英伟达的依赖，苹果采取了多种策略。</p>
  <p>但随着AI角力的深入，苹果面临着训练更大更好的模型的压力，这将需要更多的高端GPU。短期内，双方的竞合关系可能仍将存在。</p>
  <h2><strong>历史恩怨：从“蜜月期”到“冰河期”</strong></h2>
  <p>苹果与英伟达的合作并非一开始就充满敌意。早在2001年，苹果就曾在其Mac电脑中采用英伟达的芯片，以提升图形处理能力。当时，双方关系良好，甚至可以用“蜜月期”来形容。</p>
  <p>然而，这段蜜月期并没有持续太久。</p>
  <p>双方关系出现裂痕的第一个重要事件，发生在2000年代中期。当时，史蒂夫·乔布斯曾公开指责英伟达窃取皮克斯动画工作室（当时乔布斯是其主要股东）的技术，这无疑给双方关系蒙上了一层阴影。</p>
  <p>2008年，双方紧张关系进一步加剧。当时，英伟达生产的一批存在缺陷的GPU芯片被应用在包括苹果MacBook Pro在内的多款笔记本电脑中，引发了大规模的质量问题，被称为“bumpgate”事件。</p>
  <p>英伟达最初拒绝承担全部责任和赔偿，这激怒了苹果，也直接导致了双方合作关系的破裂。苹果不得不延长受影响MacBook的保修期，并承受了巨大的经济和声誉损失。</p>
  <p>据The Information援引苹果内部人士透露，英伟达高管长期以来将苹果视为一个“要求苛刻”且“利润微薄”的客户，不愿为其投入过多资源。而苹果在iPod成功后，也变得更加强势，认为英伟达难以合作。此外，英伟达试图对苹果移动设备使用的图形芯片收取许可费，也进一步激化了矛盾。</p>
  <h2><strong>商业与技术策略的博弈</strong></h2>
  <p>除了历史恩怨，苹果拒用英伟达也与其一贯的商业策略密切相关。</p>
  <p>苹果一直强调对产品软硬件的全面掌控，力求打造完整的生态系统。为了实现这一目标，苹果不断加强自主研发能力，减少对外部供应商的依赖。</p>
  <p>在芯片领域，苹果更是走在了行业前列。从iPhone的A系列芯片，到Mac的M系列芯片，苹果不断推出性能卓越的自研芯片，逐渐摆脱了对英特尔等传统芯片巨头的依赖。在这种背景下，苹果自然也不愿意在AI芯片领域受制于英伟达。</p>
  <p>苹果希望拥有对关键技术的完全控制权，以确保产品性能的优化和差异化竞争优势。而大量采购英伟达的GPU，无疑会削弱苹果在AI领域的主导权，使其在产品创新和技术路线上受到限制。</p>
  <p>此外，英伟达的GPU虽然性能强大，但也存在功耗高、发热量大的问题，这对于追求轻薄便携的苹果产品来说是一个不小的挑战。苹果一直致力于将产品做得更轻、更薄、更高效，而英伟达的GPU在一定程度上与其设计理念相悖。</p>
  <p>苹果曾多次要求英伟达为其MacBook定制低功耗、低发热的GPU芯片，但未能如愿。这促使苹果转向了AMD，并与其合作开发定制的图形芯片。虽然AMD的芯片在性能上略逊于英伟达，但其在功耗和散热方面的表现更符合苹果的需求。</p>
  <h2><strong>AI浪潮下的新挑战</strong></h2>
  <p>近年来，人工智能技术的爆发式发展给苹果带来了新的挑战。为了在AI领域保持竞争力，苹果需要训练更大规模、更复杂的AI模型，这无疑需要更强大的计算能力和更多的GPU资源。</p>
  <p>为了摆脱对英伟达的依赖，苹果采取了多管齐下的策略。</p>
  <p>首先，苹果主要通过亚马逊和微软等云服务提供商租赁英伟达的GPU，而不是大量购买。这种方式可以避免大量资金投入和长期依赖。</p>
  <p>其次，苹果曾使用AMD的图形芯片，并与谷歌合作使用其TPU（张量处理单元）进行AI模型训练。</p>
  <p>此外，苹果正在与博通合作开发自己的AI服务器芯片，代号为“Baltra”，预计到2026年可投入量产。这款芯片不仅用于推理，还有可能用于训练AI模型。</p>
  <p>尽管苹果一直努力摆脱对英伟达的依赖，但在短期内，双方的竞合关系可能仍将长期存在。掌握核心技术，才能在激烈的市场竞争中立于不败之地。</p>
  <p>本文不构成个人投资建议，不代表平台观点，市场有风险，投资需谨慎，请独立判断和决策。</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/BpDKEb4jmy_IcrSCe2hrUw" rel="noopener noreferrer nofollow" target="_blank">“华尔街见闻”</a>，作者：张雅琦，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3095158054336643</id>
            <title>微软开源视频Tokenizer新SOTA，显著优于Cosmos Tokenizer和Open-Sora</title>
            <link>https://www.36kr.com/p/3095158054336643</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3095158054336643</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Dec 2024 04:17:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <VidTok, Tokenizer, 视频生成, 开源>
<br>
<br>
总结: 微软开源了一款名为VidTok的视频Tokenizer，旨在将高维视频数据转换为紧凑的视觉Token，以提高视频生成模型的性能。VidTok在多种设定下的性能显著优于现有的SOTA模型，支持多样化的隐空间和灵活的压缩率，适用于不同的使用需求。该模型采用创新的混合架构设计、先进的量化技术和增强的训练策略，显著降低了计算成本，同时保持了高质量的重建效果。VidTok的开源为视频生成和世界模型领域提供了新的工具，支持后续微调以优化特定领域的性能。 </div>
                        <hr>
                    
                    <p>Sora、Genie等模型会都用到的Tokenizer，微软下手了——</p>
  <p>开源了一套全能的Video Tokenizer，名为<strong>VidTok</strong>。</p>
  <p>Sora等视频生成模型工作中，都会利用Tokenizer将原始的高维视频数据（如图像和视频帧）转换为更为紧凑的视觉Token，再以视觉Token为目标训练生成模型。</p>
  <p>而最新的VidTok，<strong>在连续和离散、不同压缩率等多种设定下</strong>，各项指标均显著优于SOTA模型。</p>
  <p>以下是涵盖PSNR、SSIM、FVD、LPIPS指标的性能比较雷达图，面积越大表示性能越好。</p>
  <p>从图中可以看出对于离散Tokenizer，VidTok显著优于英伟达Cosmos Tokenizer；对于连续Tokenizer，VidTok也比Open-Sora、CogVideoX有更高的性能。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_7482cfc7605947738300d5a9b12e9f00@5888275_oswg250852oswg1080oswg346_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>这项研究由来自微软亚研院、上海交通大学、北京大学的研究人员共同完成。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_f0a309fabda34fc59c9c0615f453534b@5888275_oswg41757oswg1080oswg402_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>目前，VidTok代码不仅开源了，还支持用户在自定义数据集上的微调，为研究者和开发者提供了一个高性能、易用的工具平台。</p>
  <h2><strong>性能全面领先，适用各种场景</strong></h2>
  <p>近年来，视频生成以及基于此的世界模型已经成为人工智能领域的热门研究方向，这两者的核心在于对视频内容的高效建模。</p>
  <p>视频中蕴含了丰富的视觉信息，不仅能够提供真实的视觉体验，更能作为具身场景中模型理解世界的中间媒介。</p>
  <p>然而，由于视频像素级表示信息高度冗余，如何通过Tokenizer对视频数据进行高效压缩和表示成为关键课题。</p>
  <p>当下很多工作如Sora，Genie等都会通过Tokenizer将原始的高维视频数据（如图像和视频帧）转换为更为紧凑的视觉Token，再以视觉Token为目标训练生成模型。</p>
  <p>可以说，视觉Token的表示能力对于最终的效果至关重要，甚至决定了模型能力的上限。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_a102ad6eb27349b0880ab4668d500794@5888275_oswg189342oswg1080oswg334_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>Tokenizer的主要作用是将高维的原始数据转换为隐空间中高效的压缩表示，使得信息的生成和处理可以在该隐空间中进行。上图展示了一个视频的Token化过程，通过转换为Token建模，能够有效降低模型训练和推理时的计算需求。</p>
  <p>根据不同的使用需求，视频Tokenizer通常有如下分类：</p>
  <ul>
   <li><strong>连续型和离散型。</strong>根据隐空间的数值分布，Tokenizer可以分为连续型和离散型，分别适用于从连续分布中采样的模型（如扩散模型等）和从离散分布中采样的模型（如语言模型等）。</li>
   <li><strong>因果型和非因果型。</strong>因果结构使得模型只依赖历史帧来对当前帧进行Tokenization，这与真实世界系统的因果性质保持一致。非因果模型则可以同时根据历史帧和未来帧对当前帧进行Tokenization，通常具有更优的重建质量。</li>
   <li><strong>不同的压缩率模型。</strong>Sora等众多工作采用了如4x8x8的视频压缩率（时间压缩4倍、空间压缩8倍），实现更高的视频压缩率而保持高质量的视频重建是目前的研究趋势。</li>
  </ul>
  <p>目前业界领先的视频模型多为闭源状态，而<strong>开源的视频Tokenizer大多受限于单一的模型设定或欠佳的重建质量</strong>，导致可用性较差。</p>
  <p>由此，来自微软亚研院、上海交通大学和北京大学的研究人员最近正式发布了开源视频Tokenizer——VidTok。</p>
  <p>在测试中，VidTok性能全面领先，适用各种场景。</p>
  <p>如下表所示，VidTok<strong>支持多样化的隐空间且具有灵活的压缩率</strong>，同时<strong>支持因果和非因果模型</strong>，以适应不同的使用需求。</p>
  <ul>
   <li>对于连续型Tokenizer，支持不同的视频压缩率、不同的隐空间通道数，同时支持因果和非因果模型。</li>
   <li>对于离散型Tokenizer，支持不同的视频压缩率、不同的码本大小，同时支持因果和非因果模型。</li>
  </ul>
  <p>更多模型在持续更新中。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_01c2177417024f79b8a785ec76814d68@5888275_oswg114486oswg985oswg361_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>为了全面评估VidTok在各个设定下的重建性能，作者将VidTok与最先进的连续和离散视频Tokenizer分别进行了对齐设定下的比较。所有模型均为4x8x8倍视频压缩率的因果模型，主要包含以下三种设定：</p>
  <ul>
   <li><strong>VidTok-FSQ：</strong>离散型，码本大小各异。基线方法包括MAGVIT-v2，OmniTokenizer，Cosmos-DV等。</li>
   <li><strong>VidTok-KL-4chn：</strong>连续型，隐空间通道数为4。基线方法包括CV-VAE，Open-Sora-v1.2，Open-Sora-Plan-v1.2等。</li>
   <li><strong>VidTok-KL-16chn：</strong>连续型，隐空间通道数为16。基线方法包括CogVideoX，Cosmos-CV等。</li>
  </ul>
  <p>定量实验结果表明，VidTok在上述三种设定下均达到了SOTA性能，在常见的视频质量评估指标PSNR、SSIM、FVD、LPIPS上具有全面的优势。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_5bb1cc21f4554d97b344d22e3fcf8718@5888275_oswg119220oswg960oswg394_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>与现有的离散Tokenizer相比，VidTok即使在使用更小的码本大小时（例如32,768），也展现出了更优的重建性能。</p>
  <p>在连续Tokenizer的设定下，无论隐空间通道数是4还是16，VidTok在所有评估指标上相比基线方法均取得了全面的提升。值得注意的是，这些提升是在没有模型大小优势的情况下达成的。</p>
  <p>除此之外，团队还进行了定性分析。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_f299cb6573e3408897a781f4d3458061@5888275_oswg864878oswg1080oswg531_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>上图中展示了以上三种设定下的与基线方法的对比结果。</p>
  <p>从视频帧的重建质量可以看出，与现有的方法相比，VidTok在各种模型设定下，均展现出了最优的重建细节保真度和主观视觉质量。证明了VidTok作为多功能视频Tokenizer的有效性。</p>
  <p>所以VidTok是如何做到的？</p>
  <h2><strong>VidTok的技术亮点解析</strong></h2>
  <p>相对于现有的视频Tokenizer，VidTok在模型架构、量化技术、训练策略上分别做了创新。</p>
  <p><strong>高效的混合模型架构设计</strong></p>
  <p>VidTok采用经典的3D编码器-解码器结构，同时创新性地结合了3D、2D和1D卷积，有效地解耦空间和时间采样。</p>
  <p>在现有研究中普遍认为，尽管计算成本较高，完全的3D架构提供了更优的重建质量。然而，VidTok发现将部分3D卷积替换为2D和1D卷积的组合，可以有效地解耦空间和时间采样，在降低计算需求的同时，保持了高水平的重建质量。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_1d01fb3ddb5943b0a70f2bc65580d4ac@5888275_oswg165962oswg1080oswg447_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>详细的网络架构如上图所示。VidTok分别处理空间采样和时间采样模块，并在时间采样模块中引入了AlphaBlender操作符。其余组件，包括输入/输出层和瓶颈层，则利用3D卷积来促进信息融合。此外，整个架构中引入了层归一化以增强稳定性和性能。实验证明该架构在重建质量和计算量之间取得了平衡。</p>
  <h3><strong>先进的量化技术</strong></h3>
  <p>VidTok引入了有限标量量化（FSQ）技术，无需显式学习码本，显著提高了模型的训练稳定性和重建性能。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_cd622b7954f44b99932d573204192573@5888275_oswg100006oswg1080oswg229_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>有限标量量化（FSQ）由「Finite scalar quantization: Vq-vae made simple」提出，其核心原理是，在隐空间表征中，每个标量条目通过四舍五入独立量化到最近的预定义标量值。</p>
  <p>与传统VQ相比，FSQ无需学习显式的码本，从而提高了训练的稳定性。实验表明，FSQ在码本利用率、重建质量和训练稳定性方面具有显著优势，作为一种先进的量化技术，有效提升了离散Tokenizer的性能。</p>
  <h3><strong>增强的训练策略</strong></h3>
  <p>VidTok采用分阶段训练策略，训练时间减少了50%，而重建质量不受影响。</p>
  <p>视频Tokenizer的训练通常是计算密集的，要求大量计算资源（例如对于256x256分辨率的视频需要3,072GPU小时的训练时长）。这就需要开发有效的策略来降低计算成本，同时保持模型性能。</p>
  <p>VidTok采用一种两阶段训练方法来应对这一挑战：首先在低分辨率视频上对完整模型进行预训练，然后仅在高分辨率视频上微调解码器。这种训练策略显著降低了计算成本——训练时间减少了一半（从3,072GPU小时降至1,536GPU 小时），而保持重建视频质量不变。</p>
  <p>该两阶段训练的另一优势是，由于第二阶段只会微调解码器，因此模型可以快速适应到新的领域数据中，而不会影响隐空间数据分布。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241226/v2_204f73472cfb417a976ca62a39caffd9@5888275_oswg37285oswg859oswg127_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>此外，由于视频Tokenizer旨在建模输入视频的运动动态，因此在模型中有效表示这些动态至关重要。VidTok使用较低帧率的数据进行训练，显著增强了模型捕捉和表示运动动态的能力，获得了更好的重建质量。</p>
  <p>VidTok的开源为视频生成、世界模型领域提供了新的工具，特别是在当前业内许多领先模型仍未开源的背景下。</p>
  <p>团队表示，VidTok支持后续微调也为其他应用提供了更广阔的使用空间，研究者可轻松将VidTok应用于特定领域数据集，为目标场景优化性能。</p>
  <p>更多细节内容感兴趣的童鞋可参阅原论文。</p>
  <p>论文地址：https://arxiv.org/abs/2412.13061</p>
  <p>项目地址：https://github.com/microsoft/vidtok</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/-b5yhTWX9KUBlOpFJT39ZA" rel="noopener noreferrer nofollow" target="_blank">“量子位”</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>