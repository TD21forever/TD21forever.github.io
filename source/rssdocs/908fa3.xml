<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/2873295747603334</id>
            <title>马斯克更换头像庆祝拜登退选，特朗普能给特斯拉带来什么？</title>
            <link>https://www.36kr.com/p/2873295747603334</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2873295747603334</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jul 2024 13:12:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 特斯拉, 马斯克, 特朗普, 拜登
<br>
<br>
总结: 美国总统拜登宣布放弃2024年总统竞选后，特斯拉CEO马斯克庆祝。马斯克支持特朗普，因特朗普有利于特斯拉发展。拜登支持电动汽车，但马斯克并非其粉丝。特朗普反对电动车政策，保护传统汽车工人。特斯拉在市场失速，特朗普政策有利于其竞争力。马斯克与特朗普关系紧密，可能成为特朗普政府顾问。 </div>
                        <hr>
                    
                    <p>美国现任总统拜登宣布放弃2024年总统竞选后，电动车巨头特斯拉（NASDAQ：TSLA）CEO埃隆·马斯克开心到更换头像庆祝。</p><p>在特朗普遭遇枪击后，马斯克立即表态支持特朗普。本周日，他在社交媒体平台X上解释了自己为何转而支持共和党候选人。“我信奉一个能够最大限度提高个人自由和价值的美国。这曾经是民主党的使命，但现在钟摆已经摆向了共和党。”马斯克在X上写道。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_1d91598d8e7e4c32b715d89d4d27c460@5888275_oswg295466oswg1080oswg526_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>如果将时间拉回到2018年，马斯克可能是“最恨”特朗普的车企CEO。彼时特朗普掀起贸易战，让马斯克在推特上痛斥：“我反对进口税，这就像穿着铅鞋参加奥运会比赛一样。”</p><p>与特朗普不同的是，拜登是电动汽车的支持者。只不过拜登“站”的也不是特斯拉，而是福特、通用、Stellantis（由菲亚特克莱斯勒集团与PSA集团合并而成）三大老牌美国车企。2021年白宫举办“清洁能源汽车活动”时，拜登邀请了通用和福特汽车，独缺当时在电动车行业制霸全球的特斯拉。</p><p>因此，虽然拜登支持电动汽车，但马斯克并不是他的“粉丝”，在这个角度上，他和特朗普是一路的。特别是2023年，特朗普强调如果他再度入主白宫，一定会废除拜登“灾难性”的电动汽车补贴政策。完全市场竞争之下，规模大、技术领先的特斯拉，对于福特、通用等老牌车企的优势将更加明显。</p><h2><strong>分野不止电动车，还有汽车工会</strong></h2><p>如果用一句话总结特朗普和拜登对美国电动汽车的态度，那就是：拜登“亲”电动汽车，特朗普“疏”电动汽车。</p><p>在大选期间，特朗普就在提名演讲中对拜登总统的电动车政策猛烈抨击，并宣誓若重新上任将立即采取行动反对。“我上任第一天就终结电动汽车指令，这样可以挽救正在走向彻底毁灭的美国汽车产业，并给美国消费者的每辆汽车节省成千上万美元。”</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_5a6c2293f2614e98b44190a97250efb5@5888275_oswg744538oswg1080oswg1080_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>特朗普的终结电动汽车政策，显然是为了打消传统燃油汽车工人们的失业担心。特朗普一直以来都是反对新能源的，主张利好美国的石油资源，做好传统汽车制造业。至于环保，则被其抛到了次要位置。</p><p>站在特朗普对立面的拜登政府，一直在推进新能源前进，推进清洁能源的开发与利用，主张环保与生态。在竞选期间，拜登政府宣布将拨款17亿美元，以帮助汽车制造商完成工厂改造和扩建，从而促进美国电动汽车行业的发展。</p><p>这笔资金将覆盖多个关键的选举州，包括密歇根州、宾夕法尼亚州和佐治亚州，以及俄亥俄州、伊利诺伊州、印第安纳州、马里兰州和弗吉尼亚州。这反映了拜登的竞选策略，旨在争取这些州的工人支持，并安抚汽车工人，让他们相信电动汽车的发展不会减少就业机会。</p><p>一直以来，美国汽车工人联合会是美国总统竞选中不容忽视的一环。</p><p>“工会下面有很多工人都是选民，可以参与投票，拜登和特朗普都想去争取得到他们的支持。现在美国汽车工人就担心电动车来了以后，做传统燃油车的工人就会失业。特朗普反对电动车实际上保护了这些美国工人的饭碗，就会赢得工会的支持。”国际智能运载科技协会秘书长张翔对源媒汇分析。</p><p>在2021年拜登竞选时，其为争取美国汽车工人联合会的支持，拿出了具体的电动汽车税收抵免政策。新政提议，由工会制造的零排放汽车，每辆最多能获得12500美元的税收抵免，而其他电动汽车的优惠额度保持不变，还是7500美元。</p><p>这份政策引发了诸多不满，因为它划出了一条界限，把电动汽车分成了工会工人制造的和非工会工人制造的，而两者的补贴差了4500美元。</p><p>反对的车企认为，这一政策是为底特律的三大汽车制造商——通用、福特和Stellantis量身定制的，因为它们的工人是工会成员。而像特斯拉这类工会坚决抵制分子，则无法享受到额外的补贴。</p><p>而特朗普与汽车工会，至少在今年2月势成水火的状态。据外媒报道，今年2月美国汽车工会主席肖恩·费恩，曾称特朗普代表的是亿万富翁阶级，他只会试图压榨美国工人，“特朗普代表的一切都与我们作为工会所坚持的价值背道而驰”。</p><h2><strong>特斯拉失速，马斯克押注特朗普？</strong></h2><p>大家都知道，现在的特斯拉日子不太好过。</p><p>今年1-6月，特斯拉全球交付 830766 台汽车，并未延续多年来的凶猛涨幅，同比下降了6.55%。特斯拉在美国市场的销量也在下滑，2023年上半年特斯拉在美国销量总数为32.49万辆，而今年上半年销量总数却只有29.92万辆。因此，特斯拉在美国市场占有率从2023年的59.8%下降到今年的51.2%。</p><p>今年6月，特斯拉被指库存积压严重。而为了存放这批滞销的汽车，特斯拉选择全部堆放在停车场，数量之大甚至从太空都能看到。</p><p>特朗普如今在美国选举中形势太好，对马斯克、特斯拉来说是不可多得的利好。</p><p>公司层面，特朗普的反电动车补贴政策有利于特斯拉凸显规模及技术优势。张翔告诉源媒汇，特斯拉现在是美国最大的电动汽车企业，现在已经达到规模经济效益了，能够做到不要补贴也能盈利。作为对手的其他企业，如福特和通用的电动汽车业务就比较难发展，这样就可以保持特斯拉的竞争力。</p><p>同时，特朗普的政策也会帮助特斯拉面对的行业竞争下降。据悉，停止电动车补贴只是特朗普振兴美国汽车工业计划的三分之一。其他两部分还有大幅增加进口汽车（含配件）关税，日本韩国一样要征收高额关税。提高关税后，特斯拉将免受美国以外电动汽车的冲击，特别是以性价比著称的中国电动汽车。</p><p>个人层面，马斯克与特朗普关系愈发紧密。有报道称，马斯克不仅在精神上坚定支持特朗普，以后每个月还可能向特朗普的竞选团队捐赠大约4500万美元竞选资金。而特朗普也表示，如果成功入主白宫，将考虑邀请马斯克出任白宫政治顾问，在移民、技术、企业监管等问题上提供意见。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_7d4c51f22ffc475ca440773e40827347@5888275_oswg31281oswg640oswg399_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>值得一提的是，今年7月初，特斯拉宣布在墨西哥建立其又一超级工厂，这座超级工厂将开启汽车生产线自动化与智能化升级的新篇章。若特朗普当选上美国总统，该超级工厂如果能获得特朗普的“助攻”，例如免税进入美国市场，特斯拉的本土竞争力将进一步提升。</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/8LbaF1gI_OsjoiNo2VCrRQ" rel="noopener noreferrer nofollow" target="_blank">“源媒汇”</a>，作者：源媒汇，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2873334586741124</id>
            <title>苹果开源7B大模型，训练过程数据集一口气全给了，网友：开放得不像苹果</title>
            <link>https://www.36kr.com/p/2873334586741124</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2873334586741124</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jul 2024 12:30:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, 开源, 7B模型, 数据管理过程
<br>
<br>
总结: 苹果发布了一个开源的7B模型，不仅效果与其他公司的模型相当，而且开放了全部训练过程和资源，引起了业内的关注。对于想要从头开始训练模型或微调现有模型的人来说，数据管理过程是必须研究的重要环节。 </div>
                        <hr>
                    
                    <p>苹果最新杀入开源大模型战场，而且比其他公司更开放。</p><p>推出<strong>7B模型</strong>，不仅效果与<strong>Llama 3 8B</strong>相当，而且一次性开源了<strong>全部训练过程和资源</strong>。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_2c6b0247e70d407bb75abc8728faf310@5888275_oswg126234oswg1080oswg574_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>要知道，不久前Nature杂志编辑Elizabeth Gibney还<strong>撰文批评</strong>：</p><blockquote><p>许多声称开源的AI模型，实际上在数据和训练方法上并不透明，无法满足真正的科学研究需求。</p></blockquote><p>而苹果这次竟然来真的！！</p><p>就连NLP科学家、AutoAWQ创建者也发出惊叹：</p><blockquote><p>Apple发布了一个击败Mistral 7B的模型，但更棒的是他们完全开源了所有内容，<strong>包括预训练数据集</strong>！</p></blockquote><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_a75f9ff8f25c498c91d73ace743957e8@5888275_oswg463524oswg1080oswg1095_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>也引来网友在线调侃：</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_ae06e67735b044d0af451eed592d1418@5888275_oswg51431oswg1080oswg233_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>至于这次开源的意义，有热心网友也帮忙总结了：</p><blockquote><p>对于任何想要从头开始训练模型或微调现有模型的人来说，<strong>数据管理过程</strong>是必须研究的。</p></blockquote><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_b62934ad5bda44e1b23643570e3feeda@5888275_oswg246312oswg1080oswg652_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>当然，除了OpenAI和苹果，上周Mistral AI联合英伟达也发布了一个12B参数小模型。</p><p>HuggingFace创始人表示，<strong>「小模型周」</strong>来了！</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_49f0436282074667afbe7799701c9c0b@5888275_oswg154033oswg1080oswg688_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>卷！继续卷！所以苹果这次发布的小模型究竟有多能打？</p><h2><strong>效果直逼Llama 3 8B</strong></h2><p>有多能打先不说，先来看Hugging Face技术主管刚“拆箱”的<strong>模型基础配置</strong>。</p><p>总结下来就是：</p><ul><li>7B基础模型，在开放数据集上使用<strong>2.5T tokens</strong>进行训练</li><li>主要是英文数据，拥有<strong>2048</strong>tokens上下文窗口</li><li>数据集包括DCLM-BASELINE、StarCoder和ProofPile2</li><li>MMLU得分接近Llama 3 8B</li><li>使用PyTorch和OpenLM框架进行训练</li></ul><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_18078d59c5b74f62bbc05972c75a62ef@5888275_oswg296574oswg1080oswg705_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>具体而言，研究团队先是提出了一个语言模型<strong>数据比较新基准</strong>——DCLM。</p><p>之所以提出这一基准，是因为团队发现：</p><blockquote><p>由机器学习 (ML) 模型从较大的数据集中<strong>自动过滤和选择高质量数据</strong>，可能是构建高质量训练集的关键。</p></blockquote><p>因此，团队使用DCLM来设计高质量数据集从而提高模型性能，尤其是在多模态领域。</p><p>其<strong>思路</strong>很简单：使用一个标准化的框架来进行实验，包括固定的模型架构、训练代码、超参数和评估，最终找出哪种数据整理策略最适合训练出高性能的模型。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_ae917909a5774b00bf6453486f29b160@5888275_oswg293235oswg1080oswg528_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>基于上述思路，团队构建了一个<strong>高质量数据集DCLM-BASELINE</strong>，并用它从头训练了一个7B参数模型——DCLM-7B。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_094aaf13ec09474f9c0b81ac47d1cab1@5888275_oswg339512oswg1080oswg786_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>DCLM-7B具体表现如何呢？</p><p>结果显示，它在MMLU基准上5-shot<strong>准确率达64%</strong>，可与Mistral-7B-v0.3（63%）和Llama 3 8B（66%）相媲美；并且在53个自然语言理解任务上的平均表现也可与Llama 3 8B相媲美，而所需计算量仅为后者的1/6。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_9b38941624cd44818fae31f99e5445af@5888275_oswg261998oswg1080oswg559_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>与其他同等大小模型相比，DCLM-7B的MMLU得分超越Mistral-7B，接近Llama 3 8B。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_21cf6869ec064aa1b3cc65e7e344e9f5@5888275_oswg221835oswg1080oswg921_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>最后，为了<strong>测试新数据集效果</strong>，有业内人士用卡帕西的llm.c训练了GPT-2 1.5B，来比较DCLM-Baseline与FineWeb-Edu这两个数据集。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_fdb646ab114f42d2b45faf7021959e3f@5888275_oswg584905oswg1080oswg1868_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>结果显示DCLM-Baseline取得了<strong>更高的平均分</strong>，且在ARC（小学生科学问题推理）、HellaSwag（常识推理）、MMLU等任务上表现更好。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_f97bfb8035254af7959e116c2009d8f0@5888275_oswg202750oswg1080oswg295_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2><strong>“小”模型成新趋势</strong></h2><p>回到开头，“小”模型最近已成新趋势。</p><p>先是HuggingFace推出了小模型家族<strong>“SmolLM”</strong>，其中包含135M、360M和1.7B型号模型。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_5c5ff4c653f646be9b150b1228f1b3b0@5888275_oswg333955oswg1080oswg608_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>它们在广泛的推理和常识基准上优于类似大小的模型。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_7768bc4dee4d4acba6905f8276f85453@5888275_oswg147920oswg1080oswg686_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>然后OpenAI突然发布了<strong>GPT-4o mini</strong>，不仅能力接近GPT-4，而且价格大幅下降。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_d540c02a057c473a899b758cd43a7f83@5888275_oswg105862oswg1080oswg567_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>就在GPT-4o mini<strong>发布同日</strong>，Mistral AI联合英伟达发布了12B参数小模型——<strong>Mistral NeMo</strong>。</p><p>从整体性能上看，Mistral NeMo在多项基准测试中，击败了Gemma 2 9B和Llama 3 8B。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_351d184d7ab84d2495269d567fa2590a@5888275_oswg358717oswg1080oswg558_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>所以，为啥大家都开始卷小模型了？</p><p>原因嘛可能正如smol AI创始人提醒的，虽然模型变小了，但在能力相近的情况下，小模型<strong>大大降低了成本</strong>。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_9971b2f8586c410f908788726e5bade1@5888275_oswg197096oswg1080oswg646_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>就像他提供的这张图，以GPT-4o mini为代表的小模型整体比右侧价格更低。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_286523cb615b41599b8d135c6411ed55@5888275_oswg428812oswg1080oswg1261_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>对此，我等吃瓜群众be like:</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240722/v2_c301d4f3b33b42588f171bea492c1319@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>所以，你更看好哪家呢？（欢迎评论区讨论留言）</p><h3>模型地址：</h3><p>https://huggingface.co/apple/DCLM-7BGitHub:https://github.com/mlfoundations/dclm</p><h3>数据集地址：</h3><p>https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0</p><h3>参考链接：</h3><p>[1]https://x.com/Yuchenj_UW/status/1813260100192334108</p><p>[2]https://x.com/casper_hansen_/status/1814269340100751382</p><p>[3]https://x.com/_philschmid/status/1814274909775995087</p><p>[4]https://x.com/LoubnaBenAllal1/status/1813252390692303069</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/JTCnjvkSqwAC2uRWEEK08g" rel="noopener noreferrer nofollow" target="_blank">“量子位”</a>，作者：一水，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>