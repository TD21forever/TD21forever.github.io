<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/2575585210230408</id>
            <title>腾讯这篇论文，暴露了它想操控所有APP的野心</title>
            <link>https://www.36kr.com/p/2575585210230408</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575585210230408</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:36:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI加持, Chatbot, 生图, 扩图应用
<br>
<br>
总结: 腾讯研究团队最新论文提出了一种可能性，即一个能操作所有APP的应用。这个应用可以根据用户的需求自动打开相关的手机App，并完成相应的任务。这个应用可以帮助用户完成各种任务，比如订闹钟、发微信消息、点外卖、购物、叫车等。这个应用可以被视为比微信更强大的超级流量入口。实现这一操作的关键在于AppAgent可以模仿人类玩手机的方式，通过图形界面来操作App。此外，AppAgent还可以通过自学习来适应市面上出现的新App。为了识别不同的App和页面按钮，AppAgent使用了GPT-4 Vision技术，该技术可以理解用户发送的图片。AppAgent会根据用户设定的目标，观察环境、思考实现目标的方式，并采取相应的行动。最后，AppAgent会对行动的结果进行复盘，并传递给下一个步骤。 </div>
                        <hr>
                    
                    <p>虽然现在各种AI加持下的Chatbot、生图、扩图应用层出不穷，但总感觉好像并没有给日常生活带来什么变化。</p><p>回想当年互联网技术从网页到移动端的进化，支付宝取代了现金、滴滴重塑了出行、美团改写了用餐方式...那么AI技术会带来什么样的变革？</p><p>腾讯研究团队的一篇最新论文带来了一种可能：一个帮你操作所有APP的应用。</p><p>只要告诉这个应用你想做什么，它就会自动打开相关的手机App，直接把事儿给你办了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_e406aa43743543fcb45f5d932d099823@5888275_oswg31624oswg1080oswg273_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>比如帮你订个工作日起床闹钟、给领导发个微信消息、上美团点杯奶茶、去拼多多买一箱最便宜的纸巾、让滴滴叫一辆去机场的车、或者把刚拍的照片P好看一下然后直接发到朋友圈，顺手配一段伤感的文字。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7c40553015844d6598f0df9df0834cbe@5888275_oswg471929oswg744oswg907_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>有了这个APPAgent就再也不用去一页页找各种APP了，未来需要咱们手动点开的可能就这一个APP。</p><p>这是什么？<strong>这不就是个比微信更强大的超级流量入口嘛！</strong></p><h2><strong>01</strong></h2><p>具体如何实现这一操作的，APPAgent分为几个步骤。</p><p>首先是如何保证能够“调用”所有应用，比如说Siri能够访问“闹钟”应用，就是从系统上植入了访问接口，而市面上的APP几乎无穷无尽，不可能每一个开发者都给Siri搞一个接头暗号。</p><p>AppAgent第一个厉害之处就来了，它不需要开发者做任何改动，可以直接模仿人类玩手机的方式，通过图形界面来<strong>操作</strong>App。</p><p>也就是说，它把人类使用APP的过程拆解成了几个具体动作，比如滑动、点击、长按、输入等。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_3c6ede57c63f40199470ca5a1ab4bb21@5888275_oswg108703oswg510oswg789_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>也就是说它在AI的加持下，自行学习人类使用APP的过程，从而达到模拟的效果。</p><p>这意味着无论市面上出现了什么新的App，只要给AppAgent一点时间自己摸索一下，它就能自己熟练使用了。</p><p>在这个过程中，最大的问题是AI如何识别这千千万万种不同的APP、不同的页面按钮分别是做什么的。</p><p>AppAgent要玩手机，首先要看得见手机屏幕。传统的方式是把常见的UI界面截图下来，人工打上标记，比如这个是输入框，那个是返回按钮，黄色袋鼠是美团，企鹅是QQ之类的。</p><p>然后通过这些数据训练出来一个能识别常见UI元素的视觉模型。</p><p>这个问题GPTV其实已经有解决方案了，GPTV其实是GPT-4 Vision的一个缩写，今年11月跟着GPT-4的上线一起来到了大众的视野。</p><p>GPT-4 Vision能做到的事简单来说，就是可以看懂用户发给它的图片。</p><p>比如像下面这样上传一张过马路的照片，然后问GPT-4照片里有多少人。它的回答是有137人，还很谦虚的说我是大概估算了一下，有些地方太暗还有些人被挡住了，所以我可能数得不准。[2]</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_1cdad5e99dd04e2580a786b34ea3c051@5888275_oswg331396oswg762oswg531_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>大家一拥而上搞出了许多花样，包括但不限于：足球比赛解说、实时解读摄像头里出现的物品、根据手绘草图在电脑上帮你画图等等。[3]</p><p>大家一致评价：识别速度快、准确性也高，不过缺点也不是没有，太贵。比如做一个13秒的足球视频解说，就要花费约30美元。</p><p>但是能用钱解决的问题，都不是问题。</p><h2><strong>02</strong></h2><p>第二步，<strong>APPAgent会根据人类设定的目标，自我思考并拆解这个目标需要哪些过程。</strong></p><p>收到一个请求后，AppAgent首先做的事情是<strong>观察</strong>环境，也就是上面提到的“看图说话”。</p><p>然后再<strong>思考</strong>怎么根据现状去实现最终目标，得出结论之后就<strong>行动</strong>。</p><p>最后根据行动之后，带来了哪些改变，做一个<strong>复盘</strong>，然后把这个复盘传递给下一个步骤。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7e9deee402534168bce5f03747ec590d@5888275_oswg310213oswg1080oswg579_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>比如说它点到了一个广告，就会发现和主要任务无关，并将这一结果的复盘向下传递，并得出结论不要点这个页面。</p><p>AppAgent在训练的时候强调了以目标为导向的逻辑，如果进入了与目标不相关的页面就返回上一页。并且还加入了现有大模型中有关用户界面的知识和人类操作的演示。</p><blockquote><p>效果好得惊人，团队在9个APP上总共测试了45个任务，AppAgent在10个步骤内就能成功的概率高达84.4%，而且平均下来只需要5.1个步骤就可以完成。</p></blockquote><p>这样每一步下去都会越来越接近目标。</p><p>这是执行任务的过程拆解，而在具体的操作上，如何让AI识别各个功能按键又是个大问题。</p><p>为了提升AppAgent在操作上的准确度和效率，团队在两件事儿上下了功夫。</p><p>首先，他们先简化了手机界面的坐标，根据前端的XML文件给每个UI元素指定了一个唯一编号。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_c943ef14e5f548f09db8c1e1bdada36b@5888275_oswg218525oswg1080oswg818_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>然后，简化了手机的交互操作，规定了以下6种操作：轻点、长按、滑动、输入文字、返回和退出。</p><p>当AppAgent思考好了行动计划以后，只要根据<strong>操作+位置编号</strong>来执行就可以了。</p><p>像上图里的“点击（3）”，意思就是执行“点击”这个操作，点的位置是编号为3的区域，也就是邮件发送按钮。</p><p>这种操作方式极大地提高了AppAgent操作的准确率。</p><blockquote><p>在过去，直接用GPT-4来操作手机完成任务，成功率仅有2.2%，而还让GPT-4来指挥，但是执行端换成上面这种操作方式之后，成功率就提升到了48.9%。</p></blockquote><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_dc5ae61bcc534da1a08eba8be8f49b59@5888275_oswg51911oswg1080oswg160_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>实际上，这篇论文提供的是一种训练AI操作APP的解决方案，其突破性的创意在于改变了智能体实施操作的学习过程。</p><p><strong>既互动方式模拟人类、识别UI靠数据标记、执行操作靠位置编号。</strong></p><p>就当下来看，这是智能代理技术最为先进的解决方案，让明年AI Agents的普及成为了一种可能。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_56f63d8b44ef4298854b36e3307f372e@5888275_oswg78689oswg1080oswg158_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这技术普及还有另一种问题：以后水军刷评论更方便了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_719f108362de402ca338cc00cb3d7279@5888275_oswg78727oswg1080oswg208_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p><strong>参考文章：</strong></p><p>[1] AppAgent: Multimodal Agents as Smartphone Users｜Tencent&nbsp;</p><p>[2] OpenAI's GPT-4 Vision explained: Transforming AI with Visual Capabilities | Encord&nbsp;</p><p>[3] 解说梅西球赛、英雄联盟，OpenAI GPT-4视觉API被开发者玩出新花样 | 机器之心&nbsp;</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/afzIIUXuM5-Xv6qvUIPmCQ" rel="noopener noreferrer nofollow" target="_blank">“新硅NewGeek”（ID:gh_b2beba60958f）</a>，作者：刘白，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2575583737030276</id>
            <title>史上最快3D数字人生成器：半小时完成训练，渲染仅需16毫秒，苹果出品</title>
            <link>https://www.36kr.com/p/2575583737030276</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575583737030276</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:35:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字人, HUGS, 视频合成, 帧率
<br>
<br>
总结: 苹果联合德国马普推出的HUGS是一种基于高斯函数的3D数字人合成工具，可以从简单的视频中提取人物骨骼并合成数字分身，实现流畅的动作和高帧率的视频合成。HUGS在细节刻画上比其他前沿技术更清晰细腻，同时在测试数据上也表现出了优异的性能，速度方面也有显著提升。 </div>
                        <hr>
                    
                    <p>之前要两天才能训练好的数字人，现在<strong>只用半小时</strong>就能完成了！</p><p>到了<strong>推理阶段，更是只要16毫秒</strong>，就能得到动作流畅、细节到位的场景视频。</p><p>而且无需复杂的采样和建模，只要随便拍一段50-100帧的视频就足够了，换算成时间不过几秒钟。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_6e952ee74cfa4bb29ce9225821f6f912@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这正是由苹果联合德国马普所推出的，基于高斯函数的3D数字人合成工具<strong>HUGS</strong>。</p><p>它可以从一段简单的视频当中提取出人物骨骼，从而合成数字分身并驱动它做出任意动作。</p><p>这个数字人可以丝滑地融合到其他场景，甚至<strong>帧率还能超越原始素材</strong>，达到60FPS。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7bedc35346bc4976abb98d1d3cfe0988@5888275_oswg538466oswg1080oswg545_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>Hugging Face的“首席羊驼官”Omar Sanseviero看到后，也给HUGS送上了hug。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_4e9f574e4dd04f92a838fbcfa030603e@5888275_oswg192159oswg1080oswg503_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>那么，HUGS可以实现怎样的效果呢？</p><h2><strong>01 100倍速生成60FPS视频</strong></h2><p>从下面这张动图可以看出，新生成的数字人可以在不同于训练素材的场景中做出不同的动作。</p><p>而新合成的画面也比原始素材更加流畅——尽管原素材只有24FPS，但HUGS合成的视频帧率达到了60FPS。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_328f10be8c98415ebba4eb496efdcab3@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>同时，HUGS也支持把多个人物融合进同一个场景。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_8f9447f544a146c2af8178438a9a4b76@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>细节刻画上，HUGS也比Neuman和Vid2Avatar这两个前SOTA更清晰细腻，也更加真实。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_44cd4c14065f48b684618a7ddc9bffa5@5888275_oswg1133993oswg1080oswg788_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>如果放到规范空间中，Neuman和HUGS的细节对比将变得更加明显。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7702fef0affa4693be0c778e45c1986b@5888275_oswg255140oswg856oswg578_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>测试数据上看，HUGS在NeuMan数据集的五个场景中的PSNR和SSIM评分都达到了SOTA水平，LPIPS误差则处于最低位。</p><p>在ZJU Mocap数据集上，针对5个不同受试者，HUGS也都超越了NerualBody、HumanNeRF等Baseline方法。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_0e3f9beed97b4475984b96daf3782ee9@5888275_oswg364142oswg1080oswg629_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>速度方面，HUGS的训练只需半小时就能完成，而此前最快的VidAvtar也要48小时，速度提升了近百倍。</p><p>渲染速度也是如此，用Baseline方法进行渲染需要2-4分钟，但HUGS只用16.6毫秒就能完成，比人眨眼的速度还快。（下图为对数坐标系）</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_cb9952be310b4a2daf5e954830c9ca91@5888275_oswg71900oswg864oswg476_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>那么，HUGS是如何实现既迅速又细腻地生成3D数字人的呢？</p><h2><strong>02 像搭积木一样渲染</strong></h2><p>HUGS首先将人物和场景分别转化为<strong>3D高斯斑点</strong>。</p><p>其中，人物部分的高斯斑点由三个<strong>多层感知机</strong>（MLP）来预测，并通过<strong>SMPL</strong>（一种人体形状模型）进行初始化。</p><p>SMPL可以用极少的参数建立实体人物到三维网格的映射，只需要10个主要参数就可以表示99%的人体形状变化。</p><p>同时，为了刻画头发和衣服等细节，HUGS也允许高斯函数在一定程度上偏离SMPL。</p><p>场景的高斯斑点通过特征三平面提供的位置编码，由多个MLP预测得到。</p><p>得到人体和场景模型的高斯斑点后，研究者对它们进行了<strong>联合优化</strong>。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_36b6a64aea8e47d3bf1d273568997f93@5888275_oswg419837oswg1080oswg657_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>得到的高斯斑点还会被进行克隆和拆分，从而增大斑点密度，不断接近真实的目标几何表面，这一过程称为<strong>Densify</strong>。</p><p>此外，研究人员还引入了<strong>线性混合动画</strong>（LBS）技术，在运动过程中对高斯斑点进行驱动。</p><p>转换为高斯斑点形式后，研究人员训练了神经网络对高斯函数的属性进行预测，形成真实的人体形状。</p><p>同时，神经网络还定义了高斯函数与人体骨骼的绑定关系，从而实现人物的运动。</p><p>这样，HUGS的渲染过程就像搭积木一样，不需要重新调用神经网络，从而实现了高速渲染。</p><p>消融实验结果表明，LBS、Densify和三平面MLP都是HUGS中的重要环节，缺少任何一个都会对合成效果造成影响。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_1bfa967cb4a94f1f955d7a63364ac292@5888275_oswg428596oswg1080oswg690_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>而人物与场景的联合优化，同样是实现刚好融合效果的关键因素。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_a74a05f39cf841a0bb6948b76fbfbfae@5888275_oswg1217071oswg1080oswg1092_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2><strong>03 One More Thing</strong></h2><p>苹果产生研究数字人的想法已经有一段时间了。</p><p>在苹果MR头显Apple Vision Pro中，就出现过高细节版本的数字分身概念——</p><p>在FaceTime通话时，头显可以创建一个“数字人”，并用它来代表用户。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_803754ad3e844fef85f9ec903c384a40@5888275_oswg303597oswg768oswg432_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>那么，对苹果的这个“数字人生成器”，你怎么看呢？</p><p>参考链接：</p><p>[1]https://appleinsider.com/articles/23/12/19/apple-isnt-standing-still-on-generative-ai-and-making-human-models-dance-is-proof[2]https://twitter.com/anuragranj/status/1737173861756485875/</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/hxSzFBAHcPWAEYKjoszwDA" rel="noopener noreferrer nofollow" target="_blank">“量子位”（ID:QbitAI）</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2575584351954562</id>
            <title>大模型被偷家，腾讯港中文新研究修正认知：CNN搞多模态不弱于Transfromer</title>
            <link>https://www.36kr.com/p/2575584351954562</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575584351954562</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:19:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Transformer, CNN, UniRepLKNet, 大核CNN架构设计
<br>
<br>
总结: 在Transformer占据多模态工具半壁江山的时代，大核CNN又“杀了回来”，成为了一匹新的黑马。腾讯AI实验室与港中文联合团队提出了一种新的CNN架构UniRepLKNet，图像识别精度和速度都超过了Transformer架构模型。切换到其他模态也无需改变模型结构，简单预处理即可接近甚至超越SOTA。这一研究修正了“Transformer在图像任务上吊打CNN”的认知，认为CNN在大一统这一点上可能不弱于Transformer。UniRepLKNet的设计遵循了大卷积核CNN架构的四条guideline，通过解耦感受野、特征抽象层次和模型深度来提高模型的表征能力。 </div>
                        <hr>
                    
                    <p>在Transformer占据多模态工具半壁江山的时代，大核CNN又“杀了回来”，成为了一匹新的黑马。</p><p>腾讯AI实验室与港中文联合团队提出了一种新的CNN架构，图像识别精度和速度都超过了Transformer架构模型。</p><p>切换到点云、音频、视频等其他模态，也无需改变模型结构，简单预处理即可接近甚至超越SOTA。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_450f620a7caa400d8e351d04969a91af@5888275_oswg125398oswg1080oswg265_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>团队提出了专门用于大核CNN架构设计的<strong>四条guideline</strong>和一种名为<strong>UniRepLKNet</strong>的强力backbone。</p><p>只要用ImageNet-22K对其进行预训练，精度和速度就都能成为SOTA——</p><p>ImageNet达到88%，COCO达到56.4 box AP，ADE20K达到55.6 mIoU，实际测速优势很大。</p><p>在时序预测的超大数据上使用UniRepLKNet，也能达到最佳水平——</p><p>例如在全球气温和风速预测上，它就超越了Nature子刊上基于Transformer的前SOTA。</p><p>更多细节，我们接着看作者投稿。</p><h2><strong>01 “Transformer时代”，为什么还需要CNN</strong></h2><p>在正式介绍UniRepLKNet的原理之前，作者首先解答了两个问题。</p><p>第一个问题是，<strong>为什么在Transformer大一统各个模态的时代还要研究CNN</strong>？</p><p>作者认为，Transformer和CNN只不过是相互交融的两种结构设计思路罢了，<strong>没有理由认为前者具有本质的优越性</strong>。</p><p>“Transformer大一统各个模态”正是研究团队试图修正的认知。</p><p>正如2022年初ConvNeXt、RepLKNet和另外一些工作问世之前，“Transformer在图像任务上吊打CNN”是主流认知。</p><p>这几项成果出现后，这一认知被修正为“CNN和Transformer在图像任务上差不多”。</p><p>本研究团队的成果将其进一步修正：在点云、音频、视频上，CNN比我们想象的强太多了。</p><p>在时序预测这种并不是CNN传统强项的领域（LSTM等曾是主流，最近两年Transformer越来越多），CNN都能超过Transformer，成功将其“偷家”。</p><p>因此，研究团队认为，<strong>CNN在大一统这一点上可能不弱于Transformer</strong>。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_3237843804b5460b9bc5d9017fb2de1a@5888275_oswg75475oswg1080oswg568_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>第二个问题是，<strong>如何将一个为图像任务设计的CNN用于音频、视频、点云、时序数据</strong>？</p><p>出于对简洁和通用性的永恒追求，将UniRepLKNet用于其他模态时，不对模型架构主体做任何改变（以下实验用的全都是UniRepLKNet-Small）。</p><p>只需要将视频、音频、点云、时序数据给处理成C×H×W的embedding map（对于图像来说，C=3），就能实现到其他模态的过渡，例如：</p><p>把音频的频谱图（Time×Frequency）看成是一幅单通道图像，即C=1，H=T，W=F；</p><p>将点云进行三视图投影，得到三幅单通道图像，C=3，H和W可以随意指定；</p><p>将视频中的各帧拼接到一起，极为简单地得到一张大图（例如，16帧的3×224×224视频拼接得到3×896×896的输入）；</p><p>对时序数据，借鉴CorrFormer中的embedding layer将数据转换为隐空间中的张量然后简单粗暴地将其reshape成一幅单通道图像。</p><p>后文展示的结果将会证明，如此简单的设计产生的效果是极为优秀的。</p><h2><strong>02 大卷积核CNN架构设计</strong></h2><p>2022年，RepLKNet提出了用超大卷积核（从13×13到31×31）来构建现代CNN以及正确使用超大卷积核的几个设计原则。</p><p>但从架构层面看，RepLKNet只是简单地用了Swin Transformer的整体架构，并没有做什么改动。</p><p>当前大核CNN架构设计<strong>要么遵循现有的CNN设计原则，要么遵循现有的Transformer设计原则</strong>。</p><p>在传统的卷积网络架构设计中，当研究者向网络中添加一个3×3或5×5卷积层时，往往会期望它同时产生三个作用：</p><p><strong>增大感受野</strong></p><p><strong>提高抽象层次</strong>，例如从线条到纹理、从纹理到物体的局部</p><p>通过增加深度而一般地<strong>提高表征能力</strong>（越深，参数越多，非线性越多，拟合能力越高）</p><p>那么，设计大卷积核CNN架构时，应该遵循怎样的原则呢？</p><p>本文指出，应该<strong>解耦上述三种要素，需要什么效果就用对应的结构来实现</strong>：</p><p>用少量大卷积核保证大感受野。</p><p>用depthwise 3×3等小卷积提高特征抽象层次。</p><p>用一些高效结构（如SE Block、Bottleneck structure等）来提高模型的深度从而增强其一般的表示能力。</p><p>这样的解耦之所以能够实现，正是大卷积核的本质优势所保证的，即不依赖深度堆叠的大感受野。</p><p>经过系统研究，本文提出了大卷积核CNN设计的四条Architectural Guidelines。</p><p>根据这些guideline，本文提出的UniRepLKNet模型结构如下——</p><p>每个block主要由depthwise conv、SE Block和FFN三个部分组成。</p><p>其中depthwise conv可以是大卷积核（图中所示的Dilated Reparam Block，其使用膨胀卷积来辅助大核卷积来捕捉稀疏的特征而且可以通过结构重参数化方法等价转换为一个卷积层），也可以只是depthwise 3x3。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_d622952b5b9f45728c49227ba37581e8@5888275_oswg210132oswg949oswg923_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2><strong>03 多项表现超越Transformer</strong></h2><p>作为图像模态中的老三样，ImageNet、COCO、ADE20K上的结果自然是不能少。论文中最多只用ImageNet-22K预训练，没有用更大的数据。</p><p>虽然大核CNN本来不是很重视ImageNet（因为图像分类任务对表征能力和感受野的要求不高，发挥不出大kernel的潜力），但UniRepLKNet还是超过了最新的诸多模型，其实际测速的结果尤为喜人。</p><p>例如，UniRepLKNet-XL的ImageNet精度达到88%，而且实际速度是DeiT III-L的三倍。量级较小的UniRepLKNet相对于FastViT等专门设计的轻量级模型的优势也非常明显。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_41577a7735a142ec9751ce8c21990f1e@5888275_oswg227517oswg672oswg629_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>在COCO目标检测任务上，UniRepLKNet最强大的竞争者是InternImage：</p><p>UniRepLKNet-L在COCO上不及InternImage-L，但是UniRepLKnet-XL超过了InternImage-XL。</p><p>考虑到InternImage团队在目标检测领域的积淀非常深厚，这一效果也算很不容易了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_81ed899c9aef422b8b31e8cbc89cfb33@5888275_oswg109080oswg675oswg359_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>在ADE20K语义分割上，UniRepLKNet的优势相当显著，最高达到55.6的mIoU。与ConvNeXt-XL相比超出了整整1.6。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_6aaab7c568d346768768b9500b1f123d@5888275_oswg143691oswg673oswg467_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>为了验证UniRepLKNet处理时序数据的能力，本文挑战了一个数据规模超大的《Nature》级别的任务：全球气温和风速预测。</p><p>尽管UniRepLKNet本来是为面向图像任务设计的，它却能超过为这个任务而设计的CorrFormer（前SOTA）。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_3fea7cc3b23f4c25b7605003fa341017@5888275_oswg252149oswg951oswg728_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这一发现尤为有趣，因为这种超大规模时间序列预测任务听起来更适合LSTM、GNN和Transformer，这次CNN却将其“偷家”了。</p><p>在音频、视频和点云任务上，本文的极简处理方法也都十分有效。</p><h2><strong>03 One More Thing</strong></h2><p>除了提出一种在图像上非常强力的backbone之外，本文所报告的这些发现似乎表明，大核CNN的潜力还没有得到完全开发。</p><p>即便在Transformer的理论强项——“大一统建模能力”上，大核CNN也比我们所想象的更为强大。</p><p>本文也报告了相关的证据：将kernel size从13减为11，这四个模态上的性能都发生了显著降低。</p><p>此外，作者已经放出了所有代码，并将所有模型和实验脚本开源。</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/J4O7Y10dl2BzujobPmY5Ug" rel="noopener noreferrer nofollow" target="_blank">“量子位”（ID:QbitAI）</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2575493394751623</id>
            <title>上市首日股价涨幅11%，背靠阿里的泛远国际能否“高开高走”？</title>
            <link>https://www.36kr.com/p/2575493394751623</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575493394751623</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 02:03:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 物流行业, 泛远国际, 跨境电子商务物流, 市场竞争
<br>
<br>
总结: 泛远国际是一家跨境电子商务物流服务供应商，面临着物流行业竞争激烈的压力。尽管跨境电子商务物流市场规模持续增长，但不确定性较强，许多企业不得不退出竞争。泛远国际的收入和盈利能力在过去几年出现了波动，主要受到市场竞争和主营市场环境波动的影响。然而，泛远国际在上半年已经出现了业绩复苏，这可能与其在跨境电商物流市场中的地位和对美国市场的依赖有关。 </div>
                        <hr>
                    
                    <p>前有登陆港股多时的京东、中通，后有近期上岸的菜鸟、顺丰，物流行业始终是一片你追我赶的红海。在这一背景下，任谁也难说强势开辟市场。</p><p>年末将至，港交所再度迎来物流企业新成员，泛远国际于11月22日正式在港交所上市。截至收盘，泛远国际股价报1.00港元，涨幅11.11%，港股市值7.80亿港元。</p><p>据悉，泛远国际主打跨境物流，是一家跨境电子商务物流服务供应商，主要从事提供端到端跨境物流服务。如今，迈入新阶段的泛远国际在这一细分赛道上究竟能跑多远？</p><h2><strong>01 行业持续“火热”，泛远国际为何“压力山大”？</strong></h2><p>近两年，新兴势力如Temu、TikTok Shop等电商平台乘势而起，他们以独特的商业模式和先进的技术手段，加速了行业格局的重塑，也为跨境物流企业打开了更丰富多元的市场格局。</p><p>受益于国际贸易及网上购物用户数量的增加，以及利好政策的支持等因素推动，跨境电子商务物流的市场规模持续增长。</p><p>海关总署数据显示，跨境电商进出口规模在2022年达到2.1万亿元，与之相对应的，2018年至2022年期间，跨境电子商务物流的市场规模由1.5万亿元增加到3.2万亿元，复合年增长率为20.4%。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_a001eb35611d45ab875aecb09a07fd64@15574914_oswg287146oswg1280oswg720_img_jpg?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>面对这一背景，作为物流市场一员的泛远国际对电子商务板块也非常重视。从收入占比来看，来源于电子商务行业客户的收入占其2022年总收入的92.1%。</p><p>同时，针对电子商务客户，泛远国际还设有较为完善的硬件设施，其服务网点遍布浙江省、上海市、广东省、福建省、四川省、河南省、山东省及香港等对电子商务物流服务需求较高的地区。</p><p>不过，虽然跨境物流前景广阔，但不确定性较强，不少跨境物流企业也因为“不堪重负”，不得不退出竞争。探迹大数据研究院数据显示，跨境物流企业注销数量持续增加，在2023年出现了注销峰值，达到69137家。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_8fa4d559bc534ae6a73b715d60d6f392@15574914_oswg118925oswg1072oswg438_img_png?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>泛远国际同样也面临不小的压力。招股书数据显示，泛远国际收入由2020年的15.1亿元下跌17.2%至2022年的12.5亿元，其中端到端跨境配送服务收入由13.6亿元下跌27.7%至9.8亿元。</p><blockquote><p>另外，盈利也处于波动状态。按非香港财务报告准则计量，2020年至2022年，泛远国际经调整净利润率分别为3.5%、2.7%、3.1%；毛利率分别为8.1%、7.4%、8.4%。</p></blockquote><p>对于2021年的业绩下滑，泛远国际在招股书中表示，主要是因为万国邮政联盟采用“Option V”邮政酬金制度，导致从国内配送至美国的邮政费用大幅增加。因此，泛远国际的配送服务在美国市场变得“失宠”。</p><p>而2022年的业绩下滑，主要因为泛远国际为改善盈利能力转变了营销策略，该策略下泛远国际提升了服务价格、降低了促销活动的强度和折扣。这导致泛远国际主要市场（北美地区）的包裹数量及计费重量出现下滑，标准配送服务收入相应减少约2.38亿元。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_1cb800681928425f905e0945019e9d23@15574914_oswg23869oswg783oswg502_img_png?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>不过，值得注意的是，今年上半年泛远国际的整体业绩开始复苏。</p><blockquote><p>招股书显示，2023年上半年泛远国际实现营收6.74亿元，同比增长10.5%；其中端到端跨境配送服务收入同比上升18.7%至5.9亿元，标准配送收入上升134.9%至2.3亿元，经济配送收入下跌21.1%至1,478万元。</p></blockquote><p>其盈利能力也有明显提升。上半年毛利率为8.1%，同比增长0.1%；由于上市开支较大，股东净利润同比下跌20.6%至1,092万元。扣除上市开支，经调整净利润同比上升19.9%至2,140万元。</p><p>总体来看，泛远国际前半场的路坎坷而艰辛，而后半程则已经出现转机，这主要源于什么？泛远国际究竟还有何倚仗？</p><h2><strong>02 泛远国际有何“底牌”，助其穿越周期？</strong></h2><p>回顾来看，泛远国际此前的发展困境主要与市场竞争、主营市场环境波动密切相关。</p><p>一方面，跨境电商物流市场竞争高度分散，泛远国际市场份额较少，市场地位还有待提高。</p><p>据弗若斯特沙利文消息，按2022年跨境电子商务物流服务产生的收益计，跨境电子商务物流业的五大本地参与者合共占市场份额的2.5%。其中，泛远国际在中国本地跨境电子商务物流服务供应商中排名介乎25名至30名，占市场份额的0.03%。</p><p>另一方面，泛远国际主要市场也存在不小的波动。业务布局上，泛远国际主攻美国市场，2023年上半年寄送到美国的包裹所产生的收益占总收入的67.8%。因此，泛远国际对美国市场的依赖性较大。</p><p>但由于外部环境的影响，泛远国际寄往美国的包裹数量持续受到冲击，从而影响整体包裹量的增长。而2020年至2022年期间，泛远国际的包裹数量和寄件均价就已经出现“量价齐跌”的趋势。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_906e44ac771e432e9c07328e07ebac4f@15574914_oswg35741oswg917oswg835_img_png?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>另外，由于市场地位并不显著，泛远国际与上游供应商议价时较为被动。因此，通过上下游来优化盈利能力难以作为泛远国际的突破口。</p><p>不过，这并不意味着泛远国际没有底牌。近年来，泛远国际的客户集中度逐步上升。五大客户占总收入比例由2020年的22.8%上升至2022年的28.5%，其中阿里巴巴是最大客户，占2022年收入的12.2%。</p><blockquote><p>招股书显示，2020-2023年6月，泛远国际向阿里巴巴国际站提供服务产生的收益分别约为3708.8万元、2.19亿元、1.53亿元、8554.8万元，该部分收入占当期总收入的百分比也由2.5%提升至12.7%。</p></blockquote><p>值得注意的是，阿里于2021年向泛远国际投资约1.39亿元，这意味着阿里不仅是泛远国际最大的客户也是其股东。二者之间的合作关系趋于稳定。</p><p>因此，电商体系成熟且布局有海外业务的阿里无疑是泛远国际极具价值的合作伙伴，也是泛远国际最有力的底牌之一。</p><p>如今，在美国市场遭遇瓶颈期的泛远国际，既然拥有阿里这张底牌，或许可以打开思路。</p><p>从阿里国际站的地域布局来看，欧美仍是商机最多的市场，此外，东南亚和中东也跻身新贵。因此，泛远国际也可以调整自身的市场布局，将主要市场从美国转移至东南亚市场。</p><p>目前，东南亚国家整体消费水平较低。根据Shoppe印尼和泰国网站相关销售数据，电商各品类产品价格低廉，以女装产品和家电类产品最为热门。</p><p>对比泛远国际最大客户阿里，其平台上跨境卖家主营产品也集中在家居用品、服装、鞋帽及配饰等品类。因此，针对相似的商品类别，泛远国际既然服务过阿里，有现成的惊险，便无需担心东南亚市场的现有需求。</p><h2><strong>03 结语</strong></h2><p>近年来，由于物流行业门槛低，企业间同质化程度高，竞争愈演愈烈。各大品牌都走上了寻求差异化发展的道路，只是在物流企业间打造差异化又谈何容易，所以各大物流企业看似实力悬殊，其实依旧回到了同一起跑线。</p><p>身处其中的泛远国际尽管业绩长期承压，但今年上半年，泛远国际终于迎来净利、营收的同步上升，而电商平台出海规模的持续增长也将带动整个跨境物流行业跨上一个新台阶，在这新一轮的浪潮中，背靠阿里的泛远国际究竟能跑出多远我们拭目以待。</p><p>本文来自微信公众号<a href="https://mp.weixin.qq.com/s/lHN4DBBGDJahHOoREqnhcQ" rel="noopener noreferrer nofollow" target="_blank">“港股研究社”（ID:ganggushe）</a>，作者：港股研究社，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>