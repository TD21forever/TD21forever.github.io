<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/3105032231014145</id>
            <title>4o-mini只有8B，o1也才300B，微软论文意外曝光GPT核心机密</title>
            <link>https://www.36kr.com/p/3105032231014145</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3105032231014145</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Jan 2025 04:18:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, OpenAI, 参数泄露, MEDEC  
<br><br>  
总结: 微软在一篇论文中意外泄露了多个大语言模型的参数，包括GPT-4和Claude 3.5 Sonnet等。论文介绍了一项与医学相关的基准测试MEDEC，旨在识别和纠正临床笔记中的错误。研究显示Claude 3.5 Sonnet在错误标志检测方面表现优于其他模型。尽管论文中有免责声明称数据为估计，但仍引发了外界对参数泄露的质疑。网友们对模型参数的真实性和技术优势进行了讨论，尤其是对Claude 3.5 Sonnet的性能表示关注。 </div>
                        <hr>
                    
                    <p>微软又把OpenAI的机密泄露了？？在论文中明晃晃写着：</p>
  <blockquote>
   <p><strong>o1-preview</strong>约300B参数，<strong>GPT-4o</strong>约200B，<strong>GPT-4o-mini</strong>约8B……‍</p>
  </blockquote>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_8b364aebf9d64f7aa04006e2439b3077@5888275_oswg284973oswg1080oswg463_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>英伟达2024年初发布B200时，就摊牌了<strong>GPT-4</strong>是1.8T MoE也就是1800B，这里微软的数字更精确，为<strong>1.76T</strong>。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_02641788593642e19b36ff63b41dcb78@5888275_oswg55101oswg464oswg180_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>除此之外，论文中给OpenAI的mini系列，Claude3.5 Sonnet也都附上了参数，总结如下：</p>
  <p>o1-preview约300B；o1-mini约100B</p>
  <p>GPT-4o约200B；GPT-4o-mini约8B</p>
  <p>Claude 3.5 Sonnet 2024-10-22版本约175B</p>
  <p>微软自己的Phi-3-7B，这个不用约了就是7B</p>
  <p>虽然论文中后面也有免责声明：</p>
  <blockquote>
   <p>确切数据尚未公开，这里大部分数字是估计的。</p>
  </blockquote>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_aa964e2b3ec7428dbe81bfad36259375@5888275_oswg43912oswg1080oswg139_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>但还是有不少人觉得事情没这么简单。</p>
  <p>比如为什么唯独没有放谷歌Gemini模型的参数估计？或许他们对放出来的数字还是有信心的。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_12e2366d4ced4a16ab47e61758f8bef7@5888275_oswg147779oswg1080oswg407_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>也有人认为，大多数模型都是在英伟达GPU上运行的，所以可以通过token生成速度来估计。</p>
  <p>只有谷歌模型是在TPU上运行的，所以不好估计。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_056f7bf021db4be8aaedd9831fbfd980@5888275_oswg142238oswg1080oswg293_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p><strong>而且微软也不是第一次干这事了。</strong></p>
  <p>23年10月，微软就在一篇论文里“意外”曝出GPT-3.5-Turbo模型的20B参数，在后续论文版本中又删除了这一信息。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_656486edd570429e9da1de9c843deb59@5888275_oswg272882oswg1080oswg376_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>就说你是故意的还是不小心的？</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_fb0519c7a6cd4626ae0235f1c32674d7@5888275_oswg421536oswg800oswg500_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <h2><strong>微软这篇论文说了什么</strong></h2>
  <p>实际上，原论文介绍了一项<strong>与医学相关</strong>的benchmark——MEDEC。</p>
  <p>12月26日就已经发布，不过是比较垂直领域的论文，可能非相关方向的人都不会看，年后才被列文虎克网友们发现。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_785d2f33f4574f299958733f3bc2797e@5888275_oswg137305oswg1080oswg521_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>研究起因是，据美国医疗机构调查显示，有1/5的患者在阅读临床笔记时报告发现了错误，而40%的患者认为这些错误可能影响他们的治疗。</p>
  <p>而且另一方面，LLMs(大语言模型）被越来越多的用于医学文档任务（如生成诊疗方法）。</p>
  <p>因此，MEDEC此番有两个任务。一是识别并发现临床笔记中的错误；二是还能予以改正。</p>
  <p>为了进行研究，MEDEC数据集包含3848份临床文本，其中包括来自三个美国医院系统的488份临床笔记，这些笔记之前未被任何LLM见过。</p>
  <p>它涵盖五种类型的错误（诊断、管理、治疗、药物治疗和致病因子），这些错误类型是通过分析医学委员会考试中最常见的问题类型选择的，并由8位医疗人员参与错误标注。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_f3cfb6170dfd49a7b9e2440096ecc378@5888275_oswg515369oswg1080oswg803_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>而参数泄露即发生在<strong>实验环节</strong>。</p>
  <p>按照实验设计，研究者将选取近期主流的大模型和小模型来参与笔记识别和纠错。</p>
  <p>而就在介绍最终选定的模型时，模型参数、发布时间一下子都被公开了。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_893d41639e6e4315bb2025fb4604c536@5888275_oswg309006oswg1080oswg416_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>对了，省去中间过程，这项研究得出的结论是：Claude 3.5 Sonnet在错误标志检测方面优于其他LLM方法，得分为70.16，第二名是o1-mini。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_956a28c4e37145ce8806242d08de2f20@5888275_oswg384735oswg1080oswg714_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <h2><strong>网友：按价格算合理</strong></h2>
  <p>每一次，ChatGPT相关模型架构和参数泄露，都会引起轩然大波，这次也不例外。</p>
  <p>23年10月，微软论文声称GPT-3.5-Turbo只有20B参数的时候，就有人感叹：难怪OpenAI对开源模型这么紧张。</p>
  <p>24年3月，英伟达确认GPT-4是1.8T MoE，而2000张B200可以在90天内完成训练的时候，大家觉得MoE已经且仍将是大模型架构趋势。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_8d28bbee6a9c4b9f844623aa4aaaee8e@5888275_oswg745325oswg1080oswg810_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>这一次，基于微软估计的数据，网友们主要有几个关注点：</p>
  <p>如果Claude 3.5 Sonnet真的比GPT-4o还小， 那Anthropic团队就拥有技术优势。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_ea005ba5b0c344758ed74d7e6d138d7d@5888275_oswg103874oswg1080oswg330_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>以及不相信GPT-4o-mini只有8B这么小。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_fc96496813894da5b1c0121b6179cb6a@5888275_oswg55227oswg1080oswg177_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>不过此前也有人根据推理成本来算，4o-mini的价格是3.5-turbo的40%，如果3.5-turbo的20B数字准确，那么4o-mini刚好是8B左右。</p>
  <p>不过这里的8B也是指MoE模型的激活参数。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_7d7f78e2ab5c4bc4b557dbf3f9cbc678@5888275_oswg112489oswg1080oswg316_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>总之，OpenAI大概是不会公布确切数字了。</p>
  <p>此前奥特曼征集2024年新年愿望，最后公布的清单中还有“开源”。2025年的最新版本里，开源已经被去掉了。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20250102/v2_175bd95b03584a8b82aaa269801f24ea@5888275_oswg346703oswg1080oswg736_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">论文地址：https://arxiv.org/pdf/2412.19260</p>
  <p>参考链接：</p>
  <p>[1]https://x.com/Yuchenj_UW/status/1874507299303379428</p>
  <p>[2]https://www.reddit.com/r/LocalLLaMA/comments/1f1vpyt/why_gpt_4o_mini_is_probably_around_8b_active/</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/bT_w-T9ElmPUXbYA1f7kCg" rel="noopener noreferrer nofollow" target="_blank">“量子位”</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>