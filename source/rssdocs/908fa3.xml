<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/3054271867752839</id>
            <title>Scaling Law百度最早提出？OpenAI/Claude都受它启发，Ilya出现在致谢名单中</title>
            <link>https://www.36kr.com/p/3054271867752839</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3054271867752839</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Nov 2024 07:14:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <Scaling Law, 百度, 深度学习, 泛化误差>
<br>
<br>
总结: Scaling Law最早由百度在2017年提出，研究了训练集大小、计算规模与模型精度之间的关系。通过实证研究，发现泛化误差与训练集大小呈幂律关系，模型大小与数据大小之间存在缩放关系。研究表明，随着训练集的增大，模型性能提升，但优化变得更加困难。该研究对深度学习的研究和实践具有重要影响，能够指导模型调试和计算系统设计。 </div>
                        <hr>
                    
                    <p>什么？Scaling Law最早是百度2017年提的？！</p>
  <p>Meta研究员翻出经典论文：</p>
  <p>大多数人可能不知道，Scaling law原始研究来自2017年的百度，而非三年后（2020年）的OpenAI。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_f6affc354c6c4f279cd6c46db3e32ef7@5091053_oswg376783oswg788oswg990_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>此研究由吴恩达主持，来自百度硅谷人工智能实验室 (SVAIL) 系统团队。</p>
  <p>他们探讨了深度学习中训练集大小、计算规模和模型精度之间的关系，并且通过大规模实证研究揭示了深度学习泛化误差和模型大小的缩放规律，还在图像和音频上进行了测试。</p>
  <p>只不过他们使用的是 LSTM，而不是Transformer；也没有将他们的发现命名为<strong>「Law」</strong>。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_4a7987d7ad3045cb8109fc670f63aff4@5091053_oswg5650oswg226oswg223_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>再回头看，其中一位作者Gregory Diamos给自己当年在百度的介绍还是<strong>LLM Scaling Law Researcher</strong>。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_4cf958afae504d629bfdfa31386e45fd@5091053_oswg92790oswg976oswg452_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>又有一网友发现，OpenAI论文还引用了2019年这位作者Gregory Diamos等人的调查。但却不知道他们2017年就有了这么一项工作。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_6445fa26e8824857817a84d96a9ce1cf@5091053_oswg188466oswg778oswg408_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>网友们纷纷表示这篇论文非常值得一读，而且完全被低估。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_f3bde3b0beec42e895c7d04b0e76446e@5091053_oswg58882oswg792oswg250_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_c4ee7e47a5da4b548155342397c393da@5091053_oswg47008oswg784oswg260_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>来赶紧看看这篇论文。</p>
  <h2><strong>深度学习Scaling是可预测的</strong></h2>
  <p>在深度学习领域，随着模型架构的不断探索、训练数据集的不断增大以及计算能力的不断提升，模型的性能也在不断提高。</p>
  <p>然而，对于训练集大小、计算规模和模型精度之间的具体关系，一直缺乏深入的理解。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_4fdb6a1fa43e44d3b7d86dbe4165f6f7@5091053_oswg194541oswg1080oswg560_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>本文通过大规模的实证研究，对多个机器学习领域（如机器翻译、语言建模、图像分类和语音识别）进行了测试，发现了一些规律：</p>
  <p>泛化误差（模型在新数据上的表现误差）与训练集大小呈现幂律关系，即随着训练集的增大，泛化误差会以一定的幂次下降。</p>
  <p>模型大小与与数据大小也存在Scaling（缩放）关系，通常模型大小的增长速度比数据大小的增长速度慢。</p>
  <p>具体来说，结合以往工作，团队将注意力集中在准确估计学习曲线和模型大小的缩放趋势上。</p>
  <p>按照一般测量方法，是选择最先进的SOTA模型，并在训练集的更大子集（碎片）上训练这些模型的 “超参数缩减 ”版本，以观察模型的准确性如何随着训练集的大小而增长。</p>
  <p>因此针对这四个领域，机器翻译、语言建模、图像分类和语音识别，找到了他们在大型数据集上显示出 SOTA 泛化误差的模型架构。</p>
  <p>这里的 “大型数据集 ”是指规模可以缩小 2-3 个数量级，但仍足以进行有价值的模型架构研究的训练集。他们为某些 ML 领域选择了一种以上的模型架构，以比较它们的扩展行为。</p>
  <p><strong>机器翻译</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_4c1ade9032de4b40ba7569b4e9483e05@5091053_oswg100827oswg892oswg426_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>团队注意到，随着训练集规模的增大，优化变得更加困难，而且模型会出现容量不足的情况，因此经验误差会偏离幂律趋势。</p>
  <p><strong>词语言模型</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_ca818a3151d84843a14aaa1337eacb87@5091053_oswg112938oswg638oswg314_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>这一结果表明，最佳拟合模型随训练分片大小呈次线性增长。</p>
  <p><strong>字符级语言模型</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_c3e084719a1648cfa9f8a73336d7f382@5091053_oswg68976oswg642oswg320_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>为了测试字符级语言建模，他们训练了深度为 10 的循环高速公路网络（RHN），结果发现该网络在十亿单词数据集上能达到最先进的（SOTA）准确率。</p>
  <p><strong>图像分类</strong>。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_92ba2eebe916407d96fc08e19d5cc1d8@5091053_oswg51305oswg656oswg292_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>图像分类同样呈现出幂律学习曲线和模型大小的缩放关系。并且还表明，在非常小的训练集上，准确率会在接近随机猜测的水平上趋于平稳。</p>
  <p><strong>语音识别</strong>。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_42ab15cb40ad4e07b48b0560262c44d7@5091053_oswg32746oswg372oswg318_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>团队训练了一系列跨度较大的模型尺寸，所以针对每个训练数据大小得出的模型尺寸缩放结果，其意义不像在语言模型（LMs）或图像分类中那么明显。</p>
  <p>随着数据量的增加，大多数模型会经历幂律泛化改进，直至数据量接近其有效容量。在这种情况下，参数为 170 万的模型的准确率在大约 170 小时的音频数据时开始趋于平稳，而参数为 600 万的模型在大约 860 小时的音频数据时趋于平稳（也就是说，大约是前者的 5 倍，这与模型尺寸的差异情况类似）。更大的模型（例如，参数为 8700 万的模型）在更大的数据集规模下，其泛化误差也更接近最佳拟合趋势。</p>
  <p>最后对于这一发现，他们表示，这些比例关系对深度学习的研究、实践和系统都有重要影响。它们可以帮助模型调试、设定准确度目标和数据集增长决策，还可以指导计算系统设计，并强调持续计算扩展的重要性。</p>
  <h2><strong>博客致谢中还有Ilya的名字</strong></h2>
  <p>此次研究主要是由当年吴恩达主持下，百度硅谷人工智能实验室 (SVAIL) 系统团队。</p>
  <p>当时的一群合著者们已经各自去到各个机构实验室、大厂继续从事大模型相关的研究。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_e070073628584e07a746b142b5645fb2@5091053_oswg103797oswg902oswg342_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>在当年博客致谢中，还出现了Ilya的名字，感谢他们参与了这一讨论。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_bdfeb771101c4276b6f1c94b3abb0c28@5091053_oswg117762oswg1080oswg214_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>两年后，也就是2019年，其中一位作者Gregory Diamos又带领团队探讨了深度学习的计算挑战。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_fc6b6af303964613bcb85cf392c56f67@5091053_oswg160915oswg1080oswg368_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>后面的OpenAI论文正是引用了这篇论文的调查讨论了Scaling Law。</p>
  <p>值得一提的是，Anthropic CEO<strong>Dario Amodei</strong>在百度研究院吴恩达团队工作过，他对Scaling Law的第一印象也是那时研究语音模型产生的。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_a3e422c1687a49c9a4bdf21609fb8fe0@5091053_oswg72782oswg1080oswg734_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>Amodei刚开始研究语音神经网络时有一种“新手撞大运”的感觉，尝试把模型和数据规模同时扩大，发现模型性能随着规模的增加而不断提升。</p>
  <p>最初，他以为这只是语音识别系统的特例。但到了2017年，看到GPT-1的结果后意识到这种现象在语言模型上同样适用。</p>
  <p>当年（2015年）他一作发表的论文Deep Speech，合著者中这位Sharan Narang正是两年后这篇论文的主要作者之一。如今后者先后去到了谷歌担任PaLM项目TL大模型负责人，然后现在是Meta当研究员。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_f004eca4012d4a9e859780ac40d8cb4c@5091053_oswg58689oswg916oswg332_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_bde2f47d484148e9865d42abaef0535a@5091053_oswg212453oswg1080oswg519_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>如今这一“冷知识”再次出现在大家的视野，让不少人回溯并重温。</p>
  <p>这当中还有人进一步表示：真正的OG论文使用了<strong>seq2seq LSTM</strong>，并且确定了参数计算曲线。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_587e96f39d944e3a9ed05d6eeb422ba3@5091053_oswg35309oswg778oswg176_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_2658f20a2fae4e0aace878b2715be72d@5091053_oswg34196oswg1080oswg440_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>当年的一作正是Ilya Sutskever。</p>
  <p>参考链接：</p>
  <p>[1]https://arxiv.org/abs/1512.02595</p>
  <p>[2]https://arxiv.org/abs/1909.01736</p>
  <p>[3]https://research.baidu.com/Blog/index-view?id=89</p>
  <p>[4]https://www.linkedin.com/in/gregory-diamos-1a8b9083/</p>
  <p>[5]https://www.linkedin.com/in/dario-amodei-3934934/</p>
  <p>[6]https://x.com/jxmnop/status/1861473014673797411?s=46&amp;t=iTysI4vQLQqCNJjSmBODPw</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/mBgXZZLpg-7bcQqlrXu8EA" rel="noopener noreferrer nofollow" target="_blank">“量子位”</a>，作者：白小交，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3053975978655235</id>
            <title>华为百万豪车，盯上特斯拉面子活</title>
            <link>https://www.36kr.com/p/3053975978655235</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3053975978655235</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Nov 2024 04:21:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <华为, 尊界S800, 高端汽车, 品牌差异化>
<br>
<br>
总结: 余承东在华为新品发布会上推出了尊界S800，标志着华为与江淮汽车合作的第四个智选车品牌，目标对标迈巴赫和劳斯莱斯等豪华品牌。尊界S800的预售价在100万至150万元之间，意在提升品牌溢价。华为选择与市场销量不佳的车企合作，以掌握主动权并提升品牌形象。尽管在中国汽车市场价格战激烈，余承东强调高性价比是销量的关键，同时华为不打算推出20万元以下的车型。未来，华为计划推出更多高端车型，以进一步巩固其市场地位。 </div>
                        <hr>
                    
                    <p>余承东吊足了大众的胃口。</p>
  <p>11月26日的华为新品发布会开始近3个小时后，外界期待已久的尊界新车型才等来亮相时刻。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_91df24e3e046402c932699734d5d87ae@13334819_img_000?x-oss-process=image/format,jpg/format,jpg/interlace,1" /></p>
  <p>这个被余承东冠以“One More Big Thing”的MAEXTRO尊界，是华为与江淮汽车合作打造的第四个智选车品牌，在此之前华为分别与赛力斯合作推出了AITO问界、与奇瑞合作了LUXEED智界、与北汽合作了STELATO享界。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_f9d9483eed904fb9a236ed4010fb87de@13334819_img_000?x-oss-process=image/format,jpg/format,jpg/interlace,1" /></p>
  <p><strong>余承东口中将对标迈巴赫和劳斯莱斯Phantom等豪华品牌的尊界S800，于发布会当天开启预定，预售价100万-150万元，意向金为2万元。</strong></p>
  <p>江淮，由此成了华为帮车企拉高品牌溢价的最新实验对象。</p>
  <p>三年前推出智选车模式时，对于为什么不找BBA（奔驰宝马奥迪）合作的原因，余承东曾解释道，华为要赢、要能活下来，主动权一定要掌握在自己手中。落实到合作车企选择上，华为瞄向的多是市场销量不佳、生存竞争力大幅下滑的品牌。</p>
  <p>在华为品牌加持下，赛力斯借助问界M7、M9售价最高超过50万元，北汽借助享界S9也卖到了40万往上，江淮更是凭借尊界S800突破百万元。</p>
  <p><strong>但在大打价格战的中国汽车市场，相比追求高溢价，性价比才是提振销量的法宝。</strong></p>
  <p>这也能从当天发布的智界新S7上略窥一二。上市一年之内，华为已经接连为这款车型开了三次发布会，且每开一次就降价一波：4月份，智界S7二次上市发布会上，该车大部分车型直降2万元；11月份，智界S7第三次发布会上，该车起售价再次调低2万元。</p>
  <p>尽管不断降价，余承东依然遵守着不做20万元以下产品的红线。今年6月的2024第十六届中国汽车蓝皮书论坛上，余承东对此进一步解释称，“很多人都在抱怨我们的车卖得太贵了，不是我们不想卖便宜，而是没有能力卖便宜，卖便宜就亏死了，亏死了就关门了，没有办法为大家继续服务了。”</p>
  <p>甚至在余承东看来，华为哪怕是做30万元以下的车，都是赔本赚吆喝。<strong>如何在维持高价的同时，还能吸引用户下定？打造品牌差异化便是可行路径之一。</strong></p>
  <p>此前，华为一度凭借其在自动驾驶技术上的优势，给旗下智选车业务带来差异化卖点。但在今年流行起来的端到端技术面前，华为的这一优势正被越来越多车企追赶上，比亚迪甚至制定了明年自动驾驶技术覆盖旗下全系车型的计划，10万元小车也有望体验高阶智驾功能。</p>
  <p>置身全新的竞争环境，通过高端化乃至豪华化，拉开与其他车企的差距，或许就成了华为想走的另一步棋。</p>
  <p>特斯拉就是一个典型的参考对象。借助售价百万元级别的Roadster、Model S和Model X的成功，特斯拉高端电动车的形象逐步形成用户认知。有了高端形象的面子加持，作为特斯拉里子的Model 3和Model Y，才一步步酝酿出了全球爆款车的潜质。</p>
  <p>如今，借助尊界，余承东也想给华为智选车再多挣点面子。</p>
  <h2>A</h2>
  <p>正式亮相的尊界S800，确实做足了面子活。</p>
  <p>与享界S9上的星河大灯采用璀璨星钻工艺一样，尊界S800的车灯有过之而无不及，其星河设计语言，出现在车辆前脸两侧的竖排灯组中，也出现在贯穿式尾灯中。这次，华为还在隐藏式门把手内加入了星空视觉灯组。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_324cd1ba37c8475489571dc281993f8f@13334819_img_000?x-oss-process=image/format,jpg/format,jpg/interlace,1" /></p>
  <p>作为一款对标迈巴赫、劳斯莱斯等超豪华品牌的车型，尊界S800在长度上做足了功夫，长5480mm，宽2000mm ，高1536mm，轴距3370mm。而且，与迈巴赫S级打造双色车身一样，尊界S800展车同样采用了上银下紫的撞色设计，并配备了豪车同款的“大饼轮毂”。</p>
  <p>其他方面，尊界S800还将搭载途灵二代龙行平台，官方称该平台历经五年打磨，并将率先采用L3级别架构设计。</p>
  <p><strong>作为一款预告版车型，余承东介绍称，尊界S800的很多保密地方，如内饰，需要等到上市时候才能一一揭晓。</strong></p>
  <p>至于此前网传的尊界S800是否首发华为ADS 4.0智驾系统，是否会配备后轮转向技术，以及是否会采用增程动力系统，也都有待正式发布会上验证。</p>
  <p>但有一项可以确认的信息是，<strong>尊界S800不会是华为与江淮合作的唯一车型。</strong>2023年2月，华为与江淮联合造车消息被曝光之际，当时中建六局联合体在中标安徽肥西新能源汽车智能产业园EPC项目中提到，该项目是合肥市重点工程，建成后将用于华为与江淮汽车在合肥共同开发新一代高端智能电动汽车。</p>
  <p>彼时，中标内容中提到，工厂将在2024年竣工，生产车型级别为“B级→D+级”，可适应 Sedan（轿车）、SUV、 MPV、Crossover、Sporty等各类车型的共平台开发，达到年产30万辆新能源乘用车生产能力。</p>
  <p>尊界S800之外，余承东还带来了华为Mate 70系列。不同于去年Mate 60系列的未发先售，Mate 70系列不再加入“先锋计划”，其屏幕、电池、影像、材质等，都被余承东逐一介绍。</p>
  <p>值得一提的是，<strong>余承东依然没有介绍Mate 70系列搭载的芯片信息。</strong>除了发布起售价5499元的Mate 70系列外，当天，余承东还对外发布了起售价11999元的Mate 70 RS非凡大师版，以及起售价12999元的折叠屏Mate X6。</p>
  <h2>B</h2>
  <p>Mate系列手机冲高的成功经验，无疑让华为打造尊界S800的高端形象有了更多底气。</p>
  <p>更重要的是，凭借去年以来问界系列的销量爆火，华为已经初步具备了特斯拉一般的里子。</p>
  <p>截至今年11月，鸿蒙智行在成立的32个月中，全系车型累计交付达到50万辆，不仅创下行业最快交付纪录，且问界M9车型还摘下了50万元以上车型的销量冠军。两大爆款问界新M7累计大定突破30万辆，问界M9累计大定18万辆。</p>
  <p><strong>初具规模效应的问界，带动赛力斯在经历四年亏损之后终于扭亏为盈，并推动华为车BU也开始迈向盈利。</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_d8461fb8646f4f66b85e03cd6ffe8d95@13334819_img_000?x-oss-process=image/format,jpg/format,jpg/interlace,1" /></p>
  <p>2024中国汽车百人会上，余承东便曾提前透露智选车业务盈利的好消息。他提到，今年前三个月智选车已经实现扭亏为盈，同期车BU接近盈亏平衡，并预计4月份后能实现盈利。</p>
  <p>根据赛力斯8月份披露的一些数据报告，大信会计师事务所（特殊普通合伙）出具的深圳引望（华为车BU拆分独立后的新公司）模拟财务报表显示，2022年—2024年上半年，引望营业收入分别为20.98亿元、47亿元和104.35亿元，归母净利润分别为-75.87亿元、-55.97亿元和22.31亿元，预估2024年全年归母净利润为33.51亿元。</p>
  <p><strong>追求高端化所带来的品牌溢价，也使得无论华为还是特斯拉，都暂时不想给年轻人造车，甚至以此卷入价格战的泥潭。</strong></p>
  <p>余承东在多个场合反复强调，华为不做20万元以下的车，“20万以下我们做会亏损，我们没有那么低的成本。”2023年2月被问到华为汽车未来会不会跟比亚迪产生竞争时，余承东如此解释道。</p>
  <p>马斯克一度在2020年曾考虑过推出一款小型、廉价、大众化的车，售价约为25000美元，但后来他又主动搁置了这个计划，并多次在内部讨论中否决这个想法，把更多精力开始倾斜给Robotaxi。</p>
  <p>不想造便宜车的纠结，一直延续至今。就在被曝出特斯拉将在明年上市廉价车型之余，今年4月份，媒体再次曝出了该平价车型被意外终止的消息，成本不够有竞争优势被认为是该项目取消的原因之一。</p>
  <p>相比如何尽可能降低成本生产一款廉价车型，马斯克显然更在意如何将自动驾驶能力融入其中。在今年三季度财报会上，被问及25000美元车型何时大规模量产时，马斯克认为，单一考虑25000美元车型是没有意义的，更重要的是算好每英里的总拥有成本的账。如何尽可能降低总拥有成本？马斯克给出的答案便是自动驾驶。</p>
  <h2>C</h2>
  <p><strong>放弃为年轻人造车的马斯克和余承东，眼下正迎来各自的新考验：</strong>特斯拉需要解决迟迟没有新车面世的难题；华为智选车业务需要向外界证明其还有进一步提升销量的能力。</p>
  <p>如何在没有新车发布的情况下，继续吸引用户购买特斯拉产品？马斯克开始押注Robotaxi 所带来的品牌差异化，一系列动作也随之展开。</p>
  <p>马斯克不仅在10月10日举办了Robotaxi Day，更是在三季度财报会上明确，Robotaxi形态的运营车辆Cybercab，将在2026年实现量产，“我对其充满信心。”</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241127/v2_246e1e6bb34143878652252ad5b28c7b@13334819_img_000?x-oss-process=image/format,jpg/format,jpg/interlace,1" /></p>
  <p>FSD，成了马斯克实现上述愿景的重中之重。截至第三季度，特斯拉FSD完全自动驾驶能力(监管版）累计行驶里程超过20亿英里（约32.2亿公里），其中超过50%是在V12版本上实现。</p>
  <p>借助强大的AI计算资源，马斯克宣称FSD V13版本很快也会发布，相比V12.5，其两次干预之间的里程数将会提升5倍，明年第二季度，最晚第三季度，在两次干预之间的里程数上，FSD将会超过人类驾驶水平。</p>
  <p>反观华为这边，据36氪汽车爆料，明年将是华为旗下智选车业务鸿蒙智行的产品大年。在售车型会超过10款，除了现有的问界M5、M7、M9，享界S9，智界S7、R7，以及尊界S800以外，还将推出问界系列全新车型问界M8，定价在30万-40万元之间，享界S9增程版，以及尊界第二款车型。</p>
  <p><strong>相比马斯克押注Robotaxi，通过技术来塑造品牌差异化，不断堆高品牌形象，则成为余承东选择的另一条品牌差异化路径。</strong></p>
  <p>尊界S800上市之后，能否帮助华为智选车业务争夺来新的用户，暂未可知，但可以确定的是，余承东借此也算圆了想为华为潜在车主打造百万豪车的梦想。2022年8月问界M7交付仪式上，余承东曾讲到，有朋友告诉他M7唯一的缺点就是价格太便宜了，“因为他过去都是买一两百万元以上的车，贵一点能体现到他的身份。”</p>
  <p>本文来自微信公众号“字母榜”，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>