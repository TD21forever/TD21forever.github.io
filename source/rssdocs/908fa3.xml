<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/3075703982748552</id>
            <title>苹果首款人工智能芯片曝光，想让 iPhone 的 AI 体验更「丝滑」</title>
            <link>https://www.36kr.com/p/3075703982748552</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3075703982748552</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Dec 2024 11:50:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, 博通, AI 芯片, Baltra  
<br><br>  
总结: 苹果与博通合作研发AI芯片，旨在减少对英伟达的依赖，推动多元化芯片来源。新芯片代号为“Baltra”，计划于2026年量产，专注于优化AI工作负载和增强机器学习功能。博通的3.5D XDSiP技术将与Baltra芯片整合，以实现低延迟通信。苹果的AI战略不仅限于设备端，还包括云计算能力，目标是提升用户的AI体验。预计到2028年，AI服务器芯片市场将达到450亿美元，苹果的进入将对现有市场领导者构成挑战。 </div>
                        <hr>
                    
                    <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_3fe8046bb9114daa8e4eeab420adda07@000000_oswg308687oswg1080oswg1242_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p><strong>苹果造芯 博通助力&nbsp;</strong></p>
  <p>继 OpenAI 宣布计划自研 AI 芯片之后，科技巨头苹果近日也传出了其正在与博通（Broadcom）联合研发 AI 芯片的消息。&nbsp;</p>
  <p>有趣的是，两家公司针对此事竟给出了几乎相同的理由： <strong>尽量避免对英伟达的依赖</strong>。实际上，「多元化芯片来源」正是苹果 AI 持续战略的一部分。&nbsp;</p>
  <p>博通似乎成为了近期 AI 硬件领域的「香饽饽」，其在短短一个多月之内就与两家 AI 领头羊企业达成合作。据悉，博通已经占据了超八成 AI ASIC 市场，其在 2025 财年的 AI 收入有望达到 170 亿美元以上，同比增速超过 40%。&nbsp;</p>
  <p>与苹果和合作消息一出，博通的股价应声上涨 6%，苹果股价也有短暂小幅上涨。这并非两者的首次合作，2023 年五月苹果就曾宣布与博通合作开发 5G 射频组件等等。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_6e7ff225f2ed438093c5466bbdecf606@000000_oswg613973oswg1000oswg666_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">图源：优分析&nbsp;</p>
  <p>据「The Information」报道，苹果 AI 芯片的代号为「Baltra」，将采用台积电先进的 N3P 工艺，计划于 2026 年投入量产。这个时间也与 OpenAI 自研 AI 芯片的量产时间重合。&nbsp;</p>
  <p>消息称，Baltra 的设计开发旨在 <strong>优化 AI 工作负载，增强 AI 和机器学习（ML）功能</strong>。这枚芯片将专用于推理任务，以及处理新数据并将其传输给大语言模型（LLMs）以生成输出。&nbsp;</p>
  <p>而此次与博通的合作重点，则是将其高性能网络技术与芯片的核心处理能力整合，确保 AI 操作所需的低延迟通信。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_ee2eb51b14cb47729f90028dabeb1c0f@000000_oswg360526oswg800oswg420_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">图源：Crypto Briefing&nbsp;</p>
  <p>近日，博通展示了一种先进的 3.5D 系统级封装技术（3.5D XDSiP），能够让制造商 <strong>超越传统光罩尺寸的限制</strong>。&nbsp;</p>
  <p>具体来讲，3.5D XDSiP 将计算芯片堆叠在一个逻辑芯片上，该逻辑芯片与高带宽内存（HBM）连接，同时将其他 I/O 功能分配到一组单独的芯片上。&nbsp;</p>
  <p>与传统的 3.5D 封装技术不同，博通的设计采用了「面对面」的方法，这种方法允许芯片之间通过混合铜键合（HBC）排布更密集的电气接口，从而实现更高的芯片间互连速度和更短的信号路由。&nbsp;</p>
  <p>博通的 3.5D XDSiP 技术本质上是一个「蓝图」，客户可以使用它来构建自己的多芯片处理器。巧合的是，博通预计这项技术的第一批部件也将于 2026 年投入生产，这与「Baltra」的投产时间不谋而合。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_41703cade6014175be83e4a1bdf4d63a@000000_oswg412832oswg1080oswg784_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">图源：The Register&nbsp;</p>
  <p>毫无疑问，这枚芯片最重要的使命，就是 <strong>为苹果自家的 Apple Intelligence 服务</strong>。&nbsp;</p>
  <p>苹果的原生 AI 功能自发布以来便一直引人关注。苹果原计划直接在设备上运行大部分 AI 功能，但某些功能（如 Siri和 Maps）在云端处理，并且对计算能力有很高的需求，现有的芯片又并非定制。于是，「Baltra」的提案应运而生。&nbsp;</p>
  <p>Baltra 是为苹果自己的数据中心而定制设计的，其用于驱动高级 AI 任务，并确保为用户带来「无缝」的 AI 体验。这意味着苹果的 AI 战略已经超出端侧，并纳入了云计算能力。&nbsp;</p>
  <p>值得一提的是，苹果刚刚发布了 iOS 18.2 正式版系统，其中新增了多项实用的 AI 功能，包括 ChatGPT 正式登陆苹果全家桶等等。未来，Baltra 将使苹果在其产品生态中部署 AI 时获得性能优势和更大的灵活性。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_2593793d35eb4ec6866dda56260b9b4f@000000_oswg961081oswg1080oswg608_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="img-desc">图源：Fast Company&nbsp;</p>
  <p>据估计，2028 年 AI 服务器芯片市场规模预计将达到 450 亿美元，而苹果在 AI 服务器芯片市场的定位将会是对现有领导者的极大挑战。&nbsp;</p>
  <p>彭博社分析指出，苹果与博通的合作进一步巩固了其在 ASIC 设计中的主导地位，这项合作预计将推动博通在 2025-2026 年之后的 AI 收入增长，并且其有望在苹果供应链中占据更多份额。&nbsp;</p>
  <p>此外，自 OpenAI 在 2022 年 12 月发布 ChatGPT 以来，苹果加快了自家服务器芯片的开发工作，以保持其在人工智能领域的竞争力。苹果的目标是在 12 个月内完成「Baltra」芯片的设计。&nbsp;</p>
  <p>本文来自微信公众号 <a href="https://mp.weixin.qq.com/s?__biz=MjgzMTAwODI0MA==&amp;mid=2652389787&amp;idx=2&amp;sn=2c338471d199c9c9ad3747c3e9ac2b6b&amp;chksm=9a7af074747a15a83c74329b017a73de1d262a51abf4a2f3ee8687638841379d908e19d6fca2&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“爱范儿”（ID：ifanr）</a>，作者：<strong>范津瑞</strong>，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3075708520969089</id>
            <title>特斯拉皮卡入华，怎么这么难</title>
            <link>https://www.36kr.com/p/3075708520969089</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3075708520969089</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Dec 2024 11:49:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 特斯拉, Cybertruck, 中国市场, 能耗申报  
<br><br>  
总结: 特斯拉Cybertruck在进入中国市场的过程中经历了多次波折。尽管工信部曾发布了Cybertruck的能耗申报信息，但该信息很快被作废。Cybertruck被定义为“纯电动越野乘用车”，这使其规避了针对皮卡的严格规定。特斯拉仍在推进Cybertruck入华，并计划生产符合中国标准的特供版车型。为了适应中国市场，Cybertruck可能需要进行改动，包括调整车身尺寸和满足新的碰撞保护标准。尽管Cybertruck在美国已开始交付，但在中国市场的合法上路仍需时间。 </div>
                        <hr>
                    
                    <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_a1bc25be6fa24838b486d6bfb90f4699@000000_oswg148513oswg1080oswg536_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>想把特斯拉Cybertruck开到国内，真的是一波三折。&nbsp;</p>
  <p>10天前，特斯拉还在辟谣Cybertruck没有进入中国市场的计划。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_5f6d1339709c40acba25741d8f82a51a@000000_oswg160350oswg1080oswg362_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>12月11日，工信部中国汽车能源消耗量查询网站，就新增了 <strong>特斯拉电动皮卡Cybertruck</strong>的能耗申报信息，众多网友一夜沸腾。&nbsp;</p>
  <p>还没高兴太久，这条申报的信息 <strong>当天就作废了</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_628a3d82a89945d3a5747602679f0850@000000_oswg95180oswg1080oswg281_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>从认证信息来看，此次是通过 <strong>单车认证</strong>的方式，引入了一台车架号为7G2CEHEE0RA000008的Cybertruck，不是大规模引进。&nbsp;</p>
  <p>而且这辆车的身份是“ <strong>纯电动越野乘用车</strong>”，而不是“多用途货车”，意味着它并非皮卡手续，不需要强制报废。&nbsp;</p>
  <p>至少又给国内粉丝留下了一丝幻想。&nbsp;</p>
  <p>关于Cybertruck，大家最关心的问题是未来会以平行进口还是官方渠道的方式进入中国，在国内能否合法上路？&nbsp;</p>
  <p>从特斯拉的角度来看，马斯克并未表明要放弃中国市场。2024特斯拉股东大会上，马斯克还称，特斯拉必须要生产符合中国、欧盟标准的Cybertruck“特供版”车型，然后才能出口，此次申报也说明特斯拉正在推进Cybertruck入华。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_d969c4250a044d2aa1f74fe57759a59d@000000_oswg65853oswg1080oswg926_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>另外，今年8月份，天津街头出现了一辆挂着“津A”绿色车牌的Cybertruck，不久后这辆车又被挂在二手平台上以360万左右的价格出售。当时分析这款车应该是以平行进口的方式进入中国并顺利上牌。&nbsp;</p>
  <p>依照8月份工信部网站上并没有申报信息来看，这次Cybertruck很有可能是想通过官方渠道进入中国市场。&nbsp;</p>
  <p>至于Cybertruck能不能适应中国市场，先看一下工信部上公开的数据。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_62119dca36fe4e4e8fb790393a73c8b8@000000_oswg322089oswg978oswg1380_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>Cybertruck的车辆种类为乘用车（M1类），驱动电机峰值功率为206/222/222kW，续航里程为618公里，电能消耗量为百公里22.6kWh。&nbsp;</p>
  <p>其整车质量为3104千克，最大设计总质量为3700千克，电能当量燃料消耗量为2.62L/100km，预估能源成本为 <strong>14.92元/100km</strong>。而高温开空调作业续驶里程平均下降15%，低温暖风作业续驶里程平均下降40%。&nbsp;</p>
  <p>关键信息是Cybertruck被定义成了乘用车，此前，由于Cybertruck车型外观与中国针对皮卡车型的政策法规存在冲突，Cybertruck一直没能在中国市场交付。在特斯拉中国的官网上，Cybertruck也不是可订购的状态。&nbsp;</p>
  <p>如今被界定为乘用车，就规避了15年强制报废等很多针对皮卡的条条框框。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_90e23693b0104c48bbd699d709e66156@000000_oswg209898oswg1080oswg702_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>此外，Cybertruck全车身都是不锈钢，车头部分还有很多接近直角的设计，完全不符合国内车辆上路的标准，今年1月份来中国巡展的Cybertruck也是被板车拉着上路的。&nbsp;</p>
  <p>所以Cybertruck想要进入中国市场，大概率需要调整，以此满足国内车辆上路标准，马斯克早在股东大会上确定了这一点。&nbsp;</p>
  <p>近期，特斯拉还被曝出正秘密采用两种不同的工程方案对Cybertruck进行改进，以适应中国市场的准入要求。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_545ab42bec7d423c8c564db657028d01@000000_oswg53076oswg1080oswg717_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>比如Cybertruck的车身 <strong>长度为5885mm，宽2026mm</strong>，这样的车身尺寸要停进国内的标准车位难度很高。为适应中国市场，改变车身尺寸或许也不是不可能。&nbsp;</p>
  <p>如果Cybertruck真的要以官方渠道进入中国，大概率是要对特斯拉Cybertruck进行一场以合法上路为目的的改动。&nbsp;</p>
  <p>还有更加不利的一点就是，最新的GB 24550—2024《汽车对行人的碰撞保护》将在 <strong>2025年1月1日</strong>开始实施。也就是说还有不到20天的时间，政策就要收紧，Cybertruck合法上路的条件会变得更加严苛。&nbsp;</p>
  <p>根据网站数据，我们还可以看出，假设特斯拉Cybertruck 618km的续驶里程是在CLTC的标准下，依照其电能消耗量为每百公里电耗22.6kWh来算的话，此次申报的特斯拉Cybertruck电池包容量约为 <strong>140kWh</strong>。&nbsp;</p>
  <p>根据其驱动电机峰值功率为206/222/222kW来看，申报的这个版本应该是 <strong>三电机版本</strong>。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_f92e32dcd37e409e91721782bc26592b@000000_oswg1235291oswg1080oswg572_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>最后就是价格了，去年年底，Cybertruck已经正式在美国开启交付，单电机后驱版车型售价为60990美元，双电机四驱版车型售价79990美元，三电机赛博野兽版车型售价为99990美元。后轮驱动版车型将会在2025年正式上市。&nbsp;</p>
  <p>从目前已有售价的车型来看，顶配车型的售价折合成人民币之后也不超百万。但在中国市场，Cybertruck百万售价几乎是板上钉钉的事。&nbsp;</p>
  <p>能源消耗量测算申报是汽车进入中国市场所需满足的条件之一，而这个条件在当天被作废，那Cybertruck想要入华销售可能还要再沉淀一段时间。&nbsp;</p>
  <p>其实早在2020年，特斯拉中国官网上就已经开启了Cybertruck的预定通道，定金为人民币1000元，不过这一预定通道后来也被关闭了。&nbsp;</p>
  <p>至于究竟什么时候能进入国内，各位看官还是耐心等等吧。&nbsp;</p>
  <p>本文来自微信公众号 <a href="https://mp.weixin.qq.com/s?__biz=Mzg5MTc3NjgxNQ==&amp;mid=2247539931&amp;idx=1&amp;sn=1ba35023024dc25534cd9025d4268c41&amp;chksm=cecb8c0769ad6d6c25191593f856b1d62114bb60bbab916242c26089d6c2b71f8feaf9f2170d&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“超电实验室”（ID：SuperEV-Lab）</a>，作者：刘雅杰，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/3075749114409865</id>
            <title>Sora之后，苹果发布视频生成大模型STIV，87亿参数一统T2V、TI2V任务</title>
            <link>https://www.36kr.com/p/3075749114409865</link>
            <guid isPermaLink="false">https://www.36kr.com/p/3075749114409865</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Dec 2024 11:44:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <苹果, 视频生成, 多模态, STIV模型>
<br>
<br>
总结: 本文介绍了苹果公司发布的多模态视频生成大模型STIV，该模型具有8.7亿参数，支持文本和图像条件的视频生成。研究中提出了统一处理文本到视频（T2V）和图像到视频（TI2V）任务的方法，并通过创新的训练策略和模型架构优化，显著提升了生成质量。STIV模型的设计包括时空注意力机制、条件嵌入和流匹配目标等，旨在提高模型的稳定性和训练效率。实验结果表明，STIV在多个基准数据集上表现优越，为未来视频生成技术的应用奠定了基础。 </div>
                        <hr>
                    
                    <p><strong>Apple MM1Team 再发新作，这次是苹果视频生成大模型，关于模型架构、训练和数据的全面报告，87 亿参数、支持多模态条件、VBench 超 PIKA，KLING，GEN-3。</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_b4348f5768534ff49f40ad3467d3a69b@46958_oswg138041oswg1080oswg350_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>论文地址: https://arxiv.org/abs/2412.07730</p>
  <p>Hugging Face link: https://huggingface.co/papers/2412.07730</p>
  <p>OpenAI 的 Sora 公布了一天之后，在一篇由多位作者署名的论文《STIV: Scalable Text and Image Conditioned Video Generation》中，苹果正式公布自家的多模态大模型研究成果 —— 这是一个具有高达 8.7B 参数的支持文本、图像条件的视频生成模型。&nbsp;</p>
  <p>近年来，视频生成领域取得了显著进展，尤其是基于 Diffusion Transformer (DiT) 架构的视频生成模型 Sora 的推出。尽管研究者已在如何将文本及其他条件融入 DiT 架构方面进行了广泛探索，如 &nbsp;PixArt-Alpha &nbsp;使用跨注意力机制，SD3 将文本与噪声块拼接并通过 MMDiT 模块应用自注意力等，但纯文本驱动的视频生成（T2V）在生成连贯、真实视频方面仍面临挑战。为此，文本 - 图像到视频（TI2V）任务被提出，通过加入初始图像帧作为参考，提供了更具约束性的生成基础。&nbsp;</p>
  <p>当前主要挑战在于如何将图像条件高效地融入 DiT 架构，同时在模型稳定性和大规模训练效率方面仍需创新。为解决这些问题，我们提出了一个<strong>全面、透明的白皮书，涵盖了模型结构，训练策略，数据和下游应用，统一了T2V和TI2V任务</strong>。&nbsp;</p>
  <p>基于以上问题，该工作的贡献与亮点主要集中在：&nbsp;</p>
  <p>提出 STIV 模型，实现 T2V 和 TI2V 任务的统一处理，并通过 JIT-CFG 显著提升生成质量；</p>
  <p>系统性研究包括 T2I、T2V 和 TI2V 模型的架构设计、高效稳定的训练技术，以及渐进式训练策略；</p>
  <p>模型易于训练且适配性强，可扩展至视频预测、帧插值和长视频生成等任务；</p>
  <p>实验结果展示了 STIV 在 VBench 基准数据集上的优势，包括详细的消融实验和对比分析。</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_f0918cedebde4b56a35d74d88bc72c4d@46958_oswg1260281oswg1080oswg1368_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>该研究不仅提升了视频生成质量，还为视频生成模型在未来多种应用场景中的推广奠定了坚实基础。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_a0241d4c147f47e4a5a87f2be09b31fc@46958_oswg106310oswg912oswg795_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p><strong>构建 STIV 的配方解析</strong></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_7e5f8ca1a85e4730b71a221af7de787f@46958_oswg269681oswg1080oswg540_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <h2><strong>基础模型架构</strong></h2>
  <p>STIV 基于 &nbsp;PixArt-Alpha &nbsp;架构，通过冻结的变分自编码器（VAE）将输入帧转换为时空潜变量，并使用可学习的 DiT 块进行处理。文本输入由 T5 分词器和内部训练的 CLIP 文本编码器处理。此外，该研究还对架构进行了以下优化：&nbsp;</p>
  <p>时空注意力分解：采用分解的时空注意力机制，分别处理空间和时间维度的特征，这使得模型能够复用 T2I 模型的权重，同时降低了计算复杂度。</p>
  <p>条件嵌入：通过对图像分辨率、裁剪坐标、采样间隔和帧数等元信息进行嵌入，并结合扩散步长和文本嵌入，生成一个统一的条件向量，应用于注意力层和前馈网络。</p>
  <p>旋转位置编码（RoPE）：利用 RoPE 提升模型处理时空相对关系的能力，适配不同分辨率的生成任务。</p>
  <p>流匹配目标：采用流匹配（Flow Matching）训练目标，以更优的条件最优传输策略替代传统扩散损失，提升生成质量。</p>
  <p><strong>模型扩展与训练优化&nbsp;</strong></p>
  <p>稳定训练策略：通过在注意力机制中应用 QK-Norm 和 sandwich-norm，以及对每层的多头注意力（MHA）和前馈网络（FFN）进行归一化，显著提升了模型训练稳定性。</p>
  <p>高效训练改进：借鉴 MaskDiT 方法，对 50% 的空间 token 进行随机掩码处理以减少计算量，并切换优化器至 AdaFactor，同时使用梯度检查点技术显著降低内存需求，支持更大规模模型的训练。</p>
  <h2><strong>融合图像条件的方法</strong></h2>
  <p><strong>简单的帧替换方法</strong></p>
  <p>在训练过程中，我们将第一个帧的噪声潜变量替换为图像条件的无噪声潜变量，然后将这些潜变量传递到 STIV 模块中，并屏蔽掉被替换帧的损失。在推理阶段，我们在每次 扩散步骤中使用原始图像条件的无噪声潜变量作为第一个帧的潜变量。</p>
  <p>帧替换策略为 STIV 的多种应用扩展提供了灵活性。例如，当 c_I (condition of image)=∅ 时，模型默认执行文本到视频（T2V）生成。而当 c_I 为初始帧时，模型则转换为典型的文本-图像到视频（TI2V）生成。此外，如果提供多个帧作为 c_I，即使没有 c_T (condition of text)，也可以用于视频预测。同时，如果将首尾帧作为 c_I提供，模型可以学习帧插值，并生成首尾帧之间的中间帧。进一步结合 T2V 和帧插值，还可以生成长时视频：T2V 用于生成关键帧，而帧插值则填补每对连续关键帧之间的中间帧。最终，通过随机选择适当的条件策略，可以训练出一个能够执行所有任务的统一模型。</p>
  <p><strong>图像条件随机丢弃&nbsp;</strong></p>
  <p>如前所述，帧替换策略为训练不同类型的模型提供了高度灵活性。我们在此展示其具体应用，即同时训练模型以执行文本到视频（T2V）和文本 - 图像到视频（TI2V）任务。在训练过程中，我们随机丢弃图像条件 cI 和文本条件 cT，类似于 T2V 模型中仅对文本条件随机丢弃的方式。&nbsp;</p>
  <p>联合图像 - 文本无分类器引导（JIT-CFG）&nbsp;</p>
  <p>无分类器引导（Classifier-Free Guidance, CFG）在文本到图像生成中表现出色，可以通过将概率质量引导到高似然区域来显著提升生成质量。在此基础上，我们提出了联合图像 - 文本无分类器引导（JIT-CFG），同时利用文本和图像条件进行引导，其速度估计公式为：&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_a870244599514de1a1b070aa1db03dab@46958_oswg15080oswg829oswg118_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>其中 s 为引导比例。当 c_I=∅ 时，该方法退化为标准的 T2V 无分类器引导。尽管可以像 InstructPix2Pix 所述引入两个独立的引导比例，以平衡图像和文本条件的强度，我们发现两步推理方法已经能够取得优异效果。此外，使用两个引导比例会增加一次前向传递，从而提高推理成本。&nbsp;</p>
  <p>实验证明图像条件随机丢弃结合 JIT-CFG 不仅能自然地实现多任务训练，还有效解决了高分辨率视频生成模型训练 的 “静止” 问题。我们推测，图像条件随机丢弃可以防止模型过度依赖图像条件，从而更好地捕捉视频训练数据中的运动信息。&nbsp;</p>
  <p><strong>渐进式训练策略&nbsp;</strong></p>
  <p>我们采用渐进式训练策略，其流程如图 4 所示。首先训练一个文本到图像（T2I）模型，用以初始化文本到视频（T2V）模型；随后，T2V 模型用于初始化 STIV 模型。为快速适应高分辨率和长时训练，我们在空间和时间维度中加入了插值的 RoPE 嵌入，并利用低分辨率、短时长模型的权重进行初始化。值得注意的是，高分辨率 T2V 模型同时结合了高分辨率 T2I 模型和低分辨率 T2V 模型的权重进行初始化。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_364091ab11c249c58ab4b1431c129cb4@46958_oswg121987oswg1080oswg441_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <h2><strong>数据</strong></h2>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_4d676448f7504c8eb13c0e50717229e9@46958_oswg330126oswg1080oswg500_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p><strong>视频预处理和特征提取细节&nbsp;</strong></p>
  <p>为了确保高质量的输入数据，我们首先解决了原始视频中不一致的动作以及诸如切换和渐变之类的不必要过渡问题。利用 PySceneDetect，我们对视频帧进行分析，识别并分割出包含突兀过渡或渐变的场景。这一过程剔除了不一致的片段，确保视频片段在视觉上保持一致性，从而减少伪影并提升整体质量。随后，我们提取了一系列初始特征用于后续筛选，包括运动分数、美学分数、文本区域、帧高度、帧宽度、清晰度分数、时间一致性以及视频方向等。&nbsp;</p>
  <p><strong>视频字幕生成与分类细节&nbsp;</strong></p>
  <p>视频 - 文本对在训练文本到视频生成模型中起着至关重要的作用。然而，许多视频数据集缺乏高质量的对齐字幕，并且通常包含噪声或不相关内容。为此，我们在数据处理流程中引入了一个额外的视频字幕生成模块，用于生成全面的文本描述。&nbsp;</p>
  <p>我们主要探索了两种方向：(1) 抽样少量帧，应用图像字幕生成器生成字幕后，再使用大型语言模型（LLM）对生成的字幕进行总结；(2) 直接使用视频专用的 LLM 生成字幕。&nbsp;</p>
  <p>在初步尝试了第一种方法后，我们发现两个主要局限性：一是图像字幕生成器只能捕捉单帧的视觉细节，导致缺乏对视频动作的描述；二是 LLM 在基于多帧字幕生成密集描述时可能会出现虚构现象（hallucination）。&nbsp;</p>
  <p>近期研究使用 GPT 家族模型创建微调数据集并训练视频 LLM。为了在大规模字幕生成中平衡质量和成本，我们选择了一种高效的视频字幕生成器。随后，我们使用 LLM 对生成的字幕进行分类，并统计视频的类别分布。&nbsp;</p>
  <p><strong>DSG-Video: 虚构检测评估&nbsp;</strong></p>
  <p>为了比较不同字幕生成技术，我们开发了一个评估模块，用于评估字幕的丰富度和准确性。&nbsp;</p>
  <p>我们通过测量字幕中提及的唯一对象的多样性来量化字幕的丰富度，并通过检测虚构对象来评估准确性。&nbsp;</p>
  <p>受文本到图像评估方法的启发，我们提出了 DSG-Video ，用于验证字幕中提到的对象是否真实出现在视频内容中。&nbsp;</p>
  <p>1. 首先，我们利用 LLM 自动生成针对字幕关键细节的问题，例如对象的身份、动作和上下文。&nbsp;</p>
  <p>举例来说，给定一段提到 “沙发上坐着一只猫” 的字幕，LLM 会生成问题，比如 “视频中是否有一只猫？” 以及 “猫是否在沙发上？”&nbsp;</p>
  <p>2. 然后，我们使用多模态 LLM 回答这些对象验证问题，通过评估视频中多个均匀采样帧的每个参考对象的存在情况。&nbsp;</p>
  <p>对于每个生成的问题（例如，“该帧中是否有猫？”），多模态 LLM 检查每个采样帧并提供响应。如果对于某个问题，所有帧的响应都表明对象不存在，则我们将其分类为虚构对象。&nbsp;</p>
  <p>这一方法确保了对视频中每个对象的逐帧验证。基于此，我们定义了两个评估指标：&nbsp;</p>
  <p>DSG-Video_i：虚构对象实例的比例（即提到的所有对象中被检测为虚构的比例）；</p>
  <p>DSG-Video_s：包含虚构对象的句子的比例（即所有句子中含虚构对象的比例）。</p>
  <h2><strong>结果</strong></h2>
  <p>基于上述研究，我们将 T2V 和 STIV 模型从 600M 参数扩展到 8.7B。&nbsp;</p>
  <p>主要结果展示在表格中，与最新的开源和闭源模型对比后，证明了我们方法的有效性。具体而言，我们基于 Panda-70M 数据集中的 20,000 条经过筛选的视频，使用预训练的视频生成模型进行了微调（SFT）。在预训练阶段采用了 MaskDiT 技术后，我们尝试对模型进行无掩码方式的微调（UnmaskSFT）。此外，我们还对 STIV 模型进行了时间插值微调，以提升生成视频的运动平滑度（+TUP）。&nbsp;</p>
  <p><strong>T2V 性能</strong></p>
  <p>表格列出了不同 T2V 模型在 VBench 上的对比结果，包括 VBench-Quality、VBench-Semantic 和 VBench-Total 分数。分析表明，扩展 T2V 模型的参数能够提升语义理解能力。具体来说，当模型从 XL 增加到 XXL 和 M 时（三种模型尺度），VBench-Semantic 分数从 72.5 提升到 72.7，最终达到 74.8。这表明更大的模型在捕获语义信息方面表现更好。然而，对于视频质量的影响相对有限，VBench-Quality 仅从 80.7 提升至 82.1。这一发现表明，模型参数扩展对语义能力的提升大于对视频质量的影响。此外，将空间分辨率从 256 提升到 512 时，VBench-Semantic 分数显著提高，从 74.8 上升到 77.0。&nbsp;</p>
  <p><strong>SFT 的影响</strong></p>
  <p>通过高质量的 SFT 数据微调模型，可以显著提升 VBench-Quality 分数，从 82.2 提升到 83.9。在无掩码条件下对模型进行微调时，语义分数略有提升。我们的最佳模型实现了 79.5 的 VBench-Semantic 分数，超越了 KLING、PIKA 和 Gen-3 等领先的闭源模型。结合时间插值技术后，我们的模型在质量评分方面超越了所有其他模型，达到了最新的行业标准。&nbsp;</p>
  <p><strong>TI2V 性能</strong></p>
  <p>如表中所示，我们的模型在与最新方法的对比中表现出色。分析表明，尽管模型参数扩展提升了 I2V 分数，但对质量的影响较小。相比之下，提高分辨率能够显著改善质量和 I2V 分数。这一趋势表明，分辨率的提高对于提升多任务生成能力尤为关键。完整的分解维度结果见文章附录。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_6042e6e025424d088ca2174a6da7fc23@46958_oswg298002oswg1080oswg823_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_6966a6abaf344a8e830f5113b4b20898@46958_oswg151678oswg1080oswg459_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <h2><strong>应用</strong></h2>
  <p><strong>视频预测&nbsp;</strong></p>
  <p>我们从 STIV-XXL 模型出发，训练一个以前四帧为条件的文本 - 视频到视频模型（STIV-V2V）。实验结果表明，在 MSRVTT 测试集和 MovieGen Bench 上，视频到视频模型的 FVD 分数显著低于文本到视频模型。这表明视频到视频模型在生成高保真和一致性视频帧方面表现出色，尤其适用于自动驾驶和嵌入式 AI 等需要高质量生成的领域。&nbsp;</p>
  <p><strong>帧插值</strong></p>
  <p>我们提出了 STIV-TUP，一个时间插值模型，以 STIV-XL 为初始模型，并在具有时间间隔的连续帧上进行训练，同时添加文本条件。实验表明，STIV 可以在文本和图像条件下进行高质量的帧插值，并且在 MSRVTT 测试集中，使用文本条件稍微优于其他条件。此外，我们将时间插值器与主模型级联，发现这种方法能够提升生成质量，同时保持其他指标稳定。&nbsp;</p>
  <p><strong>多视角生成</strong></p>
  <p>多视角生成旨在从给定的输入图像创建新视角。这项任务对视角一致性要求较高，依赖于良好预训练的视频生成模型。通过将视频生成模型适配为多视角生成，我们可以验证预训练是否有效捕获了 3D 信息，从而提升生成效果。&nbsp;</p>
  <p>我们使用某些新视角相机的定义，并以初始帧为给定图像，预测接下来的新视角帧。通过训练一个 TI2V 模型并调整分辨率和训练步数，我们实现了与现有方法相当的表现，同时验证了我们的时空注意力机制在保持 3D 一致性方面的有效性。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_ef33bbccae4145e3b9da0b85ee249781@46958_oswg488897oswg1080oswg1324_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p><strong>长视频生成</strong></p>
  <p>我们开发了一种高效生成长视频的分层框架，包括两种模式的训练：(1) 关键帧预测，学习以较大时间间隔采样的帧；(2) 插值帧生成，通过学习连续帧，并将首尾帧作为条件。在采样阶段，首先使用关键帧预测模式生成关键帧，再通过插值模式生成中间帧，从而实现长视频生成。&nbsp;</p>
  <p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20241212/v2_3c46dd30706c4db596f1ddd3556259ca@46958_oswg1128548oswg1080oswg1121_img_000?x-oss-process=image/format,jpg/interlace,1" /></p>
  <p>更多关于模型结构、图像条件融合方法，训练策略的各种消融实验以及其他研究细节，请参考原论文。&nbsp;</p>
  <p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/6mbe80LmzkH-5eGgIys6PQ" rel="noopener noreferrer nofollow" target="_blank">“机器之心”</a>，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>