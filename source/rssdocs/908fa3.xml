<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>36氪 - 最新资讯频道</title>
        <link>https://www.36kr.com/information/web_news</link>
        
        <item>
            <id>https://www.36kr.com/p/2575585210230408</id>
            <title>腾讯这篇论文，暴露了它想操控所有APP的野心</title>
            <link>https://www.36kr.com/p/2575585210230408</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575585210230408</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:36:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI加持, Chatbot, 生图, 扩图应用
<br>
<br>
总结: 腾讯研究团队最新论文提出了一种可能性，即一个能操作所有APP的应用。这个应用可以根据用户的需求自动打开相关的手机App，并完成相应的任务。这个应用可以帮助用户完成各种任务，比如订闹钟、发微信消息、点外卖、购物、叫车等。这个应用可以被视为比微信更强大的超级流量入口。实现这一操作的关键在于AppAgent可以模仿人类玩手机的方式，通过图形界面来操作App。此外，AppAgent还可以通过自学习来适应市面上出现的新App。为了识别不同的App和页面按钮，AppAgent使用了GPT-4 Vision技术，该技术可以理解用户发送的图片。AppAgent会根据用户设定的目标，观察环境、思考实现目标的方式，并采取相应的行动。最后，AppAgent会对行动的结果进行复盘，并传递给下一个步骤。 </div>
                        <hr>
                    
                    <p>虽然现在各种AI加持下的Chatbot、生图、扩图应用层出不穷，但总感觉好像并没有给日常生活带来什么变化。</p><p>回想当年互联网技术从网页到移动端的进化，支付宝取代了现金、滴滴重塑了出行、美团改写了用餐方式...那么AI技术会带来什么样的变革？</p><p>腾讯研究团队的一篇最新论文带来了一种可能：一个帮你操作所有APP的应用。</p><p>只要告诉这个应用你想做什么，它就会自动打开相关的手机App，直接把事儿给你办了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_e406aa43743543fcb45f5d932d099823@5888275_oswg31624oswg1080oswg273_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>比如帮你订个工作日起床闹钟、给领导发个微信消息、上美团点杯奶茶、去拼多多买一箱最便宜的纸巾、让滴滴叫一辆去机场的车、或者把刚拍的照片P好看一下然后直接发到朋友圈，顺手配一段伤感的文字。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7c40553015844d6598f0df9df0834cbe@5888275_oswg471929oswg744oswg907_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>有了这个APPAgent就再也不用去一页页找各种APP了，未来需要咱们手动点开的可能就这一个APP。</p><p>这是什么？<strong>这不就是个比微信更强大的超级流量入口嘛！</strong></p><h2><strong>01</strong></h2><p>具体如何实现这一操作的，APPAgent分为几个步骤。</p><p>首先是如何保证能够“调用”所有应用，比如说Siri能够访问“闹钟”应用，就是从系统上植入了访问接口，而市面上的APP几乎无穷无尽，不可能每一个开发者都给Siri搞一个接头暗号。</p><p>AppAgent第一个厉害之处就来了，它不需要开发者做任何改动，可以直接模仿人类玩手机的方式，通过图形界面来<strong>操作</strong>App。</p><p>也就是说，它把人类使用APP的过程拆解成了几个具体动作，比如滑动、点击、长按、输入等。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_3c6ede57c63f40199470ca5a1ab4bb21@5888275_oswg108703oswg510oswg789_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>也就是说它在AI的加持下，自行学习人类使用APP的过程，从而达到模拟的效果。</p><p>这意味着无论市面上出现了什么新的App，只要给AppAgent一点时间自己摸索一下，它就能自己熟练使用了。</p><p>在这个过程中，最大的问题是AI如何识别这千千万万种不同的APP、不同的页面按钮分别是做什么的。</p><p>AppAgent要玩手机，首先要看得见手机屏幕。传统的方式是把常见的UI界面截图下来，人工打上标记，比如这个是输入框，那个是返回按钮，黄色袋鼠是美团，企鹅是QQ之类的。</p><p>然后通过这些数据训练出来一个能识别常见UI元素的视觉模型。</p><p>这个问题GPTV其实已经有解决方案了，GPTV其实是GPT-4 Vision的一个缩写，今年11月跟着GPT-4的上线一起来到了大众的视野。</p><p>GPT-4 Vision能做到的事简单来说，就是可以看懂用户发给它的图片。</p><p>比如像下面这样上传一张过马路的照片，然后问GPT-4照片里有多少人。它的回答是有137人，还很谦虚的说我是大概估算了一下，有些地方太暗还有些人被挡住了，所以我可能数得不准。[2]</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_1cdad5e99dd04e2580a786b34ea3c051@5888275_oswg331396oswg762oswg531_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>大家一拥而上搞出了许多花样，包括但不限于：足球比赛解说、实时解读摄像头里出现的物品、根据手绘草图在电脑上帮你画图等等。[3]</p><p>大家一致评价：识别速度快、准确性也高，不过缺点也不是没有，太贵。比如做一个13秒的足球视频解说，就要花费约30美元。</p><p>但是能用钱解决的问题，都不是问题。</p><h2><strong>02</strong></h2><p>第二步，<strong>APPAgent会根据人类设定的目标，自我思考并拆解这个目标需要哪些过程。</strong></p><p>收到一个请求后，AppAgent首先做的事情是<strong>观察</strong>环境，也就是上面提到的“看图说话”。</p><p>然后再<strong>思考</strong>怎么根据现状去实现最终目标，得出结论之后就<strong>行动</strong>。</p><p>最后根据行动之后，带来了哪些改变，做一个<strong>复盘</strong>，然后把这个复盘传递给下一个步骤。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7e9deee402534168bce5f03747ec590d@5888275_oswg310213oswg1080oswg579_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>比如说它点到了一个广告，就会发现和主要任务无关，并将这一结果的复盘向下传递，并得出结论不要点这个页面。</p><p>AppAgent在训练的时候强调了以目标为导向的逻辑，如果进入了与目标不相关的页面就返回上一页。并且还加入了现有大模型中有关用户界面的知识和人类操作的演示。</p><blockquote><p>效果好得惊人，团队在9个APP上总共测试了45个任务，AppAgent在10个步骤内就能成功的概率高达84.4%，而且平均下来只需要5.1个步骤就可以完成。</p></blockquote><p>这样每一步下去都会越来越接近目标。</p><p>这是执行任务的过程拆解，而在具体的操作上，如何让AI识别各个功能按键又是个大问题。</p><p>为了提升AppAgent在操作上的准确度和效率，团队在两件事儿上下了功夫。</p><p>首先，他们先简化了手机界面的坐标，根据前端的XML文件给每个UI元素指定了一个唯一编号。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_c943ef14e5f548f09db8c1e1bdada36b@5888275_oswg218525oswg1080oswg818_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>然后，简化了手机的交互操作，规定了以下6种操作：轻点、长按、滑动、输入文字、返回和退出。</p><p>当AppAgent思考好了行动计划以后，只要根据<strong>操作+位置编号</strong>来执行就可以了。</p><p>像上图里的“点击（3）”，意思就是执行“点击”这个操作，点的位置是编号为3的区域，也就是邮件发送按钮。</p><p>这种操作方式极大地提高了AppAgent操作的准确率。</p><blockquote><p>在过去，直接用GPT-4来操作手机完成任务，成功率仅有2.2%，而还让GPT-4来指挥，但是执行端换成上面这种操作方式之后，成功率就提升到了48.9%。</p></blockquote><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_dc5ae61bcc534da1a08eba8be8f49b59@5888275_oswg51911oswg1080oswg160_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>实际上，这篇论文提供的是一种训练AI操作APP的解决方案，其突破性的创意在于改变了智能体实施操作的学习过程。</p><p><strong>既互动方式模拟人类、识别UI靠数据标记、执行操作靠位置编号。</strong></p><p>就当下来看，这是智能代理技术最为先进的解决方案，让明年AI Agents的普及成为了一种可能。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_56f63d8b44ef4298854b36e3307f372e@5888275_oswg78689oswg1080oswg158_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这技术普及还有另一种问题：以后水军刷评论更方便了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_719f108362de402ca338cc00cb3d7279@5888275_oswg78727oswg1080oswg208_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p><strong>参考文章：</strong></p><p>[1] AppAgent: Multimodal Agents as Smartphone Users｜Tencent&nbsp;</p><p>[2] OpenAI's GPT-4 Vision explained: Transforming AI with Visual Capabilities | Encord&nbsp;</p><p>[3] 解说梅西球赛、英雄联盟，OpenAI GPT-4视觉API被开发者玩出新花样 | 机器之心&nbsp;</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/afzIIUXuM5-Xv6qvUIPmCQ" rel="noopener noreferrer nofollow" target="_blank">“新硅NewGeek”（ID:gh_b2beba60958f）</a>，作者：刘白，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2575583737030276</id>
            <title>史上最快3D数字人生成器：半小时完成训练，渲染仅需16毫秒，苹果出品</title>
            <link>https://www.36kr.com/p/2575583737030276</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575583737030276</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:35:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字人, HUGS, 视频合成, 帧率
<br>
<br>
总结: 苹果联合德国马普推出的HUGS是一种基于高斯函数的3D数字人合成工具，可以从简单的视频中提取人物骨骼并合成数字分身，实现流畅的动作和高帧率的视频合成。HUGS在细节刻画上比其他前沿技术更清晰细腻，同时在测试数据上也表现出了优异的性能，速度方面也有显著提升。 </div>
                        <hr>
                    
                    <p>之前要两天才能训练好的数字人，现在<strong>只用半小时</strong>就能完成了！</p><p>到了<strong>推理阶段，更是只要16毫秒</strong>，就能得到动作流畅、细节到位的场景视频。</p><p>而且无需复杂的采样和建模，只要随便拍一段50-100帧的视频就足够了，换算成时间不过几秒钟。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_6e952ee74cfa4bb29ce9225821f6f912@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这正是由苹果联合德国马普所推出的，基于高斯函数的3D数字人合成工具<strong>HUGS</strong>。</p><p>它可以从一段简单的视频当中提取出人物骨骼，从而合成数字分身并驱动它做出任意动作。</p><p>这个数字人可以丝滑地融合到其他场景，甚至<strong>帧率还能超越原始素材</strong>，达到60FPS。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7bedc35346bc4976abb98d1d3cfe0988@5888275_oswg538466oswg1080oswg545_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>Hugging Face的“首席羊驼官”Omar Sanseviero看到后，也给HUGS送上了hug。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_4e9f574e4dd04f92a838fbcfa030603e@5888275_oswg192159oswg1080oswg503_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>那么，HUGS可以实现怎样的效果呢？</p><h2><strong>01 100倍速生成60FPS视频</strong></h2><p>从下面这张动图可以看出，新生成的数字人可以在不同于训练素材的场景中做出不同的动作。</p><p>而新合成的画面也比原始素材更加流畅——尽管原素材只有24FPS，但HUGS合成的视频帧率达到了60FPS。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_328f10be8c98415ebba4eb496efdcab3@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>同时，HUGS也支持把多个人物融合进同一个场景。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_8f9447f544a146c2af8178438a9a4b76@5888275_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>细节刻画上，HUGS也比Neuman和Vid2Avatar这两个前SOTA更清晰细腻，也更加真实。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_44cd4c14065f48b684618a7ddc9bffa5@5888275_oswg1133993oswg1080oswg788_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>如果放到规范空间中，Neuman和HUGS的细节对比将变得更加明显。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_7702fef0affa4693be0c778e45c1986b@5888275_oswg255140oswg856oswg578_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>测试数据上看，HUGS在NeuMan数据集的五个场景中的PSNR和SSIM评分都达到了SOTA水平，LPIPS误差则处于最低位。</p><p>在ZJU Mocap数据集上，针对5个不同受试者，HUGS也都超越了NerualBody、HumanNeRF等Baseline方法。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_0e3f9beed97b4475984b96daf3782ee9@5888275_oswg364142oswg1080oswg629_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>速度方面，HUGS的训练只需半小时就能完成，而此前最快的VidAvtar也要48小时，速度提升了近百倍。</p><p>渲染速度也是如此，用Baseline方法进行渲染需要2-4分钟，但HUGS只用16.6毫秒就能完成，比人眨眼的速度还快。（下图为对数坐标系）</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_cb9952be310b4a2daf5e954830c9ca91@5888275_oswg71900oswg864oswg476_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>那么，HUGS是如何实现既迅速又细腻地生成3D数字人的呢？</p><h2><strong>02 像搭积木一样渲染</strong></h2><p>HUGS首先将人物和场景分别转化为<strong>3D高斯斑点</strong>。</p><p>其中，人物部分的高斯斑点由三个<strong>多层感知机</strong>（MLP）来预测，并通过<strong>SMPL</strong>（一种人体形状模型）进行初始化。</p><p>SMPL可以用极少的参数建立实体人物到三维网格的映射，只需要10个主要参数就可以表示99%的人体形状变化。</p><p>同时，为了刻画头发和衣服等细节，HUGS也允许高斯函数在一定程度上偏离SMPL。</p><p>场景的高斯斑点通过特征三平面提供的位置编码，由多个MLP预测得到。</p><p>得到人体和场景模型的高斯斑点后，研究者对它们进行了<strong>联合优化</strong>。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_36b6a64aea8e47d3bf1d273568997f93@5888275_oswg419837oswg1080oswg657_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>得到的高斯斑点还会被进行克隆和拆分，从而增大斑点密度，不断接近真实的目标几何表面，这一过程称为<strong>Densify</strong>。</p><p>此外，研究人员还引入了<strong>线性混合动画</strong>（LBS）技术，在运动过程中对高斯斑点进行驱动。</p><p>转换为高斯斑点形式后，研究人员训练了神经网络对高斯函数的属性进行预测，形成真实的人体形状。</p><p>同时，神经网络还定义了高斯函数与人体骨骼的绑定关系，从而实现人物的运动。</p><p>这样，HUGS的渲染过程就像搭积木一样，不需要重新调用神经网络，从而实现了高速渲染。</p><p>消融实验结果表明，LBS、Densify和三平面MLP都是HUGS中的重要环节，缺少任何一个都会对合成效果造成影响。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_1bfa967cb4a94f1f955d7a63364ac292@5888275_oswg428596oswg1080oswg690_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>而人物与场景的联合优化，同样是实现刚好融合效果的关键因素。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_a74a05f39cf841a0bb6948b76fbfbfae@5888275_oswg1217071oswg1080oswg1092_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2><strong>03 One More Thing</strong></h2><p>苹果产生研究数字人的想法已经有一段时间了。</p><p>在苹果MR头显Apple Vision Pro中，就出现过高细节版本的数字分身概念——</p><p>在FaceTime通话时，头显可以创建一个“数字人”，并用它来代表用户。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_803754ad3e844fef85f9ec903c384a40@5888275_oswg303597oswg768oswg432_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>那么，对苹果的这个“数字人生成器”，你怎么看呢？</p><p>参考链接：</p><p>[1]https://appleinsider.com/articles/23/12/19/apple-isnt-standing-still-on-generative-ai-and-making-human-models-dance-is-proof[2]https://twitter.com/anuragranj/status/1737173861756485875/</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/hxSzFBAHcPWAEYKjoszwDA" rel="noopener noreferrer nofollow" target="_blank">“量子位”（ID:QbitAI）</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.36kr.com/p/2575584351954562</id>
            <title>大模型被偷家，腾讯港中文新研究修正认知：CNN搞多模态不弱于Transfromer</title>
            <link>https://www.36kr.com/p/2575584351954562</link>
            <guid isPermaLink="false">https://www.36kr.com/p/2575584351954562</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:19:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Transformer, CNN, UniRepLKNet, 大核CNN架构设计
<br>
<br>
总结: 在Transformer占据多模态工具半壁江山的时代，大核CNN又“杀了回来”，成为了一匹新的黑马。腾讯AI实验室与港中文联合团队提出了一种新的CNN架构UniRepLKNet，图像识别精度和速度都超过了Transformer架构模型。切换到其他模态也无需改变模型结构，简单预处理即可接近甚至超越SOTA。这一研究修正了“Transformer在图像任务上吊打CNN”的认知，认为CNN在大一统这一点上可能不弱于Transformer。UniRepLKNet的设计遵循了大卷积核CNN架构的四条guideline，通过解耦感受野、特征抽象层次和模型深度来提高模型的表征能力。 </div>
                        <hr>
                    
                    <p>在Transformer占据多模态工具半壁江山的时代，大核CNN又“杀了回来”，成为了一匹新的黑马。</p><p>腾讯AI实验室与港中文联合团队提出了一种新的CNN架构，图像识别精度和速度都超过了Transformer架构模型。</p><p>切换到点云、音频、视频等其他模态，也无需改变模型结构，简单预处理即可接近甚至超越SOTA。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_450f620a7caa400d8e351d04969a91af@5888275_oswg125398oswg1080oswg265_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>团队提出了专门用于大核CNN架构设计的<strong>四条guideline</strong>和一种名为<strong>UniRepLKNet</strong>的强力backbone。</p><p>只要用ImageNet-22K对其进行预训练，精度和速度就都能成为SOTA——</p><p>ImageNet达到88%，COCO达到56.4 box AP，ADE20K达到55.6 mIoU，实际测速优势很大。</p><p>在时序预测的超大数据上使用UniRepLKNet，也能达到最佳水平——</p><p>例如在全球气温和风速预测上，它就超越了Nature子刊上基于Transformer的前SOTA。</p><p>更多细节，我们接着看作者投稿。</p><h2><strong>01 “Transformer时代”，为什么还需要CNN</strong></h2><p>在正式介绍UniRepLKNet的原理之前，作者首先解答了两个问题。</p><p>第一个问题是，<strong>为什么在Transformer大一统各个模态的时代还要研究CNN</strong>？</p><p>作者认为，Transformer和CNN只不过是相互交融的两种结构设计思路罢了，<strong>没有理由认为前者具有本质的优越性</strong>。</p><p>“Transformer大一统各个模态”正是研究团队试图修正的认知。</p><p>正如2022年初ConvNeXt、RepLKNet和另外一些工作问世之前，“Transformer在图像任务上吊打CNN”是主流认知。</p><p>这几项成果出现后，这一认知被修正为“CNN和Transformer在图像任务上差不多”。</p><p>本研究团队的成果将其进一步修正：在点云、音频、视频上，CNN比我们想象的强太多了。</p><p>在时序预测这种并不是CNN传统强项的领域（LSTM等曾是主流，最近两年Transformer越来越多），CNN都能超过Transformer，成功将其“偷家”。</p><p>因此，研究团队认为，<strong>CNN在大一统这一点上可能不弱于Transformer</strong>。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_3237843804b5460b9bc5d9017fb2de1a@5888275_oswg75475oswg1080oswg568_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>第二个问题是，<strong>如何将一个为图像任务设计的CNN用于音频、视频、点云、时序数据</strong>？</p><p>出于对简洁和通用性的永恒追求，将UniRepLKNet用于其他模态时，不对模型架构主体做任何改变（以下实验用的全都是UniRepLKNet-Small）。</p><p>只需要将视频、音频、点云、时序数据给处理成C×H×W的embedding map（对于图像来说，C=3），就能实现到其他模态的过渡，例如：</p><p>把音频的频谱图（Time×Frequency）看成是一幅单通道图像，即C=1，H=T，W=F；</p><p>将点云进行三视图投影，得到三幅单通道图像，C=3，H和W可以随意指定；</p><p>将视频中的各帧拼接到一起，极为简单地得到一张大图（例如，16帧的3×224×224视频拼接得到3×896×896的输入）；</p><p>对时序数据，借鉴CorrFormer中的embedding layer将数据转换为隐空间中的张量然后简单粗暴地将其reshape成一幅单通道图像。</p><p>后文展示的结果将会证明，如此简单的设计产生的效果是极为优秀的。</p><h2><strong>02 大卷积核CNN架构设计</strong></h2><p>2022年，RepLKNet提出了用超大卷积核（从13×13到31×31）来构建现代CNN以及正确使用超大卷积核的几个设计原则。</p><p>但从架构层面看，RepLKNet只是简单地用了Swin Transformer的整体架构，并没有做什么改动。</p><p>当前大核CNN架构设计<strong>要么遵循现有的CNN设计原则，要么遵循现有的Transformer设计原则</strong>。</p><p>在传统的卷积网络架构设计中，当研究者向网络中添加一个3×3或5×5卷积层时，往往会期望它同时产生三个作用：</p><p><strong>增大感受野</strong></p><p><strong>提高抽象层次</strong>，例如从线条到纹理、从纹理到物体的局部</p><p>通过增加深度而一般地<strong>提高表征能力</strong>（越深，参数越多，非线性越多，拟合能力越高）</p><p>那么，设计大卷积核CNN架构时，应该遵循怎样的原则呢？</p><p>本文指出，应该<strong>解耦上述三种要素，需要什么效果就用对应的结构来实现</strong>：</p><p>用少量大卷积核保证大感受野。</p><p>用depthwise 3×3等小卷积提高特征抽象层次。</p><p>用一些高效结构（如SE Block、Bottleneck structure等）来提高模型的深度从而增强其一般的表示能力。</p><p>这样的解耦之所以能够实现，正是大卷积核的本质优势所保证的，即不依赖深度堆叠的大感受野。</p><p>经过系统研究，本文提出了大卷积核CNN设计的四条Architectural Guidelines。</p><p>根据这些guideline，本文提出的UniRepLKNet模型结构如下——</p><p>每个block主要由depthwise conv、SE Block和FFN三个部分组成。</p><p>其中depthwise conv可以是大卷积核（图中所示的Dilated Reparam Block，其使用膨胀卷积来辅助大核卷积来捕捉稀疏的特征而且可以通过结构重参数化方法等价转换为一个卷积层），也可以只是depthwise 3x3。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_d622952b5b9f45728c49227ba37581e8@5888275_oswg210132oswg949oswg923_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2><strong>03 多项表现超越Transformer</strong></h2><p>作为图像模态中的老三样，ImageNet、COCO、ADE20K上的结果自然是不能少。论文中最多只用ImageNet-22K预训练，没有用更大的数据。</p><p>虽然大核CNN本来不是很重视ImageNet（因为图像分类任务对表征能力和感受野的要求不高，发挥不出大kernel的潜力），但UniRepLKNet还是超过了最新的诸多模型，其实际测速的结果尤为喜人。</p><p>例如，UniRepLKNet-XL的ImageNet精度达到88%，而且实际速度是DeiT III-L的三倍。量级较小的UniRepLKNet相对于FastViT等专门设计的轻量级模型的优势也非常明显。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_41577a7735a142ec9751ce8c21990f1e@5888275_oswg227517oswg672oswg629_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>在COCO目标检测任务上，UniRepLKNet最强大的竞争者是InternImage：</p><p>UniRepLKNet-L在COCO上不及InternImage-L，但是UniRepLKnet-XL超过了InternImage-XL。</p><p>考虑到InternImage团队在目标检测领域的积淀非常深厚，这一效果也算很不容易了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_81ed899c9aef422b8b31e8cbc89cfb33@5888275_oswg109080oswg675oswg359_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>在ADE20K语义分割上，UniRepLKNet的优势相当显著，最高达到55.6的mIoU。与ConvNeXt-XL相比超出了整整1.6。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_6aaab7c568d346768768b9500b1f123d@5888275_oswg143691oswg673oswg467_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>为了验证UniRepLKNet处理时序数据的能力，本文挑战了一个数据规模超大的《Nature》级别的任务：全球气温和风速预测。</p><p>尽管UniRepLKNet本来是为面向图像任务设计的，它却能超过为这个任务而设计的CorrFormer（前SOTA）。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231225/v2_3fea7cc3b23f4c25b7605003fa341017@5888275_oswg252149oswg951oswg728_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这一发现尤为有趣，因为这种超大规模时间序列预测任务听起来更适合LSTM、GNN和Transformer，这次CNN却将其“偷家”了。</p><p>在音频、视频和点云任务上，本文的极简处理方法也都十分有效。</p><h2><strong>03 One More Thing</strong></h2><p>除了提出一种在图像上非常强力的backbone之外，本文所报告的这些发现似乎表明，大核CNN的潜力还没有得到完全开发。</p><p>即便在Transformer的理论强项——“大一统建模能力”上，大核CNN也比我们所想象的更为强大。</p><p>本文也报告了相关的证据：将kernel size从13减为11，这四个模态上的性能都发生了显著降低。</p><p>此外，作者已经放出了所有代码，并将所有模型和实验脚本开源。</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/J4O7Y10dl2BzujobPmY5Ug" rel="noopener noreferrer nofollow" target="_blank">“量子位”（ID:QbitAI）</a>，作者：关注前沿科技，36氪经授权发布。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>