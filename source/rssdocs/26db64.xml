<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Will / @FinanceYF5</title>
        <link>https://nitter.cz/FinanceYF5</link>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727644170339426395#m</id>
            <title>马斯克太牛了

原版 @Tesla Roadster 的所有设计和工程现已完全开源。</title>
            <link>https://nitter.cz/FinanceYF5/status/1727644170339426395#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727644170339426395#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 11:04:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克太牛了<br />
<br />
原版 <a href="https://nitter.cz/Tesla" title="Tesla">@Tesla</a> Roadster 的所有设计和工程现已完全开源。</p>
<p><a href="https://nitter.cz/elonmusk/status/1727392569238159491#m">nitter.cz/elonmusk/status/1727392569238159491#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727555715282378978#m</id>
            <title>R to @FinanceYF5: 补充一个其他企业 PS</title>
            <link>https://nitter.cz/FinanceYF5/status/1727555715282378978#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727555715282378978#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 05:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补充一个其他企业 PS</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tQ3doZ2JzQUFMWXpKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727543580066185326#m</id>
            <title>R to @FinanceYF5: 补充一个英伟达Q3收入，还是赚钱
181亿总收入，145亿数据中心收入，净利润92亿。</title>
            <link>https://nitter.cz/FinanceYF5/status/1727543580066185326#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727543580066185326#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:24:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补充一个英伟达Q3收入，还是赚钱<br />
181亿总收入，145亿数据中心收入，净利润92亿。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sM2d6Q2FNQUF2VlZ0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727541205893058726#m</id>
            <title>R to @FinanceYF5: 数据来自：Similarweb</title>
            <link>https://nitter.cz/FinanceYF5/status/1727541205893058726#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727541205893058726#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:14:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>数据来自：Similarweb</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sMWNOWWFNQUFSRkVELmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sMWV1eGJNQUVkVldnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sMWdEZGJVQUFrRkU4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727540980465950953#m</id>
            <title>R to @FinanceYF5: 测算结果</title>
            <link>https://nitter.cz/FinanceYF5/status/1727540980465950953#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727540980465950953#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:14:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测算结果</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sMVdQTmJzQUFFcnFILnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727540849247244369#m</id>
            <title>更新了一下截止11.20的gpts平台流量

下面附带了具体社交流量和外链导航的测算：

1、PH给470票的带去了2万visit（20%），给第二名Hunter带去了0.3万Visit（1%）
2、推特给Rundown带去了4.5万Visit（21%）。给GPTshunter带去了1.4万Visit（5%）</title>
            <link>https://nitter.cz/FinanceYF5/status/1727540849247244369#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727540849247244369#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 04:13:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>更新了一下截止11.20的gpts平台流量<br />
<br />
下面附带了具体社交流量和外链导航的测算：<br />
<br />
1、PH给470票的带去了2万visit（20%），给第二名Hunter带去了0.3万Visit（1%）<br />
2、推特给Rundown带去了4.5万Visit（21%）。给GPTshunter带去了1.4万Visit（5%）</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sdDJyWGJFQUFKWFNfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727513810339246524#m</id>
            <title>R to @FinanceYF5: 做了一个中文版</title>
            <link>https://nitter.cz/FinanceYF5/status/1727513810339246524#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727513810339246524#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:26:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>做了一个中文版</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sY3BXTWJJQUFOSVFfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727509303823827258#m</id>
            <title>R to @FinanceYF5: 原帖； 在领英上，
https://www.linkedin.com/posts/kelvinmu_artificialintelligence-generativeai-startups-activity-7130222861025251328-WKPI?utm_source=share&amp;utm_medium=member_desktop</title>
            <link>https://nitter.cz/FinanceYF5/status/1727509303823827258#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727509303823827258#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:08:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原帖； 在领英上，<br />
<a href="https://www.linkedin.com/posts/kelvinmu_artificialintelligence-generativeai-startups-activity-7130222861025251328-WKPI?utm_source=share&amp;utm_medium=member_desktop">linkedin.com/posts/kelvinmu_…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sWWpSd2FzQUFMcXhJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727509185116680633#m</id>
            <title>谁在通过生成式人工智能赚钱？

查看此图，显示生成人工智能中收入/筹款资金的分布

总括：

- 尽管仅获得 13% 的风险投资，但绝大多数收入都被基础设施公司获取。

- 大部分融资都归模型和应用公司所有，尽管它们只占收入的 10%。

需要思考的一些问题：

 1️⃣ 为什么会存在这种二分法？

主要原因是 Gen AI 还处于早期阶段。类似的模式贯穿历史，基础设施领域的开发和货币化先于应用程序领域。例如，互联网基础设施先于搜索和电子商务等互联网应用。

 2️⃣ 这种模式很快就会改变吗？

我预计当前的收入分配在短期内将维持目前的模式。尽管由于起始基础较小，我们可能会观察到模型和应用程序/工具领域的一些增长，但它不太可能赶上基础设施领域。仅 Nvidia 的数据中心收入预计明年将翻一番，达到 40B 美元。此外，只要英伟达继续保持垄断地位，基础设施/芯片的价格就会保持在高位。最后，我们目前拥有的大部分计算都分配给了训练端——我们还没有看到推理端的重大扩展。出于所有这些原因，我预计在可预见的未来，基础设施仍将是主要的收入引擎。

 3️⃣ 最终状态会是什么样子？

从长远来看，我预计应用程序和建模堆栈的 TAM 将比基础设施堆栈大 2-4 倍。如果我们看看 SaaS 等更成熟的行业，毛利率通常在 70-90% 左右，其中大部分 COG 都用于云托管成本。有人可能会说，原生生成人工智能企业也需要存在类似的利润状况。

 4️⃣ 筹款会发生什么？

考虑到基础模型公司筹集的资金中有高达 80% 用于计算，我预计模型的筹款规模将与基础设施收入成正比。然而，应用程序/工具领域可能会更加波动，具体取决于公司是否能够增长到其估值。这取决于企业采用生成式人工智能解决方案的速度，而生成式人工智能解决方案迄今为止才刚刚起步。

简而言之，过去 12 个月里生成式 AI 的唯一捐助者似乎是 Nvidia。虽然这种情况不会永远持续下去，但重大转变需要时间才能实现。

来源：Kelvin Mu</title>
            <link>https://nitter.cz/FinanceYF5/status/1727509185116680633#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727509185116680633#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:07:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谁在通过生成式人工智能赚钱？<br />
<br />
查看此图，显示生成人工智能中收入/筹款资金的分布<br />
<br />
总括：<br />
<br />
- 尽管仅获得 13% 的风险投资，但绝大多数收入都被基础设施公司获取。<br />
<br />
- 大部分融资都归模型和应用公司所有，尽管它们只占收入的 10%。<br />
<br />
需要思考的一些问题：<br />
<br />
 1️⃣ 为什么会存在这种二分法？<br />
<br />
主要原因是 Gen AI 还处于早期阶段。类似的模式贯穿历史，基础设施领域的开发和货币化先于应用程序领域。例如，互联网基础设施先于搜索和电子商务等互联网应用。<br />
<br />
 2️⃣ 这种模式很快就会改变吗？<br />
<br />
我预计当前的收入分配在短期内将维持目前的模式。尽管由于起始基础较小，我们可能会观察到模型和应用程序/工具领域的一些增长，但它不太可能赶上基础设施领域。仅 Nvidia 的数据中心收入预计明年将翻一番，达到 40B 美元。此外，只要英伟达继续保持垄断地位，基础设施/芯片的价格就会保持在高位。最后，我们目前拥有的大部分计算都分配给了训练端——我们还没有看到推理端的重大扩展。出于所有这些原因，我预计在可预见的未来，基础设施仍将是主要的收入引擎。<br />
<br />
 3️⃣ 最终状态会是什么样子？<br />
<br />
从长远来看，我预计应用程序和建模堆栈的 TAM 将比基础设施堆栈大 2-4 倍。如果我们看看 SaaS 等更成熟的行业，毛利率通常在 70-90% 左右，其中大部分 COG 都用于云托管成本。有人可能会说，原生生成人工智能企业也需要存在类似的利润状况。<br />
<br />
 4️⃣ 筹款会发生什么？<br />
<br />
考虑到基础模型公司筹集的资金中有高达 80% 用于计算，我预计模型的筹款规模将与基础设施收入成正比。然而，应用程序/工具领域可能会更加波动，具体取决于公司是否能够增长到其估值。这取决于企业采用生成式人工智能解决方案的速度，而生成式人工智能解决方案迄今为止才刚刚起步。<br />
<br />
简而言之，过去 12 个月里生成式 AI 的唯一捐助者似乎是 Nvidia。虽然这种情况不会永远持续下去，但重大转变需要时间才能实现。<br />
<br />
来源：Kelvin Mu</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sWHl2bmJrQUFRc3VGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727506930078478478#m</id>
            <title>R to @FinanceYF5: 原文：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727506930078478478#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727506930078478478#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:58:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原文：</p>
<p><a href="https://nitter.cz/absoluttig/status/1722791559132008902#m">nitter.cz/absoluttig/status/1722791559132008902#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727506806841450680#m</id>
            <title>R to @FinanceYF5: 如果云提供商有 GPU 可供出租，为什么还需要新贵呢？ Azure、AWS 和 GCP 等超大规模企业仅通过长期固定合同以“合理”价格提供 GPU——例如AWS 将以 110 万美元的价格向您租用 8 台 H100，为期 3 年（如果您不提前预订，价格会翻倍！）。较小的公司有更多零星的需求，并且正在寻找例如1,000 个 GPU 1 个月，然后 200 个 GPU 3 个月，然后 500 个 GPU 5 个月，依此类推。

超大规模初创公司 CoreWeave、Lambda、ML Foundry 和 Together 正在努力通过提供对 GPU 的简单、可变、租用（“无服务器”）访问来弥补这一差距。这些企业今年的需求近乎无限，并且其硬件设置的回报非常快。这些企业与上一个周期的加密货币矿工具有相似的风险状况：你必须在 GPU 交易上进行杠杆押注，就像加密货币矿工在加密货币上进行杠杆押注一样。

在需求方面，我们有充分的理由相信对可变 GPU 的需求将会持续下去。如果您是一家中小型企业（例如筹集了 0-1 亿美元）并且想要训练自己的尖端基础模型，您可能需要 5,000 台 Nvidia H100 – 您无法以每台 3 万美元以上的价格自行购买和安装它们设备，即使可以，您也不会经常运行它们以达到收支平衡。使用第三方经销商是有意义的。

AWS 今年夏天宣布推出 20,000 个 H100 GPU 集群，一旦完成，该集群应该可以满足更大规模的 ML 训练运行的需求。他们利用供应短缺的机会，将营业利润率隐含地定价为 68%。但从长远来看，AWS 和 Azure 并不想成为 GPU 租赁企业——他们希望以高利润销售更广泛的服务。从这个意义上说，CoreWeave 和 Lambda 如果能够经受住供需波动的考验，就能像 Rackspace 或 DigitalOcean 那样持久发展。

英伟达以高容量和优惠的价格奖励这些经销商，因为他们没有制造像微软和谷歌那样的竞争芯片。这对英伟达来说是一把双刃剑，因为支持经销商可以让超大规模厂商更有动力去竞争。

经销商抢在其他人之前，通过在线提供备受追捧的 GPU 供应而受益。他们能建立持久的企业吗？差异化的关键在于规模、性价比、供应可扩展性以及软件产品的稳健性：

1. 规模：您需要一个关键规模来实现两个关键目的：1）支持在尖端模型上运行更大型的语言模型训练（例如，一次运行 5,000 多个 2020 年老式 Nvidia A100 或尖端 H100，持续数周，2）在高度变化的客户需求中保持高利用率。

2. 性价比：如果您将这些公司视为 GPU 时间经销商，那么供应成本确实很重要。作为 Nvidia 合作伙伴享有优惠定价（即获得与 Azure、AWS 等相同的定价）可以节省 30% 的集群构建成本。像克鲁索这样的玩家正在通过更便宜的能源来降低供应成本。 CoreWeave 声称，由于集群架构、虚拟化方法、冷却技术、互连质量等，他们的 A100 的性价比比超大规模的 A100 高出 30%。Salad 等一些公司甚至试图通过寻找消费者拥有的潜在 GPU 来聚集更便宜的供应。

3. 供应可扩展性：Together 和 ML Foundry 等公司已从中期初创公司未充分利用的私有集群中“获取”（租用）潜在 GPU 供应。虽然仍处于技术研究阶段，但综合聚合 GPU 集群可以大规模解锁 GPU 方程式的供应方（类似于 Airbnb 对家庭所做的事情）。

4. 软件：一些公司正在“向上堆栈”，即在 GPU 之上改善开发人员体验。 Modal、Lambda 和 RunPod 均声称通过软件提供更快的入门和更流畅的开发人员体验。

鉴于这些企业的主要收入来源是 GPU 小时，它们本质上是对 GPU 需求扩展的杠杆赌注，以及对 H100 寿命的赌注（如果 GPU 在 3 年内过时，折旧打击将是痛苦的） ）。要对这些公司进行投资，您需要对中期 GPU 消耗扩展有强有力的方向性看法，或者相信构建更广泛的软件套件的长期影响力。

结论

无数的基础模型模仿者类似于现代的货物崇拜。他们正在尽最大努力重现第一波产品的魔力，但没有拥有所有所需的成分。

鉴于计算支出的增加似乎与智能成对数关系，很难想象计算不会呈指数级增长的未来。但不同寻常的是，我对 20 年的前景比 5 年的前景更加清晰。从中期来看，过度构建 GPU 集群的风险很高。

为了更好地了解 GPU 的中期前景，有几件事值得关注：

1. 大型基础模型需求增长：OpenAI 和其他大型 LLM 提供商在未来 3-5 年内将增加多少 GPU 支出？许多模型提供商都在复制 OpenAI 所做的一切，因此他们的 GPU 消耗应该能够高度预测整体 GPU 需求的增长。最重要的是，GPU 支出大户增加集群规模的速度是否会比需求长尾衰退的速度更快？

2. 推理与训练组合：看到用于推理的 GPU 使用量的变化将澄清该行业在未来几年是否会继续依赖 H100，或者转向更专门于推理的芯片。

3.替代品的性价比：其他人需要数年时间才能赶上Nvidia前沿芯片的绝对性能，但我会关注新半导体产品的性价比。这里的标准并不是与英伟达相对较低的销货成本，而是与他们的零售价——这就是他们的定价能力可能受到攻击的时候。

公众

英伟达的非凡增长伴随着两个充满挑战的客户群的诅咒：

1. 一群有强烈动机开发竞争性芯片项目的集中买家。

2. 大量客户将在明年运行投机性的“构建/微调您自己的基础模型”实验，其中 GPU 需求的可预测性是不可能的。

考虑到基于发布时间表的 GPU 支出的周期性，以及包含无限指数增长的估值，我预计 Nvidia 将在未来 6-9 个月内达到峰值，并在 2-3 年时间内降温。如果我看到大型基础模型的大型 H100 集群扩展计划，或者即将推出的 Office Copilot 等人工智能产品的高附加率，我会改变主意。

我认为购买 AMD 是押注 GPU 长期增长而不挤入 Nvidia 交易的好方法。他们在加速计算的早期竞赛中失败了，但有很多公司愿意花费数十亿美元激励他们取得成功，而且他们即将推出的 MI 芯片系列前景广阔。如果有人可以削弱 Nvidia 的主导地位，那一定是 AMD。

如果您根据我的博客文章进行交易亏损，请不要起诉我（免责声明：截至发布时，我做多 NVDA 和 AMD）。

初创公司

1. 构建软件来减少对 Nvidia 的依赖将具有巨大的价值，特别是如果它通过利用其他 GPU 来明显提高性价比的话。如果像 Modular 这样的新 ML 框架可以使工作负载与底层 GPU 硬件无关，那么这将释放更便宜的供应，并改变性价比曲线。

2. 小心在错误的时间投资 GPU 交易（和/或过度扩大“长期 GPU”交易的风险），例如在 2021 年投资加密采矿设备。在上一个加密周期中，存在很大的风险采矿业早期就能赚钱，直到供应过剩、利润率被压垮。

3. 如果AMD芯片最终在LLM的推理（部署）阶段发挥有意义的作用，将会建造大型的新数据中心，因为目前的数据中心主要集中在最适合训练的H100/A100上。以推理为中心的集群可能看起来完全不同，需要混合使用前沿的 Nvidia 和 AMD 芯片。

感谢 Divyahans Gupta、Akshat Bubna、Melisa Tokmak Luttig、Harry Elliott、Philip Clark、Axel Ericsson、Cat Wu、Lachy Groom、David Cahn、Ben Gilbert 和 Matt Kraning 对本文的想法和反馈。</title>
            <link>https://nitter.cz/FinanceYF5/status/1727506806841450680#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727506806841450680#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:58:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果云提供商有 GPU 可供出租，为什么还需要新贵呢？ Azure、AWS 和 GCP 等超大规模企业仅通过长期固定合同以“合理”价格提供 GPU——例如AWS 将以 110 万美元的价格向您租用 8 台 H100，为期 3 年（如果您不提前预订，价格会翻倍！）。较小的公司有更多零星的需求，并且正在寻找例如1,000 个 GPU 1 个月，然后 200 个 GPU 3 个月，然后 500 个 GPU 5 个月，依此类推。<br />
<br />
超大规模初创公司 CoreWeave、Lambda、ML Foundry 和 Together 正在努力通过提供对 GPU 的简单、可变、租用（“无服务器”）访问来弥补这一差距。这些企业今年的需求近乎无限，并且其硬件设置的回报非常快。这些企业与上一个周期的加密货币矿工具有相似的风险状况：你必须在 GPU 交易上进行杠杆押注，就像加密货币矿工在加密货币上进行杠杆押注一样。<br />
<br />
在需求方面，我们有充分的理由相信对可变 GPU 的需求将会持续下去。如果您是一家中小型企业（例如筹集了 0-1 亿美元）并且想要训练自己的尖端基础模型，您可能需要 5,000 台 Nvidia H100 – 您无法以每台 3 万美元以上的价格自行购买和安装它们设备，即使可以，您也不会经常运行它们以达到收支平衡。使用第三方经销商是有意义的。<br />
<br />
AWS 今年夏天宣布推出 20,000 个 H100 GPU 集群，一旦完成，该集群应该可以满足更大规模的 ML 训练运行的需求。他们利用供应短缺的机会，将营业利润率隐含地定价为 68%。但从长远来看，AWS 和 Azure 并不想成为 GPU 租赁企业——他们希望以高利润销售更广泛的服务。从这个意义上说，CoreWeave 和 Lambda 如果能够经受住供需波动的考验，就能像 Rackspace 或 DigitalOcean 那样持久发展。<br />
<br />
英伟达以高容量和优惠的价格奖励这些经销商，因为他们没有制造像微软和谷歌那样的竞争芯片。这对英伟达来说是一把双刃剑，因为支持经销商可以让超大规模厂商更有动力去竞争。<br />
<br />
经销商抢在其他人之前，通过在线提供备受追捧的 GPU 供应而受益。他们能建立持久的企业吗？差异化的关键在于规模、性价比、供应可扩展性以及软件产品的稳健性：<br />
<br />
1. 规模：您需要一个关键规模来实现两个关键目的：1）支持在尖端模型上运行更大型的语言模型训练（例如，一次运行 5,000 多个 2020 年老式 Nvidia A100 或尖端 H100，持续数周，2）在高度变化的客户需求中保持高利用率。<br />
<br />
2. 性价比：如果您将这些公司视为 GPU 时间经销商，那么供应成本确实很重要。作为 Nvidia 合作伙伴享有优惠定价（即获得与 Azure、AWS 等相同的定价）可以节省 30% 的集群构建成本。像克鲁索这样的玩家正在通过更便宜的能源来降低供应成本。 CoreWeave 声称，由于集群架构、虚拟化方法、冷却技术、互连质量等，他们的 A100 的性价比比超大规模的 A100 高出 30%。Salad 等一些公司甚至试图通过寻找消费者拥有的潜在 GPU 来聚集更便宜的供应。<br />
<br />
3. 供应可扩展性：Together 和 ML Foundry 等公司已从中期初创公司未充分利用的私有集群中“获取”（租用）潜在 GPU 供应。虽然仍处于技术研究阶段，但综合聚合 GPU 集群可以大规模解锁 GPU 方程式的供应方（类似于 Airbnb 对家庭所做的事情）。<br />
<br />
4. 软件：一些公司正在“向上堆栈”，即在 GPU 之上改善开发人员体验。 Modal、Lambda 和 RunPod 均声称通过软件提供更快的入门和更流畅的开发人员体验。<br />
<br />
鉴于这些企业的主要收入来源是 GPU 小时，它们本质上是对 GPU 需求扩展的杠杆赌注，以及对 H100 寿命的赌注（如果 GPU 在 3 年内过时，折旧打击将是痛苦的） ）。要对这些公司进行投资，您需要对中期 GPU 消耗扩展有强有力的方向性看法，或者相信构建更广泛的软件套件的长期影响力。<br />
<br />
结论<br />
<br />
无数的基础模型模仿者类似于现代的货物崇拜。他们正在尽最大努力重现第一波产品的魔力，但没有拥有所有所需的成分。<br />
<br />
鉴于计算支出的增加似乎与智能成对数关系，很难想象计算不会呈指数级增长的未来。但不同寻常的是，我对 20 年的前景比 5 年的前景更加清晰。从中期来看，过度构建 GPU 集群的风险很高。<br />
<br />
为了更好地了解 GPU 的中期前景，有几件事值得关注：<br />
<br />
1. 大型基础模型需求增长：OpenAI 和其他大型 LLM 提供商在未来 3-5 年内将增加多少 GPU 支出？许多模型提供商都在复制 OpenAI 所做的一切，因此他们的 GPU 消耗应该能够高度预测整体 GPU 需求的增长。最重要的是，GPU 支出大户增加集群规模的速度是否会比需求长尾衰退的速度更快？<br />
<br />
2. 推理与训练组合：看到用于推理的 GPU 使用量的变化将澄清该行业在未来几年是否会继续依赖 H100，或者转向更专门于推理的芯片。<br />
<br />
3.替代品的性价比：其他人需要数年时间才能赶上Nvidia前沿芯片的绝对性能，但我会关注新半导体产品的性价比。这里的标准并不是与英伟达相对较低的销货成本，而是与他们的零售价——这就是他们的定价能力可能受到攻击的时候。<br />
<br />
公众<br />
<br />
英伟达的非凡增长伴随着两个充满挑战的客户群的诅咒：<br />
<br />
1. 一群有强烈动机开发竞争性芯片项目的集中买家。<br />
<br />
2. 大量客户将在明年运行投机性的“构建/微调您自己的基础模型”实验，其中 GPU 需求的可预测性是不可能的。<br />
<br />
考虑到基于发布时间表的 GPU 支出的周期性，以及包含无限指数增长的估值，我预计 Nvidia 将在未来 6-9 个月内达到峰值，并在 2-3 年时间内降温。如果我看到大型基础模型的大型 H100 集群扩展计划，或者即将推出的 Office Copilot 等人工智能产品的高附加率，我会改变主意。<br />
<br />
我认为购买 AMD 是押注 GPU 长期增长而不挤入 Nvidia 交易的好方法。他们在加速计算的早期竞赛中失败了，但有很多公司愿意花费数十亿美元激励他们取得成功，而且他们即将推出的 MI 芯片系列前景广阔。如果有人可以削弱 Nvidia 的主导地位，那一定是 AMD。<br />
<br />
如果您根据我的博客文章进行交易亏损，请不要起诉我（免责声明：截至发布时，我做多 NVDA 和 AMD）。<br />
<br />
初创公司<br />
<br />
1. 构建软件来减少对 Nvidia 的依赖将具有巨大的价值，特别是如果它通过利用其他 GPU 来明显提高性价比的话。如果像 Modular 这样的新 ML 框架可以使工作负载与底层 GPU 硬件无关，那么这将释放更便宜的供应，并改变性价比曲线。<br />
<br />
2. 小心在错误的时间投资 GPU 交易（和/或过度扩大“长期 GPU”交易的风险），例如在 2021 年投资加密采矿设备。在上一个加密周期中，存在很大的风险采矿业早期就能赚钱，直到供应过剩、利润率被压垮。<br />
<br />
3. 如果AMD芯片最终在LLM的推理（部署）阶段发挥有意义的作用，将会建造大型的新数据中心，因为目前的数据中心主要集中在最适合训练的H100/A100上。以推理为中心的集群可能看起来完全不同，需要混合使用前沿的 Nvidia 和 AMD 芯片。<br />
<br />
感谢 Divyahans Gupta、Akshat Bubna、Melisa Tokmak Luttig、Harry Elliott、Philip Clark、Axel Ericsson、Cat Wu、Lachy Groom、David Cahn、Ben Gilbert 和 Matt Kraning 对本文的想法和反馈。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727506313301872697#m</id>
            <title>R to @FinanceYF5: 供应端：Nvidia 对 GPU 的垄断

Nvidia 将其 H100 产量增加两倍，到 2024 年出货量将超过 200 万台，主要瓶颈在于台积电。您可以阅读 Dylan Patel 的 SemiAnalysis 对上游零部件供应链瓶颈的详细分析。

Nvidia 垄断了 GPU 供应的市场份额（占收入的 80% 以上），当市场关注度较低时，这在某种程度上是一个合理的平衡。如果您每年在 GPU 上花费 1000 万美元甚至 5000 万美元的计算费用，而这只占您成本结构的一小部分，那么很容易外包给 Nvidia。

但如果您每年在 GPU 上花费 5 亿美元，并且它们成为您可变成本结构的重要组成部分，那么计算就不同了。现在，GPU 支出超过 1 亿美元、渴望扩大利润率的公司数量要大得多。过去一年，打破英伟达市场主导地位的动机增加了十倍。

这意味着 Nvidia 的客户群比表面上看到的风险更大：客户的长尾可能无法证明他们新生的基础模型 GPU 集群有任何投资回报，但最大的客户正在非常努力地削弱他们对 Nvidia 的依赖。

借用贝索斯的一句话，“你的利润就是我的机会”——英伟达今年创造了数百亿美元的机会。

超大规模者
谷歌于 2015 年开始 TPU 开发，作为当时加速计算（搜索、广告、翻译）的最大消费者之一。 TPU 的性能不如 Nvidia 的 H100 系列（他们甚至没有尝试比较），但也没有必要——他们通过内部芯片开发大幅降低了成本。 Nvidia 以 30,000 美元的价格出售 H100 单元，其 COGS 约为 3,320 美元，创造了软件层面的毛利率，远远高于 GPU 供应链的任何其他环节。

微软正在 AMD 的帮助下开发自己的 AI 芯片，称为 Athena 项目，The Information 于 2023 年 4 月报道了该项目。该项目于 2019 年启动，共有 300 名员工。有传言称，微软将优先支持AMD即将推出的MI300芯片。

就连亚马逊也在加大自己的芯片开发力度，尽管是在后面。他们似乎仍在制定自己的人工智能战略：他们正在开发自己的芯片，尝试训练自己的基础模型，投资 Anthropic，并托管 Llama-2 实例。通常，策略数量越多，信念水平越低。

AMD
像AMD这样的玩家正在尽力追赶，或者至少压低A100/H100的垄断定价权。他们的前沿MI300芯片不会打败H100，但用业内人士的话来说，“大家都在指望AMD降低成本”——换句话说，你不必相信MI300绝对压倒H100威胁英伟达定价权的条款；你只需要削弱他们的性价比。

MosaicML 已让 AMD 的 MI250 用于训练运行，但鉴于 AMD 软件堆栈与 Nvidia 专有且占主导地位的 CUDA 语言不同，因此很难在实践中使用 AMD 堆栈，除非 OpenAI 的 Triton 或 Modular 可以缩小软件支持差距。在私有云高管的世界里，“谁会冒险从一家初创公司部署 10,000 个 AMD GPU 或 10,000 个硅芯片？这几乎是 3 亿美元的投资。”

初创公司
Cerebras 筹集了超过 7.2 亿美元资金并推出了一款芯片，是最值得信赖的半导体初创公司。该架构与 Nvidia 截然不同，芯片尺寸增加了 56 倍，以减少跨 GPU 互连的需求。虽然他们的表现似乎与英伟达有竞争力，但他们需要建立开发人员的信任才能获得有意义的市场份额。即使他们无法足够快地建立商业规模生产，该技术对于第二或第三位芯片供应商来说也具有巨大的并购价值。

对于较新的程序，新芯片从白板到流片至少需要5年时间。即使 AMD 的努力按计划进行，至少要到 2025 年 Nvidia 对 GPU 供应的垄断才会被打破。但他们的目标只会越来越大，虽然我认为 Nvidia 最终会保持其 GPU 主导地位，但其定价能力可能会在中期受到挤压。

后 Nvidia 供应链：租赁 GPU 是一门好生意吗？
英伟达的不同寻常之处在于，它通常不直接向最终客户销售产品。 GPU设计和生产之后，就有一个由经销商和系统集成商组成的后期生产供应链：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727506313301872697#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727506313301872697#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:56:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>供应端：Nvidia 对 GPU 的垄断<br />
<br />
Nvidia 将其 H100 产量增加两倍，到 2024 年出货量将超过 200 万台，主要瓶颈在于台积电。您可以阅读 Dylan Patel 的 SemiAnalysis 对上游零部件供应链瓶颈的详细分析。<br />
<br />
Nvidia 垄断了 GPU 供应的市场份额（占收入的 80% 以上），当市场关注度较低时，这在某种程度上是一个合理的平衡。如果您每年在 GPU 上花费 1000 万美元甚至 5000 万美元的计算费用，而这只占您成本结构的一小部分，那么很容易外包给 Nvidia。<br />
<br />
但如果您每年在 GPU 上花费 5 亿美元，并且它们成为您可变成本结构的重要组成部分，那么计算就不同了。现在，GPU 支出超过 1 亿美元、渴望扩大利润率的公司数量要大得多。过去一年，打破英伟达市场主导地位的动机增加了十倍。<br />
<br />
这意味着 Nvidia 的客户群比表面上看到的风险更大：客户的长尾可能无法证明他们新生的基础模型 GPU 集群有任何投资回报，但最大的客户正在非常努力地削弱他们对 Nvidia 的依赖。<br />
<br />
借用贝索斯的一句话，“你的利润就是我的机会”——英伟达今年创造了数百亿美元的机会。<br />
<br />
超大规模者<br />
谷歌于 2015 年开始 TPU 开发，作为当时加速计算（搜索、广告、翻译）的最大消费者之一。 TPU 的性能不如 Nvidia 的 H100 系列（他们甚至没有尝试比较），但也没有必要——他们通过内部芯片开发大幅降低了成本。 Nvidia 以 30,000 美元的价格出售 H100 单元，其 COGS 约为 3,320 美元，创造了软件层面的毛利率，远远高于 GPU 供应链的任何其他环节。<br />
<br />
微软正在 AMD 的帮助下开发自己的 AI 芯片，称为 Athena 项目，The Information 于 2023 年 4 月报道了该项目。该项目于 2019 年启动，共有 300 名员工。有传言称，微软将优先支持AMD即将推出的MI300芯片。<br />
<br />
就连亚马逊也在加大自己的芯片开发力度，尽管是在后面。他们似乎仍在制定自己的人工智能战略：他们正在开发自己的芯片，尝试训练自己的基础模型，投资 Anthropic，并托管 Llama-2 实例。通常，策略数量越多，信念水平越低。<br />
<br />
AMD<br />
像AMD这样的玩家正在尽力追赶，或者至少压低A100/H100的垄断定价权。他们的前沿MI300芯片不会打败H100，但用业内人士的话来说，“大家都在指望AMD降低成本”——换句话说，你不必相信MI300绝对压倒H100威胁英伟达定价权的条款；你只需要削弱他们的性价比。<br />
<br />
MosaicML 已让 AMD 的 MI250 用于训练运行，但鉴于 AMD 软件堆栈与 Nvidia 专有且占主导地位的 CUDA 语言不同，因此很难在实践中使用 AMD 堆栈，除非 OpenAI 的 Triton 或 Modular 可以缩小软件支持差距。在私有云高管的世界里，“谁会冒险从一家初创公司部署 10,000 个 AMD GPU 或 10,000 个硅芯片？这几乎是 3 亿美元的投资。”<br />
<br />
初创公司<br />
Cerebras 筹集了超过 7.2 亿美元资金并推出了一款芯片，是最值得信赖的半导体初创公司。该架构与 Nvidia 截然不同，芯片尺寸增加了 56 倍，以减少跨 GPU 互连的需求。虽然他们的表现似乎与英伟达有竞争力，但他们需要建立开发人员的信任才能获得有意义的市场份额。即使他们无法足够快地建立商业规模生产，该技术对于第二或第三位芯片供应商来说也具有巨大的并购价值。<br />
<br />
对于较新的程序，新芯片从白板到流片至少需要5年时间。即使 AMD 的努力按计划进行，至少要到 2025 年 Nvidia 对 GPU 供应的垄断才会被打破。但他们的目标只会越来越大，虽然我认为 Nvidia 最终会保持其 GPU 主导地位，但其定价能力可能会在中期受到挤压。<br />
<br />
后 Nvidia 供应链：租赁 GPU 是一门好生意吗？<br />
英伟达的不同寻常之处在于，它通常不直接向最终客户销售产品。 GPU设计和生产之后，就有一个由经销商和系统集成商组成的后期生产供应链：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sVjBpd2JnQUF2VWMyLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727505782131089834#m</id>
            <title>R to @FinanceYF5: 至于企业方面的第二名，API 市场同样是一边倒的，只有 Llama-2、Claude 和 Cohere 等少数亚军。更多基础设施提供商将会上线，但就像 2010 年代的云基础设施提供商一样，您只需要少数提供商即可满足 80% 的市场需求。

这导致了一个令人费解的场景：数千家科技公司看似对 GPU 有着无限的需求，但只有少数玩家看到了投资回报。行业估计表明，OpenAI GPT-4 的 Microsoft 集群约为 20-25k H100，并且在未来 18 个月内将有 300 万个 H100 上线。我们真正需要多少个 GPT-4 级别的模型？

我知道有一个令人信服的乐观案例——OpenAI 可以扩展 GPT-5 以在更大的集群上进行训练，其他人也会效仿，并且可能有数以万计的公司在微调模型。但从 2023 年开始，超大规模企业可能需要一些时间来消化他们的大量 H100 订单。每年数百万台 H100 的需求取决于一个多型号的世界，我不确定这个世界是否可持续。

投资回报率差距

到 2024 年底，Nvidia 的 H100 销量将达到 100-120b 美元。目前，关键的人工智能应用程序（OpenAI、Midjourney、Copilot 等）的收入总计不到 50 亿美元。当考虑到能源和劳动力等辅助成本时，这意味着人工智能应用程序收入需要增长至少 20-50 倍才能证明资本投资的合理性，通过大型科技公司的本土初创公司和 100 亿美元以上的热门 LLM 产品。正如 David Cahn 明确指出的那样：在当前支出水平下，每年的资本支出有 100b+ 美元的缺口需要填补。

第一批 ChatGPT 行动派应该会在接下来的几个季度展示他们的投资回报率。回报将变化很大——虽然计算支出的分布范围令人惊讶，但收入和用户增长却并非如此。

第二波快速追随者基础模型的早期结果已经没有显示出与第一波相同的投资回报率——尽管 2023 年推出了数十种基础模型产品，但没有一个比 2022 年推出的第一批产品更出色。新颖的 X 因素和免费的全球公关使早期的获胜者发挥了作用，我怀疑绝大多数行动基金会模型将陷入一个受伤的世界。

推理

如今，规模化的人工智能应用程序相对较少——构建足够准确、可靠和可信的交互范例需要时间。即使是微软相对生产化的 GPU 集群也主要运行训练工作负载，少数用于推理。大多数大型科技公司估计，到 2025 年，他们的 GPU 工作负载将转向 30% 的训练和 70% 的推理（当然，这可能低估了一些主要 AI 实验室的训练增长）。

不过，推理需求可能会在短时间内大大超过可用的 GPU 供应。例如，Midjourney 拥有约 10,000 多个用于推理的 GPU，以及约 150 万活跃用户。如果他们以相同的效率达到 3000 万活跃用户，他们可能需要数十万个 GPU。有数十家拥有数百万用户的软件公司将在未来几个月内发布由法学硕士支持的产品。如果其中一小部分有效地交叉销售，推理需求很容易就会耗尽所有可用的 GPU 容量。

GPU 市场一直非常重视训练（英伟达在这方面表现出色），而不是推理。相对于 2010 年代软件的“零边际成本”世界，推理是一项异常大的支出项目，成本结构确实很重要。

考虑到 GPU 在传输到 CPU 的时间里处于空闲状态，H100 对于推理型工作负载来说通常是多余的，这就是 Nvidia 创建低端 L4 和 A10 设备的原因。推理是其他公司与 Nvidia 竞争的机会：谷歌的 v5e TPU 已经是一种流行的推理选项（但需要 GCP 锁定），而 AMD 的 MI300 已经证明它非常适合作为 CPU-GPU 混合体进行推理。

随着价格性能占主导地位的推理能力的提高，AMD 有机会通过削弱 Nvidia 来成为主要提供商。相对于 Nvidia，AMD 的 AI 收入基础较小，但足以成为可靠的替代方案，预计 2024 年 AI 芯片销售额将达到 20 亿美元。这应该确保它们成为任何大规模 GPU 采购中必须评估的对象。鉴于训练和推理的产品要求不同，我们可以看到 GPU 市场细分的增加。</title>
            <link>https://nitter.cz/FinanceYF5/status/1727505782131089834#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727505782131089834#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:54:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>至于企业方面的第二名，API 市场同样是一边倒的，只有 Llama-2、Claude 和 Cohere 等少数亚军。更多基础设施提供商将会上线，但就像 2010 年代的云基础设施提供商一样，您只需要少数提供商即可满足 80% 的市场需求。<br />
<br />
这导致了一个令人费解的场景：数千家科技公司看似对 GPU 有着无限的需求，但只有少数玩家看到了投资回报。行业估计表明，OpenAI GPT-4 的 Microsoft 集群约为 20-25k H100，并且在未来 18 个月内将有 300 万个 H100 上线。我们真正需要多少个 GPT-4 级别的模型？<br />
<br />
我知道有一个令人信服的乐观案例——OpenAI 可以扩展 GPT-5 以在更大的集群上进行训练，其他人也会效仿，并且可能有数以万计的公司在微调模型。但从 2023 年开始，超大规模企业可能需要一些时间来消化他们的大量 H100 订单。每年数百万台 H100 的需求取决于一个多型号的世界，我不确定这个世界是否可持续。<br />
<br />
投资回报率差距<br />
<br />
到 2024 年底，Nvidia 的 H100 销量将达到 100-120b 美元。目前，关键的人工智能应用程序（OpenAI、Midjourney、Copilot 等）的收入总计不到 50 亿美元。当考虑到能源和劳动力等辅助成本时，这意味着人工智能应用程序收入需要增长至少 20-50 倍才能证明资本投资的合理性，通过大型科技公司的本土初创公司和 100 亿美元以上的热门 LLM 产品。正如 David Cahn 明确指出的那样：在当前支出水平下，每年的资本支出有 100b+ 美元的缺口需要填补。<br />
<br />
第一批 ChatGPT 行动派应该会在接下来的几个季度展示他们的投资回报率。回报将变化很大——虽然计算支出的分布范围令人惊讶，但收入和用户增长却并非如此。<br />
<br />
第二波快速追随者基础模型的早期结果已经没有显示出与第一波相同的投资回报率——尽管 2023 年推出了数十种基础模型产品，但没有一个比 2022 年推出的第一批产品更出色。新颖的 X 因素和免费的全球公关使早期的获胜者发挥了作用，我怀疑绝大多数行动基金会模型将陷入一个受伤的世界。<br />
<br />
推理<br />
<br />
如今，规模化的人工智能应用程序相对较少——构建足够准确、可靠和可信的交互范例需要时间。即使是微软相对生产化的 GPU 集群也主要运行训练工作负载，少数用于推理。大多数大型科技公司估计，到 2025 年，他们的 GPU 工作负载将转向 30% 的训练和 70% 的推理（当然，这可能低估了一些主要 AI 实验室的训练增长）。<br />
<br />
不过，推理需求可能会在短时间内大大超过可用的 GPU 供应。例如，Midjourney 拥有约 10,000 多个用于推理的 GPU，以及约 150 万活跃用户。如果他们以相同的效率达到 3000 万活跃用户，他们可能需要数十万个 GPU。有数十家拥有数百万用户的软件公司将在未来几个月内发布由法学硕士支持的产品。如果其中一小部分有效地交叉销售，推理需求很容易就会耗尽所有可用的 GPU 容量。<br />
<br />
GPU 市场一直非常重视训练（英伟达在这方面表现出色），而不是推理。相对于 2010 年代软件的“零边际成本”世界，推理是一项异常大的支出项目，成本结构确实很重要。<br />
<br />
考虑到 GPU 在传输到 CPU 的时间里处于空闲状态，H100 对于推理型工作负载来说通常是多余的，这就是 Nvidia 创建低端 L4 和 A10 设备的原因。推理是其他公司与 Nvidia 竞争的机会：谷歌的 v5e TPU 已经是一种流行的推理选项（但需要 GCP 锁定），而 AMD 的 MI300 已经证明它非常适合作为 CPU-GPU 混合体进行推理。<br />
<br />
随着价格性能占主导地位的推理能力的提高，AMD 有机会通过削弱 Nvidia 来成为主要提供商。相对于 Nvidia，AMD 的 AI 收入基础较小，但足以成为可靠的替代方案，预计 2024 年 AI 芯片销售额将达到 20 亿美元。这应该确保它们成为任何大规模 GPU 采购中必须评估的对象。鉴于训练和推理的产品要求不同，我们可以看到 GPU 市场细分的增加。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727504892527583668#m</id>
            <title>R to @FinanceYF5: 地理规模

20% 的需求来自中国科技：由于禁止向中国销售高性能 GPU，许多中国科技公司一直在囤积大量芯片，以应对美国出口管制的加强。

中国科技巨头百度、字节跳动、腾讯和阿里巴巴将斥资 10 亿美元购买将于今年交付的芯片（主要是 A800，根据美中人工智能法规的稀释版 A100），并斥资 40 亿美元购买 2024 年的芯片。占 Nvidia 收入的 20%（如果考虑到中东 GPU 采购的潜在转嫁，可能会更高），加强出口管制可能会在中期内造成重大阻力。

长期展望：投资回报率来了吗？

在数十年的时间范围内，我们肯定会成倍扩展全球计算，因此很容易对 GPU 进行长期看好。尝试预测 5 年需求要困难得多。了解关键杠杆——模型大小、模型数量、模型投资回报率和推理——将帮助我们对总需求的发展方向做出一些猜测。

缩放模型的数量和大小

为了预测长期训练 GPU 需求，您需要了解 1) 基础模型有多大，以及 2) 有多少模型将获胜。

第一点，如果 OpenAI、Bard 和 Anthropic 等大型模型确定缩放定律成立（即更多计算 → 更多智能），那么它们可以在未来几年轻松地将前沿 GPU 需求增加一个数量级，其他人也会效仿。另一方面，许多 B2B AI 产品通过微调特定领域的约束输出模型来最小化底层模型大小 - 这些模型的 GPU 密集程度要低得多。

关于消费者方面的第二点，早期指标表明模型成功将遵循幂律分布。在排名前 50 位的消费者 AI 应用程序中，ChatGPT 占所有网络流量的 60%（当然，这是一个不完美的衡量标准），并且运行在全球 H100 容量的不到 5% 上：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727504892527583668#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727504892527583668#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:50:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>地理规模<br />
<br />
20% 的需求来自中国科技：由于禁止向中国销售高性能 GPU，许多中国科技公司一直在囤积大量芯片，以应对美国出口管制的加强。<br />
<br />
中国科技巨头百度、字节跳动、腾讯和阿里巴巴将斥资 10 亿美元购买将于今年交付的芯片（主要是 A800，根据美中人工智能法规的稀释版 A100），并斥资 40 亿美元购买 2024 年的芯片。占 Nvidia 收入的 20%（如果考虑到中东 GPU 采购的潜在转嫁，可能会更高），加强出口管制可能会在中期内造成重大阻力。<br />
<br />
长期展望：投资回报率来了吗？<br />
<br />
在数十年的时间范围内，我们肯定会成倍扩展全球计算，因此很容易对 GPU 进行长期看好。尝试预测 5 年需求要困难得多。了解关键杠杆——模型大小、模型数量、模型投资回报率和推理——将帮助我们对总需求的发展方向做出一些猜测。<br />
<br />
缩放模型的数量和大小<br />
<br />
为了预测长期训练 GPU 需求，您需要了解 1) 基础模型有多大，以及 2) 有多少模型将获胜。<br />
<br />
第一点，如果 OpenAI、Bard 和 Anthropic 等大型模型确定缩放定律成立（即更多计算 → 更多智能），那么它们可以在未来几年轻松地将前沿 GPU 需求增加一个数量级，其他人也会效仿。另一方面，许多 B2B AI 产品通过微调特定领域的约束输出模型来最小化底层模型大小 - 这些模型的 GPU 密集程度要低得多。<br />
<br />
关于消费者方面的第二点，早期指标表明模型成功将遵循幂律分布。在排名前 50 位的消费者 AI 应用程序中，ChatGPT 占所有网络流量的 60%（当然，这是一个不完美的衡量标准），并且运行在全球 H100 容量的不到 5% 上：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sVDBpa2FFQUF2aDdLLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727503968677605469#m</id>
            <title>R to @FinanceYF5: 考虑到更高质量的客户群和直接转化为生产力收益的投资回报率，人工智能驱动的需求应该比加密货币挖矿的增长更持久。但由于 GPU 通常是作为不规则的资本支出购买的，因此半导体行业历来都是一个繁荣与萧条的行业。

尽管后视镜中看似完美的指数式增长，但英伟达不可预测的增长却创造了大型科技公司最不稳定的收入基础之一：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727503968677605469#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727503968677605469#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:46:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>考虑到更高质量的客户群和直接转化为生产力收益的投资回报率，人工智能驱动的需求应该比加密货币挖矿的增长更持久。但由于 GPU 通常是作为不规则的资本支出购买的，因此半导体行业历来都是一个繁荣与萧条的行业。<br />
<br />
尽管后视镜中看似完美的指数式增长，但英伟达不可预测的增长却创造了大型科技公司最不稳定的收入基础之一：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sVHN3dWFVQUFDR2FtLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727503884472737935#m</id>
            <title>R to @FinanceYF5: 如果将其乘以整个行业，您可能会发现长尾的 GPU 需求存在巨大的暂时缺口。中期问题是，获胜者的推理能力是否会在低投资回报率训练结束之前出现。

需求的周期性

预测 GPU 需求的一个挑战是，每次发布都具有高度周期性和前瞻性——例如，在 2020 年 A100 发布仅 3 年后，Nvidia 就开始减少产量。

虽然 Nvidia 的 GPU 在 2010 年代初主要用于游戏，但 2017 年和 2020 年加密货币繁荣极大地加速了其增长。随着加密货币开始降温，人工智能热潮在近乎完美的时机开始了：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727503884472737935#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727503884472737935#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:46:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果将其乘以整个行业，您可能会发现长尾的 GPU 需求存在巨大的暂时缺口。中期问题是，获胜者的推理能力是否会在低投资回报率训练结束之前出现。<br />
<br />
需求的周期性<br />
<br />
预测 GPU 需求的一个挑战是，每次发布都具有高度周期性和前瞻性——例如，在 2020 年 A100 发布仅 3 年后，Nvidia 就开始减少产量。<br />
<br />
虽然 Nvidia 的 GPU 在 2010 年代初主要用于游戏，但 2017 年和 2020 年加密货币繁荣极大地加速了其增长。随着加密货币开始降温，人工智能热潮在近乎完美的时机开始了：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sVG5lMmFnQUFGZU1yLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727503776922361882#m</id>
            <title>R to @FinanceYF5: GPU 需求曲线：增长还是过剩？

2023 年夏天，Nvidia GPU 上的运行情况已经很明显了。截至 2023 年夏季，H100 已售罄，直至 2024 年第一季度。

GPU 效率提升和供应链解锁有着不可阻挡的力量，对抗超过 1 亿美元的 LLM 培训运行的不可移动的墙壁。我们不可能知道 GPU 短缺何时结束，但我们可以通过更好地了解当前的需求动态及其发展方向来确定短缺的持续时间。

当今：到底谁在购买 GPU？
有少数下游参与者购买 Nvidia GPU：超大规模企业（谷歌、微软、亚马逊）、大型基础模型以及初创公司和企业中数量惊人的中型 AI 团队。

出于内部目的需要大规模 GPU 集群的大型科技公司最适合培训自己的LLM，因为他们已经拥有 ML 专业知识和对大型 H100 集群的内部需求。核心超大规模提供商（AWS、Azure、GCP）每年的资本支出均超过 200 亿美元，这一数字每年仍在以 25% 以上的速度增长。资本支出竞赛导致规模较小的云提供商奋起直追：甲骨文的资本支出翻了两次，从 2021 年的 2.1 亿美元增至 2022 年的 4.5 亿美元，再到 2023 年的 8.7 亿美元。

超大规模企业没有报告确切的 GPU 支出，但业内人士估计，每个参与者将约 20-35% 的数据中心资本支出用于 GPU 扩建（GPU 采购订单、周边数据中心/电源等），其中 Microsoft是最具攻击性的。 GPU 为云服务带来的收入比例仍然很小，但增长迅速。粗略估计在 5-15% 范围内，其中 Azure 处于高端。

每个超大规模服务器都有 15 万多个 A100，单个集群最多可达 25,000 个单位。但他们基本上暂停了 A100 的采购，转而购买下一代 H100。最大的 H100 集群有 20-30,000 台（Microsoft、Meta、Amazon 和 Google），用于内部产品的组合、通过 Azure/AWS/GCP 的外部租赁及其各自的 AI 合作伙伴（OpenAI、Inflection、Anthropic） ）。中尾购买者拥有数千台 A100 和 H100，例如 Stability、Tesla、Lambda Labs、Hugging Face、Aleph Alpha 和 Andromeda。

鉴于其高带宽和内存，H100 能够很好地服务于 LLM 培训用例。但对于看似相邻的应用程序，如稳定扩散（图像生成模型）、较小的模型训练（甚至 GPT-3 规模）和 LLM 推理，H100 的性能就显得有些过分了。目前尚不清楚稳定扩散和落后的 LLM 推理等其他用例是否会将其内存和带宽需求扩大到需要 H100。

中期展望：预计波动
Nvidia 计划明年将 H100 的产量增加到 200 万台以上（有人说高达 3-400 万台）。需求能匹配吗？为了对 2024 年的情况做出有根据的猜测，我们可以观察两组购买者的需求：超大规模购买者和长尾购买者。

超大规模者
超大规模企业今年将收到数以万计的 H100，从低端的 2 万台到高端的 10 万台以上不等。微软是 H100 的最大购买者之一（如果不是最大的话），而谷歌希望支持其内部 TPU 工作，同时也在加大 H100 的购买力度，以支持其 26,000 个 H100 A3 集群。

Nvidia 在第二季度收益报告中披露，一家云服务提供商占季度数据中心收入的 29%，在 2023 年前 6 个月总计购买了 3.9 亿美元（！），相当于约 13 万台 H100。我怀疑这是 Microsoft 提前购买的产品，以支持即将推出的 Office Copilot、OpenAI 计算和 Azure 服务。他们甚至通过 CoreWeave 和 Oracle 保留了多余的容量——如果 Office Copilot 起飞，这是一个灵活扩展 GPU 容量的伟大举措，同时在需求增加时最大限度地降低 GPU 资本支出超支的风险。

超大规模设备约占当今所有 H100 需求的 50%。在这些超大规模的采购中，很大一部分是为了支持即将推出的产品，例如 Office Copilot 和 Google Gemini。大型科技公司支持 LLM 的产品的附加率将是 Nvidia H100 中期需求的最早指标。

以绝对美元计算，这套产品无疑将得到广泛采用，但我怀疑，在这些公司解决产品问题、特定垂直用例和销售动议之前，这还不足以证明对 H100 进行更大的投资是合理的。

举个例子：GitHub Copilot 是增长最快的人工智能产品之一，一年后 ARR 超过 1 亿美元，但与 1) 所有 VSCode 用户（1400 万用户意味着 Copilot 渗透率低于 5%）和 2) 微软的相比相对较小。整体AI投资数百亿。为了让 GPU 投资获得回报，他们需要更广泛的 Office Copilot 产品，将 Office 500 亿美元的收入基础增加至少 10%，即收入超过 50 亿美元。

长尾巴
如果大型科技公司只是 H100 需求故事的一部分，那么还有谁在大量购买？除了超大规模企业之外，还有巨大的长尾买家。即使在大型科技公司，很大一部分 H100（约 30-50%）也将被其客户用作云产品（AWS、GCP、Azure）的一部分。这意味着 H100 总利用率的约 2/3 是长尾。

似乎每个人都订购了几百到几千台 H100——Cloudflare、Palantir、HP、Voltage Park、Cohere，甚至沙特阿拉伯和阿联酋等民族国家。那些还没有购买芯片的人正在大举花钱，从 GPU 所有者那里保留 GPU 时间。

对于那些在自己的 GPU 上训练或微调模型的人来说，集群利用率通常很低。 ChatGPT 的反常之处在于，产品一推出就基本上实现了产品与市场的契合。这意味着对以推理为中心的 GPU 的内部需求迅速增加，从而保持了 GPU 的高利用率。

大多数训练基础模型的公司都不会那么快地实现产品与市场的契合——要么是模型不够好，要么是产品不够好，要么是上市不够好。这可能会创建一条内部 GPU 需求曲线，大致如下：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727503776922361882#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727503776922361882#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:46:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPU 需求曲线：增长还是过剩？<br />
<br />
2023 年夏天，Nvidia GPU 上的运行情况已经很明显了。截至 2023 年夏季，H100 已售罄，直至 2024 年第一季度。<br />
<br />
GPU 效率提升和供应链解锁有着不可阻挡的力量，对抗超过 1 亿美元的 LLM 培训运行的不可移动的墙壁。我们不可能知道 GPU 短缺何时结束，但我们可以通过更好地了解当前的需求动态及其发展方向来确定短缺的持续时间。<br />
<br />
当今：到底谁在购买 GPU？<br />
有少数下游参与者购买 Nvidia GPU：超大规模企业（谷歌、微软、亚马逊）、大型基础模型以及初创公司和企业中数量惊人的中型 AI 团队。<br />
<br />
出于内部目的需要大规模 GPU 集群的大型科技公司最适合培训自己的LLM，因为他们已经拥有 ML 专业知识和对大型 H100 集群的内部需求。核心超大规模提供商（AWS、Azure、GCP）每年的资本支出均超过 200 亿美元，这一数字每年仍在以 25% 以上的速度增长。资本支出竞赛导致规模较小的云提供商奋起直追：甲骨文的资本支出翻了两次，从 2021 年的 2.1 亿美元增至 2022 年的 4.5 亿美元，再到 2023 年的 8.7 亿美元。<br />
<br />
超大规模企业没有报告确切的 GPU 支出，但业内人士估计，每个参与者将约 20-35% 的数据中心资本支出用于 GPU 扩建（GPU 采购订单、周边数据中心/电源等），其中 Microsoft是最具攻击性的。 GPU 为云服务带来的收入比例仍然很小，但增长迅速。粗略估计在 5-15% 范围内，其中 Azure 处于高端。<br />
<br />
每个超大规模服务器都有 15 万多个 A100，单个集群最多可达 25,000 个单位。但他们基本上暂停了 A100 的采购，转而购买下一代 H100。最大的 H100 集群有 20-30,000 台（Microsoft、Meta、Amazon 和 Google），用于内部产品的组合、通过 Azure/AWS/GCP 的外部租赁及其各自的 AI 合作伙伴（OpenAI、Inflection、Anthropic） ）。中尾购买者拥有数千台 A100 和 H100，例如 Stability、Tesla、Lambda Labs、Hugging Face、Aleph Alpha 和 Andromeda。<br />
<br />
鉴于其高带宽和内存，H100 能够很好地服务于 LLM 培训用例。但对于看似相邻的应用程序，如稳定扩散（图像生成模型）、较小的模型训练（甚至 GPT-3 规模）和 LLM 推理，H100 的性能就显得有些过分了。目前尚不清楚稳定扩散和落后的 LLM 推理等其他用例是否会将其内存和带宽需求扩大到需要 H100。<br />
<br />
中期展望：预计波动<br />
Nvidia 计划明年将 H100 的产量增加到 200 万台以上（有人说高达 3-400 万台）。需求能匹配吗？为了对 2024 年的情况做出有根据的猜测，我们可以观察两组购买者的需求：超大规模购买者和长尾购买者。<br />
<br />
超大规模者<br />
超大规模企业今年将收到数以万计的 H100，从低端的 2 万台到高端的 10 万台以上不等。微软是 H100 的最大购买者之一（如果不是最大的话），而谷歌希望支持其内部 TPU 工作，同时也在加大 H100 的购买力度，以支持其 26,000 个 H100 A3 集群。<br />
<br />
Nvidia 在第二季度收益报告中披露，一家云服务提供商占季度数据中心收入的 29%，在 2023 年前 6 个月总计购买了 3.9 亿美元（！），相当于约 13 万台 H100。我怀疑这是 Microsoft 提前购买的产品，以支持即将推出的 Office Copilot、OpenAI 计算和 Azure 服务。他们甚至通过 CoreWeave 和 Oracle 保留了多余的容量——如果 Office Copilot 起飞，这是一个灵活扩展 GPU 容量的伟大举措，同时在需求增加时最大限度地降低 GPU 资本支出超支的风险。<br />
<br />
超大规模设备约占当今所有 H100 需求的 50%。在这些超大规模的采购中，很大一部分是为了支持即将推出的产品，例如 Office Copilot 和 Google Gemini。大型科技公司支持 LLM 的产品的附加率将是 Nvidia H100 中期需求的最早指标。<br />
<br />
以绝对美元计算，这套产品无疑将得到广泛采用，但我怀疑，在这些公司解决产品问题、特定垂直用例和销售动议之前，这还不足以证明对 H100 进行更大的投资是合理的。<br />
<br />
举个例子：GitHub Copilot 是增长最快的人工智能产品之一，一年后 ARR 超过 1 亿美元，但与 1) 所有 VSCode 用户（1400 万用户意味着 Copilot 渗透率低于 5%）和 2) 微软的相比相对较小。整体AI投资数百亿。为了让 GPU 投资获得回报，他们需要更广泛的 Office Copilot 产品，将 Office 500 亿美元的收入基础增加至少 10%，即收入超过 50 亿美元。<br />
<br />
长尾巴<br />
如果大型科技公司只是 H100 需求故事的一部分，那么还有谁在大量购买？除了超大规模企业之外，还有巨大的长尾买家。即使在大型科技公司，很大一部分 H100（约 30-50%）也将被其客户用作云产品（AWS、GCP、Azure）的一部分。这意味着 H100 总利用率的约 2/3 是长尾。<br />
<br />
似乎每个人都订购了几百到几千台 H100——Cloudflare、Palantir、HP、Voltage Park、Cohere，甚至沙特阿拉伯和阿联酋等民族国家。那些还没有购买芯片的人正在大举花钱，从 GPU 所有者那里保留 GPU 时间。<br />
<br />
对于那些在自己的 GPU 上训练或微调模型的人来说，集群利用率通常很低。 ChatGPT 的反常之处在于，产品一推出就基本上实现了产品与市场的契合。这意味着对以推理为中心的 GPU 的内部需求迅速增加，从而保持了 GPU 的高利用率。<br />
<br />
大多数训练基础模型的公司都不会那么快地实现产品与市场的契合——要么是模型不够好，要么是产品不够好，要么是上市不够好。这可能会创建一条内部 GPU 需求曲线，大致如下：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sVEtRS2FnQUE1UUxOLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727503209269506177#m</id>
            <title>R to @FinanceYF5: 截至第二季度，Nvidia 的数据中心收入大约为 A100 的一半和 H100 的一半，但随着 A100 产量的减少，该收入迅速转向 H100。对于训练大规模 LLM（考虑 GPT-4 级别），H100 在速度（训练时间）和成本（即每美元的失败次数）方面都是最好的。对于较小的语言模型和图像模型（例如 GPT-3 或稳定扩散），A100（甚至更便宜的 A10）对于训练和推理来说都是经济高效的 GPU。

为了将模型训练到 GPT-3 级别的性能，所需的计算量相对较低 - 这解释了为什么在 ChatGPT 推出的短短几个月内就有如此多的演示接近 GPT-3 的性能。有数十个集群，拥有 1,000 多台 A100。

对于 GPT-4 级别的性能，标准要高得多。只有微软、亚马逊、谷歌和 Facebook 等少数公司能够在单个集群中使用数万个前沿 GPU，更不用说高质量数据采集和训练技术方面的障碍了。扩展数据采集和计算既困难又昂贵——这就是为什么没有人能够超越 GPT-4 的一般性能，即使是在发布 8 个月后也是如此。

此图表应该让您了解训练各种模型所需的计算规模：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727503209269506177#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727503209269506177#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:43:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>截至第二季度，Nvidia 的数据中心收入大约为 A100 的一半和 H100 的一半，但随着 A100 产量的减少，该收入迅速转向 H100。对于训练大规模 LLM（考虑 GPT-4 级别），H100 在速度（训练时间）和成本（即每美元的失败次数）方面都是最好的。对于较小的语言模型和图像模型（例如 GPT-3 或稳定扩散），A100（甚至更便宜的 A10）对于训练和推理来说都是经济高效的 GPU。<br />
<br />
为了将模型训练到 GPT-3 级别的性能，所需的计算量相对较低 - 这解释了为什么在 ChatGPT 推出的短短几个月内就有如此多的演示接近 GPT-3 的性能。有数十个集群，拥有 1,000 多台 A100。<br />
<br />
对于 GPT-4 级别的性能，标准要高得多。只有微软、亚马逊、谷歌和 Facebook 等少数公司能够在单个集群中使用数万个前沿 GPU，更不用说高质量数据采集和训练技术方面的障碍了。扩展数据采集和计算既困难又昂贵——这就是为什么没有人能够超越 GPT-4 的一般性能，即使是在发布 8 个月后也是如此。<br />
<br />
此图表应该让您了解训练各种模型所需的计算规模：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sUy1GeGIwQUFrM3kzLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727503070698115174#m</id>
            <title>R to @FinanceYF5: Nvidia 产品阵容概览

以下设备是 Nvidia 的高性能计算系列，旨在用于大规模工作负载，并且通常组装在集群中（许多 GPU 联网形成一台超级计算机）：</title>
            <link>https://nitter.cz/FinanceYF5/status/1727503070698115174#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727503070698115174#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:43:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nvidia 产品阵容概览<br />
<br />
以下设备是 Nvidia 的高性能计算系列，旨在用于大规模工作负载，并且通常组装在集群中（许多 GPU 联网形成一台超级计算机）：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sUzRXSWFNQUFwZjVtLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/FinanceYF5/status/1727502987365605846#m</id>
            <title>R to @FinanceYF5: 很难知道是否存在某些效率边界，我们应该停止对增量计算性能的投资。例如，在过去十年中，铺设互联网光纤的利润大幅下降。

但我一直对推动计算前沿的新兴现象感到惊讶。 2010 年，像 GPT-4 这样的模型在技术上并不可行——最大的超级计算机的计算能力相当于约 1,000 个 H100。如果数据和计算的每个数量级的增加都会推动 LLM IQ 分数的跃升，那么下一个数量级可能会发生什么？

大多数技术指数实际上只是 S 曲线——互联网采用率、广告收入、SaaS 市场规模——但计算可能是少数几个能够持久保持真正指数的指数之一，更类似于 GDP 或能源消耗（尽管能源最终也会呈指数增长）停滞）。到 2043 年，从后视镜来看，2023 年的计算市场显然会很小。

当我在斯坦福大学读本科时，很少有人关注电气工程——它被认为是一个已解决的问题，相反，每个人都转向软件领域。但忽视硬件层是一个错误——我们有责任将计算硬件理解为最接近持久指数的事物之一。

英伟达帝国

许多大型科技公司都计划摆脱对 Nvidia 的依赖，但如果你想在当今大型基础模型的前沿竞争，Nvidia 是唯一实用的选择。

我的朋友 Ben Gilbert 在这里做了一个关于 Nvidia 公司历史的精彩系列文章，简而言之：该公司成立于 1993 年，旨在将 3D 图形引入游戏，并于 1999 年发布了第一款 GPU。他们于 2006 年发明了 CUDA，允许并行处理跨多个 GPU，为开发用于训练更大工作负载的 GPU 集群奠定了基础。

他们在超级计算机领域处于垄断地位，为全球 500 强超级计算机中的 74% 以及几乎所有大型人工智能训练集群提供动力。初创公司将其与公司的关系视为核心差异化因素（例如，率先购买芯片）。虽然 Nvidia 最初是一家消费 GPU 公司，但 Nvidia 2023 年旗帜的核心是数据中心业务。

数据中心业务
Nvidia 的数据中心业务销售用于 AI + 高性能计算工作负载的 GPU。第一季度的收入运行率达到 170 亿美元（占收入的 56%），第二季度的运行率翻了一番多（！），达到 410 亿美元（占收入的 76%）。华尔街分析师预计 2024 年（25 财年）将是又一个辉煌的一年，数据中心收入将达到 60-1000 亿美元。</title>
            <link>https://nitter.cz/FinanceYF5/status/1727502987365605846#m</link>
            <guid isPermaLink="false">https://nitter.cz/FinanceYF5/status/1727502987365605846#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 01:43:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>很难知道是否存在某些效率边界，我们应该停止对增量计算性能的投资。例如，在过去十年中，铺设互联网光纤的利润大幅下降。<br />
<br />
但我一直对推动计算前沿的新兴现象感到惊讶。 2010 年，像 GPT-4 这样的模型在技术上并不可行——最大的超级计算机的计算能力相当于约 1,000 个 H100。如果数据和计算的每个数量级的增加都会推动 LLM IQ 分数的跃升，那么下一个数量级可能会发生什么？<br />
<br />
大多数技术指数实际上只是 S 曲线——互联网采用率、广告收入、SaaS 市场规模——但计算可能是少数几个能够持久保持真正指数的指数之一，更类似于 GDP 或能源消耗（尽管能源最终也会呈指数增长）停滞）。到 2043 年，从后视镜来看，2023 年的计算市场显然会很小。<br />
<br />
当我在斯坦福大学读本科时，很少有人关注电气工程——它被认为是一个已解决的问题，相反，每个人都转向软件领域。但忽视硬件层是一个错误——我们有责任将计算硬件理解为最接近持久指数的事物之一。<br />
<br />
英伟达帝国<br />
<br />
许多大型科技公司都计划摆脱对 Nvidia 的依赖，但如果你想在当今大型基础模型的前沿竞争，Nvidia 是唯一实用的选择。<br />
<br />
我的朋友 Ben Gilbert 在这里做了一个关于 Nvidia 公司历史的精彩系列文章，简而言之：该公司成立于 1993 年，旨在将 3D 图形引入游戏，并于 1999 年发布了第一款 GPU。他们于 2006 年发明了 CUDA，允许并行处理跨多个 GPU，为开发用于训练更大工作负载的 GPU 集群奠定了基础。<br />
<br />
他们在超级计算机领域处于垄断地位，为全球 500 强超级计算机中的 74% 以及几乎所有大型人工智能训练集群提供动力。初创公司将其与公司的关系视为核心差异化因素（例如，率先购买芯片）。虽然 Nvidia 最初是一家消费 GPU 公司，但 Nvidia 2023 年旗帜的核心是数据中心业务。<br />
<br />
数据中心业务<br />
Nvidia 的数据中心业务销售用于 AI + 高性能计算工作负载的 GPU。第一季度的收入运行率达到 170 亿美元（占收入的 56%），第二季度的运行率翻了一番多（！），达到 410 亿美元（占收入的 76%）。华尔街分析师预计 2024 年（25 财年）将是又一个辉煌的一年，数据中心收入将达到 60-1000 亿美元。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9sU3llRGIwQUEtaUdNLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>