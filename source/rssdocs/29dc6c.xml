<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730485302593225108#m</id>
            <title>给你们展示个魔法，实时作画...

用的Freepik的Pikaso

拖拽小图标或者上传素材即可实时生图...

技术好的，可以自己画笔画，不过我感觉是内置了一些固定风格，不够自由！

体验地址：https://www.freepik.com/pikaso 

目前需要邀请码！（我看看能要到吗）</title>
            <link>https://nitter.cz/xiaohuggg/status/1730485302593225108#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730485302593225108#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 07:13:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>给你们展示个魔法，实时作画...<br />
<br />
用的Freepik的Pikaso<br />
<br />
拖拽小图标或者上传素材即可实时生图...<br />
<br />
技术好的，可以自己画笔画，不过我感觉是内置了一些固定风格，不够自由！<br />
<br />
体验地址：<a href="https://www.freepik.com/pikaso">freepik.com/pikaso</a> <br />
<br />
目前需要邀请码！（我看看能要到吗）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0ODIyOTQ5NzExMTc1NjgvcHUvaW1nLzgtMmVSOWpsaFc0VVVRb0QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476929500004627#m</id>
            <title>R to @xiaohuggg: 生成的另一种类型风格演示

你们可以玩玩</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476929500004627#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476929500004627#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:40:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>生成的另一种类型风格演示<br />
<br />
你们可以玩玩</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzY3OTI3MTEwOTAxNzYvcHUvaW1nLzIwbE13S2F4eFpoM0w2RmkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476711857586572#m</id>
            <title>R to @xiaohuggg: 这是上面演示视频中生成的音乐

🎵</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476711857586572#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476711857586572#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:39:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是上面演示视频中生成的音乐<br />
<br />
🎵</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzY1NDA2MjkzMjM3NzYvcHUvaW1nL2xrMEtwUHB3Mm9PeE9xbmsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476486820597883#m</id>
            <title>Stable Audio升级了一些新功能 生成音乐更可控

- 内置了风格提示库，随便点点就能生成音乐

- 支持通过上传音乐来生成音乐。

- 增加控制选项，如种子、步数、提示强度等

-能够生成并下载44.1 kHz立体声的高质量音频

- 现在可以直接通过链接分享生成的音乐

- 直接帮你把把生成好的音乐坐车了视频，方便下载

- 免费版本允许每月生成20个音轨，每个音轨最长45秒

- 付费版本每月11.99美元，提供500个音轨生成，每个音轨最长90秒

体验：http://stableaudio.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476486820597883#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476486820597883#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:38:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable Audio升级了一些新功能 生成音乐更可控<br />
<br />
- 内置了风格提示库，随便点点就能生成音乐<br />
<br />
- 支持通过上传音乐来生成音乐。<br />
<br />
- 增加控制选项，如种子、步数、提示强度等<br />
<br />
-能够生成并下载44.1 kHz立体声的高质量音频<br />
<br />
- 现在可以直接通过链接分享生成的音乐<br />
<br />
- 直接帮你把把生成好的音乐坐车了视频，方便下载<br />
<br />
- 免费版本允许每月生成20个音轨，每个音轨最长45秒<br />
<br />
- 付费版本每月11.99美元，提供500个音轨生成，每个音轨最长90秒<br />
<br />
体验：<a href="http://stableaudio.com/">stableaudio.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzU3ODEwNjYwMTQ3MjAvcHUvaW1nL1EtSGhfMnlLRllQdGxSYnguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730456579382849756#m</id>
            <title>R to @xiaohuggg: 里面的视频都是这样

包含一个第一人称视角和多个全方位的视角

让AI来学习人类是如何进行各种技能操作的</title>
            <link>https://nitter.cz/xiaohuggg/status/1730456579382849756#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730456579382849756#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 05:19:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>里面的视频都是这样<br />
<br />
包含一个第一人称视角和多个全方位的视角<br />
<br />
让AI来学习人类是如何进行各种技能操作的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NTYwNDUyNDY3MzAyNDAvcHUvaW1nL1I3a2VWN2ZJanVsYTlsUUouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730455784092549356#m</id>
            <title>Ego-Exo4D：用于视频学习和多模态感知研究的基础数据集

由Meta AI和15所大学共同开发，这个项目收集了两种类型的视频 ：

一种是戴着相机的人看到的（类似眼睛看世界）

另一种是从周围环境拍摄的（旁观者看这个人）

数据集目的是帮助AI更全面地学习和了解一个人在特定环境下是如何行动的。

Ego-Exo4D包含一个基础数据集和基准测试套件，旨在支持视频学习和多模态感知的研究。

主要特点：

1、首人称和第三人称视角的结合：

•Ego-Exo4D的核心是同时捕捉首人称“自我中心”视角（通过参与者佩戴的可穿戴相机）和多个“外心”视角（通过围绕参与者的相机）。
•这两种视角是互补的：自我中心视角揭示了参与者所看到和听到的内容，而外心视角揭示了周围的场景和上下文。

2、专注于熟练的人类活动：

•Ego-Exo4D专注于熟练的人类活动，如运动、音乐、烹饪、舞蹈和自行车维修。
•在视频中理解人类技能的AI进步可以促进许多应用，例如在增强现实（AR）系统中，佩戴智能眼镜的人可以通过虚拟AI教练的指导快速学习新技能。

3、数据集的规模和多样性：

•Ego-Exo4D构成了迄今为止最大的公共同步首人称和第三人称视频数据集。
•该数据集的建立需要招募不同领域的专家，汇集了多样化的人群来创建一个多方面的AI数据集。

4、多模态数据捕捉：

•使用Meta的独特Aria眼镜捕捉的所有自我中心视频都伴随着时间对齐的七通道音频、惯性测量单元（IMU）和两个广角灰度相机等传感器。
•所有数据序列还提供通过Project Aria的先进机器感知服务获得的眼动追踪、头部姿态和环境的3D点云。

5.基准测试和未来应用：

•除了数据，Ego-Exo4D还引入了基础任务的基准测试，以激发社区的努力。
•该项目的长远目标是使AI能够以新的方式帮助人们学习新技能，并为未来的机器人提供通过观察熟练的人类专家行动来获得复杂灵巧操作技能的洞察。

目的：

1、帮助研究人工智能：这些视频数据可以帮助科学家们训练和改进人工智能系统，让它们更好地理解人类的行为和技能，比如运动、音乐、烹饪、舞蹈和自行车维修。

2、应用于增强现实和机器人技术：这个项目的数据可以用来开发新的增强现实应用（比如通过智能眼镜学习新技能）和帮助机器人学习如何模仿人类的动作。

3、提供大量多样的视频：Ego-Exo4D收集了大量不同场景和活动的视频，这对于研究人工智能来说是一个宝贵的资源。

4、推动人工智能的新挑战：这个项目不仅提供数据，还设立了一些挑战，鼓励科学家们开发新技术，比如从视频中识别动作的细节，或者评估一个人做某件事的技能水平。

Ego-Exo4D项目就像是一个超级工具箱，可以帮助科学家们让人工智能更好地理解和模仿人类。

详细介绍：https://ai.meta.com/blog/ego-exo4d-video-learning-perception/
论文：https://ego-exo4d-data.org/paper/ego-exo4d.pdf
Ego-Exo4D网站：https://ego-exo4d-data.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730455784092549356#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730455784092549356#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 05:16:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ego-Exo4D：用于视频学习和多模态感知研究的基础数据集<br />
<br />
由Meta AI和15所大学共同开发，这个项目收集了两种类型的视频 ：<br />
<br />
一种是戴着相机的人看到的（类似眼睛看世界）<br />
<br />
另一种是从周围环境拍摄的（旁观者看这个人）<br />
<br />
数据集目的是帮助AI更全面地学习和了解一个人在特定环境下是如何行动的。<br />
<br />
Ego-Exo4D包含一个基础数据集和基准测试套件，旨在支持视频学习和多模态感知的研究。<br />
<br />
主要特点：<br />
<br />
1、首人称和第三人称视角的结合：<br />
<br />
•Ego-Exo4D的核心是同时捕捉首人称“自我中心”视角（通过参与者佩戴的可穿戴相机）和多个“外心”视角（通过围绕参与者的相机）。<br />
•这两种视角是互补的：自我中心视角揭示了参与者所看到和听到的内容，而外心视角揭示了周围的场景和上下文。<br />
<br />
2、专注于熟练的人类活动：<br />
<br />
•Ego-Exo4D专注于熟练的人类活动，如运动、音乐、烹饪、舞蹈和自行车维修。<br />
•在视频中理解人类技能的AI进步可以促进许多应用，例如在增强现实（AR）系统中，佩戴智能眼镜的人可以通过虚拟AI教练的指导快速学习新技能。<br />
<br />
3、数据集的规模和多样性：<br />
<br />
•Ego-Exo4D构成了迄今为止最大的公共同步首人称和第三人称视频数据集。<br />
•该数据集的建立需要招募不同领域的专家，汇集了多样化的人群来创建一个多方面的AI数据集。<br />
<br />
4、多模态数据捕捉：<br />
<br />
•使用Meta的独特Aria眼镜捕捉的所有自我中心视频都伴随着时间对齐的七通道音频、惯性测量单元（IMU）和两个广角灰度相机等传感器。<br />
•所有数据序列还提供通过Project Aria的先进机器感知服务获得的眼动追踪、头部姿态和环境的3D点云。<br />
<br />
5.基准测试和未来应用：<br />
<br />
•除了数据，Ego-Exo4D还引入了基础任务的基准测试，以激发社区的努力。<br />
•该项目的长远目标是使AI能够以新的方式帮助人们学习新技能，并为未来的机器人提供通过观察熟练的人类专家行动来获得复杂灵巧操作技能的洞察。<br />
<br />
目的：<br />
<br />
1、帮助研究人工智能：这些视频数据可以帮助科学家们训练和改进人工智能系统，让它们更好地理解人类的行为和技能，比如运动、音乐、烹饪、舞蹈和自行车维修。<br />
<br />
2、应用于增强现实和机器人技术：这个项目的数据可以用来开发新的增强现实应用（比如通过智能眼镜学习新技能）和帮助机器人学习如何模仿人类的动作。<br />
<br />
3、提供大量多样的视频：Ego-Exo4D收集了大量不同场景和活动的视频，这对于研究人工智能来说是一个宝贵的资源。<br />
<br />
4、推动人工智能的新挑战：这个项目不仅提供数据，还设立了一些挑战，鼓励科学家们开发新技术，比如从视频中识别动作的细节，或者评估一个人做某件事的技能水平。<br />
<br />
Ego-Exo4D项目就像是一个超级工具箱，可以帮助科学家们让人工智能更好地理解和模仿人类。<br />
<br />
详细介绍：<a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">ai.meta.com/blog/ego-exo4d-v…</a><br />
论文：<a href="https://ego-exo4d-data.org/paper/ego-exo4d.pdf">ego-exo4d-data.org/paper/ego…</a><br />
Ego-Exo4D网站：<a href="https://ego-exo4d-data.org/">ego-exo4d-data.org/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NTQ3NTYzNzg5OTI2NDAvcHUvaW1nL19KdHp5S2JPRV9uVjJiVEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730427886065324492#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1730427886065324492#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730427886065324492#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 03:25:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MjY2MDIyNzY5OTkxNjgvcHUvaW1nL3BUSlAteWNDcU5sSmQ5MmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730427883779404184#m</id>
            <title>Pikaso 即将推出的实时摄像头画画功能

利用摄像头可以实时进行图像生成

真实炫酷啊...

技术发展太快了...

@freepik 的老板 @ompemi 给了我一个邀请码，我一会录个视频...</title>
            <link>https://nitter.cz/xiaohuggg/status/1730427883779404184#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730427883779404184#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 03:25:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pikaso 即将推出的实时摄像头画画功能<br />
<br />
利用摄像头可以实时进行图像生成<br />
<br />
真实炫酷啊...<br />
<br />
技术发展太快了...<br />
<br />
<a href="https://nitter.cz/freepik" title="Freepik">@freepik</a> 的老板 <a href="https://nitter.cz/ompemi" title="Omar Pera">@ompemi</a> 给了我一个邀请码，我一会录个视频...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MjY1ODA1NjcyNzM0NzIvcHUvaW1nL0I3UkVybkt0UVNLYWxoc3AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730421265754927482#m</id>
            <title>空间计算技术如何彻底改变我们观看体育赛事的方式?

LIVEPLEX展示了通过空间计算技术，你可以在家沉浸式观看体育赛事的演示。

你可以任意改变观看角度，近距离观察喜欢的运动员，甚至从教练的角度分析比赛。

这不仅仅是观看比赛，更是一种全新的体验方式，以前只有亲自到场才能体验到。

这项技术将在2024年开始推广应用。

主要技术特点：

- 互动性增强：空间计算提供了一种更加互动和沉浸式的观看体验。

- 多角度观看：观众可以自由改变观看角度，获得不同的视角。

- 深入体验：提供了一种近乎身临其境的体验，让观众能够更深入地了解和体验比赛。

这项技术不仅改变了观看体育赛事的方式，也为家庭娱乐和体育传播带来了新的可能性。

空间计算是一种集成了增强现实（AR）、虚拟现实（VR）、计算机视觉和人工智能（AI）等多个技术的先进计算形式。它的核心在于创造一个能够与现实世界交互的数字环境。

以下是空间计算的一些关键技术原理：

增强现实（AR）和虚拟现实（VR）：AR通过在用户的现实世界视野中叠加数字信息来增强现实体验。
VR则是创造一个完全虚拟的环境，用户可以在其中进行交互。

在观看体育赛事的应用中，这些技术可以用来创建一个仿佛置身于体育场的体验，允许用户从不同角度观看比赛。

计算机视觉：计算机视觉技术使计算机能够理解和解释数字图像和视频。在空间计算中，这项技术用于追踪用户的动作和环境，以便适当地调整数字内容。

人工智能（AI）：AI在空间计算中用于处理和响应用户的行为，以及优化用户体验。AI可以用于个性化设置，比如根据用户的偏好调整视角或提供特定的游戏分析。

传感器和追踪技术：空间计算设备通常配备有多种传感器，如摄像头、红外传感器和运动追踪器。
这些传感器帮助系统理解用户的位置和动作，从而提供适应用户移动的交互体验。

空间计算通过结合这些技术，创造了一种新的交互方式，使用户能够以前所未有的方式体验数字内容。在观看体育赛事的应用中，这意味着用户可以获得更加沉浸和互动的体验，仿佛亲自在现场一样。</title>
            <link>https://nitter.cz/xiaohuggg/status/1730421265754927482#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730421265754927482#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 02:59:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>空间计算技术如何彻底改变我们观看体育赛事的方式?<br />
<br />
LIVEPLEX展示了通过空间计算技术，你可以在家沉浸式观看体育赛事的演示。<br />
<br />
你可以任意改变观看角度，近距离观察喜欢的运动员，甚至从教练的角度分析比赛。<br />
<br />
这不仅仅是观看比赛，更是一种全新的体验方式，以前只有亲自到场才能体验到。<br />
<br />
这项技术将在2024年开始推广应用。<br />
<br />
主要技术特点：<br />
<br />
- 互动性增强：空间计算提供了一种更加互动和沉浸式的观看体验。<br />
<br />
- 多角度观看：观众可以自由改变观看角度，获得不同的视角。<br />
<br />
- 深入体验：提供了一种近乎身临其境的体验，让观众能够更深入地了解和体验比赛。<br />
<br />
这项技术不仅改变了观看体育赛事的方式，也为家庭娱乐和体育传播带来了新的可能性。<br />
<br />
空间计算是一种集成了增强现实（AR）、虚拟现实（VR）、计算机视觉和人工智能（AI）等多个技术的先进计算形式。它的核心在于创造一个能够与现实世界交互的数字环境。<br />
<br />
以下是空间计算的一些关键技术原理：<br />
<br />
增强现实（AR）和虚拟现实（VR）：AR通过在用户的现实世界视野中叠加数字信息来增强现实体验。<br />
VR则是创造一个完全虚拟的环境，用户可以在其中进行交互。<br />
<br />
在观看体育赛事的应用中，这些技术可以用来创建一个仿佛置身于体育场的体验，允许用户从不同角度观看比赛。<br />
<br />
计算机视觉：计算机视觉技术使计算机能够理解和解释数字图像和视频。在空间计算中，这项技术用于追踪用户的动作和环境，以便适当地调整数字内容。<br />
<br />
人工智能（AI）：AI在空间计算中用于处理和响应用户的行为，以及优化用户体验。AI可以用于个性化设置，比如根据用户的偏好调整视角或提供特定的游戏分析。<br />
<br />
传感器和追踪技术：空间计算设备通常配备有多种传感器，如摄像头、红外传感器和运动追踪器。<br />
这些传感器帮助系统理解用户的位置和动作，从而提供适应用户移动的交互体验。<br />
<br />
空间计算通过结合这些技术，创造了一种新的交互方式，使用户能够以前所未有的方式体验数字内容。在观看体育赛事的应用中，这意味着用户可以获得更加沉浸和互动的体验，仿佛亲自在现场一样。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MTk4MDEwOTE0MjQyNTYvcHUvaW1nL2RaNmZIQlJCODZOZExaRWguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730405657994780909#m</id>
            <title>R to @xiaohuggg: 演示视频：

Whispering 耳语</title>
            <link>https://nitter.cz/xiaohuggg/status/1730405657994780909#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730405657994780909#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 01:57:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频：<br />
<br />
Whispering 耳语</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MDU2MzQ0MjI4NDk1MzYvcHUvaW1nL19CZXNXMVhkam5odXJ0R3kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730405325399105677#m</id>
            <title>R to @xiaohuggg: 演示视频：

Excited 兴奋的</title>
            <link>https://nitter.cz/xiaohuggg/status/1730405325399105677#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730405325399105677#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 01:55:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频：<br />
<br />
Excited 兴奋的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MDUyNDI4OTI5MjY5NzYvcHUvaW1nL0RCMnVRbFNxdFJYeHhqRk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730404081972461921#m</id>
            <title>Meta AI 发布实时人工智能语言翻译模型：Seamless

这个模型统一了之前的三个Seamless系列模型，可以实时翻译100多种语言，延迟不到2秒钟，说话者仍在讲话时就开始翻译。

Seamless翻译不仅仅是文字上的转换，还能保持说话者的情感和语气、语调等，使得翻译后的语音更加自然和真实。

主要特点：

1、保持原声情感：SeamlessExpressive模型专注于在语音到语音翻译中保持原始语音的表达性，包括语调、情感和风格。保留说话人的语气和情感。

2、实时翻译：实时翻译功能，大约只有两秒的延迟。与传统的翻译系统相比，它在说话者仍在讲话时就开始翻译，使得对话更加流畅和自然。

3、支持多种语言：支持近100种输入和输出语言的自动语音识别和语音到文本翻译，以及近100种输入语言和36种输出语言的语音到语音翻译。

4、毒性缓解和准确性：在构建AI翻译系统时，Meta特别关注准确性和避免误解。他们探索了如何减少翻译过程中可能出现的错误和不当内容，这对于确保沟通的质量和安全性至关重要。

5、音频水印技术：为了防止滥用和模仿，Meta还开发了一种音频水印技术。这种技术可以在不被人耳察觉的情况下嵌入音频，以确保音频来源的可追溯性。

Seamless模型统一了SeamlessExpressive、SeamlessStreaming和SeamlessM4T v2的功能。旨在实现多语言、表达性和流畅的语音翻译。

这些模型的主要特点和功能：

SeamlessM4T v2：这是一个改进版的大规模多语言和多模态翻译模型。提高了语音生成任务的质量和推理延迟。它基于更新的UnitY2框架，训练了更多低资源语言数据。SeamlessM4T v2为其他模型提供了基础。

SeamlessM4T v2 实现了 100 种语言的最先进的语音到语音和语音到文本结果的翻译。在同一模型中，它在平均自动语音识别方面也击败了 Whisper v3，特别是对于资源较低的语言。

SeamlessM4T v2 与8 月份发布的模型相比提高了 10%，在翻译成英语时比最强的级联模型提高了 17% 以上。对于语音到语音翻译，SeamlessM4T v2 在翻译成英语时比 SeamlessM4T (v1) 提高了 15% 以上，在从英语翻译时提高了 25%。

支持以下任务：
•语音到语音翻译（S2ST）
•语音到文本翻译（S2TT）
•文本到语音翻译（T2ST）
•文本到文本翻译（T2TT）
•自动语音识别（ASR）

SeamlessExpressive：这个模型能够在翻译过程中保持声音的风格和韵律。与以往的表达性语音研究相比，SeamlessExpressive关注了一些未被充分探索的韵律方面，如语速和停顿，同时还保留了说话者的声音风格。

SeamlessStreaming：

这是一个流式翻译模型，支持语音输入和语音/文本输出。它支持以下任务：
•语音到语音翻译（S2ST）
•语音到文本翻译（S2TT）
•自动语音识别（ASR）

这个模型利用高效单调多头注意力（EMMA）机制，能够在不等待完整源语句的情况下生成低延迟的目标翻译。SeamlessStreaming是首个能够实现多种源语言和目标语言的同时语音到语音/文本翻译的模型。

Meta AI还发布了一系列与Seamless Communication项目相关的元数据、数据和数据对齐工具，以支持研究社区。

SeamlessAlign扩展的元数据：包含额外的115,000小时语音和文本对齐数据，加上现有的470,000小时。这个最新版本的SeamlessAlign涵盖了更广泛的语言范围，从之前的37种增加到76种。这个语料库是迄今为止总体积和语言覆盖范围最大的公共语音/语音和语音/文本平行语料库。

详细介绍：https://ai.meta.com/blog/seamless-communication
官方网站：https://ai.meta.com/research/seamless-communication/
论文：https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/
GitHub：https://github.com/facebookresearch/seamless_communication
在线体验：https://seamless.metademolab.com/expressive?utm_source=metaai&amp;utm_medium=web&amp;utm_campaign=fair10&amp;utm_content=blog</title>
            <link>https://nitter.cz/xiaohuggg/status/1730404081972461921#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730404081972461921#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 01:50:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta AI 发布实时人工智能语言翻译模型：Seamless<br />
<br />
这个模型统一了之前的三个Seamless系列模型，可以实时翻译100多种语言，延迟不到2秒钟，说话者仍在讲话时就开始翻译。<br />
<br />
Seamless翻译不仅仅是文字上的转换，还能保持说话者的情感和语气、语调等，使得翻译后的语音更加自然和真实。<br />
<br />
主要特点：<br />
<br />
1、保持原声情感：SeamlessExpressive模型专注于在语音到语音翻译中保持原始语音的表达性，包括语调、情感和风格。保留说话人的语气和情感。<br />
<br />
2、实时翻译：实时翻译功能，大约只有两秒的延迟。与传统的翻译系统相比，它在说话者仍在讲话时就开始翻译，使得对话更加流畅和自然。<br />
<br />
3、支持多种语言：支持近100种输入和输出语言的自动语音识别和语音到文本翻译，以及近100种输入语言和36种输出语言的语音到语音翻译。<br />
<br />
4、毒性缓解和准确性：在构建AI翻译系统时，Meta特别关注准确性和避免误解。他们探索了如何减少翻译过程中可能出现的错误和不当内容，这对于确保沟通的质量和安全性至关重要。<br />
<br />
5、音频水印技术：为了防止滥用和模仿，Meta还开发了一种音频水印技术。这种技术可以在不被人耳察觉的情况下嵌入音频，以确保音频来源的可追溯性。<br />
<br />
Seamless模型统一了SeamlessExpressive、SeamlessStreaming和SeamlessM4T v2的功能。旨在实现多语言、表达性和流畅的语音翻译。<br />
<br />
这些模型的主要特点和功能：<br />
<br />
SeamlessM4T v2：这是一个改进版的大规模多语言和多模态翻译模型。提高了语音生成任务的质量和推理延迟。它基于更新的UnitY2框架，训练了更多低资源语言数据。SeamlessM4T v2为其他模型提供了基础。<br />
<br />
SeamlessM4T v2 实现了 100 种语言的最先进的语音到语音和语音到文本结果的翻译。在同一模型中，它在平均自动语音识别方面也击败了 Whisper v3，特别是对于资源较低的语言。<br />
<br />
SeamlessM4T v2 与8 月份发布的模型相比提高了 10%，在翻译成英语时比最强的级联模型提高了 17% 以上。对于语音到语音翻译，SeamlessM4T v2 在翻译成英语时比 SeamlessM4T (v1) 提高了 15% 以上，在从英语翻译时提高了 25%。<br />
<br />
支持以下任务：<br />
•语音到语音翻译（S2ST）<br />
•语音到文本翻译（S2TT）<br />
•文本到语音翻译（T2ST）<br />
•文本到文本翻译（T2TT）<br />
•自动语音识别（ASR）<br />
<br />
SeamlessExpressive：这个模型能够在翻译过程中保持声音的风格和韵律。与以往的表达性语音研究相比，SeamlessExpressive关注了一些未被充分探索的韵律方面，如语速和停顿，同时还保留了说话者的声音风格。<br />
<br />
SeamlessStreaming：<br />
<br />
这是一个流式翻译模型，支持语音输入和语音/文本输出。它支持以下任务：<br />
•语音到语音翻译（S2ST）<br />
•语音到文本翻译（S2TT）<br />
•自动语音识别（ASR）<br />
<br />
这个模型利用高效单调多头注意力（EMMA）机制，能够在不等待完整源语句的情况下生成低延迟的目标翻译。SeamlessStreaming是首个能够实现多种源语言和目标语言的同时语音到语音/文本翻译的模型。<br />
<br />
Meta AI还发布了一系列与Seamless Communication项目相关的元数据、数据和数据对齐工具，以支持研究社区。<br />
<br />
SeamlessAlign扩展的元数据：包含额外的115,000小时语音和文本对齐数据，加上现有的470,000小时。这个最新版本的SeamlessAlign涵盖了更广泛的语言范围，从之前的37种增加到76种。这个语料库是迄今为止总体积和语言覆盖范围最大的公共语音/语音和语音/文本平行语料库。<br />
<br />
详细介绍：<a href="https://ai.meta.com/blog/seamless-communication">ai.meta.com/blog/seamless-co…</a><br />
官方网站：<a href="https://ai.meta.com/research/seamless-communication/">ai.meta.com/research/seamles…</a><br />
论文：<a href="https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/">ai.meta.com/research/publica…</a><br />
GitHub：<a href="https://github.com/facebookresearch/seamless_communication">github.com/facebookresearch/…</a><br />
在线体验：<a href="https://seamless.metademolab.com/expressive?utm_source=metaai&amp;utm_medium=web&amp;utm_campaign=fair10&amp;utm_content=blog">seamless.metademolab.com/exp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MDI1NjUzODc2NTcyMTYvcHUvaW1nL2pqWjVXN2Z0NGg1VUcySFIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730383764604485682#m</id>
            <title>R to @xiaohuggg: 一些外观照片</title>
            <link>https://nitter.cz/xiaohuggg/status/1730383764604485682#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730383764604485682#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 00:30:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些外观照片</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJ6cmFZQUFiMnNKLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJ5M2FvQUFWMU9YLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJ6dGFnQUF4bTFtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJ6eGFnQUFhcGF5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730383751233130992#m</id>
            <title>R to @xiaohuggg: 还有个电池扩展包

安装后续可以增加到755公里

还可以反向给其他车辆充电🔋😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1730383751233130992#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730383751233130992#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 00:30:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还有个电池扩展包<br />
<br />
安装后续可以增加到755公里<br />
<br />
还可以反向给其他车辆充电🔋😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJEamFRQUFLdG1FLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJEcGFzQUEwbG1BLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJEbmJjQUFSUkE4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzJEU2FrQUFkVkUyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730383738666942972#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1730383738666942972#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730383738666942972#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 00:30:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzFZX2J3QUFXZUw4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzFZN2JBQUFWbjMtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzFZX2J3QUVkazBkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730383727010959519#m</id>
            <title>R to @xiaohuggg: 内饰照片</title>
            <link>https://nitter.cz/xiaohuggg/status/1730383727010959519#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730383727010959519#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 00:30:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>内饰照片</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzB2dWF3QUF1U0JoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzB2dGJ3QUE5OVJTLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzB3TmJFQUFsRnNLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzB2d2JrQUF0b0tCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730383715921203297#m</id>
            <title>R to @xiaohuggg: 一些扩展</title>
            <link>https://nitter.cz/xiaohuggg/status/1730383715921203297#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730383715921203297#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 00:30:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些扩展</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzBIcmJZQUFyRlJsLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzBHemJVQUFKaDdpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzBIcGFrQUVkd2RXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FPTzBIcGFBQUEta251LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730383705108254848#m</id>
            <title>特斯拉举行 #Cybertruck 交付仪式  6分钟中英双语视频回看📺

Cybertruck配置和定价：

后轮驱动版本60990美元（约43.5万人民币），续航250英里（约402公里）

四驱版：79990美元（约57万人民币）续航340英里（547公里），0-60英里/小时加速时间为4.1秒，最高时速达112英里（180公里）

野兽版：99990美元（约71.4万人民币）三电机，845马力，扭矩达10,296磅·英尺，续航里程约为320英里（约514公里）

后轮驱动版本将于2025年上市。双电机和三电机版本将于2024年上市。</title>
            <link>https://nitter.cz/xiaohuggg/status/1730383705108254848#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730383705108254848#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 00:29:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>特斯拉举行 <a href="https://nitter.cz/search?q=%23Cybertruck">#Cybertruck</a> 交付仪式  6分钟中英双语视频回看📺<br />
<br />
Cybertruck配置和定价：<br />
<br />
后轮驱动版本60990美元（约43.5万人民币），续航250英里（约402公里）<br />
<br />
四驱版：79990美元（约57万人民币）续航340英里（547公里），0-60英里/小时加速时间为4.1秒，最高时速达112英里（180公里）<br />
<br />
野兽版：99990美元（约71.4万人民币）三电机，845马力，扭矩达10,296磅·英尺，续航里程约为320英里（约514公里）<br />
<br />
后轮驱动版本将于2025年上市。双电机和三电机版本将于2024年上市。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMwMzgyODY2OTY0NzA5Mzc2L2ltZy9OekxzYzhUVHNabk9ueEVMLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730249158605594870#m</id>
            <title>WeChatMsg：一个提取微信聊天记录的工具

它可以将微信聊天记录导出为HTML、Word、CSV格式的文档，并对聊天记录进行分析，生成年度聊天报告等！

感觉可以用来导入到GPTs，克隆一个自己😂

主要功能：

1.破解微信数据库：该工具能够破解手机（安卓或苹果）和PC端微信的数据库。

2.还原微信聊天界面：支持将文本、图片和表情包等内容还原为微信聊天界面的样式。

3.导出聊天记录：用户可以将聊天记录导出为HTML、Word、CSV等格式的文档，方便永久保存。

4.聊天数据分析：工具提供了对聊天数据的分析功能，可以生成可视化的年度聊天报告。

5.项目正在持续更新中，未来可能会支持更多功能，如导出全部表情包、合并多个备份数据等。

GitHub：https://github.com/LC044/WeChatMsg</title>
            <link>https://nitter.cz/xiaohuggg/status/1730249158605594870#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730249158605594870#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 15:35:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WeChatMsg：一个提取微信聊天记录的工具<br />
<br />
它可以将微信聊天记录导出为HTML、Word、CSV格式的文档，并对聊天记录进行分析，生成年度聊天报告等！<br />
<br />
感觉可以用来导入到GPTs，克隆一个自己😂<br />
<br />
主要功能：<br />
<br />
1.破解微信数据库：该工具能够破解手机（安卓或苹果）和PC端微信的数据库。<br />
<br />
2.还原微信聊天界面：支持将文本、图片和表情包等内容还原为微信聊天界面的样式。<br />
<br />
3.导出聊天记录：用户可以将聊天记录导出为HTML、Word、CSV等格式的文档，方便永久保存。<br />
<br />
4.聊天数据分析：工具提供了对聊天数据的分析功能，可以生成可视化的年度聊天报告。<br />
<br />
5.项目正在持续更新中，未来可能会支持更多功能，如导出全部表情包、合并多个备份数据等。<br />
<br />
GitHub：<a href="https://github.com/LC044/WeChatMsg">github.com/LC044/WeChatMsg</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FNVWJ6cWJJQUE2djVOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730236358436720819#m</id>
            <title>R to @xiaohuggg: 几分钟前 Stability AI CEO

辟谣说没有出售计划，也没接触过人任何收购方

😂 https://x.com/emostaque/status/1730233378169934095?</title>
            <link>https://nitter.cz/xiaohuggg/status/1730236358436720819#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730236358436720819#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 Nov 2023 14:44:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>几分钟前 Stability AI CEO<br />
<br />
辟谣说没有出售计划，也没接触过人任何收购方<br />
<br />
😂 <a href="https://x.com/emostaque/status/1730233378169934095">x.com/emostaque/status/17302…</a>?</p>
<p><a href="https://nitter.cz/EMostaque/status/1730233378169934095#m">nitter.cz/EMostaque/status/1730233378169934095#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>