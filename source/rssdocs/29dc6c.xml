<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732224902302945612#m</id>
            <title>将DALL·E 3 与草图软件@tldraw 集成

写一个主Prompt 然后使用链条来控制图像生成…😎

在链条上输入关键词或者其他辅助prompt即可生成图片

同时还能将多个链条合并组合，合并图片🫡

这样是不是可以可视化的保证图片一致性？？？</title>
            <link>https://nitter.cz/xiaohuggg/status/1732224902302945612#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732224902302945612#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 02:26:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将DALL·E 3 与草图软件<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 集成<br />
<br />
写一个主Prompt 然后使用链条来控制图像生成…😎<br />
<br />
在链条上输入关键词或者其他辅助prompt即可生成图片<br />
<br />
同时还能将多个链条合并组合，合并图片🫡<br />
<br />
这样是不是可以可视化的保证图片一致性？？？</p>
<p><a href="https://nitter.cz/miiura/status/1732040947477987555#m">nitter.cz/miiura/status/1732040947477987555#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732208782330179981#m</id>
            <title>每一次技术革命都是生产资料再分配的过程，但最终还是会集中在少数人手中！

很少惠及普罗大众！

要想惠及普罗大众就要改变社会统治结构！

那么就是要由AI来统治人类！所以AI最后能否上位？😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1732208782330179981#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732208782330179981#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 01:22:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>每一次技术革命都是生产资料再分配的过程，但最终还是会集中在少数人手中！<br />
<br />
很少惠及普罗大众！<br />
<br />
要想惠及普罗大众就要改变社会统治结构！<br />
<br />
那么就是要由AI来统治人类！所以AI最后能否上位？😎</p>
<p><a href="https://nitter.cz/dotey/status/1732076418903789628#m">nitter.cz/dotey/status/1732076418903789628#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732050840515719654#m</id>
            <title>什么是量子计算？

量子计算机的工作原理是什么？

量子计算机和传统计算机有什么不同？

看完这个视频你就懂了，这个是给小朋友科普的，我想你也应该能看懂！😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1732050840515719654#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732050840515719654#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 14:54:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>什么是量子计算？<br />
<br />
量子计算机的工作原理是什么？<br />
<br />
量子计算机和传统计算机有什么不同？<br />
<br />
看完这个视频你就懂了，这个是给小朋友科普的，我想你也应该能看懂！😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMyMDUwNTQyNzc4NzkzOTg0L2ltZy9zWGFQbjE1OWhTd2gwX1EtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732032732644196698#m</id>
            <title>R to @xiaohuggg: 和微软的这个一样

https://x.com/xiaohuggg/status/1730547607716643080</title>
            <link>https://nitter.cz/xiaohuggg/status/1732032732644196698#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732032732644196698#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 13:42:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>和微软的这个一样<br />
<br />
<a href="https://x.com/xiaohuggg/status/1730547607716643080">x.com/xiaohuggg/status/17305…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730547607716643080#m">nitter.cz/xiaohuggg/status/1730547607716643080#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732026172509421697#m</id>
            <title>VividTalk：单张照片+一段音频即可让照片说话

你只需要提供一张人物的静态照片和一段语音录音，VividTalk就能将它们结合起来，制作出一个看起来像是实际说话的人物的视频。

而且面部表情和头部动作都很自然，口型可以同步、支持多种语言，不同风格，如真实风格、卡通风格等。

该项目由由南京大学、阿里巴巴、字节跳动和南开大学共同开发。

和前几天我发的微软的这个项目几乎一样：https://twitter.com/xiaohuggg/status/1730547607716643080

VividTalk通过先进的音频到3D网格映射技术和网格到视频的转换技术，实现了高质量、逼真的音频驱动的说话头像视频生成。

其工作原理的详细说明：

1、音频到网格的映射（第一阶段）：

在这一阶段，VividTalk首先将输入的音频映射到3D网格上。这涉及学习两种类型的运动：非刚性表情运动和刚性头部运动。

对于表情运动，技术使用混合形状（blendshape）和顶点作为中间表示，以最大化模型的表示能力。混合形状提供了全局的粗略运动，而顶点偏移则描述了更细致的嘴唇运动。

对于自然的头部运动，VividTalk提出了一个新颖的可学习的头部姿势代码本，采用了两阶段训练机制。

2、网格到视频的转换（第二阶段）：

在第二阶段，VividTalk使用双分支运动-VAE（变分自编码器）和生成器将学习到的网格转换为密集的运动，并基于这些运动逐帧合成高质量的视频。

这一过程涉及将3D网格的运动转换为2D密集运动，然后输入到生成器中，以合成最终的视频帧。

3、高视觉质量和真实感：

VividTalk生成的视频具有高视觉质量，包括逼真的面部表情、多样的头部姿势，并且在嘴唇同步方面有显著提升。

通过这种方法，VividTalk能够生成与输入音频高度同步的逼真说话头像视频，提高了视频的真实感和动态性。

项目及演示：https://humanaigc.github.io/vivid-talk/
论文：https://arxiv.org/pdf/2312.01841.pdf
GitHub：https://github.com/HumanAIGC/VividTalk</title>
            <link>https://nitter.cz/xiaohuggg/status/1732026172509421697#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732026172509421697#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 13:16:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VividTalk：单张照片+一段音频即可让照片说话<br />
<br />
你只需要提供一张人物的静态照片和一段语音录音，VividTalk就能将它们结合起来，制作出一个看起来像是实际说话的人物的视频。<br />
<br />
而且面部表情和头部动作都很自然，口型可以同步、支持多种语言，不同风格，如真实风格、卡通风格等。<br />
<br />
该项目由由南京大学、阿里巴巴、字节跳动和南开大学共同开发。<br />
<br />
和前几天我发的微软的这个项目几乎一样：<a href="https://nitter.cz/xiaohuggg/status/1730547607716643080">nitter.cz/xiaohuggg/status…</a><br />
<br />
VividTalk通过先进的音频到3D网格映射技术和网格到视频的转换技术，实现了高质量、逼真的音频驱动的说话头像视频生成。<br />
<br />
其工作原理的详细说明：<br />
<br />
1、音频到网格的映射（第一阶段）：<br />
<br />
在这一阶段，VividTalk首先将输入的音频映射到3D网格上。这涉及学习两种类型的运动：非刚性表情运动和刚性头部运动。<br />
<br />
对于表情运动，技术使用混合形状（blendshape）和顶点作为中间表示，以最大化模型的表示能力。混合形状提供了全局的粗略运动，而顶点偏移则描述了更细致的嘴唇运动。<br />
<br />
对于自然的头部运动，VividTalk提出了一个新颖的可学习的头部姿势代码本，采用了两阶段训练机制。<br />
<br />
2、网格到视频的转换（第二阶段）：<br />
<br />
在第二阶段，VividTalk使用双分支运动-VAE（变分自编码器）和生成器将学习到的网格转换为密集的运动，并基于这些运动逐帧合成高质量的视频。<br />
<br />
这一过程涉及将3D网格的运动转换为2D密集运动，然后输入到生成器中，以合成最终的视频帧。<br />
<br />
3、高视觉质量和真实感：<br />
<br />
VividTalk生成的视频具有高视觉质量，包括逼真的面部表情、多样的头部姿势，并且在嘴唇同步方面有显著提升。<br />
<br />
通过这种方法，VividTalk能够生成与输入音频高度同步的逼真说话头像视频，提高了视频的真实感和动态性。<br />
<br />
项目及演示：<a href="https://humanaigc.github.io/vivid-talk/">humanaigc.github.io/vivid-ta…</a><br />
论文：<a href="https://arxiv.org/pdf/2312.01841.pdf">arxiv.org/pdf/2312.01841.pdf</a><br />
GitHub：<a href="https://github.com/HumanAIGC/VividTalk">github.com/HumanAIGC/VividTa…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIwMTk4ODU4MzQ5MzYzMjAvcHUvaW1nL1NYalFmdkNJNVRTVTlBdTcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732014004950974917#m</id>
            <title>HeyGen推出Avatar2.0 ：Instant Avatar 即时虚拟分身

- 只需要5分钟，使用手机即可创造一个自己的虚拟分身。

- 多语言支持：通过内置的翻译工具，支持创建多语言内容。

- 口型同步：支持口型同步和多语言声音匹配

- 免费使用：而且这项服务是免费的...

用户可以利用这项技术制作可扩展的、定制化的视频内容，同步到全球所有的平台。

详细：https://www.heygen.com/article/introducing-avatar-2-0-instant-avatar</title>
            <link>https://nitter.cz/xiaohuggg/status/1732014004950974917#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732014004950974917#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 12:28:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HeyGen推出Avatar2.0 ：Instant Avatar 即时虚拟分身<br />
<br />
- 只需要5分钟，使用手机即可创造一个自己的虚拟分身。<br />
<br />
- 多语言支持：通过内置的翻译工具，支持创建多语言内容。<br />
<br />
- 口型同步：支持口型同步和多语言声音匹配<br />
<br />
- 免费使用：而且这项服务是免费的...<br />
<br />
用户可以利用这项技术制作可扩展的、定制化的视频内容，同步到全球所有的平台。<br />
<br />
详细：<a href="https://www.heygen.com/article/introducing-avatar-2-0-instant-avatar">heygen.com/article/introduci…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIwMTMwOTk2NDgxODQzMjAvcHUvaW1nL2hqa3lfVDMxcFptRGNCX3YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731970985014780212#m</id>
            <title>R to @xiaohuggg: 这应用不就来了

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1731970985014780212#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731970985014780212#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 09:37:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这应用不就来了<br />
<br />
😂</p>
<p><a href="https://nitter.cz/bdsqlsz/status/1731863081578336704#m">nitter.cz/bdsqlsz/status/1731863081578336704#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731950689482428603#m</id>
            <title>R to @xiaohuggg: MetaHuman渲染 和 AI渲染对比</title>
            <link>https://nitter.cz/xiaohuggg/status/1731950689482428603#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731950689482428603#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 08:16:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MetaHuman渲染 和 AI渲染对比</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE5NTAzNzczNTkzMzEzMjgvcHUvaW1nL2hwTjNEekFQVjk1MXNqd0MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731950309042229688#m</id>
            <title>使用ComfyUI  + SD + AnimateDiff

都能做出这种效果了？？？

😐

作者@DreamStarter_1 将很快公布制作方法 …</title>
            <link>https://nitter.cz/xiaohuggg/status/1731950309042229688#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731950309042229688#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 08:15:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用ComfyUI  + SD + AnimateDiff<br />
<br />
都能做出这种效果了？？？<br />
<br />
😐<br />
<br />
作者<a href="https://nitter.cz/DreamStarter_1" title="DreamStarter">@DreamStarter_1</a> 将很快公布制作方法 …</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FmNGdQSlhFQUFmOG9CLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731943630892577188#m</id>
            <title>R to @xiaohuggg: IBM 研究主管达里奥·吉尔 (Dario Gil) 表示：“这台机器不同于我们曾经制造过的任何机器。” 

@60Minutes 节目报道了 IBM 最新的量子计算机，他们采访了整个建造过程，并解释了什么是量子计算机以及它的意义。

这是迄今为止最先进的量子计算机。

详细内容：https://www.cbsnews.com/news/quantum-computing-google-ibm-advances-60-minutes-transcript/?ftag=CNM-00-10aab7d&amp;linkId=252720847</title>
            <link>https://nitter.cz/xiaohuggg/status/1731943630892577188#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731943630892577188#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 07:48:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>IBM 研究主管达里奥·吉尔 (Dario Gil) 表示：“这台机器不同于我们曾经制造过的任何机器。” <br />
<br />
<a href="https://nitter.cz/60Minutes" title="60 Minutes">@60Minutes</a> 节目报道了 IBM 最新的量子计算机，他们采访了整个建造过程，并解释了什么是量子计算机以及它的意义。<br />
<br />
这是迄今为止最先进的量子计算机。<br />
<br />
详细内容：<a href="https://www.cbsnews.com/news/quantum-computing-google-ibm-advances-60-minutes-transcript/?ftag=CNM-00-10aab7d&amp;linkId=252720847">cbsnews.com/news/quantum-com…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE5NDMxMTAzNzUzMzc5ODQvcHUvaW1nL1dNWDg1RDVWY29oc1JteVMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731943053928341690#m</id>
            <title>IBM在其年度量子峰会上宣布了一系列重大量子计算进展。

IBM发布了全球首个模块化规模实用化的量子计算机IBM Quantum System 2。

以及下一代量子处理器IBM Condor和Heron，其中Condor拥有1121个超导量子位。Heron拥有133个固定频率的量子位。

量子位（Qubits）：量子计算机使用量子位（或称为qubits）来存储信息。每增加一个量子位，计算机的能力就会加倍，这是指数级增长。

主要内容：

1、2、IBM Condor量子处理器：IBM Condor是一款拥有1,121个超导量子位的新型量子处理器，基于IBM的交叉共振门技术。Condor在芯片设计的规模和产量上推动了极限，量子位密度提高了50%，在量子位制造和层压板尺寸方面取得了进步，并在单个稀释制冷器内包含了超过一英里的高密度低温柔性IO线路。

IBM Quantum Heron处理器：这是IBM迄今为止错误率最低的量子计算芯片。IBM Quantum Heron处理器是IBM首款拥有133个固定频率量子位的处理器，具有可调节的耦合器，其设备性能比之前的处理器提高了3-5倍，几乎消除了串扰。这项技术有望成为未来硬件开发的基础。

3、IBM Quantum System Two：IBM Quantum System Two是可扩展量子计算的基石，结合了低温基础设施、第三代控制电子设备和经典运行时服务器。这个系统将用于实现量子中心超级计算的并行电路执行。

4、Qiskit 1.0：IBM宣布了Qiskit 1.0，这是世界上最广泛使用的开源量子编程软件。它具有新功能，帮助计算科学家更容易和更快地执行量子电路。

5、生成式AI模型：IBM展示了工程化的生成式AI模型，用于自动化量子代码开发，并优化量子电路。

6、量子发展路线图至2033年：IBM发布了延伸至2033年的量子发展路线图，设定了新的目标，以显著提高门操作的质量。这样做将增加能够运行的量子电路的大小，并有助于实现大规模量子计算的全部潜力。

详细：https://newsroom.ibm.com/2023-12-04-IBM-Debuts-Next-Generation-Quantum-Processor-IBM-Quantum-System-Two,-Extends-Roadmap-to-Advance-Era-of-Quantum-Utility</title>
            <link>https://nitter.cz/xiaohuggg/status/1731943053928341690#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731943053928341690#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 07:46:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>IBM在其年度量子峰会上宣布了一系列重大量子计算进展。<br />
<br />
IBM发布了全球首个模块化规模实用化的量子计算机IBM Quantum System 2。<br />
<br />
以及下一代量子处理器IBM Condor和Heron，其中Condor拥有1121个超导量子位。Heron拥有133个固定频率的量子位。<br />
<br />
量子位（Qubits）：量子计算机使用量子位（或称为qubits）来存储信息。每增加一个量子位，计算机的能力就会加倍，这是指数级增长。<br />
<br />
主要内容：<br />
<br />
1、2、IBM Condor量子处理器：IBM Condor是一款拥有1,121个超导量子位的新型量子处理器，基于IBM的交叉共振门技术。Condor在芯片设计的规模和产量上推动了极限，量子位密度提高了50%，在量子位制造和层压板尺寸方面取得了进步，并在单个稀释制冷器内包含了超过一英里的高密度低温柔性IO线路。<br />
<br />
IBM Quantum Heron处理器：这是IBM迄今为止错误率最低的量子计算芯片。IBM Quantum Heron处理器是IBM首款拥有133个固定频率量子位的处理器，具有可调节的耦合器，其设备性能比之前的处理器提高了3-5倍，几乎消除了串扰。这项技术有望成为未来硬件开发的基础。<br />
<br />
3、IBM Quantum System Two：IBM Quantum System Two是可扩展量子计算的基石，结合了低温基础设施、第三代控制电子设备和经典运行时服务器。这个系统将用于实现量子中心超级计算的并行电路执行。<br />
<br />
4、Qiskit 1.0：IBM宣布了Qiskit 1.0，这是世界上最广泛使用的开源量子编程软件。它具有新功能，帮助计算科学家更容易和更快地执行量子电路。<br />
<br />
5、生成式AI模型：IBM展示了工程化的生成式AI模型，用于自动化量子代码开发，并优化量子电路。<br />
<br />
6、量子发展路线图至2033年：IBM发布了延伸至2033年的量子发展路线图，设定了新的目标，以显著提高门操作的质量。这样做将增加能够运行的量子电路的大小，并有助于实现大规模量子计算的全部潜力。<br />
<br />
详细：<a href="https://newsroom.ibm.com/2023-12-04-IBM-Debuts-Next-Generation-Quantum-Processor-IBM-Quantum-System-Two,-Extends-Roadmap-to-Advance-Era-of-Quantum-Utility">newsroom.ibm.com/2023-12-04-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE5MzcxNzY4MTkyNDkxNTIvcHUvaW1nL2hqMU53WU56ZVRENk9ZbXUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731904722007970150#m</id>
            <title>Pika 对视频特定区域修改功能演示

'Modify Region' 🌟</title>
            <link>https://nitter.cz/xiaohuggg/status/1731904722007970150#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731904722007970150#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 05:13:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pika 对视频特定区域修改功能演示<br />
<br />
'Modify Region' 🌟</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE4ODU3MDM5NTE1MDMzNjAvcHUvaW1nLzFuUGRnNzNOZWVYMjlYMXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731901971534168427#m</id>
            <title>R to @xiaohuggg: DeepMind说他们在发现漏洞后，于8月30日向OpenAI披露了这一漏洞...

但是直到今天OpenAI 才修复了这个漏洞

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1731901971534168427#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731901971534168427#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 05:03:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind说他们在发现漏洞后，于8月30日向OpenAI披露了这一漏洞...<br />
<br />
但是直到今天OpenAI 才修复了这个漏洞<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Fqek9TMWJBQUFlMTQyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731896401301557749#m</id>
            <title>通过再生疗法逆转听力损失

我们的耳朵里有一种微小毛细胞，它们对我们能否听到声音非常重要。

但是这些细胞很容易因为噪音或某些药物等原因而死亡，而且一旦死亡就不会再长出来。

麻省理工的衍生公司Frequency Therapeutics研究团队发现了一种小分子药物，注射到耳朵里，让这些毛细胞重新长出来。

在他们的临床试验中，一些参与者在接受了这种治疗后，他们的听力有了明显的改善。

详细：https://news.mit.edu/2022/frequency-therapeutics-hearing-regeneration-0329</title>
            <link>https://nitter.cz/xiaohuggg/status/1731896401301557749#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731896401301557749#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 04:40:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过再生疗法逆转听力损失<br />
<br />
我们的耳朵里有一种微小毛细胞，它们对我们能否听到声音非常重要。<br />
<br />
但是这些细胞很容易因为噪音或某些药物等原因而死亡，而且一旦死亡就不会再长出来。<br />
<br />
麻省理工的衍生公司Frequency Therapeutics研究团队发现了一种小分子药物，注射到耳朵里，让这些毛细胞重新长出来。<br />
<br />
在他们的临床试验中，一些参与者在接受了这种治疗后，他们的听力有了明显的改善。<br />
<br />
详细：<a href="https://news.mit.edu/2022/frequency-therapeutics-hearing-regeneration-0329">news.mit.edu/2022/frequency-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FqdVFrU2FRQUE2MGRuLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731888448582373761#m</id>
            <title>Suno @suno_ai_+  Midjourney+D-ID

创作唱歌视频👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1731888448582373761#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731888448582373761#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 04:09:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Suno <a href="https://nitter.cz/suno_ai_" title="Suno">@suno_ai_</a>+  Midjourney+D-ID<br />
<br />
创作唱歌视频👍</p>
<p><a href="https://nitter.cz/anukaakash/status/1731628181600526781#m">nitter.cz/anukaakash/status/1731628181600526781#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731882948750643325#m</id>
            <title>昨天吐槽 @Magnific_AI 不给白嫖的机会

今天就发善心了❤️

只要是之前注册的账号，今天登录都能获得50个代币🟡 

但是今天注册的新账号好像没有...😃

我刚登陆进去果然有50个🟡 

你们可以进去碰碰运气：https://magnific.ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1731882948750643325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731882948750643325#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 03:47:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨天吐槽 <a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 不给白嫖的机会<br />
<br />
今天就发善心了❤️<br />
<br />
只要是之前注册的账号，今天登录都能获得50个代币🟡 <br />
<br />
但是今天注册的新账号好像没有...😃<br />
<br />
我刚登陆进去果然有50个🟡 <br />
<br />
你们可以进去碰碰运气：<a href="https://magnific.ai/">magnific.ai/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE4ODI3ODI3MzYxNzkyMDAvcHUvaW1nLy1YTEZpamZnN0JsTHIwZjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731869471705292843#m</id>
            <title>R to @xiaohuggg: 这个是阿里的同样的项目</title>
            <link>https://nitter.cz/xiaohuggg/status/1731869471705292843#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731869471705292843#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 02:53:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是阿里的同样的项目</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730133378501067046#m">nitter.cz/xiaohuggg/status/1730133378501067046#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731868943340707855#m</id>
            <title>阿里前几天发的只靠单张照片和动作就能生成跳舞视频的项目一下就被字节跳动秒了...

因为他们没有发布代码和演示，字节今天直接就放出了同样的项目并提供了代码和演示。

MagicAnimate：基于扩散模型的人类图像动画框架

不仅支持把静止的图片变成动作视频。

还能结合文本成动画，而且还支持多人照片。

MagicAnimate是一个基于扩散模型的人类图像动画框架，旨在增强时间一致性、忠实保留参考图像，并提高动画的真实感。

主要功能特点：

1、时间一致性动画：MagicAnimate的目标是根据运动序列使参考图像动起来，并保持时间上的一致性。能够确保动画在时间上的连贯性，动画中的动作看起来自然流畅，没有突兀的变化。

2、忠实于原图：在动画化过程中，它能够保持对原始参考图像的高度忠实度，确保动画中的人物或对象与原图保持一致。

3、跨身份动画：MagicAnimate还能够进行跨身份动画，即使用来自不同视频的运动序列来动画化参考图像。网站展示了三个身份和两个运动序列的视频结果。

4、未见领域动画：该项目能够动画化未见领域的图像，例如油画和电影角色，使其执行跑步或瑜伽等动作。

5、与T2I扩散模型结合：MagicAnimate还可以与DALLE3生成的参考图像结合，使其执行各种动作。每个参考图像的文本提示也在视频下方展示。

6、多人动画：该框架还支持多人动画，根据给定的运动序列动画化多个人物。

MagicAnimate使用视频扩散模型和外观编码器来进行时间建模和身份保持。为了支持长视频动画，开发了一个简单的视频融合策略，在推理过程中产生平滑的视频过渡。

主要工作原理：

1、视频扩散模型：MagicAnimate使用一种称为视频扩散模型的技术。这种模型能够处理时间序列数据，即它不仅考虑单个图像，还考虑图像随时间的变化。这使得生成的动画在时间上保持连贯和一致。

2、外观编码器：为了保持动画中人物的身份和外观特征与原始图像一致，MagicAnimate使用外观编码器。这个编码器确保即使在动画过程中，人物的基本特征（如面部特征、服装等）保持不变。

3、参考图像和目标动作序列：在生成动画时，MagicAnimate需要两个输入：一是参考图像（如人物照片），二是目标动作序列（描述人物应该如何移动）。这些动作序列可以是预先定义的，也可以是根据特定任务动态生成的。

4、视频融合策略：为了支持长视频动画的生成，MagicAnimate采用了视频融合策略。这种策略能够在动画的不同部分之间平滑过渡，避免突兀的切换，从而生成更自然的长时动画。

5、多样化应用：除了基本的图像动画化，MagicAnimate还能应用于更多场景，如将未见领域的图像（例如油画或电影角色）动画化，或者结合文本描述生成动画。

这种技术在动画制作、游戏设计、虚拟现实等领域具有广泛的应用潜力。

项目及演示：https://showlab.github.io/magicanimate/
论文：https://arxiv.org/abs/2311.16498
GitHub：https://github.com/magic-research/magic-animate

Huggingface在线测试：https://huggingface.co/spaces/zcxu-eric/magicanimate

Colab在线测试：https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb</title>
            <link>https://nitter.cz/xiaohuggg/status/1731868943340707855#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731868943340707855#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 02:51:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里前几天发的只靠单张照片和动作就能生成跳舞视频的项目一下就被字节跳动秒了...<br />
<br />
因为他们没有发布代码和演示，字节今天直接就放出了同样的项目并提供了代码和演示。<br />
<br />
MagicAnimate：基于扩散模型的人类图像动画框架<br />
<br />
不仅支持把静止的图片变成动作视频。<br />
<br />
还能结合文本成动画，而且还支持多人照片。<br />
<br />
MagicAnimate是一个基于扩散模型的人类图像动画框架，旨在增强时间一致性、忠实保留参考图像，并提高动画的真实感。<br />
<br />
主要功能特点：<br />
<br />
1、时间一致性动画：MagicAnimate的目标是根据运动序列使参考图像动起来，并保持时间上的一致性。能够确保动画在时间上的连贯性，动画中的动作看起来自然流畅，没有突兀的变化。<br />
<br />
2、忠实于原图：在动画化过程中，它能够保持对原始参考图像的高度忠实度，确保动画中的人物或对象与原图保持一致。<br />
<br />
3、跨身份动画：MagicAnimate还能够进行跨身份动画，即使用来自不同视频的运动序列来动画化参考图像。网站展示了三个身份和两个运动序列的视频结果。<br />
<br />
4、未见领域动画：该项目能够动画化未见领域的图像，例如油画和电影角色，使其执行跑步或瑜伽等动作。<br />
<br />
5、与T2I扩散模型结合：MagicAnimate还可以与DALLE3生成的参考图像结合，使其执行各种动作。每个参考图像的文本提示也在视频下方展示。<br />
<br />
6、多人动画：该框架还支持多人动画，根据给定的运动序列动画化多个人物。<br />
<br />
MagicAnimate使用视频扩散模型和外观编码器来进行时间建模和身份保持。为了支持长视频动画，开发了一个简单的视频融合策略，在推理过程中产生平滑的视频过渡。<br />
<br />
主要工作原理：<br />
<br />
1、视频扩散模型：MagicAnimate使用一种称为视频扩散模型的技术。这种模型能够处理时间序列数据，即它不仅考虑单个图像，还考虑图像随时间的变化。这使得生成的动画在时间上保持连贯和一致。<br />
<br />
2、外观编码器：为了保持动画中人物的身份和外观特征与原始图像一致，MagicAnimate使用外观编码器。这个编码器确保即使在动画过程中，人物的基本特征（如面部特征、服装等）保持不变。<br />
<br />
3、参考图像和目标动作序列：在生成动画时，MagicAnimate需要两个输入：一是参考图像（如人物照片），二是目标动作序列（描述人物应该如何移动）。这些动作序列可以是预先定义的，也可以是根据特定任务动态生成的。<br />
<br />
4、视频融合策略：为了支持长视频动画的生成，MagicAnimate采用了视频融合策略。这种策略能够在动画的不同部分之间平滑过渡，避免突兀的切换，从而生成更自然的长时动画。<br />
<br />
5、多样化应用：除了基本的图像动画化，MagicAnimate还能应用于更多场景，如将未见领域的图像（例如油画或电影角色）动画化，或者结合文本描述生成动画。<br />
<br />
这种技术在动画制作、游戏设计、虚拟现实等领域具有广泛的应用潜力。<br />
<br />
项目及演示：<a href="https://showlab.github.io/magicanimate/">showlab.github.io/magicanima…</a><br />
论文：<a href="https://arxiv.org/abs/2311.16498">arxiv.org/abs/2311.16498</a><br />
GitHub：<a href="https://github.com/magic-research/magic-animate">github.com/magic-research/ma…</a><br />
<br />
Huggingface在线测试：<a href="https://huggingface.co/spaces/zcxu-eric/magicanimate">huggingface.co/spaces/zcxu-e…</a><br />
<br />
Colab在线测试：<a href="https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb">colab.research.google.com/gi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE4NjgzOTM0NjMyNDY4NDgvcHUvaW1nL1RmdkJGaThaMHpDUUxoOVAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731852214304456888#m</id>
            <title>GTA 6 预告片被泄露，迫使 Rockstar Games 提前发布正式版…

😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1731852214304456888#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731852214304456888#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 01:45:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GTA 6 预告片被泄露，迫使 Rockstar Games 提前发布正式版…<br />
<br />
😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMxODUxOTU0NTA5NTA4NjA4L2ltZy9WWXlWZzZYSDROeTFhVURJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>