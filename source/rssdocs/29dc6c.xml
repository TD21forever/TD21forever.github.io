<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750829664766198026#m</id>
            <title>Diffuse to Choose：在线购物“虚拟试穿”模型

这个模型能让你在将任何商品放入任何环境中，同时和环境完美融合！

比如，你可以把一个在线商店的椅子放进你的客厅的照片里，看看它实际放在那里会是什么样子。

同时保证在不同环境中看起来自然和真实！

简而言之，它帮助用户更好地了解产品在真实环境中的样子，提高了在线购物的体验。

1、虚拟试穿技术：允许用户在不同环境中虚拟放置商品，实现逼真的在线购物体验。

2、与传统扩散模型相比，DTC模型能更好地捕捉商品细节，提升修复质量。采用特殊的算法，将来自参考图像的细粒度特征直接融入主扩散模型的潜在特征图中，保证产品与环境的高度融合。

3、高效平衡：在快速推断与保持高保真细节方面达到了有效的平衡。

4、广泛测试与评估：在不同数据集上测试，证明了DTC模型相较于现有技术的优越性。

5、场景适应性：能够处理多种场景中的图像，确保产品与场景的无缝整合。

6、快速推断能力：提供快速且高效的零次射推断，加快虚拟试穿过程。

项目及演示：https://diffuse2choose.github.io
论文：https://arxiv.org/abs/2401.13795</title>
            <link>https://nitter.cz/xiaohuggg/status/1750829664766198026#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750829664766198026#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 10:34:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Diffuse to Choose：在线购物“虚拟试穿”模型<br />
<br />
这个模型能让你在将任何商品放入任何环境中，同时和环境完美融合！<br />
<br />
比如，你可以把一个在线商店的椅子放进你的客厅的照片里，看看它实际放在那里会是什么样子。<br />
<br />
同时保证在不同环境中看起来自然和真实！<br />
<br />
简而言之，它帮助用户更好地了解产品在真实环境中的样子，提高了在线购物的体验。<br />
<br />
1、虚拟试穿技术：允许用户在不同环境中虚拟放置商品，实现逼真的在线购物体验。<br />
<br />
2、与传统扩散模型相比，DTC模型能更好地捕捉商品细节，提升修复质量。采用特殊的算法，将来自参考图像的细粒度特征直接融入主扩散模型的潜在特征图中，保证产品与环境的高度融合。<br />
<br />
3、高效平衡：在快速推断与保持高保真细节方面达到了有效的平衡。<br />
<br />
4、广泛测试与评估：在不同数据集上测试，证明了DTC模型相较于现有技术的优越性。<br />
<br />
5、场景适应性：能够处理多种场景中的图像，确保产品与场景的无缝整合。<br />
<br />
6、快速推断能力：提供快速且高效的零次射推断，加快虚拟试穿过程。<br />
<br />
项目及演示：<a href="https://diffuse2choose.github.io">diffuse2choose.github.io</a><br />
论文：<a href="https://arxiv.org/abs/2401.13795">arxiv.org/abs/2401.13795</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwODI5NTQyODkyMjU3MjgwL2ltZy9nR3JzeDJiZ1JRZkZqOG1XLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750796206874443880#m</id>
            <title>R to @xiaohuggg: Chrome 121版本已经新增在Android上支持WebGPU特性！

WebGPU，现在默认在运行Android 12及更高版本的设备上启用，特别是那些使用高通和ARM GPU的设备。支持范围将逐步扩大到更多Android设备，包括运行Android 11的设备。

https://developer.chrome.com/blog/new-in-webgpu-121</title>
            <link>https://nitter.cz/xiaohuggg/status/1750796206874443880#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750796206874443880#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 08:21:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chrome 121版本已经新增在Android上支持WebGPU特性！<br />
<br />
WebGPU，现在默认在运行Android 12及更高版本的设备上启用，特别是那些使用高通和ARM GPU的设备。支持范围将逐步扩大到更多Android设备，包括运行Android 11的设备。<br />
<br />
<a href="https://developer.chrome.com/blog/new-in-webgpu-121">developer.chrome.com/blog/ne…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MDQxOTUyOTcwNjk4MzQyNC8zZ0k1V29OTj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750796204668342624#m</id>
            <title>Web LLM：在浏览器中运行大语言模型

该项目利用WebGPU加速，无需服务器支持，所有操作都在浏览器内运行。

这为构建面向每个人的AI助手开启了新的可能性。

这意味着以后大语言模型可以在任意设备上运行！

主要特点：

1.支持多种模型：支持多种模型，包括Llama 2 7B/13B、Llama 2 70B、Mistral 7B以及WizardMath等。

2.运行环境和要求：项目在Chrome 113中提供WebGPU支持。用户可以在支持的浏览器中直接尝试不同的模型。首次运行时需要下载模型参数，之后的运行将更快。

3.聊天演示：提供了基于Llama 2、Mistral-7B及其变体和RedPajama-INCITE-Chat-3B-v1模型的聊天演示。未来还将支持更多模型。

4.开源和开发支持：项目鼓励开发者使用WebLLM作为基础npm包，并在其上构建自己的Web应用程序。相关文档和GitHub资源可供参考。

项目目标与愿景：

该项目旨在为生态系统带来更多多样性，尤其是将LLMs直接嵌入到客户端并在浏览器内运行。这样做可以降低成本、增强个性化和保护隐私。

详细：https://webllm.mlc.ai

GitHub：https://github.com/mlc-ai/web-llm

演示视频 @charlie_ruan</title>
            <link>https://nitter.cz/xiaohuggg/status/1750796204668342624#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750796204668342624#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 08:21:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Web LLM：在浏览器中运行大语言模型<br />
<br />
该项目利用WebGPU加速，无需服务器支持，所有操作都在浏览器内运行。<br />
<br />
这为构建面向每个人的AI助手开启了新的可能性。<br />
<br />
这意味着以后大语言模型可以在任意设备上运行！<br />
<br />
主要特点：<br />
<br />
1.支持多种模型：支持多种模型，包括Llama 2 7B/13B、Llama 2 70B、Mistral 7B以及WizardMath等。<br />
<br />
2.运行环境和要求：项目在Chrome 113中提供WebGPU支持。用户可以在支持的浏览器中直接尝试不同的模型。首次运行时需要下载模型参数，之后的运行将更快。<br />
<br />
3.聊天演示：提供了基于Llama 2、Mistral-7B及其变体和RedPajama-INCITE-Chat-3B-v1模型的聊天演示。未来还将支持更多模型。<br />
<br />
4.开源和开发支持：项目鼓励开发者使用WebLLM作为基础npm包，并在其上构建自己的Web应用程序。相关文档和GitHub资源可供参考。<br />
<br />
项目目标与愿景：<br />
<br />
该项目旨在为生态系统带来更多多样性，尤其是将LLMs直接嵌入到客户端并在浏览器内运行。这样做可以降低成本、增强个性化和保护隐私。<br />
<br />
详细：<a href="https://webllm.mlc.ai">webllm.mlc.ai</a><br />
<br />
GitHub：<a href="https://github.com/mlc-ai/web-llm">github.com/mlc-ai/web-llm</a><br />
<br />
演示视频 <a href="https://nitter.cz/charlie_ruan" title="Charlie Ruan">@charlie_ruan</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwNzk2MTM5ODYyMTM4ODgwL2ltZy9uU0VGRW1HM1dJSDN1TXRNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750788482484846748#m</id>
            <title>曼谷有X友吗

🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1750788482484846748#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750788482484846748#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 07:51:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>曼谷有X友吗<br />
<br />
🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmRmFRQUFNUzViLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmRmJVQUFZSDJlLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmSWFZQUE2Ti0tLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750706833751408858#m</id>
            <title>SUPIR：通过增加模型的规模（即增加模型的参数数量）提升图像修复的能力。

通过参数增加使得模型不仅能够修复图像中的错误或损坏，还能根据文本提示进行智能修复。

例如根据描述来改变图像中的特定细节。这样的处理方式提升了图像修复的质量和智能度，使得模型能够更准确、更灵活地恢复和改进图像。

SUPIR的主要功能：

图像修复： SUPIR的核心功能是对低质量或损坏的图像进行修复，提高其视觉质量。这包括处理如模糊、噪点、色彩失真等问题，使图像恢复到高清晰度和高质量状态。

文本引导的修复： SUPIR能够根据文本提示来指导图像修复。这意味着用户可以通过文本描述来指定希望修复或改变的图像部分，使得修复过程更加定制化和精确。

核心技术创新：

1、模型放大： SUPIR通过扩大模型规模（即增加模型的参数数量）来提升图像修复的能力。这种放大使得模型能够学习更多的特征，处理更复杂的图像修复任务。

2、多模态技术： 结合了图像处理和文本处理的技术，允许模型不仅理解图像内容，还能理解与之相关的文本描述，从而进行更准确的修复。

3、高质量训练数据集：收集了2000万高质量图像和文本注释，用于训练和控制图像修复。利用大量高分辨率、高质量的图像和相关文本注释作为训练数据，提高了模型的性能和适用性。

4、负质量提示： 通过引入质量较差的图像样本和相应的负面描述作为训练数据，进一步提升模型在感知质量方面的表现。

工作原理：

1、图像编码与解码： SUPIR利用一个编码器将低质量图像映射到潜在空间，然后使用解码器重建修复后的图像。

2、文本处理： 通过一个多模态语言模型，SUPIR能够理解与图像相关的文本描述，并将这些信息融入到图像修复过程中。

3、适配器设计： SUPIR设计了一个大规模适配器，用于将模型的生成能力调整到与输入图像相匹配的状态，确保修复过程符合用户的具体需求。

4、采样方法： 采用特殊的采样方法，用于指导图像的恢复过程，以防止过度生成，确保修复后的图像保持真实和高质量。

实验结果：

在多种IR任务上展示了出色的修复效果，特别是在复杂和具挑战性的真实世界场景中

1、多样化的图像修复任务： SUPIR被应用于各种类型的图像修复任务，包括但不限于去噪、去模糊、超分辨率、色彩校正等。这显示了其广泛的适用性和灵活性。

2、真实世界的复杂场景处理： 实验中的一个重要亮点是SUPIR在处理真实世界复杂场景中的高效表现。这些场景通常包含多种类型的图像退化，如不均匀光照、运动模糊和天气影响等，这些都是传统图像修复方法难以处理的。

3、高级特性的应用： SUPIR展示了如何根据复杂的文本描述进行定制化修复。例如，它可以根据用户提供的描述，调整图像中特定对象的纹理或颜色，或者改变场景的某些元素。

4、质量评估： 在实验中，SUPIR修复的图像在质量上得到了显著提升。这通过与现有技术的对比评估，以及视觉质量和客观指标（如图像清晰度、纹理细节等）的测量来证实。

5、挑战性任务的处理： 特别值得注意的是，SUPIR在处理一些传统方法难以解决的挑战性任务时表现突出，如极度模糊或严重损坏的图像修复。

6、用户定制和互动性： 实验还展示了SUPIR在用户交互方面的能力，用户可以通过简单的文本指令控制图像的修复过程，这为图像修复提供了新的互动维度。

项目及演示：https://supir.xpixel.group/
论文：https://arxiv.org/abs/2401.13627</title>
            <link>https://nitter.cz/xiaohuggg/status/1750706833751408858#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750706833751408858#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:26:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SUPIR：通过增加模型的规模（即增加模型的参数数量）提升图像修复的能力。<br />
<br />
通过参数增加使得模型不仅能够修复图像中的错误或损坏，还能根据文本提示进行智能修复。<br />
<br />
例如根据描述来改变图像中的特定细节。这样的处理方式提升了图像修复的质量和智能度，使得模型能够更准确、更灵活地恢复和改进图像。<br />
<br />
SUPIR的主要功能：<br />
<br />
图像修复： SUPIR的核心功能是对低质量或损坏的图像进行修复，提高其视觉质量。这包括处理如模糊、噪点、色彩失真等问题，使图像恢复到高清晰度和高质量状态。<br />
<br />
文本引导的修复： SUPIR能够根据文本提示来指导图像修复。这意味着用户可以通过文本描述来指定希望修复或改变的图像部分，使得修复过程更加定制化和精确。<br />
<br />
核心技术创新：<br />
<br />
1、模型放大： SUPIR通过扩大模型规模（即增加模型的参数数量）来提升图像修复的能力。这种放大使得模型能够学习更多的特征，处理更复杂的图像修复任务。<br />
<br />
2、多模态技术： 结合了图像处理和文本处理的技术，允许模型不仅理解图像内容，还能理解与之相关的文本描述，从而进行更准确的修复。<br />
<br />
3、高质量训练数据集：收集了2000万高质量图像和文本注释，用于训练和控制图像修复。利用大量高分辨率、高质量的图像和相关文本注释作为训练数据，提高了模型的性能和适用性。<br />
<br />
4、负质量提示： 通过引入质量较差的图像样本和相应的负面描述作为训练数据，进一步提升模型在感知质量方面的表现。<br />
<br />
工作原理：<br />
<br />
1、图像编码与解码： SUPIR利用一个编码器将低质量图像映射到潜在空间，然后使用解码器重建修复后的图像。<br />
<br />
2、文本处理： 通过一个多模态语言模型，SUPIR能够理解与图像相关的文本描述，并将这些信息融入到图像修复过程中。<br />
<br />
3、适配器设计： SUPIR设计了一个大规模适配器，用于将模型的生成能力调整到与输入图像相匹配的状态，确保修复过程符合用户的具体需求。<br />
<br />
4、采样方法： 采用特殊的采样方法，用于指导图像的恢复过程，以防止过度生成，确保修复后的图像保持真实和高质量。<br />
<br />
实验结果：<br />
<br />
在多种IR任务上展示了出色的修复效果，特别是在复杂和具挑战性的真实世界场景中<br />
<br />
1、多样化的图像修复任务： SUPIR被应用于各种类型的图像修复任务，包括但不限于去噪、去模糊、超分辨率、色彩校正等。这显示了其广泛的适用性和灵活性。<br />
<br />
2、真实世界的复杂场景处理： 实验中的一个重要亮点是SUPIR在处理真实世界复杂场景中的高效表现。这些场景通常包含多种类型的图像退化，如不均匀光照、运动模糊和天气影响等，这些都是传统图像修复方法难以处理的。<br />
<br />
3、高级特性的应用： SUPIR展示了如何根据复杂的文本描述进行定制化修复。例如，它可以根据用户提供的描述，调整图像中特定对象的纹理或颜色，或者改变场景的某些元素。<br />
<br />
4、质量评估： 在实验中，SUPIR修复的图像在质量上得到了显著提升。这通过与现有技术的对比评估，以及视觉质量和客观指标（如图像清晰度、纹理细节等）的测量来证实。<br />
<br />
5、挑战性任务的处理： 特别值得注意的是，SUPIR在处理一些传统方法难以解决的挑战性任务时表现突出，如极度模糊或严重损坏的图像修复。<br />
<br />
6、用户定制和互动性： 实验还展示了SUPIR在用户交互方面的能力，用户可以通过简单的文本指令控制图像的修复过程，这为图像修复提供了新的互动维度。<br />
<br />
项目及演示：<a href="https://supir.xpixel.group/">supir.xpixel.group/</a><br />
论文：<a href="https://arxiv.org/abs/2401.13627">arxiv.org/abs/2401.13627</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA3MDYxMjY4MDk4OTkwMDgvcHUvaW1nL19FS1RQS2k3TjkwZFpNLV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750704152605499508#m</id>
            <title>Adept Fuyu-Heavy：Adept Fuyu-Heavy是专为数字代理设计的新型多模态模型。

宣称是世界上第三大能力超强的多模态模型，仅次于GPT4-V和Gemini Ultra。

它特别擅长理解用户界面，这意味着可以解释和操作各种软件和应用程序的界面。

能够帮助用户执行各种任务，如自动化流程、响应查询、提供信息等。

Adept Fuyu-Heavy在多项评估和基准测试中展示了卓越的性能。

1、多模态基准测试：在MMM（Multimodal Multitask）基准测试中，Fuyu-Heavy的表现优于Gemini Pro，突显了其在多模态任务上的能力。

2、文本基准测试：尽管Fuyu-Heavy需分配部分容量处理图像数据，但在标准的文本只评估中，它的表现与Gemini Pro大体相当，甚至在MMLU（多模态语言理解）基准测试中超过了Gemini Pro。

3、长形式对话性能：经过有监督的微调和直接优化阶段后，Fuyu-Heavy在最常用的聊天评估——MT-Bench和AlpacaEval 1.0——中的表现与Claude 2.0相当，尽管它是一个更小的模型，且部分容量用于图像建模。

4、多模态性能标准：在MMM（Multimodal Multitask）基准测试上，Fuyu-Heavy略微优于Gemini Pro。此外，还包括了在VQAv2（一个视觉问答基准）和AI2D（一个图表理解数据集）上的结果。

Adept Fuyu-Heavy的主要能力包括：

1、多模态理解和生成： Fuyu-Heavy能够处理和理解多种类型的数据，如文本和图像，并能够基于这些数据生成相应的输出。这使其在多模态任务上表现出色。

2、高效的图像和文本处理： 尽管需要部分容量用于图像建模，Fuyu-Heavy在标准文本基准测试中的表现匹敌或超越同级别的模型。

3、优化的模型架构： Fuyu-Heavy通过扩展和优化Fuyu架构，有效处理任意大小和形状的图像，并有效利用现有的变压器模型优化。

4、长形式对话性能： 经过特定训练阶段优化，Fuyu-Heavy在长形式对话和交互中表现出色。

5、用户界面理解： 特别擅长于理解数字用户界面（UI），如网站和应用程序，提供有效的自动化解决方案。意味着Fuyu-Heavy的开发重点是使其能够适应和优化数字代理的功能，如提高用户界面理解、增强自动化决策能力、提供更准确的信息检索和内容生成等。

6、跨模态内容生成： 能够生成跨越文本和图像的内容，适用于多种应用场景。

详细：https://www.adept.ai/blog/adept-fuyu-heavy</title>
            <link>https://nitter.cz/xiaohuggg/status/1750704152605499508#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750704152605499508#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:16:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Adept Fuyu-Heavy：Adept Fuyu-Heavy是专为数字代理设计的新型多模态模型。<br />
<br />
宣称是世界上第三大能力超强的多模态模型，仅次于GPT4-V和Gemini Ultra。<br />
<br />
它特别擅长理解用户界面，这意味着可以解释和操作各种软件和应用程序的界面。<br />
<br />
能够帮助用户执行各种任务，如自动化流程、响应查询、提供信息等。<br />
<br />
Adept Fuyu-Heavy在多项评估和基准测试中展示了卓越的性能。<br />
<br />
1、多模态基准测试：在MMM（Multimodal Multitask）基准测试中，Fuyu-Heavy的表现优于Gemini Pro，突显了其在多模态任务上的能力。<br />
<br />
2、文本基准测试：尽管Fuyu-Heavy需分配部分容量处理图像数据，但在标准的文本只评估中，它的表现与Gemini Pro大体相当，甚至在MMLU（多模态语言理解）基准测试中超过了Gemini Pro。<br />
<br />
3、长形式对话性能：经过有监督的微调和直接优化阶段后，Fuyu-Heavy在最常用的聊天评估——MT-Bench和AlpacaEval 1.0——中的表现与Claude 2.0相当，尽管它是一个更小的模型，且部分容量用于图像建模。<br />
<br />
4、多模态性能标准：在MMM（Multimodal Multitask）基准测试上，Fuyu-Heavy略微优于Gemini Pro。此外，还包括了在VQAv2（一个视觉问答基准）和AI2D（一个图表理解数据集）上的结果。<br />
<br />
Adept Fuyu-Heavy的主要能力包括：<br />
<br />
1、多模态理解和生成： Fuyu-Heavy能够处理和理解多种类型的数据，如文本和图像，并能够基于这些数据生成相应的输出。这使其在多模态任务上表现出色。<br />
<br />
2、高效的图像和文本处理： 尽管需要部分容量用于图像建模，Fuyu-Heavy在标准文本基准测试中的表现匹敌或超越同级别的模型。<br />
<br />
3、优化的模型架构： Fuyu-Heavy通过扩展和优化Fuyu架构，有效处理任意大小和形状的图像，并有效利用现有的变压器模型优化。<br />
<br />
4、长形式对话性能： 经过特定训练阶段优化，Fuyu-Heavy在长形式对话和交互中表现出色。<br />
<br />
5、用户界面理解： 特别擅长于理解数字用户界面（UI），如网站和应用程序，提供有效的自动化解决方案。意味着Fuyu-Heavy的开发重点是使其能够适应和优化数字代理的功能，如提高用户界面理解、增强自动化决策能力、提供更准确的信息检索和内容生成等。<br />
<br />
6、跨模态内容生成： 能够生成跨越文本和图像的内容，适用于多种应用场景。<br />
<br />
详细：<a href="https://www.adept.ai/blog/adept-fuyu-heavy">adept.ai/blog/adept-fuyu-hea…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA1NDAzMTY3ODMwMjIwODAvcHUvaW1nL1pacTRtSmdVU2t3MUNwekkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750694801517969900#m</id>
            <title>SpeechGPT-Gen：为大语言模型提供内在的跨模态对话能力

它不仅能处理传统的文本数据，还能理解和生成语音数据，实现文本与语音之间的无缝对话。

能够接收语音输入，理解其内容，并以语音形式做出回应。

为大语言模型在处理和生成语音方面提供了强大的支持。

SpeechGPT-Gen是一个包含8亿参数的模型，有效地处理语义和知觉信息。

主要功能特点：

SpeechGPT-Gen能够接收语音输入，理解其内容，并以语音形式做出回应。这种能力使得模型在执行语音到语音对话任务时表现出色。

1、零样本文本到语音转换（Text-to-Speech）： 
SpeechGPT-Gen能够将给定的文本转换成语音，而且不需要提前的样本或训练数据。能够直接从文本生成自然听起来的语音。

2、零样本语音转换（Voice Conversion）：
 它可以改变源语音的音色，使其听起来像是另一个给定的发言者所说。这对于个性化语音应用非常有用。

3、语音到语音对话（Speech-to-Speech Dialogue）： 
SpeechGPT-Gen能够理解语音指令，并以类似的音色生成语音回应。这对于开发更自然的交互式语音系统特别重要。

解决的问题：

1、提高语音生成效率： 传统的大型语音语言模型在处理语义和知觉信息时存在冗余，导致效率低下。SpeechGPT-Gen通过分离这两种信息的建模，有效地提高了语音生成的效率。

2、增强模型的泛化能力： 它在未见过的发言者上展示了优秀的泛化能力，这意味着它能够处理各种不同的语音类型和风格。

3、提供更多样的语音应用： 通过支持零样本的文本到语音、语音转换和语音对话，SpeechGPT-Gen为语音技术的应用提供了更广泛的可能性，如语音合成、个性化语音生成等。

技术创新：

1、链式信息生成： SpeechGPT-Gen引入了这种新方法来分离和处理语音的语义和知觉信息。这种方法减少了传统方法中的冗余和低效率。

2、自回归和非自回归模型的结合：
自回归模型： 用于处理语义信息。基于大型语言模型（LLM），这一部分负责理解和生成语音的含义和内容。

非自回归模型： 使用流匹配技术（flow matching）处理知觉信息，即声音的感觉特征（如音色和节奏）。

3、语义信息的先验注入： 在流匹配中，SpeechGPT-Gen引入了一种创新的方法，将语义信息注入到先验分布中，从而提高流匹配的效率。

工作原理：

1、语义建模： 通过自回归模型，SpeechGPT-Gen首先对语音或文本的语义内容进行建模。这一步涉及理解语音或文本的含义，为后续的知觉建模提供基础。

2、知觉建模： 接着，非自回归模型通过流匹配技术处理知觉信息，即生成具有特定音色和风格的语音。

3、流匹配技术： 流匹配通过建立从简单先验分布到复杂数据分布的转换来工作。在SpeechGPT-Gen中，这个过程利用了语义信息作为先验，从而更高效地生成知觉信息。

4、综合生成： 通过这两个步骤，SpeechGPT-Gen能够先理解语音或文本的意义，然后生成具有相应含义的语音输出，具有良好的语义和知觉一致性。

这种结合了自回归和非自回归模型的方法，以及流匹配技术的应用，使得SpeechGPT-Gen在多种跨模态任务（如零样本文本到语音、语音转换和语音到语音对话）中表现出色，具有较强的灵活性和效率。

项目及演示：https://0nutation.github.io/SpeechGPT-Gen.github.io/
论文：https://arxiv.org/abs/2401.13527
GitHub：https://github.com/0nutation/SpeechGPT</title>
            <link>https://nitter.cz/xiaohuggg/status/1750694801517969900#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750694801517969900#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 01:39:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SpeechGPT-Gen：为大语言模型提供内在的跨模态对话能力<br />
<br />
它不仅能处理传统的文本数据，还能理解和生成语音数据，实现文本与语音之间的无缝对话。<br />
<br />
能够接收语音输入，理解其内容，并以语音形式做出回应。<br />
<br />
为大语言模型在处理和生成语音方面提供了强大的支持。<br />
<br />
SpeechGPT-Gen是一个包含8亿参数的模型，有效地处理语义和知觉信息。<br />
<br />
主要功能特点：<br />
<br />
SpeechGPT-Gen能够接收语音输入，理解其内容，并以语音形式做出回应。这种能力使得模型在执行语音到语音对话任务时表现出色。<br />
<br />
1、零样本文本到语音转换（Text-to-Speech）： <br />
SpeechGPT-Gen能够将给定的文本转换成语音，而且不需要提前的样本或训练数据。能够直接从文本生成自然听起来的语音。<br />
<br />
2、零样本语音转换（Voice Conversion）：<br />
 它可以改变源语音的音色，使其听起来像是另一个给定的发言者所说。这对于个性化语音应用非常有用。<br />
<br />
3、语音到语音对话（Speech-to-Speech Dialogue）： <br />
SpeechGPT-Gen能够理解语音指令，并以类似的音色生成语音回应。这对于开发更自然的交互式语音系统特别重要。<br />
<br />
解决的问题：<br />
<br />
1、提高语音生成效率： 传统的大型语音语言模型在处理语义和知觉信息时存在冗余，导致效率低下。SpeechGPT-Gen通过分离这两种信息的建模，有效地提高了语音生成的效率。<br />
<br />
2、增强模型的泛化能力： 它在未见过的发言者上展示了优秀的泛化能力，这意味着它能够处理各种不同的语音类型和风格。<br />
<br />
3、提供更多样的语音应用： 通过支持零样本的文本到语音、语音转换和语音对话，SpeechGPT-Gen为语音技术的应用提供了更广泛的可能性，如语音合成、个性化语音生成等。<br />
<br />
技术创新：<br />
<br />
1、链式信息生成： SpeechGPT-Gen引入了这种新方法来分离和处理语音的语义和知觉信息。这种方法减少了传统方法中的冗余和低效率。<br />
<br />
2、自回归和非自回归模型的结合：<br />
自回归模型： 用于处理语义信息。基于大型语言模型（LLM），这一部分负责理解和生成语音的含义和内容。<br />
<br />
非自回归模型： 使用流匹配技术（flow matching）处理知觉信息，即声音的感觉特征（如音色和节奏）。<br />
<br />
3、语义信息的先验注入： 在流匹配中，SpeechGPT-Gen引入了一种创新的方法，将语义信息注入到先验分布中，从而提高流匹配的效率。<br />
<br />
工作原理：<br />
<br />
1、语义建模： 通过自回归模型，SpeechGPT-Gen首先对语音或文本的语义内容进行建模。这一步涉及理解语音或文本的含义，为后续的知觉建模提供基础。<br />
<br />
2、知觉建模： 接着，非自回归模型通过流匹配技术处理知觉信息，即生成具有特定音色和风格的语音。<br />
<br />
3、流匹配技术： 流匹配通过建立从简单先验分布到复杂数据分布的转换来工作。在SpeechGPT-Gen中，这个过程利用了语义信息作为先验，从而更高效地生成知觉信息。<br />
<br />
4、综合生成： 通过这两个步骤，SpeechGPT-Gen能够先理解语音或文本的意义，然后生成具有相应含义的语音输出，具有良好的语义和知觉一致性。<br />
<br />
这种结合了自回归和非自回归模型的方法，以及流匹配技术的应用，使得SpeechGPT-Gen在多种跨模态任务（如零样本文本到语音、语音转换和语音到语音对话）中表现出色，具有较强的灵活性和效率。<br />
<br />
项目及演示：<a href="https://0nutation.github.io/SpeechGPT-Gen.github.io/">0nutation.github.io/SpeechGP…</a><br />
论文：<a href="https://arxiv.org/abs/2401.13527">arxiv.org/abs/2401.13527</a><br />
GitHub：<a href="https://github.com/0nutation/SpeechGPT">github.com/0nutation/SpeechG…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA1MzQ5MzQ5MjkwMjI5NzYvcHUvaW1nL1RhRVBUVWZWX2FvQUdLVEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750688990104330481#m</id>
            <title>OpenAI推出新一代嵌入模型、同时发布新的API使用管理工具，以及GPT-3.5 Turbo降价！

同时更新的GPT-4 Turbo预览版模型，提高了代码生成等任务的完成度，以及减少错误！

新嵌入模型推出：

• 推出了两种新的嵌入模型：text-embedding-3-small和text-embedding-3-large。 

新一代向量大模型text-embedding-3，embedding长度升级，价格最高下降5倍，包含2个版本，新增了一个可以控制生成的向量长度的参数！

新一代嵌入模型性能对比：

• 小型模型(text-embedding-3-small)对比：与上一代模型text-embedding-ada-002相比，新模型在多语言检索基准(MIRACL)上的平均得分从31.4%提高到44.0%，在英语任务基准(MTEB)上的平均得分从61.0%提高到62.3%。

• 大型模型(text-embedding-3-large)对比：与text-embedding-ada-002相比，在MIRACL上平均得分从31.4%提高到54.9%，在MTEB上从61.0%提高到64.6%。

• 新模型的性能普遍优于上一代模型，尤其是在多语言检索方面表现出色。

GPT-3.5 Turbo降价对比：

• 新的GPT-3.5 Turbo模型gpt-3.5-turbo-0125的输入价格降低50%，至$0.0005 /1K tokens，输出价格降低25%，至$0.0015 /1K tokens。

• 降价旨在帮助客户扩大规模使用，同时该模型还包含多项改进，如提高响应准确度和修复非英语语言功能调用的文本编码问题。

新内容审核模型发布：

• 发布了新的内容审核模型text-moderation-007，作为提高内容安全的一部分。

API使用和管理改进：

• 提供了更多的API使用可视化和控制工具，如API密钥权限分配和API使用情况的仪表板。

详细：https://openai.com/blog/new-embedding-models-and-api-updates</title>
            <link>https://nitter.cz/xiaohuggg/status/1750688990104330481#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750688990104330481#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 01:15:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI推出新一代嵌入模型、同时发布新的API使用管理工具，以及GPT-3.5 Turbo降价！<br />
<br />
同时更新的GPT-4 Turbo预览版模型，提高了代码生成等任务的完成度，以及减少错误！<br />
<br />
新嵌入模型推出：<br />
<br />
• 推出了两种新的嵌入模型：text-embedding-3-small和text-embedding-3-large。 <br />
<br />
新一代向量大模型text-embedding-3，embedding长度升级，价格最高下降5倍，包含2个版本，新增了一个可以控制生成的向量长度的参数！<br />
<br />
新一代嵌入模型性能对比：<br />
<br />
• 小型模型(text-embedding-3-small)对比：与上一代模型text-embedding-ada-002相比，新模型在多语言检索基准(MIRACL)上的平均得分从31.4%提高到44.0%，在英语任务基准(MTEB)上的平均得分从61.0%提高到62.3%。<br />
<br />
• 大型模型(text-embedding-3-large)对比：与text-embedding-ada-002相比，在MIRACL上平均得分从31.4%提高到54.9%，在MTEB上从61.0%提高到64.6%。<br />
<br />
• 新模型的性能普遍优于上一代模型，尤其是在多语言检索方面表现出色。<br />
<br />
GPT-3.5 Turbo降价对比：<br />
<br />
• 新的GPT-3.5 Turbo模型gpt-3.5-turbo-0125的输入价格降低50%，至$0.0005 /1K tokens，输出价格降低25%，至$0.0015 /1K tokens。<br />
<br />
• 降价旨在帮助客户扩大规模使用，同时该模型还包含多项改进，如提高响应准确度和修复非英语语言功能调用的文本编码问题。<br />
<br />
新内容审核模型发布：<br />
<br />
• 发布了新的内容审核模型text-moderation-007，作为提高内容安全的一部分。<br />
<br />
API使用和管理改进：<br />
<br />
• 提供了更多的API使用可视化和控制工具，如API密钥权限分配和API使用情况的仪表板。<br />
<br />
详细：<a href="https://openai.com/blog/new-embedding-models-and-api-updates">openai.com/blog/new-embeddin…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V1eVdtN2JRQUF0QlI0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V1eVdtN2J3QUFGS2lhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750400886676070495#m</id>
            <title>26种多模态大模型研究报告

这篇论文对目前市面上26种多模态大语言模型（MM-LLMs）进行了全面的研究和分析。提供了对多模态大语言模型的深入了解。

详细介绍了模型架构和训练流程的设计公式。

26 种现存的 MM-LLMs，每种模型都有其独特的设计和功能。

主要内容：

1.模型架构和训练流程：

论文详细描述了这些模型的架构和训练流程，突出了它们如何结合了传统大型语言模型（LLMs）的能力，并支持多模态输入和输出。

2.模型概览：

• 研究涵盖了26种不同的 MM-LLMs，每个模型都有其独特的设计和功能特点。

• 这些模型被分为不同的类别，根据它们的架构和功能进行了分类。

3.性能评估：对这些模型在主流基准测试上的性能进行了回顾，分析了它们在不同任务上的表现。

4.训练策略：总结了提高 MM-LLMs 性能的关键训练策略，包括数据处理和模型优化等。

5.研究方向和资源：讨论了 MM-LLMs 的未来研究方向，并提供了实时跟踪这些模型最新发展的资源。

论文：https://arxiv.org/abs/2401.13601
PDF：https://arxiv.org/pdf/2401.13601.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1750400886676070495#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750400886676070495#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 06:11:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>26种多模态大模型研究报告<br />
<br />
这篇论文对目前市面上26种多模态大语言模型（MM-LLMs）进行了全面的研究和分析。提供了对多模态大语言模型的深入了解。<br />
<br />
详细介绍了模型架构和训练流程的设计公式。<br />
<br />
26 种现存的 MM-LLMs，每种模型都有其独特的设计和功能。<br />
<br />
主要内容：<br />
<br />
1.模型架构和训练流程：<br />
<br />
论文详细描述了这些模型的架构和训练流程，突出了它们如何结合了传统大型语言模型（LLMs）的能力，并支持多模态输入和输出。<br />
<br />
2.模型概览：<br />
<br />
• 研究涵盖了26种不同的 MM-LLMs，每个模型都有其独特的设计和功能特点。<br />
<br />
• 这些模型被分为不同的类别，根据它们的架构和功能进行了分类。<br />
<br />
3.性能评估：对这些模型在主流基准测试上的性能进行了回顾，分析了它们在不同任务上的表现。<br />
<br />
4.训练策略：总结了提高 MM-LLMs 性能的关键训练策略，包括数据处理和模型优化等。<br />
<br />
5.研究方向和资源：讨论了 MM-LLMs 的未来研究方向，并提供了实时跟踪这些模型最新发展的资源。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.13601">arxiv.org/abs/2401.13601</a><br />
PDF：<a href="https://arxiv.org/pdf/2401.13601.pdf">arxiv.org/pdf/2401.13601.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Vxc1VXa2JjQUFCb3VNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750391646121017610#m</id>
            <title>利用红外激光反射攻击自动驾驶车辆，攻击成功率达到100%。

研究人员提出了一种使用红外线激光攻击自动驾驶车辆的方法。

攻击目标是车辆的摄像头系统，特别是那些用于识别道路标志的摄像头。使得自动驾驶汽车无法识别道路标志！

攻击原理：

1.红外线激光的不可见性：

•人眼无法看到红外线激光，但很多车载摄像头没有红外线滤镜，因此能够捕捉到这种光。
•这意味着攻击者可以对道路标志发射红外线激光，而不会被路人注意到。

2.误导识别系统：

•自动驾驶车辆使用深度学习模型通过摄像头捕捉的图像来识别道路标志。
•红外线激光可以改变摄像头捕捉的图像，使得AI识别系统误读这些标志。

3.静止道路标志上的红外线激光投射：

•研究提出在静止的道路标志上投射红外线激光，这种方式相对容易执行，隐蔽性较高。
•激光可以精准地修改标志上的特定部分，导致AI系统对标志的解读出错。

4.攻击效果：

•在室内实验中，所有深度学习模型的攻击成功率达到100%。

•在户外环境下，特别是夜间条件下，攻击成功率在80%至100%之间。

这项研究旨在提前理解并应对可能误导自动驾驶车辆AI识别系统的威胁，以便于未来加强这些系统的安全性和可靠性。

论文：https://arxiv.org/abs/2401.03582</title>
            <link>https://nitter.cz/xiaohuggg/status/1750391646121017610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750391646121017610#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 05:34:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>利用红外激光反射攻击自动驾驶车辆，攻击成功率达到100%。<br />
<br />
研究人员提出了一种使用红外线激光攻击自动驾驶车辆的方法。<br />
<br />
攻击目标是车辆的摄像头系统，特别是那些用于识别道路标志的摄像头。使得自动驾驶汽车无法识别道路标志！<br />
<br />
攻击原理：<br />
<br />
1.红外线激光的不可见性：<br />
<br />
•人眼无法看到红外线激光，但很多车载摄像头没有红外线滤镜，因此能够捕捉到这种光。<br />
•这意味着攻击者可以对道路标志发射红外线激光，而不会被路人注意到。<br />
<br />
2.误导识别系统：<br />
<br />
•自动驾驶车辆使用深度学习模型通过摄像头捕捉的图像来识别道路标志。<br />
•红外线激光可以改变摄像头捕捉的图像，使得AI识别系统误读这些标志。<br />
<br />
3.静止道路标志上的红外线激光投射：<br />
<br />
•研究提出在静止的道路标志上投射红外线激光，这种方式相对容易执行，隐蔽性较高。<br />
•激光可以精准地修改标志上的特定部分，导致AI系统对标志的解读出错。<br />
<br />
4.攻击效果：<br />
<br />
•在室内实验中，所有深度学习模型的攻击成功率达到100%。<br />
<br />
•在户外环境下，特别是夜间条件下，攻击成功率在80%至100%之间。<br />
<br />
这项研究旨在提前理解并应对可能误导自动驾驶车辆AI识别系统的威胁，以便于未来加强这些系统的安全性和可靠性。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.03582">arxiv.org/abs/2401.03582</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwMzkxNDA4NzY5NTA3MzI4L2ltZy9VaGVZNVFvemdrck5lTlpCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750367771446345834#m</id>
            <title>📢ChatGPT 新增了两个小功能

一个是：Always expand code output

如果打开此功能，代码解释器的输出结果将自动展开，输出完整代码，不需要额外点击！

另一个新功能是允许将左侧边栏中的所有“对话历史记录”批量移动到存档中。

这俩功能可以在设置->General中打开…</title>
            <link>https://nitter.cz/xiaohuggg/status/1750367771446345834#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750367771446345834#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 03:59:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>📢ChatGPT 新增了两个小功能<br />
<br />
一个是：Always expand code output<br />
<br />
如果打开此功能，代码解释器的输出结果将自动展开，输出完整代码，不需要额外点击！<br />
<br />
另一个新功能是允许将左侧边栏中的所有“对话历史记录”批量移动到存档中。<br />
<br />
这俩功能可以在设置-&gt;General中打开…</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VxT05HeGJFQUFJVFBELmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VxT05JM2JZQUE4blA5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750340749768691734#m</id>
            <title>本视频中的所有3D模型都是使用Luma AI的Genie 3D模型工具生成的。

人工智能驱动的3D工作流程解锁了无限的创意可能性。🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1750340749768691734#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750340749768691734#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 02:12:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本视频中的所有3D模型都是使用Luma AI的Genie 3D模型工具生成的。<br />
<br />
人工智能驱动的3D工作流程解锁了无限的创意可能性。🫡</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAyMzc0OTk2MjI5MjQyODgvcHUvaW1nL05fdFRGNU13UGl4dUlFTk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1750229816039592354#m</id>
            <title>RT by @xiaohuggg: #开源项目推荐：Nextra

如果你想搭建一个使用Markdown静态文件（MDX）的漂亮博客网站，或者是一个文档网站，可以考虑Nextra，基于Nextjs，部署前会编译成静态网页，支持语法高亮等高级语法。支持站内搜索。

https://github.com/shuding/nextra</title>
            <link>https://nitter.cz/dotey/status/1750229816039592354#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1750229816039592354#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 18:51:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23开源项目推荐">#开源项目推荐</a>：Nextra<br />
<br />
如果你想搭建一个使用Markdown静态文件（MDX）的漂亮博客网站，或者是一个文档网站，可以考虑Nextra，基于Nextjs，部署前会编译成静态网页，支持语法高亮等高级语法。支持站内搜索。<br />
<br />
<a href="https://github.com/shuding/nextra">github.com/shuding/nextra</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VvUWhiT1djQUE4NDE1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750331578302259493#m</id>
            <title>R to @xiaohuggg: 他们还开发了一个AI编程工具：CS50 Duck 

CS50 Duck包含 http://CS50.ai 网站和 VS 插件，是 CS50 课程推出的一个编程辅助工具。其中，http://CS50.ai 网站可以通过 GitHub 账号授权登录并免费使用。

不同于 ChatGPT 或 GitHub Copilot 直接给大段代码，CS50 Duck 更像是一位循循善诱的助教，尝试引导你去寻找答案。

http://CS50.ai 的使用限制机制是 ♥ 的数量，每次开始有10个 ♥，每次互动消耗一个，每三分钟恢复一个。

课程明确规定，学习过程中只允许使用 CS50 课内提供的AI工具，不能借助其他的生成式AI工具，有效避免了AI带来的学术诚信危机。

学生大量的正向反馈表明，谨慎地将AI整合到教育环境中，通过提供持续的、定制化的支持来增强学习体验，是一种更科学的教育方式。</title>
            <link>https://nitter.cz/xiaohuggg/status/1750331578302259493#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750331578302259493#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 01:35:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他们还开发了一个AI编程工具：CS50 Duck <br />
<br />
CS50 Duck包含 <a href="http://CS50.ai">CS50.ai</a> 网站和 VS 插件，是 CS50 课程推出的一个编程辅助工具。其中，<a href="http://CS50.ai">CS50.ai</a> 网站可以通过 GitHub 账号授权登录并免费使用。<br />
<br />
不同于 ChatGPT 或 GitHub Copilot 直接给大段代码，CS50 Duck 更像是一位循循善诱的助教，尝试引导你去寻找答案。<br />
<br />
<a href="http://CS50.ai">CS50.ai</a> 的使用限制机制是 ♥ 的数量，每次开始有10个 ♥，每次互动消耗一个，每三分钟恢复一个。<br />
<br />
课程明确规定，学习过程中只允许使用 CS50 课内提供的AI工具，不能借助其他的生成式AI工具，有效避免了AI带来的学术诚信危机。<br />
<br />
学生大量的正向反馈表明，谨慎地将AI整合到教育环境中，通过提供持续的、定制化的支持来增强学习体验，是一种更科学的教育方式。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VuRVVaX2FRQUFuX2xmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750331575081026039#m</id>
            <title>哈佛大学CS50x 2024课程

CS50简介： 这是哈佛大学的一门计算机科学和编程入门课程，适合专业学生和非专业学生，无论是否有编程经验（CS50学生有三分之二之前从未学过编程）。

教学内容： 课程不仅仅是教授编程语言的技术课程，它还着重于教学生如何解决问题，无论是通过编码还是不编码的方式。

课程由哈佛大学计算机科学实践教授 David J. Malan @davidjmalan 教授，课程妙趣横生。

涵盖计算思维、抽象、算法、数据结构以及更广泛的计算机科学概念。问题集灵感来源于艺术、人文、社会科学和科学。

编程语言： 课程从C语言开始，学习函数、变量、条件语句、循环等，以及计算机本身如何工作。随后转向Python，这是一种更高级的语言。还有SQL、HTML、CSS和JavaScript。

生可以在edX这个在线学习平台上注册，并完成CS50x课程的所有要求。

完成课程后，学生可以选择支付费用获得由edX颁发的官方验证证书。

开始学习：https://cs50.harvard.edu/x/2024/

中英文双语字幕  https://www.bilibili.com/video/BV16k4y1X7KZ</title>
            <link>https://nitter.cz/xiaohuggg/status/1750331575081026039#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750331575081026039#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 01:35:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈佛大学CS50x 2024课程<br />
<br />
CS50简介： 这是哈佛大学的一门计算机科学和编程入门课程，适合专业学生和非专业学生，无论是否有编程经验（CS50学生有三分之二之前从未学过编程）。<br />
<br />
教学内容： 课程不仅仅是教授编程语言的技术课程，它还着重于教学生如何解决问题，无论是通过编码还是不编码的方式。<br />
<br />
课程由哈佛大学计算机科学实践教授 David J. Malan <a href="https://nitter.cz/davidjmalan" title="David J. Malan">@davidjmalan</a> 教授，课程妙趣横生。<br />
<br />
涵盖计算思维、抽象、算法、数据结构以及更广泛的计算机科学概念。问题集灵感来源于艺术、人文、社会科学和科学。<br />
<br />
编程语言： 课程从C语言开始，学习函数、变量、条件语句、循环等，以及计算机本身如何工作。随后转向Python，这是一种更高级的语言。还有SQL、HTML、CSS和JavaScript。<br />
<br />
生可以在edX这个在线学习平台上注册，并完成CS50x课程的所有要求。<br />
<br />
完成课程后，学生可以选择支付费用获得由edX颁发的官方验证证书。<br />
<br />
开始学习：<a href="https://cs50.harvard.edu/x/2024/">cs50.harvard.edu/x/2024/</a><br />
<br />
中英文双语字幕  <a href="https://www.bilibili.com/video/BV16k4y1X7KZ">bilibili.com/video/BV16k4y1X…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VuRmJ3aWE4QUFuMlB3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750174478431391774#m</id>
            <title>马上要发售了

据说还能接入ChatGPT

高低要整一台😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1750174478431391774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750174478431391774#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 15:11:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马上要发售了<br />
<br />
据说还能接入ChatGPT<br />
<br />
高低要整一台😐</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwOTU5OTQwMTA5NTk4NzIvcHUvaW1nLzMzT3dDRzdSUzZzNXVUdHQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750173470946988230#m</id>
            <title>兄弟们，发财机会！

现在只要在Poe上创建自己的聊天机器人，分享你的机器人，如果有人通过你的聊天机器人订阅了Poe服务！

每带来一个给50美金🤑

希望奥特曼能学习下🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1750173470946988230#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750173470946988230#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 15:07:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，发财机会！<br />
<br />
现在只要在Poe上创建自己的聊天机器人，分享你的机器人，如果有人通过你的聊天机器人订阅了Poe服务！<br />
<br />
每带来一个给50美金🤑<br />
<br />
希望奥特曼能学习下🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VuZGZUeWJrQUFEdU9ULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141457636450774#m</id>
            <title>R to @xiaohuggg: Lumiere 还可以基于文本编辑视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141457636450774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141457636450774#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Lumiere 还可以基于文本编辑视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDEwODgyMDIxNjYyNzIvcHUvaW1nLzg3dVd0X2ZEUEdxa1FZb20uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141455681888556#m</id>
            <title>R to @xiaohuggg: 风格化视频生成</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141455681888556#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141455681888556#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>风格化视频生成</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDEwMjE2OTczMTQ4MTYvcHUvaW1nL0VxVDFObEZuWWlNU2czc3EuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141453794509040#m</id>
            <title>R to @xiaohuggg: 一些案例</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141453794509040#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141453794509040#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些案例</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDA3NjIwOTUwNzEyMzIvcHUvaW1nL3c0WUZwMDdpelMyU1MxRWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>