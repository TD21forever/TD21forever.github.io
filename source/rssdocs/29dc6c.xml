<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727149007074709589#m</id>
            <title>马斯克：

Grok会在下周对所有 X Premium+用户开放。

已经开始邀请部分用户体验全新的 Grok 聊天体验，如果受邀用户未购买 Premium + 订阅，会推荐其购买，价格为每月 16 美元。😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1727149007074709589#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727149007074709589#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 02:16:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克：<br />
<br />
Grok会在下周对所有 X Premium+用户开放。<br />
<br />
已经开始邀请部分用户体验全新的 Grok 聊天体验，如果受邀用户未购买 Premium + 订阅，会推荐其购买，价格为每月 16 美元。😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9nTlBxZ2JVQUEtNHZsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727148300292530186#m</id>
            <title>Runway Gen-2 Motion Brush运动笔刷教程（中文字幕）

🎥 精确控制运动：Motion Brush工具让你能够在视频中精确地控制特定区域的运动。

🔄 运动方向和速度设置：可以为选定区域指定运动方向和速度。

⚡ 独立的速度控制：可以独立控制水平、垂直和接近度方向的速度。

📹 与相机控制独立：Motion Brush的运动控制与相机运动独立，用户可以同时使用这两种功能进行实验。

使用方法：

1、打开Gen-2：在RunwayML的Gen-2中添加一个图像提示。

2、使用文本到视频：如果使用文本到视频功能，确保生成预览，然后使用其中一个预览图像作为图像输入。

3、选择Motion Brush：在工具中选择Motion Brush选项。

4、绘制控制区域：在视频中想要控制的区域上绘制，以指定运动方向和速度。

详细：https://academy.runwayml.com/gen2/gen2-motion-brush</title>
            <link>https://nitter.cz/xiaohuggg/status/1727148300292530186#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727148300292530186#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 02:13:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway Gen-2 Motion Brush运动笔刷教程（中文字幕）<br />
<br />
🎥 精确控制运动：Motion Brush工具让你能够在视频中精确地控制特定区域的运动。<br />
<br />
🔄 运动方向和速度设置：可以为选定区域指定运动方向和速度。<br />
<br />
⚡ 独立的速度控制：可以独立控制水平、垂直和接近度方向的速度。<br />
<br />
📹 与相机控制独立：Motion Brush的运动控制与相机运动独立，用户可以同时使用这两种功能进行实验。<br />
<br />
使用方法：<br />
<br />
1、打开Gen-2：在RunwayML的Gen-2中添加一个图像提示。<br />
<br />
2、使用文本到视频：如果使用文本到视频功能，确保生成预览，然后使用其中一个预览图像作为图像输入。<br />
<br />
3、选择Motion Brush：在工具中选择Motion Brush选项。<br />
<br />
4、绘制控制区域：在视频中想要控制的区域上绘制，以指定运动方向和速度。<br />
<br />
详细：<a href="https://academy.runwayml.com/gen2/gen2-motion-brush">academy.runwayml.com/gen2/ge…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjcxNDY2MzU3NjE5OTE2ODAvcHUvaW1nL3k0a0NOcDNhcVptRjJpcDIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727139721363788078#m</id>
            <title>R to @xiaohuggg: 一个本地运行模型的工具，我正在测试...

晚一点录视频😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1727139721363788078#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727139721363788078#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 01:39:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个本地运行模型的工具，我正在测试...<br />
<br />
晚一点录视频😂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727137100984889729#m</id>
            <title>正在安装 GPT4 ALL ...</title>
            <link>https://nitter.cz/xiaohuggg/status/1727137100984889729#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727137100984889729#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 01:29:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>正在安装 GPT4 ALL ...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9nRjlXOWJNQUFoNGVzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727131071719071950#m</id>
            <title>OpenAI 新任CEO Emmett Shear 称：

如果OpenAI董事会无法提供解雇Altman 的证据，那么他将辞职。

另据知情人士透露，奥特曼正与 OpenAI董事会成员和公司临时 CEO 正在进行谈判，谈判成员涉及CEO Emmett Shear、董事会成员 Adam D'Angelo 以及一些投资者，推动 Altman复职。

https://www.bloomberg.com/news/articles/2023-11-21/altman-openai-board-open-talks-to-negotiate-his-possible-return</title>
            <link>https://nitter.cz/xiaohuggg/status/1727131071719071950#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727131071719071950#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 01:05:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 新任CEO Emmett Shear 称：<br />
<br />
如果OpenAI董事会无法提供解雇Altman 的证据，那么他将辞职。<br />
<br />
另据知情人士透露，奥特曼正与 OpenAI董事会成员和公司临时 CEO 正在进行谈判，谈判成员涉及CEO Emmett Shear、董事会成员 Adam D'Angelo 以及一些投资者，推动 Altman复职。<br />
<br />
<a href="https://www.bloomberg.com/news/articles/2023-11-21/altman-openai-board-open-talks-to-negotiate-his-possible-return">bloomberg.com/news/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzAyMDUzMjI3MTYwMzcxMi9IRTZrSlNYNj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727128906120257663#m</id>
            <title>币安赵长鹏与美国政府达成和解，承认洗钱指控。

赵长鹏缴纳5000万美元罚金，辞去币安CEO职位，未来不得再参与公司事务。

币安接受美国政府指派的监督员。 ​​​

币安公司也将承认一项刑事指控，并同意支付总计43亿美元罚款，其中包括用于监管机构提出的罚款。</title>
            <link>https://nitter.cz/xiaohuggg/status/1727128906120257663#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727128906120257663#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:56:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>币安赵长鹏与美国政府达成和解，承认洗钱指控。<br />
<br />
赵长鹏缴纳5000万美元罚金，辞去币安CEO职位，未来不得再参与公司事务。<br />
<br />
币安接受美国政府指派的监督员。 ​​​<br />
<br />
币安公司也将承认一项刑事指控，并同意支付总计43亿美元罚款，其中包括用于监管机构提出的罚款。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9mLVdEeGJzQUFyRUo3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727124741423771768#m</id>
            <title>R to @xiaohuggg: 3D效果好像还不错👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1727124741423771768#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727124741423771768#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:40:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3D效果好像还不错👍</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3MTI0NzI5NTk1ODk5OTA1L2ltZy94a0hKMzNmQjN1dW1PWGNELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727123892324774089#m</id>
            <title>💡 Stability AI 发布其最新的Stable Video Diffustion 视频开源模型！支持：

- 文本到视频 
- 图像到视频 
- 14 或 25 帧，576 x 1024分辨率
- 多视图生成 
- 帧插值 
- 支持3D 场景
- 通过 LoRA 控制摄像机

Stability AI称正在开发一个新的网络平台，包括一个文本到视频的界面。这个工具将展示Stable Video Diffusion在广告、教育、娱乐等多个领域的实际应用。

详细介绍：https://stability.ai/news/stable-video-diffusion-open-ai-video-model

GitHub：https://github.com/Stability-AI/generative-models
论文：https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets
HuggingFace：https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt</title>
            <link>https://nitter.cz/xiaohuggg/status/1727123892324774089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727123892324774089#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:36:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>💡 Stability AI 发布其最新的Stable Video Diffustion 视频开源模型！支持：<br />
<br />
- 文本到视频 <br />
- 图像到视频 <br />
- 14 或 25 帧，576 x 1024分辨率<br />
- 多视图生成 <br />
- 帧插值 <br />
- 支持3D 场景<br />
- 通过 LoRA 控制摄像机<br />
<br />
Stability AI称正在开发一个新的网络平台，包括一个文本到视频的界面。这个工具将展示Stable Video Diffusion在广告、教育、娱乐等多个领域的实际应用。<br />
<br />
详细介绍：<a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model">stability.ai/news/stable-vid…</a><br />
<br />
GitHub：<a href="https://github.com/Stability-AI/generative-models">github.com/Stability-AI/gene…</a><br />
论文：<a href="https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets">stability.ai/research/stable…</a><br />
HuggingFace：<a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt">huggingface.co/stabilityai/s…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3MTIzNzE5Mzk3NzQ4NzM2L2ltZy9YT1lVQ0lzWldnRExIeWZOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727119128581100001#m</id>
            <title>Claude 发布2.1 版本，进行重大升级更新，发布一系列新功能：

1. **增强的处理能力**
   - 上下文处理量大幅提升：Claude 2.1 现在能处理高达 200K上下文标记，约等于 150,000 个单词或 500 页文本。

2. **准确性提升**
   - 虚假/幻觉陈述减少：相比之前版本，虚假或幻觉类陈述减少了 2 倍。

3. **新功能测试**
   - 早期支持企业级功能：允许将 Claude 连接到公司的 API、数据库和 Web 服务，目前处于测试阶段。

4. **可用性更新**
   - Claude 2.1 在线上线：升级版本已在 http://claude.ai 的 Thropic 托管聊天机器人界面和付费的 Claude Pro API 层推出。

5. **价格与访问**
   - 高级上下文限制：目前，200,000 个代币的上下文限制仅适用于 Pro 用户，价格与 ChatGPT Plus 订阅（目前暂停）相似，为每月 20 美元。

6. **系统提示功能**
   - 自定义指导与上下文：Anthropic 引入了系统提示功能，允许用户为 Claude 提供自定义说明和上下文，以提升其在特定任务上的性能。

   - 功能详细说明：
     - 设定目标与角色：允许用户设定目标、指定 Claude 的角色或语气。
     - 建立规则和约束：用户可以建立规则和约束。
     - 提供背景知识：可提供相关背景知识。
     - 定义验证标准：定义输出验证的标准。
   - 响应优化：通过这种方式提示 Claude，可以塑造更准确、更一致的响应，保持角色扮演的特征，并更可靠地遵循提供的指南。
   - 目标：系统提示的最终目的是增强 Claude 在预期实际应用中的能力。

详细内容：https://www.anthropic.com/index/claude-2-1</title>
            <link>https://nitter.cz/xiaohuggg/status/1727119128581100001#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727119128581100001#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:17:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Claude 发布2.1 版本，进行重大升级更新，发布一系列新功能：<br />
<br />
1. **增强的处理能力**<br />
   - 上下文处理量大幅提升：Claude 2.1 现在能处理高达 200K上下文标记，约等于 150,000 个单词或 500 页文本。<br />
<br />
2. **准确性提升**<br />
   - 虚假/幻觉陈述减少：相比之前版本，虚假或幻觉类陈述减少了 2 倍。<br />
<br />
3. **新功能测试**<br />
   - 早期支持企业级功能：允许将 Claude 连接到公司的 API、数据库和 Web 服务，目前处于测试阶段。<br />
<br />
4. **可用性更新**<br />
   - Claude 2.1 在线上线：升级版本已在 <a href="http://claude.ai">claude.ai</a> 的 Thropic 托管聊天机器人界面和付费的 Claude Pro API 层推出。<br />
<br />
5. **价格与访问**<br />
   - 高级上下文限制：目前，200,000 个代币的上下文限制仅适用于 Pro 用户，价格与 ChatGPT Plus 订阅（目前暂停）相似，为每月 20 美元。<br />
<br />
6. **系统提示功能**<br />
   - 自定义指导与上下文：Anthropic 引入了系统提示功能，允许用户为 Claude 提供自定义说明和上下文，以提升其在特定任务上的性能。<br />
<br />
   - 功能详细说明：<br />
     - 设定目标与角色：允许用户设定目标、指定 Claude 的角色或语气。<br />
     - 建立规则和约束：用户可以建立规则和约束。<br />
     - 提供背景知识：可提供相关背景知识。<br />
     - 定义验证标准：定义输出验证的标准。<br />
   - 响应优化：通过这种方式提示 Claude，可以塑造更准确、更一致的响应，保持角色扮演的特征，并更可靠地遵循提供的指南。<br />
   - 目标：系统提示的最终目的是增强 Claude 在预期实际应用中的能力。<br />
<br />
详细内容：<a href="https://www.anthropic.com/index/claude-2-1">anthropic.com/index/claude-2…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3MTE5MDU4OTY4MTk1MDcyL2ltZy9DTmhDdWZPNGRtZXVsdk1DLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727112003372892187#m</id>
            <title>奥特曼被解雇后

ChatGPT发布首个更新内容

语音功能向所有免费用户开放

更新下载客户端即可直接使用语音功能</title>
            <link>https://nitter.cz/xiaohuggg/status/1727112003372892187#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727112003372892187#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 23:49:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>奥特曼被解雇后<br />
<br />
ChatGPT发布首个更新内容<br />
<br />
语音功能向所有免费用户开放<br />
<br />
更新下载客户端即可直接使用语音功能</p>
<p><a href="https://nitter.cz/OpenAI/status/1727065166188274145#m">nitter.cz/OpenAI/status/1727065166188274145#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726966015052169228#m</id>
            <title>Stable to Video 要来了…

看效果是有点炸裂👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1726966015052169228#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726966015052169228#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 14:09:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable to Video 要来了…<br />
<br />
看效果是有点炸裂👍</p>
<p><a href="https://nitter.cz/EMostaque/status/1726929962211647855#m">nitter.cz/EMostaque/status/1726929962211647855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726955744866750674#m</id>
            <title>使用@suno_ai_ 和 Meta的 Audiocraft

制作的说唱歌曲😎

当然还融合了博主的一点点才华👌</title>
            <link>https://nitter.cz/xiaohuggg/status/1726955744866750674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726955744866750674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 13:28:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用<a href="https://nitter.cz/suno_ai_" title="Suno">@suno_ai_</a> 和 Meta的 Audiocraft<br />
<br />
制作的说唱歌曲😎<br />
<br />
当然还融合了博主的一点点才华👌</p>
<p><a href="https://nitter.cz/hanqing_me/status/1726934224043974725#m">nitter.cz/hanqing_me/status/1726934224043974725#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726943097387683902#m</id>
            <title>自从GPTs出来后，人人都能做，大量涌现，然后就出现了各种GPTs导航站。

最后的结果是连GPTs导航站都需要导航了😂

这个GPTs导航站思路不错，使用者可以对GPTs进行投票，把高质量的GPT筛选出来...避免鱼龙混杂！

还可以按类别筛选找到自己需要的GPT，还可提交自己的GPT...

🔗http://GPTseek.com

不过感觉目前还是比较简陋，有很大升级空间，比如可以增加评论点评、使用调用情况、以及更详细的GPTs功能升级迭代近况什么的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726943097387683902#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726943097387683902#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 12:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>自从GPTs出来后，人人都能做，大量涌现，然后就出现了各种GPTs导航站。<br />
<br />
最后的结果是连GPTs导航站都需要导航了😂<br />
<br />
这个GPTs导航站思路不错，使用者可以对GPTs进行投票，把高质量的GPT筛选出来...避免鱼龙混杂！<br />
<br />
还可以按类别筛选找到自己需要的GPT，还可提交自己的GPT...<br />
<br />
🔗<a href="http://GPTseek.com">GPTseek.com</a><br />
<br />
不过感觉目前还是比较简陋，有很大升级空间，比如可以增加评论点评、使用调用情况、以及更详细的GPTs功能升级迭代近况什么的。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY5MzkxNDIxOTI1MjUzMTIvcHUvaW1nL1A5TnFtYU5UcFA2UDFYNEkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726928308288561284#m</id>
            <title>Video-LLaVA：大型视觉语言模型 能更好地理解和处理图像和视频。

它通过一种特殊的技术，将图像和视频中的信息转换成类似于文字的格式，使得计算机能够用处理语言的方式来处理视觉信息。

这使得模型能够更好地理解视频内容，就像理解一段文字一样。

测试了下，感觉还不错的！但是要用英文😂

而且该模型能够同时处理文本和视觉信号，提供了一种多模态的理解能力，这对于自动问答系统和其他需要理解复杂视觉信息的应用非常有用。

Video-LLaVA在多个图像和视频基准测试中表现出色，显示了其在不同类型的视觉数据上的强大适应能力。

主要工作原理特点：

1、统一视觉表示：Video-LLaVA通过将图像和视频的特征预先对齐到一个统一的视觉特征空间中，解决了以往模型在处理视觉信息时存在的不对齐问题。这种方法使得大型语言模型（LLM）能够更有效地学习和理解多模态（图像和视频）信息。

2、联合训练：该模型不仅对图像和视频特征进行预对齐，还进行了图像和视频的联合训练。这种训练方式有助于提高模型在理解多模态信息方面的能力。

3、高效的模型结构：Video-LLaVA包括语言绑定编码器（LanguageBind encoders）用于提取原始视觉信号（如图像或视频）的特征，大语言模型（如Vicuna），视觉投影层，以及词嵌入层。这些组件共同工作，提供了一个高效且强大的视觉语言处理框架。

4、优越的性能：在多个图像和视频基准测试中，Video-LLaVA展示了优越的性能。它不仅在图像理解方面超越了先进的视觉语言模型，如mPLUG-owl-7B和InstructBLIP-7B，而且在视频理解方面也超越了专门为视频设计的模型，如Video-ChatGPT。

GitHub：https://github.com/PKU-YuanGroup/Video-LLaVA
论文：https://arxiv.org/abs/2311.10122
HuggingFace演示：https://huggingface.co/spaces/LanguageBind/Video-LLaVA
在线体验：https://replicate.com/nateraw/video-llava</title>
            <link>https://nitter.cz/xiaohuggg/status/1726928308288561284#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726928308288561284#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 11:39:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Video-LLaVA：大型视觉语言模型 能更好地理解和处理图像和视频。<br />
<br />
它通过一种特殊的技术，将图像和视频中的信息转换成类似于文字的格式，使得计算机能够用处理语言的方式来处理视觉信息。<br />
<br />
这使得模型能够更好地理解视频内容，就像理解一段文字一样。<br />
<br />
测试了下，感觉还不错的！但是要用英文😂<br />
<br />
而且该模型能够同时处理文本和视觉信号，提供了一种多模态的理解能力，这对于自动问答系统和其他需要理解复杂视觉信息的应用非常有用。<br />
<br />
Video-LLaVA在多个图像和视频基准测试中表现出色，显示了其在不同类型的视觉数据上的强大适应能力。<br />
<br />
主要工作原理特点：<br />
<br />
1、统一视觉表示：Video-LLaVA通过将图像和视频的特征预先对齐到一个统一的视觉特征空间中，解决了以往模型在处理视觉信息时存在的不对齐问题。这种方法使得大型语言模型（LLM）能够更有效地学习和理解多模态（图像和视频）信息。<br />
<br />
2、联合训练：该模型不仅对图像和视频特征进行预对齐，还进行了图像和视频的联合训练。这种训练方式有助于提高模型在理解多模态信息方面的能力。<br />
<br />
3、高效的模型结构：Video-LLaVA包括语言绑定编码器（LanguageBind encoders）用于提取原始视觉信号（如图像或视频）的特征，大语言模型（如Vicuna），视觉投影层，以及词嵌入层。这些组件共同工作，提供了一个高效且强大的视觉语言处理框架。<br />
<br />
4、优越的性能：在多个图像和视频基准测试中，Video-LLaVA展示了优越的性能。它不仅在图像理解方面超越了先进的视觉语言模型，如mPLUG-owl-7B和InstructBLIP-7B，而且在视频理解方面也超越了专门为视频设计的模型，如Video-ChatGPT。<br />
<br />
GitHub：<a href="https://github.com/PKU-YuanGroup/Video-LLaVA">github.com/PKU-YuanGroup/Vid…</a><br />
论文：<a href="https://arxiv.org/abs/2311.10122">arxiv.org/abs/2311.10122</a><br />
HuggingFace演示：<a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA">huggingface.co/spaces/Langua…</a><br />
在线体验：<a href="https://replicate.com/nateraw/video-llava">replicate.com/nateraw/video-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY5MjgwMDUwOTQ4NTA1NjAvcHUvaW1nL2U3YlA1UzBGOHl6czdqekwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726891372136013900#m</id>
            <title>百度：目前文心一言用户数已达7000万。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726891372136013900#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726891372136013900#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 09:12:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>百度：目前文心一言用户数已达7000万。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726880322879635723#m</id>
            <title>RealtimeTTS ：能够实时地将文本转换成语音

🚀 即时反应：文本输入的同时，RealtimeTTS立即开始语音合成，无需等待整个文本输入完毕。

🌊 流式处理：能够处理持续不断的文本流，而不仅限于单个、静态的文本块。

🔍 句子分割：使用先进的算法精准识别句子的结束点，加快语音合成的开始。

📏 适应不同长度：无论文本是长是短，都能保持快速响应，适用于各种长度的文本。

🌐 多引擎兼容：能够与多个语音合成引擎协同工作，例如Azure、Elevenlabs、Coqui XTTS等。

🔧 扩展性：它还允许添加自定义的文本到语音引擎，提供了更大的灵活性和扩展性。

RealtimeTTS非常适合需要实时语音反馈的应用场景的工具，如交互式教学、游戏、实时翻译或语音助手等。通过即时反应和流式处理，它能够提供一个流畅且自然的用户体验。

GitHub：https://github.com/KoljaB/RealtimeTTS</title>
            <link>https://nitter.cz/xiaohuggg/status/1726880322879635723#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726880322879635723#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 08:28:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RealtimeTTS ：能够实时地将文本转换成语音<br />
<br />
🚀 即时反应：文本输入的同时，RealtimeTTS立即开始语音合成，无需等待整个文本输入完毕。<br />
<br />
🌊 流式处理：能够处理持续不断的文本流，而不仅限于单个、静态的文本块。<br />
<br />
🔍 句子分割：使用先进的算法精准识别句子的结束点，加快语音合成的开始。<br />
<br />
📏 适应不同长度：无论文本是长是短，都能保持快速响应，适用于各种长度的文本。<br />
<br />
🌐 多引擎兼容：能够与多个语音合成引擎协同工作，例如Azure、Elevenlabs、Coqui XTTS等。<br />
<br />
🔧 扩展性：它还允许添加自定义的文本到语音引擎，提供了更大的灵活性和扩展性。<br />
<br />
RealtimeTTS非常适合需要实时语音反馈的应用场景的工具，如交互式教学、游戏、实时翻译或语音助手等。通过即时反应和流式处理，它能够提供一个流畅且自然的用户体验。<br />
<br />
GitHub：<a href="https://github.com/KoljaB/RealtimeTTS">github.com/KoljaB/RealtimeTT…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY4Nzg0OTQxOTY3MTE0MjQvcHUvaW1nLzRPdWlPZjZXZ2QwTFpCYWMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>