<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749022543170912309#m</id>
            <title>R to @xiaohuggg: 感觉是憋了很久，都能做咖啡了，稳定性很强。

通过深度学习开发的先进视觉模型，具有精确识别各种材料和尺寸的杯子和工具的能力。

熟练地制作拿铁咖啡...</title>
            <link>https://nitter.cz/xiaohuggg/status/1749022543170912309#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749022543170912309#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 10:54:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感觉是憋了很久，都能做咖啡了，稳定性很强。<br />
<br />
通过深度学习开发的先进视觉模型，具有精确识别各种材料和尺寸的杯子和工具的能力。<br />
<br />
熟练地制作拿铁咖啡...</p>
<p><a href="https://nitter.cz/MagicLab244144/status/1748651992648720570#m">nitter.cz/MagicLab244144/status/1748651992648720570#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749021418451574869#m</id>
            <title>一家今年刚成立的机器人创业公司：@MagicLab244144

网上搜不到这家公司的太多信息... 

他们放出了一段机器人演示视频，可以实现波士顿动力液压人形机器人的后空翻功能。🤓

全长仅1分钟的视频包含了不少料：电驱人形机器人，直接吊起三名壮汉...🫡

感觉今年机器人可能会迎来自己的“GPT时刻”...</title>
            <link>https://nitter.cz/xiaohuggg/status/1749021418451574869#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749021418451574869#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 10:49:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一家今年刚成立的机器人创业公司：<a href="https://nitter.cz/MagicLab244144" title="MagicLab">@MagicLab244144</a><br />
<br />
网上搜不到这家公司的太多信息... <br />
<br />
他们放出了一段机器人演示视频，可以实现波士顿动力液压人形机器人的后空翻功能。🤓<br />
<br />
全长仅1分钟的视频包含了不少料：电驱人形机器人，直接吊起三名壮汉...🫡<br />
<br />
感觉今年机器人可能会迎来自己的“GPT时刻”...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkwMjA4OTUzMjk2MTk5NjgvcHUvaW1nL3FpQWZfSnZ2NURhVGJVSnguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748959398226280732#m</id>
            <title>R to @xiaohuggg: 官方示范</title>
            <link>https://nitter.cz/xiaohuggg/status/1748959398226280732#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748959398226280732#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:43:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>官方示范</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgzNDQ1Nzg1NTY1MjI0OTcvcHUvaW1nL1Rqc2lkZVZLd0lVVzZoSmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748958891462062196#m</id>
            <title>R to @xiaohuggg: 案例 5</title>
            <link>https://nitter.cz/xiaohuggg/status/1748958891462062196#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748958891462062196#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:41:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>案例 5</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgxNDgzNzY0NzY5ODczOTIvcHUvaW1nL0E4MlJrQTB1TlBIUTJUQnkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748958631956193506#m</id>
            <title>R to @xiaohuggg: 案例 4</title>
            <link>https://nitter.cz/xiaohuggg/status/1748958631956193506#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748958631956193506#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:40:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>案例 4</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgwODY2MzUzNjU4Njc1MjIvcHUvaW1nL1d6cXZwV2VmOVBQcS1PTmYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748958200907649044#m</id>
            <title>R to @xiaohuggg: 案例 3</title>
            <link>https://nitter.cz/xiaohuggg/status/1748958200907649044#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748958200907649044#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:38:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>案例 3</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgwODAxNjg1MzMzODExMjAvcHUvaW1nL0tyMUNvRjhoOXNRd19kRE0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748957910951174411#m</id>
            <title>R to @xiaohuggg: 案例 2</title>
            <link>https://nitter.cz/xiaohuggg/status/1748957910951174411#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748957910951174411#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:37:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>案例 2</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDgyNjY5NjA2NDE1NDQxOTIvcHUvaW1nL2tkcVA4TEM0eHlFTEFwUjAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748957135235698828#m</id>
            <title>Runway

Multi-Motion Brush运动笔刷功能的

正确用法🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1748957135235698828#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748957135235698828#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 06:34:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway<br />
<br />
Multi-Motion Brush运动笔刷功能的<br />
<br />
正确用法🫡</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1747521907480842603#m">nitter.cz/xiaohuggg/status/1747521907480842603#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg0MzcxOTEyODQ3MjM3MTIvcHUvaW1nL05ra1hZZ202QlNONHJHdFQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748942568526844109#m</id>
            <title>TikTok似乎修改了规则

现在国内也可以直接刷TikTok了，以前拔卡、换区、梯子…都不能访问！

现在只需要在iPhone设置里面把地区改为香港、大陆之外即可！

然后梯子也挂香港、大陆之外即可访问了！

亲测有效…👀⬇️</title>
            <link>https://nitter.cz/xiaohuggg/status/1748942568526844109#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748942568526844109#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 05:36:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TikTok似乎修改了规则<br />
<br />
现在国内也可以直接刷TikTok了，以前拔卡、换区、梯子…都不能访问！<br />
<br />
现在只需要在iPhone设置里面把地区改为香港、大陆之外即可！<br />
<br />
然后梯子也挂香港、大陆之外即可访问了！<br />
<br />
亲测有效…👀⬇️</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ4OTQyMDY2NjE5NjUwMDQ5L2ltZy9YOHg5eEhad0ViUGsxOWZGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748914170580599022#m</id>
            <title>33岁日本作家Rie Kudan赢得日本顶级文学奖芥川奖，她在颁奖典礼上公开宣布，她使用了ChatGPT来写作她的获奖小说中大约5%的内容。

据报道，评委们称Rie Kudan的小说 “几乎完美无瑕”

她的科幻小说《Tokyo-to Dojo-to / Tokyo Sympathy Tower》讲述了一个高耸的监狱塔的故事，主题围绕人工智能。

芥川奖是日本纯文学领域的顶级奖项，每半年颁发一次，授予新兴作家。获奖者通常会受到大量媒体关注。

《日本时报》周五报道称，社交媒体上的反应迅速而严厉，许多评论者表示担心，如果允许人工智能竞争最高奖项，文学的未来会是什么样子？

有关在创意领域使用生成式AI的争议仍然很大，因为这些系统是基于大量其他作者作品的语料库训练的。

详细：https://www.vice.com/en/article/k7z58y/rie-kudan-akutagawa-prize-used-chatgpt</title>
            <link>https://nitter.cz/xiaohuggg/status/1748914170580599022#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748914170580599022#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 03:43:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>33岁日本作家Rie Kudan赢得日本顶级文学奖芥川奖，她在颁奖典礼上公开宣布，她使用了ChatGPT来写作她的获奖小说中大约5%的内容。<br />
<br />
据报道，评委们称Rie Kudan的小说 “几乎完美无瑕”<br />
<br />
她的科幻小说《Tokyo-to Dojo-to / Tokyo Sympathy Tower》讲述了一个高耸的监狱塔的故事，主题围绕人工智能。<br />
<br />
芥川奖是日本纯文学领域的顶级奖项，每半年颁发一次，授予新兴作家。获奖者通常会受到大量媒体关注。<br />
<br />
《日本时报》周五报道称，社交媒体上的反应迅速而严厉，许多评论者表示担心，如果允许人工智能竞争最高奖项，文学的未来会是什么样子？<br />
<br />
有关在创意领域使用生成式AI的争议仍然很大，因为这些系统是基于大量其他作者作品的语料库训练的。<br />
<br />
详细：<a href="https://www.vice.com/en/article/k7z58y/rie-kudan-akutagawa-prize-used-chatgpt">vice.com/en/article/k7z58y/r…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VWaWhFRGJ3QUE2MGRzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748910327658598697#m</id>
            <title>Medivis SurgicalAR ：已通过FDA批准、由AI驱动的增强现实手术平台。

通过使用AR全息图像，医生可以更精确地规划手术步骤，并在执行过程中获得实时的视觉辅助。

使医生能够以前所未有的细节和清晰度查看病人的内部结构。从而提高手术的成功率和安全性。

在手术过程中还能提供即时反馈。

在神经外科、骨科和重建手术等领域，SurgicalAR 提供了一种新的手术视觉化方法，使医生能够在手术过程中看到高度详细的全息图像。

Medivis的技术已在超过40家家医院和医学院中部署，支持医生在手术室、ICU和诊所中使用，为不同医疗情景提供支持。

Medivis SurgicalAR 的技术集成：

1、增强现实（AR）：SurgicalAR 使用增强现实技术，通过全息图像为医生提供手术过程中的视觉辅助。这种技术能够将3D图像直观地展示在医生的视野中，使他们能够更精确地定位和了解病人的内部结构。

2、人工智能（AI）：SurgicalAR 结合了人工智能，特别是在图像处理和数据分析方面。AI能够帮助解析医学图像，提供更深入的诊断信息，并辅助医生制定更有效的手术计划。

3、计算机视觉：用于精确识别和追踪手术区域，提高手术的精确性和安全性。它还支持实时图像分析，确保手术过程的精准度。

Medivis SurgicalAR 的主要功能包括：

1、增强现实导航：使用增强现实技术在手术中提供全息图像导航，帮助医生更准确地定位和执行手术步骤。

2、高级可视化：提供高度真实的医学图像渲染，使医生能够以前所未有的细节和清晰度查看病人的内部结构。

3、手势控制操作：通过AR/AI用户界面，医生可以使用直观的手势来操作和探索3D医学图像，如旋转、缩放和裁剪。

4、数据集成：允许从多种来源导入医学数据（如CT、MRI和超声数据），并快速将这些数据转换为3D全息图像。

5、实时反馈：在手术过程中提供即时反馈，帮助医生做出更准确的临床决策。

6、个性化手术规划：根据病人特定的医学数据定制手术方案，提高手术的个性化和精准度。

7、多学科应用：适用于多个医学领域，包括神经外科、骨科和重建手术等。

Medivis SurgicalAR 通过结合增强现实和人工智能技术，为医生提供了一个强大的工具，以提高手术的精确性和安全性，并改善病人的治疗结果。

官网：https://www.medivis.com/surgical-ar

视频介绍：http://bit.ly/3O5RwGA</title>
            <link>https://nitter.cz/xiaohuggg/status/1748910327658598697#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748910327658598697#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 03:28:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Medivis SurgicalAR ：已通过FDA批准、由AI驱动的增强现实手术平台。<br />
<br />
通过使用AR全息图像，医生可以更精确地规划手术步骤，并在执行过程中获得实时的视觉辅助。<br />
<br />
使医生能够以前所未有的细节和清晰度查看病人的内部结构。从而提高手术的成功率和安全性。<br />
<br />
在手术过程中还能提供即时反馈。<br />
<br />
在神经外科、骨科和重建手术等领域，SurgicalAR 提供了一种新的手术视觉化方法，使医生能够在手术过程中看到高度详细的全息图像。<br />
<br />
Medivis的技术已在超过40家家医院和医学院中部署，支持医生在手术室、ICU和诊所中使用，为不同医疗情景提供支持。<br />
<br />
Medivis SurgicalAR 的技术集成：<br />
<br />
1、增强现实（AR）：SurgicalAR 使用增强现实技术，通过全息图像为医生提供手术过程中的视觉辅助。这种技术能够将3D图像直观地展示在医生的视野中，使他们能够更精确地定位和了解病人的内部结构。<br />
<br />
2、人工智能（AI）：SurgicalAR 结合了人工智能，特别是在图像处理和数据分析方面。AI能够帮助解析医学图像，提供更深入的诊断信息，并辅助医生制定更有效的手术计划。<br />
<br />
3、计算机视觉：用于精确识别和追踪手术区域，提高手术的精确性和安全性。它还支持实时图像分析，确保手术过程的精准度。<br />
<br />
Medivis SurgicalAR 的主要功能包括：<br />
<br />
1、增强现实导航：使用增强现实技术在手术中提供全息图像导航，帮助医生更准确地定位和执行手术步骤。<br />
<br />
2、高级可视化：提供高度真实的医学图像渲染，使医生能够以前所未有的细节和清晰度查看病人的内部结构。<br />
<br />
3、手势控制操作：通过AR/AI用户界面，医生可以使用直观的手势来操作和探索3D医学图像，如旋转、缩放和裁剪。<br />
<br />
4、数据集成：允许从多种来源导入医学数据（如CT、MRI和超声数据），并快速将这些数据转换为3D全息图像。<br />
<br />
5、实时反馈：在手术过程中提供即时反馈，帮助医生做出更准确的临床决策。<br />
<br />
6、个性化手术规划：根据病人特定的医学数据定制手术方案，提高手术的个性化和精准度。<br />
<br />
7、多学科应用：适用于多个医学领域，包括神经外科、骨科和重建手术等。<br />
<br />
Medivis SurgicalAR 通过结合增强现实和人工智能技术，为医生提供了一个强大的工具，以提高手术的精确性和安全性，并改善病人的治疗结果。<br />
<br />
官网：<a href="https://www.medivis.com/surgical-ar">medivis.com/surgical-ar</a><br />
<br />
视频介绍：<a href="http://bit.ly/3O5RwGA">bit.ly/3O5RwGA</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg5MDY4MTUwNjMyOTgwNDgvcHUvaW1nL2pnaFRRRmxkdTh3ZEk4SmIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748900504468619358#m</id>
            <title>NFL Pick-Em's LLM Bot ：基于 OpenAI 构建的AI代理，可以预测ESPN体育比赛结果。

在 2023 NFL季赛中，这个代理赢得了作者当地群组的 pick-em's 比赛，并在 ESPN 所有用户中排名前 15% （约一百万用户参加）。

2023 年 ESPN 的顶级球员人类预测者的准确率大约是71%，而代理的准确率则是60%。

工作原理：

1、数据收集：使用网络爬虫技术，从ESPN网站上爬取相关的统计数据和新闻文章。爬取的数据包括各个NFL团队的信息、每周比赛列表、进攻、防守、失误和特殊团队的统计数据，以及新闻头条和文章内容。

统计数据，包括总码数、传球码数、冲球码数、每场比赛得分等，以及防守数据，如擒抱、拦截和抢断等。

特别队伍统计数据：收集关于特别队伍表现的数据，例如射门成功率、平均踢球距离和回传码数。

失误数据：包括数据如失误次数和被拦截次数。

相关新闻：收集关于球队的最新消息，包括球员伤病、转会、教练更换或场外事件，这些可能影响球队表现。

2、数据分析：利用GPT 4，对收集到的数据进行分析。程序会将每场比赛的相关统计数据和新闻信息输入给GPT 4，并让它预测比赛的获胜者。

将收集到的数据输入LLM。模型将分析这些统计数据和新闻，以了解两支球队的当前状态、优势、劣势和近期变化。

可以对LLM进行训练或微调，使其更好地理解类似比赛的历史数据模式和结果。

3、预测获胜者：基于分析的结果，代理知道如何处理特定的任务，例如分析新闻文章（确定文章主要讨论的团队、提取文章摘要等）和预测比赛获胜者。

模型还可以提供预测的理由，基于分析的数据，使预测更加深入。

4、优化和反馈循环：比赛结束后，可以将实际结果反馈到系统中，以帮助优化预测算法，随着时间的推移提高准确性。

尽管LLM能够处理和分析大量数据，但其预测的质量和准确性在很大程度上取决于所提供数据的质量以及模型对这一特定任务的训练程度。这种方法结合了统计分析和基于AI的预测建模，为体育分析提供了一种新颖的方式。

GitHub：https://github.com/stevekrenzel/pick-ems</title>
            <link>https://nitter.cz/xiaohuggg/status/1748900504468619358#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748900504468619358#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 02:49:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NFL Pick-Em's LLM Bot ：基于 OpenAI 构建的AI代理，可以预测ESPN体育比赛结果。<br />
<br />
在 2023 NFL季赛中，这个代理赢得了作者当地群组的 pick-em's 比赛，并在 ESPN 所有用户中排名前 15% （约一百万用户参加）。<br />
<br />
2023 年 ESPN 的顶级球员人类预测者的准确率大约是71%，而代理的准确率则是60%。<br />
<br />
工作原理：<br />
<br />
1、数据收集：使用网络爬虫技术，从ESPN网站上爬取相关的统计数据和新闻文章。爬取的数据包括各个NFL团队的信息、每周比赛列表、进攻、防守、失误和特殊团队的统计数据，以及新闻头条和文章内容。<br />
<br />
统计数据，包括总码数、传球码数、冲球码数、每场比赛得分等，以及防守数据，如擒抱、拦截和抢断等。<br />
<br />
特别队伍统计数据：收集关于特别队伍表现的数据，例如射门成功率、平均踢球距离和回传码数。<br />
<br />
失误数据：包括数据如失误次数和被拦截次数。<br />
<br />
相关新闻：收集关于球队的最新消息，包括球员伤病、转会、教练更换或场外事件，这些可能影响球队表现。<br />
<br />
2、数据分析：利用GPT 4，对收集到的数据进行分析。程序会将每场比赛的相关统计数据和新闻信息输入给GPT 4，并让它预测比赛的获胜者。<br />
<br />
将收集到的数据输入LLM。模型将分析这些统计数据和新闻，以了解两支球队的当前状态、优势、劣势和近期变化。<br />
<br />
可以对LLM进行训练或微调，使其更好地理解类似比赛的历史数据模式和结果。<br />
<br />
3、预测获胜者：基于分析的结果，代理知道如何处理特定的任务，例如分析新闻文章（确定文章主要讨论的团队、提取文章摘要等）和预测比赛获胜者。<br />
<br />
模型还可以提供预测的理由，基于分析的数据，使预测更加深入。<br />
<br />
4、优化和反馈循环：比赛结束后，可以将实际结果反馈到系统中，以帮助优化预测算法，随着时间的推移提高准确性。<br />
<br />
尽管LLM能够处理和分析大量数据，但其预测的质量和准确性在很大程度上取决于所提供数据的质量以及模型对这一特定任务的训练程度。这种方法结合了统计分析和基于AI的预测建模，为体育分析提供了一种新颖的方式。<br />
<br />
GitHub：<a href="https://github.com/stevekrenzel/pick-ems">github.com/stevekrenzel/pick…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VWWHZQZGJVQUFQUkJyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748896852144161055#m</id>
            <title>有人说是AI

这不是视频来啦

😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1748896852144161055#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748896852144161055#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 02:34:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有人说是AI<br />
<br />
这不是视频来啦<br />
<br />
😎</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1748708069654098010#m">nitter.cz/xiaohuggg/status/1748708069654098010#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ4ODk2Nzg0Mzg5NDE0OTEyL2ltZy9ERHNreVQ2SGlFcFhuajRDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748708099052150825#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748708099052150825#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748708099052150825#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 14:04:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3ZkRGFNQUFPUzdtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748708084326043859#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748708084326043859#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748708084326043859#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 14:04:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3VqcWFRQUVhb2phLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3VqcWFRQUk5cTBXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748708069654098010#m</id>
            <title>这个节目可以上春晚

哈哈哈哈</title>
            <link>https://nitter.cz/xiaohuggg/status/1748708069654098010#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748708069654098010#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 14:04:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个节目可以上春晚<br />
<br />
哈哈哈哈</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3R1emFzQUFfVlFoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748683726417256771#m</id>
            <title>SAMPLE：自动化蛋白质设计系统

它可以自己设计和测试新的蛋白质，而不需要人类的帮助。就像一个能自己做实验的机器人科学家。

它能自主学习蛋白质的结构和功能之间的关系，然后自己进行蛋白质设计，同时在实验室里自动进行测试。

SAMPLE由一个AI代理驱动，可广泛应用于生物工程和合成生物学。

研究团队用这个系统在一个特定的蛋白质领域（糖苷水解酶）进行了实验。他们让四个这样的机器人系统自主工作，这些系统通过自己的学习和实验，成功地创造出了一些比原始蛋白质更稳定的新蛋白质。

背景知识：

蛋白质工程是一个复杂的工程，用于创造具有有用功能和行为的新蛋白质，但它过程缓慢、费力且需要专业知识，限制了其广泛应用。

威斯康星大学麦迪逊分校的研究人员开发出了一个结合人工智能和实验自动化的系统，可以在没有人工干预的情况下自主地进行蛋白质工程。

SAMPLE主要能力：

1、自主设计蛋白质：SAMPLE能够自己设计新的蛋白质结构，这是基于它对蛋白质序列与其功能关系的理解。

2、自动化实验：SAMPLE通过全自动化的实验室设备来测试它设计的蛋白质。这包括合成基因、表达蛋白质，以及进行生化活性的测量。

3、数据驱动的优化：SAMPLE通过分析实验结果来不断学习和优化，以改进其对蛋白质设计的理解。

工作原理：

1、智能代理：SAMPLE包括一个智能代理，这个代理利用已有的数据来学习蛋白质序列和功能之间的关系。这相当于一个内部模型，用于预测哪些蛋白质设计可能是有效的。

2、实验反馈循环：智能代理设计蛋白质后，会将这些设计发送到实验室环境中进行测试。然后，它会接收实验数据并用这些信息来更新和改进其内部模型。

3、探索与优化：SAMPLE在实验过程中平衡探索（尝试新的和不确定的设计）和优化（根据现有知识改进设计）。

该研究结果已经发表在了Nature上：https://www.nature.com/articles/s44286-023-00002-4

论文：https://www.nature.com/articles/s44286-023-00002-4.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1748683726417256771#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748683726417256771#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:27:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SAMPLE：自动化蛋白质设计系统<br />
<br />
它可以自己设计和测试新的蛋白质，而不需要人类的帮助。就像一个能自己做实验的机器人科学家。<br />
<br />
它能自主学习蛋白质的结构和功能之间的关系，然后自己进行蛋白质设计，同时在实验室里自动进行测试。<br />
<br />
SAMPLE由一个AI代理驱动，可广泛应用于生物工程和合成生物学。<br />
<br />
研究团队用这个系统在一个特定的蛋白质领域（糖苷水解酶）进行了实验。他们让四个这样的机器人系统自主工作，这些系统通过自己的学习和实验，成功地创造出了一些比原始蛋白质更稳定的新蛋白质。<br />
<br />
背景知识：<br />
<br />
蛋白质工程是一个复杂的工程，用于创造具有有用功能和行为的新蛋白质，但它过程缓慢、费力且需要专业知识，限制了其广泛应用。<br />
<br />
威斯康星大学麦迪逊分校的研究人员开发出了一个结合人工智能和实验自动化的系统，可以在没有人工干预的情况下自主地进行蛋白质工程。<br />
<br />
SAMPLE主要能力：<br />
<br />
1、自主设计蛋白质：SAMPLE能够自己设计新的蛋白质结构，这是基于它对蛋白质序列与其功能关系的理解。<br />
<br />
2、自动化实验：SAMPLE通过全自动化的实验室设备来测试它设计的蛋白质。这包括合成基因、表达蛋白质，以及进行生化活性的测量。<br />
<br />
3、数据驱动的优化：SAMPLE通过分析实验结果来不断学习和优化，以改进其对蛋白质设计的理解。<br />
<br />
工作原理：<br />
<br />
1、智能代理：SAMPLE包括一个智能代理，这个代理利用已有的数据来学习蛋白质序列和功能之间的关系。这相当于一个内部模型，用于预测哪些蛋白质设计可能是有效的。<br />
<br />
2、实验反馈循环：智能代理设计蛋白质后，会将这些设计发送到实验室环境中进行测试。然后，它会接收实验数据并用这些信息来更新和改进其内部模型。<br />
<br />
3、探索与优化：SAMPLE在实验过程中平衡探索（尝试新的和不确定的设计）和优化（根据现有知识改进设计）。<br />
<br />
该研究结果已经发表在了Nature上：<a href="https://www.nature.com/articles/s44286-023-00002-4">nature.com/articles/s44286-0…</a><br />
<br />
论文：<a href="https://www.nature.com/articles/s44286-023-00002-4.pdf">nature.com/articles/s44286-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTTm5nM2E4QUFmUGcyLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTTnZRd2FzQUEyZ3Z0LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTTjBFd2JjQUFfeG9TLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748572050271420663#m</id>
            <title>WhisperSpeech：一个开源的文本到语音系统

牛P的是它是通过对OpenAI的Whisper语音识别模型反向工程来实现的。

通过这种反转过程，WhisperSpeech能够接收文本输入，并利用修改后的Whisper模型生成听起来自然的语音输出。

输出的语音在发音准确性和自然度方面都非常的优秀。

WhisperSpeech 项目路线图：

-声学标记提取：改进声学标记的提取过程。

-语义标记提取：使用Whisper模型生成和量化语义标记。

-S->A模型转换：开发将语义标记转换为声学标记的模型。

-T->S模型转换：实现从文本标记到语义标记的转换。

-提升EnCodec语音质量：优化EnCodec模型以提高语音合成质量。

-短句推理优化：改善系统处理短句的能力。

-扩展情感语音数据集：收集更大的情感语音数据。

-文档化LibriLight数据集：详细记录HuggingFace上的数据集。

-多语言语音收集：聚集社区资源，收集多种语言的语音。

-训练多语言模型：开发支持多语言的文本到语音模型。

GitHub：https://github.com/collabora/WhisperSpeech
网站：https://collabora.github.io/WhisperSpeech/
在线体验：https://replicate.com/lucataco/whisperspeech-small</title>
            <link>https://nitter.cz/xiaohuggg/status/1748572050271420663#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748572050271420663#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 05:03:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WhisperSpeech：一个开源的文本到语音系统<br />
<br />
牛P的是它是通过对OpenAI的Whisper语音识别模型反向工程来实现的。<br />
<br />
通过这种反转过程，WhisperSpeech能够接收文本输入，并利用修改后的Whisper模型生成听起来自然的语音输出。<br />
<br />
输出的语音在发音准确性和自然度方面都非常的优秀。<br />
<br />
WhisperSpeech 项目路线图：<br />
<br />
-声学标记提取：改进声学标记的提取过程。<br />
<br />
-语义标记提取：使用Whisper模型生成和量化语义标记。<br />
<br />
-S->A模型转换：开发将语义标记转换为声学标记的模型。<br />
<br />
-T->S模型转换：实现从文本标记到语义标记的转换。<br />
<br />
-提升EnCodec语音质量：优化EnCodec模型以提高语音合成质量。<br />
<br />
-短句推理优化：改善系统处理短句的能力。<br />
<br />
-扩展情感语音数据集：收集更大的情感语音数据。<br />
<br />
-文档化LibriLight数据集：详细记录HuggingFace上的数据集。<br />
<br />
-多语言语音收集：聚集社区资源，收集多种语言的语音。<br />
<br />
-训练多语言模型：开发支持多语言的文本到语音模型。<br />
<br />
GitHub：<a href="https://github.com/collabora/WhisperSpeech">github.com/collabora/Whisper…</a><br />
网站：<a href="https://collabora.github.io/WhisperSpeech/">collabora.github.io/WhisperS…</a><br />
在线体验：<a href="https://replicate.com/lucataco/whisperspeech-small">replicate.com/lucataco/whisp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg1NDQ2MzE2OTkyODM5NjgvcHUvaW1nL3ZTSWtUOGNUdjdKaFlZcHIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>