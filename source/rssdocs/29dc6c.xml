<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740645327416492486#m</id>
            <title>R to @xiaohuggg: FlowPilot 白天测试视频：https://www.youtube.com/watch?v=mt86H67DhE0

FlowPilot 夜间测试视频：https://www.youtube.com/watch?v=FBB2XRMej9M</title>
            <link>https://nitter.cz/xiaohuggg/status/1740645327416492486#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740645327416492486#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 08:06:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>FlowPilot 白天测试视频：<a href="https://www.youtube.com/watch?v=mt86H67DhE0">youtube.com/watch?v=mt86H67D…</a><br />
<br />
FlowPilot 夜间测试视频：<a href="https://www.youtube.com/watch?v=FBB2XRMej9M">youtube.com/watch?v=FBB2XRMe…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczODIxNjgxMjY0OTgyODM1Mi9pbHFXSDZWdD9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740644167506968710#m</id>
            <title>R to @xiaohuggg: Flowpilot是一个基于 Openpilot 开发的的开源驾驶辅助系统

Openpilot以前发过，下面是它的介绍

https://x.com/xiaohuggg/status/1717374629944402201</title>
            <link>https://nitter.cz/xiaohuggg/status/1740644167506968710#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740644167506968710#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 08:01:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Flowpilot是一个基于 Openpilot 开发的的开源驾驶辅助系统<br />
<br />
Openpilot以前发过，下面是它的介绍<br />
<br />
<a href="https://x.com/xiaohuggg/status/1717374629944402201">x.com/xiaohuggg/status/17173…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1717374629944402201#m">nitter.cz/xiaohuggg/status/1717374629944402201#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740643895137222770#m</id>
            <title>Flowpilot：只需一台二手手机 让老旧的燃油车也能实现自动驾驶

Flowpilot是一个开源的自动驾驶辅助系统。它可以运行在Android系统上。

你只需要在一台普通智能手机上运行Flowpilot，然后通过熊猫硬件连接车辆的OBD-II 端口，再把它和手机相连。

这样就能用手机驱动车辆进行自动辅助驾驶。

支持包括本田、丰田、现代、日产、起亚、克莱斯勒、雷克萨斯、讴歌、奥迪、大众等超过200种车型。

演示视频为二手 Redmi Note 9 Pro 运行 Flowpilot，进行辅助驾驶Alto K10汽车！

Flowpilot可以实现的功能：

- 自适应巡航控制： 允许车辆自动调整速度以保持与前车的安全距离，无需驾驶员手动控制油门或刹车。

- 自动车道居中： 帮助车辆保持在车道中央，通过自动调整方向盘来纠正车辆的位置。

- 前方碰撞警告： 如果系统检测到前方有碰撞风险，它会发出警告，提醒驾驶员采取行动。

- 车道偏离警告： 当车辆无意中偏离车道时，系统会发出警告，提醒驾驶员调整车辆方向。

- 驾驶员监控： 监控驾驶员的注意力和疲劳程度，确保驾驶员在使用辅助驾驶功能时仍保持专注。

工作原理：

Flowpilot是一个基于 Openpilot 的开源驾驶辅助系统，它可以在运行 Linux、Windows 和 Android 的设备上运行。

Flowpilot 会记录道路摄像头、CAN、GPS、IMU、磁力计、温度传感器、崩溃和操作系统日志。驾驶员面向摄像头的数据仅在用户明确选择时记录。

硬件要求

兼容车辆： 需要一辆与Flowpilot兼容的车辆，这通常意味着车辆已经具备一定的自适应巡航控制和车道保持辅助功能。

专用硬件： Flowpilot项目中提到的熊猫（Panda）设备，这是一个连接车辆 OBD-II 端口的硬件，用于读取车辆的 CAN 数据。

手机或其他设备： 运行 Flowpilot软件的设备，如智能手机。

安装和配置

例子：在一辆兼容的车上安装和使用 Flowpilot

假设你有一辆兼容的车（比如一辆有自适应巡航控制和车道保持辅助功能的本田车），你想在这辆车上使用 "Flowpilot" 来增强驾驶辅助功能。

步骤 1: 准备硬件

购买熊猫设备： 这是一个连接到车辆 OBD-II 端口的小型硬件设备，用于读取车辆的数据。

准备手机： 确保你的手机能运行 "Flowpilot" 应用程序。

步骤 2: 安装硬件

连接熊猫设备： 将熊猫设备插入车辆的 OBD-II 端口（通常位于驾驶员座位下方的仪表板下）。

连接手机： 使用 USB 线将熊猫设备连接到你的手机。

步骤 3: 安装软件

下载和安装 Flowpilot： 在你的手机上下载并安装 Flowpilot应用程序。

步骤 4: 配置和校准

打开应用程序： 在手机上打开 Flowpilot应用程序。
进行配置： 按照应用程序的指示配置和校准系统，确保它能正确读取车辆数据。

步骤 5: 使用 Flowpilot

开车时使用： 当你开车时，Flowpilot将通过手机屏幕提供实时的驾驶辅助信息，如车道保持和自适应巡航控制提示。

驾驶员监控： 即使使用了 Flowpilot，作为驾驶员的你也需要时刻关注道路情况，并准备在必要时接管车辆控制。

警告⚠️

该软件为测试版本，仅用于研究目的，并非产品。使用者需自行负责遵守当地法律法规。

这个项目为技术爱好者和开发者提供了一个平台，用于探索和实验基于 openpilot 的驾驶辅助系统。

GitHub：https://github.com/flowdriveai/flowpilot</title>
            <link>https://nitter.cz/xiaohuggg/status/1740643895137222770#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740643895137222770#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 08:00:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Flowpilot：只需一台二手手机 让老旧的燃油车也能实现自动驾驶<br />
<br />
Flowpilot是一个开源的自动驾驶辅助系统。它可以运行在Android系统上。<br />
<br />
你只需要在一台普通智能手机上运行Flowpilot，然后通过熊猫硬件连接车辆的OBD-II 端口，再把它和手机相连。<br />
<br />
这样就能用手机驱动车辆进行自动辅助驾驶。<br />
<br />
支持包括本田、丰田、现代、日产、起亚、克莱斯勒、雷克萨斯、讴歌、奥迪、大众等超过200种车型。<br />
<br />
演示视频为二手 Redmi Note 9 Pro 运行 Flowpilot，进行辅助驾驶Alto K10汽车！<br />
<br />
Flowpilot可以实现的功能：<br />
<br />
- 自适应巡航控制： 允许车辆自动调整速度以保持与前车的安全距离，无需驾驶员手动控制油门或刹车。<br />
<br />
- 自动车道居中： 帮助车辆保持在车道中央，通过自动调整方向盘来纠正车辆的位置。<br />
<br />
- 前方碰撞警告： 如果系统检测到前方有碰撞风险，它会发出警告，提醒驾驶员采取行动。<br />
<br />
- 车道偏离警告： 当车辆无意中偏离车道时，系统会发出警告，提醒驾驶员调整车辆方向。<br />
<br />
- 驾驶员监控： 监控驾驶员的注意力和疲劳程度，确保驾驶员在使用辅助驾驶功能时仍保持专注。<br />
<br />
工作原理：<br />
<br />
Flowpilot是一个基于 Openpilot 的开源驾驶辅助系统，它可以在运行 Linux、Windows 和 Android 的设备上运行。<br />
<br />
Flowpilot 会记录道路摄像头、CAN、GPS、IMU、磁力计、温度传感器、崩溃和操作系统日志。驾驶员面向摄像头的数据仅在用户明确选择时记录。<br />
<br />
硬件要求<br />
<br />
兼容车辆： 需要一辆与Flowpilot兼容的车辆，这通常意味着车辆已经具备一定的自适应巡航控制和车道保持辅助功能。<br />
<br />
专用硬件： Flowpilot项目中提到的熊猫（Panda）设备，这是一个连接车辆 OBD-II 端口的硬件，用于读取车辆的 CAN 数据。<br />
<br />
手机或其他设备： 运行 Flowpilot软件的设备，如智能手机。<br />
<br />
安装和配置<br />
<br />
例子：在一辆兼容的车上安装和使用 Flowpilot<br />
<br />
假设你有一辆兼容的车（比如一辆有自适应巡航控制和车道保持辅助功能的本田车），你想在这辆车上使用 "Flowpilot" 来增强驾驶辅助功能。<br />
<br />
步骤 1: 准备硬件<br />
<br />
购买熊猫设备： 这是一个连接到车辆 OBD-II 端口的小型硬件设备，用于读取车辆的数据。<br />
<br />
准备手机： 确保你的手机能运行 "Flowpilot" 应用程序。<br />
<br />
步骤 2: 安装硬件<br />
<br />
连接熊猫设备： 将熊猫设备插入车辆的 OBD-II 端口（通常位于驾驶员座位下方的仪表板下）。<br />
<br />
连接手机： 使用 USB 线将熊猫设备连接到你的手机。<br />
<br />
步骤 3: 安装软件<br />
<br />
下载和安装 Flowpilot： 在你的手机上下载并安装 Flowpilot应用程序。<br />
<br />
步骤 4: 配置和校准<br />
<br />
打开应用程序： 在手机上打开 Flowpilot应用程序。<br />
进行配置： 按照应用程序的指示配置和校准系统，确保它能正确读取车辆数据。<br />
<br />
步骤 5: 使用 Flowpilot<br />
<br />
开车时使用： 当你开车时，Flowpilot将通过手机屏幕提供实时的驾驶辅助信息，如车道保持和自适应巡航控制提示。<br />
<br />
驾驶员监控： 即使使用了 Flowpilot，作为驾驶员的你也需要时刻关注道路情况，并准备在必要时接管车辆控制。<br />
<br />
警告⚠️<br />
<br />
该软件为测试版本，仅用于研究目的，并非产品。使用者需自行负责遵守当地法律法规。<br />
<br />
这个项目为技术爱好者和开发者提供了一个平台，用于探索和实验基于 openpilot 的驾驶辅助系统。<br />
<br />
GitHub：<a href="https://github.com/flowdriveai/flowpilot">github.com/flowdriveai/flowp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDA2NDIzMTE2NDA2NjYxMTIvcHUvaW1nL1paZEVneVdsdERnbWNodVEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740622292747702488#m</id>
            <title>据说美国现在是大众创新、万众创业的状态😂

热火朝天！</title>
            <link>https://nitter.cz/xiaohuggg/status/1740622292747702488#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740622292747702488#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 06:34:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据说美国现在是大众创新、万众创业的状态😂<br />
<br />
热火朝天！</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQwNjIyMjM4NDIzMDY0NTc3L2ltZy9HeV9lYjV1T1hYczZHRkRELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740577088980336803#m</id>
            <title>MotionCtrl：控制生成视频中的摄像机和物体运动。

MotionCtrl是一个视频运动控制器，它可以和AI视频生成工具一起使用，用来控制视频中的相机动作，比如转动、缩放和移动。

同时它还能控制视频中物体的移动路径或方式，可以让物体按照特定的路径或方式移动。

让AI生成的视频看起来更加生动和真实。

MotionCtrl能够和其他视频生成工具一起使用，增强视频的创造性和多样性。

由于能够独立控制相机和物体运动，MotionCtrl可以创造出更加复杂和动态的视频效果，增强视觉体验。

MotionCtrl”的主要功能：

1、控制相机运动： 它可以控制视频中的相机动作，比如转动、缩放和移动，让视频看起来更加生动和真实。

2、控制物体运动： 它还能控制视频中物体的移动，可以让物体按照特定的路径或方式移动。

3、灵活性和统一性： 这个工具既灵活又统一，它还能够同时处理相机和物体的运动，提供更丰富的视频制作可能性。

4、与其他视频生成方法集成：MotionCtrl控制器可以部署在LVDM、VideoCrafter1（LVDM的改进版本）和AnimateDiff上。它还与其他视频生成方法（如SVD）兼容，其集成的结果和模型将很快提供。

5、泛化能力： 一旦训练完成，可以适应广泛的相机姿态和轨迹。

技术细节：

MotionCtrl由两个主要部分组成：相机运动控制模块（CMCM）和物体运动控制模块（OMCM）。这两个模块分别负责处理视频中的相机运动和物体运动。

1、架构设计： MotionCtrl基于去噪U-Net结构进行扩展，增加了专门的模块来控制相机和物体的运动。这种设计使得它能够更有效地处理视频中的运动元素。

2、相机运动控制模块（CMCM）： 这个模块负责处理视频中的相机运动。它通过集成相机姿态序列来控制相机的移动，如平移、缩放和旋转。

3、物体运动控制模块（OMCM）： 这个模块负责控制视频中物体的运动。它使用卷积层和降采样来从物体轨迹中提取多尺度特征，从而指导物体的运动。

优势： 相比于其他方法，MotionCtrl能够更有效和独立地控制相机和物体运动，提供更细致的运动控制，并且对视频中物体的外观或形状影响最小。

项目及演示：https://wzhouxiff.github.io/projects/MotionCtrl/
论文：https://arxiv.org/pdf/2312.03641.pdf
GitHub：https://github.com/TencentARC/MotionCtrl
在线演示：https://huggingface.co/spaces/TencentARC/MotionCtrl</title>
            <link>https://nitter.cz/xiaohuggg/status/1740577088980336803#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740577088980336803#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 03:34:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MotionCtrl：控制生成视频中的摄像机和物体运动。<br />
<br />
MotionCtrl是一个视频运动控制器，它可以和AI视频生成工具一起使用，用来控制视频中的相机动作，比如转动、缩放和移动。<br />
<br />
同时它还能控制视频中物体的移动路径或方式，可以让物体按照特定的路径或方式移动。<br />
<br />
让AI生成的视频看起来更加生动和真实。<br />
<br />
MotionCtrl能够和其他视频生成工具一起使用，增强视频的创造性和多样性。<br />
<br />
由于能够独立控制相机和物体运动，MotionCtrl可以创造出更加复杂和动态的视频效果，增强视觉体验。<br />
<br />
MotionCtrl”的主要功能：<br />
<br />
1、控制相机运动： 它可以控制视频中的相机动作，比如转动、缩放和移动，让视频看起来更加生动和真实。<br />
<br />
2、控制物体运动： 它还能控制视频中物体的移动，可以让物体按照特定的路径或方式移动。<br />
<br />
3、灵活性和统一性： 这个工具既灵活又统一，它还能够同时处理相机和物体的运动，提供更丰富的视频制作可能性。<br />
<br />
4、与其他视频生成方法集成：MotionCtrl控制器可以部署在LVDM、VideoCrafter1（LVDM的改进版本）和AnimateDiff上。它还与其他视频生成方法（如SVD）兼容，其集成的结果和模型将很快提供。<br />
<br />
5、泛化能力： 一旦训练完成，可以适应广泛的相机姿态和轨迹。<br />
<br />
技术细节：<br />
<br />
MotionCtrl由两个主要部分组成：相机运动控制模块（CMCM）和物体运动控制模块（OMCM）。这两个模块分别负责处理视频中的相机运动和物体运动。<br />
<br />
1、架构设计： MotionCtrl基于去噪U-Net结构进行扩展，增加了专门的模块来控制相机和物体的运动。这种设计使得它能够更有效地处理视频中的运动元素。<br />
<br />
2、相机运动控制模块（CMCM）： 这个模块负责处理视频中的相机运动。它通过集成相机姿态序列来控制相机的移动，如平移、缩放和旋转。<br />
<br />
3、物体运动控制模块（OMCM）： 这个模块负责控制视频中物体的运动。它使用卷积层和降采样来从物体轨迹中提取多尺度特征，从而指导物体的运动。<br />
<br />
优势： 相比于其他方法，MotionCtrl能够更有效和独立地控制相机和物体运动，提供更细致的运动控制，并且对视频中物体的外观或形状影响最小。<br />
<br />
项目及演示：<a href="https://wzhouxiff.github.io/projects/MotionCtrl/">wzhouxiff.github.io/projects…</a><br />
论文：<a href="https://arxiv.org/pdf/2312.03641.pdf">arxiv.org/pdf/2312.03641.pdf</a><br />
GitHub：<a href="https://github.com/TencentARC/MotionCtrl">github.com/TencentARC/Motion…</a><br />
在线演示：<a href="https://huggingface.co/spaces/TencentARC/MotionCtrl">huggingface.co/spaces/Tencen…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDA1NzM1NjI4ODc2NzE4MDgvcHUvaW1nLzlNSVU5VXppNDFUZDFybmEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740566806069457215#m</id>
            <title>R to @xiaohuggg: 演示视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1740566806069457215#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740566806069457215#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 02:53:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDA1NjY2ODI1NjgyMTY1NzcvcHUvaW1nL2ZaWlYxWFhNQlI5RGZ3NXcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740564544660111865#m</id>
            <title>Spiritme AI ：一个有点类似Heygen，能帮你克隆虚拟形象的工具

你只需要通过iPhone随便拍摄一段大约5分钟的视频。它会用视频将用来捕捉你的外貌、动作和可能的表情。

然后克隆一个虚拟的你！

最牛的是它可以将PPT、PDF文件直接转换成解说视频，只需两次点击操作即可。

主要功能：

1、虚拟形象创建：使用iPhone拍摄一段大约5分钟的视频。这个视频将用来捕捉你的外貌、动作和可能的表情。制作这个视频时，你不需要有任何表演经验或特殊技能。简单地做自己就可以了。

2、文本到视频转换：允许用户输入文本，然后自动生成使用虚拟形象演说该文本的视频。虚拟形象可以根据视频的内容或脚本自然地展示各种表情。

3、AI拍摄助手：视频制作的自动化过程，比如自动调整拍摄角度、光线或其他视觉效果，以确保生成的视频具有高质量的视觉表现。

4、AI ScriptWriter功能：可以根据用户提供的信息或内容，如演示文稿、PDF文件，自动编写视频的对话或旁白，从而简化视频制作过程。只需两次点击操作。

5、多语言支持：用户可以生成多种语言的视频

体验：https://spiritme.tech/</title>
            <link>https://nitter.cz/xiaohuggg/status/1740564544660111865#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740564544660111865#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 02:45:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Spiritme AI ：一个有点类似Heygen，能帮你克隆虚拟形象的工具<br />
<br />
你只需要通过iPhone随便拍摄一段大约5分钟的视频。它会用视频将用来捕捉你的外貌、动作和可能的表情。<br />
<br />
然后克隆一个虚拟的你！<br />
<br />
最牛的是它可以将PPT、PDF文件直接转换成解说视频，只需两次点击操作即可。<br />
<br />
主要功能：<br />
<br />
1、虚拟形象创建：使用iPhone拍摄一段大约5分钟的视频。这个视频将用来捕捉你的外貌、动作和可能的表情。制作这个视频时，你不需要有任何表演经验或特殊技能。简单地做自己就可以了。<br />
<br />
2、文本到视频转换：允许用户输入文本，然后自动生成使用虚拟形象演说该文本的视频。虚拟形象可以根据视频的内容或脚本自然地展示各种表情。<br />
<br />
3、AI拍摄助手：视频制作的自动化过程，比如自动调整拍摄角度、光线或其他视觉效果，以确保生成的视频具有高质量的视觉表现。<br />
<br />
4、AI ScriptWriter功能：可以根据用户提供的信息或内容，如演示文稿、PDF文件，自动编写视频的对话或旁白，从而简化视频制作过程。只需两次点击操作。<br />
<br />
5、多语言支持：用户可以生成多种语言的视频<br />
<br />
体验：<a href="https://spiritme.tech/">spiritme.tech/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAzMzc5MTA4ODc2NjE1NjgvcHUvaW1nLzVQYTRkN0JYZWNydlpVd0YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740546965753217280#m</id>
            <title>英伟达发布了一款面向中国市场的新显卡：为RTX 4090D

这款显卡的起售价为12999元人民币，与RTX 4090的价格相同。

RTX 4090D在性能、效率和AI驱动的图形效果方面实现了显著的提升。它配备了24GB GDDR6X显存，基础频率高于RTX 4090，但由于综合运算性能的限制，其CUDA和Tensor核心数量低于RTX 4090。

RTX 4090 D的发布是英伟达针对美国新出口限制的回应，旨在为中国游戏玩家提供高性能的游戏体验。

这款显卡的CUDA核心数量从RTX 4090的16384个减至14592个，Tensor核心数量从512个减至456个，RT核心数量从128个减至114个。不过，RTX 4090 D的基础频率达到了2280 MHz，比RTX 4090的2235 MHz高，加速频率则与RTX 4090相同，均为2.52GHz。

这款显卡预计将在2024年1月中旬上架。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740546965753217280#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740546965753217280#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 01:35:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>英伟达发布了一款面向中国市场的新显卡：为RTX 4090D<br />
<br />
这款显卡的起售价为12999元人民币，与RTX 4090的价格相同。<br />
<br />
RTX 4090D在性能、效率和AI驱动的图形效果方面实现了显著的提升。它配备了24GB GDDR6X显存，基础频率高于RTX 4090，但由于综合运算性能的限制，其CUDA和Tensor核心数量低于RTX 4090。<br />
<br />
RTX 4090 D的发布是英伟达针对美国新出口限制的回应，旨在为中国游戏玩家提供高性能的游戏体验。<br />
<br />
这款显卡的CUDA核心数量从RTX 4090的16384个减至14592个，Tensor核心数量从512个减至456个，RT核心数量从128个减至114个。不过，RTX 4090 D的基础频率达到了2280 MHz，比RTX 4090的2235 MHz高，加速频率则与RTX 4090相同，均为2.52GHz。<br />
<br />
这款显卡预计将在2024年1月中旬上架。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NlcU80R2FBQUEtQk1ILmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NlcU80SGF3QUFibF9XLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NlcU80TWJBQUFvQkttLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740388169424814315#m</id>
            <title>😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1740388169424814315#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740388169424814315#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 15:04:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>😂</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQwMzg4MTQ5Nzc4NjYxMzc2L2ltZy9wT0tMUHpnVjBvRmhQNmJsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740378222200828408#m</id>
            <title>R to @xiaohuggg: 手机版太糊了，建议电脑看

马斯克特么的什么时候能解决这个视频糊的问题

2024年了，视频还这个德行！

怎么混啊</title>
            <link>https://nitter.cz/xiaohuggg/status/1740378222200828408#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740378222200828408#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 14:24:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>手机版太糊了，建议电脑看<br />
<br />
马斯克特么的什么时候能解决这个视频糊的问题<br />
<br />
2024年了，视频还这个德行！<br />
<br />
怎么混啊</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740375887613501770#m</id>
            <title>左边是超过30个工作人员，使用高速摄像机拍摄的广告片，制作时间约为一个月。

右边是用Pika翻拍的，使用了文本、图片到视频生成的。

当然视频是Pika的创意总监@MatanCohenGrumi 制作的，可能比较熟练。

但是这代表了一个方向，明年估计AI视频领域会诞生一部真正的AI电影！😀</title>
            <link>https://nitter.cz/xiaohuggg/status/1740375887613501770#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740375887613501770#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 14:15:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>左边是超过30个工作人员，使用高速摄像机拍摄的广告片，制作时间约为一个月。<br />
<br />
右边是用Pika翻拍的，使用了文本、图片到视频生成的。<br />
<br />
当然视频是Pika的创意总监<a href="https://nitter.cz/MatanCohenGrumi" title="Matan Cohen-Grumi">@MatanCohenGrumi</a> 制作的，可能比较熟练。<br />
<br />
但是这代表了一个方向，明年估计AI视频领域会诞生一部真正的AI电影！😀</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAzNjY4OTE2NzQzNzAwNDgvcHUvaW1nL2w0a3g4bFFVYUR5OS1hckkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740360789742268498#m</id>
            <title>近日麻省理工大学科学家们利用人工智能技术取得了突破性的发现，发现了一种新的抗生素类别，用于对抗耐药性金黄色葡萄球菌（MRSA）。

科学家们在抗生素研究方面已经有60多年没有取得重要的进展了。

这是60 年来发现的首个新型抗生素。

该研究结果发表在了《自然》杂志上。https://www.nature.com/articles/s41586-023-06887-8

麻省理工大学的研究团队训练了一个扩大的深度学习模型，使用扩展的数据集，评估了约39000种化合物对MRSA的抗生素活性。

这种方法结合了抗微生物活性预测和毒性评估，以识别对人体细胞危害最小的有效化合物。

- 活性和毒性预测的整合： 模型不仅预测了哪些化合物可能作为抗生素有效，还评估了它们对人类细胞的潜在毒性。这种双重方法有助于识别既对细菌有效又对人类安全的化合物。

- 筛选数百万种化合物：使用训练有素的AI模型，研究人员筛选了大约1200万种现有的商业化合物。如果没有AI的帮助，由于所需的时间和资源，这种大规模筛选几乎是不切实际的。

- 发现新的抗生素候选物：随后，研究人员获得了大约 280 种此类化合物，并在实验室环境中进行了针对 MRSA 的测试。

这种方法使他们从同一类别中识别出两种有前途的候选抗生素。

在涉及两种小鼠模型的实验中——一种用于 MRSA 皮肤感染，另一种用于 MRSA 全身感染——这些化合物中的每一种都将 MRSA 数量减少了 10 倍。

 - 打开“黑箱”：研究团队的目标是揭开深度学习模型的“黑箱”。这些模型包含大量模拟神经连接的计算，而人们通常不清楚其内部运作机制。通过这项研究，他们能够更深入地了解模型是如何学习并预测某些分子会成为良好抗生素的。

扩展资料：

耐甲氧西林的金黄色葡萄球菌（MRSA）是一种特别的细菌，对多种常用抗生素有抵抗力，这使得它引起的感染难以治疗。

根据欧洲疾病预防和控制中心 (ECDC) 的数据，欧盟每年发生近 150000 例 MRSA 感染，而该地区每年有近 35000 人死于抗菌药物耐药性感染。

MRSA感染的特点和严重性如下：

1 .多种感染： MRSA可以导致多种类型的感染。它通常感染皮肤，但也可以影响其他身体部位。

2 .从轻微到严重： MRSA感染的严重程度可以从轻微的皮肤感染（如疖子或脓肿）发展到更严重的健康问题。在一些情况下，它可能引起深层组织感染，甚至是生命威胁性的疾病，如肺炎和血流感染。

3 .耐药性： MRSA之所以难以治疗，主要是因为它对多种常用的抗生素，包括甲氧西林、青霉素和其他β-内酰胺类抗生素产生了耐药性。这意味着这些通常有效的药物对MRSA不再有效。

4 .医院和社区感染： MRSA感染可以在医院（医院获得性MRSA）和日常社区环境（社区获得性MRSA）中发生。医院获得性MRSA通常更严重，因为它影响的是已经有健康问题的患者。

5 .预防和控制： 由于MRSA的耐药性，预防和控制措施非常重要。这包括良好的卫生习惯、正确使用抗生素和在医院中采取感染控制措施。

因此，针对MRSA的新抗生素的发现对于医学领域来说是一个重要的进步，因为它提供了一个新的工具来对抗这种难以治疗的细菌。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740360789742268498#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740360789742268498#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 13:15:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近日麻省理工大学科学家们利用人工智能技术取得了突破性的发现，发现了一种新的抗生素类别，用于对抗耐药性金黄色葡萄球菌（MRSA）。<br />
<br />
科学家们在抗生素研究方面已经有60多年没有取得重要的进展了。<br />
<br />
这是60 年来发现的首个新型抗生素。<br />
<br />
该研究结果发表在了《自然》杂志上。<a href="https://www.nature.com/articles/s41586-023-06887-8">nature.com/articles/s41586-0…</a><br />
<br />
麻省理工大学的研究团队训练了一个扩大的深度学习模型，使用扩展的数据集，评估了约39000种化合物对MRSA的抗生素活性。<br />
<br />
这种方法结合了抗微生物活性预测和毒性评估，以识别对人体细胞危害最小的有效化合物。<br />
<br />
- 活性和毒性预测的整合： 模型不仅预测了哪些化合物可能作为抗生素有效，还评估了它们对人类细胞的潜在毒性。这种双重方法有助于识别既对细菌有效又对人类安全的化合物。<br />
<br />
- 筛选数百万种化合物：使用训练有素的AI模型，研究人员筛选了大约1200万种现有的商业化合物。如果没有AI的帮助，由于所需的时间和资源，这种大规模筛选几乎是不切实际的。<br />
<br />
- 发现新的抗生素候选物：随后，研究人员获得了大约 280 种此类化合物，并在实验室环境中进行了针对 MRSA 的测试。<br />
<br />
这种方法使他们从同一类别中识别出两种有前途的候选抗生素。<br />
<br />
在涉及两种小鼠模型的实验中——一种用于 MRSA 皮肤感染，另一种用于 MRSA 全身感染——这些化合物中的每一种都将 MRSA 数量减少了 10 倍。<br />
<br />
 - 打开“黑箱”：研究团队的目标是揭开深度学习模型的“黑箱”。这些模型包含大量模拟神经连接的计算，而人们通常不清楚其内部运作机制。通过这项研究，他们能够更深入地了解模型是如何学习并预测某些分子会成为良好抗生素的。<br />
<br />
扩展资料：<br />
<br />
耐甲氧西林的金黄色葡萄球菌（MRSA）是一种特别的细菌，对多种常用抗生素有抵抗力，这使得它引起的感染难以治疗。<br />
<br />
根据欧洲疾病预防和控制中心 (ECDC) 的数据，欧盟每年发生近 150000 例 MRSA 感染，而该地区每年有近 35000 人死于抗菌药物耐药性感染。<br />
<br />
MRSA感染的特点和严重性如下：<br />
<br />
1 .多种感染： MRSA可以导致多种类型的感染。它通常感染皮肤，但也可以影响其他身体部位。<br />
<br />
2 .从轻微到严重： MRSA感染的严重程度可以从轻微的皮肤感染（如疖子或脓肿）发展到更严重的健康问题。在一些情况下，它可能引起深层组织感染，甚至是生命威胁性的疾病，如肺炎和血流感染。<br />
<br />
3 .耐药性： MRSA之所以难以治疗，主要是因为它对多种常用的抗生素，包括甲氧西林、青霉素和其他β-内酰胺类抗生素产生了耐药性。这意味着这些通常有效的药物对MRSA不再有效。<br />
<br />
4 .医院和社区感染： MRSA感染可以在医院（医院获得性MRSA）和日常社区环境（社区获得性MRSA）中发生。医院获得性MRSA通常更严重，因为它影响的是已经有健康问题的患者。<br />
<br />
5 .预防和控制： 由于MRSA的耐药性，预防和控制措施非常重要。这包括良好的卫生习惯、正确使用抗生素和在医院中采取感染控制措施。<br />
<br />
因此，针对MRSA的新抗生素的发现对于医学领域来说是一个重要的进步，因为它提供了一个新的工具来对抗这种难以治疗的细菌。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiX0pNX2J3QUFaUlhELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740314433380127017#m</id>
            <title>R to @xiaohuggg: 百公里加速时间为2.78秒，最高时速265km/h。

 CLTC续航里程达800km，800V超级快充实现充电5分钟，续航220km，15分钟充电实现510km续航。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740314433380127017#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740314433380127017#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 10:11:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>百公里加速时间为2.78秒，最高时速265km/h。<br />
<br />
 CLTC续航里程达800km，800V超级快充实现充电5分钟，续航220km，15分钟充电实现510km续航。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV2xZUWFvQUFkb3czLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV240amFVQUFmWnZuLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV3FsQ2E4QUFibE0wLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV3ZGMWFBQUFaU0s3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740313720226812162#m</id>
            <title>兄弟们

我看了实车

确实很漂亮，甚至比保时捷还好看！

如果屁股后面的字能拿掉就挺好了，据说售价是30以内！低配版可能19.9😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1740313720226812162#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740313720226812162#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 10:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们<br />
<br />
我看了实车<br />
<br />
确实很漂亮，甚至比保时捷还好看！<br />
<br />
如果屁股后面的字能拿掉就挺好了，据说售价是30以内！低配版可能19.9😐</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JNdWJvQUFWT19SLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JMOGJnQUFlX1RZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JMX2JZQUFHV0g0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JMMmIwQUFWS3plLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740309636811755941#m</id>
            <title>UniRef++：在图片或视频中找到并标记出特定的物体

它牛P之处在于，你可以用文字来描述你想在图像或视频中要找的东西，它能帮你定位并标记出来。

比如，你可以说“一只坐在草地上的狗”。UniRef++会理解这个描述，并在图像或视频中找到并标记出符合这个描述的狗。

如果你有一张图像，上面已经标记了你感兴趣的物体（比如一张狗的照片，狗的部分已经被圈出来了），你也可以用这张图像来告诉UniRef++你想找的是什么。

UniRef++会使用这张标记好的图像作为参考，在其他图像或视频中找到并标记出类似的狗。

总的来说，UniRef++能够理解你的文字描述或参考图像，然后在其他图像或视频中找到并标记出你想要的物体。这对于自动图像编辑、视频内容分析等应用非常有用。

UniRef++的主要功能特点：

1、多任务统一处理：UniRef++能够处理多种基于参考的对象分割任务，包括图像分割、少样本图像分割、视频中的对象分割等。这意味着它可以使用同一套技术来处理不同类型的图像和视频分析任务。不管你是要在一张静态的图片中找东西，还是要在一段动态的视频中追踪某个东西，UniRef++都能帮忙。

2、灵活的参考处理：它可以使用多种类型的参考来指导分割任务，包括语言描述（如文字说明一个对象是什么）和标注的掩膜（即图像中已经标记出的特定区域）。

3、实时处理能力：尤其在处理视频对象分割时，UniRef++能够实时跟踪和分割视频中的对象，这对于动态场景分析非常重要。在视频中，它可以实时追踪物体，即使物体在移动也没问题。

4、高效性能：UniRef++在多个基准测试中展现了优秀的性能，特别是在图像和视频对象分割方面，它能够与或超过当前的最先进技术。

技术原理：

1、UniFusion模块：这是UniRef++的核心组件，负责将不同类型的参考信息（如语言描述或图像掩膜）融合到图像处理流程中。这种融合方式使得模型能够更准确地理解和定位要分割的对象。

2、基于Transformer的架构：UniRef++使用了Transformer模型，这是一种强大的深度学习架构，通常用于处理语言数据。在UniRef++中，Transformer被用来处理图像和视频数据，以实现精确的对象识别和分割。

3、多向融合策略：该模型采用了一种多向融合策略，可以根据任务的不同（如图像分割或视频对象跟踪）灵活地处理不同类型的输入和参考信息。

4、实例级分割：UniRef++将这些任务视为实例级分割问题，即不仅仅是识别出图像中的对象，还要精确地分割出每个实例（即单个对象）。

GitHub：https://github.com/FoundationVision/UniRef
论文：https://arxiv.org/abs/2312.15715</title>
            <link>https://nitter.cz/xiaohuggg/status/1740309636811755941#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740309636811755941#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 09:52:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>UniRef++：在图片或视频中找到并标记出特定的物体<br />
<br />
它牛P之处在于，你可以用文字来描述你想在图像或视频中要找的东西，它能帮你定位并标记出来。<br />
<br />
比如，你可以说“一只坐在草地上的狗”。UniRef++会理解这个描述，并在图像或视频中找到并标记出符合这个描述的狗。<br />
<br />
如果你有一张图像，上面已经标记了你感兴趣的物体（比如一张狗的照片，狗的部分已经被圈出来了），你也可以用这张图像来告诉UniRef++你想找的是什么。<br />
<br />
UniRef++会使用这张标记好的图像作为参考，在其他图像或视频中找到并标记出类似的狗。<br />
<br />
总的来说，UniRef++能够理解你的文字描述或参考图像，然后在其他图像或视频中找到并标记出你想要的物体。这对于自动图像编辑、视频内容分析等应用非常有用。<br />
<br />
UniRef++的主要功能特点：<br />
<br />
1、多任务统一处理：UniRef++能够处理多种基于参考的对象分割任务，包括图像分割、少样本图像分割、视频中的对象分割等。这意味着它可以使用同一套技术来处理不同类型的图像和视频分析任务。不管你是要在一张静态的图片中找东西，还是要在一段动态的视频中追踪某个东西，UniRef++都能帮忙。<br />
<br />
2、灵活的参考处理：它可以使用多种类型的参考来指导分割任务，包括语言描述（如文字说明一个对象是什么）和标注的掩膜（即图像中已经标记出的特定区域）。<br />
<br />
3、实时处理能力：尤其在处理视频对象分割时，UniRef++能够实时跟踪和分割视频中的对象，这对于动态场景分析非常重要。在视频中，它可以实时追踪物体，即使物体在移动也没问题。<br />
<br />
4、高效性能：UniRef++在多个基准测试中展现了优秀的性能，特别是在图像和视频对象分割方面，它能够与或超过当前的最先进技术。<br />
<br />
技术原理：<br />
<br />
1、UniFusion模块：这是UniRef++的核心组件，负责将不同类型的参考信息（如语言描述或图像掩膜）融合到图像处理流程中。这种融合方式使得模型能够更准确地理解和定位要分割的对象。<br />
<br />
2、基于Transformer的架构：UniRef++使用了Transformer模型，这是一种强大的深度学习架构，通常用于处理语言数据。在UniRef++中，Transformer被用来处理图像和视频数据，以实现精确的对象识别和分割。<br />
<br />
3、多向融合策略：该模型采用了一种多向融合策略，可以根据任务的不同（如图像分割或视频对象跟踪）灵活地处理不同类型的输入和参考信息。<br />
<br />
4、实例级分割：UniRef++将这些任务视为实例级分割问题，即不仅仅是识别出图像中的对象，还要精确地分割出每个实例（即单个对象）。<br />
<br />
GitHub：<a href="https://github.com/FoundationVision/UniRef">github.com/FoundationVision/…</a><br />
论文：<a href="https://arxiv.org/abs/2312.15715">arxiv.org/abs/2312.15715</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAyMjA3ODI2NjI2MTkxMzYvcHUvaW1nL1gzMldjdXk5YTZWczE3anEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740257062637191667#m</id>
            <title>来参加小米汽车发布会了

听雷总吹牛p

一开场就调子很高，很期待价格🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1740257062637191667#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740257062637191667#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 06:23:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来参加小米汽车发布会了<br />
<br />
听雷总吹牛p<br />
<br />
一开场就调子很高，很期待价格🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWliS2FNQUVlWXVJLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWlhRGFnQUFTTXVPLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWlaLWEwQUVSX0J3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWlhRWFzQUFFeTJ5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740212612036751744#m</id>
            <title>R to @xiaohuggg: 演示视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1740212612036751744#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740212612036751744#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 03:26:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAyMTIyODgwNjQ1MDM4MDgvcHUvaW1nL3cyQnk3cTMyV1NvSllxbDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740212609549541884#m</id>
            <title>Assistive Video：一个新的AI生成视频的工具

只需输入描述所想看到内容的提示或上传图片，模型便会生成一段4秒钟的视频。

你可以控制视频质量、与提示的一致性、运动的强度，设置种子等...

从宣传视频来看还是很不错，挺高清的。。。

体验地址：https://assistive.chat/product/video</title>
            <link>https://nitter.cz/xiaohuggg/status/1740212609549541884#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740212609549541884#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 03:26:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Assistive Video：一个新的AI生成视频的工具<br />
<br />
只需输入描述所想看到内容的提示或上传图片，模型便会生成一段4秒钟的视频。<br />
<br />
你可以控制视频质量、与提示的一致性、运动的强度，设置种子等...<br />
<br />
从宣传视频来看还是很不错，挺高清的。。。<br />
<br />
体验地址：<a href="https://assistive.chat/product/video">assistive.chat/product/video</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAyMTIyMTI5MTkzMzI4NjQvcHUvaW1nLzRkRG5HTlN5eEltMUVpOW0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>