<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745763961323262056#m</id>
            <title>LEGO：一个由字节跳动和复旦大学研发的多模态理解和图像定位模型。

LEGO能够处理和理解多种类型的输入，支持图像、音频和视频输入，并对这些信息进行分析和理解。

模型还具备精准定位的能力。例如在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，在音频中识别出特定声音的来源。

主要功能特点：

1、多模态理解：LEGO模型能够处理和理解多种类型的输入，包括图像、音频和视频。这意味着它可以从不同的数据源中提取信息，并对这些信息进行分析和理解。

2、强大的定位能力：模型具备在多种模态中进行精准定位的能力。例如，在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，或者在音频中识别出特定声音的来源。

3、高质量数据集的构建：为了解决数据有限的问题，研究团队构建了一个多样化且高质量的多模态训练数据集。这个数据集含有丰富的空间和时间信息，为模型的训练和优化提供了宝贵的资源。

4、应对复杂任务：LEGO模型可以处理包含多个元素和复杂指令的任务。它能够根据详细的描述或指令来分析和解释内容，提供准确的输出。

5、广泛的应用潜力：由于其多模态理解和定位的能力，LEGO模型适用于广泛的应用场景，包括内容创作、教育、娱乐、安全监控等领域。

6、实时处理和响应：LEGO模型能够快速处理输入并生成响应，这对于需要实时分析和反馈的应用场景非常重要。

工作原理：

LEGO项目的工作原理包括对多种模态数据的处理、特征提取、融合和上下文分析，最终根据用户的需求生成精确的定位和响应。这种多模态的方法使得模型能够更全面和深入地理解和响应各种复杂的查询和指令。

1、数据处理：LEGO模型首先处理多种类型的输入数据，包括图像、音频和视频。这一步骤涉及解析和预处理这些不同形式的数据，使其适合于进一步的分析。

2、特征提取：模型提取每种输入数据的关键特征。例如，对于图像，它可能识别出图中的物体、颜色、形状等；对于音频，它可能提取声音的节奏、强度、音色等；对于视频，它既提取视觉特征，又考虑时间序列的变化。

3、多模态融合：模型将从各种数据源提取的特征进行融合。这一步骤是多模态理解的关键，因为它涉及到将不同来源的信息整合在一起，形成一个统一的、多层次的理解。

4、上下文分析：LEGO模型分析整合后的数据以及相应的上下文信息。这可能包括识别图像中场景的背景信息、理解音频中的语境或解读视频中的叙事流。

5、定位和响应生成：根据用户的指令或查询，模型进行定位和响应。在图像中，这可能意味着标识出特定物体的位置；在音频中，可能是识别特定声音的来源；在视频中，可能是找到某个特定时间点的事件。

6、输出结果：最后，模型根据分析和定位的结果，生成响应。这可能是一段文本描述、一个标记了特定物体的图像、一个突出了特定声音的音频片段，或者是视频的一个特定片段。

项目及演示：https://lzw-lzw.github.io/LEGO.github.io/
论文：https://arxiv.org/abs/2401.06071
GitHub：https://github.com/lzw-lzw/LEGO</title>
            <link>https://nitter.cz/xiaohuggg/status/1745763961323262056#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745763961323262056#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 11:05:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LEGO：一个由字节跳动和复旦大学研发的多模态理解和图像定位模型。<br />
<br />
LEGO能够处理和理解多种类型的输入，支持图像、音频和视频输入，并对这些信息进行分析和理解。<br />
<br />
模型还具备精准定位的能力。例如在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，在音频中识别出特定声音的来源。<br />
<br />
主要功能特点：<br />
<br />
1、多模态理解：LEGO模型能够处理和理解多种类型的输入，包括图像、音频和视频。这意味着它可以从不同的数据源中提取信息，并对这些信息进行分析和理解。<br />
<br />
2、强大的定位能力：模型具备在多种模态中进行精准定位的能力。例如，在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，或者在音频中识别出特定声音的来源。<br />
<br />
3、高质量数据集的构建：为了解决数据有限的问题，研究团队构建了一个多样化且高质量的多模态训练数据集。这个数据集含有丰富的空间和时间信息，为模型的训练和优化提供了宝贵的资源。<br />
<br />
4、应对复杂任务：LEGO模型可以处理包含多个元素和复杂指令的任务。它能够根据详细的描述或指令来分析和解释内容，提供准确的输出。<br />
<br />
5、广泛的应用潜力：由于其多模态理解和定位的能力，LEGO模型适用于广泛的应用场景，包括内容创作、教育、娱乐、安全监控等领域。<br />
<br />
6、实时处理和响应：LEGO模型能够快速处理输入并生成响应，这对于需要实时分析和反馈的应用场景非常重要。<br />
<br />
工作原理：<br />
<br />
LEGO项目的工作原理包括对多种模态数据的处理、特征提取、融合和上下文分析，最终根据用户的需求生成精确的定位和响应。这种多模态的方法使得模型能够更全面和深入地理解和响应各种复杂的查询和指令。<br />
<br />
1、数据处理：LEGO模型首先处理多种类型的输入数据，包括图像、音频和视频。这一步骤涉及解析和预处理这些不同形式的数据，使其适合于进一步的分析。<br />
<br />
2、特征提取：模型提取每种输入数据的关键特征。例如，对于图像，它可能识别出图中的物体、颜色、形状等；对于音频，它可能提取声音的节奏、强度、音色等；对于视频，它既提取视觉特征，又考虑时间序列的变化。<br />
<br />
3、多模态融合：模型将从各种数据源提取的特征进行融合。这一步骤是多模态理解的关键，因为它涉及到将不同来源的信息整合在一起，形成一个统一的、多层次的理解。<br />
<br />
4、上下文分析：LEGO模型分析整合后的数据以及相应的上下文信息。这可能包括识别图像中场景的背景信息、理解音频中的语境或解读视频中的叙事流。<br />
<br />
5、定位和响应生成：根据用户的指令或查询，模型进行定位和响应。在图像中，这可能意味着标识出特定物体的位置；在音频中，可能是识别特定声音的来源；在视频中，可能是找到某个特定时间点的事件。<br />
<br />
6、输出结果：最后，模型根据分析和定位的结果，生成响应。这可能是一段文本描述、一个标记了特定物体的图像、一个突出了特定声音的音频片段，或者是视频的一个特定片段。<br />
<br />
项目及演示：<a href="https://lzw-lzw.github.io/LEGO.github.io/">lzw-lzw.github.io/LEGO.githu…</a><br />
论文：<a href="https://arxiv.org/abs/2401.06071">arxiv.org/abs/2401.06071</a><br />
GitHub：<a href="https://github.com/lzw-lzw/LEGO">github.com/lzw-lzw/LEGO</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDU3NjI0NjgzMzk3ODE2MzIvcHUvaW1nLzhBU2UzQXhJc0JKV242WG0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745713962325536881#m</id>
            <title>许多语言由于缺乏足够的数据而在现有的机器翻译模型中得不到支持。

苹果提出一种对比校准指令（AlignInstruct）的方法来强化跨语言理解和生成能力，特别是在数据稀缺的情况下。

该方法能够改进模型在未见和低资源语言翻译方面的表现。

使用该方法模型在多达24种未见语言上展现了有效翻译能力。
主要内容：

论文介绍了MT指令（MTInstruct）和对比校准指令（AlignInstruct）的使用，以解决两个在机器翻译领域的挑战：一是扩展支持的语言以包括之前未见的语言；二是缺乏低资源语言的数据。

1、使用MT指令（MTInstruct）：为了解决第一个挑战，即扩展支持的语言，研究者使用MTInstruct，这是一种直接的方法，通过机器翻译指令进行模型微调。

目的：MTInstruct旨在改善大型语言模型（LLMs）处理未见语言的能力。未见语言是指模型在训练时没有或仅有非常有限的数据的语言。

方法：通过直接的机器翻译指令，对模型进行特定的微调。这意味着模型接收到具体的指令，告诉它怎样从一种语言翻译到另一种语言。

举例：假设有一种语言叫做X语，它在互联网上的资料非常少，所以常规的机器翻译模型对它不太了解。现在，研究人员想让模型学会如何把英语翻译成X语。

使用MTInstruct方法，他们会给模型一些特定的指令，比如“把下面的英语句子翻译成X语”。然后，他们给模型提供一些英语和X语的对照样本。

经过这样的训练，即使之前模型几乎不了解X语，它也能开始理解如何把英语翻译成X语。

2、对比校准指令（AlignInstruct）：针对第二个挑战，即低资源语言中固有的弱跨语言信号，研究者提出使用AlignInstruct。这种方法强调通过基于统计词对齐构建的跨语言鉴别器进行跨语言监督。

目的：AlignInstruct专门用于处理低资源语言，即那些可用于训练的平行文本数据很少的语言。

方法：使用统计词对齐构建的跨语言鉴别器。这种鉴别器通过比较和校准不同语言之间相同意义的词汇，来提升翻译的准确性。

特点：这种方法侧重于跨语言监督，即监督模型在处理不同语言时的表现，特别是在词汇对齐和语义转换方面。

举例：现在有一种叫做Y语的低资源语言，它的资料非常少，甚至比X语还要少。为了提高模型翻译Y语的能力，研究人员使用了AlignInstruct方法。

他们首先分析已有的少量Y语和其他语言（比如英语）的对照样本，找出两种语言中相同意义词汇或短语的对应关系。

然后，他们用这些对照信息来训练模型，让它在翻译Y语时能够更准确地找到与英语中相对应的词汇或表达。

通过这两种方法，大语言模型就能更好地处理那些以前几乎没有见过或者资料很少的语言，从而使机器翻译服务能够覆盖更多的语种。
实验结果：

1、未见语言翻译实验：使用MTInstruct方法，研究者对BLOOMZ模型进行了微调，以测试其在未见语言的翻译能力。

实验涉及了多达24种未见语言。

结果显示，即使是对这些之前未见过的语言，经过MTInstruct微调的模型也能有效地进行翻译。

该实验证明了即使在缺乏大量训练数据的情况下，模型仍能学习如何翻译新的语言。

2、低资源语言翻译实验：使用AlignInstruct方法，研究者专注于提升模型在低资源语言翻译方面的性能。

实验使用基于统计词对齐的跨语言鉴别器进行跨语言监督。

结果显示，AlignInstruct在涉及英语的48个翻译方向中持续提高了翻译质量。

该实验表明AlignInstruct能有效提升模型在处理低资源语言时的翻译准确性和效率。

论文：https://arxiv.org/abs/2401.05811

PDF：https://arxiv.org/pdf/2401.05811.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1745713962325536881#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745713962325536881#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 07:46:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>许多语言由于缺乏足够的数据而在现有的机器翻译模型中得不到支持。<br />
<br />
苹果提出一种对比校准指令（AlignInstruct）的方法来强化跨语言理解和生成能力，特别是在数据稀缺的情况下。<br />
<br />
该方法能够改进模型在未见和低资源语言翻译方面的表现。<br />
<br />
使用该方法模型在多达24种未见语言上展现了有效翻译能力。<br />
主要内容：<br />
<br />
论文介绍了MT指令（MTInstruct）和对比校准指令（AlignInstruct）的使用，以解决两个在机器翻译领域的挑战：一是扩展支持的语言以包括之前未见的语言；二是缺乏低资源语言的数据。<br />
<br />
1、使用MT指令（MTInstruct）：为了解决第一个挑战，即扩展支持的语言，研究者使用MTInstruct，这是一种直接的方法，通过机器翻译指令进行模型微调。<br />
<br />
目的：MTInstruct旨在改善大型语言模型（LLMs）处理未见语言的能力。未见语言是指模型在训练时没有或仅有非常有限的数据的语言。<br />
<br />
方法：通过直接的机器翻译指令，对模型进行特定的微调。这意味着模型接收到具体的指令，告诉它怎样从一种语言翻译到另一种语言。<br />
<br />
举例：假设有一种语言叫做X语，它在互联网上的资料非常少，所以常规的机器翻译模型对它不太了解。现在，研究人员想让模型学会如何把英语翻译成X语。<br />
<br />
使用MTInstruct方法，他们会给模型一些特定的指令，比如“把下面的英语句子翻译成X语”。然后，他们给模型提供一些英语和X语的对照样本。<br />
<br />
经过这样的训练，即使之前模型几乎不了解X语，它也能开始理解如何把英语翻译成X语。<br />
<br />
2、对比校准指令（AlignInstruct）：针对第二个挑战，即低资源语言中固有的弱跨语言信号，研究者提出使用AlignInstruct。这种方法强调通过基于统计词对齐构建的跨语言鉴别器进行跨语言监督。<br />
<br />
目的：AlignInstruct专门用于处理低资源语言，即那些可用于训练的平行文本数据很少的语言。<br />
<br />
方法：使用统计词对齐构建的跨语言鉴别器。这种鉴别器通过比较和校准不同语言之间相同意义的词汇，来提升翻译的准确性。<br />
<br />
特点：这种方法侧重于跨语言监督，即监督模型在处理不同语言时的表现，特别是在词汇对齐和语义转换方面。<br />
<br />
举例：现在有一种叫做Y语的低资源语言，它的资料非常少，甚至比X语还要少。为了提高模型翻译Y语的能力，研究人员使用了AlignInstruct方法。<br />
<br />
他们首先分析已有的少量Y语和其他语言（比如英语）的对照样本，找出两种语言中相同意义词汇或短语的对应关系。<br />
<br />
然后，他们用这些对照信息来训练模型，让它在翻译Y语时能够更准确地找到与英语中相对应的词汇或表达。<br />
<br />
通过这两种方法，大语言模型就能更好地处理那些以前几乎没有见过或者资料很少的语言，从而使机器翻译服务能够覆盖更多的语种。<br />
实验结果：<br />
<br />
1、未见语言翻译实验：使用MTInstruct方法，研究者对BLOOMZ模型进行了微调，以测试其在未见语言的翻译能力。<br />
<br />
实验涉及了多达24种未见语言。<br />
<br />
结果显示，即使是对这些之前未见过的语言，经过MTInstruct微调的模型也能有效地进行翻译。<br />
<br />
该实验证明了即使在缺乏大量训练数据的情况下，模型仍能学习如何翻译新的语言。<br />
<br />
2、低资源语言翻译实验：使用AlignInstruct方法，研究者专注于提升模型在低资源语言翻译方面的性能。<br />
<br />
实验使用基于统计词对齐的跨语言鉴别器进行跨语言监督。<br />
<br />
结果显示，AlignInstruct在涉及英语的48个翻译方向中持续提高了翻译质量。<br />
<br />
该实验表明AlignInstruct能有效提升模型在处理低资源语言时的翻译准确性和效率。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.05811">arxiv.org/abs/2401.05811</a><br />
<br />
PDF：<a href="https://arxiv.org/pdf/2401.05811.pdf">arxiv.org/pdf/2401.05811.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RvRHJzRmFVQUFGWDl6LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RvRVBqdmJzQUFERFVCLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745703281652101321#m</id>
            <title>Ssm 奥特曼在YC W24 启动会上的演讲要点：

- 奥特曼暗示我们可能已经非常接近实现通用人工智能（AGI）

- 他建议应该以通用人工智能的实现为前提进行创业和技术开发。（不要再瞎折腾）

- GPT-5可能会相对于GPT-4有一个指数级的跳跃，尽管GPT-4已经领先近两年，至今无人超越。

- 这个进展将会给初创企业和现有公司带来了许多问题和挑战。（AGI将覆盖一大批创业者）

- 建议使用最先进的模型（State of the Art, SOTA），而不是花费太多时间进行微调和优化。（徒劳无功）

- 最正确的做法是设想一个“上帝般的”模型正在运作，然后基于这种设想来构建最好的产品。（要极其有远见）

- @OpenAI API将继续变得更快、更可靠、更便宜。然而，性能和成本之间始终存在平衡。例如，尽管电池技术已显着改进，但 iPhone 仍将保持 1-1.5 天的电池寿命以优化性能。

- 不建议建立产品业务主要致力于解决当前 GPT4的限制的内容。因为大多数限制将在 GPT-5 中部分/全部修复。（你会被覆盖）

- 初创公司更需要情境优化，而不是行为优化。通过 RAG 等提供更多信息可能比微调更有益。

综合@SullyOmarr 和 @RealRichomie 内容</title>
            <link>https://nitter.cz/xiaohuggg/status/1745703281652101321#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745703281652101321#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 07:04:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ssm 奥特曼在YC W24 启动会上的演讲要点：<br />
<br />
- 奥特曼暗示我们可能已经非常接近实现通用人工智能（AGI）<br />
<br />
- 他建议应该以通用人工智能的实现为前提进行创业和技术开发。（不要再瞎折腾）<br />
<br />
- GPT-5可能会相对于GPT-4有一个指数级的跳跃，尽管GPT-4已经领先近两年，至今无人超越。<br />
<br />
- 这个进展将会给初创企业和现有公司带来了许多问题和挑战。（AGI将覆盖一大批创业者）<br />
<br />
- 建议使用最先进的模型（State of the Art, SOTA），而不是花费太多时间进行微调和优化。（徒劳无功）<br />
<br />
- 最正确的做法是设想一个“上帝般的”模型正在运作，然后基于这种设想来构建最好的产品。（要极其有远见）<br />
<br />
- <a href="https://nitter.cz/OpenAI" title="OpenAI">@OpenAI</a> API将继续变得更快、更可靠、更便宜。然而，性能和成本之间始终存在平衡。例如，尽管电池技术已显着改进，但 iPhone 仍将保持 1-1.5 天的电池寿命以优化性能。<br />
<br />
- 不建议建立产品业务主要致力于解决当前 GPT4的限制的内容。因为大多数限制将在 GPT-5 中部分/全部修复。（你会被覆盖）<br />
<br />
- 初创公司更需要情境优化，而不是行为优化。通过 RAG 等提供更多信息可能比微调更有益。<br />
<br />
综合<a href="https://nitter.cz/SullyOmarr" title="Sully">@SullyOmarr</a> 和 <a href="https://nitter.cz/RealRichomie" title="Richard He">@RealRichomie</a> 内容</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RueW1jbGF3QUFxNWYyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745678102565712113#m</id>
            <title>R to @xiaohuggg: 不需要穿戴专门的动作捕捉设备或使用其他专业的硬件。

随时随地通过一台手机即可实现运动和面部捕捉。

用户仅通过普通的视频录像来捕捉复杂的全身动作和面部表情，无需额外的专业设备或服装。

这大大降低了运动捕捉的门槛，使得更多的创作者和开发者能够利用这一技术。</title>
            <link>https://nitter.cz/xiaohuggg/status/1745678102565712113#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745678102565712113#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 05:24:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不需要穿戴专门的动作捕捉设备或使用其他专业的硬件。<br />
<br />
随时随地通过一台手机即可实现运动和面部捕捉。<br />
<br />
用户仅通过普通的视频录像来捕捉复杂的全身动作和面部表情，无需额外的专业设备或服装。<br />
<br />
这大大降低了运动捕捉的门槛，使得更多的创作者和开发者能够利用这一技术。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDU2NzQ3NDkxMTA1OTE0ODgvcHUvaW1nL1RDTHhHeXZkS2prT190TFouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745678100829233310#m</id>
            <title>AI 动作捕捉公司 @RADicalMotionAI 为其AI动作捕捉服务 Radical Core和Radical Live增加了面部捕捉的支持。

允许用户从单一视频中同时捕捉到一个人的整体身体动作和面部表情。

用户只需要上传一个视频，就可以同时捕捉到里面人物的全身动作和面部表情。

以前这可能需要单独的系统或多个视频来做到这一点。

软件分析视频，识别里面人物的动作和表情，然后将这些信息转化为动画数据。

捕捉到的动画数据可以保存为FBX文件格式。以便在 DCC 应用程序或游戏引擎中使用。

除了导出为文件，这些动画数据还可以实时发送到流行的3D动画和游戏开发软件，如Blender、Unity和Unreal Engine。这意味着动画师和开发者可以实时看到他们视频中的动作和表情在3D模型上的效果。

官网：http://radicalmotion.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1745678100829233310#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745678100829233310#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 05:24:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI 动作捕捉公司 <a href="https://nitter.cz/RADicalMotionAI" title="RADiCAL">@RADicalMotionAI</a> 为其AI动作捕捉服务 Radical Core和Radical Live增加了面部捕捉的支持。<br />
<br />
允许用户从单一视频中同时捕捉到一个人的整体身体动作和面部表情。<br />
<br />
用户只需要上传一个视频，就可以同时捕捉到里面人物的全身动作和面部表情。<br />
<br />
以前这可能需要单独的系统或多个视频来做到这一点。<br />
<br />
软件分析视频，识别里面人物的动作和表情，然后将这些信息转化为动画数据。<br />
<br />
捕捉到的动画数据可以保存为FBX文件格式。以便在 DCC 应用程序或游戏引擎中使用。<br />
<br />
除了导出为文件，这些动画数据还可以实时发送到流行的3D动画和游戏开发软件，如Blender、Unity和Unreal Engine。这意味着动画师和开发者可以实时看到他们视频中的动作和表情在3D模型上的效果。<br />
<br />
官网：<a href="http://radicalmotion.com/">radicalmotion.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDU2NzI5NDI3NTM4Njk4MjQvcHUvaW1nL21ELTZCeHZ4bW85eEpmS1UuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745670879978414168#m</id>
            <title>Ready Player Me团队使用Stable Diffusion和ControlNet 为3D虚拟角色制作更真实和多样化的服装。

- 根据文本描述生成服装样式：比如，如果你告诉模型你想要一个“蒸汽朋克”风格的衣服，它就能自动创建出这种风格的衣服纹理。

- 自动处理复杂的纹理设计：这个模型可以自动在3D模型的服装上生成复杂的纹理，这些纹理不仅符合模型的形状，还具有逼真的细节和质感。

-准确贴合3D模型：ControlNet技术确保生成的纹理能够准确适应3D模型的具体形状和细节，比如衣服的褶皱、口袋的位置、拉链等。

- 理适配到模型上：这个技术还能确保新生成的衣服纹理能够非常自然地贴合到你的虚拟人物模型上，看起来就像是真的穿在人物身上一样。

- 适用于各种风格：无论是未来风格、古典风格还是任何其他风格，这项技术都能够生成与之相匹配的纹理。这让设计师能够快速试验不同的设计理念，生成各种各样的服装样式。

- 为了训练ControlNet，他们创建了一个包含约1000个现有Ready Player Me资产的数据集，并使用渲染和Blip-2自动标注。

详细介绍：https://readyplayer.me/blog/how-we-used-stable-diffusion-to-enable-geometry-aware-avatar-outfit-texturing</title>
            <link>https://nitter.cz/xiaohuggg/status/1745670879978414168#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745670879978414168#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 04:55:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ready Player Me团队使用Stable Diffusion和ControlNet 为3D虚拟角色制作更真实和多样化的服装。<br />
<br />
- 根据文本描述生成服装样式：比如，如果你告诉模型你想要一个“蒸汽朋克”风格的衣服，它就能自动创建出这种风格的衣服纹理。<br />
<br />
- 自动处理复杂的纹理设计：这个模型可以自动在3D模型的服装上生成复杂的纹理，这些纹理不仅符合模型的形状，还具有逼真的细节和质感。<br />
<br />
-准确贴合3D模型：ControlNet技术确保生成的纹理能够准确适应3D模型的具体形状和细节，比如衣服的褶皱、口袋的位置、拉链等。<br />
<br />
- 理适配到模型上：这个技术还能确保新生成的衣服纹理能够非常自然地贴合到你的虚拟人物模型上，看起来就像是真的穿在人物身上一样。<br />
<br />
- 适用于各种风格：无论是未来风格、古典风格还是任何其他风格，这项技术都能够生成与之相匹配的纹理。这让设计师能够快速试验不同的设计理念，生成各种各样的服装样式。<br />
<br />
- 为了训练ControlNet，他们创建了一个包含约1000个现有Ready Player Me资产的数据集，并使用渲染和Blip-2自动标注。<br />
<br />
详细介绍：<a href="https://readyplayer.me/blog/how-we-used-stable-diffusion-to-enable-geometry-aware-avatar-outfit-texturing">readyplayer.me/blog/how-we-u…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDU2Njc5NDAyOTk4MTI4NjQvcHUvaW1nL1dFeHJyNnp5Vi13MFZBcnEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651708259447279#m</id>
            <title>R to @xiaohuggg: ChatGPT Team采用创建者先付费，新增成员后付费机制。

如果一个团队在订阅期内增加了超出初始购买的席位数量的成员，对于超出初始购买的成员数量，团队在（月末）进行支付。

这就是漏洞所在，所以现在很多人在卖Team的账号，你买了，可能月末他们就跑了...

小心点！

计费规则：https://help.openai.com/en/articles/8792536-manage-billing-on-the-chatgpt-team-subscription-plan</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651708259447279#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651708259447279#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Team采用创建者先付费，新增成员后付费机制。<br />
<br />
如果一个团队在订阅期内增加了超出初始购买的席位数量的成员，对于超出初始购买的成员数量，团队在（月末）进行支付。<br />
<br />
这就是漏洞所在，所以现在很多人在卖Team的账号，你买了，可能月末他们就跑了...<br />
<br />
小心点！<br />
<br />
计费规则：<a href="https://help.openai.com/en/articles/8792536-manage-billing-on-the-chatgpt-team-subscription-plan">help.openai.com/en/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuTEhqemJvQUFFMGdELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651705713512946#m</id>
            <title>R to @xiaohuggg: 在设置界面可以在Team和个人账号之间来回切换

切换回去如果你个人账号是普通就是普通，是Plus就是Plus，和Team无关

在这里要提醒，如果你同时开通Plus账号和Team账号，那么它会同时计费。如果你想只用Team账号，则需要取消Plus的计费计划。

但是如果取消，你之前在个人号创建的GPTs都不能用了，😃

如果开通Team计划，同时想取消个人账号，那么需要你把之前创建的GPTs手动迁移过来。

不过OpenAI说在接下来的几周内，将推出帐户迁移选项，使用户能够：将 ChatGPT 个人（免费/增强版）工作区迁移到 ChatGPT 团队工作区，从而将所有聊天和 ​​GPT 转移到后者。</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651705713512946#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651705713512946#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在设置界面可以在Team和个人账号之间来回切换<br />
<br />
切换回去如果你个人账号是普通就是普通，是Plus就是Plus，和Team无关<br />
<br />
在这里要提醒，如果你同时开通Plus账号和Team账号，那么它会同时计费。如果你想只用Team账号，则需要取消Plus的计费计划。<br />
<br />
但是如果取消，你之前在个人号创建的GPTs都不能用了，😃<br />
<br />
如果开通Team计划，同时想取消个人账号，那么需要你把之前创建的GPTs手动迁移过来。<br />
<br />
不过OpenAI说在接下来的几周内，将推出帐户迁移选项，使用户能够：将 ChatGPT 个人（免费/增强版）工作区迁移到 ChatGPT 团队工作区，从而将所有聊天和 ​​GPT 转移到后者。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuR0REVWJBQUF2QWlrLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651703725347176#m</id>
            <title>R to @xiaohuggg: Team每个成员的每小时聊天上限是100条/3小时

而Plus用户是40条/3小时</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651703725347176#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651703725347176#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Team每个成员的每小时聊天上限是100条/3小时<br />
<br />
而Plus用户是40条/3小时</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuRmxoT2JJQUFsa2RWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651701175263728#m</id>
            <title>R to @xiaohuggg: 管理员可以对成员工作区进行设置，包括成员是否可以使用第三方GPT，防止信息泄露。

ChatGPT Team 上的共享和工作区 GPT 的默认设置无法修改：

- 聊天只能与 Workspace 成员共享
- 在工作区中创建的 GPT 可以在工作区中与任何有链接的人共享，或者根据 GPT 创建者的设置公开
-工作区 GPT 功能（例如插件、自定义操作和使用 Bing 浏览）默认打开</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651701175263728#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651701175263728#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>管理员可以对成员工作区进行设置，包括成员是否可以使用第三方GPT，防止信息泄露。<br />
<br />
ChatGPT Team 上的共享和工作区 GPT 的默认设置无法修改：<br />
<br />
- 聊天只能与 Workspace 成员共享<br />
- 在工作区中创建的 GPT 可以在工作区中与任何有链接的人共享，或者根据 GPT 创建者的设置公开<br />
-工作区 GPT 功能（例如插件、自定义操作和使用 Bing 浏览）默认打开</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuTWUzdmJZQUE1emxoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651699048714553#m</id>
            <title>R to @xiaohuggg: 可以创建只能Team团队成员能够访问的GPTs</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651699048714553#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651699048714553#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可以创建只能Team团队成员能够访问的GPTs</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuTXhIcmJNQUEzcm1pLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651696452436367#m</id>
            <title>R to @xiaohuggg: 每个成员有独立的工作区

和Plus个人账号界面一样

虽然本Team有132人，但是我看不到他们的聊天内容

我进来后的样子。。。</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651696452436367#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651696452436367#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>每个成员有独立的工作区<br />
<br />
和Plus个人账号界面一样<br />
<br />
虽然本Team有132人，但是我看不到他们的聊天内容<br />
<br />
我进来后的样子。。。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuRldSMWFzQUEwTU5lLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651693893960097#m</id>
            <title>R to @xiaohuggg: 看看这个Team已经132人了

哈哈哈😃</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651693893960097#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651693893960097#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看看这个Team已经132人了<br />
<br />
哈哈哈😃</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuRHZyN2IwQUFqaFM1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651691637379406#m</id>
            <title>R to @xiaohuggg: 支持对团队成员的角色设置</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651691637379406#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651691637379406#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持对团队成员的角色设置</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuSkFLaGJ3QUFkY0lULnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745651689271787874#m</id>
            <title>R to @xiaohuggg: 然后就是你可以邀请成员加入

支持批量邮件邀请，每个Team 最少俩人，上限似乎是150，超过150可以申请使用ChatGPT Enterprise</title>
            <link>https://nitter.cz/xiaohuggg/status/1745651689271787874#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745651689271787874#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:39:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>然后就是你可以邀请成员加入<br />
<br />
支持批量邮件邀请，每个Team 最少俩人，上限似乎是150，超过150可以申请使用ChatGPT Enterprise</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RuRWd5TGFVQUFOa1lmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>