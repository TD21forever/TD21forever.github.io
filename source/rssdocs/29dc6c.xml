<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755248727252668723#m</id>
            <title>🔔http://Xiaohu.AI日报「2月7日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1755248727252668723#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755248727252668723#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 15:14:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月7日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z2bGFBa2J3QUVsMFYtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755247215319384311#m</id>
            <title>Google 将Brad的更名计划和Gemini Ultra的发行日期从 2月 7日更改为 2月8日

🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1755247215319384311#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755247215319384311#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 15:08:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 将Brad的更名计划和Gemini Ultra的发行日期从 2月 7日更改为 2月8日<br />
<br />
🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z2a0NNZmEwQUFGOTkyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755230274143899769#m</id>
            <title>R to @xiaohuggg: 【Zho】@ZHOZHO672070 已将 RMBG v1.4 in ComfyUI 更新为 v1.5 版！

1⃣增加批量处理功能，可以直接一键去除视频背景

2⃣增加蒙版输出功能，直接输出mask，同样支持批量

实测下来，应该是目前去除视频背景最好的模型了！！去除 SVD1.1 生成的 97 帧视频的背景，在云端 T4 只需30s，简直飞速！

项目地址：https://github.com/ZHO-ZHO-ZHO/ComfyUI-BRIA_AI-RMBG</title>
            <link>https://nitter.cz/xiaohuggg/status/1755230274143899769#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755230274143899769#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 14:01:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>【Zho】<a href="https://nitter.cz/ZHOZHO672070" title="-Zho-">@ZHOZHO672070</a> 已将 RMBG v1.4 in ComfyUI 更新为 v1.5 版！<br />
<br />
1⃣增加批量处理功能，可以直接一键去除视频背景<br />
<br />
2⃣增加蒙版输出功能，直接输出mask，同样支持批量<br />
<br />
实测下来，应该是目前去除视频背景最好的模型了！！去除 SVD1.1 生成的 97 帧视频的背景，在云端 T4 只需30s，简直飞速！<br />
<br />
项目地址：<a href="https://github.com/ZHO-ZHO-ZHO/ComfyUI-BRIA_AI-RMBG">github.com/ZHO-ZHO-ZHO/Comfy…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU1MjMwMTc3NDA2NDIzMDQxL2ltZy9yOFBxbmZNaVZrcWRYcXdULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755220511947468919#m</id>
            <title>Stable Video 处理图像运动能力效果演示

这是一项还没有开放的能力，正在内测！

本视频没有进行任何放大和添加效果处理，只是添加了背景音乐！

可以看出Stable Video在处理动作场景，尤其是当视频中包含跑步者、汽车或舞者等动态元素时，能力惊人！🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1755220511947468919#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755220511947468919#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 13:22:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable Video 处理图像运动能力效果演示<br />
<br />
这是一项还没有开放的能力，正在内测！<br />
<br />
本视频没有进行任何放大和添加效果处理，只是添加了背景音乐！<br />
<br />
可以看出Stable Video在处理动作场景，尤其是当视频中包含跑步者、汽车或舞者等动态元素时，能力惊人！🫡</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQ1ODEzMDUxMTM2NDA5NjAvcHUvaW1nL0xrV1k4cDFrNkpLWnBkRV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755183564021473404#m</id>
            <title>Medivis推出Apple Vision Pro版本的SurgicalAR手术应用。

该应用可以将2D的医疗成像数据（如MRI/CT扫描）转化为3D的可视化的互动影像，帮助医生提高手术精度。

医生可以放大、缩小或旋转这些3D模型，帮助他们从各种角度检查需要手术的部位。

应用还能测量身体内部的结构，比如说要手术的肿瘤大小，这样医生就可以更准确地制定手术计划。

下载：https://apps.apple.com/us/app/surgicalar-vision/id6473222082</title>
            <link>https://nitter.cz/xiaohuggg/status/1755183564021473404#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755183564021473404#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 10:55:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Medivis推出Apple Vision Pro版本的SurgicalAR手术应用。<br />
<br />
该应用可以将2D的医疗成像数据（如MRI/CT扫描）转化为3D的可视化的互动影像，帮助医生提高手术精度。<br />
<br />
医生可以放大、缩小或旋转这些3D模型，帮助他们从各种角度检查需要手术的部位。<br />
<br />
应用还能测量身体内部的结构，比如说要手术的肿瘤大小，这样医生就可以更准确地制定手术计划。<br />
<br />
下载：<a href="https://apps.apple.com/us/app/surgicalar-vision/id6473222082">apps.apple.com/us/app/surgic…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1748910327658598697#m">nitter.cz/xiaohuggg/status/1748910327658598697#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUxODI0NjU0NjIwMTgwNDkvcHUvaW1nL0ttLXJYdEs4eFBBV0ZqcS0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755173835396239826#m</id>
            <title>DALL·E 3可能很快就会推出图像编辑器功能！

用来控制图像生成的一致性😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1755173835396239826#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755173835396239826#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 10:17:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DALL·E 3可能很快就会推出图像编辑器功能！<br />
<br />
用来控制图像生成的一致性😐</p>
<p><a href="https://nitter.cz/btibor91/status/1755158169838317844#m">nitter.cz/btibor91/status/1755158169838317844#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755140675270795646#m</id>
            <title>MiniCPM：由面壁智能与清华大学开发的能在手机端运行的端侧大模型

MiniCPM通过Int4量化技术，实现了模型的端侧部署能力。

可以在手机等移动设备上运行，提供与人类说话速度相当的流式输出速度。

在中文处理、数学问题和编程能力方面，其表现超越了Llama2-13B、MPT-30B、Falcon-40B等模型。

其中核心模型MiniCPM-2B拥有24亿非词嵌入参数。

主要特性：

1、优异的性能：MiniCPM在多项公开的综合性评测集上展现了优秀的性能，特别是在中文、数学和编程能力方面，其整体性能超过了许多大型模型，如Llama2-13B、MPT-30B和Falcon-40B。

2、多模态能力：基于MiniCPM-2B开发的多模态模型MiniCPM-V，在同规模模型中表现最佳，超越了现有的多模态大模型，如Phi-2构建的模型。

3、端侧部署：经过Int4量化处理后，MiniCPM只占2GB空间，可以在手机上进行部署和推理，提供流畅的输出速度，略高于人类的说话速度。

4、资源高效利用：MiniCPM支持在普通的GPU上进行高效的参数微调和全参数微调，这使得继续训练和二次开发的成本相对较低。即便是在资源有限的硬件上，也可以持续训练MiniCPM模型。

5、兼容性与灵活性：MiniCPM模型兼容多种平台和框架，如Huggingface Transformers和vLLM，为用户提供了灵活的使用选择。无论是进行文本生成、理解任务还是多模态任务处理，MiniCPM都能提供强大的支持。

详细介绍：https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a
GitHub：https://github.com/OpenBMB/MiniCPM</title>
            <link>https://nitter.cz/xiaohuggg/status/1755140675270795646#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755140675270795646#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 08:05:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MiniCPM：由面壁智能与清华大学开发的能在手机端运行的端侧大模型<br />
<br />
MiniCPM通过Int4量化技术，实现了模型的端侧部署能力。<br />
<br />
可以在手机等移动设备上运行，提供与人类说话速度相当的流式输出速度。<br />
<br />
在中文处理、数学问题和编程能力方面，其表现超越了Llama2-13B、MPT-30B、Falcon-40B等模型。<br />
<br />
其中核心模型MiniCPM-2B拥有24亿非词嵌入参数。<br />
<br />
主要特性：<br />
<br />
1、优异的性能：MiniCPM在多项公开的综合性评测集上展现了优秀的性能，特别是在中文、数学和编程能力方面，其整体性能超过了许多大型模型，如Llama2-13B、MPT-30B和Falcon-40B。<br />
<br />
2、多模态能力：基于MiniCPM-2B开发的多模态模型MiniCPM-V，在同规模模型中表现最佳，超越了现有的多模态大模型，如Phi-2构建的模型。<br />
<br />
3、端侧部署：经过Int4量化处理后，MiniCPM只占2GB空间，可以在手机上进行部署和推理，提供流畅的输出速度，略高于人类的说话速度。<br />
<br />
4、资源高效利用：MiniCPM支持在普通的GPU上进行高效的参数微调和全参数微调，这使得继续训练和二次开发的成本相对较低。即便是在资源有限的硬件上，也可以持续训练MiniCPM模型。<br />
<br />
5、兼容性与灵活性：MiniCPM模型兼容多种平台和框架，如Huggingface Transformers和vLLM，为用户提供了灵活的使用选择。无论是进行文本生成、理解任务还是多模态任务处理，MiniCPM都能提供强大的支持。<br />
<br />
详细介绍：<a href="https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a">shengdinghu.notion.site/Mini…</a><br />
GitHub：<a href="https://github.com/OpenBMB/MiniCPM">github.com/OpenBMB/MiniCPM</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUwNzkxNjk1MjcyMTQwODAvcHUvaW1nL0cweHBpMGlsWkJxelY0Ty0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755101265137446977#m</id>
            <title>感觉没有用全力

哈哈哈哈😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1755101265137446977#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755101265137446977#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 05:28:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感觉没有用全力<br />
<br />
哈哈哈哈😂</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU0MTQ1NTkxNzA4NTkwMDgwL2ltZy9jck1UQV9yMDFPZEo1c0pPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755084128721277336#m</id>
            <title>兄弟们，这个牛P了 

Anything in Any Scene：在现有动态视频中无缝插入任何物体

它可以让你在任何视频场景中自然地添加任何物体，比如在街道上行走的视频中插入一辆汽车，或者在室内视频中添加一把椅子。

而且保证被添加的物体的位置、光照效果和整体风格与原视频完美融合，就像原本就存在一样。

主要特点：

1、物体的准确放置：确保新插入的物体在视频中的位置看起来自然、合理，与视频场景的其他元素和空间布局协调一致。

2、光照和阴影的真实模拟：通过分析和模拟视频中的光照条件及其对物体的影响，生成看起来自然的阴影和光照效果，增强物体与环境的整合度。

3、风格一致性：应用风格转换技术，调整和优化视频的视觉效果，使得插入的物体在色彩、纹理等方面与背景视频保持一致，进一步提升整个视频的真实感和观感质量。

工作原理：

1、几何定位与物体放置：

首先，该技术使用先进的物体识别和场景理解算法来分析视频中的环境，包括识别视频中的空间布局、物体位置和场景深度信息。

然后，它计算出新物体在场景中的最佳位置，确保其与环境中的其他物体和地形正确对齐，以保持几何一致性。

2、光照估计与模拟：

通过分析视频中的光照条件，包括光源方向、强度以及光照在不同表面上的反射和散射效果，该技术能够对场景的光照环境进行准确估计。

接着，它模拟如何将相同的光照效果应用于新增加的物体上，包括生成逼真的阴影，以确保物体看起来自然地融入其所处的光照环境。

3、风格转换与视觉一致性：

使用风格转换技术来调整插入物体的视觉属性，如纹理、颜色和对比度，使其与周围环境的风格保持一致。

通过这种方式，不仅物体的几何形状和光照效果与原始视频相匹配，其视觉风格也与背景场景融为一体，提高了整体的视觉协调性。

4、动态视频处理：

为了在连续的视频帧中保持物体的稳定性和连贯性，该技术采用动态跟踪和视频稳定技术，确保随着场景变化，插入的物体能够自然地移动和适应新的视角和位置。

这包括处理相机运动引起的视角变化，确保物体在整个视频序列中保持正确的位置和方向。

项目及演示：https://anythinginanyscene.github.io/
GitHub：https://github.com/AnythingInAnyScene/anything_in_anyscene</title>
            <link>https://nitter.cz/xiaohuggg/status/1755084128721277336#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755084128721277336#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 04:20:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，这个牛P了 <br />
<br />
Anything in Any Scene：在现有动态视频中无缝插入任何物体<br />
<br />
它可以让你在任何视频场景中自然地添加任何物体，比如在街道上行走的视频中插入一辆汽车，或者在室内视频中添加一把椅子。<br />
<br />
而且保证被添加的物体的位置、光照效果和整体风格与原视频完美融合，就像原本就存在一样。<br />
<br />
主要特点：<br />
<br />
1、物体的准确放置：确保新插入的物体在视频中的位置看起来自然、合理，与视频场景的其他元素和空间布局协调一致。<br />
<br />
2、光照和阴影的真实模拟：通过分析和模拟视频中的光照条件及其对物体的影响，生成看起来自然的阴影和光照效果，增强物体与环境的整合度。<br />
<br />
3、风格一致性：应用风格转换技术，调整和优化视频的视觉效果，使得插入的物体在色彩、纹理等方面与背景视频保持一致，进一步提升整个视频的真实感和观感质量。<br />
<br />
工作原理：<br />
<br />
1、几何定位与物体放置：<br />
<br />
首先，该技术使用先进的物体识别和场景理解算法来分析视频中的环境，包括识别视频中的空间布局、物体位置和场景深度信息。<br />
<br />
然后，它计算出新物体在场景中的最佳位置，确保其与环境中的其他物体和地形正确对齐，以保持几何一致性。<br />
<br />
2、光照估计与模拟：<br />
<br />
通过分析视频中的光照条件，包括光源方向、强度以及光照在不同表面上的反射和散射效果，该技术能够对场景的光照环境进行准确估计。<br />
<br />
接着，它模拟如何将相同的光照效果应用于新增加的物体上，包括生成逼真的阴影，以确保物体看起来自然地融入其所处的光照环境。<br />
<br />
3、风格转换与视觉一致性：<br />
<br />
使用风格转换技术来调整插入物体的视觉属性，如纹理、颜色和对比度，使其与周围环境的风格保持一致。<br />
<br />
通过这种方式，不仅物体的几何形状和光照效果与原始视频相匹配，其视觉风格也与背景场景融为一体，提高了整体的视觉协调性。<br />
<br />
4、动态视频处理：<br />
<br />
为了在连续的视频帧中保持物体的稳定性和连贯性，该技术采用动态跟踪和视频稳定技术，确保随着场景变化，插入的物体能够自然地移动和适应新的视角和位置。<br />
<br />
这包括处理相机运动引起的视角变化，确保物体在整个视频序列中保持正确的位置和方向。<br />
<br />
项目及演示：<a href="https://anythinginanyscene.github.io/">anythinginanyscene.github.io…</a><br />
GitHub：<a href="https://github.com/AnythingInAnyScene/anything_in_anyscene">github.com/AnythingInAnyScen…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUwNzQyNjY5OTgxMTYzNTIvcHUvaW1nL0FnY0pMSi0tWWVaMS1fR00uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755075274918752631#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1755075274918752631#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755075274918752631#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 03:45:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUwNjg1MTk4NDExNDQ4MzIvcHUvaW1nL3lHSjBGd3BMZHpHZEdpVjkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755075272410538450#m</id>
            <title>RMBG-1.4：专门的去除照片背景模型

该模型经过精心选择的数据集训练，从网络购物的商品图片到游戏和广告中的内容，几乎包含了所有你能想到的场景。

不管是电商需要清除产品背景，还是广告制作需要更换不同的背景场景，都非常有效。

可以非常精确地把你不想要的背景“擦掉”。

模型使用超过12000张高质量、高分辨率、手工标记（像素级精度）、完全授权的图片进行训练。数据集中平衡考虑了性别、种族和不同类型的残疾人群，展示了模型的多功能性。

模型下载：https://huggingface.co/briaai/RMBG-1.4

Code：https://huggingface.co/spaces/briaai/BRIA-RMBG-1.4/tree/main

在线体验：https://huggingface.co/spaces/briaai/BRIA-RMBG-1.4</title>
            <link>https://nitter.cz/xiaohuggg/status/1755075272410538450#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755075272410538450#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 03:45:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RMBG-1.4：专门的去除照片背景模型<br />
<br />
该模型经过精心选择的数据集训练，从网络购物的商品图片到游戏和广告中的内容，几乎包含了所有你能想到的场景。<br />
<br />
不管是电商需要清除产品背景，还是广告制作需要更换不同的背景场景，都非常有效。<br />
<br />
可以非常精确地把你不想要的背景“擦掉”。<br />
<br />
模型使用超过12000张高质量、高分辨率、手工标记（像素级精度）、完全授权的图片进行训练。数据集中平衡考虑了性别、种族和不同类型的残疾人群，展示了模型的多功能性。<br />
<br />
模型下载：<a href="https://huggingface.co/briaai/RMBG-1.4">huggingface.co/briaai/RMBG-1…</a><br />
<br />
Code：<a href="https://huggingface.co/spaces/briaai/BRIA-RMBG-1.4/tree/main">huggingface.co/spaces/briaai…</a><br />
<br />
在线体验：<a href="https://huggingface.co/spaces/briaai/BRIA-RMBG-1.4">huggingface.co/spaces/briaai…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUwNjgwNzcxMjgxODc5MDQvcHUvaW1nL3g2STlGS1lrdGNxbFkyczQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755062275000733905#m</id>
            <title>Stability AI 展示了一种新型的文本到语音（TTS）模型

它不需要提前录制人声样本作为参考，只需用文字描述想要的声音特征，比如“一个英国口音的女声，语速较快”，模型就能生成相应的声音。

它还能根据文本描述来调整语音的性别、口音、语速和音调等多个特征。

不仅模仿还能根据描述合成新的声音...

主要功能特点：

1、高保真语音生成：该模型能够仅根据文字描述在广泛的口音、韵律风格、通道条件和声学条件下生成高保真度的语音，提供丰富多样的听觉体验。

2、自然语言控制：通过自然语言提示来直观地控制说话者的身份和风格，无需参考语音录音，简化了语音生成过程，使其更加灵活和易于使用。

它能够接受关于说话者身份（比如性别、口音）、说话风格（快速、慢速、高音、低音等）、录音条件（好像在一个安静的房间里或有背景噪音的环境中）等方面的文字描述，并根据这些描述生成相应的语音。

3、可扩展的标记方法：研究提出了一种新的、可扩展的方法来标记说话者身份、风格和录音条件，允许在大型数据集上训练模型，从而提高了模型的适用性和灵活性。

4、音频质量的显著提升：通过提出的方法显著提高音频保真度，即便完全依赖现有数据，也能超越最近的工作，提高了语音的清晰度和真实感。

5、属性细粒度控制：模型支持对多种语音属性的细粒度控制，包括性别、说话者音调、音调调制、说话速率、通道条件和口音，为用户提供定制化的语音输出选项。

6、创造新的声音：不仅仅模仿已知的声音，还能创造出全新的、只通过文字描述就能设定的声音风格和特征。

工作原理：

1、数据集标记：他们开创了一种技术进步，使得模型可以自动学习和理解怎样根据文字的描述来生成人的语音。

他们使用了一个非常庞大的数据集——包含了45,000小时的语音记录——来训练他们的人工智能模型。模型通过学习这些语音数据，能够理解并模仿人类语音的不同特征，比如改变语音听起来的性别（男声或女声）、口音（比如英国口音或美国口音）、说话的速度（快或慢），以及音调的高低。

重要的是，尽管这个巨大的语音数据集中只有一小部分是高质量的录音，研究者们的技术仍然能够利用这些高质量的样本来提高整个模型生成语音的自然度和真实感。这意味着，基于这个模型，即使是用非常有限的高质量语音数据，也能生成听起来非常自然和真实的人声，这在技术上是一个显著的进步。

2、语音生成模型训练：使用标记好的大规模数据集，研究者训练了一个深度学习模型，这个模型学会了如何基于输入的自然语言描述生成语音。模型训练涉及学习不同的声音属性之间的关系，以及如何根据描述中的要求调整这些属性。

项目及演示：https://www.text-description-to-speech.com/
论文：https://arxiv.org/abs/2402.01912</title>
            <link>https://nitter.cz/xiaohuggg/status/1755062275000733905#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755062275000733905#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 02:53:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI 展示了一种新型的文本到语音（TTS）模型<br />
<br />
它不需要提前录制人声样本作为参考，只需用文字描述想要的声音特征，比如“一个英国口音的女声，语速较快”，模型就能生成相应的声音。<br />
<br />
它还能根据文本描述来调整语音的性别、口音、语速和音调等多个特征。<br />
<br />
不仅模仿还能根据描述合成新的声音...<br />
<br />
主要功能特点：<br />
<br />
1、高保真语音生成：该模型能够仅根据文字描述在广泛的口音、韵律风格、通道条件和声学条件下生成高保真度的语音，提供丰富多样的听觉体验。<br />
<br />
2、自然语言控制：通过自然语言提示来直观地控制说话者的身份和风格，无需参考语音录音，简化了语音生成过程，使其更加灵活和易于使用。<br />
<br />
它能够接受关于说话者身份（比如性别、口音）、说话风格（快速、慢速、高音、低音等）、录音条件（好像在一个安静的房间里或有背景噪音的环境中）等方面的文字描述，并根据这些描述生成相应的语音。<br />
<br />
3、可扩展的标记方法：研究提出了一种新的、可扩展的方法来标记说话者身份、风格和录音条件，允许在大型数据集上训练模型，从而提高了模型的适用性和灵活性。<br />
<br />
4、音频质量的显著提升：通过提出的方法显著提高音频保真度，即便完全依赖现有数据，也能超越最近的工作，提高了语音的清晰度和真实感。<br />
<br />
5、属性细粒度控制：模型支持对多种语音属性的细粒度控制，包括性别、说话者音调、音调调制、说话速率、通道条件和口音，为用户提供定制化的语音输出选项。<br />
<br />
6、创造新的声音：不仅仅模仿已知的声音，还能创造出全新的、只通过文字描述就能设定的声音风格和特征。<br />
<br />
工作原理：<br />
<br />
1、数据集标记：他们开创了一种技术进步，使得模型可以自动学习和理解怎样根据文字的描述来生成人的语音。<br />
<br />
他们使用了一个非常庞大的数据集——包含了45,000小时的语音记录——来训练他们的人工智能模型。模型通过学习这些语音数据，能够理解并模仿人类语音的不同特征，比如改变语音听起来的性别（男声或女声）、口音（比如英国口音或美国口音）、说话的速度（快或慢），以及音调的高低。<br />
<br />
重要的是，尽管这个巨大的语音数据集中只有一小部分是高质量的录音，研究者们的技术仍然能够利用这些高质量的样本来提高整个模型生成语音的自然度和真实感。这意味着，基于这个模型，即使是用非常有限的高质量语音数据，也能生成听起来非常自然和真实的人声，这在技术上是一个显著的进步。<br />
<br />
2、语音生成模型训练：使用标记好的大规模数据集，研究者训练了一个深度学习模型，这个模型学会了如何基于输入的自然语言描述生成语音。模型训练涉及学习不同的声音属性之间的关系，以及如何根据描述中的要求调整这些属性。<br />
<br />
项目及演示：<a href="https://www.text-description-to-speech.com/">text-description-to-speech.c…</a><br />
论文：<a href="https://arxiv.org/abs/2402.01912">arxiv.org/abs/2402.01912</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUwNjE2NjU1ODkzMTM1MzYvcHUvaW1nLzFnSWVXSUlzR0ZfdGF0UU0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755054688591073325#m</id>
            <title>OpenAI 宣布，其图像生成器 DALL-E 3 将开始为所生成的图像添加来自内容来源和真实性联盟 (C2PA) 的水印，以帮助用户识别使用人工智能 (AI) 生成的内容。

水印包含两个部分：不可见的元数据组件和可见的 CR 符号，后者位于每个图像的左上角。

生成的图像中将嵌入何种元数据：

• 通过API生成的图像将包含一个签名，表明它们是通过底层DALL·E 3模型生成的。

• 在ChatGPT内产生的图像将包含一个额外的清单，指出内容是使用ChatGPT创建的，形成双重出处线索。

用户可在 2 月 12 日后看到这些变化。

用户可以通过 Content Credentials Verify 等网站查询由 OpenAI 平台生成的任何图像的来源（使用了哪个 AI 工具）。

C2PA在DALL·E 3中的应用 | OpenAI帮助中心

C2PA（可信内容保护联盟）是一个开放技术标准，允许发布者、公司等在媒体内容中嵌入元数据，以验证其来源和相关信息。这一标准不仅适用于AI生成的图像，也被相机制造商、新闻组织等采用，以证明媒体内容的来源和历史（或出处）。

尽管C2PA元数据有助于解决出处问题，但它并非万能。元数据可能会因意外或故意而被移除，例如，大多数社交媒体平台今天会从上传的图像中移除元数据，截屏操作也可能导致元数据的丢失。因此，缺乏此元数据的图像可能是也可能不是通过ChatGPT或API生成的。

C2PA ：https://c2pa.org/

详细：https://help.openai.com/en/articles/8912793-c2pa-in-dall-e-3</title>
            <link>https://nitter.cz/xiaohuggg/status/1755054688591073325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755054688591073325#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 02:23:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 宣布，其图像生成器 DALL-E 3 将开始为所生成的图像添加来自内容来源和真实性联盟 (C2PA) 的水印，以帮助用户识别使用人工智能 (AI) 生成的内容。<br />
<br />
水印包含两个部分：不可见的元数据组件和可见的 CR 符号，后者位于每个图像的左上角。<br />
<br />
生成的图像中将嵌入何种元数据：<br />
<br />
• 通过API生成的图像将包含一个签名，表明它们是通过底层DALL·E 3模型生成的。<br />
<br />
• 在ChatGPT内产生的图像将包含一个额外的清单，指出内容是使用ChatGPT创建的，形成双重出处线索。<br />
<br />
用户可在 2 月 12 日后看到这些变化。<br />
<br />
用户可以通过 Content Credentials Verify 等网站查询由 OpenAI 平台生成的任何图像的来源（使用了哪个 AI 工具）。<br />
<br />
C2PA在DALL·E 3中的应用 | OpenAI帮助中心<br />
<br />
C2PA（可信内容保护联盟）是一个开放技术标准，允许发布者、公司等在媒体内容中嵌入元数据，以验证其来源和相关信息。这一标准不仅适用于AI生成的图像，也被相机制造商、新闻组织等采用，以证明媒体内容的来源和历史（或出处）。<br />
<br />
尽管C2PA元数据有助于解决出处问题，但它并非万能。元数据可能会因意外或故意而被移除，例如，大多数社交媒体平台今天会从上传的图像中移除元数据，截屏操作也可能导致元数据的丢失。因此，缺乏此元数据的图像可能是也可能不是通过ChatGPT或API生成的。<br />
<br />
C2PA ：<a href="https://c2pa.org/">c2pa.org/</a><br />
<br />
详细：<a href="https://help.openai.com/en/articles/8912793-c2pa-in-dall-e-3">help.openai.com/en/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZzMDdpcWFRQUFzaW5QLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755047896419078427#m</id>
            <title>传染的真快

🤔

 https://twitter.com/chrisclarkefly/status/1754968856932528364/video/1</title>
            <link>https://nitter.cz/xiaohuggg/status/1755047896419078427#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755047896419078427#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 01:56:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>传染的真快<br />
<br />
🤔<br />
<br />
 <a href="https://nitter.cz/chrisclarkefly/status/1754968856932528364/video/1">nitter.cz/chrisclarkefly/s…</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754895426061480000#m</id>
            <title>苹果所有初代设备

大集合😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1754895426061480000#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754895426061480000#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 Feb 2024 15:50:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>苹果所有初代设备<br />
<br />
大集合😎</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Zxa0ZUSGFVQUF6ajdLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754875655836033507#m</id>
            <title>供应链报告称， #AppleVisionPro  将于 4 月或 5 月在中国推出。 👀</title>
            <link>https://nitter.cz/xiaohuggg/status/1754875655836033507#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754875655836033507#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 Feb 2024 14:32:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>供应链报告称， <a href="https://nitter.cz/search?q=%23AppleVisionPro">#AppleVisionPro</a>  将于 4 月或 5 月在中国推出。 👀</p>
<p><a href="https://nitter.cz/appleinsider/status/1754851338117161278#m">nitter.cz/appleinsider/status/1754851338117161278#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754862694329626817#m</id>
            <title>一个名为OnlyFake的地下网站声称使用他们的AI技术，仅需15美元就能生成以假乱真的假身份证和驾驶证等

根据404 Media的验证，这个网站的确几乎能瞬间制作出假身份证。而且通过了加密货币交易所OKX的身份验证。

根据测试，OnlyFake能够创建非常真实的加利福尼亚州驾驶执照，包括任意的姓名、生物信息、地址、到期日期和签名。

照片甚至给人一种身份证放在蓬松地毯上拍照的感觉，许多网站需要这样的照片来进行验证。

404 Media还使用该网站生成的另一张假身份证成功通过了加密货币交易所OKX的身份验证过程，而OKX因被犯罪分子使用而出现在多个法庭记录中。

与手工精心制作假身份证（一项需要多年精通的犯罪技能）或等待邮寄购买的身份证到达（存在被拦截的风险）相比，OnlyFake允许几乎任何人在几分钟内生成足够真实的假身份证，这些假身份证可能足以绕过各种在线验证系统，或至少欺骗某些人。

OnlyFake的Telegram账户上发布的公告称，“使用Photoshop渲染文件的时代即将结束”。该服务还声称使用“生成器”每天创建多达20,000份文件。服务的所有者，化名为John Wick的人告诉404 Media，可以使用Excel表格中的数据一次性生成数百份文件。

详细：https://www.404media.co/inside-the-underground-site-where-ai-neural-networks-churns-out-fake-ids-onlyfake/</title>
            <link>https://nitter.cz/xiaohuggg/status/1754862694329626817#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754862694329626817#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 Feb 2024 13:40:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个名为OnlyFake的地下网站声称使用他们的AI技术，仅需15美元就能生成以假乱真的假身份证和驾驶证等<br />
<br />
根据404 Media的验证，这个网站的确几乎能瞬间制作出假身份证。而且通过了加密货币交易所OKX的身份验证。<br />
<br />
根据测试，OnlyFake能够创建非常真实的加利福尼亚州驾驶执照，包括任意的姓名、生物信息、地址、到期日期和签名。<br />
<br />
照片甚至给人一种身份证放在蓬松地毯上拍照的感觉，许多网站需要这样的照片来进行验证。<br />
<br />
404 Media还使用该网站生成的另一张假身份证成功通过了加密货币交易所OKX的身份验证过程，而OKX因被犯罪分子使用而出现在多个法庭记录中。<br />
<br />
与手工精心制作假身份证（一项需要多年精通的犯罪技能）或等待邮寄购买的身份证到达（存在被拦截的风险）相比，OnlyFake允许几乎任何人在几分钟内生成足够真实的假身份证，这些假身份证可能足以绕过各种在线验证系统，或至少欺骗某些人。<br />
<br />
OnlyFake的Telegram账户上发布的公告称，“使用Photoshop渲染文件的时代即将结束”。该服务还声称使用“生成器”每天创建多达20,000份文件。服务的所有者，化名为John Wick的人告诉404 Media，可以使用Excel表格中的数据一次性生成数百份文件。<br />
<br />
详细：<a href="https://www.404media.co/inside-the-underground-site-where-ai-neural-networks-churns-out-fake-ids-onlyfake/">404media.co/inside-the-under…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZxR0FneWFrQUFCOFlhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754857535268659311#m</id>
            <title>PopAi升级一波新功能，今天刚上ProductHunt，大家有空可以给投个票：https://www.producthunt.com/posts/popai 

接入了DALLE3 除了生成图像，还可以:

- 将任何图像中的文本提取成可编辑的格式
- 使用截图校对设计上的用词
- 拍照作业自动检查
- 1分钟内翻译图像中的文字
- 上传的任何图像获取Midjourney/Dall-e3提示...
...

传送门：https://popai.saaslink.net/UqVjJf</title>
            <link>https://nitter.cz/xiaohuggg/status/1754857535268659311#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754857535268659311#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 06 Feb 2024 13:20:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PopAi升级一波新功能，今天刚上ProductHunt，大家有空可以给投个票：<a href="https://www.producthunt.com/posts/popai">producthunt.com/posts/popai</a> <br />
<br />
接入了DALLE3 除了生成图像，还可以:<br />
<br />
- 将任何图像中的文本提取成可编辑的格式<br />
- 使用截图校对设计上的用词<br />
- 拍照作业自动检查<br />
- 1分钟内翻译图像中的文字<br />
- 上传的任何图像获取Midjourney/Dall-e3提示...<br />
...<br />
<br />
传送门：<a href="https://popai.saaslink.net/UqVjJf">popai.saaslink.net/UqVjJf</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1723994341709160750#m">nitter.cz/xiaohuggg/status/1723994341709160750#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZwLTVObmJJQUFseUJSLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>