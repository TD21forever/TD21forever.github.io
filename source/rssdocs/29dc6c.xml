<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726478584792186916#m</id>
            <title>OpenAI 董事会已聘请Twitch 的前首席执行官。Emmett Shear 担任OpenAI的首席执行官。🤖</title>
            <link>https://nitter.cz/xiaohuggg/status/1726478584792186916#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726478584792186916#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 05:52:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 董事会已聘请Twitch 的前首席执行官。Emmett Shear 担任OpenAI的首席执行官。🤖</p>
<p><a href="https://nitter.cz/emilychangtv/status/1726468006786859101#m">nitter.cz/emilychangtv/status/1726468006786859101#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726462766553387285#m</id>
            <title>这个挺搞笑的，哈哈哈 我翻译了一下 你们看看

开发者@charliebholtz 利用GPT-4V和ElevenLabs的语音克隆技术模仿大卫·阿滕伯勒的声音（类似我们这的赵忠祥），来实时的解说自己的生活。

它编写了脚本，每五秒从他的摄像头拍摄一张照片，发送给GPT-4V，然后生成阿滕伯勒叙述风格的文本转声音输出。😂

演示中用其标志性的叙述风格描述Holtz，例如他的眼镜、头发和衣服。这种叙述风格模仿了BBC野生动物纪录片的方式。（类似我们这的动物世界😅）

整个过程是这样的：摄像头捕捉图像 → GPT-4 Vision分析图像并生成文本 → 文本通过ElevenLabs的语音合成系统转换成阿滕伯勒的声音。这个过程实现了将视觉信息转换为具有特定声音和风格的语音叙述，创造出一种新颖的互动体验。

这个项目的代码已经公开在GitHub上：https://github.com/cbh123/narrator</title>
            <link>https://nitter.cz/xiaohuggg/status/1726462766553387285#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726462766553387285#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 04:49:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺搞笑的，哈哈哈 我翻译了一下 你们看看<br />
<br />
开发者<a href="https://nitter.cz/charliebholtz" title="Charlie Holtz">@charliebholtz</a> 利用GPT-4V和ElevenLabs的语音克隆技术模仿大卫·阿滕伯勒的声音（类似我们这的赵忠祥），来实时的解说自己的生活。<br />
<br />
它编写了脚本，每五秒从他的摄像头拍摄一张照片，发送给GPT-4V，然后生成阿滕伯勒叙述风格的文本转声音输出。😂<br />
<br />
演示中用其标志性的叙述风格描述Holtz，例如他的眼镜、头发和衣服。这种叙述风格模仿了BBC野生动物纪录片的方式。（类似我们这的动物世界😅）<br />
<br />
整个过程是这样的：摄像头捕捉图像 → GPT-4 Vision分析图像并生成文本 → 文本通过ElevenLabs的语音合成系统转换成阿滕伯勒的声音。这个过程实现了将视觉信息转换为具有特定声音和风格的语音叙述，创造出一种新颖的互动体验。<br />
<br />
这个项目的代码已经公开在GitHub上：<a href="https://github.com/cbh123/narrator">github.com/cbh123/narrator</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY0NDUyNDQ3NTYyNDY1MjgvcHUvaW1nL3hpQ2J4Q1lNdE9UaXp1bjMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726439252316364814#m</id>
            <title>StyleTTS 2：一个开源的媲美 Elevenlabs 的文本转语音工具

🌈 多样化的语音风格：StyleTTS 2能够自动生成多种不同的语音风格，无需依赖特定的参考语音。

🗣 更自然的语音：采用特殊的训练方法，使得生成的语音更加贴近真人的说话方式。

⚡ 高效生成：利用扩散模型技术，高效地生成不同风格的语音。

🎚 精确的语音控制：提供对语音的精确控制，包括语速、语调等方面。

👤 接近真人的语音合成：在测试中，生成的语音质量接近于真人录音。

🔊 适应不同说话者：即使没有特定说话者的样本，也能生成高质量的语音。

工作原原理及特点：

StyleTTS 2利用风格扩散和与大型语音语言模型（SLM）的对抗性训练来实现接近人类水平的TTS合成。

这个模型与其前身不同之处在于，它通过扩散模型将风格建模为一个潜在的随机变量，以生成最适合文本的风格，而不需要参考语音，实现了高效的潜在扩散，同时受益于扩散模型提供的多样化语音合成。

1、非自回归架构：与传统的自回归TTS模型不同，StyleTTS 2采用非自回归架构。它在生成语音时不需要依次预测每个音频样本，而是可以并行生成整个语音序列。这种方法大大提高了语音合成的速度。

2、风格编码器：StyleTTS 2包含一个风格编码器，它能够从参考音频中提取风格特征。这些风格特征包括韵律、语调、语速等，使得生成的语音不仅准确传达文本信息，还能够模仿参考音频的风格和情感。

3、端到端生成：StyleTTS 2实现了端到端的语音生成。它直接从文本和风格向量生成音频波形，而不是先生成梅尔频谱图再转换为音频。这种方法简化了传统TTS系统中的多步骤流程，提高了效率和生成语音的自然度。

4、风格扩散和对抗训练：StyleTTS 2结合了风格扩散和对抗训练技术。风格扩散是指通过风格编码器生成固定长度的风格向量，这些向量能够捕捉到不同的语音风格。对抗训练则是通过生成对抗网络（GAN）来提高语音的自然度和真实感。

5、高质量语音合成：通过这些技术，StyleTTS 2能够生成高质量、自然流畅且具有表现力的语音。它在多个数据集上的性能评估显示，其生成的语音质量接近甚至超过了人类的录音。

6、多样性和灵活性：StyleTTS 2的设计允许它适应不同的语音风格和情感，使其在多种应用场景中都能生成适宜的语音输出。

StyleTTS 2在多个评估结果方面表现出色：

1、高质量语音合成：在多个测试中，StyleTTS 2生成的语音质量非常高，接近或达到了真人录音的水平。这表明了其在模仿人类语音方面的高效能力。

2、比较平均意见得分（CMOS）：在LJSpeech数据集上的评估显示，StyleTTS 2的语音生成质量超过了人类录音，获得了统计上显著的CMOS得分。CMOS是评估语音合成质量的一个重要指标，高CMOS得分意味着更高的语音质量和自然度。

3、多说话者数据集表现：在VCTK数据集上，StyleTTS 2也展现了优异的性能，达到了人类水平。这个数据集包含多个说话者的语音，表明StyleTTS 2能够适应不同说话者的特点，生成多样化且高质量的语音。

4、自然度和表现力：StyleTTS 2不仅在语音的清晰度和准确度上表现优秀，还在自然度和表现力方面取得了显著成果。这意味着生成的语音不仅仅是清晰可懂，还能够传达丰富的情感和语调变化。

StyleTTS 2的评估结果显示了其在文本到语音合成领域的先进性能，特别是在语音质量、自然度和多样性方面。

项目及演示：https://styletts2.github.io/
GitHub：https://github.com/yl4579/StyleTTS2
论文：https://arxiv.org/abs/2306.07691
Colab在线体验：https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/</title>
            <link>https://nitter.cz/xiaohuggg/status/1726439252316364814#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726439252316364814#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 03:16:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>StyleTTS 2：一个开源的媲美 Elevenlabs 的文本转语音工具<br />
<br />
🌈 多样化的语音风格：StyleTTS 2能够自动生成多种不同的语音风格，无需依赖特定的参考语音。<br />
<br />
🗣 更自然的语音：采用特殊的训练方法，使得生成的语音更加贴近真人的说话方式。<br />
<br />
⚡ 高效生成：利用扩散模型技术，高效地生成不同风格的语音。<br />
<br />
🎚 精确的语音控制：提供对语音的精确控制，包括语速、语调等方面。<br />
<br />
👤 接近真人的语音合成：在测试中，生成的语音质量接近于真人录音。<br />
<br />
🔊 适应不同说话者：即使没有特定说话者的样本，也能生成高质量的语音。<br />
<br />
工作原原理及特点：<br />
<br />
StyleTTS 2利用风格扩散和与大型语音语言模型（SLM）的对抗性训练来实现接近人类水平的TTS合成。<br />
<br />
这个模型与其前身不同之处在于，它通过扩散模型将风格建模为一个潜在的随机变量，以生成最适合文本的风格，而不需要参考语音，实现了高效的潜在扩散，同时受益于扩散模型提供的多样化语音合成。<br />
<br />
1、非自回归架构：与传统的自回归TTS模型不同，StyleTTS 2采用非自回归架构。它在生成语音时不需要依次预测每个音频样本，而是可以并行生成整个语音序列。这种方法大大提高了语音合成的速度。<br />
<br />
2、风格编码器：StyleTTS 2包含一个风格编码器，它能够从参考音频中提取风格特征。这些风格特征包括韵律、语调、语速等，使得生成的语音不仅准确传达文本信息，还能够模仿参考音频的风格和情感。<br />
<br />
3、端到端生成：StyleTTS 2实现了端到端的语音生成。它直接从文本和风格向量生成音频波形，而不是先生成梅尔频谱图再转换为音频。这种方法简化了传统TTS系统中的多步骤流程，提高了效率和生成语音的自然度。<br />
<br />
4、风格扩散和对抗训练：StyleTTS 2结合了风格扩散和对抗训练技术。风格扩散是指通过风格编码器生成固定长度的风格向量，这些向量能够捕捉到不同的语音风格。对抗训练则是通过生成对抗网络（GAN）来提高语音的自然度和真实感。<br />
<br />
5、高质量语音合成：通过这些技术，StyleTTS 2能够生成高质量、自然流畅且具有表现力的语音。它在多个数据集上的性能评估显示，其生成的语音质量接近甚至超过了人类的录音。<br />
<br />
6、多样性和灵活性：StyleTTS 2的设计允许它适应不同的语音风格和情感，使其在多种应用场景中都能生成适宜的语音输出。<br />
<br />
StyleTTS 2在多个评估结果方面表现出色：<br />
<br />
1、高质量语音合成：在多个测试中，StyleTTS 2生成的语音质量非常高，接近或达到了真人录音的水平。这表明了其在模仿人类语音方面的高效能力。<br />
<br />
2、比较平均意见得分（CMOS）：在LJSpeech数据集上的评估显示，StyleTTS 2的语音生成质量超过了人类录音，获得了统计上显著的CMOS得分。CMOS是评估语音合成质量的一个重要指标，高CMOS得分意味着更高的语音质量和自然度。<br />
<br />
3、多说话者数据集表现：在VCTK数据集上，StyleTTS 2也展现了优异的性能，达到了人类水平。这个数据集包含多个说话者的语音，表明StyleTTS 2能够适应不同说话者的特点，生成多样化且高质量的语音。<br />
<br />
4、自然度和表现力：StyleTTS 2不仅在语音的清晰度和准确度上表现优秀，还在自然度和表现力方面取得了显著成果。这意味着生成的语音不仅仅是清晰可懂，还能够传达丰富的情感和语调变化。<br />
<br />
StyleTTS 2的评估结果显示了其在文本到语音合成领域的先进性能，特别是在语音质量、自然度和多样性方面。<br />
<br />
项目及演示：<a href="https://styletts2.github.io/">styletts2.github.io/</a><br />
GitHub：<a href="https://github.com/yl4579/StyleTTS2">github.com/yl4579/StyleTTS2</a><br />
论文：<a href="https://arxiv.org/abs/2306.07691">arxiv.org/abs/2306.07691</a><br />
Colab在线体验：<a href="https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/">colab.research.google.com/gi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY0MzA2OTI5MjQ2NDEyODAvcHUvaW1nL21iMUx5Q284Yjkwb1lBMnMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726420710703411675#m</id>
            <title>R to @xiaohuggg: 也可以使用爬取的内容创建Assistant自定义助手

这样你就可以通过一个 API来访问这些生成的知识。

可以将这些知识集成到你自己的产品或应用中去。

简单来说，就是提供了一种方式，让你能够在你的软件或产品中使用这些爬取并整理好的知识。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726420710703411675#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726420710703411675#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 02:02:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以使用爬取的内容创建Assistant自定义助手<br />
<br />
这样你就可以通过一个 API来访问这些生成的知识。<br />
<br />
可以将这些知识集成到你自己的产品或应用中去。<br />
<br />
简单来说，就是提供了一种方式，让你能够在你的软件或产品中使用这些爬取并整理好的知识。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9WNVhwamIwQUFFb0JRLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfVjVYcGpiMEFBRW9CUS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726420708002029783#m</id>
            <title>R to @xiaohuggg: 🌐 爬取网站内容：用户通过配置文件设置目标网址和选择器，GPT-Crawler 自动从这些网站上收集信息。

📁 生成知识文件：爬取的内容被整理成 JSON 文件，这个文件包含了从网站上获取的所有知识。

🤖 创建自定义 GPT：利用这个知识文件，用户可以在 OpenAI 平台上创建自己定制的 GPT 聊天机器人。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726420708002029783#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726420708002029783#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 02:02:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🌐 爬取网站内容：用户通过配置文件设置目标网址和选择器，GPT-Crawler 自动从这些网站上收集信息。<br />
<br />
📁 生成知识文件：爬取的内容被整理成 JSON 文件，这个文件包含了从网站上获取的所有知识。<br />
<br />
🤖 创建自定义 GPT：利用这个知识文件，用户可以在 OpenAI 平台上创建自己定制的 GPT 聊天机器人。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9WNGxRR2FZQUE0NFR3LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfVjRsUUdhWUFBNDRUdy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726420705368273215#m</id>
            <title>GPT-Crawler ：一个开源的知识库自动爬虫工具

它能从一个或多个网址爬取网站内容，然后生成JSON文件格式。

这样爬取的内容可以直接导入到GPTs知识库中，方便你创建自定义知识库的GPTs。

比如你有自己的网站或者资料库，但是整理起来太麻烦，就可以使用这个工具。

GitHub：https://github.com/BuilderIO/gpt-crawler</title>
            <link>https://nitter.cz/xiaohuggg/status/1726420705368273215#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726420705368273215#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 02:02:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT-Crawler ：一个开源的知识库自动爬虫工具<br />
<br />
它能从一个或多个网址爬取网站内容，然后生成JSON文件格式。<br />
<br />
这样爬取的内容可以直接导入到GPTs知识库中，方便你创建自定义知识库的GPTs。<br />
<br />
比如你有自己的网站或者资料库，但是整理起来太麻烦，就可以使用这个工具。<br />
<br />
GitHub：<a href="https://github.com/BuilderIO/gpt-crawler">github.com/BuilderIO/gpt-cra…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9WNFlNaWFvQUFkM0syLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfVjRZTWlhb0FBZDNLMi5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726243969368215722#m</id>
            <title>以前的美丽万种风情

现在的漂亮千篇一律

哪一个是你当年的心中女神？😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1726243969368215722#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726243969368215722#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 14:20:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>以前的美丽万种风情<br />
<br />
现在的漂亮千篇一律<br />
<br />
哪一个是你当年的心中女神？😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI2MjQzODE0MjQ2MDgwNTEyL2ltZy9JbW56LV9QRlVyeTViZVRoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726214623739904109#m</id>
            <title>R to @xiaohuggg: 还可以自己输入Prompt进行即时画图

每打一个字母、一个单词

左边就实时展现，魔幻😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1726214623739904109#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726214623739904109#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 12:23:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还可以自己输入Prompt进行即时画图<br />
<br />
每打一个字母、一个单词<br />
<br />
左边就实时展现，魔幻😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYyMTQxNzA5MDE4OTcyMTYvcHUvaW1nL1ZSbTJEQk9BSUt2WEs1TWEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726214119077085693#m</id>
            <title>LCM即时绘画，在线体验

LCM LoRA能实现即时绘图即时生成的功能，可以说是所见所得。

@ilumine_ai 制作了一个 @huggingface 在线体验地址：https://huggingface.co/spaces/ilumine-AI/LCM-Painter

你可以体验它的伟大之处。真的很牛P！

由于内置了很多Prompt，随便瞎画都可以，体验很流畅。

LCM LoRA由清华大学@SimianLuo 开发。

LCM LoRA旨在加速稳定扩散模型（Stable Diffusion）的运行速度。通过使用LCM LoRA，可以将推理步骤减少到仅2至8步，而不是传统的25至50步。

在一台高性能的3090显卡上，运行SDXL模型只需要大约1秒钟。甚至在Mac电脑上，速度也提高了10倍。

LCM LoRA可以用于文本到图像（Text-to-Image）的转换。例如，它可以与稳定扩散的基础模型结合使用，并通过改变调度器（scheduler）来减少推理步骤。

此外，LCM LoRA还支持图像修复（inpainting）和与其他LoRA模型结合使用，以生成风格化的图像。

模型下载：https://huggingface.co/latent-consistency/lcm-lora-sdxl

LCM LoRA详细介绍：https://huggingface.co/blog/lcm_lora

论文：https://arxiv.org/pdf/2311.05556.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1726214119077085693#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726214119077085693#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 12:21:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LCM即时绘画，在线体验<br />
<br />
LCM LoRA能实现即时绘图即时生成的功能，可以说是所见所得。<br />
<br />
<a href="https://nitter.cz/ilumine_ai" title="ilumine AI">@ilumine_ai</a> 制作了一个 <a href="https://nitter.cz/huggingface" title="Hugging Face">@huggingface</a> 在线体验地址：<a href="https://huggingface.co/spaces/ilumine-AI/LCM-Painter">huggingface.co/spaces/ilumin…</a><br />
<br />
你可以体验它的伟大之处。真的很牛P！<br />
<br />
由于内置了很多Prompt，随便瞎画都可以，体验很流畅。<br />
<br />
LCM LoRA由清华大学<a href="https://nitter.cz/SimianLuo" title="luo allen">@SimianLuo</a> 开发。<br />
<br />
LCM LoRA旨在加速稳定扩散模型（Stable Diffusion）的运行速度。通过使用LCM LoRA，可以将推理步骤减少到仅2至8步，而不是传统的25至50步。<br />
<br />
在一台高性能的3090显卡上，运行SDXL模型只需要大约1秒钟。甚至在Mac电脑上，速度也提高了10倍。<br />
<br />
LCM LoRA可以用于文本到图像（Text-to-Image）的转换。例如，它可以与稳定扩散的基础模型结合使用，并通过改变调度器（scheduler）来减少推理步骤。<br />
<br />
此外，LCM LoRA还支持图像修复（inpainting）和与其他LoRA模型结合使用，以生成风格化的图像。<br />
<br />
模型下载：<a href="https://huggingface.co/latent-consistency/lcm-lora-sdxl">huggingface.co/latent-consis…</a><br />
<br />
LCM LoRA详细介绍：<a href="https://huggingface.co/blog/lcm_lora">huggingface.co/blog/lcm_lora</a><br />
<br />
论文：<a href="https://arxiv.org/pdf/2311.05556.pdf">arxiv.org/pdf/2311.05556.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYxOTk1NTU5NDYxMTkxNjgvcHUvaW1nL1lKLXRUSGh5OFRMUlNDWUguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726198779727257956#m</id>
            <title>VM Pill：一个可吞咽装置 在你体内追踪你的生命体征

追踪生命体征：这个设备可以像药丸一样吞下，然后在体内追踪呼吸和心率等关键生命体征。

这对于需要持续监测的患者，如慢性病患者或容易发生阿片类药物过量的人，非常有用。

高精度监测：研究人员给西弗吉尼亚大学的 10 名睡眠呼吸暂停患者服用了 VM 药丸。

该设备可检测患者呼吸何时停止并监测其呼吸频率，准确率高达 92.7%。与外部设备相比，该药丸可以监测心率，准确度至少为 96%。试验还表明该设备是安全的，所有患者在实验后的几天内都通过了该设备。

便捷的健康监测：根据研究的首席作者、麻省理工学院机械工程副教授兼布莱根妇女医院的胃肠病学家 Giovanni Traverso 的说法，这个设备可以在不需要医院访问的情况下帮助诊断和监测多种健康状况，从而使医疗保健更加方便和支持性。
这种可吞咽的设备大约和多种维生素药丸一样大，内含传感器，能够在通过消化系统时测量生命体征。它由生物相容材料制成，设计上能够安全通过人体，不造成任何伤害。该设备通过无线方式将收集到的数据传输到智能手机或电脑，实现实时监测。
在一项人体试验中，健康志愿者吞咽了这个设备，然后进行了各种体力活动，如在跑步机上跑步或睡觉。该设备准确地检测到了志愿者的心率和呼吸率，即使在剧烈运动期间也是如此。该设备还能检测到患者的呼吸停止，并以 92.7% 的准确率监测他们的呼吸率。
研究人员认为，这个设备对于有阿片类药物过量风险的患者特别有益，因为它可以连续监测他们的呼吸和心率，以便及时发现任何不适或过量的迹象。这可能允许早期干预，从而挽救生命。

此外，这个设备还可以用于其他医疗应用，如监测慢性病患者、追踪药物效果或评估整体健康和福祉。它消除了侵入性程序或持续医院访问的需要，使医疗保健对患者来说更加方便和可及。

Traverso 表示，当前版本的 VM Pill 只能在体内停留大约一天，但他们正在努力改进该设备，使其能够在体内停留更长时间以进行长期监测。他们还希望升级该设备，以便一旦检测到症状，它就可以自动输送药物，以扭转阿片类药物过量等情况。

详细：https://interestingengineering.com/health/swallowable-device-tracking-vital-signs-inside-the-body-in-human-trial
论文：https://www.cell.com/device/fulltext/S2666-9986(23)00184-9</title>
            <link>https://nitter.cz/xiaohuggg/status/1726198779727257956#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726198779727257956#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 11:20:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VM Pill：一个可吞咽装置 在你体内追踪你的生命体征<br />
<br />
追踪生命体征：这个设备可以像药丸一样吞下，然后在体内追踪呼吸和心率等关键生命体征。<br />
<br />
这对于需要持续监测的患者，如慢性病患者或容易发生阿片类药物过量的人，非常有用。<br />
<br />
高精度监测：研究人员给西弗吉尼亚大学的 10 名睡眠呼吸暂停患者服用了 VM 药丸。<br />
<br />
该设备可检测患者呼吸何时停止并监测其呼吸频率，准确率高达 92.7%。与外部设备相比，该药丸可以监测心率，准确度至少为 96%。试验还表明该设备是安全的，所有患者在实验后的几天内都通过了该设备。<br />
<br />
便捷的健康监测：根据研究的首席作者、麻省理工学院机械工程副教授兼布莱根妇女医院的胃肠病学家 Giovanni Traverso 的说法，这个设备可以在不需要医院访问的情况下帮助诊断和监测多种健康状况，从而使医疗保健更加方便和支持性。<br />
这种可吞咽的设备大约和多种维生素药丸一样大，内含传感器，能够在通过消化系统时测量生命体征。它由生物相容材料制成，设计上能够安全通过人体，不造成任何伤害。该设备通过无线方式将收集到的数据传输到智能手机或电脑，实现实时监测。<br />
在一项人体试验中，健康志愿者吞咽了这个设备，然后进行了各种体力活动，如在跑步机上跑步或睡觉。该设备准确地检测到了志愿者的心率和呼吸率，即使在剧烈运动期间也是如此。该设备还能检测到患者的呼吸停止，并以 92.7% 的准确率监测他们的呼吸率。<br />
研究人员认为，这个设备对于有阿片类药物过量风险的患者特别有益，因为它可以连续监测他们的呼吸和心率，以便及时发现任何不适或过量的迹象。这可能允许早期干预，从而挽救生命。<br />
<br />
此外，这个设备还可以用于其他医疗应用，如监测慢性病患者、追踪药物效果或评估整体健康和福祉。它消除了侵入性程序或持续医院访问的需要，使医疗保健对患者来说更加方便和可及。<br />
<br />
Traverso 表示，当前版本的 VM Pill 只能在体内停留大约一天，但他们正在努力改进该设备，使其能够在体内停留更长时间以进行长期监测。他们还希望升级该设备，以便一旦检测到症状，它就可以自动输送药物，以扭转阿片类药物过量等情况。<br />
<br />
详细：<a href="https://interestingengineering.com/health/swallowable-device-tracking-vital-signs-inside-the-body-in-human-trial">interestingengineering.com/h…</a><br />
论文：<a href="https://www.cell.com/device/fulltext/S2666-9986(23)00184-9">cell.com/device/fulltext/S26…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TdGZWX2E0QUFCOVBvLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TdGZWNGJvQUFjRzZ2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TdGZWN2JZQUFEMk4wLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726172579071922338#m</id>
            <title>WPS就使用用户文档进行AI训练道歉 称是误会🙃

近期，有用户发现WPS在更新其隐私政策后提到，会将用户主动上传的文档用于训练人工智能。

这意味着在使用WPS的过程中，用户的隐私可能在不知不觉中被“窃取”，因为用户已经同意了隐私政策。

作为一款办公软件，WPS将用户文档用于训练人工智能引起了争议，因为文档可能涉及大量机密信息，被窃取可能会造成重大问题。

 • WPS官方对此回应称，之前版本的表述为用户造成了困扰。他们已经更新了隐私政策，去除了容易引起误解的表述，并确保其内容与实际操作严格对应。

 • WPS官方声明，所有用户文档不会被用于任何AI训练目的，也不会在未经用户同意的情况下用于任何场景。他们重申，始终严格遵守所有可适用的用户隐私保护法律和标准。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726172579071922338#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726172579071922338#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 09:36:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WPS就使用用户文档进行AI训练道歉 称是误会🙃<br />
<br />
近期，有用户发现WPS在更新其隐私政策后提到，会将用户主动上传的文档用于训练人工智能。<br />
<br />
这意味着在使用WPS的过程中，用户的隐私可能在不知不觉中被“窃取”，因为用户已经同意了隐私政策。<br />
<br />
作为一款办公软件，WPS将用户文档用于训练人工智能引起了争议，因为文档可能涉及大量机密信息，被窃取可能会造成重大问题。<br />
<br />
 • WPS官方对此回应称，之前版本的表述为用户造成了困扰。他们已经更新了隐私政策，去除了容易引起误解的表述，并确保其内容与实际操作严格对应。<br />
<br />
 • WPS官方声明，所有用户文档不会被用于任何AI训练目的，也不会在未经用户同意的情况下用于任何场景。他们重申，始终严格遵守所有可适用的用户隐私保护法律和标准。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TWXpxcWJBQUF5V0RGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726132734098276861#m</id>
            <title>Insanely Fast Whisper：极速音频转录工具，能在98 秒的时间内转录 300 分钟（5 小时）的音频。

是一个基于 OpenAI 的 Whisper Large v3 模型的改进项目，旨在实现极快速度的音频转录。这个项目通过使用 🤗 Transformers、Optimum 和 flash-attn 技术，使得转录速度大幅提升。

提供了一个简单的命令行界面，用户可以通过几个简单的命令在电脑上使用这个工具来转录音频。

除了基本的 Whisper 模型，还可以选择其他版本，比如 distil-whisper，以适应不同的需求。

该项目目前只支持 Nvidia GPU。
提供了不同的配置选项，以最大化转录吞吐量。

GitHub：https://github.com/chenxwh/insanely-fast-whisper

在线演示：https://replicate.com/vaibhavs10/incredibly-fast-whisper

视频：测试几次来看16分钟的录音，大概是14秒到27秒估计是需要强大GPU才能实现宣称的效果！</title>
            <link>https://nitter.cz/xiaohuggg/status/1726132734098276861#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726132734098276861#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 06:58:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Insanely Fast Whisper：极速音频转录工具，能在98 秒的时间内转录 300 分钟（5 小时）的音频。<br />
<br />
是一个基于 OpenAI 的 Whisper Large v3 模型的改进项目，旨在实现极快速度的音频转录。这个项目通过使用 🤗 Transformers、Optimum 和 flash-attn 技术，使得转录速度大幅提升。<br />
<br />
提供了一个简单的命令行界面，用户可以通过几个简单的命令在电脑上使用这个工具来转录音频。<br />
<br />
除了基本的 Whisper 模型，还可以选择其他版本，比如 distil-whisper，以适应不同的需求。<br />
<br />
该项目目前只支持 Nvidia GPU。<br />
提供了不同的配置选项，以最大化转录吞吐量。<br />
<br />
GitHub：<a href="https://github.com/chenxwh/insanely-fast-whisper">github.com/chenxwh/insanely-…</a><br />
<br />
在线演示：<a href="https://replicate.com/vaibhavs10/incredibly-fast-whisper">replicate.com/vaibhavs10/inc…</a><br />
<br />
视频：测试几次来看16分钟的录音，大概是14秒到27秒估计是需要强大GPU才能实现宣称的效果！</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYxMTcwNjcyOTkxNTE4NzIvcHUvaW1nL2dHMGxZVHNzamhiZm9NWjMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726112197448024114#m</id>
            <title>OpenStax ：免费提供各种学科的K12和大学教科书资源

OpenStax 是Rice University的一部分，他们提供了多个学科领域的教科书，这些学科包括商业、人文学科、数学、科学和社会科学等。

资源包括讲座幻灯片、实验室手册、题库等。

可以通过网络浏览、下载 PDF 文件和打印等多种选项轻松访问教材。

OpenStax 的教科书和资源经过专业人士编写和同行评审，确保质量和准确性。

访问：https://openstax.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1726112197448024114#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726112197448024114#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 05:36:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenStax ：免费提供各种学科的K12和大学教科书资源<br />
<br />
OpenStax 是Rice University的一部分，他们提供了多个学科领域的教科书，这些学科包括商业、人文学科、数学、科学和社会科学等。<br />
<br />
资源包括讲座幻灯片、实验室手册、题库等。<br />
<br />
可以通过网络浏览、下载 PDF 文件和打印等多种选项轻松访问教材。<br />
<br />
OpenStax 的教科书和资源经过专业人士编写和同行评审，确保质量和准确性。<br />
<br />
访问：<a href="https://openstax.org/">openstax.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9SaG5iVmFjQUFfeFgzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726077207318339639#m</id>
            <title>R to @xiaohuggg: 他还说："现在正在发生的技术变革，将彻底改变我们生活方式、经济和社会结构以及其他可能性的限制....

这在OpenAI的历史上有四次 ，而最近一次是在过去几周内。

我有幸在【拨开无知的面纱和探索未知的边界】时在场，

能够做到这一点是我职业生涯中的荣誉。"</title>
            <link>https://nitter.cz/xiaohuggg/status/1726077207318339639#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726077207318339639#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 03:17:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他还说："现在正在发生的技术变革，将彻底改变我们生活方式、经济和社会结构以及其他可能性的限制....<br />
<br />
这在OpenAI的历史上有四次 ，而最近一次是在过去几周内。<br />
<br />
我有幸在【拨开无知的面纱和探索未知的边界】时在场，<br />
<br />
能够做到这一点是我职业生涯中的荣誉。"</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYwNzYxMzAxMjQ1MzM3NjAvcHUvaW1nL0dFTEtHZGd6TGl2Z3F1MlkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726076059769335909#m</id>
            <title>在Sam 奥特曼被解雇的前一天，他在APEC的一次会议上，他暗示了OpenAI已经开发出比现在GPT 4更加强大，让人无法想象，远远超出人们期待的东西...

这可能是被解雇的导火索，导致Ilya等董事会成员认为他隐瞒了很多信息，没有和董事会分享。

Sam 称：“模型的能力将会有一个没人预料到的飞跃。[这将会]与人们预期不同。将会是惊人的！”

这似乎在暗示OpenAI内部可能已经实现了技术和模型上的巨大飞跃。而Ilya 等董事会成员可能因此感到恐慌或者不知情，所以采取了行动。认为奥特曼在隐瞒一些技术进展，没有分享信息。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726076059769335909#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726076059769335909#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 03:12:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在Sam 奥特曼被解雇的前一天，他在APEC的一次会议上，他暗示了OpenAI已经开发出比现在GPT 4更加强大，让人无法想象，远远超出人们期待的东西...<br />
<br />
这可能是被解雇的导火索，导致Ilya等董事会成员认为他隐瞒了很多信息，没有和董事会分享。<br />
<br />
Sam 称：“模型的能力将会有一个没人预料到的飞跃。[这将会]与人们预期不同。将会是惊人的！”<br />
<br />
这似乎在暗示OpenAI内部可能已经实现了技术和模型上的巨大飞跃。而Ilya 等董事会成员可能因此感到恐慌或者不知情，所以采取了行动。认为奥特曼在隐瞒一些技术进展，没有分享信息。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYwNjA4NjQzMTc1NTQ2ODgvcHUvaW1nL2IxVUJaMUhqYnJSVjZGSVYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726070163332530361#m</id>
            <title>你还别说

自从奥特曼被解雇后

GPT稳定和快了很多</title>
            <link>https://nitter.cz/xiaohuggg/status/1726070163332530361#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726070163332530361#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 02:49:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>你还别说<br />
<br />
自从奥特曼被解雇后<br />
<br />
GPT稳定和快了很多</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726053135209382391#m</id>
            <title>Mustango：可以通过文本提示生成完整的音乐

Mustango有点类似Midjourney，可以使用详细的文本提示，如你可以详细描述音乐风格、节奏、和弦、使用什么乐器等，来生成相应风格的音乐

最重要的是它能够理解和处理音乐特定的技术语言，如和弦、节奏、速度和调性，从而提供更精确的音乐生成控制。

这里有几个关键点需要理解：

1、文本到音乐的转换：用户可以输入一段文本，这段文本可能包含关于音乐的各种描述，比如音乐的风格（如古典、爵士）、节奏（快速、慢节奏）、和弦类型（大调、小调）等。Mustango系统会解析这些文本提示，并基于这些提示生成音乐。

2、可控性：在这个上下文中，"可控"意味着用户可以通过他们的文本输入来影响和指导音乐的生成。例如，如果用户指定他们想要一首快节奏的爵士乐，那么生成的音乐将反映出这些特定的特征。

3、技术实现：为了实现这种从文本到音乐的转换，Mustango利用了先进的人工智能技术，包括潜在扩散模型和Flan-T5模型。这些技术使得系统能够理解文本提示中的复杂信息，并将这些信息转化为音乐创作的指导。

技术细节和原理：

系统利用最新的潜在扩散模型（Latent Diffusion Model, LDM）和Flan-T5模型，结合音乐领域的知识，实现了从文本到音乐的高度可控生成。

1、潜在扩散模型（Latent Diffusion Model, LDM）：这是一种生成模型，通过逐步去除噪声来生成数据。在音乐生成中，它可以从一个随机噪声状态逐步转换成具有特定特征的音乐片段。

2、Flan-T5模型：这是一种基于Transformer的文本处理模型，用于理解和处理用户输入的文本提示。它可以从文本中提取音乐相关的信息，如风格、节奏和和弦。

3、MuNet（音乐领域知识通知UNet子模块）：这是Mustango的核心组件，它结合了音乐特定的特征和文本嵌入到去噪过程中。MuNet能够从文本提示中预测音乐特征，并将这些特征融入音乐生成过程。

4、数据增强方法：为了克服高质量音乐文本数据集的有限可用性，Mustango采用了一种新颖的数据增强方法。这包括改变音乐音频的和声、节奏和动态特征，并使用最先进的音乐信息检索方法提取音乐特征，然后将这些特征附加到现有的文本描述中。

通过这些技术和原理的结合，Mustango能够从简单的文本提示中生成高质量、高度可控的音乐，为音乐创作和人工智能领域带来了新的可能性。

MusicBench数据集：

MusicBench数据集是Mustango项目中使用的一个关键组成部分。这个数据集包含了大量的音乐实例，每个实例都附有详细的音乐理论描述。

MusicBench数据集为Mustango项目提供了一个丰富的、多样化的音乐资源库，使得从文本到音乐的生成过程更加精确和多样化。

以下是MusicBench数据集的一些详细内容：

1、数据集规模：MusicBench数据集包含超过52000个音乐实例。这些实例涵盖了多种音乐风格和类型，为音乐生成模型提供了丰富的训练材料。

2、音乐理论描述：每个音乐实例都附有基于音乐理论的描述。这些描述可能包括音乐的节奏、和弦、调性、速度等信息。这些详细的描述使得数据集不仅适用于音乐生成，还适用于音乐理论和分析的研究。

3、多样性：MusicBench数据集包含了多种风格的音乐，从古典到现代流行音乐，从爵士乐到电子音乐等。这种多样性确保了生成的音乐可以覆盖广泛的听众口味和需求。

4、用途：这个数据集主要用于训练和评估音乐生成模型，如Mustango。通过这个数据集，模型可以学习如何根据文本提示生成音乐，并且能够理解和应用音乐理论中的复杂概念。

5、数据增强：Mustango项目中还采用了数据增强方法，这意味着原始的MusicBench数据集被进一步加工和扩展，以提高模型的性能和适应性。

通过广泛的实验，Mustango在音乐生成质量上达到了业界领先水平。

在通过音乐特定文本提示进行控制方面，Mustango的表现远超其他模型，尤其是在和弦、节奏、调性和速度方面。

项目地址：https://amaai-lab.github.io/mustango/
论文：https://arxiv.org/abs/2311.08355
GitHub：https://github.com/AMAAI-Lab/mustango
在线 体验：https://huggingface.co/spaces/declare-lab/mustango</title>
            <link>https://nitter.cz/xiaohuggg/status/1726053135209382391#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726053135209382391#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 01:41:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mustango：可以通过文本提示生成完整的音乐<br />
<br />
Mustango有点类似Midjourney，可以使用详细的文本提示，如你可以详细描述音乐风格、节奏、和弦、使用什么乐器等，来生成相应风格的音乐<br />
<br />
最重要的是它能够理解和处理音乐特定的技术语言，如和弦、节奏、速度和调性，从而提供更精确的音乐生成控制。<br />
<br />
这里有几个关键点需要理解：<br />
<br />
1、文本到音乐的转换：用户可以输入一段文本，这段文本可能包含关于音乐的各种描述，比如音乐的风格（如古典、爵士）、节奏（快速、慢节奏）、和弦类型（大调、小调）等。Mustango系统会解析这些文本提示，并基于这些提示生成音乐。<br />
<br />
2、可控性：在这个上下文中，"可控"意味着用户可以通过他们的文本输入来影响和指导音乐的生成。例如，如果用户指定他们想要一首快节奏的爵士乐，那么生成的音乐将反映出这些特定的特征。<br />
<br />
3、技术实现：为了实现这种从文本到音乐的转换，Mustango利用了先进的人工智能技术，包括潜在扩散模型和Flan-T5模型。这些技术使得系统能够理解文本提示中的复杂信息，并将这些信息转化为音乐创作的指导。<br />
<br />
技术细节和原理：<br />
<br />
系统利用最新的潜在扩散模型（Latent Diffusion Model, LDM）和Flan-T5模型，结合音乐领域的知识，实现了从文本到音乐的高度可控生成。<br />
<br />
1、潜在扩散模型（Latent Diffusion Model, LDM）：这是一种生成模型，通过逐步去除噪声来生成数据。在音乐生成中，它可以从一个随机噪声状态逐步转换成具有特定特征的音乐片段。<br />
<br />
2、Flan-T5模型：这是一种基于Transformer的文本处理模型，用于理解和处理用户输入的文本提示。它可以从文本中提取音乐相关的信息，如风格、节奏和和弦。<br />
<br />
3、MuNet（音乐领域知识通知UNet子模块）：这是Mustango的核心组件，它结合了音乐特定的特征和文本嵌入到去噪过程中。MuNet能够从文本提示中预测音乐特征，并将这些特征融入音乐生成过程。<br />
<br />
4、数据增强方法：为了克服高质量音乐文本数据集的有限可用性，Mustango采用了一种新颖的数据增强方法。这包括改变音乐音频的和声、节奏和动态特征，并使用最先进的音乐信息检索方法提取音乐特征，然后将这些特征附加到现有的文本描述中。<br />
<br />
通过这些技术和原理的结合，Mustango能够从简单的文本提示中生成高质量、高度可控的音乐，为音乐创作和人工智能领域带来了新的可能性。<br />
<br />
MusicBench数据集：<br />
<br />
MusicBench数据集是Mustango项目中使用的一个关键组成部分。这个数据集包含了大量的音乐实例，每个实例都附有详细的音乐理论描述。<br />
<br />
MusicBench数据集为Mustango项目提供了一个丰富的、多样化的音乐资源库，使得从文本到音乐的生成过程更加精确和多样化。<br />
<br />
以下是MusicBench数据集的一些详细内容：<br />
<br />
1、数据集规模：MusicBench数据集包含超过52000个音乐实例。这些实例涵盖了多种音乐风格和类型，为音乐生成模型提供了丰富的训练材料。<br />
<br />
2、音乐理论描述：每个音乐实例都附有基于音乐理论的描述。这些描述可能包括音乐的节奏、和弦、调性、速度等信息。这些详细的描述使得数据集不仅适用于音乐生成，还适用于音乐理论和分析的研究。<br />
<br />
3、多样性：MusicBench数据集包含了多种风格的音乐，从古典到现代流行音乐，从爵士乐到电子音乐等。这种多样性确保了生成的音乐可以覆盖广泛的听众口味和需求。<br />
<br />
4、用途：这个数据集主要用于训练和评估音乐生成模型，如Mustango。通过这个数据集，模型可以学习如何根据文本提示生成音乐，并且能够理解和应用音乐理论中的复杂概念。<br />
<br />
5、数据增强：Mustango项目中还采用了数据增强方法，这意味着原始的MusicBench数据集被进一步加工和扩展，以提高模型的性能和适应性。<br />
<br />
通过广泛的实验，Mustango在音乐生成质量上达到了业界领先水平。<br />
<br />
在通过音乐特定文本提示进行控制方面，Mustango的表现远超其他模型，尤其是在和弦、节奏、调性和速度方面。<br />
<br />
项目地址：<a href="https://amaai-lab.github.io/mustango/">amaai-lab.github.io/mustango…</a><br />
论文：<a href="https://arxiv.org/abs/2311.08355">arxiv.org/abs/2311.08355</a><br />
GitHub：<a href="https://github.com/AMAAI-Lab/mustango">github.com/AMAAI-Lab/mustang…</a><br />
在线 体验：<a href="https://huggingface.co/spaces/declare-lab/mustango">huggingface.co/spaces/declar…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU4Nzc4OTE1NjM3NTM0NzIvcHUvaW1nL0tiVFlNMzZBdHQ0Sk9nNlguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1726026751011082506#m</id>
            <title>RT by @xiaohuggg: 这封信应该写出了很多人的心声吧：

亲爱的 Mira 和 Ilya，恭喜你们在 OpenAI 所取得的惊人成就。鉴于你们两位作为创业者从未筹集过资金，接下来的情况可能需要我来说明一下。

首先，你们必须开始筹资，因为你们公司的每笔交易成本模式根本不切实际。记住，每次有人向 ChatGPT 提出一个愚蠢的问题，你们就要花费 0.3 美元。有时候，我甚至会重复问同一个问题五次。

更重要的是，最近那笔估值高达 800 亿美元的交易已经成为过去，就像埃及的法老一样，虽然听起来很辉煌，但它已经成为历史尘埃，不会再回来了。

不仅如此，你们还意外冒犯了你们最大的合作伙伴，微软。尽管他们在公开场合会说些恭维话，但你我都明白，他们肯定对此感到极度愤怒。

你们的一些顶尖研究员已经离职，而如果说 Sam Altman 特别擅长的是什么，那无疑就是筹集和运用资本。在你们的名片上印上 “CEO” 和 “实际 CEO” 之前，Sam 可能就已经启动了一个新公司，筹集到了 10 亿美元的投资，并向你们所有即将离去的顶尖产品人员和研究员发出了邀请。

这样一来，你们迎接下周的，将是失去了顶级交易人、顶尖研究员、最具远见的产品领袖、最重要的合作伙伴和最大的投资者，而你们的业务单位经济模型也非常糟糕。

而且，别忘了，你们俩并不是真正的企业家。你们董事会的大多数成员从未真正从事过科技行业工作。你们从未经历过向众多投资者推销、经过种种流程最终达成交易的过程中所遭遇的屡屡拒绝。有时候，即使投资者嘴上说“是”，心里却并非如此。有时甚至在合同上签字了，却并不打算真的打款。你们将不得不亲身经历这一切痛苦和拒绝，并承受为你们团队成员提供生计的巨大压力，比如他们的房贷、车贷、孩子的学费等，就像你们在上一次公司野餐时一起玩耍的那些人。

在你们被市场的残酷现实击垮后，你们将最终将 OpenAI 卖给微软，并成为微软位于多雨的西雅图 4 号大楼的全球首席产品经理。微软不会解雇你们，萨蒂亚总是会对你们说恰当的话，因为他是一个讲究荣誉的人。

但在你心底，当你在 Netflix 上观看关于 OpenAI 的电影，看到 Joseph-Gordon Levitt 的妻子扮演的董事会成员在 Google Meet 上解雇 Sam Altman 的场景时，你会反思并意识到，你本可以拥有一切——你本可以成为一家价值 1 万亿美元公司的掌舵人。但历史终将忘记你。Sam、Greg 和所有其他人都会继续前进，把你留在过去。e/acc 最终还是会创造出 OpenAI 的基本替代品。</title>
            <link>https://nitter.cz/dotey/status/1726026751011082506#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1726026751011082506#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 23:57:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这封信应该写出了很多人的心声吧：<br />
<br />
亲爱的 Mira 和 Ilya，恭喜你们在 OpenAI 所取得的惊人成就。鉴于你们两位作为创业者从未筹集过资金，接下来的情况可能需要我来说明一下。<br />
<br />
首先，你们必须开始筹资，因为你们公司的每笔交易成本模式根本不切实际。记住，每次有人向 ChatGPT 提出一个愚蠢的问题，你们就要花费 0.3 美元。有时候，我甚至会重复问同一个问题五次。<br />
<br />
更重要的是，最近那笔估值高达 800 亿美元的交易已经成为过去，就像埃及的法老一样，虽然听起来很辉煌，但它已经成为历史尘埃，不会再回来了。<br />
<br />
不仅如此，你们还意外冒犯了你们最大的合作伙伴，微软。尽管他们在公开场合会说些恭维话，但你我都明白，他们肯定对此感到极度愤怒。<br />
<br />
你们的一些顶尖研究员已经离职，而如果说 Sam Altman 特别擅长的是什么，那无疑就是筹集和运用资本。在你们的名片上印上 “CEO” 和 “实际 CEO” 之前，Sam 可能就已经启动了一个新公司，筹集到了 10 亿美元的投资，并向你们所有即将离去的顶尖产品人员和研究员发出了邀请。<br />
<br />
这样一来，你们迎接下周的，将是失去了顶级交易人、顶尖研究员、最具远见的产品领袖、最重要的合作伙伴和最大的投资者，而你们的业务单位经济模型也非常糟糕。<br />
<br />
而且，别忘了，你们俩并不是真正的企业家。你们董事会的大多数成员从未真正从事过科技行业工作。你们从未经历过向众多投资者推销、经过种种流程最终达成交易的过程中所遭遇的屡屡拒绝。有时候，即使投资者嘴上说“是”，心里却并非如此。有时甚至在合同上签字了，却并不打算真的打款。你们将不得不亲身经历这一切痛苦和拒绝，并承受为你们团队成员提供生计的巨大压力，比如他们的房贷、车贷、孩子的学费等，就像你们在上一次公司野餐时一起玩耍的那些人。<br />
<br />
在你们被市场的残酷现实击垮后，你们将最终将 OpenAI 卖给微软，并成为微软位于多雨的西雅图 4 号大楼的全球首席产品经理。微软不会解雇你们，萨蒂亚总是会对你们说恰当的话，因为他是一个讲究荣誉的人。<br />
<br />
但在你心底，当你在 Netflix 上观看关于 OpenAI 的电影，看到 Joseph-Gordon Levitt 的妻子扮演的董事会成员在 Google Meet 上解雇 Sam Altman 的场景时，你会反思并意识到，你本可以拥有一切——你本可以成为一家价值 1 万亿美元公司的掌舵人。但历史终将忘记你。Sam、Greg 和所有其他人都会继续前进，把你留在过去。e/acc 最终还是会创造出 OpenAI 的基本替代品。</p>
<p><a href="https://nitter.cz/varun_mathur/status/1725971418238849154#m">nitter.cz/varun_mathur/status/1725971418238849154#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>