<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743101491421024287#m</id>
            <title>GPTs 将允许向用户发送Email

也就是允许向用户群发推送消息...

🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1743101491421024287#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743101491421024287#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 02:45:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs 将允许向用户发送Email<br />
<br />
也就是允许向用户群发推送消息...<br />
<br />
🤔</p>
<p><a href="https://nitter.cz/imrat/status/1742983298903842958#m">nitter.cz/imrat/status/1742983298903842958#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RDOU1rZmFnQUFPVXpRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743095818100658376#m</id>
            <title>书面的备忘录比PPT演示效果好的原因：

1、PPT 的设计目的是说服，这是一种销售工具。但是在公司内部，你想要的是真相，而不是业务主管的推销。

2、PPT 的缺点是，它对作者相对容易，对观众来说却很难抓住要点。备忘录正好相反，写好一份六页的备忘录，对于作者是很难的。

你可能需要两周的时间，先写出初稿，然后再重写，不断加工，确保你的文字是准确和可靠的。所以，备忘录对作者非常困难，但对观众就好多了，半小时的阅读就能搞清楚事情的来龙去脉，也看得出作者对这个问题的熟悉程度。

3、备忘录可以节省会议时间。如果是 PPT演示，高管们会不停地打断提问，结果发现答案就在下一张幻灯片。
但是，阅读备忘录时，你必须先把所有问题写在空白处，当你读到最后一页时，发现很多问题已经得到了解答，这就节省了当众提问的时间。

4、PPT演示过程中，主讲人也许会根据现场情况，临时决定隐藏或修改一些讲法。备忘录就没有这个问题，主讲人必须事先给出完整描述，你更能了解他的真实想法。

5、PPT通常只是一些要点，不是完整的句子，有利于隐藏很多草率的想法。而备忘录是完整的段落，必须有主题句，有动词和名词，你很难隐藏自己的草率思维。

备忘录迫使作者处于最佳状态，你能得到一个人真正最好的想法。如果 PPT演示，你们可能要讨论很久，发言人才能进入最佳状态。从长远来看，备忘录节省了你的时间。</title>
            <link>https://nitter.cz/xiaohuggg/status/1743095818100658376#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743095818100658376#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 02:23:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>书面的备忘录比PPT演示效果好的原因：<br />
<br />
1、PPT 的设计目的是说服，这是一种销售工具。但是在公司内部，你想要的是真相，而不是业务主管的推销。<br />
<br />
2、PPT 的缺点是，它对作者相对容易，对观众来说却很难抓住要点。备忘录正好相反，写好一份六页的备忘录，对于作者是很难的。<br />
<br />
你可能需要两周的时间，先写出初稿，然后再重写，不断加工，确保你的文字是准确和可靠的。所以，备忘录对作者非常困难，但对观众就好多了，半小时的阅读就能搞清楚事情的来龙去脉，也看得出作者对这个问题的熟悉程度。<br />
<br />
3、备忘录可以节省会议时间。如果是 PPT演示，高管们会不停地打断提问，结果发现答案就在下一张幻灯片。<br />
但是，阅读备忘录时，你必须先把所有问题写在空白处，当你读到最后一页时，发现很多问题已经得到了解答，这就节省了当众提问的时间。<br />
<br />
4、PPT演示过程中，主讲人也许会根据现场情况，临时决定隐藏或修改一些讲法。备忘录就没有这个问题，主讲人必须事先给出完整描述，你更能了解他的真实想法。<br />
<br />
5、PPT通常只是一些要点，不是完整的句子，有利于隐藏很多草率的想法。而备忘录是完整的段落，必须有主题句，有动词和名词，你很难隐藏自己的草率思维。<br />
<br />
备忘录迫使作者处于最佳状态，你能得到一个人真正最好的想法。如果 PPT演示，你们可能要讨论很久，发言人才能进入最佳状态。从长远来看，备忘录节省了你的时间。</p>
<p><a href="https://nitter.cz/ruanyf/status/1743080828786425994#m">nitter.cz/ruanyf/status/1743080828786425994#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743089154127470719#m</id>
            <title>R to @xiaohuggg: 英文效果演示，整体还不错：</title>
            <link>https://nitter.cz/xiaohuggg/status/1743089154127470719#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743089154127470719#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 01:56:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>英文效果演示，整体还不错：</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI5NzgzNTE4MzUzNTcxODQvcHUvaW1nL1A5Rnh4SlFrVE9hM29vUkQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743089151317270983#m</id>
            <title>DreamTalk这个效果还行啊 😀

中文效果差一点

测了下，中文语速慢还可以，语速快有时候跟不上，英文口型效果对的很不错。

感兴趣的可以体验下，记得交作业：https://huggingface.co/spaces/fffiloni/dreamtalk</title>
            <link>https://nitter.cz/xiaohuggg/status/1743089151317270983#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743089151317270983#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 01:56:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTalk这个效果还行啊 😀<br />
<br />
中文效果差一点<br />
<br />
测了下，中文语速慢还可以，语速快有时候跟不上，英文口型效果对的很不错。<br />
<br />
感兴趣的可以体验下，记得交作业：<a href="https://huggingface.co/spaces/fffiloni/dreamtalk">huggingface.co/spaces/fffilo…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1736627340623692177#m">nitter.cz/xiaohuggg/status/1736627340623692177#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDMwODg3MDYzMTAwNjYxNzYvcHUvaW1nL3YxYlpkUmI2aTN1bmhpdjguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743085420320313528#m</id>
            <title>生成式AI搜索引擎 Perplexity 宣布获得7360万美元B轮融资，估值达5.2亿美元。

Perplexity公布了一些数据：

- Perplexity 的月活跃用户增长到了1000万
- 2023年，Perplexity处理了超过5亿次查询
- 超过100万人安装了Perplexity的移动应用
- 包括此轮融资在内，Perplexity总共筹集了1亿美元

机构风险投资合伙人（IVP）领投了B轮融资。此轮融资包括 NEA、Elad Gil、Nat Friedman、Databricks、NVIDIA、杰夫·贝索斯（通过贝索斯探险基金）等...

这轮融资对Perplexity来说是一个重要的里程碑，凸显了其在AI原生搜索领域日益增长的影响力

详细：https://blog.perplexity.ai/blog/perplexity-raises-series-b-funding-round</title>
            <link>https://nitter.cz/xiaohuggg/status/1743085420320313528#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743085420320313528#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 01:42:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>生成式AI搜索引擎 Perplexity 宣布获得7360万美元B轮融资，估值达5.2亿美元。<br />
<br />
Perplexity公布了一些数据：<br />
<br />
- Perplexity 的月活跃用户增长到了1000万<br />
- 2023年，Perplexity处理了超过5亿次查询<br />
- 超过100万人安装了Perplexity的移动应用<br />
- 包括此轮融资在内，Perplexity总共筹集了1亿美元<br />
<br />
机构风险投资合伙人（IVP）领投了B轮融资。此轮融资包括 NEA、Elad Gil、Nat Friedman、Databricks、NVIDIA、杰夫·贝索斯（通过贝索斯探险基金）等...<br />
<br />
这轮融资对Perplexity来说是一个重要的里程碑，凸显了其在AI原生搜索领域日益增长的影响力<br />
<br />
详细：<a href="https://blog.perplexity.ai/blog/perplexity-raises-series-b-funding-round">blog.perplexity.ai/blog/perp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDMwODUzNjI0NzI0OTcxNTIvcHUvaW1nL2dhLUlLX2JQWWtVelpCZE4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743077527881679241#m</id>
            <title>Mobile ALOHA能干的事情很多：

- 洗衣服 👔 👖
- 自充电 ⚡️
- 使用真空吸尘器
- 水生植物 🌳
- 装载和卸载洗碗机
- 使用咖啡机 ☕️
- 从冰箱中取出饮料并打开啤酒 🍺
- 打开门 🚪
- 和宠物一起玩 🐱
- 扔掉垃圾
- 打开/关闭灯 💡</title>
            <link>https://nitter.cz/xiaohuggg/status/1743077527881679241#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743077527881679241#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 01:10:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mobile ALOHA能干的事情很多：<br />
<br />
- 洗衣服 👔 👖<br />
- 自充电 ⚡️<br />
- 使用真空吸尘器<br />
- 水生植物 🌳<br />
- 装载和卸载洗碗机<br />
- 使用咖啡机 ☕️<br />
- 从冰箱中取出饮料并打开啤酒 🍺<br />
- 打开门 🚪<br />
- 和宠物一起玩 🐱<br />
- 扔掉垃圾<br />
- 打开/关闭灯 💡</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1742719653536006621#m">nitter.cz/xiaohuggg/status/1742719653536006621#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI5NzE4NzUyOTMyOTA1MDAvcHUvaW1nL0toZXpkMzMyZVBVWnhENEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743070998344442065#m</id>
            <title>GPT Store来了

现在可以验证提交申请了

👌</title>
            <link>https://nitter.cz/xiaohuggg/status/1743070998344442065#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743070998344442065#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jan 2024 00:44:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT Store来了<br />
<br />
现在可以验证提交申请了<br />
<br />
👌</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RDaDA0RWFNQUFsRGVpLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742927470570143797#m</id>
            <title>OpenAI妥协 准备向媒体购买内容训练AI

The Information 报道，两名与OpenAI 谈判的媒体高管透露，OpenAI已经向一些媒体公司开出了每年 100 万-500 万美元，以获得将新闻内容用于训练自家大语言模型的授权许可。

报道称，这样的数目哪怕是对于一些小型出版商来说都是“很小”的，OpenAI 可能很难达成交易。

目前 OpenAI 正在与十几家出版商进行谈判，

一位高管同时也称，苹果公司当前试图在生成式 AI 领域赶超 OpenAI 和谷歌，也在努力与出版商达成协议。

报道引述其他熟知内情的人士消息称，苹果的出价比 OpenAI 更高，同时也希望获得的内容使用权能够比 OpenAI 更广泛。他表示，苹果希望能够以公司认为必要的“任何方式”将内容运用在未来的 AI 产品上。

OpenAI希望达成类似于此前与新闻出版巨头 Axel Springer 达成的协议。OpenAI 表示，该公司尊重内容创作者和所有者的权利，并正在努力确保创作者们受益于 AI 技术和新的收入模式。

去年 12 月，Axel Springer 与 OpenAI 达成了一项史无前例的协议：允许 ChatGPT 对来自 Politico 和 Business Insider 等媒体的新闻文章进行总结摘要。</title>
            <link>https://nitter.cz/xiaohuggg/status/1742927470570143797#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742927470570143797#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 15:14:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI妥协 准备向媒体购买内容训练AI<br />
<br />
The Information 报道，两名与OpenAI 谈判的媒体高管透露，OpenAI已经向一些媒体公司开出了每年 100 万-500 万美元，以获得将新闻内容用于训练自家大语言模型的授权许可。<br />
<br />
报道称，这样的数目哪怕是对于一些小型出版商来说都是“很小”的，OpenAI 可能很难达成交易。<br />
<br />
目前 OpenAI 正在与十几家出版商进行谈判，<br />
<br />
一位高管同时也称，苹果公司当前试图在生成式 AI 领域赶超 OpenAI 和谷歌，也在努力与出版商达成协议。<br />
<br />
报道引述其他熟知内情的人士消息称，苹果的出价比 OpenAI 更高，同时也希望获得的内容使用权能够比 OpenAI 更广泛。他表示，苹果希望能够以公司认为必要的“任何方式”将内容运用在未来的 AI 产品上。<br />
<br />
OpenAI希望达成类似于此前与新闻出版巨头 Axel Springer 达成的协议。OpenAI 表示，该公司尊重内容创作者和所有者的权利，并正在努力确保创作者们受益于 AI 技术和新的收入模式。<br />
<br />
去年 12 月，Axel Springer 与 OpenAI 达成了一项史无前例的协议：允许 ChatGPT 对来自 Politico 和 Business Insider 等媒体的新闻文章进行总结摘要。</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1740008017448559006#m">nitter.cz/xiaohuggg/status/1740008017448559006#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RBZlNaNWFJQUFaQW5vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742919805987078410#m</id>
            <title>开通星球了

准备割韭菜了🔪你们小心点！

🔔 http://Xiaohu.AI日报「1月4日」汇总
✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1742919805987078410#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742919805987078410#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 14:43:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开通星球了<br />
<br />
准备割韭菜了🔪你们小心点！<br />
<br />
🔔 <a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「1月4日」汇总<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RBWVVYX2JRQUFQYVZYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742839505412137338#m</id>
            <title>兄弟们炸裂了

Meta AI又发布了一个炸裂的东西：从音频生成全身逼真的虚拟人物形象。

它可以从多人对话中语音中生成与对话相对应的逼真面部表情、完整身体和手势动作。

这些生成的虚拟人物不仅在视觉上很逼真，而且能够准确地反映出对话中的手势和表情细节，如指点、手腕抖动、耸肩、微笑、嘲笑等。

工作原理：

该项目结合了向量量化的样本多样性和通过扩散获得的高频细节的优势，以生成更具动态性和表现力的动作。

1、数据集捕获：首先捕获了一组丰富的双人对话数据集，这些数据集允许进行逼真的重建。

2、运动模型构建：项目构建了一个包括面部运动模型、引导姿势预测器和身体运动模型的复合运动模型。

3、面部运动生成：使用预训练的唇部回归器处理音频，提取面部运动相关的特征。
利用条件扩散模型根据这些特征生成面部运动。

4、身体运动生成：以音频为输入，自回归地输出每秒1帧的向量量化（VQ）引导姿势。将音频和引导姿势一起输入到扩散模型中，以30帧/秒的速度生成高频身体运动。

5、虚拟人物渲染：将生成的面部和身体运动传入训练好的虚拟人物渲染器，生成逼真的虚拟人物。

6、结果展示：最终展示的是根据音频生成的全身逼真虚拟人物，这些虚拟人物能够表现出对话中的细微表情和手势动作。

项目及演示：https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/
论文：https://arxiv.org/pdf/2401.01885.pdf
GitHub：https://github.com/facebookresearch/audio2photoreal/
Demo：https://colab.research.google.com/drive/1lnX3d-3T3LaO3nlN6R8s6pPvVNAk5mdK?usp=sharing</title>
            <link>https://nitter.cz/xiaohuggg/status/1742839505412137338#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742839505412137338#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 09:24:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们炸裂了<br />
<br />
Meta AI又发布了一个炸裂的东西：从音频生成全身逼真的虚拟人物形象。<br />
<br />
它可以从多人对话中语音中生成与对话相对应的逼真面部表情、完整身体和手势动作。<br />
<br />
这些生成的虚拟人物不仅在视觉上很逼真，而且能够准确地反映出对话中的手势和表情细节，如指点、手腕抖动、耸肩、微笑、嘲笑等。<br />
<br />
工作原理：<br />
<br />
该项目结合了向量量化的样本多样性和通过扩散获得的高频细节的优势，以生成更具动态性和表现力的动作。<br />
<br />
1、数据集捕获：首先捕获了一组丰富的双人对话数据集，这些数据集允许进行逼真的重建。<br />
<br />
2、运动模型构建：项目构建了一个包括面部运动模型、引导姿势预测器和身体运动模型的复合运动模型。<br />
<br />
3、面部运动生成：使用预训练的唇部回归器处理音频，提取面部运动相关的特征。<br />
利用条件扩散模型根据这些特征生成面部运动。<br />
<br />
4、身体运动生成：以音频为输入，自回归地输出每秒1帧的向量量化（VQ）引导姿势。将音频和引导姿势一起输入到扩散模型中，以30帧/秒的速度生成高频身体运动。<br />
<br />
5、虚拟人物渲染：将生成的面部和身体运动传入训练好的虚拟人物渲染器，生成逼真的虚拟人物。<br />
<br />
6、结果展示：最终展示的是根据音频生成的全身逼真虚拟人物，这些虚拟人物能够表现出对话中的细微表情和手势动作。<br />
<br />
项目及演示：<a href="https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/">people.eecs.berkeley.edu/~ev…</a><br />
论文：<a href="https://arxiv.org/pdf/2401.01885.pdf">arxiv.org/pdf/2401.01885.pdf</a><br />
GitHub：<a href="https://github.com/facebookresearch/audio2photoreal/">github.com/facebookresearch/…</a><br />
Demo：<a href="https://colab.research.google.com/drive/1lnX3d-3T3LaO3nlN6R8s6pPvVNAk5mdK?usp=sharing">colab.research.google.com/dr…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI4MzQxMDEyOTA0ODM3MTIvcHUvaW1nLzJTa3l3blA5SU1BTUdTTEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742827196434268231#m</id>
            <title>Apache Answer：一个开源的问答平台，可以作为社区论坛、帮助中心和知识管理平台

主要功能：

- 问答社区：帮助成员解答问题，提升社区参与度
- 内容管理：可以使用标签对问题和内容进行分类
- 搜索功能：支持搜索，以快速找到所需答案
- 插件拓展：支持通过插件和其他服务来拓展功能

GitHub：https://github.com/apache/incubator-answer
官网：https://answer.apache.org/zh-CN/</title>
            <link>https://nitter.cz/xiaohuggg/status/1742827196434268231#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742827196434268231#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 08:35:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Apache Answer：一个开源的问答平台，可以作为社区论坛、帮助中心和知识管理平台<br />
<br />
主要功能：<br />
<br />
- 问答社区：帮助成员解答问题，提升社区参与度<br />
- 内容管理：可以使用标签对问题和内容进行分类<br />
- 搜索功能：支持搜索，以快速找到所需答案<br />
- 插件拓展：支持通过插件和其他服务来拓展功能<br />
<br />
GitHub：<a href="https://github.com/apache/incubator-answer">github.com/apache/incubator-…</a><br />
官网：<a href="https://answer.apache.org/zh-CN/">answer.apache.org/zh-CN/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI4MjYwMDY0MjYzMzcyODAvcHUvaW1nL0FrbExTTXlwcndPTFJlWVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742779985012953468#m</id>
            <title>CoMoSVC：一种高效、高质量的歌声转换方法

它可以将一个人的歌声转换成另一个人的歌声。同时能够保持了声音的自然度和真实感。

最牛P的是CoMoSVC实现了一步采样。意思是它可以在单次操作中即可完成声音的转换，大大加快了处理速度。

该项目由香港大学和微软亚洲研究员开发，CoMoSVC 在高质量音频转换和快速处理速度之间提供了平衡，是SVC领域的重大进步。

CoMoSVC实现歌声转换的过程涉及几个关键步骤：

1、基于扩散的教师模型设计：首先，CoMoSVC设计了一个专门针对歌声转换的基于扩散的教师模型。这个模型通过学习大量的歌声数据，能够理解和模仿不同歌手的声音特征。

2、学生模型的提炼：接着，CoMoSVC利用自我一致性属性进一步提炼出一个学生模型。这个过程涉及从教师模型中提取关键信息，并简化模型结构，以便于快速有效地进行声音转换。

3、一步采样过程：不同于传统的迭代采样过程，CoMoSVC实现了一步采样。这意味着它可以在单次操作中完成声音的转换，大大加快了处理速度。

4、音频质量和速度的平衡：CoMoSVC在保持高音质转换的同时，优化了推理速度。这是通过精心设计的模型架构和算法优化实现的，确保转换后的音频既自然又忠实于目标歌手的风格。

在传统的基于扩散的声音转换模型中，通常需要多个迭代步骤来逐渐生成目标音频，这个过程可能既复杂又耗时。而CoMoSVC通过其创新的模型设计和算法优化，实现了快速且高效的一步采样，这大大减少了转换所需的时间，同时保持了音频质量。

这种一步采样的方法使CoMoSVC在实际应用中更加实用，特别是在需要快速处理大量数据的场景，如实时音频处理、音乐制作等领域。

项目及演示：https://comosvc.github.io/
论文：https://arxiv.org/pdf/2401.01792.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1742779985012953468#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742779985012953468#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 05:28:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>CoMoSVC：一种高效、高质量的歌声转换方法<br />
<br />
它可以将一个人的歌声转换成另一个人的歌声。同时能够保持了声音的自然度和真实感。<br />
<br />
最牛P的是CoMoSVC实现了一步采样。意思是它可以在单次操作中即可完成声音的转换，大大加快了处理速度。<br />
<br />
该项目由香港大学和微软亚洲研究员开发，CoMoSVC 在高质量音频转换和快速处理速度之间提供了平衡，是SVC领域的重大进步。<br />
<br />
CoMoSVC实现歌声转换的过程涉及几个关键步骤：<br />
<br />
1、基于扩散的教师模型设计：首先，CoMoSVC设计了一个专门针对歌声转换的基于扩散的教师模型。这个模型通过学习大量的歌声数据，能够理解和模仿不同歌手的声音特征。<br />
<br />
2、学生模型的提炼：接着，CoMoSVC利用自我一致性属性进一步提炼出一个学生模型。这个过程涉及从教师模型中提取关键信息，并简化模型结构，以便于快速有效地进行声音转换。<br />
<br />
3、一步采样过程：不同于传统的迭代采样过程，CoMoSVC实现了一步采样。这意味着它可以在单次操作中完成声音的转换，大大加快了处理速度。<br />
<br />
4、音频质量和速度的平衡：CoMoSVC在保持高音质转换的同时，优化了推理速度。这是通过精心设计的模型架构和算法优化实现的，确保转换后的音频既自然又忠实于目标歌手的风格。<br />
<br />
在传统的基于扩散的声音转换模型中，通常需要多个迭代步骤来逐渐生成目标音频，这个过程可能既复杂又耗时。而CoMoSVC通过其创新的模型设计和算法优化，实现了快速且高效的一步采样，这大大减少了转换所需的时间，同时保持了音频质量。<br />
<br />
这种一步采样的方法使CoMoSVC在实际应用中更加实用，特别是在需要快速处理大量数据的场景，如实时音频处理、音乐制作等领域。<br />
<br />
项目及演示：<a href="https://comosvc.github.io/">comosvc.github.io/</a><br />
论文：<a href="https://arxiv.org/pdf/2401.01792.pdf">arxiv.org/pdf/2401.01792.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3NzkwNTA3NjUyMzgyNzIvcHUvaW1nL09NYTQ3XzhXNklFcHZTUl8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742768663307190274#m</id>
            <title>SeeAct：一个基于GPT-4V通用网络代理

它可以在多种不同网站上识别网页上各种元素，执行各种不同的任务，

例如，在苹果官网上比较iPhone 15 Pro Max和iPhone 13 Pro Max的区别，并给出购买建议。

在旅游网站上搜索航班信息等。

SeeAct展示了从推测性规划、网页内容推理到错误自我纠正等多种能力。

SeeAct的创新之处在于它结合了多模态模型的视觉感知能力和自然语言处理能力，使其能够理解和操作网页内容。

SeeAct的主要能力：

1、执行网站任务：SeeAct能够在任何网站上执行特定任务，例如在电商网站上比较产品、在旅游网站上搜索航班信息等。

2、动作生成：模拟人类浏览网页，分析任务和之前的动作，生成动作描述。它首先进行动作生成，即产生完成任务所需每一步的文本描述。例如，如果任务是在苹果官网上比较两款iPhone，SeeAct会生成如“导航到iPhone分类”这样的动作描述。

3、动作定位：接着进行动作定位，识别网页上与动作描述相对应的HTML元素和操作。例如，它会找到并识别“iPhone”按钮，并执行点击操作。

4、多种能力展示：SeeAct展示了多种能力，包括推测性规划（预测接下来的步骤）、网页内容推理（理解网页上的信息）和错误自我纠正（识别并纠正之前的错误）。

5、适用于多种网站：SeeAct不仅限于特定类型的网站，它能够适应并在多种不同的网站上执行任务。

项目及演示：https://osu-nlp-group.github.io/SeeAct/
论文：https://arxiv.org/abs/2401.01614
GitHub：https://github.com/OSU-NLP-Group/SeeAct</title>
            <link>https://nitter.cz/xiaohuggg/status/1742768663307190274#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742768663307190274#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 04:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SeeAct：一个基于GPT-4V通用网络代理<br />
<br />
它可以在多种不同网站上识别网页上各种元素，执行各种不同的任务，<br />
<br />
例如，在苹果官网上比较iPhone 15 Pro Max和iPhone 13 Pro Max的区别，并给出购买建议。<br />
<br />
在旅游网站上搜索航班信息等。<br />
<br />
SeeAct展示了从推测性规划、网页内容推理到错误自我纠正等多种能力。<br />
<br />
SeeAct的创新之处在于它结合了多模态模型的视觉感知能力和自然语言处理能力，使其能够理解和操作网页内容。<br />
<br />
SeeAct的主要能力：<br />
<br />
1、执行网站任务：SeeAct能够在任何网站上执行特定任务，例如在电商网站上比较产品、在旅游网站上搜索航班信息等。<br />
<br />
2、动作生成：模拟人类浏览网页，分析任务和之前的动作，生成动作描述。它首先进行动作生成，即产生完成任务所需每一步的文本描述。例如，如果任务是在苹果官网上比较两款iPhone，SeeAct会生成如“导航到iPhone分类”这样的动作描述。<br />
<br />
3、动作定位：接着进行动作定位，识别网页上与动作描述相对应的HTML元素和操作。例如，它会找到并识别“iPhone”按钮，并执行点击操作。<br />
<br />
4、多种能力展示：SeeAct展示了多种能力，包括推测性规划（预测接下来的步骤）、网页内容推理（理解网页上的信息）和错误自我纠正（识别并纠正之前的错误）。<br />
<br />
5、适用于多种网站：SeeAct不仅限于特定类型的网站，它能够适应并在多种不同的网站上执行任务。<br />
<br />
项目及演示：<a href="https://osu-nlp-group.github.io/SeeAct/">osu-nlp-group.github.io/SeeA…</a><br />
论文：<a href="https://arxiv.org/abs/2401.01614">arxiv.org/abs/2401.01614</a><br />
GitHub：<a href="https://github.com/OSU-NLP-Group/SeeAct">github.com/OSU-NLP-Group/See…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3NTc4NDAyNDU0NDQ2MDgvcHUvaW1nL0xhdGFraGYzUkY2RnJTdzcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742749156589162708#m</id>
            <title>R to @xiaohuggg: 效果展示</title>
            <link>https://nitter.cz/xiaohuggg/status/1742749156589162708#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742749156589162708#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 03:25:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>效果展示</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5ODdQemFZQUEwdVE2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5OUIxaGFvQUVMWXNkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742749153267257699#m</id>
            <title>AnyText：解决图像生成中，文字无法和图融合、变形、乱码的问题

该项目由阿里巴巴开发，AnyText支持在图像中生成和编辑多种语言的文本，使其与背景无缝融合。

该模型还解决了合成文本中模糊、不可读或错误字符的问题。

AnyText可以与现有的扩散模型集成，用于准确渲染或编辑文本。

AnyText能够在图像中高度精确地生成或编辑文本。例如，它可以在一张照片中添加逼真的文本标签，或者更改图像中现有文本的内容，同时保持自然的外观和与背景的一致性。

能够处理和生成多种不同语言环境下的文本。它可以生成包括中文、英文、日文、韩文等在内的多种语言的文本。

AnyText的组成部分：

1、辅助潜在模块

功能：这个模块负责生成图像中文本的潜在特征。它使用文本字形（即文本的视觉表示）、文本在图像中的位置和遮罩图像（可能用于指示文本应该出现的区域）作为输入。

作用：通过分析这些输入，辅助潜在模块能够理解文本应该如何在特定图像中呈现，包括文本的样式、大小和位置。这对于在图像中生成新文本或编辑现有文本至关重要。

2、文本嵌入模块

功能：这个模块结合了OCR（光学字符识别）模型编码的笔画数据和图像标题嵌入。OCR模型提取图像中现有文本的特征，而图像标题嵌入则提供了图像内容的语义理解。

作用：文本嵌入模块将这些信息融合在一起，生成与图像背景和上下文无缝融合的文本。这意味着生成的文本不仅在视觉上与背景匹配，而且在语义上与图像的整体主题一致。

AnyText还提供的一个大规模多语言文本图像数据集：

AnyWord-3M数据集

数据集规模：包含约303万个图像-文本对，涵盖多种语言。

文本行和字符统计：中文：约290万行文本，包含约1509万个字符。

英文：约627万行文本，包含约635万个字符。

其他语言：包括日文、韩文、阿拉伯语、孟加拉语和印地语，共约1.17万行文本，约5.95万个字符。

OCR注释：数据集中的图像-文本对都带有OCR（光学字符识别）注释，这有助于训练和评估文本生成模型。
多语言支持：数据集支持多种语言，包括中文、英文、日文、韩文、阿拉伯语、孟加拉语和印地语，使其适用于多语言视觉文本生成和编辑任务。

GitHub：https://github.com/tyxsspa/AnyText
论文：https://arxiv.org/abs/2311.03054
ModelScope 在线演示：http://modelscope.cn/studios/damo/studio_anytext
HuggingFace 在线演示：http://huggingface.co/spaces/modelscope/AnyText</title>
            <link>https://nitter.cz/xiaohuggg/status/1742749153267257699#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742749153267257699#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 03:25:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnyText：解决图像生成中，文字无法和图融合、变形、乱码的问题<br />
<br />
该项目由阿里巴巴开发，AnyText支持在图像中生成和编辑多种语言的文本，使其与背景无缝融合。<br />
<br />
该模型还解决了合成文本中模糊、不可读或错误字符的问题。<br />
<br />
AnyText可以与现有的扩散模型集成，用于准确渲染或编辑文本。<br />
<br />
AnyText能够在图像中高度精确地生成或编辑文本。例如，它可以在一张照片中添加逼真的文本标签，或者更改图像中现有文本的内容，同时保持自然的外观和与背景的一致性。<br />
<br />
能够处理和生成多种不同语言环境下的文本。它可以生成包括中文、英文、日文、韩文等在内的多种语言的文本。<br />
<br />
AnyText的组成部分：<br />
<br />
1、辅助潜在模块<br />
<br />
功能：这个模块负责生成图像中文本的潜在特征。它使用文本字形（即文本的视觉表示）、文本在图像中的位置和遮罩图像（可能用于指示文本应该出现的区域）作为输入。<br />
<br />
作用：通过分析这些输入，辅助潜在模块能够理解文本应该如何在特定图像中呈现，包括文本的样式、大小和位置。这对于在图像中生成新文本或编辑现有文本至关重要。<br />
<br />
2、文本嵌入模块<br />
<br />
功能：这个模块结合了OCR（光学字符识别）模型编码的笔画数据和图像标题嵌入。OCR模型提取图像中现有文本的特征，而图像标题嵌入则提供了图像内容的语义理解。<br />
<br />
作用：文本嵌入模块将这些信息融合在一起，生成与图像背景和上下文无缝融合的文本。这意味着生成的文本不仅在视觉上与背景匹配，而且在语义上与图像的整体主题一致。<br />
<br />
AnyText还提供的一个大规模多语言文本图像数据集：<br />
<br />
AnyWord-3M数据集<br />
<br />
数据集规模：包含约303万个图像-文本对，涵盖多种语言。<br />
<br />
文本行和字符统计：中文：约290万行文本，包含约1509万个字符。<br />
<br />
英文：约627万行文本，包含约635万个字符。<br />
<br />
其他语言：包括日文、韩文、阿拉伯语、孟加拉语和印地语，共约1.17万行文本，约5.95万个字符。<br />
<br />
OCR注释：数据集中的图像-文本对都带有OCR（光学字符识别）注释，这有助于训练和评估文本生成模型。<br />
多语言支持：数据集支持多种语言，包括中文、英文、日文、韩文、阿拉伯语、孟加拉语和印地语，使其适用于多语言视觉文本生成和编辑任务。<br />
<br />
GitHub：<a href="https://github.com/tyxsspa/AnyText">github.com/tyxsspa/AnyText</a><br />
论文：<a href="https://arxiv.org/abs/2311.03054">arxiv.org/abs/2311.03054</a><br />
ModelScope 在线演示：<a href="http://modelscope.cn/studios/damo/studio_anytext">modelscope.cn/studios/damo/s…</a><br />
HuggingFace 在线演示：<a href="http://huggingface.co/spaces/modelscope/AnyText">huggingface.co/spaces/models…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5NzdoOGEwQUFHTG1nLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742736494509588846#m</id>
            <title>R to @xiaohuggg: 视频演示里生成的照片 

嘿嘿，我感觉效果还是很不错的</title>
            <link>https://nitter.cz/xiaohuggg/status/1742736494509588846#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742736494509588846#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 02:35:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频演示里生成的照片 <br />
<br />
嘿嘿，我感觉效果还是很不错的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSRmIwQUE2M1VTLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSRmJJQUE3alVaLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSSGFzQUFIa3h0LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSRWJZQUFFZHFLLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742736491292606866#m</id>
            <title>IP-Adapter-FaceID：上传自己照片 分分钟克隆一个自己

该模型利用面部识别模型的面部ID嵌入，可以更准确地捕捉和再现特定人物的面部特征。

结合文本描述生成可以生成高度个性化且与原始面部特征一致的图像。

意思就是你只要上传几张自己的照片，就能生成你在各种场景下的照片，克隆你的脸。

模型地址：https://huggingface.co/h94/IP-Adapter-FaceID
在线体验：https://huggingface.co/spaces/multimodalart/Ip-Adapter-FaceID</title>
            <link>https://nitter.cz/xiaohuggg/status/1742736491292606866#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742736491292606866#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 02:35:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>IP-Adapter-FaceID：上传自己照片 分分钟克隆一个自己<br />
<br />
该模型利用面部识别模型的面部ID嵌入，可以更准确地捕捉和再现特定人物的面部特征。<br />
<br />
结合文本描述生成可以生成高度个性化且与原始面部特征一致的图像。<br />
<br />
意思就是你只要上传几张自己的照片，就能生成你在各种场景下的照片，克隆你的脸。<br />
<br />
模型地址：<a href="https://huggingface.co/h94/IP-Adapter-FaceID">huggingface.co/h94/IP-Adapte…</a><br />
在线体验：<a href="https://huggingface.co/spaces/multimodalart/Ip-Adapter-FaceID">huggingface.co/spaces/multim…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3MzYyOTI3NzE5NTg3ODQvcHUvaW1nLzV2T0pfb2FUd1BNZklJdDEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742730134753550369#m</id>
            <title>兄弟们 SEO过时了，现在是GEO了 😂

随着 Bard &amp; Perplexity 等基于 LLM 的搜索引擎的崛起，机器人直接输出答案，这让内容创建者通过SEO来改进他们的网站，已经逐渐变得越来越难。

普林斯顿大学、艾伦科技研究所提出了GEO的概念：生成引擎优化。

他们提出了一个专门针对生成引擎的印象度量标准！

实验表明，使用GEO的简单策略可以在商业生成引擎上显著提高内容的可见性，提升幅度高达40%。

生成引擎与传统搜索引擎的区别：

- 传统搜索引擎通常提供一个链接列表，直接指向相关网页。

- 生成引擎则使用大型语言模型（LLM）来生成更丰富、综合性的回答，这些回答可能直接包含了用户查询的答案，而不仅仅是链接。

GEO的印象度量标准（Customized Visibility Metrics:）：

1、内容可见性：衡量内容在生成引擎回答中出现的频率和显著性。例如，一个网站的信息是否经常被引擎用来构建回答。

2、信息准确性：评估生成引擎提供的信息与原始内容的一致性。这对于确保生成引擎正确理解和呈现网站内容非常重要。

3、用户参与度：测量用户与生成引擎提供的内容的互动程度。这可能包括用户对生成回答的点击率、阅读时间等。

4、内容影响力：评估内容在生成引擎回答中的权威性和影响力。例如，内容是否被视为某个领域的权威来源。

通过这些专门设计的度量标准，GEO帮助内容创作者更好地理解他们的内容在生成引擎中的表现，并提供了优化这些内容以提高其在生成引擎中可见性和有效性的策略。

GEO的原理：

1、多模态理解：生成引擎不仅处理文本信息，还可能结合视觉和空间布局等其他模态的信息。GEO的原理包括理解这些多模态数据的处理方式。

2、内容综合性：与传统搜索引擎不同，生成引擎倾向于提供更加综合和完整的回答，而不是简单的链接。GEO的原理在于理解如何使内容更适合这种综合性的呈现。

3、语义理解：生成引擎使用先进的语言模型，能够深入理解内容的语义。GEO的原理包括优化内容以提高其在语义层面的清晰度和相关性。

GEO的策略：

1、结构化内容：优化网站和内容的结构，使其更容易被生成引擎解析和引用。这可能包括使用清晰的标题、子标题和元标签。

2、关键信息突出：确保重要信息（如产品特点、服务优势）容易被找到和理解，以便生成引擎可以有效地提取和使用这些信息。

3、增强语义相关性：使用关键词和短语来提高内容的语义相关性，使其更符合目标受众的搜索意图。

4、利用GEO度量标准：使用GEO提供的专门度量标准来评估和优化内容在生成引擎中的表现。

5、持续监测和调整：定期监测内容在生成引擎中的表现，并根据反馈进行调整。这可能包括分析用户行为数据和生成引擎的反馈。

6、适应生成引擎的变化：由于生成引擎和大型语言模型不断进化，GEO策略需要灵活适应这些变化，持续更新优化方法。

通过实施这些策略，GEO帮助内容创作者提高他们的网站和内容在新一代搜索引擎中的可见性和有效性，从而更好地满足用户的搜索需求。

全面的基准测试：GEO-BENCH

GEO引入了一个名为GEO-BENCH的多样化基准测试，包含10,000个查询，用于评估和比较不同优化方法的效果。这是一个针对 GEO 查询定制的基准，用于评估不同的策略。

10,000个查询：GEO-BENCH包含10,000个不同的查询，这些查询覆盖了多个领域、难度级别和类别。这样的多样性确保了基准测试能够全面评估不同类型的内容和优化策略。

数据集构成：这个基准测试由多个来源的数据集组成，包括MS Macro、ORCAS-1、Natural Questions等，这些数据集代表了不同类型的用户查询和搜索场景。

训练集和测试集：GEO-BENCH包括8,000个查询的训练集和各1,000个查询的验证集和测试集，使得内容创作者和研究人员能够在标准化的环境中训练和测试他们的优化策略。

公共排行榜：GEO-BENCH提供了一个公共排行榜，定期更新以展示最新的测试结果，从而促进不同方法之间的健康竞争和进步。

项目地址：https://generative-engines.com/GEO/
论文：https://arxiv.org/pdf/2311.09735.pdf
GitHub：https://github.com/GEO-optim/GEO
GEO-BENCH：https://huggingface.co/datasets/GEO-Optim/geo-bench</title>
            <link>https://nitter.cz/xiaohuggg/status/1742730134753550369#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742730134753550369#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 02:10:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 SEO过时了，现在是GEO了 😂<br />
<br />
随着 Bard & Perplexity 等基于 LLM 的搜索引擎的崛起，机器人直接输出答案，这让内容创建者通过SEO来改进他们的网站，已经逐渐变得越来越难。<br />
<br />
普林斯顿大学、艾伦科技研究所提出了GEO的概念：生成引擎优化。<br />
<br />
他们提出了一个专门针对生成引擎的印象度量标准！<br />
<br />
实验表明，使用GEO的简单策略可以在商业生成引擎上显著提高内容的可见性，提升幅度高达40%。<br />
<br />
生成引擎与传统搜索引擎的区别：<br />
<br />
- 传统搜索引擎通常提供一个链接列表，直接指向相关网页。<br />
<br />
- 生成引擎则使用大型语言模型（LLM）来生成更丰富、综合性的回答，这些回答可能直接包含了用户查询的答案，而不仅仅是链接。<br />
<br />
GEO的印象度量标准（Customized Visibility Metrics:）：<br />
<br />
1、内容可见性：衡量内容在生成引擎回答中出现的频率和显著性。例如，一个网站的信息是否经常被引擎用来构建回答。<br />
<br />
2、信息准确性：评估生成引擎提供的信息与原始内容的一致性。这对于确保生成引擎正确理解和呈现网站内容非常重要。<br />
<br />
3、用户参与度：测量用户与生成引擎提供的内容的互动程度。这可能包括用户对生成回答的点击率、阅读时间等。<br />
<br />
4、内容影响力：评估内容在生成引擎回答中的权威性和影响力。例如，内容是否被视为某个领域的权威来源。<br />
<br />
通过这些专门设计的度量标准，GEO帮助内容创作者更好地理解他们的内容在生成引擎中的表现，并提供了优化这些内容以提高其在生成引擎中可见性和有效性的策略。<br />
<br />
GEO的原理：<br />
<br />
1、多模态理解：生成引擎不仅处理文本信息，还可能结合视觉和空间布局等其他模态的信息。GEO的原理包括理解这些多模态数据的处理方式。<br />
<br />
2、内容综合性：与传统搜索引擎不同，生成引擎倾向于提供更加综合和完整的回答，而不是简单的链接。GEO的原理在于理解如何使内容更适合这种综合性的呈现。<br />
<br />
3、语义理解：生成引擎使用先进的语言模型，能够深入理解内容的语义。GEO的原理包括优化内容以提高其在语义层面的清晰度和相关性。<br />
<br />
GEO的策略：<br />
<br />
1、结构化内容：优化网站和内容的结构，使其更容易被生成引擎解析和引用。这可能包括使用清晰的标题、子标题和元标签。<br />
<br />
2、关键信息突出：确保重要信息（如产品特点、服务优势）容易被找到和理解，以便生成引擎可以有效地提取和使用这些信息。<br />
<br />
3、增强语义相关性：使用关键词和短语来提高内容的语义相关性，使其更符合目标受众的搜索意图。<br />
<br />
4、利用GEO度量标准：使用GEO提供的专门度量标准来评估和优化内容在生成引擎中的表现。<br />
<br />
5、持续监测和调整：定期监测内容在生成引擎中的表现，并根据反馈进行调整。这可能包括分析用户行为数据和生成引擎的反馈。<br />
<br />
6、适应生成引擎的变化：由于生成引擎和大型语言模型不断进化，GEO策略需要灵活适应这些变化，持续更新优化方法。<br />
<br />
通过实施这些策略，GEO帮助内容创作者提高他们的网站和内容在新一代搜索引擎中的可见性和有效性，从而更好地满足用户的搜索需求。<br />
<br />
全面的基准测试：GEO-BENCH<br />
<br />
GEO引入了一个名为GEO-BENCH的多样化基准测试，包含10,000个查询，用于评估和比较不同优化方法的效果。这是一个针对 GEO 查询定制的基准，用于评估不同的策略。<br />
<br />
10,000个查询：GEO-BENCH包含10,000个不同的查询，这些查询覆盖了多个领域、难度级别和类别。这样的多样性确保了基准测试能够全面评估不同类型的内容和优化策略。<br />
<br />
数据集构成：这个基准测试由多个来源的数据集组成，包括MS Macro、ORCAS-1、Natural Questions等，这些数据集代表了不同类型的用户查询和搜索场景。<br />
<br />
训练集和测试集：GEO-BENCH包括8,000个查询的训练集和各1,000个查询的验证集和测试集，使得内容创作者和研究人员能够在标准化的环境中训练和测试他们的优化策略。<br />
<br />
公共排行榜：GEO-BENCH提供了一个公共排行榜，定期更新以展示最新的测试结果，从而促进不同方法之间的健康竞争和进步。<br />
<br />
项目地址：<a href="https://generative-engines.com/GEO/">generative-engines.com/GEO/</a><br />
论文：<a href="https://arxiv.org/pdf/2311.09735.pdf">arxiv.org/pdf/2311.09735.pdf</a><br />
GitHub：<a href="https://github.com/GEO-optim/GEO">github.com/GEO-optim/GEO</a><br />
GEO-BENCH：<a href="https://huggingface.co/datasets/GEO-Optim/geo-bench">huggingface.co/datasets/GEO-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5cFJWdmFZQUFPeHBGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742719780828963272#m</id>
            <title>R to @xiaohuggg: 开发者介绍：</title>
            <link>https://nitter.cz/xiaohuggg/status/1742719780828963272#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742719780828963272#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 01:29:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开发者介绍：</p>
<p><a href="https://nitter.cz/tonyzzhao/status/1742603121682153852#m">nitter.cz/tonyzzhao/status/1742603121682153852#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>