<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724969696217399486#m</id>
            <title>DeepMind和Raspberry Pi Foundation推出一套免费的课程，专门教授11至14岁学生学习基础的人工智能知识。

课程包括教学计划、幻灯片、工作表和视频，让教师能够在中学课堂上进行AI和机器学习的教学。

内容从搜索引擎、社交媒体内容推荐、自动驾驶汽车、面部识别软件到AI聊天机器人和图像生成等。

这些课程专注于与年轻人相关的AI应用，并且设计得易于多种科目的教师使用。

基于这些课程，他们正在设计一个开放式的AI挑战，让年轻人通过开发一个由他们创建的机器学习模型驱动的分类应用程序来获得实际经验。年轻人将学习如何使用他们创建的数据集（包括音频、文本、数字或图像）来训练ML模型，然后在Scratch中编写使用他们训练和测试的模型的应用程序。

课程官网：https://experience-ai.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724969696217399486#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724969696217399486#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Nov 2023 01:56:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind和Raspberry Pi Foundation推出一套免费的课程，专门教授11至14岁学生学习基础的人工智能知识。<br />
<br />
课程包括教学计划、幻灯片、工作表和视频，让教师能够在中学课堂上进行AI和机器学习的教学。<br />
<br />
内容从搜索引擎、社交媒体内容推荐、自动驾驶汽车、面部识别软件到AI聊天机器人和图像生成等。<br />
<br />
这些课程专注于与年轻人相关的AI应用，并且设计得易于多种科目的教师使用。<br />
<br />
基于这些课程，他们正在设计一个开放式的AI挑战，让年轻人通过开发一个由他们创建的机器学习模型驱动的分类应用程序来获得实际经验。年轻人将学习如何使用他们创建的数据集（包括音频、文本、数字或图像）来训练ML模型，然后在Scratch中编写使用他们训练和测试的模型的应用程序。<br />
<br />
课程官网：<a href="https://experience-ai.org/">experience-ai.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9CU3REUWEwQUV2Y2dvLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724961447011328255#m</id>
            <title>中国宣称开发出世界上最快的互联网 不到1秒能传输150部4K电影

该项目名为FITI，其网络传输速度达到1.2T比特每秒，即超过1200千兆比特每秒。

这一速度使得该网络能够在不到一秒的时间内发送150部4K电影。不到半小时的时间内可以传输完Netflix的全球内容库。

该测试网络通过一个跨越3000公里的光纤电缆，连接北京、武汉和广州。基于中国自主研发的下一代互联网核心路由器1.2T超高速IPv6接口和3×400G超高速多光路聚合等关键核心技术。

这一技术的实现超前于行业预期，原本预计到2025年才会达到这样的速度。

这是中国教育和研究网络（Cernet）的一部分，属于中国未来互联网技术基础设施（FITI）项目。FITI高性能主干网的核心节点分布在全国31个省区市35个城市的40所高校，基于3万多公里光纤通信网络，可为各类用户提供未来互联网各种技术试验服务。

该网络使用的所有软件和硬件均为国内生产，展示了中国在路由器、交换机和光纤技术方面的自主研发能力。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724961447011328255#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724961447011328255#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Nov 2023 01:23:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>中国宣称开发出世界上最快的互联网 不到1秒能传输150部4K电影<br />
<br />
该项目名为FITI，其网络传输速度达到1.2T比特每秒，即超过1200千兆比特每秒。<br />
<br />
这一速度使得该网络能够在不到一秒的时间内发送150部4K电影。不到半小时的时间内可以传输完Netflix的全球内容库。<br />
<br />
该测试网络通过一个跨越3000公里的光纤电缆，连接北京、武汉和广州。基于中国自主研发的下一代互联网核心路由器1.2T超高速IPv6接口和3×400G超高速多光路聚合等关键核心技术。<br />
<br />
这一技术的实现超前于行业预期，原本预计到2025年才会达到这样的速度。<br />
<br />
这是中国教育和研究网络（Cernet）的一部分，属于中国未来互联网技术基础设施（FITI）项目。FITI高性能主干网的核心节点分布在全国31个省区市35个城市的40所高校，基于3万多公里光纤通信网络，可为各类用户提供未来互联网各种技术试验服务。<br />
<br />
该网络使用的所有软件和硬件均为国内生产，展示了中国在路由器、交换机和光纤技术方面的自主研发能力。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9CTFFxMGJVQUF4YnBPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724955699602771975#m</id>
            <title>💥ChatGPT Team Plan 应该是准备要上线了

- 在3.5页面，出现了ChatGPT Team Plan的引导

- 支持用户人数从之前泄露的3 个变为了2个  

- 3.5用户现在也可以看到新的Gizmo UI

- 充值通道是关闭的，和暂停Plus注册有关

我感觉针对Plus用户会马上上线，该功能更像是要打击账号共享的情况，你们注意了！

目前共享账号情况很严重，严重影响了OpenAI收入和服务压力，可以预见的是奥特曼可能要下手了，推出ChatGPT Team Plan的同时打击账号共享行为，同时估计又要封一批账号了！

图片内容信息来源 @btibor91</title>
            <link>https://nitter.cz/xiaohuggg/status/1724955699602771975#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724955699602771975#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Nov 2023 01:01:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>💥ChatGPT Team Plan 应该是准备要上线了<br />
<br />
- 在3.5页面，出现了ChatGPT Team Plan的引导<br />
<br />
- 支持用户人数从之前泄露的3 个变为了2个  <br />
<br />
- 3.5用户现在也可以看到新的Gizmo UI<br />
<br />
- 充值通道是关闭的，和暂停Plus注册有关<br />
<br />
我感觉针对Plus用户会马上上线，该功能更像是要打击账号共享的情况，你们注意了！<br />
<br />
目前共享账号情况很严重，严重影响了OpenAI收入和服务压力，可以预见的是奥特曼可能要下手了，推出ChatGPT Team Plan的同时打击账号共享行为，同时估计又要封一批账号了！<br />
<br />
图片内容信息来源 <a href="https://nitter.cz/btibor91" title="Tibor Blaho">@btibor91</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9CRHJ2RmJrQUE2VHRaLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9CRHVtMWJrQUF5MmQ2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724950322442035478#m</id>
            <title>微软推出两款AI芯片 但是不卖

在西雅图的Ignite开发者大会上，微软推出了两款定制的AI芯片，分别是Azure Maia AI芯片和Azure Cobalt CPU。

Azure Maia AI芯片：这款芯片专为运行云端AI工作负载而设计，比如大型语言模型的训练和推理。

Maia芯片将用于支持微软在Azure上的大型AI工作负载，包括与OpenAI的多亿美元合作项目。

Maia芯片采用5纳米TSMC工艺制造，拥有1050亿个晶体管。它支持低于8位的数据类型（MX数据类型），以优化硬件和软件的协同设计，从而加快模型训练和推理时间。

Azure Cobalt CPU：这是一款基于Arm Neoverse CSS设计的128核心芯片，专为微软定制，用于支持Azure上的一般云服务。

Cobalt CPU在性能和功耗管理方面进行了精心设计，包括每个核心和每个虚拟机的性能和功耗控制能力。微软目前正在对其Cobalt CPU进行测试，计划明年向客户提供多种工作负载的虚拟机。

这两款芯片都是微软内部设计的，结合了对整个云服务器堆栈的深度改造，以优化性能、功耗和成本。

Maia芯片是微软设计的第一款完全液冷服务器处理器，旨在实现更高密度的服务器部署和更高效率。

这两款芯片均采用台湾半导体制造公司（TSMC）的5纳米制造工艺。Maia芯片将通过标准以太网网络电缆连接，而不是微软之前为OpenAI构建的超级计算机中使用的更昂贵的Nvidia定制网络技术。

此外，微软还与AMD、Arm、Intel、Meta、Nvidia和Qualcomm等公司合作，标准化AI模型的下一代数据格式。

但是微软表示，不打算出售这些芯片，而是将使用它们来支持自己的订阅软件产品，并作为其 Azure 云计算服务的一部分。

来源：https://www.theverge.com/2023/11/15/23960345/microsoft-cpu-gpu-ai-chips-azure-maia-cobalt-specifications-cloud-infrastructure</title>
            <link>https://nitter.cz/xiaohuggg/status/1724950322442035478#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724950322442035478#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Nov 2023 00:39:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软推出两款AI芯片 但是不卖<br />
<br />
在西雅图的Ignite开发者大会上，微软推出了两款定制的AI芯片，分别是Azure Maia AI芯片和Azure Cobalt CPU。<br />
<br />
Azure Maia AI芯片：这款芯片专为运行云端AI工作负载而设计，比如大型语言模型的训练和推理。<br />
<br />
Maia芯片将用于支持微软在Azure上的大型AI工作负载，包括与OpenAI的多亿美元合作项目。<br />
<br />
Maia芯片采用5纳米TSMC工艺制造，拥有1050亿个晶体管。它支持低于8位的数据类型（MX数据类型），以优化硬件和软件的协同设计，从而加快模型训练和推理时间。<br />
<br />
Azure Cobalt CPU：这是一款基于Arm Neoverse CSS设计的128核心芯片，专为微软定制，用于支持Azure上的一般云服务。<br />
<br />
Cobalt CPU在性能和功耗管理方面进行了精心设计，包括每个核心和每个虚拟机的性能和功耗控制能力。微软目前正在对其Cobalt CPU进行测试，计划明年向客户提供多种工作负载的虚拟机。<br />
<br />
这两款芯片都是微软内部设计的，结合了对整个云服务器堆栈的深度改造，以优化性能、功耗和成本。<br />
<br />
Maia芯片是微软设计的第一款完全液冷服务器处理器，旨在实现更高密度的服务器部署和更高效率。<br />
<br />
这两款芯片均采用台湾半导体制造公司（TSMC）的5纳米制造工艺。Maia芯片将通过标准以太网网络电缆连接，而不是微软之前为OpenAI构建的超级计算机中使用的更昂贵的Nvidia定制网络技术。<br />
<br />
此外，微软还与AMD、Arm、Intel、Meta、Nvidia和Qualcomm等公司合作，标准化AI模型的下一代数据格式。<br />
<br />
但是微软表示，不打算出售这些芯片，而是将使用它们来支持自己的订阅软件产品，并作为其 Azure 云计算服务的一部分。<br />
<br />
来源：<a href="https://www.theverge.com/2023/11/15/23960345/microsoft-cpu-gpu-ai-chips-azure-maia-cobalt-specifications-cloud-infrastructure">theverge.com/2023/11/15/2396…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9CQUwwUmJjQUEwajh1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724804784375607429#m</id>
            <title>财报电话会上，针对美国面向芯片领域的禁令问题，腾讯高层表示，我们的芯片下单比较早，目前库存水平比较高，包括H800型号的芯片库存等。

现有的库存水平可以支持腾讯大模型几代的更新。腾讯的云能力不会因为芯片禁令受到影响。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724804784375607429#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724804784375607429#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 15:01:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>财报电话会上，针对美国面向芯片领域的禁令问题，腾讯高层表示，我们的芯片下单比较早，目前库存水平比较高，包括H800型号的芯片库存等。<br />
<br />
现有的库存水平可以支持腾讯大模型几代的更新。腾讯的云能力不会因为芯片禁令受到影响。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0tOHpud2FvQUFhakpoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724781225355931741#m</id>
            <title>R to @xiaohuggg: 之前发的这个视频原作者找到了，抖音@乙人教动画 

这个是制作过程！👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1724781225355931741#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724781225355931741#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 13:27:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前发的这个视频原作者找到了，抖音@乙人教动画 <br />
<br />
这个是制作过程！👍</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI0NzgxMDcyNDIzMjgwNjQwL2ltZy9nOXcwV1NQbjZhNjhUb2xNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724712023009984763#m</id>
            <title>Apple Vision Pro 用户教学视频曝光

Apple VisionOS beta 6 中添加了新手入门视频

根据视频内容，用户只需要眼睛注视屏幕的元素即可选中目标

使用手指轻轻捏去即可操作，操作非常简单方便。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724712023009984763#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724712023009984763#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 08:52:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Apple Vision Pro 用户教学视频曝光<br />
<br />
Apple VisionOS beta 6 中添加了新手入门视频<br />
<br />
根据视频内容，用户只需要眼睛注视屏幕的元素即可选中目标<br />
<br />
使用手指轻轻捏去即可操作，操作非常简单方便。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ3MTE0MzY4ODExMTMwODgvcHUvaW1nL0NMWDhQU0tWVVNKcWpYSnouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724706784332664998#m</id>
            <title>Story-to-Motion：根据文本故事内容生成连续的角色的动画

该项目商汤科技研究院开发，能够处理复杂的文本描述，并将这些描述转换成具体的动作和位置信息。

它不仅能生成单一动作，还能连续地生成一系列动作，创造出连贯的动画效果。

Story-to-Motion一个关键特点是它能够生成无限长的角色动画。

这意味着，理论上，只要提供的文本故事足够长且内容连续，这个系统就能不断地根据文本内容生成相应的角色动作和行为，从而创造出持续不断的动画序列。

主要原理：

1、文本解析与动作调度：首先，系统使用大型语言模型来解析输入的长文本故事。这个过程涉及从文本中提取关键信息，如角色的动作、位置和情境。这些信息被转换成一系列的（文本，位置）对，用于后续的动作生成。

2、文本驱动的动作检索：系统接着根据提取的信息检索合适的动作。这一步骤结合了动作匹配技术、动作语义理解和轨迹约束，以确保生成的动作不仅与文本内容相符，而且在空间上也是合理的。

3、动作合成与过渡处理：系统设计了一个特殊的渐进式掩码变换器，用于处理动作之间的过渡。这个变换器解决了动作合成中常见的问题，如不自然的姿势和脚部滑动，确保动作的自然流畅。

4、无限动画生成：由于系统能够连续处理文本中的动作描述，它可以生成无限长的动画序列。这意味着只要文本故事持续，动画也会相应地持续生成。

项目及演示：https://story2motion.github.io/
论文：https://arxiv.org/abs/2311.07446</title>
            <link>https://nitter.cz/xiaohuggg/status/1724706784332664998#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724706784332664998#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 08:31:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Story-to-Motion：根据文本故事内容生成连续的角色的动画<br />
<br />
该项目商汤科技研究院开发，能够处理复杂的文本描述，并将这些描述转换成具体的动作和位置信息。<br />
<br />
它不仅能生成单一动作，还能连续地生成一系列动作，创造出连贯的动画效果。<br />
<br />
Story-to-Motion一个关键特点是它能够生成无限长的角色动画。<br />
<br />
这意味着，理论上，只要提供的文本故事足够长且内容连续，这个系统就能不断地根据文本内容生成相应的角色动作和行为，从而创造出持续不断的动画序列。<br />
<br />
主要原理：<br />
<br />
1、文本解析与动作调度：首先，系统使用大型语言模型来解析输入的长文本故事。这个过程涉及从文本中提取关键信息，如角色的动作、位置和情境。这些信息被转换成一系列的（文本，位置）对，用于后续的动作生成。<br />
<br />
2、文本驱动的动作检索：系统接着根据提取的信息检索合适的动作。这一步骤结合了动作匹配技术、动作语义理解和轨迹约束，以确保生成的动作不仅与文本内容相符，而且在空间上也是合理的。<br />
<br />
3、动作合成与过渡处理：系统设计了一个特殊的渐进式掩码变换器，用于处理动作之间的过渡。这个变换器解决了动作合成中常见的问题，如不自然的姿势和脚部滑动，确保动作的自然流畅。<br />
<br />
4、无限动画生成：由于系统能够连续处理文本中的动作描述，它可以生成无限长的动画序列。这意味着只要文本故事持续，动画也会相应地持续生成。<br />
<br />
项目及演示：<a href="https://story2motion.github.io/">story2motion.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.07446">arxiv.org/abs/2311.07446</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2OTU4OTIwMzEzNDA1NDQvcHUvaW1nL2QtQXdOWm9Eazgyb3JwOW4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724686568710164589#m</id>
            <title>R to @xiaohuggg: 3D互动门卡演示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1724686568710164589#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724686568710164589#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 07:11:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3D互动门卡演示：</p>
<p><a href="https://nitter.cz/atq_ren/status/1724506661081989315#m">nitter.cz/atq_ren/status/1724506661081989315#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724686268611928499#m</id>
            <title>R to @xiaohuggg: 应用案例，利用Spline制作一个3D门卡

教程：https://www.youtube.com/watch?v=Fv6WA9-73uk</title>
            <link>https://nitter.cz/xiaohuggg/status/1724686268611928499#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724686268611928499#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 07:10:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>应用案例，利用Spline制作一个3D门卡<br />
<br />
教程：<a href="https://www.youtube.com/watch?v=Fv6WA9-73uk">youtube.com/watch?v=Fv6WA9-7…</a></p>
<p><a href="https://nitter.cz/Aximoris/status/1724522933756874871#m">nitter.cz/Aximoris/status/1724522933756874871#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyMzk3MzcwNjUzODQ3NTUyMS9WRGhoNnUxWD9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724685982996500786#m</id>
            <title>Spline @Splinetool 宣布支持高斯泼溅（Gaussian Splatting）

现在可以使用 @Polycam3D  或者 @LumaLabsAI  从手机上捕捉任何 3D 物体，导出 .ply 文件。

然后将其导入到 Spline 中进行裁剪、调整，并嵌入到网站上。

3D高斯溅射是一种3D图像处理技术，它可以把现实世界中的物体或场景转换成3D模型，并在电脑上实时显示。特点是设置简单、渲染速度快，而且生成的3D图像质量很高。

教程：https://docs.spline.design/e17b7c105ef0433f8c5d2b39d512614e

演示：https://my.spline.design/girlstudio-8b6211e0b6ab456c8764297c6ff3ed45/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724685982996500786#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724685982996500786#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 07:09:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Spline <a href="https://nitter.cz/Splinetool" title="Spline">@Splinetool</a> 宣布支持高斯泼溅（Gaussian Splatting）<br />
<br />
现在可以使用 <a href="https://nitter.cz/Polycam3D" title="polycam">@Polycam3D</a>  或者 <a href="https://nitter.cz/LumaLabsAI" title="Luma AI">@LumaLabsAI</a>  从手机上捕捉任何 3D 物体，导出 .ply 文件。<br />
<br />
然后将其导入到 Spline 中进行裁剪、调整，并嵌入到网站上。<br />
<br />
3D高斯溅射是一种3D图像处理技术，它可以把现实世界中的物体或场景转换成3D模型，并在电脑上实时显示。特点是设置简单、渲染速度快，而且生成的3D图像质量很高。<br />
<br />
教程：<a href="https://docs.spline.design/e17b7c105ef0433f8c5d2b39d512614e">docs.spline.design/e17b7c105…</a><br />
<br />
演示：<a href="https://my.spline.design/girlstudio-8b6211e0b6ab456c8764297c6ff3ed45/">my.spline.design/girlstudio-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2ODMzNzUwNjY0MzE0ODgvcHUvaW1nL0ZGWi1xaDhPU3dnM2pQWHAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724643055532413310#m</id>
            <title>DeepMind 开发出一种用于天气预报的人工智能模型：GraphCast 。

它可以在不到一分钟的时间内完成10天内的天气预报，准确性超过了业界公认的高标准的欧洲中期天气预报中心（ECMWF）的高分辨率天气模拟系统（HRES）。

它还能够提前预测像飓风、洪水等极端天气事件。

DeepMind已开源GraphCast的代码。

主要特点：

1、高精度天气预测：GraphCast 能够提供长达10天的天气预测，其准确性超过了行业标准的高分辨率天气模拟系统（HRES），由欧洲中期天气预报中心（ECMWF）制作。

2、极端天气事件的早期预警：GraphCast 能够更早地预测极端天气事件，如准确预测气旋的路径、识别与洪水风险相关的大气河流，以及预测极端温度的发生。这种能力有助于通过更好的准备来挽救生命。

3、基于深度学习的天气预测系统：GraphCast 是一个基于机器学习和图神经网络（GNNs）的天气预测系统。通过训练，GraphCast 学习识别这些数据中的天气模式和趋势。例如，它可以学习识别导致风暴或高温的特定气候条件。

4、全球覆盖：它在全球范围内以0.25度经纬度的高分辨率进行预测，覆盖了地球表面的超过一百万个网格点。能够提供全球范围内的天气预报，这对于国际旅行、全球业务运营和气候研究都非常有用。

5、高效的预测模型：尽管GraphCast的训练过程计算量大，但最终的预测模型非常高效。使用GraphCast进行10天的预测仅需不到一分钟的时间，而传统方法如HRES可能需要数小时的超级计算机计算。

6、持续学习和适应：随着时间的推移，GraphCast 可以继续从新的气象数据中学习，不断提高其预测的准确性和可靠性。

7、广泛的应用：GraphCast 已被多个天气机构使用，包括 ECMWF，该机构已经在其网站上运行了模型预测的实时实验。https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850

8、开源代码：为了使基于AI的天气预报更加普及，DeepMind 已开源 GraphCast 模型的代码，使全球的科学家和预报员都能从中受益。

详细介绍：https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/

Science 论文：https://www.science.org/doi/10.1126/science.adi2336

Paper PDF：https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/Learning_skillful_medium-range_global_weather_forecasting.pdf

开源代码：https://github.com/google-deepmind/graphcast</title>
            <link>https://nitter.cz/xiaohuggg/status/1724643055532413310#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724643055532413310#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 04:18:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind 开发出一种用于天气预报的人工智能模型：GraphCast 。<br />
<br />
它可以在不到一分钟的时间内完成10天内的天气预报，准确性超过了业界公认的高标准的欧洲中期天气预报中心（ECMWF）的高分辨率天气模拟系统（HRES）。<br />
<br />
它还能够提前预测像飓风、洪水等极端天气事件。<br />
<br />
DeepMind已开源GraphCast的代码。<br />
<br />
主要特点：<br />
<br />
1、高精度天气预测：GraphCast 能够提供长达10天的天气预测，其准确性超过了行业标准的高分辨率天气模拟系统（HRES），由欧洲中期天气预报中心（ECMWF）制作。<br />
<br />
2、极端天气事件的早期预警：GraphCast 能够更早地预测极端天气事件，如准确预测气旋的路径、识别与洪水风险相关的大气河流，以及预测极端温度的发生。这种能力有助于通过更好的准备来挽救生命。<br />
<br />
3、基于深度学习的天气预测系统：GraphCast 是一个基于机器学习和图神经网络（GNNs）的天气预测系统。通过训练，GraphCast 学习识别这些数据中的天气模式和趋势。例如，它可以学习识别导致风暴或高温的特定气候条件。<br />
<br />
4、全球覆盖：它在全球范围内以0.25度经纬度的高分辨率进行预测，覆盖了地球表面的超过一百万个网格点。能够提供全球范围内的天气预报，这对于国际旅行、全球业务运营和气候研究都非常有用。<br />
<br />
5、高效的预测模型：尽管GraphCast的训练过程计算量大，但最终的预测模型非常高效。使用GraphCast进行10天的预测仅需不到一分钟的时间，而传统方法如HRES可能需要数小时的超级计算机计算。<br />
<br />
6、持续学习和适应：随着时间的推移，GraphCast 可以继续从新的气象数据中学习，不断提高其预测的准确性和可靠性。<br />
<br />
7、广泛的应用：GraphCast 已被多个天气机构使用，包括 ECMWF，该机构已经在其网站上运行了模型预测的实时实验。<a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850">charts.ecmwf.int/products/gr…</a><br />
<br />
8、开源代码：为了使基于AI的天气预报更加普及，DeepMind 已开源 GraphCast 模型的代码，使全球的科学家和预报员都能从中受益。<br />
<br />
详细介绍：<a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">deepmind.google/discover/blo…</a><br />
<br />
Science 论文：<a href="https://www.science.org/doi/10.1126/science.adi2336">science.org/doi/10.1126/scie…</a><br />
<br />
Paper PDF：<a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/Learning_skillful_medium-range_global_weather_forecasting.pdf">storage.googleapis.com/deepm…</a><br />
<br />
开源代码：<a href="https://github.com/google-deepmind/graphcast">github.com/google-deepmind/g…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2NDI5MDMxNDkxMjk3MjgvcHUvaW1nL3FOZTRJTURmQXc3Z2tTeVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724633998553706602#m</id>
            <title>R to @xiaohuggg: ChatGPT Plus会员账号要值钱了，这一段时间，卖账号估计能赚一笔！🙂</title>
            <link>https://nitter.cz/xiaohuggg/status/1724633998553706602#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724633998553706602#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 03:42:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Plus会员账号要值钱了，这一段时间，卖账号估计能赚一笔！🙂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724631012259557686#m</id>
            <title>⏰快讯：服务压力陡增@sama 奥特曼宣布

暂停ChatGPT Plus 会员注册

持续时间未知...</title>
            <link>https://nitter.cz/xiaohuggg/status/1724631012259557686#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724631012259557686#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 03:30:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>⏰快讯：服务压力陡增<a href="https://nitter.cz/sama" title="Sam Altman">@sama</a> 奥特曼宣布<br />
<br />
暂停ChatGPT Plus 会员注册<br />
<br />
持续时间未知...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi04ZDh5WWIwQUFyVkVELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724599830645997662#m</id>
            <title>R to @xiaohuggg: 也可以对年龄、人种等进行设置，面部会自动生成相应的年龄面部和人种...</title>
            <link>https://nitter.cz/xiaohuggg/status/1724599830645997662#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724599830645997662#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:26:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以对年龄、人种等进行设置，面部会自动生成相应的年龄面部和人种...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTg5OTEyMzE1NTc2MzIvcHUvaW1nL0otNDIxZVdDZXIxNlA5WDUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724598926748323891#m</id>
            <title>R to @xiaohuggg: 可以说不同的方言

支持将您自己的面部上传到生成模型。一些没有脸部的图像也可以生成面对对话。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724598926748323891#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724598926748323891#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:23:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可以说不同的方言<br />
<br />
支持将您自己的面部上传到生成模型。一些没有脸部的图像也可以生成面对对话。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTg1MjYxMTY4MTQ4NDgvcHUvaW1nL1pPd2M1a3lQaVB0aWllc3cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724597996225232906#m</id>
            <title>这个超有意思 哈哈哈哈 😂 字节跳动搞的

ChatAnything：可以与 LLMs 增强角色进行视频聊天

你可以通过文本描述，生成具有独特个性、外观和声音的虚拟角色人物。

而且这个人物不仅有自己的外观，还有独特的声音和个性，还可以跟你进行语音对话和视频聊天。😅

ChatAnything的工作原理基于几个关键的技术组件，这些组件共同协作，使得用户能够通过文本描述来创建和动画化个性化的虚拟角色。

以下是其主要工作流程和组件：

1、生成个性化角色：

•用户提供一个文本描述，定义想要创建的角色的特征和个性。

•大语言模型（LLM）根据这些描述生成一个具有特定个性的角色。

2、声音和外观的生成：

•声音混合（MoV）：这一部分利用文本到语音（TTS）技术。根据用户的文本描述，系统自动选择最匹配的声音特征，为角色生成独特的声音。

•扩散器混合（MoD）：结合了文本到图像生成技术和说话头部算法。这一步骤简化了生成说话对象的外观的过程。

3、动画化角色：

•一旦角色的个性、声音和外观被生成，系统使用这些信息来动画化角色。

•这包括将声音信号与生成的图像相结合，使角色能够根据用户的指令进行“说话”和“表现”。

4、面部运动的生成：

•为了使生成的角色更加逼真，系统还包括了面部运动的生成。

•这一部分涉及到像素级引导，它在图像生成阶段注入人脸标记，以确保面部运动的自然性和准确性。

ChatAnything的目标是通过文本输入，使用户能够以任何人格化的方式动画化任何事物。

项目地址：https://chatanything.github.io/
论文：https://arxiv.org/abs/2311.06772
GitHub：https://github.com/zhoudaquan/ChatAnything
在线演示：https://26fed97b4a7706bed0.gradio.live/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724597996225232906#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724597996225232906#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:19:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个超有意思 哈哈哈哈 😂 字节跳动搞的<br />
<br />
ChatAnything：可以与 LLMs 增强角色进行视频聊天<br />
<br />
你可以通过文本描述，生成具有独特个性、外观和声音的虚拟角色人物。<br />
<br />
而且这个人物不仅有自己的外观，还有独特的声音和个性，还可以跟你进行语音对话和视频聊天。😅<br />
<br />
ChatAnything的工作原理基于几个关键的技术组件，这些组件共同协作，使得用户能够通过文本描述来创建和动画化个性化的虚拟角色。<br />
<br />
以下是其主要工作流程和组件：<br />
<br />
1、生成个性化角色：<br />
<br />
•用户提供一个文本描述，定义想要创建的角色的特征和个性。<br />
<br />
•大语言模型（LLM）根据这些描述生成一个具有特定个性的角色。<br />
<br />
2、声音和外观的生成：<br />
<br />
•声音混合（MoV）：这一部分利用文本到语音（TTS）技术。根据用户的文本描述，系统自动选择最匹配的声音特征，为角色生成独特的声音。<br />
<br />
•扩散器混合（MoD）：结合了文本到图像生成技术和说话头部算法。这一步骤简化了生成说话对象的外观的过程。<br />
<br />
3、动画化角色：<br />
<br />
•一旦角色的个性、声音和外观被生成，系统使用这些信息来动画化角色。<br />
<br />
•这包括将声音信号与生成的图像相结合，使角色能够根据用户的指令进行“说话”和“表现”。<br />
<br />
4、面部运动的生成：<br />
<br />
•为了使生成的角色更加逼真，系统还包括了面部运动的生成。<br />
<br />
•这一部分涉及到像素级引导，它在图像生成阶段注入人脸标记，以确保面部运动的自然性和准确性。<br />
<br />
ChatAnything的目标是通过文本输入，使用户能够以任何人格化的方式动画化任何事物。<br />
<br />
项目地址：<a href="https://chatanything.github.io/">chatanything.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.06772">arxiv.org/abs/2311.06772</a><br />
GitHub：<a href="https://github.com/zhoudaquan/ChatAnything">github.com/zhoudaquan/ChatAn…</a><br />
在线演示：<a href="https://26fed97b4a7706bed0.gradio.live/">26fed97b4a7706bed0.gradio.li…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTUzMTk0MTYyNzA4NDgvcHUvaW1nL0JnZ2xVbHU0dk54Z2tUVk8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>