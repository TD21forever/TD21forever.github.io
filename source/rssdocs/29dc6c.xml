<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757769874154422392#m</id>
            <title>R to @xiaohuggg: FRIDAY在GAIA基准测试中相对于以前的最佳系统取得了35%的相对提升，展现了其强大的泛化能力和通过从先前任务积累的技能来处理未见过的应用的能力。

在最简单的Level-1任务中，FRIDAY实现了40.86%的成功率，标志着相比之前最好的系统（30.3%）显著的性能提升。

即使在最具挑战性的Level-3任务中，FRIDAY也实现了6.12%的成功率，这是之前的系统未能解决的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1757769874154422392#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757769874154422392#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 14:12:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>FRIDAY在GAIA基准测试中相对于以前的最佳系统取得了35%的相对提升，展现了其强大的泛化能力和通过从先前任务积累的技能来处理未见过的应用的能力。<br />
<br />
在最简单的Level-1任务中，FRIDAY实现了40.86%的成功率，标志着相比之前最好的系统（30.3%）显著的性能提升。<br />
<br />
即使在最具挑战性的Level-3任务中，FRIDAY也实现了6.12%的成功率，这是之前的系统未能解决的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dSYWVBS2IwQUFDUmVaLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757769869637132381#m</id>
            <title>OS-Copilot：能够与操作系统（OS）进行交互，完成一系列广泛且复杂的计算机任务的智能代理框架。

它能够自我学习和改进，处理各种操作系统级别的任务。

包括但不限于文件管理、数据处理、环境设置、多媒体操作、网页浏览、代码编写、第三方应用交互、自动化测试等。

该项目由上海AI实验室、华东师范大学、普林斯顿大学和香港大学的研究人员共同开发。

OS-Copilot为Linux和MacOS提供了一个通用接口，整合了常见的操作系统操控实践，如Python代码解释器、bash终端、鼠标/键盘控制和API调用。

研究人员基于OS-Copilot框架创建的一个实例或具体化代理：FRIDAY代理

FRIDAY能够从图片、视频或者文本中学习，并且能够执行一系列的计算机任务，比如在Excel中绘图，或者创建一个网站。最重要的是，FRIDAY能够通过做任务来学习新的技能，就像人类一样，通过不断的尝试和练习变得更擅长。

FRIDAY代理的主要功能：

1、自我学习与改进：FRIDAY能够通过自我指导学习不断地积累新的技能和知识，使其能够处理更多未知的任务和应用。这包括学习如何更有效地使用软件应用、执行特定任务的最佳实践等。

2、广泛的任务执行：FRIDAY设计为一个通用代理，能够执行多种计算机任务，包括数据分析、文件管理、自动化编程任务、多媒体编辑、网页浏览和信息检索等。

3、操作系统级别的交互：FRIDAY能够与操作系统内的各种元素进行交互，如文件系统、终端、第三方应用程序等，实现对这些元素的控制和管理。

4、第三方应用程序的集成与控制：利用OS-Copilot框架，FRIDAY可以与多种第三方应用程序进行集成和控制，如办公软件（Microsoft Office、Google Docs）、编程环境（IDEs）、社交媒体平台等。

5、自动化复杂工作流：FRIDAY能够自动化复杂的工作流程，如从执行数据收集、处理到报告生成的整个流程，大大提高工作效率。

6、自定义任务和工作流的创建：用户可以定制FRIDAY代理执行特定的任务和工作流，代理可以学习这些自定义操作并随时间改进执行效率。

7、高度的适应性和灵活性：FRIDAY通过自我学习的能力，能够适应各种新场景和需求，灵活地处理各种计算机任务。

8、提高生产力和效率：通过自动化日常任务和复杂工作流，FRIDAY旨在显著提高用户的生产力和工作效率。

9、FRIDAY代理通过这些功能，展示了OS-Copilot框架的强大潜力，为用户提供了一个高度灵活、能自我学习和改进的通用计算机代理，旨在处理各种计算机相关任务，从而解放用户的双手，提高工作效率。

FRIDAY代理与操作系统配合可以：

- 自动化日常任务：比如设置工作环境、自动整理文件、备份数据等。

- 数据处理与分析：在像Excel这样的应用程序中自动执行计算、生成图表等。

- 多媒体内容创建：比如自动在PowerPoint中创建演示文稿或编辑视频。

- 网页浏览和信息检索：自动化网页搜索，快速汇总和收集特定信息。

- 编程和脚本执行：自动生成代码片段或执行特定脚本来完成任务。

- 第三方应用交互：与邮件客户端、日历、社交媒体等第三方应用程序进行交互，执行如发送邮件、更新事件等任务。

- 自我学习和改进：通过自我指导学习新技能和应用，不断提升其执行任务的效率和准确度。

项目及演示：https://os-copilot.github.io/
论文：https://arxiv.org/abs/2402.07456
GitHub：https://github.com/OS-Copilot/FRIDAY</title>
            <link>https://nitter.cz/xiaohuggg/status/1757769869637132381#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757769869637132381#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 14:12:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OS-Copilot：能够与操作系统（OS）进行交互，完成一系列广泛且复杂的计算机任务的智能代理框架。<br />
<br />
它能够自我学习和改进，处理各种操作系统级别的任务。<br />
<br />
包括但不限于文件管理、数据处理、环境设置、多媒体操作、网页浏览、代码编写、第三方应用交互、自动化测试等。<br />
<br />
该项目由上海AI实验室、华东师范大学、普林斯顿大学和香港大学的研究人员共同开发。<br />
<br />
OS-Copilot为Linux和MacOS提供了一个通用接口，整合了常见的操作系统操控实践，如Python代码解释器、bash终端、鼠标/键盘控制和API调用。<br />
<br />
研究人员基于OS-Copilot框架创建的一个实例或具体化代理：FRIDAY代理<br />
<br />
FRIDAY能够从图片、视频或者文本中学习，并且能够执行一系列的计算机任务，比如在Excel中绘图，或者创建一个网站。最重要的是，FRIDAY能够通过做任务来学习新的技能，就像人类一样，通过不断的尝试和练习变得更擅长。<br />
<br />
FRIDAY代理的主要功能：<br />
<br />
1、自我学习与改进：FRIDAY能够通过自我指导学习不断地积累新的技能和知识，使其能够处理更多未知的任务和应用。这包括学习如何更有效地使用软件应用、执行特定任务的最佳实践等。<br />
<br />
2、广泛的任务执行：FRIDAY设计为一个通用代理，能够执行多种计算机任务，包括数据分析、文件管理、自动化编程任务、多媒体编辑、网页浏览和信息检索等。<br />
<br />
3、操作系统级别的交互：FRIDAY能够与操作系统内的各种元素进行交互，如文件系统、终端、第三方应用程序等，实现对这些元素的控制和管理。<br />
<br />
4、第三方应用程序的集成与控制：利用OS-Copilot框架，FRIDAY可以与多种第三方应用程序进行集成和控制，如办公软件（Microsoft Office、Google Docs）、编程环境（IDEs）、社交媒体平台等。<br />
<br />
5、自动化复杂工作流：FRIDAY能够自动化复杂的工作流程，如从执行数据收集、处理到报告生成的整个流程，大大提高工作效率。<br />
<br />
6、自定义任务和工作流的创建：用户可以定制FRIDAY代理执行特定的任务和工作流，代理可以学习这些自定义操作并随时间改进执行效率。<br />
<br />
7、高度的适应性和灵活性：FRIDAY通过自我学习的能力，能够适应各种新场景和需求，灵活地处理各种计算机任务。<br />
<br />
8、提高生产力和效率：通过自动化日常任务和复杂工作流，FRIDAY旨在显著提高用户的生产力和工作效率。<br />
<br />
9、FRIDAY代理通过这些功能，展示了OS-Copilot框架的强大潜力，为用户提供了一个高度灵活、能自我学习和改进的通用计算机代理，旨在处理各种计算机相关任务，从而解放用户的双手，提高工作效率。<br />
<br />
FRIDAY代理与操作系统配合可以：<br />
<br />
- 自动化日常任务：比如设置工作环境、自动整理文件、备份数据等。<br />
<br />
- 数据处理与分析：在像Excel这样的应用程序中自动执行计算、生成图表等。<br />
<br />
- 多媒体内容创建：比如自动在PowerPoint中创建演示文稿或编辑视频。<br />
<br />
- 网页浏览和信息检索：自动化网页搜索，快速汇总和收集特定信息。<br />
<br />
- 编程和脚本执行：自动生成代码片段或执行特定脚本来完成任务。<br />
<br />
- 第三方应用交互：与邮件客户端、日历、社交媒体等第三方应用程序进行交互，执行如发送邮件、更新事件等任务。<br />
<br />
- 自我学习和改进：通过自我指导学习新技能和应用，不断提升其执行任务的效率和准确度。<br />
<br />
项目及演示：<a href="https://os-copilot.github.io/">os-copilot.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2402.07456">arxiv.org/abs/2402.07456</a><br />
GitHub：<a href="https://github.com/OS-Copilot/FRIDAY">github.com/OS-Copilot/FRIDAY</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTc2Mjg0NTkzMTExMzI2NzIvcHUvaW1nL1R3ai12UEV1bUswNU40aFYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757620469069656265#m</id>
            <title>字节跳动的一个新的视频控制技术

Boximator：通过双重盒子约束，精确控制视频中对象的位置、形状或运动路径。

简单来说就是通过在图像上画出额外的方框（盒子Box）和添加文字提示来控制图像的动作。

Box分为两种，软盒子和硬盒子，硬盒子用于精确定位，而软盒子用来控制对象在指定区域内自由移动。

使用Boximator，你可以指定一个对象在视频的开始和结束时应该在哪里，以及它应该如何移动。

Boximator的主要工作原理：

通过两种类型的“盒子”（硬盒子和软盒子）允许用户精确控制视频中对象的位置、形状或运动路径。

硬盒子用于精确定位，而软盒子提供了更大的灵活性，允许对象在指定区域内自由移动。

1、双重盒子约束：

使用硬盒子（Hard Box）：在视频的起始帧，用户通过硬盒子精确地标出对象的初始位置和大小。这些硬盒子为视频合成提供了明确的起点。

设定目标状态：用户同样可以在视频的某个特定帧（通常是结束帧）使用硬盒子来指定对象应到达的位置和大小，定义对象的目标状态。

应用软盒子（Soft Box）：软盒子用来描述对象在视频中运动的大致范围或路径。它们不像硬盒子那样精确，但提供了对象在两个硬盒子约束之间移动时的自由度。

运动控制：通过在视频的不同帧之间设置软盒子，用户可以大致控制对象的移动路径。例如，可以通过在中间帧中设置软盒子来指导对象移动的方向和范围。

2、自我跟踪学习机制：

为了解决模型学习盒子和对象关系的难题，Boximator采用了自我跟踪的技术。通过这种方式，即使在视频的后续帧中没有直接的用户定义盒子，Boximator也能够预测对象的运动，确保对象按照用户的初衷移动。

这意味着即使用户没有为每一帧都设置明确的盒子约束，Boximator也能够根据已有的约束推断对象的运动。

基于用户定义的盒子约束和模型的自我跟踪学习，Boximator生成符合预期的对象运动和动作，实现从初始状态到目标状态的平滑过渡。

3、视频合成：

整合对象运动：在视频合成过程中，Boximator综合考虑硬盒子和软盒子的约束，以及模型对对象运动的预测，生成包含用户指定动作的视频。

优化和调整：用户可以根据初步生成的视频效果，调整盒子的位置、大小或运动路径，以进一步优化视频中的动作表现。

举例解释：

让我们通过一个具体的例子来解释如何通过双重盒子约束（硬盒子和软盒子）来控制视频的生成，假设我们想创造一个简单的视频：一只小猫从桌子的一边跳到另一边。

1.、定义初始状态：

硬盒子约束：在视频的第一帧，我们用一个硬盒子精确标出小猫的位置，假设小猫初始时静静地坐在桌子的左侧。这个硬盒子精确定义了小猫的起始位置和大小。

2、设定目标状态：

硬盒子约束：在视频的最后一帧，我们再次使用一个硬盒子来标定小猫跳跃后应到达的位置，即桌子的右侧。这个硬盒子为小猫的跳跃提供了一个明确的目标位置。

3、描述跳跃过程：

软盒子约束：为了让小猫的跳跃看起来自然，我们在视频的中间帧中设置一个或多个软盒子，描述小猫跳跃时经过的大致范围。例如，我们可以在桌子上方设置一个软盒子，表示小猫跳跃时会通过这个空间区域。

4、视频合成：

自我跟踪学习和动作生成：Boximator利用这些盒子约束和自我跟踪技术，预测和生成小猫从桌子一侧跳到另一侧的动作。通过软硬盒子的指导，小猫的运动路径、跳跃的弧线和最终落点都将符合用户的设定。

5、优化和调整：

如果初次生成的视频中小猫的跳跃动作不够自然或与预期有偏差，用户可以调整中间帧的软盒子位置或大小，甚至添加更多的软盒子来更精细地控制跳跃过程。Boximator会根据这些调整重新生成视频，直到达到满意的效果。
通过这个例子，我们可以看到，硬盒子和软盒子的结合使用，为用户提供了一个强大而灵活的方式来精确控制视频中的对象动作，无论是简单的跳跃还是更复杂的场景，都能够通过这种方法来实现想要的效果。

保留和增强基模型能力

在训练Boximator时，原始视频模型的权重被冻结，这样可以保留并利用基模型已经学习到的知识，仅在此基础上训练控制模块，提高效率和效果。

通过冻结基模型权重并仅训练控制模块，Boximator能够在不牺牲原始视频质量的情况下，增加对视频中对象动作的控制。这种方法保留了基模型的知识，同时增强了模型的应用范围和可控性。

插件式集成

Boximator可以作为现有视频扩散模型的插件使用，这允许它轻松集成到各种视频扩散模型中，这提供了极大的灵活性，使得Boximator可以广泛应用于不同的视频生成任务。

项目及演示：https://boximator.github.io/
论文：https://arxiv.org/abs/2402.01566
GitHub：coming soon...
视频介绍：https://www.youtube.com/watch?v=reto_TYsYyQ</title>
            <link>https://nitter.cz/xiaohuggg/status/1757620469069656265#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757620469069656265#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 04:19:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>字节跳动的一个新的视频控制技术<br />
<br />
Boximator：通过双重盒子约束，精确控制视频中对象的位置、形状或运动路径。<br />
<br />
简单来说就是通过在图像上画出额外的方框（盒子Box）和添加文字提示来控制图像的动作。<br />
<br />
Box分为两种，软盒子和硬盒子，硬盒子用于精确定位，而软盒子用来控制对象在指定区域内自由移动。<br />
<br />
使用Boximator，你可以指定一个对象在视频的开始和结束时应该在哪里，以及它应该如何移动。<br />
<br />
Boximator的主要工作原理：<br />
<br />
通过两种类型的“盒子”（硬盒子和软盒子）允许用户精确控制视频中对象的位置、形状或运动路径。<br />
<br />
硬盒子用于精确定位，而软盒子提供了更大的灵活性，允许对象在指定区域内自由移动。<br />
<br />
1、双重盒子约束：<br />
<br />
使用硬盒子（Hard Box）：在视频的起始帧，用户通过硬盒子精确地标出对象的初始位置和大小。这些硬盒子为视频合成提供了明确的起点。<br />
<br />
设定目标状态：用户同样可以在视频的某个特定帧（通常是结束帧）使用硬盒子来指定对象应到达的位置和大小，定义对象的目标状态。<br />
<br />
应用软盒子（Soft Box）：软盒子用来描述对象在视频中运动的大致范围或路径。它们不像硬盒子那样精确，但提供了对象在两个硬盒子约束之间移动时的自由度。<br />
<br />
运动控制：通过在视频的不同帧之间设置软盒子，用户可以大致控制对象的移动路径。例如，可以通过在中间帧中设置软盒子来指导对象移动的方向和范围。<br />
<br />
2、自我跟踪学习机制：<br />
<br />
为了解决模型学习盒子和对象关系的难题，Boximator采用了自我跟踪的技术。通过这种方式，即使在视频的后续帧中没有直接的用户定义盒子，Boximator也能够预测对象的运动，确保对象按照用户的初衷移动。<br />
<br />
这意味着即使用户没有为每一帧都设置明确的盒子约束，Boximator也能够根据已有的约束推断对象的运动。<br />
<br />
基于用户定义的盒子约束和模型的自我跟踪学习，Boximator生成符合预期的对象运动和动作，实现从初始状态到目标状态的平滑过渡。<br />
<br />
3、视频合成：<br />
<br />
整合对象运动：在视频合成过程中，Boximator综合考虑硬盒子和软盒子的约束，以及模型对对象运动的预测，生成包含用户指定动作的视频。<br />
<br />
优化和调整：用户可以根据初步生成的视频效果，调整盒子的位置、大小或运动路径，以进一步优化视频中的动作表现。<br />
<br />
举例解释：<br />
<br />
让我们通过一个具体的例子来解释如何通过双重盒子约束（硬盒子和软盒子）来控制视频的生成，假设我们想创造一个简单的视频：一只小猫从桌子的一边跳到另一边。<br />
<br />
1.、定义初始状态：<br />
<br />
硬盒子约束：在视频的第一帧，我们用一个硬盒子精确标出小猫的位置，假设小猫初始时静静地坐在桌子的左侧。这个硬盒子精确定义了小猫的起始位置和大小。<br />
<br />
2、设定目标状态：<br />
<br />
硬盒子约束：在视频的最后一帧，我们再次使用一个硬盒子来标定小猫跳跃后应到达的位置，即桌子的右侧。这个硬盒子为小猫的跳跃提供了一个明确的目标位置。<br />
<br />
3、描述跳跃过程：<br />
<br />
软盒子约束：为了让小猫的跳跃看起来自然，我们在视频的中间帧中设置一个或多个软盒子，描述小猫跳跃时经过的大致范围。例如，我们可以在桌子上方设置一个软盒子，表示小猫跳跃时会通过这个空间区域。<br />
<br />
4、视频合成：<br />
<br />
自我跟踪学习和动作生成：Boximator利用这些盒子约束和自我跟踪技术，预测和生成小猫从桌子一侧跳到另一侧的动作。通过软硬盒子的指导，小猫的运动路径、跳跃的弧线和最终落点都将符合用户的设定。<br />
<br />
5、优化和调整：<br />
<br />
如果初次生成的视频中小猫的跳跃动作不够自然或与预期有偏差，用户可以调整中间帧的软盒子位置或大小，甚至添加更多的软盒子来更精细地控制跳跃过程。Boximator会根据这些调整重新生成视频，直到达到满意的效果。<br />
通过这个例子，我们可以看到，硬盒子和软盒子的结合使用，为用户提供了一个强大而灵活的方式来精确控制视频中的对象动作，无论是简单的跳跃还是更复杂的场景，都能够通过这种方法来实现想要的效果。<br />
<br />
保留和增强基模型能力<br />
<br />
在训练Boximator时，原始视频模型的权重被冻结，这样可以保留并利用基模型已经学习到的知识，仅在此基础上训练控制模块，提高效率和效果。<br />
<br />
通过冻结基模型权重并仅训练控制模块，Boximator能够在不牺牲原始视频质量的情况下，增加对视频中对象动作的控制。这种方法保留了基模型的知识，同时增强了模型的应用范围和可控性。<br />
<br />
插件式集成<br />
<br />
Boximator可以作为现有视频扩散模型的插件使用，这允许它轻松集成到各种视频扩散模型中，这提供了极大的灵活性，使得Boximator可以广泛应用于不同的视频生成任务。<br />
<br />
项目及演示：<a href="https://boximator.github.io/">boximator.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2402.01566">arxiv.org/abs/2402.01566</a><br />
GitHub：coming soon...<br />
视频介绍：<a href="https://www.youtube.com/watch?v=reto_TYsYyQ">youtube.com/watch?v=reto_TYs…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTc0MTkzMTQ3ODgwOTgwNDgvcHUvaW1nL0ozSU5wREpSZEg0aXdtU1guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757598068579684422#m</id>
            <title>迎财神

💰💰💰</title>
            <link>https://nitter.cz/xiaohuggg/status/1757598068579684422#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757598068579684422#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 02:50:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>迎财神<br />
<br />
💰💰💰</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3NTk4MDUzNzM2MDYyOTc2L2ltZy9pVjBQTVdqWDVRVzdsMDFkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757596275451781452#m</id>
            <title>更聪明、更快速、能处理更多任务

一种含糊其辞的表达

自从上次CEO风波以后，奥特曼在谈到公司未来东西的时候都开始故意降低人们的预期！

不再吹牛逼了😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1757596275451781452#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757596275451781452#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 02:43:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>更聪明、更快速、能处理更多任务<br />
<br />
一种含糊其辞的表达<br />
<br />
自从上次CEO风波以后，奥特曼在谈到公司未来东西的时候都开始故意降低人们的预期！<br />
<br />
不再吹牛逼了😂</p>
<p><a href="https://nitter.cz/dotey/status/1757539255285412101#m">nitter.cz/dotey/status/1757539255285412101#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757593879468294424#m</id>
            <title>Elevenlabs的Speech to Speech 现已提供 29 种语言版本。

核心功能：

语音到语音变换：将你的声音转换成任意另一个角色的声音，并控制其情感和表达方式。确保每一个抑扬顿挫、停顿和调制都被捕捉并高保真重现。

只需点击一下，就能轻松为你的视频、播客、游戏等创建自定义AI声音。

体验：https://elevenlabs.io/voice-changer</title>
            <link>https://nitter.cz/xiaohuggg/status/1757593879468294424#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757593879468294424#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 02:33:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Elevenlabs的Speech to Speech 现已提供 29 种语言版本。<br />
<br />
核心功能：<br />
<br />
语音到语音变换：将你的声音转换成任意另一个角色的声音，并控制其情感和表达方式。确保每一个抑扬顿挫、停顿和调制都被捕捉并高保真重现。<br />
<br />
只需点击一下，就能轻松为你的视频、播客、游戏等创建自定义AI声音。<br />
<br />
体验：<a href="https://elevenlabs.io/voice-changer">elevenlabs.io/voice-changer</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1727519973718040838#m">nitter.cz/xiaohuggg/status/1727519973718040838#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3NTkzNzQ3ODg4NzU0Njg4L2ltZy9LRV9xbUk3b2RJQU1NRzFwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757582677002367414#m</id>
            <title>R to @xiaohuggg: 如果你的某些对话不想ChatGPT记住

又不想关闭该功能

他们提供了一个临时会话窗口

可以进行临时对话，该对话内容不会储存和被记住！</title>
            <link>https://nitter.cz/xiaohuggg/status/1757582677002367414#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757582677002367414#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 01:48:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你的某些对话不想ChatGPT记住<br />
<br />
又不想关闭该功能<br />
<br />
他们提供了一个临时会话窗口<br />
<br />
可以进行临时对话，该对话内容不会储存和被记住！</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0dRd0hSVWJVQUF4LURhLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dHUXdIUlViVUFBeC1EYS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757582651928768711#m</id>
            <title>R to @xiaohuggg: 您可以控制 ChatGPT 的记忆内存。

可以查看和删除记忆或完全关闭该功能。</title>
            <link>https://nitter.cz/xiaohuggg/status/1757582651928768711#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757582651928768711#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 01:48:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>您可以控制 ChatGPT 的记忆内存。<br />
<br />
可以查看和删除记忆或完全关闭该功能。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0dRd0Z1NWFnQUFwX2c5LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dHUXdGdTVhZ0FBcF9nOS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757582626372935696#m</id>
            <title>ChatGPT永久记忆功能正式上线

OpenAI宣布ChatGPT的一项重要更新：增加了记忆功能和新的用户控制选项。

GPT现在可以在与用户的交互中跨聊天记住你们互动的所有信息，并在后续对话中利用这些信息来提供更相关和个性化的回答。

之前测试的内容是GPT可以利用你们之间的对话来自我进化和学习，但是这一次没有提及这些功能！只说了改进聊天回答！

主要特点和功能：

• 记忆功能： ChatGPT可以跨所有聊天记住用户讨论的事情，减少用户重复提供信息的需要，让未来的对话更加有用。

• 用户控制： 用户可以显式地告诉ChatGPT记住某些内容，询问它记住了什么，通过对话或设置告诉它忘记某些信息，甚至可以完全关闭记忆功能。

• 如何工作： 用户与ChatGPT聊天时，可以要求它记住特定的信息或让它自行捕捉细节。ChatGPT的记忆功能会随着使用的增加而改善，并且用户会逐渐注意到这种改进。

• 隐私和安全标准： 记忆功能带来了额外的隐私和安全考虑，OpenAI采取措施评估和减轻偏见，并避免ChatGPT主动记住敏感信息，除非用户明确要求。

• 企业和团队用户： 对于企业和团队用户，记忆功能可以在使用ChatGPT进行工作时提供帮助，学习用户的风格和偏好，并在过去的互动上建立，节省时间并提供更相关和深刻的响应。

• GPTs也将拥有记忆： GPTs将拥有自己独特的记忆。构建者将有选项为他们的GPTs启用记忆功能。与用户的聊天一样，记忆不会与构建者共享。

详细：https://openai.com/blog/memory-and-new-controls-for-chatgpt</title>
            <link>https://nitter.cz/xiaohuggg/status/1757582626372935696#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757582626372935696#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 01:48:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT永久记忆功能正式上线<br />
<br />
OpenAI宣布ChatGPT的一项重要更新：增加了记忆功能和新的用户控制选项。<br />
<br />
GPT现在可以在与用户的交互中跨聊天记住你们互动的所有信息，并在后续对话中利用这些信息来提供更相关和个性化的回答。<br />
<br />
之前测试的内容是GPT可以利用你们之间的对话来自我进化和学习，但是这一次没有提及这些功能！只说了改进聊天回答！<br />
<br />
主要特点和功能：<br />
<br />
• 记忆功能： ChatGPT可以跨所有聊天记住用户讨论的事情，减少用户重复提供信息的需要，让未来的对话更加有用。<br />
<br />
• 用户控制： 用户可以显式地告诉ChatGPT记住某些内容，询问它记住了什么，通过对话或设置告诉它忘记某些信息，甚至可以完全关闭记忆功能。<br />
<br />
• 如何工作： 用户与ChatGPT聊天时，可以要求它记住特定的信息或让它自行捕捉细节。ChatGPT的记忆功能会随着使用的增加而改善，并且用户会逐渐注意到这种改进。<br />
<br />
• 隐私和安全标准： 记忆功能带来了额外的隐私和安全考虑，OpenAI采取措施评估和减轻偏见，并避免ChatGPT主动记住敏感信息，除非用户明确要求。<br />
<br />
• 企业和团队用户： 对于企业和团队用户，记忆功能可以在使用ChatGPT进行工作时提供帮助，学习用户的风格和偏好，并在过去的互动上建立，节省时间并提供更相关和深刻的响应。<br />
<br />
• GPTs也将拥有记忆： GPTs将拥有自己独特的记忆。构建者将有选项为他们的GPTs启用记忆功能。与用户的聊天一样，记忆不会与构建者共享。<br />
<br />
详细：<a href="https://openai.com/blog/memory-and-new-controls-for-chatgpt">openai.com/blog/memory-and-n…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0dRd0VIbWIwQUVtUUNNLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dHUXdFSG1iMEFFbVFDTS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757574748324979146#m</id>
            <title>NVIDIA发布一个可以运行在PC上本地模型：Chat With RTX

该模型可以与你自己的内容（文档、笔记、视频或其他数据）相连接，

利用检索增强生成（RAG）、TensorRT-LLM和RTX加速技术，用户可以个性化地创建一个聊天机器人。

快速获得上下文相关的答案。所有操作均在用户本地完成！

它专为搭载NVIDIA RTX显卡的Windows系统设计，可以个性化地与用户的文件、笔记和视频内容进行互动。

主要功能和特点包括：

1. 个性化内容访问：能够连接到用户的个人内容，如文档、笔记、视频等，提供针对性的回答和信息。

2. 高级技术支持：利用检索增强生成（RAG）、TensorRT-LLM和RTX加速技术，确保聊天机器人的回答既相关又快速。

3. 本地运行：所有功能都在用户的Windows RTX PC或工作站上本地运行，保证了数据的安全性和处理的速度。

4. 支持多种文件格式：能够处理多种文件格式，包括文本、PDF、DOC/DOCX和XML等，使用户可以轻松地将不同类型的内容纳入其个性化的聊天体验中。

5. 视频内容查询：可以通过提供YouTube播放列表的URL来加载视频的文字转录，从而使用户能够查询视频内容。

6. 系统要求：对运行环境有一定要求，需要Windows 11操作系统、NVIDIA GeForce RTX 30或40系列GPU、至少8GB的VRAM和16GB或更多的RAM。

•平台： Windows
•GPU： NVIDIA GeForce™ RTX 30或40系列GPU或NVIDIA RTX™ Ampere或Ada世代GPU，至少需要8GB的VRAM
•RAM： 16GB或更高
•操作系统： Windows 11
•驱动程序： 535.11或更高版本

详细：https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1757574748324979146#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757574748324979146#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 Feb 2024 01:17:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NVIDIA发布一个可以运行在PC上本地模型：Chat With RTX<br />
<br />
该模型可以与你自己的内容（文档、笔记、视频或其他数据）相连接，<br />
<br />
利用检索增强生成（RAG）、TensorRT-LLM和RTX加速技术，用户可以个性化地创建一个聊天机器人。<br />
<br />
快速获得上下文相关的答案。所有操作均在用户本地完成！<br />
<br />
它专为搭载NVIDIA RTX显卡的Windows系统设计，可以个性化地与用户的文件、笔记和视频内容进行互动。<br />
<br />
主要功能和特点包括：<br />
<br />
1. 个性化内容访问：能够连接到用户的个人内容，如文档、笔记、视频等，提供针对性的回答和信息。<br />
<br />
2. 高级技术支持：利用检索增强生成（RAG）、TensorRT-LLM和RTX加速技术，确保聊天机器人的回答既相关又快速。<br />
<br />
3. 本地运行：所有功能都在用户的Windows RTX PC或工作站上本地运行，保证了数据的安全性和处理的速度。<br />
<br />
4. 支持多种文件格式：能够处理多种文件格式，包括文本、PDF、DOC/DOCX和XML等，使用户可以轻松地将不同类型的内容纳入其个性化的聊天体验中。<br />
<br />
5. 视频内容查询：可以通过提供YouTube播放列表的URL来加载视频的文字转录，从而使用户能够查询视频内容。<br />
<br />
6. 系统要求：对运行环境有一定要求，需要Windows 11操作系统、NVIDIA GeForce RTX 30或40系列GPU、至少8GB的VRAM和16GB或更多的RAM。<br />
<br />
•平台： Windows<br />
•GPU： NVIDIA GeForce™ RTX 30或40系列GPU或NVIDIA RTX™ Ampere或Ada世代GPU，至少需要8GB的VRAM<br />
•RAM： 16GB或更高<br />
•操作系统： Windows 11<br />
•驱动程序： 535.11或更高版本<br />
<br />
详细：<a href="https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai/">nvidia.com/en-us/ai-on-rtx/c…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3NTc0NjkyNTg2OTAxNTA0L2ltZy9NSW41M3lSZEhNcU45MDVCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757433016010609137#m</id>
            <title>R to @xiaohuggg: 在线体验：https://huggingface.co/spaces/multimodalart/stable-cascade

作者：@multimodalart</title>
            <link>https://nitter.cz/xiaohuggg/status/1757433016010609137#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757433016010609137#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:54:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在线体验：<a href="https://huggingface.co/spaces/multimodalart/stable-cascade">huggingface.co/spaces/multim…</a><br />
<br />
作者：<a href="https://nitter.cz/multimodalart" title="apolinario (multimodal.art)">@multimodalart</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3NDMyOTU1MzE5MDA1MTg0L2ltZy9nLTlnOWxpdVFMTUVXa0s3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757431125579047408#m</id>
            <title>Stability AI 推出推出一种全新的图像生成模型：Stable Cascade

Stable Cascade特别的地方在于它通过一个“三阶段方法”的来让图片生成变得更高质量、更灵活，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求！

而且与SD相比训练成本降低16倍！

通俗来讲它通过一个叫做“三步走”的过程来完成：

1.第一步，它先把你的文字转换成一种叫做“潜在表示”的小图块，可以想象成一个非常模糊的小图像草稿。

2.第二步，通过两个阶段（我们叫它Stage A和B）把这个模糊的草稿变成一个清晰的高分辨率图像。这就像是先画出一个大概的草图，然后再逐步细化，最后上色，使其变得栩栩如生。

3.第三步，就是让这个过程变得高效且灵活。Stable Cascade能够让你只对文字转换那部分（第一步）进行调整和改善，而不需要每次都重新画整个图像。这样不仅节省了大量时间，还大大降低了需要的计算资源。

而且，它能根据不同的需要，使用不同“大小”的模型来完成任务。如果你想要更高质量的图像，可以选择“更大”的模型；如果你的电脑配置较低，也有“更小”的模型可供选择，这样就能确保每个人都能使用这项技术。

Stable Cascade在制作图像时，不仅仅关注图像看起来是否美观，还会确保图像与你输入的文字尽可能吻合。这意味着，它能够理解你的文字，然后创造出与之相匹配的场景。

核心特点和技术细节：

• 潜在生成阶段（Stage C）：将用户输入转换为紧凑的24x24潜在表示，这些表示随后传递给潜在解码阶段（Stage A &amp; B）。这一阶段用于压缩图像，与Stable Diffusion中的VAE（变分自编码器）的作用相似，但实现了更高的压缩率。

• 解码与高分辨率生成（Stage A &amp; B）：通过将文本条件生成（Stage C）与解码到高分辨率像素空间（Stage A &amp; B）解耦，允许在Stage C上单独完成额外的训练或微调，包括ControlNets和LoRAs。这比训练类似大小的Stable Diffusion模型降低了16倍的成本。

模型参数：

Stage C有两种不同的模型，分别为10亿（1B）和36亿（3.6B）参数；

Stage B则为7亿（700M）和15亿（1.5B）参数。推荐使用3.6B参数的Stage C模型，因为这个模型输出的质量最高。但对于那些希望关注最低硬件要求的用户，可以使用1B参数版本。对于Stage B，两者都能获得出色的结果，但15亿参数的模型在重建细节方面表现更佳。

性能比较和用户体验：

• 性能比较：在多个模型比较中，Stable Cascade在提示对齐和美学质量方面表现最佳。人类评估使用了混合的部分提示和美学提示来显示结果。

• 推理速度和VRAM要求：Stable Cascade的推理速度与其他模型（如SDXL，Playground v2等）进行了比较，显示了其在推理速度上的优势。预期的VRAM需求约为20GB，但通过使用较小的变种可以进一步降低，尽管这可能会降低最终输出的质量。

高效的训练和微调：

通过将文本条件生成与解码到高分辨率像素空间的过程解耦，Stable Cascade允许在Stage C进行额外的训练或微调，包括ControlNets和LoRAs，实现了与训练类似大小的Stable Diffusion模型相比16倍的成本降低。

总之：Stable Cascade通过其模块化方法和创新的三阶段处理，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求，使更多的用户能够访问和利用先进的文本到图像生成技术。

详细：https://stability.ai/news/introducing-stable-cascade

GitHub：https://github.com/Stability-AI/StableCascade

HuggingFace：https://github.com/Stability-AI/StableCascade</title>
            <link>https://nitter.cz/xiaohuggg/status/1757431125579047408#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757431125579047408#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:46:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI 推出推出一种全新的图像生成模型：Stable Cascade<br />
<br />
Stable Cascade特别的地方在于它通过一个“三阶段方法”的来让图片生成变得更高质量、更灵活，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求！<br />
<br />
而且与SD相比训练成本降低16倍！<br />
<br />
通俗来讲它通过一个叫做“三步走”的过程来完成：<br />
<br />
1.第一步，它先把你的文字转换成一种叫做“潜在表示”的小图块，可以想象成一个非常模糊的小图像草稿。<br />
<br />
2.第二步，通过两个阶段（我们叫它Stage A和B）把这个模糊的草稿变成一个清晰的高分辨率图像。这就像是先画出一个大概的草图，然后再逐步细化，最后上色，使其变得栩栩如生。<br />
<br />
3.第三步，就是让这个过程变得高效且灵活。Stable Cascade能够让你只对文字转换那部分（第一步）进行调整和改善，而不需要每次都重新画整个图像。这样不仅节省了大量时间，还大大降低了需要的计算资源。<br />
<br />
而且，它能根据不同的需要，使用不同“大小”的模型来完成任务。如果你想要更高质量的图像，可以选择“更大”的模型；如果你的电脑配置较低，也有“更小”的模型可供选择，这样就能确保每个人都能使用这项技术。<br />
<br />
Stable Cascade在制作图像时，不仅仅关注图像看起来是否美观，还会确保图像与你输入的文字尽可能吻合。这意味着，它能够理解你的文字，然后创造出与之相匹配的场景。<br />
<br />
核心特点和技术细节：<br />
<br />
• 潜在生成阶段（Stage C）：将用户输入转换为紧凑的24x24潜在表示，这些表示随后传递给潜在解码阶段（Stage A & B）。这一阶段用于压缩图像，与Stable Diffusion中的VAE（变分自编码器）的作用相似，但实现了更高的压缩率。<br />
<br />
• 解码与高分辨率生成（Stage A & B）：通过将文本条件生成（Stage C）与解码到高分辨率像素空间（Stage A & B）解耦，允许在Stage C上单独完成额外的训练或微调，包括ControlNets和LoRAs。这比训练类似大小的Stable Diffusion模型降低了16倍的成本。<br />
<br />
模型参数：<br />
<br />
Stage C有两种不同的模型，分别为10亿（1B）和36亿（3.6B）参数；<br />
<br />
Stage B则为7亿（700M）和15亿（1.5B）参数。推荐使用3.6B参数的Stage C模型，因为这个模型输出的质量最高。但对于那些希望关注最低硬件要求的用户，可以使用1B参数版本。对于Stage B，两者都能获得出色的结果，但15亿参数的模型在重建细节方面表现更佳。<br />
<br />
性能比较和用户体验：<br />
<br />
• 性能比较：在多个模型比较中，Stable Cascade在提示对齐和美学质量方面表现最佳。人类评估使用了混合的部分提示和美学提示来显示结果。<br />
<br />
• 推理速度和VRAM要求：Stable Cascade的推理速度与其他模型（如SDXL，Playground v2等）进行了比较，显示了其在推理速度上的优势。预期的VRAM需求约为20GB，但通过使用较小的变种可以进一步降低，尽管这可能会降低最终输出的质量。<br />
<br />
高效的训练和微调：<br />
<br />
通过将文本条件生成与解码到高分辨率像素空间的过程解耦，Stable Cascade允许在Stage C进行额外的训练或微调，包括ControlNets和LoRAs，实现了与训练类似大小的Stable Diffusion模型相比16倍的成本降低。<br />
<br />
总之：Stable Cascade通过其模块化方法和创新的三阶段处理，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求，使更多的用户能够访问和利用先进的文本到图像生成技术。<br />
<br />
详细：<a href="https://stability.ai/news/introducing-stable-cascade">stability.ai/news/introducin…</a><br />
<br />
GitHub：<a href="https://github.com/Stability-AI/StableCascade">github.com/Stability-AI/Stab…</a><br />
<br />
HuggingFace：<a href="https://github.com/Stability-AI/StableCascade">github.com/Stability-AI/Stab…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPbVNJZ2E4QUFoUHN6LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPbVNJaWFrQUFGQlVFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPbVNJaWFFQUFMSHlzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757388396996280482#m</id>
            <title>NVIDIA市值已超过亚马逊和谷歌，成为全球市值超过万亿切，排名第四的公司。

自2024年1月1日起，仅用了不到6周的时间，NVIDIA的市值增加了6500亿美元的市值。这一增长额超过了特斯拉整个公司的市值！

截至2024年2月12日，全球市值排行如下：

1. 微软 (Microsoft)：$3.115万亿
2. 苹果 (Apple)：$2.904万亿
3. 沙特阿美 (Saudi Aramco)：$2.034万亿
4. NVIDIA：$1.831万亿
5. Alphabet (谷歌的母公司)：$1.820万亿
6. 亚马逊 (Amazon)：$1.803万亿
7. Meta (Facebook的母公司)：$1.217万亿</title>
            <link>https://nitter.cz/xiaohuggg/status/1757388396996280482#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757388396996280482#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 12:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NVIDIA市值已超过亚马逊和谷歌，成为全球市值超过万亿切，排名第四的公司。<br />
<br />
自2024年1月1日起，仅用了不到6周的时间，NVIDIA的市值增加了6500亿美元的市值。这一增长额超过了特斯拉整个公司的市值！<br />
<br />
截至2024年2月12日，全球市值排行如下：<br />
<br />
1. 微软 (Microsoft)：$3.115万亿<br />
2. 苹果 (Apple)：$2.904万亿<br />
3. 沙特阿美 (Saudi Aramco)：$2.034万亿<br />
4. NVIDIA：$1.831万亿<br />
5. Alphabet (谷歌的母公司)：$1.820万亿<br />
6. 亚马逊 (Amazon)：$1.803万亿<br />
7. Meta (Facebook的母公司)：$1.217万亿</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dOX2JUUWF3QUE3eWZRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757355823460991485#m</id>
            <title>notesGPT：开源的语音笔记GPT工具

它能够录下你的语音内容，然后自动帮助你把这些语音转成文字，对内容进行总结，并且生成相关的任务列表！

notesGPT的主要功能包括：

1. 录制语音笔记：用户可以直接通过该工具录制他们的语音笔记，无需手动输入文字，方便快捷。

2. 自动转录：录制的语音笔记会自动被转换成文本形式，便于阅读和编辑。

3. 内容总结：notesGPT能够自动分析转录的文本内容，并提供一个简洁的总结，帮助用户快速把握笔记的关键信息。

4. 生成行动项：根据语音笔记的内容，自动生成相关的行动项或任务，帮助用户更有效地组织和规划工作或学习任务。

5. 100%免费和开源：notesGPT是一个完全开放源代码的项目，用户可以免费使用所有功能，同时社区的开发者也可以参与到项目的改进和扩展中来。

主要功能和技术栈：

•Convex：数据库和云函数
•Next.js App Router：框架
•Together Inference：LLM (Mixtral)
•Together Embeddings：搜索用的嵌入式数据库
•Convex File Storage：存储语音笔记
•Convex Vector search：向量搜索
•Replicate：Whisper转录
•Clerk：用户认证
•Tailwind CSS：样式设计

在线体验：http://usenotesgpt.com
GitHub：https://github.com/Nutlope/notesGPT</title>
            <link>https://nitter.cz/xiaohuggg/status/1757355823460991485#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757355823460991485#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 10:47:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>notesGPT：开源的语音笔记GPT工具<br />
<br />
它能够录下你的语音内容，然后自动帮助你把这些语音转成文字，对内容进行总结，并且生成相关的任务列表！<br />
<br />
notesGPT的主要功能包括：<br />
<br />
1. 录制语音笔记：用户可以直接通过该工具录制他们的语音笔记，无需手动输入文字，方便快捷。<br />
<br />
2. 自动转录：录制的语音笔记会自动被转换成文本形式，便于阅读和编辑。<br />
<br />
3. 内容总结：notesGPT能够自动分析转录的文本内容，并提供一个简洁的总结，帮助用户快速把握笔记的关键信息。<br />
<br />
4. 生成行动项：根据语音笔记的内容，自动生成相关的行动项或任务，帮助用户更有效地组织和规划工作或学习任务。<br />
<br />
5. 100%免费和开源：notesGPT是一个完全开放源代码的项目，用户可以免费使用所有功能，同时社区的开发者也可以参与到项目的改进和扩展中来。<br />
<br />
主要功能和技术栈：<br />
<br />
•Convex：数据库和云函数<br />
•Next.js App Router：框架<br />
•Together Inference：LLM (Mixtral)<br />
•Together Embeddings：搜索用的嵌入式数据库<br />
•Convex File Storage：存储语音笔记<br />
•Convex Vector search：向量搜索<br />
•Replicate：Whisper转录<br />
•Clerk：用户认证<br />
•Tailwind CSS：样式设计<br />
<br />
在线体验：<a href="http://usenotesgpt.com">usenotesgpt.com</a><br />
GitHub：<a href="https://github.com/Nutlope/notesGPT">github.com/Nutlope/notesGPT</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3MzU1NzYxMzg5NTA2NTYwL2ltZy9xYXpvcGJEa0UwQ1VvaFhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757319738450215079#m</id>
            <title>通过VR 也能操控真实物体

隔空控制家里电器开关</title>
            <link>https://nitter.cz/xiaohuggg/status/1757319738450215079#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757319738450215079#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:24:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过VR 也能操控真实物体<br />
<br />
隔空控制家里电器开关</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcxODM2MzY4MDgzNDM1NTIvcHUvaW1nL0paaWp0TlpyQm9yVUNfTXAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757240458944844074#m</id>
            <title>ElevenLabs推出一个新的平台，允许你分享自己的声音并赚取收入。

你可以在ElevenLabs的声音库中创建分享自己的AI声音，每当有人使用这些声音时，就能获得一定的回报。

操作流程如下：

-前往VoiceLab并上传30分钟以上的音频
-为您的声音命名并进行描述
-设置您的价格和使用参数
-添加您的支付详情，并在使用时获得支付

ElevenLabs还提供了一些高级选项，比如与平台合作，创建特别高质量的默认声音，这样不仅能获得保证的前期收入，还能提升个人品牌的影响力。

详细：https://elevenlabs.io/voice-actors</title>
            <link>https://nitter.cz/xiaohuggg/status/1757240458944844074#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757240458944844074#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 03:09:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ElevenLabs推出一个新的平台，允许你分享自己的声音并赚取收入。<br />
<br />
你可以在ElevenLabs的声音库中创建分享自己的AI声音，每当有人使用这些声音时，就能获得一定的回报。<br />
<br />
操作流程如下：<br />
<br />
-前往VoiceLab并上传30分钟以上的音频<br />
-为您的声音命名并进行描述<br />
-设置您的价格和使用参数<br />
-添加您的支付详情，并在使用时获得支付<br />
<br />
ElevenLabs还提供了一些高级选项，比如与平台合作，创建特别高质量的默认声音，这样不仅能获得保证的前期收入，还能提升个人品牌的影响力。<br />
<br />
详细：<a href="https://elevenlabs.io/voice-actors">elevenlabs.io/voice-actors</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcyMzE5NDUwNDA1MDY4ODAvcHUvaW1nL0FlcVBKSmRBRnZvbmhyemIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757231958088949810#m</id>
            <title>Azure OpenAI Service宣布了一系列新功能：包括公开预览的Assistants API、新的文本到语音（TTS）功能、即将推出的GPT-4 Turbo和GPT-3.5 Turbo模型更新、新的嵌入模型以及微调API的更新。

与之前的聊天完成API相比，Assistants API能够记住之前的对话内容，创建持久化和无限长的线程。

Assistants API 是一项由 Azure OpenAI 提供的新服务，它旨在帮助开发者在他们的应用程序中更容易地创建高质量的人工智能助手体验。与之前的聊天完成API相比，Assistants API提供了以下几个关键改进：

1、状态性演进： Assistants API能够记住之前的对话内容，这意味着它可以在对话过程中维持上下文，提供更连贯、更个性化的交流体验。

2、持久化和无限长的线程： 开发者可以创建持久化的对话线程，这些线程可以无限长，不受模型上下文窗口大小的限制。这样，即使是长时间或复杂的对话也能流畅进行。

3、简化对话状态管理： 由于Assistants API自身支持对话状态的管理，开发者无需自己构建复杂的系统来追踪对话状态或管理线程。这降低了开发复杂度，使得创建动态、互动的AI助手变得更简单。

Azure OpenAI Service更新和新增功能主要包括：

1、Assistants API公开预览：这是一个新功能，旨在让开发者在自己的应用中轻松创建类似副驾驶的高质量体验。Assistants API支持持久化和无限长的线程，简化了开发者在管理对话状态和集成工具方面的工作，提高了开发效率。

2、文本到语音（TTS）功能：引入了新的文本到语音模型，能够将文本转换为具有个性和风格的高质量人声，适用于客户支持、培训视频、实时流媒体等多种场景。

3、GPT-4 Turbo和GPT-3.5 Turbo模型更新：推出了GPT-4 Turbo和GPT-3.5 Turbo的更新版本，包括改进的代码生成能力、减少“懒惰”现象、修复了影响非英文UTF-8生成的错误等。提供了改进的指令遵循、JSON模式、可复现的输出、并行函数调用等特性。

5、新的嵌入模型和微调API更新：引入了新的嵌入模型和微调API的更新，包括一个新模型、支持持续微调和更优惠的定价。这些更新使得构建自定义模型和使用更长消息进行微调变得更加简单和经济。

6、定价优化：为训练和托管微调模型在Azure OpenAI Service上的成本提供了降价，包括将GPT-3.5-Turbo的训练和托管成本降低了50%。

详细：https://msft.it/6019ihVTb</title>
            <link>https://nitter.cz/xiaohuggg/status/1757231958088949810#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757231958088949810#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 02:35:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Azure OpenAI Service宣布了一系列新功能：包括公开预览的Assistants API、新的文本到语音（TTS）功能、即将推出的GPT-4 Turbo和GPT-3.5 Turbo模型更新、新的嵌入模型以及微调API的更新。<br />
<br />
与之前的聊天完成API相比，Assistants API能够记住之前的对话内容，创建持久化和无限长的线程。<br />
<br />
Assistants API 是一项由 Azure OpenAI 提供的新服务，它旨在帮助开发者在他们的应用程序中更容易地创建高质量的人工智能助手体验。与之前的聊天完成API相比，Assistants API提供了以下几个关键改进：<br />
<br />
1、状态性演进： Assistants API能够记住之前的对话内容，这意味着它可以在对话过程中维持上下文，提供更连贯、更个性化的交流体验。<br />
<br />
2、持久化和无限长的线程： 开发者可以创建持久化的对话线程，这些线程可以无限长，不受模型上下文窗口大小的限制。这样，即使是长时间或复杂的对话也能流畅进行。<br />
<br />
3、简化对话状态管理： 由于Assistants API自身支持对话状态的管理，开发者无需自己构建复杂的系统来追踪对话状态或管理线程。这降低了开发复杂度，使得创建动态、互动的AI助手变得更简单。<br />
<br />
Azure OpenAI Service更新和新增功能主要包括：<br />
<br />
1、Assistants API公开预览：这是一个新功能，旨在让开发者在自己的应用中轻松创建类似副驾驶的高质量体验。Assistants API支持持久化和无限长的线程，简化了开发者在管理对话状态和集成工具方面的工作，提高了开发效率。<br />
<br />
2、文本到语音（TTS）功能：引入了新的文本到语音模型，能够将文本转换为具有个性和风格的高质量人声，适用于客户支持、培训视频、实时流媒体等多种场景。<br />
<br />
3、GPT-4 Turbo和GPT-3.5 Turbo模型更新：推出了GPT-4 Turbo和GPT-3.5 Turbo的更新版本，包括改进的代码生成能力、减少“懒惰”现象、修复了影响非英文UTF-8生成的错误等。提供了改进的指令遵循、JSON模式、可复现的输出、并行函数调用等特性。<br />
<br />
5、新的嵌入模型和微调API更新：引入了新的嵌入模型和微调API的更新，包括一个新模型、支持持续微调和更优惠的定价。这些更新使得构建自定义模型和使用更长消息进行微调变得更加简单和经济。<br />
<br />
6、定价优化：为训练和托管微调模型在Azure OpenAI Service上的成本提供了降价，包括将GPT-3.5-Turbo的训练和托管成本降低了50%。<br />
<br />
详细：<a href="https://msft.it/6019ihVTb">msft.it/6019ihVTb</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcyMjg3NDI5MzI1NjYwMTYvcHUvaW1nL2t3ekxWczRVTTFKSEJGQWcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757225933138952546#m</id>
            <title>Keyframer：利用大语言模型（LLMs）将静态图像转换成动画。

该项目由苹果开发，通过自然语言提示，可以从静态SVG图像创建动画插图。

同时通过LLMs生成的CSS动画代码，用户可以对生成的动画进行细化编辑。

这意味着，无论你是想让一个设计中的元素旋转、移动还是淡入淡出，你都可以通过描述这些动作的文字来实现。

Keyframer主要功能：

1、自然语言控制：用户可以使用自然语言提示来指导动画的创作过程，无需深入了解动画制作的复杂技术细节。

2、迭代设计支持：Keyframer允许用户通过编辑大语言模型生成的CSS动画代码或属性来迭代和精细化他们的设计，提供了一种直接和灵活的方式来调整动画效果。

3、设计变体探索：用户可以请求设计变体，这支持了创意过程和设计探索，帮助用户发现和尝试新的设计方向。

4、分类用户提示策略：通过用户研究，Keyframer贡献了用户提示策略的分类，包括用于描述运动的语义提示类型和“分解”提示风格，这有助于用户更有效地与工具交互。

Keyframer将动画设计的门槛大大降低，让更广泛的受众能够参与到动画创作中，不仅对设计师有益，对于任何对动画创作感兴趣的人来说都是一个有价值的工具。

工作原理：

1、自然语言处理： 用户通过自然语言描述他们想要为静态SVG图像创建的动画效果。这些描述可以包括动画的方向、速度、效果等详细信息。

2、LLM生成动画代码： Keyframer利用大语言模型（如GPT-3）来解析用户的自然语言描述。基于这些描述，LLM生成相应的CSS动画代码。这段代码是动画效果的“配方”，指定了动画的具体行为，如动画的持续时间、变化的属性（例如位置、大小、颜色等）以及动画的时间函数。

3、动画渲染和预览： 生成的CSS代码应用于输入的SVG图像，创建动画效果。用户可以在Keyframer的界面中实时预览这些动画，以评估效果是否符合他们的预期。

4、直接编辑功能： 为了进一步细化和定制动画，Keyframer提供了代码编辑器和属性编辑器。用户可以预览这段代码生成的动画效果，如果效果符合预期，可以直接使用；如果需要调整，用户还可以通过编辑生成的CSS代码来细化动画的效果。这一步不需要用户编写代码，但他们可以直接修改代码来调整动画的各种参数（如速度、方向、延迟等），以达到完全符合他们设计意图的动画效果。

5、设计迭代与探索： 用户可以通过添加新的自然语言提示来请求设计变体或对动画进行迭代。这支持了创意思维和设计探索过程，允许用户在看到初步效果后，继续调整和完善他们的动画设计。

6、用户提示策略和反馈循环： Keyframer支持分解式提示风格，用户可以分步骤地描述和细化动画的各个部分。系统的设计允许用户基于生成的动画效果和直接编辑的反馈，不断调整他们的提示策略，实现设计目标的逐步精细化。

实验结果：

研究表明，即使是动画和编程经验较少的用户，也能够使用Keyframer有效地创建动画。通过直接编辑生成的CSS代码，用户可以对动画进行精细调整，这增强了他们对设计过程的控制感和满意度。

研究还发现，LLM生成的输出有时会出现意外的、创意的解决方案，这为设计过程带来了意外的惊喜和新的灵感。这些意外的输出有助于扩展用户的设计思维，推动他们探索新的创意可能性。
论文：https://arxiv.org/abs/2402.06071

PDF：https://arxiv.org/pdf/2402.06071.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1757225933138952546#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757225933138952546#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 02:11:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Keyframer：利用大语言模型（LLMs）将静态图像转换成动画。<br />
<br />
该项目由苹果开发，通过自然语言提示，可以从静态SVG图像创建动画插图。<br />
<br />
同时通过LLMs生成的CSS动画代码，用户可以对生成的动画进行细化编辑。<br />
<br />
这意味着，无论你是想让一个设计中的元素旋转、移动还是淡入淡出，你都可以通过描述这些动作的文字来实现。<br />
<br />
Keyframer主要功能：<br />
<br />
1、自然语言控制：用户可以使用自然语言提示来指导动画的创作过程，无需深入了解动画制作的复杂技术细节。<br />
<br />
2、迭代设计支持：Keyframer允许用户通过编辑大语言模型生成的CSS动画代码或属性来迭代和精细化他们的设计，提供了一种直接和灵活的方式来调整动画效果。<br />
<br />
3、设计变体探索：用户可以请求设计变体，这支持了创意过程和设计探索，帮助用户发现和尝试新的设计方向。<br />
<br />
4、分类用户提示策略：通过用户研究，Keyframer贡献了用户提示策略的分类，包括用于描述运动的语义提示类型和“分解”提示风格，这有助于用户更有效地与工具交互。<br />
<br />
Keyframer将动画设计的门槛大大降低，让更广泛的受众能够参与到动画创作中，不仅对设计师有益，对于任何对动画创作感兴趣的人来说都是一个有价值的工具。<br />
<br />
工作原理：<br />
<br />
1、自然语言处理： 用户通过自然语言描述他们想要为静态SVG图像创建的动画效果。这些描述可以包括动画的方向、速度、效果等详细信息。<br />
<br />
2、LLM生成动画代码： Keyframer利用大语言模型（如GPT-3）来解析用户的自然语言描述。基于这些描述，LLM生成相应的CSS动画代码。这段代码是动画效果的“配方”，指定了动画的具体行为，如动画的持续时间、变化的属性（例如位置、大小、颜色等）以及动画的时间函数。<br />
<br />
3、动画渲染和预览： 生成的CSS代码应用于输入的SVG图像，创建动画效果。用户可以在Keyframer的界面中实时预览这些动画，以评估效果是否符合他们的预期。<br />
<br />
4、直接编辑功能： 为了进一步细化和定制动画，Keyframer提供了代码编辑器和属性编辑器。用户可以预览这段代码生成的动画效果，如果效果符合预期，可以直接使用；如果需要调整，用户还可以通过编辑生成的CSS代码来细化动画的效果。这一步不需要用户编写代码，但他们可以直接修改代码来调整动画的各种参数（如速度、方向、延迟等），以达到完全符合他们设计意图的动画效果。<br />
<br />
5、设计迭代与探索： 用户可以通过添加新的自然语言提示来请求设计变体或对动画进行迭代。这支持了创意思维和设计探索过程，允许用户在看到初步效果后，继续调整和完善他们的动画设计。<br />
<br />
6、用户提示策略和反馈循环： Keyframer支持分解式提示风格，用户可以分步骤地描述和细化动画的各个部分。系统的设计允许用户基于生成的动画效果和直接编辑的反馈，不断调整他们的提示策略，实现设计目标的逐步精细化。<br />
<br />
实验结果：<br />
<br />
研究表明，即使是动画和编程经验较少的用户，也能够使用Keyframer有效地创建动画。通过直接编辑生成的CSS代码，用户可以对动画进行精细调整，这增强了他们对设计过程的控制感和满意度。<br />
<br />
研究还发现，LLM生成的输出有时会出现意外的、创意的解决方案，这为设计过程带来了意外的惊喜和新的灵感。这些意外的输出有助于扩展用户的设计思维，推动他们探索新的创意可能性。<br />
论文：<a href="https://arxiv.org/abs/2402.06071">arxiv.org/abs/2402.06071</a><br />
<br />
PDF：<a href="https://arxiv.org/pdf/2402.06071.pdf">arxiv.org/pdf/2402.06071.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dMcnFkMGJrQUEzdDE2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dMcnFlR2FNQUF6ZGFjLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dMcnFlS2J3QUE2TEhELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757022189201858603#m</id>
            <title>Franco Ronconi 介绍了一个Canvastique3D工具

这是一个结合了OpenCV（一个开源计算机视觉库）和OpenAI技术的工具

这个工具允许设计师在3D模型上实时预览他们的手工设计，这意味着设计师可以立即看到他们的设计在虚拟三维空间中的样子，而不需要等待制作实体样品。</title>
            <link>https://nitter.cz/xiaohuggg/status/1757022189201858603#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757022189201858603#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 12:41:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Franco Ronconi 介绍了一个Canvastique3D工具<br />
<br />
这是一个结合了OpenCV（一个开源计算机视觉库）和OpenAI技术的工具<br />
<br />
这个工具允许设计师在3D模型上实时预览他们的手工设计，这意味着设计师可以立即看到他们的设计在虚拟三维空间中的样子，而不需要等待制作实体样品。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI4MDc1NDMwNzA2MTM1MDQvcHUvaW1nL1hNWUpUZDN1TEc4NkJtSVIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756977011782979921#m</id>
            <title>MoneyPrinter：自动创建YouTube短视频的自动化赚钱项目

主要功能：

- 自动视频生成：只要输入视频话题即可自动产生与之相关的短视频。

- 音乐和字体自定义：可以上传自己的MP3文件压缩包和字体，自定义视频音乐背景和字体。

-自动将生成的视频上传到YouTube。

整个过程几乎不需要用户有太多的视频编辑技能，只需要简单的操作和等待程序完成工作。

MoneyPrinter的背后技术主要依赖于Python编程语言和MoviePy视频编辑库，以及YouTube的API用于视频上传，使得从视频创意到发布的整个流程自动化和无缝连接。

MoviePy是一个强大的视频处理库，能够编辑视频、添加音乐背景和文本等。

GitHub：https://github.com/FujiwaraChoki/MoneyPrinter</title>
            <link>https://nitter.cz/xiaohuggg/status/1756977011782979921#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756977011782979921#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 09:42:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MoneyPrinter：自动创建YouTube短视频的自动化赚钱项目<br />
<br />
主要功能：<br />
<br />
- 自动视频生成：只要输入视频话题即可自动产生与之相关的短视频。<br />
<br />
- 音乐和字体自定义：可以上传自己的MP3文件压缩包和字体，自定义视频音乐背景和字体。<br />
<br />
-自动将生成的视频上传到YouTube。<br />
<br />
整个过程几乎不需要用户有太多的视频编辑技能，只需要简单的操作和等待程序完成工作。<br />
<br />
MoneyPrinter的背后技术主要依赖于Python编程语言和MoviePy视频编辑库，以及YouTube的API用于视频上传，使得从视频创意到发布的整个流程自动化和无缝连接。<br />
<br />
MoviePy是一个强大的视频处理库，能够编辑视频、添加音乐背景和文本等。<br />
<br />
GitHub：<a href="https://github.com/FujiwaraChoki/MoneyPrinter">github.com/FujiwaraChoki/Mon…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjkwOTI0MzM4MDUzMTIvcHUvaW1nL1FLTlB4S3gyOF9nYzAtUTguanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>