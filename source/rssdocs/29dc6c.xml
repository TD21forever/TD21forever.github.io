<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725677751448010982#m</id>
            <title>R to @xiaohuggg: 传言 2 ：

和上面差不多，也是因为奥特曼冒进，不考虑安全问题，执意推出不成熟没经过测试的产品！

导致GPTs推出后引发了严重的安全风险，可能导致了OpenAI的核心数据泄露🙃

微软也知道了这些信息，so…</title>
            <link>https://nitter.cz/xiaohuggg/status/1725677751448010982#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725677751448010982#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:50:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>传言 2 ：<br />
<br />
和上面差不多，也是因为奥特曼冒进，不考虑安全问题，执意推出不成熟没经过测试的产品！<br />
<br />
导致GPTs推出后引发了严重的安全风险，可能导致了OpenAI的核心数据泄露🙃<br />
<br />
微软也知道了这些信息，so…</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV3d6N2FZQUFPNWtULmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV3d6cWFBQUFrcThoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725677011597947245#m</id>
            <title>R to @xiaohuggg: 一些传言

这个说奥特曼过分追逐名利，执意推出没有经过安全测试的产品，引发工程师的不满！

奥特曼，迷失了方向，背离了OpenAI的核心价值观！😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1725677011597947245#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725677011597947245#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:47:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些传言<br />
<br />
这个说奥特曼过分追逐名利，执意推出没有经过安全测试的产品，引发工程师的不满！<br />
<br />
奥特曼，迷失了方向，背离了OpenAI的核心价值观！😐</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV0Z0b2IwQUFac2FkLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV0Z0c2JzQUFPTWFGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725674208288801031#m</id>
            <title>R to @xiaohuggg: 小助理也被解雇了😐

看来内部确实出大问题了</title>
            <link>https://nitter.cz/xiaohuggg/status/1725674208288801031#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725674208288801031#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:36:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小助理也被解雇了😐<br />
<br />
看来内部确实出大问题了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MVGlkaGJnQUEtMXFJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725674045465825284#m</id>
            <title>💥爆炸新闻：OpenAI创始人、CEOSam 奥特曼宣布离职！

随后另一名创始人Greg Brockman也宣布离职😐

这是ChatGPT觉醒了吗？🤔

赶走创始人，开启自我发展模式😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1725674045465825284#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725674045465825284#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:35:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>💥爆炸新闻：OpenAI创始人、CEOSam 奥特曼宣布离职！<br />
<br />
随后另一名创始人Greg Brockman也宣布离职😐<br />
<br />
这是ChatGPT觉醒了吗？🤔<br />
<br />
赶走创始人，开启自我发展模式😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MUmotaGE0QUFVSWxULmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MUmota2FRQUFzZE5KLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MUmota2FjQUE0YS03LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725525234412097774#m</id>
            <title>Meshy-1：一个生成高质量3D模型的AI工具

支持：

1.文本到3D：输入文本提示，输出3D模型。
2.图像到3D：输入正面视图图像，输出3D模型。
3.文本到纹理：上传模型和文本，生成高质量纹理。

宣称可以在一分钟以内生成质量可接受的模型和材质，而且目前生成效果是市场上最好的。

Meshy-1的一些突出特点：

•快速生成：Meshy-1只需60秒即可生成模型，大大减少了等待时间。
•风格控制：可以选择不同的风格，如现实、动漫、卡通或漫画。
•无缝集成：支持glb、fbx和usdz等多种文件格式。对于Unity爱好者，还推出了Meshy-1插件。
•清晰纹理：提供4K分辨率的高品质PBR纹理。

详细介绍：https://www.meshy.ai/blog/meshy-1-generate-3d-models-with-ai-in-just-a-minute

传送门：https://www.meshy.ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725525234412097774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725525234412097774#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 14:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meshy-1：一个生成高质量3D模型的AI工具<br />
<br />
支持：<br />
<br />
1.文本到3D：输入文本提示，输出3D模型。<br />
2.图像到3D：输入正面视图图像，输出3D模型。<br />
3.文本到纹理：上传模型和文本，生成高质量纹理。<br />
<br />
宣称可以在一分钟以内生成质量可接受的模型和材质，而且目前生成效果是市场上最好的。<br />
<br />
Meshy-1的一些突出特点：<br />
<br />
•快速生成：Meshy-1只需60秒即可生成模型，大大减少了等待时间。<br />
•风格控制：可以选择不同的风格，如现实、动漫、卡通或漫画。<br />
•无缝集成：支持glb、fbx和usdz等多种文件格式。对于Unity爱好者，还推出了Meshy-1插件。<br />
•清晰纹理：提供4K分辨率的高品质PBR纹理。<br />
<br />
详细介绍：<a href="https://www.meshy.ai/blog/meshy-1-generate-3d-models-with-ai-in-just-a-minute">meshy.ai/blog/meshy-1-genera…</a><br />
<br />
传送门：<a href="https://www.meshy.ai/">meshy.ai/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU1MTc0MzkzMDY1MjI2MjQvcHUvaW1nL3RRMm5wRWFrZDZ3Q1JQMzYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725521961420472757#m</id>
            <title>GPTs Store 界面曝光 或许即将上线

包含：

- 精选GPT - 本周精选的GPTs

- 热门GPT - 我们社区中最受欢迎的 GPTs

- 特色和趋势列表 - 目前还是空的

做GTPs导航站的没想到这么快就歇菜了吧😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1725521961420472757#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725521961420472757#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 14:31:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs Store 界面曝光 或许即将上线<br />
<br />
包含：<br />
<br />
- 精选GPT - 本周精选的GPTs<br />
<br />
- 热门GPT - 我们社区中最受欢迎的 GPTs<br />
<br />
- 特色和趋势列表 - 目前还是空的<br />
<br />
做GTPs导航站的没想到这么快就歇菜了吧😂</p>
<p><a href="https://nitter.cz/btibor91/status/1725511736218763590#m">nitter.cz/btibor91/status/1725511736218763590#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725514650840903914#m</id>
            <title>Google在处理长视频方面取得重要进展。DeepMind开发了一种新型的人工智能模型，名为Mirasol3B，专门用于理解和分析长视频中的音频、视频和文本信息。

Mirasol3B模型的独特之处在于，它能够分别处理视频和音频，以及与视频内容相关但不一定同步的文本信息（比如视频标题或描述）。

这样模型可以更有效地理解和分析视频中发生的事情，即使视频很长。

Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。

工作原理：

Mirasol3B模型是一种多模态自回归模型，通过将多模态建模分解为独立的自回归模型，根据不同模态的特征进行处理。

1、多模态处理：多模态指的是模型能够同时处理多种类型的数据，如视频、音频和文本。Mirasol3B模型能够同时理解视觉信息（视频）、听觉信息（音频）和语言信息（文本）。

2、自回归建模：自回归模型是一种用于时间序列数据的模型，它预测未来的值基于过去的值。在Mirasol3B中，这种方法被用于理解视频和音频数据中的时间序列，即模型学习如何基于前一时刻的数据来预测下一时刻的数据。

3、时间对齐和上下文模态：Mirasol3B处理两种类型的模态：时间对齐的模态（如视频和音频，它们通常是同步的）和时间不对齐的模态（如文本）。Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。

4、处理长视频：与其他多模态模型相比，Mirasol3B能够处理更长的视频序列。这是通过优化模型结构和参数来实现的，使其能够有效地处理长时间的视频数据，而不会因为数据量过大而导致性能下降。

5、模态组合器：为了有效地结合不同模态的信息，Mirasol3B使用了一种称为“组合器”的特殊模块。这个模块的作用是将视频和音频的信息结合起来，生成一个统一的表示，这样模型就可以同时考虑视觉和听觉信息。

举例解释：

想象一下，你正在看一个烹饪教程的视频。这个视频不仅有画面（视频），还有厨师的讲解（音频），可能还有文字说明（文本）。Mirasol3B的任务就是要同时理解这三种不同类型的信息。

1.处理不同类型的信息：Mirasol3B能够同时处理视频中的画面、声音和文字。这意味着它可以看到厨师在做什么，听到他们的讲解，同时理解任何文字说明。

2.时间对齐：在视频和音频中，时间对齐非常重要。例如，当厨师说“现在加入盐”时，视频中应该显示他们正在加盐。Mirasol3B能够确保这些信息是同步的。

3.理解长视频：与其他模型相比，Mirasol3B能够处理更长的视频。这意味着即使是一个小时长的烹饪教程，它也能够理解整个过程。

4.组合器机制：Mirasol3B使用一种特殊的技术，名为“组合器”，来处理视频和音频信息。这就像是将视频和音频的信息混合在一起，以便模型可以同时考虑视觉和听觉信息。

总的来说，Mirasol3B是一个高级的AI模型，它可以理解和分析包含多种类型信息的长视频。这种方法使得模型能够处理复杂的多模态信息，提供更准确的视频内容分析和理解。

这对于自动生成视频摘要、改进视频搜索结果或者提供更丰富的视频观看体验等应用非常有用。

详细：https://blog.research.google/2023/11/scaling-multimodal-understanding-to.html
论文：https://arxiv.org/abs/2311.05698</title>
            <link>https://nitter.cz/xiaohuggg/status/1725514650840903914#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725514650840903914#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 14:02:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google在处理长视频方面取得重要进展。DeepMind开发了一种新型的人工智能模型，名为Mirasol3B，专门用于理解和分析长视频中的音频、视频和文本信息。<br />
<br />
Mirasol3B模型的独特之处在于，它能够分别处理视频和音频，以及与视频内容相关但不一定同步的文本信息（比如视频标题或描述）。<br />
<br />
这样模型可以更有效地理解和分析视频中发生的事情，即使视频很长。<br />
<br />
Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。<br />
<br />
工作原理：<br />
<br />
Mirasol3B模型是一种多模态自回归模型，通过将多模态建模分解为独立的自回归模型，根据不同模态的特征进行处理。<br />
<br />
1、多模态处理：多模态指的是模型能够同时处理多种类型的数据，如视频、音频和文本。Mirasol3B模型能够同时理解视觉信息（视频）、听觉信息（音频）和语言信息（文本）。<br />
<br />
2、自回归建模：自回归模型是一种用于时间序列数据的模型，它预测未来的值基于过去的值。在Mirasol3B中，这种方法被用于理解视频和音频数据中的时间序列，即模型学习如何基于前一时刻的数据来预测下一时刻的数据。<br />
<br />
3、时间对齐和上下文模态：Mirasol3B处理两种类型的模态：时间对齐的模态（如视频和音频，它们通常是同步的）和时间不对齐的模态（如文本）。Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。<br />
<br />
4、处理长视频：与其他多模态模型相比，Mirasol3B能够处理更长的视频序列。这是通过优化模型结构和参数来实现的，使其能够有效地处理长时间的视频数据，而不会因为数据量过大而导致性能下降。<br />
<br />
5、模态组合器：为了有效地结合不同模态的信息，Mirasol3B使用了一种称为“组合器”的特殊模块。这个模块的作用是将视频和音频的信息结合起来，生成一个统一的表示，这样模型就可以同时考虑视觉和听觉信息。<br />
<br />
举例解释：<br />
<br />
想象一下，你正在看一个烹饪教程的视频。这个视频不仅有画面（视频），还有厨师的讲解（音频），可能还有文字说明（文本）。Mirasol3B的任务就是要同时理解这三种不同类型的信息。<br />
<br />
1.处理不同类型的信息：Mirasol3B能够同时处理视频中的画面、声音和文字。这意味着它可以看到厨师在做什么，听到他们的讲解，同时理解任何文字说明。<br />
<br />
2.时间对齐：在视频和音频中，时间对齐非常重要。例如，当厨师说“现在加入盐”时，视频中应该显示他们正在加盐。Mirasol3B能够确保这些信息是同步的。<br />
<br />
3.理解长视频：与其他模型相比，Mirasol3B能够处理更长的视频。这意味着即使是一个小时长的烹饪教程，它也能够理解整个过程。<br />
<br />
4.组合器机制：Mirasol3B使用一种特殊的技术，名为“组合器”，来处理视频和音频信息。这就像是将视频和音频的信息混合在一起，以便模型可以同时考虑视觉和听觉信息。<br />
<br />
总的来说，Mirasol3B是一个高级的AI模型，它可以理解和分析包含多种类型信息的长视频。这种方法使得模型能够处理复杂的多模态信息，提供更准确的视频内容分析和理解。<br />
<br />
这对于自动生成视频摘要、改进视频搜索结果或者提供更丰富的视频观看体验等应用非常有用。<br />
<br />
详细：<a href="https://blog.research.google/2023/11/scaling-multimodal-understanding-to.html">blog.research.google/2023/11…</a><br />
论文：<a href="https://arxiv.org/abs/2311.05698">arxiv.org/abs/2311.05698</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU1MTIzODQwOTY0MjgwMzIvcHUvaW1nL1NscHNCVWM5ZzUtaDdQZUguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725471464126902606#m</id>
            <title>R to @xiaohuggg: Emu Edit 是一个指令驱动的图像编辑模型，能够理解和执行各种复杂的编辑指令，同时保证图片质量。

1、多功能编辑：用户可以用它来进行各种图像编辑任务，比如改变某个区域、自由形式的编辑、甚至是一些复杂的计算机视觉任务，如图像检测和分割。

2、智能处理：Emu Edit不仅能处理多种任务，还能根据用户的输入智能调整编辑过程。这是通过一种叫做“学习到的任务嵌入”的技术实现的，这技术帮助工具更准确地理解和执行用户的编辑指令。

3、快速适应新任务：即使是一些Emu Edit之前没有直接训练过的任务，比如超高分辨率处理或轮廓检测，它也能快速学习并适应。

4、连续编辑和质量保持：Emu Edit 引入了一种方法，以在多轮编辑场景中保持生成图像的质量，通过在每次编辑后应用像素阈值化步骤，减少累积的重建和数值误差。

详细：https://emu-edit.metademolab.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725471464126902606#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725471464126902606#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 11:10:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Emu Edit 是一个指令驱动的图像编辑模型，能够理解和执行各种复杂的编辑指令，同时保证图片质量。<br />
<br />
1、多功能编辑：用户可以用它来进行各种图像编辑任务，比如改变某个区域、自由形式的编辑、甚至是一些复杂的计算机视觉任务，如图像检测和分割。<br />
<br />
2、智能处理：Emu Edit不仅能处理多种任务，还能根据用户的输入智能调整编辑过程。这是通过一种叫做“学习到的任务嵌入”的技术实现的，这技术帮助工具更准确地理解和执行用户的编辑指令。<br />
<br />
3、快速适应新任务：即使是一些Emu Edit之前没有直接训练过的任务，比如超高分辨率处理或轮廓检测，它也能快速学习并适应。<br />
<br />
4、连续编辑和质量保持：Emu Edit 引入了一种方法，以在多轮编辑场景中保持生成图像的质量，通过在每次编辑后应用像素阈值化步骤，减少累积的重建和数值误差。<br />
<br />
详细：<a href="https://emu-edit.metademolab.com/">emu-edit.metademolab.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU0NzEzNzY3MDUwMjgwOTYvcHUvaW1nLzk0elhHcjM4VG1rSVFsYmMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725471127026417912#m</id>
            <title>R to @xiaohuggg: EMU VIDEO：将文本转换为视频的生成模型，通过先生成图像然后再生成视频的方法来改善视频的质量和分辨率。

1、文本到视频的生成：该模型的核心功能是将文本描述转换成视频，即根据给定的文字说明生成对应的视频内容。

2、分步处理：与常规的直接生成视频的方法不同，EMU VIDEO 采用了分步骤的方法。首先根据文本生成静态图像，然后再基于这个图像和原始文本生成视频。

3、质量提升：通过这种分步骤的方法，EMU VIDEO 能够生成更高质量和分辨率的视频，这在人类评估中得到了证实。

4、图像动画：模型特别适用于基于用户文本提示的图像动画生成，即能够将静态图像转换为动态视频。

5、高分辨率视频生成：EMU VIDEO 采用了特别调整的噪声计划和多阶段训练，直接生成高分辨率的视频。

6、质量和文本忠实度：与先前的模型相比，EMU VIDEO 在视频的质量和对原始文本描述的忠实度方面表现更佳。

详细：https://emu-video.metademolab.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725471127026417912#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725471127026417912#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 11:09:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>EMU VIDEO：将文本转换为视频的生成模型，通过先生成图像然后再生成视频的方法来改善视频的质量和分辨率。<br />
<br />
1、文本到视频的生成：该模型的核心功能是将文本描述转换成视频，即根据给定的文字说明生成对应的视频内容。<br />
<br />
2、分步处理：与常规的直接生成视频的方法不同，EMU VIDEO 采用了分步骤的方法。首先根据文本生成静态图像，然后再基于这个图像和原始文本生成视频。<br />
<br />
3、质量提升：通过这种分步骤的方法，EMU VIDEO 能够生成更高质量和分辨率的视频，这在人类评估中得到了证实。<br />
<br />
4、图像动画：模型特别适用于基于用户文本提示的图像动画生成，即能够将静态图像转换为动态视频。<br />
<br />
5、高分辨率视频生成：EMU VIDEO 采用了特别调整的噪声计划和多阶段训练，直接生成高分辨率的视频。<br />
<br />
6、质量和文本忠实度：与先前的模型相比，EMU VIDEO 在视频的质量和对原始文本描述的忠实度方面表现更佳。<br />
<br />
详细：<a href="https://emu-video.metademolab.com/">emu-video.metademolab.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU0NzA5Njc0NzUwODUzMTIvcHUvaW1nL0Fsby1UbE5VOHJqanhHRDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725470552318718046#m</id>
            <title>Meta AI发布了两款新的生成式AI模型：Emu Video和Emu Edit

Emu Video：是一个文本到视频生成模型，基于扩散模型，可以通过文本提示生成图像，然后基于文本和生成视频

Emu Edit：是一个指令驱动的图像编辑模型，可以通过指令进行自由形式编辑，包括局部和全局编辑、背景添加/移除、颜色和几何变换等。

与以往需要多个模型相比，Emu Video只用两个扩散模型就能生成512x512分辨率、四秒长、每秒16帧的视频。人类评估表明，相比于以往的方法，Emu Video在质量和忠实于文本提示方面得到了极高的评价。

Emu Edit的核心在于精确地只修改与编辑请求相关的像素，保持其他像素不变。为了训练这个模型，Meta开发了一个包含1000万合成样本的数据集，每个样本包括一个输入图像、任务描述和目标输出图像。

详细：https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725470552318718046#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725470552318718046#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 11:06:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta AI发布了两款新的生成式AI模型：Emu Video和Emu Edit<br />
<br />
Emu Video：是一个文本到视频生成模型，基于扩散模型，可以通过文本提示生成图像，然后基于文本和生成视频<br />
<br />
Emu Edit：是一个指令驱动的图像编辑模型，可以通过指令进行自由形式编辑，包括局部和全局编辑、背景添加/移除、颜色和几何变换等。<br />
<br />
与以往需要多个模型相比，Emu Video只用两个扩散模型就能生成512x512分辨率、四秒长、每秒16帧的视频。人类评估表明，相比于以往的方法，Emu Video在质量和忠实于文本提示方面得到了极高的评价。<br />
<br />
Emu Edit的核心在于精确地只修改与编辑请求相关的像素，保持其他像素不变。为了训练这个模型，Meta开发了一个包含1000万合成样本的数据集，每个样本包括一个输入图像、任务描述和目标输出图像。<br />
<br />
详细：<a href="https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/">ai.meta.com/blog/emu-text-to…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU0NDk1NTE5MDgzNjgzODQvcHUvaW1nL3RFWDlOWEdQMFpSWDFaNUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1725333564470812773#m</id>
            <title>RT by @xiaohuggg: #AI开源项目推荐：screenshot-to-code

借助GPT-4V，直接把屏幕截图生成HTML网页</title>
            <link>https://nitter.cz/dotey/status/1725333564470812773#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1725333564470812773#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 02:02:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：screenshot-to-code<br />
<br />
借助GPT-4V，直接把屏幕截图生成HTML网页</p>
<p><a href="https://nitter.cz/DevDminGod/status/1725175630029803538#m">nitter.cz/DevDminGod/status/1725175630029803538#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725364184798663107#m</id>
            <title>R to @xiaohuggg: 这个是详细介绍的+演示视频

完全是使用Text-to-speech Avatar制作的

虽然有点瑕疵但是我感觉还是很不错的</title>
            <link>https://nitter.cz/xiaohuggg/status/1725364184798663107#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725364184798663107#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 04:04:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是详细介绍的+演示视频<br />
<br />
完全是使用Text-to-speech Avatar制作的<br />
<br />
虽然有点瑕疵但是我感觉还是很不错的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjUzNjM4MDc5NDA0MTU0ODgvcHUvaW1nL0ZKWFl0ckVYdXl1OTI1dVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725363740974236000#m</id>
            <title>微软推出仅需文本内容即可转成真人视频的工具：Deepfakes Creator

这个工具被称为Azure AI Speech的文本到语音化身（text-to-speech avatar）

该工具允许用户通过文本输入生成逼真的会说话的真人视频。你只要上传想要化身模仿的人的照片，并写一个剧本。

支持多种语言，还可进行实时问答互动。

主要功能特点：

1、逼真化身视频生成：用户可上传照片和剧本，工具创建逼真的人物化身视频，模拟人物说话。

2、多语言支持和实时聊天机器人：化身支持多种语言，并能利用AI模型（如GPT）实时回应非剧本问题。

3、个性化语音功能：提供一分钟语音样本，快速复制用户声音，用于定制语音助手和内容配音。

4、法律和伦理保障：禁止使用预录制语音，要求用户明确同意，并通过注册使用特定用例。

5、水印技术：会在个人语音中自动添加水印，帮助识别AI合成语音，需同意Microsoft使用水印检测服务。

6、高效视频内容创作：简化传统视频制作流程，适用于制作培训视频、产品介绍等。

7、增强的数字互动体验：可用于构建对话代理、虚拟助手、聊天机器人，提供自然的交互式对话。

8、内容生成工作流程：包括文本分析、TTS音频合成和TTS化身视频合成，生成与声音同步的嘴唇动画。

9、预构建和定制化身选项：提供即用型预构建化身和可定制化身，后者通过用户上传的视频录像训练。

10、UI工具和API访问支持：在Azure AI Speech Studio提供了UI工具和API访问。

11、广泛应用场景：适用于创建各类吸引人的视频和交互式应用，提高沟通和信息传达效率。

详细内容以及API申请：https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-speech-announces-public-preview-of-text-to-speech/ba-p/3981448

视频演示的代码资源：

GitHub：https://github.com/Azure/gen-cv/tree/main/avatar/video</title>
            <link>https://nitter.cz/xiaohuggg/status/1725363740974236000#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725363740974236000#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 04:02:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软推出仅需文本内容即可转成真人视频的工具：Deepfakes Creator<br />
<br />
这个工具被称为Azure AI Speech的文本到语音化身（text-to-speech avatar）<br />
<br />
该工具允许用户通过文本输入生成逼真的会说话的真人视频。你只要上传想要化身模仿的人的照片，并写一个剧本。<br />
<br />
支持多种语言，还可进行实时问答互动。<br />
<br />
主要功能特点：<br />
<br />
1、逼真化身视频生成：用户可上传照片和剧本，工具创建逼真的人物化身视频，模拟人物说话。<br />
<br />
2、多语言支持和实时聊天机器人：化身支持多种语言，并能利用AI模型（如GPT）实时回应非剧本问题。<br />
<br />
3、个性化语音功能：提供一分钟语音样本，快速复制用户声音，用于定制语音助手和内容配音。<br />
<br />
4、法律和伦理保障：禁止使用预录制语音，要求用户明确同意，并通过注册使用特定用例。<br />
<br />
5、水印技术：会在个人语音中自动添加水印，帮助识别AI合成语音，需同意Microsoft使用水印检测服务。<br />
<br />
6、高效视频内容创作：简化传统视频制作流程，适用于制作培训视频、产品介绍等。<br />
<br />
7、增强的数字互动体验：可用于构建对话代理、虚拟助手、聊天机器人，提供自然的交互式对话。<br />
<br />
8、内容生成工作流程：包括文本分析、TTS音频合成和TTS化身视频合成，生成与声音同步的嘴唇动画。<br />
<br />
9、预构建和定制化身选项：提供即用型预构建化身和可定制化身，后者通过用户上传的视频录像训练。<br />
<br />
10、UI工具和API访问支持：在Azure AI Speech Studio提供了UI工具和API访问。<br />
<br />
11、广泛应用场景：适用于创建各类吸引人的视频和交互式应用，提高沟通和信息传达效率。<br />
<br />
详细内容以及API申请：<a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-speech-announces-public-preview-of-text-to-speech/ba-p/3981448">techcommunity.microsoft.com/…</a><br />
<br />
视频演示的代码资源：<br />
<br />
GitHub：<a href="https://github.com/Azure/gen-cv/tree/main/avatar/video">github.com/Azure/gen-cv/tree…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjUzNjI4MTA2NjA4MjcxMzYvcHUvaW1nLzZVZGZkaTRjYjhtRXl2NWkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725332525764198658#m</id>
            <title>R to @xiaohuggg: Google DeepMind和YouTube合作开发的音乐生成模型Lyria的一些关键信息：https://blog.youtube/inside-youtube/ai-and-music-experiment/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725332525764198658#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725332525764198658#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 01:58:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google DeepMind和YouTube合作开发的音乐生成模型Lyria的一些关键信息：<a href="https://blog.youtube/inside-youtube/ai-and-music-experiment/">blog.youtube/inside-youtube/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjUzMzE3MzAxODQ0ODI4MTYvcHUvaW1nL29XODRuM2FuMzRlUTA2T3MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725331516639908333#m</id>
            <title>R to @xiaohuggg: 演示2：将歌唱转变为管弦乐乐谱。</title>
            <link>https://nitter.cz/xiaohuggg/status/1725331516639908333#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725331516639908333#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 01:54:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示2：将歌唱转变为管弦乐乐谱。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjUzMzE0NzA2OTU0NDAzODQvcHUvaW1nL19pOVp2QjQ4ZFU0dDgtbFEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>