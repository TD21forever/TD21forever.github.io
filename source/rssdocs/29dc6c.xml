<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742719780828963272#m</id>
            <title>R to @xiaohuggg: 开发者介绍：</title>
            <link>https://nitter.cz/xiaohuggg/status/1742719780828963272#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742719780828963272#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 01:29:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开发者介绍：</p>
<p><a href="https://nitter.cz/tonyzzhao/status/1742603121682153852#m">nitter.cz/tonyzzhao/status/1742603121682153852#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742719653536006621#m</id>
            <title>Mobile ALOHA：一个可以模仿人类自主学习的机器人操作系统。

由坦福大学开发，专门设计用于执行需要双手和全身协调的复杂移动任务。

可以通过模仿学习（即观察人类操作然后模仿这些动作），仅通过50次任务演示，共同训练，它就能够自主完成日常生活中的各种任务。

如做饭、开柜放东西、自己坐电梯。

最重要的是：该机器人的软硬件全部都是开源的。

主要功能特点：

1、低成本全身远程操作系统：Mobile ALOHA是一个经济实惠的系统，它允许用户通过全身远程操作来收集数据。这种设计使得系统更易于普及和使用。

2、双手移动操作：该系统专注于模仿需要双手和全身控制的移动操作任务，这种能力在传统的桌面操作机器人中通常是缺失的。如烹饪、清洁或其他需要双手协作的活动。

3、自主模仿学习：Mobile ALOHA利用模仿学习技术，通过观察人类的演示来训练机器人执行复杂任务。通过每个任务50次演示，共同训练可以将成功率提高到90%，使Mobile ALOHA能够自主完成复杂的移动操作任务。

3、数据集共同训练：使用Mobile ALOHA收集的数据，研究团队进行了监督行为克隆，并发现与现有的静态ALOHA数据集共同训练可以提高移动操作任务的性能。

4、高成功率的任务执行：通过共同训练和模仿学习，Mobile ALOHA能够以高达90%的成功率自主完成复杂的移动操作任务。

5、多样化的应用场景：Mobile ALOHA能够执行多种复杂任务，如烹饪、打开柜门、操作电梯和清洁工作，展示了其广泛的应用潜力。

6、与物联网设备的兼容性：除了执行物理任务外，Mobile ALOHA还计划在2025年与物联网（IoT）设备连接，进一步扩展其应用范围。

Mobile ALOHA的硬件组成：

Mobile ALOHA被装在了一个为仓库设计的移动台座上：Tracer AGV。

它可以承载 100kg，移动速度高达 1.6m/s，而成本仅为 7k 美元，同时使得机器人的占地面积减少了 45%，重量减轻了 15 公斤。  机器人垂直高度可达65厘米至200厘米，距底座100厘米。

- 机械臂：Mobile ALOHA配备了两个机械臂，每个臂有多个自由度，使其能够执行复杂的双手操作任务。

- 移动基座：系统包括一个移动基座，使机器人能够在不同的环境中移动和定位。

- 摄像头：Mobile ALOHA配备了两个手腕摄像头和一个顶部摄像头，用于捕捉环境和操作任务的视觉信息。

- 自带电源和计算能力：系统具备自带的电源和计算能力，使其能夠独立完成任务而不依赖于外部电源或计算设备。

- 技术规格：
• 重量：75公斤。
• 尺寸：80宽 x 84长 x 140高厘米（不含操纵杆）；90宽 x 135长 x 140高厘米。
• 负载能力：每个手臂750克，基座55公斤。
• 手臂重复定位精度：1毫米。
• 手臂定位精度：5-8毫米。
• 电池寿命：12小时（1620Wh）。
• 最大拉力：100牛顿，垂直距离100厘米。
• 滚动阻力：13牛顿（乙烯基地板）。
        • 移动速度：可以达到人类正常步行的1.42米/秒

项目及演示：https://mobile-aloha.github.io/
学习代码：https://github.com/MarkFzp/act-plus-plus
硬件代码：https://github.com/MarkFzp/mobile-aloha
论文：https://mobile-aloha.github.io/
教程：https://docs.google.com/document/d/1_3yhWjodSNNYlpxkRCPIlvIAaQ76Nqk2wsqhnEVM6Dc</title>
            <link>https://nitter.cz/xiaohuggg/status/1742719653536006621#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742719653536006621#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 01:28:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mobile ALOHA：一个可以模仿人类自主学习的机器人操作系统。<br />
<br />
由坦福大学开发，专门设计用于执行需要双手和全身协调的复杂移动任务。<br />
<br />
可以通过模仿学习（即观察人类操作然后模仿这些动作），仅通过50次任务演示，共同训练，它就能够自主完成日常生活中的各种任务。<br />
<br />
如做饭、开柜放东西、自己坐电梯。<br />
<br />
最重要的是：该机器人的软硬件全部都是开源的。<br />
<br />
主要功能特点：<br />
<br />
1、低成本全身远程操作系统：Mobile ALOHA是一个经济实惠的系统，它允许用户通过全身远程操作来收集数据。这种设计使得系统更易于普及和使用。<br />
<br />
2、双手移动操作：该系统专注于模仿需要双手和全身控制的移动操作任务，这种能力在传统的桌面操作机器人中通常是缺失的。如烹饪、清洁或其他需要双手协作的活动。<br />
<br />
3、自主模仿学习：Mobile ALOHA利用模仿学习技术，通过观察人类的演示来训练机器人执行复杂任务。通过每个任务50次演示，共同训练可以将成功率提高到90%，使Mobile ALOHA能够自主完成复杂的移动操作任务。<br />
<br />
3、数据集共同训练：使用Mobile ALOHA收集的数据，研究团队进行了监督行为克隆，并发现与现有的静态ALOHA数据集共同训练可以提高移动操作任务的性能。<br />
<br />
4、高成功率的任务执行：通过共同训练和模仿学习，Mobile ALOHA能够以高达90%的成功率自主完成复杂的移动操作任务。<br />
<br />
5、多样化的应用场景：Mobile ALOHA能够执行多种复杂任务，如烹饪、打开柜门、操作电梯和清洁工作，展示了其广泛的应用潜力。<br />
<br />
6、与物联网设备的兼容性：除了执行物理任务外，Mobile ALOHA还计划在2025年与物联网（IoT）设备连接，进一步扩展其应用范围。<br />
<br />
Mobile ALOHA的硬件组成：<br />
<br />
Mobile ALOHA被装在了一个为仓库设计的移动台座上：Tracer AGV。<br />
<br />
它可以承载 100kg，移动速度高达 1.6m/s，而成本仅为 7k 美元，同时使得机器人的占地面积减少了 45%，重量减轻了 15 公斤。  机器人垂直高度可达65厘米至200厘米，距底座100厘米。<br />
<br />
- 机械臂：Mobile ALOHA配备了两个机械臂，每个臂有多个自由度，使其能够执行复杂的双手操作任务。<br />
<br />
- 移动基座：系统包括一个移动基座，使机器人能够在不同的环境中移动和定位。<br />
<br />
- 摄像头：Mobile ALOHA配备了两个手腕摄像头和一个顶部摄像头，用于捕捉环境和操作任务的视觉信息。<br />
<br />
- 自带电源和计算能力：系统具备自带的电源和计算能力，使其能夠独立完成任务而不依赖于外部电源或计算设备。<br />
<br />
- 技术规格：<br />
• 重量：75公斤。<br />
• 尺寸：80宽 x 84长 x 140高厘米（不含操纵杆）；90宽 x 135长 x 140高厘米。<br />
• 负载能力：每个手臂750克，基座55公斤。<br />
• 手臂重复定位精度：1毫米。<br />
• 手臂定位精度：5-8毫米。<br />
• 电池寿命：12小时（1620Wh）。<br />
• 最大拉力：100牛顿，垂直距离100厘米。<br />
• 滚动阻力：13牛顿（乙烯基地板）。<br />
        • 移动速度：可以达到人类正常步行的1.42米/秒<br />
<br />
项目及演示：<a href="https://mobile-aloha.github.io/">mobile-aloha.github.io/</a><br />
学习代码：<a href="https://github.com/MarkFzp/act-plus-plus">github.com/MarkFzp/act-plus-…</a><br />
硬件代码：<a href="https://github.com/MarkFzp/mobile-aloha">github.com/MarkFzp/mobile-al…</a><br />
论文：<a href="https://mobile-aloha.github.io/">mobile-aloha.github.io/</a><br />
教程：<a href="https://docs.google.com/document/d/1_3yhWjodSNNYlpxkRCPIlvIAaQ76Nqk2wsqhnEVM6Dc">docs.google.com/document/d/1…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3MTk0MTcwOTA0OTg1NjAvcHUvaW1nL0dWNVdSSGJLVF9oRllXUHguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742710713091788984#m</id>
            <title>SpaceX发射了首批六颗具备直接对接蜂窝网络（Direct to Cell）能力的Starlink卫星。

这将允许在地球上任何地方可以使用手机和Starlink卫星连接。

这些卫星配备了先进的调制解调器，使得全球各地的移动网络运营商能够使用Starlink提供无缝的全球通话和网络服务，而无需更换硬件或固件。
除了美国的T-Mobile外，其他几个国家的运营商也已签约使用这些直接对接蜂窝网络的卫星。SpaceX提到的其他运营商包括加拿大的Rogers、日本的KDDI、澳大利亚的Optus、新西兰的One NZ、瑞士的Salt以及智利和秘鲁的Entel。

尽管SpaceX首席执行官埃隆·马斯克（Elon Musk）表示这些卫星将“允许在地球上任何地方进行手机连接”，但他也描述了一个重要的带宽限制。马斯克写道：“注意，这只支持每个波束约7Mb的数据传输，而且波束非常大，所以虽然这对于没有蜂窝网络连接的地区来说是一个很好的解决方案，但它与现有的陆地蜂窝网络相比并没有显著的竞争力。”
Starlink的直接对接蜂窝网络服务预计将在2024年提供文本消息服务，随后在2025年开始提供语音和数据服务。Starlink的低地轨道卫星将与标准LTE手机兼容，不像早期服务需要专门为卫星通信设计的手机。SpaceX的直接对接蜂窝网络卫星还将在2025年与物联网（IoT）设备连接。
官网：http://direct.starlink.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1742710713091788984#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742710713091788984#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 00:53:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SpaceX发射了首批六颗具备直接对接蜂窝网络（Direct to Cell）能力的Starlink卫星。<br />
<br />
这将允许在地球上任何地方可以使用手机和Starlink卫星连接。<br />
<br />
这些卫星配备了先进的调制解调器，使得全球各地的移动网络运营商能够使用Starlink提供无缝的全球通话和网络服务，而无需更换硬件或固件。<br />
除了美国的T-Mobile外，其他几个国家的运营商也已签约使用这些直接对接蜂窝网络的卫星。SpaceX提到的其他运营商包括加拿大的Rogers、日本的KDDI、澳大利亚的Optus、新西兰的One NZ、瑞士的Salt以及智利和秘鲁的Entel。<br />
<br />
尽管SpaceX首席执行官埃隆·马斯克（Elon Musk）表示这些卫星将“允许在地球上任何地方进行手机连接”，但他也描述了一个重要的带宽限制。马斯克写道：“注意，这只支持每个波束约7Mb的数据传输，而且波束非常大，所以虽然这对于没有蜂窝网络连接的地区来说是一个很好的解决方案，但它与现有的陆地蜂窝网络相比并没有显著的竞争力。”<br />
Starlink的直接对接蜂窝网络服务预计将在2024年提供文本消息服务，随后在2025年开始提供语音和数据服务。Starlink的低地轨道卫星将与标准LTE手机兼容，不像早期服务需要专门为卫星通信设计的手机。SpaceX的直接对接蜂窝网络卫星还将在2025年与物联网（IoT）设备连接。<br />
官网：<a href="http://direct.starlink.com">direct.starlink.com</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5WThRTWIwQUFyel81LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5Wk1vRGFZQUF5dEl1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5Wlk2YmJBQUF5WmlDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742519700108808564#m</id>
            <title>比特币急速下挫

触及41000美元/枚，为12月18日来新低，跌幅一度超过10%。</title>
            <link>https://nitter.cz/xiaohuggg/status/1742519700108808564#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742519700108808564#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 12:14:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>比特币急速下挫<br />
<br />
触及41000美元/枚，为12月18日来新低，跌幅一度超过10%。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742473942252855795#m</id>
            <title>这下模型训练没有了版权和训练数据顾虑了🤓

微软研究团队最新成果：他们已经开始使用【合成数据】来训练AI模型了。

微软使用大语言模型生成了近100种语言、数十万个文本嵌入任务的“模拟”文本数据，然后用这些数据来训练 AI 。

这大幅度降低了训练成本，提高了效率，同时还减少了模型的偏见。

背景知识：

要让计算机理解和处理人类的语言，我们需要把语言（比如句子或段落）转换成计算机能理解的形式，这就是所谓的“文本嵌入”。文本嵌入就是把人类语言翻译成计算机的语言。

传统上，要让计算机做好这件事，我们需要给它看很多很多的例子（这就是所谓的训练数据），让它学习怎样把文本转换成它能理解的形式。但这个过程很复杂，需要很多数据和很长时间。

微软的这份论文提出了一种新方法：“合成数据”。

他们使用大语言模型（LLM）来生成了很多不同语言的“模拟”文本数据，然后用这些数据来训练 AI 理解人类语言。这样做的好处是，他们不需要真实的数据就能训练出很好的文本嵌入模型，而且这个过程比传统方法更快、更高效。

如何生成合成数据：

1、使用大语言模型：首先，他们利用了大型语言模型，如GPT-4或类似的高级模型。这些模型已经通过大量的文本数据进行了预训练，因此具有强大的语言生成能力。

2、任务定义和提示设计：研究团队定义了一系列文本嵌入任务，并为这些任务设计了特定的提示。这些提示被用来指导语言模型生成特定类型的文本。例如，他们可能会设计一个提示来生成关于某个特定主题的问答对，或者创建一个场景描述。

3、生成合成数据：接下来，研究团队使用这些提示来引导语言模型生成数据。模型根据给定的任务提示产生文本，这些文本涵盖了各种主题和风格。生成的文本是合成的，但质量足以模拟真实世界的语言使用情况。

4、多样性和覆盖率：为了确保生成的数据具有多样性并覆盖多种语言，研究团队可能会使用多种提示模板，并在多种语言中生成数据。这样可以确保模型不仅在资源丰富的语言（如英语）中表现良好，也能处理资源较少的语言。

5、数据清洗和格式化：生成的数据经过筛选和优化，确保质量和多样性。生成的数据需要经过清洗和格式化，以确保它们符合训练需要。这可能包括去除重复内容、修正格式错误等。

合成数据的优势：

通过这种方法，微软的研究团队能够生成大量高质量的合成数据，用于训练和改进大型语言模型，从而提高文本嵌入的质量。这种方法的优势在于它不依赖于大量的标注真实数据，从而减少了数据收集和处理的工作量，同时还能提供丰富多样的训练材料。

1、覆盖范围广：合成数据可以覆盖更广泛的场景和用例，包括那些在真实数据集中可能很少见或完全不存在的情况。这有助于模型学习更全面的语言模式和概念。这些数据覆盖了近100种语言的数十万个文本嵌入任务。这在传统数据收集方法中很难实现。

2、减少偏见：由于不依赖现实世界的数据集，合成数据可以减少因数据收集过程中的偏见和局限性而引入的问题。真实数据集可能包含偏见或不平衡（例如，某些群体的代表性不足）。通过合成数据，可以有意识地减少这些偏见，创建更公平和平衡的数据集。

3、灵活性和可扩展性：合成数据允许研究人员精确控制数据集的特性，如分布、复杂性和难度等，从而可以针对特定的研究或应用需求定制数据。因此生成合成数据的方法具有很高的灵活性，可以根据需要调整以生成各种类型的数据。

4、成本效率：收集和标注大量高质量的真实数据非常昂贵且耗时。相比之下，生成合成数据的成本通常更低，且过程更快。

5、快速迭代和改进：合成数据的生成过程可以根据模型性能的反馈快速调整，从而支持更快的迭代和改进。

6、隐私和安全：使用合成数据可以避免处理敏感或个人数据，从而减少隐私和安全风险。

实验结果表明：

1、数据生成统计：研究团队成功生成了大约50万个示例，其中包含15万个独特的指令。这些数据涵盖了93种不同的语言，其中英语占主导地位。

2、模型性能：在多种语言的MIRACL数据集上，使用合成数据训练的模型（E5mistral-7b）在nDCG@10和Recall@100两个指标上表现出色。这表明模型能够有效地检索相关文档，并且在多种语言上都有良好的表现。

3、对比训练的影响：在包含对比预训练的设置下，模型在多个数据集上的表现有所提升。这说明对比预训练对于提高模型性能是有益的。

4、多任务适应性：模型在多种任务类型上表现良好，包括文本检索、文本聚类、句子嵌入等，显示了其广泛的适用性。

这些实验结果表明，使用合成数据训练的大型语言模型在多语言、多任务场景中都能取得优异的性能，证明了合成数据方法的有效性和实用性。

论文：https://arxiv.org/abs/2401.00368
PDF：https://arxiv.org/pdf/2401.00368.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1742473942252855795#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742473942252855795#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:12:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这下模型训练没有了版权和训练数据顾虑了🤓<br />
<br />
微软研究团队最新成果：他们已经开始使用【合成数据】来训练AI模型了。<br />
<br />
微软使用大语言模型生成了近100种语言、数十万个文本嵌入任务的“模拟”文本数据，然后用这些数据来训练 AI 。<br />
<br />
这大幅度降低了训练成本，提高了效率，同时还减少了模型的偏见。<br />
<br />
背景知识：<br />
<br />
要让计算机理解和处理人类的语言，我们需要把语言（比如句子或段落）转换成计算机能理解的形式，这就是所谓的“文本嵌入”。文本嵌入就是把人类语言翻译成计算机的语言。<br />
<br />
传统上，要让计算机做好这件事，我们需要给它看很多很多的例子（这就是所谓的训练数据），让它学习怎样把文本转换成它能理解的形式。但这个过程很复杂，需要很多数据和很长时间。<br />
<br />
微软的这份论文提出了一种新方法：“合成数据”。<br />
<br />
他们使用大语言模型（LLM）来生成了很多不同语言的“模拟”文本数据，然后用这些数据来训练 AI 理解人类语言。这样做的好处是，他们不需要真实的数据就能训练出很好的文本嵌入模型，而且这个过程比传统方法更快、更高效。<br />
<br />
如何生成合成数据：<br />
<br />
1、使用大语言模型：首先，他们利用了大型语言模型，如GPT-4或类似的高级模型。这些模型已经通过大量的文本数据进行了预训练，因此具有强大的语言生成能力。<br />
<br />
2、任务定义和提示设计：研究团队定义了一系列文本嵌入任务，并为这些任务设计了特定的提示。这些提示被用来指导语言模型生成特定类型的文本。例如，他们可能会设计一个提示来生成关于某个特定主题的问答对，或者创建一个场景描述。<br />
<br />
3、生成合成数据：接下来，研究团队使用这些提示来引导语言模型生成数据。模型根据给定的任务提示产生文本，这些文本涵盖了各种主题和风格。生成的文本是合成的，但质量足以模拟真实世界的语言使用情况。<br />
<br />
4、多样性和覆盖率：为了确保生成的数据具有多样性并覆盖多种语言，研究团队可能会使用多种提示模板，并在多种语言中生成数据。这样可以确保模型不仅在资源丰富的语言（如英语）中表现良好，也能处理资源较少的语言。<br />
<br />
5、数据清洗和格式化：生成的数据经过筛选和优化，确保质量和多样性。生成的数据需要经过清洗和格式化，以确保它们符合训练需要。这可能包括去除重复内容、修正格式错误等。<br />
<br />
合成数据的优势：<br />
<br />
通过这种方法，微软的研究团队能够生成大量高质量的合成数据，用于训练和改进大型语言模型，从而提高文本嵌入的质量。这种方法的优势在于它不依赖于大量的标注真实数据，从而减少了数据收集和处理的工作量，同时还能提供丰富多样的训练材料。<br />
<br />
1、覆盖范围广：合成数据可以覆盖更广泛的场景和用例，包括那些在真实数据集中可能很少见或完全不存在的情况。这有助于模型学习更全面的语言模式和概念。这些数据覆盖了近100种语言的数十万个文本嵌入任务。这在传统数据收集方法中很难实现。<br />
<br />
2、减少偏见：由于不依赖现实世界的数据集，合成数据可以减少因数据收集过程中的偏见和局限性而引入的问题。真实数据集可能包含偏见或不平衡（例如，某些群体的代表性不足）。通过合成数据，可以有意识地减少这些偏见，创建更公平和平衡的数据集。<br />
<br />
3、灵活性和可扩展性：合成数据允许研究人员精确控制数据集的特性，如分布、复杂性和难度等，从而可以针对特定的研究或应用需求定制数据。因此生成合成数据的方法具有很高的灵活性，可以根据需要调整以生成各种类型的数据。<br />
<br />
4、成本效率：收集和标注大量高质量的真实数据非常昂贵且耗时。相比之下，生成合成数据的成本通常更低，且过程更快。<br />
<br />
5、快速迭代和改进：合成数据的生成过程可以根据模型性能的反馈快速调整，从而支持更快的迭代和改进。<br />
<br />
6、隐私和安全：使用合成数据可以避免处理敏感或个人数据，从而减少隐私和安全风险。<br />
<br />
实验结果表明：<br />
<br />
1、数据生成统计：研究团队成功生成了大约50万个示例，其中包含15万个独特的指令。这些数据涵盖了93种不同的语言，其中英语占主导地位。<br />
<br />
2、模型性能：在多种语言的MIRACL数据集上，使用合成数据训练的模型（E5mistral-7b）在nDCG@10和Recall@100两个指标上表现出色。这表明模型能够有效地检索相关文档，并且在多种语言上都有良好的表现。<br />
<br />
3、对比训练的影响：在包含对比预训练的设置下，模型在多个数据集上的表现有所提升。这说明对比预训练对于提高模型性能是有益的。<br />
<br />
4、多任务适应性：模型在多种任务类型上表现良好，包括文本检索、文本聚类、句子嵌入等，显示了其广泛的适用性。<br />
<br />
这些实验结果表明，使用合成数据训练的大型语言模型在多语言、多任务场景中都能取得优异的性能，证明了合成数据方法的有效性和实用性。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.00368">arxiv.org/abs/2401.00368</a><br />
PDF：<a href="https://arxiv.org/pdf/2401.00368.pdf">arxiv.org/pdf/2401.00368.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M2Q2ZpOWFrQUF5T2hmLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742427654006202439#m</id>
            <title>兄弟们，这个好！

Pile：一款开源的界面非常整洁美观的AI日记软件

可以帮助你撰写和保存日记条目，记录你的思考和经历，当备忘录也可以！

内置了OpenAI 的API功能，可以自己写提示词让AI帮你扩展你的想法和日记。

还可以使用AI来搜索日记内容或对整个日记提出问题。

开源、安全、隐私！

下载：https://udara.io/pile/
GitHub：https://github.com/UdaraJay/Pile
作者：@TGUPJ</title>
            <link>https://nitter.cz/xiaohuggg/status/1742427654006202439#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742427654006202439#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 06:08:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，这个好！<br />
<br />
Pile：一款开源的界面非常整洁美观的AI日记软件<br />
<br />
可以帮助你撰写和保存日记条目，记录你的思考和经历，当备忘录也可以！<br />
<br />
内置了OpenAI 的API功能，可以自己写提示词让AI帮你扩展你的想法和日记。<br />
<br />
还可以使用AI来搜索日记内容或对整个日记提出问题。<br />
<br />
开源、安全、隐私！<br />
<br />
下载：<a href="https://udara.io/pile/">udara.io/pile/</a><br />
GitHub：<a href="https://github.com/UdaraJay/Pile">github.com/UdaraJay/Pile</a><br />
作者：<a href="https://nitter.cz/TGUPJ" title="Udara">@TGUPJ</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0MTgyNDg3MjQwNjIyMDgvcHUvaW1nL0RYZXlfYXVmSzR1ejkyUnYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742410353198416282#m</id>
            <title>VCoder：大语言模型的眼睛

VCoder的一个视觉编码器，能够帮助MLLM更好地理解和分析图像内容。提高模型在识别图像中的对象、理解图像场景方面的能力。

它可以帮助模型显示图片中不同物体的轮廓或深度图（显示物体距离相机的远近）。还能更准确的理解图片中的物体是什么，甚至能数出图片中有多少人。

它的功能包括：

1、增强视觉感知能力：VCoder通过提供额外的视觉编码器，帮助MLLM更好地理解和分析图像内容。

2、处理特殊类型的图像：VCoder能够处理分割图和深度图等特殊类型的图像。分割图可以帮助模型识别和理解图像中不同物体的边界和形状，而深度图则提供了物体距离相机远近的信息。

3、改善对象感知任务：VCoder通过提供额外的感知模态输入（如分割图或深度图）显著提高了MLLMs的对象感知能力。这包括更准确地识别和计数图像中的对象。

实验结果：

VCoder与开源的多模态LLMs（如MiniGPT-4、InstructBLIP、LLaVA-1.5和CogVLM）进行了比较，并在COST验证集上进行了测试。

VCoder在对象识别任务中表现最佳，特别是在对象计数和识别方面优于基线模型。

在处理复杂场景中的对象计数和识别任务时，VCoder展示了更高的准确性，尤其是在场景中有许多实体时。

对比GPT-4V：实验表明，GPT-4V在所有对象识别任务中的表现一致，但在与VCoder的比较中，GPT-4V在对象级感知方面落后于VCoder。

项目及演示：https://praeclarumjj3.github.io/vcoder/
论文：https://arxiv.org/abs/2312.14233
GitHub：https://github.com/SHI-Labs/VCoder
在线演示：https://huggingface.co/spaces/shi-labs/VCoder</title>
            <link>https://nitter.cz/xiaohuggg/status/1742410353198416282#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742410353198416282#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 04:59:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VCoder：大语言模型的眼睛<br />
<br />
VCoder的一个视觉编码器，能够帮助MLLM更好地理解和分析图像内容。提高模型在识别图像中的对象、理解图像场景方面的能力。<br />
<br />
它可以帮助模型显示图片中不同物体的轮廓或深度图（显示物体距离相机的远近）。还能更准确的理解图片中的物体是什么，甚至能数出图片中有多少人。<br />
<br />
它的功能包括：<br />
<br />
1、增强视觉感知能力：VCoder通过提供额外的视觉编码器，帮助MLLM更好地理解和分析图像内容。<br />
<br />
2、处理特殊类型的图像：VCoder能够处理分割图和深度图等特殊类型的图像。分割图可以帮助模型识别和理解图像中不同物体的边界和形状，而深度图则提供了物体距离相机远近的信息。<br />
<br />
3、改善对象感知任务：VCoder通过提供额外的感知模态输入（如分割图或深度图）显著提高了MLLMs的对象感知能力。这包括更准确地识别和计数图像中的对象。<br />
<br />
实验结果：<br />
<br />
VCoder与开源的多模态LLMs（如MiniGPT-4、InstructBLIP、LLaVA-1.5和CogVLM）进行了比较，并在COST验证集上进行了测试。<br />
<br />
VCoder在对象识别任务中表现最佳，特别是在对象计数和识别方面优于基线模型。<br />
<br />
在处理复杂场景中的对象计数和识别任务时，VCoder展示了更高的准确性，尤其是在场景中有许多实体时。<br />
<br />
对比GPT-4V：实验表明，GPT-4V在所有对象识别任务中的表现一致，但在与VCoder的比较中，GPT-4V在对象级感知方面落后于VCoder。<br />
<br />
项目及演示：<a href="https://praeclarumjj3.github.io/vcoder/">praeclarumjj3.github.io/vcod…</a><br />
论文：<a href="https://arxiv.org/abs/2312.14233">arxiv.org/abs/2312.14233</a><br />
GitHub：<a href="https://github.com/SHI-Labs/VCoder">github.com/SHI-Labs/VCoder</a><br />
在线演示：<a href="https://huggingface.co/spaces/shi-labs/VCoder">huggingface.co/spaces/shi-la…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0MDg5ODAyOTM2MTU2MTYvcHUvaW1nLzhvV1dZOGVVdWd3SVJRbnouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742392686970331154#m</id>
            <title>R to @xiaohuggg: MUSIC ORIENTED DATASETS

 面向音乐的数据集：https://crypto-code.github.io/M2UGen-Demo/#datagen</title>
            <link>https://nitter.cz/xiaohuggg/status/1742392686970331154#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742392686970331154#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 03:49:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MUSIC ORIENTED DATASETS<br />
<br />
 面向音乐的数据集：<a href="https://crypto-code.github.io/M2UGen-Demo/#datagen">crypto-code.github.io/M2UGen…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M0NHhFTmJBQUFvcUdOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742392489766707632#m</id>
            <title>R to @xiaohuggg: M2UGen演示视频：</title>
            <link>https://nitter.cz/xiaohuggg/status/1742392489766707632#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742392489766707632#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 03:48:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>M2UGen演示视频：</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQyMzkyMjUyMDY3MTY4MjU2L2ltZy96dGdqcVZ2WHJVVkJvU2IzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742392202482061509#m</id>
            <title>兄弟们，这个模型很强大！👍🏻

M2UGen：多模态音乐理解和生成模型

该模型由腾讯与新加坡国立大学开发，M2UGen能够理解各种音乐，包括风格、演奏乐器、表达的情绪情感等，并进行音乐问答。

而且还能根据文本、图像、视频和音频生成各种音乐，同时对生成的音乐也能理解并根据文字描述对音乐进行编辑。

M2UGen 的主要功能：

- 音乐问答：M2UGen 能够理解不同类型的音乐，包括它们的风格、使用的乐器、表达的情绪和情感等。然后根据提出的问题，模型能够理解并回答与音乐相关的查询。

- 文本到音乐生成：用户可以输入文本，模型会根据这些文本生成相应的音乐。

- 图像到音乐生成：模型能够根据提供的图片内容生成匹配的音乐。

-视频到音乐生成：根据视频内容，模型能理解视频的主要内容，并生成相应的音乐。

- 音乐编辑：用户可以对已生成的音乐进行编辑，例如改变乐器、调整节奏等，而且只需要通过文本描述即可。

M2UGen 使用了多种编码器，包括用于音乐理解的 MERT、用于图像理解的 ViT 和用于视频理解的 ViViT，以及作为音乐生成模型（音乐解码器）的 MusicGen/AudioLDM2 模型。

此外，该模型还结合了适配器和 LLaMA 2 模型。

工作原理：

1、多模态输入处理：M2UGen能够处理多种类型的输入，包括文本、图像、视频和音频。

它使用特定的编码器来理解不同的输入模态。例如，使用MERT模型处理音乐输入，ViT模型处理图像输入，ViViT模型处理视频输入。

2、音乐理解：利用LLaMA 2模型，M2UGen能够理解音乐的各个方面，如风格、乐器使用和情感表达。它能够对音乐相关的问题进行回答，这涉及到对音乐内容的深入理解。

3、音乐生成：M2UGen不仅能理解音乐，还能根据不同的输入生成音乐。它探索使用AudioLDM 2和MusicGen等模型来根据文本、图像或视频输入生成音乐。

4、数据集生成与训练：为了训练M2UGen，开发者使用了MU-LLaMA和MPT-7B模型来生成大量的多模态音乐配对数据集。这些数据集帮助M2UGen学习如何从不同的输入中提取信息并生成相应的音乐。

项目及演示：https://crypto-code.github.io/M2UGen-Demo/
论文：https://arxiv.org/abs/2311.11255
GitHub：https://github.com/shansongliu/M2UGen</title>
            <link>https://nitter.cz/xiaohuggg/status/1742392202482061509#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742392202482061509#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 03:47:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，这个模型很强大！👍🏻<br />
<br />
M2UGen：多模态音乐理解和生成模型<br />
<br />
该模型由腾讯与新加坡国立大学开发，M2UGen能够理解各种音乐，包括风格、演奏乐器、表达的情绪情感等，并进行音乐问答。<br />
<br />
而且还能根据文本、图像、视频和音频生成各种音乐，同时对生成的音乐也能理解并根据文字描述对音乐进行编辑。<br />
<br />
M2UGen 的主要功能：<br />
<br />
- 音乐问答：M2UGen 能够理解不同类型的音乐，包括它们的风格、使用的乐器、表达的情绪和情感等。然后根据提出的问题，模型能够理解并回答与音乐相关的查询。<br />
<br />
- 文本到音乐生成：用户可以输入文本，模型会根据这些文本生成相应的音乐。<br />
<br />
- 图像到音乐生成：模型能够根据提供的图片内容生成匹配的音乐。<br />
<br />
-视频到音乐生成：根据视频内容，模型能理解视频的主要内容，并生成相应的音乐。<br />
<br />
- 音乐编辑：用户可以对已生成的音乐进行编辑，例如改变乐器、调整节奏等，而且只需要通过文本描述即可。<br />
<br />
M2UGen 使用了多种编码器，包括用于音乐理解的 MERT、用于图像理解的 ViT 和用于视频理解的 ViViT，以及作为音乐生成模型（音乐解码器）的 MusicGen/AudioLDM2 模型。<br />
<br />
此外，该模型还结合了适配器和 LLaMA 2 模型。<br />
<br />
工作原理：<br />
<br />
1、多模态输入处理：M2UGen能够处理多种类型的输入，包括文本、图像、视频和音频。<br />
<br />
它使用特定的编码器来理解不同的输入模态。例如，使用MERT模型处理音乐输入，ViT模型处理图像输入，ViViT模型处理视频输入。<br />
<br />
2、音乐理解：利用LLaMA 2模型，M2UGen能够理解音乐的各个方面，如风格、乐器使用和情感表达。它能够对音乐相关的问题进行回答，这涉及到对音乐内容的深入理解。<br />
<br />
3、音乐生成：M2UGen不仅能理解音乐，还能根据不同的输入生成音乐。它探索使用AudioLDM 2和MusicGen等模型来根据文本、图像或视频输入生成音乐。<br />
<br />
4、数据集生成与训练：为了训练M2UGen，开发者使用了MU-LLaMA和MPT-7B模型来生成大量的多模态音乐配对数据集。这些数据集帮助M2UGen学习如何从不同的输入中提取信息并生成相应的音乐。<br />
<br />
项目及演示：<a href="https://crypto-code.github.io/M2UGen-Demo/">crypto-code.github.io/M2UGen…</a><br />
论文：<a href="https://arxiv.org/abs/2311.11255">arxiv.org/abs/2311.11255</a><br />
GitHub：<a href="https://github.com/shansongliu/M2UGen">github.com/shansongliu/M2UGe…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDIzODg0NDkwNTM5MjEyODAvcHUvaW1nL0g3bGpPWktGQzRVNllXa0wuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742382786990969226#m</id>
            <title>DreamTalk代码已经发布

现在可以开动了

能根据音频让人物头像照片说话、唱歌同时保持嘴型和表情一致。

GitHub：https://github.com/ali-vilab/dreamtalk

HuggingFace：https://huggingface.co/damo-vilab/dreamtalk</title>
            <link>https://nitter.cz/xiaohuggg/status/1742382786990969226#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742382786990969226#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 03:10:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTalk代码已经发布<br />
<br />
现在可以开动了<br />
<br />
能根据音频让人物头像照片说话、唱歌同时保持嘴型和表情一致。<br />
<br />
GitHub：<a href="https://github.com/ali-vilab/dreamtalk">github.com/ali-vilab/dreamta…</a><br />
<br />
HuggingFace：<a href="https://huggingface.co/damo-vilab/dreamtalk">huggingface.co/damo-vilab/dr…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1736627340623692177#m">nitter.cz/xiaohuggg/status/1736627340623692177#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0MjA5NjU1NjY2MzQyNzA3Mi9KZGpXNHduRj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742377903818711044#m</id>
            <title>SVG-Loaders：纯SVG格式的加载图标和小动画。

- 免费下载：这些图标和动画可以为网页和应用提供视觉上吸引人的加载提示。

- 纯SVG格式：所有的加载图标和动画都是用SVG（可缩放矢量图形）格式制作的，确保了高质量和可伸缩性。

- 多样的设计：项目包括多种不同风格和设计的加载图标，适用于各种界面和应用场景。

- 易于使用：用户可以直接下载或通过Bower安装SVG-Loaders，并将其集成到自己的项目中。

- 自定义颜色：用户可以通过修改SVG文件中的填充属性来改变图标的颜色。

GitHub：https://github.com/SamHerbert/SVG-Loaders</title>
            <link>https://nitter.cz/xiaohuggg/status/1742377903818711044#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742377903818711044#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 02:50:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SVG-Loaders：纯SVG格式的加载图标和小动画。<br />
<br />
- 免费下载：这些图标和动画可以为网页和应用提供视觉上吸引人的加载提示。<br />
<br />
- 纯SVG格式：所有的加载图标和动画都是用SVG（可缩放矢量图形）格式制作的，确保了高质量和可伸缩性。<br />
<br />
- 多样的设计：项目包括多种不同风格和设计的加载图标，适用于各种界面和应用场景。<br />
<br />
- 易于使用：用户可以直接下载或通过Bower安装SVG-Loaders，并将其集成到自己的项目中。<br />
<br />
- 自定义颜色：用户可以通过修改SVG文件中的填充属性来改变图标的颜色。<br />
<br />
GitHub：<a href="https://github.com/SamHerbert/SVG-Loaders">github.com/SamHerbert/SVG-Lo…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDIzNzczNTg4MDI0OTc1MzYvcHUvaW1nL2gtYk5PbFU2TGo4R0NDeDkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742375554517791132#m</id>
            <title>相比而言

我们是安全第一，什么都要备案😀

尽管中国拥有14亿人口，但在AI用户数量上并未进入前20名。这可能是因为中国的科技巨头开发了自己的本土语言AI工具，同时中国对AI进行了严格的监管。在分析的前50个AI工具中，中国排名第47位。</title>
            <link>https://nitter.cz/xiaohuggg/status/1742375554517791132#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742375554517791132#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 02:41:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>相比而言<br />
<br />
我们是安全第一，什么都要备案😀<br />
<br />
尽管中国拥有14亿人口，但在AI用户数量上并未进入前20名。这可能是因为中国的科技巨头开发了自己的本土语言AI工具，同时中国对AI进行了严格的监管。在分析的前50个AI工具中，中国排名第47位。</p>
<p><a href="https://nitter.cz/dotey/status/1742229424425037947#m">nitter.cz/dotey/status/1742229424425037947#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742373301929091472#m</id>
            <title>这些人速度真快，666

1月1号米老鼠版权不是过期了嘛，任何人都可以使用

然后现在米老鼠的SD模型已经出来了 

Mickey-1928：一个基于Stable-Diffusion-xl的微调版本，专门训练用于生成米老鼠、米妮和皮特的图像。

模型使用了来自1928年公共领域的96张静止画面训练，目的是生成符合1928年设计风格的米老鼠、米妮和皮特的图像。

数据集来源：数据集包括来自三部米老鼠卡通的静止画面，分别是《Gallopin' Gaucho》（40张彩色静止画面）、《Plane Crazy》（22张静止画面）和《Steamboat Willie》（34张静止画面）。

模型作者：@Dorialexander

模型下载：https://huggingface.co/Pclanglais/Mickey-1928

在线体验：https://huggingface.co/spaces/shewster/Pclanglais-Mickey-1928</title>
            <link>https://nitter.cz/xiaohuggg/status/1742373301929091472#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742373301929091472#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 02:32:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这些人速度真快，666<br />
<br />
1月1号米老鼠版权不是过期了嘛，任何人都可以使用<br />
<br />
然后现在米老鼠的SD模型已经出来了 <br />
<br />
Mickey-1928：一个基于Stable-Diffusion-xl的微调版本，专门训练用于生成米老鼠、米妮和皮特的图像。<br />
<br />
模型使用了来自1928年公共领域的96张静止画面训练，目的是生成符合1928年设计风格的米老鼠、米妮和皮特的图像。<br />
<br />
数据集来源：数据集包括来自三部米老鼠卡通的静止画面，分别是《Gallopin' Gaucho》（40张彩色静止画面）、《Plane Crazy》（22张静止画面）和《Steamboat Willie》（34张静止画面）。<br />
<br />
模型作者：<a href="https://nitter.cz/Dorialexander" title="Alexander Doria">@Dorialexander</a><br />
<br />
模型下载：<a href="https://huggingface.co/Pclanglais/Mickey-1928">huggingface.co/Pclanglais/Mi…</a><br />
<br />
在线体验：<a href="https://huggingface.co/spaces/shewster/Pclanglais-Mickey-1928">huggingface.co/spaces/shewst…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1742010140113674467#m">nitter.cz/xiaohuggg/status/1742010140113674467#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M0bWhEYmJRQUFVdnVyLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M0bWpReWJjQUFXRVdLLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M0bWxYUWFjQUEyOEd5LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>