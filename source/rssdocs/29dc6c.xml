<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757433016010609137#m</id>
            <title>R to @xiaohuggg: 在线体验：https://huggingface.co/spaces/multimodalart/stable-cascade

作者：@multimodalart</title>
            <link>https://nitter.cz/xiaohuggg/status/1757433016010609137#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757433016010609137#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:54:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在线体验：<a href="https://huggingface.co/spaces/multimodalart/stable-cascade">huggingface.co/spaces/multim…</a><br />
<br />
作者：<a href="https://nitter.cz/multimodalart" title="apolinario (multimodal.art)">@multimodalart</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3NDMyOTU1MzE5MDA1MTg0L2ltZy9nLTlnOWxpdVFMTUVXa0s3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757431125579047408#m</id>
            <title>Stability AI 推出推出一种全新的图像生成模型：Stable Cascade

Stable Cascade特别的地方在于它通过一个“三阶段方法”的来让图片生成变得更高质量、更灵活，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求！

而且与SD相比训练成本降低16倍！

通俗来讲它通过一个叫做“三步走”的过程来完成：

1.第一步，它先把你的文字转换成一种叫做“潜在表示”的小图块，可以想象成一个非常模糊的小图像草稿。

2.第二步，通过两个阶段（我们叫它Stage A和B）把这个模糊的草稿变成一个清晰的高分辨率图像。这就像是先画出一个大概的草图，然后再逐步细化，最后上色，使其变得栩栩如生。

3.第三步，就是让这个过程变得高效且灵活。Stable Cascade能够让你只对文字转换那部分（第一步）进行调整和改善，而不需要每次都重新画整个图像。这样不仅节省了大量时间，还大大降低了需要的计算资源。

而且，它能根据不同的需要，使用不同“大小”的模型来完成任务。如果你想要更高质量的图像，可以选择“更大”的模型；如果你的电脑配置较低，也有“更小”的模型可供选择，这样就能确保每个人都能使用这项技术。

Stable Cascade在制作图像时，不仅仅关注图像看起来是否美观，还会确保图像与你输入的文字尽可能吻合。这意味着，它能够理解你的文字，然后创造出与之相匹配的场景。

核心特点和技术细节：

• 潜在生成阶段（Stage C）：将用户输入转换为紧凑的24x24潜在表示，这些表示随后传递给潜在解码阶段（Stage A &amp; B）。这一阶段用于压缩图像，与Stable Diffusion中的VAE（变分自编码器）的作用相似，但实现了更高的压缩率。

• 解码与高分辨率生成（Stage A &amp; B）：通过将文本条件生成（Stage C）与解码到高分辨率像素空间（Stage A &amp; B）解耦，允许在Stage C上单独完成额外的训练或微调，包括ControlNets和LoRAs。这比训练类似大小的Stable Diffusion模型降低了16倍的成本。

模型参数：

Stage C有两种不同的模型，分别为10亿（1B）和36亿（3.6B）参数；

Stage B则为7亿（700M）和15亿（1.5B）参数。推荐使用3.6B参数的Stage C模型，因为这个模型输出的质量最高。但对于那些希望关注最低硬件要求的用户，可以使用1B参数版本。对于Stage B，两者都能获得出色的结果，但15亿参数的模型在重建细节方面表现更佳。

性能比较和用户体验：

• 性能比较：在多个模型比较中，Stable Cascade在提示对齐和美学质量方面表现最佳。人类评估使用了混合的部分提示和美学提示来显示结果。

• 推理速度和VRAM要求：Stable Cascade的推理速度与其他模型（如SDXL，Playground v2等）进行了比较，显示了其在推理速度上的优势。预期的VRAM需求约为20GB，但通过使用较小的变种可以进一步降低，尽管这可能会降低最终输出的质量。

高效的训练和微调：

通过将文本条件生成与解码到高分辨率像素空间的过程解耦，Stable Cascade允许在Stage C进行额外的训练或微调，包括ControlNets和LoRAs，实现了与训练类似大小的Stable Diffusion模型相比16倍的成本降低。

总之：Stable Cascade通过其模块化方法和创新的三阶段处理，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求，使更多的用户能够访问和利用先进的文本到图像生成技术。

详细：https://stability.ai/news/introducing-stable-cascade

GitHub：https://github.com/Stability-AI/StableCascade

HuggingFace：https://github.com/Stability-AI/StableCascade</title>
            <link>https://nitter.cz/xiaohuggg/status/1757431125579047408#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757431125579047408#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 15:46:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI 推出推出一种全新的图像生成模型：Stable Cascade<br />
<br />
Stable Cascade特别的地方在于它通过一个“三阶段方法”的来让图片生成变得更高质量、更灵活，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求！<br />
<br />
而且与SD相比训练成本降低16倍！<br />
<br />
通俗来讲它通过一个叫做“三步走”的过程来完成：<br />
<br />
1.第一步，它先把你的文字转换成一种叫做“潜在表示”的小图块，可以想象成一个非常模糊的小图像草稿。<br />
<br />
2.第二步，通过两个阶段（我们叫它Stage A和B）把这个模糊的草稿变成一个清晰的高分辨率图像。这就像是先画出一个大概的草图，然后再逐步细化，最后上色，使其变得栩栩如生。<br />
<br />
3.第三步，就是让这个过程变得高效且灵活。Stable Cascade能够让你只对文字转换那部分（第一步）进行调整和改善，而不需要每次都重新画整个图像。这样不仅节省了大量时间，还大大降低了需要的计算资源。<br />
<br />
而且，它能根据不同的需要，使用不同“大小”的模型来完成任务。如果你想要更高质量的图像，可以选择“更大”的模型；如果你的电脑配置较低，也有“更小”的模型可供选择，这样就能确保每个人都能使用这项技术。<br />
<br />
Stable Cascade在制作图像时，不仅仅关注图像看起来是否美观，还会确保图像与你输入的文字尽可能吻合。这意味着，它能够理解你的文字，然后创造出与之相匹配的场景。<br />
<br />
核心特点和技术细节：<br />
<br />
• 潜在生成阶段（Stage C）：将用户输入转换为紧凑的24x24潜在表示，这些表示随后传递给潜在解码阶段（Stage A & B）。这一阶段用于压缩图像，与Stable Diffusion中的VAE（变分自编码器）的作用相似，但实现了更高的压缩率。<br />
<br />
• 解码与高分辨率生成（Stage A & B）：通过将文本条件生成（Stage C）与解码到高分辨率像素空间（Stage A & B）解耦，允许在Stage C上单独完成额外的训练或微调，包括ControlNets和LoRAs。这比训练类似大小的Stable Diffusion模型降低了16倍的成本。<br />
<br />
模型参数：<br />
<br />
Stage C有两种不同的模型，分别为10亿（1B）和36亿（3.6B）参数；<br />
<br />
Stage B则为7亿（700M）和15亿（1.5B）参数。推荐使用3.6B参数的Stage C模型，因为这个模型输出的质量最高。但对于那些希望关注最低硬件要求的用户，可以使用1B参数版本。对于Stage B，两者都能获得出色的结果，但15亿参数的模型在重建细节方面表现更佳。<br />
<br />
性能比较和用户体验：<br />
<br />
• 性能比较：在多个模型比较中，Stable Cascade在提示对齐和美学质量方面表现最佳。人类评估使用了混合的部分提示和美学提示来显示结果。<br />
<br />
• 推理速度和VRAM要求：Stable Cascade的推理速度与其他模型（如SDXL，Playground v2等）进行了比较，显示了其在推理速度上的优势。预期的VRAM需求约为20GB，但通过使用较小的变种可以进一步降低，尽管这可能会降低最终输出的质量。<br />
<br />
高效的训练和微调：<br />
<br />
通过将文本条件生成与解码到高分辨率像素空间的过程解耦，Stable Cascade允许在Stage C进行额外的训练或微调，包括ControlNets和LoRAs，实现了与训练类似大小的Stable Diffusion模型相比16倍的成本降低。<br />
<br />
总之：Stable Cascade通过其模块化方法和创新的三阶段处理，不仅在美学和功能性上设定了新的标准，还大大降低了对高端硬件的需求，使更多的用户能够访问和利用先进的文本到图像生成技术。<br />
<br />
详细：<a href="https://stability.ai/news/introducing-stable-cascade">stability.ai/news/introducin…</a><br />
<br />
GitHub：<a href="https://github.com/Stability-AI/StableCascade">github.com/Stability-AI/Stab…</a><br />
<br />
HuggingFace：<a href="https://github.com/Stability-AI/StableCascade">github.com/Stability-AI/Stab…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPbVNJZ2E4QUFoUHN6LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPbVNJaWFrQUFGQlVFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dPbVNJaWFFQUFMSHlzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757388396996280482#m</id>
            <title>NVIDIA市值已超过亚马逊和谷歌，成为全球市值超过万亿切，排名第四的公司。

自2024年1月1日起，仅用了不到6周的时间，NVIDIA的市值增加了6500亿美元的市值。这一增长额超过了特斯拉整个公司的市值！

截至2024年2月12日，全球市值排行如下：

1. 微软 (Microsoft)：$3.115万亿
2. 苹果 (Apple)：$2.904万亿
3. 沙特阿美 (Saudi Aramco)：$2.034万亿
4. NVIDIA：$1.831万亿
5. Alphabet (谷歌的母公司)：$1.820万亿
6. 亚马逊 (Amazon)：$1.803万亿
7. Meta (Facebook的母公司)：$1.217万亿</title>
            <link>https://nitter.cz/xiaohuggg/status/1757388396996280482#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757388396996280482#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 12:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NVIDIA市值已超过亚马逊和谷歌，成为全球市值超过万亿切，排名第四的公司。<br />
<br />
自2024年1月1日起，仅用了不到6周的时间，NVIDIA的市值增加了6500亿美元的市值。这一增长额超过了特斯拉整个公司的市值！<br />
<br />
截至2024年2月12日，全球市值排行如下：<br />
<br />
1. 微软 (Microsoft)：$3.115万亿<br />
2. 苹果 (Apple)：$2.904万亿<br />
3. 沙特阿美 (Saudi Aramco)：$2.034万亿<br />
4. NVIDIA：$1.831万亿<br />
5. Alphabet (谷歌的母公司)：$1.820万亿<br />
6. 亚马逊 (Amazon)：$1.803万亿<br />
7. Meta (Facebook的母公司)：$1.217万亿</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dOX2JUUWF3QUE3eWZRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757355823460991485#m</id>
            <title>notesGPT：开源的语音笔记GPT工具

它能够录下你的语音内容，然后自动帮助你把这些语音转成文字，对内容进行总结，并且生成相关的任务列表！

notesGPT的主要功能包括：

1. 录制语音笔记：用户可以直接通过该工具录制他们的语音笔记，无需手动输入文字，方便快捷。

2. 自动转录：录制的语音笔记会自动被转换成文本形式，便于阅读和编辑。

3. 内容总结：notesGPT能够自动分析转录的文本内容，并提供一个简洁的总结，帮助用户快速把握笔记的关键信息。

4. 生成行动项：根据语音笔记的内容，自动生成相关的行动项或任务，帮助用户更有效地组织和规划工作或学习任务。

5. 100%免费和开源：notesGPT是一个完全开放源代码的项目，用户可以免费使用所有功能，同时社区的开发者也可以参与到项目的改进和扩展中来。

主要功能和技术栈：

•Convex：数据库和云函数
•Next.js App Router：框架
•Together Inference：LLM (Mixtral)
•Together Embeddings：搜索用的嵌入式数据库
•Convex File Storage：存储语音笔记
•Convex Vector search：向量搜索
•Replicate：Whisper转录
•Clerk：用户认证
•Tailwind CSS：样式设计

在线体验：http://usenotesgpt.com
GitHub：https://github.com/Nutlope/notesGPT</title>
            <link>https://nitter.cz/xiaohuggg/status/1757355823460991485#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757355823460991485#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 10:47:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>notesGPT：开源的语音笔记GPT工具<br />
<br />
它能够录下你的语音内容，然后自动帮助你把这些语音转成文字，对内容进行总结，并且生成相关的任务列表！<br />
<br />
notesGPT的主要功能包括：<br />
<br />
1. 录制语音笔记：用户可以直接通过该工具录制他们的语音笔记，无需手动输入文字，方便快捷。<br />
<br />
2. 自动转录：录制的语音笔记会自动被转换成文本形式，便于阅读和编辑。<br />
<br />
3. 内容总结：notesGPT能够自动分析转录的文本内容，并提供一个简洁的总结，帮助用户快速把握笔记的关键信息。<br />
<br />
4. 生成行动项：根据语音笔记的内容，自动生成相关的行动项或任务，帮助用户更有效地组织和规划工作或学习任务。<br />
<br />
5. 100%免费和开源：notesGPT是一个完全开放源代码的项目，用户可以免费使用所有功能，同时社区的开发者也可以参与到项目的改进和扩展中来。<br />
<br />
主要功能和技术栈：<br />
<br />
•Convex：数据库和云函数<br />
•Next.js App Router：框架<br />
•Together Inference：LLM (Mixtral)<br />
•Together Embeddings：搜索用的嵌入式数据库<br />
•Convex File Storage：存储语音笔记<br />
•Convex Vector search：向量搜索<br />
•Replicate：Whisper转录<br />
•Clerk：用户认证<br />
•Tailwind CSS：样式设计<br />
<br />
在线体验：<a href="http://usenotesgpt.com">usenotesgpt.com</a><br />
GitHub：<a href="https://github.com/Nutlope/notesGPT">github.com/Nutlope/notesGPT</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU3MzU1NzYxMzg5NTA2NTYwL2ltZy9xYXpvcGJEa0UwQ1VvaFhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757319738450215079#m</id>
            <title>通过VR 也能操控真实物体

隔空控制家里电器开关</title>
            <link>https://nitter.cz/xiaohuggg/status/1757319738450215079#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757319738450215079#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 08:24:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过VR 也能操控真实物体<br />
<br />
隔空控制家里电器开关</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcxODM2MzY4MDgzNDM1NTIvcHUvaW1nL0paaWp0TlpyQm9yVUNfTXAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757240458944844074#m</id>
            <title>ElevenLabs推出一个新的平台，允许你分享自己的声音并赚取收入。

你可以在ElevenLabs的声音库中创建分享自己的AI声音，每当有人使用这些声音时，就能获得一定的回报。

操作流程如下：

-前往VoiceLab并上传30分钟以上的音频
-为您的声音命名并进行描述
-设置您的价格和使用参数
-添加您的支付详情，并在使用时获得支付

ElevenLabs还提供了一些高级选项，比如与平台合作，创建特别高质量的默认声音，这样不仅能获得保证的前期收入，还能提升个人品牌的影响力。

详细：https://elevenlabs.io/voice-actors</title>
            <link>https://nitter.cz/xiaohuggg/status/1757240458944844074#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757240458944844074#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 03:09:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ElevenLabs推出一个新的平台，允许你分享自己的声音并赚取收入。<br />
<br />
你可以在ElevenLabs的声音库中创建分享自己的AI声音，每当有人使用这些声音时，就能获得一定的回报。<br />
<br />
操作流程如下：<br />
<br />
-前往VoiceLab并上传30分钟以上的音频<br />
-为您的声音命名并进行描述<br />
-设置您的价格和使用参数<br />
-添加您的支付详情，并在使用时获得支付<br />
<br />
ElevenLabs还提供了一些高级选项，比如与平台合作，创建特别高质量的默认声音，这样不仅能获得保证的前期收入，还能提升个人品牌的影响力。<br />
<br />
详细：<a href="https://elevenlabs.io/voice-actors">elevenlabs.io/voice-actors</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcyMzE5NDUwNDA1MDY4ODAvcHUvaW1nL0FlcVBKSmRBRnZvbmhyemIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757231958088949810#m</id>
            <title>Azure OpenAI Service宣布了一系列新功能：包括公开预览的Assistants API、新的文本到语音（TTS）功能、即将推出的GPT-4 Turbo和GPT-3.5 Turbo模型更新、新的嵌入模型以及微调API的更新。

与之前的聊天完成API相比，Assistants API能够记住之前的对话内容，创建持久化和无限长的线程。

Assistants API 是一项由 Azure OpenAI 提供的新服务，它旨在帮助开发者在他们的应用程序中更容易地创建高质量的人工智能助手体验。与之前的聊天完成API相比，Assistants API提供了以下几个关键改进：

1、状态性演进： Assistants API能够记住之前的对话内容，这意味着它可以在对话过程中维持上下文，提供更连贯、更个性化的交流体验。

2、持久化和无限长的线程： 开发者可以创建持久化的对话线程，这些线程可以无限长，不受模型上下文窗口大小的限制。这样，即使是长时间或复杂的对话也能流畅进行。

3、简化对话状态管理： 由于Assistants API自身支持对话状态的管理，开发者无需自己构建复杂的系统来追踪对话状态或管理线程。这降低了开发复杂度，使得创建动态、互动的AI助手变得更简单。

Azure OpenAI Service更新和新增功能主要包括：

1、Assistants API公开预览：这是一个新功能，旨在让开发者在自己的应用中轻松创建类似副驾驶的高质量体验。Assistants API支持持久化和无限长的线程，简化了开发者在管理对话状态和集成工具方面的工作，提高了开发效率。

2、文本到语音（TTS）功能：引入了新的文本到语音模型，能够将文本转换为具有个性和风格的高质量人声，适用于客户支持、培训视频、实时流媒体等多种场景。

3、GPT-4 Turbo和GPT-3.5 Turbo模型更新：推出了GPT-4 Turbo和GPT-3.5 Turbo的更新版本，包括改进的代码生成能力、减少“懒惰”现象、修复了影响非英文UTF-8生成的错误等。提供了改进的指令遵循、JSON模式、可复现的输出、并行函数调用等特性。

5、新的嵌入模型和微调API更新：引入了新的嵌入模型和微调API的更新，包括一个新模型、支持持续微调和更优惠的定价。这些更新使得构建自定义模型和使用更长消息进行微调变得更加简单和经济。

6、定价优化：为训练和托管微调模型在Azure OpenAI Service上的成本提供了降价，包括将GPT-3.5-Turbo的训练和托管成本降低了50%。

详细：https://msft.it/6019ihVTb</title>
            <link>https://nitter.cz/xiaohuggg/status/1757231958088949810#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757231958088949810#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 02:35:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Azure OpenAI Service宣布了一系列新功能：包括公开预览的Assistants API、新的文本到语音（TTS）功能、即将推出的GPT-4 Turbo和GPT-3.5 Turbo模型更新、新的嵌入模型以及微调API的更新。<br />
<br />
与之前的聊天完成API相比，Assistants API能够记住之前的对话内容，创建持久化和无限长的线程。<br />
<br />
Assistants API 是一项由 Azure OpenAI 提供的新服务，它旨在帮助开发者在他们的应用程序中更容易地创建高质量的人工智能助手体验。与之前的聊天完成API相比，Assistants API提供了以下几个关键改进：<br />
<br />
1、状态性演进： Assistants API能够记住之前的对话内容，这意味着它可以在对话过程中维持上下文，提供更连贯、更个性化的交流体验。<br />
<br />
2、持久化和无限长的线程： 开发者可以创建持久化的对话线程，这些线程可以无限长，不受模型上下文窗口大小的限制。这样，即使是长时间或复杂的对话也能流畅进行。<br />
<br />
3、简化对话状态管理： 由于Assistants API自身支持对话状态的管理，开发者无需自己构建复杂的系统来追踪对话状态或管理线程。这降低了开发复杂度，使得创建动态、互动的AI助手变得更简单。<br />
<br />
Azure OpenAI Service更新和新增功能主要包括：<br />
<br />
1、Assistants API公开预览：这是一个新功能，旨在让开发者在自己的应用中轻松创建类似副驾驶的高质量体验。Assistants API支持持久化和无限长的线程，简化了开发者在管理对话状态和集成工具方面的工作，提高了开发效率。<br />
<br />
2、文本到语音（TTS）功能：引入了新的文本到语音模型，能够将文本转换为具有个性和风格的高质量人声，适用于客户支持、培训视频、实时流媒体等多种场景。<br />
<br />
3、GPT-4 Turbo和GPT-3.5 Turbo模型更新：推出了GPT-4 Turbo和GPT-3.5 Turbo的更新版本，包括改进的代码生成能力、减少“懒惰”现象、修复了影响非英文UTF-8生成的错误等。提供了改进的指令遵循、JSON模式、可复现的输出、并行函数调用等特性。<br />
<br />
5、新的嵌入模型和微调API更新：引入了新的嵌入模型和微调API的更新，包括一个新模型、支持持续微调和更优惠的定价。这些更新使得构建自定义模型和使用更长消息进行微调变得更加简单和经济。<br />
<br />
6、定价优化：为训练和托管微调模型在Azure OpenAI Service上的成本提供了降价，包括将GPT-3.5-Turbo的训练和托管成本降低了50%。<br />
<br />
详细：<a href="https://msft.it/6019ihVTb">msft.it/6019ihVTb</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTcyMjg3NDI5MzI1NjYwMTYvcHUvaW1nL2t3ekxWczRVTTFKSEJGQWcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757225933138952546#m</id>
            <title>Keyframer：利用大语言模型（LLMs）将静态图像转换成动画。

该项目由苹果开发，通过自然语言提示，可以从静态SVG图像创建动画插图。

同时通过LLMs生成的CSS动画代码，用户可以对生成的动画进行细化编辑。

这意味着，无论你是想让一个设计中的元素旋转、移动还是淡入淡出，你都可以通过描述这些动作的文字来实现。

Keyframer主要功能：

1、自然语言控制：用户可以使用自然语言提示来指导动画的创作过程，无需深入了解动画制作的复杂技术细节。

2、迭代设计支持：Keyframer允许用户通过编辑大语言模型生成的CSS动画代码或属性来迭代和精细化他们的设计，提供了一种直接和灵活的方式来调整动画效果。

3、设计变体探索：用户可以请求设计变体，这支持了创意过程和设计探索，帮助用户发现和尝试新的设计方向。

4、分类用户提示策略：通过用户研究，Keyframer贡献了用户提示策略的分类，包括用于描述运动的语义提示类型和“分解”提示风格，这有助于用户更有效地与工具交互。

Keyframer将动画设计的门槛大大降低，让更广泛的受众能够参与到动画创作中，不仅对设计师有益，对于任何对动画创作感兴趣的人来说都是一个有价值的工具。

工作原理：

1、自然语言处理： 用户通过自然语言描述他们想要为静态SVG图像创建的动画效果。这些描述可以包括动画的方向、速度、效果等详细信息。

2、LLM生成动画代码： Keyframer利用大语言模型（如GPT-3）来解析用户的自然语言描述。基于这些描述，LLM生成相应的CSS动画代码。这段代码是动画效果的“配方”，指定了动画的具体行为，如动画的持续时间、变化的属性（例如位置、大小、颜色等）以及动画的时间函数。

3、动画渲染和预览： 生成的CSS代码应用于输入的SVG图像，创建动画效果。用户可以在Keyframer的界面中实时预览这些动画，以评估效果是否符合他们的预期。

4、直接编辑功能： 为了进一步细化和定制动画，Keyframer提供了代码编辑器和属性编辑器。用户可以预览这段代码生成的动画效果，如果效果符合预期，可以直接使用；如果需要调整，用户还可以通过编辑生成的CSS代码来细化动画的效果。这一步不需要用户编写代码，但他们可以直接修改代码来调整动画的各种参数（如速度、方向、延迟等），以达到完全符合他们设计意图的动画效果。

5、设计迭代与探索： 用户可以通过添加新的自然语言提示来请求设计变体或对动画进行迭代。这支持了创意思维和设计探索过程，允许用户在看到初步效果后，继续调整和完善他们的动画设计。

6、用户提示策略和反馈循环： Keyframer支持分解式提示风格，用户可以分步骤地描述和细化动画的各个部分。系统的设计允许用户基于生成的动画效果和直接编辑的反馈，不断调整他们的提示策略，实现设计目标的逐步精细化。

实验结果：

研究表明，即使是动画和编程经验较少的用户，也能够使用Keyframer有效地创建动画。通过直接编辑生成的CSS代码，用户可以对动画进行精细调整，这增强了他们对设计过程的控制感和满意度。

研究还发现，LLM生成的输出有时会出现意外的、创意的解决方案，这为设计过程带来了意外的惊喜和新的灵感。这些意外的输出有助于扩展用户的设计思维，推动他们探索新的创意可能性。
论文：https://arxiv.org/abs/2402.06071

PDF：https://arxiv.org/pdf/2402.06071.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1757225933138952546#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757225933138952546#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 Feb 2024 02:11:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Keyframer：利用大语言模型（LLMs）将静态图像转换成动画。<br />
<br />
该项目由苹果开发，通过自然语言提示，可以从静态SVG图像创建动画插图。<br />
<br />
同时通过LLMs生成的CSS动画代码，用户可以对生成的动画进行细化编辑。<br />
<br />
这意味着，无论你是想让一个设计中的元素旋转、移动还是淡入淡出，你都可以通过描述这些动作的文字来实现。<br />
<br />
Keyframer主要功能：<br />
<br />
1、自然语言控制：用户可以使用自然语言提示来指导动画的创作过程，无需深入了解动画制作的复杂技术细节。<br />
<br />
2、迭代设计支持：Keyframer允许用户通过编辑大语言模型生成的CSS动画代码或属性来迭代和精细化他们的设计，提供了一种直接和灵活的方式来调整动画效果。<br />
<br />
3、设计变体探索：用户可以请求设计变体，这支持了创意过程和设计探索，帮助用户发现和尝试新的设计方向。<br />
<br />
4、分类用户提示策略：通过用户研究，Keyframer贡献了用户提示策略的分类，包括用于描述运动的语义提示类型和“分解”提示风格，这有助于用户更有效地与工具交互。<br />
<br />
Keyframer将动画设计的门槛大大降低，让更广泛的受众能够参与到动画创作中，不仅对设计师有益，对于任何对动画创作感兴趣的人来说都是一个有价值的工具。<br />
<br />
工作原理：<br />
<br />
1、自然语言处理： 用户通过自然语言描述他们想要为静态SVG图像创建的动画效果。这些描述可以包括动画的方向、速度、效果等详细信息。<br />
<br />
2、LLM生成动画代码： Keyframer利用大语言模型（如GPT-3）来解析用户的自然语言描述。基于这些描述，LLM生成相应的CSS动画代码。这段代码是动画效果的“配方”，指定了动画的具体行为，如动画的持续时间、变化的属性（例如位置、大小、颜色等）以及动画的时间函数。<br />
<br />
3、动画渲染和预览： 生成的CSS代码应用于输入的SVG图像，创建动画效果。用户可以在Keyframer的界面中实时预览这些动画，以评估效果是否符合他们的预期。<br />
<br />
4、直接编辑功能： 为了进一步细化和定制动画，Keyframer提供了代码编辑器和属性编辑器。用户可以预览这段代码生成的动画效果，如果效果符合预期，可以直接使用；如果需要调整，用户还可以通过编辑生成的CSS代码来细化动画的效果。这一步不需要用户编写代码，但他们可以直接修改代码来调整动画的各种参数（如速度、方向、延迟等），以达到完全符合他们设计意图的动画效果。<br />
<br />
5、设计迭代与探索： 用户可以通过添加新的自然语言提示来请求设计变体或对动画进行迭代。这支持了创意思维和设计探索过程，允许用户在看到初步效果后，继续调整和完善他们的动画设计。<br />
<br />
6、用户提示策略和反馈循环： Keyframer支持分解式提示风格，用户可以分步骤地描述和细化动画的各个部分。系统的设计允许用户基于生成的动画效果和直接编辑的反馈，不断调整他们的提示策略，实现设计目标的逐步精细化。<br />
<br />
实验结果：<br />
<br />
研究表明，即使是动画和编程经验较少的用户，也能够使用Keyframer有效地创建动画。通过直接编辑生成的CSS代码，用户可以对动画进行精细调整，这增强了他们对设计过程的控制感和满意度。<br />
<br />
研究还发现，LLM生成的输出有时会出现意外的、创意的解决方案，这为设计过程带来了意外的惊喜和新的灵感。这些意外的输出有助于扩展用户的设计思维，推动他们探索新的创意可能性。<br />
论文：<a href="https://arxiv.org/abs/2402.06071">arxiv.org/abs/2402.06071</a><br />
<br />
PDF：<a href="https://arxiv.org/pdf/2402.06071.pdf">arxiv.org/pdf/2402.06071.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dMcnFkMGJrQUEzdDE2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dMcnFlR2FNQUF6ZGFjLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0dMcnFlS2J3QUE2TEhELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757022189201858603#m</id>
            <title>Franco Ronconi 介绍了一个Canvastique3D工具

这是一个结合了OpenCV（一个开源计算机视觉库）和OpenAI技术的工具

这个工具允许设计师在3D模型上实时预览他们的手工设计，这意味着设计师可以立即看到他们的设计在虚拟三维空间中的样子，而不需要等待制作实体样品。</title>
            <link>https://nitter.cz/xiaohuggg/status/1757022189201858603#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757022189201858603#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 12:41:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Franco Ronconi 介绍了一个Canvastique3D工具<br />
<br />
这是一个结合了OpenCV（一个开源计算机视觉库）和OpenAI技术的工具<br />
<br />
这个工具允许设计师在3D模型上实时预览他们的手工设计，这意味着设计师可以立即看到他们的设计在虚拟三维空间中的样子，而不需要等待制作实体样品。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI4MDc1NDMwNzA2MTM1MDQvcHUvaW1nL1hNWUpUZDN1TEc4NkJtSVIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756977011782979921#m</id>
            <title>MoneyPrinter：自动创建YouTube短视频的自动化赚钱项目

主要功能：

- 自动视频生成：只要输入视频话题即可自动产生与之相关的短视频。

- 音乐和字体自定义：可以上传自己的MP3文件压缩包和字体，自定义视频音乐背景和字体。

-自动将生成的视频上传到YouTube。

整个过程几乎不需要用户有太多的视频编辑技能，只需要简单的操作和等待程序完成工作。

MoneyPrinter的背后技术主要依赖于Python编程语言和MoviePy视频编辑库，以及YouTube的API用于视频上传，使得从视频创意到发布的整个流程自动化和无缝连接。

MoviePy是一个强大的视频处理库，能够编辑视频、添加音乐背景和文本等。

GitHub：https://github.com/FujiwaraChoki/MoneyPrinter</title>
            <link>https://nitter.cz/xiaohuggg/status/1756977011782979921#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756977011782979921#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 09:42:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MoneyPrinter：自动创建YouTube短视频的自动化赚钱项目<br />
<br />
主要功能：<br />
<br />
- 自动视频生成：只要输入视频话题即可自动产生与之相关的短视频。<br />
<br />
- 音乐和字体自定义：可以上传自己的MP3文件压缩包和字体，自定义视频音乐背景和字体。<br />
<br />
-自动将生成的视频上传到YouTube。<br />
<br />
整个过程几乎不需要用户有太多的视频编辑技能，只需要简单的操作和等待程序完成工作。<br />
<br />
MoneyPrinter的背后技术主要依赖于Python编程语言和MoviePy视频编辑库，以及YouTube的API用于视频上传，使得从视频创意到发布的整个流程自动化和无缝连接。<br />
<br />
MoviePy是一个强大的视频处理库，能够编辑视频、添加音乐背景和文本等。<br />
<br />
GitHub：<a href="https://github.com/FujiwaraChoki/MoneyPrinter">github.com/FujiwaraChoki/Mon…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjkwOTI0MzM4MDUzMTIvcHUvaW1nL1FLTlB4S3gyOF9nYzAtUTguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962301620699315#m</id>
            <title>R to @xiaohuggg: 多个移动箱+混合摄像机移动</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962301620699315#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962301620699315#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多个移动箱+混合摄像机移动</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjIxNTUzNzc5Mzg0MzIvcHUvaW1nL1A3SHVZODZHRkppZGMtWVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962297107640406#m</id>
            <title>R to @xiaohuggg: 单移动箱+混合摄像机移动</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962297107640406#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962297107640406#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>单移动箱+混合摄像机移动</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjIwMzcxNDQ2MTY5NjAvcHUvaW1nL3JLbEhKOEllU2dxVjVtV3AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962293081104729#m</id>
            <title>R to @xiaohuggg: 混合摄像机移动 （X+Y）</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962293081104729#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962293081104729#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>混合摄像机移动 （X+Y）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjE4MjE3NTg3ODM0ODgvcHUvaW1nL3hlQnFic3hRNkpCeS0xZEkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962287691444265#m</id>
            <title>Direct-a-Video：像导演一样生成视频

它可以允许用户通过自然语言独立或共同控制摄像机移动和/或对象运动。

比如你只需要告诉它你想要什么样的对象运动（比如小狗跑动）和摄像机怎么移动（比如跟着小狗），剩下的工作就交给它了。

在Direct-a-Video项目中，用户可以控制以下类型的运动：

摄像机移动控制

-基础摄像机移动
-混合摄像机移动 (X+Y)
-混合摄像机移动 (X+Z)

对象运动控制

-摄像机移动和对象运动的联合控制
-静态框 + 混合摄像机移动
-单个移动框 + 混合摄像机移动
-多个移动框 + 混合摄像机移动

1、物体运动：用户可以指定一个或多个物体在视频中的具体运动路径，如前进、后退、上升、下降、旋转等。这意味着如果你想让视频中的一个球向左滚动，或者一个人物向摄像机走近，都可以通过文本指令实现。

2、摄像机移动：用户还可以控制摄像机的移动方式，包括平移（左右移动）、倾斜（上下移动）、缩放（放大或缩小画面）等。这可以帮助创造出从不同角度和距离观察场景的效果，比如模拟从高空俯瞰或者近距离跟踪某个物体的视角。

3、联合控制物体和摄像机运动：Direct-a-Video独特的功能在于，它允许用户同时控制物体的运动和摄像机的移动。这意味着你可以创作出更加动态和复杂的视频场景，比如在跟随一个移动物体的同时，摄像机也在进行缩放或者旋转，以创造出电影般的视觉效果。

工作原理：

Direct-a-Video通过两个主要机制实现对视频生成的细粒度控制：对象运动控制和相机移动控制。这两种控制机制独立运作，但也可以联合使用，为用户提供了高度的定制能力和创造性的自由。以下是其工作原理的详细解释：

1、文本解析和意图理解：系统首先解析用户输入的文本，理解用户希望在视频中看到的摄像机移动和物体动作的具体要求。

2、对象运动控制

空间交叉注意调制：Direct-a-Video利用空间交叉注意调制来控制对象在视频中的运动。这种方法依赖于模型固有的先验知识，无需额外的优化过程。

用户通过输入文本提示来指定对象及其在视频中的运动轨迹（例如，一个对象从屏幕一边移动到另一边）。模型使用这些文本提示来引导对象在视频帧中的空间和时间放置。

3、相机移动控制

时间交叉注意层：为了模拟相机移动（如平移、缩放等），Direct-a-Video引入了时间交叉注意层。这些层能够解释用户通过参数指定的相机移动，从而在视频生成过程中实现相机视角的变化。

4、自监督学习：模型通过在小规模数据集上应用基于增强的自监督学习方法来训练时间交叉注意层，这一过程无需显式的运动注释。训练阶段，视频样本经过增强处理（例如模拟相机的平移和缩放），以训练模型理解和实现相机移动。

5、联合控制

用户可以单独控制对象运动或相机移动，也可以同时控制两者，实现更复杂的视频创作。例如，用户可以设计一个场景，其中一个对象在屏幕上移动，同时相机围绕场景进行平移和缩放，创造出动态且引人入胜的视频内容。

这种设计使得Direct-a-Video能够以一种非常灵活和动态的方式生成视频，用户可以通过简单的文本描述来“导演”视频中的场景，实现了高度个性化和创意的视频内容创作。

项目及演示：https://direct-a-video.github.io/</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962287691444265#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962287691444265#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Direct-a-Video：像导演一样生成视频<br />
<br />
它可以允许用户通过自然语言独立或共同控制摄像机移动和/或对象运动。<br />
<br />
比如你只需要告诉它你想要什么样的对象运动（比如小狗跑动）和摄像机怎么移动（比如跟着小狗），剩下的工作就交给它了。<br />
<br />
在Direct-a-Video项目中，用户可以控制以下类型的运动：<br />
<br />
摄像机移动控制<br />
<br />
-基础摄像机移动<br />
-混合摄像机移动 (X+Y)<br />
-混合摄像机移动 (X+Z)<br />
<br />
对象运动控制<br />
<br />
-摄像机移动和对象运动的联合控制<br />
-静态框 + 混合摄像机移动<br />
-单个移动框 + 混合摄像机移动<br />
-多个移动框 + 混合摄像机移动<br />
<br />
1、物体运动：用户可以指定一个或多个物体在视频中的具体运动路径，如前进、后退、上升、下降、旋转等。这意味着如果你想让视频中的一个球向左滚动，或者一个人物向摄像机走近，都可以通过文本指令实现。<br />
<br />
2、摄像机移动：用户还可以控制摄像机的移动方式，包括平移（左右移动）、倾斜（上下移动）、缩放（放大或缩小画面）等。这可以帮助创造出从不同角度和距离观察场景的效果，比如模拟从高空俯瞰或者近距离跟踪某个物体的视角。<br />
<br />
3、联合控制物体和摄像机运动：Direct-a-Video独特的功能在于，它允许用户同时控制物体的运动和摄像机的移动。这意味着你可以创作出更加动态和复杂的视频场景，比如在跟随一个移动物体的同时，摄像机也在进行缩放或者旋转，以创造出电影般的视觉效果。<br />
<br />
工作原理：<br />
<br />
Direct-a-Video通过两个主要机制实现对视频生成的细粒度控制：对象运动控制和相机移动控制。这两种控制机制独立运作，但也可以联合使用，为用户提供了高度的定制能力和创造性的自由。以下是其工作原理的详细解释：<br />
<br />
1、文本解析和意图理解：系统首先解析用户输入的文本，理解用户希望在视频中看到的摄像机移动和物体动作的具体要求。<br />
<br />
2、对象运动控制<br />
<br />
空间交叉注意调制：Direct-a-Video利用空间交叉注意调制来控制对象在视频中的运动。这种方法依赖于模型固有的先验知识，无需额外的优化过程。<br />
<br />
用户通过输入文本提示来指定对象及其在视频中的运动轨迹（例如，一个对象从屏幕一边移动到另一边）。模型使用这些文本提示来引导对象在视频帧中的空间和时间放置。<br />
<br />
3、相机移动控制<br />
<br />
时间交叉注意层：为了模拟相机移动（如平移、缩放等），Direct-a-Video引入了时间交叉注意层。这些层能够解释用户通过参数指定的相机移动，从而在视频生成过程中实现相机视角的变化。<br />
<br />
4、自监督学习：模型通过在小规模数据集上应用基于增强的自监督学习方法来训练时间交叉注意层，这一过程无需显式的运动注释。训练阶段，视频样本经过增强处理（例如模拟相机的平移和缩放），以训练模型理解和实现相机移动。<br />
<br />
5、联合控制<br />
<br />
用户可以单独控制对象运动或相机移动，也可以同时控制两者，实现更复杂的视频创作。例如，用户可以设计一个场景，其中一个对象在屏幕上移动，同时相机围绕场景进行平移和缩放，创造出动态且引人入胜的视频内容。<br />
<br />
这种设计使得Direct-a-Video能够以一种非常灵活和动态的方式生成视频，用户可以通过简单的文本描述来“导演”视频中的场景，实现了高度个性化和创意的视频内容创作。<br />
<br />
项目及演示：<a href="https://direct-a-video.github.io/">direct-a-video.github.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5Mjk2ODEzNjUwNDUyNDgvcHUvaW1nL1BjNzFRUFJqbDk0Z3I0SHIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756938524044189894#m</id>
            <title>R to @xiaohuggg: 该广告的幕后故事：

https://youtu.be/YmXUbzpDY5s</title>
            <link>https://nitter.cz/xiaohuggg/status/1756938524044189894#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756938524044189894#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 07:09:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>该广告的幕后故事：<br />
<br />
<a href="https://youtu.be/YmXUbzpDY5s">youtu.be/YmXUbzpDY5s</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1NjA0ODQwODUyNjEzNTI5Ni82TFY3b0NzdT9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756938521179517436#m</id>
            <title>盲人导演 @adamdavidmorse，在他的超级碗广告首秀中，展示了Pixel 8 的“Guided Frame”功能。

这项功能利用 Google AI 技术，使得盲人或视力低下的人士更容易拍摄照片并分享日常生活。</title>
            <link>https://nitter.cz/xiaohuggg/status/1756938521179517436#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756938521179517436#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 07:09:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>盲人导演 <a href="https://nitter.cz/adamdavidmorse" title="Adam Morse">@adamdavidmorse</a>，在他的超级碗广告首秀中，展示了Pixel 8 的“Guided Frame”功能。<br />
<br />
这项功能利用 Google AI 技术，使得盲人或视力低下的人士更容易拍摄照片并分享日常生活。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5MzczMzQ5ODQxOTIwMDAvcHUvaW1nL0RTNVNfdGxXM2psc2hINUwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756917238467379401#m</id>
            <title>这个好，我很需要🫰

牛P，还能下载视频...</title>
            <link>https://nitter.cz/xiaohuggg/status/1756917238467379401#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756917238467379401#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 05:44:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个好，我很需要🫰<br />
<br />
牛P，还能下载视频...</p>
<p><a href="https://nitter.cz/dotey/status/1756901189646422132#m">nitter.cz/dotey/status/1756901189646422132#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756915564692545660#m</id>
            <title>纽约大学的一个研究团队开发了一种新技术，能够在短短18秒内教会一架无人机如何稳定飞行。

该程序可以在一台普通的MacBook Pro上运行。通过模拟飞行环境来训练无人机，让它学会如何保持空中悬停并按照指定路径飞行。

只需18秒钟就能实现这一切。

这种方法不仅限于简单的小型无人机——它几乎可以适用于任何无人机，包括更大、更昂贵的无人机，甚至是你自己从头开始建造的无人机。

工作原理：

1、端到端控制：通过深度强化学习（Deep RL）实现四旋翼无人机从感知到动作输出的直接映射，无需复杂的中间处理层，提高了控制策略的直接性和效率。

2、不对称演员-评论家架构：采用了一种新颖的基于RL的训练框架，其中演员直接从状态到动作的映射进行决策，而评论家则利用额外的信息（如仿真中的精确状态）来评估动作的好坏，帮助演员更快学习。

通过给予奖励和惩罚来教会模型执行某项任务。在这个项目中，无人机通过尝试不同的动作，根据其对任务成功率的影响获得反馈，从而学习如何飞行。

3、高度优化的仿真器：开发了一个能在消费级笔记本电脑上模拟约5个月飞行时间每秒的高性能仿真器，这个仿真器使得无人机的训练过程极为快速。这种方法允许无人机在没有任何风险的情况下进行无数次尝试和错误，快速学习飞行技能。

4、课程学习（Curriculum Learning）：为了提高训练效率，研究者采用了课程学习策略，即从简单任务开始逐步过渡到更复杂的任务。这种方法让无人机先学习基本的飞行控制，然后逐渐学习执行更复杂的飞行动作。

5、调整奖励函数：训练过程中，研究者会调整奖励函数，即改变给予无人机的反馈，以鼓励它学习如何稳定飞行和执行特定的飞行路径。一开始，奖励机制较为宽松，随着训练的深入，会逐渐增加对飞行精确度和鲁棒性的要求。

6、Sim2Real转移策略：通过精心设计的训练范式和仿真环境，确保了无人机控制策略可以平滑地从仿真环境转移到真实环境，克服了仿真与现实之间的差距。

该项目利用高度优化的仿真器和有效的学习策略，实现了在仅18秒内完成无人机飞行控制策略的训练，大大减少了从理论到实践的时间。

项目将代码和仿真器已经开源。

论文：https://arxiv.org/abs/2311.13081
GitHub：https://github.com/arplaboratory/learning-to-fly
视频介绍：https://youtu.be/NRD43ZA1D-4</title>
            <link>https://nitter.cz/xiaohuggg/status/1756915564692545660#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756915564692545660#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 05:38:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>纽约大学的一个研究团队开发了一种新技术，能够在短短18秒内教会一架无人机如何稳定飞行。<br />
<br />
该程序可以在一台普通的MacBook Pro上运行。通过模拟飞行环境来训练无人机，让它学会如何保持空中悬停并按照指定路径飞行。<br />
<br />
只需18秒钟就能实现这一切。<br />
<br />
这种方法不仅限于简单的小型无人机——它几乎可以适用于任何无人机，包括更大、更昂贵的无人机，甚至是你自己从头开始建造的无人机。<br />
<br />
工作原理：<br />
<br />
1、端到端控制：通过深度强化学习（Deep RL）实现四旋翼无人机从感知到动作输出的直接映射，无需复杂的中间处理层，提高了控制策略的直接性和效率。<br />
<br />
2、不对称演员-评论家架构：采用了一种新颖的基于RL的训练框架，其中演员直接从状态到动作的映射进行决策，而评论家则利用额外的信息（如仿真中的精确状态）来评估动作的好坏，帮助演员更快学习。<br />
<br />
通过给予奖励和惩罚来教会模型执行某项任务。在这个项目中，无人机通过尝试不同的动作，根据其对任务成功率的影响获得反馈，从而学习如何飞行。<br />
<br />
3、高度优化的仿真器：开发了一个能在消费级笔记本电脑上模拟约5个月飞行时间每秒的高性能仿真器，这个仿真器使得无人机的训练过程极为快速。这种方法允许无人机在没有任何风险的情况下进行无数次尝试和错误，快速学习飞行技能。<br />
<br />
4、课程学习（Curriculum Learning）：为了提高训练效率，研究者采用了课程学习策略，即从简单任务开始逐步过渡到更复杂的任务。这种方法让无人机先学习基本的飞行控制，然后逐渐学习执行更复杂的飞行动作。<br />
<br />
5、调整奖励函数：训练过程中，研究者会调整奖励函数，即改变给予无人机的反馈，以鼓励它学习如何稳定飞行和执行特定的飞行路径。一开始，奖励机制较为宽松，随着训练的深入，会逐渐增加对飞行精确度和鲁棒性的要求。<br />
<br />
6、Sim2Real转移策略：通过精心设计的训练范式和仿真环境，确保了无人机控制策略可以平滑地从仿真环境转移到真实环境，克服了仿真与现实之间的差距。<br />
<br />
该项目利用高度优化的仿真器和有效的学习策略，实现了在仅18秒内完成无人机飞行控制策略的训练，大大减少了从理论到实践的时间。<br />
<br />
项目将代码和仿真器已经开源。<br />
<br />
论文：<a href="https://arxiv.org/abs/2311.13081">arxiv.org/abs/2311.13081</a><br />
GitHub：<a href="https://github.com/arplaboratory/learning-to-fly">github.com/arplaboratory/lea…</a><br />
视频介绍：<a href="https://youtu.be/NRD43ZA1D-4">youtu.be/NRD43ZA1D-4</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5MTUxMjc4NTk5NzAwNDgvcHUvaW1nL2ZjNk1tUTBMWUNmYV83ZWwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756531196933419330#m</id>
            <title>1X's ：神经网络视觉端到端学习机器人

该机器人能够完全独立地执行任务，无需人类远程操控或通过预设脚本。

所有动作都是实时通过神经网络计算得出。

机器人基于视觉的端到端神经网络直接从图像中学习如何控制其动作，包括驾驶、操纵手臂和抓取器、控制躯干和头部等。

演示视频未加速剪辑...

通过训练，机器人能够理解和执行一系列广泛的物理行为，如清洁、整理、拾起物体以及与人类和其他机器人进行社交互动。

项目采用了一种策略，通过在几分钟内收集数据并在桌面GPU上进行训练，快速微调模型以适应特定的任务，从而使机器人能够迅速学习新的技能。

技术原理：

1、演示数据集构建：团队首先收集了一组包含30个EVE机器人执行各种任务的演示数据。这些数据非常多样，包括机器人进行清洁、整理家庭、拾起物体以及与人和其他机器人社交互动等物理行为的实例。

2、基础模型训练：使用这些演示数据，团队训练了一个“基础模型”。这个模型能够理解一系列广泛的物理行为，为机器人提供了对各种任务基本动作的理解基础。

3、模型微调：接下来，团队针对特定的任务类型对基础模型进行了微调，生成了多个专门的能力模型。例如，他们创建了专门用于门操作的模型和另一个专注于仓库任务的模型。

4、进一步微调以适应特定任务：团队进一步微调这些专门模型，使它们能够执行更具体的任务，比如打开一个特定的门。这种微调过程使模型能够对特定的行为或动作有更精确的理解和执行能力。

5、快速引入新技能：通过这种分层微调策略，团队可以在短时间内（仅几分钟的数据收集和在桌面GPU上的训练）快速地为机器人引入新的技能。

详细：https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed

演示视频中没有远程操控、没有计算机图形、没有剪辑、没有视频加速、没有脚本轨迹回放。一切都通过神经网络控制，完全自主，以1X速度进行。

机器人硬件信息：

1X的目标是设计出能够在任何场景中有效工作的通用安卓机器人，以应对现实世界的不可预测性。

EVE是1X技术公司开发的一款高级安卓机器人，旨在为商业行业提供智能的工作解决方案。EVE设计用于执行各种任务，从物流操作到保安巡逻等。这款机器人集安全性、平衡性和智能于一身，可以轻松融入现有的工作流程中，与团队无缝协作。

主要特点包括：

安全第一：每台EVE在部署前都会在真实世界场景中进行测试，其软性、仿生机械设计从内而外确保安全性，适合在各种空间中工作。

平衡性能：EVE能够处理重物，同时也足够柔和以处理易碎物品，无论是在仓库还是分发中心，都能轻松融入您的物流工作流程。

智能行为：EVE通过观察专家的动作进行学习，如移动设备、开门、完成订单等，然后通过其体现的学习模型重现动作并开始工作。随着时间的积累，EVE在基础知识上构建经验，使得未来任务（如“移动那个箱子”）变得更加简单。

EVE的规格：

高度：1.86米
重量：86公斤
最高速度：14.4公里/小时
携带能力：15公斤
运行时间：6小时

EVE通过人工智能自主操作，默认情况下能够导航工作空间，执行如开不同手柄的门、从远处识别人或物体、像人类一样穿越非结构化空间的任务。EVE在帮助人类操作者执行任务时，利用其力量、精确度和传感器执行如在办公楼巡逻和检查员工ID徽章的任务。EVE会关注潜在的危险或错误，并在操作者需要接管时报告。</title>
            <link>https://nitter.cz/xiaohuggg/status/1756531196933419330#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756531196933419330#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 04:10:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>1X's ：神经网络视觉端到端学习机器人<br />
<br />
该机器人能够完全独立地执行任务，无需人类远程操控或通过预设脚本。<br />
<br />
所有动作都是实时通过神经网络计算得出。<br />
<br />
机器人基于视觉的端到端神经网络直接从图像中学习如何控制其动作，包括驾驶、操纵手臂和抓取器、控制躯干和头部等。<br />
<br />
演示视频未加速剪辑...<br />
<br />
通过训练，机器人能够理解和执行一系列广泛的物理行为，如清洁、整理、拾起物体以及与人类和其他机器人进行社交互动。<br />
<br />
项目采用了一种策略，通过在几分钟内收集数据并在桌面GPU上进行训练，快速微调模型以适应特定的任务，从而使机器人能够迅速学习新的技能。<br />
<br />
技术原理：<br />
<br />
1、演示数据集构建：团队首先收集了一组包含30个EVE机器人执行各种任务的演示数据。这些数据非常多样，包括机器人进行清洁、整理家庭、拾起物体以及与人和其他机器人社交互动等物理行为的实例。<br />
<br />
2、基础模型训练：使用这些演示数据，团队训练了一个“基础模型”。这个模型能够理解一系列广泛的物理行为，为机器人提供了对各种任务基本动作的理解基础。<br />
<br />
3、模型微调：接下来，团队针对特定的任务类型对基础模型进行了微调，生成了多个专门的能力模型。例如，他们创建了专门用于门操作的模型和另一个专注于仓库任务的模型。<br />
<br />
4、进一步微调以适应特定任务：团队进一步微调这些专门模型，使它们能够执行更具体的任务，比如打开一个特定的门。这种微调过程使模型能够对特定的行为或动作有更精确的理解和执行能力。<br />
<br />
5、快速引入新技能：通过这种分层微调策略，团队可以在短时间内（仅几分钟的数据收集和在桌面GPU上的训练）快速地为机器人引入新的技能。<br />
<br />
详细：<a href="https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed">1x.tech/discover/all-neural-…</a><br />
<br />
演示视频中没有远程操控、没有计算机图形、没有剪辑、没有视频加速、没有脚本轨迹回放。一切都通过神经网络控制，完全自主，以1X速度进行。<br />
<br />
机器人硬件信息：<br />
<br />
1X的目标是设计出能够在任何场景中有效工作的通用安卓机器人，以应对现实世界的不可预测性。<br />
<br />
EVE是1X技术公司开发的一款高级安卓机器人，旨在为商业行业提供智能的工作解决方案。EVE设计用于执行各种任务，从物流操作到保安巡逻等。这款机器人集安全性、平衡性和智能于一身，可以轻松融入现有的工作流程中，与团队无缝协作。<br />
<br />
主要特点包括：<br />
<br />
安全第一：每台EVE在部署前都会在真实世界场景中进行测试，其软性、仿生机械设计从内而外确保安全性，适合在各种空间中工作。<br />
<br />
平衡性能：EVE能够处理重物，同时也足够柔和以处理易碎物品，无论是在仓库还是分发中心，都能轻松融入您的物流工作流程。<br />
<br />
智能行为：EVE通过观察专家的动作进行学习，如移动设备、开门、完成订单等，然后通过其体现的学习模型重现动作并开始工作。随着时间的积累，EVE在基础知识上构建经验，使得未来任务（如“移动那个箱子”）变得更加简单。<br />
<br />
EVE的规格：<br />
<br />
高度：1.86米<br />
重量：86公斤<br />
最高速度：14.4公里/小时<br />
携带能力：15公斤<br />
运行时间：6小时<br />
<br />
EVE通过人工智能自主操作，默认情况下能够导航工作空间，执行如开不同手柄的门、从远处识别人或物体、像人类一样穿越非结构化空间的任务。EVE在帮助人类操作者执行任务时，利用其力量、精确度和传感器执行如在办公楼巡逻和检查员工ID徽章的任务。EVE会关注潜在的危险或错误，并在操作者需要接管时报告。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY1MjE4ODk2NTExMzQ0NjQvcHUvaW1nL1o3aEZuSWdxQTJsdTFOTGIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>