<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724804784375607429#m</id>
            <title>财报电话会上，针对美国面向芯片领域的禁令问题，腾讯高层表示，我们的芯片下单比较早，目前库存水平比较高，包括H800型号的芯片库存等。

现有的库存水平可以支持腾讯大模型几代的更新。腾讯的云能力不会因为芯片禁令受到影响。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724804784375607429#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724804784375607429#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 15:01:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>财报电话会上，针对美国面向芯片领域的禁令问题，腾讯高层表示，我们的芯片下单比较早，目前库存水平比较高，包括H800型号的芯片库存等。<br />
<br />
现有的库存水平可以支持腾讯大模型几代的更新。腾讯的云能力不会因为芯片禁令受到影响。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0tOHpud2FvQUFhakpoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724781225355931741#m</id>
            <title>R to @xiaohuggg: 之前发的这个视频原作者找到了，抖音@乙人教动画 

这个是制作过程！👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1724781225355931741#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724781225355931741#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 13:27:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前发的这个视频原作者找到了，抖音@乙人教动画 <br />
<br />
这个是制作过程！👍</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI0NzgxMDcyNDIzMjgwNjQwL2ltZy9nOXcwV1NQbjZhNjhUb2xNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724712023009984763#m</id>
            <title>Apple Vision Pro 用户教学视频曝光

Apple VisionOS beta 6 中添加了新手入门视频

根据视频内容，用户只需要眼睛注视屏幕的元素即可选中目标

使用手指轻轻捏去即可操作，操作非常简单方便。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724712023009984763#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724712023009984763#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 08:52:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Apple Vision Pro 用户教学视频曝光<br />
<br />
Apple VisionOS beta 6 中添加了新手入门视频<br />
<br />
根据视频内容，用户只需要眼睛注视屏幕的元素即可选中目标<br />
<br />
使用手指轻轻捏去即可操作，操作非常简单方便。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ3MTE0MzY4ODExMTMwODgvcHUvaW1nL0NMWDhQU0tWVVNKcWpYSnouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724706784332664998#m</id>
            <title>Story-to-Motion：根据文本故事内容生成连续的角色的动画

该项目商汤科技研究院开发，能够处理复杂的文本描述，并将这些描述转换成具体的动作和位置信息。

它不仅能生成单一动作，还能连续地生成一系列动作，创造出连贯的动画效果。

Story-to-Motion一个关键特点是它能够生成无限长的角色动画。

这意味着，理论上，只要提供的文本故事足够长且内容连续，这个系统就能不断地根据文本内容生成相应的角色动作和行为，从而创造出持续不断的动画序列。

主要原理：

1、文本解析与动作调度：首先，系统使用大型语言模型来解析输入的长文本故事。这个过程涉及从文本中提取关键信息，如角色的动作、位置和情境。这些信息被转换成一系列的（文本，位置）对，用于后续的动作生成。

2、文本驱动的动作检索：系统接着根据提取的信息检索合适的动作。这一步骤结合了动作匹配技术、动作语义理解和轨迹约束，以确保生成的动作不仅与文本内容相符，而且在空间上也是合理的。

3、动作合成与过渡处理：系统设计了一个特殊的渐进式掩码变换器，用于处理动作之间的过渡。这个变换器解决了动作合成中常见的问题，如不自然的姿势和脚部滑动，确保动作的自然流畅。

4、无限动画生成：由于系统能够连续处理文本中的动作描述，它可以生成无限长的动画序列。这意味着只要文本故事持续，动画也会相应地持续生成。

项目及演示：https://story2motion.github.io/
论文：https://arxiv.org/abs/2311.07446</title>
            <link>https://nitter.cz/xiaohuggg/status/1724706784332664998#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724706784332664998#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 08:31:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Story-to-Motion：根据文本故事内容生成连续的角色的动画<br />
<br />
该项目商汤科技研究院开发，能够处理复杂的文本描述，并将这些描述转换成具体的动作和位置信息。<br />
<br />
它不仅能生成单一动作，还能连续地生成一系列动作，创造出连贯的动画效果。<br />
<br />
Story-to-Motion一个关键特点是它能够生成无限长的角色动画。<br />
<br />
这意味着，理论上，只要提供的文本故事足够长且内容连续，这个系统就能不断地根据文本内容生成相应的角色动作和行为，从而创造出持续不断的动画序列。<br />
<br />
主要原理：<br />
<br />
1、文本解析与动作调度：首先，系统使用大型语言模型来解析输入的长文本故事。这个过程涉及从文本中提取关键信息，如角色的动作、位置和情境。这些信息被转换成一系列的（文本，位置）对，用于后续的动作生成。<br />
<br />
2、文本驱动的动作检索：系统接着根据提取的信息检索合适的动作。这一步骤结合了动作匹配技术、动作语义理解和轨迹约束，以确保生成的动作不仅与文本内容相符，而且在空间上也是合理的。<br />
<br />
3、动作合成与过渡处理：系统设计了一个特殊的渐进式掩码变换器，用于处理动作之间的过渡。这个变换器解决了动作合成中常见的问题，如不自然的姿势和脚部滑动，确保动作的自然流畅。<br />
<br />
4、无限动画生成：由于系统能够连续处理文本中的动作描述，它可以生成无限长的动画序列。这意味着只要文本故事持续，动画也会相应地持续生成。<br />
<br />
项目及演示：<a href="https://story2motion.github.io/">story2motion.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.07446">arxiv.org/abs/2311.07446</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2OTU4OTIwMzEzNDA1NDQvcHUvaW1nL2QtQXdOWm9Eazgyb3JwOW4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724686568710164589#m</id>
            <title>R to @xiaohuggg: 3D互动门卡演示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1724686568710164589#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724686568710164589#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 07:11:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3D互动门卡演示：</p>
<p><a href="https://nitter.cz/atq_ren/status/1724506661081989315#m">nitter.cz/atq_ren/status/1724506661081989315#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724686268611928499#m</id>
            <title>R to @xiaohuggg: 应用案例，利用Spline制作一个3D门卡

教程：https://www.youtube.com/watch?v=Fv6WA9-73uk</title>
            <link>https://nitter.cz/xiaohuggg/status/1724686268611928499#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724686268611928499#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 07:10:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>应用案例，利用Spline制作一个3D门卡<br />
<br />
教程：<a href="https://www.youtube.com/watch?v=Fv6WA9-73uk">youtube.com/watch?v=Fv6WA9-7…</a></p>
<p><a href="https://nitter.cz/Aximoris/status/1724522933756874871#m">nitter.cz/Aximoris/status/1724522933756874871#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyMzk3MzcwNjUzODQ3NTUyMS9WRGhoNnUxWD9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724685982996500786#m</id>
            <title>Spline @Splinetool 宣布支持高斯泼溅（Gaussian Splatting）

现在可以使用 @Polycam3D  或者 @LumaLabsAI  从手机上捕捉任何 3D 物体，导出 .ply 文件。

然后将其导入到 Spline 中进行裁剪、调整，并嵌入到网站上。

3D高斯溅射是一种3D图像处理技术，它可以把现实世界中的物体或场景转换成3D模型，并在电脑上实时显示。特点是设置简单、渲染速度快，而且生成的3D图像质量很高。

教程：https://docs.spline.design/e17b7c105ef0433f8c5d2b39d512614e

演示：https://my.spline.design/girlstudio-8b6211e0b6ab456c8764297c6ff3ed45/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724685982996500786#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724685982996500786#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 07:09:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Spline <a href="https://nitter.cz/Splinetool" title="Spline">@Splinetool</a> 宣布支持高斯泼溅（Gaussian Splatting）<br />
<br />
现在可以使用 <a href="https://nitter.cz/Polycam3D" title="polycam">@Polycam3D</a>  或者 <a href="https://nitter.cz/LumaLabsAI" title="Luma AI">@LumaLabsAI</a>  从手机上捕捉任何 3D 物体，导出 .ply 文件。<br />
<br />
然后将其导入到 Spline 中进行裁剪、调整，并嵌入到网站上。<br />
<br />
3D高斯溅射是一种3D图像处理技术，它可以把现实世界中的物体或场景转换成3D模型，并在电脑上实时显示。特点是设置简单、渲染速度快，而且生成的3D图像质量很高。<br />
<br />
教程：<a href="https://docs.spline.design/e17b7c105ef0433f8c5d2b39d512614e">docs.spline.design/e17b7c105…</a><br />
<br />
演示：<a href="https://my.spline.design/girlstudio-8b6211e0b6ab456c8764297c6ff3ed45/">my.spline.design/girlstudio-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2ODMzNzUwNjY0MzE0ODgvcHUvaW1nL0ZGWi1xaDhPU3dnM2pQWHAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724643055532413310#m</id>
            <title>DeepMind 开发出一种用于天气预报的人工智能模型：GraphCast 。

它可以在不到一分钟的时间内完成10天内的天气预报，准确性超过了业界公认的高标准的欧洲中期天气预报中心（ECMWF）的高分辨率天气模拟系统（HRES）。

它还能够提前预测像飓风、洪水等极端天气事件。

DeepMind已开源GraphCast的代码。

主要特点：

1、高精度天气预测：GraphCast 能够提供长达10天的天气预测，其准确性超过了行业标准的高分辨率天气模拟系统（HRES），由欧洲中期天气预报中心（ECMWF）制作。

2、极端天气事件的早期预警：GraphCast 能够更早地预测极端天气事件，如准确预测气旋的路径、识别与洪水风险相关的大气河流，以及预测极端温度的发生。这种能力有助于通过更好的准备来挽救生命。

3、基于深度学习的天气预测系统：GraphCast 是一个基于机器学习和图神经网络（GNNs）的天气预测系统。通过训练，GraphCast 学习识别这些数据中的天气模式和趋势。例如，它可以学习识别导致风暴或高温的特定气候条件。

4、全球覆盖：它在全球范围内以0.25度经纬度的高分辨率进行预测，覆盖了地球表面的超过一百万个网格点。能够提供全球范围内的天气预报，这对于国际旅行、全球业务运营和气候研究都非常有用。

5、高效的预测模型：尽管GraphCast的训练过程计算量大，但最终的预测模型非常高效。使用GraphCast进行10天的预测仅需不到一分钟的时间，而传统方法如HRES可能需要数小时的超级计算机计算。

6、持续学习和适应：随着时间的推移，GraphCast 可以继续从新的气象数据中学习，不断提高其预测的准确性和可靠性。

7、广泛的应用：GraphCast 已被多个天气机构使用，包括 ECMWF，该机构已经在其网站上运行了模型预测的实时实验。https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850

8、开源代码：为了使基于AI的天气预报更加普及，DeepMind 已开源 GraphCast 模型的代码，使全球的科学家和预报员都能从中受益。

详细介绍：https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/

Science 论文：https://www.science.org/doi/10.1126/science.adi2336

Paper PDF：https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/Learning_skillful_medium-range_global_weather_forecasting.pdf

开源代码：https://github.com/google-deepmind/graphcast</title>
            <link>https://nitter.cz/xiaohuggg/status/1724643055532413310#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724643055532413310#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 04:18:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind 开发出一种用于天气预报的人工智能模型：GraphCast 。<br />
<br />
它可以在不到一分钟的时间内完成10天内的天气预报，准确性超过了业界公认的高标准的欧洲中期天气预报中心（ECMWF）的高分辨率天气模拟系统（HRES）。<br />
<br />
它还能够提前预测像飓风、洪水等极端天气事件。<br />
<br />
DeepMind已开源GraphCast的代码。<br />
<br />
主要特点：<br />
<br />
1、高精度天气预测：GraphCast 能够提供长达10天的天气预测，其准确性超过了行业标准的高分辨率天气模拟系统（HRES），由欧洲中期天气预报中心（ECMWF）制作。<br />
<br />
2、极端天气事件的早期预警：GraphCast 能够更早地预测极端天气事件，如准确预测气旋的路径、识别与洪水风险相关的大气河流，以及预测极端温度的发生。这种能力有助于通过更好的准备来挽救生命。<br />
<br />
3、基于深度学习的天气预测系统：GraphCast 是一个基于机器学习和图神经网络（GNNs）的天气预测系统。通过训练，GraphCast 学习识别这些数据中的天气模式和趋势。例如，它可以学习识别导致风暴或高温的特定气候条件。<br />
<br />
4、全球覆盖：它在全球范围内以0.25度经纬度的高分辨率进行预测，覆盖了地球表面的超过一百万个网格点。能够提供全球范围内的天气预报，这对于国际旅行、全球业务运营和气候研究都非常有用。<br />
<br />
5、高效的预测模型：尽管GraphCast的训练过程计算量大，但最终的预测模型非常高效。使用GraphCast进行10天的预测仅需不到一分钟的时间，而传统方法如HRES可能需要数小时的超级计算机计算。<br />
<br />
6、持续学习和适应：随着时间的推移，GraphCast 可以继续从新的气象数据中学习，不断提高其预测的准确性和可靠性。<br />
<br />
7、广泛的应用：GraphCast 已被多个天气机构使用，包括 ECMWF，该机构已经在其网站上运行了模型预测的实时实验。<a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850">charts.ecmwf.int/products/gr…</a><br />
<br />
8、开源代码：为了使基于AI的天气预报更加普及，DeepMind 已开源 GraphCast 模型的代码，使全球的科学家和预报员都能从中受益。<br />
<br />
详细介绍：<a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">deepmind.google/discover/blo…</a><br />
<br />
Science 论文：<a href="https://www.science.org/doi/10.1126/science.adi2336">science.org/doi/10.1126/scie…</a><br />
<br />
Paper PDF：<a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/Learning_skillful_medium-range_global_weather_forecasting.pdf">storage.googleapis.com/deepm…</a><br />
<br />
开源代码：<a href="https://github.com/google-deepmind/graphcast">github.com/google-deepmind/g…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2NDI5MDMxNDkxMjk3MjgvcHUvaW1nL3FOZTRJTURmQXc3Z2tTeVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724633998553706602#m</id>
            <title>R to @xiaohuggg: ChatGPT Plus会员账号要值钱了，这一段时间，卖账号估计能赚一笔！🙂</title>
            <link>https://nitter.cz/xiaohuggg/status/1724633998553706602#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724633998553706602#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 03:42:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Plus会员账号要值钱了，这一段时间，卖账号估计能赚一笔！🙂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724631012259557686#m</id>
            <title>⏰快讯：服务压力陡增@sama 奥特曼宣布

暂停ChatGPT Plus 会员注册

持续时间未知...</title>
            <link>https://nitter.cz/xiaohuggg/status/1724631012259557686#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724631012259557686#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 03:30:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>⏰快讯：服务压力陡增<a href="https://nitter.cz/sama" title="Sam Altman">@sama</a> 奥特曼宣布<br />
<br />
暂停ChatGPT Plus 会员注册<br />
<br />
持续时间未知...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi04ZDh5WWIwQUFyVkVELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724599830645997662#m</id>
            <title>R to @xiaohuggg: 也可以对年龄、人种等进行设置，面部会自动生成相应的年龄面部和人种...</title>
            <link>https://nitter.cz/xiaohuggg/status/1724599830645997662#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724599830645997662#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:26:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以对年龄、人种等进行设置，面部会自动生成相应的年龄面部和人种...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTg5OTEyMzE1NTc2MzIvcHUvaW1nL0otNDIxZVdDZXIxNlA5WDUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724598926748323891#m</id>
            <title>R to @xiaohuggg: 可以说不同的方言

支持将您自己的面部上传到生成模型。一些没有脸部的图像也可以生成面对对话。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724598926748323891#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724598926748323891#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:23:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可以说不同的方言<br />
<br />
支持将您自己的面部上传到生成模型。一些没有脸部的图像也可以生成面对对话。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTg1MjYxMTY4MTQ4NDgvcHUvaW1nL1pPd2M1a3lQaVB0aWllc3cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724597996225232906#m</id>
            <title>这个超有意思 哈哈哈哈 😂 字节跳动搞的

ChatAnything：可以与 LLMs 增强角色进行视频聊天

你可以通过文本描述，生成具有独特个性、外观和声音的虚拟角色人物。

而且这个人物不仅有自己的外观，还有独特的声音和个性，还可以跟你进行语音对话和视频聊天。😅

ChatAnything的工作原理基于几个关键的技术组件，这些组件共同协作，使得用户能够通过文本描述来创建和动画化个性化的虚拟角色。

以下是其主要工作流程和组件：

1、生成个性化角色：

•用户提供一个文本描述，定义想要创建的角色的特征和个性。

•大语言模型（LLM）根据这些描述生成一个具有特定个性的角色。

2、声音和外观的生成：

•声音混合（MoV）：这一部分利用文本到语音（TTS）技术。根据用户的文本描述，系统自动选择最匹配的声音特征，为角色生成独特的声音。

•扩散器混合（MoD）：结合了文本到图像生成技术和说话头部算法。这一步骤简化了生成说话对象的外观的过程。

3、动画化角色：

•一旦角色的个性、声音和外观被生成，系统使用这些信息来动画化角色。

•这包括将声音信号与生成的图像相结合，使角色能够根据用户的指令进行“说话”和“表现”。

4、面部运动的生成：

•为了使生成的角色更加逼真，系统还包括了面部运动的生成。

•这一部分涉及到像素级引导，它在图像生成阶段注入人脸标记，以确保面部运动的自然性和准确性。

ChatAnything的目标是通过文本输入，使用户能够以任何人格化的方式动画化任何事物。

项目地址：https://chatanything.github.io/
论文：https://arxiv.org/abs/2311.06772
GitHub：https://github.com/zhoudaquan/ChatAnything
在线演示：https://26fed97b4a7706bed0.gradio.live/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724597996225232906#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724597996225232906#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:19:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个超有意思 哈哈哈哈 😂 字节跳动搞的<br />
<br />
ChatAnything：可以与 LLMs 增强角色进行视频聊天<br />
<br />
你可以通过文本描述，生成具有独特个性、外观和声音的虚拟角色人物。<br />
<br />
而且这个人物不仅有自己的外观，还有独特的声音和个性，还可以跟你进行语音对话和视频聊天。😅<br />
<br />
ChatAnything的工作原理基于几个关键的技术组件，这些组件共同协作，使得用户能够通过文本描述来创建和动画化个性化的虚拟角色。<br />
<br />
以下是其主要工作流程和组件：<br />
<br />
1、生成个性化角色：<br />
<br />
•用户提供一个文本描述，定义想要创建的角色的特征和个性。<br />
<br />
•大语言模型（LLM）根据这些描述生成一个具有特定个性的角色。<br />
<br />
2、声音和外观的生成：<br />
<br />
•声音混合（MoV）：这一部分利用文本到语音（TTS）技术。根据用户的文本描述，系统自动选择最匹配的声音特征，为角色生成独特的声音。<br />
<br />
•扩散器混合（MoD）：结合了文本到图像生成技术和说话头部算法。这一步骤简化了生成说话对象的外观的过程。<br />
<br />
3、动画化角色：<br />
<br />
•一旦角色的个性、声音和外观被生成，系统使用这些信息来动画化角色。<br />
<br />
•这包括将声音信号与生成的图像相结合，使角色能够根据用户的指令进行“说话”和“表现”。<br />
<br />
4、面部运动的生成：<br />
<br />
•为了使生成的角色更加逼真，系统还包括了面部运动的生成。<br />
<br />
•这一部分涉及到像素级引导，它在图像生成阶段注入人脸标记，以确保面部运动的自然性和准确性。<br />
<br />
ChatAnything的目标是通过文本输入，使用户能够以任何人格化的方式动画化任何事物。<br />
<br />
项目地址：<a href="https://chatanything.github.io/">chatanything.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.06772">arxiv.org/abs/2311.06772</a><br />
GitHub：<a href="https://github.com/zhoudaquan/ChatAnything">github.com/zhoudaquan/ChatAn…</a><br />
在线演示：<a href="https://26fed97b4a7706bed0.gradio.live/">26fed97b4a7706bed0.gradio.li…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTUzMTk0MTYyNzA4NDgvcHUvaW1nL0JnZ2xVbHU0dk54Z2tUVk8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724591752424849469#m</id>
            <title>R to @xiaohuggg: 手机上的演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1724591752424849469#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724591752424849469#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 00:54:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>手机上的演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTExMzk5MTM5NDA5OTIvcHUvaW1nL01rbjFjTnIxR1RJZ25MR2YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724591088961507622#m</id>
            <title>Endless Zoom ：可以不断无限放大和扩充图像

Endless Zoom利用潜在一致性模型（LCMs）来实现图像的无限扩充。

通过LCMs的快速图像生成能力，可以生成连续的、无缝连接的图像内容，从而实现“无限扩充”。你可以不断地放大或缩小图像，而图像内容会持续生成，不会重复或结束。

在线体验：https://endless-zoom.vercel.app/

作者：@chigozienri

GitHub：https://github.com/replicate/endless-zoom/

LCMs是一个基于Stable Diffusion的图像生成模型，但生成图像的速度更快，只需要4到8步就能生成一张高质量的图像。

通过在 M1 或 M2 Mac 上运行 LCMs，你可以以每秒一张的速度生成 512x512 图像。

LCMs介绍：
https://x.com/xiaohuggg/status/1717562806822981835</title>
            <link>https://nitter.cz/xiaohuggg/status/1724591088961507622#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724591088961507622#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 00:52:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Endless Zoom ：可以不断无限放大和扩充图像<br />
<br />
Endless Zoom利用潜在一致性模型（LCMs）来实现图像的无限扩充。<br />
<br />
通过LCMs的快速图像生成能力，可以生成连续的、无缝连接的图像内容，从而实现“无限扩充”。你可以不断地放大或缩小图像，而图像内容会持续生成，不会重复或结束。<br />
<br />
在线体验：<a href="https://endless-zoom.vercel.app/">endless-zoom.vercel.app/</a><br />
<br />
作者：<a href="https://nitter.cz/chigozienri" title="Chigozie Nri">@chigozienri</a><br />
<br />
GitHub：<a href="https://github.com/replicate/endless-zoom/">github.com/replicate/endless…</a><br />
<br />
LCMs是一个基于Stable Diffusion的图像生成模型，但生成图像的速度更快，只需要4到8步就能生成一张高质量的图像。<br />
<br />
通过在 M1 或 M2 Mac 上运行 LCMs，你可以以每秒一张的速度生成 512x512 图像。<br />
<br />
LCMs介绍：<br />
<a href="https://x.com/xiaohuggg/status/1717562806822981835">x.com/xiaohuggg/status/17175…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1ODMyMzczNzEyMzYzNTIvcHUvaW1nL2VmT0Nxam9kRWN4X3RGbDMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724575584851005828#m</id>
            <title>R to @xiaohuggg: 还有钢琴的🎹</title>
            <link>https://nitter.cz/xiaohuggg/status/1724575584851005828#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724575584851005828#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 23:50:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还有钢琴的🎹</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MTk1MzE1Mzc1ODUxMDI4NDkvcHUvaW1nLy1ZbFVabEZVcXNFdFA5ak8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724574547360509958#m</id>
            <title>学吉他的完美工具

cool…</title>
            <link>https://nitter.cz/xiaohuggg/status/1724574547360509958#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724574547360509958#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 23:46:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学吉他的完美工具<br />
<br />
cool…</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0NjIxMjk3MDc3MzI5OTUvcHUvaW1nL1dtM1JTNHdQNHIzSWFRMjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>