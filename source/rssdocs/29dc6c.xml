<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751585830911836333#m</id>
            <title>谷歌 TPU v5p AI 芯片击败英伟达H100 速度是其的3.4到4.8倍

Google最近推出了其最新旗舰张量处理单元（TPU）v5p，这是一款专门设计的AI加速器，用于AI训练和推理，标志着Google在对抗市场领导者Nvidia的GPU方面迈出了重要一步。

TPU v5p已被部署以支持Google的“AI超级计算机”架构，这是一种专门为运行AI应用而构建的超级计算架构，与通常运行科学工作负载的超级计算机不同。

核心规格对比：

TPU v5p：每个集群（Pod）拥有8,960个芯片，相比之下v4版为4,096个芯片。新Pod提供4,800Gbps的吞吐量，并具有95GB的高带宽内存（HBM），而v4版为32GB HBM RAM。

Nvidia H100：被认为是AI工作负载最佳的图形卡之一，其训练工作负载的速度是Nvidia A100 GPU的四倍。

性能对比：

Google的v5p TPU在训练大型语言模型方面的速度是TPU v4的2.8倍，提供2.1倍的价值。尽管今年早些时候发布的中间版本TPU v5e在性价比方面表现最佳，但其速度仅是TPU v4的1.9倍，这使得TPU v5p成为最强大的选项。

根据Google自己的数据，TPU v4在性能上估计比A100快1.2到1.7倍。粗略计算表明，TPU v5p的速度大约是A100的3.4到4.8倍，这使其与H100相当或更优，尽管需要更详细的基准测试才能得出结论。

Google的TPU v5p AI芯片以其在速度、内存和带宽方面的显著提升，成为Nvidia H100的有力竞争者。

不同于Nvidia的做法，Google的定制TPU仅在内部使用，用于支持其自身的产品和服务，包括Gmail、YouTube和Android等服务，并且已被用于训练Gemini AI模型。

详细：https://www.techradar.com/pro/google-is-rapidly-turning-into-a-formidable-opponent-to-bff-nvidia-the-tpu-v5p-ai-chip-powering-its-hypercomputer-is-faster-and-has-more-memory-and-bandwidth-than-ever-before-beating-even-the-mighty-h100</title>
            <link>https://nitter.cz/xiaohuggg/status/1751585830911836333#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751585830911836333#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 12:39:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌 TPU v5p AI 芯片击败英伟达H100 速度是其的3.4到4.8倍<br />
<br />
Google最近推出了其最新旗舰张量处理单元（TPU）v5p，这是一款专门设计的AI加速器，用于AI训练和推理，标志着Google在对抗市场领导者Nvidia的GPU方面迈出了重要一步。<br />
<br />
TPU v5p已被部署以支持Google的“AI超级计算机”架构，这是一种专门为运行AI应用而构建的超级计算架构，与通常运行科学工作负载的超级计算机不同。<br />
<br />
核心规格对比：<br />
<br />
TPU v5p：每个集群（Pod）拥有8,960个芯片，相比之下v4版为4,096个芯片。新Pod提供4,800Gbps的吞吐量，并具有95GB的高带宽内存（HBM），而v4版为32GB HBM RAM。<br />
<br />
Nvidia H100：被认为是AI工作负载最佳的图形卡之一，其训练工作负载的速度是Nvidia A100 GPU的四倍。<br />
<br />
性能对比：<br />
<br />
Google的v5p TPU在训练大型语言模型方面的速度是TPU v4的2.8倍，提供2.1倍的价值。尽管今年早些时候发布的中间版本TPU v5e在性价比方面表现最佳，但其速度仅是TPU v4的1.9倍，这使得TPU v5p成为最强大的选项。<br />
<br />
根据Google自己的数据，TPU v4在性能上估计比A100快1.2到1.7倍。粗略计算表明，TPU v5p的速度大约是A100的3.4到4.8倍，这使其与H100相当或更优，尽管需要更详细的基准测试才能得出结论。<br />
<br />
Google的TPU v5p AI芯片以其在速度、内存和带宽方面的显著提升，成为Nvidia H100的有力竞争者。<br />
<br />
不同于Nvidia的做法，Google的定制TPU仅在内部使用，用于支持其自身的产品和服务，包括Gmail、YouTube和Android等服务，并且已被用于训练Gemini AI模型。<br />
<br />
详细：<a href="https://www.techradar.com/pro/google-is-rapidly-turning-into-a-formidable-opponent-to-bff-nvidia-the-tpu-v5p-ai-chip-powering-its-hypercomputer-is-faster-and-has-more-memory-and-bandwidth-than-ever-before-beating-even-the-mighty-h100">techradar.com/pro/google-is-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U3aHAybWFZQUFNMnF3LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751523966899126743#m</id>
            <title>拜登政府提出了一项新提案，要求美国的云计算公司必须识别是否有外国实体正在访问美国数据中心以进行人工智能模型的训练。

这是一系列旨在防止中国利用美国技术开发人工智能的措施之一。

美国商务部长吉娜·雷蒙多（Gina Raimondo）表示，不能允许非国家行为者、中国或其他我们不希望访问我们云的人，使用美国的云计算资源来训练他们的模型。

这项被称为“了解你的客户”（Know Your Customer）的规定已于周五公开审查，并将于周一正式发布。

这项提案旨在限制中国通过绕过现有限制使用美国云资源来训练他们的AI模型，特别是在美国政府对中国发展高级AI系统出于多种国家安全考虑而采取措施阻止北京接收尖端美国技术以加强其军事力量的背景下。

提案将要求美国云计算公司通过“了解你的客户程序”或“客户识别计划”验证外国人注册或维护使用美国云计算的账户的身份，并设置识别外国用户的最低标准，要求云计算公司每年证明合规性。

https://www.pymnts.com/news/security-and-risk/2024/biden-administration-aims-to-require-cloud-computing-companies-to-disclose-customers/</title>
            <link>https://nitter.cz/xiaohuggg/status/1751523966899126743#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751523966899126743#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 08:33:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>拜登政府提出了一项新提案，要求美国的云计算公司必须识别是否有外国实体正在访问美国数据中心以进行人工智能模型的训练。<br />
<br />
这是一系列旨在防止中国利用美国技术开发人工智能的措施之一。<br />
<br />
美国商务部长吉娜·雷蒙多（Gina Raimondo）表示，不能允许非国家行为者、中国或其他我们不希望访问我们云的人，使用美国的云计算资源来训练他们的模型。<br />
<br />
这项被称为“了解你的客户”（Know Your Customer）的规定已于周五公开审查，并将于周一正式发布。<br />
<br />
这项提案旨在限制中国通过绕过现有限制使用美国云资源来训练他们的AI模型，特别是在美国政府对中国发展高级AI系统出于多种国家安全考虑而采取措施阻止北京接收尖端美国技术以加强其军事力量的背景下。<br />
<br />
提案将要求美国云计算公司通过“了解你的客户程序”或“客户识别计划”验证外国人注册或维护使用美国云计算的账户的身份，并设置识别外国用户的最低标准，要求云计算公司每年证明合规性。<br />
<br />
<a href="https://www.pymnts.com/news/security-and-risk/2024/biden-administration-aims-to-require-cloud-computing-companies-to-disclose-customers/">pymnts.com/news/security-and…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0U2cHdoWWFnQUVpRXplLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751446948803178868#m</id>
            <title>AI产品案例：

GPT 4V视觉模态的正确用法

识别物体，并给出单词

辅助小朋友轻松有趣的学习英语...</title>
            <link>https://nitter.cz/xiaohuggg/status/1751446948803178868#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751446948803178868#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 03:27:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI产品案例：<br />
<br />
GPT 4V视觉模态的正确用法<br />
<br />
识别物体，并给出单词<br />
<br />
辅助小朋友轻松有趣的学习英语...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTE0NDMzNDM0MTcyNTc5ODQvcHUvaW1nL1NwWHZEc0JnQmVWWHEzQ1QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751442652388814956#m</id>
            <title>兄弟们 这个厉害了...

StreamRAG：一个视频搜索和流媒体代理工具

他可以让你在2分钟内基于你的视频数据构建一个定制的个人GPT，然后你可以和你的视频进行对话。

它能够在数百小时的视频内容中找到你输符合你需求的相关视频时刻，并立即返回一个视频剪辑。

也就是说它能搜索视频内容的任意时刻。

它能够迅速浏览存储的大量视频资料，找到包含这些内容或主题的视频片段，并把这些片段展示给你，这样你就能直接观看到与你搜索内容相关的视频部分。

主要能力：

StreamRAG允许用户上传视频，创建视频集合，并在这些视频中进行搜索，以获得实时的视频回应或编辑。此外，用户还可以将他们的视频集合发布到ChatGPT商店，以便他人搜索和使用。

1、视频库创建： 上传多个视频以创建视频库或集合。

2、视频搜索与回应： 在这些视频中搜索，能立即获得实时的视频回应或编译结果。

3、GPTs发布： 在ChatGPT的GPT商店发布你的可搜索集合。

4、文本回答总结（RAG）： 接收总结性的文本回答。

5、视频关键洞察： 从特定视频中获得关键洞察，例如“第31集的要点”。

GitHub：https://github.com/video-db/StreamRAG
作者：@ashu_trv</title>
            <link>https://nitter.cz/xiaohuggg/status/1751442652388814956#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751442652388814956#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Jan 2024 03:10:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 这个厉害了...<br />
<br />
StreamRAG：一个视频搜索和流媒体代理工具<br />
<br />
他可以让你在2分钟内基于你的视频数据构建一个定制的个人GPT，然后你可以和你的视频进行对话。<br />
<br />
它能够在数百小时的视频内容中找到你输符合你需求的相关视频时刻，并立即返回一个视频剪辑。<br />
<br />
也就是说它能搜索视频内容的任意时刻。<br />
<br />
它能够迅速浏览存储的大量视频资料，找到包含这些内容或主题的视频片段，并把这些片段展示给你，这样你就能直接观看到与你搜索内容相关的视频部分。<br />
<br />
主要能力：<br />
<br />
StreamRAG允许用户上传视频，创建视频集合，并在这些视频中进行搜索，以获得实时的视频回应或编辑。此外，用户还可以将他们的视频集合发布到ChatGPT商店，以便他人搜索和使用。<br />
<br />
1、视频库创建： 上传多个视频以创建视频库或集合。<br />
<br />
2、视频搜索与回应： 在这些视频中搜索，能立即获得实时的视频回应或编译结果。<br />
<br />
3、GPTs发布： 在ChatGPT的GPT商店发布你的可搜索集合。<br />
<br />
4、文本回答总结（RAG）： 接收总结性的文本回答。<br />
<br />
5、视频关键洞察： 从特定视频中获得关键洞察，例如“第31集的要点”。<br />
<br />
GitHub：<a href="https://github.com/video-db/StreamRAG">github.com/video-db/StreamRA…</a><br />
作者：<a href="https://nitter.cz/ashu_trv" title="Ashutosh">@ashu_trv</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTE0NDAzODE4Nzc5MTk3NDQvcHUvaW1nL0Y1eU9KUDhlU0FBbHppRm4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751293428896579719#m</id>
            <title>切糕刺客🥷

刀刀要命…

周末快乐…哈哈哈哈😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1751293428896579719#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751293428896579719#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:17:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>切糕刺客🥷<br />
<br />
刀刀要命…<br />
<br />
周末快乐…哈哈哈哈😂</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUxMjkyNTI0OTA2NTU3NDQwL2ltZy9EdmlPWGVfeC1QRDUwNjRYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751202779501384052#m</id>
            <title>Gabriele Romagnoli @GabRoXR  分享使用 @ShapesXR 工具为 #AppleVisionPro 进行空间计算创新设计，打破2D屏幕局限性！

你可以灵活地在虚拟现实（VR）和混合现实（MR）这两种模式中无缝切换。

让你根据设计的需要来调整自己的视角和与对象的互动方式！

• 如果你需要对某个设计元素进行细微的调整或操作，你可以选择“缩小”视角，这样就能更精确地控制和操纵这个对象。

• 相反，如果你想要从更广阔的视角来观察你设计的整个场景或对象（比如一个玩具房子），你可以选择“放大”视角，这样就能获得一个宏观的视图。

使用 @ShapesXR 工具为 #VisionPro 设计的流程：

1、导入与同步：将你在 Figma 中创建的资产和组件导入到 ShapesXR，开始在混合现实中设计，无需3D设计技能。

2、原型化互动：利用 Quest 2 或 3 的头部姿势或 Quest Pro 的眼球追踪技术，原型化注视和捏合等互动。

3、无缝切换：在虚拟现实 (VR) 和混合现实 (MR) 之间无缝切换，根据需要调整视角大小，精确操纵对象或从宏观角度审视设计。

这一流程突破了传统2D设计的限制，为设计师提供了一个直观、灵活的工作环境，以勇敢、大胆的方式进行空间设计。</title>
            <link>https://nitter.cz/xiaohuggg/status/1751202779501384052#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751202779501384052#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 11:17:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gabriele Romagnoli <a href="https://nitter.cz/GabRoXR" title="Gabriele Romagnoli">@GabRoXR</a>  分享使用 <a href="https://nitter.cz/ShapesXR" title="ShapesXR">@ShapesXR</a> 工具为 <a href="https://nitter.cz/search?q=%23AppleVisionPro">#AppleVisionPro</a> 进行空间计算创新设计，打破2D屏幕局限性！<br />
<br />
你可以灵活地在虚拟现实（VR）和混合现实（MR）这两种模式中无缝切换。<br />
<br />
让你根据设计的需要来调整自己的视角和与对象的互动方式！<br />
<br />
• 如果你需要对某个设计元素进行细微的调整或操作，你可以选择“缩小”视角，这样就能更精确地控制和操纵这个对象。<br />
<br />
• 相反，如果你想要从更广阔的视角来观察你设计的整个场景或对象（比如一个玩具房子），你可以选择“放大”视角，这样就能获得一个宏观的视图。<br />
<br />
使用 <a href="https://nitter.cz/ShapesXR" title="ShapesXR">@ShapesXR</a> 工具为 <a href="https://nitter.cz/search?q=%23VisionPro">#VisionPro</a> 设计的流程：<br />
<br />
1、导入与同步：将你在 Figma 中创建的资产和组件导入到 ShapesXR，开始在混合现实中设计，无需3D设计技能。<br />
<br />
2、原型化互动：利用 Quest 2 或 3 的头部姿势或 Quest Pro 的眼球追踪技术，原型化注视和捏合等互动。<br />
<br />
3、无缝切换：在虚拟现实 (VR) 和混合现实 (MR) 之间无缝切换，根据需要调整视角大小，精确操纵对象或从宏观角度审视设计。<br />
<br />
这一流程突破了传统2D设计的限制，为设计师提供了一个直观、灵活的工作环境，以勇敢、大胆的方式进行空间设计。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUxMjAyNjQyOTk3NzIzMTM2L2ltZy90Z1A3T0NrM1VnYlhMZ2R5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751197122022768769#m</id>
            <title>选择内容

自动总结…</title>
            <link>https://nitter.cz/xiaohuggg/status/1751197122022768769#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751197122022768769#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 10:55:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>选择内容<br />
<br />
自动总结…</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkzODQ4NjM5NTY3ODMxMDQvcHUvaW1nLzBlM09EcXYyTlFIbkxDWkUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751088229397495938#m</id>
            <title>R to @xiaohuggg: 控制选项</title>
            <link>https://nitter.cz/xiaohuggg/status/1751088229397495938#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751088229397495938#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 03:42:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>控制选项</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0UwZGRlNGJrQUFnRWd0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751088219461189705#m</id>
            <title>DALL·E 3将允许对图像进行更精细化控制 

新的DALL·E Contorls允许调整提示精度（加强/严格），选择风格（自动/自然/鲜艳），设定长宽比（自动/正方形/宽屏/垂直）等，

比起只能通过聊天提示生成图片，有了很大进步，用户可以控制图片生成效果！</title>
            <link>https://nitter.cz/xiaohuggg/status/1751088219461189705#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751088219461189705#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 03:42:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DALL·E 3将允许对图像进行更精细化控制 <br />
<br />
新的DALL·E Contorls允许调整提示精度（加强/严格），选择风格（自动/自然/鲜艳），设定长宽比（自动/正方形/宽屏/垂直）等，<br />
<br />
比起只能通过聊天提示生成图片，有了很大进步，用户可以控制图片生成效果！</p>
<p><a href="https://nitter.cz/btibor91/status/1750965235987501452#m">nitter.cz/btibor91/status/1750965235987501452#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751081213459415164#m</id>
            <title>DuckDB：基于大语言模型的文本到SQL

DuckDB-NSQL-7B，这是一个专门为DuckDB数据库设计的文本到SQL的模型。

你可以使用自然语言说描述你的需求，它会自动转换成SQL代码。比如，你告诉它“从test.csv文件创建一个新表”，它就能自动写出相应的SQL命令。

也就是可以使用自然语言来和你的数据库聊天。

这大大简化了数据库查询的过程，使得即使是不太懂SQL语言的用户也能轻松地与数据库进行交互和数据处理。

DuckDB-NSQL-7B模型是基于大约200,000条合成生成并验证的DuckDB SQL查询以及来自Numbers Station的超过250,000条一般性文本到SQL问题训练而成的。

它不仅能生成有用的DuckDB代码片段，还能生成用于回答分析问题的SQL查询。

DuckDB-NSQL-7B主要特点：

1、自然语言处理能力： 能够理解和处理自然语言输入，将用户用普通话语描述的数据查询需求转换成SQL查询代码。

2、针对DuckDB优化： 专为DuckDB数据库定制，能够充分利用DuckDB的特性和功能。

3、高效的查询生成： 对于常见的数据查询任务，如创建表、选择数据、排序和过滤等，都能快速生成准确的SQL代码。

4、用户友好的交互： 用户无需深入了解SQL语法，只需通过自然的语言描述就可以进行复杂的数据查询。

5、文档式的查询指导： 模型知识覆盖DuckDB 0.9.2中记录的所有功能，包括官方扩展，类似于一个随时可用的文档查询工具。

6、低延迟： 为了提供低延迟的SQL辅助特性，该模型采用了相对较小的模型大小，使得推理过程更快、成本更低。

7、广泛的应用场景： 不仅能生成DuckDB的代码片段，还能生成用于回答分析性问题的SQL查询。

8、开源和易于访问： 模型权重在Hugging Face上完全公开，方便用户下载和使用。

9、本地运行支持： 支持与llama.cpp一起在本地完全体验，提供了完整的本地运行指导。

用户可以在Hugging Face空间上尝试这个模型。只需用自然语言指令提示模型，描述你想要的查询类型。以下是一些示例：

示例1：从test.csv创建一个名为tmp的新表。
示例2：从出租车表中获取以_amount结尾的所有列。
示例3：从出租车表中获取乘客数量、行程距离和费用金额，并按所有这些排序。
示例4：获取2022年12月最长的行程。

如果想要在本地完全体验DuckDB-NSQL-7B，可以前往GitHub仓库或GGUF readme查看更多信息。

详细介绍：https://motherduck.com/blog/duckdb-text2sql-llm/
GitHub：https://github.com/NumbersStationAI/DuckDB-NSQL
Hugging Face：https://huggingface.co/spaces/motherduckdb/DuckDB-NSQL-7B</title>
            <link>https://nitter.cz/xiaohuggg/status/1751081213459415164#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751081213459415164#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 03:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DuckDB：基于大语言模型的文本到SQL<br />
<br />
DuckDB-NSQL-7B，这是一个专门为DuckDB数据库设计的文本到SQL的模型。<br />
<br />
你可以使用自然语言说描述你的需求，它会自动转换成SQL代码。比如，你告诉它“从test.csv文件创建一个新表”，它就能自动写出相应的SQL命令。<br />
<br />
也就是可以使用自然语言来和你的数据库聊天。<br />
<br />
这大大简化了数据库查询的过程，使得即使是不太懂SQL语言的用户也能轻松地与数据库进行交互和数据处理。<br />
<br />
DuckDB-NSQL-7B模型是基于大约200,000条合成生成并验证的DuckDB SQL查询以及来自Numbers Station的超过250,000条一般性文本到SQL问题训练而成的。<br />
<br />
它不仅能生成有用的DuckDB代码片段，还能生成用于回答分析问题的SQL查询。<br />
<br />
DuckDB-NSQL-7B主要特点：<br />
<br />
1、自然语言处理能力： 能够理解和处理自然语言输入，将用户用普通话语描述的数据查询需求转换成SQL查询代码。<br />
<br />
2、针对DuckDB优化： 专为DuckDB数据库定制，能够充分利用DuckDB的特性和功能。<br />
<br />
3、高效的查询生成： 对于常见的数据查询任务，如创建表、选择数据、排序和过滤等，都能快速生成准确的SQL代码。<br />
<br />
4、用户友好的交互： 用户无需深入了解SQL语法，只需通过自然的语言描述就可以进行复杂的数据查询。<br />
<br />
5、文档式的查询指导： 模型知识覆盖DuckDB 0.9.2中记录的所有功能，包括官方扩展，类似于一个随时可用的文档查询工具。<br />
<br />
6、低延迟： 为了提供低延迟的SQL辅助特性，该模型采用了相对较小的模型大小，使得推理过程更快、成本更低。<br />
<br />
7、广泛的应用场景： 不仅能生成DuckDB的代码片段，还能生成用于回答分析性问题的SQL查询。<br />
<br />
8、开源和易于访问： 模型权重在Hugging Face上完全公开，方便用户下载和使用。<br />
<br />
9、本地运行支持： 支持与llama.cpp一起在本地完全体验，提供了完整的本地运行指导。<br />
<br />
用户可以在Hugging Face空间上尝试这个模型。只需用自然语言指令提示模型，描述你想要的查询类型。以下是一些示例：<br />
<br />
示例1：从test.csv创建一个名为tmp的新表。<br />
示例2：从出租车表中获取以_amount结尾的所有列。<br />
示例3：从出租车表中获取乘客数量、行程距离和费用金额，并按所有这些排序。<br />
示例4：获取2022年12月最长的行程。<br />
<br />
如果想要在本地完全体验DuckDB-NSQL-7B，可以前往GitHub仓库或GGUF readme查看更多信息。<br />
<br />
详细介绍：<a href="https://motherduck.com/blog/duckdb-text2sql-llm/">motherduck.com/blog/duckdb-t…</a><br />
GitHub：<a href="https://github.com/NumbersStationAI/DuckDB-NSQL">github.com/NumbersStationAI/…</a><br />
Hugging Face：<a href="https://huggingface.co/spaces/motherduckdb/DuckDB-NSQL-7B">huggingface.co/spaces/mother…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0V4N2EtTGJVQUFtM1YxLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dFeDdhLUxiVUFBbTNWMS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751064514886594962#m</id>
            <title>好文推荐：AI时代UX的高标准：Perplexity

Perplexity通过AI重塑了网络搜索的方式，获得了业界的关注和商业成功。

这篇文章通过以下几个方面来阐述Perplexity是如何成功应用 Jakob Nielson 在1994年提出的：“10个可用性启发式原则”来提升用户体验。

1、系统状态的可见性： 设计应始终让用户了解正在发生的事情，通过适当的反馈在合理的时间内。

2、使用用户的语言： 使用用户熟悉的词汇、短语和概念，而不是内部术语。

3、用户控制和自由度： 用户经常会误操作，他们需要一个明显的“紧急出口”来离开不想要的动作，而无需经历复杂的过程。

4、一致性和标准化： 用户不应该对不同的词语、情境或行为是否表示同一事物感到困惑。

5、错误预防： 良好的错误信息很重要，但最好的设计是事先预防问题的发生。

6、识别而非回忆： 尽量减少用户的记忆负担，使元素、动作和选项可见。

7、灵活性和使用效率： 隐藏对初学者不可见的快捷方式，可以加快专家用户的交互速度。

8、美观和简约设计： 界面中不应包含不相关或很少需要的信息。

9、帮助用户识别、诊断和从错误中恢复： 错误信息应该用简单的语言表达，准确指出问题，并提出建设性的解决方案。

10、帮助和文档： 最好的系统是不需要额外解释的，但有时可能需要提供文档帮助用户完成任务。

文章最后强调，Perplexity如何将这些原则成功地融入其产品设计中，使其成为AI产品中用户体验的典范。

对于2024年从事AI产品开发的人来说，可以从Perplexity的例子中学习，即怎样通过传统的用户体验原则来提升现代技术产品的易用性。

Perplexity展示了即便是最先进的技术，也需要以用户为中心，简化设计，并提供直观的用户界面来满足广泛的用户需求。

通过细致入微地考虑用户体验的各个方面，AI产品可以更好地被大众接受和使用。

图文完整内容：https://mttmr.com/2024/01/10/perplexitys-high-bar-for-ux-in-the-age-of-ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1751064514886594962#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751064514886594962#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 02:08:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>好文推荐：AI时代UX的高标准：Perplexity<br />
<br />
Perplexity通过AI重塑了网络搜索的方式，获得了业界的关注和商业成功。<br />
<br />
这篇文章通过以下几个方面来阐述Perplexity是如何成功应用 Jakob Nielson 在1994年提出的：“10个可用性启发式原则”来提升用户体验。<br />
<br />
1、系统状态的可见性： 设计应始终让用户了解正在发生的事情，通过适当的反馈在合理的时间内。<br />
<br />
2、使用用户的语言： 使用用户熟悉的词汇、短语和概念，而不是内部术语。<br />
<br />
3、用户控制和自由度： 用户经常会误操作，他们需要一个明显的“紧急出口”来离开不想要的动作，而无需经历复杂的过程。<br />
<br />
4、一致性和标准化： 用户不应该对不同的词语、情境或行为是否表示同一事物感到困惑。<br />
<br />
5、错误预防： 良好的错误信息很重要，但最好的设计是事先预防问题的发生。<br />
<br />
6、识别而非回忆： 尽量减少用户的记忆负担，使元素、动作和选项可见。<br />
<br />
7、灵活性和使用效率： 隐藏对初学者不可见的快捷方式，可以加快专家用户的交互速度。<br />
<br />
8、美观和简约设计： 界面中不应包含不相关或很少需要的信息。<br />
<br />
9、帮助用户识别、诊断和从错误中恢复： 错误信息应该用简单的语言表达，准确指出问题，并提出建设性的解决方案。<br />
<br />
10、帮助和文档： 最好的系统是不需要额外解释的，但有时可能需要提供文档帮助用户完成任务。<br />
<br />
文章最后强调，Perplexity如何将这些原则成功地融入其产品设计中，使其成为AI产品中用户体验的典范。<br />
<br />
对于2024年从事AI产品开发的人来说，可以从Perplexity的例子中学习，即怎样通过传统的用户体验原则来提升现代技术产品的易用性。<br />
<br />
Perplexity展示了即便是最先进的技术，也需要以用户为中心，简化设计，并提供直观的用户界面来满足广泛的用户需求。<br />
<br />
通过细致入微地考虑用户体验的各个方面，AI产品可以更好地被大众接受和使用。<br />
<br />
图文完整内容：<a href="https://mttmr.com/2024/01/10/perplexitys-high-bar-for-ux-in-the-age-of-ai/">mttmr.com/2024/01/10/perplex…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA5MTAwMjE0OTQ2MDc4NzIvcHUvaW1nL0hiM2xxaDdQS2RnQmp6VWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751055273110954319#m</id>
            <title>R to @xiaohuggg: 操作演示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1751055273110954319#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751055273110954319#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 01:31:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>操作演示：</p>
<p><a href="https://nitter.cz/danshipper/status/1751017376143794415#m">nitter.cz/danshipper/status/1751017376143794415#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751055149001551943#m</id>
            <title>R to @xiaohuggg: 输入 @ 可以显示最近使用过的4个GPTs…

也可以通过搜索查找其他…

然后输入你的需求即可！</title>
            <link>https://nitter.cz/xiaohuggg/status/1751055149001551943#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751055149001551943#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 01:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>输入 @ 可以显示最近使用过的4个GPTs…<br />
<br />
也可以通过搜索查找其他…<br />
<br />
然后输入你的需求即可！</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V6X1gzMGF3QUF1cDg2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751055137827946681#m</id>
            <title>ChatGPT又更新了 推出了新的Mention功能！

你可以在 ChatGPT的聊天窗口中通过 @ 来直接召唤任何GPTs，就像Discord里面召唤其他机器人一样！

这样不用来回切换窗口就能完成不同任务！

这样可以实现调用多个机器人的联动操作，完成一个任务🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1751055137827946681#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751055137827946681#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 01:30:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT又更新了 推出了新的Mention功能！<br />
<br />
你可以在 ChatGPT的聊天窗口中通过 @ 来直接召唤任何GPTs，就像Discord里面召唤其他机器人一样！<br />
<br />
这样不用来回切换窗口就能完成不同任务！<br />
<br />
这样可以实现调用多个机器人的联动操作，完成一个任务🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V6X1hWdGJ3QUFmV2U4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750829664766198026#m</id>
            <title>Diffuse to Choose：在线购物“虚拟试穿”模型

这个模型能让你在将任何商品放入任何环境中，同时和环境完美融合！

比如，你可以把一个在线商店的椅子放进你的客厅的照片里，看看它实际放在那里会是什么样子。

同时保证在不同环境中看起来自然和真实！

简而言之，它帮助用户更好地了解产品在真实环境中的样子，提高了在线购物的体验。

1、虚拟试穿技术：允许用户在不同环境中虚拟放置商品，实现逼真的在线购物体验。

2、与传统扩散模型相比，DTC模型能更好地捕捉商品细节，提升修复质量。采用特殊的算法，将来自参考图像的细粒度特征直接融入主扩散模型的潜在特征图中，保证产品与环境的高度融合。

3、高效平衡：在快速推断与保持高保真细节方面达到了有效的平衡。

4、广泛测试与评估：在不同数据集上测试，证明了DTC模型相较于现有技术的优越性。

5、场景适应性：能够处理多种场景中的图像，确保产品与场景的无缝整合。

6、快速推断能力：提供快速且高效的零次射推断，加快虚拟试穿过程。

项目及演示：https://diffuse2choose.github.io
论文：https://arxiv.org/abs/2401.13795</title>
            <link>https://nitter.cz/xiaohuggg/status/1750829664766198026#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750829664766198026#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 10:34:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Diffuse to Choose：在线购物“虚拟试穿”模型<br />
<br />
这个模型能让你在将任何商品放入任何环境中，同时和环境完美融合！<br />
<br />
比如，你可以把一个在线商店的椅子放进你的客厅的照片里，看看它实际放在那里会是什么样子。<br />
<br />
同时保证在不同环境中看起来自然和真实！<br />
<br />
简而言之，它帮助用户更好地了解产品在真实环境中的样子，提高了在线购物的体验。<br />
<br />
1、虚拟试穿技术：允许用户在不同环境中虚拟放置商品，实现逼真的在线购物体验。<br />
<br />
2、与传统扩散模型相比，DTC模型能更好地捕捉商品细节，提升修复质量。采用特殊的算法，将来自参考图像的细粒度特征直接融入主扩散模型的潜在特征图中，保证产品与环境的高度融合。<br />
<br />
3、高效平衡：在快速推断与保持高保真细节方面达到了有效的平衡。<br />
<br />
4、广泛测试与评估：在不同数据集上测试，证明了DTC模型相较于现有技术的优越性。<br />
<br />
5、场景适应性：能够处理多种场景中的图像，确保产品与场景的无缝整合。<br />
<br />
6、快速推断能力：提供快速且高效的零次射推断，加快虚拟试穿过程。<br />
<br />
项目及演示：<a href="https://diffuse2choose.github.io">diffuse2choose.github.io</a><br />
论文：<a href="https://arxiv.org/abs/2401.13795">arxiv.org/abs/2401.13795</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwODI5NTQyODkyMjU3MjgwL2ltZy9nR3JzeDJiZ1JRZkZqOG1XLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750796206874443880#m</id>
            <title>R to @xiaohuggg: Chrome 121版本已经新增在Android上支持WebGPU特性！

WebGPU，现在默认在运行Android 12及更高版本的设备上启用，特别是那些使用高通和ARM GPU的设备。支持范围将逐步扩大到更多Android设备，包括运行Android 11的设备。

https://developer.chrome.com/blog/new-in-webgpu-121</title>
            <link>https://nitter.cz/xiaohuggg/status/1750796206874443880#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750796206874443880#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 08:21:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chrome 121版本已经新增在Android上支持WebGPU特性！<br />
<br />
WebGPU，现在默认在运行Android 12及更高版本的设备上启用，特别是那些使用高通和ARM GPU的设备。支持范围将逐步扩大到更多Android设备，包括运行Android 11的设备。<br />
<br />
<a href="https://developer.chrome.com/blog/new-in-webgpu-121">developer.chrome.com/blog/ne…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MDQxOTUyOTcwNjk4MzQyNC8zZ0k1V29OTj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750796204668342624#m</id>
            <title>Web LLM：在浏览器中运行大语言模型

该项目利用WebGPU加速，无需服务器支持，所有操作都在浏览器内运行。

这为构建面向每个人的AI助手开启了新的可能性。

这意味着以后大语言模型可以在任意设备上运行！

主要特点：

1.支持多种模型：支持多种模型，包括Llama 2 7B/13B、Llama 2 70B、Mistral 7B以及WizardMath等。

2.运行环境和要求：项目在Chrome 113中提供WebGPU支持。用户可以在支持的浏览器中直接尝试不同的模型。首次运行时需要下载模型参数，之后的运行将更快。

3.聊天演示：提供了基于Llama 2、Mistral-7B及其变体和RedPajama-INCITE-Chat-3B-v1模型的聊天演示。未来还将支持更多模型。

4.开源和开发支持：项目鼓励开发者使用WebLLM作为基础npm包，并在其上构建自己的Web应用程序。相关文档和GitHub资源可供参考。

项目目标与愿景：

该项目旨在为生态系统带来更多多样性，尤其是将LLMs直接嵌入到客户端并在浏览器内运行。这样做可以降低成本、增强个性化和保护隐私。

详细：https://webllm.mlc.ai

GitHub：https://github.com/mlc-ai/web-llm

演示视频 @charlie_ruan</title>
            <link>https://nitter.cz/xiaohuggg/status/1750796204668342624#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750796204668342624#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 08:21:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Web LLM：在浏览器中运行大语言模型<br />
<br />
该项目利用WebGPU加速，无需服务器支持，所有操作都在浏览器内运行。<br />
<br />
这为构建面向每个人的AI助手开启了新的可能性。<br />
<br />
这意味着以后大语言模型可以在任意设备上运行！<br />
<br />
主要特点：<br />
<br />
1.支持多种模型：支持多种模型，包括Llama 2 7B/13B、Llama 2 70B、Mistral 7B以及WizardMath等。<br />
<br />
2.运行环境和要求：项目在Chrome 113中提供WebGPU支持。用户可以在支持的浏览器中直接尝试不同的模型。首次运行时需要下载模型参数，之后的运行将更快。<br />
<br />
3.聊天演示：提供了基于Llama 2、Mistral-7B及其变体和RedPajama-INCITE-Chat-3B-v1模型的聊天演示。未来还将支持更多模型。<br />
<br />
4.开源和开发支持：项目鼓励开发者使用WebLLM作为基础npm包，并在其上构建自己的Web应用程序。相关文档和GitHub资源可供参考。<br />
<br />
项目目标与愿景：<br />
<br />
该项目旨在为生态系统带来更多多样性，尤其是将LLMs直接嵌入到客户端并在浏览器内运行。这样做可以降低成本、增强个性化和保护隐私。<br />
<br />
详细：<a href="https://webllm.mlc.ai">webllm.mlc.ai</a><br />
<br />
GitHub：<a href="https://github.com/mlc-ai/web-llm">github.com/mlc-ai/web-llm</a><br />
<br />
演示视频 <a href="https://nitter.cz/charlie_ruan" title="Charlie Ruan">@charlie_ruan</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwNzk2MTM5ODYyMTM4ODgwL2ltZy9uU0VGRW1HM1dJSDN1TXRNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750788482484846748#m</id>
            <title>曼谷有X友吗

🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1750788482484846748#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750788482484846748#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 07:51:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>曼谷有X友吗<br />
<br />
🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmRmFRQUFNUzViLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmRmJVQUFZSDJlLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmSWFZQUE2Ti0tLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750706833751408858#m</id>
            <title>SUPIR：通过增加模型的规模（即增加模型的参数数量）提升图像修复的能力。

通过参数增加使得模型不仅能够修复图像中的错误或损坏，还能根据文本提示进行智能修复。

例如根据描述来改变图像中的特定细节。这样的处理方式提升了图像修复的质量和智能度，使得模型能够更准确、更灵活地恢复和改进图像。

SUPIR的主要功能：

图像修复： SUPIR的核心功能是对低质量或损坏的图像进行修复，提高其视觉质量。这包括处理如模糊、噪点、色彩失真等问题，使图像恢复到高清晰度和高质量状态。

文本引导的修复： SUPIR能够根据文本提示来指导图像修复。这意味着用户可以通过文本描述来指定希望修复或改变的图像部分，使得修复过程更加定制化和精确。

核心技术创新：

1、模型放大： SUPIR通过扩大模型规模（即增加模型的参数数量）来提升图像修复的能力。这种放大使得模型能够学习更多的特征，处理更复杂的图像修复任务。

2、多模态技术： 结合了图像处理和文本处理的技术，允许模型不仅理解图像内容，还能理解与之相关的文本描述，从而进行更准确的修复。

3、高质量训练数据集：收集了2000万高质量图像和文本注释，用于训练和控制图像修复。利用大量高分辨率、高质量的图像和相关文本注释作为训练数据，提高了模型的性能和适用性。

4、负质量提示： 通过引入质量较差的图像样本和相应的负面描述作为训练数据，进一步提升模型在感知质量方面的表现。

工作原理：

1、图像编码与解码： SUPIR利用一个编码器将低质量图像映射到潜在空间，然后使用解码器重建修复后的图像。

2、文本处理： 通过一个多模态语言模型，SUPIR能够理解与图像相关的文本描述，并将这些信息融入到图像修复过程中。

3、适配器设计： SUPIR设计了一个大规模适配器，用于将模型的生成能力调整到与输入图像相匹配的状态，确保修复过程符合用户的具体需求。

4、采样方法： 采用特殊的采样方法，用于指导图像的恢复过程，以防止过度生成，确保修复后的图像保持真实和高质量。

实验结果：

在多种IR任务上展示了出色的修复效果，特别是在复杂和具挑战性的真实世界场景中

1、多样化的图像修复任务： SUPIR被应用于各种类型的图像修复任务，包括但不限于去噪、去模糊、超分辨率、色彩校正等。这显示了其广泛的适用性和灵活性。

2、真实世界的复杂场景处理： 实验中的一个重要亮点是SUPIR在处理真实世界复杂场景中的高效表现。这些场景通常包含多种类型的图像退化，如不均匀光照、运动模糊和天气影响等，这些都是传统图像修复方法难以处理的。

3、高级特性的应用： SUPIR展示了如何根据复杂的文本描述进行定制化修复。例如，它可以根据用户提供的描述，调整图像中特定对象的纹理或颜色，或者改变场景的某些元素。

4、质量评估： 在实验中，SUPIR修复的图像在质量上得到了显著提升。这通过与现有技术的对比评估，以及视觉质量和客观指标（如图像清晰度、纹理细节等）的测量来证实。

5、挑战性任务的处理： 特别值得注意的是，SUPIR在处理一些传统方法难以解决的挑战性任务时表现突出，如极度模糊或严重损坏的图像修复。

6、用户定制和互动性： 实验还展示了SUPIR在用户交互方面的能力，用户可以通过简单的文本指令控制图像的修复过程，这为图像修复提供了新的互动维度。

项目及演示：https://supir.xpixel.group/
论文：https://arxiv.org/abs/2401.13627</title>
            <link>https://nitter.cz/xiaohuggg/status/1750706833751408858#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750706833751408858#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:26:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SUPIR：通过增加模型的规模（即增加模型的参数数量）提升图像修复的能力。<br />
<br />
通过参数增加使得模型不仅能够修复图像中的错误或损坏，还能根据文本提示进行智能修复。<br />
<br />
例如根据描述来改变图像中的特定细节。这样的处理方式提升了图像修复的质量和智能度，使得模型能够更准确、更灵活地恢复和改进图像。<br />
<br />
SUPIR的主要功能：<br />
<br />
图像修复： SUPIR的核心功能是对低质量或损坏的图像进行修复，提高其视觉质量。这包括处理如模糊、噪点、色彩失真等问题，使图像恢复到高清晰度和高质量状态。<br />
<br />
文本引导的修复： SUPIR能够根据文本提示来指导图像修复。这意味着用户可以通过文本描述来指定希望修复或改变的图像部分，使得修复过程更加定制化和精确。<br />
<br />
核心技术创新：<br />
<br />
1、模型放大： SUPIR通过扩大模型规模（即增加模型的参数数量）来提升图像修复的能力。这种放大使得模型能够学习更多的特征，处理更复杂的图像修复任务。<br />
<br />
2、多模态技术： 结合了图像处理和文本处理的技术，允许模型不仅理解图像内容，还能理解与之相关的文本描述，从而进行更准确的修复。<br />
<br />
3、高质量训练数据集：收集了2000万高质量图像和文本注释，用于训练和控制图像修复。利用大量高分辨率、高质量的图像和相关文本注释作为训练数据，提高了模型的性能和适用性。<br />
<br />
4、负质量提示： 通过引入质量较差的图像样本和相应的负面描述作为训练数据，进一步提升模型在感知质量方面的表现。<br />
<br />
工作原理：<br />
<br />
1、图像编码与解码： SUPIR利用一个编码器将低质量图像映射到潜在空间，然后使用解码器重建修复后的图像。<br />
<br />
2、文本处理： 通过一个多模态语言模型，SUPIR能够理解与图像相关的文本描述，并将这些信息融入到图像修复过程中。<br />
<br />
3、适配器设计： SUPIR设计了一个大规模适配器，用于将模型的生成能力调整到与输入图像相匹配的状态，确保修复过程符合用户的具体需求。<br />
<br />
4、采样方法： 采用特殊的采样方法，用于指导图像的恢复过程，以防止过度生成，确保修复后的图像保持真实和高质量。<br />
<br />
实验结果：<br />
<br />
在多种IR任务上展示了出色的修复效果，特别是在复杂和具挑战性的真实世界场景中<br />
<br />
1、多样化的图像修复任务： SUPIR被应用于各种类型的图像修复任务，包括但不限于去噪、去模糊、超分辨率、色彩校正等。这显示了其广泛的适用性和灵活性。<br />
<br />
2、真实世界的复杂场景处理： 实验中的一个重要亮点是SUPIR在处理真实世界复杂场景中的高效表现。这些场景通常包含多种类型的图像退化，如不均匀光照、运动模糊和天气影响等，这些都是传统图像修复方法难以处理的。<br />
<br />
3、高级特性的应用： SUPIR展示了如何根据复杂的文本描述进行定制化修复。例如，它可以根据用户提供的描述，调整图像中特定对象的纹理或颜色，或者改变场景的某些元素。<br />
<br />
4、质量评估： 在实验中，SUPIR修复的图像在质量上得到了显著提升。这通过与现有技术的对比评估，以及视觉质量和客观指标（如图像清晰度、纹理细节等）的测量来证实。<br />
<br />
5、挑战性任务的处理： 特别值得注意的是，SUPIR在处理一些传统方法难以解决的挑战性任务时表现突出，如极度模糊或严重损坏的图像修复。<br />
<br />
6、用户定制和互动性： 实验还展示了SUPIR在用户交互方面的能力，用户可以通过简单的文本指令控制图像的修复过程，这为图像修复提供了新的互动维度。<br />
<br />
项目及演示：<a href="https://supir.xpixel.group/">supir.xpixel.group/</a><br />
论文：<a href="https://arxiv.org/abs/2401.13627">arxiv.org/abs/2401.13627</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA3MDYxMjY4MDk4OTkwMDgvcHUvaW1nL19FS1RQS2k3TjkwZFpNLV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750704152605499508#m</id>
            <title>Adept Fuyu-Heavy：Adept Fuyu-Heavy是专为数字代理设计的新型多模态模型。

宣称是世界上第三大能力超强的多模态模型，仅次于GPT4-V和Gemini Ultra。

它特别擅长理解用户界面，这意味着可以解释和操作各种软件和应用程序的界面。

能够帮助用户执行各种任务，如自动化流程、响应查询、提供信息等。

Adept Fuyu-Heavy在多项评估和基准测试中展示了卓越的性能。

1、多模态基准测试：在MMM（Multimodal Multitask）基准测试中，Fuyu-Heavy的表现优于Gemini Pro，突显了其在多模态任务上的能力。

2、文本基准测试：尽管Fuyu-Heavy需分配部分容量处理图像数据，但在标准的文本只评估中，它的表现与Gemini Pro大体相当，甚至在MMLU（多模态语言理解）基准测试中超过了Gemini Pro。

3、长形式对话性能：经过有监督的微调和直接优化阶段后，Fuyu-Heavy在最常用的聊天评估——MT-Bench和AlpacaEval 1.0——中的表现与Claude 2.0相当，尽管它是一个更小的模型，且部分容量用于图像建模。

4、多模态性能标准：在MMM（Multimodal Multitask）基准测试上，Fuyu-Heavy略微优于Gemini Pro。此外，还包括了在VQAv2（一个视觉问答基准）和AI2D（一个图表理解数据集）上的结果。

Adept Fuyu-Heavy的主要能力包括：

1、多模态理解和生成： Fuyu-Heavy能够处理和理解多种类型的数据，如文本和图像，并能够基于这些数据生成相应的输出。这使其在多模态任务上表现出色。

2、高效的图像和文本处理： 尽管需要部分容量用于图像建模，Fuyu-Heavy在标准文本基准测试中的表现匹敌或超越同级别的模型。

3、优化的模型架构： Fuyu-Heavy通过扩展和优化Fuyu架构，有效处理任意大小和形状的图像，并有效利用现有的变压器模型优化。

4、长形式对话性能： 经过特定训练阶段优化，Fuyu-Heavy在长形式对话和交互中表现出色。

5、用户界面理解： 特别擅长于理解数字用户界面（UI），如网站和应用程序，提供有效的自动化解决方案。意味着Fuyu-Heavy的开发重点是使其能够适应和优化数字代理的功能，如提高用户界面理解、增强自动化决策能力、提供更准确的信息检索和内容生成等。

6、跨模态内容生成： 能够生成跨越文本和图像的内容，适用于多种应用场景。

详细：https://www.adept.ai/blog/adept-fuyu-heavy</title>
            <link>https://nitter.cz/xiaohuggg/status/1750704152605499508#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750704152605499508#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:16:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Adept Fuyu-Heavy：Adept Fuyu-Heavy是专为数字代理设计的新型多模态模型。<br />
<br />
宣称是世界上第三大能力超强的多模态模型，仅次于GPT4-V和Gemini Ultra。<br />
<br />
它特别擅长理解用户界面，这意味着可以解释和操作各种软件和应用程序的界面。<br />
<br />
能够帮助用户执行各种任务，如自动化流程、响应查询、提供信息等。<br />
<br />
Adept Fuyu-Heavy在多项评估和基准测试中展示了卓越的性能。<br />
<br />
1、多模态基准测试：在MMM（Multimodal Multitask）基准测试中，Fuyu-Heavy的表现优于Gemini Pro，突显了其在多模态任务上的能力。<br />
<br />
2、文本基准测试：尽管Fuyu-Heavy需分配部分容量处理图像数据，但在标准的文本只评估中，它的表现与Gemini Pro大体相当，甚至在MMLU（多模态语言理解）基准测试中超过了Gemini Pro。<br />
<br />
3、长形式对话性能：经过有监督的微调和直接优化阶段后，Fuyu-Heavy在最常用的聊天评估——MT-Bench和AlpacaEval 1.0——中的表现与Claude 2.0相当，尽管它是一个更小的模型，且部分容量用于图像建模。<br />
<br />
4、多模态性能标准：在MMM（Multimodal Multitask）基准测试上，Fuyu-Heavy略微优于Gemini Pro。此外，还包括了在VQAv2（一个视觉问答基准）和AI2D（一个图表理解数据集）上的结果。<br />
<br />
Adept Fuyu-Heavy的主要能力包括：<br />
<br />
1、多模态理解和生成： Fuyu-Heavy能够处理和理解多种类型的数据，如文本和图像，并能够基于这些数据生成相应的输出。这使其在多模态任务上表现出色。<br />
<br />
2、高效的图像和文本处理： 尽管需要部分容量用于图像建模，Fuyu-Heavy在标准文本基准测试中的表现匹敌或超越同级别的模型。<br />
<br />
3、优化的模型架构： Fuyu-Heavy通过扩展和优化Fuyu架构，有效处理任意大小和形状的图像，并有效利用现有的变压器模型优化。<br />
<br />
4、长形式对话性能： 经过特定训练阶段优化，Fuyu-Heavy在长形式对话和交互中表现出色。<br />
<br />
5、用户界面理解： 特别擅长于理解数字用户界面（UI），如网站和应用程序，提供有效的自动化解决方案。意味着Fuyu-Heavy的开发重点是使其能够适应和优化数字代理的功能，如提高用户界面理解、增强自动化决策能力、提供更准确的信息检索和内容生成等。<br />
<br />
6、跨模态内容生成： 能够生成跨越文本和图像的内容，适用于多种应用场景。<br />
<br />
详细：<a href="https://www.adept.ai/blog/adept-fuyu-heavy">adept.ai/blog/adept-fuyu-hea…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA1NDAzMTY3ODMwMjIwODAvcHUvaW1nL1pacTRtSmdVU2t3MUNwekkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>