<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727192504368587211#m</id>
            <title>MagicDance：能生成真实的人类舞蹈视频，同时实现动作和面部表情的转移。

TikTok搞的项目，它可以把一个人的舞蹈动作和面部表情转移到另一个人身上，同时保持这个人的身份特征不变。

比如，你可以让一个视频里的舞者看起来像是另一个人在跳舞，但舞蹈动作和表情都保持原样。

他们使用了一种特殊的训练方法，可以分开处理人的动作和外观，比如面部表情和穿着。这意味着它可以很好地控制视频中人物的上半身和面部特征，甚至是背景。

MagicDance还可以用于制作卡通风格的动画，只需要提供舞蹈的姿势，就可以生成动画。

该项目的主要特点和功能包括：

1、真实的人类视频生成：MagicDance能够生成真实的人类舞蹈视频，同时保持目标身份不变，这是通过将问题分解为外观控制和运动控制两个任务来实现的。

2、外观和姿势控制的解耦：该模型通过两阶段训练策略来解耦人类运动和外观（例如面部表情、肤色和服装），包括外观控制块的预训练和外观-姿势-联合控制块的微调。

3、稳定扩散的插件/扩展：MagicDance被设计为Stable Diffusion的一个方便的插件/扩展，不需要对Stable Diffusion UNet参数进行微调，确保了现有模型权重的兼容性。

4、强大的泛化能力：该模型在未见身份和复杂运动序列上表现出良好的泛化能力，无需任何额外数据的微调。

5、广泛的应用范围：MagicDance可以用于图像/视频风格化、编辑、数字人合成，甚至可能用于训练感知模型的数据生成。

6、零镜头2D动画生成：该模型不仅能够从一个身份转移到另一个身份，还能够在仅有姿势输入的情况下实现卡通风格的造型。

7、在TikTok数据集上的卓越性能：在TikTok数据集上进行的广泛实验表明了该模型在视频生成方面的优越性能。

项目及演示：https://boese0601.github.io/magicdance/
论文：https://arxiv.org/abs/2311.12052
GitHub：https://github.com/Boese0601/MagicDance</title>
            <link>https://nitter.cz/xiaohuggg/status/1727192504368587211#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727192504368587211#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 05:09:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MagicDance：能生成真实的人类舞蹈视频，同时实现动作和面部表情的转移。<br />
<br />
TikTok搞的项目，它可以把一个人的舞蹈动作和面部表情转移到另一个人身上，同时保持这个人的身份特征不变。<br />
<br />
比如，你可以让一个视频里的舞者看起来像是另一个人在跳舞，但舞蹈动作和表情都保持原样。<br />
<br />
他们使用了一种特殊的训练方法，可以分开处理人的动作和外观，比如面部表情和穿着。这意味着它可以很好地控制视频中人物的上半身和面部特征，甚至是背景。<br />
<br />
MagicDance还可以用于制作卡通风格的动画，只需要提供舞蹈的姿势，就可以生成动画。<br />
<br />
该项目的主要特点和功能包括：<br />
<br />
1、真实的人类视频生成：MagicDance能够生成真实的人类舞蹈视频，同时保持目标身份不变，这是通过将问题分解为外观控制和运动控制两个任务来实现的。<br />
<br />
2、外观和姿势控制的解耦：该模型通过两阶段训练策略来解耦人类运动和外观（例如面部表情、肤色和服装），包括外观控制块的预训练和外观-姿势-联合控制块的微调。<br />
<br />
3、稳定扩散的插件/扩展：MagicDance被设计为Stable Diffusion的一个方便的插件/扩展，不需要对Stable Diffusion UNet参数进行微调，确保了现有模型权重的兼容性。<br />
<br />
4、强大的泛化能力：该模型在未见身份和复杂运动序列上表现出良好的泛化能力，无需任何额外数据的微调。<br />
<br />
5、广泛的应用范围：MagicDance可以用于图像/视频风格化、编辑、数字人合成，甚至可能用于训练感知模型的数据生成。<br />
<br />
6、零镜头2D动画生成：该模型不仅能够从一个身份转移到另一个身份，还能够在仅有姿势输入的情况下实现卡通风格的造型。<br />
<br />
7、在TikTok数据集上的卓越性能：在TikTok数据集上进行的广泛实验表明了该模型在视频生成方面的优越性能。<br />
<br />
项目及演示：<a href="https://boese0601.github.io/magicdance/">boese0601.github.io/magicdan…</a><br />
论文：<a href="https://arxiv.org/abs/2311.12052">arxiv.org/abs/2311.12052</a><br />
GitHub：<a href="https://github.com/Boese0601/MagicDance">github.com/Boese0601/MagicDa…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjcxOTIyMTYyMjgyOTg3NTIvcHUvaW1nLzVTNFZKcEFFdXJjRERnYkguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727183444441174375#m</id>
            <title>Lookahead decoding：一种创新的并行解码算法，加速大LLM的推理过程。

Lookahead decoding就像是给大语言模型装上了涡轮增压器，可以让模型同时处理多个令牌，在生成文字时速度更快，模型生成文本的速度可以提高1.5到2.3倍。

这对于需要快速回应的应用，比如在线聊天机器人或者语音助手，特别有用。

传统的自回归解码步骤生成一个令牌（token）是非常缓慢且难以优化的，这对于需要快速响应的实际应用（如聊天机器人和个人助理）构成了挑战。

就像人类写作时一个字一个字写。但是，Lookahead decoding 技术可以让模型同时处理多个部分，这就像是能同时写下几个字，而不是一个接一个。

这种方法的核心在于打破传统自回归解码中的顺序依赖性，通过同时提取和验证n-grams（n元语法模型）来实现更快的解码速度。

其主要特点包括：

1、并行解码：Lookahead decoding通过并行处理n-grams来加速解码过程，与传统的逐步生成单个令牌的方法相比，大幅提高了效率。它可以同时处理多个字，而不是像以前那样一个接一个。

2、雅可比迭代法：该算法采用雅可比迭代法来处理解码过程中的非线性方程组，这种方法有助于提高并行处理的效率。

3、无需草稿模型或额外数据存储：与某些其他加速技术不同，Lookahead decoding不依赖于草稿模型或额外的数据存储，简化了实现过程。不需要额外的复杂设置或存储空间。

4、线性减少解码步骤：该方法能够根据每个解码步骤使用的浮点运算（FLOPs）线性减少解码步骤数，从而提高效率。能够更快地完成整个文字生成的过程。

4、与HuggingFace兼容：Lookahead decoding的实现与HuggingFace的transformers库兼容，使得用户可以轻松地在现有的模型中应用这种新技术。

Lookahead decoding为需要快速响应的应用（如聊天机器人和个人助理）提供了一种有效的解决方案，特别是在生成长序列时，能够显著减少延迟。

详细介绍：https://lmsys.org/blog/2023-11-21-lookahead-decoding/
GitHub：https://github.com/hao-ai-lab/LookaheadDecoding

视频演示为：LLaMA-2-Chat 7B Lookahead decoding解码加速演示。蓝色字体是在解码步骤中并行生成的标记。</title>
            <link>https://nitter.cz/xiaohuggg/status/1727183444441174375#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727183444441174375#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 04:33:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Lookahead decoding：一种创新的并行解码算法，加速大LLM的推理过程。<br />
<br />
Lookahead decoding就像是给大语言模型装上了涡轮增压器，可以让模型同时处理多个令牌，在生成文字时速度更快，模型生成文本的速度可以提高1.5到2.3倍。<br />
<br />
这对于需要快速回应的应用，比如在线聊天机器人或者语音助手，特别有用。<br />
<br />
传统的自回归解码步骤生成一个令牌（token）是非常缓慢且难以优化的，这对于需要快速响应的实际应用（如聊天机器人和个人助理）构成了挑战。<br />
<br />
就像人类写作时一个字一个字写。但是，Lookahead decoding 技术可以让模型同时处理多个部分，这就像是能同时写下几个字，而不是一个接一个。<br />
<br />
这种方法的核心在于打破传统自回归解码中的顺序依赖性，通过同时提取和验证n-grams（n元语法模型）来实现更快的解码速度。<br />
<br />
其主要特点包括：<br />
<br />
1、并行解码：Lookahead decoding通过并行处理n-grams来加速解码过程，与传统的逐步生成单个令牌的方法相比，大幅提高了效率。它可以同时处理多个字，而不是像以前那样一个接一个。<br />
<br />
2、雅可比迭代法：该算法采用雅可比迭代法来处理解码过程中的非线性方程组，这种方法有助于提高并行处理的效率。<br />
<br />
3、无需草稿模型或额外数据存储：与某些其他加速技术不同，Lookahead decoding不依赖于草稿模型或额外的数据存储，简化了实现过程。不需要额外的复杂设置或存储空间。<br />
<br />
4、线性减少解码步骤：该方法能够根据每个解码步骤使用的浮点运算（FLOPs）线性减少解码步骤数，从而提高效率。能够更快地完成整个文字生成的过程。<br />
<br />
4、与HuggingFace兼容：Lookahead decoding的实现与HuggingFace的transformers库兼容，使得用户可以轻松地在现有的模型中应用这种新技术。<br />
<br />
Lookahead decoding为需要快速响应的应用（如聊天机器人和个人助理）提供了一种有效的解决方案，特别是在生成长序列时，能够显著减少延迟。<br />
<br />
详细介绍：<a href="https://lmsys.org/blog/2023-11-21-lookahead-decoding/">lmsys.org/blog/2023-11-21-lo…</a><br />
GitHub：<a href="https://github.com/hao-ai-lab/LookaheadDecoding">github.com/hao-ai-lab/Lookah…</a><br />
<br />
视频演示为：LLaMA-2-Chat 7B Lookahead decoding解码加速演示。蓝色字体是在解码步骤中并行生成的标记。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjcxODE3ODAwMDI5ODgwMzIvcHUvaW1nL2kxV3hkX3c1UThKbG9nVHcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727149007074709589#m</id>
            <title>马斯克：

Grok会在下周对所有 X Premium+用户开放。

已经开始邀请部分用户体验全新的 Grok 聊天体验，如果受邀用户未购买 Premium + 订阅，会推荐其购买，价格为每月 16 美元。😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1727149007074709589#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727149007074709589#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 02:16:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克：<br />
<br />
Grok会在下周对所有 X Premium+用户开放。<br />
<br />
已经开始邀请部分用户体验全新的 Grok 聊天体验，如果受邀用户未购买 Premium + 订阅，会推荐其购买，价格为每月 16 美元。😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9nTlBxZ2JVQUEtNHZsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727148300292530186#m</id>
            <title>Runway Gen-2 Motion Brush运动笔刷教程（中文字幕）

🎥 精确控制运动：Motion Brush工具让你能够在视频中精确地控制特定区域的运动。

🔄 运动方向和速度设置：可以为选定区域指定运动方向和速度。

⚡ 独立的速度控制：可以独立控制水平、垂直和接近度方向的速度。

📹 与相机控制独立：Motion Brush的运动控制与相机运动独立，用户可以同时使用这两种功能进行实验。

使用方法：

1、打开Gen-2：在RunwayML的Gen-2中添加一个图像提示。

2、使用文本到视频：如果使用文本到视频功能，确保生成预览，然后使用其中一个预览图像作为图像输入。

3、选择Motion Brush：在工具中选择Motion Brush选项。

4、绘制控制区域：在视频中想要控制的区域上绘制，以指定运动方向和速度。

详细：https://academy.runwayml.com/gen2/gen2-motion-brush</title>
            <link>https://nitter.cz/xiaohuggg/status/1727148300292530186#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727148300292530186#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 02:13:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway Gen-2 Motion Brush运动笔刷教程（中文字幕）<br />
<br />
🎥 精确控制运动：Motion Brush工具让你能够在视频中精确地控制特定区域的运动。<br />
<br />
🔄 运动方向和速度设置：可以为选定区域指定运动方向和速度。<br />
<br />
⚡ 独立的速度控制：可以独立控制水平、垂直和接近度方向的速度。<br />
<br />
📹 与相机控制独立：Motion Brush的运动控制与相机运动独立，用户可以同时使用这两种功能进行实验。<br />
<br />
使用方法：<br />
<br />
1、打开Gen-2：在RunwayML的Gen-2中添加一个图像提示。<br />
<br />
2、使用文本到视频：如果使用文本到视频功能，确保生成预览，然后使用其中一个预览图像作为图像输入。<br />
<br />
3、选择Motion Brush：在工具中选择Motion Brush选项。<br />
<br />
4、绘制控制区域：在视频中想要控制的区域上绘制，以指定运动方向和速度。<br />
<br />
详细：<a href="https://academy.runwayml.com/gen2/gen2-motion-brush">academy.runwayml.com/gen2/ge…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjcxNDY2MzU3NjE5OTE2ODAvcHUvaW1nL3k0a0NOcDNhcVptRjJpcDIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727139721363788078#m</id>
            <title>R to @xiaohuggg: 一个本地运行模型的工具，我正在测试...

晚一点录视频😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1727139721363788078#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727139721363788078#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 01:39:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个本地运行模型的工具，我正在测试...<br />
<br />
晚一点录视频😂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727137100984889729#m</id>
            <title>正在安装 GPT4 ALL ...</title>
            <link>https://nitter.cz/xiaohuggg/status/1727137100984889729#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727137100984889729#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 01:29:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>正在安装 GPT4 ALL ...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9nRjlXOWJNQUFoNGVzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727131071719071950#m</id>
            <title>OpenAI 新任CEO Emmett Shear 称：

如果OpenAI董事会无法提供解雇Altman 的证据，那么他将辞职。

另据知情人士透露，奥特曼正与 OpenAI董事会成员和公司临时 CEO 正在进行谈判，谈判成员涉及CEO Emmett Shear、董事会成员 Adam D'Angelo 以及一些投资者，推动 Altman复职。

https://www.bloomberg.com/news/articles/2023-11-21/altman-openai-board-open-talks-to-negotiate-his-possible-return</title>
            <link>https://nitter.cz/xiaohuggg/status/1727131071719071950#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727131071719071950#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 01:05:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 新任CEO Emmett Shear 称：<br />
<br />
如果OpenAI董事会无法提供解雇Altman 的证据，那么他将辞职。<br />
<br />
另据知情人士透露，奥特曼正与 OpenAI董事会成员和公司临时 CEO 正在进行谈判，谈判成员涉及CEO Emmett Shear、董事会成员 Adam D'Angelo 以及一些投资者，推动 Altman复职。<br />
<br />
<a href="https://www.bloomberg.com/news/articles/2023-11-21/altman-openai-board-open-talks-to-negotiate-his-possible-return">bloomberg.com/news/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNzAyMDUzMjI3MTYwMzcxMi9IRTZrSlNYNj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727128906120257663#m</id>
            <title>币安赵长鹏与美国政府达成和解，承认洗钱指控。

赵长鹏缴纳5000万美元罚金，辞去币安CEO职位，未来不得再参与公司事务。

币安接受美国政府指派的监督员。 ​​​

币安公司也将承认一项刑事指控，并同意支付总计43亿美元罚款，其中包括用于监管机构提出的罚款。</title>
            <link>https://nitter.cz/xiaohuggg/status/1727128906120257663#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727128906120257663#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:56:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>币安赵长鹏与美国政府达成和解，承认洗钱指控。<br />
<br />
赵长鹏缴纳5000万美元罚金，辞去币安CEO职位，未来不得再参与公司事务。<br />
<br />
币安接受美国政府指派的监督员。 ​​​<br />
<br />
币安公司也将承认一项刑事指控，并同意支付总计43亿美元罚款，其中包括用于监管机构提出的罚款。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9mLVdEeGJzQUFyRUo3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727124741423771768#m</id>
            <title>R to @xiaohuggg: 3D效果好像还不错👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1727124741423771768#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727124741423771768#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:40:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3D效果好像还不错👍</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3MTI0NzI5NTk1ODk5OTA1L2ltZy94a0hKMzNmQjN1dW1PWGNELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727123892324774089#m</id>
            <title>💡 Stability AI 发布其最新的Stable Video Diffustion 视频开源模型！支持：

- 文本到视频 
- 图像到视频 
- 14 或 25 帧，576 x 1024分辨率
- 多视图生成 
- 帧插值 
- 支持3D 场景
- 通过 LoRA 控制摄像机

Stability AI称正在开发一个新的网络平台，包括一个文本到视频的界面。这个工具将展示Stable Video Diffusion在广告、教育、娱乐等多个领域的实际应用。

详细介绍：https://stability.ai/news/stable-video-diffusion-open-ai-video-model

GitHub：https://github.com/Stability-AI/generative-models
论文：https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets
HuggingFace：https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt</title>
            <link>https://nitter.cz/xiaohuggg/status/1727123892324774089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727123892324774089#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:36:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>💡 Stability AI 发布其最新的Stable Video Diffustion 视频开源模型！支持：<br />
<br />
- 文本到视频 <br />
- 图像到视频 <br />
- 14 或 25 帧，576 x 1024分辨率<br />
- 多视图生成 <br />
- 帧插值 <br />
- 支持3D 场景<br />
- 通过 LoRA 控制摄像机<br />
<br />
Stability AI称正在开发一个新的网络平台，包括一个文本到视频的界面。这个工具将展示Stable Video Diffusion在广告、教育、娱乐等多个领域的实际应用。<br />
<br />
详细介绍：<a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model">stability.ai/news/stable-vid…</a><br />
<br />
GitHub：<a href="https://github.com/Stability-AI/generative-models">github.com/Stability-AI/gene…</a><br />
论文：<a href="https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets">stability.ai/research/stable…</a><br />
HuggingFace：<a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt">huggingface.co/stabilityai/s…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3MTIzNzE5Mzk3NzQ4NzM2L2ltZy9YT1lVQ0lzWldnRExIeWZOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727119128581100001#m</id>
            <title>Claude 发布2.1 版本，进行重大升级更新，发布一系列新功能：

1. **增强的处理能力**
   - 上下文处理量大幅提升：Claude 2.1 现在能处理高达 200K上下文标记，约等于 150,000 个单词或 500 页文本。

2. **准确性提升**
   - 虚假/幻觉陈述减少：相比之前版本，虚假或幻觉类陈述减少了 2 倍。

3. **新功能测试**
   - 早期支持企业级功能：允许将 Claude 连接到公司的 API、数据库和 Web 服务，目前处于测试阶段。

4. **可用性更新**
   - Claude 2.1 在线上线：升级版本已在 http://claude.ai 的 Thropic 托管聊天机器人界面和付费的 Claude Pro API 层推出。

5. **价格与访问**
   - 高级上下文限制：目前，200,000 个代币的上下文限制仅适用于 Pro 用户，价格与 ChatGPT Plus 订阅（目前暂停）相似，为每月 20 美元。

6. **系统提示功能**
   - 自定义指导与上下文：Anthropic 引入了系统提示功能，允许用户为 Claude 提供自定义说明和上下文，以提升其在特定任务上的性能。

   - 功能详细说明：
     - 设定目标与角色：允许用户设定目标、指定 Claude 的角色或语气。
     - 建立规则和约束：用户可以建立规则和约束。
     - 提供背景知识：可提供相关背景知识。
     - 定义验证标准：定义输出验证的标准。
   - 响应优化：通过这种方式提示 Claude，可以塑造更准确、更一致的响应，保持角色扮演的特征，并更可靠地遵循提供的指南。
   - 目标：系统提示的最终目的是增强 Claude 在预期实际应用中的能力。

详细内容：https://www.anthropic.com/index/claude-2-1</title>
            <link>https://nitter.cz/xiaohuggg/status/1727119128581100001#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727119128581100001#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 00:17:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Claude 发布2.1 版本，进行重大升级更新，发布一系列新功能：<br />
<br />
1. **增强的处理能力**<br />
   - 上下文处理量大幅提升：Claude 2.1 现在能处理高达 200K上下文标记，约等于 150,000 个单词或 500 页文本。<br />
<br />
2. **准确性提升**<br />
   - 虚假/幻觉陈述减少：相比之前版本，虚假或幻觉类陈述减少了 2 倍。<br />
<br />
3. **新功能测试**<br />
   - 早期支持企业级功能：允许将 Claude 连接到公司的 API、数据库和 Web 服务，目前处于测试阶段。<br />
<br />
4. **可用性更新**<br />
   - Claude 2.1 在线上线：升级版本已在 <a href="http://claude.ai">claude.ai</a> 的 Thropic 托管聊天机器人界面和付费的 Claude Pro API 层推出。<br />
<br />
5. **价格与访问**<br />
   - 高级上下文限制：目前，200,000 个代币的上下文限制仅适用于 Pro 用户，价格与 ChatGPT Plus 订阅（目前暂停）相似，为每月 20 美元。<br />
<br />
6. **系统提示功能**<br />
   - 自定义指导与上下文：Anthropic 引入了系统提示功能，允许用户为 Claude 提供自定义说明和上下文，以提升其在特定任务上的性能。<br />
<br />
   - 功能详细说明：<br />
     - 设定目标与角色：允许用户设定目标、指定 Claude 的角色或语气。<br />
     - 建立规则和约束：用户可以建立规则和约束。<br />
     - 提供背景知识：可提供相关背景知识。<br />
     - 定义验证标准：定义输出验证的标准。<br />
   - 响应优化：通过这种方式提示 Claude，可以塑造更准确、更一致的响应，保持角色扮演的特征，并更可靠地遵循提供的指南。<br />
   - 目标：系统提示的最终目的是增强 Claude 在预期实际应用中的能力。<br />
<br />
详细内容：<a href="https://www.anthropic.com/index/claude-2-1">anthropic.com/index/claude-2…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3MTE5MDU4OTY4MTk1MDcyL2ltZy9DTmhDdWZPNGRtZXVsdk1DLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727112003372892187#m</id>
            <title>奥特曼被解雇后

ChatGPT发布首个更新内容

语音功能向所有免费用户开放

更新下载客户端即可直接使用语音功能</title>
            <link>https://nitter.cz/xiaohuggg/status/1727112003372892187#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727112003372892187#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 23:49:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>奥特曼被解雇后<br />
<br />
ChatGPT发布首个更新内容<br />
<br />
语音功能向所有免费用户开放<br />
<br />
更新下载客户端即可直接使用语音功能</p>
<p><a href="https://nitter.cz/OpenAI/status/1727065166188274145#m">nitter.cz/OpenAI/status/1727065166188274145#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726966015052169228#m</id>
            <title>Stable to Video 要来了…

看效果是有点炸裂👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1726966015052169228#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726966015052169228#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 14:09:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable to Video 要来了…<br />
<br />
看效果是有点炸裂👍</p>
<p><a href="https://nitter.cz/EMostaque/status/1726929962211647855#m">nitter.cz/EMostaque/status/1726929962211647855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726955744866750674#m</id>
            <title>使用@suno_ai_ 和 Meta的 Audiocraft

制作的说唱歌曲😎

当然还融合了博主的一点点才华👌</title>
            <link>https://nitter.cz/xiaohuggg/status/1726955744866750674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726955744866750674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 13:28:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用<a href="https://nitter.cz/suno_ai_" title="Suno">@suno_ai_</a> 和 Meta的 Audiocraft<br />
<br />
制作的说唱歌曲😎<br />
<br />
当然还融合了博主的一点点才华👌</p>
<p><a href="https://nitter.cz/hanqing_me/status/1726934224043974725#m">nitter.cz/hanqing_me/status/1726934224043974725#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726943097387683902#m</id>
            <title>自从GPTs出来后，人人都能做，大量涌现，然后就出现了各种GPTs导航站。

最后的结果是连GPTs导航站都需要导航了😂

这个GPTs导航站思路不错，使用者可以对GPTs进行投票，把高质量的GPT筛选出来...避免鱼龙混杂！

还可以按类别筛选找到自己需要的GPT，还可提交自己的GPT...

🔗http://GPTseek.com

不过感觉目前还是比较简陋，有很大升级空间，比如可以增加评论点评、使用调用情况、以及更详细的GPTs功能升级迭代近况什么的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726943097387683902#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726943097387683902#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 12:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>自从GPTs出来后，人人都能做，大量涌现，然后就出现了各种GPTs导航站。<br />
<br />
最后的结果是连GPTs导航站都需要导航了😂<br />
<br />
这个GPTs导航站思路不错，使用者可以对GPTs进行投票，把高质量的GPT筛选出来...避免鱼龙混杂！<br />
<br />
还可以按类别筛选找到自己需要的GPT，还可提交自己的GPT...<br />
<br />
🔗<a href="http://GPTseek.com">GPTseek.com</a><br />
<br />
不过感觉目前还是比较简陋，有很大升级空间，比如可以增加评论点评、使用调用情况、以及更详细的GPTs功能升级迭代近况什么的。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY5MzkxNDIxOTI1MjUzMTIvcHUvaW1nL1A5TnFtYU5UcFA2UDFYNEkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726928308288561284#m</id>
            <title>Video-LLaVA：大型视觉语言模型 能更好地理解和处理图像和视频。

它通过一种特殊的技术，将图像和视频中的信息转换成类似于文字的格式，使得计算机能够用处理语言的方式来处理视觉信息。

这使得模型能够更好地理解视频内容，就像理解一段文字一样。

测试了下，感觉还不错的！但是要用英文😂

而且该模型能够同时处理文本和视觉信号，提供了一种多模态的理解能力，这对于自动问答系统和其他需要理解复杂视觉信息的应用非常有用。

Video-LLaVA在多个图像和视频基准测试中表现出色，显示了其在不同类型的视觉数据上的强大适应能力。

主要工作原理特点：

1、统一视觉表示：Video-LLaVA通过将图像和视频的特征预先对齐到一个统一的视觉特征空间中，解决了以往模型在处理视觉信息时存在的不对齐问题。这种方法使得大型语言模型（LLM）能够更有效地学习和理解多模态（图像和视频）信息。

2、联合训练：该模型不仅对图像和视频特征进行预对齐，还进行了图像和视频的联合训练。这种训练方式有助于提高模型在理解多模态信息方面的能力。

3、高效的模型结构：Video-LLaVA包括语言绑定编码器（LanguageBind encoders）用于提取原始视觉信号（如图像或视频）的特征，大语言模型（如Vicuna），视觉投影层，以及词嵌入层。这些组件共同工作，提供了一个高效且强大的视觉语言处理框架。

4、优越的性能：在多个图像和视频基准测试中，Video-LLaVA展示了优越的性能。它不仅在图像理解方面超越了先进的视觉语言模型，如mPLUG-owl-7B和InstructBLIP-7B，而且在视频理解方面也超越了专门为视频设计的模型，如Video-ChatGPT。

GitHub：https://github.com/PKU-YuanGroup/Video-LLaVA
论文：https://arxiv.org/abs/2311.10122
HuggingFace演示：https://huggingface.co/spaces/LanguageBind/Video-LLaVA
在线体验：https://replicate.com/nateraw/video-llava</title>
            <link>https://nitter.cz/xiaohuggg/status/1726928308288561284#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726928308288561284#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 11:39:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Video-LLaVA：大型视觉语言模型 能更好地理解和处理图像和视频。<br />
<br />
它通过一种特殊的技术，将图像和视频中的信息转换成类似于文字的格式，使得计算机能够用处理语言的方式来处理视觉信息。<br />
<br />
这使得模型能够更好地理解视频内容，就像理解一段文字一样。<br />
<br />
测试了下，感觉还不错的！但是要用英文😂<br />
<br />
而且该模型能够同时处理文本和视觉信号，提供了一种多模态的理解能力，这对于自动问答系统和其他需要理解复杂视觉信息的应用非常有用。<br />
<br />
Video-LLaVA在多个图像和视频基准测试中表现出色，显示了其在不同类型的视觉数据上的强大适应能力。<br />
<br />
主要工作原理特点：<br />
<br />
1、统一视觉表示：Video-LLaVA通过将图像和视频的特征预先对齐到一个统一的视觉特征空间中，解决了以往模型在处理视觉信息时存在的不对齐问题。这种方法使得大型语言模型（LLM）能够更有效地学习和理解多模态（图像和视频）信息。<br />
<br />
2、联合训练：该模型不仅对图像和视频特征进行预对齐，还进行了图像和视频的联合训练。这种训练方式有助于提高模型在理解多模态信息方面的能力。<br />
<br />
3、高效的模型结构：Video-LLaVA包括语言绑定编码器（LanguageBind encoders）用于提取原始视觉信号（如图像或视频）的特征，大语言模型（如Vicuna），视觉投影层，以及词嵌入层。这些组件共同工作，提供了一个高效且强大的视觉语言处理框架。<br />
<br />
4、优越的性能：在多个图像和视频基准测试中，Video-LLaVA展示了优越的性能。它不仅在图像理解方面超越了先进的视觉语言模型，如mPLUG-owl-7B和InstructBLIP-7B，而且在视频理解方面也超越了专门为视频设计的模型，如Video-ChatGPT。<br />
<br />
GitHub：<a href="https://github.com/PKU-YuanGroup/Video-LLaVA">github.com/PKU-YuanGroup/Vid…</a><br />
论文：<a href="https://arxiv.org/abs/2311.10122">arxiv.org/abs/2311.10122</a><br />
HuggingFace演示：<a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA">huggingface.co/spaces/Langua…</a><br />
在线体验：<a href="https://replicate.com/nateraw/video-llava">replicate.com/nateraw/video-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY5MjgwMDUwOTQ4NTA1NjAvcHUvaW1nL2U3YlA1UzBGOHl6czdqekwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>