<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735204522061148426#m</id>
            <title>ChatGPT作为非人类 入选Nature2023年度十大科学人物

顶级科学期刊《Nature》公布2023 年在科学领域产生重大影响的十位人物。

让人惊讶的是，今年还有一位非人类入选 ，那就是过去一年出尽风头的聊天机器人： ChatGPT。

《自然》特写部主编 Richard Monastersky 表示：

“虽然这个工具不算人物，也不完全满足《自然》十大人物的评选条件，但我们将其破例纳入榜单，从而承认生成式 AI 给科学发展和进步带来的巨大改变。”

其中OpenAI的首席科学家 Ilya Sutskever也上榜了...

以下是他们成就的详细介绍：

1、Kalpana Kalahasti（卡尔帕纳·卡拉哈斯提）

成就：作为工程师和经理，她在确保印度的月球任务 Chandrayaan-3 成功着陆月球方面发挥了关键作用，使印度成为第四个实现这一壮举的国家。

2、Marina Silva（玛丽娜·席尔瓦）

成就：帮助巴西亚马逊控制了猖獗的森林砍伐，并重建了被前政府削弱的机构。

3、Katsuhiko Hayashi（林胜彦）

成就：用雄性小鼠的细胞制造出了有活力的卵子，这一工作有助于拯救濒临灭绝的物种。

4、Annie Kritcher（安妮·克里彻）

成就：这位物理学家帮助美国国家点火装置产生了曾经只在氢弹和恒星中看到的核反应。

5、Eleni Myrivili 

成就：作为联合国首席高温官，正在帮助世界做好应对气候变化威胁的准备。

6、Ilya Sutskever

成就：ChatGPT 和其他正在改变社会的人工智能系统的先驱。

7、James Hamlin

成就：这位物理学家帮助发现了室温超导性的耸人听闻的说法的缺陷。

8、Svetlana Mojsov

成就：因参与开发价值数十亿美元的减肥药物而逐渐获得认可。

9、Halidou Tinto

成就：由于这位研究人员的严格测试，推动了治疗致命疾病的第二种疫苗问世。

10、Thomas Powles

成就：这位医生和癌症研究人员领导了一项治疗严重膀胱癌的变革性临床试验。

11、ChatGPT

成就：作为生成式人工智能的典型代表，预示着科学领域一个崭新时代的可能到来。

所有11位的完整个人档案点击这里查看：https://www.nature.com/immersive/d41586-023-03919-1/index.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1735204522061148426#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735204522061148426#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 07:46:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT作为非人类 入选Nature2023年度十大科学人物<br />
<br />
顶级科学期刊《Nature》公布2023 年在科学领域产生重大影响的十位人物。<br />
<br />
让人惊讶的是，今年还有一位非人类入选 ，那就是过去一年出尽风头的聊天机器人： ChatGPT。<br />
<br />
《自然》特写部主编 Richard Monastersky 表示：<br />
<br />
“虽然这个工具不算人物，也不完全满足《自然》十大人物的评选条件，但我们将其破例纳入榜单，从而承认生成式 AI 给科学发展和进步带来的巨大改变。”<br />
<br />
其中OpenAI的首席科学家 Ilya Sutskever也上榜了...<br />
<br />
以下是他们成就的详细介绍：<br />
<br />
1、Kalpana Kalahasti（卡尔帕纳·卡拉哈斯提）<br />
<br />
成就：作为工程师和经理，她在确保印度的月球任务 Chandrayaan-3 成功着陆月球方面发挥了关键作用，使印度成为第四个实现这一壮举的国家。<br />
<br />
2、Marina Silva（玛丽娜·席尔瓦）<br />
<br />
成就：帮助巴西亚马逊控制了猖獗的森林砍伐，并重建了被前政府削弱的机构。<br />
<br />
3、Katsuhiko Hayashi（林胜彦）<br />
<br />
成就：用雄性小鼠的细胞制造出了有活力的卵子，这一工作有助于拯救濒临灭绝的物种。<br />
<br />
4、Annie Kritcher（安妮·克里彻）<br />
<br />
成就：这位物理学家帮助美国国家点火装置产生了曾经只在氢弹和恒星中看到的核反应。<br />
<br />
5、Eleni Myrivili <br />
<br />
成就：作为联合国首席高温官，正在帮助世界做好应对气候变化威胁的准备。<br />
<br />
6、Ilya Sutskever<br />
<br />
成就：ChatGPT 和其他正在改变社会的人工智能系统的先驱。<br />
<br />
7、James Hamlin<br />
<br />
成就：这位物理学家帮助发现了室温超导性的耸人听闻的说法的缺陷。<br />
<br />
8、Svetlana Mojsov<br />
<br />
成就：因参与开发价值数十亿美元的减肥药物而逐渐获得认可。<br />
<br />
9、Halidou Tinto<br />
<br />
成就：由于这位研究人员的严格测试，推动了治疗致命疾病的第二种疫苗问世。<br />
<br />
10、Thomas Powles<br />
<br />
成就：这位医生和癌症研究人员领导了一项治疗严重膀胱癌的变革性临床试验。<br />
<br />
11、ChatGPT<br />
<br />
成就：作为生成式人工智能的典型代表，预示着科学领域一个崭新时代的可能到来。<br />
<br />
所有11位的完整个人档案点击这里查看：<a href="https://www.nature.com/immersive/d41586-023-03919-1/index.html">nature.com/immersive/d41586-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTcDI5VWFZQUFxeTVGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735184195121823841#m</id>
            <title>R to @xiaohuggg: 可以试试这个公式 分享一下你们

测试的案例看看</title>
            <link>https://nitter.cz/xiaohuggg/status/1735184195121823841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735184195121823841#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 06:25:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可以试试这个公式 分享一下你们<br />
<br />
测试的案例看看</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTY3dZOGJJQUFqNWRmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735184007569330248#m</id>
            <title>知名品牌设计博主、我的好朋友@Salmaaboukarr  分享了一个特定的提示公式。

使用 GPT 4 和 Midjourney 为你的产品快速、低成本的生成高质量产品背景图。

小白也能轻松上手...😀

Salma 的提示公式：

生成图像的提示公式是：

“Generate a (lifestyle/editorial) image for my (描述你的产品/品牌)”，

然后按照以下结构填写细节：

(Composition) (subject) (Colours) (Scene Description) (Style of Shot)。

（构图）（主题）（色彩）（场景描述）（拍摄风格）

操作步骤：首先使用 GPT/DALL·E 生成图像，然后将相同的提示复制并粘贴到 Midjourney 中，以创建最终图像。

可选步骤：可以使用 http://Magnific.ai 来提升图像质量和品质。

最后：可以关注@Salmaaboukarr 她专注使用AI来设计品牌内容图像，可以学到很多东西。❤️</title>
            <link>https://nitter.cz/xiaohuggg/status/1735184007569330248#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735184007569330248#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 06:24:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>知名品牌设计博主、我的好朋友<a href="https://nitter.cz/Salmaaboukarr" title="Salma">@Salmaaboukarr</a>  分享了一个特定的提示公式。<br />
<br />
使用 GPT 4 和 Midjourney 为你的产品快速、低成本的生成高质量产品背景图。<br />
<br />
小白也能轻松上手...😀<br />
<br />
Salma 的提示公式：<br />
<br />
生成图像的提示公式是：<br />
<br />
“Generate a (lifestyle/editorial) image for my (描述你的产品/品牌)”，<br />
<br />
然后按照以下结构填写细节：<br />
<br />
(Composition) (subject) (Colours) (Scene Description) (Style of Shot)。<br />
<br />
（构图）（主题）（色彩）（场景描述）（拍摄风格）<br />
<br />
操作步骤：首先使用 GPT/DALL·E 生成图像，然后将相同的提示复制并粘贴到 Midjourney 中，以创建最终图像。<br />
<br />
可选步骤：可以使用 <a href="http://Magnific.ai">Magnific.ai</a> 来提升图像质量和品质。<br />
<br />
最后：可以关注<a href="https://nitter.cz/Salmaaboukarr" title="Salma">@Salmaaboukarr</a> 她专注使用AI来设计品牌内容图像，可以学到很多东西。❤️</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTVVhUVmFVQUF2M2M0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735175996285096021#m</id>
            <title>R to @xiaohuggg: Outfit Anyone 在线体验地址：https://huggingface.co/spaces/HumanAIGC/OutfitAnyone</title>
            <link>https://nitter.cz/xiaohuggg/status/1735175996285096021#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735175996285096021#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:52:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Outfit Anyone 在线体验地址：<a href="https://huggingface.co/spaces/HumanAIGC/OutfitAnyone">huggingface.co/spaces/HumanA…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTVk40OGFZQUFEQzFhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735171253428998556#m</id>
            <title>R to @xiaohuggg: 动漫角色也支持</title>
            <link>https://nitter.cz/xiaohuggg/status/1735171253428998556#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735171253428998556#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:33:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>动漫角色也支持</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0JTUkJ1cWFjQUFpM0pTLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dCU1JCdXFhY0FBaTNKUy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735171028077449512#m</id>
            <title>R to @xiaohuggg: 支持Outfit 全套服装 一起更换</title>
            <link>https://nitter.cz/xiaohuggg/status/1735171028077449512#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735171028077449512#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:33:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持Outfit 全套服装 一起更换</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNzA5MTgxNjk4ODI2MjQvcHUvaW1nL2hZYS1zVkVOSUJVbndld2MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735170872774963222#m</id>
            <title>Outfit Anyone：由阿里巴巴开发的一个虚拟试衣技术，可以让你在电脑上看到自己穿上任何衣服的样子。

- 逼真的效果：它能创建非常真实的图片，让用户看起来就像真的穿上了那些衣服一样。

- 适用于任何衣服和人物：无论是什么样的衣服或是什么样的人，都能完美适配。

- 个性化时尚展示：可以展示各种独特和时尚的服装搭配。

- 适应不同体型：能够适应不同的体型，为各种身材的人提供试衣效果。

- 强大泛化能力：模型具有强大的泛化能力，可以支持动画角色的新服装形象创建。

- 细节增强：它能够显着增强服装的质感和真实感，同时保持服装的一致性。

- Outfit Anyone + Animate Anyone：与 Animate Anybody（图像动作视频模型https://twitter.com/xiaohuggg/status/1730133378501067046）的集成，以实现任何角色的服装变化和动态视频生成。

项目地址：https://humanaigc.github.io/outfit-anyone/
GitHub：https://github.com/HumanAIGC/OutfitAnyone（coming soon...）</title>
            <link>https://nitter.cz/xiaohuggg/status/1735170872774963222#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735170872774963222#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:32:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Outfit Anyone：由阿里巴巴开发的一个虚拟试衣技术，可以让你在电脑上看到自己穿上任何衣服的样子。<br />
<br />
- 逼真的效果：它能创建非常真实的图片，让用户看起来就像真的穿上了那些衣服一样。<br />
<br />
- 适用于任何衣服和人物：无论是什么样的衣服或是什么样的人，都能完美适配。<br />
<br />
- 个性化时尚展示：可以展示各种独特和时尚的服装搭配。<br />
<br />
- 适应不同体型：能够适应不同的体型，为各种身材的人提供试衣效果。<br />
<br />
- 强大泛化能力：模型具有强大的泛化能力，可以支持动画角色的新服装形象创建。<br />
<br />
- 细节增强：它能够显着增强服装的质感和真实感，同时保持服装的一致性。<br />
<br />
- Outfit Anyone + Animate Anyone：与 Animate Anybody（图像动作视频模型<a href="https://nitter.cz/xiaohuggg/status/1730133378501067046">nitter.cz/xiaohuggg/status…</a>）的集成，以实现任何角色的服装变化和动态视频生成。<br />
<br />
项目地址：<a href="https://humanaigc.github.io/outfit-anyone/">humanaigc.github.io/outfit-a…</a><br />
GitHub：<a href="https://github.com/HumanAIGC/OutfitAnyone">github.com/HumanAIGC/OutfitA…</a>（coming soon...）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNzAxOTU0MzY3NTI4OTYvcHUvaW1nL1hrMFktSENMNWxfbExZTmwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735149171781533870#m</id>
            <title>DeepMind 开发的文本到图像生成技术：Imagen 2

- 根据文字生成图片：利用文本提示生成高质量、逼真的图像。

- 更真实的图像生成：在渲染真实的手和人脸等方面取得了进步，减少了视觉干扰元素。

- 改善图片和文字的匹配：通过更好地理解文字描述，Imagen 2 能更准确地生成与描述相符的图片。

- 图像编辑能力：支持“修图”和“扩图”功能，用户可以通过提供参考图像和图像遮罩来生成新内容，直接进入原始图像，或将原始图像扩展到其边界之外。

技术创新：

Imagen 2使用基于扩散的技术来提高图像生成的灵活性，特别是在控制和调整图像风格方面。用户可以通过结合参考图像和文本提示来引导图像生成过程，创造出符合特定风格和内容的视觉作品。

它通过利用其训练数据的自然分布来生成更逼真的图像，而不是采用预设的风格。

开发团队特别强调了对图像附加文字描述（字幕）的重要性。这种方法的具体含义和目的如下：

增强训练数据集：为了提高模型生成图像的质量和准确性，Imagen 2 的训练数据集中包含了更多详细的图像字幕。这些字幕为模型提供了关于图像内容的额外信息。

学习不同的字幕风格：通过训练，Imagen 2 学会了理解和解释不同风格的文字描述。这意味着模型能够处理和响应各种类型的用户输入，无论其语言风格如何。

更好地理解用户提示：增加对图像字幕的描述不仅提高了模型对特定图像内容的理解，还使其能够更准确地根据用户的文本提示生成图像。这是因为模型通过学习大量的图像和对应字幕，能够更好地把握文本和视觉内容之间的关联。

 Imagen 2 现已在Vertex AI 上全面推出，你可以在几秒钟内将用户的想象力转化为高品质的视觉素材。

- 文字转图片生成：使用文字提示来生成新图片。

- 使用文字提示修改图片：允许用户使用文字提示来修改整个上传或生成的图片。

- 局部图片修改：用户可以定义遮盖区域，仅修改上传或生成的图片的某些部分。

- 图片放大：支持放大现有的、生成的或编辑后的图片。

- 主题模型微调：允许用户使用特定主题（如特定的手提包或鞋）来微调模型，以生成图片。

- 视觉字幕：提供图片的文字说明。

- 视觉问答 (VQA)：提供有关图片的问题解答。

- 多语言文本渲染：支持多种语言，能够在图像中准确地覆盖文本。

- 标志生成：能够创造公司或产品标志，并将其覆盖在图像中。

Imagen 2详细介绍：https://deepmind.google/technologies/imagen-2/

Imagen 2 on Vertex AI：https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available</title>
            <link>https://nitter.cz/xiaohuggg/status/1735149171781533870#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735149171781533870#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 04:06:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind 开发的文本到图像生成技术：Imagen 2<br />
<br />
- 根据文字生成图片：利用文本提示生成高质量、逼真的图像。<br />
<br />
- 更真实的图像生成：在渲染真实的手和人脸等方面取得了进步，减少了视觉干扰元素。<br />
<br />
- 改善图片和文字的匹配：通过更好地理解文字描述，Imagen 2 能更准确地生成与描述相符的图片。<br />
<br />
- 图像编辑能力：支持“修图”和“扩图”功能，用户可以通过提供参考图像和图像遮罩来生成新内容，直接进入原始图像，或将原始图像扩展到其边界之外。<br />
<br />
技术创新：<br />
<br />
Imagen 2使用基于扩散的技术来提高图像生成的灵活性，特别是在控制和调整图像风格方面。用户可以通过结合参考图像和文本提示来引导图像生成过程，创造出符合特定风格和内容的视觉作品。<br />
<br />
它通过利用其训练数据的自然分布来生成更逼真的图像，而不是采用预设的风格。<br />
<br />
开发团队特别强调了对图像附加文字描述（字幕）的重要性。这种方法的具体含义和目的如下：<br />
<br />
增强训练数据集：为了提高模型生成图像的质量和准确性，Imagen 2 的训练数据集中包含了更多详细的图像字幕。这些字幕为模型提供了关于图像内容的额外信息。<br />
<br />
学习不同的字幕风格：通过训练，Imagen 2 学会了理解和解释不同风格的文字描述。这意味着模型能够处理和响应各种类型的用户输入，无论其语言风格如何。<br />
<br />
更好地理解用户提示：增加对图像字幕的描述不仅提高了模型对特定图像内容的理解，还使其能够更准确地根据用户的文本提示生成图像。这是因为模型通过学习大量的图像和对应字幕，能够更好地把握文本和视觉内容之间的关联。<br />
<br />
 Imagen 2 现已在Vertex AI 上全面推出，你可以在几秒钟内将用户的想象力转化为高品质的视觉素材。<br />
<br />
- 文字转图片生成：使用文字提示来生成新图片。<br />
<br />
- 使用文字提示修改图片：允许用户使用文字提示来修改整个上传或生成的图片。<br />
<br />
- 局部图片修改：用户可以定义遮盖区域，仅修改上传或生成的图片的某些部分。<br />
<br />
- 图片放大：支持放大现有的、生成的或编辑后的图片。<br />
<br />
- 主题模型微调：允许用户使用特定主题（如特定的手提包或鞋）来微调模型，以生成图片。<br />
<br />
- 视觉字幕：提供图片的文字说明。<br />
<br />
- 视觉问答 (VQA)：提供有关图片的问题解答。<br />
<br />
- 多语言文本渲染：支持多种语言，能够在图像中准确地覆盖文本。<br />
<br />
- 标志生成：能够创造公司或产品标志，并将其覆盖在图像中。<br />
<br />
Imagen 2详细介绍：<a href="https://deepmind.google/technologies/imagen-2/">deepmind.google/technologies…</a><br />
<br />
Imagen 2 on Vertex AI：<a href="https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available">cloud.google.com/blog/produc…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDY0OTkwNjIwMTM5NTIvcHUvaW1nL21ubm9BQ0NjeURlUG1lY0guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735143546867569089#m</id>
            <title>Dolphins：一种新型的视觉-语言模型，旨在模拟人类驾驶能力

Dolphins能理解和处理视频、文字指令和驾驶信号。分析驾驶场景，比如识别城市交叉路口、夜间交通等。

它还能预测和规划车辆行为，比如为什么要慢行。

Dolphins还展现了类似人类的能力，即时学习和适应、通过上下文学习的反思和错误恢复。

该项目由来自威斯康星大学麦迪逊分校、NVIDIA、密歇根大学和斯坦福大学的研究人员共同参与。旨在让自动驾驶汽车更好地理解复杂的驾驶环境，类似于人类司机的理解能力。

独特功能:

Dolphins 具备全面理解复杂和长尾开放世界驾驶场景的能力，并能解决一系列 AV 任务。

能够处理包括场景理解、预测和规划在内的多种任务，感知静态和动态场景，整合环境因素，并有效处理下游预测和规划任务。

Dolphins 主要能做以下几件事：

1、理解复杂驾驶场景：Dolphins 能够分析和理解各种驾驶环境，比如城市交叉路口、夜间繁忙道路、隧道内行驶等。

2、处理多模态输入：能够处理包括视频（或图像）数据、文本指令和历史控制信号在内的多模态输入，以生成与提供的指令相对应的输出。

3、预测和规划行为：能够预测车辆在不同情况下的行为，如在交通拥堵时的低速行驶。它还能为车辆制定未来的行动计划，比如在遇到红灯时停车等待。

4、提供详细的场景描述：能够详细描述视频中的驾驶相关信息，如路况、交通状况、其他车辆和行人的行为等。

5、适应和学习：它具备快速学习和适应新场景的能力，能够根据实时情况做出反应和调整。Dolphins 展现了类似人类的能力，包括无梯度的即时适应、通过上下文学习的反思和错误恢复。

技术细节和原理：

1、基于 OpenFlamingo 构建：Dolphins 基于开源的预训练视觉-语言模型 OpenFlamingo 构建，并针对驾驶领域进行了特定的指令调整和数据构建。

2、BDD-X 数据集的应用：通过利用 BDD-X 数据集，Dolphins 融合了四种不同的自动驾驶车辆（AV）任务，以全面理解复杂的驾驶场景。

3、GCoT 过程：通过创新的 GCoT 过程，Dolphins 的推理能力得到增强。这个过程涉及构建图像指令跟随数据集，并基于公共的视觉问答（VQA）数据集、视觉指令数据集和 ChatGPT 来培养 OpenFlamingo 模型的细粒度推理能力。

4、驾驶领域的定制：Dolphins 通过构建特定于驾驶的指令数据和进行指令调整，被专门调整以适应驾驶领域。

项目及演示：https://vlm-driver.github.io/
论文：https://arxiv.org/abs/2312.00438
代码：https://github.com/vlm-driver/Dolphins</title>
            <link>https://nitter.cz/xiaohuggg/status/1735143546867569089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735143546867569089#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 03:43:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Dolphins：一种新型的视觉-语言模型，旨在模拟人类驾驶能力<br />
<br />
Dolphins能理解和处理视频、文字指令和驾驶信号。分析驾驶场景，比如识别城市交叉路口、夜间交通等。<br />
<br />
它还能预测和规划车辆行为，比如为什么要慢行。<br />
<br />
Dolphins还展现了类似人类的能力，即时学习和适应、通过上下文学习的反思和错误恢复。<br />
<br />
该项目由来自威斯康星大学麦迪逊分校、NVIDIA、密歇根大学和斯坦福大学的研究人员共同参与。旨在让自动驾驶汽车更好地理解复杂的驾驶环境，类似于人类司机的理解能力。<br />
<br />
独特功能:<br />
<br />
Dolphins 具备全面理解复杂和长尾开放世界驾驶场景的能力，并能解决一系列 AV 任务。<br />
<br />
能够处理包括场景理解、预测和规划在内的多种任务，感知静态和动态场景，整合环境因素，并有效处理下游预测和规划任务。<br />
<br />
Dolphins 主要能做以下几件事：<br />
<br />
1、理解复杂驾驶场景：Dolphins 能够分析和理解各种驾驶环境，比如城市交叉路口、夜间繁忙道路、隧道内行驶等。<br />
<br />
2、处理多模态输入：能够处理包括视频（或图像）数据、文本指令和历史控制信号在内的多模态输入，以生成与提供的指令相对应的输出。<br />
<br />
3、预测和规划行为：能够预测车辆在不同情况下的行为，如在交通拥堵时的低速行驶。它还能为车辆制定未来的行动计划，比如在遇到红灯时停车等待。<br />
<br />
4、提供详细的场景描述：能够详细描述视频中的驾驶相关信息，如路况、交通状况、其他车辆和行人的行为等。<br />
<br />
5、适应和学习：它具备快速学习和适应新场景的能力，能够根据实时情况做出反应和调整。Dolphins 展现了类似人类的能力，包括无梯度的即时适应、通过上下文学习的反思和错误恢复。<br />
<br />
技术细节和原理：<br />
<br />
1、基于 OpenFlamingo 构建：Dolphins 基于开源的预训练视觉-语言模型 OpenFlamingo 构建，并针对驾驶领域进行了特定的指令调整和数据构建。<br />
<br />
2、BDD-X 数据集的应用：通过利用 BDD-X 数据集，Dolphins 融合了四种不同的自动驾驶车辆（AV）任务，以全面理解复杂的驾驶场景。<br />
<br />
3、GCoT 过程：通过创新的 GCoT 过程，Dolphins 的推理能力得到增强。这个过程涉及构建图像指令跟随数据集，并基于公共的视觉问答（VQA）数据集、视觉指令数据集和 ChatGPT 来培养 OpenFlamingo 模型的细粒度推理能力。<br />
<br />
4、驾驶领域的定制：Dolphins 通过构建特定于驾驶的指令数据和进行指令调整，被专门调整以适应驾驶领域。<br />
<br />
项目及演示：<a href="https://vlm-driver.github.io/">vlm-driver.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.00438">arxiv.org/abs/2312.00438</a><br />
代码：<a href="https://github.com/vlm-driver/Dolphins">github.com/vlm-driver/Dolphi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDEzNTUxODg1OTI2NDAvcHUvaW1nL0RKUmZFN1BMdEhRcWVVYlouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735129903937560615#m</id>
            <title>这个挺好玩的 哈哈哈😂

用摄像头进行输入，以30+fps AI实时生图，把你变成另一个人...

可以使用提示词控制你想要生成什么图像或者模仿谁谁谁...

在电脑上尝试：http://fal.ai/camera</title>
            <link>https://nitter.cz/xiaohuggg/status/1735129903937560615#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735129903937560615#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 02:49:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺好玩的 哈哈哈😂<br />
<br />
用摄像头进行输入，以30+fps AI实时生图，把你变成另一个人...<br />
<br />
可以使用提示词控制你想要生成什么图像或者模仿谁谁谁...<br />
<br />
在电脑上尝试：<a href="http://fal.ai/camera">fal.ai/camera</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxMjc5MzI1MjY1ODM4MDgvcHUvaW1nL2poLUo4b1hIZXRFc1RyeUEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735126281552044403#m</id>
            <title>ChatGPT Plus 恢复订阅了

搞到了更多GPU...

看来目前最大的阻碍依然是算力问题...</title>
            <link>https://nitter.cz/xiaohuggg/status/1735126281552044403#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735126281552044403#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 02:35:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Plus 恢复订阅了<br />
<br />
搞到了更多GPU...<br />
<br />
看来目前最大的阻碍依然是算力问题...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JSbi1yR2JRQUFCNjdxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734958068595831011#m</id>
            <title>Google宣布Gemini Pro 版本已经向开发者和企业开放，可用于构建 AI 应用。

最重要的是目前完全免费！🆓💰

• 免费使用：目前可以在限制内免费使用，并且未来将提供具有竞争力的定价。

• 特性：支持包括函数调用、嵌入、语义检索、自定义知识基础和聊天功能。

• 语言支持：支持全球 180 多个国家和地区的 38 种语言。

开发者目前可以通过 Google AI Studio 免费访问 Gemini Pro 和 Gemini Pro Vision，适用于大多数应用程序开发需求。

Google计划在明年初推出 Gemini Ultra…

详细：https://blog.google/technology/ai/gemini-api-developers-cloud/?utm_sourc</title>
            <link>https://nitter.cz/xiaohuggg/status/1734958068595831011#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734958068595831011#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 15:26:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google宣布Gemini Pro 版本已经向开发者和企业开放，可用于构建 AI 应用。<br />
<br />
最重要的是目前完全免费！🆓💰<br />
<br />
• 免费使用：目前可以在限制内免费使用，并且未来将提供具有竞争力的定价。<br />
<br />
• 特性：支持包括函数调用、嵌入、语义检索、自定义知识基础和聊天功能。<br />
<br />
• 语言支持：支持全球 180 多个国家和地区的 38 种语言。<br />
<br />
开发者目前可以通过 Google AI Studio 免费访问 Gemini Pro 和 Gemini Pro Vision，适用于大多数应用程序开发需求。<br />
<br />
Google计划在明年初推出 Gemini Ultra…<br />
<br />
详细：<a href="https://blog.google/technology/ai/gemini-api-developers-cloud/?utm_sourc">blog.google/technology/ai/ge…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM0OTU4MDAyNTg1ODY2MjQxL2ltZy9rV21zUHM5dlh2bDVNN2ZKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</id>
            <title>R to @xiaohuggg: 将Logo转换成一些真实场景图</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882384385110292#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将Logo转换成一些真实场景图</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIyNTM3OTExNzQ2NTYvcHUvaW1nLzZXTkFWMkkzSGNwOFBFZVEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</id>
            <title>R to @xiaohuggg: 图像放大增强演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882381759443163#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>图像放大增强演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIxMzA1NjI2MTMyNDgvcHUvaW1nL3Y5Zm9JSEpzem1DXzZSeU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882379439981033#m</id>
            <title>http://Krea.AI因为实时生图火爆全网，一直处于内测阶段，今天正式开放访问。

主要功能：

- 实时生成：根据提示和编辑器实时编辑创造完美的图像

- 放大增强：提升图像分辨率增强图像画质

- Logo Illusions：将Logo转换成一些真实场景中

- AI Patterns：创建一些类似中世纪螺旋AI图像</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882379439981033#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882379439981033#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="http://Krea.AI">Krea.AI</a>因为实时生图火爆全网，一直处于内测阶段，今天正式开放访问。<br />
<br />
主要功能：<br />
<br />
- 实时生成：根据提示和编辑器实时编辑创造完美的图像<br />
<br />
- 放大增强：提升图像分辨率增强图像画质<br />
<br />
- Logo Illusions：将Logo转换成一些真实场景中<br />
<br />
- AI Patterns：创建一些类似中世纪螺旋AI图像</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODE5NTIzOTI3MjQ0ODAvcHUvaW1nL0tmWkxTWmtkNU9MR2FPRWguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734873428174458986#m</id>
            <title>CopilotKit：一个开源项目，可以在任何基于React的Web应用中构建内嵌的AI聊天机器人和AI驱动的文本区域。

主要特点：

- 内嵌AI聊天机器人：机器人可以理解应用的当前状态，并在应用内执行操作。

- AI驱动的文本区域：提供AI生成和编辑文本的功能，可以替换任何标准的文本输入区域

- 自动上下文感知：根据上下文自动完成建议

- 全面的定制化：允许完全定制提示工程和UI设计

- 支持多种模型：可以与不同的前端和后端SDK结合，支持多种大语言模型。

使用场景：

文本生成和编辑：在应用中提供AI辅助的文本生成和编辑功能，例如自动完成和内容生成。

交互式聊天机器人：创建可以与应用前端和后端以及第三方服务交互的聊天机器人。

总之：CopilotKit 是一个为React开发者提供强大AI集成能力的工具集，通过简化AI功能的集成过程，使得创建交互式和智能的Web应用变得更加容易。

GitHub：https://github.com/CopilotKit/CopilotKit</title>
            <link>https://nitter.cz/xiaohuggg/status/1734873428174458986#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734873428174458986#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 09:50:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>CopilotKit：一个开源项目，可以在任何基于React的Web应用中构建内嵌的AI聊天机器人和AI驱动的文本区域。<br />
<br />
主要特点：<br />
<br />
- 内嵌AI聊天机器人：机器人可以理解应用的当前状态，并在应用内执行操作。<br />
<br />
- AI驱动的文本区域：提供AI生成和编辑文本的功能，可以替换任何标准的<textarea></textarea>文本输入区域<br />
<br />
- 自动上下文感知：根据上下文自动完成建议<br />
<br />
- 全面的定制化：允许完全定制提示工程和UI设计<br />
<br />
- 支持多种模型：可以与不同的前端和后端SDK结合，支持多种大语言模型。<br />
<br />
使用场景：<br />
<br />
文本生成和编辑：在应用中提供AI辅助的文本生成和编辑功能，例如自动完成和内容生成。<br />
<br />
交互式聊天机器人：创建可以与应用前端和后端以及第三方服务交互的聊天机器人。<br />
<br />
总之：CopilotKit 是一个为React开发者提供强大AI集成能力的工具集，通过简化AI功能的集成过程，使得创建交互式和智能的Web应用变得更加容易。<br />
<br />
GitHub：<a href="https://github.com/CopilotKit/CopilotKit">github.com/CopilotKit/Copilo…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MjAwNzk4NDQ2NzE0ODgvcHUvaW1nL3libkVkempJTlNmaWZ4R3MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</id>
            <title>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。

超越了不久刚发布的Gemini Ultra的90.04%😅

在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。

这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。

具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。

然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。

这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。

这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。

微软公布了其最新的研究成果和Medprompt+ 仓库。

详细内容：https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/

Medprompt+ 仓库：https://github.com/microsoft/promptbase</title>
            <link>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734811424348914156#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。<br />
<br />
超越了不久刚发布的Gemini Ultra的90.04%😅<br />
<br />
在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。<br />
<br />
这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。<br />
<br />
具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。<br />
<br />
然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。<br />
<br />
这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。<br />
<br />
这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。<br />
<br />
微软公布了其最新的研究成果和Medprompt+ 仓库。<br />
<br />
详细内容：<a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">microsoft.com/en-us/research…</a><br />
<br />
Medprompt+ 仓库：<a href="https://github.com/microsoft/promptbase">github.com/microsoft/promptb…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729862138796351499#m">nitter.cz/xiaohuggg/status/1729862138796351499#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>