<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752339432857035051#m</id>
            <title>Apple Vision Pro 评测解禁了

网红、大V们 开始刷屏了...

🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1752339432857035051#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752339432857035051#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 14:34:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Apple Vision Pro 评测解禁了<br />
<br />
网红、大V们 开始刷屏了...<br />
<br />
🤓</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752331938940219463#m</id>
            <title>R to @xiaohuggg: 4、文本信息识别与处理
Qwen-VL-Plus/Max 现在可以有效地从表格和文档中提取信息，并重新格式化这些信息以满足自定义输出要求。

此外，它还具有识别和转换密集文本的有效机制，这在处理包含大量信息的文档时非常有效。它支持具有极端纵横比的图像，确保灵活地处理各种视觉内容。</title>
            <link>https://nitter.cz/xiaohuggg/status/1752331938940219463#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752331938940219463#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 14:04:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>4、文本信息识别与处理<br />
Qwen-VL-Plus/Max 现在可以有效地从表格和文档中提取信息，并重新格式化这些信息以满足自定义输出要求。<br />
<br />
此外，它还具有识别和转换密集文本的有效机制，这在处理包含大量信息的文档时非常有效。它支持具有极端纵横比的图像，确保灵活地处理各种视觉内容。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZHSW1zMmJZQUFNTHBYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752331640901431358#m</id>
            <title>R to @xiaohuggg: 3、视觉推理能力：解决实际问题  

最新的 Qwen-VL 最显着的进步之一是它能够基于视觉输入进行复杂推理。

这种增强的视觉推理能力远远超出了单纯的内容描述，延伸到对流程图、图表和其他符号系统等复杂表示的理解和解释。

在问题解决和推理领域，Qwen-VL-Plus/Max不仅擅长数学问题解决和信息组织，而且擅长对图表和图形进行更深入的解释和分析。</title>
            <link>https://nitter.cz/xiaohuggg/status/1752331640901431358#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752331640901431358#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 14:03:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3、视觉推理能力：解决实际问题  <br />
<br />
最新的 Qwen-VL 最显着的进步之一是它能够基于视觉输入进行复杂推理。<br />
<br />
这种增强的视觉推理能力远远超出了单纯的内容描述，延伸到对流程图、图表和其他符号系统等复杂表示的理解和解释。<br />
<br />
在问题解决和推理领域，Qwen-VL-Plus/Max不仅擅长数学问题解决和信息组织，而且擅长对图表和图形进行更深入的解释和分析。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZHSVZNcGJjQUFnbUNwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752331273019019704#m</id>
            <title>R to @xiaohuggg: 2、视觉代理能力：视觉定位 

除了在描述和识别方面的基本功能外，Qwen-VL 还具有令人印象深刻的精确定位和查询特定元素的能力。 

 例如，它可以准确地突出图像中的黑色汽车。此外，Qwen-VL 还能够根据场景的当前背景做出判断、推论和决策。</title>
            <link>https://nitter.cz/xiaohuggg/status/1752331273019019704#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752331273019019704#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 14:01:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2、视觉代理能力：视觉定位 <br />
<br />
除了在描述和识别方面的基本功能外，Qwen-VL 还具有令人印象深刻的精确定位和查询特定元素的能力。 <br />
<br />
 例如，它可以准确地突出图像中的黑色汽车。此外，Qwen-VL 还能够根据场景的当前背景做出判断、推论和决策。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZHSF9kR2JBQUFTLVFfLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752330947071234126#m</id>
            <title>R to @xiaohuggg: Qwen-VL-Plus和Qwen-VL-Max模型的新版本不仅在基准测试中展示了卓越的性能，而且在真实世界场景中的问题解决能力上也显示出显著的提升。  

模型能够轻松地进行对话、识别名人和地标、生成文本，特别是在描述和解释视觉内容方面的能力有了显著增强。

  1、基本识别能力</title>
            <link>https://nitter.cz/xiaohuggg/status/1752330947071234126#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752330947071234126#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 14:00:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Qwen-VL-Plus和Qwen-VL-Max模型的新版本不仅在基准测试中展示了卓越的性能，而且在真实世界场景中的问题解决能力上也显示出显著的提升。  <br />
<br />
模型能够轻松地进行对话、识别名人和地标、生成文本，特别是在描述和解释视觉内容方面的能力有了显著增强。<br />
<br />
  1、基本识别能力</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZHSHNvOGJrQUFjQ2NsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752330697229115490#m</id>
            <title>R to @xiaohuggg: 与开源版Qwen-VL相比，Qwen-VL-Plus和Qwen-VL-Max在多个文本-图像多模态任务中的表现与Gemini Ultra和OpenAI的GPT-4V不相上下，显著超越了以往开源模型的最佳成果。  

特别值得注意的是，在中文问答和中文文本理解任务上，Qwen-VL-Max超过了OpenAI的GPT-4V和Google的Gemini。</title>
            <link>https://nitter.cz/xiaohuggg/status/1752330697229115490#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752330697229115490#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 13:59:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>与开源版Qwen-VL相比，Qwen-VL-Plus和Qwen-VL-Max在多个文本-图像多模态任务中的表现与Gemini Ultra和OpenAI的GPT-4V不相上下，显著超越了以往开源模型的最佳成果。  <br />
<br />
特别值得注意的是，在中文问答和中文文本理解任务上，Qwen-VL-Max超过了OpenAI的GPT-4V和Google的Gemini。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZHSGQ1N2FvQUFpZXl2LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752329993601057165#m</id>
            <title>阿里巴巴多模态模型Qwen-VL升级更新：Qwen-VL-Plus和Qwen-VL-Max。

- 在多个文本-图像多模态任务上与Gemini Ultra和GPT-4V相当。

- Qwen-VL-Max在中文问答和中文文本理解任务上超越了GPT-4V和Gemini

- 图像相关推理能力上的大幅提升；

- 在识别、提取和分析图像及其中文本细节上的显著增强；

- 支持超过一百万像素的高清图像和各种宽高比的图像的处理。

模型介绍：

Qwen-VL-Plus：针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高像素分辨率和图像输入的任意宽高比。在广泛的视觉任务上提供了显著的性能提升。

Qwen-VL-Max：与增强版相比，进一步改进了视觉推理和指令跟随能力，提供了更高级别的视觉感知和认知理解。在更广泛的复杂任务上提供了最优性能。

测试了下Gif图也能识别....挺厉害...

详细介绍：https://qwenlm.github.io/blog/qwen-vl/

在线体验：https://huggingface.co/spaces/Qwen/Qwen-VL-Max</title>
            <link>https://nitter.cz/xiaohuggg/status/1752329993601057165#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752329993601057165#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 13:56:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里巴巴多模态模型Qwen-VL升级更新：Qwen-VL-Plus和Qwen-VL-Max。<br />
<br />
- 在多个文本-图像多模态任务上与Gemini Ultra和GPT-4V相当。<br />
<br />
- Qwen-VL-Max在中文问答和中文文本理解任务上超越了GPT-4V和Gemini<br />
<br />
- 图像相关推理能力上的大幅提升；<br />
<br />
- 在识别、提取和分析图像及其中文本细节上的显著增强；<br />
<br />
- 支持超过一百万像素的高清图像和各种宽高比的图像的处理。<br />
<br />
模型介绍：<br />
<br />
Qwen-VL-Plus：针对细节识别能力和文本识别能力进行了显著升级，支持高达数百万像素的超高像素分辨率和图像输入的任意宽高比。在广泛的视觉任务上提供了显著的性能提升。<br />
<br />
Qwen-VL-Max：与增强版相比，进一步改进了视觉推理和指令跟随能力，提供了更高级别的视觉感知和认知理解。在更广泛的复杂任务上提供了最优性能。<br />
<br />
测试了下Gif图也能识别....挺厉害...<br />
<br />
详细介绍：<a href="https://qwenlm.github.io/blog/qwen-vl/">qwenlm.github.io/blog/qwen-v…</a><br />
<br />
在线体验：<a href="https://huggingface.co/spaces/Qwen/Qwen-VL-Max">huggingface.co/spaces/Qwen/Q…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZHRXA3NWF3QUFCeFEyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752316028070326451#m</id>
            <title>SERL：机器人强化学习软件套件

通常教机器人学习一个新动作需要很多次尝试和错误，就像一个人反复练习骑自行车一样。SERL的特别之处在于，它能让机器人通过更少的尝试就能快速学会新任务。

它只需25到 50分钟就能学会一个新任务。

而且能执行多种复杂任务，例如组装电路板、布线或者移动物体。

主要特点：

1、高效学习：通常教机器人学习一个新动作需要很多次尝试和错误，就像一个人反复练习骑自行车一样。SERL的特别之处在于，它能让机器人通过更少的尝试就能快速学会新任务，就像有了一个超级学习能力一样。

2、适应性强：SERL能够让机器人在面对未知或变化的情况时也能够适应并完成任务。比如，如果机器人学会了在一个桌子上装配零件，即使后来桌子的位置改变了，它也能够调整自己的动作继续完成装配。

3、多任务能力：这个软件工具包教会机器人执行多种任务，比如组装电路板、布线或者移动物体到新位置。机器人不仅能学会这些任务，还能在遇到干扰或环境变化时，灵活调整自己的行为。

4、快速执行：通常，让机器人学习一个新任务需要很长时间。但是，使用SERL，机器人可以在大约半小时到一个小时之间就学会一个新任务，这比以往的方法要快得多。

5、高成功率：使用SERL的机器人在完成任务时更加可靠，即使在复杂或不确定的环境中也能保持高成功率。

应用案例：

1、PCB电路组装：

情境：机器人需要将电子元件精确地安装到电路板上，这在制造业中是一个常见任务。

挑战：电路板的位置可能会变化，或者视线可能受到遮挡，机器人需要能够适应这些变化并成功完成任务。

SERL如何帮助：即使在电路板未固定或位置改变的情况下，SERL训练的机器人仍能准确地将电子元件安装到位。即使存在视线遮挡或其他干扰物，它也能调整自己的动作完成装配。

2、电缆布线：

情境：在很多机械和电子设备中，需要将电缆按照特定路径安装到位，这要求高度的精确度和适应性。

挑战：布线过程中可能会遇到干扰，如电缆被动地移动或夹子位置改变。

SERL如何帮助：SERL训练的机器人能够在持续的干扰下成功完成电缆的布线任务。即使夹子的位置与训练时不同，机器人也能泛化其学到的技能，适应新的情况并正确地布线。

3、物体重新定位：

情境：在仓库管理或零售业中，机器人可能需要将物品从一个地方移动到另一个地方，这要求机器人能够识别并搬运特定的物品。

挑战：目标物体周围可能有干扰物，或者需要搬运的物体与机器人之前训练使用的不同。

SERL如何帮助：即使在有多个干扰物存在的情况下，
SERL训练的机器人也能成功地识别并搬运目标物品。它甚至可以适应搬运之前未见过的物品，显示出良好的泛化能力。

工作原理：

由加州大学伯克利分校、华盛顿大学、斯坦福大学以及Intrinsic Innovation LLC的研究人员共同开发。该软件套件特别强调样本效率高的离策略算法、多样的奖励规定方法，以及针对流行机器人的高级控制器。

1、强化学习基础

在强化学习中，一个代理（机器人）通过与环境交互来学习如何执行任务。它在尝试不同行为时会收到奖励或惩罚，目标是学习一套策略，使得累积获得的奖励最大化。

2、样本高效学习

离策略算法：SERL利用所谓的“离策略”算法，这意味着机器人可以从过去的经验中学习，而不仅仅是最近的尝试。这使得每次交互的学习效果最大化，因为从历史数据中提取的知识可以被重复利用来改进决策过程。

高级模拟技术：通过高级模拟技术，SERL在虚拟环境中快速生成大量的训练场景，从而让机器人在面对现实世界的复杂性前，已经有了充分的准备。

3、多样的奖励规定方法

奖励设计：SERL提供了多种奖励规定方法，允许研究人员或开发人员根据特定任务的需求定制奖励结构。这种灵活性有助于精确指导机器人学习特定任务的最有效策略。
高级控制器和任务泛化

控制器：为了确保机器人可以在多种物理环境中有效操作，SERL集成了高级控制器，这些控制器能够处理从简单到复杂的动作序列，提高任务执行的准确性和效率。

泛化能力：SERL还特别注重提高机器人的泛化能力，即在面对新场景或任务变化时，能够适应并成功执行任务。这通过模拟训练中的“扰动”（如环境变化、视线遮挡等）来实现，训练机器人识别并适应这些变化。

4、快速训练周期

通过这些技术和方法的结合，SERL大大缩短了训练时间，同时保持了高成功率和鲁棒性，使机器人能够在短时间内学习完成复杂任务，并在现实世界中有效应用。

项目及演示：https://serl-robot.github.io/
论文：https://arxiv.org/abs/2401.16013
GitHub：https://github.com/rail-berkeley/serl</title>
            <link>https://nitter.cz/xiaohuggg/status/1752316028070326451#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752316028070326451#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 13:01:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SERL：机器人强化学习软件套件<br />
<br />
通常教机器人学习一个新动作需要很多次尝试和错误，就像一个人反复练习骑自行车一样。SERL的特别之处在于，它能让机器人通过更少的尝试就能快速学会新任务。<br />
<br />
它只需25到 50分钟就能学会一个新任务。<br />
<br />
而且能执行多种复杂任务，例如组装电路板、布线或者移动物体。<br />
<br />
主要特点：<br />
<br />
1、高效学习：通常教机器人学习一个新动作需要很多次尝试和错误，就像一个人反复练习骑自行车一样。SERL的特别之处在于，它能让机器人通过更少的尝试就能快速学会新任务，就像有了一个超级学习能力一样。<br />
<br />
2、适应性强：SERL能够让机器人在面对未知或变化的情况时也能够适应并完成任务。比如，如果机器人学会了在一个桌子上装配零件，即使后来桌子的位置改变了，它也能够调整自己的动作继续完成装配。<br />
<br />
3、多任务能力：这个软件工具包教会机器人执行多种任务，比如组装电路板、布线或者移动物体到新位置。机器人不仅能学会这些任务，还能在遇到干扰或环境变化时，灵活调整自己的行为。<br />
<br />
4、快速执行：通常，让机器人学习一个新任务需要很长时间。但是，使用SERL，机器人可以在大约半小时到一个小时之间就学会一个新任务，这比以往的方法要快得多。<br />
<br />
5、高成功率：使用SERL的机器人在完成任务时更加可靠，即使在复杂或不确定的环境中也能保持高成功率。<br />
<br />
应用案例：<br />
<br />
1、PCB电路组装：<br />
<br />
情境：机器人需要将电子元件精确地安装到电路板上，这在制造业中是一个常见任务。<br />
<br />
挑战：电路板的位置可能会变化，或者视线可能受到遮挡，机器人需要能够适应这些变化并成功完成任务。<br />
<br />
SERL如何帮助：即使在电路板未固定或位置改变的情况下，SERL训练的机器人仍能准确地将电子元件安装到位。即使存在视线遮挡或其他干扰物，它也能调整自己的动作完成装配。<br />
<br />
2、电缆布线：<br />
<br />
情境：在很多机械和电子设备中，需要将电缆按照特定路径安装到位，这要求高度的精确度和适应性。<br />
<br />
挑战：布线过程中可能会遇到干扰，如电缆被动地移动或夹子位置改变。<br />
<br />
SERL如何帮助：SERL训练的机器人能够在持续的干扰下成功完成电缆的布线任务。即使夹子的位置与训练时不同，机器人也能泛化其学到的技能，适应新的情况并正确地布线。<br />
<br />
3、物体重新定位：<br />
<br />
情境：在仓库管理或零售业中，机器人可能需要将物品从一个地方移动到另一个地方，这要求机器人能够识别并搬运特定的物品。<br />
<br />
挑战：目标物体周围可能有干扰物，或者需要搬运的物体与机器人之前训练使用的不同。<br />
<br />
SERL如何帮助：即使在有多个干扰物存在的情况下，<br />
SERL训练的机器人也能成功地识别并搬运目标物品。它甚至可以适应搬运之前未见过的物品，显示出良好的泛化能力。<br />
<br />
工作原理：<br />
<br />
由加州大学伯克利分校、华盛顿大学、斯坦福大学以及Intrinsic Innovation LLC的研究人员共同开发。该软件套件特别强调样本效率高的离策略算法、多样的奖励规定方法，以及针对流行机器人的高级控制器。<br />
<br />
1、强化学习基础<br />
<br />
在强化学习中，一个代理（机器人）通过与环境交互来学习如何执行任务。它在尝试不同行为时会收到奖励或惩罚，目标是学习一套策略，使得累积获得的奖励最大化。<br />
<br />
2、样本高效学习<br />
<br />
离策略算法：SERL利用所谓的“离策略”算法，这意味着机器人可以从过去的经验中学习，而不仅仅是最近的尝试。这使得每次交互的学习效果最大化，因为从历史数据中提取的知识可以被重复利用来改进决策过程。<br />
<br />
高级模拟技术：通过高级模拟技术，SERL在虚拟环境中快速生成大量的训练场景，从而让机器人在面对现实世界的复杂性前，已经有了充分的准备。<br />
<br />
3、多样的奖励规定方法<br />
<br />
奖励设计：SERL提供了多种奖励规定方法，允许研究人员或开发人员根据特定任务的需求定制奖励结构。这种灵活性有助于精确指导机器人学习特定任务的最有效策略。<br />
高级控制器和任务泛化<br />
<br />
控制器：为了确保机器人可以在多种物理环境中有效操作，SERL集成了高级控制器，这些控制器能够处理从简单到复杂的动作序列，提高任务执行的准确性和效率。<br />
<br />
泛化能力：SERL还特别注重提高机器人的泛化能力，即在面对新场景或任务变化时，能够适应并成功执行任务。这通过模拟训练中的“扰动”（如环境变化、视线遮挡等）来实现，训练机器人识别并适应这些变化。<br />
<br />
4、快速训练周期<br />
<br />
通过这些技术和方法的结合，SERL大大缩短了训练时间，同时保持了高成功率和鲁棒性，使机器人能够在短时间内学习完成复杂任务，并在现实世界中有效应用。<br />
<br />
项目及演示：<a href="https://serl-robot.github.io/">serl-robot.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2401.16013">arxiv.org/abs/2401.16013</a><br />
GitHub：<a href="https://github.com/rail-berkeley/serl">github.com/rail-berkeley/ser…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTIzMTM4OTI1MzYxOTMwMjQvcHUvaW1nL2ZKbnhJWVVrc1R0Rm9iVDYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752304318353703137#m</id>
            <title>WhisperFusion：与 AI 无缝语音对话（超低延迟）

WhisperFusion是一个基于 WhisperLive（把声音转文字） 和 WhisperSpeech（理解这些文字） 的能力构建。

可以让你和AI机器人无缝语音对话。

同时它还整合了Mistral模型，增强对转录文本上下文的理解。

使得它能更好地理解人说的每句话背后的意思。

WhisperFusion 的主要功能和特点：

1、实时语音转文本：它能够实时地将说话声音转换成文字，方便进行后续的处理和回应。

2、整合大语言模型：通过加入 Mistral 这样的大语言模型，WhisperFusion 能够更好地理解转换成文字的语音内容，提高回应的准确性和相关性。

3、性能优化：使用 TensorRT 技术对语言模型和 Whisper 进行了优化，确保了快速、高效的处理能力，特别是在实时语音转文本的应用中。

4、推理加速：利用 torch.compile 对 WhisperSpeech 进行优化，通过即时编译（JIT）PyTorch 代码，进一步加快了处理速度，减少了延迟。

5、易于使用：提供预构建的 Docker 容器，包含了所有必要的组件和模型，用户可以很容易地开始使用 WhisperFusion，体验其功能。

GitHub：https://github.com/collabora/WhisperFusion</title>
            <link>https://nitter.cz/xiaohuggg/status/1752304318353703137#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752304318353703137#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 12:14:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WhisperFusion：与 AI 无缝语音对话（超低延迟）<br />
<br />
WhisperFusion是一个基于 WhisperLive（把声音转文字） 和 WhisperSpeech（理解这些文字） 的能力构建。<br />
<br />
可以让你和AI机器人无缝语音对话。<br />
<br />
同时它还整合了Mistral模型，增强对转录文本上下文的理解。<br />
<br />
使得它能更好地理解人说的每句话背后的意思。<br />
<br />
WhisperFusion 的主要功能和特点：<br />
<br />
1、实时语音转文本：它能够实时地将说话声音转换成文字，方便进行后续的处理和回应。<br />
<br />
2、整合大语言模型：通过加入 Mistral 这样的大语言模型，WhisperFusion 能够更好地理解转换成文字的语音内容，提高回应的准确性和相关性。<br />
<br />
3、性能优化：使用 TensorRT 技术对语言模型和 Whisper 进行了优化，确保了快速、高效的处理能力，特别是在实时语音转文本的应用中。<br />
<br />
4、推理加速：利用 torch.compile 对 WhisperSpeech 进行优化，通过即时编译（JIT）PyTorch 代码，进一步加快了处理速度，减少了延迟。<br />
<br />
5、易于使用：提供预构建的 Docker 容器，包含了所有必要的组件和模型，用户可以很容易地开始使用 WhisperFusion，体验其功能。<br />
<br />
GitHub：<a href="https://github.com/collabora/WhisperFusion">github.com/collabora/Whisper…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTIzMDM5NzY0MDg4NzA5MTIvcHUvaW1nL3VNQTFQT3Y3aDVwdmVoLVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1752208438011060328#m</id>
            <title>RT by @xiaohuggg: 马斯克人机接口 Neuralink 相关视频（中英文字幕）：

想象一下，仅凭你的思维就能与亲人交流、上网浏览、甚至玩游戏的愉悦体验。
这一切，得益于在你大脑负责规划动作的区域植入一个既微小又不易察觉的装置。
这个设备能够解读你的神经活动，让你只需想象动作，就能操作电脑或智能手机，无需任何线缆或身体动作。</title>
            <link>https://nitter.cz/dotey/status/1752208438011060328#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1752208438011060328#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 05:53:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克人机接口 Neuralink 相关视频（中英文字幕）：<br />
<br />
想象一下，仅凭你的思维就能与亲人交流、上网浏览、甚至玩游戏的愉悦体验。<br />
这一切，得益于在你大脑负责规划动作的区域植入一个既微小又不易察觉的装置。<br />
这个设备能够解读你的神经活动，让你只需想象动作，就能操作电脑或智能手机，无需任何线缆或身体动作。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTIyMDgzNzA3NjM3NzYwMDAvcHUvaW1nL2M5bm4wVVJOYVBSQVpzbXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752240467251753258#m</id>
            <title>微软 Edge 浏览器被指窃取 Chrome 数据，会自动打开后者标签页

The Verge 编辑 @tomwarren 在安装 Windows 更新后发现，其平时不常使用的微软 Edge 浏览器自动打开了他在 Chrome 中浏览的标签页。

尽管他从未开启过 Edge 的 Chrome 数据导入设置，但更新后 Edge 强行获取了 Chrome 数据。

此外，其他用户也反映过类似问题。微软的一个导入功能可能存在漏洞，会在用户不知情的情况下自动导入其他浏览器数据。微软此前曾被指控诱导用户使用 Edge 浏览器。

详细：https://www.theverge.com/24054329/microsoft-edge-automatic-chrome-import-data-feature</title>
            <link>https://nitter.cz/xiaohuggg/status/1752240467251753258#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752240467251753258#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 08:00:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软 Edge 浏览器被指窃取 Chrome 数据，会自动打开后者标签页<br />
<br />
The Verge 编辑 <a href="https://nitter.cz/tomwarren" title="Tom Warren">@tomwarren</a> 在安装 Windows 更新后发现，其平时不常使用的微软 Edge 浏览器自动打开了他在 Chrome 中浏览的标签页。<br />
<br />
尽管他从未开启过 Edge 的 Chrome 数据导入设置，但更新后 Edge 强行获取了 Chrome 数据。<br />
<br />
此外，其他用户也反映过类似问题。微软的一个导入功能可能存在漏洞，会在用户不知情的情况下自动导入其他浏览器数据。微软此前曾被指控诱导用户使用 Edge 浏览器。<br />
<br />
详细：<a href="https://www.theverge.com/24054329/microsoft-edge-automatic-chrome-import-data-feature">theverge.com/24054329/micros…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZFMWFkeGFRQUEza29VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752178546506858510#m</id>
            <title>R to @xiaohuggg: Nijijourney V6 图像转视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1752178546506858510#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752178546506858510#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:54:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Nijijourney V6 图像转视频</p>
<p><a href="https://nitter.cz/ammaar/status/1752170543321924009#m">nitter.cz/ammaar/status/1752170543321924009#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752176033997152535#m</id>
            <title>马斯克宣布@Neuralink 公司已于1月28日完成了首例人类大脑芯片植入手术，目前被植入者恢复良好。

马斯克还宣布其脑机接口公司 Neuralink 的首款产品名为：Telepathy「心灵感应」。

该产品允许人们仅通过思考就能控制手机或电脑，进而控制几乎任何设备。最初的用户将是那些失去四肢使用能力的人。

马斯克称初步结果显示，植入设备在检测神经元尖峰活动方面表现出了有希望的结果。翻译下就是Neuralink的技术在监测大脑活动方面取得了初步的积极成果。</title>
            <link>https://nitter.cz/xiaohuggg/status/1752176033997152535#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752176033997152535#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:44:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克宣布<a href="https://nitter.cz/Neuralink" title="Neuralink">@Neuralink</a> 公司已于1月28日完成了首例人类大脑芯片植入手术，目前被植入者恢复良好。<br />
<br />
马斯克还宣布其脑机接口公司 Neuralink 的首款产品名为：Telepathy「心灵感应」。<br />
<br />
该产品允许人们仅通过思考就能控制手机或电脑，进而控制几乎任何设备。最初的用户将是那些失去四肢使用能力的人。<br />
<br />
马斯克称初步结果显示，植入设备在检测神经元尖峰活动方面表现出了有希望的结果。翻译下就是Neuralink的技术在监测大脑活动方面取得了初步的积极成果。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZENG1jMWJjQUFUU094LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752172486614299134#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1752172486614299134#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752172486614299134#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:30:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEM2xnYWJRQUFIMHZNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752172445510037775#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1752172445510037775#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752172445510037775#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:30:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEM2ZEUGFjQUFZS0o5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752172241348182522#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1752172241348182522#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752172241348182522#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:29:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEM1dFNGFRQUFiMTR1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752171978679898317#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1752171978679898317#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752171978679898317#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:28:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEMmNxZGJnQUExeVZLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752171976414945444#m</id>
            <title>R to @xiaohuggg: 一些案例：</title>
            <link>https://nitter.cz/xiaohuggg/status/1752171976414945444#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752171976414945444#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:28:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些案例：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEMlhaSGFNQUFBVnlFLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752171973453791450#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1752171973453791450#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752171973453791450#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:28:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEMnNRUGFnQUFlRFFXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEMl9uUmFjQUFPdmFkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1752171968550551771#m</id>
            <title>R to @xiaohuggg: 你可以在命令后加上后缀--niji 6，或者在niji的discord机器人的/settings里面启用Niji的V6版本</title>
            <link>https://nitter.cz/xiaohuggg/status/1752171968550551771#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1752171968550551771#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:28:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>你可以在命令后加上后缀--niji 6，或者在niji的discord机器人的/settings里面启用Niji的V6版本</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZEMmx6MmJzQUFOeDdzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>