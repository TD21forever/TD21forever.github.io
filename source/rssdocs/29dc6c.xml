<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1739208666967151076#m</id>
            <title>HAAR：通过文本描述生成逼真的3D人类发型。

HAAR允许你通过文本描述来指定你想要的发型。例如，你可以输入“长卷发”或“短直发”，HAAR将根据这些描述生成相应的3D发型。

HAAR使用3D发丝作为其基础表示形式，不仅在视觉上逼真，而且在结构上也非常接近真实的人类发型。

HAAR项目的创新之处在于其与当前AI生成模型的不同之处，具体体现在以下几个方面：

1、使用3D发丝作为基础表示：当前的AI生成模型通常依赖于2D视觉先验（即基于二维图像的信息）来重建3D内容。这意味着它们主要从平面图像中推断出三维结构。

HAAR使用3D发丝表示，能够精确地模拟复杂的发型结构，如发丝的层次、密度和排列方式。这对于传统的基于2D图像的重建方法来说是非常困难的，因为2D图像通常只能提供表面的视觉信息，而无法捕捉到发型的内部结构。

它能够精确地模拟真实世界中人类头发的细微结构和样式。这种3D发丝建模方法使得生成的发型不仅在视觉上逼真，而且在结构上也非常接近真实的人类发型。

2、生成高度遮挡的发型结构：一些发型，特别是那些复杂或高度遮挡的发型，对传统的3D重建方法来说是一个挑战，因为它们难以从2D图像中准确捕捉到所有的3D细节。

HAAR能够生成这些复杂的发型结构，包括那些在视觉上高度遮挡的部分，这在以往的技术中是难以实现的。

3、文本引导的生成方法：HAAR采用了文本引导的方法来生成发型。用户可以通过简单的文本描述来指导发型的生成，例如“长卷发”或“短直发”。

为了实现这一点，HAAR利用2D视觉问答系统来自动注释合成的发型模型，这些注释随后用于训练一个潜在的扩散模型。这个模型能够理解文本描述，并将其转化为具体的3D发型设计。

应用场景：

HAAR生成的发型可以使用现成的计算机图形技术进行渲染，适用于动画、游戏开发、虚拟现实等多种场景。

项目及演示：https://haar.is.tue.mpg.de/
论文：https://arxiv.org/abs/2312.11666
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1739208666967151076#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1739208666967151076#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 08:57:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HAAR：通过文本描述生成逼真的3D人类发型。<br />
<br />
HAAR允许你通过文本描述来指定你想要的发型。例如，你可以输入“长卷发”或“短直发”，HAAR将根据这些描述生成相应的3D发型。<br />
<br />
HAAR使用3D发丝作为其基础表示形式，不仅在视觉上逼真，而且在结构上也非常接近真实的人类发型。<br />
<br />
HAAR项目的创新之处在于其与当前AI生成模型的不同之处，具体体现在以下几个方面：<br />
<br />
1、使用3D发丝作为基础表示：当前的AI生成模型通常依赖于2D视觉先验（即基于二维图像的信息）来重建3D内容。这意味着它们主要从平面图像中推断出三维结构。<br />
<br />
HAAR使用3D发丝表示，能够精确地模拟复杂的发型结构，如发丝的层次、密度和排列方式。这对于传统的基于2D图像的重建方法来说是非常困难的，因为2D图像通常只能提供表面的视觉信息，而无法捕捉到发型的内部结构。<br />
<br />
它能够精确地模拟真实世界中人类头发的细微结构和样式。这种3D发丝建模方法使得生成的发型不仅在视觉上逼真，而且在结构上也非常接近真实的人类发型。<br />
<br />
2、生成高度遮挡的发型结构：一些发型，特别是那些复杂或高度遮挡的发型，对传统的3D重建方法来说是一个挑战，因为它们难以从2D图像中准确捕捉到所有的3D细节。<br />
<br />
HAAR能够生成这些复杂的发型结构，包括那些在视觉上高度遮挡的部分，这在以往的技术中是难以实现的。<br />
<br />
3、文本引导的生成方法：HAAR采用了文本引导的方法来生成发型。用户可以通过简单的文本描述来指导发型的生成，例如“长卷发”或“短直发”。<br />
<br />
为了实现这一点，HAAR利用2D视觉问答系统来自动注释合成的发型模型，这些注释随后用于训练一个潜在的扩散模型。这个模型能够理解文本描述，并将其转化为具体的3D发型设计。<br />
<br />
应用场景：<br />
<br />
HAAR生成的发型可以使用现成的计算机图形技术进行渲染，适用于动画、游戏开发、虚拟现实等多种场景。<br />
<br />
项目及演示：<a href="https://haar.is.tue.mpg.de/">haar.is.tue.mpg.de/</a><br />
论文：<a href="https://arxiv.org/abs/2312.11666">arxiv.org/abs/2312.11666</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzkxODA4NDI3OTMyMTM5NTIvcHUvaW1nL05JVm96U0poS2JreUp0NVIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1739178877153681846#m</id>
            <title>Clone-Voice：带有 WebUI 简单易操作的声音克隆工具

- 基于Coqui AI的TTS模型开发，可以把一个声音变成另一个声音

- 支持多种语言：包括中文、英文、日文、韩文、法文等16种语言。

- 简单易用：可以通过Web界面轻松操作，鼠标点点就行。

- 无需强大的电脑配置，没有N卡GPU也可以使用。

- 支持在线从麦克风录制声音克隆，录音时长建议在5秒到20秒之间。

GitHub：https://github.com/jianchang512/clone-voice</title>
            <link>https://nitter.cz/xiaohuggg/status/1739178877153681846#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1739178877153681846#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 06:58:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Clone-Voice：带有 WebUI 简单易操作的声音克隆工具<br />
<br />
- 基于Coqui AI的TTS模型开发，可以把一个声音变成另一个声音<br />
<br />
- 支持多种语言：包括中文、英文、日文、韩文、法文等16种语言。<br />
<br />
- 简单易用：可以通过Web界面轻松操作，鼠标点点就行。<br />
<br />
- 无需强大的电脑配置，没有N卡GPU也可以使用。<br />
<br />
- 支持在线从麦克风录制声音克隆，录音时长建议在5秒到20秒之间。<br />
<br />
GitHub：<a href="https://github.com/jianchang512/clone-voice">github.com/jianchang512/clon…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzkxNjQ3MTg4MDIzNDU5ODUvcHUvaW1nLzhqb2NycnBBR1FsbHo3TkQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1739160631620816904#m</id>
            <title>一款由AI开发的AI游戏

游戏的背景设定在一个The Nexus的虚拟空间，人类与AI爆发了冲突，你作为人类战士，你需要假扮AI，潜入这个由AI控制的空间，盗取名为ZetaMaster核心代码来拯救人类。

该游戏基于AI-Town平台，角色设定和对话完全由GPT4 生成，视觉音效由Dalle-3、Midjourney和Stable Audio生成。

详细介绍：http://ramondario.com/thus-spoke-zaranova.html

游戏入口：https://zaranova.xyz/（需要英语好的才行，嘿嘿）</title>
            <link>https://nitter.cz/xiaohuggg/status/1739160631620816904#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1739160631620816904#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 05:46:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一款由AI开发的AI游戏<br />
<br />
游戏的背景设定在一个The Nexus的虚拟空间，人类与AI爆发了冲突，你作为人类战士，你需要假扮AI，潜入这个由AI控制的空间，盗取名为ZetaMaster核心代码来拯救人类。<br />
<br />
该游戏基于AI-Town平台，角色设定和对话完全由GPT4 生成，视觉音效由Dalle-3、Midjourney和Stable Audio生成。<br />
<br />
详细介绍：<a href="http://ramondario.com/thus-spoke-zaranova.html">ramondario.com/thus-spoke-za…</a><br />
<br />
游戏入口：<a href="https://zaranova.xyz/">zaranova.xyz/</a>（需要英语好的才行，嘿嘿）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzkxNjA1NTM4NDE2ODQ0ODAvcHUvaW1nL3N2cHo2ejA0Zmo5bDJRM0ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1739129039850492275#m</id>
            <title>AnyDoor ：可以将任何对象巧妙的放入到新的图像、视频场景中

现在已经放出了在线演示，大家可以体验：https://huggingface.co/spaces/xichenhku/AnyDoor-online

简单来说，就是任意门，隔空传送，将一个图像里面的物体、对象传送到另一个图像，当然也可以是多个！😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1739129039850492275#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1739129039850492275#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 03:40:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnyDoor ：可以将任何对象巧妙的放入到新的图像、视频场景中<br />
<br />
现在已经放出了在线演示，大家可以体验：<a href="https://huggingface.co/spaces/xichenhku/AnyDoor-online">huggingface.co/spaces/xichen…</a><br />
<br />
简单来说，就是任意门，隔空传送，将一个图像里面的物体、对象传送到另一个图像，当然也可以是多个！😄</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1682721078718906368#m">nitter.cz/xiaohuggg/status/1682721078718906368#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczODc2ODEzMzgyODI3MjEyOC9Bcm5XanlwWT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1739114693023920260#m</id>
            <title>Tracking Any Object Amodally ：以非直观方式追踪任何对象

特别是在物体部分遮挡或不完全可见的情况下，也能理解其完整结构。

该项目能由卡内基梅隆大学和丰田研究所开发，让AI能像人一样，即使在只能看到物体一部分的情况下，也能识别并追踪这个物体的整体。

比如，当我们看到一辆部分被墙遮挡的车时，即使只看到一半，我们也能知道那是一辆车，并且脑海中会自动想象出车的全貌。

这个项目就是要让计算机也能做到这一点。在物体被遮挡的情况下，计算机仍能“理解”并追踪到这个物体的完整形状和位置。

这对于自动驾驶车辆来说特别重要。想象一下，如果自动驾驶车辆能像人一样，即使在复杂的环境中也能准确识别和追踪部分被遮挡的行人或其他车辆，那么它的驾驶就会更安全、更可靠。

这个项目能大幅提升计算机视觉系统的智能，让它们在处理遮挡物体时更加像人类，从而在自动驾驶、视频监控等领域发挥更大的作用。

TAO-Amodal数据集：

为提高物体追踪技术他们专门设计了一个数据集：TAO-Amodal

这个数据集包含了大量的视频序列，其中包括各种被遮挡或部分可见的物体，以及详细标注信息。

它可以让AI能够更好地理解和追踪那些我们只能看到一部分的物体。

- 数据集规模：包含880种多样的类别，覆盖数千个视频序列。

- 注释类型：数据集包括完全不可见、部分出框和被遮挡物体的 amodal（非模态）和 modal（模态）边界框。

- 重点：TAO-Amodal 数据集的重点在于评估当前追踪器在遮挡推理方面的能力，通过追踪任何物体（Tracking Any Object）的 Amodal 感知（Amodal perception）来实现。

Amodal Expander 插件

项目还开发了一个轻量级插件模块“Amodal Expander 插件”，这个是一个轻量级的插件模块，用于改进物体追踪器的功能。

Modal 追踪器：这种追踪器主要关注物体的可见部分。它在物体完全可见时表现良好，但如果物体被遮挡或部分不可见，它可能无法准确追踪。

Amodal 追踪器：与Modal追踪器不同，Amodal 追踪器能够理解和推断物体的完整形状，即使物体部分被遮挡或不完全可见。这意味着即使在复杂的视觉环境中，Amodal 追踪器也能更准确地追踪物体。

Amodal Expander插件的作用是将标准的 Modal 追踪器转换为 Amodal 追踪器。这是通过在少量视频序列上对追踪器进行微调实现的。微调后的追踪器能够更好地处理部分遮挡或不完全可见的物体，从而在追踪这些物体时更为有效和准确。

在TAO-Amodal数据集上的测试结果显示，该技术在检测和追踪被遮挡物体方面取得了3.3%和1.6%的改进。特别是在追踪人物方面，与现有的模态追踪技术相比，性能提高了2倍。 

项目地址：https://tao-amodal.github.io/
论文：https://arxiv.org/abs/2312.12433
GitHub：https://github.com/WesleyHsieh0806/TAO-Amodal</title>
            <link>https://nitter.cz/xiaohuggg/status/1739114693023920260#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1739114693023920260#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Dec 2023 02:43:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Tracking Any Object Amodally ：以非直观方式追踪任何对象<br />
<br />
特别是在物体部分遮挡或不完全可见的情况下，也能理解其完整结构。<br />
<br />
该项目能由卡内基梅隆大学和丰田研究所开发，让AI能像人一样，即使在只能看到物体一部分的情况下，也能识别并追踪这个物体的整体。<br />
<br />
比如，当我们看到一辆部分被墙遮挡的车时，即使只看到一半，我们也能知道那是一辆车，并且脑海中会自动想象出车的全貌。<br />
<br />
这个项目就是要让计算机也能做到这一点。在物体被遮挡的情况下，计算机仍能“理解”并追踪到这个物体的完整形状和位置。<br />
<br />
这对于自动驾驶车辆来说特别重要。想象一下，如果自动驾驶车辆能像人一样，即使在复杂的环境中也能准确识别和追踪部分被遮挡的行人或其他车辆，那么它的驾驶就会更安全、更可靠。<br />
<br />
这个项目能大幅提升计算机视觉系统的智能，让它们在处理遮挡物体时更加像人类，从而在自动驾驶、视频监控等领域发挥更大的作用。<br />
<br />
TAO-Amodal数据集：<br />
<br />
为提高物体追踪技术他们专门设计了一个数据集：TAO-Amodal<br />
<br />
这个数据集包含了大量的视频序列，其中包括各种被遮挡或部分可见的物体，以及详细标注信息。<br />
<br />
它可以让AI能够更好地理解和追踪那些我们只能看到一部分的物体。<br />
<br />
- 数据集规模：包含880种多样的类别，覆盖数千个视频序列。<br />
<br />
- 注释类型：数据集包括完全不可见、部分出框和被遮挡物体的 amodal（非模态）和 modal（模态）边界框。<br />
<br />
- 重点：TAO-Amodal 数据集的重点在于评估当前追踪器在遮挡推理方面的能力，通过追踪任何物体（Tracking Any Object）的 Amodal 感知（Amodal perception）来实现。<br />
<br />
Amodal Expander 插件<br />
<br />
项目还开发了一个轻量级插件模块“Amodal Expander 插件”，这个是一个轻量级的插件模块，用于改进物体追踪器的功能。<br />
<br />
Modal 追踪器：这种追踪器主要关注物体的可见部分。它在物体完全可见时表现良好，但如果物体被遮挡或部分不可见，它可能无法准确追踪。<br />
<br />
Amodal 追踪器：与Modal追踪器不同，Amodal 追踪器能够理解和推断物体的完整形状，即使物体部分被遮挡或不完全可见。这意味着即使在复杂的视觉环境中，Amodal 追踪器也能更准确地追踪物体。<br />
<br />
Amodal Expander插件的作用是将标准的 Modal 追踪器转换为 Amodal 追踪器。这是通过在少量视频序列上对追踪器进行微调实现的。微调后的追踪器能够更好地处理部分遮挡或不完全可见的物体，从而在追踪这些物体时更为有效和准确。<br />
<br />
在TAO-Amodal数据集上的测试结果显示，该技术在检测和追踪被遮挡物体方面取得了3.3%和1.6%的改进。特别是在追踪人物方面，与现有的模态追踪技术相比，性能提高了2倍。 <br />
<br />
项目地址：<a href="https://tao-amodal.github.io/">tao-amodal.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.12433">arxiv.org/abs/2312.12433</a><br />
GitHub：<a href="https://github.com/WesleyHsieh0806/TAO-Amodal">github.com/WesleyHsieh0806/T…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg4OTExNDMwNTA1Nzk5NjgvcHUvaW1nL2pIUWNZQTdyekpLYXdIOFMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738881963384356883#m</id>
            <title>Fairy：通过自然语言指令就能对视频进行编辑

该项目由Meta GenAI开发，你可以用简单的文字描述就对能视频进行风格转换（如将视频转换为梵高、漫画风格）、物体或角色变换（将视频中的物体或角色转换成其他形式）等编辑。

而且速度贼快，它能在14秒内生成120帧的512×384视频（4秒时长，30 FPS）。

Fairy主要功能特点：

1、风格转换：Fairy能够将视频中的图像风格转换为不同的艺术风格，例如梵高或毕加索风格。这种转换不仅改变颜色和纹理，还保持视频的原始内容和结构。

2、物体或角色变换：Fairy可以将视频中的特定物体或角色转换成其他形式，例如将人物转换成木雕或金属骑士雕塑。这种编辑涉及到复杂的形状和纹理变化。

3、长视频处理：由于Fairy的高效性和内存管理优势，它能够处理相对较长的视频，而不会遇到内存问题。

4、细节保留的编辑：在进行风格转换或物体变换时，Fairy能够保留视频中的重要细节，确保编辑后的视频仍然保持高质量和真实感。

5、时间连贯性：Fairy特别注重在编辑过程中保持视频的时间连贯性，确保从一帧到下一帧的过渡自然和流畅。

6、快速处理视频：Fairy不仅解决了以前模型的内存和处理速度限制，还通过独特的数据增强策略改善了时间一致性。Fairy能够在14秒内生成120帧的512×384视频（4秒时长，30 FPS），比之前的工作快至少44倍。

技术细节：

Fairy是一个基于图像编辑扩散模型的视频编辑工具，专为视频编辑应用而设计。它采用了一种基于锚点的跨帧注意力机制，这种机制能够在视频帧之间隐式地传播扩散特征，从而确保时间上的连贯性和高保真度的视频合成。

1、基于锚点的跨帧注意力机制：

锚点选择：Fairy从视频中选择关键帧作为“锚点”。这些锚点帧帮助维持视频的视觉一致性和时间连贯性。

跨帧注意力：Fairy利用跨帧注意力机制来分析和链接这些锚点帧，从而在整个视频中传播关键视觉特征。

2、扩散模型特征的应用：

扩散过程：Fairy使用基于扩散模型的方法来生成或编辑视频帧。这种方法通过逐步添加和去除噪声来改变图像，适用于复杂的图像和视频编辑任务。

3、数据增强策略：

仿射变换适应性：为了提高模型对不同视角和变换的适应性，Fairy采用了特殊的数据增强策略，使模型能够更好地处理视频中的自然运动和变换。

4、高效并行处理：多GPU支持：Fairy的设计支持并行计算，使其能够在多个GPU上同时处理视频，从而显著提高处理速度。

项目地址：https://fairy-video2video.github.io/
论文：https://arxiv.org/pdf/2312.13834.pdf
GitHub：coming soon...
更多演示视频：https://fairy-video2video.github.io/supp/index.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1738881963384356883#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738881963384356883#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 11:19:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Fairy：通过自然语言指令就能对视频进行编辑<br />
<br />
该项目由Meta GenAI开发，你可以用简单的文字描述就对能视频进行风格转换（如将视频转换为梵高、漫画风格）、物体或角色变换（将视频中的物体或角色转换成其他形式）等编辑。<br />
<br />
而且速度贼快，它能在14秒内生成120帧的512×384视频（4秒时长，30 FPS）。<br />
<br />
Fairy主要功能特点：<br />
<br />
1、风格转换：Fairy能够将视频中的图像风格转换为不同的艺术风格，例如梵高或毕加索风格。这种转换不仅改变颜色和纹理，还保持视频的原始内容和结构。<br />
<br />
2、物体或角色变换：Fairy可以将视频中的特定物体或角色转换成其他形式，例如将人物转换成木雕或金属骑士雕塑。这种编辑涉及到复杂的形状和纹理变化。<br />
<br />
3、长视频处理：由于Fairy的高效性和内存管理优势，它能够处理相对较长的视频，而不会遇到内存问题。<br />
<br />
4、细节保留的编辑：在进行风格转换或物体变换时，Fairy能够保留视频中的重要细节，确保编辑后的视频仍然保持高质量和真实感。<br />
<br />
5、时间连贯性：Fairy特别注重在编辑过程中保持视频的时间连贯性，确保从一帧到下一帧的过渡自然和流畅。<br />
<br />
6、快速处理视频：Fairy不仅解决了以前模型的内存和处理速度限制，还通过独特的数据增强策略改善了时间一致性。Fairy能够在14秒内生成120帧的512×384视频（4秒时长，30 FPS），比之前的工作快至少44倍。<br />
<br />
技术细节：<br />
<br />
Fairy是一个基于图像编辑扩散模型的视频编辑工具，专为视频编辑应用而设计。它采用了一种基于锚点的跨帧注意力机制，这种机制能够在视频帧之间隐式地传播扩散特征，从而确保时间上的连贯性和高保真度的视频合成。<br />
<br />
1、基于锚点的跨帧注意力机制：<br />
<br />
锚点选择：Fairy从视频中选择关键帧作为“锚点”。这些锚点帧帮助维持视频的视觉一致性和时间连贯性。<br />
<br />
跨帧注意力：Fairy利用跨帧注意力机制来分析和链接这些锚点帧，从而在整个视频中传播关键视觉特征。<br />
<br />
2、扩散模型特征的应用：<br />
<br />
扩散过程：Fairy使用基于扩散模型的方法来生成或编辑视频帧。这种方法通过逐步添加和去除噪声来改变图像，适用于复杂的图像和视频编辑任务。<br />
<br />
3、数据增强策略：<br />
<br />
仿射变换适应性：为了提高模型对不同视角和变换的适应性，Fairy采用了特殊的数据增强策略，使模型能够更好地处理视频中的自然运动和变换。<br />
<br />
4、高效并行处理：多GPU支持：Fairy的设计支持并行计算，使其能够在多个GPU上同时处理视频，从而显著提高处理速度。<br />
<br />
项目地址：<a href="https://fairy-video2video.github.io/">fairy-video2video.github.io/</a><br />
论文：<a href="https://arxiv.org/pdf/2312.13834.pdf">arxiv.org/pdf/2312.13834.pdf</a><br />
GitHub：coming soon...<br />
更多演示视频：<a href="https://fairy-video2video.github.io/supp/index.html">fairy-video2video.github.io/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg1NzAxNzI0NDM3OTU0NTYvcHUvaW1nLzA4ZFZlOXZXR0JiRXZpbEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738852465494433900#m</id>
            <title>Merry Christmas 🎄

🧑‍🎄</title>
            <link>https://nitter.cz/xiaohuggg/status/1738852465494433900#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738852465494433900#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 09:21:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Merry Christmas 🎄<br />
<br />
🧑‍🎄</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM4ODUyMzQwOTg2NDU4MTEyL2ltZy9sU2ZEZEJ1c0I1WWtkZ1RyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738812289015484630#m</id>
            <title>兄弟们

全球首位由AI生成的 AV女优的片子出了

经过本人鉴定就是个换脸玩意🤣

和AI根本不搭噶，截取了前5分钟展示你们看看😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1738812289015484630#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738812289015484630#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 06:42:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们<br />
<br />
全球首位由AI生成的 AV女优的片子出了<br />
<br />
经过本人鉴定就是个换脸玩意🤣<br />
<br />
和AI根本不搭噶，截取了前5分钟展示你们看看😅</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729099853698093128#m">nitter.cz/xiaohuggg/status/1729099853698093128#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg4MTEzMTcyNTgyNDAwMDEvcHUvaW1nL3FHTlBfdEJiSVJ4bDVjS0UuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738808040726688009#m</id>
            <title>MidJourney V6 几乎可以复制任何动画风格！

建议你们点开主题帖子看看，确实是很强

但是真不知道MidJourney回头怎么解决版权问题

在这么下去估计迟早要被告！</title>
            <link>https://nitter.cz/xiaohuggg/status/1738808040726688009#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738808040726688009#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 06:25:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MidJourney V6 几乎可以复制任何动画风格！<br />
<br />
建议你们点开主题帖子看看，确实是很强<br />
<br />
但是真不知道MidJourney回头怎么解决版权问题<br />
<br />
在这么下去估计迟早要被告！</p>
<p><a href="https://nitter.cz/ProperPrompter/status/1738542425503907891#m">nitter.cz/ProperPrompter/status/1738542425503907891#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738801598204367007#m</id>
            <title>Beeper/imessage：一个 Matrix-iMessage 桥接项目

将 Apple 的 iMessage 服务与 Matrix 协议进行桥接，从而在不同的平台和设备上使用 iMessage。

支持实时聊天，用户可以无缝地在 Matrix 和 iMessage 之间进行通信。

https://github.com/beeper/imessage</title>
            <link>https://nitter.cz/xiaohuggg/status/1738801598204367007#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738801598204367007#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 05:59:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Beeper/imessage：一个 Matrix-iMessage 桥接项目<br />
<br />
将 Apple 的 iMessage 服务与 Matrix 协议进行桥接，从而在不同的平台和设备上使用 iMessage。<br />
<br />
支持实时聊天，用户可以无缝地在 Matrix 和 iMessage 之间进行通信。<br />
<br />
<a href="https://github.com/beeper/imessage">github.com/beeper/imessage</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNzg2NzU3MjI3MDIzNTY0OC9Hbnh4dmlfZT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738768758943191465#m</id>
            <title>R to @xiaohuggg: 歌曲：圣诞要吃饺</title>
            <link>https://nitter.cz/xiaohuggg/status/1738768758943191465#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738768758943191465#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 03:49:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>歌曲：圣诞要吃饺</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg3Njg2OTcwNDM1OTkzNjAvcHUvaW1nL3BIR3ZfMWljVzYxZ3dkY04uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738757087696707971#m</id>
            <title>R to @xiaohuggg: 上面视频演示所生成的音乐</title>
            <link>https://nitter.cz/xiaohuggg/status/1738757087696707971#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738757087696707971#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 03:02:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>上面视频演示所生成的音乐</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg3NTYwODAzOTY3OTU5MDQvcHUvaW1nLzE0NlNfam1Vd1B5dnY0c1IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738757086006386879#m</id>
            <title>Suno AI推出圣诞特别版🎄🎁

每人送50个圣诞积分，可以免费生成圣诞音乐歌曲

只需要描述你喜欢的风格或者想要的氛围啊、心情啊，随便什么都可以生成圣诞风格的音乐。

有通用模式（傻瓜模式）和自定义模式（自己设定歌词曲风），自己根据自己能力来选择。

很简单！🎅🏻

体验地址：https://app.suno.ai/create/holiday/</title>
            <link>https://nitter.cz/xiaohuggg/status/1738757086006386879#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738757086006386879#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 03:02:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Suno AI推出圣诞特别版🎄🎁<br />
<br />
每人送50个圣诞积分，可以免费生成圣诞音乐歌曲<br />
<br />
只需要描述你喜欢的风格或者想要的氛围啊、心情啊，随便什么都可以生成圣诞风格的音乐。<br />
<br />
有通用模式（傻瓜模式）和自定义模式（自己设定歌词曲风），自己根据自己能力来选择。<br />
<br />
很简单！🎅🏻<br />
<br />
体验地址：<a href="https://app.suno.ai/create/holiday/">app.suno.ai/create/holiday/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg3NTYwNDcxMzYwMzA3MjAvcHUvaW1nL3dHcHZBdko5d0o0TzhXTFIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738754784826679447#m</id>
            <title>所以说Midjourney V6是支持3D图像生成的

但是没有宣传</title>
            <link>https://nitter.cz/xiaohuggg/status/1738754784826679447#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738754784826679447#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 02:53:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以说Midjourney V6是支持3D图像生成的<br />
<br />
但是没有宣传</p>
<p><a href="https://nitter.cz/op7418/status/1738614091349172544#m">nitter.cz/op7418/status/1738614091349172544#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738748270153957567#m</id>
            <title>把http://arXiv.org的论文链接改为http://Talk2arXiv.org 

即可和论文进行聊天对话

比如将：http://arxiv.org/pdf/2312.11514.pdf

修改为：http://talk2arxiv.org/pdf/2312.11514.pdf

即可和该论文进行直接聊天对话，不过需要你的OpenAI API...

跑了下，感觉是个测试版，没针对论文做优化，只能对话，无法定位到论文具体位置！😀

有大佬可以改进下吗？</title>
            <link>https://nitter.cz/xiaohuggg/status/1738748270153957567#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738748270153957567#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 02:27:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>把<a href="http://arXiv.org">arXiv.org</a>的论文链接改为<a href="http://Talk2arXiv.org">Talk2arXiv.org</a> <br />
<br />
即可和论文进行聊天对话<br />
<br />
比如将：<a href="http://arxiv.org/pdf/2312.11514.pdf">arxiv.org/pdf/2312.11514.pdf</a><br />
<br />
修改为：<a href="http://talk2arxiv.org/pdf/2312.11514.pdf">talk2arxiv.org/pdf/2312.1151…</a><br />
<br />
即可和该论文进行直接聊天对话，不过需要你的OpenAI API...<br />
<br />
跑了下，感觉是个测试版，没针对论文做优化，只能对话，无法定位到论文具体位置！😀<br />
<br />
有大佬可以改进下吗？</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg3NDU3NDkyMTc5MTg5NzcvcHUvaW1nLzVTYU9sdERIUzB6dWRqa2suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738746944737128452#m</id>
            <title>苹果发布了一个多模态大模型，但是很多人似乎没有注意？？？

苹果12月14日释放了一个名为Ferret的多模态大语言模型，该模型不仅可以准确识别图像并描述其内容。

同时它还能够识别和定位图像中的各种元素，无论你用怎样的方式描述图像内容，Ferret都能准确地在图像中找到并识别出来。

Ferret拥有 (7B, 13B)两个版本，为了增强 Ferret 模型的能力苹果特别收集了一个GRIT 数据集。它包含了1.1M个样本，这些样本包含了丰富的层次空间知识。

主要功能和特点：

Ferret能够理解和处理图像与文本之间的复杂关系。这个模型的特别之处在于它能够识别和定位图像中的各种元素，无论这些元素是什么形状或大小。

比如在对话中引用图像的特定部分，或者根据文本描述在图像中找到特定物体。

Ferret 就像是一个能够理解图片和文字并将它们联系起来的智能系统。无论你在文本中提到图像的哪个部分，或者用怎样的方式描述，Ferret 都能准确地在图像中找到并识别出来。

1、多模态理解：Ferret 能够同时处理和理解图像（视觉信息）和文本（语言信息），这使得它能够在多种不同的模式之间建立联系。

2、空间指代理解：它能够识别和理解图像中特定区域的含义，即使这些区域的形状和大小各不相同。例如，如果文本提到图像中的某个特定部分，Ferret 能够识别出这部分是指什么。

3、理解复杂的文本描述：Ferret 能够理解各种类型的文本描述，无论这些描述是具体的还是抽象的。比如，“图像中红色车辆旁边的小狗”或“画面右上角的笑脸”。

4、开放词汇描述精准定位：根据这些文本描述，Ferret 能够在提供的图像中准确地找到并标记出相应的物体或区域。例如，它可以识别并指出图像中的“小狗”或“笑脸”的确切位置。无论用户如何描述他们想要找到的图像中的内容，Ferret 都能理解并响应。

5、混合区域表示：Ferret 使用一种创新的表示方法来处理图像中的区域。这种表示结合了离散坐标（如点或边界框的位置）和连续特征（如区域的视觉内容）。这允许模型理解和处理各种形状和大小的区域，从而提高了对图像的空间理解能力。

6、空间感知的视觉采样器：为了处理不同形状的区域，Ferret 引入了一个空间感知的视觉采样器。这个采样器能够根据区域的形状和稀疏性提取视觉特征，使模型能够处理从简单点到复杂多边形等各种形状的区域。

7、多样的区域输入：Ferret具有识别和理解图像中各种不同类型区域的能力。

它可以处理以下类型的区域输入：

点：Ferret 能够识别图像中的特定点，例如用户指定的一个具体位置。

边界框：它可以识别和理解图像中的边界框，这些边界框通常用来标记图像中的物体或特定区域。

自由形状：Ferret 还能处理更复杂的自由形状，比如手绘的轮廓、不规则图形或任意多边形。这种能力使得它可以更精确地识别和理解图像中的复杂区域。

这种处理多样区域输入的能力使得 Ferret 在图像理解方面非常灵活和强大，能够适应各种不同的应用场景和用户需求。无论用户提供的是简单的点标记、常规的边界框，还是复杂的自由形状，Ferret 都能准确地识别和处理。

8、GRIT 数据集：GRIT 数据集是专门为了训练和增强 Ferret 而收集的，包含了1.1M个样本。

 这个数据集包含了丰富的层次空间知识，这意味着它涵盖了从简单物体到复杂空间关系的各种信息。 包含95K难负样本，这些是特别设计的挑战性样本，用于提高模型在处理困难情况下的鲁棒性和准确性。

主要表现：

1、Ferret-Bench评估：Ferret-Bench是为了评估Ferret而引入的一系列新任务，包括指称描述、指称推理和对话中的定位。在这些任务上，Ferret相比现有的最佳多模态大型语言模型（MLLM）平均提高了20.4%。这一结果表明Ferret在处理更复杂、更接近真实世界应用的任务时具有显著的优势。

2、改善对象幻觉：Ferret 在描述图像细节时能够减少错误或虚构的内容，这在自动图像描述和分析领域尤为重要。
它减轻了对象幻觉的问题，即在生成文本描述时减少了对不存在的对象的错误引用，提高了描述的准确性和可靠性。

3、Ferret 不仅在传统的指代和定位任务中表现优异，它能够更准确地理解和处理图像中的空间信息和语义。而且在需要指代/定位、语义、知识和推理的任务中也表现出色。 

Ferret 能够更准确地描述图像细节，减少在生成文本时对不存在的对象的幻觉。 通过其创新的方法和技术，为多模态语言模型在空间理解和定位方面提供了新的可能性，特别是在处理复杂的图像和文本交互时。

适用于多种应用场景：

由于其强大的图像和文本处理能力，Ferret 适用于多种应用场景，包括图像搜索、自动图像标注、交互式媒体探索等。

GitHub：https://github.com/apple/ml-ferret
论文：https://arxiv.org/abs/2310.07704</title>
            <link>https://nitter.cz/xiaohuggg/status/1738746944737128452#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738746944737128452#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 02:22:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>苹果发布了一个多模态大模型，但是很多人似乎没有注意？？？<br />
<br />
苹果12月14日释放了一个名为Ferret的多模态大语言模型，该模型不仅可以准确识别图像并描述其内容。<br />
<br />
同时它还能够识别和定位图像中的各种元素，无论你用怎样的方式描述图像内容，Ferret都能准确地在图像中找到并识别出来。<br />
<br />
Ferret拥有 (7B, 13B)两个版本，为了增强 Ferret 模型的能力苹果特别收集了一个GRIT 数据集。它包含了1.1M个样本，这些样本包含了丰富的层次空间知识。<br />
<br />
主要功能和特点：<br />
<br />
Ferret能够理解和处理图像与文本之间的复杂关系。这个模型的特别之处在于它能够识别和定位图像中的各种元素，无论这些元素是什么形状或大小。<br />
<br />
比如在对话中引用图像的特定部分，或者根据文本描述在图像中找到特定物体。<br />
<br />
Ferret 就像是一个能够理解图片和文字并将它们联系起来的智能系统。无论你在文本中提到图像的哪个部分，或者用怎样的方式描述，Ferret 都能准确地在图像中找到并识别出来。<br />
<br />
1、多模态理解：Ferret 能够同时处理和理解图像（视觉信息）和文本（语言信息），这使得它能够在多种不同的模式之间建立联系。<br />
<br />
2、空间指代理解：它能够识别和理解图像中特定区域的含义，即使这些区域的形状和大小各不相同。例如，如果文本提到图像中的某个特定部分，Ferret 能够识别出这部分是指什么。<br />
<br />
3、理解复杂的文本描述：Ferret 能够理解各种类型的文本描述，无论这些描述是具体的还是抽象的。比如，“图像中红色车辆旁边的小狗”或“画面右上角的笑脸”。<br />
<br />
4、开放词汇描述精准定位：根据这些文本描述，Ferret 能够在提供的图像中准确地找到并标记出相应的物体或区域。例如，它可以识别并指出图像中的“小狗”或“笑脸”的确切位置。无论用户如何描述他们想要找到的图像中的内容，Ferret 都能理解并响应。<br />
<br />
5、混合区域表示：Ferret 使用一种创新的表示方法来处理图像中的区域。这种表示结合了离散坐标（如点或边界框的位置）和连续特征（如区域的视觉内容）。这允许模型理解和处理各种形状和大小的区域，从而提高了对图像的空间理解能力。<br />
<br />
6、空间感知的视觉采样器：为了处理不同形状的区域，Ferret 引入了一个空间感知的视觉采样器。这个采样器能够根据区域的形状和稀疏性提取视觉特征，使模型能够处理从简单点到复杂多边形等各种形状的区域。<br />
<br />
7、多样的区域输入：Ferret具有识别和理解图像中各种不同类型区域的能力。<br />
<br />
它可以处理以下类型的区域输入：<br />
<br />
点：Ferret 能够识别图像中的特定点，例如用户指定的一个具体位置。<br />
<br />
边界框：它可以识别和理解图像中的边界框，这些边界框通常用来标记图像中的物体或特定区域。<br />
<br />
自由形状：Ferret 还能处理更复杂的自由形状，比如手绘的轮廓、不规则图形或任意多边形。这种能力使得它可以更精确地识别和理解图像中的复杂区域。<br />
<br />
这种处理多样区域输入的能力使得 Ferret 在图像理解方面非常灵活和强大，能够适应各种不同的应用场景和用户需求。无论用户提供的是简单的点标记、常规的边界框，还是复杂的自由形状，Ferret 都能准确地识别和处理。<br />
<br />
8、GRIT 数据集：GRIT 数据集是专门为了训练和增强 Ferret 而收集的，包含了1.1M个样本。<br />
<br />
 这个数据集包含了丰富的层次空间知识，这意味着它涵盖了从简单物体到复杂空间关系的各种信息。 包含95K难负样本，这些是特别设计的挑战性样本，用于提高模型在处理困难情况下的鲁棒性和准确性。<br />
<br />
主要表现：<br />
<br />
1、Ferret-Bench评估：Ferret-Bench是为了评估Ferret而引入的一系列新任务，包括指称描述、指称推理和对话中的定位。在这些任务上，Ferret相比现有的最佳多模态大型语言模型（MLLM）平均提高了20.4%。这一结果表明Ferret在处理更复杂、更接近真实世界应用的任务时具有显著的优势。<br />
<br />
2、改善对象幻觉：Ferret 在描述图像细节时能够减少错误或虚构的内容，这在自动图像描述和分析领域尤为重要。<br />
它减轻了对象幻觉的问题，即在生成文本描述时减少了对不存在的对象的错误引用，提高了描述的准确性和可靠性。<br />
<br />
3、Ferret 不仅在传统的指代和定位任务中表现优异，它能够更准确地理解和处理图像中的空间信息和语义。而且在需要指代/定位、语义、知识和推理的任务中也表现出色。 <br />
<br />
Ferret 能够更准确地描述图像细节，减少在生成文本时对不存在的对象的幻觉。 通过其创新的方法和技术，为多模态语言模型在空间理解和定位方面提供了新的可能性，特别是在处理复杂的图像和文本交互时。<br />
<br />
适用于多种应用场景：<br />
<br />
由于其强大的图像和文本处理能力，Ferret 适用于多种应用场景，包括图像搜索、自动图像标注、交互式媒体探索等。<br />
<br />
GitHub：<a href="https://github.com/apple/ml-ferret">github.com/apple/ml-ferret</a><br />
论文：<a href="https://arxiv.org/abs/2310.07704">arxiv.org/abs/2310.07704</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NFODNUSGE4QUEyTUliLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NFODNUR2JJQUFQb040LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NFODNURWIwQUFUekpELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738733784252686781#m</id>
            <title>奥特曼公布网友的新年愿望清单：

除了第一条备注了，其他都没

是不是暗示其他都能实现

😎

AGI（请耐心等待）
GPT-5
更好的语音模式
更高的速率限制
更好的 GPT
更好的推理
控制清醒/行为的程度
视频
个性化
更好的浏览
'使用 openai 登录'
开源</title>
            <link>https://nitter.cz/xiaohuggg/status/1738733784252686781#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738733784252686781#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 01:30:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>奥特曼公布网友的新年愿望清单：<br />
<br />
除了第一条备注了，其他都没<br />
<br />
是不是暗示其他都能实现<br />
<br />
😎<br />
<br />
AGI（请耐心等待）<br />
GPT-5<br />
更好的语音模式<br />
更高的速率限制<br />
更好的 GPT<br />
更好的推理<br />
控制清醒/行为的程度<br />
视频<br />
个性化<br />
更好的浏览<br />
'使用 openai 登录'<br />
开源</p>
<p><a href="https://nitter.cz/sama/status/1738673279085457661#m">nitter.cz/sama/status/1738673279085457661#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738580319555743937#m</id>
            <title>TikTok Downloader：TikTok、抖音数据采集工具

- 视频和图集下载：支持批量下载TikTok和抖音的无水印视频和图集、喜欢的或收藏的作品。

- 数据采集：支持采集TikTok和抖音的详细数据，包括账号信息、评论数据、直播推流地址等。

- 多账号支持：支持多账号批量下载作品。

- 自动化功能：自动跳过已下载的文件，持久化保存采集数据。

- 多种模式支持：提供终端命令行模式、Web UI交互模式和Web API接口模式。

- 多平台兼容：支持Windows、macOS和Linux操作系统。

GitHub：https://github.com/JoeanAmier/TikTokDownloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1738580319555743937#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738580319555743937#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 15:20:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TikTok Downloader：TikTok、抖音数据采集工具<br />
<br />
- 视频和图集下载：支持批量下载TikTok和抖音的无水印视频和图集、喜欢的或收藏的作品。<br />
<br />
- 数据采集：支持采集TikTok和抖音的详细数据，包括账号信息、评论数据、直播推流地址等。<br />
<br />
- 多账号支持：支持多账号批量下载作品。<br />
<br />
- 自动化功能：自动跳过已下载的文件，持久化保存采集数据。<br />
<br />
- 多种模式支持：提供终端命令行模式、Web UI交互模式和Web API接口模式。<br />
<br />
- 多平台兼容：支持Windows、macOS和Linux操作系统。<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/TikTokDownloader">github.com/JoeanAmier/TikTok…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NDc0tDVGFVQUE4OFViLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>