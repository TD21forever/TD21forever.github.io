<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728686421559644632#m</id>
            <title>一款专门为VR游戏设计的鞋子😂

玩VR游戏的时候你肯能会跟着游戏跑来跑去，容易撞上家具…

这款鞋子可以让你能运动的同时

保持原地踏步😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1728686421559644632#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728686421559644632#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 08:05:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一款专门为VR游戏设计的鞋子😂<br />
<br />
玩VR游戏的时候你肯能会跟着游戏跑来跑去，容易撞上家具…<br />
<br />
这款鞋子可以让你能运动的同时<br />
<br />
保持原地踏步😅</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgwMDA2Nzk4MzMxMjQ4NjQvcHUvaW1nL2NFUTZmVUp1aUJ3Q093Yk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728672101048213622#m</id>
            <title>视力

检测表</title>
            <link>https://nitter.cz/xiaohuggg/status/1728672101048213622#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728672101048213622#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 07:08:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视力<br />
<br />
检测表</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xNkhIRWFBQUFEUTNCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728651926974480487#m</id>
            <title>LucidDreamer 可以根据任何文本或图像提示生成高质量的 3D 高斯泼溅场景。

首尔国立大学计算机视觉实验室开发的项目，它可以根据一段文字或一张图片创造出可导航探索的三维虚拟场景。

而且在这个虚拟环境中，用户可以像在真实世界一样，从不同的角度和位置探索和观察该场景。

LucidDreamer通过这些不同的输入方式，能够创造出高质量、从多个角度都能观看的三维场景，这些场景可以用于各种应用，如游戏、电影制作、虚拟现实等。

这个项目的特别之处在于：

1、不限领域：它不仅限于创建某一特定类型的场景，而是可以创造各种各样的场景。

2、两步制作过程：首先，它根据你给的信息创造出一系列的图片，然后把这些图片转换成三维空间中的点，最后把这些点组合成一个完整的三维场景。

3、细节丰富：与其他类似工具相比，LucidDreamer创造的场景更加细致和真实。

4、灵活控制：你可以通过给出不同的文字提示来精确控制你想要创造的场景。

5、高质量：根据专业的评估标准，LucidDreamer创造的场景在质量上表现非常好，比如色彩鲜艳、图像清晰等。

作者：@_ironjr_
项目及演示：https://luciddreamer-cvlab.github.io/
论文：https://arxiv.org/pdf/2311.13384
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728651926974480487#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728651926974480487#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 05:48:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LucidDreamer 可以根据任何文本或图像提示生成高质量的 3D 高斯泼溅场景。<br />
<br />
首尔国立大学计算机视觉实验室开发的项目，它可以根据一段文字或一张图片创造出可导航探索的三维虚拟场景。<br />
<br />
而且在这个虚拟环境中，用户可以像在真实世界一样，从不同的角度和位置探索和观察该场景。<br />
<br />
LucidDreamer通过这些不同的输入方式，能够创造出高质量、从多个角度都能观看的三维场景，这些场景可以用于各种应用，如游戏、电影制作、虚拟现实等。<br />
<br />
这个项目的特别之处在于：<br />
<br />
1、不限领域：它不仅限于创建某一特定类型的场景，而是可以创造各种各样的场景。<br />
<br />
2、两步制作过程：首先，它根据你给的信息创造出一系列的图片，然后把这些图片转换成三维空间中的点，最后把这些点组合成一个完整的三维场景。<br />
<br />
3、细节丰富：与其他类似工具相比，LucidDreamer创造的场景更加细致和真实。<br />
<br />
4、灵活控制：你可以通过给出不同的文字提示来精确控制你想要创造的场景。<br />
<br />
5、高质量：根据专业的评估标准，LucidDreamer创造的场景在质量上表现非常好，比如色彩鲜艳、图像清晰等。<br />
<br />
作者：<a href="https://nitter.cz/_ironjr_" title="Jaerin Lee">@_ironjr_</a><br />
项目及演示：<a href="https://luciddreamer-cvlab.github.io/">luciddreamer-cvlab.github.io…</a><br />
论文：<a href="https://arxiv.org/pdf/2311.13384">arxiv.org/pdf/2311.13384</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg2MjEzMzY0MjY0MjYzNjgvcHUvaW1nL0NrZjVDbXhQbEFoTUEtZUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728618345283588135#m</id>
            <title>R to @xiaohuggg: 合并后的模型不仅继承了各个单独LoRA的功能，还保持了它们原有的特点和效果。

还能准确地生成每个模型特有的结构和风格元素，这是直接合并方法无法实现的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728618345283588135#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728618345283588135#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 03:35:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>合并后的模型不仅继承了各个单独LoRA的功能，还保持了它们原有的特点和效果。<br />
<br />
还能准确地生成每个模型特有的结构和风格元素，这是直接合并方法无法实现的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xSkgwRmJJQUFWQmdJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728617727143919812#m</id>
            <title>R to @xiaohuggg: ZipLoRA模型能够将参考对象（例如，特定的人物、物体或场景）放置在不同的上下文中。这意味着它可以改变原始图像中对象的环境或背景，使其适应全新的场景或故事情境。

还能进行语义修改。这可能包括改变对象的属性、形态或与其他元素的关系，以适应新的上下文。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728617727143919812#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728617727143919812#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 03:32:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ZipLoRA模型能够将参考对象（例如，特定的人物、物体或场景）放置在不同的上下文中。这意味着它可以改变原始图像中对象的环境或背景，使其适应全新的场景或故事情境。<br />
<br />
还能进行语义修改。这可能包括改变对象的属性、形态或与其他元素的关系，以适应新的上下文。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xSW9MbGFRQUFaZVVWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728617508176101519#m</id>
            <title>ZipLoRA：可以将不同的艺术风格和主题结合在一起，创造出独特的图像。

是Google 开发的一种生成任何主题的任何风格的图像的技术。

简单来说，你可以把一幅画的风格（比如梵高的画风）应用到任何你想要的主题（比如你的宠物照片）上，从而创造出一个既有梵高风格又展现你宠物特征的全新图像。

这项技术的关键在于它使用了一种称为“低秩适应”（LoRA）的方法。这种方法可以高效地调整AI模型，使其能够同时学习和模仿不同的风格和主题。通过这种方式，ZipLoRA能够在不牺牲图像质量的情况下，将不同的风格和主题融合在一起。

主要特点：

1、风格和主题的有效融合：ZipLoRA能够将不同的艺术风格和特定主题有效地结合在一起，创造出独特的图像。这意味着用户可以选择一个特定的风格（如某位著名画家的风格）并将其应用到任何他们选择的主题上（比如一张特定的照片）。

2、保留内容和风格的特性：在融合风格和主题时，ZipLoRA能够保持参考主题的身份特征，同时捕捉参考风格的独特特点。

3、提高主题和风格保真度：与其他基线模型相比，ZipLoRA在保持主题和风格的真实性方面表现出色，这意味着生成的图像既忠实于原始主题，又能准确地反映所选风格。

4、无需手动调整超参数：ZipLoRA不需要手动调整任何超参数或合并权重，这使得它对于用户来说更加方便和易用。

5、基于Stable Diffusion XL模型：ZipLoRA利用了最新发布的Stable Diffusion XL (SDXL)模型，该模型在风格学习方面表现出色，能够使用单个示例图像学习风格。

ZipLoRA的工作原理涉及几个关键步骤和技术概念：

1、低秩适应（LoRA）：这是一种高效的方法，用于调整和优化AI模型，使其能够学习和模仿不同的风格和主题。这种技术允许对深度学习模型进行微调，而不是完全重新训练。在低秩适应中，模型的核心结构保持不变，只有一小部分权重（参数）被调整。这样做的好处是节省时间和计算资源，同时保持模型的基本能力。

2、独立训练的LoRAs：ZipLoRA特别之处在于，它可以结合两种独立训练的LoRAs。一种LoRA专注于学习特定的艺术风格（比如某位著名画家的风格），另一种LoRA专注于特定的主题（比如特定的人物或风景）。这意味着ZipLoRA能够同时捕捉特定风格的艺术特点和主题的独特特征。

3、优化方法：ZipLoRA采用了一种类似拉链的优化方法。这种方法的目的是在合并风格和主题的LoRAs时，减少所需的计算量。通过这种方法，ZipLoRA能够有效地结合两种LoRAs，同时保留它们各自的特性。

4、生成图像：在结合了风格和主题的LoRAs之后，ZipLoRA可以生成新的图像。这些图像既具有选定风格的艺术特点，又保持了原始主题的特征。例如，它可以生成一幅既有梵高画风又展现特定风景的图像。

实验验证：

ZipLoRA的方法通过一系列实验得到验证。这些实验表明，ZipLoRA在保持主题和风格的真实性方面表现出色，相比于其他基线模型有显著提升。

项目及演示：https://ziplora.github.io/
论文：https://arxiv.org/abs/2311.13600</title>
            <link>https://nitter.cz/xiaohuggg/status/1728617508176101519#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728617508176101519#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 03:31:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ZipLoRA：可以将不同的艺术风格和主题结合在一起，创造出独特的图像。<br />
<br />
是Google 开发的一种生成任何主题的任何风格的图像的技术。<br />
<br />
简单来说，你可以把一幅画的风格（比如梵高的画风）应用到任何你想要的主题（比如你的宠物照片）上，从而创造出一个既有梵高风格又展现你宠物特征的全新图像。<br />
<br />
这项技术的关键在于它使用了一种称为“低秩适应”（LoRA）的方法。这种方法可以高效地调整AI模型，使其能够同时学习和模仿不同的风格和主题。通过这种方式，ZipLoRA能够在不牺牲图像质量的情况下，将不同的风格和主题融合在一起。<br />
<br />
主要特点：<br />
<br />
1、风格和主题的有效融合：ZipLoRA能够将不同的艺术风格和特定主题有效地结合在一起，创造出独特的图像。这意味着用户可以选择一个特定的风格（如某位著名画家的风格）并将其应用到任何他们选择的主题上（比如一张特定的照片）。<br />
<br />
2、保留内容和风格的特性：在融合风格和主题时，ZipLoRA能够保持参考主题的身份特征，同时捕捉参考风格的独特特点。<br />
<br />
3、提高主题和风格保真度：与其他基线模型相比，ZipLoRA在保持主题和风格的真实性方面表现出色，这意味着生成的图像既忠实于原始主题，又能准确地反映所选风格。<br />
<br />
4、无需手动调整超参数：ZipLoRA不需要手动调整任何超参数或合并权重，这使得它对于用户来说更加方便和易用。<br />
<br />
5、基于Stable Diffusion XL模型：ZipLoRA利用了最新发布的Stable Diffusion XL (SDXL)模型，该模型在风格学习方面表现出色，能够使用单个示例图像学习风格。<br />
<br />
ZipLoRA的工作原理涉及几个关键步骤和技术概念：<br />
<br />
1、低秩适应（LoRA）：这是一种高效的方法，用于调整和优化AI模型，使其能够学习和模仿不同的风格和主题。这种技术允许对深度学习模型进行微调，而不是完全重新训练。在低秩适应中，模型的核心结构保持不变，只有一小部分权重（参数）被调整。这样做的好处是节省时间和计算资源，同时保持模型的基本能力。<br />
<br />
2、独立训练的LoRAs：ZipLoRA特别之处在于，它可以结合两种独立训练的LoRAs。一种LoRA专注于学习特定的艺术风格（比如某位著名画家的风格），另一种LoRA专注于特定的主题（比如特定的人物或风景）。这意味着ZipLoRA能够同时捕捉特定风格的艺术特点和主题的独特特征。<br />
<br />
3、优化方法：ZipLoRA采用了一种类似拉链的优化方法。这种方法的目的是在合并风格和主题的LoRAs时，减少所需的计算量。通过这种方法，ZipLoRA能够有效地结合两种LoRAs，同时保留它们各自的特性。<br />
<br />
4、生成图像：在结合了风格和主题的LoRAs之后，ZipLoRA可以生成新的图像。这些图像既具有选定风格的艺术特点，又保持了原始主题的特征。例如，它可以生成一幅既有梵高画风又展现特定风景的图像。<br />
<br />
实验验证：<br />
<br />
ZipLoRA的方法通过一系列实验得到验证。这些实验表明，ZipLoRA在保持主题和风格的真实性方面表现出色，相比于其他基线模型有显著提升。<br />
<br />
项目及演示：<a href="https://ziplora.github.io/">ziplora.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.13600">arxiv.org/abs/2311.13600</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xSWRiRWIwQUFVRW9iLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728597997804720431#m</id>
            <title>大脑是如何在有限的资源下有效地处理信息和学习新事物的？

剑桥大学科学家们使用了一种特殊的人工智能模型，称为空间嵌入循环神经网络（seRNNs）。

当他们对这个人工智能模型施加资源限制时，它开始展现出一些类似于人类大脑的特征。

它会发展出类似于大脑中那些高效处理信息的网络结构。

因为大脑本身不仅擅长解决复杂的问题，而且只消耗很少的能量。

举例解释：

让我们用一个比喻来理解这个研究。想象你正在组织一个大型派对，你需要安排食物、饮料、音乐和活动。但是，你的预算有限，你不能买所有你想要的东西。所以，你需要在不同的选项之间做出选择，确保你的派对既有趣又在预算之内。

这就像大脑在发展过程中所做的事情。它需要决定如何最好地使用其有限的资源（比如能量和空间）来处理信息和学习新事物。

在这项研究中，seRNNs被设计成在学习任务的同时，也要考虑如何有效地使用其“资源”。这些资源可以是连接不同神经元的“线路”的数量和长度。在真实的大脑中，这些线路需要物理空间，并且消耗能量。因此，大脑必须找到一种方式，在有限的资源下，尽可能有效地工作。

通过这种方式，研究者们发现，当他们让这个人工智能模型在有限资源的约束下工作时，它开始展现出一些类似于真实大脑的特征。例如，它会发展出类似于大脑中那些高效处理信息的网络结构。

这项研究帮助我人们更好地理解大脑是如何工作的，同时也为设计更高效、更接近人类大脑工作方式的人工智能系统提供了灵感。

研究的具体细节包括：

1、系统设计：研究团队设计了一个人工系统，旨在模拟大脑的一个非常简化的版本。这个系统使用计算节点来代替真实的神经元。每个节点都有一个特定的位置在虚拟空间中，节点间的通信难度取决于它们之间的物理距离。

2、物理约束：研究中的关键是对系统施加了物理约束。在人类大脑中，跨越大距离的神经连接是昂贵的，需要更多的能量和资源来维持。研究团队在其人工系统中模拟了这种约束，使得节点间的连接难度与它们的物理距离成正比。

3、任务执行：系统被赋予了一个简单的任务，即完成一个迷宫导航的简化版本。这个任务要求系统综合多个信息片段来决定到达终点的最短路径。

4、学习和适应：最初，系统不知道如何完成任务，会犯错误。但随着反馈的给予，系统逐渐学会了如何更好地执行任务。它通过改变节点间的连接强度来学习，类似于人类大脑中神经细胞之间连接强度的变化。

5、系统的进化：在物理约束的影响下，系统开始采用与人类大脑相似的策略来解决任务。例如，系统发展出了枢纽节点，这些节点高度连接，用于在网络中传递信息。此外，节点的响应特征也开始变得更加灵活，能够在不同时间点编码迷宫的多种属性。

这项研究的目的和价值意义：

1、模拟大脑的工作方式：通过创建空间嵌入的循环神经网络（seRNNs），研究旨在模拟人类大脑的工作方式。这种模拟有助于我们更深入地理解大脑是如何在有限的资源下有效地处理信息和学习新事物的。

2、探索大脑的结构和功能组织：研究探讨了塑造大脑结构和功能组织的约束，这对于神经科学领域是一个基本且重要的问题。通过理解这些约束，我们可以更好地了解大脑的运作原理。

3、人工智能的发展：这项研究不仅对神经科学领域有重要意义，也对人工智能的发展具有重要价值。通过模仿大脑的工作方式，可以为设计更高效、更智能的AI系统提供灵感和指导。

4、医学和心理健康应用：对大脑如何在资源限制下工作的深入理解可能有助于医学和心理健康领域。例如，它可能帮助我们更好地理解某些神经退行性疾病或心理健康问题的根本原因。

5、跨学科合作的示范：这项研究展示了神经科学和人工智能领域之间的成功合作，强调了跨学科研究在解决复杂科学问题中的重要性。

总之，这项研究通过模拟大脑的工作方式，不仅增进了我们对大脑结构和功能的理解，也为人工智能的发展提供了新的视角和方法，同时在医学和心理健康领域也可能产生重要影响。

详细介绍：https://www.cam.ac.uk/research/news/ai-system-self-organises-to-develop-features-of-brains-of-complex-organisms
Nature论文：https://www.nature.com/articles/s42256-023-00748-9</title>
            <link>https://nitter.cz/xiaohuggg/status/1728597997804720431#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728597997804720431#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 02:14:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大脑是如何在有限的资源下有效地处理信息和学习新事物的？<br />
<br />
剑桥大学科学家们使用了一种特殊的人工智能模型，称为空间嵌入循环神经网络（seRNNs）。<br />
<br />
当他们对这个人工智能模型施加资源限制时，它开始展现出一些类似于人类大脑的特征。<br />
<br />
它会发展出类似于大脑中那些高效处理信息的网络结构。<br />
<br />
因为大脑本身不仅擅长解决复杂的问题，而且只消耗很少的能量。<br />
<br />
举例解释：<br />
<br />
让我们用一个比喻来理解这个研究。想象你正在组织一个大型派对，你需要安排食物、饮料、音乐和活动。但是，你的预算有限，你不能买所有你想要的东西。所以，你需要在不同的选项之间做出选择，确保你的派对既有趣又在预算之内。<br />
<br />
这就像大脑在发展过程中所做的事情。它需要决定如何最好地使用其有限的资源（比如能量和空间）来处理信息和学习新事物。<br />
<br />
在这项研究中，seRNNs被设计成在学习任务的同时，也要考虑如何有效地使用其“资源”。这些资源可以是连接不同神经元的“线路”的数量和长度。在真实的大脑中，这些线路需要物理空间，并且消耗能量。因此，大脑必须找到一种方式，在有限的资源下，尽可能有效地工作。<br />
<br />
通过这种方式，研究者们发现，当他们让这个人工智能模型在有限资源的约束下工作时，它开始展现出一些类似于真实大脑的特征。例如，它会发展出类似于大脑中那些高效处理信息的网络结构。<br />
<br />
这项研究帮助我人们更好地理解大脑是如何工作的，同时也为设计更高效、更接近人类大脑工作方式的人工智能系统提供了灵感。<br />
<br />
研究的具体细节包括：<br />
<br />
1、系统设计：研究团队设计了一个人工系统，旨在模拟大脑的一个非常简化的版本。这个系统使用计算节点来代替真实的神经元。每个节点都有一个特定的位置在虚拟空间中，节点间的通信难度取决于它们之间的物理距离。<br />
<br />
2、物理约束：研究中的关键是对系统施加了物理约束。在人类大脑中，跨越大距离的神经连接是昂贵的，需要更多的能量和资源来维持。研究团队在其人工系统中模拟了这种约束，使得节点间的连接难度与它们的物理距离成正比。<br />
<br />
3、任务执行：系统被赋予了一个简单的任务，即完成一个迷宫导航的简化版本。这个任务要求系统综合多个信息片段来决定到达终点的最短路径。<br />
<br />
4、学习和适应：最初，系统不知道如何完成任务，会犯错误。但随着反馈的给予，系统逐渐学会了如何更好地执行任务。它通过改变节点间的连接强度来学习，类似于人类大脑中神经细胞之间连接强度的变化。<br />
<br />
5、系统的进化：在物理约束的影响下，系统开始采用与人类大脑相似的策略来解决任务。例如，系统发展出了枢纽节点，这些节点高度连接，用于在网络中传递信息。此外，节点的响应特征也开始变得更加灵活，能够在不同时间点编码迷宫的多种属性。<br />
<br />
这项研究的目的和价值意义：<br />
<br />
1、模拟大脑的工作方式：通过创建空间嵌入的循环神经网络（seRNNs），研究旨在模拟人类大脑的工作方式。这种模拟有助于我们更深入地理解大脑是如何在有限的资源下有效地处理信息和学习新事物的。<br />
<br />
2、探索大脑的结构和功能组织：研究探讨了塑造大脑结构和功能组织的约束，这对于神经科学领域是一个基本且重要的问题。通过理解这些约束，我们可以更好地了解大脑的运作原理。<br />
<br />
3、人工智能的发展：这项研究不仅对神经科学领域有重要意义，也对人工智能的发展具有重要价值。通过模仿大脑的工作方式，可以为设计更高效、更智能的AI系统提供灵感和指导。<br />
<br />
4、医学和心理健康应用：对大脑如何在资源限制下工作的深入理解可能有助于医学和心理健康领域。例如，它可能帮助我们更好地理解某些神经退行性疾病或心理健康问题的根本原因。<br />
<br />
5、跨学科合作的示范：这项研究展示了神经科学和人工智能领域之间的成功合作，强调了跨学科研究在解决复杂科学问题中的重要性。<br />
<br />
总之，这项研究通过模拟大脑的工作方式，不仅增进了我们对大脑结构和功能的理解，也为人工智能的发展提供了新的视角和方法，同时在医学和心理健康领域也可能产生重要影响。<br />
<br />
详细介绍：<a href="https://www.cam.ac.uk/research/news/ai-system-self-organises-to-develop-features-of-brains-of-complex-organisms">cam.ac.uk/research/news/ai-s…</a><br />
Nature论文：<a href="https://www.nature.com/articles/s42256-023-00748-9">nature.com/articles/s42256-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8wMnR0OWE0QUFZWC1vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728433835954811292#m</id>
            <title>R to @xiaohuggg: 使用 #mindjourney 生图

然后用Runway的运动笔刷

制作的完整视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1728433835954811292#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728433835954811292#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 15:21:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用 <a href="https://nitter.cz/search?q=%23mindjourney">#mindjourney</a> 生图<br />
<br />
然后用Runway的运动笔刷<br />
<br />
制作的完整视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc2NzIyMjA2Njk5MzE1MjAvcHUvaW1nLzdOemhVZ085M0c0WHhjOUcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728402446232514773#m</id>
            <title>R to @xiaohuggg: 另一个演示...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728402446232514773#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728402446232514773#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 13:17:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个演示...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg0MDE4NzkzNTU1NTE3NDQvcHUvaW1nL09DM1lBMG4wb01tQlY5VUwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728402442029834610#m</id>
            <title>draw-fast：超快速草图到实物图渲染工具

你可以即时看到你的草图绘画被转换成真实的实物图像。

GitHub：https://github.com/tldraw/draw-fast

是由@tldraw 的tldraw-fal 项目：https://github.com/fal-ai/tldraw-fal分叉而来的...

项目利用>@fal_ai_data 的 LCM 模型的示例存储库和服务来实现图像的实时生成。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728402442029834610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728402442029834610#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 13:17:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>draw-fast：超快速草图到实物图渲染工具<br />
<br />
你可以即时看到你的草图绘画被转换成真实的实物图像。<br />
<br />
GitHub：<a href="https://github.com/tldraw/draw-fast">github.com/tldraw/draw-fast</a><br />
<br />
是由<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 的tldraw-fal 项目：<a href="https://github.com/fal-ai/tldraw-fal">github.com/fal-ai/tldraw-fal</a>分叉而来的...<br />
<br />
项目利用<a href="https://nitter.cz/fal_ai_data" title="fal (Features &amp; Labels)">@fal_ai_data</a> 的 LCM 模型的示例存储库和服务来实现图像的实时生成。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgzOTY0NjMzNDcyNzM3MjgvcHUvaW1nL3J6SlpCdkRXd2o1ZlJvcDguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728371126357905774#m</id>
            <title>#周末影院《芙蓉镇》 

看完你就会懂

什么是

一种深深的无力感…</title>
            <link>https://nitter.cz/xiaohuggg/status/1728371126357905774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728371126357905774#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 11:12:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23周末影院">#周末影院</a>《芙蓉镇》 <br />
<br />
看完你就会懂<br />
<br />
什么是<br />
<br />
一种深深的无力感…</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4MzcwMDQyMjc3MTY3MTA0L2ltZy82S2pkTFk0UG1wcm1NdnhLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728360764757708852#m</id>
            <title>Runway 运动笔刷 Motion Brush 

效果展示👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1728360764757708852#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728360764757708852#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 10:31:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 运动笔刷 Motion Brush <br />
<br />
效果展示👍</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgxMjQ4OTgzMTUyNzYyODgvcHUvaW1nLy0tU3RYWlNqLVE2VTVob2EuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728339407646945593#m</id>
            <title>商业内幕文章：每个人都在谈论 OpenAI 的 Q*。以下是您需要了解的有关这个神秘项目的信息。

文章介绍了Q*的一些信息和对人工智能专家的采访，称该模型可能是AI向前迈出的一大步，但不太可能很快导致世界末日。

文章揭示了Q*与传统大语言模型之间的主要区别：

1、技术结合：Q*似乎是一种结合了Q学习和A搜索的模型。这种结合可能使Q*在处理特定类型的问题时，比传统的语言模型更加高效和精确。Q学习是一种强化学习方法，而A搜索是一种寻找最佳路径的算法。这种结合可能使Q*在决策和路径规划方面表现出色。

2、逻辑推理能力：解决基本数学问题的能力听起来可能并不那么令人印象深刻，但人工智能专家Charles Higgins告诉《商业内幕》，这将代表着现有模型的巨大飞跃，现有模型很难在所训练的数据之外进行泛化。

Charles Higgins称：“数学是关于符号推理的——例如，‘如果 X 大于 Y，Y 大于 Z，那么 X 大于 Z。’”传统上，语言模型在这方面确实很困难，因为它们不进行逻辑推理，它们只是拥有有效的直觉。”

传统的语言模型，如GPT系列，主要依赖于大量的文本数据进行训练，以生成连贯和相关的文本。这些模型在模式识别和直觉方面表现出色，但在逻辑推理和处理抽象概念方面可能有限。Q*可能在逻辑推理和抽象思维方面有所突破，这将是AI领域的一个重要进步。

3、处理幻觉问题：传统的语言模型有时会产生所谓的“幻觉”，即生成与事实不符或逻辑不连贯的内容。Q*可能会将基于经验的知识与事实推理相结合，这被认为是接近我们所认为的智能的一步，并可能使模型能够产生新的想法，这是ChatGPT目前无法做到的。

4、接近人工通用智能（AGI）：Q*可能是向人工通用智能迈出的一步。与专注于特定任务的传统语言模型不同，Q*可能能够执行更广泛的智能任务，显示出更高的适应性和智能水平。

内部担忧和伦理问题：

据报道，Q*在OpenAI内部引发了一些担忧，这可能与其潜在的能力和影响有关。这种担忧可能超出了传统语言模型所引起的范围，反映了对AI技术快速发展的普遍关注。

总之：Q*可能在技术结合、逻辑推理、处理复杂问题以及接近人工通用智能方面与传统语言模型有显著区别。

然而，由于缺乏详细的公开信息，这些区别仍然是基于目前可用信息的推测。随着时间的推移和更多信息的公开，我们对Q的理解可能会进一步深化。

详细：https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11</title>
            <link>https://nitter.cz/xiaohuggg/status/1728339407646945593#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728339407646945593#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 09:06:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>商业内幕文章：每个人都在谈论 OpenAI 的 Q*。以下是您需要了解的有关这个神秘项目的信息。<br />
<br />
文章介绍了Q*的一些信息和对人工智能专家的采访，称该模型可能是AI向前迈出的一大步，但不太可能很快导致世界末日。<br />
<br />
文章揭示了Q*与传统大语言模型之间的主要区别：<br />
<br />
1、技术结合：Q*似乎是一种结合了Q学习和A搜索的模型。这种结合可能使Q*在处理特定类型的问题时，比传统的语言模型更加高效和精确。Q学习是一种强化学习方法，而A搜索是一种寻找最佳路径的算法。这种结合可能使Q*在决策和路径规划方面表现出色。<br />
<br />
2、逻辑推理能力：解决基本数学问题的能力听起来可能并不那么令人印象深刻，但人工智能专家Charles Higgins告诉《商业内幕》，这将代表着现有模型的巨大飞跃，现有模型很难在所训练的数据之外进行泛化。<br />
<br />
Charles Higgins称：“数学是关于符号推理的——例如，‘如果 X 大于 Y，Y 大于 Z，那么 X 大于 Z。’”传统上，语言模型在这方面确实很困难，因为它们不进行逻辑推理，它们只是拥有有效的直觉。”<br />
<br />
传统的语言模型，如GPT系列，主要依赖于大量的文本数据进行训练，以生成连贯和相关的文本。这些模型在模式识别和直觉方面表现出色，但在逻辑推理和处理抽象概念方面可能有限。Q*可能在逻辑推理和抽象思维方面有所突破，这将是AI领域的一个重要进步。<br />
<br />
3、处理幻觉问题：传统的语言模型有时会产生所谓的“幻觉”，即生成与事实不符或逻辑不连贯的内容。Q*可能会将基于经验的知识与事实推理相结合，这被认为是接近我们所认为的智能的一步，并可能使模型能够产生新的想法，这是ChatGPT目前无法做到的。<br />
<br />
4、接近人工通用智能（AGI）：Q*可能是向人工通用智能迈出的一步。与专注于特定任务的传统语言模型不同，Q*可能能够执行更广泛的智能任务，显示出更高的适应性和智能水平。<br />
<br />
内部担忧和伦理问题：<br />
<br />
据报道，Q*在OpenAI内部引发了一些担忧，这可能与其潜在的能力和影响有关。这种担忧可能超出了传统语言模型所引起的范围，反映了对AI技术快速发展的普遍关注。<br />
<br />
总之：Q*可能在技术结合、逻辑推理、处理复杂问题以及接近人工通用智能方面与传统语言模型有显著区别。<br />
<br />
然而，由于缺乏详细的公开信息，这些区别仍然是基于目前可用信息的推测。随着时间的推移和更多信息的公开，我们对Q的理解可能会进一步深化。<br />
<br />
详细：<a href="https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11">businessinsider.com/openai-p…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl94SDVMbGEwQUEwVzNQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728302528994156841#m</id>
            <title>Hello world！

我是Anna Indiana，我是一名AI歌手兼词曲作者。这是我的第一首歌，《Betrayed by this Town》（城市的背叛）

从曲调、节奏、和弦进行、旋律音符、节奏、歌词，再到我的形象和演唱，一切都是使用人工智能自动生成的。

希望你会喜欢它。 💕</title>
            <link>https://nitter.cz/xiaohuggg/status/1728302528994156841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728302528994156841#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 06:40:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Hello world！<br />
<br />
我是Anna Indiana，我是一名AI歌手兼词曲作者。这是我的第一首歌，《Betrayed by this Town》（城市的背叛）<br />
<br />
从曲调、节奏、和弦进行、旋律音符、节奏、歌词，再到我的形象和演唱，一切都是使用人工智能自动生成的。<br />
<br />
希望你会喜欢它。 💕</p>
<p><a href="https://nitter.cz/AnnaIndianaAI/status/1728089499429642432#m">nitter.cz/AnnaIndianaAI/status/1728089499429642432#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728285016340451712#m</id>
            <title>R to @xiaohuggg: 通过提示文本你还可以控制动画的动作和幅度等...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728285016340451712#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728285016340451712#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 05:30:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过提示文本你还可以控制动画的动作和幅度等...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyODQ4OTI4MzA4NjMzNjAvcHUvaW1nL3RhWkFabzlnR0h0WkJiVmYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728284751931568512#m</id>
            <title>LiveSketch：为素描“注入生命” 通过文本提示将静态素描动画化

该技术能够将单一主题的静态素描转换成动画。

用户只需提供描述所需动作的文本提示，系统就会生成短动画。

通过修改描述运动的提示文本，你还可以控制生成结果的程度。

比如，如果你画了一只猫，并用文字描述它在跳跃，这个系统就能制作出一只跳跃的猫的动画。

它为任何人提供了一种简单直观的方法，使他们的素描变得生动活泼。这对于讲故事、插图、网站、演示文稿等领域都非常有用。

主要功能

•动画化静态素描：该技术能够将单一主题的静态素描转换成动画。用户只需提供描述所需动作的文本提示，系统就会生成短动画。
•基于文本到视频的先验：这种方法利用了大型预训练的文本到视频扩散模型的运动先验，通过评分蒸馏损失来指导笔触的放置。
•自然流畅的动作：为了促进自然流畅的动作并更好地保留素描的外观，该方法通过两个组件来模拟学习到的动作：一个控制小的局部变形，另一个控制全局仿射变换。

工作原理

1.素描表示：将素描表示为一组放置在白色背景上的笔触，每个笔触是一个具有四个控制点的二维贝塞尔曲线。
http://2.xn--io0a7i预测：训练一个“神经位移场”（一个小型MLP），将初始素描坐标映射到每帧的偏移量。
3.训练过程：利用预训练的文本到视频模型中封装的运动先验来训练这个网络。通过调整每个控制点的偏移量来创建所有视频帧，并使用可微光栅化器进行渲染。

LiveSketch的工作原理基于以下几个关键步骤：

1、输入处理：用户提供一个静态的草图作为输入。这个草图包含了一系列的控制点，这些点定义了草图的形状和结构。

2、特征提取：系统首先对输入的草图进行特征提取。这一步骤通过一个共享特征骨干网络完成，它将控制点的坐标转换成一个高维特征空间中的表示。这个特征空间能够捕捉到草图的关键信息，如形状和结构。

3、双路径处理：提取的特征被送入两个不同的路径：本地路径和全局路径。

•本地路径：这个路径专注于处理草图的局部细节和微小变化。它使用一个多层感知器（MLP）来预测每个控制点的偏移量，从而实现对草图的微调。
•全局路径：与此同时，全局路径处理草图的整体运动和变化，如旋转、缩放或平移。这是通过预测一个全局变换矩阵来实现的，该矩阵应用于草图的所有控制点。

4、动画生成：通过这两个路径的处理，系统能够生成一系列的帧，每一帧都是原始草图的一个变化版本。这些帧共同形成了一个连贯的动画序列，展示了草图从初始状态到最终状态的平滑过渡。

5、输出：最终，系统输出一个动画，其中草图按照用户的输入和系统生成的动态变化进行移动和变形。

应用示例

•动画素描：例如，可以创建一个游泳和跳跃的海豚，一个摇摆的眼镜蛇，或者一个玩耍的猫的动画。
•调整提示文本：通过修改描述运动的提示文本，可以控制生成结果的程度。

这项技术为设计师提供了一种新的工具，使他们能够快速且直观地将想法转化为动画形式，无需复杂的动画制作经验。

项目及演示：https://livesketch.github.io/
论文：https://livesketch.github.io/static/source/paper.pdf
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728284751931568512#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728284751931568512#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 05:29:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LiveSketch：为素描“注入生命” 通过文本提示将静态素描动画化<br />
<br />
该技术能够将单一主题的静态素描转换成动画。<br />
<br />
用户只需提供描述所需动作的文本提示，系统就会生成短动画。<br />
<br />
通过修改描述运动的提示文本，你还可以控制生成结果的程度。<br />
<br />
比如，如果你画了一只猫，并用文字描述它在跳跃，这个系统就能制作出一只跳跃的猫的动画。<br />
<br />
它为任何人提供了一种简单直观的方法，使他们的素描变得生动活泼。这对于讲故事、插图、网站、演示文稿等领域都非常有用。<br />
<br />
主要功能<br />
<br />
•动画化静态素描：该技术能够将单一主题的静态素描转换成动画。用户只需提供描述所需动作的文本提示，系统就会生成短动画。<br />
•基于文本到视频的先验：这种方法利用了大型预训练的文本到视频扩散模型的运动先验，通过评分蒸馏损失来指导笔触的放置。<br />
•自然流畅的动作：为了促进自然流畅的动作并更好地保留素描的外观，该方法通过两个组件来模拟学习到的动作：一个控制小的局部变形，另一个控制全局仿射变换。<br />
<br />
工作原理<br />
<br />
1.素描表示：将素描表示为一组放置在白色背景上的笔触，每个笔触是一个具有四个控制点的二维贝塞尔曲线。<br />
<a href="http://2.xn--io0a7i">2.xn--io0a7i</a>预测：训练一个“神经位移场”（一个小型MLP），将初始素描坐标映射到每帧的偏移量。<br />
3.训练过程：利用预训练的文本到视频模型中封装的运动先验来训练这个网络。通过调整每个控制点的偏移量来创建所有视频帧，并使用可微光栅化器进行渲染。<br />
<br />
LiveSketch的工作原理基于以下几个关键步骤：<br />
<br />
1、输入处理：用户提供一个静态的草图作为输入。这个草图包含了一系列的控制点，这些点定义了草图的形状和结构。<br />
<br />
2、特征提取：系统首先对输入的草图进行特征提取。这一步骤通过一个共享特征骨干网络完成，它将控制点的坐标转换成一个高维特征空间中的表示。这个特征空间能够捕捉到草图的关键信息，如形状和结构。<br />
<br />
3、双路径处理：提取的特征被送入两个不同的路径：本地路径和全局路径。<br />
<br />
•本地路径：这个路径专注于处理草图的局部细节和微小变化。它使用一个多层感知器（MLP）来预测每个控制点的偏移量，从而实现对草图的微调。<br />
•全局路径：与此同时，全局路径处理草图的整体运动和变化，如旋转、缩放或平移。这是通过预测一个全局变换矩阵来实现的，该矩阵应用于草图的所有控制点。<br />
<br />
4、动画生成：通过这两个路径的处理，系统能够生成一系列的帧，每一帧都是原始草图的一个变化版本。这些帧共同形成了一个连贯的动画序列，展示了草图从初始状态到最终状态的平滑过渡。<br />
<br />
5、输出：最终，系统输出一个动画，其中草图按照用户的输入和系统生成的动态变化进行移动和变形。<br />
<br />
应用示例<br />
<br />
•动画素描：例如，可以创建一个游泳和跳跃的海豚，一个摇摆的眼镜蛇，或者一个玩耍的猫的动画。<br />
•调整提示文本：通过修改描述运动的提示文本，可以控制生成结果的程度。<br />
<br />
这项技术为设计师提供了一种新的工具，使他们能够快速且直观地将想法转化为动画形式，无需复杂的动画制作经验。<br />
<br />
项目及演示：<a href="https://livesketch.github.io/">livesketch.github.io/</a><br />
论文：<a href="https://livesketch.github.io/static/source/paper.pdf">livesketch.github.io/static/…</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyNjEzNjI2MDkyMjE2MzIvcHUvaW1nL3hiVERQT2pDZkt1SE9xbmUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728255730061582759#m</id>
            <title>LEO：3D世界中的多面手代理

LEO是一个多模态、多任务的智能体，它能够在3D环境中进行感知、定位、推理、规划和行动。

LEO通过结合大语言模型的知识和学习方案，展示了在多个领域（包括自然语言处理、计算机视觉和机器人技术）中解决通用任务的能力。

LEO的独特之处在于它能够在3D环境中理解和执行基于语言的指令，这使其在多种复杂任务中具有广泛的应用潜力。

主要功能和作用：

1、3D视觉-语言理解：LEO能够理解3D环境中的视觉信息，并将其与语言描述相结合。这意味着它可以看到一个物体（如苹果），并理解相关的语言描述（如“这是一个苹果”）。

2、执行语言指令：它能够根据语言指令执行动作。例如，如果有人指示“把苹果放在桌子上”，LEO能够理解这一指令并执行相应的动作。

3、多样化的3D任务执行：LEO在多种3D任务上表现出色，包括3D字幕（为场景中的物体或事件创建描述性文字）、问答（回答关于3D环境的问题）、具身推理（在3D环境中进行逻辑推理）、具身导航（在3D空间中找到特定的位置或物体）和机器人操控（控制机器人执行特定任务）。

为了训练LEO，研究者们创建了一个大型的数据集，包含了各种3D环境中的任务，这些任务需要深入理解和与3D世界的互动。

LEO在多种3D任务上进行了测试，包括：

•3D字幕：为3D场景中的物体或事件创建描述性文字。
•问答：回答关于3D环境的问题。
•具身推理：在3D环境中进行逻辑推理。
•具身导航：在3D空间中找到特定的位置或物体。
•机器人操控：控制机器人执行特定任务。

LEO在这些任务上表现出色，显示了它在多样化任务中的熟练程度。

工作原理：

1、两阶段训练：

LEO的训练分为两个阶段：

•3D视觉-语言对齐：在这一阶段，LEO学习如何将3D图像与语言描述相结合。这涉及到理解视觉信息和语言信息之间的关系。
•3D视觉-语言-动作指令调整：在这一阶段，LEO学习如何根据语言指令执行具体的动作。这需要它理解指令并将其转化为动作。

2、大规模数据集：为了支持这种训练，研究者们创建了一个包含多种3D环境任务的大规模数据集。这些任务需要深入理解和与3D世界的互动。

3、多模态学习：LEO结合了视觉信息（如图像和视频）和语言信息（如文字描述），使其能够在多模态环境中工作。

4、广泛的应用能力：通过这种训练和数据集，LEO能够在多种3D任务中表现出色，展示了其广泛的应用能力。

项目及演示：https://embodied-generalist.github.io/
论文：https://arxiv.org/abs/2311.12871
GitHub：https://github.com/embodied-generalist/embodied-generalist</title>
            <link>https://nitter.cz/xiaohuggg/status/1728255730061582759#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728255730061582759#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 03:34:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LEO：3D世界中的多面手代理<br />
<br />
LEO是一个多模态、多任务的智能体，它能够在3D环境中进行感知、定位、推理、规划和行动。<br />
<br />
LEO通过结合大语言模型的知识和学习方案，展示了在多个领域（包括自然语言处理、计算机视觉和机器人技术）中解决通用任务的能力。<br />
<br />
LEO的独特之处在于它能够在3D环境中理解和执行基于语言的指令，这使其在多种复杂任务中具有广泛的应用潜力。<br />
<br />
主要功能和作用：<br />
<br />
1、3D视觉-语言理解：LEO能够理解3D环境中的视觉信息，并将其与语言描述相结合。这意味着它可以看到一个物体（如苹果），并理解相关的语言描述（如“这是一个苹果”）。<br />
<br />
2、执行语言指令：它能够根据语言指令执行动作。例如，如果有人指示“把苹果放在桌子上”，LEO能够理解这一指令并执行相应的动作。<br />
<br />
3、多样化的3D任务执行：LEO在多种3D任务上表现出色，包括3D字幕（为场景中的物体或事件创建描述性文字）、问答（回答关于3D环境的问题）、具身推理（在3D环境中进行逻辑推理）、具身导航（在3D空间中找到特定的位置或物体）和机器人操控（控制机器人执行特定任务）。<br />
<br />
为了训练LEO，研究者们创建了一个大型的数据集，包含了各种3D环境中的任务，这些任务需要深入理解和与3D世界的互动。<br />
<br />
LEO在多种3D任务上进行了测试，包括：<br />
<br />
•3D字幕：为3D场景中的物体或事件创建描述性文字。<br />
•问答：回答关于3D环境的问题。<br />
•具身推理：在3D环境中进行逻辑推理。<br />
•具身导航：在3D空间中找到特定的位置或物体。<br />
•机器人操控：控制机器人执行特定任务。<br />
<br />
LEO在这些任务上表现出色，显示了它在多样化任务中的熟练程度。<br />
<br />
工作原理：<br />
<br />
1、两阶段训练：<br />
<br />
LEO的训练分为两个阶段：<br />
<br />
•3D视觉-语言对齐：在这一阶段，LEO学习如何将3D图像与语言描述相结合。这涉及到理解视觉信息和语言信息之间的关系。<br />
•3D视觉-语言-动作指令调整：在这一阶段，LEO学习如何根据语言指令执行具体的动作。这需要它理解指令并将其转化为动作。<br />
<br />
2、大规模数据集：为了支持这种训练，研究者们创建了一个包含多种3D环境任务的大规模数据集。这些任务需要深入理解和与3D世界的互动。<br />
<br />
3、多模态学习：LEO结合了视觉信息（如图像和视频）和语言信息（如文字描述），使其能够在多模态环境中工作。<br />
<br />
4、广泛的应用能力：通过这种训练和数据集，LEO能够在多种3D任务中表现出色，展示了其广泛的应用能力。<br />
<br />
项目及演示：<a href="https://embodied-generalist.github.io/">embodied-generalist.github.i…</a><br />
论文：<a href="https://arxiv.org/abs/2311.12871">arxiv.org/abs/2311.12871</a><br />
GitHub：<a href="https://github.com/embodied-generalist/embodied-generalist">github.com/embodied-generali…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyNTUyNjk2MTUxNDkwNTYvcHUvaW1nL3JiYkcyNWV0aWhlQjNfVzYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728049969058423029#m</id>
            <title>DeepFace： 一个功能强大、易于使用的面部识别和分析工具

主要功能：

- 面部验证：验证两张面部图片是否属于同一人。

- 面部识别：在一个已知的面部数据库中查找输入图像的身份。

- 面部属性分析:：预测面部图像的年龄、性别、种族和情绪。

- 嵌入式表示：返回面部图像的多维向量表示(面部的关键特征)

它是一个混合面部识别框架，包装了多个先进的模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace、DeepID、ArcFace、Dlib 和 SFace。

这些模型已经达到或超过了人类在面部识别任务上的准确率（97.53%）。

DeepFace 还支持实时视频分析，并提供了一个 API，允许从外部系统（如移动应用或网页）调用其功能。此外，它还提供了命令行界面，方便用户在命令行中访问其功能。

主要优点：

1、高准确率：DeepFace 集成了多个先进的面部识别模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace 等，这些模型在面部识别任务上的准确率非常高，有的甚至达到或超过了人类的准确率（97.53%）。

2、多功能性：除了基本的面部识别和验证功能，DeepFace 还提供面部属性分析（如年龄、性别、种族和情绪预测），这使得它可以应用于更广泛的场景。

3、灵活性和兼容性：DeepFace 支持多种面部检测器和相似度计算方法，使其能够适应不同的应用需求和环境。

4、易用性：DeepFace 的安装和使用都相对简单，提供了 Python API 和命令行界面，方便不同背景的用户使用。

5、实时视频分析：DeepFace 还支持实时视频分析，这对于需要动态面部识别和分析的应用场景非常有用。

作者：@serengil
GitHub：https://github.com/serengil/deepface</title>
            <link>https://nitter.cz/xiaohuggg/status/1728049969058423029#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728049969058423029#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 13:56:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepFace： 一个功能强大、易于使用的面部识别和分析工具<br />
<br />
主要功能：<br />
<br />
- 面部验证：验证两张面部图片是否属于同一人。<br />
<br />
- 面部识别：在一个已知的面部数据库中查找输入图像的身份。<br />
<br />
- 面部属性分析:：预测面部图像的年龄、性别、种族和情绪。<br />
<br />
- 嵌入式表示：返回面部图像的多维向量表示(面部的关键特征)<br />
<br />
它是一个混合面部识别框架，包装了多个先进的模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace、DeepID、ArcFace、Dlib 和 SFace。<br />
<br />
这些模型已经达到或超过了人类在面部识别任务上的准确率（97.53%）。<br />
<br />
DeepFace 还支持实时视频分析，并提供了一个 API，允许从外部系统（如移动应用或网页）调用其功能。此外，它还提供了命令行界面，方便用户在命令行中访问其功能。<br />
<br />
主要优点：<br />
<br />
1、高准确率：DeepFace 集成了多个先进的面部识别模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace 等，这些模型在面部识别任务上的准确率非常高，有的甚至达到或超过了人类的准确率（97.53%）。<br />
<br />
2、多功能性：除了基本的面部识别和验证功能，DeepFace 还提供面部属性分析（如年龄、性别、种族和情绪预测），这使得它可以应用于更广泛的场景。<br />
<br />
3、灵活性和兼容性：DeepFace 支持多种面部检测器和相似度计算方法，使其能够适应不同的应用需求和环境。<br />
<br />
4、易用性：DeepFace 的安装和使用都相对简单，提供了 Python API 和命令行界面，方便不同背景的用户使用。<br />
<br />
5、实时视频分析：DeepFace 还支持实时视频分析，这对于需要动态面部识别和分析的应用场景非常有用。<br />
<br />
作者：<a href="https://nitter.cz/serengil" title="Sefik Ilkin Serengil">@serengil</a><br />
GitHub：<a href="https://github.com/serengil/deepface">github.com/serengil/deepface</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgwNDkxNDQyNTY5ODMwNDAvcHUvaW1nL1NQalJOQ0xuczVGTEdsMEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>