<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740566806069457215#m</id>
            <title>R to @xiaohuggg: 演示视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1740566806069457215#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740566806069457215#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 02:53:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDA1NjY2ODI1NjgyMTY1NzcvcHUvaW1nL2ZaWlYxWFhNQlI5RGZ3NXcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740564544660111865#m</id>
            <title>Spiritme AI ：一个有点类似Heygen，能帮你克隆虚拟形象的工具

你只需要通过iPhone随便拍摄一段大约5分钟的视频。它会用视频将用来捕捉你的外貌、动作和可能的表情。

然后克隆一个虚拟的你！

最牛的是它可以将PPT、PDF文件直接转换成解说视频，只需两次点击操作即可。

主要功能：

1、虚拟形象创建：使用iPhone拍摄一段大约5分钟的视频。这个视频将用来捕捉你的外貌、动作和可能的表情。制作这个视频时，你不需要有任何表演经验或特殊技能。简单地做自己就可以了。

2、文本到视频转换：允许用户输入文本，然后自动生成使用虚拟形象演说该文本的视频。虚拟形象可以根据视频的内容或脚本自然地展示各种表情。

3、AI拍摄助手：视频制作的自动化过程，比如自动调整拍摄角度、光线或其他视觉效果，以确保生成的视频具有高质量的视觉表现。

4、AI ScriptWriter功能：可以根据用户提供的信息或内容，如演示文稿、PDF文件，自动编写视频的对话或旁白，从而简化视频制作过程。只需两次点击操作。

5、多语言支持：用户可以生成多种语言的视频

体验：https://spiritme.tech/</title>
            <link>https://nitter.cz/xiaohuggg/status/1740564544660111865#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740564544660111865#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 02:45:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Spiritme AI ：一个有点类似Heygen，能帮你克隆虚拟形象的工具<br />
<br />
你只需要通过iPhone随便拍摄一段大约5分钟的视频。它会用视频将用来捕捉你的外貌、动作和可能的表情。<br />
<br />
然后克隆一个虚拟的你！<br />
<br />
最牛的是它可以将PPT、PDF文件直接转换成解说视频，只需两次点击操作即可。<br />
<br />
主要功能：<br />
<br />
1、虚拟形象创建：使用iPhone拍摄一段大约5分钟的视频。这个视频将用来捕捉你的外貌、动作和可能的表情。制作这个视频时，你不需要有任何表演经验或特殊技能。简单地做自己就可以了。<br />
<br />
2、文本到视频转换：允许用户输入文本，然后自动生成使用虚拟形象演说该文本的视频。虚拟形象可以根据视频的内容或脚本自然地展示各种表情。<br />
<br />
3、AI拍摄助手：视频制作的自动化过程，比如自动调整拍摄角度、光线或其他视觉效果，以确保生成的视频具有高质量的视觉表现。<br />
<br />
4、AI ScriptWriter功能：可以根据用户提供的信息或内容，如演示文稿、PDF文件，自动编写视频的对话或旁白，从而简化视频制作过程。只需两次点击操作。<br />
<br />
5、多语言支持：用户可以生成多种语言的视频<br />
<br />
体验：<a href="https://spiritme.tech/">spiritme.tech/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAzMzc5MTA4ODc2NjE1NjgvcHUvaW1nLzVQYTRkN0JYZWNydlpVd0YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740546965753217280#m</id>
            <title>英伟达发布了一款面向中国市场的新显卡：为RTX 4090D

这款显卡的起售价为12999元人民币，与RTX 4090的价格相同。

RTX 4090D在性能、效率和AI驱动的图形效果方面实现了显著的提升。它配备了24GB GDDR6X显存，基础频率高于RTX 4090，但由于综合运算性能的限制，其CUDA和Tensor核心数量低于RTX 4090。

RTX 4090 D的发布是英伟达针对美国新出口限制的回应，旨在为中国游戏玩家提供高性能的游戏体验。

这款显卡的CUDA核心数量从RTX 4090的16384个减至14592个，Tensor核心数量从512个减至456个，RT核心数量从128个减至114个。不过，RTX 4090 D的基础频率达到了2280 MHz，比RTX 4090的2235 MHz高，加速频率则与RTX 4090相同，均为2.52GHz。

这款显卡预计将在2024年1月中旬上架。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740546965753217280#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740546965753217280#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Dec 2023 01:35:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>英伟达发布了一款面向中国市场的新显卡：为RTX 4090D<br />
<br />
这款显卡的起售价为12999元人民币，与RTX 4090的价格相同。<br />
<br />
RTX 4090D在性能、效率和AI驱动的图形效果方面实现了显著的提升。它配备了24GB GDDR6X显存，基础频率高于RTX 4090，但由于综合运算性能的限制，其CUDA和Tensor核心数量低于RTX 4090。<br />
<br />
RTX 4090 D的发布是英伟达针对美国新出口限制的回应，旨在为中国游戏玩家提供高性能的游戏体验。<br />
<br />
这款显卡的CUDA核心数量从RTX 4090的16384个减至14592个，Tensor核心数量从512个减至456个，RT核心数量从128个减至114个。不过，RTX 4090 D的基础频率达到了2280 MHz，比RTX 4090的2235 MHz高，加速频率则与RTX 4090相同，均为2.52GHz。<br />
<br />
这款显卡预计将在2024年1月中旬上架。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NlcU80R2FBQUEtQk1ILmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NlcU80SGF3QUFibF9XLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NlcU80TWJBQUFvQkttLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740388169424814315#m</id>
            <title>😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1740388169424814315#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740388169424814315#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 15:04:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>😂</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQwMzg4MTQ5Nzc4NjYxMzc2L2ltZy9wT0tMUHpnVjBvRmhQNmJsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740378222200828408#m</id>
            <title>R to @xiaohuggg: 手机版太糊了，建议电脑看

马斯克特么的什么时候能解决这个视频糊的问题

2024年了，视频还这个德行！

怎么混啊</title>
            <link>https://nitter.cz/xiaohuggg/status/1740378222200828408#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740378222200828408#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 14:24:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>手机版太糊了，建议电脑看<br />
<br />
马斯克特么的什么时候能解决这个视频糊的问题<br />
<br />
2024年了，视频还这个德行！<br />
<br />
怎么混啊</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740375887613501770#m</id>
            <title>左边是超过30个工作人员，使用高速摄像机拍摄的广告片，制作时间约为一个月。

右边是用Pika翻拍的，使用了文本、图片到视频生成的。

当然视频是Pika的创意总监@MatanCohenGrumi 制作的，可能比较熟练。

但是这代表了一个方向，明年估计AI视频领域会诞生一部真正的AI电影！😀</title>
            <link>https://nitter.cz/xiaohuggg/status/1740375887613501770#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740375887613501770#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 14:15:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>左边是超过30个工作人员，使用高速摄像机拍摄的广告片，制作时间约为一个月。<br />
<br />
右边是用Pika翻拍的，使用了文本、图片到视频生成的。<br />
<br />
当然视频是Pika的创意总监<a href="https://nitter.cz/MatanCohenGrumi" title="Matan Cohen-Grumi">@MatanCohenGrumi</a> 制作的，可能比较熟练。<br />
<br />
但是这代表了一个方向，明年估计AI视频领域会诞生一部真正的AI电影！😀</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAzNjY4OTE2NzQzNzAwNDgvcHUvaW1nL2w0a3g4bFFVYUR5OS1hckkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740360789742268498#m</id>
            <title>近日麻省理工大学科学家们利用人工智能技术取得了突破性的发现，发现了一种新的抗生素类别，用于对抗耐药性金黄色葡萄球菌（MRSA）。

科学家们在抗生素研究方面已经有60多年没有取得重要的进展了。

这是60 年来发现的首个新型抗生素。

该研究结果发表在了《自然》杂志上。https://www.nature.com/articles/s41586-023-06887-8

麻省理工大学的研究团队训练了一个扩大的深度学习模型，使用扩展的数据集，评估了约39000种化合物对MRSA的抗生素活性。

这种方法结合了抗微生物活性预测和毒性评估，以识别对人体细胞危害最小的有效化合物。

- 活性和毒性预测的整合： 模型不仅预测了哪些化合物可能作为抗生素有效，还评估了它们对人类细胞的潜在毒性。这种双重方法有助于识别既对细菌有效又对人类安全的化合物。

- 筛选数百万种化合物：使用训练有素的AI模型，研究人员筛选了大约1200万种现有的商业化合物。如果没有AI的帮助，由于所需的时间和资源，这种大规模筛选几乎是不切实际的。

- 发现新的抗生素候选物：随后，研究人员获得了大约 280 种此类化合物，并在实验室环境中进行了针对 MRSA 的测试。

这种方法使他们从同一类别中识别出两种有前途的候选抗生素。

在涉及两种小鼠模型的实验中——一种用于 MRSA 皮肤感染，另一种用于 MRSA 全身感染——这些化合物中的每一种都将 MRSA 数量减少了 10 倍。

 - 打开“黑箱”：研究团队的目标是揭开深度学习模型的“黑箱”。这些模型包含大量模拟神经连接的计算，而人们通常不清楚其内部运作机制。通过这项研究，他们能够更深入地了解模型是如何学习并预测某些分子会成为良好抗生素的。

扩展资料：

耐甲氧西林的金黄色葡萄球菌（MRSA）是一种特别的细菌，对多种常用抗生素有抵抗力，这使得它引起的感染难以治疗。

根据欧洲疾病预防和控制中心 (ECDC) 的数据，欧盟每年发生近 150000 例 MRSA 感染，而该地区每年有近 35000 人死于抗菌药物耐药性感染。

MRSA感染的特点和严重性如下：

1 .多种感染： MRSA可以导致多种类型的感染。它通常感染皮肤，但也可以影响其他身体部位。

2 .从轻微到严重： MRSA感染的严重程度可以从轻微的皮肤感染（如疖子或脓肿）发展到更严重的健康问题。在一些情况下，它可能引起深层组织感染，甚至是生命威胁性的疾病，如肺炎和血流感染。

3 .耐药性： MRSA之所以难以治疗，主要是因为它对多种常用的抗生素，包括甲氧西林、青霉素和其他β-内酰胺类抗生素产生了耐药性。这意味着这些通常有效的药物对MRSA不再有效。

4 .医院和社区感染： MRSA感染可以在医院（医院获得性MRSA）和日常社区环境（社区获得性MRSA）中发生。医院获得性MRSA通常更严重，因为它影响的是已经有健康问题的患者。

5 .预防和控制： 由于MRSA的耐药性，预防和控制措施非常重要。这包括良好的卫生习惯、正确使用抗生素和在医院中采取感染控制措施。

因此，针对MRSA的新抗生素的发现对于医学领域来说是一个重要的进步，因为它提供了一个新的工具来对抗这种难以治疗的细菌。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740360789742268498#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740360789742268498#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 13:15:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近日麻省理工大学科学家们利用人工智能技术取得了突破性的发现，发现了一种新的抗生素类别，用于对抗耐药性金黄色葡萄球菌（MRSA）。<br />
<br />
科学家们在抗生素研究方面已经有60多年没有取得重要的进展了。<br />
<br />
这是60 年来发现的首个新型抗生素。<br />
<br />
该研究结果发表在了《自然》杂志上。<a href="https://www.nature.com/articles/s41586-023-06887-8">nature.com/articles/s41586-0…</a><br />
<br />
麻省理工大学的研究团队训练了一个扩大的深度学习模型，使用扩展的数据集，评估了约39000种化合物对MRSA的抗生素活性。<br />
<br />
这种方法结合了抗微生物活性预测和毒性评估，以识别对人体细胞危害最小的有效化合物。<br />
<br />
- 活性和毒性预测的整合： 模型不仅预测了哪些化合物可能作为抗生素有效，还评估了它们对人类细胞的潜在毒性。这种双重方法有助于识别既对细菌有效又对人类安全的化合物。<br />
<br />
- 筛选数百万种化合物：使用训练有素的AI模型，研究人员筛选了大约1200万种现有的商业化合物。如果没有AI的帮助，由于所需的时间和资源，这种大规模筛选几乎是不切实际的。<br />
<br />
- 发现新的抗生素候选物：随后，研究人员获得了大约 280 种此类化合物，并在实验室环境中进行了针对 MRSA 的测试。<br />
<br />
这种方法使他们从同一类别中识别出两种有前途的候选抗生素。<br />
<br />
在涉及两种小鼠模型的实验中——一种用于 MRSA 皮肤感染，另一种用于 MRSA 全身感染——这些化合物中的每一种都将 MRSA 数量减少了 10 倍。<br />
<br />
 - 打开“黑箱”：研究团队的目标是揭开深度学习模型的“黑箱”。这些模型包含大量模拟神经连接的计算，而人们通常不清楚其内部运作机制。通过这项研究，他们能够更深入地了解模型是如何学习并预测某些分子会成为良好抗生素的。<br />
<br />
扩展资料：<br />
<br />
耐甲氧西林的金黄色葡萄球菌（MRSA）是一种特别的细菌，对多种常用抗生素有抵抗力，这使得它引起的感染难以治疗。<br />
<br />
根据欧洲疾病预防和控制中心 (ECDC) 的数据，欧盟每年发生近 150000 例 MRSA 感染，而该地区每年有近 35000 人死于抗菌药物耐药性感染。<br />
<br />
MRSA感染的特点和严重性如下：<br />
<br />
1 .多种感染： MRSA可以导致多种类型的感染。它通常感染皮肤，但也可以影响其他身体部位。<br />
<br />
2 .从轻微到严重： MRSA感染的严重程度可以从轻微的皮肤感染（如疖子或脓肿）发展到更严重的健康问题。在一些情况下，它可能引起深层组织感染，甚至是生命威胁性的疾病，如肺炎和血流感染。<br />
<br />
3 .耐药性： MRSA之所以难以治疗，主要是因为它对多种常用的抗生素，包括甲氧西林、青霉素和其他β-内酰胺类抗生素产生了耐药性。这意味着这些通常有效的药物对MRSA不再有效。<br />
<br />
4 .医院和社区感染： MRSA感染可以在医院（医院获得性MRSA）和日常社区环境（社区获得性MRSA）中发生。医院获得性MRSA通常更严重，因为它影响的是已经有健康问题的患者。<br />
<br />
5 .预防和控制： 由于MRSA的耐药性，预防和控制措施非常重要。这包括良好的卫生习惯、正确使用抗生素和在医院中采取感染控制措施。<br />
<br />
因此，针对MRSA的新抗生素的发现对于医学领域来说是一个重要的进步，因为它提供了一个新的工具来对抗这种难以治疗的细菌。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiX0pNX2J3QUFaUlhELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740314433380127017#m</id>
            <title>R to @xiaohuggg: 百公里加速时间为2.78秒，最高时速265km/h。

 CLTC续航里程达800km，800V超级快充实现充电5分钟，续航220km，15分钟充电实现510km续航。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740314433380127017#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740314433380127017#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 10:11:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>百公里加速时间为2.78秒，最高时速265km/h。<br />
<br />
 CLTC续航里程达800km，800V超级快充实现充电5分钟，续航220km，15分钟充电实现510km续航。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV2xZUWFvQUFkb3czLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV240amFVQUFmWnZuLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV3FsQ2E4QUFibE0wLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV3ZGMWFBQUFaU0s3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740313720226812162#m</id>
            <title>兄弟们

我看了实车

确实很漂亮，甚至比保时捷还好看！

如果屁股后面的字能拿掉就挺好了，据说售价是30以内！低配版可能19.9😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1740313720226812162#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740313720226812162#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 10:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们<br />
<br />
我看了实车<br />
<br />
确实很漂亮，甚至比保时捷还好看！<br />
<br />
如果屁股后面的字能拿掉就挺好了，据说售价是30以内！低配版可能19.9😐</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JNdWJvQUFWT19SLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JMOGJnQUFlX1RZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JMX2JZQUFHV0g0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NiV0JMMmIwQUFWS3plLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740309636811755941#m</id>
            <title>UniRef++：在图片或视频中找到并标记出特定的物体

它牛P之处在于，你可以用文字来描述你想在图像或视频中要找的东西，它能帮你定位并标记出来。

比如，你可以说“一只坐在草地上的狗”。UniRef++会理解这个描述，并在图像或视频中找到并标记出符合这个描述的狗。

如果你有一张图像，上面已经标记了你感兴趣的物体（比如一张狗的照片，狗的部分已经被圈出来了），你也可以用这张图像来告诉UniRef++你想找的是什么。

UniRef++会使用这张标记好的图像作为参考，在其他图像或视频中找到并标记出类似的狗。

总的来说，UniRef++能够理解你的文字描述或参考图像，然后在其他图像或视频中找到并标记出你想要的物体。这对于自动图像编辑、视频内容分析等应用非常有用。

UniRef++的主要功能特点：

1、多任务统一处理：UniRef++能够处理多种基于参考的对象分割任务，包括图像分割、少样本图像分割、视频中的对象分割等。这意味着它可以使用同一套技术来处理不同类型的图像和视频分析任务。不管你是要在一张静态的图片中找东西，还是要在一段动态的视频中追踪某个东西，UniRef++都能帮忙。

2、灵活的参考处理：它可以使用多种类型的参考来指导分割任务，包括语言描述（如文字说明一个对象是什么）和标注的掩膜（即图像中已经标记出的特定区域）。

3、实时处理能力：尤其在处理视频对象分割时，UniRef++能够实时跟踪和分割视频中的对象，这对于动态场景分析非常重要。在视频中，它可以实时追踪物体，即使物体在移动也没问题。

4、高效性能：UniRef++在多个基准测试中展现了优秀的性能，特别是在图像和视频对象分割方面，它能够与或超过当前的最先进技术。

技术原理：

1、UniFusion模块：这是UniRef++的核心组件，负责将不同类型的参考信息（如语言描述或图像掩膜）融合到图像处理流程中。这种融合方式使得模型能够更准确地理解和定位要分割的对象。

2、基于Transformer的架构：UniRef++使用了Transformer模型，这是一种强大的深度学习架构，通常用于处理语言数据。在UniRef++中，Transformer被用来处理图像和视频数据，以实现精确的对象识别和分割。

3、多向融合策略：该模型采用了一种多向融合策略，可以根据任务的不同（如图像分割或视频对象跟踪）灵活地处理不同类型的输入和参考信息。

4、实例级分割：UniRef++将这些任务视为实例级分割问题，即不仅仅是识别出图像中的对象，还要精确地分割出每个实例（即单个对象）。

GitHub：https://github.com/FoundationVision/UniRef
论文：https://arxiv.org/abs/2312.15715</title>
            <link>https://nitter.cz/xiaohuggg/status/1740309636811755941#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740309636811755941#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 09:52:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>UniRef++：在图片或视频中找到并标记出特定的物体<br />
<br />
它牛P之处在于，你可以用文字来描述你想在图像或视频中要找的东西，它能帮你定位并标记出来。<br />
<br />
比如，你可以说“一只坐在草地上的狗”。UniRef++会理解这个描述，并在图像或视频中找到并标记出符合这个描述的狗。<br />
<br />
如果你有一张图像，上面已经标记了你感兴趣的物体（比如一张狗的照片，狗的部分已经被圈出来了），你也可以用这张图像来告诉UniRef++你想找的是什么。<br />
<br />
UniRef++会使用这张标记好的图像作为参考，在其他图像或视频中找到并标记出类似的狗。<br />
<br />
总的来说，UniRef++能够理解你的文字描述或参考图像，然后在其他图像或视频中找到并标记出你想要的物体。这对于自动图像编辑、视频内容分析等应用非常有用。<br />
<br />
UniRef++的主要功能特点：<br />
<br />
1、多任务统一处理：UniRef++能够处理多种基于参考的对象分割任务，包括图像分割、少样本图像分割、视频中的对象分割等。这意味着它可以使用同一套技术来处理不同类型的图像和视频分析任务。不管你是要在一张静态的图片中找东西，还是要在一段动态的视频中追踪某个东西，UniRef++都能帮忙。<br />
<br />
2、灵活的参考处理：它可以使用多种类型的参考来指导分割任务，包括语言描述（如文字说明一个对象是什么）和标注的掩膜（即图像中已经标记出的特定区域）。<br />
<br />
3、实时处理能力：尤其在处理视频对象分割时，UniRef++能够实时跟踪和分割视频中的对象，这对于动态场景分析非常重要。在视频中，它可以实时追踪物体，即使物体在移动也没问题。<br />
<br />
4、高效性能：UniRef++在多个基准测试中展现了优秀的性能，特别是在图像和视频对象分割方面，它能够与或超过当前的最先进技术。<br />
<br />
技术原理：<br />
<br />
1、UniFusion模块：这是UniRef++的核心组件，负责将不同类型的参考信息（如语言描述或图像掩膜）融合到图像处理流程中。这种融合方式使得模型能够更准确地理解和定位要分割的对象。<br />
<br />
2、基于Transformer的架构：UniRef++使用了Transformer模型，这是一种强大的深度学习架构，通常用于处理语言数据。在UniRef++中，Transformer被用来处理图像和视频数据，以实现精确的对象识别和分割。<br />
<br />
3、多向融合策略：该模型采用了一种多向融合策略，可以根据任务的不同（如图像分割或视频对象跟踪）灵活地处理不同类型的输入和参考信息。<br />
<br />
4、实例级分割：UniRef++将这些任务视为实例级分割问题，即不仅仅是识别出图像中的对象，还要精确地分割出每个实例（即单个对象）。<br />
<br />
GitHub：<a href="https://github.com/FoundationVision/UniRef">github.com/FoundationVision/…</a><br />
论文：<a href="https://arxiv.org/abs/2312.15715">arxiv.org/abs/2312.15715</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAyMjA3ODI2NjI2MTkxMzYvcHUvaW1nL1gzMldjdXk5YTZWczE3anEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740257062637191667#m</id>
            <title>来参加小米汽车发布会了

听雷总吹牛p

一开场就调子很高，很期待价格🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1740257062637191667#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740257062637191667#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 06:23:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来参加小米汽车发布会了<br />
<br />
听雷总吹牛p<br />
<br />
一开场就调子很高，很期待价格🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWliS2FNQUVlWXVJLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWlhRGFnQUFTTXVPLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWlaLWEwQUVSX0J3LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NhaWlhRWFzQUFFeTJ5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740212612036751744#m</id>
            <title>R to @xiaohuggg: 演示视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1740212612036751744#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740212612036751744#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 03:26:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAyMTIyODgwNjQ1MDM4MDgvcHUvaW1nL3cyQnk3cTMyV1NvSllxbDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740212609549541884#m</id>
            <title>Assistive Video：一个新的AI生成视频的工具

只需输入描述所想看到内容的提示或上传图片，模型便会生成一段4秒钟的视频。

你可以控制视频质量、与提示的一致性、运动的强度，设置种子等...

从宣传视频来看还是很不错，挺高清的。。。

体验地址：https://assistive.chat/product/video</title>
            <link>https://nitter.cz/xiaohuggg/status/1740212609549541884#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740212609549541884#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 03:26:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Assistive Video：一个新的AI生成视频的工具<br />
<br />
只需输入描述所想看到内容的提示或上传图片，模型便会生成一段4秒钟的视频。<br />
<br />
你可以控制视频质量、与提示的一致性、运动的强度，设置种子等...<br />
<br />
从宣传视频来看还是很不错，挺高清的。。。<br />
<br />
体验地址：<a href="https://assistive.chat/product/video">assistive.chat/product/video</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDAyMTIyMTI5MTkzMzI4NjQvcHUvaW1nLzRkRG5HTlN5eEltMUVpOW0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1740094346211549275#m</id>
            <title>RT by @xiaohuggg: 如果你觉得Whisper在识别中文语音的时候幻觉严重，不妨试试阿里达摩院的Paraformer模型，对中文应该支持更好！

项目地址：https://github.com/alibaba-damo-academy/FunASR
中文说明：https://github.com/alibaba-damo-academy/FunASR/blob/main/README_zh.md</title>
            <link>https://nitter.cz/dotey/status/1740094346211549275#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1740094346211549275#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 19:36:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果你觉得Whisper在识别中文语音的时候幻觉严重，不妨试试阿里达摩院的Paraformer模型，对中文应该支持更好！<br />
<br />
项目地址：<a href="https://github.com/alibaba-damo-academy/FunASR">github.com/alibaba-damo-acad…</a><br />
中文说明：<a href="https://github.com/alibaba-damo-academy/FunASR/blob/main/README_zh.md">github.com/alibaba-damo-acad…</a></p>
<p><a href="https://nitter.cz/hylarucoder/status/1739494196921483663#m">nitter.cz/hylarucoder/status/1739494196921483663#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NZT2l0VVdnQUFSZWlELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740185246212239650#m</id>
            <title>知名律师解读纽约时报起诉OpenAI侵犯其版权，用于训练AI模型

她表示纽约时报提供的证据很充分，而且OpenAI似乎使用了纽约时报大量的内容作为一个单一数据集进行了AI训练！

她认为该案具有历史意义，可能是人工智能和版权的分水岭。🤔

引发连锁反应…</title>
            <link>https://nitter.cz/xiaohuggg/status/1740185246212239650#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740185246212239650#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 01:37:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>知名律师解读纽约时报起诉OpenAI侵犯其版权，用于训练AI模型<br />
<br />
她表示纽约时报提供的证据很充分，而且OpenAI似乎使用了纽约时报大量的内容作为一个单一数据集进行了AI训练！<br />
<br />
她认为该案具有历史意义，可能是人工智能和版权的分水岭。🤔<br />
<br />
引发连锁反应…</p>
<p><a href="https://nitter.cz/CeciliaZin/status/1740109462319644905#m">nitter.cz/CeciliaZin/status/1740109462319644905#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740175634280972462#m</id>
            <title>纽约时报要求OpenAI和微软赔偿数十亿美元，并销毁使用《纽约时报》内容训练的人工智能模型。😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1740175634280972462#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740175634280972462#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Dec 2023 00:59:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>纽约时报要求OpenAI和微软赔偿数十亿美元，并销毁使用《纽约时报》内容训练的人工智能模型。😂</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1740008017448559006#m">nitter.cz/xiaohuggg/status/1740008017448559006#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740044951961162093#m</id>
            <title>R to @xiaohuggg: x.com/chaseleantj/status/173…</title>
            <link>https://nitter.cz/xiaohuggg/status/1740044951961162093#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740044951961162093#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 16:20:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://x.com/chaseleantj/status/1739975317224362200?s=46">x.com/chaseleantj/status/173…</a></p>
<p><a href="https://nitter.cz/chaseleantj/status/1739975317224362200#m">nitter.cz/chaseleantj/status/1739975317224362200#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740012571263209962#m</id>
            <title>Chase Lean 在 Twitter 上分享了关于如何使用 Midjourney 制作矢量插图的技巧。

他提到，你可以使用 Midjourney 制作多种风格的平面矢量图。

他给出了一个示例提示和技巧：

1、使用“stylize”参数：这个参数控制插图的风格。默认值是100。通过调整这个值，你可以改变插图的外观和风格。低“stylize”值（例如 0）：会产生更简洁、极简的图像。

在这种设置下，Midjourney 更严格地遵循你的指令，产生更干净、简化的图像。

高“stylize”值（例如 1000）：会产生更详细、艺术化的图像。但这也可能导致图像变得更混乱，且 Midjourney 可能不会完全遵循你的指令。

2、理解 Midjourney v5 与 v6 的区别：Midjourney v6：默认情况下，比 v5 更详细和艺术化。但它在处理极简图像方面可能有所挑战。

3、改善极简图像的方法：如果你想在 v6 中制作极简风格的图像，可以使用低 stylize 值（0到100之间），在提示中添加“极简主义”和“纯白背景”，并在提示末尾添加“--style raw”。

4、转换为 .svg 文件：完成插图后，可以使用 http://vectorizer.ai 将图像转换为 .svg 文件，这对于进一步的编辑和使用非常有用。</title>
            <link>https://nitter.cz/xiaohuggg/status/1740012571263209962#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740012571263209962#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 14:11:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chase Lean 在 Twitter 上分享了关于如何使用 Midjourney 制作矢量插图的技巧。<br />
<br />
他提到，你可以使用 Midjourney 制作多种风格的平面矢量图。<br />
<br />
他给出了一个示例提示和技巧：<br />
<br />
1、使用“stylize”参数：这个参数控制插图的风格。默认值是100。通过调整这个值，你可以改变插图的外观和风格。低“stylize”值（例如 0）：会产生更简洁、极简的图像。<br />
<br />
在这种设置下，Midjourney 更严格地遵循你的指令，产生更干净、简化的图像。<br />
<br />
高“stylize”值（例如 1000）：会产生更详细、艺术化的图像。但这也可能导致图像变得更混乱，且 Midjourney 可能不会完全遵循你的指令。<br />
<br />
2、理解 Midjourney v5 与 v6 的区别：Midjourney v6：默认情况下，比 v5 更详细和艺术化。但它在处理极简图像方面可能有所挑战。<br />
<br />
3、改善极简图像的方法：如果你想在 v6 中制作极简风格的图像，可以使用低 stylize 值（0到100之间），在提示中添加“极简主义”和“纯白背景”，并在提示末尾添加“--style raw”。<br />
<br />
4、转换为 .svg 文件：完成插图后，可以使用 <a href="http://vectorizer.ai">vectorizer.ai</a> 将图像转换为 .svg 文件，这对于进一步的编辑和使用非常有用。</p>
<p><a href="https://nitter.cz/chaseleantj/status/1739978741798130002#m">nitter.cz/chaseleantj/status/1739978741798130002#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NYRU5MeGF3QUVjZmJZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1740008017448559006#m</id>
            <title>糟了...

纽约时报和多位普利策获奖作者起诉微软和 OpenAI 侵犯版权

纽约时报指控微软和 OpenAI 在未经许可的情况下使用其版权内容（包括数百万篇文章）来训练他们的人工智能工具。

除了纽约时报之外，还有一些普利策奖获奖作者和其他非小说类作家加入了这一诉讼。

指控 OpenAI 和微软滥用他们的书籍内容训练 OpenAI 的 GPT 大语言模型，侵犯了他们的版权。

前几天苹果花了5000w美金从几大新闻机构买版权估计是刺激到其他人了，看到有利可图了，然后他们开始起诉了！哈哈哈...

下一个我觉得是Midjourney...

华尔街日报 - 《纽约时报》起诉微软和OpenAI侵犯版权：https://www.wsj.com/tech/ai/new-york-times-sues-microsoft-and-openai-alleging-copyright-infringement-fd85e1c4

路透社 - 普利策获奖作者加入 OpenAI 和微软版权诉讼：https://www.reuters.com/legal/pulitzer-winning-authors-join-openai-microsoft-copyright-lawsuit-2023-12-20/</title>
            <link>https://nitter.cz/xiaohuggg/status/1740008017448559006#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1740008017448559006#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Dec 2023 13:53:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>糟了...<br />
<br />
纽约时报和多位普利策获奖作者起诉微软和 OpenAI 侵犯版权<br />
<br />
纽约时报指控微软和 OpenAI 在未经许可的情况下使用其版权内容（包括数百万篇文章）来训练他们的人工智能工具。<br />
<br />
除了纽约时报之外，还有一些普利策奖获奖作者和其他非小说类作家加入了这一诉讼。<br />
<br />
指控 OpenAI 和微软滥用他们的书籍内容训练 OpenAI 的 GPT 大语言模型，侵犯了他们的版权。<br />
<br />
前几天苹果花了5000w美金从几大新闻机构买版权估计是刺激到其他人了，看到有利可图了，然后他们开始起诉了！哈哈哈...<br />
<br />
下一个我觉得是Midjourney...<br />
<br />
华尔街日报 - 《纽约时报》起诉微软和OpenAI侵犯版权：<a href="https://www.wsj.com/tech/ai/new-york-times-sues-microsoft-and-openai-alleging-copyright-infringement-fd85e1c4">wsj.com/tech/ai/new-york-tim…</a><br />
<br />
路透社 - 普利策获奖作者加入 OpenAI 和微软版权诉讼：<a href="https://www.reuters.com/legal/pulitzer-winning-authors-join-openai-microsoft-copyright-lawsuit-2023-12-20/">reuters.com/legal/pulitzer-w…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NXX1loVmJ3QUFfNmFhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>