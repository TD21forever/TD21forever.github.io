<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729693538613629377#m</id>
            <title>昨晚Pika发布了新的AI视频模型 很炸裂，但是让我更炸裂的是这家公司，很神奇...

- Pika公司目前只有4个人（包括俩华裔女老板）
- 公司今年4月份才成立
- 已经连续完成三轮融资，5500万美元
- 俩创始人曾参加Runway的“AI电影节”比赛，但没入围
- 明年计划团队人数扩充到20人😂
- 投资人建议他们添加一种将文本嵌入视频的方式。凌晨3点他收到一条短信，说这个功能已经准备好了。

创始团队：Demi Guo（CEO）和 Chenlin Meng（CTO）

Demi Guo 郭文景毕业于杭州二中，被哈佛提前录取，父母是 MIT 博士，斯坦福大学的计算机科学博士生。

Chenlin Meng 孟晨琳斯坦福大学的计算机科学博士生，研究领域：涵盖了多个方面，包括图像合成与编辑、扩散模型的简化、自回归模型的改进等。

创业初衷：在尝试使用生成性AI制作电影并参加Runway的“AI电影节”时，Guo和她的团队遇到了困难。尽管技术团队很强，但他们在制作电影方面遇到了挑战，这激发了他们创建更易于使用的AI视频生成工具的想法。

Pika的成立：2023年4月，Guo和Meng离开斯坦福大学，成立了Pika，专注于开发易于使用的AI视频生成器。

快速增长：自成立以来，Pika迅速吸引了500,000用户尝试其软件，并且每周产生数百万新视频。这种迅速增长引起了硅谷投资者的极大兴趣。

融资成功：Pika在短时间内连续完成了三轮融资，总计筹集了5500万美元。其中，第一和第二轮融资由前GitHub CEO Nat Friedman领投，最近的一轮融资（3500万美元的A轮）由Lightspeed Venture Partners领投，使Pika的估值达到2亿至3亿美元。

产品发展：Pika最初只专注于生成动漫视频。随后，他们扩展了产品功能，包括在视频中嵌入文本和编辑视频中的对象（例如给机器人添加太阳镜）。

未来规划：Pika目前正在使用几百个GPU构建一个新版本的AI模型，以提供更好的性能和更精细的编辑功能。同时，公司也在研发用于过滤版权材料的算法。

团队扩张：随着新一轮融资的完成，Guo计划明年将Pika的团队扩大到大约20人，主要是工程师和研究人员。

信息来源：https://www.forbes.com/sites/kenrickcai/2023/11/27/pika-ai-video-generator-editor-series-a/?sh=e7fd5ae421bd</title>
            <link>https://nitter.cz/xiaohuggg/status/1729693538613629377#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729693538613629377#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 02:47:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨晚Pika发布了新的AI视频模型 很炸裂，但是让我更炸裂的是这家公司，很神奇...<br />
<br />
- Pika公司目前只有4个人（包括俩华裔女老板）<br />
- 公司今年4月份才成立<br />
- 已经连续完成三轮融资，5500万美元<br />
- 俩创始人曾参加Runway的“AI电影节”比赛，但没入围<br />
- 明年计划团队人数扩充到20人😂<br />
- 投资人建议他们添加一种将文本嵌入视频的方式。凌晨3点他收到一条短信，说这个功能已经准备好了。<br />
<br />
创始团队：Demi Guo（CEO）和 Chenlin Meng（CTO）<br />
<br />
Demi Guo 郭文景毕业于杭州二中，被哈佛提前录取，父母是 MIT 博士，斯坦福大学的计算机科学博士生。<br />
<br />
Chenlin Meng 孟晨琳斯坦福大学的计算机科学博士生，研究领域：涵盖了多个方面，包括图像合成与编辑、扩散模型的简化、自回归模型的改进等。<br />
<br />
创业初衷：在尝试使用生成性AI制作电影并参加Runway的“AI电影节”时，Guo和她的团队遇到了困难。尽管技术团队很强，但他们在制作电影方面遇到了挑战，这激发了他们创建更易于使用的AI视频生成工具的想法。<br />
<br />
Pika的成立：2023年4月，Guo和Meng离开斯坦福大学，成立了Pika，专注于开发易于使用的AI视频生成器。<br />
<br />
快速增长：自成立以来，Pika迅速吸引了500,000用户尝试其软件，并且每周产生数百万新视频。这种迅速增长引起了硅谷投资者的极大兴趣。<br />
<br />
融资成功：Pika在短时间内连续完成了三轮融资，总计筹集了5500万美元。其中，第一和第二轮融资由前GitHub CEO Nat Friedman领投，最近的一轮融资（3500万美元的A轮）由Lightspeed Venture Partners领投，使Pika的估值达到2亿至3亿美元。<br />
<br />
产品发展：Pika最初只专注于生成动漫视频。随后，他们扩展了产品功能，包括在视频中嵌入文本和编辑视频中的对象（例如给机器人添加太阳镜）。<br />
<br />
未来规划：Pika目前正在使用几百个GPU构建一个新版本的AI模型，以提供更好的性能和更精细的编辑功能。同时，公司也在研发用于过滤版权材料的算法。<br />
<br />
团队扩张：随着新一轮融资的完成，Guo计划明年将Pika的团队扩大到大约20人，主要是工程师和研究人员。<br />
<br />
信息来源：<a href="https://www.forbes.com/sites/kenrickcai/2023/11/27/pika-ai-video-generator-editor-series-a/?sh=e7fd5ae421bd">forbes.com/sites/kenrickcai/…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729538310136348926#m">nitter.cz/xiaohuggg/status/1729538310136348926#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FFVnpaVGEwQUFqZVFaLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729680868560793901#m</id>
            <title>Stability AI 推出实时文本图像生成模型 ：SDXL Turbo 

它能够快速从文本描述中生成高质量的图像，同时减少计算量和生成时间。 可以实现秒级和毫秒级别出图！

SDXL Turbo 通过新的蒸馏技术实现了最先进的性能，将所需的步骤数量从 50 个减少到 1 个，从而实现前所未有质量的单步图像快速生成。

SDXL Turbo的推理速度大幅提高。A100的情况下，SDXL Turbo以207ms生成512x512的图像！

现在可以在Clipdrop上免费试用：https://clipdrop.co/stable-diffusion-turbo

在性能方面，SDXL Turbo通过与多个不同模型（如StyleGAN-T++、OpenMUSE、IF-XL、SDXL、LCM-XL）进行比较测试，展示了其优势。在这些测试中，人类评估者被要求选择与给定提示最匹配的输出，并评估图像质量。

SDXL Turbo在这些盲测中表现出色，能够在较少的步骤中超越LCM-XL和SDXL等多步骤模型，同时不牺牲图像质量，显著减少了计算量。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729680868560793901#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729680868560793901#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 01:57:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI 推出实时文本图像生成模型 ：SDXL Turbo <br />
<br />
它能够快速从文本描述中生成高质量的图像，同时减少计算量和生成时间。 可以实现秒级和毫秒级别出图！<br />
<br />
SDXL Turbo 通过新的蒸馏技术实现了最先进的性能，将所需的步骤数量从 50 个减少到 1 个，从而实现前所未有质量的单步图像快速生成。<br />
<br />
SDXL Turbo的推理速度大幅提高。A100的情况下，SDXL Turbo以207ms生成512x512的图像！<br />
<br />
现在可以在Clipdrop上免费试用：<a href="https://clipdrop.co/stable-diffusion-turbo">clipdrop.co/stable-diffusion…</a><br />
<br />
在性能方面，SDXL Turbo通过与多个不同模型（如StyleGAN-T++、OpenMUSE、IF-XL、SDXL、LCM-XL）进行比较测试，展示了其优势。在这些测试中，人类评估者被要求选择与给定提示最匹配的输出，并评估图像质量。<br />
<br />
SDXL Turbo在这些盲测中表现出色，能够在较少的步骤中超越LCM-XL和SDXL等多步骤模型，同时不牺牲图像质量，显著减少了计算量。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NjgwODEwMjA5NjY5MTIwL2ltZy9CajBxLWRJNXJyZ3VwN0V2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729538310136348926#m</id>
            <title>炸裂了💥

Pika 1.0发布重大的产品升级

发布了一个新的AI模型，能够使用文本生成和编辑多种风格的视频，如3D动画、动漫、卡通和电影风格。

质量非常高！

而且还难对视频内容进行精准的控制和编辑，例如调整视频的宽高比、更改视频中人物的衣服，给猩猩戴墨镜…

还没正式发布，现在可以排队：https://pika.art/</title>
            <link>https://nitter.cz/xiaohuggg/status/1729538310136348926#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729538310136348926#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 16:30:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>炸裂了💥<br />
<br />
Pika 1.0发布重大的产品升级<br />
<br />
发布了一个新的AI模型，能够使用文本生成和编辑多种风格的视频，如3D动画、动漫、卡通和电影风格。<br />
<br />
质量非常高！<br />
<br />
而且还难对视频内容进行精准的控制和编辑，例如调整视频的宽高比、更改视频中人物的衣服，给猩猩戴墨镜…<br />
<br />
还没正式发布，现在可以排队：<a href="https://pika.art/">pika.art/</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NTM2NzMzNzExMzk2ODY0L2ltZy9XUFh6b0xKTVhjSTNjY0p4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729527205833932949#m</id>
            <title>一种让别人知道你摘掉戴眼镜后真实视力的方法。

用相机对准眼镜拍照，然后在iPhone上长按“AF锁定”。

这样，眼镜被移开后，通过相机屏幕看到的画面就是佩戴该眼镜的人的真实视力状况。

这种方法可以让视力正常人理解近视者的视觉体验，也会对店铺或城市规划有所帮助。

这个方法的原理基于相机的自动对焦（AF）功能。

当你用相机对准眼镜并长按以锁定自动对焦时，相机会根据眼镜的光学特性来调整焦距。

这样，当眼镜被移开，相机的焦点仍然保持在原来根据眼镜调整的位置。

因此，通过相机屏幕看到的画面会模拟出佩戴该眼镜的人的视力状况。

video：@sakata_yoshi</title>
            <link>https://nitter.cz/xiaohuggg/status/1729527205833932949#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729527205833932949#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 15:46:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一种让别人知道你摘掉戴眼镜后真实视力的方法。<br />
<br />
用相机对准眼镜拍照，然后在iPhone上长按“AF锁定”。<br />
<br />
这样，眼镜被移开后，通过相机屏幕看到的画面就是佩戴该眼镜的人的真实视力状况。<br />
<br />
这种方法可以让视力正常人理解近视者的视觉体验，也会对店铺或城市规划有所帮助。<br />
<br />
这个方法的原理基于相机的自动对焦（AF）功能。<br />
<br />
当你用相机对准眼镜并长按以锁定自动对焦时，相机会根据眼镜的光学特性来调整焦距。<br />
<br />
这样，当眼镜被移开，相机的焦点仍然保持在原来根据眼镜调整的位置。<br />
<br />
因此，通过相机屏幕看到的画面会模拟出佩戴该眼镜的人的视力状况。<br />
<br />
video：<a href="https://nitter.cz/sakata_yoshi" title="よしひこ@メガネ屋4代目👓1級眼鏡作製技能士">@sakata_yoshi</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NTI3MTQ3NTI0NzQ3MjY1L2ltZy9KcThLdnoySVVmV0VfMEhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729494711076004286#m</id>
            <title>R to @xiaohuggg: drawfast在线体验：http://drawfast.tldraw.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1729494711076004286#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729494711076004286#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 13:37:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>drawfast在线体验：<a href="http://drawfast.tldraw.com">drawfast.tldraw.com</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FCVjNYTldZQUFSQVp6LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBQlYzWE5XWUFBUkFaei5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729462359486582975#m</id>
            <title>RAGs：允许创建和定制自己的RAG流水线，并在自己的数据上使用它，全部通过自然语言完成。

这意味着现在你可以设置一个“基于你自己的数据的ChatGPT”，而且不需要编码。

使用RAGs创建的机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。

能生成更准确、信息更丰富的回答。

安装简单：

这个项目受到了OpenAI GPTs的启发。RAGs能让你能够通过简单的自然语言描述来创建和定制自己的聊天机器人。这个过程不需要编程知识，只需按照几个步骤操作即可：

1、描述任务：告诉RAGs你想让机器人做什么，比如回答问题或总结信息。

2、设置参数：在一个界面上调整一些选项，比如要查找的信息数量。

3、与机器人互动：设置好后，你就可以开始向这个机器人提问了。

安装RAGs的步骤也很简单，只需下载代码、安装必要的软件包，然后运行程序即可。这个工具适合那些想要自己的聊天机器人但不懂编程的人。

它支持多种LLMs（大语言模型），包括OpenAI和Anthropic的模型。用户可以通过自然语言或手动方式为嵌入模型和LLM设置配置。

主要能力和特点：

使用RAGs创建的聊天机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。

这种机器人的特点和能力包括：

1、信息检索能力：机器人能够访问和搜索大量的文档和数据，以找到与用户查询相关的信息。这意味着它可以从外部源获取数据，而不仅仅依赖于预先训练的知识。

2、高质量的回答生成：结合检索到的信息和内置的语言模型（ChatGPT），机器人能够生成更准确、信息丰富的回答。

3、适应性强：由于它结合了检索和生成，这种机器人能够更好地处理复杂的问题，尤其是那些需要实时信息或专业知识的问题。

4、灵活性和定制性：用户可以根据自己的需求定制机器人的行为，例如指定信息检索的来源、调整回答的详细程度等。

5、适用于多种应用：这种机器人适用于各种场景，如客户服务、教育、研究辅助等，尤其是在需要处理大量信息和数据的领域。

介绍：https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1

GitHub：https://github.com/run-llama/rags</title>
            <link>https://nitter.cz/xiaohuggg/status/1729462359486582975#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729462359486582975#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 11:28:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RAGs：允许创建和定制自己的RAG流水线，并在自己的数据上使用它，全部通过自然语言完成。<br />
<br />
这意味着现在你可以设置一个“基于你自己的数据的ChatGPT”，而且不需要编码。<br />
<br />
使用RAGs创建的机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。<br />
<br />
能生成更准确、信息更丰富的回答。<br />
<br />
安装简单：<br />
<br />
这个项目受到了OpenAI GPTs的启发。RAGs能让你能够通过简单的自然语言描述来创建和定制自己的聊天机器人。这个过程不需要编程知识，只需按照几个步骤操作即可：<br />
<br />
1、描述任务：告诉RAGs你想让机器人做什么，比如回答问题或总结信息。<br />
<br />
2、设置参数：在一个界面上调整一些选项，比如要查找的信息数量。<br />
<br />
3、与机器人互动：设置好后，你就可以开始向这个机器人提问了。<br />
<br />
安装RAGs的步骤也很简单，只需下载代码、安装必要的软件包，然后运行程序即可。这个工具适合那些想要自己的聊天机器人但不懂编程的人。<br />
<br />
它支持多种LLMs（大语言模型），包括OpenAI和Anthropic的模型。用户可以通过自然语言或手动方式为嵌入模型和LLM设置配置。<br />
<br />
主要能力和特点：<br />
<br />
使用RAGs创建的聊天机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。<br />
<br />
这种机器人的特点和能力包括：<br />
<br />
1、信息检索能力：机器人能够访问和搜索大量的文档和数据，以找到与用户查询相关的信息。这意味着它可以从外部源获取数据，而不仅仅依赖于预先训练的知识。<br />
<br />
2、高质量的回答生成：结合检索到的信息和内置的语言模型（ChatGPT），机器人能够生成更准确、信息丰富的回答。<br />
<br />
3、适应性强：由于它结合了检索和生成，这种机器人能够更好地处理复杂的问题，尤其是那些需要实时信息或专业知识的问题。<br />
<br />
4、灵活性和定制性：用户可以根据自己的需求定制机器人的行为，例如指定信息检索的来源、调整回答的详细程度等。<br />
<br />
5、适用于多种应用：这种机器人适用于各种场景，如客户服务、教育、研究辅助等，尤其是在需要处理大量信息和数据的领域。<br />
<br />
介绍：<a href="https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1">blog.llamaindex.ai/introduci…</a><br />
<br />
GitHub：<a href="https://github.com/run-llama/rags">github.com/run-llama/rags</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjk0NjIxNzM1NDg5MDAzNTIvcHUvaW1nL1dKQWtLSC1ac2owc2xtdUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729442483594305738#m</id>
            <title>R to @xiaohuggg: 测试是支持iPhone的，开始是我手机iOS17 beta版本问题！登陆不上！正式版iOS17是可以的！

你们可以玩玩！</title>
            <link>https://nitter.cz/xiaohuggg/status/1729442483594305738#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729442483594305738#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 10:09:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试是支持iPhone的，开始是我手机iOS17 beta版本问题！登陆不上！正式版iOS17是可以的！<br />
<br />
你们可以玩玩！</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NDQyMzk1ODM2ODgyOTQ0L2ltZy81OUpiSlV5UWg2anFSd1ZYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729404520814575674#m</id>
            <title>TRASH BABY：混合任意两张照片生成新照片

你可以任意拍摄两张照片，这个APP会将它们混合成一个新的图像。

混合完毕后你可以继续添加新的照片，每次添加都会与之前的图像混合，创造出一系列独特且多变的图像作品。

也可以选择自己的照片进行混合...🥱

不过目前只支持iPad，下载链接：https://apps.apple.com/us/app/trash-baby/id6446822932</title>
            <link>https://nitter.cz/xiaohuggg/status/1729404520814575674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729404520814575674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:39:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TRASH BABY：混合任意两张照片生成新照片<br />
<br />
你可以任意拍摄两张照片，这个APP会将它们混合成一个新的图像。<br />
<br />
混合完毕后你可以继续添加新的照片，每次添加都会与之前的图像混合，创造出一系列独特且多变的图像作品。<br />
<br />
也可以选择自己的照片进行混合...🥱<br />
<br />
不过目前只支持iPad，下载链接：<a href="https://apps.apple.com/us/app/trash-baby/id6446822932">apps.apple.com/us/app/trash-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzODgwODIxNTAxNzg4MTYvcHUvaW1nL1pMZVdiU3RMYURiNWYzcmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729395131474935856#m</id>
            <title>如果有人能出一本日历

什么时间应该发什么，什么时候不能发什么

在国内应该会卖的很好

如果能利用AI再加个自动检测提示就更好了🙄</title>
            <link>https://nitter.cz/xiaohuggg/status/1729395131474935856#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729395131474935856#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:01:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果有人能出一本日历<br />
<br />
什么时间应该发什么，什么时候不能发什么<br />
<br />
在国内应该会卖的很好<br />
<br />
如果能利用AI再加个自动检测提示就更好了🙄</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729356197906837678#m</id>
            <title>Cleanlab：自动检测机器学习数据集中的问题，进行自动清理修复和整理。

Cleanlab能够自动识别和修复数据集中的多种问题，如错误的标签、异常值、重复数据等。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，它可以帮助识别这种错误。

可以处理图像、文本、音频、表格数据等多种类型的数据。

Cleanlab 的工作原理和特点包括：

1、使用现有模型估计问题：Cleanlab 不是从头开始训练一个新模型，而是利用已有的机器学习模型来分析数据集。它通过这些模型的输出来估计数据集中可能存在的问题，比如哪些数据可能被错误标记，或者哪些数据是异常值。

2、发现标签问题：Cleanlab 能够自动识别数据集中的多种问题，如错误的标签、异常值、重复数据等。这种自动检测功能大大简化了数据清理的过程。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，Cleanlab 可以帮助识别这种错误。

3、训练鲁棒模型：通过识别和修复数据集中的问题，Cleanlab 可以帮助训练出对噪声和错误更加鲁棒的模型。这意味着即使在数据不完美的情况下，模型也能表现得更好。

4、适用于多种数据类型和任务：Cleanlab 不仅适用于图像数据，还可以处理文本、音频、表格数据等多种类型的数据。它支持多种机器学习任务，包括分类、回归、对象检测等。

5、数据质量量化：除了识别和修复问题，Cleanlab 还能量化数据集的整体质量，帮助用户了解数据集的健康状况。

详细介绍：https://cleanlab.ai/blog/studio-multi-label/

GitHub：https://github.com/cleanlab/cleanlab/

案例：https://github.com/cleanlab/examples

视频演示为：以 CelebA 多标签数据集为例，展示了 Cleanlab Studio 如何快速改进数据集。

CelebA 包含面部图像及其相关标签（如“戴眼镜”、“戴耳环”等），每张图像可以有多个标签。Cleanlab Studio 能够自动发现数百个缺失和不正确的标签，以及一些可能含糊的样本和异常值。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729356197906837678#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729356197906837678#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 04:27:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Cleanlab：自动检测机器学习数据集中的问题，进行自动清理修复和整理。<br />
<br />
Cleanlab能够自动识别和修复数据集中的多种问题，如错误的标签、异常值、重复数据等。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，它可以帮助识别这种错误。<br />
<br />
可以处理图像、文本、音频、表格数据等多种类型的数据。<br />
<br />
Cleanlab 的工作原理和特点包括：<br />
<br />
1、使用现有模型估计问题：Cleanlab 不是从头开始训练一个新模型，而是利用已有的机器学习模型来分析数据集。它通过这些模型的输出来估计数据集中可能存在的问题，比如哪些数据可能被错误标记，或者哪些数据是异常值。<br />
<br />
2、发现标签问题：Cleanlab 能够自动识别数据集中的多种问题，如错误的标签、异常值、重复数据等。这种自动检测功能大大简化了数据清理的过程。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，Cleanlab 可以帮助识别这种错误。<br />
<br />
3、训练鲁棒模型：通过识别和修复数据集中的问题，Cleanlab 可以帮助训练出对噪声和错误更加鲁棒的模型。这意味着即使在数据不完美的情况下，模型也能表现得更好。<br />
<br />
4、适用于多种数据类型和任务：Cleanlab 不仅适用于图像数据，还可以处理文本、音频、表格数据等多种类型的数据。它支持多种机器学习任务，包括分类、回归、对象检测等。<br />
<br />
5、数据质量量化：除了识别和修复问题，Cleanlab 还能量化数据集的整体质量，帮助用户了解数据集的健康状况。<br />
<br />
详细介绍：<a href="https://cleanlab.ai/blog/studio-multi-label/">cleanlab.ai/blog/studio-mult…</a><br />
<br />
GitHub：<a href="https://github.com/cleanlab/cleanlab/">github.com/cleanlab/cleanlab…</a><br />
<br />
案例：<a href="https://github.com/cleanlab/examples">github.com/cleanlab/examples</a><br />
<br />
视频演示为：以 CelebA 多标签数据集为例，展示了 Cleanlab Studio 如何快速改进数据集。<br />
<br />
CelebA 包含面部图像及其相关标签（如“戴眼镜”、“戴耳环”等），每张图像可以有多个标签。Cleanlab Studio 能够自动发现数百个缺失和不正确的标签，以及一些可能含糊的样本和异常值。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzNDYzMzUyNTI1NDk2MzIvcHUvaW1nL2pibHJfRUQydmg3QkRJX2MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729336931258093805#m</id>
            <title>R to @xiaohuggg: 测试结果是：

如果是已经比较清晰的提升效果会很明显！

但是如果是很老的影片，提升效果就不是很明显了。

根据演示来看，似乎对动画视频效果提升比较好。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729336931258093805#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729336931258093805#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 03:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果是：<br />
<br />
如果是已经比较清晰的提升效果会很明显！<br />
<br />
但是如果是很老的影片，提升效果就不是很明显了。<br />
<br />
根据演示来看，似乎对动画视频效果提升比较好。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzMzY2MjczMTc4ODY5NzYvcHUvaW1nL3o5ZU0xRzBYNFlEcG1MalAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729336570115920325#m</id>
            <title>Real-ESRGAN-Video ：可以将视频清晰度提升到2K 或 4K

你只需要上传一个视频，选择你想要的清晰度，比如全高清（FHD）、2K 或 4K。它会自动提升视频的质量。处理完的视频可以直接在网页上预览，也可以下载到电脑上。

还提供了几种不同的模型处理模式，可以根据视频的内容选择最合适的。

- 标准模型（RealESRGAN_x4plus）：适用于大多数普通视频，能够提升视频的清晰度和细节。

- 动画专用模型（RealESRGAN_x4plus_anime_6B）：专门为动画视频设计，能更好地处理动画中的线条和颜色。

- 其他特殊模型（如 realesr-animevideov3）：针对特定类型的视频内容进行了优化，比如更适合处理低光照视频或是特定风格的视频。

用户可以根据自己的视频内容（比如是普通视频、动画、或者有特殊风格的视频）来选择最合适的模型，以获得最佳的视频增强效果。

在线体验：https://replicate.com/lucataco/real-esrgan-video

Colab：https://github.com/yuvraj108c/4k-video-upscaler-colab

GitHub：https://github.com/xinntao/Real-ESRGAN</title>
            <link>https://nitter.cz/xiaohuggg/status/1729336570115920325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729336570115920325#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 03:09:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Real-ESRGAN-Video ：可以将视频清晰度提升到2K 或 4K<br />
<br />
你只需要上传一个视频，选择你想要的清晰度，比如全高清（FHD）、2K 或 4K。它会自动提升视频的质量。处理完的视频可以直接在网页上预览，也可以下载到电脑上。<br />
<br />
还提供了几种不同的模型处理模式，可以根据视频的内容选择最合适的。<br />
<br />
- 标准模型（RealESRGAN_x4plus）：适用于大多数普通视频，能够提升视频的清晰度和细节。<br />
<br />
- 动画专用模型（RealESRGAN_x4plus_anime_6B）：专门为动画视频设计，能更好地处理动画中的线条和颜色。<br />
<br />
- 其他特殊模型（如 realesr-animevideov3）：针对特定类型的视频内容进行了优化，比如更适合处理低光照视频或是特定风格的视频。<br />
<br />
用户可以根据自己的视频内容（比如是普通视频、动画、或者有特殊风格的视频）来选择最合适的模型，以获得最佳的视频增强效果。<br />
<br />
在线体验：<a href="https://replicate.com/lucataco/real-esrgan-video">replicate.com/lucataco/real-…</a><br />
<br />
Colab：<a href="https://github.com/yuvraj108c/4k-video-upscaler-colab">github.com/yuvraj108c/4k-vid…</a><br />
<br />
GitHub：<a href="https://github.com/xinntao/Real-ESRGAN">github.com/xinntao/Real-ESRG…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzMzYzMzEyNDE5NDMwNDAvcHUvaW1nLzJ4cnc1ZVg5Q01lRTAyUEsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729313540887175590#m</id>
            <title>Awesome-Assistants ：各种各样的AI助手

这个项目收集和展示各种AI助手，包括聊天机器人、语音助手、自动化工具等。

你可以方便地将这些AI助手集成到他们自己的应用或系统中，无论他们使用的是什么编程语言。

GitHub：https://github.com/awesome-assistants/awesome-assistants</title>
            <link>https://nitter.cz/xiaohuggg/status/1729313540887175590#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729313540887175590#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 01:37:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Awesome-Assistants ：各种各样的AI助手<br />
<br />
这个项目收集和展示各种AI助手，包括聊天机器人、语音助手、自动化工具等。<br />
<br />
你可以方便地将这些AI助手集成到他们自己的应用或系统中，无论他们使用的是什么编程语言。<br />
<br />
GitHub：<a href="https://github.com/awesome-assistants/awesome-assistants">github.com/awesome-assistant…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9fQmY2bGJRQUEzZ0lPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729288978283827360#m</id>
            <title>和GPT玩井字棋游戏

GPT结合@tldraw 实时画图后的新玩法

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1729288978283827360#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729288978283827360#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 23:59:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>和GPT玩井字棋游戏<br />
<br />
GPT结合<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 实时画图后的新玩法<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkxODQwMzQ5NzI5MjU5NTIvcHUvaW1nL3RxQnBkQ0RBQUwtZ2pteFkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729167719394869646#m</id>
            <title>牛p了

用文字描述指挥AI画画😂

你只需要描述你要画什么就行

右边会按照你的要求即时创作，还会展示创作过程👍👍👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1729167719394869646#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729167719394869646#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 15:58:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>牛p了<br />
<br />
用文字描述指挥AI画画😂<br />
<br />
你只需要描述你要画什么就行<br />
<br />
右边会按照你的要求即时创作，还会展示创作过程👍👍👍</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl84NFF6T1hrQUEyRk1YLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfODRRek9Ya0FBMkZNWC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</id>
            <title>R to @xiaohuggg: 所以这种片子以后就统称为：A片</title>
            <link>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729102242429702348#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:37:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以这种片子以后就统称为：A片</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</id>
            <title>全球首位由AI生成的AV女演员木花愛（木花あい）出道

由日本一家专精于成人动作片的公司h.m.p推出。

木花愛：身高165厘米，三围为88G/55/85。

木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。

链接就不放了😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729099853698093128#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:28:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>全球首位由AI生成的AV女演员木花愛（木花あい）出道<br />
<br />
由日本一家专精于成人动作片的公司h.m.p推出。<br />
<br />
木花愛：身高165厘米，三围为88G/55/85。<br />
<br />
木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。<br />
<br />
链接就不放了😅</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83OTVWNGJRQUEwRkZFLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83LU1sYmJFQUFyd1VXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</id>
            <title>DiffusionMat：一个先进的高质量视频抠图框架

DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。

DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：

1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。

2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。

3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。

4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。

5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。

项目及演示：https://cnnlstm.github.io/DiffusionMat
论文：https://arxiv.org/pdf/2311.13535.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729091603900559731#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 10:55:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DiffusionMat：一个先进的高质量视频抠图框架<br />
<br />
DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。<br />
<br />
DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：<br />
<br />
1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。<br />
<br />
2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。<br />
<br />
3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。<br />
<br />
4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。<br />
<br />
5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。<br />
<br />
项目及演示：<a href="https://cnnlstm.github.io/DiffusionMat">cnnlstm.github.io/DiffusionM…</a><br />
论文：<a href="https://arxiv.org/pdf/2311.13535.pdf">arxiv.org/pdf/2311.13535.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMzk4MDI3NTMwMDc2MTYvcHUvaW1nL0VmTkUtUTZYQXlYNEVkejguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</id>
            <title>Comfy Workflows：ComfyUI工作流分享站

收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。

省去了很多麻烦🫡

而且你也可以自己分享自己的工作流。

网站还支持在线运行工作流，不过要花点钱🥱

🔗：http://ComfyWorkflows.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729063470124183735#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 09:03:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfy Workflows：ComfyUI工作流分享站<br />
<br />
收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。<br />
<br />
省去了很多麻烦🫡<br />
<br />
而且你也可以自己分享自己的工作流。<br />
<br />
网站还支持在线运行工作流，不过要花点钱🥱<br />
<br />
🔗：<a href="http://ComfyWorkflows.com">ComfyWorkflows.com</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwNjMxNjk4NDk3NzgxNzYvcHUvaW1nL2VtTnJlTGNqeXlNQUlBV1ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>