<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</id>
            <title>R to @xiaohuggg: 测试结果：
https://huggingface.co/openchat/openchat_3.5</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002803887214842#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:02:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果：<br />
<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bVpOWWIwQUF5dUJOLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bWRhQmFVQUVDSmRLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bW5tTmF3QUFUWm1kLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002211404087412#m</id>
            <title>OpenChat-3.5-7B ：在各种基准测试上超越ChatGPT

OpenChat使用了C-RLFT（一种受离线强化学习启发的策略）进行微调。

它能通过分析已有的对话数据和反馈来改进模型的表现。还可以从错误中学习。

测试了下，虽然只有7B大小，确实效果和GPT不分上下。

牛P的是它能在24GB RAM的消费级GPU上运行。

OpenChat还提供了一个Web UI界面，方便用户与模型进行交互。

性能和评估：

在实际应用中，OpenChat展示了优异的性能。它在多个基准测试中表现出色，特别是在遵循指令和泛化能力方面，超越了其他同类的开源语言模型。

在基准测试方面，OpenChat-3.5的7B模型在多个测试中的平均得分为61.6，超过了ChatGPT（March版本）的61.5。

在于http://X.AI 330 亿参数的Grok的比拼中OpenChat-3.5-7B

OpenChat工作原理：

1、预训练语言模型：OpenChat的核心是一个大型的预训练语言模型。这些模型通过分析和学习大量的文本数据，掌握了语言的结构、语法和语义。这使得OpenChat能够理解用户的输入，并生成流畅、连贯的回应。

2、微调方法（C-RLFT）：OpenChat采用了一种名为条件化强化学习微调（Conditioned-RLFT, C-RLFT）的方法。这种方法特别适用于处理混合质量的数据。在传统的微调方法中，所有的训练数据都被视为同等重要，这可能导致模型在处理质量不一的数据时效果不佳。C-RLFT通过将不同数据源视为不同的奖励标签，使模型能够更有效地从这些数据中学习。

3、类条件策略学习：在C-RLFT中，OpenChat学习了一个类条件策略，这意味着它可以根据输入数据的类型（例如，不同的数据源或质量）来调整其响应。这种策略使得OpenChat在处理各种不同类型的输入时更加灵活和有效。

4、单阶段监督学习：OpenChat使用了一种单阶段的监督学习方法。这种方法不依赖于传统的强化学习技术，而是通过最大化奖励并减少与参考策略之间的差异来优化模型。这种方法提高了学习效率，并有助于减少训练过程中的错误。

详细：https://huggingface.co/openchat/openchat_3.5
GitHub：https://github.com/imoneoi/openchat
论文：https://arxiv.org/pdf/2309.11235.pdf
在线体验：https://openchat.team/</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002211404087412#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002211404087412#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:00:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenChat-3.5-7B ：在各种基准测试上超越ChatGPT<br />
<br />
OpenChat使用了C-RLFT（一种受离线强化学习启发的策略）进行微调。<br />
<br />
它能通过分析已有的对话数据和反馈来改进模型的表现。还可以从错误中学习。<br />
<br />
测试了下，虽然只有7B大小，确实效果和GPT不分上下。<br />
<br />
牛P的是它能在24GB RAM的消费级GPU上运行。<br />
<br />
OpenChat还提供了一个Web UI界面，方便用户与模型进行交互。<br />
<br />
性能和评估：<br />
<br />
在实际应用中，OpenChat展示了优异的性能。它在多个基准测试中表现出色，特别是在遵循指令和泛化能力方面，超越了其他同类的开源语言模型。<br />
<br />
在基准测试方面，OpenChat-3.5的7B模型在多个测试中的平均得分为61.6，超过了ChatGPT（March版本）的61.5。<br />
<br />
在于<a href="http://X.AI">X.AI</a> 330 亿参数的Grok的比拼中OpenChat-3.5-7B<br />
<br />
OpenChat工作原理：<br />
<br />
1、预训练语言模型：OpenChat的核心是一个大型的预训练语言模型。这些模型通过分析和学习大量的文本数据，掌握了语言的结构、语法和语义。这使得OpenChat能够理解用户的输入，并生成流畅、连贯的回应。<br />
<br />
2、微调方法（C-RLFT）：OpenChat采用了一种名为条件化强化学习微调（Conditioned-RLFT, C-RLFT）的方法。这种方法特别适用于处理混合质量的数据。在传统的微调方法中，所有的训练数据都被视为同等重要，这可能导致模型在处理质量不一的数据时效果不佳。C-RLFT通过将不同数据源视为不同的奖励标签，使模型能够更有效地从这些数据中学习。<br />
<br />
3、类条件策略学习：在C-RLFT中，OpenChat学习了一个类条件策略，这意味着它可以根据输入数据的类型（例如，不同的数据源或质量）来调整其响应。这种策略使得OpenChat在处理各种不同类型的输入时更加灵活和有效。<br />
<br />
4、单阶段监督学习：OpenChat使用了一种单阶段的监督学习方法。这种方法不依赖于传统的强化学习技术，而是通过最大化奖励并减少与参考策略之间的差异来优化模型。这种方法提高了学习效率，并有助于减少训练过程中的错误。<br />
<br />
详细：<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a><br />
GitHub：<a href="https://github.com/imoneoi/openchat">github.com/imoneoi/openchat</a><br />
论文：<a href="https://arxiv.org/pdf/2309.11235.pdf">arxiv.org/pdf/2309.11235.pdf</a><br />
在线体验：<a href="https://openchat.team/">openchat.team/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMDE4NzkzOTk3OTI2NDAvcHUvaW1nL2lNU3lwZ194ekxQTjgwek8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1728959646138880026#m</id>
            <title>RT by @xiaohuggg: OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：

Intro: Large Language Model (LLM) talk

大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。

LLM Inference

首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。

以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。

现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。

在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。

除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。

你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。

比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。

我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。

那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。</title>
            <link>https://nitter.cz/dotey/status/1728959646138880026#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1728959646138880026#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:11:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：<br />
<br />
Intro: Large Language Model (LLM) talk<br />
<br />
大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。<br />
<br />
LLM Inference<br />
<br />
首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。<br />
<br />
以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。<br />
<br />
现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。<br />
<br />
在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。<br />
<br />
除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。<br />
<br />
你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。<br />
<br />
比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。<br />
<br />
我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。<br />
<br />
那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。</p>
<p><a href="https://nitter.cz/karpathy/status/1727731541781152035#m">nitter.cz/karpathy/status/1727731541781152035#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg5NTg2NTQ5Nzg2NjI0MDIvcHUvaW1nL096ak1ReDBBU0JqM29IUkYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728989154812498295#m</id>
            <title>R to @xiaohuggg: 0.025秒一张图…

😂

真是666</title>
            <link>https://nitter.cz/xiaohuggg/status/1728989154812498295#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728989154812498295#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 04:08:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>0.025秒一张图…<br />
<br />
😂<br />
<br />
真是666</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4NzcxODMyNjgwMTgxNzYwL2ltZy9UbDFWQmtBRFkzRFQxMURZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728988899161333970#m</id>
            <title>一位日本博主展示了LCM现在可以以大约40fps的速度生成图像，这使得完全实现实时应用成为可能。

@cumulo_autumn 展示了一个演示视频，该视频以1倍速（即实时速度）运行，包括OBS的屏幕录制和VRoid的渲染，运行速度约为36fps。他指出，如果不录制视频，速度可以达到39fps。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728988899161333970#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728988899161333970#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 04:07:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一位日本博主展示了LCM现在可以以大约40fps的速度生成图像，这使得完全实现实时应用成为可能。<br />
<br />
<a href="https://nitter.cz/cumulo_autumn" title="あき先生 | AI Vtuber『しずく』開発中">@cumulo_autumn</a> 展示了一个演示视频，该视频以1倍速（即实时速度）运行，包括OBS的屏幕录制和VRoid的渲染，运行速度约为36fps。他指出，如果不录制视频，速度可以达到39fps。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4NzY4Mzc0MTI0MjgxODU2L2ltZy9FRkx5RFBZQ3FZNGVYXzNiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728968230864351237#m</id>
            <title>UIDraw ：在手机上绘制简单UI草图转换成网站。

使用 GPT-4 Vision 和 PencilKit/PKCanvasView （可绘制的画布）技术，用户可以绘制用户界面(UI)，然后将其转换成HTML代码。

GitHub：https://github.com/jordansinger/UIDraw</title>
            <link>https://nitter.cz/xiaohuggg/status/1728968230864351237#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728968230864351237#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:45:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>UIDraw ：在手机上绘制简单UI草图转换成网站。<br />
<br />
使用 GPT-4 Vision 和 PencilKit/PKCanvasView （可绘制的画布）技术，用户可以绘制用户界面(UI)，然后将其转换成HTML代码。<br />
<br />
GitHub：<a href="https://github.com/jordansinger/UIDraw">github.com/jordansinger/UIDr…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4ODQ4NTQ3MjA4OTE2OTkyL2ltZy9xWS16SjliYjh0LXlOaGNhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728963473152045089#m</id>
            <title>Loom：一个创新的写作工具，可以让你和AI一起创作故事或文章

Loom基于GPT-3，采用了一种独特的树形结构来组织文本。

每个故事或文章的部分都像树的一个分支，你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。

举例解释：

假设你想写一个关于太空探险的故事。你已经有了一个大致的想法，但还不确定具体的情节和方向。这时，你可以使用Loom来帮助你发展这个故事。

1、开始创作：首先，你在Loom的主文本框中输入你的初始想法，比如“一队宇航员在遥远的星系发现了一个未知的行星”。

2、生成内容：接下来，你可以让AI帮你生成接下来的情节。比如，你可以让AI为你生成关于这个未知行星的描述，或者宇航员在行星上的遭遇。

3、探索不同的情节线：AI生成的内容会以树形结构展现。你可以在这个树上看到不同的分支，每个分支代表一个不同的故事方向。比如，一个分支可能是宇航员在行星上发现了外星生命的迹象，另一个分支可能是他们遇到了技术故障。

4、选择和发展：你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。

5、编辑和完善：在创作的过程中，你可以随时编辑和修改AI生成的内容，或者添加你自己的想法和细节，使故事更加丰富和完整。

6、保存和分享：完成故事后，你可以将整个故事树以JSON格式保存下来，也可以分享给其他人，让他们看到你的创作过程和最终成果。

通过这种方式，Loom让你能够以一种非线性和互动的方式创作故事，同时结合了AI的智能和你自己的创造力。

Loom的主要特点和功能包括：

1、基于GPT 3：Loom基于GPT 3开发，允许用户与GPT-3合作创作内容。用户可以输入一些文本或想法，然后让AI基于这些输入生成新的内容或建议。

2、树形写作界面：Loom采用了一种独特的树形结构来组织文本。每个故事或文章的部分都像树的一个分支，用户可以在任何分支上继续发展故事，或者探索不同的情节方向。

3、多视角导航：用户可以在树形结构中自由导航，探索不同的故事线索和发展。这种方式使得故事创作更加灵活和多元。

4、内容生成和编辑：用户可以编辑树中的任何节点，并使用AI来生成新的节点或内容。这为创作提供了额外的灵感和帮助。

5、文件输入/输出：Loom支持以JSON格式导入和导出故事树，方便用户保存和分享他们的创作。

6、块多元宇宙模式：这是一个实验性的功能，用于展示和演示如何在不同的块（或情节片段）之间进行切换和探索。

5、热键和快捷操作：Loom提供了一系列热键和快捷操作，使用户能够快速进行各种操作，如打开文件、保存、生成内容等。

GitHub：https://github.com/socketteer/loom
实例：https://generative.ink/meta/block-multiverse/</title>
            <link>https://nitter.cz/xiaohuggg/status/1728963473152045089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728963473152045089#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:26:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Loom：一个创新的写作工具，可以让你和AI一起创作故事或文章<br />
<br />
Loom基于GPT-3，采用了一种独特的树形结构来组织文本。<br />
<br />
每个故事或文章的部分都像树的一个分支，你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。<br />
<br />
举例解释：<br />
<br />
假设你想写一个关于太空探险的故事。你已经有了一个大致的想法，但还不确定具体的情节和方向。这时，你可以使用Loom来帮助你发展这个故事。<br />
<br />
1、开始创作：首先，你在Loom的主文本框中输入你的初始想法，比如“一队宇航员在遥远的星系发现了一个未知的行星”。<br />
<br />
2、生成内容：接下来，你可以让AI帮你生成接下来的情节。比如，你可以让AI为你生成关于这个未知行星的描述，或者宇航员在行星上的遭遇。<br />
<br />
3、探索不同的情节线：AI生成的内容会以树形结构展现。你可以在这个树上看到不同的分支，每个分支代表一个不同的故事方向。比如，一个分支可能是宇航员在行星上发现了外星生命的迹象，另一个分支可能是他们遇到了技术故障。<br />
<br />
4、选择和发展：你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。<br />
<br />
5、编辑和完善：在创作的过程中，你可以随时编辑和修改AI生成的内容，或者添加你自己的想法和细节，使故事更加丰富和完整。<br />
<br />
6、保存和分享：完成故事后，你可以将整个故事树以JSON格式保存下来，也可以分享给其他人，让他们看到你的创作过程和最终成果。<br />
<br />
通过这种方式，Loom让你能够以一种非线性和互动的方式创作故事，同时结合了AI的智能和你自己的创造力。<br />
<br />
Loom的主要特点和功能包括：<br />
<br />
1、基于GPT 3：Loom基于GPT 3开发，允许用户与GPT-3合作创作内容。用户可以输入一些文本或想法，然后让AI基于这些输入生成新的内容或建议。<br />
<br />
2、树形写作界面：Loom采用了一种独特的树形结构来组织文本。每个故事或文章的部分都像树的一个分支，用户可以在任何分支上继续发展故事，或者探索不同的情节方向。<br />
<br />
3、多视角导航：用户可以在树形结构中自由导航，探索不同的故事线索和发展。这种方式使得故事创作更加灵活和多元。<br />
<br />
4、内容生成和编辑：用户可以编辑树中的任何节点，并使用AI来生成新的节点或内容。这为创作提供了额外的灵感和帮助。<br />
<br />
5、文件输入/输出：Loom支持以JSON格式导入和导出故事树，方便用户保存和分享他们的创作。<br />
<br />
6、块多元宇宙模式：这是一个实验性的功能，用于展示和演示如何在不同的块（或情节片段）之间进行切换和探索。<br />
<br />
5、热键和快捷操作：Loom提供了一系列热键和快捷操作，使用户能够快速进行各种操作，如打开文件、保存、生成内容等。<br />
<br />
GitHub：<a href="https://github.com/socketteer/loom">github.com/socketteer/loom</a><br />
实例：<a href="https://generative.ink/meta/block-multiverse/">generative.ink/meta/block-mu…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhRV2JjQUF6dFY4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhRMmJjQUFtSzZILmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhReWE0QUFKZ2NLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhRd2FjQUFRa2t3LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728779202542154064#m</id>
            <title>2023年世界百万富翁的迁移图

接待新百万富翁最多的前三个国家；澳大利亚（预计迁入5,200名）、阿联酋（4,500名）、新加坡（3,200名）…

百万富翁流失最多的三个国家：中国（预计流失13,500名）、印度（6,500名）、英国（3,200名）…

预计到2023年年底将有122,000名HNWIs 高净值个人（HNWIs）迁移到新的国家。

在这里，HNWIs被定义为拥有至少100万美元净资产的个人。

以下是详细：

接待新百万富翁的国家主要包括：

1. 澳大利亚（预计迁入5,200名）
2. 阿联酋（4,500名）
3. 新加坡（3,200名）
4. 美国（2,100名）
5. 瑞士（1,800名）
6. 加拿大（1,600名）
7. 希腊（1,200名）
8. 法国（1,000名）
9. 葡萄牙（800名）
10. 新西兰（700名）

而失去最多百万富翁的国家包括：

1. 中国（预计流失13,500名）
2. 印度（6,500名）
3. 英国（3,200名）
4. 俄罗斯（3,000名）
5. 巴西（1,200名）
6. 香港特别行政区（1,000名）
7. 韩国（800名）
8. 墨西哥（700名）
9. 南非（500名）
10. 日本（300名）

详细：https://www.visualcapitalist.com/mapped-the-migration-of-the-worlds-millionaires-in-2023/#google_vignette</title>
            <link>https://nitter.cz/xiaohuggg/status/1728779202542154064#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728779202542154064#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 14:14:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2023年世界百万富翁的迁移图<br />
<br />
接待新百万富翁最多的前三个国家；澳大利亚（预计迁入5,200名）、阿联酋（4,500名）、新加坡（3,200名）…<br />
<br />
百万富翁流失最多的三个国家：中国（预计流失13,500名）、印度（6,500名）、英国（3,200名）…<br />
<br />
预计到2023年年底将有122,000名HNWIs 高净值个人（HNWIs）迁移到新的国家。<br />
<br />
在这里，HNWIs被定义为拥有至少100万美元净资产的个人。<br />
<br />
以下是详细：<br />
<br />
接待新百万富翁的国家主要包括：<br />
<br />
1. 澳大利亚（预计迁入5,200名）<br />
2. 阿联酋（4,500名）<br />
3. 新加坡（3,200名）<br />
4. 美国（2,100名）<br />
5. 瑞士（1,800名）<br />
6. 加拿大（1,600名）<br />
7. 希腊（1,200名）<br />
8. 法国（1,000名）<br />
9. 葡萄牙（800名）<br />
10. 新西兰（700名）<br />
<br />
而失去最多百万富翁的国家包括：<br />
<br />
1. 中国（预计流失13,500名）<br />
2. 印度（6,500名）<br />
3. 英国（3,200名）<br />
4. 俄罗斯（3,000名）<br />
5. 巴西（1,200名）<br />
6. 香港特别行政区（1,000名）<br />
7. 韩国（800名）<br />
8. 墨西哥（700名）<br />
9. 南非（500名）<br />
10. 日本（300名）<br />
<br />
详细：<a href="https://www.visualcapitalist.com/mapped-the-migration-of-the-worlds-millionaires-in-2023/#google_vignette">visualcapitalist.com/mapped-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zYkFXZ2FBQUFtdmVDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728758009483125062#m</id>
            <title>你们知道国产大模型有多少个了吗？？

188个！一百八十八个！！！

可谓是遥遥领先...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728758009483125062#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728758009483125062#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 12:50:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>你们知道国产大模型有多少个了吗？？<br />
<br />
188个！一百八十八个！！！<br />
<br />
可谓是遥遥领先...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zR1FfNmJnQUFobHkzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728750849973956702#m</id>
            <title>R to @xiaohuggg: 详细报道：

认识第一个每月收入高达 10,000 欧元的西班牙 AI 模型

https://www.euronews.com/next/2023/11/22/meet-the-first-spanish-ai-model-earning-up-to-10000-per-month</title>
            <link>https://nitter.cz/xiaohuggg/status/1728750849973956702#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728750849973956702#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 12:21:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>详细报道：<br />
<br />
认识第一个每月收入高达 10,000 欧元的西班牙 AI 模型<br />
<br />
<a href="https://www.euronews.com/next/2023/11/22/meet-the-first-spanish-ai-model-earning-up-to-10000-per-month">euronews.com/next/2023/11/22…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zQnVTamJvQUFEeTdvLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728750397765128539#m</id>
            <title>西班牙一家MCN公司因为对真实模特和网红的不靠谱合作感到厌烦，于是创造了自己的AI网红Aitana。

Aitana平均每月带来3300美元的收入，有时甚至高达10900美元。这些收入主要来自广告，Aitana还成为了一家运动补充品品牌的大使。

即使在媒体揭露她是AI创造后，许多粉丝仍然表达了对她的喜爱。🙄

她在Instagram上拥有超过13万粉丝...😅

设计团队设定了Aitana的基本特征和个性。她被设计成一个健身爱好者，性格坚定而复杂。她的外观、兴趣和特点都是基于对社会趋势的分析。

Aitana的实际形象是通过人工智能技术和Photoshop的结合创造出来的。设计团队使用这些工具来生成她的图像，并确保她的外观接近完美。

由于Aitana是一个虚拟人物，设计团队需要定期为她创造“生活”。这包括决定她一周内的活动、她将访问的地方以及将上传到社交媒体的照片。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728750397765128539#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728750397765128539#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 12:19:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>西班牙一家MCN公司因为对真实模特和网红的不靠谱合作感到厌烦，于是创造了自己的AI网红Aitana。<br />
<br />
Aitana平均每月带来3300美元的收入，有时甚至高达10900美元。这些收入主要来自广告，Aitana还成为了一家运动补充品品牌的大使。<br />
<br />
即使在媒体揭露她是AI创造后，许多粉丝仍然表达了对她的喜爱。🙄<br />
<br />
她在Instagram上拥有超过13万粉丝...😅<br />
<br />
设计团队设定了Aitana的基本特征和个性。她被设计成一个健身爱好者，性格坚定而复杂。她的外观、兴趣和特点都是基于对社会趋势的分析。<br />
<br />
Aitana的实际形象是通过人工智能技术和Photoshop的结合创造出来的。设计团队使用这些工具来生成她的图像，并确保她的外观接近完美。<br />
<br />
由于Aitana是一个虚拟人物，设计团队需要定期为她创造“生活”。这包括决定她一周内的活动、她将访问的地方以及将上传到社交媒体的照片。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8yX01MU2J3QUFSSmtrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zQktqTWFrQUFHMmM1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728742043349094427#m</id>
            <title>PlugBear：将ChatGPT等LLM连接到其他在线工具和服务

比如，你可以选择把你的GPTs应用连接到Slack、Discord、WhatsApp等外部即时通讯平台，这样你的AI就可以在Slack里回答问题或执行任务。

这意味着用户可以轻松地将他们的AI应用集成到常用的通讯和协作平台中。

PlugBear的优点：

1、简单的设置过程：用户只需几个点击就可以完成设置，无需复杂的配置。

2、添加频道和应用：用户可以选择他们喜欢的频道来连接他们的AI机器人，并添加他们使用的LLM应用。这提供了灵活性，允许用户根据自己的需求和偏好进行定制。

3、连接频道与应用：用户可以将这些频道和应用相互连接，并定义触发AI的条件。这种方式使得AI应用的部署和使用变得更加高效和灵活。

4、一次开发，多处连接：PlugBear的理念是“一次开发，到处连接”。这意味着用户无需在不同平台上重复进行AI集成工作，节省了大量时间和资源。

PlugBear是一个旨在简化和加速LLM应用与各种工具和平台集成的服务。它支持众多 LLM 应用程序构建器和框架，包括 OpenAI 的 GPT、LangChain 等。

PlugBear就像是一个桥梁，让你的AI应用能够轻松地和其他软件服务连接和交流，扩大了AI应用的使用场景和功能。

访问：https://plugbear.io/</title>
            <link>https://nitter.cz/xiaohuggg/status/1728742043349094427#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728742043349094427#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 11:46:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PlugBear：将ChatGPT等LLM连接到其他在线工具和服务<br />
<br />
比如，你可以选择把你的GPTs应用连接到Slack、Discord、WhatsApp等外部即时通讯平台，这样你的AI就可以在Slack里回答问题或执行任务。<br />
<br />
这意味着用户可以轻松地将他们的AI应用集成到常用的通讯和协作平台中。<br />
<br />
PlugBear的优点：<br />
<br />
1、简单的设置过程：用户只需几个点击就可以完成设置，无需复杂的配置。<br />
<br />
2、添加频道和应用：用户可以选择他们喜欢的频道来连接他们的AI机器人，并添加他们使用的LLM应用。这提供了灵活性，允许用户根据自己的需求和偏好进行定制。<br />
<br />
3、连接频道与应用：用户可以将这些频道和应用相互连接，并定义触发AI的条件。这种方式使得AI应用的部署和使用变得更加高效和灵活。<br />
<br />
4、一次开发，多处连接：PlugBear的理念是“一次开发，到处连接”。这意味着用户无需在不同平台上重复进行AI集成工作，节省了大量时间和资源。<br />
<br />
PlugBear是一个旨在简化和加速LLM应用与各种工具和平台集成的服务。它支持众多 LLM 应用程序构建器和框架，包括 OpenAI 的 GPT、LangChain 等。<br />
<br />
PlugBear就像是一个桥梁，让你的AI应用能够轻松地和其他软件服务连接和交流，扩大了AI应用的使用场景和功能。<br />
<br />
访问：<a href="https://plugbear.io/">plugbear.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg2Njc2MTEyODA2MTc0NzIvcHUvaW1nL3h4RDRFUTZZTWRhaVJJQXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728686421559644632#m</id>
            <title>一款专门为VR游戏设计的鞋子😂

玩VR游戏的时候你肯能会跟着游戏跑来跑去，容易撞上家具…

这款鞋子可以让你能运动的同时

保持原地踏步😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1728686421559644632#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728686421559644632#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 08:05:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一款专门为VR游戏设计的鞋子😂<br />
<br />
玩VR游戏的时候你肯能会跟着游戏跑来跑去，容易撞上家具…<br />
<br />
这款鞋子可以让你能运动的同时<br />
<br />
保持原地踏步😅</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgwMDA2Nzk4MzMxMjQ4NjQvcHUvaW1nL2NFUTZmVUp1aUJ3Q093Yk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728672101048213622#m</id>
            <title>视力

检测表</title>
            <link>https://nitter.cz/xiaohuggg/status/1728672101048213622#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728672101048213622#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 07:08:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视力<br />
<br />
检测表</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xNkhIRWFBQUFEUTNCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728651926974480487#m</id>
            <title>LucidDreamer 可以根据任何文本或图像提示生成高质量的 3D 高斯泼溅场景。

首尔国立大学计算机视觉实验室开发的项目，它可以根据一段文字或一张图片创造出可导航探索的三维虚拟场景。

而且在这个虚拟环境中，用户可以像在真实世界一样，从不同的角度和位置探索和观察该场景。

LucidDreamer通过这些不同的输入方式，能够创造出高质量、从多个角度都能观看的三维场景，这些场景可以用于各种应用，如游戏、电影制作、虚拟现实等。

这个项目的特别之处在于：

1、不限领域：它不仅限于创建某一特定类型的场景，而是可以创造各种各样的场景。

2、两步制作过程：首先，它根据你给的信息创造出一系列的图片，然后把这些图片转换成三维空间中的点，最后把这些点组合成一个完整的三维场景。

3、细节丰富：与其他类似工具相比，LucidDreamer创造的场景更加细致和真实。

4、灵活控制：你可以通过给出不同的文字提示来精确控制你想要创造的场景。

5、高质量：根据专业的评估标准，LucidDreamer创造的场景在质量上表现非常好，比如色彩鲜艳、图像清晰等。

作者：@_ironjr_
项目及演示：https://luciddreamer-cvlab.github.io/
论文：https://arxiv.org/pdf/2311.13384
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728651926974480487#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728651926974480487#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 05:48:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LucidDreamer 可以根据任何文本或图像提示生成高质量的 3D 高斯泼溅场景。<br />
<br />
首尔国立大学计算机视觉实验室开发的项目，它可以根据一段文字或一张图片创造出可导航探索的三维虚拟场景。<br />
<br />
而且在这个虚拟环境中，用户可以像在真实世界一样，从不同的角度和位置探索和观察该场景。<br />
<br />
LucidDreamer通过这些不同的输入方式，能够创造出高质量、从多个角度都能观看的三维场景，这些场景可以用于各种应用，如游戏、电影制作、虚拟现实等。<br />
<br />
这个项目的特别之处在于：<br />
<br />
1、不限领域：它不仅限于创建某一特定类型的场景，而是可以创造各种各样的场景。<br />
<br />
2、两步制作过程：首先，它根据你给的信息创造出一系列的图片，然后把这些图片转换成三维空间中的点，最后把这些点组合成一个完整的三维场景。<br />
<br />
3、细节丰富：与其他类似工具相比，LucidDreamer创造的场景更加细致和真实。<br />
<br />
4、灵活控制：你可以通过给出不同的文字提示来精确控制你想要创造的场景。<br />
<br />
5、高质量：根据专业的评估标准，LucidDreamer创造的场景在质量上表现非常好，比如色彩鲜艳、图像清晰等。<br />
<br />
作者：<a href="https://nitter.cz/_ironjr_" title="Jaerin Lee">@_ironjr_</a><br />
项目及演示：<a href="https://luciddreamer-cvlab.github.io/">luciddreamer-cvlab.github.io…</a><br />
论文：<a href="https://arxiv.org/pdf/2311.13384">arxiv.org/pdf/2311.13384</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg2MjEzMzY0MjY0MjYzNjgvcHUvaW1nL0NrZjVDbXhQbEFoTUEtZUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728618345283588135#m</id>
            <title>R to @xiaohuggg: 合并后的模型不仅继承了各个单独LoRA的功能，还保持了它们原有的特点和效果。

还能准确地生成每个模型特有的结构和风格元素，这是直接合并方法无法实现的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728618345283588135#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728618345283588135#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 03:35:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>合并后的模型不仅继承了各个单独LoRA的功能，还保持了它们原有的特点和效果。<br />
<br />
还能准确地生成每个模型特有的结构和风格元素，这是直接合并方法无法实现的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xSkgwRmJJQUFWQmdJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728617727143919812#m</id>
            <title>R to @xiaohuggg: ZipLoRA模型能够将参考对象（例如，特定的人物、物体或场景）放置在不同的上下文中。这意味着它可以改变原始图像中对象的环境或背景，使其适应全新的场景或故事情境。

还能进行语义修改。这可能包括改变对象的属性、形态或与其他元素的关系，以适应新的上下文。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728617727143919812#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728617727143919812#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 03:32:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ZipLoRA模型能够将参考对象（例如，特定的人物、物体或场景）放置在不同的上下文中。这意味着它可以改变原始图像中对象的环境或背景，使其适应全新的场景或故事情境。<br />
<br />
还能进行语义修改。这可能包括改变对象的属性、形态或与其他元素的关系，以适应新的上下文。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xSW9MbGFRQUFaZVVWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728617508176101519#m</id>
            <title>ZipLoRA：可以将不同的艺术风格和主题结合在一起，创造出独特的图像。

是Google 开发的一种生成任何主题的任何风格的图像的技术。

简单来说，你可以把一幅画的风格（比如梵高的画风）应用到任何你想要的主题（比如你的宠物照片）上，从而创造出一个既有梵高风格又展现你宠物特征的全新图像。

这项技术的关键在于它使用了一种称为“低秩适应”（LoRA）的方法。这种方法可以高效地调整AI模型，使其能够同时学习和模仿不同的风格和主题。通过这种方式，ZipLoRA能够在不牺牲图像质量的情况下，将不同的风格和主题融合在一起。

主要特点：

1、风格和主题的有效融合：ZipLoRA能够将不同的艺术风格和特定主题有效地结合在一起，创造出独特的图像。这意味着用户可以选择一个特定的风格（如某位著名画家的风格）并将其应用到任何他们选择的主题上（比如一张特定的照片）。

2、保留内容和风格的特性：在融合风格和主题时，ZipLoRA能够保持参考主题的身份特征，同时捕捉参考风格的独特特点。

3、提高主题和风格保真度：与其他基线模型相比，ZipLoRA在保持主题和风格的真实性方面表现出色，这意味着生成的图像既忠实于原始主题，又能准确地反映所选风格。

4、无需手动调整超参数：ZipLoRA不需要手动调整任何超参数或合并权重，这使得它对于用户来说更加方便和易用。

5、基于Stable Diffusion XL模型：ZipLoRA利用了最新发布的Stable Diffusion XL (SDXL)模型，该模型在风格学习方面表现出色，能够使用单个示例图像学习风格。

ZipLoRA的工作原理涉及几个关键步骤和技术概念：

1、低秩适应（LoRA）：这是一种高效的方法，用于调整和优化AI模型，使其能够学习和模仿不同的风格和主题。这种技术允许对深度学习模型进行微调，而不是完全重新训练。在低秩适应中，模型的核心结构保持不变，只有一小部分权重（参数）被调整。这样做的好处是节省时间和计算资源，同时保持模型的基本能力。

2、独立训练的LoRAs：ZipLoRA特别之处在于，它可以结合两种独立训练的LoRAs。一种LoRA专注于学习特定的艺术风格（比如某位著名画家的风格），另一种LoRA专注于特定的主题（比如特定的人物或风景）。这意味着ZipLoRA能够同时捕捉特定风格的艺术特点和主题的独特特征。

3、优化方法：ZipLoRA采用了一种类似拉链的优化方法。这种方法的目的是在合并风格和主题的LoRAs时，减少所需的计算量。通过这种方法，ZipLoRA能够有效地结合两种LoRAs，同时保留它们各自的特性。

4、生成图像：在结合了风格和主题的LoRAs之后，ZipLoRA可以生成新的图像。这些图像既具有选定风格的艺术特点，又保持了原始主题的特征。例如，它可以生成一幅既有梵高画风又展现特定风景的图像。

实验验证：

ZipLoRA的方法通过一系列实验得到验证。这些实验表明，ZipLoRA在保持主题和风格的真实性方面表现出色，相比于其他基线模型有显著提升。

项目及演示：https://ziplora.github.io/
论文：https://arxiv.org/abs/2311.13600</title>
            <link>https://nitter.cz/xiaohuggg/status/1728617508176101519#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728617508176101519#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 03:31:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ZipLoRA：可以将不同的艺术风格和主题结合在一起，创造出独特的图像。<br />
<br />
是Google 开发的一种生成任何主题的任何风格的图像的技术。<br />
<br />
简单来说，你可以把一幅画的风格（比如梵高的画风）应用到任何你想要的主题（比如你的宠物照片）上，从而创造出一个既有梵高风格又展现你宠物特征的全新图像。<br />
<br />
这项技术的关键在于它使用了一种称为“低秩适应”（LoRA）的方法。这种方法可以高效地调整AI模型，使其能够同时学习和模仿不同的风格和主题。通过这种方式，ZipLoRA能够在不牺牲图像质量的情况下，将不同的风格和主题融合在一起。<br />
<br />
主要特点：<br />
<br />
1、风格和主题的有效融合：ZipLoRA能够将不同的艺术风格和特定主题有效地结合在一起，创造出独特的图像。这意味着用户可以选择一个特定的风格（如某位著名画家的风格）并将其应用到任何他们选择的主题上（比如一张特定的照片）。<br />
<br />
2、保留内容和风格的特性：在融合风格和主题时，ZipLoRA能够保持参考主题的身份特征，同时捕捉参考风格的独特特点。<br />
<br />
3、提高主题和风格保真度：与其他基线模型相比，ZipLoRA在保持主题和风格的真实性方面表现出色，这意味着生成的图像既忠实于原始主题，又能准确地反映所选风格。<br />
<br />
4、无需手动调整超参数：ZipLoRA不需要手动调整任何超参数或合并权重，这使得它对于用户来说更加方便和易用。<br />
<br />
5、基于Stable Diffusion XL模型：ZipLoRA利用了最新发布的Stable Diffusion XL (SDXL)模型，该模型在风格学习方面表现出色，能够使用单个示例图像学习风格。<br />
<br />
ZipLoRA的工作原理涉及几个关键步骤和技术概念：<br />
<br />
1、低秩适应（LoRA）：这是一种高效的方法，用于调整和优化AI模型，使其能够学习和模仿不同的风格和主题。这种技术允许对深度学习模型进行微调，而不是完全重新训练。在低秩适应中，模型的核心结构保持不变，只有一小部分权重（参数）被调整。这样做的好处是节省时间和计算资源，同时保持模型的基本能力。<br />
<br />
2、独立训练的LoRAs：ZipLoRA特别之处在于，它可以结合两种独立训练的LoRAs。一种LoRA专注于学习特定的艺术风格（比如某位著名画家的风格），另一种LoRA专注于特定的主题（比如特定的人物或风景）。这意味着ZipLoRA能够同时捕捉特定风格的艺术特点和主题的独特特征。<br />
<br />
3、优化方法：ZipLoRA采用了一种类似拉链的优化方法。这种方法的目的是在合并风格和主题的LoRAs时，减少所需的计算量。通过这种方法，ZipLoRA能够有效地结合两种LoRAs，同时保留它们各自的特性。<br />
<br />
4、生成图像：在结合了风格和主题的LoRAs之后，ZipLoRA可以生成新的图像。这些图像既具有选定风格的艺术特点，又保持了原始主题的特征。例如，它可以生成一幅既有梵高画风又展现特定风景的图像。<br />
<br />
实验验证：<br />
<br />
ZipLoRA的方法通过一系列实验得到验证。这些实验表明，ZipLoRA在保持主题和风格的真实性方面表现出色，相比于其他基线模型有显著提升。<br />
<br />
项目及演示：<a href="https://ziplora.github.io/">ziplora.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.13600">arxiv.org/abs/2311.13600</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8xSWRiRWIwQUFVRW9iLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>