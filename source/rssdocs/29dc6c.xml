<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730839121311183264#m</id>
            <title>Autoware ：一个开源的自动驾驶系统

Autoware 基于机器人操作系统 (ROS) 构建，可在各种车辆和应用中实现自动驾驶的商业部署。

Autoware 的主要功能和特点：

1、模块化架构：Autoware 包含自动驾驶所需的所有功能（如感知、定位、规划、控制），并采用模块化架构，具有清晰定义的接口和 API。

2、可扩展性：Autoware 的开源软件设计用于跨广泛的自动应用程序的可扩展性，并通过应用最佳实践和标准来实现现实世界部署中的高质量和安全性。

3、不断进化：Autoware 持续发展，提供更多功能，以实现从路边到路边的 L4 级自动驾驶。

4、多种应用场景：包括密集城市区域、高速公路和最终服务目的地的整合，提供完整的自动驾驶体验。

Autoware 的应用场景：

人员运输：使用公交车和班车的人员移动功能在 Autoware v3.0 中得到开发和支持。

货物运输和自动赛车：在 Autoware v2.0 中开发并支持货物运输功能（Autoware 也用于自动赛车比赛）。

自动代客泊车（AVP）：在 Autoware v1.0 中开发并支持自动代客泊车功能。

Autoware已被超过500加企业使用，并在20多个国家地区的30多种车辆上运行。

GitHub：https://github.com/autowarefoundation/autoware
官网：https://autoware.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730839121311183264#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730839121311183264#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 06:39:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Autoware ：一个开源的自动驾驶系统<br />
<br />
Autoware 基于机器人操作系统 (ROS) 构建，可在各种车辆和应用中实现自动驾驶的商业部署。<br />
<br />
Autoware 的主要功能和特点：<br />
<br />
1、模块化架构：Autoware 包含自动驾驶所需的所有功能（如感知、定位、规划、控制），并采用模块化架构，具有清晰定义的接口和 API。<br />
<br />
2、可扩展性：Autoware 的开源软件设计用于跨广泛的自动应用程序的可扩展性，并通过应用最佳实践和标准来实现现实世界部署中的高质量和安全性。<br />
<br />
3、不断进化：Autoware 持续发展，提供更多功能，以实现从路边到路边的 L4 级自动驾驶。<br />
<br />
4、多种应用场景：包括密集城市区域、高速公路和最终服务目的地的整合，提供完整的自动驾驶体验。<br />
<br />
Autoware 的应用场景：<br />
<br />
人员运输：使用公交车和班车的人员移动功能在 Autoware v3.0 中得到开发和支持。<br />
<br />
货物运输和自动赛车：在 Autoware v2.0 中开发并支持货物运输功能（Autoware 也用于自动赛车比赛）。<br />
<br />
自动代客泊车（AVP）：在 Autoware v1.0 中开发并支持自动代客泊车功能。<br />
<br />
Autoware已被超过500加企业使用，并在20多个国家地区的30多种车辆上运行。<br />
<br />
GitHub：<a href="https://github.com/autowarefoundation/autoware">github.com/autowarefoundatio…</a><br />
官网：<a href="https://autoware.org/">autoware.org/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA4Mzg4ODE4MjA2OTY1NzYvcHUvaW1nL2hTZmZFeU1kdXlvTkRmS3guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730820836637245452#m</id>
            <title>塔夫茨大学和哈佛大学维斯研究所团队开发出一种能够在实验室培养皿中移动的微小活体机器人。

他们将这些创造物称为“Anthrobots”，在实验中，Anthrobots 能够移动到模拟损伤的人类神经元上，并促进了受损区域的生长。

这意味着Anthrobots可能有助于修复或治疗受损的人类组织。

Anthrobots 的研究展示了活体材料在机器人技术中的新应用，为未来医疗技术的发展提供了新的可能性。

Anthrobots具有以下特性：

1、由人类细胞构成：Anthrobots 是由成人气管细胞制成，这些细胞来自不同年龄和性别的匿名捐赠者。

2、能够自我移动：这些细胞上有像毛发一样的纤毛，能够来回摆动，使得Anthrobots能够在实验室培养皿中移动。

3、促进神经元生长：在实验中，科学家们发现，这些由人类细胞制成的Anthrobots能够移动到实验室培养皿中模拟损伤的人类神经元上。更重要的是，它们能够促进这些受损神经元区域的生长。这意味着Anthrobots可能有助于修复或治疗受损的人类组织，尽管这种机制目前还不完全清楚。

4、自组装能力：不同于其他需要手工制作的生物机器人，Anthrobots是通过自然的自组装过程形成的。这意味着每个Anthrobots都是从单个细胞开始，自然地生长和发展成为一个完整的机器人实体。这种自组装的能力展示了生物材料在机器人技术中的新用途，同时也减少了制造这些机器人所需的复杂手工操作。

5、不同的形状和大小：制造出的Anthrobots形状和大小各异，有的呈球形并完全覆盖着纤毛，有的则形状更像橄榄球，纤毛覆盖不规则。

6、不构成伦理或安全问题：根据研究负责人的说法，Anthrobots不是从人类胚胎制成，也没有进行任何基因修改，因此不会引起伦理或安全方面的担忧。

尽管目前研究还处于早期阶段，但科学家们正在探索Anthrobots在医学上的潜在应用，例如帮助治疗伤口或受损组织。

详细报道：https://edition.cnn.com/2023/11/30/world/living-robots-from-human-cells-scn/index.html
该研究周四发表在《先进科学》杂志上：https://onlinelibrary.wiley.com/doi/10.1002/advs.202303575</title>
            <link>https://nitter.cz/xiaohuggg/status/1730820836637245452#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730820836637245452#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 05:26:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>塔夫茨大学和哈佛大学维斯研究所团队开发出一种能够在实验室培养皿中移动的微小活体机器人。<br />
<br />
他们将这些创造物称为“Anthrobots”，在实验中，Anthrobots 能够移动到模拟损伤的人类神经元上，并促进了受损区域的生长。<br />
<br />
这意味着Anthrobots可能有助于修复或治疗受损的人类组织。<br />
<br />
Anthrobots 的研究展示了活体材料在机器人技术中的新应用，为未来医疗技术的发展提供了新的可能性。<br />
<br />
Anthrobots具有以下特性：<br />
<br />
1、由人类细胞构成：Anthrobots 是由成人气管细胞制成，这些细胞来自不同年龄和性别的匿名捐赠者。<br />
<br />
2、能够自我移动：这些细胞上有像毛发一样的纤毛，能够来回摆动，使得Anthrobots能够在实验室培养皿中移动。<br />
<br />
3、促进神经元生长：在实验中，科学家们发现，这些由人类细胞制成的Anthrobots能够移动到实验室培养皿中模拟损伤的人类神经元上。更重要的是，它们能够促进这些受损神经元区域的生长。这意味着Anthrobots可能有助于修复或治疗受损的人类组织，尽管这种机制目前还不完全清楚。<br />
<br />
4、自组装能力：不同于其他需要手工制作的生物机器人，Anthrobots是通过自然的自组装过程形成的。这意味着每个Anthrobots都是从单个细胞开始，自然地生长和发展成为一个完整的机器人实体。这种自组装的能力展示了生物材料在机器人技术中的新用途，同时也减少了制造这些机器人所需的复杂手工操作。<br />
<br />
5、不同的形状和大小：制造出的Anthrobots形状和大小各异，有的呈球形并完全覆盖着纤毛，有的则形状更像橄榄球，纤毛覆盖不规则。<br />
<br />
6、不构成伦理或安全问题：根据研究负责人的说法，Anthrobots不是从人类胚胎制成，也没有进行任何基因修改，因此不会引起伦理或安全方面的担忧。<br />
<br />
尽管目前研究还处于早期阶段，但科学家们正在探索Anthrobots在医学上的潜在应用，例如帮助治疗伤口或受损组织。<br />
<br />
详细报道：<a href="https://edition.cnn.com/2023/11/30/world/living-robots-from-human-cells-scn/index.html">edition.cnn.com/2023/11/30/w…</a><br />
该研究周四发表在《先进科学》杂志上：<a href="https://onlinelibrary.wiley.com/doi/10.1002/advs.202303575">onlinelibrary.wiley.com/doi/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FVYjVsVWIwQUVmU0hsLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FVYjhhVGE4QUFERmpGLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FVY0FSU2FrQUFVYlprLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730795955375739093#m</id>
            <title>Marker ：快速准确地将各种文件转换为 Markdown 格式

📄支持多种格式：能够将 PDF、EPUB、MOBI文件转换 Markdown 格式。

⚡ 高速和高准确度：提供快速的转换速度和高准确度，（针对书籍和科学论文进行了优化）速度比 nougat 快 10 倍。

🚫 去除多余元素：自动去除页眉、页脚和其他不必要元素。

🔢 方程转 LaTeX：能够将大多数数学方程转换为 LaTeX 格式，适用于科学和学术文档。

💻 格式化代码和表格：能够识别并格式化文档中的代码块和表格。

🔍 OCR 功能：如果需要，它会使用OCR来处理文档

🌍 支持多种语言：支持多种语言

GitHub：https://github.com/VikParuchuri/marker</title>
            <link>https://nitter.cz/xiaohuggg/status/1730795955375739093#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730795955375739093#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 03:48:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Marker ：快速准确地将各种文件转换为 Markdown 格式<br />
<br />
📄支持多种格式：能够将 PDF、EPUB、MOBI文件转换 Markdown 格式。<br />
<br />
⚡ 高速和高准确度：提供快速的转换速度和高准确度，（针对书籍和科学论文进行了优化）速度比 nougat 快 10 倍。<br />
<br />
🚫 去除多余元素：自动去除页眉、页脚和其他不必要元素。<br />
<br />
🔢 方程转 LaTeX：能够将大多数数学方程转换为 LaTeX 格式，适用于科学和学术文档。<br />
<br />
💻 格式化代码和表格：能够识别并格式化文档中的代码块和表格。<br />
<br />
🔍 OCR 功能：如果需要，它会使用OCR来处理文档<br />
<br />
🌍 支持多种语言：支持多种语言<br />
<br />
GitHub：<a href="https://github.com/VikParuchuri/marker">github.com/VikParuchuri/mark…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FVRmp4VGJjQUExU2NQLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FVRnNMS2JRQUFUc3JtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730774966701027828#m</id>
            <title>ChatGPT-Web：一个基于 OpenAI API 的开源聊天界面。

界面和官方页面很像，无需复杂的设置或安装额外的软件。你只需输入自己的 OpenAI API 密钥，就可以开始使用。

功能强大：支持语音输入、DALL·E图像生成、导出聊天记录Markdown、代码高亮、流式输出...

所有消息都存储在用户浏览器的本地存储。

OpenAI 按代币计费，便宜很多，如果你每月使用量不超过超过 1000 万个代币，用API比 ChatGPT Plus会员更划算。可以试试！

主要功能特点：

🌐 界面简洁易用：可直接在浏览器中与 ChatGPT 交流
🎤 支持语音输入：支持语音输入，方便进行语音对话
📤 导出聊天记录：可将聊天记录导出为 Markdown 文件
🖥️ 代码高亮显示：识别和高亮显示代码块，适合技术对话
🎨 支持图像生成：支持使用 DALL·E 模型生成图像
📱 支持手机平板：方便移动设备用户使用
🔄 流式响应显示：实时显示 API 的响应，让用户看到回答的生成过程
🔒 本地隐私保护：聊天记录仅存储在用户的浏览器本地，保证私密性
💾 对话持续不丢失：关闭浏览器后可继续之前的对话，不丢失聊天记录

除了 OpenAI，ChatGPT-Web 还提供了使用 Petals swarm 作为免费 API 选项，以支持开放的聊天模型，如 Llama 2。

您还可以使用 ChatGPT-web 作为桌面应用程序。为此，请先安装 Rust。然后，只需运行 npm run tauri dev （对于桌面应用程序的开发版本）或 npm run tauri build （对于桌面应用程序的生产版本）。桌面应用程序将构建在 src-tauri/target 文件夹中。

GitHub：https://github.com/Niek/chatgpt-web
在线演示：https://niek.github.io/chatgpt-web/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730774966701027828#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730774966701027828#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 02:24:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT-Web：一个基于 OpenAI API 的开源聊天界面。<br />
<br />
界面和官方页面很像，无需复杂的设置或安装额外的软件。你只需输入自己的 OpenAI API 密钥，就可以开始使用。<br />
<br />
功能强大：支持语音输入、DALL·E图像生成、导出聊天记录Markdown、代码高亮、流式输出...<br />
<br />
所有消息都存储在用户浏览器的本地存储。<br />
<br />
OpenAI 按代币计费，便宜很多，如果你每月使用量不超过超过 1000 万个代币，用API比 ChatGPT Plus会员更划算。可以试试！<br />
<br />
主要功能特点：<br />
<br />
🌐 界面简洁易用：可直接在浏览器中与 ChatGPT 交流<br />
🎤 支持语音输入：支持语音输入，方便进行语音对话<br />
📤 导出聊天记录：可将聊天记录导出为 Markdown 文件<br />
🖥️ 代码高亮显示：识别和高亮显示代码块，适合技术对话<br />
🎨 支持图像生成：支持使用 DALL·E 模型生成图像<br />
📱 支持手机平板：方便移动设备用户使用<br />
🔄 流式响应显示：实时显示 API 的响应，让用户看到回答的生成过程<br />
🔒 本地隐私保护：聊天记录仅存储在用户的浏览器本地，保证私密性<br />
💾 对话持续不丢失：关闭浏览器后可继续之前的对话，不丢失聊天记录<br />
<br />
除了 OpenAI，ChatGPT-Web 还提供了使用 Petals swarm 作为免费 API 选项，以支持开放的聊天模型，如 Llama 2。<br />
<br />
您还可以使用 ChatGPT-web 作为桌面应用程序。为此，请先安装 Rust。然后，只需运行 npm run tauri dev （对于桌面应用程序的开发版本）或 npm run tauri build （对于桌面应用程序的生产版本）。桌面应用程序将构建在 src-tauri/target 文件夹中。<br />
<br />
GitHub：<a href="https://github.com/Niek/chatgpt-web">github.com/Niek/chatgpt-web</a><br />
在线演示：<a href="https://niek.github.io/chatgpt-web/">niek.github.io/chatgpt-web/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUeU05YmFjQUE5VThULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730760301061304740#m</id>
            <title>这个影子细节也处理了

厉害👍

好久没关注@WonderDynamics 了，其实可以用wonder studio 做一些短视频创业！😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1730760301061304740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730760301061304740#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 01:26:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个影子细节也处理了<br />
<br />
厉害👍<br />
<br />
好久没关注<a href="https://nitter.cz/WonderDynamics" title="Wonder Dynamics">@WonderDynamics</a> 了，其实可以用wonder studio 做一些短视频创业！😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMwNjg4NjMzNzkzOTA4NzM2L2ltZy9wT1M0Zjh0eklKUmI4a0VGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730611412261916704#m</id>
            <title>真正的

街舞🕺🏻</title>
            <link>https://nitter.cz/xiaohuggg/status/1730611412261916704#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730611412261916704#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 15:34:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>真正的<br />
<br />
街舞🕺🏻</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMwNjExMjU5OTc5MjgwMzg0L2ltZy9kTVp4V1FpTDhsRlhtcEF6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730548066246410631#m</id>
            <title>R to @xiaohuggg: GAIA不仅能根据语音自动生成头部动作，还允许用户自定义头部的动作。例如，如果用户想要虚拟头像在说话时摇头或点头，他们可以指定这样的动作，而GAIA将能够在不影响嘴唇运动与语音同步的情况下实现这一动作。

这增加了虚拟头像视频生成的灵活性和可控性，使其更适用于各种不同的应用场景。</title>
            <link>https://nitter.cz/xiaohuggg/status/1730548066246410631#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730548066246410631#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 11:23:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GAIA不仅能根据语音自动生成头部动作，还允许用户自定义头部的动作。例如，如果用户想要虚拟头像在说话时摇头或点头，他们可以指定这样的动作，而GAIA将能够在不影响嘴唇运动与语音同步的情况下实现这一动作。<br />
<br />
这增加了虚拟头像视频生成的灵活性和可控性，使其更适用于各种不同的应用场景。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA1NDc2NzA0MDg5MTI4OTYvcHUvaW1nL3F6LUo1bXJveHNqNXlLaVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730547607716643080#m</id>
            <title>微软的这个项目厉害了！！

GAIA的：能够从语音和单张肖像图片合成自然的会说话的头像视频。

它甚至支持诸如“悲伤”、“张开嘴”或“惊讶”等文本提示，来指导视频生成。

GAIA还允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。

可以接受语音、视频或文字指令创建会说话的人物头像视频。

主要功能：

1、根据语音生成会说话的虚拟人物：如果你给GAIA一个语音录音，它可以创建一个虚拟人物的视频，这个人物的嘴唇和面部表情会跟着语音动。

2、根据视频生成会说话的虚拟人物：GAIA可以观察一个真人在视频里的动作，然后创建一个虚拟人物模仿这些动作。

3、控制虚拟人物的头部姿势：你可以告诉GAIA让虚拟人物的头部做出特定的动作，比如点头或摇头。

4、完全控制虚拟人物的表情：GAIA允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。

5、根据文字指令生成虚拟人物动作：你可以给GAIA一些文字指令，比如“请微笑”，它就会创建一个按照这些指令动作的虚拟人物视频

主要工作原理：

1.分离运动和外观表示：

•GAIA首先将每个视频帧分离成运动和外观两部分的表示。这意味着它可以区分哪些部分是因为说话而动（如嘴唇运动），哪些部分是保持不变的（如头发、眼睛的位置）。

2.使用变分自编码器（VAE）：

•VAE被用来编码视频帧中的这些分离表示，并从这些表示中重建原始帧。这个过程帮助模型学习如何准确地捕捉和再现人物的面部特征和表情。

3.基于语音的运动序列生成：

•扩散模型被优化以生成基于语音序列和参考肖像图片的运动序列。这意味着模型可以根据给定的语音输入（如一段对话）生成相应的面部运动。

4.在推理过程中的应用：

•在实际应用中，扩散模型接受输入的语音序列和参考肖像图片作为条件，并生成运动序列。然后，这些运动序列被解码成视频，展示虚拟头像的说话和表情动作。

5.控制和文本指令的应用：
•GAIA还允许通过编辑生成过程中的面部标记点来控制任意面部属性，或根据文本指令生成虚拟头像的视频剪辑。

项目及演示：https://microsoft.github.io/GAIA/
论文：https://arxiv.org/abs/2311.15230
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1730547607716643080#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730547607716643080#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 11:21:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软的这个项目厉害了！！<br />
<br />
GAIA的：能够从语音和单张肖像图片合成自然的会说话的头像视频。<br />
<br />
它甚至支持诸如“悲伤”、“张开嘴”或“惊讶”等文本提示，来指导视频生成。<br />
<br />
GAIA还允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。<br />
<br />
可以接受语音、视频或文字指令创建会说话的人物头像视频。<br />
<br />
主要功能：<br />
<br />
1、根据语音生成会说话的虚拟人物：如果你给GAIA一个语音录音，它可以创建一个虚拟人物的视频，这个人物的嘴唇和面部表情会跟着语音动。<br />
<br />
2、根据视频生成会说话的虚拟人物：GAIA可以观察一个真人在视频里的动作，然后创建一个虚拟人物模仿这些动作。<br />
<br />
3、控制虚拟人物的头部姿势：你可以告诉GAIA让虚拟人物的头部做出特定的动作，比如点头或摇头。<br />
<br />
4、完全控制虚拟人物的表情：GAIA允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。<br />
<br />
5、根据文字指令生成虚拟人物动作：你可以给GAIA一些文字指令，比如“请微笑”，它就会创建一个按照这些指令动作的虚拟人物视频<br />
<br />
主要工作原理：<br />
<br />
1.分离运动和外观表示：<br />
<br />
•GAIA首先将每个视频帧分离成运动和外观两部分的表示。这意味着它可以区分哪些部分是因为说话而动（如嘴唇运动），哪些部分是保持不变的（如头发、眼睛的位置）。<br />
<br />
2.使用变分自编码器（VAE）：<br />
<br />
•VAE被用来编码视频帧中的这些分离表示，并从这些表示中重建原始帧。这个过程帮助模型学习如何准确地捕捉和再现人物的面部特征和表情。<br />
<br />
3.基于语音的运动序列生成：<br />
<br />
•扩散模型被优化以生成基于语音序列和参考肖像图片的运动序列。这意味着模型可以根据给定的语音输入（如一段对话）生成相应的面部运动。<br />
<br />
4.在推理过程中的应用：<br />
<br />
•在实际应用中，扩散模型接受输入的语音序列和参考肖像图片作为条件，并生成运动序列。然后，这些运动序列被解码成视频，展示虚拟头像的说话和表情动作。<br />
<br />
5.控制和文本指令的应用：<br />
•GAIA还允许通过编辑生成过程中的面部标记点来控制任意面部属性，或根据文本指令生成虚拟头像的视频剪辑。<br />
<br />
项目及演示：<a href="https://microsoft.github.io/GAIA/">microsoft.github.io/GAIA/</a><br />
论文：<a href="https://arxiv.org/abs/2311.15230">arxiv.org/abs/2311.15230</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA1NDY5MjAwNDM4MTkwMDgvcHUvaW1nL3M3LTFzN1V2d3RySHMxbkEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730541672759799984#m</id>
            <title>R to @xiaohuggg: Seamless翻译演示：

https://x.com/multimodalart/status/1730319682098393451</title>
            <link>https://nitter.cz/xiaohuggg/status/1730541672759799984#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730541672759799984#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 10:57:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Seamless翻译演示：<br />
<br />
<a href="https://x.com/multimodalart/status/1730319682098393451">x.com/multimodalart/status/1…</a></p>
<p><a href="https://nitter.cz/multimodalart/status/1730319682098393451#m">nitter.cz/multimodalart/status/1730319682098393451#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730524761548370322#m</id>
            <title>R to @xiaohuggg: 我要杀死它...😂

老板又把邀请码增加到500了

燃烧他的GPU吧...🔥</title>
            <link>https://nitter.cz/xiaohuggg/status/1730524761548370322#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730524761548370322#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 09:50:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我要杀死它...😂<br />
<br />
老板又把邀请码增加到500了<br />
<br />
燃烧他的GPU吧...🔥</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FRT3UwcGFjQUEtc3JPLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730521049576722624#m</id>
            <title>教程：让ChatGPT完全控制你的电脑  

它像人类用户一样使用鼠标和键盘，自主进行操作

博主@MatthewBerman 使用了前几天另外一位博主@josh_bickett 开发的Self-Operating Computer（自动化计算机框架） 进行了这项测试。

该框架利用GPT 4V，让AI看着电脑屏幕，然后自主使用鼠标和键盘来完成任务。

这个框架设计得可以和不同的AI模型一起工作，目前已经可以和GPT-4v这样的模型集成。

目前，这个AI模型在估计鼠标点击的具体位置时还不够准确。项目团队正在开发一个新的AI模型（Agent-1-Vision），这个模型在预测鼠标点击位置方面会更准确。

感兴趣的可以去试试，视频中也有详细教程！

Self-Operating Computer：https://github.com/OthersideAI/self-operating-computer

视频来源：https://www.youtube.com/watch?v=UKRti40U8IA</title>
            <link>https://nitter.cz/xiaohuggg/status/1730521049576722624#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730521049576722624#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 09:35:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>教程：让ChatGPT完全控制你的电脑  <br />
<br />
它像人类用户一样使用鼠标和键盘，自主进行操作<br />
<br />
博主<a href="https://nitter.cz/MatthewBerman" title="MatthewBerman">@MatthewBerman</a> 使用了前几天另外一位博主<a href="https://nitter.cz/josh_bickett" title="Josh Bickett">@josh_bickett</a> 开发的Self-Operating Computer（自动化计算机框架） 进行了这项测试。<br />
<br />
该框架利用GPT 4V，让AI看着电脑屏幕，然后自主使用鼠标和键盘来完成任务。<br />
<br />
这个框架设计得可以和不同的AI模型一起工作，目前已经可以和GPT-4v这样的模型集成。<br />
<br />
目前，这个AI模型在估计鼠标点击的具体位置时还不够准确。项目团队正在开发一个新的AI模型（Agent-1-Vision），这个模型在预测鼠标点击位置方面会更准确。<br />
<br />
感兴趣的可以去试试，视频中也有详细教程！<br />
<br />
Self-Operating Computer：<a href="https://github.com/OthersideAI/self-operating-computer">github.com/OthersideAI/self-…</a><br />
<br />
视频来源：<a href="https://www.youtube.com/watch?v=UKRti40U8IA">youtube.com/watch?v=UKRti40U…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA1MTc4MTkyMzY5NjY0MDAvcHUvaW1nL2dPeDhIV1BLVXNtVTJsRDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730510553590952297#m</id>
            <title>嘴上说不要，身体还是很诚实

最后老板给了我100个邀请码

Code: HUG （通用的，100个领完就没了）

http://freepik.com/pikaso</title>
            <link>https://nitter.cz/xiaohuggg/status/1730510553590952297#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730510553590952297#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 08:54:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>嘴上说不要，身体还是很诚实<br />
<br />
最后老板给了我100个邀请码<br />
<br />
Code: HUG （通用的，100个领完就没了）<br />
<br />
<a href="http://freepik.com/pikaso">freepik.com/pikaso</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730485302593225108#m">nitter.cz/xiaohuggg/status/1730485302593225108#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FRQnpSOGFVQUFGV1A3LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730485302593225108#m</id>
            <title>给你们展示个魔法，实时作画...

用的Freepik的Pikaso

拖拽小图标或者上传素材即可实时生图...

技术好的，可以自己画笔画，不过我感觉是内置了一些固定风格，不够自由！

体验地址：https://www.freepik.com/pikaso 

目前需要邀请码！（我看看能要到吗）</title>
            <link>https://nitter.cz/xiaohuggg/status/1730485302593225108#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730485302593225108#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 07:13:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>给你们展示个魔法，实时作画...<br />
<br />
用的Freepik的Pikaso<br />
<br />
拖拽小图标或者上传素材即可实时生图...<br />
<br />
技术好的，可以自己画笔画，不过我感觉是内置了一些固定风格，不够自由！<br />
<br />
体验地址：<a href="https://www.freepik.com/pikaso">freepik.com/pikaso</a> <br />
<br />
目前需要邀请码！（我看看能要到吗）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0ODIyOTQ5NzExMTc1NjgvcHUvaW1nLzgtMmVSOWpsaFc0VVVRb0QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476929500004627#m</id>
            <title>R to @xiaohuggg: 生成的另一种类型风格演示

你们可以玩玩</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476929500004627#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476929500004627#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:40:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>生成的另一种类型风格演示<br />
<br />
你们可以玩玩</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzY3OTI3MTEwOTAxNzYvcHUvaW1nLzIwbE13S2F4eFpoM0w2RmkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476711857586572#m</id>
            <title>R to @xiaohuggg: 这是上面演示视频中生成的音乐

🎵</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476711857586572#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476711857586572#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:39:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是上面演示视频中生成的音乐<br />
<br />
🎵</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzY1NDA2MjkzMjM3NzYvcHUvaW1nL2xrMEtwUHB3Mm9PeE9xbmsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476486820597883#m</id>
            <title>Stable Audio升级了一些新功能 生成音乐更可控

- 内置了风格提示库，随便点点就能生成音乐

- 支持通过上传音乐来生成音乐。

- 增加控制选项，如种子、步数、提示强度等

-能够生成并下载44.1 kHz立体声的高质量音频

- 现在可以直接通过链接分享生成的音乐

- 直接帮你把把生成好的音乐坐车了视频，方便下载

- 免费版本允许每月生成20个音轨，每个音轨最长45秒

- 付费版本每月11.99美元，提供500个音轨生成，每个音轨最长90秒

体验：http://stableaudio.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476486820597883#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476486820597883#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:38:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable Audio升级了一些新功能 生成音乐更可控<br />
<br />
- 内置了风格提示库，随便点点就能生成音乐<br />
<br />
- 支持通过上传音乐来生成音乐。<br />
<br />
- 增加控制选项，如种子、步数、提示强度等<br />
<br />
-能够生成并下载44.1 kHz立体声的高质量音频<br />
<br />
- 现在可以直接通过链接分享生成的音乐<br />
<br />
- 直接帮你把把生成好的音乐坐车了视频，方便下载<br />
<br />
- 免费版本允许每月生成20个音轨，每个音轨最长45秒<br />
<br />
- 付费版本每月11.99美元，提供500个音轨生成，每个音轨最长90秒<br />
<br />
体验：<a href="http://stableaudio.com/">stableaudio.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzU3ODEwNjYwMTQ3MjAvcHUvaW1nL1EtSGhfMnlLRllQdGxSYnguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730456579382849756#m</id>
            <title>R to @xiaohuggg: 里面的视频都是这样

包含一个第一人称视角和多个全方位的视角

让AI来学习人类是如何进行各种技能操作的</title>
            <link>https://nitter.cz/xiaohuggg/status/1730456579382849756#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730456579382849756#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 05:19:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>里面的视频都是这样<br />
<br />
包含一个第一人称视角和多个全方位的视角<br />
<br />
让AI来学习人类是如何进行各种技能操作的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NTYwNDUyNDY3MzAyNDAvcHUvaW1nL1I3a2VWN2ZJanVsYTlsUUouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730455784092549356#m</id>
            <title>Ego-Exo4D：用于视频学习和多模态感知研究的基础数据集

由Meta AI和15所大学共同开发，这个项目收集了两种类型的视频 ：

一种是戴着相机的人看到的（类似眼睛看世界）

另一种是从周围环境拍摄的（旁观者看这个人）

数据集目的是帮助AI更全面地学习和了解一个人在特定环境下是如何行动的。

Ego-Exo4D包含一个基础数据集和基准测试套件，旨在支持视频学习和多模态感知的研究。

主要特点：

1、首人称和第三人称视角的结合：

•Ego-Exo4D的核心是同时捕捉首人称“自我中心”视角（通过参与者佩戴的可穿戴相机）和多个“外心”视角（通过围绕参与者的相机）。
•这两种视角是互补的：自我中心视角揭示了参与者所看到和听到的内容，而外心视角揭示了周围的场景和上下文。

2、专注于熟练的人类活动：

•Ego-Exo4D专注于熟练的人类活动，如运动、音乐、烹饪、舞蹈和自行车维修。
•在视频中理解人类技能的AI进步可以促进许多应用，例如在增强现实（AR）系统中，佩戴智能眼镜的人可以通过虚拟AI教练的指导快速学习新技能。

3、数据集的规模和多样性：

•Ego-Exo4D构成了迄今为止最大的公共同步首人称和第三人称视频数据集。
•该数据集的建立需要招募不同领域的专家，汇集了多样化的人群来创建一个多方面的AI数据集。

4、多模态数据捕捉：

•使用Meta的独特Aria眼镜捕捉的所有自我中心视频都伴随着时间对齐的七通道音频、惯性测量单元（IMU）和两个广角灰度相机等传感器。
•所有数据序列还提供通过Project Aria的先进机器感知服务获得的眼动追踪、头部姿态和环境的3D点云。

5.基准测试和未来应用：

•除了数据，Ego-Exo4D还引入了基础任务的基准测试，以激发社区的努力。
•该项目的长远目标是使AI能够以新的方式帮助人们学习新技能，并为未来的机器人提供通过观察熟练的人类专家行动来获得复杂灵巧操作技能的洞察。

目的：

1、帮助研究人工智能：这些视频数据可以帮助科学家们训练和改进人工智能系统，让它们更好地理解人类的行为和技能，比如运动、音乐、烹饪、舞蹈和自行车维修。

2、应用于增强现实和机器人技术：这个项目的数据可以用来开发新的增强现实应用（比如通过智能眼镜学习新技能）和帮助机器人学习如何模仿人类的动作。

3、提供大量多样的视频：Ego-Exo4D收集了大量不同场景和活动的视频，这对于研究人工智能来说是一个宝贵的资源。

4、推动人工智能的新挑战：这个项目不仅提供数据，还设立了一些挑战，鼓励科学家们开发新技术，比如从视频中识别动作的细节，或者评估一个人做某件事的技能水平。

Ego-Exo4D项目就像是一个超级工具箱，可以帮助科学家们让人工智能更好地理解和模仿人类。

详细介绍：https://ai.meta.com/blog/ego-exo4d-video-learning-perception/
论文：https://ego-exo4d-data.org/paper/ego-exo4d.pdf
Ego-Exo4D网站：https://ego-exo4d-data.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730455784092549356#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730455784092549356#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 05:16:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ego-Exo4D：用于视频学习和多模态感知研究的基础数据集<br />
<br />
由Meta AI和15所大学共同开发，这个项目收集了两种类型的视频 ：<br />
<br />
一种是戴着相机的人看到的（类似眼睛看世界）<br />
<br />
另一种是从周围环境拍摄的（旁观者看这个人）<br />
<br />
数据集目的是帮助AI更全面地学习和了解一个人在特定环境下是如何行动的。<br />
<br />
Ego-Exo4D包含一个基础数据集和基准测试套件，旨在支持视频学习和多模态感知的研究。<br />
<br />
主要特点：<br />
<br />
1、首人称和第三人称视角的结合：<br />
<br />
•Ego-Exo4D的核心是同时捕捉首人称“自我中心”视角（通过参与者佩戴的可穿戴相机）和多个“外心”视角（通过围绕参与者的相机）。<br />
•这两种视角是互补的：自我中心视角揭示了参与者所看到和听到的内容，而外心视角揭示了周围的场景和上下文。<br />
<br />
2、专注于熟练的人类活动：<br />
<br />
•Ego-Exo4D专注于熟练的人类活动，如运动、音乐、烹饪、舞蹈和自行车维修。<br />
•在视频中理解人类技能的AI进步可以促进许多应用，例如在增强现实（AR）系统中，佩戴智能眼镜的人可以通过虚拟AI教练的指导快速学习新技能。<br />
<br />
3、数据集的规模和多样性：<br />
<br />
•Ego-Exo4D构成了迄今为止最大的公共同步首人称和第三人称视频数据集。<br />
•该数据集的建立需要招募不同领域的专家，汇集了多样化的人群来创建一个多方面的AI数据集。<br />
<br />
4、多模态数据捕捉：<br />
<br />
•使用Meta的独特Aria眼镜捕捉的所有自我中心视频都伴随着时间对齐的七通道音频、惯性测量单元（IMU）和两个广角灰度相机等传感器。<br />
•所有数据序列还提供通过Project Aria的先进机器感知服务获得的眼动追踪、头部姿态和环境的3D点云。<br />
<br />
5.基准测试和未来应用：<br />
<br />
•除了数据，Ego-Exo4D还引入了基础任务的基准测试，以激发社区的努力。<br />
•该项目的长远目标是使AI能够以新的方式帮助人们学习新技能，并为未来的机器人提供通过观察熟练的人类专家行动来获得复杂灵巧操作技能的洞察。<br />
<br />
目的：<br />
<br />
1、帮助研究人工智能：这些视频数据可以帮助科学家们训练和改进人工智能系统，让它们更好地理解人类的行为和技能，比如运动、音乐、烹饪、舞蹈和自行车维修。<br />
<br />
2、应用于增强现实和机器人技术：这个项目的数据可以用来开发新的增强现实应用（比如通过智能眼镜学习新技能）和帮助机器人学习如何模仿人类的动作。<br />
<br />
3、提供大量多样的视频：Ego-Exo4D收集了大量不同场景和活动的视频，这对于研究人工智能来说是一个宝贵的资源。<br />
<br />
4、推动人工智能的新挑战：这个项目不仅提供数据，还设立了一些挑战，鼓励科学家们开发新技术，比如从视频中识别动作的细节，或者评估一个人做某件事的技能水平。<br />
<br />
Ego-Exo4D项目就像是一个超级工具箱，可以帮助科学家们让人工智能更好地理解和模仿人类。<br />
<br />
详细介绍：<a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">ai.meta.com/blog/ego-exo4d-v…</a><br />
论文：<a href="https://ego-exo4d-data.org/paper/ego-exo4d.pdf">ego-exo4d-data.org/paper/ego…</a><br />
Ego-Exo4D网站：<a href="https://ego-exo4d-data.org/">ego-exo4d-data.org/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NTQ3NTYzNzg5OTI2NDAvcHUvaW1nL19KdHp5S2JPRV9uVjJiVEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730427886065324492#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1730427886065324492#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730427886065324492#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 03:25:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MjY2MDIyNzY5OTkxNjgvcHUvaW1nL3BUSlAteWNDcU5sSmQ5MmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>