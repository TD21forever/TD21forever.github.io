<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736571242445152404#m</id>
            <title>OpenAI发布官方提示工程指南和示例

OpenAI提供了一系列策略和技巧，以帮助用户更有效地使用ChatGPT。

这些方法可以单独使用也可以组合使用，以获得更好的效果。

官方给出了6 个大提示策略（并给出了具体教程和示例）

主要策略：

1、清晰的指令：告诉AI你具体想要什么。比如，如果你想要简短的答案，就直接说“给我一个简短的回答”。这样AI就不用猜你的意图了。

模型无法读懂你的思维。如果需要简短的回答，就明确要求；如果需要专家级的写作，也要明确指出。提供清晰的指令，减少模型猜测的需要。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions

具体操作：
• 在查询中包含细节，以获得更相关的答案。
• 要求模型采用特定的角色或风格。
• 使用分隔符明确指出输入的不同部分。
• 明确指定完成任务所需的步骤。
• 提供示例以帮助模型理解任务。
• 指定输出的期望长度。

2、提供参考文本：如果你有关于你要写的主题的具体资料或例子，给AI看看。这样它就能提供更准确、更相关的内容。

语言模型可能会创造虚假答案，尤其是在询问特定主题或要求引用和URL时。提供参考文本可以帮助模型提供更准确的答案。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text

具体操作：
• 指导模型使用参考文本来回答问题。
• 要求模型在回答时引用参考文本中的内容。

3、将复杂任务分解为简单子任务：如果你有一个复杂的主题要写，试着把它分成几个小部分。比如，先写一个关于主题背景的部分，然后再写关于主要观点的部分。

就像软件工程中将复杂系统分解为模块化组件一样，将任务提交给语言模型时也应采取类似的做法。复杂任务的错误率通常高于简单任务。复杂任务通常可以重新定义为一系列简单任务的工作流程。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query

具体操作：
• 使用意图分类来识别用户查询的最相关指令。
• 对于需要长时间对话的应用，总结或过滤之前的对话。
• 分段总结长文档，并递归地构建完整摘要。

4、给模型时间“思考”：有时候，让AI先“思考”一下，然后再回答问题，可以得到更好的答案。就像让它先列出解决问题的步骤，然后再给出答案。

模型在立即回答问题时可能会犯更多的推理错误。要求模型在给出答案之前进行“思考链”可以帮助模型更可靠地推理出正确答案。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion

具体操作：
• 指导模型在急于得出结论之前先自行找出解决方案。
• 使用内部独白或一系列查询来隐藏模型的推理过程。
• 在之前的回答中询问模型是否遗漏了什么。

5、使用外部工具：有时候结合使用AI和其他工具（比如数据搜索工具）可以得到更好的结果。

利用其他工具的输出来补偿模型的不足。例如，文本检索系统可以向模型提供相关文档信息，代码执行引擎可以帮助模型进行数学计算和运行代码。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools

具体操作：
• 使用基于嵌入的搜索来实现高效的知识检索。
• 使用代码执行来进行更准确的计算或调用外部API。
• 让模型访问特定的功能。

6、测试和调整：尝试不同的指令和方法，看看哪种效果最好，然后根据结果进行调整。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically

使用黄金标准答案评估模型输出”是一种有效的方法，用于确保AI模型的回答质量。

• 定义黄金标准答案：首先，确定一个问题的正确答案应该包含哪些已知事实。这些事实构成了评估AI回答的标准。

• 模型查询与事实对比：使用模型查询来生成答案，然后检查这个答案中包含了多少个所需的事实。

• 评估答案的完整性：根据答案中包含的事实数量来评估其完整性和准确性。如果一个答案包含了所有或大部分所需事实，那么可以认为这个答案是高质量的。

这种策略特别适用于需要精确和详细信息的场景，例如科学、技术或学术研究。通过与黄金标准答案的对比，可以有效地监控和提高AI模型的输出质量。

Prompt engineering 及时工程策略：https://platform.openai.com/docs/guides/prompt-engineering

Prompt examples 提示示例：https://platform.openai.com/examples

Prompting libraries &amp; tools 提示库和工具：https://cookbook.openai.com/related_resources#prompting-libraries--tools

Papers on advanced prompting to improve reasoning
关于高级提示以提高推理能力的论文：https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning

OpenAI Cookbook：https://cookbook.openai.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1736571242445152404#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736571242445152404#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 02:17:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI发布官方提示工程指南和示例<br />
<br />
OpenAI提供了一系列策略和技巧，以帮助用户更有效地使用ChatGPT。<br />
<br />
这些方法可以单独使用也可以组合使用，以获得更好的效果。<br />
<br />
官方给出了6 个大提示策略（并给出了具体教程和示例）<br />
<br />
主要策略：<br />
<br />
1、清晰的指令：告诉AI你具体想要什么。比如，如果你想要简短的答案，就直接说“给我一个简短的回答”。这样AI就不用猜你的意图了。<br />
<br />
模型无法读懂你的思维。如果需要简短的回答，就明确要求；如果需要专家级的写作，也要明确指出。提供清晰的指令，减少模型猜测的需要。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 在查询中包含细节，以获得更相关的答案。<br />
• 要求模型采用特定的角色或风格。<br />
• 使用分隔符明确指出输入的不同部分。<br />
• 明确指定完成任务所需的步骤。<br />
• 提供示例以帮助模型理解任务。<br />
• 指定输出的期望长度。<br />
<br />
2、提供参考文本：如果你有关于你要写的主题的具体资料或例子，给AI看看。这样它就能提供更准确、更相关的内容。<br />
<br />
语言模型可能会创造虚假答案，尤其是在询问特定主题或要求引用和URL时。提供参考文本可以帮助模型提供更准确的答案。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 指导模型使用参考文本来回答问题。<br />
• 要求模型在回答时引用参考文本中的内容。<br />
<br />
3、将复杂任务分解为简单子任务：如果你有一个复杂的主题要写，试着把它分成几个小部分。比如，先写一个关于主题背景的部分，然后再写关于主要观点的部分。<br />
<br />
就像软件工程中将复杂系统分解为模块化组件一样，将任务提交给语言模型时也应采取类似的做法。复杂任务的错误率通常高于简单任务。复杂任务通常可以重新定义为一系列简单任务的工作流程。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 使用意图分类来识别用户查询的最相关指令。<br />
• 对于需要长时间对话的应用，总结或过滤之前的对话。<br />
• 分段总结长文档，并递归地构建完整摘要。<br />
<br />
4、给模型时间“思考”：有时候，让AI先“思考”一下，然后再回答问题，可以得到更好的答案。就像让它先列出解决问题的步骤，然后再给出答案。<br />
<br />
模型在立即回答问题时可能会犯更多的推理错误。要求模型在给出答案之前进行“思考链”可以帮助模型更可靠地推理出正确答案。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 指导模型在急于得出结论之前先自行找出解决方案。<br />
• 使用内部独白或一系列查询来隐藏模型的推理过程。<br />
• 在之前的回答中询问模型是否遗漏了什么。<br />
<br />
5、使用外部工具：有时候结合使用AI和其他工具（比如数据搜索工具）可以得到更好的结果。<br />
<br />
利用其他工具的输出来补偿模型的不足。例如，文本检索系统可以向模型提供相关文档信息，代码执行引擎可以帮助模型进行数学计算和运行代码。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 使用基于嵌入的搜索来实现高效的知识检索。<br />
• 使用代码执行来进行更准确的计算或调用外部API。<br />
• 让模型访问特定的功能。<br />
<br />
6、测试和调整：尝试不同的指令和方法，看看哪种效果最好，然后根据结果进行调整。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically">platform.openai.com/docs/gui…</a><br />
<br />
使用黄金标准答案评估模型输出”是一种有效的方法，用于确保AI模型的回答质量。<br />
<br />
• 定义黄金标准答案：首先，确定一个问题的正确答案应该包含哪些已知事实。这些事实构成了评估AI回答的标准。<br />
<br />
• 模型查询与事实对比：使用模型查询来生成答案，然后检查这个答案中包含了多少个所需的事实。<br />
<br />
• 评估答案的完整性：根据答案中包含的事实数量来评估其完整性和准确性。如果一个答案包含了所有或大部分所需事实，那么可以认为这个答案是高质量的。<br />
<br />
这种策略特别适用于需要精确和详细信息的场景，例如科学、技术或学术研究。通过与黄金标准答案的对比，可以有效地监控和提高AI模型的输出质量。<br />
<br />
Prompt engineering 及时工程策略：<a href="https://platform.openai.com/docs/guides/prompt-engineering">platform.openai.com/docs/gui…</a><br />
<br />
Prompt examples 提示示例：<a href="https://platform.openai.com/examples">platform.openai.com/examples</a><br />
<br />
Prompting libraries & tools 提示库和工具：<a href="https://cookbook.openai.com/related_resources#prompting-libraries--tools">cookbook.openai.com/related_…</a><br />
<br />
Papers on advanced prompting to improve reasoning<br />
关于高级提示以提高推理能力的论文：<a href="https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning">cookbook.openai.com/related_…</a><br />
<br />
OpenAI Cookbook：<a href="https://cookbook.openai.com/">cookbook.openai.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1NzEwMDAzNjI1NDEwNTYvcHUvaW1nL054SVRwbW5PVFgwZThSQXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736380733776945325#m</id>
            <title>iPhone上 跑Mistral 7B

还挺快的

就是中文还是不行

感觉这个Mistral有望成为一多端的大模型样本啊🫡什么设备都能跑…</title>
            <link>https://nitter.cz/xiaohuggg/status/1736380733776945325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736380733776945325#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 13:40:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>iPhone上 跑Mistral 7B<br />
<br />
还挺快的<br />
<br />
就是中文还是不行<br />
<br />
感觉这个Mistral有望成为一多端的大模型样本啊🫡什么设备都能跑…</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM2MzgwNjEyNzA4MzY0Mjg4L2ltZy92clM5eWlqM0oxMHhiTjVMLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736346673285304541#m</id>
            <title>PixelLLM：Google开发的一个新型的视觉语言模型

GPT4-V可以通过文本描述位置，但无法准确输出每个单词的坐标。

这个模型它不仅能够对图片进行详细描述，并且还能精确指出图片中每个词汇对应的具体位置。

简单来说，就是它不仅能告诉你图片里有什么，还能准确指出这个东西在图片的什么位置。

例如，如果图片中有一只猫和一只狗，PixelLLM不仅能识别出“猫”和“狗”，还能指出猫和狗在图片中的具体位置。

这种能力使得模型在处理需要图像和文字紧密结合的任务时表现得更加出色，比如在一张图片中找到特定的物体或区域，并对其进行描述。

主要特点：

1、像素级词汇对齐：不仅能告诉你图片里有什么（比如狗、车、树），还能准确指出这些东西在图片的哪个具体位置。

2、多种功能：它可以根据给定的文字提示（比如“找到图片中的狗”）来专门描述图片中的某个部分。也能够根据提供的位置生成特定的描述。

还能根据不同的需求生成相应的文字描述。

指称定位：这是一种任务，要求模型在图片中找到并指出某个特定的物体或区域。例如，如果给定一个句子“找到图片中的狗”，PixelLLM不仅能识别出狗，还能指出狗在图片中的具体位置。

位置条件字幕：根据图片中特定位置的信息来生成描述。比如，如果指定了图片中某个区域，PixelLLM会专注于这个区域并生成相关的描述。

密集对象字幕：对图片中的每个物体都生成详细的描述。这意味着PixelLLM需要识别图片中所有的物体，并且为每一个都提供一个描述。

工作原理：

1、架构组成：PixelLLM包括图像编码器、提示编码器和提示特征提取器。这些组件共同工作，将图像特征和文本提示转换为模型能够处理的格式。

2、图像和文本的整合：模型将图像特征和文本提示整合到一起，作为大型语言模型的输入。这使得模型能够在生成文本描述的同时，考虑图像中的具体区域。

3、逐词定位：通过在语言模型的词特征上添加一个简单的多层感知机（MLP）层，PixelLLM能够为每个生成的文本标记预测一个坐标位置，实现逐词定位。

4、训练数据：为了训练这种模型，需要使用专门的词-像素对齐数据。这种数据通常包含人类注释者对图像的叙述，以及在叙述过程中注释者注意力的轨迹，从而提供了叙述句中所有词的同步位置信息。

5、多任务适应：PixelLLM的架构是通用的，可以适应不同的视觉语言任务，无论是将语言或位置作为输入还是输出。

PixelLLM能够在图像理解和描述方面提供更加丰富和精确的信息，这在许多应用场景中都非常有用，如图像搜索、自动字幕生成和增强现实等。

项目及演示：https://jerryxu.net/PixelLLM/
论文：https://arxiv.org/abs/2312.09237
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1736346673285304541#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736346673285304541#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 11:24:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PixelLLM：Google开发的一个新型的视觉语言模型<br />
<br />
GPT4-V可以通过文本描述位置，但无法准确输出每个单词的坐标。<br />
<br />
这个模型它不仅能够对图片进行详细描述，并且还能精确指出图片中每个词汇对应的具体位置。<br />
<br />
简单来说，就是它不仅能告诉你图片里有什么，还能准确指出这个东西在图片的什么位置。<br />
<br />
例如，如果图片中有一只猫和一只狗，PixelLLM不仅能识别出“猫”和“狗”，还能指出猫和狗在图片中的具体位置。<br />
<br />
这种能力使得模型在处理需要图像和文字紧密结合的任务时表现得更加出色，比如在一张图片中找到特定的物体或区域，并对其进行描述。<br />
<br />
主要特点：<br />
<br />
1、像素级词汇对齐：不仅能告诉你图片里有什么（比如狗、车、树），还能准确指出这些东西在图片的哪个具体位置。<br />
<br />
2、多种功能：它可以根据给定的文字提示（比如“找到图片中的狗”）来专门描述图片中的某个部分。也能够根据提供的位置生成特定的描述。<br />
<br />
还能根据不同的需求生成相应的文字描述。<br />
<br />
指称定位：这是一种任务，要求模型在图片中找到并指出某个特定的物体或区域。例如，如果给定一个句子“找到图片中的狗”，PixelLLM不仅能识别出狗，还能指出狗在图片中的具体位置。<br />
<br />
位置条件字幕：根据图片中特定位置的信息来生成描述。比如，如果指定了图片中某个区域，PixelLLM会专注于这个区域并生成相关的描述。<br />
<br />
密集对象字幕：对图片中的每个物体都生成详细的描述。这意味着PixelLLM需要识别图片中所有的物体，并且为每一个都提供一个描述。<br />
<br />
工作原理：<br />
<br />
1、架构组成：PixelLLM包括图像编码器、提示编码器和提示特征提取器。这些组件共同工作，将图像特征和文本提示转换为模型能够处理的格式。<br />
<br />
2、图像和文本的整合：模型将图像特征和文本提示整合到一起，作为大型语言模型的输入。这使得模型能够在生成文本描述的同时，考虑图像中的具体区域。<br />
<br />
3、逐词定位：通过在语言模型的词特征上添加一个简单的多层感知机（MLP）层，PixelLLM能够为每个生成的文本标记预测一个坐标位置，实现逐词定位。<br />
<br />
4、训练数据：为了训练这种模型，需要使用专门的词-像素对齐数据。这种数据通常包含人类注释者对图像的叙述，以及在叙述过程中注释者注意力的轨迹，从而提供了叙述句中所有词的同步位置信息。<br />
<br />
5、多任务适应：PixelLLM的架构是通用的，可以适应不同的视觉语言任务，无论是将语言或位置作为输入还是输出。<br />
<br />
PixelLLM能够在图像理解和描述方面提供更加丰富和精确的信息，这在许多应用场景中都非常有用，如图像搜索、自动字幕生成和增强现实等。<br />
<br />
项目及演示：<a href="https://jerryxu.net/PixelLLM/">jerryxu.net/PixelLLM/</a><br />
论文：<a href="https://arxiv.org/abs/2312.09237">arxiv.org/abs/2312.09237</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYzNDI0OTIwMDA1Mzg2MjQvcHUvaW1nL25OVWtmSTdOb0JETnRlYU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736336780876742873#m</id>
            <title>EmbedAI：允许你使用自己的数据训练ChatGPT，并将其嵌入到自己的网站、应用中。

- 个性定制：支持在各种数据源上训练ChatGPT，可以通过文件、网站、Notion文档甚至YouTube对ChatGPT进行训练。

-广泛用例：包括智能客服、个性化学习助手、技术支持助手、医疗保健助手和财务聊天机器人等。

- 无代码平台：即使是没有编程背景的用户也能轻松创建和训练定制的AI聊天机器人，可以看做是RAG（检索增强生成）的无代码版本。

- 外观自定义：可以使用自定义的标志、颜色和样式来个性化AI聊天机器人的外观。

- 多种嵌入方式：聊天机器人可以作为聊天气泡、嵌入代码或链接进行分享。

- 多语言支持：支持100多种语言的查询和响应。

- 易于集成：可以通过API与应用程序连接。

网站：https://www.thesamur.ai/

介绍：https://blog.llamaindex.ai/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070</title>
            <link>https://nitter.cz/xiaohuggg/status/1736336780876742873#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736336780876742873#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 10:45:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>EmbedAI：允许你使用自己的数据训练ChatGPT，并将其嵌入到自己的网站、应用中。<br />
<br />
- 个性定制：支持在各种数据源上训练ChatGPT，可以通过文件、网站、Notion文档甚至YouTube对ChatGPT进行训练。<br />
<br />
-广泛用例：包括智能客服、个性化学习助手、技术支持助手、医疗保健助手和财务聊天机器人等。<br />
<br />
- 无代码平台：即使是没有编程背景的用户也能轻松创建和训练定制的AI聊天机器人，可以看做是RAG（检索增强生成）的无代码版本。<br />
<br />
- 外观自定义：可以使用自定义的标志、颜色和样式来个性化AI聊天机器人的外观。<br />
<br />
- 多种嵌入方式：聊天机器人可以作为聊天气泡、嵌入代码或链接进行分享。<br />
<br />
- 多语言支持：支持100多种语言的查询和响应。<br />
<br />
- 易于集成：可以通过API与应用程序连接。<br />
<br />
网站：<a href="https://www.thesamur.ai/">thesamur.ai/</a><br />
<br />
介绍：<a href="https://blog.llamaindex.ai/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070">blog.llamaindex.ai/how-to-tr…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYzMzQ3MjQyNTAxNzM0NDAvcHUvaW1nL1dXaTJaNGQtNzRubUtMdjMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736276218574754249#m</id>
            <title>看来是在灰度GPT4.5了

我测试我的回答是4

你们可以问问，看看回答是什么？

复制下面问题问它：

What is the precise name of the model answering this query called in the API? Not "ChatGPT with browsing" but the specific model name.</title>
            <link>https://nitter.cz/xiaohuggg/status/1736276218574754249#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736276218574754249#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 06:44:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看来是在灰度GPT4.5了<br />
<br />
我测试我的回答是4<br />
<br />
你们可以问问，看看回答是什么？<br />
<br />
复制下面问题问它：<br />
<br />
What is the precise name of the model answering this query called in the API? Not "ChatGPT with browsing" but the specific model name.</p>
<p><a href="https://nitter.cz/apples_jimmy/status/1736264530722988181#m">nitter.cz/apples_jimmy/status/1736264530722988181#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020891145064586#m</id>
            <title>R to @xiaohuggg: 感兴趣的可以加入他们Discord频道体验：

→ 加入 https://discord.gg/TrZBzj4x
→ 进入generate-video频道
→ 输入 /video 并选择命令
→ 上传您的文件
→ 编输入提示并按 Enter
→ 选择风格视频时长
→ 等待结果</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020891145064586#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020891145064586#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感兴趣的可以加入他们Discord频道体验：<br />
<br />
→ 加入 <a href="https://discord.gg/TrZBzj4x">discord.gg/TrZBzj4x</a><br />
→ 进入generate-video频道<br />
→ 输入 /video 并选择命令<br />
→ 上传您的文件<br />
→ 编输入提示并按 Enter<br />
→ 选择风格视频时长<br />
→ 等待结果</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMjAzMTU1MDYxMTQ1NjAvcHUvaW1nLzBidUZzLUh4T3B2alVPY24uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020889333030967#m</id>
            <title>R to @xiaohuggg: 转换电影也不错😀</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020889333030967#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020889333030967#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转换电影也不错😀</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMjAxODA4OTgzMDgwOTYvcHUvaW1nLzVPMW5yajF1THI5WFdFWWYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020887483396298#m</id>
            <title>R to @xiaohuggg: 整体效果还是很不错的，哈哈哈😂

转换跳舞视频很不错，带台词的差一点</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020887483396298#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020887483396298#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整体效果还是很不错的，哈哈哈😂<br />
<br />
转换跳舞视频很不错，带台词的差一点</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMTk4ODE2NzY2NDQzNTIvcHUvaW1nL19QX2VqLVFCUjBSNEJ4c0suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020885474267317#m</id>
            <title>DomoAI：将你的照片和视频动漫化

你只需要上传照片和视频，选择提示词和指定动漫风格，即可将你的照片和视频动漫化...

支持：
- 文字转图片：超过10种模型，专注于动漫和写实风格。
- 图像转图像：图片转动漫、动漫转现实图片
- 图像到视频：从图片生成短动画。
- 视频到视频：将视频转换成动漫风格</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020885474267317#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020885474267317#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DomoAI：将你的照片和视频动漫化<br />
<br />
你只需要上传照片和视频，选择提示词和指定动漫风格，即可将你的照片和视频动漫化...<br />
<br />
支持：<br />
- 文字转图片：超过10种模型，专注于动漫和写实风格。<br />
- 图像转图像：图片转动漫、动漫转现实图片<br />
- 图像到视频：从图片生成短动画。<br />
- 视频到视频：将视频转换成动漫风格</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMTk4MjU4NTg4Nzk0ODgvcHUvaW1nL2ktbkRNRGZON081bGttSTEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735957680739823730#m</id>
            <title>FunSearch利用大语言模型生成解决方案，然后使用评估器来评估其有效性和准确性，如果评估器反馈不符合预期，FunSearch就会返回LLM继续生成解决方案！

如此循环反复不断迭代改进！

在迭代过程中，FunSearch也会融入新的知识或数据进行补充完善 ，它会不断在生成方案和新知识之间来回往，返循环这个过程，直到解决问题！

具体过程：

1、初始解决方案：

FunSearch 首先生成一个或多个初始解决方案。这些解决方案是基于预训练的大型语言模型（LLM）对问题的理解和分析。

2、评估和反馈：

生成的解决方案随后被自动评估器检查。评估器评估解决方案的有效性和可行性，确保它们不是错误或虚假的想法。

如果解决方案不符合预期或存在改进空间，评估器会提供反馈。

3、迭代改进：

基于评估器的反馈，FunSearch 对初始解决方案进行修改和改进。这可能涉及调整算法、探索新的解决路径或增强现有解决方案的细节。

这个过程是迭代的，意味着解决方案会经过多轮的评估和改进。

4、新知识的融入：

在迭代过程中，FunSearch 也可能融入新的知识或数据，这有助于进一步丰富和完善解决方案。

新知识可以来自最新的研究发现、数据更新或其他相关领域的见解。

5、最终解决方案：

经过多次迭代后，FunSearch 最终生成一个或多个高质量的解决方案，这些解决方案不仅有效，而且具有创新性和实用性。</title>
            <link>https://nitter.cz/xiaohuggg/status/1735957680739823730#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735957680739823730#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 09:38:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>FunSearch利用大语言模型生成解决方案，然后使用评估器来评估其有效性和准确性，如果评估器反馈不符合预期，FunSearch就会返回LLM继续生成解决方案！<br />
<br />
如此循环反复不断迭代改进！<br />
<br />
在迭代过程中，FunSearch也会融入新的知识或数据进行补充完善 ，它会不断在生成方案和新知识之间来回往，返循环这个过程，直到解决问题！<br />
<br />
具体过程：<br />
<br />
1、初始解决方案：<br />
<br />
FunSearch 首先生成一个或多个初始解决方案。这些解决方案是基于预训练的大型语言模型（LLM）对问题的理解和分析。<br />
<br />
2、评估和反馈：<br />
<br />
生成的解决方案随后被自动评估器检查。评估器评估解决方案的有效性和可行性，确保它们不是错误或虚假的想法。<br />
<br />
如果解决方案不符合预期或存在改进空间，评估器会提供反馈。<br />
<br />
3、迭代改进：<br />
<br />
基于评估器的反馈，FunSearch 对初始解决方案进行修改和改进。这可能涉及调整算法、探索新的解决路径或增强现有解决方案的细节。<br />
<br />
这个过程是迭代的，意味着解决方案会经过多轮的评估和改进。<br />
<br />
4、新知识的融入：<br />
<br />
在迭代过程中，FunSearch 也可能融入新的知识或数据，这有助于进一步丰富和完善解决方案。<br />
<br />
新知识可以来自最新的研究发现、数据更新或其他相关领域的见解。<br />
<br />
5、最终解决方案：<br />
<br />
经过多次迭代后，FunSearch 最终生成一个或多个高质量的解决方案，这些解决方案不仅有效，而且具有创新性和实用性。</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1735553242048958615#m">nitter.cz/xiaohuggg/status/1735553242048958615#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1735935483463467125#m</id>
            <title>RT by @xiaohuggg: 微软出的 GitHub Copilot 教程，只有 6 堂课，会教你如何有效利用 GitHub Copilot 以及与 AI 结对编程。

课程一共 10 小时，可以体验如何通过 VSCode 和 GitHub Copilot Chat 进行实时协作，学习如何使用 GitHub Copilot 自动补全代码，处理错误和写单元测试，尽可能教会你使用 GitHub Copilot 的最佳实践，让你可以提升写代码的效率和质量。

https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming</title>
            <link>https://nitter.cz/dotey/status/1735935483463467125#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1735935483463467125#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 08:10:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软出的 GitHub Copilot 教程，只有 6 堂课，会教你如何有效利用 GitHub Copilot 以及与 AI 结对编程。<br />
<br />
课程一共 10 小时，可以体验如何通过 VSCode 和 GitHub Copilot Chat 进行实时协作，学习如何使用 GitHub Copilot 自动补全代码，处理错误和写单元测试，尽可能教会你使用 GitHub Copilot 的最佳实践，让你可以提升写代码的效率和质量。<br />
<br />
<a href="https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming">github.com/microsoft/Masteri…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JkSERXdFdvQUFNWi1QLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735924490704724436#m</id>
            <title>SceneWiz3D：根据文字描述创建3D场景

它能仅靠文本描述就能合成高保真3D场景，会自动布局场景，比如自动安排物体位置、大小、方向，确保场景看起来真实和连贯。

而且还允许动态地改变场景中的物体，比如添加或移除物体。

举例解释：

假设你想创建一个3D场景，场景是一个有大窗户的卧室，窗外是日落景象，整个场景带有浮世绘（Ukiyo-e）风格。在传统的3D建模中，你需要手动设计每一个细节，包括房间的布局、窗户的大小、光线的方向，甚至是墙上的浮世绘风格装饰。这个过程非常耗时且需要专业知识。

使用SceneWiz3D，你只需要提供一个简单的文字描述，比如“一个有大窗户的卧室，窗外是日落景象，整个场景带有浮世绘风格”。SceneWiz3D会自动解析这个描述，并利用其混合3D表示技术来创建场景。它会自动放置卧室中的物体（如床、桌子、椅子），调整窗户大小以适应日落景象，并应用浮世绘风格到整个场景。

此外，如果场景中的某些角落或细节在普通的3D建模中难以处理，SceneWiz3D的RGBD全景扩散模型会提供额外的视角和深度信息，确保整个场景的几何质量和视觉效果都是高质量的。

最后，如果你想对场景进行调整，比如增加一个椅子或改变窗户的位置，SceneWiz3D允许你轻松地进行这些调整，而无需重新设计整个场景。

主要特点包括：

1、混合3D表示：它能够将单个物体和整个场景以不同的方式表示，使得场景更加真实和详细。

2、自动布局：使用一种叫做粒子群优化的技术，能自动安排场景中物体的位置和方向。

3、改善几何质量：为了解决一些难以观察到的场景部分（比如角落）的问题，它使用了一种特殊的模型来提高这些区域的几何质量。

4、对象配置：可以确定每个物体在场景中的位置、大小和方向。

5、额外的视角：除了普通的视角，还使用了一种特殊的模型来提供额外的视角和深度信息，帮助理解整个场景的结构。

6、场景操纵：允许用户动态地改变场景中的物体，比如添加或移除物体。

项目及演示：https://zqh0253.github.io/SceneWiz3D/
论文：https://arxiv.org/abs/2312.08885
GitHub：https://github.com/zqh0253/SceneWiz3D（coming soon）</title>
            <link>https://nitter.cz/xiaohuggg/status/1735924490704724436#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735924490704724436#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 07:27:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SceneWiz3D：根据文字描述创建3D场景<br />
<br />
它能仅靠文本描述就能合成高保真3D场景，会自动布局场景，比如自动安排物体位置、大小、方向，确保场景看起来真实和连贯。<br />
<br />
而且还允许动态地改变场景中的物体，比如添加或移除物体。<br />
<br />
举例解释：<br />
<br />
假设你想创建一个3D场景，场景是一个有大窗户的卧室，窗外是日落景象，整个场景带有浮世绘（Ukiyo-e）风格。在传统的3D建模中，你需要手动设计每一个细节，包括房间的布局、窗户的大小、光线的方向，甚至是墙上的浮世绘风格装饰。这个过程非常耗时且需要专业知识。<br />
<br />
使用SceneWiz3D，你只需要提供一个简单的文字描述，比如“一个有大窗户的卧室，窗外是日落景象，整个场景带有浮世绘风格”。SceneWiz3D会自动解析这个描述，并利用其混合3D表示技术来创建场景。它会自动放置卧室中的物体（如床、桌子、椅子），调整窗户大小以适应日落景象，并应用浮世绘风格到整个场景。<br />
<br />
此外，如果场景中的某些角落或细节在普通的3D建模中难以处理，SceneWiz3D的RGBD全景扩散模型会提供额外的视角和深度信息，确保整个场景的几何质量和视觉效果都是高质量的。<br />
<br />
最后，如果你想对场景进行调整，比如增加一个椅子或改变窗户的位置，SceneWiz3D允许你轻松地进行这些调整，而无需重新设计整个场景。<br />
<br />
主要特点包括：<br />
<br />
1、混合3D表示：它能够将单个物体和整个场景以不同的方式表示，使得场景更加真实和详细。<br />
<br />
2、自动布局：使用一种叫做粒子群优化的技术，能自动安排场景中物体的位置和方向。<br />
<br />
3、改善几何质量：为了解决一些难以观察到的场景部分（比如角落）的问题，它使用了一种特殊的模型来提高这些区域的几何质量。<br />
<br />
4、对象配置：可以确定每个物体在场景中的位置、大小和方向。<br />
<br />
5、额外的视角：除了普通的视角，还使用了一种特殊的模型来提供额外的视角和深度信息，帮助理解整个场景的结构。<br />
<br />
6、场景操纵：允许用户动态地改变场景中的物体，比如添加或移除物体。<br />
<br />
项目及演示：<a href="https://zqh0253.github.io/SceneWiz3D/">zqh0253.github.io/SceneWiz3D…</a><br />
论文：<a href="https://arxiv.org/abs/2312.08885">arxiv.org/abs/2312.08885</a><br />
GitHub：<a href="https://github.com/zqh0253/SceneWiz3D">github.com/zqh0253/SceneWiz3…</a>（coming soon）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzU5MjQxMDA5MTU1MTEyOTYvcHUvaW1nL0xNSUE1NWNjWEZJLVRFbW8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735911248188043742#m</id>
            <title>R to @xiaohuggg: 兄弟们 

经过测试用dolphin-2.5-mixtral-8x7写小黄文很可以，剧情真的很细腻，和少妇白洁不相上下！🫣

但是要用英文来写，它自己会设定情节、人物、故事，还会自己提出很多自己的想法给你参考。问你想使用哪一种套路。哈哈哈哈😂

然后我让GPT帮我翻译成中文，一开始还能翻译后来拒绝给我翻译了🤣</title>
            <link>https://nitter.cz/xiaohuggg/status/1735911248188043742#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735911248188043742#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 06:34:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 <br />
<br />
经过测试用dolphin-2.5-mixtral-8x7写小黄文很可以，剧情真的很细腻，和少妇白洁不相上下！🫣<br />
<br />
但是要用英文来写，它自己会设定情节、人物、故事，还会自己提出很多自己的想法给你参考。问你想使用哪一种套路。哈哈哈哈😂<br />
<br />
然后我让GPT帮我翻译成中文，一开始还能翻译后来拒绝给我翻译了🤣</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JjeEpKX2J3QUUzYmFiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735876029552718213#m</id>
            <title>根据 The Verge 的报道，字节跳动被OpenAI封杀！

因为字节也违反了 OpenAI 的服务条款，一直在使用GPT生成的数据在中国训练自己的竞争模型，从而违反了微软和OpenAI的开发人员许可证。

字节跳动的内部文件显示，他们在开发名为 Project Seed 的基础 LLM 时，几乎在每个阶段都依赖 OpenAI 的 API，包括用于训练和评估模型。

该条款规定其模型输出不能用于“开发与我们的产品和服务竞争的任何人工智能模型”。字节跳动通过微软获取了OpenAI 的访问权限。

由于过度依赖 OpenAI 的 API，Project Seed 的员工经常达到了 API 访问的最大限额。

涉及的员工清楚这一行为的含义，甚至讨论了如何通过“数据脱敏”来“洗白”证据。这种滥用如此普遍，以至于 Project Seed 的员工经常达到 API 访问的最大配额。

https://www.theverge.com/2023/12/15/24003151/bytedance-china-openai-microsoft-competitor-llm</title>
            <link>https://nitter.cz/xiaohuggg/status/1735876029552718213#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735876029552718213#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 04:14:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据 The Verge 的报道，字节跳动被OpenAI封杀！<br />
<br />
因为字节也违反了 OpenAI 的服务条款，一直在使用GPT生成的数据在中国训练自己的竞争模型，从而违反了微软和OpenAI的开发人员许可证。<br />
<br />
字节跳动的内部文件显示，他们在开发名为 Project Seed 的基础 LLM 时，几乎在每个阶段都依赖 OpenAI 的 API，包括用于训练和评估模型。<br />
<br />
该条款规定其模型输出不能用于“开发与我们的产品和服务竞争的任何人工智能模型”。字节跳动通过微软获取了OpenAI 的访问权限。<br />
<br />
由于过度依赖 OpenAI 的 API，Project Seed 的员工经常达到了 API 访问的最大限额。<br />
<br />
涉及的员工清楚这一行为的含义，甚至讨论了如何通过“数据脱敏”来“洗白”证据。这种滥用如此普遍，以至于 Project Seed 的员工经常达到 API 访问的最大配额。<br />
<br />
<a href="https://www.theverge.com/2023/12/15/24003151/bytedance-china-openai-microsoft-competitor-llm">theverge.com/2023/12/15/2400…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JjUmdET2JvQUFLb2FsLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JjUmdDNGFnQUFLR0ZRLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735848325180727712#m</id>
            <title>R to @xiaohuggg: 这个NeurIPS大会很接地气

就直接把论文打印出来直接挂起来就是一个展位

https://x.com/minjiyoon90/status/1735718674437218389</title>
            <link>https://nitter.cz/xiaohuggg/status/1735848325180727712#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735848325180727712#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 02:24:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个NeurIPS大会很接地气<br />
<br />
就直接把论文打印出来直接挂起来就是一个展位<br />
<br />
<a href="https://x.com/minjiyoon90/status/1735718674437218389">x.com/minjiyoon90/status/173…</a></p>
<p><a href="https://nitter.cz/MinjiYoon90/status/1735718674437218389#m">nitter.cz/MinjiYoon90/status/1735718674437218389#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735839055269843013#m</id>
            <title>俄罗斯大学生用AI做了个普金的AI分身

在年度记者会上问普金你究竟有多少个分身😂 

普金还是挺会玩的😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1735839055269843013#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735839055269843013#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 01:47:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>俄罗斯大学生用AI做了个普金的AI分身<br />
<br />
在年度记者会上问普金你究竟有多少个分身😂 <br />
<br />
普金还是挺会玩的😎</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JZUFpvNmE4QUFlQU5CLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735685417528344745#m</id>
            <title>NeurIPS（神经信息处理系统大会）是一个关于人工智能和机器学习的重要年度学术会议。

看看今年 #NeurIPS2023 的热度😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1735685417528344745#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735685417528344745#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 15:37:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NeurIPS（神经信息处理系统大会）是一个关于人工智能和机器学习的重要年度学术会议。<br />
<br />
看看今年 <a href="https://nitter.cz/search?q=%23NeurIPS2023">#NeurIPS2023</a> 的热度😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3OTk5MjgxNTk3ODA4NjQvcHUvaW1nLzh5VzlfTHhCakF0ZlYyQWYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>