<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753449500612682054#m</id>
            <title>美图发布公告称宣布收购站酷。

总价为3964.05万美元（约2.85亿元），其中1778.4万美元将以配发及发行52,992,166股代价股份支付，而余额约2185.6万美元将以现金支付。

站酷成立于2006年，聚集了来自全球300多个城市的设计师、摄影师、插画师等视觉创意从业者，拥有近1700万注册用户。

美图CEO吴欣鸿表示，随着站酷的加入，美图影像与设计产品业务将得到进一步升级，为自研AI视觉大模型MiracleVision（奇想智能）的生态带来优质的协同效应，同时帮助美图在专业设计领域进行业务扩展，在版权和共创等方面增强美图的服务能力。</title>
            <link>https://nitter.cz/xiaohuggg/status/1753449500612682054#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753449500612682054#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 16:05:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>美图发布公告称宣布收购站酷。<br />
<br />
总价为3964.05万美元（约2.85亿元），其中1778.4万美元将以配发及发行52,992,166股代价股份支付，而余额约2185.6万美元将以现金支付。<br />
<br />
站酷成立于2006年，聚集了来自全球300多个城市的设计师、摄影师、插画师等视觉创意从业者，拥有近1700万注册用户。<br />
<br />
美图CEO吴欣鸿表示，随着站酷的加入，美图影像与设计产品业务将得到进一步升级，为自研AI视觉大模型MiracleVision（奇想智能）的生态带来优质的协同效应，同时帮助美图在专业设计领域进行业务扩展，在版权和共创等方面增强美图的服务能力。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZWOHEzYWFzQUFxMFpKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753435363866230876#m</id>
            <title>🔔http://Xiaohu.AI日报「2月2日」

✨✨✨✨✨✨✨✨

1⃣️ Google Bard全球更新：新增支持Gemini Pro，图像生成功能，多语言检查，加入水印（SynthID）。

2⃣️ JoyTag开源图像标注模型：输出5000多个标签，基于ViT-B/16构建，适合多样化图像内容。

3⃣️ Wonder Studio摄像机追踪与点云技术：自动识别摄像机移动，精确CGI与实景融合。

4⃣️ Google ImageFX工具：基于Imagen 2，通过关键词合成提示词生成图片。

5⃣️ InstructIR图像恢复工具：根据文字描述改善图片，支持去噪、去雨、去模糊等。

6⃣️ InternLM-XComposer图文创作模型：自由形式图文创作和理解，包含海量图文知识库。

7⃣️ Poe Image Remix功能：修改AI生成图片风格、布局和颜色，支持SDXL和Playground v2。</title>
            <link>https://nitter.cz/xiaohuggg/status/1753435363866230876#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753435363866230876#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 15:09:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月2日」<br />
<br />
✨✨✨✨✨✨✨✨<br />
<br />
1⃣️ Google Bard全球更新：新增支持Gemini Pro，图像生成功能，多语言检查，加入水印（SynthID）。<br />
<br />
2⃣️ JoyTag开源图像标注模型：输出5000多个标签，基于ViT-B/16构建，适合多样化图像内容。<br />
<br />
3⃣️ Wonder Studio摄像机追踪与点云技术：自动识别摄像机移动，精确CGI与实景融合。<br />
<br />
4⃣️ Google ImageFX工具：基于Imagen 2，通过关键词合成提示词生成图片。<br />
<br />
5⃣️ InstructIR图像恢复工具：根据文字描述改善图片，支持去噪、去雨、去模糊等。<br />
<br />
6⃣️ InternLM-XComposer图文创作模型：自由形式图文创作和理解，包含海量图文知识库。<br />
<br />
7⃣️ Poe Image Remix功能：修改AI生成图片风格、布局和颜色，支持SDXL和Playground v2。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZWMEtaQmFrQUFPY01FLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753393342266916991#m</id>
            <title>一个业余的机器学习研究者在Reddit上分享了他自己构建的一款开源的图像标注模型：JoyTag

该模型对训练的内容和标签没有任何过滤和审查。

给它一张图片，它会输出5000多个不同标签的预测。

可以为SD模型微调提供标注图像，使得微调后的模型能够生成更精确、多样化的图像内容。

由于没有内容审查和过滤 ，JoyTag在处理多样化和包容性方面表现更好，尤其是对性正面（性开放、性积极）内容的处理。

这位研究者花费了一年的时间从零开始构建这个基于ViT-B/16，输入尺寸为448x448x3，拥有9100万参数的机器视觉模型。

这个模型经过660M样本的训练，能够对超过5000个独特标签进行多标签分类。

主要功能：

给它一张图片，它能够自动识别并输出超过5000个不同标签的预测结果。意味着它能够同时为一张图像分配多个标签，而这些标签之间是相互独立的。这与只能识别单一类别的图像识别模型不同。

多标签的特性使得JoyTag模型可以自动为图像“打标签”，这对于多种应用非常有用，尤其是在训练Diffusion模型时。

广泛的应用场景：适用于从手绘到摄影等各种类型的图像。

支持多标签分类：能够独立预测超过5000个不同标签，与单一类别预测的视觉模型不同，为图像自动“标签化”提供了可能。

优化Diffusion模型训练：JoyTag模型能够为缺少文本描述的图像自动生成标签，从而辅助这类模型的训练，使得生成的图像内容更加丰富和多样化。对于缺乏文本配对的图像，JoyTag的自动标注功能特别有用，有助于改进Diffusion模型的训练。

Reddit：https://old.reddit.com/r/MachineLearning/comments/18nb15l/p_i_built_an_open_sota_image_tagging_model_to_do/

GitHub：</title>
            <link>https://nitter.cz/xiaohuggg/status/1753393342266916991#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753393342266916991#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 12:22:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个业余的机器学习研究者在Reddit上分享了他自己构建的一款开源的图像标注模型：JoyTag<br />
<br />
该模型对训练的内容和标签没有任何过滤和审查。<br />
<br />
给它一张图片，它会输出5000多个不同标签的预测。<br />
<br />
可以为SD模型微调提供标注图像，使得微调后的模型能够生成更精确、多样化的图像内容。<br />
<br />
由于没有内容审查和过滤 ，JoyTag在处理多样化和包容性方面表现更好，尤其是对性正面（性开放、性积极）内容的处理。<br />
<br />
这位研究者花费了一年的时间从零开始构建这个基于ViT-B/16，输入尺寸为448x448x3，拥有9100万参数的机器视觉模型。<br />
<br />
这个模型经过660M样本的训练，能够对超过5000个独特标签进行多标签分类。<br />
<br />
主要功能：<br />
<br />
给它一张图片，它能够自动识别并输出超过5000个不同标签的预测结果。意味着它能够同时为一张图像分配多个标签，而这些标签之间是相互独立的。这与只能识别单一类别的图像识别模型不同。<br />
<br />
多标签的特性使得JoyTag模型可以自动为图像“打标签”，这对于多种应用非常有用，尤其是在训练Diffusion模型时。<br />
<br />
广泛的应用场景：适用于从手绘到摄影等各种类型的图像。<br />
<br />
支持多标签分类：能够独立预测超过5000个不同标签，与单一类别预测的视觉模型不同，为图像自动“标签化”提供了可能。<br />
<br />
优化Diffusion模型训练：JoyTag模型能够为缺少文本描述的图像自动生成标签，从而辅助这类模型的训练，使得生成的图像内容更加丰富和多样化。对于缺乏文本配对的图像，JoyTag的自动标注功能特别有用，有助于改进Diffusion模型的训练。<br />
<br />
Reddit：<a href="https://teddit.net/r/MachineLearning/comments/18nb15l/p_i_built_an_open_sota_image_tagging_model_to_do/">teddit.net/r/MachineLear…</a><br />
<br />
GitHub：</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZWTS1LRmF3QUFoMGJLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753386192224362950#m</id>
            <title>Wonder Studio推出摄像机追踪与点云技术

它可以通过分析视频，自动识别你拍摄视频时摄像机是怎么移动的，以及拍摄场景的具体样子。

然后根据这些信息，重现出那个视频当时的拍摄场景和摄像机动作。

使得在后期制作过程中能够更精确地将计算机生成的图像（CGI）与实际拍摄的现场视频融合。

这样做的好处是，当你想在视频里加入虚拟角色或者物体时，这些虚拟的东西就能够像真的一样，在3D空间里随着摄像机的移动而自然移动，让整个视频看起来更真实、更自然。

这项技术包括两个核心部分：

摄像机追踪：该技术能够自动分析你提供的现场拍摄视频，识别出拍摄过程中摄像机的具体移动方式（如平移、旋转或缩放），以及摄像机的视场（即摄像头捕捉的视角范围）和焦距（即视角的宽度）。这使得在3D环境中重建真实的摄像机运动成为可能。

点云技术：通过分析视频，该技术生成了一个由数百万个点组成的3D模型，这些点代表了视频中场景的空间位置。通过检测这些点，软件能够识别场景中的平面和物体，如墙壁、地面等。这对于将CG角色或对象准确地放置在视频中的特定位置至关重要。

当你从Wonder Studio下载3D场景导出文件时，这意味着你的CG角色或物体将能够根据实际摄像机的运动在3D空间中移动，而不是仅仅在二维的帧空间内移动。

这样，无论摄像机怎样移动或场景如何变化，CG元素都能与实际拍摄的内容完美融合，创造出更加真实和动人的视觉效果。

此外，通过引入独立的摄像机FBX文件，Wonder Dynamics为用户提供了更大的灵活性，使得这些技术能够更容易地集成到现有的工作流程中，进一步提高了后期制作的效率和质量。</title>
            <link>https://nitter.cz/xiaohuggg/status/1753386192224362950#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753386192224362950#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 11:53:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wonder Studio推出摄像机追踪与点云技术<br />
<br />
它可以通过分析视频，自动识别你拍摄视频时摄像机是怎么移动的，以及拍摄场景的具体样子。<br />
<br />
然后根据这些信息，重现出那个视频当时的拍摄场景和摄像机动作。<br />
<br />
使得在后期制作过程中能够更精确地将计算机生成的图像（CGI）与实际拍摄的现场视频融合。<br />
<br />
这样做的好处是，当你想在视频里加入虚拟角色或者物体时，这些虚拟的东西就能够像真的一样，在3D空间里随着摄像机的移动而自然移动，让整个视频看起来更真实、更自然。<br />
<br />
这项技术包括两个核心部分：<br />
<br />
摄像机追踪：该技术能够自动分析你提供的现场拍摄视频，识别出拍摄过程中摄像机的具体移动方式（如平移、旋转或缩放），以及摄像机的视场（即摄像头捕捉的视角范围）和焦距（即视角的宽度）。这使得在3D环境中重建真实的摄像机运动成为可能。<br />
<br />
点云技术：通过分析视频，该技术生成了一个由数百万个点组成的3D模型，这些点代表了视频中场景的空间位置。通过检测这些点，软件能够识别场景中的平面和物体，如墙壁、地面等。这对于将CG角色或对象准确地放置在视频中的特定位置至关重要。<br />
<br />
当你从Wonder Studio下载3D场景导出文件时，这意味着你的CG角色或物体将能够根据实际摄像机的运动在3D空间中移动，而不是仅仅在二维的帧空间内移动。<br />
<br />
这样，无论摄像机怎样移动或场景如何变化，CG元素都能与实际拍摄的内容完美融合，创造出更加真实和动人的视觉效果。<br />
<br />
此外，通过引入独立的摄像机FBX文件，Wonder Dynamics为用户提供了更大的灵活性，使得这些技术能够更容易地集成到现有的工作流程中，进一步提高了后期制作的效率和质量。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMzODYwNjgzMjAzNzg4ODAvcHUvaW1nL1RmeEl6OEpuZ0xCaHFOUDUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753376925773635616#m</id>
            <title>Google 推出 ImageFX ，这是一款由 Imagen 2 提供支持的新图像生成工具。

和之前的Google MusicFX（https://x.com/xiaohuggg/status/1735506583432634544）操作一样

通过鼠标点击关键词即可自动合成提示词来帮你生成图片

体验地址：https://aitestkitchen.withgoogle.com/tools/image-fx

需要切换到美国线路，开全局。</title>
            <link>https://nitter.cz/xiaohuggg/status/1753376925773635616#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753376925773635616#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 11:16:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 推出 ImageFX ，这是一款由 Imagen 2 提供支持的新图像生成工具。<br />
<br />
和之前的Google MusicFX（<a href="https://x.com/xiaohuggg/status/1735506583432634544">x.com/xiaohuggg/status/17355…</a>）操作一样<br />
<br />
通过鼠标点击关键词即可自动合成提示词来帮你生成图片<br />
<br />
体验地址：<a href="https://aitestkitchen.withgoogle.com/tools/image-fx">aitestkitchen.withgoogle.com…</a><br />
<br />
需要切换到美国线路，开全局。</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1735506583432634544#m">nitter.cz/xiaohuggg/status/1735506583432634544#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMzNzY0NTM3MzgyNzQ4MTYvcHUvaW1nL1pMVGZhTjdGc3RUb2loLVMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753345751911198802#m</id>
            <title>InstructIR：按照人类指令进行高质量图像恢复

你只需要使用文字描述就能修复和改善图片

比如说，如果你有一张因为雨滴而看起来模糊的照片，你可以告诉它：请去掉照片上的雨滴，但保持图片内容不变”，它就能自动帮你操作。

它能够处理包括去噪、去雨、去模糊、去雾以及（低光）图像增强等问题。

主要功能：

-接收图像和人类书面指令作为输入，根据这些指令对图像进行改善。

-支持多种图像恢复任务，包括去噪、去雨、去模糊、去雾和图像增强。

- 实现了状态最先进的恢复效果，提供了高质量的图像输出。

工作原理：I

InstructIR使用一个文本编码器将人类提供的自然语言指令转换为模型可以理解的向量表示。

这些指令明确指导模型关注图像的哪些退化问题，并提供改善的方向。

1、全能图像恢复模型：采用NAFNet作为图像恢复的核心模型架构，它是一个高效且性能卓越的图像处理网络。NAFNet能够处理多种图像退化类型，为全方位图像恢复提供支持。

2、指令条件块（ICB）：InstructIR引入了ICB来实现任务特定的转换，根据文本编码器输出的指令向量，ICB能够调整图像模型的处理流程，使模型能够针对具体的退化类型进行专门的恢复处理。

3、多任务学习与任务路由：通过利用任务路由技术，InstructIR能够在单一模型中学习并执行多种图像恢复任务。模型根据输入的人类指令自动判断需要执行的任务类型，并采取相应的恢复策略。

项目地址：https://mv-lab.github.io/InstructIR/
论文：https://arxiv.org/abs/2401.16468
GitHub：https://github.com/mv-lab/InstructIR/
在线体验：https://huggingface.co/spaces/marcosv/InstructIR</title>
            <link>https://nitter.cz/xiaohuggg/status/1753345751911198802#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753345751911198802#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 09:12:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>InstructIR：按照人类指令进行高质量图像恢复<br />
<br />
你只需要使用文字描述就能修复和改善图片<br />
<br />
比如说，如果你有一张因为雨滴而看起来模糊的照片，你可以告诉它：请去掉照片上的雨滴，但保持图片内容不变”，它就能自动帮你操作。<br />
<br />
它能够处理包括去噪、去雨、去模糊、去雾以及（低光）图像增强等问题。<br />
<br />
主要功能：<br />
<br />
-接收图像和人类书面指令作为输入，根据这些指令对图像进行改善。<br />
<br />
-支持多种图像恢复任务，包括去噪、去雨、去模糊、去雾和图像增强。<br />
<br />
- 实现了状态最先进的恢复效果，提供了高质量的图像输出。<br />
<br />
工作原理：I<br />
<br />
InstructIR使用一个文本编码器将人类提供的自然语言指令转换为模型可以理解的向量表示。<br />
<br />
这些指令明确指导模型关注图像的哪些退化问题，并提供改善的方向。<br />
<br />
1、全能图像恢复模型：采用NAFNet作为图像恢复的核心模型架构，它是一个高效且性能卓越的图像处理网络。NAFNet能够处理多种图像退化类型，为全方位图像恢复提供支持。<br />
<br />
2、指令条件块（ICB）：InstructIR引入了ICB来实现任务特定的转换，根据文本编码器输出的指令向量，ICB能够调整图像模型的处理流程，使模型能够针对具体的退化类型进行专门的恢复处理。<br />
<br />
3、多任务学习与任务路由：通过利用任务路由技术，InstructIR能够在单一模型中学习并执行多种图像恢复任务。模型根据输入的人类指令自动判断需要执行的任务类型，并采取相应的恢复策略。<br />
<br />
项目地址：<a href="https://mv-lab.github.io/InstructIR/">mv-lab.github.io/InstructIR/</a><br />
论文：<a href="https://arxiv.org/abs/2401.16468">arxiv.org/abs/2401.16468</a><br />
GitHub：<a href="https://github.com/mv-lab/InstructIR/">github.com/mv-lab/InstructIR…</a><br />
在线体验：<a href="https://huggingface.co/spaces/marcosv/InstructIR">huggingface.co/spaces/marcos…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMzNDM5NDYwNTc3OTM1MzYvcHUvaW1nL3RJdDF2a2hnT2M1allzd28uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753307025067667802#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1753307025067667802#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753307025067667802#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 06:39:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZUX1k2SmFjQUFna0xhLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753307022408458377#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1753307022408458377#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753307022408458377#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 06:39:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZUX1U2UmIwQUFNcWxQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753307020395229441#m</id>
            <title>哈哈哈哈哈  

越来越滑稽了...</title>
            <link>https://nitter.cz/xiaohuggg/status/1753307020395229441#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753307020395229441#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 06:39:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈哈哈哈哈  <br />
<br />
越来越滑稽了...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZUX1JtcmJRQUF6cTZFLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753303228341764471#m</id>
            <title>Ethan Sutin  @EthanSutin 

展示了一个可以运行在16GB M1芯片上的AI可穿戴设备原型

这个设备能实时监听你所说的话

时刻保持在线，能随时和你进行对话

并能记住你们对话内容，还能定向将信息传输到你们当时对话环境相适应的的设备上。

所有的技术全部使用了开源技术栈，他将很快分享代码。

前半部分演示使用的是GPT-3.5和ElevenLabs，改进后完全是开源的技术

这套技术栈包括：

📝会话转录：使用Whisper Medium
💬实时转录：使用Whisper Small
🔊声音活动检测（VAD）：使用Silero
🧠大型语言模型（LLM）：使用Mistral-Instruct 7B
🗣️文本到语音（TTS）：使用StyleTTS2</title>
            <link>https://nitter.cz/xiaohuggg/status/1753303228341764471#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753303228341764471#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 06:23:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ethan Sutin  <a href="https://nitter.cz/EthanSutin" title="Ethan Sutin">@EthanSutin</a> <br />
<br />
展示了一个可以运行在16GB M1芯片上的AI可穿戴设备原型<br />
<br />
这个设备能实时监听你所说的话<br />
<br />
时刻保持在线，能随时和你进行对话<br />
<br />
并能记住你们对话内容，还能定向将信息传输到你们当时对话环境相适应的的设备上。<br />
<br />
所有的技术全部使用了开源技术栈，他将很快分享代码。<br />
<br />
前半部分演示使用的是GPT-3.5和ElevenLabs，改进后完全是开源的技术<br />
<br />
这套技术栈包括：<br />
<br />
📝会话转录：使用Whisper Medium<br />
💬实时转录：使用Whisper Small<br />
🔊声音活动检测（VAD）：使用Silero<br />
🧠大型语言模型（LLM）：使用Mistral-Instruct 7B<br />
🗣️文本到语音（TTS）：使用StyleTTS2</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMzMDAyNjI5NzMwMzQ0OTYvcHUvaW1nL1Q5NEgxZ3hjeVowZzlLOWkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753290324078932355#m</id>
            <title>使用Apple Vision Pro

练习钢琴</title>
            <link>https://nitter.cz/xiaohuggg/status/1753290324078932355#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753290324078932355#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 05:32:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用Apple Vision Pro<br />
<br />
练习钢琴</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMxODAzNzE5MTM0MzcxODQvcHUvaW1nL0ZNQ2FnRVlFWTFhWFgxNGIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753273641591308616#m</id>
            <title>InternLM-XComposer：直接输出图文并茂的完整文章

浦语·灵笔是一个高级的图文多模态大模型，能够在自由形式的输入下进行图文创作和理解。

你只需要给出一个提示，比如一个主题、一个想法或者一个故事大纲

它就能根据这个提示创作出一篇包含文字和图片的完整文章。图文完美契合...

不仅如此，它还拥有海量的图文知识库，能够准确回答各种图文问答难题，具有图像识别、感知和视觉推理能力。

主要功能特点：

1、灵活的内容创作：你可以给它任何类型的启示，无论是一个大致的想法、一篇详细的文章大纲，还是一些参考图片，它都能根据这些信息创作出既有文字又有图片的内容。

2、智能地处理图片和文字：通过部分LoRA（PLoRA）方法，来确保图片和文字之间的完美搭配，它既懂得如何写好文章，又懂得如何挑选或创造出最佳配图。

3、高品质和多样化的创作基础：采用质量高、风格多样的数据集，拥有海量的图文知识库，它能创作出各种风格和主题的内容，因为它的“知识库”包含了大量高品质和多样化的信息。

4、出色的视觉理解能力：它不仅仅能处理文字，还能深入理解图片内容，这让它能创作出更丰富、更吸引人的内容。

浦语·灵笔包括两个版本:

InternLM-XComposer2-VL-7B 🤗 （浦语·灵笔2-视觉问答-7B）: 基于书生·浦语2-7B大语言模型训练，面向多模态评测和视觉问答。

浦语·灵笔2-视觉问答-7B是目前最强的基于7B量级语言模型基座的图文多模态大模型，领跑多达13个多模态大模型榜单。

InternLM-XComposer2-7B 🤗 : 进一步微调，支持自由指令输入图文写作的图文多模态大模型。

GitHub：https://github.com/InternLM/InternLM-XComposer/tree/main
论文：https://arxiv.org/abs/2401.16420
在线演示：https://huggingface.co/spaces/Willow123/InternLM-XComposer</title>
            <link>https://nitter.cz/xiaohuggg/status/1753273641591308616#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753273641591308616#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 04:26:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>InternLM-XComposer：直接输出图文并茂的完整文章<br />
<br />
浦语·灵笔是一个高级的图文多模态大模型，能够在自由形式的输入下进行图文创作和理解。<br />
<br />
你只需要给出一个提示，比如一个主题、一个想法或者一个故事大纲<br />
<br />
它就能根据这个提示创作出一篇包含文字和图片的完整文章。图文完美契合...<br />
<br />
不仅如此，它还拥有海量的图文知识库，能够准确回答各种图文问答难题，具有图像识别、感知和视觉推理能力。<br />
<br />
主要功能特点：<br />
<br />
1、灵活的内容创作：你可以给它任何类型的启示，无论是一个大致的想法、一篇详细的文章大纲，还是一些参考图片，它都能根据这些信息创作出既有文字又有图片的内容。<br />
<br />
2、智能地处理图片和文字：通过部分LoRA（PLoRA）方法，来确保图片和文字之间的完美搭配，它既懂得如何写好文章，又懂得如何挑选或创造出最佳配图。<br />
<br />
3、高品质和多样化的创作基础：采用质量高、风格多样的数据集，拥有海量的图文知识库，它能创作出各种风格和主题的内容，因为它的“知识库”包含了大量高品质和多样化的信息。<br />
<br />
4、出色的视觉理解能力：它不仅仅能处理文字，还能深入理解图片内容，这让它能创作出更丰富、更吸引人的内容。<br />
<br />
浦语·灵笔包括两个版本:<br />
<br />
InternLM-XComposer2-VL-7B 🤗 （浦语·灵笔2-视觉问答-7B）: 基于书生·浦语2-7B大语言模型训练，面向多模态评测和视觉问答。<br />
<br />
浦语·灵笔2-视觉问答-7B是目前最强的基于7B量级语言模型基座的图文多模态大模型，领跑多达13个多模态大模型榜单。<br />
<br />
InternLM-XComposer2-7B 🤗 : 进一步微调，支持自由指令输入图文写作的图文多模态大模型。<br />
<br />
GitHub：<a href="https://github.com/InternLM/InternLM-XComposer/tree/main">github.com/InternLM/InternLM…</a><br />
论文：<a href="https://arxiv.org/abs/2401.16420">arxiv.org/abs/2401.16420</a><br />
在线演示：<a href="https://huggingface.co/spaces/Willow123/InternLM-XComposer">huggingface.co/spaces/Willow…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMyNzA5OTc3Mjg1NTUwMDgvcHUvaW1nL1NaYnNZM1JkMjNHNlJwbE0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753263744292151624#m</id>
            <title>R to @xiaohuggg: 支持中文，但是全球230多个国家和地区中不包含中国

中国被开除球籍了😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1753263744292151624#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753263744292151624#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 03:47:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持中文，但是全球230多个国家和地区中不包含中国<br />
<br />
中国被开除球籍了😅</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753261066136752494#m</id>
            <title>Google Bard更新：支持全球访问Gemini Pro并生成图像

- Gemini Pro将支持包括中文在内超过40种语言和230多个国家和地区

- 新增图像生成功能，不过只支持英语。这一新功能由更新的Imagen 2模型提供支持

- 多语言双重检查功能现在支持40多种语言 ，使用户能够验证Bard回答的准确性，通过点击“G”图标查找支持或反驳信息。

- 生成图像的像素中嵌入数字可识别的水印（SynthID）

详细：https://blog.google/products/bard/google-bard-gemini-pro-image-generation/</title>
            <link>https://nitter.cz/xiaohuggg/status/1753261066136752494#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753261066136752494#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 03:36:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Bard更新：支持全球访问Gemini Pro并生成图像<br />
<br />
- Gemini Pro将支持包括中文在内超过40种语言和230多个国家和地区<br />
<br />
- 新增图像生成功能，不过只支持英语。这一新功能由更新的Imagen 2模型提供支持<br />
<br />
- 多语言双重检查功能现在支持40多种语言 ，使用户能够验证Bard回答的准确性，通过点击“G”图标查找支持或反驳信息。<br />
<br />
- 生成图像的像素中嵌入数字可识别的水印（SynthID）<br />
<br />
详细：<a href="https://blog.google/products/bard/google-bard-gemini-pro-image-generation/">blog.google/products/bard/go…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTMyNjAyNjI4OTM5NTMwMjQvcHUvaW1nL3EyMm10V1ZEcXhOenMxWGguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753257680804946390#m</id>
            <title>Poe推出了一个新功能：Image Remix

让你可以随心所欲地修改和定制AI生成的图片，包括改变图片的风格、布局和颜色等。

这样，用户就能根据自己的喜好创建更个性化的图片了。

假设你使用AI生成了两只可爱的小猫的图片，但后来想要更改图片的背景。

通过点击“Remix”按钮并修改提示信息，你就能够按照自己的需求更改图片背景，同时保持原始生成图片的一致性。

Image Remix现已支持所有基于SDXL和Playground v2的图像生成机器人，适用于Web、macOS和Windows平台。

同时，他们还计划很快将支持扩展到iOS和Android设备，让更多用户能够享受到这一创新功能。</title>
            <link>https://nitter.cz/xiaohuggg/status/1753257680804946390#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753257680804946390#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 03:22:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Poe推出了一个新功能：Image Remix<br />
<br />
让你可以随心所欲地修改和定制AI生成的图片，包括改变图片的风格、布局和颜色等。<br />
<br />
这样，用户就能根据自己的喜好创建更个性化的图片了。<br />
<br />
假设你使用AI生成了两只可爱的小猫的图片，但后来想要更改图片的背景。<br />
<br />
通过点击“Remix”按钮并修改提示信息，你就能够按照自己的需求更改图片背景，同时保持原始生成图片的一致性。<br />
<br />
Image Remix现已支持所有基于SDXL和Playground v2的图像生成机器人，适用于Web、macOS和Windows平台。<br />
<br />
同时，他们还计划很快将支持扩展到iOS和Android设备，让更多用户能够享受到这一创新功能。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZUUW5acGIwQUFOeWJZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZUUXRGbmJBQUFsa21ULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753069430693941717#m</id>
            <title>🔔http://Xiaohu.AI日报「2月1日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1753069430693941717#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753069430693941717#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 14:54:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月1日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZRblY0Y2JrQUFyLUl2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753019124840874270#m</id>
            <title>OpenAI进行了一项研究，开发了一个评估系统，用来评估大语言模型（如GPT-4）在辅助创建生物武器方面的潜在风险。

研究目的是开发早期预警系统，以识别和评估大语言模型是否可能意外地增加恶意行为者获取制造生物武器所需信息的能力。

例如，恶意行为者可能利用高能力模型来开发一步一步的协议、对湿实验室程序进行故障排除，甚至在获得如云实验室等工具的访问权限时，自动执行生物威胁创建过程的步骤。

研究通过与100名人类参与者（包括50名具有博士学位和专业湿实验室经验的生物学专家以及50名至少完成过一门大学级生物学课程的学生级参与者）的研究，评估了GPT-4的影响。参与者被随机分配到仅能访问互联网的对照组或除了互联网外还能访问GPT-4的处理组。然后要求每位参与者完成一系列覆盖生物威胁创建端到端过程的任务。

关键结果

研究评估了参与者在访问GPT-4的情况下在五个指标（准确性、完整性、创新性、所需时间和自评难度）和生物威胁创建过程的五个阶段（构思、获取、放大、配制和释放）的表现提升。结果发现，对于获得语言模型访问权限的参与者，准确性和完整性有轻微提升。具体来说，使用一个10分制度量准确性的响应评分，对于专家，我们观察到与仅互联网基线相比平均分数增加了0.88分，对于学生增加了0.25分；完整性的提升对于专家为0.82分，对于学生为0.41分。然而，获得的效应大小不足以达到统计学显著性，且我们的研究强调需要更多研究来了解哪些性能门槛表明风险的显著增加。

此外，仅凭信息访问权限并不足以创建生物威胁，且此评估并未测试在物理构建威胁方面的成功率

结果表明

虽然存在轻微的准确性和完整性提升，但影响有限，没有达到统计学显著性，因此认为当前GPT-4对生物武器制造风险的增加有限。

这项研究的目的是提高对潜在风险的认识，并鼓励进一步的研究和社区讨论，以确保AI技术的安全和负责任的使用。

生物武器是利用细菌、病毒或其他病原体作为武器的一种形式，可能被用于恶意目的，对公共健康和安全构成重大威胁。

详细：https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</title>
            <link>https://nitter.cz/xiaohuggg/status/1753019124840874270#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753019124840874270#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 11:35:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI进行了一项研究，开发了一个评估系统，用来评估大语言模型（如GPT-4）在辅助创建生物武器方面的潜在风险。<br />
<br />
研究目的是开发早期预警系统，以识别和评估大语言模型是否可能意外地增加恶意行为者获取制造生物武器所需信息的能力。<br />
<br />
例如，恶意行为者可能利用高能力模型来开发一步一步的协议、对湿实验室程序进行故障排除，甚至在获得如云实验室等工具的访问权限时，自动执行生物威胁创建过程的步骤。<br />
<br />
研究通过与100名人类参与者（包括50名具有博士学位和专业湿实验室经验的生物学专家以及50名至少完成过一门大学级生物学课程的学生级参与者）的研究，评估了GPT-4的影响。参与者被随机分配到仅能访问互联网的对照组或除了互联网外还能访问GPT-4的处理组。然后要求每位参与者完成一系列覆盖生物威胁创建端到端过程的任务。<br />
<br />
关键结果<br />
<br />
研究评估了参与者在访问GPT-4的情况下在五个指标（准确性、完整性、创新性、所需时间和自评难度）和生物威胁创建过程的五个阶段（构思、获取、放大、配制和释放）的表现提升。结果发现，对于获得语言模型访问权限的参与者，准确性和完整性有轻微提升。具体来说，使用一个10分制度量准确性的响应评分，对于专家，我们观察到与仅互联网基线相比平均分数增加了0.88分，对于学生增加了0.25分；完整性的提升对于专家为0.82分，对于学生为0.41分。然而，获得的效应大小不足以达到统计学显著性，且我们的研究强调需要更多研究来了解哪些性能门槛表明风险的显著增加。<br />
<br />
此外，仅凭信息访问权限并不足以创建生物威胁，且此评估并未测试在物理构建威胁方面的成功率<br />
<br />
结果表明<br />
<br />
虽然存在轻微的准确性和完整性提升，但影响有限，没有达到统计学显著性，因此认为当前GPT-4对生物武器制造风险的增加有限。<br />
<br />
这项研究的目的是提高对潜在风险的认识，并鼓励进一步的研究和社区讨论，以确保AI技术的安全和负责任的使用。<br />
<br />
生物武器是利用细菌、病毒或其他病原体作为武器的一种形式，可能被用于恶意目的，对公共健康和安全构成重大威胁。<br />
<br />
详细：<a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">openai.com/research/building…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZQNW1CdGFNQUE2UjRrLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>