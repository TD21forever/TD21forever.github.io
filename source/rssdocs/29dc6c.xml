<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733055007833092357#m</id>
            <title>我们已收到您关于 GPT4 变得更加懒惰的所有反馈！

自 11 月 11 日以来我们就没有更新过模型，这当然不是故意的。

模型行为可能是不可预测的，我们正在研究修复它🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1733055007833092357#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733055007833092357#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:24:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们已收到您关于 GPT4 变得更加懒惰的所有反馈！<br />
<br />
自 11 月 11 日以来我们就没有更新过模型，这当然不是故意的。<br />
<br />
模型行为可能是不可预测的，我们正在研究修复它🫡</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1732979491071549792#m">nitter.cz/ChatGPTapp/status/1732979491071549792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732990584619778077#m</id>
            <title>NewsNerd HackerBot：一个Hacker News 机器人🫡

它可以自动从 Hacker News 上抓取各种类型的新闻故事，比如最热门的、最新的或者特定主题的故事。

- 关键词过滤：如果你对某个特定主题感兴趣，比如想了解有关某个知名科技人物或公司的新闻，你可以告诉这个程序，它会帮你筛选出相关的新闻故事。

- 本地运行：这个程序是开源的，意味着你可以自己下载代码，然后在你的电脑上运行它。

- 未来计划：开发者还打算让这个程序能分析新闻故事的评论，或者分析链接到的文章内容。

GitHub：https://github.com/neural-maze/talking_with_hn
作者：@MTrofficus</title>
            <link>https://nitter.cz/xiaohuggg/status/1732990584619778077#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732990584619778077#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 05:08:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NewsNerd HackerBot：一个Hacker News 机器人🫡<br />
<br />
它可以自动从 Hacker News 上抓取各种类型的新闻故事，比如最热门的、最新的或者特定主题的故事。<br />
<br />
- 关键词过滤：如果你对某个特定主题感兴趣，比如想了解有关某个知名科技人物或公司的新闻，你可以告诉这个程序，它会帮你筛选出相关的新闻故事。<br />
<br />
- 本地运行：这个程序是开源的，意味着你可以自己下载代码，然后在你的电脑上运行它。<br />
<br />
- 未来计划：开发者还打算让这个程序能分析新闻故事的评论，或者分析链接到的文章内容。<br />
<br />
GitHub：<a href="https://github.com/neural-maze/talking_with_hn">github.com/neural-maze/talki…</a><br />
作者：<a href="https://nitter.cz/MTrofficus" title="Miguel Otero Pedrido">@MTrofficus</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODUwMjE2OTc1NjQ2NzIvcHUvaW1nL2pSNTJYd0N1OVl5NDNIelIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732981463489110318#m</id>
            <title>R to @xiaohuggg: LooseControl还可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。

例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化等。</title>
            <link>https://nitter.cz/xiaohuggg/status/1732981463489110318#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732981463489110318#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:32:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LooseControl还可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。<br />
<br />
例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化等。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODE0MTczMDU2Njk2MzIvcHUvaW1nL2ZVdlI2RTM0aUZYWE14aDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732981317560881423#m</id>
            <title>LooseControl：一个创新的图像生成框架，可根据深度信息来引导图像的生成。

LooseControl在生成图像时会考虑物体之间的空间关系，可通过简单地描述物体在场景中的位置来创建复杂的场景。

例如你想设计一个房间，但只有一些基本想法，你只需要描述你的想法，它会根据你的描述设计一张真实的效果图。

LooseControl 提供了一种新颖的方式来设计复杂场景并执行语义编辑。

可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。

例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化。

基本背景：

LooseControl 允许用户在图像生成过程中使用深度条件，这意味着它可以根据物体在场景中的深度（即远近关系）来引导图像的生成。这里的“深度条件”指的是在生成图像时考虑物体之间的空间关系，比如哪个物体在前面，哪个在后面。

以前的技术，如 ControlNet，也使用了深度信息来生成图像，但它们通常需要非常详细和准确的深度图。深度图是一种显示场景中每个点距离观察者远近的图像，通常需要专业的设备和软件来创建。

LooseControl 的创新之处在于，它不需要这么详细的深度图。它允许用户以更简单、更灵活的方式指定深度条件。例如，用户可以只指定场景的大致布局和物体的大致位置，而不是提供完整的深度图。这使得用户可以更容易地创建复杂的场景图像，即使他们没有专业的图像处理技能或设备。

简单来说，LooseControl 让图像生成变得更加容易和直观，用户可以通过简单地描述物体在场景中的位置来创建复杂的图像，而不需要复杂的技术支持。

LooseControl的主要特点：

1、场景边界控制 (C1)：用户可以通过指定场景的边界来粗略地定义场景，而不需要提供详细的深度图。

2、3D 盒子控制 (C2)：除了场景边界，用户还可以通过大致的 3D 边界盒子来指定目标对象的位置，而不是其确切的形状和外观。

3、编辑机制：

3D 盒子编辑 (E1)：用户可以通过更改、添加或移除盒子来细化图像，同时保持图像的风格不变。

属性编辑 (E2)：提供可能的编辑方向来改变场景的某个特定方面，如整体对象密度或特定对象。

4、应用场景：使用 LooseControl和文本指导，用户可以仅通过指定场景边界和主要对象的位置来创建复杂的环境（例如房间、街景等）。

举例解释：

假如你是一个室内设计师，需要设计一个房间的样子，但你只有一些基本的想法，比如房间里应该有一张沙发、一张桌子和一盏灯。LooseControl 就是一个可以帮助你把这些基本想法变成真实图像的工具。

1、场景边界控制：你可以告诉 LooseControl 房间的大致布局，比如沙发在哪里、桌子和灯在哪里。你不需要提供详细的图纸，只需要大概描述房间的布局。

2、3D 盒子控制：你可以定义每个物体（如沙发、桌子、灯）的位置和大小。就像在电脑游戏中放置物体一样，你可以决定它们在房间中的位置和朝向。

3、编辑功能：

3D 盒子编辑：如果你觉得沙发的位置不太对，或者想换一个形状不同的桌子，你可以轻松地调整它们，而不会影响到房间的其他部分。

属性编辑：如果你想改变房间的整体风格，比如从现代风格变成复古风格，LooseControl 也可以帮助你做到这一点。

总的来说，LooseControl 就像是一个高级的室内设计软件，让你可以轻松地设计和调整房间的样子，即使你只有一些非常基本的想法。

工作原理：

LooseControl基于 ControlNet 和 StableDiffusion 模型，通过 LoRA（Low Rank）网络适配和自动合成必要的训练数据来实现。

LooseControl依赖于对 ControlNet 和 StableDiffusion 模型的改进和适配。通过使用 LoRA网络适配和自动合成训练数据。

LooseControl能够在保留原始生成权重的同时进行微调。此外，通过操作注意力层中的“键”和“值”，以及对 ControlNet Jacobian 的奇异值分析，实现了上述的编辑功能。

项目及演示：https://shariqfarooq123.github.io/loose-control/
论文：https://arxiv.org/abs/2312.03079
GitHub：https://github.com/shariqfarooq123/LooseControl
在线Demo：https://huggingface.co/spaces/shariqfarooq/LooseControl</title>
            <link>https://nitter.cz/xiaohuggg/status/1732981317560881423#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732981317560881423#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:31:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LooseControl：一个创新的图像生成框架，可根据深度信息来引导图像的生成。<br />
<br />
LooseControl在生成图像时会考虑物体之间的空间关系，可通过简单地描述物体在场景中的位置来创建复杂的场景。<br />
<br />
例如你想设计一个房间，但只有一些基本想法，你只需要描述你的想法，它会根据你的描述设计一张真实的效果图。<br />
<br />
LooseControl 提供了一种新颖的方式来设计复杂场景并执行语义编辑。<br />
<br />
可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。<br />
<br />
例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化。<br />
<br />
基本背景：<br />
<br />
LooseControl 允许用户在图像生成过程中使用深度条件，这意味着它可以根据物体在场景中的深度（即远近关系）来引导图像的生成。这里的“深度条件”指的是在生成图像时考虑物体之间的空间关系，比如哪个物体在前面，哪个在后面。<br />
<br />
以前的技术，如 ControlNet，也使用了深度信息来生成图像，但它们通常需要非常详细和准确的深度图。深度图是一种显示场景中每个点距离观察者远近的图像，通常需要专业的设备和软件来创建。<br />
<br />
LooseControl 的创新之处在于，它不需要这么详细的深度图。它允许用户以更简单、更灵活的方式指定深度条件。例如，用户可以只指定场景的大致布局和物体的大致位置，而不是提供完整的深度图。这使得用户可以更容易地创建复杂的场景图像，即使他们没有专业的图像处理技能或设备。<br />
<br />
简单来说，LooseControl 让图像生成变得更加容易和直观，用户可以通过简单地描述物体在场景中的位置来创建复杂的图像，而不需要复杂的技术支持。<br />
<br />
LooseControl的主要特点：<br />
<br />
1、场景边界控制 (C1)：用户可以通过指定场景的边界来粗略地定义场景，而不需要提供详细的深度图。<br />
<br />
2、3D 盒子控制 (C2)：除了场景边界，用户还可以通过大致的 3D 边界盒子来指定目标对象的位置，而不是其确切的形状和外观。<br />
<br />
3、编辑机制：<br />
<br />
3D 盒子编辑 (E1)：用户可以通过更改、添加或移除盒子来细化图像，同时保持图像的风格不变。<br />
<br />
属性编辑 (E2)：提供可能的编辑方向来改变场景的某个特定方面，如整体对象密度或特定对象。<br />
<br />
4、应用场景：使用 LooseControl和文本指导，用户可以仅通过指定场景边界和主要对象的位置来创建复杂的环境（例如房间、街景等）。<br />
<br />
举例解释：<br />
<br />
假如你是一个室内设计师，需要设计一个房间的样子，但你只有一些基本的想法，比如房间里应该有一张沙发、一张桌子和一盏灯。LooseControl 就是一个可以帮助你把这些基本想法变成真实图像的工具。<br />
<br />
1、场景边界控制：你可以告诉 LooseControl 房间的大致布局，比如沙发在哪里、桌子和灯在哪里。你不需要提供详细的图纸，只需要大概描述房间的布局。<br />
<br />
2、3D 盒子控制：你可以定义每个物体（如沙发、桌子、灯）的位置和大小。就像在电脑游戏中放置物体一样，你可以决定它们在房间中的位置和朝向。<br />
<br />
3、编辑功能：<br />
<br />
3D 盒子编辑：如果你觉得沙发的位置不太对，或者想换一个形状不同的桌子，你可以轻松地调整它们，而不会影响到房间的其他部分。<br />
<br />
属性编辑：如果你想改变房间的整体风格，比如从现代风格变成复古风格，LooseControl 也可以帮助你做到这一点。<br />
<br />
总的来说，LooseControl 就像是一个高级的室内设计软件，让你可以轻松地设计和调整房间的样子，即使你只有一些非常基本的想法。<br />
<br />
工作原理：<br />
<br />
LooseControl基于 ControlNet 和 StableDiffusion 模型，通过 LoRA（Low Rank）网络适配和自动合成必要的训练数据来实现。<br />
<br />
LooseControl依赖于对 ControlNet 和 StableDiffusion 模型的改进和适配。通过使用 LoRA网络适配和自动合成训练数据。<br />
<br />
LooseControl能够在保留原始生成权重的同时进行微调。此外，通过操作注意力层中的“键”和“值”，以及对 ControlNet Jacobian 的奇异值分析，实现了上述的编辑功能。<br />
<br />
项目及演示：<a href="https://shariqfarooq123.github.io/loose-control/">shariqfarooq123.github.io/lo…</a><br />
论文：<a href="https://arxiv.org/abs/2312.03079">arxiv.org/abs/2312.03079</a><br />
GitHub：<a href="https://github.com/shariqfarooq123/LooseControl">github.com/shariqfarooq123/L…</a><br />
在线Demo：<a href="https://huggingface.co/spaces/shariqfarooq/LooseControl">huggingface.co/spaces/shariq…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODAwMDg1ODk5MTQxMTIvcHUvaW1nL2d6UUFCLUh6UEhDU0pISmQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732969634674983299#m</id>
            <title>R to @xiaohuggg: 只用3张照片生成的的一个场景</title>
            <link>https://nitter.cz/xiaohuggg/status/1732969634674983299#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732969634674983299#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:45:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>只用3张照片生成的的一个场景</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5Njk1MzU0MDg2Njg2NzIvcHUvaW1nL0k1UU0wVDk0N0NKbXNmQmguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732969487157317909#m</id>
            <title>ReconFusion：只需3张照片即可重建真实世界3D场景

NeRF 和 3D Gaussian splats （3D高斯泼溅）的最大缺陷就是每次都要从头开始训练，而且每次需要几十到数百个输入图像，非常耗时。

ReconFusion提出一种新的 3D 重建方法，只需要最少3张图片即可重建该图像的360度3D场景。

而且效果非常逼真...

ReconFusion的主要特点和优点：

1、少量照片即可重建：与传统的 3D 重建技术相比，ReconFusion 只需少量照片即可重建出高质量的 3D 模型。这大大减少了捕捉过程中所需的时间和资源。

2、逼真的几何和纹理合成：在照片覆盖不到的区域，ReconFusion 能够合成逼真的几何形状和纹理，提高了模型的真实感和细节丰富度。

3、使用扩散先验进行优化：该技术利用扩散先验进行新视角合成，这种先验在合成和多视角数据集上进行训练，有助于规范基于神经辐射场（NeRF）的 3D 重建流程。

4、提高重建质量：ReconFusion 在各种真实世界数据集上进行了广泛评估，显示出在少视角 3D 重建方面相比以往方法有显著的性能提升。

5、适用于复杂场景：该技术适用于各种真实世界场景的重建，包括面向前方和 360 度的场景，增强了其应用的灵活性和广泛性。

项目及演示：https://reconfusion.github.io/
论文：https://arxiv.org/abs/2312.02981</title>
            <link>https://nitter.cz/xiaohuggg/status/1732969487157317909#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732969487157317909#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:44:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ReconFusion：只需3张照片即可重建真实世界3D场景<br />
<br />
NeRF 和 3D Gaussian splats （3D高斯泼溅）的最大缺陷就是每次都要从头开始训练，而且每次需要几十到数百个输入图像，非常耗时。<br />
<br />
ReconFusion提出一种新的 3D 重建方法，只需要最少3张图片即可重建该图像的360度3D场景。<br />
<br />
而且效果非常逼真...<br />
<br />
ReconFusion的主要特点和优点：<br />
<br />
1、少量照片即可重建：与传统的 3D 重建技术相比，ReconFusion 只需少量照片即可重建出高质量的 3D 模型。这大大减少了捕捉过程中所需的时间和资源。<br />
<br />
2、逼真的几何和纹理合成：在照片覆盖不到的区域，ReconFusion 能够合成逼真的几何形状和纹理，提高了模型的真实感和细节丰富度。<br />
<br />
3、使用扩散先验进行优化：该技术利用扩散先验进行新视角合成，这种先验在合成和多视角数据集上进行训练，有助于规范基于神经辐射场（NeRF）的 3D 重建流程。<br />
<br />
4、提高重建质量：ReconFusion 在各种真实世界数据集上进行了广泛评估，显示出在少视角 3D 重建方面相比以往方法有显著的性能提升。<br />
<br />
5、适用于复杂场景：该技术适用于各种真实世界场景的重建，包括面向前方和 360 度的场景，增强了其应用的灵活性和广泛性。<br />
<br />
项目及演示：<a href="https://reconfusion.github.io/">reconfusion.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02981">arxiv.org/abs/2312.02981</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5NjY2MDI1NDE1NzIwOTYvcHUvaW1nL21ZX01obDBqQ0U0cTdNdEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732967294895288347#m</id>
            <title>所以说在任何事情中人其实是最不可控的因素

人会掺杂很多的情感、个人利益和道德因素

还是AI更可控😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1732967294895288347#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732967294895288347#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:36:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以说在任何事情中人其实是最不可控的因素<br />
<br />
人会掺杂很多的情感、个人利益和道德因素<br />
<br />
还是AI更可控😅</p>
<p><a href="https://nitter.cz/dotey/status/1732896556389429676#m">nitter.cz/dotey/status/1732896556389429676#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732752805654630610#m</id>
            <title>现在恐怕整个互联网

就推特的视频画质最渣了吧

发个视频，感觉还不如十年前的清晰

唉😮‍💨</title>
            <link>https://nitter.cz/xiaohuggg/status/1732752805654630610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732752805654630610#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 13:23:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>现在恐怕整个互联网<br />
<br />
就推特的视频画质最渣了吧<br />
<br />
发个视频，感觉还不如十年前的清晰<br />
<br />
唉😮‍💨</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732749893641666841#m</id>
            <title>Demeter：一款Meta Quest混合现实游戏，把你的家变成游戏场景。

该游戏利用你家里的实际环境（比如墙壁和家具）来创造游戏关卡，让你在自己的客厅里攀爬、跳跃、飞行和奔跑。

故事情节是你要帮助一个来自外星的角色探索她的星球，并解开她为什么会出现在你家的谜团。😂

Demeter是迄今为止使用混合现实技术制作的最大型游戏，其引人入胜的故事、传输性的表演和激动人心的音乐共同营造了戏剧性的紧张氛围。

游戏支持单人模式，适用于 Meta Quest 3、Meta Quest Pro 和 Meta Quest 2 平台。

游戏预计将于 2024 年 1 月 25 日发布。售价19.99 美元.</title>
            <link>https://nitter.cz/xiaohuggg/status/1732749893641666841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732749893641666841#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 13:12:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Demeter：一款Meta Quest混合现实游戏，把你的家变成游戏场景。<br />
<br />
该游戏利用你家里的实际环境（比如墙壁和家具）来创造游戏关卡，让你在自己的客厅里攀爬、跳跃、飞行和奔跑。<br />
<br />
故事情节是你要帮助一个来自外星的角色探索她的星球，并解开她为什么会出现在你家的谜团。😂<br />
<br />
Demeter是迄今为止使用混合现实技术制作的最大型游戏，其引人入胜的故事、传输性的表演和激动人心的音乐共同营造了戏剧性的紧张氛围。<br />
<br />
游戏支持单人模式，适用于 Meta Quest 3、Meta Quest Pro 和 Meta Quest 2 平台。<br />
<br />
游戏预计将于 2024 年 1 月 25 日发布。售价19.99 美元.</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI3MzAyNTU1OTk4OTg2MjQvcHUvaW1nL1RKcTZIVy1CMGRqVUFtZVcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732731878946570566#m</id>
            <title>Sound ID：可使用手机监听周围的鸟叫，并实时预测是什么鸟在鸣叫

Sound ID是Merlin Bird ID应用程序的一个功能，目前，Merlin可以基于声音识别美国和加拿大的458种鸟类。

Sound ID 可在离线设备上运行，无需网络连接。

Merlin提供了一个免费的全球鸟类指南，包含各种鸟类照片、声音、地图等信息。

Sound ID 的工作原理：

1、当手机录制声音时，Merlin 将音频转换为一种称为频谱图的图像。频谱图绘制了录音中出现的声音频率随时间的变化。

2、这个频谱图图像随后被输入到一个现代计算机视觉模型——深度卷积神经网络中。

3、该模型经过训练，能够基于包含鸟类声音的 140 小时音频和包含非鸟类背景声音（如口哨声和汽车噪音）的 126 小时音频来识别鸟类。

4、Merlin 的 Sound ID 工具使用了包含每只鸟发声精确时刻的音频数据进行训练。

这个数据的生成过程需要声音识别专家仔细聆听每个音频文件，因此模型有机会学习到更准确的声音与物种对应关系。

Sound ID：https://merlin.allaboutbirds.org/sound-id/

Sound ID 背后的机器学习原理：https://www.macaulaylibrary.org/machine-learning/

Merlin Bird ID 网站是了一个免费的全球鸟类指南，包含照片、声音、地图等信息。

鸟类识别向导：用户可以通过回答三个简单的问题来识别他们看到或听到的鸟类，Merlin 将提供可能的匹配列表。
Merlin 为所有水平的观鸟者和户外爱好者提供快速识别帮助，帮助他们了解世界各地的鸟类。

声音识别（Sound ID）：Sound ID 功能可以听取周围的鸟鸣，并实时显示可能的鸟类建议。

用户可以将录音与 Merlin 中的歌声和叫声进行比较以确认所听到的内容。Sound ID 完全离线工作，因此用户无论身在何处都可以识别鸟鸣。

目前适用于美国、加拿大、欧洲的鸟类，以及中南美洲和印度的一些常见鸟类。

照片识别：用户可以拍摄鸟类的照片，或从相机卷中选择一张照片，Photo ID 将提供一份可能匹配的简短列表。
Photo ID 也完全离线工作，因此用户可以在任何地方识别照片中的鸟类。

保存鸟类到生活清单：用户可以通过“Save My Bird”功能构建自己的数字观鸟回忆录。每次识别一种鸟类时，点击“这是我的鸟！”按钮，Merlin 将其添加到用户不断增长的生活清单中。

探索附近的鸟类清单：Merlin 由 eBird 提供支持，允许用户根据所在位置构建定制的鸟类清单。
用户可以使用过滤选项探索不同地点或一年中不同时间的鸟类，或切换显示已下载的鸟类包中的所有物种。

应用下载：https://merlin.allaboutbirds.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1732731878946570566#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732731878946570566#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 12:00:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sound ID：可使用手机监听周围的鸟叫，并实时预测是什么鸟在鸣叫<br />
<br />
Sound ID是Merlin Bird ID应用程序的一个功能，目前，Merlin可以基于声音识别美国和加拿大的458种鸟类。<br />
<br />
Sound ID 可在离线设备上运行，无需网络连接。<br />
<br />
Merlin提供了一个免费的全球鸟类指南，包含各种鸟类照片、声音、地图等信息。<br />
<br />
Sound ID 的工作原理：<br />
<br />
1、当手机录制声音时，Merlin 将音频转换为一种称为频谱图的图像。频谱图绘制了录音中出现的声音频率随时间的变化。<br />
<br />
2、这个频谱图图像随后被输入到一个现代计算机视觉模型——深度卷积神经网络中。<br />
<br />
3、该模型经过训练，能够基于包含鸟类声音的 140 小时音频和包含非鸟类背景声音（如口哨声和汽车噪音）的 126 小时音频来识别鸟类。<br />
<br />
4、Merlin 的 Sound ID 工具使用了包含每只鸟发声精确时刻的音频数据进行训练。<br />
<br />
这个数据的生成过程需要声音识别专家仔细聆听每个音频文件，因此模型有机会学习到更准确的声音与物种对应关系。<br />
<br />
Sound ID：<a href="https://merlin.allaboutbirds.org/sound-id/">merlin.allaboutbirds.org/sou…</a><br />
<br />
Sound ID 背后的机器学习原理：<a href="https://www.macaulaylibrary.org/machine-learning/">macaulaylibrary.org/machine-…</a><br />
<br />
Merlin Bird ID 网站是了一个免费的全球鸟类指南，包含照片、声音、地图等信息。<br />
<br />
鸟类识别向导：用户可以通过回答三个简单的问题来识别他们看到或听到的鸟类，Merlin 将提供可能的匹配列表。<br />
Merlin 为所有水平的观鸟者和户外爱好者提供快速识别帮助，帮助他们了解世界各地的鸟类。<br />
<br />
声音识别（Sound ID）：Sound ID 功能可以听取周围的鸟鸣，并实时显示可能的鸟类建议。<br />
<br />
用户可以将录音与 Merlin 中的歌声和叫声进行比较以确认所听到的内容。Sound ID 完全离线工作，因此用户无论身在何处都可以识别鸟鸣。<br />
<br />
目前适用于美国、加拿大、欧洲的鸟类，以及中南美洲和印度的一些常见鸟类。<br />
<br />
照片识别：用户可以拍摄鸟类的照片，或从相机卷中选择一张照片，Photo ID 将提供一份可能匹配的简短列表。<br />
Photo ID 也完全离线工作，因此用户可以在任何地方识别照片中的鸟类。<br />
<br />
保存鸟类到生活清单：用户可以通过“Save My Bird”功能构建自己的数字观鸟回忆录。每次识别一种鸟类时，点击“这是我的鸟！”按钮，Merlin 将其添加到用户不断增长的生活清单中。<br />
<br />
探索附近的鸟类清单：Merlin 由 eBird 提供支持，允许用户根据所在位置构建定制的鸟类清单。<br />
用户可以使用过滤选项探索不同地点或一年中不同时间的鸟类，或切换显示已下载的鸟类包中的所有物种。<br />
<br />
应用下载：<a href="https://merlin.allaboutbirds.org/">merlin.allaboutbirds.org/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI3MjU1NjQ0MTM1NTg3ODQvcHUvaW1nL0w2YUs3dGZzbTZfZWt4R04uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732707212743811420#m</id>
            <title>Neum AI：为RAG提供一整套全面解决方案

Neum AI是一个用于大规模管理向量嵌入创建和同步的顶级框架。帮助开发者利用他们的数据来通过检索增强生成（RAG）为大语言模型提供精确的上下文。

🏭 高吞吐量的分布式架构，能够处理数十亿个数据点。允许高度并行化以优化嵌入生成和摄取。

🧱 内置的数据连接器，连接常见的数据源、嵌入服务和向量存储。

🔄 实时同步数据源，确保数据始终是最新的。

♻ 可定制的数据预处理，包括加载、分块和选择。

🤝 统一的数据管理，支持带有元数据的混合检索。Neum AI自动增强和跟踪元数据，提供丰富的检索体验。

Neum AI还提供了一个云平台，支持大规模、分布式架构，可以处理数百万个文档的向量嵌入。

GitHub：https://github.com/NeumTry/NeumAI
网站：https://www.neum.ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1732707212743811420#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732707212743811420#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 10:22:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Neum AI：为RAG提供一整套全面解决方案<br />
<br />
Neum AI是一个用于大规模管理向量嵌入创建和同步的顶级框架。帮助开发者利用他们的数据来通过检索增强生成（RAG）为大语言模型提供精确的上下文。<br />
<br />
🏭 高吞吐量的分布式架构，能够处理数十亿个数据点。允许高度并行化以优化嵌入生成和摄取。<br />
<br />
🧱 内置的数据连接器，连接常见的数据源、嵌入服务和向量存储。<br />
<br />
🔄 实时同步数据源，确保数据始终是最新的。<br />
<br />
♻ 可定制的数据预处理，包括加载、分块和选择。<br />
<br />
🤝 统一的数据管理，支持带有元数据的混合检索。Neum AI自动增强和跟踪元数据，提供丰富的检索体验。<br />
<br />
Neum AI还提供了一个云平台，支持大规模、分布式架构，可以处理数百万个文档的向量嵌入。<br />
<br />
GitHub：<a href="https://github.com/NeumTry/NeumAI">github.com/NeumTry/NeumAI</a><br />
网站：<a href="https://www.neum.ai/">neum.ai/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0F2UUJVVWJZQUFUZzBnLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732613061104882087#m</id>
            <title>AMD发布最强AI芯片 Instinct MI300X AI 加速器和 Instinct MI300A 数据中心 APU，声称比 Nvidia 的竞争 GPU 领先 1.6 倍。

与 Nvidia 竞争产品相比，在以下几个关键方面展示了显著优势：

配置方面：

内存容量：MI300X：拥有 192 GB 的 HBM3 内存，是 Nvidia GPU 的两倍以上。

MI300A：提供 128 GB 的 HBM3 内存，仍然比 Nvidia H100 SXM GPU 的内存容量高 1.6 倍。

带宽：MI300X 和 MI300A：都提供高达 5.3 TB/s 的带宽，这在当前的 GPU 市场中是非常高的。

计算性能：

MI300X：在 AI 推理工作负载中，性能比 Nvidia H100 高出 1.6 倍。

在 HPC 工作负载的 FP64 和 FP32 向量矩阵吞吐量方面，比 Nvidia H100 高出 2.4 倍。

MI300A：在 FP64 Matrix/DGEMM 和 FP64/FP32 Vector TFLOPS 方面，声称比 Nvidia H100 有 1.8 倍的优势。
在 OpenFOAM HPC 测试中，比 Nvidia H100 快 4 倍。

详细配置：

Instinct MI300X AI 加速器：

设计：采用“3.5D”封装技术，结合了 3D 堆叠的 GPU 和 I/O 芯片组。

性能：拥有 304 个计算单元、192GB 的 HBM3 内存和 5.3 TB/s 的带宽。

能效：在某些 AI 推理工作负载中，性能比 Nvidia H100 高出 1.6 倍。

内存容量：比 Nvidia GPU 的 HBM3 内存容量多两倍以上，达到每个 192 GB。

系统配置：设计为以八个为一组工作，整体系统拥有 1.5TB 的总 HBM3 内存和 10.4 Petaflops 的性能。

Instinct MI300A 数据中心 APU：

创新：结合了 CPU 和 GPU 的世界首款数据中心 APU。

构成：包含 24 个 Zen 4 CPU 核心和 228 个 CDNA 3 计算单元。

内存：拥有 128GB 的 HBM3 内存和 5.3 TB/s 的带宽。

能效：在 FP64 Matrix/DGEMM 和 FP64/FP32 Vector TFLOPS 方面，声称比 Nvidia H100 有 1.8 倍的优势。

官方详细介绍：https://www.amd.com/en/products/accelerators/instinct/mi300.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1732613061104882087#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732613061104882087#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 04:08:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AMD发布最强AI芯片 Instinct MI300X AI 加速器和 Instinct MI300A 数据中心 APU，声称比 Nvidia 的竞争 GPU 领先 1.6 倍。<br />
<br />
与 Nvidia 竞争产品相比，在以下几个关键方面展示了显著优势：<br />
<br />
配置方面：<br />
<br />
内存容量：MI300X：拥有 192 GB 的 HBM3 内存，是 Nvidia GPU 的两倍以上。<br />
<br />
MI300A：提供 128 GB 的 HBM3 内存，仍然比 Nvidia H100 SXM GPU 的内存容量高 1.6 倍。<br />
<br />
带宽：MI300X 和 MI300A：都提供高达 5.3 TB/s 的带宽，这在当前的 GPU 市场中是非常高的。<br />
<br />
计算性能：<br />
<br />
MI300X：在 AI 推理工作负载中，性能比 Nvidia H100 高出 1.6 倍。<br />
<br />
在 HPC 工作负载的 FP64 和 FP32 向量矩阵吞吐量方面，比 Nvidia H100 高出 2.4 倍。<br />
<br />
MI300A：在 FP64 Matrix/DGEMM 和 FP64/FP32 Vector TFLOPS 方面，声称比 Nvidia H100 有 1.8 倍的优势。<br />
在 OpenFOAM HPC 测试中，比 Nvidia H100 快 4 倍。<br />
<br />
详细配置：<br />
<br />
Instinct MI300X AI 加速器：<br />
<br />
设计：采用“3.5D”封装技术，结合了 3D 堆叠的 GPU 和 I/O 芯片组。<br />
<br />
性能：拥有 304 个计算单元、192GB 的 HBM3 内存和 5.3 TB/s 的带宽。<br />
<br />
能效：在某些 AI 推理工作负载中，性能比 Nvidia H100 高出 1.6 倍。<br />
<br />
内存容量：比 Nvidia GPU 的 HBM3 内存容量多两倍以上，达到每个 192 GB。<br />
<br />
系统配置：设计为以八个为一组工作，整体系统拥有 1.5TB 的总 HBM3 内存和 10.4 Petaflops 的性能。<br />
<br />
Instinct MI300A 数据中心 APU：<br />
<br />
创新：结合了 CPU 和 GPU 的世界首款数据中心 APU。<br />
<br />
构成：包含 24 个 Zen 4 CPU 核心和 228 个 CDNA 3 计算单元。<br />
<br />
内存：拥有 128GB 的 HBM3 内存和 5.3 TB/s 的带宽。<br />
<br />
能效：在 FP64 Matrix/DGEMM 和 FP64/FP32 Vector TFLOPS 方面，声称比 Nvidia H100 有 1.8 倍的优势。<br />
<br />
官方详细介绍：<a href="https://www.amd.com/en/products/accelerators/instinct/mi300.html">amd.com/en/products/accelera…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI2MDkyOTU3MjI5MTM3OTIvcHUvaW1nL2N5QlRfeUNIRm9vb2RDaVIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732593267009896495#m</id>
            <title>R to @xiaohuggg: 视频画面扩充演示，牛P @doganuraldesign</title>
            <link>https://nitter.cz/xiaohuggg/status/1732593267009896495#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732593267009896495#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:49:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频画面扩充演示，牛P <a href="https://nitter.cz/doganuraldesign" title="Dogan Ural">@doganuraldesign</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIzODM3MjI0Mjk3MzA4MTYvcHUvaW1nL1hTVGtabDFmQWRVXzZ2NUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732593265063666024#m</id>
            <title>R to @xiaohuggg: 所有这些剪辑都是文本转视频👀@nickfloats</title>
            <link>https://nitter.cz/xiaohuggg/status/1732593265063666024#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732593265063666024#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:49:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所有这些剪辑都是文本转视频👀<a href="https://nitter.cz/nickfloats" title="Nick St. Pierre">@nickfloats</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMyMjY1OTY4OTg5MDAzNzc2L2ltZy9TdTZoS2lRYXRqV3FRNVJILmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732593262983307768#m</id>
            <title>R to @xiaohuggg: 一些编辑特效功能展示 @Martin_Haerlin</title>
            <link>https://nitter.cz/xiaohuggg/status/1732593262983307768#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732593262983307768#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些编辑特效功能展示 <a href="https://nitter.cz/Martin_Haerlin" title="Martin Haerlin">@Martin_Haerlin</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI0MzA2MjczMTQ1NDg3MzYvcHUvaW1nL3lQcXFIQzRZMzdrd1pKLXAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732593260965810197#m</id>
            <title>R to @xiaohuggg: 100% 文本转视频 @DaveJWVillalva</title>
            <link>https://nitter.cz/xiaohuggg/status/1732593260965810197#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732593260965810197#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>100% 文本转视频 <a href="https://nitter.cz/DaveJWVillalva" title="Dave Villalva">@DaveJWVillalva</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI0MzE1ODc2OTcyMTM0NDAvcHUvaW1nL1J4TENpUnlxNjN1VXFNZEwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>