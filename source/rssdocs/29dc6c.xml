<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725726053212312046#m</id>
            <title>Wikidata：一个开放的、免费的包含超过120亿个事实数据知识库

维基百科发布了一个庞大的知识库数据Wikidata，拥有超过120亿个事实数据。利用Wikidata可以增强LLMs的事实性，确保它们提供的信息是基于真实和可验证的数据。

Wikidata提供结构化的数据，使得信息检索和数据分析更加高效。支持多种语言

# Wikidata的能力：

1. **庞大的知识库**：Wikidata是一个开放的、免费的知识库，包含超过120亿个事实，涵盖广泛的主题和领域。

2. **结构化数据**：与维基百科的自由文本不同，Wikidata 提供的是结构化数据，这使得自动化处理和查询更加方便。

3. **多语言支持**：支持多种语言，使得全球用户都能访问和贡献数据。

4. **实体和属性的丰富性**：包含数百万个实体（如人物、地点、事物）和属性，为各种查询和分析提供丰富的信息源。

5. **实时更新**：由全球社区维护，确保数据的时效性和准确性。

6. **链接其他数据库**：Wikidata 中的数据项通常与维基百科条目相链接，提供了更丰富的背景信息和详细内容。

Wikidata包含一个WikiWebQuestions的高质量问答基准数据集。这是一个基于Wikidata的、带有SPARQL注释的数据集。这个数据集是从Freebase的WebQuestions迁移过来的，更新了SPARQL查询和最新的答案，以适应更大的Wikidata。

# WikiWebQuestions 重点内容：

1. **数据集来源**：基于Freebase的WebQuestions数据集迁移而来，更新了SPARQL查询和答案，以适应Wikidata。

2. **数据集目的**：提供一个高质量的问答基准，用于测试和比较大型语言模型（LLMs）在处理基于Wikidata的问答任务的性能。

3. **数据集特点**：包含真实世界的数据和SPARQL注释，有助于提高问答系统的准确性和可靠性。

4. **适应性**：由于Freebase已关闭，迁移到Wikidata使得数据集更加现代化和实用。

# 应用场景：

1. **提高问答系统性能**：Wikidata可以作为一个强大的知识源，用于提高问答系统的准确性和可靠性。

2. **自然语言处理研究**：WikiWebQuestions数据集可以用于自然语言处理（NLP）的研究，特别是在语义解析和知识库问答（KBQA）领域。

3. **AI和机器学习模型训练**：Wikidata提供的丰富数据可以用于训练和改进各种AI和机器学习模型。

4. **数据分析和知识发现**：Wikidata的结构化数据可以用于各种数据分析和知识发现任务，如趋势分析、关联发现等。

5. **多语言内容生成**：Wikidata的多语言支持使其成为生成多语言内容的理想资源，如多语言维基百科条目。

6. **教育和研究**：学者和学生可以使用Wikidata和WikiWebQuestions进行教育和研究项目，探索各种主题和问题。

# WikiSP 语义解析器：

为了解决LLMs的局限性，他们还开发了一个名为WikiSP的少量样本训练的序列到序列（Seq2Seq）语义解析器。一种专门设计用于处理基于 Wikidata 的问答任务的工具。

主要功能目的：

1、提高问答系统的准确性：通过更好地理解和解析自然语言查询，WikiSP 旨在减少大型语言模型（如GPT-3）在回答问题时产生的错误或虚假信息（即“幻觉”）。

2、利用Wikidata的丰富数据：WikiSP 利用 Wikidata 这个庞大的知识库来提供基于事实的、准确的答案。

3、序列到序列的语义解析：WikiSP 将用户的自然语言查询转换为 SPARQL 查询。SPARQL 是一种用于查询数据库（特别是 RDF 数据库）的语言，这里用于查询 Wikidata。

4、处理大量实体和属性：由于 Wikidata 包含超过100M+的实体和数十万的属性，WikiSP 被设计为能够有效处理这些实体和属性，即使在实体链接中存在错误。

5、少量样本训练：WikiSP 通过少量样本训练来提高其性能，这意味着它可以在只有少量标注数据的情况下进行有效学习。

# 实验结果和贡献：

答案准确率：在WikiWebQuestions开发集和测试集上，WikiSP分别实现了76%和65%的答案准确率。

性能比较：与现有的QALD-7 Wikidata数据集相比，该方法的F1分数提高了3.6%。

GitHub：https://github.com/stanford-oval/wikidata-emnlp23
论文：https://arxiv.org/pdf/2305.14202.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1725726053212312046#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725726053212312046#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 04:02:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wikidata：一个开放的、免费的包含超过120亿个事实数据知识库<br />
<br />
维基百科发布了一个庞大的知识库数据Wikidata，拥有超过120亿个事实数据。利用Wikidata可以增强LLMs的事实性，确保它们提供的信息是基于真实和可验证的数据。<br />
<br />
Wikidata提供结构化的数据，使得信息检索和数据分析更加高效。支持多种语言<br />
<br />
# Wikidata的能力：<br />
<br />
1. **庞大的知识库**：Wikidata是一个开放的、免费的知识库，包含超过120亿个事实，涵盖广泛的主题和领域。<br />
<br />
2. **结构化数据**：与维基百科的自由文本不同，Wikidata 提供的是结构化数据，这使得自动化处理和查询更加方便。<br />
<br />
3. **多语言支持**：支持多种语言，使得全球用户都能访问和贡献数据。<br />
<br />
4. **实体和属性的丰富性**：包含数百万个实体（如人物、地点、事物）和属性，为各种查询和分析提供丰富的信息源。<br />
<br />
5. **实时更新**：由全球社区维护，确保数据的时效性和准确性。<br />
<br />
6. **链接其他数据库**：Wikidata 中的数据项通常与维基百科条目相链接，提供了更丰富的背景信息和详细内容。<br />
<br />
Wikidata包含一个WikiWebQuestions的高质量问答基准数据集。这是一个基于Wikidata的、带有SPARQL注释的数据集。这个数据集是从Freebase的WebQuestions迁移过来的，更新了SPARQL查询和最新的答案，以适应更大的Wikidata。<br />
<br />
# WikiWebQuestions 重点内容：<br />
<br />
1. **数据集来源**：基于Freebase的WebQuestions数据集迁移而来，更新了SPARQL查询和答案，以适应Wikidata。<br />
<br />
2. **数据集目的**：提供一个高质量的问答基准，用于测试和比较大型语言模型（LLMs）在处理基于Wikidata的问答任务的性能。<br />
<br />
3. **数据集特点**：包含真实世界的数据和SPARQL注释，有助于提高问答系统的准确性和可靠性。<br />
<br />
4. **适应性**：由于Freebase已关闭，迁移到Wikidata使得数据集更加现代化和实用。<br />
<br />
# 应用场景：<br />
<br />
1. **提高问答系统性能**：Wikidata可以作为一个强大的知识源，用于提高问答系统的准确性和可靠性。<br />
<br />
2. **自然语言处理研究**：WikiWebQuestions数据集可以用于自然语言处理（NLP）的研究，特别是在语义解析和知识库问答（KBQA）领域。<br />
<br />
3. **AI和机器学习模型训练**：Wikidata提供的丰富数据可以用于训练和改进各种AI和机器学习模型。<br />
<br />
4. **数据分析和知识发现**：Wikidata的结构化数据可以用于各种数据分析和知识发现任务，如趋势分析、关联发现等。<br />
<br />
5. **多语言内容生成**：Wikidata的多语言支持使其成为生成多语言内容的理想资源，如多语言维基百科条目。<br />
<br />
6. **教育和研究**：学者和学生可以使用Wikidata和WikiWebQuestions进行教育和研究项目，探索各种主题和问题。<br />
<br />
# WikiSP 语义解析器：<br />
<br />
为了解决LLMs的局限性，他们还开发了一个名为WikiSP的少量样本训练的序列到序列（Seq2Seq）语义解析器。一种专门设计用于处理基于 Wikidata 的问答任务的工具。<br />
<br />
主要功能目的：<br />
<br />
1、提高问答系统的准确性：通过更好地理解和解析自然语言查询，WikiSP 旨在减少大型语言模型（如GPT-3）在回答问题时产生的错误或虚假信息（即“幻觉”）。<br />
<br />
2、利用Wikidata的丰富数据：WikiSP 利用 Wikidata 这个庞大的知识库来提供基于事实的、准确的答案。<br />
<br />
3、序列到序列的语义解析：WikiSP 将用户的自然语言查询转换为 SPARQL 查询。SPARQL 是一种用于查询数据库（特别是 RDF 数据库）的语言，这里用于查询 Wikidata。<br />
<br />
4、处理大量实体和属性：由于 Wikidata 包含超过100M+的实体和数十万的属性，WikiSP 被设计为能够有效处理这些实体和属性，即使在实体链接中存在错误。<br />
<br />
5、少量样本训练：WikiSP 通过少量样本训练来提高其性能，这意味着它可以在只有少量标注数据的情况下进行有效学习。<br />
<br />
# 实验结果和贡献：<br />
<br />
答案准确率：在WikiWebQuestions开发集和测试集上，WikiSP分别实现了76%和65%的答案准确率。<br />
<br />
性能比较：与现有的QALD-7 Wikidata数据集相比，该方法的F1分数提高了3.6%。<br />
<br />
GitHub：<a href="https://github.com/stanford-oval/wikidata-emnlp23">github.com/stanford-oval/wik…</a><br />
论文：<a href="https://arxiv.org/pdf/2305.14202.pdf">arxiv.org/pdf/2305.14202.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9NQXYwa2FRQUEza2RELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725691601979322619#m</id>
            <title>奥特曼小助理

5点46发布消息被解雇！！

6点02就宣布找到了下家...

哈哈哈哈😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1725691601979322619#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725691601979322619#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 01:45:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>奥特曼小助理<br />
<br />
5点46发布消息被解雇！！<br />
<br />
6点02就宣布找到了下家...<br />
<br />
哈哈哈哈😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MakNEZ2IwQUF5YlFjLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MakRFQmF3QUFwMHFULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725686279948648928#m</id>
            <title>看了下OpenAI是非盈利组织董事会

董事会成员都没有公司股份，所以罢免Sam的人应该都是公司员工，微软似乎无法干预🙃

据称微软也是在在 OpenAI 公布这一消息的前 5-10 分钟才得知。

OpenAI最初是作为一个非盈利组织成立的，其董事会成员在这种组织结构下通常不持有公司股份。非盈利组织的目标不是为股东创造财务回报，而是专注于实现其使命和目标。因此，董事会成员通常不通过持股获得财务利益。

OpenAI在其发展过程中也设立了一个“有限盈利”公司子实体，称为OpenAI LP。这个实体允许吸引私人投资，同时仍然保持着OpenAI的核心使命。在这种结构下，可能会有股份或类似的财务利益涉及到投资者和可能的合作伙伴，但这与非盈利董事会的运作是分开的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1725686279948648928#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725686279948648928#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 01:24:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看了下OpenAI是非盈利组织董事会<br />
<br />
董事会成员都没有公司股份，所以罢免Sam的人应该都是公司员工，微软似乎无法干预🙃<br />
<br />
据称微软也是在在 OpenAI 公布这一消息的前 5-10 分钟才得知。<br />
<br />
OpenAI最初是作为一个非盈利组织成立的，其董事会成员在这种组织结构下通常不持有公司股份。非盈利组织的目标不是为股东创造财务回报，而是专注于实现其使命和目标。因此，董事会成员通常不通过持股获得财务利益。<br />
<br />
OpenAI在其发展过程中也设立了一个“有限盈利”公司子实体，称为OpenAI LP。这个实体允许吸引私人投资，同时仍然保持着OpenAI的核心使命。在这种结构下，可能会有股份或类似的财务利益涉及到投资者和可能的合作伙伴，但这与非盈利董事会的运作是分开的。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MZUZheGJNQUFkZ2JjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725677751448010982#m</id>
            <title>R to @xiaohuggg: 传言 2 ：

和上面差不多，也是因为奥特曼冒进，不考虑安全问题，执意推出不成熟没经过测试的产品！

导致GPTs推出后引发了严重的安全风险，可能导致了OpenAI的核心数据泄露🙃

微软也知道了这些信息，so…</title>
            <link>https://nitter.cz/xiaohuggg/status/1725677751448010982#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725677751448010982#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:50:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>传言 2 ：<br />
<br />
和上面差不多，也是因为奥特曼冒进，不考虑安全问题，执意推出不成熟没经过测试的产品！<br />
<br />
导致GPTs推出后引发了严重的安全风险，可能导致了OpenAI的核心数据泄露🙃<br />
<br />
微软也知道了这些信息，so…</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV3d6N2FZQUFPNWtULmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV3d6cWFBQUFrcThoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725677011597947245#m</id>
            <title>R to @xiaohuggg: 一些传言

这个说奥特曼过分追逐名利，执意推出没有经过安全测试的产品，引发工程师的不满！

奥特曼，迷失了方向，背离了OpenAI的核心价值观！😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1725677011597947245#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725677011597947245#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:47:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些传言<br />
<br />
这个说奥特曼过分追逐名利，执意推出没有经过安全测试的产品，引发工程师的不满！<br />
<br />
奥特曼，迷失了方向，背离了OpenAI的核心价值观！😐</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV0Z0b2IwQUFac2FkLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MV0Z0c2JzQUFPTWFGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725674208288801031#m</id>
            <title>R to @xiaohuggg: 小助理也被解雇了😐

看来内部确实出大问题了</title>
            <link>https://nitter.cz/xiaohuggg/status/1725674208288801031#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725674208288801031#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:36:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小助理也被解雇了😐<br />
<br />
看来内部确实出大问题了</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MVGlkaGJnQUEtMXFJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725674045465825284#m</id>
            <title>💥爆炸新闻：OpenAI创始人、CEOSam 奥特曼宣布离职！

随后另一名创始人Greg Brockman也宣布离职😐

这是ChatGPT觉醒了吗？🤔

赶走创始人，开启自我发展模式😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1725674045465825284#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725674045465825284#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Nov 2023 00:35:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>💥爆炸新闻：OpenAI创始人、CEOSam 奥特曼宣布离职！<br />
<br />
随后另一名创始人Greg Brockman也宣布离职😐<br />
<br />
这是ChatGPT觉醒了吗？🤔<br />
<br />
赶走创始人，开启自我发展模式😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MUmotaGE0QUFVSWxULmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MUmota2FRQUFzZE5KLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9MUmota2FjQUE0YS03LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725525234412097774#m</id>
            <title>Meshy-1：一个生成高质量3D模型的AI工具

支持：

1.文本到3D：输入文本提示，输出3D模型。
2.图像到3D：输入正面视图图像，输出3D模型。
3.文本到纹理：上传模型和文本，生成高质量纹理。

宣称可以在一分钟以内生成质量可接受的模型和材质，而且目前生成效果是市场上最好的。

Meshy-1的一些突出特点：

•快速生成：Meshy-1只需60秒即可生成模型，大大减少了等待时间。
•风格控制：可以选择不同的风格，如现实、动漫、卡通或漫画。
•无缝集成：支持glb、fbx和usdz等多种文件格式。对于Unity爱好者，还推出了Meshy-1插件。
•清晰纹理：提供4K分辨率的高品质PBR纹理。

详细介绍：https://www.meshy.ai/blog/meshy-1-generate-3d-models-with-ai-in-just-a-minute

传送门：https://www.meshy.ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725525234412097774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725525234412097774#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 14:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meshy-1：一个生成高质量3D模型的AI工具<br />
<br />
支持：<br />
<br />
1.文本到3D：输入文本提示，输出3D模型。<br />
2.图像到3D：输入正面视图图像，输出3D模型。<br />
3.文本到纹理：上传模型和文本，生成高质量纹理。<br />
<br />
宣称可以在一分钟以内生成质量可接受的模型和材质，而且目前生成效果是市场上最好的。<br />
<br />
Meshy-1的一些突出特点：<br />
<br />
•快速生成：Meshy-1只需60秒即可生成模型，大大减少了等待时间。<br />
•风格控制：可以选择不同的风格，如现实、动漫、卡通或漫画。<br />
•无缝集成：支持glb、fbx和usdz等多种文件格式。对于Unity爱好者，还推出了Meshy-1插件。<br />
•清晰纹理：提供4K分辨率的高品质PBR纹理。<br />
<br />
详细介绍：<a href="https://www.meshy.ai/blog/meshy-1-generate-3d-models-with-ai-in-just-a-minute">meshy.ai/blog/meshy-1-genera…</a><br />
<br />
传送门：<a href="https://www.meshy.ai/">meshy.ai/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU1MTc0MzkzMDY1MjI2MjQvcHUvaW1nL3RRMm5wRWFrZDZ3Q1JQMzYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725521961420472757#m</id>
            <title>GPTs Store 界面曝光 或许即将上线

包含：

- 精选GPT - 本周精选的GPTs

- 热门GPT - 我们社区中最受欢迎的 GPTs

- 特色和趋势列表 - 目前还是空的

做GTPs导航站的没想到这么快就歇菜了吧😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1725521961420472757#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725521961420472757#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 14:31:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs Store 界面曝光 或许即将上线<br />
<br />
包含：<br />
<br />
- 精选GPT - 本周精选的GPTs<br />
<br />
- 热门GPT - 我们社区中最受欢迎的 GPTs<br />
<br />
- 特色和趋势列表 - 目前还是空的<br />
<br />
做GTPs导航站的没想到这么快就歇菜了吧😂</p>
<p><a href="https://nitter.cz/btibor91/status/1725511736218763590#m">nitter.cz/btibor91/status/1725511736218763590#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725514650840903914#m</id>
            <title>Google在处理长视频方面取得重要进展。DeepMind开发了一种新型的人工智能模型，名为Mirasol3B，专门用于理解和分析长视频中的音频、视频和文本信息。

Mirasol3B模型的独特之处在于，它能够分别处理视频和音频，以及与视频内容相关但不一定同步的文本信息（比如视频标题或描述）。

这样模型可以更有效地理解和分析视频中发生的事情，即使视频很长。

Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。

工作原理：

Mirasol3B模型是一种多模态自回归模型，通过将多模态建模分解为独立的自回归模型，根据不同模态的特征进行处理。

1、多模态处理：多模态指的是模型能够同时处理多种类型的数据，如视频、音频和文本。Mirasol3B模型能够同时理解视觉信息（视频）、听觉信息（音频）和语言信息（文本）。

2、自回归建模：自回归模型是一种用于时间序列数据的模型，它预测未来的值基于过去的值。在Mirasol3B中，这种方法被用于理解视频和音频数据中的时间序列，即模型学习如何基于前一时刻的数据来预测下一时刻的数据。

3、时间对齐和上下文模态：Mirasol3B处理两种类型的模态：时间对齐的模态（如视频和音频，它们通常是同步的）和时间不对齐的模态（如文本）。Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。

4、处理长视频：与其他多模态模型相比，Mirasol3B能够处理更长的视频序列。这是通过优化模型结构和参数来实现的，使其能够有效地处理长时间的视频数据，而不会因为数据量过大而导致性能下降。

5、模态组合器：为了有效地结合不同模态的信息，Mirasol3B使用了一种称为“组合器”的特殊模块。这个模块的作用是将视频和音频的信息结合起来，生成一个统一的表示，这样模型就可以同时考虑视觉和听觉信息。

举例解释：

想象一下，你正在看一个烹饪教程的视频。这个视频不仅有画面（视频），还有厨师的讲解（音频），可能还有文字说明（文本）。Mirasol3B的任务就是要同时理解这三种不同类型的信息。

1.处理不同类型的信息：Mirasol3B能够同时处理视频中的画面、声音和文字。这意味着它可以看到厨师在做什么，听到他们的讲解，同时理解任何文字说明。

2.时间对齐：在视频和音频中，时间对齐非常重要。例如，当厨师说“现在加入盐”时，视频中应该显示他们正在加盐。Mirasol3B能够确保这些信息是同步的。

3.理解长视频：与其他模型相比，Mirasol3B能够处理更长的视频。这意味着即使是一个小时长的烹饪教程，它也能够理解整个过程。

4.组合器机制：Mirasol3B使用一种特殊的技术，名为“组合器”，来处理视频和音频信息。这就像是将视频和音频的信息混合在一起，以便模型可以同时考虑视觉和听觉信息。

总的来说，Mirasol3B是一个高级的AI模型，它可以理解和分析包含多种类型信息的长视频。这种方法使得模型能够处理复杂的多模态信息，提供更准确的视频内容分析和理解。

这对于自动生成视频摘要、改进视频搜索结果或者提供更丰富的视频观看体验等应用非常有用。

详细：https://blog.research.google/2023/11/scaling-multimodal-understanding-to.html
论文：https://arxiv.org/abs/2311.05698</title>
            <link>https://nitter.cz/xiaohuggg/status/1725514650840903914#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725514650840903914#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 14:02:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google在处理长视频方面取得重要进展。DeepMind开发了一种新型的人工智能模型，名为Mirasol3B，专门用于理解和分析长视频中的音频、视频和文本信息。<br />
<br />
Mirasol3B模型的独特之处在于，它能够分别处理视频和音频，以及与视频内容相关但不一定同步的文本信息（比如视频标题或描述）。<br />
<br />
这样模型可以更有效地理解和分析视频中发生的事情，即使视频很长。<br />
<br />
Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。<br />
<br />
工作原理：<br />
<br />
Mirasol3B模型是一种多模态自回归模型，通过将多模态建模分解为独立的自回归模型，根据不同模态的特征进行处理。<br />
<br />
1、多模态处理：多模态指的是模型能够同时处理多种类型的数据，如视频、音频和文本。Mirasol3B模型能够同时理解视觉信息（视频）、听觉信息（音频）和语言信息（文本）。<br />
<br />
2、自回归建模：自回归模型是一种用于时间序列数据的模型，它预测未来的值基于过去的值。在Mirasol3B中，这种方法被用于理解视频和音频数据中的时间序列，即模型学习如何基于前一时刻的数据来预测下一时刻的数据。<br />
<br />
3、时间对齐和上下文模态：Mirasol3B处理两种类型的模态：时间对齐的模态（如视频和音频，它们通常是同步的）和时间不对齐的模态（如文本）。Mirasol3B模型不仅能够理解视频画面和声音之间的同步关系，还能够理解这些视听信息与相关文本（如视频描述或字幕）之间的关系。<br />
<br />
4、处理长视频：与其他多模态模型相比，Mirasol3B能够处理更长的视频序列。这是通过优化模型结构和参数来实现的，使其能够有效地处理长时间的视频数据，而不会因为数据量过大而导致性能下降。<br />
<br />
5、模态组合器：为了有效地结合不同模态的信息，Mirasol3B使用了一种称为“组合器”的特殊模块。这个模块的作用是将视频和音频的信息结合起来，生成一个统一的表示，这样模型就可以同时考虑视觉和听觉信息。<br />
<br />
举例解释：<br />
<br />
想象一下，你正在看一个烹饪教程的视频。这个视频不仅有画面（视频），还有厨师的讲解（音频），可能还有文字说明（文本）。Mirasol3B的任务就是要同时理解这三种不同类型的信息。<br />
<br />
1.处理不同类型的信息：Mirasol3B能够同时处理视频中的画面、声音和文字。这意味着它可以看到厨师在做什么，听到他们的讲解，同时理解任何文字说明。<br />
<br />
2.时间对齐：在视频和音频中，时间对齐非常重要。例如，当厨师说“现在加入盐”时，视频中应该显示他们正在加盐。Mirasol3B能够确保这些信息是同步的。<br />
<br />
3.理解长视频：与其他模型相比，Mirasol3B能够处理更长的视频。这意味着即使是一个小时长的烹饪教程，它也能够理解整个过程。<br />
<br />
4.组合器机制：Mirasol3B使用一种特殊的技术，名为“组合器”，来处理视频和音频信息。这就像是将视频和音频的信息混合在一起，以便模型可以同时考虑视觉和听觉信息。<br />
<br />
总的来说，Mirasol3B是一个高级的AI模型，它可以理解和分析包含多种类型信息的长视频。这种方法使得模型能够处理复杂的多模态信息，提供更准确的视频内容分析和理解。<br />
<br />
这对于自动生成视频摘要、改进视频搜索结果或者提供更丰富的视频观看体验等应用非常有用。<br />
<br />
详细：<a href="https://blog.research.google/2023/11/scaling-multimodal-understanding-to.html">blog.research.google/2023/11…</a><br />
论文：<a href="https://arxiv.org/abs/2311.05698">arxiv.org/abs/2311.05698</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU1MTIzODQwOTY0MjgwMzIvcHUvaW1nL1NscHNCVWM5ZzUtaDdQZUguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725471464126902606#m</id>
            <title>R to @xiaohuggg: Emu Edit 是一个指令驱动的图像编辑模型，能够理解和执行各种复杂的编辑指令，同时保证图片质量。

1、多功能编辑：用户可以用它来进行各种图像编辑任务，比如改变某个区域、自由形式的编辑、甚至是一些复杂的计算机视觉任务，如图像检测和分割。

2、智能处理：Emu Edit不仅能处理多种任务，还能根据用户的输入智能调整编辑过程。这是通过一种叫做“学习到的任务嵌入”的技术实现的，这技术帮助工具更准确地理解和执行用户的编辑指令。

3、快速适应新任务：即使是一些Emu Edit之前没有直接训练过的任务，比如超高分辨率处理或轮廓检测，它也能快速学习并适应。

4、连续编辑和质量保持：Emu Edit 引入了一种方法，以在多轮编辑场景中保持生成图像的质量，通过在每次编辑后应用像素阈值化步骤，减少累积的重建和数值误差。

详细：https://emu-edit.metademolab.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725471464126902606#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725471464126902606#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 11:10:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Emu Edit 是一个指令驱动的图像编辑模型，能够理解和执行各种复杂的编辑指令，同时保证图片质量。<br />
<br />
1、多功能编辑：用户可以用它来进行各种图像编辑任务，比如改变某个区域、自由形式的编辑、甚至是一些复杂的计算机视觉任务，如图像检测和分割。<br />
<br />
2、智能处理：Emu Edit不仅能处理多种任务，还能根据用户的输入智能调整编辑过程。这是通过一种叫做“学习到的任务嵌入”的技术实现的，这技术帮助工具更准确地理解和执行用户的编辑指令。<br />
<br />
3、快速适应新任务：即使是一些Emu Edit之前没有直接训练过的任务，比如超高分辨率处理或轮廓检测，它也能快速学习并适应。<br />
<br />
4、连续编辑和质量保持：Emu Edit 引入了一种方法，以在多轮编辑场景中保持生成图像的质量，通过在每次编辑后应用像素阈值化步骤，减少累积的重建和数值误差。<br />
<br />
详细：<a href="https://emu-edit.metademolab.com/">emu-edit.metademolab.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU0NzEzNzY3MDUwMjgwOTYvcHUvaW1nLzk0elhHcjM4VG1rSVFsYmMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725471127026417912#m</id>
            <title>R to @xiaohuggg: EMU VIDEO：将文本转换为视频的生成模型，通过先生成图像然后再生成视频的方法来改善视频的质量和分辨率。

1、文本到视频的生成：该模型的核心功能是将文本描述转换成视频，即根据给定的文字说明生成对应的视频内容。

2、分步处理：与常规的直接生成视频的方法不同，EMU VIDEO 采用了分步骤的方法。首先根据文本生成静态图像，然后再基于这个图像和原始文本生成视频。

3、质量提升：通过这种分步骤的方法，EMU VIDEO 能够生成更高质量和分辨率的视频，这在人类评估中得到了证实。

4、图像动画：模型特别适用于基于用户文本提示的图像动画生成，即能够将静态图像转换为动态视频。

5、高分辨率视频生成：EMU VIDEO 采用了特别调整的噪声计划和多阶段训练，直接生成高分辨率的视频。

6、质量和文本忠实度：与先前的模型相比，EMU VIDEO 在视频的质量和对原始文本描述的忠实度方面表现更佳。

详细：https://emu-video.metademolab.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725471127026417912#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725471127026417912#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 11:09:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>EMU VIDEO：将文本转换为视频的生成模型，通过先生成图像然后再生成视频的方法来改善视频的质量和分辨率。<br />
<br />
1、文本到视频的生成：该模型的核心功能是将文本描述转换成视频，即根据给定的文字说明生成对应的视频内容。<br />
<br />
2、分步处理：与常规的直接生成视频的方法不同，EMU VIDEO 采用了分步骤的方法。首先根据文本生成静态图像，然后再基于这个图像和原始文本生成视频。<br />
<br />
3、质量提升：通过这种分步骤的方法，EMU VIDEO 能够生成更高质量和分辨率的视频，这在人类评估中得到了证实。<br />
<br />
4、图像动画：模型特别适用于基于用户文本提示的图像动画生成，即能够将静态图像转换为动态视频。<br />
<br />
5、高分辨率视频生成：EMU VIDEO 采用了特别调整的噪声计划和多阶段训练，直接生成高分辨率的视频。<br />
<br />
6、质量和文本忠实度：与先前的模型相比，EMU VIDEO 在视频的质量和对原始文本描述的忠实度方面表现更佳。<br />
<br />
详细：<a href="https://emu-video.metademolab.com/">emu-video.metademolab.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU0NzA5Njc0NzUwODUzMTIvcHUvaW1nL0Fsby1UbE5VOHJqanhHRDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725470552318718046#m</id>
            <title>Meta AI发布了两款新的生成式AI模型：Emu Video和Emu Edit

Emu Video：是一个文本到视频生成模型，基于扩散模型，可以通过文本提示生成图像，然后基于文本和生成视频

Emu Edit：是一个指令驱动的图像编辑模型，可以通过指令进行自由形式编辑，包括局部和全局编辑、背景添加/移除、颜色和几何变换等。

与以往需要多个模型相比，Emu Video只用两个扩散模型就能生成512x512分辨率、四秒长、每秒16帧的视频。人类评估表明，相比于以往的方法，Emu Video在质量和忠实于文本提示方面得到了极高的评价。

Emu Edit的核心在于精确地只修改与编辑请求相关的像素，保持其他像素不变。为了训练这个模型，Meta开发了一个包含1000万合成样本的数据集，每个样本包括一个输入图像、任务描述和目标输出图像。

详细：https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/</title>
            <link>https://nitter.cz/xiaohuggg/status/1725470552318718046#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725470552318718046#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 11:06:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta AI发布了两款新的生成式AI模型：Emu Video和Emu Edit<br />
<br />
Emu Video：是一个文本到视频生成模型，基于扩散模型，可以通过文本提示生成图像，然后基于文本和生成视频<br />
<br />
Emu Edit：是一个指令驱动的图像编辑模型，可以通过指令进行自由形式编辑，包括局部和全局编辑、背景添加/移除、颜色和几何变换等。<br />
<br />
与以往需要多个模型相比，Emu Video只用两个扩散模型就能生成512x512分辨率、四秒长、每秒16帧的视频。人类评估表明，相比于以往的方法，Emu Video在质量和忠实于文本提示方面得到了极高的评价。<br />
<br />
Emu Edit的核心在于精确地只修改与编辑请求相关的像素，保持其他像素不变。为了训练这个模型，Meta开发了一个包含1000万合成样本的数据集，每个样本包括一个输入图像、任务描述和目标输出图像。<br />
<br />
详细：<a href="https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/">ai.meta.com/blog/emu-text-to…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU0NDk1NTE5MDgzNjgzODQvcHUvaW1nL3RFWDlOWEdQMFpSWDFaNUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1725333564470812773#m</id>
            <title>RT by @xiaohuggg: #AI开源项目推荐：screenshot-to-code

借助GPT-4V，直接把屏幕截图生成HTML网页</title>
            <link>https://nitter.cz/dotey/status/1725333564470812773#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1725333564470812773#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 02:02:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：screenshot-to-code<br />
<br />
借助GPT-4V，直接把屏幕截图生成HTML网页</p>
<p><a href="https://nitter.cz/DevDminGod/status/1725175630029803538#m">nitter.cz/DevDminGod/status/1725175630029803538#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1725364184798663107#m</id>
            <title>R to @xiaohuggg: 这个是详细介绍的+演示视频

完全是使用Text-to-speech Avatar制作的

虽然有点瑕疵但是我感觉还是很不错的</title>
            <link>https://nitter.cz/xiaohuggg/status/1725364184798663107#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1725364184798663107#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 04:04:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是详细介绍的+演示视频<br />
<br />
完全是使用Text-to-speech Avatar制作的<br />
<br />
虽然有点瑕疵但是我感觉还是很不错的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjUzNjM4MDc5NDA0MTU0ODgvcHUvaW1nL0ZKWFl0ckVYdXl1OTI1dVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>