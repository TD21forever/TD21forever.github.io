<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726510109902397522#m</id>
            <title>微软首席执行官纳德拉：

OpenAI创始人Sam Altman和Brockman将加入微软。

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1726510109902397522#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726510109902397522#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 07:57:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软首席执行官纳德拉：<br />
<br />
OpenAI创始人Sam Altman和Brockman将加入微软。<br />
<br />
😂</p>
<p><a href="https://nitter.cz/satyanadella/status/1726509045803336122#m">nitter.cz/satyanadella/status/1726509045803336122#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726500331302138094#m</id>
            <title>我想 Sam和 Ilya 之间应该是发生了

这个视频里面的事情了...

我劝你们不要现在乱站队，小心将来拉清单！

🙂</title>
            <link>https://nitter.cz/xiaohuggg/status/1726500331302138094#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726500331302138094#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 07:18:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我想 Sam和 Ilya 之间应该是发生了<br />
<br />
这个视频里面的事情了...<br />
<br />
我劝你们不要现在乱站队，小心将来拉清单！<br />
<br />
🙂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY0OTg3MTY4NzY2OTc2MDAvcHUvaW1nL0JTSFFKWTZ5Q2xuUlRnWUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726478584792186916#m</id>
            <title>OpenAI 董事会已聘请Twitch 的前首席执行官。Emmett Shear 担任OpenAI的首席执行官。🤖</title>
            <link>https://nitter.cz/xiaohuggg/status/1726478584792186916#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726478584792186916#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 05:52:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 董事会已聘请Twitch 的前首席执行官。Emmett Shear 担任OpenAI的首席执行官。🤖</p>
<p><a href="https://nitter.cz/emilychangtv/status/1726468006786859101#m">nitter.cz/emilychangtv/status/1726468006786859101#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726462766553387285#m</id>
            <title>这个挺搞笑的，哈哈哈 我翻译了一下 你们看看

开发者@charliebholtz 利用GPT-4V和ElevenLabs的语音克隆技术模仿大卫·阿滕伯勒的声音（类似我们这的赵忠祥），来实时的解说自己的生活。

它编写了脚本，每五秒从他的摄像头拍摄一张照片，发送给GPT-4V，然后生成阿滕伯勒叙述风格的文本转声音输出。😂

演示中用其标志性的叙述风格描述Holtz，例如他的眼镜、头发和衣服。这种叙述风格模仿了BBC野生动物纪录片的方式。（类似我们这的动物世界😅）

整个过程是这样的：摄像头捕捉图像 → GPT-4 Vision分析图像并生成文本 → 文本通过ElevenLabs的语音合成系统转换成阿滕伯勒的声音。这个过程实现了将视觉信息转换为具有特定声音和风格的语音叙述，创造出一种新颖的互动体验。

这个项目的代码已经公开在GitHub上：https://github.com/cbh123/narrator</title>
            <link>https://nitter.cz/xiaohuggg/status/1726462766553387285#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726462766553387285#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 04:49:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺搞笑的，哈哈哈 我翻译了一下 你们看看<br />
<br />
开发者<a href="https://nitter.cz/charliebholtz" title="Charlie Holtz">@charliebholtz</a> 利用GPT-4V和ElevenLabs的语音克隆技术模仿大卫·阿滕伯勒的声音（类似我们这的赵忠祥），来实时的解说自己的生活。<br />
<br />
它编写了脚本，每五秒从他的摄像头拍摄一张照片，发送给GPT-4V，然后生成阿滕伯勒叙述风格的文本转声音输出。😂<br />
<br />
演示中用其标志性的叙述风格描述Holtz，例如他的眼镜、头发和衣服。这种叙述风格模仿了BBC野生动物纪录片的方式。（类似我们这的动物世界😅）<br />
<br />
整个过程是这样的：摄像头捕捉图像 → GPT-4 Vision分析图像并生成文本 → 文本通过ElevenLabs的语音合成系统转换成阿滕伯勒的声音。这个过程实现了将视觉信息转换为具有特定声音和风格的语音叙述，创造出一种新颖的互动体验。<br />
<br />
这个项目的代码已经公开在GitHub上：<a href="https://github.com/cbh123/narrator">github.com/cbh123/narrator</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY0NDUyNDQ3NTYyNDY1MjgvcHUvaW1nL3hpQ2J4Q1lNdE9UaXp1bjMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726439252316364814#m</id>
            <title>StyleTTS 2：一个开源的媲美 Elevenlabs 的文本转语音工具

🌈 多样化的语音风格：StyleTTS 2能够自动生成多种不同的语音风格，无需依赖特定的参考语音。

🗣 更自然的语音：采用特殊的训练方法，使得生成的语音更加贴近真人的说话方式。

⚡ 高效生成：利用扩散模型技术，高效地生成不同风格的语音。

🎚 精确的语音控制：提供对语音的精确控制，包括语速、语调等方面。

👤 接近真人的语音合成：在测试中，生成的语音质量接近于真人录音。

🔊 适应不同说话者：即使没有特定说话者的样本，也能生成高质量的语音。

工作原原理及特点：

StyleTTS 2利用风格扩散和与大型语音语言模型（SLM）的对抗性训练来实现接近人类水平的TTS合成。

这个模型与其前身不同之处在于，它通过扩散模型将风格建模为一个潜在的随机变量，以生成最适合文本的风格，而不需要参考语音，实现了高效的潜在扩散，同时受益于扩散模型提供的多样化语音合成。

1、非自回归架构：与传统的自回归TTS模型不同，StyleTTS 2采用非自回归架构。它在生成语音时不需要依次预测每个音频样本，而是可以并行生成整个语音序列。这种方法大大提高了语音合成的速度。

2、风格编码器：StyleTTS 2包含一个风格编码器，它能够从参考音频中提取风格特征。这些风格特征包括韵律、语调、语速等，使得生成的语音不仅准确传达文本信息，还能够模仿参考音频的风格和情感。

3、端到端生成：StyleTTS 2实现了端到端的语音生成。它直接从文本和风格向量生成音频波形，而不是先生成梅尔频谱图再转换为音频。这种方法简化了传统TTS系统中的多步骤流程，提高了效率和生成语音的自然度。

4、风格扩散和对抗训练：StyleTTS 2结合了风格扩散和对抗训练技术。风格扩散是指通过风格编码器生成固定长度的风格向量，这些向量能够捕捉到不同的语音风格。对抗训练则是通过生成对抗网络（GAN）来提高语音的自然度和真实感。

5、高质量语音合成：通过这些技术，StyleTTS 2能够生成高质量、自然流畅且具有表现力的语音。它在多个数据集上的性能评估显示，其生成的语音质量接近甚至超过了人类的录音。

6、多样性和灵活性：StyleTTS 2的设计允许它适应不同的语音风格和情感，使其在多种应用场景中都能生成适宜的语音输出。

StyleTTS 2在多个评估结果方面表现出色：

1、高质量语音合成：在多个测试中，StyleTTS 2生成的语音质量非常高，接近或达到了真人录音的水平。这表明了其在模仿人类语音方面的高效能力。

2、比较平均意见得分（CMOS）：在LJSpeech数据集上的评估显示，StyleTTS 2的语音生成质量超过了人类录音，获得了统计上显著的CMOS得分。CMOS是评估语音合成质量的一个重要指标，高CMOS得分意味着更高的语音质量和自然度。

3、多说话者数据集表现：在VCTK数据集上，StyleTTS 2也展现了优异的性能，达到了人类水平。这个数据集包含多个说话者的语音，表明StyleTTS 2能够适应不同说话者的特点，生成多样化且高质量的语音。

4、自然度和表现力：StyleTTS 2不仅在语音的清晰度和准确度上表现优秀，还在自然度和表现力方面取得了显著成果。这意味着生成的语音不仅仅是清晰可懂，还能够传达丰富的情感和语调变化。

StyleTTS 2的评估结果显示了其在文本到语音合成领域的先进性能，特别是在语音质量、自然度和多样性方面。

项目及演示：https://styletts2.github.io/
GitHub：https://github.com/yl4579/StyleTTS2
论文：https://arxiv.org/abs/2306.07691
Colab在线体验：https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/</title>
            <link>https://nitter.cz/xiaohuggg/status/1726439252316364814#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726439252316364814#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 03:16:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>StyleTTS 2：一个开源的媲美 Elevenlabs 的文本转语音工具<br />
<br />
🌈 多样化的语音风格：StyleTTS 2能够自动生成多种不同的语音风格，无需依赖特定的参考语音。<br />
<br />
🗣 更自然的语音：采用特殊的训练方法，使得生成的语音更加贴近真人的说话方式。<br />
<br />
⚡ 高效生成：利用扩散模型技术，高效地生成不同风格的语音。<br />
<br />
🎚 精确的语音控制：提供对语音的精确控制，包括语速、语调等方面。<br />
<br />
👤 接近真人的语音合成：在测试中，生成的语音质量接近于真人录音。<br />
<br />
🔊 适应不同说话者：即使没有特定说话者的样本，也能生成高质量的语音。<br />
<br />
工作原原理及特点：<br />
<br />
StyleTTS 2利用风格扩散和与大型语音语言模型（SLM）的对抗性训练来实现接近人类水平的TTS合成。<br />
<br />
这个模型与其前身不同之处在于，它通过扩散模型将风格建模为一个潜在的随机变量，以生成最适合文本的风格，而不需要参考语音，实现了高效的潜在扩散，同时受益于扩散模型提供的多样化语音合成。<br />
<br />
1、非自回归架构：与传统的自回归TTS模型不同，StyleTTS 2采用非自回归架构。它在生成语音时不需要依次预测每个音频样本，而是可以并行生成整个语音序列。这种方法大大提高了语音合成的速度。<br />
<br />
2、风格编码器：StyleTTS 2包含一个风格编码器，它能够从参考音频中提取风格特征。这些风格特征包括韵律、语调、语速等，使得生成的语音不仅准确传达文本信息，还能够模仿参考音频的风格和情感。<br />
<br />
3、端到端生成：StyleTTS 2实现了端到端的语音生成。它直接从文本和风格向量生成音频波形，而不是先生成梅尔频谱图再转换为音频。这种方法简化了传统TTS系统中的多步骤流程，提高了效率和生成语音的自然度。<br />
<br />
4、风格扩散和对抗训练：StyleTTS 2结合了风格扩散和对抗训练技术。风格扩散是指通过风格编码器生成固定长度的风格向量，这些向量能够捕捉到不同的语音风格。对抗训练则是通过生成对抗网络（GAN）来提高语音的自然度和真实感。<br />
<br />
5、高质量语音合成：通过这些技术，StyleTTS 2能够生成高质量、自然流畅且具有表现力的语音。它在多个数据集上的性能评估显示，其生成的语音质量接近甚至超过了人类的录音。<br />
<br />
6、多样性和灵活性：StyleTTS 2的设计允许它适应不同的语音风格和情感，使其在多种应用场景中都能生成适宜的语音输出。<br />
<br />
StyleTTS 2在多个评估结果方面表现出色：<br />
<br />
1、高质量语音合成：在多个测试中，StyleTTS 2生成的语音质量非常高，接近或达到了真人录音的水平。这表明了其在模仿人类语音方面的高效能力。<br />
<br />
2、比较平均意见得分（CMOS）：在LJSpeech数据集上的评估显示，StyleTTS 2的语音生成质量超过了人类录音，获得了统计上显著的CMOS得分。CMOS是评估语音合成质量的一个重要指标，高CMOS得分意味着更高的语音质量和自然度。<br />
<br />
3、多说话者数据集表现：在VCTK数据集上，StyleTTS 2也展现了优异的性能，达到了人类水平。这个数据集包含多个说话者的语音，表明StyleTTS 2能够适应不同说话者的特点，生成多样化且高质量的语音。<br />
<br />
4、自然度和表现力：StyleTTS 2不仅在语音的清晰度和准确度上表现优秀，还在自然度和表现力方面取得了显著成果。这意味着生成的语音不仅仅是清晰可懂，还能够传达丰富的情感和语调变化。<br />
<br />
StyleTTS 2的评估结果显示了其在文本到语音合成领域的先进性能，特别是在语音质量、自然度和多样性方面。<br />
<br />
项目及演示：<a href="https://styletts2.github.io/">styletts2.github.io/</a><br />
GitHub：<a href="https://github.com/yl4579/StyleTTS2">github.com/yl4579/StyleTTS2</a><br />
论文：<a href="https://arxiv.org/abs/2306.07691">arxiv.org/abs/2306.07691</a><br />
Colab在线体验：<a href="https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/">colab.research.google.com/gi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY0MzA2OTI5MjQ2NDEyODAvcHUvaW1nL21iMUx5Q284Yjkwb1lBMnMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726420710703411675#m</id>
            <title>R to @xiaohuggg: 也可以使用爬取的内容创建Assistant自定义助手

这样你就可以通过一个 API来访问这些生成的知识。

可以将这些知识集成到你自己的产品或应用中去。

简单来说，就是提供了一种方式，让你能够在你的软件或产品中使用这些爬取并整理好的知识。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726420710703411675#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726420710703411675#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 02:02:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以使用爬取的内容创建Assistant自定义助手<br />
<br />
这样你就可以通过一个 API来访问这些生成的知识。<br />
<br />
可以将这些知识集成到你自己的产品或应用中去。<br />
<br />
简单来说，就是提供了一种方式，让你能够在你的软件或产品中使用这些爬取并整理好的知识。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9WNVhwamIwQUFFb0JRLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfVjVYcGpiMEFBRW9CUS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726420708002029783#m</id>
            <title>R to @xiaohuggg: 🌐 爬取网站内容：用户通过配置文件设置目标网址和选择器，GPT-Crawler 自动从这些网站上收集信息。

📁 生成知识文件：爬取的内容被整理成 JSON 文件，这个文件包含了从网站上获取的所有知识。

🤖 创建自定义 GPT：利用这个知识文件，用户可以在 OpenAI 平台上创建自己定制的 GPT 聊天机器人。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726420708002029783#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726420708002029783#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 02:02:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🌐 爬取网站内容：用户通过配置文件设置目标网址和选择器，GPT-Crawler 自动从这些网站上收集信息。<br />
<br />
📁 生成知识文件：爬取的内容被整理成 JSON 文件，这个文件包含了从网站上获取的所有知识。<br />
<br />
🤖 创建自定义 GPT：利用这个知识文件，用户可以在 OpenAI 平台上创建自己定制的 GPT 聊天机器人。</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9WNGxRR2FZQUE0NFR3LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfVjRsUUdhWUFBNDRUdy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726420705368273215#m</id>
            <title>GPT-Crawler ：一个开源的知识库自动爬虫工具

它能从一个或多个网址爬取网站内容，然后生成JSON文件格式。

这样爬取的内容可以直接导入到GPTs知识库中，方便你创建自定义知识库的GPTs。

比如你有自己的网站或者资料库，但是整理起来太麻烦，就可以使用这个工具。

GitHub：https://github.com/BuilderIO/gpt-crawler</title>
            <link>https://nitter.cz/xiaohuggg/status/1726420705368273215#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726420705368273215#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 02:02:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT-Crawler ：一个开源的知识库自动爬虫工具<br />
<br />
它能从一个或多个网址爬取网站内容，然后生成JSON文件格式。<br />
<br />
这样爬取的内容可以直接导入到GPTs知识库中，方便你创建自定义知识库的GPTs。<br />
<br />
比如你有自己的网站或者资料库，但是整理起来太麻烦，就可以使用这个工具。<br />
<br />
GitHub：<a href="https://github.com/BuilderIO/gpt-crawler">github.com/BuilderIO/gpt-cra…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9WNFlNaWFvQUFkM0syLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfVjRZTWlhb0FBZDNLMi5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726243969368215722#m</id>
            <title>以前的美丽万种风情

现在的漂亮千篇一律

哪一个是你当年的心中女神？😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1726243969368215722#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726243969368215722#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 14:20:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>以前的美丽万种风情<br />
<br />
现在的漂亮千篇一律<br />
<br />
哪一个是你当年的心中女神？😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI2MjQzODE0MjQ2MDgwNTEyL2ltZy9JbW56LV9QRlVyeTViZVRoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726214623739904109#m</id>
            <title>R to @xiaohuggg: 还可以自己输入Prompt进行即时画图

每打一个字母、一个单词

左边就实时展现，魔幻😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1726214623739904109#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726214623739904109#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 12:23:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还可以自己输入Prompt进行即时画图<br />
<br />
每打一个字母、一个单词<br />
<br />
左边就实时展现，魔幻😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYyMTQxNzA5MDE4OTcyMTYvcHUvaW1nL1ZSbTJEQk9BSUt2WEs1TWEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726214119077085693#m</id>
            <title>LCM即时绘画，在线体验

LCM LoRA能实现即时绘图即时生成的功能，可以说是所见所得。

@ilumine_ai 制作了一个 @huggingface 在线体验地址：https://huggingface.co/spaces/ilumine-AI/LCM-Painter

你可以体验它的伟大之处。真的很牛P！

由于内置了很多Prompt，随便瞎画都可以，体验很流畅。

LCM LoRA由清华大学@SimianLuo 开发。

LCM LoRA旨在加速稳定扩散模型（Stable Diffusion）的运行速度。通过使用LCM LoRA，可以将推理步骤减少到仅2至8步，而不是传统的25至50步。

在一台高性能的3090显卡上，运行SDXL模型只需要大约1秒钟。甚至在Mac电脑上，速度也提高了10倍。

LCM LoRA可以用于文本到图像（Text-to-Image）的转换。例如，它可以与稳定扩散的基础模型结合使用，并通过改变调度器（scheduler）来减少推理步骤。

此外，LCM LoRA还支持图像修复（inpainting）和与其他LoRA模型结合使用，以生成风格化的图像。

模型下载：https://huggingface.co/latent-consistency/lcm-lora-sdxl

LCM LoRA详细介绍：https://huggingface.co/blog/lcm_lora

论文：https://arxiv.org/pdf/2311.05556.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1726214119077085693#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726214119077085693#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 12:21:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LCM即时绘画，在线体验<br />
<br />
LCM LoRA能实现即时绘图即时生成的功能，可以说是所见所得。<br />
<br />
<a href="https://nitter.cz/ilumine_ai" title="ilumine AI">@ilumine_ai</a> 制作了一个 <a href="https://nitter.cz/huggingface" title="Hugging Face">@huggingface</a> 在线体验地址：<a href="https://huggingface.co/spaces/ilumine-AI/LCM-Painter">huggingface.co/spaces/ilumin…</a><br />
<br />
你可以体验它的伟大之处。真的很牛P！<br />
<br />
由于内置了很多Prompt，随便瞎画都可以，体验很流畅。<br />
<br />
LCM LoRA由清华大学<a href="https://nitter.cz/SimianLuo" title="Allen Luo">@SimianLuo</a> 开发。<br />
<br />
LCM LoRA旨在加速稳定扩散模型（Stable Diffusion）的运行速度。通过使用LCM LoRA，可以将推理步骤减少到仅2至8步，而不是传统的25至50步。<br />
<br />
在一台高性能的3090显卡上，运行SDXL模型只需要大约1秒钟。甚至在Mac电脑上，速度也提高了10倍。<br />
<br />
LCM LoRA可以用于文本到图像（Text-to-Image）的转换。例如，它可以与稳定扩散的基础模型结合使用，并通过改变调度器（scheduler）来减少推理步骤。<br />
<br />
此外，LCM LoRA还支持图像修复（inpainting）和与其他LoRA模型结合使用，以生成风格化的图像。<br />
<br />
模型下载：<a href="https://huggingface.co/latent-consistency/lcm-lora-sdxl">huggingface.co/latent-consis…</a><br />
<br />
LCM LoRA详细介绍：<a href="https://huggingface.co/blog/lcm_lora">huggingface.co/blog/lcm_lora</a><br />
<br />
论文：<a href="https://arxiv.org/pdf/2311.05556.pdf">arxiv.org/pdf/2311.05556.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYxOTk1NTU5NDYxMTkxNjgvcHUvaW1nL1lKLXRUSGh5OFRMUlNDWUguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726198779727257956#m</id>
            <title>VM Pill：一个可吞咽装置 在你体内追踪你的生命体征

追踪生命体征：这个设备可以像药丸一样吞下，然后在体内追踪呼吸和心率等关键生命体征。

这对于需要持续监测的患者，如慢性病患者或容易发生阿片类药物过量的人，非常有用。

高精度监测：研究人员给西弗吉尼亚大学的 10 名睡眠呼吸暂停患者服用了 VM 药丸。

该设备可检测患者呼吸何时停止并监测其呼吸频率，准确率高达 92.7%。与外部设备相比，该药丸可以监测心率，准确度至少为 96%。试验还表明该设备是安全的，所有患者在实验后的几天内都通过了该设备。

便捷的健康监测：根据研究的首席作者、麻省理工学院机械工程副教授兼布莱根妇女医院的胃肠病学家 Giovanni Traverso 的说法，这个设备可以在不需要医院访问的情况下帮助诊断和监测多种健康状况，从而使医疗保健更加方便和支持性。
这种可吞咽的设备大约和多种维生素药丸一样大，内含传感器，能够在通过消化系统时测量生命体征。它由生物相容材料制成，设计上能够安全通过人体，不造成任何伤害。该设备通过无线方式将收集到的数据传输到智能手机或电脑，实现实时监测。
在一项人体试验中，健康志愿者吞咽了这个设备，然后进行了各种体力活动，如在跑步机上跑步或睡觉。该设备准确地检测到了志愿者的心率和呼吸率，即使在剧烈运动期间也是如此。该设备还能检测到患者的呼吸停止，并以 92.7% 的准确率监测他们的呼吸率。
研究人员认为，这个设备对于有阿片类药物过量风险的患者特别有益，因为它可以连续监测他们的呼吸和心率，以便及时发现任何不适或过量的迹象。这可能允许早期干预，从而挽救生命。

此外，这个设备还可以用于其他医疗应用，如监测慢性病患者、追踪药物效果或评估整体健康和福祉。它消除了侵入性程序或持续医院访问的需要，使医疗保健对患者来说更加方便和可及。

Traverso 表示，当前版本的 VM Pill 只能在体内停留大约一天，但他们正在努力改进该设备，使其能够在体内停留更长时间以进行长期监测。他们还希望升级该设备，以便一旦检测到症状，它就可以自动输送药物，以扭转阿片类药物过量等情况。

详细：https://interestingengineering.com/health/swallowable-device-tracking-vital-signs-inside-the-body-in-human-trial
论文：https://www.cell.com/device/fulltext/S2666-9986(23)00184-9</title>
            <link>https://nitter.cz/xiaohuggg/status/1726198779727257956#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726198779727257956#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 11:20:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VM Pill：一个可吞咽装置 在你体内追踪你的生命体征<br />
<br />
追踪生命体征：这个设备可以像药丸一样吞下，然后在体内追踪呼吸和心率等关键生命体征。<br />
<br />
这对于需要持续监测的患者，如慢性病患者或容易发生阿片类药物过量的人，非常有用。<br />
<br />
高精度监测：研究人员给西弗吉尼亚大学的 10 名睡眠呼吸暂停患者服用了 VM 药丸。<br />
<br />
该设备可检测患者呼吸何时停止并监测其呼吸频率，准确率高达 92.7%。与外部设备相比，该药丸可以监测心率，准确度至少为 96%。试验还表明该设备是安全的，所有患者在实验后的几天内都通过了该设备。<br />
<br />
便捷的健康监测：根据研究的首席作者、麻省理工学院机械工程副教授兼布莱根妇女医院的胃肠病学家 Giovanni Traverso 的说法，这个设备可以在不需要医院访问的情况下帮助诊断和监测多种健康状况，从而使医疗保健更加方便和支持性。<br />
这种可吞咽的设备大约和多种维生素药丸一样大，内含传感器，能够在通过消化系统时测量生命体征。它由生物相容材料制成，设计上能够安全通过人体，不造成任何伤害。该设备通过无线方式将收集到的数据传输到智能手机或电脑，实现实时监测。<br />
在一项人体试验中，健康志愿者吞咽了这个设备，然后进行了各种体力活动，如在跑步机上跑步或睡觉。该设备准确地检测到了志愿者的心率和呼吸率，即使在剧烈运动期间也是如此。该设备还能检测到患者的呼吸停止，并以 92.7% 的准确率监测他们的呼吸率。<br />
研究人员认为，这个设备对于有阿片类药物过量风险的患者特别有益，因为它可以连续监测他们的呼吸和心率，以便及时发现任何不适或过量的迹象。这可能允许早期干预，从而挽救生命。<br />
<br />
此外，这个设备还可以用于其他医疗应用，如监测慢性病患者、追踪药物效果或评估整体健康和福祉。它消除了侵入性程序或持续医院访问的需要，使医疗保健对患者来说更加方便和可及。<br />
<br />
Traverso 表示，当前版本的 VM Pill 只能在体内停留大约一天，但他们正在努力改进该设备，使其能够在体内停留更长时间以进行长期监测。他们还希望升级该设备，以便一旦检测到症状，它就可以自动输送药物，以扭转阿片类药物过量等情况。<br />
<br />
详细：<a href="https://interestingengineering.com/health/swallowable-device-tracking-vital-signs-inside-the-body-in-human-trial">interestingengineering.com/h…</a><br />
论文：<a href="https://www.cell.com/device/fulltext/S2666-9986(23)00184-9">cell.com/device/fulltext/S26…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TdGZWX2E0QUFCOVBvLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TdGZWNGJvQUFjRzZ2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TdGZWN2JZQUFEMk4wLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726172579071922338#m</id>
            <title>WPS就使用用户文档进行AI训练道歉 称是误会🙃

近期，有用户发现WPS在更新其隐私政策后提到，会将用户主动上传的文档用于训练人工智能。

这意味着在使用WPS的过程中，用户的隐私可能在不知不觉中被“窃取”，因为用户已经同意了隐私政策。

作为一款办公软件，WPS将用户文档用于训练人工智能引起了争议，因为文档可能涉及大量机密信息，被窃取可能会造成重大问题。

 • WPS官方对此回应称，之前版本的表述为用户造成了困扰。他们已经更新了隐私政策，去除了容易引起误解的表述，并确保其内容与实际操作严格对应。

 • WPS官方声明，所有用户文档不会被用于任何AI训练目的，也不会在未经用户同意的情况下用于任何场景。他们重申，始终严格遵守所有可适用的用户隐私保护法律和标准。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726172579071922338#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726172579071922338#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 09:36:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WPS就使用用户文档进行AI训练道歉 称是误会🙃<br />
<br />
近期，有用户发现WPS在更新其隐私政策后提到，会将用户主动上传的文档用于训练人工智能。<br />
<br />
这意味着在使用WPS的过程中，用户的隐私可能在不知不觉中被“窃取”，因为用户已经同意了隐私政策。<br />
<br />
作为一款办公软件，WPS将用户文档用于训练人工智能引起了争议，因为文档可能涉及大量机密信息，被窃取可能会造成重大问题。<br />
<br />
 • WPS官方对此回应称，之前版本的表述为用户造成了困扰。他们已经更新了隐私政策，去除了容易引起误解的表述，并确保其内容与实际操作严格对应。<br />
<br />
 • WPS官方声明，所有用户文档不会被用于任何AI训练目的，也不会在未经用户同意的情况下用于任何场景。他们重申，始终严格遵守所有可适用的用户隐私保护法律和标准。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9TWXpxcWJBQUF5V0RGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726132734098276861#m</id>
            <title>Insanely Fast Whisper：极速音频转录工具，能在98 秒的时间内转录 300 分钟（5 小时）的音频。

是一个基于 OpenAI 的 Whisper Large v3 模型的改进项目，旨在实现极快速度的音频转录。这个项目通过使用 🤗 Transformers、Optimum 和 flash-attn 技术，使得转录速度大幅提升。

提供了一个简单的命令行界面，用户可以通过几个简单的命令在电脑上使用这个工具来转录音频。

除了基本的 Whisper 模型，还可以选择其他版本，比如 distil-whisper，以适应不同的需求。

该项目目前只支持 Nvidia GPU。
提供了不同的配置选项，以最大化转录吞吐量。

GitHub：https://github.com/chenxwh/insanely-fast-whisper

在线演示：https://replicate.com/vaibhavs10/incredibly-fast-whisper

视频：测试几次来看16分钟的录音，大概是14秒到27秒估计是需要强大GPU才能实现宣称的效果！</title>
            <link>https://nitter.cz/xiaohuggg/status/1726132734098276861#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726132734098276861#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 06:58:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Insanely Fast Whisper：极速音频转录工具，能在98 秒的时间内转录 300 分钟（5 小时）的音频。<br />
<br />
是一个基于 OpenAI 的 Whisper Large v3 模型的改进项目，旨在实现极快速度的音频转录。这个项目通过使用 🤗 Transformers、Optimum 和 flash-attn 技术，使得转录速度大幅提升。<br />
<br />
提供了一个简单的命令行界面，用户可以通过几个简单的命令在电脑上使用这个工具来转录音频。<br />
<br />
除了基本的 Whisper 模型，还可以选择其他版本，比如 distil-whisper，以适应不同的需求。<br />
<br />
该项目目前只支持 Nvidia GPU。<br />
提供了不同的配置选项，以最大化转录吞吐量。<br />
<br />
GitHub：<a href="https://github.com/chenxwh/insanely-fast-whisper">github.com/chenxwh/insanely-…</a><br />
<br />
在线演示：<a href="https://replicate.com/vaibhavs10/incredibly-fast-whisper">replicate.com/vaibhavs10/inc…</a><br />
<br />
视频：测试几次来看16分钟的录音，大概是14秒到27秒估计是需要强大GPU才能实现宣称的效果！</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYxMTcwNjcyOTkxNTE4NzIvcHUvaW1nL2dHMGxZVHNzamhiZm9NWjMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726112197448024114#m</id>
            <title>OpenStax ：免费提供各种学科的K12和大学教科书资源

OpenStax 是Rice University的一部分，他们提供了多个学科领域的教科书，这些学科包括商业、人文学科、数学、科学和社会科学等。

资源包括讲座幻灯片、实验室手册、题库等。

可以通过网络浏览、下载 PDF 文件和打印等多种选项轻松访问教材。

OpenStax 的教科书和资源经过专业人士编写和同行评审，确保质量和准确性。

访问：https://openstax.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1726112197448024114#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726112197448024114#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 05:36:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenStax ：免费提供各种学科的K12和大学教科书资源<br />
<br />
OpenStax 是Rice University的一部分，他们提供了多个学科领域的教科书，这些学科包括商业、人文学科、数学、科学和社会科学等。<br />
<br />
资源包括讲座幻灯片、实验室手册、题库等。<br />
<br />
可以通过网络浏览、下载 PDF 文件和打印等多种选项轻松访问教材。<br />
<br />
OpenStax 的教科书和资源经过专业人士编写和同行评审，确保质量和准确性。<br />
<br />
访问：<a href="https://openstax.org/">openstax.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9SaG5iVmFjQUFfeFgzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726077207318339639#m</id>
            <title>R to @xiaohuggg: 他还说："现在正在发生的技术变革，将彻底改变我们生活方式、经济和社会结构以及其他可能性的限制....

这在OpenAI的历史上有四次 ，而最近一次是在过去几周内。

我有幸在【拨开无知的面纱和探索未知的边界】时在场，

能够做到这一点是我职业生涯中的荣誉。"</title>
            <link>https://nitter.cz/xiaohuggg/status/1726077207318339639#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726077207318339639#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 03:17:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他还说："现在正在发生的技术变革，将彻底改变我们生活方式、经济和社会结构以及其他可能性的限制....<br />
<br />
这在OpenAI的历史上有四次 ，而最近一次是在过去几周内。<br />
<br />
我有幸在【拨开无知的面纱和探索未知的边界】时在场，<br />
<br />
能够做到这一点是我职业生涯中的荣誉。"</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYwNzYxMzAxMjQ1MzM3NjAvcHUvaW1nL0dFTEtHZGd6TGl2Z3F1MlkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726076059769335909#m</id>
            <title>在Sam 奥特曼被解雇的前一天，他在APEC的一次会议上，他暗示了OpenAI已经开发出比现在GPT 4更加强大，让人无法想象，远远超出人们期待的东西...

这可能是被解雇的导火索，导致Ilya等董事会成员认为他隐瞒了很多信息，没有和董事会分享。

Sam 称：“模型的能力将会有一个没人预料到的飞跃。[这将会]与人们预期不同。将会是惊人的！”

这似乎在暗示OpenAI内部可能已经实现了技术和模型上的巨大飞跃。而Ilya 等董事会成员可能因此感到恐慌或者不知情，所以采取了行动。认为奥特曼在隐瞒一些技术进展，没有分享信息。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726076059769335909#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726076059769335909#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 03:12:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在Sam 奥特曼被解雇的前一天，他在APEC的一次会议上，他暗示了OpenAI已经开发出比现在GPT 4更加强大，让人无法想象，远远超出人们期待的东西...<br />
<br />
这可能是被解雇的导火索，导致Ilya等董事会成员认为他隐瞒了很多信息，没有和董事会分享。<br />
<br />
Sam 称：“模型的能力将会有一个没人预料到的飞跃。[这将会]与人们预期不同。将会是惊人的！”<br />
<br />
这似乎在暗示OpenAI内部可能已经实现了技术和模型上的巨大飞跃。而Ilya 等董事会成员可能因此感到恐慌或者不知情，所以采取了行动。认为奥特曼在隐瞒一些技术进展，没有分享信息。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjYwNjA4NjQzMTc1NTQ2ODgvcHUvaW1nL2IxVUJaMUhqYnJSVjZGSVYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726070163332530361#m</id>
            <title>你还别说

自从奥特曼被解雇后

GPT稳定和快了很多</title>
            <link>https://nitter.cz/xiaohuggg/status/1726070163332530361#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726070163332530361#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Nov 2023 02:49:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>你还别说<br />
<br />
自从奥特曼被解雇后<br />
<br />
GPT稳定和快了很多</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>