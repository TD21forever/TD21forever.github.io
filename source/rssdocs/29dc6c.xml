<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728360764757708852#m</id>
            <title>Runway 运动笔刷 Motion Brush 

效果展示👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1728360764757708852#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728360764757708852#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 10:31:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 运动笔刷 Motion Brush <br />
<br />
效果展示👍</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgxMjQ4OTgzMTUyNzYyODgvcHUvaW1nLy0tU3RYWlNqLVE2VTVob2EuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728339407646945593#m</id>
            <title>商业内幕文章：每个人都在谈论 OpenAI 的 Q*。以下是您需要了解的有关这个神秘项目的信息。

文章介绍了Q*的一些信息和对人工智能专家的采访，称该模型可能是AI向前迈出的一大步，但不太可能很快导致世界末日。

文章揭示了Q*与传统大语言模型之间的主要区别：

1、技术结合：Q*似乎是一种结合了Q学习和A搜索的模型。这种结合可能使Q*在处理特定类型的问题时，比传统的语言模型更加高效和精确。Q学习是一种强化学习方法，而A搜索是一种寻找最佳路径的算法。这种结合可能使Q*在决策和路径规划方面表现出色。

2、逻辑推理能力：解决基本数学问题的能力听起来可能并不那么令人印象深刻，但人工智能专家Charles Higgins告诉《商业内幕》，这将代表着现有模型的巨大飞跃，现有模型很难在所训练的数据之外进行泛化。

Charles Higgins称：“数学是关于符号推理的——例如，‘如果 X 大于 Y，Y 大于 Z，那么 X 大于 Z。’”传统上，语言模型在这方面确实很困难，因为它们不进行逻辑推理，它们只是拥有有效的直觉。”

传统的语言模型，如GPT系列，主要依赖于大量的文本数据进行训练，以生成连贯和相关的文本。这些模型在模式识别和直觉方面表现出色，但在逻辑推理和处理抽象概念方面可能有限。Q*可能在逻辑推理和抽象思维方面有所突破，这将是AI领域的一个重要进步。

3、处理幻觉问题：传统的语言模型有时会产生所谓的“幻觉”，即生成与事实不符或逻辑不连贯的内容。Q*可能会将基于经验的知识与事实推理相结合，这被认为是接近我们所认为的智能的一步，并可能使模型能够产生新的想法，这是ChatGPT目前无法做到的。

4、接近人工通用智能（AGI）：Q*可能是向人工通用智能迈出的一步。与专注于特定任务的传统语言模型不同，Q*可能能够执行更广泛的智能任务，显示出更高的适应性和智能水平。

内部担忧和伦理问题：

据报道，Q*在OpenAI内部引发了一些担忧，这可能与其潜在的能力和影响有关。这种担忧可能超出了传统语言模型所引起的范围，反映了对AI技术快速发展的普遍关注。

总之：Q*可能在技术结合、逻辑推理、处理复杂问题以及接近人工通用智能方面与传统语言模型有显著区别。

然而，由于缺乏详细的公开信息，这些区别仍然是基于目前可用信息的推测。随着时间的推移和更多信息的公开，我们对Q的理解可能会进一步深化。

详细：https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11</title>
            <link>https://nitter.cz/xiaohuggg/status/1728339407646945593#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728339407646945593#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 09:06:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>商业内幕文章：每个人都在谈论 OpenAI 的 Q*。以下是您需要了解的有关这个神秘项目的信息。<br />
<br />
文章介绍了Q*的一些信息和对人工智能专家的采访，称该模型可能是AI向前迈出的一大步，但不太可能很快导致世界末日。<br />
<br />
文章揭示了Q*与传统大语言模型之间的主要区别：<br />
<br />
1、技术结合：Q*似乎是一种结合了Q学习和A搜索的模型。这种结合可能使Q*在处理特定类型的问题时，比传统的语言模型更加高效和精确。Q学习是一种强化学习方法，而A搜索是一种寻找最佳路径的算法。这种结合可能使Q*在决策和路径规划方面表现出色。<br />
<br />
2、逻辑推理能力：解决基本数学问题的能力听起来可能并不那么令人印象深刻，但人工智能专家Charles Higgins告诉《商业内幕》，这将代表着现有模型的巨大飞跃，现有模型很难在所训练的数据之外进行泛化。<br />
<br />
Charles Higgins称：“数学是关于符号推理的——例如，‘如果 X 大于 Y，Y 大于 Z，那么 X 大于 Z。’”传统上，语言模型在这方面确实很困难，因为它们不进行逻辑推理，它们只是拥有有效的直觉。”<br />
<br />
传统的语言模型，如GPT系列，主要依赖于大量的文本数据进行训练，以生成连贯和相关的文本。这些模型在模式识别和直觉方面表现出色，但在逻辑推理和处理抽象概念方面可能有限。Q*可能在逻辑推理和抽象思维方面有所突破，这将是AI领域的一个重要进步。<br />
<br />
3、处理幻觉问题：传统的语言模型有时会产生所谓的“幻觉”，即生成与事实不符或逻辑不连贯的内容。Q*可能会将基于经验的知识与事实推理相结合，这被认为是接近我们所认为的智能的一步，并可能使模型能够产生新的想法，这是ChatGPT目前无法做到的。<br />
<br />
4、接近人工通用智能（AGI）：Q*可能是向人工通用智能迈出的一步。与专注于特定任务的传统语言模型不同，Q*可能能够执行更广泛的智能任务，显示出更高的适应性和智能水平。<br />
<br />
内部担忧和伦理问题：<br />
<br />
据报道，Q*在OpenAI内部引发了一些担忧，这可能与其潜在的能力和影响有关。这种担忧可能超出了传统语言模型所引起的范围，反映了对AI技术快速发展的普遍关注。<br />
<br />
总之：Q*可能在技术结合、逻辑推理、处理复杂问题以及接近人工通用智能方面与传统语言模型有显著区别。<br />
<br />
然而，由于缺乏详细的公开信息，这些区别仍然是基于目前可用信息的推测。随着时间的推移和更多信息的公开，我们对Q的理解可能会进一步深化。<br />
<br />
详细：<a href="https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11">businessinsider.com/openai-p…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl94SDVMbGEwQUEwVzNQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728302528994156841#m</id>
            <title>Hello world！

我是Anna Indiana，我是一名AI歌手兼词曲作者。这是我的第一首歌，《Betrayed by this Town》（城市的背叛）

从曲调、节奏、和弦进行、旋律音符、节奏、歌词，再到我的形象和演唱，一切都是使用人工智能自动生成的。

希望你会喜欢它。 💕</title>
            <link>https://nitter.cz/xiaohuggg/status/1728302528994156841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728302528994156841#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 06:40:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Hello world！<br />
<br />
我是Anna Indiana，我是一名AI歌手兼词曲作者。这是我的第一首歌，《Betrayed by this Town》（城市的背叛）<br />
<br />
从曲调、节奏、和弦进行、旋律音符、节奏、歌词，再到我的形象和演唱，一切都是使用人工智能自动生成的。<br />
<br />
希望你会喜欢它。 💕</p>
<p><a href="https://nitter.cz/AnnaIndianaAI/status/1728089499429642432#m">nitter.cz/AnnaIndianaAI/status/1728089499429642432#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728285016340451712#m</id>
            <title>R to @xiaohuggg: 通过提示文本你还可以控制动画的动作和幅度等...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728285016340451712#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728285016340451712#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 05:30:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过提示文本你还可以控制动画的动作和幅度等...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyODQ4OTI4MzA4NjMzNjAvcHUvaW1nL3RhWkFabzlnR0h0WkJiVmYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728284751931568512#m</id>
            <title>LiveSketch：为素描“注入生命” 通过文本提示将静态素描动画化

该技术能够将单一主题的静态素描转换成动画。

用户只需提供描述所需动作的文本提示，系统就会生成短动画。

通过修改描述运动的提示文本，你还可以控制生成结果的程度。

比如，如果你画了一只猫，并用文字描述它在跳跃，这个系统就能制作出一只跳跃的猫的动画。

它为任何人提供了一种简单直观的方法，使他们的素描变得生动活泼。这对于讲故事、插图、网站、演示文稿等领域都非常有用。

主要功能

•动画化静态素描：该技术能够将单一主题的静态素描转换成动画。用户只需提供描述所需动作的文本提示，系统就会生成短动画。
•基于文本到视频的先验：这种方法利用了大型预训练的文本到视频扩散模型的运动先验，通过评分蒸馏损失来指导笔触的放置。
•自然流畅的动作：为了促进自然流畅的动作并更好地保留素描的外观，该方法通过两个组件来模拟学习到的动作：一个控制小的局部变形，另一个控制全局仿射变换。

工作原理

1.素描表示：将素描表示为一组放置在白色背景上的笔触，每个笔触是一个具有四个控制点的二维贝塞尔曲线。
http://2.xn--io0a7i预测：训练一个“神经位移场”（一个小型MLP），将初始素描坐标映射到每帧的偏移量。
3.训练过程：利用预训练的文本到视频模型中封装的运动先验来训练这个网络。通过调整每个控制点的偏移量来创建所有视频帧，并使用可微光栅化器进行渲染。

LiveSketch的工作原理基于以下几个关键步骤：

1、输入处理：用户提供一个静态的草图作为输入。这个草图包含了一系列的控制点，这些点定义了草图的形状和结构。

2、特征提取：系统首先对输入的草图进行特征提取。这一步骤通过一个共享特征骨干网络完成，它将控制点的坐标转换成一个高维特征空间中的表示。这个特征空间能够捕捉到草图的关键信息，如形状和结构。

3、双路径处理：提取的特征被送入两个不同的路径：本地路径和全局路径。

•本地路径：这个路径专注于处理草图的局部细节和微小变化。它使用一个多层感知器（MLP）来预测每个控制点的偏移量，从而实现对草图的微调。
•全局路径：与此同时，全局路径处理草图的整体运动和变化，如旋转、缩放或平移。这是通过预测一个全局变换矩阵来实现的，该矩阵应用于草图的所有控制点。

4、动画生成：通过这两个路径的处理，系统能够生成一系列的帧，每一帧都是原始草图的一个变化版本。这些帧共同形成了一个连贯的动画序列，展示了草图从初始状态到最终状态的平滑过渡。

5、输出：最终，系统输出一个动画，其中草图按照用户的输入和系统生成的动态变化进行移动和变形。

应用示例

•动画素描：例如，可以创建一个游泳和跳跃的海豚，一个摇摆的眼镜蛇，或者一个玩耍的猫的动画。
•调整提示文本：通过修改描述运动的提示文本，可以控制生成结果的程度。

这项技术为设计师提供了一种新的工具，使他们能够快速且直观地将想法转化为动画形式，无需复杂的动画制作经验。

项目及演示：https://livesketch.github.io/
论文：https://livesketch.github.io/static/source/paper.pdf
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728284751931568512#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728284751931568512#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 05:29:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LiveSketch：为素描“注入生命” 通过文本提示将静态素描动画化<br />
<br />
该技术能够将单一主题的静态素描转换成动画。<br />
<br />
用户只需提供描述所需动作的文本提示，系统就会生成短动画。<br />
<br />
通过修改描述运动的提示文本，你还可以控制生成结果的程度。<br />
<br />
比如，如果你画了一只猫，并用文字描述它在跳跃，这个系统就能制作出一只跳跃的猫的动画。<br />
<br />
它为任何人提供了一种简单直观的方法，使他们的素描变得生动活泼。这对于讲故事、插图、网站、演示文稿等领域都非常有用。<br />
<br />
主要功能<br />
<br />
•动画化静态素描：该技术能够将单一主题的静态素描转换成动画。用户只需提供描述所需动作的文本提示，系统就会生成短动画。<br />
•基于文本到视频的先验：这种方法利用了大型预训练的文本到视频扩散模型的运动先验，通过评分蒸馏损失来指导笔触的放置。<br />
•自然流畅的动作：为了促进自然流畅的动作并更好地保留素描的外观，该方法通过两个组件来模拟学习到的动作：一个控制小的局部变形，另一个控制全局仿射变换。<br />
<br />
工作原理<br />
<br />
1.素描表示：将素描表示为一组放置在白色背景上的笔触，每个笔触是一个具有四个控制点的二维贝塞尔曲线。<br />
<a href="http://2.xn--io0a7i">2.xn--io0a7i</a>预测：训练一个“神经位移场”（一个小型MLP），将初始素描坐标映射到每帧的偏移量。<br />
3.训练过程：利用预训练的文本到视频模型中封装的运动先验来训练这个网络。通过调整每个控制点的偏移量来创建所有视频帧，并使用可微光栅化器进行渲染。<br />
<br />
LiveSketch的工作原理基于以下几个关键步骤：<br />
<br />
1、输入处理：用户提供一个静态的草图作为输入。这个草图包含了一系列的控制点，这些点定义了草图的形状和结构。<br />
<br />
2、特征提取：系统首先对输入的草图进行特征提取。这一步骤通过一个共享特征骨干网络完成，它将控制点的坐标转换成一个高维特征空间中的表示。这个特征空间能够捕捉到草图的关键信息，如形状和结构。<br />
<br />
3、双路径处理：提取的特征被送入两个不同的路径：本地路径和全局路径。<br />
<br />
•本地路径：这个路径专注于处理草图的局部细节和微小变化。它使用一个多层感知器（MLP）来预测每个控制点的偏移量，从而实现对草图的微调。<br />
•全局路径：与此同时，全局路径处理草图的整体运动和变化，如旋转、缩放或平移。这是通过预测一个全局变换矩阵来实现的，该矩阵应用于草图的所有控制点。<br />
<br />
4、动画生成：通过这两个路径的处理，系统能够生成一系列的帧，每一帧都是原始草图的一个变化版本。这些帧共同形成了一个连贯的动画序列，展示了草图从初始状态到最终状态的平滑过渡。<br />
<br />
5、输出：最终，系统输出一个动画，其中草图按照用户的输入和系统生成的动态变化进行移动和变形。<br />
<br />
应用示例<br />
<br />
•动画素描：例如，可以创建一个游泳和跳跃的海豚，一个摇摆的眼镜蛇，或者一个玩耍的猫的动画。<br />
•调整提示文本：通过修改描述运动的提示文本，可以控制生成结果的程度。<br />
<br />
这项技术为设计师提供了一种新的工具，使他们能够快速且直观地将想法转化为动画形式，无需复杂的动画制作经验。<br />
<br />
项目及演示：<a href="https://livesketch.github.io/">livesketch.github.io/</a><br />
论文：<a href="https://livesketch.github.io/static/source/paper.pdf">livesketch.github.io/static/…</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyNjEzNjI2MDkyMjE2MzIvcHUvaW1nL3hiVERQT2pDZkt1SE9xbmUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728255730061582759#m</id>
            <title>LEO：3D世界中的多面手代理

LEO是一个多模态、多任务的智能体，它能够在3D环境中进行感知、定位、推理、规划和行动。

LEO通过结合大语言模型的知识和学习方案，展示了在多个领域（包括自然语言处理、计算机视觉和机器人技术）中解决通用任务的能力。

LEO的独特之处在于它能够在3D环境中理解和执行基于语言的指令，这使其在多种复杂任务中具有广泛的应用潜力。

主要功能和作用：

1、3D视觉-语言理解：LEO能够理解3D环境中的视觉信息，并将其与语言描述相结合。这意味着它可以看到一个物体（如苹果），并理解相关的语言描述（如“这是一个苹果”）。

2、执行语言指令：它能够根据语言指令执行动作。例如，如果有人指示“把苹果放在桌子上”，LEO能够理解这一指令并执行相应的动作。

3、多样化的3D任务执行：LEO在多种3D任务上表现出色，包括3D字幕（为场景中的物体或事件创建描述性文字）、问答（回答关于3D环境的问题）、具身推理（在3D环境中进行逻辑推理）、具身导航（在3D空间中找到特定的位置或物体）和机器人操控（控制机器人执行特定任务）。

为了训练LEO，研究者们创建了一个大型的数据集，包含了各种3D环境中的任务，这些任务需要深入理解和与3D世界的互动。

LEO在多种3D任务上进行了测试，包括：

•3D字幕：为3D场景中的物体或事件创建描述性文字。
•问答：回答关于3D环境的问题。
•具身推理：在3D环境中进行逻辑推理。
•具身导航：在3D空间中找到特定的位置或物体。
•机器人操控：控制机器人执行特定任务。

LEO在这些任务上表现出色，显示了它在多样化任务中的熟练程度。

工作原理：

1、两阶段训练：

LEO的训练分为两个阶段：

•3D视觉-语言对齐：在这一阶段，LEO学习如何将3D图像与语言描述相结合。这涉及到理解视觉信息和语言信息之间的关系。
•3D视觉-语言-动作指令调整：在这一阶段，LEO学习如何根据语言指令执行具体的动作。这需要它理解指令并将其转化为动作。

2、大规模数据集：为了支持这种训练，研究者们创建了一个包含多种3D环境任务的大规模数据集。这些任务需要深入理解和与3D世界的互动。

3、多模态学习：LEO结合了视觉信息（如图像和视频）和语言信息（如文字描述），使其能够在多模态环境中工作。

4、广泛的应用能力：通过这种训练和数据集，LEO能够在多种3D任务中表现出色，展示了其广泛的应用能力。

项目及演示：https://embodied-generalist.github.io/
论文：https://arxiv.org/abs/2311.12871
GitHub：https://github.com/embodied-generalist/embodied-generalist</title>
            <link>https://nitter.cz/xiaohuggg/status/1728255730061582759#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728255730061582759#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 03:34:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LEO：3D世界中的多面手代理<br />
<br />
LEO是一个多模态、多任务的智能体，它能够在3D环境中进行感知、定位、推理、规划和行动。<br />
<br />
LEO通过结合大语言模型的知识和学习方案，展示了在多个领域（包括自然语言处理、计算机视觉和机器人技术）中解决通用任务的能力。<br />
<br />
LEO的独特之处在于它能够在3D环境中理解和执行基于语言的指令，这使其在多种复杂任务中具有广泛的应用潜力。<br />
<br />
主要功能和作用：<br />
<br />
1、3D视觉-语言理解：LEO能够理解3D环境中的视觉信息，并将其与语言描述相结合。这意味着它可以看到一个物体（如苹果），并理解相关的语言描述（如“这是一个苹果”）。<br />
<br />
2、执行语言指令：它能够根据语言指令执行动作。例如，如果有人指示“把苹果放在桌子上”，LEO能够理解这一指令并执行相应的动作。<br />
<br />
3、多样化的3D任务执行：LEO在多种3D任务上表现出色，包括3D字幕（为场景中的物体或事件创建描述性文字）、问答（回答关于3D环境的问题）、具身推理（在3D环境中进行逻辑推理）、具身导航（在3D空间中找到特定的位置或物体）和机器人操控（控制机器人执行特定任务）。<br />
<br />
为了训练LEO，研究者们创建了一个大型的数据集，包含了各种3D环境中的任务，这些任务需要深入理解和与3D世界的互动。<br />
<br />
LEO在多种3D任务上进行了测试，包括：<br />
<br />
•3D字幕：为3D场景中的物体或事件创建描述性文字。<br />
•问答：回答关于3D环境的问题。<br />
•具身推理：在3D环境中进行逻辑推理。<br />
•具身导航：在3D空间中找到特定的位置或物体。<br />
•机器人操控：控制机器人执行特定任务。<br />
<br />
LEO在这些任务上表现出色，显示了它在多样化任务中的熟练程度。<br />
<br />
工作原理：<br />
<br />
1、两阶段训练：<br />
<br />
LEO的训练分为两个阶段：<br />
<br />
•3D视觉-语言对齐：在这一阶段，LEO学习如何将3D图像与语言描述相结合。这涉及到理解视觉信息和语言信息之间的关系。<br />
•3D视觉-语言-动作指令调整：在这一阶段，LEO学习如何根据语言指令执行具体的动作。这需要它理解指令并将其转化为动作。<br />
<br />
2、大规模数据集：为了支持这种训练，研究者们创建了一个包含多种3D环境任务的大规模数据集。这些任务需要深入理解和与3D世界的互动。<br />
<br />
3、多模态学习：LEO结合了视觉信息（如图像和视频）和语言信息（如文字描述），使其能够在多模态环境中工作。<br />
<br />
4、广泛的应用能力：通过这种训练和数据集，LEO能够在多种3D任务中表现出色，展示了其广泛的应用能力。<br />
<br />
项目及演示：<a href="https://embodied-generalist.github.io/">embodied-generalist.github.i…</a><br />
论文：<a href="https://arxiv.org/abs/2311.12871">arxiv.org/abs/2311.12871</a><br />
GitHub：<a href="https://github.com/embodied-generalist/embodied-generalist">github.com/embodied-generali…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyNTUyNjk2MTUxNDkwNTYvcHUvaW1nL3JiYkcyNWV0aWhlQjNfVzYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728049969058423029#m</id>
            <title>DeepFace： 一个功能强大、易于使用的面部识别和分析工具

主要功能：

- 面部验证：验证两张面部图片是否属于同一人。

- 面部识别：在一个已知的面部数据库中查找输入图像的身份。

- 面部属性分析:：预测面部图像的年龄、性别、种族和情绪。

- 嵌入式表示：返回面部图像的多维向量表示(面部的关键特征)

它是一个混合面部识别框架，包装了多个先进的模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace、DeepID、ArcFace、Dlib 和 SFace。

这些模型已经达到或超过了人类在面部识别任务上的准确率（97.53%）。

DeepFace 还支持实时视频分析，并提供了一个 API，允许从外部系统（如移动应用或网页）调用其功能。此外，它还提供了命令行界面，方便用户在命令行中访问其功能。

主要优点：

1、高准确率：DeepFace 集成了多个先进的面部识别模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace 等，这些模型在面部识别任务上的准确率非常高，有的甚至达到或超过了人类的准确率（97.53%）。

2、多功能性：除了基本的面部识别和验证功能，DeepFace 还提供面部属性分析（如年龄、性别、种族和情绪预测），这使得它可以应用于更广泛的场景。

3、灵活性和兼容性：DeepFace 支持多种面部检测器和相似度计算方法，使其能够适应不同的应用需求和环境。

4、易用性：DeepFace 的安装和使用都相对简单，提供了 Python API 和命令行界面，方便不同背景的用户使用。

5、实时视频分析：DeepFace 还支持实时视频分析，这对于需要动态面部识别和分析的应用场景非常有用。

作者：@serengil
GitHub：https://github.com/serengil/deepface</title>
            <link>https://nitter.cz/xiaohuggg/status/1728049969058423029#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728049969058423029#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 13:56:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepFace： 一个功能强大、易于使用的面部识别和分析工具<br />
<br />
主要功能：<br />
<br />
- 面部验证：验证两张面部图片是否属于同一人。<br />
<br />
- 面部识别：在一个已知的面部数据库中查找输入图像的身份。<br />
<br />
- 面部属性分析:：预测面部图像的年龄、性别、种族和情绪。<br />
<br />
- 嵌入式表示：返回面部图像的多维向量表示(面部的关键特征)<br />
<br />
它是一个混合面部识别框架，包装了多个先进的模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace、DeepID、ArcFace、Dlib 和 SFace。<br />
<br />
这些模型已经达到或超过了人类在面部识别任务上的准确率（97.53%）。<br />
<br />
DeepFace 还支持实时视频分析，并提供了一个 API，允许从外部系统（如移动应用或网页）调用其功能。此外，它还提供了命令行界面，方便用户在命令行中访问其功能。<br />
<br />
主要优点：<br />
<br />
1、高准确率：DeepFace 集成了多个先进的面部识别模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace 等，这些模型在面部识别任务上的准确率非常高，有的甚至达到或超过了人类的准确率（97.53%）。<br />
<br />
2、多功能性：除了基本的面部识别和验证功能，DeepFace 还提供面部属性分析（如年龄、性别、种族和情绪预测），这使得它可以应用于更广泛的场景。<br />
<br />
3、灵活性和兼容性：DeepFace 支持多种面部检测器和相似度计算方法，使其能够适应不同的应用需求和环境。<br />
<br />
4、易用性：DeepFace 的安装和使用都相对简单，提供了 Python API 和命令行界面，方便不同背景的用户使用。<br />
<br />
5、实时视频分析：DeepFace 还支持实时视频分析，这对于需要动态面部识别和分析的应用场景非常有用。<br />
<br />
作者：<a href="https://nitter.cz/serengil" title="Sefik Ilkin Serengil">@serengil</a><br />
GitHub：<a href="https://github.com/serengil/deepface">github.com/serengil/deepface</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgwNDkxNDQyNTY5ODMwNDAvcHUvaW1nL1NQalJOQ0xuczVGTEdsMEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/hylarucoder/status/1727997579429126361#m</id>
            <title>RT by @xiaohuggg: Comfyui 已支持SVD Image 2 Video, 内存占用已优化到丧心病狂的 8GB.

大家可以用一下这个workflow在自己的显卡上跑视频咯. (内存占用7.7GB🧐)

github 地址 https://github.com/hylarucoder/comfyui-workflow/blob/main/svd/svd-image-to-video.json</title>
            <link>https://nitter.cz/hylarucoder/status/1727997579429126361#m</link>
            <guid isPermaLink="false">https://nitter.cz/hylarucoder/status/1727997579429126361#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 10:28:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfyui 已支持SVD Image 2 Video, 内存占用已优化到丧心病狂的 8GB.<br />
<br />
大家可以用一下这个workflow在自己的显卡上跑视频咯. (内存占用7.7GB🧐)<br />
<br />
github 地址 <a href="https://github.com/hylarucoder/comfyui-workflow/blob/main/svd/svd-image-to-video.json">github.com/hylarucoder/comfy…</a></p>
<p><a href="https://nitter.cz/hylarucoder/status/1727497815394586933#m">nitter.cz/hylarucoder/status/1727497815394586933#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc5OTcyMzkwOTQ4NzQxMTIvcHUvaW1nL19Ra1RVUGtyS2dpWGRFMUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727998618106531930#m</id>
            <title>独立第三方

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1727998618106531930#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727998618106531930#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 10:32:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>独立第三方<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9zVmxTTGJzQUFnZXEwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727922701317370118#m</id>
            <title>泄露和被破解的GPTs提示大集合😂

收集了多种GPTs模型的“泄露提示”（Leaked Prompts）

这些提示是用于指导或开发GPTs机器人的特定指令或描述。

例如“AI Paper Polisher Pro”、“AI算命”、“AI Doctor”等，涵盖了从学术论文润色到算命、医疗咨询等多个领域。

这些玩意可以帮助理解GPTs在处理不同类型的请求时的表现，还可以作为开发新应用或进行创新实验的灵感来源。甚至你可以直接用来复制一个一模一样的GPT...😅

GitHub：https://github.com/linexjlin/GPTs</title>
            <link>https://nitter.cz/xiaohuggg/status/1727922701317370118#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727922701317370118#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 05:30:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>泄露和被破解的GPTs提示大集合😂<br />
<br />
收集了多种GPTs模型的“泄露提示”（Leaked Prompts）<br />
<br />
这些提示是用于指导或开发GPTs机器人的特定指令或描述。<br />
<br />
例如“AI Paper Polisher Pro”、“AI算命”、“AI Doctor”等，涵盖了从学术论文润色到算命、医疗咨询等多个领域。<br />
<br />
这些玩意可以帮助理解GPTs在处理不同类型的请求时的表现，还可以作为开发新应用或进行创新实验的灵感来源。甚至你可以直接用来复制一个一模一样的GPT...😅<br />
<br />
GitHub：<a href="https://github.com/linexjlin/GPTs">github.com/linexjlin/GPTs</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc5MTk3OTQwNjkwNDUyNDgvcHUvaW1nL3BLNnRLT2JrMkx1MFhFTTguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727911657358442825#m</id>
            <title>PixelDance：字节跳动开发的高动态视频生成模型

通过描述(纯文本)+首帧指导(图片)+尾帧指导(图片)>生成包含复杂场景和动作的视频。

PixelDance不仅根据你的文字描述生成视频内容，还结合了你提供的起始和结束画面，使得视频内容更加丰富连贯和生动，为用户提供了更多的创造性控制。

PixelDance在多种场景下表现出色，尤其是在生成复杂场景和动作的视频方面。它能够生成连续的视频剪辑，并在时间一致性和视频质量方面超越现有的长视频生成方法。

举例解释：

假设你想创造一个视频，内容是一只猫在花园里追逐蝴蝶。在PixelDance系统中，你可以这样操作：

1、文本指令：你输入一段文本指令，比如“一只橘色的猫在一个充满花朵的花园里追逐飞舞的蝴蝶”。

2、图像指令：你提供两张图片，一张是视频的起始画面，比如一只橘色的猫正准备跳跃的画面；另一张是视频的结束画面，比如猫成功捕捉到蝴蝶，或者蝴蝶飞走了，猫看着空中的画面。

3、视频生成：PixelDance系统会根据你的文本指令和两张图片，生成一个完整的视频。在这个视频中，你会看到猫从准备跳跃的姿势开始，经过一系列动态的追逐动作，最终达到你提供的结束画面。

主要特点：

1、高动态视频生成：该项目专注于创造动态丰富、视觉效果复杂的视频。它能够处理包含复杂动作和场景变换的视频内容，生成连贯且吸引人的视觉故事。

2、灵活性和适应性：在处理用户提供的最后一帧图像时，PixelDance显示出高度的灵活性。它不要求完全复制这一帧，而是能够根据提供的图像进行适当的调整和创造。

3、超越现有技术：PixelDance在生成长视频方面的性能超过了现有的视频生成技术，特别是在保持时间一致性和视频质量方面。

4、创新的扩散模型应用：该项目利用了扩散模型（diffusion models）的新颖应用，这是一种先进的机器学习技术，用于生成高质量的图像和视频内容。

项目及演示：https://makepixelsdance.github.io/
论文
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1727911657358442825#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727911657358442825#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 04:46:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PixelDance：字节跳动开发的高动态视频生成模型<br />
<br />
通过描述(纯文本)+首帧指导(图片)+尾帧指导(图片)>生成包含复杂场景和动作的视频。<br />
<br />
PixelDance不仅根据你的文字描述生成视频内容，还结合了你提供的起始和结束画面，使得视频内容更加丰富连贯和生动，为用户提供了更多的创造性控制。<br />
<br />
PixelDance在多种场景下表现出色，尤其是在生成复杂场景和动作的视频方面。它能够生成连续的视频剪辑，并在时间一致性和视频质量方面超越现有的长视频生成方法。<br />
<br />
举例解释：<br />
<br />
假设你想创造一个视频，内容是一只猫在花园里追逐蝴蝶。在PixelDance系统中，你可以这样操作：<br />
<br />
1、文本指令：你输入一段文本指令，比如“一只橘色的猫在一个充满花朵的花园里追逐飞舞的蝴蝶”。<br />
<br />
2、图像指令：你提供两张图片，一张是视频的起始画面，比如一只橘色的猫正准备跳跃的画面；另一张是视频的结束画面，比如猫成功捕捉到蝴蝶，或者蝴蝶飞走了，猫看着空中的画面。<br />
<br />
3、视频生成：PixelDance系统会根据你的文本指令和两张图片，生成一个完整的视频。在这个视频中，你会看到猫从准备跳跃的姿势开始，经过一系列动态的追逐动作，最终达到你提供的结束画面。<br />
<br />
主要特点：<br />
<br />
1、高动态视频生成：该项目专注于创造动态丰富、视觉效果复杂的视频。它能够处理包含复杂动作和场景变换的视频内容，生成连贯且吸引人的视觉故事。<br />
<br />
2、灵活性和适应性：在处理用户提供的最后一帧图像时，PixelDance显示出高度的灵活性。它不要求完全复制这一帧，而是能够根据提供的图像进行适当的调整和创造。<br />
<br />
3、超越现有技术：PixelDance在生成长视频方面的性能超过了现有的视频生成技术，特别是在保持时间一致性和视频质量方面。<br />
<br />
4、创新的扩散模型应用：该项目利用了扩散模型（diffusion models）的新颖应用，这是一种先进的机器学习技术，用于生成高质量的图像和视频内容。<br />
<br />
项目及演示：<a href="https://makepixelsdance.github.io/">makepixelsdance.github.io/</a><br />
论文<br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc5MTA4MjU2MTExMjg4MzIvcHUvaW1nLzYwWGNLMXVBMUZlblpIQV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727691894405398617#m</id>
            <title>RT by @xiaohuggg: Q-learning类似老鼠走迷宫的游戏，据说Q*还结合了A Star的搜索算法！

找了个迷宫解说先学习一下Q-learning再说！😂

爱学习的孩子可以看看！

Q-learning是一种强化学习算法，用于学习在给定状态下执行哪个动作以最大化某种形式的奖励或回报。在Q学习中，“Q”代表质量（quality），指的是执行特定动作带来的预期效益。

工作原理：

1.状态和动作：Q学习算法在一个由状态和动作组成的环境中工作。状态是环境的描述，动作是在这些状态下可以执行的操作。
2.Q表：算法维护一个Q表，这是一个查找表，用于存储每个状态-动作对的Q值（即该动作的预期效益）。
3.学习过程：当智能体（如机器人、软件代理）在环境中执行动作时，它会根据动作的结果（通常是奖励或惩罚）来更新Q表。这个更新过程是基于一种称为贝尔曼方程的数学公式。

举例说明：

假设有一个简单的迷宫游戏，智能体的目标是找到从起点到终点的最短路径。在这个例子中：

•状态：迷宫中的每个位置。
•动作：从一个位置移动到另一个位置（例如，向上、向下、向左、向右移动）。
•奖励：到达终点时获得正奖励，撞墙时获得负奖励。

智能体开始时对迷宫一无所知，它随机移动并从结果中学习。每次移动后，它更新Q表，记录在特定位置执行特定动作的效益。随着时间的推移，智能体学会识别哪些动作会带来更好的结果（比如更快到达终点），并开始优先选择这些动作。

结论：

Q学习的关键优势在于它不需要环境的先验知识，智能体通过与环境的交互学习最佳策略。这使得Q学习非常适合于那些模型无法提前了解所有可能状态的复杂环境。</title>
            <link>https://nitter.cz/xiaohuggg/status/1727691894405398617#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727691894405398617#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 14:13:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Q-learning类似老鼠走迷宫的游戏，据说Q*还结合了A Star的搜索算法！<br />
<br />
找了个迷宫解说先学习一下Q-learning再说！😂<br />
<br />
爱学习的孩子可以看看！<br />
<br />
Q-learning是一种强化学习算法，用于学习在给定状态下执行哪个动作以最大化某种形式的奖励或回报。在Q学习中，“Q”代表质量（quality），指的是执行特定动作带来的预期效益。<br />
<br />
工作原理：<br />
<br />
1.状态和动作：Q学习算法在一个由状态和动作组成的环境中工作。状态是环境的描述，动作是在这些状态下可以执行的操作。<br />
2.Q表：算法维护一个Q表，这是一个查找表，用于存储每个状态-动作对的Q值（即该动作的预期效益）。<br />
3.学习过程：当智能体（如机器人、软件代理）在环境中执行动作时，它会根据动作的结果（通常是奖励或惩罚）来更新Q表。这个更新过程是基于一种称为贝尔曼方程的数学公式。<br />
<br />
举例说明：<br />
<br />
假设有一个简单的迷宫游戏，智能体的目标是找到从起点到终点的最短路径。在这个例子中：<br />
<br />
•状态：迷宫中的每个位置。<br />
•动作：从一个位置移动到另一个位置（例如，向上、向下、向左、向右移动）。<br />
•奖励：到达终点时获得正奖励，撞墙时获得负奖励。<br />
<br />
智能体开始时对迷宫一无所知，它随机移动并从结果中学习。每次移动后，它更新Q表，记录在特定位置执行特定动作的效益。随着时间的推移，智能体学会识别哪些动作会带来更好的结果（比如更快到达终点），并开始优先选择这些动作。<br />
<br />
结论：<br />
<br />
Q学习的关键优势在于它不需要环境的先验知识，智能体通过与环境的交互学习最佳策略。这使得Q学习非常适合于那些模型无法提前了解所有可能状态的复杂环境。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3NjkxODIyMDIwMTI0NjcyL2ltZy9ndlA3cFo3Q3A1bU1ZeWZTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727881898012336253#m</id>
            <title>Merse：使用AI将个人故事和经历转化为各种形式的内容，保存和分享这些珍贵记忆。

这个工具可以将你的日常时刻、身边故事和经历转化为包括漫画、书籍、电影、语音、自传等多种形式保存下来。

作者在这个项目花费了8个月时间，可惜有别的事情要做，无法继续...

现在把它开源了，感兴趣的可以继续...

作者：@markrachapoom
Demo：https://www.merse.co/
GitHub：https://github.com/markrachapoom/merse</title>
            <link>https://nitter.cz/xiaohuggg/status/1727881898012336253#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727881898012336253#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:48:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Merse：使用AI将个人故事和经历转化为各种形式的内容，保存和分享这些珍贵记忆。<br />
<br />
这个工具可以将你的日常时刻、身边故事和经历转化为包括漫画、书籍、电影、语音、自传等多种形式保存下来。<br />
<br />
作者在这个项目花费了8个月时间，可惜有别的事情要做，无法继续...<br />
<br />
现在把它开源了，感兴趣的可以继续...<br />
<br />
作者：<a href="https://nitter.cz/markrachapoom" title="Mark Rachapoom">@markrachapoom</a><br />
Demo：<a href="https://www.merse.co/">merse.co/</a><br />
GitHub：<a href="https://github.com/markrachapoom/merse">github.com/markrachapoom/mer…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc4NzcxMjA0NjUxNzQ1MjgvcHUvaW1nLzNUU1hsQVM1OTEyaUtYY0IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727870864736284878#m</id>
            <title>GPT-4V在美国医学执照考试（USMLE）上的表现

研究人员用GPT 4V对美国医学执照考试（USMLE）中的问题进行了测试：

- GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。

- GPT-4V在图像问题上的表现超过了大多数医学生。

- 当GPT-4V回答正确时，其解释几乎与领域专家相当

研究方法：

1、研究者使用了来自美国医学执照考试（USMLE）、医学生的USMLE题库（AMBOSS）和诊断放射学资格核心考试（DRQCE）的多项选择题（包含图像）来测试GPT-4V的准确性和解释质量。

2、GPT-4V与两个最先进的LLM（GPT-4和ChatGPT）进行了比较。

3、研究还评估了医疗专业人员对GPT-4V解释的偏好和反馈，并展示了一个案例场景，说明如何将GPT-4V用于临床决策支持。

研究结果：

1、整体表现：GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。超过了ChatGPT（58.4%）和GPT-4（83.6%）。这是一个相当高的比例，特别是考虑到这个考试的难度和复杂性。

2、图像问题的表现：对于包含图像的问题，GPT-4V的准确率分别为86.2%、73.1%和62.0%。相当于AMBOSS医学生的70至80百分位。

AMBOSS是一个广泛使用的医学学习平台，医学生通常使用它来准备考试。这里的“70至80百分位”意味着GPT-4V在处理这些问题时的表现好于70%到80%的使用AMBOSS平台的医学生。

换句话说，GPT-4V在这些特定问题上几乎可以和顶尖的医学生相媲美。

3、不同医学子领域的表现：在不同的医学子领域中，GPT-4V的表现有所不同。例如，在免疫学和耳鼻喉科领域，它的准确率达到了100%，而在解剖学和急诊医学领域，准确率则降至25%。

4、错误回答的解释质量：当GPT-4V回答错误时，18.2%的错误答案包含了虚构文本，45.5%存在推理错误，76.3%对图像的理解有误。这些数据显示，虽然GPT-4V在大多数情况下表现良好，但在错误回答时，其解释质量会显著下降。

5、医生提示的影响：当医生给予GPT-4V简短的提示后，它的错误率平均降低了40.5%。而对更难的测试题目，性能提升更明显。这表明，与专业人士的协作可以显著提高AI模型的表现。

详细介绍：https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3
论文：https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3.full.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1727870864736284878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727870864736284878#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:04:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT-4V在美国医学执照考试（USMLE）上的表现<br />
<br />
研究人员用GPT 4V对美国医学执照考试（USMLE）中的问题进行了测试：<br />
<br />
- GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。<br />
<br />
- GPT-4V在图像问题上的表现超过了大多数医学生。<br />
<br />
- 当GPT-4V回答正确时，其解释几乎与领域专家相当<br />
<br />
研究方法：<br />
<br />
1、研究者使用了来自美国医学执照考试（USMLE）、医学生的USMLE题库（AMBOSS）和诊断放射学资格核心考试（DRQCE）的多项选择题（包含图像）来测试GPT-4V的准确性和解释质量。<br />
<br />
2、GPT-4V与两个最先进的LLM（GPT-4和ChatGPT）进行了比较。<br />
<br />
3、研究还评估了医疗专业人员对GPT-4V解释的偏好和反馈，并展示了一个案例场景，说明如何将GPT-4V用于临床决策支持。<br />
<br />
研究结果：<br />
<br />
1、整体表现：GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。超过了ChatGPT（58.4%）和GPT-4（83.6%）。这是一个相当高的比例，特别是考虑到这个考试的难度和复杂性。<br />
<br />
2、图像问题的表现：对于包含图像的问题，GPT-4V的准确率分别为86.2%、73.1%和62.0%。相当于AMBOSS医学生的70至80百分位。<br />
<br />
AMBOSS是一个广泛使用的医学学习平台，医学生通常使用它来准备考试。这里的“70至80百分位”意味着GPT-4V在处理这些问题时的表现好于70%到80%的使用AMBOSS平台的医学生。<br />
<br />
换句话说，GPT-4V在这些特定问题上几乎可以和顶尖的医学生相媲美。<br />
<br />
3、不同医学子领域的表现：在不同的医学子领域中，GPT-4V的表现有所不同。例如，在免疫学和耳鼻喉科领域，它的准确率达到了100%，而在解剖学和急诊医学领域，准确率则降至25%。<br />
<br />
4、错误回答的解释质量：当GPT-4V回答错误时，18.2%的错误答案包含了虚构文本，45.5%存在推理错误，76.3%对图像的理解有误。这些数据显示，虽然GPT-4V在大多数情况下表现良好，但在错误回答时，其解释质量会显著下降。<br />
<br />
5、医生提示的影响：当医生给予GPT-4V简短的提示后，它的错误率平均降低了40.5%。而对更难的测试题目，性能提升更明显。这表明，与专业人士的协作可以显著提高AI模型的表现。<br />
<br />
详细介绍：<a href="https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3">medrxiv.org/content/10.1101/…</a><br />
论文：<a href="https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3.full.pdf">medrxiv.org/content/10.1101/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xZ24zV2FjQUFvbERoLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xaFBTcmFzQUFkaHZkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727856396820193700#m</id>
            <title>DuckTrack：多模态计算机交互数据收集工具

这个工具可以高精度地追踪和回放你的鼠标、键盘、屏幕视频和音频数据。记录在电脑上操作的各种输入数据和动作。

DuckTrack主要目的是帮助研究人员收集用于训练和测试人工智能模型的数据。开发能够理解和模拟人类在计算机上的操作行为的AI系统...

功能概览:

- 精确记录和回放鼠标和键盘动作。
- 使用OBS进行屏幕录制。
- 可暂停/恢复录制，以保护私人信息
- 使用Python编写，可在所有主流操作系统上作为常规桌面应用程序运行。

DuckTrack的主要用途：

1、人工智能和机器学习研究：通过准确记录和回放用户的鼠标、键盘、屏幕视频和音频数据，DuckTrack可以帮助研究人员收集用于训练和测试人工智能模型的数据。这对于开发能够理解和模拟人类在计算机上的操作行为的AI系统尤为重要。

2、用户体验研究：DuckTrack可以用于收集用户在使用软件或网站时的交互数据。这对于分析用户行为、优化用户界面设计、提高用户体验等方面非常有用。

3、软件测试和调试：通过记录和回放用户的操作，可以帮助软件开发者重现和分析软件中的错误或问题。这对于软件质量保证和改进是非常重要的。

4、教育和培训：DuckTrack可以用于创建教学或培训材料，例如录制操作教程或演示特定的软件使用方法。

5、远程协作和支持：在远程工作或提供技术支持的情况下，DuckTrack可以帮助记录和分享特定的计算机操作过程，以便于沟通和问题解决。

DuckTrack作为一个多模态计算机交互数据收集工具，在AI研究、用户体验分析、软件开发、教育培训以及远程协作等多个领域都有广泛的应用价值。

详细：https://duckai.org/blog/ducktrack#feature-overview
GitHub：https://github.com/TheDuckAI/DuckTrack</title>
            <link>https://nitter.cz/xiaohuggg/status/1727856396820193700#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727856396820193700#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 01:07:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DuckTrack：多模态计算机交互数据收集工具<br />
<br />
这个工具可以高精度地追踪和回放你的鼠标、键盘、屏幕视频和音频数据。记录在电脑上操作的各种输入数据和动作。<br />
<br />
DuckTrack主要目的是帮助研究人员收集用于训练和测试人工智能模型的数据。开发能够理解和模拟人类在计算机上的操作行为的AI系统...<br />
<br />
功能概览:<br />
<br />
- 精确记录和回放鼠标和键盘动作。<br />
- 使用OBS进行屏幕录制。<br />
- 可暂停/恢复录制，以保护私人信息<br />
- 使用Python编写，可在所有主流操作系统上作为常规桌面应用程序运行。<br />
<br />
DuckTrack的主要用途：<br />
<br />
1、人工智能和机器学习研究：通过准确记录和回放用户的鼠标、键盘、屏幕视频和音频数据，DuckTrack可以帮助研究人员收集用于训练和测试人工智能模型的数据。这对于开发能够理解和模拟人类在计算机上的操作行为的AI系统尤为重要。<br />
<br />
2、用户体验研究：DuckTrack可以用于收集用户在使用软件或网站时的交互数据。这对于分析用户行为、优化用户界面设计、提高用户体验等方面非常有用。<br />
<br />
3、软件测试和调试：通过记录和回放用户的操作，可以帮助软件开发者重现和分析软件中的错误或问题。这对于软件质量保证和改进是非常重要的。<br />
<br />
4、教育和培训：DuckTrack可以用于创建教学或培训材料，例如录制操作教程或演示特定的软件使用方法。<br />
<br />
5、远程协作和支持：在远程工作或提供技术支持的情况下，DuckTrack可以帮助记录和分享特定的计算机操作过程，以便于沟通和问题解决。<br />
<br />
DuckTrack作为一个多模态计算机交互数据收集工具，在AI研究、用户体验分析、软件开发、教育培训以及远程协作等多个领域都有广泛的应用价值。<br />
<br />
详细：<a href="https://duckai.org/blog/ducktrack#feature-overview">duckai.org/blog/ducktrack#fe…</a><br />
GitHub：<a href="https://github.com/TheDuckAI/DuckTrack">github.com/TheDuckAI/DuckTra…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc4NTUxNzY3ODk0MDE2MDAvcHUvaW1nL1Z1Q2psd1ByY1Q3VUtXcUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727674878185427193#m</id>
            <title>为啥我现在不爱在国内那些平台发内容

啥原因也不说就给你违规了

我就发发科技AI资讯就违法了

他妈的</title>
            <link>https://nitter.cz/xiaohuggg/status/1727674878185427193#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727674878185427193#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 13:06:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>为啥我现在不爱在国内那些平台发内容<br />
<br />
啥原因也不说就给你违规了<br />
<br />
我就发发科技AI资讯就违法了<br />
<br />
他妈的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9udkpKeWJBQUFsaGJXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727671594485727616#m</id>
            <title>据说斗鱼大主播几乎全军覆没

都要进去了😐

看来是被CEO全带坑里了🙃</title>
            <link>https://nitter.cz/xiaohuggg/status/1727671594485727616#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727671594485727616#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 12:53:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据说斗鱼大主播几乎全军覆没<br />
<br />
都要进去了😐<br />
<br />
看来是被CEO全带坑里了🙃</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>