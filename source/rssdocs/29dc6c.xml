<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751293428896579719#m</id>
            <title>切糕刺客🥷

刀刀要命…

周末快乐…哈哈哈哈😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1751293428896579719#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751293428896579719#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 17:17:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>切糕刺客🥷<br />
<br />
刀刀要命…<br />
<br />
周末快乐…哈哈哈哈😂</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUxMjkyNTI0OTA2NTU3NDQwL2ltZy9EdmlPWGVfeC1QRDUwNjRYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751202779501384052#m</id>
            <title>Gabriele Romagnoli @GabRoXR  分享使用 @ShapesXR 工具为 #AppleVisionPro 进行空间计算创新设计，打破2D屏幕局限性！

你可以灵活地在虚拟现实（VR）和混合现实（MR）这两种模式中无缝切换。

让你根据设计的需要来调整自己的视角和与对象的互动方式！

• 如果你需要对某个设计元素进行细微的调整或操作，你可以选择“缩小”视角，这样就能更精确地控制和操纵这个对象。

• 相反，如果你想要从更广阔的视角来观察你设计的整个场景或对象（比如一个玩具房子），你可以选择“放大”视角，这样就能获得一个宏观的视图。

使用 @ShapesXR 工具为 #VisionPro 设计的流程：

1、导入与同步：将你在 Figma 中创建的资产和组件导入到 ShapesXR，开始在混合现实中设计，无需3D设计技能。

2、原型化互动：利用 Quest 2 或 3 的头部姿势或 Quest Pro 的眼球追踪技术，原型化注视和捏合等互动。

3、无缝切换：在虚拟现实 (VR) 和混合现实 (MR) 之间无缝切换，根据需要调整视角大小，精确操纵对象或从宏观角度审视设计。

这一流程突破了传统2D设计的限制，为设计师提供了一个直观、灵活的工作环境，以勇敢、大胆的方式进行空间设计。</title>
            <link>https://nitter.cz/xiaohuggg/status/1751202779501384052#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751202779501384052#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 11:17:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gabriele Romagnoli <a href="https://nitter.cz/GabRoXR" title="Gabriele Romagnoli">@GabRoXR</a>  分享使用 <a href="https://nitter.cz/ShapesXR" title="ShapesXR">@ShapesXR</a> 工具为 <a href="https://nitter.cz/search?q=%23AppleVisionPro">#AppleVisionPro</a> 进行空间计算创新设计，打破2D屏幕局限性！<br />
<br />
你可以灵活地在虚拟现实（VR）和混合现实（MR）这两种模式中无缝切换。<br />
<br />
让你根据设计的需要来调整自己的视角和与对象的互动方式！<br />
<br />
• 如果你需要对某个设计元素进行细微的调整或操作，你可以选择“缩小”视角，这样就能更精确地控制和操纵这个对象。<br />
<br />
• 相反，如果你想要从更广阔的视角来观察你设计的整个场景或对象（比如一个玩具房子），你可以选择“放大”视角，这样就能获得一个宏观的视图。<br />
<br />
使用 <a href="https://nitter.cz/ShapesXR" title="ShapesXR">@ShapesXR</a> 工具为 <a href="https://nitter.cz/search?q=%23VisionPro">#VisionPro</a> 设计的流程：<br />
<br />
1、导入与同步：将你在 Figma 中创建的资产和组件导入到 ShapesXR，开始在混合现实中设计，无需3D设计技能。<br />
<br />
2、原型化互动：利用 Quest 2 或 3 的头部姿势或 Quest Pro 的眼球追踪技术，原型化注视和捏合等互动。<br />
<br />
3、无缝切换：在虚拟现实 (VR) 和混合现实 (MR) 之间无缝切换，根据需要调整视角大小，精确操纵对象或从宏观角度审视设计。<br />
<br />
这一流程突破了传统2D设计的限制，为设计师提供了一个直观、灵活的工作环境，以勇敢、大胆的方式进行空间设计。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUxMjAyNjQyOTk3NzIzMTM2L2ltZy90Z1A3T0NrM1VnYlhMZ2R5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751197122022768769#m</id>
            <title>选择内容

自动总结…</title>
            <link>https://nitter.cz/xiaohuggg/status/1751197122022768769#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751197122022768769#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 10:55:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>选择内容<br />
<br />
自动总结…</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDkzODQ4NjM5NTY3ODMxMDQvcHUvaW1nLzBlM09EcXYyTlFIbkxDWkUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751088229397495938#m</id>
            <title>R to @xiaohuggg: 控制选项</title>
            <link>https://nitter.cz/xiaohuggg/status/1751088229397495938#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751088229397495938#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 03:42:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>控制选项</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0UwZGRlNGJrQUFnRWd0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751088219461189705#m</id>
            <title>DALL·E 3将允许对图像进行更精细化控制 

新的DALL·E Contorls允许调整提示精度（加强/严格），选择风格（自动/自然/鲜艳），设定长宽比（自动/正方形/宽屏/垂直）等，

比起只能通过聊天提示生成图片，有了很大进步，用户可以控制图片生成效果！</title>
            <link>https://nitter.cz/xiaohuggg/status/1751088219461189705#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751088219461189705#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 03:42:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DALL·E 3将允许对图像进行更精细化控制 <br />
<br />
新的DALL·E Contorls允许调整提示精度（加强/严格），选择风格（自动/自然/鲜艳），设定长宽比（自动/正方形/宽屏/垂直）等，<br />
<br />
比起只能通过聊天提示生成图片，有了很大进步，用户可以控制图片生成效果！</p>
<p><a href="https://nitter.cz/btibor91/status/1750965235987501452#m">nitter.cz/btibor91/status/1750965235987501452#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751081213459415164#m</id>
            <title>DuckDB：基于大语言模型的文本到SQL

DuckDB-NSQL-7B，这是一个专门为DuckDB数据库设计的文本到SQL的模型。

你可以使用自然语言说描述你的需求，它会自动转换成SQL代码。比如，你告诉它“从test.csv文件创建一个新表”，它就能自动写出相应的SQL命令。

也就是可以使用自然语言来和你的数据库聊天。

这大大简化了数据库查询的过程，使得即使是不太懂SQL语言的用户也能轻松地与数据库进行交互和数据处理。

DuckDB-NSQL-7B模型是基于大约200,000条合成生成并验证的DuckDB SQL查询以及来自Numbers Station的超过250,000条一般性文本到SQL问题训练而成的。

它不仅能生成有用的DuckDB代码片段，还能生成用于回答分析问题的SQL查询。

DuckDB-NSQL-7B主要特点：

1、自然语言处理能力： 能够理解和处理自然语言输入，将用户用普通话语描述的数据查询需求转换成SQL查询代码。

2、针对DuckDB优化： 专为DuckDB数据库定制，能够充分利用DuckDB的特性和功能。

3、高效的查询生成： 对于常见的数据查询任务，如创建表、选择数据、排序和过滤等，都能快速生成准确的SQL代码。

4、用户友好的交互： 用户无需深入了解SQL语法，只需通过自然的语言描述就可以进行复杂的数据查询。

5、文档式的查询指导： 模型知识覆盖DuckDB 0.9.2中记录的所有功能，包括官方扩展，类似于一个随时可用的文档查询工具。

6、低延迟： 为了提供低延迟的SQL辅助特性，该模型采用了相对较小的模型大小，使得推理过程更快、成本更低。

7、广泛的应用场景： 不仅能生成DuckDB的代码片段，还能生成用于回答分析性问题的SQL查询。

8、开源和易于访问： 模型权重在Hugging Face上完全公开，方便用户下载和使用。

9、本地运行支持： 支持与llama.cpp一起在本地完全体验，提供了完整的本地运行指导。

用户可以在Hugging Face空间上尝试这个模型。只需用自然语言指令提示模型，描述你想要的查询类型。以下是一些示例：

示例1：从test.csv创建一个名为tmp的新表。
示例2：从出租车表中获取以_amount结尾的所有列。
示例3：从出租车表中获取乘客数量、行程距离和费用金额，并按所有这些排序。
示例4：获取2022年12月最长的行程。

如果想要在本地完全体验DuckDB-NSQL-7B，可以前往GitHub仓库或GGUF readme查看更多信息。

详细介绍：https://motherduck.com/blog/duckdb-text2sql-llm/
GitHub：https://github.com/NumbersStationAI/DuckDB-NSQL
Hugging Face：https://huggingface.co/spaces/motherduckdb/DuckDB-NSQL-7B</title>
            <link>https://nitter.cz/xiaohuggg/status/1751081213459415164#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751081213459415164#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 03:14:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DuckDB：基于大语言模型的文本到SQL<br />
<br />
DuckDB-NSQL-7B，这是一个专门为DuckDB数据库设计的文本到SQL的模型。<br />
<br />
你可以使用自然语言说描述你的需求，它会自动转换成SQL代码。比如，你告诉它“从test.csv文件创建一个新表”，它就能自动写出相应的SQL命令。<br />
<br />
也就是可以使用自然语言来和你的数据库聊天。<br />
<br />
这大大简化了数据库查询的过程，使得即使是不太懂SQL语言的用户也能轻松地与数据库进行交互和数据处理。<br />
<br />
DuckDB-NSQL-7B模型是基于大约200,000条合成生成并验证的DuckDB SQL查询以及来自Numbers Station的超过250,000条一般性文本到SQL问题训练而成的。<br />
<br />
它不仅能生成有用的DuckDB代码片段，还能生成用于回答分析问题的SQL查询。<br />
<br />
DuckDB-NSQL-7B主要特点：<br />
<br />
1、自然语言处理能力： 能够理解和处理自然语言输入，将用户用普通话语描述的数据查询需求转换成SQL查询代码。<br />
<br />
2、针对DuckDB优化： 专为DuckDB数据库定制，能够充分利用DuckDB的特性和功能。<br />
<br />
3、高效的查询生成： 对于常见的数据查询任务，如创建表、选择数据、排序和过滤等，都能快速生成准确的SQL代码。<br />
<br />
4、用户友好的交互： 用户无需深入了解SQL语法，只需通过自然的语言描述就可以进行复杂的数据查询。<br />
<br />
5、文档式的查询指导： 模型知识覆盖DuckDB 0.9.2中记录的所有功能，包括官方扩展，类似于一个随时可用的文档查询工具。<br />
<br />
6、低延迟： 为了提供低延迟的SQL辅助特性，该模型采用了相对较小的模型大小，使得推理过程更快、成本更低。<br />
<br />
7、广泛的应用场景： 不仅能生成DuckDB的代码片段，还能生成用于回答分析性问题的SQL查询。<br />
<br />
8、开源和易于访问： 模型权重在Hugging Face上完全公开，方便用户下载和使用。<br />
<br />
9、本地运行支持： 支持与llama.cpp一起在本地完全体验，提供了完整的本地运行指导。<br />
<br />
用户可以在Hugging Face空间上尝试这个模型。只需用自然语言指令提示模型，描述你想要的查询类型。以下是一些示例：<br />
<br />
示例1：从test.csv创建一个名为tmp的新表。<br />
示例2：从出租车表中获取以_amount结尾的所有列。<br />
示例3：从出租车表中获取乘客数量、行程距离和费用金额，并按所有这些排序。<br />
示例4：获取2022年12月最长的行程。<br />
<br />
如果想要在本地完全体验DuckDB-NSQL-7B，可以前往GitHub仓库或GGUF readme查看更多信息。<br />
<br />
详细介绍：<a href="https://motherduck.com/blog/duckdb-text2sql-llm/">motherduck.com/blog/duckdb-t…</a><br />
GitHub：<a href="https://github.com/NumbersStationAI/DuckDB-NSQL">github.com/NumbersStationAI/…</a><br />
Hugging Face：<a href="https://huggingface.co/spaces/motherduckdb/DuckDB-NSQL-7B">huggingface.co/spaces/mother…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0V4N2EtTGJVQUFtM1YxLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dFeDdhLUxiVUFBbTNWMS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751064514886594962#m</id>
            <title>好文推荐：AI时代UX的高标准：Perplexity

Perplexity通过AI重塑了网络搜索的方式，获得了业界的关注和商业成功。

这篇文章通过以下几个方面来阐述Perplexity是如何成功应用 Jakob Nielson 在1994年提出的：“10个可用性启发式原则”来提升用户体验。

1、系统状态的可见性： 设计应始终让用户了解正在发生的事情，通过适当的反馈在合理的时间内。

2、使用用户的语言： 使用用户熟悉的词汇、短语和概念，而不是内部术语。

3、用户控制和自由度： 用户经常会误操作，他们需要一个明显的“紧急出口”来离开不想要的动作，而无需经历复杂的过程。

4、一致性和标准化： 用户不应该对不同的词语、情境或行为是否表示同一事物感到困惑。

5、错误预防： 良好的错误信息很重要，但最好的设计是事先预防问题的发生。

6、识别而非回忆： 尽量减少用户的记忆负担，使元素、动作和选项可见。

7、灵活性和使用效率： 隐藏对初学者不可见的快捷方式，可以加快专家用户的交互速度。

8、美观和简约设计： 界面中不应包含不相关或很少需要的信息。

9、帮助用户识别、诊断和从错误中恢复： 错误信息应该用简单的语言表达，准确指出问题，并提出建设性的解决方案。

10、帮助和文档： 最好的系统是不需要额外解释的，但有时可能需要提供文档帮助用户完成任务。

文章最后强调，Perplexity如何将这些原则成功地融入其产品设计中，使其成为AI产品中用户体验的典范。

对于2024年从事AI产品开发的人来说，可以从Perplexity的例子中学习，即怎样通过传统的用户体验原则来提升现代技术产品的易用性。

Perplexity展示了即便是最先进的技术，也需要以用户为中心，简化设计，并提供直观的用户界面来满足广泛的用户需求。

通过细致入微地考虑用户体验的各个方面，AI产品可以更好地被大众接受和使用。

图文完整内容：https://mttmr.com/2024/01/10/perplexitys-high-bar-for-ux-in-the-age-of-ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1751064514886594962#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751064514886594962#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 02:08:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>好文推荐：AI时代UX的高标准：Perplexity<br />
<br />
Perplexity通过AI重塑了网络搜索的方式，获得了业界的关注和商业成功。<br />
<br />
这篇文章通过以下几个方面来阐述Perplexity是如何成功应用 Jakob Nielson 在1994年提出的：“10个可用性启发式原则”来提升用户体验。<br />
<br />
1、系统状态的可见性： 设计应始终让用户了解正在发生的事情，通过适当的反馈在合理的时间内。<br />
<br />
2、使用用户的语言： 使用用户熟悉的词汇、短语和概念，而不是内部术语。<br />
<br />
3、用户控制和自由度： 用户经常会误操作，他们需要一个明显的“紧急出口”来离开不想要的动作，而无需经历复杂的过程。<br />
<br />
4、一致性和标准化： 用户不应该对不同的词语、情境或行为是否表示同一事物感到困惑。<br />
<br />
5、错误预防： 良好的错误信息很重要，但最好的设计是事先预防问题的发生。<br />
<br />
6、识别而非回忆： 尽量减少用户的记忆负担，使元素、动作和选项可见。<br />
<br />
7、灵活性和使用效率： 隐藏对初学者不可见的快捷方式，可以加快专家用户的交互速度。<br />
<br />
8、美观和简约设计： 界面中不应包含不相关或很少需要的信息。<br />
<br />
9、帮助用户识别、诊断和从错误中恢复： 错误信息应该用简单的语言表达，准确指出问题，并提出建设性的解决方案。<br />
<br />
10、帮助和文档： 最好的系统是不需要额外解释的，但有时可能需要提供文档帮助用户完成任务。<br />
<br />
文章最后强调，Perplexity如何将这些原则成功地融入其产品设计中，使其成为AI产品中用户体验的典范。<br />
<br />
对于2024年从事AI产品开发的人来说，可以从Perplexity的例子中学习，即怎样通过传统的用户体验原则来提升现代技术产品的易用性。<br />
<br />
Perplexity展示了即便是最先进的技术，也需要以用户为中心，简化设计，并提供直观的用户界面来满足广泛的用户需求。<br />
<br />
通过细致入微地考虑用户体验的各个方面，AI产品可以更好地被大众接受和使用。<br />
<br />
图文完整内容：<a href="https://mttmr.com/2024/01/10/perplexitys-high-bar-for-ux-in-the-age-of-ai/">mttmr.com/2024/01/10/perplex…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA5MTAwMjE0OTQ2MDc4NzIvcHUvaW1nL0hiM2xxaDdQS2RnQmp6VWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751055273110954319#m</id>
            <title>R to @xiaohuggg: 操作演示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1751055273110954319#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751055273110954319#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 01:31:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>操作演示：</p>
<p><a href="https://nitter.cz/danshipper/status/1751017376143794415#m">nitter.cz/danshipper/status/1751017376143794415#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751055149001551943#m</id>
            <title>R to @xiaohuggg: 输入 @ 可以显示最近使用过的4个GPTs…

也可以通过搜索查找其他…

然后输入你的需求即可！</title>
            <link>https://nitter.cz/xiaohuggg/status/1751055149001551943#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751055149001551943#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 01:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>输入 @ 可以显示最近使用过的4个GPTs…<br />
<br />
也可以通过搜索查找其他…<br />
<br />
然后输入你的需求即可！</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V6X1gzMGF3QUF1cDg2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1751055137827946681#m</id>
            <title>ChatGPT又更新了 推出了新的Mention功能！

你可以在 ChatGPT的聊天窗口中通过 @ 来直接召唤任何GPTs，就像Discord里面召唤其他机器人一样！

这样不用来回切换窗口就能完成不同任务！

这样可以实现调用多个机器人的联动操作，完成一个任务🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1751055137827946681#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1751055137827946681#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 27 Jan 2024 01:30:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT又更新了 推出了新的Mention功能！<br />
<br />
你可以在 ChatGPT的聊天窗口中通过 @ 来直接召唤任何GPTs，就像Discord里面召唤其他机器人一样！<br />
<br />
这样不用来回切换窗口就能完成不同任务！<br />
<br />
这样可以实现调用多个机器人的联动操作，完成一个任务🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V6X1hWdGJ3QUFmV2U4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750829664766198026#m</id>
            <title>Diffuse to Choose：在线购物“虚拟试穿”模型

这个模型能让你在将任何商品放入任何环境中，同时和环境完美融合！

比如，你可以把一个在线商店的椅子放进你的客厅的照片里，看看它实际放在那里会是什么样子。

同时保证在不同环境中看起来自然和真实！

简而言之，它帮助用户更好地了解产品在真实环境中的样子，提高了在线购物的体验。

1、虚拟试穿技术：允许用户在不同环境中虚拟放置商品，实现逼真的在线购物体验。

2、与传统扩散模型相比，DTC模型能更好地捕捉商品细节，提升修复质量。采用特殊的算法，将来自参考图像的细粒度特征直接融入主扩散模型的潜在特征图中，保证产品与环境的高度融合。

3、高效平衡：在快速推断与保持高保真细节方面达到了有效的平衡。

4、广泛测试与评估：在不同数据集上测试，证明了DTC模型相较于现有技术的优越性。

5、场景适应性：能够处理多种场景中的图像，确保产品与场景的无缝整合。

6、快速推断能力：提供快速且高效的零次射推断，加快虚拟试穿过程。

项目及演示：https://diffuse2choose.github.io
论文：https://arxiv.org/abs/2401.13795</title>
            <link>https://nitter.cz/xiaohuggg/status/1750829664766198026#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750829664766198026#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 10:34:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Diffuse to Choose：在线购物“虚拟试穿”模型<br />
<br />
这个模型能让你在将任何商品放入任何环境中，同时和环境完美融合！<br />
<br />
比如，你可以把一个在线商店的椅子放进你的客厅的照片里，看看它实际放在那里会是什么样子。<br />
<br />
同时保证在不同环境中看起来自然和真实！<br />
<br />
简而言之，它帮助用户更好地了解产品在真实环境中的样子，提高了在线购物的体验。<br />
<br />
1、虚拟试穿技术：允许用户在不同环境中虚拟放置商品，实现逼真的在线购物体验。<br />
<br />
2、与传统扩散模型相比，DTC模型能更好地捕捉商品细节，提升修复质量。采用特殊的算法，将来自参考图像的细粒度特征直接融入主扩散模型的潜在特征图中，保证产品与环境的高度融合。<br />
<br />
3、高效平衡：在快速推断与保持高保真细节方面达到了有效的平衡。<br />
<br />
4、广泛测试与评估：在不同数据集上测试，证明了DTC模型相较于现有技术的优越性。<br />
<br />
5、场景适应性：能够处理多种场景中的图像，确保产品与场景的无缝整合。<br />
<br />
6、快速推断能力：提供快速且高效的零次射推断，加快虚拟试穿过程。<br />
<br />
项目及演示：<a href="https://diffuse2choose.github.io">diffuse2choose.github.io</a><br />
论文：<a href="https://arxiv.org/abs/2401.13795">arxiv.org/abs/2401.13795</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwODI5NTQyODkyMjU3MjgwL2ltZy9nR3JzeDJiZ1JRZkZqOG1XLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750796206874443880#m</id>
            <title>R to @xiaohuggg: Chrome 121版本已经新增在Android上支持WebGPU特性！

WebGPU，现在默认在运行Android 12及更高版本的设备上启用，特别是那些使用高通和ARM GPU的设备。支持范围将逐步扩大到更多Android设备，包括运行Android 11的设备。

https://developer.chrome.com/blog/new-in-webgpu-121</title>
            <link>https://nitter.cz/xiaohuggg/status/1750796206874443880#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750796206874443880#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 08:21:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chrome 121版本已经新增在Android上支持WebGPU特性！<br />
<br />
WebGPU，现在默认在运行Android 12及更高版本的设备上启用，特别是那些使用高通和ARM GPU的设备。支持范围将逐步扩大到更多Android设备，包括运行Android 11的设备。<br />
<br />
<a href="https://developer.chrome.com/blog/new-in-webgpu-121">developer.chrome.com/blog/ne…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1MDQxOTUyOTcwNjk4MzQyNC8zZ0k1V29OTj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750796204668342624#m</id>
            <title>Web LLM：在浏览器中运行大语言模型

该项目利用WebGPU加速，无需服务器支持，所有操作都在浏览器内运行。

这为构建面向每个人的AI助手开启了新的可能性。

这意味着以后大语言模型可以在任意设备上运行！

主要特点：

1.支持多种模型：支持多种模型，包括Llama 2 7B/13B、Llama 2 70B、Mistral 7B以及WizardMath等。

2.运行环境和要求：项目在Chrome 113中提供WebGPU支持。用户可以在支持的浏览器中直接尝试不同的模型。首次运行时需要下载模型参数，之后的运行将更快。

3.聊天演示：提供了基于Llama 2、Mistral-7B及其变体和RedPajama-INCITE-Chat-3B-v1模型的聊天演示。未来还将支持更多模型。

4.开源和开发支持：项目鼓励开发者使用WebLLM作为基础npm包，并在其上构建自己的Web应用程序。相关文档和GitHub资源可供参考。

项目目标与愿景：

该项目旨在为生态系统带来更多多样性，尤其是将LLMs直接嵌入到客户端并在浏览器内运行。这样做可以降低成本、增强个性化和保护隐私。

详细：https://webllm.mlc.ai

GitHub：https://github.com/mlc-ai/web-llm

演示视频 @charlie_ruan</title>
            <link>https://nitter.cz/xiaohuggg/status/1750796204668342624#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750796204668342624#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 08:21:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Web LLM：在浏览器中运行大语言模型<br />
<br />
该项目利用WebGPU加速，无需服务器支持，所有操作都在浏览器内运行。<br />
<br />
这为构建面向每个人的AI助手开启了新的可能性。<br />
<br />
这意味着以后大语言模型可以在任意设备上运行！<br />
<br />
主要特点：<br />
<br />
1.支持多种模型：支持多种模型，包括Llama 2 7B/13B、Llama 2 70B、Mistral 7B以及WizardMath等。<br />
<br />
2.运行环境和要求：项目在Chrome 113中提供WebGPU支持。用户可以在支持的浏览器中直接尝试不同的模型。首次运行时需要下载模型参数，之后的运行将更快。<br />
<br />
3.聊天演示：提供了基于Llama 2、Mistral-7B及其变体和RedPajama-INCITE-Chat-3B-v1模型的聊天演示。未来还将支持更多模型。<br />
<br />
4.开源和开发支持：项目鼓励开发者使用WebLLM作为基础npm包，并在其上构建自己的Web应用程序。相关文档和GitHub资源可供参考。<br />
<br />
项目目标与愿景：<br />
<br />
该项目旨在为生态系统带来更多多样性，尤其是将LLMs直接嵌入到客户端并在浏览器内运行。这样做可以降低成本、增强个性化和保护隐私。<br />
<br />
详细：<a href="https://webllm.mlc.ai">webllm.mlc.ai</a><br />
<br />
GitHub：<a href="https://github.com/mlc-ai/web-llm">github.com/mlc-ai/web-llm</a><br />
<br />
演示视频 <a href="https://nitter.cz/charlie_ruan" title="Charlie Ruan">@charlie_ruan</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwNzk2MTM5ODYyMTM4ODgwL2ltZy9uU0VGRW1HM1dJSDN1TXRNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750788482484846748#m</id>
            <title>曼谷有X友吗

🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1750788482484846748#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750788482484846748#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 07:51:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>曼谷有X友吗<br />
<br />
🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmRmFRQUFNUzViLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmRmJVQUFZSDJlLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V3TTBmSWFZQUE2Ti0tLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750706833751408858#m</id>
            <title>SUPIR：通过增加模型的规模（即增加模型的参数数量）提升图像修复的能力。

通过参数增加使得模型不仅能够修复图像中的错误或损坏，还能根据文本提示进行智能修复。

例如根据描述来改变图像中的特定细节。这样的处理方式提升了图像修复的质量和智能度，使得模型能够更准确、更灵活地恢复和改进图像。

SUPIR的主要功能：

图像修复： SUPIR的核心功能是对低质量或损坏的图像进行修复，提高其视觉质量。这包括处理如模糊、噪点、色彩失真等问题，使图像恢复到高清晰度和高质量状态。

文本引导的修复： SUPIR能够根据文本提示来指导图像修复。这意味着用户可以通过文本描述来指定希望修复或改变的图像部分，使得修复过程更加定制化和精确。

核心技术创新：

1、模型放大： SUPIR通过扩大模型规模（即增加模型的参数数量）来提升图像修复的能力。这种放大使得模型能够学习更多的特征，处理更复杂的图像修复任务。

2、多模态技术： 结合了图像处理和文本处理的技术，允许模型不仅理解图像内容，还能理解与之相关的文本描述，从而进行更准确的修复。

3、高质量训练数据集：收集了2000万高质量图像和文本注释，用于训练和控制图像修复。利用大量高分辨率、高质量的图像和相关文本注释作为训练数据，提高了模型的性能和适用性。

4、负质量提示： 通过引入质量较差的图像样本和相应的负面描述作为训练数据，进一步提升模型在感知质量方面的表现。

工作原理：

1、图像编码与解码： SUPIR利用一个编码器将低质量图像映射到潜在空间，然后使用解码器重建修复后的图像。

2、文本处理： 通过一个多模态语言模型，SUPIR能够理解与图像相关的文本描述，并将这些信息融入到图像修复过程中。

3、适配器设计： SUPIR设计了一个大规模适配器，用于将模型的生成能力调整到与输入图像相匹配的状态，确保修复过程符合用户的具体需求。

4、采样方法： 采用特殊的采样方法，用于指导图像的恢复过程，以防止过度生成，确保修复后的图像保持真实和高质量。

实验结果：

在多种IR任务上展示了出色的修复效果，特别是在复杂和具挑战性的真实世界场景中

1、多样化的图像修复任务： SUPIR被应用于各种类型的图像修复任务，包括但不限于去噪、去模糊、超分辨率、色彩校正等。这显示了其广泛的适用性和灵活性。

2、真实世界的复杂场景处理： 实验中的一个重要亮点是SUPIR在处理真实世界复杂场景中的高效表现。这些场景通常包含多种类型的图像退化，如不均匀光照、运动模糊和天气影响等，这些都是传统图像修复方法难以处理的。

3、高级特性的应用： SUPIR展示了如何根据复杂的文本描述进行定制化修复。例如，它可以根据用户提供的描述，调整图像中特定对象的纹理或颜色，或者改变场景的某些元素。

4、质量评估： 在实验中，SUPIR修复的图像在质量上得到了显著提升。这通过与现有技术的对比评估，以及视觉质量和客观指标（如图像清晰度、纹理细节等）的测量来证实。

5、挑战性任务的处理： 特别值得注意的是，SUPIR在处理一些传统方法难以解决的挑战性任务时表现突出，如极度模糊或严重损坏的图像修复。

6、用户定制和互动性： 实验还展示了SUPIR在用户交互方面的能力，用户可以通过简单的文本指令控制图像的修复过程，这为图像修复提供了新的互动维度。

项目及演示：https://supir.xpixel.group/
论文：https://arxiv.org/abs/2401.13627</title>
            <link>https://nitter.cz/xiaohuggg/status/1750706833751408858#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750706833751408858#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:26:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SUPIR：通过增加模型的规模（即增加模型的参数数量）提升图像修复的能力。<br />
<br />
通过参数增加使得模型不仅能够修复图像中的错误或损坏，还能根据文本提示进行智能修复。<br />
<br />
例如根据描述来改变图像中的特定细节。这样的处理方式提升了图像修复的质量和智能度，使得模型能够更准确、更灵活地恢复和改进图像。<br />
<br />
SUPIR的主要功能：<br />
<br />
图像修复： SUPIR的核心功能是对低质量或损坏的图像进行修复，提高其视觉质量。这包括处理如模糊、噪点、色彩失真等问题，使图像恢复到高清晰度和高质量状态。<br />
<br />
文本引导的修复： SUPIR能够根据文本提示来指导图像修复。这意味着用户可以通过文本描述来指定希望修复或改变的图像部分，使得修复过程更加定制化和精确。<br />
<br />
核心技术创新：<br />
<br />
1、模型放大： SUPIR通过扩大模型规模（即增加模型的参数数量）来提升图像修复的能力。这种放大使得模型能够学习更多的特征，处理更复杂的图像修复任务。<br />
<br />
2、多模态技术： 结合了图像处理和文本处理的技术，允许模型不仅理解图像内容，还能理解与之相关的文本描述，从而进行更准确的修复。<br />
<br />
3、高质量训练数据集：收集了2000万高质量图像和文本注释，用于训练和控制图像修复。利用大量高分辨率、高质量的图像和相关文本注释作为训练数据，提高了模型的性能和适用性。<br />
<br />
4、负质量提示： 通过引入质量较差的图像样本和相应的负面描述作为训练数据，进一步提升模型在感知质量方面的表现。<br />
<br />
工作原理：<br />
<br />
1、图像编码与解码： SUPIR利用一个编码器将低质量图像映射到潜在空间，然后使用解码器重建修复后的图像。<br />
<br />
2、文本处理： 通过一个多模态语言模型，SUPIR能够理解与图像相关的文本描述，并将这些信息融入到图像修复过程中。<br />
<br />
3、适配器设计： SUPIR设计了一个大规模适配器，用于将模型的生成能力调整到与输入图像相匹配的状态，确保修复过程符合用户的具体需求。<br />
<br />
4、采样方法： 采用特殊的采样方法，用于指导图像的恢复过程，以防止过度生成，确保修复后的图像保持真实和高质量。<br />
<br />
实验结果：<br />
<br />
在多种IR任务上展示了出色的修复效果，特别是在复杂和具挑战性的真实世界场景中<br />
<br />
1、多样化的图像修复任务： SUPIR被应用于各种类型的图像修复任务，包括但不限于去噪、去模糊、超分辨率、色彩校正等。这显示了其广泛的适用性和灵活性。<br />
<br />
2、真实世界的复杂场景处理： 实验中的一个重要亮点是SUPIR在处理真实世界复杂场景中的高效表现。这些场景通常包含多种类型的图像退化，如不均匀光照、运动模糊和天气影响等，这些都是传统图像修复方法难以处理的。<br />
<br />
3、高级特性的应用： SUPIR展示了如何根据复杂的文本描述进行定制化修复。例如，它可以根据用户提供的描述，调整图像中特定对象的纹理或颜色，或者改变场景的某些元素。<br />
<br />
4、质量评估： 在实验中，SUPIR修复的图像在质量上得到了显著提升。这通过与现有技术的对比评估，以及视觉质量和客观指标（如图像清晰度、纹理细节等）的测量来证实。<br />
<br />
5、挑战性任务的处理： 特别值得注意的是，SUPIR在处理一些传统方法难以解决的挑战性任务时表现突出，如极度模糊或严重损坏的图像修复。<br />
<br />
6、用户定制和互动性： 实验还展示了SUPIR在用户交互方面的能力，用户可以通过简单的文本指令控制图像的修复过程，这为图像修复提供了新的互动维度。<br />
<br />
项目及演示：<a href="https://supir.xpixel.group/">supir.xpixel.group/</a><br />
论文：<a href="https://arxiv.org/abs/2401.13627">arxiv.org/abs/2401.13627</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA3MDYxMjY4MDk4OTkwMDgvcHUvaW1nL19FS1RQS2k3TjkwZFpNLV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750704152605499508#m</id>
            <title>Adept Fuyu-Heavy：Adept Fuyu-Heavy是专为数字代理设计的新型多模态模型。

宣称是世界上第三大能力超强的多模态模型，仅次于GPT4-V和Gemini Ultra。

它特别擅长理解用户界面，这意味着可以解释和操作各种软件和应用程序的界面。

能够帮助用户执行各种任务，如自动化流程、响应查询、提供信息等。

Adept Fuyu-Heavy在多项评估和基准测试中展示了卓越的性能。

1、多模态基准测试：在MMM（Multimodal Multitask）基准测试中，Fuyu-Heavy的表现优于Gemini Pro，突显了其在多模态任务上的能力。

2、文本基准测试：尽管Fuyu-Heavy需分配部分容量处理图像数据，但在标准的文本只评估中，它的表现与Gemini Pro大体相当，甚至在MMLU（多模态语言理解）基准测试中超过了Gemini Pro。

3、长形式对话性能：经过有监督的微调和直接优化阶段后，Fuyu-Heavy在最常用的聊天评估——MT-Bench和AlpacaEval 1.0——中的表现与Claude 2.0相当，尽管它是一个更小的模型，且部分容量用于图像建模。

4、多模态性能标准：在MMM（Multimodal Multitask）基准测试上，Fuyu-Heavy略微优于Gemini Pro。此外，还包括了在VQAv2（一个视觉问答基准）和AI2D（一个图表理解数据集）上的结果。

Adept Fuyu-Heavy的主要能力包括：

1、多模态理解和生成： Fuyu-Heavy能够处理和理解多种类型的数据，如文本和图像，并能够基于这些数据生成相应的输出。这使其在多模态任务上表现出色。

2、高效的图像和文本处理： 尽管需要部分容量用于图像建模，Fuyu-Heavy在标准文本基准测试中的表现匹敌或超越同级别的模型。

3、优化的模型架构： Fuyu-Heavy通过扩展和优化Fuyu架构，有效处理任意大小和形状的图像，并有效利用现有的变压器模型优化。

4、长形式对话性能： 经过特定训练阶段优化，Fuyu-Heavy在长形式对话和交互中表现出色。

5、用户界面理解： 特别擅长于理解数字用户界面（UI），如网站和应用程序，提供有效的自动化解决方案。意味着Fuyu-Heavy的开发重点是使其能够适应和优化数字代理的功能，如提高用户界面理解、增强自动化决策能力、提供更准确的信息检索和内容生成等。

6、跨模态内容生成： 能够生成跨越文本和图像的内容，适用于多种应用场景。

详细：https://www.adept.ai/blog/adept-fuyu-heavy</title>
            <link>https://nitter.cz/xiaohuggg/status/1750704152605499508#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750704152605499508#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:16:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Adept Fuyu-Heavy：Adept Fuyu-Heavy是专为数字代理设计的新型多模态模型。<br />
<br />
宣称是世界上第三大能力超强的多模态模型，仅次于GPT4-V和Gemini Ultra。<br />
<br />
它特别擅长理解用户界面，这意味着可以解释和操作各种软件和应用程序的界面。<br />
<br />
能够帮助用户执行各种任务，如自动化流程、响应查询、提供信息等。<br />
<br />
Adept Fuyu-Heavy在多项评估和基准测试中展示了卓越的性能。<br />
<br />
1、多模态基准测试：在MMM（Multimodal Multitask）基准测试中，Fuyu-Heavy的表现优于Gemini Pro，突显了其在多模态任务上的能力。<br />
<br />
2、文本基准测试：尽管Fuyu-Heavy需分配部分容量处理图像数据，但在标准的文本只评估中，它的表现与Gemini Pro大体相当，甚至在MMLU（多模态语言理解）基准测试中超过了Gemini Pro。<br />
<br />
3、长形式对话性能：经过有监督的微调和直接优化阶段后，Fuyu-Heavy在最常用的聊天评估——MT-Bench和AlpacaEval 1.0——中的表现与Claude 2.0相当，尽管它是一个更小的模型，且部分容量用于图像建模。<br />
<br />
4、多模态性能标准：在MMM（Multimodal Multitask）基准测试上，Fuyu-Heavy略微优于Gemini Pro。此外，还包括了在VQAv2（一个视觉问答基准）和AI2D（一个图表理解数据集）上的结果。<br />
<br />
Adept Fuyu-Heavy的主要能力包括：<br />
<br />
1、多模态理解和生成： Fuyu-Heavy能够处理和理解多种类型的数据，如文本和图像，并能够基于这些数据生成相应的输出。这使其在多模态任务上表现出色。<br />
<br />
2、高效的图像和文本处理： 尽管需要部分容量用于图像建模，Fuyu-Heavy在标准文本基准测试中的表现匹敌或超越同级别的模型。<br />
<br />
3、优化的模型架构： Fuyu-Heavy通过扩展和优化Fuyu架构，有效处理任意大小和形状的图像，并有效利用现有的变压器模型优化。<br />
<br />
4、长形式对话性能： 经过特定训练阶段优化，Fuyu-Heavy在长形式对话和交互中表现出色。<br />
<br />
5、用户界面理解： 特别擅长于理解数字用户界面（UI），如网站和应用程序，提供有效的自动化解决方案。意味着Fuyu-Heavy的开发重点是使其能够适应和优化数字代理的功能，如提高用户界面理解、增强自动化决策能力、提供更准确的信息检索和内容生成等。<br />
<br />
6、跨模态内容生成： 能够生成跨越文本和图像的内容，适用于多种应用场景。<br />
<br />
详细：<a href="https://www.adept.ai/blog/adept-fuyu-heavy">adept.ai/blog/adept-fuyu-hea…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA1NDAzMTY3ODMwMjIwODAvcHUvaW1nL1pacTRtSmdVU2t3MUNwekkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750694801517969900#m</id>
            <title>SpeechGPT-Gen：为大语言模型提供内在的跨模态对话能力

它不仅能处理传统的文本数据，还能理解和生成语音数据，实现文本与语音之间的无缝对话。

能够接收语音输入，理解其内容，并以语音形式做出回应。

为大语言模型在处理和生成语音方面提供了强大的支持。

SpeechGPT-Gen是一个包含8亿参数的模型，有效地处理语义和知觉信息。

主要功能特点：

SpeechGPT-Gen能够接收语音输入，理解其内容，并以语音形式做出回应。这种能力使得模型在执行语音到语音对话任务时表现出色。

1、零样本文本到语音转换（Text-to-Speech）： 
SpeechGPT-Gen能够将给定的文本转换成语音，而且不需要提前的样本或训练数据。能够直接从文本生成自然听起来的语音。

2、零样本语音转换（Voice Conversion）：
 它可以改变源语音的音色，使其听起来像是另一个给定的发言者所说。这对于个性化语音应用非常有用。

3、语音到语音对话（Speech-to-Speech Dialogue）： 
SpeechGPT-Gen能够理解语音指令，并以类似的音色生成语音回应。这对于开发更自然的交互式语音系统特别重要。

解决的问题：

1、提高语音生成效率： 传统的大型语音语言模型在处理语义和知觉信息时存在冗余，导致效率低下。SpeechGPT-Gen通过分离这两种信息的建模，有效地提高了语音生成的效率。

2、增强模型的泛化能力： 它在未见过的发言者上展示了优秀的泛化能力，这意味着它能够处理各种不同的语音类型和风格。

3、提供更多样的语音应用： 通过支持零样本的文本到语音、语音转换和语音对话，SpeechGPT-Gen为语音技术的应用提供了更广泛的可能性，如语音合成、个性化语音生成等。

技术创新：

1、链式信息生成： SpeechGPT-Gen引入了这种新方法来分离和处理语音的语义和知觉信息。这种方法减少了传统方法中的冗余和低效率。

2、自回归和非自回归模型的结合：
自回归模型： 用于处理语义信息。基于大型语言模型（LLM），这一部分负责理解和生成语音的含义和内容。

非自回归模型： 使用流匹配技术（flow matching）处理知觉信息，即声音的感觉特征（如音色和节奏）。

3、语义信息的先验注入： 在流匹配中，SpeechGPT-Gen引入了一种创新的方法，将语义信息注入到先验分布中，从而提高流匹配的效率。

工作原理：

1、语义建模： 通过自回归模型，SpeechGPT-Gen首先对语音或文本的语义内容进行建模。这一步涉及理解语音或文本的含义，为后续的知觉建模提供基础。

2、知觉建模： 接着，非自回归模型通过流匹配技术处理知觉信息，即生成具有特定音色和风格的语音。

3、流匹配技术： 流匹配通过建立从简单先验分布到复杂数据分布的转换来工作。在SpeechGPT-Gen中，这个过程利用了语义信息作为先验，从而更高效地生成知觉信息。

4、综合生成： 通过这两个步骤，SpeechGPT-Gen能够先理解语音或文本的意义，然后生成具有相应含义的语音输出，具有良好的语义和知觉一致性。

这种结合了自回归和非自回归模型的方法，以及流匹配技术的应用，使得SpeechGPT-Gen在多种跨模态任务（如零样本文本到语音、语音转换和语音到语音对话）中表现出色，具有较强的灵活性和效率。

项目及演示：https://0nutation.github.io/SpeechGPT-Gen.github.io/
论文：https://arxiv.org/abs/2401.13527
GitHub：https://github.com/0nutation/SpeechGPT</title>
            <link>https://nitter.cz/xiaohuggg/status/1750694801517969900#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750694801517969900#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 01:39:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SpeechGPT-Gen：为大语言模型提供内在的跨模态对话能力<br />
<br />
它不仅能处理传统的文本数据，还能理解和生成语音数据，实现文本与语音之间的无缝对话。<br />
<br />
能够接收语音输入，理解其内容，并以语音形式做出回应。<br />
<br />
为大语言模型在处理和生成语音方面提供了强大的支持。<br />
<br />
SpeechGPT-Gen是一个包含8亿参数的模型，有效地处理语义和知觉信息。<br />
<br />
主要功能特点：<br />
<br />
SpeechGPT-Gen能够接收语音输入，理解其内容，并以语音形式做出回应。这种能力使得模型在执行语音到语音对话任务时表现出色。<br />
<br />
1、零样本文本到语音转换（Text-to-Speech）： <br />
SpeechGPT-Gen能够将给定的文本转换成语音，而且不需要提前的样本或训练数据。能够直接从文本生成自然听起来的语音。<br />
<br />
2、零样本语音转换（Voice Conversion）：<br />
 它可以改变源语音的音色，使其听起来像是另一个给定的发言者所说。这对于个性化语音应用非常有用。<br />
<br />
3、语音到语音对话（Speech-to-Speech Dialogue）： <br />
SpeechGPT-Gen能够理解语音指令，并以类似的音色生成语音回应。这对于开发更自然的交互式语音系统特别重要。<br />
<br />
解决的问题：<br />
<br />
1、提高语音生成效率： 传统的大型语音语言模型在处理语义和知觉信息时存在冗余，导致效率低下。SpeechGPT-Gen通过分离这两种信息的建模，有效地提高了语音生成的效率。<br />
<br />
2、增强模型的泛化能力： 它在未见过的发言者上展示了优秀的泛化能力，这意味着它能够处理各种不同的语音类型和风格。<br />
<br />
3、提供更多样的语音应用： 通过支持零样本的文本到语音、语音转换和语音对话，SpeechGPT-Gen为语音技术的应用提供了更广泛的可能性，如语音合成、个性化语音生成等。<br />
<br />
技术创新：<br />
<br />
1、链式信息生成： SpeechGPT-Gen引入了这种新方法来分离和处理语音的语义和知觉信息。这种方法减少了传统方法中的冗余和低效率。<br />
<br />
2、自回归和非自回归模型的结合：<br />
自回归模型： 用于处理语义信息。基于大型语言模型（LLM），这一部分负责理解和生成语音的含义和内容。<br />
<br />
非自回归模型： 使用流匹配技术（flow matching）处理知觉信息，即声音的感觉特征（如音色和节奏）。<br />
<br />
3、语义信息的先验注入： 在流匹配中，SpeechGPT-Gen引入了一种创新的方法，将语义信息注入到先验分布中，从而提高流匹配的效率。<br />
<br />
工作原理：<br />
<br />
1、语义建模： 通过自回归模型，SpeechGPT-Gen首先对语音或文本的语义内容进行建模。这一步涉及理解语音或文本的含义，为后续的知觉建模提供基础。<br />
<br />
2、知觉建模： 接着，非自回归模型通过流匹配技术处理知觉信息，即生成具有特定音色和风格的语音。<br />
<br />
3、流匹配技术： 流匹配通过建立从简单先验分布到复杂数据分布的转换来工作。在SpeechGPT-Gen中，这个过程利用了语义信息作为先验，从而更高效地生成知觉信息。<br />
<br />
4、综合生成： 通过这两个步骤，SpeechGPT-Gen能够先理解语音或文本的意义，然后生成具有相应含义的语音输出，具有良好的语义和知觉一致性。<br />
<br />
这种结合了自回归和非自回归模型的方法，以及流匹配技术的应用，使得SpeechGPT-Gen在多种跨模态任务（如零样本文本到语音、语音转换和语音到语音对话）中表现出色，具有较强的灵活性和效率。<br />
<br />
项目及演示：<a href="https://0nutation.github.io/SpeechGPT-Gen.github.io/">0nutation.github.io/SpeechGP…</a><br />
论文：<a href="https://arxiv.org/abs/2401.13527">arxiv.org/abs/2401.13527</a><br />
GitHub：<a href="https://github.com/0nutation/SpeechGPT">github.com/0nutation/SpeechG…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTA1MzQ5MzQ5MjkwMjI5NzYvcHUvaW1nL1RhRVBUVWZWX2FvQUdLVEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750688990104330481#m</id>
            <title>OpenAI推出新一代嵌入模型、同时发布新的API使用管理工具，以及GPT-3.5 Turbo降价！

同时更新的GPT-4 Turbo预览版模型，提高了代码生成等任务的完成度，以及减少错误！

新嵌入模型推出：

• 推出了两种新的嵌入模型：text-embedding-3-small和text-embedding-3-large。 

新一代向量大模型text-embedding-3，embedding长度升级，价格最高下降5倍，包含2个版本，新增了一个可以控制生成的向量长度的参数！

新一代嵌入模型性能对比：

• 小型模型(text-embedding-3-small)对比：与上一代模型text-embedding-ada-002相比，新模型在多语言检索基准(MIRACL)上的平均得分从31.4%提高到44.0%，在英语任务基准(MTEB)上的平均得分从61.0%提高到62.3%。

• 大型模型(text-embedding-3-large)对比：与text-embedding-ada-002相比，在MIRACL上平均得分从31.4%提高到54.9%，在MTEB上从61.0%提高到64.6%。

• 新模型的性能普遍优于上一代模型，尤其是在多语言检索方面表现出色。

GPT-3.5 Turbo降价对比：

• 新的GPT-3.5 Turbo模型gpt-3.5-turbo-0125的输入价格降低50%，至$0.0005 /1K tokens，输出价格降低25%，至$0.0015 /1K tokens。

• 降价旨在帮助客户扩大规模使用，同时该模型还包含多项改进，如提高响应准确度和修复非英语语言功能调用的文本编码问题。

新内容审核模型发布：

• 发布了新的内容审核模型text-moderation-007，作为提高内容安全的一部分。

API使用和管理改进：

• 提供了更多的API使用可视化和控制工具，如API密钥权限分配和API使用情况的仪表板。

详细：https://openai.com/blog/new-embedding-models-and-api-updates</title>
            <link>https://nitter.cz/xiaohuggg/status/1750688990104330481#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750688990104330481#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 01:15:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI推出新一代嵌入模型、同时发布新的API使用管理工具，以及GPT-3.5 Turbo降价！<br />
<br />
同时更新的GPT-4 Turbo预览版模型，提高了代码生成等任务的完成度，以及减少错误！<br />
<br />
新嵌入模型推出：<br />
<br />
• 推出了两种新的嵌入模型：text-embedding-3-small和text-embedding-3-large。 <br />
<br />
新一代向量大模型text-embedding-3，embedding长度升级，价格最高下降5倍，包含2个版本，新增了一个可以控制生成的向量长度的参数！<br />
<br />
新一代嵌入模型性能对比：<br />
<br />
• 小型模型(text-embedding-3-small)对比：与上一代模型text-embedding-ada-002相比，新模型在多语言检索基准(MIRACL)上的平均得分从31.4%提高到44.0%，在英语任务基准(MTEB)上的平均得分从61.0%提高到62.3%。<br />
<br />
• 大型模型(text-embedding-3-large)对比：与text-embedding-ada-002相比，在MIRACL上平均得分从31.4%提高到54.9%，在MTEB上从61.0%提高到64.6%。<br />
<br />
• 新模型的性能普遍优于上一代模型，尤其是在多语言检索方面表现出色。<br />
<br />
GPT-3.5 Turbo降价对比：<br />
<br />
• 新的GPT-3.5 Turbo模型gpt-3.5-turbo-0125的输入价格降低50%，至$0.0005 /1K tokens，输出价格降低25%，至$0.0015 /1K tokens。<br />
<br />
• 降价旨在帮助客户扩大规模使用，同时该模型还包含多项改进，如提高响应准确度和修复非英语语言功能调用的文本编码问题。<br />
<br />
新内容审核模型发布：<br />
<br />
• 发布了新的内容审核模型text-moderation-007，作为提高内容安全的一部分。<br />
<br />
API使用和管理改进：<br />
<br />
• 提供了更多的API使用可视化和控制工具，如API密钥权限分配和API使用情况的仪表板。<br />
<br />
详细：<a href="https://openai.com/blog/new-embedding-models-and-api-updates">openai.com/blog/new-embeddin…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V1eVdtN2JRQUF0QlI0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0V1eVdtN2J3QUFGS2lhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750400886676070495#m</id>
            <title>26种多模态大模型研究报告

这篇论文对目前市面上26种多模态大语言模型（MM-LLMs）进行了全面的研究和分析。提供了对多模态大语言模型的深入了解。

详细介绍了模型架构和训练流程的设计公式。

26 种现存的 MM-LLMs，每种模型都有其独特的设计和功能。

主要内容：

1.模型架构和训练流程：

论文详细描述了这些模型的架构和训练流程，突出了它们如何结合了传统大型语言模型（LLMs）的能力，并支持多模态输入和输出。

2.模型概览：

• 研究涵盖了26种不同的 MM-LLMs，每个模型都有其独特的设计和功能特点。

• 这些模型被分为不同的类别，根据它们的架构和功能进行了分类。

3.性能评估：对这些模型在主流基准测试上的性能进行了回顾，分析了它们在不同任务上的表现。

4.训练策略：总结了提高 MM-LLMs 性能的关键训练策略，包括数据处理和模型优化等。

5.研究方向和资源：讨论了 MM-LLMs 的未来研究方向，并提供了实时跟踪这些模型最新发展的资源。

论文：https://arxiv.org/abs/2401.13601
PDF：https://arxiv.org/pdf/2401.13601.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1750400886676070495#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750400886676070495#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 06:11:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>26种多模态大模型研究报告<br />
<br />
这篇论文对目前市面上26种多模态大语言模型（MM-LLMs）进行了全面的研究和分析。提供了对多模态大语言模型的深入了解。<br />
<br />
详细介绍了模型架构和训练流程的设计公式。<br />
<br />
26 种现存的 MM-LLMs，每种模型都有其独特的设计和功能。<br />
<br />
主要内容：<br />
<br />
1.模型架构和训练流程：<br />
<br />
论文详细描述了这些模型的架构和训练流程，突出了它们如何结合了传统大型语言模型（LLMs）的能力，并支持多模态输入和输出。<br />
<br />
2.模型概览：<br />
<br />
• 研究涵盖了26种不同的 MM-LLMs，每个模型都有其独特的设计和功能特点。<br />
<br />
• 这些模型被分为不同的类别，根据它们的架构和功能进行了分类。<br />
<br />
3.性能评估：对这些模型在主流基准测试上的性能进行了回顾，分析了它们在不同任务上的表现。<br />
<br />
4.训练策略：总结了提高 MM-LLMs 性能的关键训练策略，包括数据处理和模型优化等。<br />
<br />
5.研究方向和资源：讨论了 MM-LLMs 的未来研究方向，并提供了实时跟踪这些模型最新发展的资源。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.13601">arxiv.org/abs/2401.13601</a><br />
PDF：<a href="https://arxiv.org/pdf/2401.13601.pdf">arxiv.org/pdf/2401.13601.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Vxc1VXa2JjQUFCb3VNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750391646121017610#m</id>
            <title>利用红外激光反射攻击自动驾驶车辆，攻击成功率达到100%。

研究人员提出了一种使用红外线激光攻击自动驾驶车辆的方法。

攻击目标是车辆的摄像头系统，特别是那些用于识别道路标志的摄像头。使得自动驾驶汽车无法识别道路标志！

攻击原理：

1.红外线激光的不可见性：

•人眼无法看到红外线激光，但很多车载摄像头没有红外线滤镜，因此能够捕捉到这种光。
•这意味着攻击者可以对道路标志发射红外线激光，而不会被路人注意到。

2.误导识别系统：

•自动驾驶车辆使用深度学习模型通过摄像头捕捉的图像来识别道路标志。
•红外线激光可以改变摄像头捕捉的图像，使得AI识别系统误读这些标志。

3.静止道路标志上的红外线激光投射：

•研究提出在静止的道路标志上投射红外线激光，这种方式相对容易执行，隐蔽性较高。
•激光可以精准地修改标志上的特定部分，导致AI系统对标志的解读出错。

4.攻击效果：

•在室内实验中，所有深度学习模型的攻击成功率达到100%。

•在户外环境下，特别是夜间条件下，攻击成功率在80%至100%之间。

这项研究旨在提前理解并应对可能误导自动驾驶车辆AI识别系统的威胁，以便于未来加强这些系统的安全性和可靠性。

论文：https://arxiv.org/abs/2401.03582</title>
            <link>https://nitter.cz/xiaohuggg/status/1750391646121017610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750391646121017610#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 05:34:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>利用红外激光反射攻击自动驾驶车辆，攻击成功率达到100%。<br />
<br />
研究人员提出了一种使用红外线激光攻击自动驾驶车辆的方法。<br />
<br />
攻击目标是车辆的摄像头系统，特别是那些用于识别道路标志的摄像头。使得自动驾驶汽车无法识别道路标志！<br />
<br />
攻击原理：<br />
<br />
1.红外线激光的不可见性：<br />
<br />
•人眼无法看到红外线激光，但很多车载摄像头没有红外线滤镜，因此能够捕捉到这种光。<br />
•这意味着攻击者可以对道路标志发射红外线激光，而不会被路人注意到。<br />
<br />
2.误导识别系统：<br />
<br />
•自动驾驶车辆使用深度学习模型通过摄像头捕捉的图像来识别道路标志。<br />
•红外线激光可以改变摄像头捕捉的图像，使得AI识别系统误读这些标志。<br />
<br />
3.静止道路标志上的红外线激光投射：<br />
<br />
•研究提出在静止的道路标志上投射红外线激光，这种方式相对容易执行，隐蔽性较高。<br />
•激光可以精准地修改标志上的特定部分，导致AI系统对标志的解读出错。<br />
<br />
4.攻击效果：<br />
<br />
•在室内实验中，所有深度学习模型的攻击成功率达到100%。<br />
<br />
•在户外环境下，特别是夜间条件下，攻击成功率在80%至100%之间。<br />
<br />
这项研究旨在提前理解并应对可能误导自动驾驶车辆AI识别系统的威胁，以便于未来加强这些系统的安全性和可靠性。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.03582">arxiv.org/abs/2401.03582</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwMzkxNDA4NzY5NTA3MzI4L2ltZy9VaGVZNVFvemdrck5lTlpCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>