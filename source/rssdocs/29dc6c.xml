<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756321049887854740#m</id>
            <title>微软在Windows 11 Insider Preview Build 26052中引入了Sudo for Windows新功能

该功能非常类似于macOS和Linux系统中终端里的sudo命令。

sudo命令允许用户执行需要管理员（root）权限的命令，而无需切换到root用户。用户只需在命令前加上sudo，然后输入自己的密码，就可以以更高权限运行该命令。

Sudo for Windows目前支持三种不同的配置选项：在新窗口中打开、输入关闭和内联。这些配置允许用户根据需要选择如何执行提权过程。

微软计划未来几个月内扩展Sudo for Windows的文档，并分享更多关于在“内联”配置下运行sudo的安全性细节。

此外，微软还宣布将这个项目开源在GitHub上。目前，他们正努力在GitHub仓库中添加更多关于该项目的信息，并将在未来几个月分享更多计划细节。

详细：https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/
GitHub：https://github.com/microsoft/sudo</title>
            <link>https://nitter.cz/xiaohuggg/status/1756321049887854740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756321049887854740#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 14:15:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软在Windows 11 Insider Preview Build 26052中引入了Sudo for Windows新功能<br />
<br />
该功能非常类似于macOS和Linux系统中终端里的sudo命令。<br />
<br />
sudo命令允许用户执行需要管理员（root）权限的命令，而无需切换到root用户。用户只需在命令前加上sudo，然后输入自己的密码，就可以以更高权限运行该命令。<br />
<br />
Sudo for Windows目前支持三种不同的配置选项：在新窗口中打开、输入关闭和内联。这些配置允许用户根据需要选择如何执行提权过程。<br />
<br />
微软计划未来几个月内扩展Sudo for Windows的文档，并分享更多关于在“内联”配置下运行sudo的安全性细节。<br />
<br />
此外，微软还宣布将这个项目开源在GitHub上。目前，他们正努力在GitHub仓库中添加更多关于该项目的信息，并将在未来几个月分享更多计划细节。<br />
<br />
详细：<a href="https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/">devblogs.microsoft.com/comma…</a><br />
GitHub：<a href="https://github.com/microsoft/sudo">github.com/microsoft/sudo</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0YteXVncWEwQUE4WWNGLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dGLXl1Z3FhMEFBOFljRi5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756316182343512348#m</id>
            <title>iMusic：基于IMU的面部表情捕捉

该项目使用了一种可以贴在脸上，叫做惯性测量单元（IMUs）的小型设备，来捕捉面部表情。

与依赖摄像头的方法不同，IMUs不需要拍摄视频，通过捕捉微小面部动作，来捕捉表情，所以更能保护个人隐私。

即使在脸部部分被遮挡的情况下，它也能有效工作。

MUSIC项目主要解决了两个问题：

在需要保护隐私的场合，如何捕捉面部表情而不依赖视频。

在脸部被部分遮挡的情况下，如何准确捕捉面部动作。

主要特点：

1、隐私保护：相比基于视频的面部捕捉方法，使用IMUs不需要捕捉用户的视觉图像，因此更能保护用户的隐私。

2、避免视线遮挡问题：由于不依赖视觉信息，即使用户的面部部分被遮挡，系统也能准确捕捉面部表情。

3、精确捕捉细微表情：IMUs能够检测细微的面部运动，帮助捕捉通常难以通过视觉方法识别的轻微表情变化。

4、灵活性和可移植性：IMUs小巧轻便，可以轻易地集成到多种设备中，使得iMusic项目的技术可以广泛应用于各种场合，包括但不限于娱乐、健康监测和人机交互等领域。

iMusic的工作原理：

1、硬件设计：微型IMUs，使其能够适合贴合面部并准确捕捉面部运动。这些IMUs被放置在面部的关键位置，以监测表情变化时的动态信息。

2、数据采集与校准：通过IMUs收集面部运动的原始数据，并进行必要的校准处理，以确保数据的准确性和一致性。

3、IMU-ARKit数据集：创建一个包含丰富的IMU和视觉信号配对的数据集，用于训练和验证面部表情捕捉模型。这个数据集覆盖了广泛的面部表情和表演，为后续的分析和模型训练提供了基础。

4、面部表情预测模型：利用特别设计的Transformer扩散模型，从IMUs收集的信号中预测面部的混合形状参数。这个模型通过两阶段训练策略来优化，使其能够准确地从IMU数据中解析出复杂的面部表情。

项目及演示：https://sites.google.com/view/projectpage-imusic
论文：https://arxiv.org/abs/2402.03944
视频：https://youtu.be/rPusR6b43ng</title>
            <link>https://nitter.cz/xiaohuggg/status/1756316182343512348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756316182343512348#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 13:56:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>iMusic：基于IMU的面部表情捕捉<br />
<br />
该项目使用了一种可以贴在脸上，叫做惯性测量单元（IMUs）的小型设备，来捕捉面部表情。<br />
<br />
与依赖摄像头的方法不同，IMUs不需要拍摄视频，通过捕捉微小面部动作，来捕捉表情，所以更能保护个人隐私。<br />
<br />
即使在脸部部分被遮挡的情况下，它也能有效工作。<br />
<br />
MUSIC项目主要解决了两个问题：<br />
<br />
在需要保护隐私的场合，如何捕捉面部表情而不依赖视频。<br />
<br />
在脸部被部分遮挡的情况下，如何准确捕捉面部动作。<br />
<br />
主要特点：<br />
<br />
1、隐私保护：相比基于视频的面部捕捉方法，使用IMUs不需要捕捉用户的视觉图像，因此更能保护用户的隐私。<br />
<br />
2、避免视线遮挡问题：由于不依赖视觉信息，即使用户的面部部分被遮挡，系统也能准确捕捉面部表情。<br />
<br />
3、精确捕捉细微表情：IMUs能够检测细微的面部运动，帮助捕捉通常难以通过视觉方法识别的轻微表情变化。<br />
<br />
4、灵活性和可移植性：IMUs小巧轻便，可以轻易地集成到多种设备中，使得iMusic项目的技术可以广泛应用于各种场合，包括但不限于娱乐、健康监测和人机交互等领域。<br />
<br />
iMusic的工作原理：<br />
<br />
1、硬件设计：微型IMUs，使其能够适合贴合面部并准确捕捉面部运动。这些IMUs被放置在面部的关键位置，以监测表情变化时的动态信息。<br />
<br />
2、数据采集与校准：通过IMUs收集面部运动的原始数据，并进行必要的校准处理，以确保数据的准确性和一致性。<br />
<br />
3、IMU-ARKit数据集：创建一个包含丰富的IMU和视觉信号配对的数据集，用于训练和验证面部表情捕捉模型。这个数据集覆盖了广泛的面部表情和表演，为后续的分析和模型训练提供了基础。<br />
<br />
4、面部表情预测模型：利用特别设计的Transformer扩散模型，从IMUs收集的信号中预测面部的混合形状参数。这个模型通过两阶段训练策略来优化，使其能够准确地从IMU数据中解析出复杂的面部表情。<br />
<br />
项目及演示：<a href="https://sites.google.com/view/projectpage-imusic">sites.google.com/view/projec…</a><br />
论文：<a href="https://arxiv.org/abs/2402.03944">arxiv.org/abs/2402.03944</a><br />
视频：<a href="https://youtu.be/rPusR6b43ng">youtu.be/rPusR6b43ng</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTYzMTEwMzg4ODExMjAyNTYvcHUvaW1nL011VnB1WlJQUV9ZajZ2OUMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756313854437732694#m</id>
            <title>OpenAI推出了一个基于Whisper模型的音频到文本的API，可以将任何音频直接转录成文本并翻译为英文。

同时在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。

主要功能：

1、音频转文字：将音频文件中的语音内容自动转换成文本形式，让用户可以读到音频里说了什么。

2、支持多种语言的翻译转录：如果音频中的语言不是英语，这个API还能先将其翻译成英语，然后再进行转录，使非英语内容也能轻松转换成文本。

3、提供时间戳：OpenAI的Whisper API提供了一个参数timestamp_granularities[]，允许用户获取带有时间戳的更结构化的JSON输出格式。这意味着，在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。

4、支持多种音频格式：支持上传25MB以内的文件，包括mp3、mp4、mpeg、mpga、m4a、wav和webm等格式。用户无需转换文件格式即可直接使用。

详细：https://platform.openai.com/docs/guides/speech-to-text</title>
            <link>https://nitter.cz/xiaohuggg/status/1756313854437732694#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756313854437732694#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 13:47:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI推出了一个基于Whisper模型的音频到文本的API，可以将任何音频直接转录成文本并翻译为英文。<br />
<br />
同时在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。<br />
<br />
主要功能：<br />
<br />
1、音频转文字：将音频文件中的语音内容自动转换成文本形式，让用户可以读到音频里说了什么。<br />
<br />
2、支持多种语言的翻译转录：如果音频中的语言不是英语，这个API还能先将其翻译成英语，然后再进行转录，使非英语内容也能轻松转换成文本。<br />
<br />
3、提供时间戳：OpenAI的Whisper API提供了一个参数timestamp_granularities[]，允许用户获取带有时间戳的更结构化的JSON输出格式。这意味着，在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。<br />
<br />
4、支持多种音频格式：支持上传25MB以内的文件，包括mp3、mp4、mpeg、mpga、m4a、wav和webm等格式。用户无需转换文件格式即可直接使用。<br />
<br />
详细：<a href="https://platform.openai.com/docs/guides/speech-to-text">platform.openai.com/docs/gui…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YtdUZyNmJjQUFoU0E0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1755487926127820830#m</id>
            <title>RT by @xiaohuggg: 升级了一下科技文章翻译GPT，增加了一个API可以将URL转成Markdown，现在只需要输入URL就可以翻译网页内容

https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi</title>
            <link>https://nitter.cz/dotey/status/1755487926127820830#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1755487926127820830#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 07:05:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>升级了一下科技文章翻译GPT，增加了一个API可以将URL转成Markdown，现在只需要输入URL就可以翻译网页内容<br />
<br />
<a href="https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi">chat.openai.com/g/g-uBhKUJJT…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5Mllkd1djQUFPdHRFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5M2NLYVhZQUFmMjQwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5LTFnVld3QUVKVFU3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756154223065251911#m</id>
            <title>马斯克：将用「X」取代手机电话

马斯克称将在几个月后注销自己的电话号码。转而使用「X」进行信息收发和通话🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1756154223065251911#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756154223065251911#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 03:12:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克：将用「X」取代手机电话<br />
<br />
马斯克称将在几个月后注销自己的电话号码。转而使用「X」进行信息收发和通话🤔</p>
<p><a href="https://nitter.cz/elonmusk/status/1755870691159626094#m">nitter.cz/elonmusk/status/1755870691159626094#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</id>
            <title>Happy Chinese New Year！

Every One…

🐲🧨🇨🇳</title>
            <link>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755985073747447827#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 16:00:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Happy Chinese New Year！<br />
<br />
Every One…<br />
<br />
🐲🧨🇨🇳</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y2REhLRGJNQUFxaHNsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755911511531520464#m</id>
            <title>做好了

自己做的

开吃🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1755911511531520464#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755911511531520464#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 11:08:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>做好了<br />
<br />
自己做的<br />
<br />
开吃🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1ySWFvQUExZHVSLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1yRWFBQUFkSkFoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1zQWJBQUFoUzNnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1yR2E0QUF3b1BVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755850647667388813#m</id>
            <title>开始做年夜饭了

不更新了 

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1755850647667388813#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755850647667388813#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 07:06:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开始做年夜饭了<br />
<br />
不更新了 <br />
<br />
😂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755840200260096210#m</id>
            <title>Google DeepMind抛弃传统的搜索方法，使用Transformer模型，训练了一个AI模型来下象棋。

该模型能够达到国际象棋大师级别的水平。甚至表现超过了AlphaZero。

这说明Transformer模型，不仅能处理语言任务，还能够在复杂的决策和策略游戏中学习和模拟高级人类智能。

该方法同时显著减少了计算需求。

研究背景：

在国际象棋AI的发展历史中，传统的方法通常依赖于搜索算法（比如alpha-beta剪枝）来预测和评估可能的移动，从而选择最佳的一步。——即考虑棋盘上所有可能的走法和结果——来决定下一步怎么走。

这种方法虽然可以工作，但需要大量的计算资源。

AlphaZero是由DeepMind开发的一种高级AI，它通过自我对弈学习棋类游戏的策略，并在国际象棋、围棋和日本将棋中取得了超越人类的表现。AlphaZero使用了一种叫做蒙特卡洛树搜索（MCTS）的算法来预测和评估可能的走法。

研究方法：

他们首先从网上搜集了1000万局棋赛的数据，然后用一个非常强大的国际象棋程序（Stockfish 16）来分析这些棋局，为每一个棋盘的每一步棋提供一个评分。这样就得到了大约150亿个数据点，用来训练他们的AI模型。

通过使用大型的Transformer模型和大量的国际象棋游戏数据进行训练，AI能够直接学习棋局中的模式和策略，而无需进行复杂的棋局搜索。

结果非常令人印象深刻：这个AI模型能够达到接近国际象棋大师级别的水平，而且在不使用任何搜索算法的情况下，还能解决复杂的棋局问题。

该模型在性能上甚至超越了AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。

（在国际象棋AI中，策略和价值判断密切配合，共同指导AI做出最佳决策。策略告诉AI它可以做什么，而价值判断则告诉AI哪些行动可能导致胜利。通过这两个组件，AI能够在没有人类直接指导的情况下，自主学习和提高自己的棋艺。）

这意味着AI可以仅通过观察当前棋盘的状态就做出高水平的决策，从而在与人类玩家的对弈中达到大师级别的表现。

这不仅是国际象棋AI领域的一个重大进步，也为使用AI解决其他复杂任务提供了新的可能性。

这项研究的意义：

1、技术创新：通过使用深度学习技术而不是传统的搜索算法来达到国际象棋大师级水平，这项研究展示了人工智能领域的一种重要技术进步。它证明了深度学习模型，特别是Transformer模型，能够在复杂的决策和策略游戏中学习和模拟高级人类智能。

2、计算效率：传统的国际象棋AI依赖于大规模的搜索树和复杂的启发式评估，这在计算上非常昂贵。这项研究通过直接从大量数据中学习决策过程，显著减少了计算需求，展示了一种更高效的方式来构建高水平的游戏AI。

3、AI泛化能力：这项研究不仅仅是关于国际象棋，它还展示了深度学习模型在没有专门设计的规则或搜索算法支持下，通过学习大量示例来泛化和解决复杂任务的能力。这为其他类型的游戏和决策制定任务提供了新的思路。

4、开拓新的应用领域：这项研究表明，类似的方法可以应用于其他需要复杂策略和决策的领域，比如自动驾驶、金融市场分析、复杂系统管理等。通过学习大量的历史数据，AI可以在这些领域内做出更加精准和高效的决策。

5、提升AI的理解和创造能力：通过在没有预定义搜索策略的情况下训练AI达到高水平的表现，这项研究为AI的自主学习和理解复杂系统提供了新的范例，同时也推动了AI在创造性任务上的应用，如生成艺术、音乐、文学作品等。

论文：https://arxiv.org/abs/2402.04494
PDF：https://arxiv.org/pdf/2402.04494.pdf

与AI下棋：https://lichess.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755840200260096210#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755840200260096210#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 06:25:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google DeepMind抛弃传统的搜索方法，使用Transformer模型，训练了一个AI模型来下象棋。<br />
<br />
该模型能够达到国际象棋大师级别的水平。甚至表现超过了AlphaZero。<br />
<br />
这说明Transformer模型，不仅能处理语言任务，还能够在复杂的决策和策略游戏中学习和模拟高级人类智能。<br />
<br />
该方法同时显著减少了计算需求。<br />
<br />
研究背景：<br />
<br />
在国际象棋AI的发展历史中，传统的方法通常依赖于搜索算法（比如alpha-beta剪枝）来预测和评估可能的移动，从而选择最佳的一步。——即考虑棋盘上所有可能的走法和结果——来决定下一步怎么走。<br />
<br />
这种方法虽然可以工作，但需要大量的计算资源。<br />
<br />
AlphaZero是由DeepMind开发的一种高级AI，它通过自我对弈学习棋类游戏的策略，并在国际象棋、围棋和日本将棋中取得了超越人类的表现。AlphaZero使用了一种叫做蒙特卡洛树搜索（MCTS）的算法来预测和评估可能的走法。<br />
<br />
研究方法：<br />
<br />
他们首先从网上搜集了1000万局棋赛的数据，然后用一个非常强大的国际象棋程序（Stockfish 16）来分析这些棋局，为每一个棋盘的每一步棋提供一个评分。这样就得到了大约150亿个数据点，用来训练他们的AI模型。<br />
<br />
通过使用大型的Transformer模型和大量的国际象棋游戏数据进行训练，AI能够直接学习棋局中的模式和策略，而无需进行复杂的棋局搜索。<br />
<br />
结果非常令人印象深刻：这个AI模型能够达到接近国际象棋大师级别的水平，而且在不使用任何搜索算法的情况下，还能解决复杂的棋局问题。<br />
<br />
该模型在性能上甚至超越了AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。<br />
<br />
（在国际象棋AI中，策略和价值判断密切配合，共同指导AI做出最佳决策。策略告诉AI它可以做什么，而价值判断则告诉AI哪些行动可能导致胜利。通过这两个组件，AI能够在没有人类直接指导的情况下，自主学习和提高自己的棋艺。）<br />
<br />
这意味着AI可以仅通过观察当前棋盘的状态就做出高水平的决策，从而在与人类玩家的对弈中达到大师级别的表现。<br />
<br />
这不仅是国际象棋AI领域的一个重大进步，也为使用AI解决其他复杂任务提供了新的可能性。<br />
<br />
这项研究的意义：<br />
<br />
1、技术创新：通过使用深度学习技术而不是传统的搜索算法来达到国际象棋大师级水平，这项研究展示了人工智能领域的一种重要技术进步。它证明了深度学习模型，特别是Transformer模型，能够在复杂的决策和策略游戏中学习和模拟高级人类智能。<br />
<br />
2、计算效率：传统的国际象棋AI依赖于大规模的搜索树和复杂的启发式评估，这在计算上非常昂贵。这项研究通过直接从大量数据中学习决策过程，显著减少了计算需求，展示了一种更高效的方式来构建高水平的游戏AI。<br />
<br />
3、AI泛化能力：这项研究不仅仅是关于国际象棋，它还展示了深度学习模型在没有专门设计的规则或搜索算法支持下，通过学习大量示例来泛化和解决复杂任务的能力。这为其他类型的游戏和决策制定任务提供了新的思路。<br />
<br />
4、开拓新的应用领域：这项研究表明，类似的方法可以应用于其他需要复杂策略和决策的领域，比如自动驾驶、金融市场分析、复杂系统管理等。通过学习大量的历史数据，AI可以在这些领域内做出更加精准和高效的决策。<br />
<br />
5、提升AI的理解和创造能力：通过在没有预定义搜索策略的情况下训练AI达到高水平的表现，这项研究为AI的自主学习和理解复杂系统提供了新的范例，同时也推动了AI在创造性任务上的应用，如生成艺术、音乐、文学作品等。<br />
<br />
论文：<a href="https://arxiv.org/abs/2402.04494">arxiv.org/abs/2402.04494</a><br />
PDF：<a href="https://arxiv.org/pdf/2402.04494.pdf">arxiv.org/pdf/2402.04494.pdf</a><br />
<br />
与AI下棋：<a href="https://lichess.org/">lichess.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzLW5wbGJ3QUFCRGxyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755832179014484162#m</id>
            <title>Vercel将9个AI集成到了一起

还创建了一个新Model Playground ，你可以在一个界面尝试数十种模型，以生成文本、图像、音频等内容。

Vercel为AI应用提供了丰富的产品基础设施，从增强客户服务流程的聊天机器人到带有语义搜索的推荐系统、检索增强生成（RAG）和生成图像服务...

为了让这一切更加简单，Vercel还提供了一套工具（AI SDK），帮助开发者在他们的网站上快速使用这些AI功能。比如，如果你想让你的网站能自动回答用户的问题，只需要几行代码就可以实现。

首批集成的9个AI：

◆ @perplexity_ai
◆ @replicate
◆ @pinecone
◆ @modal_labs
◆ >@fal_ai_data
◆ @lmnt_com
◆ @togethercompute
◆ @elevenlabsio
◆ @anyscalecompute

详细：https://vercel.com/blog/ai-integrations

体验：https://vercel.com/ai</title>
            <link>https://nitter.cz/xiaohuggg/status/1755832179014484162#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755832179014484162#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 05:53:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vercel将9个AI集成到了一起<br />
<br />
还创建了一个新Model Playground ，你可以在一个界面尝试数十种模型，以生成文本、图像、音频等内容。<br />
<br />
Vercel为AI应用提供了丰富的产品基础设施，从增强客户服务流程的聊天机器人到带有语义搜索的推荐系统、检索增强生成（RAG）和生成图像服务...<br />
<br />
为了让这一切更加简单，Vercel还提供了一套工具（AI SDK），帮助开发者在他们的网站上快速使用这些AI功能。比如，如果你想让你的网站能自动回答用户的问题，只需要几行代码就可以实现。<br />
<br />
首批集成的9个AI：<br />
<br />
◆ <a href="https://nitter.cz/perplexity_ai" title="Perplexity">@perplexity_ai</a><br />
◆ <a href="https://nitter.cz/replicate" title="Replicate">@replicate</a><br />
◆ <a href="https://nitter.cz/pinecone" title="Pinecone">@pinecone</a><br />
◆ <a href="https://nitter.cz/modal_labs" title="Modal Labs">@modal_labs</a><br />
◆ <a href="https://nitter.cz/fal_ai_data" title="fal (Features &amp; Labels)">@fal_ai_data</a><br />
◆ <a href="https://nitter.cz/lmnt_com" title="LMNT">@lmnt_com</a><br />
◆ <a href="https://nitter.cz/togethercompute" title="Together AI">@togethercompute</a><br />
◆ <a href="https://nitter.cz/elevenlabsio" title="ElevenLabs">@elevenlabsio</a><br />
◆ <a href="https://nitter.cz/anyscalecompute" title="Anyscale">@anyscalecompute</a><br />
<br />
详细：<a href="https://vercel.com/blog/ai-integrations">vercel.com/blog/ai-integrati…</a><br />
<br />
体验：<a href="https://vercel.com/ai">vercel.com/ai</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzMUJtNGJvQUFaXzAtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzMUZJcmEwQUFoRVZ3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755824687811346514#m</id>
            <title>ComfyUI 3D Pack：ComfyUI现在能够处理3D图像了

可以将图片很快的转换成一个3D模型，在RTX3080 GPU上不到30秒。

可以直接看到3D模型，还能自动创建不同的相机角度，帮助你从各种方向查看3D模型。

通过3D高斯扩散改善模型质量。让3D模型看起来更真实，更有立体感。

支持多种格式导出。

方法：

集成了多种先进的3D处理算法，如大视图高斯模型（LGM）、三平面高斯变换器（Triplane Gaussian Transformers）等。

提供了一套工具和工作流，以便用户能够轻松地将2D图像转换为3D模型，并进行进一步的处理和优化。

作者：@MrForExample 
GitHub：https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main</title>
            <link>https://nitter.cz/xiaohuggg/status/1755824687811346514#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755824687811346514#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 05:23:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI 3D Pack：ComfyUI现在能够处理3D图像了<br />
<br />
可以将图片很快的转换成一个3D模型，在RTX3080 GPU上不到30秒。<br />
<br />
可以直接看到3D模型，还能自动创建不同的相机角度，帮助你从各种方向查看3D模型。<br />
<br />
通过3D高斯扩散改善模型质量。让3D模型看起来更真实，更有立体感。<br />
<br />
支持多种格式导出。<br />
<br />
方法：<br />
<br />
集成了多种先进的3D处理算法，如大视图高斯模型（LGM）、三平面高斯变换器（Triplane Gaussian Transformers）等。<br />
<br />
提供了一套工具和工作流，以便用户能够轻松地将2D图像转换为3D模型，并进行进一步的处理和优化。<br />
<br />
作者：<a href="https://nitter.cz/MrForExample" title="Mr. For Example">@MrForExample</a> <br />
GitHub：<a href="https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main">github.com/MrForExample/Comf…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU4MjQ0MzQxMDIwNzEyOTYvcHUvaW1nL0REV0pRbk1abm5TRm1CRXEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</id>
            <title>🔔http://Xiaohu.AI日报「2月8日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755619904743690629#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 15:49:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月8日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwMl9aYmJBQUFmc2puLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</id>
            <title>R to @xiaohuggg: Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。

他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。

Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</title>
            <link>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755607353167401134#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:59:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。<br />
<br />
他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。<br />
<br />
Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</p>
<p><a href="https://nitter.cz/emollick/status/1755596564817699049#m">nitter.cz/emollick/status/1755596564817699049#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</id>
            <title>R to @xiaohuggg: Gemini Advanced权益

• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！

• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。

• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。

• 其他权益: 包括2TB的Google One存储空间等会员权益。</title>
            <link>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755605438882894012#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:52:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Advanced权益<br />
<br />
• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！<br />
<br />
• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。<br />
<br />
• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。<br />
<br />
• 其他权益: 包括2TB的Google One存储空间等会员权益。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwcDFhRmFFQUFPdGhKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</id>
            <title>Gemini Ultra 上线  Bard 正式更名为 Gemini 

更新内容和之前泄漏的一样

核心要点：

- Gemini Ultra上线，Bard更名为Gemini

- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。

- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。

- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。

- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。

详细：https://gemini.google.com/updates</title>
            <link>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755600097239536034#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Ultra 上线  Bard 正式更名为 Gemini <br />
<br />
更新内容和之前泄漏的一样<br />
<br />
核心要点：<br />
<br />
- Gemini Ultra上线，Bard更名为Gemini<br />
<br />
- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。<br />
<br />
- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。<br />
<br />
- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。<br />
<br />
- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。<br />
<br />
详细：<a href="https://gemini.google.com/updates">gemini.google.com/updates</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Yway1iQ2IwQUF0M2ZULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</id>
            <title>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。

例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。

这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。

详细：https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app</title>
            <link>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755587992067125449#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 13:42:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。<br />
<br />
例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。<br />
<br />
这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。<br />
<br />
详细：<a href="https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app">theinformation.com/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwWjk3b2FrQUEyd2FjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755565282285015484#m</id>
            <title>Stability AI发布Stable Audio AudioSparx 1.0 音乐模型

- 高效生成长格式音频：根据文字提示，快速生成长达95秒的44.1kHz立体声音乐和声音。

- 可变长度的音频输出：实现对生成音频的内容和长度进行精细控制，支持可变长度的音频输出。

- 立体声音频渲染：能够渲染立体声信号，提供丰富和深度的音频体验。

- 快速推理时间：在A100 GPU上仅需8秒即可生成长达95秒的立体声音频，显示出极高的计算效率。

- 结构化音乐生成：不像其他工具那样随机制作，这个工具能够根据你的文字提示，制作出有明确结构的音乐，比如有开头、中间发展和结尾，让音乐听起来更有感觉。

- 性能优于 AudioLDM2 和 MusicGen——请查看论文中的指标。

解决的问题：

提高了长格式音频的生成效率，克服了固定大小输出的限制，允许生成可变长度的音频。

通过潜在扩散模型和时间条件化，实现了对生成音频长度的精细控制，同时保持了计算效率。

论文： https://arxiv.org/abs/2402.04825  
代码： https://github.com/Stability-AI/stable-audio-tools
指标： https://github.com/Stability-AI/stable-audio-metrics
演示： https://stability-ai.github.io/stable-audio-demo/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755565282285015484#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755565282285015484#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 12:12:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI发布Stable Audio AudioSparx 1.0 音乐模型<br />
<br />
- 高效生成长格式音频：根据文字提示，快速生成长达95秒的44.1kHz立体声音乐和声音。<br />
<br />
- 可变长度的音频输出：实现对生成音频的内容和长度进行精细控制，支持可变长度的音频输出。<br />
<br />
- 立体声音频渲染：能够渲染立体声信号，提供丰富和深度的音频体验。<br />
<br />
- 快速推理时间：在A100 GPU上仅需8秒即可生成长达95秒的立体声音频，显示出极高的计算效率。<br />
<br />
- 结构化音乐生成：不像其他工具那样随机制作，这个工具能够根据你的文字提示，制作出有明确结构的音乐，比如有开头、中间发展和结尾，让音乐听起来更有感觉。<br />
<br />
- 性能优于 AudioLDM2 和 MusicGen——请查看论文中的指标。<br />
<br />
解决的问题：<br />
<br />
提高了长格式音频的生成效率，克服了固定大小输出的限制，允许生成可变长度的音频。<br />
<br />
通过潜在扩散模型和时间条件化，实现了对生成音频长度的精细控制，同时保持了计算效率。<br />
<br />
论文： <a href="https://arxiv.org/abs/2402.04825">arxiv.org/abs/2402.04825</a>  <br />
代码： <a href="https://github.com/Stability-AI/stable-audio-tools">github.com/Stability-AI/stab…</a><br />
指标： <a href="https://github.com/Stability-AI/stable-audio-metrics">github.com/Stability-AI/stab…</a><br />
演示： <a href="https://stability-ai.github.io/stable-audio-demo/">stability-ai.github.io/stabl…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU1NjUxNjQxMzk4MjMxMDUvcHUvaW1nL0hMVjRhQ3pra0Qtak9wbU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755558069143306328#m</id>
            <title>Vision Pro拆解第二部分：显示分辨率是多少

在iFixit对Vision Pro的初步拆解之后，他们对这款设备的双显示屏、多个传感器、透镜和复杂设计的电池包进行了更深入的研究。

他们发现，Vision Pro的双显示屏非常惊人：你可以在一个iPhone 15 Pro像素的空间里放下50多个Vision Pro像素。

显示屏分辨率：

- Apple声称Vision Pro的每只眼睛的显示屏拥有“超过4K电视的像素数”。但是，当显示屏如此接近你的眼球时，4K或任何K的概念意味着什么呢？

- 显示屏的实际亮区为27.5mm宽乘24mm高，大约为一英寸见方。使用Evident Scientific DSX1000显微镜测量，每个像素为7.5微米，大约是一个红血球的大小。每个像素大致为方形，红色和绿色子像素堆叠在一起，蓝色子像素大小加倍并位于一侧。

- 根据这些测量值，亮区的总像素数为3660px乘3200px，相当于0.98平方英寸内压缩了12,078,000像素。

像素密度（PPI）与每度像素（PPD）：

- 视网膜的像素密度（PPI）达到了惊人的3,386 PPI，而iPhone 15 Pro Max的PPI约为460，这意味着Vision Pro的像素密度是iPhone的约54倍。

- 虽然Vision Pro的水平分辨率没有达到消费级4K UHD标准的3840像素宽，但它仍然是目前看到的最高密度显示屏。

- VR工程师更喜欢使用“每度像素”（PPD）这一指标来衡量显示屏的“优良程度”，这是一个考虑到观看距离的视角中水平像素的数量。Vision Pro的PPD大致估算为34，而65英寸4K电视从6.5英尺远处观看时的平均PPD为95，iPhone 15 Pro Max从1英尺远处观看时的平均PPD也为94。

电池与可维修性：

- Vision Pro使用了一个复杂且过度设计的电池解决方案，如果单独购买，该电池包的价格为200美元。电池包由三个类似iPhone电池大小的电池组堆叠而成，总容量为46.08Wh，但外壳上标记的容量为35.9Wh，这表明Apple可能出于延长电池寿命的目的而故意低估了容量。

- Vision Pro的电池包设计考虑了用户体验，内置温度传感器和加速度计，并采用非标准的13伏输出来满足Vision Pro的处理需求。

总的来说，Vision Pro的显示屏拥有超高的分辨率（PPI），但由于非常靠近眼睛，它的角分辨率较低。尽管如此，Vision Pro提供了迄今为止所见过的最高密度显示体验，展示了Apple在高科技领域的领先地位。

详细：https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution</title>
            <link>https://nitter.cz/xiaohuggg/status/1755558069143306328#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755558069143306328#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 11:43:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vision Pro拆解第二部分：显示分辨率是多少<br />
<br />
在iFixit对Vision Pro的初步拆解之后，他们对这款设备的双显示屏、多个传感器、透镜和复杂设计的电池包进行了更深入的研究。<br />
<br />
他们发现，Vision Pro的双显示屏非常惊人：你可以在一个iPhone 15 Pro像素的空间里放下50多个Vision Pro像素。<br />
<br />
显示屏分辨率：<br />
<br />
- Apple声称Vision Pro的每只眼睛的显示屏拥有“超过4K电视的像素数”。但是，当显示屏如此接近你的眼球时，4K或任何K的概念意味着什么呢？<br />
<br />
- 显示屏的实际亮区为27.5mm宽乘24mm高，大约为一英寸见方。使用Evident Scientific DSX1000显微镜测量，每个像素为7.5微米，大约是一个红血球的大小。每个像素大致为方形，红色和绿色子像素堆叠在一起，蓝色子像素大小加倍并位于一侧。<br />
<br />
- 根据这些测量值，亮区的总像素数为3660px乘3200px，相当于0.98平方英寸内压缩了12,078,000像素。<br />
<br />
像素密度（PPI）与每度像素（PPD）：<br />
<br />
- 视网膜的像素密度（PPI）达到了惊人的3,386 PPI，而iPhone 15 Pro Max的PPI约为460，这意味着Vision Pro的像素密度是iPhone的约54倍。<br />
<br />
- 虽然Vision Pro的水平分辨率没有达到消费级4K UHD标准的3840像素宽，但它仍然是目前看到的最高密度显示屏。<br />
<br />
- VR工程师更喜欢使用“每度像素”（PPD）这一指标来衡量显示屏的“优良程度”，这是一个考虑到观看距离的视角中水平像素的数量。Vision Pro的PPD大致估算为34，而65英寸4K电视从6.5英尺远处观看时的平均PPD为95，iPhone 15 Pro Max从1英尺远处观看时的平均PPD也为94。<br />
<br />
电池与可维修性：<br />
<br />
- Vision Pro使用了一个复杂且过度设计的电池解决方案，如果单独购买，该电池包的价格为200美元。电池包由三个类似iPhone电池大小的电池组堆叠而成，总容量为46.08Wh，但外壳上标记的容量为35.9Wh，这表明Apple可能出于延长电池寿命的目的而故意低估了容量。<br />
<br />
- Vision Pro的电池包设计考虑了用户体验，内置温度传感器和加速度计，并采用非标准的13伏输出来满足Vision Pro的处理需求。<br />
<br />
总的来说，Vision Pro的显示屏拥有超高的分辨率（PPI），但由于非常靠近眼睛，它的角分辨率较低。尽管如此，Vision Pro提供了迄今为止所见过的最高密度显示体验，展示了Apple在高科技领域的领先地位。<br />
<br />
详细：<a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution">ifixit.com/News/90409/vision…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU1MjEyODE2NDk0MTAwNDgvcHUvaW1nL0FDYVhHNkptbzl5WC1EaUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>