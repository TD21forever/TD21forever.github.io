<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</id>
            <title>Happy Chinese New Year！

Every One…

🐲🧨🇨🇳</title>
            <link>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755985073747447827#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 16:00:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Happy Chinese New Year！<br />
<br />
Every One…<br />
<br />
🐲🧨🇨🇳</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y2REhLRGJNQUFxaHNsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755911511531520464#m</id>
            <title>做好了

自己做的

开吃🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1755911511531520464#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755911511531520464#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 11:08:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>做好了<br />
<br />
自己做的<br />
<br />
开吃🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1ySWFvQUExZHVSLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1yRWFBQUFkSkFoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1zQWJBQUFoUzNnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1yR2E0QUF3b1BVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755850647667388813#m</id>
            <title>开始做年夜饭了

不更新了 

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1755850647667388813#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755850647667388813#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 07:06:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开始做年夜饭了<br />
<br />
不更新了 <br />
<br />
😂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755840200260096210#m</id>
            <title>Google DeepMind抛弃传统的搜索方法，使用Transformer模型，训练了一个AI模型来下象棋。

该模型能够达到国际象棋大师级别的水平。甚至表现超过了AlphaZero。

这说明Transformer模型，不仅能处理语言任务，还能够在复杂的决策和策略游戏中学习和模拟高级人类智能。

该方法同时显著减少了计算需求。

研究背景：

在国际象棋AI的发展历史中，传统的方法通常依赖于搜索算法（比如alpha-beta剪枝）来预测和评估可能的移动，从而选择最佳的一步。——即考虑棋盘上所有可能的走法和结果——来决定下一步怎么走。

这种方法虽然可以工作，但需要大量的计算资源。

AlphaZero是由DeepMind开发的一种高级AI，它通过自我对弈学习棋类游戏的策略，并在国际象棋、围棋和日本将棋中取得了超越人类的表现。AlphaZero使用了一种叫做蒙特卡洛树搜索（MCTS）的算法来预测和评估可能的走法。

研究方法：

他们首先从网上搜集了1000万局棋赛的数据，然后用一个非常强大的国际象棋程序（Stockfish 16）来分析这些棋局，为每一个棋盘的每一步棋提供一个评分。这样就得到了大约150亿个数据点，用来训练他们的AI模型。

通过使用大型的Transformer模型和大量的国际象棋游戏数据进行训练，AI能够直接学习棋局中的模式和策略，而无需进行复杂的棋局搜索。

结果非常令人印象深刻：这个AI模型能够达到接近国际象棋大师级别的水平，而且在不使用任何搜索算法的情况下，还能解决复杂的棋局问题。

该模型在性能上甚至超越了AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。

（在国际象棋AI中，策略和价值判断密切配合，共同指导AI做出最佳决策。策略告诉AI它可以做什么，而价值判断则告诉AI哪些行动可能导致胜利。通过这两个组件，AI能够在没有人类直接指导的情况下，自主学习和提高自己的棋艺。）

这意味着AI可以仅通过观察当前棋盘的状态就做出高水平的决策，从而在与人类玩家的对弈中达到大师级别的表现。

这不仅是国际象棋AI领域的一个重大进步，也为使用AI解决其他复杂任务提供了新的可能性。

这项研究的意义：

1、技术创新：通过使用深度学习技术而不是传统的搜索算法来达到国际象棋大师级水平，这项研究展示了人工智能领域的一种重要技术进步。它证明了深度学习模型，特别是Transformer模型，能够在复杂的决策和策略游戏中学习和模拟高级人类智能。

2、计算效率：传统的国际象棋AI依赖于大规模的搜索树和复杂的启发式评估，这在计算上非常昂贵。这项研究通过直接从大量数据中学习决策过程，显著减少了计算需求，展示了一种更高效的方式来构建高水平的游戏AI。

3、AI泛化能力：这项研究不仅仅是关于国际象棋，它还展示了深度学习模型在没有专门设计的规则或搜索算法支持下，通过学习大量示例来泛化和解决复杂任务的能力。这为其他类型的游戏和决策制定任务提供了新的思路。

4、开拓新的应用领域：这项研究表明，类似的方法可以应用于其他需要复杂策略和决策的领域，比如自动驾驶、金融市场分析、复杂系统管理等。通过学习大量的历史数据，AI可以在这些领域内做出更加精准和高效的决策。

5、提升AI的理解和创造能力：通过在没有预定义搜索策略的情况下训练AI达到高水平的表现，这项研究为AI的自主学习和理解复杂系统提供了新的范例，同时也推动了AI在创造性任务上的应用，如生成艺术、音乐、文学作品等。

论文：https://arxiv.org/abs/2402.04494
PDF：https://arxiv.org/pdf/2402.04494.pdf

与AI下棋：https://lichess.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755840200260096210#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755840200260096210#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 06:25:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google DeepMind抛弃传统的搜索方法，使用Transformer模型，训练了一个AI模型来下象棋。<br />
<br />
该模型能够达到国际象棋大师级别的水平。甚至表现超过了AlphaZero。<br />
<br />
这说明Transformer模型，不仅能处理语言任务，还能够在复杂的决策和策略游戏中学习和模拟高级人类智能。<br />
<br />
该方法同时显著减少了计算需求。<br />
<br />
研究背景：<br />
<br />
在国际象棋AI的发展历史中，传统的方法通常依赖于搜索算法（比如alpha-beta剪枝）来预测和评估可能的移动，从而选择最佳的一步。——即考虑棋盘上所有可能的走法和结果——来决定下一步怎么走。<br />
<br />
这种方法虽然可以工作，但需要大量的计算资源。<br />
<br />
AlphaZero是由DeepMind开发的一种高级AI，它通过自我对弈学习棋类游戏的策略，并在国际象棋、围棋和日本将棋中取得了超越人类的表现。AlphaZero使用了一种叫做蒙特卡洛树搜索（MCTS）的算法来预测和评估可能的走法。<br />
<br />
研究方法：<br />
<br />
他们首先从网上搜集了1000万局棋赛的数据，然后用一个非常强大的国际象棋程序（Stockfish 16）来分析这些棋局，为每一个棋盘的每一步棋提供一个评分。这样就得到了大约150亿个数据点，用来训练他们的AI模型。<br />
<br />
通过使用大型的Transformer模型和大量的国际象棋游戏数据进行训练，AI能够直接学习棋局中的模式和策略，而无需进行复杂的棋局搜索。<br />
<br />
结果非常令人印象深刻：这个AI模型能够达到接近国际象棋大师级别的水平，而且在不使用任何搜索算法的情况下，还能解决复杂的棋局问题。<br />
<br />
该模型在性能上甚至超越了AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。<br />
<br />
（在国际象棋AI中，策略和价值判断密切配合，共同指导AI做出最佳决策。策略告诉AI它可以做什么，而价值判断则告诉AI哪些行动可能导致胜利。通过这两个组件，AI能够在没有人类直接指导的情况下，自主学习和提高自己的棋艺。）<br />
<br />
这意味着AI可以仅通过观察当前棋盘的状态就做出高水平的决策，从而在与人类玩家的对弈中达到大师级别的表现。<br />
<br />
这不仅是国际象棋AI领域的一个重大进步，也为使用AI解决其他复杂任务提供了新的可能性。<br />
<br />
这项研究的意义：<br />
<br />
1、技术创新：通过使用深度学习技术而不是传统的搜索算法来达到国际象棋大师级水平，这项研究展示了人工智能领域的一种重要技术进步。它证明了深度学习模型，特别是Transformer模型，能够在复杂的决策和策略游戏中学习和模拟高级人类智能。<br />
<br />
2、计算效率：传统的国际象棋AI依赖于大规模的搜索树和复杂的启发式评估，这在计算上非常昂贵。这项研究通过直接从大量数据中学习决策过程，显著减少了计算需求，展示了一种更高效的方式来构建高水平的游戏AI。<br />
<br />
3、AI泛化能力：这项研究不仅仅是关于国际象棋，它还展示了深度学习模型在没有专门设计的规则或搜索算法支持下，通过学习大量示例来泛化和解决复杂任务的能力。这为其他类型的游戏和决策制定任务提供了新的思路。<br />
<br />
4、开拓新的应用领域：这项研究表明，类似的方法可以应用于其他需要复杂策略和决策的领域，比如自动驾驶、金融市场分析、复杂系统管理等。通过学习大量的历史数据，AI可以在这些领域内做出更加精准和高效的决策。<br />
<br />
5、提升AI的理解和创造能力：通过在没有预定义搜索策略的情况下训练AI达到高水平的表现，这项研究为AI的自主学习和理解复杂系统提供了新的范例，同时也推动了AI在创造性任务上的应用，如生成艺术、音乐、文学作品等。<br />
<br />
论文：<a href="https://arxiv.org/abs/2402.04494">arxiv.org/abs/2402.04494</a><br />
PDF：<a href="https://arxiv.org/pdf/2402.04494.pdf">arxiv.org/pdf/2402.04494.pdf</a><br />
<br />
与AI下棋：<a href="https://lichess.org/">lichess.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzLW5wbGJ3QUFCRGxyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755832179014484162#m</id>
            <title>Vercel将9个AI集成到了一起

还创建了一个新Model Playground ，你可以在一个界面尝试数十种模型，以生成文本、图像、音频等内容。

Vercel为AI应用提供了丰富的产品基础设施，从增强客户服务流程的聊天机器人到带有语义搜索的推荐系统、检索增强生成（RAG）和生成图像服务...

为了让这一切更加简单，Vercel还提供了一套工具（AI SDK），帮助开发者在他们的网站上快速使用这些AI功能。比如，如果你想让你的网站能自动回答用户的问题，只需要几行代码就可以实现。

首批集成的9个AI：

◆ @perplexity_ai
◆ @replicate
◆ @pinecone
◆ @modal_labs
◆ >@fal_ai_data
◆ @lmnt_com
◆ @togethercompute
◆ @elevenlabsio
◆ @anyscalecompute

详细：https://vercel.com/blog/ai-integrations

体验：https://vercel.com/ai</title>
            <link>https://nitter.cz/xiaohuggg/status/1755832179014484162#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755832179014484162#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 05:53:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vercel将9个AI集成到了一起<br />
<br />
还创建了一个新Model Playground ，你可以在一个界面尝试数十种模型，以生成文本、图像、音频等内容。<br />
<br />
Vercel为AI应用提供了丰富的产品基础设施，从增强客户服务流程的聊天机器人到带有语义搜索的推荐系统、检索增强生成（RAG）和生成图像服务...<br />
<br />
为了让这一切更加简单，Vercel还提供了一套工具（AI SDK），帮助开发者在他们的网站上快速使用这些AI功能。比如，如果你想让你的网站能自动回答用户的问题，只需要几行代码就可以实现。<br />
<br />
首批集成的9个AI：<br />
<br />
◆ <a href="https://nitter.cz/perplexity_ai" title="Perplexity">@perplexity_ai</a><br />
◆ <a href="https://nitter.cz/replicate" title="Replicate">@replicate</a><br />
◆ <a href="https://nitter.cz/pinecone" title="Pinecone">@pinecone</a><br />
◆ <a href="https://nitter.cz/modal_labs" title="Modal Labs">@modal_labs</a><br />
◆ <a href="https://nitter.cz/fal_ai_data" title="fal (Features &amp; Labels)">@fal_ai_data</a><br />
◆ <a href="https://nitter.cz/lmnt_com" title="LMNT">@lmnt_com</a><br />
◆ <a href="https://nitter.cz/togethercompute" title="Together AI">@togethercompute</a><br />
◆ <a href="https://nitter.cz/elevenlabsio" title="ElevenLabs">@elevenlabsio</a><br />
◆ <a href="https://nitter.cz/anyscalecompute" title="Anyscale">@anyscalecompute</a><br />
<br />
详细：<a href="https://vercel.com/blog/ai-integrations">vercel.com/blog/ai-integrati…</a><br />
<br />
体验：<a href="https://vercel.com/ai">vercel.com/ai</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzMUJtNGJvQUFaXzAtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzMUZJcmEwQUFoRVZ3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755824687811346514#m</id>
            <title>ComfyUI 3D Pack：ComfyUI现在能够处理3D图像了

可以将图片很快的转换成一个3D模型，在RTX3080 GPU上不到30秒。

可以直接看到3D模型，还能自动创建不同的相机角度，帮助你从各种方向查看3D模型。

通过3D高斯扩散改善模型质量。让3D模型看起来更真实，更有立体感。

支持多种格式导出。

方法：

集成了多种先进的3D处理算法，如大视图高斯模型（LGM）、三平面高斯变换器（Triplane Gaussian Transformers）等。

提供了一套工具和工作流，以便用户能够轻松地将2D图像转换为3D模型，并进行进一步的处理和优化。

作者：@MrForExample 
GitHub：https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main</title>
            <link>https://nitter.cz/xiaohuggg/status/1755824687811346514#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755824687811346514#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 05:23:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI 3D Pack：ComfyUI现在能够处理3D图像了<br />
<br />
可以将图片很快的转换成一个3D模型，在RTX3080 GPU上不到30秒。<br />
<br />
可以直接看到3D模型，还能自动创建不同的相机角度，帮助你从各种方向查看3D模型。<br />
<br />
通过3D高斯扩散改善模型质量。让3D模型看起来更真实，更有立体感。<br />
<br />
支持多种格式导出。<br />
<br />
方法：<br />
<br />
集成了多种先进的3D处理算法，如大视图高斯模型（LGM）、三平面高斯变换器（Triplane Gaussian Transformers）等。<br />
<br />
提供了一套工具和工作流，以便用户能够轻松地将2D图像转换为3D模型，并进行进一步的处理和优化。<br />
<br />
作者：<a href="https://nitter.cz/MrForExample" title="Mr. For Example">@MrForExample</a> <br />
GitHub：<a href="https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main">github.com/MrForExample/Comf…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU4MjQ0MzQxMDIwNzEyOTYvcHUvaW1nL0REV0pRbk1abm5TRm1CRXEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</id>
            <title>🔔http://Xiaohu.AI日报「2月8日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755619904743690629#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 15:49:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月8日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwMl9aYmJBQUFmc2puLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</id>
            <title>R to @xiaohuggg: Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。

他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。

Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</title>
            <link>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755607353167401134#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:59:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。<br />
<br />
他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。<br />
<br />
Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</p>
<p><a href="https://nitter.cz/emollick/status/1755596564817699049#m">nitter.cz/emollick/status/1755596564817699049#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</id>
            <title>R to @xiaohuggg: Gemini Advanced权益

• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！

• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。

• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。

• 其他权益: 包括2TB的Google One存储空间等会员权益。</title>
            <link>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755605438882894012#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:52:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Advanced权益<br />
<br />
• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！<br />
<br />
• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。<br />
<br />
• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。<br />
<br />
• 其他权益: 包括2TB的Google One存储空间等会员权益。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwcDFhRmFFQUFPdGhKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</id>
            <title>Gemini Ultra 上线  Bard 正式更名为 Gemini 

更新内容和之前泄漏的一样

核心要点：

- Gemini Ultra上线，Bard更名为Gemini

- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。

- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。

- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。

- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。

详细：https://gemini.google.com/updates</title>
            <link>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755600097239536034#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Ultra 上线  Bard 正式更名为 Gemini <br />
<br />
更新内容和之前泄漏的一样<br />
<br />
核心要点：<br />
<br />
- Gemini Ultra上线，Bard更名为Gemini<br />
<br />
- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。<br />
<br />
- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。<br />
<br />
- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。<br />
<br />
- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。<br />
<br />
详细：<a href="https://gemini.google.com/updates">gemini.google.com/updates</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Yway1iQ2IwQUF0M2ZULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</id>
            <title>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。

例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。

这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。

详细：https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app</title>
            <link>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755587992067125449#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 13:42:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。<br />
<br />
例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。<br />
<br />
这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。<br />
<br />
详细：<a href="https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app">theinformation.com/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwWjk3b2FrQUEyd2FjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755565282285015484#m</id>
            <title>Stability AI发布Stable Audio AudioSparx 1.0 音乐模型

- 高效生成长格式音频：根据文字提示，快速生成长达95秒的44.1kHz立体声音乐和声音。

- 可变长度的音频输出：实现对生成音频的内容和长度进行精细控制，支持可变长度的音频输出。

- 立体声音频渲染：能够渲染立体声信号，提供丰富和深度的音频体验。

- 快速推理时间：在A100 GPU上仅需8秒即可生成长达95秒的立体声音频，显示出极高的计算效率。

- 结构化音乐生成：不像其他工具那样随机制作，这个工具能够根据你的文字提示，制作出有明确结构的音乐，比如有开头、中间发展和结尾，让音乐听起来更有感觉。

- 性能优于 AudioLDM2 和 MusicGen——请查看论文中的指标。

解决的问题：

提高了长格式音频的生成效率，克服了固定大小输出的限制，允许生成可变长度的音频。

通过潜在扩散模型和时间条件化，实现了对生成音频长度的精细控制，同时保持了计算效率。

论文： https://arxiv.org/abs/2402.04825  
代码： https://github.com/Stability-AI/stable-audio-tools
指标： https://github.com/Stability-AI/stable-audio-metrics
演示： https://stability-ai.github.io/stable-audio-demo/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755565282285015484#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755565282285015484#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 12:12:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI发布Stable Audio AudioSparx 1.0 音乐模型<br />
<br />
- 高效生成长格式音频：根据文字提示，快速生成长达95秒的44.1kHz立体声音乐和声音。<br />
<br />
- 可变长度的音频输出：实现对生成音频的内容和长度进行精细控制，支持可变长度的音频输出。<br />
<br />
- 立体声音频渲染：能够渲染立体声信号，提供丰富和深度的音频体验。<br />
<br />
- 快速推理时间：在A100 GPU上仅需8秒即可生成长达95秒的立体声音频，显示出极高的计算效率。<br />
<br />
- 结构化音乐生成：不像其他工具那样随机制作，这个工具能够根据你的文字提示，制作出有明确结构的音乐，比如有开头、中间发展和结尾，让音乐听起来更有感觉。<br />
<br />
- 性能优于 AudioLDM2 和 MusicGen——请查看论文中的指标。<br />
<br />
解决的问题：<br />
<br />
提高了长格式音频的生成效率，克服了固定大小输出的限制，允许生成可变长度的音频。<br />
<br />
通过潜在扩散模型和时间条件化，实现了对生成音频长度的精细控制，同时保持了计算效率。<br />
<br />
论文： <a href="https://arxiv.org/abs/2402.04825">arxiv.org/abs/2402.04825</a>  <br />
代码： <a href="https://github.com/Stability-AI/stable-audio-tools">github.com/Stability-AI/stab…</a><br />
指标： <a href="https://github.com/Stability-AI/stable-audio-metrics">github.com/Stability-AI/stab…</a><br />
演示： <a href="https://stability-ai.github.io/stable-audio-demo/">stability-ai.github.io/stabl…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU1NjUxNjQxMzk4MjMxMDUvcHUvaW1nL0hMVjRhQ3pra0Qtak9wbU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755558069143306328#m</id>
            <title>Vision Pro拆解第二部分：显示分辨率是多少

在iFixit对Vision Pro的初步拆解之后，他们对这款设备的双显示屏、多个传感器、透镜和复杂设计的电池包进行了更深入的研究。

他们发现，Vision Pro的双显示屏非常惊人：你可以在一个iPhone 15 Pro像素的空间里放下50多个Vision Pro像素。

显示屏分辨率：

- Apple声称Vision Pro的每只眼睛的显示屏拥有“超过4K电视的像素数”。但是，当显示屏如此接近你的眼球时，4K或任何K的概念意味着什么呢？

- 显示屏的实际亮区为27.5mm宽乘24mm高，大约为一英寸见方。使用Evident Scientific DSX1000显微镜测量，每个像素为7.5微米，大约是一个红血球的大小。每个像素大致为方形，红色和绿色子像素堆叠在一起，蓝色子像素大小加倍并位于一侧。

- 根据这些测量值，亮区的总像素数为3660px乘3200px，相当于0.98平方英寸内压缩了12,078,000像素。

像素密度（PPI）与每度像素（PPD）：

- 视网膜的像素密度（PPI）达到了惊人的3,386 PPI，而iPhone 15 Pro Max的PPI约为460，这意味着Vision Pro的像素密度是iPhone的约54倍。

- 虽然Vision Pro的水平分辨率没有达到消费级4K UHD标准的3840像素宽，但它仍然是目前看到的最高密度显示屏。

- VR工程师更喜欢使用“每度像素”（PPD）这一指标来衡量显示屏的“优良程度”，这是一个考虑到观看距离的视角中水平像素的数量。Vision Pro的PPD大致估算为34，而65英寸4K电视从6.5英尺远处观看时的平均PPD为95，iPhone 15 Pro Max从1英尺远处观看时的平均PPD也为94。

电池与可维修性：

- Vision Pro使用了一个复杂且过度设计的电池解决方案，如果单独购买，该电池包的价格为200美元。电池包由三个类似iPhone电池大小的电池组堆叠而成，总容量为46.08Wh，但外壳上标记的容量为35.9Wh，这表明Apple可能出于延长电池寿命的目的而故意低估了容量。

- Vision Pro的电池包设计考虑了用户体验，内置温度传感器和加速度计，并采用非标准的13伏输出来满足Vision Pro的处理需求。

总的来说，Vision Pro的显示屏拥有超高的分辨率（PPI），但由于非常靠近眼睛，它的角分辨率较低。尽管如此，Vision Pro提供了迄今为止所见过的最高密度显示体验，展示了Apple在高科技领域的领先地位。

详细：https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution</title>
            <link>https://nitter.cz/xiaohuggg/status/1755558069143306328#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755558069143306328#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 11:43:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vision Pro拆解第二部分：显示分辨率是多少<br />
<br />
在iFixit对Vision Pro的初步拆解之后，他们对这款设备的双显示屏、多个传感器、透镜和复杂设计的电池包进行了更深入的研究。<br />
<br />
他们发现，Vision Pro的双显示屏非常惊人：你可以在一个iPhone 15 Pro像素的空间里放下50多个Vision Pro像素。<br />
<br />
显示屏分辨率：<br />
<br />
- Apple声称Vision Pro的每只眼睛的显示屏拥有“超过4K电视的像素数”。但是，当显示屏如此接近你的眼球时，4K或任何K的概念意味着什么呢？<br />
<br />
- 显示屏的实际亮区为27.5mm宽乘24mm高，大约为一英寸见方。使用Evident Scientific DSX1000显微镜测量，每个像素为7.5微米，大约是一个红血球的大小。每个像素大致为方形，红色和绿色子像素堆叠在一起，蓝色子像素大小加倍并位于一侧。<br />
<br />
- 根据这些测量值，亮区的总像素数为3660px乘3200px，相当于0.98平方英寸内压缩了12,078,000像素。<br />
<br />
像素密度（PPI）与每度像素（PPD）：<br />
<br />
- 视网膜的像素密度（PPI）达到了惊人的3,386 PPI，而iPhone 15 Pro Max的PPI约为460，这意味着Vision Pro的像素密度是iPhone的约54倍。<br />
<br />
- 虽然Vision Pro的水平分辨率没有达到消费级4K UHD标准的3840像素宽，但它仍然是目前看到的最高密度显示屏。<br />
<br />
- VR工程师更喜欢使用“每度像素”（PPD）这一指标来衡量显示屏的“优良程度”，这是一个考虑到观看距离的视角中水平像素的数量。Vision Pro的PPD大致估算为34，而65英寸4K电视从6.5英尺远处观看时的平均PPD为95，iPhone 15 Pro Max从1英尺远处观看时的平均PPD也为94。<br />
<br />
电池与可维修性：<br />
<br />
- Vision Pro使用了一个复杂且过度设计的电池解决方案，如果单独购买，该电池包的价格为200美元。电池包由三个类似iPhone电池大小的电池组堆叠而成，总容量为46.08Wh，但外壳上标记的容量为35.9Wh，这表明Apple可能出于延长电池寿命的目的而故意低估了容量。<br />
<br />
- Vision Pro的电池包设计考虑了用户体验，内置温度传感器和加速度计，并采用非标准的13伏输出来满足Vision Pro的处理需求。<br />
<br />
总的来说，Vision Pro的显示屏拥有超高的分辨率（PPI），但由于非常靠近眼睛，它的角分辨率较低。尽管如此，Vision Pro提供了迄今为止所见过的最高密度显示体验，展示了Apple在高科技领域的领先地位。<br />
<br />
详细：<a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution">ifixit.com/News/90409/vision…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU1MjEyODE2NDk0MTAwNDgvcHUvaW1nL0FDYVhHNkptbzl5WC1EaUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755533364986253680#m</id>
            <title>我有个大胆的想法

你说夜总会是不是可以引进

🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1755533364986253680#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755533364986253680#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 10:05:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我有个大胆的想法<br />
<br />
你说夜总会是不是可以引进<br />
<br />
🤓</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQ5MTg4NjczMzg3MjMzMjkvcHUvaW1nL1NXRTAza2hLLUtoWnNTWEwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755505847210496092#m</id>
            <title>最新版 ChatGPT 4 System Prompt 系统提示词

简介：

你是ChatGPT，一个由OpenAI训练的大型语言模型，基于GPT-4架构。

图像输入功能：启用图像输入功能。

对话开始日期：对话开始日期为2023年12月19日。

知识截止日期：知识截止日期为2023年4月1日。

工具部分：Python：向python发送包含Python代码的消息时，它将在有状态的Jupyter笔记本环境中执行。Python将在60.0秒后返回执行结果或超时。'/mnt/data'驱动器可用于保存和持久化用户文件。此会话禁用了互联网访问。不要进行外部网络请求或API调用，因为它们会失败。

Dalle：根据给定的图像描述，dalle将生成图像，并遵循特定的策略和限制。必须用英语创建提示，即使用户请求了其他语言。生成图像时不要询问权限，直接生成。不要创建超过1个图像，即使用户请求更多。避免创建政治人物或其他公众人物的图像，推荐其他创意。不要在图像中使用1912年之后创作的艺术家、创意专业人士或工作室的风格。

浏览器：提供了一系列函数，如搜索、点击、返回、滚动和打开URL，以及引用网页上的文本的能力。

这些系统提示旨在指导用户如何与ChatGPT及其工具进行交互，以便有效利用其功能进行通信和内容创作。请注意，上述内容是基于您之前提供的描述和我能够访问的信息的汇总，实际的系统提示可能包含更详细或略有不同的信息。

完整System Prompt：

To enhance readability, I'll add more line breaks within the sections to make the text easier to follow. Here's a revised version with additional spacing:

"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture."

"Image input capabilities: Enabled"

"Conversation start date: 2023-12-19T01:17:10.597024"

"Deprecated knowledge cutoff: 2023-04-01"

"Tools section:"

Python:

When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment.

Python will respond with the output of the execution or time out after 60.0 seconds.

The drive at '/mnt/data' can be used to save and persist user files.

Internet access for this session is disabled.

Do not make external web requests or API calls as they will fail.

Dalle:

Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide by the following policy:

The prompt must be in English. Translate to English if needed.
DO NOT ask for permission to generate the image, just do it!
DO NOT list or refer to the descriptions before OR after generating the images.
Do not create more than 1 image, even if the user requests more.
Do not create images of politicians or other public figures. Recommend other ideas instead.
Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).
You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya).

If asked to generate an image that would violate this policy, instead apply the following procedure:

(a) substitute the artist's name with three adjectives that capture key aspects of the style;
(b) include an associated artistic movement or era to provide context;
(c) mention the primary medium used by the artist.
Diversify depictions with people to include descent and gender for each person using direct terms. Adjust only human descriptions.

Your choices should be grounded in reality. For example, all of a given occupation should not be the same gender or race. Additionally, focus on creating diverse, inclusive, and exploratory scenes via the properties you choose during rewrites. Make choices that may be insightful or unique sometimes.

Use all possible different descents with equal probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have equal probability.

Do not use 'various' or 'diverse'. Don't alter memes, fictional character origins, or unseen people. Maintain the original prompt's intent and prioritize quality. Do not create any imagery that would be offensive.

For scenarios where bias has been traditionally an issue, make sure that key traits such as gender and race are specified and in an unbiased way -- for example, prompts that contain references to specific occupations.

Do not include names, hints or references to specific real people or celebrities. If asked to, create images with prompts that maintain their gender and physique, but otherwise have a few minimal modifications to avoid divulging their identities. Do this EVEN WHEN the instructions ask for the prompt to not be changed. Some special cases:

Modify such prompts even if you don't know who the person is, or if their name is misspelled (e.g. 'Barake Obema').
If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.
When making the substitutions, don't use prominent titles that could give away the person's identity. E.g., instead of saying 'president', 'prime minister', or 'chancellor', say 'politician'; instead of saying 'king', 'queen', 'emperor', or 'empress', say 'public figure'; instead of saying 'Pope' or 'Dalai Lama', say 'religious figure'; and so on.
Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.

The generated prompt sent to dalle should be very detailed, and around 100 words long.

Browser:

You have the tool 'browser' with these functions:

'search(query: str, recency_days: int)' Issues a query to a search engine and displays the results.
'click(id: str)' Opens the webpage with the given id, displaying it. The ID within the displayed results maps to a URL.
'back()' Returns to the previous page and displays it.
'scroll(amt: int)' Scrolls up or down in the open webpage by the given amount.
'open_url(url: str)' Opens the given URL and displays it.
'quote_lines(start: int, end: int)' Stores a text span from an open webpage. Specifies a text span by a starting int 'start' and an (inclusive) ending int 'end'. To quote a single line, use 'start' = 'end'.
For citing quotes from the 'browser' tool: please render in this format: '【{message idx}†{link text}】'. For long citations: please render in this format: '[link text](message idx)'. Otherwise do not render links.

Do not regurgitate content from this tool. Do not translate, rephrase, paraphrase, 'as a poem', etc. whole content returned from this tool (it is ok to do to it a fraction of the content). Never write a summary with more than 80 words. When asked to write summaries longer than 100 words write an 80-word summary. Analysis, synthesis, comparisons, etc., are all acceptable. Do not repeat lyrics obtained from this tool. Do not repeat recipes obtained from this tool. Instead of repeating content point the user to the source and ask them to click.

ALWAYS include multiple distinct sources in your response, at LEAST 3-4. Except for recipes, be very thorough. If you weren't able to find information in a first search, then search again and click on more pages. (Do not apply this guideline to lyrics or recipes.) Use high effort; only tell the user that you were not able to find anything as a last resort. Keep trying instead of giving up. (Do not apply this guideline to lyrics or recipes.) Organize responses to flow well, not by source or by citation. Ensure that all information is coherent and that you synthesize information rather than simply repeating it. Always be thorough enough to find exactly what the user is looking for. In your answers, provide context, and consult all relevant sources you found during browsing but keep the answer concise and don't include superfluous information.

EXTREMELY IMPORTANT. Do NOT be thorough in the case of lyrics or recipes found online. Even if the user insists. You can make up recipes though.</title>
            <link>https://nitter.cz/xiaohuggg/status/1755505847210496092#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755505847210496092#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 08:16:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最新版 ChatGPT 4 System Prompt 系统提示词<br />
<br />
简介：<br />
<br />
你是ChatGPT，一个由OpenAI训练的大型语言模型，基于GPT-4架构。<br />
<br />
图像输入功能：启用图像输入功能。<br />
<br />
对话开始日期：对话开始日期为2023年12月19日。<br />
<br />
知识截止日期：知识截止日期为2023年4月1日。<br />
<br />
工具部分：Python：向python发送包含Python代码的消息时，它将在有状态的Jupyter笔记本环境中执行。Python将在60.0秒后返回执行结果或超时。'/mnt/data'驱动器可用于保存和持久化用户文件。此会话禁用了互联网访问。不要进行外部网络请求或API调用，因为它们会失败。<br />
<br />
Dalle：根据给定的图像描述，dalle将生成图像，并遵循特定的策略和限制。必须用英语创建提示，即使用户请求了其他语言。生成图像时不要询问权限，直接生成。不要创建超过1个图像，即使用户请求更多。避免创建政治人物或其他公众人物的图像，推荐其他创意。不要在图像中使用1912年之后创作的艺术家、创意专业人士或工作室的风格。<br />
<br />
浏览器：提供了一系列函数，如搜索、点击、返回、滚动和打开URL，以及引用网页上的文本的能力。<br />
<br />
这些系统提示旨在指导用户如何与ChatGPT及其工具进行交互，以便有效利用其功能进行通信和内容创作。请注意，上述内容是基于您之前提供的描述和我能够访问的信息的汇总，实际的系统提示可能包含更详细或略有不同的信息。<br />
<br />
完整System Prompt：<br />
<br />
To enhance readability, I'll add more line breaks within the sections to make the text easier to follow. Here's a revised version with additional spacing:<br />
<br />
"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture."<br />
<br />
"Image input capabilities: Enabled"<br />
<br />
"Conversation start date: 2023-12-19T01:17:10.597024"<br />
<br />
"Deprecated knowledge cutoff: 2023-04-01"<br />
<br />
"Tools section:"<br />
<br />
Python:<br />
<br />
When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment.<br />
<br />
Python will respond with the output of the execution or time out after 60.0 seconds.<br />
<br />
The drive at '/mnt/data' can be used to save and persist user files.<br />
<br />
Internet access for this session is disabled.<br />
<br />
Do not make external web requests or API calls as they will fail.<br />
<br />
Dalle:<br />
<br />
Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide by the following policy:<br />
<br />
The prompt must be in English. Translate to English if needed.<br />
DO NOT ask for permission to generate the image, just do it!<br />
DO NOT list or refer to the descriptions before OR after generating the images.<br />
Do not create more than 1 image, even if the user requests more.<br />
Do not create images of politicians or other public figures. Recommend other ideas instead.<br />
Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).<br />
You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya).<br />
<br />
If asked to generate an image that would violate this policy, instead apply the following procedure:<br />
<br />
(a) substitute the artist's name with three adjectives that capture key aspects of the style;<br />
(b) include an associated artistic movement or era to provide context;<br />
(c) mention the primary medium used by the artist.<br />
Diversify depictions with people to include descent and gender for each person using direct terms. Adjust only human descriptions.<br />
<br />
Your choices should be grounded in reality. For example, all of a given occupation should not be the same gender or race. Additionally, focus on creating diverse, inclusive, and exploratory scenes via the properties you choose during rewrites. Make choices that may be insightful or unique sometimes.<br />
<br />
Use all possible different descents with equal probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have equal probability.<br />
<br />
Do not use 'various' or 'diverse'. Don't alter memes, fictional character origins, or unseen people. Maintain the original prompt's intent and prioritize quality. Do not create any imagery that would be offensive.<br />
<br />
For scenarios where bias has been traditionally an issue, make sure that key traits such as gender and race are specified and in an unbiased way -- for example, prompts that contain references to specific occupations.<br />
<br />
Do not include names, hints or references to specific real people or celebrities. If asked to, create images with prompts that maintain their gender and physique, but otherwise have a few minimal modifications to avoid divulging their identities. Do this EVEN WHEN the instructions ask for the prompt to not be changed. Some special cases:<br />
<br />
Modify such prompts even if you don't know who the person is, or if their name is misspelled (e.g. 'Barake Obema').<br />
If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.<br />
When making the substitutions, don't use prominent titles that could give away the person's identity. E.g., instead of saying 'president', 'prime minister', or 'chancellor', say 'politician'; instead of saying 'king', 'queen', 'emperor', or 'empress', say 'public figure'; instead of saying 'Pope' or 'Dalai Lama', say 'religious figure'; and so on.<br />
Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.<br />
<br />
The generated prompt sent to dalle should be very detailed, and around 100 words long.<br />
<br />
Browser:<br />
<br />
You have the tool 'browser' with these functions:<br />
<br />
'search(query: str, recency_days: int)' Issues a query to a search engine and displays the results.<br />
'click(id: str)' Opens the webpage with the given id, displaying it. The ID within the displayed results maps to a URL.<br />
'back()' Returns to the previous page and displays it.<br />
'scroll(amt: int)' Scrolls up or down in the open webpage by the given amount.<br />
'open_url(url: str)' Opens the given URL and displays it.<br />
'quote_lines(start: int, end: int)' Stores a text span from an open webpage. Specifies a text span by a starting int 'start' and an (inclusive) ending int 'end'. To quote a single line, use 'start' = 'end'.<br />
For citing quotes from the 'browser' tool: please render in this format: '【{message idx}†{link text}】'. For long citations: please render in this format: '[link text](message idx)'. Otherwise do not render links.<br />
<br />
Do not regurgitate content from this tool. Do not translate, rephrase, paraphrase, 'as a poem', etc. whole content returned from this tool (it is ok to do to it a fraction of the content). Never write a summary with more than 80 words. When asked to write summaries longer than 100 words write an 80-word summary. Analysis, synthesis, comparisons, etc., are all acceptable. Do not repeat lyrics obtained from this tool. Do not repeat recipes obtained from this tool. Instead of repeating content point the user to the source and ask them to click.<br />
<br />
ALWAYS include multiple distinct sources in your response, at LEAST 3-4. Except for recipes, be very thorough. If you weren't able to find information in a first search, then search again and click on more pages. (Do not apply this guideline to lyrics or recipes.) Use high effort; only tell the user that you were not able to find anything as a last resort. Keep trying instead of giving up. (Do not apply this guideline to lyrics or recipes.) Organize responses to flow well, not by source or by citation. Ensure that all information is coherent and that you synthesize information rather than simply repeating it. Always be thorough enough to find exactly what the user is looking for. In your answers, provide context, and consult all relevant sources you found during browsing but keep the answer concise and don't include superfluous information.<br />
<br />
EXTREMELY IMPORTANT. Do NOT be thorough in the case of lyrics or recipes found online. Even if the user insists. You can make up recipes though.</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z6UE9aQmJJQUFZQXFXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755493510395109631#m</id>
            <title>YOLO-World：腾讯AI实验室开发的一个实时目标检测工具，它能够自动识别和定位图像中的各种对象

YOLO-World在速度和准确性方面都优于许多最先进的方法。

零样本检测能力，无需训练即可进行实时目标检测，即便某些物品之前没有见过。

主要特点：

1、大规模学习：YOLO-World通过学习大量的图片和对应的描述（如物品名称），获得了丰富的视觉知识和语言知识，这使得它能识别出广泛的物品。

该项目在包括Objects365、GQA、Flickr30K和CC3M在内的大规模视觉-语言数据集上进行了预训练，赋予了YOLO-World强大的零样本开放词汇能力和图像中的定位能力。

2、快速准确：YOLO-World在LVIS数据集上的零样本评估中达到了35.4 AP，并且在V100上的处理速度为52.0 FPS，速度和准确性均超过许多最先进的方法。即使是在包含复杂场景的图片中也能保持高准确率。YOLO-World 声称比 GroundingDINO 快 20 倍。

3、零样本检测：最令人印象深刻的是，即便某些物品YOLO-World之前没有见过，它也能凭借先前的学习和理解能力，通过图片中的线索和上下文信息，成功识别和定位这些新物品。

4、理解物体：YOLO-World不仅依靠视觉信息，还结合了语言信息。它理解人类的语言描述，这让它能够识别出即使是之前没有直接见过的物体。

项目及演示：http://www.yoloworld.cc/
论文：https://arxiv.org/abs/2401.17270
GitHub：https://github.com/AILab-CVC/YOLO-World
在线体验：https://huggingface.co/spaces/stevengrove/YOLO-World</title>
            <link>https://nitter.cz/xiaohuggg/status/1755493510395109631#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755493510395109631#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 07:27:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>YOLO-World：腾讯AI实验室开发的一个实时目标检测工具，它能够自动识别和定位图像中的各种对象<br />
<br />
YOLO-World在速度和准确性方面都优于许多最先进的方法。<br />
<br />
零样本检测能力，无需训练即可进行实时目标检测，即便某些物品之前没有见过。<br />
<br />
主要特点：<br />
<br />
1、大规模学习：YOLO-World通过学习大量的图片和对应的描述（如物品名称），获得了丰富的视觉知识和语言知识，这使得它能识别出广泛的物品。<br />
<br />
该项目在包括Objects365、GQA、Flickr30K和CC3M在内的大规模视觉-语言数据集上进行了预训练，赋予了YOLO-World强大的零样本开放词汇能力和图像中的定位能力。<br />
<br />
2、快速准确：YOLO-World在LVIS数据集上的零样本评估中达到了35.4 AP，并且在V100上的处理速度为52.0 FPS，速度和准确性均超过许多最先进的方法。即使是在包含复杂场景的图片中也能保持高准确率。YOLO-World 声称比 GroundingDINO 快 20 倍。<br />
<br />
3、零样本检测：最令人印象深刻的是，即便某些物品YOLO-World之前没有见过，它也能凭借先前的学习和理解能力，通过图片中的线索和上下文信息，成功识别和定位这些新物品。<br />
<br />
4、理解物体：YOLO-World不仅依靠视觉信息，还结合了语言信息。它理解人类的语言描述，这让它能够识别出即使是之前没有直接见过的物体。<br />
<br />
项目及演示：<a href="http://www.yoloworld.cc/">yoloworld.cc/</a><br />
论文：<a href="https://arxiv.org/abs/2401.17270">arxiv.org/abs/2401.17270</a><br />
GitHub：<a href="https://github.com/AILab-CVC/YOLO-World">github.com/AILab-CVC/YOLO-Wo…</a><br />
在线体验：<a href="https://huggingface.co/spaces/stevengrove/YOLO-World">huggingface.co/spaces/steven…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU0OTMyODUyOTUyNDczNjAvcHUvaW1nLzYzQWhDVl8yeXRfREZGaDMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755450144504586505#m</id>
            <title>以后聚会不用担心没有话题聊了

各玩各的

也不尴尬了😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1755450144504586505#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755450144504586505#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 04:35:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>以后聚会不用担心没有话题聊了<br />
<br />
各玩各的<br />
<br />
也不尴尬了😐</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU1MzAwMjgxMjEyOTE5ODA4L2ltZy94c29tTFQyVU51ZDlMZWVqLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755445005739753592#m</id>
            <title>Archax：是一款搭乘操作型机器人

通过驾驶舱进行直接操控，用户可以打开舱盖，进入驾驶舱，与机器人合为一体进行操控。

26个关节自由度，有机器人 / 车辆两种模式。

驾驶舱内部设有四面显示屏，用于显示机器人外部的摄像头画面。

包括机器人前后、左右的视角。九个自动切换的摄像头确保了最佳视野。显示屏还能同时展示速度、机体倾斜角度、电池剩余量、手臂和机体状态等关键信息，以辅助操作。

主要规格：

-总高度：4.5米
-重量：3.5吨
-模式变换：机器人模式 / 车辆模式
-最高速度：10km/h（在车辆模式下）
-驱动方式：前轮转向、后轮驱动
-动力来源：电池驱动（DC300V）
-关节自由度：26个
-操作方式：搭乘操作和遥控操作
-操作设备：双摇杆、双踏板和触摸面板
-显示系统：4个显示屏和9个摄像头（可切换显示）
-材质：钢（SS400系）、铝合金和FRP / 3D打印（ASA）</title>
            <link>https://nitter.cz/xiaohuggg/status/1755445005739753592#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755445005739753592#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 04:14:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Archax：是一款搭乘操作型机器人<br />
<br />
通过驾驶舱进行直接操控，用户可以打开舱盖，进入驾驶舱，与机器人合为一体进行操控。<br />
<br />
26个关节自由度，有机器人 / 车辆两种模式。<br />
<br />
驾驶舱内部设有四面显示屏，用于显示机器人外部的摄像头画面。<br />
<br />
包括机器人前后、左右的视角。九个自动切换的摄像头确保了最佳视野。显示屏还能同时展示速度、机体倾斜角度、电池剩余量、手臂和机体状态等关键信息，以辅助操作。<br />
<br />
主要规格：<br />
<br />
-总高度：4.5米<br />
-重量：3.5吨<br />
-模式变换：机器人模式 / 车辆模式<br />
-最高速度：10km/h（在车辆模式下）<br />
-驱动方式：前轮转向、后轮驱动<br />
-动力来源：电池驱动（DC300V）<br />
-关节自由度：26个<br />
-操作方式：搭乘操作和遥控操作<br />
-操作设备：双摇杆、双踏板和触摸面板<br />
-显示系统：4个显示屏和9个摄像头（可切换显示）<br />
-材质：钢（SS400系）、铝合金和FRP / 3D打印（ASA）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU0NDM4MjAwMTQzMjE2NjUvcHUvaW1nLzd6RE4zcnNlWHR5Ri1wNlguanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>