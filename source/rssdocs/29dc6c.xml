<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737743924800962960#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1737743924800962960#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737743924800962960#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:56:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/theapathyman/status/1737732405518291417#m">nitter.cz/theapathyman/status/1737732405518291417#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737742501073805808#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1737742501073805808#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737742501073805808#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:51:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/SagaSeigou/status/1737734385762488412#m">nitter.cz/SagaSeigou/status/1737734385762488412#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737742437693653220#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1737742437693653220#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737742437693653220#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:50:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/TS86142/status/1737735072177123535#m">nitter.cz/TS86142/status/1737735072177123535#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737742328956342465#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1737742328956342465#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737742328956342465#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:50:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/iamnanyi/status/1737737414687535529#m">nitter.cz/iamnanyi/status/1737737414687535529#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737742189428638180#m</id>
            <title>R to @xiaohuggg: 一些案例</title>
            <link>https://nitter.cz/xiaohuggg/status/1737742189428638180#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737742189428638180#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些案例</p>
<p><a href="https://nitter.cz/shanhe84322504/status/1737740905120153730#m">nitter.cz/shanhe84322504/status/1737740905120153730#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737737795299647943#m</id>
            <title>R to @xiaohuggg: 历史版本对比...</title>
            <link>https://nitter.cz/xiaohuggg/status/1737737795299647943#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737737795299647943#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:32:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>历史版本对比...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IydlE5MGJNQUFGbG90LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737737427706651106#m</id>
            <title>#Midjourney V6上线

一些反馈发现（单簧管总结）：

- 提示词长度现在是350+
- 您可以指定颜色和其他细节
- 你可以在画布上放置你想要的东西，如添加文本
- 您可以提示多个主题
- 你可以像ChatGPT一样和Midjourney聊天
- V6能够理解标点和语法的细微差别(即：熊猫吃、射、走）
- 可以使用V6制作niji那样的漫画
- 可以通过描述图像来为其添加框架或边框

如何启用：在 /settings 下拉菜单中选择 V6，或在提示后输入 --v 6。

官方对V6 模型的新特性：

1、更准确的提示跟随和更长的提示

•改进的响应能力：V6 模型对用户输入的响应更加准确，能够更好地理解和执行复杂的提示。

•处理更长的提示：V6 能够处理更长的文本提示，这意味着用户可以提供更详细的描述和指令，从而获得更精确的输出结果。

2、提升的连贯性和模型知识

•改善的连贯性：V6 在生成内容时的连贯性得到了显著提升，这使得输出结果更加流畅和自然。

•增强的模型知识：V6 拥有更丰富的知识库和更好的理解能力，能够更准确地处理复杂的查询和任务。

3、改进的图像提示和混合

•增强的图像生成：V6 在图像生成方面的能力得到了增强，能够根据文本提示创建更精细和逼真的图像。

•改善的图像混合：V6 提供了更好的图像混合功能，使得不同元素和风格的结合更加和谐自然。

4、文本绘制能力

•用户可以在他们的提示中指定要在图像上显示的文本，模型将会根据这些指示在生成的图像中包含相应的文本内容。

•绘制文本：V6 可以在图像中绘制文本。为了获得最佳效果，文本应该用引号标出。这样做可以帮助模型区分哪些是描述性的提示，哪些是实际要在图像中呈现的文本。

•风格调整：使用 --style raw 或较低的 --stylize 值可能有助于提高文本绘制的准确性和质量。这是因为不同的风格设置可能会影响文本在图像中的呈现方式。

例如，如果用户想要生成一张图像，上面写着用马克笔在便利贴上写的“Hello World!”，他们可以使用类似于以下的提示：“/imagine a photo of the text 'Hello World!' written with a marker on a sticky note --ar 16:9 --v 6”。

5、改进的放大器

•分辨率提升：V6 提供了改进的放大器功能，可以将图像分辨率提高 2 倍。

•两种模式：提供了 ‘subtle’（微妙）和 ‘creative’（创意）两种模式，以适应不同的图像处理需求。

V6 的风格和提示：

1、提示方式的变化：V6 的提示方式与 V5 显著不同，用户需要重新学习如何进行提示。V6 提供了更细致的控制能力，用户可能需要更精确地描述他们想要的输出，以获得最佳结果。

2、对提示的敏感性：V6 对提示中的每个词汇都非常敏感。因此，应避免使用如“奖项获得者”、“4K”、“8K”这类可能被视为“垃圾”的词汇，因为它们可能不会提升结果质量，反而可能导致模型混淆。

用户应尽量明确和具体地描述他们想要的内容。例如，如果需要一个特定风格的图像，应直接说明这一点。

3、风格和美学：使用 --style raw 可获得更真实的图像。--stylize 的不同值会影响提示理解和美学效果。

使用 --style raw：为了获得更真实、更少主观色彩的图像，建议使用 --style raw 参数。这个设置倾向于生成更接近真实世界、更少艺术化处理的图像。

--stylize 参数的影响：低值：较低的 --stylize 值（默认为 100）可能会导致更好的提示理解，但可能牺牲一些美学效果。

高值：较高的 --stylize 值（最高可达 1000）可能会增强图像的美学和艺术效果，但可能会降低对提示的精确理解。

支持的功能和参数：

支持的功能包括 --ar, --chaos, --weird, --tile, --stylize, --style raw, Vary (subtle), Vary (strong), Remix, /blend, /describe（仅 V5 版本）。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737737427706651106#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737737427706651106#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 07:31:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> V6上线<br />
<br />
一些反馈发现（单簧管总结）：<br />
<br />
- 提示词长度现在是350+<br />
- 您可以指定颜色和其他细节<br />
- 你可以在画布上放置你想要的东西，如添加文本<br />
- 您可以提示多个主题<br />
- 你可以像ChatGPT一样和Midjourney聊天<br />
- V6能够理解标点和语法的细微差别(即：熊猫吃、射、走）<br />
- 可以使用V6制作niji那样的漫画<br />
- 可以通过描述图像来为其添加框架或边框<br />
<br />
如何启用：在 /settings 下拉菜单中选择 V6，或在提示后输入 --v 6。<br />
<br />
官方对V6 模型的新特性：<br />
<br />
1、更准确的提示跟随和更长的提示<br />
<br />
•改进的响应能力：V6 模型对用户输入的响应更加准确，能够更好地理解和执行复杂的提示。<br />
<br />
•处理更长的提示：V6 能够处理更长的文本提示，这意味着用户可以提供更详细的描述和指令，从而获得更精确的输出结果。<br />
<br />
2、提升的连贯性和模型知识<br />
<br />
•改善的连贯性：V6 在生成内容时的连贯性得到了显著提升，这使得输出结果更加流畅和自然。<br />
<br />
•增强的模型知识：V6 拥有更丰富的知识库和更好的理解能力，能够更准确地处理复杂的查询和任务。<br />
<br />
3、改进的图像提示和混合<br />
<br />
•增强的图像生成：V6 在图像生成方面的能力得到了增强，能够根据文本提示创建更精细和逼真的图像。<br />
<br />
•改善的图像混合：V6 提供了更好的图像混合功能，使得不同元素和风格的结合更加和谐自然。<br />
<br />
4、文本绘制能力<br />
<br />
•用户可以在他们的提示中指定要在图像上显示的文本，模型将会根据这些指示在生成的图像中包含相应的文本内容。<br />
<br />
•绘制文本：V6 可以在图像中绘制文本。为了获得最佳效果，文本应该用引号标出。这样做可以帮助模型区分哪些是描述性的提示，哪些是实际要在图像中呈现的文本。<br />
<br />
•风格调整：使用 --style raw 或较低的 --stylize 值可能有助于提高文本绘制的准确性和质量。这是因为不同的风格设置可能会影响文本在图像中的呈现方式。<br />
<br />
例如，如果用户想要生成一张图像，上面写着用马克笔在便利贴上写的“Hello World!”，他们可以使用类似于以下的提示：“/imagine a photo of the text 'Hello World!' written with a marker on a sticky note --ar 16:9 --v 6”。<br />
<br />
5、改进的放大器<br />
<br />
•分辨率提升：V6 提供了改进的放大器功能，可以将图像分辨率提高 2 倍。<br />
<br />
•两种模式：提供了 ‘subtle’（微妙）和 ‘creative’（创意）两种模式，以适应不同的图像处理需求。<br />
<br />
V6 的风格和提示：<br />
<br />
1、提示方式的变化：V6 的提示方式与 V5 显著不同，用户需要重新学习如何进行提示。V6 提供了更细致的控制能力，用户可能需要更精确地描述他们想要的输出，以获得最佳结果。<br />
<br />
2、对提示的敏感性：V6 对提示中的每个词汇都非常敏感。因此，应避免使用如“奖项获得者”、“4K”、“8K”这类可能被视为“垃圾”的词汇，因为它们可能不会提升结果质量，反而可能导致模型混淆。<br />
<br />
用户应尽量明确和具体地描述他们想要的内容。例如，如果需要一个特定风格的图像，应直接说明这一点。<br />
<br />
3、风格和美学：使用 --style raw 可获得更真实的图像。--stylize 的不同值会影响提示理解和美学效果。<br />
<br />
使用 --style raw：为了获得更真实、更少主观色彩的图像，建议使用 --style raw 参数。这个设置倾向于生成更接近真实世界、更少艺术化处理的图像。<br />
<br />
--stylize 参数的影响：低值：较低的 --stylize 值（默认为 100）可能会导致更好的提示理解，但可能牺牲一些美学效果。<br />
<br />
高值：较高的 --stylize 值（最高可达 1000）可能会增强图像的美学和艺术效果，但可能会降低对提示的精确理解。<br />
<br />
支持的功能和参数：<br />
<br />
支持的功能包括 --ar, --chaos, --weird, --tile, --stylize, --style raw, Vary (subtle), Vary (strong), Remix, /blend, /describe（仅 V5 版本）。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IydTUxNGFrQUFHNG5uLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737709816494367013#m</id>
            <title>卧槽 

出问题了

我X帖子都没了，被清空了？ 你们呢</title>
            <link>https://nitter.cz/xiaohuggg/status/1737709816494367013#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737709816494367013#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 05:41:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽 <br />
<br />
出问题了<br />
<br />
我X帖子都没了，被清空了？ 你们呢</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737684134968045894#m</id>
            <title>R to @xiaohuggg: 演示视频

在线体验：https://text-to-cad.zoo.dev</title>
            <link>https://nitter.cz/xiaohuggg/status/1737684134968045894#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737684134968045894#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 03:59:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频<br />
<br />
在线体验：<a href="https://text-to-cad.zoo.dev">text-to-cad.zoo.dev</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2ODQwMTUyMDM5MzQyMDgvcHUvaW1nL0R3VkNTQUhOWGV3NXJ6R20uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737683961428717693#m</id>
            <title>兄弟们，又有人要失业了🫣

Text-to-CAD ：通过文本提示生成 CAD文件。

只需要输入自然语言描述，它就能根据这些描述创建相应的 B-Rep CAD 文件和网格模型。

生成的模型可以导入到用户选择的任何 CAD 程序中。

Text-to-CAD 背后的基础设施利用了 Zoo 的设计 API 和机器学习 API。

这些 API 能够程序化地分析训练数据，并生成 CAD 文件。

体验地址：https://zoo.dev/text-to-cad
API申请：https://zoo.dev/machine-learning-api</title>
            <link>https://nitter.cz/xiaohuggg/status/1737683961428717693#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737683961428717693#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 03:58:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，又有人要失业了🫣<br />
<br />
Text-to-CAD ：通过文本提示生成 CAD文件。<br />
<br />
只需要输入自然语言描述，它就能根据这些描述创建相应的 B-Rep CAD 文件和网格模型。<br />
<br />
生成的模型可以导入到用户选择的任何 CAD 程序中。<br />
<br />
Text-to-CAD 背后的基础设施利用了 Zoo 的设计 API 和机器学习 API。<br />
<br />
这些 API 能够程序化地分析训练数据，并生成 CAD 文件。<br />
<br />
体验地址：<a href="https://zoo.dev/text-to-cad">zoo.dev/text-to-cad</a><br />
API申请：<a href="https://zoo.dev/machine-learning-api">zoo.dev/machine-learning-api</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2ODM2NDc2NTI4NjgwOTYvcHUvaW1nL3FMWnFXTmtNYmdneG5WeEUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737674325111853373#m</id>
            <title>我每天用到的最多的提示词

就三个字：说人话

不行你们试试，哈哈哈哈

随便什么问题，等GPT说完了，你拿出这三个字保准很好使！🥱</title>
            <link>https://nitter.cz/xiaohuggg/status/1737674325111853373#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737674325111853373#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 03:20:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我每天用到的最多的提示词<br />
<br />
就三个字：说人话<br />
<br />
不行你们试试，哈哈哈哈<br />
<br />
随便什么问题，等GPT说完了，你拿出这三个字保准很好使！🥱</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737669115630985719#m</id>
            <title>R to @xiaohuggg: 由 Coscientist 生成的代码。

分别执行几个步骤：定义方法的元数据、加载实验器皿模块、设置液体处理器、执行所需的试剂转移、设置加热器-振动器模块、运行反应以及关闭模块。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737669115630985719#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737669115630985719#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:59:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>由 Coscientist 生成的代码。<br />
<br />
分别执行几个步骤：定义方法的元数据、加载实验器皿模块、设置液体处理器、执行所需的试剂转移、设置加热器-振动器模块、运行反应以及关闭模块。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Ixd2dDZGJrQUEtUXpYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737668680627183915#m</id>
            <title>一个名为 Coscientist 的实验室AI工具，在不到4分钟的时间成功复现诺奖研究成果！

Coscientist 是由卡内基梅隆大学和 Emerald Cloud Lab 的研究团队共同开发，它由 GPT-4 驱动，能够自主设计、规划和执行真实世界的化学实验。

该研究成果已发表在Nature上！

Coscientist 在不到四分钟内设计出了一个准确的程序。

成功实现了所需的化学反应，复现了2010 年诺贝尔化学奖的研究成果：

成功优化钯催化偶联反应（美国化学家 Richard Fred Heck 与两位日本化学家 Ei-ichi Negishi 和 Akira Suzuki 因“对有机合成中钯催化偶联反应的研究”获得了 2010 年诺贝尔化学奖）

Coscientist在六个不同任务中展示了其加速研究的潜力：

1、化学合成规划：能够规划已知化合物的化学合成，使用公开数据进行有效搜索。

2、文档搜索和导航：能够高效搜索和浏览大量硬件文档。

3、执行高级命令：使用文档来执行云实验室中的高级命令。

4、控制实验仪器：精确控制液体处理仪器，执行低级指令。

5、解决复杂科学任务：处理需要多个硬件模块和数据源整合的复杂科学任务。

6、优化问题解决：分析之前收集的实验数据来解决优化问题。

Coscientist 的具体能力和工作原理：

具体能力：

1、自主实验设计：Coscientist 能够自主设计化学实验，包括选择合适的化学反应和实验条件。Coscientist 能够规划已知化合物的化学合成，并有效搜索和浏览大量硬件文档。

2、实验执行：它能够实际执行设计的实验，包括操作实验室设备和处理化学品。能够使用低级指令精确控制液体处理仪器，并解决需要多个硬件模块和数据源整合的复杂科学任务。

3、数据分析：Coscientist 能够分析实验结果，提出结论，并根据结果调整实验设计。

4、自我纠正：在实验过程中，它能够识别并纠正错误，比如在编写控制设备代码时的错误。

工作原理：

Coscientist 通过与多个模块的交互（包括网络和文档搜索、代码执行以及实验），获取解决复杂问题所需的知识。

系统的主模块是 Planner，以 GPT-4 为基础。作为实验室助手，负责基于用户输入规划实验。

系统提示定义了四个命令（‘GOOGLE’、‘PYTHON’、‘DOCUMENTATION’、‘EXPERIMENT’）来收集知识。

其中，GOOGLE 命令负责在互联网上进行搜索，PYTHON 命令执行代码，而 DOCUMENTATION 命令检索和总结必要的文档。此外，这些命令还可以执行子操作。

1、基于 GPT-4：Coscientist 的核心是 GPT-4，这是一个先进的大型语言模型，能够理解和生成自然语言。

2、多模态输入处理：除了文本输入，Coscientist 可能还能处理图像和其他类型的数据，以更全面地理解实验环境。

3、自然语言处理：它通过自然语言处理技术理解实验要求和目标，然后生成相应的实验计划。

4、机器人和自动化技术：Coscientist 可能与实验室的机器人和自动化设备相结合，以执行实验操作。

5、数据分析和机器学习：使用数据分析和机器学习技术来解释实验结果，并基于这些结果进行自我学习和改进。

Coscientist 的成功表明，AI 可以有效地加速科学发现的速度和数量，改善实验结果的可复制性和可靠性。

Coscientist 被视为建立自动化实验室的关键一步，预示着未来更多令人兴奋的发展。

Nature报道：https://www.nature.com/articles/s41586-023-06792-0

论文：https://www.nature.com/articles/s41586-023-06792-0.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1737668680627183915#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737668680627183915#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:57:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个名为 Coscientist 的实验室AI工具，在不到4分钟的时间成功复现诺奖研究成果！<br />
<br />
Coscientist 是由卡内基梅隆大学和 Emerald Cloud Lab 的研究团队共同开发，它由 GPT-4 驱动，能够自主设计、规划和执行真实世界的化学实验。<br />
<br />
该研究成果已发表在Nature上！<br />
<br />
Coscientist 在不到四分钟内设计出了一个准确的程序。<br />
<br />
成功实现了所需的化学反应，复现了2010 年诺贝尔化学奖的研究成果：<br />
<br />
成功优化钯催化偶联反应（美国化学家 Richard Fred Heck 与两位日本化学家 Ei-ichi Negishi 和 Akira Suzuki 因“对有机合成中钯催化偶联反应的研究”获得了 2010 年诺贝尔化学奖）<br />
<br />
Coscientist在六个不同任务中展示了其加速研究的潜力：<br />
<br />
1、化学合成规划：能够规划已知化合物的化学合成，使用公开数据进行有效搜索。<br />
<br />
2、文档搜索和导航：能够高效搜索和浏览大量硬件文档。<br />
<br />
3、执行高级命令：使用文档来执行云实验室中的高级命令。<br />
<br />
4、控制实验仪器：精确控制液体处理仪器，执行低级指令。<br />
<br />
5、解决复杂科学任务：处理需要多个硬件模块和数据源整合的复杂科学任务。<br />
<br />
6、优化问题解决：分析之前收集的实验数据来解决优化问题。<br />
<br />
Coscientist 的具体能力和工作原理：<br />
<br />
具体能力：<br />
<br />
1、自主实验设计：Coscientist 能够自主设计化学实验，包括选择合适的化学反应和实验条件。Coscientist 能够规划已知化合物的化学合成，并有效搜索和浏览大量硬件文档。<br />
<br />
2、实验执行：它能够实际执行设计的实验，包括操作实验室设备和处理化学品。能够使用低级指令精确控制液体处理仪器，并解决需要多个硬件模块和数据源整合的复杂科学任务。<br />
<br />
3、数据分析：Coscientist 能够分析实验结果，提出结论，并根据结果调整实验设计。<br />
<br />
4、自我纠正：在实验过程中，它能够识别并纠正错误，比如在编写控制设备代码时的错误。<br />
<br />
工作原理：<br />
<br />
Coscientist 通过与多个模块的交互（包括网络和文档搜索、代码执行以及实验），获取解决复杂问题所需的知识。<br />
<br />
系统的主模块是 Planner，以 GPT-4 为基础。作为实验室助手，负责基于用户输入规划实验。<br />
<br />
系统提示定义了四个命令（‘GOOGLE’、‘PYTHON’、‘DOCUMENTATION’、‘EXPERIMENT’）来收集知识。<br />
<br />
其中，GOOGLE 命令负责在互联网上进行搜索，PYTHON 命令执行代码，而 DOCUMENTATION 命令检索和总结必要的文档。此外，这些命令还可以执行子操作。<br />
<br />
1、基于 GPT-4：Coscientist 的核心是 GPT-4，这是一个先进的大型语言模型，能够理解和生成自然语言。<br />
<br />
2、多模态输入处理：除了文本输入，Coscientist 可能还能处理图像和其他类型的数据，以更全面地理解实验环境。<br />
<br />
3、自然语言处理：它通过自然语言处理技术理解实验要求和目标，然后生成相应的实验计划。<br />
<br />
4、机器人和自动化技术：Coscientist 可能与实验室的机器人和自动化设备相结合，以执行实验操作。<br />
<br />
5、数据分析和机器学习：使用数据分析和机器学习技术来解释实验结果，并基于这些结果进行自我学习和改进。<br />
<br />
Coscientist 的成功表明，AI 可以有效地加速科学发现的速度和数量，改善实验结果的可复制性和可靠性。<br />
<br />
Coscientist 被视为建立自动化实验室的关键一步，预示着未来更多令人兴奋的发展。<br />
<br />
Nature报道：<a href="https://www.nature.com/articles/s41586-023-06792-0">nature.com/articles/s41586-0…</a><br />
<br />
论文：<a href="https://www.nature.com/articles/s41586-023-06792-0.pdf">nature.com/articles/s41586-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxdlVMemFvQUFSYURfLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0IxdmFkdGJVQUFfaERFLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737656196629471517#m</id>
            <title>PowerInfer：让普通电脑也能跑大语言模型

由上海交通大学开发，目的是在配备消费级GPU的个人电脑上提供高速的大语言模型推理服务。

PowerInfer 无缝整合了 CPU 和 GPU 的内存和计算能力，优化了内存和计算资源，从而在个人电脑上高效地运行复杂的 AI 模型。

比llama.cpp快11倍...

它支持多种不同的大型语言模型！

在测试中，PowerInfer在单个 NVIDIA RTX 4090 GPU 上达到了平均每秒生成 13.20 个令牌的速率，峰值可达 29.08 个令牌。接近顶级服务器级 GPU 的性能。

PowerInfer 对比llama.cpp 在运行 Falcon(ReLU)-40B-FP16 的单个 RTX 4090(24G) 上实现 11 倍加速！

其主要工作原理：

通过智能地分配和优化计算任务在 CPU 和 GPU 之间的处理，以及利用大型语言模型中的局部性特征，从而在个人电脑上高效地运行复杂的 AI 模型。这种方法使得即使是不具备高端服务器硬件的用户也能体验到高速的 AI 模型推理性能。

激活局部性利用：PowerInfer 利用了大语言模型推理中的高局部性。大语言模型在各种输入中，只有一小部分神经元（称为“热神经元”）持续激活，而大多数神经元（“冷神经元”）则根据特定输入变化。

GPU-CPU 混合推理：为了提高效率，PowerInfer 预先将热神经元加载到 GPU 上，以实现快速访问。这减少了 GPU 的内存需求。同时，它在 CPU 上计算冷神经元的激活，减少了 CPU 和 GPU 之间的数据传输。

GitHub：https://github.com/SJTU-IPADS/PowerInfer
论文：https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1737656196629471517#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737656196629471517#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:08:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PowerInfer：让普通电脑也能跑大语言模型<br />
<br />
由上海交通大学开发，目的是在配备消费级GPU的个人电脑上提供高速的大语言模型推理服务。<br />
<br />
PowerInfer 无缝整合了 CPU 和 GPU 的内存和计算能力，优化了内存和计算资源，从而在个人电脑上高效地运行复杂的 AI 模型。<br />
<br />
比llama.cpp快11倍...<br />
<br />
它支持多种不同的大型语言模型！<br />
<br />
在测试中，PowerInfer在单个 NVIDIA RTX 4090 GPU 上达到了平均每秒生成 13.20 个令牌的速率，峰值可达 29.08 个令牌。接近顶级服务器级 GPU 的性能。<br />
<br />
PowerInfer 对比llama.cpp 在运行 Falcon(ReLU)-40B-FP16 的单个 RTX 4090(24G) 上实现 11 倍加速！<br />
<br />
其主要工作原理：<br />
<br />
通过智能地分配和优化计算任务在 CPU 和 GPU 之间的处理，以及利用大型语言模型中的局部性特征，从而在个人电脑上高效地运行复杂的 AI 模型。这种方法使得即使是不具备高端服务器硬件的用户也能体验到高速的 AI 模型推理性能。<br />
<br />
激活局部性利用：PowerInfer 利用了大语言模型推理中的高局部性。大语言模型在各种输入中，只有一小部分神经元（称为“热神经元”）持续激活，而大多数神经元（“冷神经元”）则根据特定输入变化。<br />
<br />
GPU-CPU 混合推理：为了提高效率，PowerInfer 预先将热神经元加载到 GPU 上，以实现快速访问。这减少了 GPU 的内存需求。同时，它在 CPU 上计算冷神经元的激活，减少了 CPU 和 GPU 之间的数据传输。<br />
<br />
GitHub：<a href="https://github.com/SJTU-IPADS/PowerInfer">github.com/SJTU-IPADS/PowerI…</a><br />
论文：<a href="https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf">ipads.se.sjtu.edu.cn/_media/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NTQwMDQ5MTY4ODM0NTYvcHUvaW1nL1M0SG54WE9tZFNhNmo1eWkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647074110570574#m</id>
            <title>R to @xiaohuggg: 之前的介绍</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647074110570574#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647074110570574#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前的介绍</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1734105617982456270#m">nitter.cz/xiaohuggg/status/1734105617982456270#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647071556194697#m</id>
            <title>R to @xiaohuggg: 测试视频，我以为会给我生成4个选，结果就给了一个

嘿嘿

生成大概需要6-8分钟</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647071556194697#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647071556194697#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试视频，我以为会给我生成4个选，结果就给了一个<br />
<br />
嘿嘿<br />
<br />
生成大概需要6-8分钟</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NDY3Mzk0MTc3MTA1OTIvcHUvaW1nL2VEeEFpNkpDU0NLN1VBakkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647069815587299#m</id>
            <title>R to @xiaohuggg: 官网演示集锦</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647069815587299#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647069815587299#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>官网演示集锦</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NDY2MTMzMzI2NDc5MzYvcHUvaW1nL2ExMzhVUHBHTTlMUHN2UmIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647067693211728#m</id>
            <title>阿里巴巴的 DreaMoving 放出在线体验地址了

DreaMoving能仅靠脸部照片和文字提示就能生成在任何场景下跳舞的视频...

测了下跳舞动作还可以，但是和背景融合度不行，人物舞蹈和背景完全是隔离的，不能完全融合！

体验地址：https://www.modelscope.cn/studios/vigen/video_generation/summary

这是官方演示视频（音乐我加的），测试在三楼↓</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647067693211728#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647067693211728#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:31:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里巴巴的 DreaMoving 放出在线体验地址了<br />
<br />
DreaMoving能仅靠脸部照片和文字提示就能生成在任何场景下跳舞的视频...<br />
<br />
测了下跳舞动作还可以，但是和背景融合度不行，人物舞蹈和背景完全是隔离的，不能完全融合！<br />
<br />
体验地址：<a href="https://www.modelscope.cn/studios/vigen/video_generation/summary">modelscope.cn/studios/vigen/…</a><br />
<br />
这是官方演示视频（音乐我加的），测试在三楼↓</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NDY1MzMyNjM0NzA1OTIvcHUvaW1nL0d5MmZ2b0xQNTc1SUFaWnUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737501193058877916#m</id>
            <title>有木有类似：http://remoteok.com

这种招聘网站的开源程序

想搭建个帮独立开发者招聘远程办公人才🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1737501193058877916#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737501193058877916#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 15:52:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有木有类似：<a href="http://remoteok.com">remoteok.com</a><br />
<br />
这种招聘网站的开源程序<br />
<br />
想搭建个帮独立开发者招聘远程办公人才🤔</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>