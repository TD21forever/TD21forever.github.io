<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736628108646867073#m</id>
            <title>R to @xiaohuggg: 支持多种语言、语气、语调

😁</title>
            <link>https://nitter.cz/xiaohuggg/status/1736628108646867073#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736628108646867073#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 06:03:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持多种语言、语气、语调<br />
<br />
😁</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2Mjc5NjM4MTM0MDg3NjgvcHUvaW1nL3Jwemo1dlJ2aGxwR2drUFUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736627871832269305#m</id>
            <title>R to @xiaohuggg: DreamTalk不仅能够处理和生成它在训练过程中见过的面部类型和表情，还能有效处理和生成它之前未见过的、来自不同数据集的面部类型和表情。

包括不同种族、年龄、性别的人物肖像，以及各种不同的表情和情绪。</title>
            <link>https://nitter.cz/xiaohuggg/status/1736627871832269305#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736627871832269305#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 06:02:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTalk不仅能够处理和生成它在训练过程中见过的面部类型和表情，还能有效处理和生成它之前未见过的、来自不同数据集的面部类型和表情。<br />
<br />
包括不同种族、年龄、性别的人物肖像，以及各种不同的表情和情绪。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2Mjc2MDk3Mzg2MjkxMjAvcHUvaW1nL1BtVUlFR1pUaU1wLW51QnkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736627340623692177#m</id>
            <title>DreamTalk：由清华大学、阿里巴巴和华中科大共同开发的一个基于扩散模型让人物头像说话的框架。

能够根据音频让人物头像照片说话、唱歌并保持嘴唇的同步和模仿表情变化。

- 高质量动画：能够生成非常真实的人物脸部动作。

- 多样化表情：不仅嘴唇动作逼真，还能展现丰富的表情。

- 支持多种语言：无论是中文、英文还是其他语言，都能很好地同步。

- 说话风格预测：能够根据语音预测说话者的风格，并同步表情。

- 适用多种场景：可以用于歌曲、不同类型的肖像，甚至在嘈杂环境中也能表现良好。

工作原理：

该项目在利用扩散模型在生成动态和表情丰富的说话头部方面取得突破。

结合了以下几个关键组件来生成表情丰富的说话头部动画：

1、去噪网络：这是核心组件之一，负责生成音频驱动的面部动作。去噪网络使用扩散模型来逐步去除噪声，从而生成清晰、高质量的面部表情。这个过程涉及从带有噪声的数据中逐步恢复出清晰的面部动作。

2、风格感知的嘴唇专家：这个组件专注于提高嘴唇动作的表现力和准确性。它通过分析说话风格来引导嘴唇同步，确保生成的动画既自然又符合说话者的风格。

3、风格预测器：为了消除对表情参考视频或文本的依赖，DreamTalk引入了一个基于扩散的风格预测器。这个预测器可以直接从音频预测目标表情，无需额外的表情参考视频或文本。

4、音频和视频处理：处理音频输入，提取关键的音频特征，并将这些特征用于驱动面部动画。同时，它还能处理视频输入，以提取和模仿特定的表情和风格。

5、数据和模型训练：为了实现这些功能，DreamTalk需要大量的数据来训练其模型，包括不同表情和说话风格的面部动画数据。通过这些数据，模型学习如何准确地生成与输入音频匹配的面部动作。

项目及演示：https://dreamtalk-project.github.io/
论文：https://arxiv.org/abs/2312.09767
GitHub：https://github.com/damo-vilab/i2vgen-xl</title>
            <link>https://nitter.cz/xiaohuggg/status/1736627340623692177#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736627340623692177#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 05:59:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTalk：由清华大学、阿里巴巴和华中科大共同开发的一个基于扩散模型让人物头像说话的框架。<br />
<br />
能够根据音频让人物头像照片说话、唱歌并保持嘴唇的同步和模仿表情变化。<br />
<br />
- 高质量动画：能够生成非常真实的人物脸部动作。<br />
<br />
- 多样化表情：不仅嘴唇动作逼真，还能展现丰富的表情。<br />
<br />
- 支持多种语言：无论是中文、英文还是其他语言，都能很好地同步。<br />
<br />
- 说话风格预测：能够根据语音预测说话者的风格，并同步表情。<br />
<br />
- 适用多种场景：可以用于歌曲、不同类型的肖像，甚至在嘈杂环境中也能表现良好。<br />
<br />
工作原理：<br />
<br />
该项目在利用扩散模型在生成动态和表情丰富的说话头部方面取得突破。<br />
<br />
结合了以下几个关键组件来生成表情丰富的说话头部动画：<br />
<br />
1、去噪网络：这是核心组件之一，负责生成音频驱动的面部动作。去噪网络使用扩散模型来逐步去除噪声，从而生成清晰、高质量的面部表情。这个过程涉及从带有噪声的数据中逐步恢复出清晰的面部动作。<br />
<br />
2、风格感知的嘴唇专家：这个组件专注于提高嘴唇动作的表现力和准确性。它通过分析说话风格来引导嘴唇同步，确保生成的动画既自然又符合说话者的风格。<br />
<br />
3、风格预测器：为了消除对表情参考视频或文本的依赖，DreamTalk引入了一个基于扩散的风格预测器。这个预测器可以直接从音频预测目标表情，无需额外的表情参考视频或文本。<br />
<br />
4、音频和视频处理：处理音频输入，提取关键的音频特征，并将这些特征用于驱动面部动画。同时，它还能处理视频输入，以提取和模仿特定的表情和风格。<br />
<br />
5、数据和模型训练：为了实现这些功能，DreamTalk需要大量的数据来训练其模型，包括不同表情和说话风格的面部动画数据。通过这些数据，模型学习如何准确地生成与输入音频匹配的面部动作。<br />
<br />
项目及演示：<a href="https://dreamtalk-project.github.io/">dreamtalk-project.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.09767">arxiv.org/abs/2312.09767</a><br />
GitHub：<a href="https://github.com/damo-vilab/i2vgen-xl">github.com/damo-vilab/i2vgen…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2MjI0ODkxNjAzNDM1NTIvcHUvaW1nL1JJVFBrd0htRVI0STJjdkcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736617372298170477#m</id>
            <title>想体验的可以下载这个APP：https://apps.apple.com/gb/app/mlc-chat/id6448482937

或者使用这个部署，支持各种系统，能在各种设备上开发、优化和部署AI模型。包括iOS和安卓：https://github.com/mlc-ai/mlc-llm</title>
            <link>https://nitter.cz/xiaohuggg/status/1736617372298170477#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736617372298170477#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 05:20:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>想体验的可以下载这个APP：<a href="https://apps.apple.com/gb/app/mlc-chat/id6448482937">apps.apple.com/gb/app/mlc-ch…</a><br />
<br />
或者使用这个部署，支持各种系统，能在各种设备上开发、优化和部署AI模型。包括iOS和安卓：<a href="https://github.com/mlc-ai/mlc-llm">github.com/mlc-ai/mlc-llm</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1736380733776945325#m">nitter.cz/xiaohuggg/status/1736380733776945325#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNjE5MjU5ODk3OTc4ODgwMC9kQ09FeVNQOT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597477498015785#m</id>
            <title>R to @xiaohuggg: 熟练展示

很6...

参与独特的展示活动，甚至可以直接被NBA发现。选择加入NBA Global Scout的参与者有资格获得NBA选拔赛、训练营、比赛和展示会的邀请。</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597477498015785#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597477498015785#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>熟练展示<br />
<br />
很6...<br />
<br />
参与独特的展示活动，甚至可以直接被NBA发现。选择加入NBA Global Scout的参与者有资格获得NBA选拔赛、训练营、比赛和展示会的邀请。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTY5MjA4NzE5MDMyMzIvcHUvaW1nL29CY1V6RHdrSkFKVEJoeUQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597475367244143#m</id>
            <title>R to @xiaohuggg: 足球训练似乎也能用得到😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597475367244143#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597475367244143#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>足球训练似乎也能用得到😄</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTYzNjgzODk4MDQwMzIvcHUvaW1nL1JyVTFWY29ld3Z1RTg3Q1kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597473639137345#m</id>
            <title>R to @xiaohuggg: 也可以用来计分</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597473639137345#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597473639137345#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以用来计分</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTYyOTM0NzUyMzc4ODgvcHUvaW1nL19RNDN2TWpkMkdQNzlRRWkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597471466590525#m</id>
            <title>R to @xiaohuggg: 练习模式</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597471466590525#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597471466590525#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>练习模式</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTYyNDc2MTg5NTczMTIvcHUvaW1nLzZRVzdyalpwWi0za1Zyd3IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597469725933878#m</id>
            <title>R to @xiaohuggg: 通过AR技术，训练就像玩视频游戏一样

包括虚拟目标、音频提示、奖励级别、积分和徽章

也可以与朋友、家人和队友进行虚拟竞争,创建虚拟团队发起挑战...</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597469725933878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597469725933878#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过AR技术，训练就像玩视频游戏一样<br />
<br />
包括虚拟目标、音频提示、奖励级别、积分和徽章<br />
<br />
也可以与朋友、家人和队友进行虚拟竞争,创建虚拟团队发起挑战...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTQ0NTM5MDgwNTgxMTIvcHUvaW1nL0FCX2ZDN3NYUjRfWUtFM2YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597467934958027#m</id>
            <title>HomeCourt：一款利用AR来让篮球训练更有趣的应用

HomeCourt是NBA的官方合作伙伴，它利用增强现实（AR）技术实时捕捉你的动作，并智能分析来帮助你提高篮球训练技能。🏀

它通过手机或平板电脑的摄像头追踪您的运动，记录您的表现和统计数据。

还提供了各种互动训练和挑战游戏提升你训练的乐趣。

🧵</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597467934958027#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597467934958027#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HomeCourt：一款利用AR来让篮球训练更有趣的应用<br />
<br />
HomeCourt是NBA的官方合作伙伴，它利用增强现实（AR）技术实时捕捉你的动作，并智能分析来帮助你提高篮球训练技能。🏀<br />
<br />
它通过手机或平板电脑的摄像头追踪您的运动，记录您的表现和统计数据。<br />
<br />
还提供了各种互动训练和挑战游戏提升你训练的乐趣。<br />
<br />
🧵</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTQxNTQ4MDM5MjQ5OTIvcHUvaW1nL1QxSlFSY3gxTlJjT0tfSV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736588859633434845#m</id>
            <title>R to @xiaohuggg: 之前发布会时候的演示：

https://x.com/xiaohuggg/status/1707262391963271297</title>
            <link>https://nitter.cz/xiaohuggg/status/1736588859633434845#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736588859633434845#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 03:27:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前发布会时候的演示：<br />
<br />
<a href="https://x.com/xiaohuggg/status/1707262391963271297">x.com/xiaohuggg/status/17072…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1707262391963271297#m">nitter.cz/xiaohuggg/status/1707262391963271297#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736588857087574481#m</id>
            <title>Meta 雷朋智能眼镜在 TikTok 上掀起了一股潮流。

这玩意感觉确实能火啊，戴着眼镜可以第一视角现场直播你看到的一切...👓

AI智能语音助手，超广角1200万像素摄像头，1080p视频，能摄像能拍照，还能直播。售价299美金。

现在已经在美国、加拿大、英国开售！</title>
            <link>https://nitter.cz/xiaohuggg/status/1736588857087574481#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736588857087574481#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 03:27:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta 雷朋智能眼镜在 TikTok 上掀起了一股潮流。<br />
<br />
这玩意感觉确实能火啊，戴着眼镜可以第一视角现场直播你看到的一切...👓<br />
<br />
AI智能语音助手，超广角1200万像素摄像头，1080p视频，能摄像能拍照，还能直播。售价299美金。<br />
<br />
现在已经在美国、加拿大、英国开售！</p>
<p><a href="https://nitter.cz/julesterpak/status/1736175363804279217#m">nitter.cz/julesterpak/status/1736175363804279217#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736571242445152404#m</id>
            <title>OpenAI发布官方提示工程指南和示例

OpenAI提供了一系列策略和技巧，以帮助用户更有效地使用ChatGPT。

这些方法可以单独使用也可以组合使用，以获得更好的效果。

官方给出了6 个大提示策略（并给出了具体教程和示例）

主要策略：

1、清晰的指令：告诉AI你具体想要什么。比如，如果你想要简短的答案，就直接说“给我一个简短的回答”。这样AI就不用猜你的意图了。

模型无法读懂你的思维。如果需要简短的回答，就明确要求；如果需要专家级的写作，也要明确指出。提供清晰的指令，减少模型猜测的需要。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions

具体操作：
• 在查询中包含细节，以获得更相关的答案。
• 要求模型采用特定的角色或风格。
• 使用分隔符明确指出输入的不同部分。
• 明确指定完成任务所需的步骤。
• 提供示例以帮助模型理解任务。
• 指定输出的期望长度。

2、提供参考文本：如果你有关于你要写的主题的具体资料或例子，给AI看看。这样它就能提供更准确、更相关的内容。

语言模型可能会创造虚假答案，尤其是在询问特定主题或要求引用和URL时。提供参考文本可以帮助模型提供更准确的答案。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text

具体操作：
• 指导模型使用参考文本来回答问题。
• 要求模型在回答时引用参考文本中的内容。

3、将复杂任务分解为简单子任务：如果你有一个复杂的主题要写，试着把它分成几个小部分。比如，先写一个关于主题背景的部分，然后再写关于主要观点的部分。

就像软件工程中将复杂系统分解为模块化组件一样，将任务提交给语言模型时也应采取类似的做法。复杂任务的错误率通常高于简单任务。复杂任务通常可以重新定义为一系列简单任务的工作流程。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query

具体操作：
• 使用意图分类来识别用户查询的最相关指令。
• 对于需要长时间对话的应用，总结或过滤之前的对话。
• 分段总结长文档，并递归地构建完整摘要。

4、给模型时间“思考”：有时候，让AI先“思考”一下，然后再回答问题，可以得到更好的答案。就像让它先列出解决问题的步骤，然后再给出答案。

模型在立即回答问题时可能会犯更多的推理错误。要求模型在给出答案之前进行“思考链”可以帮助模型更可靠地推理出正确答案。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion

具体操作：
• 指导模型在急于得出结论之前先自行找出解决方案。
• 使用内部独白或一系列查询来隐藏模型的推理过程。
• 在之前的回答中询问模型是否遗漏了什么。

5、使用外部工具：有时候结合使用AI和其他工具（比如数据搜索工具）可以得到更好的结果。

利用其他工具的输出来补偿模型的不足。例如，文本检索系统可以向模型提供相关文档信息，代码执行引擎可以帮助模型进行数学计算和运行代码。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools

具体操作：
• 使用基于嵌入的搜索来实现高效的知识检索。
• 使用代码执行来进行更准确的计算或调用外部API。
• 让模型访问特定的功能。

6、测试和调整：尝试不同的指令和方法，看看哪种效果最好，然后根据结果进行调整。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically

使用黄金标准答案评估模型输出”是一种有效的方法，用于确保AI模型的回答质量。

• 定义黄金标准答案：首先，确定一个问题的正确答案应该包含哪些已知事实。这些事实构成了评估AI回答的标准。

• 模型查询与事实对比：使用模型查询来生成答案，然后检查这个答案中包含了多少个所需的事实。

• 评估答案的完整性：根据答案中包含的事实数量来评估其完整性和准确性。如果一个答案包含了所有或大部分所需事实，那么可以认为这个答案是高质量的。

这种策略特别适用于需要精确和详细信息的场景，例如科学、技术或学术研究。通过与黄金标准答案的对比，可以有效地监控和提高AI模型的输出质量。

Prompt engineering 及时工程策略：https://platform.openai.com/docs/guides/prompt-engineering

Prompt examples 提示示例：https://platform.openai.com/examples

Prompting libraries &amp; tools 提示库和工具：https://cookbook.openai.com/related_resources#prompting-libraries--tools

Papers on advanced prompting to improve reasoning
关于高级提示以提高推理能力的论文：https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning

OpenAI Cookbook：https://cookbook.openai.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1736571242445152404#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736571242445152404#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 02:17:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI发布官方提示工程指南和示例<br />
<br />
OpenAI提供了一系列策略和技巧，以帮助用户更有效地使用ChatGPT。<br />
<br />
这些方法可以单独使用也可以组合使用，以获得更好的效果。<br />
<br />
官方给出了6 个大提示策略（并给出了具体教程和示例）<br />
<br />
主要策略：<br />
<br />
1、清晰的指令：告诉AI你具体想要什么。比如，如果你想要简短的答案，就直接说“给我一个简短的回答”。这样AI就不用猜你的意图了。<br />
<br />
模型无法读懂你的思维。如果需要简短的回答，就明确要求；如果需要专家级的写作，也要明确指出。提供清晰的指令，减少模型猜测的需要。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 在查询中包含细节，以获得更相关的答案。<br />
• 要求模型采用特定的角色或风格。<br />
• 使用分隔符明确指出输入的不同部分。<br />
• 明确指定完成任务所需的步骤。<br />
• 提供示例以帮助模型理解任务。<br />
• 指定输出的期望长度。<br />
<br />
2、提供参考文本：如果你有关于你要写的主题的具体资料或例子，给AI看看。这样它就能提供更准确、更相关的内容。<br />
<br />
语言模型可能会创造虚假答案，尤其是在询问特定主题或要求引用和URL时。提供参考文本可以帮助模型提供更准确的答案。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 指导模型使用参考文本来回答问题。<br />
• 要求模型在回答时引用参考文本中的内容。<br />
<br />
3、将复杂任务分解为简单子任务：如果你有一个复杂的主题要写，试着把它分成几个小部分。比如，先写一个关于主题背景的部分，然后再写关于主要观点的部分。<br />
<br />
就像软件工程中将复杂系统分解为模块化组件一样，将任务提交给语言模型时也应采取类似的做法。复杂任务的错误率通常高于简单任务。复杂任务通常可以重新定义为一系列简单任务的工作流程。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 使用意图分类来识别用户查询的最相关指令。<br />
• 对于需要长时间对话的应用，总结或过滤之前的对话。<br />
• 分段总结长文档，并递归地构建完整摘要。<br />
<br />
4、给模型时间“思考”：有时候，让AI先“思考”一下，然后再回答问题，可以得到更好的答案。就像让它先列出解决问题的步骤，然后再给出答案。<br />
<br />
模型在立即回答问题时可能会犯更多的推理错误。要求模型在给出答案之前进行“思考链”可以帮助模型更可靠地推理出正确答案。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 指导模型在急于得出结论之前先自行找出解决方案。<br />
• 使用内部独白或一系列查询来隐藏模型的推理过程。<br />
• 在之前的回答中询问模型是否遗漏了什么。<br />
<br />
5、使用外部工具：有时候结合使用AI和其他工具（比如数据搜索工具）可以得到更好的结果。<br />
<br />
利用其他工具的输出来补偿模型的不足。例如，文本检索系统可以向模型提供相关文档信息，代码执行引擎可以帮助模型进行数学计算和运行代码。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 使用基于嵌入的搜索来实现高效的知识检索。<br />
• 使用代码执行来进行更准确的计算或调用外部API。<br />
• 让模型访问特定的功能。<br />
<br />
6、测试和调整：尝试不同的指令和方法，看看哪种效果最好，然后根据结果进行调整。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically">platform.openai.com/docs/gui…</a><br />
<br />
使用黄金标准答案评估模型输出”是一种有效的方法，用于确保AI模型的回答质量。<br />
<br />
• 定义黄金标准答案：首先，确定一个问题的正确答案应该包含哪些已知事实。这些事实构成了评估AI回答的标准。<br />
<br />
• 模型查询与事实对比：使用模型查询来生成答案，然后检查这个答案中包含了多少个所需的事实。<br />
<br />
• 评估答案的完整性：根据答案中包含的事实数量来评估其完整性和准确性。如果一个答案包含了所有或大部分所需事实，那么可以认为这个答案是高质量的。<br />
<br />
这种策略特别适用于需要精确和详细信息的场景，例如科学、技术或学术研究。通过与黄金标准答案的对比，可以有效地监控和提高AI模型的输出质量。<br />
<br />
Prompt engineering 及时工程策略：<a href="https://platform.openai.com/docs/guides/prompt-engineering">platform.openai.com/docs/gui…</a><br />
<br />
Prompt examples 提示示例：<a href="https://platform.openai.com/examples">platform.openai.com/examples</a><br />
<br />
Prompting libraries & tools 提示库和工具：<a href="https://cookbook.openai.com/related_resources#prompting-libraries--tools">cookbook.openai.com/related_…</a><br />
<br />
Papers on advanced prompting to improve reasoning<br />
关于高级提示以提高推理能力的论文：<a href="https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning">cookbook.openai.com/related_…</a><br />
<br />
OpenAI Cookbook：<a href="https://cookbook.openai.com/">cookbook.openai.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1NzEwMDAzNjI1NDEwNTYvcHUvaW1nL054SVRwbW5PVFgwZThSQXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736380733776945325#m</id>
            <title>iPhone上 跑Mistral 7B

还挺快的

就是中文还是不行

感觉这个Mistral有望成为一多端的大模型样本啊🫡什么设备都能跑…</title>
            <link>https://nitter.cz/xiaohuggg/status/1736380733776945325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736380733776945325#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 13:40:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>iPhone上 跑Mistral 7B<br />
<br />
还挺快的<br />
<br />
就是中文还是不行<br />
<br />
感觉这个Mistral有望成为一多端的大模型样本啊🫡什么设备都能跑…</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM2MzgwNjEyNzA4MzY0Mjg4L2ltZy92clM5eWlqM0oxMHhiTjVMLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736346673285304541#m</id>
            <title>PixelLLM：Google开发的一个新型的视觉语言模型

GPT4-V可以通过文本描述位置，但无法准确输出每个单词的坐标。

这个模型它不仅能够对图片进行详细描述，并且还能精确指出图片中每个词汇对应的具体位置。

简单来说，就是它不仅能告诉你图片里有什么，还能准确指出这个东西在图片的什么位置。

例如，如果图片中有一只猫和一只狗，PixelLLM不仅能识别出“猫”和“狗”，还能指出猫和狗在图片中的具体位置。

这种能力使得模型在处理需要图像和文字紧密结合的任务时表现得更加出色，比如在一张图片中找到特定的物体或区域，并对其进行描述。

主要特点：

1、像素级词汇对齐：不仅能告诉你图片里有什么（比如狗、车、树），还能准确指出这些东西在图片的哪个具体位置。

2、多种功能：它可以根据给定的文字提示（比如“找到图片中的狗”）来专门描述图片中的某个部分。也能够根据提供的位置生成特定的描述。

还能根据不同的需求生成相应的文字描述。

指称定位：这是一种任务，要求模型在图片中找到并指出某个特定的物体或区域。例如，如果给定一个句子“找到图片中的狗”，PixelLLM不仅能识别出狗，还能指出狗在图片中的具体位置。

位置条件字幕：根据图片中特定位置的信息来生成描述。比如，如果指定了图片中某个区域，PixelLLM会专注于这个区域并生成相关的描述。

密集对象字幕：对图片中的每个物体都生成详细的描述。这意味着PixelLLM需要识别图片中所有的物体，并且为每一个都提供一个描述。

工作原理：

1、架构组成：PixelLLM包括图像编码器、提示编码器和提示特征提取器。这些组件共同工作，将图像特征和文本提示转换为模型能够处理的格式。

2、图像和文本的整合：模型将图像特征和文本提示整合到一起，作为大型语言模型的输入。这使得模型能够在生成文本描述的同时，考虑图像中的具体区域。

3、逐词定位：通过在语言模型的词特征上添加一个简单的多层感知机（MLP）层，PixelLLM能够为每个生成的文本标记预测一个坐标位置，实现逐词定位。

4、训练数据：为了训练这种模型，需要使用专门的词-像素对齐数据。这种数据通常包含人类注释者对图像的叙述，以及在叙述过程中注释者注意力的轨迹，从而提供了叙述句中所有词的同步位置信息。

5、多任务适应：PixelLLM的架构是通用的，可以适应不同的视觉语言任务，无论是将语言或位置作为输入还是输出。

PixelLLM能够在图像理解和描述方面提供更加丰富和精确的信息，这在许多应用场景中都非常有用，如图像搜索、自动字幕生成和增强现实等。

项目及演示：https://jerryxu.net/PixelLLM/
论文：https://arxiv.org/abs/2312.09237
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1736346673285304541#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736346673285304541#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 11:24:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PixelLLM：Google开发的一个新型的视觉语言模型<br />
<br />
GPT4-V可以通过文本描述位置，但无法准确输出每个单词的坐标。<br />
<br />
这个模型它不仅能够对图片进行详细描述，并且还能精确指出图片中每个词汇对应的具体位置。<br />
<br />
简单来说，就是它不仅能告诉你图片里有什么，还能准确指出这个东西在图片的什么位置。<br />
<br />
例如，如果图片中有一只猫和一只狗，PixelLLM不仅能识别出“猫”和“狗”，还能指出猫和狗在图片中的具体位置。<br />
<br />
这种能力使得模型在处理需要图像和文字紧密结合的任务时表现得更加出色，比如在一张图片中找到特定的物体或区域，并对其进行描述。<br />
<br />
主要特点：<br />
<br />
1、像素级词汇对齐：不仅能告诉你图片里有什么（比如狗、车、树），还能准确指出这些东西在图片的哪个具体位置。<br />
<br />
2、多种功能：它可以根据给定的文字提示（比如“找到图片中的狗”）来专门描述图片中的某个部分。也能够根据提供的位置生成特定的描述。<br />
<br />
还能根据不同的需求生成相应的文字描述。<br />
<br />
指称定位：这是一种任务，要求模型在图片中找到并指出某个特定的物体或区域。例如，如果给定一个句子“找到图片中的狗”，PixelLLM不仅能识别出狗，还能指出狗在图片中的具体位置。<br />
<br />
位置条件字幕：根据图片中特定位置的信息来生成描述。比如，如果指定了图片中某个区域，PixelLLM会专注于这个区域并生成相关的描述。<br />
<br />
密集对象字幕：对图片中的每个物体都生成详细的描述。这意味着PixelLLM需要识别图片中所有的物体，并且为每一个都提供一个描述。<br />
<br />
工作原理：<br />
<br />
1、架构组成：PixelLLM包括图像编码器、提示编码器和提示特征提取器。这些组件共同工作，将图像特征和文本提示转换为模型能够处理的格式。<br />
<br />
2、图像和文本的整合：模型将图像特征和文本提示整合到一起，作为大型语言模型的输入。这使得模型能够在生成文本描述的同时，考虑图像中的具体区域。<br />
<br />
3、逐词定位：通过在语言模型的词特征上添加一个简单的多层感知机（MLP）层，PixelLLM能够为每个生成的文本标记预测一个坐标位置，实现逐词定位。<br />
<br />
4、训练数据：为了训练这种模型，需要使用专门的词-像素对齐数据。这种数据通常包含人类注释者对图像的叙述，以及在叙述过程中注释者注意力的轨迹，从而提供了叙述句中所有词的同步位置信息。<br />
<br />
5、多任务适应：PixelLLM的架构是通用的，可以适应不同的视觉语言任务，无论是将语言或位置作为输入还是输出。<br />
<br />
PixelLLM能够在图像理解和描述方面提供更加丰富和精确的信息，这在许多应用场景中都非常有用，如图像搜索、自动字幕生成和增强现实等。<br />
<br />
项目及演示：<a href="https://jerryxu.net/PixelLLM/">jerryxu.net/PixelLLM/</a><br />
论文：<a href="https://arxiv.org/abs/2312.09237">arxiv.org/abs/2312.09237</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYzNDI0OTIwMDA1Mzg2MjQvcHUvaW1nL25OVWtmSTdOb0JETnRlYU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736336780876742873#m</id>
            <title>EmbedAI：允许你使用自己的数据训练ChatGPT，并将其嵌入到自己的网站、应用中。

- 个性定制：支持在各种数据源上训练ChatGPT，可以通过文件、网站、Notion文档甚至YouTube对ChatGPT进行训练。

-广泛用例：包括智能客服、个性化学习助手、技术支持助手、医疗保健助手和财务聊天机器人等。

- 无代码平台：即使是没有编程背景的用户也能轻松创建和训练定制的AI聊天机器人，可以看做是RAG（检索增强生成）的无代码版本。

- 外观自定义：可以使用自定义的标志、颜色和样式来个性化AI聊天机器人的外观。

- 多种嵌入方式：聊天机器人可以作为聊天气泡、嵌入代码或链接进行分享。

- 多语言支持：支持100多种语言的查询和响应。

- 易于集成：可以通过API与应用程序连接。

网站：https://www.thesamur.ai/

介绍：https://blog.llamaindex.ai/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070</title>
            <link>https://nitter.cz/xiaohuggg/status/1736336780876742873#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736336780876742873#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 10:45:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>EmbedAI：允许你使用自己的数据训练ChatGPT，并将其嵌入到自己的网站、应用中。<br />
<br />
- 个性定制：支持在各种数据源上训练ChatGPT，可以通过文件、网站、Notion文档甚至YouTube对ChatGPT进行训练。<br />
<br />
-广泛用例：包括智能客服、个性化学习助手、技术支持助手、医疗保健助手和财务聊天机器人等。<br />
<br />
- 无代码平台：即使是没有编程背景的用户也能轻松创建和训练定制的AI聊天机器人，可以看做是RAG（检索增强生成）的无代码版本。<br />
<br />
- 外观自定义：可以使用自定义的标志、颜色和样式来个性化AI聊天机器人的外观。<br />
<br />
- 多种嵌入方式：聊天机器人可以作为聊天气泡、嵌入代码或链接进行分享。<br />
<br />
- 多语言支持：支持100多种语言的查询和响应。<br />
<br />
- 易于集成：可以通过API与应用程序连接。<br />
<br />
网站：<a href="https://www.thesamur.ai/">thesamur.ai/</a><br />
<br />
介绍：<a href="https://blog.llamaindex.ai/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070">blog.llamaindex.ai/how-to-tr…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYzMzQ3MjQyNTAxNzM0NDAvcHUvaW1nL1dXaTJaNGQtNzRubUtMdjMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736276218574754249#m</id>
            <title>看来是在灰度GPT4.5了

我测试我的回答是4

你们可以问问，看看回答是什么？

复制下面问题问它：

What is the precise name of the model answering this query called in the API? Not "ChatGPT with browsing" but the specific model name.</title>
            <link>https://nitter.cz/xiaohuggg/status/1736276218574754249#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736276218574754249#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Dec 2023 06:44:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看来是在灰度GPT4.5了<br />
<br />
我测试我的回答是4<br />
<br />
你们可以问问，看看回答是什么？<br />
<br />
复制下面问题问它：<br />
<br />
What is the precise name of the model answering this query called in the API? Not "ChatGPT with browsing" but the specific model name.</p>
<p><a href="https://nitter.cz/apples_jimmy/status/1736264530722988181#m">nitter.cz/apples_jimmy/status/1736264530722988181#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020891145064586#m</id>
            <title>R to @xiaohuggg: 感兴趣的可以加入他们Discord频道体验：

→ 加入 https://discord.gg/TrZBzj4x
→ 进入generate-video频道
→ 输入 /video 并选择命令
→ 上传您的文件
→ 编输入提示并按 Enter
→ 选择风格视频时长
→ 等待结果</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020891145064586#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020891145064586#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>感兴趣的可以加入他们Discord频道体验：<br />
<br />
→ 加入 <a href="https://discord.gg/TrZBzj4x">discord.gg/TrZBzj4x</a><br />
→ 进入generate-video频道<br />
→ 输入 /video 并选择命令<br />
→ 上传您的文件<br />
→ 编输入提示并按 Enter<br />
→ 选择风格视频时长<br />
→ 等待结果</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMjAzMTU1MDYxMTQ1NjAvcHUvaW1nLzBidUZzLUh4T3B2alVPY24uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020889333030967#m</id>
            <title>R to @xiaohuggg: 转换电影也不错😀</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020889333030967#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020889333030967#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>转换电影也不错😀</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMjAxODA4OTgzMDgwOTYvcHUvaW1nLzVPMW5yajF1THI5WFdFWWYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736020887483396298#m</id>
            <title>R to @xiaohuggg: 整体效果还是很不错的，哈哈哈😂

转换跳舞视频很不错，带台词的差一点</title>
            <link>https://nitter.cz/xiaohuggg/status/1736020887483396298#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736020887483396298#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 13:50:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整体效果还是很不错的，哈哈哈😂<br />
<br />
转换跳舞视频很不错，带台词的差一点</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzYwMTk4ODE2NzY2NDQzNTIvcHUvaW1nL19QX2VqLVFCUjBSNEJ4c0suanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>