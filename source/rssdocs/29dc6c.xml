<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735240744057901542#m</id>
            <title>读心术成真！

悉尼科技大学开发了一种能够解码大脑中的想法并将其转换为文本的技术。

这项技术只需要一个便携式设备，不无需通过手术或其他侵入性方法。就能够解读人脑的思维，即使用户没有口头表达，也能将思维转换为文本。

他们通过将多任务EEG编码器与大语言模型连接，从脑电信号中解码信息。

这项研究被选为 NeurIPS 会议的聚焦论文。NeurIPS 是一个顶级年度会议，展示了世界领先的人工智能和机器学习研究。

技术原理：

脑电图（EEG）记录：在研究中，参与者在无声阅读文本段落时佩戴记录头皮上电脑活动的脑电图帽子。

AI 模型 DeWave：研究者开发的 AI 模型 DeWave 将 EEG 信号转换为单词和句子，通过学习大量 EEG 数据来实现。

在这项研究中，参与者戴着特殊的EEG读取帽子默默地阅读文本段落，帽子通过脑电图（EEG）记录通过头皮的脑电活动。在下面的视频中可以看到该技术的演示。

脑电图波被分割成不同的单元，捕捉人脑的特定特征和模式。这是通过研究人员开发的名为 DeWave 的人工智能模型来完成的。 DeWave 通过学习大量脑电图数据将脑电图信号翻译成单词和句子。

这是第一个将离散编码技术纳入大脑到文本翻译过程的技术，引入了一种创新的神经解码方法。与大语言模型的集成也开辟了神经科学和人工智能的新领域

翻译准确性：

目前，翻译准确率大约为 40%，研究人员希望将其提高到接近传统语言翻译或语音识别程序的水平，即接近 90%。

这项技术代表了人工智能和神经科学领域的一个重要突破，为无声沟通和人机交互提供了新的可能性。

应用前景：

帮助无法说话的人：这项技术可以帮助因疾病或伤害（如中风或瘫痪）而无法说话的人进行沟通。

人机交互：它还可以用于无缝的人机交互，例如操作仿生手臂或机器人。

详细内容：https://www.uts.edu.au/news/tech-design/portable-non-invasive-mind-reading-ai-turns-thoughts-text</title>
            <link>https://nitter.cz/xiaohuggg/status/1735240744057901542#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735240744057901542#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 10:10:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>读心术成真！<br />
<br />
悉尼科技大学开发了一种能够解码大脑中的想法并将其转换为文本的技术。<br />
<br />
这项技术只需要一个便携式设备，不无需通过手术或其他侵入性方法。就能够解读人脑的思维，即使用户没有口头表达，也能将思维转换为文本。<br />
<br />
他们通过将多任务EEG编码器与大语言模型连接，从脑电信号中解码信息。<br />
<br />
这项研究被选为 NeurIPS 会议的聚焦论文。NeurIPS 是一个顶级年度会议，展示了世界领先的人工智能和机器学习研究。<br />
<br />
技术原理：<br />
<br />
脑电图（EEG）记录：在研究中，参与者在无声阅读文本段落时佩戴记录头皮上电脑活动的脑电图帽子。<br />
<br />
AI 模型 DeWave：研究者开发的 AI 模型 DeWave 将 EEG 信号转换为单词和句子，通过学习大量 EEG 数据来实现。<br />
<br />
在这项研究中，参与者戴着特殊的EEG读取帽子默默地阅读文本段落，帽子通过脑电图（EEG）记录通过头皮的脑电活动。在下面的视频中可以看到该技术的演示。<br />
<br />
脑电图波被分割成不同的单元，捕捉人脑的特定特征和模式。这是通过研究人员开发的名为 DeWave 的人工智能模型来完成的。 DeWave 通过学习大量脑电图数据将脑电图信号翻译成单词和句子。<br />
<br />
这是第一个将离散编码技术纳入大脑到文本翻译过程的技术，引入了一种创新的神经解码方法。与大语言模型的集成也开辟了神经科学和人工智能的新领域<br />
<br />
翻译准确性：<br />
<br />
目前，翻译准确率大约为 40%，研究人员希望将其提高到接近传统语言翻译或语音识别程序的水平，即接近 90%。<br />
<br />
这项技术代表了人工智能和神经科学领域的一个重要突破，为无声沟通和人机交互提供了新的可能性。<br />
<br />
应用前景：<br />
<br />
帮助无法说话的人：这项技术可以帮助因疾病或伤害（如中风或瘫痪）而无法说话的人进行沟通。<br />
<br />
人机交互：它还可以用于无缝的人机交互，例如操作仿生手臂或机器人。<br />
<br />
详细内容：<a href="https://www.uts.edu.au/news/tech-design/portable-non-invasive-mind-reading-ai-turns-thoughts-text">uts.edu.au/news/tech-design/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUyMTUxMzk1OTc3NjI1NjAvcHUvaW1nLzk4b0JUTlZqdnNqUjJqX2IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735220490413994357#m</id>
            <title>R to @xiaohuggg: Imagen 2、Meta AI、Dalle 3 、Midjourney

效果对比

https://x.com/anukaakash/status/1735216895081718109</title>
            <link>https://nitter.cz/xiaohuggg/status/1735220490413994357#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735220490413994357#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 08:49:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Imagen 2、Meta AI、Dalle 3 、Midjourney<br />
<br />
效果对比<br />
<br />
<a href="https://x.com/anukaakash/status/1735216895081718109">x.com/anukaakash/status/1735…</a></p>
<p><a href="https://nitter.cz/anukaakash/status/1735216895081718109#m">nitter.cz/anukaakash/status/1735216895081718109#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735213721092587685#m</id>
            <title>Google 团队开发出一种可流式传输，实时创建非常逼真的三维场景的技术：SMERF

SMERF 能够在最大300平米的房间内渲染出非常细致的图像，精度达到厘米级别。

最牛P的是，它只在普通智能手机和笔记本电脑就能进行60 fps 或更高的速度实时渲染，并提供完整的六自由度（6DOF）3D导航体验。

SMERF 旨在解决实时视图合成的挑战，特别是在渲染大型、近照片级真实场景时的性能和质量问题。

主要特点：

- 实时渲染大型场景：SMERF 能够快速生成大型三维场景的图像，如室内空间或户外环境，而且几乎没有延迟。SMERF 能够在大场景（最大 300 平方米）中实现实时、高保真的视图合成。

- 高效的内存使用：它使用高效的方法来处理数据，使得即使在内存有限的设备上也能流畅运行。包括普通的智能手机和笔记本电脑。

- 高质量的图像：生成的图像质量高，细节丰富，接近照片级真实感，精度达到厘米级别。

- 六自由度（6DOF）导航：用户可以在三维场景中自由移动，包括向上、向下、向左、向右、前进和后退，以及围绕三个轴旋转。

技术创新：

1、分层模型划分方案：SMERF 采用了一种分层的模型划分方法。这意味着它将大型场景分解成多个层次或部分，每个部分单独处理，从而提高了处理效率和渲染速度。

2、蒸馏训练策略：蒸馏训练是一种机器学习技术，它涉及将知识从一个大型、复杂的模型（教师模型）转移到一个更小、更高效的模型（学生模型）。这种策略有助于提高模型的性能，同时保持较低的计算成本。

项目及演示：https://smerf-3d.github.io/
论文：https://arxiv.org/abs/2312.07541</title>
            <link>https://nitter.cz/xiaohuggg/status/1735213721092587685#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735213721092587685#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 08:22:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 团队开发出一种可流式传输，实时创建非常逼真的三维场景的技术：SMERF<br />
<br />
SMERF 能够在最大300平米的房间内渲染出非常细致的图像，精度达到厘米级别。<br />
<br />
最牛P的是，它只在普通智能手机和笔记本电脑就能进行60 fps 或更高的速度实时渲染，并提供完整的六自由度（6DOF）3D导航体验。<br />
<br />
SMERF 旨在解决实时视图合成的挑战，特别是在渲染大型、近照片级真实场景时的性能和质量问题。<br />
<br />
主要特点：<br />
<br />
- 实时渲染大型场景：SMERF 能够快速生成大型三维场景的图像，如室内空间或户外环境，而且几乎没有延迟。SMERF 能够在大场景（最大 300 平方米）中实现实时、高保真的视图合成。<br />
<br />
- 高效的内存使用：它使用高效的方法来处理数据，使得即使在内存有限的设备上也能流畅运行。包括普通的智能手机和笔记本电脑。<br />
<br />
- 高质量的图像：生成的图像质量高，细节丰富，接近照片级真实感，精度达到厘米级别。<br />
<br />
- 六自由度（6DOF）导航：用户可以在三维场景中自由移动，包括向上、向下、向左、向右、前进和后退，以及围绕三个轴旋转。<br />
<br />
技术创新：<br />
<br />
1、分层模型划分方案：SMERF 采用了一种分层的模型划分方法。这意味着它将大型场景分解成多个层次或部分，每个部分单独处理，从而提高了处理效率和渲染速度。<br />
<br />
2、蒸馏训练策略：蒸馏训练是一种机器学习技术，它涉及将知识从一个大型、复杂的模型（教师模型）转移到一个更小、更高效的模型（学生模型）。这种策略有助于提高模型的性能，同时保持较低的计算成本。<br />
<br />
项目及演示：<a href="https://smerf-3d.github.io/">smerf-3d.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.07541">arxiv.org/abs/2312.07541</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxOTYxMjY1MTU3NDA2NzIvcHUvaW1nL1VGakVvOTBWMlRPMHBTeUcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735204522061148426#m</id>
            <title>ChatGPT作为非人类 入选Nature2023年度十大科学人物

顶级科学期刊《Nature》公布2023 年在科学领域产生重大影响的十位人物。

让人惊讶的是，今年还有一位非人类入选 ，那就是过去一年出尽风头的聊天机器人： ChatGPT。

《自然》特写部主编 Richard Monastersky 表示：

“虽然这个工具不算人物，也不完全满足《自然》十大人物的评选条件，但我们将其破例纳入榜单，从而承认生成式 AI 给科学发展和进步带来的巨大改变。”

其中OpenAI的首席科学家 Ilya Sutskever也上榜了...

以下是他们成就的详细介绍：

1、Kalpana Kalahasti（卡尔帕纳·卡拉哈斯提）

成就：作为工程师和经理，她在确保印度的月球任务 Chandrayaan-3 成功着陆月球方面发挥了关键作用，使印度成为第四个实现这一壮举的国家。

2、Marina Silva（玛丽娜·席尔瓦）

成就：帮助巴西亚马逊控制了猖獗的森林砍伐，并重建了被前政府削弱的机构。

3、Katsuhiko Hayashi（林胜彦）

成就：用雄性小鼠的细胞制造出了有活力的卵子，这一工作有助于拯救濒临灭绝的物种。

4、Annie Kritcher（安妮·克里彻）

成就：这位物理学家帮助美国国家点火装置产生了曾经只在氢弹和恒星中看到的核反应。

5、Eleni Myrivili 

成就：作为联合国首席高温官，正在帮助世界做好应对气候变化威胁的准备。

6、Ilya Sutskever

成就：ChatGPT 和其他正在改变社会的人工智能系统的先驱。

7、James Hamlin

成就：这位物理学家帮助发现了室温超导性的耸人听闻的说法的缺陷。

8、Svetlana Mojsov

成就：因参与开发价值数十亿美元的减肥药物而逐渐获得认可。

9、Halidou Tinto

成就：由于这位研究人员的严格测试，推动了治疗致命疾病的第二种疫苗问世。

10、Thomas Powles

成就：这位医生和癌症研究人员领导了一项治疗严重膀胱癌的变革性临床试验。

11、ChatGPT

成就：作为生成式人工智能的典型代表，预示着科学领域一个崭新时代的可能到来。

所有11位的完整个人档案点击这里查看：https://www.nature.com/immersive/d41586-023-03919-1/index.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1735204522061148426#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735204522061148426#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 07:46:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT作为非人类 入选Nature2023年度十大科学人物<br />
<br />
顶级科学期刊《Nature》公布2023 年在科学领域产生重大影响的十位人物。<br />
<br />
让人惊讶的是，今年还有一位非人类入选 ，那就是过去一年出尽风头的聊天机器人： ChatGPT。<br />
<br />
《自然》特写部主编 Richard Monastersky 表示：<br />
<br />
“虽然这个工具不算人物，也不完全满足《自然》十大人物的评选条件，但我们将其破例纳入榜单，从而承认生成式 AI 给科学发展和进步带来的巨大改变。”<br />
<br />
其中OpenAI的首席科学家 Ilya Sutskever也上榜了...<br />
<br />
以下是他们成就的详细介绍：<br />
<br />
1、Kalpana Kalahasti（卡尔帕纳·卡拉哈斯提）<br />
<br />
成就：作为工程师和经理，她在确保印度的月球任务 Chandrayaan-3 成功着陆月球方面发挥了关键作用，使印度成为第四个实现这一壮举的国家。<br />
<br />
2、Marina Silva（玛丽娜·席尔瓦）<br />
<br />
成就：帮助巴西亚马逊控制了猖獗的森林砍伐，并重建了被前政府削弱的机构。<br />
<br />
3、Katsuhiko Hayashi（林胜彦）<br />
<br />
成就：用雄性小鼠的细胞制造出了有活力的卵子，这一工作有助于拯救濒临灭绝的物种。<br />
<br />
4、Annie Kritcher（安妮·克里彻）<br />
<br />
成就：这位物理学家帮助美国国家点火装置产生了曾经只在氢弹和恒星中看到的核反应。<br />
<br />
5、Eleni Myrivili <br />
<br />
成就：作为联合国首席高温官，正在帮助世界做好应对气候变化威胁的准备。<br />
<br />
6、Ilya Sutskever<br />
<br />
成就：ChatGPT 和其他正在改变社会的人工智能系统的先驱。<br />
<br />
7、James Hamlin<br />
<br />
成就：这位物理学家帮助发现了室温超导性的耸人听闻的说法的缺陷。<br />
<br />
8、Svetlana Mojsov<br />
<br />
成就：因参与开发价值数十亿美元的减肥药物而逐渐获得认可。<br />
<br />
9、Halidou Tinto<br />
<br />
成就：由于这位研究人员的严格测试，推动了治疗致命疾病的第二种疫苗问世。<br />
<br />
10、Thomas Powles<br />
<br />
成就：这位医生和癌症研究人员领导了一项治疗严重膀胱癌的变革性临床试验。<br />
<br />
11、ChatGPT<br />
<br />
成就：作为生成式人工智能的典型代表，预示着科学领域一个崭新时代的可能到来。<br />
<br />
所有11位的完整个人档案点击这里查看：<a href="https://www.nature.com/immersive/d41586-023-03919-1/index.html">nature.com/immersive/d41586-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTcDI5VWFZQUFxeTVGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735184195121823841#m</id>
            <title>R to @xiaohuggg: 可以试试这个公式 分享一下你们

测试的案例看看</title>
            <link>https://nitter.cz/xiaohuggg/status/1735184195121823841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735184195121823841#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 06:25:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可以试试这个公式 分享一下你们<br />
<br />
测试的案例看看</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTY3dZOGJJQUFqNWRmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735184007569330248#m</id>
            <title>知名品牌设计博主、我的好朋友@Salmaaboukarr  分享了一个特定的提示公式。

使用 GPT 4 和 Midjourney 为你的产品快速、低成本的生成高质量产品背景图。

小白也能轻松上手...😀

Salma 的提示公式：

生成图像的提示公式是：

“Generate a (lifestyle/editorial) image for my (描述你的产品/品牌)”，

然后按照以下结构填写细节：

(Composition) (subject) (Colours) (Scene Description) (Style of Shot)。

（构图）（主题）（色彩）（场景描述）（拍摄风格）

操作步骤：首先使用 GPT/DALL·E 生成图像，然后将相同的提示复制并粘贴到 Midjourney 中，以创建最终图像。

可选步骤：可以使用 http://Magnific.ai 来提升图像质量和品质。

最后：可以关注@Salmaaboukarr 她专注使用AI来设计品牌内容图像，可以学到很多东西。❤️</title>
            <link>https://nitter.cz/xiaohuggg/status/1735184007569330248#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735184007569330248#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 06:24:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>知名品牌设计博主、我的好朋友<a href="https://nitter.cz/Salmaaboukarr" title="Salma">@Salmaaboukarr</a>  分享了一个特定的提示公式。<br />
<br />
使用 GPT 4 和 Midjourney 为你的产品快速、低成本的生成高质量产品背景图。<br />
<br />
小白也能轻松上手...😀<br />
<br />
Salma 的提示公式：<br />
<br />
生成图像的提示公式是：<br />
<br />
“Generate a (lifestyle/editorial) image for my (描述你的产品/品牌)”，<br />
<br />
然后按照以下结构填写细节：<br />
<br />
(Composition) (subject) (Colours) (Scene Description) (Style of Shot)。<br />
<br />
（构图）（主题）（色彩）（场景描述）（拍摄风格）<br />
<br />
操作步骤：首先使用 GPT/DALL·E 生成图像，然后将相同的提示复制并粘贴到 Midjourney 中，以创建最终图像。<br />
<br />
可选步骤：可以使用 <a href="http://Magnific.ai">Magnific.ai</a> 来提升图像质量和品质。<br />
<br />
最后：可以关注<a href="https://nitter.cz/Salmaaboukarr" title="Salma">@Salmaaboukarr</a> 她专注使用AI来设计品牌内容图像，可以学到很多东西。❤️</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTVVhUVmFVQUF2M2M0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735175996285096021#m</id>
            <title>R to @xiaohuggg: Outfit Anyone 在线体验地址：https://huggingface.co/spaces/HumanAIGC/OutfitAnyone</title>
            <link>https://nitter.cz/xiaohuggg/status/1735175996285096021#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735175996285096021#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:52:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Outfit Anyone 在线体验地址：<a href="https://huggingface.co/spaces/HumanAIGC/OutfitAnyone">huggingface.co/spaces/HumanA…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JTVk40OGFZQUFEQzFhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735171253428998556#m</id>
            <title>R to @xiaohuggg: 动漫角色也支持</title>
            <link>https://nitter.cz/xiaohuggg/status/1735171253428998556#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735171253428998556#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:33:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>动漫角色也支持</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0JTUkJ1cWFjQUFpM0pTLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dCU1JCdXFhY0FBaTNKUy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735171028077449512#m</id>
            <title>R to @xiaohuggg: 支持Outfit 全套服装 一起更换</title>
            <link>https://nitter.cz/xiaohuggg/status/1735171028077449512#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735171028077449512#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:33:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持Outfit 全套服装 一起更换</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNzA5MTgxNjk4ODI2MjQvcHUvaW1nL2hZYS1zVkVOSUJVbndld2MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735170872774963222#m</id>
            <title>Outfit Anyone：由阿里巴巴开发的一个虚拟试衣技术，可以让你在电脑上看到自己穿上任何衣服的样子。

- 逼真的效果：它能创建非常真实的图片，让用户看起来就像真的穿上了那些衣服一样。

- 适用于任何衣服和人物：无论是什么样的衣服或是什么样的人，都能完美适配。

- 个性化时尚展示：可以展示各种独特和时尚的服装搭配。

- 适应不同体型：能够适应不同的体型，为各种身材的人提供试衣效果。

- 强大泛化能力：模型具有强大的泛化能力，可以支持动画角色的新服装形象创建。

- 细节增强：它能够显着增强服装的质感和真实感，同时保持服装的一致性。

- Outfit Anyone + Animate Anyone：与 Animate Anybody（图像动作视频模型https://twitter.com/xiaohuggg/status/1730133378501067046）的集成，以实现任何角色的服装变化和动态视频生成。

项目地址：https://humanaigc.github.io/outfit-anyone/
GitHub：https://github.com/HumanAIGC/OutfitAnyone（coming soon...）</title>
            <link>https://nitter.cz/xiaohuggg/status/1735170872774963222#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735170872774963222#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 05:32:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Outfit Anyone：由阿里巴巴开发的一个虚拟试衣技术，可以让你在电脑上看到自己穿上任何衣服的样子。<br />
<br />
- 逼真的效果：它能创建非常真实的图片，让用户看起来就像真的穿上了那些衣服一样。<br />
<br />
- 适用于任何衣服和人物：无论是什么样的衣服或是什么样的人，都能完美适配。<br />
<br />
- 个性化时尚展示：可以展示各种独特和时尚的服装搭配。<br />
<br />
- 适应不同体型：能够适应不同的体型，为各种身材的人提供试衣效果。<br />
<br />
- 强大泛化能力：模型具有强大的泛化能力，可以支持动画角色的新服装形象创建。<br />
<br />
- 细节增强：它能够显着增强服装的质感和真实感，同时保持服装的一致性。<br />
<br />
- Outfit Anyone + Animate Anyone：与 Animate Anybody（图像动作视频模型<a href="https://nitter.cz/xiaohuggg/status/1730133378501067046">nitter.cz/xiaohuggg/status…</a>）的集成，以实现任何角色的服装变化和动态视频生成。<br />
<br />
项目地址：<a href="https://humanaigc.github.io/outfit-anyone/">humanaigc.github.io/outfit-a…</a><br />
GitHub：<a href="https://github.com/HumanAIGC/OutfitAnyone">github.com/HumanAIGC/OutfitA…</a>（coming soon...）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNzAxOTU0MzY3NTI4OTYvcHUvaW1nL1hrMFktSENMNWxfbExZTmwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735149171781533870#m</id>
            <title>DeepMind 开发的文本到图像生成技术：Imagen 2

- 根据文字生成图片：利用文本提示生成高质量、逼真的图像。

- 更真实的图像生成：在渲染真实的手和人脸等方面取得了进步，减少了视觉干扰元素。

- 改善图片和文字的匹配：通过更好地理解文字描述，Imagen 2 能更准确地生成与描述相符的图片。

- 图像编辑能力：支持“修图”和“扩图”功能，用户可以通过提供参考图像和图像遮罩来生成新内容，直接进入原始图像，或将原始图像扩展到其边界之外。

技术创新：

Imagen 2使用基于扩散的技术来提高图像生成的灵活性，特别是在控制和调整图像风格方面。用户可以通过结合参考图像和文本提示来引导图像生成过程，创造出符合特定风格和内容的视觉作品。

它通过利用其训练数据的自然分布来生成更逼真的图像，而不是采用预设的风格。

开发团队特别强调了对图像附加文字描述（字幕）的重要性。这种方法的具体含义和目的如下：

增强训练数据集：为了提高模型生成图像的质量和准确性，Imagen 2 的训练数据集中包含了更多详细的图像字幕。这些字幕为模型提供了关于图像内容的额外信息。

学习不同的字幕风格：通过训练，Imagen 2 学会了理解和解释不同风格的文字描述。这意味着模型能够处理和响应各种类型的用户输入，无论其语言风格如何。

更好地理解用户提示：增加对图像字幕的描述不仅提高了模型对特定图像内容的理解，还使其能够更准确地根据用户的文本提示生成图像。这是因为模型通过学习大量的图像和对应字幕，能够更好地把握文本和视觉内容之间的关联。

 Imagen 2 现已在Vertex AI 上全面推出，你可以在几秒钟内将用户的想象力转化为高品质的视觉素材。

- 文字转图片生成：使用文字提示来生成新图片。

- 使用文字提示修改图片：允许用户使用文字提示来修改整个上传或生成的图片。

- 局部图片修改：用户可以定义遮盖区域，仅修改上传或生成的图片的某些部分。

- 图片放大：支持放大现有的、生成的或编辑后的图片。

- 主题模型微调：允许用户使用特定主题（如特定的手提包或鞋）来微调模型，以生成图片。

- 视觉字幕：提供图片的文字说明。

- 视觉问答 (VQA)：提供有关图片的问题解答。

- 多语言文本渲染：支持多种语言，能够在图像中准确地覆盖文本。

- 标志生成：能够创造公司或产品标志，并将其覆盖在图像中。

Imagen 2详细介绍：https://deepmind.google/technologies/imagen-2/

Imagen 2 on Vertex AI：https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available</title>
            <link>https://nitter.cz/xiaohuggg/status/1735149171781533870#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735149171781533870#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 04:06:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind 开发的文本到图像生成技术：Imagen 2<br />
<br />
- 根据文字生成图片：利用文本提示生成高质量、逼真的图像。<br />
<br />
- 更真实的图像生成：在渲染真实的手和人脸等方面取得了进步，减少了视觉干扰元素。<br />
<br />
- 改善图片和文字的匹配：通过更好地理解文字描述，Imagen 2 能更准确地生成与描述相符的图片。<br />
<br />
- 图像编辑能力：支持“修图”和“扩图”功能，用户可以通过提供参考图像和图像遮罩来生成新内容，直接进入原始图像，或将原始图像扩展到其边界之外。<br />
<br />
技术创新：<br />
<br />
Imagen 2使用基于扩散的技术来提高图像生成的灵活性，特别是在控制和调整图像风格方面。用户可以通过结合参考图像和文本提示来引导图像生成过程，创造出符合特定风格和内容的视觉作品。<br />
<br />
它通过利用其训练数据的自然分布来生成更逼真的图像，而不是采用预设的风格。<br />
<br />
开发团队特别强调了对图像附加文字描述（字幕）的重要性。这种方法的具体含义和目的如下：<br />
<br />
增强训练数据集：为了提高模型生成图像的质量和准确性，Imagen 2 的训练数据集中包含了更多详细的图像字幕。这些字幕为模型提供了关于图像内容的额外信息。<br />
<br />
学习不同的字幕风格：通过训练，Imagen 2 学会了理解和解释不同风格的文字描述。这意味着模型能够处理和响应各种类型的用户输入，无论其语言风格如何。<br />
<br />
更好地理解用户提示：增加对图像字幕的描述不仅提高了模型对特定图像内容的理解，还使其能够更准确地根据用户的文本提示生成图像。这是因为模型通过学习大量的图像和对应字幕，能够更好地把握文本和视觉内容之间的关联。<br />
<br />
 Imagen 2 现已在Vertex AI 上全面推出，你可以在几秒钟内将用户的想象力转化为高品质的视觉素材。<br />
<br />
- 文字转图片生成：使用文字提示来生成新图片。<br />
<br />
- 使用文字提示修改图片：允许用户使用文字提示来修改整个上传或生成的图片。<br />
<br />
- 局部图片修改：用户可以定义遮盖区域，仅修改上传或生成的图片的某些部分。<br />
<br />
- 图片放大：支持放大现有的、生成的或编辑后的图片。<br />
<br />
- 主题模型微调：允许用户使用特定主题（如特定的手提包或鞋）来微调模型，以生成图片。<br />
<br />
- 视觉字幕：提供图片的文字说明。<br />
<br />
- 视觉问答 (VQA)：提供有关图片的问题解答。<br />
<br />
- 多语言文本渲染：支持多种语言，能够在图像中准确地覆盖文本。<br />
<br />
- 标志生成：能够创造公司或产品标志，并将其覆盖在图像中。<br />
<br />
Imagen 2详细介绍：<a href="https://deepmind.google/technologies/imagen-2/">deepmind.google/technologies…</a><br />
<br />
Imagen 2 on Vertex AI：<a href="https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available">cloud.google.com/blog/produc…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDY0OTkwNjIwMTM5NTIvcHUvaW1nL21ubm9BQ0NjeURlUG1lY0guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735143546867569089#m</id>
            <title>Dolphins：一种新型的视觉-语言模型，旨在模拟人类驾驶能力

Dolphins能理解和处理视频、文字指令和驾驶信号。分析驾驶场景，比如识别城市交叉路口、夜间交通等。

它还能预测和规划车辆行为，比如为什么要慢行。

Dolphins还展现了类似人类的能力，即时学习和适应、通过上下文学习的反思和错误恢复。

该项目由来自威斯康星大学麦迪逊分校、NVIDIA、密歇根大学和斯坦福大学的研究人员共同参与。旨在让自动驾驶汽车更好地理解复杂的驾驶环境，类似于人类司机的理解能力。

独特功能:

Dolphins 具备全面理解复杂和长尾开放世界驾驶场景的能力，并能解决一系列 AV 任务。

能够处理包括场景理解、预测和规划在内的多种任务，感知静态和动态场景，整合环境因素，并有效处理下游预测和规划任务。

Dolphins 主要能做以下几件事：

1、理解复杂驾驶场景：Dolphins 能够分析和理解各种驾驶环境，比如城市交叉路口、夜间繁忙道路、隧道内行驶等。

2、处理多模态输入：能够处理包括视频（或图像）数据、文本指令和历史控制信号在内的多模态输入，以生成与提供的指令相对应的输出。

3、预测和规划行为：能够预测车辆在不同情况下的行为，如在交通拥堵时的低速行驶。它还能为车辆制定未来的行动计划，比如在遇到红灯时停车等待。

4、提供详细的场景描述：能够详细描述视频中的驾驶相关信息，如路况、交通状况、其他车辆和行人的行为等。

5、适应和学习：它具备快速学习和适应新场景的能力，能够根据实时情况做出反应和调整。Dolphins 展现了类似人类的能力，包括无梯度的即时适应、通过上下文学习的反思和错误恢复。

技术细节和原理：

1、基于 OpenFlamingo 构建：Dolphins 基于开源的预训练视觉-语言模型 OpenFlamingo 构建，并针对驾驶领域进行了特定的指令调整和数据构建。

2、BDD-X 数据集的应用：通过利用 BDD-X 数据集，Dolphins 融合了四种不同的自动驾驶车辆（AV）任务，以全面理解复杂的驾驶场景。

3、GCoT 过程：通过创新的 GCoT 过程，Dolphins 的推理能力得到增强。这个过程涉及构建图像指令跟随数据集，并基于公共的视觉问答（VQA）数据集、视觉指令数据集和 ChatGPT 来培养 OpenFlamingo 模型的细粒度推理能力。

4、驾驶领域的定制：Dolphins 通过构建特定于驾驶的指令数据和进行指令调整，被专门调整以适应驾驶领域。

项目及演示：https://vlm-driver.github.io/
论文：https://arxiv.org/abs/2312.00438
代码：https://github.com/vlm-driver/Dolphins</title>
            <link>https://nitter.cz/xiaohuggg/status/1735143546867569089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735143546867569089#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 03:43:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Dolphins：一种新型的视觉-语言模型，旨在模拟人类驾驶能力<br />
<br />
Dolphins能理解和处理视频、文字指令和驾驶信号。分析驾驶场景，比如识别城市交叉路口、夜间交通等。<br />
<br />
它还能预测和规划车辆行为，比如为什么要慢行。<br />
<br />
Dolphins还展现了类似人类的能力，即时学习和适应、通过上下文学习的反思和错误恢复。<br />
<br />
该项目由来自威斯康星大学麦迪逊分校、NVIDIA、密歇根大学和斯坦福大学的研究人员共同参与。旨在让自动驾驶汽车更好地理解复杂的驾驶环境，类似于人类司机的理解能力。<br />
<br />
独特功能:<br />
<br />
Dolphins 具备全面理解复杂和长尾开放世界驾驶场景的能力，并能解决一系列 AV 任务。<br />
<br />
能够处理包括场景理解、预测和规划在内的多种任务，感知静态和动态场景，整合环境因素，并有效处理下游预测和规划任务。<br />
<br />
Dolphins 主要能做以下几件事：<br />
<br />
1、理解复杂驾驶场景：Dolphins 能够分析和理解各种驾驶环境，比如城市交叉路口、夜间繁忙道路、隧道内行驶等。<br />
<br />
2、处理多模态输入：能够处理包括视频（或图像）数据、文本指令和历史控制信号在内的多模态输入，以生成与提供的指令相对应的输出。<br />
<br />
3、预测和规划行为：能够预测车辆在不同情况下的行为，如在交通拥堵时的低速行驶。它还能为车辆制定未来的行动计划，比如在遇到红灯时停车等待。<br />
<br />
4、提供详细的场景描述：能够详细描述视频中的驾驶相关信息，如路况、交通状况、其他车辆和行人的行为等。<br />
<br />
5、适应和学习：它具备快速学习和适应新场景的能力，能够根据实时情况做出反应和调整。Dolphins 展现了类似人类的能力，包括无梯度的即时适应、通过上下文学习的反思和错误恢复。<br />
<br />
技术细节和原理：<br />
<br />
1、基于 OpenFlamingo 构建：Dolphins 基于开源的预训练视觉-语言模型 OpenFlamingo 构建，并针对驾驶领域进行了特定的指令调整和数据构建。<br />
<br />
2、BDD-X 数据集的应用：通过利用 BDD-X 数据集，Dolphins 融合了四种不同的自动驾驶车辆（AV）任务，以全面理解复杂的驾驶场景。<br />
<br />
3、GCoT 过程：通过创新的 GCoT 过程，Dolphins 的推理能力得到增强。这个过程涉及构建图像指令跟随数据集，并基于公共的视觉问答（VQA）数据集、视觉指令数据集和 ChatGPT 来培养 OpenFlamingo 模型的细粒度推理能力。<br />
<br />
4、驾驶领域的定制：Dolphins 通过构建特定于驾驶的指令数据和进行指令调整，被专门调整以适应驾驶领域。<br />
<br />
项目及演示：<a href="https://vlm-driver.github.io/">vlm-driver.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.00438">arxiv.org/abs/2312.00438</a><br />
代码：<a href="https://github.com/vlm-driver/Dolphins">github.com/vlm-driver/Dolphi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDEzNTUxODg1OTI2NDAvcHUvaW1nL0RKUmZFN1BMdEhRcWVVYlouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735129903937560615#m</id>
            <title>这个挺好玩的 哈哈哈😂

用摄像头进行输入，以30+fps AI实时生图，把你变成另一个人...

可以使用提示词控制你想要生成什么图像或者模仿谁谁谁...

在电脑上尝试：http://fal.ai/camera</title>
            <link>https://nitter.cz/xiaohuggg/status/1735129903937560615#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735129903937560615#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 02:49:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺好玩的 哈哈哈😂<br />
<br />
用摄像头进行输入，以30+fps AI实时生图，把你变成另一个人...<br />
<br />
可以使用提示词控制你想要生成什么图像或者模仿谁谁谁...<br />
<br />
在电脑上尝试：<a href="http://fal.ai/camera">fal.ai/camera</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxMjc5MzI1MjY1ODM4MDgvcHUvaW1nL2poLUo4b1hIZXRFc1RyeUEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735126281552044403#m</id>
            <title>ChatGPT Plus 恢复订阅了

搞到了更多GPU...

看来目前最大的阻碍依然是算力问题...</title>
            <link>https://nitter.cz/xiaohuggg/status/1735126281552044403#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735126281552044403#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 02:35:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Plus 恢复订阅了<br />
<br />
搞到了更多GPU...<br />
<br />
看来目前最大的阻碍依然是算力问题...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JSbi1yR2JRQUFCNjdxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734958068595831011#m</id>
            <title>Google宣布Gemini Pro 版本已经向开发者和企业开放，可用于构建 AI 应用。

最重要的是目前完全免费！🆓💰

• 免费使用：目前可以在限制内免费使用，并且未来将提供具有竞争力的定价。

• 特性：支持包括函数调用、嵌入、语义检索、自定义知识基础和聊天功能。

• 语言支持：支持全球 180 多个国家和地区的 38 种语言。

开发者目前可以通过 Google AI Studio 免费访问 Gemini Pro 和 Gemini Pro Vision，适用于大多数应用程序开发需求。

Google计划在明年初推出 Gemini Ultra…

详细：https://blog.google/technology/ai/gemini-api-developers-cloud/?utm_sourc</title>
            <link>https://nitter.cz/xiaohuggg/status/1734958068595831011#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734958068595831011#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 15:26:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google宣布Gemini Pro 版本已经向开发者和企业开放，可用于构建 AI 应用。<br />
<br />
最重要的是目前完全免费！🆓💰<br />
<br />
• 免费使用：目前可以在限制内免费使用，并且未来将提供具有竞争力的定价。<br />
<br />
• 特性：支持包括函数调用、嵌入、语义检索、自定义知识基础和聊天功能。<br />
<br />
• 语言支持：支持全球 180 多个国家和地区的 38 种语言。<br />
<br />
开发者目前可以通过 Google AI Studio 免费访问 Gemini Pro 和 Gemini Pro Vision，适用于大多数应用程序开发需求。<br />
<br />
Google计划在明年初推出 Gemini Ultra…<br />
<br />
详细：<a href="https://blog.google/technology/ai/gemini-api-developers-cloud/?utm_sourc">blog.google/technology/ai/ge…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM0OTU4MDAyNTg1ODY2MjQxL2ltZy9rV21zUHM5dlh2bDVNN2ZKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</id>
            <title>R to @xiaohuggg: 将Logo转换成一些真实场景图</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882384385110292#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将Logo转换成一些真实场景图</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIyNTM3OTExNzQ2NTYvcHUvaW1nLzZXTkFWMkkzSGNwOFBFZVEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</id>
            <title>R to @xiaohuggg: 图像放大增强演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882381759443163#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>图像放大增强演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIxMzA1NjI2MTMyNDgvcHUvaW1nL3Y5Zm9JSEpzem1DXzZSeU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>