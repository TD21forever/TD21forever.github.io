<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724643055532413310#m</id>
            <title>DeepMind 开发出一种用于天气预报的人工智能模型：GraphCast 。

它可以在不到一分钟的时间内完成10天内的天气预报，准确性超过了业界公认的高标准的欧洲中期天气预报中心（ECMWF）的高分辨率天气模拟系统（HRES）。

它还能够提前预测像飓风、洪水等极端天气事件。

DeepMind已开源GraphCast的代码。

主要特点：

1、高精度天气预测：GraphCast 能够提供长达10天的天气预测，其准确性超过了行业标准的高分辨率天气模拟系统（HRES），由欧洲中期天气预报中心（ECMWF）制作。

2、极端天气事件的早期预警：GraphCast 能够更早地预测极端天气事件，如准确预测气旋的路径、识别与洪水风险相关的大气河流，以及预测极端温度的发生。这种能力有助于通过更好的准备来挽救生命。

3、基于深度学习的天气预测系统：GraphCast 是一个基于机器学习和图神经网络（GNNs）的天气预测系统。通过训练，GraphCast 学习识别这些数据中的天气模式和趋势。例如，它可以学习识别导致风暴或高温的特定气候条件。

4、全球覆盖：它在全球范围内以0.25度经纬度的高分辨率进行预测，覆盖了地球表面的超过一百万个网格点。能够提供全球范围内的天气预报，这对于国际旅行、全球业务运营和气候研究都非常有用。

5、高效的预测模型：尽管GraphCast的训练过程计算量大，但最终的预测模型非常高效。使用GraphCast进行10天的预测仅需不到一分钟的时间，而传统方法如HRES可能需要数小时的超级计算机计算。

6、持续学习和适应：随着时间的推移，GraphCast 可以继续从新的气象数据中学习，不断提高其预测的准确性和可靠性。

7、广泛的应用：GraphCast 已被多个天气机构使用，包括 ECMWF，该机构已经在其网站上运行了模型预测的实时实验。https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850

8、开源代码：为了使基于AI的天气预报更加普及，DeepMind 已开源 GraphCast 模型的代码，使全球的科学家和预报员都能从中受益。

详细介绍：https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/

Science 论文：https://www.science.org/doi/10.1126/science.adi2336

Paper PDF：https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/Learning_skillful_medium-range_global_weather_forecasting.pdf

开源代码：https://github.com/google-deepmind/graphcast</title>
            <link>https://nitter.cz/xiaohuggg/status/1724643055532413310#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724643055532413310#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 04:18:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind 开发出一种用于天气预报的人工智能模型：GraphCast 。<br />
<br />
它可以在不到一分钟的时间内完成10天内的天气预报，准确性超过了业界公认的高标准的欧洲中期天气预报中心（ECMWF）的高分辨率天气模拟系统（HRES）。<br />
<br />
它还能够提前预测像飓风、洪水等极端天气事件。<br />
<br />
DeepMind已开源GraphCast的代码。<br />
<br />
主要特点：<br />
<br />
1、高精度天气预测：GraphCast 能够提供长达10天的天气预测，其准确性超过了行业标准的高分辨率天气模拟系统（HRES），由欧洲中期天气预报中心（ECMWF）制作。<br />
<br />
2、极端天气事件的早期预警：GraphCast 能够更早地预测极端天气事件，如准确预测气旋的路径、识别与洪水风险相关的大气河流，以及预测极端温度的发生。这种能力有助于通过更好的准备来挽救生命。<br />
<br />
3、基于深度学习的天气预测系统：GraphCast 是一个基于机器学习和图神经网络（GNNs）的天气预测系统。通过训练，GraphCast 学习识别这些数据中的天气模式和趋势。例如，它可以学习识别导致风暴或高温的特定气候条件。<br />
<br />
4、全球覆盖：它在全球范围内以0.25度经纬度的高分辨率进行预测，覆盖了地球表面的超过一百万个网格点。能够提供全球范围内的天气预报，这对于国际旅行、全球业务运营和气候研究都非常有用。<br />
<br />
5、高效的预测模型：尽管GraphCast的训练过程计算量大，但最终的预测模型非常高效。使用GraphCast进行10天的预测仅需不到一分钟的时间，而传统方法如HRES可能需要数小时的超级计算机计算。<br />
<br />
6、持续学习和适应：随着时间的推移，GraphCast 可以继续从新的气象数据中学习，不断提高其预测的准确性和可靠性。<br />
<br />
7、广泛的应用：GraphCast 已被多个天气机构使用，包括 ECMWF，该机构已经在其网站上运行了模型预测的实时实验。<a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850">charts.ecmwf.int/products/gr…</a><br />
<br />
8、开源代码：为了使基于AI的天气预报更加普及，DeepMind 已开源 GraphCast 模型的代码，使全球的科学家和预报员都能从中受益。<br />
<br />
详细介绍：<a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">deepmind.google/discover/blo…</a><br />
<br />
Science 论文：<a href="https://www.science.org/doi/10.1126/science.adi2336">science.org/doi/10.1126/scie…</a><br />
<br />
Paper PDF：<a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/Learning_skillful_medium-range_global_weather_forecasting.pdf">storage.googleapis.com/deepm…</a><br />
<br />
开源代码：<a href="https://github.com/google-deepmind/graphcast">github.com/google-deepmind/g…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ2NDI5MDMxNDkxMjk3MjgvcHUvaW1nL3FOZTRJTURmQXc3Z2tTeVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724633998553706602#m</id>
            <title>R to @xiaohuggg: ChatGPT Plus会员账号要值钱了，这一段时间，卖账号估计能赚一笔！🙂</title>
            <link>https://nitter.cz/xiaohuggg/status/1724633998553706602#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724633998553706602#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 03:42:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Plus会员账号要值钱了，这一段时间，卖账号估计能赚一笔！🙂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724631012259557686#m</id>
            <title>⏰快讯：服务压力陡增@sama 奥特曼宣布

暂停ChatGPT Plus 会员注册

持续时间未知...</title>
            <link>https://nitter.cz/xiaohuggg/status/1724631012259557686#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724631012259557686#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 03:30:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>⏰快讯：服务压力陡增<a href="https://nitter.cz/sama" title="Sam Altman">@sama</a> 奥特曼宣布<br />
<br />
暂停ChatGPT Plus 会员注册<br />
<br />
持续时间未知...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi04ZDh5WWIwQUFyVkVELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724599830645997662#m</id>
            <title>R to @xiaohuggg: 也可以对年龄、人种等进行设置，面部会自动生成相应的年龄面部和人种...</title>
            <link>https://nitter.cz/xiaohuggg/status/1724599830645997662#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724599830645997662#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:26:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以对年龄、人种等进行设置，面部会自动生成相应的年龄面部和人种...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTg5OTEyMzE1NTc2MzIvcHUvaW1nL0otNDIxZVdDZXIxNlA5WDUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724598926748323891#m</id>
            <title>R to @xiaohuggg: 可以说不同的方言

支持将您自己的面部上传到生成模型。一些没有脸部的图像也可以生成面对对话。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724598926748323891#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724598926748323891#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:23:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可以说不同的方言<br />
<br />
支持将您自己的面部上传到生成模型。一些没有脸部的图像也可以生成面对对话。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTg1MjYxMTY4MTQ4NDgvcHUvaW1nL1pPd2M1a3lQaVB0aWllc3cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724597996225232906#m</id>
            <title>这个超有意思 哈哈哈哈 😂 字节跳动搞的

ChatAnything：可以与 LLMs 增强角色进行视频聊天

你可以通过文本描述，生成具有独特个性、外观和声音的虚拟角色人物。

而且这个人物不仅有自己的外观，还有独特的声音和个性，还可以跟你进行语音对话和视频聊天。😅

ChatAnything的工作原理基于几个关键的技术组件，这些组件共同协作，使得用户能够通过文本描述来创建和动画化个性化的虚拟角色。

以下是其主要工作流程和组件：

1、生成个性化角色：

•用户提供一个文本描述，定义想要创建的角色的特征和个性。

•大语言模型（LLM）根据这些描述生成一个具有特定个性的角色。

2、声音和外观的生成：

•声音混合（MoV）：这一部分利用文本到语音（TTS）技术。根据用户的文本描述，系统自动选择最匹配的声音特征，为角色生成独特的声音。

•扩散器混合（MoD）：结合了文本到图像生成技术和说话头部算法。这一步骤简化了生成说话对象的外观的过程。

3、动画化角色：

•一旦角色的个性、声音和外观被生成，系统使用这些信息来动画化角色。

•这包括将声音信号与生成的图像相结合，使角色能够根据用户的指令进行“说话”和“表现”。

4、面部运动的生成：

•为了使生成的角色更加逼真，系统还包括了面部运动的生成。

•这一部分涉及到像素级引导，它在图像生成阶段注入人脸标记，以确保面部运动的自然性和准确性。

ChatAnything的目标是通过文本输入，使用户能够以任何人格化的方式动画化任何事物。

项目地址：https://chatanything.github.io/
论文：https://arxiv.org/abs/2311.06772
GitHub：https://github.com/zhoudaquan/ChatAnything
在线演示：https://26fed97b4a7706bed0.gradio.live/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724597996225232906#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724597996225232906#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 01:19:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个超有意思 哈哈哈哈 😂 字节跳动搞的<br />
<br />
ChatAnything：可以与 LLMs 增强角色进行视频聊天<br />
<br />
你可以通过文本描述，生成具有独特个性、外观和声音的虚拟角色人物。<br />
<br />
而且这个人物不仅有自己的外观，还有独特的声音和个性，还可以跟你进行语音对话和视频聊天。😅<br />
<br />
ChatAnything的工作原理基于几个关键的技术组件，这些组件共同协作，使得用户能够通过文本描述来创建和动画化个性化的虚拟角色。<br />
<br />
以下是其主要工作流程和组件：<br />
<br />
1、生成个性化角色：<br />
<br />
•用户提供一个文本描述，定义想要创建的角色的特征和个性。<br />
<br />
•大语言模型（LLM）根据这些描述生成一个具有特定个性的角色。<br />
<br />
2、声音和外观的生成：<br />
<br />
•声音混合（MoV）：这一部分利用文本到语音（TTS）技术。根据用户的文本描述，系统自动选择最匹配的声音特征，为角色生成独特的声音。<br />
<br />
•扩散器混合（MoD）：结合了文本到图像生成技术和说话头部算法。这一步骤简化了生成说话对象的外观的过程。<br />
<br />
3、动画化角色：<br />
<br />
•一旦角色的个性、声音和外观被生成，系统使用这些信息来动画化角色。<br />
<br />
•这包括将声音信号与生成的图像相结合，使角色能够根据用户的指令进行“说话”和“表现”。<br />
<br />
4、面部运动的生成：<br />
<br />
•为了使生成的角色更加逼真，系统还包括了面部运动的生成。<br />
<br />
•这一部分涉及到像素级引导，它在图像生成阶段注入人脸标记，以确保面部运动的自然性和准确性。<br />
<br />
ChatAnything的目标是通过文本输入，使用户能够以任何人格化的方式动画化任何事物。<br />
<br />
项目地址：<a href="https://chatanything.github.io/">chatanything.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2311.06772">arxiv.org/abs/2311.06772</a><br />
GitHub：<a href="https://github.com/zhoudaquan/ChatAnything">github.com/zhoudaquan/ChatAn…</a><br />
在线演示：<a href="https://26fed97b4a7706bed0.gradio.live/">26fed97b4a7706bed0.gradio.li…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTUzMTk0MTYyNzA4NDgvcHUvaW1nL0JnZ2xVbHU0dk54Z2tUVk8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724591752424849469#m</id>
            <title>R to @xiaohuggg: 手机上的演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1724591752424849469#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724591752424849469#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 00:54:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>手机上的演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTExMzk5MTM5NDA5OTIvcHUvaW1nL01rbjFjTnIxR1RJZ25MR2YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724591088961507622#m</id>
            <title>Endless Zoom ：可以不断无限放大和扩充图像

Endless Zoom利用潜在一致性模型（LCMs）来实现图像的无限扩充。

通过LCMs的快速图像生成能力，可以生成连续的、无缝连接的图像内容，从而实现“无限扩充”。你可以不断地放大或缩小图像，而图像内容会持续生成，不会重复或结束。

在线体验：https://endless-zoom.vercel.app/

作者：@chigozienri

GitHub：https://github.com/replicate/endless-zoom/

LCMs是一个基于Stable Diffusion的图像生成模型，但生成图像的速度更快，只需要4到8步就能生成一张高质量的图像。

通过在 M1 或 M2 Mac 上运行 LCMs，你可以以每秒一张的速度生成 512x512 图像。

LCMs介绍：
https://x.com/xiaohuggg/status/1717562806822981835</title>
            <link>https://nitter.cz/xiaohuggg/status/1724591088961507622#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724591088961507622#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 00:52:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Endless Zoom ：可以不断无限放大和扩充图像<br />
<br />
Endless Zoom利用潜在一致性模型（LCMs）来实现图像的无限扩充。<br />
<br />
通过LCMs的快速图像生成能力，可以生成连续的、无缝连接的图像内容，从而实现“无限扩充”。你可以不断地放大或缩小图像，而图像内容会持续生成，不会重复或结束。<br />
<br />
在线体验：<a href="https://endless-zoom.vercel.app/">endless-zoom.vercel.app/</a><br />
<br />
作者：<a href="https://nitter.cz/chigozienri" title="Chigozie Nri">@chigozienri</a><br />
<br />
GitHub：<a href="https://github.com/replicate/endless-zoom/">github.com/replicate/endless…</a><br />
<br />
LCMs是一个基于Stable Diffusion的图像生成模型，但生成图像的速度更快，只需要4到8步就能生成一张高质量的图像。<br />
<br />
通过在 M1 或 M2 Mac 上运行 LCMs，你可以以每秒一张的速度生成 512x512 图像。<br />
<br />
LCMs介绍：<br />
<a href="https://x.com/xiaohuggg/status/1717562806822981835">x.com/xiaohuggg/status/17175…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1ODMyMzczNzEyMzYzNTIvcHUvaW1nL2VmT0Nxam9kRWN4X3RGbDMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724575584851005828#m</id>
            <title>R to @xiaohuggg: 还有钢琴的🎹</title>
            <link>https://nitter.cz/xiaohuggg/status/1724575584851005828#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724575584851005828#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 23:50:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还有钢琴的🎹</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MTk1MzE1Mzc1ODUxMDI4NDkvcHUvaW1nLy1ZbFVabEZVcXNFdFA5ak8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724574547360509958#m</id>
            <title>学吉他的完美工具

cool…</title>
            <link>https://nitter.cz/xiaohuggg/status/1724574547360509958#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724574547360509958#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 23:46:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学吉他的完美工具<br />
<br />
cool…</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0NjIxMjk3MDc3MzI5OTUvcHUvaW1nL1dtM1JTNHdQNHIzSWFRMjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724441907273597151#m</id>
            <title>酷站推荐：

一个收录不可描述声音的网站（见视频）

收录的非常多，都是用户自己上传的，100%真实

你也可以上传分享自己的

😂

没敢点开声音，自己进去听：https://orgasmsoundlibrary.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724441907273597151#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724441907273597151#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 14:59:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>酷站推荐：<br />
<br />
一个收录不可描述声音的网站（见视频）<br />
<br />
收录的非常多，都是用户自己上传的，100%真实<br />
<br />
你也可以上传分享自己的<br />
<br />
😂<br />
<br />
没敢点开声音，自己进去听：<a href="https://orgasmsoundlibrary.com/">orgasmsoundlibrary.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0MzQzNDM2NDMyMTc5MjAvcHUvaW1nL1kzRmdFMm9QbG13VjFJWTEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724430161574027280#m</id>
            <title>Music ControlNet ：一种类似于SD ControlNetD 能精准控制音乐生成的模型

它特别的地方在于，可以让使用者非常精确地控制音乐的各种元素，比如旋律、音量的强弱，以及节奏的快慢。

甚至可以细致地调整音乐的每一个小细节。

Music ControlNet不仅能够控制音乐的全局属性（如风格、情绪和节奏），还能精确控制音乐的时间变化属性，例如节拍的位置和音乐的动态变化。

它能够根据用户的指令，生成符合要求的音乐。比如说，如果你想要一段旋律在特定的时间点出现，或者想要音乐在某个部分变得更加激烈，Music ControlNet 都能做到。

工作原理：

Music ControlNet采用了一种类似于图像领域ControlNet方法的像素级控制方式。它通过从训练音频中提取控制信息，然后对基于扩散的条件生成模型进行微调，从而实现对音频频谱图的控制。这种方法包括旋律、动态和节奏控制。

该模型还提供了一种新策略，允许创作者输入部分指定时间的控制信息。在评估时，研究人员不仅考虑了从音频中提取的控制信息，还考虑了创作者可能提供的控制信息，证明了该模型能够根据输入的控制信息生成真实的音乐。

Music ControlNet 的工作原理基于几个关键技术：

1、控制信息提取：首先，从训练用的音频中提取出控制信息。这些信息包括音乐的旋律、动态（音量的强弱变化）和节奏等元素。

2、条件生成模型：使用一种叫做“条件生成模型”的技术。这种模型可以根据给定的条件（在这里是控制信息）来生成音频。Music ControlNet 对这种模型进行了特别的调整，使其能够更好地适应音乐生成的任务。

3、频谱图控制：Music ControlNet 专注于控制音频的频谱图。频谱图是一种显示音频信号频率分布的图表，通过控制这个频谱图，模型可以精确地生成符合特定旋律、动态和节奏的音乐。

4、部分指定时间控制：这个特性允许用户只对音乐的某些部分指定控制信息。比如说，你可以只指定音乐开始的旋律，然后让模型自己决定剩下的部分。这给了用户更多的灵活性，同时也保留了一定的创造性。

5、微调和生成：通过微调预训练的模型，Music ControlNet 能够根据用户提供的控制信息生成音乐。这个过程涉及到对模型的参数进行细微调整，以适应特定的音乐风格或要求。

实验结果：

与其他少数可比较的音乐生成模型相比，例如MusicGen（一个接受文本和旋律输入的模型），Music ControlNet在保持输入旋律忠实度方面表现更好，尽管其参数数量少35倍，训练数据少11倍，并且能够实现两种额外的时间变化控制。

项目及演示：https://musiccontrolnet.github.io/web/
论文：https://arxiv.org/abs/2311.07069</title>
            <link>https://nitter.cz/xiaohuggg/status/1724430161574027280#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724430161574027280#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 14:12:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Music ControlNet ：一种类似于SD ControlNetD 能精准控制音乐生成的模型<br />
<br />
它特别的地方在于，可以让使用者非常精确地控制音乐的各种元素，比如旋律、音量的强弱，以及节奏的快慢。<br />
<br />
甚至可以细致地调整音乐的每一个小细节。<br />
<br />
Music ControlNet不仅能够控制音乐的全局属性（如风格、情绪和节奏），还能精确控制音乐的时间变化属性，例如节拍的位置和音乐的动态变化。<br />
<br />
它能够根据用户的指令，生成符合要求的音乐。比如说，如果你想要一段旋律在特定的时间点出现，或者想要音乐在某个部分变得更加激烈，Music ControlNet 都能做到。<br />
<br />
工作原理：<br />
<br />
Music ControlNet采用了一种类似于图像领域ControlNet方法的像素级控制方式。它通过从训练音频中提取控制信息，然后对基于扩散的条件生成模型进行微调，从而实现对音频频谱图的控制。这种方法包括旋律、动态和节奏控制。<br />
<br />
该模型还提供了一种新策略，允许创作者输入部分指定时间的控制信息。在评估时，研究人员不仅考虑了从音频中提取的控制信息，还考虑了创作者可能提供的控制信息，证明了该模型能够根据输入的控制信息生成真实的音乐。<br />
<br />
Music ControlNet 的工作原理基于几个关键技术：<br />
<br />
1、控制信息提取：首先，从训练用的音频中提取出控制信息。这些信息包括音乐的旋律、动态（音量的强弱变化）和节奏等元素。<br />
<br />
2、条件生成模型：使用一种叫做“条件生成模型”的技术。这种模型可以根据给定的条件（在这里是控制信息）来生成音频。Music ControlNet 对这种模型进行了特别的调整，使其能够更好地适应音乐生成的任务。<br />
<br />
3、频谱图控制：Music ControlNet 专注于控制音频的频谱图。频谱图是一种显示音频信号频率分布的图表，通过控制这个频谱图，模型可以精确地生成符合特定旋律、动态和节奏的音乐。<br />
<br />
4、部分指定时间控制：这个特性允许用户只对音乐的某些部分指定控制信息。比如说，你可以只指定音乐开始的旋律，然后让模型自己决定剩下的部分。这给了用户更多的灵活性，同时也保留了一定的创造性。<br />
<br />
5、微调和生成：通过微调预训练的模型，Music ControlNet 能够根据用户提供的控制信息生成音乐。这个过程涉及到对模型的参数进行细微调整，以适应特定的音乐风格或要求。<br />
<br />
实验结果：<br />
<br />
与其他少数可比较的音乐生成模型相比，例如MusicGen（一个接受文本和旋律输入的模型），Music ControlNet在保持输入旋律忠实度方面表现更好，尽管其参数数量少35倍，训练数据少11倍，并且能够实现两种额外的时间变化控制。<br />
<br />
项目及演示：<a href="https://musiccontrolnet.github.io/web/">musiccontrolnet.github.io/we…</a><br />
论文：<a href="https://arxiv.org/abs/2311.07069">arxiv.org/abs/2311.07069</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0Mjk3MDM4NTM4NzkyOTYvcHUvaW1nL3BNVEFsVkhMRjJYVkhMaGouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724356117059477614#m</id>
            <title>人工智能爱上男主

争风吃醋

杀死男主女友和一屋子人类😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1724356117059477614#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724356117059477614#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 09:18:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>人工智能爱上男主<br />
<br />
争风吃醋<br />
<br />
杀死男主女友和一屋子人类😐</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI0MzU1Mzc2NDI5MjkzNTY4L2ltZy9YcWpJdkhfcWdfbHBHMkhGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724317866013704384#m</id>
            <title>学习如何在GPTs中调用外部API

👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1724317866013704384#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724317866013704384#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 06:46:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学习如何在GPTs中调用外部API<br />
<br />
👍</p>
<p><a href="https://nitter.cz/dotey/status/1724305358254952799#m">nitter.cz/dotey/status/1724305358254952799#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724285898874106350#m</id>
            <title>没有一个人投诉

也没有一个部门出来管管

居然要一个外国公司来出面整治，国将不国啊

双 11 期间，跳转广告泛滥，一碰手机就跳转到购物APP！苹果公司下手整治：已通知国内多家头部 App 要求它们移除陀螺仪权限，摇一摇跳转广告被禁止。

这些收到苹果通知的 App 包括但不限于在线视频软件、短视频软件、音频软件、邮箱软件等。

摇一摇功能调用的陀螺仪权限，是一种很早就有的功能，可以用来抢电视红包、识别歌曲等，现在被用来做广告跳转，而有些手机没有陀螺仪权限开关，用户无法自行关闭。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724285898874106350#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724285898874106350#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 04:39:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>没有一个人投诉<br />
<br />
也没有一个部门出来管管<br />
<br />
居然要一个外国公司来出面整治，国将不国啊<br />
<br />
双 11 期间，跳转广告泛滥，一碰手机就跳转到购物APP！苹果公司下手整治：已通知国内多家头部 App 要求它们移除陀螺仪权限，摇一摇跳转广告被禁止。<br />
<br />
这些收到苹果通知的 App 包括但不限于在线视频软件、短视频软件、音频软件、邮箱软件等。<br />
<br />
摇一摇功能调用的陀螺仪权限，是一种很早就有的功能，可以用来抢电视红包、识别歌曲等，现在被用来做广告跳转，而有些手机没有陀螺仪权限开关，用户无法自行关闭。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0za2lwSWJVQUFPX1VyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>