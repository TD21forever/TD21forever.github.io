<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754163531476500861#m</id>
            <title>🔔http://Xiaohu.AI日报「2月4日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1754163531476500861#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754163531476500861#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 15:22:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月4日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZnS2JXYWJZQUFuRmhXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754129432716120533#m</id>
            <title>之前曝光的HeyGen AI视频实时聊天功能上线了！

现在你可以和HeyGen CEO的AI分身进行实时视频聊天，你打字他会用视频回答你！

视频回答完全由AI实时生成！

体验地址：https://labs.heygen.com/streaming-avatar</title>
            <link>https://nitter.cz/xiaohuggg/status/1754129432716120533#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754129432716120533#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 13:07:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前曝光的HeyGen AI视频实时聊天功能上线了！<br />
<br />
现在你可以和HeyGen CEO的AI分身进行实时视频聊天，你打字他会用视频回答你！<br />
<br />
视频回答完全由AI实时生成！<br />
<br />
体验地址：<a href="https://labs.heygen.com/streaming-avatar">labs.heygen.com/streaming-av…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1749367612197499282#m">nitter.cz/xiaohuggg/status/1749367612197499282#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU0MTI5MzY5MDc2MDM5NjgwL2ltZy9VQ2VURk9BQ19nekZYUS1yLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754118113807970659#m</id>
            <title>Stable Video Diffusion (SVD) 模型1.1 发布

这个测试视频说明了一切，牛P

其前一代相比，主要变化包括：

1、微调优化：通过特定条件下的微调，提高了视频输出的一致性和质量。

2、改进的生成性能：在生成视频的清晰度、分辨率以及帧数上可能有所改进，提供更加流畅和高质量的视觉体验。

3、固定条件下的性能提升：通过在固定条件下进行微调，SVD 1.1在特定设置下展现出比先前版本更优的性能，这包括更好的运动一致性和视觉效果，同时保持了条件的可调整性，以适应不同的应用需求。

4、适应性和局限性：SVD 1.1继续探索模型的适应性和局限性，鼓励负责任使用。

模型下载：https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1</title>
            <link>https://nitter.cz/xiaohuggg/status/1754118113807970659#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754118113807970659#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 12:22:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable Video Diffusion (SVD) 模型1.1 发布<br />
<br />
这个测试视频说明了一切，牛P<br />
<br />
其前一代相比，主要变化包括：<br />
<br />
1、微调优化：通过特定条件下的微调，提高了视频输出的一致性和质量。<br />
<br />
2、改进的生成性能：在生成视频的清晰度、分辨率以及帧数上可能有所改进，提供更加流畅和高质量的视觉体验。<br />
<br />
3、固定条件下的性能提升：通过在固定条件下进行微调，SVD 1.1在特定设置下展现出比先前版本更优的性能，这包括更好的运动一致性和视觉效果，同时保持了条件的可调整性，以适应不同的应用需求。<br />
<br />
4、适应性和局限性：SVD 1.1继续探索模型的适应性和局限性，鼓励负责任使用。<br />
<br />
模型下载：<a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1">huggingface.co/stabilityai/s…</a></p>
<p><a href="https://nitter.cz/lepadphone/status/1754053349559919008#m">nitter.cz/lepadphone/status/1754053349559919008#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754112096193949958#m</id>
            <title>Polycam登陆Apple Vision Pro

用户可以通过Apple Vision Pro浏览Polycam数百万个原生3D资源库，而且可以像在现实中一样拿起它们，放到你想要的任何地方。

如果你看到一个有趣的物体或景象，只需用iPhone扫描，Polycam就能把它变成一个3D模型。

然后，你可以戴上你的Apple Vision Pro眼镜，在虚拟空间中与这个模型互动，就像它真的在你面前一样。

Polycam还允许你把自己喜欢的3D模型放在一起，创造出一个全新的虚拟环境。无论是装饰虚拟家园，还是制作一个3D艺术作品，都只受限于你的想象力。

详细：https://poly.cam/vision-pro</title>
            <link>https://nitter.cz/xiaohuggg/status/1754112096193949958#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754112096193949958#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 11:58:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Polycam登陆Apple Vision Pro<br />
<br />
用户可以通过Apple Vision Pro浏览Polycam数百万个原生3D资源库，而且可以像在现实中一样拿起它们，放到你想要的任何地方。<br />
<br />
如果你看到一个有趣的物体或景象，只需用iPhone扫描，Polycam就能把它变成一个3D模型。<br />
<br />
然后，你可以戴上你的Apple Vision Pro眼镜，在虚拟空间中与这个模型互动，就像它真的在你面前一样。<br />
<br />
Polycam还允许你把自己喜欢的3D模型放在一起，创造出一个全新的虚拟环境。无论是装饰虚拟家园，还是制作一个3D艺术作品，都只受限于你的想象力。<br />
<br />
详细：<a href="https://poly.cam/vision-pro">poly.cam/vision-pro</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQxMTA1NTA0NTQ1NDY0MzMvcHUvaW1nL2s5NXNYYjF2TGM0bXBYTU4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754100742603100579#m</id>
            <title>Gemini Ultra即将上线，Bard将更名为 Gemini

一份泄露的文档显示，Google的Gemini Ultra模型将在2月7号上线，同时Google聊天机器人Bard将更名为Gemini。

Gemini将开启付费计划：Gemini Advanced

是一个类似ChatGPT Plus的付费模式，可以访问Gemini Ultra 1.0，Gemini Pro可能将继续免费。

核心要点：

- Gemini Ultra上线，Bard更名为Gemini

- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。

- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。

- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。

- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。</title>
            <link>https://nitter.cz/xiaohuggg/status/1754100742603100579#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754100742603100579#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 11:13:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Ultra即将上线，Bard将更名为 Gemini<br />
<br />
一份泄露的文档显示，Google的Gemini Ultra模型将在2月7号上线，同时Google聊天机器人Bard将更名为Gemini。<br />
<br />
Gemini将开启付费计划：Gemini Advanced<br />
<br />
是一个类似ChatGPT Plus的付费模式，可以访问Gemini Ultra 1.0，Gemini Pro可能将继续免费。<br />
<br />
核心要点：<br />
<br />
- Gemini Ultra上线，Bard更名为Gemini<br />
<br />
- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。<br />
<br />
- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。<br />
<br />
- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。<br />
<br />
- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZmUTNnaWJrQUF1OFdNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754058907264376833#m</id>
            <title>兄弟们，这个厉害了💥

ML Blocks：无代码AI图像生成和分析工作流平台

它提供了一个拖放式的界面，允许用户轻松地创建复杂的图像处理工作流，无需编写任何代码。

你只根据需要将不同的功能块（如图像编辑功能和AI模型）组合在一起，即可实现个性化的图像自动化处理。

该工具主要解决在电商领域遇到的批量处理图片问题。

ML Blocks允许用户创建可以处理多步骤图像生成或分析管道的自定义图像处理工作流，使用基于图的工作流。用户只需按顺序连接几个块，如去背景 -> 裁剪 -> AI上采样，就可以在几分钟内得到完整的图像处理工作流。

主要功能：

- 生成图像：使用 Stable Diffusion 等 AI 模型生成或绘制图像。

- 编辑图像：提供编辑功能，如裁剪、调整大小、重新着色等，来修改图像。

- 分析图像：利用检测或分割模型从图像中提取数据。

实际应用示例：

基于提示模糊图像特定区域：传统方法需要使用DINO模型生成提示中提到的对象周围的边界框，然后使用像Segment Anything这样的分割模型生成这些区域的遮罩，最后使用Pillow或OpenCV库编写模糊功能来模糊遮罩区域。

而使用ML Blocks，用户只需将分割、遮罩和模糊块连接起来，就能在2分钟内完成工作流程。

你还可以自动生成博客帖子或推文的横幅图像、根据提示移除图像中的对象、去除背景并用AI创建新背景等多种工作流程。

传送门：https://www.mlblocks.com/
工作原理：https://blog.mlblocks.com/p/what-on-earth-is-ml-blocks</title>
            <link>https://nitter.cz/xiaohuggg/status/1754058907264376833#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754058907264376833#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 08:26:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，这个厉害了💥<br />
<br />
ML Blocks：无代码AI图像生成和分析工作流平台<br />
<br />
它提供了一个拖放式的界面，允许用户轻松地创建复杂的图像处理工作流，无需编写任何代码。<br />
<br />
你只根据需要将不同的功能块（如图像编辑功能和AI模型）组合在一起，即可实现个性化的图像自动化处理。<br />
<br />
该工具主要解决在电商领域遇到的批量处理图片问题。<br />
<br />
ML Blocks允许用户创建可以处理多步骤图像生成或分析管道的自定义图像处理工作流，使用基于图的工作流。用户只需按顺序连接几个块，如去背景 -> 裁剪 -> AI上采样，就可以在几分钟内得到完整的图像处理工作流。<br />
<br />
主要功能：<br />
<br />
- 生成图像：使用 Stable Diffusion 等 AI 模型生成或绘制图像。<br />
<br />
- 编辑图像：提供编辑功能，如裁剪、调整大小、重新着色等，来修改图像。<br />
<br />
- 分析图像：利用检测或分割模型从图像中提取数据。<br />
<br />
实际应用示例：<br />
<br />
基于提示模糊图像特定区域：传统方法需要使用DINO模型生成提示中提到的对象周围的边界框，然后使用像Segment Anything这样的分割模型生成这些区域的遮罩，最后使用Pillow或OpenCV库编写模糊功能来模糊遮罩区域。<br />
<br />
而使用ML Blocks，用户只需将分割、遮罩和模糊块连接起来，就能在2分钟内完成工作流程。<br />
<br />
你还可以自动生成博客帖子或推文的横幅图像、根据提示移除图像中的对象、去除背景并用AI创建新背景等多种工作流程。<br />
<br />
传送门：<a href="https://www.mlblocks.com/">mlblocks.com/</a><br />
工作原理：<a href="https://blog.mlblocks.com/p/what-on-earth-is-ml-blocks">blog.mlblocks.com/p/what-on-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQwMjM2MDczNDc2NDY0NjQvcHUvaW1nL1pySTNtYUF5YTdwS0pzdkMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754033623769272615#m</id>
            <title>Welcome To The Future...

Human...</title>
            <link>https://nitter.cz/xiaohuggg/status/1754033623769272615#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754033623769272615#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 06:46:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Welcome To The Future...<br />
<br />
Human...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQwMzI5MjI4ODg0MDQ5OTIvcHUvaW1nL2tmYnhGX1NxZ0lfb2pjYUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754027450181517353#m</id>
            <title>iFixit 发布 Apple Vision Pro 拆解视频

虽然完全看不懂

但是看起来是个很精密复杂的玩意...

完整视频：https://youtu.be/JVJPAYwY8Us</title>
            <link>https://nitter.cz/xiaohuggg/status/1754027450181517353#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754027450181517353#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 06:21:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>iFixit 发布 Apple Vision Pro 拆解视频<br />
<br />
虽然完全看不懂<br />
<br />
但是看起来是个很精密复杂的玩意...<br />
<br />
完整视频：<a href="https://youtu.be/JVJPAYwY8Us">youtu.be/JVJPAYwY8Us</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQwMjY4Njg5NTU5MjI0MzIvcHUvaW1nL3FINkxqWkVNMjFVLTd3NzEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754021393967292429#m</id>
            <title>Google研究团队开发了一种名为TimesFM的时间序列预测模型

时间序列预测就是基于过去的数据来预测未来事件的发生。

这在商业、金融和科研等多个领域都非常重要。帮助人们做出更明智的决策。

比如，商家可以用它来预测未来产品需求，金融分析师可以用来预测股市的变动。

甚至预测天气...

举例解释：

假设你经营着一家小型零售店，你希望能够预测接下来一个月每天的顾客流量，这样就可以更好地管理库存，安排员工的工作班次，甚至优化促销活动来吸引更多顾客。这就是一个时间序列预测的例子，因为你试图基于过去的数据来预测未来的事件。

在没有TimesFM这样的工具之前，你可能需要依靠简单的经验规则（比如假期期间顾客更多）或者手动分析过去的销售数据，这既费时又不一定准确。而且，对于突发事件（比如一次大型的促销活动）的影响，传统方法很难准确预测。

现在，让我们看看引入TimesFM后情况会如何改变：

使用TimesFM进行预测的步骤：

1、收集数据：首先，你需要收集过去一段时间内每天的顾客流量数据，包括任何可能影响这些数字的因素，比如促销活动、节假日等。

2、输入TimesFM：接下来，你将这些数据输入到TimesFM模型中。TimesFM会分析这些数据，学习过去顾客流量的模式和任何可识别的规律。

3、预测未来：基于它从数据中学到的知识，TimesFM可以预测接下来一个月每天的顾客流量。这个预测考虑了多种因素，包括季节性变化、特殊事件的影响等。

结果：

库存管理：凭借这些预测，你可以更精准地订购库存，避免过多或不足的情况。

员工排班：你也可以根据预测结果安排更多员工在预期顾客流量较大的日子工作，确保顾客满意。

促销策略：此外，如果预测显示某几天顾客流量可能会下降，你可以提前规划促销活动来吸引顾客。

通过TimesFM，预测变得更加简单和准确，帮助你作出更好的商业决策，提高效率和盈利能力。

TimesFM的主要功能特点包括：

1、高效的时间序列预测：TimesFM专为时间序列预测设计，能够处理各种类型的时间序列数据，包括零售、金融、天气等领域的数据，提供准确的未来预测。

2、零样本学习能力：即使在没有特定于目标数据集的额外训练的情况下，TimesFM也能展现出色的预测性能。这意味着它可以直接应用于新的时间序列数据集，立即提供有用的预测结果。

3、基于大数据的预训练模型：TimesFM通过在包含1000亿个时间点的大型时间序列数据集上进行预训练，学习了时间序列数据的广泛模式和规律。这种大规模预训练是其优异性能的关键。

4、相对较小的模型规模：尽管TimesFM具有强大的预测能力，它的模型规模（2亿参数）相对较小，这使得它在资源消耗方面更为高效，便于在各种环境中部署和使用。

5、适应不同领域和时间粒度：TimesFM能够处理不同领域（如交通、天气、需求预测等）和不同时间粒度（从几分钟到年度数据）的时间序列数据，展示了其广泛的适用性和灵活性。

6、支持长期预测：TimesFM通过创新的模型架构，能够有效地进行长期预测，即预测远超过输入数据长度的未来时间点，这在许多应用场景中是非常有价值的。

7、易于集成和使用：Google计划在Google Cloud Vertex AI中提供TimesFM，这将使得外部客户能够轻松地集成和使用这个强大的时间序列预测工具，无需深入了解模型的内部工作机制。

详细：https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1754021393967292429#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754021393967292429#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 05:57:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google研究团队开发了一种名为TimesFM的时间序列预测模型<br />
<br />
时间序列预测就是基于过去的数据来预测未来事件的发生。<br />
<br />
这在商业、金融和科研等多个领域都非常重要。帮助人们做出更明智的决策。<br />
<br />
比如，商家可以用它来预测未来产品需求，金融分析师可以用来预测股市的变动。<br />
<br />
甚至预测天气...<br />
<br />
举例解释：<br />
<br />
假设你经营着一家小型零售店，你希望能够预测接下来一个月每天的顾客流量，这样就可以更好地管理库存，安排员工的工作班次，甚至优化促销活动来吸引更多顾客。这就是一个时间序列预测的例子，因为你试图基于过去的数据来预测未来的事件。<br />
<br />
在没有TimesFM这样的工具之前，你可能需要依靠简单的经验规则（比如假期期间顾客更多）或者手动分析过去的销售数据，这既费时又不一定准确。而且，对于突发事件（比如一次大型的促销活动）的影响，传统方法很难准确预测。<br />
<br />
现在，让我们看看引入TimesFM后情况会如何改变：<br />
<br />
使用TimesFM进行预测的步骤：<br />
<br />
1、收集数据：首先，你需要收集过去一段时间内每天的顾客流量数据，包括任何可能影响这些数字的因素，比如促销活动、节假日等。<br />
<br />
2、输入TimesFM：接下来，你将这些数据输入到TimesFM模型中。TimesFM会分析这些数据，学习过去顾客流量的模式和任何可识别的规律。<br />
<br />
3、预测未来：基于它从数据中学到的知识，TimesFM可以预测接下来一个月每天的顾客流量。这个预测考虑了多种因素，包括季节性变化、特殊事件的影响等。<br />
<br />
结果：<br />
<br />
库存管理：凭借这些预测，你可以更精准地订购库存，避免过多或不足的情况。<br />
<br />
员工排班：你也可以根据预测结果安排更多员工在预期顾客流量较大的日子工作，确保顾客满意。<br />
<br />
促销策略：此外，如果预测显示某几天顾客流量可能会下降，你可以提前规划促销活动来吸引顾客。<br />
<br />
通过TimesFM，预测变得更加简单和准确，帮助你作出更好的商业决策，提高效率和盈利能力。<br />
<br />
TimesFM的主要功能特点包括：<br />
<br />
1、高效的时间序列预测：TimesFM专为时间序列预测设计，能够处理各种类型的时间序列数据，包括零售、金融、天气等领域的数据，提供准确的未来预测。<br />
<br />
2、零样本学习能力：即使在没有特定于目标数据集的额外训练的情况下，TimesFM也能展现出色的预测性能。这意味着它可以直接应用于新的时间序列数据集，立即提供有用的预测结果。<br />
<br />
3、基于大数据的预训练模型：TimesFM通过在包含1000亿个时间点的大型时间序列数据集上进行预训练，学习了时间序列数据的广泛模式和规律。这种大规模预训练是其优异性能的关键。<br />
<br />
4、相对较小的模型规模：尽管TimesFM具有强大的预测能力，它的模型规模（2亿参数）相对较小，这使得它在资源消耗方面更为高效，便于在各种环境中部署和使用。<br />
<br />
5、适应不同领域和时间粒度：TimesFM能够处理不同领域（如交通、天气、需求预测等）和不同时间粒度（从几分钟到年度数据）的时间序列数据，展示了其广泛的适用性和灵活性。<br />
<br />
6、支持长期预测：TimesFM通过创新的模型架构，能够有效地进行长期预测，即预测远超过输入数据长度的未来时间点，这在许多应用场景中是非常有价值的。<br />
<br />
7、易于集成和使用：Google计划在Google Cloud Vertex AI中提供TimesFM，这将使得外部客户能够轻松地集成和使用这个强大的时间序列预测工具，无需深入了解模型的内部工作机制。<br />
<br />
详细：<a href="https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html">blog.research.google/2024/02…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZlSkoxTGF3QUFVNGlYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1754014054165983539#m</id>
            <title>Image</title>
            <link>https://nitter.cz/xiaohuggg/status/1754014054165983539#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1754014054165983539#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 05:28:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ZlQ2V3SmJzQUFDbzdtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753984077592744003#m</id>
            <title>R to @xiaohuggg: 11、AI换脸😂

嫌弃自己家里的那一位不满意

随便换🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1753984077592744003#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753984077592744003#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 03:29:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>11、AI换脸😂<br />
<br />
嫌弃自己家里的那一位不满意<br />
<br />
随便换🤓</p>
<p><a href="https://nitter.cz/iamjesserichard/status/1753826288568238339#m">nitter.cz/iamjesserichard/status/1753826288568238339#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753980967239500162#m</id>
            <title>🔔http://Xiaohu.AI日报「2月3日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1753980967239500162#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753980967239500162#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 04 Feb 2024 03:17:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月3日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Zka1l1QWFrQUFtX0ZMLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753762727657603513#m</id>
            <title>R to @xiaohuggg: Video-to-Video

视频到视频的转换演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1753762727657603513#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753762727657603513#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 12:49:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Video-to-Video<br />
<br />
视频到视频的转换演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTM3NTgwMzg5OTE5NjYyMDgvcHUvaW1nLzlHNHZfSmhpM2hoUU5xYVouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753762725325591017#m</id>
            <title>R to @xiaohuggg: Motion Brush + Drag

运动笔刷 + 拖动</title>
            <link>https://nitter.cz/xiaohuggg/status/1753762725325591017#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753762725325591017#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 12:49:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Motion Brush + Drag<br />
<br />
运动笔刷 + 拖动</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTM3NTc2NzA0MTMzNTcwNTYvcHUvaW1nL2I3Z0xuUWVrVElpZ0EtdTYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753762722972590499#m</id>
            <title>R to @xiaohuggg: Motion Brush 运动笔刷</title>
            <link>https://nitter.cz/xiaohuggg/status/1753762722972590499#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753762722972590499#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 12:49:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Motion Brush 运动笔刷</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTM3NTczNDAxNDU0Nzk2ODAvcHUvaW1nLzdNdkdKd3M4R2x2LXZaNTMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753762720397357082#m</id>
            <title>R to @xiaohuggg: Motion Drag 运动拖动</title>
            <link>https://nitter.cz/xiaohuggg/status/1753762720397357082#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753762720397357082#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 12:49:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Motion Drag 运动拖动</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTM3NTcwNDc0NzUyNzM3MjgvcHUvaW1nLzlMa3RidkpDTXBYaGh0VDIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753762717805277348#m</id>
            <title>Motion-I2V：可以像Runway的运动笔刷一样控制视频生成的模型

Motion-I2V能够从一张静态图片生成连贯且可控制的视频，效果惊人！🫡

同时它还能对生成视频进行运动控制，例如可以生成辆车在道路上行驶的视频，而且还能控制车辆的行驶方向和速度等细节等。

Motion-I2V的主要功能特点：

1、图像到视频的自动转换：能够将静态图像自动转换成连贯的动态视频，通过模拟图像中对象的自然运动，使静态图像“活”起来。

2、显式运动建模：通过两个阶段的处理，首先预测图像中物体的运动轨迹，然后根据这些轨迹生成动态视频，这种方法明确考虑物体的移动方式，使生成的视频更加自然和真实。

3、运动控制：用户可以精确控制视频中的运动轨迹和运动区域，例如指定物体移动的方向、速度等。这种控制是通过对运动场预测器的训练实现的，为用户提供了比传统方法更高的可控性。

4、一致性和可控性：生成的视频不仅在视觉上连贯，而且用户可以通过简单的操作控制视频内容，使得生成的视频既满足特定的视觉要求，又能反映用户的创意意图。

5、支持零样本视频到视频转换：除了从图像生成视频外，Motion-I2V还支持将一个视频转换为另一个风格的视频，而无需额外的训练样本，这一功能基于其先进的运动增强和时间注意力技术。

6、高质量视频生成：与现有的图像到视频转换技术相比，Motion-I2V能够在存在大的运动和视角变化时，生成更一致、质量更高的视频内容。

工作原理：

与直接学习复杂的图像到视频映射的传统方法不同，它将整个图像到视频的转换过程分为两个步骤，并且在这两个步骤中明确地考虑了物体的移动情况。

第一步骤：这个阶段主要关注如何预测图片中物体的移动轨迹。简单来说，就是确定图片中的每个点在未来的视频帧中应该移动到哪里去。这通过一个特别设计的预测器来实现，它能够基于当前的图片和一些指令（比如文本描述）来预测出物体的移动方式。

第二步骤：有了这些移动轨迹之后，下一步就是根据这些轨迹将图片中的内容“拖动”到视频的每一帧中去，从而创建出连贯的视频。这个阶段使用了一种特殊的技术，可以更好地处理时间上的信息，使得视频中的运动看起来更自然。

与其他类似技术相比，Motion-I2V的优势在于它生成的视频不仅看起来更连贯（即使在运动和视角变化很大的情况下），而且用户还可以更精确地控制视频中的运动情况，比如运动的方向和速度。

此外，这个技术还能够做到从一个视频转换到另一个视频，而不需要额外的训练数据，这是通过在第二步骤中引入的一种新技术实现的。

项目及演示：https://xiaoyushi97.github.io/Motion-I2V
论文：https://arxiv.org/abs/2401.15977
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1753762717805277348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753762717805277348#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 12:49:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Motion-I2V：可以像Runway的运动笔刷一样控制视频生成的模型<br />
<br />
Motion-I2V能够从一张静态图片生成连贯且可控制的视频，效果惊人！🫡<br />
<br />
同时它还能对生成视频进行运动控制，例如可以生成辆车在道路上行驶的视频，而且还能控制车辆的行驶方向和速度等细节等。<br />
<br />
Motion-I2V的主要功能特点：<br />
<br />
1、图像到视频的自动转换：能够将静态图像自动转换成连贯的动态视频，通过模拟图像中对象的自然运动，使静态图像“活”起来。<br />
<br />
2、显式运动建模：通过两个阶段的处理，首先预测图像中物体的运动轨迹，然后根据这些轨迹生成动态视频，这种方法明确考虑物体的移动方式，使生成的视频更加自然和真实。<br />
<br />
3、运动控制：用户可以精确控制视频中的运动轨迹和运动区域，例如指定物体移动的方向、速度等。这种控制是通过对运动场预测器的训练实现的，为用户提供了比传统方法更高的可控性。<br />
<br />
4、一致性和可控性：生成的视频不仅在视觉上连贯，而且用户可以通过简单的操作控制视频内容，使得生成的视频既满足特定的视觉要求，又能反映用户的创意意图。<br />
<br />
5、支持零样本视频到视频转换：除了从图像生成视频外，Motion-I2V还支持将一个视频转换为另一个风格的视频，而无需额外的训练样本，这一功能基于其先进的运动增强和时间注意力技术。<br />
<br />
6、高质量视频生成：与现有的图像到视频转换技术相比，Motion-I2V能够在存在大的运动和视角变化时，生成更一致、质量更高的视频内容。<br />
<br />
工作原理：<br />
<br />
与直接学习复杂的图像到视频映射的传统方法不同，它将整个图像到视频的转换过程分为两个步骤，并且在这两个步骤中明确地考虑了物体的移动情况。<br />
<br />
第一步骤：这个阶段主要关注如何预测图片中物体的移动轨迹。简单来说，就是确定图片中的每个点在未来的视频帧中应该移动到哪里去。这通过一个特别设计的预测器来实现，它能够基于当前的图片和一些指令（比如文本描述）来预测出物体的移动方式。<br />
<br />
第二步骤：有了这些移动轨迹之后，下一步就是根据这些轨迹将图片中的内容“拖动”到视频的每一帧中去，从而创建出连贯的视频。这个阶段使用了一种特殊的技术，可以更好地处理时间上的信息，使得视频中的运动看起来更自然。<br />
<br />
与其他类似技术相比，Motion-I2V的优势在于它生成的视频不仅看起来更连贯（即使在运动和视角变化很大的情况下），而且用户还可以更精确地控制视频中的运动情况，比如运动的方向和速度。<br />
<br />
此外，这个技术还能够做到从一个视频转换到另一个视频，而不需要额外的训练数据，这是通过在第二步骤中引入的一种新技术实现的。<br />
<br />
项目及演示：<a href="https://xiaoyushi97.github.io/Motion-I2V">xiaoyushi97.github.io/Motion…</a><br />
论文：<a href="https://arxiv.org/abs/2401.15977">arxiv.org/abs/2401.15977</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTM3NTk1MzcyNjAzNTU1ODQvcHUvaW1nL2plRlFuOVctdDlVMHpCSF8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753695040717902197#m</id>
            <title>R to @xiaohuggg: 10、CellWalk 使用Apple Vision Pro体验令人难以置信的学习体验，深入细胞内了解其分子结构。

https://x.com/timd_ca/status/1753250624677007492</title>
            <link>https://nitter.cz/xiaohuggg/status/1753695040717902197#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753695040717902197#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 08:20:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>10、CellWalk 使用Apple Vision Pro体验令人难以置信的学习体验，深入细胞内了解其分子结构。<br />
<br />
<a href="https://x.com/timd_ca/status/1753250624677007492">x.com/timd_ca/status/1753250…</a></p>
<p><a href="https://nitter.cz/timd_ca/status/1753250624677007492#m">nitter.cz/timd_ca/status/1753250624677007492#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753694311328391609#m</id>
            <title>R to @xiaohuggg: 9、Vibescape：一款适用于 Vision Pro 的沉浸式冥想应用程序

可以体验身临其境的冥想</title>
            <link>https://nitter.cz/xiaohuggg/status/1753694311328391609#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753694311328391609#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 08:18:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9、Vibescape：一款适用于 Vision Pro 的沉浸式冥想应用程序<br />
<br />
可以体验身临其境的冥想</p>
<p><a href="https://nitter.cz/dreamwieber/status/1752485999450689907#m">nitter.cz/dreamwieber/status/1752485999450689907#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1753693949771088171#m</id>
            <title>R to @xiaohuggg: 8、Zillow 足不出户 身临其境的餐馆待售房屋</title>
            <link>https://nitter.cz/xiaohuggg/status/1753693949771088171#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1753693949771088171#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 03 Feb 2024 08:16:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8、Zillow 足不出户 身临其境的餐馆待售房屋</p>
<p><a href="https://nitter.cz/richontech/status/1753053877610660023#m">nitter.cz/richontech/status/1753053877610660023#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>