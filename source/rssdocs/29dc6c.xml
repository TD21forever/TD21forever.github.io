<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750400886676070495#m</id>
            <title>26种多模态大模型研究报告

这篇论文对目前市面上26种多模态大语言模型（MM-LLMs）进行了全面的研究和分析。提供了对多模态大语言模型的深入了解。

详细介绍了模型架构和训练流程的设计公式。

26 种现存的 MM-LLMs，每种模型都有其独特的设计和功能。

主要内容：

1.模型架构和训练流程：

论文详细描述了这些模型的架构和训练流程，突出了它们如何结合了传统大型语言模型（LLMs）的能力，并支持多模态输入和输出。

2.模型概览：

• 研究涵盖了26种不同的 MM-LLMs，每个模型都有其独特的设计和功能特点。

• 这些模型被分为不同的类别，根据它们的架构和功能进行了分类。

3.性能评估：对这些模型在主流基准测试上的性能进行了回顾，分析了它们在不同任务上的表现。

4.训练策略：总结了提高 MM-LLMs 性能的关键训练策略，包括数据处理和模型优化等。

5.研究方向和资源：讨论了 MM-LLMs 的未来研究方向，并提供了实时跟踪这些模型最新发展的资源。

论文：https://arxiv.org/abs/2401.13601
PDF：https://arxiv.org/pdf/2401.13601.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1750400886676070495#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750400886676070495#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 06:11:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>26种多模态大模型研究报告<br />
<br />
这篇论文对目前市面上26种多模态大语言模型（MM-LLMs）进行了全面的研究和分析。提供了对多模态大语言模型的深入了解。<br />
<br />
详细介绍了模型架构和训练流程的设计公式。<br />
<br />
26 种现存的 MM-LLMs，每种模型都有其独特的设计和功能。<br />
<br />
主要内容：<br />
<br />
1.模型架构和训练流程：<br />
<br />
论文详细描述了这些模型的架构和训练流程，突出了它们如何结合了传统大型语言模型（LLMs）的能力，并支持多模态输入和输出。<br />
<br />
2.模型概览：<br />
<br />
• 研究涵盖了26种不同的 MM-LLMs，每个模型都有其独特的设计和功能特点。<br />
<br />
• 这些模型被分为不同的类别，根据它们的架构和功能进行了分类。<br />
<br />
3.性能评估：对这些模型在主流基准测试上的性能进行了回顾，分析了它们在不同任务上的表现。<br />
<br />
4.训练策略：总结了提高 MM-LLMs 性能的关键训练策略，包括数据处理和模型优化等。<br />
<br />
5.研究方向和资源：讨论了 MM-LLMs 的未来研究方向，并提供了实时跟踪这些模型最新发展的资源。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.13601">arxiv.org/abs/2401.13601</a><br />
PDF：<a href="https://arxiv.org/pdf/2401.13601.pdf">arxiv.org/pdf/2401.13601.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Vxc1VXa2JjQUFCb3VNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750391646121017610#m</id>
            <title>利用红外激光反射攻击自动驾驶车辆，攻击成功率达到100%。

研究人员提出了一种使用红外线激光攻击自动驾驶车辆的方法。

攻击目标是车辆的摄像头系统，特别是那些用于识别道路标志的摄像头。使得自动驾驶汽车无法识别道路标志！

攻击原理：

1.红外线激光的不可见性：

•人眼无法看到红外线激光，但很多车载摄像头没有红外线滤镜，因此能够捕捉到这种光。
•这意味着攻击者可以对道路标志发射红外线激光，而不会被路人注意到。

2.误导识别系统：

•自动驾驶车辆使用深度学习模型通过摄像头捕捉的图像来识别道路标志。
•红外线激光可以改变摄像头捕捉的图像，使得AI识别系统误读这些标志。

3.静止道路标志上的红外线激光投射：

•研究提出在静止的道路标志上投射红外线激光，这种方式相对容易执行，隐蔽性较高。
•激光可以精准地修改标志上的特定部分，导致AI系统对标志的解读出错。

4.攻击效果：

•在室内实验中，所有深度学习模型的攻击成功率达到100%。

•在户外环境下，特别是夜间条件下，攻击成功率在80%至100%之间。

这项研究旨在提前理解并应对可能误导自动驾驶车辆AI识别系统的威胁，以便于未来加强这些系统的安全性和可靠性。

论文：https://arxiv.org/abs/2401.03582</title>
            <link>https://nitter.cz/xiaohuggg/status/1750391646121017610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750391646121017610#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 05:34:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>利用红外激光反射攻击自动驾驶车辆，攻击成功率达到100%。<br />
<br />
研究人员提出了一种使用红外线激光攻击自动驾驶车辆的方法。<br />
<br />
攻击目标是车辆的摄像头系统，特别是那些用于识别道路标志的摄像头。使得自动驾驶汽车无法识别道路标志！<br />
<br />
攻击原理：<br />
<br />
1.红外线激光的不可见性：<br />
<br />
•人眼无法看到红外线激光，但很多车载摄像头没有红外线滤镜，因此能够捕捉到这种光。<br />
•这意味着攻击者可以对道路标志发射红外线激光，而不会被路人注意到。<br />
<br />
2.误导识别系统：<br />
<br />
•自动驾驶车辆使用深度学习模型通过摄像头捕捉的图像来识别道路标志。<br />
•红外线激光可以改变摄像头捕捉的图像，使得AI识别系统误读这些标志。<br />
<br />
3.静止道路标志上的红外线激光投射：<br />
<br />
•研究提出在静止的道路标志上投射红外线激光，这种方式相对容易执行，隐蔽性较高。<br />
•激光可以精准地修改标志上的特定部分，导致AI系统对标志的解读出错。<br />
<br />
4.攻击效果：<br />
<br />
•在室内实验中，所有深度学习模型的攻击成功率达到100%。<br />
<br />
•在户外环境下，特别是夜间条件下，攻击成功率在80%至100%之间。<br />
<br />
这项研究旨在提前理解并应对可能误导自动驾驶车辆AI识别系统的威胁，以便于未来加强这些系统的安全性和可靠性。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.03582">arxiv.org/abs/2401.03582</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzUwMzkxNDA4NzY5NTA3MzI4L2ltZy9VaGVZNVFvemdrck5lTlpCLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750367771446345834#m</id>
            <title>📢ChatGPT 新增了两个小功能

一个是：Always expand code output

如果打开此功能，代码解释器的输出结果将自动展开，输出完整代码，不需要额外点击！

另一个新功能是允许将左侧边栏中的所有“对话历史记录”批量移动到存档中。

这俩功能可以在设置->General中打开…</title>
            <link>https://nitter.cz/xiaohuggg/status/1750367771446345834#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750367771446345834#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 03:59:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>📢ChatGPT 新增了两个小功能<br />
<br />
一个是：Always expand code output<br />
<br />
如果打开此功能，代码解释器的输出结果将自动展开，输出完整代码，不需要额外点击！<br />
<br />
另一个新功能是允许将左侧边栏中的所有“对话历史记录”批量移动到存档中。<br />
<br />
这俩功能可以在设置-&gt;General中打开…</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VxT05HeGJFQUFJVFBELmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VxT05JM2JZQUE4blA5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750340749768691734#m</id>
            <title>本视频中的所有3D模型都是使用Luma AI的Genie 3D模型工具生成的。

人工智能驱动的3D工作流程解锁了无限的创意可能性。🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1750340749768691734#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750340749768691734#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 02:12:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本视频中的所有3D模型都是使用Luma AI的Genie 3D模型工具生成的。<br />
<br />
人工智能驱动的3D工作流程解锁了无限的创意可能性。🫡</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAyMzc0OTk2MjI5MjQyODgvcHUvaW1nL05fdFRGNU13UGl4dUlFTk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1750229816039592354#m</id>
            <title>RT by @xiaohuggg: #开源项目推荐：Nextra

如果你想搭建一个使用Markdown静态文件（MDX）的漂亮博客网站，或者是一个文档网站，可以考虑Nextra，基于Nextjs，部署前会编译成静态网页，支持语法高亮等高级语法。支持站内搜索。

https://github.com/shuding/nextra</title>
            <link>https://nitter.cz/dotey/status/1750229816039592354#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1750229816039592354#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 18:51:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23开源项目推荐">#开源项目推荐</a>：Nextra<br />
<br />
如果你想搭建一个使用Markdown静态文件（MDX）的漂亮博客网站，或者是一个文档网站，可以考虑Nextra，基于Nextjs，部署前会编译成静态网页，支持语法高亮等高级语法。支持站内搜索。<br />
<br />
<a href="https://github.com/shuding/nextra">github.com/shuding/nextra</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VvUWhiT1djQUE4NDE1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750331578302259493#m</id>
            <title>R to @xiaohuggg: 他们还开发了一个AI编程工具：CS50 Duck 

CS50 Duck包含 http://CS50.ai 网站和 VS 插件，是 CS50 课程推出的一个编程辅助工具。其中，http://CS50.ai 网站可以通过 GitHub 账号授权登录并免费使用。

不同于 ChatGPT 或 GitHub Copilot 直接给大段代码，CS50 Duck 更像是一位循循善诱的助教，尝试引导你去寻找答案。

http://CS50.ai 的使用限制机制是 ♥ 的数量，每次开始有10个 ♥，每次互动消耗一个，每三分钟恢复一个。

课程明确规定，学习过程中只允许使用 CS50 课内提供的AI工具，不能借助其他的生成式AI工具，有效避免了AI带来的学术诚信危机。

学生大量的正向反馈表明，谨慎地将AI整合到教育环境中，通过提供持续的、定制化的支持来增强学习体验，是一种更科学的教育方式。</title>
            <link>https://nitter.cz/xiaohuggg/status/1750331578302259493#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750331578302259493#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 01:35:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>他们还开发了一个AI编程工具：CS50 Duck <br />
<br />
CS50 Duck包含 <a href="http://CS50.ai">CS50.ai</a> 网站和 VS 插件，是 CS50 课程推出的一个编程辅助工具。其中，<a href="http://CS50.ai">CS50.ai</a> 网站可以通过 GitHub 账号授权登录并免费使用。<br />
<br />
不同于 ChatGPT 或 GitHub Copilot 直接给大段代码，CS50 Duck 更像是一位循循善诱的助教，尝试引导你去寻找答案。<br />
<br />
<a href="http://CS50.ai">CS50.ai</a> 的使用限制机制是 ♥ 的数量，每次开始有10个 ♥，每次互动消耗一个，每三分钟恢复一个。<br />
<br />
课程明确规定，学习过程中只允许使用 CS50 课内提供的AI工具，不能借助其他的生成式AI工具，有效避免了AI带来的学术诚信危机。<br />
<br />
学生大量的正向反馈表明，谨慎地将AI整合到教育环境中，通过提供持续的、定制化的支持来增强学习体验，是一种更科学的教育方式。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VuRVVaX2FRQUFuX2xmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750331575081026039#m</id>
            <title>哈佛大学CS50x 2024课程

CS50简介： 这是哈佛大学的一门计算机科学和编程入门课程，适合专业学生和非专业学生，无论是否有编程经验（CS50学生有三分之二之前从未学过编程）。

教学内容： 课程不仅仅是教授编程语言的技术课程，它还着重于教学生如何解决问题，无论是通过编码还是不编码的方式。

课程由哈佛大学计算机科学实践教授 David J. Malan @davidjmalan 教授，课程妙趣横生。

涵盖计算思维、抽象、算法、数据结构以及更广泛的计算机科学概念。问题集灵感来源于艺术、人文、社会科学和科学。

编程语言： 课程从C语言开始，学习函数、变量、条件语句、循环等，以及计算机本身如何工作。随后转向Python，这是一种更高级的语言。还有SQL、HTML、CSS和JavaScript。

生可以在edX这个在线学习平台上注册，并完成CS50x课程的所有要求。

完成课程后，学生可以选择支付费用获得由edX颁发的官方验证证书。

开始学习：https://cs50.harvard.edu/x/2024/

中英文双语字幕  https://www.bilibili.com/video/BV16k4y1X7KZ</title>
            <link>https://nitter.cz/xiaohuggg/status/1750331575081026039#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750331575081026039#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 01:35:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哈佛大学CS50x 2024课程<br />
<br />
CS50简介： 这是哈佛大学的一门计算机科学和编程入门课程，适合专业学生和非专业学生，无论是否有编程经验（CS50学生有三分之二之前从未学过编程）。<br />
<br />
教学内容： 课程不仅仅是教授编程语言的技术课程，它还着重于教学生如何解决问题，无论是通过编码还是不编码的方式。<br />
<br />
课程由哈佛大学计算机科学实践教授 David J. Malan <a href="https://nitter.cz/davidjmalan" title="David J. Malan">@davidjmalan</a> 教授，课程妙趣横生。<br />
<br />
涵盖计算思维、抽象、算法、数据结构以及更广泛的计算机科学概念。问题集灵感来源于艺术、人文、社会科学和科学。<br />
<br />
编程语言： 课程从C语言开始，学习函数、变量、条件语句、循环等，以及计算机本身如何工作。随后转向Python，这是一种更高级的语言。还有SQL、HTML、CSS和JavaScript。<br />
<br />
生可以在edX这个在线学习平台上注册，并完成CS50x课程的所有要求。<br />
<br />
完成课程后，学生可以选择支付费用获得由edX颁发的官方验证证书。<br />
<br />
开始学习：<a href="https://cs50.harvard.edu/x/2024/">cs50.harvard.edu/x/2024/</a><br />
<br />
中英文双语字幕  <a href="https://www.bilibili.com/video/BV16k4y1X7KZ">bilibili.com/video/BV16k4y1X…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VuRmJ3aWE4QUFuMlB3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750174478431391774#m</id>
            <title>马上要发售了

据说还能接入ChatGPT

高低要整一台😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1750174478431391774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750174478431391774#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 15:11:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马上要发售了<br />
<br />
据说还能接入ChatGPT<br />
<br />
高低要整一台😐</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwOTU5OTQwMTA5NTk4NzIvcHUvaW1nLzMzT3dDRzdSUzZzNXVUdHQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750173470946988230#m</id>
            <title>兄弟们，发财机会！

现在只要在Poe上创建自己的聊天机器人，分享你的机器人，如果有人通过你的聊天机器人订阅了Poe服务！

每带来一个给50美金🤑

希望奥特曼能学习下🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1750173470946988230#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750173470946988230#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 15:07:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，发财机会！<br />
<br />
现在只要在Poe上创建自己的聊天机器人，分享你的机器人，如果有人通过你的聊天机器人订阅了Poe服务！<br />
<br />
每带来一个给50美金🤑<br />
<br />
希望奥特曼能学习下🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VuZGZUeWJrQUFEdU9ULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141457636450774#m</id>
            <title>R to @xiaohuggg: Lumiere 还可以基于文本编辑视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141457636450774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141457636450774#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Lumiere 还可以基于文本编辑视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDEwODgyMDIxNjYyNzIvcHUvaW1nLzg3dVd0X2ZEUEdxa1FZb20uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141455681888556#m</id>
            <title>R to @xiaohuggg: 风格化视频生成</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141455681888556#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141455681888556#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>风格化视频生成</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDEwMjE2OTczMTQ4MTYvcHUvaW1nL0VxVDFObEZuWWlNU2czc3EuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141453794509040#m</id>
            <title>R to @xiaohuggg: 一些案例</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141453794509040#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141453794509040#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些案例</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDA3NjIwOTUwNzEyMzIvcHUvaW1nL3c0WUZwMDdpelMyU1MxRWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750141451831562514#m</id>
            <title>Lumiere：一次性生成整个视频

Google Research团队开发的基于空间时间的文本到视频扩散模型。

它采用了创新的空间时间U-Net架构，能够一次性生成整个视频的时间长度，不同于其他模型那样逐帧合成视频。

确保了生成视频的连贯性和逼真度。

支持文本到视频、图像到视频 、风格化视频生成 、视频编辑等

主要功能特点：

1、文本到视频的扩散模型： Lumiere能够根据文本提示生成视频，实现了从文本描述到视频内容的直接转换。

2、空间时间U-Net架构： 与其他需要逐步合成视频的模型不同，Lumiere能够一次性完成整个视频的制作。这种独特的架构允许Lumiere一次性生成整个视频的时间长度，不同于其他模型那样逐帧合成视频。

3、全局时间一致性： 由于其架构的特点，Lumiere更容易实现视频内容的全局时间一致性，确保视频的连贯性和逼真度。

4、多尺度空间时间处理： Lumiere通过在多个空间时间尺度上处理视频来学习直接生成视频，这是一种先进的方法。

5、风格化视频生成： 使用单个参考图像，Lumiere可以按照目标风格生成视频，这种能力在其他视频生成模型中较为罕见。

6、广泛的内容创作和视频编辑应用： Lumiere支持多种内容创作任务和视频编辑应用，如图像到视频、视频修补和风格化生成。

视频样式化编辑： 使用文本基础的图像编辑方法，Lumiere可以对视频进行一致性的样式编辑。

影像合成能力： 该模型能在用户指定的区域内对图像内容进行动画化处理，为静态图像增添动态效果。

视频修补功能： Lumiere提供视频修补功能，能够在视频中修改和修饰特定内容。

项目及演示：https://lumiere-video.github.io/
论文：https://arxiv.org/abs/2401.12945</title>
            <link>https://nitter.cz/xiaohuggg/status/1750141451831562514#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750141451831562514#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 13:00:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Lumiere：一次性生成整个视频<br />
<br />
Google Research团队开发的基于空间时间的文本到视频扩散模型。<br />
<br />
它采用了创新的空间时间U-Net架构，能够一次性生成整个视频的时间长度，不同于其他模型那样逐帧合成视频。<br />
<br />
确保了生成视频的连贯性和逼真度。<br />
<br />
支持文本到视频、图像到视频 、风格化视频生成 、视频编辑等<br />
<br />
主要功能特点：<br />
<br />
1、文本到视频的扩散模型： Lumiere能够根据文本提示生成视频，实现了从文本描述到视频内容的直接转换。<br />
<br />
2、空间时间U-Net架构： 与其他需要逐步合成视频的模型不同，Lumiere能够一次性完成整个视频的制作。这种独特的架构允许Lumiere一次性生成整个视频的时间长度，不同于其他模型那样逐帧合成视频。<br />
<br />
3、全局时间一致性： 由于其架构的特点，Lumiere更容易实现视频内容的全局时间一致性，确保视频的连贯性和逼真度。<br />
<br />
4、多尺度空间时间处理： Lumiere通过在多个空间时间尺度上处理视频来学习直接生成视频，这是一种先进的方法。<br />
<br />
5、风格化视频生成： 使用单个参考图像，Lumiere可以按照目标风格生成视频，这种能力在其他视频生成模型中较为罕见。<br />
<br />
6、广泛的内容创作和视频编辑应用： Lumiere支持多种内容创作任务和视频编辑应用，如图像到视频、视频修补和风格化生成。<br />
<br />
视频样式化编辑： 使用文本基础的图像编辑方法，Lumiere可以对视频进行一致性的样式编辑。<br />
<br />
影像合成能力： 该模型能在用户指定的区域内对图像内容进行动画化处理，为静态图像增添动态效果。<br />
<br />
视频修补功能： Lumiere提供视频修补功能，能够在视频中修改和修饰特定内容。<br />
<br />
项目及演示：<a href="https://lumiere-video.github.io/">lumiere-video.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2401.12945">arxiv.org/abs/2401.12945</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAxNDAzOTQ0NDMzNTgyMDgvcHUvaW1nL3BtbklPVDNGbWFtdzVXaVMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750084449818333194#m</id>
            <title>R to @xiaohuggg: 这个是原版英文视频

上面那个是我用AI Dubbing &amp; Video Translator翻译的

很牛叉</title>
            <link>https://nitter.cz/xiaohuggg/status/1750084449818333194#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750084449818333194#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 09:13:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是原版英文视频<br />
<br />
上面那个是我用AI Dubbing &amp; Video Translator翻译的<br />
<br />
很牛叉</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwODQwMzcwNDAwODcwNDAvcHUvaW1nL3FOR2xaUHVpUk5WWEhhQ0suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750084448048312780#m</id>
            <title>卧槽 ElevenLabs 这个产品炸裂了💥

ElevenLabs 推出一个全自动化的AI配音或视频翻译工具。

你只需要上传视频或者粘贴视频链接，这个工具就能全自动的在几十秒到几分钟内将你的视频翻译成29种语言。

更牛P的是接克隆原视频里面的声音，来给你配音。

就算是视频里面有多个人说话也能全部克隆翻译。😂

下面这个视频就是我翻译的中文官方视频！

AI Dubbing &amp; Video Translator的主要功能：

1、视频翻译和配音： 将视频的声音从一种语言翻译成另一种语言，并且能保持原始发言者的声音特质不变。

2、广泛应用： 可以用于多种场合，比如让不同语言的观众看懂外语视频，或者让公司的宣传和培训视频能被不同国家的员工理解。

3、简单操作： 只需上传视频，选择想要的语言，剩下的翻译和配音工作都由AI自动完成。

4、支持多种视频平台： 直接复制粘贴YouTube、TikTok、Twitter等平台上的视频链接即可进行配音。

5、保持原声风格： 在翻译时，AI会尽量保持视频中人物的原声调和风格。

6、适用于多发言者视频： 能够处理多个人物的对话，确保每个人的声音都能被正确识别和翻译。

7、全自动化流程： 从上传视频到翻译、配音，整个过程都是自动的，无需人工干预。

8、多语言支持： 支持将视频翻译成29种不同的语言，覆盖全球多数主要语种。

9、多种文件格式支持： 不仅支持视频文件，还能处理音频文件，如MP3、MP4等。

在线体验：https://elevenlabs.io/dubbing</title>
            <link>https://nitter.cz/xiaohuggg/status/1750084448048312780#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750084448048312780#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 09:13:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>卧槽 ElevenLabs 这个产品炸裂了💥<br />
<br />
ElevenLabs 推出一个全自动化的AI配音或视频翻译工具。<br />
<br />
你只需要上传视频或者粘贴视频链接，这个工具就能全自动的在几十秒到几分钟内将你的视频翻译成29种语言。<br />
<br />
更牛P的是接克隆原视频里面的声音，来给你配音。<br />
<br />
就算是视频里面有多个人说话也能全部克隆翻译。😂<br />
<br />
下面这个视频就是我翻译的中文官方视频！<br />
<br />
AI Dubbing & Video Translator的主要功能：<br />
<br />
1、视频翻译和配音： 将视频的声音从一种语言翻译成另一种语言，并且能保持原始发言者的声音特质不变。<br />
<br />
2、广泛应用： 可以用于多种场合，比如让不同语言的观众看懂外语视频，或者让公司的宣传和培训视频能被不同国家的员工理解。<br />
<br />
3、简单操作： 只需上传视频，选择想要的语言，剩下的翻译和配音工作都由AI自动完成。<br />
<br />
4、支持多种视频平台： 直接复制粘贴YouTube、TikTok、Twitter等平台上的视频链接即可进行配音。<br />
<br />
5、保持原声风格： 在翻译时，AI会尽量保持视频中人物的原声调和风格。<br />
<br />
6、适用于多发言者视频： 能够处理多个人物的对话，确保每个人的声音都能被正确识别和翻译。<br />
<br />
7、全自动化流程： 从上传视频到翻译、配音，整个过程都是自动的，无需人工干预。<br />
<br />
8、多语言支持： 支持将视频翻译成29种不同的语言，覆盖全球多数主要语种。<br />
<br />
9、多种文件格式支持： 不仅支持视频文件，还能处理音频文件，如MP3、MP4等。<br />
<br />
在线体验：<a href="https://elevenlabs.io/dubbing">elevenlabs.io/dubbing</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwODI2OTA3NDgxMjUxODQvcHUvaW1nL1ZoSGxGYUp2SEpGT0xiNjQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750078889181986928#m</id>
            <title>MedSAM：通用医学影像分割模型

MedSAM是一种医学影像分割工具，它能够自动识别和描绘医学影像中的重要区域，比如肿瘤或其他组织的病变。

通过学习大量医学影像和对应的掩模（即正确的分割结果），它能够处理各种不同的医学影像和复杂情况。

它可以帮助医生更快、更准确地诊断疾病。

MedSAM是基于深度学习技术开发的，在现有的分割基础模型SAM的基础上进行改进和微调。

使用包含超过100万医学影像-掩模对的大规模数据集进行训练，覆盖了10种成像方式、超过30种癌症类型以及多种成像协议。

MedSAM已经在《Nature Communications》上发表。

MedSAM的详细功能解析：

1、通用医学影像分割

应用范围广泛： MedSAM能够处理各种医学影像分割任务，适用于多种不同的解剖结构、病理条件，如肿瘤、器官、组织等。

多种成像模式兼容： 它不仅支持常见的成像模式如CT（计算机断层扫描）和MRI（磁共振成像），也能处理超声波、内窥镜等其他成像方式的影像。

全面覆盖： 能够识别和分割出各种复杂形态和大小的医学影像目标，提供全面的医学影像分析。

2、高度适应性

灵活应对各种变化： 无论是成像技术的变化、不同的解剖结构特点，还是病理条件的多样性，MedSAM都能准确适应。

广泛的病理条件处理： 从常见病变到罕见病理状态，MedSAM能够有效识别和分割，支持医学研究和临床诊断。

适应不同成像条件： 对不同成像设备或技术产生的影像具有良好的适应性，能够保持分割的准确性和一致性。

3、交互式分割

用户引导的精准分割： 用户可以通过绘制边界框等方式对感兴趣区域进行标记，MedSAM据此进行精确的分割。

提高分割精度： 这种交互式方法有助于提高分割的精度，尤其是在复杂或模糊区域的处理上。

适用性增强： 通过用户的直观输入，MedSAM能够更好地理解和执行特定的医学影像分割任务，提高了其在实际应用中的适用性和灵活性。

MedSAM实验结果：

1、内部验证：

86个内部验证任务： MedSAM在一个包含86个不同任务的测试集上进行了测试。这些任务涵盖了各种医学影像分割的场景。

优于现有模型： 在这些测试中，MedSAM的表现一致地优于当前市场上最先进的医学影像分割模型。

鲁棒性： MedSAM显示出良好的鲁棒性，即在不同的任务和条件下都能保持稳定和高效的分割性能。

2、外部验证

60个外部验证任务： 在另外60个任务上进行了外部验证，这些任务包括新的数据集和MedSAM之前未接触过的分割目标。

展现泛化能力： 在这些新的挑战中，MedSAM展示了其出色的泛化能力，能够有效处理未知或未见过的数据和分割任务。

3、与专家模型比较

与专业模型相当或更好： 当MedSAM的性能与那些专门为同一成像方式（如CT、MRI）训练的专家模型相比较时，MedSAM不仅表现得与这些模型相当，甚至在某些情况下还超越了它们。

Nature：https://www.nature.com/articles/s41467-024-44824-z

论文：https://arxiv.org/abs/2304.12306

GitHub：https://github.com/bowang-lab/MedSAM

他们还开发了一个轻量级模型LiteMedSAM，提供了10倍的速度提升，同时保持准确性。</title>
            <link>https://nitter.cz/xiaohuggg/status/1750078889181986928#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750078889181986928#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 08:51:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MedSAM：通用医学影像分割模型<br />
<br />
MedSAM是一种医学影像分割工具，它能够自动识别和描绘医学影像中的重要区域，比如肿瘤或其他组织的病变。<br />
<br />
通过学习大量医学影像和对应的掩模（即正确的分割结果），它能够处理各种不同的医学影像和复杂情况。<br />
<br />
它可以帮助医生更快、更准确地诊断疾病。<br />
<br />
MedSAM是基于深度学习技术开发的，在现有的分割基础模型SAM的基础上进行改进和微调。<br />
<br />
使用包含超过100万医学影像-掩模对的大规模数据集进行训练，覆盖了10种成像方式、超过30种癌症类型以及多种成像协议。<br />
<br />
MedSAM已经在《Nature Communications》上发表。<br />
<br />
MedSAM的详细功能解析：<br />
<br />
1、通用医学影像分割<br />
<br />
应用范围广泛： MedSAM能够处理各种医学影像分割任务，适用于多种不同的解剖结构、病理条件，如肿瘤、器官、组织等。<br />
<br />
多种成像模式兼容： 它不仅支持常见的成像模式如CT（计算机断层扫描）和MRI（磁共振成像），也能处理超声波、内窥镜等其他成像方式的影像。<br />
<br />
全面覆盖： 能够识别和分割出各种复杂形态和大小的医学影像目标，提供全面的医学影像分析。<br />
<br />
2、高度适应性<br />
<br />
灵活应对各种变化： 无论是成像技术的变化、不同的解剖结构特点，还是病理条件的多样性，MedSAM都能准确适应。<br />
<br />
广泛的病理条件处理： 从常见病变到罕见病理状态，MedSAM能够有效识别和分割，支持医学研究和临床诊断。<br />
<br />
适应不同成像条件： 对不同成像设备或技术产生的影像具有良好的适应性，能够保持分割的准确性和一致性。<br />
<br />
3、交互式分割<br />
<br />
用户引导的精准分割： 用户可以通过绘制边界框等方式对感兴趣区域进行标记，MedSAM据此进行精确的分割。<br />
<br />
提高分割精度： 这种交互式方法有助于提高分割的精度，尤其是在复杂或模糊区域的处理上。<br />
<br />
适用性增强： 通过用户的直观输入，MedSAM能够更好地理解和执行特定的医学影像分割任务，提高了其在实际应用中的适用性和灵活性。<br />
<br />
MedSAM实验结果：<br />
<br />
1、内部验证：<br />
<br />
86个内部验证任务： MedSAM在一个包含86个不同任务的测试集上进行了测试。这些任务涵盖了各种医学影像分割的场景。<br />
<br />
优于现有模型： 在这些测试中，MedSAM的表现一致地优于当前市场上最先进的医学影像分割模型。<br />
<br />
鲁棒性： MedSAM显示出良好的鲁棒性，即在不同的任务和条件下都能保持稳定和高效的分割性能。<br />
<br />
2、外部验证<br />
<br />
60个外部验证任务： 在另外60个任务上进行了外部验证，这些任务包括新的数据集和MedSAM之前未接触过的分割目标。<br />
<br />
展现泛化能力： 在这些新的挑战中，MedSAM展示了其出色的泛化能力，能够有效处理未知或未见过的数据和分割任务。<br />
<br />
3、与专家模型比较<br />
<br />
与专业模型相当或更好： 当MedSAM的性能与那些专门为同一成像方式（如CT、MRI）训练的专家模型相比较时，MedSAM不仅表现得与这些模型相当，甚至在某些情况下还超越了它们。<br />
<br />
Nature：<a href="https://www.nature.com/articles/s41467-024-44824-z">nature.com/articles/s41467-0…</a><br />
<br />
论文：<a href="https://arxiv.org/abs/2304.12306">arxiv.org/abs/2304.12306</a><br />
<br />
GitHub：<a href="https://github.com/bowang-lab/MedSAM">github.com/bowang-lab/MedSAM</a><br />
<br />
他们还开发了一个轻量级模型LiteMedSAM，提供了10倍的速度提升，同时保持准确性。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwNzgxNzYyMjU4ODIxMTIvcHUvaW1nLy1ycDdxZU1aRXJRcmVCYVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750059988037587430#m</id>
            <title>兄弟们 发现一个好玩的东西🤓

HuixiangDou：利用AI解决群聊场景中冷场的问题😃

茴香豆是一个基于大语言模型的群聊知识助手，它能够自动识别并回答群聊中的技术相关的问题，且不会被群聊中的非技术内容干扰。

可以集成到即时聊天工具（如微信、飞书）的群聊中。

主要针对技术问题，特别是与编程、算法、软件开发等相关的问题。

由于是开源的可以改造成各种行业类型的机器人，专门回答各种问题。

主要功能特点：

1、技术问题解答：像技术专家一样回答问题： 它能像一位懂技术的好朋友那样回答群聊中的技术问题，比如关于最新的编程技巧、计算机视觉或深度学习等方面的问题。

2、适应群聊场景：特别设计用于即时通讯工具中的群聊环境，如微信和飞书，能够有效地在这些环境中运作。

3、避免信息泛滥：不让群聊变成杂货铺： 能够帮助群聊保持整洁，防止无关的聊天信息充斥整个对话，让重要的技术讨论更加突出。

4、领域特定知识理解：精通各种专业知识： 它不仅仅是一个能上网查资料的工具，还能理解和处理那些特别专业的技术问题，包括最新的开源项目信息。

5、高度定制化回应：量身定制回答： 根据群聊里的具体讨论内容和背景，茴香豆能提供非常符合情境的回答，确保每次回答都相关且精准。

6、长上下文处理能力：记忆力超群： 即使是长时间或复杂的对话，茴香豆也能跟上，理解整个对话的历史，回答更加详细和深入的技术问题。

7、支持远程和本地LLM服务：茴香豆支持使用本地LLM模型，也支持通过远程API（如OpenAI的API）来处理问题，这为用户提供了灵活性。

8、搜索增强：茴香豆可以通过集成如Sourcegraph这样的代码搜索工具，增强对疑难问题的解答能力。

9、调参和优化：茴香豆支持根据业务场景进行调参，以优化问答效果，这包括调整搜索结果个数、修改搜索结果偏序等。

GitHub：https://github.com/InternLM/HuixiangDou
论文：https://arxiv.org/abs/2401.08772

HuixiangDou的最终版本专注于增强聊天模型的长上下文处理能力，并在以下三个方面扩展了响应流水线，以提高提供有效答案的可能性：

1、扩展的长上下文处理能力

目的： 处理更长的对话或文本，使模型能够理解和回应更复杂的技术问题。

实现方式： 通过调整和优化模型架构，使其能够处理并维持更长篇幅的对话历史，从而在群聊环境中更准确地回应用户查询。

2、增强的响应流水线

搜索增强： 使用多种搜索技术（如文档片段检索）来找到与用户查询最相关的信息，确保回答的准确性和相关性。
LLM提示技术： 利用大型语言模型的自然语言处理能力，通过精心设计的提示来提取和处理关键信息，更准确地定位用户问题的核心。

回答评估和筛选： 在提供答案之前，使用模型对回答的相关性和准确性进行评估，确保只有高质量的回答被呈现给用户。

3、提升回答质量的其他改进

仓库搜索功能： 特别针对技术问题，允许模型直接从相关的代码仓库或文档中检索信息，提供更专业和详细的答案。

参数调整和优化： 根据实际应用场景和用户反馈，调整模型的参数和设置，以达到最佳的回答效果。

多模态输入处理： 除了文本信息外，模型还能处理其他类型的输入（如代码片段），从而在更广泛的场景中提供帮助。</title>
            <link>https://nitter.cz/xiaohuggg/status/1750059988037587430#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750059988037587430#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 07:36:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 发现一个好玩的东西🤓<br />
<br />
HuixiangDou：利用AI解决群聊场景中冷场的问题😃<br />
<br />
茴香豆是一个基于大语言模型的群聊知识助手，它能够自动识别并回答群聊中的技术相关的问题，且不会被群聊中的非技术内容干扰。<br />
<br />
可以集成到即时聊天工具（如微信、飞书）的群聊中。<br />
<br />
主要针对技术问题，特别是与编程、算法、软件开发等相关的问题。<br />
<br />
由于是开源的可以改造成各种行业类型的机器人，专门回答各种问题。<br />
<br />
主要功能特点：<br />
<br />
1、技术问题解答：像技术专家一样回答问题： 它能像一位懂技术的好朋友那样回答群聊中的技术问题，比如关于最新的编程技巧、计算机视觉或深度学习等方面的问题。<br />
<br />
2、适应群聊场景：特别设计用于即时通讯工具中的群聊环境，如微信和飞书，能够有效地在这些环境中运作。<br />
<br />
3、避免信息泛滥：不让群聊变成杂货铺： 能够帮助群聊保持整洁，防止无关的聊天信息充斥整个对话，让重要的技术讨论更加突出。<br />
<br />
4、领域特定知识理解：精通各种专业知识： 它不仅仅是一个能上网查资料的工具，还能理解和处理那些特别专业的技术问题，包括最新的开源项目信息。<br />
<br />
5、高度定制化回应：量身定制回答： 根据群聊里的具体讨论内容和背景，茴香豆能提供非常符合情境的回答，确保每次回答都相关且精准。<br />
<br />
6、长上下文处理能力：记忆力超群： 即使是长时间或复杂的对话，茴香豆也能跟上，理解整个对话的历史，回答更加详细和深入的技术问题。<br />
<br />
7、支持远程和本地LLM服务：茴香豆支持使用本地LLM模型，也支持通过远程API（如OpenAI的API）来处理问题，这为用户提供了灵活性。<br />
<br />
8、搜索增强：茴香豆可以通过集成如Sourcegraph这样的代码搜索工具，增强对疑难问题的解答能力。<br />
<br />
9、调参和优化：茴香豆支持根据业务场景进行调参，以优化问答效果，这包括调整搜索结果个数、修改搜索结果偏序等。<br />
<br />
GitHub：<a href="https://github.com/InternLM/HuixiangDou">github.com/InternLM/Huixiang…</a><br />
论文：<a href="https://arxiv.org/abs/2401.08772">arxiv.org/abs/2401.08772</a><br />
<br />
HuixiangDou的最终版本专注于增强聊天模型的长上下文处理能力，并在以下三个方面扩展了响应流水线，以提高提供有效答案的可能性：<br />
<br />
1、扩展的长上下文处理能力<br />
<br />
目的： 处理更长的对话或文本，使模型能够理解和回应更复杂的技术问题。<br />
<br />
实现方式： 通过调整和优化模型架构，使其能够处理并维持更长篇幅的对话历史，从而在群聊环境中更准确地回应用户查询。<br />
<br />
2、增强的响应流水线<br />
<br />
搜索增强： 使用多种搜索技术（如文档片段检索）来找到与用户查询最相关的信息，确保回答的准确性和相关性。<br />
LLM提示技术： 利用大型语言模型的自然语言处理能力，通过精心设计的提示来提取和处理关键信息，更准确地定位用户问题的核心。<br />
<br />
回答评估和筛选： 在提供答案之前，使用模型对回答的相关性和准确性进行评估，确保只有高质量的回答被呈现给用户。<br />
<br />
3、提升回答质量的其他改进<br />
<br />
仓库搜索功能： 特别针对技术问题，允许模型直接从相关的代码仓库或文档中检索信息，提供更专业和详细的答案。<br />
<br />
参数调整和优化： 根据实际应用场景和用户反馈，调整模型的参数和设置，以达到最佳的回答效果。<br />
<br />
多模态输入处理： 除了文本信息外，模型还能处理其他类型的输入（如代码片段），从而在更广泛的场景中提供帮助。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwNTkyMDMyMTIwNDYzMzYvcHUvaW1nL0pwSVU4cDQzX0ZwVjZuN18uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749758300076511742#m</id>
            <title>背景知识：

这哥们的视频爆了，1.5亿流量

获得了X 25万美金广告费，全部拿来抽奖了！

背景知识的背景知识：

MrBeast 是美国知名社交媒体网红，YouTube第一网红！

尽管在X平台上获得了26.3万美元的广告收入，这对于MrBeast来说并不算多。《福布斯》报道，他去年的收入为5400万美元，是2023年收入最高的创作者。

MrBeast还接近与亚马逊达成价值1亿美元的节目协议。 

MrBeast 来说，这并不算多。同时，他本人也认为这 1.5 亿浏览量可能并非完全自然产生，而是广告商推动的结果。</title>
            <link>https://nitter.cz/xiaohuggg/status/1749758300076511742#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749758300076511742#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 11:37:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>背景知识：<br />
<br />
这哥们的视频爆了，1.5亿流量<br />
<br />
获得了X 25万美金广告费，全部拿来抽奖了！<br />
<br />
背景知识的背景知识：<br />
<br />
MrBeast 是美国知名社交媒体网红，YouTube第一网红！<br />
<br />
尽管在X平台上获得了26.3万美元的广告收入，这对于MrBeast来说并不算多。《福布斯》报道，他去年的收入为5400万美元，是2023年收入最高的创作者。<br />
<br />
MrBeast还接近与亚马逊达成价值1亿美元的节目协议。 <br />
<br />
MrBeast 来说，这并不算多。同时，他本人也认为这 1.5 亿浏览量可能并非完全自然产生，而是广告商推动的结果。</p>
<p><a href="https://nitter.cz/MrBeast/status/1749500209061663043#m">nitter.cz/MrBeast/status/1749500209061663043#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoajVHb2IwQUE1dGNRLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoajVHbWF3QUU1T0xsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726672218415580#m</id>
            <title>R to @xiaohuggg: 带有 ControlNet 的 RPG</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726672218415580#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726672218415580#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>带有 ControlNet 的 RPG</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRS1hTGFFQUFEbzdZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRkZsRGFVQUFEZVJ3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>