<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731896401301557749#m</id>
            <title>通过再生疗法逆转听力损失

我们的耳朵里有一种微小毛细胞，它们对我们能否听到声音非常重要。

但是这些细胞很容易因为噪音或某些药物等原因而死亡，而且一旦死亡就不会再长出来。

麻省理工的衍生公司Frequency Therapeutics研究团队发现了一种小分子药物，注射到耳朵里，让这些毛细胞重新长出来。

在他们的临床试验中，一些参与者在接受了这种治疗后，他们的听力有了明显的改善。

详细：https://news.mit.edu/2022/frequency-therapeutics-hearing-regeneration-0329</title>
            <link>https://nitter.cz/xiaohuggg/status/1731896401301557749#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731896401301557749#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 04:40:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过再生疗法逆转听力损失<br />
<br />
我们的耳朵里有一种微小毛细胞，它们对我们能否听到声音非常重要。<br />
<br />
但是这些细胞很容易因为噪音或某些药物等原因而死亡，而且一旦死亡就不会再长出来。<br />
<br />
麻省理工的衍生公司Frequency Therapeutics研究团队发现了一种小分子药物，注射到耳朵里，让这些毛细胞重新长出来。<br />
<br />
在他们的临床试验中，一些参与者在接受了这种治疗后，他们的听力有了明显的改善。<br />
<br />
详细：<a href="https://news.mit.edu/2022/frequency-therapeutics-hearing-regeneration-0329">news.mit.edu/2022/frequency-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FqdVFrU2FRQUE2MGRuLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731888448582373761#m</id>
            <title>Suno @suno_ai_+  Midjourney+D-ID

创作唱歌视频👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1731888448582373761#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731888448582373761#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 04:09:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Suno <a href="https://nitter.cz/suno_ai_" title="Suno">@suno_ai_</a>+  Midjourney+D-ID<br />
<br />
创作唱歌视频👍</p>
<p><a href="https://nitter.cz/anukaakash/status/1731628181600526781#m">nitter.cz/anukaakash/status/1731628181600526781#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731882948750643325#m</id>
            <title>昨天吐槽 @Magnific_AI 不给白嫖的机会

今天就发善心了❤️

只要是之前注册的账号，今天登录都能获得50个代币🟡 

但是今天注册的新账号好像没有...😃

我刚登陆进去果然有50个🟡 

你们可以进去碰碰运气：https://magnific.ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1731882948750643325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731882948750643325#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 03:47:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨天吐槽 <a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 不给白嫖的机会<br />
<br />
今天就发善心了❤️<br />
<br />
只要是之前注册的账号，今天登录都能获得50个代币🟡 <br />
<br />
但是今天注册的新账号好像没有...😃<br />
<br />
我刚登陆进去果然有50个🟡 <br />
<br />
你们可以进去碰碰运气：<a href="https://magnific.ai/">magnific.ai/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE4ODI3ODI3MzYxNzkyMDAvcHUvaW1nLy1YTEZpamZnN0JsTHIwZjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731869471705292843#m</id>
            <title>R to @xiaohuggg: 这个是阿里的同样的项目</title>
            <link>https://nitter.cz/xiaohuggg/status/1731869471705292843#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731869471705292843#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 02:53:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个是阿里的同样的项目</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730133378501067046#m">nitter.cz/xiaohuggg/status/1730133378501067046#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731868943340707855#m</id>
            <title>阿里前几天发的只靠单张照片和动作就能生成跳舞视频的项目一下就被字节跳动秒了...

因为他们没有发布代码和演示，字节今天直接就放出了同样的项目并提供了代码和演示。

MagicAnimate：基于扩散模型的人类图像动画框架

不仅支持把静止的图片变成动作视频。

还能结合文本成动画，而且还支持多人照片。

MagicAnimate是一个基于扩散模型的人类图像动画框架，旨在增强时间一致性、忠实保留参考图像，并提高动画的真实感。

主要功能特点：

1、时间一致性动画：MagicAnimate的目标是根据运动序列使参考图像动起来，并保持时间上的一致性。能够确保动画在时间上的连贯性，动画中的动作看起来自然流畅，没有突兀的变化。

2、忠实于原图：在动画化过程中，它能够保持对原始参考图像的高度忠实度，确保动画中的人物或对象与原图保持一致。

3、跨身份动画：MagicAnimate还能够进行跨身份动画，即使用来自不同视频的运动序列来动画化参考图像。网站展示了三个身份和两个运动序列的视频结果。

4、未见领域动画：该项目能够动画化未见领域的图像，例如油画和电影角色，使其执行跑步或瑜伽等动作。

5、与T2I扩散模型结合：MagicAnimate还可以与DALLE3生成的参考图像结合，使其执行各种动作。每个参考图像的文本提示也在视频下方展示。

6、多人动画：该框架还支持多人动画，根据给定的运动序列动画化多个人物。

MagicAnimate使用视频扩散模型和外观编码器来进行时间建模和身份保持。为了支持长视频动画，开发了一个简单的视频融合策略，在推理过程中产生平滑的视频过渡。

主要工作原理：

1、视频扩散模型：MagicAnimate使用一种称为视频扩散模型的技术。这种模型能够处理时间序列数据，即它不仅考虑单个图像，还考虑图像随时间的变化。这使得生成的动画在时间上保持连贯和一致。

2、外观编码器：为了保持动画中人物的身份和外观特征与原始图像一致，MagicAnimate使用外观编码器。这个编码器确保即使在动画过程中，人物的基本特征（如面部特征、服装等）保持不变。

3、参考图像和目标动作序列：在生成动画时，MagicAnimate需要两个输入：一是参考图像（如人物照片），二是目标动作序列（描述人物应该如何移动）。这些动作序列可以是预先定义的，也可以是根据特定任务动态生成的。

4、视频融合策略：为了支持长视频动画的生成，MagicAnimate采用了视频融合策略。这种策略能够在动画的不同部分之间平滑过渡，避免突兀的切换，从而生成更自然的长时动画。

5、多样化应用：除了基本的图像动画化，MagicAnimate还能应用于更多场景，如将未见领域的图像（例如油画或电影角色）动画化，或者结合文本描述生成动画。

这种技术在动画制作、游戏设计、虚拟现实等领域具有广泛的应用潜力。

项目及演示：https://showlab.github.io/magicanimate/
论文：https://arxiv.org/abs/2311.16498
GitHub：https://github.com/magic-research/magic-animate

Huggingface在线测试：https://huggingface.co/spaces/zcxu-eric/magicanimate

Colab在线测试：https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb</title>
            <link>https://nitter.cz/xiaohuggg/status/1731868943340707855#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731868943340707855#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 02:51:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里前几天发的只靠单张照片和动作就能生成跳舞视频的项目一下就被字节跳动秒了...<br />
<br />
因为他们没有发布代码和演示，字节今天直接就放出了同样的项目并提供了代码和演示。<br />
<br />
MagicAnimate：基于扩散模型的人类图像动画框架<br />
<br />
不仅支持把静止的图片变成动作视频。<br />
<br />
还能结合文本成动画，而且还支持多人照片。<br />
<br />
MagicAnimate是一个基于扩散模型的人类图像动画框架，旨在增强时间一致性、忠实保留参考图像，并提高动画的真实感。<br />
<br />
主要功能特点：<br />
<br />
1、时间一致性动画：MagicAnimate的目标是根据运动序列使参考图像动起来，并保持时间上的一致性。能够确保动画在时间上的连贯性，动画中的动作看起来自然流畅，没有突兀的变化。<br />
<br />
2、忠实于原图：在动画化过程中，它能够保持对原始参考图像的高度忠实度，确保动画中的人物或对象与原图保持一致。<br />
<br />
3、跨身份动画：MagicAnimate还能够进行跨身份动画，即使用来自不同视频的运动序列来动画化参考图像。网站展示了三个身份和两个运动序列的视频结果。<br />
<br />
4、未见领域动画：该项目能够动画化未见领域的图像，例如油画和电影角色，使其执行跑步或瑜伽等动作。<br />
<br />
5、与T2I扩散模型结合：MagicAnimate还可以与DALLE3生成的参考图像结合，使其执行各种动作。每个参考图像的文本提示也在视频下方展示。<br />
<br />
6、多人动画：该框架还支持多人动画，根据给定的运动序列动画化多个人物。<br />
<br />
MagicAnimate使用视频扩散模型和外观编码器来进行时间建模和身份保持。为了支持长视频动画，开发了一个简单的视频融合策略，在推理过程中产生平滑的视频过渡。<br />
<br />
主要工作原理：<br />
<br />
1、视频扩散模型：MagicAnimate使用一种称为视频扩散模型的技术。这种模型能够处理时间序列数据，即它不仅考虑单个图像，还考虑图像随时间的变化。这使得生成的动画在时间上保持连贯和一致。<br />
<br />
2、外观编码器：为了保持动画中人物的身份和外观特征与原始图像一致，MagicAnimate使用外观编码器。这个编码器确保即使在动画过程中，人物的基本特征（如面部特征、服装等）保持不变。<br />
<br />
3、参考图像和目标动作序列：在生成动画时，MagicAnimate需要两个输入：一是参考图像（如人物照片），二是目标动作序列（描述人物应该如何移动）。这些动作序列可以是预先定义的，也可以是根据特定任务动态生成的。<br />
<br />
4、视频融合策略：为了支持长视频动画的生成，MagicAnimate采用了视频融合策略。这种策略能够在动画的不同部分之间平滑过渡，避免突兀的切换，从而生成更自然的长时动画。<br />
<br />
5、多样化应用：除了基本的图像动画化，MagicAnimate还能应用于更多场景，如将未见领域的图像（例如油画或电影角色）动画化，或者结合文本描述生成动画。<br />
<br />
这种技术在动画制作、游戏设计、虚拟现实等领域具有广泛的应用潜力。<br />
<br />
项目及演示：<a href="https://showlab.github.io/magicanimate/">showlab.github.io/magicanima…</a><br />
论文：<a href="https://arxiv.org/abs/2311.16498">arxiv.org/abs/2311.16498</a><br />
GitHub：<a href="https://github.com/magic-research/magic-animate">github.com/magic-research/ma…</a><br />
<br />
Huggingface在线测试：<a href="https://huggingface.co/spaces/zcxu-eric/magicanimate">huggingface.co/spaces/zcxu-e…</a><br />
<br />
Colab在线测试：<a href="https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb">colab.research.google.com/gi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE4NjgzOTM0NjMyNDY4NDgvcHUvaW1nL1RmdkJGaThaMHpDUUxoOVAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731852214304456888#m</id>
            <title>GTA 6 预告片被泄露，迫使 Rockstar Games 提前发布正式版…

😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1731852214304456888#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731852214304456888#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 01:45:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GTA 6 预告片被泄露，迫使 Rockstar Games 提前发布正式版…<br />
<br />
😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMxODUxOTU0NTA5NTA4NjA4L2ltZy9WWXlWZzZYSDROeTFhVURJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731682210963472831#m</id>
            <title>可惜这个人说的话

没有任何一家媒体敢发

也没有任何博主敢发到国内平台</title>
            <link>https://nitter.cz/xiaohuggg/status/1731682210963472831#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731682210963472831#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 14:29:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>可惜这个人说的话<br />
<br />
没有任何一家媒体敢发<br />
<br />
也没有任何博主敢发到国内平台</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMxNjgxNzk4NzkzNDAwMzIwL2ltZy9pMUdwVXNNVmFEWGVWTHRKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731665076103545187#m</id>
            <title>R to @xiaohuggg: 演示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1731665076103545187#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731665076103545187#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 13:21:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示：</p>
<p><a href="https://nitter.cz/iamneubert/status/1731309980165259686#m">nitter.cz/iamneubert/status/1731309980165259686#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731665074253955433#m</id>
            <title>🧪 Lo-Fi 日本动漫美学 by @runwayml

提示词模版：

[Japanese Ghibli Aesthetic] [Prompt] [Vibrant, Optimistic, Lo-fi, Minimalistic] / No Preset</title>
            <link>https://nitter.cz/xiaohuggg/status/1731665074253955433#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731665074253955433#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 13:21:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🧪 Lo-Fi 日本动漫美学 by <a href="https://nitter.cz/runwayml" title="Runway">@runwayml</a><br />
<br />
提示词模版：<br />
<br />
[Japanese Ghibli Aesthetic] [Prompt] [Vibrant, Optimistic, Lo-fi, Minimalistic] / No Preset</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA2MjU1MTI4MDMwMTI2MTEvcHUvaW1nL1pfVkVIVDRQTEVXajBvSkYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731601609933865214#m</id>
            <title>MoMask：根据文字描述来生成3D动画人物动作。

它不仅能生成常见的动作（如走路、跑步），还能根据更具体的描述生成复杂的动作（如特定类型的舞蹈动作）。

它的工作方式有点像是把人类的动作分解成一系列小块，每个小块代表一个特定的动作。

然后，根据文字描述，它会选择合适的动作小块，组合起来。

就像搭积木一样，最终形成一个完整的、流畅的动作序列。

此外，它还能够根据需要“填补”动作序列中的空白部分，比如如果一个动作序列中间缺少一部分，MoMask能够智能地补全这部分，使整个动作看起来自然流畅。

工作原理可以分为几个关键步骤：

1、分层量化表示：首先，MoMask使用一种称为“分层量化”的技术来表示人类的动作。这意味着它把复杂的人类动作分解成多个层次的“动作标记”。每个标记都是动作的一个小部分，就像是用来描述动作的“字母”一样。

2、向量量化：在基础层，MoMask通过一种叫做“向量量化”的过程获取一系列运动标记。这个过程类似于把连续的动作数据转换成一系列离散的标记，这些标记能够更精确地捕捉动作的细节。

3、残差标记：在随后的层级中，MoMask生成所谓的“残差标记”，这些标记代表了从基础层动作标记中提取的更高阶的动作信息。

4、双向变换器：MoMask使用两个不同的双向变换器来处理这些标记。第一个变换器（掩码变换器）用于预测在训练阶段根据文本输入随机掩盖的运动标记。在生成（即推理）阶段，从一个空序列开始，掩码变换器逐步填充缺失的标记。第二个变换器（残差变换器）学习基于当前层的结果来逐步预测下一层的标记。

5、文本驱动的生成：在实际应用中，MoMask能够根据给定的文字描述生成相应的3D人类动作。例如，如果描述是“一个人在跳舞”，MoMask会生成一个符合这一描述的动作序列。

6、除了直接根据文本生成动作，MoMask还可以用于其他相关任务，如“时间内插”，即在现有动作片段中填补特定区域，使其符合文本描述。

项目及演示：https://ericguo5513.github.io/momask/
论文：https://arxiv.org/abs/2312.00063
GitHub：https://github.com/EricGuo5513/momask-codes</title>
            <link>https://nitter.cz/xiaohuggg/status/1731601609933865214#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731601609933865214#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 09:09:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MoMask：根据文字描述来生成3D动画人物动作。<br />
<br />
它不仅能生成常见的动作（如走路、跑步），还能根据更具体的描述生成复杂的动作（如特定类型的舞蹈动作）。<br />
<br />
它的工作方式有点像是把人类的动作分解成一系列小块，每个小块代表一个特定的动作。<br />
<br />
然后，根据文字描述，它会选择合适的动作小块，组合起来。<br />
<br />
就像搭积木一样，最终形成一个完整的、流畅的动作序列。<br />
<br />
此外，它还能够根据需要“填补”动作序列中的空白部分，比如如果一个动作序列中间缺少一部分，MoMask能够智能地补全这部分，使整个动作看起来自然流畅。<br />
<br />
工作原理可以分为几个关键步骤：<br />
<br />
1、分层量化表示：首先，MoMask使用一种称为“分层量化”的技术来表示人类的动作。这意味着它把复杂的人类动作分解成多个层次的“动作标记”。每个标记都是动作的一个小部分，就像是用来描述动作的“字母”一样。<br />
<br />
2、向量量化：在基础层，MoMask通过一种叫做“向量量化”的过程获取一系列运动标记。这个过程类似于把连续的动作数据转换成一系列离散的标记，这些标记能够更精确地捕捉动作的细节。<br />
<br />
3、残差标记：在随后的层级中，MoMask生成所谓的“残差标记”，这些标记代表了从基础层动作标记中提取的更高阶的动作信息。<br />
<br />
4、双向变换器：MoMask使用两个不同的双向变换器来处理这些标记。第一个变换器（掩码变换器）用于预测在训练阶段根据文本输入随机掩盖的运动标记。在生成（即推理）阶段，从一个空序列开始，掩码变换器逐步填充缺失的标记。第二个变换器（残差变换器）学习基于当前层的结果来逐步预测下一层的标记。<br />
<br />
5、文本驱动的生成：在实际应用中，MoMask能够根据给定的文字描述生成相应的3D人类动作。例如，如果描述是“一个人在跳舞”，MoMask会生成一个符合这一描述的动作序列。<br />
<br />
6、除了直接根据文本生成动作，MoMask还可以用于其他相关任务，如“时间内插”，即在现有动作片段中填补特定区域，使其符合文本描述。<br />
<br />
项目及演示：<a href="https://ericguo5513.github.io/momask/">ericguo5513.github.io/momask…</a><br />
论文：<a href="https://arxiv.org/abs/2312.00063">arxiv.org/abs/2312.00063</a><br />
GitHub：<a href="https://github.com/EricGuo5513/momask-codes">github.com/EricGuo5513/momas…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE1OTk1Njc2ODUzOTg1MjgvcHUvaW1nL3JwRTJubFp6blBtM3hFQjUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731576291898638617#m</id>
            <title>这个早就有了
不就是那什么吗
这个我们国家也能做
xxx早就做了，就知道跪舔
很早之前我就做过一个这种类似的
这有啥难度，多少年前的技术了，还吹...</title>
            <link>https://nitter.cz/xiaohuggg/status/1731576291898638617#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731576291898638617#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 07:28:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个早就有了<br />
不就是那什么吗<br />
这个我们国家也能做<br />
xxx早就做了，就知道跪舔<br />
很早之前我就做过一个这种类似的<br />
这有啥难度，多少年前的技术了，还吹...</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731560293971767673#m</id>
            <title>GPT-4能处理和理解混乱文本 即时你输入错误的情况下

东京大学的研究人员测试了GPT-4在处理混乱文本方面的表现。

研究人员创建了一些字母顺序被混乱的句子，然后让GPT-4尝试恢复它们的原始顺序。

即使是在字母完全混乱的极端情况下，它也能几乎完美地恢复原始句子。

研究显示，GPT-4具有在一定程度上理解并纠正用户输入错误内容的能力。这包括但不限于：

字母顺序混乱：如上述例子所示，即使单词内的字母顺序被打乱，GPT-4也能够识别出原始的、正确的单词。

拼写错误：GPT-4能够识别并纠正常见的拼写错误，理解用户的真实意图。

语法错误：GPT-4还能在一定程度上理解并处理含有语法错误的句子。

不完整或含糊的输入：即使用户的输入不完整或含糊不清，GPT-4也能尝试根据上下文提供合理的回答或建议。

研究结果：

1、GPT-4处理混乱文本的表现：即使在单词内部的字母完全混乱的情况下，GPT-4也能够准确地识别和重组这些字母，恢复成正确的单词和句子。例如，如果输入的是“eTh cta sat no eht amt”，GPT-4能够识别出正确的句子应该是“The cat sat on the mat”。

2、编辑距离减少95%：编辑距离是一种衡量两个字符串之间差异的方法，通常通过计算将一个字符串转换成另一个字符串所需的最少单字符编辑（插入、删除、替换）次数。在这项研究中，GPT-4能够将混乱的句子恢复到接近原始句子的状态，使得编辑距离减少了95%。这意味着GPT-4非常有效地纠正了混乱的文本。

3、问题回答能力：即使在给定的上下文是混乱的情况下，GPT-4仍然能够理解问题的本质并提供准确的答案。这表明GPT-4不仅能处理和纠正混乱的文本，还能在这种情况下理解和回应复杂的查询。

4、与其他模型的对比：GPT-4在处理极端混乱文本的能力更加突出。

论文：https://arxiv.org/abs/2311.18805</title>
            <link>https://nitter.cz/xiaohuggg/status/1731560293971767673#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731560293971767673#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 06:25:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT-4能处理和理解混乱文本 即时你输入错误的情况下<br />
<br />
东京大学的研究人员测试了GPT-4在处理混乱文本方面的表现。<br />
<br />
研究人员创建了一些字母顺序被混乱的句子，然后让GPT-4尝试恢复它们的原始顺序。<br />
<br />
即使是在字母完全混乱的极端情况下，它也能几乎完美地恢复原始句子。<br />
<br />
研究显示，GPT-4具有在一定程度上理解并纠正用户输入错误内容的能力。这包括但不限于：<br />
<br />
字母顺序混乱：如上述例子所示，即使单词内的字母顺序被打乱，GPT-4也能够识别出原始的、正确的单词。<br />
<br />
拼写错误：GPT-4能够识别并纠正常见的拼写错误，理解用户的真实意图。<br />
<br />
语法错误：GPT-4还能在一定程度上理解并处理含有语法错误的句子。<br />
<br />
不完整或含糊的输入：即使用户的输入不完整或含糊不清，GPT-4也能尝试根据上下文提供合理的回答或建议。<br />
<br />
研究结果：<br />
<br />
1、GPT-4处理混乱文本的表现：即使在单词内部的字母完全混乱的情况下，GPT-4也能够准确地识别和重组这些字母，恢复成正确的单词和句子。例如，如果输入的是“eTh cta sat no eht amt”，GPT-4能够识别出正确的句子应该是“The cat sat on the mat”。<br />
<br />
2、编辑距离减少95%：编辑距离是一种衡量两个字符串之间差异的方法，通常通过计算将一个字符串转换成另一个字符串所需的最少单字符编辑（插入、删除、替换）次数。在这项研究中，GPT-4能够将混乱的句子恢复到接近原始句子的状态，使得编辑距离减少了95%。这意味着GPT-4非常有效地纠正了混乱的文本。<br />
<br />
3、问题回答能力：即使在给定的上下文是混乱的情况下，GPT-4仍然能够理解问题的本质并提供准确的答案。这表明GPT-4不仅能处理和纠正混乱的文本，还能在这种情况下理解和回应复杂的查询。<br />
<br />
4、与其他模型的对比：GPT-4在处理极端混乱文本的能力更加突出。<br />
<br />
论文：<a href="https://arxiv.org/abs/2311.18805">arxiv.org/abs/2311.18805</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FlNzY3UWJ3QUEzUVI5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731537268421353754#m</id>
            <title>R to @xiaohuggg: 二维码实际上是用不可见的激光照射的。

但是通过智能手机可以看到激光的痕迹（红点），肉眼是无法察觉的。

视频演示为通过智能手机实际读取波长为785nm 照射的二维码（按 10m、50m 和 100m 的顺序）的结果。</title>
            <link>https://nitter.cz/xiaohuggg/status/1731537268421353754#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731537268421353754#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 04:53:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>二维码实际上是用不可见的激光照射的。<br />
<br />
但是通过智能手机可以看到激光的痕迹（红点），肉眼是无法察觉的。<br />
<br />
视频演示为通过智能手机实际读取波长为785nm 照射的二维码（按 10m、50m 和 100m 的顺序）的结果。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE1MzY3NzMzNDY2NzY3MzcvcHUvaW1nL19pbzBXMXN2U2t4UHV5SF8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731536662810026348#m</id>
            <title>超远距离 二维码伪造替换攻击技术

日本东海大学的研究人员开发出一种技术能够从最远100米的距离使用不可见光激光照射QR码，然后将其替换为伪造的QR码。😂

这种攻击可以在任意时间点对QR码进行照射，实现实时动态攻击。

非常隐蔽几乎无法防范！

研究者计划进行更长距离（如1公里）的实验。

实验结果：

不同距离的效果：在10米、20米、30米、40米的距离上，URL2（恶意网站）被成功读取。但在50米和100米的距离上，URL1（正常网站）和URL2交替出现。

攻击原理：

1、不可见光激光照射：攻击者使用不可见光激光（如红外线或其他波长的激光）直接照射到QR码上。

2、修改QR码信息：激光照射改变了QR码的部分像素，从而改变了QR码编码的信息。这种改变对肉眼几乎不可见，但可以被扫描设备识别。

3、引导至恶意网站：修改后的QR码可以将用户引导至攻击者指定的恶意网站或其他目的地，而不是原本的合法网站。

攻击特点：

1、隐蔽性：由于使用的是不可见光激光，攻击对普通用户来说几乎是不可察觉的。即使QR码被修改，肉眼也难以分辨出任何变化。

2、远距离操作：这种攻击可以从远距离（如100米甚至更远）进行，增加了攻击的隐蔽性和灵活性。

3、动态操作：攻击者可以在任意时间点对QR码进行照射，实现动态的攻击方式。

4、影响因素：实验室环境中的空调导致空气流动，使激光位置发生2-3毫米的变动，从而影响了实验结果。空气波动可能会影响效果。

5、难以防范：由于攻击的隐蔽性，普通用户和设备难以识别和防范这种攻击。

研究者计划在更长距离（如1公里）进行实验，这将要求提高激光照射的精度。

论文：https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=228707&amp;item_no=1&amp;page_id=13&amp;block_id=8</title>
            <link>https://nitter.cz/xiaohuggg/status/1731536662810026348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731536662810026348#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 04:51:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>超远距离 二维码伪造替换攻击技术<br />
<br />
日本东海大学的研究人员开发出一种技术能够从最远100米的距离使用不可见光激光照射QR码，然后将其替换为伪造的QR码。😂<br />
<br />
这种攻击可以在任意时间点对QR码进行照射，实现实时动态攻击。<br />
<br />
非常隐蔽几乎无法防范！<br />
<br />
研究者计划进行更长距离（如1公里）的实验。<br />
<br />
实验结果：<br />
<br />
不同距离的效果：在10米、20米、30米、40米的距离上，URL2（恶意网站）被成功读取。但在50米和100米的距离上，URL1（正常网站）和URL2交替出现。<br />
<br />
攻击原理：<br />
<br />
1、不可见光激光照射：攻击者使用不可见光激光（如红外线或其他波长的激光）直接照射到QR码上。<br />
<br />
2、修改QR码信息：激光照射改变了QR码的部分像素，从而改变了QR码编码的信息。这种改变对肉眼几乎不可见，但可以被扫描设备识别。<br />
<br />
3、引导至恶意网站：修改后的QR码可以将用户引导至攻击者指定的恶意网站或其他目的地，而不是原本的合法网站。<br />
<br />
攻击特点：<br />
<br />
1、隐蔽性：由于使用的是不可见光激光，攻击对普通用户来说几乎是不可察觉的。即使QR码被修改，肉眼也难以分辨出任何变化。<br />
<br />
2、远距离操作：这种攻击可以从远距离（如100米甚至更远）进行，增加了攻击的隐蔽性和灵活性。<br />
<br />
3、动态操作：攻击者可以在任意时间点对QR码进行照射，实现动态的攻击方式。<br />
<br />
4、影响因素：实验室环境中的空调导致空气流动，使激光位置发生2-3毫米的变动，从而影响了实验结果。空气波动可能会影响效果。<br />
<br />
5、难以防范：由于攻击的隐蔽性，普通用户和设备难以识别和防范这种攻击。<br />
<br />
研究者计划在更长距离（如1公里）进行实验，这将要求提高激光照射的精度。<br />
<br />
论文：<a href="https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=228707&amp;item_no=1&amp;page_id=13&amp;block_id=8">ipsj.ixsq.nii.ac.jp/ej/?acti…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FlbW5PVmFVQUEyZDBxLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FlbXBiWmJVQUFKR1UyLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731514782107459588#m</id>
            <title>R to @xiaohuggg: 不仅是提升分辨率

可以把像素风格转换成真人风格

不过我感觉是还是不太值得我花费一个月39美金啊</title>
            <link>https://nitter.cz/xiaohuggg/status/1731514782107459588#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731514782107459588#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 03:24:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>不仅是提升分辨率<br />
<br />
可以把像素风格转换成真人风格<br />
<br />
不过我感觉是还是不太值得我花费一个月39美金啊</p>
<p><a href="https://nitter.cz/javilopen/status/1730986775135305851#m">nitter.cz/javilopen/status/1730986775135305851#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731514779221848194#m</id>
            <title>这个 @Magnific_AI 一上来就收钱

一点白嫖和体验的机会都不给我😂

而且还很贵，一开始我以为就是图像分辨率提升工具

现在看来小看它了！</title>
            <link>https://nitter.cz/xiaohuggg/status/1731514779221848194#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731514779221848194#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 03:24:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个 <a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 一上来就收钱<br />
<br />
一点白嫖和体验的机会都不给我😂<br />
<br />
而且还很贵，一开始我以为就是图像分辨率提升工具<br />
<br />
现在看来小看它了！</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FlUXk0Q2FVQUFPVGMzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731506874586190053#m</id>
            <title>炸裂了💥

网络购物将因此发生改变😂

 ✅ 搜索任何网页
 ✅ 物理上抓取你想要的物品
 ✅ 在现实中中查看它到底是什么

看评论博主说是使用 Quest 3 的Figmin XR 应用程序的做的！混合现实增强体验！</title>
            <link>https://nitter.cz/xiaohuggg/status/1731506874586190053#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731506874586190053#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 02:53:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>炸裂了💥<br />
<br />
网络购物将因此发生改变😂<br />
<br />
 ✅ 搜索任何网页<br />
 ✅ 物理上抓取你想要的物品<br />
 ✅ 在现实中中查看它到底是什么<br />
<br />
看评论博主说是使用 Quest 3 的Figmin XR 应用程序的做的！混合现实增强体验！</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE0MjY1NzUxMzUyMzIwMDAvcHUvaW1nL0VGbXVwTnYtZ29rcUlzYUUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731499487326298540#m</id>
            <title>从12米远的地方偷拍玩手机的你，成功恢复你在手机上输入的内容👻

芝加哥大学的研究人员研究出一种新的攻击方法，使用不到60美元的望远镜头装在智能手机上，从建筑物内（窗户后面）拍摄大约12米远正在打字的受害者，成功恢复了被输入的内容。

该方法不需要预先训练、键盘知识、本地传感器或侧信道等！

攻击概述：

1、攻击性质：这是一种在公共场合可执行的基于视频的击键推断攻击。

2、使用设备：攻击者只需要一台普通的RGB摄像机，从正面拍摄目标的打字手指。

3、独特方法：与以往方法不同，这种攻击不依赖于侧信道数据或其他假设，只需目标的打字手的正面视图。它不需要预先训练、键盘知识、目标的训练数据、本地传感器或侧信道。

4、样本场景：室内休息室场景，攻击者在观看视频时记录受害者的打字动作。长距离户外场景，攻击者使用带有廉价望远镜头的智能手机，从大约12米远的地方拍摄庭院中打字的受害者。

5、多样化条件：攻击在不同条件下进行评估，包括不同的环境（室内/室外）、攻击距离、障碍物和键盘设备（可见/隐形键盘，不同大小/布局）。

6、用户研究：研究涉及16名不同的用户，他们具有不同的打字风格和能力。攻击在几乎所有场景中都显示出高效果，并在行为差异显著的参与者中表现良好。

主要技术原理：

1、视频分析

手部追踪：首先，利用视频分析技术追踪并分析目标人物的手指动作。

击键检测：通过分析手指的移动和位置变化，检测击键动作。

2、数据处理

自我教学系统：使用一个双层结构的自我教学系统来处理视频数据。这个系统包括两个主要部分：击键的检测和聚类：利用手部追踪的结果来检测击键动作，并将它们进行分类。

隐马尔可夫模型（HMM）：使用HMM来识别具体的击键动作。

3、推断输入内容

语言模型：结合语言模型来分析和推断击键序列，从而推测出被输入的内容。

3D-CNN模型：使用3D卷积神经网络（CNN）模型进一步处理数据，提高推断的准确性。

详细介绍：https://sandlab.cs.uchicago.edu/keystroke/
论文：http://people.cs.uchicago.edu/~ravenben/publications/pdf/keystroke-usenix23.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1731499487326298540#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731499487326298540#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 02:23:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>从12米远的地方偷拍玩手机的你，成功恢复你在手机上输入的内容👻<br />
<br />
芝加哥大学的研究人员研究出一种新的攻击方法，使用不到60美元的望远镜头装在智能手机上，从建筑物内（窗户后面）拍摄大约12米远正在打字的受害者，成功恢复了被输入的内容。<br />
<br />
该方法不需要预先训练、键盘知识、本地传感器或侧信道等！<br />
<br />
攻击概述：<br />
<br />
1、攻击性质：这是一种在公共场合可执行的基于视频的击键推断攻击。<br />
<br />
2、使用设备：攻击者只需要一台普通的RGB摄像机，从正面拍摄目标的打字手指。<br />
<br />
3、独特方法：与以往方法不同，这种攻击不依赖于侧信道数据或其他假设，只需目标的打字手的正面视图。它不需要预先训练、键盘知识、目标的训练数据、本地传感器或侧信道。<br />
<br />
4、样本场景：室内休息室场景，攻击者在观看视频时记录受害者的打字动作。长距离户外场景，攻击者使用带有廉价望远镜头的智能手机，从大约12米远的地方拍摄庭院中打字的受害者。<br />
<br />
5、多样化条件：攻击在不同条件下进行评估，包括不同的环境（室内/室外）、攻击距离、障碍物和键盘设备（可见/隐形键盘，不同大小/布局）。<br />
<br />
6、用户研究：研究涉及16名不同的用户，他们具有不同的打字风格和能力。攻击在几乎所有场景中都显示出高效果，并在行为差异显著的参与者中表现良好。<br />
<br />
主要技术原理：<br />
<br />
1、视频分析<br />
<br />
手部追踪：首先，利用视频分析技术追踪并分析目标人物的手指动作。<br />
<br />
击键检测：通过分析手指的移动和位置变化，检测击键动作。<br />
<br />
2、数据处理<br />
<br />
自我教学系统：使用一个双层结构的自我教学系统来处理视频数据。这个系统包括两个主要部分：击键的检测和聚类：利用手部追踪的结果来检测击键动作，并将它们进行分类。<br />
<br />
隐马尔可夫模型（HMM）：使用HMM来识别具体的击键动作。<br />
<br />
3、推断输入内容<br />
<br />
语言模型：结合语言模型来分析和推断击键序列，从而推测出被输入的内容。<br />
<br />
3D-CNN模型：使用3D卷积神经网络（CNN）模型进一步处理数据，提高推断的准确性。<br />
<br />
详细介绍：<a href="https://sandlab.cs.uchicago.edu/keystroke/">sandlab.cs.uchicago.edu/keys…</a><br />
论文：<a href="http://people.cs.uchicago.edu/~ravenben/publications/pdf/keystroke-usenix23.pdf">people.cs.uchicago.edu/~rave…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE0OTg4NTc3MjM1NjQwMzIvcHUvaW1nL25uT1FackVGTGszTDFrM0cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>