<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736711906969424022#m</id>
            <title>R to @xiaohuggg: 该项目汉化自ComfyUI Portrait Master，用于生成详细和个性化的人物肖像提示词。

可以设置人物的性别、国籍、眼睛颜色、发型等，还可以调整面部表情、脸型和肤色的细节。  

GitHub：https://github.com/florestefano1975/comfyui-portrait-master?tab=readme-ov-file</title>
            <link>https://nitter.cz/xiaohuggg/status/1736711906969424022#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736711906969424022#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 11:35:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>该项目汉化自ComfyUI Portrait Master，用于生成详细和个性化的人物肖像提示词。<br />
<br />
可以设置人物的性别、国籍、眼睛颜色、发型等，还可以调整面部表情、脸型和肤色的细节。  <br />
<br />
GitHub：<a href="https://github.com/florestefano1975/comfyui-portrait-master?tab=readme-ov-file">github.com/florestefano1975/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JvS1BlaGEwQUFua2k2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JvS1JGSWIwQUEtMnRrLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736711661476798972#m</id>
            <title>R to @xiaohuggg: 肖像大师中文版2.0，新增6项参数，扩充2项参数，更新并新增3种工作流：

眼睛颜色（8种）
头发颜色（9种）
灯光类型（32种）
灯光方向（10种）
提高照片真实感
负面提示词
镜头类型（+3种）
发型（+19种）
新增SAG+SVD视频工作流

  详细下载和工作流信息：
https://waytoagi.feishu.cn/wiki/HcPuw0W1IiljU8kMYPgcPNHznxh</title>
            <link>https://nitter.cz/xiaohuggg/status/1736711661476798972#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736711661476798972#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 11:35:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>肖像大师中文版2.0，新增6项参数，扩充2项参数，更新并新增3种工作流：<br />
<br />
眼睛颜色（8种）<br />
头发颜色（9种）<br />
灯光类型（32种）<br />
灯光方向（10种）<br />
提高照片真实感<br />
负面提示词<br />
镜头类型（+3种）<br />
发型（+19种）<br />
新增SAG+SVD视频工作流<br />
<br />
  详细下载和工作流信息：<br />
<a href="https://waytoagi.feishu.cn/wiki/HcPuw0W1IiljU8kMYPgcPNHznxh">waytoagi.feishu.cn/wiki/HcPu…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JvS0NLY2JVQUFJOEZ0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736710794589675763#m</id>
            <title>ComfyUI Portrait Master 肖像大师 简体中文版来啦！

超详细参数设置！再也不用为不会写人像提示词发愁啦！重新优化为json列表更方便自定义和扩展！已包含标准工作流和turbo工作流...

肖像大师中文版2.0 ：https://github.com/ZHO-ZHO-ZHO/comfyui-portrait-master-zh-cn</title>
            <link>https://nitter.cz/xiaohuggg/status/1736710794589675763#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736710794589675763#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 11:31:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI Portrait Master 肖像大师 简体中文版来啦！<br />
<br />
超详细参数设置！再也不用为不会写人像提示词发愁啦！重新优化为json列表更方便自定义和扩展！已包含标准工作流和turbo工作流...<br />
<br />
肖像大师中文版2.0 ：<a href="https://github.com/ZHO-ZHO-ZHO/comfyui-portrait-master-zh-cn">github.com/ZHO-ZHO-ZHO/comfy…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JvSFdUbWFBQUFWdm1zLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736675597206819308#m</id>
            <title>R to @xiaohuggg: 得益于Gaussian Splatting技术

Gaussian-SLAM可以实时渲染重建3D场景</title>
            <link>https://nitter.cz/xiaohuggg/status/1736675597206819308#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736675597206819308#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 09:11:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>得益于Gaussian Splatting技术<br />
<br />
Gaussian-SLAM可以实时渲染重建3D场景</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2NzQ5NjAwNDE3MzAwNDgvcHUvaW1nL0xvb3ZWTnJUaTBKNVRWRXcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736674788679311709#m</id>
            <title>Gaussian-SLAM：能够从视频流中重建出逼真的3D场景

通过观看一个视频，Gaussian-SLAM可以分析视频中的图像，能够理解视频中的环境布局和物体的位置。

然后利用这些图像数据来创建、还原可以从各个角度观察的3D模型，重建现实世界场景。

而是这个过程是实时渲染的...

举例解释：

想象一下，你有一个视频，这个视频是在一个公园里拍摄的，里面有树木、长椅、小路和人们。通常，视频只能提供二维的视角，你只能看到从摄像机角度拍摄的场景。

现在，使用Gaussian-SLAM技术，我们可以分析这个视频，识别出视频中的各个物体（如树木、长椅等），并了解它们在空间中的相对位置。Gaussian-SLAM通过分析视频中物体的移动和视角变化，计算出这些物体在三维空间中的位置和形状。

最终，这项技术可以创建一个三维模型，这个模型是公园的数字复制品。在这个三维模型中，你可以像在真实世界一样，从任何角度查看公园的每个角落。你可以看到树木的具体位置、长椅的样子，甚至是人们在公园中的活动。

这就像是把一个真实的场景转换成了一个可以在计算机上查看和探索的3D虚拟环境。

这种技术对于创建虚拟现实体验、视频游戏中的环境，或者帮助自动驾驶汽车更好地理解它们周围的世界非常有用。

Gaussian-SLAM的主要功能特点和工作原理如下：

主要功能特点：

1、光学真实的渲染：能够以高度真实的方式重建和渲染真实世界和合成场景。

2、高斯斑点场景表示：使用高斯斑点作为场景的主要表示单位，这是一种新颖的方法，与传统的点云或网格表示不同。

3、交互式时间重建：允许在交互时间内重建场景，即重建过程足够快，可以实时渲染或近实时进行。

4、适用于单目RGBD输入：针对单目RGBD（红绿蓝深度）输入数据进行优化，适用于多种场景。

Gaussian-SLAM特别针对的是RGBD摄像头的输入数据进行优化。

RGBD摄像头除了捕捉普通的彩色图像外，还能提供每个像素点的深度信息，即物体距离摄像头的距离。这种深度信息对于创建准确的三维场景模型至关重要。

工作原理

1、数据处理：接收RGBD关键帧输入，进行子采样并考虑颜色梯度。

2、3D高斯初始化：将采样点投影到3D空间，在这些采样位置初始化新的高斯。

3、场景构建：新的3D高斯被添加到全局地图的当前活动部分中，形成场景的一部分。

4、关键帧存储与渲染：输入的RGBD关键帧暂时存储，与对活动子图有贡献的其他关键帧一起。然后，渲染所有对活动子图有贡献的关键帧。

5、优化与更新：计算与子图输入关键帧相关的深度和颜色损失，然后更新活动子图中3D高斯的参数。

应用场景

Gaussian-SLAM适用于需要高度真实感和精确度的SLAM应用，如自动驾驶、机器人导航、增强现实和虚拟现实等。

项目及演示：https://vladimiryugay.github.io/gaussian_slam/
论文：https://ivi.fnwi.uva.nl/cv/paper/GaussianSLAM.pdf
GitHub：https://github.com/VladimirYugay/Gaussian-SLAM</title>
            <link>https://nitter.cz/xiaohuggg/status/1736674788679311709#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736674788679311709#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 09:08:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gaussian-SLAM：能够从视频流中重建出逼真的3D场景<br />
<br />
通过观看一个视频，Gaussian-SLAM可以分析视频中的图像，能够理解视频中的环境布局和物体的位置。<br />
<br />
然后利用这些图像数据来创建、还原可以从各个角度观察的3D模型，重建现实世界场景。<br />
<br />
而是这个过程是实时渲染的...<br />
<br />
举例解释：<br />
<br />
想象一下，你有一个视频，这个视频是在一个公园里拍摄的，里面有树木、长椅、小路和人们。通常，视频只能提供二维的视角，你只能看到从摄像机角度拍摄的场景。<br />
<br />
现在，使用Gaussian-SLAM技术，我们可以分析这个视频，识别出视频中的各个物体（如树木、长椅等），并了解它们在空间中的相对位置。Gaussian-SLAM通过分析视频中物体的移动和视角变化，计算出这些物体在三维空间中的位置和形状。<br />
<br />
最终，这项技术可以创建一个三维模型，这个模型是公园的数字复制品。在这个三维模型中，你可以像在真实世界一样，从任何角度查看公园的每个角落。你可以看到树木的具体位置、长椅的样子，甚至是人们在公园中的活动。<br />
<br />
这就像是把一个真实的场景转换成了一个可以在计算机上查看和探索的3D虚拟环境。<br />
<br />
这种技术对于创建虚拟现实体验、视频游戏中的环境，或者帮助自动驾驶汽车更好地理解它们周围的世界非常有用。<br />
<br />
Gaussian-SLAM的主要功能特点和工作原理如下：<br />
<br />
主要功能特点：<br />
<br />
1、光学真实的渲染：能够以高度真实的方式重建和渲染真实世界和合成场景。<br />
<br />
2、高斯斑点场景表示：使用高斯斑点作为场景的主要表示单位，这是一种新颖的方法，与传统的点云或网格表示不同。<br />
<br />
3、交互式时间重建：允许在交互时间内重建场景，即重建过程足够快，可以实时渲染或近实时进行。<br />
<br />
4、适用于单目RGBD输入：针对单目RGBD（红绿蓝深度）输入数据进行优化，适用于多种场景。<br />
<br />
Gaussian-SLAM特别针对的是RGBD摄像头的输入数据进行优化。<br />
<br />
RGBD摄像头除了捕捉普通的彩色图像外，还能提供每个像素点的深度信息，即物体距离摄像头的距离。这种深度信息对于创建准确的三维场景模型至关重要。<br />
<br />
工作原理<br />
<br />
1、数据处理：接收RGBD关键帧输入，进行子采样并考虑颜色梯度。<br />
<br />
2、3D高斯初始化：将采样点投影到3D空间，在这些采样位置初始化新的高斯。<br />
<br />
3、场景构建：新的3D高斯被添加到全局地图的当前活动部分中，形成场景的一部分。<br />
<br />
4、关键帧存储与渲染：输入的RGBD关键帧暂时存储，与对活动子图有贡献的其他关键帧一起。然后，渲染所有对活动子图有贡献的关键帧。<br />
<br />
5、优化与更新：计算与子图输入关键帧相关的深度和颜色损失，然后更新活动子图中3D高斯的参数。<br />
<br />
应用场景<br />
<br />
Gaussian-SLAM适用于需要高度真实感和精确度的SLAM应用，如自动驾驶、机器人导航、增强现实和虚拟现实等。<br />
<br />
项目及演示：<a href="https://vladimiryugay.github.io/gaussian_slam/">vladimiryugay.github.io/gaus…</a><br />
论文：<a href="https://ivi.fnwi.uva.nl/cv/paper/GaussianSLAM.pdf">ivi.fnwi.uva.nl/cv/paper/Gau…</a><br />
GitHub：<a href="https://github.com/VladimirYugay/Gaussian-SLAM">github.com/VladimirYugay/Gau…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2NzQ2OTg5MjExNTY2MDgvcHUvaW1nL1k4MXVsalhIdFB4RjFENTYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736628108646867073#m</id>
            <title>R to @xiaohuggg: 支持多种语言、语气、语调

😁</title>
            <link>https://nitter.cz/xiaohuggg/status/1736628108646867073#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736628108646867073#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 06:03:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持多种语言、语气、语调<br />
<br />
😁</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2Mjc5NjM4MTM0MDg3NjgvcHUvaW1nL3Jwemo1dlJ2aGxwR2drUFUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736627871832269305#m</id>
            <title>R to @xiaohuggg: DreamTalk不仅能够处理和生成它在训练过程中见过的面部类型和表情，还能有效处理和生成它之前未见过的、来自不同数据集的面部类型和表情。

包括不同种族、年龄、性别的人物肖像，以及各种不同的表情和情绪。</title>
            <link>https://nitter.cz/xiaohuggg/status/1736627871832269305#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736627871832269305#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 06:02:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTalk不仅能够处理和生成它在训练过程中见过的面部类型和表情，还能有效处理和生成它之前未见过的、来自不同数据集的面部类型和表情。<br />
<br />
包括不同种族、年龄、性别的人物肖像，以及各种不同的表情和情绪。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2Mjc2MDk3Mzg2MjkxMjAvcHUvaW1nL1BtVUlFR1pUaU1wLW51QnkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736627340623692177#m</id>
            <title>DreamTalk：由清华大学、阿里巴巴和华中科大共同开发的一个基于扩散模型让人物头像说话的框架。

能够根据音频让人物头像照片说话、唱歌并保持嘴唇的同步和模仿表情变化。

- 高质量动画：能够生成非常真实的人物脸部动作。

- 多样化表情：不仅嘴唇动作逼真，还能展现丰富的表情。

- 支持多种语言：无论是中文、英文还是其他语言，都能很好地同步。

- 说话风格预测：能够根据语音预测说话者的风格，并同步表情。

- 适用多种场景：可以用于歌曲、不同类型的肖像，甚至在嘈杂环境中也能表现良好。

工作原理：

该项目在利用扩散模型在生成动态和表情丰富的说话头部方面取得突破。

结合了以下几个关键组件来生成表情丰富的说话头部动画：

1、去噪网络：这是核心组件之一，负责生成音频驱动的面部动作。去噪网络使用扩散模型来逐步去除噪声，从而生成清晰、高质量的面部表情。这个过程涉及从带有噪声的数据中逐步恢复出清晰的面部动作。

2、风格感知的嘴唇专家：这个组件专注于提高嘴唇动作的表现力和准确性。它通过分析说话风格来引导嘴唇同步，确保生成的动画既自然又符合说话者的风格。

3、风格预测器：为了消除对表情参考视频或文本的依赖，DreamTalk引入了一个基于扩散的风格预测器。这个预测器可以直接从音频预测目标表情，无需额外的表情参考视频或文本。

4、音频和视频处理：处理音频输入，提取关键的音频特征，并将这些特征用于驱动面部动画。同时，它还能处理视频输入，以提取和模仿特定的表情和风格。

5、数据和模型训练：为了实现这些功能，DreamTalk需要大量的数据来训练其模型，包括不同表情和说话风格的面部动画数据。通过这些数据，模型学习如何准确地生成与输入音频匹配的面部动作。

项目及演示：https://dreamtalk-project.github.io/
论文：https://arxiv.org/abs/2312.09767
GitHub：https://github.com/damo-vilab/i2vgen-xl</title>
            <link>https://nitter.cz/xiaohuggg/status/1736627340623692177#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736627340623692177#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 05:59:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTalk：由清华大学、阿里巴巴和华中科大共同开发的一个基于扩散模型让人物头像说话的框架。<br />
<br />
能够根据音频让人物头像照片说话、唱歌并保持嘴唇的同步和模仿表情变化。<br />
<br />
- 高质量动画：能够生成非常真实的人物脸部动作。<br />
<br />
- 多样化表情：不仅嘴唇动作逼真，还能展现丰富的表情。<br />
<br />
- 支持多种语言：无论是中文、英文还是其他语言，都能很好地同步。<br />
<br />
- 说话风格预测：能够根据语音预测说话者的风格，并同步表情。<br />
<br />
- 适用多种场景：可以用于歌曲、不同类型的肖像，甚至在嘈杂环境中也能表现良好。<br />
<br />
工作原理：<br />
<br />
该项目在利用扩散模型在生成动态和表情丰富的说话头部方面取得突破。<br />
<br />
结合了以下几个关键组件来生成表情丰富的说话头部动画：<br />
<br />
1、去噪网络：这是核心组件之一，负责生成音频驱动的面部动作。去噪网络使用扩散模型来逐步去除噪声，从而生成清晰、高质量的面部表情。这个过程涉及从带有噪声的数据中逐步恢复出清晰的面部动作。<br />
<br />
2、风格感知的嘴唇专家：这个组件专注于提高嘴唇动作的表现力和准确性。它通过分析说话风格来引导嘴唇同步，确保生成的动画既自然又符合说话者的风格。<br />
<br />
3、风格预测器：为了消除对表情参考视频或文本的依赖，DreamTalk引入了一个基于扩散的风格预测器。这个预测器可以直接从音频预测目标表情，无需额外的表情参考视频或文本。<br />
<br />
4、音频和视频处理：处理音频输入，提取关键的音频特征，并将这些特征用于驱动面部动画。同时，它还能处理视频输入，以提取和模仿特定的表情和风格。<br />
<br />
5、数据和模型训练：为了实现这些功能，DreamTalk需要大量的数据来训练其模型，包括不同表情和说话风格的面部动画数据。通过这些数据，模型学习如何准确地生成与输入音频匹配的面部动作。<br />
<br />
项目及演示：<a href="https://dreamtalk-project.github.io/">dreamtalk-project.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.09767">arxiv.org/abs/2312.09767</a><br />
GitHub：<a href="https://github.com/damo-vilab/i2vgen-xl">github.com/damo-vilab/i2vgen…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY2MjI0ODkxNjAzNDM1NTIvcHUvaW1nL1JJVFBrd0htRVI0STJjdkcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736617372298170477#m</id>
            <title>想体验的可以下载这个APP：https://apps.apple.com/gb/app/mlc-chat/id6448482937

或者使用这个部署，支持各种系统，能在各种设备上开发、优化和部署AI模型。包括iOS和安卓：https://github.com/mlc-ai/mlc-llm</title>
            <link>https://nitter.cz/xiaohuggg/status/1736617372298170477#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736617372298170477#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 05:20:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>想体验的可以下载这个APP：<a href="https://apps.apple.com/gb/app/mlc-chat/id6448482937">apps.apple.com/gb/app/mlc-ch…</a><br />
<br />
或者使用这个部署，支持各种系统，能在各种设备上开发、优化和部署AI模型。包括iOS和安卓：<a href="https://github.com/mlc-ai/mlc-llm">github.com/mlc-ai/mlc-llm</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1736380733776945325#m">nitter.cz/xiaohuggg/status/1736380733776945325#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNjE5MjU5ODk3OTc4ODgwMC9kQ09FeVNQOT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597477498015785#m</id>
            <title>R to @xiaohuggg: 熟练展示

很6...

参与独特的展示活动，甚至可以直接被NBA发现。选择加入NBA Global Scout的参与者有资格获得NBA选拔赛、训练营、比赛和展示会的邀请。</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597477498015785#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597477498015785#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>熟练展示<br />
<br />
很6...<br />
<br />
参与独特的展示活动，甚至可以直接被NBA发现。选择加入NBA Global Scout的参与者有资格获得NBA选拔赛、训练营、比赛和展示会的邀请。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTY5MjA4NzE5MDMyMzIvcHUvaW1nL29CY1V6RHdrSkFKVEJoeUQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597475367244143#m</id>
            <title>R to @xiaohuggg: 足球训练似乎也能用得到😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597475367244143#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597475367244143#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>足球训练似乎也能用得到😄</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTYzNjgzODk4MDQwMzIvcHUvaW1nL1JyVTFWY29ld3Z1RTg3Q1kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597473639137345#m</id>
            <title>R to @xiaohuggg: 也可以用来计分</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597473639137345#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597473639137345#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>也可以用来计分</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTYyOTM0NzUyMzc4ODgvcHUvaW1nL19RNDN2TWpkMkdQNzlRRWkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597471466590525#m</id>
            <title>R to @xiaohuggg: 练习模式</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597471466590525#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597471466590525#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>练习模式</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTYyNDc2MTg5NTczMTIvcHUvaW1nLzZRVzdyalpwWi0za1Zyd3IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597469725933878#m</id>
            <title>R to @xiaohuggg: 通过AR技术，训练就像玩视频游戏一样

包括虚拟目标、音频提示、奖励级别、积分和徽章

也可以与朋友、家人和队友进行虚拟竞争,创建虚拟团队发起挑战...</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597469725933878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597469725933878#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过AR技术，训练就像玩视频游戏一样<br />
<br />
包括虚拟目标、音频提示、奖励级别、积分和徽章<br />
<br />
也可以与朋友、家人和队友进行虚拟竞争,创建虚拟团队发起挑战...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTQ0NTM5MDgwNTgxMTIvcHUvaW1nL0FCX2ZDN3NYUjRfWUtFM2YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736597467934958027#m</id>
            <title>HomeCourt：一款利用AR来让篮球训练更有趣的应用

HomeCourt是NBA的官方合作伙伴，它利用增强现实（AR）技术实时捕捉你的动作，并智能分析来帮助你提高篮球训练技能。🏀

它通过手机或平板电脑的摄像头追踪您的运动，记录您的表现和统计数据。

还提供了各种互动训练和挑战游戏提升你训练的乐趣。

🧵</title>
            <link>https://nitter.cz/xiaohuggg/status/1736597467934958027#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736597467934958027#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 04:01:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HomeCourt：一款利用AR来让篮球训练更有趣的应用<br />
<br />
HomeCourt是NBA的官方合作伙伴，它利用增强现实（AR）技术实时捕捉你的动作，并智能分析来帮助你提高篮球训练技能。🏀<br />
<br />
它通过手机或平板电脑的摄像头追踪您的运动，记录您的表现和统计数据。<br />
<br />
还提供了各种互动训练和挑战游戏提升你训练的乐趣。<br />
<br />
🧵</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1OTQxNTQ4MDM5MjQ5OTIvcHUvaW1nL1QxSlFSY3gxTlJjT0tfSV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736588859633434845#m</id>
            <title>R to @xiaohuggg: 之前发布会时候的演示：

https://x.com/xiaohuggg/status/1707262391963271297</title>
            <link>https://nitter.cz/xiaohuggg/status/1736588859633434845#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736588859633434845#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 03:27:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前发布会时候的演示：<br />
<br />
<a href="https://x.com/xiaohuggg/status/1707262391963271297">x.com/xiaohuggg/status/17072…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1707262391963271297#m">nitter.cz/xiaohuggg/status/1707262391963271297#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736588857087574481#m</id>
            <title>Meta 雷朋智能眼镜在 TikTok 上掀起了一股潮流。

这玩意感觉确实能火啊，戴着眼镜可以第一视角现场直播你看到的一切...👓

AI智能语音助手，超广角1200万像素摄像头，1080p视频，能摄像能拍照，还能直播。售价299美金。

现在已经在美国、加拿大、英国开售！</title>
            <link>https://nitter.cz/xiaohuggg/status/1736588857087574481#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736588857087574481#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 03:27:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Meta 雷朋智能眼镜在 TikTok 上掀起了一股潮流。<br />
<br />
这玩意感觉确实能火啊，戴着眼镜可以第一视角现场直播你看到的一切...👓<br />
<br />
AI智能语音助手，超广角1200万像素摄像头，1080p视频，能摄像能拍照，还能直播。售价299美金。<br />
<br />
现在已经在美国、加拿大、英国开售！</p>
<p><a href="https://nitter.cz/julesterpak/status/1736175363804279217#m">nitter.cz/julesterpak/status/1736175363804279217#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1736571242445152404#m</id>
            <title>OpenAI发布官方提示工程指南和示例

OpenAI提供了一系列策略和技巧，以帮助用户更有效地使用ChatGPT。

这些方法可以单独使用也可以组合使用，以获得更好的效果。

官方给出了6 个大提示策略（并给出了具体教程和示例）

主要策略：

1、清晰的指令：告诉AI你具体想要什么。比如，如果你想要简短的答案，就直接说“给我一个简短的回答”。这样AI就不用猜你的意图了。

模型无法读懂你的思维。如果需要简短的回答，就明确要求；如果需要专家级的写作，也要明确指出。提供清晰的指令，减少模型猜测的需要。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions

具体操作：
• 在查询中包含细节，以获得更相关的答案。
• 要求模型采用特定的角色或风格。
• 使用分隔符明确指出输入的不同部分。
• 明确指定完成任务所需的步骤。
• 提供示例以帮助模型理解任务。
• 指定输出的期望长度。

2、提供参考文本：如果你有关于你要写的主题的具体资料或例子，给AI看看。这样它就能提供更准确、更相关的内容。

语言模型可能会创造虚假答案，尤其是在询问特定主题或要求引用和URL时。提供参考文本可以帮助模型提供更准确的答案。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text

具体操作：
• 指导模型使用参考文本来回答问题。
• 要求模型在回答时引用参考文本中的内容。

3、将复杂任务分解为简单子任务：如果你有一个复杂的主题要写，试着把它分成几个小部分。比如，先写一个关于主题背景的部分，然后再写关于主要观点的部分。

就像软件工程中将复杂系统分解为模块化组件一样，将任务提交给语言模型时也应采取类似的做法。复杂任务的错误率通常高于简单任务。复杂任务通常可以重新定义为一系列简单任务的工作流程。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query

具体操作：
• 使用意图分类来识别用户查询的最相关指令。
• 对于需要长时间对话的应用，总结或过滤之前的对话。
• 分段总结长文档，并递归地构建完整摘要。

4、给模型时间“思考”：有时候，让AI先“思考”一下，然后再回答问题，可以得到更好的答案。就像让它先列出解决问题的步骤，然后再给出答案。

模型在立即回答问题时可能会犯更多的推理错误。要求模型在给出答案之前进行“思考链”可以帮助模型更可靠地推理出正确答案。

示例：https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion

具体操作：
• 指导模型在急于得出结论之前先自行找出解决方案。
• 使用内部独白或一系列查询来隐藏模型的推理过程。
• 在之前的回答中询问模型是否遗漏了什么。

5、使用外部工具：有时候结合使用AI和其他工具（比如数据搜索工具）可以得到更好的结果。

利用其他工具的输出来补偿模型的不足。例如，文本检索系统可以向模型提供相关文档信息，代码执行引擎可以帮助模型进行数学计算和运行代码。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools

具体操作：
• 使用基于嵌入的搜索来实现高效的知识检索。
• 使用代码执行来进行更准确的计算或调用外部API。
• 让模型访问特定的功能。

6、测试和调整：尝试不同的指令和方法，看看哪种效果最好，然后根据结果进行调整。

示例：https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically

使用黄金标准答案评估模型输出”是一种有效的方法，用于确保AI模型的回答质量。

• 定义黄金标准答案：首先，确定一个问题的正确答案应该包含哪些已知事实。这些事实构成了评估AI回答的标准。

• 模型查询与事实对比：使用模型查询来生成答案，然后检查这个答案中包含了多少个所需的事实。

• 评估答案的完整性：根据答案中包含的事实数量来评估其完整性和准确性。如果一个答案包含了所有或大部分所需事实，那么可以认为这个答案是高质量的。

这种策略特别适用于需要精确和详细信息的场景，例如科学、技术或学术研究。通过与黄金标准答案的对比，可以有效地监控和提高AI模型的输出质量。

Prompt engineering 及时工程策略：https://platform.openai.com/docs/guides/prompt-engineering

Prompt examples 提示示例：https://platform.openai.com/examples

Prompting libraries &amp; tools 提示库和工具：https://cookbook.openai.com/related_resources#prompting-libraries--tools

Papers on advanced prompting to improve reasoning
关于高级提示以提高推理能力的论文：https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning

OpenAI Cookbook：https://cookbook.openai.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1736571242445152404#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1736571242445152404#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 18 Dec 2023 02:17:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI发布官方提示工程指南和示例<br />
<br />
OpenAI提供了一系列策略和技巧，以帮助用户更有效地使用ChatGPT。<br />
<br />
这些方法可以单独使用也可以组合使用，以获得更好的效果。<br />
<br />
官方给出了6 个大提示策略（并给出了具体教程和示例）<br />
<br />
主要策略：<br />
<br />
1、清晰的指令：告诉AI你具体想要什么。比如，如果你想要简短的答案，就直接说“给我一个简短的回答”。这样AI就不用猜你的意图了。<br />
<br />
模型无法读懂你的思维。如果需要简短的回答，就明确要求；如果需要专家级的写作，也要明确指出。提供清晰的指令，减少模型猜测的需要。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 在查询中包含细节，以获得更相关的答案。<br />
• 要求模型采用特定的角色或风格。<br />
• 使用分隔符明确指出输入的不同部分。<br />
• 明确指定完成任务所需的步骤。<br />
• 提供示例以帮助模型理解任务。<br />
• 指定输出的期望长度。<br />
<br />
2、提供参考文本：如果你有关于你要写的主题的具体资料或例子，给AI看看。这样它就能提供更准确、更相关的内容。<br />
<br />
语言模型可能会创造虚假答案，尤其是在询问特定主题或要求引用和URL时。提供参考文本可以帮助模型提供更准确的答案。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-answer-using-a-reference-text">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 指导模型使用参考文本来回答问题。<br />
• 要求模型在回答时引用参考文本中的内容。<br />
<br />
3、将复杂任务分解为简单子任务：如果你有一个复杂的主题要写，试着把它分成几个小部分。比如，先写一个关于主题背景的部分，然后再写关于主要观点的部分。<br />
<br />
就像软件工程中将复杂系统分解为模块化组件一样，将任务提交给语言模型时也应采取类似的做法。复杂任务的错误率通常高于简单任务。复杂任务通常可以重新定义为一系列简单任务的工作流程。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-intent-classification-to-identify-the-most-relevant-instructions-for-a-user-query">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 使用意图分类来识别用户查询的最相关指令。<br />
• 对于需要长时间对话的应用，总结或过滤之前的对话。<br />
• 分段总结长文档，并递归地构建完整摘要。<br />
<br />
4、给模型时间“思考”：有时候，让AI先“思考”一下，然后再回答问题，可以得到更好的答案。就像让它先列出解决问题的步骤，然后再给出答案。<br />
<br />
模型在立即回答问题时可能会犯更多的推理错误。要求模型在给出答案之前进行“思考链”可以帮助模型更可靠地推理出正确答案。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/tactic-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 指导模型在急于得出结论之前先自行找出解决方案。<br />
• 使用内部独白或一系列查询来隐藏模型的推理过程。<br />
• 在之前的回答中询问模型是否遗漏了什么。<br />
<br />
5、使用外部工具：有时候结合使用AI和其他工具（比如数据搜索工具）可以得到更好的结果。<br />
<br />
利用其他工具的输出来补偿模型的不足。例如，文本检索系统可以向模型提供相关文档信息，代码执行引擎可以帮助模型进行数学计算和运行代码。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-use-external-tools">platform.openai.com/docs/gui…</a><br />
<br />
具体操作：<br />
• 使用基于嵌入的搜索来实现高效的知识检索。<br />
• 使用代码执行来进行更准确的计算或调用外部API。<br />
• 让模型访问特定的功能。<br />
<br />
6、测试和调整：尝试不同的指令和方法，看看哪种效果最好，然后根据结果进行调整。<br />
<br />
示例：<a href="https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically">platform.openai.com/docs/gui…</a><br />
<br />
使用黄金标准答案评估模型输出”是一种有效的方法，用于确保AI模型的回答质量。<br />
<br />
• 定义黄金标准答案：首先，确定一个问题的正确答案应该包含哪些已知事实。这些事实构成了评估AI回答的标准。<br />
<br />
• 模型查询与事实对比：使用模型查询来生成答案，然后检查这个答案中包含了多少个所需的事实。<br />
<br />
• 评估答案的完整性：根据答案中包含的事实数量来评估其完整性和准确性。如果一个答案包含了所有或大部分所需事实，那么可以认为这个答案是高质量的。<br />
<br />
这种策略特别适用于需要精确和详细信息的场景，例如科学、技术或学术研究。通过与黄金标准答案的对比，可以有效地监控和提高AI模型的输出质量。<br />
<br />
Prompt engineering 及时工程策略：<a href="https://platform.openai.com/docs/guides/prompt-engineering">platform.openai.com/docs/gui…</a><br />
<br />
Prompt examples 提示示例：<a href="https://platform.openai.com/examples">platform.openai.com/examples</a><br />
<br />
Prompting libraries & tools 提示库和工具：<a href="https://cookbook.openai.com/related_resources#prompting-libraries--tools">cookbook.openai.com/related_…</a><br />
<br />
Papers on advanced prompting to improve reasoning<br />
关于高级提示以提高推理能力的论文：<a href="https://cookbook.openai.com/related_resources#papers-on-advanced-prompting-to-improve-reasoning">cookbook.openai.com/related_…</a><br />
<br />
OpenAI Cookbook：<a href="https://cookbook.openai.com/">cookbook.openai.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzY1NzEwMDAzNjI1NDEwNTYvcHUvaW1nL054SVRwbW5PVFgwZThSQXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>