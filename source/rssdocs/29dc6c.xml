<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</id>
            <title>R to @xiaohuggg: 所以这种片子以后就统称为：A片</title>
            <link>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729102242429702348#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:37:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以这种片子以后就统称为：A片</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</id>
            <title>全球首位由AI生成的AV女演员木花愛（木花あい）出道

由日本一家专精于成人动作片的公司h.m.p推出。

木花愛：身高165厘米，三围为88G/55/85。

木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。

链接就不放了😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729099853698093128#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:28:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>全球首位由AI生成的AV女演员木花愛（木花あい）出道<br />
<br />
由日本一家专精于成人动作片的公司h.m.p推出。<br />
<br />
木花愛：身高165厘米，三围为88G/55/85。<br />
<br />
木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。<br />
<br />
链接就不放了😅</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83OTVWNGJRQUEwRkZFLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83LU1sYmJFQUFyd1VXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</id>
            <title>DiffusionMat：一个先进的高质量视频抠图框架

DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。

DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：

1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。

2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。

3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。

4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。

5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。

项目及演示：https://cnnlstm.github.io/DiffusionMat
论文：https://arxiv.org/pdf/2311.13535.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729091603900559731#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 10:55:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DiffusionMat：一个先进的高质量视频抠图框架<br />
<br />
DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。<br />
<br />
DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：<br />
<br />
1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。<br />
<br />
2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。<br />
<br />
3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。<br />
<br />
4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。<br />
<br />
5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。<br />
<br />
项目及演示：<a href="https://cnnlstm.github.io/DiffusionMat">cnnlstm.github.io/DiffusionM…</a><br />
论文：<a href="https://arxiv.org/pdf/2311.13535.pdf">arxiv.org/pdf/2311.13535.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMzk4MDI3NTMwMDc2MTYvcHUvaW1nL0VmTkUtUTZYQXlYNEVkejguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</id>
            <title>Comfy Workflows：ComfyUI工作流分享站

收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。

省去了很多麻烦🫡

而且你也可以自己分享自己的工作流。

网站还支持在线运行工作流，不过要花点钱🥱

🔗：http://ComfyWorkflows.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729063470124183735#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 09:03:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfy Workflows：ComfyUI工作流分享站<br />
<br />
收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。<br />
<br />
省去了很多麻烦🫡<br />
<br />
而且你也可以自己分享自己的工作流。<br />
<br />
网站还支持在线运行工作流，不过要花点钱🥱<br />
<br />
🔗：<a href="http://ComfyWorkflows.com">ComfyWorkflows.com</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwNjMxNjk4NDk3NzgxNzYvcHUvaW1nL2VtTnJlTGNqeXlNQUlBV1ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729037971578626302#m</id>
            <title>What Q* could be？

Q*可能是一种结合了多种强化学习和搜索技术的方法，用于提高语言模型在复杂任务中的性能和推理能力。

通过结合自我对弈、前瞻性规划、思维树推理和过程奖励模型利用合成数据自我训练，Q*可能是一种具备自我思考能力、能够自我进化和学习的高级人工智能。

ML 科学家（RL、RLHF、社会、机器人）@natolambert 内森·兰伯特撰文描述了他对Q* 的一些看法

了对Q*可能的工作原理进行了深入分析：

通俗易懂解释就是：

1、结合两种策略：Q*是把两种策略混合在一起。一种是像玩国际象棋时预测对手下一步会怎么走（这叫Q学习），另一种是像谷歌地图一样找到从一个地方到另一个地方的最佳路线（这叫A搜索）。

2、思考多种可能：Q*会像一个思考多种可能性的人一样，考虑不同的解决方案，然后选择最好的那个。就像你在解决一个难题时，会想到很多不同的答案，然后挑选最合适的一个。

3、自己和自己比赛：Q*还会像一个棋手那样，和自己下棋来提高自己的技能。它通过不断地和自己的不同版本比赛，学习如何做出更好的决策。

4、每一步都打分：在解决问题的过程中，Q*会给每一步都打分，这样就能知道哪些步骤是好的，哪些不是。就像你在做选择题时，对每个选项进行评估，然后选择最有可能正确的那个。

5、使用合成数据训练：Q*使用大量的虚拟数据来训练自己，这样就不需要真实世界的数据那么多。这就像是通过模拟考试来准备真正的考试。

6、学习如何更好地解决问题：最后，Q*会通过一种叫做离线强化学习的方法来提高自己的能力。这就像是通过回顾过去的经验来学习如何在未来做得更好。

专业解释：

1、结合Q学习和A*搜索：Q*可能是Q学习（一种强化学习算法）和A*搜索（一种图搜索算法）的结合。这个方法可能涉及通过“思维树”（tree-of-thoughts）在语言/推理步骤上进行搜索。

2、思维树推理（Tree-of-Thoughts Reasoning）：Q*可能利用所谓的“思维树”来进行推理。这种方法通过提示语言模型创建多个推理路径，这些路径可能会或不会在正确答案处汇合。这种方法类似于递归式的提示技术，用于提高推理性能。

3、自我对弈和前瞻性规划：Q*可能结合了自我对弈的概念，即通过与自己的不同版本对弈来提高性能，以及前瞻性规划，即使用模型预测未来以产生更好的行动或输出。

4、过程奖励模型（Process Reward Models, PRMs）：Q*可能使用PRMs来为每个推理步骤打分，而不是整个回答。这允许在推理问题上进行更精细的生成和优化。

5、合成数据的使用：Q*可能大量使用合成数据来训练和优化模型。合成数据的使用可以减少对人类评分者的依赖，提高数据生成的效率和多样性。

6、离线强化学习的应用：Q*可能通过离线强化学习进行优化，这与现有的RLHF（强化学习人类反馈）工具类似，但采用了多步骤的方法。

详细内容：https://www.interconnects.ai/p/q-star</title>
            <link>https://nitter.cz/xiaohuggg/status/1729037971578626302#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729037971578626302#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 07:22:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>What Q* could be？<br />
<br />
Q*可能是一种结合了多种强化学习和搜索技术的方法，用于提高语言模型在复杂任务中的性能和推理能力。<br />
<br />
通过结合自我对弈、前瞻性规划、思维树推理和过程奖励模型利用合成数据自我训练，Q*可能是一种具备自我思考能力、能够自我进化和学习的高级人工智能。<br />
<br />
ML 科学家（RL、RLHF、社会、机器人）<a href="https://nitter.cz/natolambert" title="Nathan Lambert">@natolambert</a> 内森·兰伯特撰文描述了他对Q* 的一些看法<br />
<br />
了对Q*可能的工作原理进行了深入分析：<br />
<br />
通俗易懂解释就是：<br />
<br />
1、结合两种策略：Q*是把两种策略混合在一起。一种是像玩国际象棋时预测对手下一步会怎么走（这叫Q学习），另一种是像谷歌地图一样找到从一个地方到另一个地方的最佳路线（这叫A搜索）。<br />
<br />
2、思考多种可能：Q*会像一个思考多种可能性的人一样，考虑不同的解决方案，然后选择最好的那个。就像你在解决一个难题时，会想到很多不同的答案，然后挑选最合适的一个。<br />
<br />
3、自己和自己比赛：Q*还会像一个棋手那样，和自己下棋来提高自己的技能。它通过不断地和自己的不同版本比赛，学习如何做出更好的决策。<br />
<br />
4、每一步都打分：在解决问题的过程中，Q*会给每一步都打分，这样就能知道哪些步骤是好的，哪些不是。就像你在做选择题时，对每个选项进行评估，然后选择最有可能正确的那个。<br />
<br />
5、使用合成数据训练：Q*使用大量的虚拟数据来训练自己，这样就不需要真实世界的数据那么多。这就像是通过模拟考试来准备真正的考试。<br />
<br />
6、学习如何更好地解决问题：最后，Q*会通过一种叫做离线强化学习的方法来提高自己的能力。这就像是通过回顾过去的经验来学习如何在未来做得更好。<br />
<br />
专业解释：<br />
<br />
1、结合Q学习和A*搜索：Q*可能是Q学习（一种强化学习算法）和A*搜索（一种图搜索算法）的结合。这个方法可能涉及通过“思维树”（tree-of-thoughts）在语言/推理步骤上进行搜索。<br />
<br />
2、思维树推理（Tree-of-Thoughts Reasoning）：Q*可能利用所谓的“思维树”来进行推理。这种方法通过提示语言模型创建多个推理路径，这些路径可能会或不会在正确答案处汇合。这种方法类似于递归式的提示技术，用于提高推理性能。<br />
<br />
3、自我对弈和前瞻性规划：Q*可能结合了自我对弈的概念，即通过与自己的不同版本对弈来提高性能，以及前瞻性规划，即使用模型预测未来以产生更好的行动或输出。<br />
<br />
4、过程奖励模型（Process Reward Models, PRMs）：Q*可能使用PRMs来为每个推理步骤打分，而不是整个回答。这允许在推理问题上进行更精细的生成和优化。<br />
<br />
5、合成数据的使用：Q*可能大量使用合成数据来训练和优化模型。合成数据的使用可以减少对人类评分者的依赖，提高数据生成的效率和多样性。<br />
<br />
6、离线强化学习的应用：Q*可能通过离线强化学习进行优化，这与现有的RLHF（强化学习人类反馈）工具类似，但采用了多步骤的方法。<br />
<br />
详细内容：<a href="https://www.interconnects.ai/p/q-star">interconnects.ai/p/q-star</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83RzNsbWJFQUF1cS1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</id>
            <title>R to @xiaohuggg: 测试结果：
https://huggingface.co/openchat/openchat_3.5</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002803887214842#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:02:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果：<br />
<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bVpOWWIwQUF5dUJOLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bWRhQmFVQUVDSmRLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bW5tTmF3QUFUWm1kLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002211404087412#m</id>
            <title>OpenChat-3.5-7B ：在各种基准测试上超越ChatGPT

OpenChat使用了C-RLFT（一种受离线强化学习启发的策略）进行微调。

它能通过分析已有的对话数据和反馈来改进模型的表现。还可以从错误中学习。

测试了下，虽然只有7B大小，确实效果和GPT不分上下。

牛P的是它能在24GB RAM的消费级GPU上运行。

OpenChat还提供了一个Web UI界面，方便用户与模型进行交互。

性能和评估：

在实际应用中，OpenChat展示了优异的性能。它在多个基准测试中表现出色，特别是在遵循指令和泛化能力方面，超越了其他同类的开源语言模型。

在基准测试方面，OpenChat-3.5的7B模型在多个测试中的平均得分为61.6，超过了ChatGPT（March版本）的61.5。

在于http://X.AI 330 亿参数的Grok的比拼中OpenChat-3.5-7B

OpenChat工作原理：

1、预训练语言模型：OpenChat的核心是一个大型的预训练语言模型。这些模型通过分析和学习大量的文本数据，掌握了语言的结构、语法和语义。这使得OpenChat能够理解用户的输入，并生成流畅、连贯的回应。

2、微调方法（C-RLFT）：OpenChat采用了一种名为条件化强化学习微调（Conditioned-RLFT, C-RLFT）的方法。这种方法特别适用于处理混合质量的数据。在传统的微调方法中，所有的训练数据都被视为同等重要，这可能导致模型在处理质量不一的数据时效果不佳。C-RLFT通过将不同数据源视为不同的奖励标签，使模型能够更有效地从这些数据中学习。

3、类条件策略学习：在C-RLFT中，OpenChat学习了一个类条件策略，这意味着它可以根据输入数据的类型（例如，不同的数据源或质量）来调整其响应。这种策略使得OpenChat在处理各种不同类型的输入时更加灵活和有效。

4、单阶段监督学习：OpenChat使用了一种单阶段的监督学习方法。这种方法不依赖于传统的强化学习技术，而是通过最大化奖励并减少与参考策略之间的差异来优化模型。这种方法提高了学习效率，并有助于减少训练过程中的错误。

详细：https://huggingface.co/openchat/openchat_3.5
GitHub：https://github.com/imoneoi/openchat
论文：https://arxiv.org/pdf/2309.11235.pdf
在线体验：https://openchat.team/</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002211404087412#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002211404087412#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:00:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenChat-3.5-7B ：在各种基准测试上超越ChatGPT<br />
<br />
OpenChat使用了C-RLFT（一种受离线强化学习启发的策略）进行微调。<br />
<br />
它能通过分析已有的对话数据和反馈来改进模型的表现。还可以从错误中学习。<br />
<br />
测试了下，虽然只有7B大小，确实效果和GPT不分上下。<br />
<br />
牛P的是它能在24GB RAM的消费级GPU上运行。<br />
<br />
OpenChat还提供了一个Web UI界面，方便用户与模型进行交互。<br />
<br />
性能和评估：<br />
<br />
在实际应用中，OpenChat展示了优异的性能。它在多个基准测试中表现出色，特别是在遵循指令和泛化能力方面，超越了其他同类的开源语言模型。<br />
<br />
在基准测试方面，OpenChat-3.5的7B模型在多个测试中的平均得分为61.6，超过了ChatGPT（March版本）的61.5。<br />
<br />
在于<a href="http://X.AI">X.AI</a> 330 亿参数的Grok的比拼中OpenChat-3.5-7B<br />
<br />
OpenChat工作原理：<br />
<br />
1、预训练语言模型：OpenChat的核心是一个大型的预训练语言模型。这些模型通过分析和学习大量的文本数据，掌握了语言的结构、语法和语义。这使得OpenChat能够理解用户的输入，并生成流畅、连贯的回应。<br />
<br />
2、微调方法（C-RLFT）：OpenChat采用了一种名为条件化强化学习微调（Conditioned-RLFT, C-RLFT）的方法。这种方法特别适用于处理混合质量的数据。在传统的微调方法中，所有的训练数据都被视为同等重要，这可能导致模型在处理质量不一的数据时效果不佳。C-RLFT通过将不同数据源视为不同的奖励标签，使模型能够更有效地从这些数据中学习。<br />
<br />
3、类条件策略学习：在C-RLFT中，OpenChat学习了一个类条件策略，这意味着它可以根据输入数据的类型（例如，不同的数据源或质量）来调整其响应。这种策略使得OpenChat在处理各种不同类型的输入时更加灵活和有效。<br />
<br />
4、单阶段监督学习：OpenChat使用了一种单阶段的监督学习方法。这种方法不依赖于传统的强化学习技术，而是通过最大化奖励并减少与参考策略之间的差异来优化模型。这种方法提高了学习效率，并有助于减少训练过程中的错误。<br />
<br />
详细：<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a><br />
GitHub：<a href="https://github.com/imoneoi/openchat">github.com/imoneoi/openchat</a><br />
论文：<a href="https://arxiv.org/pdf/2309.11235.pdf">arxiv.org/pdf/2309.11235.pdf</a><br />
在线体验：<a href="https://openchat.team/">openchat.team/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMDE4NzkzOTk3OTI2NDAvcHUvaW1nL2lNU3lwZ194ekxQTjgwek8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1728959646138880026#m</id>
            <title>RT by @xiaohuggg: OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：

Intro: Large Language Model (LLM) talk

大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。

LLM Inference

首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。

以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。

现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。

在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。

除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。

你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。

比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。

我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。

那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。</title>
            <link>https://nitter.cz/dotey/status/1728959646138880026#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1728959646138880026#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:11:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：<br />
<br />
Intro: Large Language Model (LLM) talk<br />
<br />
大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。<br />
<br />
LLM Inference<br />
<br />
首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。<br />
<br />
以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。<br />
<br />
现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。<br />
<br />
在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。<br />
<br />
除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。<br />
<br />
你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。<br />
<br />
比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。<br />
<br />
我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。<br />
<br />
那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。</p>
<p><a href="https://nitter.cz/karpathy/status/1727731541781152035#m">nitter.cz/karpathy/status/1727731541781152035#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg5NTg2NTQ5Nzg2NjI0MDIvcHUvaW1nL096ak1ReDBBU0JqM29IUkYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728989154812498295#m</id>
            <title>R to @xiaohuggg: 0.025秒一张图…

😂

真是666</title>
            <link>https://nitter.cz/xiaohuggg/status/1728989154812498295#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728989154812498295#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 04:08:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>0.025秒一张图…<br />
<br />
😂<br />
<br />
真是666</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4NzcxODMyNjgwMTgxNzYwL2ltZy9UbDFWQmtBRFkzRFQxMURZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728988899161333970#m</id>
            <title>一位日本博主展示了LCM现在可以以大约40fps的速度生成图像，这使得完全实现实时应用成为可能。

@cumulo_autumn 展示了一个演示视频，该视频以1倍速（即实时速度）运行，包括OBS的屏幕录制和VRoid的渲染，运行速度约为36fps。他指出，如果不录制视频，速度可以达到39fps。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728988899161333970#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728988899161333970#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 04:07:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一位日本博主展示了LCM现在可以以大约40fps的速度生成图像，这使得完全实现实时应用成为可能。<br />
<br />
<a href="https://nitter.cz/cumulo_autumn" title="あき先生 | AI Vtuber『しずく』開発中">@cumulo_autumn</a> 展示了一个演示视频，该视频以1倍速（即实时速度）运行，包括OBS的屏幕录制和VRoid的渲染，运行速度约为36fps。他指出，如果不录制视频，速度可以达到39fps。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4NzY4Mzc0MTI0MjgxODU2L2ltZy9FRkx5RFBZQ3FZNGVYXzNiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728968230864351237#m</id>
            <title>UIDraw ：在手机上绘制简单UI草图转换成网站。

使用 GPT-4 Vision 和 PencilKit/PKCanvasView （可绘制的画布）技术，用户可以绘制用户界面(UI)，然后将其转换成HTML代码。

GitHub：https://github.com/jordansinger/UIDraw</title>
            <link>https://nitter.cz/xiaohuggg/status/1728968230864351237#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728968230864351237#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:45:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>UIDraw ：在手机上绘制简单UI草图转换成网站。<br />
<br />
使用 GPT-4 Vision 和 PencilKit/PKCanvasView （可绘制的画布）技术，用户可以绘制用户界面(UI)，然后将其转换成HTML代码。<br />
<br />
GitHub：<a href="https://github.com/jordansinger/UIDraw">github.com/jordansinger/UIDr…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4ODQ4NTQ3MjA4OTE2OTkyL2ltZy9xWS16SjliYjh0LXlOaGNhLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728963473152045089#m</id>
            <title>Loom：一个创新的写作工具，可以让你和AI一起创作故事或文章

Loom基于GPT-3，采用了一种独特的树形结构来组织文本。

每个故事或文章的部分都像树的一个分支，你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。

举例解释：

假设你想写一个关于太空探险的故事。你已经有了一个大致的想法，但还不确定具体的情节和方向。这时，你可以使用Loom来帮助你发展这个故事。

1、开始创作：首先，你在Loom的主文本框中输入你的初始想法，比如“一队宇航员在遥远的星系发现了一个未知的行星”。

2、生成内容：接下来，你可以让AI帮你生成接下来的情节。比如，你可以让AI为你生成关于这个未知行星的描述，或者宇航员在行星上的遭遇。

3、探索不同的情节线：AI生成的内容会以树形结构展现。你可以在这个树上看到不同的分支，每个分支代表一个不同的故事方向。比如，一个分支可能是宇航员在行星上发现了外星生命的迹象，另一个分支可能是他们遇到了技术故障。

4、选择和发展：你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。

5、编辑和完善：在创作的过程中，你可以随时编辑和修改AI生成的内容，或者添加你自己的想法和细节，使故事更加丰富和完整。

6、保存和分享：完成故事后，你可以将整个故事树以JSON格式保存下来，也可以分享给其他人，让他们看到你的创作过程和最终成果。

通过这种方式，Loom让你能够以一种非线性和互动的方式创作故事，同时结合了AI的智能和你自己的创造力。

Loom的主要特点和功能包括：

1、基于GPT 3：Loom基于GPT 3开发，允许用户与GPT-3合作创作内容。用户可以输入一些文本或想法，然后让AI基于这些输入生成新的内容或建议。

2、树形写作界面：Loom采用了一种独特的树形结构来组织文本。每个故事或文章的部分都像树的一个分支，用户可以在任何分支上继续发展故事，或者探索不同的情节方向。

3、多视角导航：用户可以在树形结构中自由导航，探索不同的故事线索和发展。这种方式使得故事创作更加灵活和多元。

4、内容生成和编辑：用户可以编辑树中的任何节点，并使用AI来生成新的节点或内容。这为创作提供了额外的灵感和帮助。

5、文件输入/输出：Loom支持以JSON格式导入和导出故事树，方便用户保存和分享他们的创作。

6、块多元宇宙模式：这是一个实验性的功能，用于展示和演示如何在不同的块（或情节片段）之间进行切换和探索。

5、热键和快捷操作：Loom提供了一系列热键和快捷操作，使用户能够快速进行各种操作，如打开文件、保存、生成内容等。

GitHub：https://github.com/socketteer/loom
实例：https://generative.ink/meta/block-multiverse/</title>
            <link>https://nitter.cz/xiaohuggg/status/1728963473152045089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728963473152045089#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:26:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Loom：一个创新的写作工具，可以让你和AI一起创作故事或文章<br />
<br />
Loom基于GPT-3，采用了一种独特的树形结构来组织文本。<br />
<br />
每个故事或文章的部分都像树的一个分支，你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。<br />
<br />
举例解释：<br />
<br />
假设你想写一个关于太空探险的故事。你已经有了一个大致的想法，但还不确定具体的情节和方向。这时，你可以使用Loom来帮助你发展这个故事。<br />
<br />
1、开始创作：首先，你在Loom的主文本框中输入你的初始想法，比如“一队宇航员在遥远的星系发现了一个未知的行星”。<br />
<br />
2、生成内容：接下来，你可以让AI帮你生成接下来的情节。比如，你可以让AI为你生成关于这个未知行星的描述，或者宇航员在行星上的遭遇。<br />
<br />
3、探索不同的情节线：AI生成的内容会以树形结构展现。你可以在这个树上看到不同的分支，每个分支代表一个不同的故事方向。比如，一个分支可能是宇航员在行星上发现了外星生命的迹象，另一个分支可能是他们遇到了技术故障。<br />
<br />
4、选择和发展：你可以选择你感兴趣的一个分支，继续在这个方向上发展故事。同时，你也可以随时回到树的其他部分，探索不同的情节可能性。<br />
<br />
5、编辑和完善：在创作的过程中，你可以随时编辑和修改AI生成的内容，或者添加你自己的想法和细节，使故事更加丰富和完整。<br />
<br />
6、保存和分享：完成故事后，你可以将整个故事树以JSON格式保存下来，也可以分享给其他人，让他们看到你的创作过程和最终成果。<br />
<br />
通过这种方式，Loom让你能够以一种非线性和互动的方式创作故事，同时结合了AI的智能和你自己的创造力。<br />
<br />
Loom的主要特点和功能包括：<br />
<br />
1、基于GPT 3：Loom基于GPT 3开发，允许用户与GPT-3合作创作内容。用户可以输入一些文本或想法，然后让AI基于这些输入生成新的内容或建议。<br />
<br />
2、树形写作界面：Loom采用了一种独特的树形结构来组织文本。每个故事或文章的部分都像树的一个分支，用户可以在任何分支上继续发展故事，或者探索不同的情节方向。<br />
<br />
3、多视角导航：用户可以在树形结构中自由导航，探索不同的故事线索和发展。这种方式使得故事创作更加灵活和多元。<br />
<br />
4、内容生成和编辑：用户可以编辑树中的任何节点，并使用AI来生成新的节点或内容。这为创作提供了额外的灵感和帮助。<br />
<br />
5、文件输入/输出：Loom支持以JSON格式导入和导出故事树，方便用户保存和分享他们的创作。<br />
<br />
6、块多元宇宙模式：这是一个实验性的功能，用于展示和演示如何在不同的块（或情节片段）之间进行切换和探索。<br />
<br />
5、热键和快捷操作：Loom提供了一系列热键和快捷操作，使用户能够快速进行各种操作，如打开文件、保存、生成内容等。<br />
<br />
GitHub：<a href="https://github.com/socketteer/loom">github.com/socketteer/loom</a><br />
实例：<a href="https://generative.ink/meta/block-multiverse/">generative.ink/meta/block-mu…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhRV2JjQUF6dFY4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhRMmJjQUFtSzZILmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhReWE0QUFKZ2NLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82REhRd2FjQUFRa2t3LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728779202542154064#m</id>
            <title>2023年世界百万富翁的迁移图

接待新百万富翁最多的前三个国家；澳大利亚（预计迁入5,200名）、阿联酋（4,500名）、新加坡（3,200名）…

百万富翁流失最多的三个国家：中国（预计流失13,500名）、印度（6,500名）、英国（3,200名）…

预计到2023年年底将有122,000名HNWIs 高净值个人（HNWIs）迁移到新的国家。

在这里，HNWIs被定义为拥有至少100万美元净资产的个人。

以下是详细：

接待新百万富翁的国家主要包括：

1. 澳大利亚（预计迁入5,200名）
2. 阿联酋（4,500名）
3. 新加坡（3,200名）
4. 美国（2,100名）
5. 瑞士（1,800名）
6. 加拿大（1,600名）
7. 希腊（1,200名）
8. 法国（1,000名）
9. 葡萄牙（800名）
10. 新西兰（700名）

而失去最多百万富翁的国家包括：

1. 中国（预计流失13,500名）
2. 印度（6,500名）
3. 英国（3,200名）
4. 俄罗斯（3,000名）
5. 巴西（1,200名）
6. 香港特别行政区（1,000名）
7. 韩国（800名）
8. 墨西哥（700名）
9. 南非（500名）
10. 日本（300名）

详细：https://www.visualcapitalist.com/mapped-the-migration-of-the-worlds-millionaires-in-2023/#google_vignette</title>
            <link>https://nitter.cz/xiaohuggg/status/1728779202542154064#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728779202542154064#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 14:14:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2023年世界百万富翁的迁移图<br />
<br />
接待新百万富翁最多的前三个国家；澳大利亚（预计迁入5,200名）、阿联酋（4,500名）、新加坡（3,200名）…<br />
<br />
百万富翁流失最多的三个国家：中国（预计流失13,500名）、印度（6,500名）、英国（3,200名）…<br />
<br />
预计到2023年年底将有122,000名HNWIs 高净值个人（HNWIs）迁移到新的国家。<br />
<br />
在这里，HNWIs被定义为拥有至少100万美元净资产的个人。<br />
<br />
以下是详细：<br />
<br />
接待新百万富翁的国家主要包括：<br />
<br />
1. 澳大利亚（预计迁入5,200名）<br />
2. 阿联酋（4,500名）<br />
3. 新加坡（3,200名）<br />
4. 美国（2,100名）<br />
5. 瑞士（1,800名）<br />
6. 加拿大（1,600名）<br />
7. 希腊（1,200名）<br />
8. 法国（1,000名）<br />
9. 葡萄牙（800名）<br />
10. 新西兰（700名）<br />
<br />
而失去最多百万富翁的国家包括：<br />
<br />
1. 中国（预计流失13,500名）<br />
2. 印度（6,500名）<br />
3. 英国（3,200名）<br />
4. 俄罗斯（3,000名）<br />
5. 巴西（1,200名）<br />
6. 香港特别行政区（1,000名）<br />
7. 韩国（800名）<br />
8. 墨西哥（700名）<br />
9. 南非（500名）<br />
10. 日本（300名）<br />
<br />
详细：<a href="https://www.visualcapitalist.com/mapped-the-migration-of-the-worlds-millionaires-in-2023/#google_vignette">visualcapitalist.com/mapped-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zYkFXZ2FBQUFtdmVDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728758009483125062#m</id>
            <title>你们知道国产大模型有多少个了吗？？

188个！一百八十八个！！！

可谓是遥遥领先...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728758009483125062#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728758009483125062#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 12:50:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>你们知道国产大模型有多少个了吗？？<br />
<br />
188个！一百八十八个！！！<br />
<br />
可谓是遥遥领先...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zR1FfNmJnQUFobHkzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728750849973956702#m</id>
            <title>R to @xiaohuggg: 详细报道：

认识第一个每月收入高达 10,000 欧元的西班牙 AI 模型

https://www.euronews.com/next/2023/11/22/meet-the-first-spanish-ai-model-earning-up-to-10000-per-month</title>
            <link>https://nitter.cz/xiaohuggg/status/1728750849973956702#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728750849973956702#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 12:21:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>详细报道：<br />
<br />
认识第一个每月收入高达 10,000 欧元的西班牙 AI 模型<br />
<br />
<a href="https://www.euronews.com/next/2023/11/22/meet-the-first-spanish-ai-model-earning-up-to-10000-per-month">euronews.com/next/2023/11/22…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zQnVTamJvQUFEeTdvLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728750397765128539#m</id>
            <title>西班牙一家MCN公司因为对真实模特和网红的不靠谱合作感到厌烦，于是创造了自己的AI网红Aitana。

Aitana平均每月带来3300美元的收入，有时甚至高达10900美元。这些收入主要来自广告，Aitana还成为了一家运动补充品品牌的大使。

即使在媒体揭露她是AI创造后，许多粉丝仍然表达了对她的喜爱。🙄

她在Instagram上拥有超过13万粉丝...😅

设计团队设定了Aitana的基本特征和个性。她被设计成一个健身爱好者，性格坚定而复杂。她的外观、兴趣和特点都是基于对社会趋势的分析。

Aitana的实际形象是通过人工智能技术和Photoshop的结合创造出来的。设计团队使用这些工具来生成她的图像，并确保她的外观接近完美。

由于Aitana是一个虚拟人物，设计团队需要定期为她创造“生活”。这包括决定她一周内的活动、她将访问的地方以及将上传到社交媒体的照片。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728750397765128539#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728750397765128539#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 12:19:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>西班牙一家MCN公司因为对真实模特和网红的不靠谱合作感到厌烦，于是创造了自己的AI网红Aitana。<br />
<br />
Aitana平均每月带来3300美元的收入，有时甚至高达10900美元。这些收入主要来自广告，Aitana还成为了一家运动补充品品牌的大使。<br />
<br />
即使在媒体揭露她是AI创造后，许多粉丝仍然表达了对她的喜爱。🙄<br />
<br />
她在Instagram上拥有超过13万粉丝...😅<br />
<br />
设计团队设定了Aitana的基本特征和个性。她被设计成一个健身爱好者，性格坚定而复杂。她的外观、兴趣和特点都是基于对社会趋势的分析。<br />
<br />
Aitana的实际形象是通过人工智能技术和Photoshop的结合创造出来的。设计团队使用这些工具来生成她的图像，并确保她的外观接近完美。<br />
<br />
由于Aitana是一个虚拟人物，设计团队需要定期为她创造“生活”。这包括决定她一周内的活动、她将访问的地方以及将上传到社交媒体的照片。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8yX01MU2J3QUFSSmtrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8zQktqTWFrQUFHMmM1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728742043349094427#m</id>
            <title>PlugBear：将ChatGPT等LLM连接到其他在线工具和服务

比如，你可以选择把你的GPTs应用连接到Slack、Discord、WhatsApp等外部即时通讯平台，这样你的AI就可以在Slack里回答问题或执行任务。

这意味着用户可以轻松地将他们的AI应用集成到常用的通讯和协作平台中。

PlugBear的优点：

1、简单的设置过程：用户只需几个点击就可以完成设置，无需复杂的配置。

2、添加频道和应用：用户可以选择他们喜欢的频道来连接他们的AI机器人，并添加他们使用的LLM应用。这提供了灵活性，允许用户根据自己的需求和偏好进行定制。

3、连接频道与应用：用户可以将这些频道和应用相互连接，并定义触发AI的条件。这种方式使得AI应用的部署和使用变得更加高效和灵活。

4、一次开发，多处连接：PlugBear的理念是“一次开发，到处连接”。这意味着用户无需在不同平台上重复进行AI集成工作，节省了大量时间和资源。

PlugBear是一个旨在简化和加速LLM应用与各种工具和平台集成的服务。它支持众多 LLM 应用程序构建器和框架，包括 OpenAI 的 GPT、LangChain 等。

PlugBear就像是一个桥梁，让你的AI应用能够轻松地和其他软件服务连接和交流，扩大了AI应用的使用场景和功能。

访问：https://plugbear.io/</title>
            <link>https://nitter.cz/xiaohuggg/status/1728742043349094427#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728742043349094427#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 11:46:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PlugBear：将ChatGPT等LLM连接到其他在线工具和服务<br />
<br />
比如，你可以选择把你的GPTs应用连接到Slack、Discord、WhatsApp等外部即时通讯平台，这样你的AI就可以在Slack里回答问题或执行任务。<br />
<br />
这意味着用户可以轻松地将他们的AI应用集成到常用的通讯和协作平台中。<br />
<br />
PlugBear的优点：<br />
<br />
1、简单的设置过程：用户只需几个点击就可以完成设置，无需复杂的配置。<br />
<br />
2、添加频道和应用：用户可以选择他们喜欢的频道来连接他们的AI机器人，并添加他们使用的LLM应用。这提供了灵活性，允许用户根据自己的需求和偏好进行定制。<br />
<br />
3、连接频道与应用：用户可以将这些频道和应用相互连接，并定义触发AI的条件。这种方式使得AI应用的部署和使用变得更加高效和灵活。<br />
<br />
4、一次开发，多处连接：PlugBear的理念是“一次开发，到处连接”。这意味着用户无需在不同平台上重复进行AI集成工作，节省了大量时间和资源。<br />
<br />
PlugBear是一个旨在简化和加速LLM应用与各种工具和平台集成的服务。它支持众多 LLM 应用程序构建器和框架，包括 OpenAI 的 GPT、LangChain 等。<br />
<br />
PlugBear就像是一个桥梁，让你的AI应用能够轻松地和其他软件服务连接和交流，扩大了AI应用的使用场景和功能。<br />
<br />
访问：<a href="https://plugbear.io/">plugbear.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg2Njc2MTEyODA2MTc0NzIvcHUvaW1nL3h4RDRFUTZZTWRhaVJJQXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728686421559644632#m</id>
            <title>一款专门为VR游戏设计的鞋子😂

玩VR游戏的时候你肯能会跟着游戏跑来跑去，容易撞上家具…

这款鞋子可以让你能运动的同时

保持原地踏步😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1728686421559644632#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728686421559644632#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 08:05:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一款专门为VR游戏设计的鞋子😂<br />
<br />
玩VR游戏的时候你肯能会跟着游戏跑来跑去，容易撞上家具…<br />
<br />
这款鞋子可以让你能运动的同时<br />
<br />
保持原地踏步😅</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgwMDA2Nzk4MzMxMjQ4NjQvcHUvaW1nL2NFUTZmVUp1aUJ3Q093Yk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>