<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744296052029870124#m</id>
            <title>麻省理工大学研究团队开发出一种新技术：Ddog

通过脑电波控制波士顿动力的机器狗。

该技术仅靠一种特殊的眼镜就能读取人的脑电波和眼动，然后把这些信号传递给机器人执行动作。

Ddog系统只需要两 iPhone和一副蓝牙眼镜就可以运行。而且还可以完全离线工作。

简单来说，如果用户想要机器人去拿一瓶水，你只需要想象这个动作，这个特殊的眼镜就能捕捉到这个想法，并告诉机器人去完成这个任务。

机器人自己能够四处走动，爬楼梯，帮你拿东西，像一个真正的小助手一样。这项技术不需要复杂的装备，只需要两部手机和一副眼镜，这让它在日常生活中很容易使用。

Ddog 项目让身体受限的人，如患有 ALS、脑瘫和脊髓损伤等身体障碍的人提供帮助。让他们能够通过思考来控制机器人，使机器人成为他们的手和脚，帮助他们更好地与周围世界互动。

工作原理：

Ddog 项目是基于 MIT 的一个名为 Brain Switch 的先进技术。Brain Switch 是一种脑机接口（BCI），它可以实时地、无需言语地捕捉用户的大脑信号，并将这些信号转化为与看护者沟通的指令。

1、脑机接口：研究者们用一种特殊的无线眼镜来读取佩戴者的大脑活动和眼睛运动。这些眼镜有传感器，能够捕捉到你的脑电波（即大脑发出的信号）。

2、信号解读：当你想让机器人执行一个动作时（比如拿东西），你的大脑会产生特定的信号。这些眼镜能识别这些信号，把它们转换成电子指令。

Ddog 的工作方式：

当 Spot 与新用户在新环境中工作时，首先需要创建工作环境的 3D 地图。

信号解读：接下来，第一部 iPhone 会通过询问用户接下来想做什么来提示用户，用户只需通过思考他们想要的东西来回答。

环境识别：第二部 iPhone 运行本地导航地图，控制 Spot 的机械臂，并使用 iPhone 的激光雷达数据增强 Spot 的激光雷达。

任务执行：两部 iPhone 相互通信以跟踪 Spot 完成任务的进度。

详细介绍：https://www.therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744296052029870124#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744296052029870124#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 09:52:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>麻省理工大学研究团队开发出一种新技术：Ddog<br />
<br />
通过脑电波控制波士顿动力的机器狗。<br />
<br />
该技术仅靠一种特殊的眼镜就能读取人的脑电波和眼动，然后把这些信号传递给机器人执行动作。<br />
<br />
Ddog系统只需要两 iPhone和一副蓝牙眼镜就可以运行。而且还可以完全离线工作。<br />
<br />
简单来说，如果用户想要机器人去拿一瓶水，你只需要想象这个动作，这个特殊的眼镜就能捕捉到这个想法，并告诉机器人去完成这个任务。<br />
<br />
机器人自己能够四处走动，爬楼梯，帮你拿东西，像一个真正的小助手一样。这项技术不需要复杂的装备，只需要两部手机和一副眼镜，这让它在日常生活中很容易使用。<br />
<br />
Ddog 项目让身体受限的人，如患有 ALS、脑瘫和脊髓损伤等身体障碍的人提供帮助。让他们能够通过思考来控制机器人，使机器人成为他们的手和脚，帮助他们更好地与周围世界互动。<br />
<br />
工作原理：<br />
<br />
Ddog 项目是基于 MIT 的一个名为 Brain Switch 的先进技术。Brain Switch 是一种脑机接口（BCI），它可以实时地、无需言语地捕捉用户的大脑信号，并将这些信号转化为与看护者沟通的指令。<br />
<br />
1、脑机接口：研究者们用一种特殊的无线眼镜来读取佩戴者的大脑活动和眼睛运动。这些眼镜有传感器，能够捕捉到你的脑电波（即大脑发出的信号）。<br />
<br />
2、信号解读：当你想让机器人执行一个动作时（比如拿东西），你的大脑会产生特定的信号。这些眼镜能识别这些信号，把它们转换成电子指令。<br />
<br />
Ddog 的工作方式：<br />
<br />
当 Spot 与新用户在新环境中工作时，首先需要创建工作环境的 3D 地图。<br />
<br />
信号解读：接下来，第一部 iPhone 会通过询问用户接下来想做什么来提示用户，用户只需通过思考他们想要的东西来回答。<br />
<br />
环境识别：第二部 iPhone 运行本地导航地图，控制 Spot 的机械臂，并使用 iPhone 的激光雷达数据增强 Spot 的激光雷达。<br />
<br />
任务执行：两部 iPhone 相互通信以跟踪 Spot 完成任务的进度。<br />
<br />
详细介绍：<a href="https://www.therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/">therobotreport.com/ddog-mit-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQyNzIxODYyNDcxNjM5MDQvcHUvaW1nL2tmRjlqbjN5bDFoUGRRUG4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744278403266826571#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1744278403266826571#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744278403266826571#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 08:42:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUcmQ5QWJjQUF1dHFrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUcmQ5MWFjQUF4c0dTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744278400561500270#m</id>
            <title>这效果如何...

神龙摆尾...

提示词在ALT里面...</title>
            <link>https://nitter.cz/xiaohuggg/status/1744278400561500270#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744278400561500270#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 08:42:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这效果如何...<br />
<br />
神龙摆尾...<br />
<br />
提示词在ALT里面...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUclQ3Y2FzQUFwbDh3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744272205658370554#m</id>
            <title>Google 发明了一种估算照片中光照条件的新方法：DiffusionLight

该技术可以在照片中加入一个看起来像是真实反射环境的铬球。这个铬球可以帮助计算出照片中的光照是怎样的。

然后，他们使用这些光照信息在照片中添加新的物体，使得这些物体看起来好像是在原来的光照条件下拍摄的一样。

简单地说就是：检测图片中的光源（光照信息），然后根据光源，把其他物体对象插入到图片中，能达到相同的光影效果，毫无违和感。

工作原理如下：

1、输入图片：你提供一张照片，比如一个室内场景。

2、添加铬球：使用 DiffusionLight 技术，在照片中的合适位置绘制一个铬球。这个球会反射出场景中的光线和色彩。

3、生成环境图：铬球的反射可以被转换成一个高动态范围（HDR）的环境图，这张图像包含了关于场景光照的信息。

4、光照估计：通过分析铬球上的反射，DiffusionLight 估计出图片中的光源位置和亮度。

5、三维对象插入：利用估计出的光照信息，三维模型可以被添加进照片中，并且以一种看起来自然和真实的方式反应光照效果。

这项技术的一个关键创新是它不需要昂贵或复杂的设备来捕获光照条件。它只需要一张图片和强大的算法。这意味着它可以用于从专业电影制作到手机摄影的各种应用，为艺术家和开发者创造新的可能性。

该技术可以用于多种输入图像，如室内外场景、特写镜头、绘画和人脸照片。

项目及演示：https://diffusionlight.github.io/
论文：https://arxiv.org/abs/2312.09168
GitHub：https://github.com/DiffusionLight/DiffusionLight</title>
            <link>https://nitter.cz/xiaohuggg/status/1744272205658370554#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744272205658370554#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 08:17:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 发明了一种估算照片中光照条件的新方法：DiffusionLight<br />
<br />
该技术可以在照片中加入一个看起来像是真实反射环境的铬球。这个铬球可以帮助计算出照片中的光照是怎样的。<br />
<br />
然后，他们使用这些光照信息在照片中添加新的物体，使得这些物体看起来好像是在原来的光照条件下拍摄的一样。<br />
<br />
简单地说就是：检测图片中的光源（光照信息），然后根据光源，把其他物体对象插入到图片中，能达到相同的光影效果，毫无违和感。<br />
<br />
工作原理如下：<br />
<br />
1、输入图片：你提供一张照片，比如一个室内场景。<br />
<br />
2、添加铬球：使用 DiffusionLight 技术，在照片中的合适位置绘制一个铬球。这个球会反射出场景中的光线和色彩。<br />
<br />
3、生成环境图：铬球的反射可以被转换成一个高动态范围（HDR）的环境图，这张图像包含了关于场景光照的信息。<br />
<br />
4、光照估计：通过分析铬球上的反射，DiffusionLight 估计出图片中的光源位置和亮度。<br />
<br />
5、三维对象插入：利用估计出的光照信息，三维模型可以被添加进照片中，并且以一种看起来自然和真实的方式反应光照效果。<br />
<br />
这项技术的一个关键创新是它不需要昂贵或复杂的设备来捕获光照条件。它只需要一张图片和强大的算法。这意味着它可以用于从专业电影制作到手机摄影的各种应用，为艺术家和开发者创造新的可能性。<br />
<br />
该技术可以用于多种输入图像，如室内外场景、特写镜头、绘画和人脸照片。<br />
<br />
项目及演示：<a href="https://diffusionlight.github.io/">diffusionlight.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.09168">arxiv.org/abs/2312.09168</a><br />
GitHub：<a href="https://github.com/DiffusionLight/DiffusionLight">github.com/DiffusionLight/Di…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQyNTU2MjMzNzY4NzE0MjQvcHUvaW1nL0JEMTM3VjlvVFdIbmZTeWwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744246627865608573#m</id>
            <title>MATHPILE：一个高质量、大规模的数学语料库，29 GB，包含约 95 亿个代币。

涵盖从 K-12 到大学、研究生水平和数学竞赛的内容，包括高质量教科书、讲义、科学论文等。

提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。

-数据来源和处理：数据最初来源于多个不同的数据源，总计大约 520 亿个令牌，占 2.2 TB 的数据量。

源数据包括 StackExchange、ProofWiki、Common Crawl、arXiv，以及其他来源。这些数据经过一系列严格的处理过程，包括数据预处理和预过滤、语言识别、清理和过滤，以及去重。

-MATHPILE 语料库：经过处理后，得到了一个以数学为中心的语料库，即 MATHPILE。这个语料库总计有 29 GB 的数据量，包含约 903,000 篇文档，以及大约 95 亿个令牌。

主要特点：

1、数学领域专注：MathPile 是专门为数学领域设计的，与通用或多语言焦点的语料库有明显区别。

2、多样性：MathPile 从广泛的来源汇集数据，包括教科书（包括讲义）、arXiv、维基百科、ProofWiki、StackExchange 和网页。它涵盖了适合 K-12、大学、研究生水平和数学竞赛的数学内容。特别是，项目发布了大量高质量教科书的显著收藏（约 0.19B 令牌）。

3、高质量：项目坚持“少即是多”的原则，即使在预训练阶段也相信数据质量胜过数量。项目的数据收集和处理努力包括复杂的预处理、预过滤、清洁、过滤和去重，确保语料库的高质量。

4、数据文档：为了增强透明度，提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。如语言识别分数和符号到单词的比率。这为用户提供了根据其需要定制数据的灵活性。

项目还进行了数据污染检测，以消除诸如 MATH 和 MMLU-STEM 等基准测试集的重复项。

通过这种专门的语料库，研究人员和开发者能够更有效地提高语言模型在数学推理方面的能力。

项目地址：https://gair-nlp.github.io/MathPile/
论文：https://arxiv.org/abs/2312.17120
GitHub：https://github.com/GAIR-NLP/MathPile
数据集：https://huggingface.co/datasets/GAIR/MathPile</title>
            <link>https://nitter.cz/xiaohuggg/status/1744246627865608573#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744246627865608573#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 06:36:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MATHPILE：一个高质量、大规模的数学语料库，29 GB，包含约 95 亿个代币。<br />
<br />
涵盖从 K-12 到大学、研究生水平和数学竞赛的内容，包括高质量教科书、讲义、科学论文等。<br />
<br />
提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。<br />
<br />
-数据来源和处理：数据最初来源于多个不同的数据源，总计大约 520 亿个令牌，占 2.2 TB 的数据量。<br />
<br />
源数据包括 StackExchange、ProofWiki、Common Crawl、arXiv，以及其他来源。这些数据经过一系列严格的处理过程，包括数据预处理和预过滤、语言识别、清理和过滤，以及去重。<br />
<br />
-MATHPILE 语料库：经过处理后，得到了一个以数学为中心的语料库，即 MATHPILE。这个语料库总计有 29 GB 的数据量，包含约 903,000 篇文档，以及大约 95 亿个令牌。<br />
<br />
主要特点：<br />
<br />
1、数学领域专注：MathPile 是专门为数学领域设计的，与通用或多语言焦点的语料库有明显区别。<br />
<br />
2、多样性：MathPile 从广泛的来源汇集数据，包括教科书（包括讲义）、arXiv、维基百科、ProofWiki、StackExchange 和网页。它涵盖了适合 K-12、大学、研究生水平和数学竞赛的数学内容。特别是，项目发布了大量高质量教科书的显著收藏（约 0.19B 令牌）。<br />
<br />
3、高质量：项目坚持“少即是多”的原则，即使在预训练阶段也相信数据质量胜过数量。项目的数据收集和处理努力包括复杂的预处理、预过滤、清洁、过滤和去重，确保语料库的高质量。<br />
<br />
4、数据文档：为了增强透明度，提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。如语言识别分数和符号到单词的比率。这为用户提供了根据其需要定制数据的灵活性。<br />
<br />
项目还进行了数据污染检测，以消除诸如 MATH 和 MMLU-STEM 等基准测试集的重复项。<br />
<br />
通过这种专门的语料库，研究人员和开发者能够更有效地提高语言模型在数学推理方面的能力。<br />
<br />
项目地址：<a href="https://gair-nlp.github.io/MathPile/">gair-nlp.github.io/MathPile/</a><br />
论文：<a href="https://arxiv.org/abs/2312.17120">arxiv.org/abs/2312.17120</a><br />
GitHub：<a href="https://github.com/GAIR-NLP/MathPile">github.com/GAIR-NLP/MathPile</a><br />
数据集：<a href="https://huggingface.co/datasets/GAIR/MathPile">huggingface.co/datasets/GAIR…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUT2xibWJJQUF1elY4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744197461802152266#m</id>
            <title>有人在 Binance 上购买了 26.9 个BTC

然后将其发送到了中本聪的死亡钱包中...

这是什么操作？？？

除非是中本聪本人才干得出这种事情吧？？？

🤣</title>
            <link>https://nitter.cz/xiaohuggg/status/1744197461802152266#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744197461802152266#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 03:20:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有人在 Binance 上购买了 26.9 个BTC<br />
<br />
然后将其发送到了中本聪的死亡钱包中...<br />
<br />
这是什么操作？？？<br />
<br />
除非是中本聪本人才干得出这种事情吧？？？<br />
<br />
🤣</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RTaUZ5a2FzQUFQcFFiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744188783908770222#m</id>
            <title>XREAL推出其新款AR眼镜：XREAL Air 2 Ultra

这款眼镜配备了双3D环境传感器，具有六自由度（6DoF）功能，能精准追踪头部运动，提供更沉浸式和真实的AR体验。

配备有计算机视觉能力，刷新率高达120Hz，最高亮度500尼特。

可获得在4米距离内观看154英寸虚拟2D屏幕投影的视觉效果。

钛合金设计，非常轻便！

XREAL Air 2 Ultra配备了一对具有计算机视觉功能的3D环境传感器，就类似双目鱼眼摄像头，可通过定位和映射来确定用户在3D空间中的位置。

这枚3D环境传感器专为开发人员创建AR应用程序和探索新的空间计算体验而设计，支持手部追踪、3D网格创建、语义场景理解、以及潜在AI拓展等功能。

XREAL Air 2 Ultra的主要功能特点：

1、六自由度（6DoF）AR眼镜：这意味着这款AR眼镜能够精准追踪用户的头部运动，包括前后、上下、左右移动，以及绕三个轴线的旋转（即俯仰、滚转和偏航）。这种追踪能力使用户在虚拟环境中能够自然地移动和查看物体，提供更沉浸式和真实的增强现实体验。

2、高清显示体验：为每只眼睛提供全高清视觉体验，视场（FOV）为52°，每度像素数（PPD）为42，刷新率高达120Hz，最高亮度为500尼特，确保在多种光照条件下图像清晰生动。

3、优质音效：具有电影级音频和定向音频技术，XREAL Air 2 Ultra支持定向音频技术，可减少声音扩散，从而更好地保护个人隐私，并降低对他人的干扰。

4、与Apple的空间视频特性兼容：支持将iPhone 15 Pro拍摄的空间视频转换成常规的左右格式，无需昂贵的Apple Vision Pro即可在XREAL Air 2系列眼镜上观看。

5、舒适的佩戴体验：采用轻巧的钛合金眼镜框，仅重80克，设计考虑了最佳的重量分布，可调节的太阳镜臂和鼻垫，确保舒适佩戴和最佳的图像对准。

6、眼部健康认证：每副XREAL Air 2 Ultra眼镜均通过TÜV Rheinland认证，确保色彩准确、舒适、低蓝光和无闪烁使用，同时超过了ISO标准，提供优质的图像质量和观看舒适度。

7、开发者支持：提供最新的NRSDK 2.2开发工具包，支持手势跟踪、深度网格、空间锚点等空间计算能力。

目前这款产品已经可以预订，价格为699美元，Nreal Light 的用户预定立减100美金。预计将在2024年3月开始发货。https://developer.xreal.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744188783908770222#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744188783908770222#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 02:46:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XREAL推出其新款AR眼镜：XREAL Air 2 Ultra<br />
<br />
这款眼镜配备了双3D环境传感器，具有六自由度（6DoF）功能，能精准追踪头部运动，提供更沉浸式和真实的AR体验。<br />
<br />
配备有计算机视觉能力，刷新率高达120Hz，最高亮度500尼特。<br />
<br />
可获得在4米距离内观看154英寸虚拟2D屏幕投影的视觉效果。<br />
<br />
钛合金设计，非常轻便！<br />
<br />
XREAL Air 2 Ultra配备了一对具有计算机视觉功能的3D环境传感器，就类似双目鱼眼摄像头，可通过定位和映射来确定用户在3D空间中的位置。<br />
<br />
这枚3D环境传感器专为开发人员创建AR应用程序和探索新的空间计算体验而设计，支持手部追踪、3D网格创建、语义场景理解、以及潜在AI拓展等功能。<br />
<br />
XREAL Air 2 Ultra的主要功能特点：<br />
<br />
1、六自由度（6DoF）AR眼镜：这意味着这款AR眼镜能够精准追踪用户的头部运动，包括前后、上下、左右移动，以及绕三个轴线的旋转（即俯仰、滚转和偏航）。这种追踪能力使用户在虚拟环境中能够自然地移动和查看物体，提供更沉浸式和真实的增强现实体验。<br />
<br />
2、高清显示体验：为每只眼睛提供全高清视觉体验，视场（FOV）为52°，每度像素数（PPD）为42，刷新率高达120Hz，最高亮度为500尼特，确保在多种光照条件下图像清晰生动。<br />
<br />
3、优质音效：具有电影级音频和定向音频技术，XREAL Air 2 Ultra支持定向音频技术，可减少声音扩散，从而更好地保护个人隐私，并降低对他人的干扰。<br />
<br />
4、与Apple的空间视频特性兼容：支持将iPhone 15 Pro拍摄的空间视频转换成常规的左右格式，无需昂贵的Apple Vision Pro即可在XREAL Air 2系列眼镜上观看。<br />
<br />
5、舒适的佩戴体验：采用轻巧的钛合金眼镜框，仅重80克，设计考虑了最佳的重量分布，可调节的太阳镜臂和鼻垫，确保舒适佩戴和最佳的图像对准。<br />
<br />
6、眼部健康认证：每副XREAL Air 2 Ultra眼镜均通过TÜV Rheinland认证，确保色彩准确、舒适、低蓝光和无闪烁使用，同时超过了ISO标准，提供优质的图像质量和观看舒适度。<br />
<br />
7、开发者支持：提供最新的NRSDK 2.2开发工具包，支持手势跟踪、深度网格、空间锚点等空间计算能力。<br />
<br />
目前这款产品已经可以预订，价格为699美元，Nreal Light 的用户预定立减100美金。预计将在2024年3月开始发货。<a href="https://developer.xreal.com/">developer.xreal.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQxODgzMDY2MjYzNDcwMDgvcHUvaW1nL3NqSjRiS1QteUo5TndFZ2IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744179160434802963#m</id>
            <title>Teachable Machine：一个由Google开发的机器学习工具

它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。

你可以用它来教电脑识别图片、声音或人的动作。

使用这个工具的步骤很简单：

1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。

2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。

3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。

Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。

1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。

2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。

3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。

开始训练：https://teachablemachine.withgoogle.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744179160434802963#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744179160434802963#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 02:08:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Teachable Machine：一个由Google开发的机器学习工具<br />
<br />
它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。<br />
<br />
你可以用它来教电脑识别图片、声音或人的动作。<br />
<br />
使用这个工具的步骤很简单：<br />
<br />
1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。<br />
<br />
2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。<br />
<br />
3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。<br />
<br />
Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。<br />
<br />
1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。<br />
<br />
2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。<br />
<br />
3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。<br />
<br />
开始训练：<a href="https://teachablemachine.withgoogle.com/">teachablemachine.withgoogle.…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5OTM4OTc5NTE1OTI0NDgvcHUvaW1nL1pRUld0cmFLVVR0TkJxWTIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744016632211726742#m</id>
            <title>🔔 http://Xiaohu.AI日报「1月7日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1744016632211726742#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744016632211726742#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 15:22:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔 <a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「1月7日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RQOTRON2JVQUFxc0paLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743998321977672058#m</id>
            <title>机器人迎来它的ChatGPT 时刻？

机器人初创公司@Figure_robot 发布了一段视频

他们家的Figure-01机器人现在可以自己煮咖啡了

这是一个使用了端到端的人工智能系统，仅通过观察人类制作咖啡的录像，10小时内学会了制作咖啡的技能。

机器人通过神经网络来处理和分析视频数据。通过观看如何制作咖啡的录像。学习人类的动作和手势，然后模仿这些动作来学习制作咖啡的过程。

无需通过编程，机器人自主学习技能。

早前FigureCEO Brett Adcock @adcock_brett 称他们刚刚取得了人工智能突破 。

机器人技术即将迎来它的ChatGPT 时刻！

说的是不是这个？</title>
            <link>https://nitter.cz/xiaohuggg/status/1743998321977672058#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743998321977672058#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 14:09:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>机器人迎来它的ChatGPT 时刻？<br />
<br />
机器人初创公司<a href="https://nitter.cz/Figure_robot" title="Figure">@Figure_robot</a> 发布了一段视频<br />
<br />
他们家的Figure-01机器人现在可以自己煮咖啡了<br />
<br />
这是一个使用了端到端的人工智能系统，仅通过观察人类制作咖啡的录像，10小时内学会了制作咖啡的技能。<br />
<br />
机器人通过神经网络来处理和分析视频数据。通过观看如何制作咖啡的录像。学习人类的动作和手势，然后模仿这些动作来学习制作咖啡的过程。<br />
<br />
无需通过编程，机器人自主学习技能。<br />
<br />
早前FigureCEO Brett Adcock <a href="https://nitter.cz/adcock_brett" title="Brett Adcock">@adcock_brett</a> 称他们刚刚取得了人工智能突破 。<br />
<br />
机器人技术即将迎来它的ChatGPT 时刻！<br />
<br />
说的是不是这个？</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5OTU4Mzg0OTY2ODE5ODQvcHUvaW1nL0dsdlV6YmlIeVppdXdFZ0MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743988216787870025#m</id>
            <title>R to @xiaohuggg: 在线体验地址：https://www.modelscope.cn/studios/XR-3D/InstructDynamicAvatar/summary</title>
            <link>https://nitter.cz/xiaohuggg/status/1743988216787870025#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743988216787870025#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 13:29:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在线体验地址：<a href="https://www.modelscope.cn/studios/XR-3D/InstructDynamicAvatar/summary">modelscope.cn/studios/XR-3D/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5ODgxMjY5NDU4MDQyODgvcHUvaW1nL2hBLVdlQ0ZyUVVVWjFBdkwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743986486780076279#m</id>
            <title>兄弟们 阿里又整活了，这次是个大活！

Make-A-Character：一句话生成超逼真的3D数字人

你只需要通过文字描述人的脸型、五官、发型等特征，它就能在不到2分钟的时间内生成一个超逼真的3D角色。

而且你可以自定义面部特征，例如脸型、眼睛颜色、发型、眉毛类型、嘴巴和鼻子，以及添加皱纹和雀斑等。

Make-A-Character（MACH）的主要特点：

1、可控制性：用户可以详细自定义面部特征，例如脸型、眼睛颜色、发型、眉毛类型、嘴巴和鼻子，以及添加皱纹和雀斑等。

2、高度逼真：角色基于真实人类扫描数据集生成，发型为实际发丝而非网格，使用Unreal Engine的物理基础渲染（PBR）技术渲染，以实现高质量实时渲染效果。

3、完整模型：每个角色都是完整模型，包括眼睛、舌头、牙齿、全身和服装，无需额外建模即可立即使用。

4、可动画化：角色配备复杂的骨骼装置，支持标准动画，增强其逼真外观和多样化应用。

5、行业兼容：生成的3D角色可以无缝集成到现有的计算机图形（CG）工作流程中，特别是在游戏和电影行业中常用的工作流程。

MACH支持英文和中文提示，可根据详细的文本描述快速生成3D角色，例如“圆脸胖女士”或“棕皮肤戴黑眼镜的男孩，绿色头发”等等。

工作原理：

Make-A-Character（MACH）结合了大语言模型、视觉模型和3D生成技术。

1、文本解析：首先，MACH使用大语言模型（比如GPT类模型）来理解用户输入的文本描述。这个过程中，它会识别出文本中提到的各种面部特征，例如脸型、眼睛形状、嘴巴形状、发型和颜色等。

2、视觉映射：接着，这些语义属性（如脸型、眼睛形状等）被映射到对应的视觉线索上。这意味着系统会根据文本中的描述生成一个参考的人脸图像。这个步骤通常使用像“Stable Diffusion”这样的图像生成模型来完成。

3、2D面部解析：生成的参考图像接下来会经过2D面部解析过程，这一过程涉及到对人脸的不同部分进行识别和分割。

4、3D生成：基于面部解析的结果，MACH开始生成目标角色的3D网格和纹理。这个过程包括创建角色的3D模型，并且将纹理（如皮肤、头发等）应用到模型上。

5、附加配件：如果文本描述中提到了其他配件（如眼镜、帽子等），这些也会在这一步骤中添加到3D角色上。

6、参数化表示和动画：最终生成的3D角色是参数化的，这意味着可以容易地对其进行动画处理，比如添加行走、说话等动作。

通过这些步骤，MACH可以快速从简单的文本描述中生成逼真的、完整的、可动画化的3D角色，适用于各种娱乐和专业场景。

项目及演示：https://human3daigc.github.io/MACH/
论文：https://github.com/Human3DAIGC/Make-A-Character
GitHub：https://arxiv.org/pdf/2312.15430.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1743986486780076279#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743986486780076279#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 13:22:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 阿里又整活了，这次是个大活！<br />
<br />
Make-A-Character：一句话生成超逼真的3D数字人<br />
<br />
你只需要通过文字描述人的脸型、五官、发型等特征，它就能在不到2分钟的时间内生成一个超逼真的3D角色。<br />
<br />
而且你可以自定义面部特征，例如脸型、眼睛颜色、发型、眉毛类型、嘴巴和鼻子，以及添加皱纹和雀斑等。<br />
<br />
Make-A-Character（MACH）的主要特点：<br />
<br />
1、可控制性：用户可以详细自定义面部特征，例如脸型、眼睛颜色、发型、眉毛类型、嘴巴和鼻子，以及添加皱纹和雀斑等。<br />
<br />
2、高度逼真：角色基于真实人类扫描数据集生成，发型为实际发丝而非网格，使用Unreal Engine的物理基础渲染（PBR）技术渲染，以实现高质量实时渲染效果。<br />
<br />
3、完整模型：每个角色都是完整模型，包括眼睛、舌头、牙齿、全身和服装，无需额外建模即可立即使用。<br />
<br />
4、可动画化：角色配备复杂的骨骼装置，支持标准动画，增强其逼真外观和多样化应用。<br />
<br />
5、行业兼容：生成的3D角色可以无缝集成到现有的计算机图形（CG）工作流程中，特别是在游戏和电影行业中常用的工作流程。<br />
<br />
MACH支持英文和中文提示，可根据详细的文本描述快速生成3D角色，例如“圆脸胖女士”或“棕皮肤戴黑眼镜的男孩，绿色头发”等等。<br />
<br />
工作原理：<br />
<br />
Make-A-Character（MACH）结合了大语言模型、视觉模型和3D生成技术。<br />
<br />
1、文本解析：首先，MACH使用大语言模型（比如GPT类模型）来理解用户输入的文本描述。这个过程中，它会识别出文本中提到的各种面部特征，例如脸型、眼睛形状、嘴巴形状、发型和颜色等。<br />
<br />
2、视觉映射：接着，这些语义属性（如脸型、眼睛形状等）被映射到对应的视觉线索上。这意味着系统会根据文本中的描述生成一个参考的人脸图像。这个步骤通常使用像“Stable Diffusion”这样的图像生成模型来完成。<br />
<br />
3、2D面部解析：生成的参考图像接下来会经过2D面部解析过程，这一过程涉及到对人脸的不同部分进行识别和分割。<br />
<br />
4、3D生成：基于面部解析的结果，MACH开始生成目标角色的3D网格和纹理。这个过程包括创建角色的3D模型，并且将纹理（如皮肤、头发等）应用到模型上。<br />
<br />
5、附加配件：如果文本描述中提到了其他配件（如眼镜、帽子等），这些也会在这一步骤中添加到3D角色上。<br />
<br />
6、参数化表示和动画：最终生成的3D角色是参数化的，这意味着可以容易地对其进行动画处理，比如添加行走、说话等动作。<br />
<br />
通过这些步骤，MACH可以快速从简单的文本描述中生成逼真的、完整的、可动画化的3D角色，适用于各种娱乐和专业场景。<br />
<br />
项目及演示：<a href="https://human3daigc.github.io/MACH/">human3daigc.github.io/MACH/</a><br />
论文：<a href="https://github.com/Human3DAIGC/Make-A-Character">github.com/Human3DAIGC/Make-…</a><br />
GitHub：<a href="https://arxiv.org/pdf/2312.15430.pdf">arxiv.org/pdf/2312.15430.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5ODM3MjM5OTE5NTc1MDQvcHUvaW1nL094R3hBd0hXMWYxVnY5dk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743974420279128261#m</id>
            <title>今天有人给我评论我点进去一看

竟然写着是12岁的小朋友😐

学习AI半年了，下面是她的学习成果总结！

我和他聊了下感觉他说话成熟质疑他年龄，他就发推表示了抗议😂

他说他12就自学考过了国内某大学英语专业的三门课程！挺有意思的，你们可以关注下给他加加粉！@zengaihua327711  😏</title>
            <link>https://nitter.cz/xiaohuggg/status/1743974420279128261#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743974420279128261#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 12:34:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天有人给我评论我点进去一看<br />
<br />
竟然写着是12岁的小朋友😐<br />
<br />
学习AI半年了，下面是她的学习成果总结！<br />
<br />
我和他聊了下感觉他说话成熟质疑他年龄，他就发推表示了抗议😂<br />
<br />
他说他12就自学考过了国内某大学英语专业的三门课程！挺有意思的，你们可以关注下给他加加粉！<a href="https://nitter.cz/zengaihua327711" title="潮影za dream">@zengaihua327711</a>  😏</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RQWGNIWmJFQUFuYWo4LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RQWGNIVWJJQUFfdTZyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743923334407127145#m</id>
            <title>R to @xiaohuggg: 测试结果：

文字位置不好控制，需要不断的调整和练习、

字体什么的也需要不断尝试

多加练习相信可以熟练掌握的</title>
            <link>https://nitter.cz/xiaohuggg/status/1743923334407127145#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743923334407127145#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 09:11:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果：<br />
<br />
文字位置不好控制，需要不断的调整和练习、<br />
<br />
字体什么的也需要不断尝试<br />
<br />
多加练习相信可以熟练掌握的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RPb3JQRWFRQUFhSzNRLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RPb3JQMmJNQUFuZTdFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RPb3JQM2E0QUFYNGh5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743922778053718318#m</id>
            <title>教程：使用Midjourney v6 创建自己的服装品牌 😃

博主分享了他使用Midjourney构建虚拟复制品牌的经验

由于Midjourney现在能够处理文本并且具有更好的自然语言理解能力。

现在可以通过指定服装风格（如街头、休闲、商务）和品牌名称来创建想象中的品牌。

同时可以在特定位置（如胸口、口袋、袖子）绣上品牌名。

提示模板：

💬  wearing , casual, business> from a brand named "", the brand name is embroidered on , pocket, sleeves>. .  --ar 2:3 --style raw --v 6

💬穿着, casual, business>来自名为"”的品牌，品牌名称绣在, pocket, sleeves >. < 服装细节>.  --ar 2:3 --style raw --v 6

⚠️注意：想要在图片上显示的文字必须用“引号”将文本括起来。</title>
            <link>https://nitter.cz/xiaohuggg/status/1743922778053718318#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743922778053718318#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 09:09:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>教程：使用Midjourney v6 创建自己的服装品牌 😃<br />
<br />
博主分享了他使用Midjourney构建虚拟复制品牌的经验<br />
<br />
由于Midjourney现在能够处理文本并且具有更好的自然语言理解能力。<br />
<br />
现在可以通过指定服装风格（如街头、休闲、商务）和品牌名称来创建想象中的品牌。<br />
<br />
同时可以在特定位置（如胸口、口袋、袖子）绣上品牌名。<br />
<br />
提示模板：<br />
<br />
💬  wearing  from a brand named "", the brand name is embroidered on . .  --ar 2:3 --style raw --v 6<br />
<br />
💬穿着来自名为"<名称>”的品牌，品牌名称绣在<span> >. < 服装细节>. <拍摄细节> --ar 2:3 --style raw --v 6<br />
<br />
⚠️注意：想要在图片上显示的文字必须用“引号”将文本括起来。</p>
<p><a href="https://nitter.cz/hugovntr/status/1742620839969689821#m">nitter.cz/hugovntr/status/1742620839969689821#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RPQmg0VWFJQUE2d0RJLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RPQ0ZldGJBQUFCdFAwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>