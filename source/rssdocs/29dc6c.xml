<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733487530698514557#m</id>
            <title>额 这么牛叉

俩角色

能打架互动了✨🫡不知道咋弄的！</title>
            <link>https://nitter.cz/xiaohuggg/status/1733487530698514557#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733487530698514557#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 14:03:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>额 这么牛叉<br />
<br />
俩角色<br />
<br />
能打架互动了✨🫡不知道咋弄的！</p>
<p><a href="https://nitter.cz/AIWarper/status/1733344112605384734#m">nitter.cz/AIWarper/status/1733344112605384734#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/lyson_ober/status/1733447932672921900#m</id>
            <title>RT by @xiaohuggg: 🔥 Meta 使用了 11 亿张来源于 Facebook 和 Instagram 上的图像来训练 AI 模型，在这里可以体验他们的「免费」文生图（text-to-image）产品：http://imagine.meta.com

我测试了一下发现配合 @Magnific_AI 可以生成出效果非常不错的图像，以下是对比图。</title>
            <link>https://nitter.cz/lyson_ober/status/1733447932672921900#m</link>
            <guid isPermaLink="false">https://nitter.cz/lyson_ober/status/1733447932672921900#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 11:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔥 Meta 使用了 11 亿张来源于 Facebook 和 Instagram 上的图像来训练 AI 模型，在这里可以体验他们的「免费」文生图（text-to-image）产品：<a href="http://imagine.meta.com">imagine.meta.com</a><br />
<br />
我测试了一下发现配合 <a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 可以生成出效果非常不错的图像，以下是对比图。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E1eHMyamFBQUFhWVNqLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733417404099887532#m</id>
            <title>Material Palette：从单张真实世界图片中提取 PBR 材料

该工具可以从一张普通照片中提取出各种建筑PBR材料（比如木头、金属、砖瓦的质感）。

首先通过分析给定照片，识别出照片中不同部分对应的材料是什么。

然后，它进一步分析这些纹理图像，提取出材料的不同特性，比如颜色、光泽和表面的粗糙度。

这对于建筑装修行业以及需要在电脑里制作或编辑三维场景的人来说特别有用，因为它可以让虚拟场景里的物体看起来更像真的。

Material Palette" 的主要功能和工作原理如下：

主要功能：

1、提取 PBR 材料：从单张真实世界的图片中提取物理基础渲染（Physically-Based Rendering, PBR）材料，包括反照率、法线和粗糙度。

它提取以下几个方面的材料特性：

1、纹理图像：使用扩散模型生成类似于场景中每种材料的纹理图像。这些纹理图像模仿了真实世界中材料的外观。

2、空间变化的双向反射分布函数（SVBRDFs）：将生成的纹理图像进一步分解为 SVBRDFs。SVBRDFs 是一种描述材料如何与光互动的模型，

包括以下几个方面：

反照率（Albedo）：材料的基本颜色和纹理。

法线（Normal）：材料表面的微观几何结构，影响光线反射的方式。

粗糙度（Roughness）：材料表面的光滑或粗糙程度，影响光线散射的特性。

3、生成逼真纹理：使用扩散模型生成类似于场景中每种材料的纹理图像。

SVBRDF 分解：将生成的纹理图像分解为空间变化的双向反射分布函数（Spatially Varying BRDFs, SVBRDFs）。

工作原理：

1、文本到图像的扩散模型：首先，项目使用微调的文本到图像扩散模型来生成类似于场景中每种材料的纹理图像。这一步骤基于对场景中存在的材料类型的理解。

2、多任务网络：接着，使用一个多任务网络将这些生成的纹理图像分解为 SVBRDFs。这包括提取材料的反照率、法线和粗糙度等属性。

3、材料的视觉呈现：通过这种方法，可以从单张图片中提取出具有高度真实感的材料属性，这些属性可以用于 3D 渲染和其他视觉效果应用中。

Material Palette利用先进的 AI 技术从单张图片中提取高质量的材料属性，这些属性对于创建逼真的 3D 场景和视觉效果至关重要。

这有几个实际应用：

1、3D 渲染和视觉效果：在电影、游戏和虚拟现实中，可以使用这些提取的材料来创建逼真的 3D 模型和场景。

2、设计和建筑可视化：建筑师和设计师可以利用这些材料来增强他们的视觉呈现，使设计更加真实和吸引人。

3、增强现实（AR）和虚拟现实（VR）：在 AR 和 VR 应用中，提取的材料可以用来改善用户的沉浸体验，使虚拟对象看起来更加真实。

4、艺术和创意产业：艺术家和创意专业人士可以使用这些材料来探索新的艺术表现形式，或在他们的作品中增加更多细节和真实感。

5、教育和培训：在教育和培训模拟中，这些材料可以用来创建更逼真的环境，帮助学习者更好地理解和互动。

项目及演示:https://astra-vision.github.io/MaterialPalette/
论文：https://arxiv.org/abs/2311.17060
GitHub：https://github.com/astra-vision/MaterialPalette</title>
            <link>https://nitter.cz/xiaohuggg/status/1733417404099887532#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733417404099887532#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 09:24:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Material Palette：从单张真实世界图片中提取 PBR 材料<br />
<br />
该工具可以从一张普通照片中提取出各种建筑PBR材料（比如木头、金属、砖瓦的质感）。<br />
<br />
首先通过分析给定照片，识别出照片中不同部分对应的材料是什么。<br />
<br />
然后，它进一步分析这些纹理图像，提取出材料的不同特性，比如颜色、光泽和表面的粗糙度。<br />
<br />
这对于建筑装修行业以及需要在电脑里制作或编辑三维场景的人来说特别有用，因为它可以让虚拟场景里的物体看起来更像真的。<br />
<br />
Material Palette" 的主要功能和工作原理如下：<br />
<br />
主要功能：<br />
<br />
1、提取 PBR 材料：从单张真实世界的图片中提取物理基础渲染（Physically-Based Rendering, PBR）材料，包括反照率、法线和粗糙度。<br />
<br />
它提取以下几个方面的材料特性：<br />
<br />
1、纹理图像：使用扩散模型生成类似于场景中每种材料的纹理图像。这些纹理图像模仿了真实世界中材料的外观。<br />
<br />
2、空间变化的双向反射分布函数（SVBRDFs）：将生成的纹理图像进一步分解为 SVBRDFs。SVBRDFs 是一种描述材料如何与光互动的模型，<br />
<br />
包括以下几个方面：<br />
<br />
反照率（Albedo）：材料的基本颜色和纹理。<br />
<br />
法线（Normal）：材料表面的微观几何结构，影响光线反射的方式。<br />
<br />
粗糙度（Roughness）：材料表面的光滑或粗糙程度，影响光线散射的特性。<br />
<br />
3、生成逼真纹理：使用扩散模型生成类似于场景中每种材料的纹理图像。<br />
<br />
SVBRDF 分解：将生成的纹理图像分解为空间变化的双向反射分布函数（Spatially Varying BRDFs, SVBRDFs）。<br />
<br />
工作原理：<br />
<br />
1、文本到图像的扩散模型：首先，项目使用微调的文本到图像扩散模型来生成类似于场景中每种材料的纹理图像。这一步骤基于对场景中存在的材料类型的理解。<br />
<br />
2、多任务网络：接着，使用一个多任务网络将这些生成的纹理图像分解为 SVBRDFs。这包括提取材料的反照率、法线和粗糙度等属性。<br />
<br />
3、材料的视觉呈现：通过这种方法，可以从单张图片中提取出具有高度真实感的材料属性，这些属性可以用于 3D 渲染和其他视觉效果应用中。<br />
<br />
Material Palette利用先进的 AI 技术从单张图片中提取高质量的材料属性，这些属性对于创建逼真的 3D 场景和视觉效果至关重要。<br />
<br />
这有几个实际应用：<br />
<br />
1、3D 渲染和视觉效果：在电影、游戏和虚拟现实中，可以使用这些提取的材料来创建逼真的 3D 模型和场景。<br />
<br />
2、设计和建筑可视化：建筑师和设计师可以利用这些材料来增强他们的视觉呈现，使设计更加真实和吸引人。<br />
<br />
3、增强现实（AR）和虚拟现实（VR）：在 AR 和 VR 应用中，提取的材料可以用来改善用户的沉浸体验，使虚拟对象看起来更加真实。<br />
<br />
4、艺术和创意产业：艺术家和创意专业人士可以使用这些材料来探索新的艺术表现形式，或在他们的作品中增加更多细节和真实感。<br />
<br />
5、教育和培训：在教育和培训模拟中，这些材料可以用来创建更逼真的环境，帮助学习者更好地理解和互动。<br />
<br />
项目及演示:<a href="https://astra-vision.github.io/MaterialPalette/">astra-vision.github.io/Mater…</a><br />
论文：<a href="https://arxiv.org/abs/2311.17060">arxiv.org/abs/2311.17060</a><br />
GitHub：<a href="https://github.com/astra-vision/MaterialPalette">github.com/astra-vision/Mate…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzM0MTQ4NzQ1NDg3MTE0MjQvcHUvaW1nL2U0alVSOERVSzZZSTZfUDUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733408732544139416#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1733408732544139416#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733408732544139416#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:50:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0E1TnYzNWJJQUFuZ2RWLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBNU52MzViSUFBbmdkVi5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733408730258321686#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1733408730258321686#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733408730258321686#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:50:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0E1T0NRUWEwQUE5eGNHLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBNU9DUVFhMEFBOXhjRy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733408727645245622#m</id>
            <title>R to @xiaohuggg: 一些效果展示</title>
            <link>https://nitter.cz/xiaohuggg/status/1733408727645245622#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733408727645245622#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:50:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些效果展示</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0E1TnRjeWJJQUFmTkZlLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBNU50Y3liSUFBZk5GZS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733408725522899316#m</id>
            <title>Wigglypaint：一个独特的、有趣的绘画工具

这个工具最大的特点是它的“多汁”和“摇晃”的绘画效果。

这些效果使得绘制的线条和图形在屏幕上好像在轻微地摇晃或震动，具有一种生动、活泼的感觉。

还配有动感的绘画声音，挺有意思的。

支持导出为gif动图...

在线体验：https://internet-janitor.itch.io/wigglypaint</title>
            <link>https://nitter.cz/xiaohuggg/status/1733408725522899316#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733408725522899316#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:50:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wigglypaint：一个独特的、有趣的绘画工具<br />
<br />
这个工具最大的特点是它的“多汁”和“摇晃”的绘画效果。<br />
<br />
这些效果使得绘制的线条和图形在屏幕上好像在轻微地摇晃或震动，具有一种生动、活泼的感觉。<br />
<br />
还配有动感的绘画声音，挺有意思的。<br />
<br />
支持导出为gif动图...<br />
<br />
在线体验：<a href="https://internet-janitor.itch.io/wigglypaint">internet-janitor.itch.io/wig…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzM0MDc5NjY1MjU4NTM2OTYvcHUvaW1nL1NDSkxPZmd6T1R1WmpVc2suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733400042298540202#m</id>
            <title>据《时代》杂志报道，使用AI“脱衣”的应用和网站正迅速流行。

仅9月份，就有2400 万人访问了这类脱衣网站。

自今年年初以来，社交媒体（包括 X 和 Reddit）上的脱衣应用广告链接数量增加了2400% 以上。

扩散模型的发布是导致使用AI制作非自愿色情内容的应用和网站增加的主要原因。

这些先进的AI技术可以免费获取，使得开发者能够更容易地创建出质量更高的图像。

这些服务使用 AI 重建图像，使图中的人物裸露。

原文：https://time.com/6344068/nudify-apps-undress-photos-women-artificial-intelligence/</title>
            <link>https://nitter.cz/xiaohuggg/status/1733400042298540202#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733400042298540202#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:15:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据《时代》杂志报道，使用AI“脱衣”的应用和网站正迅速流行。<br />
<br />
仅9月份，就有2400 万人访问了这类脱衣网站。<br />
<br />
自今年年初以来，社交媒体（包括 X 和 Reddit）上的脱衣应用广告链接数量增加了2400% 以上。<br />
<br />
扩散模型的发布是导致使用AI制作非自愿色情内容的应用和网站增加的主要原因。<br />
<br />
这些先进的AI技术可以免费获取，使得开发者能够更容易地创建出质量更高的图像。<br />
<br />
这些服务使用 AI 重建图像，使图中的人物裸露。<br />
<br />
原文：<a href="https://time.com/6344068/nudify-apps-undress-photos-women-artificial-intelligence/">time.com/6344068/nudify-apps…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E1R0piaWFJQUEtaEx5LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733359311152439549#m</id>
            <title>周末开心一下

🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1733359311152439549#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733359311152439549#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 05:33:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>周末开心一下<br />
<br />
🤓</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMzMzU5MDgyMTY4NTQ1MjgwL2ltZy9CZVBYMS1WZmJ2OFo5d2lHLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733349917501141390#m</id>
            <title>Wikimedia Wikisource 数据集，现在已经在 Hugging Face Hub 上提供。

- 数据集包含了来自 Wikimedia Wikisource 的最新转储
- 涵盖了 73 种不同的语言
- 数据以 Parquet 格式提供
- 可用来增强语言模型，更好地理解和生成文本
- 免费使用

下载：https://huggingface.co/datasets/wikimedia/wikisource</title>
            <link>https://nitter.cz/xiaohuggg/status/1733349917501141390#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733349917501141390#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:56:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wikimedia Wikisource 数据集，现在已经在 Hugging Face Hub 上提供。<br />
<br />
- 数据集包含了来自 Wikimedia Wikisource 的最新转储<br />
- 涵盖了 73 种不同的语言<br />
- 数据以 Parquet 格式提供<br />
- 可用来增强语言模型，更好地理解和生成文本<br />
- 免费使用<br />
<br />
下载：<a href="https://huggingface.co/datasets/wikimedia/wikisource">huggingface.co/datasets/wiki…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1725726053212312046#m">nitter.cz/xiaohuggg/status/1725726053212312046#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzEyNTgxOTk0NDM0MTUwNC9ZeWNGSlpScj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733336646509289865#m</id>
            <title>Google Gemini 最新演示

测试Gemini能否理解使用Emoji Kitchen的表情符号创建的一些非常规的的Emoji图像 ！

 Emoji Kitchen可以允许你组合不同的表情符号来创建新的表情符号 。

这个演示测试了Gemini能否理解如何使用 Emoji Kitchen 创建一些不寻常的非常规的表情符号 👀</title>
            <link>https://nitter.cz/xiaohuggg/status/1733336646509289865#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733336646509289865#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:03:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Gemini 最新演示<br />
<br />
测试Gemini能否理解使用Emoji Kitchen的表情符号创建的一些非常规的的Emoji图像 ！<br />
<br />
 Emoji Kitchen可以允许你组合不同的表情符号来创建新的表情符号 。<br />
<br />
这个演示测试了Gemini能否理解如何使用 Emoji Kitchen 创建一些不寻常的非常规的表情符号 👀</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMzMzU5NTQ0MTEzNjQzNTIvcHUvaW1nL2RiS0JJYlBzOEd5SXdfVmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733153199555834351#m</id>
            <title>R to @xiaohuggg: 官方演示效果：

总体感觉是很不错的，而且是开源免费的，这让@Magnific_AI 陷入了尴尬境地。

不过Magnific AI对普通用户来说还是比较易用的。

上手成本比较低...</title>
            <link>https://nitter.cz/xiaohuggg/status/1733153199555834351#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733153199555834351#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 15:54:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>官方演示效果：<br />
<br />
总体感觉是很不错的，而且是开源免费的，这让<a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 陷入了尴尬境地。<br />
<br />
不过Magnific AI对普通用户来说还是比较易用的。<br />
<br />
上手成本比较低...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMxNTI1OTc0ODQ0NDk3OTIvcHUvaW1nL0RNdzdmZWhpNkZyMzhvU2QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733152544208327089#m</id>
            <title>DemoFusion：超更高分辨率的图像生成 

一个图像增强工具，它可以提高 SDXL的图像生成分辨率，可以把生成图像的分辨率提高4倍、16倍，甚至更高。

它不仅能让图片变清晰，还能改善图片中的小细节（比如纹理和边缘）从而生成更自然和逼真的图像。

这直接把 @Magnific_AI 整不会了！😂

DemoFusion还可以无缝集成到基于 LDM 的多种应用中，如ControlNet...还能够放大真实图像...

主要功能特点：

1、高分辨率图像生成：DemoFusion 专注于利用潜在扩散模型（LDMs）生成更高分辨率的图像，突破了传统图像生成技术的限制。

2、渐进式上采样：该框架通过逐步提高图像的分辨率来生成更清晰、更详细的图像。这种渐进式方法允许更精细地控制图像质量。它会逐步提高图片的清晰度，这样你可以先看到一个大概的效果，然后再慢慢变得更清晰。

3、跳过残差和扩张采样机制：DemoFusion 使用这些先进的技术来改善图像的局部细节和全局一致性，从而生成更自然和逼真的图像。

4、与 ControlNet 的集成：可以无缝集成到基于 LDM 的多种应用中，例如与 ControlNet 结合，实现可控的高分辨率图像生成。

5、放大真实图像：还能够放大真实图像，通过编码的真实图像表示来替换初始阶段的输出，实现图像的高分辨率放大。

6、无需大量内存和调整：DemoFusion 设计得既高效又易于使用，不需要大量的内存资源或复杂的调整过程。

技术细节：

1、渐进式上采样（Progressive Upscaling）：这种方法涉及逐步提高图像的分辨率。

DemoFusion 从较低分辨率的图像开始，然后通过一个“上采样-扩散-去噪”循环逐渐提升图像的分辨率。
在每个循环中，先对图像进行上采样（增加像素数量），然后通过扩散和去噪过程来提高图像质量。

2、跳过残差（Skip Residual）：在“上采样-扩散-去噪”循环的每个阶段，DemoFusion 使用来自前一个扩散过程的中间噪声逆转表示作为跳过残差。

这有助于在高分辨率和低分辨率图像之间保持全局一致性。

3、扩张采样（Dilated Sampling）：为了在局部去噪路径中建立全局去噪路径，DemoFusion 引入了扩张采样。

这种方法促进了更全局一致的内容生成，有助于在整个图像中保持语义上的连贯性。

4、与现有模型的集成：DemoFusion 可以作为一个插件般地扩展现有的图像生成模型，如 SDXL。

它不需要额外的训练，可以直接应用于现有模型，提供分辨率的显著提升。

项目及演示：https://ruoyidu.github.io/demofusion/demofusion.html
论文：

https://drive.google.com/file/d/1pAWCfpEgwy4UAkUqTGDuqypJt-5bl8se/view?usp=sharing

GitHub：https://github.com/PRIS-CV/DemoFusion

Demo演示：https://replicate.com/lucataco/demofusion

在线体验由 @radamar 提供：https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL</title>
            <link>https://nitter.cz/xiaohuggg/status/1733152544208327089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733152544208327089#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 15:52:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DemoFusion：超更高分辨率的图像生成 <br />
<br />
一个图像增强工具，它可以提高 SDXL的图像生成分辨率，可以把生成图像的分辨率提高4倍、16倍，甚至更高。<br />
<br />
它不仅能让图片变清晰，还能改善图片中的小细节（比如纹理和边缘）从而生成更自然和逼真的图像。<br />
<br />
这直接把 <a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 整不会了！😂<br />
<br />
DemoFusion还可以无缝集成到基于 LDM 的多种应用中，如ControlNet...还能够放大真实图像...<br />
<br />
主要功能特点：<br />
<br />
1、高分辨率图像生成：DemoFusion 专注于利用潜在扩散模型（LDMs）生成更高分辨率的图像，突破了传统图像生成技术的限制。<br />
<br />
2、渐进式上采样：该框架通过逐步提高图像的分辨率来生成更清晰、更详细的图像。这种渐进式方法允许更精细地控制图像质量。它会逐步提高图片的清晰度，这样你可以先看到一个大概的效果，然后再慢慢变得更清晰。<br />
<br />
3、跳过残差和扩张采样机制：DemoFusion 使用这些先进的技术来改善图像的局部细节和全局一致性，从而生成更自然和逼真的图像。<br />
<br />
4、与 ControlNet 的集成：可以无缝集成到基于 LDM 的多种应用中，例如与 ControlNet 结合，实现可控的高分辨率图像生成。<br />
<br />
5、放大真实图像：还能够放大真实图像，通过编码的真实图像表示来替换初始阶段的输出，实现图像的高分辨率放大。<br />
<br />
6、无需大量内存和调整：DemoFusion 设计得既高效又易于使用，不需要大量的内存资源或复杂的调整过程。<br />
<br />
技术细节：<br />
<br />
1、渐进式上采样（Progressive Upscaling）：这种方法涉及逐步提高图像的分辨率。<br />
<br />
DemoFusion 从较低分辨率的图像开始，然后通过一个“上采样-扩散-去噪”循环逐渐提升图像的分辨率。<br />
在每个循环中，先对图像进行上采样（增加像素数量），然后通过扩散和去噪过程来提高图像质量。<br />
<br />
2、跳过残差（Skip Residual）：在“上采样-扩散-去噪”循环的每个阶段，DemoFusion 使用来自前一个扩散过程的中间噪声逆转表示作为跳过残差。<br />
<br />
这有助于在高分辨率和低分辨率图像之间保持全局一致性。<br />
<br />
3、扩张采样（Dilated Sampling）：为了在局部去噪路径中建立全局去噪路径，DemoFusion 引入了扩张采样。<br />
<br />
这种方法促进了更全局一致的内容生成，有助于在整个图像中保持语义上的连贯性。<br />
<br />
4、与现有模型的集成：DemoFusion 可以作为一个插件般地扩展现有的图像生成模型，如 SDXL。<br />
<br />
它不需要额外的训练，可以直接应用于现有模型，提供分辨率的显著提升。<br />
<br />
项目及演示：<a href="https://ruoyidu.github.io/demofusion/demofusion.html">ruoyidu.github.io/demofusion…</a><br />
论文：<br />
<br />
<a href="https://drive.google.com/file/d/1pAWCfpEgwy4UAkUqTGDuqypJt-5bl8se/view?usp=sharing">drive.google.com/file/d/1pAW…</a><br />
<br />
GitHub：<a href="https://github.com/PRIS-CV/DemoFusion">github.com/PRIS-CV/DemoFusio…</a><br />
<br />
Demo演示：<a href="https://replicate.com/lucataco/demofusion">replicate.com/lucataco/demof…</a><br />
<br />
在线体验由 <a href="https://nitter.cz/radamar" title="Radamés Ajna">@radamar</a> 提供：<a href="https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL">huggingface.co/spaces/radame…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMxNDk5Mzg4MDMxODc3MTIvcHUvaW1nL1BPN25hOXlYMzhmRHJob2suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733130290191905266#m</id>
            <title>据BBC报道：Google 承认，其展示的Gemini 的演示视频经过剪辑以使其看起来更好。

这个视频展示了 AI 如何实时响应口头提示和视频。然而，Google 在视频描述中表示，并非一切如视频所示——为了演示的目的，他们加快了响应速度。

此外，Google 还承认 AI 实际上根本没有对声音或视频做出反应。

视频实际上是通过使用视频画面中的静态图像帧，并通过文本提示来引导 AI 制作的。

例如：演示视频中，一个人向 Google 的 AI 展示物体并提出一系列问题。例如，演示者拿起一个橡皮鸭并询问 Gemini 是否会漂浮。AI 最初不确定它是由什么材料制成的，但在演示者挤压它（并指出这会发出吱吱声）后，AI 正确识别了物体。

而实际过程：AI 实际上是被展示了橡皮鸭的静态图像，并被问及其材料。然后，它通过文本提示得知橡皮鸭被挤压时会发出吱吱声，从而做出正确的识别。

详细：https://bbc.in/4a7T109</title>
            <link>https://nitter.cz/xiaohuggg/status/1733130290191905266#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733130290191905266#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 14:23:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据BBC报道：Google 承认，其展示的Gemini 的演示视频经过剪辑以使其看起来更好。<br />
<br />
这个视频展示了 AI 如何实时响应口头提示和视频。然而，Google 在视频描述中表示，并非一切如视频所示——为了演示的目的，他们加快了响应速度。<br />
<br />
此外，Google 还承认 AI 实际上根本没有对声音或视频做出反应。<br />
<br />
视频实际上是通过使用视频画面中的静态图像帧，并通过文本提示来引导 AI 制作的。<br />
<br />
例如：演示视频中，一个人向 Google 的 AI 展示物体并提出一系列问题。例如，演示者拿起一个橡皮鸭并询问 Gemini 是否会漂浮。AI 最初不确定它是由什么材料制成的，但在演示者挤压它（并指出这会发出吱吱声）后，AI 正确识别了物体。<br />
<br />
而实际过程：AI 实际上是被展示了橡皮鸭的静态图像，并被问及其材料。然后，它通过文本提示得知橡皮鸭被挤压时会发出吱吱声，从而做出正确的识别。<br />
<br />
详细：<a href="https://bbc.in/4a7T109">bbc.in/4a7T109</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ExUXo0SWFVQUFvTEpDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733055007833092357#m</id>
            <title>我们已收到您关于 GPT4 变得更加懒惰的所有反馈！

自 11 月 11 日以来我们就没有更新过模型，这当然不是故意的。

模型行为可能是不可预测的，我们正在研究修复它🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1733055007833092357#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733055007833092357#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:24:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们已收到您关于 GPT4 变得更加懒惰的所有反馈！<br />
<br />
自 11 月 11 日以来我们就没有更新过模型，这当然不是故意的。<br />
<br />
模型行为可能是不可预测的，我们正在研究修复它🫡</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1732979491071549792#m">nitter.cz/ChatGPTapp/status/1732979491071549792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732990584619778077#m</id>
            <title>NewsNerd HackerBot：一个Hacker News 机器人🫡

它可以自动从 Hacker News 上抓取各种类型的新闻故事，比如最热门的、最新的或者特定主题的故事。

- 关键词过滤：如果你对某个特定主题感兴趣，比如想了解有关某个知名科技人物或公司的新闻，你可以告诉这个程序，它会帮你筛选出相关的新闻故事。

- 本地运行：这个程序是开源的，意味着你可以自己下载代码，然后在你的电脑上运行它。

- 未来计划：开发者还打算让这个程序能分析新闻故事的评论，或者分析链接到的文章内容。

GitHub：https://github.com/neural-maze/talking_with_hn
作者：@MTrofficus</title>
            <link>https://nitter.cz/xiaohuggg/status/1732990584619778077#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732990584619778077#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 05:08:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NewsNerd HackerBot：一个Hacker News 机器人🫡<br />
<br />
它可以自动从 Hacker News 上抓取各种类型的新闻故事，比如最热门的、最新的或者特定主题的故事。<br />
<br />
- 关键词过滤：如果你对某个特定主题感兴趣，比如想了解有关某个知名科技人物或公司的新闻，你可以告诉这个程序，它会帮你筛选出相关的新闻故事。<br />
<br />
- 本地运行：这个程序是开源的，意味着你可以自己下载代码，然后在你的电脑上运行它。<br />
<br />
- 未来计划：开发者还打算让这个程序能分析新闻故事的评论，或者分析链接到的文章内容。<br />
<br />
GitHub：<a href="https://github.com/neural-maze/talking_with_hn">github.com/neural-maze/talki…</a><br />
作者：<a href="https://nitter.cz/MTrofficus" title="Miguel Otero Pedrido">@MTrofficus</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODUwMjE2OTc1NjQ2NzIvcHUvaW1nL2pSNTJYd0N1OVl5NDNIelIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732981463489110318#m</id>
            <title>R to @xiaohuggg: LooseControl还可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。

例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化等。</title>
            <link>https://nitter.cz/xiaohuggg/status/1732981463489110318#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732981463489110318#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:32:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LooseControl还可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。<br />
<br />
例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化等。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODE0MTczMDU2Njk2MzIvcHUvaW1nL2ZVdlI2RTM0aUZYWE14aDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732981317560881423#m</id>
            <title>LooseControl：一个创新的图像生成框架，可根据深度信息来引导图像的生成。

LooseControl在生成图像时会考虑物体之间的空间关系，可通过简单地描述物体在场景中的位置来创建复杂的场景。

例如你想设计一个房间，但只有一些基本想法，你只需要描述你的想法，它会根据你的描述设计一张真实的效果图。

LooseControl 提供了一种新颖的方式来设计复杂场景并执行语义编辑。

可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。

例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化。

基本背景：

LooseControl 允许用户在图像生成过程中使用深度条件，这意味着它可以根据物体在场景中的深度（即远近关系）来引导图像的生成。这里的“深度条件”指的是在生成图像时考虑物体之间的空间关系，比如哪个物体在前面，哪个在后面。

以前的技术，如 ControlNet，也使用了深度信息来生成图像，但它们通常需要非常详细和准确的深度图。深度图是一种显示场景中每个点距离观察者远近的图像，通常需要专业的设备和软件来创建。

LooseControl 的创新之处在于，它不需要这么详细的深度图。它允许用户以更简单、更灵活的方式指定深度条件。例如，用户可以只指定场景的大致布局和物体的大致位置，而不是提供完整的深度图。这使得用户可以更容易地创建复杂的场景图像，即使他们没有专业的图像处理技能或设备。

简单来说，LooseControl 让图像生成变得更加容易和直观，用户可以通过简单地描述物体在场景中的位置来创建复杂的图像，而不需要复杂的技术支持。

LooseControl的主要特点：

1、场景边界控制 (C1)：用户可以通过指定场景的边界来粗略地定义场景，而不需要提供详细的深度图。

2、3D 盒子控制 (C2)：除了场景边界，用户还可以通过大致的 3D 边界盒子来指定目标对象的位置，而不是其确切的形状和外观。

3、编辑机制：

3D 盒子编辑 (E1)：用户可以通过更改、添加或移除盒子来细化图像，同时保持图像的风格不变。

属性编辑 (E2)：提供可能的编辑方向来改变场景的某个特定方面，如整体对象密度或特定对象。

4、应用场景：使用 LooseControl和文本指导，用户可以仅通过指定场景边界和主要对象的位置来创建复杂的环境（例如房间、街景等）。

举例解释：

假如你是一个室内设计师，需要设计一个房间的样子，但你只有一些基本的想法，比如房间里应该有一张沙发、一张桌子和一盏灯。LooseControl 就是一个可以帮助你把这些基本想法变成真实图像的工具。

1、场景边界控制：你可以告诉 LooseControl 房间的大致布局，比如沙发在哪里、桌子和灯在哪里。你不需要提供详细的图纸，只需要大概描述房间的布局。

2、3D 盒子控制：你可以定义每个物体（如沙发、桌子、灯）的位置和大小。就像在电脑游戏中放置物体一样，你可以决定它们在房间中的位置和朝向。

3、编辑功能：

3D 盒子编辑：如果你觉得沙发的位置不太对，或者想换一个形状不同的桌子，你可以轻松地调整它们，而不会影响到房间的其他部分。

属性编辑：如果你想改变房间的整体风格，比如从现代风格变成复古风格，LooseControl 也可以帮助你做到这一点。

总的来说，LooseControl 就像是一个高级的室内设计软件，让你可以轻松地设计和调整房间的样子，即使你只有一些非常基本的想法。

工作原理：

LooseControl基于 ControlNet 和 StableDiffusion 模型，通过 LoRA（Low Rank）网络适配和自动合成必要的训练数据来实现。

LooseControl依赖于对 ControlNet 和 StableDiffusion 模型的改进和适配。通过使用 LoRA网络适配和自动合成训练数据。

LooseControl能够在保留原始生成权重的同时进行微调。此外，通过操作注意力层中的“键”和“值”，以及对 ControlNet Jacobian 的奇异值分析，实现了上述的编辑功能。

项目及演示：https://shariqfarooq123.github.io/loose-control/
论文：https://arxiv.org/abs/2312.03079
GitHub：https://github.com/shariqfarooq123/LooseControl
在线Demo：https://huggingface.co/spaces/shariqfarooq/LooseControl</title>
            <link>https://nitter.cz/xiaohuggg/status/1732981317560881423#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732981317560881423#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:31:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LooseControl：一个创新的图像生成框架，可根据深度信息来引导图像的生成。<br />
<br />
LooseControl在生成图像时会考虑物体之间的空间关系，可通过简单地描述物体在场景中的位置来创建复杂的场景。<br />
<br />
例如你想设计一个房间，但只有一些基本想法，你只需要描述你的想法，它会根据你的描述设计一张真实的效果图。<br />
<br />
LooseControl 提供了一种新颖的方式来设计复杂场景并执行语义编辑。<br />
<br />
可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。<br />
<br />
例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化。<br />
<br />
基本背景：<br />
<br />
LooseControl 允许用户在图像生成过程中使用深度条件，这意味着它可以根据物体在场景中的深度（即远近关系）来引导图像的生成。这里的“深度条件”指的是在生成图像时考虑物体之间的空间关系，比如哪个物体在前面，哪个在后面。<br />
<br />
以前的技术，如 ControlNet，也使用了深度信息来生成图像，但它们通常需要非常详细和准确的深度图。深度图是一种显示场景中每个点距离观察者远近的图像，通常需要专业的设备和软件来创建。<br />
<br />
LooseControl 的创新之处在于，它不需要这么详细的深度图。它允许用户以更简单、更灵活的方式指定深度条件。例如，用户可以只指定场景的大致布局和物体的大致位置，而不是提供完整的深度图。这使得用户可以更容易地创建复杂的场景图像，即使他们没有专业的图像处理技能或设备。<br />
<br />
简单来说，LooseControl 让图像生成变得更加容易和直观，用户可以通过简单地描述物体在场景中的位置来创建复杂的图像，而不需要复杂的技术支持。<br />
<br />
LooseControl的主要特点：<br />
<br />
1、场景边界控制 (C1)：用户可以通过指定场景的边界来粗略地定义场景，而不需要提供详细的深度图。<br />
<br />
2、3D 盒子控制 (C2)：除了场景边界，用户还可以通过大致的 3D 边界盒子来指定目标对象的位置，而不是其确切的形状和外观。<br />
<br />
3、编辑机制：<br />
<br />
3D 盒子编辑 (E1)：用户可以通过更改、添加或移除盒子来细化图像，同时保持图像的风格不变。<br />
<br />
属性编辑 (E2)：提供可能的编辑方向来改变场景的某个特定方面，如整体对象密度或特定对象。<br />
<br />
4、应用场景：使用 LooseControl和文本指导，用户可以仅通过指定场景边界和主要对象的位置来创建复杂的环境（例如房间、街景等）。<br />
<br />
举例解释：<br />
<br />
假如你是一个室内设计师，需要设计一个房间的样子，但你只有一些基本的想法，比如房间里应该有一张沙发、一张桌子和一盏灯。LooseControl 就是一个可以帮助你把这些基本想法变成真实图像的工具。<br />
<br />
1、场景边界控制：你可以告诉 LooseControl 房间的大致布局，比如沙发在哪里、桌子和灯在哪里。你不需要提供详细的图纸，只需要大概描述房间的布局。<br />
<br />
2、3D 盒子控制：你可以定义每个物体（如沙发、桌子、灯）的位置和大小。就像在电脑游戏中放置物体一样，你可以决定它们在房间中的位置和朝向。<br />
<br />
3、编辑功能：<br />
<br />
3D 盒子编辑：如果你觉得沙发的位置不太对，或者想换一个形状不同的桌子，你可以轻松地调整它们，而不会影响到房间的其他部分。<br />
<br />
属性编辑：如果你想改变房间的整体风格，比如从现代风格变成复古风格，LooseControl 也可以帮助你做到这一点。<br />
<br />
总的来说，LooseControl 就像是一个高级的室内设计软件，让你可以轻松地设计和调整房间的样子，即使你只有一些非常基本的想法。<br />
<br />
工作原理：<br />
<br />
LooseControl基于 ControlNet 和 StableDiffusion 模型，通过 LoRA（Low Rank）网络适配和自动合成必要的训练数据来实现。<br />
<br />
LooseControl依赖于对 ControlNet 和 StableDiffusion 模型的改进和适配。通过使用 LoRA网络适配和自动合成训练数据。<br />
<br />
LooseControl能够在保留原始生成权重的同时进行微调。此外，通过操作注意力层中的“键”和“值”，以及对 ControlNet Jacobian 的奇异值分析，实现了上述的编辑功能。<br />
<br />
项目及演示：<a href="https://shariqfarooq123.github.io/loose-control/">shariqfarooq123.github.io/lo…</a><br />
论文：<a href="https://arxiv.org/abs/2312.03079">arxiv.org/abs/2312.03079</a><br />
GitHub：<a href="https://github.com/shariqfarooq123/LooseControl">github.com/shariqfarooq123/L…</a><br />
在线Demo：<a href="https://huggingface.co/spaces/shariqfarooq/LooseControl">huggingface.co/spaces/shariq…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODAwMDg1ODk5MTQxMTIvcHUvaW1nL2d6UUFCLUh6UEhDU0pISmQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732969634674983299#m</id>
            <title>R to @xiaohuggg: 只用3张照片生成的的一个场景</title>
            <link>https://nitter.cz/xiaohuggg/status/1732969634674983299#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732969634674983299#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:45:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>只用3张照片生成的的一个场景</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5Njk1MzU0MDg2Njg2NzIvcHUvaW1nL0k1UU0wVDk0N0NKbXNmQmguanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>