<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729462359486582975#m</id>
            <title>RAGs：允许创建和定制自己的RAG流水线，并在自己的数据上使用它，全部通过自然语言完成。

这意味着现在你可以设置一个“基于你自己的数据的ChatGPT”，而且不需要编码。

使用RAGs创建的机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。

能生成更准确、信息更丰富的回答。

安装简单：

这个项目受到了OpenAI GPTs的启发。RAGs能让你能够通过简单的自然语言描述来创建和定制自己的聊天机器人。这个过程不需要编程知识，只需按照几个步骤操作即可：

1、描述任务：告诉RAGs你想让机器人做什么，比如回答问题或总结信息。

2、设置参数：在一个界面上调整一些选项，比如要查找的信息数量。

3、与机器人互动：设置好后，你就可以开始向这个机器人提问了。

安装RAGs的步骤也很简单，只需下载代码、安装必要的软件包，然后运行程序即可。这个工具适合那些想要自己的聊天机器人但不懂编程的人。

它支持多种LLMs（大语言模型），包括OpenAI和Anthropic的模型。用户可以通过自然语言或手动方式为嵌入模型和LLM设置配置。

主要能力和特点：

使用RAGs创建的聊天机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。

这种机器人的特点和能力包括：

1、信息检索能力：机器人能够访问和搜索大量的文档和数据，以找到与用户查询相关的信息。这意味着它可以从外部源获取数据，而不仅仅依赖于预先训练的知识。

2、高质量的回答生成：结合检索到的信息和内置的语言模型（ChatGPT），机器人能够生成更准确、信息丰富的回答。

3、适应性强：由于它结合了检索和生成，这种机器人能够更好地处理复杂的问题，尤其是那些需要实时信息或专业知识的问题。

4、灵活性和定制性：用户可以根据自己的需求定制机器人的行为，例如指定信息检索的来源、调整回答的详细程度等。

5、适用于多种应用：这种机器人适用于各种场景，如客户服务、教育、研究辅助等，尤其是在需要处理大量信息和数据的领域。

介绍：https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1

GitHub：https://github.com/run-llama/rags</title>
            <link>https://nitter.cz/xiaohuggg/status/1729462359486582975#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729462359486582975#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 11:28:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RAGs：允许创建和定制自己的RAG流水线，并在自己的数据上使用它，全部通过自然语言完成。<br />
<br />
这意味着现在你可以设置一个“基于你自己的数据的ChatGPT”，而且不需要编码。<br />
<br />
使用RAGs创建的机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。<br />
<br />
能生成更准确、信息更丰富的回答。<br />
<br />
安装简单：<br />
<br />
这个项目受到了OpenAI GPTs的启发。RAGs能让你能够通过简单的自然语言描述来创建和定制自己的聊天机器人。这个过程不需要编程知识，只需按照几个步骤操作即可：<br />
<br />
1、描述任务：告诉RAGs你想让机器人做什么，比如回答问题或总结信息。<br />
<br />
2、设置参数：在一个界面上调整一些选项，比如要查找的信息数量。<br />
<br />
3、与机器人互动：设置好后，你就可以开始向这个机器人提问了。<br />
<br />
安装RAGs的步骤也很简单，只需下载代码、安装必要的软件包，然后运行程序即可。这个工具适合那些想要自己的聊天机器人但不懂编程的人。<br />
<br />
它支持多种LLMs（大语言模型），包括OpenAI和Anthropic的模型。用户可以通过自然语言或手动方式为嵌入模型和LLM设置配置。<br />
<br />
主要能力和特点：<br />
<br />
使用RAGs创建的聊天机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。<br />
<br />
这种机器人的特点和能力包括：<br />
<br />
1、信息检索能力：机器人能够访问和搜索大量的文档和数据，以找到与用户查询相关的信息。这意味着它可以从外部源获取数据，而不仅仅依赖于预先训练的知识。<br />
<br />
2、高质量的回答生成：结合检索到的信息和内置的语言模型（ChatGPT），机器人能够生成更准确、信息丰富的回答。<br />
<br />
3、适应性强：由于它结合了检索和生成，这种机器人能够更好地处理复杂的问题，尤其是那些需要实时信息或专业知识的问题。<br />
<br />
4、灵活性和定制性：用户可以根据自己的需求定制机器人的行为，例如指定信息检索的来源、调整回答的详细程度等。<br />
<br />
5、适用于多种应用：这种机器人适用于各种场景，如客户服务、教育、研究辅助等，尤其是在需要处理大量信息和数据的领域。<br />
<br />
介绍：<a href="https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1">blog.llamaindex.ai/introduci…</a><br />
<br />
GitHub：<a href="https://github.com/run-llama/rags">github.com/run-llama/rags</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjk0NjIxNzM1NDg5MDAzNTIvcHUvaW1nL1dKQWtLSC1ac2owc2xtdUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729442483594305738#m</id>
            <title>R to @xiaohuggg: 测试是支持iPhone的，开始是我手机iOS17 beta版本问题！登陆不上！正式版iOS17是可以的！

你们可以玩玩！</title>
            <link>https://nitter.cz/xiaohuggg/status/1729442483594305738#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729442483594305738#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 10:09:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试是支持iPhone的，开始是我手机iOS17 beta版本问题！登陆不上！正式版iOS17是可以的！<br />
<br />
你们可以玩玩！</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NDQyMzk1ODM2ODgyOTQ0L2ltZy81OUpiSlV5UWg2anFSd1ZYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729404520814575674#m</id>
            <title>TRASH BABY：混合任意两张照片生成新照片

你可以任意拍摄两张照片，这个APP会将它们混合成一个新的图像。

混合完毕后你可以继续添加新的照片，每次添加都会与之前的图像混合，创造出一系列独特且多变的图像作品。

也可以选择自己的照片进行混合...🥱

不过目前只支持iPad，下载链接：https://apps.apple.com/us/app/trash-baby/id6446822932</title>
            <link>https://nitter.cz/xiaohuggg/status/1729404520814575674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729404520814575674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:39:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TRASH BABY：混合任意两张照片生成新照片<br />
<br />
你可以任意拍摄两张照片，这个APP会将它们混合成一个新的图像。<br />
<br />
混合完毕后你可以继续添加新的照片，每次添加都会与之前的图像混合，创造出一系列独特且多变的图像作品。<br />
<br />
也可以选择自己的照片进行混合...🥱<br />
<br />
不过目前只支持iPad，下载链接：<a href="https://apps.apple.com/us/app/trash-baby/id6446822932">apps.apple.com/us/app/trash-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzODgwODIxNTAxNzg4MTYvcHUvaW1nL1pMZVdiU3RMYURiNWYzcmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729395131474935856#m</id>
            <title>如果有人能出一本日历

什么时间应该发什么，什么时候不能发什么

在国内应该会卖的很好

如果能利用AI再加个自动检测提示就更好了🙄</title>
            <link>https://nitter.cz/xiaohuggg/status/1729395131474935856#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729395131474935856#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:01:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果有人能出一本日历<br />
<br />
什么时间应该发什么，什么时候不能发什么<br />
<br />
在国内应该会卖的很好<br />
<br />
如果能利用AI再加个自动检测提示就更好了🙄</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729356197906837678#m</id>
            <title>Cleanlab：自动检测机器学习数据集中的问题，进行自动清理修复和整理。

Cleanlab能够自动识别和修复数据集中的多种问题，如错误的标签、异常值、重复数据等。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，它可以帮助识别这种错误。

可以处理图像、文本、音频、表格数据等多种类型的数据。

Cleanlab 的工作原理和特点包括：

1、使用现有模型估计问题：Cleanlab 不是从头开始训练一个新模型，而是利用已有的机器学习模型来分析数据集。它通过这些模型的输出来估计数据集中可能存在的问题，比如哪些数据可能被错误标记，或者哪些数据是异常值。

2、发现标签问题：Cleanlab 能够自动识别数据集中的多种问题，如错误的标签、异常值、重复数据等。这种自动检测功能大大简化了数据清理的过程。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，Cleanlab 可以帮助识别这种错误。

3、训练鲁棒模型：通过识别和修复数据集中的问题，Cleanlab 可以帮助训练出对噪声和错误更加鲁棒的模型。这意味着即使在数据不完美的情况下，模型也能表现得更好。

4、适用于多种数据类型和任务：Cleanlab 不仅适用于图像数据，还可以处理文本、音频、表格数据等多种类型的数据。它支持多种机器学习任务，包括分类、回归、对象检测等。

5、数据质量量化：除了识别和修复问题，Cleanlab 还能量化数据集的整体质量，帮助用户了解数据集的健康状况。

详细介绍：https://cleanlab.ai/blog/studio-multi-label/

GitHub：https://github.com/cleanlab/cleanlab/

案例：https://github.com/cleanlab/examples

视频演示为：以 CelebA 多标签数据集为例，展示了 Cleanlab Studio 如何快速改进数据集。

CelebA 包含面部图像及其相关标签（如“戴眼镜”、“戴耳环”等），每张图像可以有多个标签。Cleanlab Studio 能够自动发现数百个缺失和不正确的标签，以及一些可能含糊的样本和异常值。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729356197906837678#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729356197906837678#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 04:27:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Cleanlab：自动检测机器学习数据集中的问题，进行自动清理修复和整理。<br />
<br />
Cleanlab能够自动识别和修复数据集中的多种问题，如错误的标签、异常值、重复数据等。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，它可以帮助识别这种错误。<br />
<br />
可以处理图像、文本、音频、表格数据等多种类型的数据。<br />
<br />
Cleanlab 的工作原理和特点包括：<br />
<br />
1、使用现有模型估计问题：Cleanlab 不是从头开始训练一个新模型，而是利用已有的机器学习模型来分析数据集。它通过这些模型的输出来估计数据集中可能存在的问题，比如哪些数据可能被错误标记，或者哪些数据是异常值。<br />
<br />
2、发现标签问题：Cleanlab 能够自动识别数据集中的多种问题，如错误的标签、异常值、重复数据等。这种自动检测功能大大简化了数据清理的过程。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，Cleanlab 可以帮助识别这种错误。<br />
<br />
3、训练鲁棒模型：通过识别和修复数据集中的问题，Cleanlab 可以帮助训练出对噪声和错误更加鲁棒的模型。这意味着即使在数据不完美的情况下，模型也能表现得更好。<br />
<br />
4、适用于多种数据类型和任务：Cleanlab 不仅适用于图像数据，还可以处理文本、音频、表格数据等多种类型的数据。它支持多种机器学习任务，包括分类、回归、对象检测等。<br />
<br />
5、数据质量量化：除了识别和修复问题，Cleanlab 还能量化数据集的整体质量，帮助用户了解数据集的健康状况。<br />
<br />
详细介绍：<a href="https://cleanlab.ai/blog/studio-multi-label/">cleanlab.ai/blog/studio-mult…</a><br />
<br />
GitHub：<a href="https://github.com/cleanlab/cleanlab/">github.com/cleanlab/cleanlab…</a><br />
<br />
案例：<a href="https://github.com/cleanlab/examples">github.com/cleanlab/examples</a><br />
<br />
视频演示为：以 CelebA 多标签数据集为例，展示了 Cleanlab Studio 如何快速改进数据集。<br />
<br />
CelebA 包含面部图像及其相关标签（如“戴眼镜”、“戴耳环”等），每张图像可以有多个标签。Cleanlab Studio 能够自动发现数百个缺失和不正确的标签，以及一些可能含糊的样本和异常值。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzNDYzMzUyNTI1NDk2MzIvcHUvaW1nL2pibHJfRUQydmg3QkRJX2MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729336931258093805#m</id>
            <title>R to @xiaohuggg: 测试结果是：

如果是已经比较清晰的提升效果会很明显！

但是如果是很老的影片，提升效果就不是很明显了。

根据演示来看，似乎对动画视频效果提升比较好。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729336931258093805#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729336931258093805#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 03:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果是：<br />
<br />
如果是已经比较清晰的提升效果会很明显！<br />
<br />
但是如果是很老的影片，提升效果就不是很明显了。<br />
<br />
根据演示来看，似乎对动画视频效果提升比较好。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzMzY2MjczMTc4ODY5NzYvcHUvaW1nL3o5ZU0xRzBYNFlEcG1MalAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729336570115920325#m</id>
            <title>Real-ESRGAN-Video ：可以将视频清晰度提升到2K 或 4K

你只需要上传一个视频，选择你想要的清晰度，比如全高清（FHD）、2K 或 4K。它会自动提升视频的质量。处理完的视频可以直接在网页上预览，也可以下载到电脑上。

还提供了几种不同的模型处理模式，可以根据视频的内容选择最合适的。

- 标准模型（RealESRGAN_x4plus）：适用于大多数普通视频，能够提升视频的清晰度和细节。

- 动画专用模型（RealESRGAN_x4plus_anime_6B）：专门为动画视频设计，能更好地处理动画中的线条和颜色。

- 其他特殊模型（如 realesr-animevideov3）：针对特定类型的视频内容进行了优化，比如更适合处理低光照视频或是特定风格的视频。

用户可以根据自己的视频内容（比如是普通视频、动画、或者有特殊风格的视频）来选择最合适的模型，以获得最佳的视频增强效果。

在线体验：https://replicate.com/lucataco/real-esrgan-video

Colab：https://github.com/yuvraj108c/4k-video-upscaler-colab

GitHub：https://github.com/xinntao/Real-ESRGAN</title>
            <link>https://nitter.cz/xiaohuggg/status/1729336570115920325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729336570115920325#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 03:09:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Real-ESRGAN-Video ：可以将视频清晰度提升到2K 或 4K<br />
<br />
你只需要上传一个视频，选择你想要的清晰度，比如全高清（FHD）、2K 或 4K。它会自动提升视频的质量。处理完的视频可以直接在网页上预览，也可以下载到电脑上。<br />
<br />
还提供了几种不同的模型处理模式，可以根据视频的内容选择最合适的。<br />
<br />
- 标准模型（RealESRGAN_x4plus）：适用于大多数普通视频，能够提升视频的清晰度和细节。<br />
<br />
- 动画专用模型（RealESRGAN_x4plus_anime_6B）：专门为动画视频设计，能更好地处理动画中的线条和颜色。<br />
<br />
- 其他特殊模型（如 realesr-animevideov3）：针对特定类型的视频内容进行了优化，比如更适合处理低光照视频或是特定风格的视频。<br />
<br />
用户可以根据自己的视频内容（比如是普通视频、动画、或者有特殊风格的视频）来选择最合适的模型，以获得最佳的视频增强效果。<br />
<br />
在线体验：<a href="https://replicate.com/lucataco/real-esrgan-video">replicate.com/lucataco/real-…</a><br />
<br />
Colab：<a href="https://github.com/yuvraj108c/4k-video-upscaler-colab">github.com/yuvraj108c/4k-vid…</a><br />
<br />
GitHub：<a href="https://github.com/xinntao/Real-ESRGAN">github.com/xinntao/Real-ESRG…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzMzYzMzEyNDE5NDMwNDAvcHUvaW1nLzJ4cnc1ZVg5Q01lRTAyUEsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729313540887175590#m</id>
            <title>Awesome-Assistants ：各种各样的AI助手

这个项目收集和展示各种AI助手，包括聊天机器人、语音助手、自动化工具等。

你可以方便地将这些AI助手集成到他们自己的应用或系统中，无论他们使用的是什么编程语言。

GitHub：https://github.com/awesome-assistants/awesome-assistants</title>
            <link>https://nitter.cz/xiaohuggg/status/1729313540887175590#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729313540887175590#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 01:37:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Awesome-Assistants ：各种各样的AI助手<br />
<br />
这个项目收集和展示各种AI助手，包括聊天机器人、语音助手、自动化工具等。<br />
<br />
你可以方便地将这些AI助手集成到他们自己的应用或系统中，无论他们使用的是什么编程语言。<br />
<br />
GitHub：<a href="https://github.com/awesome-assistants/awesome-assistants">github.com/awesome-assistant…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9fQmY2bGJRQUEzZ0lPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729288978283827360#m</id>
            <title>和GPT玩井字棋游戏

GPT结合@tldraw 实时画图后的新玩法

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1729288978283827360#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729288978283827360#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 23:59:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>和GPT玩井字棋游戏<br />
<br />
GPT结合<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 实时画图后的新玩法<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkxODQwMzQ5NzI5MjU5NTIvcHUvaW1nL3RxQnBkQ0RBQUwtZ2pteFkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729167719394869646#m</id>
            <title>牛p了

用文字描述指挥AI画画😂

你只需要描述你要画什么就行

右边会按照你的要求即时创作，还会展示创作过程👍👍👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1729167719394869646#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729167719394869646#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 15:58:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>牛p了<br />
<br />
用文字描述指挥AI画画😂<br />
<br />
你只需要描述你要画什么就行<br />
<br />
右边会按照你的要求即时创作，还会展示创作过程👍👍👍</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl84NFF6T1hrQUEyRk1YLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfODRRek9Ya0FBMkZNWC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</id>
            <title>R to @xiaohuggg: 所以这种片子以后就统称为：A片</title>
            <link>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729102242429702348#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:37:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以这种片子以后就统称为：A片</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</id>
            <title>全球首位由AI生成的AV女演员木花愛（木花あい）出道

由日本一家专精于成人动作片的公司h.m.p推出。

木花愛：身高165厘米，三围为88G/55/85。

木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。

链接就不放了😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729099853698093128#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:28:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>全球首位由AI生成的AV女演员木花愛（木花あい）出道<br />
<br />
由日本一家专精于成人动作片的公司h.m.p推出。<br />
<br />
木花愛：身高165厘米，三围为88G/55/85。<br />
<br />
木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。<br />
<br />
链接就不放了😅</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83OTVWNGJRQUEwRkZFLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83LU1sYmJFQUFyd1VXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</id>
            <title>DiffusionMat：一个先进的高质量视频抠图框架

DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。

DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：

1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。

2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。

3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。

4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。

5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。

项目及演示：https://cnnlstm.github.io/DiffusionMat
论文：https://arxiv.org/pdf/2311.13535.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729091603900559731#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 10:55:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DiffusionMat：一个先进的高质量视频抠图框架<br />
<br />
DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。<br />
<br />
DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：<br />
<br />
1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。<br />
<br />
2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。<br />
<br />
3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。<br />
<br />
4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。<br />
<br />
5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。<br />
<br />
项目及演示：<a href="https://cnnlstm.github.io/DiffusionMat">cnnlstm.github.io/DiffusionM…</a><br />
论文：<a href="https://arxiv.org/pdf/2311.13535.pdf">arxiv.org/pdf/2311.13535.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMzk4MDI3NTMwMDc2MTYvcHUvaW1nL0VmTkUtUTZYQXlYNEVkejguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</id>
            <title>Comfy Workflows：ComfyUI工作流分享站

收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。

省去了很多麻烦🫡

而且你也可以自己分享自己的工作流。

网站还支持在线运行工作流，不过要花点钱🥱

🔗：http://ComfyWorkflows.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729063470124183735#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 09:03:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfy Workflows：ComfyUI工作流分享站<br />
<br />
收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。<br />
<br />
省去了很多麻烦🫡<br />
<br />
而且你也可以自己分享自己的工作流。<br />
<br />
网站还支持在线运行工作流，不过要花点钱🥱<br />
<br />
🔗：<a href="http://ComfyWorkflows.com">ComfyWorkflows.com</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwNjMxNjk4NDk3NzgxNzYvcHUvaW1nL2VtTnJlTGNqeXlNQUlBV1ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729037971578626302#m</id>
            <title>What Q* could be？

Q*可能是一种结合了多种强化学习和搜索技术的方法，用于提高语言模型在复杂任务中的性能和推理能力。

通过结合自我对弈、前瞻性规划、思维树推理和过程奖励模型利用合成数据自我训练，Q*可能是一种具备自我思考能力、能够自我进化和学习的高级人工智能。

ML 科学家（RL、RLHF、社会、机器人）@natolambert 内森·兰伯特撰文描述了他对Q* 的一些看法

了对Q*可能的工作原理进行了深入分析：

通俗易懂解释就是：

1、结合两种策略：Q*是把两种策略混合在一起。一种是像玩国际象棋时预测对手下一步会怎么走（这叫Q学习），另一种是像谷歌地图一样找到从一个地方到另一个地方的最佳路线（这叫A搜索）。

2、思考多种可能：Q*会像一个思考多种可能性的人一样，考虑不同的解决方案，然后选择最好的那个。就像你在解决一个难题时，会想到很多不同的答案，然后挑选最合适的一个。

3、自己和自己比赛：Q*还会像一个棋手那样，和自己下棋来提高自己的技能。它通过不断地和自己的不同版本比赛，学习如何做出更好的决策。

4、每一步都打分：在解决问题的过程中，Q*会给每一步都打分，这样就能知道哪些步骤是好的，哪些不是。就像你在做选择题时，对每个选项进行评估，然后选择最有可能正确的那个。

5、使用合成数据训练：Q*使用大量的虚拟数据来训练自己，这样就不需要真实世界的数据那么多。这就像是通过模拟考试来准备真正的考试。

6、学习如何更好地解决问题：最后，Q*会通过一种叫做离线强化学习的方法来提高自己的能力。这就像是通过回顾过去的经验来学习如何在未来做得更好。

专业解释：

1、结合Q学习和A*搜索：Q*可能是Q学习（一种强化学习算法）和A*搜索（一种图搜索算法）的结合。这个方法可能涉及通过“思维树”（tree-of-thoughts）在语言/推理步骤上进行搜索。

2、思维树推理（Tree-of-Thoughts Reasoning）：Q*可能利用所谓的“思维树”来进行推理。这种方法通过提示语言模型创建多个推理路径，这些路径可能会或不会在正确答案处汇合。这种方法类似于递归式的提示技术，用于提高推理性能。

3、自我对弈和前瞻性规划：Q*可能结合了自我对弈的概念，即通过与自己的不同版本对弈来提高性能，以及前瞻性规划，即使用模型预测未来以产生更好的行动或输出。

4、过程奖励模型（Process Reward Models, PRMs）：Q*可能使用PRMs来为每个推理步骤打分，而不是整个回答。这允许在推理问题上进行更精细的生成和优化。

5、合成数据的使用：Q*可能大量使用合成数据来训练和优化模型。合成数据的使用可以减少对人类评分者的依赖，提高数据生成的效率和多样性。

6、离线强化学习的应用：Q*可能通过离线强化学习进行优化，这与现有的RLHF（强化学习人类反馈）工具类似，但采用了多步骤的方法。

详细内容：https://www.interconnects.ai/p/q-star</title>
            <link>https://nitter.cz/xiaohuggg/status/1729037971578626302#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729037971578626302#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 07:22:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>What Q* could be？<br />
<br />
Q*可能是一种结合了多种强化学习和搜索技术的方法，用于提高语言模型在复杂任务中的性能和推理能力。<br />
<br />
通过结合自我对弈、前瞻性规划、思维树推理和过程奖励模型利用合成数据自我训练，Q*可能是一种具备自我思考能力、能够自我进化和学习的高级人工智能。<br />
<br />
ML 科学家（RL、RLHF、社会、机器人）<a href="https://nitter.cz/natolambert" title="Nathan Lambert">@natolambert</a> 内森·兰伯特撰文描述了他对Q* 的一些看法<br />
<br />
了对Q*可能的工作原理进行了深入分析：<br />
<br />
通俗易懂解释就是：<br />
<br />
1、结合两种策略：Q*是把两种策略混合在一起。一种是像玩国际象棋时预测对手下一步会怎么走（这叫Q学习），另一种是像谷歌地图一样找到从一个地方到另一个地方的最佳路线（这叫A搜索）。<br />
<br />
2、思考多种可能：Q*会像一个思考多种可能性的人一样，考虑不同的解决方案，然后选择最好的那个。就像你在解决一个难题时，会想到很多不同的答案，然后挑选最合适的一个。<br />
<br />
3、自己和自己比赛：Q*还会像一个棋手那样，和自己下棋来提高自己的技能。它通过不断地和自己的不同版本比赛，学习如何做出更好的决策。<br />
<br />
4、每一步都打分：在解决问题的过程中，Q*会给每一步都打分，这样就能知道哪些步骤是好的，哪些不是。就像你在做选择题时，对每个选项进行评估，然后选择最有可能正确的那个。<br />
<br />
5、使用合成数据训练：Q*使用大量的虚拟数据来训练自己，这样就不需要真实世界的数据那么多。这就像是通过模拟考试来准备真正的考试。<br />
<br />
6、学习如何更好地解决问题：最后，Q*会通过一种叫做离线强化学习的方法来提高自己的能力。这就像是通过回顾过去的经验来学习如何在未来做得更好。<br />
<br />
专业解释：<br />
<br />
1、结合Q学习和A*搜索：Q*可能是Q学习（一种强化学习算法）和A*搜索（一种图搜索算法）的结合。这个方法可能涉及通过“思维树”（tree-of-thoughts）在语言/推理步骤上进行搜索。<br />
<br />
2、思维树推理（Tree-of-Thoughts Reasoning）：Q*可能利用所谓的“思维树”来进行推理。这种方法通过提示语言模型创建多个推理路径，这些路径可能会或不会在正确答案处汇合。这种方法类似于递归式的提示技术，用于提高推理性能。<br />
<br />
3、自我对弈和前瞻性规划：Q*可能结合了自我对弈的概念，即通过与自己的不同版本对弈来提高性能，以及前瞻性规划，即使用模型预测未来以产生更好的行动或输出。<br />
<br />
4、过程奖励模型（Process Reward Models, PRMs）：Q*可能使用PRMs来为每个推理步骤打分，而不是整个回答。这允许在推理问题上进行更精细的生成和优化。<br />
<br />
5、合成数据的使用：Q*可能大量使用合成数据来训练和优化模型。合成数据的使用可以减少对人类评分者的依赖，提高数据生成的效率和多样性。<br />
<br />
6、离线强化学习的应用：Q*可能通过离线强化学习进行优化，这与现有的RLHF（强化学习人类反馈）工具类似，但采用了多步骤的方法。<br />
<br />
详细内容：<a href="https://www.interconnects.ai/p/q-star">interconnects.ai/p/q-star</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83RzNsbWJFQUF1cS1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</id>
            <title>R to @xiaohuggg: 测试结果：
https://huggingface.co/openchat/openchat_3.5</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002803887214842#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:02:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果：<br />
<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bVpOWWIwQUF5dUJOLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bWRhQmFVQUVDSmRLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bW5tTmF3QUFUWm1kLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002211404087412#m</id>
            <title>OpenChat-3.5-7B ：在各种基准测试上超越ChatGPT

OpenChat使用了C-RLFT（一种受离线强化学习启发的策略）进行微调。

它能通过分析已有的对话数据和反馈来改进模型的表现。还可以从错误中学习。

测试了下，虽然只有7B大小，确实效果和GPT不分上下。

牛P的是它能在24GB RAM的消费级GPU上运行。

OpenChat还提供了一个Web UI界面，方便用户与模型进行交互。

性能和评估：

在实际应用中，OpenChat展示了优异的性能。它在多个基准测试中表现出色，特别是在遵循指令和泛化能力方面，超越了其他同类的开源语言模型。

在基准测试方面，OpenChat-3.5的7B模型在多个测试中的平均得分为61.6，超过了ChatGPT（March版本）的61.5。

在于http://X.AI 330 亿参数的Grok的比拼中OpenChat-3.5-7B

OpenChat工作原理：

1、预训练语言模型：OpenChat的核心是一个大型的预训练语言模型。这些模型通过分析和学习大量的文本数据，掌握了语言的结构、语法和语义。这使得OpenChat能够理解用户的输入，并生成流畅、连贯的回应。

2、微调方法（C-RLFT）：OpenChat采用了一种名为条件化强化学习微调（Conditioned-RLFT, C-RLFT）的方法。这种方法特别适用于处理混合质量的数据。在传统的微调方法中，所有的训练数据都被视为同等重要，这可能导致模型在处理质量不一的数据时效果不佳。C-RLFT通过将不同数据源视为不同的奖励标签，使模型能够更有效地从这些数据中学习。

3、类条件策略学习：在C-RLFT中，OpenChat学习了一个类条件策略，这意味着它可以根据输入数据的类型（例如，不同的数据源或质量）来调整其响应。这种策略使得OpenChat在处理各种不同类型的输入时更加灵活和有效。

4、单阶段监督学习：OpenChat使用了一种单阶段的监督学习方法。这种方法不依赖于传统的强化学习技术，而是通过最大化奖励并减少与参考策略之间的差异来优化模型。这种方法提高了学习效率，并有助于减少训练过程中的错误。

详细：https://huggingface.co/openchat/openchat_3.5
GitHub：https://github.com/imoneoi/openchat
论文：https://arxiv.org/pdf/2309.11235.pdf
在线体验：https://openchat.team/</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002211404087412#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002211404087412#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:00:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenChat-3.5-7B ：在各种基准测试上超越ChatGPT<br />
<br />
OpenChat使用了C-RLFT（一种受离线强化学习启发的策略）进行微调。<br />
<br />
它能通过分析已有的对话数据和反馈来改进模型的表现。还可以从错误中学习。<br />
<br />
测试了下，虽然只有7B大小，确实效果和GPT不分上下。<br />
<br />
牛P的是它能在24GB RAM的消费级GPU上运行。<br />
<br />
OpenChat还提供了一个Web UI界面，方便用户与模型进行交互。<br />
<br />
性能和评估：<br />
<br />
在实际应用中，OpenChat展示了优异的性能。它在多个基准测试中表现出色，特别是在遵循指令和泛化能力方面，超越了其他同类的开源语言模型。<br />
<br />
在基准测试方面，OpenChat-3.5的7B模型在多个测试中的平均得分为61.6，超过了ChatGPT（March版本）的61.5。<br />
<br />
在于<a href="http://X.AI">X.AI</a> 330 亿参数的Grok的比拼中OpenChat-3.5-7B<br />
<br />
OpenChat工作原理：<br />
<br />
1、预训练语言模型：OpenChat的核心是一个大型的预训练语言模型。这些模型通过分析和学习大量的文本数据，掌握了语言的结构、语法和语义。这使得OpenChat能够理解用户的输入，并生成流畅、连贯的回应。<br />
<br />
2、微调方法（C-RLFT）：OpenChat采用了一种名为条件化强化学习微调（Conditioned-RLFT, C-RLFT）的方法。这种方法特别适用于处理混合质量的数据。在传统的微调方法中，所有的训练数据都被视为同等重要，这可能导致模型在处理质量不一的数据时效果不佳。C-RLFT通过将不同数据源视为不同的奖励标签，使模型能够更有效地从这些数据中学习。<br />
<br />
3、类条件策略学习：在C-RLFT中，OpenChat学习了一个类条件策略，这意味着它可以根据输入数据的类型（例如，不同的数据源或质量）来调整其响应。这种策略使得OpenChat在处理各种不同类型的输入时更加灵活和有效。<br />
<br />
4、单阶段监督学习：OpenChat使用了一种单阶段的监督学习方法。这种方法不依赖于传统的强化学习技术，而是通过最大化奖励并减少与参考策略之间的差异来优化模型。这种方法提高了学习效率，并有助于减少训练过程中的错误。<br />
<br />
详细：<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a><br />
GitHub：<a href="https://github.com/imoneoi/openchat">github.com/imoneoi/openchat</a><br />
论文：<a href="https://arxiv.org/pdf/2309.11235.pdf">arxiv.org/pdf/2309.11235.pdf</a><br />
在线体验：<a href="https://openchat.team/">openchat.team/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMDE4NzkzOTk3OTI2NDAvcHUvaW1nL2lNU3lwZ194ekxQTjgwek8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1728959646138880026#m</id>
            <title>RT by @xiaohuggg: OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：

Intro: Large Language Model (LLM) talk

大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。

LLM Inference

首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。

以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。

现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。

在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。

除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。

你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。

比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。

我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。

那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。</title>
            <link>https://nitter.cz/dotey/status/1728959646138880026#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1728959646138880026#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:11:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：<br />
<br />
Intro: Large Language Model (LLM) talk<br />
<br />
大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。<br />
<br />
LLM Inference<br />
<br />
首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。<br />
<br />
以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。<br />
<br />
现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。<br />
<br />
在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。<br />
<br />
除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。<br />
<br />
你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。<br />
<br />
比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。<br />
<br />
我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。<br />
<br />
那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。</p>
<p><a href="https://nitter.cz/karpathy/status/1727731541781152035#m">nitter.cz/karpathy/status/1727731541781152035#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg5NTg2NTQ5Nzg2NjI0MDIvcHUvaW1nL096ak1ReDBBU0JqM29IUkYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728989154812498295#m</id>
            <title>R to @xiaohuggg: 0.025秒一张图…

😂

真是666</title>
            <link>https://nitter.cz/xiaohuggg/status/1728989154812498295#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728989154812498295#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 04:08:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>0.025秒一张图…<br />
<br />
😂<br />
<br />
真是666</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4NzcxODMyNjgwMTgxNzYwL2ltZy9UbDFWQmtBRFkzRFQxMURZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728988899161333970#m</id>
            <title>一位日本博主展示了LCM现在可以以大约40fps的速度生成图像，这使得完全实现实时应用成为可能。

@cumulo_autumn 展示了一个演示视频，该视频以1倍速（即实时速度）运行，包括OBS的屏幕录制和VRoid的渲染，运行速度约为36fps。他指出，如果不录制视频，速度可以达到39fps。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728988899161333970#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728988899161333970#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 04:07:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一位日本博主展示了LCM现在可以以大约40fps的速度生成图像，这使得完全实现实时应用成为可能。<br />
<br />
<a href="https://nitter.cz/cumulo_autumn" title="あき先生 | AI Vtuber『しずく』開発中">@cumulo_autumn</a> 展示了一个演示视频，该视频以1倍速（即实时速度）运行，包括OBS的屏幕录制和VRoid的渲染，运行速度约为36fps。他指出，如果不录制视频，速度可以达到39fps。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4NzY4Mzc0MTI0MjgxODU2L2ltZy9FRkx5RFBZQ3FZNGVYXzNiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>