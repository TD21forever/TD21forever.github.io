<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</id>
            <title>XHS-Downloader：小红书采集器 

✅ 采集小红书图文/视频作品信息
✅ 提取小红书图文/视频作品下载地址
✅ 下载小红书无水印图文/视频作品文件
✅ 自动跳过已下载的作品文件
✅ 作品文件完整性处理机制
✅ 持久化储存作品信息至文件

GitHub：https://github.com/JoeanAmier/XHS-Downloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737460981331288421#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XHS-Downloader：小红书采集器 <br />
<br />
✅ 采集小红书图文/视频作品信息<br />
✅ 提取小红书图文/视频作品下载地址<br />
✅ 下载小红书无水印图文/视频作品文件<br />
✅ 自动跳过已下载的作品文件<br />
✅ 作品文件完整性处理机制<br />
✅ 持久化储存作品信息至文件<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/XHS-Downloader">github.com/JoeanAmier/XHS-Do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5elhIR2JVQUFjb2hQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737442315319603200#m</id>
            <title>OpenAI董事会将拥有是否发布新AI大模型的决定权以及推翻OpenAI领导团队决定的权利。

OpenAI 本周发布了一份长达 27 页的框架文件，制定了如何防范 AI 大模型「灾难性风险」的路线图和安全等级。https://openai.com/safety/preparedness

新治理框架对AI大模型的发布进行了制衡，OpenAI表示，公司领导层拥有是否发布新AI模型的决策权，但董事会拥有最终发布的决定权，以及“推翻OpenAI领导团队决定的权利”。

OpenAI还表示，即便董事会拥有否决部署有潜在风险的人工智能模型的权利，OpenAI仍然会事先通过安全检查来确保这些人工智能大模型的部署是安全的。

安全等级：

为此，OpenAI将会专门建立一个“准备”团队（preparedness team），由麻省理工学院（MIT）教授Aleksander Madry领导，以监控和减轻OpenAI人工智能模型的潜在风险。该团队将负责评估并密切监控AI模型的潜在风险，并将这些不同的风险进行评分，将风险分类为“低”、“中”、“高”或“严重”。

OpenAI的治理框架也指出：“只有在降低风险后评分为‘中’或以下的AI模型才能部署，并且只有在降低风险后得分为‘高’或以下的模型才能进一步开发。”

该公司表示，目前这份监管框架性的文件仍处于“测试版”阶段，预计将根据反馈定期更新。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737442315319603200#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737442315319603200#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 11:58:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI董事会将拥有是否发布新AI大模型的决定权以及推翻OpenAI领导团队决定的权利。<br />
<br />
OpenAI 本周发布了一份长达 27 页的框架文件，制定了如何防范 AI 大模型「灾难性风险」的路线图和安全等级。<a href="https://openai.com/safety/preparedness">openai.com/safety/preparedne…</a><br />
<br />
新治理框架对AI大模型的发布进行了制衡，OpenAI表示，公司领导层拥有是否发布新AI模型的决策权，但董事会拥有最终发布的决定权，以及“推翻OpenAI领导团队决定的权利”。<br />
<br />
OpenAI还表示，即便董事会拥有否决部署有潜在风险的人工智能模型的权利，OpenAI仍然会事先通过安全检查来确保这些人工智能大模型的部署是安全的。<br />
<br />
安全等级：<br />
<br />
为此，OpenAI将会专门建立一个“准备”团队（preparedness team），由麻省理工学院（MIT）教授Aleksander Madry领导，以监控和减轻OpenAI人工智能模型的潜在风险。该团队将负责评估并密切监控AI模型的潜在风险，并将这些不同的风险进行评分，将风险分类为“低”、“中”、“高”或“严重”。<br />
<br />
OpenAI的治理框架也指出：“只有在降低风险后评分为‘中’或以下的AI模型才能部署，并且只有在降低风险后得分为‘高’或以下的模型才能进一步开发。”<br />
<br />
该公司表示，目前这份监管框架性的文件仍处于“测试版”阶段，预计将根据反馈定期更新。</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDQzODgyOTcyODA5NjI1Ny9Id1JrZmQ4az9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737426889822929238#m</id>
            <title>还说国内这帮人会玩、会商业化

Google这个模型其实和这个思路差不多，要是能实现这个视频说的功能就牛p了！

其实昨天Runway发布的语音功能也是为了想实现旁白+视频的故事模式。

主要是目前的生图和视频几乎都是抽卡一样，不知道这家是怎么能保证稳定和符合预期的输出！😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1737426889822929238#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737426889822929238#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 10:57:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还说国内这帮人会玩、会商业化<br />
<br />
Google这个模型其实和这个思路差不多，要是能实现这个视频说的功能就牛p了！<br />
<br />
其实昨天Runway发布的语音功能也是为了想实现旁白+视频的故事模式。<br />
<br />
主要是目前的生图和视频几乎都是抽卡一样，不知道这家是怎么能保证稳定和符合预期的输出！😐</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737371348467618039#m">nitter.cz/xiaohuggg/status/1737371348467618039#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM3NDI2NzQ1MjExNjUwMDQ4L2ltZy8ya2EtRmpYTlJHbTFoX3h1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737383561303564487#m</id>
            <title>R to @xiaohuggg: 另外还有个 SVD 结合AnimateDiff Refiner的教程

避免动画闪烁

https://www.patreon.com/posts/ai-svd-with-more-93812677</title>
            <link>https://nitter.cz/xiaohuggg/status/1737383561303564487#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737383561303564487#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:04:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另外还有个 SVD 结合AnimateDiff Refiner的教程<br />
<br />
避免动画闪烁<br />
<br />
<a href="https://www.patreon.com/posts/ai-svd-with-more-93812677">patreon.com/posts/ai-svd-wit…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODMyMDEwMzgwMzY5OTIvcHUvaW1nL1ZuN3diZU5wQlVVb3ZxQnIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737383138882670747#m</id>
            <title>R to @xiaohuggg: AnimateDiff - Hip Hop Girl演示

https://www.patreon.com/posts/ai-animatediff-93266466</title>
            <link>https://nitter.cz/xiaohuggg/status/1737383138882670747#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737383138882670747#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:03:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnimateDiff - Hip Hop Girl演示<br />
<br />
<a href="https://www.patreon.com/posts/ai-animatediff-93266466">patreon.com/posts/ai-animate…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODMwMDI4MTk0MzI0NDgvcHUvaW1nL1B3ZHAzVlhOcjN5Z1JiVDkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737382923983299001#m</id>
            <title>R to @xiaohuggg: 抽象类型的动画演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1737382923983299001#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737382923983299001#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:02:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>抽象类型的动画演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODI2OTcxNzYzNTA3MjAvcHUvaW1nL291OGEtNlFGb1p0N0F0aUUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737382606541627582#m</id>
            <title>R to @xiaohuggg: 一些演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1737382606541627582#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737382606541627582#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:01:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODI1NzQzNDYxNzAzNjgvcHUvaW1nL2VEVkMyYUFUNDRnRXBvOTkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737382498332815734#m</id>
            <title>这个视频的制作教程和workflow找到了

由于太过于技术，我看不太懂

反正就是应用了 LCM，渲染图像只需要10个采样步骤，大约减半了内存处理需求。这意味着你可以在批处理范围内输入更多的图像，同时还可以在制作原始通道时提高分辨率。

喜欢AI动画的SD大佬们可以研究下

工作流程分为五个部分：

ControlNet Passes Export：控制网络导出。
Animation Raw - LCM：原始动画 - LCM。
AnimateDiff Refiner - LCM：动画差异细化 - LCM。
AnimateDiff Face Fix - LCM：动画差异面部修复 - LCM。
Batch Face Swap - ReActor [实验性]：批量面部交换 - ReActor。

这种分解方式旨在提高处理效率，同时对内存和用户友好。

详细教程：https://www.patreon.com/posts/update-animate-94523632

视频教程：https://youtu.be/HbfDjAMFi6w</title>
            <link>https://nitter.cz/xiaohuggg/status/1737382498332815734#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737382498332815734#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:00:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个视频的制作教程和workflow找到了<br />
<br />
由于太过于技术，我看不太懂<br />
<br />
反正就是应用了 LCM，渲染图像只需要10个采样步骤，大约减半了内存处理需求。这意味着你可以在批处理范围内输入更多的图像，同时还可以在制作原始通道时提高分辨率。<br />
<br />
喜欢AI动画的SD大佬们可以研究下<br />
<br />
工作流程分为五个部分：<br />
<br />
ControlNet Passes Export：控制网络导出。<br />
Animation Raw - LCM：原始动画 - LCM。<br />
AnimateDiff Refiner - LCM：动画差异细化 - LCM。<br />
AnimateDiff Face Fix - LCM：动画差异面部修复 - LCM。<br />
Batch Face Swap - ReActor [实验性]：批量面部交换 - ReActor。<br />
<br />
这种分解方式旨在提高处理效率，同时对内存和用户友好。<br />
<br />
详细教程：<a href="https://www.patreon.com/posts/update-animate-94523632">patreon.com/posts/update-ani…</a><br />
<br />
视频教程：<a href="https://youtu.be/HbfDjAMFi6w">youtu.be/HbfDjAMFi6w</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNzYxNDQwNTAzMTExNjgvcHUvaW1nL1lKT0tUVmRsNDN5SlBFdFIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737371651313193099#m</id>
            <title>R to @xiaohuggg: 演示视频：

VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。

为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。

这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。

这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737371651313193099#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737371651313193099#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 07:17:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频：<br />
<br />
VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。<br />
<br />
为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。<br />
<br />
这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。<br />
<br />
这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNzE1OTI0MDgzNDY2MjQvcHUvaW1nL2lmRFVva0p6blZJX29tazguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737371534074016149#m</id>
            <title>R to @xiaohuggg: 根据视频生成音频</title>
            <link>https://nitter.cz/xiaohuggg/status/1737371534074016149#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737371534074016149#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 07:17:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据视频生成音频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNzE0NzM4NjA0OTc0MDgvcHUvaW1nL2RMelhCRkNQSmIwZ2RyQm4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737371348467618039#m</id>
            <title>Google的一个新的视频模型：VideoPoet  

它可以根据文字描述来生成视频。但它不是基于扩散模型，而本身就是个LLM，可以理解和处理多模态信息，并将它们融合到视频生成过程中。  

不仅能生成视频，还能给视频加上风格化的效果，还可修复和扩展视频，甚至从视频中生成音频。  

一条龙服务...

例如，VideoPoet 可以根据文本描述生成视频，或者将一张静态图片转换成动态视频。它还能理解和生成音频，甚至是编写用于视频处理的代码。

这种多模态学习能力使得 VideoPoet 在视频生成方面更加灵活和强大，能够处理更复杂和多样化的任务。

演示视频：

VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。

为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。

这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。

这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。

VideoPoet 的主要功能特点：

1、广泛的视频生成任务：VideoPoet 能够处理多种视频生成任务，包括文本到视频、图像到视频、视频风格化、视频修复和扩展、以及视频到音频。

2、多模态学习能力：与主要基于扩散的视频生成模型不同，VideoPoet 作为一个大型语言模型，在多种模态上展现出卓越的学习能力，包括语言、代码和音频。

3、集成多种视频生成能力：VideoPoet 在单一的大型语言模型中集成了多种视频生成能力，而不是依赖于针对每项任务单独训练的组件。

5、任务设计：VideoPoet 能够根据不同的任务需求（如文本到视频、图像到视频等）调整其生成过程。每种任务类型都由特定的任务标记指示，以引导模型进行相应的视频生成。

6、长视频生成：通过连续预测的方式，VideoPoet 能够生成更长的视频。它通过在每一步中仅考虑视频的最后一部分（例如最后1秒），然后预测接下来的内容，从而实现视频的延伸。

7.、交互式视频编辑：允许用户交互式地编辑视频，例如改变视频中对象的动作或行为。这是通过在输入视频的基础上添加新的文本提示来实现的。

8、图像到视频的控制：能够根据文本提示将输入图像动画化，编辑其内容。

9、相机运动控制：通过在文本提示中添加特定的相机运动描述（如缩放、平移、弧形拍摄等）它能够在生成的视频中实现这些相机运动。

工作原理：

VideoPoet基于大语言模型（LLM），结合了多模态学习和自回归模型。

VideoPoet 使用大语言模型（LLM）用于处理和生成文本，但经过训练，也能理解和生成视频和音频。

结合了多模态学习，VideoPoet 能处理多种类型的输入和输出（如文本、图像、视频和音频），它可以将不同类型的信息（如文本描述和图像内容）结合起来，创造出新的视频内容。

自回归模型：它在生成视频的每一步都依赖于之前的步骤。这样，它可以逐渐构建起整个视频，确保视频内容的连贯性和一致性。

视频和音频的编码与解码：为了处理视频和音频，VideoPoet 使用特殊的编码器（如 MAGVIT V2 和 SoundStream）和解码器将这些内容转换为模型能理解的格式，然后再将生成的内容转换回可视或可听的格式。

详细介绍：https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html
演示：https://sites.research.google/videopoet/</title>
            <link>https://nitter.cz/xiaohuggg/status/1737371348467618039#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737371348467618039#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 07:16:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google的一个新的视频模型：VideoPoet  <br />
<br />
它可以根据文字描述来生成视频。但它不是基于扩散模型，而本身就是个LLM，可以理解和处理多模态信息，并将它们融合到视频生成过程中。  <br />
<br />
不仅能生成视频，还能给视频加上风格化的效果，还可修复和扩展视频，甚至从视频中生成音频。  <br />
<br />
一条龙服务...<br />
<br />
例如，VideoPoet 可以根据文本描述生成视频，或者将一张静态图片转换成动态视频。它还能理解和生成音频，甚至是编写用于视频处理的代码。<br />
<br />
这种多模态学习能力使得 VideoPoet 在视频生成方面更加灵活和强大，能够处理更复杂和多样化的任务。<br />
<br />
演示视频：<br />
<br />
VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。<br />
<br />
为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。<br />
<br />
这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。<br />
<br />
这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。<br />
<br />
VideoPoet 的主要功能特点：<br />
<br />
1、广泛的视频生成任务：VideoPoet 能够处理多种视频生成任务，包括文本到视频、图像到视频、视频风格化、视频修复和扩展、以及视频到音频。<br />
<br />
2、多模态学习能力：与主要基于扩散的视频生成模型不同，VideoPoet 作为一个大型语言模型，在多种模态上展现出卓越的学习能力，包括语言、代码和音频。<br />
<br />
3、集成多种视频生成能力：VideoPoet 在单一的大型语言模型中集成了多种视频生成能力，而不是依赖于针对每项任务单独训练的组件。<br />
<br />
5、任务设计：VideoPoet 能够根据不同的任务需求（如文本到视频、图像到视频等）调整其生成过程。每种任务类型都由特定的任务标记指示，以引导模型进行相应的视频生成。<br />
<br />
6、长视频生成：通过连续预测的方式，VideoPoet 能够生成更长的视频。它通过在每一步中仅考虑视频的最后一部分（例如最后1秒），然后预测接下来的内容，从而实现视频的延伸。<br />
<br />
7.、交互式视频编辑：允许用户交互式地编辑视频，例如改变视频中对象的动作或行为。这是通过在输入视频的基础上添加新的文本提示来实现的。<br />
<br />
8、图像到视频的控制：能够根据文本提示将输入图像动画化，编辑其内容。<br />
<br />
9、相机运动控制：通过在文本提示中添加特定的相机运动描述（如缩放、平移、弧形拍摄等）它能够在生成的视频中实现这些相机运动。<br />
<br />
工作原理：<br />
<br />
VideoPoet基于大语言模型（LLM），结合了多模态学习和自回归模型。<br />
<br />
VideoPoet 使用大语言模型（LLM）用于处理和生成文本，但经过训练，也能理解和生成视频和音频。<br />
<br />
结合了多模态学习，VideoPoet 能处理多种类型的输入和输出（如文本、图像、视频和音频），它可以将不同类型的信息（如文本描述和图像内容）结合起来，创造出新的视频内容。<br />
<br />
自回归模型：它在生成视频的每一步都依赖于之前的步骤。这样，它可以逐渐构建起整个视频，确保视频内容的连贯性和一致性。<br />
<br />
视频和音频的编码与解码：为了处理视频和音频，VideoPoet 使用特殊的编码器（如 MAGVIT V2 和 SoundStream）和解码器将这些内容转换为模型能理解的格式，然后再将生成的内容转换回可视或可听的格式。<br />
<br />
详细介绍：<a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html">blog.research.google/2023/12…</a><br />
演示：<a href="https://sites.research.google/videopoet/">sites.research.google/videop…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNjk5NTU5MzM1ODk1MDQvcHUvaW1nL3BoRzRKQWlCVTltYkNzVWouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737321380511891550#m</id>
            <title>HyFluid：从视频中“读懂”流体的运动状态

假如你有一些视频片段，里面有水、烟或其他流动的物质。HyFluid 能够分析这些视频，然后告诉你这些流体是如何移动的，比如它们的速度和密度。

更牛P的是它不仅能告诉你这个流体现在是怎么流动的，还能预测它将来会怎么流动。

人类肉眼看不到的也能识别...

这个项目特别厉害的地方在于，它能处理那些看起来很混乱、难以捉摸的流体运动。通常，我们很难从肉眼看到的流体运动中准确地知道流体是怎么流动的，但 HyFluid 利用先进的计算方法，可以做到这一点。

HyFluid 项目的主要特点和功能：

1、流体动态重建：HyFluid 能够从稀疏的多视角视频中推断出流体的密度和速度，这对于理解和模拟流体动力学非常重要。比如拍摄烟雾或雾的视频，然后告诉你这些流体是怎么流动的，比如它们流动得有多快，有多密集。

2、重建3D流体场景：特别擅长处理那些我们无法压缩的流体，就像烟雾或雾这样的东西。它能创建出这些流体在三维空间中的模型，让我们能更好地理解它们的行为。

3、重新制作和改变流体动态：HyFluid 让你能够从不同的角度重新看流体是怎么动的，甚至还可以改变它们的动态。这对于制作电影里的特效或者进行科学实验来说非常有用。

4、预测流体将来会怎样：除了告诉我们流体现在是怎么动的，HyFluid 还可以预测它们将来会怎样流动。这对于做科学预测或者创造动态的电脑模拟场景很有帮助。

5、物理基础的损失函数：为了解决流体速度的视觉模糊问题，HyFluid 引入了一系列基于物理的损失函数，这些函数强制推断出物理上可行的速度场，这些速度场是无散度的，并驱动密度的传输。

6、混合神经速度表示：为了处理流体速度的湍流特性，HyFluid 设计了一种混合神经速度表示，包括一个基础神经速度场，用于捕捉大部分无旋能量，以及一个基于涡旋粒子的速度，用于模拟剩余的湍流速度。

7、恢复涡流流动细节：HyFluid 的方法能够恢复涡流流动的细节，为各种学习和重建应用开辟了可能性，包括3D不可压缩流体的重新模拟和编辑、未来预测以及神经动态场景构建。

应用示例：HyFluid 可以从几个多视角视频（例如三个视角）中推断出3D流体的密度和速度场，并允许从新的视角重新模拟流体动力学，预测流体的未来演变，并将恢复的3D密度和流体外观轻松地合成到任何类似NeRF的动态场景中。

HyFluid 项目展示了如何利用先进的机器学习技术来解决复杂的流体动力学问题，为流体动力学的研究和应用提供了新的工具和方法。

项目及演示：https://kovenyu.com/hyfluid/
论文：https://arxiv.org/pdf/2312.06561.pdf
GitHub：https://github.com/y-zheng18/HyFluid</title>
            <link>https://nitter.cz/xiaohuggg/status/1737321380511891550#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737321380511891550#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 03:57:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HyFluid：从视频中“读懂”流体的运动状态<br />
<br />
假如你有一些视频片段，里面有水、烟或其他流动的物质。HyFluid 能够分析这些视频，然后告诉你这些流体是如何移动的，比如它们的速度和密度。<br />
<br />
更牛P的是它不仅能告诉你这个流体现在是怎么流动的，还能预测它将来会怎么流动。<br />
<br />
人类肉眼看不到的也能识别...<br />
<br />
这个项目特别厉害的地方在于，它能处理那些看起来很混乱、难以捉摸的流体运动。通常，我们很难从肉眼看到的流体运动中准确地知道流体是怎么流动的，但 HyFluid 利用先进的计算方法，可以做到这一点。<br />
<br />
HyFluid 项目的主要特点和功能：<br />
<br />
1、流体动态重建：HyFluid 能够从稀疏的多视角视频中推断出流体的密度和速度，这对于理解和模拟流体动力学非常重要。比如拍摄烟雾或雾的视频，然后告诉你这些流体是怎么流动的，比如它们流动得有多快，有多密集。<br />
<br />
2、重建3D流体场景：特别擅长处理那些我们无法压缩的流体，就像烟雾或雾这样的东西。它能创建出这些流体在三维空间中的模型，让我们能更好地理解它们的行为。<br />
<br />
3、重新制作和改变流体动态：HyFluid 让你能够从不同的角度重新看流体是怎么动的，甚至还可以改变它们的动态。这对于制作电影里的特效或者进行科学实验来说非常有用。<br />
<br />
4、预测流体将来会怎样：除了告诉我们流体现在是怎么动的，HyFluid 还可以预测它们将来会怎样流动。这对于做科学预测或者创造动态的电脑模拟场景很有帮助。<br />
<br />
5、物理基础的损失函数：为了解决流体速度的视觉模糊问题，HyFluid 引入了一系列基于物理的损失函数，这些函数强制推断出物理上可行的速度场，这些速度场是无散度的，并驱动密度的传输。<br />
<br />
6、混合神经速度表示：为了处理流体速度的湍流特性，HyFluid 设计了一种混合神经速度表示，包括一个基础神经速度场，用于捕捉大部分无旋能量，以及一个基于涡旋粒子的速度，用于模拟剩余的湍流速度。<br />
<br />
7、恢复涡流流动细节：HyFluid 的方法能够恢复涡流流动的细节，为各种学习和重建应用开辟了可能性，包括3D不可压缩流体的重新模拟和编辑、未来预测以及神经动态场景构建。<br />
<br />
应用示例：HyFluid 可以从几个多视角视频（例如三个视角）中推断出3D流体的密度和速度场，并允许从新的视角重新模拟流体动力学，预测流体的未来演变，并将恢复的3D密度和流体外观轻松地合成到任何类似NeRF的动态场景中。<br />
<br />
HyFluid 项目展示了如何利用先进的机器学习技术来解决复杂的流体动力学问题，为流体动力学的研究和应用提供了新的工具和方法。<br />
<br />
项目及演示：<a href="https://kovenyu.com/hyfluid/">kovenyu.com/hyfluid/</a><br />
论文：<a href="https://arxiv.org/pdf/2312.06561.pdf">arxiv.org/pdf/2312.06561.pdf</a><br />
GitHub：<a href="https://github.com/y-zheng18/HyFluid">github.com/y-zheng18/HyFluid</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczMjEwOTUxNTI1MTMwMjQvcHUvaW1nL0s2MHVmWEFhd1VJMThmMkEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737316352145371140#m</id>
            <title>使用AnimateDiff 

将视频转换为任何其他风格样式 

搬运洗稿利器😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1737316352145371140#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737316352145371140#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 03:37:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用AnimateDiff <br />
<br />
将视频转换为任何其他风格样式 <br />
<br />
搬运洗稿利器😄</p>
<p><a href="https://nitter.cz/DiffusionPics/status/1737218402136043590#m">nitter.cz/DiffusionPics/status/1737218402136043590#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737311836176105673#m</id>
            <title>Twitch近日对其“色情内容处理方法”进行了大规模更新。

更新后的指导方针允许主播在穿着完整且有正确标签的情况下，直播中刻意突出乳房、臀部或盆骨区域的内容。

然后一些女主播就开始了疯狂试探......

测试这个条款极限到底是什么...😀

Twitch 还声称，其之前对这类内容的态度不符合行业标准，导致女性主播受到不成比例的惩罚。现在，包括虚构的完全暴露的女性乳房和/或无论性别的生殖器或臀部、体绘以及涉及脱衣舞或脱衣舞姿势的色情舞蹈等内容，在有正确标签的情况下也被允许。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737311836176105673#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737311836176105673#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 03:19:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Twitch近日对其“色情内容处理方法”进行了大规模更新。<br />
<br />
更新后的指导方针允许主播在穿着完整且有正确标签的情况下，直播中刻意突出乳房、臀部或盆骨区域的内容。<br />
<br />
然后一些女主播就开始了疯狂试探......<br />
<br />
测试这个条款极限到底是什么...😀<br />
<br />
Twitch 还声称，其之前对这类内容的态度不符合行业标准，导致女性主播受到不成比例的惩罚。现在，包括虚构的完全暴露的女性乳房和/或无论性别的生殖器或臀部、体绘以及涉及脱衣舞或脱衣舞姿势的色情舞蹈等内容，在有正确标签的情况下也被允许。</p>
<p><a href="https://nitter.cz/Dexerto/status/1737152845609062908#m">nitter.cz/Dexerto/status/1737152845609062908#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737306585674977785#m</id>
            <title>R to @xiaohuggg: Runway 文字转语音功能

配合Runway生成的视频

绝了👋</title>
            <link>https://nitter.cz/xiaohuggg/status/1737306585674977785#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737306585674977785#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 02:59:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 文字转语音功能<br />
<br />
配合Runway生成的视频<br />
<br />
绝了👋</p>
<p><a href="https://nitter.cz/iamneubert/status/1737155682950938934#m">nitter.cz/iamneubert/status/1737155682950938934#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737305135670546861#m</id>
            <title>微软宣布与 Suno 合作，将 Suno 的人工智能音乐创作能力引入到Copilot 中。

Suno能够从一句话生成完整的歌曲——包括歌词、乐器伴奏和歌声。

你只需按照以下步骤操作即可体验：

- 打开 Microsoft Edge，访问 http://copilot.microsoft.com 并登录。

- 启用 Suno 插件或点击显示“使用 Suno 制作音乐”的 Suno 标志。

- 发送创作提示，例如：“为我创作一首关于与家人冒险的流行歌曲”。

- 稍等一会即可完成，欣赏你的新音乐。

详细：https://blogs.bing.com/cmsctx/pv/JandJ/culture/en-US/wg/d46f1dcb-1009-4263-8d42-15c762ec5019/h/e256ae63c417284cb35df9ddd14d1e9f807ad5053e5e825a1f9b2a4569ca6a13/-/cms/getdoc/c4d04949-3857-4608-afb3-bb10eb3ed45a/pv.aspx</title>
            <link>https://nitter.cz/xiaohuggg/status/1737305135670546861#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737305135670546861#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 02:53:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软宣布与 Suno 合作，将 Suno 的人工智能音乐创作能力引入到Copilot 中。<br />
<br />
Suno能够从一句话生成完整的歌曲——包括歌词、乐器伴奏和歌声。<br />
<br />
你只需按照以下步骤操作即可体验：<br />
<br />
- 打开 Microsoft Edge，访问 <a href="http://copilot.microsoft.com">copilot.microsoft.com</a> 并登录。<br />
<br />
- 启用 Suno 插件或点击显示“使用 Suno 制作音乐”的 Suno 标志。<br />
<br />
- 发送创作提示，例如：“为我创作一首关于与家人冒险的流行歌曲”。<br />
<br />
- 稍等一会即可完成，欣赏你的新音乐。<br />
<br />
详细：<a href="https://blogs.bing.com/cmsctx/pv/JandJ/culture/en-US/wg/d46f1dcb-1009-4263-8d42-15c762ec5019/h/e256ae63c417284cb35df9ddd14d1e9f807ad5053e5e825a1f9b2a4569ca6a13/-/cms/getdoc/c4d04949-3857-4608-afb3-bb10eb3ed45a/pv.aspx">blogs.bing.com/cmsctx/pv/Jan…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1700133814096498922#m">nitter.cz/xiaohuggg/status/1700133814096498922#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737299753078305093#m</id>
            <title>R to @xiaohuggg: 歌声转换在线体验

目前支持以下一些歌手...嘿嘿 

HF：https://huggingface.co/spaces/amphion/singing_voice_conversion</title>
            <link>https://nitter.cz/xiaohuggg/status/1737299753078305093#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737299753078305093#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 02:31:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>歌声转换在线体验<br />
<br />
目前支持以下一些歌手...嘿嘿 <br />
<br />
HF：<a href="https://huggingface.co/spaces/amphion/singing_voice_conversion">huggingface.co/spaces/amphio…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J3Z3ltb2E0QUFNWnJoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737299090848018565#m</id>
            <title>🔊 Amphion：是一个开源工具包，可实现语音、声音和歌唱功能。

除了文字转语音功能，它还能可以将一首歌的声音换成另一个歌手的声音。还支持声音转换、歌声合成、文本到音频、文本到音乐等功能！

功能十分强大！

演示视频：Taylor Swift演唱中文歌曲 🎵

Amphion 支持的音频生成任务涵盖了从文本到音乐的广泛领域，每个任务都有其独特的应用和技术要求。

主要功能：

1、文本到语音：将文本转换为口语化的语音。

应用：用于制作语音助手、自动语音回复系统、为视障人士阅读文本等。

2、歌声合成：创造虚拟歌手的声音，可以从文本或旋律生成歌声。

应用：用于音乐制作、虚拟偶像创作等。

3、声音转换：改变一个人的声音，使其听起来像另一个人。

应用：用于娱乐、声音设计、匿名通信等。

4、歌声转换：将一首歌的演唱者的声音转换成另一个演唱者的声音。

应用：用于音乐制作、个性化音乐体验等。

5、文本到音频：不仅将文本转换为语音，还可以转换成其他类型的音频，如音效或音乐片段。

应用：用于创造音效、音乐片段、音频故事等。

6、文本到音乐：从文本描述中生成音乐。
应用：用于自动音乐创作、根据情感或故事情节创作音乐等。

模型支持：该工具包支持多种模型和架构，如FastSpeech2、VITS、Vall-E、NaturalSpeech2等，用于不同的音频生成任务。

声码器支持：Amphion 支持多种神经声码器，包括基于GAN的声码器（如MelGAN、HiFi-GAN）、基于流的声码器（如WaveGlow）、基于扩散的声码器（如Diffwave）等。

数据集支持：Amphion 统一了开源数据集的数据预处理，支持多种数据集，如AudioCaps、LibriTTS、LJSpeech等。

GitHub：https://github.com/open-mmlab/Amphion
论文：https://arxiv.org/abs/2312.09911
HuggingFace演示：https://huggingface.co/amphion</title>
            <link>https://nitter.cz/xiaohuggg/status/1737299090848018565#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737299090848018565#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 02:29:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔊 Amphion：是一个开源工具包，可实现语音、声音和歌唱功能。<br />
<br />
除了文字转语音功能，它还能可以将一首歌的声音换成另一个歌手的声音。还支持声音转换、歌声合成、文本到音频、文本到音乐等功能！<br />
<br />
功能十分强大！<br />
<br />
演示视频：Taylor Swift演唱中文歌曲 🎵<br />
<br />
Amphion 支持的音频生成任务涵盖了从文本到音乐的广泛领域，每个任务都有其独特的应用和技术要求。<br />
<br />
主要功能：<br />
<br />
1、文本到语音：将文本转换为口语化的语音。<br />
<br />
应用：用于制作语音助手、自动语音回复系统、为视障人士阅读文本等。<br />
<br />
2、歌声合成：创造虚拟歌手的声音，可以从文本或旋律生成歌声。<br />
<br />
应用：用于音乐制作、虚拟偶像创作等。<br />
<br />
3、声音转换：改变一个人的声音，使其听起来像另一个人。<br />
<br />
应用：用于娱乐、声音设计、匿名通信等。<br />
<br />
4、歌声转换：将一首歌的演唱者的声音转换成另一个演唱者的声音。<br />
<br />
应用：用于音乐制作、个性化音乐体验等。<br />
<br />
5、文本到音频：不仅将文本转换为语音，还可以转换成其他类型的音频，如音效或音乐片段。<br />
<br />
应用：用于创造音效、音乐片段、音频故事等。<br />
<br />
6、文本到音乐：从文本描述中生成音乐。<br />
应用：用于自动音乐创作、根据情感或故事情节创作音乐等。<br />
<br />
模型支持：该工具包支持多种模型和架构，如FastSpeech2、VITS、Vall-E、NaturalSpeech2等，用于不同的音频生成任务。<br />
<br />
声码器支持：Amphion 支持多种神经声码器，包括基于GAN的声码器（如MelGAN、HiFi-GAN）、基于流的声码器（如WaveGlow）、基于扩散的声码器（如Diffwave）等。<br />
<br />
数据集支持：Amphion 统一了开源数据集的数据预处理，支持多种数据集，如AudioCaps、LibriTTS、LJSpeech等。<br />
<br />
GitHub：<a href="https://github.com/open-mmlab/Amphion">github.com/open-mmlab/Amphio…</a><br />
论文：<a href="https://arxiv.org/abs/2312.09911">arxiv.org/abs/2312.09911</a><br />
HuggingFace演示：<a href="https://huggingface.co/amphion">huggingface.co/amphion</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzcyOTU4MTI2NDcxMDQ1MTIvcHUvaW1nL0NlcGMtNUNOVnhHS0kzb1kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>