<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748942568526844109#m</id>
            <title>TikTok似乎修改了规则

现在国内也可以直接刷TikTok了，以前拔卡、换区、梯子…都不能访问！

现在只需要在iPhone设置里面把地区改为香港、大陆之外即可！

然后梯子也挂香港、大陆之外即可访问了！

亲测有效…👀⬇️</title>
            <link>https://nitter.cz/xiaohuggg/status/1748942568526844109#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748942568526844109#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 05:36:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TikTok似乎修改了规则<br />
<br />
现在国内也可以直接刷TikTok了，以前拔卡、换区、梯子…都不能访问！<br />
<br />
现在只需要在iPhone设置里面把地区改为香港、大陆之外即可！<br />
<br />
然后梯子也挂香港、大陆之外即可访问了！<br />
<br />
亲测有效…👀⬇️</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ4OTQyMDY2NjE5NjUwMDQ5L2ltZy9YOHg5eEhad0ViUGsxOWZGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748914170580599022#m</id>
            <title>33岁日本作家Rie Kudan赢得日本顶级文学奖芥川奖，她在颁奖典礼上公开宣布，她使用了ChatGPT来写作她的获奖小说中大约5%的内容。

据报道，评委们称Rie Kudan的小说 “几乎完美无瑕”

她的科幻小说《Tokyo-to Dojo-to / Tokyo Sympathy Tower》讲述了一个高耸的监狱塔的故事，主题围绕人工智能。

芥川奖是日本纯文学领域的顶级奖项，每半年颁发一次，授予新兴作家。获奖者通常会受到大量媒体关注。

《日本时报》周五报道称，社交媒体上的反应迅速而严厉，许多评论者表示担心，如果允许人工智能竞争最高奖项，文学的未来会是什么样子？

有关在创意领域使用生成式AI的争议仍然很大，因为这些系统是基于大量其他作者作品的语料库训练的。

详细：https://www.vice.com/en/article/k7z58y/rie-kudan-akutagawa-prize-used-chatgpt</title>
            <link>https://nitter.cz/xiaohuggg/status/1748914170580599022#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748914170580599022#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 03:43:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>33岁日本作家Rie Kudan赢得日本顶级文学奖芥川奖，她在颁奖典礼上公开宣布，她使用了ChatGPT来写作她的获奖小说中大约5%的内容。<br />
<br />
据报道，评委们称Rie Kudan的小说 “几乎完美无瑕”<br />
<br />
她的科幻小说《Tokyo-to Dojo-to / Tokyo Sympathy Tower》讲述了一个高耸的监狱塔的故事，主题围绕人工智能。<br />
<br />
芥川奖是日本纯文学领域的顶级奖项，每半年颁发一次，授予新兴作家。获奖者通常会受到大量媒体关注。<br />
<br />
《日本时报》周五报道称，社交媒体上的反应迅速而严厉，许多评论者表示担心，如果允许人工智能竞争最高奖项，文学的未来会是什么样子？<br />
<br />
有关在创意领域使用生成式AI的争议仍然很大，因为这些系统是基于大量其他作者作品的语料库训练的。<br />
<br />
详细：<a href="https://www.vice.com/en/article/k7z58y/rie-kudan-akutagawa-prize-used-chatgpt">vice.com/en/article/k7z58y/r…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VWaWhFRGJ3QUE2MGRzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748910327658598697#m</id>
            <title>Medivis SurgicalAR ：已通过FDA批准、由AI驱动的增强现实手术平台。

通过使用AR全息图像，医生可以更精确地规划手术步骤，并在执行过程中获得实时的视觉辅助。

使医生能够以前所未有的细节和清晰度查看病人的内部结构。从而提高手术的成功率和安全性。

在手术过程中还能提供即时反馈。

在神经外科、骨科和重建手术等领域，SurgicalAR 提供了一种新的手术视觉化方法，使医生能够在手术过程中看到高度详细的全息图像。

Medivis的技术已在超过40家家医院和医学院中部署，支持医生在手术室、ICU和诊所中使用，为不同医疗情景提供支持。

Medivis SurgicalAR 的技术集成：

1、增强现实（AR）：SurgicalAR 使用增强现实技术，通过全息图像为医生提供手术过程中的视觉辅助。这种技术能够将3D图像直观地展示在医生的视野中，使他们能够更精确地定位和了解病人的内部结构。

2、人工智能（AI）：SurgicalAR 结合了人工智能，特别是在图像处理和数据分析方面。AI能够帮助解析医学图像，提供更深入的诊断信息，并辅助医生制定更有效的手术计划。

3、计算机视觉：用于精确识别和追踪手术区域，提高手术的精确性和安全性。它还支持实时图像分析，确保手术过程的精准度。

Medivis SurgicalAR 的主要功能包括：

1、增强现实导航：使用增强现实技术在手术中提供全息图像导航，帮助医生更准确地定位和执行手术步骤。

2、高级可视化：提供高度真实的医学图像渲染，使医生能够以前所未有的细节和清晰度查看病人的内部结构。

3、手势控制操作：通过AR/AI用户界面，医生可以使用直观的手势来操作和探索3D医学图像，如旋转、缩放和裁剪。

4、数据集成：允许从多种来源导入医学数据（如CT、MRI和超声数据），并快速将这些数据转换为3D全息图像。

5、实时反馈：在手术过程中提供即时反馈，帮助医生做出更准确的临床决策。

6、个性化手术规划：根据病人特定的医学数据定制手术方案，提高手术的个性化和精准度。

7、多学科应用：适用于多个医学领域，包括神经外科、骨科和重建手术等。

Medivis SurgicalAR 通过结合增强现实和人工智能技术，为医生提供了一个强大的工具，以提高手术的精确性和安全性，并改善病人的治疗结果。

官网：https://www.medivis.com/surgical-ar

视频介绍：http://bit.ly/3O5RwGA</title>
            <link>https://nitter.cz/xiaohuggg/status/1748910327658598697#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748910327658598697#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 03:28:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Medivis SurgicalAR ：已通过FDA批准、由AI驱动的增强现实手术平台。<br />
<br />
通过使用AR全息图像，医生可以更精确地规划手术步骤，并在执行过程中获得实时的视觉辅助。<br />
<br />
使医生能够以前所未有的细节和清晰度查看病人的内部结构。从而提高手术的成功率和安全性。<br />
<br />
在手术过程中还能提供即时反馈。<br />
<br />
在神经外科、骨科和重建手术等领域，SurgicalAR 提供了一种新的手术视觉化方法，使医生能够在手术过程中看到高度详细的全息图像。<br />
<br />
Medivis的技术已在超过40家家医院和医学院中部署，支持医生在手术室、ICU和诊所中使用，为不同医疗情景提供支持。<br />
<br />
Medivis SurgicalAR 的技术集成：<br />
<br />
1、增强现实（AR）：SurgicalAR 使用增强现实技术，通过全息图像为医生提供手术过程中的视觉辅助。这种技术能够将3D图像直观地展示在医生的视野中，使他们能够更精确地定位和了解病人的内部结构。<br />
<br />
2、人工智能（AI）：SurgicalAR 结合了人工智能，特别是在图像处理和数据分析方面。AI能够帮助解析医学图像，提供更深入的诊断信息，并辅助医生制定更有效的手术计划。<br />
<br />
3、计算机视觉：用于精确识别和追踪手术区域，提高手术的精确性和安全性。它还支持实时图像分析，确保手术过程的精准度。<br />
<br />
Medivis SurgicalAR 的主要功能包括：<br />
<br />
1、增强现实导航：使用增强现实技术在手术中提供全息图像导航，帮助医生更准确地定位和执行手术步骤。<br />
<br />
2、高级可视化：提供高度真实的医学图像渲染，使医生能够以前所未有的细节和清晰度查看病人的内部结构。<br />
<br />
3、手势控制操作：通过AR/AI用户界面，医生可以使用直观的手势来操作和探索3D医学图像，如旋转、缩放和裁剪。<br />
<br />
4、数据集成：允许从多种来源导入医学数据（如CT、MRI和超声数据），并快速将这些数据转换为3D全息图像。<br />
<br />
5、实时反馈：在手术过程中提供即时反馈，帮助医生做出更准确的临床决策。<br />
<br />
6、个性化手术规划：根据病人特定的医学数据定制手术方案，提高手术的个性化和精准度。<br />
<br />
7、多学科应用：适用于多个医学领域，包括神经外科、骨科和重建手术等。<br />
<br />
Medivis SurgicalAR 通过结合增强现实和人工智能技术，为医生提供了一个强大的工具，以提高手术的精确性和安全性，并改善病人的治疗结果。<br />
<br />
官网：<a href="https://www.medivis.com/surgical-ar">medivis.com/surgical-ar</a><br />
<br />
视频介绍：<a href="http://bit.ly/3O5RwGA">bit.ly/3O5RwGA</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg5MDY4MTUwNjMyOTgwNDgvcHUvaW1nL2pnaFRRRmxkdTh3ZEk4SmIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748900504468619358#m</id>
            <title>NFL Pick-Em's LLM Bot ：基于 OpenAI 构建的AI代理，可以预测ESPN体育比赛结果。

在 2023 NFL季赛中，这个代理赢得了作者当地群组的 pick-em's 比赛，并在 ESPN 所有用户中排名前 15% （约一百万用户参加）。

2023 年 ESPN 的顶级球员人类预测者的准确率大约是71%，而代理的准确率则是60%。

工作原理：

1、数据收集：使用网络爬虫技术，从ESPN网站上爬取相关的统计数据和新闻文章。爬取的数据包括各个NFL团队的信息、每周比赛列表、进攻、防守、失误和特殊团队的统计数据，以及新闻头条和文章内容。

统计数据，包括总码数、传球码数、冲球码数、每场比赛得分等，以及防守数据，如擒抱、拦截和抢断等。

特别队伍统计数据：收集关于特别队伍表现的数据，例如射门成功率、平均踢球距离和回传码数。

失误数据：包括数据如失误次数和被拦截次数。

相关新闻：收集关于球队的最新消息，包括球员伤病、转会、教练更换或场外事件，这些可能影响球队表现。

2、数据分析：利用GPT 4，对收集到的数据进行分析。程序会将每场比赛的相关统计数据和新闻信息输入给GPT 4，并让它预测比赛的获胜者。

将收集到的数据输入LLM。模型将分析这些统计数据和新闻，以了解两支球队的当前状态、优势、劣势和近期变化。

可以对LLM进行训练或微调，使其更好地理解类似比赛的历史数据模式和结果。

3、预测获胜者：基于分析的结果，代理知道如何处理特定的任务，例如分析新闻文章（确定文章主要讨论的团队、提取文章摘要等）和预测比赛获胜者。

模型还可以提供预测的理由，基于分析的数据，使预测更加深入。

4、优化和反馈循环：比赛结束后，可以将实际结果反馈到系统中，以帮助优化预测算法，随着时间的推移提高准确性。

尽管LLM能够处理和分析大量数据，但其预测的质量和准确性在很大程度上取决于所提供数据的质量以及模型对这一特定任务的训练程度。这种方法结合了统计分析和基于AI的预测建模，为体育分析提供了一种新颖的方式。

GitHub：https://github.com/stevekrenzel/pick-ems</title>
            <link>https://nitter.cz/xiaohuggg/status/1748900504468619358#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748900504468619358#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 02:49:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NFL Pick-Em's LLM Bot ：基于 OpenAI 构建的AI代理，可以预测ESPN体育比赛结果。<br />
<br />
在 2023 NFL季赛中，这个代理赢得了作者当地群组的 pick-em's 比赛，并在 ESPN 所有用户中排名前 15% （约一百万用户参加）。<br />
<br />
2023 年 ESPN 的顶级球员人类预测者的准确率大约是71%，而代理的准确率则是60%。<br />
<br />
工作原理：<br />
<br />
1、数据收集：使用网络爬虫技术，从ESPN网站上爬取相关的统计数据和新闻文章。爬取的数据包括各个NFL团队的信息、每周比赛列表、进攻、防守、失误和特殊团队的统计数据，以及新闻头条和文章内容。<br />
<br />
统计数据，包括总码数、传球码数、冲球码数、每场比赛得分等，以及防守数据，如擒抱、拦截和抢断等。<br />
<br />
特别队伍统计数据：收集关于特别队伍表现的数据，例如射门成功率、平均踢球距离和回传码数。<br />
<br />
失误数据：包括数据如失误次数和被拦截次数。<br />
<br />
相关新闻：收集关于球队的最新消息，包括球员伤病、转会、教练更换或场外事件，这些可能影响球队表现。<br />
<br />
2、数据分析：利用GPT 4，对收集到的数据进行分析。程序会将每场比赛的相关统计数据和新闻信息输入给GPT 4，并让它预测比赛的获胜者。<br />
<br />
将收集到的数据输入LLM。模型将分析这些统计数据和新闻，以了解两支球队的当前状态、优势、劣势和近期变化。<br />
<br />
可以对LLM进行训练或微调，使其更好地理解类似比赛的历史数据模式和结果。<br />
<br />
3、预测获胜者：基于分析的结果，代理知道如何处理特定的任务，例如分析新闻文章（确定文章主要讨论的团队、提取文章摘要等）和预测比赛获胜者。<br />
<br />
模型还可以提供预测的理由，基于分析的数据，使预测更加深入。<br />
<br />
4、优化和反馈循环：比赛结束后，可以将实际结果反馈到系统中，以帮助优化预测算法，随着时间的推移提高准确性。<br />
<br />
尽管LLM能够处理和分析大量数据，但其预测的质量和准确性在很大程度上取决于所提供数据的质量以及模型对这一特定任务的训练程度。这种方法结合了统计分析和基于AI的预测建模，为体育分析提供了一种新颖的方式。<br />
<br />
GitHub：<a href="https://github.com/stevekrenzel/pick-ems">github.com/stevekrenzel/pick…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VWWHZQZGJVQUFQUkJyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748896852144161055#m</id>
            <title>有人说是AI

这不是视频来啦

😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1748896852144161055#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748896852144161055#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 02:34:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有人说是AI<br />
<br />
这不是视频来啦<br />
<br />
😎</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1748708069654098010#m">nitter.cz/xiaohuggg/status/1748708069654098010#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ4ODk2Nzg0Mzg5NDE0OTEyL2ltZy9ERHNreVQ2SGlFcFhuajRDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748708099052150825#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748708099052150825#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748708099052150825#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 14:04:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3ZkRGFNQUFPUzdtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748708084326043859#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748708084326043859#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748708084326043859#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 14:04:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3VqcWFRQUVhb2phLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3VqcWFRQUk5cTBXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748708069654098010#m</id>
            <title>这个节目可以上春晚

哈哈哈哈</title>
            <link>https://nitter.cz/xiaohuggg/status/1748708069654098010#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748708069654098010#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 14:04:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个节目可以上春晚<br />
<br />
哈哈哈哈</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTb3R1emFzQUFfVlFoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748683726417256771#m</id>
            <title>SAMPLE：自动化蛋白质设计系统

它可以自己设计和测试新的蛋白质，而不需要人类的帮助。就像一个能自己做实验的机器人科学家。

它能自主学习蛋白质的结构和功能之间的关系，然后自己进行蛋白质设计，同时在实验室里自动进行测试。

SAMPLE由一个AI代理驱动，可广泛应用于生物工程和合成生物学。

研究团队用这个系统在一个特定的蛋白质领域（糖苷水解酶）进行了实验。他们让四个这样的机器人系统自主工作，这些系统通过自己的学习和实验，成功地创造出了一些比原始蛋白质更稳定的新蛋白质。

背景知识：

蛋白质工程是一个复杂的工程，用于创造具有有用功能和行为的新蛋白质，但它过程缓慢、费力且需要专业知识，限制了其广泛应用。

威斯康星大学麦迪逊分校的研究人员开发出了一个结合人工智能和实验自动化的系统，可以在没有人工干预的情况下自主地进行蛋白质工程。

SAMPLE主要能力：

1、自主设计蛋白质：SAMPLE能够自己设计新的蛋白质结构，这是基于它对蛋白质序列与其功能关系的理解。

2、自动化实验：SAMPLE通过全自动化的实验室设备来测试它设计的蛋白质。这包括合成基因、表达蛋白质，以及进行生化活性的测量。

3、数据驱动的优化：SAMPLE通过分析实验结果来不断学习和优化，以改进其对蛋白质设计的理解。

工作原理：

1、智能代理：SAMPLE包括一个智能代理，这个代理利用已有的数据来学习蛋白质序列和功能之间的关系。这相当于一个内部模型，用于预测哪些蛋白质设计可能是有效的。

2、实验反馈循环：智能代理设计蛋白质后，会将这些设计发送到实验室环境中进行测试。然后，它会接收实验数据并用这些信息来更新和改进其内部模型。

3、探索与优化：SAMPLE在实验过程中平衡探索（尝试新的和不确定的设计）和优化（根据现有知识改进设计）。

该研究结果已经发表在了Nature上：https://www.nature.com/articles/s44286-023-00002-4

论文：https://www.nature.com/articles/s44286-023-00002-4.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1748683726417256771#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748683726417256771#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 12:27:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SAMPLE：自动化蛋白质设计系统<br />
<br />
它可以自己设计和测试新的蛋白质，而不需要人类的帮助。就像一个能自己做实验的机器人科学家。<br />
<br />
它能自主学习蛋白质的结构和功能之间的关系，然后自己进行蛋白质设计，同时在实验室里自动进行测试。<br />
<br />
SAMPLE由一个AI代理驱动，可广泛应用于生物工程和合成生物学。<br />
<br />
研究团队用这个系统在一个特定的蛋白质领域（糖苷水解酶）进行了实验。他们让四个这样的机器人系统自主工作，这些系统通过自己的学习和实验，成功地创造出了一些比原始蛋白质更稳定的新蛋白质。<br />
<br />
背景知识：<br />
<br />
蛋白质工程是一个复杂的工程，用于创造具有有用功能和行为的新蛋白质，但它过程缓慢、费力且需要专业知识，限制了其广泛应用。<br />
<br />
威斯康星大学麦迪逊分校的研究人员开发出了一个结合人工智能和实验自动化的系统，可以在没有人工干预的情况下自主地进行蛋白质工程。<br />
<br />
SAMPLE主要能力：<br />
<br />
1、自主设计蛋白质：SAMPLE能够自己设计新的蛋白质结构，这是基于它对蛋白质序列与其功能关系的理解。<br />
<br />
2、自动化实验：SAMPLE通过全自动化的实验室设备来测试它设计的蛋白质。这包括合成基因、表达蛋白质，以及进行生化活性的测量。<br />
<br />
3、数据驱动的优化：SAMPLE通过分析实验结果来不断学习和优化，以改进其对蛋白质设计的理解。<br />
<br />
工作原理：<br />
<br />
1、智能代理：SAMPLE包括一个智能代理，这个代理利用已有的数据来学习蛋白质序列和功能之间的关系。这相当于一个内部模型，用于预测哪些蛋白质设计可能是有效的。<br />
<br />
2、实验反馈循环：智能代理设计蛋白质后，会将这些设计发送到实验室环境中进行测试。然后，它会接收实验数据并用这些信息来更新和改进其内部模型。<br />
<br />
3、探索与优化：SAMPLE在实验过程中平衡探索（尝试新的和不确定的设计）和优化（根据现有知识改进设计）。<br />
<br />
该研究结果已经发表在了Nature上：<a href="https://www.nature.com/articles/s44286-023-00002-4">nature.com/articles/s44286-0…</a><br />
<br />
论文：<a href="https://www.nature.com/articles/s44286-023-00002-4.pdf">nature.com/articles/s44286-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTTm5nM2E4QUFmUGcyLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTTnZRd2FzQUEyZ3Z0LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VTTjBFd2JjQUFfeG9TLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748572050271420663#m</id>
            <title>WhisperSpeech：一个开源的文本到语音系统

牛P的是它是通过对OpenAI的Whisper语音识别模型反向工程来实现的。

通过这种反转过程，WhisperSpeech能够接收文本输入，并利用修改后的Whisper模型生成听起来自然的语音输出。

输出的语音在发音准确性和自然度方面都非常的优秀。

WhisperSpeech 项目路线图：

-声学标记提取：改进声学标记的提取过程。

-语义标记提取：使用Whisper模型生成和量化语义标记。

-S->A模型转换：开发将语义标记转换为声学标记的模型。

-T->S模型转换：实现从文本标记到语义标记的转换。

-提升EnCodec语音质量：优化EnCodec模型以提高语音合成质量。

-短句推理优化：改善系统处理短句的能力。

-扩展情感语音数据集：收集更大的情感语音数据。

-文档化LibriLight数据集：详细记录HuggingFace上的数据集。

-多语言语音收集：聚集社区资源，收集多种语言的语音。

-训练多语言模型：开发支持多语言的文本到语音模型。

GitHub：https://github.com/collabora/WhisperSpeech
网站：https://collabora.github.io/WhisperSpeech/
在线体验：https://replicate.com/lucataco/whisperspeech-small</title>
            <link>https://nitter.cz/xiaohuggg/status/1748572050271420663#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748572050271420663#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 05:03:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>WhisperSpeech：一个开源的文本到语音系统<br />
<br />
牛P的是它是通过对OpenAI的Whisper语音识别模型反向工程来实现的。<br />
<br />
通过这种反转过程，WhisperSpeech能够接收文本输入，并利用修改后的Whisper模型生成听起来自然的语音输出。<br />
<br />
输出的语音在发音准确性和自然度方面都非常的优秀。<br />
<br />
WhisperSpeech 项目路线图：<br />
<br />
-声学标记提取：改进声学标记的提取过程。<br />
<br />
-语义标记提取：使用Whisper模型生成和量化语义标记。<br />
<br />
-S->A模型转换：开发将语义标记转换为声学标记的模型。<br />
<br />
-T->S模型转换：实现从文本标记到语义标记的转换。<br />
<br />
-提升EnCodec语音质量：优化EnCodec模型以提高语音合成质量。<br />
<br />
-短句推理优化：改善系统处理短句的能力。<br />
<br />
-扩展情感语音数据集：收集更大的情感语音数据。<br />
<br />
-文档化LibriLight数据集：详细记录HuggingFace上的数据集。<br />
<br />
-多语言语音收集：聚集社区资源，收集多种语言的语音。<br />
<br />
-训练多语言模型：开发支持多语言的文本到语音模型。<br />
<br />
GitHub：<a href="https://github.com/collabora/WhisperSpeech">github.com/collabora/Whisper…</a><br />
网站：<a href="https://collabora.github.io/WhisperSpeech/">collabora.github.io/WhisperS…</a><br />
在线体验：<a href="https://replicate.com/lucataco/whisperspeech-small">replicate.com/lucataco/whisp…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg1NDQ2MzE2OTkyODM5NjgvcHUvaW1nL3ZTSWtUOGNUdjdKaFlZcHIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748554598368121184#m</id>
            <title>DiffusionGPT：由LLM驱动的文本到图像生成系统

由字节跳动开发，DiffusionGPT的牛P之处在于它集成了多种领域的专家图像生成模型。

然后使用LLM来对接这些图像生成模型，让LLM来处理和理解各种文本提示。

最后根据理解的信息选择最合适的图像模型来生成图像。

这样就和GPT 4一样，通过聊天画图...

DiffusionGPT主要特点：

1、多样化文本提示处理：DiffusionGPT 能够理解和处理各种类型的文本提示，包括具体的指令、抽象的灵感、复杂的假设等。

2、集成多个领域专家模型：系统集成了多种领域的图像扩散模型，每个模型在其特定领域具有专业的图像生成能力。这类模型专注于特定领域的图像生成，比如自然景观、人物肖像、艺术作品等。

这意味着系统不仅能够生成普通的图像，还能够处理更特定、更复杂的图像生成任务，比如特定风格或类型的图像。模仿特定艺术家的风格、漫画风格或摄影技术。

3、大语言模型驱动：DiffusionGPT 使用大语言模型（LLM）来解析和理解用户输入的文本提示。这个过程类似于其他基于 LLM 的系统（如 GPT-4）处理文本的方式，但特别应用于理解用于图像生成的指令和描述。

4、智能选择合适的图像模型：基于对文本提示的理解，DiffusionGPT 能够智能地选择最合适的图像生成模型来生成图像。这不仅包括选择正确的模型，还涉及调整生成参数以最好地满足用户的需求。

5、输出高质量图像：通过精准地匹配文本提示与最佳生成模型，DiffusionGPT 能生成高质量、与用户需求高度吻合的图像。

6、用户反馈与优势数据库：结合用户反馈和优势数据库，系统能够根据用户偏好调整模型选择，提升图像生成的相关性和质量。

例如：在系统的早期使用中，用户可能提供对生成图像的反馈，比如“这张图片的颜色太暗了”。DiffusionGPT 利用这些反馈来调整其模型选择，使得未来的图像生成更符合用户的偏好。

主要工作原理：

1、输入解析：用户提供文本提示，如描述、指令或灵感。
大型语言模型（LLM）负责解析这些文本提示，理解其含义和需求。

2、思维树（Tree-of-Thought）构建：根据不同的图像生成任务，系统构建了一个“思维树”，这是一种组织不同图像生成模型的结构。
思维树基于先验知识和人类反馈，涵盖了多种领域的专家级模型。

3、模型选择：根据 LLM 解析的结果，系统通过思维树来确定最适合当前文本提示的图像生成模型。在选择过程中，可能还会考虑用户的偏好和历史反馈，这些信息存储在优势数据库中。

4、图像生成：一旦选定了合适的模型，该模型就会被用来生成图像。生成的图像将与输入的文本提示紧密相关，并反映出用户的意图和偏好。

5、结果输出：最终生成的图像会呈现给用户。
这些图像可以是多样化的，包括但不限于具体描述的场景、概念艺术作品或符合特定风格的图像。

6、用户反馈优化过程：

用户对生成图像的反馈被用来丰富优势数据库，进而帮助系统更好地理解用户偏好，优化后续的模型选择和图像生成。

实验结果：

DiffusionGPT 在生成人类和场景等类别的图像时展现了高度的真实性和细节。

与基准模型（如 SD1.5）相比，DiffusionGPT 生成的图像在视觉保真度、捕捉细节方面有明显提升。

DiffusionGPT 在图像奖励和美学评分方面的表现优于传统的稳定扩散模型。

在进行图像生成质量的量化评估时，DiffusionGPT 展示了较高的评分，说明其生成的图像在质量和美学上更受青睐。

项目地址：http://diffusiongpt.github.io/
论文：http://arxiv.org/abs/2401.10061
GitHub：http://github.com/DiffusionGPT/DiffusionGPT

在线演示：

DiffusionGPT：https://huggingface.co/spaces/DiffusionGPT/DiffusionGPT

DiffusionGPT-XL：https://huggingface.co/spaces/DiffusionGPT/DiffusionGPT-XL</title>
            <link>https://nitter.cz/xiaohuggg/status/1748554598368121184#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748554598368121184#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 03:54:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DiffusionGPT：由LLM驱动的文本到图像生成系统<br />
<br />
由字节跳动开发，DiffusionGPT的牛P之处在于它集成了多种领域的专家图像生成模型。<br />
<br />
然后使用LLM来对接这些图像生成模型，让LLM来处理和理解各种文本提示。<br />
<br />
最后根据理解的信息选择最合适的图像模型来生成图像。<br />
<br />
这样就和GPT 4一样，通过聊天画图...<br />
<br />
DiffusionGPT主要特点：<br />
<br />
1、多样化文本提示处理：DiffusionGPT 能够理解和处理各种类型的文本提示，包括具体的指令、抽象的灵感、复杂的假设等。<br />
<br />
2、集成多个领域专家模型：系统集成了多种领域的图像扩散模型，每个模型在其特定领域具有专业的图像生成能力。这类模型专注于特定领域的图像生成，比如自然景观、人物肖像、艺术作品等。<br />
<br />
这意味着系统不仅能够生成普通的图像，还能够处理更特定、更复杂的图像生成任务，比如特定风格或类型的图像。模仿特定艺术家的风格、漫画风格或摄影技术。<br />
<br />
3、大语言模型驱动：DiffusionGPT 使用大语言模型（LLM）来解析和理解用户输入的文本提示。这个过程类似于其他基于 LLM 的系统（如 GPT-4）处理文本的方式，但特别应用于理解用于图像生成的指令和描述。<br />
<br />
4、智能选择合适的图像模型：基于对文本提示的理解，DiffusionGPT 能够智能地选择最合适的图像生成模型来生成图像。这不仅包括选择正确的模型，还涉及调整生成参数以最好地满足用户的需求。<br />
<br />
5、输出高质量图像：通过精准地匹配文本提示与最佳生成模型，DiffusionGPT 能生成高质量、与用户需求高度吻合的图像。<br />
<br />
6、用户反馈与优势数据库：结合用户反馈和优势数据库，系统能够根据用户偏好调整模型选择，提升图像生成的相关性和质量。<br />
<br />
例如：在系统的早期使用中，用户可能提供对生成图像的反馈，比如“这张图片的颜色太暗了”。DiffusionGPT 利用这些反馈来调整其模型选择，使得未来的图像生成更符合用户的偏好。<br />
<br />
主要工作原理：<br />
<br />
1、输入解析：用户提供文本提示，如描述、指令或灵感。<br />
大型语言模型（LLM）负责解析这些文本提示，理解其含义和需求。<br />
<br />
2、思维树（Tree-of-Thought）构建：根据不同的图像生成任务，系统构建了一个“思维树”，这是一种组织不同图像生成模型的结构。<br />
思维树基于先验知识和人类反馈，涵盖了多种领域的专家级模型。<br />
<br />
3、模型选择：根据 LLM 解析的结果，系统通过思维树来确定最适合当前文本提示的图像生成模型。在选择过程中，可能还会考虑用户的偏好和历史反馈，这些信息存储在优势数据库中。<br />
<br />
4、图像生成：一旦选定了合适的模型，该模型就会被用来生成图像。生成的图像将与输入的文本提示紧密相关，并反映出用户的意图和偏好。<br />
<br />
5、结果输出：最终生成的图像会呈现给用户。<br />
这些图像可以是多样化的，包括但不限于具体描述的场景、概念艺术作品或符合特定风格的图像。<br />
<br />
6、用户反馈优化过程：<br />
<br />
用户对生成图像的反馈被用来丰富优势数据库，进而帮助系统更好地理解用户偏好，优化后续的模型选择和图像生成。<br />
<br />
实验结果：<br />
<br />
DiffusionGPT 在生成人类和场景等类别的图像时展现了高度的真实性和细节。<br />
<br />
与基准模型（如 SD1.5）相比，DiffusionGPT 生成的图像在视觉保真度、捕捉细节方面有明显提升。<br />
<br />
DiffusionGPT 在图像奖励和美学评分方面的表现优于传统的稳定扩散模型。<br />
<br />
在进行图像生成质量的量化评估时，DiffusionGPT 展示了较高的评分，说明其生成的图像在质量和美学上更受青睐。<br />
<br />
项目地址：<a href="http://diffusiongpt.github.io/">diffusiongpt.github.io/</a><br />
论文：<a href="http://arxiv.org/abs/2401.10061">arxiv.org/abs/2401.10061</a><br />
GitHub：<a href="http://github.com/DiffusionGPT/DiffusionGPT">github.com/DiffusionGPT/Diff…</a><br />
<br />
在线演示：<br />
<br />
DiffusionGPT：<a href="https://huggingface.co/spaces/DiffusionGPT/DiffusionGPT">huggingface.co/spaces/Diffus…</a><br />
<br />
DiffusionGPT-XL：<a href="https://huggingface.co/spaces/DiffusionGPT/DiffusionGPT-XL">huggingface.co/spaces/Diffus…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VRZEl4c2J3QUFneHB2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VRZEl4c2JVQUFlWVVlLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VRZEl5R2JVQUE3RERtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VRVjdwUGFVQUF6Vjg2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748548848933642366#m</id>
            <title>大BOSS释放了

Stable Video Diffusion最新进展

似乎视频效果和清晰度都上了一个新高度🥱</title>
            <link>https://nitter.cz/xiaohuggg/status/1748548848933642366#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748548848933642366#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 03:31:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大BOSS释放了<br />
<br />
Stable Video Diffusion最新进展<br />
<br />
似乎视频效果和清晰度都上了一个新高度🥱</p>
<p><a href="https://nitter.cz/EMostaque/status/1748405750907457548#m">nitter.cz/EMostaque/status/1748405750907457548#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748547405606494267#m</id>
            <title>R to @xiaohuggg: Apple Vision Pro官方使用导览

https://www.apple.com/apple-vision-pro/guided-tour/</title>
            <link>https://nitter.cz/xiaohuggg/status/1748547405606494267#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748547405606494267#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 03:26:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Apple Vision Pro官方使用导览<br />
<br />
<a href="https://www.apple.com/apple-vision-pro/guided-tour/">apple.com/apple-vision-pro/g…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0ODMzMjA2OTcxNTkzMTEzNi9NRFlBcjFjWT9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748540618215924156#m</id>
            <title>👓 #AppleVisionPro 全方位介绍和使用指南

中英文字幕 全长10分钟

欢迎转发收藏观看🫰</title>
            <link>https://nitter.cz/xiaohuggg/status/1748540618215924156#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748540618215924156#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 20 Jan 2024 02:59:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>👓 <a href="https://nitter.cz/search?q=%23AppleVisionPro">#AppleVisionPro</a> 全方位介绍和使用指南<br />
<br />
中英文字幕 全长10分钟<br />
<br />
欢迎转发收藏观看🫰</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDg1Mzg5MDI5NzIwNzYwMzIvcHUvaW1nL1VYQkNmRTBWSlAwcFFwUHEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748361202408083774#m</id>
            <title>#AppleVisionPro 生产过程

👀</title>
            <link>https://nitter.cz/xiaohuggg/status/1748361202408083774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748361202408083774#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 15:06:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AppleVisionPro">#AppleVisionPro</a> 生产过程<br />
<br />
👀</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ4MzYwNzk1MjQ2MDA2MjczL2ltZy85cDBRMEs4M3pGc000OUVpLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748316762649813170#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748316762649813170#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748316762649813170#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 12:09:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VORUM0b2JNQUE4eXd1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VORUhqZWFNQUFFWXg3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748316760091353184#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748316760091353184#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748316760091353184#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 12:09:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VORDh0aWFJQUF6cXhmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VORC15ZWF3QUFEdEViLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1748316757901844511#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1748316757901844511#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1748316757901844511#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 12:09:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VORDJYR2IwQUFyaDRXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VORDVSS2JrQUFKWjBMLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>