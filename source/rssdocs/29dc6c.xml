<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727881898012336253#m</id>
            <title>Merse：使用AI将个人故事和经历转化为各种形式的内容，保存和分享这些珍贵记忆。

这个工具可以将你的日常时刻、身边故事和经历转化为包括漫画、书籍、电影、语音、自传等多种形式保存下来。

作者在这个项目花费了8个月时间，可惜有别的事情要做，无法继续...

现在把它开源了，感兴趣的可以继续...

作者：@markrachapoom
Demo：https://www.merse.co/
GitHub：https://github.com/markrachapoom/merse</title>
            <link>https://nitter.cz/xiaohuggg/status/1727881898012336253#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727881898012336253#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:48:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Merse：使用AI将个人故事和经历转化为各种形式的内容，保存和分享这些珍贵记忆。<br />
<br />
这个工具可以将你的日常时刻、身边故事和经历转化为包括漫画、书籍、电影、语音、自传等多种形式保存下来。<br />
<br />
作者在这个项目花费了8个月时间，可惜有别的事情要做，无法继续...<br />
<br />
现在把它开源了，感兴趣的可以继续...<br />
<br />
作者：<a href="https://nitter.cz/markrachapoom" title="Mark Rachapoom">@markrachapoom</a><br />
Demo：<a href="https://www.merse.co/">merse.co/</a><br />
GitHub：<a href="https://github.com/markrachapoom/merse">github.com/markrachapoom/mer…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc4NzcxMjA0NjUxNzQ1MjgvcHUvaW1nLzNUU1hsQVM1OTEyaUtYY0IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727870864736284878#m</id>
            <title>GPT-4V在美国医学执照考试（USMLE）上的表现

研究人员用GPT 4V对美国医学执照考试（USMLE）中的问题进行了测试：

- GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。

- GPT-4V在图像问题上的表现超过了大多数医学生。

- 当GPT-4V回答正确时，其解释几乎与领域专家相当

研究方法：

1、研究者使用了来自美国医学执照考试（USMLE）、医学生的USMLE题库（AMBOSS）和诊断放射学资格核心考试（DRQCE）的多项选择题（包含图像）来测试GPT-4V的准确性和解释质量。

2、GPT-4V与两个最先进的LLM（GPT-4和ChatGPT）进行了比较。

3、研究还评估了医疗专业人员对GPT-4V解释的偏好和反馈，并展示了一个案例场景，说明如何将GPT-4V用于临床决策支持。

研究结果：

1、整体表现：GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。超过了ChatGPT（58.4%）和GPT-4（83.6%）。这是一个相当高的比例，特别是考虑到这个考试的难度和复杂性。

2、图像问题的表现：对于包含图像的问题，GPT-4V的准确率分别为86.2%、73.1%和62.0%。相当于AMBOSS医学生的70至80百分位。

AMBOSS是一个广泛使用的医学学习平台，医学生通常使用它来准备考试。这里的“70至80百分位”意味着GPT-4V在处理这些问题时的表现好于70%到80%的使用AMBOSS平台的医学生。

换句话说，GPT-4V在这些特定问题上几乎可以和顶尖的医学生相媲美。

3、不同医学子领域的表现：在不同的医学子领域中，GPT-4V的表现有所不同。例如，在免疫学和耳鼻喉科领域，它的准确率达到了100%，而在解剖学和急诊医学领域，准确率则降至25%。

4、错误回答的解释质量：当GPT-4V回答错误时，18.2%的错误答案包含了虚构文本，45.5%存在推理错误，76.3%对图像的理解有误。这些数据显示，虽然GPT-4V在大多数情况下表现良好，但在错误回答时，其解释质量会显著下降。

5、医生提示的影响：当医生给予GPT-4V简短的提示后，它的错误率平均降低了40.5%。而对更难的测试题目，性能提升更明显。这表明，与专业人士的协作可以显著提高AI模型的表现。

详细介绍：https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3
论文：https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3.full.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1727870864736284878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727870864736284878#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:04:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPT-4V在美国医学执照考试（USMLE）上的表现<br />
<br />
研究人员用GPT 4V对美国医学执照考试（USMLE）中的问题进行了测试：<br />
<br />
- GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。<br />
<br />
- GPT-4V在图像问题上的表现超过了大多数医学生。<br />
<br />
- 当GPT-4V回答正确时，其解释几乎与领域专家相当<br />
<br />
研究方法：<br />
<br />
1、研究者使用了来自美国医学执照考试（USMLE）、医学生的USMLE题库（AMBOSS）和诊断放射学资格核心考试（DRQCE）的多项选择题（包含图像）来测试GPT-4V的准确性和解释质量。<br />
<br />
2、GPT-4V与两个最先进的LLM（GPT-4和ChatGPT）进行了比较。<br />
<br />
3、研究还评估了医疗专业人员对GPT-4V解释的偏好和反馈，并展示了一个案例场景，说明如何将GPT-4V用于临床决策支持。<br />
<br />
研究结果：<br />
<br />
1、整体表现：GPT-4V在美国医学执照考试（USMLE）中的整体准确率达到了90.7%。超过了ChatGPT（58.4%）和GPT-4（83.6%）。这是一个相当高的比例，特别是考虑到这个考试的难度和复杂性。<br />
<br />
2、图像问题的表现：对于包含图像的问题，GPT-4V的准确率分别为86.2%、73.1%和62.0%。相当于AMBOSS医学生的70至80百分位。<br />
<br />
AMBOSS是一个广泛使用的医学学习平台，医学生通常使用它来准备考试。这里的“70至80百分位”意味着GPT-4V在处理这些问题时的表现好于70%到80%的使用AMBOSS平台的医学生。<br />
<br />
换句话说，GPT-4V在这些特定问题上几乎可以和顶尖的医学生相媲美。<br />
<br />
3、不同医学子领域的表现：在不同的医学子领域中，GPT-4V的表现有所不同。例如，在免疫学和耳鼻喉科领域，它的准确率达到了100%，而在解剖学和急诊医学领域，准确率则降至25%。<br />
<br />
4、错误回答的解释质量：当GPT-4V回答错误时，18.2%的错误答案包含了虚构文本，45.5%存在推理错误，76.3%对图像的理解有误。这些数据显示，虽然GPT-4V在大多数情况下表现良好，但在错误回答时，其解释质量会显著下降。<br />
<br />
5、医生提示的影响：当医生给予GPT-4V简短的提示后，它的错误率平均降低了40.5%。而对更难的测试题目，性能提升更明显。这表明，与专业人士的协作可以显著提高AI模型的表现。<br />
<br />
详细介绍：<a href="https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3">medrxiv.org/content/10.1101/…</a><br />
论文：<a href="https://www.medrxiv.org/content/10.1101/2023.10.26.23297629v3.full.pdf">medrxiv.org/content/10.1101/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xZ24zV2FjQUFvbERoLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9xaFBTcmFzQUFkaHZkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727856396820193700#m</id>
            <title>DuckTrack：多模态计算机交互数据收集工具

这个工具可以高精度地追踪和回放你的鼠标、键盘、屏幕视频和音频数据。记录在电脑上操作的各种输入数据和动作。

DuckTrack主要目的是帮助研究人员收集用于训练和测试人工智能模型的数据。开发能够理解和模拟人类在计算机上的操作行为的AI系统...

功能概览:

- 精确记录和回放鼠标和键盘动作。
- 使用OBS进行屏幕录制。
- 可暂停/恢复录制，以保护私人信息
- 使用Python编写，可在所有主流操作系统上作为常规桌面应用程序运行。

DuckTrack的主要用途：

1、人工智能和机器学习研究：通过准确记录和回放用户的鼠标、键盘、屏幕视频和音频数据，DuckTrack可以帮助研究人员收集用于训练和测试人工智能模型的数据。这对于开发能够理解和模拟人类在计算机上的操作行为的AI系统尤为重要。

2、用户体验研究：DuckTrack可以用于收集用户在使用软件或网站时的交互数据。这对于分析用户行为、优化用户界面设计、提高用户体验等方面非常有用。

3、软件测试和调试：通过记录和回放用户的操作，可以帮助软件开发者重现和分析软件中的错误或问题。这对于软件质量保证和改进是非常重要的。

4、教育和培训：DuckTrack可以用于创建教学或培训材料，例如录制操作教程或演示特定的软件使用方法。

5、远程协作和支持：在远程工作或提供技术支持的情况下，DuckTrack可以帮助记录和分享特定的计算机操作过程，以便于沟通和问题解决。

DuckTrack作为一个多模态计算机交互数据收集工具，在AI研究、用户体验分析、软件开发、教育培训以及远程协作等多个领域都有广泛的应用价值。

详细：https://duckai.org/blog/ducktrack#feature-overview
GitHub：https://github.com/TheDuckAI/DuckTrack</title>
            <link>https://nitter.cz/xiaohuggg/status/1727856396820193700#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727856396820193700#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 01:07:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DuckTrack：多模态计算机交互数据收集工具<br />
<br />
这个工具可以高精度地追踪和回放你的鼠标、键盘、屏幕视频和音频数据。记录在电脑上操作的各种输入数据和动作。<br />
<br />
DuckTrack主要目的是帮助研究人员收集用于训练和测试人工智能模型的数据。开发能够理解和模拟人类在计算机上的操作行为的AI系统...<br />
<br />
功能概览:<br />
<br />
- 精确记录和回放鼠标和键盘动作。<br />
- 使用OBS进行屏幕录制。<br />
- 可暂停/恢复录制，以保护私人信息<br />
- 使用Python编写，可在所有主流操作系统上作为常规桌面应用程序运行。<br />
<br />
DuckTrack的主要用途：<br />
<br />
1、人工智能和机器学习研究：通过准确记录和回放用户的鼠标、键盘、屏幕视频和音频数据，DuckTrack可以帮助研究人员收集用于训练和测试人工智能模型的数据。这对于开发能够理解和模拟人类在计算机上的操作行为的AI系统尤为重要。<br />
<br />
2、用户体验研究：DuckTrack可以用于收集用户在使用软件或网站时的交互数据。这对于分析用户行为、优化用户界面设计、提高用户体验等方面非常有用。<br />
<br />
3、软件测试和调试：通过记录和回放用户的操作，可以帮助软件开发者重现和分析软件中的错误或问题。这对于软件质量保证和改进是非常重要的。<br />
<br />
4、教育和培训：DuckTrack可以用于创建教学或培训材料，例如录制操作教程或演示特定的软件使用方法。<br />
<br />
5、远程协作和支持：在远程工作或提供技术支持的情况下，DuckTrack可以帮助记录和分享特定的计算机操作过程，以便于沟通和问题解决。<br />
<br />
DuckTrack作为一个多模态计算机交互数据收集工具，在AI研究、用户体验分析、软件开发、教育培训以及远程协作等多个领域都有广泛的应用价值。<br />
<br />
详细：<a href="https://duckai.org/blog/ducktrack#feature-overview">duckai.org/blog/ducktrack#fe…</a><br />
GitHub：<a href="https://github.com/TheDuckAI/DuckTrack">github.com/TheDuckAI/DuckTra…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc4NTUxNzY3ODk0MDE2MDAvcHUvaW1nL1Z1Q2psd1ByY1Q3VUtXcUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727691894405398617#m</id>
            <title>R to @xiaohuggg: Q-learning类似老鼠走迷宫的游戏，据说Q*还结合了A Star的搜索算法！

找了个迷宫解说先学习一下Q-learning再说！😂

爱学习的孩子可以看看！

Q-learning是一种强化学习算法，用于学习在给定状态下执行哪个动作以最大化某种形式的奖励或回报。在Q学习中，“Q”代表质量（quality），指的是执行特定动作带来的预期效益。

工作原理：

1.状态和动作：Q学习算法在一个由状态和动作组成的环境中工作。状态是环境的描述，动作是在这些状态下可以执行的操作。
2.Q表：算法维护一个Q表，这是一个查找表，用于存储每个状态-动作对的Q值（即该动作的预期效益）。
3.学习过程：当智能体（如机器人、软件代理）在环境中执行动作时，它会根据动作的结果（通常是奖励或惩罚）来更新Q表。这个更新过程是基于一种称为贝尔曼方程的数学公式。

举例说明：

假设有一个简单的迷宫游戏，智能体的目标是找到从起点到终点的最短路径。在这个例子中：

•状态：迷宫中的每个位置。
•动作：从一个位置移动到另一个位置（例如，向上、向下、向左、向右移动）。
•奖励：到达终点时获得正奖励，撞墙时获得负奖励。

智能体开始时对迷宫一无所知，它随机移动并从结果中学习。每次移动后，它更新Q表，记录在特定位置执行特定动作的效益。随着时间的推移，智能体学会识别哪些动作会带来更好的结果（比如更快到达终点），并开始优先选择这些动作。

结论：

Q学习的关键优势在于它不需要环境的先验知识，智能体通过与环境的交互学习最佳策略。这使得Q学习非常适合于那些模型无法提前了解所有可能状态的复杂环境。</title>
            <link>https://nitter.cz/xiaohuggg/status/1727691894405398617#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727691894405398617#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 14:13:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Q-learning类似老鼠走迷宫的游戏，据说Q*还结合了A Star的搜索算法！<br />
<br />
找了个迷宫解说先学习一下Q-learning再说！😂<br />
<br />
爱学习的孩子可以看看！<br />
<br />
Q-learning是一种强化学习算法，用于学习在给定状态下执行哪个动作以最大化某种形式的奖励或回报。在Q学习中，“Q”代表质量（quality），指的是执行特定动作带来的预期效益。<br />
<br />
工作原理：<br />
<br />
1.状态和动作：Q学习算法在一个由状态和动作组成的环境中工作。状态是环境的描述，动作是在这些状态下可以执行的操作。<br />
2.Q表：算法维护一个Q表，这是一个查找表，用于存储每个状态-动作对的Q值（即该动作的预期效益）。<br />
3.学习过程：当智能体（如机器人、软件代理）在环境中执行动作时，它会根据动作的结果（通常是奖励或惩罚）来更新Q表。这个更新过程是基于一种称为贝尔曼方程的数学公式。<br />
<br />
举例说明：<br />
<br />
假设有一个简单的迷宫游戏，智能体的目标是找到从起点到终点的最短路径。在这个例子中：<br />
<br />
•状态：迷宫中的每个位置。<br />
•动作：从一个位置移动到另一个位置（例如，向上、向下、向左、向右移动）。<br />
•奖励：到达终点时获得正奖励，撞墙时获得负奖励。<br />
<br />
智能体开始时对迷宫一无所知，它随机移动并从结果中学习。每次移动后，它更新Q表，记录在特定位置执行特定动作的效益。随着时间的推移，智能体学会识别哪些动作会带来更好的结果（比如更快到达终点），并开始优先选择这些动作。<br />
<br />
结论：<br />
<br />
Q学习的关键优势在于它不需要环境的先验知识，智能体通过与环境的交互学习最佳策略。这使得Q学习非常适合于那些模型无法提前了解所有可能状态的复杂环境。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3NjkxODIyMDIwMTI0NjcyL2ltZy9ndlA3cFo3Q3A1bU1ZeWZTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727674878185427193#m</id>
            <title>为啥我现在不爱在国内那些平台发内容

啥原因也不说就给你违规了

我就发发科技AI资讯就违法了

他妈的</title>
            <link>https://nitter.cz/xiaohuggg/status/1727674878185427193#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727674878185427193#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 13:06:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>为啥我现在不爱在国内那些平台发内容<br />
<br />
啥原因也不说就给你违规了<br />
<br />
我就发发科技AI资讯就违法了<br />
<br />
他妈的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9udkpKeWJBQUFsaGJXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727671594485727616#m</id>
            <title>据说斗鱼大主播几乎全军覆没

都要进去了😐

看来是被CEO全带坑里了🙃</title>
            <link>https://nitter.cz/xiaohuggg/status/1727671594485727616#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727671594485727616#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 12:53:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据说斗鱼大主播几乎全军覆没<br />
<br />
都要进去了😐<br />
<br />
看来是被CEO全带坑里了🙃</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727622388358267384#m</id>
            <title>Flowty Realtime LCM Canvas：实时草图转图像

可以运行在MacBook上的LCM LoRA实时草图转图像开源程序。

在MacBook Pro上进行了测试，M2 Max配置，每次渲染大约需要1.2秒。

通过改变UI中的模型ID，可以使用不同的模型。可以调整参数以获得更好的结果，例如改变canvas的大小。

GitHub：https://github.com/flowtyone/flowty-realtime-lcm-canvas</title>
            <link>https://nitter.cz/xiaohuggg/status/1727622388358267384#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727622388358267384#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 09:37:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Flowty Realtime LCM Canvas：实时草图转图像<br />
<br />
可以运行在MacBook上的LCM LoRA实时草图转图像开源程序。<br />
<br />
在MacBook Pro上进行了测试，M2 Max配置，每次渲染大约需要1.2秒。<br />
<br />
通过改变UI中的模型ID，可以使用不同的模型。可以调整参数以获得更好的结果，例如改变canvas的大小。<br />
<br />
GitHub：<a href="https://github.com/flowtyone/flowty-realtime-lcm-canvas">github.com/flowtyone/flowty-…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl9tX1ZYeWJjQUFJTlZQLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfbV9WWHliY0FBSU5WUC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727609231866773878#m</id>
            <title>MCVD：通用的视频生成模型 可以预测“未来”和“重建”过去

它可以：

🎥 视频生成：创造全新的视频片段。

🔮 视频预测：根据已有视频片段，预测可能发生的画面

🔄 过去重建：根据已有视频片段，重建倒推过去画面

🔗 视频插值：在两个已知视频片段之间，创造出连接的两段的中间画面。

MCVD是一个多功能的视频生成模型，能够进行视频的前向/后向预测和插值，适用于多种视频处理任务。

例如：你有一段视频，但想要在视频中添加或改变一些内容，比如让视频中的人物做不同的动作，或者在视频的某个时间点添加新的场景。MCVD就是一种可以帮你做到这些的技术。

它的特别之处在于，使用这个技术，你可以用一个模型来完成多种不同的视频编辑任务。比如，你可以用它来填补视频中缺失的部分，预测接下来会发生什么，或者完全创造一个新的视频片段。

工作原理和特点：

工作原理：
1、基于分数的扩散损失函数：MCVD使用这种损失函数来生成新的视频帧。这种方法通过逐步去除噪声来生成图像，从而创造出逼真的视频帧。

2、高斯噪声注入和去噪：在训练过程中，模型会向当前的视频帧注入高斯噪声，然后根据过去或未来的帧信息去除这些噪声。

3、随机遮蔽训练：MCVD在训练时会随机遮蔽过去或未来的帧，这使得模型能够适应不同的视频生成任务。

4、2D卷积U-Net架构：与更复杂的3D、循环或变换器架构相比，MCVD采用了2D卷积U-Net架构，这使得模型在处理视频数据时更为高效。

5、时空自适应归一化（SPATIN）：MCVD通过这种技术处理过去和未来的帧，增强了模型对时间序列数据的处理能力。

主要特点：
1、通用视频生成模型：MCVD能够处理多种视频生成任务，包括前向/后向预测和插值。

2、多种应用场景：它能够进行无条件生成、未来预测、过去重建和插值等多种视频处理任务。

3、高质量和多样化的视频样本：MCVD能够产生高质量和多样化的视频样本，适用于各种应用。

4、低资源需求：与其他大型视频处理模型相比，MCVD只需要1-4个GPU即可进行有效训练。

5、良好的扩展性：模型在通道数量方面具有良好的扩展性，可以根据需要进行进一步的扩展。

项目及演示：https://mask-cond-video-diffusion.github.io/
论文：https://arxiv.org/abs/2205.09853
GitHub：https://github.com/voletiv/mcvd-pytorch</title>
            <link>https://nitter.cz/xiaohuggg/status/1727609231866773878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727609231866773878#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 08:45:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MCVD：通用的视频生成模型 可以预测“未来”和“重建”过去<br />
<br />
它可以：<br />
<br />
🎥 视频生成：创造全新的视频片段。<br />
<br />
🔮 视频预测：根据已有视频片段，预测可能发生的画面<br />
<br />
🔄 过去重建：根据已有视频片段，重建倒推过去画面<br />
<br />
🔗 视频插值：在两个已知视频片段之间，创造出连接的两段的中间画面。<br />
<br />
MCVD是一个多功能的视频生成模型，能够进行视频的前向/后向预测和插值，适用于多种视频处理任务。<br />
<br />
例如：你有一段视频，但想要在视频中添加或改变一些内容，比如让视频中的人物做不同的动作，或者在视频的某个时间点添加新的场景。MCVD就是一种可以帮你做到这些的技术。<br />
<br />
它的特别之处在于，使用这个技术，你可以用一个模型来完成多种不同的视频编辑任务。比如，你可以用它来填补视频中缺失的部分，预测接下来会发生什么，或者完全创造一个新的视频片段。<br />
<br />
工作原理和特点：<br />
<br />
工作原理：<br />
1、基于分数的扩散损失函数：MCVD使用这种损失函数来生成新的视频帧。这种方法通过逐步去除噪声来生成图像，从而创造出逼真的视频帧。<br />
<br />
2、高斯噪声注入和去噪：在训练过程中，模型会向当前的视频帧注入高斯噪声，然后根据过去或未来的帧信息去除这些噪声。<br />
<br />
3、随机遮蔽训练：MCVD在训练时会随机遮蔽过去或未来的帧，这使得模型能够适应不同的视频生成任务。<br />
<br />
4、2D卷积U-Net架构：与更复杂的3D、循环或变换器架构相比，MCVD采用了2D卷积U-Net架构，这使得模型在处理视频数据时更为高效。<br />
<br />
5、时空自适应归一化（SPATIN）：MCVD通过这种技术处理过去和未来的帧，增强了模型对时间序列数据的处理能力。<br />
<br />
主要特点：<br />
1、通用视频生成模型：MCVD能够处理多种视频生成任务，包括前向/后向预测和插值。<br />
<br />
2、多种应用场景：它能够进行无条件生成、未来预测、过去重建和插值等多种视频处理任务。<br />
<br />
3、高质量和多样化的视频样本：MCVD能够产生高质量和多样化的视频样本，适用于各种应用。<br />
<br />
4、低资源需求：与其他大型视频处理模型相比，MCVD只需要1-4个GPU即可进行有效训练。<br />
<br />
5、良好的扩展性：模型在通道数量方面具有良好的扩展性，可以根据需要进行进一步的扩展。<br />
<br />
项目及演示：<a href="https://mask-cond-video-diffusion.github.io/">mask-cond-video-diffusion.gi…</a><br />
论文：<a href="https://arxiv.org/abs/2205.09853">arxiv.org/abs/2205.09853</a><br />
GitHub：<a href="https://github.com/voletiv/mcvd-pytorch">github.com/voletiv/mcvd-pyto…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc2MDc3NjcyNDA5NTM4NTYvcHUvaW1nL1NDdkZWbTNDSzJRYlJiNkMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1727468251154620804#m</id>
            <title>RT by @xiaohuggg: #AI开源项目推荐：vscode-ui-sketcher

UI Sketcher是一个VSCode插件，借助GPT-4V的多模态能力，在插件中画出界面草图，就能生成一个基于ReactNative的UI界面。

需要OpenAI的API Key

https://github.com/pAIrprogio/vscode-ui-sketcher</title>
            <link>https://nitter.cz/dotey/status/1727468251154620804#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1727468251154620804#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Nov 2023 23:25:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23AI开源项目推荐">#AI开源项目推荐</a>：vscode-ui-sketcher<br />
<br />
UI Sketcher是一个VSCode插件，借助GPT-4V的多模态能力，在插件中画出界面草图，就能生成一个基于ReactNative的UI界面。<br />
<br />
需要OpenAI的API Key<br />
<br />
<a href="https://github.com/pAIrprogio/vscode-ui-sketcher">github.com/pAIrprogio/vscode…</a></p>
<p><a href="https://nitter.cz/taishiyadeee/status/1727175786963300531#m">nitter.cz/taishiyadeee/status/1727175786963300531#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNjg5NjAxNTE4NDE1MDUyOC9sYUJZWlJCcj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727569904424132709#m</id>
            <title>R to @xiaohuggg: Ilya Sutskever 说到的他们已经克服了训练数据的问题

https://youtu.be/Ft0gTO2K85A</title>
            <link>https://nitter.cz/xiaohuggg/status/1727569904424132709#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727569904424132709#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 06:08:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ilya Sutskever 说到的他们已经克服了训练数据的问题<br />
<br />
<a href="https://youtu.be/Ft0gTO2K85A">youtu.be/Ft0gTO2K85A</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTcyNDI0MTE5ODI1NTQxMTIwMC9GS0JtUUI2Zj9mb3JtYXQ9anBnJm5hbWU9MTIwMHg2Mjc=" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727569403292881261#m</id>
            <title>R to @xiaohuggg: 早间路透社的报道：</title>
            <link>https://nitter.cz/xiaohuggg/status/1727569403292881261#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727569403292881261#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 06:06:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>早间路透社的报道：</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1727507238896730169#m">nitter.cz/xiaohuggg/status/1727507238896730169#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727568964065382411#m</id>
            <title>更多Q*（Q-Star）信息爆料和猜测：

- Q*可能具备自主学习和自我改进的能力。

- Q*模型可进行自主决策，可能已具备轻微自我意识。

- GPT-Zero项目解决了数据问题，自己”生产“数据。

- OpenAI可能正在利用计算机合成数据进行训练。

据Reddit用户爆料和猜测：Q*可能是一种非常先进的具有“可怕数学能力”的模型，已经具备自主学习和自我改进的能力。

该模型能够通过评估其行为的长期后果，在广泛的场景中做出复杂的决策。

Q*可能与强化学习中的Q-learning算法有关，这是一种评估在特定情境下采取特定行动的好坏的方法。还提到了Q-Value和Bellman方程，这些都是强化学习中的重要概念，用于指导AI在不同情境下做出最优决策。

简而言之，Q*似乎是一个高级的人工智能模型，能够在多种情境中学习和做出最优决策，具有自主学习和自我改进的能力。

Reddit原帖：https://www.reddit.com/r/OpenAI/comments/181n8am/what_is_q/

而根据@theinformation 今天的报道，OpenAI的首席科学家 Ilya Sutskever 领衔的一个名叫 GPT-Zero 的项目实现了巨大突破。他们克服了训练数据限制的困难，可以自己合成训练数据。

 据悉，Ilya Sutskever的 GPT-Zero 的项目，帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。

此前Ilya在一次采访中说到：

“Without going into details I'll just say the Data Limit can be overcome..."  

“无需详细说明，我只是说数据限制是可以克服的......”  

GPT-Zero项目研究的主要使用计算机生成的数据来训练模型，而不是从互联网上提取的真实世界数据，因为OpenAI已经获得了互联网上能获得的几乎所有的真实文本数据，已经无法再获得足够的数据来进行下一阶段训练。

OpenAI 研究团队利用GPT-Zero这一创新成果，构建了能解决基础数学问题的模型，这一直是现有 AI 模型的难题，无法进行复杂的推理能力。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Ilya Sutskever 的研究成果，开发出了这个名为 Q*（Q-Star）的模型。

Theinformation报道：https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern

而本月奥特曼在接受金融时报采访的时候曾表达：开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。

他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识的能力。

种种迹象表明Q*（Q-Star）的模型确实是超越了GPT 4非常多的模型，可以说是直接跨越了几代，具有自主学习和自我改进的能力，甚至就像之前Ilya表达的可能已经表现出了轻微的自我意识能力。

以上内容为综合报道、爆料的可能性猜测总结，不一定准确，请注意分辨！</title>
            <link>https://nitter.cz/xiaohuggg/status/1727568964065382411#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727568964065382411#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 06:05:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>更多Q*（Q-Star）信息爆料和猜测：<br />
<br />
- Q*可能具备自主学习和自我改进的能力。<br />
<br />
- Q*模型可进行自主决策，可能已具备轻微自我意识。<br />
<br />
- GPT-Zero项目解决了数据问题，自己”生产“数据。<br />
<br />
- OpenAI可能正在利用计算机合成数据进行训练。<br />
<br />
据Reddit用户爆料和猜测：Q*可能是一种非常先进的具有“可怕数学能力”的模型，已经具备自主学习和自我改进的能力。<br />
<br />
该模型能够通过评估其行为的长期后果，在广泛的场景中做出复杂的决策。<br />
<br />
Q*可能与强化学习中的Q-learning算法有关，这是一种评估在特定情境下采取特定行动的好坏的方法。还提到了Q-Value和Bellman方程，这些都是强化学习中的重要概念，用于指导AI在不同情境下做出最优决策。<br />
<br />
简而言之，Q*似乎是一个高级的人工智能模型，能够在多种情境中学习和做出最优决策，具有自主学习和自我改进的能力。<br />
<br />
Reddit原帖：<a href="https://teddit.net/r/OpenAI/comments/181n8am/what_is_q/">teddit.net/r/OpenAI/comments…</a><br />
<br />
而根据<a href="https://nitter.cz/theinformation" title="The Information">@theinformation</a> 今天的报道，OpenAI的首席科学家 Ilya Sutskever 领衔的一个名叫 GPT-Zero 的项目实现了巨大突破。他们克服了训练数据限制的困难，可以自己合成训练数据。<br />
<br />
 据悉，Ilya Sutskever的 GPT-Zero 的项目，帮助 OpenAI 克服了在获取足够高质量数据来训练新模型方面的限制。<br />
<br />
此前Ilya在一次采访中说到：<br />
<br />
“Without going into details I'll just say the Data Limit can be overcome..."  <br />
<br />
“无需详细说明，我只是说数据限制是可以克服的......”  <br />
<br />
GPT-Zero项目研究的主要使用计算机生成的数据来训练模型，而不是从互联网上提取的真实世界数据，因为OpenAI已经获得了互联网上能获得的几乎所有的真实文本数据，已经无法再获得足够的数据来进行下一阶段训练。<br />
<br />
OpenAI 研究团队利用GPT-Zero这一创新成果，构建了能解决基础数学问题的模型，这一直是现有 AI 模型的难题，无法进行复杂的推理能力。两位顶尖研究人员 Jakub Pachocki 和 Szymon Sidor 运用 Ilya Sutskever 的研究成果，开发出了这个名为 Q*（Q-Star）的模型。<br />
<br />
Theinformation报道：<a href="https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern">theinformation.com/articles/…</a><br />
<br />
而本月奥特曼在接受金融时报采访的时候曾表达：开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。<br />
<br />
他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识的能力。<br />
<br />
种种迹象表明Q*（Q-Star）的模型确实是超越了GPT 4非常多的模型，可以说是直接跨越了几代，具有自主学习和自我改进的能力，甚至就像之前Ilya表达的可能已经表现出了轻微的自我意识能力。<br />
<br />
以上内容为综合报道、爆料的可能性猜测总结，不一定准确，请注意分辨！</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tS2RGYWE4QUFQRVdTLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9tT1BPdmJnQUE5Q3UxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727555325556039898#m</id>
            <title>将任何网站变成播客/有声读物

利用OpenAI的TTS语音API

随便输入一个网址即可语音朗读这个网页的主要内容

它会自动识别网页内容的语言，自动选择该语言的语音

这只是一个Demo测试，需要输入自己的OpenAI API...

作者：@phuctm97
体验： http://readany.vercel.app</title>
            <link>https://nitter.cz/xiaohuggg/status/1727555325556039898#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727555325556039898#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 05:11:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将任何网站变成播客/有声读物<br />
<br />
利用OpenAI的TTS语音API<br />
<br />
随便输入一个网址即可语音朗读这个网页的主要内容<br />
<br />
它会自动识别网页内容的语言，自动选择该语言的语音<br />
<br />
这只是一个Demo测试，需要输入自己的OpenAI API...<br />
<br />
作者：<a href="https://nitter.cz/phuctm97" title="Minh-Phuc Tran">@phuctm97</a><br />
体验： <a href="http://readany.vercel.app">readany.vercel.app</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc1NTUwMjc4MTQ5NzM0NDAvcHUvaW1nL0Vidm1lMEVwdnZjRGNIN3IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727534814784581931#m</id>
            <title>Runway推出一个7x24小时直播的 AI 视频频道

里面的内容全是Gen AI生成的视频

点进去看了下，确实牛P，震撼到我了，里面的视频真的有的很不错，堪称大片了！

一进去是一个直播频道，不停的滚动直播AI视频，然后还可以选择其他视频节目观看。

📺传送门：http://Runway.TV</title>
            <link>https://nitter.cz/xiaohuggg/status/1727534814784581931#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727534814784581931#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 03:49:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway推出一个7x24小时直播的 AI 视频频道<br />
<br />
里面的内容全是Gen AI生成的视频<br />
<br />
点进去看了下，确实牛P，震撼到我了，里面的视频真的有的很不错，堪称大片了！<br />
<br />
一进去是一个直播频道，不停的滚动直播AI视频，然后还可以选择其他视频节目观看。<br />
<br />
📺传送门：<a href="http://Runway.TV">Runway.TV</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc1MzQ1MjY0NjgwOTYwMDAvcHUvaW1nL0lSOUIxZHpxWDUzM3JOWEkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727528903433138304#m</id>
            <title>Inflection AI公司宣布 Inflection-2 模型训练完成

宣称在同等计算能力类别中全球第一，同时也是当今世界上能力第二强的大语言模型。😃

在大多个数测试中超过了Google的PaLM 2模型，在多模态学习理解测试中，Inflection-2是除GPT-4外最强！

Inflection-2将很快部署到其旗下聊天机器人：Pi

训练及测试结果：

Inflection-2是在大约5,000个NVIDIA H100 GPU上训练的，使用了fp8混合精度，总计约10²⁵ FLOPs的计算量。这使得它在训练计算类别上与Google的PaLM 2 Large模型相同。

此外，Inflection-2的设计考虑到了服务效率，即将为Pi项目提供支持。通过从A100转移到H100 GPU以及高度优化的推理实现，Inflection-2在成本和服务速度方面相比Inflection-1有了显著提升，尽管其规模是Inflection-1的数倍。

Inflection-2的测试结果显示，它在多个标准的人工智能性能基准测试中表现优异。这些测试包括MMLU、TriviaQA、HellaSwag和GSM8k等知名的AI性能评估标准。

在这些测试中，Inflection-2不仅超越了其前身Inflection-1，而且在大多数测试中甚至超过了Google的PaLM 2 Large模型。

Inflection-2在多个标准AI性能基准测试中表现出色，但在与GPT-4的比较中，GPT-4仍然是性能最强的模型之一。尽管Inflection-2在某些方面超越了其他先进的模型，如Google的PaLM 2 Large模型，但它在与GPT-4的直接比较中，GPT-4在多数测试中仍然占据优势。

特别是在MMLU（多模态学习理解）测试中，Inflection-2是除GPT-4之外表现最好的模型，甚至超过了使用链式思考推理的Claude 2。这表明Inflection-2在处理高难度的多模态任务方面具有强大的能力，但GPT-4在整体性能上仍然保持领先。

详细：https://inflection.ai/inflection-2</title>
            <link>https://nitter.cz/xiaohuggg/status/1727528903433138304#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727528903433138304#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 03:26:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Inflection AI公司宣布 Inflection-2 模型训练完成<br />
<br />
宣称在同等计算能力类别中全球第一，同时也是当今世界上能力第二强的大语言模型。😃<br />
<br />
在大多个数测试中超过了Google的PaLM 2模型，在多模态学习理解测试中，Inflection-2是除GPT-4外最强！<br />
<br />
Inflection-2将很快部署到其旗下聊天机器人：Pi<br />
<br />
训练及测试结果：<br />
<br />
Inflection-2是在大约5,000个NVIDIA H100 GPU上训练的，使用了fp8混合精度，总计约10²⁵ FLOPs的计算量。这使得它在训练计算类别上与Google的PaLM 2 Large模型相同。<br />
<br />
此外，Inflection-2的设计考虑到了服务效率，即将为Pi项目提供支持。通过从A100转移到H100 GPU以及高度优化的推理实现，Inflection-2在成本和服务速度方面相比Inflection-1有了显著提升，尽管其规模是Inflection-1的数倍。<br />
<br />
Inflection-2的测试结果显示，它在多个标准的人工智能性能基准测试中表现优异。这些测试包括MMLU、TriviaQA、HellaSwag和GSM8k等知名的AI性能评估标准。<br />
<br />
在这些测试中，Inflection-2不仅超越了其前身Inflection-1，而且在大多数测试中甚至超过了Google的PaLM 2 Large模型。<br />
<br />
Inflection-2在多个标准AI性能基准测试中表现出色，但在与GPT-4的比较中，GPT-4仍然是性能最强的模型之一。尽管Inflection-2在某些方面超越了其他先进的模型，如Google的PaLM 2 Large模型，但它在与GPT-4的直接比较中，GPT-4在多数测试中仍然占据优势。<br />
<br />
特别是在MMLU（多模态学习理解）测试中，Inflection-2是除GPT-4之外表现最好的模型，甚至超过了使用链式思考推理的Claude 2。这表明Inflection-2在处理高难度的多模态任务方面具有强大的能力，但GPT-4在整体性能上仍然保持领先。<br />
<br />
详细：<a href="https://inflection.ai/inflection-2">inflection.ai/inflection-2</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9scU5ieWJBQUFOcXR3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727519973718040838#m</id>
            <title>ElevenLabs 推出AI语音转换器：Voice-Changer

可以将一种声音变成另一种声音，或者将你的声音转换成不同的角色，并控制其情感和传递方式。

主要特点和功能包括：

🔊 多样化的声音选择：您可以从数千种优质声音库中选择多种声音，或者在几分钟内创建自己的声音。

🌈 情感范围：保持内容的情感完整性，提供多种声音配置。

🎙️ 细微之处的保留：确保每个抑扬顿挫、停顿和调制都被高保真地捕捉和再现。

🌟 一致的质量：无论内容类型如何，都能实现完美的传递效果，确保您的声音始终如一。

🎚️ 精确控制：使用高级设置自定义您的声音，实现清晰度和真实性的完美平衡。

⚖️ 稳定性选择：根据内容的基调，在表现力变化和一致稳定性之间选择。

🔍 清晰度和相似度增强：优化清晰、无瑕疵的声音或增强说话者相似度。

🎨 风格夸张：强调声音风格或优先考虑速度和稳定性。

🖱️ 简单易用：上传或录制音频，选择您想要模仿的声音，自定义设置，然后生成并下载MP3文件。

尝试一下：https://elevenlabs.io/voice-changer</title>
            <link>https://nitter.cz/xiaohuggg/status/1727519973718040838#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727519973718040838#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:50:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ElevenLabs 推出AI语音转换器：Voice-Changer<br />
<br />
可以将一种声音变成另一种声音，或者将你的声音转换成不同的角色，并控制其情感和传递方式。<br />
<br />
主要特点和功能包括：<br />
<br />
🔊 多样化的声音选择：您可以从数千种优质声音库中选择多种声音，或者在几分钟内创建自己的声音。<br />
<br />
🌈 情感范围：保持内容的情感完整性，提供多种声音配置。<br />
<br />
🎙️ 细微之处的保留：确保每个抑扬顿挫、停顿和调制都被高保真地捕捉和再现。<br />
<br />
🌟 一致的质量：无论内容类型如何，都能实现完美的传递效果，确保您的声音始终如一。<br />
<br />
🎚️ 精确控制：使用高级设置自定义您的声音，实现清晰度和真实性的完美平衡。<br />
<br />
⚖️ 稳定性选择：根据内容的基调，在表现力变化和一致稳定性之间选择。<br />
<br />
🔍 清晰度和相似度增强：优化清晰、无瑕疵的声音或增强说话者相似度。<br />
<br />
🎨 风格夸张：强调声音风格或优先考虑速度和稳定性。<br />
<br />
🖱️ 简单易用：上传或录制音频，选择您想要模仿的声音，自定义设置，然后生成并下载MP3文件。<br />
<br />
尝试一下：<a href="https://elevenlabs.io/voice-changer">elevenlabs.io/voice-changer</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc1MTE5NjU3NjQxMjg3NjgvcHUvaW1nL3JIZ0xrX211VnpFMUxFVEcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727514832185004251#m</id>
            <title>R to @xiaohuggg: 亚太经合组织峰会上说的内容见这里：</title>
            <link>https://nitter.cz/xiaohuggg/status/1727514832185004251#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727514832185004251#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 02:30:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>亚太经合组织峰会上说的内容见这里：</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1726076059769335909#m">nitter.cz/xiaohuggg/status/1726076059769335909#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>