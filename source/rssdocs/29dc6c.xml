<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738122323650376150#m</id>
            <title>#Midjourney V6 效果炸裂

点主题贴看更多效果图...</title>
            <link>https://nitter.cz/xiaohuggg/status/1738122323650376150#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738122323650376150#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 09:00:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> V6 效果炸裂<br />
<br />
点主题贴看更多效果图...</p>
<p><a href="https://nitter.cz/hanqing_me/status/1738026967700742463#m">nitter.cz/hanqing_me/status/1738026967700742463#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738113688794763739#m</id>
            <title>一个有趣的研究...

TasteTime Machine：味道时光机

明治大学的宮下芳明教授开发了一个装置，这个装置能够让你感受食物在任何时间段的味道。

例如，它可以使新鲜食物尝起来像是放置几天后变质的味道.

同时也可以逆向操作（例如，将成熟或变质的食物恢复到之前的味道）

假设你今天做了一锅咖喱。通常，咖喱在放置一晚后，其味道会因为香料和成分的融合而变得更加浓郁。但是，你今天就想尝到这种“放置一晚”的味道。

- 使用“Taste Time Machine”：你可以使用这个装置对刚做好的咖喱进行处理。

- 该装置通过添加或调整特定的味道成分，模拟出咖喱放置一晚后的味道。

- 结果：经过处理后，你的咖喱现在尝起来就像是已经放置了一晚，拥有更加浓郁和融合的味道。

工作原理：

某些食物（如酒、奶酪）随着时间的推移会变得更美味，而另一些食物（如新鲜水果、烹饪食品）则可能因氧化或腐败而失去风味。这个研究项目探索了如何通过技术手段模拟这种味道随时间的变化。

研究团队通过味觉传感器测量食品的时序变化，并通过液体喷雾来再现时间的变化。例如，可以使新鲜的食物味道变得像是熟成后的味道，或者反过来，使熟成的食物恢复到之前的味道。

1、味道测量与数据收集：使用味觉传感器对特定食物（如番茄、咖喱）在不同时间点的味道进行测量。

收集的数据包括食物随时间变化的味道特征，如酸度、甜度、香料的浓度等。

2、建立味道变化模型：根据收集的数据，建立数学模型来描述食物味道随时间的变化规律。
这个模型可以预测食物在未来某个时间点的味道，或反向推算出过去某个时间点的味道。

3、味道再现技术 - TTTV3 设备：使用一种特殊的装置（ TTTV3），它装有多个管道泵，能够精确地混合不同浓度的味道溶液。

通过调整和混合不同的味道成分（如酸、糖、盐等），模拟出特定时间点的食物味道。

4、味道的正向和逆向变化：

正向变化：模拟食物随时间变化的自然熟成过程，如将新做的咖喱调整为放置一晚后的味道。

逆向变化：将已经变化的食物味道恢复到之前的状态，如将成熟的番茄恢复到未成熟时的味道。

应用场景：

这种技术可以用于多种场景，比如：

食品科学教育：教育学生了解不同食物随时间变化的味道。

食品研发：开发新的食品配方，快速测试食物随时间变化的味道。

餐饮体验：为顾客提供独特的餐饮体验，比如即时模拟熟成食品的味道。

论文：https://www.wiss.org/WISS2023Proceedings/data/08.pdf

TTTV3装置介绍：https://www.wiss.org/WISS2023Proceedings/data/2-C06.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1738113688794763739#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738113688794763739#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 08:26:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一个有趣的研究...<br />
<br />
TasteTime Machine：味道时光机<br />
<br />
明治大学的宮下芳明教授开发了一个装置，这个装置能够让你感受食物在任何时间段的味道。<br />
<br />
例如，它可以使新鲜食物尝起来像是放置几天后变质的味道.<br />
<br />
同时也可以逆向操作（例如，将成熟或变质的食物恢复到之前的味道）<br />
<br />
假设你今天做了一锅咖喱。通常，咖喱在放置一晚后，其味道会因为香料和成分的融合而变得更加浓郁。但是，你今天就想尝到这种“放置一晚”的味道。<br />
<br />
- 使用“Taste Time Machine”：你可以使用这个装置对刚做好的咖喱进行处理。<br />
<br />
- 该装置通过添加或调整特定的味道成分，模拟出咖喱放置一晚后的味道。<br />
<br />
- 结果：经过处理后，你的咖喱现在尝起来就像是已经放置了一晚，拥有更加浓郁和融合的味道。<br />
<br />
工作原理：<br />
<br />
某些食物（如酒、奶酪）随着时间的推移会变得更美味，而另一些食物（如新鲜水果、烹饪食品）则可能因氧化或腐败而失去风味。这个研究项目探索了如何通过技术手段模拟这种味道随时间的变化。<br />
<br />
研究团队通过味觉传感器测量食品的时序变化，并通过液体喷雾来再现时间的变化。例如，可以使新鲜的食物味道变得像是熟成后的味道，或者反过来，使熟成的食物恢复到之前的味道。<br />
<br />
1、味道测量与数据收集：使用味觉传感器对特定食物（如番茄、咖喱）在不同时间点的味道进行测量。<br />
<br />
收集的数据包括食物随时间变化的味道特征，如酸度、甜度、香料的浓度等。<br />
<br />
2、建立味道变化模型：根据收集的数据，建立数学模型来描述食物味道随时间的变化规律。<br />
这个模型可以预测食物在未来某个时间点的味道，或反向推算出过去某个时间点的味道。<br />
<br />
3、味道再现技术 - TTTV3 设备：使用一种特殊的装置（ TTTV3），它装有多个管道泵，能够精确地混合不同浓度的味道溶液。<br />
<br />
通过调整和混合不同的味道成分（如酸、糖、盐等），模拟出特定时间点的食物味道。<br />
<br />
4、味道的正向和逆向变化：<br />
<br />
正向变化：模拟食物随时间变化的自然熟成过程，如将新做的咖喱调整为放置一晚后的味道。<br />
<br />
逆向变化：将已经变化的食物味道恢复到之前的状态，如将成熟的番茄恢复到未成熟时的味道。<br />
<br />
应用场景：<br />
<br />
这种技术可以用于多种场景，比如：<br />
<br />
食品科学教育：教育学生了解不同食物随时间变化的味道。<br />
<br />
食品研发：开发新的食品配方，快速测试食物随时间变化的味道。<br />
<br />
餐饮体验：为顾客提供独特的餐饮体验，比如即时模拟熟成食品的味道。<br />
<br />
论文：<a href="https://www.wiss.org/WISS2023Proceedings/data/08.pdf">wiss.org/WISS2023Proceedings…</a><br />
<br />
TTTV3装置介绍：<a href="https://www.wiss.org/WISS2023Proceedings/data/2-C06.pdf">wiss.org/WISS2023Proceedings…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4RW9zUWJJQUFUOU5HLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4RXgyVWFnQUFkYjNxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4RkJURGFVQUE0RW9ZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738106503050260854#m</id>
            <title>截止13:26，腾讯港股下跌13.56%，网易港股下跌20.02%

两者市值下降4646亿人民币

版署征求意见稿全文11259字

所以据此计算该文每个汉字价值4126万人民币</title>
            <link>https://nitter.cz/xiaohuggg/status/1738106503050260854#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738106503050260854#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 07:57:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>截止13:26，腾讯港股下跌13.56%，网易港股下跌20.02%<br />
<br />
两者市值下降4646亿人民币<br />
<br />
版署征求意见稿全文11259字<br />
<br />
所以据此计算该文每个汉字价值4126万人民币</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738084768095637829#m</id>
            <title>R to @xiaohuggg: AppAgent 演示在 X 上浏览和搜索并关注用户

👋</title>
            <link>https://nitter.cz/xiaohuggg/status/1738084768095637829#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738084768095637829#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 06:31:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AppAgent 演示在 X 上浏览和搜索并关注用户<br />
<br />
👋</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwODQ3MTg2OTA5MzA2ODgvcHUvaW1nL0w2OWRqQ1prYWJmaUlacEcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738084326506676298#m</id>
            <title>R to @xiaohuggg: AppAgent 演示使用Gmail进行邮件撰写和发布

😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1738084326506676298#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738084326506676298#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 06:29:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AppAgent 演示使用Gmail进行邮件撰写和发布<br />
<br />
😄</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwODQxNTcxOTA5ODc3NzYvcHUvaW1nL0U4Y1FDTUswb3ZDWUhkVjUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738083914193965528#m</id>
            <title>AppAgent：让AI模仿人类在手机上操作APP

AppAgent可以通过自主学习和模仿人类的点击和滑动手势，能够在手机上执行各种任务。

它可以在社交媒体上发帖、帮你撰写和发送邮件 、使用地图、在线购物，甚至进行复杂的图像编辑...

AppAgent在50 个任务上进行了广泛测试，涵盖了10种不同的应用程序。

该项目由腾讯和德州大学达拉斯分校的研究团开发。

主要功能特点：

- 多模态代理：AppAgent 是一个基于大语言模型的多模态代理，它能够处理和理解多种类型的信息（如文本、图像、触控操作等）。这使得它能够理解复杂的任务并在各种不同的应用程序中执行这些任务。

- 直观交互：它能通过模仿人类的直观动作（如点击和滑动屏幕）来与智能手机应用程序交互。就像一个真人用户一样。

- 自主学习：AppAgent 通过观察和分析不同应用程序中的用户界面交互。并学习这些交互模式，并将所获得的知识编译成文档。

- 构建知识库：通过这些交互，AppAgent 构建了一个知识库，记录了不同应用程序的操作方法和界面布局。这个知识库随后用于指导代理在不同应用程序中执行任务。

-执行复杂任务：一旦学习了应用程序的操作方式，AppAgent 就能够执行跨应用程序的复杂任务，如发送电子邮件、编辑图片或进行在线购物。

项目及演示：https://appagent-official.github.io/
论文：https://arxiv.org/abs/2312.13771
GitHub：https://github.com/mnotgod96/AppAgent</title>
            <link>https://nitter.cz/xiaohuggg/status/1738083914193965528#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738083914193965528#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 06:27:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AppAgent：让AI模仿人类在手机上操作APP<br />
<br />
AppAgent可以通过自主学习和模仿人类的点击和滑动手势，能够在手机上执行各种任务。<br />
<br />
它可以在社交媒体上发帖、帮你撰写和发送邮件 、使用地图、在线购物，甚至进行复杂的图像编辑...<br />
<br />
AppAgent在50 个任务上进行了广泛测试，涵盖了10种不同的应用程序。<br />
<br />
该项目由腾讯和德州大学达拉斯分校的研究团开发。<br />
<br />
主要功能特点：<br />
<br />
- 多模态代理：AppAgent 是一个基于大语言模型的多模态代理，它能够处理和理解多种类型的信息（如文本、图像、触控操作等）。这使得它能够理解复杂的任务并在各种不同的应用程序中执行这些任务。<br />
<br />
- 直观交互：它能通过模仿人类的直观动作（如点击和滑动屏幕）来与智能手机应用程序交互。就像一个真人用户一样。<br />
<br />
- 自主学习：AppAgent 通过观察和分析不同应用程序中的用户界面交互。并学习这些交互模式，并将所获得的知识编译成文档。<br />
<br />
- 构建知识库：通过这些交互，AppAgent 构建了一个知识库，记录了不同应用程序的操作方法和界面布局。这个知识库随后用于指导代理在不同应用程序中执行任务。<br />
<br />
-执行复杂任务：一旦学习了应用程序的操作方式，AppAgent 就能够执行跨应用程序的复杂任务，如发送电子邮件、编辑图片或进行在线购物。<br />
<br />
项目及演示：<a href="https://appagent-official.github.io/">appagent-official.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.13771">arxiv.org/abs/2312.13771</a><br />
GitHub：<a href="https://github.com/mnotgod96/AppAgent">github.com/mnotgod96/AppAgen…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwODEwNzU4ODc1MDk1MDQvcHUvaW1nL1YyN2ZPN3M3dWVYWDF4MncuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738064778461851793#m</id>
            <title>国家新闻出版署：网络游戏不得设置每日登录、首次充值等诱导性奖励。

港股手游股持续下挫，截至目前，网易跌超22%，腾讯跌近14%，哔哩哔哩跌超10%，金山软件跌超5%。

到底是谁在做空中国？

总感觉下面的人和上面的人在对着干！</title>
            <link>https://nitter.cz/xiaohuggg/status/1738064778461851793#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738064778461851793#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 05:11:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>国家新闻出版署：网络游戏不得设置每日登录、首次充值等诱导性奖励。<br />
<br />
港股手游股持续下挫，截至目前，网易跌超22%，腾讯跌近14%，哔哩哔哩跌超10%，金山软件跌超5%。<br />
<br />
到底是谁在做空中国？<br />
<br />
总感觉下面的人和上面的人在对着干！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738061133662331294#m</id>
            <title>R to @xiaohuggg: 多语言演示

支持情感表情生成，包括中文、日语、法语、德语等。</title>
            <link>https://nitter.cz/xiaohuggg/status/1738061133662331294#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738061133662331294#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 04:57:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多语言演示<br />
<br />
支持情感表情生成，包括中文、日语、法语、德语等。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwNjA5NzE3MzcwNDI5NDQvcHUvaW1nL1dNcW5qclZvWXh6eExCcTUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738061131833618570#m</id>
            <title>R to @xiaohuggg: 跨越时空的对话演示

莱昂纳多主要表现愤怒，而蒙娜丽莎则表现快乐。</title>
            <link>https://nitter.cz/xiaohuggg/status/1738061131833618570#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738061131833618570#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 04:57:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>跨越时空的对话演示<br />
<br />
莱昂纳多主要表现愤怒，而蒙娜丽莎则表现快乐。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwNjA3OTM3OTM2NjI5NzYvcHUvaW1nL0VGbDM5YUFDb0hvLXBMeE8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738061130034266560#m</id>
            <title>最近这个让照片说话的项目很多

这不，字节跳动又搞了个

DREAM-Talk ：能从单张图像生成逼真的、带有情感的、能说话的面部动画。

支持各种情感表达，如愤怒、快乐、悲伤、惊讶等。表情会根据音频中的情感变化

多语言支持：支持包括中文、日文、法语、德语等。

项目及演示：https://magic-research.github.io/dream-talk/</title>
            <link>https://nitter.cz/xiaohuggg/status/1738061130034266560#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738061130034266560#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 04:57:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近这个让照片说话的项目很多<br />
<br />
这不，字节跳动又搞了个<br />
<br />
DREAM-Talk ：能从单张图像生成逼真的、带有情感的、能说话的面部动画。<br />
<br />
支持各种情感表达，如愤怒、快乐、悲伤、惊讶等。表情会根据音频中的情感变化<br />
<br />
多语言支持：支持包括中文、日文、法语、德语等。<br />
<br />
项目及演示：<a href="https://magic-research.github.io/dream-talk/">magic-research.github.io/dre…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwNjA3NDQzOTczNDA2NzIvcHUvaW1nL3BYNHNhYjhmRGhSMm1feUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738046778082312607#m</id>
            <title>R to @xiaohuggg: Spirited Away (千与千寻)漫画

 识别演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1738046778082312607#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738046778082312607#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 04:00:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Spirited Away (千与千寻)漫画<br />
<br />
 识别演示</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0I3SU9pTWJvQUFmUG84LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dCN0lPaU1ib0FBZlBvOC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738046595655192852#m</id>
            <title>R to @xiaohuggg: 演示视频 

基于对输入的掩码区域的识别，Osprey 可以生成语义描述，包括简短描述和详细描述。</title>
            <link>https://nitter.cz/xiaohuggg/status/1738046595655192852#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738046595655192852#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 03:59:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频 <br />
<br />
基于对输入的掩码区域的识别，Osprey 可以生成语义描述，包括简短描述和详细描述。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwNDYyMTA3MTg3ODk2MzIvcHUvaW1nL3FYVnlNTnlRbUdSNkYyRWouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738046153877508458#m</id>
            <title>Osprey：让LLM能够精确到像素级别的理解和解释图像

当用户在图像上选择一个区域时，Osprey 将专注于对这些区域进行特定分析，它可以精确到像素级别的分析和识别。

从而提供更丰富和详细的信息。

这在医学图像分析中非常有用，Osprey 可以用来识别和描述特定的组织或病变。

Osprey 通过结合像素级掩码区域和语言指令，扩展了多模态大语言模型（MLLMs），实现对图像更细粒度的视觉理解。使得多模态大型语言模型在处理图像时，能够更精确地理解图像中的具体区域和细节，从而提供更丰富和详细的信息。

主要功能特点：

1、像素级掩码区域分析：用户可以在图像上选择特定的区域（掩码区域）可以精确到像素级别。Osprey 将专注于这些区域进行分析。

2、细粒度视觉理解：通过使用像素级掩码区域，Osprey 可以关注图像中的一个非常小的或具体的区域，甚至是细节层面的内容，并对这个区域进行深入分析。

例如，如果图像中有一朵花，Osprey 可以专门分析并理解这朵花的特征，而不是整个图像。

3、语言指令响应：结合用户提供的语言指令，Osprey 能够根据用户选择的图像区域生成描述。这些描述可以是简短的（如“一朵红色的玫瑰”）或详细的（描述花的形状、颜色、周围环境等）。

这种功能使得 Osprey 不仅能识别图像中的对象，还能提供关于这些对象的有意义的信息。

4、与SAM 模型集成：SAM(Segment Anything Model) 是Meta AI一个模型，用于图像中的对象分割和识别。Osprey 可以与 SAM 无缝集成，可以利用 SAM 的能力来识别和分割图像中的任何对象。

这种集成使得 Osprey 不仅能识别图像中的特定部分，还能理解和描述这些部分的语义内容（即它们的意义和属性）。

应用场景：

医学图像分析：在医学领域，Osprey 可以用于分析和解释诊断图像，如 MRI 或 CT 扫描。它可以帮助医生识别和描述图像中的特定组织、病变或异常。

艺术品和文化遗产研究：在艺术品分析中，Osprey 可以帮助研究人员和历史学家详细解释和理解画作、雕塑或其他艺术品中的特定元素。

教育和培训：在教育领域，Osprey 可以用于创建交互式学习材料，帮助学生更好地理解复杂的图像，如生物学或地理学图表。

机器人视觉和自动化：在机器人技术和自动化领域，Osprey 可以帮助机器人更准确地识别和理解它们所看到的环境，从而提高它们的导航和决策能力。

安全和监控：在安全和监控应用中，Osprey 可以用于分析监控摄像头的图像，识别和描述特定的事件或对象。

GitHub：https://github.com/CircleRadon/Osprey
论文：https://arxiv.org/abs/2312.10032
在线演示：http://111.0.123.204:8000/</title>
            <link>https://nitter.cz/xiaohuggg/status/1738046153877508458#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738046153877508458#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 03:57:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Osprey：让LLM能够精确到像素级别的理解和解释图像<br />
<br />
当用户在图像上选择一个区域时，Osprey 将专注于对这些区域进行特定分析，它可以精确到像素级别的分析和识别。<br />
<br />
从而提供更丰富和详细的信息。<br />
<br />
这在医学图像分析中非常有用，Osprey 可以用来识别和描述特定的组织或病变。<br />
<br />
Osprey 通过结合像素级掩码区域和语言指令，扩展了多模态大语言模型（MLLMs），实现对图像更细粒度的视觉理解。使得多模态大型语言模型在处理图像时，能够更精确地理解图像中的具体区域和细节，从而提供更丰富和详细的信息。<br />
<br />
主要功能特点：<br />
<br />
1、像素级掩码区域分析：用户可以在图像上选择特定的区域（掩码区域）可以精确到像素级别。Osprey 将专注于这些区域进行分析。<br />
<br />
2、细粒度视觉理解：通过使用像素级掩码区域，Osprey 可以关注图像中的一个非常小的或具体的区域，甚至是细节层面的内容，并对这个区域进行深入分析。<br />
<br />
例如，如果图像中有一朵花，Osprey 可以专门分析并理解这朵花的特征，而不是整个图像。<br />
<br />
3、语言指令响应：结合用户提供的语言指令，Osprey 能够根据用户选择的图像区域生成描述。这些描述可以是简短的（如“一朵红色的玫瑰”）或详细的（描述花的形状、颜色、周围环境等）。<br />
<br />
这种功能使得 Osprey 不仅能识别图像中的对象，还能提供关于这些对象的有意义的信息。<br />
<br />
4、与SAM 模型集成：SAM(Segment Anything Model) 是Meta AI一个模型，用于图像中的对象分割和识别。Osprey 可以与 SAM 无缝集成，可以利用 SAM 的能力来识别和分割图像中的任何对象。<br />
<br />
这种集成使得 Osprey 不仅能识别图像中的特定部分，还能理解和描述这些部分的语义内容（即它们的意义和属性）。<br />
<br />
应用场景：<br />
<br />
医学图像分析：在医学领域，Osprey 可以用于分析和解释诊断图像，如 MRI 或 CT 扫描。它可以帮助医生识别和描述图像中的特定组织、病变或异常。<br />
<br />
艺术品和文化遗产研究：在艺术品分析中，Osprey 可以帮助研究人员和历史学家详细解释和理解画作、雕塑或其他艺术品中的特定元素。<br />
<br />
教育和培训：在教育领域，Osprey 可以用于创建交互式学习材料，帮助学生更好地理解复杂的图像，如生物学或地理学图表。<br />
<br />
机器人视觉和自动化：在机器人技术和自动化领域，Osprey 可以帮助机器人更准确地识别和理解它们所看到的环境，从而提高它们的导航和决策能力。<br />
<br />
安全和监控：在安全和监控应用中，Osprey 可以用于分析监控摄像头的图像，识别和描述特定的事件或对象。<br />
<br />
GitHub：<a href="https://github.com/CircleRadon/Osprey">github.com/CircleRadon/Ospre…</a><br />
论文：<a href="https://arxiv.org/abs/2312.10032">arxiv.org/abs/2312.10032</a><br />
在线演示：http://111.0.123.204:8000/</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwNDU4NDUxMTM4NDM3MTIvcHUvaW1nLzN1T0NnNUF2VDdNcF9oY18uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738029296973988176#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1738029296973988176#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738029296973988176#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 02:50:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/flyfront/status/1737849992478626181#m">nitter.cz/flyfront/status/1737849992478626181#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738029134276903283#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1738029134276903283#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738029134276903283#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 02:50:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/Maoku/status/1737857712564899880#m">nitter.cz/Maoku/status/1737857712564899880#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738029017192960501#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1738029017192960501#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738029017192960501#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 02:49:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<p><a href="https://nitter.cz/cumulo_autumn/status/1737817646715023417#m">nitter.cz/cumulo_autumn/status/1737817646715023417#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738028938226786446#m</id>
            <title>R to @xiaohuggg: 一些案例展示</title>
            <link>https://nitter.cz/xiaohuggg/status/1738028938226786446#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738028938226786446#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 02:49:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些案例展示</p>
<p><a href="https://nitter.cz/kiyoshi_shin/status/1737895105502281807#m">nitter.cz/kiyoshi_shin/status/1737895105502281807#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738028693845655835#m</id>
            <title>实时画图的开源解决方案

StreamDiffusion：能够以超过100fps的速度实现实时的交互式图像生成 ！ 

它能够在极短的时间内完成图像的生成。可以在10ms内生成一张图像，一分钟可以生成超过6000张图像。

可用于：
- 实时图像快速生成：适用于需要即时反馈的场景
- 文本到图像转换：适用于创意设计和内容创作
- 交互式绘图体验：可以实时交互以获得所需的图像输出
- 多样化的图像风格：支持生成不同风格和类型的图像

性能如何：

- StreamDiffusion 的批处理策略比传统的序列去噪方法快约 1.5 倍。RCFG 技术比传统的无分类引导快达 2.05 倍。

- 在 RTX 4090 GPU 上，图像到图像的生成速度可达 91.07fps，比 Diffusers 团队的 AutoPipline 快 59.56 倍。

- 在 RTX 3060 和 RTX 4090 GPU 上，能量消耗分别降低了 2.39 倍和 1.99 倍

- 使用 GPU：RTX 4090，CPU：Core i9-13900K，操作系统：Ubuntu 22.04.3 LTS 

SD-turbo模型：文本到图像，每秒可以生成约 106.16 帧图像。图像到图像每秒生成约 93.897 帧图像。

LCM-LoRA + KohakuV2 模型：文本到图像每秒生成约 38.023 帧图像。图像到图像每秒生成约 37.133 帧图像。

StreamDiffusion 在实时图像生成方面的性能，特别适合于需要快速生成图像的应用场景，如实时视频处理、游戏开发、艺术创作等。能够在短时间内生成高质量的图像，为用户提供了强大的创作和编辑能力。

GitHub：https://github.com/cumulo-autumn/StreamDiffusion
论文：https://arxiv.org/abs/2312.12491</title>
            <link>https://nitter.cz/xiaohuggg/status/1738028693845655835#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738028693845655835#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 02:48:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>实时画图的开源解决方案<br />
<br />
StreamDiffusion：能够以超过100fps的速度实现实时的交互式图像生成 ！ <br />
<br />
它能够在极短的时间内完成图像的生成。可以在10ms内生成一张图像，一分钟可以生成超过6000张图像。<br />
<br />
可用于：<br />
- 实时图像快速生成：适用于需要即时反馈的场景<br />
- 文本到图像转换：适用于创意设计和内容创作<br />
- 交互式绘图体验：可以实时交互以获得所需的图像输出<br />
- 多样化的图像风格：支持生成不同风格和类型的图像<br />
<br />
性能如何：<br />
<br />
- StreamDiffusion 的批处理策略比传统的序列去噪方法快约 1.5 倍。RCFG 技术比传统的无分类引导快达 2.05 倍。<br />
<br />
- 在 RTX 4090 GPU 上，图像到图像的生成速度可达 91.07fps，比 Diffusers 团队的 AutoPipline 快 59.56 倍。<br />
<br />
- 在 RTX 3060 和 RTX 4090 GPU 上，能量消耗分别降低了 2.39 倍和 1.99 倍<br />
<br />
- 使用 GPU：RTX 4090，CPU：Core i9-13900K，操作系统：Ubuntu 22.04.3 LTS <br />
<br />
SD-turbo模型：文本到图像，每秒可以生成约 106.16 帧图像。图像到图像每秒生成约 93.897 帧图像。<br />
<br />
LCM-LoRA + KohakuV2 模型：文本到图像每秒生成约 38.023 帧图像。图像到图像每秒生成约 37.133 帧图像。<br />
<br />
StreamDiffusion 在实时图像生成方面的性能，特别适合于需要快速生成图像的应用场景，如实时视频处理、游戏开发、艺术创作等。能够在短时间内生成高质量的图像，为用户提供了强大的创作和编辑能力。<br />
<br />
GitHub：<a href="https://github.com/cumulo-autumn/StreamDiffusion">github.com/cumulo-autumn/Str…</a><br />
论文：<a href="https://arxiv.org/abs/2312.12491">arxiv.org/abs/2312.12491</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzgwMjc2NDU4ODYyMDU5NTIvcHUvaW1nL212Zi1zYngwLTlucFRsd0wuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738017141704786073#m</id>
            <title>ChatGPT插件将废弃⚠️

OpenAI将引导开发者将ChatGPT插件迁移到GPTs…

但是我目前感觉插件更好用，可以多种组合…😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1738017141704786073#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738017141704786073#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 02:02:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT插件将废弃⚠️<br />
<br />
OpenAI将引导开发者将ChatGPT插件迁移到GPTs…<br />
<br />
但是我目前感觉插件更好用，可以多种组合…😐</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I2dFVRLWJnQUFHaERoLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>