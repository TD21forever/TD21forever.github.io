<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724591752424849469#m</id>
            <title>R to @xiaohuggg: 手机上的演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1724591752424849469#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724591752424849469#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 00:54:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>手机上的演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1OTExMzk5MTM5NDA5OTIvcHUvaW1nL01rbjFjTnIxR1RJZ25MR2YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724591088961507622#m</id>
            <title>Endless Zoom ：可以不断无限放大和扩充图像

Endless Zoom利用潜在一致性模型（LCMs）来实现图像的无限扩充。

通过LCMs的快速图像生成能力，可以生成连续的、无缝连接的图像内容，从而实现“无限扩充”。你可以不断地放大或缩小图像，而图像内容会持续生成，不会重复或结束。

在线体验：https://endless-zoom.vercel.app/

作者：@chigozienri

GitHub：https://github.com/replicate/endless-zoom/

LCMs是一个基于Stable Diffusion的图像生成模型，但生成图像的速度更快，只需要4到8步就能生成一张高质量的图像。

通过在 M1 或 M2 Mac 上运行 LCMs，你可以以每秒一张的速度生成 512x512 图像。

LCMs介绍：
https://x.com/xiaohuggg/status/1717562806822981835</title>
            <link>https://nitter.cz/xiaohuggg/status/1724591088961507622#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724591088961507622#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Nov 2023 00:52:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Endless Zoom ：可以不断无限放大和扩充图像<br />
<br />
Endless Zoom利用潜在一致性模型（LCMs）来实现图像的无限扩充。<br />
<br />
通过LCMs的快速图像生成能力，可以生成连续的、无缝连接的图像内容，从而实现“无限扩充”。你可以不断地放大或缩小图像，而图像内容会持续生成，不会重复或结束。<br />
<br />
在线体验：<a href="https://endless-zoom.vercel.app/">endless-zoom.vercel.app/</a><br />
<br />
作者：<a href="https://nitter.cz/chigozienri" title="Chigozie Nri">@chigozienri</a><br />
<br />
GitHub：<a href="https://github.com/replicate/endless-zoom/">github.com/replicate/endless…</a><br />
<br />
LCMs是一个基于Stable Diffusion的图像生成模型，但生成图像的速度更快，只需要4到8步就能生成一张高质量的图像。<br />
<br />
通过在 M1 或 M2 Mac 上运行 LCMs，你可以以每秒一张的速度生成 512x512 图像。<br />
<br />
LCMs介绍：<br />
<a href="https://x.com/xiaohuggg/status/1717562806822981835">x.com/xiaohuggg/status/17175…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ1ODMyMzczNzEyMzYzNTIvcHUvaW1nL2VmT0Nxam9kRWN4X3RGbDMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724575584851005828#m</id>
            <title>R to @xiaohuggg: 还有钢琴的🎹</title>
            <link>https://nitter.cz/xiaohuggg/status/1724575584851005828#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724575584851005828#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 23:50:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还有钢琴的🎹</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MTk1MzE1Mzc1ODUxMDI4NDkvcHUvaW1nLy1ZbFVabEZVcXNFdFA5ak8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724574547360509958#m</id>
            <title>学吉他的完美工具

cool…</title>
            <link>https://nitter.cz/xiaohuggg/status/1724574547360509958#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724574547360509958#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 23:46:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学吉他的完美工具<br />
<br />
cool…</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0NjIxMjk3MDc3MzI5OTUvcHUvaW1nL1dtM1JTNHdQNHIzSWFRMjEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724441907273597151#m</id>
            <title>酷站推荐：

一个收录不可描述声音的网站（见视频）

收录的非常多，都是用户自己上传的，100%真实

你也可以上传分享自己的

😂

没敢点开声音，自己进去听：https://orgasmsoundlibrary.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1724441907273597151#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724441907273597151#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 14:59:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>酷站推荐：<br />
<br />
一个收录不可描述声音的网站（见视频）<br />
<br />
收录的非常多，都是用户自己上传的，100%真实<br />
<br />
你也可以上传分享自己的<br />
<br />
😂<br />
<br />
没敢点开声音，自己进去听：<a href="https://orgasmsoundlibrary.com/">orgasmsoundlibrary.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0MzQzNDM2NDMyMTc5MjAvcHUvaW1nL1kzRmdFMm9QbG13VjFJWTEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724430161574027280#m</id>
            <title>Music ControlNet ：一种类似于SD ControlNetD 能精准控制音乐生成的模型

它特别的地方在于，可以让使用者非常精确地控制音乐的各种元素，比如旋律、音量的强弱，以及节奏的快慢。

甚至可以细致地调整音乐的每一个小细节。

Music ControlNet不仅能够控制音乐的全局属性（如风格、情绪和节奏），还能精确控制音乐的时间变化属性，例如节拍的位置和音乐的动态变化。

它能够根据用户的指令，生成符合要求的音乐。比如说，如果你想要一段旋律在特定的时间点出现，或者想要音乐在某个部分变得更加激烈，Music ControlNet 都能做到。

工作原理：

Music ControlNet采用了一种类似于图像领域ControlNet方法的像素级控制方式。它通过从训练音频中提取控制信息，然后对基于扩散的条件生成模型进行微调，从而实现对音频频谱图的控制。这种方法包括旋律、动态和节奏控制。

该模型还提供了一种新策略，允许创作者输入部分指定时间的控制信息。在评估时，研究人员不仅考虑了从音频中提取的控制信息，还考虑了创作者可能提供的控制信息，证明了该模型能够根据输入的控制信息生成真实的音乐。

Music ControlNet 的工作原理基于几个关键技术：

1、控制信息提取：首先，从训练用的音频中提取出控制信息。这些信息包括音乐的旋律、动态（音量的强弱变化）和节奏等元素。

2、条件生成模型：使用一种叫做“条件生成模型”的技术。这种模型可以根据给定的条件（在这里是控制信息）来生成音频。Music ControlNet 对这种模型进行了特别的调整，使其能够更好地适应音乐生成的任务。

3、频谱图控制：Music ControlNet 专注于控制音频的频谱图。频谱图是一种显示音频信号频率分布的图表，通过控制这个频谱图，模型可以精确地生成符合特定旋律、动态和节奏的音乐。

4、部分指定时间控制：这个特性允许用户只对音乐的某些部分指定控制信息。比如说，你可以只指定音乐开始的旋律，然后让模型自己决定剩下的部分。这给了用户更多的灵活性，同时也保留了一定的创造性。

5、微调和生成：通过微调预训练的模型，Music ControlNet 能够根据用户提供的控制信息生成音乐。这个过程涉及到对模型的参数进行细微调整，以适应特定的音乐风格或要求。

实验结果：

与其他少数可比较的音乐生成模型相比，例如MusicGen（一个接受文本和旋律输入的模型），Music ControlNet在保持输入旋律忠实度方面表现更好，尽管其参数数量少35倍，训练数据少11倍，并且能够实现两种额外的时间变化控制。

项目及演示：https://musiccontrolnet.github.io/web/
论文：https://arxiv.org/abs/2311.07069</title>
            <link>https://nitter.cz/xiaohuggg/status/1724430161574027280#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724430161574027280#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 14:12:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Music ControlNet ：一种类似于SD ControlNetD 能精准控制音乐生成的模型<br />
<br />
它特别的地方在于，可以让使用者非常精确地控制音乐的各种元素，比如旋律、音量的强弱，以及节奏的快慢。<br />
<br />
甚至可以细致地调整音乐的每一个小细节。<br />
<br />
Music ControlNet不仅能够控制音乐的全局属性（如风格、情绪和节奏），还能精确控制音乐的时间变化属性，例如节拍的位置和音乐的动态变化。<br />
<br />
它能够根据用户的指令，生成符合要求的音乐。比如说，如果你想要一段旋律在特定的时间点出现，或者想要音乐在某个部分变得更加激烈，Music ControlNet 都能做到。<br />
<br />
工作原理：<br />
<br />
Music ControlNet采用了一种类似于图像领域ControlNet方法的像素级控制方式。它通过从训练音频中提取控制信息，然后对基于扩散的条件生成模型进行微调，从而实现对音频频谱图的控制。这种方法包括旋律、动态和节奏控制。<br />
<br />
该模型还提供了一种新策略，允许创作者输入部分指定时间的控制信息。在评估时，研究人员不仅考虑了从音频中提取的控制信息，还考虑了创作者可能提供的控制信息，证明了该模型能够根据输入的控制信息生成真实的音乐。<br />
<br />
Music ControlNet 的工作原理基于几个关键技术：<br />
<br />
1、控制信息提取：首先，从训练用的音频中提取出控制信息。这些信息包括音乐的旋律、动态（音量的强弱变化）和节奏等元素。<br />
<br />
2、条件生成模型：使用一种叫做“条件生成模型”的技术。这种模型可以根据给定的条件（在这里是控制信息）来生成音频。Music ControlNet 对这种模型进行了特别的调整，使其能够更好地适应音乐生成的任务。<br />
<br />
3、频谱图控制：Music ControlNet 专注于控制音频的频谱图。频谱图是一种显示音频信号频率分布的图表，通过控制这个频谱图，模型可以精确地生成符合特定旋律、动态和节奏的音乐。<br />
<br />
4、部分指定时间控制：这个特性允许用户只对音乐的某些部分指定控制信息。比如说，你可以只指定音乐开始的旋律，然后让模型自己决定剩下的部分。这给了用户更多的灵活性，同时也保留了一定的创造性。<br />
<br />
5、微调和生成：通过微调预训练的模型，Music ControlNet 能够根据用户提供的控制信息生成音乐。这个过程涉及到对模型的参数进行细微调整，以适应特定的音乐风格或要求。<br />
<br />
实验结果：<br />
<br />
与其他少数可比较的音乐生成模型相比，例如MusicGen（一个接受文本和旋律输入的模型），Music ControlNet在保持输入旋律忠实度方面表现更好，尽管其参数数量少35倍，训练数据少11倍，并且能够实现两种额外的时间变化控制。<br />
<br />
项目及演示：<a href="https://musiccontrolnet.github.io/web/">musiccontrolnet.github.io/we…</a><br />
论文：<a href="https://arxiv.org/abs/2311.07069">arxiv.org/abs/2311.07069</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQ0Mjk3MDM4NTM4NzkyOTYvcHUvaW1nL3BNVEFsVkhMRjJYVkhMaGouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724356117059477614#m</id>
            <title>人工智能爱上男主

争风吃醋

杀死男主女友和一屋子人类😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1724356117059477614#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724356117059477614#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 09:18:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>人工智能爱上男主<br />
<br />
争风吃醋<br />
<br />
杀死男主女友和一屋子人类😐</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI0MzU1Mzc2NDI5MjkzNTY4L2ltZy9YcWpJdkhfcWdfbHBHMkhGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724317866013704384#m</id>
            <title>学习如何在GPTs中调用外部API

👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1724317866013704384#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724317866013704384#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 06:46:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>学习如何在GPTs中调用外部API<br />
<br />
👍</p>
<p><a href="https://nitter.cz/dotey/status/1724305358254952799#m">nitter.cz/dotey/status/1724305358254952799#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724285898874106350#m</id>
            <title>没有一个人投诉

也没有一个部门出来管管

居然要一个外国公司来出面整治，国将不国啊

双 11 期间，跳转广告泛滥，一碰手机就跳转到购物APP！苹果公司下手整治：已通知国内多家头部 App 要求它们移除陀螺仪权限，摇一摇跳转广告被禁止。

这些收到苹果通知的 App 包括但不限于在线视频软件、短视频软件、音频软件、邮箱软件等。

摇一摇功能调用的陀螺仪权限，是一种很早就有的功能，可以用来抢电视红包、识别歌曲等，现在被用来做广告跳转，而有些手机没有陀螺仪权限开关，用户无法自行关闭。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724285898874106350#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724285898874106350#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 04:39:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>没有一个人投诉<br />
<br />
也没有一个部门出来管管<br />
<br />
居然要一个外国公司来出面整治，国将不国啊<br />
<br />
双 11 期间，跳转广告泛滥，一碰手机就跳转到购物APP！苹果公司下手整治：已通知国内多家头部 App 要求它们移除陀螺仪权限，摇一摇跳转广告被禁止。<br />
<br />
这些收到苹果通知的 App 包括但不限于在线视频软件、短视频软件、音频软件、邮箱软件等。<br />
<br />
摇一摇功能调用的陀螺仪权限，是一种很早就有的功能，可以用来抢电视红包、识别歌曲等，现在被用来做广告跳转，而有些手机没有陀螺仪权限开关，用户无法自行关闭。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0za2lwSWJVQUFPX1VyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724278540269682737#m</id>
            <title>MM-Navigator：基于GPT-4V的智能手机界面导航助手

MM-Navigator能够像人类用户一样与智能手机屏幕进行交互，并确定为了完成给定指令而采取的后续动作。

研究发现，GPT-4V在没有任何先前训练的情况下就能很好地理解和操作智能手机界面。具有先进的屏幕解释、动作推理和精确动作定位能力。

研究者们首先在他们收集的iOS屏幕数据集上对MM-Navigator进行基准测试。

MM-Navigator能够根据屏幕上的信息和用户的指令，生成合理的动作描述，并准确地执行这些动作。

在测试中，这个系统在生成动作描述方面的准确率达到91%，在执行单个指令时的准确率为75%。

此外，他们还在一个Android屏幕导航数据集的子集上评估了该模型，其中该模型在零样本情况下的表现超过了之前的GUI导航器。

论文：https://arxiv.org/pdf/2311.07562.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1724278540269682737#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724278540269682737#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 04:10:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MM-Navigator：基于GPT-4V的智能手机界面导航助手<br />
<br />
MM-Navigator能够像人类用户一样与智能手机屏幕进行交互，并确定为了完成给定指令而采取的后续动作。<br />
<br />
研究发现，GPT-4V在没有任何先前训练的情况下就能很好地理解和操作智能手机界面。具有先进的屏幕解释、动作推理和精确动作定位能力。<br />
<br />
研究者们首先在他们收集的iOS屏幕数据集上对MM-Navigator进行基准测试。<br />
<br />
MM-Navigator能够根据屏幕上的信息和用户的指令，生成合理的动作描述，并准确地执行这些动作。<br />
<br />
在测试中，这个系统在生成动作描述方面的准确率达到91%，在执行单个指令时的准确率为75%。<br />
<br />
此外，他们还在一个Android屏幕导航数据集的子集上评估了该模型，其中该模型在零样本情况下的表现超过了之前的GUI导航器。<br />
<br />
论文：<a href="https://arxiv.org/pdf/2311.07562.pdf">arxiv.org/pdf/2311.07562.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0zZHBWM2JNQUE3THVpLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0zZHcxMWJnQUF1RlM1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724248442669879806#m</id>
            <title>AI VoiceOver：使用 OpenAI 的 GPT 4V API 和 TTS 可以识别视频里面的内容并自动为视频添加语音解说。

只需要上传100M以内的视频即可，系统会自动分析识别视频内容，然后生成解说词再转换成语音自动配音解说。

在线体验：https://gptv-app.vercel.app/

作者：@taishik_</title>
            <link>https://nitter.cz/xiaohuggg/status/1724248442669879806#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724248442669879806#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 02:10:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI VoiceOver：使用 OpenAI 的 GPT 4V API 和 TTS 可以识别视频里面的内容并自动为视频添加语音解说。<br />
<br />
只需要上传100M以内的视频即可，系统会自动分析识别视频内容，然后生成解说词再转换成语音自动配音解说。<br />
<br />
在线体验：<a href="https://gptv-app.vercel.app/">gptv-app.vercel.app/</a><br />
<br />
作者：<a href="https://nitter.cz/taishik_" title="Taishi 🇯🇵🇨🇦">@taishik_</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQyNDcxMjMwMjQzODQwMDAvcHUvaW1nL3pjTjVXWmg5U3NWWVNpUTAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724244607092064767#m</id>
            <title>大语言模型的幻觉排行榜 GPT 4最低 Google垫底

榜单比较了不同大语言模型在总结短文档时产生幻觉（hallucination）的表现。

GPT-4的准确率为97.0%，幻觉率为3.0%，回答率为100.0%。

Google Palm 的两款表现垫底，其中Palm Chat 2的准确率为72.8%，幻觉率高达27.2%，回答率为88.8%。

这个排行榜是由@vectara 的幻觉评估模型计算得出的，该模型评估了LLM在总结文档时引入幻觉的频率。排行榜的数据会随着模型和LLM的更新而定期更新。

排行榜上的数据包括不同模型的准确率、幻觉率、回答率和平均总结长度（词数）。例如，GPT-4的准确率为97.0%，幻觉率为3.0%，回答率为100.0%，平均总结长度为81.1词。而GPT-3.5、Llama 2 70B、Llama 2 7B等其他模型也有类似的数据。

为了确定这个排行榜，Vectara训练了一个模型来检测LLM输出中的幻觉，使用了来自对总结模型的事实一致性研究的各种开源数据集。然后，他们通过公共API向上述LLM提供了1000个短文档，并要求它们总结每个文档，仅使用文档中呈现的事实。在这1000个文档中，只有831个文档被每个模型总结，其余文档由于内容限制被至少一个模型拒绝。使用这831个文档，他们计算了每个模型的总体准确率（无幻觉）和幻觉率（100 - 准确率）。

该模型已在Hugging Face上开源供商业使用，网址为：https://huggingface.co/vectara/hallucination_evaluation_model

GitHub：https://github.com/vectara/hallucination-leaderboard</title>
            <link>https://nitter.cz/xiaohuggg/status/1724244607092064767#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724244607092064767#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 01:55:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大语言模型的幻觉排行榜 GPT 4最低 Google垫底<br />
<br />
榜单比较了不同大语言模型在总结短文档时产生幻觉（hallucination）的表现。<br />
<br />
GPT-4的准确率为97.0%，幻觉率为3.0%，回答率为100.0%。<br />
<br />
Google Palm 的两款表现垫底，其中Palm Chat 2的准确率为72.8%，幻觉率高达27.2%，回答率为88.8%。<br />
<br />
这个排行榜是由<a href="https://nitter.cz/vectara" title="Vectara">@vectara</a> 的幻觉评估模型计算得出的，该模型评估了LLM在总结文档时引入幻觉的频率。排行榜的数据会随着模型和LLM的更新而定期更新。<br />
<br />
排行榜上的数据包括不同模型的准确率、幻觉率、回答率和平均总结长度（词数）。例如，GPT-4的准确率为97.0%，幻觉率为3.0%，回答率为100.0%，平均总结长度为81.1词。而GPT-3.5、Llama 2 70B、Llama 2 7B等其他模型也有类似的数据。<br />
<br />
为了确定这个排行榜，Vectara训练了一个模型来检测LLM输出中的幻觉，使用了来自对总结模型的事实一致性研究的各种开源数据集。然后，他们通过公共API向上述LLM提供了1000个短文档，并要求它们总结每个文档，仅使用文档中呈现的事实。在这1000个文档中，只有831个文档被每个模型总结，其余文档由于内容限制被至少一个模型拒绝。使用这831个文档，他们计算了每个模型的总体准确率（无幻觉）和幻觉率（100 - 准确率）。<br />
<br />
该模型已在Hugging Face上开源供商业使用，网址为：<a href="https://huggingface.co/vectara/hallucination_evaluation_model">huggingface.co/vectara/hallu…</a><br />
<br />
GitHub：<a href="https://github.com/vectara/hallucination-leaderboard">github.com/vectara/hallucina…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0yX1J2dmFZQUEyWmhULnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724239489302974768#m</id>
            <title>英伟达发布最新AI芯片H200  推理速度提升2倍，使用成本降低一半

性能提升：H200的推断速度几乎是H100的两倍，

内存升级：H200是首款采用HBM3e内存的GPU，提供141GB的HBM3e（1.4倍），显存带宽从3.35TB/秒提升至4.8TB/秒（2倍）。

成本和规模：H100芯片的成本在25,000到40,000美元之间，H200 在保持与 H100 相同功耗配置的同时，实现了前所未有的性能，使 AI 工厂和超级计算系统更快、更环保，为 AI 和科学界带来经济优势

兼容性：H200与H100兼容，便于现有用户升级。

基于Hopper架构：H200基于英伟达的Hopper架构。

Transformer Engine：支持加速基于Transformer架构的大型语言模型和其他深度学习模型。

上市时间：计划于明年二季度上市，2024年将H100的产量增加两倍。

云服务和部署：从2024年第二季度开始，将提供搭载H200的系统和云实例。亚马逊网络服务、谷歌云、微软Azure和甲骨文云基础设施将成为首批提供基于H200的云实例的云服务提供商。

详细：https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446</title>
            <link>https://nitter.cz/xiaohuggg/status/1724239489302974768#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724239489302974768#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 01:35:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>英伟达发布最新AI芯片H200  推理速度提升2倍，使用成本降低一半<br />
<br />
性能提升：H200的推断速度几乎是H100的两倍，<br />
<br />
内存升级：H200是首款采用HBM3e内存的GPU，提供141GB的HBM3e（1.4倍），显存带宽从3.35TB/秒提升至4.8TB/秒（2倍）。<br />
<br />
成本和规模：H100芯片的成本在25,000到40,000美元之间，H200 在保持与 H100 相同功耗配置的同时，实现了前所未有的性能，使 AI 工厂和超级计算系统更快、更环保，为 AI 和科学界带来经济优势<br />
<br />
兼容性：H200与H100兼容，便于现有用户升级。<br />
<br />
基于Hopper架构：H200基于英伟达的Hopper架构。<br />
<br />
Transformer Engine：支持加速基于Transformer架构的大型语言模型和其他深度学习模型。<br />
<br />
上市时间：计划于明年二季度上市，2024年将H100的产量增加两倍。<br />
<br />
云服务和部署：从2024年第二季度开始，将提供搭载H200的系统和云实例。亚马逊网络服务、谷歌云、微软Azure和甲骨文云基础设施将成为首批提供基于H200的云实例的云服务提供商。<br />
<br />
详细：<a href="https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446">nvdam.widen.net/s/nb5zzzsjdf…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0yNDh1QmFJQUE1SXdzLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi0yNUYwOWFJQUF5WVFPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724075967948492986#m</id>
            <title>D-POINT ：一款开源数字手写笔，它使用摄像头跟踪和惯性测量来实现 6DoF（六自由度）输入，它能够精确地追踪笔在空间中的位置和方向，然后在数字设备上创建相应的输入。具有低延迟、压力敏感度和亚毫米精度。

该手写笔可在任何平面上使用，不需要特定的硬件或表面，并可与消费级网络摄像头配合使用。

GitHub：https://github.com/Jcparkyn/dpoint

硬件设计：笔的主体是通过 3D 打印制成的两半部分。它包含一个力感应器、一个通过 USB-C 充电的锂离子电池，以及一个用于逻辑和蓝牙的基于 Arduino 的开发板。

视觉姿态估计（VPE）：VPE 过程包括四个主要步骤：标记检测、滚动快门校正、透视 n 点（PnP）算法和坐标转换。这些步骤共同工作以估计笔相对于摄像头的姿态。

惯性融合：使用扩展卡尔曼滤波器（EKF）将 VPE 估计与来自加速度计和陀螺仪的惯性数据融合，并使用 Rauch-Tung-Striebel (RTS) 算法实时精化估计。为了解决摄像头帧的时间延迟问题，使用了负时间测量更新算法。</title>
            <link>https://nitter.cz/xiaohuggg/status/1724075967948492986#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724075967948492986#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 13 Nov 2023 14:45:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>D-POINT ：一款开源数字手写笔，它使用摄像头跟踪和惯性测量来实现 6DoF（六自由度）输入，它能够精确地追踪笔在空间中的位置和方向，然后在数字设备上创建相应的输入。具有低延迟、压力敏感度和亚毫米精度。<br />
<br />
该手写笔可在任何平面上使用，不需要特定的硬件或表面，并可与消费级网络摄像头配合使用。<br />
<br />
GitHub：<a href="https://github.com/Jcparkyn/dpoint">github.com/Jcparkyn/dpoint</a><br />
<br />
硬件设计：笔的主体是通过 3D 打印制成的两半部分。它包含一个力感应器、一个通过 USB-C 充电的锂离子电池，以及一个用于逻辑和蓝牙的基于 Arduino 的开发板。<br />
<br />
视觉姿态估计（VPE）：VPE 过程包括四个主要步骤：标记检测、滚动快门校正、透视 n 点（PnP）算法和坐标转换。这些步骤共同工作以估计笔相对于摄像头的姿态。<br />
<br />
惯性融合：使用扩展卡尔曼滤波器（EKF）将 VPE 估计与来自加速度计和陀螺仪的惯性数据融合，并使用 Rauch-Tung-Striebel (RTS) 算法实时精化估计。为了解决摄像头帧的时间延迟问题，使用了负时间测量更新算法。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjQwNzU2NjkzODk1NTM2NjQvcHUvaW1nL2s1RjNqZk4xZDNYeFhjN3guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1724025206187217049#m</id>
            <title>OpenAI CEO Sam Altman 在接受金融时报采访中，透露了更多OpenAI的计划：

他们正在寻求从微软获得更多资金支持，以构建真正的通用人工智能（AGI）。同时还透露了关于GPT 5的一些信息和公司AGI愿景目标！

他认为：AI 模型需要能够超越现有知识，创造新的知识的能力！

信息量很大！

以下是文章的主要内容：

1.OpenAI 和微软的合作：Altman 表示，与微软首席执行官 Satya Nadella 的合作“进行得非常顺利”，他预计“随着时间的推移会从这家科技巨头和其他投资者那里筹集更多资金”，以应对构建更复杂 AI 模型的高昂成本。今年早些时候，微软在一项“多年”协议中向 OpenAI 投资了 100 亿美元，据了解，这使得总部位于旧金山的公司估值达到了 290 亿美元。

2.OpenAI 的商业模式：OpenAI 最近宣布了一系列新工具和对其现有模型 GPT-4 的升级，这些工具包括可以针对特定应用进行调整和定制的 ChatGPT 定制版本，以及一个 GPT 商店，即最佳应用的市场。最终目标将是与最受欢迎的 GPT 创建者分成收入，这种商业模式类似于苹果的 App Store。

3.GPT-5 的开发：Altman 透露，公司正在开发下一代 AI 模型 GPT-5，尽管他没有承诺发布时间表。这将需要更多数据来训练，Altman 表示，这些数据将来自互联网上公开可用的数据集以及公司的专有数据。OpenAI 最近发出了征集大规模数据集的呼吁，特别是那些“今天在互联网上尚未公开轻松获取”的数据集，尤其是长篇写作或任何格式的对话。

4. AI 芯片的竞争：为了训练其模型，OpenAI 和大多数其他大型 AI 公司一样使用 Nvidia 的高级 H100 芯片。Altman 提到，由于 Nvidia 的芯片供应短缺，今年一直存在“严重的紧张局势”。然而，随着谷歌、微软、AMD 和英特尔等其他公司准备发布竞争对手的 AI 芯片，对 Nvidia 的依赖可能不会持续太久。

5.人工通用智能的发展：尽管 OpenAI 取得了消费者成功，但 Altman 表示，公司寻求向构建人工通用智能方向取得进展。他认为，大型语言模型（LLM），即支撑 ChatGPT 的模型，是“构建 AGI 的核心部分之一，但在其上还会有很多其他部分”。他还强调了语言作为信息压缩的重要性，这是他认为像谷歌 DeepMind 这样的公司忽视的一个因素。

OpenAI的通用人工智能（AGI）的愿景：

1.构建与人类智能相当的软件：Sam Altman 的愿景是创建一种计算机软件，其智能水平与人类相当。这种软件被称为人工通用智能（AGI），它能够执行与人类智能相似的复杂任务和决策。

2.安全性和效益：Altman 强调，构建 AGI 的过程中需要考虑如何确保其安全，并且要弄清楚如何从中获得益处。这意味着在开发过程中，安全性和伦理问题将是重要的考虑因素。

3.构建更强大的自主代理：OpenAI 正在努力构建更加自主的代理，这些代理能够执行各种任务和动作，例如执行代码、进行支付、发送电子邮件或提交索赔。随着时间的推移，这些代理的能力将变得越来越强大，任务也将变得越来越复杂。

4.开发下一代 AI 模型：OpenAI 正在开发 GPT-5，这是其下一代 AI 模型。GPT-5 预计将比其前身更复杂，但其确切的新能力和技能在训练之前难以预测。

5.理解和创造新知识：Altman 认为，开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识。

原文：https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded</title>
            <link>https://nitter.cz/xiaohuggg/status/1724025206187217049#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1724025206187217049#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 13 Nov 2023 11:23:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI CEO Sam Altman 在接受金融时报采访中，透露了更多OpenAI的计划：<br />
<br />
他们正在寻求从微软获得更多资金支持，以构建真正的通用人工智能（AGI）。同时还透露了关于GPT 5的一些信息和公司AGI愿景目标！<br />
<br />
他认为：AI 模型需要能够超越现有知识，创造新的知识的能力！<br />
<br />
信息量很大！<br />
<br />
以下是文章的主要内容：<br />
<br />
1.OpenAI 和微软的合作：Altman 表示，与微软首席执行官 Satya Nadella 的合作“进行得非常顺利”，他预计“随着时间的推移会从这家科技巨头和其他投资者那里筹集更多资金”，以应对构建更复杂 AI 模型的高昂成本。今年早些时候，微软在一项“多年”协议中向 OpenAI 投资了 100 亿美元，据了解，这使得总部位于旧金山的公司估值达到了 290 亿美元。<br />
<br />
2.OpenAI 的商业模式：OpenAI 最近宣布了一系列新工具和对其现有模型 GPT-4 的升级，这些工具包括可以针对特定应用进行调整和定制的 ChatGPT 定制版本，以及一个 GPT 商店，即最佳应用的市场。最终目标将是与最受欢迎的 GPT 创建者分成收入，这种商业模式类似于苹果的 App Store。<br />
<br />
3.GPT-5 的开发：Altman 透露，公司正在开发下一代 AI 模型 GPT-5，尽管他没有承诺发布时间表。这将需要更多数据来训练，Altman 表示，这些数据将来自互联网上公开可用的数据集以及公司的专有数据。OpenAI 最近发出了征集大规模数据集的呼吁，特别是那些“今天在互联网上尚未公开轻松获取”的数据集，尤其是长篇写作或任何格式的对话。<br />
<br />
4. AI 芯片的竞争：为了训练其模型，OpenAI 和大多数其他大型 AI 公司一样使用 Nvidia 的高级 H100 芯片。Altman 提到，由于 Nvidia 的芯片供应短缺，今年一直存在“严重的紧张局势”。然而，随着谷歌、微软、AMD 和英特尔等其他公司准备发布竞争对手的 AI 芯片，对 Nvidia 的依赖可能不会持续太久。<br />
<br />
5.人工通用智能的发展：尽管 OpenAI 取得了消费者成功，但 Altman 表示，公司寻求向构建人工通用智能方向取得进展。他认为，大型语言模型（LLM），即支撑 ChatGPT 的模型，是“构建 AGI 的核心部分之一，但在其上还会有很多其他部分”。他还强调了语言作为信息压缩的重要性，这是他认为像谷歌 DeepMind 这样的公司忽视的一个因素。<br />
<br />
OpenAI的通用人工智能（AGI）的愿景：<br />
<br />
1.构建与人类智能相当的软件：Sam Altman 的愿景是创建一种计算机软件，其智能水平与人类相当。这种软件被称为人工通用智能（AGI），它能够执行与人类智能相似的复杂任务和决策。<br />
<br />
2.安全性和效益：Altman 强调，构建 AGI 的过程中需要考虑如何确保其安全，并且要弄清楚如何从中获得益处。这意味着在开发过程中，安全性和伦理问题将是重要的考虑因素。<br />
<br />
3.构建更强大的自主代理：OpenAI 正在努力构建更加自主的代理，这些代理能够执行各种任务和动作，例如执行代码、进行支付、发送电子邮件或提交索赔。随着时间的推移，这些代理的能力将变得越来越强大，任务也将变得越来越复杂。<br />
<br />
4.开发下一代 AI 模型：OpenAI 正在开发 GPT-5，这是其下一代 AI 模型。GPT-5 预计将比其前身更复杂，但其确切的新能力和技能在训练之前难以预测。<br />
<br />
5.理解和创造新知识：Altman 认为，开发 AGI 的最大挑战之一是使这些系统能够进行基本的理解和创新。他比喻说，就像艾萨克·牛顿（Isaac Newton）发明微积分一样，AI 模型也需要能够超越现有知识，创造新的知识。<br />
<br />
原文：<a href="https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded">ft.com/content/dd9ba2f6-f509…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRi16MXVCSWFjQUFtbHlyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>