<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735149171781533870#m</id>
            <title>DeepMind 开发的文本到图像生成技术：Imagen 2

- 根据文字生成图片：利用文本提示生成高质量、逼真的图像。

- 更真实的图像生成：在渲染真实的手和人脸等方面取得了进步，减少了视觉干扰元素。

- 改善图片和文字的匹配：通过更好地理解文字描述，Imagen 2 能更准确地生成与描述相符的图片。

- 图像编辑能力：支持“修图”和“扩图”功能，用户可以通过提供参考图像和图像遮罩来生成新内容，直接进入原始图像，或将原始图像扩展到其边界之外。

技术创新：

Imagen 2使用基于扩散的技术来提高图像生成的灵活性，特别是在控制和调整图像风格方面。用户可以通过结合参考图像和文本提示来引导图像生成过程，创造出符合特定风格和内容的视觉作品。

它通过利用其训练数据的自然分布来生成更逼真的图像，而不是采用预设的风格。

开发团队特别强调了对图像附加文字描述（字幕）的重要性。这种方法的具体含义和目的如下：

增强训练数据集：为了提高模型生成图像的质量和准确性，Imagen 2 的训练数据集中包含了更多详细的图像字幕。这些字幕为模型提供了关于图像内容的额外信息。

学习不同的字幕风格：通过训练，Imagen 2 学会了理解和解释不同风格的文字描述。这意味着模型能够处理和响应各种类型的用户输入，无论其语言风格如何。

更好地理解用户提示：增加对图像字幕的描述不仅提高了模型对特定图像内容的理解，还使其能够更准确地根据用户的文本提示生成图像。这是因为模型通过学习大量的图像和对应字幕，能够更好地把握文本和视觉内容之间的关联。

 Imagen 2 现已在Vertex AI 上全面推出，你可以在几秒钟内将用户的想象力转化为高品质的视觉素材。

- 文字转图片生成：使用文字提示来生成新图片。

- 使用文字提示修改图片：允许用户使用文字提示来修改整个上传或生成的图片。

- 局部图片修改：用户可以定义遮盖区域，仅修改上传或生成的图片的某些部分。

- 图片放大：支持放大现有的、生成的或编辑后的图片。

- 主题模型微调：允许用户使用特定主题（如特定的手提包或鞋）来微调模型，以生成图片。

- 视觉字幕：提供图片的文字说明。

- 视觉问答 (VQA)：提供有关图片的问题解答。

- 多语言文本渲染：支持多种语言，能够在图像中准确地覆盖文本。

- 标志生成：能够创造公司或产品标志，并将其覆盖在图像中。

Imagen 2详细介绍：https://deepmind.google/technologies/imagen-2/

Imagen 2 on Vertex AI：https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available</title>
            <link>https://nitter.cz/xiaohuggg/status/1735149171781533870#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735149171781533870#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 04:06:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind 开发的文本到图像生成技术：Imagen 2<br />
<br />
- 根据文字生成图片：利用文本提示生成高质量、逼真的图像。<br />
<br />
- 更真实的图像生成：在渲染真实的手和人脸等方面取得了进步，减少了视觉干扰元素。<br />
<br />
- 改善图片和文字的匹配：通过更好地理解文字描述，Imagen 2 能更准确地生成与描述相符的图片。<br />
<br />
- 图像编辑能力：支持“修图”和“扩图”功能，用户可以通过提供参考图像和图像遮罩来生成新内容，直接进入原始图像，或将原始图像扩展到其边界之外。<br />
<br />
技术创新：<br />
<br />
Imagen 2使用基于扩散的技术来提高图像生成的灵活性，特别是在控制和调整图像风格方面。用户可以通过结合参考图像和文本提示来引导图像生成过程，创造出符合特定风格和内容的视觉作品。<br />
<br />
它通过利用其训练数据的自然分布来生成更逼真的图像，而不是采用预设的风格。<br />
<br />
开发团队特别强调了对图像附加文字描述（字幕）的重要性。这种方法的具体含义和目的如下：<br />
<br />
增强训练数据集：为了提高模型生成图像的质量和准确性，Imagen 2 的训练数据集中包含了更多详细的图像字幕。这些字幕为模型提供了关于图像内容的额外信息。<br />
<br />
学习不同的字幕风格：通过训练，Imagen 2 学会了理解和解释不同风格的文字描述。这意味着模型能够处理和响应各种类型的用户输入，无论其语言风格如何。<br />
<br />
更好地理解用户提示：增加对图像字幕的描述不仅提高了模型对特定图像内容的理解，还使其能够更准确地根据用户的文本提示生成图像。这是因为模型通过学习大量的图像和对应字幕，能够更好地把握文本和视觉内容之间的关联。<br />
<br />
 Imagen 2 现已在Vertex AI 上全面推出，你可以在几秒钟内将用户的想象力转化为高品质的视觉素材。<br />
<br />
- 文字转图片生成：使用文字提示来生成新图片。<br />
<br />
- 使用文字提示修改图片：允许用户使用文字提示来修改整个上传或生成的图片。<br />
<br />
- 局部图片修改：用户可以定义遮盖区域，仅修改上传或生成的图片的某些部分。<br />
<br />
- 图片放大：支持放大现有的、生成的或编辑后的图片。<br />
<br />
- 主题模型微调：允许用户使用特定主题（如特定的手提包或鞋）来微调模型，以生成图片。<br />
<br />
- 视觉字幕：提供图片的文字说明。<br />
<br />
- 视觉问答 (VQA)：提供有关图片的问题解答。<br />
<br />
- 多语言文本渲染：支持多种语言，能够在图像中准确地覆盖文本。<br />
<br />
- 标志生成：能够创造公司或产品标志，并将其覆盖在图像中。<br />
<br />
Imagen 2详细介绍：<a href="https://deepmind.google/technologies/imagen-2/">deepmind.google/technologies…</a><br />
<br />
Imagen 2 on Vertex AI：<a href="https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available">cloud.google.com/blog/produc…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDY0OTkwNjIwMTM5NTIvcHUvaW1nL21ubm9BQ0NjeURlUG1lY0guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735143546867569089#m</id>
            <title>Dolphins：一种新型的视觉-语言模型，旨在模拟人类驾驶能力

Dolphins能理解和处理视频、文字指令和驾驶信号。分析驾驶场景，比如识别城市交叉路口、夜间交通等。

它还能预测和规划车辆行为，比如为什么要慢行。

Dolphins还展现了类似人类的能力，即时学习和适应、通过上下文学习的反思和错误恢复。

该项目由来自威斯康星大学麦迪逊分校、NVIDIA、密歇根大学和斯坦福大学的研究人员共同参与。旨在让自动驾驶汽车更好地理解复杂的驾驶环境，类似于人类司机的理解能力。

独特功能:

Dolphins 具备全面理解复杂和长尾开放世界驾驶场景的能力，并能解决一系列 AV 任务。

能够处理包括场景理解、预测和规划在内的多种任务，感知静态和动态场景，整合环境因素，并有效处理下游预测和规划任务。

Dolphins 主要能做以下几件事：

1、理解复杂驾驶场景：Dolphins 能够分析和理解各种驾驶环境，比如城市交叉路口、夜间繁忙道路、隧道内行驶等。

2、处理多模态输入：能够处理包括视频（或图像）数据、文本指令和历史控制信号在内的多模态输入，以生成与提供的指令相对应的输出。

3、预测和规划行为：能够预测车辆在不同情况下的行为，如在交通拥堵时的低速行驶。它还能为车辆制定未来的行动计划，比如在遇到红灯时停车等待。

4、提供详细的场景描述：能够详细描述视频中的驾驶相关信息，如路况、交通状况、其他车辆和行人的行为等。

5、适应和学习：它具备快速学习和适应新场景的能力，能够根据实时情况做出反应和调整。Dolphins 展现了类似人类的能力，包括无梯度的即时适应、通过上下文学习的反思和错误恢复。

技术细节和原理：

1、基于 OpenFlamingo 构建：Dolphins 基于开源的预训练视觉-语言模型 OpenFlamingo 构建，并针对驾驶领域进行了特定的指令调整和数据构建。

2、BDD-X 数据集的应用：通过利用 BDD-X 数据集，Dolphins 融合了四种不同的自动驾驶车辆（AV）任务，以全面理解复杂的驾驶场景。

3、GCoT 过程：通过创新的 GCoT 过程，Dolphins 的推理能力得到增强。这个过程涉及构建图像指令跟随数据集，并基于公共的视觉问答（VQA）数据集、视觉指令数据集和 ChatGPT 来培养 OpenFlamingo 模型的细粒度推理能力。

4、驾驶领域的定制：Dolphins 通过构建特定于驾驶的指令数据和进行指令调整，被专门调整以适应驾驶领域。

项目及演示：https://vlm-driver.github.io/
论文：https://arxiv.org/abs/2312.00438
代码：https://github.com/vlm-driver/Dolphins</title>
            <link>https://nitter.cz/xiaohuggg/status/1735143546867569089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735143546867569089#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 03:43:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Dolphins：一种新型的视觉-语言模型，旨在模拟人类驾驶能力<br />
<br />
Dolphins能理解和处理视频、文字指令和驾驶信号。分析驾驶场景，比如识别城市交叉路口、夜间交通等。<br />
<br />
它还能预测和规划车辆行为，比如为什么要慢行。<br />
<br />
Dolphins还展现了类似人类的能力，即时学习和适应、通过上下文学习的反思和错误恢复。<br />
<br />
该项目由来自威斯康星大学麦迪逊分校、NVIDIA、密歇根大学和斯坦福大学的研究人员共同参与。旨在让自动驾驶汽车更好地理解复杂的驾驶环境，类似于人类司机的理解能力。<br />
<br />
独特功能:<br />
<br />
Dolphins 具备全面理解复杂和长尾开放世界驾驶场景的能力，并能解决一系列 AV 任务。<br />
<br />
能够处理包括场景理解、预测和规划在内的多种任务，感知静态和动态场景，整合环境因素，并有效处理下游预测和规划任务。<br />
<br />
Dolphins 主要能做以下几件事：<br />
<br />
1、理解复杂驾驶场景：Dolphins 能够分析和理解各种驾驶环境，比如城市交叉路口、夜间繁忙道路、隧道内行驶等。<br />
<br />
2、处理多模态输入：能够处理包括视频（或图像）数据、文本指令和历史控制信号在内的多模态输入，以生成与提供的指令相对应的输出。<br />
<br />
3、预测和规划行为：能够预测车辆在不同情况下的行为，如在交通拥堵时的低速行驶。它还能为车辆制定未来的行动计划，比如在遇到红灯时停车等待。<br />
<br />
4、提供详细的场景描述：能够详细描述视频中的驾驶相关信息，如路况、交通状况、其他车辆和行人的行为等。<br />
<br />
5、适应和学习：它具备快速学习和适应新场景的能力，能够根据实时情况做出反应和调整。Dolphins 展现了类似人类的能力，包括无梯度的即时适应、通过上下文学习的反思和错误恢复。<br />
<br />
技术细节和原理：<br />
<br />
1、基于 OpenFlamingo 构建：Dolphins 基于开源的预训练视觉-语言模型 OpenFlamingo 构建，并针对驾驶领域进行了特定的指令调整和数据构建。<br />
<br />
2、BDD-X 数据集的应用：通过利用 BDD-X 数据集，Dolphins 融合了四种不同的自动驾驶车辆（AV）任务，以全面理解复杂的驾驶场景。<br />
<br />
3、GCoT 过程：通过创新的 GCoT 过程，Dolphins 的推理能力得到增强。这个过程涉及构建图像指令跟随数据集，并基于公共的视觉问答（VQA）数据集、视觉指令数据集和 ChatGPT 来培养 OpenFlamingo 模型的细粒度推理能力。<br />
<br />
4、驾驶领域的定制：Dolphins 通过构建特定于驾驶的指令数据和进行指令调整，被专门调整以适应驾驶领域。<br />
<br />
项目及演示：<a href="https://vlm-driver.github.io/">vlm-driver.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.00438">arxiv.org/abs/2312.00438</a><br />
代码：<a href="https://github.com/vlm-driver/Dolphins">github.com/vlm-driver/Dolphi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxNDEzNTUxODg1OTI2NDAvcHUvaW1nL0RKUmZFN1BMdEhRcWVVYlouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735129903937560615#m</id>
            <title>这个挺好玩的 哈哈哈😂

用摄像头进行输入，以30+fps AI实时生图，把你变成另一个人...

可以使用提示词控制你想要生成什么图像或者模仿谁谁谁...

在电脑上尝试：http://fal.ai/camera</title>
            <link>https://nitter.cz/xiaohuggg/status/1735129903937560615#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735129903937560615#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 02:49:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺好玩的 哈哈哈😂<br />
<br />
用摄像头进行输入，以30+fps AI实时生图，把你变成另一个人...<br />
<br />
可以使用提示词控制你想要生成什么图像或者模仿谁谁谁...<br />
<br />
在电脑上尝试：<a href="http://fal.ai/camera">fal.ai/camera</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzUxMjc5MzI1MjY1ODM4MDgvcHUvaW1nL2poLUo4b1hIZXRFc1RyeUEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1735126281552044403#m</id>
            <title>ChatGPT Plus 恢复订阅了

搞到了更多GPU...

看来目前最大的阻碍依然是算力问题...</title>
            <link>https://nitter.cz/xiaohuggg/status/1735126281552044403#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1735126281552044403#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 02:35:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ChatGPT Plus 恢复订阅了<br />
<br />
搞到了更多GPU...<br />
<br />
看来目前最大的阻碍依然是算力问题...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JSbi1yR2JRQUFCNjdxLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734958068595831011#m</id>
            <title>Google宣布Gemini Pro 版本已经向开发者和企业开放，可用于构建 AI 应用。

最重要的是目前完全免费！🆓💰

• 免费使用：目前可以在限制内免费使用，并且未来将提供具有竞争力的定价。

• 特性：支持包括函数调用、嵌入、语义检索、自定义知识基础和聊天功能。

• 语言支持：支持全球 180 多个国家和地区的 38 种语言。

开发者目前可以通过 Google AI Studio 免费访问 Gemini Pro 和 Gemini Pro Vision，适用于大多数应用程序开发需求。

Google计划在明年初推出 Gemini Ultra…

详细：https://blog.google/technology/ai/gemini-api-developers-cloud/?utm_sourc</title>
            <link>https://nitter.cz/xiaohuggg/status/1734958068595831011#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734958068595831011#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 15:26:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google宣布Gemini Pro 版本已经向开发者和企业开放，可用于构建 AI 应用。<br />
<br />
最重要的是目前完全免费！🆓💰<br />
<br />
• 免费使用：目前可以在限制内免费使用，并且未来将提供具有竞争力的定价。<br />
<br />
• 特性：支持包括函数调用、嵌入、语义检索、自定义知识基础和聊天功能。<br />
<br />
• 语言支持：支持全球 180 多个国家和地区的 38 种语言。<br />
<br />
开发者目前可以通过 Google AI Studio 免费访问 Gemini Pro 和 Gemini Pro Vision，适用于大多数应用程序开发需求。<br />
<br />
Google计划在明年初推出 Gemini Ultra…<br />
<br />
详细：<a href="https://blog.google/technology/ai/gemini-api-developers-cloud/?utm_sourc">blog.google/technology/ai/ge…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM0OTU4MDAyNTg1ODY2MjQxL2ltZy9rV21zUHM5dlh2bDVNN2ZKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</id>
            <title>R to @xiaohuggg: 将Logo转换成一些真实场景图</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882384385110292#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将Logo转换成一些真实场景图</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIyNTM3OTExNzQ2NTYvcHUvaW1nLzZXTkFWMkkzSGNwOFBFZVEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</id>
            <title>R to @xiaohuggg: 图像放大增强演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882381759443163#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>图像放大增强演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIxMzA1NjI2MTMyNDgvcHUvaW1nL3Y5Zm9JSEpzem1DXzZSeU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882379439981033#m</id>
            <title>http://Krea.AI因为实时生图火爆全网，一直处于内测阶段，今天正式开放访问。

主要功能：

- 实时生成：根据提示和编辑器实时编辑创造完美的图像

- 放大增强：提升图像分辨率增强图像画质

- Logo Illusions：将Logo转换成一些真实场景中

- AI Patterns：创建一些类似中世纪螺旋AI图像</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882379439981033#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882379439981033#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="http://Krea.AI">Krea.AI</a>因为实时生图火爆全网，一直处于内测阶段，今天正式开放访问。<br />
<br />
主要功能：<br />
<br />
- 实时生成：根据提示和编辑器实时编辑创造完美的图像<br />
<br />
- 放大增强：提升图像分辨率增强图像画质<br />
<br />
- Logo Illusions：将Logo转换成一些真实场景中<br />
<br />
- AI Patterns：创建一些类似中世纪螺旋AI图像</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODE5NTIzOTI3MjQ0ODAvcHUvaW1nL0tmWkxTWmtkNU9MR2FPRWguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734873428174458986#m</id>
            <title>CopilotKit：一个开源项目，可以在任何基于React的Web应用中构建内嵌的AI聊天机器人和AI驱动的文本区域。

主要特点：

- 内嵌AI聊天机器人：机器人可以理解应用的当前状态，并在应用内执行操作。

- AI驱动的文本区域：提供AI生成和编辑文本的功能，可以替换任何标准的文本输入区域

- 自动上下文感知：根据上下文自动完成建议

- 全面的定制化：允许完全定制提示工程和UI设计

- 支持多种模型：可以与不同的前端和后端SDK结合，支持多种大语言模型。

使用场景：

文本生成和编辑：在应用中提供AI辅助的文本生成和编辑功能，例如自动完成和内容生成。

交互式聊天机器人：创建可以与应用前端和后端以及第三方服务交互的聊天机器人。

总之：CopilotKit 是一个为React开发者提供强大AI集成能力的工具集，通过简化AI功能的集成过程，使得创建交互式和智能的Web应用变得更加容易。

GitHub：https://github.com/CopilotKit/CopilotKit</title>
            <link>https://nitter.cz/xiaohuggg/status/1734873428174458986#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734873428174458986#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 09:50:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>CopilotKit：一个开源项目，可以在任何基于React的Web应用中构建内嵌的AI聊天机器人和AI驱动的文本区域。<br />
<br />
主要特点：<br />
<br />
- 内嵌AI聊天机器人：机器人可以理解应用的当前状态，并在应用内执行操作。<br />
<br />
- AI驱动的文本区域：提供AI生成和编辑文本的功能，可以替换任何标准的<textarea></textarea>文本输入区域<br />
<br />
- 自动上下文感知：根据上下文自动完成建议<br />
<br />
- 全面的定制化：允许完全定制提示工程和UI设计<br />
<br />
- 支持多种模型：可以与不同的前端和后端SDK结合，支持多种大语言模型。<br />
<br />
使用场景：<br />
<br />
文本生成和编辑：在应用中提供AI辅助的文本生成和编辑功能，例如自动完成和内容生成。<br />
<br />
交互式聊天机器人：创建可以与应用前端和后端以及第三方服务交互的聊天机器人。<br />
<br />
总之：CopilotKit 是一个为React开发者提供强大AI集成能力的工具集，通过简化AI功能的集成过程，使得创建交互式和智能的Web应用变得更加容易。<br />
<br />
GitHub：<a href="https://github.com/CopilotKit/CopilotKit">github.com/CopilotKit/Copilo…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MjAwNzk4NDQ2NzE0ODgvcHUvaW1nL3libkVkempJTlNmaWZ4R3MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</id>
            <title>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。

超越了不久刚发布的Gemini Ultra的90.04%😅

在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。

这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。

具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。

然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。

这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。

这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。

微软公布了其最新的研究成果和Medprompt+ 仓库。

详细内容：https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/

Medprompt+ 仓库：https://github.com/microsoft/promptbase</title>
            <link>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734811424348914156#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。<br />
<br />
超越了不久刚发布的Gemini Ultra的90.04%😅<br />
<br />
在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。<br />
<br />
这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。<br />
<br />
具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。<br />
<br />
然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。<br />
<br />
这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。<br />
<br />
这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。<br />
<br />
微软公布了其最新的研究成果和Medprompt+ 仓库。<br />
<br />
详细内容：<a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">microsoft.com/en-us/research…</a><br />
<br />
Medprompt+ 仓库：<a href="https://github.com/microsoft/promptbase">github.com/microsoft/promptb…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729862138796351499#m">nitter.cz/xiaohuggg/status/1729862138796351499#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734805452750528964#m</id>
            <title>R to @xiaohuggg: 测试演示效果</title>
            <link>https://nitter.cz/xiaohuggg/status/1734805452750528964#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734805452750528964#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:20:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试演示效果</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MDUzNDQ5NDAwNzcwNTYvcHUvaW1nLzRpUVZNMVl5TXdGRkEwQU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734805244280995935#m</id>
            <title>Shader App：第一个无代码增强现实（AR）创作工具

通过AI即时定制你的个性外观，你只需要输入文字提示、滑动操作或使用语音即可生成增强现实（AR）一样的个性外观。

也可以使用生成的 AR 效果录制视频，与他人分享。

测了下感觉是抖音玩剩下的，不过牛p的是可以只使用提示就能创建AR外观。

@shaderapp 目前处测试阶段，可以加入其 Discord 成为首批测试者。

Discord：http://discord.gg/shaderapp</title>
            <link>https://nitter.cz/xiaohuggg/status/1734805244280995935#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734805244280995935#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:19:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Shader App：第一个无代码增强现实（AR）创作工具<br />
<br />
通过AI即时定制你的个性外观，你只需要输入文字提示、滑动操作或使用语音即可生成增强现实（AR）一样的个性外观。<br />
<br />
也可以使用生成的 AR 效果录制视频，与他人分享。<br />
<br />
测了下感觉是抖音玩剩下的，不过牛p的是可以只使用提示就能创建AR外观。<br />
<br />
<a href="https://nitter.cz/shaderapp" title="Shader">@shaderapp</a> 目前处测试阶段，可以加入其 Discord 成为首批测试者。<br />
<br />
Discord：<a href="http://discord.gg/shaderapp">discord.gg/shaderapp</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1NjI1MzUzOTgxNDYwNDgvcHUvaW1nL2oyUWNfcVk3bWtsWkJ6RDYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734708814983602256#m</id>
            <title>RT by @xiaohuggg: 哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。

另外http://lmsys.org的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。

建议有空也可以上去测试评选一下：http://chat.lmsys.org</title>
            <link>https://nitter.cz/dotey/status/1734708814983602256#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734708814983602256#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 22:56:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。<br />
<br />
另外<a href="http://lmsys.org">lmsys.org</a>的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。<br />
<br />
建议有空也可以上去测试评选一下：<a href="http://chat.lmsys.org">chat.lmsys.org</a></p>
<p><a href="https://nitter.cz/lmsysorg/status/1734680611393073289#m">nitter.cz/lmsysorg/status/1734680611393073289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734780399275167756#m</id>
            <title>R to @xiaohuggg: 评分情况</title>
            <link>https://nitter.cz/xiaohuggg/status/1734780399275167756#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734780399275167756#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:40:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>评分情况</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdGZNNGJNQUFGMzI5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734778292157444479#m</id>
            <title>微软发布最新开发的小型语言模型 Phi-2

- Phi-2 仅有 2.7B参数

- Phi-2 超越了分别拥有7B和13B参数的Mistral和Llama-2模型

- 甚至在多步推理任务上超越了参数量是其25倍的Llama-2-70B模型

- 微软称Phi-2性能优异得益于其训练数据的质量非常高，他们弄了一个“教科书级”的数据集

“教科书级”的数据集：为了训练 Phi-2，研究团队创建了特定的数据集，这些数据集专门设计用来教授模型进行常识推理和理解一般知识。这些合成数据集可能包含各种情景和问题，旨在提高模型在处理现实世界问题时的准确性和可靠性。

知识迁移：另外研究团队还成功地将已经学习到的知识和模式从较小的Phi-1.5模型转移到了较大的Phi-2模型。这不仅提高了 Phi-2 的学习效率，还加速了其训练过程，使其能够更快地达到高水平的性能。

详细：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734778292157444479#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734778292157444479#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:32:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软发布最新开发的小型语言模型 Phi-2<br />
<br />
- Phi-2 仅有 2.7B参数<br />
<br />
- Phi-2 超越了分别拥有7B和13B参数的Mistral和Llama-2模型<br />
<br />
- 甚至在多步推理任务上超越了参数量是其25倍的Llama-2-70B模型<br />
<br />
- 微软称Phi-2性能优异得益于其训练数据的质量非常高，他们弄了一个“教科书级”的数据集<br />
<br />
“教科书级”的数据集：为了训练 Phi-2，研究团队创建了特定的数据集，这些数据集专门设计用来教授模型进行常识推理和理解一般知识。这些合成数据集可能包含各种情景和问题，旨在提高模型在处理现实世界问题时的准确性和可靠性。<br />
<br />
知识迁移：另外研究团队还成功地将已经学习到的知识和模式从较小的Phi-1.5模型转移到了较大的Phi-2模型。这不仅提高了 Phi-2 的学习效率，还加速了其训练过程，使其能够更快地达到高水平的性能。<br />
<br />
详细：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNcU4wSGIwQUFfamhULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>