<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737656196629471517#m</id>
            <title>PowerInfer：让普通电脑也能跑大语言模型

由上海交通大学开发，目的是在配备消费级GPU的个人电脑上提供高速的大语言模型推理服务。

PowerInfer 无缝整合了 CPU 和 GPU 的内存和计算能力，优化了内存和计算资源，从而在个人电脑上高效地运行复杂的 AI 模型。

比llama.cpp快11倍...

它支持多种不同的大型语言模型！

在测试中，PowerInfer在单个 NVIDIA RTX 4090 GPU 上达到了平均每秒生成 13.20 个令牌的速率，峰值可达 29.08 个令牌。接近顶级服务器级 GPU 的性能。

PowerInfer 对比llama.cpp 在运行 Falcon(ReLU)-40B-FP16 的单个 RTX 4090(24G) 上实现 11 倍加速！

其主要工作原理：

通过智能地分配和优化计算任务在 CPU 和 GPU 之间的处理，以及利用大型语言模型中的局部性特征，从而在个人电脑上高效地运行复杂的 AI 模型。这种方法使得即使是不具备高端服务器硬件的用户也能体验到高速的 AI 模型推理性能。

激活局部性利用：PowerInfer 利用了大语言模型推理中的高局部性。大语言模型在各种输入中，只有一小部分神经元（称为“热神经元”）持续激活，而大多数神经元（“冷神经元”）则根据特定输入变化。

GPU-CPU 混合推理：为了提高效率，PowerInfer 预先将热神经元加载到 GPU 上，以实现快速访问。这减少了 GPU 的内存需求。同时，它在 CPU 上计算冷神经元的激活，减少了 CPU 和 GPU 之间的数据传输。

GitHub：https://github.com/SJTU-IPADS/PowerInfer
论文：https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1737656196629471517#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737656196629471517#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 02:08:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PowerInfer：让普通电脑也能跑大语言模型<br />
<br />
由上海交通大学开发，目的是在配备消费级GPU的个人电脑上提供高速的大语言模型推理服务。<br />
<br />
PowerInfer 无缝整合了 CPU 和 GPU 的内存和计算能力，优化了内存和计算资源，从而在个人电脑上高效地运行复杂的 AI 模型。<br />
<br />
比llama.cpp快11倍...<br />
<br />
它支持多种不同的大型语言模型！<br />
<br />
在测试中，PowerInfer在单个 NVIDIA RTX 4090 GPU 上达到了平均每秒生成 13.20 个令牌的速率，峰值可达 29.08 个令牌。接近顶级服务器级 GPU 的性能。<br />
<br />
PowerInfer 对比llama.cpp 在运行 Falcon(ReLU)-40B-FP16 的单个 RTX 4090(24G) 上实现 11 倍加速！<br />
<br />
其主要工作原理：<br />
<br />
通过智能地分配和优化计算任务在 CPU 和 GPU 之间的处理，以及利用大型语言模型中的局部性特征，从而在个人电脑上高效地运行复杂的 AI 模型。这种方法使得即使是不具备高端服务器硬件的用户也能体验到高速的 AI 模型推理性能。<br />
<br />
激活局部性利用：PowerInfer 利用了大语言模型推理中的高局部性。大语言模型在各种输入中，只有一小部分神经元（称为“热神经元”）持续激活，而大多数神经元（“冷神经元”）则根据特定输入变化。<br />
<br />
GPU-CPU 混合推理：为了提高效率，PowerInfer 预先将热神经元加载到 GPU 上，以实现快速访问。这减少了 GPU 的内存需求。同时，它在 CPU 上计算冷神经元的激活，减少了 CPU 和 GPU 之间的数据传输。<br />
<br />
GitHub：<a href="https://github.com/SJTU-IPADS/PowerInfer">github.com/SJTU-IPADS/PowerI…</a><br />
论文：<a href="https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf">ipads.se.sjtu.edu.cn/_media/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NTQwMDQ5MTY4ODM0NTYvcHUvaW1nL1M0SG54WE9tZFNhNmo1eWkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647074110570574#m</id>
            <title>R to @xiaohuggg: 之前的介绍</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647074110570574#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647074110570574#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前的介绍</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1734105617982456270#m">nitter.cz/xiaohuggg/status/1734105617982456270#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647071556194697#m</id>
            <title>R to @xiaohuggg: 测试视频，我以为会给我生成4个选，结果就给了一个

嘿嘿

生成大概需要6-8分钟</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647071556194697#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647071556194697#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试视频，我以为会给我生成4个选，结果就给了一个<br />
<br />
嘿嘿<br />
<br />
生成大概需要6-8分钟</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NDY3Mzk0MTc3MTA1OTIvcHUvaW1nL2VEeEFpNkpDU0NLN1VBakkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647069815587299#m</id>
            <title>R to @xiaohuggg: 官网演示集锦</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647069815587299#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647069815587299#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>官网演示集锦</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NDY2MTMzMzI2NDc5MzYvcHUvaW1nL2ExMzhVUHBHTTlMUHN2UmIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737647067693211728#m</id>
            <title>阿里巴巴的 DreaMoving 放出在线体验地址了

DreaMoving能仅靠脸部照片和文字提示就能生成在任何场景下跳舞的视频...

测了下跳舞动作还可以，但是和背景融合度不行，人物舞蹈和背景完全是隔离的，不能完全融合！

体验地址：https://www.modelscope.cn/studios/vigen/video_generation/summary

这是官方演示视频（音乐我加的），测试在三楼↓</title>
            <link>https://nitter.cz/xiaohuggg/status/1737647067693211728#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737647067693211728#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Dec 2023 01:31:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里巴巴的 DreaMoving 放出在线体验地址了<br />
<br />
DreaMoving能仅靠脸部照片和文字提示就能生成在任何场景下跳舞的视频...<br />
<br />
测了下跳舞动作还可以，但是和背景融合度不行，人物舞蹈和背景完全是隔离的，不能完全融合！<br />
<br />
体验地址：<a href="https://www.modelscope.cn/studios/vigen/video_generation/summary">modelscope.cn/studios/vigen/…</a><br />
<br />
这是官方演示视频（音乐我加的），测试在三楼↓</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzc2NDY1MzMyNjM0NzA1OTIvcHUvaW1nL0d5MmZ2b0xQNTc1SUFaWnUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737501193058877916#m</id>
            <title>有木有类似：http://remoteok.com

这种招聘网站的开源程序

想搭建个帮独立开发者招聘远程办公人才🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1737501193058877916#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737501193058877916#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 15:52:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有木有类似：<a href="http://remoteok.com">remoteok.com</a><br />
<br />
这种招聘网站的开源程序<br />
<br />
想搭建个帮独立开发者招聘远程办公人才🤔</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737474857124671732#m</id>
            <title>R to @xiaohuggg: ComfyUI Portrait Master 2.2 版本发布

新增了一个姿势库。

提供了与 Portrait Master 兼容的工作流程文件，包括对姿势的管理和控制。

集成了放大器和两个 ControlNet 以管理角色的姿势。

GitHub：https://github.com/florestefano1975/comfyui-portrait-master/</title>
            <link>https://nitter.cz/xiaohuggg/status/1737474857124671732#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737474857124671732#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 14:07:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI Portrait Master 2.2 版本发布<br />
<br />
新增了一个姿势库。<br />
<br />
提供了与 Portrait Master 兼容的工作流程文件，包括对姿势的管理和控制。<br />
<br />
集成了放大器和两个 ControlNet 以管理角色的姿势。<br />
<br />
GitHub：<a href="https://github.com/florestefano1975/comfyui-portrait-master/">github.com/florestefano1975/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5LTlvb1cwQUFnRDVOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</id>
            <title>XHS-Downloader：小红书采集器 

✅ 采集小红书图文/视频作品信息
✅ 提取小红书图文/视频作品下载地址
✅ 下载小红书无水印图文/视频作品文件
✅ 自动跳过已下载的作品文件
✅ 作品文件完整性处理机制
✅ 持久化储存作品信息至文件

GitHub：https://github.com/JoeanAmier/XHS-Downloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1737460981331288421#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737460981331288421#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 13:12:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XHS-Downloader：小红书采集器 <br />
<br />
✅ 采集小红书图文/视频作品信息<br />
✅ 提取小红书图文/视频作品下载地址<br />
✅ 下载小红书无水印图文/视频作品文件<br />
✅ 自动跳过已下载的作品文件<br />
✅ 作品文件完整性处理机制<br />
✅ 持久化储存作品信息至文件<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/XHS-Downloader">github.com/JoeanAmier/XHS-Do…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0J5elhIR2JVQUFjb2hQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737442315319603200#m</id>
            <title>OpenAI董事会将拥有是否发布新AI大模型的决定权以及推翻OpenAI领导团队决定的权利。

OpenAI 本周发布了一份长达 27 页的框架文件，制定了如何防范 AI 大模型「灾难性风险」的路线图和安全等级。https://openai.com/safety/preparedness

新治理框架对AI大模型的发布进行了制衡，OpenAI表示，公司领导层拥有是否发布新AI模型的决策权，但董事会拥有最终发布的决定权，以及“推翻OpenAI领导团队决定的权利”。

OpenAI还表示，即便董事会拥有否决部署有潜在风险的人工智能模型的权利，OpenAI仍然会事先通过安全检查来确保这些人工智能大模型的部署是安全的。

安全等级：

为此，OpenAI将会专门建立一个“准备”团队（preparedness team），由麻省理工学院（MIT）教授Aleksander Madry领导，以监控和减轻OpenAI人工智能模型的潜在风险。该团队将负责评估并密切监控AI模型的潜在风险，并将这些不同的风险进行评分，将风险分类为“低”、“中”、“高”或“严重”。

OpenAI的治理框架也指出：“只有在降低风险后评分为‘中’或以下的AI模型才能部署，并且只有在降低风险后得分为‘高’或以下的模型才能进一步开发。”

该公司表示，目前这份监管框架性的文件仍处于“测试版”阶段，预计将根据反馈定期更新。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737442315319603200#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737442315319603200#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 11:58:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI董事会将拥有是否发布新AI大模型的决定权以及推翻OpenAI领导团队决定的权利。<br />
<br />
OpenAI 本周发布了一份长达 27 页的框架文件，制定了如何防范 AI 大模型「灾难性风险」的路线图和安全等级。<a href="https://openai.com/safety/preparedness">openai.com/safety/preparedne…</a><br />
<br />
新治理框架对AI大模型的发布进行了制衡，OpenAI表示，公司领导层拥有是否发布新AI模型的决策权，但董事会拥有最终发布的决定权，以及“推翻OpenAI领导团队决定的权利”。<br />
<br />
OpenAI还表示，即便董事会拥有否决部署有潜在风险的人工智能模型的权利，OpenAI仍然会事先通过安全检查来确保这些人工智能大模型的部署是安全的。<br />
<br />
安全等级：<br />
<br />
为此，OpenAI将会专门建立一个“准备”团队（preparedness team），由麻省理工学院（MIT）教授Aleksander Madry领导，以监控和减轻OpenAI人工智能模型的潜在风险。该团队将负责评估并密切监控AI模型的潜在风险，并将这些不同的风险进行评分，将风险分类为“低”、“中”、“高”或“严重”。<br />
<br />
OpenAI的治理框架也指出：“只有在降低风险后评分为‘中’或以下的AI模型才能部署，并且只有在降低风险后得分为‘高’或以下的模型才能进一步开发。”<br />
<br />
该公司表示，目前这份监管框架性的文件仍处于“测试版”阶段，预计将根据反馈定期更新。</p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczNDQzODgyOTcyODA5NjI1Ny9Id1JrZmQ4az9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737426889822929238#m</id>
            <title>还说国内这帮人会玩、会商业化

Google这个模型其实和这个思路差不多，要是能实现这个视频说的功能就牛p了！

其实昨天Runway发布的语音功能也是为了想实现旁白+视频的故事模式。

主要是目前的生图和视频几乎都是抽卡一样，不知道这家是怎么能保证稳定和符合预期的输出！😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1737426889822929238#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737426889822929238#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 10:57:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还说国内这帮人会玩、会商业化<br />
<br />
Google这个模型其实和这个思路差不多，要是能实现这个视频说的功能就牛p了！<br />
<br />
其实昨天Runway发布的语音功能也是为了想实现旁白+视频的故事模式。<br />
<br />
主要是目前的生图和视频几乎都是抽卡一样，不知道这家是怎么能保证稳定和符合预期的输出！😐</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1737371348467618039#m">nitter.cz/xiaohuggg/status/1737371348467618039#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM3NDI2NzQ1MjExNjUwMDQ4L2ltZy8ya2EtRmpYTlJHbTFoX3h1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737383561303564487#m</id>
            <title>R to @xiaohuggg: 另外还有个 SVD 结合AnimateDiff Refiner的教程

避免动画闪烁

https://www.patreon.com/posts/ai-svd-with-more-93812677</title>
            <link>https://nitter.cz/xiaohuggg/status/1737383561303564487#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737383561303564487#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:04:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另外还有个 SVD 结合AnimateDiff Refiner的教程<br />
<br />
避免动画闪烁<br />
<br />
<a href="https://www.patreon.com/posts/ai-svd-with-more-93812677">patreon.com/posts/ai-svd-wit…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODMyMDEwMzgwMzY5OTIvcHUvaW1nL1ZuN3diZU5wQlVVb3ZxQnIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737383138882670747#m</id>
            <title>R to @xiaohuggg: AnimateDiff - Hip Hop Girl演示

https://www.patreon.com/posts/ai-animatediff-93266466</title>
            <link>https://nitter.cz/xiaohuggg/status/1737383138882670747#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737383138882670747#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:03:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnimateDiff - Hip Hop Girl演示<br />
<br />
<a href="https://www.patreon.com/posts/ai-animatediff-93266466">patreon.com/posts/ai-animate…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODMwMDI4MTk0MzI0NDgvcHUvaW1nL1B3ZHAzVlhOcjN5Z1JiVDkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737382923983299001#m</id>
            <title>R to @xiaohuggg: 抽象类型的动画演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1737382923983299001#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737382923983299001#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:02:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>抽象类型的动画演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODI2OTcxNzYzNTA3MjAvcHUvaW1nL291OGEtNlFGb1p0N0F0aUUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737382606541627582#m</id>
            <title>R to @xiaohuggg: 一些演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1737382606541627582#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737382606541627582#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:01:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一些演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczODI1NzQzNDYxNzAzNjgvcHUvaW1nL2VEVkMyYUFUNDRnRXBvOTkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737382498332815734#m</id>
            <title>这个视频的制作教程和workflow找到了

由于太过于技术，我看不太懂

反正就是应用了 LCM，渲染图像只需要10个采样步骤，大约减半了内存处理需求。这意味着你可以在批处理范围内输入更多的图像，同时还可以在制作原始通道时提高分辨率。

喜欢AI动画的SD大佬们可以研究下

工作流程分为五个部分：

ControlNet Passes Export：控制网络导出。
Animation Raw - LCM：原始动画 - LCM。
AnimateDiff Refiner - LCM：动画差异细化 - LCM。
AnimateDiff Face Fix - LCM：动画差异面部修复 - LCM。
Batch Face Swap - ReActor [实验性]：批量面部交换 - ReActor。

这种分解方式旨在提高处理效率，同时对内存和用户友好。

详细教程：https://www.patreon.com/posts/update-animate-94523632

视频教程：https://youtu.be/HbfDjAMFi6w</title>
            <link>https://nitter.cz/xiaohuggg/status/1737382498332815734#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737382498332815734#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 08:00:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个视频的制作教程和workflow找到了<br />
<br />
由于太过于技术，我看不太懂<br />
<br />
反正就是应用了 LCM，渲染图像只需要10个采样步骤，大约减半了内存处理需求。这意味着你可以在批处理范围内输入更多的图像，同时还可以在制作原始通道时提高分辨率。<br />
<br />
喜欢AI动画的SD大佬们可以研究下<br />
<br />
工作流程分为五个部分：<br />
<br />
ControlNet Passes Export：控制网络导出。<br />
Animation Raw - LCM：原始动画 - LCM。<br />
AnimateDiff Refiner - LCM：动画差异细化 - LCM。<br />
AnimateDiff Face Fix - LCM：动画差异面部修复 - LCM。<br />
Batch Face Swap - ReActor [实验性]：批量面部交换 - ReActor。<br />
<br />
这种分解方式旨在提高处理效率，同时对内存和用户友好。<br />
<br />
详细教程：<a href="https://www.patreon.com/posts/update-animate-94523632">patreon.com/posts/update-ani…</a><br />
<br />
视频教程：<a href="https://youtu.be/HbfDjAMFi6w">youtu.be/HbfDjAMFi6w</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNzYxNDQwNTAzMTExNjgvcHUvaW1nL1lKT0tUVmRsNDN5SlBFdFIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737371651313193099#m</id>
            <title>R to @xiaohuggg: 演示视频：

VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。

为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。

这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。

这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。</title>
            <link>https://nitter.cz/xiaohuggg/status/1737371651313193099#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737371651313193099#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 07:17:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频：<br />
<br />
VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。<br />
<br />
为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。<br />
<br />
这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。<br />
<br />
这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNzE1OTI0MDgzNDY2MjQvcHUvaW1nL2lmRFVva0p6blZJX29tazguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737371534074016149#m</id>
            <title>R to @xiaohuggg: 根据视频生成音频</title>
            <link>https://nitter.cz/xiaohuggg/status/1737371534074016149#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737371534074016149#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 07:17:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据视频生成音频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNzE0NzM4NjA0OTc0MDgvcHUvaW1nL2RMelhCRkNQSmIwZ2RyQm4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1737371348467618039#m</id>
            <title>Google的一个新的视频模型：VideoPoet  

它可以根据文字描述来生成视频。但它不是基于扩散模型，而本身就是个LLM，可以理解和处理多模态信息，并将它们融合到视频生成过程中。  

不仅能生成视频，还能给视频加上风格化的效果，还可修复和扩展视频，甚至从视频中生成音频。  

一条龙服务...

例如，VideoPoet 可以根据文本描述生成视频，或者将一张静态图片转换成动态视频。它还能理解和生成音频，甚至是编写用于视频处理的代码。

这种多模态学习能力使得 VideoPoet 在视频生成方面更加灵活和强大，能够处理更复杂和多样化的任务。

演示视频：

VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。

为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。

这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。

这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。

VideoPoet 的主要功能特点：

1、广泛的视频生成任务：VideoPoet 能够处理多种视频生成任务，包括文本到视频、图像到视频、视频风格化、视频修复和扩展、以及视频到音频。

2、多模态学习能力：与主要基于扩散的视频生成模型不同，VideoPoet 作为一个大型语言模型，在多种模态上展现出卓越的学习能力，包括语言、代码和音频。

3、集成多种视频生成能力：VideoPoet 在单一的大型语言模型中集成了多种视频生成能力，而不是依赖于针对每项任务单独训练的组件。

5、任务设计：VideoPoet 能够根据不同的任务需求（如文本到视频、图像到视频等）调整其生成过程。每种任务类型都由特定的任务标记指示，以引导模型进行相应的视频生成。

6、长视频生成：通过连续预测的方式，VideoPoet 能够生成更长的视频。它通过在每一步中仅考虑视频的最后一部分（例如最后1秒），然后预测接下来的内容，从而实现视频的延伸。

7.、交互式视频编辑：允许用户交互式地编辑视频，例如改变视频中对象的动作或行为。这是通过在输入视频的基础上添加新的文本提示来实现的。

8、图像到视频的控制：能够根据文本提示将输入图像动画化，编辑其内容。

9、相机运动控制：通过在文本提示中添加特定的相机运动描述（如缩放、平移、弧形拍摄等）它能够在生成的视频中实现这些相机运动。

工作原理：

VideoPoet基于大语言模型（LLM），结合了多模态学习和自回归模型。

VideoPoet 使用大语言模型（LLM）用于处理和生成文本，但经过训练，也能理解和生成视频和音频。

结合了多模态学习，VideoPoet 能处理多种类型的输入和输出（如文本、图像、视频和音频），它可以将不同类型的信息（如文本描述和图像内容）结合起来，创造出新的视频内容。

自回归模型：它在生成视频的每一步都依赖于之前的步骤。这样，它可以逐渐构建起整个视频，确保视频内容的连贯性和一致性。

视频和音频的编码与解码：为了处理视频和音频，VideoPoet 使用特殊的编码器（如 MAGVIT V2 和 SoundStream）和解码器将这些内容转换为模型能理解的格式，然后再将生成的内容转换回可视或可听的格式。

详细介绍：https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html
演示：https://sites.research.google/videopoet/</title>
            <link>https://nitter.cz/xiaohuggg/status/1737371348467618039#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1737371348467618039#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 20 Dec 2023 07:16:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google的一个新的视频模型：VideoPoet  <br />
<br />
它可以根据文字描述来生成视频。但它不是基于扩散模型，而本身就是个LLM，可以理解和处理多模态信息，并将它们融合到视频生成过程中。  <br />
<br />
不仅能生成视频，还能给视频加上风格化的效果，还可修复和扩展视频，甚至从视频中生成音频。  <br />
<br />
一条龙服务...<br />
<br />
例如，VideoPoet 可以根据文本描述生成视频，或者将一张静态图片转换成动态视频。它还能理解和生成音频，甚至是编写用于视频处理的代码。<br />
<br />
这种多模态学习能力使得 VideoPoet 在视频生成方面更加灵活和强大，能够处理更复杂和多样化的任务。<br />
<br />
演示视频：<br />
<br />
VideoPoet 模型默认生成的是竖屏方向的视频，这主要是为了适应短视频内容的需求。为了展示其能力，Google Research 团队制作了一部由 VideoPoet 生成的短片，内容是由许多短片段组成的。<br />
<br />
为了制作这部短片，团队首先使用 Bard 编写了一个关于一只旅行的浣熊的短故事。Bard 不仅提供了故事的场景分解，还列出了伴随每个场景的提示。这些提示被用来指导 VideoPoet 生成与故事相匹配的视频片段。<br />
<br />
这个过程展示了 VideoPoet 在视频内容创作方面的多样性和创造力。通过结合不同的技术和工具，如 Bard 的故事创作能力和 VideoPoet 的视频生成能力，可以创造出富有想象力和吸引力的视觉内容。<br />
<br />
这种方法为视频制作和故事叙述提供了新的可能性，尤其适合制作短视频和社交媒体内容。<br />
<br />
VideoPoet 的主要功能特点：<br />
<br />
1、广泛的视频生成任务：VideoPoet 能够处理多种视频生成任务，包括文本到视频、图像到视频、视频风格化、视频修复和扩展、以及视频到音频。<br />
<br />
2、多模态学习能力：与主要基于扩散的视频生成模型不同，VideoPoet 作为一个大型语言模型，在多种模态上展现出卓越的学习能力，包括语言、代码和音频。<br />
<br />
3、集成多种视频生成能力：VideoPoet 在单一的大型语言模型中集成了多种视频生成能力，而不是依赖于针对每项任务单独训练的组件。<br />
<br />
5、任务设计：VideoPoet 能够根据不同的任务需求（如文本到视频、图像到视频等）调整其生成过程。每种任务类型都由特定的任务标记指示，以引导模型进行相应的视频生成。<br />
<br />
6、长视频生成：通过连续预测的方式，VideoPoet 能够生成更长的视频。它通过在每一步中仅考虑视频的最后一部分（例如最后1秒），然后预测接下来的内容，从而实现视频的延伸。<br />
<br />
7.、交互式视频编辑：允许用户交互式地编辑视频，例如改变视频中对象的动作或行为。这是通过在输入视频的基础上添加新的文本提示来实现的。<br />
<br />
8、图像到视频的控制：能够根据文本提示将输入图像动画化，编辑其内容。<br />
<br />
9、相机运动控制：通过在文本提示中添加特定的相机运动描述（如缩放、平移、弧形拍摄等）它能够在生成的视频中实现这些相机运动。<br />
<br />
工作原理：<br />
<br />
VideoPoet基于大语言模型（LLM），结合了多模态学习和自回归模型。<br />
<br />
VideoPoet 使用大语言模型（LLM）用于处理和生成文本，但经过训练，也能理解和生成视频和音频。<br />
<br />
结合了多模态学习，VideoPoet 能处理多种类型的输入和输出（如文本、图像、视频和音频），它可以将不同类型的信息（如文本描述和图像内容）结合起来，创造出新的视频内容。<br />
<br />
自回归模型：它在生成视频的每一步都依赖于之前的步骤。这样，它可以逐渐构建起整个视频，确保视频内容的连贯性和一致性。<br />
<br />
视频和音频的编码与解码：为了处理视频和音频，VideoPoet 使用特殊的编码器（如 MAGVIT V2 和 SoundStream）和解码器将这些内容转换为模型能理解的格式，然后再将生成的内容转换回可视或可听的格式。<br />
<br />
详细介绍：<a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html">blog.research.google/2023/12…</a><br />
演示：<a href="https://sites.research.google/videopoet/">sites.research.google/videop…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzczNjk5NTU5MzM1ODk1MDQvcHUvaW1nL3BoRzRKQWlCVTltYkNzVWouanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>