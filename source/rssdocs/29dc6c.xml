<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</id>
            <title>R to @xiaohuggg: 将Logo转换成一些真实场景图</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882384385110292#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882384385110292#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将Logo转换成一些真实场景图</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIyNTM3OTExNzQ2NTYvcHUvaW1nLzZXTkFWMkkzSGNwOFBFZVEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</id>
            <title>R to @xiaohuggg: 图像放大增强演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882381759443163#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882381759443163#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>图像放大增强演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODIxMzA1NjI2MTMyNDgvcHUvaW1nL3Y5Zm9JSEpzem1DXzZSeU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734882379439981033#m</id>
            <title>http://Krea.AI因为实时生图火爆全网，一直处于内测阶段，今天正式开放访问。

主要功能：

- 实时生成：根据提示和编辑器实时编辑创造完美的图像

- 放大增强：提升图像分辨率增强图像画质

- Logo Illusions：将Logo转换成一些真实场景中

- AI Patterns：创建一些类似中世纪螺旋AI图像</title>
            <link>https://nitter.cz/xiaohuggg/status/1734882379439981033#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734882379439981033#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:26:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="http://Krea.AI">Krea.AI</a>因为实时生图火爆全网，一直处于内测阶段，今天正式开放访问。<br />
<br />
主要功能：<br />
<br />
- 实时生成：根据提示和编辑器实时编辑创造完美的图像<br />
<br />
- 放大增强：提升图像分辨率增强图像画质<br />
<br />
- Logo Illusions：将Logo转换成一些真实场景中<br />
<br />
- AI Patterns：创建一些类似中世纪螺旋AI图像</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4ODE5NTIzOTI3MjQ0ODAvcHUvaW1nL0tmWkxTWmtkNU9MR2FPRWguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734873428174458986#m</id>
            <title>CopilotKit：一个开源项目，可以在任何基于React的Web应用中构建内嵌的AI聊天机器人和AI驱动的文本区域。

主要特点：

- 内嵌AI聊天机器人：机器人可以理解应用的当前状态，并在应用内执行操作。

- AI驱动的文本区域：提供AI生成和编辑文本的功能，可以替换任何标准的文本输入区域

- 自动上下文感知：根据上下文自动完成建议

- 全面的定制化：允许完全定制提示工程和UI设计

- 支持多种模型：可以与不同的前端和后端SDK结合，支持多种大语言模型。

使用场景：

文本生成和编辑：在应用中提供AI辅助的文本生成和编辑功能，例如自动完成和内容生成。

交互式聊天机器人：创建可以与应用前端和后端以及第三方服务交互的聊天机器人。

总之：CopilotKit 是一个为React开发者提供强大AI集成能力的工具集，通过简化AI功能的集成过程，使得创建交互式和智能的Web应用变得更加容易。

GitHub：https://github.com/CopilotKit/CopilotKit</title>
            <link>https://nitter.cz/xiaohuggg/status/1734873428174458986#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734873428174458986#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 09:50:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>CopilotKit：一个开源项目，可以在任何基于React的Web应用中构建内嵌的AI聊天机器人和AI驱动的文本区域。<br />
<br />
主要特点：<br />
<br />
- 内嵌AI聊天机器人：机器人可以理解应用的当前状态，并在应用内执行操作。<br />
<br />
- AI驱动的文本区域：提供AI生成和编辑文本的功能，可以替换任何标准的<textarea></textarea>文本输入区域<br />
<br />
- 自动上下文感知：根据上下文自动完成建议<br />
<br />
- 全面的定制化：允许完全定制提示工程和UI设计<br />
<br />
- 支持多种模型：可以与不同的前端和后端SDK结合，支持多种大语言模型。<br />
<br />
使用场景：<br />
<br />
文本生成和编辑：在应用中提供AI辅助的文本生成和编辑功能，例如自动完成和内容生成。<br />
<br />
交互式聊天机器人：创建可以与应用前端和后端以及第三方服务交互的聊天机器人。<br />
<br />
总之：CopilotKit 是一个为React开发者提供强大AI集成能力的工具集，通过简化AI功能的集成过程，使得创建交互式和智能的Web应用变得更加容易。<br />
<br />
GitHub：<a href="https://github.com/CopilotKit/CopilotKit">github.com/CopilotKit/Copilo…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MjAwNzk4NDQ2NzE0ODgvcHUvaW1nL3libkVkempJTlNmaWZ4R3MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</id>
            <title>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。

超越了不久刚发布的Gemini Ultra的90.04%😅

在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。

这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。

具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。

然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。

这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。

这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。

微软公布了其最新的研究成果和Medprompt+ 仓库。

详细内容：https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/

Medprompt+ 仓库：https://github.com/microsoft/promptbase</title>
            <link>https://nitter.cz/xiaohuggg/status/1734811424348914156#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734811424348914156#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:44:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软研究团队改进了之前的Medprompt提示策略，使GPT-4在MMLU上的表现达到了90.10%，这是迄今为止GPT-4在该测试上取得的最高分数。<br />
<br />
超越了不久刚发布的Gemini Ultra的90.04%😅<br />
<br />
在微软研究团队开发的Medprompt+策略中，GPT-4模型使用一种特定的策略来决定最终的答案。<br />
<br />
这个策略考虑了模型对不同候选答案的置信度，也就是模型认为每个答案正确的可能性。<br />
<br />
具体来说，当GPT-4使用Medprompt+策略回答问题时，它不仅生成答案，还评估每个答案的置信度。这个置信度是基于模型内部计算的，反映了模型对自己给出的答案有多确信。<br />
<br />
然后，GPT-4根据这些置信度来选择最终答案。如果模型对某个答案的置信度很高，那么这个答案就更有可能被选为最终答案。<br />
<br />
这种方法使得GPT-4在回答问题时更加精确，因为它不仅仅是随机选择答案，而是基于对每个可能答案的置信度来做出更加有根据的选择。<br />
<br />
这表明，通过系统化的提示工程和策略创新，可以显著提高大型语言模型在复杂任务上的性能。<br />
<br />
微软公布了其最新的研究成果和Medprompt+ 仓库。<br />
<br />
详细内容：<a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/">microsoft.com/en-us/research…</a><br />
<br />
Medprompt+ 仓库：<a href="https://github.com/microsoft/promptbase">github.com/microsoft/promptb…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1729862138796351499#m">nitter.cz/xiaohuggg/status/1729862138796351499#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734805452750528964#m</id>
            <title>R to @xiaohuggg: 测试演示效果</title>
            <link>https://nitter.cz/xiaohuggg/status/1734805452750528964#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734805452750528964#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:20:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试演示效果</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ4MDUzNDQ5NDAwNzcwNTYvcHUvaW1nLzRpUVZNMVl5TXdGRkEwQU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734805244280995935#m</id>
            <title>Shader App：第一个无代码增强现实（AR）创作工具

通过AI即时定制你的个性外观，你只需要输入文字提示、滑动操作或使用语音即可生成增强现实（AR）一样的个性外观。

也可以使用生成的 AR 效果录制视频，与他人分享。

测了下感觉是抖音玩剩下的，不过牛p的是可以只使用提示就能创建AR外观。

@shaderapp 目前处测试阶段，可以加入其 Discord 成为首批测试者。

Discord：http://discord.gg/shaderapp</title>
            <link>https://nitter.cz/xiaohuggg/status/1734805244280995935#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734805244280995935#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 05:19:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Shader App：第一个无代码增强现实（AR）创作工具<br />
<br />
通过AI即时定制你的个性外观，你只需要输入文字提示、滑动操作或使用语音即可生成增强现实（AR）一样的个性外观。<br />
<br />
也可以使用生成的 AR 效果录制视频，与他人分享。<br />
<br />
测了下感觉是抖音玩剩下的，不过牛p的是可以只使用提示就能创建AR外观。<br />
<br />
<a href="https://nitter.cz/shaderapp" title="Shader">@shaderapp</a> 目前处测试阶段，可以加入其 Discord 成为首批测试者。<br />
<br />
Discord：<a href="http://discord.gg/shaderapp">discord.gg/shaderapp</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1NjI1MzUzOTgxNDYwNDgvcHUvaW1nL2oyUWNfcVk3bWtsWkJ6RDYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1734708814983602256#m</id>
            <title>RT by @xiaohuggg: 哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。

另外http://lmsys.org的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。

建议有空也可以上去测试评选一下：http://chat.lmsys.org</title>
            <link>https://nitter.cz/dotey/status/1734708814983602256#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1734708814983602256#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 22:56:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>哇塞，Mixtral-8x7b 已经成为排名第一的开源模型。<br />
<br />
另外<a href="http://lmsys.org">lmsys.org</a>的数据是非常靠谱的，因为它完全是用户上去评分，用户输入一个问题，会随机有两个模型给你回答，用户根据回复的结果选择一个结果最好的模型，在打分之前用户完全不知道是哪个模型。<br />
<br />
建议有空也可以上去测试评选一下：<a href="http://chat.lmsys.org">chat.lmsys.org</a></p>
<p><a href="https://nitter.cz/lmsysorg/status/1734680611393073289#m">nitter.cz/lmsysorg/status/1734680611393073289#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734780399275167756#m</id>
            <title>R to @xiaohuggg: 评分情况</title>
            <link>https://nitter.cz/xiaohuggg/status/1734780399275167756#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734780399275167756#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:40:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>评分情况</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNdGZNNGJNQUFGMzI5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734778292157444479#m</id>
            <title>微软发布最新开发的小型语言模型 Phi-2

- Phi-2 仅有 2.7B参数

- Phi-2 超越了分别拥有7B和13B参数的Mistral和Llama-2模型

- 甚至在多步推理任务上超越了参数量是其25倍的Llama-2-70B模型

- 微软称Phi-2性能优异得益于其训练数据的质量非常高，他们弄了一个“教科书级”的数据集

“教科书级”的数据集：为了训练 Phi-2，研究团队创建了特定的数据集，这些数据集专门设计用来教授模型进行常识推理和理解一般知识。这些合成数据集可能包含各种情景和问题，旨在提高模型在处理现实世界问题时的准确性和可靠性。

知识迁移：另外研究团队还成功地将已经学习到的知识和模式从较小的Phi-1.5模型转移到了较大的Phi-2模型。这不仅提高了 Phi-2 的学习效率，还加速了其训练过程，使其能够更快地达到高水平的性能。

详细：https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734778292157444479#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734778292157444479#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:32:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软发布最新开发的小型语言模型 Phi-2<br />
<br />
- Phi-2 仅有 2.7B参数<br />
<br />
- Phi-2 超越了分别拥有7B和13B参数的Mistral和Llama-2模型<br />
<br />
- 甚至在多步推理任务上超越了参数量是其25倍的Llama-2-70B模型<br />
<br />
- 微软称Phi-2性能优异得益于其训练数据的质量非常高，他们弄了一个“教科书级”的数据集<br />
<br />
“教科书级”的数据集：为了训练 Phi-2，研究团队创建了特定的数据集，这些数据集专门设计用来教授模型进行常识推理和理解一般知识。这些合成数据集可能包含各种情景和问题，旨在提高模型在处理现实世界问题时的准确性和可靠性。<br />
<br />
知识迁移：另外研究团队还成功地将已经学习到的知识和模式从较小的Phi-1.5模型转移到了较大的Phi-2模型。这不仅提高了 Phi-2 的学习效率，还加速了其训练过程，使其能够更快地达到高水平的性能。<br />
<br />
详细：<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">microsoft.com/en-us/research…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JNcU4wSGIwQUFfamhULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734769901964066878#m</id>
            <title>Tesla 公布最新Optimus Gen2 机器人的演示视频

比3月份公布的Gen 1进步非常大，走路拿东西都已经很流畅了...

- 2个自由度的颈部关节
- 走路速度稳定度提高了30%
- 不影响功能的情况下，总重减少了10公斤
- 身体平衡能力更强，可以深蹲
- 新的手部关节，11个自由度，更加灵活
- 轻松拿捏鸡蛋
- 最后还表演了跳舞...</title>
            <link>https://nitter.cz/xiaohuggg/status/1734769901964066878#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734769901964066878#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:59:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Tesla 公布最新Optimus Gen2 机器人的演示视频<br />
<br />
比3月份公布的Gen 1进步非常大，走路拿东西都已经很流畅了...<br />
<br />
- 2个自由度的颈部关节<br />
- 走路速度稳定度提高了30%<br />
- 不影响功能的情况下，总重减少了10公斤<br />
- 身体平衡能力更强，可以深蹲<br />
- 新的手部关节，11个自由度，更加灵活<br />
- 轻松拿捏鸡蛋<br />
- 最后还表演了跳舞...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NjY2Njc3Njk4MjMyMzIvcHUvaW1nL3FrcVEyZll1dUR0d3J6RmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734764757453025539#m</id>
            <title>Upscale-A-Video：视频增加工具 提升视频清晰度和细节

由南洋理工大学 S-Lab 实验室开发，它能够将低分辨率视频转换为高分辨率，同时提高视频的清晰度和细节。

最重要的是：它可以通过文本提示来修改视频内容，比如提升特定物体的细节或改善整体的视觉效果。或者生成或修改视频内容、风格、细节等。

主要能力：

1、视频质量提升：Upscale-A-Video能够将分辨率较低的视频转换成高分辨率视频。例如，如果原视频模糊不清，这个工具可以使其变得更加清晰，细节更丰富。这对于改善老旧视频或质量较差的视频特别有用。

2、时间一致性：在提高视频分辨率的同时，能确保视频的每一帧都平滑过渡，没有不自然的跳跃或变化。这样，视频看起来就像是原本就以高分辨率拍摄的，而不是经过后期处理的。

3、文本引导的内容生成：允许用户通过输入文本提示来引导视频内容的生成。例如，用户可以输入特定的描述或指令，Upscale-A-Video将根据这些文本提示来调整视频的视觉内容。这种方法使得视频不仅在技术上得到提升，还能更好地符合用户的创意或需求。

- 视频质量提升：通过文本提示，用户可以指导模型专注于视频中的某些方面，比如提升特定物体的细节或改善整体的视觉效果。

- 内容生成和修改：在某些情况下，文本提示可能还用于生成或修改视频内容。例如，如果文本提示描述了某种特定的视觉风格或元素，模型可能会尝试按照这些指示调整视频内容。

工作原理：

在视频超分辨率领域，提高输出视频的保真度和时间一致性是一个主要挑战。这主要是因为扩散模型在生成过程中的随机性，可能导致视频中出现时间上的不连贯性。

Upscale-A-Video 使用一个文本引导的潜在扩散框架来进行视频增强。这意味着它可以根据文本提示来生成更高质量的视频内容。

该框架通过两个关键机制确保时间连贯性：局部上，它将时间层集成到 U-Net 和 VAE-Decoder 中，保持短序列的一致性；全局上，引入了一个无需训练的流引导的循环潜在传播模块，通过在整个序列中传播和融合潜在信息来增强视频的整体稳定性。

技术细节：

1、局部和全局处理结合：

• 局部处理：视频被分割成片段，每个片段使用具有时间层的 U-Net 进行处理，以确保片段内的一致性。

在视频的每个小片段内，使用集成了时间层的 U-Net 和 VAE-Decoder 来保持片段内的一致性。

VAE-Decoder 主要用来减少剩余的闪烁伪影，以实现低级一致性。

• 全局处理：通过一个流引导的循环潜在传播模块，在整个视频序列中传播和融合潜在信息，以增强视频的整体稳定性。这个模块在视频的不同部分之间建立联系，确保整个视频的一致性。

2、潜在扩散模型：

• 扩散过程：在潜在空间中，视频内容通过引入噪声和逐步去噪的过程来生成。这个过程允许模型逐步构建出高质量的视频内容。

• 文本引导：用户可以通过文本提示来指导视频内容的生成，使得最终的视频不仅质量高，还能符合特定的视觉风格或主题。

3、平衡恢复和生成：

• 可调节的噪声水平：通过调整加入到输入中的噪声水平，可以在恢复原始内容和生成新内容之间找到平衡点。较低的噪声水平倾向于恢复原始内容，而较高的噪声水平则鼓励生成更细致的细节。

这种方法适用于各种真实世界的视频，包括老电影片段和现代实拍视频。

项目及演示：https://shangchenzhou.com/projects/upscale-a-video/
论文：https://arxiv.org/abs/2312.06640
GitHub：https://github.com/sczhou/Upscale-A-Video</title>
            <link>https://nitter.cz/xiaohuggg/status/1734764757453025539#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734764757453025539#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:38:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Upscale-A-Video：视频增加工具 提升视频清晰度和细节<br />
<br />
由南洋理工大学 S-Lab 实验室开发，它能够将低分辨率视频转换为高分辨率，同时提高视频的清晰度和细节。<br />
<br />
最重要的是：它可以通过文本提示来修改视频内容，比如提升特定物体的细节或改善整体的视觉效果。或者生成或修改视频内容、风格、细节等。<br />
<br />
主要能力：<br />
<br />
1、视频质量提升：Upscale-A-Video能够将分辨率较低的视频转换成高分辨率视频。例如，如果原视频模糊不清，这个工具可以使其变得更加清晰，细节更丰富。这对于改善老旧视频或质量较差的视频特别有用。<br />
<br />
2、时间一致性：在提高视频分辨率的同时，能确保视频的每一帧都平滑过渡，没有不自然的跳跃或变化。这样，视频看起来就像是原本就以高分辨率拍摄的，而不是经过后期处理的。<br />
<br />
3、文本引导的内容生成：允许用户通过输入文本提示来引导视频内容的生成。例如，用户可以输入特定的描述或指令，Upscale-A-Video将根据这些文本提示来调整视频的视觉内容。这种方法使得视频不仅在技术上得到提升，还能更好地符合用户的创意或需求。<br />
<br />
- 视频质量提升：通过文本提示，用户可以指导模型专注于视频中的某些方面，比如提升特定物体的细节或改善整体的视觉效果。<br />
<br />
- 内容生成和修改：在某些情况下，文本提示可能还用于生成或修改视频内容。例如，如果文本提示描述了某种特定的视觉风格或元素，模型可能会尝试按照这些指示调整视频内容。<br />
<br />
工作原理：<br />
<br />
在视频超分辨率领域，提高输出视频的保真度和时间一致性是一个主要挑战。这主要是因为扩散模型在生成过程中的随机性，可能导致视频中出现时间上的不连贯性。<br />
<br />
Upscale-A-Video 使用一个文本引导的潜在扩散框架来进行视频增强。这意味着它可以根据文本提示来生成更高质量的视频内容。<br />
<br />
该框架通过两个关键机制确保时间连贯性：局部上，它将时间层集成到 U-Net 和 VAE-Decoder 中，保持短序列的一致性；全局上，引入了一个无需训练的流引导的循环潜在传播模块，通过在整个序列中传播和融合潜在信息来增强视频的整体稳定性。<br />
<br />
技术细节：<br />
<br />
1、局部和全局处理结合：<br />
<br />
• 局部处理：视频被分割成片段，每个片段使用具有时间层的 U-Net 进行处理，以确保片段内的一致性。<br />
<br />
在视频的每个小片段内，使用集成了时间层的 U-Net 和 VAE-Decoder 来保持片段内的一致性。<br />
<br />
VAE-Decoder 主要用来减少剩余的闪烁伪影，以实现低级一致性。<br />
<br />
• 全局处理：通过一个流引导的循环潜在传播模块，在整个视频序列中传播和融合潜在信息，以增强视频的整体稳定性。这个模块在视频的不同部分之间建立联系，确保整个视频的一致性。<br />
<br />
2、潜在扩散模型：<br />
<br />
• 扩散过程：在潜在空间中，视频内容通过引入噪声和逐步去噪的过程来生成。这个过程允许模型逐步构建出高质量的视频内容。<br />
<br />
• 文本引导：用户可以通过文本提示来指导视频内容的生成，使得最终的视频不仅质量高，还能符合特定的视觉风格或主题。<br />
<br />
3、平衡恢复和生成：<br />
<br />
• 可调节的噪声水平：通过调整加入到输入中的噪声水平，可以在恢复原始内容和生成新内容之间找到平衡点。较低的噪声水平倾向于恢复原始内容，而较高的噪声水平则鼓励生成更细致的细节。<br />
<br />
这种方法适用于各种真实世界的视频，包括老电影片段和现代实拍视频。<br />
<br />
项目及演示：<a href="https://shangchenzhou.com/projects/upscale-a-video/">shangchenzhou.com/projects/u…</a><br />
论文：<a href="https://arxiv.org/abs/2312.06640">arxiv.org/abs/2312.06640</a><br />
GitHub：<a href="https://github.com/sczhou/Upscale-A-Video">github.com/sczhou/Upscale-A-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NjQwNjQ3MzQzNTU0NTYvcHUvaW1nL21zdk5vYUpxUnY0ZzVmVjkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734753455510794492#m</id>
            <title>#Midjourney Alpha 网站

支持生成图片的宽高比例选择了

最近MJ的进度很缓慢啊😮‍💨</title>
            <link>https://nitter.cz/xiaohuggg/status/1734753455510794492#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734753455510794492#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:53:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23Midjourney">#Midjourney</a> Alpha 网站<br />
<br />
支持生成图片的宽高比例选择了<br />
<br />
最近MJ的进度很缓慢啊😮‍💨</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ3NDE3OTgwMDA0NjM4NzIvcHUvaW1nL0RCelVSWWQ3UHk2bzVybGwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734746436544413792#m</id>
            <title>Chanel 1 宣布他们将推出一个全新的网络新闻节目

这个节目的所有内容全部由AI制作

下面这个22分钟的新闻演示视频，所有主播、画面、声音、内容都通过AI制作完成🙃</title>
            <link>https://nitter.cz/xiaohuggg/status/1734746436544413792#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734746436544413792#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:25:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chanel 1 宣布他们将推出一个全新的网络新闻节目<br />
<br />
这个节目的所有内容全部由AI制作<br />
<br />
下面这个22分钟的新闻演示视频，所有主播、画面、声音、内容都通过AI制作完成🙃</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM0NTg3OTkzOTEzMDE2MzIwL2ltZy9jMUpWMUNoVlVsaGx4MXA5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734594915886330236#m</id>
            <title>有什么办法能让上传到X上面的视频

在720P的情况下也很清晰呢

求教下

实在受不了X这个模糊的视频了😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1734594915886330236#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734594915886330236#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 15:23:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有什么办法能让上传到X上面的视频<br />
<br />
在720P的情况下也很清晰呢<br />
<br />
求教下<br />
<br />
实在受不了X这个模糊的视频了😐</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>