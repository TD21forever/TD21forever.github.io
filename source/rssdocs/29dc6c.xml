<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732444629155651631#m</id>
            <title>Gemini 分为Ultra、Pro和Nano三种不同规模的优化版本。  

其中Gemini Nano 模型能够在手机等移动设备上运行😂

Google Pixel 8 Pro现在已经在运行Gemini Nano，提供了两个主要功能：

在Pixel 8 Pro的录音机应用中，Gemini Nano提供了一个“摘要”功能。这使用户能够获取其录制的对话、访谈、演讲等内容的摘要，即使在没有网络连接的情况下也能使用。

Gemini Nano还在Gboard中开始支持“智能回复”功能，作为开发者预览版。目前这个功能可在WhatsApp中使用，并计划明年扩展到更多应用。这个AI模型通过提供具有对话意识的高质量回复建议，节省用户的时间。

详细：https://blog.google/products/pixel/pixel-feature-drop-december-2023/</title>
            <link>https://nitter.cz/xiaohuggg/status/1732444629155651631#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732444629155651631#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 16:59:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini 分为Ultra、Pro和Nano三种不同规模的优化版本。  <br />
<br />
其中Gemini Nano 模型能够在手机等移动设备上运行😂<br />
<br />
Google Pixel 8 Pro现在已经在运行Gemini Nano，提供了两个主要功能：<br />
<br />
在Pixel 8 Pro的录音机应用中，Gemini Nano提供了一个“摘要”功能。这使用户能够获取其录制的对话、访谈、演讲等内容的摘要，即使在没有网络连接的情况下也能使用。<br />
<br />
Gemini Nano还在Gboard中开始支持“智能回复”功能，作为开发者预览版。目前这个功能可在WhatsApp中使用，并计划明年扩展到更多应用。这个AI模型通过提供具有对话意识的高质量回复建议，节省用户的时间。<br />
<br />
详细：<a href="https://blog.google/products/pixel/pixel-feature-drop-december-2023/">blog.google/products/pixel/p…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI0NDExNTI3ODY0NjA2NzIvcHUvaW1nL21xaFhBb2F3TG5tampxNHIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732438592096727043#m</id>
            <title>Google Gemini AI模型官方测试视频 （中文翻译）

通过这个视频你可以全面的了解Gemini AI的能力！

根据这个测试来看确实是很强大，进行了全方位的测试，从正常对话、视图能力、逻辑推理能力、语言翻译能、图像生成能力等都进行了各种测试演示。</title>
            <link>https://nitter.cz/xiaohuggg/status/1732438592096727043#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732438592096727043#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 16:35:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Gemini AI模型官方测试视频 （中文翻译）<br />
<br />
通过这个视频你可以全面的了解Gemini AI的能力！<br />
<br />
根据这个测试来看确实是很强大，进行了全方位的测试，从正常对话、视图能力、逻辑推理能力、语言翻译能、图像生成能力等都进行了各种测试演示。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI0MzY5OTYxNDcyNzM3MjgvcHUvaW1nL1F5bE0wWTlKSmJJdk13a3guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732431494470369767#m</id>
            <title>R to @xiaohuggg: 完整详细介绍：https://deepmind.google/technologies/gemini/#introduction

详细报告：https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1732431494470369767#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732431494470369767#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 16:07:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>完整详细介绍：<a href="https://deepmind.google/technologies/gemini/#introduction">deepmind.google/technologies…</a><br />
<br />
详细报告：<a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">storage.googleapis.com/deepm…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMjQxNDk4MTU1ODEzMjczNy9FX0lWbDZWMD9mb3JtYXQ9cG5nJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732430452156420482#m</id>
            <title>Google Bard已经同步更新了Gemini模型

Bard现在已经正在使用特别调整的Gemini Pro版本，以实现更高级的推理、规划、理解等能力。

在多个行业标准基准测试中，Gemini Pro版本在八个中的六个上超过了GPT-3.5，包括在MMLU（大规模多任务语言理解）和GSM8K（衡量小学数学推理能力）。

它将首先在 170 多个国家和地区提供英语版本，并在不久的将来扩展到更多语言和地区。

明年初，Google还将推出Bard Advanced，提供对Gemini Ultra等最先进模型和功能的首次访问。

https://blog.google/products/bard/google-bard-try-gemini-ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1732430452156420482#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732430452156420482#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 16:03:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Bard已经同步更新了Gemini模型<br />
<br />
Bard现在已经正在使用特别调整的Gemini Pro版本，以实现更高级的推理、规划、理解等能力。<br />
<br />
在多个行业标准基准测试中，Gemini Pro版本在八个中的六个上超过了GPT-3.5，包括在MMLU（大规模多任务语言理解）和GSM8K（衡量小学数学推理能力）。<br />
<br />
它将首先在 170 多个国家和地区提供英语版本，并在不久的将来扩展到更多语言和地区。<br />
<br />
明年初，Google还将推出Bard Advanced，提供对Gemini Ultra等最先进模型和功能的首次访问。<br />
<br />
<a href="https://blog.google/products/bard/google-bard-try-gemini-ai/">blog.google/products/bard/go…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyVVJ2Z2FNQUEtZFVHLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732428501775700067#m</id>
            <title>R to @xiaohuggg: 从数据来看

几乎所有测试中都超越了GPT 4😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1732428501775700067#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732428501775700067#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 15:55:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>从数据来看<br />
<br />
几乎所有测试中都超越了GPT 4😄</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyUnBUeGJBQUFkM3JmLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyUjJ6eGJjQUFUUkRqLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732427456328708190#m</id>
            <title>R to @xiaohuggg: Gemini 分为Ultra、Pro和Nano三种不同规模的优化版本。

Gemini 模型能够高效运行在从数据中心到移动设备的各种平台上。

Gemini 是第一个在 MMLU（大规模多任务语言理解）方面超越人类专家的模型。得分90%、人类是89.8%、GPT 4是86.4%...</title>
            <link>https://nitter.cz/xiaohuggg/status/1732427456328708190#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732427456328708190#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 15:51:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini 分为Ultra、Pro和Nano三种不同规模的优化版本。<br />
<br />
Gemini 模型能够高效运行在从数据中心到移动设备的各种平台上。<br />
<br />
Gemini 是第一个在 MMLU（大规模多任务语言理解）方面超越人类专家的模型。得分90%、人类是89.8%、GPT 4是86.4%...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyUXFzVWJNQUE5cEdJLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732424297728380948#m</id>
            <title>R to @xiaohuggg: Gemini被设计为天生的多模态模型，从一开始就在不同模态上进行预训练，然后通过额外的多模态数据进行微调，以进一步提高其有效性。这使得Gemini能够从根本上更好地理解和推理各种输入。

能够理解和操作包括文本、代码、音频、图像和视频在内的不同类型的信息。</title>
            <link>https://nitter.cz/xiaohuggg/status/1732424297728380948#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732424297728380948#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 15:38:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini被设计为天生的多模态模型，从一开始就在不同模态上进行预训练，然后通过额外的多模态数据进行微调，以进一步提高其有效性。这使得Gemini能够从根本上更好地理解和推理各种输入。<br />
<br />
能够理解和操作包括文本、代码、音频、图像和视频在内的不同类型的信息。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyT3JkTmIwQUE3OWQ4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732420940582236360#m</id>
            <title>谷歌发布Gemini人工智能模型

宣称从图像、音频和视频理解到数学推理，Gemini Ultra 的性能在大语言模型 32 个广泛使用的学术基准中的 30 个上超过了当前最先进的结果。

Gemini Ultra 的得分高达 90.0%，是第一个在 MMLU（大规模多任务语言理解）上超越人类专家的模型。

该模型结合了数学、物理、历史、法律、医学和伦理学等 57 个科目来测试知识和解决问题的能力。

https://blog.google/technology/ai/google-gemini-ai/</title>
            <link>https://nitter.cz/xiaohuggg/status/1732420940582236360#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732420940582236360#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 15:25:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谷歌发布Gemini人工智能模型<br />
<br />
宣称从图像、音频和视频理解到数学推理，Gemini Ultra 的性能在大语言模型 32 个广泛使用的学术基准中的 30 个上超过了当前最先进的结果。<br />
<br />
Gemini Ultra 的得分高达 90.0%，是第一个在 MMLU（大规模多任务语言理解）上超越人类专家的模型。<br />
<br />
该模型结合了数学、物理、历史、法律、医学和伦理学等 57 个科目来测试知识和解决问题的能力。<br />
<br />
<a href="https://blog.google/technology/ai/google-gemini-ai/">blog.google/technology/ai/go…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyTHFFTGJZQUFDUWd4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732342321117839744#m</id>
            <title>消息称Google计划在本周发布其最新的AI聊天机器人：Gemini

Gemini被认为是GPT 4的有力竞争对手，消息称和GPT 4能力不相上下！

据《The Information》报道称，Google计划在2024年公布Gemini。Google似乎改变了策略，以在OpenAI经历董事会重组和内部动荡期间抢占舆论焦点…✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1732342321117839744#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732342321117839744#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 10:12:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>消息称Google计划在本周发布其最新的AI聊天机器人：Gemini<br />
<br />
Gemini被认为是GPT 4的有力竞争对手，消息称和GPT 4能力不相上下！<br />
<br />
据《The Information》报道称，Google计划在2024年公布Gemini。Google似乎改变了策略，以在OpenAI经历董事会重组和内部动荡期间抢占舆论焦点…✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FxRUoyQ2F3QUF1VjI1LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732311380932583924#m</id>
            <title>央视龙年春晚吉祥物

好多人说

这是AI生成的

我感觉这是一年来AI受到的最大的侮辱</title>
            <link>https://nitter.cz/xiaohuggg/status/1732311380932583924#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732311380932583924#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 08:09:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>央视龙年春晚吉祥物<br />
<br />
好多人说<br />
<br />
这是AI生成的<br />
<br />
我感觉这是一年来AI受到的最大的侮辱</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FwblBObWFVQUFoU0pYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732302746886471820#m</id>
            <title>苹果发布专为 Apple 芯片设计的高效机器学习框架：MLX

MLX 的 API 设计与 NumPy 和 PyTorch 相似，让你能够在苹果电脑上方便地建立和训练机器学习模型。

使得在苹果电脑上进行机器学习相关的开发和研究变得更加简单和高效。

演示显示可在 M2 Ultra 上运行的 Llama v1 7B 模型。 

代码： https://github.com/ml-explore/mlx  

文档：https://ml-explore.github.io/mlx/build/html/index.html

MLX 示例存储库提供了一些示例，包括：

- Transformer 语言模型训练。
- 使用 LLaMA 或 Mistral 进行的大规模文本生
- 通过 LoRA 进行的参数高效微调
- 使用稳定扩散技术的图像生成
- 使用 OpenAI 的 Whisper 进行语音识别。

案例：https://github.com/ml-explore/mlx-examples

主要特点：

1、熟悉的 API：：MLX 的 API 设计与 NumPy 和 PyTorch 相似，使得用户可以方便地构建和训练复杂的机器学习模型。

2、自动微分和向量化：MLX 支持自动微分和自动向量化，这对于优化和加速机器学习模型的训练过程非常有用。

3、高效的内存管理：MLX 的统一内存模型允许在不同设备（如 CPU 和 GPU）之间高效地共享和处理数据，无需频繁地移动数据。

4、动态图构建和延迟计算：MLX 支持动态图构建和延迟计算，这使得模型的开发和调试更加灵活和高效。

MLX Data 是 Apple 机器学习研究为您带来的一个与框架无关的数据加载库。它可与 PyTorch、Jax 或 MLX 配合使用。

高效且灵活，例如能够每秒加载和处理 1000 张图像，同时还能对生成的批次运行任意 Python 转换。

代码： https://github.com/ml-explore/mlx-data

文档： https://ml-explore.github.io/mlx-data/build/html/index.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1732302746886471820#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732302746886471820#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 07:35:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>苹果发布专为 Apple 芯片设计的高效机器学习框架：MLX<br />
<br />
MLX 的 API 设计与 NumPy 和 PyTorch 相似，让你能够在苹果电脑上方便地建立和训练机器学习模型。<br />
<br />
使得在苹果电脑上进行机器学习相关的开发和研究变得更加简单和高效。<br />
<br />
演示显示可在 M2 Ultra 上运行的 Llama v1 7B 模型。 <br />
<br />
代码： <a href="https://github.com/ml-explore/mlx">github.com/ml-explore/mlx</a>  <br />
<br />
文档：<a href="https://ml-explore.github.io/mlx/build/html/index.html">ml-explore.github.io/mlx/bui…</a><br />
<br />
MLX 示例存储库提供了一些示例，包括：<br />
<br />
- Transformer 语言模型训练。<br />
- 使用 LLaMA 或 Mistral 进行的大规模文本生<br />
- 通过 LoRA 进行的参数高效微调<br />
- 使用稳定扩散技术的图像生成<br />
- 使用 OpenAI 的 Whisper 进行语音识别。<br />
<br />
案例：<a href="https://github.com/ml-explore/mlx-examples">github.com/ml-explore/mlx-ex…</a><br />
<br />
主要特点：<br />
<br />
1、熟悉的 API：：MLX 的 API 设计与 NumPy 和 PyTorch 相似，使得用户可以方便地构建和训练复杂的机器学习模型。<br />
<br />
2、自动微分和向量化：MLX 支持自动微分和自动向量化，这对于优化和加速机器学习模型的训练过程非常有用。<br />
<br />
3、高效的内存管理：MLX 的统一内存模型允许在不同设备（如 CPU 和 GPU）之间高效地共享和处理数据，无需频繁地移动数据。<br />
<br />
4、动态图构建和延迟计算：MLX 支持动态图构建和延迟计算，这使得模型的开发和调试更加灵活和高效。<br />
<br />
MLX Data 是 Apple 机器学习研究为您带来的一个与框架无关的数据加载库。它可与 PyTorch、Jax 或 MLX 配合使用。<br />
<br />
高效且灵活，例如能够每秒加载和处理 1000 张图像，同时还能对生成的批次运行任意 Python 转换。<br />
<br />
代码： <a href="https://github.com/ml-explore/mlx-data">github.com/ml-explore/mlx-da…</a><br />
<br />
文档： <a href="https://ml-explore.github.io/mlx-data/build/html/index.html">ml-explore.github.io/mlx-dat…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIzMDIxODA5NjE2ODU1MDQvcHUvaW1nLzdlS0tlWTdVbDZkaUctT1ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732278818847777042#m</id>
            <title>R to @xiaohuggg: Vid2DensePose 通过将视频转换为 DensePose 格式，可以更精确地控制和动画化视频中的人物。

Vid2DensePose与 MagicAnimate 集成，生成的 DensePose 数据可以直接用于 MagicAnimate，从而提高动画的质量和一致性。https://github.com/Flode-Labs/vid2densepose/tree/main</title>
            <link>https://nitter.cz/xiaohuggg/status/1732278818847777042#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732278818847777042#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 06:00:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vid2DensePose 通过将视频转换为 DensePose 格式，可以更精确地控制和动画化视频中的人物。<br />
<br />
Vid2DensePose与 MagicAnimate 集成，生成的 DensePose 数据可以直接用于 MagicAnimate，从而提高动画的质量和一致性。<a href="https://github.com/Flode-Labs/vid2densepose/tree/main">github.com/Flode-Labs/vid2de…</a></p>
<p><a href="https://nitter.cz/nacho_gorriti_/status/1732106540474126381#m">nitter.cz/nacho_gorriti_/status/1732106540474126381#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732198994145591383#m</id>
            <title>RT by @xiaohuggg: Bing 官方博客发布的 Copilot 更新：

Copilot的持续AI革新

今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。

GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。

全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 http://bing.com/create 或指令 Copilot 制作图片，即刻体验这些功能。

欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</title>
            <link>https://nitter.cz/dotey/status/1732198994145591383#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732198994145591383#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Bing 官方博客发布的 Copilot 更新：<br />
<br />
Copilot的持续AI革新<br />
<br />
今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。<br />
<br />
GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。<br />
<br />
全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 <a href="http://bing.com/create">bing.com/create</a> 或指令 Copilot 制作图片，即刻体验这些功能。<br />
<br />
欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQndwLVhBQUFLak1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</id>
            <title>Generative Powers of Ten：基于文本的多尺度图像生成技术

是一种图像无限缩放技术，而且质量非常高清！

它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。

可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。

该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。

这项技术的关键特点包括：

- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。

- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。

- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。

- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。

- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。

多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。

该项目基于一种联合采样算法：

联合采样算法的核心特点

并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。

迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。

优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。

它使用了以下几个关键步骤和技术：

1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。

2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。

3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。

4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。

5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。

项目及演示：https://powers-of-10.github.io/
论文：https://arxiv.org/abs/2312.02149</title>
            <link>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732247057237500130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:54:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Generative Powers of Ten：基于文本的多尺度图像生成技术<br />
<br />
是一种图像无限缩放技术，而且质量非常高清！<br />
<br />
它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。<br />
<br />
可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。<br />
<br />
该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。<br />
<br />
这项技术的关键特点包括：<br />
<br />
- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。<br />
<br />
- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。<br />
<br />
- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。<br />
<br />
- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。<br />
<br />
- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。<br />
<br />
多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。<br />
<br />
该项目基于一种联合采样算法：<br />
<br />
联合采样算法的核心特点<br />
<br />
并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。<br />
<br />
迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。<br />
<br />
优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。<br />
<br />
它使用了以下几个关键步骤和技术：<br />
<br />
1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。<br />
<br />
2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。<br />
<br />
3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。<br />
<br />
4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。<br />
<br />
5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。<br />
<br />
项目及演示：<a href="https://powers-of-10.github.io/">powers-of-10.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02149">arxiv.org/abs/2312.02149</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIyNDMyNzc2NDUzMzY1NzYvcHUvaW1nL2ZwZ2pVYmN2Zkd2OV9WWFEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732235284555927967#m</id>
            <title>DeepMind：开发出可以向人类学习的人工智能

Nature发表了一篇Google DeepMind的研究成果：研究人员在3D模拟环境中使用神经网络和强化学习，展示了AI智能体如何在没有直接从人类那里获取数据的情况下，通过观察来学习和模仿人类的行为。

这项研究被视为向人工通用智能（AGI）迈进的一大步。

研究背景：

智力包括有效的知识获取，通常依赖于文化传播——个体之间的知识转移。

人类智力在很大程度上依赖于这一过程，从而能够通过社会学习吸收文化知识。这种知识被称为文化，而从一个个体向另一个个体的知识传递被称为文化传播。

文化传播是一种社会学习形式，通过与其他智能体的接触来协助学习。

技术原理：

这项技术，正是利用了这一现象，它使得AI智能体能够通过观察人类的行为来学习并模仿这些行为。这种学习方式被称为“文化传播”，它是一种社会学习形式，意味着智能体不是单独学习，而是通过与人类或其他智能体的互动来获取知识。

这种智能体能够在丰富的3D物理模拟环境中与人类共同玩耍。

该研究展示了AI智能体如何在没有先前人类数据的情况下模仿人类行为的能力。这项研究通过在3D模拟环境中使用神经网络和强化学习（RL），使AI智能体能够实时、高保真地获取和利用信息，类似于人类跨代积累和精炼知识的方式。

举例解释：

假设有一个AI智能体，我们想让它学会如何玩乒乓球。在传统的学习方法中，我们可能需要编写详细的规则和指令来教会AI如何打乒乓球。但在这项研究中，AI智能体可以通过观察真人打乒乓球的视频来学习。它会注意到人类是如何握拍、如何挥拍、如何移动身体来接球和击球的。

技术细节：

深度强化学习：这种学习方法让AI智能体通过反复尝试和错误来优化其行为。例如，AI可能一开始打球时总是失误，但随着学习的深入，它会逐渐学会如何更准确地击中球。

模仿学习：AI智能体通过观察人类的行为来学习。在乒乓球的例子中，AI会分析人类运动员的动作，然后尝试复制这些动作。

新的学习环境（GoalCycle3D）：研究人员为AI智能体提供了一个3D模拟环境，让它们可以在一个控制和安全的环境中练习和实践所学的技能。

GoalCycle3D框架为AI探索提供了一个复杂的范式，建立在先前的工作基础上，创造了一个更具沉浸感和真实性的环境。该框架通过将任务划分为不同的元素（世界、游戏和共玩者），为RL建立了多样化的环境。

详细：https://www.nature.com/articles/s41467-023-42875-2</title>
            <link>https://nitter.cz/xiaohuggg/status/1732235284555927967#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732235284555927967#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind：开发出可以向人类学习的人工智能<br />
<br />
Nature发表了一篇Google DeepMind的研究成果：研究人员在3D模拟环境中使用神经网络和强化学习，展示了AI智能体如何在没有直接从人类那里获取数据的情况下，通过观察来学习和模仿人类的行为。<br />
<br />
这项研究被视为向人工通用智能（AGI）迈进的一大步。<br />
<br />
研究背景：<br />
<br />
智力包括有效的知识获取，通常依赖于文化传播——个体之间的知识转移。<br />
<br />
人类智力在很大程度上依赖于这一过程，从而能够通过社会学习吸收文化知识。这种知识被称为文化，而从一个个体向另一个个体的知识传递被称为文化传播。<br />
<br />
文化传播是一种社会学习形式，通过与其他智能体的接触来协助学习。<br />
<br />
技术原理：<br />
<br />
这项技术，正是利用了这一现象，它使得AI智能体能够通过观察人类的行为来学习并模仿这些行为。这种学习方式被称为“文化传播”，它是一种社会学习形式，意味着智能体不是单独学习，而是通过与人类或其他智能体的互动来获取知识。<br />
<br />
这种智能体能够在丰富的3D物理模拟环境中与人类共同玩耍。<br />
<br />
该研究展示了AI智能体如何在没有先前人类数据的情况下模仿人类行为的能力。这项研究通过在3D模拟环境中使用神经网络和强化学习（RL），使AI智能体能够实时、高保真地获取和利用信息，类似于人类跨代积累和精炼知识的方式。<br />
<br />
举例解释：<br />
<br />
假设有一个AI智能体，我们想让它学会如何玩乒乓球。在传统的学习方法中，我们可能需要编写详细的规则和指令来教会AI如何打乒乓球。但在这项研究中，AI智能体可以通过观察真人打乒乓球的视频来学习。它会注意到人类是如何握拍、如何挥拍、如何移动身体来接球和击球的。<br />
<br />
技术细节：<br />
<br />
深度强化学习：这种学习方法让AI智能体通过反复尝试和错误来优化其行为。例如，AI可能一开始打球时总是失误，但随着学习的深入，它会逐渐学会如何更准确地击中球。<br />
<br />
模仿学习：AI智能体通过观察人类的行为来学习。在乒乓球的例子中，AI会分析人类运动员的动作，然后尝试复制这些动作。<br />
<br />
新的学习环境（GoalCycle3D）：研究人员为AI智能体提供了一个3D模拟环境，让它们可以在一个控制和安全的环境中练习和实践所学的技能。<br />
<br />
GoalCycle3D框架为AI探索提供了一个复杂的范式，建立在先前的工作基础上，创造了一个更具沉浸感和真实性的环境。该框架通过将任务划分为不同的元素（世界、游戏和共玩者），为RL建立了多样化的环境。<br />
<br />
详细：<a href="https://www.nature.com/articles/s41467-023-42875-2">nature.com/articles/s41467-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvaUdyRGFnQUFabU5NLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732224902302945612#m</id>
            <title>将DALL·E 3 与草图软件@tldraw 集成

写一个主Prompt 然后使用链条来控制图像生成…😎

在链条上输入关键词或者其他辅助prompt即可生成图片

同时还能将多个链条合并组合，合并图片🫡

这样是不是可以可视化的保证图片一致性？？？</title>
            <link>https://nitter.cz/xiaohuggg/status/1732224902302945612#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732224902302945612#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 02:26:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将DALL·E 3 与草图软件<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 集成<br />
<br />
写一个主Prompt 然后使用链条来控制图像生成…😎<br />
<br />
在链条上输入关键词或者其他辅助prompt即可生成图片<br />
<br />
同时还能将多个链条合并组合，合并图片🫡<br />
<br />
这样是不是可以可视化的保证图片一致性？？？</p>
<p><a href="https://nitter.cz/miiura/status/1732040947477987555#m">nitter.cz/miiura/status/1732040947477987555#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>