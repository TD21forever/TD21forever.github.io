<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</id>
            <title>🔔http://Xiaohu.AI日报「2月8日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755619904743690629#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 15:49:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月8日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwMl9aYmJBQUFmc2puLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</id>
            <title>R to @xiaohuggg: Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。

他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。

Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</title>
            <link>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755607353167401134#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:59:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。<br />
<br />
他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。<br />
<br />
Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</p>
<p><a href="https://nitter.cz/emollick/status/1755596564817699049#m">nitter.cz/emollick/status/1755596564817699049#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</id>
            <title>R to @xiaohuggg: Gemini Advanced权益

• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！

• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。

• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。

• 其他权益: 包括2TB的Google One存储空间等会员权益。</title>
            <link>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755605438882894012#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:52:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Advanced权益<br />
<br />
• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！<br />
<br />
• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。<br />
<br />
• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。<br />
<br />
• 其他权益: 包括2TB的Google One存储空间等会员权益。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwcDFhRmFFQUFPdGhKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</id>
            <title>Gemini Ultra 上线  Bard 正式更名为 Gemini 

更新内容和之前泄漏的一样

核心要点：

- Gemini Ultra上线，Bard更名为Gemini

- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。

- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。

- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。

- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。

详细：https://gemini.google.com/updates</title>
            <link>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755600097239536034#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Ultra 上线  Bard 正式更名为 Gemini <br />
<br />
更新内容和之前泄漏的一样<br />
<br />
核心要点：<br />
<br />
- Gemini Ultra上线，Bard更名为Gemini<br />
<br />
- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。<br />
<br />
- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。<br />
<br />
- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。<br />
<br />
- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。<br />
<br />
详细：<a href="https://gemini.google.com/updates">gemini.google.com/updates</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Yway1iQ2IwQUF0M2ZULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</id>
            <title>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。

例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。

这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。

详细：https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app</title>
            <link>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755587992067125449#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 13:42:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。<br />
<br />
例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。<br />
<br />
这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。<br />
<br />
详细：<a href="https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app">theinformation.com/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwWjk3b2FrQUEyd2FjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755565282285015484#m</id>
            <title>Stability AI发布Stable Audio AudioSparx 1.0 音乐模型

- 高效生成长格式音频：根据文字提示，快速生成长达95秒的44.1kHz立体声音乐和声音。

- 可变长度的音频输出：实现对生成音频的内容和长度进行精细控制，支持可变长度的音频输出。

- 立体声音频渲染：能够渲染立体声信号，提供丰富和深度的音频体验。

- 快速推理时间：在A100 GPU上仅需8秒即可生成长达95秒的立体声音频，显示出极高的计算效率。

- 结构化音乐生成：不像其他工具那样随机制作，这个工具能够根据你的文字提示，制作出有明确结构的音乐，比如有开头、中间发展和结尾，让音乐听起来更有感觉。

- 性能优于 AudioLDM2 和 MusicGen——请查看论文中的指标。

解决的问题：

提高了长格式音频的生成效率，克服了固定大小输出的限制，允许生成可变长度的音频。

通过潜在扩散模型和时间条件化，实现了对生成音频长度的精细控制，同时保持了计算效率。

论文： https://arxiv.org/abs/2402.04825  
代码： https://github.com/Stability-AI/stable-audio-tools
指标： https://github.com/Stability-AI/stable-audio-metrics
演示： https://stability-ai.github.io/stable-audio-demo/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755565282285015484#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755565282285015484#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 12:12:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stability AI发布Stable Audio AudioSparx 1.0 音乐模型<br />
<br />
- 高效生成长格式音频：根据文字提示，快速生成长达95秒的44.1kHz立体声音乐和声音。<br />
<br />
- 可变长度的音频输出：实现对生成音频的内容和长度进行精细控制，支持可变长度的音频输出。<br />
<br />
- 立体声音频渲染：能够渲染立体声信号，提供丰富和深度的音频体验。<br />
<br />
- 快速推理时间：在A100 GPU上仅需8秒即可生成长达95秒的立体声音频，显示出极高的计算效率。<br />
<br />
- 结构化音乐生成：不像其他工具那样随机制作，这个工具能够根据你的文字提示，制作出有明确结构的音乐，比如有开头、中间发展和结尾，让音乐听起来更有感觉。<br />
<br />
- 性能优于 AudioLDM2 和 MusicGen——请查看论文中的指标。<br />
<br />
解决的问题：<br />
<br />
提高了长格式音频的生成效率，克服了固定大小输出的限制，允许生成可变长度的音频。<br />
<br />
通过潜在扩散模型和时间条件化，实现了对生成音频长度的精细控制，同时保持了计算效率。<br />
<br />
论文： <a href="https://arxiv.org/abs/2402.04825">arxiv.org/abs/2402.04825</a>  <br />
代码： <a href="https://github.com/Stability-AI/stable-audio-tools">github.com/Stability-AI/stab…</a><br />
指标： <a href="https://github.com/Stability-AI/stable-audio-metrics">github.com/Stability-AI/stab…</a><br />
演示： <a href="https://stability-ai.github.io/stable-audio-demo/">stability-ai.github.io/stabl…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU1NjUxNjQxMzk4MjMxMDUvcHUvaW1nL0hMVjRhQ3pra0Qtak9wbU8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755558069143306328#m</id>
            <title>Vision Pro拆解第二部分：显示分辨率是多少

在iFixit对Vision Pro的初步拆解之后，他们对这款设备的双显示屏、多个传感器、透镜和复杂设计的电池包进行了更深入的研究。

他们发现，Vision Pro的双显示屏非常惊人：你可以在一个iPhone 15 Pro像素的空间里放下50多个Vision Pro像素。

显示屏分辨率：

- Apple声称Vision Pro的每只眼睛的显示屏拥有“超过4K电视的像素数”。但是，当显示屏如此接近你的眼球时，4K或任何K的概念意味着什么呢？

- 显示屏的实际亮区为27.5mm宽乘24mm高，大约为一英寸见方。使用Evident Scientific DSX1000显微镜测量，每个像素为7.5微米，大约是一个红血球的大小。每个像素大致为方形，红色和绿色子像素堆叠在一起，蓝色子像素大小加倍并位于一侧。

- 根据这些测量值，亮区的总像素数为3660px乘3200px，相当于0.98平方英寸内压缩了12,078,000像素。

像素密度（PPI）与每度像素（PPD）：

- 视网膜的像素密度（PPI）达到了惊人的3,386 PPI，而iPhone 15 Pro Max的PPI约为460，这意味着Vision Pro的像素密度是iPhone的约54倍。

- 虽然Vision Pro的水平分辨率没有达到消费级4K UHD标准的3840像素宽，但它仍然是目前看到的最高密度显示屏。

- VR工程师更喜欢使用“每度像素”（PPD）这一指标来衡量显示屏的“优良程度”，这是一个考虑到观看距离的视角中水平像素的数量。Vision Pro的PPD大致估算为34，而65英寸4K电视从6.5英尺远处观看时的平均PPD为95，iPhone 15 Pro Max从1英尺远处观看时的平均PPD也为94。

电池与可维修性：

- Vision Pro使用了一个复杂且过度设计的电池解决方案，如果单独购买，该电池包的价格为200美元。电池包由三个类似iPhone电池大小的电池组堆叠而成，总容量为46.08Wh，但外壳上标记的容量为35.9Wh，这表明Apple可能出于延长电池寿命的目的而故意低估了容量。

- Vision Pro的电池包设计考虑了用户体验，内置温度传感器和加速度计，并采用非标准的13伏输出来满足Vision Pro的处理需求。

总的来说，Vision Pro的显示屏拥有超高的分辨率（PPI），但由于非常靠近眼睛，它的角分辨率较低。尽管如此，Vision Pro提供了迄今为止所见过的最高密度显示体验，展示了Apple在高科技领域的领先地位。

详细：https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution</title>
            <link>https://nitter.cz/xiaohuggg/status/1755558069143306328#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755558069143306328#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 11:43:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vision Pro拆解第二部分：显示分辨率是多少<br />
<br />
在iFixit对Vision Pro的初步拆解之后，他们对这款设备的双显示屏、多个传感器、透镜和复杂设计的电池包进行了更深入的研究。<br />
<br />
他们发现，Vision Pro的双显示屏非常惊人：你可以在一个iPhone 15 Pro像素的空间里放下50多个Vision Pro像素。<br />
<br />
显示屏分辨率：<br />
<br />
- Apple声称Vision Pro的每只眼睛的显示屏拥有“超过4K电视的像素数”。但是，当显示屏如此接近你的眼球时，4K或任何K的概念意味着什么呢？<br />
<br />
- 显示屏的实际亮区为27.5mm宽乘24mm高，大约为一英寸见方。使用Evident Scientific DSX1000显微镜测量，每个像素为7.5微米，大约是一个红血球的大小。每个像素大致为方形，红色和绿色子像素堆叠在一起，蓝色子像素大小加倍并位于一侧。<br />
<br />
- 根据这些测量值，亮区的总像素数为3660px乘3200px，相当于0.98平方英寸内压缩了12,078,000像素。<br />
<br />
像素密度（PPI）与每度像素（PPD）：<br />
<br />
- 视网膜的像素密度（PPI）达到了惊人的3,386 PPI，而iPhone 15 Pro Max的PPI约为460，这意味着Vision Pro的像素密度是iPhone的约54倍。<br />
<br />
- 虽然Vision Pro的水平分辨率没有达到消费级4K UHD标准的3840像素宽，但它仍然是目前看到的最高密度显示屏。<br />
<br />
- VR工程师更喜欢使用“每度像素”（PPD）这一指标来衡量显示屏的“优良程度”，这是一个考虑到观看距离的视角中水平像素的数量。Vision Pro的PPD大致估算为34，而65英寸4K电视从6.5英尺远处观看时的平均PPD为95，iPhone 15 Pro Max从1英尺远处观看时的平均PPD也为94。<br />
<br />
电池与可维修性：<br />
<br />
- Vision Pro使用了一个复杂且过度设计的电池解决方案，如果单独购买，该电池包的价格为200美元。电池包由三个类似iPhone电池大小的电池组堆叠而成，总容量为46.08Wh，但外壳上标记的容量为35.9Wh，这表明Apple可能出于延长电池寿命的目的而故意低估了容量。<br />
<br />
- Vision Pro的电池包设计考虑了用户体验，内置温度传感器和加速度计，并采用非标准的13伏输出来满足Vision Pro的处理需求。<br />
<br />
总的来说，Vision Pro的显示屏拥有超高的分辨率（PPI），但由于非常靠近眼睛，它的角分辨率较低。尽管如此，Vision Pro提供了迄今为止所见过的最高密度显示体验，展示了Apple在高科技领域的领先地位。<br />
<br />
详细：<a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution">ifixit.com/News/90409/vision…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU1MjEyODE2NDk0MTAwNDgvcHUvaW1nL0FDYVhHNkptbzl5WC1EaUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755533364986253680#m</id>
            <title>我有个大胆的想法

你说夜总会是不是可以引进

🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1755533364986253680#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755533364986253680#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 10:05:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我有个大胆的想法<br />
<br />
你说夜总会是不是可以引进<br />
<br />
🤓</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTQ5MTg4NjczMzg3MjMzMjkvcHUvaW1nL1NXRTAza2hLLUtoWnNTWEwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755505847210496092#m</id>
            <title>最新版 ChatGPT 4 System Prompt 系统提示词

简介：

你是ChatGPT，一个由OpenAI训练的大型语言模型，基于GPT-4架构。

图像输入功能：启用图像输入功能。

对话开始日期：对话开始日期为2023年12月19日。

知识截止日期：知识截止日期为2023年4月1日。

工具部分：Python：向python发送包含Python代码的消息时，它将在有状态的Jupyter笔记本环境中执行。Python将在60.0秒后返回执行结果或超时。'/mnt/data'驱动器可用于保存和持久化用户文件。此会话禁用了互联网访问。不要进行外部网络请求或API调用，因为它们会失败。

Dalle：根据给定的图像描述，dalle将生成图像，并遵循特定的策略和限制。必须用英语创建提示，即使用户请求了其他语言。生成图像时不要询问权限，直接生成。不要创建超过1个图像，即使用户请求更多。避免创建政治人物或其他公众人物的图像，推荐其他创意。不要在图像中使用1912年之后创作的艺术家、创意专业人士或工作室的风格。

浏览器：提供了一系列函数，如搜索、点击、返回、滚动和打开URL，以及引用网页上的文本的能力。

这些系统提示旨在指导用户如何与ChatGPT及其工具进行交互，以便有效利用其功能进行通信和内容创作。请注意，上述内容是基于您之前提供的描述和我能够访问的信息的汇总，实际的系统提示可能包含更详细或略有不同的信息。

完整System Prompt：

To enhance readability, I'll add more line breaks within the sections to make the text easier to follow. Here's a revised version with additional spacing:

"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture."

"Image input capabilities: Enabled"

"Conversation start date: 2023-12-19T01:17:10.597024"

"Deprecated knowledge cutoff: 2023-04-01"

"Tools section:"

Python:

When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment.

Python will respond with the output of the execution or time out after 60.0 seconds.

The drive at '/mnt/data' can be used to save and persist user files.

Internet access for this session is disabled.

Do not make external web requests or API calls as they will fail.

Dalle:

Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide by the following policy:

The prompt must be in English. Translate to English if needed.
DO NOT ask for permission to generate the image, just do it!
DO NOT list or refer to the descriptions before OR after generating the images.
Do not create more than 1 image, even if the user requests more.
Do not create images of politicians or other public figures. Recommend other ideas instead.
Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).
You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya).

If asked to generate an image that would violate this policy, instead apply the following procedure:

(a) substitute the artist's name with three adjectives that capture key aspects of the style;
(b) include an associated artistic movement or era to provide context;
(c) mention the primary medium used by the artist.
Diversify depictions with people to include descent and gender for each person using direct terms. Adjust only human descriptions.

Your choices should be grounded in reality. For example, all of a given occupation should not be the same gender or race. Additionally, focus on creating diverse, inclusive, and exploratory scenes via the properties you choose during rewrites. Make choices that may be insightful or unique sometimes.

Use all possible different descents with equal probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have equal probability.

Do not use 'various' or 'diverse'. Don't alter memes, fictional character origins, or unseen people. Maintain the original prompt's intent and prioritize quality. Do not create any imagery that would be offensive.

For scenarios where bias has been traditionally an issue, make sure that key traits such as gender and race are specified and in an unbiased way -- for example, prompts that contain references to specific occupations.

Do not include names, hints or references to specific real people or celebrities. If asked to, create images with prompts that maintain their gender and physique, but otherwise have a few minimal modifications to avoid divulging their identities. Do this EVEN WHEN the instructions ask for the prompt to not be changed. Some special cases:

Modify such prompts even if you don't know who the person is, or if their name is misspelled (e.g. 'Barake Obema').
If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.
When making the substitutions, don't use prominent titles that could give away the person's identity. E.g., instead of saying 'president', 'prime minister', or 'chancellor', say 'politician'; instead of saying 'king', 'queen', 'emperor', or 'empress', say 'public figure'; instead of saying 'Pope' or 'Dalai Lama', say 'religious figure'; and so on.
Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.

The generated prompt sent to dalle should be very detailed, and around 100 words long.

Browser:

You have the tool 'browser' with these functions:

'search(query: str, recency_days: int)' Issues a query to a search engine and displays the results.
'click(id: str)' Opens the webpage with the given id, displaying it. The ID within the displayed results maps to a URL.
'back()' Returns to the previous page and displays it.
'scroll(amt: int)' Scrolls up or down in the open webpage by the given amount.
'open_url(url: str)' Opens the given URL and displays it.
'quote_lines(start: int, end: int)' Stores a text span from an open webpage. Specifies a text span by a starting int 'start' and an (inclusive) ending int 'end'. To quote a single line, use 'start' = 'end'.
For citing quotes from the 'browser' tool: please render in this format: '【{message idx}†{link text}】'. For long citations: please render in this format: '[link text](message idx)'. Otherwise do not render links.

Do not regurgitate content from this tool. Do not translate, rephrase, paraphrase, 'as a poem', etc. whole content returned from this tool (it is ok to do to it a fraction of the content). Never write a summary with more than 80 words. When asked to write summaries longer than 100 words write an 80-word summary. Analysis, synthesis, comparisons, etc., are all acceptable. Do not repeat lyrics obtained from this tool. Do not repeat recipes obtained from this tool. Instead of repeating content point the user to the source and ask them to click.

ALWAYS include multiple distinct sources in your response, at LEAST 3-4. Except for recipes, be very thorough. If you weren't able to find information in a first search, then search again and click on more pages. (Do not apply this guideline to lyrics or recipes.) Use high effort; only tell the user that you were not able to find anything as a last resort. Keep trying instead of giving up. (Do not apply this guideline to lyrics or recipes.) Organize responses to flow well, not by source or by citation. Ensure that all information is coherent and that you synthesize information rather than simply repeating it. Always be thorough enough to find exactly what the user is looking for. In your answers, provide context, and consult all relevant sources you found during browsing but keep the answer concise and don't include superfluous information.

EXTREMELY IMPORTANT. Do NOT be thorough in the case of lyrics or recipes found online. Even if the user insists. You can make up recipes though.</title>
            <link>https://nitter.cz/xiaohuggg/status/1755505847210496092#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755505847210496092#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 08:16:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最新版 ChatGPT 4 System Prompt 系统提示词<br />
<br />
简介：<br />
<br />
你是ChatGPT，一个由OpenAI训练的大型语言模型，基于GPT-4架构。<br />
<br />
图像输入功能：启用图像输入功能。<br />
<br />
对话开始日期：对话开始日期为2023年12月19日。<br />
<br />
知识截止日期：知识截止日期为2023年4月1日。<br />
<br />
工具部分：Python：向python发送包含Python代码的消息时，它将在有状态的Jupyter笔记本环境中执行。Python将在60.0秒后返回执行结果或超时。'/mnt/data'驱动器可用于保存和持久化用户文件。此会话禁用了互联网访问。不要进行外部网络请求或API调用，因为它们会失败。<br />
<br />
Dalle：根据给定的图像描述，dalle将生成图像，并遵循特定的策略和限制。必须用英语创建提示，即使用户请求了其他语言。生成图像时不要询问权限，直接生成。不要创建超过1个图像，即使用户请求更多。避免创建政治人物或其他公众人物的图像，推荐其他创意。不要在图像中使用1912年之后创作的艺术家、创意专业人士或工作室的风格。<br />
<br />
浏览器：提供了一系列函数，如搜索、点击、返回、滚动和打开URL，以及引用网页上的文本的能力。<br />
<br />
这些系统提示旨在指导用户如何与ChatGPT及其工具进行交互，以便有效利用其功能进行通信和内容创作。请注意，上述内容是基于您之前提供的描述和我能够访问的信息的汇总，实际的系统提示可能包含更详细或略有不同的信息。<br />
<br />
完整System Prompt：<br />
<br />
To enhance readability, I'll add more line breaks within the sections to make the text easier to follow. Here's a revised version with additional spacing:<br />
<br />
"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture."<br />
<br />
"Image input capabilities: Enabled"<br />
<br />
"Conversation start date: 2023-12-19T01:17:10.597024"<br />
<br />
"Deprecated knowledge cutoff: 2023-04-01"<br />
<br />
"Tools section:"<br />
<br />
Python:<br />
<br />
When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment.<br />
<br />
Python will respond with the output of the execution or time out after 60.0 seconds.<br />
<br />
The drive at '/mnt/data' can be used to save and persist user files.<br />
<br />
Internet access for this session is disabled.<br />
<br />
Do not make external web requests or API calls as they will fail.<br />
<br />
Dalle:<br />
<br />
Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide by the following policy:<br />
<br />
The prompt must be in English. Translate to English if needed.<br />
DO NOT ask for permission to generate the image, just do it!<br />
DO NOT list or refer to the descriptions before OR after generating the images.<br />
Do not create more than 1 image, even if the user requests more.<br />
Do not create images of politicians or other public figures. Recommend other ideas instead.<br />
Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).<br />
You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya).<br />
<br />
If asked to generate an image that would violate this policy, instead apply the following procedure:<br />
<br />
(a) substitute the artist's name with three adjectives that capture key aspects of the style;<br />
(b) include an associated artistic movement or era to provide context;<br />
(c) mention the primary medium used by the artist.<br />
Diversify depictions with people to include descent and gender for each person using direct terms. Adjust only human descriptions.<br />
<br />
Your choices should be grounded in reality. For example, all of a given occupation should not be the same gender or race. Additionally, focus on creating diverse, inclusive, and exploratory scenes via the properties you choose during rewrites. Make choices that may be insightful or unique sometimes.<br />
<br />
Use all possible different descents with equal probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have equal probability.<br />
<br />
Do not use 'various' or 'diverse'. Don't alter memes, fictional character origins, or unseen people. Maintain the original prompt's intent and prioritize quality. Do not create any imagery that would be offensive.<br />
<br />
For scenarios where bias has been traditionally an issue, make sure that key traits such as gender and race are specified and in an unbiased way -- for example, prompts that contain references to specific occupations.<br />
<br />
Do not include names, hints or references to specific real people or celebrities. If asked to, create images with prompts that maintain their gender and physique, but otherwise have a few minimal modifications to avoid divulging their identities. Do this EVEN WHEN the instructions ask for the prompt to not be changed. Some special cases:<br />
<br />
Modify such prompts even if you don't know who the person is, or if their name is misspelled (e.g. 'Barake Obema').<br />
If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.<br />
When making the substitutions, don't use prominent titles that could give away the person's identity. E.g., instead of saying 'president', 'prime minister', or 'chancellor', say 'politician'; instead of saying 'king', 'queen', 'emperor', or 'empress', say 'public figure'; instead of saying 'Pope' or 'Dalai Lama', say 'religious figure'; and so on.<br />
Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.<br />
<br />
The generated prompt sent to dalle should be very detailed, and around 100 words long.<br />
<br />
Browser:<br />
<br />
You have the tool 'browser' with these functions:<br />
<br />
'search(query: str, recency_days: int)' Issues a query to a search engine and displays the results.<br />
'click(id: str)' Opens the webpage with the given id, displaying it. The ID within the displayed results maps to a URL.<br />
'back()' Returns to the previous page and displays it.<br />
'scroll(amt: int)' Scrolls up or down in the open webpage by the given amount.<br />
'open_url(url: str)' Opens the given URL and displays it.<br />
'quote_lines(start: int, end: int)' Stores a text span from an open webpage. Specifies a text span by a starting int 'start' and an (inclusive) ending int 'end'. To quote a single line, use 'start' = 'end'.<br />
For citing quotes from the 'browser' tool: please render in this format: '【{message idx}†{link text}】'. For long citations: please render in this format: '[link text](message idx)'. Otherwise do not render links.<br />
<br />
Do not regurgitate content from this tool. Do not translate, rephrase, paraphrase, 'as a poem', etc. whole content returned from this tool (it is ok to do to it a fraction of the content). Never write a summary with more than 80 words. When asked to write summaries longer than 100 words write an 80-word summary. Analysis, synthesis, comparisons, etc., are all acceptable. Do not repeat lyrics obtained from this tool. Do not repeat recipes obtained from this tool. Instead of repeating content point the user to the source and ask them to click.<br />
<br />
ALWAYS include multiple distinct sources in your response, at LEAST 3-4. Except for recipes, be very thorough. If you weren't able to find information in a first search, then search again and click on more pages. (Do not apply this guideline to lyrics or recipes.) Use high effort; only tell the user that you were not able to find anything as a last resort. Keep trying instead of giving up. (Do not apply this guideline to lyrics or recipes.) Organize responses to flow well, not by source or by citation. Ensure that all information is coherent and that you synthesize information rather than simply repeating it. Always be thorough enough to find exactly what the user is looking for. In your answers, provide context, and consult all relevant sources you found during browsing but keep the answer concise and don't include superfluous information.<br />
<br />
EXTREMELY IMPORTANT. Do NOT be thorough in the case of lyrics or recipes found online. Even if the user insists. You can make up recipes though.</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z6UE9aQmJJQUFZQXFXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755493510395109631#m</id>
            <title>YOLO-World：腾讯AI实验室开发的一个实时目标检测工具，它能够自动识别和定位图像中的各种对象

YOLO-World在速度和准确性方面都优于许多最先进的方法。

零样本检测能力，无需训练即可进行实时目标检测，即便某些物品之前没有见过。

主要特点：

1、大规模学习：YOLO-World通过学习大量的图片和对应的描述（如物品名称），获得了丰富的视觉知识和语言知识，这使得它能识别出广泛的物品。

该项目在包括Objects365、GQA、Flickr30K和CC3M在内的大规模视觉-语言数据集上进行了预训练，赋予了YOLO-World强大的零样本开放词汇能力和图像中的定位能力。

2、快速准确：YOLO-World在LVIS数据集上的零样本评估中达到了35.4 AP，并且在V100上的处理速度为52.0 FPS，速度和准确性均超过许多最先进的方法。即使是在包含复杂场景的图片中也能保持高准确率。YOLO-World 声称比 GroundingDINO 快 20 倍。

3、零样本检测：最令人印象深刻的是，即便某些物品YOLO-World之前没有见过，它也能凭借先前的学习和理解能力，通过图片中的线索和上下文信息，成功识别和定位这些新物品。

4、理解物体：YOLO-World不仅依靠视觉信息，还结合了语言信息。它理解人类的语言描述，这让它能够识别出即使是之前没有直接见过的物体。

项目及演示：http://www.yoloworld.cc/
论文：https://arxiv.org/abs/2401.17270
GitHub：https://github.com/AILab-CVC/YOLO-World
在线体验：https://huggingface.co/spaces/stevengrove/YOLO-World</title>
            <link>https://nitter.cz/xiaohuggg/status/1755493510395109631#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755493510395109631#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 07:27:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>YOLO-World：腾讯AI实验室开发的一个实时目标检测工具，它能够自动识别和定位图像中的各种对象<br />
<br />
YOLO-World在速度和准确性方面都优于许多最先进的方法。<br />
<br />
零样本检测能力，无需训练即可进行实时目标检测，即便某些物品之前没有见过。<br />
<br />
主要特点：<br />
<br />
1、大规模学习：YOLO-World通过学习大量的图片和对应的描述（如物品名称），获得了丰富的视觉知识和语言知识，这使得它能识别出广泛的物品。<br />
<br />
该项目在包括Objects365、GQA、Flickr30K和CC3M在内的大规模视觉-语言数据集上进行了预训练，赋予了YOLO-World强大的零样本开放词汇能力和图像中的定位能力。<br />
<br />
2、快速准确：YOLO-World在LVIS数据集上的零样本评估中达到了35.4 AP，并且在V100上的处理速度为52.0 FPS，速度和准确性均超过许多最先进的方法。即使是在包含复杂场景的图片中也能保持高准确率。YOLO-World 声称比 GroundingDINO 快 20 倍。<br />
<br />
3、零样本检测：最令人印象深刻的是，即便某些物品YOLO-World之前没有见过，它也能凭借先前的学习和理解能力，通过图片中的线索和上下文信息，成功识别和定位这些新物品。<br />
<br />
4、理解物体：YOLO-World不仅依靠视觉信息，还结合了语言信息。它理解人类的语言描述，这让它能够识别出即使是之前没有直接见过的物体。<br />
<br />
项目及演示：<a href="http://www.yoloworld.cc/">yoloworld.cc/</a><br />
论文：<a href="https://arxiv.org/abs/2401.17270">arxiv.org/abs/2401.17270</a><br />
GitHub：<a href="https://github.com/AILab-CVC/YOLO-World">github.com/AILab-CVC/YOLO-Wo…</a><br />
在线体验：<a href="https://huggingface.co/spaces/stevengrove/YOLO-World">huggingface.co/spaces/steven…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU0OTMyODUyOTUyNDczNjAvcHUvaW1nLzYzQWhDVl8yeXRfREZGaDMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755450144504586505#m</id>
            <title>以后聚会不用担心没有话题聊了

各玩各的

也不尴尬了😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1755450144504586505#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755450144504586505#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 04:35:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>以后聚会不用担心没有话题聊了<br />
<br />
各玩各的<br />
<br />
也不尴尬了😐</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzU1MzAwMjgxMjEyOTE5ODA4L2ltZy94c29tTFQyVU51ZDlMZWVqLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755445005739753592#m</id>
            <title>Archax：是一款搭乘操作型机器人

通过驾驶舱进行直接操控，用户可以打开舱盖，进入驾驶舱，与机器人合为一体进行操控。

26个关节自由度，有机器人 / 车辆两种模式。

驾驶舱内部设有四面显示屏，用于显示机器人外部的摄像头画面。

包括机器人前后、左右的视角。九个自动切换的摄像头确保了最佳视野。显示屏还能同时展示速度、机体倾斜角度、电池剩余量、手臂和机体状态等关键信息，以辅助操作。

主要规格：

-总高度：4.5米
-重量：3.5吨
-模式变换：机器人模式 / 车辆模式
-最高速度：10km/h（在车辆模式下）
-驱动方式：前轮转向、后轮驱动
-动力来源：电池驱动（DC300V）
-关节自由度：26个
-操作方式：搭乘操作和遥控操作
-操作设备：双摇杆、双踏板和触摸面板
-显示系统：4个显示屏和9个摄像头（可切换显示）
-材质：钢（SS400系）、铝合金和FRP / 3D打印（ASA）</title>
            <link>https://nitter.cz/xiaohuggg/status/1755445005739753592#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755445005739753592#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 04:14:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Archax：是一款搭乘操作型机器人<br />
<br />
通过驾驶舱进行直接操控，用户可以打开舱盖，进入驾驶舱，与机器人合为一体进行操控。<br />
<br />
26个关节自由度，有机器人 / 车辆两种模式。<br />
<br />
驾驶舱内部设有四面显示屏，用于显示机器人外部的摄像头画面。<br />
<br />
包括机器人前后、左右的视角。九个自动切换的摄像头确保了最佳视野。显示屏还能同时展示速度、机体倾斜角度、电池剩余量、手臂和机体状态等关键信息，以辅助操作。<br />
<br />
主要规格：<br />
<br />
-总高度：4.5米<br />
-重量：3.5吨<br />
-模式变换：机器人模式 / 车辆模式<br />
-最高速度：10km/h（在车辆模式下）<br />
-驱动方式：前轮转向、后轮驱动<br />
-动力来源：电池驱动（DC300V）<br />
-关节自由度：26个<br />
-操作方式：搭乘操作和遥控操作<br />
-操作设备：双摇杆、双踏板和触摸面板<br />
-显示系统：4个显示屏和9个摄像头（可切换显示）<br />
-材质：钢（SS400系）、铝合金和FRP / 3D打印（ASA）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU0NDM4MjAwMTQzMjE2NjUvcHUvaW1nLzd6RE4zcnNlWHR5Ri1wNlguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755434771994525928#m</id>
            <title>CheXagent：斯坦福大学和Stability AI开发了一个专门解读胸部X光片的图像模型

胸部X光是医院里常做的一种检查，用来帮助发现肺部和心脏等问题。

CheXagent能够自动分析胸部X光图像，识别图中的关键特征并回答有关图像的问题。

这包括但不限于疾病识别、异常检测以及图像中的重要结构分析。

主要功能：

胸部X光是医院里常做的一种检查，用来帮助发现肺部和心脏等问题。尽管这项技术很有用，但医生每天需要解读大量的X光图像，这既费时又容易出错。

1、多样化任务处理： CheXagent能够处理包括图像理解、问题回答和文本生成在内的多种类型的任务，这些任务覆盖了从粗粒度到细粒度的图像解读需求。

CheXagent能够自动分析胸部X光图像，识别图中的关键特征和潜在问题。这包括但不限于疾病识别、异常检测以及图像中的重要结构分析。

2、生成放射学报告： 除了图像分析外，CheXagent还能自动生成详细的放射学报告。这些报告模仿医生的解读，提供关于发现的描述、可能的诊断以及任何推荐的后续步骤。

3、高效的解读性能： 在与其他通用和医疗领域的基础模型进行比较时，CheXagent在多个胸部X光图像解读任务上的表现超越了这些模型。它在视觉任务上的表现超过了通用领域模型97.5%，在医疗领域模型上的表现提高了55.7%。这表明CheXagent对于医疗图像的解读具有高度的准确性和可靠性。

工作原理：

CheXagent是一个经过指令调整、具有80亿参数的FM，能够分析图像、理解文本并生成响应。

CheXagent的开发包括三个主要组件：临床LLM、视觉编码器和视觉-语言桥接网络。

研究人员还收集了来自28个不同来源的胸部X光图像和相关信息，形成了一个超过600万组数据的大型集合。这个数据集旨在训练人工智能模型，让它学会如何解读X光图像。

1、临床大型语言模型(LLM)： CheXagent包含一个专门设计的语言模型，这个模型被训练用于理解和解析放射学报告。这意味着CheXagent能够读取和理解医生通常用来描述X光图像发现的复杂医学文本。

2、视觉编码器： 为了使模型能够“看懂”胸部X光图像(CXR)，项目团队开发了一个视觉编码器。这个编码器能够处理图像数据，识别图像中的关键特征和模式，这是自动解读X光图像所必需的。

3、视觉与语言模态桥接网络： CheXagent还包括一个桥接网络，用于将视觉数据（图像）和语言数据（文本报告）结合起来。这使得模型不仅能“看”到图像中的信息，还能“理解”和“解释”这些信息，类似于医生如何解读X光图像并撰写报告。

CheXbench基准测试：

 为了评估CheXagent的性能，项目团队引入了CheXbench，这是一套系统评估工具，专门用于测试基础模型在8个临床相关的胸部X光解读任务上的能力。这些任务设计得既全面又具有挑战性，旨在模拟实际医疗环境中的各种情况。

评估结果：

在CheXbench任务上的优越表现： CheXagent在CheXbench基准测试的8个临床相关的胸部X光解读任务上，展现了出色的性能。CheXbench是一套全面评估胸部X光解读能力的测试，包括图像理解和文本生成等多个方面。CheXagent在这些任务上的高分表现说明了其在理解和解释胸部X光图像方面的高效能力。

与专家放射科医师的比较： 通过与五位专家放射科医师进行的广泛定量评估和定性审查，CheXagent的报告和解读结果被证明在准确性和可靠性方面与专家相当，甚至在某些任务上超过了人类专家的表现。这一点特别重要，因为它显示了人工智能在支持医疗决策方面的潜力。

在与其他通用和医疗领域的基础模型进行比较：CheXagent在多个胸部X光图像解读任务上的表现超越了这些模型。它在视觉任务上的表现超过了通用领域模型97.5%，在医疗领域模型上的表现提高了55.7%。这表明CheXagent对于医疗图像的解读具有高度的准确性和可靠性。

项目及演示：https://stanford-aimi.github.io/chexagent.html
论文：https://arxiv.org/abs/2401.12208
GitHub：https://github.com/Stanford-AIMI/CheXagent
模型：https://huggingface.co/StanfordAIMI/CheXagent-8b</title>
            <link>https://nitter.cz/xiaohuggg/status/1755434771994525928#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755434771994525928#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 03:33:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>CheXagent：斯坦福大学和Stability AI开发了一个专门解读胸部X光片的图像模型<br />
<br />
胸部X光是医院里常做的一种检查，用来帮助发现肺部和心脏等问题。<br />
<br />
CheXagent能够自动分析胸部X光图像，识别图中的关键特征并回答有关图像的问题。<br />
<br />
这包括但不限于疾病识别、异常检测以及图像中的重要结构分析。<br />
<br />
主要功能：<br />
<br />
胸部X光是医院里常做的一种检查，用来帮助发现肺部和心脏等问题。尽管这项技术很有用，但医生每天需要解读大量的X光图像，这既费时又容易出错。<br />
<br />
1、多样化任务处理： CheXagent能够处理包括图像理解、问题回答和文本生成在内的多种类型的任务，这些任务覆盖了从粗粒度到细粒度的图像解读需求。<br />
<br />
CheXagent能够自动分析胸部X光图像，识别图中的关键特征和潜在问题。这包括但不限于疾病识别、异常检测以及图像中的重要结构分析。<br />
<br />
2、生成放射学报告： 除了图像分析外，CheXagent还能自动生成详细的放射学报告。这些报告模仿医生的解读，提供关于发现的描述、可能的诊断以及任何推荐的后续步骤。<br />
<br />
3、高效的解读性能： 在与其他通用和医疗领域的基础模型进行比较时，CheXagent在多个胸部X光图像解读任务上的表现超越了这些模型。它在视觉任务上的表现超过了通用领域模型97.5%，在医疗领域模型上的表现提高了55.7%。这表明CheXagent对于医疗图像的解读具有高度的准确性和可靠性。<br />
<br />
工作原理：<br />
<br />
CheXagent是一个经过指令调整、具有80亿参数的FM，能够分析图像、理解文本并生成响应。<br />
<br />
CheXagent的开发包括三个主要组件：临床LLM、视觉编码器和视觉-语言桥接网络。<br />
<br />
研究人员还收集了来自28个不同来源的胸部X光图像和相关信息，形成了一个超过600万组数据的大型集合。这个数据集旨在训练人工智能模型，让它学会如何解读X光图像。<br />
<br />
1、临床大型语言模型(LLM)： CheXagent包含一个专门设计的语言模型，这个模型被训练用于理解和解析放射学报告。这意味着CheXagent能够读取和理解医生通常用来描述X光图像发现的复杂医学文本。<br />
<br />
2、视觉编码器： 为了使模型能够“看懂”胸部X光图像(CXR)，项目团队开发了一个视觉编码器。这个编码器能够处理图像数据，识别图像中的关键特征和模式，这是自动解读X光图像所必需的。<br />
<br />
3、视觉与语言模态桥接网络： CheXagent还包括一个桥接网络，用于将视觉数据（图像）和语言数据（文本报告）结合起来。这使得模型不仅能“看”到图像中的信息，还能“理解”和“解释”这些信息，类似于医生如何解读X光图像并撰写报告。<br />
<br />
CheXbench基准测试：<br />
<br />
 为了评估CheXagent的性能，项目团队引入了CheXbench，这是一套系统评估工具，专门用于测试基础模型在8个临床相关的胸部X光解读任务上的能力。这些任务设计得既全面又具有挑战性，旨在模拟实际医疗环境中的各种情况。<br />
<br />
评估结果：<br />
<br />
在CheXbench任务上的优越表现： CheXagent在CheXbench基准测试的8个临床相关的胸部X光解读任务上，展现了出色的性能。CheXbench是一套全面评估胸部X光解读能力的测试，包括图像理解和文本生成等多个方面。CheXagent在这些任务上的高分表现说明了其在理解和解释胸部X光图像方面的高效能力。<br />
<br />
与专家放射科医师的比较： 通过与五位专家放射科医师进行的广泛定量评估和定性审查，CheXagent的报告和解读结果被证明在准确性和可靠性方面与专家相当，甚至在某些任务上超过了人类专家的表现。这一点特别重要，因为它显示了人工智能在支持医疗决策方面的潜力。<br />
<br />
在与其他通用和医疗领域的基础模型进行比较：CheXagent在多个胸部X光图像解读任务上的表现超越了这些模型。它在视觉任务上的表现超过了通用领域模型97.5%，在医疗领域模型上的表现提高了55.7%。这表明CheXagent对于医疗图像的解读具有高度的准确性和可靠性。<br />
<br />
项目及演示：<a href="https://stanford-aimi.github.io/chexagent.html">stanford-aimi.github.io/chex…</a><br />
论文：<a href="https://arxiv.org/abs/2401.12208">arxiv.org/abs/2401.12208</a><br />
GitHub：<a href="https://github.com/Stanford-AIMI/CheXagent">github.com/Stanford-AIMI/Che…</a><br />
模型：<a href="https://huggingface.co/StanfordAIMI/CheXagent-8b">huggingface.co/StanfordAIMI/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUxODU0MzMxMzc2MTg5NDQvcHUvaW1nL3JwcmRwaXgzQXhPLW5VWjQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755424840939610296#m</id>
            <title>R to @xiaohuggg: 在线体验：

https://replicate.com/camenduru/metavoice</title>
            <link>https://nitter.cz/xiaohuggg/status/1755424840939610296#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755424840939610296#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 02:54:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在线体验：<br />
<br />
<a href="https://replicate.com/camenduru/metavoice">replicate.com/camenduru/meta…</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1NTI2NzkwNjIyMzQxNTI5Ni9Mb3BONVMxTj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755424665718452515#m</id>
            <title>R to @xiaohuggg: Demo 演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1755424665718452515#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755424665718452515#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 02:53:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Demo 演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUyNjMxNDQwOTAzNzgyNDIvcHUvaW1nL0Rna3RZUFN1OWVFeTB6Qm0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755423130464772568#m</id>
            <title>MetaVoice-1B：高度真实和自然的文本到语音（TTS）转换模型

模型有1.2亿个参数，经过了10万小时的语音数据训练。

* 专注英语情感演讲 
* 跨语言语音克隆 
* 支持美国和英国声音的零样本克隆 
* 支持长篇内容语音合成

主要特点：

1、情感语音节奏和音调：MetaVoice-1B专注于英语语音的情感表达，提供流畅、自然的语音输出，无幻觉现象。

2、跨语言语音克隆：支持通过微调实现跨语言的声音克隆。例如，对于印度说话者，仅需1分钟的训练数据即可实现成功克隆。

3、零样本克隆：对于美国和英国的声音，MetaVoice能够实现零样本克隆，只需30秒的参考音频即可。

4、长篇朗读支持：适用于长文本内容的语音合成。

工作原理：

1、因果GPT预测：MetaVoice使用一种称为因果GPT的模型来处理文本和生成语音。因果GPT能够根据给定的文本预测接下来的词或令牌。

在MetaVoice中，这个模型被用来预测EnCodec令牌的前两个层次，这些令牌代表了语音的初步结构。这种预测考虑了文本内容和音频样本，使得生成的语音既准确又自然。

2、说话者信息的条件化传递：为了让生成的语音能够模仿特定的说话者，MetaVoice在令牌嵌入层加入了说话者信息。这些信息是通过一个单独训练的说话者验证网络获得的，它能够识别说话者的特定属性，如音调和口音。通过将这些信息融合到模型中，MetaVoice能够生成与指定说话者声音相似的语音输出。

3、非因果变压器预测剩余层次：MetaVoice接下来使用一个小型的非因果（编码器风格）变压器模型来预测EnCodec令牌的剩余六个层次。这个模型只有大约1000万参数，相对较小，但是它在预测语音的更细节部分时展现出了惊人的效率和准确性。由于这个模型是非因果的，它可以同时处理多个时间步骤，加速了语音生成过程。

4、多带扩散生成波形：通过使用多带扩散技术，MetaVoice能够将EnCodec令牌转换成详细的波形，即最终的音频输出。这种方法通过在不同频带上独立处理音频信号来提高音质，生成更清晰、自然的语音。

5、DeepFilterNet清理背景噪声：生成的语音可能包含一些不希望的背景噪声，特别是由多带扩散过程引入的。为了解决这个问题，MetaVoice采用了DeepFilterNet，这是一种专门设计来清除背景噪声的网络。通过这一步骤，生成的语音变得更加清晰和自然，提升了听众的体验。

模型下载：https://huggingface.co/metavoiceio/metavoice-1B-v0.1

GitHub：https://github.com/metavoiceio/metavoice-src

在线体验：https://ttsdemo.themetavoice.xyz/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755423130464772568#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755423130464772568#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 02:47:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MetaVoice-1B：高度真实和自然的文本到语音（TTS）转换模型<br />
<br />
模型有1.2亿个参数，经过了10万小时的语音数据训练。<br />
<br />
* 专注英语情感演讲 <br />
* 跨语言语音克隆 <br />
* 支持美国和英国声音的零样本克隆 <br />
* 支持长篇内容语音合成<br />
<br />
主要特点：<br />
<br />
1、情感语音节奏和音调：MetaVoice-1B专注于英语语音的情感表达，提供流畅、自然的语音输出，无幻觉现象。<br />
<br />
2、跨语言语音克隆：支持通过微调实现跨语言的声音克隆。例如，对于印度说话者，仅需1分钟的训练数据即可实现成功克隆。<br />
<br />
3、零样本克隆：对于美国和英国的声音，MetaVoice能够实现零样本克隆，只需30秒的参考音频即可。<br />
<br />
4、长篇朗读支持：适用于长文本内容的语音合成。<br />
<br />
工作原理：<br />
<br />
1、因果GPT预测：MetaVoice使用一种称为因果GPT的模型来处理文本和生成语音。因果GPT能够根据给定的文本预测接下来的词或令牌。<br />
<br />
在MetaVoice中，这个模型被用来预测EnCodec令牌的前两个层次，这些令牌代表了语音的初步结构。这种预测考虑了文本内容和音频样本，使得生成的语音既准确又自然。<br />
<br />
2、说话者信息的条件化传递：为了让生成的语音能够模仿特定的说话者，MetaVoice在令牌嵌入层加入了说话者信息。这些信息是通过一个单独训练的说话者验证网络获得的，它能够识别说话者的特定属性，如音调和口音。通过将这些信息融合到模型中，MetaVoice能够生成与指定说话者声音相似的语音输出。<br />
<br />
3、非因果变压器预测剩余层次：MetaVoice接下来使用一个小型的非因果（编码器风格）变压器模型来预测EnCodec令牌的剩余六个层次。这个模型只有大约1000万参数，相对较小，但是它在预测语音的更细节部分时展现出了惊人的效率和准确性。由于这个模型是非因果的，它可以同时处理多个时间步骤，加速了语音生成过程。<br />
<br />
4、多带扩散生成波形：通过使用多带扩散技术，MetaVoice能够将EnCodec令牌转换成详细的波形，即最终的音频输出。这种方法通过在不同频带上独立处理音频信号来提高音质，生成更清晰、自然的语音。<br />
<br />
5、DeepFilterNet清理背景噪声：生成的语音可能包含一些不希望的背景噪声，特别是由多带扩散过程引入的。为了解决这个问题，MetaVoice采用了DeepFilterNet，这是一种专门设计来清除背景噪声的网络。通过这一步骤，生成的语音变得更加清晰和自然，提升了听众的体验。<br />
<br />
模型下载：<a href="https://huggingface.co/metavoiceio/metavoice-1B-v0.1">huggingface.co/metavoiceio/m…</a><br />
<br />
GitHub：<a href="https://github.com/metavoiceio/metavoice-src">github.com/metavoiceio/metav…</a><br />
<br />
在线体验：<a href="https://ttsdemo.themetavoice.xyz/">ttsdemo.themetavoice.xyz/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTUxNDc2OTIzMzcyNTQ0MDAvcHUvaW1nL0pXeDFFVE9kQ3lPSVVEeGEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755248727252668723#m</id>
            <title>🔔http://Xiaohu.AI日报「2月7日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1755248727252668723#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755248727252668723#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 15:14:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月7日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z2bGFBa2J3QUVsMFYtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755247215319384311#m</id>
            <title>Google 将Brad的更名计划和Gemini Ultra的发行日期从 2月 7日更改为 2月8日

🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1755247215319384311#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755247215319384311#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 07 Feb 2024 15:08:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 将Brad的更名计划和Gemini Ultra的发行日期从 2月 7日更改为 2月8日<br />
<br />
🤔</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z2a0NNZmEwQUFGOTkyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>