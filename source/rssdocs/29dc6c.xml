<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734177740826480673#m</id>
            <title>翻译了一下视频，方便大家能看懂

主要翻译了后半段演示的部分

确实和Gemini的演示水平一样，哈哈哈哈

只是没有加速，这才是正常的演示示范，Google如果这样正常演示其实也是很惊讶的，非要作假😂

想实现视频里效果的你们自己也可以试试，小哥分享了代码：https://github.com/gregsadetsky/sagittarius</title>
            <link>https://nitter.cz/xiaohuggg/status/1734177740826480673#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734177740826480673#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:46:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>翻译了一下视频，方便大家能看懂<br />
<br />
主要翻译了后半段演示的部分<br />
<br />
确实和Gemini的演示水平一样，哈哈哈哈<br />
<br />
只是没有加速，这才是正常的演示示范，Google如果这样正常演示其实也是很惊讶的，非要作假😂<br />
<br />
想实现视频里效果的你们自己也可以试试，小哥分享了代码：<a href="https://github.com/gregsadetsky/sagittarius">github.com/gregsadetsky/sagi…</a></p>
<p><a href="https://nitter.cz/dotey/status/1734104301155262545#m">nitter.cz/dotey/status/1734104301155262545#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQxNzY4ODE2NzM2OTUyMzIvcHUvaW1nL0R1RXY3NzJUaDN5c0owbmsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734170307697721740#m</id>
            <title>这个挺有意思🤓

让物体在虚拟场景中按照特定轨迹移动

#Quset3</title>
            <link>https://nitter.cz/xiaohuggg/status/1734170307697721740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734170307697721740#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:16:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺有意思🤓<br />
<br />
让物体在虚拟场景中按照特定轨迹移动<br />
<br />
<a href="https://nitter.cz/search?q=%23Quset3">#Quset3</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQxMTg0MzE4ODMxNDUyMTYvcHUvaW1nL2hwRE5TRUR3NnlqSFdHYVcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</id>
            <title>Mixtral AI公布MoE 8x7B详细细节：

• 32k上下文。
• 支持英语、法语、意大利语、德语和西班牙语。
• 性能超过Llama 2系列和GPT3.5
• 在代码生成方面表现强劲。
• 在MT-Bench上达到8.3的分数。

技术细节：

•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。

•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。

详细内容：https://mistral.ai/news/mixtral-of-experts/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734139143259861185#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 09:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral AI公布MoE 8x7B详细细节：<br />
<br />
• 32k上下文。<br />
• 支持英语、法语、意大利语、德语和西班牙语。<br />
• 性能超过Llama 2系列和GPT3.5<br />
• 在代码生成方面表现强劲。<br />
• 在MT-Bench上达到8.3的分数。<br />
<br />
技术细节：<br />
<br />
•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。<br />
<br />
•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。<br />
<br />
详细内容：<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1733694954260901907#m">nitter.cz/xiaohuggg/status/1733694954260901907#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbU9ZcmE0QUFyS0V2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734105617982456270#m</id>
            <title>阿里又整活了

DreaMoving：能够生成目标身份在任何地方跳舞的视频。

你可以指定一个特定的人物（如名人、朋友或任何特定形象），让他正在任意背景或场景（可以是真实的地点如海滩、城市街道或任何虚构的场景）下跳舞。

更牛的是仅靠脸部照片或文字提示也能生成跳舞视频...

还可指定人物动作和姿势

DreaMoving是一个基于扩散模型的人类舞蹈视频生成框架。能够根据指导序列和简单的内容描述（仅文本提示、仅图像提示或文本和图像提示）生成高质量、高保真度的视频。

用户可以通过下几种输入内容来生成和控制视频：

1、文本提示：对视频内容的简单描述，比如场景设置、人物动作或任何特定的主题。例如，“一个女孩在海滩上跳舞”。

文本提示也可以用于描述视频中的背景环境。

2、参考图像：这些图像用于指定视频中人物的外观。例如，用户可以上传一个人脸图像来确保视频中的人物具有相同或类似的面部特征。

如果需要，还可以包括衣着或身体特征的图像。

3、姿势或深度序列：这是定义视频中人物动作的关键输入。姿势序列指定了人物在视频中的具体动作和姿态。
用户可以通过提供特定的舞蹈动作序列来控制视频中的舞蹈风格和动作。

4、可选的衣物图像：如果用户想要在视频中指定人物的服装样式，可以上传相关的衣物图像。

技术细节：

架构: DreaMoving 基于 Stable-Diffusion 模型构建，包括去噪 U-Net、视频控制网（Video ControlNet）和内容引导器（Content Guider）。

数据收集与预处理: 收集了约1000个高质量的人类舞蹈视频，经过剪辑和处理，得到约6000个短视频片段。

运动块: 为了提高时间一致性和运动保真度，将运动块集成到去噪 U-Net 和控制网中。

内容引导器: 设计用于控制生成视频的内容，包括人物外观和背景。使用图像提示精确引导人物外观，文本提示生成背景。

模型训练: 包括内容引导器训练、长帧预训练、视频控制网训练和表情微调。

模型推理: 输入包括文本提示、参考图像和姿势或深度序列。通过调整控制网和内容引导器中的参数来控制视频内容。

项目地址：https://dreamoving.github.io/dreamoving/

论文：https://arxiv.org/abs/2312.05107

代码还没有，是个很基础的演示，估计是为了抢流量...🤣</title>
            <link>https://nitter.cz/xiaohuggg/status/1734105617982456270#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734105617982456270#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:59:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里又整活了<br />
<br />
DreaMoving：能够生成目标身份在任何地方跳舞的视频。<br />
<br />
你可以指定一个特定的人物（如名人、朋友或任何特定形象），让他正在任意背景或场景（可以是真实的地点如海滩、城市街道或任何虚构的场景）下跳舞。<br />
<br />
更牛的是仅靠脸部照片或文字提示也能生成跳舞视频...<br />
<br />
还可指定人物动作和姿势<br />
<br />
DreaMoving是一个基于扩散模型的人类舞蹈视频生成框架。能够根据指导序列和简单的内容描述（仅文本提示、仅图像提示或文本和图像提示）生成高质量、高保真度的视频。<br />
<br />
用户可以通过下几种输入内容来生成和控制视频：<br />
<br />
1、文本提示：对视频内容的简单描述，比如场景设置、人物动作或任何特定的主题。例如，“一个女孩在海滩上跳舞”。<br />
<br />
文本提示也可以用于描述视频中的背景环境。<br />
<br />
2、参考图像：这些图像用于指定视频中人物的外观。例如，用户可以上传一个人脸图像来确保视频中的人物具有相同或类似的面部特征。<br />
<br />
如果需要，还可以包括衣着或身体特征的图像。<br />
<br />
3、姿势或深度序列：这是定义视频中人物动作的关键输入。姿势序列指定了人物在视频中的具体动作和姿态。<br />
用户可以通过提供特定的舞蹈动作序列来控制视频中的舞蹈风格和动作。<br />
<br />
4、可选的衣物图像：如果用户想要在视频中指定人物的服装样式，可以上传相关的衣物图像。<br />
<br />
技术细节：<br />
<br />
架构: DreaMoving 基于 Stable-Diffusion 模型构建，包括去噪 U-Net、视频控制网（Video ControlNet）和内容引导器（Content Guider）。<br />
<br />
数据收集与预处理: 收集了约1000个高质量的人类舞蹈视频，经过剪辑和处理，得到约6000个短视频片段。<br />
<br />
运动块: 为了提高时间一致性和运动保真度，将运动块集成到去噪 U-Net 和控制网中。<br />
<br />
内容引导器: 设计用于控制生成视频的内容，包括人物外观和背景。使用图像提示精确引导人物外观，文本提示生成背景。<br />
<br />
模型训练: 包括内容引导器训练、长帧预训练、视频控制网训练和表情微调。<br />
<br />
模型推理: 输入包括文本提示、参考图像和姿势或深度序列。通过调整控制网和内容引导器中的参数来控制视频内容。<br />
<br />
项目地址：<a href="https://dreamoving.github.io/dreamoving/">dreamoving.github.io/dreamov…</a><br />
<br />
论文：<a href="https://arxiv.org/abs/2312.05107">arxiv.org/abs/2312.05107</a><br />
<br />
代码还没有，是个很基础的演示，估计是为了抢流量...🤣</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwOTk1ODQyNjY0MDc5MzYvcHUvaW1nL2RyejRqMWpJQ3hLT1VQWkUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734098519655682497#m</id>
            <title>R to @xiaohuggg: 还可以根据照片生成动作视频

据说本周支持生成舞蹈视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1734098519655682497#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734098519655682497#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:31:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>还可以根据照片生成动作视频<br />
<br />
据说本周支持生成舞蹈视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwOTgzNzY3ODA5MTA1OTIvcHUvaW1nLzRXNnU0Y05yODJaalVHdVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734098211177222550#m</id>
            <title>R to @xiaohuggg: 生成产品广告视频演示</title>
            <link>https://nitter.cz/xiaohuggg/status/1734098211177222550#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734098211177222550#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:30:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>生成产品广告视频演示</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwOTgxOTEyNjY4MDc4MDgvcHUvaW1nL1JCbnpIZVlIbjExRUhXdjQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734097871346302988#m</id>
            <title>R to @xiaohuggg: 和几个主流AI视频工具的对比</title>
            <link>https://nitter.cz/xiaohuggg/status/1734097871346302988#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734097871346302988#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:28:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>和几个主流AI视频工具的对比</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0JEQXZmM2FVQUEtX1pVLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dCREF2ZjNhVUFBLV9aVS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734097702265503844#m</id>
            <title>R to @xiaohuggg: 生成了一堆鱼的效果感觉很不错</title>
            <link>https://nitter.cz/xiaohuggg/status/1734097702265503844#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734097702265503844#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:28:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>生成了一堆鱼的效果感觉很不错</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwOTc2MjcyNDYxNjE5MjAvcHUvaW1nL3BuOFFkU2NIcS1zOUpYbTMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734097567124959651#m</id>
            <title>AI 视频新秀NeverEnds迎来重大更新

@NeverEnds_ai 是一个能够从文本和图片生成视频的 AI 工具。

最近推出了 2.0 版本，增加了图生视频功能，并支持手机端体验。

- NeverEnds强调更具真实感和实用性的AI视频

- 强化AI视频的人物刻画

- 降低操作门槛 

简单测试了下，在动漫、广告视频上表现的很不错。

据说下一步将整合文字和语音功能，可以直接剪辑制作视频，打造低成本、低门槛地AI 视频全流程视频制作工具。

感兴趣的可以去他们Discord频道试试，

无限免费使用：https://discord.gg/eTTMZJ2USx

官网：https://neverends.life</title>
            <link>https://nitter.cz/xiaohuggg/status/1734097567124959651#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734097567124959651#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:27:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI 视频新秀NeverEnds迎来重大更新<br />
<br />
<a href="https://nitter.cz/NeverEnds_ai" title="NeverEnds | AI Video generation">@NeverEnds_ai</a> 是一个能够从文本和图片生成视频的 AI 工具。<br />
<br />
最近推出了 2.0 版本，增加了图生视频功能，并支持手机端体验。<br />
<br />
- NeverEnds强调更具真实感和实用性的AI视频<br />
<br />
- 强化AI视频的人物刻画<br />
<br />
- 降低操作门槛 <br />
<br />
简单测试了下，在动漫、广告视频上表现的很不错。<br />
<br />
据说下一步将整合文字和语音功能，可以直接剪辑制作视频，打造低成本、低门槛地AI 视频全流程视频制作工具。<br />
<br />
感兴趣的可以去他们Discord频道试试，<br />
<br />
无限免费使用：<a href="https://discord.gg/eTTMZJ2USx">discord.gg/eTTMZJ2USx</a><br />
<br />
官网：<a href="https://neverends.life">neverends.life</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwNDExNTMwNzM1MDQyNTYvcHUvaW1nL2FZYTFuWkNhYXkzbWkyblMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734063806090002616#m</id>
            <title>Google NotebookLM 开放访问了，不用排队了，但是最好是美国IP...

测了下之前提出的很多功能似乎都没实现，主要是一个类似和文档聊天的东西》

- 易于使用的检索增强生成（RAG）界面，可以上传多个来源

-支持上传PDF 、调用Google Drive 文件、复制文本内容进行对话

- AI回答的内容可随时保存为笔记

- 中文问答不太灵光，估计是模型Gemini，但你可以让它输出中文回答😅

- 可以跳转到并显示答案生成的引用段落，这对于快速检查输出非常有用。

- 对图像或视觉内容不太行。

访问：https://notebooklm.google.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734063806090002616#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734063806090002616#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 04:13:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google NotebookLM 开放访问了，不用排队了，但是最好是美国IP...<br />
<br />
测了下之前提出的很多功能似乎都没实现，主要是一个类似和文档聊天的东西》<br />
<br />
- 易于使用的检索增强生成（RAG）界面，可以上传多个来源<br />
<br />
-支持上传PDF 、调用Google Drive 文件、复制文本内容进行对话<br />
<br />
- AI回答的内容可随时保存为笔记<br />
<br />
- 中文问答不太灵光，估计是模型Gemini，但你可以让它输出中文回答😅<br />
<br />
- 可以跳转到并显示答案生成的引用段落，这对于快速检查输出非常有用。<br />
<br />
- 对图像或视觉内容不太行。<br />
<br />
访问：<a href="https://notebooklm.google.com/">notebooklm.google.com/</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1679673732271575041#m">nitter.cz/xiaohuggg/status/1679673732271575041#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwNjM0Mzk3OTcxNzgzNjgvcHUvaW1nL1RvYjVIS3hYTERjMnZyRkwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734050473894916202#m</id>
            <title>从人类大脑活动重建图像 准确率高达75%

以往，从大脑活动中重建图像仅在被试者实际看到图像时，或者当图像类型（如面孔、或简单图形）被指定时才成为可能。

来自日本国立量子科学研究所（QST）和其他机构的研究团队证实，仅凭思考、想象就能在一定程度上重建各种图像，如风景和复杂图形。

主要内容：

研究人员首先记录被试者在功能性磁共振成像（fMRI）机器中观看1200张不同图像时的大脑活动。

同时，通过让AI识别这些图像，创建了包含约613万个因素（如颜色、形状和纹理）的“评分图表”。

研究团队还制作了一个神经信号转换程序，该程序将大脑活动与评分图表匹配，当输入新的大脑活动时，创建新的评分图表。

接下来，被试者被展示了与1200张图像不同的图像，并在30分钟到一小时后被要求想象他们所看到的图像类型，同时测量他们的大脑活动。将记录输入神经信号转换器后，创建了评分图表。

这些图表被输入到另一个生成性AI程序中以重建图像，并经历了500步的修订过程。

这一过程最终实现了从重建的图像中准确识别原始图像的能力，它能够以90.7%的准确率识别被试者所看到的图像。

以及以75.6%的准确率识别被试者所想象的图像。远高于之前仅达到50.4%准确率的方法。

这项研究可能会导致新形式的沟通，无需使用言语。

QST研究员Kei Majima表示：“这是人类首次窥视另一个人的头脑的重大成就。我希望这项研究能促进对人类心灵的进一步理解。”

研究结果发表在国际科学期刊《神经网络》的在线版上，题为“从人类大脑活动重建心理图像：通过深度神经网络基于贝叶斯估计的心理成像神经解码”。

链接：https://www.sciencedirect.com/science/article/pii/S0893608023006470

### 研究方法

研究团队采用了一种增强的方法来从大脑活动中重建自然图像。这种方法的核心是利用贝叶斯估计框架，并结合语义信息的辅助。具体来说，他们的方法包括以下几个关键步骤：

1. **贝叶斯估计框架**：这是一种统计方法，用于根据已有数据（在这种情况下是大脑活动数据）来估计参数（即图像的特征）。贝叶斯估计能够结合先验知识和观测数据来生成后验知识，这在图像重建中非常有用。

2. **引入语义信息**：除了使用从大脑活动中直接解码的信息，研究团队还引入了语义信息来辅助图像重建。这意味着他们不仅考虑了大脑活动中的视觉信息，还考虑了与这些视觉信息相关的语义内容，如对象的类别、场景的含义等。

### 实验过程

实验过程分为几个阶段：

1. **数据收集**：使用功能性磁共振成像（fMRI）技术记录被试者在观看1200张不同图像时的大脑活动。fMRI能够捕捉大脑在视觉刺激下的活动变化。

2. **神经信号转换**：研究团队开发了一个神经信号转换程序，该程序能够将fMRI数据中记录的大脑活动与AI生成的评分图表相匹配。这些评分图表可能包含了图像的各种特征，如颜色、形状、纹理等。

3. **图像重建**：通过将大脑活动数据与评分图表相结合，研究团队能够重建出被试者所看到的图像。

### 实验结果

- **准确率提升**：与之前的方法相比，这种新框架在重建图像方面取得了显著的进步。具体来说，它能够以90.7%的准确率识别被试者所看到的图像，以及以75.6%的准确率识别被试者所想象的图像。

相比之下，之前的方法在识别被看到的图像方面的准确率为64.3%，在识别被想象的图像方面的准确率为50.4%。

- **机会准确率**：这里的机会准确率（50.0%）是指在没有任何信息的情况下，通过随机猜测所能达到的准确率。新方法的准确率远高于这个基线，表明其有效性。

这项研究在神经科学和人工智能领域具有重要意义，因为它不仅提高了从大脑活动中重建图像的准确率，而且还展示了结合语义信息可以显著提高重建质量的潜力。特别是在理解和解码人类心理图像方面。</title>
            <link>https://nitter.cz/xiaohuggg/status/1734050473894916202#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734050473894916202#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 03:20:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>从人类大脑活动重建图像 准确率高达75%<br />
<br />
以往，从大脑活动中重建图像仅在被试者实际看到图像时，或者当图像类型（如面孔、或简单图形）被指定时才成为可能。<br />
<br />
来自日本国立量子科学研究所（QST）和其他机构的研究团队证实，仅凭思考、想象就能在一定程度上重建各种图像，如风景和复杂图形。<br />
<br />
主要内容：<br />
<br />
研究人员首先记录被试者在功能性磁共振成像（fMRI）机器中观看1200张不同图像时的大脑活动。<br />
<br />
同时，通过让AI识别这些图像，创建了包含约613万个因素（如颜色、形状和纹理）的“评分图表”。<br />
<br />
研究团队还制作了一个神经信号转换程序，该程序将大脑活动与评分图表匹配，当输入新的大脑活动时，创建新的评分图表。<br />
<br />
接下来，被试者被展示了与1200张图像不同的图像，并在30分钟到一小时后被要求想象他们所看到的图像类型，同时测量他们的大脑活动。将记录输入神经信号转换器后，创建了评分图表。<br />
<br />
这些图表被输入到另一个生成性AI程序中以重建图像，并经历了500步的修订过程。<br />
<br />
这一过程最终实现了从重建的图像中准确识别原始图像的能力，它能够以90.7%的准确率识别被试者所看到的图像。<br />
<br />
以及以75.6%的准确率识别被试者所想象的图像。远高于之前仅达到50.4%准确率的方法。<br />
<br />
这项研究可能会导致新形式的沟通，无需使用言语。<br />
<br />
QST研究员Kei Majima表示：“这是人类首次窥视另一个人的头脑的重大成就。我希望这项研究能促进对人类心灵的进一步理解。”<br />
<br />
研究结果发表在国际科学期刊《神经网络》的在线版上，题为“从人类大脑活动重建心理图像：通过深度神经网络基于贝叶斯估计的心理成像神经解码”。<br />
<br />
链接：<a href="https://www.sciencedirect.com/science/article/pii/S0893608023006470">sciencedirect.com/science/ar…</a><br />
<br />
### 研究方法<br />
<br />
研究团队采用了一种增强的方法来从大脑活动中重建自然图像。这种方法的核心是利用贝叶斯估计框架，并结合语义信息的辅助。具体来说，他们的方法包括以下几个关键步骤：<br />
<br />
1. **贝叶斯估计框架**：这是一种统计方法，用于根据已有数据（在这种情况下是大脑活动数据）来估计参数（即图像的特征）。贝叶斯估计能够结合先验知识和观测数据来生成后验知识，这在图像重建中非常有用。<br />
<br />
2. **引入语义信息**：除了使用从大脑活动中直接解码的信息，研究团队还引入了语义信息来辅助图像重建。这意味着他们不仅考虑了大脑活动中的视觉信息，还考虑了与这些视觉信息相关的语义内容，如对象的类别、场景的含义等。<br />
<br />
### 实验过程<br />
<br />
实验过程分为几个阶段：<br />
<br />
1. **数据收集**：使用功能性磁共振成像（fMRI）技术记录被试者在观看1200张不同图像时的大脑活动。fMRI能够捕捉大脑在视觉刺激下的活动变化。<br />
<br />
2. **神经信号转换**：研究团队开发了一个神经信号转换程序，该程序能够将fMRI数据中记录的大脑活动与AI生成的评分图表相匹配。这些评分图表可能包含了图像的各种特征，如颜色、形状、纹理等。<br />
<br />
3. **图像重建**：通过将大脑活动数据与评分图表相结合，研究团队能够重建出被试者所看到的图像。<br />
<br />
### 实验结果<br />
<br />
- **准确率提升**：与之前的方法相比，这种新框架在重建图像方面取得了显著的进步。具体来说，它能够以90.7%的准确率识别被试者所看到的图像，以及以75.6%的准确率识别被试者所想象的图像。<br />
<br />
相比之下，之前的方法在识别被看到的图像方面的准确率为64.3%，在识别被想象的图像方面的准确率为50.4%。<br />
<br />
- **机会准确率**：这里的机会准确率（50.0%）是指在没有任何信息的情况下，通过随机猜测所能达到的准确率。新方法的准确率远高于这个基线，表明其有效性。<br />
<br />
这项研究在神经科学和人工智能领域具有重要意义，因为它不仅提高了从大脑活动中重建图像的准确率，而且还展示了结合语义信息可以显著提高重建质量的潜力。特别是在理解和解码人类心理图像方面。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JDVm9sMGJrQUEwNS1RLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JDVm9sNGE4QUFoSy1qLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JDVm9sMmJZQUE5NkpILmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734037408239571336#m</id>
            <title>AnythingLLM ：可以与任何内容聊天的私人ChatGPT

它能够把各种文档、资料或者内容转换成一种格式，让LLM（如ChatGPT）在聊天时可以引用这些内容。

然后你就可以用它来和各种文档、内容、资料聊天，支持多个用户同时使用，还可以设置谁能看或改哪些内容。

支持的多种LLM、嵌入器和向量数据库...

主要特点：

1.多用户支持和权限管理：允许多个用户同时使用，并可设置不同的权限。
2.支持多种文档类型：包括 PDF、TXT、DOCX 等。
3.简易的文档管理界面：通过用户界面管理向量数据库中的文档。
4.两种聊天模式：对话模式保留之前的问题和回答，查询模式则是简单的针对文档的问答。
5.聊天中的引用标注：链接到原始文档源和文本。
6.简单的技术栈，便于快速迭代。
7.100% 云部署就绪。
8.“自带你的 LLM”模式：可以选择使用商业或开源的 LLM。
9.高效的成本节约措施：对于大型文档，只需嵌入一次，比其他文档聊天机器人解决方案节省 90% 的成本。
10.完整的开发者 API：支持自定义集成。

支持的 LLM、嵌入器和向量数据库

•LLM：包括任何开源的 llama.cpp 兼容模型、OpenAI、Azure OpenAI、Anthropic ClaudeV2、LM Studio 和 LocalAi。
•嵌入器：AnythingLLM 原生嵌入器、OpenAI、Azure OpenAI、LM Studio 和 LocalAi。
•向量数据库：LanceDB（默认）、Pinecone、Chroma、Weaviate 和 QDrant。

技术概览

项目包含三个主要部分：

•collector：Python 工具，可快速将在线资源或本地文档转换为 LLM 可用格式。
•frontend：ViteJS + React 前端，用于创建和管理 LLM 可使用的所有内容。
•server：NodeJS + Express 服务器，处理所有向量数据库管理和 LLM 交互。

GitHub：https://github.com/Mintplex-Labs/anything-llm</title>
            <link>https://nitter.cz/xiaohuggg/status/1734037408239571336#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734037408239571336#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 02:28:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnythingLLM ：可以与任何内容聊天的私人ChatGPT<br />
<br />
它能够把各种文档、资料或者内容转换成一种格式，让LLM（如ChatGPT）在聊天时可以引用这些内容。<br />
<br />
然后你就可以用它来和各种文档、内容、资料聊天，支持多个用户同时使用，还可以设置谁能看或改哪些内容。<br />
<br />
支持的多种LLM、嵌入器和向量数据库...<br />
<br />
主要特点：<br />
<br />
1.多用户支持和权限管理：允许多个用户同时使用，并可设置不同的权限。<br />
2.支持多种文档类型：包括 PDF、TXT、DOCX 等。<br />
3.简易的文档管理界面：通过用户界面管理向量数据库中的文档。<br />
4.两种聊天模式：对话模式保留之前的问题和回答，查询模式则是简单的针对文档的问答。<br />
5.聊天中的引用标注：链接到原始文档源和文本。<br />
6.简单的技术栈，便于快速迭代。<br />
7.100% 云部署就绪。<br />
8.“自带你的 LLM”模式：可以选择使用商业或开源的 LLM。<br />
9.高效的成本节约措施：对于大型文档，只需嵌入一次，比其他文档聊天机器人解决方案节省 90% 的成本。<br />
10.完整的开发者 API：支持自定义集成。<br />
<br />
支持的 LLM、嵌入器和向量数据库<br />
<br />
•LLM：包括任何开源的 llama.cpp 兼容模型、OpenAI、Azure OpenAI、Anthropic ClaudeV2、LM Studio 和 LocalAi。<br />
•嵌入器：AnythingLLM 原生嵌入器、OpenAI、Azure OpenAI、LM Studio 和 LocalAi。<br />
•向量数据库：LanceDB（默认）、Pinecone、Chroma、Weaviate 和 QDrant。<br />
<br />
技术概览<br />
<br />
项目包含三个主要部分：<br />
<br />
•collector：Python 工具，可快速将在线资源或本地文档转换为 LLM 可用格式。<br />
•frontend：ViteJS + React 前端，用于创建和管理 LLM 可使用的所有内容。<br />
•server：NodeJS + Express 服务器，处理所有向量数据库管理和 LLM 交互。<br />
<br />
GitHub：<a href="https://github.com/Mintplex-Labs/anything-llm">github.com/Mintplex-Labs/any…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0JDRXJldGJVQUFfYVh3LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dCQ0VyZXRiVUFBX2FYdy5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734028007323701349#m</id>
            <title>3D高斯泼溅（Gaussian Splatting）这个技术似乎经历了一场“寒武纪大爆发”。

自2023年7月发表开创性论文以来，发展速度贼快，可以说是天天有研究成果、周周有产品上线...

@janusch_patas 和 @henrypearce4D 整理了一个3D高斯泼溅相关的优秀资源列表

收集了众多相关工具、资源和研究成果，包括：

1、生态系统和工具：

•PlayCanvas发布了一个在线散射编辑器SuperSplat。
•Luma AI发布了Unreal Engine 5插件和基于WebGL的Three.js视图库：Luma WebGL Library。
•Spline添加了高斯散射支持，配备简单的编辑工具。
•讨论了各种开源工具作者之间可能的“通用散射格式”。

2、研究论文：

•多篇论文探讨了高斯散射在3D编辑、实时点云重照明、逆渲染、数据压缩、防锯齿、辐射场模型压缩、高斯散射着色函数、可重照明的高斯编解码器头像、高效3D网格重建和高质量网格渲染等方面的应用。

3、Unity高斯散射项目：

•作者创建的Unity高斯散射项目意外地获得了1300多个GitHub星标。
•项目新增了对HDRP和URP渲染管线的支持，以及更细致的散射编辑工具。
•项目还包括一些与高斯散射无关但可能在其他地方有用的部分，如AMD FidelityFX GPU基数排序的Unity移植、K-means聚类的C#实现等。

对3D高斯泼溅（Gaussian Splatting）这个技术感兴趣的可以看看。

详细介绍：https://aras-p.info/blog/2023/12/08/Gaussian-explosion/

Awesome 3D Gaussian Splatting项目：

主要内容

•资源列表：包含了关于3D高斯散射的各种论文和开源资源。
•论文分类：涵盖了动态与变形、扩散、头像、SLAM、网格提取与物理、正则化与优化、编辑、渲染、压缩、语言嵌入等多个子领域。
•开源实现：提供了各种3D高斯散射的开源实现参考。
•工具和实用程序：包括游戏引擎、查看器、实用工具等。
•博客文章和教程视频：提供了关于3D高斯散射的博客文章和教程视频。

GitHub：https://github.com/MrNeRF/awesome-3D-gaussian-splatting</title>
            <link>https://nitter.cz/xiaohuggg/status/1734028007323701349#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734028007323701349#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 01:51:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3D高斯泼溅（Gaussian Splatting）这个技术似乎经历了一场“寒武纪大爆发”。<br />
<br />
自2023年7月发表开创性论文以来，发展速度贼快，可以说是天天有研究成果、周周有产品上线...<br />
<br />
<a href="https://nitter.cz/janusch_patas" title="MrNeRF">@janusch_patas</a> 和 <a href="https://nitter.cz/henrypearce4D" title="Henry Pearce">@henrypearce4D</a> 整理了一个3D高斯泼溅相关的优秀资源列表<br />
<br />
收集了众多相关工具、资源和研究成果，包括：<br />
<br />
1、生态系统和工具：<br />
<br />
•PlayCanvas发布了一个在线散射编辑器SuperSplat。<br />
•Luma AI发布了Unreal Engine 5插件和基于WebGL的Three.js视图库：Luma WebGL Library。<br />
•Spline添加了高斯散射支持，配备简单的编辑工具。<br />
•讨论了各种开源工具作者之间可能的“通用散射格式”。<br />
<br />
2、研究论文：<br />
<br />
•多篇论文探讨了高斯散射在3D编辑、实时点云重照明、逆渲染、数据压缩、防锯齿、辐射场模型压缩、高斯散射着色函数、可重照明的高斯编解码器头像、高效3D网格重建和高质量网格渲染等方面的应用。<br />
<br />
3、Unity高斯散射项目：<br />
<br />
•作者创建的Unity高斯散射项目意外地获得了1300多个GitHub星标。<br />
•项目新增了对HDRP和URP渲染管线的支持，以及更细致的散射编辑工具。<br />
•项目还包括一些与高斯散射无关但可能在其他地方有用的部分，如AMD FidelityFX GPU基数排序的Unity移植、K-means聚类的C#实现等。<br />
<br />
对3D高斯泼溅（Gaussian Splatting）这个技术感兴趣的可以看看。<br />
<br />
详细介绍：<a href="https://aras-p.info/blog/2023/12/08/Gaussian-explosion/">aras-p.info/blog/2023/12/08/…</a><br />
<br />
Awesome 3D Gaussian Splatting项目：<br />
<br />
主要内容<br />
<br />
•资源列表：包含了关于3D高斯散射的各种论文和开源资源。<br />
•论文分类：涵盖了动态与变形、扩散、头像、SLAM、网格提取与物理、正则化与优化、编辑、渲染、压缩、语言嵌入等多个子领域。<br />
•开源实现：提供了各种3D高斯散射的开源实现参考。<br />
•工具和实用程序：包括游戏引擎、查看器、实用工具等。<br />
•博客文章和教程视频：提供了关于3D高斯散射的博客文章和教程视频。<br />
<br />
GitHub：<a href="https://github.com/MrNeRF/awesome-3D-gaussian-splatting">github.com/MrNeRF/awesome-3D…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwMjc4NDc1MDM4ODQyODgvcHUvaW1nL0Jqei0zLUZOU0RwMTJENVMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734018030823489766#m</id>
            <title>？？？？😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1734018030823489766#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734018030823489766#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 01:11:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>？？？？😂</p>
<p><a href="https://nitter.cz/blizaine/status/1733892911534620927#m">nitter.cz/blizaine/status/1733892911534620927#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733830275254415844#m</id>
            <title>理想汽车发布Mind GPT多模态大模型

官方称Mind GPT是全自研的多模态认知大模型，它可以与汽车进行一个完美的融合，让每位家庭成员都能在车里体验到最好的AI。

按照国内大模型的惯例，毫无悬念

Mind GPT在中文大语言模型评测榜单C-EVAL和CMMLU榜单中双双取得第一🤣

理想称从0到1构建了Mind GPT原始基座模型，模型结构采用了自研的TaskFormer神经网络架构，基于用车、娱乐、出行等场景使用SFT、RLHF等技术进行了一系列的训练。让Mind GPT拥有了理解、生成、知识记忆及推理的三大能力。

Mind GPT在行业里到底是什么水平呢？

官方称在目前国内极具权威性的，中文大语言模型评测榜单C-EVAL，覆盖了人文、社科、理工等多个方向共52个学科，Mind GPT在58个参加测评的大模型中排行第一名。

同时，还有涵盖从基础学科到高级专业包含67个主题领域的评测榜单CMMLU，Mind GPT也获得第一名，拿下了双冠军。

Mind GPT基于理想同学的重点场景，量身定制了覆盖111个领域、超过1000种以上的专属能力，而且还在不断进化和快速成长中。

目前Mind  GPT还处于内测中...</title>
            <link>https://nitter.cz/xiaohuggg/status/1733830275254415844#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733830275254415844#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 12:45:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>理想汽车发布Mind GPT多模态大模型<br />
<br />
官方称Mind GPT是全自研的多模态认知大模型，它可以与汽车进行一个完美的融合，让每位家庭成员都能在车里体验到最好的AI。<br />
<br />
按照国内大模型的惯例，毫无悬念<br />
<br />
Mind GPT在中文大语言模型评测榜单C-EVAL和CMMLU榜单中双双取得第一🤣<br />
<br />
理想称从0到1构建了Mind GPT原始基座模型，模型结构采用了自研的TaskFormer神经网络架构，基于用车、娱乐、出行等场景使用SFT、RLHF等技术进行了一系列的训练。让Mind GPT拥有了理解、生成、知识记忆及推理的三大能力。<br />
<br />
Mind GPT在行业里到底是什么水平呢？<br />
<br />
官方称在目前国内极具权威性的，中文大语言模型评测榜单C-EVAL，覆盖了人文、社科、理工等多个方向共52个学科，Mind GPT在58个参加测评的大模型中排行第一名。<br />
<br />
同时，还有涵盖从基础学科到高级专业包含67个主题领域的评测榜单CMMLU，Mind GPT也获得第一名，拿下了双冠军。<br />
<br />
Mind GPT基于理想同学的重点场景，量身定制了覆盖111个领域、超过1000种以上的专属能力，而且还在不断进化和快速成长中。<br />
<br />
目前Mind  GPT还处于内测中...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FfTllsVmJJQUFHbDFULmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FfTmFuRmFvQUFuWUV2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FfTmNWYWFZQUFuNlR5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733780086527262768#m</id>
            <title>R to @xiaohuggg: 根据文字描述来生成“奇妙旅程”场景。

可以是简单描述、诗句、故事、俳句等等...

很神奇...</title>
            <link>https://nitter.cz/xiaohuggg/status/1733780086527262768#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733780086527262768#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 09:25:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>根据文字描述来生成“奇妙旅程”场景。<br />
<br />
可以是简单描述、诗句、故事、俳句等等...<br />
<br />
很神奇...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzM3Nzk3MDY2MDMwODE3MjgvcHUvaW1nL29nb01MUXBDXzVwemF4ZEwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>