<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733400042298540202#m</id>
            <title>据《时代》杂志报道，使用AI“脱衣”的应用和网站正迅速流行。

仅9月份，就有2400 万人访问了这类脱衣网站。

自今年年初以来，社交媒体（包括 X 和 Reddit）上的脱衣应用广告链接数量增加了2400% 以上。

扩散模型的发布是导致使用AI制作非自愿色情内容的应用和网站增加的主要原因。

这些先进的AI技术可以免费获取，使得开发者能够更容易地创建出质量更高的图像。

这些服务使用 AI 重建图像，使图中的人物裸露。

原文：https://time.com/6344068/nudify-apps-undress-photos-women-artificial-intelligence/</title>
            <link>https://nitter.cz/xiaohuggg/status/1733400042298540202#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733400042298540202#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 08:15:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据《时代》杂志报道，使用AI“脱衣”的应用和网站正迅速流行。<br />
<br />
仅9月份，就有2400 万人访问了这类脱衣网站。<br />
<br />
自今年年初以来，社交媒体（包括 X 和 Reddit）上的脱衣应用广告链接数量增加了2400% 以上。<br />
<br />
扩散模型的发布是导致使用AI制作非自愿色情内容的应用和网站增加的主要原因。<br />
<br />
这些先进的AI技术可以免费获取，使得开发者能够更容易地创建出质量更高的图像。<br />
<br />
这些服务使用 AI 重建图像，使图中的人物裸露。<br />
<br />
原文：<a href="https://time.com/6344068/nudify-apps-undress-photos-women-artificial-intelligence/">time.com/6344068/nudify-apps…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0E1R0piaWFJQUEtaEx5LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733359311152439549#m</id>
            <title>周末开心一下

🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1733359311152439549#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733359311152439549#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 05:33:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>周末开心一下<br />
<br />
🤓</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMzMzU5MDgyMTY4NTQ1MjgwL2ltZy9CZVBYMS1WZmJ2OFo5d2lHLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733349917501141390#m</id>
            <title>Wikimedia Wikisource 数据集，现在已经在 Hugging Face Hub 上提供。

- 数据集包含了来自 Wikimedia Wikisource 的最新转储
- 涵盖了 73 种不同的语言
- 数据以 Parquet 格式提供
- 可用来增强语言模型，更好地理解和生成文本
- 免费使用

下载：https://huggingface.co/datasets/wikimedia/wikisource</title>
            <link>https://nitter.cz/xiaohuggg/status/1733349917501141390#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733349917501141390#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:56:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Wikimedia Wikisource 数据集，现在已经在 Hugging Face Hub 上提供。<br />
<br />
- 数据集包含了来自 Wikimedia Wikisource 的最新转储<br />
- 涵盖了 73 种不同的语言<br />
- 数据以 Parquet 格式提供<br />
- 可用来增强语言模型，更好地理解和生成文本<br />
- 免费使用<br />
<br />
下载：<a href="https://huggingface.co/datasets/wikimedia/wikisource">huggingface.co/datasets/wiki…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1725726053212312046#m">nitter.cz/xiaohuggg/status/1725726053212312046#m</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTczMzEyNTgxOTk0NDM0MTUwNC9ZeWNGSlpScj9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733336646509289865#m</id>
            <title>Google Gemini 最新演示

测试Gemini能否理解使用Emoji Kitchen的表情符号创建的一些非常规的的Emoji图像 ！

 Emoji Kitchen可以允许你组合不同的表情符号来创建新的表情符号 。

这个演示测试了Gemini能否理解如何使用 Emoji Kitchen 创建一些不寻常的非常规的表情符号 👀</title>
            <link>https://nitter.cz/xiaohuggg/status/1733336646509289865#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733336646509289865#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 04:03:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Gemini 最新演示<br />
<br />
测试Gemini能否理解使用Emoji Kitchen的表情符号创建的一些非常规的的Emoji图像 ！<br />
<br />
 Emoji Kitchen可以允许你组合不同的表情符号来创建新的表情符号 。<br />
<br />
这个演示测试了Gemini能否理解如何使用 Emoji Kitchen 创建一些不寻常的非常规的表情符号 👀</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMzMzU5NTQ0MTEzNjQzNTIvcHUvaW1nL2RiS0JJYlBzOEd5SXdfVmouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733153199555834351#m</id>
            <title>R to @xiaohuggg: 官方演示效果：

总体感觉是很不错的，而且是开源免费的，这让@Magnific_AI 陷入了尴尬境地。

不过Magnific AI对普通用户来说还是比较易用的。

上手成本比较低...</title>
            <link>https://nitter.cz/xiaohuggg/status/1733153199555834351#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733153199555834351#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 15:54:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>官方演示效果：<br />
<br />
总体感觉是很不错的，而且是开源免费的，这让<a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 陷入了尴尬境地。<br />
<br />
不过Magnific AI对普通用户来说还是比较易用的。<br />
<br />
上手成本比较低...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMxNTI1OTc0ODQ0NDk3OTIvcHUvaW1nL0RNdzdmZWhpNkZyMzhvU2QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733152544208327089#m</id>
            <title>DemoFusion：超更高分辨率的图像生成 

一个图像增强工具，它可以提高 SDXL的图像生成分辨率，可以把生成图像的分辨率提高4倍、16倍，甚至更高。

它不仅能让图片变清晰，还能改善图片中的小细节（比如纹理和边缘）从而生成更自然和逼真的图像。

这直接把 @Magnific_AI 整不会了！😂

DemoFusion还可以无缝集成到基于 LDM 的多种应用中，如ControlNet...还能够放大真实图像...

主要功能特点：

1、高分辨率图像生成：DemoFusion 专注于利用潜在扩散模型（LDMs）生成更高分辨率的图像，突破了传统图像生成技术的限制。

2、渐进式上采样：该框架通过逐步提高图像的分辨率来生成更清晰、更详细的图像。这种渐进式方法允许更精细地控制图像质量。它会逐步提高图片的清晰度，这样你可以先看到一个大概的效果，然后再慢慢变得更清晰。

3、跳过残差和扩张采样机制：DemoFusion 使用这些先进的技术来改善图像的局部细节和全局一致性，从而生成更自然和逼真的图像。

4、与 ControlNet 的集成：可以无缝集成到基于 LDM 的多种应用中，例如与 ControlNet 结合，实现可控的高分辨率图像生成。

5、放大真实图像：还能够放大真实图像，通过编码的真实图像表示来替换初始阶段的输出，实现图像的高分辨率放大。

6、无需大量内存和调整：DemoFusion 设计得既高效又易于使用，不需要大量的内存资源或复杂的调整过程。

技术细节：

1、渐进式上采样（Progressive Upscaling）：这种方法涉及逐步提高图像的分辨率。

DemoFusion 从较低分辨率的图像开始，然后通过一个“上采样-扩散-去噪”循环逐渐提升图像的分辨率。
在每个循环中，先对图像进行上采样（增加像素数量），然后通过扩散和去噪过程来提高图像质量。

2、跳过残差（Skip Residual）：在“上采样-扩散-去噪”循环的每个阶段，DemoFusion 使用来自前一个扩散过程的中间噪声逆转表示作为跳过残差。

这有助于在高分辨率和低分辨率图像之间保持全局一致性。

3、扩张采样（Dilated Sampling）：为了在局部去噪路径中建立全局去噪路径，DemoFusion 引入了扩张采样。

这种方法促进了更全局一致的内容生成，有助于在整个图像中保持语义上的连贯性。

4、与现有模型的集成：DemoFusion 可以作为一个插件般地扩展现有的图像生成模型，如 SDXL。

它不需要额外的训练，可以直接应用于现有模型，提供分辨率的显著提升。

项目及演示：https://ruoyidu.github.io/demofusion/demofusion.html
论文：

https://drive.google.com/file/d/1pAWCfpEgwy4UAkUqTGDuqypJt-5bl8se/view?usp=sharing

GitHub：https://github.com/PRIS-CV/DemoFusion

Demo演示：https://replicate.com/lucataco/demofusion

在线体验由 @radamar 提供：https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL</title>
            <link>https://nitter.cz/xiaohuggg/status/1733152544208327089#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733152544208327089#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 15:52:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DemoFusion：超更高分辨率的图像生成 <br />
<br />
一个图像增强工具，它可以提高 SDXL的图像生成分辨率，可以把生成图像的分辨率提高4倍、16倍，甚至更高。<br />
<br />
它不仅能让图片变清晰，还能改善图片中的小细节（比如纹理和边缘）从而生成更自然和逼真的图像。<br />
<br />
这直接把 <a href="https://nitter.cz/Magnific_AI" title="Magnific.ai">@Magnific_AI</a> 整不会了！😂<br />
<br />
DemoFusion还可以无缝集成到基于 LDM 的多种应用中，如ControlNet...还能够放大真实图像...<br />
<br />
主要功能特点：<br />
<br />
1、高分辨率图像生成：DemoFusion 专注于利用潜在扩散模型（LDMs）生成更高分辨率的图像，突破了传统图像生成技术的限制。<br />
<br />
2、渐进式上采样：该框架通过逐步提高图像的分辨率来生成更清晰、更详细的图像。这种渐进式方法允许更精细地控制图像质量。它会逐步提高图片的清晰度，这样你可以先看到一个大概的效果，然后再慢慢变得更清晰。<br />
<br />
3、跳过残差和扩张采样机制：DemoFusion 使用这些先进的技术来改善图像的局部细节和全局一致性，从而生成更自然和逼真的图像。<br />
<br />
4、与 ControlNet 的集成：可以无缝集成到基于 LDM 的多种应用中，例如与 ControlNet 结合，实现可控的高分辨率图像生成。<br />
<br />
5、放大真实图像：还能够放大真实图像，通过编码的真实图像表示来替换初始阶段的输出，实现图像的高分辨率放大。<br />
<br />
6、无需大量内存和调整：DemoFusion 设计得既高效又易于使用，不需要大量的内存资源或复杂的调整过程。<br />
<br />
技术细节：<br />
<br />
1、渐进式上采样（Progressive Upscaling）：这种方法涉及逐步提高图像的分辨率。<br />
<br />
DemoFusion 从较低分辨率的图像开始，然后通过一个“上采样-扩散-去噪”循环逐渐提升图像的分辨率。<br />
在每个循环中，先对图像进行上采样（增加像素数量），然后通过扩散和去噪过程来提高图像质量。<br />
<br />
2、跳过残差（Skip Residual）：在“上采样-扩散-去噪”循环的每个阶段，DemoFusion 使用来自前一个扩散过程的中间噪声逆转表示作为跳过残差。<br />
<br />
这有助于在高分辨率和低分辨率图像之间保持全局一致性。<br />
<br />
3、扩张采样（Dilated Sampling）：为了在局部去噪路径中建立全局去噪路径，DemoFusion 引入了扩张采样。<br />
<br />
这种方法促进了更全局一致的内容生成，有助于在整个图像中保持语义上的连贯性。<br />
<br />
4、与现有模型的集成：DemoFusion 可以作为一个插件般地扩展现有的图像生成模型，如 SDXL。<br />
<br />
它不需要额外的训练，可以直接应用于现有模型，提供分辨率的显著提升。<br />
<br />
项目及演示：<a href="https://ruoyidu.github.io/demofusion/demofusion.html">ruoyidu.github.io/demofusion…</a><br />
论文：<br />
<br />
<a href="https://drive.google.com/file/d/1pAWCfpEgwy4UAkUqTGDuqypJt-5bl8se/view?usp=sharing">drive.google.com/file/d/1pAW…</a><br />
<br />
GitHub：<a href="https://github.com/PRIS-CV/DemoFusion">github.com/PRIS-CV/DemoFusio…</a><br />
<br />
Demo演示：<a href="https://replicate.com/lucataco/demofusion">replicate.com/lucataco/demof…</a><br />
<br />
在线体验由 <a href="https://nitter.cz/radamar" title="Radamés Ajna">@radamar</a> 提供：<a href="https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL">huggingface.co/spaces/radame…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzMxNDk5Mzg4MDMxODc3MTIvcHUvaW1nL1BPN25hOXlYMzhmRHJob2suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733130290191905266#m</id>
            <title>据BBC报道：Google 承认，其展示的Gemini 的演示视频经过剪辑以使其看起来更好。

这个视频展示了 AI 如何实时响应口头提示和视频。然而，Google 在视频描述中表示，并非一切如视频所示——为了演示的目的，他们加快了响应速度。

此外，Google 还承认 AI 实际上根本没有对声音或视频做出反应。

视频实际上是通过使用视频画面中的静态图像帧，并通过文本提示来引导 AI 制作的。

例如：演示视频中，一个人向 Google 的 AI 展示物体并提出一系列问题。例如，演示者拿起一个橡皮鸭并询问 Gemini 是否会漂浮。AI 最初不确定它是由什么材料制成的，但在演示者挤压它（并指出这会发出吱吱声）后，AI 正确识别了物体。

而实际过程：AI 实际上是被展示了橡皮鸭的静态图像，并被问及其材料。然后，它通过文本提示得知橡皮鸭被挤压时会发出吱吱声，从而做出正确的识别。

详细：https://bbc.in/4a7T109</title>
            <link>https://nitter.cz/xiaohuggg/status/1733130290191905266#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733130290191905266#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 14:23:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据BBC报道：Google 承认，其展示的Gemini 的演示视频经过剪辑以使其看起来更好。<br />
<br />
这个视频展示了 AI 如何实时响应口头提示和视频。然而，Google 在视频描述中表示，并非一切如视频所示——为了演示的目的，他们加快了响应速度。<br />
<br />
此外，Google 还承认 AI 实际上根本没有对声音或视频做出反应。<br />
<br />
视频实际上是通过使用视频画面中的静态图像帧，并通过文本提示来引导 AI 制作的。<br />
<br />
例如：演示视频中，一个人向 Google 的 AI 展示物体并提出一系列问题。例如，演示者拿起一个橡皮鸭并询问 Gemini 是否会漂浮。AI 最初不确定它是由什么材料制成的，但在演示者挤压它（并指出这会发出吱吱声）后，AI 正确识别了物体。<br />
<br />
而实际过程：AI 实际上是被展示了橡皮鸭的静态图像，并被问及其材料。然后，它通过文本提示得知橡皮鸭被挤压时会发出吱吱声，从而做出正确的识别。<br />
<br />
详细：<a href="https://bbc.in/4a7T109">bbc.in/4a7T109</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0ExUXo0SWFVQUFvTEpDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1733055007833092357#m</id>
            <title>我们已收到您关于 GPT4 变得更加懒惰的所有反馈！

自 11 月 11 日以来我们就没有更新过模型，这当然不是故意的。

模型行为可能是不可预测的，我们正在研究修复它🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1733055007833092357#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1733055007833092357#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:24:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们已收到您关于 GPT4 变得更加懒惰的所有反馈！<br />
<br />
自 11 月 11 日以来我们就没有更新过模型，这当然不是故意的。<br />
<br />
模型行为可能是不可预测的，我们正在研究修复它🫡</p>
<p><a href="https://nitter.cz/ChatGPTapp/status/1732979491071549792#m">nitter.cz/ChatGPTapp/status/1732979491071549792#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732990584619778077#m</id>
            <title>NewsNerd HackerBot：一个Hacker News 机器人🫡

它可以自动从 Hacker News 上抓取各种类型的新闻故事，比如最热门的、最新的或者特定主题的故事。

- 关键词过滤：如果你对某个特定主题感兴趣，比如想了解有关某个知名科技人物或公司的新闻，你可以告诉这个程序，它会帮你筛选出相关的新闻故事。

- 本地运行：这个程序是开源的，意味着你可以自己下载代码，然后在你的电脑上运行它。

- 未来计划：开发者还打算让这个程序能分析新闻故事的评论，或者分析链接到的文章内容。

GitHub：https://github.com/neural-maze/talking_with_hn
作者：@MTrofficus</title>
            <link>https://nitter.cz/xiaohuggg/status/1732990584619778077#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732990584619778077#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 05:08:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>NewsNerd HackerBot：一个Hacker News 机器人🫡<br />
<br />
它可以自动从 Hacker News 上抓取各种类型的新闻故事，比如最热门的、最新的或者特定主题的故事。<br />
<br />
- 关键词过滤：如果你对某个特定主题感兴趣，比如想了解有关某个知名科技人物或公司的新闻，你可以告诉这个程序，它会帮你筛选出相关的新闻故事。<br />
<br />
- 本地运行：这个程序是开源的，意味着你可以自己下载代码，然后在你的电脑上运行它。<br />
<br />
- 未来计划：开发者还打算让这个程序能分析新闻故事的评论，或者分析链接到的文章内容。<br />
<br />
GitHub：<a href="https://github.com/neural-maze/talking_with_hn">github.com/neural-maze/talki…</a><br />
作者：<a href="https://nitter.cz/MTrofficus" title="Miguel Otero Pedrido">@MTrofficus</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODUwMjE2OTc1NjQ2NzIvcHUvaW1nL2pSNTJYd0N1OVl5NDNIelIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732981463489110318#m</id>
            <title>R to @xiaohuggg: LooseControl还可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。

例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化等。</title>
            <link>https://nitter.cz/xiaohuggg/status/1732981463489110318#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732981463489110318#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:32:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LooseControl还可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。<br />
<br />
例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化等。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODE0MTczMDU2Njk2MzIvcHUvaW1nL2ZVdlI2RTM0aUZYWE14aDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732981317560881423#m</id>
            <title>LooseControl：一个创新的图像生成框架，可根据深度信息来引导图像的生成。

LooseControl在生成图像时会考虑物体之间的空间关系，可通过简单地描述物体在场景中的位置来创建复杂的场景。

例如你想设计一个房间，但只有一些基本想法，你只需要描述你的想法，它会根据你的描述设计一张真实的效果图。

LooseControl 提供了一种新颖的方式来设计复杂场景并执行语义编辑。

可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。

例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化。

基本背景：

LooseControl 允许用户在图像生成过程中使用深度条件，这意味着它可以根据物体在场景中的深度（即远近关系）来引导图像的生成。这里的“深度条件”指的是在生成图像时考虑物体之间的空间关系，比如哪个物体在前面，哪个在后面。

以前的技术，如 ControlNet，也使用了深度信息来生成图像，但它们通常需要非常详细和准确的深度图。深度图是一种显示场景中每个点距离观察者远近的图像，通常需要专业的设备和软件来创建。

LooseControl 的创新之处在于，它不需要这么详细的深度图。它允许用户以更简单、更灵活的方式指定深度条件。例如，用户可以只指定场景的大致布局和物体的大致位置，而不是提供完整的深度图。这使得用户可以更容易地创建复杂的场景图像，即使他们没有专业的图像处理技能或设备。

简单来说，LooseControl 让图像生成变得更加容易和直观，用户可以通过简单地描述物体在场景中的位置来创建复杂的图像，而不需要复杂的技术支持。

LooseControl的主要特点：

1、场景边界控制 (C1)：用户可以通过指定场景的边界来粗略地定义场景，而不需要提供详细的深度图。

2、3D 盒子控制 (C2)：除了场景边界，用户还可以通过大致的 3D 边界盒子来指定目标对象的位置，而不是其确切的形状和外观。

3、编辑机制：

3D 盒子编辑 (E1)：用户可以通过更改、添加或移除盒子来细化图像，同时保持图像的风格不变。

属性编辑 (E2)：提供可能的编辑方向来改变场景的某个特定方面，如整体对象密度或特定对象。

4、应用场景：使用 LooseControl和文本指导，用户可以仅通过指定场景边界和主要对象的位置来创建复杂的环境（例如房间、街景等）。

举例解释：

假如你是一个室内设计师，需要设计一个房间的样子，但你只有一些基本的想法，比如房间里应该有一张沙发、一张桌子和一盏灯。LooseControl 就是一个可以帮助你把这些基本想法变成真实图像的工具。

1、场景边界控制：你可以告诉 LooseControl 房间的大致布局，比如沙发在哪里、桌子和灯在哪里。你不需要提供详细的图纸，只需要大概描述房间的布局。

2、3D 盒子控制：你可以定义每个物体（如沙发、桌子、灯）的位置和大小。就像在电脑游戏中放置物体一样，你可以决定它们在房间中的位置和朝向。

3、编辑功能：

3D 盒子编辑：如果你觉得沙发的位置不太对，或者想换一个形状不同的桌子，你可以轻松地调整它们，而不会影响到房间的其他部分。

属性编辑：如果你想改变房间的整体风格，比如从现代风格变成复古风格，LooseControl 也可以帮助你做到这一点。

总的来说，LooseControl 就像是一个高级的室内设计软件，让你可以轻松地设计和调整房间的样子，即使你只有一些非常基本的想法。

工作原理：

LooseControl基于 ControlNet 和 StableDiffusion 模型，通过 LoRA（Low Rank）网络适配和自动合成必要的训练数据来实现。

LooseControl依赖于对 ControlNet 和 StableDiffusion 模型的改进和适配。通过使用 LoRA网络适配和自动合成训练数据。

LooseControl能够在保留原始生成权重的同时进行微调。此外，通过操作注意力层中的“键”和“值”，以及对 ControlNet Jacobian 的奇异值分析，实现了上述的编辑功能。

项目及演示：https://shariqfarooq123.github.io/loose-control/
论文：https://arxiv.org/abs/2312.03079
GitHub：https://github.com/shariqfarooq123/LooseControl
在线Demo：https://huggingface.co/spaces/shariqfarooq/LooseControl</title>
            <link>https://nitter.cz/xiaohuggg/status/1732981317560881423#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732981317560881423#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:31:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LooseControl：一个创新的图像生成框架，可根据深度信息来引导图像的生成。<br />
<br />
LooseControl在生成图像时会考虑物体之间的空间关系，可通过简单地描述物体在场景中的位置来创建复杂的场景。<br />
<br />
例如你想设计一个房间，但只有一些基本想法，你只需要描述你的想法，它会根据你的描述设计一张真实的效果图。<br />
<br />
LooseControl 提供了一种新颖的方式来设计复杂场景并执行语义编辑。<br />
<br />
可以对图像进行的更加智能和细致的修改，这些修改不仅影响图像的特定元素，还能理解和适应场景的整体语义和环境。<br />
<br />
例如，当你在场景中进行编辑时，LooseControl 能够理解并适应这些变化，包括光照的变化。<br />
<br />
基本背景：<br />
<br />
LooseControl 允许用户在图像生成过程中使用深度条件，这意味着它可以根据物体在场景中的深度（即远近关系）来引导图像的生成。这里的“深度条件”指的是在生成图像时考虑物体之间的空间关系，比如哪个物体在前面，哪个在后面。<br />
<br />
以前的技术，如 ControlNet，也使用了深度信息来生成图像，但它们通常需要非常详细和准确的深度图。深度图是一种显示场景中每个点距离观察者远近的图像，通常需要专业的设备和软件来创建。<br />
<br />
LooseControl 的创新之处在于，它不需要这么详细的深度图。它允许用户以更简单、更灵活的方式指定深度条件。例如，用户可以只指定场景的大致布局和物体的大致位置，而不是提供完整的深度图。这使得用户可以更容易地创建复杂的场景图像，即使他们没有专业的图像处理技能或设备。<br />
<br />
简单来说，LooseControl 让图像生成变得更加容易和直观，用户可以通过简单地描述物体在场景中的位置来创建复杂的图像，而不需要复杂的技术支持。<br />
<br />
LooseControl的主要特点：<br />
<br />
1、场景边界控制 (C1)：用户可以通过指定场景的边界来粗略地定义场景，而不需要提供详细的深度图。<br />
<br />
2、3D 盒子控制 (C2)：除了场景边界，用户还可以通过大致的 3D 边界盒子来指定目标对象的位置，而不是其确切的形状和外观。<br />
<br />
3、编辑机制：<br />
<br />
3D 盒子编辑 (E1)：用户可以通过更改、添加或移除盒子来细化图像，同时保持图像的风格不变。<br />
<br />
属性编辑 (E2)：提供可能的编辑方向来改变场景的某个特定方面，如整体对象密度或特定对象。<br />
<br />
4、应用场景：使用 LooseControl和文本指导，用户可以仅通过指定场景边界和主要对象的位置来创建复杂的环境（例如房间、街景等）。<br />
<br />
举例解释：<br />
<br />
假如你是一个室内设计师，需要设计一个房间的样子，但你只有一些基本的想法，比如房间里应该有一张沙发、一张桌子和一盏灯。LooseControl 就是一个可以帮助你把这些基本想法变成真实图像的工具。<br />
<br />
1、场景边界控制：你可以告诉 LooseControl 房间的大致布局，比如沙发在哪里、桌子和灯在哪里。你不需要提供详细的图纸，只需要大概描述房间的布局。<br />
<br />
2、3D 盒子控制：你可以定义每个物体（如沙发、桌子、灯）的位置和大小。就像在电脑游戏中放置物体一样，你可以决定它们在房间中的位置和朝向。<br />
<br />
3、编辑功能：<br />
<br />
3D 盒子编辑：如果你觉得沙发的位置不太对，或者想换一个形状不同的桌子，你可以轻松地调整它们，而不会影响到房间的其他部分。<br />
<br />
属性编辑：如果你想改变房间的整体风格，比如从现代风格变成复古风格，LooseControl 也可以帮助你做到这一点。<br />
<br />
总的来说，LooseControl 就像是一个高级的室内设计软件，让你可以轻松地设计和调整房间的样子，即使你只有一些非常基本的想法。<br />
<br />
工作原理：<br />
<br />
LooseControl基于 ControlNet 和 StableDiffusion 模型，通过 LoRA（Low Rank）网络适配和自动合成必要的训练数据来实现。<br />
<br />
LooseControl依赖于对 ControlNet 和 StableDiffusion 模型的改进和适配。通过使用 LoRA网络适配和自动合成训练数据。<br />
<br />
LooseControl能够在保留原始生成权重的同时进行微调。此外，通过操作注意力层中的“键”和“值”，以及对 ControlNet Jacobian 的奇异值分析，实现了上述的编辑功能。<br />
<br />
项目及演示：<a href="https://shariqfarooq123.github.io/loose-control/">shariqfarooq123.github.io/lo…</a><br />
论文：<a href="https://arxiv.org/abs/2312.03079">arxiv.org/abs/2312.03079</a><br />
GitHub：<a href="https://github.com/shariqfarooq123/LooseControl">github.com/shariqfarooq123/L…</a><br />
在线Demo：<a href="https://huggingface.co/spaces/shariqfarooq/LooseControl">huggingface.co/spaces/shariq…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5ODAwMDg1ODk5MTQxMTIvcHUvaW1nL2d6UUFCLUh6UEhDU0pISmQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732969634674983299#m</id>
            <title>R to @xiaohuggg: 只用3张照片生成的的一个场景</title>
            <link>https://nitter.cz/xiaohuggg/status/1732969634674983299#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732969634674983299#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:45:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>只用3张照片生成的的一个场景</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5Njk1MzU0MDg2Njg2NzIvcHUvaW1nL0k1UU0wVDk0N0NKbXNmQmguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732969487157317909#m</id>
            <title>ReconFusion：只需3张照片即可重建真实世界3D场景

NeRF 和 3D Gaussian splats （3D高斯泼溅）的最大缺陷就是每次都要从头开始训练，而且每次需要几十到数百个输入图像，非常耗时。

ReconFusion提出一种新的 3D 重建方法，只需要最少3张图片即可重建该图像的360度3D场景。

而且效果非常逼真...

ReconFusion的主要特点和优点：

1、少量照片即可重建：与传统的 3D 重建技术相比，ReconFusion 只需少量照片即可重建出高质量的 3D 模型。这大大减少了捕捉过程中所需的时间和资源。

2、逼真的几何和纹理合成：在照片覆盖不到的区域，ReconFusion 能够合成逼真的几何形状和纹理，提高了模型的真实感和细节丰富度。

3、使用扩散先验进行优化：该技术利用扩散先验进行新视角合成，这种先验在合成和多视角数据集上进行训练，有助于规范基于神经辐射场（NeRF）的 3D 重建流程。

4、提高重建质量：ReconFusion 在各种真实世界数据集上进行了广泛评估，显示出在少视角 3D 重建方面相比以往方法有显著的性能提升。

5、适用于复杂场景：该技术适用于各种真实世界场景的重建，包括面向前方和 360 度的场景，增强了其应用的灵活性和广泛性。

项目及演示：https://reconfusion.github.io/
论文：https://arxiv.org/abs/2312.02981</title>
            <link>https://nitter.cz/xiaohuggg/status/1732969487157317909#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732969487157317909#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:44:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ReconFusion：只需3张照片即可重建真实世界3D场景<br />
<br />
NeRF 和 3D Gaussian splats （3D高斯泼溅）的最大缺陷就是每次都要从头开始训练，而且每次需要几十到数百个输入图像，非常耗时。<br />
<br />
ReconFusion提出一种新的 3D 重建方法，只需要最少3张图片即可重建该图像的360度3D场景。<br />
<br />
而且效果非常逼真...<br />
<br />
ReconFusion的主要特点和优点：<br />
<br />
1、少量照片即可重建：与传统的 3D 重建技术相比，ReconFusion 只需少量照片即可重建出高质量的 3D 模型。这大大减少了捕捉过程中所需的时间和资源。<br />
<br />
2、逼真的几何和纹理合成：在照片覆盖不到的区域，ReconFusion 能够合成逼真的几何形状和纹理，提高了模型的真实感和细节丰富度。<br />
<br />
3、使用扩散先验进行优化：该技术利用扩散先验进行新视角合成，这种先验在合成和多视角数据集上进行训练，有助于规范基于神经辐射场（NeRF）的 3D 重建流程。<br />
<br />
4、提高重建质量：ReconFusion 在各种真实世界数据集上进行了广泛评估，显示出在少视角 3D 重建方面相比以往方法有显著的性能提升。<br />
<br />
5、适用于复杂场景：该技术适用于各种真实世界场景的重建，包括面向前方和 360 度的场景，增强了其应用的灵活性和广泛性。<br />
<br />
项目及演示：<a href="https://reconfusion.github.io/">reconfusion.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02981">arxiv.org/abs/2312.02981</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI5NjY2MDI1NDE1NzIwOTYvcHUvaW1nL21ZX01obDBqQ0U0cTdNdEEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732967294895288347#m</id>
            <title>所以说在任何事情中人其实是最不可控的因素

人会掺杂很多的情感、个人利益和道德因素

还是AI更可控😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1732967294895288347#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732967294895288347#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:36:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以说在任何事情中人其实是最不可控的因素<br />
<br />
人会掺杂很多的情感、个人利益和道德因素<br />
<br />
还是AI更可控😅</p>
<p><a href="https://nitter.cz/dotey/status/1732896556389429676#m">nitter.cz/dotey/status/1732896556389429676#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732752805654630610#m</id>
            <title>现在恐怕整个互联网

就推特的视频画质最渣了吧

发个视频，感觉还不如十年前的清晰

唉😮‍💨</title>
            <link>https://nitter.cz/xiaohuggg/status/1732752805654630610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732752805654630610#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 13:23:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>现在恐怕整个互联网<br />
<br />
就推特的视频画质最渣了吧<br />
<br />
发个视频，感觉还不如十年前的清晰<br />
<br />
唉😮‍💨</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732749893641666841#m</id>
            <title>Demeter：一款Meta Quest混合现实游戏，把你的家变成游戏场景。

该游戏利用你家里的实际环境（比如墙壁和家具）来创造游戏关卡，让你在自己的客厅里攀爬、跳跃、飞行和奔跑。

故事情节是你要帮助一个来自外星的角色探索她的星球，并解开她为什么会出现在你家的谜团。😂

Demeter是迄今为止使用混合现实技术制作的最大型游戏，其引人入胜的故事、传输性的表演和激动人心的音乐共同营造了戏剧性的紧张氛围。

游戏支持单人模式，适用于 Meta Quest 3、Meta Quest Pro 和 Meta Quest 2 平台。

游戏预计将于 2024 年 1 月 25 日发布。售价19.99 美元.</title>
            <link>https://nitter.cz/xiaohuggg/status/1732749893641666841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732749893641666841#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 13:12:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Demeter：一款Meta Quest混合现实游戏，把你的家变成游戏场景。<br />
<br />
该游戏利用你家里的实际环境（比如墙壁和家具）来创造游戏关卡，让你在自己的客厅里攀爬、跳跃、飞行和奔跑。<br />
<br />
故事情节是你要帮助一个来自外星的角色探索她的星球，并解开她为什么会出现在你家的谜团。😂<br />
<br />
Demeter是迄今为止使用混合现实技术制作的最大型游戏，其引人入胜的故事、传输性的表演和激动人心的音乐共同营造了戏剧性的紧张氛围。<br />
<br />
游戏支持单人模式，适用于 Meta Quest 3、Meta Quest Pro 和 Meta Quest 2 平台。<br />
<br />
游戏预计将于 2024 年 1 月 25 日发布。售价19.99 美元.</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI3MzAyNTU1OTk4OTg2MjQvcHUvaW1nL1RKcTZIVy1CMGRqVUFtZVcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732731878946570566#m</id>
            <title>Sound ID：可使用手机监听周围的鸟叫，并实时预测是什么鸟在鸣叫

Sound ID是Merlin Bird ID应用程序的一个功能，目前，Merlin可以基于声音识别美国和加拿大的458种鸟类。

Sound ID 可在离线设备上运行，无需网络连接。

Merlin提供了一个免费的全球鸟类指南，包含各种鸟类照片、声音、地图等信息。

Sound ID 的工作原理：

1、当手机录制声音时，Merlin 将音频转换为一种称为频谱图的图像。频谱图绘制了录音中出现的声音频率随时间的变化。

2、这个频谱图图像随后被输入到一个现代计算机视觉模型——深度卷积神经网络中。

3、该模型经过训练，能够基于包含鸟类声音的 140 小时音频和包含非鸟类背景声音（如口哨声和汽车噪音）的 126 小时音频来识别鸟类。

4、Merlin 的 Sound ID 工具使用了包含每只鸟发声精确时刻的音频数据进行训练。

这个数据的生成过程需要声音识别专家仔细聆听每个音频文件，因此模型有机会学习到更准确的声音与物种对应关系。

Sound ID：https://merlin.allaboutbirds.org/sound-id/

Sound ID 背后的机器学习原理：https://www.macaulaylibrary.org/machine-learning/

Merlin Bird ID 网站是了一个免费的全球鸟类指南，包含照片、声音、地图等信息。

鸟类识别向导：用户可以通过回答三个简单的问题来识别他们看到或听到的鸟类，Merlin 将提供可能的匹配列表。
Merlin 为所有水平的观鸟者和户外爱好者提供快速识别帮助，帮助他们了解世界各地的鸟类。

声音识别（Sound ID）：Sound ID 功能可以听取周围的鸟鸣，并实时显示可能的鸟类建议。

用户可以将录音与 Merlin 中的歌声和叫声进行比较以确认所听到的内容。Sound ID 完全离线工作，因此用户无论身在何处都可以识别鸟鸣。

目前适用于美国、加拿大、欧洲的鸟类，以及中南美洲和印度的一些常见鸟类。

照片识别：用户可以拍摄鸟类的照片，或从相机卷中选择一张照片，Photo ID 将提供一份可能匹配的简短列表。
Photo ID 也完全离线工作，因此用户可以在任何地方识别照片中的鸟类。

保存鸟类到生活清单：用户可以通过“Save My Bird”功能构建自己的数字观鸟回忆录。每次识别一种鸟类时，点击“这是我的鸟！”按钮，Merlin 将其添加到用户不断增长的生活清单中。

探索附近的鸟类清单：Merlin 由 eBird 提供支持，允许用户根据所在位置构建定制的鸟类清单。
用户可以使用过滤选项探索不同地点或一年中不同时间的鸟类，或切换显示已下载的鸟类包中的所有物种。

应用下载：https://merlin.allaboutbirds.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1732731878946570566#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732731878946570566#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 12:00:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sound ID：可使用手机监听周围的鸟叫，并实时预测是什么鸟在鸣叫<br />
<br />
Sound ID是Merlin Bird ID应用程序的一个功能，目前，Merlin可以基于声音识别美国和加拿大的458种鸟类。<br />
<br />
Sound ID 可在离线设备上运行，无需网络连接。<br />
<br />
Merlin提供了一个免费的全球鸟类指南，包含各种鸟类照片、声音、地图等信息。<br />
<br />
Sound ID 的工作原理：<br />
<br />
1、当手机录制声音时，Merlin 将音频转换为一种称为频谱图的图像。频谱图绘制了录音中出现的声音频率随时间的变化。<br />
<br />
2、这个频谱图图像随后被输入到一个现代计算机视觉模型——深度卷积神经网络中。<br />
<br />
3、该模型经过训练，能够基于包含鸟类声音的 140 小时音频和包含非鸟类背景声音（如口哨声和汽车噪音）的 126 小时音频来识别鸟类。<br />
<br />
4、Merlin 的 Sound ID 工具使用了包含每只鸟发声精确时刻的音频数据进行训练。<br />
<br />
这个数据的生成过程需要声音识别专家仔细聆听每个音频文件，因此模型有机会学习到更准确的声音与物种对应关系。<br />
<br />
Sound ID：<a href="https://merlin.allaboutbirds.org/sound-id/">merlin.allaboutbirds.org/sou…</a><br />
<br />
Sound ID 背后的机器学习原理：<a href="https://www.macaulaylibrary.org/machine-learning/">macaulaylibrary.org/machine-…</a><br />
<br />
Merlin Bird ID 网站是了一个免费的全球鸟类指南，包含照片、声音、地图等信息。<br />
<br />
鸟类识别向导：用户可以通过回答三个简单的问题来识别他们看到或听到的鸟类，Merlin 将提供可能的匹配列表。<br />
Merlin 为所有水平的观鸟者和户外爱好者提供快速识别帮助，帮助他们了解世界各地的鸟类。<br />
<br />
声音识别（Sound ID）：Sound ID 功能可以听取周围的鸟鸣，并实时显示可能的鸟类建议。<br />
<br />
用户可以将录音与 Merlin 中的歌声和叫声进行比较以确认所听到的内容。Sound ID 完全离线工作，因此用户无论身在何处都可以识别鸟鸣。<br />
<br />
目前适用于美国、加拿大、欧洲的鸟类，以及中南美洲和印度的一些常见鸟类。<br />
<br />
照片识别：用户可以拍摄鸟类的照片，或从相机卷中选择一张照片，Photo ID 将提供一份可能匹配的简短列表。<br />
Photo ID 也完全离线工作，因此用户可以在任何地方识别照片中的鸟类。<br />
<br />
保存鸟类到生活清单：用户可以通过“Save My Bird”功能构建自己的数字观鸟回忆录。每次识别一种鸟类时，点击“这是我的鸟！”按钮，Merlin 将其添加到用户不断增长的生活清单中。<br />
<br />
探索附近的鸟类清单：Merlin 由 eBird 提供支持，允许用户根据所在位置构建定制的鸟类清单。<br />
用户可以使用过滤选项探索不同地点或一年中不同时间的鸟类，或切换显示已下载的鸟类包中的所有物种。<br />
<br />
应用下载：<a href="https://merlin.allaboutbirds.org/">merlin.allaboutbirds.org/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI3MjU1NjQ0MTM1NTg3ODQvcHUvaW1nL0w2YUs3dGZzbTZfZWt4R04uanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>