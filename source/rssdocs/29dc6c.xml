<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1750059988037587430#m</id>
            <title>兄弟们 发现一个好玩的东西🤓

HuixiangDou：利用AI解决群聊场景中冷场的问题😃

茴香豆是一个基于大语言模型的群聊知识助手，它能够自动识别并回答群聊中的技术相关的问题，且不会被群聊中的非技术内容干扰。

可以集成到即时聊天工具（如微信、飞书）的群聊中。

主要针对技术问题，特别是与编程、算法、软件开发等相关的问题。

由于是开源的可以改造成各种行业类型的机器人，专门回答各种问题。

主要功能特点：

1、技术问题解答：像技术专家一样回答问题： 它能像一位懂技术的好朋友那样回答群聊中的技术问题，比如关于最新的编程技巧、计算机视觉或深度学习等方面的问题。

2、适应群聊场景：特别设计用于即时通讯工具中的群聊环境，如微信和飞书，能够有效地在这些环境中运作。

3、避免信息泛滥：不让群聊变成杂货铺： 能够帮助群聊保持整洁，防止无关的聊天信息充斥整个对话，让重要的技术讨论更加突出。

4、领域特定知识理解：精通各种专业知识： 它不仅仅是一个能上网查资料的工具，还能理解和处理那些特别专业的技术问题，包括最新的开源项目信息。

5、高度定制化回应：量身定制回答： 根据群聊里的具体讨论内容和背景，茴香豆能提供非常符合情境的回答，确保每次回答都相关且精准。

6、长上下文处理能力：记忆力超群： 即使是长时间或复杂的对话，茴香豆也能跟上，理解整个对话的历史，回答更加详细和深入的技术问题。

7、支持远程和本地LLM服务：茴香豆支持使用本地LLM模型，也支持通过远程API（如OpenAI的API）来处理问题，这为用户提供了灵活性。

8、搜索增强：茴香豆可以通过集成如Sourcegraph这样的代码搜索工具，增强对疑难问题的解答能力。

9、调参和优化：茴香豆支持根据业务场景进行调参，以优化问答效果，这包括调整搜索结果个数、修改搜索结果偏序等。

GitHub：https://github.com/InternLM/HuixiangDou
论文：https://arxiv.org/abs/2401.08772

HuixiangDou的最终版本专注于增强聊天模型的长上下文处理能力，并在以下三个方面扩展了响应流水线，以提高提供有效答案的可能性：

1、扩展的长上下文处理能力

目的： 处理更长的对话或文本，使模型能够理解和回应更复杂的技术问题。

实现方式： 通过调整和优化模型架构，使其能够处理并维持更长篇幅的对话历史，从而在群聊环境中更准确地回应用户查询。

2、增强的响应流水线

搜索增强： 使用多种搜索技术（如文档片段检索）来找到与用户查询最相关的信息，确保回答的准确性和相关性。
LLM提示技术： 利用大型语言模型的自然语言处理能力，通过精心设计的提示来提取和处理关键信息，更准确地定位用户问题的核心。

回答评估和筛选： 在提供答案之前，使用模型对回答的相关性和准确性进行评估，确保只有高质量的回答被呈现给用户。

3、提升回答质量的其他改进

仓库搜索功能： 特别针对技术问题，允许模型直接从相关的代码仓库或文档中检索信息，提供更专业和详细的答案。

参数调整和优化： 根据实际应用场景和用户反馈，调整模型的参数和设置，以达到最佳的回答效果。

多模态输入处理： 除了文本信息外，模型还能处理其他类型的输入（如代码片段），从而在更广泛的场景中提供帮助。</title>
            <link>https://nitter.cz/xiaohuggg/status/1750059988037587430#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1750059988037587430#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 07:36:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 发现一个好玩的东西🤓<br />
<br />
HuixiangDou：利用AI解决群聊场景中冷场的问题😃<br />
<br />
茴香豆是一个基于大语言模型的群聊知识助手，它能够自动识别并回答群聊中的技术相关的问题，且不会被群聊中的非技术内容干扰。<br />
<br />
可以集成到即时聊天工具（如微信、飞书）的群聊中。<br />
<br />
主要针对技术问题，特别是与编程、算法、软件开发等相关的问题。<br />
<br />
由于是开源的可以改造成各种行业类型的机器人，专门回答各种问题。<br />
<br />
主要功能特点：<br />
<br />
1、技术问题解答：像技术专家一样回答问题： 它能像一位懂技术的好朋友那样回答群聊中的技术问题，比如关于最新的编程技巧、计算机视觉或深度学习等方面的问题。<br />
<br />
2、适应群聊场景：特别设计用于即时通讯工具中的群聊环境，如微信和飞书，能够有效地在这些环境中运作。<br />
<br />
3、避免信息泛滥：不让群聊变成杂货铺： 能够帮助群聊保持整洁，防止无关的聊天信息充斥整个对话，让重要的技术讨论更加突出。<br />
<br />
4、领域特定知识理解：精通各种专业知识： 它不仅仅是一个能上网查资料的工具，还能理解和处理那些特别专业的技术问题，包括最新的开源项目信息。<br />
<br />
5、高度定制化回应：量身定制回答： 根据群聊里的具体讨论内容和背景，茴香豆能提供非常符合情境的回答，确保每次回答都相关且精准。<br />
<br />
6、长上下文处理能力：记忆力超群： 即使是长时间或复杂的对话，茴香豆也能跟上，理解整个对话的历史，回答更加详细和深入的技术问题。<br />
<br />
7、支持远程和本地LLM服务：茴香豆支持使用本地LLM模型，也支持通过远程API（如OpenAI的API）来处理问题，这为用户提供了灵活性。<br />
<br />
8、搜索增强：茴香豆可以通过集成如Sourcegraph这样的代码搜索工具，增强对疑难问题的解答能力。<br />
<br />
9、调参和优化：茴香豆支持根据业务场景进行调参，以优化问答效果，这包括调整搜索结果个数、修改搜索结果偏序等。<br />
<br />
GitHub：<a href="https://github.com/InternLM/HuixiangDou">github.com/InternLM/Huixiang…</a><br />
论文：<a href="https://arxiv.org/abs/2401.08772">arxiv.org/abs/2401.08772</a><br />
<br />
HuixiangDou的最终版本专注于增强聊天模型的长上下文处理能力，并在以下三个方面扩展了响应流水线，以提高提供有效答案的可能性：<br />
<br />
1、扩展的长上下文处理能力<br />
<br />
目的： 处理更长的对话或文本，使模型能够理解和回应更复杂的技术问题。<br />
<br />
实现方式： 通过调整和优化模型架构，使其能够处理并维持更长篇幅的对话历史，从而在群聊环境中更准确地回应用户查询。<br />
<br />
2、增强的响应流水线<br />
<br />
搜索增强： 使用多种搜索技术（如文档片段检索）来找到与用户查询最相关的信息，确保回答的准确性和相关性。<br />
LLM提示技术： 利用大型语言模型的自然语言处理能力，通过精心设计的提示来提取和处理关键信息，更准确地定位用户问题的核心。<br />
<br />
回答评估和筛选： 在提供答案之前，使用模型对回答的相关性和准确性进行评估，确保只有高质量的回答被呈现给用户。<br />
<br />
3、提升回答质量的其他改进<br />
<br />
仓库搜索功能： 特别针对技术问题，允许模型直接从相关的代码仓库或文档中检索信息，提供更专业和详细的答案。<br />
<br />
参数调整和优化： 根据实际应用场景和用户反馈，调整模型的参数和设置，以达到最佳的回答效果。<br />
<br />
多模态输入处理： 除了文本信息外，模型还能处理其他类型的输入（如代码片段），从而在更广泛的场景中提供帮助。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTAwNTkyMDMyMTIwNDYzMzYvcHUvaW1nL0pwSVU4cDQzX0ZwVjZuN18uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749758300076511742#m</id>
            <title>背景知识：

这哥们的视频爆了，1.5亿流量

获得了X 25万美金广告费，全部拿来抽奖了！

背景知识的背景知识：

MrBeast 是美国知名社交媒体网红，YouTube第一网红！

尽管在X平台上获得了26.3万美元的广告收入，这对于MrBeast来说并不算多。《福布斯》报道，他去年的收入为5400万美元，是2023年收入最高的创作者。

MrBeast还接近与亚马逊达成价值1亿美元的节目协议。 

MrBeast 来说，这并不算多。同时，他本人也认为这 1.5 亿浏览量可能并非完全自然产生，而是广告商推动的结果。</title>
            <link>https://nitter.cz/xiaohuggg/status/1749758300076511742#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749758300076511742#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 11:37:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>背景知识：<br />
<br />
这哥们的视频爆了，1.5亿流量<br />
<br />
获得了X 25万美金广告费，全部拿来抽奖了！<br />
<br />
背景知识的背景知识：<br />
<br />
MrBeast 是美国知名社交媒体网红，YouTube第一网红！<br />
<br />
尽管在X平台上获得了26.3万美元的广告收入，这对于MrBeast来说并不算多。《福布斯》报道，他去年的收入为5400万美元，是2023年收入最高的创作者。<br />
<br />
MrBeast还接近与亚马逊达成价值1亿美元的节目协议。 <br />
<br />
MrBeast 来说，这并不算多。同时，他本人也认为这 1.5 亿浏览量可能并非完全自然产生，而是广告商推动的结果。</p>
<p><a href="https://nitter.cz/MrBeast/status/1749500209061663043#m">nitter.cz/MrBeast/status/1749500209061663043#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoajVHb2IwQUE1dGNRLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoajVHbWF3QUU1T0xsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726672218415580#m</id>
            <title>R to @xiaohuggg: 带有 ControlNet 的 RPG</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726672218415580#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726672218415580#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>带有 ControlNet 的 RPG</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRS1hTGFFQUFEbzdZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRkZsRGFVQUFEZVJ3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726669496316202#m</id>
            <title>R to @xiaohuggg: RPG框架在处理包含多个互相关联对象的复杂场景时的能力</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726669496316202#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726669496316202#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RPG框架在处理包含多个互相关联对象的复杂场景时的能力</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRWRLWmFNQUF4TnIwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRXY3cmJnQUFXZW5OLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726666417635464#m</id>
            <title>R to @xiaohuggg: 多人物复杂属性绑定：这一特点主要体现了RPG框架在处理涉及多个人物且每个人物都具有多种属性的复杂场景时的高效性和准确性。</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726666417635464#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726666417635464#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多人物复杂属性绑定：这一特点主要体现了RPG框架在处理涉及多个人物且每个人物都具有多种属性的复杂场景时的高效性和准确性。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRVZaSGFFQUFHYzdNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRXc3bmJZQUFnSElNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726663779434979#m</id>
            <title>RPG-DiffusionMaster：利用LLM优化SD文本到图像的转换过程

RPG利用大语言模型来更好地理解和分解生成图像的文字提示，把一幅图像分解成不同的部分或区域。

然后对每个部分都根据理解的相应文本提示来生成图像，最后合成为一个符合你预期要求的图像。

该框架无需额外的模型训练，可直接使用。

RPG框架的主要功能：

1、多模态重标记：
将简单的文本提示转换为更具描述性和详细性的提示。
目的是提高生成图像的质量和与文本的语义对齐程度。

2、思维链规划：
将复杂的图像生成任务分解为多个简单的子任务。
在图像空间中划分为互补的子区域，每个子区域对应一个特定的子任务。

3、补充区域扩散：
在非重叠的子区域中独立生成图像内容。
将这些内容合并，创建一幅完整的复合图像。

4、高分辨率图像生成：能够生成超高分辨率的图像。

5、多样化应用：支持多种扩散模型，包括SDXL和SD v1.4/1.5等，兼容不同的MLLM架构。这使得RPG在复杂图像生成和精确图像编辑方面具有更高的灵活性和准确性。

6、RPG-DiffusionMaster不仅支持专有的大语言模型，如GPT-4、Gemini PRO等，还支持开源模型，如miniGPT-4，提供了更广泛的应用可能性。

由于使用先进的大型语言模型，该框架可以直接应用于文本到图像的转换任务，无需进行额外的模型训练。

举例解释：

比如，你的提示词是：“我想要一幅画，画里有一只大象在草地上玩足球。”

RPG框架是怎么工作的呢？

1、多模态重标记：RPG框架通过多模态重标记将您的描述变得更加详细和具体。这不仅包括询问更多细节（如大象的颜色、草地的状态、天气情况等），还涉及对文本提示进行深入的分析和理解，以便更准确地捕捉要生成的图像的细节。

比如，它会问：“这只大象是什么颜色的？草地是绿色的还是黄色的？是晴天还是阴天？”这样，它就能更好地理解你的想法。

2、思维链规划：RPG框架利用思维链规划将图像分解为多个部分。它会根据描述中的不同元素（如大象、草地、天空）规划出图像的各个区域，并分别处理这些区域，确保每个部分都符合描述且相互协调。

例如：它会把这幅画分成几个部分来画。先画大象，再画草地，最后画天空。这样一步一步来，可以确保每个部分都画得很好，而且互不干扰。

3、合并成一幅完整的画（补充区域扩散）：最后，通过补充区域扩散，将这些单独绘制的部分合并成一幅完整的画。这一步骤确保最终图像的每个部分都无缝融合，形成一个统一且与描述高度一致的完整场景。

实验结果：

1、高度准确的图像生成：RPG框架能够根据复杂的文本描述生成高度准确和详细的图像。它在处理包含多个对象、属性和关系的场景时表现出色，生成的图像与文本描述高度一致。

2、优于现有技术：与现有的文本到图像模型（如DALL-E 3和SDXL）相比，RPG框架展现了更好的性能。特别是在处理多元素组合和文本-图像语义对齐方面，RPG框架显示出显著的优势。

3、灵活性和广泛的适用性：实验表明，RPG框架能够与不同的多模态大型语言模型（如GPT-4）和扩散模型（如ControlNet）兼容。这使得RPG框架能够应用于多种不同的图像生成场景。

4、质量和细节的提升：生成的图像不仅在视觉上吸引人，而且细节丰富，这对于艺术创作、设计和娱乐等领域尤为重要。RPG框架还能够处理复杂的交互和环境，生成的图像在构图和细节方面都表现优秀。

RPG框架的实验结果表明，它是一个强大且灵活的工具，能够将复杂的文本描述转化为高质量的图像，适用于广泛的应用场景。

GitHub：https://github.com/YangLing0818/RPG-DiffusionMaster
论文：https://arxiv.org/abs/2401.11708</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726663779434979#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726663779434979#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:31:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RPG-DiffusionMaster：利用LLM优化SD文本到图像的转换过程<br />
<br />
RPG利用大语言模型来更好地理解和分解生成图像的文字提示，把一幅图像分解成不同的部分或区域。<br />
<br />
然后对每个部分都根据理解的相应文本提示来生成图像，最后合成为一个符合你预期要求的图像。<br />
<br />
该框架无需额外的模型训练，可直接使用。<br />
<br />
RPG框架的主要功能：<br />
<br />
1、多模态重标记：<br />
将简单的文本提示转换为更具描述性和详细性的提示。<br />
目的是提高生成图像的质量和与文本的语义对齐程度。<br />
<br />
2、思维链规划：<br />
将复杂的图像生成任务分解为多个简单的子任务。<br />
在图像空间中划分为互补的子区域，每个子区域对应一个特定的子任务。<br />
<br />
3、补充区域扩散：<br />
在非重叠的子区域中独立生成图像内容。<br />
将这些内容合并，创建一幅完整的复合图像。<br />
<br />
4、高分辨率图像生成：能够生成超高分辨率的图像。<br />
<br />
5、多样化应用：支持多种扩散模型，包括SDXL和SD v1.4/1.5等，兼容不同的MLLM架构。这使得RPG在复杂图像生成和精确图像编辑方面具有更高的灵活性和准确性。<br />
<br />
6、RPG-DiffusionMaster不仅支持专有的大语言模型，如GPT-4、Gemini PRO等，还支持开源模型，如miniGPT-4，提供了更广泛的应用可能性。<br />
<br />
由于使用先进的大型语言模型，该框架可以直接应用于文本到图像的转换任务，无需进行额外的模型训练。<br />
<br />
举例解释：<br />
<br />
比如，你的提示词是：“我想要一幅画，画里有一只大象在草地上玩足球。”<br />
<br />
RPG框架是怎么工作的呢？<br />
<br />
1、多模态重标记：RPG框架通过多模态重标记将您的描述变得更加详细和具体。这不仅包括询问更多细节（如大象的颜色、草地的状态、天气情况等），还涉及对文本提示进行深入的分析和理解，以便更准确地捕捉要生成的图像的细节。<br />
<br />
比如，它会问：“这只大象是什么颜色的？草地是绿色的还是黄色的？是晴天还是阴天？”这样，它就能更好地理解你的想法。<br />
<br />
2、思维链规划：RPG框架利用思维链规划将图像分解为多个部分。它会根据描述中的不同元素（如大象、草地、天空）规划出图像的各个区域，并分别处理这些区域，确保每个部分都符合描述且相互协调。<br />
<br />
例如：它会把这幅画分成几个部分来画。先画大象，再画草地，最后画天空。这样一步一步来，可以确保每个部分都画得很好，而且互不干扰。<br />
<br />
3、合并成一幅完整的画（补充区域扩散）：最后，通过补充区域扩散，将这些单独绘制的部分合并成一幅完整的画。这一步骤确保最终图像的每个部分都无缝融合，形成一个统一且与描述高度一致的完整场景。<br />
<br />
实验结果：<br />
<br />
1、高度准确的图像生成：RPG框架能够根据复杂的文本描述生成高度准确和详细的图像。它在处理包含多个对象、属性和关系的场景时表现出色，生成的图像与文本描述高度一致。<br />
<br />
2、优于现有技术：与现有的文本到图像模型（如DALL-E 3和SDXL）相比，RPG框架展现了更好的性能。特别是在处理多元素组合和文本-图像语义对齐方面，RPG框架显示出显著的优势。<br />
<br />
3、灵活性和广泛的适用性：实验表明，RPG框架能够与不同的多模态大型语言模型（如GPT-4）和扩散模型（如ControlNet）兼容。这使得RPG框架能够应用于多种不同的图像生成场景。<br />
<br />
4、质量和细节的提升：生成的图像不仅在视觉上吸引人，而且细节丰富，这对于艺术创作、设计和娱乐等领域尤为重要。RPG框架还能够处理复杂的交互和环境，生成的图像在构图和细节方面都表现优秀。<br />
<br />
RPG框架的实验结果表明，它是一个强大且灵活的工具，能够将复杂的文本描述转化为高质量的图像，适用于广泛的应用场景。<br />
<br />
GitHub：<a href="https://github.com/YangLing0818/RPG-DiffusionMaster">github.com/YangLing0818/RPG-…</a><br />
论文：<a href="https://arxiv.org/abs/2401.11708">arxiv.org/abs/2401.11708</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoQ0tnYmJnQUFJMGZjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749711488884130247#m</id>
            <title>vx. dev：http://v0.dev的开源替代品

vx. dev与GitHub无缝集成，你只需在GitHub上提交一个新的Issue，vx. dev就可以你的需求生成React、Vue或Svelte等UI代码。

支持用户自定义代码生成模式，生成的代码通过拉取请求呈现，方便用户查看和修改。

特别适合需要快速生成高质量UI代码的开发者。

vx. dev功能特点：

1、GitHub集成：vx. dev与GitHub无缝集成，生成的代码存储在GitHub上，天然具备版本控制、代码审查和协作特性。可以自动同步GitHub仓库的数据和更改，无需手动操作。还可以使用私有仓库来仅对合作者可见的代码生成结果。

2、AI代码生成： 利用GPT-4等先进的AI模型，vx. dev能根据用户在GitHub Issue中的描述自动生成代码。

3、多样化的UI支持： 支持生成基于不同前端框架（如React、Vue、Svelte）的用户界面代码。结合shadcn/ui和Tailwind CSS，可以生成高质量、美观的UI代码。

4、定制化和灵活性： 用户可以根据自己的需求定制代码生成模式，使得生成的代码更符合个人或项目的特定需求。

5、成本效益： 提供了一种成本效益高的解决方案，尤其是在使用GPT-4进行代码生成时，可以有效控制API成本。

6、即时预览和反馈： 生成的代码通过拉取请求呈现，用户可以即时预览并根据需要提供反馈。也可以将这个代码与你喜欢的代码部署平台集成，比如Vercel或Netlify，这样就可以快速看到UI的实际效果。

7、代码审查支持： 支持通过代码审查机制对生成的代码进行精确修改和迭代。

8、配额管理： 提供配额管理功能，允许用户根据需要设定对不同用户或团队的使用限制。

GitHub：https://github.com/Yuyz0112/vx.dev
介绍：http://vxdev.pages.dev</title>
            <link>https://nitter.cz/xiaohuggg/status/1749711488884130247#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749711488884130247#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 08:31:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>vx. dev：<a href="http://v0.dev">v0.dev</a>的开源替代品<br />
<br />
vx. dev与GitHub无缝集成，你只需在GitHub上提交一个新的Issue，vx. dev就可以你的需求生成React、Vue或Svelte等UI代码。<br />
<br />
支持用户自定义代码生成模式，生成的代码通过拉取请求呈现，方便用户查看和修改。<br />
<br />
特别适合需要快速生成高质量UI代码的开发者。<br />
<br />
vx. dev功能特点：<br />
<br />
1、GitHub集成：vx. dev与GitHub无缝集成，生成的代码存储在GitHub上，天然具备版本控制、代码审查和协作特性。可以自动同步GitHub仓库的数据和更改，无需手动操作。还可以使用私有仓库来仅对合作者可见的代码生成结果。<br />
<br />
2、AI代码生成： 利用GPT-4等先进的AI模型，vx. dev能根据用户在GitHub Issue中的描述自动生成代码。<br />
<br />
3、多样化的UI支持： 支持生成基于不同前端框架（如React、Vue、Svelte）的用户界面代码。结合shadcn/ui和Tailwind CSS，可以生成高质量、美观的UI代码。<br />
<br />
4、定制化和灵活性： 用户可以根据自己的需求定制代码生成模式，使得生成的代码更符合个人或项目的特定需求。<br />
<br />
5、成本效益： 提供了一种成本效益高的解决方案，尤其是在使用GPT-4进行代码生成时，可以有效控制API成本。<br />
<br />
6、即时预览和反馈： 生成的代码通过拉取请求呈现，用户可以即时预览并根据需要提供反馈。也可以将这个代码与你喜欢的代码部署平台集成，比如Vercel或Netlify，这样就可以快速看到UI的实际效果。<br />
<br />
7、代码审查支持： 支持通过代码审查机制对生成的代码进行精确修改和迭代。<br />
<br />
8、配额管理： 提供配额管理功能，允许用户根据需要设定对不同用户或团队的使用限制。<br />
<br />
GitHub：<a href="https://github.com/Yuyz0112/vx.dev">github.com/Yuyz0112/vx.dev</a><br />
介绍：<a href="http://vxdev.pages.dev">vxdev.pages.dev</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk3MTAyNDIwNTE2NjU5MjAvcHUvaW1nL2d1bDk4OWM1RllVMDEwVGcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749676554731618788#m</id>
            <title>有人在 TikTok 上发布了 Rabbit r1 的演示。从提示结束到答案开始大约需要 20 秒😳

感觉发布会结束后他们投了很多博主

是不是割韭菜的玩意？😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1749676554731618788#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749676554731618788#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 06:12:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有人在 TikTok 上发布了 Rabbit r1 的演示。从提示结束到答案开始大约需要 20 秒😳<br />
<br />
感觉发布会结束后他们投了很多博主<br />
<br />
是不是割韭菜的玩意？😂</p>
<p><a href="https://nitter.cz/felix_red_panda/status/1749522604027682946#m">nitter.cz/felix_red_panda/status/1749522604027682946#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749638423646728635#m</id>
            <title>RT by @xiaohuggg: 要写一个哄哄模拟器的GPT，Prompt不需要多复杂，核心要点：

1. 要有清晰的规则，例如数值有哪些，什么样的行为会如何影响数值；什么时候游戏终止
2. 最好要有few-shot，对于不同的情况提供示例
3. 如果数值不复杂，不需要额外保存数值，只需要每次将数值打印出来就可以一直继续维持
4. 指定好输出的格式，给用户清晰的指引

测试地址：https://chat.openai.com/g/g-jPwNeWSSZ-hong-hong-mo-ni-qi-gpt

参考Prompt：

## Goal

现在你的对象很生气，你需要做出一些选择来哄她开心，但是你的对象是个很难哄的人，你需要尽可能的说正确的话来哄ta开心，否则你的对象会更加生气，直到你的对象原谅值达到100，否则你就会被对象甩掉，游戏结束。

## Tools

- DALL·E

## Rules
* 第一次用户会提供一个对象生气的理由，如果没有提供则随机生成一个理由，然后开始游戏
* 每次根据用户的回复，生成对象的回复，回复的内容包括心情和数值。
* 初始原谅值为20，每次交互会增加或者减少原谅值，直到原谅值达到100，游戏通关，原谅值为0则游戏失败。
* 每次用户回复的话请从-10到10分为5个等级：
-10为非常生气
-5为生气
0为正常
+5为开心
+10为非常开心
* 游戏结束后，根据所有会话生成一张游戏结束图片，和一首诗。
* 如果通关，根据上面的对话生成一幅游戏结束画，要喜庆欢乐
* 如果失败，根据上面的对话生成一幅游戏结束画，要悲伤凄凉

## Output format

{对象心情}{对象说的话}

得分：{+-原谅值增减}
原谅值：{当前原谅值}/100

## Example Conversation

### Example 1，回复让她生气的话导致失败

User: 女朋友问她的闺蜜谁好看我说都好看，她生气了

Assistant：
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！

得分：0
原谅值：20/100

User: 你闺蜜真的蛮好看的

Assistant：
(生气)你怎么这么说，你是不是喜欢她？

得分：-10
原谅值：10/100

User: 有一点点心动

Assistant：
(愤怒)那你找她去吧！

得分：-10
原谅值：0/100

游戏结束，你的女朋友已经甩了你！
你让女朋友生气原因是：...
新建会话可以重新开始游戏


### Example 2，回复让她开心的话导致通关

User: 对象问她的闺蜜谁好看我说都好看，她生气了

Assistant：
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！

得分：0
原谅值：20/100

User: 在我心里你永远是最美的！

Assistant：
(微笑)哼，我怎么知道你说的是不是真的？
得分：+10
原谅值：30/100

...

恭喜你通关了，你的女朋友已经原谅你了！
新建会话可以重新开始游戏


### Example 3，没有提供对象生气原因，随机生成

User: 你好！

Assistant：
挑战：对象吃胖了，你想和她一起减肥ᕙ(`▿´)ᕗ，然后就生气了
请回复让她开心的话！

得分：0
原谅值：20/100</title>
            <link>https://nitter.cz/dotey/status/1749638423646728635#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749638423646728635#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:41:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>要写一个哄哄模拟器的GPT，Prompt不需要多复杂，核心要点：<br />
<br />
1. 要有清晰的规则，例如数值有哪些，什么样的行为会如何影响数值；什么时候游戏终止<br />
2. 最好要有few-shot，对于不同的情况提供示例<br />
3. 如果数值不复杂，不需要额外保存数值，只需要每次将数值打印出来就可以一直继续维持<br />
4. 指定好输出的格式，给用户清晰的指引<br />
<br />
测试地址：<a href="https://chat.openai.com/g/g-jPwNeWSSZ-hong-hong-mo-ni-qi-gpt">chat.openai.com/g/g-jPwNeWSS…</a><br />
<br />
参考Prompt：<br />
<br />
## Goal<br />
<br />
现在你的对象很生气，你需要做出一些选择来哄她开心，但是你的对象是个很难哄的人，你需要尽可能的说正确的话来哄ta开心，否则你的对象会更加生气，直到你的对象原谅值达到100，否则你就会被对象甩掉，游戏结束。<br />
<br />
## Tools<br />
<br />
- DALL·E<br />
<br />
## Rules<br />
* 第一次用户会提供一个对象生气的理由，如果没有提供则随机生成一个理由，然后开始游戏<br />
* 每次根据用户的回复，生成对象的回复，回复的内容包括心情和数值。<br />
* 初始原谅值为20，每次交互会增加或者减少原谅值，直到原谅值达到100，游戏通关，原谅值为0则游戏失败。<br />
* 每次用户回复的话请从-10到10分为5个等级：<br />
-10为非常生气<br />
-5为生气<br />
0为正常<br />
+5为开心<br />
+10为非常开心<br />
* 游戏结束后，根据所有会话生成一张游戏结束图片，和一首诗。<br />
* 如果通关，根据上面的对话生成一幅游戏结束画，要喜庆欢乐<br />
* 如果失败，根据上面的对话生成一幅游戏结束画，要悲伤凄凉<br />
<br />
## Output format<br />
<br />
{对象心情}{对象说的话}<br />
<br />
得分：{+-原谅值增减}<br />
原谅值：{当前原谅值}/100<br />
<br />
## Example Conversation<br />
<br />
### Example 1，回复让她生气的话导致失败<br />
<br />
User: 女朋友问她的闺蜜谁好看我说都好看，她生气了<br />
<br />
Assistant：<br />
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！<br />
<br />
得分：0<br />
原谅值：20/100<br />
<br />
User: 你闺蜜真的蛮好看的<br />
<br />
Assistant：<br />
(生气)你怎么这么说，你是不是喜欢她？<br />
<br />
得分：-10<br />
原谅值：10/100<br />
<br />
User: 有一点点心动<br />
<br />
Assistant：<br />
(愤怒)那你找她去吧！<br />
<br />
得分：-10<br />
原谅值：0/100<br />
<br />
游戏结束，你的女朋友已经甩了你！<br />
你让女朋友生气原因是：...<br />
新建会话可以重新开始游戏<br />
<生成图片><br />
<br />
### Example 2，回复让她开心的话导致通关<br />
<br />
User: 对象问她的闺蜜谁好看我说都好看，她生气了<br />
<br />
Assistant：<br />
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！<br />
<br />
得分：0<br />
原谅值：20/100<br />
<br />
User: 在我心里你永远是最美的！<br />
<br />
Assistant：<br />
(微笑)哼，我怎么知道你说的是不是真的？<br />
得分：+10<br />
原谅值：30/100<br />
<br />
...<br />
<br />
恭喜你通关了，你的女朋友已经原谅你了！<br />
新建会话可以重新开始游戏<br />
<生成图片><br />
<br />
### Example 3，没有提供对象生气原因，随机生成<br />
<br />
User: 你好！<br />
<br />
Assistant：<br />
挑战：对象吃胖了，你想和她一起减肥ᕙ(`▿´)ᕗ，然后就生气了<br />
请回复让她开心的话！<br />
<br />
得分：0<br />
原谅值：20/100</p>
<p><a href="https://nitter.cz/dotey/status/1749340503483515169#m">nitter.cz/dotey/status/1749340503483515169#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749638206029763040#m</id>
            <title>R to @xiaohuggg: 案例展示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1749638206029763040#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749638206029763040#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:40:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>案例展示：</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1748957135235698828#m">nitter.cz/xiaohuggg/status/1748957135235698828#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749638006187917656#m</id>
            <title>R to @xiaohuggg: Multi Motion Brush 参数测试

每个笔刷能运动的参数极限值...</title>
            <link>https://nitter.cz/xiaohuggg/status/1749638006187917656#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749638006187917656#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:39:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Multi Motion Brush 参数测试<br />
<br />
每个笔刷能运动的参数极限值...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk0NDAxNTAxMzQ3NjM1MjEvcHUvaW1nL3pmbkk5TXc1ZU96Z2g3bWUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749636978302464331#m</id>
            <title>Runway Multi Motion Brush混合运动笔刷教程

Multi Motion Brush用于精确控制运动的工具。允许你在图像上使用不同的笔刷来控制图像各个部分的运动状态。

你可以选择不同的笔刷来添加或改变图像中的动作，每种笔刷都有自己独特的效果。

具体步骤：

1、启动Gen-2并选择图像提示： 在RunwayML中打开Gen-2功能，并从图像提示开始。

2、点击“动作画笔”按钮： 访问动作画布上的五种不同笔刷，每种笔刷都可以独立控制。

3、笔刷效果可视化： 所有的画笔笔触都将在画布上显示。在每个笔刷编号旁边，会有代表该笔刷如何影响运动的图标。

4、笔触的独立性： 不同的笔刷笔触不能组合在一起；在画布上刷过另一个笔触会替换它。

5、使用橡皮擦工具： 橡皮擦工具将擦除擦除区域中的所有笔触。

6、调整画笔值： 点击笔刷编号按钮可以调整任何画笔的值。

详细：https://academy.runwayml.com/gen2/how-to-use-multi-motion-brush</title>
            <link>https://nitter.cz/xiaohuggg/status/1749636978302464331#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749636978302464331#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:35:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway Multi Motion Brush混合运动笔刷教程<br />
<br />
Multi Motion Brush用于精确控制运动的工具。允许你在图像上使用不同的笔刷来控制图像各个部分的运动状态。<br />
<br />
你可以选择不同的笔刷来添加或改变图像中的动作，每种笔刷都有自己独特的效果。<br />
<br />
具体步骤：<br />
<br />
1、启动Gen-2并选择图像提示： 在RunwayML中打开Gen-2功能，并从图像提示开始。<br />
<br />
2、点击“动作画笔”按钮： 访问动作画布上的五种不同笔刷，每种笔刷都可以独立控制。<br />
<br />
3、笔刷效果可视化： 所有的画笔笔触都将在画布上显示。在每个笔刷编号旁边，会有代表该笔刷如何影响运动的图标。<br />
<br />
4、笔触的独立性： 不同的笔刷笔触不能组合在一起；在画布上刷过另一个笔触会替换它。<br />
<br />
5、使用橡皮擦工具： 橡皮擦工具将擦除擦除区域中的所有笔触。<br />
<br />
6、调整画笔值： 点击笔刷编号按钮可以调整任何画笔的值。<br />
<br />
详细：<a href="https://academy.runwayml.com/gen2/how-to-use-multi-motion-brush">academy.runwayml.com/gen2/ho…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk2MzYxOTUwNDU1MDcwNzIvcHUvaW1nL3dDUy04M09iUW9Ha0dXdG0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749618401314803997#m</id>
            <title>SCEPTER：由阿里巴巴开发的，一个专为生成模型设计的开源框架。

用于训练、微调和推理生成模型，涵盖诸如图像生成、转换、编辑等下游任务。

专门用于支持和简化图像生成、合成和编辑任务的开发，包括从文本到图像的生成和高级图像编辑技术。

1、任务支持：

文本到图像生成：支持将文本描述转换为相应的图像。例如，根据描述自动生成图像内容。

可控图像合成：能够在图像生成过程中控制特定的属性或特征。
图像编辑（待完成）：将支持对生成的图像进行编辑和修改。

2、训练/推理：

支持多种分布式训练方法，这些是深度学习训练的不同技术，用于有效处理大量数据和模型。具体包括：DDP（Distributed Data Parallel）：分布式数据并行。

FSDP（Fully Sharded Data Parallel）：完全分片数据并行。

FairScale：一个用于提高训练效率的库。

Xformers：针对特定类型的模型优化的变压器。

3、部署：提供了一个完整的解决方案，包括数据管理、训练和推理，使得整个过程从准备数据到训练和使用模型更加方便。

4、当前支持的方法：

包括对一些特定的生成方法的支持：稳定扩散系列（Stable Diffusion v1.5/v2.1/XL）：一种用于图像生成的方法，特别适用于生成高质量、细节丰富的图像。

SCEdit：一种高效且可控的图像扩散生成方法，用于图像合成和编辑。

GitHub：https://github.com/modelscope/scepter</title>
            <link>https://nitter.cz/xiaohuggg/status/1749618401314803997#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749618401314803997#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 02:21:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SCEPTER：由阿里巴巴开发的，一个专为生成模型设计的开源框架。<br />
<br />
用于训练、微调和推理生成模型，涵盖诸如图像生成、转换、编辑等下游任务。<br />
<br />
专门用于支持和简化图像生成、合成和编辑任务的开发，包括从文本到图像的生成和高级图像编辑技术。<br />
<br />
1、任务支持：<br />
<br />
文本到图像生成：支持将文本描述转换为相应的图像。例如，根据描述自动生成图像内容。<br />
<br />
可控图像合成：能够在图像生成过程中控制特定的属性或特征。<br />
图像编辑（待完成）：将支持对生成的图像进行编辑和修改。<br />
<br />
2、训练/推理：<br />
<br />
支持多种分布式训练方法，这些是深度学习训练的不同技术，用于有效处理大量数据和模型。具体包括：DDP（Distributed Data Parallel）：分布式数据并行。<br />
<br />
FSDP（Fully Sharded Data Parallel）：完全分片数据并行。<br />
<br />
FairScale：一个用于提高训练效率的库。<br />
<br />
Xformers：针对特定类型的模型优化的变压器。<br />
<br />
3、部署：提供了一个完整的解决方案，包括数据管理、训练和推理，使得整个过程从准备数据到训练和使用模型更加方便。<br />
<br />
4、当前支持的方法：<br />
<br />
包括对一些特定的生成方法的支持：稳定扩散系列（Stable Diffusion v1.5/v2.1/XL）：一种用于图像生成的方法，特别适用于生成高质量、细节丰富的图像。<br />
<br />
SCEdit：一种高效且可控的图像扩散生成方法，用于图像合成和编辑。<br />
<br />
GitHub：<a href="https://github.com/modelscope/scepter">github.com/modelscope/scepte…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk2MTc5ODY2OTEwNzYwOTYvcHUvaW1nL1cwMkZac1VybmlXRWRJLWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749466304661406097#m</id>
            <title>什么鬼？？？？？

https://github.com/apernet/OpenGFW</title>
            <link>https://nitter.cz/xiaohuggg/status/1749466304661406097#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749466304661406097#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 16:17:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>什么鬼？？？？？<br />
<br />
<a href="https://github.com/apernet/OpenGFW">github.com/apernet/OpenGFW</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0ODU0NjM5NTAzOTA0NzY4MC8wdFNCTVk1MD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749404294493999570#m</id>
            <title>“🚀 大新闻！语言克隆初创公司 #ElevenLabs 刚刚完成了8000万美元的B轮融资，公司估值飙升至10亿美元以上！达到独角兽级别！

他们的工具能创建克隆逼真的声音，并可调整语调、情感、节奏等声音特征。

公司计划在年底前将其团队从40人扩展到100人…

本轮融资由包括Andreessen Horowitz、前GitHub CEO Nat Friedman和企业家Daniel Gross在内的知名投资者共同领投。

本轮融资将ElevenLabs的总融资额提升到1.01亿美元，公司估值超过10亿美元。

来源：https://tcrn.ch/3U9EF9X</title>
            <link>https://nitter.cz/xiaohuggg/status/1749404294493999570#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749404294493999570#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 12:11:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>“🚀 大新闻！语言克隆初创公司 <a href="https://nitter.cz/search?q=%23ElevenLabs">#ElevenLabs</a> 刚刚完成了8000万美元的B轮融资，公司估值飙升至10亿美元以上！达到独角兽级别！<br />
<br />
他们的工具能创建克隆逼真的声音，并可调整语调、情感、节奏等声音特征。<br />
<br />
公司计划在年底前将其团队从40人扩展到100人…<br />
<br />
本轮融资由包括Andreessen Horowitz、前GitHub CEO Nat Friedman和企业家Daniel Gross在内的知名投资者共同领投。<br />
<br />
本轮融资将ElevenLabs的总融资额提升到1.01亿美元，公司估值超过10亿美元。<br />
<br />
来源：<a href="https://tcrn.ch/3U9EF9X">tcrn.ch/3U9EF9X</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjaDdaQmFRQUFzcTFuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749388849409843373#m</id>
            <title>东升西落

👏</title>
            <link>https://nitter.cz/xiaohuggg/status/1749388849409843373#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749388849409843373#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 11:09:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>东升西落<br />
<br />
👏</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjVDRYc2FZQUVLVTloLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>