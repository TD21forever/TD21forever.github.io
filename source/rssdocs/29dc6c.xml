<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729538310136348926#m</id>
            <title>炸裂了💥

Pika 1.0发布重大的产品升级

发布了一个新的AI模型，能够使用文本生成和编辑多种风格的视频，如3D动画、动漫、卡通和电影风格。

质量非常高！

而且还难对视频内容进行精准的控制和编辑，例如调整视频的宽高比、更改视频中人物的衣服，给猩猩戴墨镜…

还没正式发布，现在可以排队：https://pika.art/</title>
            <link>https://nitter.cz/xiaohuggg/status/1729538310136348926#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729538310136348926#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 16:30:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>炸裂了💥<br />
<br />
Pika 1.0发布重大的产品升级<br />
<br />
发布了一个新的AI模型，能够使用文本生成和编辑多种风格的视频，如3D动画、动漫、卡通和电影风格。<br />
<br />
质量非常高！<br />
<br />
而且还难对视频内容进行精准的控制和编辑，例如调整视频的宽高比、更改视频中人物的衣服，给猩猩戴墨镜…<br />
<br />
还没正式发布，现在可以排队：<a href="https://pika.art/">pika.art/</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NTM2NzMzNzExMzk2ODY0L2ltZy9XUFh6b0xKTVhjSTNjY0p4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729527205833932949#m</id>
            <title>一种让别人知道你摘掉戴眼镜后真实视力的方法。

用相机对准眼镜拍照，然后在iPhone上长按“AF锁定”。

这样，眼镜被移开后，通过相机屏幕看到的画面就是佩戴该眼镜的人的真实视力状况。

这种方法可以让视力正常人理解近视者的视觉体验，也会对店铺或城市规划有所帮助。

这个方法的原理基于相机的自动对焦（AF）功能。

当你用相机对准眼镜并长按以锁定自动对焦时，相机会根据眼镜的光学特性来调整焦距。

这样，当眼镜被移开，相机的焦点仍然保持在原来根据眼镜调整的位置。

因此，通过相机屏幕看到的画面会模拟出佩戴该眼镜的人的视力状况。

video：@sakata_yoshi</title>
            <link>https://nitter.cz/xiaohuggg/status/1729527205833932949#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729527205833932949#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 15:46:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>一种让别人知道你摘掉戴眼镜后真实视力的方法。<br />
<br />
用相机对准眼镜拍照，然后在iPhone上长按“AF锁定”。<br />
<br />
这样，眼镜被移开后，通过相机屏幕看到的画面就是佩戴该眼镜的人的真实视力状况。<br />
<br />
这种方法可以让视力正常人理解近视者的视觉体验，也会对店铺或城市规划有所帮助。<br />
<br />
这个方法的原理基于相机的自动对焦（AF）功能。<br />
<br />
当你用相机对准眼镜并长按以锁定自动对焦时，相机会根据眼镜的光学特性来调整焦距。<br />
<br />
这样，当眼镜被移开，相机的焦点仍然保持在原来根据眼镜调整的位置。<br />
<br />
因此，通过相机屏幕看到的画面会模拟出佩戴该眼镜的人的视力状况。<br />
<br />
video：<a href="https://nitter.cz/sakata_yoshi" title="よしひこ@メガネ屋4代目👓1級眼鏡作製技能士">@sakata_yoshi</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NTI3MTQ3NTI0NzQ3MjY1L2ltZy9KcThLdnoySVVmV0VfMEhuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729494711076004286#m</id>
            <title>R to @xiaohuggg: drawfast在线体验：http://drawfast.tldraw.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1729494711076004286#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729494711076004286#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 13:37:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>drawfast在线体验：<a href="http://drawfast.tldraw.com">drawfast.tldraw.com</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0FCVjNYTldZQUFSQVp6LmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dBQlYzWE5XWUFBUkFaei5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729462359486582975#m</id>
            <title>RAGs：允许创建和定制自己的RAG流水线，并在自己的数据上使用它，全部通过自然语言完成。

这意味着现在你可以设置一个“基于你自己的数据的ChatGPT”，而且不需要编码。

使用RAGs创建的机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。

能生成更准确、信息更丰富的回答。

安装简单：

这个项目受到了OpenAI GPTs的启发。RAGs能让你能够通过简单的自然语言描述来创建和定制自己的聊天机器人。这个过程不需要编程知识，只需按照几个步骤操作即可：

1、描述任务：告诉RAGs你想让机器人做什么，比如回答问题或总结信息。

2、设置参数：在一个界面上调整一些选项，比如要查找的信息数量。

3、与机器人互动：设置好后，你就可以开始向这个机器人提问了。

安装RAGs的步骤也很简单，只需下载代码、安装必要的软件包，然后运行程序即可。这个工具适合那些想要自己的聊天机器人但不懂编程的人。

它支持多种LLMs（大语言模型），包括OpenAI和Anthropic的模型。用户可以通过自然语言或手动方式为嵌入模型和LLM设置配置。

主要能力和特点：

使用RAGs创建的聊天机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。

这种机器人的特点和能力包括：

1、信息检索能力：机器人能够访问和搜索大量的文档和数据，以找到与用户查询相关的信息。这意味着它可以从外部源获取数据，而不仅仅依赖于预先训练的知识。

2、高质量的回答生成：结合检索到的信息和内置的语言模型（ChatGPT），机器人能够生成更准确、信息丰富的回答。

3、适应性强：由于它结合了检索和生成，这种机器人能够更好地处理复杂的问题，尤其是那些需要实时信息或专业知识的问题。

4、灵活性和定制性：用户可以根据自己的需求定制机器人的行为，例如指定信息检索的来源、调整回答的详细程度等。

5、适用于多种应用：这种机器人适用于各种场景，如客户服务、教育、研究辅助等，尤其是在需要处理大量信息和数据的领域。

介绍：https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1

GitHub：https://github.com/run-llama/rags</title>
            <link>https://nitter.cz/xiaohuggg/status/1729462359486582975#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729462359486582975#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 11:28:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RAGs：允许创建和定制自己的RAG流水线，并在自己的数据上使用它，全部通过自然语言完成。<br />
<br />
这意味着现在你可以设置一个“基于你自己的数据的ChatGPT”，而且不需要编码。<br />
<br />
使用RAGs创建的机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。<br />
<br />
能生成更准确、信息更丰富的回答。<br />
<br />
安装简单：<br />
<br />
这个项目受到了OpenAI GPTs的启发。RAGs能让你能够通过简单的自然语言描述来创建和定制自己的聊天机器人。这个过程不需要编程知识，只需按照几个步骤操作即可：<br />
<br />
1、描述任务：告诉RAGs你想让机器人做什么，比如回答问题或总结信息。<br />
<br />
2、设置参数：在一个界面上调整一些选项，比如要查找的信息数量。<br />
<br />
3、与机器人互动：设置好后，你就可以开始向这个机器人提问了。<br />
<br />
安装RAGs的步骤也很简单，只需下载代码、安装必要的软件包，然后运行程序即可。这个工具适合那些想要自己的聊天机器人但不懂编程的人。<br />
<br />
它支持多种LLMs（大语言模型），包括OpenAI和Anthropic的模型。用户可以通过自然语言或手动方式为嵌入模型和LLM设置配置。<br />
<br />
主要能力和特点：<br />
<br />
使用RAGs创建的聊天机器人是一种结合了信息检索和文本生成能力的智能聊天机器人。<br />
<br />
这种机器人的特点和能力包括：<br />
<br />
1、信息检索能力：机器人能够访问和搜索大量的文档和数据，以找到与用户查询相关的信息。这意味着它可以从外部源获取数据，而不仅仅依赖于预先训练的知识。<br />
<br />
2、高质量的回答生成：结合检索到的信息和内置的语言模型（ChatGPT），机器人能够生成更准确、信息丰富的回答。<br />
<br />
3、适应性强：由于它结合了检索和生成，这种机器人能够更好地处理复杂的问题，尤其是那些需要实时信息或专业知识的问题。<br />
<br />
4、灵活性和定制性：用户可以根据自己的需求定制机器人的行为，例如指定信息检索的来源、调整回答的详细程度等。<br />
<br />
5、适用于多种应用：这种机器人适用于各种场景，如客户服务、教育、研究辅助等，尤其是在需要处理大量信息和数据的领域。<br />
<br />
介绍：<a href="https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1">blog.llamaindex.ai/introduci…</a><br />
<br />
GitHub：<a href="https://github.com/run-llama/rags">github.com/run-llama/rags</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjk0NjIxNzM1NDg5MDAzNTIvcHUvaW1nL1dKQWtLSC1ac2owc2xtdUkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729442483594305738#m</id>
            <title>R to @xiaohuggg: 测试是支持iPhone的，开始是我手机iOS17 beta版本问题！登陆不上！正式版iOS17是可以的！

你们可以玩玩！</title>
            <link>https://nitter.cz/xiaohuggg/status/1729442483594305738#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729442483594305738#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 10:09:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试是支持iPhone的，开始是我手机iOS17 beta版本问题！登陆不上！正式版iOS17是可以的！<br />
<br />
你们可以玩玩！</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI5NDQyMzk1ODM2ODgyOTQ0L2ltZy81OUpiSlV5UWg2anFSd1ZYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729404520814575674#m</id>
            <title>TRASH BABY：混合任意两张照片生成新照片

你可以任意拍摄两张照片，这个APP会将它们混合成一个新的图像。

混合完毕后你可以继续添加新的照片，每次添加都会与之前的图像混合，创造出一系列独特且多变的图像作品。

也可以选择自己的照片进行混合...🥱

不过目前只支持iPad，下载链接：https://apps.apple.com/us/app/trash-baby/id6446822932</title>
            <link>https://nitter.cz/xiaohuggg/status/1729404520814575674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729404520814575674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:39:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TRASH BABY：混合任意两张照片生成新照片<br />
<br />
你可以任意拍摄两张照片，这个APP会将它们混合成一个新的图像。<br />
<br />
混合完毕后你可以继续添加新的照片，每次添加都会与之前的图像混合，创造出一系列独特且多变的图像作品。<br />
<br />
也可以选择自己的照片进行混合...🥱<br />
<br />
不过目前只支持iPad，下载链接：<a href="https://apps.apple.com/us/app/trash-baby/id6446822932">apps.apple.com/us/app/trash-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzODgwODIxNTAxNzg4MTYvcHUvaW1nL1pMZVdiU3RMYURiNWYzcmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729395131474935856#m</id>
            <title>如果有人能出一本日历

什么时间应该发什么，什么时候不能发什么

在国内应该会卖的很好

如果能利用AI再加个自动检测提示就更好了🙄</title>
            <link>https://nitter.cz/xiaohuggg/status/1729395131474935856#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729395131474935856#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:01:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>如果有人能出一本日历<br />
<br />
什么时间应该发什么，什么时候不能发什么<br />
<br />
在国内应该会卖的很好<br />
<br />
如果能利用AI再加个自动检测提示就更好了🙄</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729356197906837678#m</id>
            <title>Cleanlab：自动检测机器学习数据集中的问题，进行自动清理修复和整理。

Cleanlab能够自动识别和修复数据集中的多种问题，如错误的标签、异常值、重复数据等。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，它可以帮助识别这种错误。

可以处理图像、文本、音频、表格数据等多种类型的数据。

Cleanlab 的工作原理和特点包括：

1、使用现有模型估计问题：Cleanlab 不是从头开始训练一个新模型，而是利用已有的机器学习模型来分析数据集。它通过这些模型的输出来估计数据集中可能存在的问题，比如哪些数据可能被错误标记，或者哪些数据是异常值。

2、发现标签问题：Cleanlab 能够自动识别数据集中的多种问题，如错误的标签、异常值、重复数据等。这种自动检测功能大大简化了数据清理的过程。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，Cleanlab 可以帮助识别这种错误。

3、训练鲁棒模型：通过识别和修复数据集中的问题，Cleanlab 可以帮助训练出对噪声和错误更加鲁棒的模型。这意味着即使在数据不完美的情况下，模型也能表现得更好。

4、适用于多种数据类型和任务：Cleanlab 不仅适用于图像数据，还可以处理文本、音频、表格数据等多种类型的数据。它支持多种机器学习任务，包括分类、回归、对象检测等。

5、数据质量量化：除了识别和修复问题，Cleanlab 还能量化数据集的整体质量，帮助用户了解数据集的健康状况。

详细介绍：https://cleanlab.ai/blog/studio-multi-label/

GitHub：https://github.com/cleanlab/cleanlab/

案例：https://github.com/cleanlab/examples

视频演示为：以 CelebA 多标签数据集为例，展示了 Cleanlab Studio 如何快速改进数据集。

CelebA 包含面部图像及其相关标签（如“戴眼镜”、“戴耳环”等），每张图像可以有多个标签。Cleanlab Studio 能够自动发现数百个缺失和不正确的标签，以及一些可能含糊的样本和异常值。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729356197906837678#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729356197906837678#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 04:27:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Cleanlab：自动检测机器学习数据集中的问题，进行自动清理修复和整理。<br />
<br />
Cleanlab能够自动识别和修复数据集中的多种问题，如错误的标签、异常值、重复数据等。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，它可以帮助识别这种错误。<br />
<br />
可以处理图像、文本、音频、表格数据等多种类型的数据。<br />
<br />
Cleanlab 的工作原理和特点包括：<br />
<br />
1、使用现有模型估计问题：Cleanlab 不是从头开始训练一个新模型，而是利用已有的机器学习模型来分析数据集。它通过这些模型的输出来估计数据集中可能存在的问题，比如哪些数据可能被错误标记，或者哪些数据是异常值。<br />
<br />
2、发现标签问题：Cleanlab 能够自动识别数据集中的多种问题，如错误的标签、异常值、重复数据等。这种自动检测功能大大简化了数据清理的过程。例如，如果一个图像被错误地标记为“猫”而实际上是“狗”，Cleanlab 可以帮助识别这种错误。<br />
<br />
3、训练鲁棒模型：通过识别和修复数据集中的问题，Cleanlab 可以帮助训练出对噪声和错误更加鲁棒的模型。这意味着即使在数据不完美的情况下，模型也能表现得更好。<br />
<br />
4、适用于多种数据类型和任务：Cleanlab 不仅适用于图像数据，还可以处理文本、音频、表格数据等多种类型的数据。它支持多种机器学习任务，包括分类、回归、对象检测等。<br />
<br />
5、数据质量量化：除了识别和修复问题，Cleanlab 还能量化数据集的整体质量，帮助用户了解数据集的健康状况。<br />
<br />
详细介绍：<a href="https://cleanlab.ai/blog/studio-multi-label/">cleanlab.ai/blog/studio-mult…</a><br />
<br />
GitHub：<a href="https://github.com/cleanlab/cleanlab/">github.com/cleanlab/cleanlab…</a><br />
<br />
案例：<a href="https://github.com/cleanlab/examples">github.com/cleanlab/examples</a><br />
<br />
视频演示为：以 CelebA 多标签数据集为例，展示了 Cleanlab Studio 如何快速改进数据集。<br />
<br />
CelebA 包含面部图像及其相关标签（如“戴眼镜”、“戴耳环”等），每张图像可以有多个标签。Cleanlab Studio 能够自动发现数百个缺失和不正确的标签，以及一些可能含糊的样本和异常值。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzNDYzMzUyNTI1NDk2MzIvcHUvaW1nL2pibHJfRUQydmg3QkRJX2MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729336931258093805#m</id>
            <title>R to @xiaohuggg: 测试结果是：

如果是已经比较清晰的提升效果会很明显！

但是如果是很老的影片，提升效果就不是很明显了。

根据演示来看，似乎对动画视频效果提升比较好。</title>
            <link>https://nitter.cz/xiaohuggg/status/1729336931258093805#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729336931258093805#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 03:10:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果是：<br />
<br />
如果是已经比较清晰的提升效果会很明显！<br />
<br />
但是如果是很老的影片，提升效果就不是很明显了。<br />
<br />
根据演示来看，似乎对动画视频效果提升比较好。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzMzY2MjczMTc4ODY5NzYvcHUvaW1nL3o5ZU0xRzBYNFlEcG1MalAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729336570115920325#m</id>
            <title>Real-ESRGAN-Video ：可以将视频清晰度提升到2K 或 4K

你只需要上传一个视频，选择你想要的清晰度，比如全高清（FHD）、2K 或 4K。它会自动提升视频的质量。处理完的视频可以直接在网页上预览，也可以下载到电脑上。

还提供了几种不同的模型处理模式，可以根据视频的内容选择最合适的。

- 标准模型（RealESRGAN_x4plus）：适用于大多数普通视频，能够提升视频的清晰度和细节。

- 动画专用模型（RealESRGAN_x4plus_anime_6B）：专门为动画视频设计，能更好地处理动画中的线条和颜色。

- 其他特殊模型（如 realesr-animevideov3）：针对特定类型的视频内容进行了优化，比如更适合处理低光照视频或是特定风格的视频。

用户可以根据自己的视频内容（比如是普通视频、动画、或者有特殊风格的视频）来选择最合适的模型，以获得最佳的视频增强效果。

在线体验：https://replicate.com/lucataco/real-esrgan-video

Colab：https://github.com/yuvraj108c/4k-video-upscaler-colab

GitHub：https://github.com/xinntao/Real-ESRGAN</title>
            <link>https://nitter.cz/xiaohuggg/status/1729336570115920325#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729336570115920325#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 03:09:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Real-ESRGAN-Video ：可以将视频清晰度提升到2K 或 4K<br />
<br />
你只需要上传一个视频，选择你想要的清晰度，比如全高清（FHD）、2K 或 4K。它会自动提升视频的质量。处理完的视频可以直接在网页上预览，也可以下载到电脑上。<br />
<br />
还提供了几种不同的模型处理模式，可以根据视频的内容选择最合适的。<br />
<br />
- 标准模型（RealESRGAN_x4plus）：适用于大多数普通视频，能够提升视频的清晰度和细节。<br />
<br />
- 动画专用模型（RealESRGAN_x4plus_anime_6B）：专门为动画视频设计，能更好地处理动画中的线条和颜色。<br />
<br />
- 其他特殊模型（如 realesr-animevideov3）：针对特定类型的视频内容进行了优化，比如更适合处理低光照视频或是特定风格的视频。<br />
<br />
用户可以根据自己的视频内容（比如是普通视频、动画、或者有特殊风格的视频）来选择最合适的模型，以获得最佳的视频增强效果。<br />
<br />
在线体验：<a href="https://replicate.com/lucataco/real-esrgan-video">replicate.com/lucataco/real-…</a><br />
<br />
Colab：<a href="https://github.com/yuvraj108c/4k-video-upscaler-colab">github.com/yuvraj108c/4k-vid…</a><br />
<br />
GitHub：<a href="https://github.com/xinntao/Real-ESRGAN">github.com/xinntao/Real-ESRG…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkzMzYzMzEyNDE5NDMwNDAvcHUvaW1nLzJ4cnc1ZVg5Q01lRTAyUEsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729313540887175590#m</id>
            <title>Awesome-Assistants ：各种各样的AI助手

这个项目收集和展示各种AI助手，包括聊天机器人、语音助手、自动化工具等。

你可以方便地将这些AI助手集成到他们自己的应用或系统中，无论他们使用的是什么编程语言。

GitHub：https://github.com/awesome-assistants/awesome-assistants</title>
            <link>https://nitter.cz/xiaohuggg/status/1729313540887175590#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729313540887175590#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 01:37:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Awesome-Assistants ：各种各样的AI助手<br />
<br />
这个项目收集和展示各种AI助手，包括聊天机器人、语音助手、自动化工具等。<br />
<br />
你可以方便地将这些AI助手集成到他们自己的应用或系统中，无论他们使用的是什么编程语言。<br />
<br />
GitHub：<a href="https://github.com/awesome-assistants/awesome-assistants">github.com/awesome-assistant…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9fQmY2bGJRQUEzZ0lPLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729288978283827360#m</id>
            <title>和GPT玩井字棋游戏

GPT结合@tldraw 实时画图后的新玩法

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1729288978283827360#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729288978283827360#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 23:59:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>和GPT玩井字棋游戏<br />
<br />
GPT结合<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 实时画图后的新玩法<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkxODQwMzQ5NzI5MjU5NTIvcHUvaW1nL3RxQnBkQ0RBQUwtZ2pteFkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729167719394869646#m</id>
            <title>牛p了

用文字描述指挥AI画画😂

你只需要描述你要画什么就行

右边会按照你的要求即时创作，还会展示创作过程👍👍👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1729167719394869646#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729167719394869646#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 15:58:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>牛p了<br />
<br />
用文字描述指挥AI画画😂<br />
<br />
你只需要描述你要画什么就行<br />
<br />
右边会按照你的要求即时创作，还会展示创作过程👍👍👍</p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvRl84NFF6T1hrQUEyRk1YLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0ZfODRRek9Ya0FBMkZNWC5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</id>
            <title>R to @xiaohuggg: 所以这种片子以后就统称为：A片</title>
            <link>https://nitter.cz/xiaohuggg/status/1729102242429702348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729102242429702348#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:37:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>所以这种片子以后就统称为：A片</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</id>
            <title>全球首位由AI生成的AV女演员木花愛（木花あい）出道

由日本一家专精于成人动作片的公司h.m.p推出。

木花愛：身高165厘米，三围为88G/55/85。

木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。

链接就不放了😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1729099853698093128#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729099853698093128#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 11:28:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>全球首位由AI生成的AV女演员木花愛（木花あい）出道<br />
<br />
由日本一家专精于成人动作片的公司h.m.p推出。<br />
<br />
木花愛：身高165厘米，三围为88G/55/85。<br />
<br />
木花愛的首部作品《世界初新人 AI 女優 完全なる美顔 木花あい AV デビュー》片长：35 分钟。售价1,966 日元起，将于12月22日正式发售。<br />
<br />
链接就不放了😅</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83OTVWNGJRQUEwRkZFLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83LU1sYmJFQUFyd1VXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</id>
            <title>DiffusionMat：一个先进的高质量视频抠图框架

DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。

DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：

1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。

2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。

3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。

4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。

5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。

项目及演示：https://cnnlstm.github.io/DiffusionMat
论文：https://arxiv.org/pdf/2311.13535.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1729091603900559731#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729091603900559731#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 10:55:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DiffusionMat：一个先进的高质量视频抠图框架<br />
<br />
DiffusionMat基于扩散模型，它能够将一个初步、粗糙的图像抠图结果转化为一个更加精细、准确的结果，从而提高抠图的质量和效果。<br />
<br />
DiffusionMat与其他同类图像抠图工具相比有几个独特的特点：<br />
<br />
1、扩散模型的应用：DiffusionMat使用了扩散模型，通过逐步去除噪声和不精确的部分来改善图像质量。这种方法在图像抠图领域是比较独特的。<br />
<br />
2、从粗糙到精细的过渡：许多传统的抠图工具要求用户提供一个相对精确的Alpha蒙版作为起点。DiffusionMat则可以从一个粗糙的Alpha蒙版开始，逐步提升其精细度和准确性。<br />
<br />
3、对细节的保留：DiffusionMat特别强调在抠图过程中保留原始图像的细节和结构。它能够更好地处理复杂的图像边缘和透明度变化，从而提供更自然、更准确的抠图结果。特别擅长处理图片中的小细节，比如头发丝或者树叶的边缘，这些通常是其他工具难以处理的。<br />
<br />
4、Alpha可靠性传播：它能更好地处理图片中透明或半透明的部分，比如玻璃窗或者薄纱，让最后的效果看起来更自然。<br />
<br />
5、专门的损失函数：DiffusionMat使用了专门设计的损失函数来优化抠图结果，这有助于在边缘和透明度方面获得更高的精确度，确保最后的图片既精确又好看。<br />
<br />
项目及演示：<a href="https://cnnlstm.github.io/DiffusionMat">cnnlstm.github.io/DiffusionM…</a><br />
论文：<a href="https://arxiv.org/pdf/2311.13535.pdf">arxiv.org/pdf/2311.13535.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwMzk4MDI3NTMwMDc2MTYvcHUvaW1nL0VmTkUtUTZYQXlYNEVkejguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</id>
            <title>Comfy Workflows：ComfyUI工作流分享站

收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。

省去了很多麻烦🫡

而且你也可以自己分享自己的工作流。

网站还支持在线运行工作流，不过要花点钱🥱

🔗：http://ComfyWorkflows.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1729063470124183735#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729063470124183735#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 09:03:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfy Workflows：ComfyUI工作流分享站<br />
<br />
收集了各种各样的Comfy Workflows，你可以从该网站直接下载并拖放到ComfyUI中，即可加载其工作流程。<br />
<br />
省去了很多麻烦🫡<br />
<br />
而且你也可以自己分享自己的工作流。<br />
<br />
网站还支持在线运行工作流，不过要花点钱🥱<br />
<br />
🔗：<a href="http://ComfyWorkflows.com">ComfyWorkflows.com</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjkwNjMxNjk4NDk3NzgxNzYvcHUvaW1nL2VtTnJlTGNqeXlNQUlBV1ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729037971578626302#m</id>
            <title>What Q* could be？

Q*可能是一种结合了多种强化学习和搜索技术的方法，用于提高语言模型在复杂任务中的性能和推理能力。

通过结合自我对弈、前瞻性规划、思维树推理和过程奖励模型利用合成数据自我训练，Q*可能是一种具备自我思考能力、能够自我进化和学习的高级人工智能。

ML 科学家（RL、RLHF、社会、机器人）@natolambert 内森·兰伯特撰文描述了他对Q* 的一些看法

了对Q*可能的工作原理进行了深入分析：

通俗易懂解释就是：

1、结合两种策略：Q*是把两种策略混合在一起。一种是像玩国际象棋时预测对手下一步会怎么走（这叫Q学习），另一种是像谷歌地图一样找到从一个地方到另一个地方的最佳路线（这叫A搜索）。

2、思考多种可能：Q*会像一个思考多种可能性的人一样，考虑不同的解决方案，然后选择最好的那个。就像你在解决一个难题时，会想到很多不同的答案，然后挑选最合适的一个。

3、自己和自己比赛：Q*还会像一个棋手那样，和自己下棋来提高自己的技能。它通过不断地和自己的不同版本比赛，学习如何做出更好的决策。

4、每一步都打分：在解决问题的过程中，Q*会给每一步都打分，这样就能知道哪些步骤是好的，哪些不是。就像你在做选择题时，对每个选项进行评估，然后选择最有可能正确的那个。

5、使用合成数据训练：Q*使用大量的虚拟数据来训练自己，这样就不需要真实世界的数据那么多。这就像是通过模拟考试来准备真正的考试。

6、学习如何更好地解决问题：最后，Q*会通过一种叫做离线强化学习的方法来提高自己的能力。这就像是通过回顾过去的经验来学习如何在未来做得更好。

专业解释：

1、结合Q学习和A*搜索：Q*可能是Q学习（一种强化学习算法）和A*搜索（一种图搜索算法）的结合。这个方法可能涉及通过“思维树”（tree-of-thoughts）在语言/推理步骤上进行搜索。

2、思维树推理（Tree-of-Thoughts Reasoning）：Q*可能利用所谓的“思维树”来进行推理。这种方法通过提示语言模型创建多个推理路径，这些路径可能会或不会在正确答案处汇合。这种方法类似于递归式的提示技术，用于提高推理性能。

3、自我对弈和前瞻性规划：Q*可能结合了自我对弈的概念，即通过与自己的不同版本对弈来提高性能，以及前瞻性规划，即使用模型预测未来以产生更好的行动或输出。

4、过程奖励模型（Process Reward Models, PRMs）：Q*可能使用PRMs来为每个推理步骤打分，而不是整个回答。这允许在推理问题上进行更精细的生成和优化。

5、合成数据的使用：Q*可能大量使用合成数据来训练和优化模型。合成数据的使用可以减少对人类评分者的依赖，提高数据生成的效率和多样性。

6、离线强化学习的应用：Q*可能通过离线强化学习进行优化，这与现有的RLHF（强化学习人类反馈）工具类似，但采用了多步骤的方法。

详细内容：https://www.interconnects.ai/p/q-star</title>
            <link>https://nitter.cz/xiaohuggg/status/1729037971578626302#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729037971578626302#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 07:22:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>What Q* could be？<br />
<br />
Q*可能是一种结合了多种强化学习和搜索技术的方法，用于提高语言模型在复杂任务中的性能和推理能力。<br />
<br />
通过结合自我对弈、前瞻性规划、思维树推理和过程奖励模型利用合成数据自我训练，Q*可能是一种具备自我思考能力、能够自我进化和学习的高级人工智能。<br />
<br />
ML 科学家（RL、RLHF、社会、机器人）<a href="https://nitter.cz/natolambert" title="Nathan Lambert">@natolambert</a> 内森·兰伯特撰文描述了他对Q* 的一些看法<br />
<br />
了对Q*可能的工作原理进行了深入分析：<br />
<br />
通俗易懂解释就是：<br />
<br />
1、结合两种策略：Q*是把两种策略混合在一起。一种是像玩国际象棋时预测对手下一步会怎么走（这叫Q学习），另一种是像谷歌地图一样找到从一个地方到另一个地方的最佳路线（这叫A搜索）。<br />
<br />
2、思考多种可能：Q*会像一个思考多种可能性的人一样，考虑不同的解决方案，然后选择最好的那个。就像你在解决一个难题时，会想到很多不同的答案，然后挑选最合适的一个。<br />
<br />
3、自己和自己比赛：Q*还会像一个棋手那样，和自己下棋来提高自己的技能。它通过不断地和自己的不同版本比赛，学习如何做出更好的决策。<br />
<br />
4、每一步都打分：在解决问题的过程中，Q*会给每一步都打分，这样就能知道哪些步骤是好的，哪些不是。就像你在做选择题时，对每个选项进行评估，然后选择最有可能正确的那个。<br />
<br />
5、使用合成数据训练：Q*使用大量的虚拟数据来训练自己，这样就不需要真实世界的数据那么多。这就像是通过模拟考试来准备真正的考试。<br />
<br />
6、学习如何更好地解决问题：最后，Q*会通过一种叫做离线强化学习的方法来提高自己的能力。这就像是通过回顾过去的经验来学习如何在未来做得更好。<br />
<br />
专业解释：<br />
<br />
1、结合Q学习和A*搜索：Q*可能是Q学习（一种强化学习算法）和A*搜索（一种图搜索算法）的结合。这个方法可能涉及通过“思维树”（tree-of-thoughts）在语言/推理步骤上进行搜索。<br />
<br />
2、思维树推理（Tree-of-Thoughts Reasoning）：Q*可能利用所谓的“思维树”来进行推理。这种方法通过提示语言模型创建多个推理路径，这些路径可能会或不会在正确答案处汇合。这种方法类似于递归式的提示技术，用于提高推理性能。<br />
<br />
3、自我对弈和前瞻性规划：Q*可能结合了自我对弈的概念，即通过与自己的不同版本对弈来提高性能，以及前瞻性规划，即使用模型预测未来以产生更好的行动或输出。<br />
<br />
4、过程奖励模型（Process Reward Models, PRMs）：Q*可能使用PRMs来为每个推理步骤打分，而不是整个回答。这允许在推理问题上进行更精细的生成和优化。<br />
<br />
5、合成数据的使用：Q*可能大量使用合成数据来训练和优化模型。合成数据的使用可以减少对人类评分者的依赖，提高数据生成的效率和多样性。<br />
<br />
6、离线强化学习的应用：Q*可能通过离线强化学习进行优化，这与现有的RLHF（强化学习人类反馈）工具类似，但采用了多步骤的方法。<br />
<br />
详细内容：<a href="https://www.interconnects.ai/p/q-star">interconnects.ai/p/q-star</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl83RzNsbWJFQUF1cS1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</id>
            <title>R to @xiaohuggg: 测试结果：
https://huggingface.co/openchat/openchat_3.5</title>
            <link>https://nitter.cz/xiaohuggg/status/1729002803887214842#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1729002803887214842#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 05:02:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>测试结果：<br />
<a href="https://huggingface.co/openchat/openchat_3.5">huggingface.co/openchat/open…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bVpOWWIwQUF5dUJOLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bWRhQmFVQUVDSmRLLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl82bW5tTmF3QUFUWm1kLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>