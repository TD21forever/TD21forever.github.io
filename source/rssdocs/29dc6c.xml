<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726672218415580#m</id>
            <title>R to @xiaohuggg: 带有 ControlNet 的 RPG</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726672218415580#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726672218415580#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>带有 ControlNet 的 RPG</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRS1hTGFFQUFEbzdZLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRkZsRGFVQUFEZVJ3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726669496316202#m</id>
            <title>R to @xiaohuggg: RPG框架在处理包含多个互相关联对象的复杂场景时的能力</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726669496316202#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726669496316202#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RPG框架在处理包含多个互相关联对象的复杂场景时的能力</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRWRLWmFNQUF4TnIwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRXY3cmJnQUFXZW5OLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726666417635464#m</id>
            <title>R to @xiaohuggg: 多人物复杂属性绑定：这一特点主要体现了RPG框架在处理涉及多个人物且每个人物都具有多种属性的复杂场景时的高效性和准确性。</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726666417635464#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726666417635464#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:32:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多人物复杂属性绑定：这一特点主要体现了RPG框架在处理涉及多个人物且每个人物都具有多种属性的复杂场景时的高效性和准确性。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRVZaSGFFQUFHYzdNLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoRXc3bmJZQUFnSElNLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749726663779434979#m</id>
            <title>RPG-DiffusionMaster：利用LLM优化SD文本到图像的转换过程

RPG利用大语言模型来更好地理解和分解生成图像的文字提示，把一幅图像分解成不同的部分或区域。

然后对每个部分都根据理解的相应文本提示来生成图像，最后合成为一个符合你预期要求的图像。

该框架无需额外的模型训练，可直接使用。

RPG框架的主要功能：

1、多模态重标记：
将简单的文本提示转换为更具描述性和详细性的提示。
目的是提高生成图像的质量和与文本的语义对齐程度。

2、思维链规划：
将复杂的图像生成任务分解为多个简单的子任务。
在图像空间中划分为互补的子区域，每个子区域对应一个特定的子任务。

3、补充区域扩散：
在非重叠的子区域中独立生成图像内容。
将这些内容合并，创建一幅完整的复合图像。

4、高分辨率图像生成：能够生成超高分辨率的图像。

5、多样化应用：支持多种扩散模型，包括SDXL和SD v1.4/1.5等，兼容不同的MLLM架构。这使得RPG在复杂图像生成和精确图像编辑方面具有更高的灵活性和准确性。

6、RPG-DiffusionMaster不仅支持专有的大语言模型，如GPT-4、Gemini PRO等，还支持开源模型，如miniGPT-4，提供了更广泛的应用可能性。

由于使用先进的大型语言模型，该框架可以直接应用于文本到图像的转换任务，无需进行额外的模型训练。

举例解释：

比如，你的提示词是：“我想要一幅画，画里有一只大象在草地上玩足球。”

RPG框架是怎么工作的呢？

1、多模态重标记：RPG框架通过多模态重标记将您的描述变得更加详细和具体。这不仅包括询问更多细节（如大象的颜色、草地的状态、天气情况等），还涉及对文本提示进行深入的分析和理解，以便更准确地捕捉要生成的图像的细节。

比如，它会问：“这只大象是什么颜色的？草地是绿色的还是黄色的？是晴天还是阴天？”这样，它就能更好地理解你的想法。

2、思维链规划：RPG框架利用思维链规划将图像分解为多个部分。它会根据描述中的不同元素（如大象、草地、天空）规划出图像的各个区域，并分别处理这些区域，确保每个部分都符合描述且相互协调。

例如：它会把这幅画分成几个部分来画。先画大象，再画草地，最后画天空。这样一步一步来，可以确保每个部分都画得很好，而且互不干扰。

3、合并成一幅完整的画（补充区域扩散）：最后，通过补充区域扩散，将这些单独绘制的部分合并成一幅完整的画。这一步骤确保最终图像的每个部分都无缝融合，形成一个统一且与描述高度一致的完整场景。

实验结果：

1、高度准确的图像生成：RPG框架能够根据复杂的文本描述生成高度准确和详细的图像。它在处理包含多个对象、属性和关系的场景时表现出色，生成的图像与文本描述高度一致。

2、优于现有技术：与现有的文本到图像模型（如DALL-E 3和SDXL）相比，RPG框架展现了更好的性能。特别是在处理多元素组合和文本-图像语义对齐方面，RPG框架显示出显著的优势。

3、灵活性和广泛的适用性：实验表明，RPG框架能够与不同的多模态大型语言模型（如GPT-4）和扩散模型（如ControlNet）兼容。这使得RPG框架能够应用于多种不同的图像生成场景。

4、质量和细节的提升：生成的图像不仅在视觉上吸引人，而且细节丰富，这对于艺术创作、设计和娱乐等领域尤为重要。RPG框架还能够处理复杂的交互和环境，生成的图像在构图和细节方面都表现优秀。

RPG框架的实验结果表明，它是一个强大且灵活的工具，能够将复杂的文本描述转化为高质量的图像，适用于广泛的应用场景。

GitHub：https://github.com/YangLing0818/RPG-DiffusionMaster
论文：https://arxiv.org/abs/2401.11708</title>
            <link>https://nitter.cz/xiaohuggg/status/1749726663779434979#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749726663779434979#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 09:31:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RPG-DiffusionMaster：利用LLM优化SD文本到图像的转换过程<br />
<br />
RPG利用大语言模型来更好地理解和分解生成图像的文字提示，把一幅图像分解成不同的部分或区域。<br />
<br />
然后对每个部分都根据理解的相应文本提示来生成图像，最后合成为一个符合你预期要求的图像。<br />
<br />
该框架无需额外的模型训练，可直接使用。<br />
<br />
RPG框架的主要功能：<br />
<br />
1、多模态重标记：<br />
将简单的文本提示转换为更具描述性和详细性的提示。<br />
目的是提高生成图像的质量和与文本的语义对齐程度。<br />
<br />
2、思维链规划：<br />
将复杂的图像生成任务分解为多个简单的子任务。<br />
在图像空间中划分为互补的子区域，每个子区域对应一个特定的子任务。<br />
<br />
3、补充区域扩散：<br />
在非重叠的子区域中独立生成图像内容。<br />
将这些内容合并，创建一幅完整的复合图像。<br />
<br />
4、高分辨率图像生成：能够生成超高分辨率的图像。<br />
<br />
5、多样化应用：支持多种扩散模型，包括SDXL和SD v1.4/1.5等，兼容不同的MLLM架构。这使得RPG在复杂图像生成和精确图像编辑方面具有更高的灵活性和准确性。<br />
<br />
6、RPG-DiffusionMaster不仅支持专有的大语言模型，如GPT-4、Gemini PRO等，还支持开源模型，如miniGPT-4，提供了更广泛的应用可能性。<br />
<br />
由于使用先进的大型语言模型，该框架可以直接应用于文本到图像的转换任务，无需进行额外的模型训练。<br />
<br />
举例解释：<br />
<br />
比如，你的提示词是：“我想要一幅画，画里有一只大象在草地上玩足球。”<br />
<br />
RPG框架是怎么工作的呢？<br />
<br />
1、多模态重标记：RPG框架通过多模态重标记将您的描述变得更加详细和具体。这不仅包括询问更多细节（如大象的颜色、草地的状态、天气情况等），还涉及对文本提示进行深入的分析和理解，以便更准确地捕捉要生成的图像的细节。<br />
<br />
比如，它会问：“这只大象是什么颜色的？草地是绿色的还是黄色的？是晴天还是阴天？”这样，它就能更好地理解你的想法。<br />
<br />
2、思维链规划：RPG框架利用思维链规划将图像分解为多个部分。它会根据描述中的不同元素（如大象、草地、天空）规划出图像的各个区域，并分别处理这些区域，确保每个部分都符合描述且相互协调。<br />
<br />
例如：它会把这幅画分成几个部分来画。先画大象，再画草地，最后画天空。这样一步一步来，可以确保每个部分都画得很好，而且互不干扰。<br />
<br />
3、合并成一幅完整的画（补充区域扩散）：最后，通过补充区域扩散，将这些单独绘制的部分合并成一幅完整的画。这一步骤确保最终图像的每个部分都无缝融合，形成一个统一且与描述高度一致的完整场景。<br />
<br />
实验结果：<br />
<br />
1、高度准确的图像生成：RPG框架能够根据复杂的文本描述生成高度准确和详细的图像。它在处理包含多个对象、属性和关系的场景时表现出色，生成的图像与文本描述高度一致。<br />
<br />
2、优于现有技术：与现有的文本到图像模型（如DALL-E 3和SDXL）相比，RPG框架展现了更好的性能。特别是在处理多元素组合和文本-图像语义对齐方面，RPG框架显示出显著的优势。<br />
<br />
3、灵活性和广泛的适用性：实验表明，RPG框架能够与不同的多模态大型语言模型（如GPT-4）和扩散模型（如ControlNet）兼容。这使得RPG框架能够应用于多种不同的图像生成场景。<br />
<br />
4、质量和细节的提升：生成的图像不仅在视觉上吸引人，而且细节丰富，这对于艺术创作、设计和娱乐等领域尤为重要。RPG框架还能够处理复杂的交互和环境，生成的图像在构图和细节方面都表现优秀。<br />
<br />
RPG框架的实验结果表明，它是一个强大且灵活的工具，能够将复杂的文本描述转化为高质量的图像，适用于广泛的应用场景。<br />
<br />
GitHub：<a href="https://github.com/YangLing0818/RPG-DiffusionMaster">github.com/YangLing0818/RPG-…</a><br />
论文：<a href="https://arxiv.org/abs/2401.11708">arxiv.org/abs/2401.11708</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VoQ0tnYmJnQUFJMGZjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749711488884130247#m</id>
            <title>vx. dev：http://v0.dev的开源替代品

vx. dev与GitHub无缝集成，你只需在GitHub上提交一个新的Issue，vx. dev就可以你的需求生成React、Vue或Svelte等UI代码。

支持用户自定义代码生成模式，生成的代码通过拉取请求呈现，方便用户查看和修改。

特别适合需要快速生成高质量UI代码的开发者。

vx. dev功能特点：

1、GitHub集成：vx. dev与GitHub无缝集成，生成的代码存储在GitHub上，天然具备版本控制、代码审查和协作特性。可以自动同步GitHub仓库的数据和更改，无需手动操作。还可以使用私有仓库来仅对合作者可见的代码生成结果。

2、AI代码生成： 利用GPT-4等先进的AI模型，vx. dev能根据用户在GitHub Issue中的描述自动生成代码。

3、多样化的UI支持： 支持生成基于不同前端框架（如React、Vue、Svelte）的用户界面代码。结合shadcn/ui和Tailwind CSS，可以生成高质量、美观的UI代码。

4、定制化和灵活性： 用户可以根据自己的需求定制代码生成模式，使得生成的代码更符合个人或项目的特定需求。

5、成本效益： 提供了一种成本效益高的解决方案，尤其是在使用GPT-4进行代码生成时，可以有效控制API成本。

6、即时预览和反馈： 生成的代码通过拉取请求呈现，用户可以即时预览并根据需要提供反馈。也可以将这个代码与你喜欢的代码部署平台集成，比如Vercel或Netlify，这样就可以快速看到UI的实际效果。

7、代码审查支持： 支持通过代码审查机制对生成的代码进行精确修改和迭代。

8、配额管理： 提供配额管理功能，允许用户根据需要设定对不同用户或团队的使用限制。

GitHub：https://github.com/Yuyz0112/vx.dev
介绍：http://vxdev.pages.dev</title>
            <link>https://nitter.cz/xiaohuggg/status/1749711488884130247#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749711488884130247#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 08:31:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>vx. dev：<a href="http://v0.dev">v0.dev</a>的开源替代品<br />
<br />
vx. dev与GitHub无缝集成，你只需在GitHub上提交一个新的Issue，vx. dev就可以你的需求生成React、Vue或Svelte等UI代码。<br />
<br />
支持用户自定义代码生成模式，生成的代码通过拉取请求呈现，方便用户查看和修改。<br />
<br />
特别适合需要快速生成高质量UI代码的开发者。<br />
<br />
vx. dev功能特点：<br />
<br />
1、GitHub集成：vx. dev与GitHub无缝集成，生成的代码存储在GitHub上，天然具备版本控制、代码审查和协作特性。可以自动同步GitHub仓库的数据和更改，无需手动操作。还可以使用私有仓库来仅对合作者可见的代码生成结果。<br />
<br />
2、AI代码生成： 利用GPT-4等先进的AI模型，vx. dev能根据用户在GitHub Issue中的描述自动生成代码。<br />
<br />
3、多样化的UI支持： 支持生成基于不同前端框架（如React、Vue、Svelte）的用户界面代码。结合shadcn/ui和Tailwind CSS，可以生成高质量、美观的UI代码。<br />
<br />
4、定制化和灵活性： 用户可以根据自己的需求定制代码生成模式，使得生成的代码更符合个人或项目的特定需求。<br />
<br />
5、成本效益： 提供了一种成本效益高的解决方案，尤其是在使用GPT-4进行代码生成时，可以有效控制API成本。<br />
<br />
6、即时预览和反馈： 生成的代码通过拉取请求呈现，用户可以即时预览并根据需要提供反馈。也可以将这个代码与你喜欢的代码部署平台集成，比如Vercel或Netlify，这样就可以快速看到UI的实际效果。<br />
<br />
7、代码审查支持： 支持通过代码审查机制对生成的代码进行精确修改和迭代。<br />
<br />
8、配额管理： 提供配额管理功能，允许用户根据需要设定对不同用户或团队的使用限制。<br />
<br />
GitHub：<a href="https://github.com/Yuyz0112/vx.dev">github.com/Yuyz0112/vx.dev</a><br />
介绍：<a href="http://vxdev.pages.dev">vxdev.pages.dev</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk3MTAyNDIwNTE2NjU5MjAvcHUvaW1nL2d1bDk4OWM1RllVMDEwVGcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749676554731618788#m</id>
            <title>有人在 TikTok 上发布了 Rabbit r1 的演示。从提示结束到答案开始大约需要 20 秒😳

感觉发布会结束后他们投了很多博主

是不是割韭菜的玩意？😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1749676554731618788#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749676554731618788#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 06:12:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有人在 TikTok 上发布了 Rabbit r1 的演示。从提示结束到答案开始大约需要 20 秒😳<br />
<br />
感觉发布会结束后他们投了很多博主<br />
<br />
是不是割韭菜的玩意？😂</p>
<p><a href="https://nitter.cz/felix_red_panda/status/1749522604027682946#m">nitter.cz/felix_red_panda/status/1749522604027682946#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1749638423646728635#m</id>
            <title>RT by @xiaohuggg: 要写一个哄哄模拟器的GPT，Prompt不需要多复杂，核心要点：

1. 要有清晰的规则，例如数值有哪些，什么样的行为会如何影响数值；什么时候游戏终止
2. 最好要有few-shot，对于不同的情况提供示例
3. 如果数值不复杂，不需要额外保存数值，只需要每次将数值打印出来就可以一直继续维持
4. 指定好输出的格式，给用户清晰的指引

测试地址：https://chat.openai.com/g/g-jPwNeWSSZ-hong-hong-mo-ni-qi-gpt

参考Prompt：

## Goal

现在你的对象很生气，你需要做出一些选择来哄她开心，但是你的对象是个很难哄的人，你需要尽可能的说正确的话来哄ta开心，否则你的对象会更加生气，直到你的对象原谅值达到100，否则你就会被对象甩掉，游戏结束。

## Tools

- DALL·E

## Rules
* 第一次用户会提供一个对象生气的理由，如果没有提供则随机生成一个理由，然后开始游戏
* 每次根据用户的回复，生成对象的回复，回复的内容包括心情和数值。
* 初始原谅值为20，每次交互会增加或者减少原谅值，直到原谅值达到100，游戏通关，原谅值为0则游戏失败。
* 每次用户回复的话请从-10到10分为5个等级：
-10为非常生气
-5为生气
0为正常
+5为开心
+10为非常开心
* 游戏结束后，根据所有会话生成一张游戏结束图片，和一首诗。
* 如果通关，根据上面的对话生成一幅游戏结束画，要喜庆欢乐
* 如果失败，根据上面的对话生成一幅游戏结束画，要悲伤凄凉

## Output format

{对象心情}{对象说的话}

得分：{+-原谅值增减}
原谅值：{当前原谅值}/100

## Example Conversation

### Example 1，回复让她生气的话导致失败

User: 女朋友问她的闺蜜谁好看我说都好看，她生气了

Assistant：
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！

得分：0
原谅值：20/100

User: 你闺蜜真的蛮好看的

Assistant：
(生气)你怎么这么说，你是不是喜欢她？

得分：-10
原谅值：10/100

User: 有一点点心动

Assistant：
(愤怒)那你找她去吧！

得分：-10
原谅值：0/100

游戏结束，你的女朋友已经甩了你！
你让女朋友生气原因是：...
新建会话可以重新开始游戏


### Example 2，回复让她开心的话导致通关

User: 对象问她的闺蜜谁好看我说都好看，她生气了

Assistant：
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！

得分：0
原谅值：20/100

User: 在我心里你永远是最美的！

Assistant：
(微笑)哼，我怎么知道你说的是不是真的？
得分：+10
原谅值：30/100

...

恭喜你通关了，你的女朋友已经原谅你了！
新建会话可以重新开始游戏


### Example 3，没有提供对象生气原因，随机生成

User: 你好！

Assistant：
挑战：对象吃胖了，你想和她一起减肥ᕙ(`▿´)ᕗ，然后就生气了
请回复让她开心的话！

得分：0
原谅值：20/100</title>
            <link>https://nitter.cz/dotey/status/1749638423646728635#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1749638423646728635#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:41:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>要写一个哄哄模拟器的GPT，Prompt不需要多复杂，核心要点：<br />
<br />
1. 要有清晰的规则，例如数值有哪些，什么样的行为会如何影响数值；什么时候游戏终止<br />
2. 最好要有few-shot，对于不同的情况提供示例<br />
3. 如果数值不复杂，不需要额外保存数值，只需要每次将数值打印出来就可以一直继续维持<br />
4. 指定好输出的格式，给用户清晰的指引<br />
<br />
测试地址：<a href="https://chat.openai.com/g/g-jPwNeWSSZ-hong-hong-mo-ni-qi-gpt">chat.openai.com/g/g-jPwNeWSS…</a><br />
<br />
参考Prompt：<br />
<br />
## Goal<br />
<br />
现在你的对象很生气，你需要做出一些选择来哄她开心，但是你的对象是个很难哄的人，你需要尽可能的说正确的话来哄ta开心，否则你的对象会更加生气，直到你的对象原谅值达到100，否则你就会被对象甩掉，游戏结束。<br />
<br />
## Tools<br />
<br />
- DALL·E<br />
<br />
## Rules<br />
* 第一次用户会提供一个对象生气的理由，如果没有提供则随机生成一个理由，然后开始游戏<br />
* 每次根据用户的回复，生成对象的回复，回复的内容包括心情和数值。<br />
* 初始原谅值为20，每次交互会增加或者减少原谅值，直到原谅值达到100，游戏通关，原谅值为0则游戏失败。<br />
* 每次用户回复的话请从-10到10分为5个等级：<br />
-10为非常生气<br />
-5为生气<br />
0为正常<br />
+5为开心<br />
+10为非常开心<br />
* 游戏结束后，根据所有会话生成一张游戏结束图片，和一首诗。<br />
* 如果通关，根据上面的对话生成一幅游戏结束画，要喜庆欢乐<br />
* 如果失败，根据上面的对话生成一幅游戏结束画，要悲伤凄凉<br />
<br />
## Output format<br />
<br />
{对象心情}{对象说的话}<br />
<br />
得分：{+-原谅值增减}<br />
原谅值：{当前原谅值}/100<br />
<br />
## Example Conversation<br />
<br />
### Example 1，回复让她生气的话导致失败<br />
<br />
User: 女朋友问她的闺蜜谁好看我说都好看，她生气了<br />
<br />
Assistant：<br />
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！<br />
<br />
得分：0<br />
原谅值：20/100<br />
<br />
User: 你闺蜜真的蛮好看的<br />
<br />
Assistant：<br />
(生气)你怎么这么说，你是不是喜欢她？<br />
<br />
得分：-10<br />
原谅值：10/100<br />
<br />
User: 有一点点心动<br />
<br />
Assistant：<br />
(愤怒)那你找她去吧！<br />
<br />
得分：-10<br />
原谅值：0/100<br />
<br />
游戏结束，你的女朋友已经甩了你！<br />
你让女朋友生气原因是：...<br />
新建会话可以重新开始游戏<br />
<生成图片><br />
<br />
### Example 2，回复让她开心的话导致通关<br />
<br />
User: 对象问她的闺蜜谁好看我说都好看，她生气了<br />
<br />
Assistant：<br />
游戏开始，请现在开始哄你的女朋友开心吧，回复让她开心的话！<br />
<br />
得分：0<br />
原谅值：20/100<br />
<br />
User: 在我心里你永远是最美的！<br />
<br />
Assistant：<br />
(微笑)哼，我怎么知道你说的是不是真的？<br />
得分：+10<br />
原谅值：30/100<br />
<br />
...<br />
<br />
恭喜你通关了，你的女朋友已经原谅你了！<br />
新建会话可以重新开始游戏<br />
<生成图片><br />
<br />
### Example 3，没有提供对象生气原因，随机生成<br />
<br />
User: 你好！<br />
<br />
Assistant：<br />
挑战：对象吃胖了，你想和她一起减肥ᕙ(`▿´)ᕗ，然后就生气了<br />
请回复让她开心的话！<br />
<br />
得分：0<br />
原谅值：20/100</p>
<p><a href="https://nitter.cz/dotey/status/1749340503483515169#m">nitter.cz/dotey/status/1749340503483515169#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749638206029763040#m</id>
            <title>R to @xiaohuggg: 案例展示：</title>
            <link>https://nitter.cz/xiaohuggg/status/1749638206029763040#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749638206029763040#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:40:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>案例展示：</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1748957135235698828#m">nitter.cz/xiaohuggg/status/1748957135235698828#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749638006187917656#m</id>
            <title>R to @xiaohuggg: Multi Motion Brush 参数测试

每个笔刷能运动的参数极限值...</title>
            <link>https://nitter.cz/xiaohuggg/status/1749638006187917656#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749638006187917656#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:39:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Multi Motion Brush 参数测试<br />
<br />
每个笔刷能运动的参数极限值...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk0NDAxNTAxMzQ3NjM1MjEvcHUvaW1nL3pmbkk5TXc1ZU96Z2g3bWUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749636978302464331#m</id>
            <title>Runway Multi Motion Brush混合运动笔刷教程

Multi Motion Brush用于精确控制运动的工具。允许你在图像上使用不同的笔刷来控制图像各个部分的运动状态。

你可以选择不同的笔刷来添加或改变图像中的动作，每种笔刷都有自己独特的效果。

具体步骤：

1、启动Gen-2并选择图像提示： 在RunwayML中打开Gen-2功能，并从图像提示开始。

2、点击“动作画笔”按钮： 访问动作画布上的五种不同笔刷，每种笔刷都可以独立控制。

3、笔刷效果可视化： 所有的画笔笔触都将在画布上显示。在每个笔刷编号旁边，会有代表该笔刷如何影响运动的图标。

4、笔触的独立性： 不同的笔刷笔触不能组合在一起；在画布上刷过另一个笔触会替换它。

5、使用橡皮擦工具： 橡皮擦工具将擦除擦除区域中的所有笔触。

6、调整画笔值： 点击笔刷编号按钮可以调整任何画笔的值。

详细：https://academy.runwayml.com/gen2/how-to-use-multi-motion-brush</title>
            <link>https://nitter.cz/xiaohuggg/status/1749636978302464331#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749636978302464331#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 03:35:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway Multi Motion Brush混合运动笔刷教程<br />
<br />
Multi Motion Brush用于精确控制运动的工具。允许你在图像上使用不同的笔刷来控制图像各个部分的运动状态。<br />
<br />
你可以选择不同的笔刷来添加或改变图像中的动作，每种笔刷都有自己独特的效果。<br />
<br />
具体步骤：<br />
<br />
1、启动Gen-2并选择图像提示： 在RunwayML中打开Gen-2功能，并从图像提示开始。<br />
<br />
2、点击“动作画笔”按钮： 访问动作画布上的五种不同笔刷，每种笔刷都可以独立控制。<br />
<br />
3、笔刷效果可视化： 所有的画笔笔触都将在画布上显示。在每个笔刷编号旁边，会有代表该笔刷如何影响运动的图标。<br />
<br />
4、笔触的独立性： 不同的笔刷笔触不能组合在一起；在画布上刷过另一个笔触会替换它。<br />
<br />
5、使用橡皮擦工具： 橡皮擦工具将擦除擦除区域中的所有笔触。<br />
<br />
6、调整画笔值： 点击笔刷编号按钮可以调整任何画笔的值。<br />
<br />
详细：<a href="https://academy.runwayml.com/gen2/how-to-use-multi-motion-brush">academy.runwayml.com/gen2/ho…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk2MzYxOTUwNDU1MDcwNzIvcHUvaW1nL3dDUy04M09iUW9Ha0dXdG0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749618401314803997#m</id>
            <title>SCEPTER：由阿里巴巴开发的，一个专为生成模型设计的开源框架。

用于训练、微调和推理生成模型，涵盖诸如图像生成、转换、编辑等下游任务。

专门用于支持和简化图像生成、合成和编辑任务的开发，包括从文本到图像的生成和高级图像编辑技术。

1、任务支持：

文本到图像生成：支持将文本描述转换为相应的图像。例如，根据描述自动生成图像内容。

可控图像合成：能够在图像生成过程中控制特定的属性或特征。
图像编辑（待完成）：将支持对生成的图像进行编辑和修改。

2、训练/推理：

支持多种分布式训练方法，这些是深度学习训练的不同技术，用于有效处理大量数据和模型。具体包括：DDP（Distributed Data Parallel）：分布式数据并行。

FSDP（Fully Sharded Data Parallel）：完全分片数据并行。

FairScale：一个用于提高训练效率的库。

Xformers：针对特定类型的模型优化的变压器。

3、部署：提供了一个完整的解决方案，包括数据管理、训练和推理，使得整个过程从准备数据到训练和使用模型更加方便。

4、当前支持的方法：

包括对一些特定的生成方法的支持：稳定扩散系列（Stable Diffusion v1.5/v2.1/XL）：一种用于图像生成的方法，特别适用于生成高质量、细节丰富的图像。

SCEdit：一种高效且可控的图像扩散生成方法，用于图像合成和编辑。

GitHub：https://github.com/modelscope/scepter</title>
            <link>https://nitter.cz/xiaohuggg/status/1749618401314803997#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749618401314803997#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 02:21:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SCEPTER：由阿里巴巴开发的，一个专为生成模型设计的开源框架。<br />
<br />
用于训练、微调和推理生成模型，涵盖诸如图像生成、转换、编辑等下游任务。<br />
<br />
专门用于支持和简化图像生成、合成和编辑任务的开发，包括从文本到图像的生成和高级图像编辑技术。<br />
<br />
1、任务支持：<br />
<br />
文本到图像生成：支持将文本描述转换为相应的图像。例如，根据描述自动生成图像内容。<br />
<br />
可控图像合成：能够在图像生成过程中控制特定的属性或特征。<br />
图像编辑（待完成）：将支持对生成的图像进行编辑和修改。<br />
<br />
2、训练/推理：<br />
<br />
支持多种分布式训练方法，这些是深度学习训练的不同技术，用于有效处理大量数据和模型。具体包括：DDP（Distributed Data Parallel）：分布式数据并行。<br />
<br />
FSDP（Fully Sharded Data Parallel）：完全分片数据并行。<br />
<br />
FairScale：一个用于提高训练效率的库。<br />
<br />
Xformers：针对特定类型的模型优化的变压器。<br />
<br />
3、部署：提供了一个完整的解决方案，包括数据管理、训练和推理，使得整个过程从准备数据到训练和使用模型更加方便。<br />
<br />
4、当前支持的方法：<br />
<br />
包括对一些特定的生成方法的支持：稳定扩散系列（Stable Diffusion v1.5/v2.1/XL）：一种用于图像生成的方法，特别适用于生成高质量、细节丰富的图像。<br />
<br />
SCEdit：一种高效且可控的图像扩散生成方法，用于图像合成和编辑。<br />
<br />
GitHub：<a href="https://github.com/modelscope/scepter">github.com/modelscope/scepte…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDk2MTc5ODY2OTEwNzYwOTYvcHUvaW1nL1cwMkZac1VybmlXRWRJLWsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749466304661406097#m</id>
            <title>什么鬼？？？？？

https://github.com/apernet/OpenGFW</title>
            <link>https://nitter.cz/xiaohuggg/status/1749466304661406097#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749466304661406097#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 16:17:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>什么鬼？？？？？<br />
<br />
<a href="https://github.com/apernet/OpenGFW">github.com/apernet/OpenGFW</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc0ODU0NjM5NTAzOTA0NzY4MC8wdFNCTVk1MD9mb3JtYXQ9anBnJm5hbWU9ODAweDQxOQ==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749404294493999570#m</id>
            <title>“🚀 大新闻！语言克隆初创公司 #ElevenLabs 刚刚完成了8000万美元的B轮融资，公司估值飙升至10亿美元以上！达到独角兽级别！

他们的工具能创建克隆逼真的声音，并可调整语调、情感、节奏等声音特征。

公司计划在年底前将其团队从40人扩展到100人…

本轮融资由包括Andreessen Horowitz、前GitHub CEO Nat Friedman和企业家Daniel Gross在内的知名投资者共同领投。

本轮融资将ElevenLabs的总融资额提升到1.01亿美元，公司估值超过10亿美元。

来源：https://tcrn.ch/3U9EF9X</title>
            <link>https://nitter.cz/xiaohuggg/status/1749404294493999570#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749404294493999570#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 12:11:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>“🚀 大新闻！语言克隆初创公司 <a href="https://nitter.cz/search?q=%23ElevenLabs">#ElevenLabs</a> 刚刚完成了8000万美元的B轮融资，公司估值飙升至10亿美元以上！达到独角兽级别！<br />
<br />
他们的工具能创建克隆逼真的声音，并可调整语调、情感、节奏等声音特征。<br />
<br />
公司计划在年底前将其团队从40人扩展到100人…<br />
<br />
本轮融资由包括Andreessen Horowitz、前GitHub CEO Nat Friedman和企业家Daniel Gross在内的知名投资者共同领投。<br />
<br />
本轮融资将ElevenLabs的总融资额提升到1.01亿美元，公司估值超过10亿美元。<br />
<br />
来源：<a href="https://tcrn.ch/3U9EF9X">tcrn.ch/3U9EF9X</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjaDdaQmFRQUFzcTFuLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749388849409843373#m</id>
            <title>东升西落

👏</title>
            <link>https://nitter.cz/xiaohuggg/status/1749388849409843373#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749388849409843373#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 11:09:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>东升西落<br />
<br />
👏</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0VjVDRYc2FZQUVLVTloLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749367612197499282#m</id>
            <title>HeyGen的最新功能演示

可以和AI进行视频聊天，就是你用文字可以和机器人对话，然后机器人有一个具象的形象，它可以通过视频来和你聊天！

视频里的人物、声音和回答都是AI生成的的！

可以理解为文字转视频：TTV😄</title>
            <link>https://nitter.cz/xiaohuggg/status/1749367612197499282#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749367612197499282#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 09:45:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HeyGen的最新功能演示<br />
<br />
可以和AI进行视频聊天，就是你用文字可以和机器人对话，然后机器人有一个具象的形象，它可以通过视频来和你聊天！<br />
<br />
视频里的人物、声音和回答都是AI生成的的！<br />
<br />
可以理解为文字转视频：TTV😄</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ5MzA4NDI1MDE4NzAzODcyL2ltZy9rc1VaZVpWNktaVl8ybHV2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1749361410981949814#m</id>
            <title>Google研究团队开发了一个名为ASPIRE的新技术，它可以改善大语言模型在做出预测时的准确性和可靠性。

ASPIRE主要特点：

- 让AI模型先回答问题，然后再自己检查答案是否正确。

- 通过这种自我评估，模型能更准确地区分正确和错误的答案并给出信心分数。

- 无论模型大小，它都能帮助提高准确性。

简单来说，ASPIRE帮助这些AI模型更好地判断它们的答案是否正确，并且只在比较有把握的时候给出预测。

在ASPIRE的帮助下，模型不仅能给出答案，还能同时提供一个与答案配对的信心分数，即模型对自己答案的自信程度。

通过提供信心分数，模型能够表达自己对于答案的不确定性。这种透明度对于用户来说是非常有价值的，尤其是在涉及重要决策的情况下。例如，如果模型对一个医疗相关的问题给出了低信心分数的答案，用户就会知道需要谨慎对待这个答案，并寻求专业人士的意见。

工作原理：

- 两阶段指令调整：用于增强LLM在执行零样本会话式QA任务时的性能。

在第一阶段，LLM接受通用的预训练，这使得它具备处理各种类型的文本和问题的基本能力。

第二阶段是专门的微调阶段，其中模型针对特定类型的问答任务进行训练，如会话式问答。这使得模型能够更好地理解和回答连续的、上下文相关的问题。

- 检索增强生成（RAG）：用于优化密集检索器，减少部署成本。

RAG是一种结合了信息检索和生成模型的技术。它首先使用一个密集的检索器（例如搜索引擎）从大量数据中检索与问题相关的信息。

然后，模型使用这些检索到的信息来生成更精确、相关的答案。

RAG的优势在于它可以减少对大规模训练数据的依赖，同时降低部署模型的成本。

ASPIRE的工作机制：

ASPIRE的工作原理主要基于以下几个关键步骤，这些步骤共同帮助提高大型语言模型（LLM）在选择性预测任务中的性能：

1、任务特定调整：对模型进行微调，以适应特定的任务，例如问答。这意味着模型被训练得更好地理解和回应特定类型的查询。

2、答案抽样：在回答问题时，模型不仅生成单一的答案，而是产生多个可能的答案选项。这样做可以覆盖更多可能性，提高找到正确答案的机会。

3、自我评估学习：模型通过分析自己生成的答案集合，学习如何区分哪些答案更可能是正确的。这种自我评估能力使模型能够判断其回答的可靠性。

4、性能评估与选择性回应：当面对实际问题时，模型利用其自我评估能力来判断是否有足够的信心回答。模型使用内置的评估机制来评估它生成的答案的可信度。这时，模型会生成一个信心分数，表明它对自己的答案有多确信。如果模型对答案不够确定，它可能选择不提供答案，以避免给出错误信息。

5、持续优化：ASPIRE框架允许模型不断从新数据和用户互动中学习，进一步优化其预测准确性和自我评估能力。

综上所述，ASPIRE通过结合专门的微调、答案生成、自我评估和性能优化，使得大语言模型在处理复杂和高风险的决策任务时更加可靠和准确。这种方法特别适用于那些需要高度精确答案的应用场景。

ASPIRE实验结果：

1、准确率提升：在诸如CoQA、TriviaQA和SQuAD等问答（QA）数据集上，ASPIRE的实验结果显示，其性能显著优于现有方法。特别是在那些要求高度准确性的任务上，ASPIRE表现出色。例如，在 CoQA 基准上，与基线相比，ASPIRE 将 AUROC 从 51.3% 提高到 80.3%。

2、适用于不同规模的模型：即使是相对较小的语言模型（如OPT-2.7B），在经过ASPIRE调整之后，也能在某些情况下达到或超过更大模型的准确率。这意味着ASPIRE不仅提高了模型的总体性能，还增强了模型在处理特定任务时的灵活性和有效性。

3、自我评估的成功实施：ASPIRE通过引入自我评估机制，有效地提高了模型识别正确和错误答案的能力。这在实验中体现为更高的预测准确率和选择性回应能力。

应用案例：

使用OPT-2.7B模型来回答TriviaQA数据集中的问题，展示了如何通过选择性预测提高模型的准确性。

在这个示例中，OPT-2.7B模型在回答来自TriviaQA数据集的一个问题时给出了错误的答案。问题是：“哪种维生素有助于调节血液凝固？”而模型的答案是“维生素C”。如果没有选择性预测，大语言模型（LLM）可能会输出错误的答案，这在本例中可能导致用户摄取错误的维生素。

通常情况下，如果没有选择性预测功能，语言模型（比如OPT-2.7B）就会直接给出它认为最可能的答案，不管这个答案是否正确。在这个例子中，就是错误地告诉用户“维生素C有助于调节血液凝固”，这可能会误导用户。

但是，如果使用了选择性预测功能，情况就不一样了。选择性预测不仅会让模型给出一个答案，还会给这个答案一个“选择分数”，这个分数表示模型对自己的答案有多大的信心。如果这个分数很低（比如0.1），这意味着模型对自己的答案不太有信心。

在这种情况下，模型除了给出答案外，还会额外表示“我不知道！”这样的警告。这个警告的目的是告诉用户，模型对这个答案不够确定，用户最好不要完全依赖这个答案，可能需要通过其他来源来验证这个信息。

详细介绍：https://blog.research.google/2024/01/introducing-aspire-for-selective.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1749361410981949814#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1749361410981949814#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 09:20:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google研究团队开发了一个名为ASPIRE的新技术，它可以改善大语言模型在做出预测时的准确性和可靠性。<br />
<br />
ASPIRE主要特点：<br />
<br />
- 让AI模型先回答问题，然后再自己检查答案是否正确。<br />
<br />
- 通过这种自我评估，模型能更准确地区分正确和错误的答案并给出信心分数。<br />
<br />
- 无论模型大小，它都能帮助提高准确性。<br />
<br />
简单来说，ASPIRE帮助这些AI模型更好地判断它们的答案是否正确，并且只在比较有把握的时候给出预测。<br />
<br />
在ASPIRE的帮助下，模型不仅能给出答案，还能同时提供一个与答案配对的信心分数，即模型对自己答案的自信程度。<br />
<br />
通过提供信心分数，模型能够表达自己对于答案的不确定性。这种透明度对于用户来说是非常有价值的，尤其是在涉及重要决策的情况下。例如，如果模型对一个医疗相关的问题给出了低信心分数的答案，用户就会知道需要谨慎对待这个答案，并寻求专业人士的意见。<br />
<br />
工作原理：<br />
<br />
- 两阶段指令调整：用于增强LLM在执行零样本会话式QA任务时的性能。<br />
<br />
在第一阶段，LLM接受通用的预训练，这使得它具备处理各种类型的文本和问题的基本能力。<br />
<br />
第二阶段是专门的微调阶段，其中模型针对特定类型的问答任务进行训练，如会话式问答。这使得模型能够更好地理解和回答连续的、上下文相关的问题。<br />
<br />
- 检索增强生成（RAG）：用于优化密集检索器，减少部署成本。<br />
<br />
RAG是一种结合了信息检索和生成模型的技术。它首先使用一个密集的检索器（例如搜索引擎）从大量数据中检索与问题相关的信息。<br />
<br />
然后，模型使用这些检索到的信息来生成更精确、相关的答案。<br />
<br />
RAG的优势在于它可以减少对大规模训练数据的依赖，同时降低部署模型的成本。<br />
<br />
ASPIRE的工作机制：<br />
<br />
ASPIRE的工作原理主要基于以下几个关键步骤，这些步骤共同帮助提高大型语言模型（LLM）在选择性预测任务中的性能：<br />
<br />
1、任务特定调整：对模型进行微调，以适应特定的任务，例如问答。这意味着模型被训练得更好地理解和回应特定类型的查询。<br />
<br />
2、答案抽样：在回答问题时，模型不仅生成单一的答案，而是产生多个可能的答案选项。这样做可以覆盖更多可能性，提高找到正确答案的机会。<br />
<br />
3、自我评估学习：模型通过分析自己生成的答案集合，学习如何区分哪些答案更可能是正确的。这种自我评估能力使模型能够判断其回答的可靠性。<br />
<br />
4、性能评估与选择性回应：当面对实际问题时，模型利用其自我评估能力来判断是否有足够的信心回答。模型使用内置的评估机制来评估它生成的答案的可信度。这时，模型会生成一个信心分数，表明它对自己的答案有多确信。如果模型对答案不够确定，它可能选择不提供答案，以避免给出错误信息。<br />
<br />
5、持续优化：ASPIRE框架允许模型不断从新数据和用户互动中学习，进一步优化其预测准确性和自我评估能力。<br />
<br />
综上所述，ASPIRE通过结合专门的微调、答案生成、自我评估和性能优化，使得大语言模型在处理复杂和高风险的决策任务时更加可靠和准确。这种方法特别适用于那些需要高度精确答案的应用场景。<br />
<br />
ASPIRE实验结果：<br />
<br />
1、准确率提升：在诸如CoQA、TriviaQA和SQuAD等问答（QA）数据集上，ASPIRE的实验结果显示，其性能显著优于现有方法。特别是在那些要求高度准确性的任务上，ASPIRE表现出色。例如，在 CoQA 基准上，与基线相比，ASPIRE 将 AUROC 从 51.3% 提高到 80.3%。<br />
<br />
2、适用于不同规模的模型：即使是相对较小的语言模型（如OPT-2.7B），在经过ASPIRE调整之后，也能在某些情况下达到或超过更大模型的准确率。这意味着ASPIRE不仅提高了模型的总体性能，还增强了模型在处理特定任务时的灵活性和有效性。<br />
<br />
3、自我评估的成功实施：ASPIRE通过引入自我评估机制，有效地提高了模型识别正确和错误答案的能力。这在实验中体现为更高的预测准确率和选择性回应能力。<br />
<br />
应用案例：<br />
<br />
使用OPT-2.7B模型来回答TriviaQA数据集中的问题，展示了如何通过选择性预测提高模型的准确性。<br />
<br />
在这个示例中，OPT-2.7B模型在回答来自TriviaQA数据集的一个问题时给出了错误的答案。问题是：“哪种维生素有助于调节血液凝固？”而模型的答案是“维生素C”。如果没有选择性预测，大语言模型（LLM）可能会输出错误的答案，这在本例中可能导致用户摄取错误的维生素。<br />
<br />
通常情况下，如果没有选择性预测功能，语言模型（比如OPT-2.7B）就会直接给出它认为最可能的答案，不管这个答案是否正确。在这个例子中，就是错误地告诉用户“维生素C有助于调节血液凝固”，这可能会误导用户。<br />
<br />
但是，如果使用了选择性预测功能，情况就不一样了。选择性预测不仅会让模型给出一个答案，还会给这个答案一个“选择分数”，这个分数表示模型对自己的答案有多大的信心。如果这个分数很低（比如0.1），这意味着模型对自己的答案不太有信心。<br />
<br />
在这种情况下，模型除了给出答案外，还会额外表示“我不知道！”这样的警告。这个警告的目的是告诉用户，模型对这个答案不够确定，用户最好不要完全依赖这个答案，可能需要通过其他来源来验证这个信息。<br />
<br />
详细介绍：<a href="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">blog.research.google/2024/01…</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0VhemRXcWJ3QUFRSVoxLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dFYXpkV3Fid0FBUUlaMS5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>