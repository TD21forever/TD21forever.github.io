<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726966015052169228#m</id>
            <title>Stable to Video 要来了…

看效果是有点炸裂👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1726966015052169228#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726966015052169228#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 14:09:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable to Video 要来了…<br />
<br />
看效果是有点炸裂👍</p>
<p><a href="https://nitter.cz/EMostaque/status/1726929962211647855#m">nitter.cz/EMostaque/status/1726929962211647855#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726955744866750674#m</id>
            <title>使用@suno_ai_ 和 Meta的 Audiocraft

制作的说唱歌曲😎

当然还融合了博主的一点点才华👌</title>
            <link>https://nitter.cz/xiaohuggg/status/1726955744866750674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726955744866750674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 13:28:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用<a href="https://nitter.cz/suno_ai_" title="Suno">@suno_ai_</a> 和 Meta的 Audiocraft<br />
<br />
制作的说唱歌曲😎<br />
<br />
当然还融合了博主的一点点才华👌</p>
<p><a href="https://nitter.cz/hanqing_me/status/1726934224043974725#m">nitter.cz/hanqing_me/status/1726934224043974725#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726943097387683902#m</id>
            <title>自从GPTs出来后，人人都能做，大量涌现，然后就出现了各种GPTs导航站。

最后的结果是连GPTs导航站都需要导航了😂

这个GPTs导航站思路不错，使用者可以对GPTs进行投票，把高质量的GPT筛选出来...避免鱼龙混杂！

还可以按类别筛选找到自己需要的GPT，还可提交自己的GPT...

🔗http://GPTseek.com

不过感觉目前还是比较简陋，有很大升级空间，比如可以增加评论点评、使用调用情况、以及更详细的GPTs功能升级迭代近况什么的。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726943097387683902#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726943097387683902#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 12:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>自从GPTs出来后，人人都能做，大量涌现，然后就出现了各种GPTs导航站。<br />
<br />
最后的结果是连GPTs导航站都需要导航了😂<br />
<br />
这个GPTs导航站思路不错，使用者可以对GPTs进行投票，把高质量的GPT筛选出来...避免鱼龙混杂！<br />
<br />
还可以按类别筛选找到自己需要的GPT，还可提交自己的GPT...<br />
<br />
🔗<a href="http://GPTseek.com">GPTseek.com</a><br />
<br />
不过感觉目前还是比较简陋，有很大升级空间，比如可以增加评论点评、使用调用情况、以及更详细的GPTs功能升级迭代近况什么的。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY5MzkxNDIxOTI1MjUzMTIvcHUvaW1nL1A5TnFtYU5UcFA2UDFYNEkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726928308288561284#m</id>
            <title>Video-LLaVA：大型视觉语言模型 能更好地理解和处理图像和视频。

它通过一种特殊的技术，将图像和视频中的信息转换成类似于文字的格式，使得计算机能够用处理语言的方式来处理视觉信息。

这使得模型能够更好地理解视频内容，就像理解一段文字一样。

测试了下，感觉还不错的！但是要用英文😂

而且该模型能够同时处理文本和视觉信号，提供了一种多模态的理解能力，这对于自动问答系统和其他需要理解复杂视觉信息的应用非常有用。

Video-LLaVA在多个图像和视频基准测试中表现出色，显示了其在不同类型的视觉数据上的强大适应能力。

主要工作原理特点：

1、统一视觉表示：Video-LLaVA通过将图像和视频的特征预先对齐到一个统一的视觉特征空间中，解决了以往模型在处理视觉信息时存在的不对齐问题。这种方法使得大型语言模型（LLM）能够更有效地学习和理解多模态（图像和视频）信息。

2、联合训练：该模型不仅对图像和视频特征进行预对齐，还进行了图像和视频的联合训练。这种训练方式有助于提高模型在理解多模态信息方面的能力。

3、高效的模型结构：Video-LLaVA包括语言绑定编码器（LanguageBind encoders）用于提取原始视觉信号（如图像或视频）的特征，大语言模型（如Vicuna），视觉投影层，以及词嵌入层。这些组件共同工作，提供了一个高效且强大的视觉语言处理框架。

4、优越的性能：在多个图像和视频基准测试中，Video-LLaVA展示了优越的性能。它不仅在图像理解方面超越了先进的视觉语言模型，如mPLUG-owl-7B和InstructBLIP-7B，而且在视频理解方面也超越了专门为视频设计的模型，如Video-ChatGPT。

GitHub：https://github.com/PKU-YuanGroup/Video-LLaVA
论文：https://arxiv.org/abs/2311.10122
HuggingFace演示：https://huggingface.co/spaces/LanguageBind/Video-LLaVA
在线体验：https://replicate.com/nateraw/video-llava</title>
            <link>https://nitter.cz/xiaohuggg/status/1726928308288561284#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726928308288561284#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 11:39:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Video-LLaVA：大型视觉语言模型 能更好地理解和处理图像和视频。<br />
<br />
它通过一种特殊的技术，将图像和视频中的信息转换成类似于文字的格式，使得计算机能够用处理语言的方式来处理视觉信息。<br />
<br />
这使得模型能够更好地理解视频内容，就像理解一段文字一样。<br />
<br />
测试了下，感觉还不错的！但是要用英文😂<br />
<br />
而且该模型能够同时处理文本和视觉信号，提供了一种多模态的理解能力，这对于自动问答系统和其他需要理解复杂视觉信息的应用非常有用。<br />
<br />
Video-LLaVA在多个图像和视频基准测试中表现出色，显示了其在不同类型的视觉数据上的强大适应能力。<br />
<br />
主要工作原理特点：<br />
<br />
1、统一视觉表示：Video-LLaVA通过将图像和视频的特征预先对齐到一个统一的视觉特征空间中，解决了以往模型在处理视觉信息时存在的不对齐问题。这种方法使得大型语言模型（LLM）能够更有效地学习和理解多模态（图像和视频）信息。<br />
<br />
2、联合训练：该模型不仅对图像和视频特征进行预对齐，还进行了图像和视频的联合训练。这种训练方式有助于提高模型在理解多模态信息方面的能力。<br />
<br />
3、高效的模型结构：Video-LLaVA包括语言绑定编码器（LanguageBind encoders）用于提取原始视觉信号（如图像或视频）的特征，大语言模型（如Vicuna），视觉投影层，以及词嵌入层。这些组件共同工作，提供了一个高效且强大的视觉语言处理框架。<br />
<br />
4、优越的性能：在多个图像和视频基准测试中，Video-LLaVA展示了优越的性能。它不仅在图像理解方面超越了先进的视觉语言模型，如mPLUG-owl-7B和InstructBLIP-7B，而且在视频理解方面也超越了专门为视频设计的模型，如Video-ChatGPT。<br />
<br />
GitHub：<a href="https://github.com/PKU-YuanGroup/Video-LLaVA">github.com/PKU-YuanGroup/Vid…</a><br />
论文：<a href="https://arxiv.org/abs/2311.10122">arxiv.org/abs/2311.10122</a><br />
HuggingFace演示：<a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA">huggingface.co/spaces/Langua…</a><br />
在线体验：<a href="https://replicate.com/nateraw/video-llava">replicate.com/nateraw/video-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY5MjgwMDUwOTQ4NTA1NjAvcHUvaW1nL2U3YlA1UzBGOHl6czdqekwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726891372136013900#m</id>
            <title>百度：目前文心一言用户数已达7000万。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726891372136013900#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726891372136013900#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 09:12:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>百度：目前文心一言用户数已达7000万。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726880322879635723#m</id>
            <title>RealtimeTTS ：能够实时地将文本转换成语音

🚀 即时反应：文本输入的同时，RealtimeTTS立即开始语音合成，无需等待整个文本输入完毕。

🌊 流式处理：能够处理持续不断的文本流，而不仅限于单个、静态的文本块。

🔍 句子分割：使用先进的算法精准识别句子的结束点，加快语音合成的开始。

📏 适应不同长度：无论文本是长是短，都能保持快速响应，适用于各种长度的文本。

🌐 多引擎兼容：能够与多个语音合成引擎协同工作，例如Azure、Elevenlabs、Coqui XTTS等。

🔧 扩展性：它还允许添加自定义的文本到语音引擎，提供了更大的灵活性和扩展性。

RealtimeTTS非常适合需要实时语音反馈的应用场景的工具，如交互式教学、游戏、实时翻译或语音助手等。通过即时反应和流式处理，它能够提供一个流畅且自然的用户体验。

GitHub：https://github.com/KoljaB/RealtimeTTS</title>
            <link>https://nitter.cz/xiaohuggg/status/1726880322879635723#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726880322879635723#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 08:28:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>RealtimeTTS ：能够实时地将文本转换成语音<br />
<br />
🚀 即时反应：文本输入的同时，RealtimeTTS立即开始语音合成，无需等待整个文本输入完毕。<br />
<br />
🌊 流式处理：能够处理持续不断的文本流，而不仅限于单个、静态的文本块。<br />
<br />
🔍 句子分割：使用先进的算法精准识别句子的结束点，加快语音合成的开始。<br />
<br />
📏 适应不同长度：无论文本是长是短，都能保持快速响应，适用于各种长度的文本。<br />
<br />
🌐 多引擎兼容：能够与多个语音合成引擎协同工作，例如Azure、Elevenlabs、Coqui XTTS等。<br />
<br />
🔧 扩展性：它还允许添加自定义的文本到语音引擎，提供了更大的灵活性和扩展性。<br />
<br />
RealtimeTTS非常适合需要实时语音反馈的应用场景的工具，如交互式教学、游戏、实时翻译或语音助手等。通过即时反应和流式处理，它能够提供一个流畅且自然的用户体验。<br />
<br />
GitHub：<a href="https://github.com/KoljaB/RealtimeTTS">github.com/KoljaB/RealtimeTT…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY4Nzg0OTQxOTY3MTE0MjQvcHUvaW1nLzRPdWlPZjZXZ2QwTFpCYWMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1726809515448840482#m</id>
            <title>RT by @xiaohuggg: 通往 AGI (人工通用智能) 的激动人心、充满危险的旅程 | Ilya Sutskever | TED

在 OpenAI 的管理层巨变震动硅谷并引起国际关注的几周前，该公司联合创始人及首席科学家 Ilya Sutskever 深入探讨了人工通用智能 (AGI) 的革命性潜力。他详细阐述了 AGI 如何可能超越人类智能，从而彻底改变我们生活的方方面面。在这次演讲中，你将听到他对 AGI 所带来的巨大机遇与潜在风险的看法，以及他对于通过前所未有的合作来确保 AGI 安全、有益发展的乐观期望。（该演讲于 2023 年 10 月 17 日录制）</title>
            <link>https://nitter.cz/dotey/status/1726809515448840482#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1726809515448840482#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 03:47:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通往 AGI (人工通用智能) 的激动人心、充满危险的旅程 | Ilya Sutskever | TED<br />
<br />
在 OpenAI 的管理层巨变震动硅谷并引起国际关注的几周前，该公司联合创始人及首席科学家 Ilya Sutskever 深入探讨了人工通用智能 (AGI) 的革命性潜力。他详细阐述了 AGI 如何可能超越人类智能，从而彻底改变我们生活的方方面面。在这次演讲中，你将听到他对 AGI 所带来的巨大机遇与潜在风险的看法，以及他对于通过前所未有的合作来确保 AGI 安全、有益发展的乐观期望。（该演讲于 2023 年 10 月 17 日录制）</p>
<p><a href="https://nitter.cz/DrJimFan/status/1726763792556839253#m">nitter.cz/DrJimFan/status/1726763792556839253#m</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI2ODA3MDIyNjM5NzEwMjA4L2ltZy91R0YwNHhBajQ2dThzUlVsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726828110258348418#m</id>
            <title>R to @xiaohuggg: Flipper Zero的主要功能：

1、多协议通信：支持RFID, NFC, 蓝牙低能耗（BLE）等多种无线通信协议。

2、硬件交互：具备与各种电子设备交互的能力，如读写RFID标签、发送红外信号等。

3、可编程性：用户可以根据自己的需求编写或修改代码，实现特定的功能。

主动搜索与攻击：

搜索功能：Flipper Zero可以被编程来搜索特定的无线信号，比如寻找特定频率的RFID标签或BLE设备。

攻击行为：如果用户编程使其执行某些攻击性行为（如模拟或干扰无线信号），Flipper Zero理论上可以用于这样的目的。但这通常涉及到法律和伦理问题。

Flipper Zero对iPhone的影响：

蓝牙攻击：Flipper Zero可以利用iOS设备的蓝牙低能耗（BLE）技术进行攻击。这种攻击会导致iPhone或iPad显示多个蓝牙连接弹窗，使设备难以正常使用。

攻击方式：攻击利用了BLE的配对过程中的漏洞。BLE广泛应用于Apple的生态系统，如AirDrop、HandOff、iBeacon、HomeKit等。

攻击影响：这种攻击可以在公共场所如地铁中造成混乱，属于一种服务拒绝（DOS）攻击。

1、
尽管iOS 17系列已经进行了多次更新，包括最新的iOS 17.2测试版，Apple仍未实施有效措施来阻止这种攻击。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726828110258348418#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726828110258348418#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 05:01:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Flipper Zero的主要功能：<br />
<br />
1、多协议通信：支持RFID, NFC, 蓝牙低能耗（BLE）等多种无线通信协议。<br />
<br />
2、硬件交互：具备与各种电子设备交互的能力，如读写RFID标签、发送红外信号等。<br />
<br />
3、可编程性：用户可以根据自己的需求编写或修改代码，实现特定的功能。<br />
<br />
主动搜索与攻击：<br />
<br />
搜索功能：Flipper Zero可以被编程来搜索特定的无线信号，比如寻找特定频率的RFID标签或BLE设备。<br />
<br />
攻击行为：如果用户编程使其执行某些攻击性行为（如模拟或干扰无线信号），Flipper Zero理论上可以用于这样的目的。但这通常涉及到法律和伦理问题。<br />
<br />
Flipper Zero对iPhone的影响：<br />
<br />
蓝牙攻击：Flipper Zero可以利用iOS设备的蓝牙低能耗（BLE）技术进行攻击。这种攻击会导致iPhone或iPad显示多个蓝牙连接弹窗，使设备难以正常使用。<br />
<br />
攻击方式：攻击利用了BLE的配对过程中的漏洞。BLE广泛应用于Apple的生态系统，如AirDrop、HandOff、iBeacon、HomeKit等。<br />
<br />
攻击影响：这种攻击可以在公共场所如地铁中造成混乱，属于一种服务拒绝（DOS）攻击。<br />
<br />
1、<br />
尽管iOS 17系列已经进行了多次更新，包括最新的iOS 17.2测试版，Apple仍未实施有效措施来阻止这种攻击。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY4Mjc4NTcwNjkxMzc5MjAvcHUvaW1nL1V6NmIxNTdFemdSWXBrZkMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726827495679500467#m</id>
            <title>R to @xiaohuggg: 另一个演示

未拆封的玩具也能控制😅</title>
            <link>https://nitter.cz/xiaohuggg/status/1726827495679500467#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726827495679500467#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 04:58:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个演示<br />
<br />
未拆封的玩具也能控制😅</p>
<p><a href="https://nitter.cz/SecurityTrybe/status/1725287732958593160#m">nitter.cz/SecurityTrybe/status/1725287732958593160#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726827491824980043#m</id>
            <title>发现看一个很牛X的设备： Flipper Zero

一个小巧的黑客工具，能够与多种无线协议进行交互，包括不限于RFID, NFC, 蓝牙低能耗（BLE），以及无线射频（RF）通信。

然后有人发现BLE设备存在安全漏洞，用这玩意可以捕获BLE设备设备信号并进行控制。

视频演示了通过Flipper Zero捕获控制成人玩具😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1726827491824980043#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726827491824980043#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 04:58:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发现看一个很牛X的设备： Flipper Zero<br />
<br />
一个小巧的黑客工具，能够与多种无线协议进行交互，包括不限于RFID, NFC, 蓝牙低能耗（BLE），以及无线射频（RF）通信。<br />
<br />
然后有人发现BLE设备存在安全漏洞，用这玩意可以捕获BLE设备设备信号并进行控制。<br />
<br />
视频演示了通过Flipper Zero捕获控制成人玩具😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY4MjUwODE3OTA0ODQ0ODAvcHUvaW1nL2ktNmxfS01YX3owQ0JCZ3kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726803125548491168#m</id>
            <title>GOAT（Go To Anything）：一种通用的机器人智能导航系统。

它的特别之处在于，可以让机器人在一个它们之前从未进入过的环境里，自主地找到并前往特定的物体。

你可以用不同的方式告诉它要找什么东西：可以是图片、语音指令，或者物体类别（如椅子或杯子）它可以自己探索环境，找到并前往目标物体。

GOAT的主要功能包括：

1、通用导航能力：能够在完全未知的环境中搜索和导航到任何以图像、语言或类别指定的对象。

2、环境感知和对象识别：通过RGB-D摄像头输入，感知系统能够检测和定位场景中的对象实例。

3、顶视图语义地图构建：将检测到的对象实例定位到场景的顶视图语义地图中。

4、对象实例记忆：维护一个对象实例记忆库，存储每个实例被观察到的图像，使代理能够定位并导航到之前观察到的任何对象实例。

5、目标导向的导航策略：全局策略根据以语言、图像或类别指定的目标对象，输出地图中的长期目标。本地策略则规划出实现这一长期目标的轨迹，并输出代理应采取的行动。

6、实时数据处理：处理当前深度图像、RGB图像和来自机载传感器的姿态读数。

7、大规模实证评估：在未见过的环境中进行“野外”评估，测试其在没有预先计算的地图或对象位置的情况下的导航能力。

8、多模态目标匹配：能够根据目标的不同模态（图像、语言或类别）进行匹配和导航。

GOAT项目的工作原理可以分为几个关键步骤：

1、环境探索与感知：当GOAT系统的机器人或智能设备进入一个新环境时，它首先需要探索这个环境。它使用装在机器上的RGB-D摄像头（这种摄像头可以捕捉彩色图像和深度信息）来观察周围的空间。

2、对象检测和定位：在探索的过程中，GOAT系统会识别和定位环境中的各种物体。它通过分析摄像头捕获的图像来识别物体，并把这些物体的位置标记在一个虚拟的地图上。这个地图是系统自己创建的，用来表示它所在的环境。

3、对象实例记忆：GOAT系统会记住它所见过的每个物体。这意味着一旦它识别并定位了一个物体，它就会把这个信息存储起来，以便以后能够快速找到这个物体。

4、接收和解析目标指令：当GOAT系统接收到一个寻找特定物体的任务时，这个任务可以是以图像、语言描述或物体类别的形式给出的。系统会解析这些指令，确定要寻找的物体是什么。

5、匹配和导航：GOAT系统会在它的记忆中查找是否见过与指令匹配的物体。如果找到了，它就会规划一条路径，导航到那个物体的位置。如果没找到，它会继续探索环境，直到找到匹配的物体。

6、动态适应和学习：随着时间的推移和环境的变化，GOAT系统能够适应新情况，并从经验中学习，以更有效地完成任务。

总的来说，GOAT项目通过智能地探索环境、识别和记忆物体，以及灵活地接收和执行各种寻找物体的任务，展现了一种高度自主和适应性强的导航能力。

应用场景：

家庭和室内环境：GOAT系统可以在家庭或其他室内环境中应用，例如帮助人们找到特定的物品，或者在复杂的室内布局中导航。

拾取和放置任务：GOAT可以用于执行拾取和放置任务，比如找到一个物体并将其搬运到另一个位置。

社交导航：GOAT还可以应用于社交导航场景，比如在有人的环境中导航，避开人群或跟随特定的人。

辅助机器人：在辅助机器人领域，GOAT可以帮助机器人更有效地在未知环境中服务，例如在医院或老人护理设施中找到和运送物品。

紧急响应和探索：在紧急响应或探索任务中，GOAT可以帮助机器人在未知或危险的环境中找到特定的物体或地点。

项目及演示：https://theophilegervet.github.io/projects/goat/
论文：https://theophilegervet.github.io/projects/goat/
GitHub：https://github.com/facebookresearch/home-robot</title>
            <link>https://nitter.cz/xiaohuggg/status/1726803125548491168#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726803125548491168#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 03:22:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GOAT（Go To Anything）：一种通用的机器人智能导航系统。<br />
<br />
它的特别之处在于，可以让机器人在一个它们之前从未进入过的环境里，自主地找到并前往特定的物体。<br />
<br />
你可以用不同的方式告诉它要找什么东西：可以是图片、语音指令，或者物体类别（如椅子或杯子）它可以自己探索环境，找到并前往目标物体。<br />
<br />
GOAT的主要功能包括：<br />
<br />
1、通用导航能力：能够在完全未知的环境中搜索和导航到任何以图像、语言或类别指定的对象。<br />
<br />
2、环境感知和对象识别：通过RGB-D摄像头输入，感知系统能够检测和定位场景中的对象实例。<br />
<br />
3、顶视图语义地图构建：将检测到的对象实例定位到场景的顶视图语义地图中。<br />
<br />
4、对象实例记忆：维护一个对象实例记忆库，存储每个实例被观察到的图像，使代理能够定位并导航到之前观察到的任何对象实例。<br />
<br />
5、目标导向的导航策略：全局策略根据以语言、图像或类别指定的目标对象，输出地图中的长期目标。本地策略则规划出实现这一长期目标的轨迹，并输出代理应采取的行动。<br />
<br />
6、实时数据处理：处理当前深度图像、RGB图像和来自机载传感器的姿态读数。<br />
<br />
7、大规模实证评估：在未见过的环境中进行“野外”评估，测试其在没有预先计算的地图或对象位置的情况下的导航能力。<br />
<br />
8、多模态目标匹配：能够根据目标的不同模态（图像、语言或类别）进行匹配和导航。<br />
<br />
GOAT项目的工作原理可以分为几个关键步骤：<br />
<br />
1、环境探索与感知：当GOAT系统的机器人或智能设备进入一个新环境时，它首先需要探索这个环境。它使用装在机器上的RGB-D摄像头（这种摄像头可以捕捉彩色图像和深度信息）来观察周围的空间。<br />
<br />
2、对象检测和定位：在探索的过程中，GOAT系统会识别和定位环境中的各种物体。它通过分析摄像头捕获的图像来识别物体，并把这些物体的位置标记在一个虚拟的地图上。这个地图是系统自己创建的，用来表示它所在的环境。<br />
<br />
3、对象实例记忆：GOAT系统会记住它所见过的每个物体。这意味着一旦它识别并定位了一个物体，它就会把这个信息存储起来，以便以后能够快速找到这个物体。<br />
<br />
4、接收和解析目标指令：当GOAT系统接收到一个寻找特定物体的任务时，这个任务可以是以图像、语言描述或物体类别的形式给出的。系统会解析这些指令，确定要寻找的物体是什么。<br />
<br />
5、匹配和导航：GOAT系统会在它的记忆中查找是否见过与指令匹配的物体。如果找到了，它就会规划一条路径，导航到那个物体的位置。如果没找到，它会继续探索环境，直到找到匹配的物体。<br />
<br />
6、动态适应和学习：随着时间的推移和环境的变化，GOAT系统能够适应新情况，并从经验中学习，以更有效地完成任务。<br />
<br />
总的来说，GOAT项目通过智能地探索环境、识别和记忆物体，以及灵活地接收和执行各种寻找物体的任务，展现了一种高度自主和适应性强的导航能力。<br />
<br />
应用场景：<br />
<br />
家庭和室内环境：GOAT系统可以在家庭或其他室内环境中应用，例如帮助人们找到特定的物品，或者在复杂的室内布局中导航。<br />
<br />
拾取和放置任务：GOAT可以用于执行拾取和放置任务，比如找到一个物体并将其搬运到另一个位置。<br />
<br />
社交导航：GOAT还可以应用于社交导航场景，比如在有人的环境中导航，避开人群或跟随特定的人。<br />
<br />
辅助机器人：在辅助机器人领域，GOAT可以帮助机器人更有效地在未知环境中服务，例如在医院或老人护理设施中找到和运送物品。<br />
<br />
紧急响应和探索：在紧急响应或探索任务中，GOAT可以帮助机器人在未知或危险的环境中找到特定的物体或地点。<br />
<br />
项目及演示：<a href="https://theophilegervet.github.io/projects/goat/">theophilegervet.github.io/pr…</a><br />
论文：<a href="https://theophilegervet.github.io/projects/goat/">theophilegervet.github.io/pr…</a><br />
GitHub：<a href="https://github.com/facebookresearch/home-robot">github.com/facebookresearch/…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY4MDI1OTI0NjA5NDc0NTYvcHUvaW1nL1NyakVEMnd2V09OQ2dmYkYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726792662332948587#m</id>
            <title>谁在控制着 OpenAI ？

在这张图中，“控制”这个词出现了四次，如果你追溯下去，你会发现：

OpenAI的控制权在技术上归属于一个独立的非营利性董事会。

这个董事会不持有任何 OpenAI 的股权。

并且在很大程度上是自我任命的！

董事会成员对自己的良知负责，而非对投资者负责。

在OpenAI 的官方架构介绍中，OpenAI称：“非营利组织主要受益者是全人类，而不是 OpenAI 投资者，”

OpenAI虽然与微软建立了战略合作伙伴关系，涉及大量投资，但 OpenAI 仍然是一家完全独立的公司，由 OpenAI Nonprofit 管理。微软没有任何董事会席位，也没有控制权。也没有对 AGI 相关决策的控制权。

OpenAI 由 OpenAI Nonprofit 董事会管理，该董事会由 OpenAI Global, LLC 员工 Greg Brockman（董事长兼总裁）、Ilya Sutskever（首席科学家）和 Sam Altman（首席执行官）以及非员工董事（Adam D’Angelo、Tasha McCauley、Helen Toner）。

目前董事会里面只有Greg Brockman和三个非员工董事（Adam D’Angelo、Tasha McCauley、Helen Toner）。

相当于目前OpenAI实际上完全由三个外部人士完全把控！😂

现在可能OpenAI的员工也意识到了这种架构的荒谬，想撤掉或者改组董事会。

但是OpenAI在设计这个架构的时候，目的就是防止外部资本或者内部发生重大变故导致对AI的控制，让OpenAI违背了初心。

以下是OpenAI的非营利组织架构主要目标和使命：

OpenAI 的结构旨在支持其为人类构建安全且有益的通用人工智能 (AGI) 的使命。该组织由一个最初的非营利实体和一个新的上限利润部门组成。该结构的关键要素包括：

Nonprofit Focus: Founded in 2015, OpenAI's nonprofit entity was initially seen as the most effective way to pursue the development of AGI without profit motives, aiming for broad societal benefits.

非营利焦点：OpenAI 的非营利实体成立于 2015 年，最初被视为不以盈利为目的追求 AGI 发展的最有效方式，旨在实现广泛的社会效益。

Capped Profit Subsidiary: Recognizing the need for significant capital, OpenAI formed a for-profit subsidiary in 2019. This entity can issue equity to raise capital and hire talent, but it operates under the nonprofit's direction and mission.

利润上限子公司：认识到对大量资本的需求，OpenAI 于 2019 年成立了一家营利性子公司。该实体可以发行股权来筹集资金和雇用人才，但它在非营利组织的指导和使命下运营。

Governance and Oversight: The OpenAI nonprofit retains overall control, with its board overseeing all activities, including the for-profit subsidiary. The board ensures that the pursuit of AGI remains aligned with the organization's core principles of safety and broad benefit.

治理和监督：OpenAI 非营利组织保留总体控制权，其董事会监督所有活动，包括营利性子公司。董事会确保 AGI 的追求与组织的安全和广泛利益的核心原则保持一致。

Profit and Mission Alignment: Profits are capped to incentivize research and development that balances commercial viability with safety and sustainability. Excess profits beyond the cap are channeled back to the nonprofit for humanity's benefit.

利润和使命的一致性：利润的上限是为了激励研究和开发，以平衡商业可行性与安全性和可持续性。超出上限的超额利润将返回非营利组织以造福人类。

Independence and AGI Development: The board, primarily independent, governs the development of AGI. Notably, AGI technologies are excluded from intellectual property licenses and commercial terms with partners like Microsoft.

独立性和 AGI 开发：董事会主要是独立的，负责管理 AGI 的开发。值得注意的是，AGI 技术被排除在知识产权许可和与微软等合作伙伴的商业条款之外。

Partnership with Microsoft: OpenAI formed a strategic partnership with Microsoft, involving significant investment but maintaining OpenAI's independence. Microsoft does not have a board seat or control over AGI-related decisions.

与微软的合作：OpenAI与微软建立了战略合作伙伴关系，涉及大量投资，但保持了OpenAI的独立性。 Microsoft 没有董事会席位，也没有对 AGI 相关决策的控制权。

Board Composition: The board includes both employees of OpenAI Global, LLC (Greg Brockman, Ilya Sutskever, Sam Altman) and non-employees (Adam D’Angelo, Tasha McCauley, Helen Toner).

董事会组成：董事会包括 OpenAI Global, LLC 的员工（Greg Brockman、Ilya Sutskever、Sam Altman）和非员工（Adam D’Angelo、Tasha McCauley、Helen Toner）。

This structure reflects OpenAI's commitment to a mission-driven approach in the development of AGI, balancing the need for capital and talent with a dedication to broad societal benefits and safety​​.

这种结构反映了 OpenAI 在 AGI 开发中对任务驱动方法的承诺，平衡资本和人才的需求，并致力于广泛的社会效益和安全​​。

OpenAI架构：https://openai.com/our-structure

Who Controls OpenAI ?
https://www.bloomberg.com/opinion/articles/2023-11-20/who-controls-openai</title>
            <link>https://nitter.cz/xiaohuggg/status/1726792662332948587#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726792662332948587#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 02:40:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>谁在控制着 OpenAI ？<br />
<br />
在这张图中，“控制”这个词出现了四次，如果你追溯下去，你会发现：<br />
<br />
OpenAI的控制权在技术上归属于一个独立的非营利性董事会。<br />
<br />
这个董事会不持有任何 OpenAI 的股权。<br />
<br />
并且在很大程度上是自我任命的！<br />
<br />
董事会成员对自己的良知负责，而非对投资者负责。<br />
<br />
在OpenAI 的官方架构介绍中，OpenAI称：“非营利组织主要受益者是全人类，而不是 OpenAI 投资者，”<br />
<br />
OpenAI虽然与微软建立了战略合作伙伴关系，涉及大量投资，但 OpenAI 仍然是一家完全独立的公司，由 OpenAI Nonprofit 管理。微软没有任何董事会席位，也没有控制权。也没有对 AGI 相关决策的控制权。<br />
<br />
OpenAI 由 OpenAI Nonprofit 董事会管理，该董事会由 OpenAI Global, LLC 员工 Greg Brockman（董事长兼总裁）、Ilya Sutskever（首席科学家）和 Sam Altman（首席执行官）以及非员工董事（Adam D’Angelo、Tasha McCauley、Helen Toner）。<br />
<br />
目前董事会里面只有Greg Brockman和三个非员工董事（Adam D’Angelo、Tasha McCauley、Helen Toner）。<br />
<br />
相当于目前OpenAI实际上完全由三个外部人士完全把控！😂<br />
<br />
现在可能OpenAI的员工也意识到了这种架构的荒谬，想撤掉或者改组董事会。<br />
<br />
但是OpenAI在设计这个架构的时候，目的就是防止外部资本或者内部发生重大变故导致对AI的控制，让OpenAI违背了初心。<br />
<br />
以下是OpenAI的非营利组织架构主要目标和使命：<br />
<br />
OpenAI 的结构旨在支持其为人类构建安全且有益的通用人工智能 (AGI) 的使命。该组织由一个最初的非营利实体和一个新的上限利润部门组成。该结构的关键要素包括：<br />
<br />
Nonprofit Focus: Founded in 2015, OpenAI's nonprofit entity was initially seen as the most effective way to pursue the development of AGI without profit motives, aiming for broad societal benefits.<br />
<br />
非营利焦点：OpenAI 的非营利实体成立于 2015 年，最初被视为不以盈利为目的追求 AGI 发展的最有效方式，旨在实现广泛的社会效益。<br />
<br />
Capped Profit Subsidiary: Recognizing the need for significant capital, OpenAI formed a for-profit subsidiary in 2019. This entity can issue equity to raise capital and hire talent, but it operates under the nonprofit's direction and mission.<br />
<br />
利润上限子公司：认识到对大量资本的需求，OpenAI 于 2019 年成立了一家营利性子公司。该实体可以发行股权来筹集资金和雇用人才，但它在非营利组织的指导和使命下运营。<br />
<br />
Governance and Oversight: The OpenAI nonprofit retains overall control, with its board overseeing all activities, including the for-profit subsidiary. The board ensures that the pursuit of AGI remains aligned with the organization's core principles of safety and broad benefit.<br />
<br />
治理和监督：OpenAI 非营利组织保留总体控制权，其董事会监督所有活动，包括营利性子公司。董事会确保 AGI 的追求与组织的安全和广泛利益的核心原则保持一致。<br />
<br />
Profit and Mission Alignment: Profits are capped to incentivize research and development that balances commercial viability with safety and sustainability. Excess profits beyond the cap are channeled back to the nonprofit for humanity's benefit.<br />
<br />
利润和使命的一致性：利润的上限是为了激励研究和开发，以平衡商业可行性与安全性和可持续性。超出上限的超额利润将返回非营利组织以造福人类。<br />
<br />
Independence and AGI Development: The board, primarily independent, governs the development of AGI. Notably, AGI technologies are excluded from intellectual property licenses and commercial terms with partners like Microsoft.<br />
<br />
独立性和 AGI 开发：董事会主要是独立的，负责管理 AGI 的开发。值得注意的是，AGI 技术被排除在知识产权许可和与微软等合作伙伴的商业条款之外。<br />
<br />
Partnership with Microsoft: OpenAI formed a strategic partnership with Microsoft, involving significant investment but maintaining OpenAI's independence. Microsoft does not have a board seat or control over AGI-related decisions.<br />
<br />
与微软的合作：OpenAI与微软建立了战略合作伙伴关系，涉及大量投资，但保持了OpenAI的独立性。 Microsoft 没有董事会席位，也没有对 AGI 相关决策的控制权。<br />
<br />
Board Composition: The board includes both employees of OpenAI Global, LLC (Greg Brockman, Ilya Sutskever, Sam Altman) and non-employees (Adam D’Angelo, Tasha McCauley, Helen Toner).<br />
<br />
董事会组成：董事会包括 OpenAI Global, LLC 的员工（Greg Brockman、Ilya Sutskever、Sam Altman）和非员工（Adam D’Angelo、Tasha McCauley、Helen Toner）。<br />
<br />
This structure reflects OpenAI's commitment to a mission-driven approach in the development of AGI, balancing the need for capital and talent with a dedication to broad societal benefits and safety​​.<br />
<br />
这种结构反映了 OpenAI 在 AGI 开发中对任务驱动方法的承诺，平衡资本和人才的需求，并致力于广泛的社会效益和安全​​。<br />
<br />
OpenAI架构：<a href="https://openai.com/our-structure">openai.com/our-structure</a><br />
<br />
Who Controls OpenAI ?<br />
<a href="https://www.bloomberg.com/opinion/articles/2023-11-20/who-controls-openai">bloomberg.com/opinion/articl…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9iTXZVemFrQUFyYkZILmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726762074833826190#m</id>
            <title>Grok 即将登陆 X 

会有一个单独的入口</title>
            <link>https://nitter.cz/xiaohuggg/status/1726762074833826190#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726762074833826190#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 00:38:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Grok 即将登陆 X <br />
<br />
会有一个单独的入口</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjY3MjU3MjI3ODQ1OTU5NjgvcHUvaW1nL0o4TENZYzdSTWVZX3MwTkYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726757449775079711#m</id>
            <title>743 of 778 🖊️签署了信</title>
            <link>https://nitter.cz/xiaohuggg/status/1726757449775079711#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726757449775079711#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 00:20:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>743 of 778 🖊️签署了信</p>
<p><a href="https://nitter.cz/xiaohuggg/status/1726601113804447765#m">nitter.cz/xiaohuggg/status/1726601113804447765#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726616427262775683#m</id>
            <title>现在最尴尬的人

就是新上任的

OpenAI CEO

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1726616427262775683#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726616427262775683#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 15:00:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>现在最尴尬的人<br />
<br />
就是新上任的<br />
<br />
OpenAI CEO<br />
<br />
😂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726609866566074676#m</id>
            <title>R to @xiaohuggg: 4、Improved Image Model：提高了图像生成的保真度、一致性和分辨率，适用于文本到图像、图像到图像和图像变化等功能。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726609866566074676#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726609866566074676#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 14:34:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>4、Improved Image Model：提高了图像生成的保真度、一致性和分辨率，适用于文本到图像、图像到图像和图像变化等功能。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI2NjA5ODA3Nzc4Njg0OTI5L2ltZy9RSVBwYlhLOEoycVRhM3RWLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726609803768971443#m</id>
            <title>R to @xiaohuggg: 3、Director Mode Update：导演模式的高级摄像机控制

以实现更精细的控制水平。现在，用户可以使用小数来更精确地调整摄像机移动。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726609803768971443#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726609803768971443#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 14:33:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3、Director Mode Update：导演模式的高级摄像机控制<br />
<br />
以实现更精细的控制水平。现在，用户可以使用小数来更精确地调整摄像机移动。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI2NjA5NzQyODcxNzk3NzYwL2ltZy9abXZ0dzVQaUtPU1BBOF95LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1726609739277369728#m</id>
            <title>R to @xiaohuggg: 2、Gen-2 Style Presets：预设风格功能

允许用户使用策划好的风格生成内容，无需复杂的提示。

包括多种风格，如动漫、建筑、卡通、电影等。</title>
            <link>https://nitter.cz/xiaohuggg/status/1726609739277369728#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1726609739277369728#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 14:33:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2、Gen-2 Style Presets：预设风格功能<br />
<br />
允许用户使用策划好的风格生成内容，无需复杂的提示。<br />
<br />
包括多种风格，如动漫、建筑、卡通、电影等。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI2NjA5Njc4NDU1Njk3NDA4L2ltZy9MRmh1U3BTSnRuT2N5c0EwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>