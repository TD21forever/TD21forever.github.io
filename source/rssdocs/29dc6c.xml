<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732311380932583924#m</id>
            <title>央视龙年春晚吉祥物

好多人说

这是AI生成的

我感觉这是一年来AI受到的最大的侮辱</title>
            <link>https://nitter.cz/xiaohuggg/status/1732311380932583924#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732311380932583924#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 08:09:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>央视龙年春晚吉祥物<br />
<br />
好多人说<br />
<br />
这是AI生成的<br />
<br />
我感觉这是一年来AI受到的最大的侮辱</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FwblBObWFVQUFoU0pYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732302746886471820#m</id>
            <title>苹果发布专为 Apple 芯片设计的高效机器学习框架：MLX

MLX 的 API 设计与 NumPy 和 PyTorch 相似，让你能够在苹果电脑上方便地建立和训练机器学习模型。

使得在苹果电脑上进行机器学习相关的开发和研究变得更加简单和高效。

演示显示可在 M2 Ultra 上运行的 Llama v1 7B 模型。 

代码： https://github.com/ml-explore/mlx  

文档：https://ml-explore.github.io/mlx/build/html/index.html

MLX 示例存储库提供了一些示例，包括：

- Transformer 语言模型训练。
- 使用 LLaMA 或 Mistral 进行的大规模文本生
- 通过 LoRA 进行的参数高效微调
- 使用稳定扩散技术的图像生成
- 使用 OpenAI 的 Whisper 进行语音识别。

案例：https://github.com/ml-explore/mlx-examples

主要特点：

1、熟悉的 API：：MLX 的 API 设计与 NumPy 和 PyTorch 相似，使得用户可以方便地构建和训练复杂的机器学习模型。

2、自动微分和向量化：MLX 支持自动微分和自动向量化，这对于优化和加速机器学习模型的训练过程非常有用。

3、高效的内存管理：MLX 的统一内存模型允许在不同设备（如 CPU 和 GPU）之间高效地共享和处理数据，无需频繁地移动数据。

4、动态图构建和延迟计算：MLX 支持动态图构建和延迟计算，这使得模型的开发和调试更加灵活和高效。

MLX Data 是 Apple 机器学习研究为您带来的一个与框架无关的数据加载库。它可与 PyTorch、Jax 或 MLX 配合使用。

高效且灵活，例如能够每秒加载和处理 1000 张图像，同时还能对生成的批次运行任意 Python 转换。

代码： https://github.com/ml-explore/mlx-data

文档： https://ml-explore.github.io/mlx-data/build/html/index.html</title>
            <link>https://nitter.cz/xiaohuggg/status/1732302746886471820#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732302746886471820#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 07:35:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>苹果发布专为 Apple 芯片设计的高效机器学习框架：MLX<br />
<br />
MLX 的 API 设计与 NumPy 和 PyTorch 相似，让你能够在苹果电脑上方便地建立和训练机器学习模型。<br />
<br />
使得在苹果电脑上进行机器学习相关的开发和研究变得更加简单和高效。<br />
<br />
演示显示可在 M2 Ultra 上运行的 Llama v1 7B 模型。 <br />
<br />
代码： <a href="https://github.com/ml-explore/mlx">github.com/ml-explore/mlx</a>  <br />
<br />
文档：<a href="https://ml-explore.github.io/mlx/build/html/index.html">ml-explore.github.io/mlx/bui…</a><br />
<br />
MLX 示例存储库提供了一些示例，包括：<br />
<br />
- Transformer 语言模型训练。<br />
- 使用 LLaMA 或 Mistral 进行的大规模文本生<br />
- 通过 LoRA 进行的参数高效微调<br />
- 使用稳定扩散技术的图像生成<br />
- 使用 OpenAI 的 Whisper 进行语音识别。<br />
<br />
案例：<a href="https://github.com/ml-explore/mlx-examples">github.com/ml-explore/mlx-ex…</a><br />
<br />
主要特点：<br />
<br />
1、熟悉的 API：：MLX 的 API 设计与 NumPy 和 PyTorch 相似，使得用户可以方便地构建和训练复杂的机器学习模型。<br />
<br />
2、自动微分和向量化：MLX 支持自动微分和自动向量化，这对于优化和加速机器学习模型的训练过程非常有用。<br />
<br />
3、高效的内存管理：MLX 的统一内存模型允许在不同设备（如 CPU 和 GPU）之间高效地共享和处理数据，无需频繁地移动数据。<br />
<br />
4、动态图构建和延迟计算：MLX 支持动态图构建和延迟计算，这使得模型的开发和调试更加灵活和高效。<br />
<br />
MLX Data 是 Apple 机器学习研究为您带来的一个与框架无关的数据加载库。它可与 PyTorch、Jax 或 MLX 配合使用。<br />
<br />
高效且灵活，例如能够每秒加载和处理 1000 张图像，同时还能对生成的批次运行任意 Python 转换。<br />
<br />
代码： <a href="https://github.com/ml-explore/mlx-data">github.com/ml-explore/mlx-da…</a><br />
<br />
文档： <a href="https://ml-explore.github.io/mlx-data/build/html/index.html">ml-explore.github.io/mlx-dat…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIzMDIxODA5NjE2ODU1MDQvcHUvaW1nLzdlS0tlWTdVbDZkaUctT1ouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732278818847777042#m</id>
            <title>R to @xiaohuggg: Vid2DensePose 通过将视频转换为 DensePose 格式，可以更精确地控制和动画化视频中的人物。

Vid2DensePose与 MagicAnimate 集成，生成的 DensePose 数据可以直接用于 MagicAnimate，从而提高动画的质量和一致性。https://github.com/Flode-Labs/vid2densepose/tree/main</title>
            <link>https://nitter.cz/xiaohuggg/status/1732278818847777042#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732278818847777042#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 06:00:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vid2DensePose 通过将视频转换为 DensePose 格式，可以更精确地控制和动画化视频中的人物。<br />
<br />
Vid2DensePose与 MagicAnimate 集成，生成的 DensePose 数据可以直接用于 MagicAnimate，从而提高动画的质量和一致性。<a href="https://github.com/Flode-Labs/vid2densepose/tree/main">github.com/Flode-Labs/vid2de…</a></p>
<p><a href="https://nitter.cz/nacho_gorriti_/status/1732106540474126381#m">nitter.cz/nacho_gorriti_/status/1732106540474126381#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1732198994145591383#m</id>
            <title>RT by @xiaohuggg: Bing 官方博客发布的 Copilot 更新：

Copilot的持续AI革新

今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。

GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。

全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 http://bing.com/create 或指令 Copilot 制作图片，即刻体验这些功能。

欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</title>
            <link>https://nitter.cz/dotey/status/1732198994145591383#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1732198994145591383#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 00:43:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Bing 官方博客发布的 Copilot 更新：<br />
<br />
Copilot的持续AI革新<br />
<br />
今日，我们不仅庆祝 Microsoft Copilot 成立一周年，还推出了几项新功能。我们迫不及待想向您展示这些功能的更多细节。<br />
<br />
GPT-4 Turbo 升级 – 很快，Copilot 将能使用 OpenAI 的最新模型 GPT-4 Turbo 来生成回答，助您应对更加复杂和长篇的任务，比如编写代码等。目前，这个模型正与部分用户进行测试，并将在接下来的几周内广泛融合至 Copilot 中。<br />
<br />
全新 DALL-E 3 模型 – 现在，您可以借助升级后的 DALL-E 3 模型，通过 Copilot 创作出质量更高、更符合要求的图片。您可通过访问 <a href="http://bing.com/create">bing.com/create</a> 或指令 Copilot 制作图片，即刻体验这些功能。<br />
<br />
欣赏下图对比，感受新模型的细节水平（点击提示体验：仿真恐龙剑龙在美甲店修饰它的骨板）。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvQndwLVhBQUFLak1VLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</id>
            <title>Generative Powers of Ten：基于文本的多尺度图像生成技术

是一种图像无限缩放技术，而且质量非常高清！

它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。

可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。

该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。

这项技术的关键特点包括：

- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。

- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。

- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。

- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。

- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。

多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。

该项目基于一种联合采样算法：

联合采样算法的核心特点

并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。

迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。

优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。

它使用了以下几个关键步骤和技术：

1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。

2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。

3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。

4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。

5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。

项目及演示：https://powers-of-10.github.io/
论文：https://arxiv.org/abs/2312.02149</title>
            <link>https://nitter.cz/xiaohuggg/status/1732247057237500130#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732247057237500130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:54:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Generative Powers of Ten：基于文本的多尺度图像生成技术<br />
<br />
是一种图像无限缩放技术，而且质量非常高清！<br />
<br />
它能够根据文本描述（你想要看到的场景的文字说明）生成一系列在不同尺度上连贯一致的图像。<br />
<br />
可以展示从非常远的景象（大到整个宇宙）到非常近的细节（小到一个细胞）。<br />
<br />
该项目受到1977年原版《Powers of Ten 十次幂》电影的启发，该电影最初展示了这种连续缩放效果。研究团队的目标是使用生成模型自动创建类似的动画，并且能够从自己的照片中创建这些缩放视频。<br />
<br />
这项技术的关键特点包括：<br />
<br />
- 连续缩放视频： 通过一系列文本提示描述不同尺度的场景，该方法可以创建无缝缩放的视频。例如，可以从森林的广角景观视图缩放到树枝上一只昆虫的特写镜头。<br />
<br />
- 多尺度生成： 它能够从大范围（如整个星系）到小范围（如单个细胞）的不同尺度生成图像。<br />
<br />
- 文本驱动： 图像的生成是基于文本提示，这意味着用户可以通过文字描述来指导图像的生成过程。<br />
<br />
- 内容一致性： 在不同的放大级别之间，生成的图像在视觉和内容上保持一致性，这是传统图像放大技术难以实现的。<br />
<br />
- 实际图像的缩放： 该技术还可以引导一个缩放级别与输入图像匹配，从而实现可以对真实图像的缩放。<br />
<br />
多样性： 通过改变种子（即生成过程的随机输入），即使是对于相同的一组输入提示，也可以获得不同的结果。<br />
<br />
该项目基于一种联合采样算法：<br />
<br />
联合采样算法的核心特点<br />
<br />
并行扩散采样过程： 该算法使用一组分布在不同缩放级别的并行扩散采样过程。这意味着算法能够同时处理多个尺度的图像，从而在每个尺度上生成图像。<br />
<br />
迭代频带合并： 为了保持不同尺度图像的一致性，这些采样过程通过一个迭代频带合并过程进行协调。这个过程确保在从一个尺度到另一个尺度的过渡中，图像内容保持连贯和一致。<br />
<br />
优化所有尺度的内容： 不同于传统的通过增加图像分辨率来生成更高细节的图像（如超分辨率或图像外推技术），这种方法同时针对所有尺度的内容进行优化。这样做的好处是，它不仅在每个尺度上生成合理的图像，而且还保持了不同尺度之间内容的一致性。<br />
<br />
它使用了以下几个关键步骤和技术：<br />
<br />
1、文本提示驱动的图像生成： 用户提供一系列文本提示，描述他们想要在不同缩放级别上看到的场景。例如，从一个星系的远景到一个细胞的微观视图。<br />
<br />
2、预训练的扩散模型： 该技术使用了一个预训练的扩散模型来同时去噪不同尺度上的多个图像。通过逐步去除噪声来生成图像，从而从随机噪声中逐步构建出清晰的图像。<br />
<br />
3、多尺度联合采样： 在每个缩放级别上，噪声图像和相应的文本提示被同时输入到同一个预训练的扩散模型中，以估计相应的清晰图像。这些图像在它们共同观察的重叠区域可能会有不一致的估计。<br />
<br />
4、多分辨率融合： 为了解决不同尺度图像在重叠区域的不一致性，该技术采用了多分辨率融合方法。这种方法将这些区域融合成一个一致的缩放堆栈，并从这个一致的表示中重新渲染不同的缩放级别。<br />
<br />
5、连续缩放视频的生成： 通过这种方法，可以生成连续缩放的视频，这些视频在视觉上平滑且内容上连贯，从一个尺度平滑过渡到另一个尺度。<br />
<br />
项目及演示：<a href="https://powers-of-10.github.io/">powers-of-10.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02149">arxiv.org/abs/2312.02149</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIyNDMyNzc2NDUzMzY1NzYvcHUvaW1nL2ZwZ2pVYmN2Zkd2OV9WWFEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732235284555927967#m</id>
            <title>DeepMind：开发出可以向人类学习的人工智能

Nature发表了一篇Google DeepMind的研究成果：研究人员在3D模拟环境中使用神经网络和强化学习，展示了AI智能体如何在没有直接从人类那里获取数据的情况下，通过观察来学习和模仿人类的行为。

这项研究被视为向人工通用智能（AGI）迈进的一大步。

研究背景：

智力包括有效的知识获取，通常依赖于文化传播——个体之间的知识转移。

人类智力在很大程度上依赖于这一过程，从而能够通过社会学习吸收文化知识。这种知识被称为文化，而从一个个体向另一个个体的知识传递被称为文化传播。

文化传播是一种社会学习形式，通过与其他智能体的接触来协助学习。

技术原理：

这项技术，正是利用了这一现象，它使得AI智能体能够通过观察人类的行为来学习并模仿这些行为。这种学习方式被称为“文化传播”，它是一种社会学习形式，意味着智能体不是单独学习，而是通过与人类或其他智能体的互动来获取知识。

这种智能体能够在丰富的3D物理模拟环境中与人类共同玩耍。

该研究展示了AI智能体如何在没有先前人类数据的情况下模仿人类行为的能力。这项研究通过在3D模拟环境中使用神经网络和强化学习（RL），使AI智能体能够实时、高保真地获取和利用信息，类似于人类跨代积累和精炼知识的方式。

举例解释：

假设有一个AI智能体，我们想让它学会如何玩乒乓球。在传统的学习方法中，我们可能需要编写详细的规则和指令来教会AI如何打乒乓球。但在这项研究中，AI智能体可以通过观察真人打乒乓球的视频来学习。它会注意到人类是如何握拍、如何挥拍、如何移动身体来接球和击球的。

技术细节：

深度强化学习：这种学习方法让AI智能体通过反复尝试和错误来优化其行为。例如，AI可能一开始打球时总是失误，但随着学习的深入，它会逐渐学会如何更准确地击中球。

模仿学习：AI智能体通过观察人类的行为来学习。在乒乓球的例子中，AI会分析人类运动员的动作，然后尝试复制这些动作。

新的学习环境（GoalCycle3D）：研究人员为AI智能体提供了一个3D模拟环境，让它们可以在一个控制和安全的环境中练习和实践所学的技能。

GoalCycle3D框架为AI探索提供了一个复杂的范式，建立在先前的工作基础上，创造了一个更具沉浸感和真实性的环境。该框架通过将任务划分为不同的元素（世界、游戏和共玩者），为RL建立了多样化的环境。

详细：https://www.nature.com/articles/s41467-023-42875-2</title>
            <link>https://nitter.cz/xiaohuggg/status/1732235284555927967#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732235284555927967#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 03:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind：开发出可以向人类学习的人工智能<br />
<br />
Nature发表了一篇Google DeepMind的研究成果：研究人员在3D模拟环境中使用神经网络和强化学习，展示了AI智能体如何在没有直接从人类那里获取数据的情况下，通过观察来学习和模仿人类的行为。<br />
<br />
这项研究被视为向人工通用智能（AGI）迈进的一大步。<br />
<br />
研究背景：<br />
<br />
智力包括有效的知识获取，通常依赖于文化传播——个体之间的知识转移。<br />
<br />
人类智力在很大程度上依赖于这一过程，从而能够通过社会学习吸收文化知识。这种知识被称为文化，而从一个个体向另一个个体的知识传递被称为文化传播。<br />
<br />
文化传播是一种社会学习形式，通过与其他智能体的接触来协助学习。<br />
<br />
技术原理：<br />
<br />
这项技术，正是利用了这一现象，它使得AI智能体能够通过观察人类的行为来学习并模仿这些行为。这种学习方式被称为“文化传播”，它是一种社会学习形式，意味着智能体不是单独学习，而是通过与人类或其他智能体的互动来获取知识。<br />
<br />
这种智能体能够在丰富的3D物理模拟环境中与人类共同玩耍。<br />
<br />
该研究展示了AI智能体如何在没有先前人类数据的情况下模仿人类行为的能力。这项研究通过在3D模拟环境中使用神经网络和强化学习（RL），使AI智能体能够实时、高保真地获取和利用信息，类似于人类跨代积累和精炼知识的方式。<br />
<br />
举例解释：<br />
<br />
假设有一个AI智能体，我们想让它学会如何玩乒乓球。在传统的学习方法中，我们可能需要编写详细的规则和指令来教会AI如何打乒乓球。但在这项研究中，AI智能体可以通过观察真人打乒乓球的视频来学习。它会注意到人类是如何握拍、如何挥拍、如何移动身体来接球和击球的。<br />
<br />
技术细节：<br />
<br />
深度强化学习：这种学习方法让AI智能体通过反复尝试和错误来优化其行为。例如，AI可能一开始打球时总是失误，但随着学习的深入，它会逐渐学会如何更准确地击中球。<br />
<br />
模仿学习：AI智能体通过观察人类的行为来学习。在乒乓球的例子中，AI会分析人类运动员的动作，然后尝试复制这些动作。<br />
<br />
新的学习环境（GoalCycle3D）：研究人员为AI智能体提供了一个3D模拟环境，让它们可以在一个控制和安全的环境中练习和实践所学的技能。<br />
<br />
GoalCycle3D框架为AI探索提供了一个复杂的范式，建立在先前的工作基础上，创造了一个更具沉浸感和真实性的环境。该框架通过将任务划分为不同的元素（世界、游戏和共玩者），为RL建立了多样化的环境。<br />
<br />
详细：<a href="https://www.nature.com/articles/s41467-023-42875-2">nature.com/articles/s41467-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FvaUdyRGFnQUFabU5NLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732224902302945612#m</id>
            <title>将DALL·E 3 与草图软件@tldraw 集成

写一个主Prompt 然后使用链条来控制图像生成…😎

在链条上输入关键词或者其他辅助prompt即可生成图片

同时还能将多个链条合并组合，合并图片🫡

这样是不是可以可视化的保证图片一致性？？？</title>
            <link>https://nitter.cz/xiaohuggg/status/1732224902302945612#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732224902302945612#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 02:26:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将DALL·E 3 与草图软件<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 集成<br />
<br />
写一个主Prompt 然后使用链条来控制图像生成…😎<br />
<br />
在链条上输入关键词或者其他辅助prompt即可生成图片<br />
<br />
同时还能将多个链条合并组合，合并图片🫡<br />
<br />
这样是不是可以可视化的保证图片一致性？？？</p>
<p><a href="https://nitter.cz/miiura/status/1732040947477987555#m">nitter.cz/miiura/status/1732040947477987555#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732208782330179981#m</id>
            <title>每一次技术革命都是生产资料再分配的过程，但最终还是会集中在少数人手中！

很少惠及普罗大众！

要想惠及普罗大众就要改变社会统治结构！

那么就是要由AI来统治人类！所以AI最后能否上位？😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1732208782330179981#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732208782330179981#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 01:22:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>每一次技术革命都是生产资料再分配的过程，但最终还是会集中在少数人手中！<br />
<br />
很少惠及普罗大众！<br />
<br />
要想惠及普罗大众就要改变社会统治结构！<br />
<br />
那么就是要由AI来统治人类！所以AI最后能否上位？😎</p>
<p><a href="https://nitter.cz/dotey/status/1732076418903789628#m">nitter.cz/dotey/status/1732076418903789628#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732050840515719654#m</id>
            <title>什么是量子计算？

量子计算机的工作原理是什么？

量子计算机和传统计算机有什么不同？

看完这个视频你就懂了，这个是给小朋友科普的，我想你也应该能看懂！😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1732050840515719654#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732050840515719654#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 14:54:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>什么是量子计算？<br />
<br />
量子计算机的工作原理是什么？<br />
<br />
量子计算机和传统计算机有什么不同？<br />
<br />
看完这个视频你就懂了，这个是给小朋友科普的，我想你也应该能看懂！😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMyMDUwNTQyNzc4NzkzOTg0L2ltZy9zWGFQbjE1OWhTd2gwX1EtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732032732644196698#m</id>
            <title>R to @xiaohuggg: 和微软的这个一样

https://x.com/xiaohuggg/status/1730547607716643080</title>
            <link>https://nitter.cz/xiaohuggg/status/1732032732644196698#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732032732644196698#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 13:42:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>和微软的这个一样<br />
<br />
<a href="https://x.com/xiaohuggg/status/1730547607716643080">x.com/xiaohuggg/status/17305…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730547607716643080#m">nitter.cz/xiaohuggg/status/1730547607716643080#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732026172509421697#m</id>
            <title>VividTalk：单张照片+一段音频即可让照片说话

你只需要提供一张人物的静态照片和一段语音录音，VividTalk就能将它们结合起来，制作出一个看起来像是实际说话的人物的视频。

而且面部表情和头部动作都很自然，口型可以同步、支持多种语言，不同风格，如真实风格、卡通风格等。

该项目由由南京大学、阿里巴巴、字节跳动和南开大学共同开发。

和前几天我发的微软的这个项目几乎一样：https://twitter.com/xiaohuggg/status/1730547607716643080

VividTalk通过先进的音频到3D网格映射技术和网格到视频的转换技术，实现了高质量、逼真的音频驱动的说话头像视频生成。

其工作原理的详细说明：

1、音频到网格的映射（第一阶段）：

在这一阶段，VividTalk首先将输入的音频映射到3D网格上。这涉及学习两种类型的运动：非刚性表情运动和刚性头部运动。

对于表情运动，技术使用混合形状（blendshape）和顶点作为中间表示，以最大化模型的表示能力。混合形状提供了全局的粗略运动，而顶点偏移则描述了更细致的嘴唇运动。

对于自然的头部运动，VividTalk提出了一个新颖的可学习的头部姿势代码本，采用了两阶段训练机制。

2、网格到视频的转换（第二阶段）：

在第二阶段，VividTalk使用双分支运动-VAE（变分自编码器）和生成器将学习到的网格转换为密集的运动，并基于这些运动逐帧合成高质量的视频。

这一过程涉及将3D网格的运动转换为2D密集运动，然后输入到生成器中，以合成最终的视频帧。

3、高视觉质量和真实感：

VividTalk生成的视频具有高视觉质量，包括逼真的面部表情、多样的头部姿势，并且在嘴唇同步方面有显著提升。

通过这种方法，VividTalk能够生成与输入音频高度同步的逼真说话头像视频，提高了视频的真实感和动态性。

项目及演示：https://humanaigc.github.io/vivid-talk/
论文：https://arxiv.org/pdf/2312.01841.pdf
GitHub：https://github.com/HumanAIGC/VividTalk</title>
            <link>https://nitter.cz/xiaohuggg/status/1732026172509421697#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732026172509421697#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 13:16:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VividTalk：单张照片+一段音频即可让照片说话<br />
<br />
你只需要提供一张人物的静态照片和一段语音录音，VividTalk就能将它们结合起来，制作出一个看起来像是实际说话的人物的视频。<br />
<br />
而且面部表情和头部动作都很自然，口型可以同步、支持多种语言，不同风格，如真实风格、卡通风格等。<br />
<br />
该项目由由南京大学、阿里巴巴、字节跳动和南开大学共同开发。<br />
<br />
和前几天我发的微软的这个项目几乎一样：<a href="https://nitter.cz/xiaohuggg/status/1730547607716643080">nitter.cz/xiaohuggg/status…</a><br />
<br />
VividTalk通过先进的音频到3D网格映射技术和网格到视频的转换技术，实现了高质量、逼真的音频驱动的说话头像视频生成。<br />
<br />
其工作原理的详细说明：<br />
<br />
1、音频到网格的映射（第一阶段）：<br />
<br />
在这一阶段，VividTalk首先将输入的音频映射到3D网格上。这涉及学习两种类型的运动：非刚性表情运动和刚性头部运动。<br />
<br />
对于表情运动，技术使用混合形状（blendshape）和顶点作为中间表示，以最大化模型的表示能力。混合形状提供了全局的粗略运动，而顶点偏移则描述了更细致的嘴唇运动。<br />
<br />
对于自然的头部运动，VividTalk提出了一个新颖的可学习的头部姿势代码本，采用了两阶段训练机制。<br />
<br />
2、网格到视频的转换（第二阶段）：<br />
<br />
在第二阶段，VividTalk使用双分支运动-VAE（变分自编码器）和生成器将学习到的网格转换为密集的运动，并基于这些运动逐帧合成高质量的视频。<br />
<br />
这一过程涉及将3D网格的运动转换为2D密集运动，然后输入到生成器中，以合成最终的视频帧。<br />
<br />
3、高视觉质量和真实感：<br />
<br />
VividTalk生成的视频具有高视觉质量，包括逼真的面部表情、多样的头部姿势，并且在嘴唇同步方面有显著提升。<br />
<br />
通过这种方法，VividTalk能够生成与输入音频高度同步的逼真说话头像视频，提高了视频的真实感和动态性。<br />
<br />
项目及演示：<a href="https://humanaigc.github.io/vivid-talk/">humanaigc.github.io/vivid-ta…</a><br />
论文：<a href="https://arxiv.org/pdf/2312.01841.pdf">arxiv.org/pdf/2312.01841.pdf</a><br />
GitHub：<a href="https://github.com/HumanAIGC/VividTalk">github.com/HumanAIGC/VividTa…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIwMTk4ODU4MzQ5MzYzMjAvcHUvaW1nL1NYalFmdkNJNVRTVTlBdTcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1732014004950974917#m</id>
            <title>HeyGen推出Avatar2.0 ：Instant Avatar 即时虚拟分身

- 只需要5分钟，使用手机即可创造一个自己的虚拟分身。

- 多语言支持：通过内置的翻译工具，支持创建多语言内容。

- 口型同步：支持口型同步和多语言声音匹配

- 免费使用：而且这项服务是免费的...

用户可以利用这项技术制作可扩展的、定制化的视频内容，同步到全球所有的平台。

详细：https://www.heygen.com/article/introducing-avatar-2-0-instant-avatar</title>
            <link>https://nitter.cz/xiaohuggg/status/1732014004950974917#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1732014004950974917#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 12:28:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>HeyGen推出Avatar2.0 ：Instant Avatar 即时虚拟分身<br />
<br />
- 只需要5分钟，使用手机即可创造一个自己的虚拟分身。<br />
<br />
- 多语言支持：通过内置的翻译工具，支持创建多语言内容。<br />
<br />
- 口型同步：支持口型同步和多语言声音匹配<br />
<br />
- 免费使用：而且这项服务是免费的...<br />
<br />
用户可以利用这项技术制作可扩展的、定制化的视频内容，同步到全球所有的平台。<br />
<br />
详细：<a href="https://www.heygen.com/article/introducing-avatar-2-0-instant-avatar">heygen.com/article/introduci…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzIwMTMwOTk2NDgxODQzMjAvcHUvaW1nL2hqa3lfVDMxcFptRGNCX3YuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731970985014780212#m</id>
            <title>R to @xiaohuggg: 这应用不就来了

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1731970985014780212#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731970985014780212#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 09:37:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这应用不就来了<br />
<br />
😂</p>
<p><a href="https://nitter.cz/bdsqlsz/status/1731863081578336704#m">nitter.cz/bdsqlsz/status/1731863081578336704#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731950689482428603#m</id>
            <title>R to @xiaohuggg: MetaHuman渲染 和 AI渲染对比</title>
            <link>https://nitter.cz/xiaohuggg/status/1731950689482428603#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731950689482428603#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 08:16:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MetaHuman渲染 和 AI渲染对比</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE5NTAzNzczNTkzMzEzMjgvcHUvaW1nL2hwTjNEekFQVjk1MXNqd0MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731950309042229688#m</id>
            <title>使用ComfyUI  + SD + AnimateDiff

都能做出这种效果了？？？

😐

作者@DreamStarter_1 将很快公布制作方法 …</title>
            <link>https://nitter.cz/xiaohuggg/status/1731950309042229688#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731950309042229688#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 08:15:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用ComfyUI  + SD + AnimateDiff<br />
<br />
都能做出这种效果了？？？<br />
<br />
😐<br />
<br />
作者<a href="https://nitter.cz/DreamStarter_1" title="DreamStarter">@DreamStarter_1</a> 将很快公布制作方法 …</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FmNGdQSlhFQUFmOG9CLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731943630892577188#m</id>
            <title>R to @xiaohuggg: IBM 研究主管达里奥·吉尔 (Dario Gil) 表示：“这台机器不同于我们曾经制造过的任何机器。” 

@60Minutes 节目报道了 IBM 最新的量子计算机，他们采访了整个建造过程，并解释了什么是量子计算机以及它的意义。

这是迄今为止最先进的量子计算机。

详细内容：https://www.cbsnews.com/news/quantum-computing-google-ibm-advances-60-minutes-transcript/?ftag=CNM-00-10aab7d&amp;linkId=252720847</title>
            <link>https://nitter.cz/xiaohuggg/status/1731943630892577188#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731943630892577188#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 07:48:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>IBM 研究主管达里奥·吉尔 (Dario Gil) 表示：“这台机器不同于我们曾经制造过的任何机器。” <br />
<br />
<a href="https://nitter.cz/60Minutes" title="60 Minutes">@60Minutes</a> 节目报道了 IBM 最新的量子计算机，他们采访了整个建造过程，并解释了什么是量子计算机以及它的意义。<br />
<br />
这是迄今为止最先进的量子计算机。<br />
<br />
详细内容：<a href="https://www.cbsnews.com/news/quantum-computing-google-ibm-advances-60-minutes-transcript/?ftag=CNM-00-10aab7d&amp;linkId=252720847">cbsnews.com/news/quantum-com…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE5NDMxMTAzNzUzMzc5ODQvcHUvaW1nL1dNWDg1RDVWY29oc1JteVMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731943053928341690#m</id>
            <title>IBM在其年度量子峰会上宣布了一系列重大量子计算进展。

IBM发布了全球首个模块化规模实用化的量子计算机IBM Quantum System 2。

以及下一代量子处理器IBM Condor和Heron，其中Condor拥有1121个超导量子位。Heron拥有133个固定频率的量子位。

量子位（Qubits）：量子计算机使用量子位（或称为qubits）来存储信息。每增加一个量子位，计算机的能力就会加倍，这是指数级增长。

主要内容：

1、2、IBM Condor量子处理器：IBM Condor是一款拥有1,121个超导量子位的新型量子处理器，基于IBM的交叉共振门技术。Condor在芯片设计的规模和产量上推动了极限，量子位密度提高了50%，在量子位制造和层压板尺寸方面取得了进步，并在单个稀释制冷器内包含了超过一英里的高密度低温柔性IO线路。

IBM Quantum Heron处理器：这是IBM迄今为止错误率最低的量子计算芯片。IBM Quantum Heron处理器是IBM首款拥有133个固定频率量子位的处理器，具有可调节的耦合器，其设备性能比之前的处理器提高了3-5倍，几乎消除了串扰。这项技术有望成为未来硬件开发的基础。

3、IBM Quantum System Two：IBM Quantum System Two是可扩展量子计算的基石，结合了低温基础设施、第三代控制电子设备和经典运行时服务器。这个系统将用于实现量子中心超级计算的并行电路执行。

4、Qiskit 1.0：IBM宣布了Qiskit 1.0，这是世界上最广泛使用的开源量子编程软件。它具有新功能，帮助计算科学家更容易和更快地执行量子电路。

5、生成式AI模型：IBM展示了工程化的生成式AI模型，用于自动化量子代码开发，并优化量子电路。

6、量子发展路线图至2033年：IBM发布了延伸至2033年的量子发展路线图，设定了新的目标，以显著提高门操作的质量。这样做将增加能够运行的量子电路的大小，并有助于实现大规模量子计算的全部潜力。

详细：https://newsroom.ibm.com/2023-12-04-IBM-Debuts-Next-Generation-Quantum-Processor-IBM-Quantum-System-Two,-Extends-Roadmap-to-Advance-Era-of-Quantum-Utility</title>
            <link>https://nitter.cz/xiaohuggg/status/1731943053928341690#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731943053928341690#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 07:46:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>IBM在其年度量子峰会上宣布了一系列重大量子计算进展。<br />
<br />
IBM发布了全球首个模块化规模实用化的量子计算机IBM Quantum System 2。<br />
<br />
以及下一代量子处理器IBM Condor和Heron，其中Condor拥有1121个超导量子位。Heron拥有133个固定频率的量子位。<br />
<br />
量子位（Qubits）：量子计算机使用量子位（或称为qubits）来存储信息。每增加一个量子位，计算机的能力就会加倍，这是指数级增长。<br />
<br />
主要内容：<br />
<br />
1、2、IBM Condor量子处理器：IBM Condor是一款拥有1,121个超导量子位的新型量子处理器，基于IBM的交叉共振门技术。Condor在芯片设计的规模和产量上推动了极限，量子位密度提高了50%，在量子位制造和层压板尺寸方面取得了进步，并在单个稀释制冷器内包含了超过一英里的高密度低温柔性IO线路。<br />
<br />
IBM Quantum Heron处理器：这是IBM迄今为止错误率最低的量子计算芯片。IBM Quantum Heron处理器是IBM首款拥有133个固定频率量子位的处理器，具有可调节的耦合器，其设备性能比之前的处理器提高了3-5倍，几乎消除了串扰。这项技术有望成为未来硬件开发的基础。<br />
<br />
3、IBM Quantum System Two：IBM Quantum System Two是可扩展量子计算的基石，结合了低温基础设施、第三代控制电子设备和经典运行时服务器。这个系统将用于实现量子中心超级计算的并行电路执行。<br />
<br />
4、Qiskit 1.0：IBM宣布了Qiskit 1.0，这是世界上最广泛使用的开源量子编程软件。它具有新功能，帮助计算科学家更容易和更快地执行量子电路。<br />
<br />
5、生成式AI模型：IBM展示了工程化的生成式AI模型，用于自动化量子代码开发，并优化量子电路。<br />
<br />
6、量子发展路线图至2033年：IBM发布了延伸至2033年的量子发展路线图，设定了新的目标，以显著提高门操作的质量。这样做将增加能够运行的量子电路的大小，并有助于实现大规模量子计算的全部潜力。<br />
<br />
详细：<a href="https://newsroom.ibm.com/2023-12-04-IBM-Debuts-Next-Generation-Quantum-Processor-IBM-Quantum-System-Two,-Extends-Roadmap-to-Advance-Era-of-Quantum-Utility">newsroom.ibm.com/2023-12-04-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE5MzcxNzY4MTkyNDkxNTIvcHUvaW1nL2hqMU53WU56ZVRENk9ZbXUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731904722007970150#m</id>
            <title>Pika 对视频特定区域修改功能演示

'Modify Region' 🌟</title>
            <link>https://nitter.cz/xiaohuggg/status/1731904722007970150#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731904722007970150#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 05:13:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pika 对视频特定区域修改功能演示<br />
<br />
'Modify Region' 🌟</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzE4ODU3MDM5NTE1MDMzNjAvcHUvaW1nLzFuUGRnNzNOZWVYMjlYMXMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1731901971534168427#m</id>
            <title>R to @xiaohuggg: DeepMind说他们在发现漏洞后，于8月30日向OpenAI披露了这一漏洞...

但是直到今天OpenAI 才修复了这个漏洞

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1731901971534168427#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1731901971534168427#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 05 Dec 2023 05:03:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepMind说他们在发现漏洞后，于8月30日向OpenAI披露了这一漏洞...<br />
<br />
但是直到今天OpenAI 才修复了这个漏洞<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Fqek9TMWJBQUFlMTQyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>