<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728597997804720431#m</id>
            <title>大脑是如何在有限的资源下有效地处理信息和学习新事物的？

剑桥大学科学家们使用了一种特殊的人工智能模型，称为空间嵌入循环神经网络（seRNNs）。

当他们对这个人工智能模型施加资源限制时，它开始展现出一些类似于人类大脑的特征。

它会发展出类似于大脑中那些高效处理信息的网络结构。

因为大脑本身不仅擅长解决复杂的问题，而且只消耗很少的能量。

举例解释：

让我们用一个比喻来理解这个研究。想象你正在组织一个大型派对，你需要安排食物、饮料、音乐和活动。但是，你的预算有限，你不能买所有你想要的东西。所以，你需要在不同的选项之间做出选择，确保你的派对既有趣又在预算之内。

这就像大脑在发展过程中所做的事情。它需要决定如何最好地使用其有限的资源（比如能量和空间）来处理信息和学习新事物。

在这项研究中，seRNNs被设计成在学习任务的同时，也要考虑如何有效地使用其“资源”。这些资源可以是连接不同神经元的“线路”的数量和长度。在真实的大脑中，这些线路需要物理空间，并且消耗能量。因此，大脑必须找到一种方式，在有限的资源下，尽可能有效地工作。

通过这种方式，研究者们发现，当他们让这个人工智能模型在有限资源的约束下工作时，它开始展现出一些类似于真实大脑的特征。例如，它会发展出类似于大脑中那些高效处理信息的网络结构。

这项研究帮助我人们更好地理解大脑是如何工作的，同时也为设计更高效、更接近人类大脑工作方式的人工智能系统提供了灵感。

研究的具体细节包括：

1、系统设计：研究团队设计了一个人工系统，旨在模拟大脑的一个非常简化的版本。这个系统使用计算节点来代替真实的神经元。每个节点都有一个特定的位置在虚拟空间中，节点间的通信难度取决于它们之间的物理距离。

2、物理约束：研究中的关键是对系统施加了物理约束。在人类大脑中，跨越大距离的神经连接是昂贵的，需要更多的能量和资源来维持。研究团队在其人工系统中模拟了这种约束，使得节点间的连接难度与它们的物理距离成正比。

3、任务执行：系统被赋予了一个简单的任务，即完成一个迷宫导航的简化版本。这个任务要求系统综合多个信息片段来决定到达终点的最短路径。

4、学习和适应：最初，系统不知道如何完成任务，会犯错误。但随着反馈的给予，系统逐渐学会了如何更好地执行任务。它通过改变节点间的连接强度来学习，类似于人类大脑中神经细胞之间连接强度的变化。

5、系统的进化：在物理约束的影响下，系统开始采用与人类大脑相似的策略来解决任务。例如，系统发展出了枢纽节点，这些节点高度连接，用于在网络中传递信息。此外，节点的响应特征也开始变得更加灵活，能够在不同时间点编码迷宫的多种属性。

这项研究的目的和价值意义：

1、模拟大脑的工作方式：通过创建空间嵌入的循环神经网络（seRNNs），研究旨在模拟人类大脑的工作方式。这种模拟有助于我们更深入地理解大脑是如何在有限的资源下有效地处理信息和学习新事物的。

2、探索大脑的结构和功能组织：研究探讨了塑造大脑结构和功能组织的约束，这对于神经科学领域是一个基本且重要的问题。通过理解这些约束，我们可以更好地了解大脑的运作原理。

3、人工智能的发展：这项研究不仅对神经科学领域有重要意义，也对人工智能的发展具有重要价值。通过模仿大脑的工作方式，可以为设计更高效、更智能的AI系统提供灵感和指导。

4、医学和心理健康应用：对大脑如何在资源限制下工作的深入理解可能有助于医学和心理健康领域。例如，它可能帮助我们更好地理解某些神经退行性疾病或心理健康问题的根本原因。

5、跨学科合作的示范：这项研究展示了神经科学和人工智能领域之间的成功合作，强调了跨学科研究在解决复杂科学问题中的重要性。

总之，这项研究通过模拟大脑的工作方式，不仅增进了我们对大脑结构和功能的理解，也为人工智能的发展提供了新的视角和方法，同时在医学和心理健康领域也可能产生重要影响。

详细介绍：https://www.cam.ac.uk/research/news/ai-system-self-organises-to-develop-features-of-brains-of-complex-organisms
Nature论文：https://www.nature.com/articles/s42256-023-00748-9</title>
            <link>https://nitter.cz/xiaohuggg/status/1728597997804720431#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728597997804720431#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 26 Nov 2023 02:14:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大脑是如何在有限的资源下有效地处理信息和学习新事物的？<br />
<br />
剑桥大学科学家们使用了一种特殊的人工智能模型，称为空间嵌入循环神经网络（seRNNs）。<br />
<br />
当他们对这个人工智能模型施加资源限制时，它开始展现出一些类似于人类大脑的特征。<br />
<br />
它会发展出类似于大脑中那些高效处理信息的网络结构。<br />
<br />
因为大脑本身不仅擅长解决复杂的问题，而且只消耗很少的能量。<br />
<br />
举例解释：<br />
<br />
让我们用一个比喻来理解这个研究。想象你正在组织一个大型派对，你需要安排食物、饮料、音乐和活动。但是，你的预算有限，你不能买所有你想要的东西。所以，你需要在不同的选项之间做出选择，确保你的派对既有趣又在预算之内。<br />
<br />
这就像大脑在发展过程中所做的事情。它需要决定如何最好地使用其有限的资源（比如能量和空间）来处理信息和学习新事物。<br />
<br />
在这项研究中，seRNNs被设计成在学习任务的同时，也要考虑如何有效地使用其“资源”。这些资源可以是连接不同神经元的“线路”的数量和长度。在真实的大脑中，这些线路需要物理空间，并且消耗能量。因此，大脑必须找到一种方式，在有限的资源下，尽可能有效地工作。<br />
<br />
通过这种方式，研究者们发现，当他们让这个人工智能模型在有限资源的约束下工作时，它开始展现出一些类似于真实大脑的特征。例如，它会发展出类似于大脑中那些高效处理信息的网络结构。<br />
<br />
这项研究帮助我人们更好地理解大脑是如何工作的，同时也为设计更高效、更接近人类大脑工作方式的人工智能系统提供了灵感。<br />
<br />
研究的具体细节包括：<br />
<br />
1、系统设计：研究团队设计了一个人工系统，旨在模拟大脑的一个非常简化的版本。这个系统使用计算节点来代替真实的神经元。每个节点都有一个特定的位置在虚拟空间中，节点间的通信难度取决于它们之间的物理距离。<br />
<br />
2、物理约束：研究中的关键是对系统施加了物理约束。在人类大脑中，跨越大距离的神经连接是昂贵的，需要更多的能量和资源来维持。研究团队在其人工系统中模拟了这种约束，使得节点间的连接难度与它们的物理距离成正比。<br />
<br />
3、任务执行：系统被赋予了一个简单的任务，即完成一个迷宫导航的简化版本。这个任务要求系统综合多个信息片段来决定到达终点的最短路径。<br />
<br />
4、学习和适应：最初，系统不知道如何完成任务，会犯错误。但随着反馈的给予，系统逐渐学会了如何更好地执行任务。它通过改变节点间的连接强度来学习，类似于人类大脑中神经细胞之间连接强度的变化。<br />
<br />
5、系统的进化：在物理约束的影响下，系统开始采用与人类大脑相似的策略来解决任务。例如，系统发展出了枢纽节点，这些节点高度连接，用于在网络中传递信息。此外，节点的响应特征也开始变得更加灵活，能够在不同时间点编码迷宫的多种属性。<br />
<br />
这项研究的目的和价值意义：<br />
<br />
1、模拟大脑的工作方式：通过创建空间嵌入的循环神经网络（seRNNs），研究旨在模拟人类大脑的工作方式。这种模拟有助于我们更深入地理解大脑是如何在有限的资源下有效地处理信息和学习新事物的。<br />
<br />
2、探索大脑的结构和功能组织：研究探讨了塑造大脑结构和功能组织的约束，这对于神经科学领域是一个基本且重要的问题。通过理解这些约束，我们可以更好地了解大脑的运作原理。<br />
<br />
3、人工智能的发展：这项研究不仅对神经科学领域有重要意义，也对人工智能的发展具有重要价值。通过模仿大脑的工作方式，可以为设计更高效、更智能的AI系统提供灵感和指导。<br />
<br />
4、医学和心理健康应用：对大脑如何在资源限制下工作的深入理解可能有助于医学和心理健康领域。例如，它可能帮助我们更好地理解某些神经退行性疾病或心理健康问题的根本原因。<br />
<br />
5、跨学科合作的示范：这项研究展示了神经科学和人工智能领域之间的成功合作，强调了跨学科研究在解决复杂科学问题中的重要性。<br />
<br />
总之，这项研究通过模拟大脑的工作方式，不仅增进了我们对大脑结构和功能的理解，也为人工智能的发展提供了新的视角和方法，同时在医学和心理健康领域也可能产生重要影响。<br />
<br />
详细介绍：<a href="https://www.cam.ac.uk/research/news/ai-system-self-organises-to-develop-features-of-brains-of-complex-organisms">cam.ac.uk/research/news/ai-s…</a><br />
Nature论文：<a href="https://www.nature.com/articles/s42256-023-00748-9">nature.com/articles/s42256-0…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl8wMnR0OWE0QUFZWC1vLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728433835954811292#m</id>
            <title>R to @xiaohuggg: 使用 #mindjourney 生图

然后用Runway的运动笔刷

制作的完整视频</title>
            <link>https://nitter.cz/xiaohuggg/status/1728433835954811292#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728433835954811292#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 15:21:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>使用 <a href="https://nitter.cz/search?q=%23mindjourney">#mindjourney</a> 生图<br />
<br />
然后用Runway的运动笔刷<br />
<br />
制作的完整视频</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc2NzIyMjA2Njk5MzE1MjAvcHUvaW1nLzdOemhVZ085M0c0WHhjOUcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728402446232514773#m</id>
            <title>R to @xiaohuggg: 另一个演示...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728402446232514773#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728402446232514773#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 13:17:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>另一个演示...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjg0MDE4NzkzNTU1NTE3NDQvcHUvaW1nL09DM1lBMG4wb01tQlY5VUwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728402442029834610#m</id>
            <title>draw-fast：超快速草图到实物图渲染工具

你可以即时看到你的草图绘画被转换成真实的实物图像。

GitHub：https://github.com/tldraw/draw-fast

是由@tldraw 的tldraw-fal 项目：https://github.com/fal-ai/tldraw-fal分叉而来的...

项目利用>@fal_ai_data 的 LCM 模型的示例存储库和服务来实现图像的实时生成。</title>
            <link>https://nitter.cz/xiaohuggg/status/1728402442029834610#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728402442029834610#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 13:17:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>draw-fast：超快速草图到实物图渲染工具<br />
<br />
你可以即时看到你的草图绘画被转换成真实的实物图像。<br />
<br />
GitHub：<a href="https://github.com/tldraw/draw-fast">github.com/tldraw/draw-fast</a><br />
<br />
是由<a href="https://nitter.cz/tldraw" title="tldraw">@tldraw</a> 的tldraw-fal 项目：<a href="https://github.com/fal-ai/tldraw-fal">github.com/fal-ai/tldraw-fal</a>分叉而来的...<br />
<br />
项目利用<a href="https://nitter.cz/fal_ai_data" title="fal (Features &amp; Labels)">@fal_ai_data</a> 的 LCM 模型的示例存储库和服务来实现图像的实时生成。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgzOTY0NjMzNDcyNzM3MjgvcHUvaW1nL3J6SlpCdkRXd2o1ZlJvcDguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728371126357905774#m</id>
            <title>#周末影院《芙蓉镇》 

看完你就会懂

什么是

一种深深的无力感…</title>
            <link>https://nitter.cz/xiaohuggg/status/1728371126357905774#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728371126357905774#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 11:12:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://nitter.cz/search?q=%23周末影院">#周末影院</a>《芙蓉镇》 <br />
<br />
看完你就会懂<br />
<br />
什么是<br />
<br />
一种深深的无力感…</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI4MzcwMDQyMjc3MTY3MTA0L2ltZy82S2pkTFk0UG1wcm1NdnhLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728360764757708852#m</id>
            <title>Runway 运动笔刷 Motion Brush 

效果展示👍</title>
            <link>https://nitter.cz/xiaohuggg/status/1728360764757708852#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728360764757708852#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 10:31:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 运动笔刷 Motion Brush <br />
<br />
效果展示👍</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgxMjQ4OTgzMTUyNzYyODgvcHUvaW1nLy0tU3RYWlNqLVE2VTVob2EuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728339407646945593#m</id>
            <title>商业内幕文章：每个人都在谈论 OpenAI 的 Q*。以下是您需要了解的有关这个神秘项目的信息。

文章介绍了Q*的一些信息和对人工智能专家的采访，称该模型可能是AI向前迈出的一大步，但不太可能很快导致世界末日。

文章揭示了Q*与传统大语言模型之间的主要区别：

1、技术结合：Q*似乎是一种结合了Q学习和A搜索的模型。这种结合可能使Q*在处理特定类型的问题时，比传统的语言模型更加高效和精确。Q学习是一种强化学习方法，而A搜索是一种寻找最佳路径的算法。这种结合可能使Q*在决策和路径规划方面表现出色。

2、逻辑推理能力：解决基本数学问题的能力听起来可能并不那么令人印象深刻，但人工智能专家Charles Higgins告诉《商业内幕》，这将代表着现有模型的巨大飞跃，现有模型很难在所训练的数据之外进行泛化。

Charles Higgins称：“数学是关于符号推理的——例如，‘如果 X 大于 Y，Y 大于 Z，那么 X 大于 Z。’”传统上，语言模型在这方面确实很困难，因为它们不进行逻辑推理，它们只是拥有有效的直觉。”

传统的语言模型，如GPT系列，主要依赖于大量的文本数据进行训练，以生成连贯和相关的文本。这些模型在模式识别和直觉方面表现出色，但在逻辑推理和处理抽象概念方面可能有限。Q*可能在逻辑推理和抽象思维方面有所突破，这将是AI领域的一个重要进步。

3、处理幻觉问题：传统的语言模型有时会产生所谓的“幻觉”，即生成与事实不符或逻辑不连贯的内容。Q*可能会将基于经验的知识与事实推理相结合，这被认为是接近我们所认为的智能的一步，并可能使模型能够产生新的想法，这是ChatGPT目前无法做到的。

4、接近人工通用智能（AGI）：Q*可能是向人工通用智能迈出的一步。与专注于特定任务的传统语言模型不同，Q*可能能够执行更广泛的智能任务，显示出更高的适应性和智能水平。

内部担忧和伦理问题：

据报道，Q*在OpenAI内部引发了一些担忧，这可能与其潜在的能力和影响有关。这种担忧可能超出了传统语言模型所引起的范围，反映了对AI技术快速发展的普遍关注。

总之：Q*可能在技术结合、逻辑推理、处理复杂问题以及接近人工通用智能方面与传统语言模型有显著区别。

然而，由于缺乏详细的公开信息，这些区别仍然是基于目前可用信息的推测。随着时间的推移和更多信息的公开，我们对Q的理解可能会进一步深化。

详细：https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11</title>
            <link>https://nitter.cz/xiaohuggg/status/1728339407646945593#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728339407646945593#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 09:06:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>商业内幕文章：每个人都在谈论 OpenAI 的 Q*。以下是您需要了解的有关这个神秘项目的信息。<br />
<br />
文章介绍了Q*的一些信息和对人工智能专家的采访，称该模型可能是AI向前迈出的一大步，但不太可能很快导致世界末日。<br />
<br />
文章揭示了Q*与传统大语言模型之间的主要区别：<br />
<br />
1、技术结合：Q*似乎是一种结合了Q学习和A搜索的模型。这种结合可能使Q*在处理特定类型的问题时，比传统的语言模型更加高效和精确。Q学习是一种强化学习方法，而A搜索是一种寻找最佳路径的算法。这种结合可能使Q*在决策和路径规划方面表现出色。<br />
<br />
2、逻辑推理能力：解决基本数学问题的能力听起来可能并不那么令人印象深刻，但人工智能专家Charles Higgins告诉《商业内幕》，这将代表着现有模型的巨大飞跃，现有模型很难在所训练的数据之外进行泛化。<br />
<br />
Charles Higgins称：“数学是关于符号推理的——例如，‘如果 X 大于 Y，Y 大于 Z，那么 X 大于 Z。’”传统上，语言模型在这方面确实很困难，因为它们不进行逻辑推理，它们只是拥有有效的直觉。”<br />
<br />
传统的语言模型，如GPT系列，主要依赖于大量的文本数据进行训练，以生成连贯和相关的文本。这些模型在模式识别和直觉方面表现出色，但在逻辑推理和处理抽象概念方面可能有限。Q*可能在逻辑推理和抽象思维方面有所突破，这将是AI领域的一个重要进步。<br />
<br />
3、处理幻觉问题：传统的语言模型有时会产生所谓的“幻觉”，即生成与事实不符或逻辑不连贯的内容。Q*可能会将基于经验的知识与事实推理相结合，这被认为是接近我们所认为的智能的一步，并可能使模型能够产生新的想法，这是ChatGPT目前无法做到的。<br />
<br />
4、接近人工通用智能（AGI）：Q*可能是向人工通用智能迈出的一步。与专注于特定任务的传统语言模型不同，Q*可能能够执行更广泛的智能任务，显示出更高的适应性和智能水平。<br />
<br />
内部担忧和伦理问题：<br />
<br />
据报道，Q*在OpenAI内部引发了一些担忧，这可能与其潜在的能力和影响有关。这种担忧可能超出了传统语言模型所引起的范围，反映了对AI技术快速发展的普遍关注。<br />
<br />
总之：Q*可能在技术结合、逻辑推理、处理复杂问题以及接近人工通用智能方面与传统语言模型有显著区别。<br />
<br />
然而，由于缺乏详细的公开信息，这些区别仍然是基于目前可用信息的推测。随着时间的推移和更多信息的公开，我们对Q的理解可能会进一步深化。<br />
<br />
详细：<a href="https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11">businessinsider.com/openai-p…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl94SDVMbGEwQUEwVzNQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728302528994156841#m</id>
            <title>Hello world！

我是Anna Indiana，我是一名AI歌手兼词曲作者。这是我的第一首歌，《Betrayed by this Town》（城市的背叛）

从曲调、节奏、和弦进行、旋律音符、节奏、歌词，再到我的形象和演唱，一切都是使用人工智能自动生成的。

希望你会喜欢它。 💕</title>
            <link>https://nitter.cz/xiaohuggg/status/1728302528994156841#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728302528994156841#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 06:40:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Hello world！<br />
<br />
我是Anna Indiana，我是一名AI歌手兼词曲作者。这是我的第一首歌，《Betrayed by this Town》（城市的背叛）<br />
<br />
从曲调、节奏、和弦进行、旋律音符、节奏、歌词，再到我的形象和演唱，一切都是使用人工智能自动生成的。<br />
<br />
希望你会喜欢它。 💕</p>
<p><a href="https://nitter.cz/AnnaIndianaAI/status/1728089499429642432#m">nitter.cz/AnnaIndianaAI/status/1728089499429642432#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728285016340451712#m</id>
            <title>R to @xiaohuggg: 通过提示文本你还可以控制动画的动作和幅度等...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728285016340451712#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728285016340451712#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 05:30:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过提示文本你还可以控制动画的动作和幅度等...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyODQ4OTI4MzA4NjMzNjAvcHUvaW1nL3RhWkFabzlnR0h0WkJiVmYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728284751931568512#m</id>
            <title>LiveSketch：为素描“注入生命” 通过文本提示将静态素描动画化

该技术能够将单一主题的静态素描转换成动画。

用户只需提供描述所需动作的文本提示，系统就会生成短动画。

通过修改描述运动的提示文本，你还可以控制生成结果的程度。

比如，如果你画了一只猫，并用文字描述它在跳跃，这个系统就能制作出一只跳跃的猫的动画。

它为任何人提供了一种简单直观的方法，使他们的素描变得生动活泼。这对于讲故事、插图、网站、演示文稿等领域都非常有用。

主要功能

•动画化静态素描：该技术能够将单一主题的静态素描转换成动画。用户只需提供描述所需动作的文本提示，系统就会生成短动画。
•基于文本到视频的先验：这种方法利用了大型预训练的文本到视频扩散模型的运动先验，通过评分蒸馏损失来指导笔触的放置。
•自然流畅的动作：为了促进自然流畅的动作并更好地保留素描的外观，该方法通过两个组件来模拟学习到的动作：一个控制小的局部变形，另一个控制全局仿射变换。

工作原理

1.素描表示：将素描表示为一组放置在白色背景上的笔触，每个笔触是一个具有四个控制点的二维贝塞尔曲线。
http://2.xn--io0a7i预测：训练一个“神经位移场”（一个小型MLP），将初始素描坐标映射到每帧的偏移量。
3.训练过程：利用预训练的文本到视频模型中封装的运动先验来训练这个网络。通过调整每个控制点的偏移量来创建所有视频帧，并使用可微光栅化器进行渲染。

LiveSketch的工作原理基于以下几个关键步骤：

1、输入处理：用户提供一个静态的草图作为输入。这个草图包含了一系列的控制点，这些点定义了草图的形状和结构。

2、特征提取：系统首先对输入的草图进行特征提取。这一步骤通过一个共享特征骨干网络完成，它将控制点的坐标转换成一个高维特征空间中的表示。这个特征空间能够捕捉到草图的关键信息，如形状和结构。

3、双路径处理：提取的特征被送入两个不同的路径：本地路径和全局路径。

•本地路径：这个路径专注于处理草图的局部细节和微小变化。它使用一个多层感知器（MLP）来预测每个控制点的偏移量，从而实现对草图的微调。
•全局路径：与此同时，全局路径处理草图的整体运动和变化，如旋转、缩放或平移。这是通过预测一个全局变换矩阵来实现的，该矩阵应用于草图的所有控制点。

4、动画生成：通过这两个路径的处理，系统能够生成一系列的帧，每一帧都是原始草图的一个变化版本。这些帧共同形成了一个连贯的动画序列，展示了草图从初始状态到最终状态的平滑过渡。

5、输出：最终，系统输出一个动画，其中草图按照用户的输入和系统生成的动态变化进行移动和变形。

应用示例

•动画素描：例如，可以创建一个游泳和跳跃的海豚，一个摇摆的眼镜蛇，或者一个玩耍的猫的动画。
•调整提示文本：通过修改描述运动的提示文本，可以控制生成结果的程度。

这项技术为设计师提供了一种新的工具，使他们能够快速且直观地将想法转化为动画形式，无需复杂的动画制作经验。

项目及演示：https://livesketch.github.io/
论文：https://livesketch.github.io/static/source/paper.pdf
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1728284751931568512#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728284751931568512#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 05:29:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LiveSketch：为素描“注入生命” 通过文本提示将静态素描动画化<br />
<br />
该技术能够将单一主题的静态素描转换成动画。<br />
<br />
用户只需提供描述所需动作的文本提示，系统就会生成短动画。<br />
<br />
通过修改描述运动的提示文本，你还可以控制生成结果的程度。<br />
<br />
比如，如果你画了一只猫，并用文字描述它在跳跃，这个系统就能制作出一只跳跃的猫的动画。<br />
<br />
它为任何人提供了一种简单直观的方法，使他们的素描变得生动活泼。这对于讲故事、插图、网站、演示文稿等领域都非常有用。<br />
<br />
主要功能<br />
<br />
•动画化静态素描：该技术能够将单一主题的静态素描转换成动画。用户只需提供描述所需动作的文本提示，系统就会生成短动画。<br />
•基于文本到视频的先验：这种方法利用了大型预训练的文本到视频扩散模型的运动先验，通过评分蒸馏损失来指导笔触的放置。<br />
•自然流畅的动作：为了促进自然流畅的动作并更好地保留素描的外观，该方法通过两个组件来模拟学习到的动作：一个控制小的局部变形，另一个控制全局仿射变换。<br />
<br />
工作原理<br />
<br />
1.素描表示：将素描表示为一组放置在白色背景上的笔触，每个笔触是一个具有四个控制点的二维贝塞尔曲线。<br />
<a href="http://2.xn--io0a7i">2.xn--io0a7i</a>预测：训练一个“神经位移场”（一个小型MLP），将初始素描坐标映射到每帧的偏移量。<br />
3.训练过程：利用预训练的文本到视频模型中封装的运动先验来训练这个网络。通过调整每个控制点的偏移量来创建所有视频帧，并使用可微光栅化器进行渲染。<br />
<br />
LiveSketch的工作原理基于以下几个关键步骤：<br />
<br />
1、输入处理：用户提供一个静态的草图作为输入。这个草图包含了一系列的控制点，这些点定义了草图的形状和结构。<br />
<br />
2、特征提取：系统首先对输入的草图进行特征提取。这一步骤通过一个共享特征骨干网络完成，它将控制点的坐标转换成一个高维特征空间中的表示。这个特征空间能够捕捉到草图的关键信息，如形状和结构。<br />
<br />
3、双路径处理：提取的特征被送入两个不同的路径：本地路径和全局路径。<br />
<br />
•本地路径：这个路径专注于处理草图的局部细节和微小变化。它使用一个多层感知器（MLP）来预测每个控制点的偏移量，从而实现对草图的微调。<br />
•全局路径：与此同时，全局路径处理草图的整体运动和变化，如旋转、缩放或平移。这是通过预测一个全局变换矩阵来实现的，该矩阵应用于草图的所有控制点。<br />
<br />
4、动画生成：通过这两个路径的处理，系统能够生成一系列的帧，每一帧都是原始草图的一个变化版本。这些帧共同形成了一个连贯的动画序列，展示了草图从初始状态到最终状态的平滑过渡。<br />
<br />
5、输出：最终，系统输出一个动画，其中草图按照用户的输入和系统生成的动态变化进行移动和变形。<br />
<br />
应用示例<br />
<br />
•动画素描：例如，可以创建一个游泳和跳跃的海豚，一个摇摆的眼镜蛇，或者一个玩耍的猫的动画。<br />
•调整提示文本：通过修改描述运动的提示文本，可以控制生成结果的程度。<br />
<br />
这项技术为设计师提供了一种新的工具，使他们能够快速且直观地将想法转化为动画形式，无需复杂的动画制作经验。<br />
<br />
项目及演示：<a href="https://livesketch.github.io/">livesketch.github.io/</a><br />
论文：<a href="https://livesketch.github.io/static/source/paper.pdf">livesketch.github.io/static/…</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyNjEzNjI2MDkyMjE2MzIvcHUvaW1nL3hiVERQT2pDZkt1SE9xbmUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728255730061582759#m</id>
            <title>LEO：3D世界中的多面手代理

LEO是一个多模态、多任务的智能体，它能够在3D环境中进行感知、定位、推理、规划和行动。

LEO通过结合大语言模型的知识和学习方案，展示了在多个领域（包括自然语言处理、计算机视觉和机器人技术）中解决通用任务的能力。

LEO的独特之处在于它能够在3D环境中理解和执行基于语言的指令，这使其在多种复杂任务中具有广泛的应用潜力。

主要功能和作用：

1、3D视觉-语言理解：LEO能够理解3D环境中的视觉信息，并将其与语言描述相结合。这意味着它可以看到一个物体（如苹果），并理解相关的语言描述（如“这是一个苹果”）。

2、执行语言指令：它能够根据语言指令执行动作。例如，如果有人指示“把苹果放在桌子上”，LEO能够理解这一指令并执行相应的动作。

3、多样化的3D任务执行：LEO在多种3D任务上表现出色，包括3D字幕（为场景中的物体或事件创建描述性文字）、问答（回答关于3D环境的问题）、具身推理（在3D环境中进行逻辑推理）、具身导航（在3D空间中找到特定的位置或物体）和机器人操控（控制机器人执行特定任务）。

为了训练LEO，研究者们创建了一个大型的数据集，包含了各种3D环境中的任务，这些任务需要深入理解和与3D世界的互动。

LEO在多种3D任务上进行了测试，包括：

•3D字幕：为3D场景中的物体或事件创建描述性文字。
•问答：回答关于3D环境的问题。
•具身推理：在3D环境中进行逻辑推理。
•具身导航：在3D空间中找到特定的位置或物体。
•机器人操控：控制机器人执行特定任务。

LEO在这些任务上表现出色，显示了它在多样化任务中的熟练程度。

工作原理：

1、两阶段训练：

LEO的训练分为两个阶段：

•3D视觉-语言对齐：在这一阶段，LEO学习如何将3D图像与语言描述相结合。这涉及到理解视觉信息和语言信息之间的关系。
•3D视觉-语言-动作指令调整：在这一阶段，LEO学习如何根据语言指令执行具体的动作。这需要它理解指令并将其转化为动作。

2、大规模数据集：为了支持这种训练，研究者们创建了一个包含多种3D环境任务的大规模数据集。这些任务需要深入理解和与3D世界的互动。

3、多模态学习：LEO结合了视觉信息（如图像和视频）和语言信息（如文字描述），使其能够在多模态环境中工作。

4、广泛的应用能力：通过这种训练和数据集，LEO能够在多种3D任务中表现出色，展示了其广泛的应用能力。

项目及演示：https://embodied-generalist.github.io/
论文：https://arxiv.org/abs/2311.12871
GitHub：https://github.com/embodied-generalist/embodied-generalist</title>
            <link>https://nitter.cz/xiaohuggg/status/1728255730061582759#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728255730061582759#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 25 Nov 2023 03:34:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LEO：3D世界中的多面手代理<br />
<br />
LEO是一个多模态、多任务的智能体，它能够在3D环境中进行感知、定位、推理、规划和行动。<br />
<br />
LEO通过结合大语言模型的知识和学习方案，展示了在多个领域（包括自然语言处理、计算机视觉和机器人技术）中解决通用任务的能力。<br />
<br />
LEO的独特之处在于它能够在3D环境中理解和执行基于语言的指令，这使其在多种复杂任务中具有广泛的应用潜力。<br />
<br />
主要功能和作用：<br />
<br />
1、3D视觉-语言理解：LEO能够理解3D环境中的视觉信息，并将其与语言描述相结合。这意味着它可以看到一个物体（如苹果），并理解相关的语言描述（如“这是一个苹果”）。<br />
<br />
2、执行语言指令：它能够根据语言指令执行动作。例如，如果有人指示“把苹果放在桌子上”，LEO能够理解这一指令并执行相应的动作。<br />
<br />
3、多样化的3D任务执行：LEO在多种3D任务上表现出色，包括3D字幕（为场景中的物体或事件创建描述性文字）、问答（回答关于3D环境的问题）、具身推理（在3D环境中进行逻辑推理）、具身导航（在3D空间中找到特定的位置或物体）和机器人操控（控制机器人执行特定任务）。<br />
<br />
为了训练LEO，研究者们创建了一个大型的数据集，包含了各种3D环境中的任务，这些任务需要深入理解和与3D世界的互动。<br />
<br />
LEO在多种3D任务上进行了测试，包括：<br />
<br />
•3D字幕：为3D场景中的物体或事件创建描述性文字。<br />
•问答：回答关于3D环境的问题。<br />
•具身推理：在3D环境中进行逻辑推理。<br />
•具身导航：在3D空间中找到特定的位置或物体。<br />
•机器人操控：控制机器人执行特定任务。<br />
<br />
LEO在这些任务上表现出色，显示了它在多样化任务中的熟练程度。<br />
<br />
工作原理：<br />
<br />
1、两阶段训练：<br />
<br />
LEO的训练分为两个阶段：<br />
<br />
•3D视觉-语言对齐：在这一阶段，LEO学习如何将3D图像与语言描述相结合。这涉及到理解视觉信息和语言信息之间的关系。<br />
•3D视觉-语言-动作指令调整：在这一阶段，LEO学习如何根据语言指令执行具体的动作。这需要它理解指令并将其转化为动作。<br />
<br />
2、大规模数据集：为了支持这种训练，研究者们创建了一个包含多种3D环境任务的大规模数据集。这些任务需要深入理解和与3D世界的互动。<br />
<br />
3、多模态学习：LEO结合了视觉信息（如图像和视频）和语言信息（如文字描述），使其能够在多模态环境中工作。<br />
<br />
4、广泛的应用能力：通过这种训练和数据集，LEO能够在多种3D任务中表现出色，展示了其广泛的应用能力。<br />
<br />
项目及演示：<a href="https://embodied-generalist.github.io/">embodied-generalist.github.i…</a><br />
论文：<a href="https://arxiv.org/abs/2311.12871">arxiv.org/abs/2311.12871</a><br />
GitHub：<a href="https://github.com/embodied-generalist/embodied-generalist">github.com/embodied-generali…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgyNTUyNjk2MTUxNDkwNTYvcHUvaW1nL3JiYkcyNWV0aWhlQjNfVzYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1728049969058423029#m</id>
            <title>DeepFace： 一个功能强大、易于使用的面部识别和分析工具

主要功能：

- 面部验证：验证两张面部图片是否属于同一人。

- 面部识别：在一个已知的面部数据库中查找输入图像的身份。

- 面部属性分析:：预测面部图像的年龄、性别、种族和情绪。

- 嵌入式表示：返回面部图像的多维向量表示(面部的关键特征)

它是一个混合面部识别框架，包装了多个先进的模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace、DeepID、ArcFace、Dlib 和 SFace。

这些模型已经达到或超过了人类在面部识别任务上的准确率（97.53%）。

DeepFace 还支持实时视频分析，并提供了一个 API，允许从外部系统（如移动应用或网页）调用其功能。此外，它还提供了命令行界面，方便用户在命令行中访问其功能。

主要优点：

1、高准确率：DeepFace 集成了多个先进的面部识别模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace 等，这些模型在面部识别任务上的准确率非常高，有的甚至达到或超过了人类的准确率（97.53%）。

2、多功能性：除了基本的面部识别和验证功能，DeepFace 还提供面部属性分析（如年龄、性别、种族和情绪预测），这使得它可以应用于更广泛的场景。

3、灵活性和兼容性：DeepFace 支持多种面部检测器和相似度计算方法，使其能够适应不同的应用需求和环境。

4、易用性：DeepFace 的安装和使用都相对简单，提供了 Python API 和命令行界面，方便不同背景的用户使用。

5、实时视频分析：DeepFace 还支持实时视频分析，这对于需要动态面部识别和分析的应用场景非常有用。

作者：@serengil
GitHub：https://github.com/serengil/deepface</title>
            <link>https://nitter.cz/xiaohuggg/status/1728049969058423029#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1728049969058423029#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 13:56:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DeepFace： 一个功能强大、易于使用的面部识别和分析工具<br />
<br />
主要功能：<br />
<br />
- 面部验证：验证两张面部图片是否属于同一人。<br />
<br />
- 面部识别：在一个已知的面部数据库中查找输入图像的身份。<br />
<br />
- 面部属性分析:：预测面部图像的年龄、性别、种族和情绪。<br />
<br />
- 嵌入式表示：返回面部图像的多维向量表示(面部的关键特征)<br />
<br />
它是一个混合面部识别框架，包装了多个先进的模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace、DeepID、ArcFace、Dlib 和 SFace。<br />
<br />
这些模型已经达到或超过了人类在面部识别任务上的准确率（97.53%）。<br />
<br />
DeepFace 还支持实时视频分析，并提供了一个 API，允许从外部系统（如移动应用或网页）调用其功能。此外，它还提供了命令行界面，方便用户在命令行中访问其功能。<br />
<br />
主要优点：<br />
<br />
1、高准确率：DeepFace 集成了多个先进的面部识别模型，如 VGG-Face、Google FaceNet、OpenFace、Facebook DeepFace 等，这些模型在面部识别任务上的准确率非常高，有的甚至达到或超过了人类的准确率（97.53%）。<br />
<br />
2、多功能性：除了基本的面部识别和验证功能，DeepFace 还提供面部属性分析（如年龄、性别、种族和情绪预测），这使得它可以应用于更广泛的场景。<br />
<br />
3、灵活性和兼容性：DeepFace 支持多种面部检测器和相似度计算方法，使其能够适应不同的应用需求和环境。<br />
<br />
4、易用性：DeepFace 的安装和使用都相对简单，提供了 Python API 和命令行界面，方便不同背景的用户使用。<br />
<br />
5、实时视频分析：DeepFace 还支持实时视频分析，这对于需要动态面部识别和分析的应用场景非常有用。<br />
<br />
作者：<a href="https://nitter.cz/serengil" title="Sefik Ilkin Serengil">@serengil</a><br />
GitHub：<a href="https://github.com/serengil/deepface">github.com/serengil/deepface</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjgwNDkxNDQyNTY5ODMwNDAvcHUvaW1nL1NQalJOQ0xuczVGTEdsMEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/hylarucoder/status/1727997579429126361#m</id>
            <title>RT by @xiaohuggg: Comfyui 已支持SVD Image 2 Video, 内存占用已优化到丧心病狂的 8GB.

大家可以用一下这个workflow在自己的显卡上跑视频咯. (内存占用7.7GB🧐)

github 地址 https://github.com/hylarucoder/comfyui-workflow/blob/main/svd/svd-image-to-video.json</title>
            <link>https://nitter.cz/hylarucoder/status/1727997579429126361#m</link>
            <guid isPermaLink="false">https://nitter.cz/hylarucoder/status/1727997579429126361#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 10:28:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Comfyui 已支持SVD Image 2 Video, 内存占用已优化到丧心病狂的 8GB.<br />
<br />
大家可以用一下这个workflow在自己的显卡上跑视频咯. (内存占用7.7GB🧐)<br />
<br />
github 地址 <a href="https://github.com/hylarucoder/comfyui-workflow/blob/main/svd/svd-image-to-video.json">github.com/hylarucoder/comfy…</a></p>
<p><a href="https://nitter.cz/hylarucoder/status/1727497815394586933#m">nitter.cz/hylarucoder/status/1727497815394586933#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc5OTcyMzkwOTQ4NzQxMTIvcHUvaW1nL19Ra1RVUGtyS2dpWGRFMUYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727998618106531930#m</id>
            <title>独立第三方

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1727998618106531930#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727998618106531930#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 10:32:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>独立第三方<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9zVmxTTGJzQUFnZXEwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727922701317370118#m</id>
            <title>泄露和被破解的GPTs提示大集合😂

收集了多种GPTs模型的“泄露提示”（Leaked Prompts）

这些提示是用于指导或开发GPTs机器人的特定指令或描述。

例如“AI Paper Polisher Pro”、“AI算命”、“AI Doctor”等，涵盖了从学术论文润色到算命、医疗咨询等多个领域。

这些玩意可以帮助理解GPTs在处理不同类型的请求时的表现，还可以作为开发新应用或进行创新实验的灵感来源。甚至你可以直接用来复制一个一模一样的GPT...😅

GitHub：https://github.com/linexjlin/GPTs</title>
            <link>https://nitter.cz/xiaohuggg/status/1727922701317370118#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727922701317370118#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 05:30:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>泄露和被破解的GPTs提示大集合😂<br />
<br />
收集了多种GPTs模型的“泄露提示”（Leaked Prompts）<br />
<br />
这些提示是用于指导或开发GPTs机器人的特定指令或描述。<br />
<br />
例如“AI Paper Polisher Pro”、“AI算命”、“AI Doctor”等，涵盖了从学术论文润色到算命、医疗咨询等多个领域。<br />
<br />
这些玩意可以帮助理解GPTs在处理不同类型的请求时的表现，还可以作为开发新应用或进行创新实验的灵感来源。甚至你可以直接用来复制一个一模一样的GPT...😅<br />
<br />
GitHub：<a href="https://github.com/linexjlin/GPTs">github.com/linexjlin/GPTs</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc5MTk3OTQwNjkwNDUyNDgvcHUvaW1nL3BLNnRLT2JrMkx1MFhFTTguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727911657358442825#m</id>
            <title>PixelDance：字节跳动开发的高动态视频生成模型

通过描述(纯文本)+首帧指导(图片)+尾帧指导(图片)>生成包含复杂场景和动作的视频。

PixelDance不仅根据你的文字描述生成视频内容，还结合了你提供的起始和结束画面，使得视频内容更加丰富连贯和生动，为用户提供了更多的创造性控制。

PixelDance在多种场景下表现出色，尤其是在生成复杂场景和动作的视频方面。它能够生成连续的视频剪辑，并在时间一致性和视频质量方面超越现有的长视频生成方法。

举例解释：

假设你想创造一个视频，内容是一只猫在花园里追逐蝴蝶。在PixelDance系统中，你可以这样操作：

1、文本指令：你输入一段文本指令，比如“一只橘色的猫在一个充满花朵的花园里追逐飞舞的蝴蝶”。

2、图像指令：你提供两张图片，一张是视频的起始画面，比如一只橘色的猫正准备跳跃的画面；另一张是视频的结束画面，比如猫成功捕捉到蝴蝶，或者蝴蝶飞走了，猫看着空中的画面。

3、视频生成：PixelDance系统会根据你的文本指令和两张图片，生成一个完整的视频。在这个视频中，你会看到猫从准备跳跃的姿势开始，经过一系列动态的追逐动作，最终达到你提供的结束画面。

主要特点：

1、高动态视频生成：该项目专注于创造动态丰富、视觉效果复杂的视频。它能够处理包含复杂动作和场景变换的视频内容，生成连贯且吸引人的视觉故事。

2、灵活性和适应性：在处理用户提供的最后一帧图像时，PixelDance显示出高度的灵活性。它不要求完全复制这一帧，而是能够根据提供的图像进行适当的调整和创造。

3、超越现有技术：PixelDance在生成长视频方面的性能超过了现有的视频生成技术，特别是在保持时间一致性和视频质量方面。

4、创新的扩散模型应用：该项目利用了扩散模型（diffusion models）的新颖应用，这是一种先进的机器学习技术，用于生成高质量的图像和视频内容。

项目及演示：https://makepixelsdance.github.io/
论文
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1727911657358442825#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727911657358442825#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 04:46:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>PixelDance：字节跳动开发的高动态视频生成模型<br />
<br />
通过描述(纯文本)+首帧指导(图片)+尾帧指导(图片)>生成包含复杂场景和动作的视频。<br />
<br />
PixelDance不仅根据你的文字描述生成视频内容，还结合了你提供的起始和结束画面，使得视频内容更加丰富连贯和生动，为用户提供了更多的创造性控制。<br />
<br />
PixelDance在多种场景下表现出色，尤其是在生成复杂场景和动作的视频方面。它能够生成连续的视频剪辑，并在时间一致性和视频质量方面超越现有的长视频生成方法。<br />
<br />
举例解释：<br />
<br />
假设你想创造一个视频，内容是一只猫在花园里追逐蝴蝶。在PixelDance系统中，你可以这样操作：<br />
<br />
1、文本指令：你输入一段文本指令，比如“一只橘色的猫在一个充满花朵的花园里追逐飞舞的蝴蝶”。<br />
<br />
2、图像指令：你提供两张图片，一张是视频的起始画面，比如一只橘色的猫正准备跳跃的画面；另一张是视频的结束画面，比如猫成功捕捉到蝴蝶，或者蝴蝶飞走了，猫看着空中的画面。<br />
<br />
3、视频生成：PixelDance系统会根据你的文本指令和两张图片，生成一个完整的视频。在这个视频中，你会看到猫从准备跳跃的姿势开始，经过一系列动态的追逐动作，最终达到你提供的结束画面。<br />
<br />
主要特点：<br />
<br />
1、高动态视频生成：该项目专注于创造动态丰富、视觉效果复杂的视频。它能够处理包含复杂动作和场景变换的视频内容，生成连贯且吸引人的视觉故事。<br />
<br />
2、灵活性和适应性：在处理用户提供的最后一帧图像时，PixelDance显示出高度的灵活性。它不要求完全复制这一帧，而是能够根据提供的图像进行适当的调整和创造。<br />
<br />
3、超越现有技术：PixelDance在生成长视频方面的性能超过了现有的视频生成技术，特别是在保持时间一致性和视频质量方面。<br />
<br />
4、创新的扩散模型应用：该项目利用了扩散模型（diffusion models）的新颖应用，这是一种先进的机器学习技术，用于生成高质量的图像和视频内容。<br />
<br />
项目及演示：<a href="https://makepixelsdance.github.io/">makepixelsdance.github.io/</a><br />
论文<br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc5MTA4MjU2MTExMjg4MzIvcHUvaW1nLzYwWGNLMXVBMUZlblpIQV8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727691894405398617#m</id>
            <title>RT by @xiaohuggg: Q-learning类似老鼠走迷宫的游戏，据说Q*还结合了A Star的搜索算法！

找了个迷宫解说先学习一下Q-learning再说！😂

爱学习的孩子可以看看！

Q-learning是一种强化学习算法，用于学习在给定状态下执行哪个动作以最大化某种形式的奖励或回报。在Q学习中，“Q”代表质量（quality），指的是执行特定动作带来的预期效益。

工作原理：

1.状态和动作：Q学习算法在一个由状态和动作组成的环境中工作。状态是环境的描述，动作是在这些状态下可以执行的操作。
2.Q表：算法维护一个Q表，这是一个查找表，用于存储每个状态-动作对的Q值（即该动作的预期效益）。
3.学习过程：当智能体（如机器人、软件代理）在环境中执行动作时，它会根据动作的结果（通常是奖励或惩罚）来更新Q表。这个更新过程是基于一种称为贝尔曼方程的数学公式。

举例说明：

假设有一个简单的迷宫游戏，智能体的目标是找到从起点到终点的最短路径。在这个例子中：

•状态：迷宫中的每个位置。
•动作：从一个位置移动到另一个位置（例如，向上、向下、向左、向右移动）。
•奖励：到达终点时获得正奖励，撞墙时获得负奖励。

智能体开始时对迷宫一无所知，它随机移动并从结果中学习。每次移动后，它更新Q表，记录在特定位置执行特定动作的效益。随着时间的推移，智能体学会识别哪些动作会带来更好的结果（比如更快到达终点），并开始优先选择这些动作。

结论：

Q学习的关键优势在于它不需要环境的先验知识，智能体通过与环境的交互学习最佳策略。这使得Q学习非常适合于那些模型无法提前了解所有可能状态的复杂环境。</title>
            <link>https://nitter.cz/xiaohuggg/status/1727691894405398617#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727691894405398617#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 14:13:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Q-learning类似老鼠走迷宫的游戏，据说Q*还结合了A Star的搜索算法！<br />
<br />
找了个迷宫解说先学习一下Q-learning再说！😂<br />
<br />
爱学习的孩子可以看看！<br />
<br />
Q-learning是一种强化学习算法，用于学习在给定状态下执行哪个动作以最大化某种形式的奖励或回报。在Q学习中，“Q”代表质量（quality），指的是执行特定动作带来的预期效益。<br />
<br />
工作原理：<br />
<br />
1.状态和动作：Q学习算法在一个由状态和动作组成的环境中工作。状态是环境的描述，动作是在这些状态下可以执行的操作。<br />
2.Q表：算法维护一个Q表，这是一个查找表，用于存储每个状态-动作对的Q值（即该动作的预期效益）。<br />
3.学习过程：当智能体（如机器人、软件代理）在环境中执行动作时，它会根据动作的结果（通常是奖励或惩罚）来更新Q表。这个更新过程是基于一种称为贝尔曼方程的数学公式。<br />
<br />
举例说明：<br />
<br />
假设有一个简单的迷宫游戏，智能体的目标是找到从起点到终点的最短路径。在这个例子中：<br />
<br />
•状态：迷宫中的每个位置。<br />
•动作：从一个位置移动到另一个位置（例如，向上、向下、向左、向右移动）。<br />
•奖励：到达终点时获得正奖励，撞墙时获得负奖励。<br />
<br />
智能体开始时对迷宫一无所知，它随机移动并从结果中学习。每次移动后，它更新Q表，记录在特定位置执行特定动作的效益。随着时间的推移，智能体学会识别哪些动作会带来更好的结果（比如更快到达终点），并开始优先选择这些动作。<br />
<br />
结论：<br />
<br />
Q学习的关键优势在于它不需要环境的先验知识，智能体通过与环境的交互学习最佳策略。这使得Q学习非常适合于那些模型无法提前了解所有可能状态的复杂环境。</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzI3NjkxODIyMDIwMTI0NjcyL2ltZy9ndlA3cFo3Q3A1bU1ZeWZTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1727881898012336253#m</id>
            <title>Merse：使用AI将个人故事和经历转化为各种形式的内容，保存和分享这些珍贵记忆。

这个工具可以将你的日常时刻、身边故事和经历转化为包括漫画、书籍、电影、语音、自传等多种形式保存下来。

作者在这个项目花费了8个月时间，可惜有别的事情要做，无法继续...

现在把它开源了，感兴趣的可以继续...

作者：@markrachapoom
Demo：https://www.merse.co/
GitHub：https://github.com/markrachapoom/merse</title>
            <link>https://nitter.cz/xiaohuggg/status/1727881898012336253#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1727881898012336253#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 02:48:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Merse：使用AI将个人故事和经历转化为各种形式的内容，保存和分享这些珍贵记忆。<br />
<br />
这个工具可以将你的日常时刻、身边故事和经历转化为包括漫画、书籍、电影、语音、自传等多种形式保存下来。<br />
<br />
作者在这个项目花费了8个月时间，可惜有别的事情要做，无法继续...<br />
<br />
现在把它开源了，感兴趣的可以继续...<br />
<br />
作者：<a href="https://nitter.cz/markrachapoom" title="Mark Rachapoom">@markrachapoom</a><br />
Demo：<a href="https://www.merse.co/">merse.co/</a><br />
GitHub：<a href="https://github.com/markrachapoom/merse">github.com/markrachapoom/mer…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mjc4NzcxMjA0NjUxNzQ1MjgvcHUvaW1nLzNUU1hsQVM1OTEyaUtYY0IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>