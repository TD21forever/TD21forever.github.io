<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1757022189201858603#m</id>
            <title>Franco Ronconi 介绍了一个Canvastique3D工具

这是一个结合了OpenCV（一个开源计算机视觉库）和OpenAI技术的工具

这个工具允许设计师在3D模型上实时预览他们的手工设计，这意味着设计师可以立即看到他们的设计在虚拟三维空间中的样子，而不需要等待制作实体样品。</title>
            <link>https://nitter.cz/xiaohuggg/status/1757022189201858603#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1757022189201858603#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 12:41:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Franco Ronconi 介绍了一个Canvastique3D工具<br />
<br />
这是一个结合了OpenCV（一个开源计算机视觉库）和OpenAI技术的工具<br />
<br />
这个工具允许设计师在3D模型上实时预览他们的手工设计，这意味着设计师可以立即看到他们的设计在虚拟三维空间中的样子，而不需要等待制作实体样品。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzI4MDc1NDMwNzA2MTM1MDQvcHUvaW1nL1hNWUpUZDN1TEc4NkJtSVIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756977011782979921#m</id>
            <title>MoneyPrinter：自动创建YouTube短视频的自动化赚钱项目

主要功能：

- 自动视频生成：只要输入视频话题即可自动产生与之相关的短视频。

- 音乐和字体自定义：可以上传自己的MP3文件压缩包和字体，自定义视频音乐背景和字体。

-自动将生成的视频上传到YouTube。

整个过程几乎不需要用户有太多的视频编辑技能，只需要简单的操作和等待程序完成工作。

MoneyPrinter的背后技术主要依赖于Python编程语言和MoviePy视频编辑库，以及YouTube的API用于视频上传，使得从视频创意到发布的整个流程自动化和无缝连接。

MoviePy是一个强大的视频处理库，能够编辑视频、添加音乐背景和文本等。

GitHub：https://github.com/FujiwaraChoki/MoneyPrinter</title>
            <link>https://nitter.cz/xiaohuggg/status/1756977011782979921#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756977011782979921#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 09:42:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MoneyPrinter：自动创建YouTube短视频的自动化赚钱项目<br />
<br />
主要功能：<br />
<br />
- 自动视频生成：只要输入视频话题即可自动产生与之相关的短视频。<br />
<br />
- 音乐和字体自定义：可以上传自己的MP3文件压缩包和字体，自定义视频音乐背景和字体。<br />
<br />
-自动将生成的视频上传到YouTube。<br />
<br />
整个过程几乎不需要用户有太多的视频编辑技能，只需要简单的操作和等待程序完成工作。<br />
<br />
MoneyPrinter的背后技术主要依赖于Python编程语言和MoviePy视频编辑库，以及YouTube的API用于视频上传，使得从视频创意到发布的整个流程自动化和无缝连接。<br />
<br />
MoviePy是一个强大的视频处理库，能够编辑视频、添加音乐背景和文本等。<br />
<br />
GitHub：<a href="https://github.com/FujiwaraChoki/MoneyPrinter">github.com/FujiwaraChoki/Mon…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjkwOTI0MzM4MDUzMTIvcHUvaW1nL1FLTlB4S3gyOF9nYzAtUTguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962301620699315#m</id>
            <title>R to @xiaohuggg: 多个移动箱+混合摄像机移动</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962301620699315#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962301620699315#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>多个移动箱+混合摄像机移动</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjIxNTUzNzc5Mzg0MzIvcHUvaW1nL1A3SHVZODZHRkppZGMtWVguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962297107640406#m</id>
            <title>R to @xiaohuggg: 单移动箱+混合摄像机移动</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962297107640406#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962297107640406#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>单移动箱+混合摄像机移动</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjIwMzcxNDQ2MTY5NjAvcHUvaW1nL3JLbEhKOEllU2dxVjVtV3AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962293081104729#m</id>
            <title>R to @xiaohuggg: 混合摄像机移动 （X+Y）</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962293081104729#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962293081104729#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>混合摄像机移动 （X+Y）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5NjE4MjE3NTg3ODM0ODgvcHUvaW1nL3hlQnFic3hRNkpCeS0xZEkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756962287691444265#m</id>
            <title>Direct-a-Video：像导演一样生成视频

它可以允许用户通过自然语言独立或共同控制摄像机移动和/或对象运动。

比如你只需要告诉它你想要什么样的对象运动（比如小狗跑动）和摄像机怎么移动（比如跟着小狗），剩下的工作就交给它了。

在Direct-a-Video项目中，用户可以控制以下类型的运动：

摄像机移动控制

-基础摄像机移动
-混合摄像机移动 (X+Y)
-混合摄像机移动 (X+Z)

对象运动控制

-摄像机移动和对象运动的联合控制
-静态框 + 混合摄像机移动
-单个移动框 + 混合摄像机移动
-多个移动框 + 混合摄像机移动

1、物体运动：用户可以指定一个或多个物体在视频中的具体运动路径，如前进、后退、上升、下降、旋转等。这意味着如果你想让视频中的一个球向左滚动，或者一个人物向摄像机走近，都可以通过文本指令实现。

2、摄像机移动：用户还可以控制摄像机的移动方式，包括平移（左右移动）、倾斜（上下移动）、缩放（放大或缩小画面）等。这可以帮助创造出从不同角度和距离观察场景的效果，比如模拟从高空俯瞰或者近距离跟踪某个物体的视角。

3、联合控制物体和摄像机运动：Direct-a-Video独特的功能在于，它允许用户同时控制物体的运动和摄像机的移动。这意味着你可以创作出更加动态和复杂的视频场景，比如在跟随一个移动物体的同时，摄像机也在进行缩放或者旋转，以创造出电影般的视觉效果。

工作原理：

Direct-a-Video通过两个主要机制实现对视频生成的细粒度控制：对象运动控制和相机移动控制。这两种控制机制独立运作，但也可以联合使用，为用户提供了高度的定制能力和创造性的自由。以下是其工作原理的详细解释：

1、文本解析和意图理解：系统首先解析用户输入的文本，理解用户希望在视频中看到的摄像机移动和物体动作的具体要求。

2、对象运动控制

空间交叉注意调制：Direct-a-Video利用空间交叉注意调制来控制对象在视频中的运动。这种方法依赖于模型固有的先验知识，无需额外的优化过程。

用户通过输入文本提示来指定对象及其在视频中的运动轨迹（例如，一个对象从屏幕一边移动到另一边）。模型使用这些文本提示来引导对象在视频帧中的空间和时间放置。

3、相机移动控制

时间交叉注意层：为了模拟相机移动（如平移、缩放等），Direct-a-Video引入了时间交叉注意层。这些层能够解释用户通过参数指定的相机移动，从而在视频生成过程中实现相机视角的变化。

4、自监督学习：模型通过在小规模数据集上应用基于增强的自监督学习方法来训练时间交叉注意层，这一过程无需显式的运动注释。训练阶段，视频样本经过增强处理（例如模拟相机的平移和缩放），以训练模型理解和实现相机移动。

5、联合控制

用户可以单独控制对象运动或相机移动，也可以同时控制两者，实现更复杂的视频创作。例如，用户可以设计一个场景，其中一个对象在屏幕上移动，同时相机围绕场景进行平移和缩放，创造出动态且引人入胜的视频内容。

这种设计使得Direct-a-Video能够以一种非常灵活和动态的方式生成视频，用户可以通过简单的文本描述来“导演”视频中的场景，实现了高度个性化和创意的视频内容创作。

项目及演示：https://direct-a-video.github.io/</title>
            <link>https://nitter.cz/xiaohuggg/status/1756962287691444265#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756962287691444265#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 08:43:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Direct-a-Video：像导演一样生成视频<br />
<br />
它可以允许用户通过自然语言独立或共同控制摄像机移动和/或对象运动。<br />
<br />
比如你只需要告诉它你想要什么样的对象运动（比如小狗跑动）和摄像机怎么移动（比如跟着小狗），剩下的工作就交给它了。<br />
<br />
在Direct-a-Video项目中，用户可以控制以下类型的运动：<br />
<br />
摄像机移动控制<br />
<br />
-基础摄像机移动<br />
-混合摄像机移动 (X+Y)<br />
-混合摄像机移动 (X+Z)<br />
<br />
对象运动控制<br />
<br />
-摄像机移动和对象运动的联合控制<br />
-静态框 + 混合摄像机移动<br />
-单个移动框 + 混合摄像机移动<br />
-多个移动框 + 混合摄像机移动<br />
<br />
1、物体运动：用户可以指定一个或多个物体在视频中的具体运动路径，如前进、后退、上升、下降、旋转等。这意味着如果你想让视频中的一个球向左滚动，或者一个人物向摄像机走近，都可以通过文本指令实现。<br />
<br />
2、摄像机移动：用户还可以控制摄像机的移动方式，包括平移（左右移动）、倾斜（上下移动）、缩放（放大或缩小画面）等。这可以帮助创造出从不同角度和距离观察场景的效果，比如模拟从高空俯瞰或者近距离跟踪某个物体的视角。<br />
<br />
3、联合控制物体和摄像机运动：Direct-a-Video独特的功能在于，它允许用户同时控制物体的运动和摄像机的移动。这意味着你可以创作出更加动态和复杂的视频场景，比如在跟随一个移动物体的同时，摄像机也在进行缩放或者旋转，以创造出电影般的视觉效果。<br />
<br />
工作原理：<br />
<br />
Direct-a-Video通过两个主要机制实现对视频生成的细粒度控制：对象运动控制和相机移动控制。这两种控制机制独立运作，但也可以联合使用，为用户提供了高度的定制能力和创造性的自由。以下是其工作原理的详细解释：<br />
<br />
1、文本解析和意图理解：系统首先解析用户输入的文本，理解用户希望在视频中看到的摄像机移动和物体动作的具体要求。<br />
<br />
2、对象运动控制<br />
<br />
空间交叉注意调制：Direct-a-Video利用空间交叉注意调制来控制对象在视频中的运动。这种方法依赖于模型固有的先验知识，无需额外的优化过程。<br />
<br />
用户通过输入文本提示来指定对象及其在视频中的运动轨迹（例如，一个对象从屏幕一边移动到另一边）。模型使用这些文本提示来引导对象在视频帧中的空间和时间放置。<br />
<br />
3、相机移动控制<br />
<br />
时间交叉注意层：为了模拟相机移动（如平移、缩放等），Direct-a-Video引入了时间交叉注意层。这些层能够解释用户通过参数指定的相机移动，从而在视频生成过程中实现相机视角的变化。<br />
<br />
4、自监督学习：模型通过在小规模数据集上应用基于增强的自监督学习方法来训练时间交叉注意层，这一过程无需显式的运动注释。训练阶段，视频样本经过增强处理（例如模拟相机的平移和缩放），以训练模型理解和实现相机移动。<br />
<br />
5、联合控制<br />
<br />
用户可以单独控制对象运动或相机移动，也可以同时控制两者，实现更复杂的视频创作。例如，用户可以设计一个场景，其中一个对象在屏幕上移动，同时相机围绕场景进行平移和缩放，创造出动态且引人入胜的视频内容。<br />
<br />
这种设计使得Direct-a-Video能够以一种非常灵活和动态的方式生成视频，用户可以通过简单的文本描述来“导演”视频中的场景，实现了高度个性化和创意的视频内容创作。<br />
<br />
项目及演示：<a href="https://direct-a-video.github.io/">direct-a-video.github.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5Mjk2ODEzNjUwNDUyNDgvcHUvaW1nL1BjNzFRUFJqbDk0Z3I0SHIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756938524044189894#m</id>
            <title>R to @xiaohuggg: 该广告的幕后故事：

https://youtu.be/YmXUbzpDY5s</title>
            <link>https://nitter.cz/xiaohuggg/status/1756938524044189894#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756938524044189894#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 07:09:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>该广告的幕后故事：<br />
<br />
<a href="https://youtu.be/YmXUbzpDY5s">youtu.be/YmXUbzpDY5s</a></p>
<img src="https://nitter.cz/pic/enc/Y2FyZF9pbWcvMTc1NjA0ODQwODUyNjEzNTI5Ni82TFY3b0NzdT9mb3JtYXQ9anBnJm5hbWU9ODAweDMyMF8x" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756938521179517436#m</id>
            <title>盲人导演 @adamdavidmorse，在他的超级碗广告首秀中，展示了Pixel 8 的“Guided Frame”功能。

这项功能利用 Google AI 技术，使得盲人或视力低下的人士更容易拍摄照片并分享日常生活。</title>
            <link>https://nitter.cz/xiaohuggg/status/1756938521179517436#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756938521179517436#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 07:09:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>盲人导演 <a href="https://nitter.cz/adamdavidmorse" title="Adam Morse">@adamdavidmorse</a>，在他的超级碗广告首秀中，展示了Pixel 8 的“Guided Frame”功能。<br />
<br />
这项功能利用 Google AI 技术，使得盲人或视力低下的人士更容易拍摄照片并分享日常生活。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5MzczMzQ5ODQxOTIwMDAvcHUvaW1nL0RTNVNfdGxXM2psc2hINUwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756917238467379401#m</id>
            <title>这个好，我很需要🫰

牛P，还能下载视频...</title>
            <link>https://nitter.cz/xiaohuggg/status/1756917238467379401#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756917238467379401#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 05:44:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个好，我很需要🫰<br />
<br />
牛P，还能下载视频...</p>
<p><a href="https://nitter.cz/dotey/status/1756901189646422132#m">nitter.cz/dotey/status/1756901189646422132#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756915564692545660#m</id>
            <title>纽约大学的一个研究团队开发了一种新技术，能够在短短18秒内教会一架无人机如何稳定飞行。

该程序可以在一台普通的MacBook Pro上运行。通过模拟飞行环境来训练无人机，让它学会如何保持空中悬停并按照指定路径飞行。

只需18秒钟就能实现这一切。

这种方法不仅限于简单的小型无人机——它几乎可以适用于任何无人机，包括更大、更昂贵的无人机，甚至是你自己从头开始建造的无人机。

工作原理：

1、端到端控制：通过深度强化学习（Deep RL）实现四旋翼无人机从感知到动作输出的直接映射，无需复杂的中间处理层，提高了控制策略的直接性和效率。

2、不对称演员-评论家架构：采用了一种新颖的基于RL的训练框架，其中演员直接从状态到动作的映射进行决策，而评论家则利用额外的信息（如仿真中的精确状态）来评估动作的好坏，帮助演员更快学习。

通过给予奖励和惩罚来教会模型执行某项任务。在这个项目中，无人机通过尝试不同的动作，根据其对任务成功率的影响获得反馈，从而学习如何飞行。

3、高度优化的仿真器：开发了一个能在消费级笔记本电脑上模拟约5个月飞行时间每秒的高性能仿真器，这个仿真器使得无人机的训练过程极为快速。这种方法允许无人机在没有任何风险的情况下进行无数次尝试和错误，快速学习飞行技能。

4、课程学习（Curriculum Learning）：为了提高训练效率，研究者采用了课程学习策略，即从简单任务开始逐步过渡到更复杂的任务。这种方法让无人机先学习基本的飞行控制，然后逐渐学习执行更复杂的飞行动作。

5、调整奖励函数：训练过程中，研究者会调整奖励函数，即改变给予无人机的反馈，以鼓励它学习如何稳定飞行和执行特定的飞行路径。一开始，奖励机制较为宽松，随着训练的深入，会逐渐增加对飞行精确度和鲁棒性的要求。

6、Sim2Real转移策略：通过精心设计的训练范式和仿真环境，确保了无人机控制策略可以平滑地从仿真环境转移到真实环境，克服了仿真与现实之间的差距。

该项目利用高度优化的仿真器和有效的学习策略，实现了在仅18秒内完成无人机飞行控制策略的训练，大大减少了从理论到实践的时间。

项目将代码和仿真器已经开源。

论文：https://arxiv.org/abs/2311.13081
GitHub：https://github.com/arplaboratory/learning-to-fly
视频介绍：https://youtu.be/NRD43ZA1D-4</title>
            <link>https://nitter.cz/xiaohuggg/status/1756915564692545660#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756915564692545660#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 Feb 2024 05:38:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>纽约大学的一个研究团队开发了一种新技术，能够在短短18秒内教会一架无人机如何稳定飞行。<br />
<br />
该程序可以在一台普通的MacBook Pro上运行。通过模拟飞行环境来训练无人机，让它学会如何保持空中悬停并按照指定路径飞行。<br />
<br />
只需18秒钟就能实现这一切。<br />
<br />
这种方法不仅限于简单的小型无人机——它几乎可以适用于任何无人机，包括更大、更昂贵的无人机，甚至是你自己从头开始建造的无人机。<br />
<br />
工作原理：<br />
<br />
1、端到端控制：通过深度强化学习（Deep RL）实现四旋翼无人机从感知到动作输出的直接映射，无需复杂的中间处理层，提高了控制策略的直接性和效率。<br />
<br />
2、不对称演员-评论家架构：采用了一种新颖的基于RL的训练框架，其中演员直接从状态到动作的映射进行决策，而评论家则利用额外的信息（如仿真中的精确状态）来评估动作的好坏，帮助演员更快学习。<br />
<br />
通过给予奖励和惩罚来教会模型执行某项任务。在这个项目中，无人机通过尝试不同的动作，根据其对任务成功率的影响获得反馈，从而学习如何飞行。<br />
<br />
3、高度优化的仿真器：开发了一个能在消费级笔记本电脑上模拟约5个月飞行时间每秒的高性能仿真器，这个仿真器使得无人机的训练过程极为快速。这种方法允许无人机在没有任何风险的情况下进行无数次尝试和错误，快速学习飞行技能。<br />
<br />
4、课程学习（Curriculum Learning）：为了提高训练效率，研究者采用了课程学习策略，即从简单任务开始逐步过渡到更复杂的任务。这种方法让无人机先学习基本的飞行控制，然后逐渐学习执行更复杂的飞行动作。<br />
<br />
5、调整奖励函数：训练过程中，研究者会调整奖励函数，即改变给予无人机的反馈，以鼓励它学习如何稳定飞行和执行特定的飞行路径。一开始，奖励机制较为宽松，随着训练的深入，会逐渐增加对飞行精确度和鲁棒性的要求。<br />
<br />
6、Sim2Real转移策略：通过精心设计的训练范式和仿真环境，确保了无人机控制策略可以平滑地从仿真环境转移到真实环境，克服了仿真与现实之间的差距。<br />
<br />
该项目利用高度优化的仿真器和有效的学习策略，实现了在仅18秒内完成无人机飞行控制策略的训练，大大减少了从理论到实践的时间。<br />
<br />
项目将代码和仿真器已经开源。<br />
<br />
论文：<a href="https://arxiv.org/abs/2311.13081">arxiv.org/abs/2311.13081</a><br />
GitHub：<a href="https://github.com/arplaboratory/learning-to-fly">github.com/arplaboratory/lea…</a><br />
视频介绍：<a href="https://youtu.be/NRD43ZA1D-4">youtu.be/NRD43ZA1D-4</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY5MTUxMjc4NTk5NzAwNDgvcHUvaW1nL2ZjNk1tUTBMWUNmYV83ZWwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756531196933419330#m</id>
            <title>1X's ：神经网络视觉端到端学习机器人

该机器人能够完全独立地执行任务，无需人类远程操控或通过预设脚本。

所有动作都是实时通过神经网络计算得出。

机器人基于视觉的端到端神经网络直接从图像中学习如何控制其动作，包括驾驶、操纵手臂和抓取器、控制躯干和头部等。

演示视频未加速剪辑...

通过训练，机器人能够理解和执行一系列广泛的物理行为，如清洁、整理、拾起物体以及与人类和其他机器人进行社交互动。

项目采用了一种策略，通过在几分钟内收集数据并在桌面GPU上进行训练，快速微调模型以适应特定的任务，从而使机器人能够迅速学习新的技能。

技术原理：

1、演示数据集构建：团队首先收集了一组包含30个EVE机器人执行各种任务的演示数据。这些数据非常多样，包括机器人进行清洁、整理家庭、拾起物体以及与人和其他机器人社交互动等物理行为的实例。

2、基础模型训练：使用这些演示数据，团队训练了一个“基础模型”。这个模型能够理解一系列广泛的物理行为，为机器人提供了对各种任务基本动作的理解基础。

3、模型微调：接下来，团队针对特定的任务类型对基础模型进行了微调，生成了多个专门的能力模型。例如，他们创建了专门用于门操作的模型和另一个专注于仓库任务的模型。

4、进一步微调以适应特定任务：团队进一步微调这些专门模型，使它们能够执行更具体的任务，比如打开一个特定的门。这种微调过程使模型能够对特定的行为或动作有更精确的理解和执行能力。

5、快速引入新技能：通过这种分层微调策略，团队可以在短时间内（仅几分钟的数据收集和在桌面GPU上的训练）快速地为机器人引入新的技能。

详细：https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed

演示视频中没有远程操控、没有计算机图形、没有剪辑、没有视频加速、没有脚本轨迹回放。一切都通过神经网络控制，完全自主，以1X速度进行。

机器人硬件信息：

1X的目标是设计出能够在任何场景中有效工作的通用安卓机器人，以应对现实世界的不可预测性。

EVE是1X技术公司开发的一款高级安卓机器人，旨在为商业行业提供智能的工作解决方案。EVE设计用于执行各种任务，从物流操作到保安巡逻等。这款机器人集安全性、平衡性和智能于一身，可以轻松融入现有的工作流程中，与团队无缝协作。

主要特点包括：

安全第一：每台EVE在部署前都会在真实世界场景中进行测试，其软性、仿生机械设计从内而外确保安全性，适合在各种空间中工作。

平衡性能：EVE能够处理重物，同时也足够柔和以处理易碎物品，无论是在仓库还是分发中心，都能轻松融入您的物流工作流程。

智能行为：EVE通过观察专家的动作进行学习，如移动设备、开门、完成订单等，然后通过其体现的学习模型重现动作并开始工作。随着时间的积累，EVE在基础知识上构建经验，使得未来任务（如“移动那个箱子”）变得更加简单。

EVE的规格：

高度：1.86米
重量：86公斤
最高速度：14.4公里/小时
携带能力：15公斤
运行时间：6小时

EVE通过人工智能自主操作，默认情况下能够导航工作空间，执行如开不同手柄的门、从远处识别人或物体、像人类一样穿越非结构化空间的任务。EVE在帮助人类操作者执行任务时，利用其力量、精确度和传感器执行如在办公楼巡逻和检查员工ID徽章的任务。EVE会关注潜在的危险或错误，并在操作者需要接管时报告。</title>
            <link>https://nitter.cz/xiaohuggg/status/1756531196933419330#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756531196933419330#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 04:10:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>1X's ：神经网络视觉端到端学习机器人<br />
<br />
该机器人能够完全独立地执行任务，无需人类远程操控或通过预设脚本。<br />
<br />
所有动作都是实时通过神经网络计算得出。<br />
<br />
机器人基于视觉的端到端神经网络直接从图像中学习如何控制其动作，包括驾驶、操纵手臂和抓取器、控制躯干和头部等。<br />
<br />
演示视频未加速剪辑...<br />
<br />
通过训练，机器人能够理解和执行一系列广泛的物理行为，如清洁、整理、拾起物体以及与人类和其他机器人进行社交互动。<br />
<br />
项目采用了一种策略，通过在几分钟内收集数据并在桌面GPU上进行训练，快速微调模型以适应特定的任务，从而使机器人能够迅速学习新的技能。<br />
<br />
技术原理：<br />
<br />
1、演示数据集构建：团队首先收集了一组包含30个EVE机器人执行各种任务的演示数据。这些数据非常多样，包括机器人进行清洁、整理家庭、拾起物体以及与人和其他机器人社交互动等物理行为的实例。<br />
<br />
2、基础模型训练：使用这些演示数据，团队训练了一个“基础模型”。这个模型能够理解一系列广泛的物理行为，为机器人提供了对各种任务基本动作的理解基础。<br />
<br />
3、模型微调：接下来，团队针对特定的任务类型对基础模型进行了微调，生成了多个专门的能力模型。例如，他们创建了专门用于门操作的模型和另一个专注于仓库任务的模型。<br />
<br />
4、进一步微调以适应特定任务：团队进一步微调这些专门模型，使它们能够执行更具体的任务，比如打开一个特定的门。这种微调过程使模型能够对特定的行为或动作有更精确的理解和执行能力。<br />
<br />
5、快速引入新技能：通过这种分层微调策略，团队可以在短时间内（仅几分钟的数据收集和在桌面GPU上的训练）快速地为机器人引入新的技能。<br />
<br />
详细：<a href="https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed">1x.tech/discover/all-neural-…</a><br />
<br />
演示视频中没有远程操控、没有计算机图形、没有剪辑、没有视频加速、没有脚本轨迹回放。一切都通过神经网络控制，完全自主，以1X速度进行。<br />
<br />
机器人硬件信息：<br />
<br />
1X的目标是设计出能够在任何场景中有效工作的通用安卓机器人，以应对现实世界的不可预测性。<br />
<br />
EVE是1X技术公司开发的一款高级安卓机器人，旨在为商业行业提供智能的工作解决方案。EVE设计用于执行各种任务，从物流操作到保安巡逻等。这款机器人集安全性、平衡性和智能于一身，可以轻松融入现有的工作流程中，与团队无缝协作。<br />
<br />
主要特点包括：<br />
<br />
安全第一：每台EVE在部署前都会在真实世界场景中进行测试，其软性、仿生机械设计从内而外确保安全性，适合在各种空间中工作。<br />
<br />
平衡性能：EVE能够处理重物，同时也足够柔和以处理易碎物品，无论是在仓库还是分发中心，都能轻松融入您的物流工作流程。<br />
<br />
智能行为：EVE通过观察专家的动作进行学习，如移动设备、开门、完成订单等，然后通过其体现的学习模型重现动作并开始工作。随着时间的积累，EVE在基础知识上构建经验，使得未来任务（如“移动那个箱子”）变得更加简单。<br />
<br />
EVE的规格：<br />
<br />
高度：1.86米<br />
重量：86公斤<br />
最高速度：14.4公里/小时<br />
携带能力：15公斤<br />
运行时间：6小时<br />
<br />
EVE通过人工智能自主操作，默认情况下能够导航工作空间，执行如开不同手柄的门、从远处识别人或物体、像人类一样穿越非结构化空间的任务。EVE在帮助人类操作者执行任务时，利用其力量、精确度和传感器执行如在办公楼巡逻和检查员工ID徽章的任务。EVE会关注潜在的危险或错误，并在操作者需要接管时报告。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY1MjE4ODk2NTExMzQ0NjQvcHUvaW1nL1o3aEZuSWdxQTJsdTFOTGIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756516368269312196#m</id>
            <title>Vision Arena：视觉模型竞技场

这个项目的目的是测试和比较不同的视觉语言模型（VLMs），比如GPT-4V、Gemini、Llava、Qwen-VL等。

用户可以在这个工具上同时测试两个视觉模型，并对它们进行投票，以决定哪个更优秀。

而且是盲测，选择你认为好的结果才会告诉你模型是什么。

测试地址：https://huggingface.co/spaces/WildVision/vision-arena</title>
            <link>https://nitter.cz/xiaohuggg/status/1756516368269312196#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756516368269312196#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 03:11:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vision Arena：视觉模型竞技场<br />
<br />
这个项目的目的是测试和比较不同的视觉语言模型（VLMs），比如GPT-4V、Gemini、Llava、Qwen-VL等。<br />
<br />
用户可以在这个工具上同时测试两个视觉模型，并对它们进行投票，以决定哪个更优秀。<br />
<br />
而且是盲测，选择你认为好的结果才会告诉你模型是什么。<br />
<br />
测试地址：<a href="https://huggingface.co/spaces/WildVision/vision-arena">huggingface.co/spaces/WildVi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTYzMjc1MzIzNTI0NDIzNjgvcHUvaW1nL2ZYWjBZdVVyWnBYVU1xN1cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756508424551227405#m</id>
            <title>Maybe：一个花了100万美金打造的个人财务的操作系统，由于没有运营成功，现在他们给开源了

用户可以用它来跟踪和管理自己的收入、支出、投资和财富。

同时它还包含一个“财务顾问”功能，使用户能够与真正的财务规划师或注册金融分析师联系，以获得专业的财务管理建议。

由于商业模式未能成功，这个项目在2023年中期关闭了。团队在2021/2022年期间投入了将近100万美元来构建这个应用，包括员工、合同工、数据提供商/服务和基础设施等成本。

现在，团队决定将Maybe作为一个完全开源的项目复兴，目的是让用户可以免费运行这个应用，管理自己的财务，并最终提供一个收费的托管版本。

GitHub：https://github.com/maybe-finance/maybe</title>
            <link>https://nitter.cz/xiaohuggg/status/1756508424551227405#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756508424551227405#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 02:40:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Maybe：一个花了100万美金打造的个人财务的操作系统，由于没有运营成功，现在他们给开源了<br />
<br />
用户可以用它来跟踪和管理自己的收入、支出、投资和财富。<br />
<br />
同时它还包含一个“财务顾问”功能，使用户能够与真正的财务规划师或注册金融分析师联系，以获得专业的财务管理建议。<br />
<br />
由于商业模式未能成功，这个项目在2023年中期关闭了。团队在2021/2022年期间投入了将近100万美元来构建这个应用，包括员工、合同工、数据提供商/服务和基础设施等成本。<br />
<br />
现在，团队决定将Maybe作为一个完全开源的项目复兴，目的是让用户可以免费运行这个应用，管理自己的财务，并最终提供一个收费的托管版本。<br />
<br />
GitHub：<a href="https://github.com/maybe-finance/maybe">github.com/maybe-finance/may…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YtMTJPamE4QUFfRWM2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756321049887854740#m</id>
            <title>微软在Windows 11 Insider Preview Build 26052中引入了Sudo for Windows新功能

该功能非常类似于macOS和Linux系统中终端里的sudo命令。

sudo命令允许用户执行需要管理员（root）权限的命令，而无需切换到root用户。用户只需在命令前加上sudo，然后输入自己的密码，就可以以更高权限运行该命令。

Sudo for Windows目前支持三种不同的配置选项：在新窗口中打开、输入关闭和内联。这些配置允许用户根据需要选择如何执行提权过程。

微软计划未来几个月内扩展Sudo for Windows的文档，并分享更多关于在“内联”配置下运行sudo的安全性细节。

此外，微软还宣布将这个项目开源在GitHub上。目前，他们正努力在GitHub仓库中添加更多关于该项目的信息，并将在未来几个月分享更多计划细节。

详细：https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/
GitHub：https://github.com/microsoft/sudo</title>
            <link>https://nitter.cz/xiaohuggg/status/1756321049887854740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756321049887854740#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 14:15:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软在Windows 11 Insider Preview Build 26052中引入了Sudo for Windows新功能<br />
<br />
该功能非常类似于macOS和Linux系统中终端里的sudo命令。<br />
<br />
sudo命令允许用户执行需要管理员（root）权限的命令，而无需切换到root用户。用户只需在命令前加上sudo，然后输入自己的密码，就可以以更高权限运行该命令。<br />
<br />
Sudo for Windows目前支持三种不同的配置选项：在新窗口中打开、输入关闭和内联。这些配置允许用户根据需要选择如何执行提权过程。<br />
<br />
微软计划未来几个月内扩展Sudo for Windows的文档，并分享更多关于在“内联”配置下运行sudo的安全性细节。<br />
<br />
此外，微软还宣布将这个项目开源在GitHub上。目前，他们正努力在GitHub仓库中添加更多关于该项目的信息，并将在未来几个月分享更多计划细节。<br />
<br />
详细：<a href="https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/">devblogs.microsoft.com/comma…</a><br />
GitHub：<a href="https://github.com/microsoft/sudo">github.com/microsoft/sudo</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0YteXVncWEwQUE4WWNGLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dGLXl1Z3FhMEFBOFljRi5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756316182343512348#m</id>
            <title>iMusic：基于IMU的面部表情捕捉

该项目使用了一种可以贴在脸上，叫做惯性测量单元（IMUs）的小型设备，来捕捉面部表情。

与依赖摄像头的方法不同，IMUs不需要拍摄视频，通过捕捉微小面部动作，来捕捉表情，所以更能保护个人隐私。

即使在脸部部分被遮挡的情况下，它也能有效工作。

MUSIC项目主要解决了两个问题：

在需要保护隐私的场合，如何捕捉面部表情而不依赖视频。

在脸部被部分遮挡的情况下，如何准确捕捉面部动作。

主要特点：

1、隐私保护：相比基于视频的面部捕捉方法，使用IMUs不需要捕捉用户的视觉图像，因此更能保护用户的隐私。

2、避免视线遮挡问题：由于不依赖视觉信息，即使用户的面部部分被遮挡，系统也能准确捕捉面部表情。

3、精确捕捉细微表情：IMUs能够检测细微的面部运动，帮助捕捉通常难以通过视觉方法识别的轻微表情变化。

4、灵活性和可移植性：IMUs小巧轻便，可以轻易地集成到多种设备中，使得iMusic项目的技术可以广泛应用于各种场合，包括但不限于娱乐、健康监测和人机交互等领域。

iMusic的工作原理：

1、硬件设计：微型IMUs，使其能够适合贴合面部并准确捕捉面部运动。这些IMUs被放置在面部的关键位置，以监测表情变化时的动态信息。

2、数据采集与校准：通过IMUs收集面部运动的原始数据，并进行必要的校准处理，以确保数据的准确性和一致性。

3、IMU-ARKit数据集：创建一个包含丰富的IMU和视觉信号配对的数据集，用于训练和验证面部表情捕捉模型。这个数据集覆盖了广泛的面部表情和表演，为后续的分析和模型训练提供了基础。

4、面部表情预测模型：利用特别设计的Transformer扩散模型，从IMUs收集的信号中预测面部的混合形状参数。这个模型通过两阶段训练策略来优化，使其能够准确地从IMU数据中解析出复杂的面部表情。

项目及演示：https://sites.google.com/view/projectpage-imusic
论文：https://arxiv.org/abs/2402.03944
视频：https://youtu.be/rPusR6b43ng</title>
            <link>https://nitter.cz/xiaohuggg/status/1756316182343512348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756316182343512348#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 13:56:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>iMusic：基于IMU的面部表情捕捉<br />
<br />
该项目使用了一种可以贴在脸上，叫做惯性测量单元（IMUs）的小型设备，来捕捉面部表情。<br />
<br />
与依赖摄像头的方法不同，IMUs不需要拍摄视频，通过捕捉微小面部动作，来捕捉表情，所以更能保护个人隐私。<br />
<br />
即使在脸部部分被遮挡的情况下，它也能有效工作。<br />
<br />
MUSIC项目主要解决了两个问题：<br />
<br />
在需要保护隐私的场合，如何捕捉面部表情而不依赖视频。<br />
<br />
在脸部被部分遮挡的情况下，如何准确捕捉面部动作。<br />
<br />
主要特点：<br />
<br />
1、隐私保护：相比基于视频的面部捕捉方法，使用IMUs不需要捕捉用户的视觉图像，因此更能保护用户的隐私。<br />
<br />
2、避免视线遮挡问题：由于不依赖视觉信息，即使用户的面部部分被遮挡，系统也能准确捕捉面部表情。<br />
<br />
3、精确捕捉细微表情：IMUs能够检测细微的面部运动，帮助捕捉通常难以通过视觉方法识别的轻微表情变化。<br />
<br />
4、灵活性和可移植性：IMUs小巧轻便，可以轻易地集成到多种设备中，使得iMusic项目的技术可以广泛应用于各种场合，包括但不限于娱乐、健康监测和人机交互等领域。<br />
<br />
iMusic的工作原理：<br />
<br />
1、硬件设计：微型IMUs，使其能够适合贴合面部并准确捕捉面部运动。这些IMUs被放置在面部的关键位置，以监测表情变化时的动态信息。<br />
<br />
2、数据采集与校准：通过IMUs收集面部运动的原始数据，并进行必要的校准处理，以确保数据的准确性和一致性。<br />
<br />
3、IMU-ARKit数据集：创建一个包含丰富的IMU和视觉信号配对的数据集，用于训练和验证面部表情捕捉模型。这个数据集覆盖了广泛的面部表情和表演，为后续的分析和模型训练提供了基础。<br />
<br />
4、面部表情预测模型：利用特别设计的Transformer扩散模型，从IMUs收集的信号中预测面部的混合形状参数。这个模型通过两阶段训练策略来优化，使其能够准确地从IMU数据中解析出复杂的面部表情。<br />
<br />
项目及演示：<a href="https://sites.google.com/view/projectpage-imusic">sites.google.com/view/projec…</a><br />
论文：<a href="https://arxiv.org/abs/2402.03944">arxiv.org/abs/2402.03944</a><br />
视频：<a href="https://youtu.be/rPusR6b43ng">youtu.be/rPusR6b43ng</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTYzMTEwMzg4ODExMjAyNTYvcHUvaW1nL011VnB1WlJQUV9ZajZ2OUMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756313854437732694#m</id>
            <title>OpenAI推出了一个基于Whisper模型的音频到文本的API，可以将任何音频直接转录成文本并翻译为英文。

同时在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。

主要功能：

1、音频转文字：将音频文件中的语音内容自动转换成文本形式，让用户可以读到音频里说了什么。

2、支持多种语言的翻译转录：如果音频中的语言不是英语，这个API还能先将其翻译成英语，然后再进行转录，使非英语内容也能轻松转换成文本。

3、提供时间戳：OpenAI的Whisper API提供了一个参数timestamp_granularities[]，允许用户获取带有时间戳的更结构化的JSON输出格式。这意味着，在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。

4、支持多种音频格式：支持上传25MB以内的文件，包括mp3、mp4、mpeg、mpga、m4a、wav和webm等格式。用户无需转换文件格式即可直接使用。

详细：https://platform.openai.com/docs/guides/speech-to-text</title>
            <link>https://nitter.cz/xiaohuggg/status/1756313854437732694#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756313854437732694#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 13:47:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI推出了一个基于Whisper模型的音频到文本的API，可以将任何音频直接转录成文本并翻译为英文。<br />
<br />
同时在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。<br />
<br />
主要功能：<br />
<br />
1、音频转文字：将音频文件中的语音内容自动转换成文本形式，让用户可以读到音频里说了什么。<br />
<br />
2、支持多种语言的翻译转录：如果音频中的语言不是英语，这个API还能先将其翻译成英语，然后再进行转录，使非英语内容也能轻松转换成文本。<br />
<br />
3、提供时间戳：OpenAI的Whisper API提供了一个参数timestamp_granularities[]，允许用户获取带有时间戳的更结构化的JSON输出格式。这意味着，在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。<br />
<br />
4、支持多种音频格式：支持上传25MB以内的文件，包括mp3、mp4、mpeg、mpga、m4a、wav和webm等格式。用户无需转换文件格式即可直接使用。<br />
<br />
详细：<a href="https://platform.openai.com/docs/guides/speech-to-text">platform.openai.com/docs/gui…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YtdUZyNmJjQUFoU0E0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1755487926127820830#m</id>
            <title>RT by @xiaohuggg: 升级了一下科技文章翻译GPT，增加了一个API可以将URL转成Markdown，现在只需要输入URL就可以翻译网页内容

https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi</title>
            <link>https://nitter.cz/dotey/status/1755487926127820830#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1755487926127820830#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 07:05:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>升级了一下科技文章翻译GPT，增加了一个API可以将URL转成Markdown，现在只需要输入URL就可以翻译网页内容<br />
<br />
<a href="https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi">chat.openai.com/g/g-uBhKUJJT…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5Mllkd1djQUFPdHRFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5M2NLYVhZQUFmMjQwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5LTFnVld3QUVKVFU3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756154223065251911#m</id>
            <title>马斯克：将用「X」取代手机电话

马斯克称将在几个月后注销自己的电话号码。转而使用「X」进行信息收发和通话🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1756154223065251911#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756154223065251911#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 03:12:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克：将用「X」取代手机电话<br />
<br />
马斯克称将在几个月后注销自己的电话号码。转而使用「X」进行信息收发和通话🤔</p>
<p><a href="https://nitter.cz/elonmusk/status/1755870691159626094#m">nitter.cz/elonmusk/status/1755870691159626094#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</id>
            <title>Happy Chinese New Year！

Every One…

🐲🧨🇨🇳</title>
            <link>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755985073747447827#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 16:00:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Happy Chinese New Year！<br />
<br />
Every One…<br />
<br />
🐲🧨🇨🇳</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y2REhLRGJNQUFxaHNsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>