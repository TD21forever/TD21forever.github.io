<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734746436544413792#m</id>
            <title>Chanel 1 宣布他们将推出一个全新的网络新闻节目

这个节目的所有内容全部由AI制作

下面这个22分钟的新闻演示视频，所有主播、画面、声音、内容都通过AI制作完成🙃</title>
            <link>https://nitter.cz/xiaohuggg/status/1734746436544413792#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734746436544413792#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:25:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Chanel 1 宣布他们将推出一个全新的网络新闻节目<br />
<br />
这个节目的所有内容全部由AI制作<br />
<br />
下面这个22分钟的新闻演示视频，所有主播、画面、声音、内容都通过AI制作完成🙃</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM0NTg3OTkzOTEzMDE2MzIwL2ltZy9jMUpWMUNoVlVsaGx4MXA5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734594915886330236#m</id>
            <title>有什么办法能让上传到X上面的视频

在720P的情况下也很清晰呢

求教下

实在受不了X这个模糊的视频了😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1734594915886330236#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734594915886330236#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 15:23:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有什么办法能让上传到X上面的视频<br />
<br />
在720P的情况下也很清晰呢<br />
<br />
求教下<br />
<br />
实在受不了X这个模糊的视频了😐</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734562244422504844#m</id>
            <title>老铁们 发现一个开源的界面非常漂亮的聊天机器人框架： Lobe Chat

支持TTS语音合成、GPT 4V多模态交互和可扩展的函数调用插件系统，可以联网、画图、爬虫等。

支持一键部署，可在1分钟内完成部署（亲测确实很快🙂），无需复杂的配置过程。一键搭建私人 ChatGPT/LLM 网页应用程序。

主要功能特点：

- 多模态支持：支持最新的GPT 4V模型，具备视觉识别能力。

- 语音会话：支持文字转语音（TTS）和语音转文字（STT）技术，提供清晰的语音输出

- 插件系统：Function Calling插件生态允许实时信息获取和处理，如自动获取最新新闻头条。

- 助手市场：内置多种精心设计的AI助手，用户可以贡献和分享个人开发的助手。

- 采用 PWA 技术：提供接近原生应用体验的网页应用。支持桌面和移动设备，提供优化的用户体验。

- 移动设备适配：针对移动设备进行了优化设计，提升移动体验。正在进行版本迭代，以实现更流畅和直观的交互。

-快速部署：使用 Vercel 平台或者 Docker 镜像，一键部署，可在 1 分钟内完成部署，无需复杂的配置过程。

- 更多特性：包括精致的 UI 设计、流畅的对话体验、数据本地化隐私安全和自定义域名等。

在线体验：https://chat-preview.lobehub.com/welcome
GitHub：https://github.com/lobehub/lobe-chat</title>
            <link>https://nitter.cz/xiaohuggg/status/1734562244422504844#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734562244422504844#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 13:14:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>老铁们 发现一个开源的界面非常漂亮的聊天机器人框架： Lobe Chat<br />
<br />
支持TTS语音合成、GPT 4V多模态交互和可扩展的函数调用插件系统，可以联网、画图、爬虫等。<br />
<br />
支持一键部署，可在1分钟内完成部署（亲测确实很快🙂），无需复杂的配置过程。一键搭建私人 ChatGPT/LLM 网页应用程序。<br />
<br />
主要功能特点：<br />
<br />
- 多模态支持：支持最新的GPT 4V模型，具备视觉识别能力。<br />
<br />
- 语音会话：支持文字转语音（TTS）和语音转文字（STT）技术，提供清晰的语音输出<br />
<br />
- 插件系统：Function Calling插件生态允许实时信息获取和处理，如自动获取最新新闻头条。<br />
<br />
- 助手市场：内置多种精心设计的AI助手，用户可以贡献和分享个人开发的助手。<br />
<br />
- 采用 PWA 技术：提供接近原生应用体验的网页应用。支持桌面和移动设备，提供优化的用户体验。<br />
<br />
- 移动设备适配：针对移动设备进行了优化设计，提升移动体验。正在进行版本迭代，以实现更流畅和直观的交互。<br />
<br />
-快速部署：使用 Vercel 平台或者 Docker 镜像，一键部署，可在 1 分钟内完成部署，无需复杂的配置过程。<br />
<br />
- 更多特性：包括精致的 UI 设计、流畅的对话体验、数据本地化隐私安全和自定义域名等。<br />
<br />
在线体验：<a href="https://chat-preview.lobehub.com/welcome">chat-preview.lobehub.com/wel…</a><br />
GitHub：<a href="https://github.com/lobehub/lobe-chat">github.com/lobehub/lobe-chat</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ1NTc1MzMzNzE4NzEyMzIvcHUvaW1nL09qSmpoOXdURzZKTkpPbE4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734500452480549005#m</id>
            <title>R to @xiaohuggg: 有的真还是笑喷了....</title>
            <link>https://nitter.cz/xiaohuggg/status/1734500452480549005#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734500452480549005#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:08:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有的真还是笑喷了....</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJdWhkLWFnQUE3WC1aLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJdWpUX2JzQUFObEcyLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJdXFvTGJFQUE2bWFQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734499784399142949#m</id>
            <title>R to @xiaohuggg: 支持中文、日文、英文

确实挺搞笑的，你们看看

吐槽小能手 哈哈哈哈</title>
            <link>https://nitter.cz/xiaohuggg/status/1734499784399142949#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734499784399142949#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:05:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>支持中文、日文、英文<br />
<br />
确实挺搞笑的，你们看看<br />
<br />
吐槽小能手 哈哈哈哈</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JJdU1WUmJjQUFLeWZQLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734499555654377759#m</id>
            <title>这个项目挺搞笑的 哈哈哈 你们看看😂

CLoT：训练LLM成为吐槽能手

用日本传统喜剧游戏“大喜利”（Oogiri）作为测试，挑战AI以吐槽高手的方式回应信息。

游戏中的挑战，AI需要理解给定图文信息来产生幽默搞笑的回答。

Oogiri 是一种需要参与者对给定的图像文做出意想不到且幽默的回应的创意游戏。

测试包括图像到文本（I2T）、文本到文本（T2T）和图像&amp;文本到文本（IT2T）

具体方法：

建立数据集：研究人员构建了一个多模态、多语言的 Oogiri-GO 数据集，包含超过 130000 个样本。

训练 AI：通过特殊的训练方法，让 AI 学会如何在游戏中给出创意和幽默的回答。

CLoT 首先将 Oogiri-GO 数据集转化为 LoT 导向的指令调整数据，以训练预训练的 LLM 达到一定的 LoT 幽默生成和辨别能力。

然后，CLoT 设计了一个探索性自我完善过程，鼓励 LLM 通过探索看似无关概念之间的平行关系来生成更多创造性的 LoT 数据，并选择高质量数据进行自我完善。

实验结果：

实验结果显示，CLoT 能够显著提高 LLM（如 Qwen 和 CogVLM）在多种 Oogiri 游戏类型中的表现。具体来说，CLoT 帮助 LLM 生成了更好的幽默内容。

量化性能提升：与原始和 CoT 集成的 LLM 相比，CLoT 集成的 LLM 在 Oogiri 游戏的多项选择和排名问题中取得了更高的性能。

创造性能力的提升：CLoT 还在其他任务（如“云猜测游戏”和“发散性联想任务”）中提高了创造性能力，显示出其卓越的泛化能力。

项目及演示：https://zhongshsh.github.io/CLoT/
论文：https://arxiv.org/abs/2312.02439
GitHub：https://github.com/sail-sg/CLoT</title>
            <link>https://nitter.cz/xiaohuggg/status/1734499555654377759#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734499555654377759#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 09:04:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个项目挺搞笑的 哈哈哈 你们看看😂<br />
<br />
CLoT：训练LLM成为吐槽能手<br />
<br />
用日本传统喜剧游戏“大喜利”（Oogiri）作为测试，挑战AI以吐槽高手的方式回应信息。<br />
<br />
游戏中的挑战，AI需要理解给定图文信息来产生幽默搞笑的回答。<br />
<br />
Oogiri 是一种需要参与者对给定的图像文做出意想不到且幽默的回应的创意游戏。<br />
<br />
测试包括图像到文本（I2T）、文本到文本（T2T）和图像&文本到文本（IT2T）<br />
<br />
具体方法：<br />
<br />
建立数据集：研究人员构建了一个多模态、多语言的 Oogiri-GO 数据集，包含超过 130000 个样本。<br />
<br />
训练 AI：通过特殊的训练方法，让 AI 学会如何在游戏中给出创意和幽默的回答。<br />
<br />
CLoT 首先将 Oogiri-GO 数据集转化为 LoT 导向的指令调整数据，以训练预训练的 LLM 达到一定的 LoT 幽默生成和辨别能力。<br />
<br />
然后，CLoT 设计了一个探索性自我完善过程，鼓励 LLM 通过探索看似无关概念之间的平行关系来生成更多创造性的 LoT 数据，并选择高质量数据进行自我完善。<br />
<br />
实验结果：<br />
<br />
实验结果显示，CLoT 能够显著提高 LLM（如 Qwen 和 CogVLM）在多种 Oogiri 游戏类型中的表现。具体来说，CLoT 帮助 LLM 生成了更好的幽默内容。<br />
<br />
量化性能提升：与原始和 CoT 集成的 LLM 相比，CLoT 集成的 LLM 在 Oogiri 游戏的多项选择和排名问题中取得了更高的性能。<br />
<br />
创造性能力的提升：CLoT 还在其他任务（如“云猜测游戏”和“发散性联想任务”）中提高了创造性能力，显示出其卓越的泛化能力。<br />
<br />
项目及演示：<a href="https://zhongshsh.github.io/CLoT/">zhongshsh.github.io/CLoT/</a><br />
论文：<a href="https://arxiv.org/abs/2312.02439">arxiv.org/abs/2312.02439</a><br />
GitHub：<a href="https://github.com/sail-sg/CLoT">github.com/sail-sg/CLoT</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTgyODU0MTM2OTEzOTIvcHUvaW1nL3pVYWk0YTlWeWRBbFFFSHcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734492844545683636#m</id>
            <title>Alter3：首款由GPT 4驱动的人形机器人

东京大学开发，这个机器人特别之处在于，它使用 GPT-4 来生成机器人的动作。

机器人能够自主地做出各种人类动作，如自拍姿势或模仿鬼的动作，无需事先编程。

而且只需通过口头反馈即可调整姿势，无需进行微调。

Alter3还能够表达诸如尴尬和快乐等情感。

项目的核心是将 GPT-4 生成的语言描述映射到机器人的身体动作上。这意味着机器人可以根据从 GPT-4 接收到的指令来执行动作。

程序代码转换：将人类动作的语言表达转换为机器人可以理解和执行的程序代码。

特点和功能：

1、自发运动：Alter3 能够采取各种姿势，如自拍姿势或“假装成鬼”的姿势，并且能够随时间生成一系列动作，而无需为每个身体部位编写程序。

2、零次学习能力：Alter3 展示了零次学习的能力，即在没有特定训练的情况下执行任务。

3、语言反馈调整：通过口头反馈可以调整姿势，无需进行微调。

4、运动生成：GPT-4 能够映射语言表达的运动到 Alter3 的身体上，包括模仿非人类运动（如鬼魂或蛇）。

5、情感表达：Alter3 通过 GPT-4 能够表达诸如尴尬和快乐等情感，并能从文本中推断出适当的情感并在物理反应中体现出来。

项目及演示：https://tnoinkwms.github.io/ALTER-LLM/
论文：https://arxiv.org/abs/2312.06571</title>
            <link>https://nitter.cz/xiaohuggg/status/1734492844545683636#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734492844545683636#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 08:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Alter3：首款由GPT 4驱动的人形机器人<br />
<br />
东京大学开发，这个机器人特别之处在于，它使用 GPT-4 来生成机器人的动作。<br />
<br />
机器人能够自主地做出各种人类动作，如自拍姿势或模仿鬼的动作，无需事先编程。<br />
<br />
而且只需通过口头反馈即可调整姿势，无需进行微调。<br />
<br />
Alter3还能够表达诸如尴尬和快乐等情感。<br />
<br />
项目的核心是将 GPT-4 生成的语言描述映射到机器人的身体动作上。这意味着机器人可以根据从 GPT-4 接收到的指令来执行动作。<br />
<br />
程序代码转换：将人类动作的语言表达转换为机器人可以理解和执行的程序代码。<br />
<br />
特点和功能：<br />
<br />
1、自发运动：Alter3 能够采取各种姿势，如自拍姿势或“假装成鬼”的姿势，并且能够随时间生成一系列动作，而无需为每个身体部位编写程序。<br />
<br />
2、零次学习能力：Alter3 展示了零次学习的能力，即在没有特定训练的情况下执行任务。<br />
<br />
3、语言反馈调整：通过口头反馈可以调整姿势，无需进行微调。<br />
<br />
4、运动生成：GPT-4 能够映射语言表达的运动到 Alter3 的身体上，包括模仿非人类运动（如鬼魂或蛇）。<br />
<br />
5、情感表达：Alter3 通过 GPT-4 能够表达诸如尴尬和快乐等情感，并能从文本中推断出适当的情感并在物理反应中体现出来。<br />
<br />
项目及演示：<a href="https://tnoinkwms.github.io/ALTER-LLM/">tnoinkwms.github.io/ALTER-LL…</a><br />
论文：<a href="https://arxiv.org/abs/2312.06571">arxiv.org/abs/2312.06571</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0OTIzOTA0NjYyMzY0MTYvcHUvaW1nL19RaHZOREZSOVhYbmVVUlYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734452696290414674#m</id>
            <title>R to @xiaohuggg: 通过一张人脸照片+动作序列

可以使用文字描述指定生成各种场景、各种服装...

但是都是同一个脸的跳舞视频！🫰

我有一个大胆的想法，不知道你们有没有？哈哈哈😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1734452696290414674#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734452696290414674#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:58:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>通过一张人脸照片+动作序列<br />
<br />
可以使用文字描述指定生成各种场景、各种服装...<br />
<br />
但是都是同一个脸的跳舞视频！🫰<br />
<br />
我有一个大胆的想法，不知道你们有没有？哈哈哈😂</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0NTE3NzI4NTU2NDQxNjAvcHUvaW1nL2pIVEt4MEZWWlYyS2EwbW8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734449213168353715#m</id>
            <title>我们常说的“你的眼睛会说话 ” 不再是一句比喻了

它 真 的 会说话🤣

杜克大学的科学家们发现，当人的眼睛在四处张望时，耳朵会产生微小的声音。

通过监听这些声音，可以准确地判断你眼睛在看什么👁

研究团队通过实验发现，不同方向的眼睛运动会产生不同的耳朵声音特征。

研究方法和结果：

实验设计：研究团队邀请了16名视力和听力正常的成年人参加实验。

参与者被要求注视电脑屏幕上的一个静态绿点，然后在绿点消失并在不同位置重新出现时（上、下、左、右或对角线方向），用眼睛跟踪这个点，而不是转动头部。

数据收集：在此过程中，研究团队使用嵌入麦克风的耳塞捕捉耳朵产生的声音，并使用眼动追踪器记录参与者的瞳孔移动。

声音分析：通过分析耳朵的声音，研究团队能够识别出不同方向眼动的独特声音特征，并据此推断出参与者的视线方向。

这项研究由心理学和神经生物学教授Jennifer Groh博士领导。

Groh博士认为，这些声音可能是由于眼睛运动刺激大脑收缩中耳肌肉（通常帮助减弱响亮声音）或毛细胞（帮助放大安静声音）时产生的。

这可能是大脑匹配视觉和听觉位置的系统的一部分，即使在头部和耳朵不动的情况下，眼睛也能移动。

详细：https://today.duke.edu/2023/11/your-eyes-talk-your-ears-scientists-know-what-theyre-saying
论文：https://www.pnas.org/doi/10.1073/pnas.2303562120</title>
            <link>https://nitter.cz/xiaohuggg/status/1734449213168353715#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734449213168353715#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:44:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我们常说的“你的眼睛会说话 ” 不再是一句比喻了<br />
<br />
它 真 的 会说话🤣<br />
<br />
杜克大学的科学家们发现，当人的眼睛在四处张望时，耳朵会产生微小的声音。<br />
<br />
通过监听这些声音，可以准确地判断你眼睛在看什么👁<br />
<br />
研究团队通过实验发现，不同方向的眼睛运动会产生不同的耳朵声音特征。<br />
<br />
研究方法和结果：<br />
<br />
实验设计：研究团队邀请了16名视力和听力正常的成年人参加实验。<br />
<br />
参与者被要求注视电脑屏幕上的一个静态绿点，然后在绿点消失并在不同位置重新出现时（上、下、左、右或对角线方向），用眼睛跟踪这个点，而不是转动头部。<br />
<br />
数据收集：在此过程中，研究团队使用嵌入麦克风的耳塞捕捉耳朵产生的声音，并使用眼动追踪器记录参与者的瞳孔移动。<br />
<br />
声音分析：通过分析耳朵的声音，研究团队能够识别出不同方向眼动的独特声音特征，并据此推断出参与者的视线方向。<br />
<br />
这项研究由心理学和神经生物学教授Jennifer Groh博士领导。<br />
<br />
Groh博士认为，这些声音可能是由于眼睛运动刺激大脑收缩中耳肌肉（通常帮助减弱响亮声音）或毛细胞（帮助放大安静声音）时产生的。<br />
<br />
这可能是大脑匹配视觉和听觉位置的系统的一部分，即使在头部和耳朵不动的情况下，眼睛也能移动。<br />
<br />
详细：<a href="https://today.duke.edu/2023/11/your-eyes-talk-your-ears-scientists-know-what-theyre-saying">today.duke.edu/2023/11/your-…</a><br />
论文：<a href="https://www.pnas.org/doi/10.1073/pnas.2303562120">pnas.org/doi/10.1073/pnas.23…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0NDc1MTQ1NDIzNTg1MjgvcHUvaW1nL0Vfa1ZKMHk1LW5EQWF5X3AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734444651451703690#m</id>
            <title>R to @xiaohuggg: 把视频翻译成中文了

视频还是很不错的，可以看看

讲了目前大语言模型是什么、工作原理，以及描绘了通用世界模型到底是什么样的....</title>
            <link>https://nitter.cz/xiaohuggg/status/1734444651451703690#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734444651451703690#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 05:26:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>把视频翻译成中文了<br />
<br />
视频还是很不错的，可以看看<br />
<br />
讲了目前大语言模型是什么、工作原理，以及描绘了通用世界模型到底是什么样的....</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0NDE1NjAxMTk2MzE4NzIvcHUvaW1nL3pZSjNDSl95d1BPOVloZlYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734425980968861984#m</id>
            <title>Digital Life Project：能够在数字环境中模拟生活的自主3D角色

该项目通过AI技术和动作合成技术，创造出能够进行社交互动并通过身体动作表达自己的虚拟角色。

这些角色不仅能进行对话，还能通过身体动作来表达情感和反应，就像人类一样。

而且能够根据所处的不同社交环境自动调整它们的行为和反应。

Digital Life Project的核心有两部分组成：

SocioMind：这是一个模拟人类心理过程的数字大脑。它能够根据少量示例来建立角色的人格，并且能够根据心理学原理进行思考和反思。这个大脑还能够自主地发起对话主题。

MoMat-MoGen：这是一个用于控制角色身体动作的系统。它结合了两种技术：一种是运动匹配，用于确保动作的质量；另一种是运动生成，用于创造多样化的动作。

通过这些Digital Life Project能够做到以下几点：

1、创建具有社会智能的3D角色：这些虚拟角色不仅能进行对话，还能通过身体动作来表达情感和反应，就像真实的人类一样。

2、模拟人类社交互动：这些角色能够在不同的社交场景中展现出适当的行为和反应，例如在交谈、共享兴趣或其他社交活动中。

3、个性化和自主性：通过SocioMind组件，每个角色都有自己的“人格”，能够根据不同的情境和经历发展和改变，表现出独特的社交行为。

4、高质量和多样化的动作：利用MoMat-MoGen技术，这些角色能够产生既自然又多样化的身体动作，增强了虚拟交互的真实感。

5、与人类互动：项目还包括一个运动字幕模块，使得虚拟角色能够识别并适当响应人类玩家的动作，实现人与虚拟角色之间的交互。

工作原理：

1. SocioMind（社会心智）

SocioMind是一个模拟人类心理过程的数字大脑。它的工作原理如下：

人格建模：利用少量示例（few-shot exemplars）来形成角色的人格。这意味着通过分析少量的数据，SocioMind能够快速学习并模拟一个角色的性格特点。

心理学原理：它结合了社会认知心理学理论，帮助角色进行思考和反思，使其行为更加符合人类的心理和社会行为模式。

自主对话：SocioMind能够使角色自主地发起和参与对话，这些对话内容和主题是根据角色的“人格”和之前的交互历史来确定的。

2. MoMat-MoGen（运动匹配与运动生成）

MoMat-MoGen是用于控制角色身体动作的系统，它结合了两种技术：运动匹配和运动生成。

运动匹配：这部分从数据库中检索高质量的运动片段，以确保角色动作的自然性和准确性。它根据角色的当前姿态和目标轨迹来选择最合适的动作。

运动生成：这部分负责创造新的动作，以适应不同的情境和剧情。它能够根据文本输入（如指令或描述）生成多样化的动作，同时保持角色之间的互动关系。

3、大语言模型代理（LLM代理）：

使用大语言模型来驱动角色的对话和决策过程。
这些模型可以生成连贯、有意义的文本，使得虚拟角色能够以自然的方式进行对话和交流。

4、综合应用

当这两个系统结合时，就能创造出能够进行复杂社交互动的3D虚拟角色。角色的行为不仅仅是预设的动作，而是根据其“人格”、之前的经历和当前的社交环境来动态生成的。这使得每个角色都有独特的社交特征和行为模式，能够在虚拟环境中模拟真实的人类社交互动。

这种技术的应用前景非常广泛，可以用于视频游戏、虚拟现实体验、在线教育、交互式娱乐等领域，为用户提供更加丰富和真实的交互体验。

作者：@liuziwei7
南洋理工大学S-Lab、商汤科技、上海人工智能实验室
项目及演示：https://digital-life-project.com/
论文：https://arxiv.org/abs/2312.04547

* 演示视频中的所有故事、互动对话、被动/主动身体动作均由人工智能生成</title>
            <link>https://nitter.cz/xiaohuggg/status/1734425980968861984#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734425980968861984#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 04:12:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Digital Life Project：能够在数字环境中模拟生活的自主3D角色<br />
<br />
该项目通过AI技术和动作合成技术，创造出能够进行社交互动并通过身体动作表达自己的虚拟角色。<br />
<br />
这些角色不仅能进行对话，还能通过身体动作来表达情感和反应，就像人类一样。<br />
<br />
而且能够根据所处的不同社交环境自动调整它们的行为和反应。<br />
<br />
Digital Life Project的核心有两部分组成：<br />
<br />
SocioMind：这是一个模拟人类心理过程的数字大脑。它能够根据少量示例来建立角色的人格，并且能够根据心理学原理进行思考和反思。这个大脑还能够自主地发起对话主题。<br />
<br />
MoMat-MoGen：这是一个用于控制角色身体动作的系统。它结合了两种技术：一种是运动匹配，用于确保动作的质量；另一种是运动生成，用于创造多样化的动作。<br />
<br />
通过这些Digital Life Project能够做到以下几点：<br />
<br />
1、创建具有社会智能的3D角色：这些虚拟角色不仅能进行对话，还能通过身体动作来表达情感和反应，就像真实的人类一样。<br />
<br />
2、模拟人类社交互动：这些角色能够在不同的社交场景中展现出适当的行为和反应，例如在交谈、共享兴趣或其他社交活动中。<br />
<br />
3、个性化和自主性：通过SocioMind组件，每个角色都有自己的“人格”，能够根据不同的情境和经历发展和改变，表现出独特的社交行为。<br />
<br />
4、高质量和多样化的动作：利用MoMat-MoGen技术，这些角色能够产生既自然又多样化的身体动作，增强了虚拟交互的真实感。<br />
<br />
5、与人类互动：项目还包括一个运动字幕模块，使得虚拟角色能够识别并适当响应人类玩家的动作，实现人与虚拟角色之间的交互。<br />
<br />
工作原理：<br />
<br />
1. SocioMind（社会心智）<br />
<br />
SocioMind是一个模拟人类心理过程的数字大脑。它的工作原理如下：<br />
<br />
人格建模：利用少量示例（few-shot exemplars）来形成角色的人格。这意味着通过分析少量的数据，SocioMind能够快速学习并模拟一个角色的性格特点。<br />
<br />
心理学原理：它结合了社会认知心理学理论，帮助角色进行思考和反思，使其行为更加符合人类的心理和社会行为模式。<br />
<br />
自主对话：SocioMind能够使角色自主地发起和参与对话，这些对话内容和主题是根据角色的“人格”和之前的交互历史来确定的。<br />
<br />
2. MoMat-MoGen（运动匹配与运动生成）<br />
<br />
MoMat-MoGen是用于控制角色身体动作的系统，它结合了两种技术：运动匹配和运动生成。<br />
<br />
运动匹配：这部分从数据库中检索高质量的运动片段，以确保角色动作的自然性和准确性。它根据角色的当前姿态和目标轨迹来选择最合适的动作。<br />
<br />
运动生成：这部分负责创造新的动作，以适应不同的情境和剧情。它能够根据文本输入（如指令或描述）生成多样化的动作，同时保持角色之间的互动关系。<br />
<br />
3、大语言模型代理（LLM代理）：<br />
<br />
使用大语言模型来驱动角色的对话和决策过程。<br />
这些模型可以生成连贯、有意义的文本，使得虚拟角色能够以自然的方式进行对话和交流。<br />
<br />
4、综合应用<br />
<br />
当这两个系统结合时，就能创造出能够进行复杂社交互动的3D虚拟角色。角色的行为不仅仅是预设的动作，而是根据其“人格”、之前的经历和当前的社交环境来动态生成的。这使得每个角色都有独特的社交特征和行为模式，能够在虚拟环境中模拟真实的人类社交互动。<br />
<br />
这种技术的应用前景非常广泛，可以用于视频游戏、虚拟现实体验、在线教育、交互式娱乐等领域，为用户提供更加丰富和真实的交互体验。<br />
<br />
作者：<a href="https://nitter.cz/liuziwei7" title="Ziwei Liu">@liuziwei7</a><br />
南洋理工大学S-Lab、商汤科技、上海人工智能实验室<br />
项目及演示：<a href="https://digital-life-project.com/">digital-life-project.com/</a><br />
论文：<a href="https://arxiv.org/abs/2312.04547">arxiv.org/abs/2312.04547</a><br />
<br />
* 演示视频中的所有故事、互动对话、被动/主动身体动作均由人工智能生成</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0MjI5MjM5ODgwODI2ODgvcHUvaW1nL05WZ0JVdjhLNm9jOW9nTVouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734409880205992322#m</id>
            <title>3DiffTection：通过单张图片进行 3D 物体检测

它能够仅靠单张图片，在复杂的视觉环境中，准确地识别和理解三维空间中的物体和其在空间中的位置和方向排列。

从而实现更准确和深入的物体检测和场景理解。

即时物体被遮挡，或者物体出现在不常见的角度和位置也能识别出来。

该项目由英伟达多伦多人工智能实验室以及多所大学联合开发。

3DiffTection 主要特点：

从图片识别物体：它能从一张普通的照片中识别出各种物体，并了解这些物体在真实世界中的位置和方向。

创建不同视角的图像：即使只有一张图片，3DiffTection 也能创造出从不同角度看这个场景的新图片。

连接不同视角的相同点：它能识别出在不同图片中相同的地方或物体，即使这些图片是从不同的角度拍摄的。

3DiffTection 的工作原理：

1、3D 感知扩散模型:：首先，3DiffTection 使用它的扩散模型来分析这张照片。这个模型经过特殊训练，能够理解物体在三维空间中的排列和方向，即使在复杂的视觉环境中。

2、几何和语义调整：通过几何调整，3DiffTection 的模型学会了如何从不同的视角理解同一个场景。比如，它可以通过理解桌子上物品的几何关系，推测出被部分遮挡物品的完整形状。

通过语义调整，模型进一步学习了如何识别和分类这些物品，即使在它们部分被遮挡或以不常见的方式呈现时。

3、视图合成和控制网：它甚至能创造出好像是从房间里其他角度拍摄的照片，帮助我们更全面地理解桌子上的场景。

4、测试时预测集成：最后，当 3DiffTection 对这张照片进行物体检测时，它会综合考虑从多个虚拟视点获得的信息，以提高检测的准确性和可靠性。

5、3D 物体检测:最终，3DiffTection 利用这些调整过的特征来进行 3D 物体检测。它能够识别图片中的物体，并理解这些物体在三维空间中的相对位置和方向。

3DiffTection 的重点在于从单张图片中提取和理解三维信息，而不是直接创建一个完整的 3D 立体图像。这种方法在自动驾驶、机器人技术和增强现实等领域非常有用，因为它能够提供关于物体三维结构的重要信息，即使是从单一视角拍摄的图片。

项目及演示：https://research.nvidia.com/labs/toronto-ai/3difftection/
论文：https://arxiv.org/abs/2311.04391
GitHub：https://github.com/nv-tlabs/3DiffTection（coming soon...）</title>
            <link>https://nitter.cz/xiaohuggg/status/1734409880205992322#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734409880205992322#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 03:08:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>3DiffTection：通过单张图片进行 3D 物体检测<br />
<br />
它能够仅靠单张图片，在复杂的视觉环境中，准确地识别和理解三维空间中的物体和其在空间中的位置和方向排列。<br />
<br />
从而实现更准确和深入的物体检测和场景理解。<br />
<br />
即时物体被遮挡，或者物体出现在不常见的角度和位置也能识别出来。<br />
<br />
该项目由英伟达多伦多人工智能实验室以及多所大学联合开发。<br />
<br />
3DiffTection 主要特点：<br />
<br />
从图片识别物体：它能从一张普通的照片中识别出各种物体，并了解这些物体在真实世界中的位置和方向。<br />
<br />
创建不同视角的图像：即使只有一张图片，3DiffTection 也能创造出从不同角度看这个场景的新图片。<br />
<br />
连接不同视角的相同点：它能识别出在不同图片中相同的地方或物体，即使这些图片是从不同的角度拍摄的。<br />
<br />
3DiffTection 的工作原理：<br />
<br />
1、3D 感知扩散模型:：首先，3DiffTection 使用它的扩散模型来分析这张照片。这个模型经过特殊训练，能够理解物体在三维空间中的排列和方向，即使在复杂的视觉环境中。<br />
<br />
2、几何和语义调整：通过几何调整，3DiffTection 的模型学会了如何从不同的视角理解同一个场景。比如，它可以通过理解桌子上物品的几何关系，推测出被部分遮挡物品的完整形状。<br />
<br />
通过语义调整，模型进一步学习了如何识别和分类这些物品，即使在它们部分被遮挡或以不常见的方式呈现时。<br />
<br />
3、视图合成和控制网：它甚至能创造出好像是从房间里其他角度拍摄的照片，帮助我们更全面地理解桌子上的场景。<br />
<br />
4、测试时预测集成：最后，当 3DiffTection 对这张照片进行物体检测时，它会综合考虑从多个虚拟视点获得的信息，以提高检测的准确性和可靠性。<br />
<br />
5、3D 物体检测:最终，3DiffTection 利用这些调整过的特征来进行 3D 物体检测。它能够识别图片中的物体，并理解这些物体在三维空间中的相对位置和方向。<br />
<br />
3DiffTection 的重点在于从单张图片中提取和理解三维信息，而不是直接创建一个完整的 3D 立体图像。这种方法在自动驾驶、机器人技术和增强现实等领域非常有用，因为它能够提供关于物体三维结构的重要信息，即使是从单一视角拍摄的图片。<br />
<br />
项目及演示：<a href="https://research.nvidia.com/labs/toronto-ai/3difftection/">research.nvidia.com/labs/tor…</a><br />
论文：<a href="https://arxiv.org/abs/2311.04391">arxiv.org/abs/2311.04391</a><br />
GitHub：<a href="https://github.com/nv-tlabs/3DiffTection">github.com/nv-tlabs/3DiffTec…</a>（coming soon...）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQ0MDkyNTE4NDg5MjUxODQvcHUvaW1nL3BNaERpVGxaQjY5VTBLM0MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734316369863393667#m</id>
            <title>Rob Lynch @RobLynch99 发布了一条关于 GPT-4-turbo 的有趣发现。

他指出，当通过 API 使用 GPT-4-turbo 时，如果系统提示中的日期是十二月而不是五月，模型完成任务（如代码生成）会更短，这一差异在统计上是显著的。

有网友调侃因为12月是假期，模型自己在给自己放假…🤓偷懒！

实验细节：

•Rob Lynch 使用相同的提示进行了 API 调用（一个要求实现机器学习任务但不使用库的代码完成任务）。

•他创建了两个系统提示，一个告诉 API 当前是五月，另一个是十二月，并比较了这两种情况下的结果分布。

•对于五月的系统提示，平均值为 4298；对于十二月的系统提示，平均值为 4086。

•在每个样本中进行了 477 次完成，t 测试的 p 值小于 2.28e-07。

总结：

Rob Lynch 在他的推文中报告的现象表明，GPT-4-turbo 模型在处理相同的任务时，会根据系统提示中的日期信息产生不同长度的输出。

具体来说，他发现当模型“认为”当前是十二月而不是五月时（通过系统提示中的日期决定），它生成的代码或文本完成任务会更短。

1.时间感知:
•这表明 GPT-4-turbo 可能对时间信息有一定的敏感性，即使这种信息是通过系统提示隐式给出的。

2.输出变化:
•模型的输出不仅取决于给定的任务或问题，还可能受到其他上下文信息（如日期）的影响。

3.行为模式:
•这种现象可能揭示了模型在处理信息时的某些内在行为模式或偏好。

猜测可能原因：

1.上下文敏感性:
•GPT-4-turbo 模型被设计为对上下文非常敏感。如果模型在处理请求时识别到特定的日期信息，它可能会根据这个信息调整其回答。这可能是因为模型在训练时接触到了与日期相关的不同类型的数据。

2.内部逻辑:
•模型可能内部包含某种逻辑，使得它在处理包含特定日期的提示时表现出不同的行为。这种逻辑可能是在模型训练过程中无意中形成的。

3.数据集偏差:
•如果模型训练使用的数据集在不同时间段有所不同，这可能导致模型对日期信息作出反应。例如，某些类型的内容可能更多地出现在特定时间段的数据中。

4.随机性或偶然性:
•这种现象也可能是随机性或特定实验设置的偶然结果，并不一定代表模型普遍行为。</title>
            <link>https://nitter.cz/xiaohuggg/status/1734316369863393667#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734316369863393667#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 20:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Rob Lynch <a href="https://nitter.cz/RobLynch99" title="Rob Lynch">@RobLynch99</a> 发布了一条关于 GPT-4-turbo 的有趣发现。<br />
<br />
他指出，当通过 API 使用 GPT-4-turbo 时，如果系统提示中的日期是十二月而不是五月，模型完成任务（如代码生成）会更短，这一差异在统计上是显著的。<br />
<br />
有网友调侃因为12月是假期，模型自己在给自己放假…🤓偷懒！<br />
<br />
实验细节：<br />
<br />
•Rob Lynch 使用相同的提示进行了 API 调用（一个要求实现机器学习任务但不使用库的代码完成任务）。<br />
<br />
•他创建了两个系统提示，一个告诉 API 当前是五月，另一个是十二月，并比较了这两种情况下的结果分布。<br />
<br />
•对于五月的系统提示，平均值为 4298；对于十二月的系统提示，平均值为 4086。<br />
<br />
•在每个样本中进行了 477 次完成，t 测试的 p 值小于 2.28e-07。<br />
<br />
总结：<br />
<br />
Rob Lynch 在他的推文中报告的现象表明，GPT-4-turbo 模型在处理相同的任务时，会根据系统提示中的日期信息产生不同长度的输出。<br />
<br />
具体来说，他发现当模型“认为”当前是十二月而不是五月时（通过系统提示中的日期决定），它生成的代码或文本完成任务会更短。<br />
<br />
1.时间感知:<br />
•这表明 GPT-4-turbo 可能对时间信息有一定的敏感性，即使这种信息是通过系统提示隐式给出的。<br />
<br />
2.输出变化:<br />
•模型的输出不仅取决于给定的任务或问题，还可能受到其他上下文信息（如日期）的影响。<br />
<br />
3.行为模式:<br />
•这种现象可能揭示了模型在处理信息时的某些内在行为模式或偏好。<br />
<br />
猜测可能原因：<br />
<br />
1.上下文敏感性:<br />
•GPT-4-turbo 模型被设计为对上下文非常敏感。如果模型在处理请求时识别到特定的日期信息，它可能会根据这个信息调整其回答。这可能是因为模型在训练时接触到了与日期相关的不同类型的数据。<br />
<br />
2.内部逻辑:<br />
•模型可能内部包含某种逻辑，使得它在处理包含特定日期的提示时表现出不同的行为。这种逻辑可能是在模型训练过程中无意中形成的。<br />
<br />
3.数据集偏差:<br />
•如果模型训练使用的数据集在不同时间段有所不同，这可能导致模型对日期信息作出反应。例如，某些类型的内容可能更多地出现在特定时间段的数据中。<br />
<br />
4.随机性或偶然性:<br />
•这种现象也可能是随机性或特定实验设置的偶然结果，并不一定代表模型普遍行为。</p>
<p><a href="https://nitter.cz/RobLynch99/status/1734278713762549970#m">nitter.cz/RobLynch99/status/1734278713762549970#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734236818772758689#m</id>
            <title>Runway 宣布正组建一个团队来开发通用世界模型（General World Models，简称 GWM）。

这个项目的目标是创建一种先进的人工智能系统，能够理解和模拟现实世界中的各种情况和互动。

Runway 正在寻找对此研究感兴趣的人才加入他们的团队。详细：https://research.runwayml.com/introducing-general-world-models</title>
            <link>https://nitter.cz/xiaohuggg/status/1734236818772758689#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734236818772758689#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 15:40:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Runway 宣布正组建一个团队来开发通用世界模型（General World Models，简称 GWM）。<br />
<br />
这个项目的目标是创建一种先进的人工智能系统，能够理解和模拟现实世界中的各种情况和互动。<br />
<br />
Runway 正在寻找对此研究感兴趣的人才加入他们的团队。详细：<a href="https://research.runwayml.com/introducing-general-world-models">research.runwayml.com/introd…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQyMTI0MTA4MzgyODIyNDEvcHUvaW1nL05HZk5RZUJSZFcwSHI1VDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734177740826480673#m</id>
            <title>翻译了一下视频，方便大家能看懂

主要翻译了后半段演示的部分

确实和Gemini的演示水平一样，哈哈哈哈

只是没有加速，这才是正常的演示示范，Google如果这样正常演示其实也是很惊讶的，非要作假😂

想实现视频里效果的你们自己也可以试试，小哥分享了代码：https://github.com/gregsadetsky/sagittarius</title>
            <link>https://nitter.cz/xiaohuggg/status/1734177740826480673#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734177740826480673#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:46:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>翻译了一下视频，方便大家能看懂<br />
<br />
主要翻译了后半段演示的部分<br />
<br />
确实和Gemini的演示水平一样，哈哈哈哈<br />
<br />
只是没有加速，这才是正常的演示示范，Google如果这样正常演示其实也是很惊讶的，非要作假😂<br />
<br />
想实现视频里效果的你们自己也可以试试，小哥分享了代码：<a href="https://github.com/gregsadetsky/sagittarius">github.com/gregsadetsky/sagi…</a></p>
<p><a href="https://nitter.cz/dotey/status/1734104301155262545#m">nitter.cz/dotey/status/1734104301155262545#m</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQxNzY4ODE2NzM2OTUyMzIvcHUvaW1nL0R1RXY3NzJUaDN5c0owbmsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734170307697721740#m</id>
            <title>这个挺有意思🤓

让物体在虚拟场景中按照特定轨迹移动

#Quset3</title>
            <link>https://nitter.cz/xiaohuggg/status/1734170307697721740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734170307697721740#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:16:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个挺有意思🤓<br />
<br />
让物体在虚拟场景中按照特定轨迹移动<br />
<br />
<a href="https://nitter.cz/search?q=%23Quset3">#Quset3</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQxMTg0MzE4ODMxNDUyMTYvcHUvaW1nL2hwRE5TRUR3NnlqSFdHYVcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</id>
            <title>Mixtral AI公布MoE 8x7B详细细节：

• 32k上下文。
• 支持英语、法语、意大利语、德语和西班牙语。
• 性能超过Llama 2系列和GPT3.5
• 在代码生成方面表现强劲。
• 在MT-Bench上达到8.3的分数。

技术细节：

•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。

•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。

详细内容：https://mistral.ai/news/mixtral-of-experts/</title>
            <link>https://nitter.cz/xiaohuggg/status/1734139143259861185#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734139143259861185#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 09:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mixtral AI公布MoE 8x7B详细细节：<br />
<br />
• 32k上下文。<br />
• 支持英语、法语、意大利语、德语和西班牙语。<br />
• 性能超过Llama 2系列和GPT3.5<br />
• 在代码生成方面表现强劲。<br />
• 在MT-Bench上达到8.3的分数。<br />
<br />
技术细节：<br />
<br />
•Mixtral是一个稀疏混合专家网络，是一个仅解码器模型，其中前馈块从8组不同的参数组中选择。在每一层，对于每个令牌，路由网络选择两组（“专家”）来处理令牌并加性地结合它们的输出。<br />
<br />
•Mixtral总共有45B个参数，但每个令牌只使用12B个参数。因此，它以与12B模型相同的速度和成本处理输入和生成输出。<br />
<br />
详细内容：<a href="https://mistral.ai/news/mixtral-of-experts/">mistral.ai/news/mixtral-of-e…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1733694954260901907#m">nitter.cz/xiaohuggg/status/1733694954260901907#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JEbU9ZcmE0QUFyS0V2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1734105617982456270#m</id>
            <title>阿里又整活了

DreaMoving：能够生成目标身份在任何地方跳舞的视频。

你可以指定一个特定的人物（如名人、朋友或任何特定形象），让他正在任意背景或场景（可以是真实的地点如海滩、城市街道或任何虚构的场景）下跳舞。

更牛的是仅靠脸部照片或文字提示也能生成跳舞视频...

还可指定人物动作和姿势

DreaMoving是一个基于扩散模型的人类舞蹈视频生成框架。能够根据指导序列和简单的内容描述（仅文本提示、仅图像提示或文本和图像提示）生成高质量、高保真度的视频。

用户可以通过下几种输入内容来生成和控制视频：

1、文本提示：对视频内容的简单描述，比如场景设置、人物动作或任何特定的主题。例如，“一个女孩在海滩上跳舞”。

文本提示也可以用于描述视频中的背景环境。

2、参考图像：这些图像用于指定视频中人物的外观。例如，用户可以上传一个人脸图像来确保视频中的人物具有相同或类似的面部特征。

如果需要，还可以包括衣着或身体特征的图像。

3、姿势或深度序列：这是定义视频中人物动作的关键输入。姿势序列指定了人物在视频中的具体动作和姿态。
用户可以通过提供特定的舞蹈动作序列来控制视频中的舞蹈风格和动作。

4、可选的衣物图像：如果用户想要在视频中指定人物的服装样式，可以上传相关的衣物图像。

技术细节：

架构: DreaMoving 基于 Stable-Diffusion 模型构建，包括去噪 U-Net、视频控制网（Video ControlNet）和内容引导器（Content Guider）。

数据收集与预处理: 收集了约1000个高质量的人类舞蹈视频，经过剪辑和处理，得到约6000个短视频片段。

运动块: 为了提高时间一致性和运动保真度，将运动块集成到去噪 U-Net 和控制网中。

内容引导器: 设计用于控制生成视频的内容，包括人物外观和背景。使用图像提示精确引导人物外观，文本提示生成背景。

模型训练: 包括内容引导器训练、长帧预训练、视频控制网训练和表情微调。

模型推理: 输入包括文本提示、参考图像和姿势或深度序列。通过调整控制网和内容引导器中的参数来控制视频内容。

项目地址：https://dreamoving.github.io/dreamoving/

论文：https://arxiv.org/abs/2312.05107

代码还没有，是个很基础的演示，估计是为了抢流量...🤣</title>
            <link>https://nitter.cz/xiaohuggg/status/1734105617982456270#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1734105617982456270#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:59:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>阿里又整活了<br />
<br />
DreaMoving：能够生成目标身份在任何地方跳舞的视频。<br />
<br />
你可以指定一个特定的人物（如名人、朋友或任何特定形象），让他正在任意背景或场景（可以是真实的地点如海滩、城市街道或任何虚构的场景）下跳舞。<br />
<br />
更牛的是仅靠脸部照片或文字提示也能生成跳舞视频...<br />
<br />
还可指定人物动作和姿势<br />
<br />
DreaMoving是一个基于扩散模型的人类舞蹈视频生成框架。能够根据指导序列和简单的内容描述（仅文本提示、仅图像提示或文本和图像提示）生成高质量、高保真度的视频。<br />
<br />
用户可以通过下几种输入内容来生成和控制视频：<br />
<br />
1、文本提示：对视频内容的简单描述，比如场景设置、人物动作或任何特定的主题。例如，“一个女孩在海滩上跳舞”。<br />
<br />
文本提示也可以用于描述视频中的背景环境。<br />
<br />
2、参考图像：这些图像用于指定视频中人物的外观。例如，用户可以上传一个人脸图像来确保视频中的人物具有相同或类似的面部特征。<br />
<br />
如果需要，还可以包括衣着或身体特征的图像。<br />
<br />
3、姿势或深度序列：这是定义视频中人物动作的关键输入。姿势序列指定了人物在视频中的具体动作和姿态。<br />
用户可以通过提供特定的舞蹈动作序列来控制视频中的舞蹈风格和动作。<br />
<br />
4、可选的衣物图像：如果用户想要在视频中指定人物的服装样式，可以上传相关的衣物图像。<br />
<br />
技术细节：<br />
<br />
架构: DreaMoving 基于 Stable-Diffusion 模型构建，包括去噪 U-Net、视频控制网（Video ControlNet）和内容引导器（Content Guider）。<br />
<br />
数据收集与预处理: 收集了约1000个高质量的人类舞蹈视频，经过剪辑和处理，得到约6000个短视频片段。<br />
<br />
运动块: 为了提高时间一致性和运动保真度，将运动块集成到去噪 U-Net 和控制网中。<br />
<br />
内容引导器: 设计用于控制生成视频的内容，包括人物外观和背景。使用图像提示精确引导人物外观，文本提示生成背景。<br />
<br />
模型训练: 包括内容引导器训练、长帧预训练、视频控制网训练和表情微调。<br />
<br />
模型推理: 输入包括文本提示、参考图像和姿势或深度序列。通过调整控制网和内容引导器中的参数来控制视频内容。<br />
<br />
项目地址：<a href="https://dreamoving.github.io/dreamoving/">dreamoving.github.io/dreamov…</a><br />
<br />
论文：<a href="https://arxiv.org/abs/2312.05107">arxiv.org/abs/2312.05107</a><br />
<br />
代码还没有，是个很基础的演示，估计是为了抢流量...🤣</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzQwOTk1ODQyNjY0MDc5MzYvcHUvaW1nL2RyejRqMWpJQ3hLT1VQWkUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>