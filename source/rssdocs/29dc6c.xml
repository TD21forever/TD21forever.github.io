<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756531196933419330#m</id>
            <title>1X's ：神经网络视觉端到端学习机器人

该机器人能够完全独立地执行任务，无需人类远程操控或通过预设脚本。

所有动作都是实时通过神经网络计算得出。

机器人基于视觉的端到端神经网络直接从图像中学习如何控制其动作，包括驾驶、操纵手臂和抓取器、控制躯干和头部等。

演示视频未加速剪辑...

通过训练，机器人能够理解和执行一系列广泛的物理行为，如清洁、整理、拾起物体以及与人类和其他机器人进行社交互动。

项目采用了一种策略，通过在几分钟内收集数据并在桌面GPU上进行训练，快速微调模型以适应特定的任务，从而使机器人能够迅速学习新的技能。

技术原理：

1、演示数据集构建：团队首先收集了一组包含30个EVE机器人执行各种任务的演示数据。这些数据非常多样，包括机器人进行清洁、整理家庭、拾起物体以及与人和其他机器人社交互动等物理行为的实例。

2、基础模型训练：使用这些演示数据，团队训练了一个“基础模型”。这个模型能够理解一系列广泛的物理行为，为机器人提供了对各种任务基本动作的理解基础。

3、模型微调：接下来，团队针对特定的任务类型对基础模型进行了微调，生成了多个专门的能力模型。例如，他们创建了专门用于门操作的模型和另一个专注于仓库任务的模型。

4、进一步微调以适应特定任务：团队进一步微调这些专门模型，使它们能够执行更具体的任务，比如打开一个特定的门。这种微调过程使模型能够对特定的行为或动作有更精确的理解和执行能力。

5、快速引入新技能：通过这种分层微调策略，团队可以在短时间内（仅几分钟的数据收集和在桌面GPU上的训练）快速地为机器人引入新的技能。

详细：https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed

演示视频中没有远程操控、没有计算机图形、没有剪辑、没有视频加速、没有脚本轨迹回放。一切都通过神经网络控制，完全自主，以1X速度进行。

机器人硬件信息：

1X的目标是设计出能够在任何场景中有效工作的通用安卓机器人，以应对现实世界的不可预测性。

EVE是1X技术公司开发的一款高级安卓机器人，旨在为商业行业提供智能的工作解决方案。EVE设计用于执行各种任务，从物流操作到保安巡逻等。这款机器人集安全性、平衡性和智能于一身，可以轻松融入现有的工作流程中，与团队无缝协作。

主要特点包括：

安全第一：每台EVE在部署前都会在真实世界场景中进行测试，其软性、仿生机械设计从内而外确保安全性，适合在各种空间中工作。

平衡性能：EVE能够处理重物，同时也足够柔和以处理易碎物品，无论是在仓库还是分发中心，都能轻松融入您的物流工作流程。

智能行为：EVE通过观察专家的动作进行学习，如移动设备、开门、完成订单等，然后通过其体现的学习模型重现动作并开始工作。随着时间的积累，EVE在基础知识上构建经验，使得未来任务（如“移动那个箱子”）变得更加简单。

EVE的规格：

高度：1.86米
重量：86公斤
最高速度：14.4公里/小时
携带能力：15公斤
运行时间：6小时

EVE通过人工智能自主操作，默认情况下能够导航工作空间，执行如开不同手柄的门、从远处识别人或物体、像人类一样穿越非结构化空间的任务。EVE在帮助人类操作者执行任务时，利用其力量、精确度和传感器执行如在办公楼巡逻和检查员工ID徽章的任务。EVE会关注潜在的危险或错误，并在操作者需要接管时报告。</title>
            <link>https://nitter.cz/xiaohuggg/status/1756531196933419330#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756531196933419330#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 04:10:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>1X's ：神经网络视觉端到端学习机器人<br />
<br />
该机器人能够完全独立地执行任务，无需人类远程操控或通过预设脚本。<br />
<br />
所有动作都是实时通过神经网络计算得出。<br />
<br />
机器人基于视觉的端到端神经网络直接从图像中学习如何控制其动作，包括驾驶、操纵手臂和抓取器、控制躯干和头部等。<br />
<br />
演示视频未加速剪辑...<br />
<br />
通过训练，机器人能够理解和执行一系列广泛的物理行为，如清洁、整理、拾起物体以及与人类和其他机器人进行社交互动。<br />
<br />
项目采用了一种策略，通过在几分钟内收集数据并在桌面GPU上进行训练，快速微调模型以适应特定的任务，从而使机器人能够迅速学习新的技能。<br />
<br />
技术原理：<br />
<br />
1、演示数据集构建：团队首先收集了一组包含30个EVE机器人执行各种任务的演示数据。这些数据非常多样，包括机器人进行清洁、整理家庭、拾起物体以及与人和其他机器人社交互动等物理行为的实例。<br />
<br />
2、基础模型训练：使用这些演示数据，团队训练了一个“基础模型”。这个模型能够理解一系列广泛的物理行为，为机器人提供了对各种任务基本动作的理解基础。<br />
<br />
3、模型微调：接下来，团队针对特定的任务类型对基础模型进行了微调，生成了多个专门的能力模型。例如，他们创建了专门用于门操作的模型和另一个专注于仓库任务的模型。<br />
<br />
4、进一步微调以适应特定任务：团队进一步微调这些专门模型，使它们能够执行更具体的任务，比如打开一个特定的门。这种微调过程使模型能够对特定的行为或动作有更精确的理解和执行能力。<br />
<br />
5、快速引入新技能：通过这种分层微调策略，团队可以在短时间内（仅几分钟的数据收集和在桌面GPU上的训练）快速地为机器人引入新的技能。<br />
<br />
详细：<a href="https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed">1x.tech/discover/all-neural-…</a><br />
<br />
演示视频中没有远程操控、没有计算机图形、没有剪辑、没有视频加速、没有脚本轨迹回放。一切都通过神经网络控制，完全自主，以1X速度进行。<br />
<br />
机器人硬件信息：<br />
<br />
1X的目标是设计出能够在任何场景中有效工作的通用安卓机器人，以应对现实世界的不可预测性。<br />
<br />
EVE是1X技术公司开发的一款高级安卓机器人，旨在为商业行业提供智能的工作解决方案。EVE设计用于执行各种任务，从物流操作到保安巡逻等。这款机器人集安全性、平衡性和智能于一身，可以轻松融入现有的工作流程中，与团队无缝协作。<br />
<br />
主要特点包括：<br />
<br />
安全第一：每台EVE在部署前都会在真实世界场景中进行测试，其软性、仿生机械设计从内而外确保安全性，适合在各种空间中工作。<br />
<br />
平衡性能：EVE能够处理重物，同时也足够柔和以处理易碎物品，无论是在仓库还是分发中心，都能轻松融入您的物流工作流程。<br />
<br />
智能行为：EVE通过观察专家的动作进行学习，如移动设备、开门、完成订单等，然后通过其体现的学习模型重现动作并开始工作。随着时间的积累，EVE在基础知识上构建经验，使得未来任务（如“移动那个箱子”）变得更加简单。<br />
<br />
EVE的规格：<br />
<br />
高度：1.86米<br />
重量：86公斤<br />
最高速度：14.4公里/小时<br />
携带能力：15公斤<br />
运行时间：6小时<br />
<br />
EVE通过人工智能自主操作，默认情况下能够导航工作空间，执行如开不同手柄的门、从远处识别人或物体、像人类一样穿越非结构化空间的任务。EVE在帮助人类操作者执行任务时，利用其力量、精确度和传感器执行如在办公楼巡逻和检查员工ID徽章的任务。EVE会关注潜在的危险或错误，并在操作者需要接管时报告。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTY1MjE4ODk2NTExMzQ0NjQvcHUvaW1nL1o3aEZuSWdxQTJsdTFOTGIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756516368269312196#m</id>
            <title>Vision Arena：视觉模型竞技场

这个项目的目的是测试和比较不同的视觉语言模型（VLMs），比如GPT-4V、Gemini、Llava、Qwen-VL等。

用户可以在这个工具上同时测试两个视觉模型，并对它们进行投票，以决定哪个更优秀。

而且是盲测，选择你认为好的结果才会告诉你模型是什么。

测试地址：https://huggingface.co/spaces/WildVision/vision-arena</title>
            <link>https://nitter.cz/xiaohuggg/status/1756516368269312196#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756516368269312196#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 03:11:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vision Arena：视觉模型竞技场<br />
<br />
这个项目的目的是测试和比较不同的视觉语言模型（VLMs），比如GPT-4V、Gemini、Llava、Qwen-VL等。<br />
<br />
用户可以在这个工具上同时测试两个视觉模型，并对它们进行投票，以决定哪个更优秀。<br />
<br />
而且是盲测，选择你认为好的结果才会告诉你模型是什么。<br />
<br />
测试地址：<a href="https://huggingface.co/spaces/WildVision/vision-arena">huggingface.co/spaces/WildVi…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTYzMjc1MzIzNTI0NDIzNjgvcHUvaW1nL2ZYWjBZdVVyWnBYVU1xN1cuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756508424551227405#m</id>
            <title>Maybe：一个花了100万美金打造的个人财务的操作系统，由于没有运营成功，现在他们给开源了

用户可以用它来跟踪和管理自己的收入、支出、投资和财富。

同时它还包含一个“财务顾问”功能，使用户能够与真正的财务规划师或注册金融分析师联系，以获得专业的财务管理建议。

由于商业模式未能成功，这个项目在2023年中期关闭了。团队在2021/2022年期间投入了将近100万美元来构建这个应用，包括员工、合同工、数据提供商/服务和基础设施等成本。

现在，团队决定将Maybe作为一个完全开源的项目复兴，目的是让用户可以免费运行这个应用，管理自己的财务，并最终提供一个收费的托管版本。

GitHub：https://github.com/maybe-finance/maybe</title>
            <link>https://nitter.cz/xiaohuggg/status/1756508424551227405#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756508424551227405#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 11 Feb 2024 02:40:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Maybe：一个花了100万美金打造的个人财务的操作系统，由于没有运营成功，现在他们给开源了<br />
<br />
用户可以用它来跟踪和管理自己的收入、支出、投资和财富。<br />
<br />
同时它还包含一个“财务顾问”功能，使用户能够与真正的财务规划师或注册金融分析师联系，以获得专业的财务管理建议。<br />
<br />
由于商业模式未能成功，这个项目在2023年中期关闭了。团队在2021/2022年期间投入了将近100万美元来构建这个应用，包括员工、合同工、数据提供商/服务和基础设施等成本。<br />
<br />
现在，团队决定将Maybe作为一个完全开源的项目复兴，目的是让用户可以免费运行这个应用，管理自己的财务，并最终提供一个收费的托管版本。<br />
<br />
GitHub：<a href="https://github.com/maybe-finance/maybe">github.com/maybe-finance/may…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YtMTJPamE4QUFfRWM2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756321049887854740#m</id>
            <title>微软在Windows 11 Insider Preview Build 26052中引入了Sudo for Windows新功能

该功能非常类似于macOS和Linux系统中终端里的sudo命令。

sudo命令允许用户执行需要管理员（root）权限的命令，而无需切换到root用户。用户只需在命令前加上sudo，然后输入自己的密码，就可以以更高权限运行该命令。

Sudo for Windows目前支持三种不同的配置选项：在新窗口中打开、输入关闭和内联。这些配置允许用户根据需要选择如何执行提权过程。

微软计划未来几个月内扩展Sudo for Windows的文档，并分享更多关于在“内联”配置下运行sudo的安全性细节。

此外，微软还宣布将这个项目开源在GitHub上。目前，他们正努力在GitHub仓库中添加更多关于该项目的信息，并将在未来几个月分享更多计划细节。

详细：https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/
GitHub：https://github.com/microsoft/sudo</title>
            <link>https://nitter.cz/xiaohuggg/status/1756321049887854740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756321049887854740#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 14:15:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软在Windows 11 Insider Preview Build 26052中引入了Sudo for Windows新功能<br />
<br />
该功能非常类似于macOS和Linux系统中终端里的sudo命令。<br />
<br />
sudo命令允许用户执行需要管理员（root）权限的命令，而无需切换到root用户。用户只需在命令前加上sudo，然后输入自己的密码，就可以以更高权限运行该命令。<br />
<br />
Sudo for Windows目前支持三种不同的配置选项：在新窗口中打开、输入关闭和内联。这些配置允许用户根据需要选择如何执行提权过程。<br />
<br />
微软计划未来几个月内扩展Sudo for Windows的文档，并分享更多关于在“内联”配置下运行sudo的安全性细节。<br />
<br />
此外，微软还宣布将这个项目开源在GitHub上。目前，他们正努力在GitHub仓库中添加更多关于该项目的信息，并将在未来几个月分享更多计划细节。<br />
<br />
详细：<a href="https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/">devblogs.microsoft.com/comma…</a><br />
GitHub：<a href="https://github.com/microsoft/sudo">github.com/microsoft/sudo</a></p>
<video loop="loop" poster="https://nitter.cz/pic/enc/dHdlZXRfdmlkZW9fdGh1bWIvR0YteXVncWEwQUE4WWNGLmpwZw==">
  <source src="https://nitter.cz/pic/enc/dmlkZW8udHdpbWcuY29tL3R3ZWV0X3ZpZGVvL0dGLXl1Z3FhMEFBOFljRi5tcDQ=" type="video/mp4" /></video>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756316182343512348#m</id>
            <title>iMusic：基于IMU的面部表情捕捉

该项目使用了一种可以贴在脸上，叫做惯性测量单元（IMUs）的小型设备，来捕捉面部表情。

与依赖摄像头的方法不同，IMUs不需要拍摄视频，通过捕捉微小面部动作，来捕捉表情，所以更能保护个人隐私。

即使在脸部部分被遮挡的情况下，它也能有效工作。

MUSIC项目主要解决了两个问题：

在需要保护隐私的场合，如何捕捉面部表情而不依赖视频。

在脸部被部分遮挡的情况下，如何准确捕捉面部动作。

主要特点：

1、隐私保护：相比基于视频的面部捕捉方法，使用IMUs不需要捕捉用户的视觉图像，因此更能保护用户的隐私。

2、避免视线遮挡问题：由于不依赖视觉信息，即使用户的面部部分被遮挡，系统也能准确捕捉面部表情。

3、精确捕捉细微表情：IMUs能够检测细微的面部运动，帮助捕捉通常难以通过视觉方法识别的轻微表情变化。

4、灵活性和可移植性：IMUs小巧轻便，可以轻易地集成到多种设备中，使得iMusic项目的技术可以广泛应用于各种场合，包括但不限于娱乐、健康监测和人机交互等领域。

iMusic的工作原理：

1、硬件设计：微型IMUs，使其能够适合贴合面部并准确捕捉面部运动。这些IMUs被放置在面部的关键位置，以监测表情变化时的动态信息。

2、数据采集与校准：通过IMUs收集面部运动的原始数据，并进行必要的校准处理，以确保数据的准确性和一致性。

3、IMU-ARKit数据集：创建一个包含丰富的IMU和视觉信号配对的数据集，用于训练和验证面部表情捕捉模型。这个数据集覆盖了广泛的面部表情和表演，为后续的分析和模型训练提供了基础。

4、面部表情预测模型：利用特别设计的Transformer扩散模型，从IMUs收集的信号中预测面部的混合形状参数。这个模型通过两阶段训练策略来优化，使其能够准确地从IMU数据中解析出复杂的面部表情。

项目及演示：https://sites.google.com/view/projectpage-imusic
论文：https://arxiv.org/abs/2402.03944
视频：https://youtu.be/rPusR6b43ng</title>
            <link>https://nitter.cz/xiaohuggg/status/1756316182343512348#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756316182343512348#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 13:56:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>iMusic：基于IMU的面部表情捕捉<br />
<br />
该项目使用了一种可以贴在脸上，叫做惯性测量单元（IMUs）的小型设备，来捕捉面部表情。<br />
<br />
与依赖摄像头的方法不同，IMUs不需要拍摄视频，通过捕捉微小面部动作，来捕捉表情，所以更能保护个人隐私。<br />
<br />
即使在脸部部分被遮挡的情况下，它也能有效工作。<br />
<br />
MUSIC项目主要解决了两个问题：<br />
<br />
在需要保护隐私的场合，如何捕捉面部表情而不依赖视频。<br />
<br />
在脸部被部分遮挡的情况下，如何准确捕捉面部动作。<br />
<br />
主要特点：<br />
<br />
1、隐私保护：相比基于视频的面部捕捉方法，使用IMUs不需要捕捉用户的视觉图像，因此更能保护用户的隐私。<br />
<br />
2、避免视线遮挡问题：由于不依赖视觉信息，即使用户的面部部分被遮挡，系统也能准确捕捉面部表情。<br />
<br />
3、精确捕捉细微表情：IMUs能够检测细微的面部运动，帮助捕捉通常难以通过视觉方法识别的轻微表情变化。<br />
<br />
4、灵活性和可移植性：IMUs小巧轻便，可以轻易地集成到多种设备中，使得iMusic项目的技术可以广泛应用于各种场合，包括但不限于娱乐、健康监测和人机交互等领域。<br />
<br />
iMusic的工作原理：<br />
<br />
1、硬件设计：微型IMUs，使其能够适合贴合面部并准确捕捉面部运动。这些IMUs被放置在面部的关键位置，以监测表情变化时的动态信息。<br />
<br />
2、数据采集与校准：通过IMUs收集面部运动的原始数据，并进行必要的校准处理，以确保数据的准确性和一致性。<br />
<br />
3、IMU-ARKit数据集：创建一个包含丰富的IMU和视觉信号配对的数据集，用于训练和验证面部表情捕捉模型。这个数据集覆盖了广泛的面部表情和表演，为后续的分析和模型训练提供了基础。<br />
<br />
4、面部表情预测模型：利用特别设计的Transformer扩散模型，从IMUs收集的信号中预测面部的混合形状参数。这个模型通过两阶段训练策略来优化，使其能够准确地从IMU数据中解析出复杂的面部表情。<br />
<br />
项目及演示：<a href="https://sites.google.com/view/projectpage-imusic">sites.google.com/view/projec…</a><br />
论文：<a href="https://arxiv.org/abs/2402.03944">arxiv.org/abs/2402.03944</a><br />
视频：<a href="https://youtu.be/rPusR6b43ng">youtu.be/rPusR6b43ng</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTYzMTEwMzg4ODExMjAyNTYvcHUvaW1nL011VnB1WlJQUV9ZajZ2OUMuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756313854437732694#m</id>
            <title>OpenAI推出了一个基于Whisper模型的音频到文本的API，可以将任何音频直接转录成文本并翻译为英文。

同时在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。

主要功能：

1、音频转文字：将音频文件中的语音内容自动转换成文本形式，让用户可以读到音频里说了什么。

2、支持多种语言的翻译转录：如果音频中的语言不是英语，这个API还能先将其翻译成英语，然后再进行转录，使非英语内容也能轻松转换成文本。

3、提供时间戳：OpenAI的Whisper API提供了一个参数timestamp_granularities[]，允许用户获取带有时间戳的更结构化的JSON输出格式。这意味着，在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。

4、支持多种音频格式：支持上传25MB以内的文件，包括mp3、mp4、mpeg、mpga、m4a、wav和webm等格式。用户无需转换文件格式即可直接使用。

详细：https://platform.openai.com/docs/guides/speech-to-text</title>
            <link>https://nitter.cz/xiaohuggg/status/1756313854437732694#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756313854437732694#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 13:47:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI推出了一个基于Whisper模型的音频到文本的API，可以将任何音频直接转录成文本并翻译为英文。<br />
<br />
同时在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。<br />
<br />
主要功能：<br />
<br />
1、音频转文字：将音频文件中的语音内容自动转换成文本形式，让用户可以读到音频里说了什么。<br />
<br />
2、支持多种语言的翻译转录：如果音频中的语言不是英语，这个API还能先将其翻译成英语，然后再进行转录，使非英语内容也能轻松转换成文本。<br />
<br />
3、提供时间戳：OpenAI的Whisper API提供了一个参数timestamp_granularities[]，允许用户获取带有时间戳的更结构化的JSON输出格式。这意味着，在转录文本的同时，API能够提供每个词或句子出现的具体时间点，帮助用户准确定位音频中的特定部分。<br />
<br />
4、支持多种音频格式：支持上传25MB以内的文件，包括mp3、mp4、mpeg、mpga、m4a、wav和webm等格式。用户无需转换文件格式即可直接使用。<br />
<br />
详细：<a href="https://platform.openai.com/docs/guides/speech-to-text">platform.openai.com/docs/gui…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YtdUZyNmJjQUFoU0E0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/dotey/status/1755487926127820830#m</id>
            <title>RT by @xiaohuggg: 升级了一下科技文章翻译GPT，增加了一个API可以将URL转成Markdown，现在只需要输入URL就可以翻译网页内容

https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi</title>
            <link>https://nitter.cz/dotey/status/1755487926127820830#m</link>
            <guid isPermaLink="false">https://nitter.cz/dotey/status/1755487926127820830#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 07:05:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>升级了一下科技文章翻译GPT，增加了一个API可以将URL转成Markdown，现在只需要输入URL就可以翻译网页内容<br />
<br />
<a href="https://chat.openai.com/g/g-uBhKUJJTl-ke-ji-wen-zhang-fan-yi">chat.openai.com/g/g-uBhKUJJT…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5Mllkd1djQUFPdHRFLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5M2NLYVhZQUFmMjQwLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Z5LTFnVld3QUVKVFU3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1756154223065251911#m</id>
            <title>马斯克：将用「X」取代手机电话

马斯克称将在几个月后注销自己的电话号码。转而使用「X」进行信息收发和通话🤔</title>
            <link>https://nitter.cz/xiaohuggg/status/1756154223065251911#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1756154223065251911#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 10 Feb 2024 03:12:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>马斯克：将用「X」取代手机电话<br />
<br />
马斯克称将在几个月后注销自己的电话号码。转而使用「X」进行信息收发和通话🤔</p>
<p><a href="https://nitter.cz/elonmusk/status/1755870691159626094#m">nitter.cz/elonmusk/status/1755870691159626094#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</id>
            <title>Happy Chinese New Year！

Every One…

🐲🧨🇨🇳</title>
            <link>https://nitter.cz/xiaohuggg/status/1755985073747447827#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755985073747447827#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 16:00:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Happy Chinese New Year！<br />
<br />
Every One…<br />
<br />
🐲🧨🇨🇳</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y2REhLRGJNQUFxaHNsLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755911511531520464#m</id>
            <title>做好了

自己做的

开吃🫡</title>
            <link>https://nitter.cz/xiaohuggg/status/1755911511531520464#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755911511531520464#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 11:08:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>做好了<br />
<br />
自己做的<br />
<br />
开吃🫡</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1ySWFvQUExZHVSLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1yRWFBQUFkSkFoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1zQWJBQUFoUzNnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Y1QU1yR2E0QUF3b1BVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755850647667388813#m</id>
            <title>开始做年夜饭了

不更新了 

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1755850647667388813#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755850647667388813#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 07:06:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开始做年夜饭了<br />
<br />
不更新了 <br />
<br />
😂</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755840200260096210#m</id>
            <title>Google DeepMind抛弃传统的搜索方法，使用Transformer模型，训练了一个AI模型来下象棋。

该模型能够达到国际象棋大师级别的水平。甚至表现超过了AlphaZero。

这说明Transformer模型，不仅能处理语言任务，还能够在复杂的决策和策略游戏中学习和模拟高级人类智能。

该方法同时显著减少了计算需求。

研究背景：

在国际象棋AI的发展历史中，传统的方法通常依赖于搜索算法（比如alpha-beta剪枝）来预测和评估可能的移动，从而选择最佳的一步。——即考虑棋盘上所有可能的走法和结果——来决定下一步怎么走。

这种方法虽然可以工作，但需要大量的计算资源。

AlphaZero是由DeepMind开发的一种高级AI，它通过自我对弈学习棋类游戏的策略，并在国际象棋、围棋和日本将棋中取得了超越人类的表现。AlphaZero使用了一种叫做蒙特卡洛树搜索（MCTS）的算法来预测和评估可能的走法。

研究方法：

他们首先从网上搜集了1000万局棋赛的数据，然后用一个非常强大的国际象棋程序（Stockfish 16）来分析这些棋局，为每一个棋盘的每一步棋提供一个评分。这样就得到了大约150亿个数据点，用来训练他们的AI模型。

通过使用大型的Transformer模型和大量的国际象棋游戏数据进行训练，AI能够直接学习棋局中的模式和策略，而无需进行复杂的棋局搜索。

结果非常令人印象深刻：这个AI模型能够达到接近国际象棋大师级别的水平，而且在不使用任何搜索算法的情况下，还能解决复杂的棋局问题。

该模型在性能上甚至超越了AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。

（在国际象棋AI中，策略和价值判断密切配合，共同指导AI做出最佳决策。策略告诉AI它可以做什么，而价值判断则告诉AI哪些行动可能导致胜利。通过这两个组件，AI能够在没有人类直接指导的情况下，自主学习和提高自己的棋艺。）

这意味着AI可以仅通过观察当前棋盘的状态就做出高水平的决策，从而在与人类玩家的对弈中达到大师级别的表现。

这不仅是国际象棋AI领域的一个重大进步，也为使用AI解决其他复杂任务提供了新的可能性。

这项研究的意义：

1、技术创新：通过使用深度学习技术而不是传统的搜索算法来达到国际象棋大师级水平，这项研究展示了人工智能领域的一种重要技术进步。它证明了深度学习模型，特别是Transformer模型，能够在复杂的决策和策略游戏中学习和模拟高级人类智能。

2、计算效率：传统的国际象棋AI依赖于大规模的搜索树和复杂的启发式评估，这在计算上非常昂贵。这项研究通过直接从大量数据中学习决策过程，显著减少了计算需求，展示了一种更高效的方式来构建高水平的游戏AI。

3、AI泛化能力：这项研究不仅仅是关于国际象棋，它还展示了深度学习模型在没有专门设计的规则或搜索算法支持下，通过学习大量示例来泛化和解决复杂任务的能力。这为其他类型的游戏和决策制定任务提供了新的思路。

4、开拓新的应用领域：这项研究表明，类似的方法可以应用于其他需要复杂策略和决策的领域，比如自动驾驶、金融市场分析、复杂系统管理等。通过学习大量的历史数据，AI可以在这些领域内做出更加精准和高效的决策。

5、提升AI的理解和创造能力：通过在没有预定义搜索策略的情况下训练AI达到高水平的表现，这项研究为AI的自主学习和理解复杂系统提供了新的范例，同时也推动了AI在创造性任务上的应用，如生成艺术、音乐、文学作品等。

论文：https://arxiv.org/abs/2402.04494
PDF：https://arxiv.org/pdf/2402.04494.pdf

与AI下棋：https://lichess.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1755840200260096210#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755840200260096210#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 06:25:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google DeepMind抛弃传统的搜索方法，使用Transformer模型，训练了一个AI模型来下象棋。<br />
<br />
该模型能够达到国际象棋大师级别的水平。甚至表现超过了AlphaZero。<br />
<br />
这说明Transformer模型，不仅能处理语言任务，还能够在复杂的决策和策略游戏中学习和模拟高级人类智能。<br />
<br />
该方法同时显著减少了计算需求。<br />
<br />
研究背景：<br />
<br />
在国际象棋AI的发展历史中，传统的方法通常依赖于搜索算法（比如alpha-beta剪枝）来预测和评估可能的移动，从而选择最佳的一步。——即考虑棋盘上所有可能的走法和结果——来决定下一步怎么走。<br />
<br />
这种方法虽然可以工作，但需要大量的计算资源。<br />
<br />
AlphaZero是由DeepMind开发的一种高级AI，它通过自我对弈学习棋类游戏的策略，并在国际象棋、围棋和日本将棋中取得了超越人类的表现。AlphaZero使用了一种叫做蒙特卡洛树搜索（MCTS）的算法来预测和评估可能的走法。<br />
<br />
研究方法：<br />
<br />
他们首先从网上搜集了1000万局棋赛的数据，然后用一个非常强大的国际象棋程序（Stockfish 16）来分析这些棋局，为每一个棋盘的每一步棋提供一个评分。这样就得到了大约150亿个数据点，用来训练他们的AI模型。<br />
<br />
通过使用大型的Transformer模型和大量的国际象棋游戏数据进行训练，AI能够直接学习棋局中的模式和策略，而无需进行复杂的棋局搜索。<br />
<br />
结果非常令人印象深刻：这个AI模型能够达到接近国际象棋大师级别的水平，而且在不使用任何搜索算法的情况下，还能解决复杂的棋局问题。<br />
<br />
该模型在性能上甚至超越了AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。<br />
<br />
（在国际象棋AI中，策略和价值判断密切配合，共同指导AI做出最佳决策。策略告诉AI它可以做什么，而价值判断则告诉AI哪些行动可能导致胜利。通过这两个组件，AI能够在没有人类直接指导的情况下，自主学习和提高自己的棋艺。）<br />
<br />
这意味着AI可以仅通过观察当前棋盘的状态就做出高水平的决策，从而在与人类玩家的对弈中达到大师级别的表现。<br />
<br />
这不仅是国际象棋AI领域的一个重大进步，也为使用AI解决其他复杂任务提供了新的可能性。<br />
<br />
这项研究的意义：<br />
<br />
1、技术创新：通过使用深度学习技术而不是传统的搜索算法来达到国际象棋大师级水平，这项研究展示了人工智能领域的一种重要技术进步。它证明了深度学习模型，特别是Transformer模型，能够在复杂的决策和策略游戏中学习和模拟高级人类智能。<br />
<br />
2、计算效率：传统的国际象棋AI依赖于大规模的搜索树和复杂的启发式评估，这在计算上非常昂贵。这项研究通过直接从大量数据中学习决策过程，显著减少了计算需求，展示了一种更高效的方式来构建高水平的游戏AI。<br />
<br />
3、AI泛化能力：这项研究不仅仅是关于国际象棋，它还展示了深度学习模型在没有专门设计的规则或搜索算法支持下，通过学习大量示例来泛化和解决复杂任务的能力。这为其他类型的游戏和决策制定任务提供了新的思路。<br />
<br />
4、开拓新的应用领域：这项研究表明，类似的方法可以应用于其他需要复杂策略和决策的领域，比如自动驾驶、金融市场分析、复杂系统管理等。通过学习大量的历史数据，AI可以在这些领域内做出更加精准和高效的决策。<br />
<br />
5、提升AI的理解和创造能力：通过在没有预定义搜索策略的情况下训练AI达到高水平的表现，这项研究为AI的自主学习和理解复杂系统提供了新的范例，同时也推动了AI在创造性任务上的应用，如生成艺术、音乐、文学作品等。<br />
<br />
论文：<a href="https://arxiv.org/abs/2402.04494">arxiv.org/abs/2402.04494</a><br />
PDF：<a href="https://arxiv.org/pdf/2402.04494.pdf">arxiv.org/pdf/2402.04494.pdf</a><br />
<br />
与AI下棋：<a href="https://lichess.org/">lichess.org/</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzLW5wbGJ3QUFCRGxyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755832179014484162#m</id>
            <title>Vercel将9个AI集成到了一起

还创建了一个新Model Playground ，你可以在一个界面尝试数十种模型，以生成文本、图像、音频等内容。

Vercel为AI应用提供了丰富的产品基础设施，从增强客户服务流程的聊天机器人到带有语义搜索的推荐系统、检索增强生成（RAG）和生成图像服务...

为了让这一切更加简单，Vercel还提供了一套工具（AI SDK），帮助开发者在他们的网站上快速使用这些AI功能。比如，如果你想让你的网站能自动回答用户的问题，只需要几行代码就可以实现。

首批集成的9个AI：

◆ @perplexity_ai
◆ @replicate
◆ @pinecone
◆ @modal_labs
◆ @fal_ai_data
◆ @lmnt_com
◆ @togethercompute
◆ @elevenlabsio
◆ @anyscalecompute

详细：https://vercel.com/blog/ai-integrations

体验：https://vercel.com/ai</title>
            <link>https://nitter.cz/xiaohuggg/status/1755832179014484162#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755832179014484162#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 05:53:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Vercel将9个AI集成到了一起<br />
<br />
还创建了一个新Model Playground ，你可以在一个界面尝试数十种模型，以生成文本、图像、音频等内容。<br />
<br />
Vercel为AI应用提供了丰富的产品基础设施，从增强客户服务流程的聊天机器人到带有语义搜索的推荐系统、检索增强生成（RAG）和生成图像服务...<br />
<br />
为了让这一切更加简单，Vercel还提供了一套工具（AI SDK），帮助开发者在他们的网站上快速使用这些AI功能。比如，如果你想让你的网站能自动回答用户的问题，只需要几行代码就可以实现。<br />
<br />
首批集成的9个AI：<br />
<br />
◆ <a href="https://nitter.cz/perplexity_ai" title="Perplexity">@perplexity_ai</a><br />
◆ <a href="https://nitter.cz/replicate" title="Replicate">@replicate</a><br />
◆ <a href="https://nitter.cz/pinecone" title="Pinecone">@pinecone</a><br />
◆ <a href="https://nitter.cz/modal_labs" title="Modal Labs">@modal_labs</a><br />
◆ <a href="https://nitter.cz/fal_ai_data" title="fal">@fal_ai_data</a><br />
◆ <a href="https://nitter.cz/lmnt_com" title="LMNT">@lmnt_com</a><br />
◆ <a href="https://nitter.cz/togethercompute" title="Together AI">@togethercompute</a><br />
◆ <a href="https://nitter.cz/elevenlabsio" title="ElevenLabs">@elevenlabsio</a><br />
◆ <a href="https://nitter.cz/anyscalecompute" title="Anyscale">@anyscalecompute</a><br />
<br />
详细：<a href="https://vercel.com/blog/ai-integrations">vercel.com/blog/ai-integrati…</a><br />
<br />
体验：<a href="https://vercel.com/ai">vercel.com/ai</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzMUJtNGJvQUFaXzAtLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YzMUZJcmEwQUFoRVZ3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755824687811346514#m</id>
            <title>ComfyUI 3D Pack：ComfyUI现在能够处理3D图像了

可以将图片很快的转换成一个3D模型，在RTX3080 GPU上不到30秒。

可以直接看到3D模型，还能自动创建不同的相机角度，帮助你从各种方向查看3D模型。

通过3D高斯扩散改善模型质量。让3D模型看起来更真实，更有立体感。

支持多种格式导出。

方法：

集成了多种先进的3D处理算法，如大视图高斯模型（LGM）、三平面高斯变换器（Triplane Gaussian Transformers）等。

提供了一套工具和工作流，以便用户能够轻松地将2D图像转换为3D模型，并进行进一步的处理和优化。

作者：@MrForExample 
GitHub：https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main</title>
            <link>https://nitter.cz/xiaohuggg/status/1755824687811346514#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755824687811346514#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 Feb 2024 05:23:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ComfyUI 3D Pack：ComfyUI现在能够处理3D图像了<br />
<br />
可以将图片很快的转换成一个3D模型，在RTX3080 GPU上不到30秒。<br />
<br />
可以直接看到3D模型，还能自动创建不同的相机角度，帮助你从各种方向查看3D模型。<br />
<br />
通过3D高斯扩散改善模型质量。让3D模型看起来更真实，更有立体感。<br />
<br />
支持多种格式导出。<br />
<br />
方法：<br />
<br />
集成了多种先进的3D处理算法，如大视图高斯模型（LGM）、三平面高斯变换器（Triplane Gaussian Transformers）等。<br />
<br />
提供了一套工具和工作流，以便用户能够轻松地将2D图像转换为3D模型，并进行进一步的处理和优化。<br />
<br />
作者：<a href="https://nitter.cz/MrForExample" title="Mr. For Example">@MrForExample</a> <br />
GitHub：<a href="https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main">github.com/MrForExample/Comf…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NTU4MjQ0MzQxMDIwNzEyOTYvcHUvaW1nL0REV0pRbk1abm5TRm1CRXEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</id>
            <title>🔔http://Xiaohu.AI日报「2月8日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1755619904743690629#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755619904743690629#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 15:49:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔<a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「2月8日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwMl9aYmJBQUFmc2puLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</id>
            <title>R to @xiaohuggg: Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。

他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。

Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</title>
            <link>https://nitter.cz/xiaohuggg/status/1755607353167401134#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755607353167401134#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:59:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ethan Mollick分享了他过去六周使用Google的新AI产品Gemini Advanced的经验。<br />
<br />
他称，Gemini Advanced在性能上与GPT-4相当，但并未明显超越GPT-4。<br />
<br />
Gemini Advanced有其独特的优势和劣势，能够为我们的AI未来提供一些见解。并且它充满了“Ghosts”！</p>
<p><a href="https://nitter.cz/emollick/status/1755596564817699049#m">nitter.cz/emollick/status/1755596564817699049#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</id>
            <title>R to @xiaohuggg: Gemini Advanced权益

• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！

• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。

• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。

• 其他权益: 包括2TB的Google One存储空间等会员权益。</title>
            <link>https://nitter.cz/xiaohuggg/status/1755605438882894012#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755605438882894012#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:52:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Advanced权益<br />
<br />
• 价格: Gemini Advanced的订阅费用为每月19.99美元。可以免费体验2月！<br />
<br />
• 功能: 包括利用Ultra 1.0模型进行推理、遵循指令、编码和创造性灵感等方面的高级功能。<br />
<br />
• 集成: 订阅者不久将能够在Gmail、文档等Google应用中使用Gemini，享受更加无缝的集成体验。<br />
<br />
• 其他权益: 包括2TB的Google One存储空间等会员权益。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwcDFhRmFFQUFPdGhKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</id>
            <title>Gemini Ultra 上线  Bard 正式更名为 Gemini 

更新内容和之前泄漏的一样

核心要点：

- Gemini Ultra上线，Bard更名为Gemini

- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。

- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。

- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。

- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。

详细：https://gemini.google.com/updates</title>
            <link>https://nitter.cz/xiaohuggg/status/1755600097239536034#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755600097239536034#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 14:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Gemini Ultra 上线  Bard 正式更名为 Gemini <br />
<br />
更新内容和之前泄漏的一样<br />
<br />
核心要点：<br />
<br />
- Gemini Ultra上线，Bard更名为Gemini<br />
<br />
- 界面优化：Gemini的用户界面经过优化，以减少视觉干扰，提高可读性，并简化导航。<br />
<br />
- Gemini Advanced付费计划：提供访问Google最强大的AI模型Ultra 1.0的能力，可以执行复杂任务如编程、逻辑推理和创造性协作等。<br />
<br />
- Gemini Advanced将引入新功能和独家特性，如增强的多模态能力和编程特性，以及上传和深入分析文件的能力。<br />
<br />
- 将推出Gemini APP，用户可以在手机上下载使用Gemini来学习、写信、规划活动等。该应用与Google的其他应用（如Gmail、Maps和YouTube）集成，支持文本、语音或图片交互。<br />
<br />
详细：<a href="https://gemini.google.com/updates">gemini.google.com/updates</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Yway1iQ2IwQUF0M2ZULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</id>
            <title>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。

例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。

这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。

详细：https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app</title>
            <link>https://nitter.cz/xiaohuggg/status/1755587992067125449#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1755587992067125449#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 Feb 2024 13:42:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>据The Information消息，OpenAI正在开发一种新型的代理软件，该软件通过有效控制用户的设备来自动完成复杂任务。<br />
<br />
例如，用户可以请求ChatGPT代理将文档中的数据转移到电子表格中进行分析，或者自动填写费用报告并输入到会计软件中。<br />
<br />
这些请求将触发代理执行点击、光标移动、文本输入等操作，这些都是人类在使用不同应用程序时会进行的动作。<br />
<br />
详细：<a href="https://www.theinformation.com/articles/openai-shifts-ai-battleground-to-software-that-operates-devices-automates-tasks?utm_source=ti_app">theinformation.com/articles/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0YwWjk3b2FrQUEyd2FjLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>