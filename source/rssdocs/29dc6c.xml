<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738737858834514286#m</id>
            <title>苹果发布了一个多模态大模型，但是很多人似乎没有注意？？？

苹果12月14日释放了一个名为Ferret的多模态大语言模型，该模型不仅可以准确识别图像并描述其内容。

同时它还能够识别和定位图像中的各种元素，无论你用怎样的方式描述图像内容，Ferret都能准确地在图像中找到并识别出来。

Ferret拥有 (7B, 13B)两个版本，为了增强 Ferret 模型的能力苹果特别收集了一个GRIT 数据集。它包含了1.1M个样本，这些样本包含了丰富的层次空间知识。

主要功能和特点：

Ferret能够理解和处理图像与文本之间的复杂关系。这个模型的特别之处在于它能够识别和定位图像中的各种元素，无论这些元素是什么形状或大小。

比如在对话中引用图像的特定部分，或者根据文本描述在图像中找到特定物体。

Ferret 就像是一个能够理解图片和文字并将它们联系起来的智能系统。无论你在文本中提到图像的哪个部分，或者用怎样的方式描述，Ferret 都能准确地在图像中找到并识别出来。

1、多模态理解：Ferret 能够同时处理和理解图像（视觉信息）和文本（语言信息），这使得它能够在多种不同的模式之间建立联系。

2、空间指代理解：它能够识别和理解图像中特定区域的含义，即使这些区域的形状和大小各不相同。例如，如果文本提到图像中的某个特定部分，Ferret 能够识别出这部分是指什么。

3、理解复杂的文本描述：Ferret 能够理解各种类型的文本描述，无论这些描述是具体的还是抽象的。比如，“图像中红色车辆旁边的小狗”或“画面右上角的笑脸”。

4、放词汇描述精准定位：根据这些文本描述，Ferret 能够在提供的图像中准确地找到并标记出相应的物体或区域。例如，它可以识别并指出图像中的“小狗”或“笑脸”的确切位置。无论用户如何描述他们想要找到的图像中的内容，Ferret 都能理解并响应。

5、混合区域表示：Ferret 使用一种创新的表示方法来处理图像中的区域。这种表示结合了离散坐标（如点或边界框的位置）和连续特征（如区域的视觉内容）。这允许模型理解和处理各种形状和大小的区域，从而提高了对图像的空间理解能力。

6、空间感知的视觉采样器：为了处理不同形状的区域，Ferret 引入了一个空间感知的视觉采样器。这个采样器能够根据区域的形状和稀疏性提取视觉特征，使模型能够处理从简单点到复杂多边形等各种形状的区域。

7、多样的区域输入：Ferret具有识别和理解图像中各种不同类型区域的能力。

它可以处理以下类型的区域输入：

点：Ferret 能够识别图像中的特定点，例如用户指定的一个具体位置。

边界框：它可以识别和理解图像中的边界框，这些边界框通常用来标记图像中的物体或特定区域。

自由形状：Ferret 还能处理更复杂的自由形状，比如手绘的轮廓、不规则图形或任意多边形。这种能力使得它可以更精确地识别和理解图像中的复杂区域。

这种处理多样区域输入的能力使得 Ferret 在图像理解方面非常灵活和强大，能够适应各种不同的应用场景和用户需求。无论用户提供的是简单的点标记、常规的边界框，还是复杂的自由形状，Ferret 都能准确地识别和处理。

8、GRIT 数据集：GRIT 数据集是专门为了训练和增强 Ferret 而收集的，包含了1.1M个样本。

 这个数据集包含了丰富的层次空间知识，这意味着它涵盖了从简单物体到复杂空间关系的各种信息。 包含95K难负样本，这些是特别设计的挑战性样本，用于提高模型在处理困难情况下的鲁棒性和准确性。

主要表现：

1、Ferret-Bench评估：Ferret-Bench是为了评估Ferret而引入的一系列新任务，包括指称描述、指称推理和对话中的定位。在这些任务上，Ferret相比现有的最佳多模态大型语言模型（MLLM）平均提高了20.4%。这一结果表明Ferret在处理更复杂、更接近真实世界应用的任务时具有显著的优势。

2、改善对象幻觉：Ferret 在描述图像细节时能够减少错误或虚构的内容，这在自动图像描述和分析领域尤为重要。
它减轻了对象幻觉的问题，即在生成文本描述时减少了对不存在的对象的错误引用，提高了描述的准确性和可靠性。

3、Ferret 不仅在传统的指代和定位任务中表现优异，它能够更准确地理解和处理图像中的空间信息和语义。而且在需要指代/定位、语义、知识和推理的任务中也表现出色。 

Ferret 能够更准确地描述图像细节，减少在生成文本时对不存在的对象的幻觉。 通过其创新的方法和技术，为多模态语言模型在空间理解和定位方面提供了新的可能性，特别是在处理复杂的图像和文本交互时。

适用于多种应用场景：

由于其强大的图像和文本处理能力，Ferret 适用于多种应用场景，包括图像搜索、自动图像标注、交互式媒体探索等。

GitHub：https://github.com/apple/ml-ferret
论文：https://arxiv.org/abs/2310.07704</title>
            <link>https://nitter.cz/xiaohuggg/status/1738737858834514286#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738737858834514286#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 01:46:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>苹果发布了一个多模态大模型，但是很多人似乎没有注意？？？<br />
<br />
苹果12月14日释放了一个名为Ferret的多模态大语言模型，该模型不仅可以准确识别图像并描述其内容。<br />
<br />
同时它还能够识别和定位图像中的各种元素，无论你用怎样的方式描述图像内容，Ferret都能准确地在图像中找到并识别出来。<br />
<br />
Ferret拥有 (7B, 13B)两个版本，为了增强 Ferret 模型的能力苹果特别收集了一个GRIT 数据集。它包含了1.1M个样本，这些样本包含了丰富的层次空间知识。<br />
<br />
主要功能和特点：<br />
<br />
Ferret能够理解和处理图像与文本之间的复杂关系。这个模型的特别之处在于它能够识别和定位图像中的各种元素，无论这些元素是什么形状或大小。<br />
<br />
比如在对话中引用图像的特定部分，或者根据文本描述在图像中找到特定物体。<br />
<br />
Ferret 就像是一个能够理解图片和文字并将它们联系起来的智能系统。无论你在文本中提到图像的哪个部分，或者用怎样的方式描述，Ferret 都能准确地在图像中找到并识别出来。<br />
<br />
1、多模态理解：Ferret 能够同时处理和理解图像（视觉信息）和文本（语言信息），这使得它能够在多种不同的模式之间建立联系。<br />
<br />
2、空间指代理解：它能够识别和理解图像中特定区域的含义，即使这些区域的形状和大小各不相同。例如，如果文本提到图像中的某个特定部分，Ferret 能够识别出这部分是指什么。<br />
<br />
3、理解复杂的文本描述：Ferret 能够理解各种类型的文本描述，无论这些描述是具体的还是抽象的。比如，“图像中红色车辆旁边的小狗”或“画面右上角的笑脸”。<br />
<br />
4、放词汇描述精准定位：根据这些文本描述，Ferret 能够在提供的图像中准确地找到并标记出相应的物体或区域。例如，它可以识别并指出图像中的“小狗”或“笑脸”的确切位置。无论用户如何描述他们想要找到的图像中的内容，Ferret 都能理解并响应。<br />
<br />
5、混合区域表示：Ferret 使用一种创新的表示方法来处理图像中的区域。这种表示结合了离散坐标（如点或边界框的位置）和连续特征（如区域的视觉内容）。这允许模型理解和处理各种形状和大小的区域，从而提高了对图像的空间理解能力。<br />
<br />
6、空间感知的视觉采样器：为了处理不同形状的区域，Ferret 引入了一个空间感知的视觉采样器。这个采样器能够根据区域的形状和稀疏性提取视觉特征，使模型能够处理从简单点到复杂多边形等各种形状的区域。<br />
<br />
7、多样的区域输入：Ferret具有识别和理解图像中各种不同类型区域的能力。<br />
<br />
它可以处理以下类型的区域输入：<br />
<br />
点：Ferret 能够识别图像中的特定点，例如用户指定的一个具体位置。<br />
<br />
边界框：它可以识别和理解图像中的边界框，这些边界框通常用来标记图像中的物体或特定区域。<br />
<br />
自由形状：Ferret 还能处理更复杂的自由形状，比如手绘的轮廓、不规则图形或任意多边形。这种能力使得它可以更精确地识别和理解图像中的复杂区域。<br />
<br />
这种处理多样区域输入的能力使得 Ferret 在图像理解方面非常灵活和强大，能够适应各种不同的应用场景和用户需求。无论用户提供的是简单的点标记、常规的边界框，还是复杂的自由形状，Ferret 都能准确地识别和处理。<br />
<br />
8、GRIT 数据集：GRIT 数据集是专门为了训练和增强 Ferret 而收集的，包含了1.1M个样本。<br />
<br />
 这个数据集包含了丰富的层次空间知识，这意味着它涵盖了从简单物体到复杂空间关系的各种信息。 包含95K难负样本，这些是特别设计的挑战性样本，用于提高模型在处理困难情况下的鲁棒性和准确性。<br />
<br />
主要表现：<br />
<br />
1、Ferret-Bench评估：Ferret-Bench是为了评估Ferret而引入的一系列新任务，包括指称描述、指称推理和对话中的定位。在这些任务上，Ferret相比现有的最佳多模态大型语言模型（MLLM）平均提高了20.4%。这一结果表明Ferret在处理更复杂、更接近真实世界应用的任务时具有显著的优势。<br />
<br />
2、改善对象幻觉：Ferret 在描述图像细节时能够减少错误或虚构的内容，这在自动图像描述和分析领域尤为重要。<br />
它减轻了对象幻觉的问题，即在生成文本描述时减少了对不存在的对象的错误引用，提高了描述的准确性和可靠性。<br />
<br />
3、Ferret 不仅在传统的指代和定位任务中表现优异，它能够更准确地理解和处理图像中的空间信息和语义。而且在需要指代/定位、语义、知识和推理的任务中也表现出色。 <br />
<br />
Ferret 能够更准确地描述图像细节，减少在生成文本时对不存在的对象的幻觉。 通过其创新的方法和技术，为多模态语言模型在空间理解和定位方面提供了新的可能性，特别是在处理复杂的图像和文本交互时。<br />
<br />
适用于多种应用场景：<br />
<br />
由于其强大的图像和文本处理能力，Ferret 适用于多种应用场景，包括图像搜索、自动图像标注、交互式媒体探索等。<br />
<br />
GitHub：<a href="https://github.com/apple/ml-ferret">github.com/apple/ml-ferret</a><br />
论文：<a href="https://arxiv.org/abs/2310.07704">arxiv.org/abs/2310.07704</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NFODNUSGE4QUEyTUliLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NFODNUR2JJQUFQb040LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NFODNURWIwQUFUekpELmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738733784252686781#m</id>
            <title>奥特曼公布网友的新年愿望清单：

除了第一条备注了，其他都没

是不是暗示其他都能实现

😎

AGI（请耐心等待）
GPT-5
更好的语音模式
更高的速率限制
更好的 GPT
更好的推理
控制清醒/行为的程度
视频
个性化
更好的浏览
'使用 openai 登录'
开源</title>
            <link>https://nitter.cz/xiaohuggg/status/1738733784252686781#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738733784252686781#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 24 Dec 2023 01:30:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>奥特曼公布网友的新年愿望清单：<br />
<br />
除了第一条备注了，其他都没<br />
<br />
是不是暗示其他都能实现<br />
<br />
😎<br />
<br />
AGI（请耐心等待）<br />
GPT-5<br />
更好的语音模式<br />
更高的速率限制<br />
更好的 GPT<br />
更好的推理<br />
控制清醒/行为的程度<br />
视频<br />
个性化<br />
更好的浏览<br />
'使用 openai 登录'<br />
开源</p>
<p><a href="https://nitter.cz/sama/status/1738673279085457661#m">nitter.cz/sama/status/1738673279085457661#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738580319555743937#m</id>
            <title>TikTok Downloader：TikTok、抖音数据采集工具

- 视频和图集下载：支持批量下载TikTok和抖音的无水印视频和图集、喜欢的或收藏的作品。

- 数据采集：支持采集TikTok和抖音的详细数据，包括账号信息、评论数据、直播推流地址等。

- 多账号支持：支持多账号批量下载作品。

- 自动化功能：自动跳过已下载的文件，持久化保存采集数据。

- 多种模式支持：提供终端命令行模式、Web UI交互模式和Web API接口模式。

- 多平台兼容：支持Windows、macOS和Linux操作系统。

GitHub：https://github.com/JoeanAmier/TikTokDownloader</title>
            <link>https://nitter.cz/xiaohuggg/status/1738580319555743937#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738580319555743937#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 15:20:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>TikTok Downloader：TikTok、抖音数据采集工具<br />
<br />
- 视频和图集下载：支持批量下载TikTok和抖音的无水印视频和图集、喜欢的或收藏的作品。<br />
<br />
- 数据采集：支持采集TikTok和抖音的详细数据，包括账号信息、评论数据、直播推流地址等。<br />
<br />
- 多账号支持：支持多账号批量下载作品。<br />
<br />
- 自动化功能：自动跳过已下载的文件，持久化保存采集数据。<br />
<br />
- 多种模式支持：提供终端命令行模式、Web UI交互模式和Web API接口模式。<br />
<br />
- 多平台兼容：支持Windows、macOS和Linux操作系统。<br />
<br />
GitHub：<a href="https://github.com/JoeanAmier/TikTokDownloader">github.com/JoeanAmier/TikTok…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NDc0tDVGFVQUE4OFViLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738555147545104669#m</id>
            <title>这确实是牛P

我感觉是比原电影更好，哈哈哈

MJ未来最大的危机可能是来源于此，那就是版权问题

可能会被告破产😃</title>
            <link>https://nitter.cz/xiaohuggg/status/1738555147545104669#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738555147545104669#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 13:40:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这确实是牛P<br />
<br />
我感觉是比原电影更好，哈哈哈<br />
<br />
MJ未来最大的危机可能是来源于此，那就是版权问题<br />
<br />
可能会被告破产😃</p>
<p><a href="https://nitter.cz/Rahll/status/1738286342390374882#m">nitter.cz/Rahll/status/1738286342390374882#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738523813695086783#m</id>
            <title>这个日本小哥 @handaru20pF 开发了一个游戏手柄：ROS-Face

它通过向肌肉发送轻微的电冲击，使肌肉收缩，从而让手柄能控制人的面部肌肉...

你可以像玩游戏一样，用手柄让人做出各种表情。🤣

它在 GitHub 上分享了代码和详细的电路图：https://github.com/maHidaka/ros_face/blob/master/README.md</title>
            <link>https://nitter.cz/xiaohuggg/status/1738523813695086783#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738523813695086783#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 11:35:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个日本小哥 <a href="https://nitter.cz/handaru20pF" title="はんだる卝⁺">@handaru20pF</a> 开发了一个游戏手柄：ROS-Face<br />
<br />
它通过向肌肉发送轻微的电冲击，使肌肉收缩，从而让手柄能控制人的面部肌肉...<br />
<br />
你可以像玩游戏一样，用手柄让人做出各种表情。🤣<br />
<br />
它在 GitHub 上分享了代码和详细的电路图：<a href="https://github.com/maHidaka/ros_face/blob/master/README.md">github.com/maHidaka/ros_face…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg1MTQ3NDE5NzMzODUyMTYvcHUvaW1nL2JfemRvOWVTbmdaYWdwU0IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738512384749371524#m</id>
            <title>R to @xiaohuggg: DreamTuner支持局部编辑（如表情变化）和全局编辑（如场景和动作的变化），即使在复杂的文本输入下也能生成高度详细的图像。

生成的图像不仅细节丰富，而且准确保持了参考图像的细节。</title>
            <link>https://nitter.cz/xiaohuggg/status/1738512384749371524#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738512384749371524#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 10:50:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTuner支持局部编辑（如表情变化）和全局编辑（如场景和动作的变化），即使在复杂的文本输入下也能生成高度详细的图像。<br />
<br />
生成的图像不仅细节丰富，而且准确保持了参考图像的细节。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NCdnRLRGE0QUFfZXF0LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738511953499414598#m</id>
            <title>R to @xiaohuggg: DreamTuner 能够成功生成与文本输入一致且保留关键主题细节的高保真图像。</title>
            <link>https://nitter.cz/xiaohuggg/status/1738511953499414598#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738511953499414598#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 10:48:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DreamTuner 能够成功生成与文本输入一致且保留关键主题细节的高保真图像。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NCdkJlcGFFQUF5T1BYLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738511391093608762#m</id>
            <title>这个项目好，可以直接商用

DreamTuner ：通过单张图片实现主题驱动的图像生成

该项目由字节跳动开发，你只需要提供一张图片，DreamTuner就能帮你生成与这张图片在主题和风格上一致的新图像。

比如你有一张可乐照片，它可以根据你的要求将可乐放在任何场景中或添加其他元素形成一张完美海报！

这个工具特别适用于需要根据特定主题或条件创建个性化图像的场景。

主要功能特点：

1、文本控制的主题驱动图像生成：DreamTuner 能够根据文本输入生成与特定主题（如动漫角色）相关的图像。

它支持局部编辑（如表情变化）和全局编辑（如场景和动作的变化），即使在复杂的文本输入下也能生成高度详细的图像。

生成的图像不仅细节丰富，而且准确保持了参考图像的细节。

2、风格主题一致：在 DreamBooth 数据集上的评估显示，通过主题编码器和自主题注意力，生成了精细化的参考，使 DreamTuner 能够成功生成与文本输入一致且保留关键主题细节的高保真图像。

3、与 ControlNet 的结合：DreamTuner 的方法可以与 ControlNet 结合，扩展到不同条件（如姿势）的应用。
在一个示例中，仅使用一张图像进行 DreamTuner 的微调，其中参考图像的姿势作为参考条件。

为了确保帧间的连贯性，自主题注意力同时使用参考图像和生成图像的前一帧，分别赋予不同的参考权重。

DreamTuner 的工作原理：

1、主题编码器：当用户上传一张参考图片时，DreamTuner 首先使用主题编码器来分析这张图片。主题编码器主要是提取图片的基本特征，如颜色、形状、风格等，这些特征代表了图片的“粗略”或“大致”身份。

2、自主题注意力层：系统中的自主题注意力层进一步处理这些特征。这些层专注于细化图片的细节，如纹理、轮廓等，确保生成的图像在视觉上与原始图片保持一致。

3、文本到图像的转换：用户可以提供文本描述来指导图像的生成。例如，用户可能描述一个场景或动作。
DreamTuner 结合提取的图片特征和用户的文本描述，生成新的图像。

4、生成高保真图像：通过这种方式，DreamTuner 能够生成与原始参考图片在风格和主题上一致的高保真图像。

以下是一个具体的例子来解释 DreamTuner 的功能和作用：

例子：创建个性化动漫角色图像

假设你是一位动漫爱好者，想要创建一个全新的动漫角色图像，但只有一张参考图像和一些想法。

1、使用单张参考图像：

•你有一张喜欢的动漫角色的图像，想要基于这个角色创造一个新的场景或表情。

•使用 DreamTuner，你可以上传这张参考图像。这张图片包含了你想要的角色风格和一些基本特征，比如发型、服装风格。

2、添加文本描述：

•你想要这个角色在不同的场景中，比如“坐在公园的长椅上”或“手里拿着一杯茶，阳光透过窗户照射进来”。

•你可以将这些描述作为文本输入到 DreamTuner 中。

3、生成新的图像：

•DreamTuner 将使用你提供的参考图像和文本描述来生成新的图像。

•这个过程中，它会保留原始参考图像的关键特征（如角色的风格和特点），同时根据你的描述添加新的元素和场景。

•你将获得一系列根据你的描述生成的新动漫角色图像，这些图像既保留了原始角色的风格，又融入了新的场景和表情。

这个过程大大简化了从单一参考图像创造出一系列一致风格和主题的图像的过程。

DreamTuner 可以用于个性化的图像创作、动漫艺术设计、广告创意等领域，特别适合那些需要根据特定主题或条件快速生成高质量图像的场景。

项目及演示：https://dreamtuner-diffusion.github.io/
论文：https://arxiv.org/abs/2312.13691
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1738511391093608762#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738511391093608762#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 10:46:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个项目好，可以直接商用<br />
<br />
DreamTuner ：通过单张图片实现主题驱动的图像生成<br />
<br />
该项目由字节跳动开发，你只需要提供一张图片，DreamTuner就能帮你生成与这张图片在主题和风格上一致的新图像。<br />
<br />
比如你有一张可乐照片，它可以根据你的要求将可乐放在任何场景中或添加其他元素形成一张完美海报！<br />
<br />
这个工具特别适用于需要根据特定主题或条件创建个性化图像的场景。<br />
<br />
主要功能特点：<br />
<br />
1、文本控制的主题驱动图像生成：DreamTuner 能够根据文本输入生成与特定主题（如动漫角色）相关的图像。<br />
<br />
它支持局部编辑（如表情变化）和全局编辑（如场景和动作的变化），即使在复杂的文本输入下也能生成高度详细的图像。<br />
<br />
生成的图像不仅细节丰富，而且准确保持了参考图像的细节。<br />
<br />
2、风格主题一致：在 DreamBooth 数据集上的评估显示，通过主题编码器和自主题注意力，生成了精细化的参考，使 DreamTuner 能够成功生成与文本输入一致且保留关键主题细节的高保真图像。<br />
<br />
3、与 ControlNet 的结合：DreamTuner 的方法可以与 ControlNet 结合，扩展到不同条件（如姿势）的应用。<br />
在一个示例中，仅使用一张图像进行 DreamTuner 的微调，其中参考图像的姿势作为参考条件。<br />
<br />
为了确保帧间的连贯性，自主题注意力同时使用参考图像和生成图像的前一帧，分别赋予不同的参考权重。<br />
<br />
DreamTuner 的工作原理：<br />
<br />
1、主题编码器：当用户上传一张参考图片时，DreamTuner 首先使用主题编码器来分析这张图片。主题编码器主要是提取图片的基本特征，如颜色、形状、风格等，这些特征代表了图片的“粗略”或“大致”身份。<br />
<br />
2、自主题注意力层：系统中的自主题注意力层进一步处理这些特征。这些层专注于细化图片的细节，如纹理、轮廓等，确保生成的图像在视觉上与原始图片保持一致。<br />
<br />
3、文本到图像的转换：用户可以提供文本描述来指导图像的生成。例如，用户可能描述一个场景或动作。<br />
DreamTuner 结合提取的图片特征和用户的文本描述，生成新的图像。<br />
<br />
4、生成高保真图像：通过这种方式，DreamTuner 能够生成与原始参考图片在风格和主题上一致的高保真图像。<br />
<br />
以下是一个具体的例子来解释 DreamTuner 的功能和作用：<br />
<br />
例子：创建个性化动漫角色图像<br />
<br />
假设你是一位动漫爱好者，想要创建一个全新的动漫角色图像，但只有一张参考图像和一些想法。<br />
<br />
1、使用单张参考图像：<br />
<br />
•你有一张喜欢的动漫角色的图像，想要基于这个角色创造一个新的场景或表情。<br />
<br />
•使用 DreamTuner，你可以上传这张参考图像。这张图片包含了你想要的角色风格和一些基本特征，比如发型、服装风格。<br />
<br />
2、添加文本描述：<br />
<br />
•你想要这个角色在不同的场景中，比如“坐在公园的长椅上”或“手里拿着一杯茶，阳光透过窗户照射进来”。<br />
<br />
•你可以将这些描述作为文本输入到 DreamTuner 中。<br />
<br />
3、生成新的图像：<br />
<br />
•DreamTuner 将使用你提供的参考图像和文本描述来生成新的图像。<br />
<br />
•这个过程中，它会保留原始参考图像的关键特征（如角色的风格和特点），同时根据你的描述添加新的元素和场景。<br />
<br />
•你将获得一系列根据你的描述生成的新动漫角色图像，这些图像既保留了原始角色的风格，又融入了新的场景和表情。<br />
<br />
这个过程大大简化了从单一参考图像创造出一系列一致风格和主题的图像的过程。<br />
<br />
DreamTuner 可以用于个性化的图像创作、动漫艺术设计、广告创意等领域，特别适合那些需要根据特定主题或条件快速生成高质量图像的场景。<br />
<br />
项目及演示：<a href="https://dreamtuner-diffusion.github.io/">dreamtuner-diffusion.github.…</a><br />
论文：<a href="https://arxiv.org/abs/2312.13691">arxiv.org/abs/2312.13691</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0NCdXp4cmE0QUF0OHV4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1738144842658795780#m</id>
            <title>RT by @xiaohuggg: Midjourney V6 对中国文化和内容的理解是真的到位，不只是物品的形象，还有对应的审美和意境。
下面这几张图我恍惚间觉得甚至有点黑神话·悟空过场动画截图的感觉了。</title>
            <link>https://nitter.cz/op7418/status/1738144842658795780#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1738144842658795780#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Dec 2023 10:29:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Midjourney V6 对中国文化和内容的理解是真的到位，不只是物品的形象，还有对应的审美和意境。<br />
下面这几张图我恍惚间觉得甚至有点黑神话·悟空过场动画截图的感觉了。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4aGRRU2JVQUF3c284LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4aGRUYmFFQUE2UjkxLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4aGRVeWFNQUFCS3NaLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0I4aGRYM2JrQUFvTVFtLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738460003386958158#m</id>
            <title>今天GPT的输出速度贼快

不知道是周末原因还是更新了模型

反正感觉比平时快好几倍

我感觉可能是圣诞节礼物要来了🎄</title>
            <link>https://nitter.cz/xiaohuggg/status/1738460003386958158#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738460003386958158#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 07:22:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>今天GPT的输出速度贼快<br />
<br />
不知道是周末原因还是更新了模型<br />
<br />
反正感觉比平时快好几倍<br />
<br />
我感觉可能是圣诞节礼物要来了🎄</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzM4NDU5ODc1MjE3Mzk5ODA4L2ltZy9VNDNVTXItX2wwTmhORS1iLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738454788227117056#m</id>
            <title>这个好，图片上写文字老是不行

尤其是中文</title>
            <link>https://nitter.cz/xiaohuggg/status/1738454788227117056#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738454788227117056#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 07:01:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个好，图片上写文字老是不行<br />
<br />
尤其是中文</p>
<p><a href="https://nitter.cz/dotey/status/1738442390313029973#m">nitter.cz/dotey/status/1738442390313029973#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738435580516765958#m</id>
            <title>兄弟们 这个牛P！

有人做了一个AI视频搜索引擎，你可以像问问题一样搜索视频。

比如，你想知道“如何做蛋糕”，只需在这个网站上输入这个问题，它就会找到相关的视频来帮你解答。

更牛P的是，你还可以跟视频进行对话，就像跟一个人聊天一样，还能帮你总结视频内容。

最牛P的是他把这个项目开源了...

该搜索引擎使用语义搜索来根据用户的查询找到相关的视频片段。

截至目前，网站已经索引了约 276556 分钟（约 17245 个YouTube视频）的内容。

项目计划增加对 TikTok 视频的索引，使用 whisper 来转录没有字幕的视频，并自动抓取 YouTube 和 Tiktok 的内容以添加新视频。同时添加一个页面来查看所有活跃的聊天。

关于演示视频：估计是演示网站还比较粗糙，我登录了几次显示成功了，但是并没有成功，导致无法演示聊天和总结功能！

主要技术原理：

1、视频存储：从YouTube视频中提取视频的转录文本，即视频中所说话的内容。

这些转录文本被分成小块，每块都有一个时间戳，标记这段话在视频中出现的时间。

分块的依据是 Hugging Face 提供的一个特定的维度大小，这可能是指使用某种算法或模型来确定如何分割文本。

2、搜索功能：用户可以通过提出问题来搜索视频。

系统使用一种叫做“向量余弦搜索”的技术，这可能是指利用机器学习算法来找到与用户问题最相关的视频内容。

对于每个搜索结果的视频，系统会进行第二次搜索，目的是在视频中找到与用户问题最相关的具体片段。

3、摘要和聊天：视频的转录文本会被发送到一个叫做 JigsawStack 的API，这个API会生成视频内容的摘要，可能是以点形式列出关键信息，也可能是连贯的文本摘要。

JigsawStack API 还负责创建和管理聊天会话。在这些会话中，根据用户的问题，系统会发送与问题相关的视频片段。

网站：https://avse.vercel.app/
GitHub：https://github.com/yoeven/ai-video-search-engine</title>
            <link>https://nitter.cz/xiaohuggg/status/1738435580516765958#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738435580516765958#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 05:45:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 这个牛P！<br />
<br />
有人做了一个AI视频搜索引擎，你可以像问问题一样搜索视频。<br />
<br />
比如，你想知道“如何做蛋糕”，只需在这个网站上输入这个问题，它就会找到相关的视频来帮你解答。<br />
<br />
更牛P的是，你还可以跟视频进行对话，就像跟一个人聊天一样，还能帮你总结视频内容。<br />
<br />
最牛P的是他把这个项目开源了...<br />
<br />
该搜索引擎使用语义搜索来根据用户的查询找到相关的视频片段。<br />
<br />
截至目前，网站已经索引了约 276556 分钟（约 17245 个YouTube视频）的内容。<br />
<br />
项目计划增加对 TikTok 视频的索引，使用 whisper 来转录没有字幕的视频，并自动抓取 YouTube 和 Tiktok 的内容以添加新视频。同时添加一个页面来查看所有活跃的聊天。<br />
<br />
关于演示视频：估计是演示网站还比较粗糙，我登录了几次显示成功了，但是并没有成功，导致无法演示聊天和总结功能！<br />
<br />
主要技术原理：<br />
<br />
1、视频存储：从YouTube视频中提取视频的转录文本，即视频中所说话的内容。<br />
<br />
这些转录文本被分成小块，每块都有一个时间戳，标记这段话在视频中出现的时间。<br />
<br />
分块的依据是 Hugging Face 提供的一个特定的维度大小，这可能是指使用某种算法或模型来确定如何分割文本。<br />
<br />
2、搜索功能：用户可以通过提出问题来搜索视频。<br />
<br />
系统使用一种叫做“向量余弦搜索”的技术，这可能是指利用机器学习算法来找到与用户问题最相关的视频内容。<br />
<br />
对于每个搜索结果的视频，系统会进行第二次搜索，目的是在视频中找到与用户问题最相关的具体片段。<br />
<br />
3、摘要和聊天：视频的转录文本会被发送到一个叫做 JigsawStack 的API，这个API会生成视频内容的摘要，可能是以点形式列出关键信息，也可能是连贯的文本摘要。<br />
<br />
JigsawStack API 还负责创建和管理聊天会话。在这些会话中，根据用户的问题，系统会发送与问题相关的视频片段。<br />
<br />
网站：<a href="https://avse.vercel.app/">avse.vercel.app/</a><br />
GitHub：<a href="https://github.com/yoeven/ai-video-search-engine">github.com/yoeven/ai-video-s…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg0MzUyMDE2NzQ3Mzk3MTIvcHUvaW1nL3JHNFcxb3BvR2tUdTNjajAuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738425501990244399#m</id>
            <title>星巴克今天人好多

差点没抢到位置😐

这里的空调最暖和，网最快🤓</title>
            <link>https://nitter.cz/xiaohuggg/status/1738425501990244399#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738425501990244399#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 05:05:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>星巴克今天人好多<br />
<br />
差点没抢到位置😐<br />
<br />
这里的空调最暖和，网最快🤓</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738417698483405262#m</id>
            <title>AI画图进化史…</title>
            <link>https://nitter.cz/xiaohuggg/status/1738417698483405262#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738417698483405262#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 04:34:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI画图进化史…</p>
<p><a href="https://nitter.cz/AiBreakfast/status/1738259185085583427#m">nitter.cz/AiBreakfast/status/1738259185085583427#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1738407819035206105#m</id>
            <title>ControlRoom3D：根据用户定义的布局和和风格文字描述来设计3D房间。

你只需要在勾勒出房间的基本布局，如家具位置和房间结构，然后再添加关于风格的文字描述。

ControlRoom3D会自动生成详细且逼真的3D房间模型。

同时它还能自动对房间深度和物体表面的精确调整，比如桌椅大小比，摆放位置等。

工作原理：

1、用户输入：用户首先定义一个3D语义代理房间。这包括房间的基本布局（如墙壁、门窗的位置）和风格描述（如“现代简约”或“复古风格”）。这些信息是通过语义边界框和文本描述提供的。这些边界框帮助系统理解房间中每个部分的位置和功能，比如哪里是床，哪里是书桌。

2、生成3D代理房间：基于用户输入，ControlRoom3D 创建一个粗略的3D房间模型。这个模型不是最终产品，而是一个框架，用于指导后续的详细设计。

3、深度和纹理生成：接下来，系统利用强大的2D模型，根据3D代理房间的几何和语义信息，生成与之一致的纹理和几何形状。这意味着系统会根据房间布局和风格描述，自动添加适当的家具、装饰和纹理。

4、深度对齐和优化：ControlRoom3D 采用一种深度对齐模块，通过迭代优化对齐损失来调整生成的3D模型，以确保其与代理房间的布局紧密对齐。这一步骤是为了提高模型的准确性和真实感。

5、法线保留损失：为了防止物体表面因适应边界框而发生扭曲，ControlRoom3D 引入了法线保留损失。这意味着在调整物体以适应空间时，它们的原始形状和表面细节会被保留。

6、最终3D房间网格：经过上述步骤后，最终生成一个详细、准确且符合用户设计意图的3D房间网格。这个网格可以用于各种AR和VR应用，如虚拟会议、游戏或教育模拟等。

可以通过一个例子来解释：

例如：你需要设计一个虚拟会议室，你希望这个会议室既专业又有创意，但你并不擅长3D建模。

1、定义房间布局和风格：首先，你在ControlRoom3D中定义出会议室的基本布局，比如会议桌的位置、椅子的排列、以及窗户的位置。同时，你还提供了关于房间风格的描述，比如“现代简约风格，以蓝色和灰色为主色调”。

2、自动生成3D房间模型：ControlRoom3D利用你提供的信息，自动生成一个3D会议室模型。这个模型不仅遵循了你的布局要求，还融入了你描述的风格元素。

3、深度和细节调整：该系统还会自动调整房间中物体的深度和细节，确保一切看起来既真实又和谐。例如，会议桌和椅子的大小比例会被精确调整，以适应整个房间的空间感。

最终，你得到了一个既符合你设计意图又具有高度真实感的3D虚拟会议室。

通过这个例子，我们可以看出ControlRoom3D的主要作用和功能：它使非专业人士能够轻松设计出符合个人需求和风格的3D环境，这些环境既真实又具有高度的定制性，非常适合用于各种AR和VR应用。

项目及演示：https://jonasschult.github.io/ControlRoom3D/
论文：https://arxiv.org/abs/2312.05208
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1738407819035206105#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1738407819035206105#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Dec 2023 03:54:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ControlRoom3D：根据用户定义的布局和和风格文字描述来设计3D房间。<br />
<br />
你只需要在勾勒出房间的基本布局，如家具位置和房间结构，然后再添加关于风格的文字描述。<br />
<br />
ControlRoom3D会自动生成详细且逼真的3D房间模型。<br />
<br />
同时它还能自动对房间深度和物体表面的精确调整，比如桌椅大小比，摆放位置等。<br />
<br />
工作原理：<br />
<br />
1、用户输入：用户首先定义一个3D语义代理房间。这包括房间的基本布局（如墙壁、门窗的位置）和风格描述（如“现代简约”或“复古风格”）。这些信息是通过语义边界框和文本描述提供的。这些边界框帮助系统理解房间中每个部分的位置和功能，比如哪里是床，哪里是书桌。<br />
<br />
2、生成3D代理房间：基于用户输入，ControlRoom3D 创建一个粗略的3D房间模型。这个模型不是最终产品，而是一个框架，用于指导后续的详细设计。<br />
<br />
3、深度和纹理生成：接下来，系统利用强大的2D模型，根据3D代理房间的几何和语义信息，生成与之一致的纹理和几何形状。这意味着系统会根据房间布局和风格描述，自动添加适当的家具、装饰和纹理。<br />
<br />
4、深度对齐和优化：ControlRoom3D 采用一种深度对齐模块，通过迭代优化对齐损失来调整生成的3D模型，以确保其与代理房间的布局紧密对齐。这一步骤是为了提高模型的准确性和真实感。<br />
<br />
5、法线保留损失：为了防止物体表面因适应边界框而发生扭曲，ControlRoom3D 引入了法线保留损失。这意味着在调整物体以适应空间时，它们的原始形状和表面细节会被保留。<br />
<br />
6、最终3D房间网格：经过上述步骤后，最终生成一个详细、准确且符合用户设计意图的3D房间网格。这个网格可以用于各种AR和VR应用，如虚拟会议、游戏或教育模拟等。<br />
<br />
可以通过一个例子来解释：<br />
<br />
例如：你需要设计一个虚拟会议室，你希望这个会议室既专业又有创意，但你并不擅长3D建模。<br />
<br />
1、定义房间布局和风格：首先，你在ControlRoom3D中定义出会议室的基本布局，比如会议桌的位置、椅子的排列、以及窗户的位置。同时，你还提供了关于房间风格的描述，比如“现代简约风格，以蓝色和灰色为主色调”。<br />
<br />
2、自动生成3D房间模型：ControlRoom3D利用你提供的信息，自动生成一个3D会议室模型。这个模型不仅遵循了你的布局要求，还融入了你描述的风格元素。<br />
<br />
3、深度和细节调整：该系统还会自动调整房间中物体的深度和细节，确保一切看起来既真实又和谐。例如，会议桌和椅子的大小比例会被精确调整，以适应整个房间的空间感。<br />
<br />
最终，你得到了一个既符合你设计意图又具有高度真实感的3D虚拟会议室。<br />
<br />
通过这个例子，我们可以看出ControlRoom3D的主要作用和功能：它使非专业人士能够轻松设计出符合个人需求和风格的3D环境，这些环境既真实又具有高度的定制性，非常适合用于各种AR和VR应用。<br />
<br />
项目及演示：<a href="https://jonasschult.github.io/ControlRoom3D/">jonasschult.github.io/Contro…</a><br />
论文：<a href="https://arxiv.org/abs/2312.05208">arxiv.org/abs/2312.05208</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3Mzg0MDczMTIzMDg3MTU1MjAvcHUvaW1nL29uVDV3Y0xQM3RZaVJ0U0kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>