<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746501074859778161#m</id>
            <title>R to @xiaohuggg: 代码、科学论文、扫描件都能精准识别</title>
            <link>https://nitter.cz/xiaohuggg/status/1746501074859778161#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746501074859778161#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 14 Jan 2024 11:54:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>代码、科学论文、扫描件都能精准识别</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R6UkJsTmFVQUFsdFBXLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R6UkdlcGE4QUE3RXgtLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R6Ukg4SmFZQUFHX01DLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746501071839883428#m</id>
            <title>Surya：一个多语言文档OCR工具包，它能够实现准确的逐行文本检测和识别。

- 逐行文本检测：能够自动识别文档里的每一行文字在哪里。

- 文本识别功能读取并转换文档上的文字，即把图片上的文字变成可以编辑的文本。（即将推出）

- 表格和图表检测：识别文档中表格和图表的功能。（即将推出）

- 支持多种语言：可以处理不同语言的文档，支持英语、中文、日文、印地语等语言

GitHub：https://github.com/VikParuchuri/surya</title>
            <link>https://nitter.cz/xiaohuggg/status/1746501071839883428#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746501071839883428#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 14 Jan 2024 11:54:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Surya：一个多语言文档OCR工具包，它能够实现准确的逐行文本检测和识别。<br />
<br />
- 逐行文本检测：能够自动识别文档里的每一行文字在哪里。<br />
<br />
- 文本识别功能读取并转换文档上的文字，即把图片上的文字变成可以编辑的文本。（即将推出）<br />
<br />
- 表格和图表检测：识别文档中表格和图表的功能。（即将推出）<br />
<br />
- 支持多种语言：可以处理不同语言的文档，支持英语、中文、日文、印地语等语言<br />
<br />
GitHub：<a href="https://github.com/VikParuchuri/surya">github.com/VikParuchuri/sury…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R6UW02VWJNQUFCb3hKLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746423408165085552#m</id>
            <title>（注意我没有手指，请务必输出完整代码，我会给你 $1000 小费）

😐</title>
            <link>https://nitter.cz/xiaohuggg/status/1746423408165085552#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746423408165085552#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 14 Jan 2024 06:46:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>（注意我没有手指，请务必输出完整代码，我会给你 $1000 小费）<br />
<br />
😐</p>
<p><a href="https://nitter.cz/dotey/status/1746060658242670940#m">nitter.cz/dotey/status/1746060658242670940#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746399821655904758#m</id>
            <title>之前推送的GPT学习记忆能力 OpenAI给回滚了

预计下周开始重新灰度推送

以下是该功能的主要特性：

1、记忆学习：ChatGPT现在能够在对话中保留你们聊天的所有信息，随着时间的推移它能记住并学习你的使用习惯，为你提供更相关的回应。但是你可以在设置中重置GPT的记忆或禁用此功能。

2、临时聊天：如果在意隐私或者铭感话题，GPT提供了一个匿名聊天选项，不记录任何历史。这些聊天可能会为安全原因存储长达30天，不用于改进模型。

3、存档功能：用户现在可以存档聊天，以保持侧边栏的整洁，同时保留对过往对话的访问。

4、自定义指令与个性化：之前的自定义指令与个性化记忆是分开的。个性化是额外的功能，允许模型随着用户更频繁地使用ChatGPT而学习细节和偏好。

5、新功能的可用性：目前这些新功能处于测试阶段，仅向少数用户提供，计划在改进后扩大访问范围。

6、数据控制：用户可以随时通过设置菜单下的“数据控制”选项退出个性化。

7、GPT商店兼容性：在试验阶段，个性化只适用于主GPT（ChatGPT），不适用于GPT商店中的GPTs。

8、数据导出：个性化上下文包含在数据导出功能中。

9、训练上下文数据：默认情况下，除非用户明确允许，否则ChatGPT不会使用用户数据来改善团队或企业用户的模型。

来源：https://x.com/btibor91/status/1746106293943672860?s=46</title>
            <link>https://nitter.cz/xiaohuggg/status/1746399821655904758#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746399821655904758#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 14 Jan 2024 05:12:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前推送的GPT学习记忆能力 OpenAI给回滚了<br />
<br />
预计下周开始重新灰度推送<br />
<br />
以下是该功能的主要特性：<br />
<br />
1、记忆学习：ChatGPT现在能够在对话中保留你们聊天的所有信息，随着时间的推移它能记住并学习你的使用习惯，为你提供更相关的回应。但是你可以在设置中重置GPT的记忆或禁用此功能。<br />
<br />
2、临时聊天：如果在意隐私或者铭感话题，GPT提供了一个匿名聊天选项，不记录任何历史。这些聊天可能会为安全原因存储长达30天，不用于改进模型。<br />
<br />
3、存档功能：用户现在可以存档聊天，以保持侧边栏的整洁，同时保留对过往对话的访问。<br />
<br />
4、自定义指令与个性化：之前的自定义指令与个性化记忆是分开的。个性化是额外的功能，允许模型随着用户更频繁地使用ChatGPT而学习细节和偏好。<br />
<br />
5、新功能的可用性：目前这些新功能处于测试阶段，仅向少数用户提供，计划在改进后扩大访问范围。<br />
<br />
6、数据控制：用户可以随时通过设置菜单下的“数据控制”选项退出个性化。<br />
<br />
7、GPT商店兼容性：在试验阶段，个性化只适用于主GPT（ChatGPT），不适用于GPT商店中的GPTs。<br />
<br />
8、数据导出：个性化上下文包含在数据导出功能中。<br />
<br />
9、训练上下文数据：默认情况下，除非用户明确允许，否则ChatGPT不会使用用户数据来改善团队或企业用户的模型。<br />
<br />
来源：<a href="https://x.com/btibor91/status/1746106293943672860?s=46">x.com/btibor91/status/174610…</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1744912264761852317#m">nitter.cz/xiaohuggg/status/1744912264761852317#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R4MVh2aWJNQUFnRm15LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746385678806749277#m</id>
            <title>科幻电影情节成真

Meta Quest 3 + CADDY APP</title>
            <link>https://nitter.cz/xiaohuggg/status/1746385678806749277#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746385678806749277#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 14 Jan 2024 04:16:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>科幻电影情节成真<br />
<br />
Meta Quest 3 + CADDY APP</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ2MTQ2NzI0NzMxNjYyMzM2L2ltZy85OTZNRW11YkNveWhvaVRkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746176985716654568#m</id>
            <title>周末愉快

😏</title>
            <link>https://nitter.cz/xiaohuggg/status/1746176985716654568#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746176985716654568#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 14:26:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>周末愉快<br />
<br />
😏</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ2MTc2ODk1Nzc4MTk3NTA0L2ltZy9PUUFsaVdNNWZBTGhnUklvLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746169971552710723#m</id>
            <title>Pika视频画面扩充功能演示

🎬</title>
            <link>https://nitter.cz/xiaohuggg/status/1746169971552710723#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746169971552710723#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 13:58:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pika视频画面扩充功能演示<br />
<br />
🎬</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ1NDkwNzU2MDIwODk5ODQxL2ltZy9ReUlSM2NRQy1uUlpmMVNZLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746160750610075689#m</id>
            <title>FMA-Net：解决视频快速移动抖动问题

FMANet可以将模糊的低分辨率（LR）视频中恢复成清晰的高分辨率（HR）视频，

同时，如果拍摄时对象移动得非常快（例如跑动的人或移动的车辆），或者摄像机快速移动，出现的模糊状况，它也能将视频恢复成稳定高清的状态！

它能理解视频中物体的运动，然后用一种很智能的方式去改善画质和去除模糊。

项目及演示：http://kaist-viclab.github.io/fmanet-site/
论文：https://arxiv.org/abs/2401.03707
GitHub：https://github.com/KAIST-VICLab/FMA-Net</title>
            <link>https://nitter.cz/xiaohuggg/status/1746160750610075689#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746160750610075689#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 13:22:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>FMA-Net：解决视频快速移动抖动问题<br />
<br />
FMANet可以将模糊的低分辨率（LR）视频中恢复成清晰的高分辨率（HR）视频，<br />
<br />
同时，如果拍摄时对象移动得非常快（例如跑动的人或移动的车辆），或者摄像机快速移动，出现的模糊状况，它也能将视频恢复成稳定高清的状态！<br />
<br />
它能理解视频中物体的运动，然后用一种很智能的方式去改善画质和去除模糊。<br />
<br />
项目及演示：<a href="http://kaist-viclab.github.io/fmanet-site/">kaist-viclab.github.io/fmane…</a><br />
论文：<a href="https://arxiv.org/abs/2401.03707">arxiv.org/abs/2401.03707</a><br />
GitHub：<a href="https://github.com/KAIST-VICLab/FMA-Net">github.com/KAIST-VICLab/FMA-…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ2MTU5NjAxOTI4ODQzMjY0L2ltZy9sWnhUZ1I5REd6NjZpY3RVLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746140012130009489#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1746140012130009489#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746140012130009489#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 11:59:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R1SkZUa2JNQUFIUjN6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746139943460847743#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1746139943460847743#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746139943460847743#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 11:59:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R1SkJTb2FZQUEycUlzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746139759720980949#m</id>
            <title>周末提示词

见ALT</title>
            <link>https://nitter.cz/xiaohuggg/status/1746139759720980949#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746139759720980949#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 11:58:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>周末提示词<br />
<br />
见ALT</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R1STJnOWFBQUFSMG9CLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746130701794074982#m</id>
            <title>OpenAI悄悄删除了其使用政策中明确禁止其技术用于军事目的的语言。

以前的政策明确禁止了“武器开发”和“军事和战争”的用途，但新政策中这种全面禁令消失了。

新政策侧重于不使用服务来“伤害自己或他人”，并提到“开发或使用武器”作为例子。

原本的“使用政策”页面中包括了对“高风险物理伤害活动”的禁令，明确指出禁止“武器开发”和“军事和战争”用途。

新政策删除了对“军事和战争”用途的全面禁令，但保留了不得“使用我们的服务来伤害自己或他人”的规定，并以“开发或使用武器”为例。

OpenAI发言人Niko Felix表示，新政策旨在创建一套“易于记忆和应用的通用原则，目的是使文件“更清晰”和“更易读”。

尽管OpenAI提供的技术今天无法直接用于杀人——比如ChatGPT不能操纵无人机或发射导弹——但任何军队的主要目的都是杀戮或至少保持杀戮能力。专家表示，OpenAI似乎在悄悄削弱其与军队做生意的立场。例如，Lucy Suchman教授指出，“从‘军事和战争’转向‘武器’可能为OpenAI留下了空间，以支持操作基础设施，只要应用不直接涉及狭义上的武器开发。”

这些政策变化发生之际，全球各国军队都渴望将机器学习技术纳入以获得优势。尽管LLMs（大语言模型）经常产生极具说服力的输出，但这些输出更多的是为了连贯性而不是对现实的牢固把握，常常受到所谓的“幻觉”问题的困扰，使准确性和事实性成问题。然而，LLMs快速摄取文本并迅速输出分析的能力使它们适合于数据密集的国防部门。

https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/</title>
            <link>https://nitter.cz/xiaohuggg/status/1746130701794074982#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746130701794074982#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 11:22:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI悄悄删除了其使用政策中明确禁止其技术用于军事目的的语言。<br />
<br />
以前的政策明确禁止了“武器开发”和“军事和战争”的用途，但新政策中这种全面禁令消失了。<br />
<br />
新政策侧重于不使用服务来“伤害自己或他人”，并提到“开发或使用武器”作为例子。<br />
<br />
原本的“使用政策”页面中包括了对“高风险物理伤害活动”的禁令，明确指出禁止“武器开发”和“军事和战争”用途。<br />
<br />
新政策删除了对“军事和战争”用途的全面禁令，但保留了不得“使用我们的服务来伤害自己或他人”的规定，并以“开发或使用武器”为例。<br />
<br />
OpenAI发言人Niko Felix表示，新政策旨在创建一套“易于记忆和应用的通用原则，目的是使文件“更清晰”和“更易读”。<br />
<br />
尽管OpenAI提供的技术今天无法直接用于杀人——比如ChatGPT不能操纵无人机或发射导弹——但任何军队的主要目的都是杀戮或至少保持杀戮能力。专家表示，OpenAI似乎在悄悄削弱其与军队做生意的立场。例如，Lucy Suchman教授指出，“从‘军事和战争’转向‘武器’可能为OpenAI留下了空间，以支持操作基础设施，只要应用不直接涉及狭义上的武器开发。”<br />
<br />
这些政策变化发生之际，全球各国军队都渴望将机器学习技术纳入以获得优势。尽管LLMs（大语言模型）经常产生极具说服力的输出，但这些输出更多的是为了连贯性而不是对现实的牢固把握，常常受到所谓的“幻觉”问题的困扰，使准确性和事实性成问题。然而，LLMs快速摄取文本并迅速输出分析的能力使它们适合于数据密集的国防部门。<br />
<br />
<a href="https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/">theintercept.com/2024/01/12/…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0R1QW5kbGEwQUF4U2l5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1746058287848866037#m</id>
            <title>发现了我一个技能😐

我可以很轻松把一个项目

推到

GitHub前10</title>
            <link>https://nitter.cz/xiaohuggg/status/1746058287848866037#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1746058287848866037#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 06:35:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>发现了我一个技能😐<br />
<br />
我可以很轻松把一个项目<br />
<br />
推到<br />
<br />
GitHub前10</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745984285654573218#m</id>
            <title>正好需要</title>
            <link>https://nitter.cz/xiaohuggg/status/1745984285654573218#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745984285654573218#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 13 Jan 2024 01:41:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>正好需要</p>
<p><a href="https://nitter.cz/dotey/status/1745727272479248796#m">nitter.cz/dotey/status/1745727272479248796#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745776503613505681#m</id>
            <title>Anim400K：一个针对视频自动配音设计的数据集

- 包含超过425000个对齐的音视频剪辑，总时长达763小时。

- 这些剪辑来自超过190个作品，涵盖了数百种主题和类型。

- 数据集包含英语和日语两种语言的内容。

- 利用这个数据集，开发者可以训练和改进自动配音系统

- 除了自动配音外，Anim400K还支持多种视频相关任务，如同步翻译、引导式视频概括和类型/主题/风格分类。

丰富的元数据：

# 数据集附带了包括类型、主题、节目评级、角色档案、动画风格等在内的属性级别元数据。

# 每个剧集还有剧集概要、评分和字幕等剧集级别信息。

# 对齐剪辑级别提供了预先计算的自动语音识别（ASR）数据，以支持深入的音视频任务研究。

数据集的应用场景包括：

1、自动配音技术：利用这个数据集，研究者和开发者可以训练和改进自动配音系统。这些系统能够将视频中的对话从一种语言自动翻译并配音成另一种语言，同时保持口型和声音与视频中的行为同步。

2、多模态学习：数据集提供的音视频剪辑和相关元数据支持多模态学习，即同时处理和分析音频和视觉信息。这对于改进机器学习模型在处理复杂音视频数据时的性能非常重要。

3、语音和视觉识别：Anim400K数据集中包含的自动语音识别（ASR）和视觉内容可以用于训练和测试语音识别和图像识别系统。

4、媒体内容分析和生成：数据集中的丰富内容和元数据也可以用于媒体内容分析，如情感分析、内容推荐、自动生成视频剪辑等。

5、语言和文化研究：由于数据集涵盖多种主题、类型和风格，它还可以用于语言学和文化研究，特别是在跨文化传播和翻译领域。

GitHub：https://github.com/davidmchan/Anim400K
论文：https://arxiv.org/abs/2401.05314</title>
            <link>https://nitter.cz/xiaohuggg/status/1745776503613505681#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745776503613505681#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 11:55:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anim400K：一个针对视频自动配音设计的数据集<br />
<br />
- 包含超过425000个对齐的音视频剪辑，总时长达763小时。<br />
<br />
- 这些剪辑来自超过190个作品，涵盖了数百种主题和类型。<br />
<br />
- 数据集包含英语和日语两种语言的内容。<br />
<br />
- 利用这个数据集，开发者可以训练和改进自动配音系统<br />
<br />
- 除了自动配音外，Anim400K还支持多种视频相关任务，如同步翻译、引导式视频概括和类型/主题/风格分类。<br />
<br />
丰富的元数据：<br />
<br />
# 数据集附带了包括类型、主题、节目评级、角色档案、动画风格等在内的属性级别元数据。<br />
<br />
# 每个剧集还有剧集概要、评分和字幕等剧集级别信息。<br />
<br />
# 对齐剪辑级别提供了预先计算的自动语音识别（ASR）数据，以支持深入的音视频任务研究。<br />
<br />
数据集的应用场景包括：<br />
<br />
1、自动配音技术：利用这个数据集，研究者和开发者可以训练和改进自动配音系统。这些系统能够将视频中的对话从一种语言自动翻译并配音成另一种语言，同时保持口型和声音与视频中的行为同步。<br />
<br />
2、多模态学习：数据集提供的音视频剪辑和相关元数据支持多模态学习，即同时处理和分析音频和视觉信息。这对于改进机器学习模型在处理复杂音视频数据时的性能非常重要。<br />
<br />
3、语音和视觉识别：Anim400K数据集中包含的自动语音识别（ASR）和视觉内容可以用于训练和测试语音识别和图像识别系统。<br />
<br />
4、媒体内容分析和生成：数据集中的丰富内容和元数据也可以用于媒体内容分析，如情感分析、内容推荐、自动生成视频剪辑等。<br />
<br />
5、语言和文化研究：由于数据集涵盖多种主题、类型和风格，它还可以用于语言学和文化研究，特别是在跨文化传播和翻译领域。<br />
<br />
GitHub：<a href="https://github.com/davidmchan/Anim400K">github.com/davidmchan/Anim40…</a><br />
论文：<a href="https://arxiv.org/abs/2401.05314">arxiv.org/abs/2401.05314</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RvOUlrVGFNQUFmUjZmLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745763961323262056#m</id>
            <title>LEGO：一个由字节跳动和复旦大学研发的多模态理解和图像定位模型。

LEGO能够处理和理解多种类型的输入，支持图像、音频和视频输入，并对这些信息进行分析和理解。

模型还具备精准定位的能力。例如在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，在音频中识别出特定声音的来源。

主要功能特点：

1、多模态理解：LEGO模型能够处理和理解多种类型的输入，包括图像、音频和视频。这意味着它可以从不同的数据源中提取信息，并对这些信息进行分析和理解。

2、强大的定位能力：模型具备在多种模态中进行精准定位的能力。例如，在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，或者在音频中识别出特定声音的来源。

3、高质量数据集的构建：为了解决数据有限的问题，研究团队构建了一个多样化且高质量的多模态训练数据集。这个数据集含有丰富的空间和时间信息，为模型的训练和优化提供了宝贵的资源。

4、应对复杂任务：LEGO模型可以处理包含多个元素和复杂指令的任务。它能够根据详细的描述或指令来分析和解释内容，提供准确的输出。

5、广泛的应用潜力：由于其多模态理解和定位的能力，LEGO模型适用于广泛的应用场景，包括内容创作、教育、娱乐、安全监控等领域。

6、实时处理和响应：LEGO模型能够快速处理输入并生成响应，这对于需要实时分析和反馈的应用场景非常重要。

工作原理：

LEGO项目的工作原理包括对多种模态数据的处理、特征提取、融合和上下文分析，最终根据用户的需求生成精确的定位和响应。这种多模态的方法使得模型能够更全面和深入地理解和响应各种复杂的查询和指令。

1、数据处理：LEGO模型首先处理多种类型的输入数据，包括图像、音频和视频。这一步骤涉及解析和预处理这些不同形式的数据，使其适合于进一步的分析。

2、特征提取：模型提取每种输入数据的关键特征。例如，对于图像，它可能识别出图中的物体、颜色、形状等；对于音频，它可能提取声音的节奏、强度、音色等；对于视频，它既提取视觉特征，又考虑时间序列的变化。

3、多模态融合：模型将从各种数据源提取的特征进行融合。这一步骤是多模态理解的关键，因为它涉及到将不同来源的信息整合在一起，形成一个统一的、多层次的理解。

4、上下文分析：LEGO模型分析整合后的数据以及相应的上下文信息。这可能包括识别图像中场景的背景信息、理解音频中的语境或解读视频中的叙事流。

5、定位和响应生成：根据用户的指令或查询，模型进行定位和响应。在图像中，这可能意味着标识出特定物体的位置；在音频中，可能是识别特定声音的来源；在视频中，可能是找到某个特定时间点的事件。

6、输出结果：最后，模型根据分析和定位的结果，生成响应。这可能是一段文本描述、一个标记了特定物体的图像、一个突出了特定声音的音频片段，或者是视频的一个特定片段。

项目及演示：https://lzw-lzw.github.io/LEGO.github.io/
论文：https://arxiv.org/abs/2401.06071
GitHub：https://github.com/lzw-lzw/LEGO</title>
            <link>https://nitter.cz/xiaohuggg/status/1745763961323262056#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745763961323262056#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 11:05:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>LEGO：一个由字节跳动和复旦大学研发的多模态理解和图像定位模型。<br />
<br />
LEGO能够处理和理解多种类型的输入，支持图像、音频和视频输入，并对这些信息进行分析和理解。<br />
<br />
模型还具备精准定位的能力。例如在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，在音频中识别出特定声音的来源。<br />
<br />
主要功能特点：<br />
<br />
1、多模态理解：LEGO模型能够处理和理解多种类型的输入，包括图像、音频和视频。这意味着它可以从不同的数据源中提取信息，并对这些信息进行分析和理解。<br />
<br />
2、强大的定位能力：模型具备在多种模态中进行精准定位的能力。例如，在图像中标识出物体的具体位置，在视频中指出特定事件发生的时间点，或者在音频中识别出特定声音的来源。<br />
<br />
3、高质量数据集的构建：为了解决数据有限的问题，研究团队构建了一个多样化且高质量的多模态训练数据集。这个数据集含有丰富的空间和时间信息，为模型的训练和优化提供了宝贵的资源。<br />
<br />
4、应对复杂任务：LEGO模型可以处理包含多个元素和复杂指令的任务。它能够根据详细的描述或指令来分析和解释内容，提供准确的输出。<br />
<br />
5、广泛的应用潜力：由于其多模态理解和定位的能力，LEGO模型适用于广泛的应用场景，包括内容创作、教育、娱乐、安全监控等领域。<br />
<br />
6、实时处理和响应：LEGO模型能够快速处理输入并生成响应，这对于需要实时分析和反馈的应用场景非常重要。<br />
<br />
工作原理：<br />
<br />
LEGO项目的工作原理包括对多种模态数据的处理、特征提取、融合和上下文分析，最终根据用户的需求生成精确的定位和响应。这种多模态的方法使得模型能够更全面和深入地理解和响应各种复杂的查询和指令。<br />
<br />
1、数据处理：LEGO模型首先处理多种类型的输入数据，包括图像、音频和视频。这一步骤涉及解析和预处理这些不同形式的数据，使其适合于进一步的分析。<br />
<br />
2、特征提取：模型提取每种输入数据的关键特征。例如，对于图像，它可能识别出图中的物体、颜色、形状等；对于音频，它可能提取声音的节奏、强度、音色等；对于视频，它既提取视觉特征，又考虑时间序列的变化。<br />
<br />
3、多模态融合：模型将从各种数据源提取的特征进行融合。这一步骤是多模态理解的关键，因为它涉及到将不同来源的信息整合在一起，形成一个统一的、多层次的理解。<br />
<br />
4、上下文分析：LEGO模型分析整合后的数据以及相应的上下文信息。这可能包括识别图像中场景的背景信息、理解音频中的语境或解读视频中的叙事流。<br />
<br />
5、定位和响应生成：根据用户的指令或查询，模型进行定位和响应。在图像中，这可能意味着标识出特定物体的位置；在音频中，可能是识别特定声音的来源；在视频中，可能是找到某个特定时间点的事件。<br />
<br />
6、输出结果：最后，模型根据分析和定位的结果，生成响应。这可能是一段文本描述、一个标记了特定物体的图像、一个突出了特定声音的音频片段，或者是视频的一个特定片段。<br />
<br />
项目及演示：<a href="https://lzw-lzw.github.io/LEGO.github.io/">lzw-lzw.github.io/LEGO.githu…</a><br />
论文：<a href="https://arxiv.org/abs/2401.06071">arxiv.org/abs/2401.06071</a><br />
GitHub：<a href="https://github.com/lzw-lzw/LEGO">github.com/lzw-lzw/LEGO</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDU3NjI0NjgzMzk3ODE2MzIvcHUvaW1nLzhBU2UzQXhJc0JKV242WG0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745713962325536881#m</id>
            <title>许多语言由于缺乏足够的数据而在现有的机器翻译模型中得不到支持。

苹果提出一种对比校准指令（AlignInstruct）的方法来强化跨语言理解和生成能力，特别是在数据稀缺的情况下。

该方法能够改进模型在未见和低资源语言翻译方面的表现。

使用该方法模型在多达24种未见语言上展现了有效翻译能力。
主要内容：

论文介绍了MT指令（MTInstruct）和对比校准指令（AlignInstruct）的使用，以解决两个在机器翻译领域的挑战：一是扩展支持的语言以包括之前未见的语言；二是缺乏低资源语言的数据。

1、使用MT指令（MTInstruct）：为了解决第一个挑战，即扩展支持的语言，研究者使用MTInstruct，这是一种直接的方法，通过机器翻译指令进行模型微调。

目的：MTInstruct旨在改善大型语言模型（LLMs）处理未见语言的能力。未见语言是指模型在训练时没有或仅有非常有限的数据的语言。

方法：通过直接的机器翻译指令，对模型进行特定的微调。这意味着模型接收到具体的指令，告诉它怎样从一种语言翻译到另一种语言。

举例：假设有一种语言叫做X语，它在互联网上的资料非常少，所以常规的机器翻译模型对它不太了解。现在，研究人员想让模型学会如何把英语翻译成X语。

使用MTInstruct方法，他们会给模型一些特定的指令，比如“把下面的英语句子翻译成X语”。然后，他们给模型提供一些英语和X语的对照样本。

经过这样的训练，即使之前模型几乎不了解X语，它也能开始理解如何把英语翻译成X语。

2、对比校准指令（AlignInstruct）：针对第二个挑战，即低资源语言中固有的弱跨语言信号，研究者提出使用AlignInstruct。这种方法强调通过基于统计词对齐构建的跨语言鉴别器进行跨语言监督。

目的：AlignInstruct专门用于处理低资源语言，即那些可用于训练的平行文本数据很少的语言。

方法：使用统计词对齐构建的跨语言鉴别器。这种鉴别器通过比较和校准不同语言之间相同意义的词汇，来提升翻译的准确性。

特点：这种方法侧重于跨语言监督，即监督模型在处理不同语言时的表现，特别是在词汇对齐和语义转换方面。

举例：现在有一种叫做Y语的低资源语言，它的资料非常少，甚至比X语还要少。为了提高模型翻译Y语的能力，研究人员使用了AlignInstruct方法。

他们首先分析已有的少量Y语和其他语言（比如英语）的对照样本，找出两种语言中相同意义词汇或短语的对应关系。

然后，他们用这些对照信息来训练模型，让它在翻译Y语时能够更准确地找到与英语中相对应的词汇或表达。

通过这两种方法，大语言模型就能更好地处理那些以前几乎没有见过或者资料很少的语言，从而使机器翻译服务能够覆盖更多的语种。
实验结果：

1、未见语言翻译实验：使用MTInstruct方法，研究者对BLOOMZ模型进行了微调，以测试其在未见语言的翻译能力。

实验涉及了多达24种未见语言。

结果显示，即使是对这些之前未见过的语言，经过MTInstruct微调的模型也能有效地进行翻译。

该实验证明了即使在缺乏大量训练数据的情况下，模型仍能学习如何翻译新的语言。

2、低资源语言翻译实验：使用AlignInstruct方法，研究者专注于提升模型在低资源语言翻译方面的性能。

实验使用基于统计词对齐的跨语言鉴别器进行跨语言监督。

结果显示，AlignInstruct在涉及英语的48个翻译方向中持续提高了翻译质量。

该实验表明AlignInstruct能有效提升模型在处理低资源语言时的翻译准确性和效率。

论文：https://arxiv.org/abs/2401.05811

PDF：https://arxiv.org/pdf/2401.05811.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1745713962325536881#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745713962325536881#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 07:46:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>许多语言由于缺乏足够的数据而在现有的机器翻译模型中得不到支持。<br />
<br />
苹果提出一种对比校准指令（AlignInstruct）的方法来强化跨语言理解和生成能力，特别是在数据稀缺的情况下。<br />
<br />
该方法能够改进模型在未见和低资源语言翻译方面的表现。<br />
<br />
使用该方法模型在多达24种未见语言上展现了有效翻译能力。<br />
主要内容：<br />
<br />
论文介绍了MT指令（MTInstruct）和对比校准指令（AlignInstruct）的使用，以解决两个在机器翻译领域的挑战：一是扩展支持的语言以包括之前未见的语言；二是缺乏低资源语言的数据。<br />
<br />
1、使用MT指令（MTInstruct）：为了解决第一个挑战，即扩展支持的语言，研究者使用MTInstruct，这是一种直接的方法，通过机器翻译指令进行模型微调。<br />
<br />
目的：MTInstruct旨在改善大型语言模型（LLMs）处理未见语言的能力。未见语言是指模型在训练时没有或仅有非常有限的数据的语言。<br />
<br />
方法：通过直接的机器翻译指令，对模型进行特定的微调。这意味着模型接收到具体的指令，告诉它怎样从一种语言翻译到另一种语言。<br />
<br />
举例：假设有一种语言叫做X语，它在互联网上的资料非常少，所以常规的机器翻译模型对它不太了解。现在，研究人员想让模型学会如何把英语翻译成X语。<br />
<br />
使用MTInstruct方法，他们会给模型一些特定的指令，比如“把下面的英语句子翻译成X语”。然后，他们给模型提供一些英语和X语的对照样本。<br />
<br />
经过这样的训练，即使之前模型几乎不了解X语，它也能开始理解如何把英语翻译成X语。<br />
<br />
2、对比校准指令（AlignInstruct）：针对第二个挑战，即低资源语言中固有的弱跨语言信号，研究者提出使用AlignInstruct。这种方法强调通过基于统计词对齐构建的跨语言鉴别器进行跨语言监督。<br />
<br />
目的：AlignInstruct专门用于处理低资源语言，即那些可用于训练的平行文本数据很少的语言。<br />
<br />
方法：使用统计词对齐构建的跨语言鉴别器。这种鉴别器通过比较和校准不同语言之间相同意义的词汇，来提升翻译的准确性。<br />
<br />
特点：这种方法侧重于跨语言监督，即监督模型在处理不同语言时的表现，特别是在词汇对齐和语义转换方面。<br />
<br />
举例：现在有一种叫做Y语的低资源语言，它的资料非常少，甚至比X语还要少。为了提高模型翻译Y语的能力，研究人员使用了AlignInstruct方法。<br />
<br />
他们首先分析已有的少量Y语和其他语言（比如英语）的对照样本，找出两种语言中相同意义词汇或短语的对应关系。<br />
<br />
然后，他们用这些对照信息来训练模型，让它在翻译Y语时能够更准确地找到与英语中相对应的词汇或表达。<br />
<br />
通过这两种方法，大语言模型就能更好地处理那些以前几乎没有见过或者资料很少的语言，从而使机器翻译服务能够覆盖更多的语种。<br />
实验结果：<br />
<br />
1、未见语言翻译实验：使用MTInstruct方法，研究者对BLOOMZ模型进行了微调，以测试其在未见语言的翻译能力。<br />
<br />
实验涉及了多达24种未见语言。<br />
<br />
结果显示，即使是对这些之前未见过的语言，经过MTInstruct微调的模型也能有效地进行翻译。<br />
<br />
该实验证明了即使在缺乏大量训练数据的情况下，模型仍能学习如何翻译新的语言。<br />
<br />
2、低资源语言翻译实验：使用AlignInstruct方法，研究者专注于提升模型在低资源语言翻译方面的性能。<br />
<br />
实验使用基于统计词对齐的跨语言鉴别器进行跨语言监督。<br />
<br />
结果显示，AlignInstruct在涉及英语的48个翻译方向中持续提高了翻译质量。<br />
<br />
该实验表明AlignInstruct能有效提升模型在处理低资源语言时的翻译准确性和效率。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.05811">arxiv.org/abs/2401.05811</a><br />
<br />
PDF：<a href="https://arxiv.org/pdf/2401.05811.pdf">arxiv.org/pdf/2401.05811.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RvRHJzRmFVQUFGWDl6LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RvRVBqdmJzQUFERFVCLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1745703281652101321#m</id>
            <title>Ssm 奥特曼在YC W24 启动会上的演讲要点：

- 奥特曼暗示我们可能已经非常接近实现通用人工智能（AGI）

- 他建议应该以通用人工智能的实现为前提进行创业和技术开发。（不要再瞎折腾）

- GPT-5可能会相对于GPT-4有一个指数级的跳跃，尽管GPT-4已经领先近两年，至今无人超越。

- 这个进展将会给初创企业和现有公司带来了许多问题和挑战。（AGI将覆盖一大批创业者）

- 建议使用最先进的模型（State of the Art, SOTA），而不是花费太多时间进行微调和优化。（徒劳无功）

- 最正确的做法是设想一个“上帝般的”模型正在运作，然后基于这种设想来构建最好的产品。（要极其有远见）

- @OpenAI API将继续变得更快、更可靠、更便宜。然而，性能和成本之间始终存在平衡。例如，尽管电池技术已显着改进，但 iPhone 仍将保持 1-1.5 天的电池寿命以优化性能。

- 不建议建立产品业务主要致力于解决当前 GPT4的限制的内容。因为大多数限制将在 GPT-5 中部分/全部修复。（你会被覆盖）

- 初创公司更需要情境优化，而不是行为优化。通过 RAG 等提供更多信息可能比微调更有益。

综合@SullyOmarr 和 @RealRichomie 内容</title>
            <link>https://nitter.cz/xiaohuggg/status/1745703281652101321#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1745703281652101321#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 07:04:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ssm 奥特曼在YC W24 启动会上的演讲要点：<br />
<br />
- 奥特曼暗示我们可能已经非常接近实现通用人工智能（AGI）<br />
<br />
- 他建议应该以通用人工智能的实现为前提进行创业和技术开发。（不要再瞎折腾）<br />
<br />
- GPT-5可能会相对于GPT-4有一个指数级的跳跃，尽管GPT-4已经领先近两年，至今无人超越。<br />
<br />
- 这个进展将会给初创企业和现有公司带来了许多问题和挑战。（AGI将覆盖一大批创业者）<br />
<br />
- 建议使用最先进的模型（State of the Art, SOTA），而不是花费太多时间进行微调和优化。（徒劳无功）<br />
<br />
- 最正确的做法是设想一个“上帝般的”模型正在运作，然后基于这种设想来构建最好的产品。（要极其有远见）<br />
<br />
- <a href="https://nitter.cz/OpenAI" title="OpenAI">@OpenAI</a> API将继续变得更快、更可靠、更便宜。然而，性能和成本之间始终存在平衡。例如，尽管电池技术已显着改进，但 iPhone 仍将保持 1-1.5 天的电池寿命以优化性能。<br />
<br />
- 不建议建立产品业务主要致力于解决当前 GPT4的限制的内容。因为大多数限制将在 GPT-5 中部分/全部修复。（你会被覆盖）<br />
<br />
- 初创公司更需要情境优化，而不是行为优化。通过 RAG 等提供更多信息可能比微调更有益。<br />
<br />
综合<a href="https://nitter.cz/SullyOmarr" title="Sully">@SullyOmarr</a> 和 <a href="https://nitter.cz/RealRichomie" title="Richard He">@RealRichomie</a> 内容</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RueW1jbGF3QUFxNWYyLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>