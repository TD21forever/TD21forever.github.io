<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744538824271929742#m</id>
            <title>启动启动启动

所有系统统统启动…

😂</title>
            <link>https://nitter.cz/xiaohuggg/status/1744538824271929742#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744538824271929742#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jan 2024 01:57:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>启动启动启动<br />
<br />
所有系统统统启动…<br />
<br />
😂</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ0NTM4NzY4NzU2MDg0NzM2L2ltZy8tRGlTZ2ROZlNmMlZMOXZSLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744377230275297443#m</id>
            <title>🔔 http://Xiaohu.AI日报「1月8日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1744377230275297443#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744377230275297443#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 15:15:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔 <a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「1月8日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RWRjF4Y1dFQUFHdFd2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744370072141910255#m</id>
            <title>R to @xiaohuggg: 我已经有了🐶

可惜不能激活🐶</title>
            <link>https://nitter.cz/xiaohuggg/status/1744370072141910255#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744370072141910255#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 14:46:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我已经有了🐶<br />
<br />
可惜不能激活🐶</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RVX1ZSclc0QUFmWW5PLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744367318921069019#m</id>
            <title>Apple Vision Pro 将于1月19日正式开启预购。

2月2日正式发售

售价为3499美元起

https://www.apple.com/newsroom/2024/01/apple-vision-pro-available-in-the-us-on-february-2/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744367318921069019#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744367318921069019#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 14:35:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Apple Vision Pro 将于1月19日正式开启预购。<br />
<br />
2月2日正式发售<br />
<br />
售价为3499美元起<br />
<br />
<a href="https://www.apple.com/newsroom/2024/01/apple-vision-pro-available-in-the-us-on-february-2/">apple.com/newsroom/2024/01/a…</a></p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzQ0MzY3MjYzMTgyOTIxNzI4L2ltZy9WZUcwNFN4Z3ZPelltZ1dGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744337216548295062#m</id>
            <title>R to @xiaohuggg: 原推

Midjourney V6 还能完全复制好莱坞和日本动画的电影帧，这些巨头们还没行动，估计马上也要开始了...</title>
            <link>https://nitter.cz/xiaohuggg/status/1744337216548295062#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744337216548295062#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 12:36:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>原推<br />
<br />
Midjourney V6 还能完全复制好莱坞和日本动画的电影帧，这些巨头们还没行动，估计马上也要开始了...</p>
<p><a href="https://nitter.cz/JonLamArt/status/1741545927435784424#m">nitter.cz/JonLamArt/status/1741545927435784424#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744336823701434753#m</id>
            <title>该来的终于来了  Midjourney将面临集体诉讼🙃

Midjourney 被曝光未经许可使用了包含 16000 名艺术家的作品风格来训练其图像生成AI。

这个名单不仅包括现代和当代著名艺术家的作品，还包括了为公司如Hasbro和Nintendo工作的商业插画师的作品。

甚至还包括了一名6岁儿童的作品。

Midjourney创建了一个艺术家的数据库，这些艺术家在数据库中被简化为不同的风格标签，然后利用这些信息来训练他们的 AI 系统。

换句话说，Midjourney使用了这些艺术家的风格和技巧，在没有艺术家本人同意的情况下进行的。

这些著名艺术家包括赛·托姆布雷、安迪·沃霍尔、安尼什·卡普尔、草间弥生、格哈德·里希特、弗里达·卡罗、安迪·沃霍尔、埃尔斯沃斯·凯利、达明安·赫斯特、阿梅迪奥·莫迪利亚尼、巴勃罗·毕加索、保罗·西涅克、诺曼·洛克威尔、保罗·塞尚、班克斯、沃尔特·迪士尼、和文森特·梵高。

其中一个值得注意的例子是六岁的孩子 Hyan Tran 的作品也被 Midjourney 爬取，这个孩子曾在 2021 年为西雅图儿童医院的筹款活动贡献艺术作品。这一发现引起了艺术家们的担忧，并促使他们寻求法律援助。

一些艺术家已经对 Midjourney 和其他公司提起了集体诉讼，因为他们的作品在未经许可的情况下被用于训练人工智能图像生成器。

详细名单：https://storage.courtlistener.com/recap/gov.uscourts.cand.407208/gov.uscourts.cand.407208.129.10.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1744336823701434753#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744336823701434753#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 12:34:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>该来的终于来了  Midjourney将面临集体诉讼🙃<br />
<br />
Midjourney 被曝光未经许可使用了包含 16000 名艺术家的作品风格来训练其图像生成AI。<br />
<br />
这个名单不仅包括现代和当代著名艺术家的作品，还包括了为公司如Hasbro和Nintendo工作的商业插画师的作品。<br />
<br />
甚至还包括了一名6岁儿童的作品。<br />
<br />
Midjourney创建了一个艺术家的数据库，这些艺术家在数据库中被简化为不同的风格标签，然后利用这些信息来训练他们的 AI 系统。<br />
<br />
换句话说，Midjourney使用了这些艺术家的风格和技巧，在没有艺术家本人同意的情况下进行的。<br />
<br />
这些著名艺术家包括赛·托姆布雷、安迪·沃霍尔、安尼什·卡普尔、草间弥生、格哈德·里希特、弗里达·卡罗、安迪·沃霍尔、埃尔斯沃斯·凯利、达明安·赫斯特、阿梅迪奥·莫迪利亚尼、巴勃罗·毕加索、保罗·西涅克、诺曼·洛克威尔、保罗·塞尚、班克斯、沃尔特·迪士尼、和文森特·梵高。<br />
<br />
其中一个值得注意的例子是六岁的孩子 Hyan Tran 的作品也被 Midjourney 爬取，这个孩子曾在 2021 年为西雅图儿童医院的筹款活动贡献艺术作品。这一发现引起了艺术家们的担忧，并促使他们寻求法律援助。<br />
<br />
一些艺术家已经对 Midjourney 和其他公司提起了集体诉讼，因为他们的作品在未经许可的情况下被用于训练人工智能图像生成器。<br />
<br />
详细名单：<a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.407208/gov.uscourts.cand.407208.129.10.pdf">storage.courtlistener.com/re…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQzMzYyMTIzMTAzMTkxMDQvcHUvaW1nL0UwX2U1UXh5cUhvUHcwa0guanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744296052029870124#m</id>
            <title>麻省理工大学研究团队开发出一种新技术：Ddog

通过脑电波控制波士顿动力的机器狗。

该技术仅靠一种特殊的眼镜就能读取人的脑电波和眼动，然后把这些信号传递给机器人执行动作。

Ddog系统只需要两 iPhone和一副蓝牙眼镜就可以运行。而且还可以完全离线工作。

简单来说，如果用户想要机器人去拿一瓶水，你只需要想象这个动作，这个特殊的眼镜就能捕捉到这个想法，并告诉机器人去完成这个任务。

机器人自己能够四处走动，爬楼梯，帮你拿东西，像一个真正的小助手一样。这项技术不需要复杂的装备，只需要两部手机和一副眼镜，这让它在日常生活中很容易使用。

Ddog 项目让身体受限的人，如患有 ALS、脑瘫和脊髓损伤等身体障碍的人提供帮助。让他们能够通过思考来控制机器人，使机器人成为他们的手和脚，帮助他们更好地与周围世界互动。

工作原理：

Ddog 项目是基于 MIT 的一个名为 Brain Switch 的先进技术。Brain Switch 是一种脑机接口（BCI），它可以实时地、无需言语地捕捉用户的大脑信号，并将这些信号转化为与看护者沟通的指令。

1、脑机接口：研究者们用一种特殊的无线眼镜来读取佩戴者的大脑活动和眼睛运动。这些眼镜有传感器，能够捕捉到你的脑电波（即大脑发出的信号）。

2、信号解读：当你想让机器人执行一个动作时（比如拿东西），你的大脑会产生特定的信号。这些眼镜能识别这些信号，把它们转换成电子指令。

Ddog 的工作方式：

当 Spot 与新用户在新环境中工作时，首先需要创建工作环境的 3D 地图。

信号解读：接下来，第一部 iPhone 会通过询问用户接下来想做什么来提示用户，用户只需通过思考他们想要的东西来回答。

环境识别：第二部 iPhone 运行本地导航地图，控制 Spot 的机械臂，并使用 iPhone 的激光雷达数据增强 Spot 的激光雷达。

任务执行：两部 iPhone 相互通信以跟踪 Spot 完成任务的进度。

详细介绍：https://www.therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744296052029870124#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744296052029870124#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 09:52:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>麻省理工大学研究团队开发出一种新技术：Ddog<br />
<br />
通过脑电波控制波士顿动力的机器狗。<br />
<br />
该技术仅靠一种特殊的眼镜就能读取人的脑电波和眼动，然后把这些信号传递给机器人执行动作。<br />
<br />
Ddog系统只需要两 iPhone和一副蓝牙眼镜就可以运行。而且还可以完全离线工作。<br />
<br />
简单来说，如果用户想要机器人去拿一瓶水，你只需要想象这个动作，这个特殊的眼镜就能捕捉到这个想法，并告诉机器人去完成这个任务。<br />
<br />
机器人自己能够四处走动，爬楼梯，帮你拿东西，像一个真正的小助手一样。这项技术不需要复杂的装备，只需要两部手机和一副眼镜，这让它在日常生活中很容易使用。<br />
<br />
Ddog 项目让身体受限的人，如患有 ALS、脑瘫和脊髓损伤等身体障碍的人提供帮助。让他们能够通过思考来控制机器人，使机器人成为他们的手和脚，帮助他们更好地与周围世界互动。<br />
<br />
工作原理：<br />
<br />
Ddog 项目是基于 MIT 的一个名为 Brain Switch 的先进技术。Brain Switch 是一种脑机接口（BCI），它可以实时地、无需言语地捕捉用户的大脑信号，并将这些信号转化为与看护者沟通的指令。<br />
<br />
1、脑机接口：研究者们用一种特殊的无线眼镜来读取佩戴者的大脑活动和眼睛运动。这些眼镜有传感器，能够捕捉到你的脑电波（即大脑发出的信号）。<br />
<br />
2、信号解读：当你想让机器人执行一个动作时（比如拿东西），你的大脑会产生特定的信号。这些眼镜能识别这些信号，把它们转换成电子指令。<br />
<br />
Ddog 的工作方式：<br />
<br />
当 Spot 与新用户在新环境中工作时，首先需要创建工作环境的 3D 地图。<br />
<br />
信号解读：接下来，第一部 iPhone 会通过询问用户接下来想做什么来提示用户，用户只需通过思考他们想要的东西来回答。<br />
<br />
环境识别：第二部 iPhone 运行本地导航地图，控制 Spot 的机械臂，并使用 iPhone 的激光雷达数据增强 Spot 的激光雷达。<br />
<br />
任务执行：两部 iPhone 相互通信以跟踪 Spot 完成任务的进度。<br />
<br />
详细介绍：<a href="https://www.therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/">therobotreport.com/ddog-mit-…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQyNzIxODYyNDcxNjM5MDQvcHUvaW1nL2tmRjlqbjN5bDFoUGRRUG4uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744278403266826571#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1744278403266826571#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744278403266826571#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 08:42:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUcmQ5QWJjQUF1dHFrLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUcmQ5MWFjQUF4c0dTLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744278400561500270#m</id>
            <title>这效果如何...

神龙摆尾...

提示词在ALT里面...</title>
            <link>https://nitter.cz/xiaohuggg/status/1744278400561500270#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744278400561500270#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 08:42:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这效果如何...<br />
<br />
神龙摆尾...<br />
<br />
提示词在ALT里面...</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUclQ3Y2FzQUFwbDh3LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744272205658370554#m</id>
            <title>Google 发明了一种估算照片中光照条件的新方法：DiffusionLight

该技术可以在照片中加入一个看起来像是真实反射环境的铬球。这个铬球可以帮助计算出照片中的光照是怎样的。

然后，他们使用这些光照信息在照片中添加新的物体，使得这些物体看起来好像是在原来的光照条件下拍摄的一样。

简单地说就是：检测图片中的光源（光照信息），然后根据光源，把其他物体对象插入到图片中，能达到相同的光影效果，毫无违和感。

工作原理如下：

1、输入图片：你提供一张照片，比如一个室内场景。

2、添加铬球：使用 DiffusionLight 技术，在照片中的合适位置绘制一个铬球。这个球会反射出场景中的光线和色彩。

3、生成环境图：铬球的反射可以被转换成一个高动态范围（HDR）的环境图，这张图像包含了关于场景光照的信息。

4、光照估计：通过分析铬球上的反射，DiffusionLight 估计出图片中的光源位置和亮度。

5、三维对象插入：利用估计出的光照信息，三维模型可以被添加进照片中，并且以一种看起来自然和真实的方式反应光照效果。

这项技术的一个关键创新是它不需要昂贵或复杂的设备来捕获光照条件。它只需要一张图片和强大的算法。这意味着它可以用于从专业电影制作到手机摄影的各种应用，为艺术家和开发者创造新的可能性。

该技术可以用于多种输入图像，如室内外场景、特写镜头、绘画和人脸照片。

项目及演示：https://diffusionlight.github.io/
论文：https://arxiv.org/abs/2312.09168
GitHub：https://github.com/DiffusionLight/DiffusionLight</title>
            <link>https://nitter.cz/xiaohuggg/status/1744272205658370554#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744272205658370554#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 08:17:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google 发明了一种估算照片中光照条件的新方法：DiffusionLight<br />
<br />
该技术可以在照片中加入一个看起来像是真实反射环境的铬球。这个铬球可以帮助计算出照片中的光照是怎样的。<br />
<br />
然后，他们使用这些光照信息在照片中添加新的物体，使得这些物体看起来好像是在原来的光照条件下拍摄的一样。<br />
<br />
简单地说就是：检测图片中的光源（光照信息），然后根据光源，把其他物体对象插入到图片中，能达到相同的光影效果，毫无违和感。<br />
<br />
工作原理如下：<br />
<br />
1、输入图片：你提供一张照片，比如一个室内场景。<br />
<br />
2、添加铬球：使用 DiffusionLight 技术，在照片中的合适位置绘制一个铬球。这个球会反射出场景中的光线和色彩。<br />
<br />
3、生成环境图：铬球的反射可以被转换成一个高动态范围（HDR）的环境图，这张图像包含了关于场景光照的信息。<br />
<br />
4、光照估计：通过分析铬球上的反射，DiffusionLight 估计出图片中的光源位置和亮度。<br />
<br />
5、三维对象插入：利用估计出的光照信息，三维模型可以被添加进照片中，并且以一种看起来自然和真实的方式反应光照效果。<br />
<br />
这项技术的一个关键创新是它不需要昂贵或复杂的设备来捕获光照条件。它只需要一张图片和强大的算法。这意味着它可以用于从专业电影制作到手机摄影的各种应用，为艺术家和开发者创造新的可能性。<br />
<br />
该技术可以用于多种输入图像，如室内外场景、特写镜头、绘画和人脸照片。<br />
<br />
项目及演示：<a href="https://diffusionlight.github.io/">diffusionlight.github.io/</a><br />
论文：<a href="https://arxiv.org/abs/2312.09168">arxiv.org/abs/2312.09168</a><br />
GitHub：<a href="https://github.com/DiffusionLight/DiffusionLight">github.com/DiffusionLight/Di…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQyNTU2MjMzNzY4NzE0MjQvcHUvaW1nL0JEMTM3VjlvVFdIbmZTeWwuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744246627865608573#m</id>
            <title>MATHPILE：一个高质量、大规模的数学语料库，29 GB，包含约 95 亿个代币。

涵盖从 K-12 到大学、研究生水平和数学竞赛的内容，包括高质量教科书、讲义、科学论文等。

提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。

-数据来源和处理：数据最初来源于多个不同的数据源，总计大约 520 亿个令牌，占 2.2 TB 的数据量。

源数据包括 StackExchange、ProofWiki、Common Crawl、arXiv，以及其他来源。这些数据经过一系列严格的处理过程，包括数据预处理和预过滤、语言识别、清理和过滤，以及去重。

-MATHPILE 语料库：经过处理后，得到了一个以数学为中心的语料库，即 MATHPILE。这个语料库总计有 29 GB 的数据量，包含约 903,000 篇文档，以及大约 95 亿个令牌。

主要特点：

1、数学领域专注：MathPile 是专门为数学领域设计的，与通用或多语言焦点的语料库有明显区别。

2、多样性：MathPile 从广泛的来源汇集数据，包括教科书（包括讲义）、arXiv、维基百科、ProofWiki、StackExchange 和网页。它涵盖了适合 K-12、大学、研究生水平和数学竞赛的数学内容。特别是，项目发布了大量高质量教科书的显著收藏（约 0.19B 令牌）。

3、高质量：项目坚持“少即是多”的原则，即使在预训练阶段也相信数据质量胜过数量。项目的数据收集和处理努力包括复杂的预处理、预过滤、清洁、过滤和去重，确保语料库的高质量。

4、数据文档：为了增强透明度，提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。如语言识别分数和符号到单词的比率。这为用户提供了根据其需要定制数据的灵活性。

项目还进行了数据污染检测，以消除诸如 MATH 和 MMLU-STEM 等基准测试集的重复项。

通过这种专门的语料库，研究人员和开发者能够更有效地提高语言模型在数学推理方面的能力。

项目地址：https://gair-nlp.github.io/MathPile/
论文：https://arxiv.org/abs/2312.17120
GitHub：https://github.com/GAIR-NLP/MathPile
数据集：https://huggingface.co/datasets/GAIR/MathPile</title>
            <link>https://nitter.cz/xiaohuggg/status/1744246627865608573#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744246627865608573#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 06:36:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MATHPILE：一个高质量、大规模的数学语料库，29 GB，包含约 95 亿个代币。<br />
<br />
涵盖从 K-12 到大学、研究生水平和数学竞赛的内容，包括高质量教科书、讲义、科学论文等。<br />
<br />
提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。<br />
<br />
-数据来源和处理：数据最初来源于多个不同的数据源，总计大约 520 亿个令牌，占 2.2 TB 的数据量。<br />
<br />
源数据包括 StackExchange、ProofWiki、Common Crawl、arXiv，以及其他来源。这些数据经过一系列严格的处理过程，包括数据预处理和预过滤、语言识别、清理和过滤，以及去重。<br />
<br />
-MATHPILE 语料库：经过处理后，得到了一个以数学为中心的语料库，即 MATHPILE。这个语料库总计有 29 GB 的数据量，包含约 903,000 篇文档，以及大约 95 亿个令牌。<br />
<br />
主要特点：<br />
<br />
1、数学领域专注：MathPile 是专门为数学领域设计的，与通用或多语言焦点的语料库有明显区别。<br />
<br />
2、多样性：MathPile 从广泛的来源汇集数据，包括教科书（包括讲义）、arXiv、维基百科、ProofWiki、StackExchange 和网页。它涵盖了适合 K-12、大学、研究生水平和数学竞赛的数学内容。特别是，项目发布了大量高质量教科书的显著收藏（约 0.19B 令牌）。<br />
<br />
3、高质量：项目坚持“少即是多”的原则，即使在预训练阶段也相信数据质量胜过数量。项目的数据收集和处理努力包括复杂的预处理、预过滤、清洁、过滤和去重，确保语料库的高质量。<br />
<br />
4、数据文档：为了增强透明度，提供详细的数据记录，包括数据集表格和质量注释，提高透明度并允许用户根据需要定制数据。如语言识别分数和符号到单词的比率。这为用户提供了根据其需要定制数据的灵活性。<br />
<br />
项目还进行了数据污染检测，以消除诸如 MATH 和 MMLU-STEM 等基准测试集的重复项。<br />
<br />
通过这种专门的语料库，研究人员和开发者能够更有效地提高语言模型在数学推理方面的能力。<br />
<br />
项目地址：<a href="https://gair-nlp.github.io/MathPile/">gair-nlp.github.io/MathPile/</a><br />
论文：<a href="https://arxiv.org/abs/2312.17120">arxiv.org/abs/2312.17120</a><br />
GitHub：<a href="https://github.com/GAIR-NLP/MathPile">github.com/GAIR-NLP/MathPile</a><br />
数据集：<a href="https://huggingface.co/datasets/GAIR/MathPile">huggingface.co/datasets/GAIR…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RUT2xibWJJQUF1elY4LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744197461802152266#m</id>
            <title>有人在 Binance 上购买了 26.9 个BTC

然后将其发送到了中本聪的死亡钱包中...

这是什么操作？？？

除非是中本聪本人才干得出这种事情吧？？？

🤣</title>
            <link>https://nitter.cz/xiaohuggg/status/1744197461802152266#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744197461802152266#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 03:20:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>有人在 Binance 上购买了 26.9 个BTC<br />
<br />
然后将其发送到了中本聪的死亡钱包中...<br />
<br />
这是什么操作？？？<br />
<br />
除非是中本聪本人才干得出这种事情吧？？？<br />
<br />
🤣</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RTaUZ5a2FzQUFQcFFiLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744188783908770222#m</id>
            <title>XREAL推出其新款AR眼镜：XREAL Air 2 Ultra

这款眼镜配备了双3D环境传感器，具有六自由度（6DoF）功能，能精准追踪头部运动，提供更沉浸式和真实的AR体验。

配备有计算机视觉能力，刷新率高达120Hz，最高亮度500尼特。

可获得在4米距离内观看154英寸虚拟2D屏幕投影的视觉效果。

钛合金设计，非常轻便！

XREAL Air 2 Ultra配备了一对具有计算机视觉功能的3D环境传感器，就类似双目鱼眼摄像头，可通过定位和映射来确定用户在3D空间中的位置。

这枚3D环境传感器专为开发人员创建AR应用程序和探索新的空间计算体验而设计，支持手部追踪、3D网格创建、语义场景理解、以及潜在AI拓展等功能。

XREAL Air 2 Ultra的主要功能特点：

1、六自由度（6DoF）AR眼镜：这意味着这款AR眼镜能够精准追踪用户的头部运动，包括前后、上下、左右移动，以及绕三个轴线的旋转（即俯仰、滚转和偏航）。这种追踪能力使用户在虚拟环境中能够自然地移动和查看物体，提供更沉浸式和真实的增强现实体验。

2、高清显示体验：为每只眼睛提供全高清视觉体验，视场（FOV）为52°，每度像素数（PPD）为42，刷新率高达120Hz，最高亮度为500尼特，确保在多种光照条件下图像清晰生动。

3、优质音效：具有电影级音频和定向音频技术，XREAL Air 2 Ultra支持定向音频技术，可减少声音扩散，从而更好地保护个人隐私，并降低对他人的干扰。

4、与Apple的空间视频特性兼容：支持将iPhone 15 Pro拍摄的空间视频转换成常规的左右格式，无需昂贵的Apple Vision Pro即可在XREAL Air 2系列眼镜上观看。

5、舒适的佩戴体验：采用轻巧的钛合金眼镜框，仅重80克，设计考虑了最佳的重量分布，可调节的太阳镜臂和鼻垫，确保舒适佩戴和最佳的图像对准。

6、眼部健康认证：每副XREAL Air 2 Ultra眼镜均通过TÜV Rheinland认证，确保色彩准确、舒适、低蓝光和无闪烁使用，同时超过了ISO标准，提供优质的图像质量和观看舒适度。

7、开发者支持：提供最新的NRSDK 2.2开发工具包，支持手势跟踪、深度网格、空间锚点等空间计算能力。

目前这款产品已经可以预订，价格为699美元，Nreal Light 的用户预定立减100美金。预计将在2024年3月开始发货。https://developer.xreal.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744188783908770222#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744188783908770222#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 02:46:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>XREAL推出其新款AR眼镜：XREAL Air 2 Ultra<br />
<br />
这款眼镜配备了双3D环境传感器，具有六自由度（6DoF）功能，能精准追踪头部运动，提供更沉浸式和真实的AR体验。<br />
<br />
配备有计算机视觉能力，刷新率高达120Hz，最高亮度500尼特。<br />
<br />
可获得在4米距离内观看154英寸虚拟2D屏幕投影的视觉效果。<br />
<br />
钛合金设计，非常轻便！<br />
<br />
XREAL Air 2 Ultra配备了一对具有计算机视觉功能的3D环境传感器，就类似双目鱼眼摄像头，可通过定位和映射来确定用户在3D空间中的位置。<br />
<br />
这枚3D环境传感器专为开发人员创建AR应用程序和探索新的空间计算体验而设计，支持手部追踪、3D网格创建、语义场景理解、以及潜在AI拓展等功能。<br />
<br />
XREAL Air 2 Ultra的主要功能特点：<br />
<br />
1、六自由度（6DoF）AR眼镜：这意味着这款AR眼镜能够精准追踪用户的头部运动，包括前后、上下、左右移动，以及绕三个轴线的旋转（即俯仰、滚转和偏航）。这种追踪能力使用户在虚拟环境中能够自然地移动和查看物体，提供更沉浸式和真实的增强现实体验。<br />
<br />
2、高清显示体验：为每只眼睛提供全高清视觉体验，视场（FOV）为52°，每度像素数（PPD）为42，刷新率高达120Hz，最高亮度为500尼特，确保在多种光照条件下图像清晰生动。<br />
<br />
3、优质音效：具有电影级音频和定向音频技术，XREAL Air 2 Ultra支持定向音频技术，可减少声音扩散，从而更好地保护个人隐私，并降低对他人的干扰。<br />
<br />
4、与Apple的空间视频特性兼容：支持将iPhone 15 Pro拍摄的空间视频转换成常规的左右格式，无需昂贵的Apple Vision Pro即可在XREAL Air 2系列眼镜上观看。<br />
<br />
5、舒适的佩戴体验：采用轻巧的钛合金眼镜框，仅重80克，设计考虑了最佳的重量分布，可调节的太阳镜臂和鼻垫，确保舒适佩戴和最佳的图像对准。<br />
<br />
6、眼部健康认证：每副XREAL Air 2 Ultra眼镜均通过TÜV Rheinland认证，确保色彩准确、舒适、低蓝光和无闪烁使用，同时超过了ISO标准，提供优质的图像质量和观看舒适度。<br />
<br />
7、开发者支持：提供最新的NRSDK 2.2开发工具包，支持手势跟踪、深度网格、空间锚点等空间计算能力。<br />
<br />
目前这款产品已经可以预订，价格为699美元，Nreal Light 的用户预定立减100美金。预计将在2024年3月开始发货。<a href="https://developer.xreal.com/">developer.xreal.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQxODgzMDY2MjYzNDcwMDgvcHUvaW1nL3NqSjRiS1QteUo5TndFZ2IuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744179160434802963#m</id>
            <title>Teachable Machine：一个由Google开发的机器学习工具

它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。

你可以用它来教电脑识别图片、声音或人的动作。

使用这个工具的步骤很简单：

1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。

2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。

3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。

Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。

1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。

2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。

3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。

开始训练：https://teachablemachine.withgoogle.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1744179160434802963#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744179160434802963#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jan 2024 02:08:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Teachable Machine：一个由Google开发的机器学习工具<br />
<br />
它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。<br />
<br />
你可以用它来教电脑识别图片、声音或人的动作。<br />
<br />
使用这个工具的步骤很简单：<br />
<br />
1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。<br />
<br />
2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。<br />
<br />
3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。<br />
<br />
Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。<br />
<br />
1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。<br />
<br />
2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。<br />
<br />
3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。<br />
<br />
开始训练：<a href="https://teachablemachine.withgoogle.com/">teachablemachine.withgoogle.…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5OTM4OTc5NTE1OTI0NDgvcHUvaW1nL1pRUld0cmFLVVR0TkJxWTIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1744016632211726742#m</id>
            <title>🔔 http://Xiaohu.AI日报「1月7日」

✨✨✨✨✨✨✨✨</title>
            <link>https://nitter.cz/xiaohuggg/status/1744016632211726742#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1744016632211726742#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 15:22:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>🔔 <a href="http://Xiaohu.AI">Xiaohu.AI</a>日报「1月7日」<br />
<br />
✨✨✨✨✨✨✨✨</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RQOTRON2JVQUFxc0paLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1743998321977672058#m</id>
            <title>机器人迎来它的ChatGPT 时刻？

机器人初创公司@Figure_robot 发布了一段视频

他们家的Figure-01机器人现在可以自己煮咖啡了

这是一个使用了端到端的人工智能系统，仅通过观察人类制作咖啡的录像，10小时内学会了制作咖啡的技能。

机器人通过神经网络来处理和分析视频数据。通过观看如何制作咖啡的录像。学习人类的动作和手势，然后模仿这些动作来学习制作咖啡的过程。

无需通过编程，机器人自主学习技能。

早前FigureCEO Brett Adcock @adcock_brett 称他们刚刚取得了人工智能突破 。

机器人技术即将迎来它的ChatGPT 时刻！

说的是不是这个？</title>
            <link>https://nitter.cz/xiaohuggg/status/1743998321977672058#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1743998321977672058#m</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Jan 2024 14:09:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>机器人迎来它的ChatGPT 时刻？<br />
<br />
机器人初创公司<a href="https://nitter.cz/Figure_robot" title="Figure">@Figure_robot</a> 发布了一段视频<br />
<br />
他们家的Figure-01机器人现在可以自己煮咖啡了<br />
<br />
这是一个使用了端到端的人工智能系统，仅通过观察人类制作咖啡的录像，10小时内学会了制作咖啡的技能。<br />
<br />
机器人通过神经网络来处理和分析视频数据。通过观看如何制作咖啡的录像。学习人类的动作和手势，然后模仿这些动作来学习制作咖啡的过程。<br />
<br />
无需通过编程，机器人自主学习技能。<br />
<br />
早前FigureCEO Brett Adcock <a href="https://nitter.cz/adcock_brett" title="Brett Adcock">@adcock_brett</a> 称他们刚刚取得了人工智能突破 。<br />
<br />
机器人技术即将迎来它的ChatGPT 时刻！<br />
<br />
说的是不是这个？</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDM5OTU4Mzg0OTY2ODE5ODQvcHUvaW1nL0dsdlV6YmlIeVppdXdFZ0MuanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>