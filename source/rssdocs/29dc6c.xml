<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730760301061304740#m</id>
            <title>这个影子细节也处理了

厉害👍

好久没关注@WonderDynamics 了，其实可以用wonder studio 做一些短视频创业！😎</title>
            <link>https://nitter.cz/xiaohuggg/status/1730760301061304740#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730760301061304740#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 01:26:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这个影子细节也处理了<br />
<br />
厉害👍<br />
<br />
好久没关注<a href="https://nitter.cz/WonderDynamics" title="Wonder Dynamics">@WonderDynamics</a> 了，其实可以用wonder studio 做一些短视频创业！😎</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMwNjg4NjMzNzkzOTA4NzM2L2ltZy9wT1M0Zjh0eklKUmI4a0VGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730611412261916704#m</id>
            <title>真正的

街舞🕺🏻</title>
            <link>https://nitter.cz/xiaohuggg/status/1730611412261916704#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730611412261916704#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 15:34:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>真正的<br />
<br />
街舞🕺🏻</p>
<img src="https://nitter.cz/pic/enc/YW1wbGlmeV92aWRlb190aHVtYi8xNzMwNjExMjU5OTc5MjgwMzg0L2ltZy9kTVp4V1FpTDhsRlhtcEF6LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730548066246410631#m</id>
            <title>R to @xiaohuggg: GAIA不仅能根据语音自动生成头部动作，还允许用户自定义头部的动作。例如，如果用户想要虚拟头像在说话时摇头或点头，他们可以指定这样的动作，而GAIA将能够在不影响嘴唇运动与语音同步的情况下实现这一动作。

这增加了虚拟头像视频生成的灵活性和可控性，使其更适用于各种不同的应用场景。</title>
            <link>https://nitter.cz/xiaohuggg/status/1730548066246410631#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730548066246410631#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 11:23:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GAIA不仅能根据语音自动生成头部动作，还允许用户自定义头部的动作。例如，如果用户想要虚拟头像在说话时摇头或点头，他们可以指定这样的动作，而GAIA将能够在不影响嘴唇运动与语音同步的情况下实现这一动作。<br />
<br />
这增加了虚拟头像视频生成的灵活性和可控性，使其更适用于各种不同的应用场景。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA1NDc2NzA0MDg5MTI4OTYvcHUvaW1nL3F6LUo1bXJveHNqNXlLaVkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730547607716643080#m</id>
            <title>微软的这个项目厉害了！！

GAIA的：能够从语音和单张肖像图片合成自然的会说话的头像视频。

它甚至支持诸如“悲伤”、“张开嘴”或“惊讶”等文本提示，来指导视频生成。

GAIA还允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。

可以接受语音、视频或文字指令创建会说话的人物头像视频。

主要功能：

1、根据语音生成会说话的虚拟人物：如果你给GAIA一个语音录音，它可以创建一个虚拟人物的视频，这个人物的嘴唇和面部表情会跟着语音动。

2、根据视频生成会说话的虚拟人物：GAIA可以观察一个真人在视频里的动作，然后创建一个虚拟人物模仿这些动作。

3、控制虚拟人物的头部姿势：你可以告诉GAIA让虚拟人物的头部做出特定的动作，比如点头或摇头。

4、完全控制虚拟人物的表情：GAIA允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。

5、根据文字指令生成虚拟人物动作：你可以给GAIA一些文字指令，比如“请微笑”，它就会创建一个按照这些指令动作的虚拟人物视频

主要工作原理：

1.分离运动和外观表示：

•GAIA首先将每个视频帧分离成运动和外观两部分的表示。这意味着它可以区分哪些部分是因为说话而动（如嘴唇运动），哪些部分是保持不变的（如头发、眼睛的位置）。

2.使用变分自编码器（VAE）：

•VAE被用来编码视频帧中的这些分离表示，并从这些表示中重建原始帧。这个过程帮助模型学习如何准确地捕捉和再现人物的面部特征和表情。

3.基于语音的运动序列生成：

•扩散模型被优化以生成基于语音序列和参考肖像图片的运动序列。这意味着模型可以根据给定的语音输入（如一段对话）生成相应的面部运动。

4.在推理过程中的应用：

•在实际应用中，扩散模型接受输入的语音序列和参考肖像图片作为条件，并生成运动序列。然后，这些运动序列被解码成视频，展示虚拟头像的说话和表情动作。

5.控制和文本指令的应用：
•GAIA还允许通过编辑生成过程中的面部标记点来控制任意面部属性，或根据文本指令生成虚拟头像的视频剪辑。

项目及演示：https://microsoft.github.io/GAIA/
论文：https://arxiv.org/abs/2311.15230
GitHub：coming soon...</title>
            <link>https://nitter.cz/xiaohuggg/status/1730547607716643080#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730547607716643080#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 11:21:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>微软的这个项目厉害了！！<br />
<br />
GAIA的：能够从语音和单张肖像图片合成自然的会说话的头像视频。<br />
<br />
它甚至支持诸如“悲伤”、“张开嘴”或“惊讶”等文本提示，来指导视频生成。<br />
<br />
GAIA还允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。<br />
<br />
可以接受语音、视频或文字指令创建会说话的人物头像视频。<br />
<br />
主要功能：<br />
<br />
1、根据语音生成会说话的虚拟人物：如果你给GAIA一个语音录音，它可以创建一个虚拟人物的视频，这个人物的嘴唇和面部表情会跟着语音动。<br />
<br />
2、根据视频生成会说话的虚拟人物：GAIA可以观察一个真人在视频里的动作，然后创建一个虚拟人物模仿这些动作。<br />
<br />
3、控制虚拟人物的头部姿势：你可以告诉GAIA让虚拟人物的头部做出特定的动作，比如点头或摇头。<br />
<br />
4、完全控制虚拟人物的表情：GAIA允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。<br />
<br />
5、根据文字指令生成虚拟人物动作：你可以给GAIA一些文字指令，比如“请微笑”，它就会创建一个按照这些指令动作的虚拟人物视频<br />
<br />
主要工作原理：<br />
<br />
1.分离运动和外观表示：<br />
<br />
•GAIA首先将每个视频帧分离成运动和外观两部分的表示。这意味着它可以区分哪些部分是因为说话而动（如嘴唇运动），哪些部分是保持不变的（如头发、眼睛的位置）。<br />
<br />
2.使用变分自编码器（VAE）：<br />
<br />
•VAE被用来编码视频帧中的这些分离表示，并从这些表示中重建原始帧。这个过程帮助模型学习如何准确地捕捉和再现人物的面部特征和表情。<br />
<br />
3.基于语音的运动序列生成：<br />
<br />
•扩散模型被优化以生成基于语音序列和参考肖像图片的运动序列。这意味着模型可以根据给定的语音输入（如一段对话）生成相应的面部运动。<br />
<br />
4.在推理过程中的应用：<br />
<br />
•在实际应用中，扩散模型接受输入的语音序列和参考肖像图片作为条件，并生成运动序列。然后，这些运动序列被解码成视频，展示虚拟头像的说话和表情动作。<br />
<br />
5.控制和文本指令的应用：<br />
•GAIA还允许通过编辑生成过程中的面部标记点来控制任意面部属性，或根据文本指令生成虚拟头像的视频剪辑。<br />
<br />
项目及演示：<a href="https://microsoft.github.io/GAIA/">microsoft.github.io/GAIA/</a><br />
论文：<a href="https://arxiv.org/abs/2311.15230">arxiv.org/abs/2311.15230</a><br />
GitHub：coming soon...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA1NDY5MjAwNDM4MTkwMDgvcHUvaW1nL3M3LTFzN1V2d3RySHMxbkEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730541672759799984#m</id>
            <title>R to @xiaohuggg: Seamless翻译演示：

https://x.com/multimodalart/status/1730319682098393451</title>
            <link>https://nitter.cz/xiaohuggg/status/1730541672759799984#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730541672759799984#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 10:57:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Seamless翻译演示：<br />
<br />
<a href="https://x.com/multimodalart/status/1730319682098393451">x.com/multimodalart/status/1…</a></p>
<p><a href="https://nitter.cz/multimodalart/status/1730319682098393451#m">nitter.cz/multimodalart/status/1730319682098393451#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730524761548370322#m</id>
            <title>R to @xiaohuggg: 我要杀死它...😂

老板又把邀请码增加到500了

燃烧他的GPU吧...🔥</title>
            <link>https://nitter.cz/xiaohuggg/status/1730524761548370322#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730524761548370322#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 09:50:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我要杀死它...😂<br />
<br />
老板又把邀请码增加到500了<br />
<br />
燃烧他的GPU吧...🔥</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FRT3UwcGFjQUEtc3JPLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730521049576722624#m</id>
            <title>教程：让ChatGPT完全控制你的电脑  

它像人类用户一样使用鼠标和键盘，自主进行操作

博主@MatthewBerman 使用了前几天另外一位博主@josh_bickett 开发的Self-Operating Computer（自动化计算机框架） 进行了这项测试。

该框架利用GPT 4V，让AI看着电脑屏幕，然后自主使用鼠标和键盘来完成任务。

这个框架设计得可以和不同的AI模型一起工作，目前已经可以和GPT-4v这样的模型集成。

目前，这个AI模型在估计鼠标点击的具体位置时还不够准确。项目团队正在开发一个新的AI模型（Agent-1-Vision），这个模型在预测鼠标点击位置方面会更准确。

感兴趣的可以去试试，视频中也有详细教程！

Self-Operating Computer：https://github.com/OthersideAI/self-operating-computer

视频来源：https://www.youtube.com/watch?v=UKRti40U8IA</title>
            <link>https://nitter.cz/xiaohuggg/status/1730521049576722624#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730521049576722624#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 09:35:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>教程：让ChatGPT完全控制你的电脑  <br />
<br />
它像人类用户一样使用鼠标和键盘，自主进行操作<br />
<br />
博主<a href="https://nitter.cz/MatthewBerman" title="MatthewBerman">@MatthewBerman</a> 使用了前几天另外一位博主<a href="https://nitter.cz/josh_bickett" title="Josh Bickett">@josh_bickett</a> 开发的Self-Operating Computer（自动化计算机框架） 进行了这项测试。<br />
<br />
该框架利用GPT 4V，让AI看着电脑屏幕，然后自主使用鼠标和键盘来完成任务。<br />
<br />
这个框架设计得可以和不同的AI模型一起工作，目前已经可以和GPT-4v这样的模型集成。<br />
<br />
目前，这个AI模型在估计鼠标点击的具体位置时还不够准确。项目团队正在开发一个新的AI模型（Agent-1-Vision），这个模型在预测鼠标点击位置方面会更准确。<br />
<br />
感兴趣的可以去试试，视频中也有详细教程！<br />
<br />
Self-Operating Computer：<a href="https://github.com/OthersideAI/self-operating-computer">github.com/OthersideAI/self-…</a><br />
<br />
视频来源：<a href="https://www.youtube.com/watch?v=UKRti40U8IA">youtube.com/watch?v=UKRti40U…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA1MTc4MTkyMzY5NjY0MDAvcHUvaW1nL2dPeDhIV1BLVXNtVTJsRDQuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730510553590952297#m</id>
            <title>嘴上说不要，身体还是很诚实

最后老板给了我100个邀请码

Code: HUG （通用的，100个领完就没了）

http://freepik.com/pikaso</title>
            <link>https://nitter.cz/xiaohuggg/status/1730510553590952297#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730510553590952297#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 08:54:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>嘴上说不要，身体还是很诚实<br />
<br />
最后老板给了我100个邀请码<br />
<br />
Code: HUG （通用的，100个领完就没了）<br />
<br />
<a href="http://freepik.com/pikaso">freepik.com/pikaso</a></p>
<p><a href="https://nitter.cz/xiaohuggg/status/1730485302593225108#m">nitter.cz/xiaohuggg/status/1730485302593225108#m</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FRQnpSOGFVQUFGV1A3LnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730485302593225108#m</id>
            <title>给你们展示个魔法，实时作画...

用的Freepik的Pikaso

拖拽小图标或者上传素材即可实时生图...

技术好的，可以自己画笔画，不过我感觉是内置了一些固定风格，不够自由！

体验地址：https://www.freepik.com/pikaso 

目前需要邀请码！（我看看能要到吗）</title>
            <link>https://nitter.cz/xiaohuggg/status/1730485302593225108#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730485302593225108#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 07:13:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>给你们展示个魔法，实时作画...<br />
<br />
用的Freepik的Pikaso<br />
<br />
拖拽小图标或者上传素材即可实时生图...<br />
<br />
技术好的，可以自己画笔画，不过我感觉是内置了一些固定风格，不够自由！<br />
<br />
体验地址：<a href="https://www.freepik.com/pikaso">freepik.com/pikaso</a> <br />
<br />
目前需要邀请码！（我看看能要到吗）</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0ODIyOTQ5NzExMTc1NjgvcHUvaW1nLzgtMmVSOWpsaFc0VVVRb0QuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476929500004627#m</id>
            <title>R to @xiaohuggg: 生成的另一种类型风格演示

你们可以玩玩</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476929500004627#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476929500004627#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:40:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>生成的另一种类型风格演示<br />
<br />
你们可以玩玩</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzY3OTI3MTEwOTAxNzYvcHUvaW1nLzIwbE13S2F4eFpoM0w2RmkuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476711857586572#m</id>
            <title>R to @xiaohuggg: 这是上面演示视频中生成的音乐

🎵</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476711857586572#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476711857586572#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:39:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这是上面演示视频中生成的音乐<br />
<br />
🎵</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzY1NDA2MjkzMjM3NzYvcHUvaW1nL2xrMEtwUHB3Mm9PeE9xbmsuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730476486820597883#m</id>
            <title>Stable Audio升级了一些新功能 生成音乐更可控

- 内置了风格提示库，随便点点就能生成音乐

- 支持通过上传音乐来生成音乐。

- 增加控制选项，如种子、步数、提示强度等

-能够生成并下载44.1 kHz立体声的高质量音频

- 现在可以直接通过链接分享生成的音乐

- 直接帮你把把生成好的音乐坐车了视频，方便下载

- 免费版本允许每月生成20个音轨，每个音轨最长45秒

- 付费版本每月11.99美元，提供500个音轨生成，每个音轨最长90秒

体验：http://stableaudio.com/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730476486820597883#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730476486820597883#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 06:38:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Stable Audio升级了一些新功能 生成音乐更可控<br />
<br />
- 内置了风格提示库，随便点点就能生成音乐<br />
<br />
- 支持通过上传音乐来生成音乐。<br />
<br />
- 增加控制选项，如种子、步数、提示强度等<br />
<br />
-能够生成并下载44.1 kHz立体声的高质量音频<br />
<br />
- 现在可以直接通过链接分享生成的音乐<br />
<br />
- 直接帮你把把生成好的音乐坐车了视频，方便下载<br />
<br />
- 免费版本允许每月生成20个音轨，每个音轨最长45秒<br />
<br />
- 付费版本每月11.99美元，提供500个音轨生成，每个音轨最长90秒<br />
<br />
体验：<a href="http://stableaudio.com/">stableaudio.com/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NzU3ODEwNjYwMTQ3MjAvcHUvaW1nL1EtSGhfMnlLRllQdGxSYnguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730456579382849756#m</id>
            <title>R to @xiaohuggg: 里面的视频都是这样

包含一个第一人称视角和多个全方位的视角

让AI来学习人类是如何进行各种技能操作的</title>
            <link>https://nitter.cz/xiaohuggg/status/1730456579382849756#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730456579382849756#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 05:19:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>里面的视频都是这样<br />
<br />
包含一个第一人称视角和多个全方位的视角<br />
<br />
让AI来学习人类是如何进行各种技能操作的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NTYwNDUyNDY3MzAyNDAvcHUvaW1nL1I3a2VWN2ZJanVsYTlsUUouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730455784092549356#m</id>
            <title>Ego-Exo4D：用于视频学习和多模态感知研究的基础数据集

由Meta AI和15所大学共同开发，这个项目收集了两种类型的视频 ：

一种是戴着相机的人看到的（类似眼睛看世界）

另一种是从周围环境拍摄的（旁观者看这个人）

数据集目的是帮助AI更全面地学习和了解一个人在特定环境下是如何行动的。

Ego-Exo4D包含一个基础数据集和基准测试套件，旨在支持视频学习和多模态感知的研究。

主要特点：

1、首人称和第三人称视角的结合：

•Ego-Exo4D的核心是同时捕捉首人称“自我中心”视角（通过参与者佩戴的可穿戴相机）和多个“外心”视角（通过围绕参与者的相机）。
•这两种视角是互补的：自我中心视角揭示了参与者所看到和听到的内容，而外心视角揭示了周围的场景和上下文。

2、专注于熟练的人类活动：

•Ego-Exo4D专注于熟练的人类活动，如运动、音乐、烹饪、舞蹈和自行车维修。
•在视频中理解人类技能的AI进步可以促进许多应用，例如在增强现实（AR）系统中，佩戴智能眼镜的人可以通过虚拟AI教练的指导快速学习新技能。

3、数据集的规模和多样性：

•Ego-Exo4D构成了迄今为止最大的公共同步首人称和第三人称视频数据集。
•该数据集的建立需要招募不同领域的专家，汇集了多样化的人群来创建一个多方面的AI数据集。

4、多模态数据捕捉：

•使用Meta的独特Aria眼镜捕捉的所有自我中心视频都伴随着时间对齐的七通道音频、惯性测量单元（IMU）和两个广角灰度相机等传感器。
•所有数据序列还提供通过Project Aria的先进机器感知服务获得的眼动追踪、头部姿态和环境的3D点云。

5.基准测试和未来应用：

•除了数据，Ego-Exo4D还引入了基础任务的基准测试，以激发社区的努力。
•该项目的长远目标是使AI能够以新的方式帮助人们学习新技能，并为未来的机器人提供通过观察熟练的人类专家行动来获得复杂灵巧操作技能的洞察。

目的：

1、帮助研究人工智能：这些视频数据可以帮助科学家们训练和改进人工智能系统，让它们更好地理解人类的行为和技能，比如运动、音乐、烹饪、舞蹈和自行车维修。

2、应用于增强现实和机器人技术：这个项目的数据可以用来开发新的增强现实应用（比如通过智能眼镜学习新技能）和帮助机器人学习如何模仿人类的动作。

3、提供大量多样的视频：Ego-Exo4D收集了大量不同场景和活动的视频，这对于研究人工智能来说是一个宝贵的资源。

4、推动人工智能的新挑战：这个项目不仅提供数据，还设立了一些挑战，鼓励科学家们开发新技术，比如从视频中识别动作的细节，或者评估一个人做某件事的技能水平。

Ego-Exo4D项目就像是一个超级工具箱，可以帮助科学家们让人工智能更好地理解和模仿人类。

详细介绍：https://ai.meta.com/blog/ego-exo4d-video-learning-perception/
论文：https://ego-exo4d-data.org/paper/ego-exo4d.pdf
Ego-Exo4D网站：https://ego-exo4d-data.org/</title>
            <link>https://nitter.cz/xiaohuggg/status/1730455784092549356#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730455784092549356#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 05:16:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Ego-Exo4D：用于视频学习和多模态感知研究的基础数据集<br />
<br />
由Meta AI和15所大学共同开发，这个项目收集了两种类型的视频 ：<br />
<br />
一种是戴着相机的人看到的（类似眼睛看世界）<br />
<br />
另一种是从周围环境拍摄的（旁观者看这个人）<br />
<br />
数据集目的是帮助AI更全面地学习和了解一个人在特定环境下是如何行动的。<br />
<br />
Ego-Exo4D包含一个基础数据集和基准测试套件，旨在支持视频学习和多模态感知的研究。<br />
<br />
主要特点：<br />
<br />
1、首人称和第三人称视角的结合：<br />
<br />
•Ego-Exo4D的核心是同时捕捉首人称“自我中心”视角（通过参与者佩戴的可穿戴相机）和多个“外心”视角（通过围绕参与者的相机）。<br />
•这两种视角是互补的：自我中心视角揭示了参与者所看到和听到的内容，而外心视角揭示了周围的场景和上下文。<br />
<br />
2、专注于熟练的人类活动：<br />
<br />
•Ego-Exo4D专注于熟练的人类活动，如运动、音乐、烹饪、舞蹈和自行车维修。<br />
•在视频中理解人类技能的AI进步可以促进许多应用，例如在增强现实（AR）系统中，佩戴智能眼镜的人可以通过虚拟AI教练的指导快速学习新技能。<br />
<br />
3、数据集的规模和多样性：<br />
<br />
•Ego-Exo4D构成了迄今为止最大的公共同步首人称和第三人称视频数据集。<br />
•该数据集的建立需要招募不同领域的专家，汇集了多样化的人群来创建一个多方面的AI数据集。<br />
<br />
4、多模态数据捕捉：<br />
<br />
•使用Meta的独特Aria眼镜捕捉的所有自我中心视频都伴随着时间对齐的七通道音频、惯性测量单元（IMU）和两个广角灰度相机等传感器。<br />
•所有数据序列还提供通过Project Aria的先进机器感知服务获得的眼动追踪、头部姿态和环境的3D点云。<br />
<br />
5.基准测试和未来应用：<br />
<br />
•除了数据，Ego-Exo4D还引入了基础任务的基准测试，以激发社区的努力。<br />
•该项目的长远目标是使AI能够以新的方式帮助人们学习新技能，并为未来的机器人提供通过观察熟练的人类专家行动来获得复杂灵巧操作技能的洞察。<br />
<br />
目的：<br />
<br />
1、帮助研究人工智能：这些视频数据可以帮助科学家们训练和改进人工智能系统，让它们更好地理解人类的行为和技能，比如运动、音乐、烹饪、舞蹈和自行车维修。<br />
<br />
2、应用于增强现实和机器人技术：这个项目的数据可以用来开发新的增强现实应用（比如通过智能眼镜学习新技能）和帮助机器人学习如何模仿人类的动作。<br />
<br />
3、提供大量多样的视频：Ego-Exo4D收集了大量不同场景和活动的视频，这对于研究人工智能来说是一个宝贵的资源。<br />
<br />
4、推动人工智能的新挑战：这个项目不仅提供数据，还设立了一些挑战，鼓励科学家们开发新技术，比如从视频中识别动作的细节，或者评估一个人做某件事的技能水平。<br />
<br />
Ego-Exo4D项目就像是一个超级工具箱，可以帮助科学家们让人工智能更好地理解和模仿人类。<br />
<br />
详细介绍：<a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">ai.meta.com/blog/ego-exo4d-v…</a><br />
论文：<a href="https://ego-exo4d-data.org/paper/ego-exo4d.pdf">ego-exo4d-data.org/paper/ego…</a><br />
Ego-Exo4D网站：<a href="https://ego-exo4d-data.org/">ego-exo4d-data.org/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0NTQ3NTYzNzg5OTI2NDAvcHUvaW1nL19KdHp5S2JPRV9uVjJiVEYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730427886065324492#m</id>
            <title>R to @xiaohuggg:</title>
            <link>https://nitter.cz/xiaohuggg/status/1730427886065324492#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730427886065324492#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 03:25:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MjY2MDIyNzY5OTkxNjgvcHUvaW1nL3BUSlAteWNDcU5sSmQ5MmcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730427883779404184#m</id>
            <title>Pikaso 即将推出的实时摄像头画画功能

利用摄像头可以实时进行图像生成

真实炫酷啊...

技术发展太快了...

@freepik 的老板 @ompemi 给了我一个邀请码，我一会录个视频...</title>
            <link>https://nitter.cz/xiaohuggg/status/1730427883779404184#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730427883779404184#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 03:25:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Pikaso 即将推出的实时摄像头画画功能<br />
<br />
利用摄像头可以实时进行图像生成<br />
<br />
真实炫酷啊...<br />
<br />
技术发展太快了...<br />
<br />
<a href="https://nitter.cz/freepik" title="Freepik">@freepik</a> 的老板 <a href="https://nitter.cz/ompemi" title="Omar Pera">@ompemi</a> 给了我一个邀请码，我一会录个视频...</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MjY1ODA1NjcyNzM0NzIvcHUvaW1nL0I3UkVybkt0UVNLYWxoc3AuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730421265754927482#m</id>
            <title>空间计算技术如何彻底改变我们观看体育赛事的方式?

LIVEPLEX展示了通过空间计算技术，你可以在家沉浸式观看体育赛事的演示。

你可以任意改变观看角度，近距离观察喜欢的运动员，甚至从教练的角度分析比赛。

这不仅仅是观看比赛，更是一种全新的体验方式，以前只有亲自到场才能体验到。

这项技术将在2024年开始推广应用。

主要技术特点：

- 互动性增强：空间计算提供了一种更加互动和沉浸式的观看体验。

- 多角度观看：观众可以自由改变观看角度，获得不同的视角。

- 深入体验：提供了一种近乎身临其境的体验，让观众能够更深入地了解和体验比赛。

这项技术不仅改变了观看体育赛事的方式，也为家庭娱乐和体育传播带来了新的可能性。

空间计算是一种集成了增强现实（AR）、虚拟现实（VR）、计算机视觉和人工智能（AI）等多个技术的先进计算形式。它的核心在于创造一个能够与现实世界交互的数字环境。

以下是空间计算的一些关键技术原理：

增强现实（AR）和虚拟现实（VR）：AR通过在用户的现实世界视野中叠加数字信息来增强现实体验。
VR则是创造一个完全虚拟的环境，用户可以在其中进行交互。

在观看体育赛事的应用中，这些技术可以用来创建一个仿佛置身于体育场的体验，允许用户从不同角度观看比赛。

计算机视觉：计算机视觉技术使计算机能够理解和解释数字图像和视频。在空间计算中，这项技术用于追踪用户的动作和环境，以便适当地调整数字内容。

人工智能（AI）：AI在空间计算中用于处理和响应用户的行为，以及优化用户体验。AI可以用于个性化设置，比如根据用户的偏好调整视角或提供特定的游戏分析。

传感器和追踪技术：空间计算设备通常配备有多种传感器，如摄像头、红外传感器和运动追踪器。
这些传感器帮助系统理解用户的位置和动作，从而提供适应用户移动的交互体验。

空间计算通过结合这些技术，创造了一种新的交互方式，使用户能够以前所未有的方式体验数字内容。在观看体育赛事的应用中，这意味着用户可以获得更加沉浸和互动的体验，仿佛亲自在现场一样。</title>
            <link>https://nitter.cz/xiaohuggg/status/1730421265754927482#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730421265754927482#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 02:59:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>空间计算技术如何彻底改变我们观看体育赛事的方式?<br />
<br />
LIVEPLEX展示了通过空间计算技术，你可以在家沉浸式观看体育赛事的演示。<br />
<br />
你可以任意改变观看角度，近距离观察喜欢的运动员，甚至从教练的角度分析比赛。<br />
<br />
这不仅仅是观看比赛，更是一种全新的体验方式，以前只有亲自到场才能体验到。<br />
<br />
这项技术将在2024年开始推广应用。<br />
<br />
主要技术特点：<br />
<br />
- 互动性增强：空间计算提供了一种更加互动和沉浸式的观看体验。<br />
<br />
- 多角度观看：观众可以自由改变观看角度，获得不同的视角。<br />
<br />
- 深入体验：提供了一种近乎身临其境的体验，让观众能够更深入地了解和体验比赛。<br />
<br />
这项技术不仅改变了观看体育赛事的方式，也为家庭娱乐和体育传播带来了新的可能性。<br />
<br />
空间计算是一种集成了增强现实（AR）、虚拟现实（VR）、计算机视觉和人工智能（AI）等多个技术的先进计算形式。它的核心在于创造一个能够与现实世界交互的数字环境。<br />
<br />
以下是空间计算的一些关键技术原理：<br />
<br />
增强现实（AR）和虚拟现实（VR）：AR通过在用户的现实世界视野中叠加数字信息来增强现实体验。<br />
VR则是创造一个完全虚拟的环境，用户可以在其中进行交互。<br />
<br />
在观看体育赛事的应用中，这些技术可以用来创建一个仿佛置身于体育场的体验，允许用户从不同角度观看比赛。<br />
<br />
计算机视觉：计算机视觉技术使计算机能够理解和解释数字图像和视频。在空间计算中，这项技术用于追踪用户的动作和环境，以便适当地调整数字内容。<br />
<br />
人工智能（AI）：AI在空间计算中用于处理和响应用户的行为，以及优化用户体验。AI可以用于个性化设置，比如根据用户的偏好调整视角或提供特定的游戏分析。<br />
<br />
传感器和追踪技术：空间计算设备通常配备有多种传感器，如摄像头、红外传感器和运动追踪器。<br />
这些传感器帮助系统理解用户的位置和动作，从而提供适应用户移动的交互体验。<br />
<br />
空间计算通过结合这些技术，创造了一种新的交互方式，使用户能够以前所未有的方式体验数字内容。在观看体育赛事的应用中，这意味着用户可以获得更加沉浸和互动的体验，仿佛亲自在现场一样。</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MTk4MDEwOTE0MjQyNTYvcHUvaW1nL2RaNmZIQlJCODZOZExaRWguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730405657994780909#m</id>
            <title>R to @xiaohuggg: 演示视频：

Whispering 耳语</title>
            <link>https://nitter.cz/xiaohuggg/status/1730405657994780909#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730405657994780909#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 01:57:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频：<br />
<br />
Whispering 耳语</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MDU2MzQ0MjI4NDk1MzYvcHUvaW1nL19CZXNXMVhkam5odXJ0R3kuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1730405325399105677#m</id>
            <title>R to @xiaohuggg: 演示视频：

Excited 兴奋的</title>
            <link>https://nitter.cz/xiaohuggg/status/1730405325399105677#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1730405325399105677#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 01 Dec 2023 01:55:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>演示视频：<br />
<br />
Excited 兴奋的</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MzA0MDUyNDI4OTI5MjY5NzYvcHUvaW1nL0RCMnVRbFNxdFJYeHhqRk0uanBn" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>