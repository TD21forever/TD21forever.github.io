<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>小互 / @xiaohuggg</title>
        <link>https://nitter.cz/xiaohuggg</link>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742779985012953468#m</id>
            <title>CoMoSVC：一种高效、高质量的歌声转换方法

它可以将一个人的歌声转换成另一个人的歌声。同时能够保持了声音的自然度和真实感。

最牛P的是CoMoSVC实现了一步采样。意思是它可以在单次操作中即可完成声音的转换，大大加快了处理速度。

该项目由香港大学和微软亚洲研究员开发，CoMoSVC 在高质量音频转换和快速处理速度之间提供了平衡，是SVC领域的重大进步。

CoMoSVC实现歌声转换的过程涉及几个关键步骤：

1、基于扩散的教师模型设计：首先，CoMoSVC设计了一个专门针对歌声转换的基于扩散的教师模型。这个模型通过学习大量的歌声数据，能够理解和模仿不同歌手的声音特征。

2、学生模型的提炼：接着，CoMoSVC利用自我一致性属性进一步提炼出一个学生模型。这个过程涉及从教师模型中提取关键信息，并简化模型结构，以便于快速有效地进行声音转换。

3、一步采样过程：不同于传统的迭代采样过程，CoMoSVC实现了一步采样。这意味着它可以在单次操作中完成声音的转换，大大加快了处理速度。

4、音频质量和速度的平衡：CoMoSVC在保持高音质转换的同时，优化了推理速度。这是通过精心设计的模型架构和算法优化实现的，确保转换后的音频既自然又忠实于目标歌手的风格。

在传统的基于扩散的声音转换模型中，通常需要多个迭代步骤来逐渐生成目标音频，这个过程可能既复杂又耗时。而CoMoSVC通过其创新的模型设计和算法优化，实现了快速且高效的一步采样，这大大减少了转换所需的时间，同时保持了音频质量。

这种一步采样的方法使CoMoSVC在实际应用中更加实用，特别是在需要快速处理大量数据的场景，如实时音频处理、音乐制作等领域。

项目及演示：https://comosvc.github.io/
论文：https://arxiv.org/pdf/2401.01792.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1742779985012953468#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742779985012953468#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 05:28:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>CoMoSVC：一种高效、高质量的歌声转换方法<br />
<br />
它可以将一个人的歌声转换成另一个人的歌声。同时能够保持了声音的自然度和真实感。<br />
<br />
最牛P的是CoMoSVC实现了一步采样。意思是它可以在单次操作中即可完成声音的转换，大大加快了处理速度。<br />
<br />
该项目由香港大学和微软亚洲研究员开发，CoMoSVC 在高质量音频转换和快速处理速度之间提供了平衡，是SVC领域的重大进步。<br />
<br />
CoMoSVC实现歌声转换的过程涉及几个关键步骤：<br />
<br />
1、基于扩散的教师模型设计：首先，CoMoSVC设计了一个专门针对歌声转换的基于扩散的教师模型。这个模型通过学习大量的歌声数据，能够理解和模仿不同歌手的声音特征。<br />
<br />
2、学生模型的提炼：接着，CoMoSVC利用自我一致性属性进一步提炼出一个学生模型。这个过程涉及从教师模型中提取关键信息，并简化模型结构，以便于快速有效地进行声音转换。<br />
<br />
3、一步采样过程：不同于传统的迭代采样过程，CoMoSVC实现了一步采样。这意味着它可以在单次操作中完成声音的转换，大大加快了处理速度。<br />
<br />
4、音频质量和速度的平衡：CoMoSVC在保持高音质转换的同时，优化了推理速度。这是通过精心设计的模型架构和算法优化实现的，确保转换后的音频既自然又忠实于目标歌手的风格。<br />
<br />
在传统的基于扩散的声音转换模型中，通常需要多个迭代步骤来逐渐生成目标音频，这个过程可能既复杂又耗时。而CoMoSVC通过其创新的模型设计和算法优化，实现了快速且高效的一步采样，这大大减少了转换所需的时间，同时保持了音频质量。<br />
<br />
这种一步采样的方法使CoMoSVC在实际应用中更加实用，特别是在需要快速处理大量数据的场景，如实时音频处理、音乐制作等领域。<br />
<br />
项目及演示：<a href="https://comosvc.github.io/">comosvc.github.io/</a><br />
论文：<a href="https://arxiv.org/pdf/2401.01792.pdf">arxiv.org/pdf/2401.01792.pdf</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3NzkwNTA3NjUyMzgyNzIvcHUvaW1nL09NYTQ3XzhXNklFcHZTUl8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742768663307190274#m</id>
            <title>SeeAct：一个基于GPT-4V通用网络代理

它可以在多种不同网站上识别网页上各种元素，执行各种不同的任务，

例如，在苹果官网上比较iPhone 15 Pro Max和iPhone 13 Pro Max的区别，并给出购买建议。

在旅游网站上搜索航班信息等。

SeeAct展示了从推测性规划、网页内容推理到错误自我纠正等多种能力。

SeeAct的创新之处在于它结合了多模态模型的视觉感知能力和自然语言处理能力，使其能够理解和操作网页内容。

SeeAct的主要能力：

1、执行网站任务：SeeAct能够在任何网站上执行特定任务，例如在电商网站上比较产品、在旅游网站上搜索航班信息等。

2、动作生成：模拟人类浏览网页，分析任务和之前的动作，生成动作描述。它首先进行动作生成，即产生完成任务所需每一步的文本描述。例如，如果任务是在苹果官网上比较两款iPhone，SeeAct会生成如“导航到iPhone分类”这样的动作描述。

3、动作定位：接着进行动作定位，识别网页上与动作描述相对应的HTML元素和操作。例如，它会找到并识别“iPhone”按钮，并执行点击操作。

4、多种能力展示：SeeAct展示了多种能力，包括推测性规划（预测接下来的步骤）、网页内容推理（理解网页上的信息）和错误自我纠正（识别并纠正之前的错误）。

5、适用于多种网站：SeeAct不仅限于特定类型的网站，它能够适应并在多种不同的网站上执行任务。

项目及演示：https://osu-nlp-group.github.io/SeeAct/
论文：https://arxiv.org/abs/2401.01614
GitHub：https://github.com/OSU-NLP-Group/SeeAct</title>
            <link>https://nitter.cz/xiaohuggg/status/1742768663307190274#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742768663307190274#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 04:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SeeAct：一个基于GPT-4V通用网络代理<br />
<br />
它可以在多种不同网站上识别网页上各种元素，执行各种不同的任务，<br />
<br />
例如，在苹果官网上比较iPhone 15 Pro Max和iPhone 13 Pro Max的区别，并给出购买建议。<br />
<br />
在旅游网站上搜索航班信息等。<br />
<br />
SeeAct展示了从推测性规划、网页内容推理到错误自我纠正等多种能力。<br />
<br />
SeeAct的创新之处在于它结合了多模态模型的视觉感知能力和自然语言处理能力，使其能够理解和操作网页内容。<br />
<br />
SeeAct的主要能力：<br />
<br />
1、执行网站任务：SeeAct能够在任何网站上执行特定任务，例如在电商网站上比较产品、在旅游网站上搜索航班信息等。<br />
<br />
2、动作生成：模拟人类浏览网页，分析任务和之前的动作，生成动作描述。它首先进行动作生成，即产生完成任务所需每一步的文本描述。例如，如果任务是在苹果官网上比较两款iPhone，SeeAct会生成如“导航到iPhone分类”这样的动作描述。<br />
<br />
3、动作定位：接着进行动作定位，识别网页上与动作描述相对应的HTML元素和操作。例如，它会找到并识别“iPhone”按钮，并执行点击操作。<br />
<br />
4、多种能力展示：SeeAct展示了多种能力，包括推测性规划（预测接下来的步骤）、网页内容推理（理解网页上的信息）和错误自我纠正（识别并纠正之前的错误）。<br />
<br />
5、适用于多种网站：SeeAct不仅限于特定类型的网站，它能够适应并在多种不同的网站上执行任务。<br />
<br />
项目及演示：<a href="https://osu-nlp-group.github.io/SeeAct/">osu-nlp-group.github.io/SeeA…</a><br />
论文：<a href="https://arxiv.org/abs/2401.01614">arxiv.org/abs/2401.01614</a><br />
GitHub：<a href="https://github.com/OSU-NLP-Group/SeeAct">github.com/OSU-NLP-Group/See…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3NTc4NDAyNDU0NDQ2MDgvcHUvaW1nL0xhdGFraGYzUkY2RnJTdzcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742749156589162708#m</id>
            <title>R to @xiaohuggg: 效果展示</title>
            <link>https://nitter.cz/xiaohuggg/status/1742749156589162708#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742749156589162708#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 03:25:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>效果展示</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5ODdQemFZQUEwdVE2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5OUIxaGFvQUVMWXNkLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742749153267257699#m</id>
            <title>AnyText：解决图像生成中，文字无法和图融合、变形、乱码的问题

该项目由阿里巴巴开发，AnyText支持在图像中生成和编辑多种语言的文本，使其与背景无缝融合。

该模型还解决了合成文本中模糊、不可读或错误字符的问题。

AnyText可以与现有的扩散模型集成，用于准确渲染或编辑文本。

AnyText能够在图像中高度精确地生成或编辑文本。例如，它可以在一张照片中添加逼真的文本标签，或者更改图像中现有文本的内容，同时保持自然的外观和与背景的一致性。

能够处理和生成多种不同语言环境下的文本。它可以生成包括中文、英文、日文、韩文等在内的多种语言的文本。

AnyText的组成部分：

1、辅助潜在模块

功能：这个模块负责生成图像中文本的潜在特征。它使用文本字形（即文本的视觉表示）、文本在图像中的位置和遮罩图像（可能用于指示文本应该出现的区域）作为输入。

作用：通过分析这些输入，辅助潜在模块能够理解文本应该如何在特定图像中呈现，包括文本的样式、大小和位置。这对于在图像中生成新文本或编辑现有文本至关重要。

2、文本嵌入模块

功能：这个模块结合了OCR（光学字符识别）模型编码的笔画数据和图像标题嵌入。OCR模型提取图像中现有文本的特征，而图像标题嵌入则提供了图像内容的语义理解。

作用：文本嵌入模块将这些信息融合在一起，生成与图像背景和上下文无缝融合的文本。这意味着生成的文本不仅在视觉上与背景匹配，而且在语义上与图像的整体主题一致。

AnyText还提供的一个大规模多语言文本图像数据集：

AnyWord-3M数据集

数据集规模：包含约303万个图像-文本对，涵盖多种语言。

文本行和字符统计：中文：约290万行文本，包含约1509万个字符。

英文：约627万行文本，包含约635万个字符。

其他语言：包括日文、韩文、阿拉伯语、孟加拉语和印地语，共约1.17万行文本，约5.95万个字符。

OCR注释：数据集中的图像-文本对都带有OCR（光学字符识别）注释，这有助于训练和评估文本生成模型。
多语言支持：数据集支持多种语言，包括中文、英文、日文、韩文、阿拉伯语、孟加拉语和印地语，使其适用于多语言视觉文本生成和编辑任务。

GitHub：https://github.com/tyxsspa/AnyText
论文：https://arxiv.org/abs/2311.03054
ModelScope 在线演示：http://modelscope.cn/studios/damo/studio_anytext
HuggingFace 在线演示：http://huggingface.co/spaces/modelscope/AnyText</title>
            <link>https://nitter.cz/xiaohuggg/status/1742749153267257699#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742749153267257699#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 03:25:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AnyText：解决图像生成中，文字无法和图融合、变形、乱码的问题<br />
<br />
该项目由阿里巴巴开发，AnyText支持在图像中生成和编辑多种语言的文本，使其与背景无缝融合。<br />
<br />
该模型还解决了合成文本中模糊、不可读或错误字符的问题。<br />
<br />
AnyText可以与现有的扩散模型集成，用于准确渲染或编辑文本。<br />
<br />
AnyText能够在图像中高度精确地生成或编辑文本。例如，它可以在一张照片中添加逼真的文本标签，或者更改图像中现有文本的内容，同时保持自然的外观和与背景的一致性。<br />
<br />
能够处理和生成多种不同语言环境下的文本。它可以生成包括中文、英文、日文、韩文等在内的多种语言的文本。<br />
<br />
AnyText的组成部分：<br />
<br />
1、辅助潜在模块<br />
<br />
功能：这个模块负责生成图像中文本的潜在特征。它使用文本字形（即文本的视觉表示）、文本在图像中的位置和遮罩图像（可能用于指示文本应该出现的区域）作为输入。<br />
<br />
作用：通过分析这些输入，辅助潜在模块能够理解文本应该如何在特定图像中呈现，包括文本的样式、大小和位置。这对于在图像中生成新文本或编辑现有文本至关重要。<br />
<br />
2、文本嵌入模块<br />
<br />
功能：这个模块结合了OCR（光学字符识别）模型编码的笔画数据和图像标题嵌入。OCR模型提取图像中现有文本的特征，而图像标题嵌入则提供了图像内容的语义理解。<br />
<br />
作用：文本嵌入模块将这些信息融合在一起，生成与图像背景和上下文无缝融合的文本。这意味着生成的文本不仅在视觉上与背景匹配，而且在语义上与图像的整体主题一致。<br />
<br />
AnyText还提供的一个大规模多语言文本图像数据集：<br />
<br />
AnyWord-3M数据集<br />
<br />
数据集规模：包含约303万个图像-文本对，涵盖多种语言。<br />
<br />
文本行和字符统计：中文：约290万行文本，包含约1509万个字符。<br />
<br />
英文：约627万行文本，包含约635万个字符。<br />
<br />
其他语言：包括日文、韩文、阿拉伯语、孟加拉语和印地语，共约1.17万行文本，约5.95万个字符。<br />
<br />
OCR注释：数据集中的图像-文本对都带有OCR（光学字符识别）注释，这有助于训练和评估文本生成模型。<br />
多语言支持：数据集支持多种语言，包括中文、英文、日文、韩文、阿拉伯语、孟加拉语和印地语，使其适用于多语言视觉文本生成和编辑任务。<br />
<br />
GitHub：<a href="https://github.com/tyxsspa/AnyText">github.com/tyxsspa/AnyText</a><br />
论文：<a href="https://arxiv.org/abs/2311.03054">arxiv.org/abs/2311.03054</a><br />
ModelScope 在线演示：<a href="http://modelscope.cn/studios/damo/studio_anytext">modelscope.cn/studios/damo/s…</a><br />
HuggingFace 在线演示：<a href="http://huggingface.co/spaces/modelscope/AnyText">huggingface.co/spaces/models…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5NzdoOGEwQUFHTG1nLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742736494509588846#m</id>
            <title>R to @xiaohuggg: 视频演示里生成的照片 

嘿嘿，我感觉效果还是很不错的</title>
            <link>https://nitter.cz/xiaohuggg/status/1742736494509588846#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742736494509588846#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 02:35:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视频演示里生成的照片 <br />
<br />
嘿嘿，我感觉效果还是很不错的</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSRmIwQUE2M1VTLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSRmJJQUE3alVaLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSSGFzQUFIa3h0LnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5eGdSRWJZQUFFZHFLLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742736491292606866#m</id>
            <title>IP-Adapter-FaceID：上传自己照片 分分钟克隆一个自己

该模型利用面部识别模型的面部ID嵌入，可以更准确地捕捉和再现特定人物的面部特征。

结合文本描述生成可以生成高度个性化且与原始面部特征一致的图像。

意思就是你只要上传几张自己的照片，就能生成你在各种场景下的照片，克隆你的脸。

模型地址：https://huggingface.co/h94/IP-Adapter-FaceID
在线体验：https://huggingface.co/spaces/multimodalart/Ip-Adapter-FaceID</title>
            <link>https://nitter.cz/xiaohuggg/status/1742736491292606866#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742736491292606866#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 02:35:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>IP-Adapter-FaceID：上传自己照片 分分钟克隆一个自己<br />
<br />
该模型利用面部识别模型的面部ID嵌入，可以更准确地捕捉和再现特定人物的面部特征。<br />
<br />
结合文本描述生成可以生成高度个性化且与原始面部特征一致的图像。<br />
<br />
意思就是你只要上传几张自己的照片，就能生成你在各种场景下的照片，克隆你的脸。<br />
<br />
模型地址：<a href="https://huggingface.co/h94/IP-Adapter-FaceID">huggingface.co/h94/IP-Adapte…</a><br />
在线体验：<a href="https://huggingface.co/spaces/multimodalart/Ip-Adapter-FaceID">huggingface.co/spaces/multim…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3MzYyOTI3NzE5NTg3ODQvcHUvaW1nLzV2T0pfb2FUd1BNZklJdDEuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742730134753550369#m</id>
            <title>兄弟们 SEO过时了，现在是GEO了 😂

随着 Bard &amp; Perplexity 等基于 LLM 的搜索引擎的崛起，机器人直接输出答案，这让内容创建者通过SEO来改进他们的网站，已经逐渐变得越来越难。

普林斯顿大学、艾伦科技研究所提出了GEO的概念：生成引擎优化。

他们提出了一个专门针对生成引擎的印象度量标准！

实验表明，使用GEO的简单策略可以在商业生成引擎上显著提高内容的可见性，提升幅度高达40%。

生成引擎与传统搜索引擎的区别：

- 传统搜索引擎通常提供一个链接列表，直接指向相关网页。

- 生成引擎则使用大型语言模型（LLM）来生成更丰富、综合性的回答，这些回答可能直接包含了用户查询的答案，而不仅仅是链接。

GEO的印象度量标准（Customized Visibility Metrics:）：

1、内容可见性：衡量内容在生成引擎回答中出现的频率和显著性。例如，一个网站的信息是否经常被引擎用来构建回答。

2、信息准确性：评估生成引擎提供的信息与原始内容的一致性。这对于确保生成引擎正确理解和呈现网站内容非常重要。

3、用户参与度：测量用户与生成引擎提供的内容的互动程度。这可能包括用户对生成回答的点击率、阅读时间等。

4、内容影响力：评估内容在生成引擎回答中的权威性和影响力。例如，内容是否被视为某个领域的权威来源。

通过这些专门设计的度量标准，GEO帮助内容创作者更好地理解他们的内容在生成引擎中的表现，并提供了优化这些内容以提高其在生成引擎中可见性和有效性的策略。

GEO的原理：

1、多模态理解：生成引擎不仅处理文本信息，还可能结合视觉和空间布局等其他模态的信息。GEO的原理包括理解这些多模态数据的处理方式。

2、内容综合性：与传统搜索引擎不同，生成引擎倾向于提供更加综合和完整的回答，而不是简单的链接。GEO的原理在于理解如何使内容更适合这种综合性的呈现。

3、语义理解：生成引擎使用先进的语言模型，能够深入理解内容的语义。GEO的原理包括优化内容以提高其在语义层面的清晰度和相关性。

GEO的策略：

1、结构化内容：优化网站和内容的结构，使其更容易被生成引擎解析和引用。这可能包括使用清晰的标题、子标题和元标签。

2、关键信息突出：确保重要信息（如产品特点、服务优势）容易被找到和理解，以便生成引擎可以有效地提取和使用这些信息。

3、增强语义相关性：使用关键词和短语来提高内容的语义相关性，使其更符合目标受众的搜索意图。

4、利用GEO度量标准：使用GEO提供的专门度量标准来评估和优化内容在生成引擎中的表现。

5、持续监测和调整：定期监测内容在生成引擎中的表现，并根据反馈进行调整。这可能包括分析用户行为数据和生成引擎的反馈。

6、适应生成引擎的变化：由于生成引擎和大型语言模型不断进化，GEO策略需要灵活适应这些变化，持续更新优化方法。

通过实施这些策略，GEO帮助内容创作者提高他们的网站和内容在新一代搜索引擎中的可见性和有效性，从而更好地满足用户的搜索需求。

全面的基准测试：GEO-BENCH

GEO引入了一个名为GEO-BENCH的多样化基准测试，包含10,000个查询，用于评估和比较不同优化方法的效果。这是一个针对 GEO 查询定制的基准，用于评估不同的策略。

10,000个查询：GEO-BENCH包含10,000个不同的查询，这些查询覆盖了多个领域、难度级别和类别。这样的多样性确保了基准测试能够全面评估不同类型的内容和优化策略。

数据集构成：这个基准测试由多个来源的数据集组成，包括MS Macro、ORCAS-1、Natural Questions等，这些数据集代表了不同类型的用户查询和搜索场景。

训练集和测试集：GEO-BENCH包括8,000个查询的训练集和各1,000个查询的验证集和测试集，使得内容创作者和研究人员能够在标准化的环境中训练和测试他们的优化策略。

公共排行榜：GEO-BENCH提供了一个公共排行榜，定期更新以展示最新的测试结果，从而促进不同方法之间的健康竞争和进步。

项目地址：https://generative-engines.com/GEO/
论文：https://arxiv.org/pdf/2311.09735.pdf
GitHub：https://github.com/GEO-optim/GEO
GEO-BENCH：https://huggingface.co/datasets/GEO-Optim/geo-bench</title>
            <link>https://nitter.cz/xiaohuggg/status/1742730134753550369#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742730134753550369#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 02:10:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们 SEO过时了，现在是GEO了 😂<br />
<br />
随着 Bard & Perplexity 等基于 LLM 的搜索引擎的崛起，机器人直接输出答案，这让内容创建者通过SEO来改进他们的网站，已经逐渐变得越来越难。<br />
<br />
普林斯顿大学、艾伦科技研究所提出了GEO的概念：生成引擎优化。<br />
<br />
他们提出了一个专门针对生成引擎的印象度量标准！<br />
<br />
实验表明，使用GEO的简单策略可以在商业生成引擎上显著提高内容的可见性，提升幅度高达40%。<br />
<br />
生成引擎与传统搜索引擎的区别：<br />
<br />
- 传统搜索引擎通常提供一个链接列表，直接指向相关网页。<br />
<br />
- 生成引擎则使用大型语言模型（LLM）来生成更丰富、综合性的回答，这些回答可能直接包含了用户查询的答案，而不仅仅是链接。<br />
<br />
GEO的印象度量标准（Customized Visibility Metrics:）：<br />
<br />
1、内容可见性：衡量内容在生成引擎回答中出现的频率和显著性。例如，一个网站的信息是否经常被引擎用来构建回答。<br />
<br />
2、信息准确性：评估生成引擎提供的信息与原始内容的一致性。这对于确保生成引擎正确理解和呈现网站内容非常重要。<br />
<br />
3、用户参与度：测量用户与生成引擎提供的内容的互动程度。这可能包括用户对生成回答的点击率、阅读时间等。<br />
<br />
4、内容影响力：评估内容在生成引擎回答中的权威性和影响力。例如，内容是否被视为某个领域的权威来源。<br />
<br />
通过这些专门设计的度量标准，GEO帮助内容创作者更好地理解他们的内容在生成引擎中的表现，并提供了优化这些内容以提高其在生成引擎中可见性和有效性的策略。<br />
<br />
GEO的原理：<br />
<br />
1、多模态理解：生成引擎不仅处理文本信息，还可能结合视觉和空间布局等其他模态的信息。GEO的原理包括理解这些多模态数据的处理方式。<br />
<br />
2、内容综合性：与传统搜索引擎不同，生成引擎倾向于提供更加综合和完整的回答，而不是简单的链接。GEO的原理在于理解如何使内容更适合这种综合性的呈现。<br />
<br />
3、语义理解：生成引擎使用先进的语言模型，能够深入理解内容的语义。GEO的原理包括优化内容以提高其在语义层面的清晰度和相关性。<br />
<br />
GEO的策略：<br />
<br />
1、结构化内容：优化网站和内容的结构，使其更容易被生成引擎解析和引用。这可能包括使用清晰的标题、子标题和元标签。<br />
<br />
2、关键信息突出：确保重要信息（如产品特点、服务优势）容易被找到和理解，以便生成引擎可以有效地提取和使用这些信息。<br />
<br />
3、增强语义相关性：使用关键词和短语来提高内容的语义相关性，使其更符合目标受众的搜索意图。<br />
<br />
4、利用GEO度量标准：使用GEO提供的专门度量标准来评估和优化内容在生成引擎中的表现。<br />
<br />
5、持续监测和调整：定期监测内容在生成引擎中的表现，并根据反馈进行调整。这可能包括分析用户行为数据和生成引擎的反馈。<br />
<br />
6、适应生成引擎的变化：由于生成引擎和大型语言模型不断进化，GEO策略需要灵活适应这些变化，持续更新优化方法。<br />
<br />
通过实施这些策略，GEO帮助内容创作者提高他们的网站和内容在新一代搜索引擎中的可见性和有效性，从而更好地满足用户的搜索需求。<br />
<br />
全面的基准测试：GEO-BENCH<br />
<br />
GEO引入了一个名为GEO-BENCH的多样化基准测试，包含10,000个查询，用于评估和比较不同优化方法的效果。这是一个针对 GEO 查询定制的基准，用于评估不同的策略。<br />
<br />
10,000个查询：GEO-BENCH包含10,000个不同的查询，这些查询覆盖了多个领域、难度级别和类别。这样的多样性确保了基准测试能够全面评估不同类型的内容和优化策略。<br />
<br />
数据集构成：这个基准测试由多个来源的数据集组成，包括MS Macro、ORCAS-1、Natural Questions等，这些数据集代表了不同类型的用户查询和搜索场景。<br />
<br />
训练集和测试集：GEO-BENCH包括8,000个查询的训练集和各1,000个查询的验证集和测试集，使得内容创作者和研究人员能够在标准化的环境中训练和测试他们的优化策略。<br />
<br />
公共排行榜：GEO-BENCH提供了一个公共排行榜，定期更新以展示最新的测试结果，从而促进不同方法之间的健康竞争和进步。<br />
<br />
项目地址：<a href="https://generative-engines.com/GEO/">generative-engines.com/GEO/</a><br />
论文：<a href="https://arxiv.org/pdf/2311.09735.pdf">arxiv.org/pdf/2311.09735.pdf</a><br />
GitHub：<a href="https://github.com/GEO-optim/GEO">github.com/GEO-optim/GEO</a><br />
GEO-BENCH：<a href="https://huggingface.co/datasets/GEO-Optim/geo-bench">huggingface.co/datasets/GEO-…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5cFJWdmFZQUFPeHBGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742719780828963272#m</id>
            <title>R to @xiaohuggg: 开发者介绍：</title>
            <link>https://nitter.cz/xiaohuggg/status/1742719780828963272#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742719780828963272#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 01:29:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>开发者介绍：</p>
<p><a href="https://nitter.cz/tonyzzhao/status/1742603121682153852#m">nitter.cz/tonyzzhao/status/1742603121682153852#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742719653536006621#m</id>
            <title>Mobile ALOHA：一个可以模仿人类自主学习的机器人操作系统。

由坦福大学开发，专门设计用于执行需要双手和全身协调的复杂移动任务。

可以通过模仿学习（即观察人类操作然后模仿这些动作），仅通过50次任务演示，共同训练，它就能够自主完成日常生活中的各种任务。

如做饭、开柜放东西、自己坐电梯。

最重要的是：该机器人的软硬件全部都是开源的。

主要功能特点：

1、低成本全身远程操作系统：Mobile ALOHA是一个经济实惠的系统，它允许用户通过全身远程操作来收集数据。这种设计使得系统更易于普及和使用。

2、双手移动操作：该系统专注于模仿需要双手和全身控制的移动操作任务，这种能力在传统的桌面操作机器人中通常是缺失的。如烹饪、清洁或其他需要双手协作的活动。

3、自主模仿学习：Mobile ALOHA利用模仿学习技术，通过观察人类的演示来训练机器人执行复杂任务。通过每个任务50次演示，共同训练可以将成功率提高到90%，使Mobile ALOHA能够自主完成复杂的移动操作任务。

3、数据集共同训练：使用Mobile ALOHA收集的数据，研究团队进行了监督行为克隆，并发现与现有的静态ALOHA数据集共同训练可以提高移动操作任务的性能。

4、高成功率的任务执行：通过共同训练和模仿学习，Mobile ALOHA能够以高达90%的成功率自主完成复杂的移动操作任务。

5、多样化的应用场景：Mobile ALOHA能够执行多种复杂任务，如烹饪、打开柜门、操作电梯和清洁工作，展示了其广泛的应用潜力。

6、与物联网设备的兼容性：除了执行物理任务外，Mobile ALOHA还计划在2025年与物联网（IoT）设备连接，进一步扩展其应用范围。

Mobile ALOHA的硬件组成：

Mobile ALOHA被装在了一个为仓库设计的移动台座上：Tracer AGV。

它可以承载 100kg，移动速度高达 1.6m/s，而成本仅为 7k 美元，同时使得机器人的占地面积减少了 45%，重量减轻了 15 公斤。  机器人垂直高度可达65厘米至200厘米，距底座100厘米。

- 机械臂：Mobile ALOHA配备了两个机械臂，每个臂有多个自由度，使其能够执行复杂的双手操作任务。

- 移动基座：系统包括一个移动基座，使机器人能够在不同的环境中移动和定位。

- 摄像头：Mobile ALOHA配备了两个手腕摄像头和一个顶部摄像头，用于捕捉环境和操作任务的视觉信息。

- 自带电源和计算能力：系统具备自带的电源和计算能力，使其能夠独立完成任务而不依赖于外部电源或计算设备。

- 技术规格：
• 重量：75公斤。
• 尺寸：80宽 x 84长 x 140高厘米（不含操纵杆）；90宽 x 135长 x 140高厘米。
• 负载能力：每个手臂750克，基座55公斤。
• 手臂重复定位精度：1毫米。
• 手臂定位精度：5-8毫米。
• 电池寿命：12小时（1620Wh）。
• 最大拉力：100牛顿，垂直距离100厘米。
• 滚动阻力：13牛顿（乙烯基地板）。
        • 移动速度：可以达到人类正常步行的1.42米/秒

项目及演示：https://mobile-aloha.github.io/
学习代码：https://github.com/MarkFzp/act-plus-plus
硬件代码：https://github.com/MarkFzp/mobile-aloha
论文：https://mobile-aloha.github.io/
教程：https://docs.google.com/document/d/1_3yhWjodSNNYlpxkRCPIlvIAaQ76Nqk2wsqhnEVM6Dc</title>
            <link>https://nitter.cz/xiaohuggg/status/1742719653536006621#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742719653536006621#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 01:28:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mobile ALOHA：一个可以模仿人类自主学习的机器人操作系统。<br />
<br />
由坦福大学开发，专门设计用于执行需要双手和全身协调的复杂移动任务。<br />
<br />
可以通过模仿学习（即观察人类操作然后模仿这些动作），仅通过50次任务演示，共同训练，它就能够自主完成日常生活中的各种任务。<br />
<br />
如做饭、开柜放东西、自己坐电梯。<br />
<br />
最重要的是：该机器人的软硬件全部都是开源的。<br />
<br />
主要功能特点：<br />
<br />
1、低成本全身远程操作系统：Mobile ALOHA是一个经济实惠的系统，它允许用户通过全身远程操作来收集数据。这种设计使得系统更易于普及和使用。<br />
<br />
2、双手移动操作：该系统专注于模仿需要双手和全身控制的移动操作任务，这种能力在传统的桌面操作机器人中通常是缺失的。如烹饪、清洁或其他需要双手协作的活动。<br />
<br />
3、自主模仿学习：Mobile ALOHA利用模仿学习技术，通过观察人类的演示来训练机器人执行复杂任务。通过每个任务50次演示，共同训练可以将成功率提高到90%，使Mobile ALOHA能够自主完成复杂的移动操作任务。<br />
<br />
3、数据集共同训练：使用Mobile ALOHA收集的数据，研究团队进行了监督行为克隆，并发现与现有的静态ALOHA数据集共同训练可以提高移动操作任务的性能。<br />
<br />
4、高成功率的任务执行：通过共同训练和模仿学习，Mobile ALOHA能够以高达90%的成功率自主完成复杂的移动操作任务。<br />
<br />
5、多样化的应用场景：Mobile ALOHA能够执行多种复杂任务，如烹饪、打开柜门、操作电梯和清洁工作，展示了其广泛的应用潜力。<br />
<br />
6、与物联网设备的兼容性：除了执行物理任务外，Mobile ALOHA还计划在2025年与物联网（IoT）设备连接，进一步扩展其应用范围。<br />
<br />
Mobile ALOHA的硬件组成：<br />
<br />
Mobile ALOHA被装在了一个为仓库设计的移动台座上：Tracer AGV。<br />
<br />
它可以承载 100kg，移动速度高达 1.6m/s，而成本仅为 7k 美元，同时使得机器人的占地面积减少了 45%，重量减轻了 15 公斤。  机器人垂直高度可达65厘米至200厘米，距底座100厘米。<br />
<br />
- 机械臂：Mobile ALOHA配备了两个机械臂，每个臂有多个自由度，使其能够执行复杂的双手操作任务。<br />
<br />
- 移动基座：系统包括一个移动基座，使机器人能够在不同的环境中移动和定位。<br />
<br />
- 摄像头：Mobile ALOHA配备了两个手腕摄像头和一个顶部摄像头，用于捕捉环境和操作任务的视觉信息。<br />
<br />
- 自带电源和计算能力：系统具备自带的电源和计算能力，使其能夠独立完成任务而不依赖于外部电源或计算设备。<br />
<br />
- 技术规格：<br />
• 重量：75公斤。<br />
• 尺寸：80宽 x 84长 x 140高厘米（不含操纵杆）；90宽 x 135长 x 140高厘米。<br />
• 负载能力：每个手臂750克，基座55公斤。<br />
• 手臂重复定位精度：1毫米。<br />
• 手臂定位精度：5-8毫米。<br />
• 电池寿命：12小时（1620Wh）。<br />
• 最大拉力：100牛顿，垂直距离100厘米。<br />
• 滚动阻力：13牛顿（乙烯基地板）。<br />
        • 移动速度：可以达到人类正常步行的1.42米/秒<br />
<br />
项目及演示：<a href="https://mobile-aloha.github.io/">mobile-aloha.github.io/</a><br />
学习代码：<a href="https://github.com/MarkFzp/act-plus-plus">github.com/MarkFzp/act-plus-…</a><br />
硬件代码：<a href="https://github.com/MarkFzp/mobile-aloha">github.com/MarkFzp/mobile-al…</a><br />
论文：<a href="https://mobile-aloha.github.io/">mobile-aloha.github.io/</a><br />
教程：<a href="https://docs.google.com/document/d/1_3yhWjodSNNYlpxkRCPIlvIAaQ76Nqk2wsqhnEVM6Dc">docs.google.com/document/d/1…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI3MTk0MTcwOTA0OTg1NjAvcHUvaW1nL0dWNVdSSGJLVF9oRllXUHguanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742710713091788984#m</id>
            <title>SpaceX发射了首批六颗具备直接对接蜂窝网络（Direct to Cell）能力的Starlink卫星。

这将允许在地球上任何地方可以使用手机和Starlink卫星连接。

这些卫星配备了先进的调制解调器，使得全球各地的移动网络运营商能够使用Starlink提供无缝的全球通话和网络服务，而无需更换硬件或固件。
除了美国的T-Mobile外，其他几个国家的运营商也已签约使用这些直接对接蜂窝网络的卫星。SpaceX提到的其他运营商包括加拿大的Rogers、日本的KDDI、澳大利亚的Optus、新西兰的One NZ、瑞士的Salt以及智利和秘鲁的Entel。

尽管SpaceX首席执行官埃隆·马斯克（Elon Musk）表示这些卫星将“允许在地球上任何地方进行手机连接”，但他也描述了一个重要的带宽限制。马斯克写道：“注意，这只支持每个波束约7Mb的数据传输，而且波束非常大，所以虽然这对于没有蜂窝网络连接的地区来说是一个很好的解决方案，但它与现有的陆地蜂窝网络相比并没有显著的竞争力。”
Starlink的直接对接蜂窝网络服务预计将在2024年提供文本消息服务，随后在2025年开始提供语音和数据服务。Starlink的低地轨道卫星将与标准LTE手机兼容，不像早期服务需要专门为卫星通信设计的手机。SpaceX的直接对接蜂窝网络卫星还将在2025年与物联网（IoT）设备连接。
官网：http://direct.starlink.com</title>
            <link>https://nitter.cz/xiaohuggg/status/1742710713091788984#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742710713091788984#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jan 2024 00:53:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SpaceX发射了首批六颗具备直接对接蜂窝网络（Direct to Cell）能力的Starlink卫星。<br />
<br />
这将允许在地球上任何地方可以使用手机和Starlink卫星连接。<br />
<br />
这些卫星配备了先进的调制解调器，使得全球各地的移动网络运营商能够使用Starlink提供无缝的全球通话和网络服务，而无需更换硬件或固件。<br />
除了美国的T-Mobile外，其他几个国家的运营商也已签约使用这些直接对接蜂窝网络的卫星。SpaceX提到的其他运营商包括加拿大的Rogers、日本的KDDI、澳大利亚的Optus、新西兰的One NZ、瑞士的Salt以及智利和秘鲁的Entel。<br />
<br />
尽管SpaceX首席执行官埃隆·马斯克（Elon Musk）表示这些卫星将“允许在地球上任何地方进行手机连接”，但他也描述了一个重要的带宽限制。马斯克写道：“注意，这只支持每个波束约7Mb的数据传输，而且波束非常大，所以虽然这对于没有蜂窝网络连接的地区来说是一个很好的解决方案，但它与现有的陆地蜂窝网络相比并没有显著的竞争力。”<br />
Starlink的直接对接蜂窝网络服务预计将在2024年提供文本消息服务，随后在2025年开始提供语音和数据服务。Starlink的低地轨道卫星将与标准LTE手机兼容，不像早期服务需要专门为卫星通信设计的手机。SpaceX的直接对接蜂窝网络卫星还将在2025年与物联网（IoT）设备连接。<br />
官网：<a href="http://direct.starlink.com">direct.starlink.com</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5WThRTWIwQUFyel81LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5Wk1vRGFZQUF5dEl1LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M5Wlk2YmJBQUF5WmlDLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742519700108808564#m</id>
            <title>比特币急速下挫

触及41000美元/枚，为12月18日来新低，跌幅一度超过10%。</title>
            <link>https://nitter.cz/xiaohuggg/status/1742519700108808564#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742519700108808564#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 12:14:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>比特币急速下挫<br />
<br />
触及41000美元/枚，为12月18日来新低，跌幅一度超过10%。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742473942252855795#m</id>
            <title>这下模型训练没有了版权和训练数据顾虑了🤓

微软研究团队最新成果：他们已经开始使用【合成数据】来训练AI模型了。

微软使用大语言模型生成了近100种语言、数十万个文本嵌入任务的“模拟”文本数据，然后用这些数据来训练 AI 。

这大幅度降低了训练成本，提高了效率，同时还减少了模型的偏见。

背景知识：

要让计算机理解和处理人类的语言，我们需要把语言（比如句子或段落）转换成计算机能理解的形式，这就是所谓的“文本嵌入”。文本嵌入就是把人类语言翻译成计算机的语言。

传统上，要让计算机做好这件事，我们需要给它看很多很多的例子（这就是所谓的训练数据），让它学习怎样把文本转换成它能理解的形式。但这个过程很复杂，需要很多数据和很长时间。

微软的这份论文提出了一种新方法：“合成数据”。

他们使用大语言模型（LLM）来生成了很多不同语言的“模拟”文本数据，然后用这些数据来训练 AI 理解人类语言。这样做的好处是，他们不需要真实的数据就能训练出很好的文本嵌入模型，而且这个过程比传统方法更快、更高效。

如何生成合成数据：

1、使用大语言模型：首先，他们利用了大型语言模型，如GPT-4或类似的高级模型。这些模型已经通过大量的文本数据进行了预训练，因此具有强大的语言生成能力。

2、任务定义和提示设计：研究团队定义了一系列文本嵌入任务，并为这些任务设计了特定的提示。这些提示被用来指导语言模型生成特定类型的文本。例如，他们可能会设计一个提示来生成关于某个特定主题的问答对，或者创建一个场景描述。

3、生成合成数据：接下来，研究团队使用这些提示来引导语言模型生成数据。模型根据给定的任务提示产生文本，这些文本涵盖了各种主题和风格。生成的文本是合成的，但质量足以模拟真实世界的语言使用情况。

4、多样性和覆盖率：为了确保生成的数据具有多样性并覆盖多种语言，研究团队可能会使用多种提示模板，并在多种语言中生成数据。这样可以确保模型不仅在资源丰富的语言（如英语）中表现良好，也能处理资源较少的语言。

5、数据清洗和格式化：生成的数据经过筛选和优化，确保质量和多样性。生成的数据需要经过清洗和格式化，以确保它们符合训练需要。这可能包括去除重复内容、修正格式错误等。

合成数据的优势：

通过这种方法，微软的研究团队能够生成大量高质量的合成数据，用于训练和改进大型语言模型，从而提高文本嵌入的质量。这种方法的优势在于它不依赖于大量的标注真实数据，从而减少了数据收集和处理的工作量，同时还能提供丰富多样的训练材料。

1、覆盖范围广：合成数据可以覆盖更广泛的场景和用例，包括那些在真实数据集中可能很少见或完全不存在的情况。这有助于模型学习更全面的语言模式和概念。这些数据覆盖了近100种语言的数十万个文本嵌入任务。这在传统数据收集方法中很难实现。

2、减少偏见：由于不依赖现实世界的数据集，合成数据可以减少因数据收集过程中的偏见和局限性而引入的问题。真实数据集可能包含偏见或不平衡（例如，某些群体的代表性不足）。通过合成数据，可以有意识地减少这些偏见，创建更公平和平衡的数据集。

3、灵活性和可扩展性：合成数据允许研究人员精确控制数据集的特性，如分布、复杂性和难度等，从而可以针对特定的研究或应用需求定制数据。因此生成合成数据的方法具有很高的灵活性，可以根据需要调整以生成各种类型的数据。

4、成本效率：收集和标注大量高质量的真实数据非常昂贵且耗时。相比之下，生成合成数据的成本通常更低，且过程更快。

5、快速迭代和改进：合成数据的生成过程可以根据模型性能的反馈快速调整，从而支持更快的迭代和改进。

6、隐私和安全：使用合成数据可以避免处理敏感或个人数据，从而减少隐私和安全风险。

实验结果表明：

1、数据生成统计：研究团队成功生成了大约50万个示例，其中包含15万个独特的指令。这些数据涵盖了93种不同的语言，其中英语占主导地位。

2、模型性能：在多种语言的MIRACL数据集上，使用合成数据训练的模型（E5mistral-7b）在nDCG@10和Recall@100两个指标上表现出色。这表明模型能够有效地检索相关文档，并且在多种语言上都有良好的表现。

3、对比训练的影响：在包含对比预训练的设置下，模型在多个数据集上的表现有所提升。这说明对比预训练对于提高模型性能是有益的。

4、多任务适应性：模型在多种任务类型上表现良好，包括文本检索、文本聚类、句子嵌入等，显示了其广泛的适用性。

这些实验结果表明，使用合成数据训练的大型语言模型在多语言、多任务场景中都能取得优异的性能，证明了合成数据方法的有效性和实用性。

论文：https://arxiv.org/abs/2401.00368
PDF：https://arxiv.org/pdf/2401.00368.pdf</title>
            <link>https://nitter.cz/xiaohuggg/status/1742473942252855795#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742473942252855795#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 09:12:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>这下模型训练没有了版权和训练数据顾虑了🤓<br />
<br />
微软研究团队最新成果：他们已经开始使用【合成数据】来训练AI模型了。<br />
<br />
微软使用大语言模型生成了近100种语言、数十万个文本嵌入任务的“模拟”文本数据，然后用这些数据来训练 AI 。<br />
<br />
这大幅度降低了训练成本，提高了效率，同时还减少了模型的偏见。<br />
<br />
背景知识：<br />
<br />
要让计算机理解和处理人类的语言，我们需要把语言（比如句子或段落）转换成计算机能理解的形式，这就是所谓的“文本嵌入”。文本嵌入就是把人类语言翻译成计算机的语言。<br />
<br />
传统上，要让计算机做好这件事，我们需要给它看很多很多的例子（这就是所谓的训练数据），让它学习怎样把文本转换成它能理解的形式。但这个过程很复杂，需要很多数据和很长时间。<br />
<br />
微软的这份论文提出了一种新方法：“合成数据”。<br />
<br />
他们使用大语言模型（LLM）来生成了很多不同语言的“模拟”文本数据，然后用这些数据来训练 AI 理解人类语言。这样做的好处是，他们不需要真实的数据就能训练出很好的文本嵌入模型，而且这个过程比传统方法更快、更高效。<br />
<br />
如何生成合成数据：<br />
<br />
1、使用大语言模型：首先，他们利用了大型语言模型，如GPT-4或类似的高级模型。这些模型已经通过大量的文本数据进行了预训练，因此具有强大的语言生成能力。<br />
<br />
2、任务定义和提示设计：研究团队定义了一系列文本嵌入任务，并为这些任务设计了特定的提示。这些提示被用来指导语言模型生成特定类型的文本。例如，他们可能会设计一个提示来生成关于某个特定主题的问答对，或者创建一个场景描述。<br />
<br />
3、生成合成数据：接下来，研究团队使用这些提示来引导语言模型生成数据。模型根据给定的任务提示产生文本，这些文本涵盖了各种主题和风格。生成的文本是合成的，但质量足以模拟真实世界的语言使用情况。<br />
<br />
4、多样性和覆盖率：为了确保生成的数据具有多样性并覆盖多种语言，研究团队可能会使用多种提示模板，并在多种语言中生成数据。这样可以确保模型不仅在资源丰富的语言（如英语）中表现良好，也能处理资源较少的语言。<br />
<br />
5、数据清洗和格式化：生成的数据经过筛选和优化，确保质量和多样性。生成的数据需要经过清洗和格式化，以确保它们符合训练需要。这可能包括去除重复内容、修正格式错误等。<br />
<br />
合成数据的优势：<br />
<br />
通过这种方法，微软的研究团队能够生成大量高质量的合成数据，用于训练和改进大型语言模型，从而提高文本嵌入的质量。这种方法的优势在于它不依赖于大量的标注真实数据，从而减少了数据收集和处理的工作量，同时还能提供丰富多样的训练材料。<br />
<br />
1、覆盖范围广：合成数据可以覆盖更广泛的场景和用例，包括那些在真实数据集中可能很少见或完全不存在的情况。这有助于模型学习更全面的语言模式和概念。这些数据覆盖了近100种语言的数十万个文本嵌入任务。这在传统数据收集方法中很难实现。<br />
<br />
2、减少偏见：由于不依赖现实世界的数据集，合成数据可以减少因数据收集过程中的偏见和局限性而引入的问题。真实数据集可能包含偏见或不平衡（例如，某些群体的代表性不足）。通过合成数据，可以有意识地减少这些偏见，创建更公平和平衡的数据集。<br />
<br />
3、灵活性和可扩展性：合成数据允许研究人员精确控制数据集的特性，如分布、复杂性和难度等，从而可以针对特定的研究或应用需求定制数据。因此生成合成数据的方法具有很高的灵活性，可以根据需要调整以生成各种类型的数据。<br />
<br />
4、成本效率：收集和标注大量高质量的真实数据非常昂贵且耗时。相比之下，生成合成数据的成本通常更低，且过程更快。<br />
<br />
5、快速迭代和改进：合成数据的生成过程可以根据模型性能的反馈快速调整，从而支持更快的迭代和改进。<br />
<br />
6、隐私和安全：使用合成数据可以避免处理敏感或个人数据，从而减少隐私和安全风险。<br />
<br />
实验结果表明：<br />
<br />
1、数据生成统计：研究团队成功生成了大约50万个示例，其中包含15万个独特的指令。这些数据涵盖了93种不同的语言，其中英语占主导地位。<br />
<br />
2、模型性能：在多种语言的MIRACL数据集上，使用合成数据训练的模型（E5mistral-7b）在nDCG@10和Recall@100两个指标上表现出色。这表明模型能够有效地检索相关文档，并且在多种语言上都有良好的表现。<br />
<br />
3、对比训练的影响：在包含对比预训练的设置下，模型在多个数据集上的表现有所提升。这说明对比预训练对于提高模型性能是有益的。<br />
<br />
4、多任务适应性：模型在多种任务类型上表现良好，包括文本检索、文本聚类、句子嵌入等，显示了其广泛的适用性。<br />
<br />
这些实验结果表明，使用合成数据训练的大型语言模型在多语言、多任务场景中都能取得优异的性能，证明了合成数据方法的有效性和实用性。<br />
<br />
论文：<a href="https://arxiv.org/abs/2401.00368">arxiv.org/abs/2401.00368</a><br />
PDF：<a href="https://arxiv.org/pdf/2401.00368.pdf">arxiv.org/pdf/2401.00368.pdf</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M2Q2ZpOWFrQUF5T2hmLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742427654006202439#m</id>
            <title>兄弟们，这个好！

Pile：一款开源的界面非常整洁美观的AI日记软件

可以帮助你撰写和保存日记条目，记录你的思考和经历，当备忘录也可以！

内置了OpenAI 的API功能，可以自己写提示词让AI帮你扩展你的想法和日记。

还可以使用AI来搜索日记内容或对整个日记提出问题。

开源、安全、隐私！

下载：https://udara.io/pile/
GitHub：https://github.com/UdaraJay/Pile
作者：@TGUPJ</title>
            <link>https://nitter.cz/xiaohuggg/status/1742427654006202439#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742427654006202439#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 06:08:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>兄弟们，这个好！<br />
<br />
Pile：一款开源的界面非常整洁美观的AI日记软件<br />
<br />
可以帮助你撰写和保存日记条目，记录你的思考和经历，当备忘录也可以！<br />
<br />
内置了OpenAI 的API功能，可以自己写提示词让AI帮你扩展你的想法和日记。<br />
<br />
还可以使用AI来搜索日记内容或对整个日记提出问题。<br />
<br />
开源、安全、隐私！<br />
<br />
下载：<a href="https://udara.io/pile/">udara.io/pile/</a><br />
GitHub：<a href="https://github.com/UdaraJay/Pile">github.com/UdaraJay/Pile</a><br />
作者：<a href="https://nitter.cz/TGUPJ" title="Udara">@TGUPJ</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0MTgyNDg3MjQwNjIyMDgvcHUvaW1nL0RYZXlfYXVmSzR1ejkyUnYuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742410353198416282#m</id>
            <title>VCoder：大语言模型的眼睛

VCoder的一个视觉编码器，能够帮助MLLM更好地理解和分析图像内容。提高模型在识别图像中的对象、理解图像场景方面的能力。

它可以帮助模型显示图片中不同物体的轮廓或深度图（显示物体距离相机的远近）。还能更准确的理解图片中的物体是什么，甚至能数出图片中有多少人。

它的功能包括：

1、增强视觉感知能力：VCoder通过提供额外的视觉编码器，帮助MLLM更好地理解和分析图像内容。

2、处理特殊类型的图像：VCoder能够处理分割图和深度图等特殊类型的图像。分割图可以帮助模型识别和理解图像中不同物体的边界和形状，而深度图则提供了物体距离相机远近的信息。

3、改善对象感知任务：VCoder通过提供额外的感知模态输入（如分割图或深度图）显著提高了MLLMs的对象感知能力。这包括更准确地识别和计数图像中的对象。

实验结果：

VCoder与开源的多模态LLMs（如MiniGPT-4、InstructBLIP、LLaVA-1.5和CogVLM）进行了比较，并在COST验证集上进行了测试。

VCoder在对象识别任务中表现最佳，特别是在对象计数和识别方面优于基线模型。

在处理复杂场景中的对象计数和识别任务时，VCoder展示了更高的准确性，尤其是在场景中有许多实体时。

对比GPT-4V：实验表明，GPT-4V在所有对象识别任务中的表现一致，但在与VCoder的比较中，GPT-4V在对象级感知方面落后于VCoder。

项目及演示：https://praeclarumjj3.github.io/vcoder/
论文：https://arxiv.org/abs/2312.14233
GitHub：https://github.com/SHI-Labs/VCoder
在线演示：https://huggingface.co/spaces/shi-labs/VCoder</title>
            <link>https://nitter.cz/xiaohuggg/status/1742410353198416282#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742410353198416282#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 04:59:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>VCoder：大语言模型的眼睛<br />
<br />
VCoder的一个视觉编码器，能够帮助MLLM更好地理解和分析图像内容。提高模型在识别图像中的对象、理解图像场景方面的能力。<br />
<br />
它可以帮助模型显示图片中不同物体的轮廓或深度图（显示物体距离相机的远近）。还能更准确的理解图片中的物体是什么，甚至能数出图片中有多少人。<br />
<br />
它的功能包括：<br />
<br />
1、增强视觉感知能力：VCoder通过提供额外的视觉编码器，帮助MLLM更好地理解和分析图像内容。<br />
<br />
2、处理特殊类型的图像：VCoder能够处理分割图和深度图等特殊类型的图像。分割图可以帮助模型识别和理解图像中不同物体的边界和形状，而深度图则提供了物体距离相机远近的信息。<br />
<br />
3、改善对象感知任务：VCoder通过提供额外的感知模态输入（如分割图或深度图）显著提高了MLLMs的对象感知能力。这包括更准确地识别和计数图像中的对象。<br />
<br />
实验结果：<br />
<br />
VCoder与开源的多模态LLMs（如MiniGPT-4、InstructBLIP、LLaVA-1.5和CogVLM）进行了比较，并在COST验证集上进行了测试。<br />
<br />
VCoder在对象识别任务中表现最佳，特别是在对象计数和识别方面优于基线模型。<br />
<br />
在处理复杂场景中的对象计数和识别任务时，VCoder展示了更高的准确性，尤其是在场景中有许多实体时。<br />
<br />
对比GPT-4V：实验表明，GPT-4V在所有对象识别任务中的表现一致，但在与VCoder的比较中，GPT-4V在对象级感知方面落后于VCoder。<br />
<br />
项目及演示：<a href="https://praeclarumjj3.github.io/vcoder/">praeclarumjj3.github.io/vcod…</a><br />
论文：<a href="https://arxiv.org/abs/2312.14233">arxiv.org/abs/2312.14233</a><br />
GitHub：<a href="https://github.com/SHI-Labs/VCoder">github.com/SHI-Labs/VCoder</a><br />
在线演示：<a href="https://huggingface.co/spaces/shi-labs/VCoder">huggingface.co/spaces/shi-la…</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDI0MDg5ODAyOTM2MTU2MTYvcHUvaW1nLzhvV1dZOGVVdWd3SVJRbnouanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/xiaohuggg/status/1742392686970331154#m</id>
            <title>R to @xiaohuggg: MUSIC ORIENTED DATASETS

 面向音乐的数据集：https://crypto-code.github.io/M2UGen-Demo/#datagen</title>
            <link>https://nitter.cz/xiaohuggg/status/1742392686970331154#m</link>
            <guid isPermaLink="false">https://nitter.cz/xiaohuggg/status/1742392686970331154#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jan 2024 03:49:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>MUSIC ORIENTED DATASETS<br />
<br />
 面向音乐的数据集：<a href="https://crypto-code.github.io/M2UGen-Demo/#datagen">crypto-code.github.io/M2UGen…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0M0NHhFTmJBQUFvcUdOLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>