<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/NWvmKhGUYAjYjdIpAedo</id>
            <title>Alluxio AI全新产品发布：无缝对接低成本对象存储AI训练解决方案</title>
            <link>https://www.infoq.cn/article/NWvmKhGUYAjYjdIpAedo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NWvmKhGUYAjYjdIpAedo</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Oct 2023 10:14:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Alluxio Enterprise AI, 数据平台公司, 企业数据基础设施, 人工智能, 机器学习
<br>
<br>
总结: Alluxio Enterprise AI是一家数据平台公司推出的新产品，旨在满足企业数据基础设施对于人工智能和机器学习的需求。该产品能够优化企业AI和分析基础设施的性能、数据可访问性、可扩展性和成本效益，助力下一代数据密集型应用的发展。 </div>
                        <hr>
                    
                    <p>（2023年10月19日，北京）Alluxio作为一家承载各类数据驱动型工作负载的数据平台公司，现推出全新的Alluxio Enterprise AI高性能数据平台, 旨在满足人工智能 (AI) 和机器学习 (ML) 负载对于企业数据基础设施不断增长的需求。 Alluxio Enterprise AI 平台可综合优化企业AI和分析基础设施的性能、数据可访问性、可扩展性和成本效益，助力生成式AI、计算机视觉、自然语言处理、大语言模型和高性能数据分析等下一代数据密集型应用的发展。</p><p>&nbsp;</p><p>为保持竞争力并在竞争中脱颖而出，各家企业都在全力推进数据和AI基础设施的现代化。在此过程中，企业家们也意识到传统的数据基础设施已经无法匹配下一代数据密集型AI负载的需求。在AI项目推进中经常遭遇的各类挑战，诸如性能低下、数据可访问性差、GPU 稀缺、数据工程复杂以及资源未充分利用等，都严重妨碍了企业获取数据价值。 <a href="https://emtemp.gcom.cloud/ngw/globalassets/en/publications/documents/2023-gartner-top-strategic-technology-trends-ebook.pdf">Gartner</a>"® 研究称，“可操作AI的价值在于能够在企业的各种环境下进行快速开发、部署、调整和维护。考虑到工程复杂性和更快的市场响应需求，开发较为灵活的AI工程数据流，构建能够在生产中进行自适应的AI模型均至关重要” ，“到 2026 年，采用AI工程来构建和管理自适应AI系统的企业，将在AI模型可操作性方面至少超越同行 25%。”</p><p>&nbsp;</p><p>Alluxio 创始人兼CEO李浩源表示：“Alluxio用最先进的大数据和Al平台为全球头部企业客户赋能，今天我们又向前迈出了一大步”， “Alluxio Enterprise AI 为客户提供高效的AI 解决方案，帮助企业加速 AI工作负载并最大限度地获取数据价值。未来的企业领导者将知道如何利用变革性AI来推进数据驱动，通过最新技术来构建和维护AI基础设施，实现超高性能、无缝访问和便捷管理。”</p><p>&nbsp;</p><p>此次新版发布后，Alluxio 即从一种产品扩展到两种产品组合——Alluxio Enterprise AI 和 Alluxio Enterprise Data，全面满足分析和AI的多样化需求。Alluxio Enterprise AI作为一款全新产品，建立在Alluxio企业版多年积累的分布式系统经验上，采用了针对AI/ML负载优化的新架构。 Alluxio Enterprise Data 是 Alluxio 企业版大数据方向的下一代版本（与Alluxio Enterprise AI平行），并将继续成为专注分析负载企业的理想选择。</p><p></p><h1>加速端到端机器学习工作流</h1><p></p><p>&nbsp;</p><p>Alluxio Enterprise AI 使得企业的AI基础设施能够在现有数据湖上实现高性能运行、无缝数据访问、可扩展且经济高效。它能帮助数据和AI领域的领导者和从业者实现AI项目的四个关键目标：1.高性能模型训练和部署，快速产生业务成效；2.跨区域和跨云负载可无缝访问数据；3.可无限扩展，已经互联网巨头内部严格测试；4. 无需使用昂贵的专用存储，在现有技术栈上即可部署，确保投资回报最大化。企业使用 Alluxio Enterprise AI后，预期训练速度可比使用提供商业服务的对象存储快达 20 倍，模型服务速度提升高达10 倍，GPU利用率达90%以上，AI 基础设施成本节约高达 90%。</p><p>&nbsp;</p><p>Alluxio Enterprise AI 拥有包含去中心化元数据的分布式系统架构，可消除访问海量小文件（常见于AI 负载）时的性能瓶颈。无论文件大小或数量如何，都能确保具备超越传统架构的无限扩展性。与传统分析不同，分布式缓存是根据 AI 负载 I/O 模式量身定制的。此外，还支持分析负载以及从数据摄取到 ETL（提取、转换、加载）、预处理、训练和服务的完整机器学习工作流 。</p><p>&nbsp;</p><p>Alluxio Enterprise AI 包含以下重要特性：</p><p>&nbsp;</p><p>性能出色的模型训练和模型服务——Alluxio Enterprise AI 显著提升企业在现有数据湖上的模型训练和服务性能。用于模型训练的强化API 集可实现优于商业化对象存储20 倍的性能。对于模型服务，Alluxio 提供超高并发性，在将离线训练集群中的模型用于在线推理时实现高达10 倍的速度提升。适合AI工作负载I/O模式的智能分布式缓存——Alluxio Enterprise AI的分布式缓存功能使得AI引擎能够通过高性能Alluxio缓存（而非缓慢的数据湖存储）来读写数据。 Alluxio的智能缓存策略专门针对AI引擎的I/O模式量身定制，包括大文件顺序访问、大文件随机访问和海量小文件访问。该优化帮助需要大量数据的GPU实现高吞吐和低延迟。训练集群持续从高性能分布式缓存中获取数据，可实现90%以上的GPU利用率。跨本地和云环境的AI 工作负载实现无缝数据访问 - Alluxio Enterprise AI 为企业提供了统一的管理界面，可以轻松管理跨不同基础设施环境的 AI 工作负载。该产品为机器学习工作流提供了真实的数据源，从根本上消除了大型企业数据湖孤岛的瓶颈。通过 Alluxio Enterprise AI 这一标准数据访问层，企业可以在不同业务部门和地理位置之间实现数据的无缝共享。经过大规模严格测试的全新分布式系统架构- Alluxio Enterprise AI 平台构建在创新的去中心化架构 DORA（去中心化对象存储库架构）之上。该架构为AI工作负载提供了无限扩展的基础，允许 AI 平台通过包括Amazon S3 在内的商业化对象存储处理多达1000 亿个对象。该新架构借助Alluxio在分布式系统方面的成熟专业知识，解决了系统可扩展性、元数据管理、高可用性和性能方面不断增长的挑战。</p><p>&nbsp;</p><p>&nbsp;Enterprise Strategy Group 分析师 Mike Leone 表示：“随着组织在整个业务范围内扩展AI的应用，优化下一代工作负载过程中的性能、成本和 GPU 利用率变得至关重要” ，“Alluxio 拥有极具优势的产品，能真正帮助数据和 AI 团队实现更高的性能、无缝的数据访问，以及模型训练和模型服务的便捷管理。”</p><p>&nbsp;</p><p>“我们与 Alluxio 合作密切，Allxuio平台对我们的数据基础设施至关重要，”Aunalytics 分析云工程总监 Rob Collins表示， “Aunalytics对于Alluxio新推出的针对企业AI的分布式系统十分期待，并看好新产品在AI 行业的巨大潜力。”</p><p>&nbsp;</p><p>“公司内部训练的大语言模型为我们的问答应用和推荐引擎提供支持，极大地增强了用户体验和参与度”，知乎数据平台团队软件工程师胡梦宇表示， “在我们的AI基础设施中，Alluxio 处于核心地位。在使用 Alluxio 作为数据访问层后，我们的模型训练性能提升了3 倍，部署性能提升了10 倍，GPU 利用率翻倍。Alluxio的Enterprise AI平台采用全新的DORA架构，能支持访问海量小文件，对此我们十分期待。在AI浪潮即将到来的时刻，Alluxio新产品让我们在支持AI应用方面更有信心。”</p><p></p><h1>在机器学习工作流中部署Alluxio</h1><p></p><p></p><p><a href="https://www.gartner.com/en/webinar/452057/1065295">Gartner</a>"&nbsp;研究显示，数据可访问性和数据量/复杂性是组织应用AI技术中遇到的三大难题之一。 Alluxio Enterprise AI可以添加到由AI计算引擎和数据湖存储组成的已有AI基础设施中。 Alluxio 位于计算和存储中间，可以在机器学习工作流中跨模型训练和模型服务工作，从而实现最大速度和最优成本。例如，将 PyTorch 作为训练和服务引擎， Amazon S3为现有数据湖：</p><p>&nbsp;</p><p>模型训练：当用户训练模型时，PyTorch数据加载器从虚拟本地路径/mnt/alluxio_fuse/training_datasets加载数据集。数据加载器不会直接从 S3 加载数据，而是从 Alluxio 缓存加载。在训练过程中，缓存的数据集将在多个epoch中使用，因此整个训练速度不再受制于访问S3而产生的瓶颈。也就是说，Alluxio通过缩短数据加载来加速训练，消除GPU空闲等待时间，提高GPU利用率。模型训练完成后，PyTorch通过Alluxio将模型文件写入S3。模型服务：最新训练的模型需要部署到推理集群。多个TorchServe实例同时从S3并发读取模型文件。Alluxio会缓存这些来自S3的最新模型文件，并以低延迟提供给推理集群。因此，最新模型一旦可用时，下游的AI应用即可将其用于推理。</p><p></p><h1>平台与现有系统集成</h1><p></p><p>&nbsp;</p><p>要将Alluxio与现有平台集成，用户可以在计算引擎和存储系统之间部署Alluxio集群。在计算引擎侧，Alluxio 可与 PyTorch、Apache Spark、TensorFlow 和 Ray 等流行的机器学习框架无缝集成。企业可以通过 REST API、POSIX API 或 S3 API 将 Alluxio 与这些计算框架集成。</p><p>&nbsp;</p><p>在存储侧，Alluxio 可连接位于任何位置（本地、云端或两者兼有）的各类文件系统或对象存储。支持的存储系统包括 OSS、COS、BOS、OBS、Amazon S3、Google GCS、Azure &nbsp;Blob Storage、MinIO、Ceph、HDFS等。</p><p>&nbsp;</p><p>Alluxio 可在本地和云端、物理机或容器化环境中运行。支持的云平台包括阿里云、腾讯云、百度云、华为云、AWS、GCP、Azure Cloud等。</p><p>&nbsp;</p><p>下载资源</p><p></p><p>Alluxio Enterprise AI 下载链接：<a href="https://www.alluxio.io/download/">https://www.alluxio.io/download/</a>"</p><p>&nbsp;</p><p>AI Infra Day</p><p></p><p>在美西时间10 月 25 日的AI Infra Day 上，Alluxio 将首次公开展示其最新发布的 Alluxio Enterprise AI平台。AI Infra Day是面向开发者的线上活动，主要探讨构建高性能、可扩展且经济高效的 AI 基础设施中的挑战及各种方案。特邀嘉宾包括Wanchao Liang（Meta ）、 Sally (Mihyoung) Lee（Uber） 和范斌（Alluxio）。活动现已开放报名：<a href="https://www.alluxio.io/ai-infra-day-2023/%E3%80%82">https://www.alluxio.io/ai-infra-day-2023/。</a>"</p><p>&nbsp;</p><p></p><blockquote>关于Alluxio&nbsp;Alluxio 是全球领先的针对分析和AI的高性能数据平台提供商，可加速企业AI产品价值变现，并最大化基础设施的投资回报率。Alluxio数据平台位于计算与存储系统之间，能够在数据工作流的各个阶段为数据平台上的工作负载提供统一视图。无论数据位于何处，该平台均可提供高性能的数据访问，简化数据工程，提高GPU利用率，并降低云计算和存储成本。企业无需使用专用存储，即可大幅加速模型训练和模型服务，并在现有数据湖上构建AI基础设施。Alluxio在头部投资者的支持下， 为全球科技、互联网、金融和电信企业提供服务，目前全球排名前 10 的互联网公司中有 9 家在使用Alluxio。了解更多信息，请访问 www.alluxio.com.cn。&nbsp;</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0AbAmTSduzAuVFFu29Nq</id>
            <title>一夜之间，有价无货！英伟达消费级 RTX 4090显卡遭全面下架，最高售价接近4万</title>
            <link>https://www.infoq.cn/article/0AbAmTSduzAuVFFu29Nq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0AbAmTSduzAuVFFu29Nq</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Oct 2023 06:08:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RTX4090, 下架, 新限令, AI芯片
<br>
<br>
总结: 北京时间10月18日下午，英伟达旗舰显卡RTX4090开始全面下架。这次下架是受到美国新限令的影响，限制向中国出售更先进的AI芯片。新规将在未来30天内生效，限制了英伟达等公司对华供应。这一限制对于GPU行业、服务器行业、算力行业以及AI行业从业者来说是一个重大影响。 </div>
                        <hr>
                    
                    <p>北京时间10月18日下午，英伟达顶级旗舰显卡 RTX4090 开始全面下架。</p><p>&nbsp;</p><p>目前，当前在京东搜索 “RTX 4090 显卡”只有少数第三方售卖，但需要预约等待到货。 同样，在淘宝搜索也是如此，标注价格基本2万起步，最高甚至接近4万元。而在二手平台咸鱼上，RTX4090售价基本1.2万起步。华硕、微星、影驰等英伟达合作商也同样纷纷下架该型号的非公显卡，官方旗舰店均已显示无货状态。</p><p><img src="https://static001.geekbang.org/infoq/4c/4c0dc2603d70908566d865fefe26c49e.jpeg" /></p><p></p><p></p><h2>“新限令”的结果</h2><p></p><p>&nbsp;</p><p>显然，这次消费级显卡 RTX4090 的下架是受当天美国“新限令”的影响。</p><p>&nbsp;</p><p>北京时间10 月 18 日，美国商务部宣布，计划限制向中国出售更先进的 AI 芯片。据悉，新的政策将限制 Nvidia A800 和 H800 芯片的出口，此外，新规将豁免笔记本电脑、智能手机和游戏设备中使用的大多数消费级芯片，但其中部分芯片仍须受到美国官员的批准和专项管控。相关规定将在未来 30 天内生效。</p><p>&nbsp;</p><p>10 月 16 日晚，美国商务部长 Gina Raimondo 表示，新措施弥补了去年 10 月所发布法规中的漏洞，未来可能“至少每年更新一次”。她解释称，此番措施的目标是限制中国获取“先进半导体，这些半导体能够推动 AI 技术发展以及对军事应用具有重大意义的复杂计算机突破”，并强调美国政府无意在经济上打压中方。</p><p>&nbsp;</p><p>有分析指出，去年10月美国实施原有的AI芯片管制规定后，英伟达推出H100和A100阉割版，分别为H800和A800，处理速度约为对应芯片的70%，它们仍可用于人工智能应用上。而本次的新限制则以“性能密度”（以每平方毫米的浮点运算次数来衡量）取代芯片间通信速度，旨在阻止公司寻找“绕过”方法。这意味着不论英伟达还是英特尔、AMD，按照算力性能密度的要求，新产品可能基本没有办法对华供应。</p><p>&nbsp;</p><p>另外，新规还扩大了半导体制造设备的出口管控，包括强化对美国人才的限制，还对中国以外的 21 个国家提出了芯片制造工具出口管控要求，原因是担心这些设备可能被转移给中国或其他国家，进而引发安全问题。</p><p>&nbsp;</p><p>更多详情可查看：</p><p><a href="https://mp.weixin.qq.com/s/IFazU7qhHkkmNWKrwFqDhw">突发！美国限制向中国出口 Nvidia H800 等先进 AI 芯片，壁仞科技、摩尔线程等中国 GPU 芯片企业被列入实体名</a>"</p><p>&nbsp;</p><p>“今夜对于无数GPU行业、服务器行业、算力行业以及AI行业从业者来说都是不眠之夜，就连消费级的4090显卡都从每张1.5万跳涨到2.5万。在高度全球化的今天，一纸大洋彼岸的禁令就这样荒谬且真实地影响了国产大模型和人工智能发展的进程。”有业内资深人士称。</p><p>&nbsp;</p><p>附：美国对华半导体制裁记录</p><p>&nbsp;</p><p>2018年10月，美国商务部发布公告，将福建晋华集成电路有限公司列入商务部实体名单，禁止美国企业向福建晋华出售技术和产品；2019年5月，美国商务部正式将华为列入“实体清单”，禁止美企向华为出售相关技术和产品2020年5月，美商务部公告将延长华为的供货临时许可证90天至8月14日，但同时升级了对华为的芯片管制，以限制华为使用美国技术软件在国外设计和制造半导体的能力；2020年12月，美国商务部以“违反美国国家安全或外交政策利益”为由，宣布将中芯国际列入“实体清单，这就意味着中芯国际生产10nm以下芯片所需要的原料和设备无法获得美国批准出口；2022年10月，美国BIS公布对中国出口管制新规，主要针对先进芯片和芯片制造设备领域；2022年11月，美国向日本和荷兰施压，要求两国的芯片制造领域相关企业立即禁止向中国出售产品，阻止先进芯片技术流入中国；2022年12月，美国商务部决定将包括长江存储、寒武纪、上海集成电路研发中心、上海微电子、深圳鹏芯微等在内的36家中国实体 (包括一家长江存储日本子公司) 加入实体清单。</p><p></p><h2>英伟达的应对策略？</h2><p></p><p>&nbsp;</p><p>对于“新限令”，英伟达方面回应称：“我们遵守所有适用的法规，同时努力提供支持不同行业的数千种应用产品。鉴于全球对我们产品的需求，我们预计（新规）短期内不会对我们的财务业绩产生实质性的影响。”</p><p>&nbsp;</p><p>不过，英伟达的市场表现并没有英伟达官方说的那么乐观。</p><p>&nbsp;</p><p>美东时间10月17日周二，美股盘中，英伟达（NVDA）一度重挫7.8%，创2022年12月以来最大盘中跌幅。截至收盘，英伟达跌4.68%，报收439.38美元，市值一夜蒸发超535亿美元（≈4000亿元人民币），最新市值1.09万亿美元。英特尔、AMD也分别收跌1.4%、1.2%，美股芯片股合计蒸发730亿美元（约合5343亿元）市值。</p><p>&nbsp;</p><p>在此背景下，人们更加确信之前爆出英伟达将推出RTX 4080 Super&nbsp;的消息。根据 @hongxing2020 爆料消息，英伟达将带来三款 RTX 40 系 SUPER 显卡，分别为 RTX 4080 SUPER、RTX 4070 Ti SUPER、RTX 4070 SUPER。有媒体求证得知，目前 3 款 SKU 基本上已经确认，但并未拿到具体信息，只知道新版 RTX 4080 将会采用 20GB GDDR6X 显存。</p><p></p><h2>大企业“备货充足”</h2><p></p><p>&nbsp;</p><p>与美股芯片股反应相反，10月18日，A股算力芯片概念股普遍上涨，好利科技一字涨停，寒武纪、弘信电子盘中大涨超10%，景嘉微、海光信息等纷纷收涨。</p><p>&nbsp;</p><p>在芯片管制措施升级消息曝出后不久，部分公司对外透露称“影响不大”、“备货充足”等。10月17日晚，恒润股份公告显示，其控股子公司上海润六尺向供应商A采购75台H800及22台A800现货，合计合同金额约2亿元。腾讯、百度等大厂也表示，“囤货充足”。但中小型AI公司的日子可能不太好过。</p><p>&nbsp;</p><p>当前，国内以大模型为代表的AI领域正在迅速发展。根据TortoiseIntelligence发布的AI指数，对世界各国人工智能进行排名，综合来看，我国仅次于美国排名第二，单项指标中，发展指标和政府策略指标更是位居首位。但新规的发布就是国内AI发展的“绊脚石”。</p><p>&nbsp;</p><p>在美对华持续制裁背景下，算力自主可控需求日益增长。IDC最新数据指出，中国本土云端AI加速芯片制造上正在快速增长，2023年上半年，中国AI服务器使用了50万块本地采购/开发的AI加速芯片。其中，华为、寒武纪、海光等国产算力被寄予厚望。</p><p>&nbsp;</p><p>浙商证券指出，国内算力芯片的发展速度取决于上游供应及下游的迭代速度，因而供应及生态体系较为完善的华为鲲鹏升腾芯片有望最先获益，具备较强技术积累和生态兼容性的海光也有望迎来更大的市场空间。</p><p>&nbsp;</p><p>科大讯飞创史人刘庆峰就曾表示，华为的GPU能力现在已经跟英伟达 A100 一样，现在已经做到对标英伟达的 A100。而前不久，华为Mate 60系列引发抢购热潮，主要因为Mate 60系列顺利上市象征着华为突破美国封锁制裁，取得阶段性胜利。全球著名半导体行业观察机构TechInsights 公开发布了对Mate60 Pro 的拆解报告：Mate60 Pro搭载了新型麒麟9000s芯片，并采用了先进的7纳米。</p><p>&nbsp;</p><p>“接下来，我们每个从业者的选择在共同定义未来，囤货赚快钱或者埋头苦干寻找替代。明天太阳照常升起，卡贩子猫博士也会继续战斗下去。”上述提到的资深人士表示。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xhlku65TOzhUtKR2yaSi</id>
            <title>最新预测：2026年逾80%企业将采用生成式AI，相比当下增长16倍</title>
            <link>https://www.infoq.cn/article/Xhlku65TOzhUtKR2yaSi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xhlku65TOzhUtKR2yaSi</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 10:31:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Gartner, 生成式 AI, 应用程序编程接口, 模型
<br>
<br>
总结: 根据Gartner的报告预测，到2026年，超过80%的企业将使用生成式AI应用程序编程接口（API）或模型，或者在相关生产环境中部署支持生成式AI的应用程序。这意味着在短短三年内，采用或创建生成式AI模型的企业数量预计将会增长16倍。 </div>
                        <hr>
                    
                    <p>日前，全球咨询公司 Gartner 发布报告称，预计在 2026 年，超过 80% 的企业将使用生成式 AI （GenAI）应用程序编程接口（API）或模型，或者在相关生产环境中部署支持生成式 AI 的应用程序。</p><p>据统计，这一比例在 2023 年还不到 5%，这意味着在短短三年内，采用或创建生成式 AI 模型的企业数量预计将会增长 16 倍。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f8/4a/f820875904ce853a26a56fb7a380f64a.jpg" /></p><p>Gartner生成式 AI 技术成熟度曲线（2023）</p><p></p><p>Gartner 发布了 2023 生成式 AI 技术成熟度曲线，并预计将在未来十年对组织产生巨大影响的一些创新包括：支持生成式 AI 的应用程序、基础模型以及 AI 信任、风险和安全管理（AI TRiSM）。</p><p></p><p>支持生成 AI 的应用程序是指利用生成 AI 来完成特定任务的应用程序。ChatGPT 就是生成人工智能应用程序的一个例子，因为它使用人工智能来合成你的文本提示并输出响应。</p><p></p><p>基础模型是指生成式 AI 应用程序的机器学习模型，例如 GPT 与 ChatGPT 的关系。这些基础模型经过大量数据的训练，用于支持可以完成各种任务的不同应用程序。</p><p></p><p>Gartner 将基础模型置于技术成熟度曲线上预期过高的峰值，预测到 2027 年，它们将支撑 60% 的自然语言处理 (NLP) 用例。</p><p></p><p>最后，AI TRiSM 是指能够解决生成式 AI 模型相关问题并确保其成功部署的一组解决方案。困扰生成式 AI 模型的一些风险包括可靠性、错误信息、偏见、隐私和公平性。</p><p></p><p>Gartner 杰出副总裁分析师 Arun Chandrasekaran 表示；“生成式 AI 已成为最高管理层的首要任务，并引发了基础模型之外的新工具的创新。”“医疗保健、生命科学、法律、金融服务和公共部门等许多行业，对生成式 AI 的需求将不断增加。”</p><p></p><p>参考链接  ：</p><p>https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026</p><p>https://www.zdnet.com/article/80-of-enterprises-will-have-incorporated-ai-by-2026-according-to-a-gartner-report/</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bf/a9/bf1622c0c2be73e4d19e8643444e2fa9.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WpVGdht7FQxo4q95TcYY</id>
            <title>小米无线充电车专利公布；禾赛获哪吒汽车新车定点合作；阿维塔的无图智能驾驶技术即将面世｜汽车科技资讯</title>
            <link>https://www.infoq.cn/article/WpVGdht7FQxo4q95TcYY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WpVGdht7FQxo4q95TcYY</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 10:27:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 极氪 009 OS 4.1, 360° 全景影像, 智能浮窗, 魔视智能, 北汽极狐量产定点, 小米无线充电车专利, 长安汽车, 无图智能驾驶技术, 禾赛, 哪吒汽车新车定点合作
<br>
<br>
总结: 极氪 009 OS 4.1 更新推送，新增360°全景影像的智能浮窗功能。魔视智能再次获得北汽极狐量产定点，为两款车型提供智能泊车系统。小米汽车科技公布了充电车及充电方法的专利，实现了无线充电。长安汽车的阿维塔无图智驾技术即将面世，将在多个城市进行测试和用户体验。禾赛科技获得哪吒汽车新车定点合作，将提供超高清远距激光雷达等智能驾驶技术。 </div>
                        <hr>
                    
                    <p></p><h2>极氪 009 OS 4.1 推送发布，360° 全景影像新增“智能浮窗”功能</h2><p></p><p></p><p>10 月 15 日，极氪 009 纯电 MPV 迎来 OS 4.1 更新推送，360 度全景影像优化升级，新增全屏 / 浮窗随需切换功能。此次更新后，360° 全景影像设置中新增“智能浮窗”功能，开启智能浮窗功能后，转向联动和窄道辅助激活时，中央显示屏自动弹起智能浮窗，全屏和浮窗设有切换设置，用户可自由选择。当低速转向和驶入狭窄路段时，全景影像可以浮窗的形式显示，避免全屏模式对地图画面信息的遮挡，并可在全屏 / 浮窗两种模式间一键切换，便捷随需。</p><p></p><p></p><h2>魔视智能宣布再获北汽极狐量产定点</h2><p></p><p></p><p>10 月 13 日，继去年 12 月宣布获得北汽极狐两款车型量产定点，魔视智能宣布再获北汽极狐量产定点，为极狐阿尔法 S、阿尔法 T 两款车型提供软硬一体的 L2+ 智能泊车系统，相关车型将于 2024 年上市。据悉，该智能泊车系统将以极具性价比的成本覆盖自动泊车（APA）、融合泊车（RPA）、全景影像系统（AVM）等泊车域主流功能。魔视智能针对 L1-L4 级低速泊车、高速行车等乘用车场景，均开展了相关的产品布局，未来，双方将携手推动高智能化、高安全性的自动驾驶产品的量产落地，打造面向下一代的智能泊车产品，引领智能泊车新时代。</p><p></p><p></p><h2>小米无线充电车专利公布</h2><p></p><p></p><p>10 月 11 日，小米汽车科技有限公司申请的“充电车及充电方法”专利公布。充电车包括电池仓、无线充电装置、自动驾驶系统，电池仓用于装载电池，无线充电装置用于将电池的电能无线传输给电动车，自动驾驶系统用于控制充电车行驶到与电动车处于预设的相对位置，在相对位置下，无线充电装置能够将电池的电能无线传输给电动车。</p><p></p><p></p><h2>长安汽车：阿维塔的无图智能驾驶技术即将面世</h2><p></p><p></p><p>由长安汽车所控股的阿维塔科技于 2023 年 10 月 9 日宣布逐步开启无图智驾 NCA，并将于近期密集开展大规模实际道路测试和用户体验。 长安汽车最新消息显示：作为华为高阶智能驾驶系统 ADS2.0 的核心能力，阿维塔的无图智驾 NCA 功能将率先覆盖北京、上海、广州、重庆、深圳、杭州六城，第二批功能交付将覆盖另外 16 座核心城市，并于年内实现国内全覆盖。随着无图智驾 NCA 的成功落地，阿维塔 11 的智驾系统率先摆脱高精地图限制，实现高速 - 城区 - 泊车三大核心场景的全面覆盖，解锁更多高频智驾场景，让智驾体验做到“越开路越熟”的同时，达成业内无图智驾最快交付速度，为用户带来“越开路越广”的智驾体验。</p><p></p><p></p><h2>禾赛获哪吒汽车新车定点合作</h2><p></p><p></p><p>2023 年 10 月 10 日，禾赛科技宣布获得哪吒汽车旗下新车前装量产项目定点，哪吒汽车新车型将搭载禾赛超高清远距激光雷达 AT128，将集中展现哪吒汽车在电动汽车智能驾驶技术方面的最新研发成果，比如目前正在研发中的轻地图、无图版的城市领航辅助驾驶系统。其中，禾赛 AT128 将会在哪吒汽车 NETA PILOT 高阶智驾系统的感知模块中起到关键作用，提高哪吒汽车新车型的感知力，保障用户出行的舒适度和安全性。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bf/a9/bf1622c0c2be73e4d19e8643444e2fa9.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43</id>
            <title>抖音同款、2023 必看：火山引擎团队整理的“易复用”的音视频处理经验都在这了</title>
            <link>https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, 视频化时代, 营销, 商品, 知识, 空间的体验, 视频处理, 商业机会转化
<br>
<br>
总结: 从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，视频处理在各种场景中发挥着越来越重要的作用。同时，随着技术的快速发展和数据量的不断增长，“加快视频化进程、商业机会转化”成为了企业音视频业务在未来主要探索方向。火山引擎视频云与NVIDIA深度合作联合打造视频转码芯片和视频处理架构开源平台，致力于帮助企业完成技术迭代、实现业务增长。他们推出了《云上新视界》线上课程，以音视频创新场景与最佳实践为核心内容，为行业注入新鲜力量和创新源泉。该课程内容涵盖音视频处理多技术维度，由资深工程师专家亲自授课，分享实战经验，帮助视频处理开发者更好地完成经验复用。 </div>
                        <hr>
                    
                    <p>从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，从娱乐、教育、工业到商业应用，视频处理在各种场景中发挥着越来越重要的作用。同时，随着技术的快速发展和数据量的不断增长，“加快视频化进程、商业机会转化”成为了企业音视频业务在未来主要探索方向。</p><p></p><p>为了帮助大家更好地完成技术迭代、实现业务增长，<a href="https://www.infoq.cn/article/qC55OH6f6852hFjlZ3o8">火山引擎视频云</a>"与 NVIDIA 深度合作联合打造视频转码芯片和视频处理架构开源平台，自 2022 年 2 月至今，已经经过了 1000+ 企业业务场景的打磨，成功完成了多个创新场景实践。</p><p></p><p>在“方便大家将已经跑通的视频处理业务场景实践经验复用”的初心下，<a href="https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO">火山引擎视频云</a>"以“面向体验，驱动创新”为核心，特别与 NVIDIA 团队合作推出《云上新视界》线上课程，致力于打造一档以音视频创新场景与最佳实践为核心内容的系列视频栏目，内容覆盖赛事直播、6DoF 互动体验、3D 人体重建、弹幕游戏等火爆热门场景，为行业注入新鲜力量和创新源泉。</p><p></p><p>该系列视频栏目是全新的<a href="https://www.infoq.cn/article/Rx45QcxHI4zZCfMR5r8J">长线课程</a>"，目前课程制作团队已完成前 6 期视频内容的策划，自 2023 年 10 月 19 日起，将在火山引擎开发者社区、字节跳动技术团队、字节跳动视频云技术团队、InfoQ 等内容平台中以“2 周 / 期”的频率进行上线更新。</p><p></p><p>《云上新视界》系列课程极具特色，相较于当前网上已有的课程更适合音视频及其相关技术从业者收看：</p><p>技术解读够详细：课程内容涵盖音视频处理多技术维度，包括但不限于视频转码、视频增强、视频分析、视频插帧、VR 等。火山引擎视频云将与 NVIDIA 共同为开发者们提供更全面、更前沿的视频处理技术指导，帮助大家全面了解多媒体处理框架（Babit Multimedia Framework，BMF） 在不同场景中的应用。实战经验够全面：课程将由来自火山引擎视频云、NVIDIA 及多位火山引擎客户伙伴的资深工程师专家亲自授课，分享他们在多个创新场景中的实践经验，如亚运会（赛事直播）、弹幕游戏、虚拟直播间、VR 空间互动、3D 人体重建、远程车控等。课程主讲人将从某行业场景痛点入手解读场景方案架构、方案优势、应用场景和最佳实践，切实帮助各行各业的视频处理开发者更好地完成经验复用。</p><p></p><p>目前该系列公开课的第一期课程《抖音大型直播画质优化实践》预计于 10 月 19 日正式上线。在该视频中，火山引擎多媒体实验室技术专家王庆将为大家解析“抖音大型赛事直播全链路画质”面临的挑战，并揭秘“抖音亚运会直播服务端与客户端画质优化”方法论与实践收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/16eaec07efaf9fbec7e0008249f12732.jpeg" /></p><p></p><p>10 月 19 日 19:30，课程将在 InfoQ 视频号、InfoQ 官网、极客时间 APP 进行上线直播，感兴趣的同学们赶紧点击“阅读原文进行报名吧！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7kwLXVzqKJHfLq4GcPtL</id>
            <title>国内首个“AI原生应用商店”上线！百度智能云：让首批敢于吃螃蟹者获益</title>
            <link>https://www.infoq.cn/article/7kwLXVzqKJHfLq4GcPtL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7kwLXVzqKJHfLq4GcPtL</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 05:44:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 大模型服务平台, 生态建设, 千帆社区
<br>
<br>
总结: 百度智能云通过建立大模型服务平台和千帆社区，实现了生态建设，为开发者和创新企业提供了支持和培训，推动了AI原生应用的创新和商业化。 </div>
                        <hr>
                    
                    <p>百度智能云已建立起国内最繁荣的AI原生产业生态。在10月17日举行的百度世界2023上，百度智能云宣布，百度智能云千帆大模型服务平台已服务17000多家客户，覆盖近500个场景。同时，新的企业和开发者还正在不断地涌入千帆，大模型调用量高速攀升。平台上既有年龄仅14岁的小开发者，也有刚成立不久的初创企业，还有已深耕行业十几年的互联网老兵，开发出了智能创作、问诊咨询、电商、短视频、游戏、情感陪伴等多样化应用。</p><p>&nbsp;</p><p>生态建设也成为百度智能云在大模型时代最重要的一环。当日，百度集团执行副总裁、百度智能云事业群总裁沈抖宣布，“云智一体”战略内涵升级为“云智一体，深入产业，生态繁荣，AI普惠”。相比过去，“生态繁荣”是百度智能云战略中新增内容。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b0/b019bd6111ad448ba379bbac0e3d9f93.png" /></p><p>百度集团副总裁袁佛玉</p><p>&nbsp;</p><p>在当日下午举行的“大模型驱动产业发展论坛”上，百度集团副总裁袁佛玉宣布，百度智能云已打造国内第一个大模型全链路生态支持体系，贴身围绕生态伙伴、创始企业，用上大模型、用好大模型的需求，为处于不同成长阶段的创新企业和开发者提供定向专属支持。这意味着，百度智能云不仅要帮助创业者、开发者实现从0到1的突破创新，还要帮助实现商业价值和社会价值。</p><p>&nbsp;</p><p>袁佛玉表示，百度智能云的大模型生态支持体系分为三部分。第一部分是从多个方面支持开发者和创新企业。百度智能云搭建业界首个大模型实训营，为新入行的开发者和亟需更新知识的企业提供赋能培训支持。AI加速器则为创新企业提供创新应用孵化支持，培养出潜力明星企业。其他方式还有销售商机支持和市场营销支持。第二部分，百度智能云打造了国内首家面向企业客户进行一站式交易的AI原生应用商店——千帆AI原生应用商店，加速AI原生应用的商业化落地。第三，百度智能云建设千帆社区，为广大的千帆开发者提供交流，分享AI原生开发案例和实践经验的平台。&nbsp;</p><p></p><h3>从0到1，业界首个大模型实训营激活AI原生应用创新</h3><p></p><p>&nbsp;</p><p>对于创业者和企业来说，谁抢先开发出爆款的AI原生应用，谁就有可能成为大模型时代的新巨头。但创新者也面临不少烦恼。比如大模型技术门槛高，技术迭代速度快，新入行的开发者不是特别了解怎么基于大模型来做开发，深耕行业多年的“老兵”也需要更新自己的知识。</p><p>&nbsp;</p><p>百度智能云推出业界首个大模型实训营——千帆AGI House，基于伙伴落地大模型不同阶段所需要的技术支持，以实践、实操为导向，支持伙伴搞清楚技术发展方向、少走弯路，用好千帆大模型平台等，致力于真正把技术前沿落地到现实生产场景中。据介绍，千帆AGI House得到了市场非常积极的回应。每场的报名人数都超过了可容纳人数的好几倍。</p><p>&nbsp;</p><p>百度智能云生态合作伙伴庖丁科技副总裁关晨光表示，庖丁科技能够帮助文心大模型更懂行业，也能让行业更容易用上文心大模型。庖丁科技支持各类集成方式，赋能各类大中小企业的需求，过去，金融从业者如果想从监管公开的信息中寻找一些长尾信息，比如哪些上市公司过去1年更换了会计师事务所，人工方式统计至少需要一个工作日的时间。而通过庖丁和百度智能云千帆合作的企业级知识问答AI——ChatDOC，可以用对话的形式跟知识库聊天，快速得到信息，效率至少提升数十倍。</p><p>&nbsp;</p><p>百度智能云还打造了“千帆社区”， 提供最务实的产品攻略，汇聚一线实践经验、前沿观点以及丰富的产品工具。用户包括有刚刚踏入AI大门、期望寻求手把手传授指导的初学者，拥有丰富的创业故事和经验、并愿意与他人分享的企业领袖，对技术追求深入至微、希望与同行切磋的技术精英。百度大模型技术和产品专家也会入驻千帆社区进行分享、交流。&nbsp;</p><p></p><h3>AI加速器+首家AI原生应用商店，让首批敢于吃螃蟹者收获成果</h3><p></p><p>&nbsp;</p><p>大模型浪潮下，第一批勇于吃螃蟹的创新者已经打造出了丰富的AI原生应用。不过，开发只是第一步，真正的应用繁荣必然包含商业化的成功。但创新者普遍缺少技术培训赋能、资本及产业落地支持，辛苦打造的 AI原生应用需要商业变现的机会和渠道。</p><p>&nbsp;</p><p>百度智能云推出的AI加速器，为创新者做好技术赋能、技术资源支持、牵引投资和营销支持。比如在技术赋能方面，百度智能云新推出的AI原生应用开发工作台，主要由应用组件、应用框架两层服务构成，将各种应用的常见模式、工具、流程，沉淀在工作台上，让开发者不用再为研发过程发愁。百度自研的AI原生应用——Comate智能编程助手，即将在10月24日全面开放，开发者和企业可以通过百度智能云官网或者直接百度搜索Baidu Comate申请使用。目前，Comate智能编程助手已在百度内部大规模使用，覆盖80%以上的工程师，平均采纳率超过40%。</p><p>&nbsp;</p><p>百度智能云还拉动很多产业机构，共同为加速器成员企业提供从产品、技术培训到资本赋能、市场推广等一站式全方位支持，包括有北大智能所、清华互联网产业研究院、赛迪等顶尖的科研院所，创业黑马、爱分析等为创业者提供全方位服务的企服平台，还有产业园区和众多知名投资机构。行行AI董事长李明顺表示，无论国内外，强应用派都是这次大模型革命的浪尖弄潮儿，中国的强应用关键在产业。行行AI正在全国构建强应用区域性赋能中心网络，愿与百度智能云AI加速器共促大模型应用生态繁荣。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/83/83c9db09687951dd0f0f6d839b8f590d.png" /></p><p>&nbsp;千帆AI加速器成员亮相</p><p>&nbsp;</p><p>目前，百度智能云AI加速器已经开营两期，共有57家企业参加训练，已开发超过22个商业化应用。学员中70%以上为企业的创始人或CXO，部分企业董事长亲自下场，全天候参加课程培训。</p><p>&nbsp;</p><p>百度智能云还推出国内首家面向企业客户进行一站式交易的AI原生应用商店——千帆AI原生应用商店，为商家提供品牌曝光和销售通路支持，进一步加速AI原生应用的商业化落地。让第一批最有勇气冲在前面的AI原生应用开发者能够“活”下来，快速成长起来，实现商业价值。</p><p>&nbsp;</p><p>千帆AI原生应用商店已在10月16日正式上线，金蝶等合作伙伴打造的首批精选应用已经入驻商店。用户可以通过“百度智能云官网”进入商店，看到新品推荐榜、热门应用榜、行业推荐榜等各类榜单，快速找到最新、最热门的AI原生应用。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9e/9e2f6b6bd54537686f7394f89114bcdb.png" /></p><p>千帆AI原生应用商店</p><p>&nbsp;</p><p>袁佛玉表示，我们希望这个应用商店不只是应用展示，还是一个便捷高效的应用交易平台。我们希望它可以连接AI原生应用供应商和需求方，不仅提升企业客户在应用选型和采购方面的效率，更可以帮助商家更快速地把应用推向市场，成为一个大模型商业机会的汇集地。</p><p>&nbsp;</p><p>百度智能云的多个合作伙伴参与了“大模型驱动产业发展论坛”，分享了自己的行业洞察和AI原生应用。英特尔资深AI架构师任而今在会上介绍了英特尔软硬件如何全面赋能生成式人工智能，实现人工智能普惠化。在现场展区，英特尔也展示了从数据中心、边缘到端设备的人工智能产品和方案，包括Gaudi2人工智能加速卡等。随着 AI 技术的不断演进，LLM、生成式 AI 和DLRM深度学习推荐已逐渐成为现代经济发展的数字引擎，而大模型也带动着未来 AI 的发展趋势，新业务场景不断涌现，海量多类型数据集的积累，对计算能力和架构的要求也越来越高。NVIDIA 资深解决方案架构师龚孝波在百度世界大会发表了主题为“适应大模型发展的 NVIDIA 技术架构演变”的演讲，为开发者们阐述了 NVIDIA 从 GPU 架构、软件、硬件等方面，如何适应大模型对计算需求与网络通信的快速增长。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QJ73po4wuwTvLKcpK1Fw</id>
            <title>突发！美国限制向中国出口Nvidia H800等先进AI芯片，壁仞科技、摩尔线程等中国GPU芯片企业被列入实体名单</title>
            <link>https://www.infoq.cn/article/QJ73po4wuwTvLKcpK1Fw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QJ73po4wuwTvLKcpK1Fw</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 04:04:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, 芯片出口限制, AI技术, 中美贸易战
<br>
<br>
总结: 美国商务部计划限制向中国出售更先进的AI芯片，以限制中国获取先进半导体，推动AI技术发展和军事应用。新规将在未来30天内生效，此举是中美贸易战的一部分。 </div>
                        <hr>
                    
                    <p></p><blockquote>据悉，美国这一新规将在向公众征求 30 天意见后生效。</blockquote><p></p><p></p><h2>美国升级对华芯片出口限制</h2><p></p><p></p><p>据路透社报道，美国商务部 10 月 17 日宣布，计划限制向中国出售更先进的 AI 芯片。据悉，新的政策将限制 Nvidia A800 和 H800 芯片的出口，此外，新规将豁免笔记本电脑、智能手机和游戏设备中使用的大多数消费级芯片，但其中部分芯片仍须受到美国官员的批准和专项管控。相关规定将在未来 30 天内生效。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d7/d7882b8c99a56d822db36ce76f2b1052.png" /></p><p></p><p>10 月 16 日晚，美国商务部长 Gina Raimondo 曾对记者表示，新措施弥补了去年 10 月所发布法规中的漏洞，未来可能“至少每年更新一次”。她解释称，此番措施的目标是限制中国获取“先进半导体，这些半导体能够推动 AI 技术发展以及对军事应用具有重大意义的复杂计算机突破”，并强调美国政府无意在经济上打压中方。她表示，中国仍可进口价值数千亿美元的美国半导体。</p><p>&nbsp;</p><p>2022 年 10 月 7 日，美国政府以出台“临时规则”形式更新《出口管理条例》，将 31 家中国实体列入“未经核实清单”，并升级对华半导体出口管制，以&nbsp;Nvidia&nbsp;A100 芯片的性能指标作为限制标准，限制对华出口高性能计算芯片。具体来说，同时满足以下两个条件的即为受管制的高性能计算芯片：</p><p>&nbsp;</p><p>芯片的 I/O 带宽传输速率大于或等于 600 Gbyte/s；“数字处理单元 原始计算单元”每次操作的比特长度乘以 TOPS 计算出的算力之和大于或等于 4800TOPS。</p><p>&nbsp;</p><p>彼时受该政策影响，Nvidia&nbsp;A100 及 H100 GPU 加速芯片都无法继续对华出口。随后，Nvidia 针对中国大陆市场推出特供版的 A800 及 H800 芯片——仅保留了强大的计算能力，但对通信速度做出了限制以保证符合此前规定要求。自去年的规定实施以来，Nvidia&nbsp;面向中国的专供芯片需求大增、带动业务一路高歌猛进。受全球供应不足影响，获准销售的几乎一切&nbsp;Nvidia 芯片都在中国大受欢迎。</p><p>&nbsp;</p><p>如今美国加码芯片出口限制，新规则对特定尺寸芯片的计算能力也做出了限制，旨在防止使用新的“Chiplet”技术方法绕开限制。因此，Nvidia A800 和 H800 芯片对华出口也将受到影响。</p><p>&nbsp;</p><p>对此，Nvidia 表示，该公司将遵守规定、且预计近期业绩不会因此受到重大影响。但截至 10 月 17 日美股收盘，Nvidia 股价下跌 4.68%，同样受到新规则影响的 AMD、Intel 等相关企业股价也有相应下滑。</p><p>&nbsp;</p><p>除了升级对华芯片出口限制，此次美国新规还扩大了面向另外 40 多个国家出口先进芯片的管控要求。据路透社报道，这项措施似乎基于英伟达在今年 8 月收到的一封信函。信中称本应受到供应限制的 A100 和 H100 芯片从中国流出至包括中东在内的其他国家。此前路透社报道的猜测也在新规则中得到证实，即对于母公司总部位于中国、澳门及其他部分国家的企业，不得将芯片移交给位于世界任何地方的下辖子公司。</p><p>&nbsp;</p><p>美国还对中国以外的 21 个国家提出了芯片制造工具出口管控要求，原因是担心这些设备可能被转移给中国或其他国家，进而引发安全问题。</p><p>&nbsp;</p><p>此外，新规还在对中出口限制清单中添加了 DUV 光刻系统，相当于对政策做出进一步收紧。此前，美国曾禁止荷兰 ASML 向部分中国先进芯片工厂提供较旧型号的深紫外光刻设备及备件。DUV 设备属于尖端 EUV 设备（目前对中国全面禁售）的前代设备，但同样属于先进的芯片制造工具，能够以更高的成本制造出几乎同等制程的芯片。</p><p>&nbsp;</p><p>ASML 在一份声明中表示，新规可能在中长期内对“我方系统在特定区域内的销售”产生影响，但该公司预计 2023 年内的财务前景应该不会受到“实质性冲击”。</p><p></p><h2>多家中国GPU芯片企业被列入实体名单</h2><p></p><p>&nbsp;</p><p>此外，美国商务部在 10 月 17 日还将壁仞科技、摩尔线程等多家中国 GPU 芯片企业列入实体名单。</p><p>&nbsp;</p><p>具体包括：北京壁仞科技开发有限公司、广州壁仞集成电路有限公司、杭州壁仞科技开发有限公司、光线云（杭州）科技有限公司、摩尔线程智能科技（北京）有限责任公司、摩尔线程智能科技（成都）有限责任公司、摩尔线程智能科技（上海）有限责任公司、上海壁仞信息科技有限公司、上海壁仞集成电路有限公司、上海壁仞科技股份有限公司、超燃半导体（南京）有限公司、苏州芯延半导体科技有限公司、珠海壁仞集成电路有限公司。</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da05d4dfac3c9968b1dbe0798fde537b.png" /></p><p></p><p>对此，壁仞科技在 10 月 17 日晚间发表声明称，公司对美国商务部此举表示强烈反对，将向美方有关政府部门积极申诉，并呼吁美国政府重新进行审视。壁仞科技以“智绘全球”为愿景，严格遵守相关国家和地区的法律、法规，并在此基础上始终合法依规经营。公司正在评估此事件可能对公司造成的影响，做好应对工作，并将与各方面积极沟通。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/41/41da8578559838606c2128232bf41056.png" /></p><p></p><p>摩尔线程也在声明中表示强烈抗议：摩尔线程自成立以来，严格遵守相关国家和地区的法律、法规，始终秉持合法、合规的企业文化和管理理念，建立了完善的出口管制合规管理体系和工作流程指引。目前公司正在与各方积极沟通，对于该事项的影响我们正在评估。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4f/4f32d3cdef00b714876c3d202f225e49.png" /></p><p></p><h2>ASML CEO：孤立中国没有希望，实际上会削弱西方自己</h2><p></p><p>&nbsp;</p><p>事实上，围绕美国出口限制政策一直存在不少反对的声音。</p><p>&nbsp;</p><p>据彭博社报道，Nvidia、Intel 和高通这三大美国本土芯片巨头的首席执行官一直在警告美国政府不要对华采取高压手段。在减少中国获得高尖端技术的同时，他们担心对向中国出口非尖端芯片的新限制将剥夺他们的大量收入来源。美国商会估计，如果出现最糟糕的情况，即对中国的销售完全停止，美国芯片公司每年可能会损失 830 亿美元、12.4 万个工作岗位，相关研发支出每年将减少 120 亿美元。</p><p>&nbsp;</p><p>ASML 现任总裁兼首席执行官 Peter Wennink 也曾在当地电视节目 Nieuwsuur 上说道，“完全孤立中国是没有希望的。如果我们不分享技术，他们就会自己去研究。”Peter Wennink 认为通过禁止技术移民和出口管制等方式孤立中国，实际上会削弱西方自己。</p><p>&nbsp;</p><p>此外，据环球网报道，香港半导体行业分析师林子恒 16 日对《环球时报》记者分析称，拜登政府在芯片领域的限制行动意图明显，就是想“扼杀”中国芯片产业，削弱中国在未来全球高科技领域竞争中的实力，并维护自身的科技霸权。林子恒认为，媒体报道的新限制措施更有针对性，可能会为中国芯片行业以及人工智能行业的发展带来新挑战，但也会激发出中国科研机构和企业的攻关动力。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/biden-cut-china-off-more-nvidia-chips-expand-curbs-more-countries-2023-10-17/">https://www.reuters.com/technology/biden-cut-china-off-more-nvidia-chips-expand-curbs-more-countries-2023-10-17/</a>"</p><p><a href="https://www.bis.doc.gov/index.php/about-bis/newsroom/2082">https://www.bis.doc.gov/index.php/about-bis/newsroom/2082</a>"</p><p><a href="https://baijiahao.baidu.com/s?id=1779952136508708360&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1779952136508708360&amp;wfr=spider&amp;for=pc</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2vE0o4dBI6N4idPFTUlt</id>
            <title>文心大模型4.0发布！李彦宏：相比GPT-4毫不逊色</title>
            <link>https://www.infoq.cn/article/2vE0o4dBI6N4idPFTUlt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2vE0o4dBI6N4idPFTUlt</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Oct 2023 03:46:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI原生应用, 文心一言, 李彦宏
<br>
<br>
总结: 李彦宏在百度世界2023上发布了文心大模型4.0版本，并展示了十余款基于文心一言的AI原生应用。他强调大模型是开发AI原生应用的基础，具备理解、生成、逻辑和记忆四大核心能力。通过插件和API的助力，大模型将推动AI原生应用生态的繁荣，促进经济增长。 </div>
                        <hr>
                    
                    <p>“大模型带来的智能涌现，这是我们开发AI原生应用的基础。”10月17日，李彦宏在百度世界2023上表示。当天，李彦宏以《手把手教你做AI原生应用》为主题发表演讲，发布文心大模型4.0版本，并带来新搜索、新地图等十余款AI原生应用。</p><p><img src="https://static001.geekbang.org/infoq/f5/f54e97d5835a6e76f2564c7ee051f29b.png" /></p><p></p><p>大会上，李彦宏宣布文心大模型4.0正式发布，开启邀请测试。他表示，这是迄今为止最强大的文心大模型，实现了基础模型的全面升级，在理解、生成、逻辑和记忆能力上都有着显著提升，综合能力“与GPT-4相比毫不逊色”。李彦宏介绍，文心4.0也同步开始邀测，现场观众扫描嘉宾证二维码，登录文心一言官网或下载最新版文心一言APP，就可以体验到文心一言的专业版；此外，企业客户也可以通过百度智能云千帆大模型平台来申请测试文心4.0&nbsp;API。</p><p></p><p>他现场展示了基于文心一言重构的百度搜索、如流、地图、网盘、文库等十余款AI原生应用，希望能拓展大家的想象力，“激发大家一起来做出更惊艳的AI原生应用来”。</p><p></p><h2>最强文心大模型4.0发布&nbsp;综合能力比GPT-4毫不逊色</h2><p></p><p></p><p>在李彦宏看来，AI原生应用的诞生，得益于大模型的理解、生成、逻辑和记忆四大核心能力，百度的AI原生应用也是基于文心一言来开发的，“这些能力是过去的时代所不具备的，因而才能打开无限的创新空间”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/aa/aaf20d6150f36ee1a7c7ddf530b95477.png" /></p><p></p><p>基于文心大模型4.0，李彦宏依次演示了四大能力的特点与应用场景。在理解能力上，他通过询问公积金异地贷款政策的案例，展示了文心一言对前后乱序、模糊意图、潜台词等复杂提示词的理解力，例如“在北京工作”等同于“在北京缴纳公积金”等等，“今天，你说的每一句话，它大概率都能听懂”。</p><p>&nbsp;</p><p>在生成能力上，李彦宏展示了文心一言如何在短短几分钟内，根据一张素材图片，迅速生成了一组广告海报、五条广告文案以及一条营销视频。据介绍，基于这一系列能力，百度已经推出了AIGC营销创意平台擎舵，让“一个人就成为一支AI营销队伍”。</p><p>&nbsp;</p><p>同时，他还通过解数学题、总结知识点等场景，展示了大模型的逻辑能力；通过数千字的小说撰写和角色、情节设置，体现了大模型的记忆能力；以及数字人医生帮助患者解读药品说明书，来展现四大能力的综合应用。</p><p>&nbsp;</p><p>“前面的演示，体现出文心大模型在理解、生成、逻辑、记忆这四大能力上的进步，这些能力是一切AI原生应用赖以生存的基础。”李彦宏表示。</p><p></p><h2>十余款AI原生应用重磅发布</h2><p></p><p></p><p>丰富的AI原生应用才是大模型的价值所在。大会上，李彦宏宣布“我们的搜索、如流、地图、网盘、文库等，都将以一个全新的面目与大家见面，”并表示，分享上述这些应用的目的，是为了拓展想象力、激发更多人做出更惊艳的AI原生应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d0149e6b4817c25ef7cb5923e194f4b1.png" /></p><p></p><p>李彦宏介绍，百度新搜索具有极致满足、推荐激发和多轮交互三个特点，当用户搜索问题时，新搜索将“不再是给你一堆链接”，而是通过对内容的理解，生成文字、图片、动态图表的多模态答案，让用户一步获取答案。在针对复杂需求时，“多轮交互”特点也可以通过提示、调整等方式，满足用户更个性化的搜索需求。</p><p>&nbsp;</p><p>同时，李彦宏还展示了用AI原生思维打造的国内第一个生成式商业智能产品：百度GBI。据介绍，相对传统BI软件的高门槛和数据分析难等问题，百度GBI可以通过自然语言交互，执行数据查询与分析任务，还支持专业知识注入，满足更复杂、专业的分析需求。</p><p>&nbsp;</p><p>通过对海量文档、图片和视频的理解和再生成，百度网盘和文库拥有了创作能力：网盘不仅能精准定位到视频某一帧，还能在几秒钟内总结完长达1小时的视频内容，并从中提炼出金句和要点；文库更是基于10亿优质资料，能实现写稿和做PPT等工作，成为名副其实的“生产力工具”。</p><p>&nbsp;</p><p>百度地图和智能办公平台如流，也通过理解、记忆等能力，变成更贴心的出行向导和超级助理：在地图上，用户只需说出需求，地图就能调动几千个服务接口，帮助用户推荐餐厅、对比多地点信息、给出出行建议；如流则可以针对群聊信息多的办公痛点，“一秒划重点”，差旅助手不仅能订机票酒店，甚至还能通过接入CRM等公司系统，总结出拜访客户的背景资料和谈话参考。</p><p>&nbsp;</p><p>正如李彦宏此前所说，AI原生应用不是对移动互联网App和PC软件的简单重复，而是要能“解决过去解决不了或解决不好的问题”。&nbsp;</p><p></p><h2>插件、API助力生态繁荣&nbsp;&nbsp;推动经济增长</h2><p></p><p></p><p>“大模型将开启一个繁荣的AI原生应用生态，”李彦宏强调，插件是一种特殊的AI原生应用，门槛最低，也最容易上手，能让开发者、创业者快速加入到生态中。他举例说，大模型接入权威法律数据的“智能法律助手”，能为用户提供法律咨询的相关建议，而简历助手插件则能帮用户一键生成简历模板。</p><p>&nbsp;</p><p>据介绍，个人及企业的数据、能力或应用，都能快速变成AI插件，增强大模型的能力，让大模型更实用易用。李彦宏表示，一个月前，百度上线了灵境插件平台，目前已经有2.7万开发者申请入驻，覆盖法律、职场、学习等多个领域。</p><p>&nbsp;</p><p>在开发AI原生应用时，大模型的基础能力至关重要。李彦宏介绍说，API是AI原生应用调用基础大模型的主要方式，企业和开发者可以在百度的千帆大模型平台上调取包括文心一言在内的大模型API，目前，千帆大模型平台已经成为中国最大的大模型开发平台，有42个主流大模型入驻，覆盖各行各业近500个场景。即日起，企业客户也可以在千帆大模型平台上申请测试文心4.0的API。</p><p></p><p>“中国有丰富的应用场景，中国用户又天然愿意拥抱新技术，有了先进的基础大模型，我们就能构建起一个繁荣的AI生态，共同创造新一轮经济增长。”李彦宏表示。</p><p>&nbsp;</p><p>此外，李彦宏表示，未来的AI原生应用一定是多模态的，在信息世界之外，一定会重构物理世界。自动驾驶就是视觉大模型重构物理世界的一个典型应用。大模型会让百度的自动驾驶能力超越经验系统，更聪明地处理复杂场景，实现更广泛的时空覆盖。目前，百度自动驾驶出行服务平台萝卜快跑累计提供服务超400万次，已经成为全球最大的自动驾驶出行服务商。</p><p>&nbsp;</p><p>“大量AI原生应用将不断涌现，数字技术与实体经济将深度融合……大模型正成为新型工业化的重要推动力。”李彦宏说。正如百度世界2023的主题是“生成未来”，在演讲结尾，李彦宏宣布，我们即将进入一个AI原生的时代，进入一个人机通过Prompt来交互的时代。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b3c871ce83566032fbf428201592da5.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/b7xuUQKHJvIFoVma98Zx</id>
            <title>这件事，已被大学生持续关注了 5 年……</title>
            <link>https://www.infoq.cn/article/b7xuUQKHJvIFoVma98Zx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/b7xuUQKHJvIFoVma98Zx</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Oct 2023 03:03:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 深圳国际金融科技大赛, 学生团队竞赛, 金融科技, 大赛盛宴
<br>
<br>
总结: 深圳国际金融科技大赛是一场面向学生团队的金融科技竞赛活动，已经连续举办了5年。大赛吸引了来自海内外高校的学生参赛，成为了具有广泛影响的赛事盛宴。大赛提供了区块链、人工智能和产品经理三个竞技赛道，参赛团队有机会获得丰厚的奖金和实习机会。此外，大赛还邀请了行业大佬担任评委和学术顾问，为参赛团队提供专业指导和支持。 </div>
                        <hr>
                    
                    <p>5 年，足以让一棵嫩芽茁壮成长——比如，一个学生完成从本科新生到研究生的转变；</p><p>5 年，也足以让一个新兴技术从初露头角发展为全球焦点——比如，金融科技打破传统金融服务边界，成为了数字化时代的重要部分……</p><p></p><p>那么，对于一年一度的“深圳国际金融科技大赛 - 西丽湖金融科技大学生挑战赛”来说，5 年又意味着什么呢？——意味着从初出茅庐的“新生赛事”成为了具有稳定基础和广泛影响的“赛事盛宴”！</p><p></p><p>自 2019 年第一届大赛落地，该品牌赛事至去年已成功举办 4 届，共吸引了 3500 余名来自海内外知名高校的学生参赛！每届大赛的举办都会在行业内引起一波浪潮！</p><p></p><p>今年，已经是大赛举办的第 5 年</p><p>在大家的热烈期盼下</p><p>大赛正式于 10 月 16 日 00:00 开赛！</p><p></p><p></p><p>🚀是的，你没听错！</p><p>2023 深圳国际金融科技大赛（ FinTechathon ）</p><p>—— 西丽湖金融科技大学生挑战赛全面启动！</p><p>正在面向国内外高校在读生火热招募中📣</p><p>突破界限，释放想象力</p><p>金融科技的未来，由你点燃！</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/355ea0cc1ea80a3fdbeb13d788a1d456.jpeg" /></p><p></p><p></p><p>这个大赛凭啥能“连续 5 年牵扯学生心”？！</p><p>一场专为学生团队打造的世界级金融科技竞赛</p><p></p><p><a href="https://www.infoq.cn/news/9AYU96ZSPoCZ6kyClK94">2023 深圳国际金融科技大赛——西丽湖金融科技大学生挑战赛</a>"（下文称“大赛”），是一场面向金融科技前沿技术领域的学生团队竞赛活动，是深圳市金融科技节的重要一环。</p><p></p><p>该赛事前身是“ FinTechathon 微众银行金融科技高校技术大赛”，在去年成功完成了品牌升级。经过 4 年的发展，大赛组委会从最初的办赛热忱中逐渐沉淀下来，对大赛的本质和价值进行深入的思考和探索，更加理性地审视了大赛的发展方向和目标，思考了如何更好地为参赛者服务。目前大赛已形成了一套完善的赛制和评选标准，赛事的整体筹备和落地已兼备成熟性，大赛的公平性和公正性有了更多保证，越来越多的优秀作品和人才脱颖而出。</p><p></p><p>从去年起，该赛事便由政、学、企三方联合共建，含金量十足！而本届大赛也依旧是在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办！</p><p></p><p></p><p>哦豁，今年的大赛搞了许多“新花活”？！</p><p>本届大赛的变与不变</p><p></p><p>本届大赛组委会将基于往届办赛经验，继续进一步提升赛事体验和评选质量。本届大赛保留了往届一样的区块链、人工智能、产品经理三个竞技赛道，三个赛道还是将分别通过初赛遴选出 10 支队伍进入决赛，每个赛道进入决赛的队伍将争夺一等奖 (1 队）、二等奖（1 队）及三等奖（1 队）！和去年一样，获奖队伍除了获得奖杯、纸质获奖证书、具有唯一标识的数字化获奖凭证“区块链数字证书”外，还将瓜分大赛组委会准备的 69W+ 的赛事奖金：</p><p>一等奖：100,000 元二等奖：80,000 元三等奖：50,000 元</p><p></p><p>（悄悄地和大家透露下：进入决赛的团队还将获得去微众银行实习的面试机会哦~）</p><p></p><p>本届大赛组委会依旧只接受“团队战”，需要 2-5 人 组队参赛，参与组队的成员不限学历、不限专业、不限年级，无论在国内还是国外，只要是高校在读生（含本科生、硕士 / 博士研究生）就可以参赛！</p><p>但，和去年不一样的是，因产品经理赛道所需参赛作品形式与另外两个赛道有所差异，故该赛道在今年增加了“复赛”，初赛将海选出 30 支队伍进入复赛进行线上答辩，复赛将选出 10 支队伍进入决赛。三个赛道的赛题也发生了变化，但较往年难度相当，具体内容可以前往大赛官网查看<a href="https://www.infoq.cn/article/%EF%BC%88https://www.infoq.cn/zones/fintechathon/campus2023/%EF%BC%89">（https://www.infoq.cn/zones/fintechathon/campus2023/）</a>"。</p><p></p><p>此外，本届大赛在初赛作品提交之前的“技术公开课”形式也发生了变化，10 月 25 日 -11 月 10 日，今年的大赛组委会除了做线上直播外，还将走到线下高校去与大家面对面交流，届时三个赛道的专家评委和金融科技行业的专家将分别围绕赛题内容展开技术干货分享。届时同学们可以密切关注大赛官方社群内发布的进校行程，关注“InfoQ 视频号”、“InfoQ 官网”直播间的大赛技术公开课的直播预告！</p><p></p><p>“就算拿不到奖”也要参加今年的大赛？!</p><p>数十位行业大佬亲自指导你的作品</p><p></p><p>本届大赛主办方将最大限度地发挥政、学、企三方的优势，坚守全面提高学生的创新能力、实践能力和就业竞争力的办赛初心。为此，大赛组委会特别邀请了国家统计局原副局长许宪春；加拿大皇家科学院院士、加拿大工程院院士、微众银行首席人工智能官杨强；清华大学五道口金融学院教授、华夏银行原行长、中国人民银行研究局原局长张健华；中国工商银行首席技术官吕仲涛；上海新金融研究院副院长、浙商银行原行长刘晓春；全国政协委员、南方科技大学副校长金李；中国银行业协会首席信息官高峰等人担当学术顾问，为大赛提供智力支持，帮助参赛团队更好地理解和应用金融科技知识。</p><p></p><p>除此之外，大赛组委会还邀请了来自中科院、清华大学、中山大学、西安电子科技大学、深圳大学、武汉大学、中央财经大学、广东财经大学、浙江财经大学、哈尔滨工业大学、微众银行等学企单位的数十位科研专家担任大赛评委，为参赛团队提供专业的指导建议，督促参赛团队把创新成果转化为实际应用，为金融科技行业提供更多有价值的技术解决方案，争取开创领域技术创新先河。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b9b06b4328d88a65eeb23cb8d5736111.jpeg" /></p><p></p><p>如此重磅的评委嘉宾阵容，意味着行业大佬对于参赛的你来说，再也不仅仅是视频里讲课的专家，而是直接帮助你订正作品内容、帮你解决技术难点的专属导师! 这些大佬的经验或许可以帮你在技术创新的道路上少走许多路，一定不能错过这样的好机会！</p><p></p><p>所以，你还在想什么？</p><p>赶紧扫描下方二维码进行报名吧！</p><p><img src="https://static001.geekbang.org/infoq/d0/d0dbb40226846cd13a49f1cd42ba5369.png" /></p><p></p><p>2023 深圳国际金融科技大赛（ FinTechathon ）</p><p>—— 西丽湖金融科技大学生挑战赛</p><p>全新就绪，等你来引爆金融科技的无限想象！</p><p>同学们可通过以下方式</p><p>了解更多大赛信息哦~</p><p></p><p>① 添加小助手随时随地了解比赛进程</p><p><img src="https://static001.geekbang.org/infoq/77/778476730106a49946b46a92c1bea68d.jpeg" /></p><p></p><p>② 登陆大赛官方网站了解更多大赛信息</p><p><a href="https://www.infoq.cn/zones/fintechathon/campus2023/">https://www.infoq.cn/zones/fintechathon/campus2023/</a>"</p><p></p><p>③ 通过大赛指定邮箱与主办方联系</p><p>fintechathon@geekbang.com</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uxPrRHWE4XEz4d7XZiL3</id>
            <title>百川智能启动2024校招，A1轮获阿里腾讯小米等3亿美元投资</title>
            <link>https://www.infoq.cn/article/uxPrRHWE4XEz4d7XZiL3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uxPrRHWE4XEz4d7XZiL3</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Oct 2023 01:40:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百川智能, 星耀计划, 校园招聘, 大模型初创企业
<br>
<br>
总结: 百川智能启动2024届校园招聘并发起“星耀计划”，面向全球精英科技人才，寻找有技术理想、热爱AI领域的应届生。百川智能是2024届校园招聘规模最大的大模型初创企业，拥有顶尖科技人才和雄厚资金支持，保持着行业领先的大模型研发速度和顶尖水准。 </div>
                        <hr>
                    
                    <p>近日，<a href="https://www.infoq.cn/article/ivM3DbowD6o9Ro4jIeGq?utm_campaign=geek_search_source&amp;utm_content=geek_search_source&amp;utm_medium=geek_search_source&amp;utm_source=geek_search_source&amp;utm_term=geek_search_source">百川智能</a>"正式启动2024届校园招聘并发起“星耀计划”。本次校招将面向海内外学生，同时覆盖北上广深等多个城市多所高校，目前百川智能是2024届校园招聘规模最大的大模型初创企业。</p><p>&nbsp;</p><p>“星耀计划”是百川智能面向全球精英科技人才的专项校园招聘计划。岗位涵盖了自然语言处理、计算机视觉、强化学习、基础架构等多个人工智能关键技术方向，旨在寻找有技术理想，热爱AI领域的精英人才。2023年11月- 2024年10月毕业的海内外应届生，均可通过百川智能校招官网进行申请<a href="https://campus.baichuan-inc.com/">官网地址</a>"，截止日期为12月31日。百川智能将为通过该计划的学生提供系统化培养和支持，助力同学们在技术领域的快速成长和飞跃。</p><p>&nbsp;</p><p>百川智能成立于2023年4月10日，由前搜狗公司CEO王小川创立。其核心团队由来自搜狗、Google、腾讯、百度、华为、微软、字节等知名科技公司的AI顶尖人才组成。目前，百川智能的团队规模170余人，其中硕士及硕士以上学历员工占比近70%，研发人员占比超80%。</p><p>&nbsp;</p><p>此前，百川智能已完成A1轮战略融资，融资金额3亿美元，阿里、腾讯、小米等科技巨头及多家顶级投资机构均参投了本轮融资。加上天使轮的5000万美元，百川智能的融资金额已达3.5亿美元。成立不到半年时间便跻身科技独角兽行列，创下国内大模型初创企业晋升独角兽速度之最。</p><p>&nbsp;</p><p>在顶尖科技人才和雄厚资金的支持下，百川智能保持了惊人的大模型研发速度。成立仅半年，百川智能便接连发布Baichuan-7B/13B，Baichuan2-7B/13B四款开源可免费商用大模型及Baichuan-53B、Baichuan2-53B两款闭源大模型，平均每28天就会发布一款新的大模型。</p><p>&nbsp;</p><p>百川智能不仅保持着行业领先的大模型研发速度，还将大模型的性能也做到了顶尖水准。Baichuan-7B/13B两款开源大模型在多个权威评测榜单均名列前茅，累积下载量超过六百万次。Baichuan2-13B在MMLU、CMMLU、MedQA、USMLE等几大权威评估基准中，以绝对优势全方位领先LLaMA2，引领开源社区走向中文开源<a href="https://www.infoq.cn/article/Qa3ExDDg5W4OiblRxpGx?utm_campaign=geek_search_source&amp;utm_content=geek_search_source&amp;utm_medium=geek_search_source&amp;utm_source=geek_search_source&amp;utm_term=geek_search_source">大模型时代</a>"。</p><p>&nbsp;</p><p>值得一提的是，8月31日百川智能率先通过国家《生成式人工智能服务管理暂行办法》备案，是首批八家公司中唯一一家今年成立的大模型初创公司，并于9月25日开放Baichuan2-53B&nbsp;API接口，正式进军To B领域，开启商业化进程。</p><p>&nbsp;</p><p>经过半年时间的发展，百川智能已经展示出了行业领先的技术竞争力和人才吸引力，在新一轮的融资过程中或将再受巨头和众多资本追捧。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/u3LfJYWerRKM6u2l4Ek2</id>
            <title>大模型时代下的技术变革：训练、负载、部署、效率、安全……都遇到了新挑战？</title>
            <link>https://www.infoq.cn/article/u3LfJYWerRKM6u2l4Ek2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/u3LfJYWerRKM6u2l4Ek2</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, AI大模型, 数据和计算资源, 应用场景支持
<br>
<br>
总结: 随着互联网的快速发展，AI大模型成为当前行业最热门的技术之一。大模型需要大量的数据和计算资源，并且对各行各业都有深远的影响。产业界和学术界在大模型的研发和应用方面有深入的合作和探索。然而，在大模型时代，算力资源、数据质量和规模对模型的性能至关重要，同时也需要解决数据安全等问题。英特尔及其伙伴在大模型技术方面取得了进展，释放出了新的机遇。 </div>
                        <hr>
                    
                    <p>随着互联网的快速发展，AI 大模型算的上是当前行业里最“炽手可热”的技术，大模型是 AI 领域的重要发展趋势。大模型需要大量的数据和计算资源，同时也需要强大的应用场景支持，对各行各业都有深远的影响，各厂商开始了“千模大战”。</p><p></p><p>当前，在 AI 大模型的研发和应用方面，产业界和学术界在很多方面都有深入的合作和探索。产业界和学术界都有各自的优势——产业界在数据采集、计算资源、应用需求理解等方面有独特的优势，学术界则在理论创新、方法研究、前沿技术探索等方面有显著的优势。</p><p></p><p>然而，在这个大模型时代，算力资源、数据质量和规模都对模型的性能有着至关重要的影响，包括数据安全也是当前亟需解决的问题。所以，在产业界和学术届深度融合探索下的 AI 大模型技术都有了哪些进展和变化？在这个过程中，是否释放出了新机遇？这两个问题的答案似乎在英特尔及其伙伴的实践中找到了。</p><p></p><p></p><h2>一、大模型的训练与负载：算力与成本之间需要寻找一个平衡</h2><p></p><p></p><p>随着人工智能和深度学习的发展，模型训练所需的数据量和处理能力在不断增加。多家研究报告显示，当前大型模型的训练数据量通常都达到了数百万甚至数千万级别。这些大型模型在进行训练时，需要处理的参数量相当庞大，例如 GPT-3 在训练时使用了 28.5 万 CPU 核心，总算力为 17.5 亿亿次，消耗了大约 250 万美元的 GPU 算力。大模型对大规模数据和计算资源的需求，对算力相关的硬件和软件都提出了更高要求。</p><p></p><p>为了提高模型的效果，往往需要采用更复杂的模型结构和训练策略，这也进一步增加了算力需求。同时，由于模型训练需要大量的时间和资源，训练时间也成了制约大模型发展的一个重要因素。对于一般企业而言，拥有如此强大的计算资源并不现实，因此企业都在积极寻找可以迭代优化模型训练和推理的基础设施。</p><p></p><p>然而算力与成本之间存在着明显的矛盾。首先，大模型训练需要大量的算力资源，而这些资源通常需要花费高昂的成本来获取。其次，数据传输和处理也会产生大量的成本，因为需要将大量数据从存储设备传输到计算设备进行处理。此外，硬件维护和软件开发也需要投入大量的人力物力。因此，在提高大模型训练效果的同时，厂商需要考虑如何平衡算力与成本之间的关系。</p><p></p><p>从整个模型的生态来看，其对于整个生态的部署要求肯定是“效率越来越高、成本越来越低”越好。英特尔院士、大数据技术全球 CTO 戴金权对此也表示：“从计算的角度来看，大模型需要很多的预训练，把模型预训练出一些比较好的基数。训练之后如何去用它、部署它，包括推理效率、微调效率，包括大模型其实是嵌入在一个端到端的一个工作流里面去后还能保持工作负载平衡。从这种计算角度来说，除预训练外，还需要做更多计算场景的策略和优化。”</p><p></p><p>戴金权的观点也显示出了英特尔的技术探索路径。为了保证负载平衡，英特尔提出了 Habana®Gaudi®2 的解决方案，其专注于深度学习的高性能解决方案，可满足大规模、高复杂性生成式 AI 和大型语言模型 (LLM) 训练工作负载的需求。</p><p></p><p>Gaudi2 采用经过验证的高性能深度学习 AI 训练处理器架构，利用 Habana 完全可编程的 TPC 和 GEMM 引擎，支持面向 AI 的高级数据类型，如 FP8、BF16、FP16、TF32 和 FP32 等，是一款性能更高的计算架构。值得一提的是，TPC 是一款 VLIW SIMD 矢量处理器，其指令集和邮件经过定制，不仅支持深度学习训练和推理工作负载，还可高效处理工作负载。</p><p></p><p>除了计算能力突出，Gaudi2 的内存带宽和容量也十分突出，其采用先进的 HBM 内存技术，内存容量高达 96GB，内存带宽高达 2.4TB/s。Gaudi 先进的 HBM 控制器已针对随机访问和线性访问进行了优化，在各种访问模式下均可提供高内存带宽。</p><p></p><p>Gaudi2 的能力其实就是帮助企业通过优化训练流程来降低成本——通过提高训练效率来减少训练时间，同时优化模型结构，减少参数量，从而降低算力和成本。除了这两种方式，企业其实还可以采用更加经济的算法和硬件资源来实现“算力与成本之间的平衡”，例如使用 GPU 代替 CPU 进行计算，目前很多硬件厂商也都在此方向上进行发力。</p><p></p><p>比如英特尔®Data Center GPU Max 系列则是专为应对最严苛的高性能计算 (HPC) 和 AI 工作负载而设计。英特尔&nbsp;®Xe Link 高速、一致的统一架构可灵活运行任何外形规格，实现纵向扩展和横向扩展。其利用“基于独立 SRAM 技术”的高达 408 MB 的 L2 高速缓存 (Rambo)、64 MB 的 L1 高速缓存，以及高达 128 GB 的高带宽内存，确保高容量和高带宽。同时还利用每个英特尔®&nbsp;Max 系列 GPU 上高达 128 个光线追踪单元，加速了科学可视化和动画过程；利用搭载深度脉动阵列的英特尔®&nbsp;Xe Matrix Extensions (XMX)，在单个设备上加速了 AI 工作负载，并启用矢量和矩阵功能，极好地帮助企业找到了算力与成本之间的平衡。</p><p></p><p></p><h2>二、大模型的部署：除了解决多场景，更重要的是提高效率</h2><p></p><p></p><p>戴金权对于“未来 AI 大模型技术创新及发展潜力”有许多值得行业从业者咂摸的观点：“大模型给了我们一个启示，大模型技术的前提不只是计算，而是训练本身，比如三阶段的训练，举个例子——很多大模型“诗写的好”，但是“写代码”不行，然后你就会发现它一般都会再发一个相应的“code 大模型”；而“什么都行”的大模型可能写代码就没有“code 大模型”写的好。其实本质上它是一个多任务或多目标的学习，所以是不是有办法来提升通用大模型的单项能力，这是一个很有意思的探索方向。但不管算力也好、成本也好、效率也好，怎么样利用是需要大家共同去探索的问题。比如大模型有很多不同的部署的场景，预训练、微调、推理、嵌入到工作流里去等等。如何通过硬件的 XPU 不同计算平台、软件上的各种技术能力来提高它的部署效率，这是另一个需要各厂商要去探索的问题。”</p><p></p><p>从戴金权的观点出发，并基于笔者对于行业的观察，我们基本上是可以总结出大模型当前的部署现状的：</p><p>模型部署难度较高：随着模型规模的不断扩大，需要消耗的计算资源、存储资源、网络资源等也越来越多，部署难度逐渐增大。对硬件资源需求大：大模型需要大量的 GPU 内存来进行计算，需要高性能的服务器来存储和传输数据，对硬件资源的需求非常大。需要支持并发处理：为了提高模型推理速度和效率，需要支持并发处理，这对服务器的并发处理能力提出了更高的要求。</p><p></p><p>从部署问题上，英特尔的合作伙伴腾讯云的解决方案就非常值得借鉴，在易用性方面，腾讯云训练集群的开启涉及复杂的系统设计，如 HCC 集群和分布式计算网络互通，并在实例设计时呈现给 AI 开发者一键部署功能，实现工程化效率提升；此外在供训练过程中，HCC 还具有高稳性能和故障自愈能力。从成本方面，腾讯云通过资源调度（如潮汐算力）实现集群效率最高。例如，在训练过程中，可能不会对加速芯片本身进行调度，而是将数据预处理或 DLC 业务与逻辑计算单元混部，以提高算力集群利用率。在部署效率方面，AI 开发者常遇到驱动版本不一致、兼容性等问题。腾讯云致力于在云原生环境中为大家提供更多一键部署和开发工具链，以缩短开发时间并提高效率。”</p><p></p><p>当然了，为了解决大模型的部署问题，<a href="https://www.infoq.cn/minibook/8XJWG3OkRtc7pBBTY172">英特尔</a>"确实没有少做努力。比如专为大模型时代发展而生的 Gaudi®&nbsp;2 在第一代基础上做了许多升级，第二代 Gaudi AI 深度学习夹层卡 HL-225B 专为数据中心实现大规模横向扩展而设计。其 AI 处理器基于第一代 Gaudi 的高效架构打造而成，目前采用 7 纳米制程工艺，在性能、可扩展性和能效方面均实现了飞跃，是一个“名副其实”的用于生成式 AI 和 LLM 训练的功能强大且经济高效的深度学习解决方案。</p><p></p><p>尤其值得说的是，在扩展性方面，Gaudi2 处理器具备出色的 2.1 Tbps 网络容量可扩展性，原生集成 21 个 100 Gbps RoCE v2 RDMA 端口，可通过直接路由实现 Guadi 处理器间通信。Gaudi2 处理器集成了专用媒体处理器，用于图像和视频解码及预处理。此外，Gaudi2 深度学习夹层卡还符合 OCP OAM 1.1（开放计算平台之开放加速器模块）等多种规范，可以为企业业务带来系统设计的灵活性。</p><p>在 2023 英特尔 On 技术创新峰会上，英特尔介绍的一台大型 AI 超级计算机，便是完全采用了英特尔至强处理器和 4000 个英特尔 Gaudi2 加速器打造的，据说它将跻身全球 TOP15 超算，目前热门 AIGC 应用 Stable Diffusion 的开发商 Stability AI 已经在全面使用它。同时英特尔首席执行官帕特·基辛格在本次峰会上还向大家透露了 Gaudi 3 的推出进程，“采用 5nm 制程的 Gaudi 3 将于明年推出，其算力是 Gaudi 2 的两倍，网络带宽、HBM 容量是 Gaudi 2 的 1.5 倍。”这意味着，大模型的部署效率问题可能在明年将实现一个飞跃式发展。</p><p></p><p>事实上，除了 Gaudi 2，为了更好地完成大模型的部署，英特尔®&nbsp;至强®&nbsp;可扩展处理器也一直在升级迭代，其无处不在的计算解决方案，配备英特尔®&nbsp;AMX 和其他集成式 AI 加速器，可在数据中心或边缘应用运行实时、中等吞吐量、低延迟的模型及应用。像阿里云通义千问大模型便是内置 AI 加速器的第四代英特尔至强可扩展处理器用于其生成式 AI 和大语言模型，英特尔技术大幅缩短了该模型的响应时间，平均加速可达 3 倍。</p><p></p><p>基辛格表示，第五代英特尔®&nbsp;至强®&nbsp;可扩展处理器未来将在同样功耗下，将有效提升数据中心的性能和存储速度，相比于第四代，该处理器在 AI 方面的性能将提升 2-3 倍。据悉，该处理器将于 12 月 14 日发布，非常值得大家密切关注。</p><p></p><p></p><h2>三、大模型的安全：将成为未来需要重点关注的问题</h2><p></p><p></p><p>今年 8 月底，首批通过备案的人工智能大模型名单出炉，这意味着这些生成式 AI 产品可以正式面向公众开放注册、提供服务。那在发布前后，大模型应用技术的开发速度或者供应商方面的技术演进上有何变化？对于该问题，戴金权表示——“如何更好地保护模型、保护数据、保护业务问题等安全问题变得越来越重要。”</p><p></p><p>所有技术在经历了爆火和高速发展的过程后，最终都会落到“安全”问题上，所以大模型也不例外。伴随着 AI 大模型的复杂性和应用范围将进一步扩大，其安全隐患将越来越多。例如，随着量子计算等新技术的出现，AI 大模型将面临更高级别的安全威胁。同时，随着数据隐私保护等法律法规的出台，企业当前越来越重视 AI 大模型的数据隐私保护工作。因此，未来需要加强技术研发，完善 AI 大模型的安全保障机制。</p><p></p><p>当前 AI 大模型安全现状并不乐观，技术漏洞是当前 AI 大模型面临的主要安全问题之一。例如，模型被黑客攻击、恶意注入病毒等问题时有发生。代码实现不当也可能导致 AI 大模型出现安全问题，比如有些模型在实现过程中可能存在未经验证的功能或逻辑漏洞，给恶意攻击者留下可乘之机。</p><p></p><p>我们溯源一下问题根本，数据质量差是影响 AI 大模型安全的重要因素之一。例如，如果数据本身存在大量噪声或缺失，将直接影响模型的训练效果和安全性。为了保护、清洗这些数据，<a href="https://www.infoq.cn/article/X50dOoVNWEIlhSvvCpiE">英特尔</a>"在机密计算领域投入大量研发资源，在 2015 年推出了英特尔®&nbsp;SGX，其是一种安全相关的指令，被内置于一些现代 Intel 中央处理器（CPU）中，它可以在基于硬件的可信执行环境中执行计算，确保任务和数据的安全性，防止被恶意程序窃取。在管理敏感数据和受监管数据方面，机密计算技术可以提高相关组织的安全级别。</p><p></p><p>此外，英特尔®&nbsp;TDX 是另一项前沿安全技术，其在虚拟机层面支持机密计算，满足虚拟机安全需求。所以英特尔的“机密计算”也被戴金权称为是一个“端到端”的能力，“大模型安全并不是只需要在一个环节安全，整个流程都需要安全，而英特尔的机密计算从数据存储、加密、整个分布式计算、网络通讯，包括远程验证等都完成了实现了安全保护。”目前英特尔作为“机密计算联盟（Confidential Computing Consortium）”成员之一，正在持续积极推动机密计算技术的标准化和普及。</p><p></p><p></p><h2>四、写在最后：AI 大模型对基础设施、硬件提出了更高要求</h2><p></p><p></p><p>随着大模型技术逐渐进入深水期，各企业在相关技术方面的验证逐渐全面，大家都已经非常明确，如果想要充分释放 AI 大模型的潜力，仅依靠软件层面的优化是不够的，基础设施硬件设备的性能和稳定性也在 AI 大模型的高效运行中扮演着至关重要的角色。</p><p></p><p>当前大模型对基础设施的要求非常高。就单从硬件方面来看，大模型需要大量的高性能计算资源，包括 CPU、GPU 和 TPU 等。这些计算资源需要具备高并发、低延迟的特点，以满足 AI 大模型的计算需求。同时，为了提高计算效率，需要采用先进的芯片设计和制造技术，加强芯片间的通信和协作。</p><p></p><p>为了满足大模型对硬件性能的高要求，硬件厂商需要不断提升自身的研发实力和技术积累。这包括对先进制程技术的掌握，以及对各种处理器架构的深入理解。此外，硬件厂商还需要与软件厂商紧密合作，共同优化大模型的性能。通过软硬件的协同创新，可以充分发挥硬件设备的性能潜力，为大模型的发展提供强大的支持，无论是从算力、效率、成本还是安全等各个方面。</p><p></p><p>于此，大模型对硬件厂商的技术能力也提出了更高的要求。这意味着硬件厂商需要具备跨学科的能力，以整合不同领域的技术资源，为企业提供更加完善的解决方案，以满足不同行业和应用场景的需求。</p><p></p><p>不仅是硬件厂商，大模型技术的发展离不开产业链上的每一个角色，众人拾柴才能火焰高，大模型时代需要学术界和产业界进行深入地合作和联动。通过联动，学术界的研究成果可以更快地应用于产业界，推动技术的发展和进步，同时产业界的需求和反馈也可以引导学术界的研究方向，使其更加贴近实际应用场景。在当前这个大模型时代的背景下，合作和联动可以促进不同组织之间的协作，实现资源的共享和整合，提高研究的效率和成果的质量。</p><p></p><p>正如戴金权所说的那样，“<a href="https://www.infoq.cn/article/cff5Oa9fYLNtU46us0ez">英特尔</a>"一直坚持开源开放，无论是从客户侧的产业界合作，还是从学术界的高校合作，英特尔都在持续推动，相信在多方的努力下，大模型技术的发展将会越来越好。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QFytIPELB8ZxSrmDpLXj</id>
            <title>OpenAI悄悄改变核心价值观惹争议：埋头搞AGI，其他的都是浮云！</title>
            <link>https://www.infoq.cn/article/QFytIPELB8ZxSrmDpLXj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QFytIPELB8ZxSrmDpLXj</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 05:41:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 核心价值观, AGI, 人工智能
<br>
<br>
总结: OpenAI最近修改了其核心价值观，将通用人工智能（AGI）纳入其中。这一变化引发了人们对其核心价值观真实性和可靠性的质疑。尽管人们对于核心价值观的转变持怀疑态度，但OpenAI的未来方向已经调整，将致力于开发安全、有益的AGI技术，借此对人类产生积极影响。 </div>
                        <hr>
                    
                    <p></p><blockquote>最近几周，OpenAI悄然修改了其网站上列出的所有“核心价值观”，更加强调 AGI（通用人工智能）的发展。</blockquote><p></p><p></p><h2>OpenAI悄悄改变核心价值观，重点聚焦AGI</h2><p></p><p>&nbsp;</p><p>据外媒报道，最近几周，作为全球领先的AI研究机构，OpenAI正悄悄对其核心价值观做出重大调整，将之前未明确列出的通用人工智能 (AGI) 纳入其中。</p><p>&nbsp;</p><p>据&nbsp;Semafor报道，该公司此前的价值观为“大胆”、“深思熟虑”、“朴实无华”、“影响力驱动”、“协作”和“以增长为导向”。这些旧点价值观将被一系列新的价值观所取代、明确将AGI列为后续工作的重中之重。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0b153e221b9044420859bf71377018d2.png" /></p><p></p><p>&nbsp;截图来源：OpenAI官网首页</p><p>&nbsp;</p><p>新变更的价值观主要包括五点：</p><p>&nbsp;</p><p>聚焦通用人工智能</p><p>OpenAI致力于构建安全、对社会有所助益的人工智能，它将对人类未来产生巨大的积极影响。</p><p>与此无关的任何事情都不在考虑范围之内。</p><p>&nbsp;</p><p>坚韧不拔、勇往直前</p><p>创造非凡的事物需要努力工作（通常是不那么吸引人的任务）和紧迫感；我们所做的每一件事都很重要。要谦逊务实，想尽一切办法做切实可行的事。</p><p>&nbsp;</p><p>坚守规模化效应</p><p>当在我们的模型、系统、自身、流程以及理想抱负达到一定规模时，就会创造奇迹。当受到质疑时，扩大规模是一种十分奏效的方式。</p><p>&nbsp;</p><p>制造出让人喜爱的东西</p><p>OpenAI的技术和产品应当对人们的生活带来革命性的积极影响。</p><p>&nbsp;</p><p>团队精神</p><p>OpenAI最大的进步和差异化来自于团队内部和之间的有效协作。虽然OpenAI的团队有着越来越多的不同身份和优先事项，但整体目标和宗旨必须保持完全一致。凡是归因自身，没有什么问题是别人的问题。</p><p>&nbsp;</p><p>OpenAI 多年来一直表示希望开发 AGI，尽管这种技术的具体细节尚不清楚。在 2018 年发布的一份使命声明中，OpenAI 将 AGI 描述为“在最具经济价值的工作中超越人类的高度自治系统”。</p><p></p><h2>价值观变更惹争议，说变就变也太随意了</h2><p></p><p>&nbsp;</p><p>这一变化引发了人们对这些核心价值观真实性和可靠性的质疑。如果一家企业能够轻松更改其核心价值观，那么这些价值观还能否称得上“核心”？这无疑会激起外界对于该公司在既定目标一致性和承诺方面的担忧。</p><p>&nbsp;</p><p>将“聚焦AGI”作为公司核心价值之举尤其值得注意。而且，OpenAI对于AGI的解释似乎也仍有含糊不清之处。今年 2 月，OpenAI 的首席执行官山姆·奥尔特曼（Sam Altman）在公司博客文章中写道，AGI 可以广义地定义为“通常比人类更聪明的系统”，但在最近一次采访中，奥特曼似乎重新将AGI定义为与普通人类等同的人工智能。</p><p>&nbsp;</p><p>这种AGI定义层面的差异，也进一步引发了关于OpenAI发展目标的讨论。作为一家随时间推移而不断调整方向的公司，OpenAI已经从一家专注于打造良好AI的非营利组织，转变成一家营利性实体。这种目标转变似乎又反过来影响了其技术定义与核心价值观。</p><p>&nbsp;</p><p>尽管人们对于核心价值观的转变持怀疑态度，但OpenAI的未来方向的确已经在就此做出调整。该公司强调将致力于开发安全、有益的AGI技术，借此对人类产生积极影响。也就是说，OpenAI在接下来的经营活动当中将优先考虑与该目标相适应的项目和举措。</p><p>&nbsp;</p><p>在新核心价值观的加持下，OpenAI明显将投入更多资源推动AGI议题。然而，他们将采取怎样的AGI定义方式和实现思路仍然有待观察。</p><p></p><h2>网友怎么看？</h2><p></p><p>&nbsp;</p><p>OpenAI价值观变更一事在Reddit上引发了积极讨论，有网友猜测，“AGI已在OpenAI内部实现了”。一些用户认为这种变更很有意思，“看起来OpenAI似乎已经弄清楚了如何让自己更加强大，现在只是在寻找人来实际建造它（AGI），新的核心价值观读起来更聚焦。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/47/475f52621980c42d460ef9dddf47c0b3.png" /></p><p></p><p>&nbsp;截图来源：Reddit</p><p>&nbsp;</p><p>但也有一些用户对于价值观的改变表现出了担忧，一名ID为Freedom_Alive的用户称：</p><p>&nbsp;</p><p></p><blockquote>“这让我想起了谷歌从其核心价值页面中删除“不作恶”的时候。”</blockquote><p></p><p>&nbsp;</p><p>言外之意，OpenAI价值观的改变也说明了公司行事风格将会与以前不同了。</p><p>&nbsp;</p><p>ID名为Accurate-Ease1675的用户则表示，“让我有点担心的是，像 OpenAI 这样的公司似乎并不理解价值观、使命、目标和愿景之间的区别。以前的价值观没问题，但修改后的价值观不是真正的价值观。它们是一些雄心勃勃的陈述的大杂烩，如果需要做一些额外的工作来解释这些所谓的价值观。 OpenAI之前的价值观中有一条是深思熟虑，现在看来，他们压根也没实现这一价值观。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://ts2.space/en/openai-updates-core-values-to-include-artificial-general-intelligence-agi/">https://ts2.space/en/openai-updates-core-values-to-include-artificial-general-intelligence-agi/</a>"</p><p><a href="https://futurism.com/the-byte/openai-core-values-agi">https://futurism.com/the-byte/openai-core-values-agi</a>"</p><p><a href="https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html">https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9AYU96ZSPoCZ6kyClK94</id>
            <title>创新风潮迭起，2023深圳国际金融科技大赛——西丽湖金融科技大学生挑战赛正式启动</title>
            <link>https://www.infoq.cn/article/9AYU96ZSPoCZ6kyClK94</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9AYU96ZSPoCZ6kyClK94</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 05:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技领域, 市场规模, 科技创新, 金融科技节
<br>
<br>
总结: 近年来，我国金融科技领域发展迅速，市场规模稳步增长。政府和企业界形成了良好的产学研合作机制，推动金融科技产业发展。深圳市金融科技节作为重要的活动之一，吸引了全球金融科技人才参与。大赛旨在鼓励学生探索金融科技领域的创新应用，为行业提供有价值的技术解决方案。同时，大赛还邀请了多位专家担任学术顾问和评委，为参赛团队提供支持和指导。 </div>
                        <hr>
                    
                    <p>近年来，我国在金融科技领域取得显著发展。根据赛迪顾问《金融科技发展白皮书》数据显示，自 2016 年起相关市场规模一直保持着 10% 左右的稳定增速，2022 年的市场规模同比增长 18.3%。10 月 8 日，《深圳市关于金融支持科技创新的实施意见》正式印发实施，明确将进一步完善金融支持科技创新体系，加大对科技型企业融资的支持力度，建立健全“基础研究 + 技术攻关 + 成果产业化 + 科技金融 + 人才支撑”的全过程创新生态链。</p><p></p><p>为了满足金融科技产业技术创新及人才需求，更好地推动金融科技产业发展，目前政府、学术界和企业界形成了良好的产学研合作机制。作为 2023 年<a href="https://www.infoq.cn/article/7ejrDIB7r5KRIuLwaRPd">深圳市金融科技节</a>"的重要一环，在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办的“2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛”（下文称“大赛”）于 10 月 16 日正式开赛。</p><p></p><p>据悉，该赛事自 2019 年落地至今，已成功举办四届并完成了<a href="https://www.infoq.cn/article/5DkdKQyjRuC9YNTgbhm6">赛事品牌升级</a>"。大赛汇聚了全球前沿的金融科技人才，其高水平的参赛者、极具挑战性的赛题内容和评委的卓越见解，在过往四届吸引了 3500 余名来自海内外知名高校的学生参赛，备受金融科技领域从业者的关注和认可。本届大赛组委会将基于往届办赛经验，继续进一步提升赛事体验和评选质量。大赛全程聚焦金融科技的前沿理论，分设人工智能、区块链、产品经理三个赛道，将通过初赛、复赛在各赛道分别遴选出 10 支队伍进入决赛角逐，并设置总额超过 69 万人民币的赛事奖金及参赛专属电子区块链证书，以奖励各赛道获得一等奖、二等奖、三等奖的队伍及成员。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7b/e1/7b9055577073aea33b430e1f0ea373e1.jpg" /></p><p></p><p>本次大赛致力于鼓励国内外高校学生积极探索金融科技领域的技术应用创新，将创新成果转化为实际应用，为金融科技行业提供更多有价值的技术解决方案。为此，大赛组委会特别邀请了国家统计局原副局长许宪春；加拿大皇家科学院院士、加拿大工程院院士、微众银行首席人工智能官杨强；清华大学五道口金融学院教授、华夏银行原行长、中国人民银行研究局原局长张健华；中国工商银行首席技术官吕仲涛；上海新金融研究院副院长、浙商银行原行长刘晓春；全国政协委员、南方科技大学副校长金李；中国银行业协会首席信息官高峰等人担当学术顾问，为大赛提供智力支持，帮助参赛团队更好地理解和应用金融科技知识。</p><p></p><p>此外，大赛组委会还邀请了来自中科院、清华大学、中山大学、西安电子科技大学、深圳大学、武汉大学、中央财经大学、广东财经大学、浙江财经大学、哈尔滨工业大学、微众银行等学企单位的数十位科研专家担任大赛评委，为参赛团队提供专业的指导建议，挖掘优秀的参赛项目和人才，以加快深圳市金融科技产业升级，抢抓金融科技发展机遇。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/dba9c5b835fb450d6ad26674e2cc743b.jpeg" /></p><p></p><p>10 月 16 日起，本届大赛正式开启报名通道，国内外高校在读生（含本科生、硕士 / 博士研究生）均可报名参赛。有兴趣的同学可点击<a href="https://www.infoq.cn/zones/fintechathon/campus2023">链接</a>"进入大赛官网 ，或识别下方海报中的二维码进行报名。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04fc67e1a319867248a9cde86965a33e.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hI5XGjZzTsLxDS2Q9zSL</id>
            <title>同盾科技软件产品及方案部 / 总经理董纪伟确认出席 FCon，分享黑灰产欺诈攻防体系的研究与实践</title>
            <link>https://www.infoq.cn/article/hI5XGjZzTsLxDS2Q9zSL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hI5XGjZzTsLxDS2Q9zSL</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 03:59:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 黑灰产欺诈攻防体系的研究与实践, 董纪伟, 欺诈攻防体系的建设路径和实现建议
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，董纪伟将发表题为《黑灰产欺诈攻防体系的研究与实践》的主题分享，解析欺诈攻防的底层逻辑，给出新一代的欺诈攻防体系的建设路径和实现建议。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。同盾科技软件产品及方案部 / 总经理董纪伟将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5573?utm_source=infoqweb&amp;utm_medium=article">黑灰产欺诈攻防体系的研究与实践</a>"》主题分享，解析欺诈攻防的底层逻辑，通过对攻防内容设计，综合评判欺诈概率，解决信息差的欺诈本质问题，并基于行业的领先实践，给出新一代的欺诈攻防体系的建设路径和实现建议。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5573?utm_source=infoqweb&amp;utm_medium=article">董纪伟</a>"，花名“阅微”，行业资深安全专家，硕士毕业于中国科学院大学计算机技术专业，曾任国密局密码算法课题组成员 ，人民银行支付清算协会反诈培训讲师，北京金融科技产业联盟金融科技领域高级技术专家。</p><p></p><p>目前担任同盾科技软件产品及方案部总经理兼策略模型总监，负责软件产品线相关解决方案、技术及架构，以及安全策略咨询及模型咨询，负责过工行、建行、邮储、广发、中信、银联等金融行业多家机构风控应用及策略模型的设计与研发，主力研发的交易监控反欺诈软件荣获 2015 年人民银行科技发展二等奖。</p><p></p><p>10 余年金融行业工作经验，FRM 金融风险管理师认证。熟悉金融领域风控与反欺诈相关产品、技术、业务及场景解决方案，擅长反欺诈规则、策略设计及特征建模；曾任人民银行下属机构研发部开发经理、项目经理、高级安全咨询顾问、反欺诈团队负责人、反欺诈项目总监等。他在本次会议的演讲内容如下：</p><p></p><p>演讲：黑灰产欺诈攻防体系的研究与实践</p><p></p><p>随着 ChatGPT 的横空出世，人工智能技术发展迅猛。但与此同时，AI 技术被黑灰产滥用的负面作用也逐步显现，移动互联网的发展也促使场景复杂化，黑灰产欺诈的攻击面、攻击点呈现爆发式增长，呈现隐匿化、团伙化、速度化的态势。此次分享的解决方案将通过黑灰产的最新趋势分析，解析欺诈攻防的底层逻辑。通过对不同主体、在不同环节的攻防内容设计，综合评判欺诈概率，解决信息差的欺诈本质问题，并基于行业的领先实践，给出新一代的欺诈攻防体系的建设路径和实现建议。</p><p></p><p>演讲提纲：</p><p></p><p>黑灰产的最新态势分析不知攻焉知防——黑灰产实施攻击的主要手法、攻击链路如何一环扣一环对于欺诈攻防的思考他山之石——如何有效构建欺诈防御体系</p><p></p><p>你将获得：</p><p></p><p>○ 了解最新的黑灰产欺诈形势</p><p>○ 知晓典型的欺诈场景，如 ChatGPT、AI 换脸、屏幕共享如何被黑产利用</p><p>○ 了解反诈的最新技术应用</p><p>○ 了解业内先进的攻防实践</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 7 折优惠 ，立省 ￥2040！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FjKxBRyykMgAsNHSHJe6</id>
            <title>京东辟谣“刘姓商人涉嫌违法被抓”；比特大陆全员工资暂停发放；一周可居家办公3 天，去哪儿灵活办公制度出炉｜Q资讯</title>
            <link>https://www.infoq.cn/article/FjKxBRyykMgAsNHSHJe6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FjKxBRyykMgAsNHSHJe6</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 01:10:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 去哪儿, 实行灵活办公制度, 比特大陆, 部矿进度严重不达标, Unity首席执行官卸任
<br>
<br>
总结: 去哪儿网实行灵活办公制度，与员工一起探索更酷的工作方式；比特大陆部矿进度严重不达标，导致员工工资被暂停发放；Unity首席执行官卸任，曾受到死亡威胁。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>去哪儿回应实行灵活办公制度：和员工一起探索更酷的工作方式；比特大陆部矿进度严重不达标，全部员工工资被暂停发放；Unity首席执行官卸任，曾受死亡威胁；印度逮捕vivo中国员工？vivo回应；威马车主反映车机、手机 App 已停服，客服电话无人接听；传百度文心4.0推理成本翻10倍；微软GitHub Copilot 亏损严重，每个用户每月亏损超 20 美元；OpenAI 的年化营收超过 13 亿美元；图灵奖得主、深度学习之父Hinton入局机器人创业；强化学习之父萨顿联手传奇程序员卡马克入局 AGI 创业，放话不依赖大模型；为成为 iOS 默认搜索引擎，Google 每年向苹果支付 180 亿-200 亿美元；美国欲打算管制开源的RISC-V，担心中国借此发展自己的半导体产业；北京应届毕业生招聘月薪超1.3万元居全国首位，人工智能领跑行业……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p></p><h4>京东：关注到有谣言称“刘姓商人涉嫌违法被抓”已报案</h4><p></p><p>&nbsp;</p><p>10月13日，京东发言人发文称，我们关注到有谣言称“刘姓商人涉嫌违法被抓”，该谣言被别有用心的人刻意发布在京东相关新闻动态下，以混淆视听、操纵舆论。我们对此恶劣行径表示强烈愤慨，并已向公安机关报案。</p><p>&nbsp;</p><p>当天上午，京东集团港股股价大跌，一度跌超12%，报每股103.5港元，创上市以来新低。目前，京东集团港股最新总市值约为3291亿港元。此外，隔夜美股京东收跌8.27%，报27.83美元。</p><p>&nbsp;</p><p></p><h4>去哪儿回应实行灵活办公制度：和员工一起探索更酷的工作方式</h4><p></p><p>&nbsp;</p><p>此前，有去哪儿员工爆料称，继携程之后去哪儿网也开始实行居家办公制度。10月10日，去哪儿COO（首席运营官）刘连春发全员信称：让员工可以自己选择办公地点是去哪儿此次的尝试，分组进行是希望 “探索更酷的工作方式，在不同模式下，找到工作效率与生活幸福指数间最大的平衡。混合办公就是为了鼓励大家多走出去看看这个世界。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/134353b4ef915034cc42a0a00adbffea.png" /></p><p></p><p>&nbsp;</p><p>据了解，去哪儿灵活办公目前只有一部分员工参与实验，分ABCD四组，分别是每周灵活0、1、2、3天，该分组会持续到明年六月。有网友调侃道：只要不在去哪儿呆着，想去哪儿去哪儿。</p><p>&nbsp;</p><p></p><h4>比特大陆部矿进度严重不达标，全部员工工资被暂停发放</h4><p></p><p>&nbsp;</p><p>据多名比特大陆内部员工确认，比特大陆发布通知：9月公司经营现金流仍未转正，尤其是部矿（指矿机进驻矿场）进度严重不达标，EMT决定暂缓发放9月份全体员工部分工资，10月7日假期之后视情况发放。多名员工透露，所有员工9月份的绩效工资被全部扣掉，基本工资也被扣了一半，被扣掉的工资不知道啥时候才能发。截至10月8日，员工们都还没收到未发的工资，2022年的年终奖至今都没有发。</p><p>&nbsp;</p><p>比特大陆曾垄断全球超七成的比特币矿机市场份额，后因两大创始人吴忌寒、詹克团争夺控制权的“内斗”事件元气大伤。今年一季度，比特大陆执行员工结构薪酬改革，在绩效考核时加入“年龄分”，基准年龄之上，年纪越大扣分越多。比特大陆员工称，在最新的薪酬调整方案中，原本的固定工资调整为基础工资+绩效工资两部分，而绩效工资与职级挂钩，T3x、T4x、T5x三档职级绩效工资比例分别为30%、50%、70%。</p><p>&nbsp;</p><p></p><h4>Unity首席执行官卸任，曾受死亡威胁</h4><p></p><p>&nbsp;</p><p>10月9日，游戏引擎开发商Unity发布一则关于《Unity宣布领导层交接》的公告，称公司CEO John Riccitiello将卸任，即日起生效。而这距9月12日那项引爆整个游戏圈的“按安装量收费”新模式推出以来，还不足一个月，Riccitiello 甚至在9月15日被曝受到了死亡威胁。</p><p>&nbsp;</p><p>9月12日，Unity在官网公告称，将从2024年1月1日开始向符合条件的游戏产品征收运行时费用“Unity Runtime Fee”，按安装量单次收费，价格区间为0.01-0.2美元。Unity这一公告在游戏圈内引发震动，多家开发商集体表达了不满和抗议。当前，外界对这新一轮的人事变动的普遍看法是，这应该是为此前“灾难性的”新收费模式所引发的巨大争议进行收场。</p><p>&nbsp;</p><p>更多详情阅读：</p><p><a href="https://mp.weixin.qq.com/s/tv4Yb5uftj2WyVPG3WtUuA">小型开发者的生存之战：Unity 想要我们的全部收入！我们要破产了</a>"</p><p>&nbsp;</p><p></p><h4>印度逮捕vivo中国员工？vivo回应</h4><p></p><p>&nbsp;</p><p>10月11日，据外媒报道，印度金融执法机构已经逮捕了中国手机公司vivo的一名中国籍员工。一名印度政府官员证实，印度执法局已经逮捕了四名与vivo有关的人，其中包括一名中国公民。截至目前，印度执法局尚未就此公开发表评论。</p><p>&nbsp;</p><p>对此，vivo 回应称，一名员工被捕，但没有详细说明被捕者的国籍。该公司在一份声明中表示：“印度执法局最近的逮捕行动令我们深感担忧。我们将动用一切可用的法律手段。”vivo还补充说，公司会“坚定地遵守其道德原则，并致力于合法合规”。</p><p>&nbsp;</p><p></p><h4>威马车主反映车机、手机 App 已停服，客服电话无人接听</h4><p></p><p>&nbsp;</p><p>近日，威马汽车配套软件无法使用的消息引发热议，而威马车友圈中有网友表示，最早从今年 8 月开始，威马的 App 就出现异常。有网友反映，手机端只能接收数据，不能控制车辆。也有网友称，威马车机系统登入二维码都已经无法显示。</p><p>&nbsp;</p><p>此外，还有众多车主反映，威马汽车目前经营异常，门店关停、无法提供汽车配件、售后服务停滞、人工客服缺位等。导致他们在购买威马汽车后无法正常进行保养、汽车出现故障后不能及时维修、签订的电池更换协议无法履行、客服热线一直处于忙线状态无法打通等，消费者权益因此受损。另据报道，尝试拨打威马汽车 400 客服电话，但处于无人接听的状态，而威马汽车社交平台的官方号也未对此做出回应。</p><p>&nbsp;</p><p></p><h4>传百度文心4.0推理成本翻10倍</h4><p></p><p>&nbsp;</p><p>近日，有媒体报道称，百度正加紧训练文心大模型4.0，这将是文心大模型3.5版本后又一个重磅版本，或将在 10 月 17 日举行的百度世界大会上发布。</p><p>&nbsp;</p><p>据报道，文心大模型4.0进展比预期快很多，将是基础模型的大升级，理解、生成、逻辑、记忆核心能力都将提升，特别是在逻辑推理、代码和数学等方面提升最明显。据悉，文心大模型4.0的推理成本相比文心大模型3.5增加10倍。此外，文心大模型4.0的参数规模也将更大，预计突破万亿级别。</p><p>&nbsp;</p><p></p><h4>微软GitHub Copilot 亏损严重，每个用户每月亏损超 20 美元</h4><p></p><p>&nbsp;</p><p>10月10日消息，过去一年，生成式 AI 的热潮为许多公司带来了巨大的利润。其中最大的受益者之一就是英伟达，其 GPU 在 2023 年被大量用于微软等公司的数据中心，使得该公司的利润和股价大幅上涨。然而，微软在其 AI 服务方面却一直难以盈利。据外媒报道，微软在其首个生成式 AI 服务 GitHub Copilot 上损失了大量资金。</p><p>&nbsp;</p><p>报道称，自推出以来，微软在 GitHub Copilot 上一直亏损惨重：“据一位知情人士透露，今年前几个月，平均每个用户让微软每月亏损超过 20 美元，有些用户每月给公司造成的损失高达 80 美元。”报道还称，微软一直在寻找更便宜的运行其 AI 服务的方式。其中一种可能是自制 AI GPU，而不是从英伟达购买。最近有消息称，微软将于 11 月 14 日在 Ignite 大会上正式公布这种 AI 芯片。</p><p>&nbsp;</p><p></p><h4>OpenAI 的年化营收超过 13 亿美元</h4><p></p><p>&nbsp;</p><p>据多位知情人士透露，OpenAI CEO Sam Altman本周告知员工，公司的收入达到年化 13 亿美元。这意味着，当前 OpenAI 的月收入超过 1 亿美元。13 亿美元的年化收入较夏季时年化 10 亿美元的收入还要高出 30%。而去年一整年，OpenAI 的收入仅为 2800 万美元。自今年 2 月推出付费版 ChatGPT 以来，该公司的收入增长速度相当可观，主要来自其“会话聊天机器人”的订阅。</p><p>&nbsp;</p><p>不过，与此同时，布朗大学的计算机科学研究人员发现了 OpenAI 的 GPT-4 安全设置中的新漏洞。他们利用一些不太常见的语言，如祖鲁语和盖尔语，即可以绕过 GPT-4 的各种限制。研究人员使用这些语言来写通常受限的提示词（prompt），发现得到回答的成功率为 79%，而仅使用英语的成功率不到 1%。研究人员承认发布这项研究可能会造成危害，并给网络犯罪分子提供灵感。值得一提的是，在向公众发布之前，该研究团队已经与 OpenAI 分享了他们的发现，以减轻这些风险。</p><p>&nbsp;</p><p></p><h4>图灵奖得主、深度学习之父Hinton入局机器人创业</h4><p></p><p>&nbsp;</p><p>10月12日消息，图灵奖得主、深度学习之父Geoffrey Hinton宣布，将加入机器人初创公司Vayu Robotics，担任顾问一职。今年5月，Hinton突然从任职十载的谷歌离职，轰动整个科技圈。他本人当时表示，这么做是为了可以自由地讨论人工智能风险。</p><p>&nbsp;</p><p>自从离职后，这位AI教父收到邀约不断，但都没能吸引到他——直到Vayu Robotics出现。Hinton给出的加入理由是，它们的技术路线和其他很多AI应用相比，AI道德风险更低。当然Vayu Robotics自身实力也很强，被英伟达AI科学家Jim Fan称为业内的“big names”。不过还有一点非常关键：Vayu Robotics的CTO尼蒂什·斯里瓦斯塔瓦（Nitish Srivastava）为Hinton门下弟子。</p><p>&nbsp;</p><p></p><h4>强化学习之父萨顿联手传奇程序员卡马克入局 AGI 创业，放话不依赖大模型</h4><p></p><p>&nbsp;</p><p>据报道，传奇程序员卡马克和强化学习之父萨顿联手创办了 AI 创业公司 Keen Technologies，他们的目标是在 2030 年向公众展示通用人工智能的可行性。</p><p>&nbsp;</p><p>与主流方法不同，他们不依赖大模型，而是追求实时的在线学习。他们相信，最终的 AGI 源代码可以由一个人编写，只需几万行。两人在萨顿任教的阿尔伯塔大学机器智能研究所（Amii）特别活动上宣布了这一消息。萨顿同时还会保持在阿尔伯塔的教职。</p><p>&nbsp;</p><p>两人在活动中都承认，与拥有成百上千员工的大公司相比，Keen Technologies 的团队规模很小。目前还在刚起步阶段，“公司整个技术团队都到了现场——只有站着的这 4 个人。”</p><p>&nbsp;</p><p></p><h4>为成为 iOS 默认搜索引擎，Google 每年向苹果支付 180 亿-200 亿美元</h4><p></p><p>&nbsp;</p><p>据外媒报道，投资管理公司联博（Bernstein）报告估算，Google 每年向苹果支付 180 亿至 200 亿美元，以保持其是 iPhone 等产品的默认搜索引擎，占苹果年度营业利润的 14-16%。</p><p>&nbsp;</p><p>Bernstein 在报告中表示，“联邦法院有可能做出对 Google 不利的裁决，并迫使其终止与苹果的搜索协议。”Bernstein 称 Google 将 22% 的广告营收投入流量获取成本（TAC，Traffic Acquisition Costs），而苹果公司在其中的占比达到 40% 左右。据此前报道，Google 每年向苹果支付数十亿美元，以便于在苹果设备上，将其设置为默认搜索引擎。本次审理大部分都是闭门进行的，官方并未披露 Google 向苹果支付了多少金额。</p><p>&nbsp;</p><p></p><h4>华为5.5G手机或明年上半年商用</h4><p></p><p>10月11日，据媒体报道，华为相关人士透露，最早今年底，各大手机厂商旗舰手机将达到5.5G的网速标准，下行速率将达5Gbps，上行速率将达500Mbps，真正的5.5G手机可能要到2024上半年到来。此外，从产业链相关人士处获悉，目前手机大厂已对5G-A（5G-Advanced / 5.5G）芯片在能力验证中，有望在2024年上半年商用。</p><p>&nbsp;</p><p>10日在全球移动宽带论坛（MBBF2023）上，华为宣布将在2024年推出面向商用的端到端5.5G全套网络设备。同时，华为轮值董事长胡厚崑表示，正努力将5G-A带进现实。大模型、ChatGPT、自动驾驶需求持续增长，对网络持续演进提出要求，而5.5G便是为未来所做的准备。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>美国欲打算管制开源的RISC-V，担心中国借此发展自己的半导体产业</h4><p></p><p>&nbsp;</p><p>近日，美国正试图开辟“科技战”的“新战线”，包括两名共和党众议院委员会主席、共和党参议员Marco Rubio和民主党参议员Mark Warner在内的一些美国议员，正要求美国政府限制美国企业参与合作研发在中国广泛使用的RISC-V开源技术。此举可能会颠覆全球科技行业的跨境合作方式。</p><p>&nbsp;</p><p>上述美国政客表示，他们担心中国正在利用美国企业之间开放合作的文化来发展自己的半导体产业，这可能会削弱美国目前在芯片领域的领先地位。这些意见是对美国公司在RISC-V方面工作施加限制的首次重大举动。</p><p>&nbsp;</p><p>美国众议院中国问题特别委员会主席Mike Gallagher在给路透社的一份声明中表示，美国商务部需要“要求任何美国个人或企业在与中国实体就RISC-V技术进行合作之前获得出口许可证”。</p><p>&nbsp;</p><p></p><h4>北京应届毕业生招聘月薪超1.3万元居全国首位，人工智能领跑行业</h4><p></p><p>10月11日，猎聘大数据研究院发布《全国高校毕业生就业趋势与展望2023》显示，北京应届生招聘月薪全国居首。从城市对应的2023届应届生平均招聘月薪看，北京以13283元位居第一；深圳、上海分别以12783元、12317元位居第二、第三。</p><p>&nbsp;</p><p>从各细分行业2023届应届生新发职位平均招聘月薪看，人工智能以18592元位居第一；区块链、养老服务、航空/航天设备分别以17467元、16992元、16042元位居第二至第四。</p><p>&nbsp;</p><p></p><h4>curl 项目披露高危漏洞</h4><p></p><p>&nbsp;</p><p>curl 项目发布了 curl 8.4.0，其中修复了一个高危漏洞 CVE-2023-38545，该漏洞被认为是至今 curl 项目发现的最严重漏洞之一。curl 作者 Daniel Stenberg 在个人博客上详细解释了这一漏洞。攻击者可通过发送长度超过 16kB 的主机名触发该漏洞，导致堆缓冲区溢出。Stenberg 表示，如果 curl 是用内存安全语言 aka Rust 开发的话那么该问题不会发生，但重写 curl 不在他的议程中。他说可能会支持用 Rust 写一些依赖项目，逐步取代部分功能，但在可预见的未来，curl 仍然是用 C 编写的。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5LWEiT9DNPzCWsoqtdQE</id>
            <title>代码生成：基于AI大模型的挑战与前景</title>
            <link>https://www.infoq.cn/article/5LWEiT9DNPzCWsoqtdQE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5LWEiT9DNPzCWsoqtdQE</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 模型, 代码生成, 人工智能, 专业模型
<br>
<br>
总结: 使用 AI 通用模型生成代码可能会导致代码质量问题，因此需要创建专业或专用的模型来解决这个问题。人工智能模型是通过模仿人脑中神经元与突触连接而成的网络构建的。计算机并不会思考，只是利用统计数据对事物进行预测、分类或组合。大语言模型的工作原理是根据统计数据预测下一个标记的最佳匹配，无法对事实进行核查。使用通用模型生成代码可能会导致混杂不同版本代码的问题。AI 可以是解决问题的好帮手，但使用 AI 工具需要检查、验证、修改、编辑或重写部分内容。 </div>
                        <hr>
                    
                    <p>使用 AI 通用模型来完成代码生成这类非常具体的任务可能会带来问题。人工智能生成的代码就像是陌生人的代码，它们可能并不符合你的代码质量标准。这种情况下，创建专业或专用的模型不失为一条出路。</p><p>&nbsp;</p><p>Luise Freese 和 Iona Varga 在<a href="https://ndcoslo.com/">2023</a>" 年的&nbsp;<a href="https://ndcoslo.com/">NDC Oslo</a>" 大会上探讨了 AI 模型的实践困境和伦理相关问题。</p><p>&nbsp;</p><p>Varga 提到，“人工智能”这个词给人一种智慧的感觉，虽然这个名字实际只是代表了这些模型的构建方式。以节点相连的形式模仿人脑中神经元与突触连接而成的网络，这类模型因此而得名“人工网络”或“人工智能”。</p><p>&nbsp;</p><p>Freese 补充道，抽象来说，计算机是完全依赖于或开或关的晶体管，通过这些开关的组合，我们得以操纵比特。由于晶体管之间没有相互的纠缠，这些开关最终会带来这样的结果：</p><p></p><p></p><blockquote>因此，计算机并不会思考，不过是我们的人工智能算法赋予了它们个性和特征，比如“让我考虑一下”这类礼貌说辞。AI 仅仅是利用统计数据对事物进行预测、分类或组合。</blockquote><p></p><p>&nbsp;</p><p>Varga 提到，AI 的问题在与使用极其通用的模型或是基础模型完成非常具体的任务。大语言模型（LLM）的工作原理是先分析问题、创建一两个词语，再根据统计数据预测下一个标记的最佳匹配。此外，LLM 本身是无法对事实进行核查的，因为这类模型的设计目的是生成而非验证。</p><p>&nbsp;</p><p>如果我们试图建立一个能解决所有 AI 问题的 AI 模型，那么我们将会创造出一种自我放大的螺旋式下降，Freese 补充道。若想实现螺旋式上升，那就应该少用基础模型，多用更为具体的模型，后者中有一部分实际就是搭建在基础模型之上的。</p><p>&nbsp;</p><p>AI 或许能生成代码，但这些代码是否能安全地使用，是否能满足我们对质量的标准要求？Varga 认为这些问题只能由真正的人类来回答，这一过程并不容小觑。归根结底，就像是代码的编写一样，调试陌生人的代码远比自己从头到尾参与其中的代码更为困难。</p><p>&nbsp;</p><p>一般模型的理解能力也更为通用，这在代码生成问题上可能会带来问题，正如 Varga 所解释的：</p><p></p><blockquote>举例来说，React v17 或 v16 这些可能没有直接反应在模型的上下文中，但模型也能了解这些代码库。或许你会发现自己生成的一个函数中会混杂有两个版本的代码。</blockquote><p></p><p>Varga 认为，多数情况下 AI 都是解决问题的好帮手。但使用 AI 就意味着你要去检查、验证、修改、编辑或重写部分内容，而这一部分可能才是我们低估 AI 工具带来工作量的地方。</p><p>&nbsp;</p><p>InfoQ 针对人工智能所带来的挑战问题采访了 <a href="https://www.linkedin.com/in/luisefreese/">Luise Freese</a>"&nbsp;和 <a href="https://www.linkedin.com/in/iona-dahlia/">Iona Varga</a>"。</p><p>&nbsp;</p><p>InfoQ：什么因素会造成 AI 的失败？</p><p></p><p></p><blockquote>Iona Varga：一般来说，AI 并不是命中注定要失败的。我是医学物理出身的，我也见过很多优秀的 AI 工具，它们能出色地完成波弹性成像的实时剪切，早期阶段的婴儿检测，甚至能检测出肿瘤专家都无法发现的肺癌细小结节。&nbsp;但由于虚假数据和扭曲事实问题的存在，这些结果并不完全可信。举例来说，川普就职典礼上，实际的到场人数是要少于最初公布的数据。试着问模型就职典礼的公园有多热闹，你大概会得到一个出乎意料的答案。但同样，数据的来源时至今日也有颇具争议的历史背景，它们可能会出于政治剧本或标准等原因而被修改。</blockquote><p></p><p></p><p>InfoQ：伦理道德如何才能帮助我们解决 AI 所带来的问题？</p><p></p><p></p><blockquote>Luise Freese：伦理道德作为工具本身是帮不上太多忙的。伦理只是一种工作的方式，就像是 DevOps 一样。一旦你有了规划，知道该做什么了，“伦理道德”就是你对“完成”的定义。我所用的数据是否覆盖了所有产品使用相关的人或事？通过这些道德的检测，我们的工作方式将会在可访问性、包容性和避免偏见方面得到改善。</blockquote><p></p><p>&nbsp;</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/producing-quality-code-AI/">The Challenges of Producing Quality Code When Using AI-Based Generalistic Models</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fVBFXyE1yTvLR6RlqATA</id>
            <title>美的供应商回应员工自愿放弃公积金；23岁斯坦福博士生修复火狐浏览器22年陈旧bug；高通拟在加州裁逾1200人｜AI一周资讯</title>
            <link>https://www.infoq.cn/article/fVBFXyE1yTvLR6RlqATA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fVBFXyE1yTvLR6RlqATA</guid>
            <pubDate></pubDate>
            <updated>Sun, 15 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 资讯, 晶讯光电, 住房公积金, 斯坦福博士生, Firefox浏览器
<br>
<br>
总结: 晶讯光电公司被曝未给员工缴纳住房公积金和失业保险，离职率较高；23岁的斯坦福博士生修复了Firefox浏览器22年的陈旧bug；AMD宣布收购开源AI软件公司Nod.ai以增强软件能力；高通计划裁员以应对需求低迷；滴滴计划在2024年在香港上市并回购员工股票；百度训练文心大模型4.0，推理成本翻了10倍；OpenAI的GPT-4存在安全漏洞，使用不常见语言可以绕过限制。 </div>
                        <hr>
                    
                    <p></p><h2>资讯</h2><p></p><p></p><h4>美的供应商回应员工自愿放弃公积金</h4><p></p><p>&nbsp;</p><p>近日，卖液晶显示产品的湖南晶讯光电股份有限公司（下称“晶讯光电”）被曝已经向深交所主板递交了《招股书》，开启IPO进程。</p><p>&nbsp;</p><p>晶讯光电一度为美的、格力、卡西欧、松下、乐心医疗等国内外知名品牌企业供货，但公司却有25.68%的员工未缴纳住房公积金，离职率也是达到30%左右。</p><p>&nbsp;</p><p>据媒体报道，2023年上半年，晶讯光电未给25.68%的员工缴纳住房公积金。这个数据在2020年更是超过员工半数，为68.74%。2021年、2022年逐渐下滑，分别是37.95%和25.66%。</p><p>&nbsp;</p><p>2020年-2022年这三年，未缴纳住房公积金的员工人数分别为1533人、858人和547人。</p><p>除公积金外，2020年-2022年，以及2023年上半年，晶讯光电未给员工缴纳失业保险的占比分别为65.61%、36.18%、24.44%和24.55%。</p><p>&nbsp;</p><p>对此，公司解释了两方面的原因，一是部分为退休返聘员工，无需缴纳社保及公积金。二是部分员工因个人原因自愿放弃缴纳。在未缴纳社保、公积金员工中，大部分员工均已签署承诺函自愿放弃缴纳社保及公积金。</p><p></p><h4>23岁斯坦福博士生修复Firefox浏览器22年陈旧bug</h4><p></p><p>&nbsp;</p><p>据IT之家10月12日消息，现年23岁的斯坦福大学一年级电机工程博士生朱一凡（Yifan Zhu，音译），首次向开源项目贡献补丁，修复了 Firefox 浏览器存在 22 年历史的工具栏鼠标提示（tooltip）bug。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a287cec16c5dfc1842a3779410e5885e.png" /></p><p></p><p>T之家注：这个 BUG 存在于 Firefox 浏览器的工具栏中，鼠标悬浮到工具栏图标之后，会跳出相关的提示。</p><p>&nbsp;</p><p>此时将浏览器从前台切换到后台，该鼠标提示会仍然留在前台。摆脱这一恼人提示的唯一方法是再次将浏览器从后台切换到前台，然后移动鼠标。</p><p>&nbsp;</p><p>朱一凡出生于 1999 年，在 Linux 上使用邮件客户端 Thunderbird 时首次遭遇该 bug，认为这个 bug 太恼人了。他试着报告该 bug，结果发现它已经存在了 22 年之久，至今还没有修复。</p><p>&nbsp;</p><p>由于这个问题非常小，至今没有人修复，于是他决定自己来修复，他表示自己只是在整个代码库里搜索 tooltip，检查候选内容，插入调试打印语句跟踪执行，然后添加计时器来解决这个问题，在鼠标移出事件后计时器将会取消。</p><p></p><h4>AMD宣布收购开源AI软件公司Nod.ai</h4><p></p><p>&nbsp;</p><p>当地时间10月10日，AMD宣布将收购加州人工智能软件初创公司Nod.ai，以强化公司的软件能力。</p><p>&nbsp;</p><p>AMD人工智能集团高级副总裁Vamsi Boppana对此表示：“收购Nod.ai将显著增强我们为人工智能客户提供开放软件的能力，使他们能轻松部署针对AMD硬件进行优化的高性能人工智能模型。”</p><p>&nbsp;</p><p>为了追赶竞争对手英伟达，AMD计划大举投资于公司人工智能芯片所需的关键软件。当前，英伟达通过十多年的努力，已凭借其软件和软件开发者生态系统，在人工智能芯片市场建立起强大的优势。</p><p>&nbsp;</p><p>AMD之前曾表示，要投资并建立一个统一的软件集合，为公司生产的各种芯片提供动力。AMD总裁Victor Peng表示：“AMD将通过内部投资和外部收购来做到这一点。”</p><p>&nbsp;</p><p>而收购Nod.ai正符合这一战略，因为它的技术能使其他公司轻松地地部署针对AMD芯片进行优化的人工智能模型。</p><p>&nbsp;</p><p>AMD并没有透露这笔交易的条款，包括收购金额。有数据显示，Nod.ai在此之前已融资约3650万美元。</p><p></p><h4>高通拟在加州裁逾1200人以降成本</h4><p></p><p></p><p>财联社消息，全球最大的智能手机芯片制造商高通公司（Qualcomm Inc.）正在裁员，以应对其主要产品需求低迷的局面。</p><p>&nbsp;</p><p>根据提交给加州就业发展部的文件，该公司将在加州圣地亚哥和圣克拉拉裁减1258个职位。高通的一名代表拒绝就此次裁员的总体规模置评，该公司目前共有5万名左右的员工。</p><p>&nbsp;</p><p>被裁撤的职位中，有750多个来自高通的工程部门，包括从主管到技术人员的级别。其余的裁员将来自内部技术人员和会计等职位。高通在通知中表示，裁员将于12月中旬开始。</p><p></p><h4>消息称滴滴将于2024年在香港上市 开始回购员工股票</h4><p></p><p></p><p>10月13日，有媒体报道，滴滴出行计划于2024年在中国香港交易所上市，并开始回购员工股票。</p><p>&nbsp;</p><p>该知情人士表示，滴滴已在近期通知现任员工，允许他们根据员工持股计划将自己的股票卖回给公司，这被视为该公司准备在香港上市的一部分。据几名前滴滴员工透露，滴滴股票的180天禁售期此前被延长，一些滴滴员工在2022年初的一个短暂窗口期间出售了股票期权。与此同时，软银集团等主要投资者或许能够通过滴滴重新上市弥补部分损失。软银预计向滴滴投资了大约110亿美元，目前持有20%的滴滴股份，价值约32亿美元。截至发稿，滴滴尚未就此置评。软银不予置评。</p><p></p><h4>传百度文心4.0推理成本翻10倍</h4><p></p><p></p><p>近日，有媒体报道称，百度正加紧训练文心大模型4.0，这将是文心大模型3.5版本后又一个重磅版本。据报道，文心大模型4.0进展比预期快很多，将是基础模型的大升级，理解、生成、逻辑、记忆核心能力都将提升，特别是在逻辑推理、代码和数学等方面提升最明显。</p><p>&nbsp;</p><p>10月10日，记者从百度内部人士基本确认了该消息，据悉，文心大模型4.0的推理成本相比文心大模型3.5增加10倍。此外，文心大模型4.0的参数规模也将更大，预计突破万亿级别。</p><p></p><h4>OpenAI安全漏洞曝光，使用不常见语言可轻易绕过ChatGPT的限制</h4><p></p><p></p><p>布朗大学的计算机科学研究人员发现了 OpenAI 的 GPT-4 安全设置中的新漏洞。他们利用一些不太常见的语言，如祖鲁语和盖尔语，即可以绕过 GPT-4 的各种限制。研究人员使用这些语言来写通常受限的提示词（prompt），发现得到回答的成功率为 79%，而仅使用英语的成功率不到 1%。</p><p>&nbsp;</p><p>研究人员承认发布这项研究可能会造成危害，并给网络犯罪分子提供灵感。值得一提的是，在向公众发布之前，该研究团队已经与 OpenAI 分享了他们的发现，以减轻这些风险。</p><p>&nbsp;</p><p></p><h2>IT业界热评新闻</h2><p></p><p></p><h4>微软计划在未来的Windows版本中取消VBScript</h4><p></p><p>&nbsp;</p><p>VBScript 是一种活跃的脚本语言，自 Windows 98、Windows NT 4.0 Option Pack 和 Windows CE 以来一直是 Windows 历史的一部分。 现在，在上市 25 年后，该语言及其主机环境已被 Microsoft 正式淘汰。</p><p>&nbsp;</p><p>微软于 2022 年 6 月淘汰了久负盛名的 IE 浏览器。现在，Windows 客户端系统中已弃用功能的官方列表的最新更新表明 VBScript 也将被弃用。 更新的页面指出，在未来的 Windows 版本中，VBScript 将仅作为根据用户输入安装的可选“按需功能”提供，再过几年，该功能将从操作系统中完全删除。</p><p>&nbsp;</p><p>正如微软官方解释的那样，按需功能（FOD）是可以在初始安装后随时添加到操作系统的 Windows 功能。 常见的 FOD 选项包括用于手写识别的语言资源、较旧的 .NET Framework (.NetFx3) 软件包、适用于 Linux 的 Windows 子系统，甚至称为 Hyper-V 的 Windows Type-1（本机）管理程序。</p><p>&nbsp;</p><p>VBScript 正式加入了一个不断增长的历史 Windows 功能列表，微软出于未指明的（有时是可疑的）原因决定取消这些功能。 饱受诟病的 Internet Explorer 浏览器已不复存在（在 Windows 11 中），写字板（首次包含在 Windows 95 中）也将很快被弃用。</p><p></p><h4>马斯克回应欧盟指责：X平台所有内容都是开源且透明的</h4><p></p><p></p><p>10月11日消息，欧洲监管机构已经向社交媒体平台X老板埃隆·马斯克Elon Musk发出严厉警告，称在以色列-哈马斯冲突期间，非法内容和虚假信息在X(原名推特)上传播。如果不遵守欧洲有关非法内容的规定，可能会被处以相当于该公司年收入6%的罚款。</p><p>&nbsp;</p><p>欧盟内部市场专员蒂埃里·布雷顿(Thierry Breton)周二在给马斯克的一封信中表示，其办公室发现的“迹象”表明，有些团体正在X上传播错误信息、“暴力及恐怖主义”内容，并敦促这位亿万富翁在24小时内做出回应。</p><p>&nbsp;</p><p>马斯克则回应Thierry Breton称，X平台的政策是，所有内容都是开源且透明的。</p><p>&nbsp;</p><p>在这封信发出之前，许多研究人员、新闻机构和其他组织都发现，X上的误导性、虚假和可疑内容大量增加，让人们对当前的冲突感到困惑。</p><p>&nbsp;</p><p>布雷顿通过X帖子分享了这封信，并给马斯克的账号加上了标签，其中一个标签提到了《数字服务法案》(Digital Services Act)，这是欧盟委员会新颁布的立法，要求在欧盟拥有超过4500万月活跃用户的平台监控和删除非法内容。</p><p>&nbsp;</p><p>布雷顿在信中提醒马斯克，DSA“就内容审核做出了非常明确的规定”，而X需要“让你们的政策条款保持清晰透明，包括允许发布哪些内容，并始终不懈地执行你们的政策。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/di3AupUdhAV9sxmWG6kh</id>
            <title>大模型部署昂贵的原因：用最贵的模型处理最基本任务，犹如“让兰博基尼送披萨”</title>
            <link>https://www.infoq.cn/article/di3AupUdhAV9sxmWG6kh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/di3AupUdhAV9sxmWG6kh</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Oct 2023 06:17:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 开发者, AI产品, 开发成本
<br>
<br>
总结: OpenAI计划推出新功能吸引开发者，降低开发成本，以便更多企业和开发者使用其AI产品。新功能包括内存存储和视觉功能等工具，使开发人员能够更便宜、更快速地构建基于AI模型的应用程序。OpenAI的年收入已突破13亿美元，但要解决开发和运行大语言模型的高计算成本问题。尽管如此，OpenAI的持续增长凸显了大型语言模型的潜力，同时也暴露了科技巨头在AI盈利方面的困境。 </div>
                        <hr>
                    
                    <p></p><h2>OpenAI计划推出新功能吸引开发者，称开发成本最高降低20倍</h2><p></p><p>近日，路透社援引消息人士称，为了吸引更多企业和开发者更多地使用其技术，OpenAI 计划下个月为旗下的AI产品推出重大更新，以便开发者们能够更便宜、更快速地构建基于其AI模型的软件应用程序。</p><p>&nbsp;</p><p>这些更新包括在其开发工具中添加内存存储。理论上，这可以将应用程序制造商的开发成本削减多达20倍，解决合作伙伴们对于价格的担忧。</p><p>&nbsp;</p><p>此外，OpenAI 还计划推出视觉功能等新工具，使开发人员能够构建具有分析图像和描述图像能力的应用程序，并希望将这些技术应用于娱乐、医学等众多领域。为开发人员提供这个工具也标志着 OpenAI 在推出多模态功能这条路上迈出了重要一步，该功能可以处理和生成除文本之外的不同类型的媒体，例如图像、音频和视频。</p><p>&nbsp;</p><p>消息人士称，这些新功能预计将于11月6日在旧金山举行的 OpenAI 首届开发者大会上推出。</p><p>&nbsp;</p><p>对于上述消息，OpenAI 拒绝置评。</p><p>&nbsp;</p><p>一直以来，让OpenAI成为其他公司构建应用程序所不可或缺的元素，是公司CEO Altman 最重要的战略目标之一，但最近该公司在吸引外部人士利用其技术开展业务方面面临着一些挑战。</p><p>&nbsp;</p><p>今年早些时候，OpenAI匆忙发布了ChatGPT插件Scholar AI，这是允许开发人员在ChatGPT内创建应用程序的附加工具。OpenAI 希望插件能够像苹果的iOS应用商店一样受欢迎，从而获得比谷歌 Bard 等竞争对手更大的优势。</p><p>&nbsp;</p><p>但这款插件被不少开发者视为一场“作秀”，并没有砸起多少水花。据该插件的开发者 Lakshya Bakshi统计，截至8月底，Scholar AI插件每天仅有约7000名用户，而ChatGPT每月吸引约 1.8 亿活跃用户。</p><p>&nbsp;</p><p>Altman公开承认还有更多工作要做。今年早些时候，Altman 在伦敦向一群开发人员承认，插件尚未获得市场关注。</p><p>&nbsp;</p><p>此外，Altman还亲自与一些开发者交谈，表达了他希望基于 OpenAI 模型构建新生态系统的愿望，虽然其模型现已融入从 DoorDash 到写作助手 Jasper 等无数应用程序中，但距离Altman的预期还有一段距离。</p><p>&nbsp;</p><p></p><h2>年收入已突破13亿美元，OpenAI即将盈利了？</h2><p></p><p>&nbsp;</p><p>在忙着让构建OpenAI 模型新生态之余，Altman对于OpenAI的营收能力也十分关注。据The Information报道，Altman本周告诉员工，OpenAI的年收入现已突破13亿美元。这意味着该公司每月的收入超过1亿美元，比去年夏天增长了30%。</p><p>&nbsp;</p><p>值得注意的是，OpenAI 2022年全年的总收入仅为2800万美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6486ace71eb2c97bc5f647ed0ccacf8.png" /></p><p></p><p>自从二月份推出付费版本的 ChatGPT 以来，OpenAI 的财务增长可谓飞速增长。此外，该公司还于8月宣布推出&nbsp;ChatGPT Enterprise，这是其面向商业用户的流行对话式 AI 聊天机器人的商业版本。</p><p>&nbsp;</p><p>也许单独来看，作为一家聚光灯下的人工智能独角兽企业，OpenAI的收入并不算高，但如果对比风头最接近OpenAI的竞争对手Anthropic的收入来看，OpenAI如今13亿美元的年收入还是比较有说服力的。</p><p>&nbsp;</p><p>上周，据外媒报道称，Anthropic 正寻求再融资20亿美元，估值为20至300亿美元。然而，Anthropic公司的年化收入仅达到1亿美元，即每月约800万美元。</p><p>&nbsp;</p><p>虽然两家公司都提供同类型的产品，但 ChatGPT 的市场成功目前已经推动 OpenAI 遥遥领先。微软、Stripe、沃尔沃和宜家等大型企业已经在使用 OpenAI 的大语言模型产品构建自家应用。</p><p>&nbsp;</p><p>收入的大幅增长可能会在即将到来的要约收购中推高 OpenAI 的私人估值。据《华尔街日报》报道，该公司的总估值可能很快就会达到令人瞠目的 80 至 900 亿美元。</p><p>&nbsp;</p><p>目前，尽管来自谷歌和 Anthropic 的竞争不断涌现，OpenAI 似乎仍将保持势头。但维持长期增长可能需要解决开发和运行大语言模型的高计算成本问题。</p><p>&nbsp;</p><p>尽管如此，对于一家去年仅产生2800万美元收入的公司来说，OpenAI 在短短几个月内收入激增至 13 亿美元，这已经是一个巨大的成功故事。这家初创公司的持续增长凸显了大型语言模型的颠覆性潜力。</p><p>&nbsp;</p><p>此外，大模型的潜力还体现在与云基础设施成本对比上的优势。</p><p>&nbsp;</p><p>国外的一家大语言模型团队最近一直在使用GPT微调​​API进行实验。他们指出，GPT-3.5 上的一次微调运行成本为 4～12 美元，并且需要大约 1～1.5 小时才能微调超过 100 万个tokens。</p><p>&nbsp;</p><p>与此同时，AWS 上单个p4d.24xlarge按需收费为每小时 32.77 美元，如果预订 1 年则为每小时 19.22 美元。每台机器都配备 8 个 Nvidia A100 GPU。假设 OpenAI 仅使用 8 个 GPU 来微调 GPT-3.5，那么使用 OpenAI 比从 Amazon 租用 GPU 便宜 3-8 倍，甚至无需具备在云上部署和运行作业所需的技术专业知识。</p><p>&nbsp;</p><p>可见，大模型提供商的优势不仅在于模型的质量，还在于他们以极端规模成本优势提供模型服务的能力。</p><p>&nbsp;</p><p></p><h2>风光背后，科技巨头也深陷AI盈利难困局</h2><p></p><p>&nbsp;</p><p>然而，虽然大模型有着诸多方面的优势，但想依靠大模型盈利在现阶段却并非容易事。</p><p>&nbsp;</p><p>据《华尔街日报》报道，微软和谷歌等大型科技公司正在努力应对将 ChatGPT 等人工智能产品转变为盈利企业的挑战。尽管公司大力投资可以生成业务备忘录或代码的AI技术，但运行高级AI模型的成本被证明是一个重大障碍。某些服务（例如 Microsoft 的 GitHub Copilot）会造成重大运营损失。</p><p>&nbsp;</p><p>用于创建文本的生成式人工智能模型的运行成本并不便宜。像为ChatGPT提供支持的大型语言模型需要配备高端、耗能芯片的强大服务器。例如，路透社的一份报告指出，每个 ChatGPT 查询的运行成本可能为 4 美分。因此，AWS首席执行官 Adam Selipsky 向《华尔街日报》表示，许多企业客户对这些AI模型的高运行成本感到不满。</p><p>&nbsp;</p><p>当前的成本挑战与AI计算的性质有关，与享有规模经济的标准软件不同，AI计算通常需要为每个查询进行新的计算。这使得AI服务的固定费用模式存在风险，因为增加客户使用量可能会增加运营成本并导致公司潜在损失。</p><p>&nbsp;</p><p>一些公司正在努力降低成本，而另一些公司则继续加大对技术的投资。微软和谷歌已对其现有软件服务引入了更昂贵的AI支持的升级，而据报道，Zoom 试图通过有时使用不太复杂的内部人AI型来执行某些任务来降低成本。Adobe 正在通过活动上限和根据使用情况收费来解决这个问题，而微软和谷歌通常坚持固定费用。</p><p>&nbsp;</p><p>微软企业战略主管克里斯·杨（Chris Young）认为，在人们找出AI的最佳使用方式之前，AI的投资回报将需要更多时间。他告诉媒体：“显然，我们现在必须将用户的兴趣转化为真正的采用。”</p><p>&nbsp;</p><p>值得注意的是，《华尔街日报》的报道称，微软的GitHub Copilot通过生成代码来帮助应用程序开发人员，尽管吸引了超过 150 万用户并集成了近一半的编码项目，但该公司一直处于亏损状态。据一位知情人士透露，用户每月为该服务支付 10 美元的固定费用，但微软为每个用户每月平均支付的费用超过 20 美元。在某些情况下，个人高级用户每月给公司带来的费用高达 80 美元。</p><p>&nbsp;</p><p>AI服务如此昂贵的原因之一是一些公司一直在寻求最强大的AI模型。例如，微软使用 OpenAI 最复杂的GPT-4来实现其许多 AI 功能。GPT-4 是最大且最昂贵的AI模型之一，需要大量的算力。《华尔街日报》打趣道，使用该模型执行总结电子邮件等基本任务就像“让兰博基尼送披萨”，这表明使用最强大的人工智能模型来完成简单的任务可能有些过头了。</p><p>&nbsp;</p><p>沿着这些思路，微软一直在为其 Bing Chat 搜索引擎助手探索成本更低的替代方案，包括 Meta 的Llama 2语言模型。然而，随着时间的推移，由于AI加速硬件的进步，运行这些复杂模型的成本可能会下降。但这段时间到底是多久，谁都无法确定。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.reuters.com/technology/openai-plans-major-updates-lure-developers-with-lower-costs-sources-2023-10-11/">https://www.reuters.com/technology/openai-plans-major-updates-lure-developers-with-lower-costs-sources-2023-10-11/</a>"</p><p>&nbsp;</p><p><a href="https://generatingconversation.substack.com/p/openai-is-too-cheap-to-beat">https://generatingconversation.substack.com/p/openai-is-too-cheap-to-beat</a>"</p><p>&nbsp;</p><p><a href="https://arstechnica.com/information-technology/2023/10/so-far-ai-hasnt-been-profitable-for-big-tech/">https://arstechnica.com/information-technology/2023/10/so-far-ai-hasnt-been-profitable-for-big-tech/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/M9GWVN8LPyGFQOSfVmu4</id>
            <title>百度世界2023剧透丨百度将发布国内首个生成式商业智能产品</title>
            <link>https://www.infoq.cn/article/M9GWVN8LPyGFQOSfVmu4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/M9GWVN8LPyGFQOSfVmu4</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Oct 2023 01:43:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型时代, 应用机会, 百度GBI, 百度网盘
<br>
<br>
总结: 百度创始人李彦宏认为，在大模型时代，创业者应该关注应用机会而不是基础服务或行业服务。百度在近期公开的18条“创业军规”中表示，卷大模型没意义，卷应用机会更大。百度将发布国内首个生成式商业智能产品百度GBI，该产品可以将数据分析的时间从以天为单位缩短到以分钟为单位。此外，百度还在大模型重构的基础上升级了百度网盘和智能工作平台如流，提供更智能的知识管理和办公服务。 </div>
                        <hr>
                    
                    <p>大模型时代，最大的发展机会在哪里？基础服务还是行业服务？百度创始人、董事长兼首席执行官李彦宏认为都不是，而是在应用。在近期公开的18条“创业军规”中，李彦宏说，对创业者来说，卷大模型没意义，卷应用机会更大。百度要做第一个把全部产品“重构”一遍的公司。</p><p>&nbsp;</p><p>10月12日，百度举办“百度世界2023媒体预沟通会”，披露了网盘、智能工作平台如流、Apollo智舱等产品基于大模型重构的最新进展。此外，据了解，在10月17日召开的“百度世界2023”上，百度还将发布国内首个生成式商业智能产品——百度GBI。</p><p></p><h2>国内首个生成式商业智能产品</h2><p></p><p></p><p>商业世界里，商场如战场，企业竞争，不是大鱼吃小鱼，而是快鱼吃慢鱼。企业要想获胜，最离不开的就是商业分析。但传统BI工具，数据发现难、工具使用难、指标覆盖有限，无论是Excel还是数据分析平台，都为高频（预设）场景设计，无法灵活地随时应对各种问题。一个复杂问题的分析洞察，往往需要数小时或数天才能完成。在市场竞争激烈，瞬息万变的情况下，这样长的决策周期，有可能失去重要的商机。</p><p>&nbsp;</p><p>百度智能云技术委员会主席、百度智能云应用产品中心总架构师孙珂表示，即将在10月17日发布的百度GBI可以把数据分析，从以天为单位，缩短到以分钟为单位。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/3d/da/3d5aee38e3c003d686281681e2444cda.png" /></p><p></p><p>首先，传统BI只有专业人士才能操作，而GBI能直接听懂总裁问题，实时执行，快速得出结论。其次，GBI提供了便捷的接入方式，企业可以接入数据，对任意数据提问、分析，而不再需要人工去跨数据库、跨表格分析。第三，GBI还具备学习能力，企业可注入本行业专业知识，让它成为行业专家。</p><p></p><h2>百度网盘再升级，视频里找东西提炼金句样样通</h2><p></p><p></p><p>“AI时代的网盘，已经不再聚焦文件中转或存储，”百度智能云网盘产品部总经理吴天昊表示，“而是进一步迈向个人与企业的知识管理，实现让信息从数据到知识的转变。”</p><p>&nbsp;</p><p>过去11年，百度网盘为8亿用户服务，每一天用户会上传超过10亿张图片。所以，百度网盘在AI重构的方向上，重点就是做好个人文件的智能服务。</p><p>&nbsp;</p><p>“云一朵”就是百度基于文心大模型打造的国内首个网盘智能助理。它不仅实现了从图形界面交互到自然语言交互的转变，还增强了多模态信息理解。用户只需要一句话，就能对网盘内的文件、图片、视频等进行操作，方便用户在网盘里、视频里“找东西”。值得一提的是，百度网盘“云一朵”还可以帮助用户快速了解视频内容，可以从视频里提炼内容重点、为视频添加字幕、将全部字幕导出文稿、甚至添加文稿标题，极大地提高信息理解和传播效率。</p><p>&nbsp;</p><p>近期，百度网盘还推出了微信端可使用的“云一朵文件助手”，转发任意公众号文章给云一朵，它就能直接“阅读”，并且在10秒内总结、提炼要点，让用户在短时间内获取“精华干货”。</p><p>&nbsp;</p><p>“百度网盘云一朵强大的视频处理、理解能力也将在百度世界大会展示给大家，敬请期待，”吴天昊说。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/68/68d3a3b5ce88eeb553ce557a1eb55da4.png" /></p><p></p><h2>打工人福音，人手一个懂你、专业的如流“超级助理”</h2><p></p><p></p><p>当前，企业线上化办公仍存在诸多痛点，大模型将如何重塑智能工作？</p><p>&nbsp;</p><p>“大模型是企业办公领域的重大机遇。”百度智能办公平台部总监和为表示，“在文心一言加持下，百度智能工作平台如流基于AI原生思维重构智能工作，激发企业创新提效。”</p><p>&nbsp;</p><p>和为重点介绍了如流打造的“超级助理”，具备懂你、专业、实时陪伴三大特点，随时随处被唤起，目前已在丰富的办公应用场景应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/febb14ce9a3d949163af5bd36e1183c8.png" /></p><p></p><p>在智能任务执行时，超级助理通过自然语言语音唤起，对于预约会议、休假、差旅行程等场景，实现复杂系统一步直达；在智能文档处理场景中，超级助理根据指令，快速找出相关文档，知识获取效率倍增，还能在浏览器Web端快速查阅、总结、翻译文献资料；在高频沟通场景，“IM智能总结”和“AI会议洞察”“AI会议纪要”可以快速提炼要点，生成结构化纪要内容。数据显示，通过使用的AI会议洞察功能，目前会议内容阅读量增长3.5倍。</p><p>&nbsp;</p><p>和为认为，“不同于助手概念下更强的工具属性，超级助理能胜任更复杂的任务，同时会更主动的帮助你完成工作，让智能工作代替勤奋工作。”2023百度世界大会现场，重构后的如流新功能将重磅亮相。</p><p></p><h2>大模型重构智能座舱，将在极越01量产搭载</h2><p></p><p></p><p>大模型在重塑千行百业的同时，也在深刻影响着汽车产业。当大模型与汽车座舱相结合，汽车将成为具备EQ和IQ的汽车机器人。</p><p>&nbsp;</p><p>百度智能驾驶事业群组（IDG）智能汽车业务部总经理苏坦表示：大模型时代，基于重构的思路，我们有机会把汽车座舱中人和机器的关系变成人和虚拟人的关系。基于文心大模型作为基础模型，和百度Apollo在百万量级智能汽车和不同场景的大量数据积累，进一步增强出了Apollo智舱大模型和智舱开发工具链。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/6b/6b8fc211e0367feebb8b513216c526e3.png" /></p><p></p><p>大模型的智能涌现带来理解、生成、推理、记忆等核心能力，让智能座舱业态都将被重构，包括交互、开发模式、架构、用户运营模式等。首先被重构的是人车交互方式，从“命令式”升级到“对话式”，交互自然度会大幅提升。交互体验提升也将让车内导航、用车等刚需场景用户体验大幅提升，与此同时，车内娱乐、服务类长尾需求将进一步释放，用户的使用场景也将得到重构。百度Apollo为汽车座舱打造了专属大模型技术底座，以文心大模型为基础，增强了大模型在汽车座舱内效果，提供更拟人化的智能交互，包括舱内理解力提升、新增多模态理解、主动交互能力、动态回复能力、响应时间优化等能力建设，满足用户对座舱的智能化需求。</p><p>&nbsp;</p><p>为了让大模型赋能的智能座舱更快速的落地，百度Apollo也重构了汽车智能座舱的技术路线，以大模型为主的车载AI原生应用开发模式，以本地化为方向的车端深度结合。Apollo将智舱AI原生应用开发范式流程化、工具化，全链路降本提效，助力行业落地探索，汽车主机厂商可以自主开发自己品牌模型和应用场景，Apollo也将提供精品车载原生应用样板参考，针对座舱生态环境内置大量常用插件调度能力，大幅降低汽车主机厂商投入成本。</p><p>&nbsp;</p><p>目前，百度Apollo智舱大模型加持的车载语音产品已经在极越01、凯迪拉克锐歌、别克E5、吉利银河L7、吉利银河L6等车型中实现量产搭载，吉利银河、哈弗等品牌也即将搭载上线。其中，极越01基于大模型本地化的语音交互，在毫秒级响应、全时免唤醒交互、多路同时交互、全页面所见所说等核心能力，在业界量产车型中均处于领先水平。</p><p></p><h2>千帆大模型平台用户规模快速增长，覆盖400多场景</h2><p></p><p></p><p>百度正在用大模型重塑自己的产品，那么创业者和企业下一个问题是，该如何打造AI原生应用呢？</p><p>&nbsp;</p><p>百度智能云AI平台副总经理李景秋表示，百度已经把这些成功的实践经验，总结成工具和方法论，沉淀在全球首个一站式大模型服务平台“千帆”上，帮助创业者和企业低成本、高效率的打造自己的AI原生应用。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1b/1bc14ee020d413a075a19424abec1d78.png" /></p><p></p><p>百度智能云持续不断的完善和升级千帆平台上的工具链。目前，千帆平台上预置了41个数据集、226套高质量的Prompt模版，企业可以针对自己的业务场景快速优化模型效果。平台上还纳管了42个国内外主流大模型，提供中文增强、性能增强、上下文增强等能力。对于企业客户关注的性能保障问题，千帆平台提供了极致稳定的训练环境。常规方法下，工程师们有30%-40%时间都花在容错和故障恢复上。现在，百度智能云自研的集群组网故障管理机制，使模型有效训练时间达到95%以上。</p><p>&nbsp;</p><p>李景秋透露，千帆平台自3月以来，用户规模快速增长。截止9月初，百度智能云千帆平台上的月活企业数近万家，覆盖制造、能源、交通等多个行业的400多个场景。</p><p>&nbsp;</p><p>“围绕AI原生应用加速落地，10月17日的百度世界2023上，千帆平台还将发布更多新产品。”李景秋表示。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0LpSixGbHNp8TlHsosRf</id>
            <title>AutoGPT放弃向量数据库！向量数据库是小题大作的方案？</title>
            <link>https://www.infoq.cn/article/0LpSixGbHNp8TlHsosRf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0LpSixGbHNp8TlHsosRf</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Oct 2023 07:06:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式 AI, 向量数据库, AutoGPT, 存储模式
<br>
<br>
总结: AutoGPT作为一种新型的AI智能体，最初采用了向量数据库作为存储记忆的方式。然而，最近AutoGPT决定放弃向量数据库，改为使用JSON文件进行存储。这一决定引发了关于向量数据库是否有附加价值的讨论。有人认为向量数据库并不是必要的，而是一种过早优化。对于大模型应用是否需要使用向量数据库，取决于应用对于矢量存储与查询的依赖程度。 </div>
                        <hr>
                    
                    <p>生成式 AI 促进了向量数据库的火爆，但如今的技术风向变化似乎也挺快。作为全球最著名的AI项目之一，AutoGPT宣布不再使用向量数据库，这一决定可能让不少人感到惊讶。毕竟从一开始，向量数据库就一直协助管理着AI智能体的长期记忆。</p><p>&nbsp;</p><p>那么这个基本设计思路怎么就变了？又该由哪种新方案代替？对于大模型应用来说，矢量数据库是必要的吗？</p><p>&nbsp;</p><p></p><h2>事情发展</h2><p></p><p>&nbsp;</p><p>AutoGPT是今年3月30日发布的一种“AI agent（AI智能体）”，类似的还有LlamaIndex和LangChain。AutoGPT一发布就名声大噪，上线仅 7 天就在 GitHub 上获得了 44,000 颗星。相较于之前一遍又一遍向模型输入提示词的用法，它能够自行工作、规划任务、将问题拆分成多个较小的部分、再逐个加以执行。毫无疑问，这是个雄心勃勃的计划。</p><p>&nbsp;</p><p>AutoGPT的设计思路还涉及一种以嵌入形式管理智能体记忆的方法，外加一套用于存储记忆并在必要时检索的向量数据库。从当时的角度看，向量数据库被认为是整个解决方案当中最重要的组成部分。而且其他通用人工智能（AGI）项目也纷纷采取同样的方法，例如BabyAGI。</p><p>&nbsp;</p><p>之前在默认情况下，AutoGPT支持五种存储模式：</p><p>&nbsp;</p><p>LocalCache (will be renamed to JSONFileMemory)RedisMilvusPineconeWeaviate</p><p>&nbsp;</p><p>但现在查看AutoGPT的说明文档，我们会发现一条令人惊讶的警告消息：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e87796186466bed4a77744015a69ea3b.jpeg" /></p><p></p><p>&nbsp;</p><p>AutoGPT最近刚刚经历了“向量记忆改造”，其删除了所有向量数据库实现，包括Milvus、Pinecone、Weaviate，仅保留几个负责记忆管理的类。如今，JSON文件成为存储记忆/嵌入的默认方式。</p><p>&nbsp;</p><p></p><h2>原因是向量数据库没有附加价值？</h2><p></p><p>&nbsp;</p><p>其实，AutoGPT的维护者Reinier van der Leer于今年5月份就在GitHub上询问大家对“增加不同存储方式的价值”的看法，因为他们想进行重构，并打算放弃除“本地”内存提供程序（现在称为json_file）之外的所有东西，同时努力实现Redis VectorMemoryProvider。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/1179fb1d6efe123490a7691034086f86.jpeg" /></p><p></p><p>&nbsp;</p><p>有开发者对此表示赞同，认为如果后端足够好，那么没有理由保留这些向量数据库。“但我建议将pinecone（如果有优势的话，那也可以是redis）集成到自定义JSONFileMemory中。”</p><p>&nbsp;</p><p>当然也会有反对者，他们认为“向量数据库比当前的 JSON 文件内存系统更高效。它们是为此类任务而构建的，可以简化开发并减少token消耗。”Reinier对此进行了反驳，“这说法太笼统了，是否有例子或假设案例来证明这一点是正确的？”</p><p>&nbsp;</p><p>至于以后要不要恢复向量数据库，该开发团队表示这肯定不是当前的首要任务，况且他们也没发现向量数据库能带来什么特别的附加价值。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcbc2f96af2d599ec5be0e6d69606382.jpeg" /></p><p></p><p>&nbsp;</p><p>在开发内存系统时，我们要关注数据结构，而不是存储机制。使用具有 JSON 持久性是最简单的实现方法，为实验留出了空间。</p><p>&nbsp;</p><p>&nbsp;</p><p>为什么AutoGPT一开始采用但现在又放弃向量数据库？是向量数据库的价值问题还是架构设计问题？InfoQ询问了流数据库公司 RisingWave（risingwave.com）创始人 &amp;CEO 吴英骏，他认为更多的是设计决策上的事情：</p><p>&nbsp;</p><p></p><blockquote>AutoGPT最开始采用矢量数据库进行矢量存储与查询，相信单纯是为了快速打造产品原型，缩短开发周期。选用矢量数据库进行初代产品的开发可以更快得到高效可靠的矢量存储查询功能。而如今，AutoGPT选择“放弃”矢量数据库，多半也是发现运维与使用矢量数据库的代价已经超过了其带来的好处。在这种情况下，重新自己造轮子更符合项目发展的长远收益。毕竟，在软件开发过程中，过早优化会带来极高开发成本与风险，导致软件复杂度不可控。</blockquote><p></p><p>&nbsp;</p><p>这也正如AutoGPT项目维护者Reinier所言，AutoGPT支持多个向量数据库，确实会拖慢开发速度。那么像AutoGPT这样的大模型应采用向量数据库并不是必要的吗？对于长期记忆，我们还有其他选择？</p><p>&nbsp;</p><p></p><h2>该如何选型？</h2><p></p><p>&nbsp;</p><p>早在4月份，<a href="https://jina.ai/news/auto-gpt-unmasked-hype-hard-truths-production-pitfalls/">就有网友对AutoGPT最初的选择提出批评</a>"，认为向量数据库是种“小题大做的解决方案”。他的论证过程也很简单：</p><p>&nbsp;</p><p></p><blockquote>假设大语言模型需要10秒钟才能生成一条结果，即需要存储的单条新记忆。那么我们获得10万条记忆的时间周期将为：100000 x 10秒 = 1000000秒——约等于11.57天。而即使我们用最简单的暴力算法（Numpy的点查询），整个过程也只需要几秒钟时间，完全不值得进行优化！也就是说，我们就连近似最近邻搜索都不需要，更不用说向量数据库了。</blockquote><p></p><p>&nbsp;</p><p>那么我们应该如何为自己的项目选型？吴英骏老师认为，对于任何大模型应用，是否需要选用矢量数据库，完全取决于该应用对于矢量存储与查询的依赖程度。</p><p>&nbsp;</p><p>“对于需要存储大量矢量的场景，如海量图像检索、音视频检索等，很显然使用矢量数据库可以获得更加强大、专业的功能，而对于数据量并没有那么大的场景来说，还不如使用Numpy等Python库计算来的高效、便捷。实际上，在矢量数据库这个赛道上，也分为轻量级矢量数据库以及重量级矢量数据库等，到底是选择PostgreSQL上的pgvector插件还是选择一个专用的分布式矢量数据库，也是需要对于特定应用做出具体分析之后再做出决策。”</p><p>&nbsp;</p><p>这个说法也符合如今AutoGPT项目的真实选择，使用np.dot进行嵌入比较：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f47c21baa19278c36c56690708f17865.png" /></p><p></p><p>&nbsp;</p><p>Andrej Karpathy也曾在Twitter上表达过此类观点。之前他利用OpenAI的API建了一个大模型应用，有网友问使用了什么向量数据库，Karpathy表示，不用追风一些“奇特的东西”，使用Python库中的np.array已经足够了。推文底下当即有人评论说，这种务实的观点应该传播到学术界和整个机器学习社区！</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0762e3adfa975859574d4e4c831d4b7.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>写在最后</h2><p></p><p>&nbsp;</p><p>目前据我们所知，不采用向量数据库的也不止AutoGPT：比如GPT Engineer、GPT Pilot甚至是GitHub Copilot等都不使用向量数据库——相反，它们通过最近文件、文件系统内的邻近度或查找对特定类/函数的引用来获取相关上下文。</p><p>&nbsp;</p><p>是否选择使用向量数据库要看情况，而AutoGPT放弃向量数据库，是朝着正确方向迈出的重要一步，即专注于提供价值、而非深陷技术泥潭。</p><p>&nbsp;</p><p>会不会有一天，向量数据库又将重返AutoGPT？向量数据库到底算不算是AI技术革命中的重要组成部分？或者说，向量数据库Pinecone成为AI长期记忆方案的愿景，只是一句空洞的口号？或许也有人认为，真正的问题是像AutoGPT这样的项目并没能带来任何价值。也许目前我们能够论证的就只有这些，余下的只有靠时间来证明......</p><p>&nbsp;</p><p>延伸阅读：</p><p><a href="https://mp.weixin.qq.com/s/gGptu_zoT4lJbZ9-4fQzzg">向量数据库？不要投资！不要投资！不要投资！</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GgM1xYCEZgMSSYkJCJzd</id>
            <title>招商银行人工智能实验室研发工程师赵文婷确认出席 FCon，分享招商银行智能审查系统建设与应用</title>
            <link>https://www.infoq.cn/article/GgM1xYCEZgMSSYkJCJzd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GgM1xYCEZgMSSYkJCJzd</guid>
            <pubDate></pubDate>
            <updated>Wed, 11 Oct 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 招商银行智能审查系统建设与应用, 赵文婷, 智能审核技术团队
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，招商银行人工智能实验室研发工程师赵文婷将分享招行的智能审查系统建设与应用，介绍智能审核技术团队的实战经验。招行智能审查系统通过多项AI技术，学习利用各类业务文档和规章制度，提供全量实时质检的多模态智能审核方案，节省审核人力，提高审核效率，降低审核风险。演讲内容包括托管合同、运管票据、专项债、电访微信和智能双录合规审查等项目。参会者将获得金融场景智能审核实战经验和智能审核落地方案及应用难点。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。招商银行人工智能实验室研发工程师赵文婷将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5562?utm_source=infoqweb&amp;utm_medium=article">招商银行智能审查系统建设与应用</a>"》主题分享，介绍招行的智能审查系统，以及智能审核技术团队相关实战经验。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5562?utm_source=infoqweb&amp;utm_medium=article">赵文婷</a>"，硕士毕业于北京航空航天大学计算机系，加入招行人工智能实验室后，长期从事语音语义理解相关算法模型研发落地工作。先后主导推进智能合同审查、智能语音质检、智能双录、智能外呼、智能协呼语义理解及 TTS 应用建设等项目，深耕自然语言处理、语音合成、声纹识别、多模态分析等技术能力。项目期间参与发表论文被 EMNLP、ACL、IEEEE Trans 等国际顶会顶刊录取，所参与团队知识工程建设相关项目曾获银保监会一等奖、人民银行二等奖，同时先后获得招行中心级年度优秀员工、年度 MVP、优秀导师等多项奖项。所推进相关技术广泛应用于招行客服、经营、风控等核心业务场景，持续推进最前沿人工智能技术在金融领域结合落地。她在本次会议的演讲内容如下：</p><p></p><p>演讲：招商银行智能审查系统建设与应用</p><p></p><p>银行业作为知识密集型领域，其各个业务场景每日能够产生大量的不同模态非结构化数据，如托管产品合同、专项债发债方案书、营销电访通话、经营客服会话等。为贯彻落实监管部门关于业务开展过程中各项规定，降低合规风险、提升服务质量，行内每年需投入大量人力做质检审查工作。</p><p></p><p>招行智能审查系统通过结合自然语言处理、语音识别、图像处理等多项 AI 技术，充分学习利用各类业务文档、规章制度等领域知识，提供了一套具有较强泛化性、可支持全量实时质检的多模态智能审核方案。审核系统致力于辅助人工开展具备较强专业性的合规审查工作，节省审核人力，提高审核效率，同时降低由审核标准不一致带来的审核风险。本次演讲将会分享招行智能审核技术团队相关实战经验。</p><p></p><p>演讲提纲：</p><p></p><p>招行智能审核系统介绍实战项目分享 </p><p>○ 托管合同智能审查 </p><p>○ 运管票据智能审查 </p><p>○ 专项债智能审查 </p><p>○ 电访微信智能审查 </p><p>○ 智能双录合规审查</p><p>总结与展望</p><p></p><p>你将获得：</p><p></p><p>○ 金融场景智能审核实战经验</p><p>○ 智能审核落地方案及应用难点</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 7 折优惠 ，立省 ￥2040！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6a8f2391b65a664a2c595b532</id>
            <title>FaceFusion：探索无限创意，创造独一无二的面孔融合艺术！</title>
            <link>https://www.infoq.cn/article/6a8f2391b65a664a2c595b532</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6a8f2391b65a664a2c595b532</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Oct 2023 09:39:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FaceFusion, 面孔融合艺术, 图像处理技术, 创造性工具
<br>
<br>
总结: FaceFusion是一种使用图像处理技术的创造性工具，它可以将不同的面部特征融合在一起，创造出独特的面孔融合艺术效果。它的潜在应用包括娱乐、虚拟化妆和艺术创作。安装和使用需要一定的技术技能。 </div>
                        <hr>
                    
                    <p></p><h1>FaceFusion：探索无限创意，创造独一无二的面孔融合艺术！</h1><p></p><p>它使用先进的图像处理技术，允许用户将不同的面部特征融合在一起，创造有趣和令人印象深刻的效果。这个项目的潜在应用包括娱乐、虚拟化妆和艺术创作，为用户提供了创造性的工具</p><p></p><h1>1.效果预览</h1><p></p><p><img src="https://static001.geekbang.org/infoq/44/44d05ccbc65018d624cff8c49f87d7e5.jpeg" /></p><p></p><h1>2.安装</h1><p></p><p>请注意，安装需要技术技能，不适合初学者。请不要在GitHub上打开平台和安装相关问题。我们有一个非常有用的<a href="https://join.facefusion.io/">Discord</a>"社区，将指导您安装FaceFusion。</p><p></p><p>Read the <a href="https://docs.facefusion.io/installation">installation</a>" now.</p><p></p><h2>2.1 使用指南</h2><p></p><p>Run the command:</p><p></p><p><code lang="text">python run.py [options]

options:
  -h, --help                                                                                       show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                                                             select a source image
  -t TARGET_PATH, --target TARGET_PATH                                                             select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                                                             specify the output file or directory
  -v, --version                                                                                    show program's version number and exit

misc:
  --skip-download                                                                                  omit automate downloads and lookups
  --headless                                                                                       run the program in headless mode

execution:
  --execution-providers {cpu} [{cpu} ...]                                                          choose from the available execution providers (choices: cpu, ...)
  --execution-thread-count EXECUTION_THREAD_COUNT                                                  specify the number of execution threads
  --execution-queue-count EXECUTION_QUEUE_COUNT                                                    specify the number of execution queries
  --max-memory MAX_MEMORY                                                                          specify the maximum amount of ram to be used (in gb)

face recognition:
  --face-recognition {reference,many}                                                              specify the method for face recognition
  --face-analyser-direction {left-right,right-left,top-bottom,bottom-top,small-large,large-small}  specify the direction used for face analysis
  --face-analyser-age {child,teen,adult,senior}                                                    specify the age used for face analysis
  --face-analyser-gender {male,female}                                                             specify the gender used for face analysis
  --reference-face-position REFERENCE_FACE_POSITION                                                specify the position of the reference face
  --reference-face-distance REFERENCE_FACE_DISTANCE                                                specify the distance between the reference face and the target face
  --reference-frame-number REFERENCE_FRAME_NUMBER                                                  specify the number of the reference frame

frame extraction:
  --trim-frame-start TRIM_FRAME_START                                                              specify the start frame for extraction
  --trim-frame-end TRIM_FRAME_END                                                                  specify the end frame for extraction
  --temp-frame-format {jpg,png}                                                                    specify the image format used for frame extraction
  --temp-frame-quality [0-100]                                                                     specify the image quality used for frame extraction
  --keep-temp                                                                                      retain temporary frames after processing

output creation:
  --output-image-quality [0-100]                                                                   specify the quality used for the output image
  --output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc}                        specify the encoder used for the output video
  --output-video-quality [0-100]                                                                   specify the quality used for the output video
  --keep-fps                                                                                       preserve the frames per second (fps) of the target
  --skip-audio                                                                                     omit audio from the target

frame processors:
  --frame-processors FRAME_PROCESSORS [FRAME_PROCESSORS ...]                                       choose from the available frame processors (choices: face_enhancer, face_swapper, frame_enhancer, ...)
  --face-enhancer-model {codeformer,gfpgan_1.2,gfpgan_1.3,gfpgan_1.4,gpen_bfr_512}                 choose from the mode for the frame processor
  --face-enhancer-blend [0-100]                                                                    specify the blend factor for the frame processor
  --face-swapper-model {inswapper_128,inswapper_128_fp16}                                          choose from the mode for the frame processor
  --frame-enhancer-model {realesrgan_x2plus,realesrgan_x4plus,realesrnet_x4plus}                   choose from the mode for the frame processor
  --frame-enhancer-blend [0-100]                                                                   specify the blend factor for the frame processor

uis:
  --ui-layouts UI_LAYOUTS [UI_LAYOUTS ...]                                                         choose from the available ui layouts (choices: benchmark, webcam, default, ...)
</code></p><p></p><h1>2.相关文档</h1><p></p><p>Read the <a href="https://docs.facefusion.io/">documentation</a>" for a deep dive.</p><p></p><p>更多优质内容请关注公号：汀丶人工智能；会提供一些相关的资源和优质文章，免费获取阅读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d86169f2cae861c778c104224c834da.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Cg71IuiHtglHxVIsQ9gs</id>
            <title>为工作6小时的名人支付500万美元报酬！Meta 为做AI聊天机器人下“血本”了</title>
            <link>https://www.infoq.cn/article/Cg71IuiHtglHxVIsQ9gs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Cg71IuiHtglHxVIsQ9gs</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Oct 2023 07:02:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, 人工智能助手, 明星肖像, AI聊天机器人
<br>
<br>
总结: Meta正在向明星付费，使用他们的肖像作为人工智能助手。Meta推出了个性化的AI助手，包括28个使用了名人肖像的聊天机器人。这些机器人具有各自的个性和故事。Meta还计划将AI角色引入元宇宙。 </div>
                        <hr>
                    
                    <p><a href="https://affiliate.insider.com/?h=5a143c235343305a9d92293b4fa90a9b5a338343d24cae824f4f77e4e80d3591&amp;postID=6523bf8b385cb39dead7e848&amp;postSlug=meta-paying-celebrity-faces-of-ai-chatbots-as-much-as-5-million-2023-10&amp;site=bi&amp;u=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fmeta-is-paying-creators-millions-for-ai-chatbots&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false">据The Information 报道，</a>"Meta 正在向Snoop Dogg、Tom Brady、MrBeast和Charli D'Amelio等明星付费，因为他们允许 Meta 使用其肖像作为 Meta 的人工智能助手。Meta 向其中一名顶级创作者支付了高达500万美元的报酬，这名创作者只需要工作室里工作6个小时。</p><p>&nbsp;</p><p></p><h2>个性化的AI助手</h2><p></p><p>&nbsp;</p><p>在9月底的Connect开发者大会上，马克·扎克伯格推出了人工智能助手Meta AI，Meta 正式加入AI聊天机器人大战。通过与微软 Bing 合作，Meta AI 可以提供实时网络结果。此外，Meta AI还能够通过提示“/imagine”生成像 Midjourney 或 OpenAI 的 DALL-E 那样的图像。</p><p>&nbsp;</p><p>除了拥有类似ChatGPT 的人工智能聊天机器人，Meta 还推出了 28 个使用了名人肖像、拥有各自个性和故事的新聊天机器人。</p><p>&nbsp;</p><p>例如，模特 Kendall Jenner 的肖像被称为Billie，她被描绘成一个大姐姐，为用户提供建议；职业美式橄榄球运动员 Tom Brady 饰演 Bru，主要做体育辩论；演员Roy Choi饰演 Max，一个经验丰富的副主厨，传授烹饪秘诀和技巧；由美国说唱歌手Snoop Dogg扮演的角色Dungeon Master将可以陪用户完成基于文字的冒险游戏等。Meta 还引进了 YouTube 上订阅人数最多的 MrBeast 和 TikTok 明星 Charli D'Amelio 等创作者。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56811facd3fe7386e3780cec5e2f97ba.png" /></p><p></p><p>扎克伯格表示，“这不仅仅是回答问题，也可以用于娱乐，可以帮助你做一些事来与周围的人建立联系，帮助你完成想要做的事情。”</p><p>&nbsp;</p><p>Meta AI 和其他 AI 都是基于 Llama 2 的。Meta 生成式人工智能副总裁 Ahmad Al-Dahle&nbsp;表示，团队花了很多时间“提炼额外的对话数据集，以便让助手以对话式且友好的语气作出回应”。&nbsp;Meta 扩展了Llama 2&nbsp;模型的上下文窗口，“这样就可以与用户建立更深入、更强大的交互”。他表示，Meta AI 也经过了调整，可以给出“非常简洁”的答案。</p><p>&nbsp;</p><p>据外媒报道，Meta 最初愿意支付超过 100 万美元来使用明星的肖像，但为大牌明星支付了更多费用，不过没有说明哪位人士获得了 500 万美元的报酬。</p><p>&nbsp;</p><p>目前，这些角色扮演AI助手已经在美国推出了测试版，除了Meta AI、Bru 和 Perry 外，他们的知识库仅限于 2023 年之前大部分存在的信息，这意味着一些回复可能已经过时。Meta 表示未来几个月内将为其他人工智能助手带来搜索。该公司还计划“在未来几周推出更多版本”，涵盖游戏、哲学和时尚等一系列兴趣领域。</p><p>&nbsp;</p><p></p><h2>AI角色还要走进元宇宙</h2><p></p><p>&nbsp;</p><p>“世界上大多数人都会通过我们（的技术）首次体验到生成式 AI。”Meta 公司首席技术官 Andrew Boz Bosworth 在Connect 大会上说道。</p><p>&nbsp;</p><p>一度表示要“all in”云宇宙的Meta 曾被认为在 AI 领域落后于微软、谷歌、OpenAI 等竞争对手，但Bosworth 坚定地说，“Meta 并没有落后，早在去年年底 ChatGPT 问世之前，我们就已经在全球范围内利用 AI 增强了自己的平台。”</p><p>&nbsp;</p><p>他表示，Meta 希望用户即使在智能手机上也可获得快速、优质的搜索结果。在去年 11 月，Meta 就发布了一款名为“Galactica”的生成式 AI 聊天机器人，它能够写文章、解数学题，偶尔也会编造答案。不过 Meta“很快”关闭了它。今年早些时候，Meta 开源了 Llama 2 模型，开发者可对其进行修补并创建自己的聊天机器人。</p><p>&nbsp;</p><p>Meta 将其庞大的用户群（某即时通讯应用的每天数十亿用户）视为与ChatGPT和其他公司相比的关键竞争优势。该助手“就在你的聊天环境中，而我们的聊天应用非常受欢迎，”Al-Dahle说道，“你不需要脱离上下文来互动或参与。”</p><p>&nbsp;</p><p>不过，扎克伯格并没有放弃在元宇宙领域的雄心。他表示，“在不远的将来，你走进一个房间，可以看到的能与之互动的数字全息图将与物理实体一样多。”他还表示，Meta计划最终让Max等AI角色以虚拟人的形式出现在元宇宙中。</p><p>&nbsp;</p><p></p><h2>一些隐忧</h2><p></p><p>&nbsp;</p><p>扎克伯格表示，人们对人工智能版本的名人有“巨大的需求”。<a href="https://www.wsj.com/tech/ai/meta-ai-chatbot-younger-users-dab6cb32">《华尔街日报》</a>"报道称，Meta 发布这些个性化的人工智能助手是为了吸引和留住 Facebook 和 Instagram 上的 Z 世代用户。然而，扎克伯格自己也承认，出于品牌安全考虑，推出此类技术可能需要更长的时间。名人也会担心自己的形象被操纵而发表不当或有争议的言论。</p><p>&nbsp;</p><p>为此，Meta 添加了很多保障措施来尽可能避免公关灾难，比如Meta AI 不能帮助制造炸弹、不会给人关于如何分手的建议等。Al-Dahle 表示，该公司花费了 6000 个小时对模型进行红队训练，以发现有问题的用例，并且在发布前，员工每天都会与该模型进行数千次对话。</p><p>&nbsp;</p><p>这并不是第一次有人大肆宣扬人工智能表演者的可能性。如今，社交媒体上有大量人工智能生成的音乐和AI名人的视频。今年 4 月，加拿大歌手Grimes表示，她将与任何在人工智能生成的歌曲中成功使用她声音的人平分版权收入。</p><p>&nbsp;</p><p>然而，并不是所有人都像扎克伯格和Grimes一样对人工智能在媒体领域的潜力持乐观态度。</p><p>Spotify 首席执行官表示，音乐行业对人工智能生成歌曲的传播存在“合理担忧”，并补充称，他的平台正在与其他伙伴合作开发保护艺术家的解决方案。</p><p>&nbsp;</p><p>与此同时，人工智能一直是当前好莱坞演员罢工的核心问题，因为他们担心工作室可能会使用人工智能生成的表演来取代演员。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/">https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/</a>"</p><p><a href="https://www.theinformation.com/articles/meta-is-paying-creators-millions-for-ai-chatbots?irclickid=VVISkU3AXxyPUmWzPTQaX27KUkFWO9x0xWl3Vs0&amp;irgwc=1&amp;utm_source=affiliate&amp;utm_medium=cpa&amp;utm_campaign=10078-Skimbit+Ltd.&amp;utm_term=businessinsider.com">https://www.theinformation.com/articles/meta-is-paying-creators-millions-for-ai-chatbots?irclickid=VVISkU3AXxyPUmWzPTQaX27KUkFWO9x0xWl3Vs0&amp;irgwc=1&amp;utm_source=affiliate&amp;utm_medium=cpa&amp;utm_campaign=10078-Skimbit+Ltd.&amp;utm_term=businessinsider.com</a>"</p><p><a href="https://www.theverge.com/2023/9/27/23891128/meta-ai-assistant-characters-whatsapp-instagram-connect">https://www.theverge.com/2023/9/27/23891128/meta-ai-assistant-characters-whatsapp-instagram-connect</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mUNqaqDCjibDqGQKWEhS</id>
            <title>GitHub基于大语言模型构建Copilot的经验和教训</title>
            <link>https://www.infoq.cn/article/mUNqaqDCjibDqGQKWEhS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mUNqaqDCjibDqGQKWEhS</guid>
            <pubDate></pubDate>
            <updated>Tue, 10 Oct 2023 01:44:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GitHub, GitHub Copilot, 大语言模型, Find it, Nail it, Scale it, SDLC, IDE
<br>
<br>
总结: GitHub在一篇博文中分享了他们在构建和扩展GitHub Copilot过程中所学到的经验教训。GitHub的AI产品负责人Shuyin Zhao描述了他们如何在三年多的时间里历经三个阶段——“Find it”、“Nail it”和“Scale it”——成功推出了GitHub Copilot。在“Find it”阶段，他们专注于找到AI可以有效解决的问题，通过一种足够专注的方式快速推向市场，并且足以产生影响。这包括确定到底是为了谁而解决问题——帮助开发人员更快地编写代码，减少上下文切换。他们还致力于确保他们所做的是对现有工具的补充，而不是替代。 </div>
                        <hr>
                    
                    <p><a href="https://github.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">GitHub</a>"在一篇文章中分享了他们在构建和扩展<a href="https://github.com/features/copilot?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">GitHub Copilot</a>"——一个使用<a href="https://www.infoq.com/llms/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">大语言模型</a>"的企业应用——过程中所学到的经验教训。</p><p></p><p>在GitHub的一篇<a href="https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">博文</a>"中，GitHub的AI产品负责人<a href="https://www.linkedin.com/in/shuyin-zhao-5758307b/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">Shuyin Zhao</a>"描述了他们如何在三年多的时间里历经三个阶段——<a href="https://www.nailthenscale.com/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">“Find it”、“Nail it”和“Scale it”</a>"——成功推出了GitHub Copilot。</p><p></p><p>在“Find it”阶段，他们专注于找到AI可以有效解决的问题，通过一种足够专注的方式快速推向市场，并且足以产生影响。</p><p></p><p>这包括确定到底是为了谁而解决问题——帮助开发人员更快地编写代码，减少上下文切换。此外，他们只关注<a href="https://stackify.com/what-is-sdlc/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">SDLC</a>"的一部分：<a href="https://www.infoq.com/ides/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">IDE</a>"中的编码功能，并结合当下的LLM的能力。这样他们就可以专注于让工具提供代码建议，而不是生成全部代码。他们还致力于确保他们所做的是对现有工具进行增强，不要求开发人员改变已有的工作流程。</p><p></p><p></p><blockquote>“在设计产品时，我们不仅要考虑输出需要人类进行评估的模型，也要考虑正在学习如何与AI互动的人类。”——Idan Gazit，GitHub Next高级研发总监</blockquote><p></p><p></p><p>在“Nail it”阶段，他们基于从<a href="https://www.infoq.com/presentations/ab-testing-spotify/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">A/B测试</a>"中获得的真实用户反馈进行迭代式产品开发。他们进行快速迭代、试错和学习。在使用Copilot的Web接口进行了简短的实验后，他们将重点转向了IDE，以减少在编辑器和Web浏览器之间切换，并让AI在后台运行。在进一步的迭代中，通过观察开发人员在编码时打开的多个IDE选项卡，GitHub Copilot可以同时处理多个文件。</p><p></p><p>随着生成式AI的迅速发展，他们开始重新审视过去所做出的决策，技术的进步和用户对它的熟悉程度有时会让过去的决策变得过时。于是，提供交互式聊天的想法开始活跃起来，他们需要基于沉没成本谬论改变决策，例如，当大语言模型的进步允许一个模型处理多种语言时，就需要改变为每种语言构建AI模型的想法。</p><p></p><p>最后，在“Scale it”阶段，他们致力于确保AI模型结果的一致性、管理用户反馈，并定义了关键性能指标，以实现应用程序的普遍可用性(GA)。他们还考虑了安全性和AI责任问题，使用过滤器来避免为用户建议不安全或具有冒犯性的代码。</p><p></p><p>改进质量和可靠性方面的工作包括缓解大语言模型的幻觉，即答案可能是不可预测的，并且每次查询都有所不同。解决这个问题的策略包括修改发送给大语言模型的参数，以减少响应的随机性，并缓存频繁的响应以减少变化和提高性能。</p><p></p><p>GitHub使用等待列表来管理技术预览版的早期用户。这意味着他们可以获得来自一小群早期采用者的评论和反馈。对真实用户反馈的深入分析使得GitHub团队能够识别出有问题的更新，并改进产品的关键性能指标，例如开发人员保留了多少由Copilot生成的代码。</p><p></p><p>最后，他们确保开发人员生成的代码是安全的，并通过过滤器来拒绝可能引入安全问题(如SQL注入)的代码建议。社区也提出了一些问题，例如Copilot的代码建议与公开的代码相重叠可能会产生许可问题或其他影响。他们为此提供了一个代码参考工具，帮助开发人员做出明智的选择。</p><p></p><p>在市场策略方面，他们向一些有影响力的社区成员展示了技术预览版，并且面向的是个人用户而不是企业。这有助于在正式发布时获得广泛的支持，从而促使企业采用它。</p><p></p><p>关键在于展示专注于特定问题的重要性、整合实验结果和用户反馈，以及在应用扩展时优先考虑用户需求。</p><p></p><p>由于生成式AI的采用仍处于早起阶段，GitHub也在密切关注市场对生成式AI工具的需求。感兴趣的读者可在GitHub的博客上阅读<a href="https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTY5MDIyNDcsImZpbGVHVUlEIjoiUktBV01YTWV6bGlCd3lxOCIsImlhdCI6MTY5NjkwMTk0NywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.P2yyG-WcNf06zArxxU7FdWgCev_twzJMqZhwco2qCJc">全文</a>"。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/github-copilot-lessons/">https://www.infoq.com/news/2023/10/github-copilot-lessons/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WIRIew4hJP5H2UDF76N8</id>
            <title>设计师主导的研发模式下，美图自研视觉大模型100天进化</title>
            <link>https://www.infoq.cn/article/WIRIew4hJP5H2UDF76N8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WIRIew4hJP5H2UDF76N8</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Oct 2023 15:06:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美图公司, AI视觉大模型, MiracleVision 3.0, 奇思妙想, 智能创作
<br>
<br>
总结: 美图公司发布了自研的AI视觉大模型MiracleVision 3.0版本，该模型具备奇思妙想和智能创作两大特性。通过提示词智能联想和提示词精准控制功能，用户可以轻松创作出真实细腻的画面细节。同时，通过深化创作、AI画面扩展、局部修改和分辨率提升功能，作品的细节和表现力得到进一步丰富和提升。美图公司在视觉大模型的研发过程中注重美学，通过设计师主导的研发模式，不断优化模型在美学上的效果，使其具备独特的竞争力。 </div>
                        <hr>
                    
                    <p>*封面图片来源自笔者使用美图秀秀-AI绘画和AI扩图功能生成</p><p></p><p>10月9日，美图公司举办15周年生日会，并发布自研AI视觉大模型MiracleVision（奇想智能）3.0版本。面世100天后，美图AI视觉大模型MiracleVision3.0将全面应用于美图旗下影像与设计产品，并落地电商、广告、游戏、动漫、影视五大行业，助力五大行业“工作流提效”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/09/09fbcd8f30e2366440489a299f274b8b.png" /></p><p></p><p>会后，围绕美图视觉大模型的独特性、研发模式、核心竞争力等问题，美图公司管理层与InfoQ等媒体展开了进一步交流。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc5bfc30f2927136209ae4912e29f769.png" /></p><p></p><p></p><h2>MiracleVision 3.0核心能力</h2><p></p><p>&nbsp;</p><p>据美图公司创始人、董事长兼首席执行官吴欣鸿介绍，三个月前刚发布时，MiracleVision的绘画水平还停留在初级阶段，如今MiracleVision 3.0版本已经能描绘出真实细腻的画面细节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/474c7f53e76edea473d64cb03b9a669f.png" /></p><p></p><p>美图公司将MiracleVision的核心能力拆解为“奇思妙想”和“智能创作”两大特性。</p><p>&nbsp;</p><p>在“奇思妙想”层面，MiracleVision通过“提示词智能联想”功能来降低大众的使用门槛，当用户输入关键词，MiracleVision可自动补充相关表述，如光影效果、质感、风格、图片质量等，推动创作平权。此外，通过“提示词精准控制”功能，MiracleVision能满足更加专业的设计要求，如使用“近景”、“远景”、“顺光”、“逆光”等描述控制最终生成效果。</p><p>&nbsp;</p><p>在“智能创作”层面，MiracleVision通过“深化创作”功能，可以进一步丰富作品细节和提升表现力。通过“AI画面扩展”功能让作品尺寸更大、细节更丰富。通过“局部修改”功能，对部分画面进行精准修改与调整。通过“分辨率提升”功能生成高清大图，让细节表现、色彩展示、物体辨识更加的精准和生动。</p><p>&nbsp;</p><p>对MiracleVision感兴趣用户可以访问AI视觉创作工具<a href="https://www.whee.com/">“WHEE”官网</a>"体验。目前美图大部分产品也都逐渐融入了MiracleVision大模型，其中美图秀秀作为一个影像入口，整合了美图大部分产品，用户也可以在美图秀秀上一站式地感受美图视觉大模型能力。</p><p></p><h2>从1.0到3.0，美图自研视觉大模型演进历程</h2><p></p><p>&nbsp;</p><p>自6月19日发布以来，美图AI视觉大模型Miracle Vision已经完成1.0、2.0、3.0三个版本的进化。</p><p>&nbsp;</p><p>美图公司设计副总裁、设计中心负责人许俊用三个关键词总结了Miracle Vision各个版本的状态。1.0版本是勤奋好学，刚到及格线，初步建立美学体系，但各个维度还需要不断训练；2.0版本是奇思妙想，通过持续训练，模型的创作力得到提升，生成结果更加有想象力；3.0版本是智能创作，在之前的基础上可以做到更加精准智能的控制，也更加精细，细节质感显著提升。</p><p>&nbsp;</p><p>在不同阶段，美图大模型团队需要解决的技术难点和挑战也各不相同。</p><p>&nbsp;</p><p>据美图公司技术副总裁、美图影像研究院（MT Lab）负责人刘洛麒介绍，在1.0阶段，团队主要工作是搭建大模型的架构和基础，使后续2.0和3.0的研发可以达到比较好的准备条件，这个阶段的难点主要在于怎么搭建好这个基础架构和平台。</p><p>&nbsp;</p><p>在2.0阶段，团队需要与外部设计师，包括艺术院校的老师和学生一起去构建一个比较高质量的数据集，使大模型在美学上可以达到比较好的状态。</p><p>&nbsp;</p><p>在3.0阶段，需要攻克的技术难点主要是模型的可控性和在垂直领域的效果精致度，其中可控性方面，不管是细节控制还是局部编辑，要能使用户想要达到的效果在模型的技术层面能达到很好的实现，这是一个很大的挑战。而垂直领域的效果精致度，需要团队花很多精力投入在每个不同的垂直领域效果调试上，针对每个领域的训练方式、生成方式和调试方式都是不一样的。</p><p>&nbsp;</p><p>美图公司集团高级副总裁、影像与设计产品事业群总裁陈剑毅补充表示，如果做通用的视觉大模型，把全网的各种图片拿过来做一些训练，其实很好做，但这样做出来的模型，最终生成的东西其实用不到实际工作过程中，因为每个垂直领域细分下去还会有特别多不同的品类，通用模型无法满足实际需求。</p><p></p><h2>做视觉大模型，美图强在哪？</h2><p></p><p>&nbsp;</p><p>围绕AI视觉大模型上，美图投入巨大。吴欣鸿透露，首先是研发费用层面，今年上半年美图的研发投入将近3亿，营收占比超过20%，在业内是一个比较高的比例；其次在团队人员层面，现在跟大模型相关的工程师在600人左右，此外还有很多设计师、产品经理等参与到了大模型相关工作。</p><p>&nbsp;</p><p>吴欣鸿向InfoQ等媒体表示，美图现在可以说是全员拥抱AI，“发展太快了，我们的认知甚至是以天为单位再刷新，所以我们需要内部有很强的紧迫感，让大家对视觉大模型有很深度的理解和应用，才能更好地去服务用户、赋能行业。”</p><p>&nbsp;</p><p>与市面上现有的其他大模型相比，美图的视觉大模型有何特别之处？刘洛麒认为，Miracle Vision的独特性在于其具备美学的倾向性，团队在研发过程中，会基于模型建立美学的评估体系，不断优化在美学上的效果，其模型架构、模型结构都是以这个为出发点来组织和建立的。</p><p>&nbsp;</p><p>在这次交流过程中，“美学”可以说是美图管理层提及频率最高的一个关键词。</p><p>&nbsp;</p><p>在美图公司高级技术副总裁杨明花看来，美图做视觉大模型的核心竞争力，除了来自过去十多年美图在数据、算法、算力等方面的长期积累，“美学”也是非常关键的一项。据她介绍，美图在这方面积累了非常多年的经验，有很深厚基础，美图的算法模型会以美学和创造性为目标来进行训练，从而达到更好的效果。</p><p>&nbsp;</p><p>具体而言，模型每次训练，都会按照美图的美学体系去评估需要调整的方向，在训练过程中，设计师和美学领域创作者的参与程度非常高。</p><p>&nbsp;</p><p>基于对“美学”的重视，美图所采取的是一个设计师主导的研发模式，美图视觉大模型的总负责人由美图公司设计副总裁、设计中心负责人许俊担任，这与业内做视觉大模型的公司都不一样。</p><p>&nbsp;</p><p>众所周知，大模型评估很难，行业内有很多榜单从不同维度来评估什么样的AI大模型更好。但在美图看来，美学和用户的连接是评估大模型更好的方式，所以团队也以这个为出发点建立大模型的评估体系，进而反推技术研发。</p><p>&nbsp;</p><p>做大模型，除了技术能力必不可少，在美图看来，形成用户反馈的闭环也很关键。而这正是美图的另一个优势，陈剑毅补充表示，基于美图众多应用产品和超过2亿的用户群体，团队能够快速得到真实用户对于大模型效果的反馈。一个效果做好之后，团队会以小流量的方式推到线上，然后立马就可以看到用户的点赞或吐槽，团队也可以跟用户交流，反复调整效果，这样模型就能以最快的速度跟应用场景结合做改进。</p><p>&nbsp;</p><p>吴欣鸿强调，把用户的正反馈或负反馈投入到训练过程中，会成为未来大模型竞争力的一个重要优势。只有构建一个技术、用户场景、商业模式的完整闭环，才能基于用户或客户产生的反馈持续改进、快速迭代，迭代速度也是竞争的关键。</p><p></p><h2>视觉大模型应用尚处于探索期</h2><p></p><p>&nbsp;</p><p>在吴欣鸿看来，对于各行各业的从业者而言，AI视觉大模型带来的改变不止限于视觉效果的提升，更重要的价值的是对工作流的改造和创新。</p><p>&nbsp;</p><p>“AI视觉大模型的本质，是无穷无尽的视觉创意库。应用层相当于内容提取器，根据用户的需求，从这个巨大的创意库中提取所需要的内容，让用户在特定场景中使用。AI视觉大模型和应用之间相辅相成，大模型为应用提供技术支撑，应用反哺大模型的效果迭代。”</p><p>&nbsp;</p><p>当前，AI视觉大模型主要被运用于生成各类艺术作品，包括绘画、摄影和设计图稿，能展现出初步的效果，但这只是起点。吴欣鸿相信AI的进化速度会很快，将来在AI的帮助下，万物皆可生成。</p><p>&nbsp;</p><p>吴欣鸿表示，虽然目前国内已经有很多团队在研发视觉大模型，但能将视觉大模型与生产环节结合的企业数量相对较少。在他看来，大模型真正在生产端普及使用需要解决三个问题：垂直领域极致效果、工作流整合、变现能力。随着AI视觉大模型和生产端的磨合，这三个问题会被逐步解决。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0d/0dc9ae995a91b1aacfc5a8bcbfb3d5ee.png" /></p><p></p><p>吴欣鸿表示，视觉大模型应用普及将经历三个阶段：探索期、高速发展期、成熟期。</p><p>&nbsp;</p><p>其中，2024年之前是探索期，厂商在这一阶段进行不断探索，效果勉强及格，视觉大模型在工作流里支持单任务的提效，验证场景的可行性；2024-2025年进入高速发展期，效果会逐步精进，有明确的场景，带来工作流的升级；2026-2030年进入成熟期，视觉大模型的生成效果会非常出色，凡是设计与创意，视觉大模型都是标配。而设计的边界也会不断拓宽，视觉大模型将助力千万设计场景，引领美学的升级与社会经济增长。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DUcAjpfd9ueWK9C1yOsN</id>
            <title>2022-2023年技术圈发生了什么？这21份报告不能错过，涵盖开发者、开源、技术和行业发展！| InfoQ研究中心</title>
            <link>https://www.infoq.cn/article/DUcAjpfd9ueWK9C1yOsN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DUcAjpfd9ueWK9C1yOsN</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Oct 2023 10:01:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ研究中心, 行业报告, 大语言模型, 中国软件技术发展, 中国开源发展研究分析, 互联网行业再进化, 中国企业研发高效能白皮书
<br>
<br>
总结: InfoQ研究中心是极客邦科技双数研究院旗下的研究机构，致力于产出深度、观点鲜明的研究成果。他们的研究报告涵盖了多个领域，如大语言模型、中国软件技术发展、中国开源发展等。这些报告通过数据收集、专家访谈和研究模型验证，为读者提供了行业洞察和趋势预测。InfoQ研究中心还将持续产出相关研究成果，关注前沿科技领域、数字化产业应用和数字人才等方面的发展。读者可以通过点击链接直接下载阅读这些报告。 </div>
                        <hr>
                    
                    <p>【导语】2022-2023年InfoQ研究中心研究成果合集，快来查看领取~</p><p><a href="https://www.infoq.cn/research">InfoQ&nbsp;研究中心</a>"隶属于极客邦科技双数研究院，秉承客观、深度的内容原则，追求研究扎实、观点鲜明、生态互动的目标，聚焦创新技术与科技行业，围绕数字经济观察、数字人才发展进行研究。 InfoQ&nbsp;研究中心旨在加速创新技术的孵化、落地与传播，服务相关产业与更广阔的市场、投资机构，&nbsp;C-level&nbsp;人士、架构师/高阶工程师等行业观察者，为全行业架设沟通与理解的桥梁，跨越从认知到决策的信息鸿沟。</p><p>自&nbsp;2022&nbsp;年成立以来，InfoQ&nbsp;研究中心已累计产出研究报告&nbsp;21&nbsp;篇，包括&nbsp;11&nbsp;篇行业报告、2&nbsp;篇开发者用户调研报告和&nbsp;8&nbsp;篇研究模型报告，覆盖人工智能、云原生、大数据、数据库、操作系统、研发效能、开源等诸多领域。现将所有报告进行集中整理，供各位读者点击链接直接下载阅读。</p><p>未来InfoQ&nbsp;研究中心也将围绕前沿科技领域、数字化产业应用和数字人才三方面，持续产出相关研究成果，欢迎大家持续关注！</p><p></p><h4>一、行业报告合集</h4><p></p><p></p><h5><a href="https://www.infoq.cn/minibook/vWO39J1tlb9xlSaIJoI6">大语言模型综合能力测评报告&nbsp;2023</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bc1838fd46b0ce367ab9b25d91fcb4e.png" /></p><p>关键词：大模型、测评、安全隐私、多模态、逻辑推理、上下文理解报告简介：InfoQ&nbsp;研究中心选取语言模型的准确性、数据基础、模型和算法能力、安全和隐私四个大维度和12个细分维度，分别对ChatGPT、Claude、Sage、天工3.5、文心一言、通义千问、讯飞星火、Moss、ChatGLM、vicuna-13B进行了3000+题目的评<a href="https://www.infoq.cn/minibook/UGhD7MTY5Z43JG5YmWP3">中国软件技术发展洞察和趋势预测报告&nbsp;2023</a>"</p><p><img src="https://static001.geekbang.org/infoq/f4/f4de6af073e0c7f6249ff6406402d006.png" /></p><p>关键词：人工智能、云原生、产业互联网、数实融合、算力基础设施报告简介：本报告是岁末年初，InfoQ&nbsp;研究中心团队献给全中国开发者的一份礼物。我们希望通过系统的行业数据收集和分析，广泛的专家访谈和调研，以及严谨的研究模型验证与调试，洞察年度技术发展热点、分析年度技术发展特征、预测年度技术发展趋势。</p><p></p><h5><a href="https://www.infoq.cn/minibook/DTAg4l8piWHrBGfU3der">中国开源发展研究分析&nbsp;2022</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b632ffdfba7eb3e3026b7402d111487b.png" /></p><p>关键词：开源项目、InfoQ开源项目指数、基础软件、开发者、社区、基金会报告简介：《中国开源发展研究分析&nbsp;2022》研究报告，为开发者，技术管理者，开源社区运营、市场，开源办公室工作人员以及其他对开源有一定基础认知，但期待进一步了解开源、理解开源的朋友，带来信息上的增量以及对开源趋势、开源人画像方面的关键洞察。</p><p></p><h5><a href="https://www.infoq.cn/minibook/Iwk2LLuMFSV4AisWG8jR">互联网行业再进化——云上AI时代</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c1037dee80443f12b38d306e657f498.png" /></p><p>关键词：云计算、AI、互联网行业、云上AI、大模型、AIGC报告简介：互联网行业巨变下，云计算与AI展现了前所未有的深度融合趋势，技术应用逐渐向云上&nbsp;AI&nbsp;模式演进。本篇报告希望通过和各位行业专家的深度访谈，回答以下问题：云上&nbsp;AI&nbsp;时代下，互联网行业如何抓住这次的发展浪潮，实现整体产业升级？目前行业内又有哪些实践探索？市面上又有怎样的解决方案？期望为整体互联网行业未来的发展和技术变革贡献一份力量。</p><p></p><h5><a href="https://www.infoq.cn/minibook/OE9HEkWmc3xTJYeTN1y7">中国企业研发高效能白皮书（合集）</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/47/477cce67efe1b5bdfb908d6a30a515e0.png" /></p><p>关键词：CI/CD、ChatOps、企业级架构、Code&nbsp;Review、研发效能管理报告简介：本份报告以中国高效能研发企业为研究对象，尝试解读市场中具有代表性的高效能研发解决方案。本次报告由五个篇章组成，包括&nbsp;CI/CD、ChatOps、企业级软件架构、Code&nbsp;Review、价值流管理与研发效能管理等五大主题。</p><p></p><h5><a href="https://www.infoq.cn/minibook/l224pkCVuCp7jlihDtGw">中国研发效能管理白皮书—从价值流管理到研发效能管理</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1ec4c8009617d6770bbe1d64478b0bfb.png" /></p><p>关键词：价值流管理、研发效能管理、指标体系、最佳实践、方法论报告简介：本报告讨论了价值流管理相关的定义、特征、主要分析指标、发展历程。再从价值流管理面临的问题出发，讨论并得出中国场景下需要在需求价值流和工程实践流的双流模型，最终落地研发效能管理。</p><p></p><h5><a href="https://www.infoq.cn/minibook/GbzsfXjVsoxv5JIarZk9">中国企业研发高效能白皮书-Code&nbsp;Review篇</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/38/3888eb05eb4db864e0ff580642665125.png" /></p><p>关键词：Code&nbsp;Review、挑战、工具、最佳实践、提效、代码评审报告简介：本报告主要围绕Code&nbsp;Review展开，阐述其概念和价值，分析Code&nbsp;Review&nbsp;的发展历程与工具市场现状。同时，根据企业、评审者、开发者在Code&nbsp;Review中面临的挑战，分析不同类型Code&nbsp;Review的解决方案，旨在为企业实现研发高效能提供参考。此外，报告中还解读了极狐GitLab&nbsp;Code&nbsp;Review的最佳实践，在案例中向读者展现Code&nbsp;Review&nbsp;工具是如何标准化研发流程并提升Code&nbsp;Review效率。</p><p></p><h5><a href="https://www.infoq.cn/minibook/0afBoBh4lBtoWOdzSOZW">中国企业研发高效能白皮书-企业级架构</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/30/305a997f19856fa9c85f60a67a828daf.png" /></p><p>关键词：架构方案、选择标准、搭建过程、最佳实践</p><p>报告简介：本报告主要介绍了企业级软件架构如何帮助研发团队提升效率。报告从企业级软件架构的定义和价值出发，通过分析常见的企业级软件架构需求，为不同业务规模的企业提供企业级架构的选择参考，旨在为企业实现研发高效能提供行之有效的方法。同时，报告以极狐GitLab&nbsp;企业级软件架构实践为例，让读者可以直观了解到企业级软件架构在企业的落地情况以及为企业研发团队提升效率提供的帮助。</p><p><a href="https://www.infoq.cn/minibook/qBPtforUFR274HePresL">中国企业研发高效能白皮书-ChatOps篇</a>"</p><p><img src="https://static001.geekbang.org/infoq/62/62a7f461b201bff26b632e6ec9859e4a.png" /></p><p>关键词：ChatOps、技术结构、自然语言生成报告简介：本报告主要研究了ChatOps是如何帮助研发团队提升效率的，不仅说明了ChatOps的概念和技术结构，而且对ChatOps市场的发展历程和趋势进行了研究与洞察。此外，根据InfoQ&nbsp;研究中心2023年1月发布的中国技术成熟度评估曲线，ChatOps&nbsp;处于准成熟技术阶段，这表明目前是采用&nbsp;ChatOps&nbsp;技术较为合适的时间点。同时，通过极狐GitLab&nbsp;ChatOps的实例，读者可以更好地了解ChatOps是如何在决策支持、研发自动化以及运维自动化三大场景赋能团队研发效率方面的。</p><p></p><h5><a href="https://www.infoq.cn/minibook/Q4eHZELtNaUvfZJV7lrK">中国企业研发高效能白皮书-CI/CD篇</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb431ce64643d7ece1caa97f66be0864.png" /></p><p>关键词：持续集成、持续部署、持续交付、平台工具报告简介：本报告主要介绍了&nbsp;CI/CD&nbsp;工具是如何帮助研发团队提升效率。这份报告不仅阐述了&nbsp;CI/CD&nbsp;的起源与发展背景，并对&nbsp;CI/CD&nbsp;市场的相关数据、厂商分布进行了研究与洞察。此外，研究发现，CI/CD&nbsp;主要通过持续性、自动化、可追溯、高效迭代四大抓手赋能研发团队。同时，通过极狐GitLab&nbsp;CI/CD&nbsp;的实例，读者可以更好地了解&nbsp;CI/CD&nbsp;是如何通过一体化平台，一站式体验、简单易用，便捷高效、数据可视，监控优化、安全构建，安全交付赋能团队研发效率方面的。</p><p></p><h5><a href="https://www.infoq.cn/minibook/wM9COli5Mx7mARVj7ZXQ">软件工程数智化研究报告—可观测应用篇&nbsp;2023</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/15/15d76ee390642c809666a29b18a98234.jpeg" /></p><p>关键词：可观测、运维、图谱、存储优化、安全报告简介：InfoQ&nbsp;研究中心联合中国信通院铸基计划重磅推出《软件工程数智化研究报告—可观测应用篇&nbsp;2023》，解析可观测性发展特征，分析评价当前市场参与者和相关可观测性解决方案，以期为企业和开发者们提供关于可观测性的最新研究成果和实践经验。</p><p></p><h4>用户研究合集</h4><p></p><p></p><h5><a href="https://www.infoq.cn/minibook/oDh5G4Rcsc1gW1O1Tou8">中国科技领导者画像研究报告&nbsp;2023</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7d1dd15187fb4fe4901fa018586cfb4.png" /></p><p>关键词：科技领导者画像、赛道转换、领导力模型、成长路径报告简介：InfoQ&nbsp;研究中心持续关注中国开发者群体，本次发布开发者人群生态系列报告，将视线聚焦在中国科技领导者人群中。中国科技领导者是中国经济高质量发展的重要推动者和护航者，研究该人群对理解中国科技以及数字经济的发展起着重要作用。本报告中国科技领导者的问卷调研及定向访谈，洞察中国科技领导者的行业流向、现阶段的人才供需矛盾以及数字化新时代下的配套服务体系。</p><p></p><h5><a href="https://www.infoq.cn/minibook/JF1iyU2U7eSg0zENZzhz">中国开发者画像洞察报告&nbsp;2022</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/04/0470b338d150cb1be63eef4b96afaa01.png" /></p><p>关键词：开发者画像、学习驱动、能力更新、行业转变报告简介：极客邦科技双数研究院权威出版《中国开发者画像洞察报告&nbsp;2022》，为你深度解读开发者人群背景，分析开发者群体面临的挑战，洞察开发者人群特征，预测开发者生态发展趋势。</p><p></p><h4>研究模型合集</h4><p></p><p></p><h5><a href="https://www.infoq.cn/minibook/IV4VhedKw1E1tY8Hleje">2023&nbsp;中国人工智能成熟度模型报告</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b01b398716e2e3f2d0501e883b01f71.png" /></p><p>关键词：人工智能、技术成熟度、大模型、AIGC、自动驾驶报告简介：本报告基于三大关键指标，参考市场规模、融资事件等公开资料，并结合了&nbsp;AI&nbsp;行业内硬件、模型、应用不同领域的各位专家观点，构建涵盖&nbsp;40+&nbsp;技术点的中国人工智能成熟度模型，为技术的应用决策和未来投资参考提供研究分析工具。</p><p></p><h5><a href="https://www.infoq.cn/minibook/q2Rhj103VtuMcdPlFGGS">2023&nbsp;中国云原生成熟度模型报告</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7eddff2d1eec93d953253ed65576570f.png" /></p><p>关键词：云原生、技术成熟度、容器编排、可观测报告简介：本报告基于三大关键指标，参考市场规模、融资事件等公开资料，并结合了云原生领域中产品服务、解决方案和应用侧的各位专家观点，构建涵盖&nbsp;20+&nbsp;技术点的中国云原生成熟度模型，为技术的应用决策和未来投资参考提供研究分析工具。</p><p></p><h5><a href="https://www.infoq.cn/minibook/9j4NSEEh2JGJAUVdQGGu">中国开源生态图谱&nbsp;2023</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/24/2447bdc6358ced01427693fd7cd5c5fe.png" /></p><p>关键词：开源、基础软件、开发者、社区、基金会、Github、Gitee报告简介：InfoQ&nbsp;研究中心希望通过《中国开源生态图谱&nbsp;2023》的发布，以中国开源项目名录和图谱的形式，为中国开源领域提供便捷易用的工具，让国内开发者、企业、研究院、基金会等开源生态了解中国开源的项目现状，并为中国开源产品添砖加瓦。</p><p></p><h5><a href="https://www.infoq.cn/minibook/qxc2IsAgmJ52TTYV1oj4">中国开源生态系列图谱——前端领域</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a4ed2f51ef7e4339ddd8f100d887da1.png" /></p><p>关键词：开源、前端、项目、社区、工具库报告简介：InfoQ&nbsp;研究中心开源领域的系列研究成果延续，继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国前端开源项目的发展情况，中国前端开源领域发展难点与未来趋势，总结发展趋势，以供广大开发者和开源社区研究。</p><p></p><h5><a href="https://www.infoq.cn/minibook/zdDoaDUkCGiLmWcPBYIz">中国开源生态图谱2023——云原生领域</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f2381c2ab72019c80906c1c8f9df954.png" /></p><p>关键词：云原生、容器、容器编排、微服务、服务网格、RocketMQ、APISIX、KubeEdge报告简介：InfoQ&nbsp;研究中心开源领域的系列研究成果延续，继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国云原生领域开源项目的发展情况，总结优质的案例与经验供广大开发者和开源社区研究。</p><p></p><h5><a href="https://www.infoq.cn/minibook/3ElKmiQIzsFC8ThhFfst">中国开源生态图谱2023——人工智能领域</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/66/66989cd4a54c45c896dca4f074c2ca64.png" /></p><p>关键词：人工智能、框架引擎、算法模型、工具、平台、数据集、昇思MindSpore、飞桨PaddlePaddle、openMLDB报告简介：InfoQ&nbsp;研究中心开源领域的系列研究成果延续，继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国人工智能领域开源项目的发展情况，总结优质的案例与经验供广大开发者和开源社区研究。</p><p></p><h5><a href="https://www.infoq.cn/minibook/pPz0K3aDcvO6pvtDBEq6">中国开源生态图谱2022——数据库领域</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0fb20e7b2b47db75e4415d56006663c5.png" /></p><p>关键词：关系型数据库、时序数据库、向量数据库、openGauss、TiDB、TDengine报告简介：InfoQ&nbsp;研究中心开源领域的系列研究成果延续，继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国数据库领域开源项目的发展情况，总结优质的案例与经验供广大开发者和开源社区研究。</p><p></p><h5><a href="https://www.infoq.cn/minibook/ARa5HwDdOveaDKavLSc3">中国开源生态图谱2022——操作系统领域</a>"</h5><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc205f7d0147a3a8864850f682a93b79.png" /></p><p>关键词：云操作系统、桌面操作系统、服务器操作系统、物联网操作系统、OpenHarmony、openEuler、OpenCloudOS报告简介：InfoQ&nbsp;研究中心开源领域的系列研究成果延续，继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国操作系统领域开源项目的发展情况，总结优质的案例与经验供广大开发者和开源社区研究。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8wWUiBa8eBWVLRMrxJaT</id>
            <title>我，一个95后，从阿里辞职与贾扬清去硅谷创业</title>
            <link>https://www.infoq.cn/article/8wWUiBa8eBWVLRMrxJaT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8wWUiBa8eBWVLRMrxJaT</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Oct 2023 06:26:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 咖啡馆, ChatGPT, AI, 创业
<br>
<br>
总结: 在旧金山的咖啡馆里，人们热烈地讨论着ChatGPT和人工智能。年轻的创业者鱼哲与贾扬清一起创办了LeptonAI，他们的目标是为厨师们提供一个优秀的中央厨房，让他们轻松获取所需的食材以便更好地准备菜肴。鱼哲的创业之路始于阿里云，他在阿里云工作期间积累了丰富的经验。现在，他们致力于探索AI的现状和发展，并将其应用于创业项目中。 </div>
                        <hr>
                    
                    <p>“在旧金山，随便进去一家咖啡馆，十分钟之内，你就会听到有人在谈论ChatGPT、AI。不管是不是有些天马行空，视线范围内的所有人都在尝试着融入和探索新的事物。”25岁决定与贾扬清一起在美国加利福尼亚州创业的鱼哲说道。</p><p>&nbsp;</p><p>鱼哲跟<a href="https://www.infoq.cn/article/Bg8*3spkPKCjCsw7MPR8">贾扬清</a>"的缘分始于阿里云。2020年，鱼哲本科毕业后入职阿里云，这是贾扬清进入阿里的第二年。当时，负责阿里云机器学习平台PAI产品线的鱼哲进入了贾扬清的团队，并与之共事了很久。2023年，贾扬清从阿里离职创业，鱼哲也选择加入这支队伍。</p><p>&nbsp;</p><p>“我非常认同扬清的创业方向，这个方向非常有趣。”鱼哲说道。在时代浪潮的推动下，每个人都在寻找自己的方向。鱼哲用这个中式的比喻来形容他们正在做的事情：我们不帮别人包饺子，而是为他们的厨师提供一个优秀的中央厨房，让厨师们可以轻而易举的获取所需的食材以便其能更好地准备自己的菜肴。</p><p>&nbsp;</p><p>那么，这个98年的“新秀”是如何一步步走向AI创业道路的？他们现在究竟在做什么样的事情？又是如何思考AI的现状和发展的呢？</p><p></p><h2>从高中开始就一直很“不正经”</h2><p></p><p>&nbsp;</p><p>2017年7月的一个周末，深圳的台风袭来，而几十位极客正在科技寺举办的黑客松上如火如荼地讨论各种项目，其中便有鱼哲的身影。</p><p>&nbsp;</p><p>在大二选择Gap Year时，鱼哲在编程猫担任算法工程师，业余时间利用图像识别和<a href="https://cloud.tencent.com/product/nlp?from_column=20065&amp;from=20065">自然语言处理</a>"技术，做了一个可以在对话中自动生成相应表情配合文字的程序，叫“表情包终结者（Meme Fighter）”，据说是因为他经常在微信群的表情包大战中惨败。</p><p>&nbsp;</p><p>两天内做出这样一个项目，对鱼哲来说并不是太难。</p><p>&nbsp;</p><p>当大多数人在为高考努力的时候，受素质教育影响的鱼哲被更愿意去探索不同的领域。那时的他对技术很感兴趣，除了一直关注最新的技术动态，他玩过单片机、也参与了一些机器人项目，算是积累了一些经验。后来在第一次接触JupyterLab时，遇到问题后的鱼哲会自己修复并提出bug报告，因此还被JupyterLab 创始人邀请参与到了项目中。</p><p>&nbsp;</p><p>举一反三也是鱼哲的强项。在编程猫工作时，他需要让模型能够应对大量业务流量。最开始无从下手，但当时听了“Instagram如何架构Python后端”的讲座后，鱼哲借鉴了其思路并实施到自己项目中，取得了不错效果。</p><p>&nbsp;</p><p>在鱼哲的成长过程中，实习工作是家常便饭，但也正是一次次的工作经历影响了他看待世界的方法，进而影响了他的职业选择。</p><p>&nbsp;</p><p>高中期间，鱼哲去了一家咨询公司做市场调研的工作。实际上，这份工作并不复杂：研究当时市场上的青少年科技夏令营主要做什么、定价情况、客户群体等，在收集到大量数据并进行分析后，推测当地人们的消费情况、对子女教育的投入等。</p><p>&nbsp;</p><p>“这种洞察力非常有趣，你可以通过一些有趣的数据看到其他人是如何生活的，就像有了上帝视角。”鱼哲说道。咨询公司对方法论和数据运用的重视也深刻影响了鱼哲，让鱼哲养成了“用数据看世界”的思维习惯。</p><p>&nbsp;</p><p>另外，这段实习经历也让鱼哲接触到了另一个跟技术无关的领域：商业运作。鱼哲开始思考将技术与商业结合起来。他认为，技术不能只停留在实验室中，只有真正落地并被大家接受和应用才能发挥更大的价值。</p><p>&nbsp;</p><p>于是，本科期间，鱼哲选择了去美国伦斯勒理工就读信息技术与网络科学专业（Information Technology and Web Science，ITWS），计算机学院和商学院各学两年，深入了解技术对商业变革的影响。根据规划，其最终的职业发展方向就是技术的落地及商业化。</p><p></p><h4>“阿里云最年轻的产品经理”</h4><p></p><p>&nbsp;</p><p>阿里云是鱼哲大学毕业后的第一份正式工作，22岁的他成了“阿里云史上最年轻的产品经理”。</p><p>&nbsp;</p><p>在阿里云，鱼哲更像是经历了一场“系统化训练”，用他的话就是，这次工作对他在“个人技术深度和广度方面的提升、个人职业规划的明朗，以及商业模式和市场的理解上，都产生了很大影响。”</p><p>&nbsp;</p><p>回忆起这段经历，鱼哲最先想到的是养成了“只要没干死，就往死里干”的态度。当时阿里云要研发很多新产品，刚入职的他心里憋着劲，将自己的工作节奏安排得非常紧：早上吃咖啡因含片，中午甚至只吃蛋白质代餐，一直工作到晚上九点或更晚。“年轻人总是会容易感动自己，以为这个世界离开了我就不行。”鱼哲笑着调侃当年的自己。</p><p>&nbsp;</p><p>鱼哲坦言自己经历了失败，“想要第一次尝试的事情也不总是正确的”，但周围阿里的同事给了他很大的包容，经过多次试错后最终可以找到正确的“打开方式”。这些努力也让他收获颇丰：经手业务一年里基本上都实现了二三十倍的增长。</p><p>&nbsp;</p><p>对鱼哲来说，“阿里云最年轻的产品经理”的标签，从某种程度上来说，代表着他年轻的特质。“年轻时，我们对许多东西都不懂，也不知道如何去应对，意识到‘自己不知道’很重要，更重要的是迎难而上的勇气和不断探索的精神”鱼哲解释道。</p><p></p><h2>选择创业，只能不停地学习</h2><p></p><p>&nbsp;</p><p>去年下半年，<a href="https://www.infoq.cn/article/iEkbUrxDh6c7svEbepKj">ChatGPT</a>"的爆火引发了AI狂潮，进而吸引了一批AI创业者，多年前就想创业的贾扬清这次终于下场。</p><p>&nbsp;</p><p>“在 AI 领域，模型的保鲜期基本上是一年左右。”贾扬清曾表示，因此他瞄准了需求更明确的方向：如何更好地部署模型，是否有更弹性的、更稳定的、更低成本的部署模式。不直接帮企业开发应用是因为许多情况下，用户比厂商更了解特定场景的实现细节，厂商无法深入解决专业领域的问题。</p><p>&nbsp;</p><p>已经在AI领域积累多年的鱼哲很认同贾扬清的观点，因此在阿里云工作三年的鱼哲加入了这个创业团队。“我的优势在于曾在甲方和乙方两方都工作过，对整体商业模式有较为深入的了解。我还有一段时间在海外工作、生活和学习，这些经历让我能更全面地看待问题。”鱼哲认真剖析了自己。</p><p>&nbsp;</p><p>如今，鱼哲在LeptonAI 担任产品负责人一职，他经常参加各种线下活动，通过与外界交流来了解市场和用户的需求，进而反推出自己应该做什么样的产品。</p><p>&nbsp;</p><p>对于鱼哲来说，大厂的很多工作相对来说都是可预测的，而现在的工作不确定性更强，但也更加让他兴奋。他如今需要更快速地学习，并充分利用自己之前的工作经验，来找到更好帮助用户实现自己AI落地的方法。</p><p>&nbsp;</p><p>没有固定的上下班时间、更注重结果，选择创业公司让他比之前更加忙碌。同时，像鱼哲这样的AI创业者，现在面临的最大挑战之一就是市场的不确定性：整个AI和机器学习领域变化迅速，每天都有新的机会和技术涌现，大家每天读论文的速度都跟不上发布速度，他们需要始终都要保持初学者的心态，不断学习和吸收新知识。</p><p>&nbsp;</p><p>“我也没有特别好的办法，只能尽力跟进最新进展，多与业内一些顶尖公司的专业人士交流，跟上这个快速发展的领域。”鱼哲说道。</p><p>&nbsp;</p><p></p><h2>“很难找出这样出色的团队”</h2><p></p><p>&nbsp;</p><p>作为一个创业公司，鱼哲所在的LeptonAI 现在主要将精力放在了三个方面：</p><p>&nbsp;</p><p>持续进行AI模型的前沿创新研究，涵盖训练、推理、编译等方面，不断提高模型从训练到生产环境等各个关键环节的竞争力；提升工程平台性能，确保整个工作流程更加高效；不断思考和调整商业模式，以确保公司在整体上保持竞争力。</p><p>&nbsp;</p><p><a href="https://www.lepton.ai/">LeptonAI </a>"的自信来自创始成员们此前资深的工作经验。创始人们在这些大厂多次带领团队实现技术和产品架构升级。比如贾扬清就曾在Meta将Pytorch打造为深受AI开发者们喜爱的框架的经历。这给 LeptonAI 的启示就是要与开发者“共鸣”：虽然Pytorch可能在性能方面不及静态图的TensorFlow，但它让开发者使用起来更方便。“我们对AI开发者的需求有很好的理解，知道他们在使用时可能遇到的问题。”</p><p>&nbsp;</p><p>除了“AI大神”贾扬清，团队很多成员之前都曾在阿里、Google、Meta和Uber等大厂工作，积累了在AI应用和AI框架方面的丰富经验。团队对云基础架构也有深入了解，能够充分利用各种云资源，包括完备的云服务商和基础的IDC。同时，新团队的成果，比如之前做的Llama 2 API 以及SDXL性能优化等，得到了开发者们认可和好评，这也让团队更加自信。</p><p>&nbsp;</p><p>“在业界，找出这样一支能够在这些方面都表现出色的团队是非常困难的。”鱼哲说道。</p><p>&nbsp;</p><p>至今为止，LeptonAI 仍然专注于开发面向应用和开发者的 AI 工具平台。不过，鱼哲也表示，顺势而为非常关键，“每个团队都需要建立自己的基本实力和核心竞争力，在此基础上，关键就看哪个团队能够更快地跟上技术热点的发展，并且能够充分利用已有的能力。”</p><p>&nbsp;</p><p>LeptonAI 不会制定过于详细的长期规划，而是倾向更灵活地应对局势，以月、周为周期来关注公司的目标和方向，不断调整和适应变化。</p><p>&nbsp;</p><p>比如，目前市场需求主要集中在大模型方面，公司则会在这方面相对投入更多资源。但这并不意味着LeptonAI 放弃了传统的深度学习或机器学习模型，因为很多企业实际上是混合模型的架构，这些传统模型并没有被舍弃。</p><p>&nbsp;</p><p></p><h2>怎么做好产品？</h2><p></p><p>&nbsp;</p><p>“我们不是过去传统意义上的服务提供者。”鱼哲强调，“我们是要将客户的行业专业知识转化为应用落地的加速器，而不是代替他们完成任务。”</p><p>&nbsp;</p><p>在对外交流过程中，鱼哲发现用户的需求多且细，比如企业很想使用一些机器学习和深度学习模型，但模型的复杂度是个阻碍；企业想在不将代码放在公共互联网上的情况下，利用代言模型来管理代码补全，但技术能力可能无法实现等。鱼哲团队要做的就是依靠工作经验找到其中确定性的东西，来解决用户真实存在的问题。</p><p>&nbsp;</p><p>当前，LeptonAI 的思路是：开发者用 Python 原生方式构建模型，无需学习容器或 Kubernetes；然后在本地调试和测试模型，再使用单个命令将它们部署到云端；之后，开发者可以通过简单、灵活的 API 在任何应用程序中使用模型。这个过程中，LeptonAI 还要帮开发者选择最适合应用程序的异构硬件，并做水平扩展来处理大量工作负载。</p><p>&nbsp;</p><p>为了方便开发者以更舒适的方式构建和打包AI应用，LeptonAI 提供了一个名为“光子（Photon）”的Python库，“光子无处不在，何时何地都能找到它，同时也象征着速度快的特性。”Photon最初是团队将机器学习模型、运行时环境以及工程代码有机结合的抽象概念。现在，Photon定义了一组处理程序和Python 依赖项，用户也可以根据情况构建自己的Photon。</p><p>&nbsp;</p><p>关于 Python作为AI服务框架的问题，业内目前存在一些争议，比如Python GIL是众所周知令人头疼的问题。为解决Python 带来的性能问题，大家的基本思路似乎是放弃Python：Hugging Face 用Rust 重写了一个 ML 框架、Modular 公司发布了名为 Mojo 的新编程语言。在鱼哲看来，Python 的应用取决于具体的使用场景。例如高频量化交易场景可能需要使用更低级别的语言来满足毫秒级延迟的要求，而在其他情况下，几十毫秒级别的延迟可能是可接受的。</p><p>&nbsp;</p><p>对于性能要求极高的场景，LeptonAI 会对原本在Python下进行的模型服务进行编译、推理、优化和加速等处理，进而保证其他方面的高效运行。比如部署在机器人或车辆上的应用，运行时资源非常有限，LeptonAI 会通过特殊的压缩手段来保持更高的性能，而用户端是无感的。</p><p>&nbsp;</p><p>LeptonAI 当前主要在公有云中提供全托管服务，但LeptonAI 给自己的定位和传统云厂商有些不同。“我们帮助客户制定自己的AI战略，这是很多厂商不提供的服务。我们能够提供很多云厂商无法提供的技术细节，我们比云厂商更深入了解AI。”鱼哲说道。</p><p>&nbsp;</p><p>目前LeptonAI 产品处于开放测试阶段，还在不断优化迭代和完善功能。比如团队推出了一个名为 <a href="https://www.infoq.cn/article/MPINGBSC8woTh558i7Fq">TUNA </a>"的功能，用户只需要上传语料，就能一键操作对模型进行微调。鱼哲总结自己产品的优势在开发者体验、价格成本和性能上。</p><p>&nbsp;</p><p>测试有时候也不仅仅针对产品，还有对开发团队心理的考验。“这个阶段，沮丧的事情有很多。”鱼哲说道，“当你抱着很高的期望尝试时，有时会发现某个基础组件并不稳定，或者是最初以为用户会非常喜欢的功能，实际做完后发现用户觉得很难用。”</p><p>&nbsp;</p><p>技术不断进步，总会有新的问题需要解决。在鱼哲看来，最重要的是保持冷静、坚定前行，因为很多事情并没有捷径可走。“这个道路上的坑也是多不胜数的，不要试图绕过，而是要努力填坑，并且越快越好。”</p><p></p><h4>承上启下的角色</h4><p></p><p>&nbsp;</p><p>现在，LeptonAI 的客户涵盖了金融、能源、自动驾驶以及信息互联网服务等领域。除了个别性能要求极高场景，LeptonAI 并不针对特定行业提供解决方案，更多是提供底层标准能力，方便用户快速应用。</p><p>&nbsp;</p><p>“我们处于一个承上启下的角色。因为在上游和下游的每个人，都有他们自己的客户（甲方）和供应商（乙方）。”鱼哲说道。</p><p>&nbsp;</p><p>LeptonAI 提供算力、模型和服务，服务方面包括通用流行模型的API服务、个性化模型的平台服务和对模型进行微调和部署的服务。这些能力背后需要计算、存储和网络三种资源支撑。LeptonAI 会从不同的供应商那里采购这些资源，包括传统云厂商和新兴云厂商。能够做好供应链整合、在价格上获得比竞争对手更大的优势，这也是LeptonAI 的核心竞争力之一。</p><p>&nbsp;</p><p>LeptonAI 的收费项主要有三部分：基于软件订阅的费用，私有模型部署的资源使用费用，和热门模型的使用费用。资源使用的定价逻辑是基于规格乘以使用时长的方式来计算。对于单位价格，LeptonAI 基于AWS、GCP、Azure等多个市场供应商来设定适当价格。</p><p>&nbsp;</p><p>鱼哲表示，LeptonAI 并不是基于各种成本来定价的，而是假设用户自己处理需要花费的成本，然后LeptonAI 在此基础上设定价格，目的是确保用户直接购买现成解决方案比自己做要更加划算。</p><p>&nbsp;</p><p>不过鱼哲强调，低成本并非是LeptonAI 的主打市场推广策略，同时还是要关注用户使用体验和产品性能。毕竟To B，从来就不是单个维度上的短跑，而是多个维度的长跑。</p><p>&nbsp;</p><p>此外，LeptonAI 也在积极融入整个行业发展中，以GitHub开源工具链SDK的方式来降低模型使用的门栏，让每一位AI开发者们通过一行命令即可拉起热门模型。</p><p>&nbsp;</p><p></p><h2>不能“拿着锤子找钉子”</h2><p></p><p>&nbsp;</p><p>关注AI多年，鱼哲这次感受到的一个显著变化是，人们不再是仅仅被炫酷的技术吸引后就不断投入资金进行尝试，反而会更加迅速地关注技术的实际应用和落地，更注重可行性和投资回报率（ROI）。人们变得更加理性，特别是在资本投入方面，也更加客观、认真地去思考技术如何落地。</p><p>&nbsp;</p><p>大模型因为聊天机器人被更多人熟知，但大模型不仅仅是聊天机器人。大模型的多模态特性可以将世界上的丰富多彩元素转化为机器可理解的格式。大模型的应用场景是非常广泛的。但对于大模型应用来说，最困难的不是训练模型，而是找到适合的应用场景和相应数据。</p><p>&nbsp;</p><p>鱼哲表示，开发大模型应用，行业经验和数据的质量是非常重要的因素：有足够的行业经验才能更好地理解目标受众的需求和应用场景；而数据的质量和多样性将直接影响模型的性能和效果。这两项确定后，拥有先发优势就非常关键，开发者一定要保持持一定的迭代速度。</p><p>&nbsp;</p><p>但在新技术落地上，找到场景也很难。“如果我现在只是拿着一个大模型去构建应用，那这就像拿着锤子找钉子。实际上，我们应该先有一个场景，然后再构建相应的应用。”鱼哲进一步说道，同时，大模型落地还需要企业里有既了解特定场景又熟悉相关技术、清楚什么能做什么不能做的人才，才能真正落地。</p><p>&nbsp;</p><p>本质上，大模型应用还处于非常早期的阶段，大多数应用仍停留在概念验证（POC）或短期上线能够使用的状态。就像Bing或者Google 搜索虽然落地了，但在特定领域的深度应用还在不断尝试中。</p><p>&nbsp;</p><p>“建议大家不要被大模型束缚住。实际落地时，除了大模型外，还可以充分利用许多已存在的深度学习模型或传统模型。例如在图像处理方面，卷积神经网络（CNN）实际上可能比大模型更适用。”鱼哲说道。</p><p>&nbsp;</p><p>如今，行业在大模型上基本形成了这样的共识：没必要一味追求大规模参数，开源会成为主流，通用大模型并不“通用”，垂直行业的大模型更被期待。鱼哲认为，下一步是努力消除基础能力和场景差距。这方面，AI Agent 被寄予厚望，人们希望借此解决单靠大模型无法解决的问题。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/73/731fcd2c495ee060a8019b81b687d7c1.png" /></p><p></p><p>AI Agent 示意图</p><p>&nbsp;</p><p>简单说来，AI Agent希望达成的效果是：一个独立思考的实体具备了多种技能，这些技能可以组合起来应用到生产中，最终交付出一个成果。其中，大模型充当了代理的大脑，并由Memory、Tools、Planning、Action几个关键组件进行补充。</p><p>&nbsp;</p><p>鱼哲设想的一个Agents应用场景是交互式搜索，比如用户去某地方开会，智能助手可以除了导航还可以提示哪里可以停车等。鱼哲始终认为，技术否能够成功取决于它是否能与特定场景良好结合，停留在实验室内的技术不见天日更难有机会被打磨，因此更接近场景的人其实更有机会。</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“我无法设想 AI 不再流行的情景。”鱼哲说道，“AI 代表了一种信息处理的方式，而人类对于信息处理方式的投入只会越来越多，不会减少。”鱼哲预计，人工智能的进步和发展会越来越深入和持久，自己也会持续在这个行业深耕下去。</p><p>&nbsp;</p><p>鱼哲坦言，自己最擅长的领域仍然是人工智能。在这个领域工作久了，他逐渐意识到，技术落地的过程比想象的复杂得多，有些事很多时候更像是一场马拉松，而不是一次短跑。他现在的首要目标是和团队一起帮助LeptonAI 发展壮大，在这个前提下，继续秉持自己的兴趣前行。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/U5N1PsYQpVIpu3ThbIN6</id>
            <title>两行代码解决大模型对话局限，港中文贾佳亚团队联合MIT发布超长文本扩展技术</title>
            <link>https://www.infoq.cn/article/U5N1PsYQpVIpu3ThbIN6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/U5N1PsYQpVIpu3ThbIN6</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Oct 2023 06:07:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: LongLoRA, 大模型对话缺陷, 70B模型, 长文本对话大语言模型
<br>
<br>
总结: 贾佳亚团队与MIT发布了名为LongLoRA的新技术，通过分组和偏移的方式解决了大模型对话缺陷，使得70B模型的文本长度可以拓展到32k tokens。同时，他们还发布了拥有70B参数量的长文本对话大语言模型LongAlpaca。 </div>
                        <hr>
                    
                    <p>近日，贾佳亚团队联合MIT发布了一项名为LongLoRA的新技术，只需两行代码、一台8卡A100机器，便可将7B模型的文本长度拓展到100k tokens、70B模型的文本长度拓展到32k tokens。同时，该研究团队还发布了首个拥有70B参数量的长文本对话大语言模型LongAlpaca。</p><p></p><h2>LongLoRA 如何解决大模型对话缺陷</h2><p></p><p>&nbsp;</p><p>“上下文越长大模型越笨”是典型的大语言模型对话缺陷。在长文本处理过程中，之前大语言模型计算量的主要开销集中在自注意力机制(self-attention)，其开销随着文本长度成平方次地增加。针对这个问题，研究团队提出LongLoRA技术，并用分组和偏移的方式来对全局自注意力机制进行模拟。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/70/70a810b92e075263b2e2a3281b062b01.png" /></p><p></p><p>简单来说，就是将长文本对应的tokens拆分成不同的组，在每组内部做自注意力计算，而分组的方式在不同注意力头&nbsp;(attention head) 上有所偏移。这样的方式既可以大幅度节约计算量，又可以维持全局感受野的传递。而这个实现方法也非常简洁，仅两行代码即可完成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6aad8dba3c72ae4947e2aab04fe01c0e.png" /></p><p></p><p>LongLoRA还探索了低秩训练的方式。原有的低秩训练方式，如LoRA [5]，无法在文本长度迁移上取得良好的效果。而LongLoRA在低秩训练的基础上，引入嵌入层&nbsp;(Embedding layer和 Normalization layers)&nbsp;进行微调，从而达到可以和全参数微调 (Full fine-tune) 逼近的效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f7959f48537fc89bf6cb73e836ae3df0.png" /></p><p></p><p>进行不同长度文本扩展和训练时，LongLoRA、LoRA和全参数微调不同技术的具体表现如下：</p><p>&nbsp;</p><p>在Perplexity-困惑度上，原有LoRA方法的性能在不断恶化，而LongLoRA和全参数微调都能在各种文本长度下维持很好的效果；在显存消耗上，相比于全参数微调，LongLoRA和原有LoRA都有大幅度的节省。例如，对于8k长度的模型训练，相比于全参数微调，LongLoRA将显存消耗从46.3GB降低到25.6GB；在训练时间上，对于64k长度的模型训练，相比于常规LoRA，LongLoRA将训练时间从90～100小时左右降低到52.4小时，而全参数微调超过1000小时。</p><p></p><p>目前，相关技术与模型已全部开源：</p><p>&nbsp;</p><p>代码和Demo地址：<a href="https://github.com/dvlab-research/LongLoRA">https://github.com/dvlab-research/LongLoRA</a>"</p><p>论文地址：<a href="https://arxiv.org/pdf/2309.12307.pdf">https://arxiv.org/pdf/2309.12307.pdf</a>"</p><p>&nbsp;</p><p></p><h2>长篇小说读后分析，LongAlpaca完胜Llama2</h2><p></p><p>&nbsp;</p><p>LongAlpaca大语言模型，利用LongLoRA技术解决了对话缺陷问题。但大语言模型处理长文本问题的一大难点还在于缺少公开的长文本对话数据。</p><p>&nbsp;</p><p>为此，研究团队特意收集了9k条长文本问答语料对，包含针对名著、论文、深度报道甚至财务报表的各类问答，此外还挑选了3k的短问答语料与9K的长问答语料混合训练，让长文本大模型同时具备短文本对话能力。这个完整的数据集被称为LongAlpaca-12k，目前已经开源。</p><p>&nbsp;</p><p>在LongAlpaca-12k数据集基础上，研究团队对不同参数大小7B、13B、70B进行了训练和评测，开源模型包括LongAlpaca-7B、LongAlpaca-13B和LongAlpaca-70B。下面是LongLoRA技术叠加12K问答语料的大模型LongAlpaca在论文方面表现：</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50194fb46842750d702e4f1a63c84faf.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>让系统新读一篇论文，并根据ICLR的审查指南，对其提出修改意见，从而提升该论文的接收率。&nbsp;LongAlpaca的意见是：通过更精确地阐明新颖性，提供更严格和更有对比性的实验结果(包括具体的数据集和指标)、更广泛的应用和未来发展方向，重点呈现关键贡献和影响，论文被接受的机会将得到提高。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/29/299a67454b8e96bedbeefbbf655d7141.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>让系统读两篇新的不同的论文，让LongAlpaca概括ICLR和CVPR两个会议之间的风格区别。&nbsp;LongAlpaca总结认为，CVPR论文倾向更具结构性和实验性的风格，专注于实用性和技术性。而ICLR的论文风格更加灵活，侧重关键的理论分析和数学推导，而非标准格式。&nbsp;可以看出，经过训练的LongAlpaca模型已经可以很轻松地接受新的长篇学术论文，在学术相关问题的回答上相当精准。</blockquote><p></p><p>&nbsp;</p><p>LongAlpaca在颇高阅读和理解门槛的经济领域的解读表现：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2b11b8e95cdefa727b358f4bef382c3.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f805ddb1b64be78f4c8c25c0a77c7c48.png" /></p><p></p><p></p><blockquote>根据国际货币基金组织从2012年到2023年的全球经济展望的概述合集，对经济形势进行逐年总结以及对未来经济趋势进行预测。&nbsp;LongAlpaca表示，2023年全球经济形势不明朗，跟2022年相比，增长预计放缓；未来几年，全球经济扩张速度偏温和并将低于预期，而地缘政治紧张、金融市场波动等大的挑战依然存在，需通过多边合作和结构性改革小心应对。</blockquote><p></p><p>&nbsp;</p><p>13B参数的LongAlpaca和Llama2在读完长篇小说后的内容分析对比：</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/0931e6dfc10c170bffe91c8ce5c3d6bb.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>名著《西游记》中，为什么孙悟空在大闹天宫时很厉害，但取经路上却屡屡受挫？&nbsp;LongAlpaca 给出了5点原因，大概可总结为： “孙悟空的不成熟、强大的对手、力量的限制、敌人的欺骗、最初缺少同伴等因素导致了他在旅途中的失败。但随着时间推移，他获得了智慧、经验和战友，使他最终战胜了最强大的敌人。”&nbsp;而 Llama2 的回答较为简单：“虽然他确实大闹天宫，但说他代表着不败的力量并不一定准确。事实上，他最终在西游记中被强大的唐三藏打败。”Llama2没有给出正确答案，并且还提出需要更多的内容信息。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/63/63852dbc48a17b22d9f25ad96a71289a.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>让系统新读一次《三体》，然后问第一部中，叶文洁为什么要联系外星人，而后期又为什么后悔？&nbsp;LongAlpaca认为，叶文洁最初接触外星人是出于理想主义和复仇，但随着时间的推移，她对人类的看法发生了变化，意识到自己可能给全人类带来的危险，她的悔恨与日俱增。&nbsp;而Llama2的回答则是，“通过与外星人的互动，她希望能更深入地了解宇宙的本质，以及自己在宇宙中的位置。叶文洁逐渐意识到，外星人和他们的技术并不能解决她的问题。”</blockquote><p></p><p>&nbsp;</p><p>从模型给出的答案可看出，一些模型如Llama2，可能在预训练过程中见过相关小说，但如果在提问时进行仅根据小说题目进行短文本提问的话，回答并不理想。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Y2lof4tq2TOqiHcN8L2g</id>
            <title>DeepMind全新AI项目曝光：可控制各类机器人，数据集有望开源</title>
            <link>https://www.infoq.cn/article/Y2lof4tq2TOqiHcN8L2g</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Y2lof4tq2TOqiHcN8L2g</guid>
            <pubDate></pubDate>
            <updated>Mon, 09 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: DeepMind, 通用AI系统, 机器人技术, Open X-Embodiment
<br>
<br>
总结: 谷歌DeepMind团队及其他研究机构共同发起的新项目旨在创建一套通用AI系统，用于解决机器人技术中的挑战。该系统能够与不同类型的物理机器人协同运作，成功执行多种任务。通过引入包含22种机器人类型数据的数据集和能够进行技能迁移的模型RT-1-X，研究人员成功克服了为每项任务、每台机器人和每种环境分别训练模型的问题。这个项目的目标是创建一套优于专用模型的通用模型，能够驱动所有类型的机器人。 </div>
                        <hr>
                    
                    <p></p><h2>DeepMind的新项目是什么？</h2><p></p><p>&nbsp;</p><p>开发机器人技术的一大挑战，就在于必须投入大量精力来为每台机器人、每项任务和每种环境训练机器学习模型。近日，谷歌DeepMind团队及其他33个研究机构正共同发起新项目，旨在创建一套通用AI系统来应对这个挑战。据称该系统能够与不同类型的物理机器人协同运作，成功执行多种任务。</p><p>&nbsp;</p><p>谷歌机器人部门高级软件工程师Pannag&nbsp;Sanketi在采访中表示，“我们观察到，机器人在专项领域表现极佳，但在通用领域却缺乏灵性。一般来讲，大家需要为每项任务、每台机器人和每种环境分别训练一套模型，从零开始调整每一个变量。”</p><p>&nbsp;</p><p>为了克服这个问题，让机器人的训练和部署变得更加轻松、快捷，谷歌DeepMind在名为Open X-Embodiment的大型共享数据库项目中引入了两大关键组件：一套包含了22种机器人类型数据的数据集，外加一系列能够跨多种任务进行技能迁移的模型 RT-1-X（这是一个源自RT-1的机器人变压器模型）。为了开发 Open X-Embodiment 数据集，研发人员在超过 100万个场景中展示了500多种技能和150,000项任务，因此，该数据集也是同类中最全面的机器人数据集。</p><p>&nbsp;</p><p>此外，研究人员还在机器人实验室和不同类型的物理装置之上对模型进行了测试，并发现与传统机器人训练方法相比，新方案确实能取得更好的成绩。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/782e8f89cce0fdce5401f39e87303470.png" /></p><p></p><p>&nbsp;来自 Open X-Embodiment 数据集的样本展示了 500 多种技能和 150,000 项任务。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/45/45211e40b4438330727247df84fe16e8.png" /></p><p></p><p>&nbsp;Open X-Embodiment 数据集结合了跨实施例、数据集和技能的数据。</p><p>&nbsp;</p><p></p><h2>结合机器人数据</h2><p></p><p>通常来讲，不同类型的机器人往往拥有独特的传感器和执行器，所以需要配合专门的软件模型。这就类似于不同生物体的大脑和神经系统需要专门进化，从而适应该生物的身体结构与所处环境。</p><p>&nbsp;</p><p>但Open X-Embodiment的诞生却出于这样一条先验性的假设：将来自不同机器人和任务的数据结合起来，就能创建一套优于专用模型的通用模型，足以驱动所有类型的机器人。这个概念在一定程度上受到大语言模型（LLM）的启发，即在使用大型通用数据集进行训练时，模型成果的匹配度甚至可以优于在特定数据集上训练的小型针对性模型。而研究人员惊喜地发现，此项原理果然也适用于机器人领域。</p><p>&nbsp;</p><p>为了创建Open X-Embodiment数据集，研究团队收集了来自不同国家20个机构的22台机器人具身的真实数据。该数据集包含超100万种情节（所谓情节，是指机器人每次尝试执行任务时所采取的一系列动作），其中具体涉及500多种技能和15万个任务示例。</p><p>&nbsp;</p><p>随附的各模型均基于Transformer，一套在大语言模型中也得以应用的深度学习架构。RT-1-X建立在Robotics Transformer 1（简称RT-1）之上，是一套适用于在真实环境下实现机器人技术规模化的多任务模型。RT-2-X则建立在RT-1后继者RT-2的基础之上——RT-2是一种视觉语言动作（VLA）模型，能够从机器人和网络数据中学习，并具备响应自然语言命令的能力。</p><p>&nbsp;</p><p>研究人员在五所不同研究实验室的五台常用机器人上测试了RT-1-X对各类任务的执行能力。与针对这些机器人开发的专用模型相比，RT-1-X在拾取和移动物体、以及开门等任务上的成功率高出50%。该模型还能将技能迁移至多种不同环境，这也是在特定视觉场景下训练出的专用模型所做不到的。由此可见，由不同示例集训练而成的模型在大多数任务中都优于专用模型。论文还提到，此模型适用于从机械手臂到四足动物在内的多种机器人。</p><p>&nbsp;</p><p>加州大学伯克利分校副教授、论文联合作者Sergey Levine写道，“对于任何曾有机器人研究经验的朋友来说，都能意识到这是多么了不起：这类模型「从来」就没能第一次就尝试成功，但这个模型却做到了。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/89/89fcd22e3582fb86c6284da0223f93f8.png" /></p><p></p><p>&nbsp;值得注意的是，即使是规模较小的RT-1-X模型，也实现了对各实验室内部专用模型的超越！对于任何曾有机器人研究经验的朋友来说，都能意识到这是多么了不起：这类模型“从来”就没能第一次就尝试成功，但这个模型却做到了。</p><p>&nbsp;</p><p>在应急技能和处理训练数据集中未涉及的新任务方面，RT-2-X的成功率可达RT-2的3倍。具体来讲，RT-2-X在需要空间认知的任务上表现出更好的性能，例如理解“将苹果放到布旁边”和“将苹果放到布上”两种要求间的区别。</p><p>&nbsp;</p><p>研究人员在Open X和RT-X的发布博文中写道，“我们的结果表明，与其他平台的数据进行联合训练之后，RT-2-X获得了原始数据集中并不具备的额外技能，使其能够执行前所未见的新任务。”</p><p></p><h2>步步迈向机器人研究的新未来</h2><p></p><p>展望未来，科学家们正在考虑将这些进展与DeepMind开发的自我改进模型RoboCat的见解相结合，希望探索出新的研究方向。RoboCat能够学会在不同机械臂上执行各种任务，然后自动设计出新的训练数据以提高自身性能。</p><p>&nbsp;</p><p>Sanketi认为，另一个潜在的研究方向，也可能是进一步研究不同数据集间的混合会如何影响跨机器人具身的能力泛化与改进效果。</p><p>&nbsp;</p><p>该团队目前已经开源了Open X-Embodiment数据集和小型RT-1-X模型，但并未公开RT-2-X模型。</p><p>&nbsp;</p><p>Sanketi总结道，“我们相信，这些工具将改变机器人的训练方式，并加速该领域的研究进展。我们希望开源相关数据，并提供安全但受限的模型以减少障碍、加速研究。机器人技术的未来离不开机器人之间的相互学习，而这一切的前提，首先要求研究人员之间能够相互学习。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://venturebeat.com/ai/deepminds-remarkable-new-ai-controls-robots-of-all-kinds/">https://venturebeat.com/ai/deepminds-remarkable-new-ai-controls-robots-of-all-kinds/</a>"</p><p><a href="https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types">https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Muye0Jrc84wvbzePFjaH</id>
            <title>RISC-V成新战场？美国议员：限制美企参与开发RISC-V开源技术，并纳入出口管制</title>
            <link>https://www.infoq.cn/article/Muye0Jrc84wvbzePFjaH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Muye0Jrc84wvbzePFjaH</guid>
            <pubDate></pubDate>
            <updated>Sun, 08 Oct 2023 06:50:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 拜登政府, RISC-V, 芯片技术, 全球科技行业
<br>
<br>
总结: 拜登政府面临来自议员的压力，要求限制美国公司开发在中国广泛使用的RISC-V芯片技术，可能会颠覆全球科技行业的跨境合作方式。 </div>
                        <hr>
                    
                    <p>据路透社近日报道，拜登政府正面临来自一些议员的压力，要求限制美国公司开发一种在中国广泛使用且不受限制的芯片技术——此举可能会颠覆全球科技行业的跨境合作方式。</p><p>&nbsp;</p><p>据悉，本次争论的焦点是 RISC-V。RISC-V 是一个基于精简指令集（RISC）原则的开源指令集架构（ISA）。2010 年，开源指令集架构 RISC-V 首次出现在美国加州大学伯克利分校，其开源架构的形式很快就吸引了包括 IBM、恩智浦、WeaternDigital、NVIDIA、Qualcomm、三星、Google、华为、Tesla 等各大厂商的加盟。</p><p>&nbsp;</p><p>与大多数指令集相比，RISC-V 指令集可以自由地用于任何目的，允许任何人设计、制造和销售 RISC-V 芯片和软件。虽然这不是第一个开源指令集，但它具有重要意义，因为其设计使其适用于现代计算设备（如仓库规模云计算机、高端移动电话和微小嵌入式系统）。设计者考虑到了这些用途中的性能与功率效率。该指令集还具有众多支持的软件，这解决了新指令集通常的弱点。</p><p>&nbsp;</p><p>然而，一些美国议员（包括两名共和党众议院委员会主席、共和党参议员马可·卢比奥和民主党参议员马克·沃纳）以国家安全为由，敦促拜登政府对 RISC-V 采取行动。议员们表示，中国正在利用美国公司之间开放合作的文化来发展自己的半导体产业，这可能会削弱美国目前在芯片领域的领先地位。</p><p>&nbsp;</p><p>众议院中国问题特别委员会主席众议员 Mike Gallagher 在给路透社的一份声明中表示，商务部需要“要求任何美国个人或公司在与中华人民共和国实体就RISC-V相关贸易往来之前获得出口许可证”。</p><p>&nbsp;</p><p>代表迈克尔众议院外交事务委员会主席Michael McCaul在给路透社的一份声明中表示，“中国正在滥用 RISC-V 来规避美国在设计芯片所需知识产权方面的主导地位。美国人不应该支持中国的技术转让战略，因为这会削弱美国的出口管制法，”McCaul表示，他希望美国商务部负责监督出口管制法规的工业与安全局采取行动，如果没有落实，他将寻求立法。</p><p>&nbsp;</p><p>美国商务部发言人在一份声明中称，该局“正在不断审查技术形势和威胁环境，并不断评估如何最好地应用我们的出口管制政策来保护国家安全和核心技术”。</p><p>&nbsp;</p><p>经过十余年的发展，RISC-V 生态不断壮大。当前，有越来越多的中国企业积极参与到 RISC-V 国际生态建设中。在此前接受 InfoQ 采访时，不少受访专家提到，公司正积极拥抱 RISC-V。如果拜登政府对 RISC-V 采取行动，限制美国企业参与RISC-V开发，不仅会影响中国突破芯片自主，也会阻碍美国和欧洲制造更便宜、更多功能的芯片。</p><p>&nbsp;</p><p>总部位于加利福尼亚州使用 RISC-V 的初创公司 SiFive 的业务开发副总裁 Jack Kang 表示，美国政府对美国公司在 RISC-V 方面的潜在限制将是一场“巨大的悲剧”。 “这就像禁止我们在互联网上工作一样，”Kang 说。“就技术、领导力、创新以及正在创造的公司和就业机会而言，这将是一个巨大的错误。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/us-china-tech-war-risc-v-chip-technology-emerges-new-battleground-2023-10-06/">https://www.reuters.com/technology/us-china-tech-war-risc-v-chip-technology-emerges-new-battleground-2023-10-06/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>