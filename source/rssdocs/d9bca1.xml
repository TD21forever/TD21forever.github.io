<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</id>
            <title>大厂期权归属前遭暴力裁员，80 余万期权泡汤；去哪儿宣布每周两天“不坐班”；萝卜快跑是人类远程代驾？客服：无人操控 | Q资讯</title>
            <link>https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 07:31:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, AI服务, 期权, 裁员
<br>
<br>
总结: 苹果公司计划增加iPhone 16系列机型的出货量，依靠AI服务提振需求；大厂员工因裁员导致期权泡汤，引发法律纠纷。萝卜快跑无人驾驶服务被曝有真人干预，百度设立无人驾驶实验基地应对问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>苹果大动作！AI 服务或为关键驱动力；大厂期权归属前遭暴力裁员，80余万期权泡汤；“萝卜快跑”有真人干预？腾讯被爆调薪！微软和苹果双双放弃 OpenAI 董事会观察员席位；谷歌开放“暗网报告”功能；中国区员工只能用 iPhone？微软回应；抖音宣布推出抖音 VR 直播；去哪儿网正式推行“3+2”混合办公模式；马斯克叫停与甲骨文的 100 亿美元谈判；“WPS 崩了”：三周内第二次；RockYou2024 文件泄露，数百万用户信息暴露；二季度 PC 出货量增长 3%；开源代码编辑器 Zed 发布原生 Linux 版本；Java 之父 James Gosling 宣布退休……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>苹果大动作！iPhone&nbsp;16出货预期大增，AI服务或为关键驱动力</h4><p></p><p>彭博社7&nbsp;月&nbsp;11&nbsp;日报道，表示苹果公司已经通知其供应商和合作伙伴，**2024&nbsp;年&nbsp;iPhone&nbsp;16&nbsp;系列机型的出货量目标要比&nbsp;iPhone&nbsp;15&nbsp;系列（8100&nbsp;万台）增长&nbsp;10%，至少要超过&nbsp;9000&nbsp;万台，**以期借助人工智能（AI）服务带来的潜在需求扫除公司2023年遭遇的阴霾。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7cedef26a96016811c9384717d707bcf.png" /></p><p></p><p>这印证了本周早些时候产业链人士的说法。知情人士表示，苹果告诉供应商和合作伙伴，其新款iPhone的出货量将较前几款增加约10%。与之相比，在2023年下半年，iPhone&nbsp;15的出货量约为8100万部。</p><p></p><p>根据知情人士的说法，苹果已经变得越来越有信心，这家科技巨头可能认为，其推出的个人智能化系统“Apple&nbsp;Intelligence”（苹果智能）中的一些功能将有助于提振iPhone&nbsp;16上市时的需求。</p><p></p><p>上月，苹果在年度全球开发者大会（WWDC）上披露了公司在AI方面新的进展，包括与OpenAI构建合作伙伴关系，推出能够优先置顶推送、总结文本、生成图片的套件，更强大的Siri等。</p><p></p><h4>大厂期权归属前遭暴力裁员，80&nbsp;余万期权泡汤</h4><p></p><p>近日，综合凤凰网和澎湃新闻消息，在得物兢兢业业工作两年后，前员工徐凯决定和老东家“对簿公堂”。</p><p></p><p>一年前，徐凯多次与公司沟通取得期权再离职未果后，到上海市仲裁委员会处申请恢复与得物的劳动关系，后被予以支持。2024&nbsp;年&nbsp;7&nbsp;月，因不服上海市仲裁委员会裁定的结果，得物继续上诉，再度将前员工诉于法庭之上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed62eadbc21f8bfc6749ed84519e39aa.webp" /></p><p></p><p>据徐凯表述，其在&nbsp;2021&nbsp;年&nbsp;9&nbsp;月加入该公司，任职为前端技术专家，税前薪资为月薪&nbsp;5&nbsp;万元左右，同时其握有部分期权。徐凯在工作期间，一直保持高效且敬业的态度，未曾出现过任何显著的工作失误。然而，在&nbsp;2023&nbsp;年&nbsp;7&nbsp;月，公司却以“未按时提交周报”和“工作时间分配不当”等理由，对他进行了绩效评估，最终给出了最低等级&nbsp;C&nbsp;的评价。紧接着，该企业启动了裁员流程，意图解除与徐凯的劳动合同。</p><p></p><p>尤为令人关注的是，此时距离徐凯手中期权的归属期满仅剩一个多月。他持有的近&nbsp;2000&nbsp;股期权，按原计划将在&nbsp;2023&nbsp;年&nbsp;9&nbsp;月行权大约&nbsp;1000&nbsp;股，价值高达&nbsp;80&nbsp;余万元。然而，裁员决定一旦生效，这些期权将自动失效，导致徐凯的潜在收益瞬间蒸发。</p><p></p><p>徐凯还表示，他在该公司工作期间常年面临着“10106”的局面，晚上&nbsp;10&nbsp;点下班是常态，且技术员工会被计算总工时，工时靠后的人就面临着被淘汰的风险。“我一开始是带团队的，风险就很高”。得物相关负责人针对以上情况回复凤凰网表示称，“该员工曾因&nbsp;3&nbsp;次绩效考核不合格（2022&nbsp;年&nbsp;Q3&nbsp;季度、2023&nbsp;年&nbsp;Q1&nbsp;季度、2023&nbsp;年&nbsp;Q2&nbsp;季度），于去年&nbsp;7&nbsp;月已经沟通解除劳动合同”。此外，得物方面还表示，“得物公司业务健康发展，欢迎优秀人才加入，感谢关注。”</p><p></p><p>另外，2023&nbsp;年&nbsp;7&nbsp;月，京东司法拍卖网披露的消息显示，“上海市中山南路&nbsp;566&nbsp;弄”一处房屋以&nbsp;1.58&nbsp;亿元成交，较起拍价&nbsp;1.25&nbsp;亿元溢价&nbsp;26.4%。根据竞拍结果显示，该套房屋由杨冰拍下。据澎湃新闻报道，该名自然人杨冰为得物创始人兼&nbsp;CEO。</p><p></p><h4>“萝卜快跑”被曝有真人干预？</h4><p></p><p>最近有网图流传，揭示了关于“萝卜快跑”无人驾驶服务的新视角——其背后竟隐藏着真人远程代驾的运作模式。图片聚焦于萝卜快跑的智控中心，清晰可见有专业人员坐在配备模拟方向盘的监控屏幕前，精准操控着车辆。</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/4512c73dde1c4c5097579a4a8129212b.webp" /></p><p></p><p>据媒体报道，百度设立的无人驾驶实验基地，其核心功能之一便是应对无人驾驶车辆可能遭遇的棘手问题，通过云端安全员的远程介入，确保车辆能够安全脱困。</p><p></p><p>鉴于极端驾驶场景并非常态，云端驾驶员相比随车安全员展现出更高的效率与灵活性。他们能以“一对多”的模式，同时服务于多辆无人驾驶车辆，极大地提升了资源利用率与响应速度。知情人士进一步透露，在无人驾驶网约车的运营体系中，后台座舱内的一名安全员能够高效监控并管理&nbsp;3&nbsp;至&nbsp;5&nbsp;台车辆，这些安全员多具备丰富的驾驶经验，来自网约车司机、公交车司机等职业背景。</p><p></p><p>此外，随着&nbsp;2023&nbsp;年&nbsp;11&nbsp;月交通运输部办公厅发布的《自动驾驶汽车运输安全服务指南&nbsp;(试行)》的正式实施，明确了在特定区域内运营的完全自动驾驶出租车，可采用远程安全员模式，并规定了远程安全员与车辆之间不得低于&nbsp;1:3&nbsp;的人车比，为无人驾驶行业的规范化发展提供了政策指引。</p><p></p><h4>腾讯被爆调薪！年底十三薪分摊到月薪</h4><p></p><p>7月10日，腾讯内部向全员发布邮件称，将调整内部的薪酬福利政策，一是将年底十三薪分摊到月薪上；二是将现有的易居租房补贴融入月薪。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03fb1bdb72e4e972cc75fd34872b2917.png" /></p><p></p><p>图片来源于网络</p><p></p><p>腾讯表示，过去两年，外部环境的变化让不少员工对于即时稳定的现金流有了更高诉求。经过慎重评估后，决定今年除正常进行年度薪酬回顾外，还将对薪酬结构做出两个调整：一是进行全公司薪酬结构的调整，把服务奖融入月薪；二是将现有的易居租房补贴融入月薪。</p><p></p><p>腾讯邮件中称，这两个举措旨在帮助大家在更高、更稳定的月收入基础上更安心地安排工作与生活。相关调整于2024年7月1日起生效，8月5日的发薪中开始体现。</p><p></p><p>腾讯表示，自成立之初便在年终为员工提供额外的十三薪，以此作为对员工一年辛勤工作的认可。随着公司规模的扩大，腾讯还引入了与业绩挂钩的奖金制度，形成了包括服务奖、绩效奖和特别奖在内的年终奖金体系，其中服务奖主要取决于员工的月工资和全年的工作时长，实质上是固定工资的补充部分。</p><p></p><p>此次将以年度薪酬回顾后的月薪为标准，把服务奖平均分摊融入到12个月固定工资中，以提升员工的月度现金流，未来年终奖也将回归到纯粹的业绩激励。</p><p></p><p>相关知情人士表示，对员工而言，这次将十三薪和租房补贴融入到月薪之后，每月员工到手的收入将增加，也比较利好生活压力比较大的应届生和有房贷的员工。</p><p></p><h4>微软和苹果双双放弃OpenAI董事会观察员席位</h4><p></p><p>据金融时报7月10日报道，在全球监管机构对大型科技公司投资AI初创企业的审查日益加剧之际，美国两大科技巨头微软和苹果都放弃了在OpenAI董事会的观察员席位——微软宣布立即退出OpenAI董事会的观察员席位，苹果也不会担任此类职务。这就意味着，OpenAI&nbsp;董事会不再设立无投票权的观察员席位。</p><p></p><p>鉴于全球监管部门审查科技巨头投资&nbsp;AI&nbsp;初创企业的活动越来越严格，微软已放弃了其在&nbsp;OpenAI&nbsp;董事会观察员的席位，而苹果将不会担任类似的职位。</p><p></p><p>微软已向&nbsp;OpenAI&nbsp;投资了&nbsp;130&nbsp;亿美元，微软在致&nbsp;OpenAI&nbsp;的一封信中表示，退出&nbsp;OpenAI&nbsp;董事会的席位“立即生效”。</p><p></p><p>据一位直接了解此事的人士透露，作为将&nbsp;ChatGPT&nbsp;整合到苹果设备的交易的一部分，外界原本预计苹果也将在&nbsp;OpenAI&nbsp;董事会中担任观察员角色，但现在它不会这么做。苹果拒绝置评。</p><p></p><h4>谷歌开放“暗网报告”功能：网罗安全事件、通知用户信息泄露</h4><p></p><p>据消息，谷歌公司7月10日宣布将于本月底向所有谷歌账号用户开放“暗网报告”功能，帮助用户更快了解网络上发生的个人数据泄露事件。 “暗网报告”功能此前仅限于购买&nbsp;Google&nbsp;One&nbsp;订阅的功能，主要监控常规网络方式无法访问的网络部分，除了排查个人信息是否已经泄露之外，还可以搜索相关漏洞信息。</p><p></p><p>谷歌在公告中表示&nbsp;Google&nbsp;One&nbsp;本月底将不再提供“暗网报告”功能，用户登录账号后可以免费访问。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61bcd4c8ceef1f69f566f497ec161cce.png" /></p><p></p><p>谷歌用户登录账号之后，可以打开“关于你的结果”页面，查找近期信息泄露事件中是否包含你的个人信息。“暗网报告”功能目前已经在全球&nbsp;46&nbsp;个国家和地区上线，可以检查与你的姓名、电子邮件、地址、用户名和密码相关的数据，如果在数据泄露事件中发现用户个人信息，就会第一时间通知用户。</p><p></p><h4>&nbsp;中国区员工只能用 iPhone？微软回应</h4><p></p><p>近日，有媒体报道称，微软已要求中国员工不能使用安卓手机，时间是从今年9月份开始。报道中提到，**微软已告知其中国员工，从今年9月份开始，他们只能在工作中使用iPhone，**此举实际上将安卓设备排除在了工作场所之外。</p><p></p><p>根据业内说法，该措施是微软全球“安全未来计划”（Secure&nbsp;Future&nbsp;Initiative）的一部分，将影响中国的数百名员工，旨在确保所有员工使用微软身份验证器Microsoft&nbsp;Authenticator（微软开发的一款应用）和Identity&nbsp;Pass等应用程序。</p><p></p><p>7月9日晚间，微软发言人就此事回应媒体记者称：**“Microsoft&nbsp;Authenticator和Identity&nbsp;Pass应用程序已正式在Apple&nbsp;Store（软件商店）和Google&nbsp;Play&nbsp;Store上架。我们希望为员工提供访问这些必要应用程序的途径，由于本地区无法使用Google移动服务，我们即向员工提供了例如iOS设备的选择。”**对此，不少网友称，若是能配发工作机就没问题。但若强制要求员工自行购买，则“不能接受”。</p><p></p><h4>抖音宣布推出抖音VR直播</h4><p></p><p>7月10日消息，抖音集团正式宣布推出抖音VR直播功能，用户现在可以在Apple&nbsp;Vision&nbsp;Pro设备上下载并体验这一创新功能。据了解，该软件支持180°、360°全景直播，即使不在现场，用户也能有身临其境的感觉，实时沉浸式看直播。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee5daa14e98e80bfd9ac36944b276734.png" /></p><p></p><p>**据介绍，抖音VR直播支持小范围6DoF&nbsp;3D直播，可进行180°或360°全景直播，用户可多屏切换、发送3D礼物。**Vision&nbsp;Pro是苹果首款头显设备，定位是MR（混合现实）设备。</p><p></p><p>MR技术结合AR和VR，通过加强虚拟对象与现实世界的交互，实现混合世界的全新体验。目前XR设备所带来的功能价值较为有限，内容生态成为产品竞争的关键因素。兼具软硬件生态的苹果入局MR，有望通过其市场影响力吸引顶尖内容制作者建立良好开发生态，同时依靠品牌影响力能够在更低的用户教育、触达成本下实现优质内容供给—平台破圈引流—消费者需求响应的良性生态。</p><p></p><h4>去哪儿网正式推行“3+2”混合办公模式，员工每周有2天可自主选择办公地点</h4><p></p><p>7&nbsp;月&nbsp;9&nbsp;日消息，去哪儿&nbsp;CEO&nbsp;陈刚发全员信宣布，从&nbsp;7&nbsp;月&nbsp;15&nbsp;日（下周一）起，每周三、周五，员工可以灵活选择办公地点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdfa591cf4dead0631f4449bc121d902.png" /></p><p></p><p>陈刚特意强调，员工按规定混合办公，“无需任何申请审批”。</p><p></p><p>据了解，混合办公的适用人员范围以入职&nbsp;6&nbsp;个月以上的标准工时正式员工为主。去年&nbsp;10&nbsp;月，去哪儿开始了为期&nbsp;9&nbsp;个月的混合办公试验。回收数据显示，员工对混合办公的各个维度反馈正面&nbsp;——&nbsp;超过九成的员工认为混合办公后幸福感有明显提升，员工主动离职率在混合办公后下降了三成。</p><p></p><p>去哪儿&nbsp;COO（首席运营官）刘连春表示，混合办公没有让公司业绩变坏，并且显著提升了员工的幸福度。那这件事情公司何乐而不为呢？他强调混合办公不会影响员工的绩效和晋升。</p><p></p><h4>马斯克叫停与甲骨文的100亿美元谈判，拟自建“超算工厂”</h4><p></p><p>当地时间7月9日，据The&nbsp;Information报道，马斯克旗下的人工智能（AI）初创公司xAI已与甲骨文终止扩大一项现有协议的谈判，这笔交易的潜在价值高达100亿美元。根据该协议，xAI将从甲骨文租用英伟达的AI芯片搭建超级计算机。</p><p></p><p>马斯克当天在X平台上回应，xAI将自行建造超级计算机，这样能保证更快速地完成，从而赶上竞争对手。目前，该公司正使用戴尔和超微电脑提供的英伟达芯片，在美国田纳西州孟菲斯建立AI数据中心。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18523a4710256617e83c83e40520693e.png" /></p><p></p><p>此次谈判自&nbsp;5&nbsp;月份以来一直在进行，经过一个多月的商谈后依然以失败告终。据悉，交易告吹导致甲骨文股价暴跌，周二股价下跌&nbsp;3%，收于&nbsp;140.68&nbsp;美元。此次下跌结束了甲骨文连续七天的上涨势头，并引发了投资者对该公司在竞争激烈的云计算市场中能否获得并维持大规模合同的担忧。</p><p></p><p>虽然新交易失败，但甲骨文和&nbsp;xAI&nbsp;将继续在基础设施需求方面进行合作。xAI&nbsp;与甲骨文签订的在&nbsp;Oracle&nbsp;Gen2&nbsp;Cloud&nbsp;中训练&nbsp;AI&nbsp;模型的现有合同仍不受影响，这表明两家公司之间的关系并未完全断绝。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/mPuhRGW8AeLgntE1QLfSyw">xAI&nbsp;和&nbsp;Oracle&nbsp;间&nbsp;100&nbsp;亿美元的生意谈崩了！有钱也租不到芯片的马斯克要自建超级计算中心，就不信“钞”能力还会失效？</a>"》</p><p></p><h4>半年报扭亏！手机市场回暖&nbsp;欧菲光逐渐走出“苹果阴影”</h4><p></p><p>近日，欧菲光（002456）发布的公告引起了广泛关注。被苹果“抛弃”的欧菲光，在经过三年的业绩低迷后，终于重新进入稳定的收入状态。7月9日晚间，该公司发布的公告显示，预计今年上半年实现3600万元–4500万元的盈利，这也是欧菲光时隔三年首次实现半年报盈利。</p><p></p><p>结合此前的公告和业内人士分析内容，欧菲光盈利主要得益于华为这个大客户重新占领手机市场，以及欧菲光在智能汽车领域的增长，且该公司本身的业务结构也在向着更健康的方向调整。</p><p></p><p>值得注意的是，虽然欧菲光对于今年下半年智能手机和智能汽车市场的预期比较乐观，但该公司仍然非常依赖智能手机市场。业内人士给出建议，欧菲光应该深挖技术“护身河”，持续走多元化的路子。</p><p></p><h4>“WPS&nbsp;崩了”：三周内第二次，官方回应“服务已恢复正常”</h4><p></p><p>7&nbsp;月&nbsp;8&nbsp;日早，WPS&nbsp;崩了冲上微博热搜。这次的崩溃对于在工作中使用&nbsp;WPS&nbsp;的小伙伴影响比较严重。</p><p></p><p>不少网友反馈，自己扫码登不上&nbsp;WPS，验证码也收不到，尝试了多种办法都不行，也有网友反馈存在无法登录、云端文件无法打开，还有网友遇到了数据无法保存等场景。</p><p></p><p>针对这一情况，WPS&nbsp;客服团队迅速响应，称&nbsp;7&nbsp;月&nbsp;8&nbsp;日早上全国范围内的云服务都出现了故障，已接到了用户反馈。目前在紧急排查修复中，部分用户已恢复，其余也在逐步恢复中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91d5786c52fe946f49b60445db223327.jpeg" /></p><p></p><p>值得一提的是，不久前，6月28日下午，多名用户反映WPS金山文档无法正常打开疑似应用崩溃，“WPS崩了”话题也登上了热搜。</p><p></p><p>有网友表示，之前碰到了很多次&nbsp;WPS&nbsp;崩溃的情况，打开&nbsp;WPS&nbsp;Office&nbsp;时遭遇了启动失败的错误，而且多次重启都未能成功。必须要卸载，重新安装新版本才能解决。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>重大数据泄露事件：RockYou2024文件泄露，数百万用户信息暴露</h4><p></p><p>7月11日消息，据网络安全公司Cybernews近日披露，一个名为"RockYou2024"的文件在暗网论坛上被公开。文件中包含了9948575739条明文密码，这一数字几乎涵盖了全球网民的真实密码集合，网络安全专家认为这是有史以来最大的密码泄露事件。</p><p></p><p>Cybernews的研究人员通过泄露密码检查器的数据交叉对照后发现，这些密码来自新旧数据泄露的混合。也就是说，RockYou2024主要还是以往密码泄露事件的汇编，据估计包含了来自总计4000个巨大被盗凭证数据库的条目，时间跨度至少达二十年之久。值得注意的是，新文件中包含了早先的RockYou2021，其中含有84亿个密码，也就是说RockYou2024在2021的基础上新增约15亿个密码，时间范围从2021年至2024年。</p><p></p><p>Cybernews警告称，一旦RockYou2024与其他泄露数据库联手，比如用户的邮箱地址和其他敏感信息，那么数据泄露、金融诈骗、身份盗窃……一连串的灾难性后果将接踵而至。</p><p></p><h4>二季度&nbsp;PC&nbsp;出货量增长&nbsp;3%，中国市场继续低迷</h4><p></p><p>IDC&nbsp;公布的数据显示，二季度&nbsp;PC&nbsp;出货量比去年同期增加&nbsp;3%，在连续七个季度下滑之后连续两个季度保持了增长。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28a1eda40eadb601e123c16d0947101d.png" /></p><p></p><p>但中国市场持续低迷阻碍了&nbsp;PC&nbsp;市场的复苏。二季度&nbsp;PC&nbsp;出货量&nbsp;6490&nbsp;万台，排除中国市场的&nbsp;PC&nbsp;出货量同比增长逾&nbsp;5%。</p><p></p><p>**IDC全球设备跟踪器集团副总裁Ryan&nbsp;Reith表示，**毫无疑问，PC市场和其他技术市场一样，由于成熟度和逆风因素，在短期内面临挑战。然而，连续两个季度的增长，加上围绕AIPC的大量市场炒作，再加上一个虽不够吸引人但可以说更重要的商用市场换机周期，似乎正是PC市场所需要的。热点显然是围绕AI的，但是，non-AIPC的购买产生的影响更大，使这个成熟的市场显示出积极的迹象。</p><p></p><p>近几个月来，大多数行业参与者都制定了AIPC的初步战略，主要关注组件方面和商用市场的潜力。尽管IDC认为，商用市场在PC行业的AI领域短期内具有最大的上升空间，但消费市场的故事尚未完全被讲述。人们都在期待苹果在今年晚些时候通过预期的产品发布来推动这一信息，但不应忽视的是，高通、英特尔和AMD都可能在消费者和商业AI&nbsp;PC领域制造声势。</p><p></p><h4>开源代码编辑器&nbsp;Zed&nbsp;发布原生&nbsp;Linux&nbsp;版本</h4><p></p><p>7月10日，Zed官方昨日发布了0.143.6版本，并正式支持Linux。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5c124710c52aa6893c39cfc4c0c1a67.png" /></p><p></p><p>据介绍，Linux&nbsp;上的&nbsp;Zed&nbsp;正在使用&nbsp;Vulkan&nbsp;API&nbsp;进行&nbsp;GPU&nbsp;加速。它同时支持&nbsp;Wayland&nbsp;和&nbsp;X11&nbsp;会话。到目前为止，Zed&nbsp;团队的开发重心主要集中在&nbsp;Ubuntu&nbsp;下的测试。</p><p></p><p>Zed&nbsp;是一款支持多人协作的代码编辑器，由&nbsp;Atom&nbsp;编辑器原作者主导，其底层采用&nbsp;Rust&nbsp;编写、默认支持&nbsp;Rust，还自带了&nbsp;rust-analyzer，主打&nbsp;“高性能”——作者表示希望将&nbsp;Zed&nbsp;打造为世界上最好的文本编辑器。</p><p></p><h4>Java&nbsp;之父&nbsp;James&nbsp;Gosling&nbsp;宣布退休</h4><p></p><p>近日，Java&nbsp;语言之父&nbsp;James&nbsp;Gosling&nbsp;在领英上发文宣布自己即将退休。他在博文中写道：“我现在终于退休了。干了这么多年软件工程师，是时候享受生活了。尽管曾经经历过疫情肆虐、业界萧条，但在亚马逊的过去七年里，我过得非常愉快。我还有很多未尽事宜要去完成，我将满怀期待开启新征程”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/0927617083be1d4e9ab9f0fcdac47b1d.jpeg" /></p><p></p><p>Java&nbsp;的故事始于&nbsp;1991&nbsp;年，当时&nbsp;Sun&nbsp;Microsystems&nbsp;试图将其在计算机工作站市场的领先地位扩展到新兴且发展迅速的个人电子产品市场。几乎没有人预料到&nbsp;Sun&nbsp;即将创建的编程语言会使计算大众化，激发了一个全球范围的社区，并成为了一个由语言、运行时平台、SDK、开源项目以及许多工具组成的持久软件开发生态系统的平台。</p><p></p><p>经过&nbsp;James&nbsp;Gosling&nbsp;领导团队数年秘密开发后，Sun&nbsp;于&nbsp;1995&nbsp;年发布了具有里程碑意义的“一次编写，随处运行”&nbsp;的&nbsp;Java&nbsp;平台，并将重点从最初的交互式电视系统设计转到了新兴的万维网应用程序上。在本世纪初，Java&nbsp;就已经开始为从智能卡到太空飞行器的一切制作动画了。如今，数以百万计的开发人员在使用&nbsp;Java&nbsp;编程，它至今仍然是工业界最受欢迎和使用最多的语言。</p><p></p><h4>微软宣布90天内将结束Win11部分版本服务</h4><p></p><p>7&nbsp;月&nbsp;9&nbsp;日，微软&nbsp;Windows&nbsp;11&nbsp;即将迎来其三岁生日，这意味着初始版本&nbsp;21H2&nbsp;和第一个功能更新版本&nbsp;22H2&nbsp;将很快将失去支持，参考微软近日发布的通知，微软警告这两个版本仅剩&nbsp;90&nbsp;天生命周期支持，之后&nbsp;Windows&nbsp;11&nbsp;21H2&nbsp;和&nbsp;22H2&nbsp;无法再获得安全更新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/255e6ea92204e463a8c09fb030950741.png" /></p><p></p><p>2024&nbsp;年&nbsp;6&nbsp;月&nbsp;Windows&nbsp;各版本占有率&nbsp;&nbsp;来源：statcounter</p><p></p><p>事实上，目前Windows&nbsp;11&nbsp;21H2已经不再支持普通消费者，唯一仍在更新的版本是企业版、教育版和物联网企业版。</p><p></p><p>微软于2021年10月5日发布了&nbsp;Windows&nbsp;11，这是在&nbsp;Windows&nbsp;10&nbsp;推出大约六年后发布的。Windows&nbsp;11&nbsp;的第一个版本是&nbsp;21H2，该版本在2023年10月对普通用户结束支持。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</id>
            <title>国产芯片大厂三年干到 70 亿市值，却一次性裁员 50 %？员工曝 CTO 不懂技术！</title>
            <link>https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 06:45:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯华章, 裁员, EDA, 市场困境
<br>
<br>
总结: 中国国产EDA大厂芯华章开始大规模裁员，引发关注。裁员比例高达50%，公司否认裁员比例，称为谣言。裁员引发了对公司战略收缩和质疑的猜测。芯华章曾是估值70亿的独角兽，但裁员已进行两波。管理层和并购被指责，显示国产EDA厂商面临严峻市场困境。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>7月8日，某职场社交平台有网友爆料称，国产EDA大厂芯华章开始大规模裁员，裁员比例高达50%，并且第一批裁员已经谈话完毕。另有认证为“芯华章科技股份有限公司员工”的网友还补充称，“不止50%，软件部裁员接近60%。留下来的人更加惶恐。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cbb1de6bc2e8f2a221f71f89cde5459.jpeg" /></p><p></p><p>&nbsp;</p><p>也有不少人关心芯华章此次裁员的赔偿方案，“百分之五十的裁员比例，赔偿总和可不是小数目。”</p><p>&nbsp;</p><p>消息传出后，有芯华章内部人士向媒体表示，“公司确实有在战略收缩，但是人员优化比例有限，裁员50%的说法是谣言。如果真像传闻那样一下子裁员50%，那公司根本就没法正常运转了。”</p><p>&nbsp;</p><p>但依然有不少对于芯华章此次战略收缩的深层猜测，其中有两方面的解释：一是公司业务方面，“管理层决策失误，技术路线步子迈大了，客户也没搞定”；二是市值套现的质疑，“如今财务造假严打，IPO收紧，就原形毕露了”。</p><p>&nbsp;</p><p>注：EDA全称Electronic&nbsp;Design&nbsp;Automation，意为电子设计自动化，是用于辅助完成超大规模集成电路芯片设计、制造、封装、测试整个流程的计算机软件，完整的集成电路设计和制造流程均需要对应的EDA工具作为支撑，因而有“芯片之母”的称号。</p><p>&nbsp;</p><p></p><h1>三年估值70亿，裁员已进行两波</h1><p></p><p>芯华章由前新思科技中国区副总经理王礼宾于2020年3月创立，当时正值国内积极倡导国产EDA工具发展，希望打破国外厂商垄断局面。</p><p>&nbsp;</p><p>刚成立一年不久，芯华章便已完成5轮融资，累计融资金额超12亿元。到成立三年时，芯华章已完成8轮投资，每轮融资均数亿元。在去年&nbsp;3&nbsp;月获得中信科&nbsp;5G&nbsp;基金的战略投资后，芯华章晋升为估值&nbsp;70&nbsp;亿元的独角兽。</p><p>&nbsp;</p><p>据了解，芯华章主要聚焦芯片EDA数字验证领域，打造从芯片到系统的验证解决方案，提供完整的验证EDA工具链服务。2021年，芯华章率先发布支持国产服务器架构的数字仿真器穹鼎GalaxSim，去年7月又推出新一代高速仿真器GalaxSim&nbsp;Turbo。</p><p>&nbsp;</p><p>关于这次的裁员，据一位认证信息为“芯华章科技股份有限公司员工”的网友介绍，他是芯华章GalaxSim部门的成员，其一再表示，“不知道还能苟多久，下次估计就是整个项目组了，毕竟没剩多少人了。”</p><p>&nbsp;</p><p>值得注意的是，就在今年4月，芯华章宣布其核心EDA软件产品已完成华为鲲鹏平台的移植工作。基于鲲鹏处理器等国产架构，芯华章逻辑仿真器GalaxSim、形式化验证工具GalaxFV，都能有效利用鲲鹏的高性能集群去提高编译与运算，显著提高了系统级芯片仿真验证效率。其中，GalaxSim在多个客户测试用例上已经取得了2-3倍的仿真性能提升，大幅降低了仿真回归测试的时间。</p><p>&nbsp;</p><p>然而，此次的“战略收缩”已不是芯华章第一次进行裁员。有知情人士透露，“第一波是去年12月，有人被裁了至今没找到满意的工作，这次人数来的更猛烈了，想谈个好价格就更难了…”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da1a0b256294d25e94bf3d11fa1d19fd.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h1>“CTO不懂技术”，并购成转折点？</h1><p></p><p>&nbsp;</p><p>“从头到尾都是资本运作的公司，一个工程师注册个公司2年市值60亿，为的就是IPO套现。如今财务造假严打，IPO收紧，就原形毕露了。”对于芯华章此次的大规模裁员消息，一位国金证券的投资理财顾问发表了这样的看法。</p><p>&nbsp;</p><p>另一位认证为“芯华章科技股份有限公司员工”的网友则把矛头指向了芯华章的管理层，“CTO酒量很好，人也仗义，可就是不太懂技术。”“CTO不懂技术这还是第一次听。”一位认证为“新思科技员工”的网友评价道。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca76c051463eedc67f8d14f7b272d8d1.jpeg" /></p><p></p><p>&nbsp;</p><p>2022年9月，芯华章收购高性能仿真软件企业“瞬曜电子”，并进行核心技术整合，并购金额没有披露，同时任瞬曜电子创始人傅勇为公司首席技术官（CTO）。</p><p>&nbsp;</p><p>公开资料显示，傅勇曾担任新思科技资深技术总监，主管亚太地区数字验证产品事业部的技术战略与客户支持。2021年，傅勇在离开新思科技中国一年后，创立了瞬曜电子，专注于数字芯片的前端验证领域，还发布了瞬系列RTL高速仿真器（ShunSim）。更早之前，傅勇毕业于清华大学电子工程系，获得了学士和硕士学位，毕业后任职于三星电子、楷登电子（Cadence）和新思科技，在EDA行业工作达25年。</p><p>&nbsp;</p><p>据了解，当时芯华章是对瞬曜电子的知识产权、产品等核心资产进行的收购。并购是EDA企业扩张的常见手段，借助技术与资本的双重力量，在扩宽产品系列的同时还消除了潜在竞争对手。</p><p>&nbsp;</p><p>不少国外的EDA巨头都通过大量并购优秀EDA点工具厂商实现了EDA全流程覆盖，Cadence通过收购Verilog、Silicon&nbsp;Perspective，解决芯片性能验证问题，将1C布局工具和S1分析工具收入囊；而Synopsys在收购Avanti后，成为了EDA史上首家可以提供顶级前后端完整1C设计方案的EDA工具商。</p><p>&nbsp;</p><p>但芯华章对瞬曜电子进行收购后，走向却似乎有所不同。有业内人士这样评价，“花了好大代价并购个寂寞，回头来看，转折点no1。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a499c83171ff5c4f92a1b01f6bd8fb85.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h1>国产EDA厂商的严峻市场困境</h1><p></p><p>&nbsp;</p><p>去年9月18日，在国内EDA开放合作创新组织举办的首届IDAS设计自动化产业峰会“数字逻辑设计与验证领域”专题分论坛上，华为海思半导体平台验证部部长傅晓对芯华章的演讲者抛出了一系列问题：“芯华章目前生产了多少机框？实际上有多少机框被客户采用？又有多少FPGA被成功导入？”</p><p>&nbsp;</p><p>当时，傅晓强调，学术圈或者企业圈沟通有个基本原则，就叫实事求是，中国要把EDA干成，不能吹，不吹才能成事。引发关注和热议后，傅晓在朋友圈内向芯华章致歉，并对芯华章表明了认可。</p><p>&nbsp;</p><p>但这一风波，也侧面反映出国产EDA厂商的发展困境。EDA市场规模有限，头部的Cadence、新思科技、Siemens&nbsp;EDA等厂商经过多年发展，市场地位稳固。相比之下，国产EDA厂商所面临的竞争环境惨烈。</p><p>&nbsp;</p><p>根据赛迪智库统计，2020&nbsp;年国际三大&nbsp;EDA&nbsp;巨头新思科技、铿腾电子和西门子&nbsp;EDA&nbsp;在国内市场占据明显的头部优势，合计占领约&nbsp;80%的市场份额；国产&nbsp;EDA&nbsp;厂商华大九天市占率约&nbsp;6%，处于国内市场第四位。</p><p>&nbsp;</p><p>中国半导体行业协会预测，到2025年中国的EDA市场规模将达到184.9亿元人民币（约合25亿美元），届时将占全球EDA市场的18.1%。但考虑到该市场的大部分份额仍被国外EDA厂商所占据，留给国产EDA厂商的市场空间依然十分有限。国内头部EDA&nbsp;厂商华大九天4月发布的财报显示，其2023年全年营收也只有10.1亿元人民币。</p><p>&nbsp;</p><p>因而，对于还未上市的国产EDA企业来说，未来可能会遭遇更为严峻的融资环境与显著提升的上市门槛，“战略收缩”或是会其应对市场的必要出路之一了。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://maimai.cn/n/content/global-topic?circle_type=9&amp;topic_id=F8W8dHiR">https://maimai.cn/n/content/global-topic?circle_type=9&amp;topic_id=F8W8dHiR</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</id>
            <title>TaD+RAG- 缓解大模型“幻觉”的组合新疗法</title>
            <link>https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 06:18:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TaD, RAG, LLM, 幻觉
<br>
<br>
总结: 本文介绍了京东联合清华大学提出的任务感知解码技术（TaD）和业内解决LLM幻觉问题的最有效系统性方案——检索增强生成技术（RAG）。大语言模型（LLM）在人类对话互动方面表现出色，但幻觉问题成为其落地应用的制约和瓶颈。幻觉问题主要来源于数据、训练和推理过程，针对这些问题提出了一些缓解策略，其中RAG通过引入信息检索过程，增强LLM的生成过程，提高准确性和鲁棒性，降低幻觉。 </div>
                        <hr>
                    
                    <p></p><p></p><p>TaD：任务感知解码技术（Task-aware Decoding，简称 TaD），京东联合清华大学针对大语言模型幻觉问题提出的一项技术，成果收录于 IJCAI2024。</p><p>RAG：检索增强生成技术（Retrieval-augmented Generation，简称 RAG），是业内解决 LLM 幻觉问题最有效的系统性方案。</p><p></p><h1>1. 背景介绍</h1><p></p><p></p><p>近来，以 ChatGPT 为代表的生成式大语言模型（Large Language Model，简称 LLM）掀起了新一轮 AI 热潮，并迅速席卷了整个社会的方方面面。得益于前所未有的模型规模、训练数据，以及引入人类反馈的训练新范式，LLM 在一定程度上具备对人类意图的理解和甄别能力，可实现生动逼真的类人对话互动，其回答的准确率、逻辑性、流畅度都已经无限接近人类水平。此外，LLM 还出现了神奇的“智能涌现”现象，其产生的强大的逻辑推理、智能规划等能力，已逐步应用到智能助理、辅助创作、科研启发等领域。京东在诸多核心业务如 AI 搜索、智能客服、智能导购、创意声称、推荐/广告、风控等场景下，均对 LLM 的落地应用进行了深入探索。这一举措提升了业务效率，增强了用户体验。</p><p></p><p>尽管具备惊艳的类人对话能力，大语言模型的另外一面——不准确性，却逐渐成为其大规模落地的制约和瓶颈。通俗地讲，LLM 生成不准确、误导性或无意义的信息被称为“幻觉”，也就是常说的“胡说八道”。当然也有学者，比如 OpenAI 的 CEO Sam Altman，将 LLM 产生的“幻觉”视为“非凡的创造力”。但是在大多数场景下，模型提供正确回答的能力至关重要，因此幻觉常常被认为是一种缺陷；尤其是在一些对输出内容准确性要求较高的场景下，比如医疗诊断、法律咨询、工业制造、售后客服等，幻觉问题导致的后果往往是灾难性的。</p><p></p><p>本文主要探索针对 LLM 幻觉问题的解决方案。</p><p></p><h1>2. 相关调研</h1><p></p><p></p><p>众所周知，大语言模型的本质依然是语言模型（Language Model，简称 LM），该模型可通过计算句子概率建模自然语言概率分布。具体而言，LM 基于统计对大量语料进行分析，按顺序预测下一个特定字/词的概率。LLM 的主要功能是根据输入文本生成连贯且上下文恰当的回复，即生成与人类语言和写作的模式结构极为一致的文本。注意到，LLM 并不擅长真正理解或传递事实信息。故而其幻觉不可彻底消除。亚利桑那州立大学教授 Subbarao Kambhampati 认为：LLM 所生成的全都是幻觉，只是有时幻觉碰巧和你的现实一致而已。新加坡国立大学计算学院的 Ziwei Xu 和 Sanjay Jain 等也认为 LLM 的幻觉无法完全消除[1]。</p><p></p><p>虽然幻觉问题无法彻底消除，但依然可以进行优化和缓解，业内也有不少相关的探索。有研究[2]总结了 LLM 产生幻觉的三大来源：数据、训练和推理，并给出了对应的缓解策略。</p><p></p><h4>2.1 数据引入的幻觉</h4><p></p><p></p><p>“病从口入”，训练数据是 LLM 的粮食，数据缺陷是使其致幻的一大原因。数据缺陷既包括数据错误、缺失、片面、过期等，也包括由于领域数据不足所导致的模型所捕获的事实知识利用率较低等问题。以下是针对训练数据类幻觉的一些技术方案：</p><p></p><p>数据清洗</p><p></p><p>针对数据相关的幻觉，最直接的方法就是收集更多高质量的事实数据，并进行数据清理。训练数据量越大、质量越高，最终训练得到的 LLM 出现幻觉的可能性就可能越小[3]。但是，训练数据总有一定的覆盖范围和时间边界，不可避免地形成知识边界，单纯从训练数据角度解决幻觉问题，并不是一个高性价比的方案。</p><p>针对“知识边界”问题，有两种主流方案：一种是知识编辑，即直接编辑模型参数弥合知识鸿沟。另一种是检索增强生成（Retrieval-augmented Generation，简称 RAG），保持模型参数不变，引入第三方独立的知识库。</p><p></p><p>知识编辑</p><p></p><p>知识编辑有两种方法：1）编辑模型参数的方法可以细粒度地调整模型的效果，但难以实现知识间泛化能力，且不合理的模型编辑可能会导致模型产生有害或不适当的输出[4]；2）外部干预的方法（不编辑模型参数）对大模型通用能力影响较小，但需要引入一个单独的模块，且需要额外的资源训练这个模块。</p><p>如何保持原始 LLM 能力不受影响的前提下，实现知识的有效更新，是 LLM 研究中的重要挑战[2]。鉴于知识编辑技术会给用户带来潜在风险，无论学术界还是业界都建议使用包含明确知识的方法，比如 RAG。</p><p></p><p>检索增强生成（RAG）</p><p></p><p>RAG 引入信息检索过程，通过第三方数据库中检索相关信息来增强 LLM 的生成过程，从而提高准确性和鲁棒性，降低幻觉。由于接入外部实时动态数据，RAG 在理论上没有知识边界的限制，且无需频繁进行 LLM 的训练，故已经成为 LLM 行业落地最佳实践方案。下图 1 为 RAG 的一个标准实现方案[11]，用户的 Query 首先会经由信息检索模块处理并召回相关文档；随后 RAG 方法将 Prompt、用户 query 和召回文档一起输入 LLM，最终由 LLM 生成最终的答案。</p><p></p><p>图 1. RAG 架构图</p><p><img src="https://static001.geekbang.org/infoq/ea/ea7b73f20dc4c921d356287164b7f45a.png" /></p><p>​</p><p>﻿﻿</p><p>RAG 借助信息检索，引入第三方事实知识，大大缓解了单纯依靠 LLM 生成答案而产生的幻觉，但由 LLM 生成的最终输出仍然有较大概率产生幻觉。因此，缓解 LLM 本身的幻觉，对整个 RAG 意义重大。</p><p></p><h4>2.2 模型训练引入的幻觉</h4><p></p><p></p><p>LLM 的整个训练过程，都可能会引入幻觉。首先，LLM 通常是 transformer 结构的单向语言模型，通过自回归的方式建模目标，天然存在单向表示不足、注意力缺陷[6]、曝光偏差[7]等问题；其次，在文本对齐阶段，无论是监督微调（SFT）还是人类反馈的强化学习（RLHF），都有可能出现有标注数据超出 LLM 知识边界、或者与 LLM 内在知识不一致的问题；这一系列对齐问题很可能放大 LLM 本身的幻觉风险[8]。</p><p></p><p>对于训练过程引入的幻觉，可以通过优化模型结构、注意力机制、训练目标、改进偏好模型等一系列手段进行缓解。但这些技术都缺乏通用性，难以在现有的 LLM 上进行迁移，实用性不高。</p><p></p><h4>2.3 推理过程引入的幻觉</h4><p></p><p></p><p>推理过程引入的幻觉，一方面源自于解码策略的抽样随机性，它与幻觉风险的增加呈正相关，尤其是采样温度升高导致低频 token 被采样的概率提升，进一步加剧了幻觉风险[9]。另一方面，注意力缺陷如上下文注意力不足、Softmax 瓶颈导致的不完美解码都会引入幻觉风险。</p><p></p><p>层对比解码（DoLa）</p><p></p><p>针对推理过程解码策略存在的缺陷，一项具有代表性且较为有效的解决方案是层对比解码（Decoding by Contrasting Layers, 简称 DoLa）[9]。模型可解释性研究发现，在基于 Transformer 的语言模型中，下层 transformer 编码“低级”信息（词性、语法），而上层中包含更加“高级”的信息（事实知识）[10]。DoLa 主要通过强调较上层中的知识相对于下层中的知识的“进步”，减少语言模型的幻觉。具体地，DoLa 通过计算上层与下层之间的 logits 差，获得输出下一个词的概率。这种对比解码方法可放大 LLM 中的事实知识，从而减少幻觉。</p><p></p><p>图 2. DoLa 示意图</p><p><img src="https://static001.geekbang.org/infoq/3c/3cbec3d739fb3141d66fed54dd85980f.png" /></p><p>​</p><p></p><p>上图 2 是 DoLa 的一个简单直观的示例。“Seattle”在所有层上都保持着很高的概率，可能仅仅因为它是一个从语法角度上讲比较合理的答案。当上层通过层对比解码注入更多的事实知识后，正确答案“Olympia”的概率会增加。可见，层对比解码（DoLa）技术可以揭示真正的答案，更好地解码出 LLM 中的事实知识，而无需检索外部知识或进行额外微调。此外，DoLa 还有动态层选择策略，保证最上层和中间层的输出差别尽可能大。</p><p></p><p>可见，DoLa 的核心思想是淡化下层语言/语法知识，尽可能放大事实性知识，但这可能导致生成内容存在语法问题；在实验中还发现 DoLa 会倾向于生成重复的句子，尤其是长上下文推理场景。此外，DoLa 不适用有监督微调，限制了 LLM 的微调优化 。</p><p></p><h1>3. 技术突破</h1><p></p><p></p><p>通过以上分析，RAG 无疑是治疗 LLM 幻觉的一副妙方，它如同 LLM 的一个强大的外挂，让其在处理事实性问题时如虎添翼。但 RAG 的最终输出仍然由 LLM 生成，缓解 LLM 本身的幻觉也极为重要，而目前业内针对 LLM 本身幻觉的技术方案存在成本高、实用落地难、易引入潜在风险等问题。</p><p></p><p>鉴于此，京东零售联合清华大学进行相关探索，提出任务感知解码（Task-aware Decoding，简称 TaD）技术[12]（成果收录于 IJCAI2024），可即插即用地应用到任何 LLM 上，通过对比有监督微调前后的输出，缓解 LLM 本身的幻觉。该方法通用性强，在多种不同 LLM 结构、微调方法、下游任务和数据集上均有效，具有广泛的适用场景。</p><p></p><p>任务感知解码（TaD）技术</p><p></p><p>关于 LLM 知识获取机制的一些研究表明，LLM 的输出并不能总是准确反映它们所拥有的知识，即使一个模型输出错误，它仍然可能拥有正确的知识[13]。此项工作主要探索 LLM 在保留预训练学到的公共知识的同时，如何更好地利用微调过程中习得的下游任务特定领域知识，进而提升其在具体任务中的效果，缓解 LLM 幻觉。</p><p></p><p>TaD 的基本原理如图 3 所示。微调前 LLM 和微调后 LLM 的输出词均为“engage”，但深入探究不难发现其相应的预测概率分布发生了明显的改变，这反映了 LLM 在微调期间试图将其固有知识尽可能地适应下游任务的特定领域知识。具体而言，经过微调，更加符合用户输入要求（“专业的”）的词“catalyze”的预测概率明显增加，而更通用的反映预训练过程习得的知识却不能更好满足下游任务用户需求的词“engage”的预测概率有所降低。TaD 巧妙利用微调后 LLM 与微调前 LLM 的输出概率分布的差异来构建知识向量，得到更贴切的输出词“catalyze”，进而增强 LLM 的输出质量，使其更符合下游任务偏好，改善幻觉。</p><p></p><p>图 3. TaD 原理图</p><p><img src="https://static001.geekbang.org/infoq/af/af2930c906a97052acb628e6d5017497.png" /></p><p></p><p>知识向量</p><p></p><p>为了直观理解 LLM 在微调阶段学习到的特定领域知识，我们引入知识向量的概念，具体如图 4 所示。微调前 LLM 的输出条件概率分布为 pθ，微调后 LLM 的输出条件概率分布为 pϕ。知识向量反应了微调前后 LLM 输出词的条件概率分布变化，也代表着 LLM 的能力从公共知识到下游特定领域知识的适应。基于 TaD 技术构建的知识向量可强化 LLM 微调过程中习得的领域特定知识，进一步改善 LLM 幻觉。</p><p></p><p>图 4. 知识向量</p><p><img src="https://static001.geekbang.org/infoq/c2/c237f091d02fda038009d5b19c39d4ae.png" /></p><p>﻿﻿</p><p>特别地，当微调数据较少时，LLM 的输出条件概率分布远远达不到最终训练目标。在此情形下，TaD 技术增强后的知识向量可以加强知识对下游任务的适应，在训练数据稀缺场景下带来更显著的效果提升。</p><p></p><p>实验结果</p><p></p><p>1）针对不同的 LLM，采用 LoRA、AdapterP 等方式、在不同的任务上进行微调，实验结果如下表 1 和表 2 所示。注意到，TaD 技术均取得了明显的正向效果提升。</p><p></p><p>表 1. Multiple Choices 和 CBQA 任务结果</p><p><img src="https://static001.geekbang.org/infoq/72/7204c1175cee2590cce2caa1f45905f3.png" /></p><p></p><p>表 2. 更具挑战性的推理任务结果</p><p><img src="https://static001.geekbang.org/infoq/d0/d04a0fc99171af777eae622bac9fa3ba.png" /></p><p>​﻿﻿</p><p>2）相比较其他对比解码技术，TaD 技术在绝大部分场景下效果占优，具体如表 3 所示。需要特别强调的一点是，其他技术可能会导致 LLM 效果下降，TaD 未表现上述风险。</p><p></p><p>表 3. 不同对比解码技术结果</p><p><img src="https://static001.geekbang.org/infoq/95/95281634c1b6ffd33e4095ac45d095d8.png" /></p><p>​﻿﻿</p><p>3）针对不同比例的训练样本进行实验，发现一个非常有趣的结果：训练样本越少，TaD 技术带来的收益越大，具体如表 4 所示。因此，即使在有限的训练数据下，TaD 技术也可以将 LLM 引导到正确的方向。由此可见，TaD 技术能够在一定程度上突破训练数据有限情形下 LLM 的效果限制。</p><p></p><p>表 4. 不同数据比例下的结果</p><p><img src="https://static001.geekbang.org/infoq/43/43edb0aa6a8d4c66f408d9e7fec712b9.png" /></p><p>​﻿﻿</p><p>可见，TaD 可以即插即用，适用于不同 LLM、不同微调方法、不同下游任务，突破了训练数据有限的瓶颈，是一项实用且易用的改善 LLM 自身幻觉的技术。</p><p></p><h1>4. 落地案例</h1><p></p><p></p><p>自从以 ChatGPT 为代表的 LLM 诞生之后，针对其应用的探索一直如火如荼，然而其幻觉已然成为限制落地的最大缺陷。综上分析，目前检索增强生成（RAG）+低幻觉的 LLM 是缓解 LLM 幻觉的最佳组合疗法。在京东通用知识问答系统的构建中，我们通过 TaD 技术实现低幻觉的 LLM，系统层面基于 RAG 注入自有事实性知识，具体方案如图 5 所示，最大程度缓解了 LLM 的生成幻觉 。</p><p></p><p>图 5. TaD+RAG 的知识问答系统</p><p><img src="https://static001.geekbang.org/infoq/ef/ef35c2a47c39ab1461fa6248279813fe.png" /></p><p>​</p><p>目前知识问答系统已经接入京东 6000+业务场景，为用户提供准确、高效、便捷的知识性问答，大大节省了运营、运维等人力开销。</p><p></p><h1>5. 思考与展望</h1><p></p><p></p><p>如果 LLM 依然按照语言模型的模式发展，那么其幻觉就无法彻底消除。目前业内还没有一种超脱语言模型范畴，且可以高效完成自然语言相关的任务新的模型结构。因此，缓解 LLM 的生成幻觉，仍然是未来一段时期的探索路径。以下是我们在系统、知识、LLM 三个层面的一些简单的思考，希望能够抛砖引玉。</p><p></p><p>系统层面——RAG+Agent+More 的复杂系统</p><p></p><p>RAG 技术确实在一些常见的自然语言处理任务中发挥出色的作用，尤其是针对简单问题和小型文档集。但是遇到一些复杂的问题和大型文档集时，RAG 技术就显得力不从心。近期有一些研究认为 RAG+Agent 才是未来的趋势[14]，Agent 能够辅助理解并规划复杂的任务。我们认为可能未来的系统可能不仅仅局限于 Agent 和 RAG，可能还要需要多种多样的内外工具调用、长短期记忆模块、自我学习模块......</p><p></p><p>知识层面——与 LLM 深度融合的注入方式</p><p></p><p>任何一个深度模型都会存在知识边界的问题，LLM 也不例外。RAG 通过检索的方式召回外部知识，以 Prompt 的形式送入 LLM 进行最终的理解和生成，一定程度上缓解 LLM 知识边界问题。但是这种知识注入的方式和 LLM 生成的过程是相对割裂的。即便已经召回了正确的知识，LLM 也可能因为本身知识边界问题生成错误的回答。因此探索如何实现外部知识和 LLM 推理的深度融合，或许是未来的一个重要的课题。</p><p></p><p>LLM 层面——低幻觉 LLM</p><p></p><p>LLM 本身的幻觉是问题的根本和瓶颈，我们认为随着 LLM 更广泛的应用，类似 TaD 可缓解 LLM 本身幻觉的探索一定会成为业内的更大的研究热点。</p><p></p><h1>6. 结语</h1><p></p><p></p><p>缓解 LLM 幻觉一定是个复杂的系统问题，我们可以综合不同的技术方案、从多个层级协同去降低 LLM 的幻觉。虽然现有方案无法保证从根本上解决幻觉，但随着不断探索，我们坚信业内终将找到限制 LLM 幻觉的更有效的方案，也期待届时 LLM 相关应用的再次爆发式增长。</p><p></p><p>京东零售一直走在 AI 技术探索的前沿，随着公司在 AI 领域的不断投入和持续深耕，我们相信京东必将产出更多先进实用的技术成果，为行业乃至整个社会带来深远持久的影响。</p><p>﻿</p><p>【参考文献】</p><p>[1] Hallucination is Inevitable: An Innate Limitation of Large Language Models</p><p>[2] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</p><p>[3] Unveiling the Causes of LLM Hallucination and Overcoming LLM Hallucination</p><p>[4] Editing Large Language Models: Problems, Methods, and Opportunities</p><p>[5] ACL 2023 Tutorial: Retrieval-based Language Models and Applications</p><p>[6] Theoretical Limitations of Self-Attention in Neural Sequence Models</p><p>[7] Sequence level training with recurrent neural networks.</p><p>[8] Discovering language model behaviors with model-written evaluations</p><p>[9] Dola: Decoding by contrasting layers improves factuality in large language models</p><p>[10] Bert rediscovers the classical nlp pipeline</p><p>[11] Retrieval-Augmented Generation for Large Language Models: A Survey</p><p>[12] TaD: A Plug-and-Play Task-Aware Decoding Method toBetter Adapt LLM on Downstream Tasks</p><p>[13] Inference-time intervention: Eliciting truthful answers from a language model</p><p>[14] Beyond RAG: Building Advanced Context-Augmented LLM Applications</p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</id>
            <title>首个专为半导体行业设计的开源大模型 SemiKong 问世</title>
            <link>https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 01:37:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Aitomatic, SemiKong, AI Alliance, 半导体行业
<br>
<br>
总结: Aitomatic推出了SemiKong，这是世界上第一个专为半导体行业设计的开源AI大型语言模型。SemiKong旨在解决半导体行业的挑战，利用领域知识和专业知识进行训练，有望降低半导体生产成本，推动行业创新。通过AI联盟的支持，SemiKong有望重新定义半导体制造业，为行业带来巨大的飞跃。 </div>
                        <hr>
                    
                    <p></p><p>7 月 10 日，国外初创公司 Aitomatic 宣布推出 SemiKong。这是世界上第一个专为半导体行业设计的开源 AI 大型语言模型（LLM）。它旨在通过将特定领域的知识纳入模型来解决半导体行业面临的一些挑战，例如有关半导体器件和工艺的物理和化学问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bc89c3afa90c31f5c1ef0a8aee82dcc4.png" /></p><p></p><p>SemiKong 由人工智能联盟（AI Alliance）成员合作研发。AI 联盟成立于 2023 年，致力于构建、支持和倡导整个 AI 技术领域的开放式创新，包括软件、数据和模型、安全、安保和信任、工具、评估、硬件、教育、开放科学和宣传。</p><p></p><p>SemiKong 基于联盟成员 Meta 开源的 Llama3 模型，利用了包括 Tokyo Electron 在内的领先半导体公司和 FPT Software 等 AI 专家的专业知识。IBM 研究院 AI 开放创新负责人 Anthony Annunziata 强调，“SemiKong DRAFT v0.6 的诞生表明，汇集不同的专业知识能推动半导体制造等关键行业的重大进步。”</p><p></p><p>SemiKong 的训练过程主要分为 3 个主要阶段：预训练领域知识——自我微调（指令数据集）——合并和量化。从放出的代码权重，可以看出 SemiKong 有 8B 的参数。它在准确性、相关性和对半导体工艺的理解方面表现出了显著的进步。</p><p></p><p>Aitomatic 表示，即使是其较小版本，在特定领域的应用中也常常超越较大的通用模型，从而有可能加速整个半导体价值链的创新并降低成本。并且，它也为那些打造适合自身的专有模型的芯片公司提供了一个有价值的基座。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/16/16ef131c1717c78c97feb525ac103847.png" /></p><p></p><p>随着 SemiKong 降低半导体生产成本，消费者可以在未来几年内以更低的价格看到功能更强大的智能手机、笔记本电脑和智能家居设备。SemiKong 于 2024 年 7 月 9 日起在 HuggingFace 和 GitHub 上提供下载。下一个更强大的版本计划于 2024 年 12 月推出，预计 2024 年 9 月将推出首批特定工艺型号。</p><p></p><p>开源地址：<a href="https://github.com/aitomatic/semikong">https://github.com/aitomatic/semikong</a>"</p><p></p><p>SemiKong 项目的领导者， Aitomatic 首席执行官 Christopher Nguyen 表示：“SemiKong 将重新定义半导体制造业。这种开放式创新模式由人工智能联盟提供支持，利用集体专业知识应对行业特定挑战。在 Aitomatic，我们正在使用 SemiKong 创建领域特定 AI 智能体，以前所未有的效率解决复杂的制造问题。”</p><p></p><p>Tokyo Electron 高级专家、半导体行业模型的早期提出者 Daisuke Oku 补充道：“SemiKong 是半导体开源 AI 的一个令人激动的开始。Aitomatic 的创新方法有可能为我们的行业带来巨大的飞跃。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.prnewswire.com/news-releases/aitomatic-unveils-semikong-ai-breakthrough-set-to-reshape-500b-semiconductor-industry-302194215.html">https://www.prnewswire.com/news-releases/aitomatic-unveils-semikong-ai-breakthrough-set-to-reshape-500b-semiconductor-industry-302194215.html</a>"</p><p></p><p><a href="https://www.semikong.ai/">https://www.semikong.ai/</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</id>
            <title>“萝卜快跑”被曝是真人在屏幕前操作；阿里云宣布与月之暗面“联姻”；去哪儿宣布每周两天自选办公地 ｜AI 周报</title>
            <link>https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 01:25:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 萝卜快跑, 无人驾驶, 安全问题, 人为干预
<br>
<br>
总结: 百度旗下自动驾驶出行服务平台“萝卜快跑”在全国多个城市开展无人自动驾驶服务，订单量激增，但面临安全与技术等问题，甚至有人为干预的传闻。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>网友称萝卜快跑其实有驾驶员人为干预；去哪儿员工每周两天可自主选择办公地点；阿里云宣布与月之暗面“联姻”；OpenAI 绝密项目「草莓」首次曝光；AMD 收购欧洲最大私人 AI 实验室 Silo AI。</blockquote><p></p><p></p><p></p><h2>行业热点</h2><p></p><p></p><p></p><h5>萝卜快跑订单疯涨，无人驾驶时代真的来了？</h5><p></p><p></p><p>7 月 10 日，百度旗下自动驾驶出行服务平台“萝卜快跑”登上微博热搜榜首。据了解，萝卜快跑已在全国 11 个城市开放载人测试运营服务，在武汉、重庆、深圳、上海、北京等城市开展全无人自动驾驶出行服务与测试。</p><p></p><p>媒体报道称，随着百台无人车的投入运营，“萝卜快跑”在武汉市全无人订单量也迎来了爆发式增长，单日单车峰值超 20 单。数据显示，萝卜快跑 APP 用户满意度评价达 4.9 分，其中 5 分满分好评占比高达 94.19%。此外，网络传言称，萝卜快跑已在武汉投放 1000 辆无人车，进而引发对网约车司机、出租车司机就业市场的深刻担忧。</p><p></p><p>目前，萝卜快跑还面临着安全与技术等方面的问题。有武汉网友 7 月 7 日下午在短视频平台发布视频，称百度旗下的自动驾驶出行服务平台“萝卜快跑”无人驾驶出租车在武汉街头与行人相撞，视频中可以看到一个行人躺在出租车前，交警正在现场，事故造成了部分车辆拥堵。这件事故引出了一个亟待解决的问题：无人驾驶车应该如何定责。目前，我国还没有建立一套完善的全国性法规，只有地方规定。此外，在长江网武汉市民留言板上，有关于“萝卜快跑”的留言，目前已累计达到 324 条。留言板多个内容显示，萝卜快跑 Robotaxi 在道路上运营期间，出现车辆在绿灯状态下停滞不前、红灯时冲入路口中央、转弯时卡顿不动等情况，并引发交通拥堵现象，对市民出行造成了一定影响。</p><p></p><p>针对此类现象，武汉经开区管委会回应称：“确认涉诉车辆为自动驾驶测试车，正在调试中。开发区交通大队将定期与公司负责人沟通和反映问题，确保道路安全。”另外，近日还有网友在社交平台称，无人驾驶的萝卜快跑其实有驾驶员人为干预。网传图片显示，在萝卜快跑汽车机器人智控中心，有真人坐在带方向盘的屏幕前操作。百度方面截至发稿没有回应。</p><p></p><p>据网信永川公众号 2023 年 7 月发布的内容，位于永川区大数据产业园的百度无人驾驶实验基地内，有云代驾安全员在 5G 云代驾舱进行远程实时控制，通过高带宽、低时延的 5G 网络，从屏幕组上观察汽车周围 360°状况，并利用方向盘、档把、脚踏板等控制器驾驶无人车辆。5G 云代驾的意义在于，在无人车没有安全员的情况下，当无人车出现解决不了的问题时，云端安全员可以帮助其远程脱困。</p><p></p><p></p><h5>三星爆发大规模罢工，韩媒：半导体部门员工是罢工主力</h5><p></p><p></p><p>据报道，韩国三星电子旗下最大工会“全国三星电子工会”于 8 日上午开始在京畿道华城市三星电子华城工厂正门前举行罢工，计划持续 3 天。该工会会员总数为 3 万人，约占三星电子员工总数（12.5 万人）的 24%。据悉，在 8 日的罢工中，工会推算有 4000 至 5000 人参与，三星公司和警方则估计有 3000 人参加。</p><p></p><p>韩媒称，这是三星电子成立 55 年来首次爆发大规模罢工。此前在 6 月初，工会部分成员曾利用休年假的形式罢工 1 天。工会此次提出的主要诉求有：全体工会成员薪酬上调、改变奖金标准、公司履行带薪休假承诺，以及对因罢工导致的工资损失进行补偿等。工会主席在接受采访时还表示，公司不透明的奖金计算方式，导致员工对自身利益的不确定性增加；若公司在 10 日前未拿出解决方案，工会将于 15 日起进行第二阶段的罢工。</p><p></p><p>韩国 SBS 电视台称，半导体部门的员工是此次罢工的主力。三星公司称半导体生产线的运行没有受到重大影响，但《东亚日报》报道称，即使许多生产线实现了自动化，操作这些生产线的重要人员也很难替换。半导体生产线一旦停止运转，恢复生产需要耗费大量时间和成本。</p><p></p><p></p><h5>腾讯全员邮件宣布调薪：员工月工资增加 3200 元等</h5><p></p><p></p><p>7 月 10 日，腾讯内部向全员发布邮件称，将调整内部的薪酬福利政策，对薪酬结构做出调整。</p><p></p><p>校招生的房补从每月 4000 元调整为按 15 个月发放，并将其纳入月薪基数中。调整后，员工每月基本工资增加 3200 元，多出来的三个月将在年终奖一起发放。根据资料，腾讯公司给校招生提供的房补标准为每月 4000 元（北上广深地区为 2000 元），三年共计 14.4 万元。员工服务奖（13 薪）从年底发放调整为平摊到 12 个月，并加入月薪基数中。腾讯邮件中称，这两个举措旨在帮助大家在更高、更稳定的月收入基础上更安心地安排工作与生活。相关调整于 2024 年 7 月 1 日起生效，8 月 5 日的发薪中开始体现。</p><p></p><p></p><h5>大模型人才紧缺，字节跳动加速争夺全球高校顶尖技术人才</h5><p></p><p></p><p>近日，字节跳动“筋斗云人才计划”启动。该计划是字节跳动面向优秀校园技术人才推出的专项招聘，意图在全球范围内，吸引和招募有志于用技术创造突破性价值的顶尖学生。</p><p></p><p>据悉，本次招聘涵盖 AI 应用、搜索、推荐、广告、AI for Science、AI Safety、机器人、隐私与安全、硬件、视频架构、工程架构等技术领域。招聘的目标群体是 2024 年 9 月 -2025 年 8 月毕业的博士群体，重点针对有亮眼学术成果、拥有顶会顶刊论文或专利的学术达人；有丰富的大赛经历，在国际知名竞赛中取得优异成绩的竞赛达人；或有极强的实践能力，参与过重大项目，擅长解决难题的实战达人。</p><p></p><p>在大模型相关技术人才招聘上，字节跳动是国内互联网大厂最积极的公司。据一位大模型行业人士透露，今年字节跳动的 AI 人才招聘规模最大。脉脉高聘人才智库数据印证了这一信息：今年上半年，字节跳动位列新发人工智能岗位最多的企业。从招聘指数上看，字节跳动以 9.53 位居第一，大幅领先于小红书（7.96）、蚂蚁集团（5.84）、美团（4.86）、腾讯（2.48）等互联网大厂。</p><p></p><p></p><h5>去哪儿员工每周三、周五可自主选择办公地点</h5><p></p><p></p><p>7 月 9 日，去哪儿 CEO 陈刚发全员信宣布，从 7 月 15 日起，每周三、周五，员工可以灵活选择办公地点。陈刚在信中强调，员工按规定混合办公，“无需任何申请审批”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/090608dcb8e70b99b8a1aedee53a7e7f.jpeg" /></p><p></p><p>据了解，混合办公的适用人员范围以入职 6 个月以上的标准工时正式员工为主。</p><p></p><p>去年 10 月，去哪儿开始了为期 9 个月的混合办公试验。回收数据显示，员工对混合办公的各个维度反馈正面 —— 超过九成的员工认为混合办公后幸福感有明显提升，员工主动离职率在混合办公后下降了三成。</p><p></p><p>去哪儿 COO（首席运营官）刘连春表示，“混合办公没有让公司业绩变坏，并且显著提升了员工的幸福度。那这件事情公司何乐而不为呢？”</p><p></p><p></p><h5>微软要求中国区员工必须使用 iPhone？微软回应</h5><p></p><p></p><p>7 月 9 日消息，据媒体报道，微软中国员工已被告知，登录公司系统时必须使用 iPhone 进行身份验证。从 9 月起，将禁止使用 Android 智能手机作为多因素身份验证设备。</p><p></p><p>据介绍，此举属于微软全球安全未来计划的一部分，将影响中国大陆的数百名员工，旨在确保所有员工都使用微软 Authenticator 密码管理器和 Identity Pass 身份验证应用。另有消息显示，由于部分中国安卓设备不支持谷歌服务，微软将为受影响员工提供 iPhone15 作为工作手机。</p><p></p><p>一位微软发言人回应表示：「Microsoft Authenticator 和 Identity Pass 应用程序已正式在 Apple Store 和 Google Play Store 上架。我们希望为员工提供访问这些必要应用程序的途径，由于本地区无法使用 Google 移动服务，我们即向员工提供了例如 iOS 设备的选择。」</p><p></p><p>对此，不少网友称，若是能配发工作机就没问题。但若强制要求员工自行购买，则“不能接受”。</p><p></p><p></p><h5>阿里云宣布与月之暗面“联姻”：帮 Kimi 技术突破</h5><p></p><p></p><p>7 月 8 日，阿里云官宣两位新“代言人”——月之暗面科技有限公司创始人杨植麟和智联招聘集团总裁张月佳。</p><p></p><p>这是月之暗面首次公开与阿里云的合作情况。信息显示，阿里云的算力和大模型服务平台，助力月之暗面提升模型推理效率，加速 Kimi 智能助手实现技术突破。此外，智联招聘集团的大模型应用，也基于阿里云实现快速部署和上线支持。</p><p></p><p></p><h5>AMD 重砸 6.65 亿美元收购欧洲最大私人 AI 实验室 Silo AI</h5><p></p><p></p><p>AMD 宣布以价值约 6.65 亿美元的全先进交易价值收购欧洲最大的私人 AI 实验室 Silo AI。该收购案预计在 2024 年下半年完成。</p><p></p><p>收购完成后，Silo AI 首席执行官兼联合创始人 Peter Sarlin 将继续领导 Silo AI 团队，向 AMD 高级副总裁 Vamsi Boppana 汇报工作。</p><p></p><p>据了解，Silo AI 总部位于芬兰赫尔辛基，业务遍及欧洲和北美，专注于端到端 AI 驱动解决方案，帮助客户快速轻松地将 AI 集成到其产品、服务和运营中。他们的工作涉及不同的市场，客户包括安联、飞利浦、劳斯莱斯和联合利华。除了 SiloGen 模型平台外，Silo AI 还在 AMD 平台上创建了最先进的开源多语言 LLM，例如 Poro 和 Viking。</p><p></p><p>AMD 在新闻稿中表示，此次收购代表该公司基于开放标准并与全球 AI 生态系统建立强有力的合作伙伴关系，并提供端到端 AI 解决方案的战略又迈出了重要一步。Silo AI 团队由世界一流的 AI 科学家和工程师组成，他们拥有丰富的经验，为云、嵌入式和终端计算市场的领先企业开发量身定制的 AI 模型、平台和解决方案。</p><p></p><p></p><h5>多方监管增压，微软放弃参与 OpenAI 董事会</h5><p></p><p></p><p>7 月 10 日，据媒体报道，随着欧美监管机构加强对人工智能市场的反垄断审查，微软公司决定放弃在美国开放人工智能研究中心 (OpenAI) 董事会中的观察员席位。</p><p></p><p>微软 9 日致函 OpenAI 说明上述决定，并解释称，OpenAI 自去年发生董事会人事震荡以来，经营管理已有改善，因此不再需要微软参与。微软选择放弃观察员席位，决定“立即生效”。</p><p></p><p>据报道，去年 OpenAI 首席执行官萨姆·奥尔特曼“离职又复职”风波过后，微软在 OpenAI 董事会获任无投票权观察员。据此前报道，微软支持并短暂聘用过奥尔特曼。</p><p></p><p></p><h5>小红书被曝获 DST 投资，估值 170 亿美元</h5><p></p><p></p><p>7 月 11 日消息，小红书获得了风险投资公司 DST Global 的支持。三位知情人士透露，小红书在最近几周进行了股份出售，公司估值达到 170 亿美元。</p><p></p><p>DST Global 曾投资过 Facebook，并与红杉中国一起参与了小红书这一轮投资，红杉中国增加了其现有股份。此外高瓴资本、博裕资本和中信资本也进行了跟投。</p><p></p><p>此前有消息称，小红书在 2023 年首次实现盈利。据四位知情人士透露，小红书去年净利润达 5 亿美元，营收达 37 亿美元。</p><p></p><p></p><h2>大模型一周大事</h2><p></p><p></p><p></p><h4>大模型发布</h4><p></p><p></p><p></p><h5>OpenAI 绝密项目「草莓」首次曝光，内部人士曾称其可能威胁人类</h5><p></p><p></p><p>7 月 13 日，据外媒报道，OpenAI 内部正在一个代号为「草莓（Strawberry）」的项目中开发一种新的人工智能模型。该项目的细节此前从未被报道过，而 OpenAI 正努力证明其提供的各类模型能够提供高级推理能力。</p><p></p><p>当被问及上述所说的草莓技术时，OpenAI 的发言人在一份声明中表示：“我们希望自身 AI 模型能够像我们（人类）一样看待和理解世界。持续研究新的 AI 能力是业界的常见做法，大家都相信这些系统的推理能力会随着时间的推移而提高。”</p><p></p><p>尽管发言人并未直接回应有关“草莓”项目的问题，但媒体报道指出，该项目之前被称为 Q*，而 Q*正是去年导致 OpenAI CEO 被意外解雇的重要导火索。</p><p></p><p>OpenAI 的内部人士曾向董事会发出警告，称 Q* 的重大发现可能对全人类构成威胁。</p><p></p><p>媒体推测，Q* 可能具备 GPT-4 所缺乏的基础数学能力，这可能意味着它具有与人类智能相媲美的推理能力。而这可能标志着 OpenAI 在实现其 AGI 目标方面迈出了重要一步。</p><p></p><p>蚂蚁集团开源 EchoMimic：支持为人像照片对口型、生成肖像动画视频</p><p></p><p>近日，蚂蚁集团推出了一项开源项目——EchoMimic，这款 AI 工具能够根据声音内容，为照片中的人物创建逼真的口型同步动画。</p><p></p><p>EchoMimic 具备较高的稳定性和自然度，通过融合音频和面部标志点（面部关键特征和结构，通常位于眼、鼻、嘴等位置）的特征，可生成更符合真实面部运动和表情变化的视频。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f920b20ecea13d28d4a7d6483eec24e1.png" /></p><p></p><p>EchoMimic 的技术核心在于其能够精确捕捉音频信号和面部特征之间的关联，并以此为基础生成动画。在训练过程中，EchoMimic 采用了先进的数据融合技术，确保了音频和面部特征的有效整合，从而提高了动画的稳定性和自然度。</p><p></p><p>经过与多个公共数据集和自收集数据集中的替代算法进行的全面比较，EchoMimic 在定量和定性评估方面均展现出卓越的性能。这一点在 EchoMimic 项目页面上的可视化效果中得到了充分体现。</p><p></p><p></p><h5>腾讯开源 web 端地图组件库 tlbs-map</h5><p></p><p></p><p>7 月 11 日，腾讯开源了其基于腾讯位置服务 JavaScript API 封装的地图组件库 —— tlbs-map，用于在网页上绘制地图，并在地图上绘制点、线、面、热力图等效果。它支持 Vue2、Vue3、React 等主流技术栈，可以帮助开发者降低地图开发的成本。</p><p></p><p>据官方介绍，tlbs-map 封装腾讯地图 API 为响应式组件，无需关心复杂的地图 API，只需要操作数据即可；同时，组件提供地图和图层实例，用户可编写自定义组件或直接调用地图 API 满足定制化需求。</p><p></p><p>为了方便开发者使用，tlbs-map 还提供了详尽的组件使用文档和示例代码，可以帮助开发者轻松上手，快速开发。</p><p></p><p></p><h5>智谱 AI 开源推出视频理解模型 CogVLM2-Video</h5><p></p><p></p><p>7 月 12 日，智谱 AI 提出了一种基于视觉模型的自动时间定位数据构建方法，生成了 3 万条与时间相关的视频问答数据。基于这个新数据集和现有的开放领域问答数据，引入了多帧视频图像和时间戳作为编码器输入，训练了一种新的视频理解模型—CogVLM2-Video。</p><p></p><p>智谱 AI 表示，目前视频理解的主流方法使模型失去了时间感知能力，无法准确地将视频帧与精确的时间戳关联起来。因此，模型缺乏时间定位、时间戳检测和总结关键时刻的能力。为了解决这些问题，团队提出了 CogVLM2-Video，这是基于 CogVLM2 图像理解模型的扩展视频模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fd0403bf707372f114caf13973410f00.jpeg" /></p><p></p><p>该模型不仅在开放域问答中实现了先进的性能，还能感知视频中的时间戳信息，从而实现时间定位和相关问答。</p><p></p><p>具体来说，这种方法就是从输入视频片段中提取帧，并为其注释时间戳信息，使后续的语言模型能够准确知道每一帧在原视频中对应的确切时间。</p><p></p><p></p><h5>几分钟生成四维内容，还能控制运动效果：北大、密歇根提出 DG4D</h5><p></p><p></p><p>近期，商汤科技 - 南洋理工大学联合 AI 研究中心 S-Lab ，上海人工智能实验室，北京大学与密歇根大学联合提出 DreamGaussian4D（DG4D），通过结合空间变换的显式建模与静态 3D Gaussian Splatting（GS）技术实现高效四维内容生成。</p><p></p><p>据悉，四维内容生成近来取得了显著进展，但是现有方法存在优化时间长、运动控制能力差、细节质量低等问题。DG4D 提出了一个包含两个主要模块的整体框架：1）图像到 4D GS ；团队使用 DreamGaussianHD 生成静态 3D GS，接着基于 HexPlane 生成基于高斯形变的动态生成；2）视频到视频纹理细化 ；团队通过细化生成 UV 空间纹理映射，并通过使用预训练的图像到视频扩散模型增强其时间一致性。</p><p></p><p>值得注意的是，DG4D 将四维内容生成的优化时间从几小时缩短到几分钟，允许视觉上控制生成的三维运动，并支持生成可以在三维引擎中真实渲染的动画网格模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f66fd24fd90c1c7d1201310546cfa29.gif" /></p><p></p><p></p><h4>企业应用</h4><p></p><p></p><p>7 月 10 日，谷歌宣布将于本月底向所有谷歌账号用户开放「暗网报告」功能，旨在帮助用户快速了解网络上发生的个人数据泄露事件，并提供相关漏洞信息的搜索服务。7 月 10 日，阿里推出专为科研人员、高校教师和学生、职场人士研发的大模型应用产品心流，其产品定位为用户的 AI 搜索助手，提供智能搜索、知识问答、智能阅读、辅助创作等能力。7 月 10 日，夸克宣布升级“超级搜索框”，推出以 AI 搜索为中心的一站式 AI 服务，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。7 月 11 日，三星表示将在今年推出基于自己人工智能（AI）模型的升级版语音助手 Bixby，据悉，这次 Bixby 的升级是三星在其设备套件上推广人工智能功能的一部分。7 月 12 日，粉笔发布了基于首个专注于职教行业的垂域大模型 AI 产品——粉笔 AI 老师 “粉笔头”，旨在让 AI 帮助老师化身“高效能人士”，向学员提供更有针对性的服务。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</id>
            <title>大模型产品琳琅满目，企业应该如何选择？</title>
            <link>https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 08:54:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 大模型, 企业, C 端
<br>
<br>
总结: AI 和大模型在企业应用中的重要性日益凸显，企业需要考虑如何有效融入大模型到业务中，尤其在面向C端应用时更应注重用户体验。与此同时，企业在选择大模型时需要深入理解应用场景，并逐步推广。在大模型的商业化和投资方面，企业需要考虑未来的可替代性，以做出合理的选择。 </div>
                        <hr>
                    
                    <p></p><p>AI 和大模型方兴未艾，我们每天都在看到和尝试不同版本、不同品牌的大模型产品，它们的能力各不相同。无论是个人还是企业，都在思考如何尽早地参与进来到大模型的浪潮当中来。</p><p></p><p>目前，一些先锋企业已经将 AI 和大模型融入到他们的日常业务和产品中，并取得了不错的效果。但更多企业仍处于观望或迷茫状态。在有限的预算内，企业要怎样进行 AI 和大模型的商业化或投资？该选择怎样的大模型融入业务？带着这些问题，InfoQ《极客有约》特别邀请了广东 CIO 联盟会长、前海尔集团 CIO 李洋老师和北京中关村科金公司 CTO 李智伟老师，与 InfoQ 社区编辑张凯峰一同探讨企业如何在众多大产业和大模型产品中做出合理的选择。对话内容部分亮点如下：</p><p></p><p>● 企业开发大模型应用时，应该更多地考虑用户体验；</p><p></p><p>● 企业需要对应用场景有深入理解，并从试点开始逐步推广；</p><p></p><p>● 对于面向 C 端的应用，“+AI”是个不错的选择；</p><p></p><p>● 企业在进行系统建设时，必须考虑到未来的可替代性。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p></p><p><a href="https://www.infoq.cn/video/0KjL5Et9SFyJ6Yrryyqf">https://www.infoq.cn/video/0KjL5Et9SFyJ6Yrryyqf</a>"</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们设置了「大模型数据集构建及评测技术落地」专题，本专题将深入探讨大模型的需求分析与数据收集、数据清洗与增强、模型评测与优化，以及技术落地与维护等关键方向。目前大会 9 折购票优惠中，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>To C 与 To B 场景和市场需求差异</h2><p></p><p></p><p>张凯峰：在 To C 和 To B 场景中，使用 AI 和大模型有什么区别？或者当企业负责人考虑将 AI 引入自己的企业时，通常会考虑哪些方面？</p><p></p><p>李智伟 ：通过查看最近 ChatGPT 一周的数据，可以发现其用户活跃数量超过了一亿。虽然这一数字很高，但大部分用户使用的都是 To C 的一些应用程序。国内的一些应用程序用户数量也很多，从用户教育角度来看，进展比较快。C 端应用程序的发展将会非常迅速。目前广泛使用的 agent 平台或 model builder 平台，都面向企业提供服务，提供公共云服务或者私有化服务。国内大部分公有云上的托管服务都由个人或小微企业进行使用。国内比较好的头部企业，其训练的模型约有 1.3 万个。</p><p></p><p>同时，通过查看今年 1 到 6 月份的公开招标网数据，我们可以看到大企业招标的情况。今年上半年到 6 月中旬，整个公开招标数量约为 234 件。其中，60% 以上的项目来自央国企。预计今年大企业客户对大模型的商业化使用将加速。</p><p></p><p>从 C 端来看，很多客户已经理解了大部分内容，但 B 端的进展仍然处于早期阶段。</p><p></p><p>李洋：从目前的趋势来看，人工智能是一个新质生产力，是工业革命的一部分。从国际上来看，人工智能的浪潮也比以往席卷得更快。其原因在于，它是由 C 端发起的。目前可以感知到的是，要把科技的生产力提高民众的感知度。对于企业来说，可能包括员工、客户以及上下游。C 端这种蜂拥而来的趋势就奠定了这次人工智能浪潮会高于前几次的基础。</p><p></p><p>对于 To B，我认为应该是未来科技革命所产生的生产力要兑现的一个非常重要的路径。目前可以看到的很多一二级市场的投资，对 C 端的投资还在逐渐增长。但如果缺乏一定的杀手级应用，甚至没有持续的宣传和科技元素的不断注入，这种热度很快就会退去。</p><p></p><p>对于 OpenAI 或国内许多做大模型的企业，无论是自研还是开源，要在 C 端实现商业变现都很难。因此，在 C 端巩固之后，随着大模型的成熟，To B 的发展应该会逐渐加速，但未来是否一定会发展成大模型还有待观察。</p><p></p><p>张凯峰：可见，C 端市场和 B 端市场所面临的情况、消费习惯以及背后的经济投入都是完全不同的。对于企业来说，AI 和大模型的应用还处于初级阶段。是否是因为满足 C 端服务更容易，但企业侧复杂的业务需求和市场竞争等因素，导致企业在接纳 AI 和大模型方面比 C 端更困难？这将具体表现在哪些方面呢？</p><p></p><p>李智伟：我认为这个问题与大模型的能力有关。大语言模型的技术能力可能更多地体现在知识的理解和生成方面更加成熟，因此在构建 C 端应用的场景中，它的融入是比较快速的。</p><p></p><p>对于知识的获取，大模型本身也采用了推荐的方式来提供服务。这种方式的技术更加成熟，能够更好地与用户交互。许多类似的 APP 或个人助手都提供了知识获取的功能。Perplexity AI 和国内的一些创新者正在开发类似搜索的应用，并且致力于提升 C 端用户的体验。当它面向 B 端企业渗透时，我们需要考虑其商业化能力。目前，商业能力主要集中于互联网、教育、金融和政企服务等行业。这些行业有一个共同点，即服务于大量 C 端用户。</p><p></p><p>面向个人用户的体验一定会延伸到企业端的员工使用中。在当下这一波浪潮中，当企业开发大模型应用时，应该更多地考虑用户体验。这也是我们一直致力于做的事情，因为我们是一家传统的营销和客户服务类产品公司，我们基本上都在开发交互类产品。</p><p></p><p>我们之所以非常重视大模型技术，是因为我们认为这项技术实际上是对整个交互体验的颠覆性升级，这是一个未来的巨大机会市场。因此，我们基本上也是将 C 端的体验产品能力应用于 B 端，以实现更深入的发展。</p><p></p><p>李洋：总的来说，C 端和 B 端的触点不太一样。以我个人为例，作为一个纯粹的 C 端用户，我对新科技产品的需求更多关注于体验感、科技感以及方便易用等方面。这也包括了一些家居、生活和工作的便利性，这些方面可以归为 B2C 类别。而从 B 端的角度来看，我们可以将大模型的应用或 AI 战略应用于整个企业中。特别是在后疫情时代，我们需要降低成本、提高效率，甚至创新商业模式，寻找新的利润增长点。因此，在企业内部，对于大模型或新技术的使用，其想法、构建和步骤可能会有所不同。</p><p></p><p></p><h2>大模型的选择与匹配</h2><p></p><p></p><p>张凯峰：通常我们会采用哪些方法来帮助企业识别在某些业务或方向上可以开始选择大模型，并与自己的业务需求相匹配。您们是否听说过一些成功或有待改进的例子，以及它们是如何操作的？</p><p></p><p>李智伟：我想先分享一下我们在做企业应用时遇到的问题。在过去一年多的时间里，很多企业的决策者都在问我们如何选择。实际上，我们并没有给出明确的答案。我认为这是一个逐步认知和迭代的过程，与企业构建的业务场景密切相关。</p><p></p><p>但是这次的情况有所不同。在传统 IT 系统中，我们通常以功能性为主导，根据客户需求构建系统。但是现在，由于整个 IT 企业架构的变化，核心变成了一个模型，我们需要将之与客户的业务深度融合，这无疑是一个很大的挑战。目前我们面临的冲突点是，很多企业出于自身发展或国家要求，会积极与厂商合作，很多场景都会进入。但是因为我们不熟悉所有场景，用户有时会受到限制。</p><p></p><p>企业需要做几个部分的工作。如果我们将这看作是一个流程，我可以提供一些具体的建议。例如，通常我们会先梳理功能性需求，但现在做法不同了。我会告诉客户，首先要进行认知对齐。如何让不同的人对大模型的认知保持一致呢？首先，参与项目的人员需要具备大模型的基本原理和能力；其次，客户方也需要有懂得大模型应用的专业人士。</p><p></p><p>其次，我们需要选择一些小的场景作为试点，以便快速响应市场变化。对于供应商来说，他们需要拥有敏捷的工具链和 demo 系统来帮助他们进行试点。去年，我们建立了一个工具链平台，并在官网上开通了线上 demo 系统。客户可以在了解系统之前先进行试用，确保能够接受它的外观和效果。试点是一个双方共创的过程，试点结束后，需要进行效果评估，然后双方再对产品进行规划并分段实施，最后总结反馈。大模型应用更注重端到端效果的优化。企业需要对应用场景有深入理解，并从试点开始逐步推广。从这个角度看，市场上可供选择的选项并不多，对于企业来说，虽然我们正在努力加速商业化，但更合理的是要看到其中的节奏。</p><p></p><p>李洋：在企业中使用 AI 和大模型的切入点比较多。建议企业在做这方面时，先确认需求，再定义相应的工具。在我的数字化转型工作中，我把它分为延产供销服务、运营风控等方面。不同行业的侧重点会有所不同。</p><p></p><p>当然，不仅限于 AI，还有区块链、云计算、大数据等技术，它们与我们所说的业务数字化层面和流程有关，以及我们所说的痛点或难点，哪些可以使用大模型或 AI 来解决？从目前的情况来看，我非常同意李总的观点。现在的问题可能是，由于大模型的火爆程度超出了某些企业的承受范围，导致我们的应用目标本末倒置。我们不应该拿着锤子去找钉子，而是应该根据钉子的特点选择合适的工具，包括大模型。</p><p></p><p>今天我还与一些企业进行了交流，他们认为，传统的机器学习、简单的规则和深度学习的神经网络也可以解决问题，不一定非要使用高量级的大模型，特别是那些对算力和数据要求很高、成本也很高的模型。因此，我认为我们应该从业务数字化和智能化的方向出发，进行全面规划，然后逐一比较，看看哪些问题可以使用人工智能工具来解决。在使用人工智能工具时也必须考虑成本、效率和效益等核心指标。</p><p></p><p>李智伟：对于这个话题，市场上有不同的提法，如“+AI”或“AI+”。“+AI”是指在现有能力的基础上添加相应的能力，而“AI+”则指下一代能力系统。我认为，对于面向 C 端的应用，加入 AI 是个不错的选择。对于 B 端应用而言，企业通常需要考虑如何重新利用现有的 IT 资产，使用 AI 原生技术的成本很高。</p><p></p><p>对于初创公司来说，重新构建企业架构的成本是难以承受的。因此，我们更倾向于鼓励企业采用 AI 技术来增强自身能力，这也是一种很好的 IT 演进思路。</p><p></p><p>张凯峰：除了自己训练大模型，还有一种可能就是用一个相对成熟的模型来训练一个自己垂直领域的小模型，供企业内部使用。这是企业在做大模型时需要考虑的选择方向之一。还有其他的方向吗？</p><p></p><p>李洋 ：现在大模型的应用模式一般分为以下几种：</p><p></p><p>● 提示词工程：使用大模型不需要重新训练或者构建数据集，但由于大模型自身的泛化能力和通用能力，企业可以通过提示词来进行引导，从而使得模型生成解决方案、文案等。</p><p></p><p>● RAG，可作为大模型的补充。作为外挂，在检索或提问过程中可以将数据融合到模型中，并生成相应结果。</p><p></p><p>● 微调，或称精调：企业可在确保质量过关的情况下，使用小部分数据，挑选出自己的模型，并将专业知识和私有数据融入其中。</p><p></p><p>● 预训练：如金融行业中的一个不普遍的领域，为了训练这种行业大模型，企业需要将购买或开源的大模型中的数据重新进行训练，使其获得具有金融行业或其他专业领域知识的行业模型。基本上可以参考以上几种方式来使用大模型。在封装开源模型的过程中，可能需要采用一些综合应用的方法。如在前期使用一些提示工程，在后期添加微调。</p><p></p><p>李智伟：对于企业来说，是否需要大模型，以及大模型的数量多少问题，需要看具体场景。在书写公文或者分析金融报告时，可以使用一个模型，无论是 prompt 还是 FT。而对于更加专业的领域而言，可能需要使用 FT，并为每个任务提供精标数据进行训练。对于整个企业来说，必须采用多模型。在小模型时代，我们在构建基于模型的软件和系统架构时，就已经采用了多模型组合的方式。而现在，更加明确的是，从扩展架构的角度来看，应该采用大模型、小模型和 RAG 的组合方式。</p><p></p><p>企业不能只依赖一个模型。现在的模型架构是大模型负责调度编排，小模型负责完成特定任务，任务完成后，我们需要把所有输出汇总并呈现给最终用户使用。目前来看，RAG 增强技术也不需要模型了，大部分只需要做 prompt。小模型的获取方式有两种：利用原有 IT 资产中的小模型，即资产再利用；另一种是在基于大模型训练后，通过剪枝和蒸馏等技术将神经网络缩小，得到小模型。去年，很多企业都认为一个模型可以解决所有问题，甚至花费数千万购买大模型。但现在人们已经转换了思想。例如我们在去年一直为一家零售客户制作电销大模型，由于我们公司在过去十几年中一直从事客服工作，所以我们使用小模型实现智能拨打电话，而大模型出现后，我们认为大模型对我们的业务更加有利，进行了替换。</p><p></p><p>大模型的应答效果和对话效果都比小模型要好，但是在效率方面存在问题。当时我们的做法是加大量的 GPU，提高并发性。但是现在看来性价比极低。从整个 IT 构建来看，只能先解决准确率问题。但是需要考虑到，长此以往，性价比是支撑不住的。我们需要考虑小型化的问题，比如通过模型的裁剪或者蒸馏来实现小型化。我们甚至需要将原本使用的小模型加入到中间过程中。例如在完成某项特殊任务时，小模型效果比较好，可以使用小模型来尽可能地减少大模型进行交互判断处理的工作量。随着时间的推移，我们的整体成本和算力需求正在逐步下降。</p><p></p><p>相比之前使用纯小模型，目前我们整体的外呼发起率可以提升到 30%，大屏通话也能增长 50%，这都是大模型带来的好处。此外，使用大模型与使用传统的人工呼叫相比，也有利于降低客户不满意度。</p><p></p><p>张凯峰：刚刚提到的模型更换问题，可以再展开一下吗？比如，在什么情况下企业需要为当前的投资考虑未来替换的可能性，以及在替换之前需要做好哪些准备工作？</p><p></p><p>李智伟：关于这个话题，实际上受到两个因素的影响。</p><p></p><p>第一个方面是，早前在大模型 GPT3.5 发布那时候，一些开源模型也随之出现，但我们当时使用下来发现这些开源模型其实效果达不到预期。为了达到我们想要的效果，还是需要重新做 SFT，当时的想法是所有的模型都是需要做训练的。但是随着基础模型的发展，特别是 ChatGPT 4o 的发布，现在国内的开源模型已经可以满足基础的需求。在过去的半年中，市场上训模型这件事其实慢慢变少了，基础模型可以直接用于一些常规场景，甚至进行信息的获取和整理。</p><p></p><p>第二个方面，目前国内虽然有约 200 家大模型和各种模型的生产公司，但都没有成熟的商业模式。市场上既存在开源模型，也有闭源模型。大多数大公司都倾向于闭源，但实际上这些做闭源的公司都希望通过消耗更多的云算力来盈利，这比销售单个模型更加有利可图。但这也带来了一个问题，如果一个企业的供应商消失了，其发动机怎样进行维修呢？</p><p></p><p>因此，企业在进行系统建设时，必须考虑到未来的可替代性。这意味着企业必须要在前期就考虑这个问题。在建构整个架构时，需要考虑到两个方面：</p><p></p><p>首先，我们需要一套供应链，以便在未来能够更换其他供应商。</p><p></p><p>还需要重点考虑一个问题。例如，去年我们开始训练大模型，并且为企业提供了许多输出。但是年初开始，当我们意识到这个问题后，就投入了大量的研发精力来开发模型部署工具链。它可以帮助企业监控和运营多个模型，甚至可以轻松地替换、上线和下线部署托管。一套标准化的能力体系可以对未来产生最大价值。这可以保证基础模型趋向标准化。企业可以在任何时间选择最适合自己的资产，还可以使用更加强大的模型来替换现有最低版本。</p><p></p><p>第二种情况是，企业针对自己的行业知识进行训练，这要求企业具备快速部署和完成的能力。此外，企业未来的混合部署需要一定的工具链支持。</p><p></p><p>张凯峰：在考虑采购国内的大模型时，无论是大厂的模型还是自行预训练的模型，哪些非功能性方向是我们需要仔细考虑的呢？</p><p></p><p>李智伟：除了要考虑性价比之外，企业还需要考虑准确性、鲁棒性和稳定性等因素。在选择模型时，企业需要考虑其应用场景。例如，在线系统需要更高的时效性，因此可能需要混合性模型部署。这是第一个。</p><p></p><p>第二个是，由于模型的泛化可能会带来负面影响，比如幻觉问题，因此在严肃场景之下，必须进行针对性数据增强训练。然而，增强训练会带来一个问题，随着模型参数的增加，其准确性和严肃性会提高，但效率也会降低。因此，需要采取一些措施，例如进行裁剪或蒸馏，以提高性能。但是在非严肃场景下，需要的是模型的泛化性，对性能指标的要求会降低。</p><p></p><p>近期我还非常关注模型的安全和合规，目前大模型在安全合规方面还有待提升。企业需要考虑到个人隐私保护问题。有多少企业的原始数据经过了严格的隐私清洗和认证呢？</p><p></p><p>另一个是多模态大模型问题。目前，多模态对数据的使用更加深入。此前，大语言模型更多关注到的是文本类理解使用。但涉及到多模态，就要考虑对于视觉、音频和视频的理解和使用，在这个过程中，数据安全是极其重要的。需要企业完成两个任务：第一，采购大模型时，需要考虑其合规性和安全性；第二，大模型使用必须经过备案，并接受审查。</p><p></p><p>李洋：人工智能领域，特别是现在的大模型和未来的发展方向，可能会像云计算一样。随着大数据等新兴技术的发展，网络安全合规方面也会有相应的审查标准。例如，如果大模型的服务提供商类似于我们的公有云提供商，作为租户，调用大模型时，租户与平台之间的责任共担和举证是非常重要的。此外，在选择大模型时，还需要考虑运营团队的能力。</p><p></p><p>大模型需要具备底座和二次开发及优化能力，但建立大团队不太现实，因此需要依赖服务商提供的人工智能，包括架构师和科学家的能力。人工智能的发展具有不确定性，可能会出现幻觉、误导或暴力等后果。我们需要考虑到的是，如何训练、采购和使用大模型，以及如何对其进行完善实施和调整。平台提供方仍在不断改进中，因此需要一个团队的支持。</p><p></p><p></p><h2>企业盈利新机遇</h2><p></p><p></p><p>张凯峰：除了模型制造商之外，那些从事大模型应用的企业，他们的盈利方向和模式可能会在哪里呢？可以结合自己的经验和故事来分享一下。</p><p></p><p>李智伟：在 B2B 领域，现在还处于早期阶段，各家还没有实现盈利的商业模式。</p><p></p><p>这次技术革命将带来许多新机会。我们以前在做内容审核时，会使用小模型。例如，做内容审核需要积累大量数据并训练专家。如果想审核某个涉政类的内容，就需要储备很长时间。但是，如果使用多模态大模型，就可以快速进入审核市场。这是对以前技术的一种弯道超车的机会。</p><p></p><p>在 B2B 领域盈利，对于企业来说，是一个非常多元化的机遇。但在中国市场，能否盈利仍然存在很大的不确定性。这种多元化的机会也带来了一些好处，比如以前从未涉足的新市场会带来新的机遇。</p><p></p><p>多模态模型和大语言模型在这一过程中都已经被开源甚至公开化了。因此企业可以更容易地进入这个市场并积累财富。</p><p></p><p>李洋：首先，AI 作为一种科技手段，只要能起到促进作用，它一定会促进原本产业的发展。比如在抖音、微信运营或公众号运营过程中，如果能在原有产业中嵌入 AI 元素，或者通过大模型实现促进，那么就可以实现盈利。但另一方面，提供大模型的公司只要向 B 端或 C 端提供相应的应用程序，就能盈利。例如，Stable Diffusion 平台或 OpenAI 开发的类似 GPT 的大模型应用程序或产品，只要被腾讯、抖音等公司采用，就可以利用大模型盈利。但是还有一个问题，在数字化和智能化的过程中，能否创造出新的盈利模式。例如华为、四大咨询公司（麦肯锡等）提供的咨询服务，如果企业能将这些咨询服务整合成一个大模型，并利用大数据分析技术，那么实施周期和效率可能会比传统方式更快。此外，还需要解决数据脱敏和隐私保护等问题。</p><p></p><p>企业应该将这种科技手段与所有产业结合起来。从第一次工业革命到第四次工业革命，我们一直在追求生产力的提升，但我们仍然需要抓住自己的主业。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</id>
            <title>Karpathy又整活儿了！一天训练出GPT-2、成本还骤降100倍，网友：dream老黄把价格再打下来</title>
            <link>https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:52:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, GPT-2, Andrej Karpathy, llm.c
<br>
<br>
总结: 本文介绍了OpenAI创始成员、前研究科学家Andrej Karpathy最近在llm.c中重现GPT-2的过程。Karpathy使用了15.58B参数的完整版本的GPT-2，并通过改进计算、软件和数据等方面，成功在24小时内以672美元的成本对该模型进行了重现。文章还提到了Karpathy的职业经历和他开发的llm.c，以及对GPT-2训练成本和硬件利用率的估算。最后，文章对比了Karpathy复刻的GPT-2与19年版本的GPT-2的输出结果，发现新模型的输出质量与GPT-2相当。 </div>
                        <hr>
                    
                    <p>OpenAI 创始成员、前研究科学家Andrej Karpathy 最近尝试在llm.c中重现了GPT-2。这里的GPT-2是15.58B参数的完整版本，最初亮相于OpenAI 2019年2月14日发布的博文《Better Language Models and their Implications》当中。</p><p></p><p>“2019年时，GPT-2 的训练工作还是一个涉及整个团队、需要规模化投入的项目。但如今5年过去，随着计算（H100 GPU）、软件（CUDA\cuBLAS、cuDNN、FlashAttention）和数据（例如FineWeb-Edu数据集）等层面的改进，我们已经能够在24个小时之内凭借单个八H100节点成功对该模型进行重现，且总成本仅为672美元。”Karpathy 说道。</p><p></p><p>Karpathy 在2017年离职后进入特斯拉担任AI 高级总监，但在2023年再次回到OpenAI组建团队，并推出了 ChatGPT。一年后，Karpathy离开了OpenAI，并出于教育意义开发了llm.c。llm.c 是简单、纯 C/CUDA 的 LLM（总计约5000行代码），无需使用涉及Python解释器或者高复杂度深度学习库（例如PyTorch/JAX、huggingface/transformers 等）的典型训练技术栈。</p><p></p><p>在Karpathy 公布了这一结果后，有网友问到当时训练 GPT-2 的成本，Karpathy 回答道：</p><p></p><p>这些信息从未公开过，但我估计成本要高得多。按乘数倍率来算，数据方面可能要高了 3 倍，硬件利用率方面高 2 倍。2019 年的计算集群可能使用的是 V100 (~100 fp16 TFLOPS)，现在可能换成了 H100 (~1,000)，这样算下来性能大概提高了 10 倍。所以成本方面非常粗略地估计，可能要高出 100 倍，也就是大约 100,000 美元左右？</p><p></p><p>对此有网友评价道，“随着英伟达对 AI 工作负载加速硬件开发的不断深入，我预计未来几年内，这款硬件的成本可能只有几十美元，并且训练时间只需几个小时。”</p><p></p><p>至于具体效果，Karpathy 与19年的GPT-2版本做了对比。同样用的当时博文介绍里的提示词“In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.” 结果新模型的输出结果相当连贯，质量也大致与GPT-2相当。</p><p></p><p>两个模型生成的文字较长，有兴趣的朋友可以点击查看：http://llmc.s3-us-west-2.amazonaws.com/html/gpt2_vs_llmc30kedu.html</p><p></p><p>下面我们来看下Karpathy的复刻过程。</p><p></p><p></p><h3>复现过程</h3><p></p><p></p><p>首先，Karpathy强调，使用llm.c训练GPT-2非常简单，因为它是用C/CUDA编写的，所以全程不涉及minconda、Python、PyTorch等。只需要一台八H100 GPU的设备即可。</p><p></p><p>“总之，不必担心，llm.c在算力要求方面非常灵活，哪怕只有一张GPU，大家也仍然可以训练出自己的GPT-2——只不过需要等待8天，而不是像我这样的1天。而如果您拥有16张GPU（例如使用新的Lambda 1 Click Clusters），还可以开展多节点训练，前后只需要等待12个小时。”Karpathy说道。</p><p></p><p>在节点启动之后，下面来看看GPT-2训练的完整说明，不用担心，Karpathy表示保证一分钟以内开始执行：</p><p></p><p><code lang="text"># install cudnn so we can use FlashAttention and run fast (optional)
# https://developer.nvidia.com/cudnn-downloads
# for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

# "install" cudnn-frontend to ~/
git clone https://github.com/NVIDIA/cudnn-frontend.git

# install MPI (optional, if you intend to use multiple GPUs)
# (you might also have to install NVIDIA NCCL if it doesn't come with your setup)
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

# download and enter llm.c repo
git clone https://github.com/karpathy/llm.c.git
cd llm.c

# download the "starter pack" (~1GB download)
# contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s
./dev/download_starter_pack.sh

# download the training dataset (FineWeb-Edu 100B token) .bin data shards
# note: this is a total of 1001 data shards. If you only want to test things
# out and don't want to do an actual run, feel free to append the number of
# training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)
# the full dataset is ~200GB, we can store it here in dev/data directory.
cd dev/data
./edu_fineweb.sh

# compile (~1 min 1st time for cuDNN mostly, few sec from then on)
cd ../../
make train_gpt2cu USE_CUDNN=1

# and train! (wait 24 hours here)
mpirun -np 8 ./train_gpt2cu \
-i "dev/data/edu_fineweb100B/edu_fineweb_train_*.bin" \
-j "dev/data/edu_fineweb100B/edu_fineweb_val_*.bin" \
-o "log_gpt2_1558M" \
-v 250 -s 300000 -g 384 \
-h 1 \
-b 16 -t 1024 \
-d 1048576 \
-r 0 \
-z 1 \
-c 0.1 \
-k "cosine" \
-l 0.0006 \
-q 0.1 \
-u 700 \
-n 2000 \
-x 32000 \
-ge 1 \
-y 1 \
-e "d48"</code></p><p></p><p>稍后会对参数做具体解释。接下来开始优化：</p><p><code lang="text">num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
...</code></p><p></p><p>可以看到，每个步骤大约需要2.75秒，而其中总共涉及3.2万个步骤，所以现在需要等待约24个小时。</p><p></p><p>在每一步中，训练作业的运行都会占用约100万个FineWeb-EDU token（数据内容来自互联网上的教育网页），并对模型的15.58亿个权重进行更新，使其能够更好地预测序列中将要出现的下一个token，到最后将总计处理3.2万 x 1048576 = 33.6 B个token。随着预测下一token的能力越来越强，loss也会随之下降。</p><p></p><p>接下来的工作是归一化（将数值范围控制在0.1至1之间），学习率也在前几个步骤中逐渐升温。从结果来看，这套模型的flops利用率（MFU）约为50%，可说是相当高效了。</p><p></p><p>现在唯一要做的，就是等待24小时让其完成，之后可以使用dev/vislog.ipynb jupyter notebook对main.log日志文件进行可视化。为此，大家需要安装Mython和matplotlib。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e045d301d587cd855b073928d7b03d6.png" /></p><p></p><p></p><h4>评估</h4><p></p><p></p><p>如左图，正在跟踪FineWeb-EDU验证数据的loss。如果大家只运行OpenAI发布的GPT-2并在此基础上评估其loss，得到的就是红色水平线（loss 2.83）。而Karpathy模型的运行结果快速将其超越，步长约为5000。</p><p></p><p>当然，这样的比较并不公平，毕竟GPT-2是在从未发布的WebText数据集上训练而成，因此素材内容可能存在很大的分布差异。比方说，如果在LR 1e-4下对OpenAI模型进行1000步微调，loss就会迅速下降至划线（loss 2.61），代表其正在快速适应新的统计数据。</p><p></p><p>但就个人而言，Karpathy 认为loss验证更多只是一种健全性检查，要实际比较模型性能，还是得借助更靠谱的第三方评估。</p><p></p><p>这时候就要请出HellaSwag评估了，这也是目前市面上表现良好、流畅、普及度高、常被引用且能够提供早期信号的评估方案之一。其中提供的都是简单的常识性场景，大模型必须据此做出正确的内容延展。</p><p></p><p>Karpathy 在右侧窗格中评估了HellaSwag，并发现在约25K步左右与GPT-2模型的性能发生交叉（早于GPT-2，据估计GPT-2的训练数据集共有约1000亿个token。但这可能与数据质量的提高有关，之前Karpathy在124M训练期间也观察到了类似的现象）。绿线为同等参数规模的GPT-3模型，其模型架构与GPT-2几乎相同、仅存在细微差别（上下文长度从1024增长至2048），同时是针对3000亿token进行了训练（相当于我们此次实验训练token量的10倍左右）。</p><p></p><p>必须承认，HellaSwag也不能算是完美的单点比较选项，毕竟它测试的主要是简单的英语和常识，并不涉及多语言、数学或者代码内容。也许是因为WebText数据集在这几个方面拥有更高的比重，所以才把模型规模推到了这样的水平，但Karpathy团队并不确定，毕竟OpenAI从未对此做出过解释。</p><p></p><p>Karpathy指出，一般来讲，在GPT-2等低能力模型上很难获得良好的评估结果，毕竟这类模型无法理解多项选择题；而且其样本质量不够高，无法正常完成涉及标准数学或者代码的评估测试。</p><p></p><p></p><h4>参数指南</h4><p></p><p></p><p>现在让我们仔细看看我们在训练中传递的参数。OpenAI发布的GPT-2虽然包含模型权重，但相关细节却很少；GPT-3版本并未开放权重，但相关细节较多。因此在多数情况下，我们只能参考GPT-3论文中提及的超参数，毕竟GPT-2论文几乎没有提到这方面信息：</p><p>&nbsp;</p><p>-i -j&nbsp;用于训练和验证分割标记文件，需要提前使用&nbsp;edu_fineweb.sh进行下载。-o&nbsp;是写入日志和检查点的输出目录。-v 250&nbsp;要求每250步执行评估并记录验证loss。-s 300000&nbsp;要求每30万步采样部分token。因为总步数不足30万，所以这其实是一种关闭采样的灵活方式，实际只会在最后采样一次。-g 384&nbsp;将最后需要采样的token数设置为384。-h 1&nbsp;要求评估HellaSwag准确性。-b 16&nbsp;将微批次大小设置为16。如果内存不足，请降低此值，例如依次尝试8、4、2、1。-t 1024将最大序列长度设置为1024，与原版GPT-2保持一致。-d 1048576&nbsp;要求总批次大小为2的20次方，与GPT-3论文中的超参数设置相同。代码将确保满足所需的总批次大小，并计算优化所需的梯度累积“内循环”步骤。例如，之前提到Karpathy拥有8张GPU，每张GPU执行16 x 1024个token，因此每个微步（即一次向前向后）对应8 x 16 x 1024 = 131072个otken，因此代码计算梯度累积步数应该为8以满足每步所需的1M批次大小。即每向前+向后8次，而后进行一次更新。-r 0&nbsp;将重新计算设置为0。重新计算是一种在计算与内存之间求取平衡的方法。如果设为-r 1，则代表在反向过程中重新计算前向传递的一部分（GeLU）。就是说Karpathy不必须通过对其缓存来节约内存，但需要付出更高的算力成本。因此如果内存不足，请尝试设置-r 1或者-r 2（同时重新计算layernorms）。-z 1&nbsp;在多个GPU上启用ZeRO-1（即优化器状态分片）。如果使用多于1张GPU进行训练，则应当选择这样的设置，且基本上应始终将其保持为开启状态。但在单GPU上，此设置没有实际效果。-c 0.1&nbsp;将权重衰减设置为0.1。只有（2D）权重的衰减与GPT-2完全相同，具体数值来自GPT-3论文。-k "cosine"&nbsp;设置余弦学习率计划，这里姑且直接使用默认值。-l 0.0006&nbsp;将最大学习率设置为6e-4。根据GPT-3论文的解释，Karpathy这个大小的模型应当使用2e-4，但这里Karpathy将其增加至三倍，似乎训练速度更快且没有引发任何问题。这项参与未经认真调整。-q 0.1代表在训练过程中，将学习率衰减至最大LR的10%，取值参考自GPT-3论文。-u 700&nbsp;表示将在前700次迭代中将学习率从0提升至最大，总批次大小为0.5M时对应3.5亿个token，取值同样来自GPT-3论文。-n 2000&nbsp;要求每2000步保存一次模型检查点。-x 32000&nbsp;要求总共32K步。之所以选择这个数字是因为其好读好记，而且正好对应24个小时。-ge 1&nbsp;为CublasLt设置最近合并的gelu重新计算设置（可选）。-y 1用于将“恢复”标记设置为开启。如果训练因任何原因而崩溃或者挂起，则可按下CTRL+C并重新运行此命令，其将尝试恢复优化。Llm.c具备按bit确定性，因此大家将获得与崩溃之前完全相同的结果。-e "d48"&nbsp;要求从头开始初始化深度为48的GPT-2模型。</p><p></p><h4>内存指南</h4><p></p><p></p><p>大多数朋友面临的最大限制，可能就是自己的GPU内存达不到80 GB。Karpathy表示，“没关系，只要有耐心，之前提到的这些任务也都能顺利运行完成，只是速度会稍慢一些。”</p><p></p><p>但如果模型实在太大，又该如何处理？Karpathy表示，最重要的是调整微批次大小-b，尝试将其缩小并保持在合适的水平。例如16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1。以此为基础，尝试使用重新计算设置-r，即0（最快，但占用的内存最大）、1（速度慢得多，但可以节约大量内存）或者2（稍慢一些，但内存节约量较少）。</p><p></p><p>下一步优化思路则是禁用fp32中的主权重，这里可怜请用 -w 0（默认值为1）来执行此操作。Karpathy并没有为参数维护fp32副本，因为根据经验，之前的几次运行都没有问题，可能是因为使用了随机舍入。</p><p></p><p>“但如果大家在亲自尝试时遇到了问题（其实可能性极低），也可以使用-t减少最大序列长度，将默认值从1024下调至512、256等。但这意味着缩小了其最大注意力范围，所以模型的性能也会变得更差。 ”Karpathy建议道。</p><p></p><p></p><h4>代码</h4><p></p><p></p><p>“虽然我可能有点倾向性，但llm.c真的非常优雅”Karpathy介绍道：</p><p></p><p></p><p>它只需要基本CUDA依赖即可运行。它是C/CUDA中最直接、最小且可读的实现。llm.c总计约有5000行C/CUDA代码。Karpathy 主要尝试使用C，而非C++，以保持代码简洁。神经网络训练只是对单个浮点数组执行相同的简单算术运算（就是加减乘除）的一个while循环，实在没必要搞得太过复杂。它的编译和运行速度极快（几秒钟内），因此可以执行更多步骤并减少等待时间。它会在开始时一次性分配所有GPU内存，并在之后的训练期间将内存占用量保持恒定。因此只要执行步骤启动，我们就能保证接下来的运行状态始终良好、不会发生OOM。具备按bit确定性。 运行效率很高，MFU略低于约50%。主要入口点和大部分代码位于文件tarin_gpt2.cu当中。该文件包含GPT-2模型定义和约2000 LOC中的训练循环，并从llmc目录处导入了一大堆带有各种实用程序和各层实现的辅助文件。cloc llmc报告了23个文件，3170 LOC，而cloc train_gpt2.cu目前为1353 LOC。</p><p></p><p></p><h4>多节点训练</h4><p></p><p></p><p>如果您是位手握大量GPU的“土豪”，llm.c也支持多节点训练。Karpathy表示，其见过的llm.c训练最多支持约500张GPU。</p><p></p><p>“个人迄今为止进行过最大的一次运行，是依靠Lambda全新一键集群功能上实现的，当时是在2个节点上使用了16张H100 GPU。Lambda团队提供了关于如何在其一键集群上训练llm.c模型的详细说明。例如在使用512-GPU H100集群时，每小时费用为2300美元，这时候整个GPT-2训练周期就仅为30分钟。当然，这时您需要增加总批次大小（例如增加到约8M）并稍微调整一下超参数。我还没有尝试过，但相信会有效而且相当爽快！ ”Karpathy说道。</p><p></p><p></p><h4>PyTorch比较</h4><p></p><p></p><p>使用Karpathy的并行PyTorch实现，与PyTorch的运行效果对比应该类似于以下形式：</p><p></p><p><code lang="text">torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin "dev/data/edu_fineweb100B/edu_fineweb_train_*.bin" \
    --input_val_bin "dev/data/edu_fineweb100B/edu_fineweb_val_*.bin" \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1</code></p><p></p><p>这里的PyTorch代码仅供参考，而非实际实现，因为其中的训练循环在某些位置可能略有不同（例如，数据加载器不会对分片进行置换等），总之大家看看就好。Karpathy还将默认词汇大小修改为50257 -&gt; 50304 以提高效率。经过一夜运行，PyTorch给出以下结果：</p><p></p><p><code lang="text">step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
...</code></p><p></p><p>Karpathy 强调，这份PyTorch脚本可能还有很大的优化空间，但至少可以当作观察基准。PyTorch占用的内存量似乎更大（此次运行约为80 GB），而llm.c仅占用了57 GB（节约比例为29%）。内存资源非常重要，因为它能帮助我们容纳更大的训练批次（例如，llm.c在这里可以将微批次提升至24个），从而加快训练速度。</p><p></p><p>其次，我们看到每次迭代大约为3386毫秒，而llm.c的迭代为2750毫秒，速度要快约19%。</p><p></p><p>另外还有一些已知优势，例如llm.c包含启动反向传递的融合分类器等优化选项，据Karpathy所说，目前的torch.compile还做不到。但Karpathy表示，这样的性能差异可能是因为他的脚本没有充分调优，所以比较结果仅供大家看看、试试和作为调试思路的启发。</p><p></p><p>“我想表达的是，llm.c的优化程度和速度水平已经相当不错，当然只是在GPT-2/3训练的特定场景之下。 ”Karpathy说道。</p><p></p><p></p><h4>最终模型</h4><p></p><p></p><p>感兴趣的朋友可以参考以下几条链接：</p><p></p><p></p><p>main.log<a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/main.log">文件</a>"。model_00032000.bin&nbsp;<a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/model_00032000.bin">llm.c</a>"&nbsp;bin模型文件我已经将模型转换为huggingface transformers GPT-2并上传至这里:&nbsp;karpathy/gpt2_1558M_final2_hf。</p><p></p><p></p><h4>模型导出</h4><p></p><p></p><p>模型导出可以按如下方式进行，例如：</p><p></p><p><code lang="text">python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export</code></p><p></p><p>之后大家可以运行Eleuther评估工具，或者运行huggingface采样管线以获取模型样本：</p><p><code lang="text"># take model for spin
import torch

output = "./gpt2_1558M_final2_hf"

# set pytorch seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

prompt = "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(output)
model = AutoModelForCausalLM.from_pretrained(output, attn_implementation="flash_attention_2", torch_dtype=torch.bfloat16, device_map='cuda')
model.eval()
tokens = tokenizer.encode(prompt, return_tensors="pt")
tokens = tokens.to('cuda')

output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4)
samples = tokenizer.batch_decode(output)
for sample in samples:
    print('-'*30)
    print(sample)</code></p><p></p><p>另外大家也可以查看dev/eval 以获取关于如何运行Eleuther Evaluation Harness、HuggingFace Open LLM Leaderboard的具体说明。</p><p></p><p></p><h5>400B token运行</h5><p></p><p></p><p>Karpathy 还尝试用远超33B token的规模训练了GPT-2。具体来讲，Karpathy将-x更改为400000以训练420B token（规模甚至比300B 的GPT-3还要大）。</p><p></p><p>结果显示，这套模型前半阶段运行得不错，但到大约33万步时开始出问题： 这套模型在HellaSwag上全面碾压了GPT-2及同等体量的GPT-3（最高性能优势可达约61%），但遗憾的是之后新模型开始不稳定并发生崩溃。</p><p></p><p>在此过程中虽然也出现过一些较小的峰值，Karpathy将代码配置为当检测到瞬时不稳定时跳过更新（使用了-sl 5.0 -sg 5.0标记），这有助于缓解并推迟问题的出现。但Karpathy承认，模型在初始化、激活范围和整体模型训练的稳定性方面还不够谨慎，对很多深层次问题也没有涉及。</p><p></p><p>这些问题会令模型逐渐变得不稳定，特别是对于规模较大、训练时间较长的模型更是如此。当然，我的实验仍在进行当中。如果大家对稳定模型训练有任何想法和建议，请在评论区中与我们分享。</p><p></p><p></p><h4>常见问题解答</h4><p></p><p></p><p>Q：我可以从llm.c中的模型里采样吗？</p><p>A：也不是不行，但效率很低而且效果不好。如果大家想要提示模型，推荐使用前文提供的huggingface版本。</p><p></p><p>Q：我能跟它聊天吗？</p><p>A：还不行，目前这个版本只完成了预训练，还没有接受过聊天微调。</p><p></p><p>Q：可以在fp8精度下训练吗？</p><p>A：不行，我们目前主要是在bf16下训练，但早期版本正在尝试当中。</p><p></p><p>Q：我的GPU不是英伟达的，可以运行llm.c吗？</p><p>A：不行，llm.c目前仅支持C/CUDA，但已经提供良好分支。比如@anothonix积极维护的AMD分叉（https://github.com/anthonix/llm.c）就相当不错。 GPT-2(124M)。这里再贴一篇关于在llm.c中训练GPT-2（124M）模型的老帖，其中包含与llm.c运行相关的更多信息。124M属于GPT-2迷你系列中的小体量模型，只有124M个参数，远低于本文讨论的1558M参数。</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>Karpathy 让我们看到了更多可能，但这似乎也难以意味着未来整个训练成本会下降。不久前，AI初创公司Anthropic的首席执行官Dario Amodei 就在采访中表示，目前GPT-4o这样的模型训练成本约为1亿美元，而目前其正在开发的AI大模型训练成本可能高达10亿美元。 他还预计，未来三年内，AI大模型的训练成本将上升至100亿美元甚至1000亿美元。</p><p></p><p>参考链接：</p><p>https://x.com/karpathy/status/1811488645175738409</p><p>https://github.com/karpathy/llm.c/discussions/677</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</id>
            <title>vivo蓝河操作系统首届技术沙龙即将举办，邀您共探Rust与AI新时代</title>
            <link>https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: vivo, 蓝河操作系统, AI 大模型, Rust语言
<br>
<br>
总结: vivo在2023年11月发布了自研操作系统蓝河操作系统（BlueOS），该系统基于Rust语言编写，接入了vivo蓝心大模型，具备先进的AI能力和多模输入子系统，提升了智慧、流畅性和安全性，同时拥有生态互联的连接技术，为用户和开发者提供更智能、安全的体验和开发平台。 </div>
                        <hr>
                    
                    <p></p><p>2023 年 11 月，vivo 在开发者大会上正式发布了自研操作系统——蓝河操作系统（BlueOS）。据称蓝河为业界首款系统框架基于 Rust 语言编写的操作系统，同时底层还接入了 vivo 蓝心大模型，「蓝河操作系统」一经发布便引起了行业的广泛关注。</p><p></p><p>在智慧方面，通过引入先进的 AI 大模型能力，BlueOS 实现了 AI 服务引擎和多模输入子系统，支持基于自然交互方式的应用开发，用户也可以通过语音、手势等多种方式与系统进行交互；在流畅性方面，通过优化算法和高性能系统架构设计，BlueOS 成功实现了资源的合理分配和高效利用，确保了应用运行丝滑流畅；在安全方面，BlueOS 的系统框架由 <a href="https://xie.infoq.cn/article/018986ea780ce3a32225de6d0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Rust 语言</a>"编写，能够从源头避免内存使用不当引起的安全漏洞；在生态互联方面，BlueOS 基于分布式设计理念的连接技术，能够广泛兼容行业标准协议，也让 vivo 在智能家居、智慧出行、智慧办公等场景具备了更大的可能性。</p><p></p><p>从某种意义上来说，蓝河操作系统（BlueOS）的诞生，代表了 vivo 在提升用户体验和安全方面的坚定承诺，不仅为用户带来了更加智能和安全的产品体验，也为开发者提供了一个高效、安全的开发平台，同时也对推动国产软件生态的发展具有积极意义。</p><p></p><p>当前，核心技术自主可控仍然是国产软件行业发展的主旋律，大模型与 AIGC 的场景化探索也越发火热。率先入局的 vivo 在这些方面有哪些新思考、新探索，BlueOS 在技术探索和生态建设上又取得了哪些新进展？</p><p></p><p>7 月 27 日，vivo 将在北京举办蓝河技术沙龙，汇聚行业大咖、资深技术专家，共同分享蓝河操作系统的最新发展动态，探讨操作系统技术的未来趋势等。你将有机会与行业大咖面对面交流，也可以探索应用开发新范式和 AI 大模型技术的前沿应用等。</p><p></p><p>蓝河新航，机遇正当时！马上报名技术沙龙，现场开阔技术视野、拓展合作机会， 与 vivo 一起，开辟下一代操作系统的新航道，共同探索未来的无限可能！</p><p></p><p>招募启动日起至活动前一天，分享预热文章至朋友圈，活动当天到场即可凭借朋友圈公开转发记录，领取蓝河定制雨伞，数量有限，先到先得！</p><p></p><p>现场参会者，更有机会获得 vivo 蓝牙耳机等重磅礼品，不容错过~</p><p></p><p>码上报名👇</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/de9a583d9d1ee251ead9b0cd3269ceab.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Lp1WKzT91CNiKaegxA8F</id>
            <title>“我的代码被微软和 OpenAI 抄了，维权后被他们耗了两年”</title>
            <link>https://www.infoq.cn/article/Lp1WKzT91CNiKaegxA8F</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Lp1WKzT91CNiKaegxA8F</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:43:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GitHub, 微软, OpenAI, Copilot
<br>
<br>
总结: 2022年11月，四名身份不明的原告（“J. Doe”）（随后增加到五名）针对GitHub、微软和OpenAI在美国发起集体诉讼，声称Copilot编码助手在训练期间使用了托管在GitHub上的开源软件，因此会向其他程序员用户建议这些公共项目的片段，且完全不考虑许可证条款的要求，例如提供适当的来源标注，因此上述公司侵犯了原创作者的知识产权。他们之所以感到不满，是因为他们认为Copilot可能会将他们编写的、本应受到版权保护的开源代码中的部分内容交付（更确切地说是复制）给其他程序员使用，且未对其劳动成果给予相应的认可、也没有遵守原始许可证的其他条款要求。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p>2022年11月，四名身份不明的原告（“J. Doe”）（随后增加到五名）针对GitHub、微软和OpenAI在美国发起集体诉讼，声称Copilot编码助手在训练期间使用了托管在GitHub上的开源软件，因此会向其他程序员用户建议这些公共项目的片段，且完全不考虑许可证条款的要求，例如提供适当的来源标注，因此上述公司侵犯了原创作者的知识产权。</p><p>&nbsp;</p><p>他们之所以感到不满，是因为他们认为Copilot可能会将他们编写的、本应受到版权保护的开源代码中的部分内容交付（更确切地说是复制）给其他程序员使用，且未对其劳动成果给予相应的认可、也没有遵守原始许可证的其他条款要求。</p><p>&nbsp;</p><p>此诉讼最初共涉及22项索赔，但随着时间推移，被告企业提出动议要求法庭驳回指控并基本受到法官Jon Tigar的支持，因此索赔数量逐渐减少。</p><p>&nbsp;</p><p>在当地时间7月5日公布的法院庭谕（<a href="https://regmedia.co.uk/2024/07/08/github_copilot_dismiss.pdf">https://regmedia.co.uk/2024/07/08/github_copilot_dismiss.pdf</a>"）中，Tigar 法官对原告提出的另一批索赔作出裁决。总体而言，GitHub、微软及OpenAI获得胜诉。其中三项索赔要求被驳回，只有一项被允许继续讨论。而根据微软及GitHub方面律师的统计，目前总计只余两项指控仍待审理。</p><p>&nbsp;</p><p>这一结果再次引发开发者们的关注和热烈讨论。</p><p>&nbsp;</p><p>“如果微软和 OpenAI 获胜，是否意味着任何人都可以合法获取专有产品的泄露源代码，并对其进行 LLM 培训，然后要求 LLM 给出一个足够不同的版本来避免侵犯版权？”有网友提出质疑后立马有人跟帖称，“我认为这正是将要发生的事情，不仅仅是代码，还有文学和艺术品。有人可能会专门为此设计和训练这样的LLM。"</p><p>&nbsp;</p><p>“我觉得真正重要的是谁有更多的钱投入到法庭上。不知怎的，我觉得如果是‘Adobe 诉称其代码被Copilot使用’，结局不会一样。”还有网友称。</p><p>&nbsp;</p><p></p><h2>22项索赔要求，20个无疾而终</h2><p></p><p>&nbsp;</p><p>为什么说7月5日被驳回的指控相当重要？因为其中一项涉及《数字千年版权法（DMCA）》第1202（b）条的侵权规定，该条款规定不得在未经许可的情况下删除关键的“版权管理”信息。根据许可证中的相关规定，此案情形应明确列出代码由谁编写以及使用条款。</p><p>&nbsp;</p><p>集体诉讼的开发者们认为，Copilot 在为用户提供项目代码片段时删除了这些信息，因此违反了该条的规定。</p><p>&nbsp;</p><p>然而法官Tigar并不同意，理由是Copilot的代码建议与开发者自身编写的版权保护成果并不完全相同，因此上述条款并不适用。</p><p>&nbsp;</p><p>事实上，据称去年GitHub已经调整了这款编程助手，确保生成代码建议时较训练代码做出细微调整，以防止输出内容被指控为直接照搬许可软件片段。</p><p>&nbsp;</p><p>随着法官Tigar驳回该项指控，原告方已无法就DMCA第1202（b）条提出新的索赔理由。</p><p>&nbsp;</p><p>但多位匿名程序员一再坚称，Copilot仍会生成与其亲手编写的代码完全相同的建议，这也成为他们诉讼期间的核心论点，而这恰恰符合DMCA设定的相同性要求。然而，法官Tigar此前裁定原告方实际上无法证明存在这种情况，导致原告只能撤回指控并对内容做出进一步修改。</p><p>&nbsp;</p><p>修改后的指控辩称，如果用户关闭Copilot的反复制安全开关，即可引发非法代码复制。他们还引用一项关于AI生成代码的研究，试图用其中列举的Copilot剽窃源代码证据支持自己的立场。但法官再次表示反对，称并无法认定微软的系统在以具有实际意义的方式剽窃他人成果。</p><p>&nbsp;</p><p>在庭审期间，法官专门引用了研究中的观察结果，即Copilot“在良性情况下很少直接提供其记住的代码，大多数原样照搬只发生在用户提示模型使用与训练数据非常相似的长代码片段的情况下。”</p><p>&nbsp;</p><p>法官总结道，“也就是说原告的主张依赖于一项研究，而该研究仅认为Copilot理论上可能根据用户提示词生成与他人所编写代码相匹配的代码，这缺乏充分的说服力。”</p><p>&nbsp;</p><p>DMCA指控只是刚刚被驳回的三项指控中的一条，另外两条分别是不当得利与惩罚性赔偿，公平来看，这两项指控恐怕也须经历修改及重新提出。</p><p>&nbsp;</p><p>除此之外，最初提出的申诉主张就只剩下两项：其一是违反开源许可，其二则是违反合同，而后者也曾经历过被驳回后重新提出。</p><p>&nbsp;</p><p>作为回应，GitHub在一份声明中表示，“我们坚信AI将改变整个世界的软件开发方式，从而提高生产力水平。更重要的是，能够改善开发者的工作感受。”“我们相信Copilot遵循相关法律，并且从起步阶段就一直致力于负责任地通过Copilot实现创新。我们将继续投资并倡导未来由AI驱动的开发者体验。”</p><p>&nbsp;</p><p></p><h2>争论不休、互相指责</h2><p></p><p>&nbsp;</p><p>从上周五针对该案提交的一份联合案件管理声明（<a href="https://regmedia.co.uk/2024/07/08/github_copilot_joint_management.pdf">https://regmedia.co.uk/2024/07/08/github_copilot_joint_management.pdf</a>"）来看，双方在调查过程中频繁向对方提出种种不满与申诉，而且双方均表示对方没有提供其应当提供的全部文件。</p><p>&nbsp;</p><p>原告的开发者们指责三家公司故意拖延时间，称目前才公开的文件早就应该提交并对外披露。而此前大部分焦点都集中在微软当时提交的唯一一份文件上，原告称这明显是在混淆视听。</p><p>&nbsp;</p><p>不愿具名的程序员们强调，“有微软员工参与了多轮由GitHub发起的讨论，表明微软只提交一份文件完全是为了拖延时间和混淆观点，再无其他实际意义。另外，微软明确知晓但并未披露其员工也直接参与了Copilot及其底层模型的创建、运营和管理。”</p><p>&nbsp;</p><p>作为Windows系统的缔造者，微软辩称文件提交不及时主要是由于收集Slack消息时遇到了“技术困难”，但原告对此并不信服。开发者们同时认为，OpenAI也应该提交更多信息，并表示谷歌图书侵权案中的被告就提交了数万份信息。</p><p>&nbsp;</p><p>而另一方面，微软和GitHub反驳称原告方要求提交的信息太多，指责他们“未能有效且真诚地关注与指控切实相关的证据。”其中包括微软2018年对GitHub的收购。</p><p>&nbsp;</p><p>与此同时，OpenAI则表示原告在索要电子邮件方面没有遵循正确的程序，因此在收到正确申请之前，其无法（或者说不会）提供任何电子邮件。</p><p>&nbsp;</p><p>三家公司还指出，DMCA版权索赔被驳回已经从根本上改变了案件性质，并认为现在应该缩小调查范围。原告方对此提出异议，理由是开源许可证违规指控所涉及的文件，与DMCA所对应的文件几乎相同。</p><p>&nbsp;</p><p>GitHub、微软及OpenAI均认为，原告没有正确回应他们的调查请求，称原告方的文件包含“JSON文件、空白HTML文件、没有任何元数据的电子邮件，以及在Slack上发送的经过不当编辑的PNG文件等信息形式”。</p><p>&nbsp;</p><p>目前原告方要求延长调查时间。尽管三家科技巨头辩称没有必要，但表示愿意接受“合理的延期”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>该事件继续引发了大家的思考：</p><p>&nbsp;</p><p></p><blockquote>“如果我是作为一个人，做了下面的事情：1. 仔细阅读并记忆一些受版权保护的代码；2. 编写与原代码在文本上完全相同的新代码。但在编写过程中，我会随机机械地调整一些标识符或其他东西，以编写具有完全相同语义但字符不相同的代码。3. 声称是新的原创代码，但没有原始版权。我认为从法律角度来说我会受到严厉惩罚。这在我看来完全就是故意侵犯版权并故意掩盖侵权行为。当机器做同样的事情时有什么不同？”</blockquote><p></p><p>&nbsp;</p><p>有网友对此表示，“关键是，当涉及到公司利益时，法律体系具有高度的选择性。”他以美国作家协会诉谷歌案为例称，谷歌的辩护理由就是合理使用。</p><p>&nbsp;</p><p>还有网友表示，机器本身什么也做不了。用户和机器共同构成了一个更大的系统，而有了自动完成功能后，用户才是主导。继而，该网友表示，“我怀疑很多版权侵权都是通过剪切粘贴和截屏功能实现的，也许我们也需要小心使用自动完成功能，避免这种情况是用户的责任。”</p><p>&nbsp;</p><p>该诉讼案引发的担忧并不是最近才有的，但是从该诉讼开始算起已经接近两年的时间，各方都没有给出答案，而且短时间内该诉讼可能也不会完结，意味着这个问题未来一段时间内仍不会有结果。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://news.ycombinator.com/item?id=40919253">https://news.ycombinator.com/item?id=40919253</a>"</p><p><a href="https://www.theregister.com/2024/07/08/github_copilot_dmca/">https://www.theregister.com/2024/07/08/github_copilot_dmca/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IzCokfzuYHjuyU4DSqtz</id>
            <title>软件工程的兴衰轮回：2 年巨变，裁员风暴下小团队逆袭，老技术反迎第二春？</title>
            <link>https://www.infoq.cn/article/IzCokfzuYHjuyU4DSqtz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IzCokfzuYHjuyU4DSqtz</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:27:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 职业安全, 科技行业变革, 创业速度, 裁员潮
<br>
<br>
总结: 在当前科技行业变革的环境下，追求职业安全比追求职位安全更为重要。小团队能够更快地行动并构建解决方案，创业速度加快。然而，2022年开始出现意料之外的裁员潮，大型科技企业纷纷裁员，即使盈利能力强也不例外。投资市场在2021年爆发，但2022年开始出现IPO寒冬。整个科技行业正在经历着巨大的变化和挑战。 </div>
                        <hr>
                    
                    <p></p><blockquote>“在当前环境下，我认为我们都应追求职业安全而非职位安全。”</blockquote><p></p><p></p><p>过去18个月来，整个科技行业迎来一系列重大变化：从招聘火热到大规模裁员，从密集上市到个位数IPO，就业市场、风险投资、IPO&nbsp;和大型科技公司都受到变革之风的严重影响。</p><p></p><p>我们正在看到，小团队能够比以往更快地行动，并更迅速地构建解决方案。以Twitter的竞争对手Blue&nbsp;Sky为例，该公司在2022年由一位CEO和两名开发人员创立。仅六个月后，其中一位开发者就独立完成了iOS和Android版本的移动应用的软发布。一年后，他们拥有了百万用户。就在上个月，他们已经达到了550万用户，团队成员也仅增加到了12位开发者。</p><p></p><p>2010年出现的Instagram，也是在第一年内就吸引了百万用户，随后两年连续增长到500万、1000万和3000万用户，且只有13名开发人员来完成这一切。比较两者，开发者人数与时间线惊人地相似。尽管Instagram的用户量更大，但两家公司都在相同的时间框架内构建了iOS和Android应用、网站以及能够处理数百万用户的后端服务。</p><p></p><p>有趣的是，在2022年构建原生应用应该比2010年容易得多，但我们似乎正在经历一种似曾相识的感觉。那么这一切的根本原因是什么？对于企业和开发团队意味着什么？未来的实用派软件工程方法又将向何处去？随之“遭殃”的软件工程师们能够如何转变自身处境？</p><p></p><h2>两年时间，求职、投资市场“高开低走”</h2><p></p><p>下面是我们观察到的科技行业过去两年间的变化：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ef6eae4a9599fefed42b8fdff8f7205.jpeg" /></p><p></p><h3>从激烈争夺人才，到创史上裁员之“最”</h3><p></p><p>2021年底的科技招聘市场呈现出前所未有的热度，求职者数量大大低于以往水平，物色人物变得愈发困难，候选者的薪酬预期也超过了企业预期。有些人当面表示接受，但转过头就跑去追求更高的报价。</p><p></p><p>招聘经理们纷纷表示：“之前从未出现过如此严峻的状况，而且在全球所有地区均是如此。几年前印度曾经出现过技术人才供不应求的火爆状态，而如今的环境相较当时又放大了许多倍。美国、英国、欧盟、东欧、南美……几乎到处都出现了同样激烈的人才争夺，预计这种情况将持续到今年年底。”</p><p></p><p>从当时的情况来看，整个就业市场已经高度趋近于求职者梦想中的“完美形态”。</p><p></p><p>时间快进到六个月后的2022年2月，《纽约时报》发表的一篇文章也得出类似结论，即科技企业正在面临招聘危机。可就在《纽约时报》讨论这一现象的同时，就业市场的风向已经开始了新一轮变化……</p><p></p><p>2022年4月和5月，意料之外的裁员潮初现端倪：一键结账初创公司Fast在筹集到1亿美元的一年之后，突然宣告破产。Klarna意外迎来大规模裁员，比例达到10%。其他几家公司的裁员计划紧随其后，包括Gorllas、Getir、Zapp（即时配送）、PayPal、SumUp、Kontist、Nuri（金融科技）、Lacework（网络安全）等。2022年秋季，大规模裁员的势头仍在继续。Lyft、Stripe、CloudKitchens、Delivery&nbsp;Hero、OpenDoor、Chime、MessageBird等公司纷纷裁员10%甚至更多。</p><p></p><p>许多选择裁决的企业都有一个共同点：公司正面临亏损，所以决定削减人力似乎也在情理之中。但之后，具备盈利能力的公司也开始裁员。</p><p></p><p>2022年11月，Meta解雇了1.1万人（占员工总数的13%），这也是这家社交媒体巨头有史以来首次裁员。几个月后，谷歌、微软和亚马逊也纷纷效仿，裁员数量创下历史新高。2022年末至2023年初的科技行业裁员，其规模堪称多年来之最。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/616ae242908519a5f9f9b3c3bf2b11af.jpeg" /></p><p></p><p>科技巨头们从2023年初开始推进大规模裁员，理由是在2020至2022年的疫情期间过度招聘。然而到今年年初，大型科技企业的裁员脚步仍未停歇，这时候过度招聘的影响早已消失、反倒是创纪录的利润实打实呈现在财报之上。</p><p></p><p>谷歌就是其中的典型案例：公司成立于1998年，此前仅在2008年进行过一次大规模裁员，而且实际比例也仅为2%（共300人）。但到2023年1月，谷歌一口气裁撤了约6%的雇员。2024年1月，在创纪录的收入与利润之下，这家搜索巨头再次动手裁员。</p><p></p><p>谷歌的行为也相当具有代表性。我们可以解读为——无论收入和利润达到怎样的创纪录高位，大型科技企业似乎都习惯了定期削减人力。</p><p></p><p>我当时分析了这些裁员背后的理由，认为：&nbsp;“Meta、谷歌和亚马逊的裁员绝非偶然之举，他们似乎是在以战略方式削减各成本中心以及存在严重亏损的部门。此外，他们很可能也是在清退那些绩效不佳的员工。”</p><p></p><h3>一年投资爆发期后，IPO&nbsp;寒冬到来</h3><p></p><p>2020年之前，风险投资对于初创公司的关注一直稳步提升。而到2021年，投资速度呈现出爆发式增长，总额几乎翻了一番：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a9de5bf21812459eff8f1ca4c06c176.jpeg" /></p><p></p><p>按年度计算的风险投资变化情况</p><p></p><p>并且，&nbsp;2021年是公开募股颇为突出的一年，期间大量科技公司成功登陆股票市场。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3aefab6fa15b57bbd070e0d791560435.png" /></p><p></p><p>2021年出现的IPO数量激增</p><p></p><p>为了让大家对这一年间IPO数量之夸张产生印象，以下列出这段时间值得关注的上市企业：GitLab（版本控制）、Rivian（电动汽车）、Couchbase（NoSQL数据库）、Affirm（先买后付）、Bumble（约会）、Duolingo（语言学习）、Robinhood（交易）、Expensify（记账）、Nubank（金融科技）、Roblox（游戏）、Coinbase（加密货币）、Squarespace（域名）、Coupang（电子商务）、DigitalOcean（托管）、Toast（餐厅科技）、Coursera（教育科技）、Udemy（教育科技）、Amplitude（分析）、AppLovin（移动分析）、UiPath（自动化）、Monday.com（项目管理）、Confluent（数据流）、滴滴出行（拼车服务）、Outbrain&nbsp;（广告）、Nerdwallet（个人理财）。</p><p></p><p>但短暂的辉煌之后，风险投资开始稳步下降。到今年第一季度，已经回落到与2018年相同的水平！</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8aedb3951541f3dbf42874b45b99cf28.jpeg" /></p><p></p><p>2019年至2024年各季度风险投资情况</p><p></p><p>2021年之后，IPO&nbsp;也迅速进入寒冬。2022年科技IPO数量迅速归零，而2023年也只有三家（ARM、Instacart以及Klaviyo）。当时的我们显然无法想象，HashiCorp在2021年12月的上市成为未来18个月间最后一例IPO。</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/5831090fb6e3bef5d1e71708f847fc73.jpeg" /></p><p></p><h2>所有变化的根源</h2><p></p><p>2022年到2023年间一系列重大变化确实汹涌而来，但原因究竟是什么？最明显的潜在因素，就是2020年至2021年的疫情爆发与封控结束，推动整个世界慢慢恢复原本的秩序。</p><p></p><p>当时，企业创始人和CTO们曾经就公司为何裁员、业务为何突然面临增长危机做出过解释。他们大多复返提及“宏观经济环境”字眼，而且在之后的公司裁员公告中也会强调这方面因素。很明显，利率变化带来的冲击比普遍预计更大。</p><p></p><p>2022年中期，美联储（FED）开美国几十年来的先河，决定大幅拉升储蓄利率：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a70cc221de651028789906588897a0a4.png" /></p><p></p><p>为什么提高利率就能缓解通货膨胀率？下面来看BBC的解释：</p><p></p><p></p><blockquote>英格兰银行通过调整利率来控制英国的通货膨胀问题——即商品价格随时间推移的上涨趋势。当通货膨胀率高企时，英格兰银行（其目标是将通货膨胀率保持在2%）可能会决定提高利率。这样做的目的在于鼓励人们减少支出，进而通过降低需求来帮助降低通货膨胀率。而随着通货膨胀的缓解，英格兰银行可能会继续维持当前利率或者重新降低利率。英格兰银行必须在减缓物价上涨的需要与损害经济的风险之间，求取微妙的平衡。</blockquote><p></p><p></p><p>如果将其中的“英格兰银行”替换成美联储或者欧洲央行，意思也是一样。总而言之，提高利率是应对通货膨胀的一种行之有效的方法。</p><p></p><p>在美国，不到一年时间里利率就从接近0%跃升到了5%左右。这无论是放到这十五年或几十年来看，超低利率都是少数。实际上，自1955年以来，总共有11.5年处于“接近零”的超低利率时期，其中有11年发生在2009年之后。也正因为如此，这段时间才被称为零利率时代（ZIRP）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea9eb2b2320b712814d2f49c8a9e6cce.jpeg" /></p><p></p><h3>利率对科技初创企业的影响</h3><p></p><p>很多朋友可能觉得，科技跟金融或者利率有什么关系？“事实恰恰相反”，这不是我的个人意见，而是彭博分析师兼专栏作家Mark&nbsp;Levine的原话。</p><p></p><p></p><blockquote>“初创公司的存在本身，就是一种低利率现象。当全球各地的利率都很低时，那么20年后的1美元跟当下的1美元基本等价，因此初创公司的商业模式就可以是‘忍受在十年之内开发AI所带来的亏损，指望在遥远的未来赚大钱’，这也确实是可以接受的。但在利率较高时，今天的1美元比未来的1美元更值钱，所以投资者们更想要保住当下看得见的真金白银……”如果2021年时，有一位魅力十足的科技创始人找上银行，表示想要通过AI、自动驾驶出租车、飞行出租车、太空出租车或者区块链之类的东西彻底改变世界，其实贷款经理很少会拒绝。毕竟当时大家不会考虑到美联储在短时间内就会提高利率，而且从业者普遍认为对于这些可能给人类未来带来颠覆性变化的事业，拿它跟利率波动对赌实在有点俗了。但事实证明，这一切的本质就是在跟利率波动对赌。</blockquote><p></p><p></p><p>在读到这段分析时，我也本能性地想要反驳。毕竟利率和科技初创公司之间就算有关联、有强关联，也不至于是这么赤裸裸的因果关系吧？但越往深处想，我就越觉得Levine的观点很有道理。所以我们不妨分析一下，当利率迅速从0%上涨到5%时，究竟会发生什么？</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3df6392dd6de2b7acba00ccf0bfe4982.jpeg" /></p><p></p><p>先来看看利率会对风险投资、IPO、大型科技企业和就业市场造成的实际影响：</p><p></p><p>风险投资减少：风险投资是一种高风险/高回报的投资类型，其理念就是把一大笔钱（例如1亿美元）投入风险投资基金，等待10年左右再获取相当可观的回报。这1亿投资可能会变成1.5亿美元甚至2亿美元。另外一种选择则是把钱存进银行，但这会侵蚀其价值，因为每年的通货膨胀率（例如2%）会逐年消弱美元的购买力。可是在利率为5%的情况下，我们几乎能够毫无风险地在十年间把1亿美元变成1.5亿美元，那为什么还要投资那些风险巨大的科技初创公司（成功的不少，但失败的更多）并甘冒十年之后血本无归的后果？科技公司IPO减少：上市的科技企业往往处于亏损状态，因为其长期处于业务增长阶段。2021年上市的大多数科技企业都属于这种情况。在高利率环境下，投资这些公司的吸引力变得更弱，毕竟除非已经拥有明确的盈利途径，否则他们可能会迅速耗尽资金，进而导致投资贬值。RIvian的市值就已经从2021年的1500亿美元降低至2024年的100亿美元（部分原因就来自投资者对其资金耗尽风险的担忧），这明显是个值得警惕的案例。相比之下，投资者只需将钱存进银行，就能获得诱人的回报。大型科技企业努力抬升利润率：在零利率政策期间，“基准”回报率为0%。而随着这条基线上涨到5%，具备盈利能力的公司需要进一步拉高利润率才能维持其估值。也就是说大型科技企业会更积极的削减成本，以确保自己的利润率更加健康，哪怕他们的利润已经相当可观。就业市场恶化：随着大型科技企业裁员，加上初创公司能够吸纳的人力越来越少（因为注入的风险投资资金缩水），市场能够提供的工作岗位也愈发稀缺。</p><p></p><p>让我们将以上结论跟过去两年的变化进行比较。下图所示，为利率提升后逻辑推断得出的影响，再跟我们截至目前观察到的实际情况进行比较：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9dd00cfeee2025ddb9d937fb0b7af207.jpeg" /></p><p></p><p>很明显，二者几乎完全相同！这可能出乎大家的预料，但利率上升确实能够解释科技市场上出现的诸多趋势。</p><p></p><h2>两波技术革命的加成，生成式&nbsp;AI&nbsp;紧随而来</h2><p></p><p>2009年左右开始的利率下调，促使更多风险资本流入初创企业。iPhone于2007年横空出世，两年之后又出现了现已停产的Windows&nbsp;Phone。智能手机改变了消费技术，也成为移动优先型科技企业及产品的催化剂，包括2006年成立的Spotify、2009年出现的WhatsApp、2010年的Instagram、2010年的Uber以及2011年的Snap等数千家企业和产品。</p><p></p><p>就在智能手机出现的同一时期，第一批云服务商也迈出了发展的第一步：</p><p></p><p>2006年:&nbsp;亚马逊云科技2008年:&nbsp;Azure2008年:&nbsp;Google&nbsp;Cloud</p><p></p><p>云服务商能够帮助初创公司以更快的速度、更低的成本完成产品开发。这些客户无需自行购买及运营本地服务器，而只需轻松租用云端虚拟服务器。如果需要更多容量，直接付费即可享受资源扩展。早期亚马逊员工Joshua&nbsp;Burgin（现担任VMWware公司工程副总裁）在《现代后端实践的过去和未来（The&nbsp;past&nbsp;and&nbsp;future&nbsp;of&nbsp;modern&nbsp;backend&nbsp;practices）》一书中对此做出了描述：</p><p></p><p></p><blockquote>这波云转型使得Netflix、Lyft以及Airbnb等早期亚马逊云科技客户得以获取与老牌科技巨头相同的计算容量，为这些尚处于起步阶段的初创公司带来了巨大推力。无需采购订单、长达数月的交付周期以及庞大的IT部门或者主机托管服务商，后发团队们只要输入信用卡号就能立即开始使用！</blockquote><p></p><p></p><p>如今亚马逊云科技最知名的客户包括Netflix、Airbnb、Stripe以及Twitch。在云计算的加持之下，每一家公司都能发展得更快，同时将前期资金投入控制在极低的水平。</p><p></p><h4>技术革命与利率环境两相作用</h4><p></p><p>智能手机与云计算革命，还几乎与随后十年间长期保持近零利率的金融市场实现了完美契合：智能手机与云计算革命中的多个关键事件，都与利率变化有关。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/51ff3ed92de508afcbf47be8d1b4f821.jpeg" /></p><p></p><p>种种宏观因素，为风险投资公司助力初创企业提供了更为有力的理由：</p><p></p><p>以移动端为优先的全新初创公司类别开始出现，只要他们行动迅速就完全有希望成为价值数十亿美元的大厂。这时候筹集大量资金对其赢得市场自然至关重要，Uber和Spotify就凭借这一策略取得了成功。初创公司可以使用云服务来解决设施扩展问题，而不必耗费数年时间慢慢搭建自己的基础设施，从而有效将投资转化为业务增长。这也是风险投资方帮助初创企业赢取市场份额的一种重要方式。</p><p></p><p>2010年代可以说是科技初创公司的黄金时代，因为这是有史以来最漫长的零利率时期，并在期间激发了两波技术革命。</p><p></p><p>如今，另外一场潜在技术革命正在酝酿：生成式AI与大语言模型（LLM）。AI革命与云计算革命具有相似之处，因为AI同样有助于提高效率（只要AI成本能够在当前的基础上不断下降）。</p><p></p><p>然而，AI革命的性质却与智能手机革命截然不同：因为AI似乎并没能像智能手机那样，为应用程序开发商们提供新的、初期免费且足够广泛的分销渠道。另外，生成式AI革命的起点发生在高利率环境之下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed66693d23287f6d49a06d39664de50d.jpeg" /></p><p></p><p>生成式AI革命（ChatGPT的发布等）发生在高利率环境当中</p><p></p><h2>软件工程师们面临新的现实</h2><p></p><p>那么，我们正面对着怎样的“新的现实”？软件工程实践将迎来怎样的改变？</p><p></p><p>概括来讲，软件工程师们越来越难找到工作，职业发展速度也有所减缓，晋升更难了。</p><p></p><p>从2020年开始到2024年的图表显示，在美国、英国和德国的Hacker&nbsp;News和Indeed上的职位列表，工作发布数量在2021年和2022年达到峰值，现在几乎回到了2019年和2020年的水平。与两年前相比，现在可供申请的职位更少，而且职位的申请者也更多。</p><p></p><p>例如，美国亚利桑那州一家初创公司Supply&nbsp;Pike的的CTO透露，他们的实习申请从去年翻了一番，软件工程申请从去年增加了三倍。来自Facebook、Google等大公司的申请人数也增加了，高级工程师不再四处面试、货比三家，而是接受第一个抛来橄榄枝的职位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57c3ce9204d4e52510a35f22a46a50f4.jpeg" /></p><p></p><p>Dominic&nbsp;Jack&nbsp;Weston&nbsp;是创始人守护者论坛成员，他曾向创始人们提问：“你们公司是否有很多人离职？”50%的创始人表示没有变化，但另外50%表示他们看到离职人数明显减少。由于离职的开发人员减少，因此招聘也相对减少，因为需要填补的职位变少了。</p><p></p><p>我们作为软件工程师需要预期的另一件事是职业增长变慢。这是因为当招聘减少，无论是因为职位填补减少还是公司增长放缓，对于高级职位的需求就减少了。如果你有一个10人的团队，并且资历分布合理，那么就不需要那么多的技术主管或高级工程师。从预算的角度来看，我们直到现在才意识到这一点。</p><p></p><p>一些公司正积极适应晋升机会减少的新局面，Shopify就是一个很好的例子。</p><p></p><p>之前Shopify有设定了C1至C10的职级，其中C6级别代表高级软件工程师，C7可能是资深工程师，员工通常会从C4或C5逐步晋升至C6。</p><p></p><p>现在他们引入了一种新的职业发展机制——从0到50的掌握分数，员工有两个途径来推动自己的职业发展：一是提升自己的掌握分数，如从C7的30分提高至40分，这不仅意味着薪资增长，还伴随着奖金和其他福利；二是转换角色范围，比如从C7的团队领导变为C8的领域领导，尽管这可能导致掌握分数的下降，但公司强调这是一项不同的职位，员工需要根据自己的意愿做出选择。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31bd4a204e1a1ff864f7a3d5e1cefcc0.jpeg" /></p><p></p><p>Shopify的这种策略非常明智，解决了在一些公司中可能四五年都没有晋升机会的问题。在Shopify，员工可以期待每半年就获得掌握分数的提升，以及随之而来的小额奖金和认可，这无疑会提高员工的满意度和忠诚度。我相信这种创新的做法会被更多具有前瞻性的公司采纳，从而为软件工程师带来更多的职业发展机会和工作动力。</p><p></p><p>另外，当处于资金更少、限制更多、更注重效率的时代，我们开始需要选择更“无聊”的技术来完成工作（Choose&nbsp;simple&nbsp;/&nbsp;boring&nbsp;technology）。虽然新技术通常有更好的表现，可以绕过一些局限，并且代表了“未来”，但实际上越来越多的人为了解决业务问题而选择了简单的旧技术。</p><p></p><p>这可能不是我们的偏好，但我们将面临来自非技术人员，特别是商业领导层的更多压力。单体式架构将变得更受欢迎，“全栈”和Typescript的扩张势头更猛，也有更多责任开始“左移”到开发者身上。</p><p></p><p>为了展示与过去几年技术选择有多大差异，让我们看看在零利率时期，技术决策是如何做出的。微服务方面，Monzo可能是继Uber之后规模最大的公司之一，他们有500名工程师管理着数千个微服务，每个工程师负责多个微服务。在2015年的零利率时期，Monzo的创始人兼CEO在公开会议上以幽默的方式概述了他们的成功步骤：第一步采用Go语言，第二步实现微服务架构，第三步和第四步则是未知和盈利。如果这真的是他们的决策逻辑，那确实引人深思。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a1705563b4f5f120b47774bcea610f5.jpeg" /></p><p></p><p>我欣赏他们分享这种策略的勇气，因为不仅仅是Monzo，很多公司的CTO甚至CEO都认同这种思维方式，因为那时即使资金充足，吸引软件工程师加入也是非常困难的，你需要提供一些有吸引力的东西来招募他们，比如酷炫的技术。</p><p></p><p>通过与许多初创公司的对话，我认为单体架构正在回归，再次变得流行。目前的趋势是，从一开始建立单体架构并持续使用它，Shopify一直是这方面的先行者。如果预见到在未来几年内团队规模将迅速扩大，微服务架构可以解决组织扩张带来的问题；但如果没有这种扩张，可能并不需要微服务，单体架构就已足够。</p><p></p><p>同时，全栈开发的浪潮正汹涌而来，我们经常听到全栈的概念，现在在各个地方都看到它的身影。以一个典型的小团队为例，他们负责开发iOS、Android、Web和后端。团队可能包括两名后端工程师、一名Web工程师、一名iOS工程师和一名Android工程师。然而，当你采用React&nbsp;Native、Flutter或其他跨平台技术时，你实际上可以拥有三种类型的全栈工程师，他们可以分别侧重于后端、Web开发以及移动开发。</p><p></p><p>尽管存在一些争议，但你可以使用这些技术生成功能相似的iOS、Android和Web应用程序，同时拥有一个更小、沟通更少团队，从管理者的角度来看，这意味着更低的预算。</p><p></p><p>在这个转变中，TypeScript起到了重要的助力，许多近年来成立的初创公司都谈到了TypeScript如何使全栈开发成为可能。我不想只强调TypeScript，但拥有一种技术，让你的工程师能够在整个技术栈中发挥作用，这确实是非常有价值的。</p><p></p><p>例如，Blue&nbsp;Sky公司几乎在所有层面上都使用TypeScript，包括后端、前端以及移动应用，结合React&nbsp;Native和Expo，使得他们所有的工程师都能够处理任何部分的代码。同样，由我Uber的前同事Thomas&nbsp;Arkman创立的Linear流行管理解决方案公司，从后端到前端也在使用单一语言类型，并表示这极大地服务了他们的技术需求。</p><p></p><h3>不同于互联网泡沫破灭时期</h3><p></p><p>变化往往在我们不经意间突然降临。经历过2001年互联网泡沫破灭的朋友，肯定会感觉如今的科技市场衰退就如同是当年那场噩梦的重演。在那段时期，软件工程师们最关注的就是在岗位缺乏保障、人生即将失控的焦虑之下，该获取哪些职业方向和战术性应对策略。以下是两位亲历者的故事：</p><p></p><p>2001年从美国计算机科学专业毕业的Niia&nbsp;Henry回忆说，“我毕业时就被裁员了，我的大多数朋友的初创公司都失败了，我们都在拼命找工作。我们不得不与同样被裁员的资深工程师竞争岗位，每天都会听到大型初创公司倒闭的消息，这真的很令人沮丧。为了积累简历上的经验，我们接受任何兼职或无薪实习的机会。”之后，她从软件工程转向网站开发的咨询工作，最终成为技术项目经理。在五六年的时间里，她不得不在非技术岗位上工作，因为那是她能找到工作的唯一方式，后来她重新回到了软件工程领域。现在她现在已是Spotify的总监了。</p><p></p><p>另一位在网络泡沫时期毕业的谷歌员工分享他的教训说：“20年后，无论你在哪家公司工作，都要确保自己处在收入一方，确保公司是在销售软件，而不是用软件来销售其他东西。”这是一种说法，如果你在利润中心工作，你的工作在注重效率的时代更安全。</p><p></p><p>Kent&nbsp;Beck是一位在2000年代和现在都工作了20年的资深人士，在被问及他所看到的相似之处时提到了一些有趣的现象：固定规格和前期设计重新流行起来，迭代开发变得不那么频繁；开发者之间有更多的交接，比如将工作交接给QA或其他人；还有更少的频繁部署到生产环境，以及更长的反馈周期。</p><p></p><p>但2024年与2000年的不同之处在于，互联网当时正在迅速增长，今天人工智能和大型语言模型正在迅速增长。到2023年年中，所有风险投资中有25%进入了人工智能。并且，2000年的技术行业要小得多。2003年，七家最大的公开交易技术公司中，Google排在第四位，Microsoft和Apple分别是第六和第七，其他的是非技术公司。而在2024年，前四名都是技术公司，实际上七个中五个是技术公司。技术行业已经变得非常庞大，无处不在，我认为唯一的问题是它会变得有多大，它仍在增长，但某个时候增长会放缓。</p><p></p><h2>给软件工程师的真诚建议</h2><p></p><p>我们已经看到，当公司裁员时，会受到影响的更多是执行者而不是规划者。软件工程经理往往成为主要的裁撤对象，因为团队结构正在被精简。我了解到一些初创企业裁掉了大量中层管理人员或工程总监，但他们没有触及软件工程师，因为这些是实际做事的人。因此，对于工程经理来说，成为实际动手实践的人，而不是被视为成本中心的规划者，变得越来越重要。</p><p></p><p>由于找工作变得更加困难，你需要为此做好更充分的准备。以下是一些建议：人脉网络极其重要，无论你是否已经拥有，如果有要利用它，如果没有就要努力构建它。推荐变得更加关键，因为求职申请众多，有时只有前50份才会被真正查看。在面试中投入无薪时间，如带回家作业，将成为常态，这正在变成一种数字游戏，你需要向很多地方申请。</p><p></p><p>还有一个有极大帮助的事情是培养商业或产品思维，这意味着要理解你所在公司的商业模式——他们是如何盈利的、是否有盈利能力、能否在某个时候盈利。建立与产品经理的关系，他们应该非常擅长理解这些并找出如何转变它；然后与工程领域之外的人员交流，如客户支持和研究部门，提出你可以帮助公司的想法。能够帮助公司赚钱或省钱的软件工程师非常有价值，我认为大多数公司都会寻找这样的人才。</p><p></p><p>最后一点是，在当前环境下，我认为我们都应追求职业安全而非职位安全。引用一位评论者的话：“我们总是在为可能的裁员做准备。被裁员的经历教会了我，没有工作是安全的，无论是作为管理者还是员工，我都不能承诺或期望工作安全。我们能做的，也是我们所有人都能做的，就是拥有职业安全——不断学习，持续在公司从事挑战性项目，持续与优秀的人合作，这是你能做的最多的事情。”</p><p></p><p>AI会不会取代软件工程师，这听起来似乎是一个很极端的问题，但我认为这种情况不太可能发生。实际上，我们正在经历的是，软件工程师的工作范围将变得更加广泛，尽管公司仍将保留安全工程师等一些特定的专业角色，但这些特殊职位的数量将会减少，可能在每50到100名软件工程师中只会有少数几个这样的角色。</p><p></p><p>这意味着，虽然专业角色依然重要，但我们会看到更多的工作被集成到软件工程师的职责之中。</p><p></p><p>但我认为，AI使开发者使用工具更加高效。当我使用像Copilot或ChatGPT这样的工具进行头脑风暴、模式识别或者学习不太熟悉的技术时，真的有很大的不同。我的建议是，让这些工具成为你的盟友，理解它们的工作原理；尝试Copilot、Cody或其他任何你能找到的AI工具，形成自己的看法，改善你的工作流程；亲自看看哪些有用，哪些没用。</p><p></p><h2>结语</h2><p></p><p>有时候，为了更好地理解科技行业的变化，我们不妨退后一步，以总览的姿态尝试理解一切。零利率的终结是个标志性事件，对于科技行业产生了重大冲击。尽管我们很难预测未来，特别是对于瞬息万变的科技行业来说，但我发现从驱动行业发展的背后力量入手反而会得到有益的启发。</p><p></p><p>从某种程度上讲，科技行业的发展史就是一部周期性繁荣与萧条的血泪史。创新是全新商业机遇的沃土，毫无疑问科技行业还会迎来更多繁荣时刻，大家唯一要做的就是在机会来临时把握住它！</p><p></p><p>“用足球来打比方：大多数球员都盯着球当前所在的位置，而我想朝着球飞向的位置提前补位。我觉得自己对科技行业这颗‘球’在未来几年的行进方向有了更多了解，所以能够更好地定位自己。”</p><p></p><p>原文链接：</p><p></p><p><a href="https://newsletter.pragmaticengineer.com/p/what-is-old-is-new-again">https://newsletter.pragmaticengineer.com/p/what-is-old-is-new-again</a>"</p><p></p><p><a href="https://www.youtube.com/watch?v=VpPPHDxR9aM">https://www.youtube.com/watch?v=VpPPHDxR9aM</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ONzGRMNCpkM5mVAYbjQi</id>
            <title>龙盈智达、中国银联、富滇银行、平安产险等确认出席FCon，分享金融数智化运营与营销创新实践</title>
            <link>https://www.infoq.cn/article/ONzGRMNCpkM5mVAYbjQi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ONzGRMNCpkM5mVAYbjQi</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 04:30:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融, 科技, 金融科技, 技术驱动
<br>
<br>
总结: 金融机构实现高质量发展需要结合科技和金融，实现技术驱动业务转型，但仍需克服最后一公里的障碍。2024年FCon全球金融科技大会将分享数字化转型和技术赋能的经验，展示金融数字化在“十四五”期间的关键进展，以及金融场景创新的实践和探索。演讲内容涵盖卡组织数字化营销实践、商业银行运营营销的挑战与路径，以及构建智能营销生态的案例和经验分享。通过分享和交流，帮助金融机构更好地应对数字化转型带来的挑战，实现业务增长和创新。 </div>
                        <hr>
                    
                    <p>从“金融”和“科技”，到“金融科技”，是金融机构实现高质量发展的必答题。但从实践来看，行业离全面释放技术要素，兑现技术驱动业务转型的价值潜力，还差最后一公里。</p><p></p><p>具体如何完成最后这一步跨越？在8月16日-17日即将于上海举办的<a href="https://fcon.infoq.cn/2024/shanghai/">2024年FCon全球金融科技大会</a>"上，龙盈智达（北京）科技有限公司副总裁宫小奕将分享<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6043">股份制银行数字化转型和技术赋能过程中的“道与术”</a>"，结合前沿技术凝结场景驱动的关键经验，分享如何以场景驱动业技融合，让技术能力深刻融入业务逻辑，让业务依托技术变革经营模式。</p><p></p><p>2024年FCon大会由中国信通院铸基计划作为官方合作机构，以“科技驱动，智启未来——激发数字金融内生力”为主题。在“十四五”收官之际，本届大会将致力于展示金融数字化在“十四五”期间的关键进展，帮助金融机构更具针对性地“查缺补漏”。</p><p></p><p>聚焦金融场景创新，大会特别策划了「<a href="https://fcon.infoq.cn/2024/shanghai/track/1689">金融数字化营销实践</a>"」和「<a href="https://fcon.infoq.cn/2024/shanghai/track/1690">金融数字化管理和运营实践</a>"」专题论坛，来自中国银联、富滇银行、平安产险、平安壹钱包、申万宏源证券、度小满等金融机构的专家将在现场分享各自业务领域的探索和实践。</p><p></p><p>上周上新精彩议题如下：</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6039">卡组织数字化营销实践</a>"</h4><p></p><p></p><p>演讲嘉宾：马永松&nbsp;中国银联智能化创新中心高级总监</p><p></p><p>卡组织营销面临业务角色多（银行机构方、商户方、钱包方、行业APP方等）、场景广泛、链条长等特点，如何发挥好卡组织的核心纽带作用，在多边业务模式中兼顾各方的经营特点与利益，并使用数字化手段更好、更快、更优的服务好卡组织体系中的各参与方，是银联目前面临的一项艰巨挑战。银联从整个生命周期管理角度，从事前、事中、事后3个阶段通过营销策划、客群圈选、活动标签、辅助调优、一站式业务指标监控、黄牛侦测与拦截、客诉诊断、事后复盘等数个环节针对营销的运营进行分解、落地。我们期待将一路走来的解决方案与实践经验，与大家进行面对面的分享与交流。</p><p></p><p>演讲提纲：</p><p>卡组织营销面临的挑战卡组织四方模式与营销挑战潜在解决思路银联数字化营销体系介绍对“数字化”的理解生命周期管理体系介绍技术类特点场景实践一站式用户筛选案例联动商户开展精细化营销活动案例联动银行开展精细化营销活动案例未来展望生命周期运营管理流程自动化从数字化到数智化卡组织网络价值的进一步延伸</p><p>听众受益：</p><p>卡组织营销模式的特点与挑战银联在数字化转型过程中面临的痛点和解决方案面向银行和商户的实践经验卡组织营销模式面向未来的演进方向</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6048">数智化时代商业银行运营营销的“坑”与“路”</a>"</h4><p></p><p></p><p>演讲嘉宾：李涛&nbsp;富滇银行数字金融中心副主任</p><p></p><p>在银行数字化转型的浪潮中，构建企业级数字化运营营销体系已成为行业焦点。其不仅需要构建强大的平台能力，更关键的是构建企业级的数字化业务运营营销体系。尽管银行的数字化营销体系借鉴了主流电商的模式，但由于受到监管和行业特性的影响，银行在实际运营中无论是在流程、合规、技术和风险控制等方面都面临很多的挑战。人工智能技术快速发展，大模型成为金融行业创新的重要发展驱动，本议题将介绍中小银行如何将PPT上的大模型应用用于银行实践，拆解其落地路径与大型银行的异同。</p><p></p><p>演讲提纲：</p><p>富滇银行数字化运营营销体系数字化运营营销的“坑”与“路”自我拷问什么是银行数字化营销和运营银行公私域运营模仿互联网电商可持续吗？点、线、平面、立体化的营销策略演进口碑到品牌的演进北极星指标是个坑吗？大而全的指标标签体系真的能赋能银行数字化营销吗？活动为什么从&nbsp;“高频”&nbsp;到&nbsp;“低频”&nbsp;到&nbsp;“固定频率”&nbsp;再回到“高频”？用户触达“陷阱”与出路公域saas运营和私域运营如何有效结合，公域流量向私域AUM转换的出路是什么？舆情事件的“坑”与机遇人工智能在富滇银行运营实践</p><p>听众受益：</p><p>企业级运营、营销系统建设架构如何构建企业级的数字化运营营销体系，覆盖数据采集、分析、决策全流程引入先进AI技术，实现个性化营销与精细化运营银行运营营销模式与演进经验提炼自富滇银行三年多的实践，展现数字化营销与运营模式的迭代升级路径从基础数据整合到智能决策支持，逐步深化数字化能力，提升客户体验与运营效率银行运营营销运营实践中遇到的挑战与对策分享在数字化转型中遭遇的常见障碍，如数据管理难题、技术整合壁垒等提出针对性解决方案，包括强化数据治理、优化技术栈、培养复合型人才等策略中小银行大模型应用实践探讨大模型在中小银行运营营销中的实际应用案例，突出差异化落地路径中小银行如何利用有限资源，有效整合大模型，赋能业务创新与增长</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6041">构建智能营销生态，让增长自然发生</a>"</h4><p></p><p></p><p>演讲嘉宾：王洪志&nbsp;中电信翼金智慧营销研究院院长&nbsp;/某大型国有银行省行首席增长官</p><p></p><p>演讲提纲：</p><p>洞察新名词背后的瓜葛数字经济数字中国5篇大文章新质生产力银行数字化转型银行数字化营销转型实践案例国有大行省分行从倒数第二到正数第二的蜕变！做对了什么？城商行零售板块全线指标超额增长！内在逻辑是什么？解构银行数字化营销转型实践什么是数字化转型，统一思想认识为什么做数字化转型，全员统一行动“遥遥领先”的数字化转型的理论架构“暴力”拆解数字化转型的实践案例银行数字化转型实践痛与悟转型中组织升维转型中文化升维转型中科技升维转型中人才升维</p><p>听众受益：</p><p>深入剖析银行数字化转型带来的冲击，引导把握银行数字化营销转型制胜关键深入剖析数字化转型战略与路径、数字化转型企业架构搭建、数字化运营与流程重构、数字化营销应用架构、数据架构等关键实现路径深入讲解数字化营销运营的场景搭建和运营方式，现场感受数字化营销运营的倍增能力理论联系实战案例，深度拆解银行数字化转型成功链路，真实场景还原：“接触互动-深度交流-项目启动-战略规划-方案执行-数据分析-策略输出-营销运营-过程监控-结果复盘”</p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6045">“治理即应用”：产险数据治理体系探索及实践</a>"</h4><p></p><p></p><p>演讲嘉宾：洪广智&nbsp;平安产险客户大数据团队平台组负责人</p><p></p><p>数据治理在产险公司的数字化转型过程中扮演着至关重要的角色。一方面产险在以“数据标准化”的治理思路，分别在业务、技术和数据三端逐步推进各项工作的落地，夯实数据基础，确保数据准确性和一致性；另一方面秉承“治理即应用”的思路，在“字段”治理的基础上，通过统一ID体系，串联客户全域数据，构建统一的客户画像，解决“客户是谁、做了什么”的问题，并通过“产品化”和算法加持，赋能业务场景，体现数据治理价值。</p><p></p><p>演讲提纲：</p><p>在数字化转型过程中产险数据治理的痛点分析基于业务场景的数据数据治理的实践效果与大模型应用探索数据标准化---开启大数据治理之路大模型应用---探索数据治理新范式从数据治理到赋能业务转变实践探索及业务案例分享构建底座：从“字段”治理延展到围绕“客户”的数据治理，构建统一客户数据视图。案例1：数据产品化--全域客户画像的构建及应用效果案例2：数据应用--基于客户数据的非车策略仿真预测，提升客户转化率</p><p>听众受益：</p><p>从业务应用视角，了解财产保险行业的数据治理模式规划及落地步骤了解数据治理成果如何赋能业务应用案例，提升数据治理的价值</p><p></p><p></p><h4><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6005">基于因果推断的智能经营模型体系</a>"</h4><p></p><p></p><p>演讲嘉宾：李东晨&nbsp;度小满数据智能经营模型负责人</p><p></p><p>在企业决策中，如何在资源有限的情况下提升经营效率，从而达到增长最大或盈利最优的目标，是每个企业持续努力的方向，其中的关键点是如何将资源投入到带来增量更大的地方。传统的机器学习/深度学习适合解决预测问题，而因果推断则直接能够学习到一个经营动作开展后带来的增量，更适合应用于企业决策。相对传统互联网场景，个人信贷存在较强的领域特性，因此在信贷核心的offer优化方向，我们提出了业界领先的因果经营模型体系，并在业务中应用落地，带来了较好的业务收益。</p><p></p><p>演讲提纲：</p><p>经营框架介绍：经营模型框架通常包含哪些模块因果模型实战：如何通过因果推断技术提升经营效率案例分享：因果offer体系在信贷领域落地案例</p><p>听众受益：</p><p>了解信贷领域的经营模型框架体系从预测到决策，因果推断技术能更好地支撑企业决策优化问题从营销到盈利，因果推断可以支撑所有资源有限情况下的最优求解问题</p><p></p><p>截止目前，大会已上线34个演讲议题，更多议题可进入FCon&nbsp;全球金融科技大会官网查看：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"。来自工银科技、北京银行、平安银行、广发银行、中信银行蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。目前大会已进入9折优惠期，单张门票立省&nbsp;480&nbsp;元（原价&nbsp;4800&nbsp;元），欢迎点击链接或扫码查看了解详情：</p><p><img src="https://static001.geekbang.org/infoq/fe/fe88b78623d830bffc29a1edf7bf4896.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LxsPR3YosPcpd6vuK8Dm</id>
            <title>超越 Transformer 与 Mamba，Meta 联合斯坦福等高校推出最强架构 TTT</title>
            <link>https://www.infoq.cn/article/LxsPR3YosPcpd6vuK8Dm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LxsPR3YosPcpd6vuK8Dm</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 01:18:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器学习模型, RNN, 自注意力机制, TTT层
<br>
<br>
总结: 近日，斯坦福、UCSD、UC 伯克利和 Meta 的研究人员提出了一种全新架构，用机器学习模型取代 RNN 的隐藏状态。这个模型通过对输入 token 进行梯度下降来压缩上下文，这种方法被称为「测试时间训练层（Test-Time-Training layers，TTT）」。作者提出了一种具有线性复杂度和表达能力强的隐藏状态的新型序列建模层。论文中提出了两种实例：TTT-Linear 和 TTT-MLP，它们的隐藏状态分别是线性模型和两层 MLP。TTT 层在理论上和实验评估中表现出色，尤其是在长上下文处理和硬件效率方面。 </div>
                        <hr>
                    
                    <p></p><p>近日，斯坦福、UCSD、UC 伯克利和 Meta 的研究人员提出了一种全新架构，用机器学习模型取代 RNN 的隐藏状态。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/11/11a12184c6b8b9f659243ab02ec7295d.png" /></p><p></p><p>这个模型通过对输入 token 进行梯度下降来压缩上下文，这种方法被称为「测试时间训练层（Test-Time-Training layers，TTT）」。该研究作者之一 Karan Dalal 表示，他相信这将根本性地改变语言模型方法。</p><p></p><p>自注意力机制在处理长上下文时表现良好，但其复杂度是二次的。现有的 RNN 层具有线性复杂度，但其在长上下文中的表现受限于其隐藏状态的表达能力。随着上下文长度的增加，成本也会越来越高。</p><p></p><p>作者提出了一种具有线性复杂度和表达能力强的隐藏状态的新型序列建模层。关键思路是让隐藏状态本身成为一个机器学习模型，并将更新规则设为自监督学习的一步。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/48/48ea262cabaa3ff0d973bff7158fd874.png" /></p><p></p><p>论文中提出了两种实例：TTT-Linear 和 TTT-MLP，它们的隐藏状态分别是线性模型和两层 MLP。团队在 125M 到 1.3B 参数规模上评估了实例，并与强大的 Transformer 和现代 RNN Mamba 进行了比较。结果显示，与 Mamba 相比，TTT-Linear 的困惑度更低，FLOP 更少（左），对长上下文的利用更好（右）：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6d/6d1b866c95df78c416d0c3804123fbc1.png" /></p><p></p><p>这个结果代表了现有 RNN 的尴尬现实。一方面，RNN（与 Transformer 相比）的主要优点是其线性（与二次型）复杂性。这种渐近优势只有在长上下文的实践中才能实现，根据下图，这个长度是 8k。另一方面，一旦上下文足够长，现有的 RNN（如 Mamba）就很难真正利用所依赖的额外信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/98/98dbf1684098cc3b25fa44d7a267a3d1.png" /></p><p></p><p>并且，大量的实验结果表明：TTT-Linear 和 TTT-MLP 都匹配或超过基线。与 Transformer 类似，它们可以通过限制更多的代币来不断减少困惑，而 Mamba 在 16k 上下文后则不能。经过初步的系统优化，TTT Linear 在 8k 环境下已经比 Transformer 更快，并且在 wall-clock 时间上与 Mamba 相匹配。</p><p></p><p>TTT 层在理论上和实验评估中表现出色，尤其是在长上下文处理和硬件效率方面。如果在实际应用中能够解决一些潜在的工程挑战，如大规模部署和集成问题，工业界对 TTT 层的接受度也将逐步提升。</p><p></p><p>论文链接：<a href="https://arxiv.org/pdf/2407.04620v1">https://arxiv.org/pdf/2407.04620v1</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8u81YeTiZrw8BE0XXXmM</id>
            <title>微软中国CTO韦青：亲身经历大模型落地的体会与思考 | QCon</title>
            <link>https://www.infoq.cn/article/8u81YeTiZrw8BE0XXXmM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8u81YeTiZrw8BE0XXXmM</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 14:04:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 演讲嘉宾, 技术浪潮, 看不见的大猩猩, 企业增长
<br>
<br>
总结: 在技术浪潮中，企业增长过程中存在着一些被忽视的关键问题，这些问题就像企业内部的“看不见的大猩猩”，可能成为发展的“卡点”。演讲嘉宾通过分享自身经验，提出解决这些问题的方法。 </div>
                        <hr>
                    
                    <p>演讲嘉宾 | 韦青 微软（中国）公司 / 首席技术官</p><p>编辑 | 蔡芳芳 傅宇琪</p><p></p><p>在大模型、AIGC 的冲击下，大多数人把目光聚焦在技术浪潮上，聚焦在那些“容易解决”的问题上，但实际上企业增长过程中还存在很多显而易见的、需要解决的、关键的问题，这些问题就像“看不见的大猩猩”一样存在于企业之中，这些问题很可能成为企业快速发展的“卡点”。</p><p></p><p>微软中国 CTO 韦青在 2024 年 4 月举办的 QCon 北京发表的<a href="https://qcon.infoq.cn/2024/beijing/presentation/5873">《看不见的大猩猩——智能时代的企业生存和发展之路》</a>"的主题演讲中，结合自身经验，聚焦企业内部这些被忽略的“大猩猩”，分享关键问题的解决之道。本文由 InfoQ 整理，经韦青老师授权发布。</p><p></p><p></p><blockquote>InfoQ 将于 8 月 18-19 日举办 AICon 上海站，我们已经邀请到了「蔚来创始人 李斌」，他将在主论坛分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。更多精彩议题可访问官网了解：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>思想的力量</h2><p></p><p></p><p>能够把一件事情做成功不容易。在实现的过程中，会有很多局限。在这个新生事物层出不穷的时代，有一个常见的根本性局限，就是人的思想局限，体现为对事实真相辨析不明和经常用旧的逻辑尝试解决新的问题。</p><p></p><p>在这个世界上，对于事物真相的判断，存在着事实、现象和观点这三个不同的维度。</p><p></p><p>获取大家公认的事实非常困难，理论上讲是不可能的。因为每个人都是通过自己的“有色眼镜”，也就是感觉器官来观察事物，得到的是感觉器官所感受的现象。人们会基于个人经历、背景和认知偏差对所观察到的现象进行解读，从而形成自己的观点。由于观点都带有主观色彩，因此既代表现象，也不代表事实，只可用作讨论的素材。人们有可能从基于观点的辩论，而达成对于现象和事实的一致意见。</p><p></p><p>如果你看过电影《肮脏的哈里》中的演员克林特·伊斯特伍德，他在该片中有一句著名的台词：“观点就像&lt;人体的排泄出口&gt;，每个人都有。（都有怪味，但每个人都认为自己的比别人的好闻）”。</p><p></p><p>理解了这个道理之后，我们在听到社会上的某种流行说法时，先要明确这是某种观点，还是大家已经达成共识的现象，以及它背后所指向的事实大致是个什么样子。不要过早地受观点的影响，以避免陷入“看不见的大猩猩”的陷阱，被信息误导，只关注到媒体让我们看到的事物，而不是正在真实发生的事物。例如，现在我们在网上、在朋友圈里看到的许多现象，很可能都是一些具有引导性的关注点。但真正的事实是什么呢？我们需要超越表面现象，深入探究事物的本质。</p><p></p><p>在技术实践中，我发现人的问题其实是最复杂的。虽然技术难题很难攻克，但技术毕竟是人创造的。一旦我们的思想方法出现问题，那么无论多么优秀的人，再怎么努力，其做事的结果也非常有限。但是思想转型是最难的，要改变思想，我们需要采用成长型思维的方式，不断学习，不断纠偏。这意味着我们必须认识到，我们的思想都是有偏差的，我们常常错误地将所观察到的现象视为事实的全部，并迅速形成一种观点，认为别人是错的，自己是对的。在这个时代，还没有人能够爬上山顶，看到未来。在这个不确定性主导的世界里，未来只要还未发生，对其判断就是一个概率问题。</p><p></p><p>人工智能的实现依靠计算机器基于数据而学习，数据的问题就像是一个房间里的大象，也可以说是看不见的大猩猩。说它是一头房间里的大象，是说这个问题很明显，但是很麻烦，大家都不愿意主动指出来。针对优质数据的积累，它无法简单地靠堆砌资金和人力，或者只要有海量算力就可以解决，它需要漫长的文明积累，不是所有的数据都具备可以被学习的知识，只有那些能够表征一个文明特征的数据才能够让机器学到代表这种文明的知识与价值观。说它是一头看不见的大猩猩，是说明这个问题经常被媒体所误导，被大众所忽视，人们看到的都是有关算力、算法的探讨，而人工智能的实现是一个复杂的系统工程，各种前提条件缺一不可，就像一个水桶，不管构建水桶的木板有多长，它的存水量由最短的那块板决定。人工智能就像一个孩子，是被数据培养出来的。如果提供给它的数据是有偏差的，那么它的行为和决策也一定是有偏差的。如果数据来自他人，那么训练出的模型的行为和偏好也将是别人的。</p><p></p><p>对于所有企业来说，我认为第一步是显而易见的，今天现场发布的报告《中国生成式 AI 开发者洞察 2024》（后台回复「开发者洞察」即可下载）已经给出了答案。但有时候，我们需要关注的是那些显而易见，但大家不愿意去触及的麻烦。例如，人才问题、数据问题、流程再造问题，这些都是我们所说的“硬核”问题。这些问题并不是多么新鲜或者伟大的问题，它们都有一个共性，就是跟脚踏实地的作风和漫长的积累相关，跟保持独立的思想和不盲从潮流相关，与每个个体与组织愿意耐心花多长时间取得成就相关，它们需要因人、因时、因地制宜，不能简单地复制粘贴。每个个体、每个组织都有自己的数据特征、流程特征、人才储备和资金储备，以及行业特征。</p><p></p><p>我们不仅应该学习别人做事的正确方法，更应该借鉴别人犯错误的教训，而不是一味的想找到所谓的“最佳实践”。因为在探索期间，就像踩雷区一样，或者像查理·芒格所说的——智慧不是在于做对每件事，而是在于知道哪些事情是错的。在一个极度不确定的时代，使用这种方法可能不能保证我们成功。但它能让我们成功的概率稍微高一点，哪怕只是一点点。</p><p></p><p>如果大家仔细观察业界所谓的成功公司，你会发现它们的成功大多都概率性的，只是因为活下来了，是幸存者，这种经验的总结很容易陷入“幸存者偏差”的陷阱。没有人在微软、OpenAI、英伟达、谷歌、华为、阿里、百度、腾讯等公司成立之初就敢说他们一定会成功。成功是因为他们在重要的事情上犯的错误少，只要不死，你生存下来的机率就变大了。因此，不是说不要向成功者取经，一是不必照搬，二是还要看看这些公司没有做什么。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a6/a6762dfadecee7ab16a2c03914102d72.png" /></p><p></p><p>什么才是真正的思想变革？我们每个人的思想实际上都是非常固化的。特别是那些越成功、经验越丰富、过去成就越多的人，他们的思想往往更加难以改变，这是一个公认的事实。</p><p></p><p>在计算机历史上，有一位非常著名且直言不讳的人物，他就是艾兹格·W·迪科斯彻。上世纪，当有人问他计算机是否能够思考时，他回答说：“提问‘计算机是否能思考’就像问‘潜艇是否能游泳’”。他的回答正确与否并不重要，重要的是那一代计算机科学家所展现出的探索精神，他们不受经验的束缚，能够洞察事物的本质，这种精神在当今时代尤为重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d5/d54914db5fd62cb2eae0b148a55fce13.png" /></p><p></p><p>我再举一个与今天更相关的例子，那就是马车与发动机的故事。</p><p></p><p>20 多年前，麻省理工学院第一任人工智能研究室主任西摩尔·派普特提出了一个思想实验，让我们想象，如果一名现代喷气发动机工程师穿越回 19 世纪初，向当时的马车夫和马车行展示喷气发动机，并询问这是否能帮到他们。</p><p></p><p>大家首先想到的可能是将喷气发动机安装到马车上，我把这解读为“AI+”，因为将发动机装到马车上，确实可以让马车比马跑得更快、更省事。但这是否是我们的最终目标呢？绝对不是。我们真正要做的，是因为新工具的出现而重构整个行业，甚至是整个社会的基本原型。</p><p></p><p>从马车到汽车的转变，交通的本质目的没有变，依然是将人和物品安全、可靠、及时、高效地从一点移动到另一点。但是，如果我们用马拉车，我们需要考虑的是在马路上每隔两公里设计一个草料堆与化粪池。而如果是汽车，我们则需要每隔 几十 公里建一个加油站和服务站。无论是加油站还是化粪池，都有它们存在的必要，也都有它们存在的前提条件。都是商机，只不过是不同思想层面的商机。这种不同的思想层面代表了不同的思维范式、工业范式和文明范式。要注意这种种发展方向之间，并没有对错，只是因为不同的人生观和价值观而选择不同的发展道路。这就是不同思想与思维方式的不同结果。</p><p></p><p>今天发布的报告《中国生成式 AI 开发者洞察 2024》已经将这些问题阐述得非常清楚。报告中提到，首先，开发需要有场景，需要理解大语言模型的开发，我将其理解为必须知道我们解决的人类问题是什么。其次，我们需要知道如何使用工具。第三，我们需要找到合适的工具。比如，如果我们要在墙上挂一幅画，大概率我们会用到钉子或螺丝，使用锤子或螺丝刀。而如果我们在工厂里组装一辆汽车，那么我们面临的问题和所需的工具及方法就会完全不同。</p><p></p><p>西摩尔·派普特曾经说过：“如果思想不改变，无论你拥有的新工具有多么先进，它又能改变什么呢？如果这个工具只是被用在马车上的一个特别优秀的引擎，它确实能让马车比马跑得快”。在当前的流程中，我们已经在应用人工智能工具，无论是生成式的还是传统的，各种类型的人工智能工具都在被使用。这使得大家普遍感觉到，人工智能工具似乎有用，但又似乎没有达到预期的效果。我的判断是：虽然目前我们在使用这些工具，但最终，我们的整个社会范式将会被人工智能所改变。这意味着从“AI+”（AI 的简单添加）到“AI 化”或“AI 乘”（AI 的深度融合和乘数效应）的转变。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/008367c55ce6904aff15fe12babf9379.png" /></p><p></p><p>我们当前所有的话语体系都在讨论应该做些什么，进行什么样的设计，包括人工智能的生成式应用。这就引出了一个问题：人工智能是否仅限于生成式应用？传统人工智能是否已经没有价值？我们其实并不关心它具体是哪种类型的人工智能，只要它能够帮助我们解决问题，通过社会架构的重新构建，它就能成为一个新的工具。</p><p></p><p>理解了这个道理之后，我们可能会认识到，在做事的过程中，一方面我们需要跟随，但我们必须记住，仅仅跟随是永远没有未来的。大多数情况下我们所面对的都是概率问题。未来的一切都与概率相关，无论是贝叶斯思维还是蒙特卡洛方法。想象一下，如果我们在座的所有人都去追求同一个目标，采用同一种范式，那么我们成功的概率是多少？或者更具体地说，你能成功的概率是多少？</p><p></p><p>无论工具多么先进，即使喷气式引擎研发得再好，如果人们想到的只是将其安装在马车上，那么我们很难找到新的出路。那么，汽车到底会是什么样子？整个社会形态将如何变化？我们是否需要建立加油站、铺设柏油马路、设置收费站？是否需要有人开始研究整个统筹学、运筹学、算法优化等科学领域，以把握先机并引领变革？</p><p></p><p>要知道我们真正需要找到的是解决某种问题的方法。那么我们的问题是什么呢？人工智能的方法很多，可以是生成式的人工智能，也可以是传统的人工智能，还可以具身的人工智能。当我们要解决的问题没明确的时候，只是谈人工智能作为一种工具和方法孰优孰劣，那也仅仅是某种观点而已。问题是：我们作为人类，有多大的信息处理能力能够看多少内容？我们到底要的是机器生成的内容还是人生的幸福感？这种幸福感是通过生成一大堆文字、图片和视频就能获得，或者只是要比其他人更强，还是需要借助机器的能力扩大人类的探索边界，扩展人类的知识，增长我们的智慧，加深我们对浩瀚宇宙真相的理解，从而让地球文明能够突破现在的局限，不再受物质与能量的束缚，进入到一个关注思想繁荣与智慧增长的信息文明时代。我认为对于这些问题的回答将决定下一步的人工智能何去何从。</p><p></p><h2>机器的使命</h2><p></p><p></p><p>回顾一下机器的概念，在这里我引用了微软董事长兼 CEO 萨提亚·纳德拉经常引用的来自道格拉斯·恩格尔巴特的观点。道格拉斯·恩格尔巴特是当代计算机文明的奠定者之一，他的技术愿景和对于人类社会发展的洞察仍然指导着当前计算机器的发展方向。恩格尔巴特持有与范内瓦·布什和司马贺这些思想家相似的观点，都认为人类社会接下来最大的挑战就是因为信息过载而造成的复杂度已经远远超出了人类所能够处理的范畴，信息技术提升了人类社会效率，也加大了复杂度，我们需要计算机器帮助我们处理因为计算机器所带来的问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e6/e6768cdd05e25b81296288e79b5f1c19.png" /></p><p></p><p>人类作为一种碳基动物，通过我们的五种传感器感知周边的世界，再通过神经传导的电化学作用将我们用传感器从周围世界感知的信号传送至我们的大脑进行处理与加工。由于信息量的泛滥和通讯的普及，再加上大多数人类的思想能力还没有进化到可以有节制地使用我们的大脑信息处理能力，一味不加选择和节制地摄入信息，就是得我们的大脑像暴饮暴食的身体一样，开始出现了大脑“肥胖症”。当然，人类经过多年的工业文明熏陶，已经知道我们的身体不能随意摄入垃圾食品，我们开始有所选择，有所判断。那我们是怎样对待大脑的呢？让我们反思一下，从今天早上醒来到现在，你所看到的消息中，哪些是真的，哪些是假的，哪些是半真半假的，哪些是有偏差的，哪些是欺骗你的，哪些是对你有帮助的，哪些是无用的，哪些是虚构的？</p><p></p><p>实际上，我们大多数人并没有深思熟虑，而是直接接受了这些信息。大家再回想一下，从今天早上醒来到现在，你所看到的消息，你还记得多少？记住的信息中，又有多少是真正能够用得上的？我们的大脑是在空转，不断地摄入“垃圾食品”，还是真正吸收到了有用的信息，让我们的思维能力得到提升？</p><p></p><p>在讨论接下来的实操内容之前，我想强调，在这个时代，能够“不被骗”就是最大的优势。我们所比拼的是什么呢？是智商、情商、理商。我现在再加上一个信商，即信息智商。我们搞 IT 的都知道网络信息安全有“零信任”原则，那么在面对信息摄取时，我们是否也应该采取信息摄取的“零信任”态度呢？</p><p></p><p>“看不见的大猩猩”的作者在 2023 年又写了一本书，名为《没有人是傻瓜》。他在书中提出，每个人都应该遵循的原则是“少信一点，多验证一点”。我们应该默认所有由像素生成的信息，大概率都有可能是假的。我们也应该像网络一样，对所有像素构成的信息采取零信任的态度，然后训练出我们自己的算法来帮助我们鉴别信息的真伪。否则，我们这些工程师、科学家可能会被一些信息误导，不是受人制约，，而是被人骗，被人牵着鼻子到处乱跑，误以为自己走对了方向。我们天天去学习别人的最佳实践，天天去学习标准答案，却忽略了一个最基本的事实：我们已经进入了一个开卷考试的时代，现在最不缺的就是标准答案，最缺的是经过独立思考而得到的适合我们自己的答案。</p><p></p><p>接下来，我想简单地展望一下未来。我认为现在有很多人在还没有开始攀登之前，就拼命想象山顶会是什么样子，是好是坏，是否可行。但实际上，大多数人甚至还没有开始他们的旅程。就目前而言，我认为智能机——不要过分夸大其作用，它就是一个智能机——但它所开创的，是帮助人类了解极大、极小、极远、极近的领域。所谓极大指的是宇宙，极小指的是量子，极远指的是太空，而极近则是更深入地了解我们自己，最终理解我们是谁。</p><p></p><p>现在的智能机以非常高的效率，推动了"AI for Science"（科学智能）和"AI for Everyone"（人人智能）的发展，尤其是"AI for Science"，科学探索得到智能机的加持后，其研究进展的速度是惊人的。如果你了解一下当前生物学、细胞学、医学以及量子物理学界的研究进展，你会发现这些进展远远超出了我们对生成式人工智能的想象。微软在英国剑桥的 AI 研究院，已经转向"AI for Science"的研究，并取得了许多突破，包括在材料科学领域的应用。众所周知，我们人类对于自身、对于量子层面的理解还是非常有限的，而机器在这些方面可以提供巨大的帮助，比如在分子、原子、细胞、DNA\线粒体、材料、能源构成等领域，还有很多未知等待我们去探索。</p><p></p><p>目前我们所观察到的，比如制作图片、视频或生成文字等，这些在办公自动化中非常有用，但它们只是智能机器能力的一个狭小的领域，。我在下图提到了一些可能性，并用红色进行了标注。但这些也只是美好的愿景，它们需要落地实现，智能技术的落地也是有次第可循的逻辑。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/70/704f24927fc31f6e7132fee0fdc20fe0.png" /></p><p></p><h2>落地的次第</h2><p></p><p></p><p>我之所以一开始讲述这么多有关思想与观念的话题，是因为如果我们不能够放下我们每个人，尤其是成功人士的思想成见，放下我们思想中的历史包袱，我们接下来的行动很可能只是在马车上装上一个引擎，比拼谁的马车装上引擎后跑得更快，而不是认识到汽车虽然开始时可能比你的马车跑得慢，但最终汽车必将远远超越马车。换句话说，对我们来说，每个人的挑战在于，我们可能仍然固守在要给马车装上引擎的想法上，因为我们需要活在当下，但我们需要开始意识到我们的目标不是如何将马车打造得更好，装上多少引擎，或者如何改造它，而是要开发出一整套全新的汽车架构和现代化交通体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/46/469caca8b6874da927c95fe5dc2e3c17.png" /></p><p></p><p>我相信大家对下图所展示的架构已经有所了解，它从基础架构出发，经过应用架构，再到开发的架构，最后落实到具体的应用场景。虽然这是一个显而易见的过程，但是即便如此明显，真正能够找到用户实际需要的应用的人却微乎其微。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/deeca5d58328098de1aca01d117cbfe6.png" /></p><p></p><p>我的观点是：“知道山在那里，并不意味着你就能登上山顶；而在山脚下，你也无法确定这是不是最高的山峰。”尽管学习、观察和借鉴他人是非常重要的，但这些只是必要条件，并不是充分条件。即便你知道山的位置，即便你有一张登山地图，这也只是其中的一部分真相。地图并不是真正的现场，而且所有的地图都是别人已经走过的路。</p><p></p><p>我印象最深的是，在读书学工程的时候，曾经调整过 PID 参数。第一次调整 PID 参数时感到很困惑：书上的理论知识都是正确的，函数也写对了，但电动机就是在那里抖动。书本的知识和实际现场出现的情况是不一样的。因此，亲身实践和实证非常关键。学习到一定程度固然重要，但最终还是要自己找到解决方案，找到适合自己的方法。</p><p></p><p>大家在观察许多人工智能的开发模型和范式时，可能会注意到有很多夸张的宣传，很多讲解倾向于“一剑封喉”的断言，只要这样做就有那样的结果。但只要一种思维逻辑具象为一种具体的方法，就会有它适用的前提条件约束。只有回归到它的思维逻辑，才能够让借鉴者根据具体情况具体分析与解决。</p><p></p><p>首先，智能机需要明确理解其目的；其次，它需要理解自身的能力边界；第三，它必须了解其操作的约束条件，也就是它所处的约束空间。接下来，智能机需要有能力寻找可用的外部资源和工具，然后开始有步骤地拆解任务，决定是顺序执行还是并行执行。在执行过程中，还需要一个实时的反馈链，也就是持续的反馈机制，不断行动，不断纠偏。</p><p></p><p>现在在网上，有些人将“reflection”翻译为“反思”，这种翻译是有问题的。机器真的会反思吗？实际上，机器只是在反馈的基础上，针对它的预测进行计算纠偏，而并不会像人类那样进行深入的反思。因此，我们不应该将描述人类思维的词汇用在机器上，这样做是非常危险的，因为它可能会导致我们对机器的能力作出错误的判断。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/622be485c9e61d78e6837d9c1e15ed05.png" /></p><p></p><p>理解了上述内容之后，我们可以进一步抽象化地看问题。实际上，我们每天从早到晚都在进行人 - 机协同的工作。作为个体，我们每个人都有自己明确的目标。在下图中，用红色表示人类正在执行的任务，用蓝色表示机器正在执行的任务。想想看，从今天早上起床到现在，无论是坐车、打车、开车来，还是听课、看手机，我们是不是一直在这样与机器协同工作？这种协同工作会出现什么样的现象呢？真正的机会在于我们是否能够识别出我们的痛点是什么。一旦我们识别出了自己的痛点，如果这种痛点可以由机器辅助，就成为智能机可以发挥作用的地方。熟悉 TRIZ 创新方法论里面 TESE 方法的人可能就看出，其实这就是逐步减少人类参与的系统创新方法，并不是什么突然出现的话题，只不过随着人类所发明的工具的进步，创新的方法也在不断演变之中。对于这一轮的智能机器而言，数据是核心，也就凸现出过去几十年一直强调数字化的重要性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd38b56e2f35bb2b6f072f5456be464e.png" /></p><p></p><p>没有数据就无法实现智能算法，没有数字化就没有数据。因此，我们仍然需要进行数字化改革。一旦我们理解了这个基本原理，就会发现有很多问题就像是屋子里的大象，虽然没人直接谈论，但它们却明显地挡在我们前进的道路上。</p><p></p><p>仅有人才的思想解放、组织的重构和流程的再造是不够的。为了实现数字化转型，我们还需要结合数字化产品和实时反馈链，以及整个产业链的协同。这不仅仅是个体和公司层面的事情，还需要整个生态系统中上下游合作伙伴的相互匹配和协调。在这种情况下，数据才是真正为了 AI 而生的数据。</p><p></p><p>作为一间合资公司的总经理，我也管理着公司，当公司试图利用智能机器的能力优化公司的客户服务时，我们也以为已经积累了很多服务数据，可以很方便的实现服务的智能化了吧？但当你尝试将 AI 应用上去时，会发现这些过去积累的数据，其建模方式并没有针对智能机器算法而优化，可用，但效果并不好。为什么？人们常常误以为机器可以学习任何东西，尤其是非结构化数据，我认为这是一个非常误导人的说法。扪心自问，机器真的能学习非结构化数据吗？机器学习真的能学习没有标签的数据吗？所谓的非监督学习，其监督已经内嵌在了提供给它的数据结构中。也就是说，数据本身必须带有逻辑，机器才能从中学习。真给机器丢一堆没有内在模式的数据，它是不可能凭空识别出模式的。</p><p></p><p>只有拥有数据，我们才能获得真正的知识，然后再将这些知识转化为 Copilot，加入到每一个流程中去。我认为这将是未来作为人类智能助手的终极解决方案。这也与我之前提到的报告相呼应，实际上并不存在哪个行业或工作内容不会被重构，所有可以被拆解成流程的工作都将被重构。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/37315f3153a052fcbff261ee5451d186.png" /></p><p></p><p>流程中的一个关键点是必须同时满足人类能理解和机器能处理的条件。回顾历史，这实际上是真正的 Web 3.0 概念，也就是十多年前的语义网，那时语义网技术被称为 Web 3.0 技术。这里的关键不在于术语本身，而在于其内涵，我们的整体愿景是将我们的生活、世界和社会构建成既能被人理解也能被机器处理的形式。我们每家公司的领域知识、专家知识，以及行业知识是否都能被建模，以至于机器能理解和处理呢？以目前流行的售后服务数字化和智能化为例，你的知识库真的能够做到既被机器理解也能被机器处理吗？以我的经验来看，这通常需要重新构建。这里重点是关注流程的智能化和智能社会的构建，无论是 AI 加法还是 AI 乘法，实际上都要考虑重构，而不是简单地在马车上加一个引擎。</p><p></p><p>精心打磨流程重构、数字建模，实现数据生成与管理，进而实现流程智能化。但遗憾的是，大多数人关注点都集中在最后一点，即流程智能化，而忽视了前三点。前三点是整个金字塔的基座，如果基座不稳固，整个结构将会动摇。此外，还有第五点，即“人在循环中”。因为机器并非万能，因此人与机器的结合是非常关键的，但这还不够。在第五点之上是第六点，即“人在环路上”，涉及到控制论的一阶、二阶和三阶控制与优化，也可称为一阶、二阶和三阶的学习过程，是对做事的“为什么做 - 做什么 - 如何做”(Why-What-How) 的反向优化过程，与机器学习所依赖的反向传播机制在概念上是相通的。</p><p></p><p>我们现在要做的第一步是将所有流程重构，让机器完成工作，赚取利润。但为了预见最终到达的位置，还必须建立一个新的人机文化。如果仅仅关注机器或技术能做什么，而不将人的因素考虑在内，这样的企业是不可能长久的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1f/1fedb0b91416ff0beafef4e4ad540271.png" /></p><p></p><p>最后，我想谈谈信息文明的最显著特征，那就是避免被欺骗，这种能力也会被别有意图的人利用来进行欺骗。我们现在生活在一个信息社会中，真的能够辨别出对自己有用的信息，并据此做出正确的判断吗？我认为这是非常困难的。我们每个人都被包裹在自己的信息茧房中，只能看到我们想看到的，只能听到我们想听到的。那么，我们如何确保自己不会被这个看不见的大猩猩——信息茧房所限制呢？这是对我们每个人的最大挑战。</p><p></p><p>今天，我分享这些内容，是因为我认为如果不解决这个问题，仅仅偶尔得到一些答案，偶尔成功一下，是远远不够的。因为如果没有建立起对未来技术发展的坚定信念，那么偶尔的成功并不能代表最终的成功。这是我从微软的一些工程师那里听到的他们的心态历程，现在分享给大家。微软的 CEO、董事长萨提亚·纳德拉在 2022 年 5 月就已经提到了 ChatGPT、OpenAI、Copilot 等概念。2022 年 11 月底 ChatGPT 发布后，大家蜂拥而上。微软的工程师其实也经历了这样的过程。起初是“AI+”，将 AI 应用到所有事物上，但第一代产品发布后发现效果不佳，那还只是一辆马车。</p><p></p><p>因此，他们转向了“Everything for AI”，即利用新的工具重构产品的流程和使用方法，包括人机交互界面、流程、判断和执行。最终他们发现，关键不在于 AI、数据、显卡或算法，而在于解决人世间的风花雪月、衣食住行。我们的产品如果能够解决人类的基本需求，就一定具有生命力。如果你的观点只局限于一个喷气发动机或者一辆马车上，那么我认为你不会取得太大的成就。</p><p></p><p>今天QCon大会的标题是“大模型正在重新定义软件”。大模型是什么？“正在”是什么？重新定义软件所代表的结果是把软件视为一辆马车加上一个引擎，称之为重新定义，还是真正地重新定义软件工程、软件流程，改变整个软件的生命周期？</p><p></p><p>微软研究院的一些研究员专门研究“AI for software engineering”，我认为这可能就是大家在软件开发行业即将面临的未来。当我们还在认为在马车上加引擎的时候，已经有一些人开始意识到这只是暂时的现象。我们真正要的是一个现代化的、以汽油、柴油驱动的交通体系。而在 100 年后，我们发现这个交通体系实际上是由电力驱动的。这将是我们未来几十年面临的机会与挑战。</p><p></p><h4>书籍推荐</h4><p></p><p></p><p>演讲最后，韦青老师向大家推荐了《提示工程：方法、技巧与行业应用》这本书，他提到：“这是本工具使用者，同时也是开发工具的老师傅写给工具使用者的书，了解机器的特性，才能更好地利用机器增强人类自身生存与发展的能力。”</p><p></p><p>对于《BPMN2.0——业务流程建模标准导论（第二版）》、《DATA 数据建模经典教程（第二版）》这两本书，韦青老师的评价是：“简单，不厚，但是有用。”</p><p></p><p>除此之外，还有以下推荐书目，感兴趣的读者可以进一步了解~</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d36c5d7863a6f5ba444bcb7fbbc9310.png" /></p><p></p><p>会议推荐：</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会</a>" ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p><img src="https://static001.geekbang.org/wechat/images/a1/a18c960aa61239381eeb060de89d19a2.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/44aSfCiMd905NBRMSFY6</id>
            <title>CPU，正在被 AI 时代抛弃？</title>
            <link>https://www.infoq.cn/article/44aSfCiMd905NBRMSFY6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/44aSfCiMd905NBRMSFY6</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 07:16:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 医院门诊, AI推理技术, 大模型应用, CPU
<br>
<br>
总结: 在医院门诊中，医生们利用AI推理技术和大模型应用提高病例撰写效率，同时保护患者隐私。在其他业务场景中，律师也利用大模型进行法律文件分析。CPU作为通用处理器在AI时代发挥重要作用，具有强大的单核性能和内存容量，能满足大模型任务需求。企业应关注CPU的效价比，而不是仅关注算力规模。 </div>
                        <hr>
                    
                    <p>在某三甲医院的门诊中，汇集了来自各地的病患，医生们正在以最专业的能力和最快的速度进行会诊。期间，医生与患者的对话可以通过语音识别技术被录入到病例系统中，随后大模型 AI 推理技术辅助进行智能总结和诊断，医生们撰写病例的效率显著提高。AI 推理的应用不仅节省了时间，也保护了患者隐私；</p><p></p><p>在法院、律所等业务场景中，律师通过<a href="https://www.infoq.cn/article/FA0iHBPehJsZZ3Xk1HiY">大模型</a>"对海量历史案例进行整理调查，并锁定出拟定法律文件中可能存在的漏洞；</p><p></p><p>……</p><p></p><p>以上场景中的大模型应用，几乎都有一个共同的特点——受行业属性限制，在应用大模型时，除了对算力的高要求，AI 训练过程中经常出现的坏卡问题也是这些行业不允许出现的。同时，为确保服务效率和隐私安全，他们一般需要将模型部署在本地，且非常看重硬件等基础设施层的稳定性和可靠性。一个中等参数或者轻量参数的模型，加上精调就可以满足他们的场景需求。</p><p></p><p>而在大模型技术落地过程中，上述需求其实不在少数，基于 CPU 的推理方案无疑是一种更具性价比的选择。不仅能够满足其业务需求，还能有效控制成本、保证系统的稳定性和数据的安全性。但这也就愈发让我们好奇，作为通用服务器，CPU 在 AI 时代可以发挥怎样的优势？其背后的技术原理又是什么？</p><p></p><h2>一、AI 时代，CPU 是否已被被边缘化？</h2><p></p><p></p><p>提起 AI 训练和 AI 推理，大家普遍会想到 GPU 更擅长处理大量并行任务，在执行计算密集型任务时表现地更出色，却忽视了 CPU 在这其中的价值。</p><p></p><p>AI 技术的不断演进——从深度神经网络（DNN）到 Transformer 大模型，对硬件的要求产生了显著变化。CPU 不仅没有被边缘化，反而持续升级以适应这些变化，并做出了重要改变。</p><p></p><p>AI 大模型也不是只有推理和训练的单一任务，还包括数据预处理、模型训练、推理和后处理等，整个过程中需要非常多软硬件及系统的配合。在 GPU 兴起并广泛应用于 AI 领域之前，CPU 就已经作为执行 AI 推理任务的主要硬件在被广泛使用。其作为通用处理器发挥着非常大的作用，整个系统的调度、任何负载的高效运行都离不开它的协同优化。</p><p></p><p>此外，CPU 的单核性能非常强大，可以处理复杂的计算任务，其核心数量也在不断增加，而且 CPU 的内存容量远大于 GPU 的显存容量，这些优势使得 CPU 能够有效运行生成式大模型任务。经过优化的大模型可以在 CPU 上高效执行，特别是当模型非常大，需要跨异构平台计算时，使用 CPU 反而能提供更快的速度和更高的效率。</p><p></p><p>而 AI 推理过程中两个重要阶段的需求，即在预填充阶段，需要高算力的矩阵乘法运算部件；在解码阶段，尤其是小批量请求时，需要更高的内存访问带宽。这些需求 CPU 都可以很好地满足。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38310c34a682db740440271f3bb77a79.webp" /></p><p></p><p>以英特尔举例，从 2017 年第一代至强®&nbsp;可扩展处理器开始就利用英特尔®&nbsp;AVX-512 技术的矢量运算能力进行 AI 加速上的尝试；再接着第二代至强®&nbsp;中导入深度学习加速技术（DL Boost）；第三代到第五代至强®&nbsp;的演进中，从 BF16 的增添再到英特尔® AMX 的入驻，可以说英特尔一直在充分利用 CPU 资源加速 AI 的道路上深耕。</p><p></p><p>在英特尔®&nbsp;AMX 大幅提升矩阵计算能力外，第五代至强® 可扩展处理器还增加了每个时钟周期的指令，有效提升了内存带宽与速度，并通过 PCIe 5.0 实现了更高的 PCIe 带宽提升。在几个时钟的周期内，一条微指令就可以把一个 16×16 的矩阵计算一次性计算出来。另外，至强® 可扩展处理器可支持 High Bandwidth Memory (HBM) 内存，和 DDR5 相比，其具有更多的访存通道和更长的读取位宽。虽然 HBM 的容量相对较小，但足以支撑大多数的大模型推理任务。</p><p></p><p>可以明确的是，AI 技术的演进还远未停止，当前以消耗大量算力为前提的模型结构也可能会发生改变，但 CPU 作为计算机系统的核心，其价值始终是难以被替代的。</p><p></p><p>同时，AI 应用的需求是多样化的，不同的应用场景需要不同的计算资源和优化策略。因此比起相互替代，CPU 和其他加速器之间的互补关系才是它们在 AI 市场中共同发展的长久之道。</p><p></p><h2>二、与其算力焦虑，不如关注效价比</h2><p></p><p></p><p>随着人工智能技术在各个领域的广泛应用，AI 推理成为了推动技术进步的关键因素。然而，随着通用大模型参数和 Token 数量不断增加，模型单次推理所需的算力也在持续增加，企业的算力焦虑扑面而来。与其关注无法短时间达到的算力规模，不如聚焦在“效价比”，即综合考量大模型训练和推理过程中所需软硬件的经济投入成本、使用效果和产品性能。</p><p></p><p>CPU 不仅是企业解决 AI 算力焦虑过程中的重要选项，更是企业追求“效价比”的优选。在大模型技术落地的“效价比”探索层面上，百度智能云和英特尔也不谋而合。</p><p></p><p>百度智能云千帆大模型平台（下文简称“千帆大模型平台”）作为一个面向开发者和企业的人工智能服务平台，提供了丰富的大模型，对大模型的推理及部署服务优化积攒了很多作为开发平台的经验，他们发现，CPU 的 AI 算力潜力将有助于提升 CPU 云服务器的资源利用率，能够满足用户快速部署 LLM 模型的需求，同时还发现了许多很适合 CPU 的使用场景：</p><p></p><p>SFT 长尾模型：每个模型的调用相对稀疏，CPU 的灵活性和通用性得以充分发挥，能够轻松管理和调度这些模型，确保每个模型在需要时都能快速响应。</p><p></p><p>小于 10b 的小参数规模大模型：由于模型规模相对较小，CPU 能够提供足够的计算能力，同时保持较低的能耗和成本。</p><p></p><p>对首 Token 时延不敏感，更注重整体吞吐的离线批量推理场景：这类场景通常要求系统能够高效处理大量的数据，而 CPU 的强大计算能力和高吞吐量特性可以很好地满足要求，能够确保推理任务的快速完成。</p><p></p><p>英特尔的测试数据也验证了千帆大模型平台团队的发现，其通过测试证明，单台双路 CPU 服务器完全可以轻松胜任几 B 到几十 B 参数的大模型推理任务，Token 生成延时完全能够达到数十毫秒的业务需求指标，而针对更大规模参数的模型，例如常用的 Llama 2-70B，CPU 同样可以通过分布式推理方式来支持。此外，批量处理任务在 CPU 集群的闲时进行，忙时可以处理其他任务，而无需维护代价高昂的 GPU 集群，这将极大节省企业的经济成本。</p><p></p><p>也正是出于在“CPU 上跑 AI”的共识，双方展开了业务上的深度合作。<a href="https://www.infoq.cn/article/ufdKm8hZltS1CmssVv1j">百度</a>"智能云千帆大模型平台采⽤基于英特尔® AMX 加速器和大模型推理软件解决方案 xFasterTransformer (xFT)，进⼀步加速英特尔® 至强® 可扩展处理器的 LLM 推理速度。</p><p></p><h2>三、将 CPU 在 AI 方面的潜能发挥到极致</h2><p></p><p></p><p>为了充分发挥 CPU 在 AI 推理方面的极限潜能，需要从两个方面进行技术探索——硬件层面的升级和软件层面的优化适配。</p><p></p><p>千帆大模型平台采用 xFT，主要进行了以下三方面的优化：</p><p></p><p>系统层面：利用英特尔® AMX/AVX512 等硬件特性，高效快速地完成矩阵 / 向量计算；优化实现针对超长上下文和输出的 Flash Attention/Flash Decoding 等核心算子，降低数据类型转换和数据重排布等开销；统一内存分配管理，降低推理任务的内存占用。</p><p></p><p>算法层面：在精度满足任务需求的条件下，提供多种针对网络激活层以及模型权重的低精度和量化方法，大幅度降低访存数据量的同时，充分发挥出英特尔® AMX 等加速部件对 BF16/INT8 等低精度数据计算的计算能力。</p><p></p><p>多节点并行：支持张量并行（Tensor Parallelism）等对模型权重进行切分的并行推理部署。使用异构集合通信的方式提高通信效率，进一步降低 70b 规模及以上 LLM 推理时延，提高较大批处理请求的吞吐。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae1ec4811551bd07e59fe841c7580d8b.webp" /></p><p></p><p>第五代至强® 可扩展处理器能在 AI 推理上能够取得如此亮眼的效果，同样离不开软件层面的优化适配。为了解决 CPU 推理性能问题，这就不得不提 xFT 开源推理框架了。</p><p></p><p>xFT 底层适用英特尔 AI 软件栈，包括 oneDNN、oneMKL、IG、oneCCL 等高性能库。用户可以调用和组装这些高性能库，形成大模型推理的关键算子，并简单组合算子来支持 Llama、文心一言等大模型。同时，xFT 最上层提供 C++ 和 Python 两套便利接口，很容易集成到现有框架或服务后端。</p><p></p><p>xFT 采用了多种优化策略来提升推理效率，其中包括张量并行和流水线并行技术，这两种技术能够显著提高并行处理的能力。通过高性能融合算子和先进的量化技术，其在保持精度的同时提高推理速度。此外，通过低精度量化和稀疏化技术，xFT 有效地降低了对内存带宽的需求，在推理速度和准确度之间取得平衡，支持多种数据类型来实现模型推理和部署，包括单一精度和混合精度，可充分利用 CPU 的计算资源和带宽资源来提高 LLM 的推理速度。</p><p></p><p>另外值得一提的是，xFT 通过“算子融合”、“最小化数据拷贝”、“重排操作”和“内存重复利用”等手段来进一步优化 LLM 的实现，这些优化策略能够最大限度地减少内存占用、提高缓存命中率并提升整体性能。通过仔细分析 LLM 的工作流程并减少不必要的计算开销，该引擎进一步提高了数据重用度和计算效率，特别是在处理 Attention 机制时，针对不同长度的序列采取了不同的优化算法来确保最高的访存效率。</p><p></p><p>目前，英特尔的大模型加速方案 xFT 已经成功集成到千帆大模型平台中，这项合作使得在千帆大模型平台上部署的多个开源大模型能够在英特尔至强® 可扩展处理器上获得最优的推理性能：</p><p></p><p>在线服务部署：用户可以利用千帆大模型平台的 CPU 资源在线部署多个开源大模型服务，这些服务不仅为客户应用提供了强大的大模型支持，还能够用于千帆大模型平台 prompt 优化工程等相关任务场景。</p><p></p><p>高性能推理：借助英特尔® 至强® 可扩展处理器和 xFT 推理解决方案，千帆大模型平台能够实现大幅提升的推理性能。这包括降低推理时延，提高服务响应速度，以及增强模型的整体吞吐能力。</p><p></p><p>定制化部署：千帆大模型平台提供了灵活的部署选项，允许用户根据具体业务需求选择最适合的硬件资源配置，从而优化大模型在实际应用中的表现和效果。</p><p></p><h2>四、写在最后</h2><p></p><p></p><p>对于千帆大模型平台来说，英特尔帮助其解决了客户在大模型应用过程中对计算资源的需求，进一步提升了大模型的性能和效率，让用户以更低的成本获取高质量的大模型服务。</p><p></p><p>大模型生态要想持续不断地往前演进，无疑要靠一个个实打实的小业务落地把整个生态构建起来，英特尔联合千帆大模型平台正是在帮助企业以最少的成本落地大模型应用，让他们在探索大模型应用时找到了更具效价比的选项。</p><p></p><p>未来，双方计划在更高性能的至强®&nbsp;产品支持、软件优化、更多模型支持以及重点客户联合支持等方面展开深入合作。旨在提升大模型运行效率和性能，为千帆大模型平台提供更完善的软件支持，确保用户能及时利用最新的技术成果，从而加速大模型生态持续向前。</p><p></p><p>更多关于至强®&nbsp;可扩展处理器为千帆大模型平台推理加速的信息，请点击<a href="https://www.intel.cn/content/www/cn/zh/artificial-intelligence/baidu-ai-cloud-accelerates-llm.html?cid=soc&amp;source=Wechat&amp;article_id=5682">链接</a>"查阅。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a057f01a6a7902481dedd2593</id>
            <title>京东.Vision首登苹果Vision Pro 背后的技术探索</title>
            <link>https://www.infoq.cn/article/a057f01a6a7902481dedd2593</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a057f01a6a7902481dedd2593</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 03:24:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, Vision Pro, 空间计算, 京东.Vision
<br>
<br>
总结: 去年6月，苹果发布了首款头显设备Apple Vision Pro，今年6月28号在中国发售。京东.Vision作为首批原生应用登陆Vision Pro平台，提供全新的购物方式。用户可以在visionOS的App Store搜索“京东.Vision”进行下载和体验。Vision Pro带来了明显的技术升级，包括VST技术、眼动追踪和空间计算概念。京东.Vision在Vision Pro上进行了技术探索和应用开发，面临了一些挑战，但也取得了一些成果。 </div>
                        <hr>
                    
                    <p>去年6月，苹果正式发布首款头显设备Apple Vision Pro，今年6月28号，Apple Vision Pro正式在中国发售。京东.Vision作为首批原生应用登陆Vision Pro平台，首期以家电家居与潮流数码产品作为切入口，未来将逐步拓展至全品类，用户可以在visionOS的App Store搜索“京东.Vision”进行下载和体验。</p><p></p><p>京东.Vision利用Vision Pro的空间计算技术，提供了一种全新的购物方式。用户可直接将心仪的家电家居或潮流数码产品以1:1等比例“拖拽”到自己家中，直接在空间计算环境中真实预览每件物品在空间中的布局和外观。负责京东.Vision开发的京东零售技术团队成员在过去一年持续关注技术发展动向，不断进行产品尝试，本文将系统性地对过程中遇到的技术问题、思考和实践做简单总结，欢迎大家一起讨论交流。</p><p></p><h1>一、与以往的头显设备相比，Vision Pro有什么不同？</h1><p></p><p>Apple Vision Pro于今年6.28号正式在中国发售。从我们的持续观察来看，苹果强大的软硬件整体设计能力，使得Vision Pro成为第一款真正意义的空间计算设备，与以往的头显设备相较，它带来了明确的技术方向和能力升级：</p><p></p><p>1.VST(Video See Through)是通过摄像头捕捉真实世界的画面，在头显内屏显示摄像头采集的画面，再实现虚实融合的展示效果。Vision Pro上将VST延迟降低到12ms，远低于其它产品的50ms以上水平。 未来很可能会继续引领其他高端设备的技术发展方向，不只是画面的采集和显示，而是采集后同步进行空间场景的数字化建模。</p><p></p><p>2.Vision Pro结合眼动追踪，实现了准确度极高的“手眼”控制系统，眼动追踪相比手柄的定位精度更高，手势操作比手柄更加方便。未来发展方向将是“手眼”操控，也会引领其它高端设备的操控方式。</p><p></p><p>3.苹果在Vision Pro上提出“空间计算”概念，即先将真实环境全部数字化，在数字化之后的真实3D空间中实现可交互，提供了更加沉浸式的互动体验。</p><p></p><h1>二、京东.Vision背后的技术探索，如何在Vision Pro上做应用开发?</h1><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3de4c4b10e269e42bf0d87d50b23ce72.png" /></p><p></p><p>作为AR/VR技术开发者，我们过往已经在手机端实现了VR全景、AR摆摆看、AR试穿戴、3D展示等产品功能，也在思考Vision Pro上最适合的产品功能。过去一年，我们重点围绕与用户实体环境相关的功能应用进行探索与创新，希望提供给用户相比手机APP跃升式的使用体验。</p><p></p><p>过程中遇到了诸多挑战： 作为苹果第一款空间计算设备，Apple Vision Pro带来了全新的visionOS平台，开发者需要适应这一平台的特质，理解其提供的无边空间画布式的交互环境。 其次，在Apple Vision Pro上开发原生3D应用，需要涉及大量对新功能的验证与试错，没有太多现成的范例可供参考。以及，由于visionOS和配套的开发工具仍在不断完善中，某些个性化应用所需的能力尚未提供，这就需要开发者进行自定义功能的扩充，如自定义手势、自定义碰撞效果和自定义组件系统等。</p><p></p><p>接下来，我们将从首页3D商品和场景展示、环境融合的空间计算应用、自定义着色器和手势等方面详细介绍。</p><p></p><h2>1. 3D商品和场景展示</h2><p></p><p>作为第一款真正的空间计算设备，Vision Pro提供了3种内容承载容器：Windows、Volumes、Spaces。默认情况下，APP启动时会进入共享空间。为了实现动态可编排的首页，我们采用Windows容器在主界面展示商品内容，并包含可交互的2D、 3D等内容形态，实现实体商品橱窗的3D展示效果。由于Volumes容器对于模型动画的兼容能力有限，我们采用RealityView进行3D模型的装载，实现了在静态首页上的动态模型展示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cddbecf800495af04308b43ddd0d55ae.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fe79e484f34173fc0ebc782cd8467bc.gif" /></p><p></p><h2>2. 虚实融合的空间计算应用</h2><p></p><p>Vision Pro搭载摄像头、激光雷达、环境光等多种传感器，通过多种传感器的组合，以及M2、R1等芯片的强大处理能力，实现了对空间环境的高精度、高鲁棒性定位和地图构建。 如下图苹果的ARKitScenes环境感知示例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc333febb16943317bff3779e916229b.png" /></p><p></p><p>在空间计算的电商场景中，我们实现了真实空间中的多个虚拟商品摆放问题，来满足用户多品搭配需求。 首先是商品在真实空间的自由移动、旋转、缩放等空间操作，涉及坐标系变换与仿射变换等技术。在3D 视觉中常用的三个坐标系：图像坐标系、相机坐标系、世界坐标系，它们之间可通过仿射变换、投影变换、刚体变换等方式实现运动。Vision Pro中通常涉及SwiftUI CoordinateSpaceProtocol与RealityCoordinateSpace两个坐标系的转换，转换过程中的世界坐标等参数便由空间计算结果提供。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28980db8bdd9f44426e0401e7a72bb6c.png" /></p><p></p><p>利用空间计算的环境感知能力实现平面检测和地图建模，结合商品的实际尺寸信息实现虚拟商品与真实空间平面、垂面的吸附、摆放等在实体空间的摆放操作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d43d2c2cca7c23febb1a111c4f0c3463.png" /></p><p></p><p>3D环绕与AR摆摆看等典型空间计算应用是将现实世界和虚拟世界融合在一起。在Vision Pro中可以使这个过程更加真实，将虚拟模型的遮挡、碰撞、光影反射等各种属性在现实世界模拟呈现。为了实现碰撞效果，首先需要进行模型与周围环境的碰撞检测，通过定义模型的碰撞形状和属性，并赋予物理属性，如质量、摩擦力和恢复系数，可以实现物理碰撞模拟。常见的碰撞形状包括：矩形，球体，胶囊，凸形状等，为了提升碰撞性能通常使用矩形碰撞形状来进行碰撞检测。碰撞检测示意图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a28a1980ca309bcd5af2ac63fc7c046d.png" /></p><p></p><p>当检测到与实体碰撞后，我们根据已经设置的实体物理属性，实时计算实体在三维空间中的移动速度与位移大小，并更新实体位置。通过模拟碰撞，我们可以实现虚拟模型实体与环境实体，虚拟模型实体与虚拟模型实体之间的碰撞运动，以及虚拟模型实体之间的叠放。</p><p></p><p>解决单个商品在实体空间的摆放之后，进一步实现多品摆放，并使多个虚拟商品可实现真实的碰撞交互，解决了用户体验多件商品搭配效果的需求。为此，我们动态调整虚拟商品的物体属性，允许模型碰撞相交，保证初始化时多个模型在视野中全部可见，之后逐步摆放到合适位置。多品摆放示意图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6fe11b2e78b05aee38561194f4e1e5fd.png" /></p><p></p><h2>3. 自定义手势识别</h2><p></p><p>Vision Pro利用摄像头和传感器进行手势识别，例如点击，捏合，缩放，旋转等。这是空间计算所能实现的最具沉浸感的方面之一，因为它允许用户通过显示器操控他们看到的数字对象。这种操控方式流畅且最为熟悉。除了苹果官方提供了Tap，Pinch，Zoom，Rotate等基础手势。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/082e6d31dc86c94a386a5f11acdd1490.png" /></p><p></p><p>我们利用Hand Tracking以及AI深度学习技术扩展了苹果的手势识别功能，让用户可以用更多自定义手势，例如与3D商品进行更精确、流畅的旋转缩放交互。Vision Pro为每只手掌提供25个关键点的数据，其中每个手指有4个关键点，手腕处一个关键点。官方提供6种基础手势，我们在此基础上丰富手势识别功能。通过将关键点信息输入到Rule-based system、DNN、LSTM等模块中，实现动态手势的识别。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/308f79346f1edbf4bfb6c4cbf44525ba.png" /></p><p></p><h2>4. 自定义着色器</h2><p></p><p>依托M2芯片+R1芯片的加持，Vision Pro提供了强大的渲染能力，使得我们可以用自定义着色器实现一些特殊的材质表现和渲染效率优化。比如上文提到的碰撞后的网格特效、商品中的呼吸灯、模型指示器中的UI九宫格等。我们通过Composer Shadergraph实现的UI九宫格，用于指示模型在世界空间中的位置，同时对于不同大小的模型需要保证UI九宫格的4角区域不发生形变，Shadergraph方案以及UI九宫格示意如下所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea5c230a67db4d010c9c90a1c42f635a.png" /></p><p></p><h2>5. 空间计算优化</h2><p></p><p>空间计算与传统运算相比，需要计算的数据量提升了一个维度。在处理大量3D数据时，我们采取了多种优化措施来保持高效率的资源应用和流畅的操作体验。例如，根据商品类别动态调整3D模型质量，合理分配面数以控制资源大小。使用Reality Composer Pro工具打包3D场景和资源，实现有效压缩。此外，通过资源预加载、动态加载与释放以及缓存减少IO等操作，提升界面流畅度和降低响应时间。</p><p></p><p>通过优化技术，我们实现了“3D无界场景”等功能，在一个“无限大”的空间场景中，我们载入了多套高精度模型组，使得用户可以在一个空间内，一站式沉浸浏览。</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92f7fd40c3993806ab9a9fbcbeb089ec.gif" /></p><p></p><h1>三、未来探索方向</h1><p></p><p>Apple Vision Pro作为下一代终端设备，正在引入更多交互方式，提升混合现实的体验效果。京东一直致力于提供多快好省的用户体验，探索更多元、丰富的购物方式。未来我们还将持续打造3D体验和全沉浸式场景体验，引入更多高质量的3D模型与场景、景深视频等资源，逐步补齐3D场景搜索、智能导购、试搭等内容，进一步提升沉浸式体验效果。期待随着技术的不断成熟，一起为用户带来更多新鲜的购物体验。</p><p></p><h1>四、 参考文献</h1><p></p><p>Andrei, Constantin-Octavian. “3D affine coordinate transformations.” (2006).</p><p></p><p>A novel hybrid bidirectional unidirectional LSTM network for dynamic hand gesture recognition with Leap Motion[J]</p><p></p><p>Dynamic Hand Gesture Recognition Based on Short-Term Sampling Neural Networks[J]</p><p></p><p><a href="https://www.cnblogs.com/ghjnwk/p/10852264.html">https://www.cnblogs.com/ghjnwk/p/10852264.html</a>"</p><p></p><p><a href="https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection">https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection</a>"</p><p></p><p><a href="https://developer.apple.com/documentation/realitykit/">https://developer.apple.com/documentation/realitykit/</a>"</p><p></p><p><a href="https://github.com/apple/ARKitScenes">https://github.com/apple/ARKitScenes</a>"</p><p></p><p><a href="https://developer.apple.com/documentation/arkit/arkit_in_ios/configuration_objects/understanding_world_tracking">https://developer.apple.com/documentation/arkit/arkit_in_ios/configuration_objects/understanding_world_tracking</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0VJSOSpO6qdakUj6dMYe</id>
            <title>夸克升级“超级搜索框”推出AI搜索为中心的一站式AI服务</title>
            <link>https://www.infoq.cn/article/0VJSOSpO6qdakUj6dMYe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0VJSOSpO6qdakUj6dMYe</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 02:09:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型时代, 生成式AI, 夸克, 超级搜索框
<br>
<br>
总结: 阿里智能信息事业群旗下的夸克推出了以AI搜索为中心的一站式AI服务，通过大模型和生成式AI技术革新搜索产品，提供智能回答、智能创作和智能总结等功能，满足用户信息检索、生成和处理的需求。 </div>
                        <hr>
                    
                    <p>大模型时代，生成式AI如何革新搜索产品？阿里智能信息事业群旗下<a href="https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji">夸克</a>"“举手答题”。7月10日，夸克升级“超级搜索框”，推出以<a href="https://www.infoq.cn/article/opVYCFjJfTrN6NH4xSQY">AI</a>"搜索为中心的一站式AI服务，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/d0/fa/d0ff42c072607d2a93a2a41b5befa8fa.png" /></p><p></p><p>“能回答、能创作、能总结的超级搜索框，是夸克对AI搜索的新定义。”阿里智能信息事业群总裁吴嘉表示，大模型、数据、场景等优势，推动夸克加速革新搜索产品，创造更大用户价值。“跨过大模型应用全新体验的临界点，夸克全面进入AI时代，一站式AI服务的创新涌现将滔滔不绝。”</p><p></p><h2>全新AI搜索，一框实现回答、创作、总结</h2><p></p><p></p><p>过往，搜索引擎依据关键词，提供网站列表排序。反复挑选、点击、阅读，以及大量不相关结果，成为用户高效获取信息的拦路虎，复杂问题也很难得到满意的回答。</p><p></p><p>AI技术跃迁点燃了搜索的价值焕新。用户打开夸克7.0版搜索框，输入问题即可体验智能回答，还有AI写作、文件总结、视频总结、拍题讲解功能。一个“超级搜索框”集纳了智能回答、智能创作和智能总结三大能力。</p><p></p><p>其中，智能回答能够更好地理解用户意图，聚合全网优质内容，更精准、直接、高效地提供图文、视频等。尤其针对复杂逻辑分析和跨学科知识，智能回答更能发挥AI的综合回答能力，为用户呈现准确、丰富的结果。</p><p></p><p>智能创作方面，夸克AI搜索满足用户各类主题、题材、篇幅的高频写作创作需求，包括文案创作、文档写作、PPT写作、简历制作等，让用户直接得到所需内容。</p><p></p><p>智能总结方面，面对几十万字长文和专业信息，夸克数秒钟就能整理出全文摘要。更惊喜的是，夸克还支持最长5小时视频的字幕导出、分段总结、整体总结、生成脑图、抽取课件PPT等，提升工作学习效率。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f4/0b/f4830b51331e672a5dc45447e7c7600b.png" /></p><p></p><p>此前，夸克升级高考AI搜索，“山东高考580分能否上985”类似问题的个性化志愿推荐能力大大提升。6月高考季，夸克高考AI搜索使用量超过1亿次。</p><p></p><p>夸克产品负责人郑嗣寿介绍，信息检索、创作和总结一直是用户的核心需求，AI搜索让人与信息距离更短。夸克始终坚持在搜索上的价值探索，不断从搜索框中生长出新内容、新工具。“让万事万物都有答案，让答案都有迹可循”。</p><p></p><h2>一站式AI服务，满足信息检索、生成与处理</h2><p></p><p></p><p>始于框但不止于框！从创立之初定位智能搜索，夸克持续突破搜索框的形态与能力边界，在“智能工具+内容+服务”模式下，上新一系列内容产品与智能工具。</p><p></p><p>站在AI时代，夸克以全新的视角去看待产品和需求。夸克7.0版以AI搜索为中心，不断延展功能场景和服务能力，面向用户信息检索、生成、处理的全域、多元需求，一体化设计产品，一站式提供AI服务。</p><p></p><p>一个“超级搜索框”实现回答、创作、总结之外，夸克一站式提供网盘、扫描、文档、CueMe、学习助手、健康助手等内容产品和智能工具，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e1/01/e118bc8ced6b9979047d830aafd5cc01.png" /></p><p></p><p>用户撰写年中总结PPT，在夸克搜索“年中总结PPT模版”，即可进入AI PPT一键创作，调整PPT内容，最后导出文件，存储和发送。在“超级搜索框”中使用AI写作，用户得到的个性化创作内容，可以保存为Word文档，在夸克网盘编辑、转换格式、分享，告别了多场景反复跳转的割裂体验。</p><p></p><p>此外，在手机、电脑、平板等多个终端上，多端一体的夸克正在逐步构建中，确保用户在不同客户端上都能享受到好用的、高质量的信息服务。</p><p></p><h2>面向用户设计产品，革新性搜索加速迭代</h2><p></p><p></p><p>满足用户最基础、最广泛的信息需求，AI搜索与大语言模型能力的契合已成为行业共识。面向用户创新价值，以下一代搜索为远景目标，夸克AI搜索长期积累了四个方面的能力与资源优势。</p><p></p><p>模型能力方面，夸克大模型去年一经发布即登顶各大性能评测榜，并持续面向用户场景深度迭代，提升性能；搜索能力上，夸克积累了用户理解、内容生态、安全合规等全面能力；数据能力上，夸克多年来在知识、经验、健康、题目等领域拥有海量的优质数据；应用场景方面，夸克长期沉淀了通用搜索和健康、教育、文档等垂直领域的众多场景，且拥有大规模用户群体。</p><p></p><p>“搜索是个生生不息的业务，AI搜索才刚刚开始，夸克AI搜索同样处在全新阶段的开始。”郑嗣寿表示，夸克会加速效果迭代和产品升级，给用户更快更准的搜索体验。他透露，在多模态交互、内容生态建设、多端一体等方面，夸克将进一步加快产品创新节奏，为用户创造无处不在的信息服务价值。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mofQeXC5geHrabzrVVU5</id>
            <title>一行代码价值百万美元：从工程技术角度看云成本优化</title>
            <link>https://www.infoq.cn/article/mofQeXC5geHrabzrVVU5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mofQeXC5geHrabzrVVU5</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jul 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云计算, 软件开发, 成本效率, 工程决策
<br>
<br>
总结: 云计算时代下的软件开发者需要关注成本效率和工程决策，每一个工程决策都是一个购买决策，需要考虑云计算的实际成本投入。软件必须盈利，云计算不仅是简单的计算机，而是一个全新的平台，需要改变传统的编码方式和系统设计思路。工程师在软件盈利能力中扮演重要角色，成本效率反映系统质量，一行代码决定公司盈利。需注意避免烧钱行为，如调试导致高昂费用和API请求造成成本增加。 </div>
                        <hr>
                    
                    <p>没有比现在成为软件开发者更好的时刻，也从来没有哪个时刻可以像现在这样，一个工程师能拥有如此大的影响力，一行代码就能决定一个组织的财务走向。和许多人一样，我一直热衷于开发高效的软件。然而，在以云为中心的世界里，效率不再仅仅关乎性能。我们现在所做的按需计算和基础设施选择都需要实实在在的资金投入，忽视了这一点可能会非常危险。</p><p></p><p></p><h2>每一个工程决策</h2><p></p><p></p><h2>都是一个购买决策</h2><p></p><p></p><p>每一个工程决策都是一个购买决策。相比在云端的花费，人们可能更关注你今天在晚餐或午餐上的花费。财务部门的某些人会盯着那 50 美元的午餐，却没有人盯着你的工程师在云计算上花费的 10000 美元。这很难理解，因为在过去，CTO、CIO 和 CFO 会监督采购流程，而如今，一个初级工程师在采购方面比公司里的任何人都拥有更多的自主权。</p><p></p><p>当今的世界正处于云计算成本时刻。经过多年的大规模增长，人们的关注点已经从不惜一切代价的增长转向了高效、有利可图的增长。有些人在想，也许云计算是个错误，也许它是个骗局。如果我们迁移到了云端，然后发现需要退出，该怎么办？对被云厂商锁定的恐惧导致许多人一只脚仍然踩在数据中心里，而且许多人发现这种方法成本很高。许多人已经发现，提升（lift）和转移（shift）成本极高（这可能是云计算最大的谎言）。这到底是怎么回事？云计算是个骗局吗？不幸的是，正是对云计算及其成本的恐惧，导致了这种云计算浪费的预言成为现实。</p><p></p><p></p><h2>破釜沉舟</h2><p></p><p></p><p>在今天和明天的经济环境中生活的我们，都需要明白，要构建伟大的软件，这些软件必须是盈利的。</p><p></p><p>云计算不只是别人的计算机那么简单，它是一个操作系统，一个全新的平台。就像科尔特斯征服新大陆一样，如果我们想要成功，就必须破釜沉舟，忘掉回家的路。然而，许多人仍在为昨天的大型机编写代码，没有意识到如果要最大限度地利用云计算，就需要重写代码。在 DevOps 运动开始之前，我们会把代码扔给运维人员，然后去解决下一个问题。现在，我们编写代码，然后扔给财务部门，让他们去操心。生活在这些经济体中的所有人都需要明白，我们要构建出色的软件，并且它们必须能够盈利。</p><p></p><p>许多软件仍在等待部署到云端。据估计，目前仍有 4.6 万亿美元的 IT 支出用于数据中心运行。尽管云计算规模在不断增长，但仍处于早期阶段。</p><p></p><p></p><blockquote>我们还有很多事情要弄清楚。如果我们要迁移到云端，它必须具有强大的经济意义。有些人坚信这是不可能的，有些人坚信这完全是个错误。我知道这些人是错的。毕竟我已经在云端，我想继续留在那里。但我也希望在有生之年看到这一天的到来。不幸的是，即使云计算每年以 50% 的速度增长，如果我们不开始用不同的方式构建软件，恐怕我们谁都活不到看到它到来的那一天。</blockquote><p></p><p></p><p>之所以存在这样的讨论，是因为我们在构建软件时还不太清楚云计算是否具有强大的经济意义。</p><p></p><p>我们必须改变这种状况。我已经看到了数据，我可以告诉你云计算具有强大的经济意义。我已经看到了这一点，但你必须用不同的方式构建软件，编写不一样的代码，并以不同的方式思考系统设计。你不能只是将在数据中心中有效运行的东西直接搬到云端，然后期望得到一个好的结果，你必须对此有不一样的思考。</p><p></p><p></p><h2>工程师在软件盈利能力中</h2><p></p><p></p><h2>所扮演的角色</h2><p></p><p></p><p>如今，成本效率常常可以反映出系统的质量。一个架构良好的系统是一个具有成本效益的系统。一行代码就能决定你所工作的公司是否盈利。</p><p></p><p>我们面临着一个共同的挑战，必须找出衡量成本效率的最佳方法。为此，我想深入代码层面。我本质上是一名工程师，所以我收集了一些价值百万美元的代码示例（在某些情况下甚至价值数百万美元）来展示烧钱是一件多么容易的事情。为了保护隐私，所有这些都已匿名化并转换成了 Python 代码。在这些示例中，仅仅几行代码就烧掉了远比他们应该花费的多得多的钱。</p><p></p><p></p><h4>&nbsp;示例 1：因调试而导致的高昂费用（即使是 DevOps 也要花钱）</h4><p></p><p></p><p>在这个示ps 也要花钱例中，一个 AWS Lambda 函数的平均月成本为 628 美元，CloudWatch 的平均月费用为 31000 美元。究竟发生了什么？不幸的是，AWS CloudWatch 的费用远超实际调用 Lambda 函数的情况太常见了。我不知道有多少人经历过这种情况，但感觉任何一个在 AWS 构建无服务器系统的人最终都会遇到这个问题。</p><p></p><p>在这个示例中，仅用于写入日志数据的年度总成本就达到了 110 万美元。造成这种情况的原因是什么？这里有两个导致因素。一些本不应该被发布的代码，却也是曾经非常重要的代码。一行善意的调试代码，当运维团队打开调试日志，并没有多想，然后将大量数据发送到了 CloudWatch。有时候，运维团队与开发团队是脱节的（我知道我们都希望认为 DevOps 总是紧密合作的，但事实并非总是如此），他们假设代码应该按照他们想的那样运行。于是，它运行了很长时间，烧掉了 110 万美元。</p><p></p><p>顺便说一句，如果他们还使用 Datadog 来收集日志，这可能会变得更加昂贵。相比之下，110 万美元算是一笔划算的交易，但无论如何，这同样是悲剧，也是不必要的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0bb8bb286a4428acdc3ebcd7dc8f6693.webp" /></p><p></p><p>有什么办法可以解决这个问题？很简单。去掉调试语句，我们知道这就是问题所在。如果我在电脑上编写示例代码，最后会把调试代码删除，因为到最后我们不需要它们。在开发和测试阶段，这些代码很有用，但部署时不要把它们放进去。它们就像是一个漏洞，一颗等待爆炸的定时炸弹。解决办法就是删除它们。</p><p></p><p></p><h4>示例 2：API 也是要花钱的</h4><p></p><p></p><p>在这个示例中，我们有一个最小可行产品（MVP）进入了生产环境。几年后，这个产品向 S3 发起数十亿次 API 请求，因为是偷偷平稳地增长，以至于没人注意到。这段代码在一年内就烧掉了 130 万美元。</p><p>这段代码存在许多挑战。作为最小可行产品（MVP），它运行得非常完美。一个想法蹦出来，把它写在纸上，然后实现它，交付它。为什么这些东西会在 for 循环里？为什么在运行过程中调用 S3 API？实际上，我们可以把所有这些内容抽离出来，并快速缓存或捕获这些信息。问题是这段代码能正常运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a9a80769b8ec4298ff873f943b7127c.webp" /></p><p></p><p>在部署好后，它运行得很好。直到多年后，当它达到一定规模时，才开始烧掉那 130 万美元。我们还发现了一个小细节。也许我不应该把这些文件传递给后续的函数进行进一步处理。这个问题的解决办法是什么？我们可以把它从 for 循环中抽离出来。提前计算或下载这些内容，一次性做完，而不是在函数里运行一百万次。与其通过传递指针方便后续查找文件，不如直接传递实际的数据。一次性使用——多么简单的操作。再次强调，我们都做过这样的事情。我们让代码跑起来，作为原型来说运行得足够好。然后，它们被悄无声息地交付，我们也没有想太多。API 调用是要花钱的。有时候，在 S3 中，API 调用的成本可能比存储本身还要高。</p><p></p><p></p><h4>示例 3：几字节如何让 DynamoDB 写入成本加倍</h4><p></p><p></p><p>在这个示例中，一位开发人员被要求添加一些简单的功能。我们写入 DynamoDB 的记录没有时间戳，我们想知道它是什么时候写入的。为什么不添加个字段呢？这应该非常简单。修改代码只需一秒钟，有人测试了，然后部署了，现在已经上线并运行了。</p><p></p><p>不久之后看看账单，DynamoDB 的成本翻了一番。这个稍微有点难发现。有人知道为什么添加时间戳的代码会让 DynamoDB 的成本比以前翻了一番吗？DynamoDB 按照 1K 元素为单元进行收费。它写入的是 1000 字节，但我们添加了一个时间戳，一个 9 字节的属性。时间戳是 ISO 格式的，即 32 个字节，加起来是 1041 字节，仅一行代码就使成本翻了一倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b129a2022132112747a7c66d87d5e87.webp" /></p><p></p><p>这个真的很难被发现。我们必须用不同的方式思考数据是如何在线上传输的。更重要的是，这对我们的成本有何影响？这个问题的解决办法是什么？我们应该做两件事。我们应该减小属性名称的大小。这是 TCP/IP 协议的一个基本属性。将其改为“ts”而不是时间戳。这样就可以减掉几个字节。我们重新格式化时间戳，这样就减少到了 20 个字节，还剩余 2 个字节。我们回到了实际需要的水平——一行代码，成本减半。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0af2982c8e495724df71c14d9347368f.webp" /></p><p></p><p></p><h4>示例 4：基础设施代码泄漏（Terraform 版）</h4><p></p><p></p><p>我们不要忘了基础设施即代码，比如 Terraform 和 CloudFormation。在接下来的示例中，我们有一个 Terraform 模板，用于创建自动伸缩组。它可以同时伸缩具有数百甚至数千个 EC2 实例的集群。有人设计了这样一个系统，每 24 小时就回收一次实例。也许是因为存在内存泄漏，他们认为这是一个很好的解决方法。不幸的是，安全部门的人担心这些数据可能是必要的，所以移除了可删除 EBS 卷的选项。这个系统运行了大约一年，慢慢积累了一堆 EBS 卷。在那一年年底，110 万美元付诸东流。这个示例有点冗长，就像大多数基础设施即代码一样，但导致这个问题的是两行代码，分别在两个不同的文件中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/71007324cc7f6b4b2f2a99edf61cd62f.webp" /></p><p></p><p>这两行代码组合每隔 24 小时会为每个创建的 EC2 实例创建一个未连接的 EBS 卷。第一行delete_on_termination被设置为 false，阻止 EBS 卷被删除。第二行max_instance_lifetime是回收时间。因为这两行在不同的文件中，所以很容易被忽略。</p><p></p><p>这两行代码意味着每次 EC2 实例启动都会创建一个 EBS 卷，而这个 EBS 卷永远不会被删除（除非手动删除）。由于自动伸缩组的最大大小为 1000（在这个示例中，在任何给定时刻，这个环境中有 300 到 600 个 EC2 实例），未连接的 EBS 卷的数量迅速增加。一年下来，累计费用超过了一百万美元。然而，解决这个问题稍微会复杂一些。对于这个问题，你必须改变流程。</p><p></p><p>你必须稍微考虑一下你的团队在实现这些东西时是怎么做的。如果创建了资源，就应该知道如何删除它们。这不仅适用于云端的成本，也适用于许多其他的情况。我们中的许多人在过去几年里一直在思考如何扩大规模，但没有足够多地思考如何缩减规模。缩减规模要难得多，也重要得多，它甚至还有可能拯救你的企业。如果你的公司是一家旅游公司，并在经历了新冠疫情后幸存下来，那么一定知道如何缩减规模。我听说过 Expedia 团队的一些了不起的事情，但并非每家公司都那么幸运。要小心那些出于好意的基础设施即代码，特别是当你需要满足来自不同团队的需求时。</p><p></p><p></p><h4>示例 5：网络传输成本</h4><p></p><p></p><p>第五个示例，我把最好的留在最后。我们都喜欢内容分发网络（CDN），它们可以更快地将内容传输给客户，让所有的东西都运行得更快。在最后这个例子中，一家公司在全球部署了 230 万台设备，他们做了一个小小的改动，而这个小改动被部署到了所有设备上。</p><p></p><p>大约 14 小时后，这个改动变成了一个问题。这个问题被修复之前，每小时导致约 4500 美元的损失。虽然最终造成了数十万美元的损失，但这与它可能造成的真正影响相比，简直是小巫见大巫。如果这个改动持续运行一年而没有人注意到——我会解释为什么可能没有人注意到它——它将成为一条价值 3900 万美元的代码行。</p><p></p><p>我确信会有人乞求那笔钱可以回来。希望在第一个月的财务报告中就有人注意到了这个问题。然而，那已经是在损失了 64.8 万美元之后了。这真是一个让人痛苦的 Bug。值得庆幸的是，这个问题在六天后被发现，相比通常情况下需要几个月才会被发现，这也算是一个小小的成功故事了。问题是，这个故事的成功标准实在太低了。</p><p></p><p>那是段怎样的代码？它看起来像这样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f68a43b76f0bfce1e183cfd1490c828.webp" /></p><p></p><p>在这段代码中，有一个出于好意的更新函数，可能是很久以前的一个实习生写的。它原本每天被调用一次，用来下载和比较一个大文件，这看起来像是一个糟糕的主意，所以有人决定改为下载元数据，认为这会更高效。具有讽刺意味的是，这个改动实际上是为了降低成本。他们部署了代码，并期望一切都朝着正确的方向发展。当他们突然发现事情并没有按预期进行时，他们并不确定接下来发生了什么。</p><p></p><p></p><blockquote>有多少人能发现这段代码中的 Bug？</blockquote><p></p><p></p><p>只是一个字符，这个字符的拼写错误让这段代码的执行切换到了成本更高的路径。同时，他们将调用频率从每天一次提高到每小时一次。问题在于，CloudFront 非常乐意提供内容。它做得很好，不仅扩展服务满足需求，还成功分发了内容。在整个过程中，没有系统受到影响，也没有错误被检测到。所有人都很高兴，因为数据流动顺畅且高效。</p><p></p><p>他们的客户可能会好奇，为什么他们家的网络会有这么多额外的数据流动，但因为一切工作正常，这个问题难以被发现。运营团队和后台监控工具没有发现错误，也没有任何警报。Datadog 没有报告任何问题，因为 CloudFront 可以轻松地应对流量的增长。唯一的异常指标是他们现在每小时要花掉 4500 美元，这在之前没有发生过——而这一切都是因为一个字符的拼写错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b0345671d436bfb1f738d620a89cb943.webp" /></p><p></p><p>真正的解决之道是重新反思整个问题。尽管这个问题可以被快速修复，但因为涉及到产品的一个关键方面，所以需要更深层次的方案。此外，这类错误不太容易通过测试捕捉到，在未达到实际规模的情况下，许多问题在测试中是无法被注意到的。这种微不足道的字符拼写错误可能会导致 3900 万美元的账单。</p><p></p><p></p><h2>学到的教训</h2><p></p><p></p><p>我们从中学到了什么？存储成本依然低廉。我们确实可以认为存储成本依然相当低廉。然而，调用 API 是有成本的，这些成本总是存在的。事实上，我们应该接受这样一个事实：在云端进行的任何操作都是有成本的。可能不多，可能是几美分，可能是几分之一 美分，总之是要花钱的。在调用 API 之前，你最好考虑到这一点。云给了我们几乎无限的规模，问题是，我没有可以无限支付的钱包。</p><p></p><p>我们有一个系统设计约束，这个约束在设计、开发和部署过程中似乎没有人关注。这个重要的经验教训是什么？我们是否应该在成为云平台软件开发者的基础上，再增加一层对成本的关注？我已经考虑了很多，再增加一件需要担心的事情似乎是雪上加霜。我们希望所有的工程师都去操心代码的成本问题吗？即使在这个新的云时代，Donald Knuth 的这句名言依然有深远的意义。</p><p></p><p></p><blockquote>"过早优化是万恶之源" —— Donald Knuth</blockquote><p></p><p></p><p>作为工程师，我们首先需要弄清楚的是，这个该死的东西是否能工作？我能解决这个问题吗？</p><p>我分享的所有这些例子在流量达到一定规模之前都不是问题。事实上，只有在你取得成功之后，它们才会成为问题。除非你在正在开发的产品或服务上取得了实质性的进展，否则这些问题不应该成为你的首要关注点。</p><p></p><p>云工程师应该考虑成本，但这应该是一个渐进持续的过程，而不是一次性的决策。</p><p></p><p>首先，我们要回答这个问题：这是否可行？然后，作为团队的一员，这样做对团队来说是对的吗？其他人如何维护我的代码？接下来，如果规模增长了，会发生什么？这个时候你应该开始考虑成本问题。</p><p></p><p>当我开始在云端构建我的第一个系统时，我对成本的概念还相当模糊。我去找 CFO 并说我想用 AWS 做一个项目时，他说，“Erik，你可以做任何你想做的，但你有 3000 美元的预算，不要一下子花光。”这已经是很久以前的事情了，那时候云计算还是一个新兴领域。我知道如果我能把项目的成本控制在 3000 美元以内，我就可以在这里尽兴地探索。所以，我开始对如何最大化投资回报产生了浓厚兴趣。这种对效率的追求得到了回报，因为我成功地将成本控制在预算之内，并自此一直在云计算领域深耕。所有人都应该这样吗？我们是否应该给每个工程师一个预算？我们更希望赋予工程师的，不仅仅是一个数字，因为一个数字可能意味着每天花费相当于一辆兰博基尼的价格，这听起来既抽象又难以理解。相反，我更希望每个人都能专注于提高效率。</p><p></p><p></p><h2>云效率比率</h2><p></p><p></p><p>为此，我想介绍一个叫做云效率比率（Cloud Efficiency Ratio，CER）的概念。这个概念既简单又直观，旨在引导你在恰当的时机才开始考虑成本优化，避免过早地陷入成本削减的泥潭。你可以通过用收入减去云成本再除以收入来计算云效率比率百分比。例如，假设你的公司每年收入 1 亿美元，云成本是 2000 万美元，那么你每收入一美元就花掉 20 美分，你的云效率比率是 80%。这个比率很棒，但你不一定需要在一开始就达到这个水平。</p><p></p><p>你应该将云效率比率视为一种合理化成本的非功能性需求。对于任何云项目，你应该让产品团队或业务部门在项目开始时定义期望的云效率比率，以及在应用程序生命周期的哪个阶段应该做出调整。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9ce7b96e214e86f0010050f0e539dd86.webp" /></p><p></p><p>记住，在研发阶段，你的主要目标是验证项目是否可行。你的 CER 可能是负的，甚至在某些情况下就应该是负的。如果你在产品发布之前就开始赚钱，那就有问题了。然而，一旦达到了 MVP 阶段，就应该开始努力实现盈亏平衡。这个阶段的 CER 低是可以接受的，它应该在 0% 到 25% 之间，因为你的目标是试图找到产品与市场的契合点。等到人们对你的产品开始感兴趣，并想要买你的产品，你就可以开始设想如果产品变得流行起来会怎样，这时的 CER 就该达到 25% 到 50%。</p><p></p><p>在你扩大产品规模，且人们在抢着购买你的产品时，你需要制定一条明确的路径，以实现健康的盈利水平，你可能想达到 50% 到 70% 的 CER。随着业务逐步进入稳定状态，如果你想拥有一个健康的业务，并成为组织的利润引擎，那么就要努力达到 80% 的 CER。利用云效率率作为目标，你可以将那些抽象的美元成本转化为具体的目标，用以指导你的云项目。这个指标可以贯穿你的整个云平台，或者具体到某个客户、功能、服务或任何新项目。作为经验法则，我建议将目标 CER 设定为 80% 左右。</p><p></p><p></p><h2>结&nbsp; &nbsp;论</h2><p></p><p></p><p>在为撰写本文做准备的过程中，我意外地发现了计算机科学领域的杰出人物 Tony Hoare。一些人认为他推动了“过早优化是万恶之源”这句名言的传播。我发现的东西让我大吃一惊。事实证明，Tony 在多年前就提出了这个概念。早在 2009 年的伦敦 QCon 大会上，他就讲述了一个价值十亿美元的教训。在演讲中，他回顾了 1965 年他发明空引用的决策，并认为这一早期的错误可能导致全球经济损失高达数十亿美元。他的观点可能并无夸张。</p><p></p><p>所有的工程师都知道，随着时间的推移，他们的代码会自成一体。它会流转到其他人手中，并继续演化，我们对它们的掌控会越来越弱。在编写代码时，我们考虑的成本可能只有几分几毛，但当这些代码在某个地方部署并运行时，每年的运行成本可能高达 100 万美元。因此，当你重新审视你的代码库时，请认真思考这一问题。愿你在这一过程中不会有任何令人沮丧的发现。</p><p></p><p>最后，我将以这句话结束：</p><p></p><p></p><blockquote>“每一个工程决策都是一个购买决策。”Erik Peterson —— CloudZero 联合创始人兼 CTO</blockquote><p></p><p></p><p>在今天，当你敲下每一行代码，都是在做一个购买决策。你在你的组织、这个经济体和这个云驱动的世界中扮演着非常重要的角色。希望你能明智地运用这种力量。</p><p></p><p>查看英文原文：</p><p></p><p>https://www.infoq.com/articles/cost-optimization-engineering-perspective/</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dUU64vuc7JlmyheyVXud</id>
            <title>千帆AppBuilder重构企业AI原生应用开发体验 | 对话AI原生《云智实验室》</title>
            <link>https://www.infoq.cn/article/dUU64vuc7JlmyheyVXud</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dUU64vuc7JlmyheyVXud</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 10:32:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 大模型, 应用开发, 千帆AppBuilder
<br>
<br>
总结: 人工智能和大模型是当前科技创新的关键力量，百度智能云的千帆AppBuilder支持企业级AI原生应用开发，重构了应用开发体验，提供了三步构建方法论。在大模型时代，应用开发经历了翻天覆地的变化，从传统的开发方式转变为自顶向下的快速构建原型。千帆AppBuilder在产业级场景应用开发中提供了灵活的自动编排能力和优质组件，持续提升服务企业的能力，拓展大模型应用的边界。 </div>
                        <hr>
                    
                    <p>人工智能和大模型正在引领当前最重要的科技创新趋势。在过去的一年中，行业关注点已从大模型研发转向实际应用，正成为推动创新和转型的关键力量。百度智能云千帆AppBuilder作为基于大模型的企业级AI原生应用开发工作台，支持应用的快速开发和发布，以零代码、低代码、代码态三种模式，满足各类开发者的不同应用开发需求。</p><p></p><p>截至目前，百度智能云千帆大模型平台服务客户数已突破15万，开发了55万个大模型应用。那么，AI原生应用的开发逻辑与传统应用开发有何不同？千帆AppBuilder如何拆解应用开发过程，其开发思路秘诀又是什么？产业级AI原生应用开发有哪些技术难点？</p><p></p><p>百度智能云对话AI原生——云智实验室系列栏目的第二期，邀请到百度主任研发架构师董大祥，围绕产业级AI原生应用开发话题，通过讨论与实操结合的方式，在线展示千帆AppBuilder如何重构企业AI原生应用开发体验，满足各类开发需求，帮助企业将AI应用的创意和构想变为现实。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/d3/67/d30797467c1b2097af6b6cf63c524567.png" /></p><p>点击链接收看：《千帆AppBuilder重构企业AI原生应用开发体验》</p><p><a href="https://www.infoq.cn/video/xNwUJxFxEmuNzzvI7Y6j">千帆AppBuilder重构企业AI原生应用开发体验_百度_王世昕_InfoQ精选视频</a>"</p><p></p><p></p><h2>大模型时代应用开发新范式</h2><p></p><p></p><p>在大模型时代，AI原生应用在产品形态上显著变化，主要归功于大语言模型的理解、生成、逻辑和记忆四大核心能力。应用交互从传统的表单和按钮，转变为自然语言对话，这一转变也引发了开发方式的变革。</p><p></p><p>传统应用开发流程复杂，需要先进行高层设计，再分解为子模块开发，最后集成到一起，这一过程耗时且风险极高。在大模型技术加持下开发者可以借助AppBuilder平台通过自然语言快速构建一个从0到1的应用原型，随后逐步丰富和完善功能，这意味着应用开发发生了翻天覆地的变化。</p><p></p><p>总结来说，过去应用开发需要大量时间和人力投入，而现在依靠大语言模型能力，可以自顶向下快速构建原型，重塑了应用开发体验。</p><p></p><p></p><h2>三步构建AI原生应用方法论</h2><p></p><p></p><p>在构建AI原生应用的过程中，平台采用自顶向下的开发模式，先将模糊的想法转化为具体的应用配置方案，然后逐步丰富和完善功能。这种方法可以让开发者在短时间内快速搭建出符合需求的应用，实现从创意到应用的高效转变。具体来说，平台形成了一套三步构建方法论，包含以下三个关键步骤：</p><p></p><p>创意描述：一句话描述你的创意。创意拆解：将创意拆解为思考模型和组件两类工作。创意实现：选择合适的工具，通过自然语言描述思考模式，实现组件。</p><p></p><p></p><h2>千帆AppBuilder产业级场景应用开发</h2><p></p><p></p><p>在实际使用应用开发平台进行产业级应用开发过程中，客户往往会面临更复杂的开发需求并对平台的效果和性能有更高的要求，例如需要多个组件连续自动调用才能实现指令需求，海量私有数据需要解析和企业开发中对组件能力的高需求等，千帆AppBuilder对这些都提供了针对性解决方案。</p><p></p><p>高效灵活的自动编排能力：平台底层模型能够精准理解用户的角色指令和对各种组件的描述，并能连续自动调用多个组件，灵活地自动编排任务。</p><p></p><p>例如创建一个旅游助手，会用到百度搜索、文生图、TTS（语音合成）三个组件能力。在创建应用角色指令阶段就会描述当用户询问信息是人的时候，需通过百度搜索组件检索人相关信息，并绘制画像。当用户询问景点信息，需使用百度搜索提炼和总结景点特色信息，并且用音频方式播报。这一系列描述需要平台及底层模型紧密配合，理解实际使用场景才能完整触发，并且任务都由两个组件连续自动调用才能完成。</p><p></p><p>私有文档集成和企业级检索增强：平台具备海量文档的管理、合理的文档解析、版面分析、知识增强以及语义检索和召回功能。企业开发者可以利用自己的知识库来回答实际场景中的问题，降低模型可能产生的误解或错误。</p><p></p><p>开放易用的优质组件：平台提供了百度搜索、代码解释器、文生图、文件转换器、网页内容理解和长文档内容理解等60多个功能强大的官方组件。开发者可以像搭建乐高积木一样，灵活组合组件并且自定义组件，满足各类灵活定制开发需求。</p><p></p><p></p><h2>AI原生应用开发前景</h2><p></p><p></p><p>千帆AppBuilder在大模型企业落地场景中，持续提升服务企业的能力，扩展大模型应用的边界。近期，在WAIC期间举办的“大模型助力新质生产力发展论坛”上，千帆AppBuilder全新升级企业级知识检索增强(RAG），扩展性、开放性、安全性显著提升；全新发布“RAG X 百度搜索”功能，将百度搜索在时效性、客观性方面的优势，与RAG在私域知识响应、语言灵活性方面的优势进行能力互补，大幅提升用户知识检索体验；发布千帆AppBuilder私有化版本，推动大模型在政务、工业、交通、金融等行业的落地。</p><p></p><p>短短几个月，技术团队实现千帆AppBuilder在工作流编排、企业级RAG等功能的数次迭代，持续将大模型的强大能力落地到各行各业。智能既是科技的进步，也是理念的变革，千帆AppBuilder将携手客户、伙伴并进，继续迎接未来的无限可能。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</id>
            <title>微软放弃在 OpenAI 董事会的观察员席位</title>
            <link>https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 09:03:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, OpenAI, 人工智能, 监管审查
<br>
<br>
总结: 微软放弃在 OpenAI 董事会的观察员席位，欧盟和美国对生成人工智能进行监管审查，微软可能面临反垄断调查，欧盟委员会正在调查大型数字市场参与者与生成式人工智能开发商和提供商之间的协议，微软已向 OpenAI 投入数十亿美元成为推动 AI 模型发展的领导者。 </div>
                        <hr>
                    
                    <p>当地时间7月10日，据外媒报道，微软表示，在欧洲和美国对生成人工智能进行监管审查之际，它将放弃在 OpenAI 董事会的观察员席位。</p><p>&nbsp;</p><p>微软副法律总顾问 Keith Dolliver 于周二晚些时候致信 OpenAI，称该职位提供了对董事会活动的深入了解，同时又不损害其独立性。</p><p>&nbsp;</p><p>但 CNBC 注意到，这封信的内容还透露，微软不再需要这个席位，因为“新成立的董事会取得了重大进展”。</p><p>&nbsp;</p><p>欧盟委员会此前表示，微软可能面临反垄断调查，因为它关注的是虚拟世界和<a href="https://aicon.infoq.cn/2024/shanghai/presentation/6042">生成人工智能</a>"市场。欧盟委员会是欧盟的执行机构，该委员会今年 1 月份表示，正在“调查大型数字市场参与者与生成式人工智能开发商和提供商之间达成的一些协议”，并将微软与 OpenAI 的合作列为其将要研究的一项具体交易。</p><p>&nbsp;</p><p>欧盟此后得出结论，观察员席位不会改变 OpenAI 的独立性，但欧盟监管机构正在寻求第三方对该交易的更多意见。英国竞争与市场管理局仍然心存疑虑。</p><p>&nbsp;</p><p>去年 11 月，在 OpenAI 首席执行官<a href="https://www.infoq.cn/article/HgI7G6Oth4C6PS4bsqR7?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> Sam Altman 被解雇</a>"、随后又迅速重新受雇的动荡时期之后，微软在 OpenAI 董事会中占据了一个无投票权的席位，以平息有关微软对该初创公司的兴趣的一些疑问。</p><p>&nbsp;</p><p>Altman 当时在给员工的一份说明中表示，<a href="https://www.infoq.cn/article/GdkidIFChUxVslICMamx?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI</a>"“显然做出了与微软合作的正确选择，我很高兴我们的新董事会将把他们列为无投票权的观察员”。</p><p>&nbsp;</p><p>OpenAI 于 2022 年底发布 ChatGPT 聊天机器人后，成为全球最热门的初创公司之一。微软已向&nbsp;这家初创公司投入了数十亿美元，据报道，迄今为止其总投资额&nbsp;已增至 130 亿美元。鉴于其对 OpenAI 的投资和合作，这家科技巨头实际上已成为推动基础 <a href="https://aicon.infoq.cn/2024/shanghai/track/1710">AI 模型</a>"发展的领导者。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z5jt2oaAFjE9SvgORK6J</id>
            <title>《中国开发者画像洞察研究报告 2024》报告发布：鸿蒙生态存在百万级人才缺口，潜在新就业岗位超过300万个</title>
            <link>https://www.infoq.cn/article/Z5jt2oaAFjE9SvgORK6J</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z5jt2oaAFjE9SvgORK6J</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 08:08:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 开发者, 数字经济, 鸿蒙
<br>
<br>
总结: 人工智能与各行业深度融合，推动创新变革，但也加剧全球对AI专业人才需求，现有人才储备难以满足市场需求，开发者在数字经济中扮演重要角色，鸿蒙生态发展迅速，带来新的就业机会和高薪酬待遇。 </div>
                        <hr>
                    
                    <p>人工智能正以前所未有的速度与各行各业深度融合，推动着各个领域的创新与变革，但同时也加剧了全球范围内对 AI 专业人才的需求，而现有人才储备难以满足市场需求，人才供需缺口日益明显。</p><p></p><p>在 AI 人才如此紧缺的形势之下，为了更好地理解在 AI 时代下的开发者群体，为年轻人们带来更具有实际意义的参考，并迭代《中国开发者画像洞察研究报告 2022》的研究结果，极客邦科技双数研究院 InfoQ 研究中心发起了本次《中国开发者画像洞察研究报告 2024》（以下简称《报告》）的研发和撰写计划。</p><p></p><p>本次报告调研联合了十余家企业和社区，通过收集数千份问卷，对多个企业机构进行访谈，分析了开发者的行为模式、工作价值、职业发展等内容，旨在为行业提供一份全面、深入的研究报告。通过这份报告，我们将更直观和深入地看到开发者群体的真实面貌，理解他们的价值追求，认识到他们在数字经济中的核心地位。</p><p></p><p></p><h2>一、数字经济加速壮大开发者群体：新兴技术领域职位需求已接近百万大关</h2><p></p><p></p><p>根据“十四五”数字经济发展规划，到 2025 年，数字经济核心产业增加值占 GDP 比重目标将达到 10%。数字化创新引领发展能力大幅提升，智能化水平明显增强，数字技术与实体经济融合取得显著成效。“十四五”规划为数字经济的未来发展指明了方向，越来越多的传统企业开始意识到数字化转型的重要性，并积极投身于科技浪潮之中，而这一趋势也为开发者提供了更多的职业机会。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d9a72dbaf015f73c0904bce738c25c0.webp" /></p><p></p><p>截至 2023 年年底，我国整体开发者人数已攀升至 2067.21 万，增速达到 2.5%。技术人才队伍的蓬勃发展和创新活力，对于开发者队伍的壮大和人才结构的优化具有重要意义，也印证了在数字经济时代，开发者群体将发挥越来越重要的作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffbf1fc18741fa3dae90572446b2bc5d.webp" /></p><p></p><p>在新兴技术领域，职位需求的激增尤为显著。目前 AI 发展已经进入以人工通用智能（AGI）为主导的行业应用拓广阶段，应用创新的涌现促使供需两端企业对技术人才的需求日益增长。2024 年多项国家政策强调了建设智能算力中心的重要性，并提出了进一步深化开放合作，发挥跨央企协同创新平台作用。在这一背景下，政府、企业和研究机构纷纷开始行动，积极迎接人工智能带来的深刻变革。基础设施的升级和企业机构拥抱“AI+”战略使得 AI 市场对人才需求出现快速增长，AI 领域的供需两端企业开始积极释放对人才的强烈需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/54f8971d9ef924f79836b851f32816c5.webp" /></p><p></p><p>此外，HarmonyOS+ 盘古大模型将为医疗、教育、家居、制造和商务等众多垂直领域带来新机遇。国产基础软件的广泛应用也将激发出更多的创业灵感和新项目，这些新项目的启动和新生态的构建将为社会创造更多的就业机会和经济增长点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdd14a1c0abd5d360e5dd0b0a48e3dcb.webp" /></p><p></p><p></p><h2>二、开发者就业新蓝海：鸿蒙相关职位薪资涨幅超预期</h2><p></p><p></p><p>软件是数字中国建设的关键支撑，特别是以 HarmonyOS 操作系统为代表的国产基础软件，处在信息产业上下游生态的枢纽位置，在信息系统中起着基础性、平台性作用，对保障信息安全也非常重要。近日，华为公司发布了 HarmonyOS 的最新进展：鸿蒙生态设备突破 9 亿台，吸引超过 254 万开发者；从操作系统内核、文件系统到编程语言、人工智能框架和大模型，全部实现自研。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/edaa0cb9fb8c92322d1261d5181a6bff.webp" /></p><p></p><p>鸿蒙生态的快速发展和原生应用数量的爆炸性增长，加上信息技术迅猛发展、软件业稳步向前等宏观环境的积极影响，催生了大量的鸿蒙工作岗位需求，为就业市场注入了一股新的活力和趋势。目前 5000 款应用已完成原生鸿蒙开发，未来的目标是支持 50 万款应用，这为开发者创造了百万级人才缺口，潜在新就业岗位超过 300 万个，鸿蒙生态亟需更多开发者投入应用开发、参与生态建设。在 InfoQ 研究中心发起的调研中，32.3% 的开发者表示其非常有意愿未来转向或继续从事鸿蒙开发岗位，这彰显了鸿蒙生态的新技术新系统对开发者的强吸引力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62be8a2c3f8a450866a09c0cf4767e63.webp" /></p><p></p><p>《报告》显示，鸿蒙开发岗收入尤为突出，获得超出预期收入的鸿蒙开发者数量是其他开发者的将近两倍。同时，鸿蒙岗位也为开发者带来了技术层面的成长机会和解决问题的能力，四成左右鸿蒙开发者表示获得了技术成长，比例高于其他开发者。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf23bd1c318cff1c4bf3441cbf54414a.webp" /></p><p></p><p>根据《报告》显示，随着工作年限的上升，人均月薪出现分水岭，特别是近年转为鸿蒙开发岗位的资深开发者，能够获得比整体新紧缺岗位更高的薪资；据公开数据统计，2024 年鸿蒙开发者平均薪资涨幅 43.1%，而安卓开发平均薪资涨幅 22.7%。鸿蒙生态的快速发展正在吸引越来越多开发者的关注，并成为就业市场的一个新热点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19e0d598612ef20cf56da38d54f5d978.webp" /></p><p></p><p>高薪酬待遇的背后，是企业对这些岗位所寄予的厚望和对人才的高度重视。企业需要能够推动技术创新、提升产品竞争力的人才。这些人才不仅需要深厚的技术功底，还要有创新思维和解决复杂问题的能力。因此，企业愿意为他们提供更高的薪资，以吸引和留住这些宝贵的人才资源。</p><p></p><p>此外，高校作为人才培养的摇篮，在鸿蒙人才培养方面同样扮演着重要角色。高校与企业合作产教融合类项目，不仅注重理论知识的传授，还通过实习实训类活动，将鸿蒙技术纳入相关专业课程体系，培养学生的鸿蒙技术应用能力和创新精神。</p><p></p><p>借助三方机构的赋能，能够为开发者提供多样化的鸿蒙技术培训课程，包括线上、线下课程及短期集训等，以满足不同学员的学习需求。同时，三方赋能机构与鸿蒙官方及企业紧密合作，通过项目实战和案例研究等活动，为开发者学员提供更多实践机会和职业发展空间。</p><p></p><p></p><h2>三、多元化成长 ：半数以上开发者选择付费学习</h2><p></p><p></p><p>目前，开发者主要就职于互联网上市公司，近半数未来开发者毕业后计划进入该类型公司。在未来短期至中期内，互联网上市公司将继续成为开发者最青睐的企业类型。随着 AGI 的蓬勃发展，小而美的独角兽企业价值凸显，尤其吸引了未来开发者的关注。新领域新技术的发展同样带动基础研究高质量发展，选择进入高校研究院工作的未来开发者规模比现有开发者规模高 8.8%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd94c20392febea8d051ba89b7245a27.webp" /></p><p></p><p>在技术日新月异的今天，每一位开发者的成长路径不再是单一的线性发展，而是一个多元化、立体化的过程。《报告》中提到，开发者面对的挑战集中在技术能力提升、工作价值和工作量三个方面，其中技术能力提升是最大挑战。这一挑战的主要源于外部环境的变化，开发者需要不断提升技能水平以适应日新月异的技术创新。</p><p></p><p>《报告》显示 37.3% 的开发者日均工作时长超过 9 小时，其中 6.7% 的开发者日均工作时长超过 12 小时，开发者加班现象普遍存在。除了工作内容多、工作时间长等工作量因素，日均工作 9 小时以上的开发者加班原因还包括对职业发展的焦虑或者对公司发展的焦虑，需要边工作边精进技术能力或尝试使用新技术解决问题，从而获得更强的职场竞争力。</p><p></p><p>开发者的成长不仅依赖于技术能力的提升，还依赖于持续的学习和进阶。根据《报告》我们可以看到，近九成的有工作经验的开发者日常有学习习惯，其中 55.7% 愿意进行付费学习。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2350413c931a511d44252974fdade0a.webp" /></p><p></p><p>《报告》中提到，技术语言、技术实战和计算机基础是开发者最主要学习的三类内容，其中技术实战是最受青睐的严肃学习内容类型。此外，针对产品运营和面试相关内容，付费开发者比例也显著高于非付费开发者比例，且开发者更愿意花时间学习这两类内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62510c4195725511c06776259c60f42f.webp" /></p><p></p><p>此外，资深高薪开发者掌握各项计算机工具产品的比例大幅高于整体开发者，其中 Kafka、Hadoop 和 Elasticsearch 在高薪开发者中的掌握率尤为突出。掌握 Java、python、SQL 和 JavaScript 成为多数资深高薪开发者的“必会技能”。</p><p></p><p>这种学习习惯的形成，既是个人职业发展的需求，也是技术发展迅速的必然结果。开发者们深知，只有不断学习，才能跟上技术发展的步伐，才能在竞争激烈的职场中保持自己的竞争力。</p><p></p><p></p><h2>四、未来机遇无限：属于开发者的全新时代</h2><p></p><p></p><p>随着技术的不断演进和市场需求的日益多样化，开发者群体正站在一个全新的历史起点上。面向未来，他们展现出更加开放的姿态，积极拥抱新技术和新系统的发展。这种开放性不仅体现在对技术的学习与应用上，更体现在对新思想、新理念的接受与融合上。</p><p></p><p>面对不断变化的市场需求，开发者们需要持续地挑战自我，突破舒适区，通过不断地学习、交流和合作，才能提升自己的技术水平，才能在企业和社会的发展中发挥更大的作用。虽然道路充满挑战，但通过不懈的努力和正确的学习路径，将会有更多的开发者在数字化时代中实现自我价值的最大化，并开创属于自己的新时代。 </p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RikCP108qlJyt3169rcY</id>
            <title>奇富科技推出智能语音模型Qifusion，语音识别准确率可达93%以上</title>
            <link>https://www.infoq.cn/article/RikCP108qlJyt3169rcY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RikCP108qlJyt3169rcY</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 03:08:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 奇富科技, Qifusion-Net, 方言识别, 语音识别技术
<br>
<br>
总结: 近日，奇富科技智能语音团队的论文《Qifusion-Net：基于特征融合的流式/非流式端到端语音识别框架》被全球语音与声学顶级会议INTERSPEECH 2024收录。该框架模型在方言种类丰富、方言识别准确和高效方面取得了显著成果，提升了语音识别技术的准确性和智能化水平，解决了方言识别中的一系列问题，对金融服务领域等业务场景具有重要意义。 </div>
                        <hr>
                    
                    <p></p><p>近日，奇富科技智能语音团队论文《Qifusion-Net：基于特征融合的流式/非流式端到端语音识别框架》(Qifusion-Net:&nbsp;Layer-adapted&nbsp;Stream/Non-stream&nbsp;Model&nbsp;for&nbsp;End-to-End&nbsp;Multi-Accent&nbsp;Speech&nbsp;Recognition)被全球语音与声学顶级会议INTERSPEECH&nbsp;2024收录。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4c0efef72dd7b539ade947a461827d98.png" /></p><p></p><p>我国地域广阔，方言种类繁多，其语法和语音特征存在显著差异。同时，由于噪声的干扰、方言的混杂现象、主观感知在标注过程中的偏差，以及人力标注工作的复杂性和系统性不足，语音识别技术的准确性和智能化水平受到了一定程度的限制。</p><p></p><p>在金融服务领域，现有的通用语音识别技术在处理方言时往往难以达到理想的效果，不仅影响了人机交互的准确性和智能化水平，也对服务的效率和质量产生了负面影响。</p><p></p><p>奇富科技引入了全自研Qifusion框架模型，并将其集成到智能营销及贷后提醒等业务场景中。在应用上，Qifusion框架模型能够提升智能营销、贷后提醒、风险控制业务应用场景识别准确率，帮助解决以上问题。并且在复杂的通话环境中，Qifusion的语音识别准确率达到了93%以上，意图识别准确率超过95%。</p><p></p><p>方言种类丰富：凭借丰富的数据样本，Qifusion框架模型在原有东北官话、胶辽官话、北京官话、冀鲁官话、中原官话、江淮官话、兰银官话和西南官话等国内八种主流方言的基础上，强化了四川、重庆、山东、河南、贵州、广东、吉林、辽宁、黑龙江等用户密集地区的方言识别能力。</p><p></p><p>方言识别准确：Qifusion框架模型具备自动识别不同口音的能力，并能在时间维度上对解码结果进行口音信息修正，使方言口音的语音识别误差率降低了30%以上，整体语音识别字错率降低了16%以上，提升了用户体验。</p><p></p><p>方言识别高效：Qifusion框架采用了创新的层自适应融合结构，能通过共享信息编码模块，更高效的提取方言信息。同时，该框架模型还支持即说即译功能，能在无需知晓额外方言信息的前提下，对不同方言口音的音频进行实时解码，实现精准的识别和转译。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b05a2911fa57421bd8d3c34ade8d911c.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c2fea73ea78c8371511990973</id>
            <title>为何我们决定从零开始创建 NGINX Gateway Fabric</title>
            <link>https://www.infoq.cn/article/c2fea73ea78c8371511990973</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c2fea73ea78c8371511990973</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 02:12:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: NGINX, Kubernetes, Gateway API, NGINX Gateway Fabric
<br>
<br>
总结: NGINX决定推出自己的Gateway API项目，而不是将Gateway API强塞到现有Ingress产品中。他们认为这样做会限制未来发展，并希望打造一款经得起时间考验的产品，满足未来需求。他们的目标是为Gateway API资源提供全面配置互操作性，规范服务网络的许多组件，摒弃厂商特定的CRD，助力创造更加美好的未来。 </div>
                        <hr>
                    
                    <p></p><blockquote>原文作者：Brian Ehlert - F5 产品管理总监，Matthew Yacobucci - F5 首席软件工程师原文链接：<a href="https://www.nginx-cn.net/blog/why-we-decided-to-start-fresh-with-our-nginx-gateway-fabric/">为何我们决定从零开始创建 NGINX Gateway Fabric</a>"转载来源：<a href="https://www.nginx-cn.net/">NGINX 中文官网</a>"</blockquote><p></p><p></p><p>NGINX 唯一中文官方社区 ，尽在&nbsp;<a href="https://www.nginx.org.cn/">nginx.org.cn</a>"</p><p>  </p><p>在 Kubernetes&nbsp;<a href="https://www.nginx-cn.net/resources/glossary/kubernetes-ingress-controller/">Ingress controllers</a>"&nbsp;领域，NGINX 取得了巨大成功。<a href="https://docs.nginx.com/nginx-ingress-controller/">NGINX Ingress Controller&nbsp;</a>"被广泛部署用于商业 Kubernetes 生产用例，同时也作为开源版进行开发和维护。因此，您可能会想当然地认为，当 Kubernetes 网络（<a href="https://gateway-api.sigs.k8s.io/">Gateway API</a>"）获得重大改进时，我们会再进一步，将其部署到我们的现有 Ingress 产品中。</p><p></p><p>然而，我们选择了一条不同的道路。通过评估全新 Gateway API 的强大潜能以及是否有可能彻底重塑 Kubernetes 中的互联处理方式，我们意识到将 Gateway API 实现强塞到现有 Ingress 产品中会限制未来发展。</p><p></p><p>因此，我们决定推出自己的 Gateway API 项目 —&nbsp;<a href="https://github.com/nginxinc/nginx-gateway-fabric">NGINX Gateway Fabric</a>"。该项目是开源项目，将以透明、协作的方式运行。我们很高兴与外部贡献者合作，并乐于分享最新成果，共同推动创新，造福于整个社区和行业。</p><p> </p><p></p><h2>我们为何决定推出自己的 Gateway API 项目</h2><p></p><p>尽管我们满怀激情和信心做出了围绕 Gateway API 创建一个全新项目的决定，但这一决定也基于合理的业务和产品战略逻辑。</p><p></p><p>想必 Kubernetes 采用者已经对 NGINX Ingress Controller 的开源版和商业版有所了解。两者都部署了经过严格测试的 NGINX 数据平面（在&nbsp;<a href="https://www.nginx-cn.net/products/nginx/">NGINX Plus</a>"&nbsp;和 NGINX 开源版反向代理中运行）。在 Kubernetes 之前，NGINX 的数据平面已在负载均衡和反向代理用例中有不凡表现。在 Kubernetes 中，我们的 Ingress controller 可完成相同类型的关键请求路由和应用交付任务。</p><p></p><p>NGINX 因轻量级、高性能、久经考验并可满足严苛环境要求的商业产品而享负盛名。我们在 Kubernetes Ingress controller 领域的产品战略与我们的反向代理产品策略相一致，即为简单用例提供强大的开源产品，并为关键业务应用环境中的生产 Ingress 控制提供具有更多特性和功能的商业产品。这一策略在 Ingress controller 领域中行之有效，部分原因是过去 Ingress controller 缺乏标准化，需要大量自定义资源定义（CRD）来提供负载均衡和反向代理高级功能，开发人员和架构师在 Kubernetes 以外的网络产品中享用这些高级功能。</p><p></p><p>我们的客户依赖并信任 NGINX Ingress Controller，其商业版本已经具备了 Gateway API 旨在提供的许多关键高级功能。此外，NGINX 很早就参与了 Gateway API 项目，并认识到 Gateway API 生态系统要达到完全成熟还需要几年的时间。（事实上，Gateway API 的许多规范都在不断演进，例如 GAMMA 规范，该规范有助于更好地集成 service mesh（服务网格）。）</p><p></p><p>但我们认为，将测试级 Gateway API 规范强塞到 NGINX Ingress Controller 中会给成熟的企业级 Ingress controller 带来无谓的不确定性和复杂性。我们销售的任何产品都必须具有稳定性和可靠性，并完全符合生产就绪要求。Gateway API 解决方案也会做到这一点，只是目前仍处于起步阶段。</p><p></p><p></p><h2>我们的 NGINX Gateway Fabric 目标</h2><p></p><p>对于 NGINX Gateway Fabric，我们的主要目标是打造一款经得起时间考验的产品，就像 NGINX Plus 和 NGINX 开源版一样。为了确保我们的 Gateway API 项目能够“满足未来需求”，我们意识到需要为其数据平面和控制平面尝试不同的架构选择。例如，我们可能需要研究不同的方法来管理四层和七层互联或尽量减少外部依赖项。我们最好从零开始，而不沿袭任何历史先例和遵从任何要求。虽然我们正在使用业经试用和测试的 NGINX 数据平面作为 NGINX Gateway Fabric 的基础组件，但对新想法也持开放态度。</p><p></p><p>我们还希望为 Gateway API 资源提供不受厂商限制的全面配置互操作性。与现有 Kubernetes Ingress 范式相比，Gateway API 最大的改进之一就是规范了服务网络的许多组件。从理论上讲，这种标准化可支持许多 Gateway API 资源轻松地进行交互和连接，助力创造更加美好的未来。</p><p></p><p>不过，创造这一未来的关键是摒弃厂商特定的 CRD（可能导致厂商锁定）。对于必须支持专为 Ingress controller 领域而设计的 CRD 的混合产品而言，这可能极具挑战性。而在以互操作性为第一要务的开源项目中，做到这一点则相对容易些。为了摒弃紧密关联的 CRD，我们需要构建一个新框架，仅关注 Gateway API 及其组成 API 所暴露的新层面。</p><p></p><p></p><h2>加入我们的 Gateway API 之旅</h2><p></p><p>目前，我们仍处于早期发展阶段。只有少数项目和产品实施了 Gateway API 规范，其中大多数都选择将其融入现有项目和产品中。</p><p></p><p>因此，现在是启动新项目的最佳时机。我们的 NGINX Gateway Fabric 项目完全开放，决策和项目管理高度透明。因为该项目使用<a href="https://go.dev/">&nbsp;Go&nbsp;</a>"编写而成，所以我们诚邀广大 Gopher 社区成员建言献策、提交 PR。</p><p>Gateway API 可能会改变整个 Kubernetes 世界。一些产品可能不再需要，新的产品或将出现。Gateway API 蕴藏着无限可能，虽然不知道它终将走向何方，但我们翘首以待。诚邀您加入我们，共创精彩未来！</p><p></p><p>您可以首先：</p><p>以贡献者的身份加入项目在实验室中试用实现执行测试并提供反馈</p><p>如欲加入该项目，请访问 GitHub 上的<a href="https://github.com/nginxinc/nginx-gateway-fabric">&nbsp;NGINX Gateway Fabric</a>"。</p><p></p><p>NGINX 唯一中文官方社区 ，尽在&nbsp;<a href="https://www.nginx.org.cn/">nginx.org.cn</a>"</p><p>更多 NGINX 相关的技术干货、互动问答、系列课程、活动资源：&nbsp;<a href="https://www.nginx.org.cn/">开源社区官网</a>"&nbsp;|&nbsp;<a href="https://mp.weixin.qq.com/s/XVE5yvDbmJtpV2alsIFwJg">微信公众号</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TFNLwiRlMsZUNjiZMrwR</id>
            <title>大模型风口下，卷应用才有价值！首期 AIGC 实践案例集锦上线啦 （免费下载）</title>
            <link>https://www.infoq.cn/article/TFNLwiRlMsZUNjiZMrwR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TFNLwiRlMsZUNjiZMrwR</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 01:55:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 大模型, 应用实践, 技术探索
<br>
<br>
总结: 2024年过半，生成式AI已从最初的技术探索过渡到应用实践阶段。各大AI公司和科技大厂都在展示他们的最新AI成果，并将焦点放在大模型的产业应用上。大模型赛道竞争激烈，市场需求和实际落地挑战加速涌现。业内关注大模型将颠覆哪些行业、何时出现杀手级大模型应用、中国大模型技术阶段、面临的瓶颈和如何在内卷环境中赚钱等实际问题。 </div>
                        <hr>
                    
                    <p>2024&nbsp;年过半，生成式&nbsp;AI&nbsp;已从最初的技术探索过渡到应用实践阶段。无论是国外的OpenAI、谷歌、苹果等头部&nbsp;AI&nbsp;公司，还是国内的百度、阿里云、字节跳动、腾讯等科技大厂，以及百川智能、零一万物等&nbsp;AI&nbsp;独角兽都在积极展示他们的最新&nbsp;AI&nbsp;成果，并不约而同地将焦点放在了大模型的产业应用上。</p><p></p><p>与此同时，大模型赛道越来越卷，各大厂商开始从“重在参与”转向激烈的价格战比拼。从通用型到行业定制化，从云端部署到端侧应用，大模型技术的市场需求和实际落地的挑战在加速涌现。</p><p></p><p>大模型将颠覆哪些行业？杀手级大模型应用何时出现？中国大模型技术处于什么阶段？面临着哪些瓶颈？如何在内卷的环境里赚到钱？这些务实的问题成为了业内关注的焦点。</p><p></p><p>今年上半年，我们精选出过往超&nbsp;20&nbsp;篇&nbsp;AIGC&nbsp;领域文章并集结成<a href="https://www.infoq.cn/minibook/HfXFv4RaAEyPUFk5HAfJ">《大模型领航者&nbsp;AIGC&nbsp;实践案例集锦》（第一期）</a>"供读者下载，包括<a href="https://qcon.infoq.cn/2024/beijing">&nbsp;QCon&nbsp;全球软件开发大会（北京站）&nbsp;</a>"2024&nbsp;和<a href="https://aicon.infoq.cn/2024/beijing">&nbsp;AICon&nbsp;全球人工智能开发与应用大会（北京站）2024&nbsp;</a>"的热门演讲，以及对钉钉、面壁智能、数势科技、腾讯、京东、字节跳动、百川智能、云知声、零一万物、达观数据等企业的独家采访。</p><p>（文末附下载）</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d9b8a4607a66019e8afafe31f637edc.jpeg" /></p><p></p><p></p><p>本册电子书共包括“大咖视野”、“观点碰撞”、“应用案例”、“技术实践”与“AI&nbsp;测评室”五个部分，从权威视角、技术实战、终端应用等多个角度深入呈现了企业在大模型落地中面临的挑战和宝贵经验。</p><p></p><p>其中，“应用案例”又涵盖互联网产品、企业生产提效、医疗、教育以及更多垂直行业的章节内容，分别从不同行业的视角展现了大模型应用的无限潜力；“AI&nbsp;测评室”则囊括了今年各热门大模型的实测。</p><p></p><p>欢迎下载阅读，与我们一同深入&nbsp;AIGC&nbsp;“前线”，探索那些前沿的技术视野与成功团队的奋斗故事。</p><p></p><h2>目录</h2><p></p><p></p><h3>大咖视野&nbsp;|&nbsp;Vision</h3><p></p><p>01&nbsp;|&nbsp;钉钉卡位战：SaaS&nbsp;挣不到的钱，Agent&nbsp;会挣到</p><p>02&nbsp;|&nbsp;26&nbsp;岁带着百人团队冲刺大模型，面壁智能天才CTO：高效比参数更重要</p><p>03&nbsp;|&nbsp;这个离开大厂去&nbsp;AI&nbsp;创业的互联网大佬，带着他的“Killer&nbsp;Agent”来了</p><p></p><h3>观点碰撞&nbsp;|&nbsp;Opinion</h3><p></p><p>04&nbsp;|&nbsp;大模型开闭源争吵不休：开源落后闭源一年，决定模型能力的不是技术？</p><p>05&nbsp;|“国外一开源，国内就创新”！面对中美大模型差异，我们该突破还是继续模仿？</p><p></p><h3>应用案例&nbsp;|&nbsp;Cases&nbsp;</h3><p></p><p></p><h4>第一章：互联网产品</h4><p></p><p>06&nbsp;|&nbsp;如何1秒内快速总结100多页文档？QQ&nbsp;浏览器首次揭秘大模型实现技术细节</p><p>07&nbsp;|&nbsp;京东商家智能助手：Multi-Agents&nbsp;在电商垂域的探索与创新</p><p></p><h4>第二章：企业生产提效</h4><p></p><p>08&nbsp;|&nbsp;字节跳动代码生成&nbsp;Copilot&nbsp;产品的应用和演进</p><p>09&nbsp;|&nbsp;大语言模型加持，是智能运维架构的未来吗？</p><p>10&nbsp;|&nbsp;用&nbsp;AI&nbsp;面试员工的企业，知道打工人在想什么吗？！</p><p>11&nbsp;|&nbsp;AI&nbsp;代码助手革新编程界：腾讯云专家汪晟杰深度剖析机遇与挑战</p><p></p><h4>第三章：垂直行业</h4><p></p><p>12&nbsp;|&nbsp;巨头们涌入的医疗大模型，何时迎来最好的商业时代？</p><p>13&nbsp;|&nbsp;AI&nbsp;老师的强大功能&nbsp;+&nbsp;真人老师的情感交流&nbsp;=&nbsp;未来教育？</p><p>14&nbsp;|&nbsp;4人团队，如何用大模型创造近千万业务价值？</p><p></p><h3>技术实践&nbsp;|&nbsp;Technology&nbsp;</h3><p></p><p></p><h4>第一章：大模型训练与推理</h4><p></p><p>15&nbsp;|&nbsp;万字干货！手把手教你如何训练超大规模集群下的大语言模型</p><p>16&nbsp;|&nbsp;当大模型推理遇到算力瓶颈，如何进行工程优化？</p><p>17&nbsp;|&nbsp;AI&nbsp;辅助内部研发效率提升，昇腾大模型推理的最佳实践</p><p></p><h4>第二章：RAG&nbsp;与智能体落地</h4><p></p><p>18&nbsp;|&nbsp;智能体技术发展趋势：谈大模型智能体与开放领域融合</p><p>19&nbsp;|&nbsp;Agent&nbsp;还没出圈，落地先有了“阻力”：进入平台期，智力能否独立担事？</p><p>20&nbsp;|&nbsp;“驯服”不受控的大模型，要搞定哪些事?</p><p></p><h3>AI&nbsp;测评室&nbsp;|&nbsp;Evaluation</h3><p></p><p>21&nbsp;|&nbsp;算数不行、还不懂中国文化，大模型现在抢不了设计师的饭碗！&nbsp;</p><p>22&nbsp;|&nbsp;首届大模型“相亲大会”开始啦！谁是你的天选CP？</p><p>23&nbsp;|&nbsp;Kimi的词+Suno的曲：能带我入选《中国新说唱》，但还是干不过原神！</p><p></p><p></p><h2>下载通道</h2><p></p><p>AIGC技术正以惊人的速度重塑着创新的边界，InfoQ&nbsp;首期《大模型领航者AIGC实践案例集锦》电子书，收录了2024&nbsp;年上半年&nbsp;InfoQ&nbsp;发布的代表性大模型应用案例，全面覆盖从行业权威分析、技术深度解析到应用实践测评的多个维度，希望帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。关注「AI前线」，回复「领航者」免费获取电子书。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a1c00053f97d109f3deea2a8ebdbc1b.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cf4168df71c3561883262bdc6</id>
            <title>What's new in PikiwiDB(Pika) v4.0.0</title>
            <link>https://www.infoq.cn/article/cf4168df71c3561883262bdc6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cf4168df71c3561883262bdc6</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 11:51:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PikiwiDB, Floyd, RocksDB, Column-Family
<br>
<br>
总结: PikiwiDB 社区宣布 Pika v4.0.0 正式发布，新版本基于第三代存储引擎 Floyd，支持多种数据结构和强类型 key，提高了系统效率和数据质量。新增了 Mget 批量查询缓存和主从复制优化，提升了查询性能和复制效果。 </div>
                        <hr>
                    
                    <p>PikiwiDB 社区荣耀地宣告 —— 经过 9 个月打磨并在生产环境稳定运行 5 个月的 PikiwiDB (Pika) v4.0.0 【下文简称 Pika】今天正式发布。希望基于第三代存储引擎 Floyd 的这个新版本能为社区用户们带来更卓越的体验。</p><p></p><h1>1 重大改进</h1><p></p><p></p><h2>1.1 第三代存储引擎 Floyd</h2><p></p><p>Floyd 如同其前代 Blackwidow，基于 RocksDB，不仅支持基础的 String 结构，也原生支持了 Hash、List、Set、Stream 及 ZSet 等 KKV 形式的复合数据结构。</p><p></p><p>RocksDB 实例数可配置</p><p></p><p>摒弃了 Blackwidow 按数据类型采用 RocksDB 实例的物理隔离模式，Floyd 采用了 RocksDB 的 Column-Family 虚拟隔离机制，在单个 RocksDB 实例下可存储所有类型的数据。用户可自由设定 Pika 实例中每个 DB【等同于 Redis DB】中 RocksDB 实例的数量，而数据的存储则依据 key 的 hash 值分配至相应的 RocksDB 实例，减小了数据的空间放大和读放大效应，实现了机器资源的高效利用。</p><p></p><p>强类型 key</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e964b56b3e007ac9074426e7011236a.png" /></p><p></p><p>基于 RocksDB 的 Column-Family 虚拟隔离机制，Floyd 把所有类型的 key 和 string 一起存储在 Column-Family 0。在此存储基础之上，不同于 Blackwidow，可明确禁止不同类型的 key 重复【强类型】，这一设计旨在杜绝潜在的数据冗余与不一致性，与 Redis 服务特性保持一致，进一步提升了系统的整体效率与数据质量。</p><p></p><p>Pika v2.x 系列版本基于存储引擎 Nemo，v3.x 系列版本基于 Blackwidow，它们因为采用了物理隔离机制，无法低成本实现强类型 key，所有在 Redis TYPE 命令的结果中可能返回多种类型，而 Floyd 则完全兼容 Redis 只返回一种类型。</p><p></p><p>Floyd 详细说明</p><p></p><p>如果对 Floyd 存储引擎感兴趣，请详阅《Floyd 存储引擎》【链接：https://github.com/OpenAtomFoundation/pika/discussions/2052】。由于 Floyd 前后进行了多个版本的迭代，所以阅读该 github discussion 文档时请注意前后时间，如有相关冲突性说法，以最新日期的文字为准。</p><p></p><p>关键 PR：PikiwiDB(Pika) 支持 Floyd 存储引擎https://github.com/OpenAtomFoundation/pika/pull/2413添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669Pika 不支持不同类型的重复 key, 写入重复 key 返回非法类型https://github.com/OpenAtomFoundation/pika/pull/2609对 HyperLogLog 和 String 进行类型隔离，确保 HyperLogLog 操作与 String 操作明确区分开https://github.com/OpenAtomFoundation/pika/pull/2720添加支持分区索引过滤的功能https://github.com/OpenAtomFoundation/pika/pull/2601</p><p></p><h2>1.2 Mget 批量查询缓存</h2><p></p><p>Pika v3.5.2 的热数据缓存只实现了对热点 Key 的点查 (如 get/hget)，在后续的 v3.5.3 和 v3.5.4 修复若干 bug 后，对热数据的点查目前已经非常稳定。然而并未支持批量查询 (如 mget etc)。</p><p></p><p>内部业务侧反馈批量查询速度比较慢，在 40C/256GiB/2TiB SATA SSD 规格机器上数据量超过 100GiB 时，Pika v3.3.6 30% 批量查询延迟超过 35ms。但由于 Pika 热数据缓存尚未支持批量查询，性能并未改善。</p><p></p><p>为了满足业务需求，Pika 团队开发了批量查询热数据缓存功能，显著提升了批量查询性能，降低了查询延迟和失败率。相关技术细节请阅读《PikiwiDB (Pika) 混合存储之批量查询》 【链接：https://mp.weixin.qq.com/s/KFLPruSdB66TMRxUfR9PbQ 】。</p><p></p><p>关键 PR ：Mget 支持多 key 查询缓存，记录未命中的 key 去 DB 中查询，提升 Pika 服务的读性能https://github.com/OpenAtomFoundation/pika/pull/2675修复 Mget 没有使用解析 ttl 的函数导致出现部分 key 的 ttl 未被更新，数据不一致的问题https://github.com/OpenAtomFoundation/pika/pull/2730</p><p></p><h2>1.3 主从复制</h2><p></p><p>Pika v3.3.6 有很多主从复制的缺陷。v4.0.0 版本对 Pika 全量复制及增量复制进行了大量优化和 bug 修复，取得了非常好的效果。</p><p></p><p>并在 info 命令中输出了 "repl_connect_status" 指标 (PR 2638)，以方便用户更加明确清晰的确定当前的主从复制状态。</p><p></p><p>关键 PR ：</p><p></p><p>修复批量扩容时，多个 slave 同时连接 master, 短时间多次 bgsave 导致部分从节点数据不完整的问题https://github.com/OpenAtomFoundation/pika/pull/2746修复 Spop 在写 binlog 时可能会出现竞态问题https://github.com/OpenAtomFoundation/pika/pull/2647修复多 DB 下全量同步超时后不重试的问题https://github.com/OpenAtomFoundation/pika/pull/2667修复多 DB 主从超时场景下，可能会出现窗口崩溃的问题https://github.com/OpenAtomFoundation/pika/pull/2666修复主从同步限速逻辑中重复解锁的问题https://github.com/OpenAtomFoundation/pika/pull/2657重构主从复制模式 slave 节点的主从同步线程模型，尽可能减少 binlog 消费阻塞问题https://github.com/OpenAtomFoundation/pika/pull/2638</p><p></p><h2>1.4 Redis Stream</h2><p></p><p>Redis Stream 类似于消息队列（MQ），以便更安全地传递消息。为了确保数据的安全性，底层引擎 BlackWidow 和 Floyd 中特别添加了对 Stream 数据类型的支持。</p><p></p><p>关键 PR：修复 pkpatternmatchdel 命令使用错误导致的 stream 类型数据删除异常的问题https://github.com/OpenAtomFoundation/pika/pull/2726修复 Keyspace 命令未计算 Stream 类型数据的问题https://github.com/OpenAtomFoundation/pika/pull/2705</p><p></p><h2>1.5 Compaction</h2><p></p><p>PikiwiDB (Pika) 的底层磁盘存储引擎 RocksDB 在进行 compaction 时会显著影响 PikiwiDB (Pika) 的读写性能。因此，控制好 compaction 是优化 Pika 读写性能的关键。</p><p></p><p>Floyd 使用了 v8.7.3 版本的 RocksDB，开放了更多 RocksDB 参数，以方便用户优化 RocksDB 性能：</p><p></p><p>enable-partitioned-index-filters： 支持加载分区索引过滤器，加快 RocksDB 查找速度。min-write-buffer-number-to-merge: 默认值为 1，如果将此值设置得更大，意味着需要更多的写缓冲区被填满后才进行 flush。这样可以减少 flush 的频率，增加数据在内存中的累积量，从而可能提高写入吞吐量。level0-stop-writes-trigger: 默认值为 36，定义了 L0 层中 sst 文件的最大数量，一旦达到这个数量，RocksDB 将会采取 暂停写入、强制 compaction 等措施来防止写入操作继续累积，以避免 L0 层变得过于庞大，进而可能导致写入放大、查询性能下降等问题。level0-slowdown-writes-trigger：默认值为 20，用于控制当 Level 0 的 SST 文件数量达到这个阈值时，触发写减速（write slowdown），防止 Level 0 的文件数量过多，导致后续 compaction 操作的压力过大。level0-file-num-compaction-trigger：默认值为 4，当 Level 0 的 SST 文件数量达到这个参数设定的阈值时，RocksDB 会开始执行 compaction 操作，将 Level 0 的文件合并到 Level 1，以减少 Level 0 的文件数量，降低读取延迟，并优化存储空间的利用率。max-subcompactions：默认值为 1，用于控制 RocksDB 中并发执行的 sub-compaction 任务数量，其值为 1 表示关闭 sub-compaction。如果系统资源充足，建议提升该参数以优化 compaction 效率。max-bytes-for-level-base：指定了 L1 SST 文件总的大小。这个大小是 RocksDB 进行数据分层管理和 compaction 决策的重要依据：如果 L1 层的大小设置得太小，可能会导致 L0 层的 compaction 过于频繁，进而影响写性能。反之，如果设置得太大，可能会占用较多的磁盘空间，并且影响读取性能，因为读取操作可能需要跨越更多的层级。Pika 没有在 pika.conf 中开放此参数给用户配置，而是使用其他参数（level0-file-num-compaction-trigger 和 write-buffer-size）计算后的结果。</p><p></p><p><code lang="text">storage_options_.options.max_bytes_for_level_base = g_pika_conf-&gt;level0_file_num_compaction_trigger() * g_pika_conf-&gt;write_buffer_size()
</code></p><p></p><p>关键 PR：添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669添加支持分区索引过滤的功能https://github.com/OpenAtomFoundation/pika/pull/2601新增 RocksDB Compaction 策略动态调整参数，用户可以根据业务调整 Compaction 策略，降低 Compaction 操作对服务性能的损耗https://github.com/OpenAtomFoundation/pika/pull/2538</p><p></p><h2>1.6 可观测性</h2><p></p><p>v3.5 版本增加了包括命中率、每秒命中次数、Redis Cache 内存使用量、Redis Cache 个数、Redis Cache DB 个数 等指标，但是在集群方面的可观测性是缺失的。v4.0.0 对 Codis-Proxy 的 P99、P999、延迟等监控指标进行采集和展示，可以直观地反映线上 Codis-proxy 的运行情况。</p><p></p><p>v4.0.0 开始还提供新的工具：根据 pika benchmark 工具压测结果自动生成可视化的统计图表。</p><p></p><p>关键 PR：Codis 支持 info 命令，可以通过该命令查询 Codis-proxy 的 info 信息https://github.com/OpenAtomFoundation/pika/pull/2688Codis-proxy 新增 P99 P95 等监控耗时指标https://github.com/OpenAtomFoundation/pika/pull/2668添加 Pika 压测指标，提升 Pika 压测效率，并输出可视化的统计图表https://github.com/OpenAtomFoundation/pika/pull/2663</p><p></p><h2>1.7 测试集</h2><p></p><p>PikiwiDB(Pika) 测试集由 gtest 单测、Redis TCL 测试集和 Go 测试集组成。v4.0.0 中丰富了诸多特性的 go test 功能，并进一步完善了基本数据类型的 TCL 测试。</p><p></p><p>关键 PR：添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669Pika Geo 数据类型增加 TCL 测试，并修复测试过程中遇到的缺陷https://github.com/OpenAtomFoundation/pika/pull/2753</p><p></p><h2>1.8 跨平台</h2><p></p><p>PikiwiDB(Pika) 以往仅支持 centos 和 ubuntu 等 linux 平台，v3.5 开始支持 Mac 等平台。v4.0.0 将对 Mac 平台的支持扩展至 FreeBSD 平台。</p><p></p><p>关键 PR：Pika 支持在 FreeBSD14 平台上进行编译https://github.com/OpenAtomFoundation/pika/pull/2711</p><p></p><h1>2 改进列表</h1><p></p><p>下面详细列出了本次发版的主要功能升级和改进。</p><p></p><h2>2.1 新特性</h2><p></p><p>Pika Geo 数据类型增加 TCL 测试，并修复测试过程中遇到的缺陷https://github.com/OpenAtomFoundation/pika/pull/2753Pika 支持在 FreeBSD14 平台上进行编译打包https://github.com/OpenAtomFoundation/pika/pull/2711Pika 线程整理，避免启动过多无用线程，对不同的线程进行命名，方便问题定位https://github.com/OpenAtomFoundation/pika/pull/2697Mget 支持多 key 查询缓存，记录未命中的 key 去 DB 中查询，提升 Pika 服务的读性能https://github.com/OpenAtomFoundation/pika/pull/2675Codis 支持 info 命令，可以通过该命令查询 Codis-proxy 的 info 信息https://github.com/OpenAtomFoundation/pika/pull/2688添加 Floyd 的 compaction-filter 的 Gtesthttps://github.com/OpenAtomFoundation/pika/pull/2669Codis-proxy 新增 P99 P95 等监控耗时指标https://github.com/OpenAtomFoundation/pika/pull/2668添加 Pika 压测指标，提升 Pika 压测效率，并输出可视化的统计图表https://github.com/OpenAtomFoundation/pika/pull/2663Pika 主从复制新增监控指标 repl_connect_status, 可以更加明确清晰的确定当前的主从复制的状态https://github.com/OpenAtomFoundation/pika/pull/2638Pika 不支持不同类型的重复 key, 写入重复 key 返回非法类型https://github.com/OpenAtomFoundation/pika/pull/2609添加支持分区索引过滤的功能https://github.com/OpenAtomFoundation/pika/pull/2601Pika 支持第三代存储引擎 Floyd, 通过支持多 rocksdb 实例、对 Blob 的使用进行优化、对过期数据的清理进行优化，提升了 Pika 实例的读写性能https://github.com/OpenAtomFoundation/pika/pull/2413</p><p></p><h2>2.2 bug 修复</h2><p></p><p>修复 iter 未被析构，导致 pkpatternmatchdel 在返回之前不会删除 iter，这可能会导致 rocksdb 永远引用一个版本，导致数据不符合预期的问题https://github.com/OpenAtomFoundation/pika/pull/2785修复 config 参数 min-blob-size 带单位时解析错误的问题https://github.com/OpenAtomFoundation/pika/pull/2767修复 zverank 返回值异常的问题https://github.com/OpenAtomFoundation/pika/pull/2673修复 Pika-port 传输数据过程中报错的问题https://github.com/OpenAtomFoundation/pika/pull/2758修复因为堆上分配的缓冲区越界导致 Dbsize 命令运行时崩溃的问题https://github.com/OpenAtomFoundation/pika/pull/2749修复批量扩容时，多个 slave 同时连接 master, 短时间多次 bgsave 导致部分从节点数据不完整的问题https://github.com/OpenAtomFoundation/pika/pull/2746修复参数未初始化导致 slotsscan 等命令不能和 bgsave 命令相互制衡的问题https://github.com/OpenAtomFoundation/pika/pull/2745修复 Slotmigrate 迁移数据的过程中，返回值设置错误，异常场景下会终止数据迁移的问题https://github.com/OpenAtomFoundation/pika/pull/2741修复 Mget 没有使用解析 ttl 的函数导致出现部分 key 的 ttl 未被更新，数据不一致的问题https://github.com/OpenAtomFoundation/pika/pull/2730修复 pkpatternmatchdel 命令使用错误导致的 stream 类型数据删除异常的问题https://github.com/OpenAtomFoundation/pika/pull/2726修复 pkpatternmatchdel 不能正确删除掉对应的 keys 的问题https://github.com/OpenAtomFoundation/pika/pull/2717修复 ACL 密码验证错误问题https://github.com/OpenAtomFoundation/pika/pull/2714修复 Keyspace 命令未计算 Stream 类型数据的问题https://github.com/OpenAtomFoundation/pika/pull/2705对部分命令定制化处理逻辑，避免写 binlog 导致从节点的 binlog 解析失败的问题https://github.com/OpenAtomFoundation/pika/pull/2793修复 Pika cmdID 赋值在 Cmd 初始函数中，可能会导致并发构造的时候出现内存泄漏的问题https://github.com/OpenAtomFoundation/pika/pull/2692修复 ExpectedStale 未考虑 String 类型，如果存在已经过期的 String 类型的 key, ExpectedStale 会返回错误的问题https://github.com/OpenAtomFoundation/pika/pull/2682修复 Spop 在写 binlog 时可能会出现竞态问题https://github.com/OpenAtomFoundation/pika/pull/2674db instance 设置不合理时，给用户错误提示https://github.com/OpenAtomFoundation/pika/pull/2672修复 server_stat 中的数据竞态问题https://github.com/OpenAtomFoundation/pika/pull/2671修复多 DB 下全量同步超时后不重试的问题https://github.com/OpenAtomFoundation/pika/pull/2667修复多 DB 下全量同步超时后不重试的问题https://github.com/OpenAtomFoundation/pika/pull/2666修复主从同步限速逻辑中重复解锁的问题https://github.com/OpenAtomFoundation/pika/pull/2657发版支持自动打包 centos7 和 centos8 平台的二进制编译包https://github.com/OpenAtomFoundation/pika/pull/2535修复 Codis 侧的 getrange 命令没有返回预期结果的问题https://github.com/OpenAtomFoundation/pika/pull/2510</p><p></p><h2>2.3 提升改进项</h2><p></p><p>更新 Pika Docker Readme, 可以按照 Readme 在 Docker 中部署 Pika 服务https://github.com/OpenAtomFoundation/pika/pull/2743优化重复查询 meta value 导致影响 Pika 服务读写性能的问题https://github.com/OpenAtomFoundation/pika/pull/2735支持对更多的 RocksDB 参数进行动态调整，用户根据不同的业务使用场景调整参数提升 Pika 的读写性能https://github.com/OpenAtomFoundation/pika/pull/2728对 HyperLogLog 和 String 进行类型隔离，确保 HyperLogLog 操作与 String 操作明确区分开https://github.com/OpenAtomFoundation/pika/pull/2720更新了 PR 标题验证，不允许在标题末尾出现中文字符https://github.com/OpenAtomFoundation/pika/pull/2718重构主从复制模式 slave 节点的主从同步线程模型，尽可能减少 binlog 消费阻塞问题https://github.com/OpenAtomFoundation/pika/pull/2638新增 RocksDB Compaction 策略动态调整参数，用户可以根据业务调整 Compaction 策略，降低 Compaction 操作对服务性能的损耗https://github.com/OpenAtomFoundation/pika/pull/2538</p><p></p><h2>2.4 发版 tag</h2><p></p><p>​    https://github.com/OpenAtomFoundation/pika/releases/tag/v4.0.0</p><p></p><h1>3 社区</h1><p></p><p>感谢所有为 v4.0.0 做出贡献的社区成员，包括 issue/PR 提交者、代码 reviewer 【排名不分先后，依据字母序列】：</p><p></p><p>AlexStocksbaerwangchejingecheniujhchienguoguangkun123gukj-spellongfar-ncylqxhubluky116Mixficsolsaz97wangshao1</p><p></p><p>PikiwiDB (Pika) 开源社区热烈欢迎您的参与和支持。如果您有任何问题、意见或建议，请扫码添加 PikiwiDB 小助手【微信号: PikiwiDB】为好友，它会拉您加入官方微信群。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pF38uV7MiVDO8hjyWEsz</id>
            <title>徐少春受邀出席世界人工智能大会，AI引领财务管理新世界</title>
            <link>https://www.infoq.cn/article/pF38uV7MiVDO8hjyWEsz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pF38uV7MiVDO8hjyWEsz</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 06:57:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金蝶集团, 人工智能大会, 智能财务, AI技术
<br>
<br>
总结: 金蝶集团董事会主席兼CEO徐少春在2024世界人工智能大会上分享了关于智能财务管理的主题演讲，强调AI技术对财务管理的影响和变化，同时强调人类的良知无法被AI替代。会议还发布了中国企业财务智能化白皮书，探讨了人工智能技术在财务领域的发展。金蝶集团致力于利用AI技术推动财务管理的数字化转型，提供智能财务解决方案，以引领财务管理新世界的发展。 </div>
                        <hr>
                    
                    <p>7月6日，金蝶集团董事会主席兼CEO徐少春受邀出席2024世界人工智能大会（WAIC），并在“智能财务”论坛发表主旨演讲《AI时代财务管理的变与不变》，分享了金蝶结合30多年财务数字化经验和数百万家客户实践总结的“企业财务管理框架”，并指出AI技术的发展正引领财务管理新世界，但人类的良知永远不可能被AI替代。</p><p></p><p>据悉，本届大会主题为“以共商促共享 以善治促善智”，而以“新质经济 智慧财务”为主题的2024WAIC“智能财务”论坛，重点研讨了人工智能技术在财务领域的理论与实践发展，现场还发布了2024年中国企业财务智能化白皮书以及举办了智能财务开放生态联盟成立仪式。</p><p>&nbsp;</p><p>随着人工智能、机器学习、大数据分析等技术的快速发展以及在财会领域的成功试水，财务数智化转型已成为企业转型最重要的方向之一。徐少春在演讲中表示，财务管理应划分为“作战”、“记录”和“支撑”三层体系，而AI技术的发展，为企业财务管理框架及其内容带来了巨大的影响和变化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/dbc6fbe420c724d49526b6904c3500ce.png" /></p><p>金蝶集团董事会主席兼CEO 徐少春</p><p></p><p>“其中，基于财务管理框架，财务管理价值模型从陀螺型向沙漏型转变；AI使财务人员在计划与控制领域从只靠经验预测转变为精准预测，让财务管理信息从数据专享转变到信息普惠、专家服务从个人精英转变到AI天团、外部报告的重点从财务指标转变到发展能力评价，企业也从传统的‘财务信息系统’升级到‘AI+财务’智能平台；同时，财务人员的思维需要从AI‘观望者’转变成为‘拥抱者’，以算法和模型为核心，来驱动决策，来进行自适应的优化和持续创新。”徐少春指出，AI为财务管理的众多领域带来了变化，而在变局中，人类的良知永远不可能被AI替代，要以良知为定海神针，以AI为万变利器，共商共建共享一个美好的财务管理新世界。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f34c66dffad55ce3e99ce393e125c3c.png" /></p><p>随后，金蝶中国执行副总裁、大型企业事业部总裁赵燕锡现场出席了《2024年中国企业财务智能化调查报告（白皮书）》发布仪式，及智能财务开放生态联盟成立仪式。</p><p><img src="https://static001.geekbang.org/infoq/30/30e74c7998318b5babc7da239a8b1c91.png" /></p><p>2024年中国企业财务智能化调查报告（白皮书）正式发布</p><p>（合影左三为金蝶中国执行副总裁、大型企业事业部总裁赵燕锡）</p><p></p><p>金蝶作为上海国家会计学院智能财务研究院成员单位，深度参与“2024 中国企业财务智能化现状调查”，深入剖析了中国企业财务智能化应用现状、发展趋势、问题挑战及建议。此外，金蝶参与建立智能财务开放生态联盟，希望以人工智能为引擎、数字化为引领，构建开放、合作、共赢的智能财务生态体系，促进智能财务领域最新技术的研究和应用，推动企业财务管理的数智化转型。</p><p>&nbsp;</p><p>人工智能是新一轮科技革命和产业变革的重要驱动力量。作为企业管理软件与云服务行业的领军者，金蝶对AI在企业管理场景下的应用更是早早开始积极探索与布局。</p><p>&nbsp;</p><p>2018年，金蝶与上国会等单位共同发起并成立智能财务研究中心（智能财务研究院前身），积极研究智能化技术在财务管理领域的应用场景。2022年，金蝶推出全球首个EBC企业管理领域的数字员工。2023年，金蝶又重磅发布中国首个财务大模型，可提供专业的分析、审核、预测、专家支持、报告生成、解读等服务，加速企业财务管理智能化跃升。</p><p>&nbsp;</p><p>今年以来，金蝶基于“AI优先”战略，将金蝶云·苍穹重构为新一代企业级AI平台，并重磅推出超级智能的AI管理助手——Cosmic，覆盖财务、人力、供应链等多种业务场景，支持业务发起、多模态智能审核以及财务指标查询和分析等智能财务功能。和传统AI产品相比，金蝶提供具备“大模型+财务”等垂直领域的真实落地实践，已经帮助金茂集团、温氏集团、厦门建发集团实现多种智能财务应用功能。</p><p>&nbsp;</p><p>深耕财务领域多年，金蝶一直致力于构建世界一流的财务管理：从国内第一款Windows财务软件到中国首个财务大模型，再到超级智能的AI管理助手Cosmic，金蝶始终以自主创新AI技术，引领财务管理数字化转型。未来，金蝶将利用AI技术帮助企业构建新质生产力，提升使用体验，改善运营效率，让每个员工都拥有一个超级智能的AI财务管理助手，打造财务管理新世界</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5cisJ5uchbX4TWq4mwBA</id>
            <title>DeepMind发布JEST算法，AI模型训练耗能降低十倍</title>
            <link>https://www.infoq.cn/article/5cisJ5uchbX4TWq4mwBA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5cisJ5uchbX4TWq4mwBA</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:40:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, 人工智能, JEST算法, 多模态对比学习
<br>
<br>
总结: 近日，谷歌的人工智能研究实验室DeepMind发布了关于训练AI模型的新研究——多模态对比学习与联合示例选择(JEST)。这项研究提出的JEST算法可以显著提高训练速度和能源效率，超越了传统模型的训练方法，通过多模态对比学习和联合示例选择优化数据的整体学习效果，为人工智能技术的发展带来重要的突破。 </div>
                        <hr>
                    
                    <p></p><p>近日，谷歌的人工智能研究实验室DeepMind发表了关于训练AI模型的新研究——多模态对比学习与联合示例选择(JEST)。</p><p></p><p>JEST算法可以将训练速度和能源效率提高一个数量级。DeepMind&nbsp;声称，“我们的方法超越了最先进的模型，迭代次数减少了&nbsp;13倍，计算量减少了10倍。”</p><p></p><p>论文链接：</p><p><a href="https://arxiv.org/pdf/2406.17711">https://arxiv.org/pdf/2406.17711</a>"</p><p></p><p>有网友激动地表示：“我没想到它来得这么快。对于模型来说，选择训练数据的能力是很强大的，因为这可以使得训练变得十分容易。你不需要再去猜测什么是高质量的训练数据，因为你有一个专门学习它的模型。”</p><p></p><p>JEST算法以一种简单的方式打破了传统的AI模型训练技术。典型的训练方法侧重于对单个数据点的学习和训练，而JEST则是对整个批次进行训练，优化了数据的整体学习效果。</p><p></p><p>多模态对比学习能够直接揭示数据之间的交互，通过选择高质量的子批次显著提高训练效率。</p><p></p><p>多模态数据交互：利用不同模态（图像、文本等）间的相互作用增强数据的表征力。例如，将图像中的对象与其描述文本相匹配，增强模型的理解。</p><p></p><p>对比目标：最大化相同概念的不同模态表示（如图像和对应文本）之间的相似度，同时最小化不相关模态之间的相似度。通过sigmoid-contrastive&nbsp;loss等对比损失函数实现。</p><p></p><p>学习效率的提升：多模态学习方法使JEST算法从数据交互中学习到更复杂的数据表示，提高了学习效率和模型性能。</p><p></p><p>联合示例选择通过评估数据子批次的整体可学习性，从大批次中选择出最有学习价值的子批次。</p><p></p><p>可学习性评分：结合当前模型的损失和预训练模型的损失，优先选择当前模型尚未学会但预训练模型已学会的数据。</p><p></p><p>评分函数：结合预训练模型的易学性评分和当前学习模型的难学性评分，得到综合的可学习性评分。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/9505f369e41090693633db33a2acbe62.png" /></p><p></p><p></p><p>但是，这个系统完全依赖于其训练数据的质量，如果没有高质量的数据集，引导技术就会分崩离析。对于业余爱好者或者业余AI开发者来说，JEST比其他方法要更难以掌控。</p><p></p><p>近年来，人工智能技术迅猛发展，大规模语言模型（LLM）如ChatGPT的应用日益广泛。然而，这些模型的训练和运行消耗了大量能源。研究称，微软用水量从2021年到22年飙升了34%，ChatGPT每处理5-50个提示就会消耗接近半升水。在这样的背景下，JEST技术的出现显得尤为重要。</p><p></p><p>参考链接：</p><p><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/google-claims-new-ai-training-tech-is-13-times-faster-and-10-times-more-power-efficient-deepminds-new-jest-optimizes-training-data-for-massive-gains">https://www.tomshardware.com/tech-industry/artificial-intelligence/google-claims-new-ai-training-tech-is-13-times-faster-and-10-times-more-power-efficient-deepminds-new-jest-optimizes-training-data-for-massive-gains</a>"</p><p><a href="https://the-decoder.com/google-deepminds-jest-speeds-up-ai-training-by-13x-while-slashing-computing-needs/">https://the-decoder.com/google-deepminds-jest-speeds-up-ai-training-by-13x-while-slashing-computing-needs/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/31135104e35b1d06aff17fe83</id>
            <title>百度Feed业务数仓建模实践</title>
            <link>https://www.infoq.cn/article/31135104e35b1d06aff17fe83</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/31135104e35b1d06aff17fe83</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:35:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据建模, 宽表, 数据一致性, 数据计算成本
<br>
<br>
总结: 本文介绍了在Feed数据宽表建模过程中的演进和实践，通过建设大宽表来简化数仓、提升效率，解决数据一致性和计算成本等问题。随着业务发展，作者提出了三个阶段的建设过程，包括小时级宽表+主题宽表建模、实时宽表建模和基于流批一体的多版本宽表建设。通过不断优化数据建模，提高数据时效性和降低成本，实现了Feed数仓的持续发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>作者 | XY导读Feed，即个性化推荐信息流，是百度 App 上承载各种类型内容（如文章、视频、图集等）的重要 topic。本文概要讲述了随着业务发展，移动生态数据研发部在 Feed 数据宽表建模上的演进过程以及一些实践：整合流量、内容、用户等数据，建设多版本宽表，实现 feed 数仓的一致性，简化数仓取数逻辑，降低成本提升效率。</blockquote><p></p><p></p><p></p><blockquote>全文3312字，预计阅读时间9分钟。</blockquote><p></p><p></p><h1>01 引言</h1><p></p><p>在宽表建模阶段之前，feed 数仓是按照传统的数仓分层建模思路进行，按照 ods----&gt;dwd----&gt;dws----&gt;ads 层进行建模，在这四层之外，还有维表 dim 层。数仓建模数据较为分散，不同主题的表分散在不同的数据表，数仓复杂且存在大量冗余：数仓各层近百张表，总体数据量近50P。下游使用数据拼接成本较高，对于内部数仓和外部用户使用，都有巨大的解释成本和使用成本。</p><p></p><p>随着业务对数据使用精细化分析的需求增多，以及底层工具对数据计算和数据查询速度的提升，数据建设的思路转向建设大宽表，尽可能下沉业务逻辑到表中，隐藏复杂性。</p><p></p><p>Feed 数仓在宽表建模阶段，共分为三个阶段：</p><p></p><p>小时级核心表+主题宽表建模小时级核心表+主题宽表建模+实时宽表基于流批一体的多版本宽表</p><p></p><p>我们按照时间顺序来说明建设的这三个阶段。</p><p></p><h1>02 阶段一：小时级宽表+主题宽表建模</h1><p></p><p>在业务快速发展、业务复杂度提高的情况下，原先的基于分层建模的数仓的一些问题——如使用成本高、取数逻辑复杂、查询性能差、时效性差等问题开始逐渐变得显著。为了简化数仓、提升时效性、降低数仓的使用门槛，我们使用场内流式TM框架建设了15 分钟级流批日志表，并基于厂内图灵数仓，整合了 feed 分发、展现、时长、播放等数据到同一张表中，并基于该表，关联用户和资源维度等，建设用户宽表、资源宽表以及用户资源宽表等。</p><p></p><p>15 分钟级流批日志表(log_qi)：基于 feed 日志产出 feed 15 分钟的流批日志表，该表主要用于对日志原始字段的解析，并下沉简单业务逻辑。可以对应之前的 ods 层。feed 小时级明细宽表(log_hi)：小时级产出，下沉复杂业务逻辑，作为 feed 主要对外服务的数据表，可以对应 dwd 层。主题宽表、中间表：拼接其他主题数据，聚合数据聚合，可以对应 ads 层。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d4ed0cf4f67aaea6e1b669c8359f8c1.png" /></p><p></p><h1>03 阶段二：实时宽表建模</h1><p></p><p>实时宽表(log_5mi)的建设，源于业务的飞速发展，业务侧对数据的时效性提出了更高的要求，用于对实验或者策略上线后效果的验证和问题的监控。现有的 15 分钟级别流批日志已经不太能满足实时监控的时效性需求。而且 15 分钟级流批日志表，只是对原始日志的解析和抽取，并没有下沉复杂的业务逻辑，下游使用该表的成本巨大，无法满足对准实时数据快速迭代的需求。因此建设了 feed 实时数据表，该表 schema 完全对齐小时级宽表，同样下沉了复杂业务逻辑，下游应用可以快速简单地获取实时数据，用于满足业务对于实时数据需求的快速迭代。</p><p></p><p>实时宽表建设后，feed 数仓相较于之前，多了一条 feed 实时数据流。如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd261f55098e56161a90f4f2813261a8.png" /></p><p></p><h1>04 阶段三：基于流批一体的多版本宽表</h1><p></p><p></p><h2>4.1 背景</h2><p></p><p>在小时级表宽、主题宽表、实时宽表建设完成后 ，随着 feed 业务的发展，这套数据建模体现在应用现有业务的时候，还是出现了一些使用上的问题。主要体现在如下方面：</p><p></p><p>口径一致性：主要体现为流式实时数据与离线数据存在的差异，在数据一致性方面遇到了挑战，而且需要维护实时和离线数据两份数据口径。数据源不统一：搜索、直播有部分数据计入到 feed，数据源与现有数据源存在较大差异，获取 feed 数据多了两部分外部数据源。数据重复加工：数据数据源的不一致，导致 feed 数据分散在不同的中间表，导致获取完整数据成本较大，内外部获取数据存在重复加工的问题。数据计算成本大：资源、用户等主题宽表的维度的拼接，在计算中中间数据可能达到 30T，且存在数据倾斜问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f3b64c62d3ebe7389037f70bee7197e2.png" /></p><p></p><h2>4.2&nbsp;建设思路</h2><p></p><p>基于前面小时级表(log_hi)、实时表(log_5mi)的建设思路，建设一张新的天级用户-资源明细数据(log_di)宽表，用这三张表重构 Feed 数仓体系，解决实时&amp;离线数据不一致问题，统一 feed 数据源和数据出口，提升用户资源常用维度产出时效。</p><p></p><p>建设新表有两个难点：</p><p></p><p>业务上，如何统一不同数据的数据源，有效整合到一张表中，并且在表中下沉复杂的业务逻辑，对外隐藏业务复杂性，只暴露下沉好的业务字段。技术上，在 feed 总体数据拼接用户、资源维度的时候，中间 shuflfe 的数据量会达到 30T，且存在较大的数据倾斜，严重影响 join 的性能。</p><p></p><p>为了解决以上两个问题，在设计阶段，将新表设计为 4 级分区，拆分为 4 个版本产出，不同版本产出不同的数据。</p><p></p><p>版本拆分思路：feed 汇总数据、资源维度、用户维度、关注关系等，产出时效不同，按照对数据时效性要求的不同以及维度表就绪的时间，不提供版本拼接不同的维度数据，既提升对应维度的产出时效，也减少数据 JOIN 时的数据量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4ba14ee44c82ea2efe712cc1b139a61f.png" /></p><p></p><p>分区设计思路：</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/9255c4e471d7c251ff4267d89f030c30.jpeg" /></p><p></p><p>计数优化思路：对拼接的资源表、关注关系表做提前过滤，减少 join 时的数据量，再采用 spark AQE 解决数据倾斜问题。</p><p></p><h2>4.3 Feed 基于流批一体的多版本的数仓体系</h2><p></p><p>天级用户-资源明细数据(log_di)宽表建设完成后，Feed 实时表(log_5mi)、小时级表(log_hi)、天级表(log_di)，由于 schema 对齐，数据一脉相承，可以视为一张大宽表——Feed 基于流批一体的多版本宽表，共 3 张表，涉及 6 个版本：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3acf2ecd691dc8df927723b556b172a1.jpeg" /></p><p></p><p>用 Feed 基于流批一体的多版本宽表重构 Feed 数仓体系，其他主题表都基于流批一体的多版本宽表进行上卷，数据出口统一到宽表。不同的时效性产出的数据，对应上层不同的应用，如报表、数据集等等。</p><p></p><p>重构后数仓示意图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd8f0a3bcfda8d78d85ee21dbeb9dd66.png" /></p><p></p><p>经过重构后的 feed 数仓，具有以下优势：</p><p></p><p>数据源统一与数据出口统一：整合了分散的不同数据源到同一张表，统一了出口，并且下沉了复杂的业务逻辑，下游用户只需要查询一张表，保障了内外部门使用 feed 数据的一致性。多版本产出不同数据：对于时效性不同的查询需求，可以在实时、小时级、天级表多个版本间进行切换，除了调整表名外，查询语句基本不需要修改。高时效性多维度整合：资源、用户等多维数据，不同版本拼接不同的维度，提高了产出时效，下游可以按需依赖。</p><p></p><h1>05 总结与规划</h1><p></p><p>业务的发展对数仓工具提出了更高的要求，工具的不断迭代又带来更多的数仓建设思路，数仓的建设也随着业务的发展不断迭代。在宽表建设阶段，经过不断摸索，最终 feed 数仓简化为基于流批一体的多版本数仓体系。后续随着 Feed 业务规模的不断扩大和复杂化，当前的数仓工具&amp;数仓体系面临的挑战也日益增多，在新的业务挑战下，我们将继续完善数仓体系，以应对不断变化的业务需求，为业务决策和创新提供坚实的数据支持。</p><p></p><p>——————END——————</p><p></p><p>推荐阅读</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591535&amp;idx=1&amp;sn=2dfebbf2cff51638f839b216c4622481&amp;chksm=c03f5613f748df053f92276ddd957e05400ccf5bb26d4ff471f15e629d4fd803efd004615499&amp;scene=21#wechat_redirect">大模型时代数据库技术创新</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591523&amp;idx=1&amp;sn=58f71dc589563adfd20005dbeebd35c7&amp;chksm=c03f561ff748df09e3f66a766dda53c7042d9d393786f3588242203445c902821fa24d61878f&amp;scene=21#wechat_redirect">低代码组件扩展方案在复杂业务场景下的设计与实践</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591401&amp;idx=1&amp;sn=e4b658e810366196dff6fbf2ba781910&amp;chksm=c03f5595f748dc83b9259f3277b0d9c1a48b177f9208adfc0788472340a3d5493da909918ddf&amp;scene=21#wechat_redirect">通过搭建 24 点小游戏应用实战，带你了解 AppBuilder 的技术原理</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591330&amp;idx=1&amp;sn=ca7665b1cd877934bdd47fda7545f0bc&amp;chksm=c03f55def748dcc819b94c649d39fd94ce14fc3391abdcb8087b3606ae98fe85a2a9323a0fa5&amp;scene=21#wechat_redirect">基于 Native 技术加速 Spark 计算引擎</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247590639&amp;idx=1&amp;sn=4c5e02fe87272ef2243f50bd3ec9fcfc&amp;chksm=c03f5293f748db85d39e5f10adbf519b70fe0edcb3d233245a9b7d5ef065cb183572b350a836&amp;scene=21#wechat_redirect">百度&amp;YY设计稿转代码的探索与实践</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8ed40a3957157db4a1bd72583</id>
            <title>StarRocks跨集群迁移最佳实践｜得物技术</title>
            <link>https://www.infoq.cn/article/8ed40a3957157db4a1bd72583</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8ed40a3957157db4a1bd72583</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:10:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: DBA, StarRocks, 版本升级, 数据迁移
<br>
<br>
总结: 本文介绍了针对StarRocks集群版本升级和数据迁移的实践经验，包括方案流程、方案设计和方案规划等内容。通过外表和Flink Connector两种方案，实现了集群间的数据同步和读写分离，同时提供了适用场景和实施步骤。 </div>
                        <hr>
                    
                    <p></p><h1>一、引言</h1><p></p><p>2024年之前，DBA维护的StarRocks集群存在在用低版本多、稳定性受组件bug影响大的问题，给日常运维带来一定压力，版本升级迫在眉睫。于是，我们在今年年初安排了针对2.5以下版本升级2.5.13的专项。这里和大家分享下，针对因版本兼容问题而不能原地升级的场景下，进行跨集群升级时迁移数据方面的实践。</p><p></p><h1>二、方案流程</h1><p></p><p></p><h2>方案可行性评估口径</h2><p></p><p>针对跨集群迁移方案的评估，主要从迁移成本角度考虑，主要分为资源成本和稳定性成本：</p><p></p><h4>资源成本</h4><p></p><p>完成迁移所需要的人力工时投入、软硬件投入（如使用哪些三方平台、需要多少机器资源、带宽资源等）。</p><p></p><h4>稳定性成本</h4><p></p><p>数据迁移过程中，线上业务一般仍会继续提供服务，则迁移操作对系统产生的压力可能影响正常的生产服务，随之会带来额外的稳定性成本。这里从迁移服务产生系统压力的可监控预警能力评估稳定性成本。</p><p></p><h2>方案设计</h2><p></p><p></p><h4>方案一：StarRocks外表</h4><p></p><p></p><p>1. 技术原理</p><p>1.19 版本开始，StarRocks支持将数据通过外表方式写入另一个StarRocks集群的表中。这可以解决用户的读写分离需求，提供更好的资源隔离。用户需要首先在目标集群上创建一张目标表，然后在源StarRocks集群上创建一个Schema信息一致的外表，并在属性中指定目标集群和表的信息。</p><p></p><p>通过INSERT INTO写入数据至StarRocks外表，可以将源集群的数据写入至目标集群。借助这一能力，可以实现如下目标：</p><p></p><p>集群间的数据同步；读写分离。向源集群中写入数据，并且源集群的数据变更同步至目标集群，目标集群提供查询服务。</p><p></p><p>2. 方案评估</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b579c29a0145d6febef152264729e2e1.webp" /></p><p></p><p>3. 适用场景</p><p></p><p>数据量较小（200G以内）；无三方平台可用；数据迁移无需考虑稳定性成本；测试场景快速验证；存在hll、bitmap类型字段，但是又没有底表数据进行数据重建（hll/bitmap类型字段借助三方组件进行迁移的方案可参考官方文档flink导入至-bitmap-列、flink导入导入至-hll-列等）；Array/Map/Row等复杂类型的迁移。</p><p></p><h4>方案二：Flink Connector</h4><p></p><p></p><p>1. 技术原理</p><p>Flink是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。随着不断迭代，Flink已提供了接口统一的批流处理模型定义，同时提供了灵活强大的DataStream API和抽象度更高的Table API，供开发人员尽情发挥，更提供了SQL支持。</p><p></p><p>Flink提供了丰富的Connector，用以打通各类数据源，形成强大的数据联通能力。StarRocks官方也推出了导入和导出Connector，满足基于Flink对StarRocks的读写能力。</p><p></p><p>2. 方案评估</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/798f24c598f8446bbd660c3c80d97c17.webp" /></p><p></p><p>3. 适用场景</p><p></p><p>数据量较大；有三方平台可用；稳定性要求高，期望控制稳定性成本；有24h持续同步需求。</p><p></p><h2>方案规划</h2><p></p><p>在同步操作前，需要明确待同步的数据范围，统计较精确的待迁移数据量，评估数据迁移所需耗时，决策数据迁移完成时间等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/519d359e74c99bfb5c726ac5b057acf8.webp" /></p><p></p><h4>方式一</h4><p></p><p>结合预期的同步完成DDL，集群每天可用于同步的时间段，推导出同步时需要达到的速率。</p><p></p><p>计算公式：</p><p></p><p>预期同步最大速率(MB/s)=待同步数据总量(MB)/同步总耗时(天)/每天可同步时间(个小时/天)</p><p></p><h4>方式二</h4><p></p><p>根据集群负载可支持的最大速率、集群每天可用于同步的时间段，计算完成同步所需的时间。</p><p></p><p>同步总耗时(天)=待同步数据总量(MB)/预期同步最大速率(MB/s)/每天可同步时间(个小时/天)</p><p></p><h4>注意</h4><p></p><p>准确的待迁移数据量评估，依赖数据时间范围的确认。对于新旧集群双写场景，同步的最晚时间是完全双写介入的那一天（包含）。预期同步最大速率(MB/s)，需要兼顾集群当前流量和预估可承受的最大流量，避免因数据同步给集群造成预期外的压力，影响线上服务稳定性。</p><p></p><h2>方案实施</h2><p></p><p></p><h4>方案一：外表</h4><p></p><p></p><p>1. 创建外表</p><p>在源集群/库上创建外表，指向目标集群。</p><p></p><p>建议创建一个外表专用db，用于与源db隔离，避免误操作风险。</p><p></p><p><code lang="text">CREATE EXTERNAL TABLE external_db.external_t
(
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    k4 VARCHAR(2048),
    k5 DATETIME
)
ENGINE=olap
DUPLICATE KEY(`timestamp`)
PARTITION BY RANGE(`timestamp`)
(PARTITION p20231016 VALUES [("2023-10-16 00:00:00"), ("2023-10-17 00:00:00")),
PARTITION p20231017 VALUES [("2023-10-17 00:00:00"), ("2023-10-18 00:00:00")))
DISTRIBUTED BY HASH(k1) BUCKETS 10
PROPERTIES
(
    "host" = "127.0.0.x",
    "port" = "9020",
    "user" = "${user}",
    "password" = "${passwd}",
    "database" = "test_db",
    "table" = "t"
);</code></p><p></p><p>2. 写入外表</p><p>在源集群/库上写入外表。</p><p></p><p><code lang="text">insert into external_db.external_t select * from db.other_table;</code></p><p></p><p>3. 优缺点</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e66092855f2b32ebc7b6c7329d54f12f.webp" /></p><p></p><h4>方案二 Flink SQL</h4><p></p><p>1. 接入实时计算平台</p><p></p><p>本方案基于我司自研的实时计算平台（Flink任务开发调度平台）实现，需要业务方先接入平台，拥有专属项目空间和计算资源，这里不再赘述。</p><p></p><p>2. 新建Flink SQL任务</p><p></p><p>同步任务SQL即为Flink SQL，分为定义数据来源表、定义数据输出表、定义同步ETL SQL三部分。</p><p></p><p>定义数据来源表</p><p></p><p>语法上遵守Flink SQL规范，更多参数设置可参见官方文档使用Flink Connector读取数据-使用 Flink SQL读取数据。</p><p></p><p>注意事项：</p><p></p><p>StarRocks与Flink SQL的数据类型映射；Flink scan参数设置，尤其是超时（time-out）类字段的设置，建议往大了设置；考虑到数据迁移的源端和目标端的库、表均同名，在定义时需要对源表和输出表的表名做区分，以免混淆错乱。比如源表命名为{table名}_source，输出表命名为{table名}_sink 。</p><p></p><p>示例：</p><p></p><p><code lang="text">CREATE TABLE rule_script_etl_source (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `cost` BIGINT,
  `name` STRING,
  `error` STRING,
  `script` STRING,
  `rule_id` STRING
) WITH (  
    'connector'='du-starrocks-1.27', --具体值以官方组件或自研组件定义为准
    'jdbc-url'='jdbc:mysql://1.1.1.1:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'scan-url'='1.1.1.1:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'scan.max-retries'='3',
    'scan.connect.timeout-ms'='600000',
    'scan.params.keep-alive-min'='1440',
    'scan.params.query-timeout-s'='86400',
    'scan.params.mem-limit-byte'='1073741824'
);</code></p><p></p><p>定义数据输出表</p><p></p><p>注意事项：</p><p></p><p>StarRocks与Flink SQL的数据类型映射；Flink sink参数设置，尤其是超时（time-out）类字段的设置，建议往大了设置；尽量进行攒批，减小对StarRocks的导入压力；考虑到数据迁移的源端和目标端的库、表均同名，在定义时需要对源表和输出表的表名做区分，以免混淆错乱。比如源表命名为{table名}_source，输出表命名为{table名}_sink ；如果输出表是主键模型，表定义中字段列表后需要加上PRIMARY KEY ({primary_key}) NOT ENFORCED。</p><p></p><p>示例：</p><p></p><p><code lang="text">CREATE TABLE rule_script_etl_sink (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `rule_id` STRING,
  `uid` BIGINT,
  `cost` BIGINT,
  `name` STRING,
  `error` BIGINT,
  `script` STRING,
  `sink_time` TIMESTAMP,
  PRIMARY KEY (`identity_id`) NOT ENFORCED  # 仅适用主键模型
) WITH (
    'connector'='du-starrocks-1.27',
    'jdbc-url'='jdbc:mysql://1.1.1.2:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'load-url'='1.1.1.2:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'sink.buffer-flush.max-rows'='400000',
    'sink.buffer-flush.max-bytes'='94371840',
    'sink.buffer-flush.interval-ms'='30000',
    'sink.connect.timeout-ms'='60000',
    'sink.wait-for-continue.timeout-ms'='60000'
);</code></p><p></p><p>定义同步ETL</p><p></p><p>一般为insert select语句；可以根据自身需求，添加一些ETL逻辑。</p><p></p><p>注意事项：</p><p></p><p>有映射关系的非同名字段，添加as，提升可阅读性；前后字段类型不一样的，需要使用case as进行显式类型转换；如果是仅输出表包含的字段，也需要在select子句中显式指出，并使用case null as {dataType}的形式进行类型转换；部分String/VARCHAR(n)类型字段中，可能存在StarRocks Flink Connector使用的默认列分隔符(参数sink.properties.column_separator，默认\t)、行分隔符(参数sink.properties.row_delimiter，默认\n)，导致导入是报“errorLog:Error:Value count does not match column count. Expect xx, but got xx. Row:xxx”错误，需要替换为自定义的分隔符；select子句尽量添加filter信息，一般是分区字段，以便Flink根据同步任务设置的并行度，拆分任务，生成合适的执行计划。</p><p></p><p>示例：</p><p></p><p><code lang="text">insert into rule_script_etl_sink
select
  `timestamp`,
  `identity_id`,
  `app`,
  `rule_id`,
  cast(null as BIGINT) `uid`,
  `cost`,
  `name`,
  cast(`error` as BIGINT) `error`,
  `script`,
  `timestamp` as `sink_time`
from rule_script_etl_source
where `timestamp` &gt;='2023-08-20 00:00:00' and `timestamp` &lt; '2023-09-20 00:00:00';</code></p><p></p><p>完整示例：</p><p></p><p><code lang="text">CREATE TABLE rule_script_etl_source (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `cost` BIGINT,
  `name` STRING,
  `error` STRING,
  `script` STRING,
  `rule_id` STRING
) WITH (  
    'connector'='du-starrocks-1.27',
    'jdbc-url'='jdbc:mysql://1.1.1.1:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'scan-url'='1.1.1.1:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'scan.max-retries'='3',
    'scan.connect.timeout-ms'='600000',
    'scan.params.keep-alive-min'='1440',
    'scan.params.query-timeout-s'='86400',
    'scan.params.mem-limit-byte'='1073741824'
);

CREATE TABLE rule_script_etl_sink (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `rule_id` STRING,
  `uid` BIGINT,
  `cost` BIGINT,
  `name` STRING,
  `error` BIGINT,
  `script` STRING,
  `sink_time` TIMESTAMP,
  PRIMARY KEY (`identity_id`) NOT ENFORCED  # 仅适用主键模型
) WITH (
    'connector'='du-starrocks-1.27',
    'jdbc-url'='jdbc:mysql://1.1.1.2:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'load-url'='1.1.1.2:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'sink.buffer-flush.max-rows'='400000',
    'sink.buffer-flush.max-bytes'='94371840',
    'sink.buffer-flush.interval-ms'='30000',
    'sink.connect.timeout-ms'='60000',
    'sink.wait-for-continue.timeout-ms'='60000',
    'sink.properties.column_separator'='#=#',  -- 自定义列分隔符
    'sink.properties.row_delimiter'='@=@'  -- 自定义行分隔符
);

insert into rule_script_etl_sink
select
  `timestamp`,
  `identity_id`,
  `app`,
  `rule_id`,
  cast(null as BIGINT) `uid`,  -- sinl表才有的字段
  `cost`,
  `name`,
  cast(`error` as BIGINT) `error`,
  `script`,
  `timestamp` as `sink_time`
from rule_script_etl_source
where `timestamp` &gt;='2023-08-20 00:00:00' and `timestamp` &lt; '2023-09-20 00:00:00';</code></p><p></p><p>3. 调度任务</p><p></p><p>在开始调度前，还需要为任务的设置合适的并行度。通常SlotNum/TM设置为1，Parallelism设置为3，以长耗时换取导入任务的运行稳定性。</p><p></p><p>为避免任务失败带来的重跑工作量，单表每次任务可以迁移部分分区，多次执行。</p><p></p><p>4. 优缺点</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d73acc9efbe8d8caa1c980da7049145.webp" /></p><p></p><h2>方案验证&amp;验收</h2><p></p><p></p><h4>验证</h4><p></p><p>可以选取不同大小的表若干，组成有梯度的待同步数据量，使用上述任一种方案，执行同步操作，并观察同步时间内集群的负载。</p><p></p><p>以集群各水位不超过80%、无业务报错为准，尝试验证集群可承载的最大同步速率，及时校正上面的数据同步规划。</p><p></p><h4>验收</h4><p></p><p>1. 集群负载</p><p></p><p>以集群各水位不超过80%、无业务报错为准。可根据集群水位情况，酌情增加或减少同步任务的并发。</p><p></p><p>2. 数据diff校验</p><p></p><p>数据行数校验</p><p></p><p>针对迁移前后数据模型未发生改变的表，一定范围内（通常是单分区级别）的数据量需要保持相等；</p><p></p><p>针对迁移前后数据模型发生改变的表，需要case by base分析。</p><p></p><p>如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5815a9230e2a45fc98f9abd2b3dbcec.webp" /></p><p></p><p>数据质量校验针对维度表，可参考分区及或表级行数校验结果；针对事实表，可以在分区级别做指标列的SUM/MAX/MIN/AVG值校验；研发也可以结合业务自定义更多的校验方式。</p><p></p><h1>三、方案成果</h1><p></p><p>基于本方案，有效地解决了原地升级异常再回滚的方案带来的不稳定风险，完成了多个集群从低版本直升2.5.13的目标，累计迁移数据逾10T，迁移流量摸高至2Gb/s（10+个节点）。</p><p></p><p>结合原地升级方式，共同构成了较完善的升级方案，尽量减少升级带给业务的闪断等影响的同时，以较高效率完成升级。</p><p></p><h1>四、方案展望</h1><p></p><p></p><h2>方案的不足</h2><p></p><p>对比云商和自建DTS平台的数据迁移功能，本方案在流程化、产品化上的建设还有较大进步空间，诸如在迁移任务的量级分析、任务拆分、持续性调度、容错等步骤都可以做更多的自动化建设。</p><p></p><p>因StarRocks 2.5.13尚未支持CDC功能，当前的迁移方案暂只能提供离线同步的能力，在跨集群升级过程中，为保障数据的一致性，仍需要花费较多的精力，诸如协调新旧集群的双写、切流、补数等。</p><p></p><h2>未来规划</h2><p></p><p>方案中一些功能点，可以封装成原子功能，供更多场景使用。封装随着新版本StarRocks稳定性逐渐增强，组件自身bug影响稳定向的概率已经非常低了，跨集群升级的场景需求也越来越少。但方案中的原子能力，诸如库表特征分析、跨集群的shcema同步、表重建等等，仍有继续打磨的空间，可以在日常运维中提供帮助。</p><p></p><p>数据迁移的实时CDC能力也是一项亟待补齐的能力，集成离线和实时迁移功能，将助力实现无感升级。</p><p></p><p>探索跨集群迁移流程将探索更多的适用场景，诸如基于资源利用率或稳定性的集群拆分、合并等场景。</p><p></p><p>引用：</p><p></p><p>https://docs.starrocks.io/zh/docs/2.5/loading/Flink-connector-starrocks/#%E5%AF%BC%E5%85%A5%E8%87%B3-bitmap-%E5%88%97</p><p></p><p>https://docs.starrocks.io/zh/docs/2.5/loading/Flink-connector-starrocks/#%E5%AF%BC%E5%85%A5%E8%87%B3-hll-%E5%88%97</p><p></p><p>https://docs.starrocks.io/zh/docs/2.5/unloading/Flink_connector/</p><p></p><p>*文/&nbsp;管虎</p><p></p><p>本文属得物技术原创，更多精彩文章请看：<a href="https://tech.dewu.com/">得物技术</a>"</p><p></p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fc6d15f00293a1869ee1aff46</id>
            <title>ELB Ingress网关助力云原生应用轻松管理流量</title>
            <link>https://www.infoq.cn/article/fc6d15f00293a1869ee1aff46</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fc6d15f00293a1869ee1aff46</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:04:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ELB Ingress, 云原生应用, 流量管理, 负载均衡
<br>
<br>
总结: 本文介绍了华为云CCE服务提供的ELB Ingress功能，帮助用户轻松管理云原生应用的流量。ELB Ingress基于Kubernetes Ingress，提供高性能、高可用、高安全的负载均衡能力，满足企业对流量管理的需求。通过ELB Ingress，用户可以灵活配置流量访问规则，实现对外访问机制，提高应用的可观测性和可维护性。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区<a href="https://bbs.huaweicloud.com/blogs/430487?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">《ELB Ingress网关助力云原生应用轻松管理流量》</a>"，作者：云容器大未来。</p><p></p><h1>背景</h1><p></p><p></p><p>通常情况下，K8s集群的容器网络平面和外部网络是隔离的，外部网络无法直接访问到集群内部的容器业务，如何为容器提供并管理统一的外部流量入口?社区提供的常见方式是使用Nodeport Service，Loadbalancer Service，Ingress等K8s资源对象来暴露集群内部的容器原生应用。Service对象提供了四层负载均衡能力，Ingress对象则提供了面向应用层访问（HTTP/HTTPS等）的七层负载均衡能力。</p><p></p><p>而随着云原生架构在企业内的普遍落地，容器作为云原生微服务应用的载体，需要面对更多挑战，如面对微服务的复杂组网，业务请求在云服务之间转发往往需要做源地址转换而导致流量分发损耗；游戏类、电商抢购类等业务在短时间内会进行频繁扩缩容，必须应对高并发的网络流量；网关入口流量应对互联网的安全攻击，如灰产、异常流量，需提供流量安全防护能力；此外，支持更加复杂的路由规则配置、多种应用层协议（HTTP、HTTPS、GRPC等）、应用蓝绿发布、流量可观测性等七层高级转发能力也逐渐成为了云原生应用的普遍诉求。Ingress Nginx，Ingress Kong，Traefik等开源社区方案虽然提供了丰富的七层流量治理功能， 但对于关键生产业务上云，企业在选择Ingress方案时,除了考虑功能性,还需要充分权衡安全性、可维护性和可靠性等方面的需求，以找到最佳平衡点。专业的云服务提供商提供托管的Ingress解决方案，能够较好的应对这些挑战。</p><p></p><p><a href="https://www.huaweicloud.com/product/cce.html">华为云CCE服务</a>"提供了基于应用型负载均衡ELB（Elastic Load Balance）的全托管免运维的企业级 Ingress 流量治理，让用户轻松应对云原生应用流量管理。</p><p></p><h1>ELB Ingress 介绍</h1><p></p><p></p><p>在K8s集群中，容器网络平面通常是独立于集群主机网络的一个隔离的网络平面，工作负载在滚动升级或者重新调度后容器的地址会有变化，这就带来一个问题：如何实现某组Pod的服务发现，并提供固定的外部访问入口？Service和Ingress对象就是K8s中实现集群内外应用统一访问入口的一种机制。</p><p></p><p>K8s社区对集群外部的流量暴露提供了三种方式：Nodeport Service、Loadbalancer Service、Ingress，前两者Service对象主要提供集群四层流量入口，Ingres对象提供七层流量治理能力。两者相互配合，共同实现K8s集群应用的对外访问机制。如下图一所示，客户端通过Ingress管理的负载均衡器，访问Ingress申明的路由，由负载均衡器将流量经过后端Service导入至后端容器。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4a4e0d7ec73508c59f11dd24a8fcb3f.png" /></p><p>图一：Ingress示例</p><p></p><p>ELB Ingress是华为云CCE服务提供的七层流量治理功能，基于社区标准Ingress API实现，提供高可用、高性能、高安全、多协议的全托管免运维负载均衡能力。同时具备弹性能力，在流量突发时支持快速扩展计算资源，支持千万级并发连接，百万级新建连接，是云原生应用流量治理的理想选择。</p><p></p><h1>ELB Ingress工作原理</h1><p></p><p></p><p>ELB Ingress部署于CCE集群的master节点上，与ELB实例对接，可将Ingress申明的容器后端地址、转发策略、路由等信息配置至ELB实例，并且支持动态更新。</p><p></p><p>图二是基于Nodeport中转的ELB Ingress工作流图，CCE Standard集群使用该方案的原理如下：</p><p></p><p>用户为集群创建Ingress资源，在Ingress中配置流量访问规则，如负载均衡器实例、URL路由、SSL证书等监听信息，以及访问的后端Service等，控制器通过标签选择器选中工作负载，将工作负载所在节点和Nodeport端口挂载至负载均衡器实例的后端；Ingress Controller监听到Ingress资源发生变化时，会根据其中定义的流量访问规则，在ELB侧重新配置监听器以及后端服务器路由;用户通过ELB访问应用程序，流量根据ELB中配置的转发策略转发到对应的Node节点，再经过Nodeport二次转发访问到关联的工作负载(Nodeport转发机制参见k8s官方文档说明)。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d298f0c858f5d3c554dfffae0be52d9.png" /></p><p>图二: Nodeport中转的ELB Ingress流程图</p><p></p><p>该方案中流量经过节点、IPTables/IPVS规则多次转发，网络性能存在损耗。在大流量场景下，网络转发效率、网络连通速度的挑战尤为突出。为此，我们推出了基于CCE Turbo集群的网络加速方案：容器直接使用VPC网络实现直通容器的ELB Ingress，将原有的“容器网络 + 虚拟机网络“两层模型简化为一层。如图三所示，集群中的Pod IP直接从VPC中分配，支持北向ELB直通容器，外部流量可以不经过节点端口转发直接访问集群中的Pod，达到流量分发零损耗的效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f679908cfcc9bd82d14e4511c82cecf7.png" /></p><p>图三:容器网络直通的ELB Ingress流程图</p><p></p><h1>ELB Ingress流量治理核心优势</h1><p></p><p></p><p>ELB Ingress基于原生Kubernetes Ingress，通过声明式API指定Ingress的路由、对接的后端服务，或者通过Annotation配置监听侧的高级选项，由系统保证最终一致性。ELB Ingress为开发者和运维人员提供了极大的开发灵活性和维护便利性，其核心优势包括：</p><p></p><p>高吞吐、高可用、高弹性</p><p></p><p>ELB Ingress搭配独享型ELB实例，最高支持2千万并发连接；通过完善的健康检查机制，保障业务实时在线，支持多可用区的同城双活容灾，无缝实时切换；弹性规格ELB实例支持根据流量负载自动弹性扩缩实例规格，适用于业务用量波动较大的场景，例如游戏、视频等行业，能满足瞬时流量同时成本最小化。</p><p></p><p>高安全性</p><p></p><p>ELB Ingress提供了端到端的全链路安全策略，如下图四是外部流量经过ELB访问CCE Turbo集群的简单示例：在访问端可配置接入WAF引擎检测并拦截恶意攻击流量，而正常流量转发至后端云服务器。通过Ingress的Annotation配置可轻松为ELB实例配置自定义安全策略，例如设置黑白名单，双向认证等。从ELB转发至后端也支持HTTPS加密信道，进一步增强整体安全性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5dcc4a6f8f6b9a455e276b0b974f7869.png" /></p><p>图四:&nbsp;外部流量访问CCE Turbo安全示例</p><p></p><p>可移植性</p><p></p><p>完全兼容社区Ingress语义，从开源Nginx Ingress等方案迁移过来仅需改造annotation即可轻松适配。</p><p></p><p>可观测性</p><p></p><p>云监控可以按时间轴查看ELB的网络流量和访问日志，动态分析并告警潜在风险；云审计可以实时监控ELB资源更新日志，针对风险动作实时告警，动态监控云上资源安全；Ingress Controller也支持丰富的普罗监控指标，如接口调用时延，reload次数等。</p><p></p><p>免维护性</p><p></p><p>ELB Ingress组件运行在集群的Master节点，用户无需关注运维问题，组件在集群升级时会自动更新，且对业务无感。</p><p></p><h1>ELB Ingress流量治理核心功能</h1><p></p><p></p><p>在社区基础功能之上，华为云ELB Ingress在负载均衡、路由规则、流量控制、安全性和可观测性等方面都有较大增强，满足了更复杂的生产环境需求。下面介绍ELB Ingress流量治理核心功能：</p><p></p><p>灰度发布</p><p></p><p>灰度发布是业界常用的版本升级平滑过渡的一种方式。在版本升级时，先让部分用户使用新版本，其他用户继续使用老版本。待新版本稳定后，再逐步扩大新版本的使用范围，直到所有用户流量都迁移到新版本上。这样可以最大限度地控制新版本发布带来的业务风险，降低故障影响范围，同时支持快速回滚。</p><p></p><p>我们提供了基于Header/Cookie/Weight的灰度发布策略，前两种策略通过将用户分成若干组，在不同的时间段内逐步引入新版本，最终扩大新版本的影响范围；基于Weight的策略则是通过控制新版本的权重，在不同时间段内逐步增加新版本的流量比例，直到完全替代旧版本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d0a529fefa223871db34e1f5c2e5f7e.jpeg" /></p><p></p><p>高级转发策略</p><p></p><p>随着云原生应用组网的日益复杂，传统的基于路由转发的七层流量治理已经难以满足需求。我们提供的高级转发策略可以很好地解决传统方案面临的局限性：</p><p></p><p>基于请求头的负载均衡：根据客户端请求头的不同值，将请求分配到不同的后端服务器。HTTP重定向到HTTPS：系统自动将HTTP监听器流量转发至HTTPS监听，提升网站安全性，防止内容篡改等。URL重定向和重写：支持将URL永久或临时映射到另一个URL。同时，支持正则表达式匹配和实现不同路径的重写规则。</p><p></p><p>慢启动</p><p></p><p>在应用滚动升级时，ELB Ingress会自动更新负载均衡器后端，并且根据后端容器实例副本数自动设置后端权重。但是，在后端健康检查通过后的上线过程中，可能面临流量突增，导致后端容器的CPU或内存资源瞬间高负荷，从而影响业务稳定性。在开启慢启动模式后，系统可以在指定时间内，逐步将流量导入到目标容器后端。这样可以缓解业务容器突增的流量压力，保护系统免受过度负载的影响，实现优雅过渡。</p><p></p><h1>小结</h1><p></p><p></p><p><a href="https://www.huaweicloud.com/product/cce.html">华为云CCE服务</a>"的ELB Ingress基于华为云应用型负载均衡ELB（Elastic Load Balance）提供强大的Ingress流量管理能力，兼容Nginx Ingress，具备处理复杂业务路由和证书自动发现的能力，支持HTTP、HTTPS和GRPC等协议，满足在云原生应用场景下对超强弹性和大规模七层流量处理能力的需求。</p><p></p><p>后续我们还将发布系列文章，详细介绍基于ELB Ingress的流量管理最佳实践，欢迎各位读者继续关注。</p><p></p><p>相关链接：</p><p></p><p>华为云云容器引擎CCE服务路由概述：<a href="https://support.huaweicloud.com/usermanual-cce/cce_10_0094.html">https://support.huaweicloud.com/usermanual-cce/cce_10_0094.html</a>"Ingress官方文档：https://kubernetes.io/docs/concepts/services-networking/ingress/</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/722052195cf10d5865fc51bba</id>
            <title>线索系统性能优化实践</title>
            <link>https://www.infoq.cn/article/722052195cf10d5865fc51bba</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/722052195cf10d5865fc51bba</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 03:49:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 线索CRM系统, 性能优化, 流程梳理与抽象, 模板方法设计模式
<br>
<br>
总结: 京东家居事业部的线索CRM系统在业务扩张和市场需求增长的情况下出现了架构不适应、代码冗余、接口响应时间长等问题，需要进行性能优化和调整以支撑业务快速发展。优化目标是统一封装和抽象线索提交流程，提高系统可维护性和扩展性，降低新渠道接入时间成本，提高接口性能和响应，保证接口正确性。通过流程梳理与抽象、创建流程拆分和模板方法设计模式的应用等策略和实施，实现了系统性能的优化和提升。 </div>
                        <hr>
                    
                    <p></p><p></p><h2>引言</h2><p></p><p>在京东家居事业部，线索CRM系统扮演着至关重要的角色，它作为构建家居场景核心解决方案集的首要环节，肩负着获客和拓展业务的重要使命。然而，随着业务的不断扩张和市场需求的日益增长，系统原有的架构开始显露出诸多不适应之处，如架构设计不再清晰，代码存在过量冗余，核心的读写接口响应时间长等问题，这些问题严重制约了业务的敏捷性和快速发展。鉴于这一状况，系统的性能优化和调整势在必行，以确保其能够更好地支撑业务的快速发展需求。</p><p></p><h2>系统优化概述</h2><p></p><p></p><h2>一. 线索提交接口的统一与性能优化</h2><p></p><p></p><h3>系统优化前存在的问题</h3><p></p><p></p><h4>1. 新渠道接入周期长，代码冗余，</h4><p></p><p>系统中存在五个主要的线索创建渠道，它们的处理流程高度相似，但是代码却是分散冗余的。每当有新渠道需要接入时，之前的做法都是从已有代码中复制粘贴并做小幅调整，缺乏抽象和封装，导致了代码的高度重复，增加了维护的难度和出错的风险。</p><p></p><p>比如当时我们为了支持多供应商这个需求，需要对线索分派商家的逻辑进行更改，由于这段逻辑分散在多处，同时由于测试对底层实现的不了解，可能会误认为只需要测试一个渠道就能覆盖基本场景，就有可能导致非必要的线上问题的产生。</p><p></p><h4>2. 性能瓶颈</h4><p></p><p>在线索创建过程中，由于业务的复杂性需要执行10来个子流程以及开发过程中的不规范导致的对线索主数据的不必要的重复更新、重复同步ES等问题，接口性能较慢，tp99将近3000ms。</p><p></p><h4>3. 数据一致性问题</h4><p></p><p>线索创建主数据，分配商家以及匹配到重复规则时需要新增运营回访记录，这些流程都涉及到对数据库的写操作，但是这些写入没有放在同一事务中，导致了某个子流程写入失败时存在数据一致性问题。</p><p></p><h3>优化目标</h3><p></p><p>我们的优化目标是对线索提交流程统一封装和抽象，提高系统的可维护性和扩展性。同时降低新渠道接入的时间成本，提高接口性能和响应，保证接口在复杂情况下的正确性。</p><p></p><h3>优化策略与实施</h3><p></p><p></p><h4>流程梳理与抽象</h4><p></p><p>我们首先对当前所有渠道的线索创建流程进行了全面的梳理，将线索的创建流程抽象化，并定义出一套标准化的流程模板。具体来说一个线索的创建包括以下流程：</p><p></p><p>(1) 入参校验</p><p></p><p>(2) 查询三级渠道</p><p></p><p>(3) 验证三级渠道开关</p><p></p><p>(4) 根据入参封装要创建的线索实体</p><p></p><p>(5) 线索是否重复的规则校验</p><p></p><p>(6) 数据库保存线索和异构到ES</p><p></p><p>(7) 读取配置规则以及后续的同步京音系统</p><p></p><p>(8) 将新线索分配到对应的商家</p><p></p><p>(9) 短信消息和京麦消息通知商家</p><p></p><p>(10) 根据线索和分配的商家信息为用户创建装修档案</p><p></p><h4>创建流程拆分</h4><p></p><p>通过分析发现，对于不同渠道的线索创建过程来说，最大的差异点在于流程(1) 和 (2), 对于流程(3)-(10)基本相似。</p><p></p><p>这些流程虽然在逻辑上紧密相连，但是对于线索创建这一业务来说最核心的流程是流程(6)及之前的流程，至于流程(7)-(10)则是线索创建后的附属操作，这些附属操作涉及到和外部门系统间复杂的交互，占用了大量资源并影响到核心流程的响应速度。</p><p></p><p>因此我们聚焦于线索创建这一核心流程，和从职责单一的角度考虑，我们将整个线索的常见进行拆分:</p><p></p><p>第一 核心流程-线索的创建。</p><p></p><p>第二 线索分配商家以及之后的通知操作</p><p></p><p>第三 为分配商家后为用户创建对应的装修档案</p><p></p><p>这三个创建流程通过京东自研消息JMQ进行串联，解耦了线索创建和附属操作的执行。通过异步处理附属操作，附属操作的耗时不会阻塞核心流程的执行，减少了对核心流程的干扰，从而大大提升了系统的响应性和吞吐量。</p><p></p><h4>模板方法设计模式的应用</h4><p></p><p>定义： 模板方法设计模式是一种行为设计模式，它在一个方法中定义了一个算法的骨架，将一些步骤的执行延迟到子类中。这样，子类可以在不改变算法结构的情况下重新定义算法的某些特定步骤。</p><p></p><p>通用类图:</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e195c0a0ebd9eef1722ebaa424cbaf2.png" /></p><p></p><p>​</p><p></p><p>示例代码:</p><p></p><p><code lang="text">public abstract class Game {
    // 模板方法，定义算法骨架
    public final void play() {
        initialize();
        startPlay();
        endPlay();
    }

    // 需要子类实现的方法
    abstract void initialize();
    abstract void startPlay();
    abstract void endPlay();
}

public class Cricket extends Game {
    @Override
    void initialize() {
        System.out.println("Cricket Game Initialized! Start playing.");
    }

    @Override
    void startPlay() {
        System.out.println("Cricket Game Started. Enjoy the game!");
    }

    @Override
    void endPlay() {
        System.out.println("Cricket Game Finished!");
    }
}

public class Football extends Game {
    @Override
    void initialize() {
        System.out.println("Football Game Initialized! Start playing.");
    }

    @Override
    void startPlay() {
        System.out.println("Football Game Started. Enjoy the game!");
    }

    @Override
    void endPlay() {
        System.out.println("Football Game Finished!");
    }
}

public class TemplateMethodPatternDemo {
    public static void main(String[] args) {
        Game game = new Cricket();
        game.play();

        System.out.println();

        game = new Football();
        game.play();
    }
}
</code></p><p></p><p></p><p>在这个例子中，Game是一个抽象类，定义了游戏的模板方法play()。Cricket和Football是具体的游戏，它们实现了Game类的抽象方法，以提供各自的游戏初始化、开始和结束的具体实现。</p><p></p><p>具体到我们系统， 流程1到10是创建线索的骨架抽象和定义。对于骨架中的子流程，我们识别出易变部分（步骤1和2）和 不易变的部分（步骤3至6）。易变部分需要交给子类去实现，不易变部分则需要统一实现。</p><p></p><h4>易变部分抽象</h4><p></p><p>对于入参校验和查询三级渠道这两个流程来说，每个渠道都存在独有的逻辑，比如，心愿单渠道需要校验心愿单类型和来源ID必传，而投放助手渠道则需校验投放单号必传；多阶段订单渠道是通过SKU来查询三级渠道，而市场部渠道则是通过媒体账号ID来查询。</p><p></p><p>因此我们对于这两个流程定义了抽象方法，并将实现细节交个具体渠道的负责。</p><p></p><h4>不变部分统一处理</h4><p></p><p>对于线索创建流程中的不易变部分，我们实现了统一的处理逻辑，如三级渠道开关验证、线索归集信息封装、重复规则校验、数据库保存以及异构到ES等流程。</p><p></p><p>同时对于所有需要数据库变更的操作放到一个事务中，保证了写入的同时成功或失败。</p><p></p><h4>​工程实践</h4><p></p><p>通过上文介绍， 编码大体实现如下：</p><p></p><p><code lang="text">//获取三级渠道
protected abstract ChannelThreeDto getChannel(ClueDTO clueDTO);

//前置状态校验
protected abstract boolean preConditionCheck(ClueDTO clueDTO);

public ResultDto submit(ClueDTO clueDTO) {

        //1.前置状态校验
        if (!preConditionCheck(clueDTO)) {
            return ResultDto.getFailedResult(ResultCodeEnum.SERVICE_ERROR.getMsg(), ResultCodeEnum.SERVICE_ERROR.getCode());
        }

        //2.获取三级渠道
        ChannelThreeDto channelThreeDto = getChannel(clueDTO);

        //3.确认渠道开关是否开启
        if (!checkChannel(channelThreeDto)) {
            return ResultDto.getFailedResult(ResultCodeEnum.SERVICE_ERROR.getMsg(), ResultCodeEnum.SERVICE_ERROR.getCode());
        }

        //4.线索重复校验
        Boolean isRepeat = checkClueRepeat(clueDTO, channelThreeDto);
        if (isRepeat) {
            return ResultDto.getFailedResult(ResultCodeEnum.SERVICE_ERROR.getMsg(), ResultCodeEnum.SERVICE_ERROR.getCode());
        }

        //5.封装线索实体对象
        ClueManageDto clueManageDto = buildClueManage(clueDTO, channelThreeDto);

        //6.数据清洗规则检查
        ClueVisitDto clueVisitDto = clueDataWash(clueManageDto, channelThreeDto);
        if (!ObjectUtils.isEmpty(clueVisitDto)) {
            clueManageDto.setClueStatus(ClueStatusEnum.INVALID.getCode());
        }

        //7.数据库保存
        boolean result = saveClueManage(clueManageDto, clueVisitDto);

        //8.发送线索创建通知，执行之后的线索分配商家等操作
        sendClueMessage(clueDistributionDTO);

        return ResultDto.getSuccessResult(result, ResultCodeEnum.SUCCESS.getCode());
    }
</code></p><p></p><p></p><p></p><h3>优化成果</h3><p></p><p>通过引入模板方法的设计模式、异步拆分以及优化事务管理策略，创建线索的系统架构得到了根本性的改进。 我们不仅提高了代码的复用率，降低了新渠道接入的成本，也极大地提升了系统的可维护性和扩展性。</p><p></p><p>现在，新渠道的接入变得更加快捷和灵活，从之前新渠道接入耗时6人/天降低到2人/天左右；同时线索创建的响应时间也从之前的3000ms降到现在的250ms左右。</p><p></p><h2>二. 线索核心写接口性能优化实践</h2><p></p><p></p><h3>背景</h3><p></p><p>在竞争激烈的市场环境中，CRM系统不仅需要准确无误地收集用户的客资信息，更重要的是要实现对这些宝贵信息的快速响应和有效跟进。用户留下联系方式的瞬间，往往是他们对产品或服务兴趣最浓厚的时刻，我们需要快速响应，抢占先机，才有可能增加用户转化为客户的可能性，因此对于核心接口的性能有较高的要求。</p><p></p><p>但是当前系统在处理线索创建、分配商家，状态变更以及商家反馈等核心流程上存在接口性能不理想的问题，比如商家反馈线索tp99耗时2000ms， 分配商家耗时1500ms。</p><p></p><h3>问题分析</h3><p></p><p>在每个核心流程中，系统会进行两项重要的操作：</p><p></p><p>1.更新数据库：将业务操作的结果持久化到数据库中。</p><p></p><p>2.数据同步到ES：将变更的数据同步到两个ES集群中（一个供运营端查询，另一个供商家端查询适用）</p><p></p><p>传统同步机制是在业务逻辑操作完成后立即进行数据同步。这种同步方式虽然简单直接，但存在几个缺点：</p><p></p><p>•性能瓶颈：同步操作耗时，导致接口响应时间增长，影响用户体验。</p><p></p><p>•复杂度增加：业务逻辑与数据同步逻辑耦合，增加了代码的复杂度和维护难度。</p><p></p><p>•扩展性受限：随着业务增长，同步操作成为系统扩展的瓶颈。</p><p></p><h3>优化方案</h3><p></p><p>针对上述问题，我们采取了一系列措施来优化系统性能，核心策略是将数据同步到ES的过程异步化。</p><p></p><p>1. 订阅Binlake变更</p><p></p><p>我们将业务逻辑操作和数据同步到ES的过程分离。业务接口只负责业务逻辑的变更和数据库的更新，而数据同步到ES的操作，通过订阅Binlake变更事件来异步执行。</p><p></p><p>2. 处理变更消息</p><p></p><p>通过订阅线索主数据和线索分配商家数据的变更消息，封装接口将线索主数据和分配商家信息同步到ES。值得注意的是，为了避免数据库变更在JMQ中的乱序性以及可能带来的数据被错误覆盖的问题，我们只关注消息中的哪个线索单号发生了变化，而不关注具体的变更细节，通过线索单号反查数据库的方式，将最新的数据同步到ES。</p><p></p><p>3. 合并更新和统一事务</p><p></p><p>在原来的线索分配商家以及商家反馈线索接口中，存在对同一个表反复更新并且多次同步ES的问题，通过底层重构，我们把所有的DML操作合并到一个事务中，减少更新次数的同时保证了数据的正确性。</p><p></p><p>4. 非核心流程异步化</p><p></p><p>把原来线索反馈商家接口中的非核心流程异步化。在商家反馈线索状态后需要触发回流操作，回流操作本身就是一个非常耗时的操作，经常导致用户反馈接口超时，但是回流本身是用户不关注的，用户只关注他反馈的动作是否完成。因此我们对回流进行异步化，反馈线索接口现在只处理线索状态的更新，回流则是通过发送JMQ消息的方式异步处理来减少用户等待时间。</p><p></p><h3>优化成效</h3><p></p><p>经过优化，线索系统的性能得到了显著提升：</p><p></p><p>1. 接口响应时间明显缩短：</p><p></p><p>(1) 线索提交 (投放助手渠道):</p><p></p><p>优化前:(2000-4000ms)优化后:（100-300ms）</p><p></p><p>(2) 线索分配商家接口:</p><p></p><p>优化前:(1000-2000ms)优化后:（100-400ms）</p><p></p><p>(3) 商家反馈线索接口:</p><p></p><p>优化前:(1000-3000ms)优化后:（30-60ms）</p><p></p><p>2. 用户体验改善：商家在反馈线索状态时不再遇到超时问题。</p><p></p><p>3. 架构清晰：业务逻辑与数据同步逻辑解耦，代码更加清晰和容易维护。</p><p></p><p>4. 扩展性提升：异步化后的数据同步流程为未来的系统扩展提供了更大的空间。</p><p></p><h2>三. 线索列表读接口性能优化实践</h2><p></p><p></p><h4>优化前的挑战</h4><p></p><p>在运营管理CRM系统的实践中，线索列表的查询功能是不可或缺的一环，它支持基于复杂组合条件对线索数据进行精细筛选。然而，在当前的系统实现中，线索列表页面需要展示每页50条或100条线索数据时，接口性能表现并不理想：响应时间普遍超过2000毫秒，有时甚至延迟至6000毫秒。这一性能瓶颈已经引起了用户的广泛关注和较为严重的负面反馈。</p><p></p><h4>优化策略</h4><p></p><p>通过对接口的分析，接口性能瓶颈主要来源于以下几个方面：</p><p></p><p>1.多次ES查询： 先根据搜索条件查询一次ES获取基础数据后，再循环遍历列表，对每个线索再查询两次ES来获取线索的手动及自动分配商家数量。</p><p></p><p>2.频繁的RPC调用：循环遍历线索列表为每个用户进行RPC调用以获取用户昵称。</p><p></p><p>3.过多的远程调用：ES查询和获取用户昵称都是调用服务端服务。</p><p></p><p>针对这些拖慢接口性能的瓶颈点，我们采取下列优化措施:</p><p></p><p>1.减少远程调用： 我们将线索运营端多次请求服务端的过程调整成单次调用。查询逻辑都下沉到服务端，由服务端查询所有字段，运营端只需要调用一次，从而显著减少了网络延迟。</p><p></p><p>2.聚合查询优化：我们利用ES的Aggregation聚合API，一次查询获取当前分页内所有线索的手动分配和自动分配商家数量，减少了多次查询的性能损耗。</p><p></p><p>代码部分实现:</p><p></p><p><code lang="text">BoolQueryBuilder query = QueryBuilders.boolQuery();
//线索单号列表过滤
query.filter(QueryBuilders.termsQuery("clueNo", clueIds));

SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
searchSourceBuilder.trackTotalHits(true);
searchSourceBuilder.query(query);

IncludeExclude includeExclude = new IncludeExclude(new String[]{"1"}, null);
//按照分配类型聚合1
AggregationBuilder aggregation2 = AggregationBuilders.terms("distributionType").field("distributionType").includeExclude(includeExclude);
//按照线索单号聚合2
AggregationBuilder aggregation1 = AggregationBuilders.terms("clueNo").size(100).field("clueNo").subAggregation(aggregation2);
searchSourceBuilder.aggregation(aggregation1);

SearchRequest searchRequest = new SearchRequest();
searchRequest.indices(vendorClueESIndexName);
searchRequest.source(searchSourceBuilder);

SearchResponse searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);
</code></p><p></p><p></p><p>1.合理使用缓存：针对用户昵称变动频率低的特点，我们引入了缓存机制, 首次RPC查询用户的昵称成功后对结果进行缓存，再次请求时直接从缓存获取昵称，减少RPC次数。</p><p></p><p>2.并行: 在处理线索列表填充手动分配和自动分配商家数量以及用户昵称的过程中，我们使用parallelStream()并行流技术，从而加快数据处理速度。</p><p></p><p>通过以上优化方案, 对于查询100条线索需要的查询次数:</p><p></p><p>优化前: 1次ES查询列表 + 200次ES查询分配商家数量 + 100次RPC</p><p></p><p>优化后: 1次ES查询列表 + 1次ES查询商家分配数量 + 100次RPC（有缓存下会减少次数）</p><p></p><h4>优化成果</h4><p></p><p>响应时间缩短：优化后(250ms以下):</p><p></p><p></p><h2>总结与展望</h2><p></p><p>通过对线索系统的深度优化，我们不仅解决了线索系统在核心流程中的性能瓶颈，也为系统的长期健康发展奠定了基础。这一实践表明，适时地对系统架构进行优化，能够有效提升系统的性能和可维护性，进而支持业务的快速增长和变化。在未来，我们将继续追踪新的技术趋势和业务需求，不断优化我们的系统，确保它们能够支撑起日益增长的业务挑战。</p><p></p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>