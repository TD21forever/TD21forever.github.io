<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/GdVpT8LwFv9dooKjvzma</id>
            <title>13分钟颠覆传统电脑！微软Copilot+ PC 抢装GPT-4o、叫板苹果，网友不买账：用大炮打蚊子</title>
            <link>https://www.infoq.cn/article/GdVpT8LwFv9dooKjvzma</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GdVpT8LwFv9dooKjvzma</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 10:47:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PC, 人工智能, Copilot+, Windows
<br>
<br>
总结: 微软推出了专为AI设计的新型Windows PC，称为Copilot+ PC，拥有强大的AI算力和全天电池续航时间，能够实现快速访问文件、实时生成图像、实时翻译字幕等功能。用户可以通过Recall功能快速定位文件和网页，进行近乎实时的图像编辑，以及实时字幕转换。Copilot+ PC的推出意味着PC将加速人工智能创新，为用户提供更丰富的AI体验。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p></p><p>“我们正处于一个转折点，PC将加速人工智能创新。只有当云和设备协同工作时，才能实现最丰富的&nbsp;AI&nbsp;体验。”现在，微软似乎正迫切希望将生成式&nbsp;AI&nbsp;带到&nbsp;Windows以及&nbsp;PC&nbsp;运行端的最前沿。</p><p></p><p>5&nbsp;月&nbsp;20&nbsp;日，&nbsp;微软宣布推出专为&nbsp;AI&nbsp;设计的新型&nbsp;Windows&nbsp;PC，称为&nbsp;Copilot+&nbsp;PC。全程介绍共13分钟，根据微软的说法，Copilot+&nbsp;PC&nbsp;拥有强大的新芯片，硬件&nbsp;AI&nbsp;算力达40+&nbsp;TOPS、具有全天电池续航时间并能“访问最先进的&nbsp;AI&nbsp;模型”，能够完成任何其他&nbsp;PC&nbsp;无法完成的事情，如快速访问任何文件及网页、实时生成和优化图像、实时翻译字幕等一系列生成式&nbsp;AI&nbsp;驱动的功能。</p><p></p><p>“Copilot+&nbsp;PC&nbsp;是有史以来最快、最智能的&nbsp;Windows&nbsp;PC。”微软声称，搭载&nbsp;Copilot+&nbsp;PC&nbsp;的Surface&nbsp;Pro，在持续的多线程性能方面比苹果M3的&nbsp;15&nbsp;英寸&nbsp;MacBook&nbsp;Air&nbsp;高出&nbsp;58%，同时还提供更长的全天电池续航时间，本地视频播放的电池电量比MacBook&nbsp;Air&nbsp;高出20%，一次充电可提供长达&nbsp;22&nbsp;小时的本地视频播放或&nbsp;15&nbsp;小时的网页浏览。</p><p></p><p>现在微软已对外开启&nbsp;Copilot+&nbsp;PC&nbsp;的预订，起价为&nbsp;999&nbsp;美元（折合人民币约为7229元），6&nbsp;月&nbsp;18&nbsp;日开始供货。据悉，首批&nbsp;Copilot+&nbsp;PC&nbsp;将配备高通的&nbsp;Snapdragon&nbsp;X&nbsp;Elite&nbsp;和&nbsp;Plus&nbsp;芯片，并采用定制的高通&nbsp;Oryon™&nbsp;CPU，英特尔和AMD的版本会随后跟上。</p><p></p><h2>提供大量快速、实时的&nbsp;AI&nbsp;功能</h2><p></p><p></p><h2>网友：大象枪打蚊子？</h2><p></p><p>Copilot+&nbsp;PC&nbsp;利用强大的处理器和多个最先进的&nbsp;AI&nbsp;模型（包括微软的几个世界级&nbsp;SLM）来解锁新的功能，引入了可以直接在设备上本地运行的体验，以消除以前对延迟、成本甚至隐私等方面的限制，帮助提高沟通、工作效率和创造力。&nbsp;</p><p></p><p>快速定位到任何文件及网页</p><p></p><p>首先是为&nbsp;PC&nbsp;用户提供“拥有照相记忆”式体验的&nbsp;Recall，该功能可以“记住”用户几周甚至几个月前在电脑上访问或操作过的所有内容和应用程序，将允许&nbsp;Windows&nbsp;11&nbsp;用户使用记住的任何提示，都可快速直观地检索到要查找的内容。微软表示，Recall可以在颜色、图像等之间建立关联，让用户在PC上以自然语言搜索几乎任何内容。</p><p></p><p>为了发挥作用，Recall&nbsp;会记录用户在&nbsp;PC&nbsp;上执行的所有操作，包括应用程序中的活动、实时会议中的通信以及访问用于研究的网站。据介绍，Recall&nbsp;使用&nbsp;Copilot+&nbsp;PC&nbsp;高级处理功能，每隔几秒钟拍摄一次活动屏幕的图像，这些快照会被加密并保存在&nbsp;PC&nbsp;的硬盘上。用户可以使用&nbsp;Recall&nbsp;访问特定时间段的快照，为他们正在搜索的事件或时刻提供上下文。</p><p></p><p>尽管进行了加密和本地存储，但新功能依然涉及到某些&nbsp;Windows&nbsp;用户的隐私问题。对此，微软声称，与Recall相关的所有用户数据都是保密的，并且都在设备上，不会用于训练AI模型。“用户可以删除单个快照、调整和删除时间范围，或者直接从“系统托盘”暂停该任务，还可以过滤应用程序和网站，使其免于保存。”</p><p></p><p>而对于该功能，有网友评价道，&nbsp;“这似乎就像用大象枪射击蚊子，微软是否根本无法弄清楚如何进行良好的&nbsp;Windows&nbsp;搜索，所以找&nbsp;AI&nbsp;来为他们做这件事。”</p><p></p><p>近乎实时的图像编辑</p><p></p><p>现在&nbsp;Windows&nbsp;中的&nbsp;AI&nbsp;比以往任何时候都多，其中一些仅在新的&nbsp;Copilot+&nbsp;PC&nbsp;上。Copilot&nbsp;现在可以分析图像，为用户提供创意构图的想法。在&nbsp;Copilot+&nbsp;PC&nbsp;上，用户可以免费、快速地生成无穷无尽的图像，并能够根据自己的喜好微调图像。</p><p></p><p>一项名为&nbsp;Cocreator&nbsp;的功能，能够将墨迹笔触与文本提示相结合，以近乎实时的方式生成新图像，用户还可以要求&nbsp;AI&nbsp;模型按照他们正在绘制的内容来更改或重新设计图像。</p><p></p><p>Cocreator&nbsp;还将照片编辑和图像创建提升到一个新的水平。借助“重新设置图像样式”功能，用户可以结合图像生成和照片编辑进行重新构想，预设样式可用于更改背景、前景或完整图像，以创建全新的图像；还可以快速启动下一个创意项目，并使用照片中的图像创建器获取视觉灵感。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/53/534c951e0a40e8084c660336a049312b.png" /></p><p></p><p></p><p>40&nbsp;种语言的实时字幕转换</p><p></p><p>由&nbsp;NPU&nbsp;提供支持的&nbsp;Live&nbsp;Captions&nbsp;可在所有&nbsp;Copilot+&nbsp;PC&nbsp;上使用，支持约40种语言的实时字幕翻译，可以将电脑中任何应用程序或视频平台中的任何实时或预先录制的音频，转换为用户选择的语言字幕体验。并且，它还允许用户使用转录和翻译语音的人工智能功能搜索他们参加的电话会议和观看的视频。</p><p></p><p>此外，Copilot+&nbsp;PC&nbsp;还和主流应用程序合作，利用&nbsp;NPU&nbsp;的强大功能来提供新的创新&nbsp;AI&nbsp;体验。&nbsp;在&nbsp;Windows&nbsp;Studio&nbsp;Effects&nbsp;中，“快速设置”选项可通过自动调整视频通话的图像来清理视频，以缓解光线不足或添加滤镜。Adobe的旗舰应用程序也即将登陆Copilot+&nbsp;PC，包括Photoshop、Lightroom和Express。使用完全通过&nbsp;NPU&nbsp;在设备上运行的&nbsp;AI&nbsp;功能，可以对文档进行更快、更智能的注释。DaVinci&nbsp;Resolve&nbsp;Studio中使用NPU加速的Magic&nbsp;Mask，轻松将视觉效果应用于物体和人物。</p><p></p><h2>“Copilot&nbsp;+&nbsp;PC”时代将到来？</h2><p></p><p>“第一波&nbsp;Copilot+&nbsp;PC&nbsp;只是一个开始。在过去一年里，我们看到了云上人工智能的惊人创新速度，Copilot能够做我们做梦都想不到的事情。现在，我们开始了设备上&nbsp;AI&nbsp;创新的新篇章，以&nbsp;AI&nbsp;为中心，完全重新构想了整个&nbsp;PC——从芯片到操作系统、应用层到云，这标志着&nbsp;Window&nbsp;平台几十年来最重大的变化。&nbsp;”</p><p></p><p>据介绍，&nbsp;Copilot+&nbsp;PC&nbsp;连接到&nbsp;Azure&nbsp;云中运行的大型语言模型&nbsp;（LLM）&nbsp;与本地的小型语言模型&nbsp;（SLM）&nbsp;，可以实现“前所未有的性能水平”，其在运行&nbsp;AI&nbsp;工作负载时的功能增强了&nbsp;20&nbsp;倍，效率提高了&nbsp;100&nbsp;倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c8521756ebf83e671c9fc9bd9e563e1a.png" /></p><p></p><p></p><p>而且，每台&nbsp;Copilot+&nbsp;PC&nbsp;都配备了强大的&nbsp;AI&nbsp;代理，只需使用&nbsp;Copilot&nbsp;键即可访问。在接下来的几周内，微软将从合作伙伴&nbsp;OpenAI&nbsp;那里获得包括&nbsp;GPT-4o&nbsp;在内的最新模型，来支持自然语音交互。</p><p></p><p>目前，为Recall和Super&nbsp;Resolution等功能提供动力的是Windows&nbsp;Copilot&nbsp;Runtime，它由~40个生成式AI模型组成，微软将其描述为Windows的“新应用层”。Windows&nbsp;Copilot&nbsp;Runtime&nbsp;是一个基于向量的系统，可与语义索引（单个&nbsp;Copilot+&nbsp;PC&nbsp;的本地系统）配合使用，使生成式&nbsp;AI&nbsp;驱动的应用程序（包括第三方应用程序）无需互联网连接即可运行。</p><p></p><p>微软表示，TikTok&nbsp;所有者字节跳动推出的流行视频编辑器&nbsp;CapCut&nbsp;也将使用&nbsp;Windows&nbsp;Copilot&nbsp;Runtime来加速其&nbsp;AI&nbsp;功能，如结合&nbsp;Copilot+&nbsp;PC&nbsp;提供的可快速去除任何视频片段背景的自动剪切功能体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc05371f721745b402f0a7a635491ff9.png" /></p><p></p><p></p><p>除微软的&nbsp;Surface&nbsp;Pro和Surface&nbsp;Laptop&nbsp;外，其&nbsp;OEM&nbsp;合作伙伴宏碁、华硕、戴尔、惠普、联想和三星也将推出Copilot&nbsp;+&nbsp;PC体验，首批型号都已公布。Surface&nbsp;Pro&nbsp;的起价为&nbsp;1000&nbsp;美元，配备&nbsp;OLED&nbsp;显示屏和&nbsp;Snapdragon&nbsp;X&nbsp;Elite&nbsp;芯片、16GB&nbsp;RAM&nbsp;和&nbsp;512GB&nbsp;SSD&nbsp;的版本售&nbsp;1500&nbsp;美元，而同样搭载&nbsp;X&nbsp;Elite&nbsp;芯片的&nbsp;Surface&nbsp;Laptop起价为&nbsp;1299&nbsp;美元。</p><p></p><p>在微软对新款Surface设备的演示和基准测试中，Surface&nbsp;Pro与&nbsp;MacBook&nbsp;Air&nbsp;进行了多项比较，都名列前茅。近年来，Windows&nbsp;PC&nbsp;一直难以跟上苹果芯片的步伐，但在得到生成式&nbsp;AI&nbsp;技术以及高通的芯片支持后，现在似乎正迎头赶上来。</p><p></p><p>参考链接：</p><p><a href="https://www.macrumors.com/2024/05/20/microsoft-ai-windows-pcs/">https://www.macrumors.com/2024/05/20/microsoft-ai-windows-pcs/</a>"</p><p><a href="https://techcrunch.com/2024/05/20/microsoft-build-2024-windows-ai-operating-system-copilot-plus-pcs/">https://techcrunch.com/2024/05/20/microsoft-build-2024-windows-ai-operating-system-copilot-plus-pcs/</a>"</p><p><a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CmqtsoF4PPsNVChj0IlC</id>
            <title>面壁智能发布最强端侧多模态模型：超越Gemini Pro 、GPT-4V，图像编码快150倍！</title>
            <link>https://www.infoq.cn/article/CmqtsoF4PPsNVChj0IlC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CmqtsoF4PPsNVChj0IlC</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 06:21:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 端侧多模态模型, OCR 能力, 多语言支持, 系统级多模态加速
<br>
<br>
总结: 5月20日，面壁智能小钢炮 MiniCPM 系列推出端侧多模态模型MiniCPM-Llama3-V 2.5并开源，具有超越其他模型的OCR能力和支持30+多种语言的特性。此外，该模型还实现了首次端侧系统级多模态加速。 </div>
                        <hr>
                    
                    <p>5月20日，面壁智能小钢炮 MiniCPM 系列推出端侧多模态模型MiniCPM-Llama3-V 2.5并开源。据悉，该模型且支持 30+ 多种语言，并且具有以下特性：</p><p>&nbsp;</p><p>最强端侧多模态综合性能：超越多模态巨无霸 Gemini Pro 、GPT-4V；OCR 能力 SOTA！9 倍像素更清晰，难图长图长文本精准识别；图像编码快 150 倍！首次端侧系统级多模态加速。</p><p>&nbsp;</p><p>MiniCPM-Llama3-V 2.5 开源地址：</p><p><a href="https://github.com/OpenBMB/MiniCPM-V">https://github.com/OpenBMB/MiniCPM-V</a>"</p><p>&nbsp;</p><p>&nbsp;MiniCPM 系列开源地址：</p><p>&nbsp;<a href="https://github.com/OpenBMB/MiniCPM">https://github.com/OpenBMB/MiniCPM</a>"</p><p>&nbsp;</p><p>Hugging Face 下载地址：</p><p><a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5">https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>8 B 端侧模型，超越 GPT-4V、Gemini Pro</h2><p></p><p>&nbsp;</p><p>MiniCPM-Llama3-V 2.5 以 8B 端侧模型参数量级，贡献了惊艳的 &nbsp;OCR（光学字符识别）SOTA 成绩，以及端侧模型中的最佳多模态综合成绩与幻觉能力水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6cefddef794310fd72a4fbb9cb821728.png" /></p><p></p><p>模型雷达图</p><p>&nbsp;</p><p>在综合评测权威平台 OpenCompass 上，MiniCPM-Llama3-V 2.5 以小博大，综合性能超越多模态“巨无霸” GPT-4V 和 Gemini Pro。</p><p>&nbsp;</p><p>OCR（光学字符识别）是多模态大模型最重要的能力之一，也是考察多模态识别与推理能力的硬核指标。新一代 MiniCPM-Llama3-V 2.5 &nbsp;在 OCR 综合能⼒权威榜单 OCRBench 上，越级超越了 GPT-4o、GPT-4V、Claude 3V Opus、Gemini Pro 等标杆模型，实现了性能 SOTA。</p><p>&nbsp;</p><p>在评估多模态大模型性能可靠性的重要指标——幻觉能力上，MiniCPM-Llama3-V 2.5 在 Object HalBench 榜单上超越了 GPT-4V 等众多模型（注：目标幻觉率应为 0）。</p><p>&nbsp;</p><p>在旨在评估多模态模型的基本现实世界空间理解能力的 RealWorldQA 榜单上，MiniCPM-Llama3-V 2.5 再次超越 GPT-4V 和 Gemini Pro，这对 8B 模型而言难能可贵。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b111c5b4b8d7c09a28bc0464f53a9ef.jpeg" /></p><p></p><p>榜单成绩：OpenCompass | OCRBench | Object HalBench | RealWorldQA</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>快 150 倍！首次端侧系统级多模态加速</h2><p></p><p>&nbsp;</p><p>面壁智能首次进行端侧系统加速，MiniCPM-Llama3-V 2.5 目前已可以高效部署在手机端。</p><p>&nbsp;</p><p>在图像编码方面，面壁首次整合 NPU 和 CPU 加速框架，并结合显存管理、编译优化技术，在 MiniCPM-Llama3-V 2.5 图像编码方面实现了 150 倍加速提升。</p><p>&nbsp;</p><p>在语言模型推理方面，目前开源社区的报告结果中，Llama 3 语言模型在手机端侧的解码速度在 0.5 token/s 上下，相比之下，多模态大模型的端侧运行面临着更大的效率挑战，经过 CPU、编译优化、显存管理等优化方式，将 MiniCPM-Llama3-V 2.5 在手机端的语言解码速度提升到 3-4 token/s。</p><p>&nbsp;</p><p>有别于常见的中英双语模型，MiniCPM-Llama3-V2.5 可支持 30+ 多种语言，包括德语、法语、西班牙语、意大利语、俄语等主流语言，基本覆盖一带一路国家。</p><p>&nbsp;</p><p>基于自研的跨语言泛化技术，仅通过少量翻译的多模态数据的指令微调，就可对多语言多模态对话性能高效泛化。</p><p><img src="https://static001.geekbang.org/infoq/ef/ef7d51f983e0e6c6777cb6e95f765443.png" /></p><p>多语言版本 LLaVABench 评测结果</p><p></p><h2>9 倍像素更清晰，难图长图长文本精准识别</h2><p></p><p>&nbsp;</p><p>OCR 技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V 2.5 对于难图、长图、长文本的精准识别，再度带来出众表现。</p><p>&nbsp;</p><p>面壁自研了高清图像高效编码技术，可以高效编码及无损识别 180 万高清像素图片，并且支持任意长宽比，包括 1:9 极限比例，突破了传统技术仅能识别 20 万像素小图的瓶颈。</p><p>&nbsp;</p><p>另外，MiniCPM-Llama3-V 2.5 在复杂推理能力上进一步突破：可更好地深入洞察图像，在更复杂、更接近人类的水平上进行思考和解决问题。该模型不仅能理解单一文本或图像等模态信息，还能跨越不同模态间的综合信息，做出更准确和深入的分析。</p><p>&nbsp;</p><p>比如，给定一张充满繁密字迹的建筑风景图，人眼难以辨别，但 MiniCPM-Llama3-V 2.5 能够一眼看懂其中的《三体》主题，还能正确推理出这些建筑是为了纪念《三体》及其对中国科幻文学的贡献而设计：</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da02c74f80c2103e8c3cea19203903d6.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b7730f320fa4310a89ea96be15892fb.png" /></p><p></p><p>&nbsp;</p><p>把同样的问题抛给GPT-4 V ，结果并不理想：</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84b52f54f4adb65f79995e6eb95858d5.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>另外，识别包含复杂逻辑的流程图是多模态模型推理能力的直观体现，MiniCPM-Llama3-V 2.5 不仅能够看懂流程图中不同模块的文字、箭头之间的空间位置和复杂逻辑关系，还能给出清晰易懂的解释说明：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/6479ab28998fae045f3b18437e463dfe.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e98652c6def8a66d38b3370559c87a89.png" /></p><p></p><p>&nbsp;</p><p>全文OCR能力方面，输入一张手机拍摄的火车票，MiniCPM-Llama3-V 2.5 也能准确提取信息，给出无误的"json"格式输出：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/817cfe9439bae162fb57590646701056.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF</id>
            <title>阿里通义千问GPT-4级主力模型降价97%，1 块钱200万 tokens</title>
            <link>https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GF4Jqtkgho4EhcsoYFLF</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 06:18:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, 通义千问, GPT-4, Qwen-Long
<br>
<br>
总结: 阿里云推出通义千问GPT-4级主力模型Qwen-Long，降价幅度高达97%，使得大模型应用进入密集探索期，推动AI应用爆发。通过公共云+API方式，企业可以以更低的成本和更高的性能使用大模型，实现多模型调用和数据安全保障。 </div>
                        <hr>
                    
                    <p>5月21日，阿里云抛出重磅炸弹：通义千问GPT-4级主力模型Qwen-Long，API输入价格从0.02元/千tokens降至0.0005元/千tokens，直降97%。这意味着，1块钱可以买200万tokens，相当于5本《新华字典》的文字量。这款模型最高支持1千万tokens长文本输入，降价后约为GPT-4价格的1/400，击穿全球底价。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db5368ebb70e7b548c0326d5516f6bf9.png" /></p><p>&nbsp;</p><p>Qwen-Long是通义千问的长文本增强版模型，性能对标GPT-4，上下文长度最高达1千万。除了输入价格降至0.0005元/千tokens，Qwen-Long输出价格也直降90%至0.002元/千tokens。</p><p>&nbsp;</p><p>相比之下，国内外厂商GPT-4、Gemini1.5 Pro、Claude 3 Sonnet及Ernie-4.0每千tokens输入价格分别为0.22元、0.025元、0.022元及0.12元，均远高于Qwen-long。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a0/a045c7a220a0cb5a9daed8f8dffaa5af.png" /></p><p>&nbsp;</p><p>通义千问本次降价共覆盖9款商业化及开源系列模型：</p><p>&nbsp;</p><p>通义千问商业化模型：Qwen-Turbo、Owen-Plus、Qwen-Long、Qwen-Max；通义千问开源模型：Qwen1.5-7B、Qwen1.5-14B、Qwen1.5-32B、Qwen1.5-72B、Qwen1.5-110B。</p><p></p><p>其中，不久前发布的通义千问旗舰款大模型Qwen-Max，API输入价格降至0.04元/千tokens，降幅达67％。Qwen-Max 在权威基准OpenCompass上性能追平GPT-4-Turbo，并在大模型竞技场Chatbot Arena中跻身全球前15。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43373d17d98dab573eed782cd21c8b9c.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>业界普遍认为，随着大模型性能逐渐提升，AI应用创新正进入密集探索期，但推理成本过高依然是制约大模型规模化应用的关键因素。</p><p>&nbsp;</p><p></p><h2>“公共云+API”企业大模型主流应用方式</h2><p></p><p>&nbsp;</p><p>在武汉AI智领者峰会现场，阿里云智能集团资深副总裁、公共云事业部总裁刘伟光表示：“作为中国第一大云计算公司，阿里云这次大幅降低大模型推理价格，就是希望加速AI应用的爆发。我们预计未来大模型API的调用量会有成千上万倍的增长。”</p><p>&nbsp;</p><p>刘伟光认为，不管是开源模型还是商业化模型，公共云+API将成为企业使用大模型的主流方式。</p><p>&nbsp;</p><p>首先，公共云的技术红利和规模效应，带来巨大的成本和性能优势。</p><p>&nbsp;</p><p>刘伟光介绍，阿里云可以从模型自身和AI基础设施两个层面不断优化，追求极致的推理成本和性能。阿里云基于自研的异构芯片互联、高性能网络HPN7.0、高性能存储CPFS、人工智能平台PAI等核心技术和产品，构建了极致弹性的AI算力调度系统，结合百炼分布式推理加速引擎，大幅压缩了模型推理成本，并加快模型推理速度。</p><p>&nbsp;</p><p>即便是同样的开源模型，在公共云上的调用价格也远远低于私有化部署。以使用Qwen-72B开源模型、每月1亿tokens用量为例，在阿里云百炼上直接调用API每月仅需600元，私有化部署的成本平均每月超1万元。</p><p>&nbsp;</p><p>其次，云上更方便进行多模型调用，并提供企业级的数据安全保障。</p><p>&nbsp;</p><p>刘伟光表示，阿里云可以为每个企业提供专属VPC环境，做到计算隔离、存储隔离、网络隔离、数据加密，充分保障数据安全。目前，阿里云已主导或深度参与10多项大模型安全相关国际国内技术标准的制定。</p><p>&nbsp;</p><p>最后，云厂商天然的开放性，能为开发者提供最丰富的模型和工具链。</p><p>&nbsp;</p><p>刘伟光表示，阿里云百炼平台上汇聚通义、百川、ChatGLM、Llama系列等上百款国内外优质模型，内置大模型定制与应用开发工具链，开发者可以便捷地测试比较不同模型，开发专属大模型，并轻松搭建RAG等应用。从选模型、调模型、搭应用到对外服务，一站式搞定。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/b1be8e2769248aa344e6d8070</id>
            <title>KubeAI大模型推理加速实践｜得物技术</title>
            <link>https://www.infoq.cn/article/b1be8e2769248aa344e6d8070</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/b1be8e2769248aa344e6d8070</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 02:13:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 推理速度, 加速优化, 注意力计算
<br>
<br>
总结: 本文分享了在部署大模型推理集群时的经验，探讨了大模型推理速度提升的方法，介绍了一些业界内的大模型加速技术，以及大模型发展面临的挑战和优化方向。其中，注意力计算是推理过程中最耗时的部分，针对其进行速度优化可以显著提高整体推理性能。 </div>
                        <hr>
                    
                    <p>一、背景</p><p>最近我们在生产环境批量部署了大模型专用推理集群，并成功让包括70B在内的大模型推理速度提升50%，大幅缩减部署成本，稳定应用于生产环境。本文基于我们在部署大模型推理集群时的一些经验，分享一些有效提升大模型的推理速度方法。最后，我们在结尾处推荐了几个经过我们评测且表现优异的大模型推理框架。希望这些建议能帮助读者在项目中选择适合自己的推理框架。</p><p>OpenAI的科学家Hyung Won Chung在2023年的公开演讲《Large Language Models》[8]中指出，大模型的某些能力仅在达到特定规模时才能显现，可见未来大模型的参数量肯定会越来越大，这也是大模型的发展趋势。随着参数量的增加，对大模型的推理速度要求越来越高，有哪些方法可以提高大模型的推理速度或吞吐量？</p><p>首先我们将探讨大模型的加速优化方向，随后文章将依据时间线，介绍一些业界内较为经典的实用大模型加速技术，包括但不限于“FlashAttention[1]”和“PageAttention[3]”等技术。</p><p>以下为按时间顺序业界的一些经典大模型推理加速技术，本文试图为读者提供一个按时间发展顺序的大模型加速方法综述。</p><p><img src="https://static001.geekbang.org/infoq/54/548ea315bfa45754e44fbba404aeeb7d.png" /></p><p></p><p>除了上面提到的技术外，提高大模型推理速度的还有大模型的量化技术等，这里先不探讨，后面有机会，我们会单独发文章来介绍。</p><p>二、大模型发展面临的挑战</p><p>未来大模型的参数量肯定会越来越大，这也是大模型的发展趋势，对推理加速的要求会越来越高。</p><p>OpenAI在其论文《Scaling Laws for Neural Language Models》[7]中介绍了大模型的扩展规则，这些规则阐释了模型能力与其规模之间的关系。具体来说，模型的能力强烈依赖于其规模，包括模型参数的数量，数据集的大小，以及训练过程中所需的计算量。此外，OpenAI的科学家Hyung Won Chung在2023年的公开演讲《Large Language Models》[8]中指出，大模型的某些能力仅在达到特定规模时才能显现。</p><p><img src="https://static001.geekbang.org/infoq/a3/a3bd942a70296078aca91f594a349071.png" /></p><p></p><p>上图摘自Hyung Won Chung演讲中的ppt[8]。图中主要表达一个观点，随着模型规模的增大，比如GPT3到GPT4，模型的能力变的越来越强，甚至会出现新的能力。</p><p>但是随着模型的规模增大，大模型的推理速度将会逐渐降低，这是因为更多的参数量需要更多的GPU计算。推理速度的下降进一步带来更差的用户体验，因此如何对大模型推理加速变得越来越重要了。</p><p><img src="https://static001.geekbang.org/infoq/7a/7a9fe75dd98901632579dd43ac1f09d5.png" /></p><p></p><p>三、大模型推理加速的优化方向</p><p>Llama2的模型结构</p><p>我们先简单了解一下Llama 2模型系列的结构，参考自Llama 2的论文[9]。目前，像Llama系列这样的大多数生成式语言模型，主要采用了Transformer架构中的Decoder模块。在Huggingface平台上，这类模型结构通常被称作CausalLM，即因果语言模型。</p><p><img src="https://static001.geekbang.org/infoq/8a/8a3b6aff983a45c63eee6b7e0b391361.png" /></p><p></p><p>上图为Llama2大模型的结构，其中最核心的是注意力计算(Llama Attention)。这也是整个推理过程中最耗费时间的模块，后面的优化大部分都是基于Attention去实施的。为了更好的理解Llama 2大模型的结构，我们先简单对Llama2模型的整个推理过程进行拆解，不感兴趣同学可以直接跳过。</p><p>用户向模型提交Prompt后，模型首先进行的操作是预测下一个字符(Token)，并将预测出的字符添加到输入中继续进行预测。这个过程会一直持续，直到模型输出一个停止符号(STOP token)，此时预测停止，模型输出最终结果。在生成下一个字符(Token)的过程中，模型需要执行N次的Llama解码器层(Llama Decoder Layer)计算。具体来说，Llama-2-7B模型执行32次计算，而Llama-2-13B模型执行40次。Llama解码器层(Llama Decoder Layer)中最关键的计算环节是注意力(Llama Attention)的计算。大部分推理时间都消耗在Attention的计算上，因此多种优化技巧都旨在提高Attention计算的效率。</p><p>大模型推理的加速方向有哪些</p><p>从Llama 2模型的结构分析中，我们可以总结出大模型在推理计算过程中表现出以下特点：</p><p>在整个推理过程中，最耗时的部分为注意力(Attention)计算。针对Attention的计算进行速度优化，可以显著提高整体推理性能。注意力(Attention)计算过程中，键值对缓存(KV Cache)占用了大量显存资源。以13B模型为例，处理一个Prompt序列大约需要3GB额外显存，并且这部分显存会被频繁地分配和释放，产生大量碎片，如果能减少显存碎片，也能提升大模型的吞吐。推理过程GPU需要处理和计算大量的参数。7B模型拥有70亿参数，而13B模型则包含130亿参数，最新全球最强大模型DBRX更是高达1300亿参数，这需要高效地处理这些参数。这里也可以有优化空间。</p><p>针对上述三个特性，目前业界提出了多种有效的优化方法，典型如下：</p><p><img src="https://static001.geekbang.org/infoq/eb/eb4f7ca7ad5e3797a2515d083e409230.png" /></p><p></p><p>1. FlashAttention-Attention计算速度优化</p><p>FlashAttention[1]在不改变Attention算子的计算结果的前提下，提升Attention算子的计算速度。FlashAttention在各种模型和任务上展示了显著的性能提升。例如，在BERT-large、GPT-2等模型上，相比于基线实现，FlashAttention能够实现15%到3倍的端到端加速。</p><p>2. PageAttention-KV Cache显存管理优化</p><p>PageAttention[3]的目标是减少显存碎片，基于PageAttention的VLLM系统能够将流行的大型语言模型（LLM）的吞吐量提高到10倍以上，同时保持耗时分布平稳。</p><p>3. MOE-缩减推理时模型参数</p><p>MOE(Mixture of Experts)[4]目标是减少模型推理时参与计算的参数量。</p><p>实验效果：Mixtral模型在多数基准测试中表现优于Llama 2 70B模型，并且其推理速度比后者快了6倍。该模型支持多种语言，具有强大的代码生成能力，并可以细化配置以遵循具体指令，从而在MT-Bench基准测试中取得了高分。</p><p>后面我们将针对上面的每个方向详细介绍。</p><p>四、FlashAttention-Attention算子计算优化</p><p>FlashAttention先后发表了两篇论文阐述对Attention算子的优化，包括FlashAttention-1[1]与FlashAttention-2[2]，我们以FlashAttention-1[1]为例了解下他的优化原理。</p><p>我们先了解下GPU的内存分层结构，参考下图，图片来自论文FlashAttention-1[1]。</p><p><img src="https://static001.geekbang.org/infoq/e3/e35f218aafbbd8d169ec2d9605d3532e.jpeg" /></p><p></p><p>GPU的内存层次结构由三个主要部分组成：SRAM、HBM和DRAM，下面为A100GPU的参考配置。</p><p>SRAM（静态随机访问存储器）具有最快的访问速度（19TB/s），但其容量相对较小（仅20MB）。</p><p>HBM（高带宽存储器）提供较大的存储空间（40GB）和高速的数据访问（1.5TB/s）。</p><p>DRAM（动态随机访问存储器），在这里特指GPU外部的主存，容量最大（超过1TB），但访问速度最慢（12.8GB/s）。</p><p>从上述配置中可以看出，内存容量越小，处理速度就越快。</p><p><img src="https://static001.geekbang.org/infoq/1b/1b9f6204e93f189a1583b1b964ce0546.png" /></p><p></p><p>在传统的Attention计算过程中，大量的输入/输出操作都是通过访问HBM来完成的。FlashAttention算法通过优化Attention计算流程，减少了对HBM的访问次数，以提高计算效率，所以它是一种IO感知的优化算法。</p><p>下图为FlashAttention的加速方法，来自论文FlashAttention-1[1]</p><p><img src="https://static001.geekbang.org/infoq/4e/4e0389e8e85ff04408383d40ecf2bb10.jpeg" /></p><p></p><p>FlashAttention利用了一个聪明的技巧来快速且内存高效地计算注意力机制，即它通过将输入数据分块（tiling）来避免一次性处理整个巨大的注意力矩阵，这通常需要大量的内存和计算资源。想象一下，我们有一个巨大的图书馆(矩阵)，而FlashAttention的方法就像是把图书馆里的书分成几个小堆，然后每次只处理一堆书。这样，我们就不需要一次性把所有书都拿出来放在桌子上（这需要很大的桌子和很多时间）。</p><p>具体来说，FlashAttention在做矩阵计算的时候，通过将数据分块并利用GPU上的快速但容量较小的存储（SRAM）去计算，有效减少了对慢速但容量大的存储（HBM）的访问。这样不仅加快了计算速度，而且大幅减少了显存的需求。</p><p>通过减少对慢速存储的依赖，FlashAttention能够显著提高模型训练的速度，同时保持或甚至提高模型的性能。例如，让BERT-large的训练比MLPerf 1.1的记录快15%，GPT-2训练速度是HuggingFace和Megatron-LM基线的三倍，长序列领域训练速度提升至2.4倍。</p><p>下图来自huggingface 对flash attention介绍的blog[14]，可以更好的理解Flash Attention对矩阵拆分的方式。</p><p><img src="https://static001.geekbang.org/infoq/b0/b0de9c9031ff76416e0ebe42e0c4f863.png" /></p><p></p><p>既然Flash Attention可以加速计算，那么支持Flash Attention计算的框架包括都有哪些，文章后半部我们会推荐一些比较优秀的推理框架。</p><p>五、PageAttention-显存管理优化</p><p>PageAttention[3]的概念最初由VLLM的作者Woosuk Kwon提出，它也是VLLM推理框架的最主要的优化策略。Woosuk Kwon在其论文中介绍了如何通过PageAttention来解决大型语言模型（LLM）服务中的一个关键问题——在不增加延迟的情况下有效管理内存以提升吞吐量。</p><p>我们先了解下大模型在推理的情况下的内存结构分布，下图来自论文[3]。</p><p><img src="https://static001.geekbang.org/infoq/bb/bbdede18f2c3f4b05325ec44e650adb9.jpeg" /></p><p></p><p>这是一个在NVIDIA A100上服务一个拥有13B参数的大型语言模型的内存布局，13B LLM 推理显存占用分部，13B LLM的参数占用26G显存，每个请求，KV Cache会占用12G显存，随着QPS的增加，KVCache会快速上升，并且会被频繁的分配与释放，系统会产生大量的显存碎片，如果不加处理，系统就会慢慢崩掉。</p><p><img src="https://static001.geekbang.org/infoq/c2/c24d910f5dcc02c410af0aa0b38c976b.png" /></p><p></p><p>那么VLLM是如何通过PageAttention解决显存碎片的问题的呢？下图来自文章[14]，为VLLM的显存管理技术。</p><p><img src="https://static001.geekbang.org/infoq/f6/f6de234255145832df46f30827cf0ed0.png" /></p><p></p><p>PageAttention的工作原理是通过将键值缓存（KV缓存）分割成固定大小的块（或“页面”），并允许这些块在内存中非连续地存储。这种方法灵感来源于操作系统的虚拟内存和分页技术，目的是为了更灵活和高效地管理内存资源。</p><p>在传统的注意力机制中，一个请求的KV缓存需要在内存中连续存储，这会导致两个主要问题：内存碎片化和无法高效共享内存。内存碎片化限制了批处理的大小，而无法共享内存则导致重复数据，浪费宝贵的内存资源。</p><p>PageAttention通过以下步骤工作来解决这些问题：</p><p>分割KV缓存：将每个请求的KV缓存划分为多个较小的块，这些块的大小是固定的，可以根据模型和硬件的具体需求进行调整。非连续存储：与传统KV缓存块在内存中连续存储不同，PageAttention允许这些块在物理内存中非连续地分布。这样，就可以根据实际需要动态地分配和回收内存块，减少内存浪费。动态管理：通过类似于操作系统中虚拟内存管理的方式，PageAttention动态地管理这些内存块。系统可以根据当前的内存使用情况，按需分配或释放KV缓存块，从而优化内存使用。内存共享：PageAttention还支持在不同请求之间或同一个请求中的不同序列之间共享KV缓存块。这种共享是灵活的，可以基于块级别进行，进一步减少内存使用和提高效率。</p><p>通过这种方式，PageAttention允许LLM服务系统在保持相同延迟的情况下，通过减少内存浪费和提高内存共享，显著提高处理请求的吞吐量。</p><p>通过PageAttention的优化，VLLM对LLaMA 7B与13B的吞吐量提升了10倍以上，下图来自文章[11]。</p><p><img src="https://static001.geekbang.org/infoq/7f/7f87ec9b5fa68b86bd74281f58645a6b.png" /></p><p></p><p>六、MOE-缩减推理时模型参数</p><p>最近发布的全球最强开源大模型1300亿参数的DBRX，以及Mistral的8x7B开源大模型都是基于MOE架构的。为什么参数量越大的模型越要使用MOE架构呢？我们以Mistral的8x7B开源大模型为例，介绍下MOE架构在性能方面的优势。</p><p><img src="https://static001.geekbang.org/infoq/b0/b08c6ad09f22721ccc236b0ce553916f.jpeg" /></p><p></p><p>说到MOE大模型，我们先对比下普通大模型与MOE大模型在结构上的区别，参考上图。在MOE大模型中，把大模型的参数分成了8个小组外加一个路由器，每个小组我们称作专家组。当请求过来的时候，MOE大模型则先有路由器从8个专家组中选择两个，只有这两个专家组参与了计算。而对比普通大模型，则需要所有参数都参加GPU计算。</p><p>所以MOE大模型要比同等级的普通大模型推理速度快四倍左右。</p><p>我们来看下Mistral MOE的实现，Mistral MOE是由mistral.ai发布的8*7B大模型[12]，下图来自论文[12]，是其8*7B大模型的专家层的结构。</p><p><img src="https://static001.geekbang.org/infoq/09/099d8d251965d1cfcb4a39c8b79c4e9f.jpeg" /></p><p></p><p>Mixtral 8x7B是一个稀疏混合专家（Sparse Mixture of Experts, SMoE）语言模型，它基于Mistral 7B的架构，但每一层都由8个前馈块（即专家）组成。在处理每个令牌时，每层的一个路由网络会选择两个专家来处理当前状态并结合它们的输出。虽然每个令牌只与两个专家交互，但在每个时间步骤中选取的专家可以不同，因此每个令牌可以接触到47B的参数，但在推理过程中只使用13B的活跃参数。</p><p>Mixtral在多项基准测试上展示了其卓越的性能，尤其是在数学、代码生成和多语言理解方面。相比于Llama 2 70B和GPT-3.5，Mixtral在大多数评估指标上表现出类似或更优的性能。具体来说，Mixtral使用的活跃参数（13B）比Llama 2 70B（70B）少5倍，但在几乎所有类别中的表现都更好或相当。</p><p>MOE大模型实现了增加参数量的同时，推理速度并不降低，是未来大模型的发展趋势。</p><p><img src="https://static001.geekbang.org/infoq/47/47f989f8b43a06ac47ac1da7e76ef7a6.png" /></p><p></p><p>七、Tensor parallelize-张量并行</p><p>如果你有多卡GPU，可以采用张量并行进一步加速大模型的推理速度。</p><p>想象一下，你有一本非常厚的书，你想一次性复印整本书，但是你的复印机一次只能复印几页。这时，你可以把这本书分成几个部分，每个部分分别复印，最后再把所有复印好的部分按顺序拼接起来，这样就完成了整本书的复印。</p><p>在张量并行中，我们要处理的大模型就像是那本厚书，而GPU则像是复印机。因为单个GPU无法一次处理整个大模型，我们就需要把模型（在这个例子中是权重张量）分成几个部分，让不同的GPU分别处理（相当于复印书的不同部分）。在处理输入数据时，就像是把书的每一页分别复印，然后再把复印好的各个部分拼接起来，形成完整的输出结果。</p><p>这样，通过分担工作，多个GPU协同完成了一个本来单个GPU无法完成的大任务。这就是张量并行的工作方式，它让我们能够处理那些非常大的模型。</p><p><img src="https://static001.geekbang.org/infoq/23/23ed8ca8a9e06e2a403c429a81f45789.png" /></p><p></p><p>图片来自文章[13]</p><p>张量并行技术用于将大模型分布式地部署在多个GPU上。以矩阵乘法来打个比方，当输入张量与第一个权重张量进行矩阵相乘时，这个操作可以视作先将权重张量按列进行分割，接着将分割后的每列分别与输入张量相乘，然后把这些乘积结果合并。这些合并后的输出会被从GPU中导出，并聚合以形成最终的输出结果，过程上图，参考文章[13]。</p><p>八、推理框架推荐</p><p>在前文中，我们探讨了几种加速和优化技术，诸如Flash Attention、Page Attention、MOE以及张量并行技术。接下来，基于我们自身的实际操作和评估，我们将向您推荐一些当前表现较为出色的推理框架。</p><p><img src="https://static001.geekbang.org/infoq/83/83ae59e30f1000c0fe189cd1fc71bd7b.png" /></p><p></p><p>九、总结与展望</p><p>在本文中，我们深入探讨了一系列旨在提升大模型推理速度的技术和方法，包括但不限于Flash Attention、Page Attention、MOE以及张量并行技术。通过在生产环境中批量部署专用大模型推理集群，我们成功地将包括70B规模模型在内的推理速度降低了50%，稳定地应用这些技术于生产环境，从而证明了这些优化方法的有效性和实用性。</p><p>随着大型模型在各个领域的应用越来越广泛，如何有效地提升推理速度、降低推理成本成为了一项挑战。我们的实践不仅展示了目前可用的一些加速技术，还基于我们的经验，推荐了几款经过评测表现优秀的大模型推理框架。这些建议旨在帮助读者在面对众多选择时，能够挑选出最适合自己需求的推理框架。</p><p>展望未来，随着技术的不断进步和新算法的不断涌现，我们相信还会有更多的加速优化技术被开发出来，进一步推动大模型推理效率的提升。最后，我们也期待未来有机会深入探讨和介绍更多提升大模型推理速度的新技术和方法。</p><p></p><p>参考资料</p><p>[1] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(https://arxiv.org/abs/2205.14135)</p><p>[2] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning(https://arxiv.org/abs/2307.08691)</p><p>[3] Efficient Memory Management for Large Language Model Serving with PagedAttention(https://arxiv.org/abs/2309.06180)</p><p>[4] mixtral-of-experts(https://mistral.ai/news/mixtral-of-experts/)</p><p>[5] Mixtral of Experts(https://arxiv.org/abs/2401.04088)</p><p>[6] MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads(https://arxiv.org/pdf/2401.10774.pdf)</p><p>[7] Scaling Laws for Neural Language Models(https://arxiv.org/pdf/2001.08361.pdf)</p><p>[8] Hyung Won Chung(OpenAI), Large Language Models (in 2023) , talked at Seoul National University</p><p>[9] Llama 2: Open Foundation and Fine-Tuned Chat Models(https://arxiv.org/abs/2307.09288)</p><p>[10] Attention Is All You Need(https://arxiv.org/pdf/1706.03762.pdf)</p><p>[11] https://blog.vllm.ai/2023/06/20/vllm.html</p><p>[12] https://arxiv.org/pdf/2401.04088.pdf</p><p>[13] https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism</p><p>[14] https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention</p><p>[15] https://blog.vllm.ai/2023/06/20/vllm.html</p><p>*文/&nbsp;linggong</p><p>本文属得物技术原创，更多精彩文章请看：<a href="https://tech.dewu.com/">得物技术</a>"</p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uolKho9bFBB7eBzqdKO6</id>
            <title>将大模型疯狂用到军事上，这家企业创始人“疯了”？</title>
            <link>https://www.infoq.cn/article/uolKho9bFBB7eBzqdKO6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uolKho9bFBB7eBzqdKO6</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 May 2024 01:25:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI应用于战争, Palantir, GPT-4, 人工智能平台
<br>
<br>
总结: 一家软件公司将AI产品供应给以色列国防军，与微软等企业合作在政府云中部署大型语言模型，展示了AI在军事领域的潜力。在会议上，Palantir CEO和其他高官讨论未来战争，展示了对暴力和战争的态度和看法。同时，会议也反映了人们对技术在战争中的重要性和影响的不同看法。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>讨论将AI应用于战争，这家企业“疯了”！</blockquote><p></p><p>&nbsp;</p><p>5月7号到8号，华盛顿特区举办了一场特别的大会。此次会议的主赞助商则是由美国企业家与风险资本家、政治活动家、PayPal联合创始人Peter Thiel参与创立的软件厂商Palantir，这家公司因在2019年疫情封闭期间联手移民和海关执法局（Ice）组织抗议活动而闻名。目前，Palantir正在向以色列国防军供应部分AI产品。</p><p>&nbsp;</p><p>此外，微软、谷歌、亚马逊、Groq等20多家企业也是这场大会的赞助商。</p><p>&nbsp;</p><p>当时，微软宣布已在隔离的Azure 政府绝密云中部署了 GPT-4 大型语言模型，供国防部使用。一旦该工具获得认可，五角大楼官员将能够在安全的环境中使用该技术。这个消息在当时被广泛关注，但更多主题可能被大家忽略。</p><p>&nbsp;</p><p>亲身参加了这次会议的外媒记者 Caroline Haskins 近日发文详细记录了自己的所见所闻。他说到，会议厅里满是代表美国军方及其数十家承包商的展位，其中包括博思艾伦公司，还有某家可称为“飞机软件界的Uber”的企业。</p><p>&nbsp;</p><p>“在这样的行业内部会议上，权贵们往往更加直言不讳、疏于掩饰——他们自认为身处安全空间，毕竟身边全都是朋友和同事。”Haskins 表示，“这也让我加倍好奇，他们对于加沙地带的AI驱动暴力行动，乃至于战争的未来形态究竟有何看法？”</p><p>&nbsp;</p><p>与会者们被告知，会议最大的亮点就是主厅后方房间里的一连串展板。事实上，那个房间专门展示一个主题，其余部分则由 Schmidt 本人和Palantir公司CEO Alex Karp主持的fire-breathing定基调。</p><p>&nbsp;</p><p>值得注意的是，2023年9月，《时代》周刊发布了首届全球百大AI人物，Karp 被评为全球AI领袖。在此前联名呼吁“暂停人工智能（AI）研究”的活动中，Karp反驳了公开反驳，并回怼那是因为他们自己没有AI产品。</p><p>&nbsp;</p><p>今年4月底，Palantir 推出了自己最新的生成式人工智能平台，该平台理论上可以为军事指挥官提供军事决策、命令下达、作战监控，实现优化决策流程、缩短决策时间、获得最优作战方案、保障作战质量的目标。Palantir 与美国政府的合作一直都很紧密。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69b0109c6a73ae7620105dd76556d606.png" /></p><p></p><p>Palantir联合创始人兼CEO Alex Karp与海军上将Tony Radakin</p><p>&nbsp;</p><p>这场大会把参与者们划分成了两类：一类将战争视为经济与战略问题，另一类将战争理解为死亡问题。Haskins表示，与会的大多数人都属于前一类。</p><p>&nbsp;</p><p>“我之前就一直在关注科技企业与军方机构之间的关系，所以本来不该对在这次会议上看到或听到的任何情况感到意外。但在会议结束、离开华盛顿返回家中时，我感到自己的生命力似乎已经被这场狂暴的活动抽干了。”Haskins写道。</p><p>&nbsp;</p><p>Haskins究竟看到了什么？下面我们跟随Haskins的视角，近距离看看会议上的权贵们都说了些什么。</p><p>&nbsp;</p><p></p><h2>Palantir CEO的“高谈阔论”</h2><p></p><p>&nbsp;</p><p>跟着大群观众，我们穿过主厅去观看核心议题的小组讨论。Karp 和 Schmidt连同CIA中情局副局长Divad Cohen，以及负责战争事务的高级官员Mark Milley在这里发表了讨论。</p><p>&nbsp;</p><p>Schmidt自我介绍时麦克风意外故障，于是Cohen把自己的麦克递了过去。Schmidt开玩笑说，“中情局总能及时伸出援手。”在接下来的90分钟里，讨论就在这样的轻松氛围下进行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5df066127e1c0978d7635fcc715e0a7.png" /></p><p></p><p>Eric Schmidt、David Cohen、Mark Milley将军、Palantir公司CEO Alex Karp以及Andrew Ross Sorkin齐聚小组讨论现场</p><p>&nbsp;</p><p>当主持人询问小组成员对于未来战争的看法时，Schmidt和Cohen都回答得相当谨慎。</p><p>&nbsp;</p><p>而向来以态度激进闻名的Karp则极度纵容暴力，而且不吝以饥渴的眼神凝视观众，希望从我们身上得到或赞许、或震惊的明确反馈。</p><p>&nbsp;</p><p>这位AI领袖首先表示，美国必须在战中“吓死我们的对手。”在谈到哈马斯去年10月7日对以色列的袭击时，他指出“如果我们美国遭遇到类似的突袭，那地球表面就会多出一个大洞。”Karp坚称，“和平活动家就是战争活动家。所以我们明显属于和平活动家。”</p><p>&nbsp;</p><p>随着Karp的加入，Milley的言辞也愈发夸张。在讨论结束时，他甚至直接将反对加沙战争的美国人称为“恐怖组织的支持者。”</p><p>&nbsp;</p><p>Schmidt同时提到了无人机和自动化技术在战争中的重要性。他本人也在低调创办自己的军用无人机公司。</p><p>&nbsp;</p><p>Cohen则敦促大家将10月7日的袭击视为对军事环境中技术升级的“重大警告”。Cohen指出，尽管以色列在防御的监控技术方面“投入了大量资金”，但仍然未能阻止垄断。“我们确实要保持一点谦逊之心。”</p><p>&nbsp;</p><p>但从多数发言者的观点来看，人们普遍比Cohen更加乐观，只是觉得系统故障属于技术问题，用更多、更新的技术就可以解决。</p><p>&nbsp;</p><p>我带着麻木的脑袋和僵硬的身体离开了小组讨论现场。</p><p>&nbsp;</p><p>我无意间听到了身边其他与会者们那冷漠的话语。他们有为这么恐怖的发言而震撼吗？完全没有，他们在讨论午餐、周末去哪玩，还有下一场小组讨论的内容。我们似乎完全生活在不同的世界里。</p><p>&nbsp;</p><p></p><h2>“我就是新时代的奥本海默”</h2><p></p><p>&nbsp;</p><p>在到处转悠了大概10分钟之后，我给手机充上电，然后向身边的另一位与会者主动打了招呼。这是位50多岁的男性，我问他对这场小组讨论怎么看，他温和地微笑回应，说Milley的二战观点很“有趣”。</p><p>&nbsp;</p><p>他问道，“你看过‘奥本海默’吗？”</p><p>&nbsp;</p><p>我说，“没有，但我读过《原子弹的诞生（The Making of the Atomic Bomb）》。”</p><p>&nbsp;</p><p>我以为他想聊聊那帮制造战争武器的家伙有多傲慢。但相反，他说他在洛斯阿拉莫斯实验室从事核武器研究。他把手伸进背包，给我拿了几根带着阿拉莫斯logo的钢笔的贴纸。</p><p>&nbsp;</p><p>他没有透过太多关于工作的细节，但给我看了几眼他花钱租来的豪车。聊了几分钟后，他开始收拾东西，并突然笑着说，“我刚刚想到一件事，我就是新时代的奥本海默！”</p><p>&nbsp;</p><p>看着他返回阿拉莫斯实验室的展位，我勉强挤出礼貌的微笑。</p><p>&nbsp;</p><p>在整场会议期间，我游走于不同的展位，并最终遇到了两个校友。在国家安全局的展位上，一名年轻女士告诉我，安全局岗位有着良好的“工作生活平衡”。我还参观了Palantir的招聘展位，那里的员工Elizabeth Watts告诉我，愿意在Palantir工作的人首先要能承受Karp的言论。“想要捍卫西方民主国家的人、对国家安全感兴趣的人都知道，这个世界并不是非黑即白。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b81a612ae31ec6032bcead3b85d7de7c.png" /></p><p></p><p>Palantir公司为士兵们提供一款新型增强现实工具</p><p>&nbsp;</p><p>在Palantir巨大的主展位中，我试着戴上VR头显来测试Palantir为士兵们提供的新型增强现实工具。有人告诉我，我可以在正常观察身边环境的同时指挥卡车或者无人机。但在戴上头显后，我发现自己的视野很不稳定而且会失焦。</p><p>&nbsp;</p><p>一位Palantir员工向我解释说，现场有很多人都在试戴这款头显。要想确保视野清晰，头显必须完美贴合使用者的头部和眼睛。但他没有主动帮助我调整头显，所以我的高科技作战视野仍然一片模糊。</p><p>&nbsp;</p><p>活动第一天晚间，Palantir举办了一场提供免费酒水的社交活动。现场只提供两款IPA精酿啤酒，我选择的叫“the Corruption”，可以说是这辈子我喝过的最难喝的酒。之后我跟一位名叫Sata的加拿大男子攀谈，他看上去也就20多岁。他说自己是Palantir的投资者，所以我当然好奇他这么年轻怎么会有闲钱投资。</p><p>&nbsp;</p><p>他说“我出了车祸”。在正常治疗之外，他把剩下的钱拿去投资。总之他的投资比较成功，但这趟会务出行却是亏的。</p><p>&nbsp;</p><p></p><h2>Palantir 地图工具：帮助选择轰炸目标</h2><p></p><p>&nbsp;</p><p>我还参加了Palantir展位上名叫“缓解平民伤害”的小组讨论。这场讨论由两名“隐私与公民自由工程师”主持，这是一对言语无味的年轻男女。他们用各种委婉的说法来形容轰炸和死亡。其中的女士称，Palantir的Gaia地图工具能帮助用户“处理目标筛选流程”以挑选“感兴趣的目标”，也就是帮助士兵选择具体要轰炸哪些地点。</p><p>&nbsp;</p><p>在交互式地图上点击了几个选项之后，目标区域就亮起了耀眼的蓝色斑点。她解释称，这些亮点代表的是医院和学校等民用区域。以往民用地点大多采用文字描述，但阅读起来费时费力。因此，Gaia利用大语言模型（例如ChatGPT）来筛选信息并加以简化。从本质上讲，选择轰炸目标的士兵们可以借此快速了解哪里聚集着大量儿童或者病人。</p><p>&nbsp;</p><p>我后来问这位工程师，“假设你在一个存在大量平区域的地方行动，比如加沙地带，Palantir 是否会阻止你在平民地点‘指定目标’？”</p><p>&nbsp;</p><p>这位女士直接回答不会，“一切由最终用户决定。”</p><p>&nbsp;</p><p>接下来我看到了一个小小的沉浸式展位，在灰色墙体上张贴着关注普通民众受到战争影响的海报。这里就是国际红十字会的地盘。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2d56653b067866ca220de612c785873e.png" /></p><p></p><p>国际红十字会的展位</p><p>&nbsp;</p><p>穿过一道看似平常的出口，我又迈进一处“紧急避难所”。这是一处为冲突地区的年轻民众提供庇护的样板房间，里面有一张小沙发，上面摆着一只摊开的睡袋，角落里还放着儿童玩具。黄色字样警告称居民应“留在指定的安全区域”。厨房桌子上的收音机似乎正在播放新闻，但信号不太稳定。</p><p>&nbsp;</p><p>这处避难所虽然规模不大，但在这场堪称军工复合体狂欢的会议上却是如此引人瞩目。终于有一个地方在呼吁人们关注战争的受害者，至少应该注意到他们。</p><p>&nbsp;</p><p>走出避难所，我与国际红十字会员工Thomas Glass聊了一会。他很专注、很投入，但看起来也很疲惫。他说他刚刚花了几周时间，在加沙南部建立了一处野战医院并搭设起了公共厨房。</p><p>&nbsp;</p><p>我问与会者们对红十字的展位有何反应。Glass表示，他遇到的大多数人都持开放态度，但也有些人问红十字跟到军工展会来干什么。也许更可悲的是，这么问的人并不是在刻意挑衅，他们是真的无法理解。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>目前，五角大楼官员已经接受了在整个部门内使用生成式AI工具。五角大楼官员将生成式AI视为一种可以在整个部门内使用的工具，具体包括增强后台功能乃至辅助一线作战人员等。</p><p>&nbsp;</p><p>然而，仍有一些安全问题需要解决。他们即将发布一项新指令，以指导该部门运用生成式AI、特别是大语言模型方面。“大语言模型很强，堪称生产力的巨大驱动引擎，让我们能够完成更多工作。但作为一项新技术……在我看来，目前仍处于AI泡沫阶段。放眼整个行业，每个人都在积极竞争，希望尽快打造出最出色的大语言模型。在这方面，我们仍存在一些差距。所以最重要的就是不能直接借用外部大语言模型、指望登录上去并输入我们的数据就能获得符合预期的AI回复。”陆军数据、工程与软件副助理秘书Jennifer Swanson表示，</p><p>&nbsp;</p><p>她指出，这样做可能会导致陆军敏感数据通过互联网及对手也能访问到的训练模型而泄露至公共领域。“这将非常危险。因此，我们正在研究内部应对之策，包括在影响等级IL5和IL6范围之内，可以采取哪些行动。”</p><p>&nbsp;</p><p>一位美军方人士表示，“我们的总体目标就是让大模型以数据功能的形式存在，借此为军方的供应商提供辅助。我们会先以特定用例试水，比如为美国政府构建一套模型的话应该是什么样子、要为军方总部构建的模型又应该是什么样子，以及这些模型之间是什么关系等等。这一切将成为推动后续合作及发展的起点。”</p><p>&nbsp;</p><p>Garciga透露，他们正在对指南备忘录中的内容做初步整理，目前内容主要集中在数据保护方面，包括设想当中的护栏应当具备哪些功能、政府及产业之间又将以何种方式围绕生成式AI开展交互等。</p><p>&nbsp;</p><p>显然，即将出台的新政策将高度关注安全问题。但这种安全只是系统的安全，更广泛的人类范畴内，可能并不在讨论范围里。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theguardian.com/technology/article/2024/may/17/ai-weapons-palantir-war-technology">https://www.theguardian.com/technology/article/2024/may/17/ai-weapons-palantir-war-technology</a>"</p><p><a href="https://defensescoop.com/2024/05/09/army-policy-guidance-use-large-language-models-llm/">https://defensescoop.com/2024/05/09/army-policy-guidance-use-large-language-models-llm/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GdkidIFChUxVslICMamx</id>
            <title>奥特曼被吓坏了：两篇小作文接连否认“封口”离职条款，但没人相信他了</title>
            <link>https://www.infoq.cn/article/GdkidIFChUxVslICMamx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GdkidIFChUxVslICMamx</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 10:39:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 离职员工, AI安全, ChatGPT
<br>
<br>
总结: OpenAI公司因为离职员工的问题和对AI安全的关注而备受争议，ChatGPT产品推出后，员工被要求保持沉默，引发了公众对公司管理的质疑。 </div>
                        <hr>
                    
                    <p></p><blockquote>“我不相信他”：负责捍卫人类利益的OpenAI团队为何分崩离析。</blockquote><p></p><p>&nbsp;</p><p></p><h2>ChatGPT 可以说话，但 OpenAI 员工不能</h2><p></p><p>&nbsp;</p><p>Ilya Sutskever与Jan Leike上周二宣布将离开OpenAI。这两位是公司superalignment“超级对齐”团队的负责人，主要工作就是确保AI技术与开发者的目标保持一致，避免对人类造成不可预测的损害。</p><p>&nbsp;</p><p>选择离开的不只有他们。自去年11月以来，也就是OpenAI董事会试图以“宫斗”方式解雇Sam Altman之时，公司里至少5名关注AI安全的员工已经或主动、或被动地离开OpenAI。</p><p>&nbsp;</p><p>这到底在闹哪样？如果大家一直在社交媒体上关注此事，可能会以为是OpenAI悄然取得了巨大技术突破。表情包“Ilya看见了什么？”认为这位前首席科学家之所以仓惶离场，是因为他看到了令人恐惧的东西——比如可能毁灭人类的AI系统。</p><p>&nbsp;</p><p>但真正的答案可能跟对技术的恐惧无关，而仍然出在人的身上——也就是OpenAI掌门Altman。熟悉该公司内情的消息人士称，关注AI安全的稳健派已经彻底对Altman失去了信任。</p><p>&nbsp;</p><p>一位不愿透露姓名的内部知情人士表示，“信任崩溃是个逐渐的过程，就如同多米诺骨牌的倒落一样。”</p><p>&nbsp;</p><p>没有多少员工愿意公开讨论这个问题。这一方面是因为OpenAI向来会与离职员工签订相当严苛的离职协议，而拒绝签署则意味着放弃补偿权益，有可能损失数百万美元。</p><p>&nbsp;</p><p>OpenAI有着极其严格的离职协议，其中包含前 OpenAI 员工必须遵守的保密和保密条款。它禁止他们在余生中批评他们的前雇主。即使承认 NDA 的存在也被视为违反了协议。</p><p>&nbsp;</p><p>离职员工如果拒绝签署该协议，或泄露协议内容，他们将有可能失去在职期间所获得的所有已归属期权。对于像 OpenAI 这样的初创公司员工来说，期权收益是一项重要甚至是高于其薪资的补偿形式。因此，用这份收益作为威胁，是让离职员工保持沉默的一种非常有效的方式。</p><p>&nbsp;</p><p>有点讽刺的是，OpenAI刚宣布了令人兴奋的新产品 ChatGPT 4o，让ChatGPT 可以像人类一样说话。是的，ChatGPT可以说话，但OpenAI 员工必须保持沉默。“封口令”的存在，让OpenAI获得了几乎一边倒的批评意见。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b565b2788472834a065ddcd8e8ffc59.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ede8bb53946c0e681171752bbb2c92ba.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>风向变了。OpenAI 正处于全民唾弃的边缘。</blockquote><p></p><p>&nbsp;</p><p></p><h2>OpenAI 否认三连，但没人相信他们了</h2><p></p><p>&nbsp;</p><p>鉴于当前舆论哗然的形势，OpenAI发出了一份声明，强调「我们从未剥夺任何现任或前雇员的应得利益，也不会因对方拒绝签署离职或禁止负面评论的协议而剥夺其利益。」而在询问这是否反映出政策内容有所变更时，OpenAI的回应是「声明反映了事实」。</p><p>&nbsp;</p><p>昨天下午，Altman本人在一条推文中承认，OpenAI公司的离职协议中确有一条关于离职员工「潜在股权撤销」的规定，但表示该公司已经在调整具体内容。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f80bf39daf65a23255fd8a0cc03aa2.jpeg" /></p><p></p><p></p><blockquote>关于近期出现的有关OpenAI如何处理权益一事的讨论：我们从未因对方拒绝签署离职协议（或禁止负面评论协议）而剥夺任何人的应得权益，未来也不会这样做。应得权益就是应得的，无需讨论。关于我们原有文件中提出的潜在股权撤销规定，尽管我们从未实际实施，但也承认这条内容本就不该存在。这是我的问题，也是我在执掌OpenAI以来最尴尬的情况；我确实不知道有这么一条，抱歉。过去这一个月来，相关团队已经在调整标准离职条款。如果有任何前员工担心旧协议引发问题，都可以与我联系并共同解决这个问题。再次抱歉。</blockquote><p></p><p>&nbsp;</p><p>但还有一位前雇员，他拒绝签署离职协议，为的就是可以自由批评该公司。Daniel Kokotajlo于2022年加入OpenAI，加入治理团队后一直希望引导公司拥抱安全部署理念，但最终于上个月选择辞职。他说这也意味着他放弃了高达85%的家庭净资产。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c5230e729c815b369119311cde9f7e6.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Kokotajlo在上周接受采访时表示，“OpenAI正在训练越来越多的AI系统，目标就是最终超越人类智能。这可能是人类有史以来最激动人心的目标，但如果我们不能谨慎行动，那也可能陷入万劫不复的深渊。”</p><p>&nbsp;</p><p>OpenAI曾明确表示希望建立通用人工智能（AGI），这是一种理想系统，能够在诸多领域实现等同甚至超越人类的智能表现。</p><p>&nbsp;</p><p>Kokotajlo坦言，“我曾充满希望，认为OpenAI就是技术发展的灯塔，会以负责任的态度逐步迈向AGI。但现在很多人都意识到根本就不可能，我逐渐对OpenAI领导层及其处理AGI的负责态度失去信任，并最终选择辞职。”</p><p>&nbsp;</p><p>Leike上周五也在X上发帖解释了自己辞去超级对齐团队联合负责人职务的原因，情况与Kokotajlo基本相似。他写道，“我一直对公司的核心优先事项设置保留意见，而形势最终发展到了临界点。”</p><p>&nbsp;</p><p>现在，Altman社交媒体下面，都是大片质疑的声音：“为什么安全要退居次要位置？”“为什么禁止前员工批评OpenAI？”“理性的人不会信任你。”</p><p>&nbsp;</p><p>Greg Brockman也不得不出面发表了一条署名为“Sam and Greg”的长推文，表示他们没有放弃安全，并且在“努力减轻风险”。网友们照样不买账，认为这都是废话，一看就是Altman的手笔。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97808a8789e9fda28b368e0a2445d540.jpeg" /></p><p></p><p>&nbsp;</p><p>外媒对此评论说，这是Altman已经被大家的反应“吓坏了”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4dd4b8a0073007243edfd6a2318dbe4a.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>OpenAI安全团队为何不信任Sam Altman？</h2><p></p><p>在回答这个问题，我们需要将时间倒回去年11月。当时，身为OpenAI董事会成员的Ilya也参与过对Altman的“逼宫”。董事会指出，Altman“在沟通中未能一直保持坦诚。”换句话说：我们不相信他。</p><p>&nbsp;</p><p>但这场行动最终失败，Altman和他最忠诚的盟友、公司总裁Greg Brockman威胁要将OpenAI的顶尖人才一股脑带去微软。也就是说除非立刻让Altman官复原职，否则OpenAI就会当场爆炸。面对这种威胁，董事会只得屈服，劫后余生的Altman比以往任何时候都更加强大，并立即组织起更支持他、愿意让他放手做事的新董事会。</p><p>&nbsp;</p><p>入宫行刺失败，事情就绝对不可能善了。</p><p>&nbsp;</p><p>虽然Ilya和Altman曾多次对外大秀二人的深厚友情，但在上周Ilya还是宣布离职，并表示将投身于“对我个人而言意义重大的项目。”几分钟后，Altman也在X上发帖，称“这让我非常难过。Ilya是……我的亲密好友。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/98/9862a19aee388cf1645f117d9d8fd507.png" /></p><p></p><p>&nbsp;</p><p>Sam Altman和 Ilya Sutskever 于 2023 年 6 月 5 日在一所大学里共同发表演讲</p><p>&nbsp;</p><p>但实际情况是，自从政变失败以来，这半年间Ilya就没有出现在OpenAI的办公室。他一直以远程方式领导超级对齐团队，以确保未来的AGI能够与人类利益保持一致。虽然想法不错，但却与公司的日常运营彼此分离，OpenAI的主要精力完全放在Altman领导下的商业化产品开发方面。在Altman复职后不久，Ilya还曾发布并迅速删除过这样一条推文：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/4406ff2a816d0b848c358011fb4838ef.png" /></p><p></p><p></p><blockquote>这个月我学到了很多。与其费心运用各种手段，倒不如“棍棒底下出孝子”。</blockquote><p></p><p>&nbsp;</p><p>因此尽管对外总在刻意展示二人的亲密关系，但在经历了政变之后，Ilya和Altman还能不能算朋友真的要打个大大的问号。</p><p>&nbsp;</p><p>而Altman对“逼宫”一事的反应，也揭示出他性格中的某些侧面：除非董事会让他官复原职，否则他就拼个鱼死网破，而且坚持对董事会成员进行大洗牌以巩固自身地位，表现出牢牢把握权力的坚定决心，甚至直接消除了未来再次面临质疑的可能性。已经有多位前同事和雇员证明Altman就是个控制狂，当面一套、背后一套——比如他曾多次在人前强调安全，但实际工作时却根本不在乎。</p><p>&nbsp;</p><p>举例来说，Altman会从沙特阿拉伯筹款，希望借此建立新的AI芯片公司，为自己的AI前沿探索储备充足的算力资源。此事让关注安全的员工们感到震惊。如果Altman真的在以安全方式构建和部署AI，为什么要以近乎疯狂的方式攫取芯片，不惜一切代价加快技术开发？他又为什么要跟沙特合作，坐视对方利用AI增强数字监控或人权侵犯等潜在风险？</p><p>&nbsp;</p><p>内部知情人士指出，于员工们而言，所有这一切都导致了文章开头提到的“失去信任，因此哪怕OpenAI如何强调自己对某件事的重视，人们也都不再相信。”</p><p>&nbsp;</p><p>而这个渐进的过程，在上周开始全面爆发。</p><p>&nbsp;</p><p>超级对齐团队联合负责人Jan Leike就拿出了鲜明的态度。他在脱离OpenAI的几小时后就在X上发帖称“我辞职了。”没有温暖的告别，也没有对公司管理层的哪怕虚与委蛇的夸赞。</p><p>&nbsp;</p><p>其他重视安全的多位前雇员则给Leike的离职帖点赞，同时加上了爱心的表情符号。Leopold Aschenbrenner作为其中一位，就是上个月被OpenAI开除的超级对齐团队成员。媒体报道指出，他和同团队的另一位研究人员Pavel Izmailov因泄漏信息而被解雇。但OpenAI方面并未提供关于泄漏的证据。鉴于每位员工在加入OpenAI时都需要签署严格的保密协议，所以对于一位身经百战的硅谷老兵来说，Altman完全可以将最无害的信息分享也定义成“泄漏”，借此把Ilya一系的员工全都清理出OpenAI之外。</p><p>&nbsp;</p><p>就在Aschenbrenner和Izmailov被离职的同一个月，另一位安全研究员Cullen O’Keefe也离开了公司。</p><p>&nbsp;</p><p>两周之前，公司一位安全研究员William Saunders在EA论坛上发表一篇神秘的帖子。EA论坛是有效利他主义运动成员们的线上聚会场所，一直在积极参与AI安全事业。Saunders总结了自己作为超级对齐团队成员在OpenAI所做的工作，写道“我于2024年2月15日从OpenAI辞职。”一位评论者则提出了核心问题：Saunders为什么要专门发帖讨论这事？</p><p>&nbsp;</p><p>Saunders回应称，“无可奉告。”用户们由此猜测，他很可能是受到了禁止负面评论协议的约束。</p><p>&nbsp;</p><p>将上述消息跟公司内部人士的话语结合起来，我们至少发现有七位前雇员都曾努力在内部推动OpenAI的安全意识，但却最终对公司主导者彻底失去信心，并最终选择退出。</p><p>&nbsp;</p><p>知情人士指出，“我认为公司里很多认真关注安全和社会影响的同事，心里都抱有一个悬而未决的疑问：为OpenAI这样的公司工作，到底是不是对的？要想让员工们放心，OpenAI就必须对所做之事深思熟虑、同时承担起责任。”</p><p>&nbsp;</p><p></p><h2>随着安全团队的解散，OpenAI的工作安全该由谁保障？</h2><p></p><p>由于Leika不再负责超级对齐团队的管理，OpenAI任命公司联合创始人John Schulman取代了他的位置。</p><p>&nbsp;</p><p>但该团队已经被掏空，Schulamn也仍忙于处理他之前的主要工作，确保OpenAI现有产品的安全。在这样的背景下，谁能指望OpenAI会认真开展具有前瞻性的安全工作？</p><p>&nbsp;</p><p>恐怕没戏。</p><p>&nbsp;</p><p>知情人士解释称，“超级对齐团队当初建立的目的，就是如果公司成功打造出通用人工智能，那么必然会引发各种类型的安全问题。该团队实际是一笔面向未来的专项投入。”</p><p>&nbsp;</p><p>但即使该团队满负荷运转，这笔“专项投入”也只能调动OpenAI内的一小部分研究人员，且只承诺为其提供20%的算力资源。现如今，这批算力可能会被移交给其他OpenAI团队，也不清楚是否将继续探索如何避免未来可能出现的AI灾难性风险。</p><p>&nbsp;</p><p>需要明确一点，这绝不是说OpenAI当前发布的产品（例如能够与用户开展顺畅对话的最新大模型GPT-4o）就会毁灭人类。但随着这项技术的快速发展，未来如何谁也不敢保证。</p><p>&nbsp;</p><p>知情人士表示，“最重要的就是搞清楚他们目前是否正在构建和部署不安全的AI系统，以及能不能指望他们安全构建和部署AGI或者超级智能。前一条我不知道，但后面这点我认为是指望不上。”</p><p>&nbsp;</p><p>Leike在上周五的X帖子中也表达了同样的担忧。他指出，他的团队一直在努力争取足够的算力来完成工作，但总体上可谓是“逆水行舟”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f98129b42c554c58c615495df81d1532.png" /></p><p></p><p></p><blockquote>之所以加入OpenAI，是因为我觉得这里是最适合开展这项研究的场所。然而很长一段时间以来，我跟OpenAI领导层在公司核心优先事项方面一直存在着分歧，这种分歧最终走到了临界点。我认为我们应该将更多的资源花在为下一代模型做好准备上，具体包括安全、监控、准备、安全对抗稳健性、超级对齐、保密性以及社会影响等相关主题。这些问题很难解决，我甚至担心OpenAI还没找到正确的路线。过去这几个月间，我的团队一直在逆水行舟。有时候我们会在算力方面遇到困难，也让这项重要的研究变得愈发举步维艰。</blockquote><p></p><p>&nbsp;</p><p>其中最重要的一条，就是Leike提到“我认为我们应该将更多的资源花在为下一代模型做好准备上，具体包括安全、监控、准备、安全对抗稳健性、超级对齐、保密性以及社会影响等相关主题。这些问题很难解决，我甚至担心OpenAI还没找到正确的路线。”</p><p>&nbsp;</p><p>当AI安全领域最举足轻重的从业者之一表示世界领先的AI厂商还没找到正确路线时，恐怕我们都有理由感到担忧。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.lesswrong.com/posts/kovCotfpTFWFXaxwi/simeon_c-s-shortform">https://www.lesswrong.com/posts/kovCotfpTFWFXaxwi/simeon_c-s-shortform</a>"</p><p><a href="https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence">https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence</a>"</p><p><a href="https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release">https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CyJK5Ki8uVC48mbAloWH</id>
            <title>发布屡次截胡？OpenAI与谷歌携新版大模型再度交锋 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/CyJK5Ki8uVC48mbAloWH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CyJK5Ki8uVC48mbAloWH</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 09:14:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 技术动态, AI巨头, 新一代模型
<br>
<br>
总结: 大模型的快速发展使得了解最新技术动态和积极学习成为从业者的必修课。本周人工智能领域迎来了一波大模型发布的高潮，包括OpenAI、谷歌、百度和腾讯等公司推出的新一代模型。这些新模型在多模态理解、长文本理解和运行速度等方面有所突破，预示着AI技术在未来将扮演更加关键的角色。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，人工智能领域迎来了一波大模型发布的高潮，行业玩家纷纷推出自家的创新成果，AI&nbsp;巨头间的角力再次升温。OpenAI、谷歌、百度和腾讯等公司相继亮相了各自的大模型。其中，OpenAI&nbsp;的新一代模型&nbsp;GPT-4o&nbsp;与谷歌的&nbsp;Gemini&nbsp;家族最为引人注目。新模型不仅在多模态理解能力、长文本理解、运行速度等性能上有所突破，更在应用场景和用户体验上带来了新的想象空间，预示着AI技术将在未来扮演更加关键的角色。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>5&nbsp;月&nbsp;12&nbsp;日，斯坦福大学的研究者开发了一个名为&nbsp;ThunderKittens&nbsp;的&nbsp;AI&nbsp;加速框架。该框架通过简化的&nbsp;CUDA&nbsp;DSL&nbsp;让开发者能够更容易地编写高效的&nbsp;GPU&nbsp;内核，显著提高了&nbsp;GPU&nbsp;利用率。&nbsp;ThunderKittens&nbsp;在&nbsp;RTX&nbsp;4090&nbsp;上实现了约&nbsp;122&nbsp;TFLOP&nbsp;的性能，且在&nbsp;H100&nbsp;上的性能比&nbsp;FlashAttention-2&nbsp;高出约&nbsp;30%。5&nbsp;月&nbsp;14&nbsp;日，OpenAI发布了新一代模型&nbsp;GPT-4o&nbsp;，这是一个全能模型。该模型集成了文本、语音、图像三种模态的理解力，能够实时生成文本、音频和图像的输出。GPT-4o&nbsp;在英语文本、代码、非英语文本、视觉和音频理解方面都有显著提升。5&nbsp;月&nbsp;15&nbsp;日，谷歌发布&nbsp;Gemini&nbsp;家族新成员&nbsp;Gemini&nbsp;1.5&nbsp;Flash&nbsp;，并宣布更新&nbsp;Gemini&nbsp;1.5&nbsp;Pro&nbsp;。Gemini&nbsp;1.5&nbsp;Flash&nbsp;是一款专为速度而优化的小型模型，旨在处理高频任务，提供快速响应。它能够分析和处理包括文本、图片和视频在内的多种信息类型，拥有高达100万个Token的处理能力。Gemini&nbsp;1.5&nbsp;Pro&nbsp;&nbsp;具备&nbsp;200&nbsp;万&nbsp;token&nbsp;的超长上下文窗口，能够处理大量信息，如&nbsp;2&nbsp;小时视频、&nbsp;22&nbsp;小时音频、超过&nbsp;6&nbsp;万行代码或&nbsp;140&nbsp;多万单词。5&nbsp;月&nbsp;15&nbsp;日，百度发布了全球首个&nbsp;L4&nbsp;级自动驾驶大模型&nbsp;Apollo&nbsp;ADFM&nbsp;，并宣称其安全性是普通人类驾驶员的10倍以上，能覆盖城市级全域复杂场景。5&nbsp;月&nbsp;16&nbsp;日，亚信科技认知增强平台&nbsp;TAC&nbsp;MaaS&nbsp;与渊思·编程大模型、渊思·自智网络大模型、渊思·智能运维大模型&nbsp;3&nbsp;个行业大模型。5&nbsp;月&nbsp;17&nbsp;日，腾讯云正式发布教育行业大模型。该模型基于自研混元大模型，融合了教材、习题、论文等资源，并通过腾讯云TI平台优化，特别在中文阅读理解、问答和教育相关任务上表现优异。5&nbsp;月&nbsp;17&nbsp;日，字节跳动发布了豆包大模型（原云雀大模型）&nbsp;AI&nbsp;产品家族。豆包大模型家族包括九款模型，满足不同场景需求，并且字节跳动还推出了&nbsp;AI&nbsp;应用产品“扣子”和豆包&nbsp;App&nbsp;。</p><p></p><h4>开源领域</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日，零一万物发布了其&nbsp;Yi&nbsp;大模型家族的新成员&nbsp;Yi-1.5&nbsp;并正式开源。&nbsp;Yi-1.5&nbsp;包含&nbsp;6B、9B、34B&nbsp;三个版本的预训练和微调模型，采用&nbsp;Apache&nbsp;2.0&nbsp;许可证。作为&nbsp;Yi-1.0&nbsp;的持续预训练版本，&nbsp;Yi-1.5&nbsp;在&nbsp;500B 个&nbsp;token&nbsp;上进行了训练，以提升编码、推理和指令执行能力，并在&nbsp;300&nbsp;万个指令调优样本上进行了精细调整。&nbsp;5&nbsp;月&nbsp;14&nbsp;日，腾讯宣布其混元文生图大模型全面开源。该模型支持中英文双语输入及理解，拥有&nbsp;15&nbsp;亿参数量，并采用了与&nbsp;Sora&nbsp;一致的&nbsp;DiT（Diffusion&nbsp;With&nbsp;Transformer）&nbsp;架构，使其在文生图生成方面表现优异，效果超越开源的&nbsp;Stable&nbsp;Diffusion&nbsp;模型。</p><p></p><h4>多模态领域</h4><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，谷歌发布了视频生成模型&nbsp;Veo&nbsp;，该模型能够根据文本提示生成超过&nbsp;60&nbsp;秒的高质量&nbsp;1080p&nbsp;视频，支持多种电影风格，并具备深层次的语言与视觉理解能力。Veo能够准确捕捉文本中的细微差别，并在视频场景中逼真呈现细节。</p><p></p><h4>科研领域</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日，百度大数据实验室与上海交通大学团队合作开发了名为&nbsp;RNAErnie&nbsp;的基于Transformer的RNA语言模型。该模型通过基序感知预训练和类型引导的微调策略，在多个数据集和任务中表现出色，准确率和F1得分显著提高，证明了其在RNA序列分析方面的优越性和泛化潜力。5&nbsp;月&nbsp;16&nbsp;日，来自亚马逊与得克萨斯大学奥斯汀分校的研究团队发表论文《SynthesizRR:&nbsp;Generating&nbsp;Diverse&nbsp;Datasets&nbsp;with&nbsp;Retrieval&nbsp;Augmentation》。&nbsp;SynthesizRR&nbsp;是一种创新的数据集合成技术，通过结合检索和精细化（Refinement）方法，解决了传统大型语言模型在生成示例时出现的重复性、偏差和风格差异问题。该技术通过引入多样化的内容“种子”，显著提升了词汇和语义的多样性，并在多个复杂任务的数据集上，与人类文本的相似性以及学生模型的提炼性能方面取得了显著进步。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日，宇树科技推出了新款人形机器人&nbsp;Unitree&nbsp;G1&nbsp;，其起步价为&nbsp;9.9&nbsp;万元人民币，相比之前推出的&nbsp;Unitree&nbsp;H1&nbsp;价格大幅下降。Unitree&nbsp;G1身高&nbsp;1.27&nbsp;米，体重&nbsp;35&nbsp;公斤，具有多达&nbsp;43&nbsp;个关节电机（基础版为&nbsp;23&nbsp;个），能够模拟复杂动作并实现精细的运动控制。这款机器人可以折叠存放，运行速度可达&nbsp;2&nbsp;米/秒，并且配备了&nbsp;3D&nbsp;LiDAR&nbsp;传感器和深度摄像头，具备360度全景深度感知能力。5&nbsp;月&nbsp;15&nbsp;日，谷歌发布名为&nbsp;Project&nbsp;Astra&nbsp;的&nbsp;AI&nbsp;Agent&nbsp;。Project&nbsp;Astra&nbsp;能够接收信息、记忆所看到的内容、处理信息并理解上下文细节，以实现与周围世界的自然交互。它在声音和视觉处理方面表现出色，能够进行无延迟的实时语音交互，并快速响应用户的问题，通过连续编码视频帧和组合视频、语音信息来处理收到的内容。</p><p></p><h3>基础设施</h3><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，谷歌发布第六代AI芯片&nbsp;Trillium&nbsp;。这款新型&nbsp;TPU&nbsp;在计算性能上实现了高达&nbsp;4.7&nbsp;倍的提升，同时内存带宽翻倍，能效比上一代产品提高了&nbsp;67%&nbsp;。Trillium&nbsp;芯片采用了谷歌自研的第三代&nbsp;SparseCore&nbsp;技术，有效加速了模型训练并降低了服务延迟。预计&nbsp;Trillium&nbsp;将在今年年底向云客户提供，进一步巩固其在云计算和AI领域的领导地位。</p><p></p><p>报告推荐</p><p>AGI&nbsp;概念引发热议。那么&nbsp;AGI&nbsp;究竟是什么？技术架构来看又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？以上问题的回答尽在《中国AGI市场发展研究报告&nbsp;2024》，欢迎大家扫码关注「AI前线」公众号，回复「AGI」领取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69f5f30dc6564327e46c59d969be2524.jpeg" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了大模型落地应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要应用大模型吗？如何考量金融大模型应用效果？欢迎大家持续关注InfoQ研究中心即将发布的《大模型在金融领域的应用洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9ab971ff9c3c1b68ee2abbf12e27f748.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/05iCMhVcvIq6QDAq5piF</id>
            <title>未来智能王松：聚焦会议垂直场景，打造最实用的AI会议助理</title>
            <link>https://www.infoq.cn/article/05iCMhVcvIq6QDAq5piF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/05iCMhVcvIq6QDAq5piF</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 03:51:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 会议耳机, 大模型, AI助理
<br>
<br>
总结: 未来智能在AI会议助理领域的全面规划和应用，通过大模型技术提升会议效率和安全性，为用户提供全方位的智能服务体验。 </div>
                        <hr>
                    
                    <p>5月17日，AICon全球人工智能开发与应用大会暨大模型应用生态展北京站召开。人工智能硬件公司未来智能CTO王松受邀参加了大会，并在大会上发表了以“探索大模型在会议领域中的应用”为主旨的演讲，分享了未来智能在办公会议垂直场景，打造 “减轻会议焦虑、提升会议效率”AIGC智能耳机的思考及规划。今年两会，国家提出了"人工智能+"蓝图，推动AI赋能各行各业，促进新质生产力发展。目前，中国已经进入到AI高速发展的阶段，越来越多的社会各界人士认为，2024年将是AI应用爆发的元年。</p><p>&nbsp;</p><p>5月15日，未来智能发布了包括讯飞会议耳机Pro 2、iFLYBUDS2两款新一代讯飞会议耳机，其中最重要的迭代，是升级了viaim AI会议助理，可一键生成「摘要总结」，或一键生成『代办事项』，大幅提升用户会后整理纪要的效率之外。此外，还新增了“有问必答”功能，让用户可以通过语音或文字，直接快速查询会议内容，进一步提高了办公效率，这也标志着讯飞会议耳机从“智能工具”成功进化为“智能助理”，也让新一代讯飞会议耳机成为了当下AIGC智能耳机硬件新标杆。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/759fe3290f1dff3cfc420687333b474c.png" /></p><p></p><p>下面是王松在大会上演讲内容的摘要，内容主要聚焦未来智能关于讯飞会议耳机AI能力整体布局思考，帮助读者进一步了解，在办公会议场景下，未来智能打造“最实用AI会议助理”的产品思维与逻辑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1ee53d5b9f4afa9d5933368ce674da91.png" /></p><p></p><h2>会前，用数据驱动会议规划</h2><p></p><p>&nbsp;</p><p>在演讲中，王松提到，讯飞会议耳机的长远目标，是能够成为用户的私人会议AI助理。在这样的愿景下，基于领先的语音识别及语义分析技术而来的全场景录音转写功能、多语种翻译功能、以及viaim AI带来的会后高效信息整理等功能，构筑了讯飞会议耳机的核心功能框架。</p><p>&nbsp;</p><p>真正的“助理”，需要能够基于用户的个人数据信息，能够帮助用户进行会议规划。王松提到的“会议规划”，是指耳机的AI能力，能够利用个人历史的会议数据，能够提前预测出某次会议的会议时长、参与人数、会议流程，进而优化会议流程，提高会议效率，减少不必要的时间成本。王松举了一个例子：假设你每周在固定时间都会开个周会，会上会讨论公司销售、生产、财务、市场、产品、技术等等的各个方面的进展和需要沟通的问题。这时，AI就能够通过历史的会议记录和内容预测出我周一早晨的这个会议时长、流程，动态的在会议中根据发言人的内容和时长来实时提醒会议节奏。这对会议主持人来讲，是非常实用的功能，甚至可以做到会议由AI 主持来自动提醒。</p><p>&nbsp;</p><p>然后，AI能根据上周甚至是数周之前的会议内容罗列出未解决的问题，然后在会议前通过 App 通知用户，用户可以提前填写进度，他不需要写所有的，只需要按照 AI 的提示或者问题来回答就可以，然后由AI 自动生成发言稿。这项功能特别适合有规律的、汇报型的会议。</p><p>&nbsp;</p><p>此外，还有一个有趣的功能：AI还能通过实时分析对会议发言人的语速、口气、情绪，以及对表述内容的分析，给出某个发言人在会议中的表现打分。在会议中，“沉默是金”的参会者，“废话文学”的参会者得到的分数都不会太高。他们的低效表现，也将会被AI识别。</p><p>&nbsp;</p><p>王松表示，未来智能目前已经做到了通过监控数据，大模型生成提醒内容，然后语音合成，最后通过会议系统发送等功能的设计，未来成熟后有望通过OTA升级在讯飞会议耳机上呈现。</p><p></p><h2>会中及会后，用大模型提升会议效率</h2><p></p><p>&nbsp;</p><p>讯飞会议耳机的核心基础功能实时录音转写。基于这项基础功能，再附加大模型能力，可以进一步提升用户在会议中的效率。</p><p>&nbsp;</p><p>王松介绍到，在会中，未来智能的大模型，可实现“实时总结”功能，用户可以向AI提问，例如问刚刚会议讲了什么，AI就能总结出刚才的会议内容；还能实现“生僻词解释”功能，在会中遇到听不懂的单词或者术语，就可以向AI提问，或者在记录中的高亮下划线词汇下点击寻找解释。在这里，AI扮演了很好的助手的角色。</p><p>&nbsp;</p><p>大模型还能帮助用户提高会议参与度。比如，上面提到的“参会者打分”，可以提醒用户改善会议中的表现。还可以通过“实时问答”的相关功能，对于会议中出现陌生问题，AI能够自动搜索并帮助写答案，提高用户的参会效率。</p><p>&nbsp;</p><p>在会后，除了目前讯飞会议耳机目前已经实现的会议记录整理和分发，viaim AI生成「待办事项」强化会后跟踪和落实行动计划等，未来还将新增会议“整体评估”等功能，不断强化讯飞会议耳机AI会议助理属性。</p><p></p><h2>用大模型，加强会议安全</h2><p></p><p>&nbsp;</p><p>在一些重要且涉密的会议中，安全是最大的课题。讯飞会议耳机同样也极为重视“安全”相关的功能开发，解决会议的后顾之忧。</p><p>&nbsp;</p><p>王松介绍到，在安全方面，未来智能通过大模型，目前主要从两个方面入手加强安全，分别是“异常行为检测”、“内容安全过滤”。异常行为方面，大模型通过分析参会者的行为模式（如发言模式等），检测出异常行为等。安全内容过滤，主要包括敏感信息检测：实时监控会议中的文本、语音和视频内容，识别和过滤敏感信息（如机密数据、敏感关键词等），防止信息泄露；不当内容监控：利用大模型检测和过滤会议中的不当内容，如不适当的语言、图像或行为，确保会议环境的专业性和安全性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a47f330cd9689d5a1ddad2d3f55e160d.png" /></p><p></p><p>通过王松的演讲，可以看到未来智能对于会议耳机AI应用方面思考，已经非常全面且深入，洞察到了会议方方面面的各类需求，并有针对性的提出了解决方案。而在消费者体验方面，则聚焦在会前、会中、会后，打造全方面的AI会议助理服务体验。当下的讯飞会议耳机，已经能够有效解决用户痛点，非常使用，当未来智能全面实现了所有的规划，人们的工作生活，获奖因此变得更加美好。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/d9NQ0Ydi3Jqxu5PmAktO</id>
            <title>演示文生图时出现sleep代码，华为回应造假嫌疑；微软将中国AI团队集体打包到美国；百度ECharts创始人“下海”养鱼｜Q资讯</title>
            <link>https://www.infoq.cn/article/d9NQ0Ydi3Jqxu5PmAktO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/d9NQ0Ydi3Jqxu5PmAktO</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 03:11:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华为, 大模型演示, 字节跳动, 价格比较
<br>
<br>
总结: 华为回应大模型演示造假事件，称并非调取预置图片；字节跳动发布豆包大模型，价格比行业便宜99%；传微软将中国AI团队集体打包去美国，官方回应。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;Tina、梓毓</p><p></p><p></p><blockquote>华为昇腾回应“大模型演示造假”：并非调取预置图片；字节跳动发布豆包大模型，价格比行业便宜&nbsp;99%！传微软将中国&nbsp;AI&nbsp;团队集体打包去美国，官方回应；字节跳动或放弃出售沐瞳科技；大厂新发岗位薪资排名揭晓；OpenAI&nbsp;首席科学家官宣离职；OpenAI&nbsp;官宣旗舰模型&nbsp;GPT-4o；网传阿里巴巴员工贪腐案，一年受贿&nbsp;9200&nbsp;多万！谷歌云一键“删库”；微软因&nbsp;Cortana&nbsp;专利侵权被判赔偿&nbsp;2.42&nbsp;亿美元；Android&nbsp;15&nbsp;引入私人空间；苹果新一代&nbsp;iPad&nbsp;Pro&nbsp;被发现存在渲染失常问题……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>华为昇腾回应“大模型演示造假”：并非调取预置图片</h4><p></p><p>5&nbsp;月&nbsp;16&nbsp;日，网传在一场华为的发布会上，其展示大模型“文生图”能力时疑似造假。事件起源于&nbsp;5&nbsp;月&nbsp;10&nbsp;日的鲲鹏昇腾开发者大会，当时在一场面向开发者的技术讨论会上，华为演示了&nbsp;mxRAG&nbsp;SDK&nbsp;功能，展示如何通过十几行代码即可完成&nbsp;RAG&nbsp;应用开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b2ef668cbcbbbb05c954029fd8a3f8f.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/36/3646e0d5d5fe416f74a20e366e9714fc.webp" /></p><p></p><p>网传视频及聊天截图显示，华为在演示文生图功能时，按下&nbsp;Crtl-C&nbsp;中断，显示对应代码为&nbsp;time.sleep(6)。作为开发者，很显然大家对这段代码非常感兴趣。国内外网友们就&nbsp;time.sleep(6)&nbsp;的作用进行了讨论，有人认为这是暂停&nbsp;6&nbsp;秒，再调取预置图片，怀疑其图文结果并非大模型生成，而是人为操控。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e4a62039d6d80d3a967e2bfc9d4575a.webp" /></p><p></p><p>也有人认为截图并不能说明华为是造假&nbsp;sleep&nbsp;6&nbsp;秒后创建（write）了图片然后在&nbsp;vscode&nbsp;里打开。但有人反驳说，视频显示&nbsp;time.sleep(6)&nbsp;是关键路径，也就是说执行&nbsp;main&nbsp;一定会执行&nbsp;time.sleep(6)，并且在视频里第二次执行的时候恰好执行了&nbsp;6&nbsp;秒就返回了。如果整个程序总共花费了&nbsp;6&nbsp;秒，那其中真正生成图片的逻辑花费了多少秒？另外，还有评论说，“sleep（6）是写在库里的，更离谱的是写在库包的&nbsp;_init_.py&nbsp;文件里，我作为超过&nbsp;5&nbsp;年的&nbsp;Python&nbsp;开发，从没见过在这个文件里放&nbsp;sleep&nbsp;语句的。”（亲爱的读者，你们如何解读这段&nbsp;Python&nbsp;代码？欢迎留言中评论！）</p><p></p><p></p><p></p><p>针对网络上的质疑，昇腾社区回应称，现场图片为实时生成，调用的是开源大模型。代码中有&nbsp;time.sleep(6)&nbsp;等表述，是命令等待读取外部开源大模型实时生成的图片，并非调取预置图片。本次展示的均为真实代码，也将在昇腾社区上开放，欢迎开发者使用并提出宝贵建议。</p><p></p><h4>字节跳动发布豆包大模型，价格比行业便宜&nbsp;99%！</h4><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，字节跳动正式对外发布豆包大模型。火山引擎是字节跳动旗下云服务平台，豆包大模型原名“云雀”，是国内首批通过算法备案的大模型之一。目前豆包大模型日均处理&nbsp;1200&nbsp;亿&nbsp;Tokens&nbsp;文本，生成&nbsp;3000&nbsp;万张图片。</p><p></p><p>火山引擎总裁谭待重点披露了豆包大模型的商业化价格——豆包主力模型在企业市场的定价为&nbsp;0.0008&nbsp;元&nbsp;/&nbsp;千&nbsp;Tokens，即&nbsp;0.8&nbsp;厘的价格可处理&nbsp;1500&nbsp;多个汉字，较行业平均价格便宜&nbsp;99.3%。市面上同规格模型的定价一般为&nbsp;0.12&nbsp;元&nbsp;/&nbsp;千&nbsp;Tokens，是豆包模型价格的&nbsp;150&nbsp;倍。</p><p></p><p>但火山方面并未披露豆包大模型的具体参数规模。有专业人士认为，不谈参数量谈价格不能说明到底有多便宜。火山方面人士对记者表示，目前参数规模已经不是衡量大模型能力的唯一指标。采访中谭待表示，“今年行业不再比拼参数规模了，因为大家都‘悟’了。”不同尺寸的模型具备不同性能，价格自然不同，但豆包是以最终能力最强的主力模型来定价，同时与行业价格进行对比。另外也有专业人士认为，如果该模型能够达到&nbsp;llama&nbsp;70b&nbsp;或者&nbsp;mixtral&nbsp;8×7b&nbsp;的效果，那性价比就非常好了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8ccbf019dc0ef271091053125e081d62.webp" /></p><p></p><h4>传微软将中国&nbsp;AI&nbsp;团队集体打包去美国，官方回应</h4><p></p><p>5&nbsp;月&nbsp;15&nbsp;日，据多家媒体报道，微软中国部分员工收到公司邮件，询问是否愿意迁移至其他地区工作，选择包括美国、澳大利亚、爱尔兰等国家在内。涉及的员工包括&nbsp;AI&nbsp;platform&nbsp;的&nbsp;Azure&nbsp;ML&nbsp;团队等有上百名员工。</p><p></p><p>报道中还提到，收到邮件的员工需要在&nbsp;6&nbsp;月&nbsp;7&nbsp;日前做决定，要么去美国，要么选择拿补偿离职。据称，微软美国还可帮助解决家属签证。</p><p></p><p>微软相关人士对媒体回应称，本次是给部分员工一个可选的内部调动机会，微软并没有说明多少员工得到了这一机会。据了解，微软中国&nbsp;C+AI&nbsp;的&nbsp;ML&nbsp;团队可以转到美国西雅图，Azure&nbsp;团队转到澳洲，DevDiv（开发平台事业部）则维持现状。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00dfd4df0a1a753b437f81007941919a.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/16/16499882e05ce08930d29dc7b9ba7e4e.webp" /></p><p></p><p>微软称，在运营管理全球业务的过程中，一直有向员工提供内部轮岗机会的机制，并表示公司将继续致力于中国市场，同时在中国和其他市场开展业务。</p><p></p><h4>字节跳动或放弃出售沐瞳科技，将任命新&nbsp;CEO</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日消息，&nbsp;据外媒报道，字节跳动放弃了出售沐瞳科技公司的计划，并计划为该游戏工作室任命一位新&nbsp;CEO。</p><p></p><p>报道称，接手沐瞳科技的新任&nbsp;CEO&nbsp;是完美世界前高管张云帆，他将取代沐瞳科技联合创始人兼&nbsp;CEO&nbsp;袁菁。公开报道显示，张云帆曾在完美世界公司担任各个职务，包括首席运营官。目前还不清楚袁菁是否会留在沐瞳科技。</p><p></p><p>对于上述消息，字节跳动方面暂未予以回应。</p><p></p><p>2021&nbsp;年&nbsp;3&nbsp;月，瞳科技&nbsp;CEO&nbsp;袁菁发布内部信，宣布沐瞳科技与字节跳动旗下游戏业务品牌朝夕光年达成战略收购协议。收购完成后，沐瞳科技将保持独立运营，并加强在游戏、电竞等领域与字节跳动的深度融合，共同开拓全球游戏市场。同时，袁菁将继续作为&nbsp;CEO&nbsp;留任，沐瞳科技的各条汇报线保持不变。对于此次收购的具体金额，沐瞳科技和字节跳动方面均未做回应。有外媒报道称，此次收购，字节跳动付出了&nbsp;100&nbsp;亿人民币的现金和价值&nbsp;150&nbsp;亿的股权，支付对价约合&nbsp;40&nbsp;亿美金。</p><p></p><p>然而，仅仅两年后，市场传出字节跳动正在寻求以&nbsp;50&nbsp;亿美元出售沐瞳科技的消息，原因与后者的业绩表现不理想有关。收购两年，字节与沐瞳科技的业务协同未能如期实现。</p><p></p><h4>大厂新发岗位薪资排名揭晓：抖音登顶，华为缺席前十</h4><p></p><p>近日，脉脉高聘人才智库发布的《2024&nbsp;春招高薪职业和人才洞察》报告显示，抖音、亚⻢逊、大疆霸榜高薪公司，岗位平均薪资超&nbsp;5&nbsp;万元（月薪）。</p><p></p><p>数据显示，2024&nbsp;年&nbsp;Q1，新发岗位平均薪资最高的&nbsp;20&nbsp;个公司中，抖音以&nbsp;55363&nbsp;元位居高薪榜第⼀，其次是亚马逊&nbsp;55295&nbsp;元，大疆&nbsp;53485&nbsp;元位列第三。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/641877dcc46f2eee158915d8f9bafe13.webp" /></p><p></p><p>2024&nbsp;年&nbsp;Q1，在新发岗位平均薪资超过&nbsp;4&nbsp;万元的企业中，AI&nbsp;四小龙之⼀的商汤科技新发岗位平均薪资涨幅&nbsp;12799&nbsp;元，位列涨幅榜第⼀，其次是&nbsp;AIGC&nbsp;领上市企业万兴科技，涨幅&nbsp;11826&nbsp;元。同样薪资增长超过⼀万元的还有自动驾驶公司元戎启行。这些企业都紧密围绕&nbsp;AI&nbsp;业务发展。</p><p></p><h4>OpenAI&nbsp;首席科学家官宣离职，GPT-4&nbsp;负责人接任</h4><p></p><p>当地时间&nbsp;5&nbsp;月&nbsp;14&nbsp;日，OpenAI&nbsp;联合创始人、首席科学家伊尔亚·苏茨克维（Ilya&nbsp;Sutskever）宣布决定离开&nbsp;OpenAI。几个月前，围绕着&nbsp;OpenAI&nbsp;联合创始人兼首席执行官山姆·奥特曼（Sam&nbsp;Altman）的罢免事件中，这位科学家曾被视为关键人物，而随着&nbsp;Sam&nbsp;Altman&nbsp;的回归和董事会改组，OpenAI&nbsp;的权力斗争落幕，也使得&nbsp;Sutskever&nbsp;如今的出走显得没有那么“意料之外”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e079ba279d221dc206344045f2bab33.webp" /></p><p></p><p>OpenAI&nbsp;CEO&nbsp;奥特曼在推特上发文表示，Ilya&nbsp;与&nbsp;OpenAI&nbsp;的分道扬镳令人非常难过。即将成为下一任&nbsp;OpenAI&nbsp;首席科学家的&nbsp;Jakub&nbsp;Pachocki&nbsp;也对自己的前任&nbsp;Ilya&nbsp;表达了感谢。</p><p></p><p></p><blockquote>Ilya&nbsp;把我带入了深度学习研究的世界，多年来一直是我的导师和伟大的合作者。他对深度学习的令人难以置信的愿景成为&nbsp;OpenAI&nbsp;和&nbsp;AI&nbsp;领域如今的基础。我非常感谢他与我们进行了无数次对话，从有关&nbsp;AI&nbsp;未来进步的高层讨论，到深入的技术白板会议。Ilya，我会想念与你一起工作的日子。</blockquote><p></p><p></p><p>另外，该公司现已聘请了金融软件公司&nbsp;Intuit&nbsp;的前&nbsp;Kubernetes&nbsp;平台工程主管&nbsp;Delyan&nbsp;Raychev，他已经在&nbsp;OpenAI&nbsp;进行招聘，&nbsp;LinkedIn&nbsp;上将其描述为“立即专注于构建和扩展我们的&nbsp;Kubernetes&nbsp;平台”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b978f58bd6c0799bd894352f2da9780.webp" /></p><p></p><p>更多阅读：《<a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651205864&amp;idx=1&amp;sn=52069452bab630e2fd7a00e67d674a73&amp;chksm=bdbbc8bb8acc41adb1bc07648f595d47fa43e6e98559390a23deaddda1948a5e05beafcba63e&amp;scene=21#wechat_redirect">OpenAI&nbsp;的元老科学家们都跑光了！一个时代结束了？</a>"》</p><p></p><h4>OpenAI&nbsp;官宣旗舰模型&nbsp;GPT-4o，完全免费、无障碍与人交谈！</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日凌晨，OpenAI&nbsp;在首次「春季新品发布会」上搬出了新一代旗舰生成模型&nbsp;GPT-4o、桌面&nbsp;App，并展示了一系列新能力。这一次，技术颠覆了产品形态，OpenAI&nbsp;用行动给全世界的科技公司上了一课。并冲上国内微博热搜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6c4510e0758652ba2ca8f85224f56be.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b8cac6bcd63a93d5b26fc9153e7a226.webp" /></p><p></p><p>在此次&nbsp;OpenAI&nbsp;仅有&nbsp;26&nbsp;分钟的春季发布会中，OpenAI&nbsp;首席技术官穆里·穆拉提（Muri&nbsp;Murati）宣布推出名为&nbsp;GPT-4o&nbsp;的新旗舰生成式&nbsp;AI&nbsp;模型，其集文本音频视觉于一身，能力全新升级。</p><p></p><p>此前不少爆料提到，OpenAI&nbsp;将推出&nbsp;AI&nbsp;搜索，与谷歌搜索竞争，从而增强&nbsp;ChatGPT&nbsp;的功能并开拓新市场，并称这款产品将在谷歌本周的开发者大会前推出。</p><p></p><p>不过，OpenAI&nbsp;CEO&nbsp;山姆·奥特曼对此否认，其表示，“不是&nbsp;GPT-5，也不是搜索引擎，但我们一直在努力开发一些我们认为人们会喜欢的新东西！对我来说就像魔法一样。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ab1b458281db51bf7647a7be35b63d4.webp" /></p><p></p><p>然而就在本周，OpenAI&nbsp;官宣了&nbsp;Altman&nbsp;口中的“就像魔法一样”的东西。另外，在&nbsp;API&nbsp;中，GPT-4o&nbsp;的价格是&nbsp;GPT-4-turbo&nbsp;的一半，速度是&nbsp;GPT-4-turbo&nbsp;的两倍、5&nbsp;倍速率限制。</p><p></p><p>更多阅读：《<a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651205710&amp;idx=2&amp;sn=e9f5a3ad81cbfcdfa910ac97f0837f17&amp;chksm=bdbbc81d8acc410b1d5202d39a2c6c0f978066983311b77622495e97c07347353a6577426080&amp;scene=21#wechat_redirect">OpenAI&nbsp;官宣旗舰模型&nbsp;GPT-4o，完全免费、无障碍与人交谈！奥特曼：这是我们最好的模型</a>"》</p><p></p><h4>网传阿里巴巴员工贪腐案：一年受贿&nbsp;9200&nbsp;多万！</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日早间，据央视新闻报道，浙江杭州警方侦破了一起民营企业内部腐败案件。王某是电商平台基础岗位的一名运营人员，他在短短一年的时间，收受商家贿赂高达&nbsp;9200&nbsp;多万元，受贿情节触目惊心。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0ae7f4f0350231be15f3a6f2386192d2.webp" /></p><p></p><p>据警方调查，王某虽职位不高，却手握店铺入驻审批的关键权力。短短一年，王某伙同多人共同收受贿赂高达&nbsp;1.3&nbsp;亿余元，其中王某受贿金额为&nbsp;9200&nbsp;多万元，&nbsp;逐渐形成了一条倒卖家具类官方旗舰店指标的灰黑产业链。就在这时，王某所在电商平台收到的一封匿名举报信，揭开了这个黑灰产业链的冰山一角。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/2266cd14b1ab369533b8ad738bafec1a.webp" /></p><p></p><p>警方通过深入调查很快查清了王某等人的犯罪事实，但在对王某实施抓捕搜查时，警方并未在其家中发现这些巨额赃款。通过调查钱的去向，警方发现，为了逃避打击，王某在实施作案前就“做足了准备”。</p><p></p><p>报道还提到，随着案件的侦破，这家电商平台也被深深触动，他们没有想到一个基层的运营人员，竟然能够受贿如此之多。在警方的建议帮助下，该电商平台重新设计了审批流程，运用大数据技术进行分析判断，以减少人为因素的干扰。</p><p></p><p>对于该报道所提及的电商企业，有记者获悉为阿里巴巴，就此向阿里巴巴方面求证，至今阿里方面暂未公开回应。</p><p></p><h4>李开复：中国需要自己的&nbsp;ChatGPT，当下国内&nbsp;AI&nbsp;工具“都还不够好”</h4><p></p><p>5&nbsp;月&nbsp;13&nbsp;日消息，彭博社刊登了对李开复的专访，李开复认为中国需要自己的&nbsp;ChatGPT，以加快人们对人工智能的兴趣、采用和投资。</p><p></p><p>李开复谈到了“ChatGPT&nbsp;时刻”——&nbsp;对于美国人来说，“ChatGPT&nbsp;时刻”发生在&nbsp;17&nbsp;个月之前_（2022&nbsp;年&nbsp;12&nbsp;月，聊天机器人&nbsp;ChatGPT&nbsp;开始大火）_。但他认为，中国用户还没有迎来“ChatGPT&nbsp;时刻”，直到现在，国内的聊天机器人或工具“都还不够好”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b4e8bc602951132ca0e0b64992db4246.webp" /></p><p></p><p>李开复披露了自家&nbsp;AI&nbsp;公司零一万物的近况：已经接近盈利。在对国内外的数据集进行模型训练之后，李开复正在将自家的模型和应用推向全球，并与国内外客户签约。“今年将是中国生成式&nbsp;AI&nbsp;的应用的爆发期。”但李开复也表示，“当&nbsp;GPT-5&nbsp;问世之后，我们将会落后一步。”</p><p></p><p>李开复称，自家的模型是在合法进入国内的&nbsp;H100s&nbsp;处理器上训练出来的。“需要是发明之母，我们从现有的计算能力中榨取一切可以榨取的东西。”</p><p></p><h4>谷歌云一键“删库”：波及&nbsp;50&nbsp;多万用户、崩溃一周</h4><p></p><p>近日，谷歌云全球首席执行官&nbsp;Thomas&nbsp;Kurian&nbsp;与澳大利亚非盈利性养老基金&nbsp;UniSuper&nbsp;的负责人联合发表声明，就&nbsp;UniSuper&nbsp;私有云账户因谷歌云服务的“错误配置”而被意外删除的事件，向&nbsp;UniSuper&nbsp;的&nbsp;62&nbsp;万名会员表达了“极其令人沮丧、极其令人失望”的歉意。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df587506f78535ad758332f0e13d5ec3.webp" /></p><p></p><p>此次故障导致&nbsp;UniSuper&nbsp;基金的&nbsp;50&nbsp;多万会员自&nbsp;5&nbsp;月&nbsp;2&nbsp;日起，在整整一周内无法访问自己的退休金账户。尽管服务已于周四开始陆续恢复，但投资账户的余额数据仍需更新，以反映上周的金额。</p><p></p><p>事件发生后，UniSuper&nbsp;基金负责人及&nbsp;Google&nbsp;Cloud&nbsp;全球&nbsp;CEO&nbsp;发表了联合声明，声明中提到，此次中断源自配置错误所引发的&nbsp;UniSuper&nbsp;云账户意外删除，而这种情况在&nbsp;Google&nbsp;Cloud&nbsp;上从未发生过。UniSuper&nbsp;管理着约&nbsp;1250&nbsp;亿美元的资金，此次服务中断引起了业界的广泛关注和担忧，同时也对全球云服务的安全性和稳定性提出了质疑。谷歌云作为全球领先的云服务提供商，此次失误对其声誉造成了重大影响。</p><p></p><p>此次事件也提醒了全球云服务用户，注意数据安全和业务连续性计划的重要性。随着云服务的普及，如何确保服务的稳定性和安全性，已成为所有云服务提供商和用户必须共同面对的挑战。</p><p></p><p>更多阅读：《<a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651205504&amp;idx=1&amp;sn=4746861e0a6286762ccce9474da5ffdd&amp;chksm=bdbbd7d38acc5ec5902e1084327da1e425c218dadf43ebf2d704885439adb6f870a73706d444&amp;scene=21#wechat_redirect">谷歌云删库宕机一周：千亿基金数据和备份被删光，技术负责人当场被裁，谷歌最后只说一句&nbsp;Sorry？</a>"》</p><p></p><h4>微软因&nbsp;Cortana&nbsp;专利侵权被判赔偿&nbsp;2.42&nbsp;亿美元</h4><p></p><p>5&nbsp;月&nbsp;12&nbsp;日消息，据路透社报道，5&nbsp;月&nbsp;10&nbsp;日，美国特拉华州联邦陪审团裁定微软公司侵犯了&nbsp;IPA&nbsp;Technologies&nbsp;的一项专利，并勒令微软向其支付&nbsp;2.42&nbsp;亿美元_（当前约&nbsp;17.5&nbsp;亿元人民币）_的赔偿金。陪审团认为，微软的&nbsp;Cortana&nbsp;语音助手软件侵犯了&nbsp;IPA&nbsp;在计算机通信软件方面的专利权。</p><p></p><p>此次裁决源于&nbsp;IPA&nbsp;Technologies&nbsp;2018&nbsp;年提起的一场诉讼。IPA&nbsp;当时指控微软的语音识别技术侵犯了与其个人数字助理和语音数据导航相关的多项专利。经过审理，案件焦点最终集中于&nbsp;IPA&nbsp;的一项专利。微软方面辩称其并未侵权，该专利本身也无效。</p><p></p><p>IPA&nbsp;Technologies&nbsp;是专利许可公司&nbsp;Wi-LAN&nbsp;的子公司，Wi-LAN&nbsp;由加拿大科技公司&nbsp;Quarterhill&nbsp;和两家投资公司共同持有。据悉，该涉案专利由&nbsp;IPA&nbsp;从&nbsp;SRI&nbsp;国际的&nbsp;Siri&nbsp;公司收购，而苹果公司在&nbsp;2010&nbsp;年收购了&nbsp;Siri&nbsp;公司及其技术，并将其应用于自家的&nbsp;Siri&nbsp;语音助手。</p><p></p><p>对于这一裁决，微软发言人表示：「我们仍然坚信微软并未侵犯&nbsp;IPA&nbsp;的专利，并将对该判决提出上诉。」IPA&nbsp;和&nbsp;Wi-LAN&nbsp;的代表暂未对此作出回应。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>Android&nbsp;15&nbsp;引入私人空间，盗窃检测和&nbsp;AV1&nbsp;支持</h4><p></p><p>近日，Google&nbsp;I/O&nbsp;开发者大会上谷歌预告了&nbsp;Android&nbsp;15&nbsp;的新功能，其中之一是私人空间（Private&nbsp;Space），应用抽屉将引入一个新的默认隐藏的部分，用于容纳敏感应用，访问该部分需要第二轮的锁屏身份验证，该验证可以与主屏幕的锁屏身份验证不同。类似工作类应用，隐藏类的应用在独立的配置文件上运行。</p><p></p><p>对系统而言，这些应用由不同的“用户”使用不同的数据运行，非私人的应用无法访问这些数据。当用户锁定私人空间时，配置文件将暂停，应用将停止活动，不会显示通知。</p><p></p><p>另一项功能是盗窃检测锁，使用加速计和&nbsp;Google&nbsp;AI&nbsp;感知是否有人从使用者手中抢走手机并试图带着它逃跑、骑车或开车逃离。任何类似盗窃的震动都会使手机自动锁定。Android&nbsp;15&nbsp;还加入了&nbsp;AV1&nbsp;编码器的软件解码。</p><p></p><h4>苹果新一代&nbsp;iPad&nbsp;Pro&nbsp;被发现存在渲染失常问题</h4><p></p><p>5&nbsp;月&nbsp;14&nbsp;日消息，据外媒报道，按计划，苹果公司在&nbsp;5&nbsp;月&nbsp;7&nbsp;日晚间的新品发布会上推出的首代&nbsp;OLED&nbsp;屏&nbsp;iPad&nbsp;Pro，将在本周三上市。</p><p></p><p>但同苹果公司此前推出的新品一样，即将上市的&nbsp;iPad&nbsp;Pro，也存在一些问题，有外媒在评测中就已发现了渲染失常问题。从报道来看，有一家专注于苹果产品的外媒在初期的评测中发现，部分特定的&nbsp;HDR&nbsp;内容在&nbsp;iPad&nbsp;Pro&nbsp;上播放时，显示为斑点或白色条文，出现在特定的蓝色色调中，例如海军蓝或靛蓝。</p><p></p><p>此外，发现新一代&nbsp;iPad&nbsp;Pro&nbsp;渲染失常的这家外媒也表示，这一问题只有在特定的环境中才能被发现，在&nbsp;iPhone&nbsp;15&nbsp;Pro&nbsp;等其他的苹果&nbsp;OLED&nbsp;屏产品上并未出现，目前也还不清楚为何会在&nbsp;iPad&nbsp;Pro&nbsp;上出现。</p><p></p><p>同此前在硬件产品中发现的问题一样，新一代&nbsp;iPad&nbsp;Pro&nbsp;渲染失常的这一问题，也将通过后续的软件升级修复。苹果已经告知外媒注意到了他们所发现的问题，正在通过软件升级修复。</p><p></p><h4>Reddit&nbsp;与&nbsp;OpenAI&nbsp;达成内容授权协议</h4><p></p><p>在&nbsp;Google&nbsp;之后，社媒平台&nbsp;Reddit&nbsp;与&nbsp;OpenAI&nbsp;达成了内容协议，这一消息推动其股价上涨逾十分之一。根据该协议，OpenAI&nbsp;将获得&nbsp;Reddit&nbsp;内容的访问权限，同时它将为&nbsp;Reddit&nbsp;提供&nbsp;AI&nbsp;驱动功能。和&nbsp;Stack&nbsp;Overflow&nbsp;类似，Reddit&nbsp;的内容都是用户创造和管理的，它的高质量内容应该早就被&nbsp;OpenAI&nbsp;抓取并被用于训练大模型。OpenAI&nbsp;等&nbsp;AI&nbsp;公司正面临来自众多版权所有者的诉讼，通过与&nbsp;Reddit&nbsp;等公司达成协议，AI&nbsp;公司正试图合法化其训练数据。</p><p></p><p>另外，截至目前，Reddit&nbsp;已经累计签署了价值&nbsp;2.03&nbsp;亿美元的授权协议，包括年初和&nbsp;google&nbsp;的&nbsp;6000&nbsp;万合同，这些合同协议期限从&nbsp;2&nbsp;年到&nbsp;3&nbsp;年不等，并且正在谈判达成更多的授权协议。</p><p></p><h4>百度知名开源项目&nbsp;ECharts&nbsp;创始人“下海”&nbsp;养鱼</h4><p></p><p>媒体近日报道称，ECharts&nbsp;创始人林峰已投身农业&nbsp;——“下海”&nbsp;养鱼并养出了顶流。</p><p></p><p>Apache&nbsp;ECharts&nbsp;是一款基于&nbsp;JavaScript&nbsp;的开源可视化图表库，最初由百度团队开源，并于&nbsp;2018&nbsp;年初捐赠给&nbsp;Apache&nbsp;基金会，成为&nbsp;ASF&nbsp;孵化级项目。</p><p></p><p>据都市快报消息，林峰就职的一米八海洋科技，核心创始团队大多来自蚂蚁和阿里，主打贻贝和大黄鱼两样海产品。1&nbsp;号员工为前阿里员工胡晓明（花名：孙权），林峰是&nbsp;3&nbsp;号员工，也是创始合伙人。在正式加入一米八海洋科技之前，林峰做过百度工程师，也自己创过业，2016&nbsp;年加入蚂蚁集团后，成为蚂蚁集团中台产品体验技术和数据可视化方向负责人，带着几百人团队。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f8499a4c9dac8df6c6e549a3ef11319.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZFjmsScSL6syvApMOZKw</id>
            <title>大佬都在讨论AGI，行业应用究竟如何？一篇报告带你拆解五大行业 50+ 场景应用现状</title>
            <link>https://www.infoq.cn/article/ZFjmsScSL6syvApMOZKw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZFjmsScSL6syvApMOZKw</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 May 2024 02:13:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工通用智能, AGI, 应用场景, 中国行业
<br>
<br>
总结: 随着人工智能技术的不断进步，人工通用智能（AGI）在中国行业中的应用场景成为热议焦点。通过分析现有的应用案例，揭示AGI技术在实际业务场景中的具体应用程度和潜在价值。InfoQ研究中心通过报告详细审视AGI在营销、金融、教育、零售以及企业服务等关键行业领域的应用情况，展示中国AGI的发展现状和企业布局。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的不断进步，人工通用智能（AGI）这一概念已经成为科技界和产业界热议的焦点。InfoQ研究中心，作为一直关注AI、大模型及其商业应用的研究机构，本次通过报告的形式，分析研究中国AGI的技术架构，详细审视AGI在当前市场中的应用情况，特别是在营销、金融、教育、零售以及企业服务等关键行业领域。通过分析现有的应用案例，揭示AGI技术在实际业务场景中的具体应用程度和潜在价值。</p><p>本篇文章将立足InfoQ研究中心刚发布的<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">《中国AGI市场发展研究报告&nbsp;2024》</a>"，说明一些现象，也提出一些问题。关于问题的解答，欢迎大家点击<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">「链接」</a>"，下载完整报告阅读。</p><p></p><h4>营销、零售、金融、教育、企业服务五大行业&nbsp;AGI&nbsp;先行</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b02c4f1acbb8a4fe2579c9496929cc74.png" /></p><p></p><p>InfoQ研究中心在研究过程中发觉，目前各行业数字基础不同，应用场景需求紧迫性不同，因此AGI在各行业应用程度也不同。因此根据探索时间、应用成果、用户反馈等内容，并结合专家访谈，InfoQ研究中心将AGI在各行业的应用生命周期划分为四个阶段：应用探索期、产品测试期、市场投放期和应用成熟期。</p><p>整体来看，营销、零售、金融、教育、企业服务场景探索早、成果多，但现阶段尚未形成完全成熟的应用。五大行业究竟应用场景如何？有哪些应用成果积累？又有哪些厂商已经躬身入局了？欢迎大家点击「阅读原文」，下载完整报告阅读。</p><p>此外，InfoQ研究中心也根据五大行业&nbsp;50+&nbsp;应用场景拆解了具体的企业图谱，以更好地展现现在中国AGI的探索现状和企业布局。</p><p></p><h5>中国行业AGI应用全景图</h5><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b05254d210092b021be798bc80d83ed9.png" /></p><p></p><h4>以教育场景为例，AGI&nbsp;三大有效能力渗透学生、教师和学校三方教育场景</h4><p></p><p>教育领域中，AGI在学校及教师侧的应用都还在非常早期，这主要受到智慧校园/教师的整体解决方案的成熟度，以及大模型应用面临技术集成、数据管理以及内容生成质量和匹配性的难题。</p><p>学生侧目前的应用大多都还在单点场景进行探索，例如作文辅导、英语口语等，这些场景的需求较为明确，且与大模型在语言生成的能力提升适配度较高，因此这些场景应用程度相对较高。但像个性化学习这类全流程型的应用，仍处于非常早期。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b84f4cb92a78f45551ba5e941ce0282.png" /></p><p></p><p></p><h4>中国&nbsp;AGI&nbsp;十大潜力发展场景研判</h4><p></p><p>InfoQ研究中心根据过往研究成果和积累，对于现有的应用场景进行了分析判断。选择出了企业内外部应用场景中的十大潜力发展场景。企业内部应用中，内容生成类包含营销中的物料生成、企业服务中的协同办公和辅助编程、游戏场景中的智能游戏NPC；专家类包含企业服务中的数据分析和知识查询，以及金融场景中的知识库。对完场景中包含零售场景中的数字人导购/直播、教育中的智慧硬件，以及营销和零售场景中的智能投放。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d18affcdf7e5c68ee0d1ca7ec1bf3db0.png" /></p><p></p><p>各行各业都在被AGI改造，InfoQ研究中心也期待同大家一起，共同探索和解密中国AGI的发展。</p><p>更多关于中国&nbsp;AGI&nbsp;发展历程、市场规模、技术架构等内容，欢迎大家点击<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">「链接」</a>"，下载完整报告阅读。同时，您也可以点击<a href="https://www.infoq.cn/theme/191">「专题链接」</a>"直达50+ InfoQ研究中心过往研究成果~</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XNDSks1uepQbyGJStGF9</id>
            <title>豆包大模型家族发布、火山方舟升级，火山引擎如何打造全栈AI技术服务？</title>
            <link>https://www.infoq.cn/article/XNDSks1uepQbyGJStGF9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XNDSks1uepQbyGJStGF9</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 10:33:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化浪潮, 大模型, AI模型应用, 火山引擎
<br>
<br>
总结: 在当前数字化浪潮中，大模型作为推动业务创新的引擎，企业对高效、经济的AI模型应用需求迫切。然而，大模型应用面临着效果不佳、高成本和难以落地等挑战。火山引擎在大会上发布了豆包大模型家族和火山方舟2.0等创新产品，以超低价定价为企业市场提供模型服务，致力于打造全栈AI技术服务生态。豆包大模型在内部业务和企业端场景中持续进化，为企业智能化升级提供支持。 </div>
                        <hr>
                    
                    <p>在当今的数字化浪潮中，大模型以其卓越的语言理解和生成能力，正成为推动业务创新的重要引擎。随着“模型即服务”在云服务领域的崛起，企业对于高效、经济的 AI 模型应用的需求日益迫切。然而，现实中大模型的应用并非一帆风顺，企业在尝试将其融入业务流程时，往往面临着效果不尽人意、成本高昂以及落地难度大等诸多挑战。</p><p></p><p>在这样的背景下，大模型的实践应用不应仅仅是市场的“噱头”，企业客户迫切需要真正“好用、能用、有用”的产品和服务。更重要的是“性价比”，产品能力固然重要，但在市场竞争如此激烈的今天，价格已经成为了企业客户决策的第一要素。</p><p></p><p>5 月 15 日，2024 火山引擎 FORCE 原动力大会上，火山引擎重磅发布了豆包大模型家族和火山方舟 2.0 等一系列创新产品，并且宣布豆包主力模型在企业市场的定价为 0.0008 元 / 千 tokens，0.8 厘就能处理 1,500 多个汉字，比行业便宜 99.3%，是当之无愧的“超低价”。</p><p></p><p>“大模型的超低定价，来源于我们在技术上有信心优化成本。” &nbsp;火山引擎总裁谭待在大会上表示，技术上的优势，为火山引擎提供了定价的底气。火山引擎不仅想为企业提供模型服务，更致力于打造一个完备而有效的全栈 AI 技术服务生态，为企业提供模型服务的全链路解决方案。</p><p></p><p>那么，相对于市面上的其他产品，字节跳动的模型产品和火山引擎的模型服务到底有何优势？火山引擎又将如何打造全栈模型服务能力，为企业的智能化升级提供支持？</p><p></p><p></p><h2>模型应用进化，实践是最好的磨刀石</h2><p></p><p></p><p>相比市面上大多数 AI 产品的“大张旗鼓”，字节跳动的 AI 产品一直保持着低调作风，但不知不觉中，以豆包 App、扣子为代表的字节系 AI 应用已然成为了用户的热门选择。</p><p></p><p>以豆包 App 为例，自 2023 年 8 月上线以来，就攻陷了各大应用市场的下载榜单，据悉，截止目前，豆包 App 下载量已经超过 1 亿，桌面端 +App 的月活用户数量已经达到了 2600 万，有超过 800 万个智能体被创建，是当之无愧的明星 AI 产品；扣子也早已在海内外打出名气，凭借超强的扩展性和性价比攻城略地。能在火热的 AI 应用市场中脱颖而出，并在大众对于 AI 认知趋于理性后，仍旧保持着高访问量、调用量和活跃度，豆包 App 和扣子的“实力”有目共睹。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eee466aa32d8a9a0b8334a8719ce8216.webp" /></p><p></p><p>这直接戳中了当下 AI 应用普遍面临着的痛点：不怕不好用，只怕没人用。基于大模型产品的技术特性，只有最大的使用量，才能打磨出最好的模型，只有最多的实践数据，才能催动“智能涌现”的发生，而在庞大使用量和实际场景的锻炼下，应用才能更好地满足用户的使用需求，实现持续迭代。</p><p></p><p>遗憾的是，对于很多产品来说，没有 GPT 那样的顶流地位，想实现“应用进化”非常艰难。</p><p></p><p>而作为行业中的“标杆案例”，豆包大模型就是在“千锤百炼”下长成的。现如今，豆包大模型平均每天处理 1200 亿 tokens（约 1800 亿汉字），生成 3000 万张图片，对于火山引擎和豆包大模型来说，模型服务早已不是“纸上谈兵”，其正在用户、企业的实际应用中不断迭代与进化。</p><p></p><p>首先是字节跳动内部 50+ 业务的持续打磨。基于字节跳动庞大的业务生态，豆包大模型光是在内部就能接触到足够丰富的业务场景。据悉，豆包大模型不仅参与到办公智能助手、数据智能分析、编程助手等企业内部的办公开发场景，还覆盖了电商导购、售后客服、营销创作等前端对客场景，在字节跳动 50 余个实际业务的打磨之下，豆包大模型被应用在一线使用场景中，在字节系产品的庞大用户量、数据量的催化下，豆包大模型得以快速迭代。</p><p></p><p>除了内部打磨、C 端实践，企业端场景的应用至关重要，这决定着模型是否能真正成为“生产力工具”。现如今，豆包大模型已经被广泛应用于火山引擎的企业服务中，覆盖了智能终端、汽车、金融、消费等多个重要行业，更多触达了 OPPO、vivo、小米、荣耀、三星、华硕、招行、捷途、吉利、北汽、智己、广汽、东风本田、海底捞、飞鹤...... 等知名企业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c4f3d25570c62e9ba2d07c8f8db6730.webp" /></p><p></p><p>大会上，火山引擎与企业客户宣布共同成立两大模型联盟：智能终端大模型联盟与汽车大模型生态联盟，在智能终端 AI、汽车全场景 AI 等领域进一步展开探索，这也让豆包大模型的未来发展充满了更多可能性。</p><p></p><p></p><h2>豆包大模型家族发布：多元场景、安全可控</h2><p></p><p></p><p>实践应用是模型能力进化的关键环节，但如何为模型服务开拓更多用户、寻找更多落地机会，则需要更深入的思考。不得不承认，当前企业客户对于模型服务存在一种“为了用而用”的误区，这往往导致模型服务与实际需求之间出现偏差。当需求得不到满足时，对前沿技术的过度依赖也可能变成一种资源浪费，这不利于整个行业的可持续发展。</p><p></p><p>对于火山引擎等行业内的领军企业来说，谁能真正洞察企业客户的深层需求，谁就能占据有利地位。从当前云计算与智能化融合的趋势来看，AI 及大模型若想助力企业业务创新升级，主要需要实现三大目标：</p><p></p><p>利用 AI 打造差异化优势，提升业务场景的创新能力，助力用户体验升级。降低成本、提高效率，通过智能化手段提升业务效率，加快决策和工作流程。在满足企业多样化需求的同时，确保模型服务的安全性和稳定性。</p><p></p><p>能满足上述三大目标的模型服务产品，不仅要有模型本身的卓越性能，还需要满足可用性、易用性、成本可控、安全合规等需求。这就要求模型服务产品本身不能有明显的缺陷。</p><p></p><p>或许正是基于这样的洞察，字节跳动选择推出豆包大模型家族，加持火山引擎的模型服务能力。</p><p></p><p>首先，模型的性能效果仍然是核心。正如上文所述，豆包大模型在模型效果上实现了显著提升。以字节跳动自研的 LLM 模型专业版“豆包通用模型 pro”为例，其最大窗口尺寸可达 128K，且全系列可精调，具备强大的理解、生成、逻辑和记忆能力，适用于问答、摘要、创作、文本分类、角色扮演等通用场景，功能全面。</p><p></p><p>针对不同的业务场景和多模态需求，豆包大模型也实现了进一步的进化。除了通用模型，还包括 5 秒即可实现声音 1:1 克隆的声音复刻模型、具有超自然语音合成能力的语音合成模型、准确率极高的语音识别模型、扣子背后的主力模型 Function Call，以及角色扮演模型、文生图模型、向量化模型等。它们共同构成了豆包大模型家族，旨在满足各行业、多元场景的服务需求。</p><p></p><p>针对企业的个性化需求，豆包的主力模型提供了通用全面的 pro 版本和低延迟、高性价比的 lite 版本，企业可以根据自身需求灵活选择。同时，全系列语言模型均支持继续预训练或 SFT 精调，使企业能够基于自身业务场景，自主开发更适配的 AI 应用。豆包将模型精调和预训练的能力赋予客户，使企业能够实现“一个模型，多元应用”。</p><p></p><p>更重要的是，这一次豆包大模型真正做到了“人人用得起”，豆包主力模型在企业市场的定价只有 0.0008 元 / 千 tokens，0.8 厘就能处理 1,500 多个汉字，比行业便宜 99.3%。在这样的“卷”的价格之下，企业可以真正做到降本增效，用低成本创造新价值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b3e44f5a0dd0054d779fcb81f23f1f8.webp" /></p><p></p><p>大会上，火山引擎总裁谭待表示，有信心通过技术优化降低成本。例如通过对模型结构的优化调整、在工程上从以前的单机推理演进到现在的分布式推理、把不同负载的推理混合调度，这些技术能够把各种各样底层算力用得更好，同时实现大幅的降低成本，让每一家企业都能用得起大模型。</p><p></p><p>在金融、科技等行业客户极为关注的安全问题上，豆包大模型作为首批通过大模型服务安全备案的产品，满足了合规性需求。在火山方舟平台全周期安全可控方案的支持下，豆包大模型在数据加密传输、信息内容安全、防止恶意攻击和数据泄露等方面提供了有力保障，让企业能够放心使用。</p><p></p><p></p><h2>从应用到平台，火山引擎全栈模型服务是如何炼成的？</h2><p></p><p></p><p>豆包大模型家族已经为火山引擎的企业客户提供了强大的模型服务解决方案，但这只是一个开始。火山引擎的终极目标是通过其大模型云计算能力，全面赋能企业，助力其在 AI 时代实现数字化与智能化的升级。这一愿景在火山方舟 2.0 的推出中得到了充分体现。</p><p></p><p>作为一站式大模型服务平台，火山方舟 2.0 在性能和系统承载力方面实现了显著提升。平台拥有海量资源，能够通过资源潮汐调度保障流量高峰时业务的稳定性。同时，其瞬时可用的特性，使得创建模型接入点后 5 秒即可使用，大大提升了业务的响应速度。极致弹性的扩缩容能力，为企业有效支撑突发流量和业务高峰提供了保障，同时降低了成本。</p><p></p><p>在企业 AI 应用的稳定性和成本控制方面，火山方舟 2.0 为企业级 AI 应用的落地提供了坚实的基础。而三大核心插件则进一步加速了企业 AI 应用的产出与创新。</p><p></p><p>联网插件提供了与头条、抖音相同的搜索能力，结合多模态交互方式和领先的意图识别技术，大幅提升了模型的信息获取能力。内容插件则依托字节跳动体系的海量内容资源，通过基于意图的垂直内容信息检索，提供了内容时效性更强的解决方案。RAG 知识库插件以其毫秒级的高性能检索和流式知识库索引更新，降低了企业在使用 AI 模型时的“幻觉”，提升了应用的实用性。</p><p></p><p>扣子专业版的推出，使得企业或创业者可以接入更多高级特性，享有企业级的各项能力。在扣子原有功能基础之上，扣子专业版提供企业级性能的智能体运行时。保障各项服务 SLA，包括但不限于并发量、响应时长等。并开放 SSO、组织权限管理等企业特性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56f69dc96c862b05ef2eaecbf640331f.webp" /></p><p></p><p>可以预见，未来将有越来越多的企业利用扣子专业版进行多场景的开发，非编程人员也可以更好地使用上 AI，为工作全流程进行提效。除此之外，火山引擎还推出了智能创作云 2.0，发布了智能数据洞察 AI 助手 ChatBI、销售 AI 助手 Sales Copilot 等 AI 应用，帮助企业快速实现 AI 升级。</p><p></p><p>全栈 AI 技术服务生态的构建不仅仅在于模型服务本身和 AI 应用开发，基础设施的匹配升级同样重要。在这次升级中，火山引擎还对旗下云底座进行了全面升级。会上，火山引擎全新发布了混合云 veStack 智算版，其拥有着万卡集群组网，3.2T 高性能无损网络的超大规模优势、可以实现 97.78% 训练加速比和分钟级故障发现和自愈的极致性能，还能够适配十余种 GPU 和主流国产化 GPU，应对本地部署需求。</p><p></p><p>通过这些升级，火山引擎展现了其全栈 AI 技术服务生态的构建能力。从基础设施到模型即服务（MaaS）、模型应用，火山引擎的技术服务生态为企业提供了全方位的支持，帮助企业在 AI 时代实现数字化与智能化的升级，推动业务的持续增长和创新。</p><p></p><p>火山引擎的这一系列动作，无疑是对 AI 技术应用的一次深刻洞察和前瞻性布局。火山方舟 2.0 的推出，不仅为企业提供了更加强大和灵活的 AI 应用开发平台，更是在推动整个行业向更高效、更智能的方向发展。随着火山引擎全栈 AI 技术服务生态的不断完善，我们有理由相信，它将为企业带来更多的可能性，开启 AI 技术应用的新篇章。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6i9qbIGk02IdUiAfNSqi</id>
            <title>AICon 2024 重磅开幕！60 余位大咖干货集结：20 年来云首次革命性变化、大模型才刚刚开始……</title>
            <link>https://www.infoq.cn/article/6i9qbIGk02IdUiAfNSqi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6i9qbIGk02IdUiAfNSqi</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 09:16:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AICon, 大会议题板块, 嘉宾阵容, 大模型应用生态展
<br>
<br>
总结: 5 月 17 日，由极客邦旗下 InfoQ 中国倾力打造的 AICon 全球软件开发大会暨智能软件开发生态展在北京正式开幕，会场内人头攒动，盛况空前！演讲嘉宾阵容强大，既有行业领军人物分享战略远见，也有技术大咖深入剖析最新成果，到场的每一位观众都受益匪浅。本次大会设置了丰富的 14 大议题板块，涵盖 AI Agent、RAG 检索生成技术、企业级生成式 AI 助手 Amazon Q、Copilot 辅助程序开发、大规模模型的训练与推理优化策略等内容，同时还特别策划了首届大模型应用生态展，让现场参会者进深入了解并沉浸式体验生成式 AI 在未来的无数可能。 </div>
                        <hr>
                    
                    <p>5 月 17 日，由极客邦旗下 InfoQ 中国倾力打造的AICon 全球人工智能开发与应用大会暨大模型应用生态展在北京正式开幕，会场内人头攒动，盛况空前！演讲嘉宾阵容强大，既有行业领军人物分享战略远见，也有技术大咖深入剖析最新成果，到场的每一位观众都受益匪浅。</p><p></p><p>本次大会设置了丰富的 14 大议题板块，涵盖 AI Agent、RAG 检索生成技术、企业级生成式 AI 助手 Amazon Q、Copilot 辅助程序开发、大规模模型的训练与推理优化策略、基础设施搭建、LLMOps 实践、多模态大模型研究、大模型与行业创新应用融合、AI 最前沿的探索领域，以及针对大模型在全球范围内的机遇与 AI Agent、RAG 检索与生成、Copilot 应用构建、大模型训练以及推理优化、基础设施构建、LLMOps、多模态大模型、大模型 + 行业创新应用、AI 前沿探索以及大模型全球化机会和挑战等。</p><p></p><p>超过 60 位来自 Google、微软、字节、阿里、科大讯飞、智谱、亚马逊云科技、月之暗面、MiniMax、无问芯穹、Lepton AI、数势科技、北京智源人工智能研究院、腾讯等行业头部企业的嘉宾将齐聚一堂，在现场带来精彩纷呈的见解与分享。</p><p></p><p>除此之外，大会还特别策划了首届大模型应用生态展，邀请众多致力于 AI 和大模型行业落地应用探索，有实践、有创新、有成果的企业，将应用案例和创新产品搬到 AICon 现场，让现场参会者进深入了解并沉浸式体验生成式 AI 在未来的无数可能。</p><p></p><p></p><h2>开幕精华：洞悉行业变迁</h2><p></p><p></p><p>本次大会于今日上午 9 点正式开幕，极客邦科技 / 事业合伙人、InfoQ 极客传媒 &amp; 极客时间企业版总经理汪丹（Yolanda）为大会致开幕辞。她首先阐述了这一年 InfoQ 围绕生成式 AI 和大模型技术发展所展开的内容工作和现有成果，接着介绍了今年 AICon 大会的所有看点，包括精彩议题和现场丰富的体验及试驾活动。现在大语言模型对不少业务来说已足够智能，而生成式 AI 的落地关键在于数据战略、大模型选择和实现方式。经过一年多的发展，中国生成式 AI 领域涌现出了不少优秀的企业和案例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19ec38f54c3855e0e77b8b9f65ef5be2.jpeg" /></p><p></p><p>继 2020 年正式推出中国技术力量年度榜单品牌之后，今年 InfoQ 再次面向 AIGC 赛道推出【中国技术力量 2024 之 AIGC 先锋榜】。现场，汪丹正式揭晓了榜单结果。经过对来自互联网、金融、通信、制造、教育等众多领域的多轮优秀案例评选，30 家杰出企业脱颖而出。</p><p></p><p>其中，凭借各自的优秀创新实践案例上榜【AIGC 最佳实践案例 TOP20】的企业，包括快手、作业帮、网易数帆、阿里云函数计算团队、蚂蚁科技集团股份有限公司、顺丰科技、李白人工智能实验室、360 集团、上海笑聘网络科技有限公司、北京文因互联科技有限公司、中国人民人寿保险股份有限公司、北京衡石科技有限公司、吉利汽车集团、深圳前海百递网络有限公司、德邦证券股份有限公司、杭州卓印智能科技有限公司、杭州座头鲸科技有限公司、上海蜜度科技股份有限公司、中国联合网络通信有限公司上海市分公司、深智透医疗科技发展（上海）有限责任公司。</p><p></p><p>而在进行技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多维度的评分后，数势科技、网易 CodeWave、北京白海科技有限公司、北京潞晨科技有限公司、硅基流动、容联云、优刻得科技股份有限公司、智子引擎、南京柯基数据科技有限公司、江苏汇智智能数字科技有限公司上榜【AIGC 最佳技术服务商 TOP10】。</p><p></p><p>随后，InfoQ 研究总监兼首席分析师姜昕蔚正式发布《中国 AGI 市场发展研究报告 2024》，并对报告进行了详细解读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec07966d0455da22e10ccd4f37094c8e.jpeg" /></p><p></p><p>报告指出，目前各行业数字基础不同，应用场景需求急迫性不同，因此 AGI 在各行业应用程度也不同。InfoQ 研究中心根据探索时间、应用成果、用户反馈等内容，并结合专家访谈，将 AGI 在各行业的应用生命周期划分为四个阶段：应用探索期、产品测试期、市场投放期和应用成熟期。</p><p></p><p>整体来看，营销、零售、金融、教育、办公场景探索早、成果多，但现阶段尚未形成完全成熟的应用。营销行业 AGI 将在四个方面引领变革，包括革新内容的创造过程和效率、改变流量的分配和获取方式、提升服务体验、降低商业洞察门槛并颠覆市场研究模式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0daa3ecedaaeb22d7af469bdcb7d14d4.png" /></p><p></p><p>零售场景中，围绕效率提升和体验优化，AGI 本轮生成能力的升级促进了 AI 商拍、营销物料生成等全新场景的诞生和发展。同时，围绕 Agent 驱动的商家助手和智能投放，各家电商平台也正在频繁发布更新。</p><p></p><p>金融行业整体处于应用探索期，正逐步向产品测试期迈进。绝大部分中小型金融机构尚未找到大模型与业务的融合点，对大模型应用处于观望阶段或仅将大模型产品应用于通用业务场景中。部分头部金融机构积极创新，不仅能通过大模型产品解决通用业务问题，还应用于解决非决策类业务问题。个别大型新兴金融科技公司已推出 AI Agent 产品或相关框架，即将迈进市场投放期。</p><p></p><p>企业服务场景中，文本总结、知识查询等协同办公相关的应用，由于需求明确、同 AGI 现阶段的能力适配性高，发展较为迅速。企业资源管理、供应链管理等涉及多个模块，技术更为复杂且安全性与可靠性要求较高，所以应用程度相对较低。</p><p></p><p>教育领域中，受智慧校园 / 教师的整体解决方案的成熟度、大模型应用面临技术集成、数据管理以及内容生成质量和匹配性影响，AGI 在学校及教师侧的应用都还处于非常早期的阶段。目前，学生侧的应用大多都还在单点场景进行探索，如作文辅导、英语口语等场景的需求较为明确，且与大模型在语言生成的能力提升适配度较高，因此应用程度相对较高；但像个性化学习这类全流程型的应用，同样仍处于早期阶段。</p><p></p><p></p><h2>主题演讲：把握技术创新潮流</h2><p></p><p></p><h4>汪玉教授：《可持续的智能：大模型高能效系统前瞻》</h4><p></p><p></p><p>在首场主题演讲中，清华大学电子工程系教授、系主任兼无问芯穹发起人汪玉探讨了大模型高能效系统的未来。他表示 AI 算法算力需求激增，硬件系统的能耗开销可能导致算力供不应求与能源使用的不可持续。在 AI 2.0 时代，生成式任务的智能算法模型规模扩大，对算力及能量的需求急剧增加。如何使用软硬件协同优化加速大模型计算、降低推理成本，成为大模型设计范式的研究重点。汪玉介绍，利用算法数据特征，面向算法模型、数据结构、数据表示、计算图进行算法电路协同设计的方法，可在保证准确率的前提下优化速度与能效，并展示了其团队如何实现全球首个单块 FPGA 上的 7B 大语言模型高效推理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d30a4d890c84d34e15cbe060757ed34f.jpeg" /></p><p></p><p>汪玉还介绍，他发起的 AI Infra 公司无问芯穹正在产业中实践相关能效方案，并针对部分芯片产能不够的问题，开发了多种不同芯片混合训练的框架。目前已经支持六种不同芯片两两组合间的百卡级别异构混合训练，接下来将支持千卡混训。由于多元芯片性能差别较大，如何在不同芯片之间进行训练负载分配，成为了混训中的一个重要课题。无问芯穹基于自研的一种预测误差小于 3% 的训练性能预测工具，可以实现对不同芯片、不同模型结构的训练性能精准预测，从而实现在多元芯片之间的负载自动切分，提升训练效率。当前，无问芯穹的 Infini-AI MaaS 平台已支持了 30 多个主流开源模型、1 个智谱闭源模型和 8 个芯片品牌，并有望于年底实现从多模型到多硬件的自动路由。结合其底层软硬件协同设计与多元芯片兼容能力，可持续加速大模型计算、降低推理成本。</p><p></p><p>此外，清华电子系孵化的公司清鹏智能也正在以自研的能源大模型为核心就能源与算力融合发展整体解决方案做相关研究。“算力本身的耗能属性需要能源的保障，同时算力的发展能够反哺能源产业进行数字化升级，在一定程度上决定了智能的可持续发展潜力。”汪玉表示，从面向智能的软硬件协同设计出发，构建 AI 2.0 时代的算力生态，促进算电双力深度融合，可为大模型的可持续发展筑稳根基。</p><p></p><p></p><h4>贾扬清：《从互联网到 AI：云产业的重构和演进》</h4><p></p><p></p><p>紧接着， Lepton AI 联合创始人兼 CEO 贾扬清讲述了 AI 领域最近一年的趋势和自身的感悟，不仅对互联网到人工智能的转型过程进行回顾，还展望了云产业与 AI 融合的新时代。他表示，AI 已经成为 IT 策略的第三个核心支柱，正极大地促进芯片和云领域的创新。在如今 AI 和大语言模型流行的情况下，很多原本需要整个工程师团队完成的功能任务可以在非常短的时间被创造完成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24d4a65e6c686a538f2a39a944608047.jpeg" /></p><p></p><p>同时，贾扬清指出，大模型实际应用中，在公域和私域面对的设计场景是不一样的，对企业来说重要的是找到可以把业务需求和 AI 能力结合起来的方法论。而小模型在企业应用中有很大潜力，不仅便宜、可定制化，而且微调模型在垂直领域能够达到比通用大模型更好的效果。</p><p></p><p>“云价值主张开始有巨变，只有高性能计算硬件和云原生软件相结合，才能保证 AI 性能。”在贾扬清看来，一个优秀的云化台相较开源 LLM 推理性能提升 3-5 倍，比公共云 GPU 产品更具成本效益，且使开发人员的效率更高。“这是 20 年来，第一次云的基础架构产生革命性的变化。”贾扬清说道。</p><p></p><p></p><h4>黎科峰博士：《大模型时代，基于 AI Agent 的数据分析与决策新趋势》</h4><p></p><p></p><p>大模型和 AI Agent 是否会颠覆 To B 软件？现场，数势科技创始人兼 CEO 黎科峰博士分享了大模型时代下基于 AI Agent 的数据分析新趋势，以及大模型技术在企业数字化转型中的关键作用。首先，他谈到了大模型和 AI Agent 在企业中的落地场景，包括业务分析、内容生成、企业知识库和风控等专业领域。接着，黎博士指出，作为企业经营的“眼”和“脑”，企业数据分析与决策要经历过往、当前、未来三个阶段：数据从结构化数据到加上部分非结构化数据，再发展到结合行业知识 / 数据；使用人群从数据工程师到业务决策者，最后发展到业务全员。“</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca8c14d4208e68fe1f785a1b6c033143.jpeg" /></p><p></p><p>大模型和 Agent 的出现，推动了企业数据分析与决策的范式变革。”黎博士表示。同时，企业也要认识到，智能分析 AI Agent 还存在几个要解决的关键问题，包括数据准确性、数据源的全面性、人际沟通的准确性和体验感、产品的智能性、以及数据计算查询效率及性能问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c462b893aaff7cd35a199560498e947a.png" /></p><p></p><p>此次大会现场，数势科技正式发布智能分析助手 SwiftAgent 2.0，全面解决上述问题，帮助企业实现数据现状 - 数据资产 - 洞察和归因 - 智能决策的完整闭环，有效释放数据价值。</p><p></p><p></p><h4>林咏华：《大模型背后的荆棘之路》</h4><p></p><p></p><p>北京智源人工智能研究院副院长兼总工程师林咏华带来了以“大模型背后的荆棘之路”为主题的演讲。她表示，大模型一年，AI 开源社区受到前所未有的关注和使用。首要问题是，选择哪个基座模型？当前评测技术的发展跟不上大模型的发展速度，且用于比较大模型性能的各种榜单容易激发各种争议，主要存在的评测问题有三项：第一，评测集被“过拟合”，难以区分真正的模型性能；第二，评测方法陈旧，不能反映大模型新的使用场景；第三，新的大模型能力不断出现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00d8036efb4eb82e5cf20f278146f8b1.jpeg" /></p><p></p><p>在训练过程中，基础模型也会出现数据问题，针对行业领域进行持续训练学习是其中的一方面，如数据的来源、已训练数据的遗忘现象如何降低、构造持续训练的数据集、领域数据和通用预训练数据的配比、多种领域数据的训练顺序。林咏华指出，基座模型的变化会影响行业模型性能和行业应用，其性能决定了下游行业模型及行业应用的性能；所依赖的基座模型发生变化后，需重新训练行业模型、重新测试下游模型性能，应用集成后的各种出错处理也要重新打磨。</p><p></p><p>为此，智源研究院牵头共建了北京人工智能数据平台和高质量训练数据集。推动三大数据使用模式，并研制大模型评测体系及开放评测平台 FlagEval，还开源了面向大模型的 Triton 算子库。“当我们拿到一个大模型（开源 / 闭源）后，一切才刚刚开始。需要各种数据、评测、算力的相关技术攻关才能让模型实现产业的落地。” 林咏华表示。</p><p></p><p></p><h4>曹志斌博士：《 The Next Wave：Explore the Strategy on Generative AI》</h4><p></p><p></p><p>接下来，亚马逊云科技的全球生成式 AI 产品营销总监曹志斌博士发表了题为《The Next Wave：Explore the Strategy on Generative AI》的深度演讲，分享了全球不同客户的生成式 AI 应用场景，剖析了下一波生成式 AI 技术浪潮中，应采纳的前沿策略与核心应对机制。他表示，生成式 AI 正在创造巨大的商业价值。关于生成式 AI 应用策略，曹志斌博士提出了三点战略建议：明确业务场景适应性；设定全面数据战略；重视实现的方法和工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa999c46e017408cd1530d90192f1b5f.jpeg" /></p><p></p><p>“不会有一个生成式 AI 基础模型能适用所有业务场景。”曹志斌博士表示，评估生成式 AI 用例的适用性，要看团队、可行性、时间表、预算、投资回报率、数据、风险；而选择大模型需考虑到六个方面，包括模型的大小和能力、预训练数据的知识截止时间、推理性能和延迟表现、是否支持灵活微调、可访问性和总体成本开支、模型相关的道德和责任问题。此外，他提到，正确的工具能够简化基础模型的调用和管理，加速构建生成式 AI 应用。</p><p>&nbsp;</p><p></p><h4>刘威：《腾讯混元大模型技术和应用实践》</h4><p></p><p></p><p>随后，腾讯杰出科学家、腾讯混元大模型技术负责人之一刘威分享了腾讯混元大模型技术与应用实践方面的最新进展。他介绍道，腾讯混元当前已升级为万亿级大模型，在这个过程积累大量自研技术，其中包括创新的专家路由 Routing 算法、独创的 MoE Scaling Law 机制以及合成数据技术，实现模型总体性能相比上一代 Dense 模型提升 50%，对比开源 MoE 模型，在代码、数学和多学科能力领先较多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65a3650f7041936a4631dfaa64d528c7.jpeg" /></p><p></p><p>在文生图方面，腾讯混元实现了基于 LLM + DiT 的生成能力；视频生成上，腾讯混元拥有文生视频、图生视频、图文生视频、视频生视频等多种能力，支持 1k~4k 的分辨率。据悉，目前腾讯混元大模型已接入 600+ 司内业务应用，包括微信读书、腾讯文档 AI 智能助手、腾讯广告妙思文生图平台等。</p><p></p><p></p><h2>现场回顾：技术洪流中的灵感碰撞</h2><p></p><p></p><p>大会现场人头攒动，座无虚席，气氛热闹非凡。与会者们反映，这次大会分享的内容不仅干货满满，且技术观点足够前沿和创新，让其受益匪浅、意犹未尽，更激发了对 AI 未来发展的无限想象和创新灵感。我们倍感欣慰与鼓舞，对每一位参与者给予的支持与认可致以最诚挚的谢意。未来，我们将继续前行，持续提供优质的技术内容和交流平台，致力于推动技术界的发展与创新，力求一路做技术传播领域的佼佼者。</p><p><img src="https://static001.geekbang.org/infoq/3b/3ba8532ad509c706a12ceb27fbd72d5c.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0fcf479ba5d842dd93129b65003ead2a.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c4ae2ee54258e5de22f3c4798274339.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84e97cf9d996a54679a2042ef5f48b7f.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/35/356004cd230204d2c22cff529b99f0b2.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcf38a888e64206cf9c7c388e851364b.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/71/711b95477b9e369a5cf174201567230c.jpeg" /></p><p></p><p></p><h2>精彩瞬间：活动亮点集锦</h2><p></p><p></p><h4>大模型应用生态展</h4><p></p><p></p><p>除延续高浓度的技术内容外，本次 AICon 还特别设置了大模型应用生态展，带到场者一起猎奇 AI 智域，探索生成式 AI 的未来可能。其中，讯飞带来可以上手体验的星火大模型 SparkDesk 问答机器人，Rokid 设置了有保卫农场、完美弧线、飞镖大赛等空间计算游戏的灵境虚拟展，商汤将主打“自动生成代码”的代码小浣熊和“聊着天就把数据分析做了”的办公小浣熊产品带给参会者，还有“造车新势力×智驾领航者”蔚来汽车的展车和亚马逊云科技满载生成式 AI 黑科技的大巴车开到现场。</p><p></p><p>在现场的【OpenTalk】交流区，多位专家大咖与到场的 AICon 开发者们面对面讨论了最新的技术趋势和技术应用经验，议题包括进击的开源大模型、基于混合检索赋能 RAG 和 Agent 应用、商汤大模型在应用场景的落地实践、数据开源如何赋能全球 AI 开源开放生态以及讯飞星火大模型应用生态创新实践等。</p><p></p><p>此次，展区现场还策划了【Workshop】区域——智能编码工具体验区，无论是资深软件工程师还是代码新手，都有机会在这里亲手试用提升编码技巧的灵感和工具，体验如何通过自然语言处理技术自动生成代码，以及利用 AI 进行代码审查和优化。让我们一起回顾这些精彩瞬间吧！</p><p><img src="https://static001.geekbang.org/infoq/6c/6c171ca91ed443be7af9c45fb00abf26.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/25/25dd44eda7d8d063289a7cfbcd5e7fb3.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32f3bd0e5ec782721534adbc82a71cac.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c875e25c2f642a0a446cc4956c545f5.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b0d3453d59cf4e6bad75da59d6de416.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce08e702ab4cf28efdfa119a65c44c20.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/39/397bb97233a114bc5cb8bcde535f2698.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e4/e4babc926952d671cdf02ada1ea8c052.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26a4e478a0d452be3eea9598071c9fad.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/61/613695e3d6030099a3b59879850e5e21.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f94bc8e5e9955e259b319bfa18d0b53.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/feb168f225ef47112dc7fdcc81ed06bf.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c5b67460a5073870fbfff7fa496195a.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d399b713ffc8ae99b0b75963189e70e.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6433852ce87d1c99105d2d2a6635e03.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a94722a0cb0db6d1a841747fe5036cb.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef3438287fac1d35ab1432d2afcab7bb.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b2f0070dbcc4ed68c3915200225874c.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/27/2786c09e0f83ab1cb30f64a59fcba624.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55a22f4638c1921f682a910c7a7a8cc6.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/35/354c9880a76bc0384e2f07287cd63a95.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/47/47693ed2db4150ed69652ffe1f0bcbbf.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af440ef73f10ee7bfeb49a1492196af9.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c9c4a07ba5c9e39b5b9b6e8d8112f84.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5ce88a1a41a9f76dd5b0cc5443d4cf1b.jpeg" /></p><p></p><p></p><h4>赞助商展示区</h4><p></p><p></p><p>AICon 的圆满举行，离不开赞助商们贡献的力量。在他们的慷慨助力下，我们得以持续推动技术的传播与发展，为行业创新注入不竭源泉。本次 AICon 大会得到了众多赞助商的大力支持，包括数势科技、亚马逊云科技、Google Cloud、支付宝小程序云、UCloud优刻得、七牛云、百道数据、未来智能、PPIO派欧云、intel 等。他们的参与不仅为大会增色不少，也为技术共享和行业发展提供了坚实基础。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/753cc990af5c2491b3ae175ce24cdf5c.jpeg" /></p><p></p><p></p><h2>AICon 晚场活动推荐</h2><p></p><p></p><p></p><h4>与三位业内大咖共议：AI 智能体落地的挑战与应对策略</h4><p></p><p>时间：5 月 17 日 18:30-20:00</p><p>这次交流对所有 InfoQ 粉丝免费开放！有线上和线下两种参与形式，扫描下方二维码，即可线上参加。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/3664702a4f38a0e61a3e2dfc238e2f14.png" /></p><p></p><p>AICon 特别策划了一场关于 AI 智能体落地的晚场圆桌讨论，邀请的三位业内专家将与大家分享他们的经验和见解，并与听众互动探讨——</p><p>蓝莺 IM CEO 梁宇鹏</p><p>机器姬 CTO 刘智勇</p><p>天弘基金 AI 负责人 平野</p><p>期待与你一同深入探讨 AI 智能体落地的挑战与应对策略。</p><p></p><h4>极客邦活动推荐</h4><p></p><p></p><p>今年， 极客邦科技旗下 InfoQ 中国已圆满启动两场技术盛会，之后还将于 8 月 18 -19 日举办上海站的 AICon 大会。如您感兴趣，可点击「阅读原文」查看更多详情。结合生成式 AI 领域的一系列最新动态，AICon 上海站将增加围绕多模态实时交互、长文本背后技术能力、AI 智能体相关的应用案例实践等更多话题内容 。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b0f14953c348896a9aabdd313b5ac53.png" /></p><p></p><p>购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hEiM1DUSJUJ898Lh2tPd</id>
            <title>开发者不可错过！与 AI 技术有关的一切都在 Microsoft AI Day</title>
            <link>https://www.infoq.cn/article/hEiM1DUSJUJ898Lh2tPd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hEiM1DUSJUJ898Lh2tPd</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 09:13:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT-4o, AI 技术, 微软 AI 技术峰会, 生成式 AI 技术
<br>
<br>
总结: 5 月 14 日凌晨，OpenAI 发布了 GPT-4o，提供了“GPT-4 级别”的智能，改进了 GPT-4 在文本、视觉和音频方面的能力。微软将举办 AI 技术峰会，探讨 AI 技术的前沿发展和应用场景，以及如何利用 AI 技术实现智能化转型。会议将涵盖生成式 AI 技术、大模型时代、企业数据与生成式 AI 技术、AI 技术在生产中的应用等内容。参与者将有机会与专家面对面交流，探索最新的 AI 技术解决方案。同时，线上直播也将提供精彩内容。 </div>
                        <hr>
                    
                    <p>5 月 14 日凌晨，OpenAI 又发布了一款名为 GPT-4o 的新旗舰生成式人工智能模型，它提供了“GPT-4 级别”的智能，改进了 GPT-4 在文本、视觉以及音频方面的能力。毋庸置疑的是，在当今这个以数据驱动的时代，AI 技术的革新正以惊人的速度重塑着各行各业的面貌。</p><p></p><p>然而，对于众多开发者和企业而言，如何紧跟 AI 技术的前沿发展、如何将这些技术有效应用于解决实际问题、如何在激烈的市场竞争中保持领先，仍是他们面临的重大挑战。为了帮助开发者和企业了解 AI 技术的前瞻见解和行业应用场景，微软将于 2024 年 6 月 14 日在北京国际饭店会议中心举办微软 AI 技术峰会（Microsoft AI Day in Beijing），主题演讲与部分精彩课程将于官方平台同步直播（文末扫码报名或预约直播～）</p><p></p><h2>洞悉 AI 技术趋势，加速企业智能化转型</h2><p></p><p></p><p>为了让大家了解微软在 AI 领域的最新进展和创新实践，微软全球资深副总裁、微软亚太研发集团主席王永东、微软中国区总裁原欣、微软亚洲区 Microsoft Azure 策略运营总经理康容、微软大中华区首席运营官陶然等微软高层将在主题分享分析 AI 技术如何影响未来的商业格局，探讨企业如何利用 AI 技术实现智能化转型。</p><p></p><p>除了前沿技术趋势，技术专题将聚焦四大技术主题，为开发者及企业带来可供参考的实践经验。</p><p></p><p>生成式 AI 技术的最新进展及创新潜力：生成式 AI 技术是当前 AI 领域的热点之一。微软将分享其在全球业务下在生成式 AI 领域的最新研究成果，探讨如何利用这一技术推动企业创新。大模型时代构建企业竞争力：随着 AI 模型规模的不断扩大，大模型已成为提升企业竞争力的重要工具。微软将分享其在全球业务下的大模型领域的实践经验，帮助国际企业构建基于大模型的核心竞争力。企业数据与生成式 AI 技术的新纪元：数据是 AI 技术的基础。微软将探讨如何合理的利用生成式 AI 技术挖掘企业数据的潜在价值，开启企业数据利用的新纪元。AI 技术提升生产的实践：AI 技术在生产领域的应用越来越广泛。微软将分享其在 AI 智能技术提升生产效率方面的实践经验，为企业提供实用的技术指导。</p><p></p><h2>实操体验 &amp; 专家面对面，深入 AI 技术实践</h2><p></p><p></p><p>参与此次 Microsoft AI Day，你将不仅仅是一个旁观者，更是一个实践者和探索者。来自微软、NVIDIA 的专家们将通过现场演示，带你一起探索如何利用最新的 AI 技术解决现实世界中的复杂问题。</p><p></p><p>此外，Microsoft AI Day 的线下展区是另一个不容错过的亮点。这里汇聚了微软及其合作伙伴的最新解决方案和应用展示。你可以带着自己的疑问和好奇，与现场的专家进行一对一的交流，获取针对性的指导和建议。无论你关心的是 AI 技术的最新动态，还是如何在特定场景下应用 AI 技术，这里都有答案。也欢迎你来打卡 GitHub、Microsoft Learn 等展位活动，带走大会专属纪念品，留下 Microsoft AI Day 的专属记忆！（搜索“微软市场活动”公众号报名，一起线下打卡~）</p><p></p><h2>线上同步转播，精彩不间断</h2><p></p><p></p><p>如果你对 AI 技术的发展趋势与落地实践感兴趣，但无法亲临现场，也可选择观看线上直播。报名通道现已开启，<a href="https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx23e7efe66bb8d9eb&amp;redirect_uri=https%3a%2f%2fchinaevent.microsoft.com%2fwcp%2fwechat%2fAuthCallback%3fwechatId%3d49666cda-230c-40d3-a87c-432b19ae135e&amp;response_type=code&amp;scope=snsapi_userinfo&amp;state=STATE#wechat_redirect">欢迎扫描下方二维码提前报名</a>"，不要错过精彩内容！</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18831e2dd1f06a5a0685609f88040e91.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NQROQ7BzTNAT8igtIaSE</id>
            <title>InfoQ 中国技术力量之【AIGC 先锋榜单】结果正式公布！</title>
            <link>https://www.infoq.cn/article/NQROQ7BzTNAT8igtIaSE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NQROQ7BzTNAT8igtIaSE</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 May 2024 00:30:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: AIGC, 技术服务商, 实践案例, 评选
        <br>
        <br>
        总结: InfoQ在今年4月份启动了"AIGC先锋榜"案例征集活动，吸引了来自不同领域的数百个优秀案例。经过评审团评分，最终评选出了AIGC最佳实践案例TOP20和AIGC最佳技术服务商TOP10。评选过程中考量了技术攻坚性、方案成熟度、场景创新性等多个维度。活动展示了生成式AI在各行业的实践探索，未来还将举办年终榜单评选活动。 </div>
                        <hr>
                    
                    <p>在今年4月份，InfoQ面向AIGC领域正式启动<a href="https://www.infoq.cn/form/?id=2098">【中国技术力量&nbsp;2024&nbsp;之AIGC先锋榜】</a>"案例征集，以期深入技术变革，洞见&nbsp;AIGC&nbsp;的产业未来。本次案例征集共分为两个维度，分别是【AIGC&nbsp;最佳实践案例】和&nbsp;【AIGC&nbsp;最佳技术服务商】。</p><p></p><p>在不到一个月的周期内，InfoQ征集到了来自互联网、金融、通信、制造、教育等众多领域的优秀案例达数百个，经过专家评审团的评分，我们的最终结果终于出来啦。根据提报材料的整体数量和质量，最终我们评选出了【AIGC最佳实践案例&nbsp;TOP20】和【AIGC最佳技术服务商TOP10】（以下排名均无先后，按照提报时间顺序展示）。</p><p><img src="https://static001.infoq.cn/resource/image/48/c1/481b712596c6798089fcf8a64b93fec1.jpg" /></p><p>其中，【AIGC最佳技术服务商】榜单，专家评委根据企业提报的信息从技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多个维度进行了评分，最终根据平均分取排名靠前的10家企业上榜。</p><p><img src="https://static001.infoq.cn/resource/image/0e/51/0e372c572b3195ff8696815fa9e59351.jpg" /></p><p></p><p><img src="https://static001.infoq.cn/resource/image/3e/03/3e3d0c815792eafa67da936b046bc103.jpg" /></p><p></p><p>【AIGC最佳实践案例】榜单，专家评委则根据企业提报的信息从场景创新性、实践成果、行业价值等多个维度进行评分，最终根据平均分确认出上榜的20家企业。</p><p></p><p>最后，再次感谢所有企业的参与，我们从中看到了生成式AI在千行百业的初步实践探索，比如智能营销、智能写作、自动驾驶、医学影像增强、智能库存分析、寄快递等众多场景。遗憾错过本次榜单的企业也欢迎积极关注InfoQ中国技术力量的年终榜单预告，我们预计将于10月份左右发起年终榜单评选，届时将通过InfoQ网站、微信公众号等渠道对外官宣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7</id>
            <title>OpenAI的元老科学家们都跑光了！一个时代结束了？</title>
            <link>https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 May 2024 07:08:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 离职, 安全意识, 人工智能
<br>
<br>
总结: 一些关键人物离开了OpenAI，其中包括拥有安全意识的人员，他们担心人工智能可能带来危险。这些离职引发了人们对OpenAI未来方向和安全性的担忧。 </div>
                        <hr>
                    
                    <p>5 月 15 日，OpenAI 联合创始人 Ilya Sutskever在社交平台上发文表示，决定离开 OpneAI。几个小时后，OpenAI 超级对齐团队的负责人Jan Leike 也宣布离职，离职宣言没有像 Ilya 那样写小作文，他就写了一句话“我辞职了（I resigned）”。</p><p>&nbsp;</p><p>值得注意的是，拥有OpenAI 20% 计算资源的超级对齐团队（Superalignment&nbsp;Team）是由上面两个人领导的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1eb5dbde273b3cdea6bff50d4e34f6b8.jpeg" /></p><p></p><p>根据统计，自OpenAI 董事会事件和 Altman 复职以来，离开 OpenAI 的具有安全意识的人名单包括：Ilya Sutskever、Jan Leike、Leopold Aschenbrenner、Pavel Izmailov、William Saunders、Daniel Kokotajlo 和 Cullen O'Keefe。</p><p>&nbsp;</p><p>此外，近期离职的人还包括非营利组织和战略计划主管Chris Clark和社会影响主管Sherry Lachman。</p><p>&nbsp;</p><p>每个OpenAI离职员工宣布离职后，几乎都可以看到有人问：What did you see ? 当然这个问题并没有人回答。</p><p>&nbsp;</p><p>“OpenAI 似乎确实没有多少使命了——他们的CEO散发着二手车推销员的气息，他最近提到考虑允许他们的人工智能生成色情内容，现在又发布了一个调情的AI女友作为他给人类的礼物。”有网友评价道。</p><p>&nbsp;</p><p></p><h2>“元老科学家”所剩无几</h2><p></p><p>&nbsp;</p><p></p><blockquote>“Karpathy 和 Ilya 现在都已从 OpenAI 消失了。看起来，现在是Sam Altman 和 Greg Brockman的表演舞台了。不得不承认，在这四个人中，Karpathy 和 Ilya 是给我印象最深刻的两个。”</blockquote><p></p><p>&nbsp;</p><p>马斯克也曾这样称赞 Ilya：Ilya Sutskever 是 OpenAI 成功的关键。 Altman 也在宣布离职的帖子里说到，“没有他，OpenAI就不会存在。”</p><p>&nbsp;</p><p>去年11月，Ilya 与另外三名董事会成员一道，迫使该公司高调的首席执行官Sam Altman辞职，但后来他表示后悔。据报道，双方争论的焦点是对 OpenAI 方向的分歧：Ilya 对 Altman 以牺牲安全工作为代价而急于推出人工智能产品感到沮丧。</p><p>&nbsp;</p><p>Altman 在被赶下台的五天后就回到了 OpenAI，重申了自己的控制权，并继续推动越来越强大的技术，这让他的一些批评者感到担忧。Ilya 仍然是OpenAI的员工，但他再也没有回去工作。</p><p>&nbsp;</p><p>围绕Ilya 工作的模糊性引发了一个迷因：Ilya 在哪里？他看到了什么？ OpenAI 联合创始人马斯克经常在他拥有的平台 X（以前的 Twitter）上<a href="https://twitter.com/elonmusk/status/1768706295291314586">亲自提出这个问题</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e5697a848732e05b4dc2ed9be0b18b7.png" /></p><p></p><p>能看到的动态是，Ilya 在去年帮助 OpenAI 创建了超级对齐团队，任务是建立防护措施，以防止人工通用智能（AGI）失控。和其他人一样，他越来越担心人工智能可能变得危险，甚至可能毁灭人类。</p><p>&nbsp;</p><p>但是，Ilya 和 Leike 领导的这个超级对齐团队人员非常不稳定。</p><p>&nbsp;</p><p>今年2月，William Saunders 离开了 OpenAI。自2021年以来，Saunders一直在安全团队工作，该团队后来成为超级对齐团队。Saunders 还是可解释性团队的经理，该团队研究如何使AGI安全，并检查模型如何以及为什么会这样表现。他与人合作撰写了几篇关于人工智能模型的论文。</p><p>&nbsp;</p><p>也是在这个月，备受尊敬的研究科学家Andrej Karpathy也宣布离开 OpenAI。他表示，自己的离开并不是因为任何事件、问题或戏剧性事件，而是他要去追求自己的项目。</p><p>&nbsp;</p><p>Karpathy 是 OpenAI 的创始成员，最初于 2017 年离开公司加入特斯拉。2022 年，他离开特斯拉，并在大约一年前重新加入 OpenAI。Karpathy 在社交媒体和 YouTube 上拥有大量粉丝，发布了有关新兴领域发人深省的文章以及解释人工智能内部运作原理的视频。</p><p>&nbsp;</p><p>3月，对齐研究员 Ryan Lowe 离开，参与过GPT-4对抗性测试的 Daniel Kokotajlo 也离开了OpenAI。Kokotajlo在他的网上论坛LessWrong个人主页上写道，他退出是因为“对AGI时代的行为失去信心”。</p><p>&nbsp;</p><p>他还曾参与关于暂停AGI开发的讨论。Kokotajlo 写道：“大多数要求暂停的人都是在试图反对‘选择性暂停’，以及要求对处于进步前沿的大型实验室的实际暂停。”</p><p>&nbsp;</p><p>他认为，目前的奥弗顿之窗（overton window ）似乎围绕评估风险和采取缓解措施的组合，这具有很高的监管俘获风险（即导致选择性暂停，而这并不适用于最需要暂停的大公司！)“我的幻灭感是我离开OpenAI的原因之一。”</p><p>&nbsp;</p><p>4月，据知情人士透露，OpenAI 解雇了两名涉嫌泄露信息的研究人员，其中包括超级对齐团队的 Leopold Aschenbrenner，Aschenbrenner 是 llya 的盟友。另一位从事推理研究的研究员 Pavel Izmailov 也曾在安全团队工作过。目前，Pavel Izmailov已经跳槽到马斯克旗下的xAI，明年也将成为纽约大学助理教授。</p><p>&nbsp;</p><p>最近，多名涉嫌透露消息给外界的“内鬼”也被OpenAI开除。</p><p>&nbsp;</p><p>“OpenAI 正在失去最优秀、最注重安全的人才。”这是大家对此的评价。鉴于最近从OpenAI离职的人数之多，不少网友都开始调侃：“我从OpenAI离职了”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5ba9f2df6d98eb6108870f7df0295af1.jpeg" /></p><p></p><p></p><h2>OpenAI 被营销支配？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“六位顶尖科学家早已离去。OpenAI 如今由营销、业务、软件和产品化人员运营。”</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/500beb93e2e06e2b7a0a5a92dbef6892.png" /></p><p></p><p>从左到右：Jakub Pachocki、Greg Brockman、Ilya Sutskeve、Sam Altman、Muri Murati</p><p>&nbsp;</p><p>上图中，除去 Ilya，几乎就是当前OpenAI的重要管理层了。</p><p>&nbsp;</p><p>OpenAI 的关键研究员 Jakub Pachocki 将接替 Ilya 担任该公司的首席科学家。在 Altman 被罢黜前几周，曾帮助监督 GPT-4 创建的 Pachocki 被提升到公司研究总监的位置，一度被提升到与Ilya 并肩的职位。</p><p>&nbsp;</p><p>Pachocki 于 2017 年加入 OpenAI Dota 团队，担任研究主管，该团队构建了一个能够在 Valve 的 Dota 2 策略游戏中击败人类玩家的人工智能系统。随后，Pachocki 成为 OpenAI 深度学习组织推理和科学的研究负责人，然后晋升为研究总监。目前尚不清楚 Pachocki 是否也会接任 OpenAI Superalignment 团队的负责人。</p><p>&nbsp;</p><p>而Jan Leike 离职后，他的职位将由该公司另一位联合创始人 John Schulman 担任。Schulman 在去年失败的董事会政变中站在了 Altman 一边。另外，Schulman 在Superalignment 团队还担任了监督者角色。</p><p>&nbsp;</p><p>当然，OpenAI也在不断引进新的人才，年轻力量正在支撑OpenAI。比如GPT-4o的多模态负责人Prafulla Dhariwal，实际只有本科学历；Sora的论文作者中有一位研究员今年刚满21岁，仅有高中毕业证。</p><p>&nbsp;</p><p>但众所周知，OpenAI更多使用的是谷歌提出的技术路线，其核心研发实力不如他们的工程能力。元老科学家们的出走还是让大家对OpenAI 的未来产生了担忧：OpenAI 还能实现 AGI 吗？</p><p>&nbsp;</p><p>AI 行业人才短缺是不争的事实，AI相关的部门很难找到合适的员工。AI 人才争夺战已经开始，甚至有企业都给出了100万美元年薪。</p><p>&nbsp;</p><p>薪酬数据和职业平台 Levels.fyi 联合创始人 Zuhayeer Musa在采访中表示，OpenAI 提供的中位工资（包括奖金和股权）为 925,000 美元。Meta 的 344 名机器学习和人工智能工程师，包括奖金和股权在内的薪酬中位数约为 40 万美元。</p><p>&nbsp;</p><p>除了巨额薪酬之外，从小型初创公司到 OpenAI、Meta 等，都在提供加速的股票兑现计划，甚至试图挖走整个团队。</p><p>&nbsp;</p><p>“他们没有护城河。那些从事科学研究的人现在正在为其他公司做研究，并且会让 OpenAI 感到震惊。”有网友对OpenAI的人才出走评价道。</p><p>&nbsp;</p><p>“OpenAI 对 Microsoft 的需要几乎就像 Microsoft 对 OpenAI 的需要一样”。有网友认为，“当下一波新的深度学习创新浪潮席卷全球时，微软会吃掉OpenAI 剩下的东西。他们赚了很多钱，但除非他们弥补失去的东西，否则就没有未来。”</p><p>&nbsp;</p><p>OpenAI与微软的紧密联系让一些人希望，至少出于对品牌保护，微软能够在安全研究上做一定的投入。但具有讽刺意味的是，微软在发布“人工智能”产品之前不进行安全检查方面是已经出名了的。</p><p>&nbsp;</p><p>还有很多人认为，OpenAI 全力以赴地让大模型这只“金鹅”产生更多收益、专注于如何通过嵌入广告实现货币化，并通过主题限制继续提供“安全”等，而不是进一步沿着 AGI 路线前进。</p><p>&nbsp;</p><p>“LLM 通往 AGI 或超级智能的机会为零。因此，如果这就是 OpenAI 在未来 5 年里要关注的全部内容，那么与Superalignment 相关的小组就没有必要了。”有网友评价道。</p><p>&nbsp;</p><p>有人推测，要么离 AGI 太远，以至于无论“对齐”意味着什么都是不必要的，要么就是奥特曼等人已确定这是商业成功的障碍。</p><p>&nbsp;</p><p>“事实证明，我们已经结盟了，这就是所谓的资本主义。”也有人说道，“资本主义本身就是一种不结盟的人工智能，从这个角度来理解就可以澄清很多事情。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“近十年后，我决定离开 OpenAI。这家公司的发展轨迹堪称奇迹，我相信OpenAI将打造出既安全又有益的人工智能。”38岁的llya 补充说，他正在启动一个新项目，但没有详细说明。</p><p>&nbsp;</p><p>Karpathy 和 Ilya 都有了自己的项目，显然，人们希望那些伟大的人工智能科学家还能在一起做一些有意义的事情。不过，我们应该很快能看到他们多年从事AI 研发的总结成果。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/ilyasut/status/1790517455628198322">https://twitter.com/ilyasut/status/1790517455628198322</a>"</p><p><a href="https://www.businessinsider.com/openai-safety-researchers-quit-superalignment-sam-altman-chatgpt-2024-5">https://www.businessinsider.com/openai-safety-researchers-quit-superalignment-sam-altman-chatgpt-2024-5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k3QwZc0Ty20kuXygmTmH</id>
            <title>百度文心智能体平台举办开发者沙龙，打造国内领先的智能体生态</title>
            <link>https://www.infoq.cn/article/k3QwZc0Ty20kuXygmTmH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k3QwZc0Ty20kuXygmTmH</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 14:47:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: 人工智能技术, 智能体, 百度文心智能体平台, 智能体生态
        <br>
        <br>
        总结: 随着人工智能技术的发展，智能体作为大模型应用的新趋势，正在改变生活和工作方式。百度文心智能体平台通过全新升级，致力于打造国内领先的智能体生态，吸引了大量技术开发者和人工智能爱好者参与。平台提供多样化的智能体，覆盖广泛应用场景，呼吁更多行业伙伴和开发者加入。通过提供详尽的智能体开发指南，平台帮助开发者快速创建和优化智能体。活动中展示了智能体在实际应用中的进阶技巧，激发开发者创意潜能，并邀请他们参加智能体大赛。2024百度移动生态万象大会将推出更多智能体相关服务和能力，致力于让智能体人人可用。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的飞速发展，智能体作为大模型应用的新趋势，正逐步改变我们的生活和工作方式。</p><p>&nbsp;</p><p>百度创始人、董事长兼首席执行官李彦宏曾表示，智能体是未来离每个人最近、最主流的大模型使用方式。在这一背景下，百度文心智能体平台（AgentBuilder）经过全新升级，致力于打造国内领先的智能体生态。</p><p>&nbsp;</p><p>5月15日，百度文心智能体平台联合InfoQ，举办了一场主题为「拥抱智能体，人人都能成为超级个体」的沙龙活动，吸引了大量技术开发者以及对人工智能充满热情的参与者。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/32/66/32c2d9e2c1f88a1e92fe3f8e9fae1f66.jpeg" /></p><p></p><p>据介绍，文心智能体平台除了开发门槛低之外，还有智能调优、广泛分发、直通商业化等特点。百度搜索也会在接下来的时间里，重点布局智能体生态，用搜索生态天然带有的「亿级用户+超级流量+精准算法」，打通「开发+分发+商业化」全链条，让智能体释放出更大潜力。</p><p>&nbsp;</p><p>&nbsp;百度文心智能体生态负责人马宝云分享了文心智能体平台的核心优势。&nbsp;</p><p>&nbsp;</p><p>百度是业内最早布局智能体的大厂之一，2023年9月，百度发布「灵境矩阵」文心一言插件生态平台，同年12月升级为「灵境矩阵智能体平台」，在今年4月举办的Create 2024百度AI开发者大会上则升级更名为「文心智能体平台」。全新升级后的文心智能体平台，有5个「超能力」：技术底子厚、开发成本低、快速可成长、分发渠道广、商业可闭环。</p><p>&nbsp;</p><p>据悉，文心智能体平台发布至今已有9个月，最近又经历了全新升级，仅仅是5月，智能体的数量就已经环比增长167%。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/fa/69/faf7d08b0450428a45df844f8600a569.jpeg" /></p><p></p><p>&nbsp;她展示了平台如何通过提供创作助手、专家顾问、AI分身、学习工具、生活帮手、互动游戏和设计助手等多样化的智能体，来满足不同用户的需求。这些智能体不仅覆盖了广泛的应用场景，也体现了平台对各行业伙伴的开放性和包容性。她呼吁更多的行业伙伴和开发者加入文心智能体。</p><p>&nbsp;</p><p>文心智能体平台高级产品经理梁伟以文心智能体平台为例，给广大开发者提供了一份详尽的「从0到1智能体开发指南」。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/36/fb/367ddyy504cdba45433315fc53720bfb.jpeg" /></p><p>&nbsp;</p><p>他展示了如何通过简单的一句话描述来快速创建智能体，通过层次分明的表单配置来完善智能体的高级设置。他还分享了如何通过智能体生成和优化指令，如何通过知识库和工具来增强智能体的功能，还介绍了数字人配置的选项，包括形象设定和语音风格，以及如何通过实时预览调优来测试智能体的效果。</p><p>&nbsp;</p><p>文心智能体平台运营经理李实则分享了智能体在实际应用中的进阶技巧。</p><p>&nbsp;</p><p>李实表示，向AI大模型提供具体指令（prompt）会直接影响智能体的效果。他建议指令应包含角色和目标、指导与限制、澄清和个性化四个部分，以确保智能体能够精确模拟特定角色的思维方式，提供符合实际情景的回答。在现场，李实展示了怎样用搜索增强和文心一格生图等工具来提升智能体的交互体验。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2f/bf/2f8c41baa14576f2c89d77bca5d625bf.jpeg" /></p><p>&nbsp;</p><p>活动特别安排了自由问答和现场互动体验环节，为参与者提供了交流和探讨智能体技术及应用的机会。参与者有机会亲身体验智能体的强大功能，感受人工智能带来的便捷和智能。</p><p>&nbsp;</p><p>据介绍，为激发开发者的创意潜能，文心智能体平台发起「文心智能体大赛」，为开发者提供百万奖金池、百亿流量包、与技术大咖深度交流、免费AI课程等支持，诚邀广大开发者积极参与，共同探索无限可能。感兴趣的开发者现在就可以报名参加。</p><p>&nbsp;</p><p>据悉，2024百度移动生态万象大会将于5月30日在苏州举办，本次大会的主题是「让智能体人人可用」，百度搜索、百度APP、百度文库、文心一言APP、百度电商等百度移动生态业务都将推出更多智能体相关的服务和能力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VKUTp0UkRPvGWHU5dT1S</id>
            <title>AIGC智能耳机硬件新标杆，未来智能发布新一代讯飞会议耳机</title>
            <link>https://www.infoq.cn/article/VKUTp0UkRPvGWHU5dT1S</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VKUTp0UkRPvGWHU5dT1S</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 10:27:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 未来智能, 讯飞会议耳机Pro 2, viaim AI, AIGC智能耳机
<br>
<br>
总结: 2024年5月15日，人工智能硬件公司未来智能发布了讯飞会议耳机Pro 2、iFLYBUDS 2以及Kit 2三款旗舰新品，为用户带来全新升级的viaim AI，也为AIGC智能耳机树立了新标杆。在发布会上，未来智能CEO马啸表示，讯飞会议耳机Pro 2是未来智能最新集大成之作，依托领先AI技术，成功进化至“智能助理”，引领了AIGC场景应用趋势。新一代产品采用全新工艺设计，全面升级音质、降噪、操控等方面，实现了多语种录音转译等功能基础上的闪录、语种扩充、viaim AI三大进化，大幅提升办公效率，成为AIGC时代的办公会议生产力标配。viaim AI会议助理智能分析记录内容，提取重点并生成摘要总结和待办事项，新增智能询问功能，全面解放用户双手，提升办公效率。多语种录音转写及翻译功能支持32种语言，让耳机化身全场景AI翻译官。硬件体验实现了进一步突破，音质全面升级，降噪深度可达48dB，续航时间长达36小时，外观设计高端质感，语音控制更便捷。讯飞会议耳机Pro 2定位于商务旗舰，iFLYBUDS 2定位于职场Buff，Kit 2是讯飞会议耳机的天生搭档，助力用户提高工作效率。 </div>
                        <hr>
                    
                    <p>2024年5月15日，人工智能硬件公司未来智能发布了讯飞会议耳机Pro&nbsp;2、iFLYBUDS 2以及Kit 2三款旗舰新品，为用户带来全新升级的viaim&nbsp;AI，也为AIGC智能耳机树立了新标杆。</p><p></p><p>在发布会上，未来智能CEO马啸表示：在AIGC领域，垂直场景的服务性工具比泛智能工具实用性更强，未来智能在垂直的办公会议领域，已经形成了数据的马太效应，打造出了非常实用的AI会议助理。以讯飞会议耳机Pro&nbsp;2为代表的未来智能新一代产品，是未来智能最新集大成之作，标志着未来智能团队多年来在办公会议垂直场景中的产品解决方案深挖，以及持续的技术积累，迎来了“质变”时刻：依托领先AI技术，讯飞会议耳机从“智能工具”成功进化至“智能助理”，成为当下最实用的AIGC智能耳机之一，引领了AIGC场景应用趋势。</p><p></p><p>新一代未来智能新品矩阵中，最具代表性的商务旗舰产品讯飞会议耳机Pro&nbsp;2采用了全新的工艺设计，带来更高级的质感体验，音质、降噪、操控等方面全面升级，更在全场景录音转文字、多语种录音转译等功能基础上，实现了闪录、语种扩充、viaim AI三大进化，进一步拓展了讯飞会议耳机的应用场景，全面释放AI生产力，大幅提升办公效率，成为AIGC时代的办公会议生产力标配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c397a4bda62148dd386d0cb993e40ea5.png" /></p><p></p><p>viaim AI再进化，讯飞会议耳机更“聪明”了</p><p></p><p>新一代讯飞会议耳机Pro&nbsp;2搭载了全新升级的viaim AI会议助理，AI性能大幅提升，让耳机变得更“聪明”了。面对冗长繁琐的会议内容，viaim AI能够智能分析记录内容，自动提取纪录中的重点，2小时会议1分钟即可一键生成「摘要总结」，大幅简化会后总结难度，还能提取纪录中的关键任务生成「待办事项」，让待办事项一目了然。</p><p></p><p>而让用户更加惊喜的升级，则是viaim AI新增了「智能询问」功能，用户只需语音/文字输入问题，viaim AI就能回答用户关于当前记录内提到的问题和扩展问题，让用户快速获取记录内容中需要的信息。新的AI功能做到了真正全面解放用户双手，再一次提升办公效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fcffc6a9684055ae2cfd7b51aa198e42.png" /></p><p></p><p>语种大幅扩充，讯飞会议耳机化身全场景AI翻译官</p><p></p><p>商务精英，经常会在不同外语环境中与不同的人打交道，一部掌握多种语言的小巧耳机，其实是最优雅的突破语言障碍的工作神器。跟随讯飞会议耳机Pro&nbsp;2等新品的发布，未来智能大幅扩充了多语种录音转写及翻译功能所支持的语言，从原来的支持11种语言扩充到支持32种语言、还在支持12种方言基础上，新增了2种民族语言，还拥有同传听译、面对面翻译两种模式，让耳机化身全场景AI翻译官，无论多复杂的语言环境也能帮助用户轻松应对。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2afc04013cee5a0dac2013c95c3eaa50.png" /></p><p></p><p>硬件全能进化，讯飞会议耳机Pro&nbsp;2旗舰品质再突破</p><p></p><p>新一代讯飞会议耳机的硬件体验也实现了进一步突破。讯飞会议耳机Pro&nbsp;2带来了全新升级的闪录功能「红点录」。在会议现场，打开充电盒盖，一键按下充电盒内红色按键，即可进入现场录音模式。无需打开APP，也无需连接手机，录音存储在耳机中，现场拾音辐射距离高达7m，左右耳机合计可存储4小时录音，轻松应对各种会议场景，确保不错过任何重要内容。「红点录」进一步拓展了讯飞会议耳机独家闪录功能应用场景，标志着讯飞会议耳机在全能全场景进化的道路上再一次实现了突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/5318c8fe3de0d1005544113c5617b563.png" /></p><p></p><p>生产力升级之外，讯飞会议耳机Pro&nbsp;2没有忘记耳机体验的进化。其采用11mm镀钛原生刚性振膜单元以及极具高弹性和刚性的TPU镀钛材质，配合讯飞AI音频实验室专业调音，实现了音质全面升级，带来旗舰级悦耳音质体验。同时，支持LHDCTM高清音频解码，至高可达1000Kbps，音质表现达到行业第一阵营，并荣获了Hi-Res金标音质认证。</p><p></p><p>降噪方面，讯飞会议耳机Pro&nbsp;2集成自适应ANC主动降噪，智能捕捉环境噪音，并根据噪音强度自动切换降噪等级，降噪深度可达48dB，即使在喧闹的场合里也能享受会议室般安静，降噪品质得到中国电子音响协会降噪等级认证：A级。此外，讯飞会议耳机Pro&nbsp;2在三麦克风通话降噪算法上新增了骨声纹拾音麦克风，利用头骨震动的方式精准采集用户声音，大幅提升通话质量，即便身处嘈杂环境也能清晰如同面对面交流。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6dc1a5cd67b649d7f528ad570b88aa7.png" /></p><p></p><p>讯飞会议耳机Pro 2的续航表现同样值得称赞，单次使用长达9小时，搭配充电盒可延长至36小时。而且还具备快速充电功能，充电5分钟可以提供长达1小时的续航时间，更支持无线充电，彻底告别续航焦虑。</p><p>外观上，讯飞会议耳机Pro 2延续了经典的滑盖设计，整体采用PPG大师漆，正面悬浮镂空全透效果logo以及充电仓真空电镀装饰，搭配夜影黑、幻影银、午夜蓝(限量版)未来科幻感配色，轻巧便携更沉稳大气，尽显高端质感。全新升级的语音控制，不仅「说话」就能操控耳机，更能一键触控录音、无感配对，带来更便捷操控及连接体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f5b4c7cc860a3739cf3d15149bf3348.png" /></p><p></p><p>新一代未来智能产品矩阵中，讯飞会议耳机Pro 2定位于“商务旗舰”，为商务精英人群量身打造。同期发布的iFLYBUDS&nbsp;2，则定位于“职场Buff”，其软件体验与讯飞会议耳机Pro&nbsp;2相近，硬件形态上则采用了半入耳式设计，更适合耳道敏感人群，即使长时间佩戴也毫无压力，可以帮助更广泛职场人持续提高工作效率。Kit 2是讯飞会议耳机的天生搭档，让耳机端的实时录音转写文字等功能在电脑上实现，带来更高效的桌面办公体验，专为深度会议用户量身定制。</p><p></p><p>目前三款新品已经在京东/天猫商城上线销售，用户可结合自身需要酌情选购。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B14OwDrE1ZZ3VMl1goHm</id>
            <title>打磨三年、支持万亿 MoE，腾讯混元模型团队的真实推理实力到底如何？</title>
            <link>https://www.infoq.cn/article/B14OwDrE1ZZ3VMl1goHm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B14OwDrE1ZZ3VMl1goHm</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 08:26:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 腾讯混元大模型, 刘凯, 推理能力, 技术实力
<br>
<br>
总结: 2023年9月，腾讯推出自研的混元大模型，支持50多个业务产品，推理性能优异。刘凯介绍了腾讯在大模型技术探索和优势方面，以及混元大模型的推理能力和压缩方法。模型规模庞大，采用不同推理和压缩方法，以及如何在保持性能效果的前提下将大模型做“小”的技术思路。 </div>
                        <hr>
                    
                    <p>采访嘉宾｜刘凯，腾讯混元大模型推理方向负责人</p><p>作者&nbsp;|&nbsp;华卫</p><p></p><p>2023&nbsp;年&nbsp;9&nbsp;月，腾讯终于在一片翘首以盼中推出自研的混元大模型。对于入局早晚的问题，腾讯董事会主席兼首席执行官马化腾曾这样说道，“我们在埋头研发，但并不急于早早做完，把半成品拿出来展示。”</p><p></p><p>据悉，混元大模型未来能支持&nbsp;50&nbsp;多个腾讯业务产品，而幻觉比主流开源大模型降低&nbsp;30%&nbsp;至&nbsp;50%、文生图推理耗时缩短至&nbsp;3-4&nbsp;秒，是混元大模型目前已达到的推理性能。那么，其背后的核心团队究竟做了哪些努力？技术实力到底如何？</p><p></p><p>就此，InfoQ&nbsp;对腾讯混元大模型推理方向负责人刘凯进行了专访，听他详细讲述了腾讯混元大模型在推理和压缩方面的技术能力与团队实践。在即将召开的<a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"上，InfoQ&nbsp;也邀请到刘凯老师来做演讲分享，他将进一步透露大模型推理加速与压缩的技术方法以及腾讯混元大模型的落地进展。</p><p>&nbsp;</p><p>以下为访谈实录，经编辑。</p><p></p><p></p><h2>如何在推理赛道扳回“一局”？</h2><p></p><p>InfoQ：作为较晚入场大模型的国内互联网大厂，腾讯团队有什么优势？</p><p>刘凯：对于晚入场这个说法，并不准确。早在2020年，腾讯出于自身业务需要已经展开预训练大模型的技术探索和积累，并率先在内部业务譬如广告上进行应用投产。腾讯对于处理前沿技术探索和输出的关系，一贯以来是比较一致的，对于正在探索的技术路线，往往会用自身业务作为试验田对方案进行反复验证和完善，之后才会对外发布和输出。</p><p>说到优势，我觉得在大模型技术的前沿探索中，腾讯在以下方面具备相当的积累和竞争力：1、在数据、算法、工程等方向，我们有一批经验丰富的专家；2、我们有一个强大的机器学习平台Angel(曾获&nbsp;2023年中国电子学会科学技术进步一等奖)；3、腾讯内部有大量适合大模型落地的业务应用场景，能在和业务的合作中助力腾讯混元团队能力的快速成长。</p><p></p><p>InfoQ：推理能力对大模型而言十分关键，腾讯混元大模型做到了什么水平？目前是否有量化的能力指标？</p><p>刘凯：目前腾讯混元大模型的吞吐能力达到开源框架的2倍以上，文生图&amp;文生视频推理耗时下降65%。规模上，模型支持万亿MoE、上下文长度保持256K以上，同时支持多种压缩方法，包括量化、蒸馏、裁剪、稀疏、并行解码、步数蒸馏等，能在保证效果无损的基础上，将吞吐提升2~8倍。</p><p></p><p>InfoQ：不同模态的内容生成框架下，混元大模型采用的推理和压缩方法有差异吗？</p><p>刘凯：会存在一定的差异。比如文生文&amp;图生文的场景，由于模型较大一般需要采用分布式推理；而文生图&amp;文生视频的扩散模型，在大部分场景下使用单卡推理即可，不过随着模型的逐步增大，我们也在支持分布式推理。</p><p>压缩方法上也存在一定的差异，文生图&amp;文生视频扩散模型使用步数蒸馏收益更大，所以蒸馏的优先级会高于其他方法；而在生文场景，量化由于简单高效，优先级最高、之后逐步是蒸馏、投机采样、裁剪稀疏等方法。</p><p></p><p>InfoQ：目前有哪些可以有效提高模型推理速度和准确度的技术？主要优化思路是什么？</p><p>刘凯：并行解码等相关技术都值得一试，其主要思路是通过使用更小的模型或者一次更多的生成token数来加快速度，同时使用base模型进行结果校验来保证生成的效果。</p><p></p><p>InfoQ：对腾讯混元大模型来说，端侧推理是一个降低推理成本的好方式吗？是否有可能实现？</p><p>刘凯：是的，端侧推理是腾讯混元大模型逐步推进的一个方向。腾讯内部有很多业务适合端侧推理，比如会议、文档、输入法等。</p><p></p><p></p><h2>将模型从大化“小”的心得</h2><p></p><p>InfoQ：模型的规模参数大到一定程度后，会产生哪些负面效应？</p><p>刘凯：模型参数的持续上升，会带来成本的上升和耗时的增加，同时也给推理优化带来了很大的挑战。首先我们知道大模型推理的瓶颈主要集中在显存和带宽上，为了放下更大的模型，我们需要进行单机多卡、多机多卡的部署。</p><p>当使用多机多卡时，带宽就涉及到显存带宽、卡间带宽、网络带宽等三个方面，其速度依次递减，耗时会逐步上升，而部署卡数的上升必然会带来卡成本及配套设备成本的上升。此外，框架3D并行能力并非无限制无损扩展，如果超大模型设计的不合理，会使得优化难度成倍上升。</p><p>InfoQ：如何在保持性能效果的前提下将大模型做“小”？腾讯有什么好的技术思路分享？</p><p>刘凯：模型压缩方法主要包括蒸馏、裁剪、稀疏、量化等。在上述方法中，量化容易实现，是最稳定的，也是各大公司广泛使用的方法。以腾讯混元大模型为例，我们在Dense以及MoE模型都大规模使用了量化模型，从精度上覆盖了INT8、FP8、INT4，并在逐步尝试2bit、1bit的压缩，目前在范围上已经支持了权重、激活、KV-Cache的量化。</p><p>由于腾讯内部应用场景很多，对模型规模有多样的需求，我们也开发了裁剪+蒸馏的方式来快速扩展模型矩阵，保证各个业务可以使用适合自己的大模型。稀疏这块，其实服务器侧的使用会比较少，但腾讯在这块有持续打磨。除了上述通用方法之外，针对大模型也有一些新的压缩方法，比如文生文当中的GQA/MQA，并行解码，Cache方案等；文生图、文生视频的步数蒸馏等。</p><p>InfoQ：现实应用中，当落地场景的训练数据未知或不可获得时，如何合理进行模型压缩？</p><p>刘凯：针对这个问题我想稍微扩展一下，首先我们知道模型压缩一般分为Training-Base和Training-Free两种方法，但大模型压缩时我们一般还是建议走Training-Free过程，因为大模型的训练过程长、成本高、调参复杂，一般情况不建议去触碰。并且，随着模型规模的增大，无损压缩的难度是减小的，所以使用简单便捷的Training-Free的方法比较好。</p><p>使用Training-Free也需要一些数据进行校准，如果获得不到训练的数据时，我们的建议是通过两种方法解决：1、选取通用数据集的数据进行校准；2、使用大模型生成一定的数据来进行校准。</p><p>InfoQ：在即将到来的AICon上，您准备向听众分享哪些方面的内容？</p><p>刘凯：在即将到来的AICon上，我会给大家分享腾讯混元大模型推理框架Angel-HCF、压缩工具SNIP的技术进展以及腾讯混元大模型的落地情况，并针对GPU底层优化、服务化能力、压缩算法的优缺点进行剖析，让大家能快速了解大模型推理相关技术。</p><p></p><p></p><h4>嘉宾介绍：</h4><p></p><p>刘凯，腾讯高级工程师，腾讯混元大模型推理方向负责人，负责文生文、文生图等大模型压缩优化及推理加速。10&nbsp;年以上&nbsp;GPU&nbsp;高性能优化经验，丰富的深度学习推理框架优化经验。带领团队完成大模型压缩&nbsp;&amp;&nbsp;推理框架从&nbsp;0&nbsp;到&nbsp;1&nbsp;的构建。</p><p>&nbsp;&nbsp;&nbsp;</p><p>活动推荐：</p><p><a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f325163430e0188b28bcaaf57a37a8ff.png" /></p><p>&nbsp;</p><p>会议即将开幕，扫码可预约主题演讲直播，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>追踪链接：<a href="https://sourl.co/faYrKr">https://sourl.co/faYrKr</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GKuBaJYaVxmQtxAJI8XB</id>
            <title>巨头们涌入的医疗大模型，何时迎来最好的商业时代？</title>
            <link>https://www.infoq.cn/article/GKuBaJYaVxmQtxAJI8XB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GKuBaJYaVxmQtxAJI8XB</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 08:21:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 医疗大模型, 商业化, 数据质量, 社会接受度
<br>
<br>
总结: 当下医疗大模型在商业化领域备受关注，但仍需克服数据质量、成本、幻觉等挑战，同时提高社会接受度。 </div>
                        <hr>
                    
                    <p>采访嘉宾｜刘升平，云知声AI&nbsp;Labs&nbsp;研发副总裁</p><p>作者&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>当下极为火爆的大模型，在医疗赛道同样炙手可热。谷歌刚刚发布了准确率达&nbsp;91.1%、性能远超&nbsp;GPT-4&nbsp;系列的多模态医学大模型Med-Gemini，国内市场亦很热闹。自2023年以来，百度、腾讯、京东等诸多大厂都相继加码医疗大模型领域，与医疗相关的大模型产品和应用如雨后春笋般正不断涌现出来，其中更不乏&nbsp;AI&nbsp;和医疗企业的手笔。</p><p>&nbsp;</p><p>目前，已有部分医疗大模型产品投入到导诊、预问诊等医院场景中。然而，医疗大模型虽有一定潜力，但现阶段仍有不少要跨越的落地门槛。</p><p>&nbsp;</p><p>为此，InfoQ&nbsp;对云知声AI&nbsp;Labs&nbsp;研发副总裁刘升平进行了专访，听他聊一聊现阶段医疗大模型的商业化能力，以及面向这类应用场景的行业大模型该如何定制优化。在即将召开的<a href="https://sourl.co/faYrKr">AICon&nbsp;全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"上，InfoQ&nbsp;也邀请到了刘升平老师来做演讲分享，他将进一步分享医疗大模型的构建方法和应用落地经验。</p><p>&nbsp;</p><p>以下为访谈实录，经编辑。</p><p></p><h2>医疗大模型距离商业化有多远？</h2><p></p><p>InfoQ：现阶段，医疗大模型要规模化落地还面临哪些现实问题？</p><p>刘升平：主要的问题还是有不少，首先是医生和患者的接受度，特别是有些场景要改变医生的使用习惯。还有一个问题是大模型的部署成本，如果在院里大规模并发使用医疗大模型，硬件成本会比较高。</p><p>&nbsp;</p><p>InfoQ：“幻觉”的偶发出现是大模型目前公认的一个问题，医疗场景对准确度要求会更高，山海在这方面是怎么做的？</p><p>刘升平：“幻觉”的确是核心要解决的问题，我们采用多种手段从多方面降低幻觉，包括保证医疗预训练语料和微调数据的质量和多样性、采用能降低知识幻觉的解码策略、融合医疗知识图谱的知识增强大模型技术、医疗知识检索增强、大模型结果后校验、大模型输出置信度评估等。</p><p>&nbsp;</p><p>InfoQ：您认为哪一个评价标准最能代表医疗大模型的水平？</p><p>刘升平：临床有效性是最能代表医疗大模型水平的关键评价标准，包括模型在实际临床环境中的诊断准确性、治疗建议的合理性以及与专业医生的决策一致性。此外，模型的鲁棒性、泛化能力、可解释性、用户友好性、数据隐私保护以及合规性也是重要的评价维度。然而，临床有效性直接关系到患者的安全和健康，因此如果把医疗大模型应用与临床实践中，它可能是最重要的评价标准。</p><p>&nbsp;</p><p>InfoQ：现在行业内有您认为还不错的其他医疗大模型产品吗？国内外均可。</p><p>刘升平：除了云知声的山海大模型医疗版，最近看到的是谷歌的多模态医疗大模型Med-Gemini，在多项临床任务评测中都表现很好，但还没有在医院得到广泛使用。</p><p>&nbsp;</p><p>InfoQ：在医疗大模型的技术实现、应用效果以及成本部署上，国内与国外有区别吗？</p><p>刘升平：没有显著区别。</p><p>&nbsp;</p><p>InfoQ：您认为医疗大模型真正迎来商业化时代还需要多久？</p><p>刘升平：预计2-5年吧。今年是医疗大模型的应用元年，有部分医院开始尝试一些医疗大模型的应用，随着这些医院推广与积累医疗大模型应用经验，预计医疗大模型会在2-5年内进入更广泛的商业化阶段。</p><p>&nbsp;</p><p>InfoQ：社会接受度上，如何让大众认可大模型的诊断或治疗方案？</p><p>刘升平：要让大众接受并信任大模型的诊断或治疗方案，是一个长期的过程，要考虑很多方面。第一，要提高模型的决策过程透明度，提供可解释的输出，让用户理解模型是如何得出结论的。这有助于建立用户信任，尤其是对于医疗决策这样敏感的问题。第二，要有严格的临床试验，证明模型的诊断或治疗方案与专业医生的判断相当或更优，且这些结果应由独立的第三方机构审核并公开。第三，要让医生参与到模型的开发和应用中，他们可以提供专业知识，确保模型的输出符合医学实践，并在实际应用中监督和调整。第四，要开展公众教育活动，解释人工智能在医疗领域的潜力和限制，消除误解，提高公众的理解和接受度。&nbsp;通过这些措施，应该可以逐步提高社会对大模型在医疗领域应用的接受度和信任度。</p><p></p><h1>山海大模型的实践经验</h1><p></p><p>InfoQ：医疗相比其他场景更复杂且严谨，难度自然也不小，驱动云知声选择在这一领域开发大模型的最重要因素是什么？</p><p>刘升平：云知声选择在医疗领域开发大模型，主要有两个关键因素。一是应用潜力，而医疗领域是一个富文本、富知识的行业，并且医疗大模型在处理医疗病历文书、辅助诊断、药物研发等方面展现出巨大潜力，因为医疗领域是一个很适合大语言模型技术的应用领域。此外，医疗AI市场具有巨大的商业价值，随着技术的成熟和接受度的提高，未来有望形成规模化的商业模式。二是专业积累，云知声深耕医疗领域多年，对医疗业务场景有深入的理解，在医疗数据和医疗AI技术有深厚的积累，也积累了数百家的医疗客户，这有助于医疗大模型的研发和商业化推广应用。</p><p>&nbsp;</p><p>InfoQ：大模型训练过程本身就对数据质量有较高要求，医疗领域的数据则更为特殊，还具有隐私保护、专业知识复杂、经验化知识难以结构化等难题，山海是如何克服的？</p><p>刘升平：山海医疗大模型在训练过程中面临数据质量、隐私保护和专业知识复杂性等挑战，我们采取了两种策略来克服这些问题。一是数据清洗与预处理，对收集到的医疗数据进行严格的清洗，去除噪声和不一致的信息，确保数据的准确性和一致性；同时使用专业的医疗知识进行预处理，如标准化术语等。二是匿名化与脱敏，在遵守相关法规的前提下，对个人健康信息进行匿名化和脱敏处理，以保护患者隐私。</p><p>&nbsp;</p><p>InfoQ：使用开源数据集可能出现产品同质化现象，山海在数据资源方面是如何使用的？</p><p>刘升平：云知声在开发山海医疗大模型时，采取了多种策略来避免产品同质化，确保模型的竞争力。第一，我们使用了不少专有数据集，即云知声多年的医疗业务积累的大量内部医疗数据。这些专有数据可以提高大模型在特定场景的应用效果。第二，&nbsp;我们采用了一些数据增强技术来自动生成训练数据，例如，通过数据合成、噪声注入、标签变换等技术，增加数据的多样性和复杂性，使模型在不同条件下表现更为全面和鲁棒。第三，我们还与医疗专家合作来确保医疗数据的准确性和专业性，同时利用专家的知识来指导数据的预处理和标注。通过这些策略，云知声的山海医疗大模型能够与只使用开源数据集训练的大模型有显著区别，并且在面向具体的医疗场景应用时有更好的效果。</p><p>&nbsp;</p><p>InfoQ：云知声的山海医疗大模型主要做了哪些场景？目前哪个场景的应用率最高？哪个场景能算作山海的“杀手锏”？</p><p>刘升平：对于云知声的山海医疗大模型，主要做了以下场景：</p><p>病历生成：包括基于医患对话的门诊病历和出院小结、手术记录生成等住院病历的生成，以及放射科报告生成等医技科报告。病历质控：对住院病历（包括病案首页）做过程和终末质控，支持1000+形式和内涵质控点，大幅提高病历的质量。单病种上报：对国家卫健委要求的57个病种做自动数据汇集及上报。医保控费：按照医保局的规范，监管医院的临床诊疗行为和收费合理性，确保医疗费用的合规。保险理赔的医疗审核：审核在保险理赔中涉及到的医疗费用，剔除不合理费用。专病库平台：将病历等临床数据自动抽取和导入到专病库。智能问诊：作为AI医生，与患者进行对话，收集症状，并提供初步的健康咨询和建议。</p><p>目前，山海应用率最高的场景是病历生成、病历质控和保险理赔的医疗审核。结合云知声在语音技术上强项开发出的门诊病历生成系统，结合云知声在医疗知识图谱的积累开发的病历质控系统和保险理赔医疗审核系统均可以视为“杀手锏”场景。</p><p>&nbsp;</p><p>InfoQ：针对于山海医疗大模型，您更推荐医疗机构采用哪种部署方式落地？具体是如何考虑的？</p><p>刘升平：云知声的山海医疗大模型在医疗机构的部署通常有以下两种方式：云端部署和私有化部署。至于选择哪种部署方式，主要考虑几个因素吧。一是如果医疗机构对数据安全有极高要求，那就倾向于私有化部署。二是考虑成本与资源，云端部署通常成本较低；私有化部署初期投入大，但长期运营成本可能更低。</p><p>&nbsp;</p><p>InfoQ：现在市面上的医疗大模型不少，国内有许多大厂也在做，山海的独特之处是什么？</p><p>刘升平：这和云知声做医疗大模型的动机是一样的，山海医疗大模型的独特之处主要有两点。&nbsp;一是在专业领域深度方面，云知声专注于医疗领域，有深厚的数据、知识、场景和客户积累，这使得山海医疗大模型在效果上业内领先，目前在医疗大模型综合评测PromptCBLUE和MedBench上都是排名第一。二是在技术融合方面，结合云知声在语音识别和医疗知识图谱技术的专长，山海医疗大模型在语音交互式医疗应用上具有优势，且在临床应用上的医疗知识幻觉也大为减少。</p><p>&nbsp;</p><p>InfoQ：在即将到来的AI&nbsp;con上，您准备向听众分享哪些方面的内容？</p><p>刘升平：主要是分享医疗大模型是怎么用的，是如何做的。我还会以医疗领域为案例，介绍面向应用场景的通用大模型定制优化方法论，相信这对于大模型的行业应用开发有一定的借鉴意义。</p><p>&nbsp;</p><p>嘉宾介绍：</p><p>刘升平，云知声AI&nbsp;Labs&nbsp;研发副总裁，北京大学数学学院博士毕业，是前&nbsp;IBM&nbsp;中国研究院资深研究员，中文信息学会语言与知识计算专委会委员。曾在语义网，机器学习、信息检索，医学信息学，自然语言处理等领域发表过数十篇学术论文和国际国内发明专利。在&nbsp;IBM&nbsp;中国研究院信息与知识组工作期间，刘博士主要负责语义技术及其应用的研发，曾多次获得过&nbsp;IBM&nbsp;研究成就奖。&nbsp;2012&nbsp;年底，刘博士加入云知声&nbsp;AI&nbsp;Labs，领导认知智能团队，负责大语言模型、知识图谱和智慧医疗等方面的研发及管理工作。在云知声期间，主持研发了山海大模型，获得国内外&nbsp;AI&nbsp;评测冠亚军&nbsp;13&nbsp;个，获得北京市科技进步奖一等奖一项。</p><p>&nbsp;</p><p>活动推荐：</p><p><a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f325163430e0188b28bcaaf57a37a8ff.png" /></p><p>&nbsp;</p><p>会议即将开幕，扫码可预约主题演讲直播，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>追踪链接：<a href="https://sourl.co/ih3ffe">https://sourl.co/ih3ffe</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fkmGs83XTyKMBJPs8aFE</id>
            <title>老便宜了！字节跳动豆包大模型开始营业，一元钱能买125万Tokens，月活用户量达2600万</title>
            <link>https://www.infoq.cn/article/fkmGs83XTyKMBJPs8aFE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fkmGs83XTyKMBJPs8aFE</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 08:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 火山引擎, 豆包大模型, 模型推理成本, 大模型服务平台
<br>
<br>
总结: 火山引擎发布了由字节跳动研发的豆包大模型家族，以厘计价定价，降低模型推理成本，推出一站式大模型服务平台火山方舟，提供多模态内容家族和个性化定制智能体。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p></p><p>5 月 15 日，火山引擎发布了字节跳动研发的豆包大模型家族，今天起正式开启对外服务。而豆包的定价，让大模型从以分计价进入到了以厘计价的时代。</p><p></p><p>“不仅效果好，人人用得起的才是好模型。”火山引擎总裁谭待表示，大的使用量，才能打磨出好模型，也能大幅降低模型推理的单位成本。</p><p></p><p>据披露，豆包主力模型 pro-32k 版的模型推理输入价格仅为 0.0008 元 / 千 Tokens，相当于一元钱就能买到 125 万 Tokens，比行业价格低 99.3%；在处理 128K 长文本时，豆包通用模型 pro 的推理输出价格为 0.005元/ 千 Tokens。</p><p></p><p>谭待认为，大模型要做好有三个关键挑战：模型效果、推理成本、落地难度，用的人越多，调用量越大，才能让模型越来越好。在 2024 火山引擎春季 Force 原动力大会上，火山引擎推出的一站式大模型服务平台火山方舟、扣子应用也带来了最新的技术升级动态升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff087a1f4f1450373a549c7ad9741cf8.jpeg" /></p><p></p><p>豆包模型官网：https://www.volcengine.com/product/doubao</p><p></p><p></p><h1>豆包模型家族亮相</h1><p></p><p></p><h1>日均处理 1200 亿 Tokens</h1><p></p><p></p><p>豆包系列模型由字节跳动研发，包括从语义、声音到图像的多模态内容家族，还可以创建个性化定制的智能体，能够通过便捷的自然语言或语音交互，高效完成互动对话、信息获取、协助创作等任务。</p><p>其中，豆包通用模型 pro 是字节跳动自研 LLM 模型专业版，具有理解、生成、逻辑和记忆等综合能力，窗口尺寸最大支持 128K 长文本，并可精调，适配场景更加通用。豆包通用模型 lite 是性价比更高的轻量版，对比 pro 版本千 Tokens 成本下降 84%、延迟降低 50%，为企业提供灵活经济的模型选择。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d19ea74921772b27e2cbe9f3fe04b0f4.jpeg" /></p><p></p><p>在声音方面，豆包有具备语音合成、声音复刻和语音识别方面的三个模型，不仅善于表达多种情绪，而且 5 秒即可实现声音一比一克隆，对音色相似度和声音自然度进行高度还原，还支持复刻声音的跨语种迁移。语音识别效果尤其在科技，教育，医疗等垂直领域表现突出，并善于处理口音、噪音等复杂场景的语音识别。</p><p>而豆包·文生图模型擅长对中国特色文化的理解和输出，豆包·Function Call 模型是当前支持扣子的主力模型，可根据不同的输入指令和情景，选择不同的函数和算法来执行相关任务。</p><p>豆包·角色扮演模型则可以根据人物设定进行演绎，具备个性化的角色创作能力、上下文感知能力强和剧情推动能力，可以满足用户更加个性化的角色扮演需求。据字节跳动产品和战略副总裁朱骏透露，豆包上已有超过 800 万个智能体被创建。</p><p></p><p>此外，朱骏还谈到很多豆包在产品设计上的思考。“用户的核心需求没有变化，包括高效获取信息、工作提效、自我表达、社交娱乐等，在快速演化的是技术。对于大模型的应用，其定义了三个设计原则：拟人化、离用户近、个性化。</p><p></p><p>豆包名字的由来正是，希望产品的名字和大模型一样是拟人化的，像身边亲密的朋友或家人在日常生活当中愿意用的昵称一样，能够成为用户随身携带的“语音百事通”、桌面端文案创作小助手、嵌入到用户现有使用环境的代码生成和注释助手。</p><p></p><p>“经过一年时间的迭代和市场验证，豆包大模型正成为国内使用量最大、应用场景最丰富的大模型之一，目前日均处理 1200 亿 Tokens 文本，生成 3000 万张图片。”谭待表示。</p><p></p><p>现场，谭待还首次披露了豆包大模型的月度活跃用户情况，双端月活用户量达到 2600 万。目前，豆包模型已用于豆包 App、扣子、河马爱学、飞书智能伙伴、抖音电商、剪映、番茄小说等字节跳动旗下产品及业务，并通过火山方舟向智能终端、汽车、金融、消费等行业的众多客户提供服务。</p><p></p><p></p><h1>火山方舟升级 2.0 版来了</h1><p></p><p></p><p>此次火山方舟平台进行了全新的升级，推出方舟 2.0 平台，新平台发布了三个重要的大模型插件。火山方舟是火山引擎发布的大模型服务平台，提供模型训练、推理、评测、精调等全方位功能与服务，并重点支撑大模型生态。</p><p></p><p>火山方舟 2.0 升级的主要亮点如下：</p><p>联网插件：提供抖音头条同款搜索能力，能够实时连接海量优质互联网数据和抖音的独有数据，并且可以通过业内领先的意图识别能力，提供给用户更准确和更全面的回答。内容插件：独家上架了抖音内容插件，可以独家的提供抖音丰富的视频和图文内容，并且作为相关重要信息去丰富大模型和用户的交互过程。RAG 知识库插件：内置了字节跳动多年实践沉淀的大规模高性能向量检索能力，百亿级别数据可以实现毫秒级检索，支持秒级索引流式更新，可以实现新增数据能够实时被检索到，知识库插件也内置了豆包向量化模型，中文场景效果领先， 可以给用户提供更好的搜索相关性。同时，文档解析环节集成了飞书优秀的文档解析能力，支持 pdf、doc、ppt、excel、txt、markdown 等多种复杂类型文档解析能力。</p><p></p><p>除了核心插件外，方舟 2.0 也对系统的承载能力、安全保护能力和算法服务能力进行全面提升。首先是系统承载能力，火山方舟提供了超过万卡公有云 GPU 资源池来支持大模型的推理服务，并能够提供 5 秒接入新建精调模型的弹性调度，仅需 3 分钟就能完成千卡扩容，来支撑企业在应用大模型过程中可能出现的突发流量和业务高峰。</p><p></p><p>在安全可信上，方舟 2.0 通过传输加密、数据加密和独有的大模型安全沙箱功能，能够在模型精调、部署和应用的过程中实现安全增强，不仅可以防止恶意攻击模型的污染，而且可以有效保护企业内部数据不会发生泄露。</p><p></p><p>算法服务方面，火山方舟平台配备了专属的大模型的算法团队。</p><p></p><p></p><h1>“人人都是 AI 应用开发者”</h1><p></p><p></p><h1>扣子专业版发布</h1><p></p><p></p><p>“AI 在通常的理解中是一个难且贵的概念，难在于大模型本身的技术复杂性，而贵在于它的训练和推理成本。目前其主要的时间场景仍局限在搜索引擎和修图工具，但大语言模型真正的潜力远不止于此。”扣子产品经理潘宇扬表示，扣子产品能够连接大模型和用户场景。</p><p></p><p>据介绍，作为新一代 AI 应用开发平台，无论是否有编程基础，都可以在扣子上快速搭建基于大模型的各类 bot，并将其发布到各种社交平台、通讯软件或部署到网站等其他渠道。</p><p></p><p>目前，扣子专业版已集成在火山引擎的大模型服务平台“火山方舟”上，提供企业级 SLA 和高级特性。招商银行、海底捞火锅、超级猩猩、猎聘等企业，已在扣子上搭建了智能体。复旦大学、浙江大学等名校也为课程和实验搭建 AI“助教”。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NH4fIgWpvAtAZE85yx5J</id>
            <title>亚马逊云科技CEO将离职：“云的未来是光明的”</title>
            <link>https://www.infoq.cn/article/NH4fIgWpvAtAZE85yx5J</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NH4fIgWpvAtAZE85yx5J</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 06:26:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊, 云计算, 首席执行官, Matt Garman
<br>
<br>
总结: 亚马逊宣布云计算业务首席执行官辞职，将由Matt Garman接任，Selipsky表示离开是为了花更多时间陪伴家人，AWS面临挑战包括云收入增长放缓和裁员，亚马逊在人工智能领域投资并面临微软Azure的竞争，Garman被描述为“战时”领导者，AWS仍是亚马逊最赚钱的业务之一。 </div>
                        <hr>
                    
                    <p>周二，据亚马逊官方消息称，云计算业务首席执行官Adam Selipsky将于下个月辞职。亚马逊表示，亚马逊网络服务销售和营销高级副总裁Matt Garman将在6月3日离开公司后接替其职位。</p><p>&nbsp;</p><p>在给员工的一份备忘录中，Selipsky表示，他将在入职大约14年后离开亚马逊AWS，以便花更多时间陪伴家人，并表示云业务的“未来是光明的”。</p><p>&nbsp;</p><p>“考虑到业务和领导团队的状况，现在是我做出改变的合适时机，并借此机会花更多时间与家人相处一段时间，充电一下，并创造一些精神上的自由空间，来反思并考虑可能性，”Selipsky写道。</p><p>&nbsp;</p><p>接棒者Matt Garman，他也是一位老将，在亚马逊工作超18年，从实习生一路成为AWS高级副总裁。Matt Garman透露，作为过渡的一部分，亚马逊未来会进行一些组织调整，在未来几周内会看到调整的详细信息。</p><p>&nbsp;</p><p>在 Selipsky 担任首席执行官的三年期间，AWS 的业务面临着诸多挑战，包括企业降本增效潮导致的云收入增长明显放缓。自去年以来，AWS 已经经历了<a href="https://www.cnbc.com/2024/04/03/amazon-layoffs-hundreds-of-jobs-cut-in-cloud-computing-unit.html">至少</a>"<a href="https://www.cnbc.com/2023/04/26/amazon-starts-layoffs-impacting-hr-and-aws-cloud-unit.html">两轮裁员，</a>"减少了超 27,000 名员工。</p><p>&nbsp;</p><p>与此同时，它必须应对生成<a href="https://www.cnbc.com/ai-artificial-intelligence/">人工智能</a>"服务需求的激增，以及来自OpenAI和微软的竞争。在 Selipsky 的领导下，亚马逊向<a href="https://www.cnbc.com/2024/05/14/anthropic-cnbc-disruptor-50.html">Anthropic</a>"投资了 40 亿美元，这是一家由前 OpenAI 员工创立的初创公司。作为计划的一部分，<a href="https://www.cnbc.com/2023/09/25/amazon-to-invest-up-to-4-billion-in-anthropic-a-rival-to-chatgpt-developer-openai.html">Anthropic 同意</a>"指定 AWS 作为其“主要”云提供商，并使用 AWS 的定制人工智能芯片。</p><p>&nbsp;</p><p>亚马逊在云领域的主导地位也受到了微软快速增长的Azure云业务的威胁。当 Selipsky 于 2021 年接任 Jassy 时，分析师估计 Azure 的规模约为 AWS 的 61%。现在，这一比例已接近 77%。微软在 OpenAI 上投资了数十亿美元，其 Azure 云为这家初创公司提供了计算资源。</p><p>&nbsp;</p><p>一位不愿透露姓名的亚马逊消息人士向媒体形容Garman是一位“战时”领导者，并表示亚马逊需要进行变革，以便在人工智能领域变得更加积极主动。该人士表示，亚马逊创始人<a href="https://www.cnbc.com/jeff-bezos/">杰夫·贝佐斯</a>"“非常积极地参与”人工智能领域的工作。</p><p>&nbsp;</p><p>AWS 仍然是云领域的领导者，并且仍然是亚马逊最赚钱的业务部门之一。最近一个季度，它创造了 94.2 亿美元的营业收入，约占亚马逊总收入的 62%。</p><p>&nbsp;</p><p><a href="https://www.ezodproxy.com/amazon/2024/proxy/images/Amazon_Proxy2024.pdf">根据一份证券备案文件，</a>"Selipsky 2022 年的薪酬为 4110 万美元，其中 4070 万美元为股票奖励。他今年没有收到股票授予。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.aboutamazon.com/news/company-news/leadership-update-aws-adam-selipsky-matt-garman">https://www.aboutamazon.com/news/company-news/leadership-update-aws-adam-selipsky-matt-garman</a>"</p><p><a href="https://www.cnbc.com/2024/05/14/amazon-web-services-ceo-adam-selipsky-to-step-down.html">https://www.cnbc.com/2024/05/14/amazon-web-services-ceo-adam-selipsky-to-step-down.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WMamw4IitCrsZbWLDQ4u</id>
            <title>金蝶发布AI管理助手 重构苍穹AI平台</title>
            <link>https://www.infoq.cn/article/WMamw4IitCrsZbWLDQ4u</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WMamw4IitCrsZbWLDQ4u</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 06:18:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金蝶云·苍穹峰会, 企业级AI技术, 超级AI管理助手, 金蝶云·苍穹PaaS平台
<br>
<br>
总结: 2024年5月15日，金蝶云·苍穹峰会在北京举行，聚焦企业级AI技术发展，发布超级AI管理助手Cosmic和新一代AI平台，推动企业管理智能化新时代。 </div>
                        <hr>
                    
                    <p>2024年5月15日，国内ToB领域极具影响力的技术峰会——金蝶云·苍穹峰会（KCCS）在北京盛大举行。本次峰会聚焦AI（人工智能）领域，中国信通院副院长魏亮，IDC中国区副总裁兼首席分析师武连峰，腾讯副总裁、腾讯云总裁邱跃鹏，<a href="https://www.infoq.cn/article/rZJPrbMsV99pdS46otVD?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">金蝶集团</a>"董事会主席兼CEO徐少春等近400位企业技术领袖、AI专家相聚一堂，共同探讨企业级AI技术发展方向，如何让AI更好地赋能企业管理，助力企业加速构建新质生产力，实现高质量发展。</p><p></p><p>当前，人工智能已成为我国发展新质生产力的关键，2024年政府工作报告提出，将开展“人工智能+”行动。面对新赛道，金蝶提前布局，2023年就已发布大模型能力平台苍穹GPT及国内首个财务领域大模型。“从当下到未来十年，一个辉煌的AI红利时代已经到来！”本次峰会上，金蝶集团董事会主席兼CEO徐少春向参会者分享了金蝶在AI领域的最新洞察，并指出，“AI将重构体验、流程和决策，未来每一个企业都需要一个超级管理智能体，每一个员工都需要一个超级智能的AI管理助手。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26390a4c5279e577e20259d90597bf5b.png" /></p><p></p><p></p><h2>智能新物种——超级AI管理助手</h2><p></p><p></p><p>历届苍穹峰会中，金蝶的新技术和新产品都备受企业和开发者的高度关注，本次苍穹峰会的AI新品发布自然备受期待。在未来感十足的发布仪式中，金蝶重磅推出面向未来的企业管理AI新物种——Cosmic：超级智能的AI管理助手。</p><p></p><p>Cosmic基于金蝶超过740万家企业的实践场景沉淀和万亿级训练数据，具备了听说读写的感知能力、能积累并利用管理经验的记忆能力、能理解并计划的思考能力以及能调动系统并实现的行动能力；并可以通过对话式交互和可协同、可扩展的AI应用，助力管理者及员工轻松应对财务管理、数据分析、合同处理、干部遴选等多项管理工作，让企业运转得更流畅、更高效。目前，Cosmic覆盖财务、人力、供应链等多种业务场景，并致力于“让人人都有一个AI管理助手”；同时，Cosmic也将AI全线赋能金蝶面向大、中、小市场的各类SaaS产品。</p><p></p><p>和传统AI产品相比，金蝶提供具备“大模型+财务”等垂直领域的真实落地实践，例如中国金茂、建发房地产、河北联通及江苏益客等，已有众多案例在推进中。</p><p></p><p>在面向大企业的金蝶云·星瀚财务云中，Cosmic支持业务发起、多模态智能审核以及财务指标查询和分析等功能。其中建发房产与金蝶共建合同中台管理系统，基于大模型AI为驱动，助力合同范本、合同起草、合同预审、合同审批、合同履约等全生命周期业务、流程、数据等智能化应用服务能力。</p><p></p><p>在人力资源管理领域中，金蝶打破了业内聚焦在单一的招聘应用场景，实现AI覆盖范围更广的同时，应用也更深入。如星瀚人力云中，Cosmic支持智慧JD助手、猜你合适，以及面试官助手等功能。其中海信集团与金蝶在人力资源管理领域共创AI应用，打造了员工活水平台及将近20个业务场景，实现员工全旅程和人才供应链全链路的智能化体验，内部招聘比例提升了120%，干部考察过程效率提升了70%。</p><p></p><p></p><h2>重构新底座——更先进的苍穹AI平台</h2><p></p><p></p><p>同时，金蝶全线云产品的数字化底座——金蝶云·苍穹PaaS平台也同步进化。本次峰会上，金蝶云·苍穹重构为新一代企业级AI平台，具备<a href="https://www.infoq.cn/article/ElML0Zu1Q2PLAeyNjRlw?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大模型</a>"、AI工具、AI可信、Agent&nbsp;Builder等能力，结合原有的云原生、低代码等基础优势，可助力企业快速开发多种AI管理助手。自2018年发布以来，金蝶云·苍穹已获得招商局集团、中国通用技术、山东重工集团、海信集团等众多500强企业青睐，成为众多大型企业的共同选择。近日，国际数据公司IDC报告也显示，金蝶凭借金蝶云·苍穹，连续4年在中国低代码与零代码软件市场占有率第一！这也彰显了金蝶在中国PaaS市场的领先地位。</p><p></p><p>Cosmic的出现也在加速改变企业管理软件的交互方式，使用户与系统的交互界面更友好、高效：从过去的导航、页签、过滤条件，层次繁琐，效率不高，进化成对话式、卡片、多媒体，个性自然，精准高效。Gartner发布的《预测2024：ERP利用自动化和人工智能改进计划工作》显示，到2027 年，60% 的客户在更换 ERP 应用程序时会选择具有平台能力和业务流程编排能力的软件。GenAI 的“自主生成有洞察力的报告”和“自动化重复性工作”能力将会改变ERP市场。</p><p></p><p>当前，金蝶正全面发力人工智能，“All&nbsp;in&nbsp;AI”，推动“订阅优先、AI优先”战略加速落地。未来，金蝶将与更多关注新兴技术的企业、政府机构，以及锐意创新的开发者们携手同行，共创企业管理智能化的新时代。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NtswPKarmY6xKIHFoGBm</id>
            <title>突发！OpenAI 创始人 Ilya 官宣离职，已有意义重大的下一步计划？</title>
            <link>https://www.infoq.cn/article/NtswPKarmY6xKIHFoGBm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NtswPKarmY6xKIHFoGBm</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 06:16:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 北京时间, OpenAI, Ilya Sutskever, AGI
<br>
<br>
总结: 北京时间5月15日早上7点，OpenAI联合创始人Ilya Sutskever在社交平台上宣布离职，表示相信OpenAI将在领导下打造安全有益的AGI。Ilya的离职引起了Altman和Brockman的回应，Jakub Pachocki接替Ilya的首席科学家职位。对于Ilya的下一步计划和未来发展，人们充满猜测和期待。 </div>
                        <hr>
                    
                    <p>北京时间&nbsp;5&nbsp;月&nbsp;15&nbsp;日早上&nbsp;7&nbsp;点，OpenAI&nbsp;联合创始人&nbsp;Ilya&nbsp;Sutskever（伊尔亚&nbsp;·苏茨克维）在社交平台上发文表示，决定离开&nbsp;OpneAI。</p><p></p><p>Ilya&nbsp;称，公司的发展轨迹堪称奇迹，相信&nbsp;OpenAI&nbsp;将在&nbsp;CEO&nbsp;Sam&nbsp;Altman（萨姆·奥特曼）、总裁&nbsp;Greg&nbsp;Brockman（格雷戈里·布罗克曼）、首席技术官&nbsp;Mira&nbsp;Murati（米拉·穆拉蒂）以及&nbsp;Jakub&nbsp;Pachocki（雅各布·帕乔基，将接任&nbsp;Ilya&nbsp;担任首席科学家）的领导下，打造既安全又有益的&nbsp;AGI（通用人工智能）。</p><p></p><p>推文的最后，Ilya&nbsp;表示他已经有了下一步计划，不过目前还不能与大家透露太多细节。</p><p></p><p>Ilya&nbsp;宣布离职的时机也非常讲究，刚好在<a href="https://mp.weixin.qq.com/s/6butUF59mbEFRLCcHy10BA">谷歌年度&nbsp;I/O&nbsp;大会</a>"之后，又把全世界的目光集中到&nbsp;OpenAI&nbsp;这边来。就像今年二月谷歌发布&nbsp;Gemini&nbsp;1.5&nbsp;后立马扔出重磅炸弹&nbsp;Sora&nbsp;一样，OpenAI&nbsp;“梅开二度”，再次截胡了谷歌的流量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed348a48033c5975e9efb0a8286c722a.png" /></p><p></p><p>对于&nbsp;Ilya&nbsp;的离开，&nbsp;Altman&nbsp;也发文做了回应。这一次，他没有按往常的习惯全用小写字母发帖，整段文字显得很正式。“&nbsp;Ilya&nbsp;无疑是我们这一代最伟大的思想家之一，是我们领域的引路人，也是我亲爱的朋友。”“如果没有他，OpenAI&nbsp;就不会有今天。”</p><p></p><p>Brockman&nbsp;也对&nbsp;Ilya&nbsp;表示了感谢，并称其为一位艺术家。“尽管人们怀疑&nbsp;AGI&nbsp;是否在可预见的未来出现，但我们会仔细思考并采取行动，坚信深度学习可以带我们到达那里。任务还远未完成，&nbsp;Ilya&nbsp;在帮助&nbsp;OpenAI&nbsp;奠定今天的基础方面发挥了关键作用。谢谢你所做的一切。”</p><p></p><p></p><h2>Jakub&nbsp;接棒&nbsp;Ilya&nbsp;</h2><p></p><p></p><p>此外，&nbsp;Altman&nbsp;也官宣由&nbsp;Jakub&nbsp;Pachocki（&nbsp;Jakub&nbsp;·帕乔奇）接替&nbsp;Ilya&nbsp;的首席科学家职位。“我很高兴他（&nbsp;Jakub&nbsp;）能在这里接过接力棒。他曾主导过我们许多最重要的项目，我非常有信心，他将带领我们快速、安全地完成使命，确保&nbsp;AGI&nbsp;惠及每一个人。”</p><p></p><p>Jakub&nbsp;本科毕业于波兰华沙大学，博士毕业于卡耐基梅隆大学，又在哈佛大学做过一年博士后。2017&nbsp;年离开学术界后，OpenAI&nbsp;是他在工业界第一份也是唯一一份全职工作。</p><p></p><p>正如&nbsp;Altman&nbsp;所说，&nbsp;Jakub&nbsp;曾担任&nbsp;OpenAI&nbsp;Dota&nbsp;游戏项目的研究主管，这是&nbsp;OpenAI&nbsp;在&nbsp;All&nbsp;in&nbsp;大语言模型之前最成功也是最出圈的项目。再后来，&nbsp;Jakub&nbsp;的名字也出现在&nbsp;ChatGPT&nbsp;和&nbsp;GPT-4&nbsp;的贡献人员名单中，对于&nbsp;GPT-4，他既是整体负责人之一，也是优化团队负责人。</p><p></p><p>去年&nbsp;11&nbsp;月，&nbsp;Altman&nbsp;被解雇风波后，时任公司研究主管的&nbsp;Jakub&nbsp;也被曝出宣布辞职。</p><p></p><p>对于&nbsp;Ilya&nbsp;的离开，&nbsp;Jakub&nbsp;发文称，&nbsp;Ilya&nbsp;将他带入了深度学习研究的世界，多年来一直是他的导师和出色的合作伙伴，“他对深度学习未来发展的远见卓识为&nbsp;OpenAI&nbsp;和人工智能领域奠定了基础。我非常感谢他与我们进行了无数次对话，从关于人工智能未来发展的高层讨论到深入的技术白板会议。”</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>目前，对于&nbsp;Ilya&nbsp;下一步会去哪？做什么，大家也给出了不少猜测。其中“加入马斯克&nbsp;xAI”是很受欢迎的说法，充满戏剧性，也并非没有可能。此外，关于&nbsp;Ilya&nbsp;是否真的看见了&nbsp;Q*&nbsp;也一直是人们关注的焦点。</p><p></p><p>当时机合适的时候，希望&nbsp;Ilya&nbsp;能为大家揭晓答案，期待那一天不会太远。</p><p></p><p>参考链接：</p><p></p><p>https://x.com/ilyasut/status/1790517460594266508</p><p></p><p>https://x.com/sama/status/1790518031640347056</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/oXcQ0G4aaI7BexV40yo5</id>
            <title>AI 代码助手革新编程界：腾讯云专家汪晟杰深度剖析机遇与挑战</title>
            <link>https://www.infoq.cn/article/oXcQ0G4aaI7BexV40yo5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oXcQ0G4aaI7BexV40yo5</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 May 2024 03:39:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 代码大模型, 工程师效率, 安全性, 隐私性
<br>
<br>
总结: 代码大模型的出现极大提升了工程师的效率，但同时也带来了安全性与隐私性问题的挑战。如何应对这些挑战？有哪些最佳实践可以帮助企业在利用这些AI工具时确保代码安全和隐私保护？软件开发者应该如何准备和适应这种由AI带来的变革？AI工具接管部分编程任务后，开发者的角色又会发生哪些实际变化？ </div>
                        <hr>
                    
                    <p>代码大模型的出现极大提升了工程师的效率，但同时也带来了安全性与隐私性问题的挑战。如何应对这些挑战？有哪些最佳实践可以帮助企业在利用这些AI工具时确保代码安全和隐私保护？软件开发者应该如何准备和适应这种由AI带来的变革？AI工具接管部分编程任务后，开发者的角色又会发生哪些实际变化？</p><p>在即将举行的<a href="https://aicon.infoq.cn/2024/beijing/">AICon全球人工智能开发与应用大会</a>"上，我们有幸邀请到了腾讯云产品专家汪晟杰，他将发表题为《代码大模型对于工程理解的探索研究》主题分享。在会前，我们对汪晟杰进行了访谈，以下为访谈实录：</p><p></p><p>嘉宾：汪晟杰</p><p>编辑：李忠良</p><p></p><h4>技术有效性和限制</h4><p></p><p>InfoQ：您如何评价当前AI代码助手如GitHub&nbsp;Copilot在理解复杂代码结构和项目架构方面的能力？</p><p>汪晟杰：当前的AI代码助手，如GitHub&nbsp;Copilot，以及腾讯云AI代码助手，都展示了在理解复杂代码结构和项目架构方面的显著进步。他们都有着如下优点：</p><p>在编写简单到中等复杂度的代码时，它们可以提供有用的代码建议和补全，从而提高开发者的代码生产力。通过分析大量的开源代码库，它们可以学习到许多编程语言和框架的最佳实践。对于某些常见的编程任务，它们可以生成准确的代码片段，减少开发者的工作量。</p><p>然而面临着成本和速度的权衡，以及如何塞下整个工程代码上下文来理解工程。譬如对于非常复杂的代码结构、大仓或者多仓的项目重度依赖的情况，AI代码助手可能无法完全理解其逻辑和设计，导致生成的代码片段不准确或不适用。最近GitHub&nbsp;Copilot的企业版的知识库可以对项目工程做Indexing+Embedding，可以大大强化本地开发并享用远端向量，从而提升对于工程理解的提问和回答。这块我将在本次分享中重点和大家分享。</p><p></p><p>InfoQ：针对多文件和大型项目，这些工具在理解上下文和逻辑关系方面表现如何？</p><p>汪晟杰：在补全场景下，对于常见的编程模式和结构，AI代码助手通过语法分析等多种策略，可以较好地识别和理解多文件之间的关系。比如你用了工厂单例模式构造一个对象，在调用上就知道我这个对象要用到工厂类。在GitHub&nbsp;Copilot&nbsp;实战中，需要打开相关的文件。在腾讯云AI助手上，我们采用了快速的语法树能力快速找到相关文件引入提示词，从而让大模型能感知到更多上下文。通过分析大量的开源代码库，它们可以学习到许多编程语言和框架的最佳实践，从而在一定程度上理解大型项目的结构和组织方式。</p><p></p><p>InfoQ：在使用如CoT和RAG这类技术时，有哪些明显的优势和存在的局限性？</p><p>汪晟杰：CoT（Chain&nbsp;of&nbsp;Thought），本质上是对于提问者的问题的思维链的拆解，并逐步去解决子任务的要求并合并成最终答案。</p><p>首先在上下文理解上：CoT有效的通过Multi-Agent方式，来拆解并安排下一轮的子任务，同时可以通过RAG进行代码推理，从而生成更符合需求的代码。其次，有高质量的代码生成：通过模拟人类程序员的思考过程，自主获得并进行下一轮的执行，可以选择不同模型、或者Function&nbsp;call来调用业务函数，或者通过上下文找到问题出错点并在下一轮进行修复方案。RAG则是保障了项目代码扩展理解能力。</p><p></p><p>InfoQ：您认为未来这些工具需要哪些改进才能更好地支持复杂的软件开发任务？</p><p>汪晟杰：当前的AI代码助手，如GitHub&nbsp;Copilot，已经在简化软件开发任务方面取得了显著的进步。然而，要更好地支持复杂的软件开发任务，未来这些工具可能需要以下几方面的改进：</p><p>更好的上下文理解：AI代码助手需要更好地理解项目的上下文，包括项目的目标、架构、已有代码的功能等。这可以通过更先进的自然语言处理和代码分析技术来实现。更强更快的代码推理能力：对于复杂的代码逻辑和算法，AI代码助手需要有更强的推理能力，以生成正确和高效的代码。这可能需要更先进的机器学习模型和算法。更全面更深的集成IDE：AI代码助手需要更广泛的支持主流的IDE，并深入地集成到集成开发环境（IDE）中，以提供更流畅和无缝的用户体验。这可能包括更好的代码提示、实时错误检测、代码重构建议等功能。更全面的编程语言和框架支持：AI代码助手需要支持更多的编程语言和框架，以满足不同开发者的需求。这可能需要分析和学习更多的开源代码库。更高的安全性和可靠性：AI代码助手需要在生成的代码中考虑到安全性和可靠性，避免引入潜在的安全风险和错误。</p><p></p><p></p><h3>安全性和隐私问题</h3><p></p><p>InfoQ：在使用AI编程助手时，如何处理和保护敏感和私有的代码数据？</p><p>汪晟杰：有以下六个方面值得考虑。</p><p>选择可信赖的AI编程助手：在选择AI编程助手时，选择那些来自可信赖来源、有良好声誉的工具，这些工具通常会遵循严格的数据保护政策和安全实践。我也建议不要把核心代码用GitHub&nbsp;Copilot去生成，因为你的代码上下文是直接经过他们海外服务器。了解数据保护政策：在使用AI编程助手之前，详细了解其数据保护政策和隐私条款。确保这些政策符合您对数据保护的要求，特别是关于数据的收集、处理和存储方面。是否提供安全私有化能力：在银行等领域腾讯云积累了很多客户实践。我们一键部署升级，并在封闭的环境、信创环境下都有着不错的客户反馈。对于技术对话解决了在不可上网的环境下，搜索技术问题找寻答案的另一种安全方法。遵循最佳实践：在编写代码时，遵循最佳实践，将敏感信息（如密码、API密钥等）从代码中分离。将这些敏感信息存储在安全的配置文件或环境变量中，而不是直接嵌入到代码中。限制访问权限：确保AI编程助手仅能访问其需要的最小权限。例如，可以限制其访问特定的代码库、分支或文件夹，以减少潜在的风险。监控和审计：定期监控和审计AI编程助手的使用情况，确保其符合您的安全和合规要求。如果发现任何异常行为，立即采取相应的措施。</p><p></p><p>InfoQ：您如何看待这些工具在训练过程中可能出现的数据泄露风险？</p><p>汪晟杰：首先，AI编程助手通常使用大量的开源代码库进行训练。虽然这些代码库本身是公开的，但在训练过程中可能会捕获到一些敏感信息，如API密钥、密码等。因此，训练过程中需要对这些潜在的敏感信息进行清理和过滤；其次，由于AI模型在训练过程中可能会学习到一些敏感信息，因此在使用模型生成代码时，有可能泄露这些信息。应用端需要针对这类问题，采用技术手段，以增加兜底逻辑，即模型训练过程中数据的隐私问题，可以由应用端做针对性的过滤。最后，用户教育和意识：对于使用AI编程助手的开发者，提供培训和意识教育，以确保他们了解如何在使用这些工具时保护敏感和私有的代码数据。这包括遵循最佳实践，将敏感信息从代码中分离等。</p><p></p><p>InfoQ：有哪些最佳实践可以帮助企业在利用这些AI工具时确保代码安全和隐私保护？</p><p>汪晟杰：一方面是用户开发习惯，在让模型基于上下文推理的时候，他会模仿你的习惯，所以将敏感信息从代码中分离，在代码库中引入代码扫描，实时监听代码生成质量。另一方面是给予仓库代码更小范围，比如我只需要把主要描述的Readme文件、接口文档、核心代码的实现类等作为RAG的来源，或者在补全上找到核心调用链的相关函数及文件</p><p></p><h4>对开发者角色的影响</h4><p></p><p>InfoQ：AI工具在接管一些编程任务后，您观察到开发者的角色有哪些实际变化？</p><p>汪晟杰：有三方面的影响，首先是更高层次的抽象：开发者可能会从处理底层代码转向处理更高级别的抽象，例如设计软件架构、优化数据结构和算法等。这将使AI代码助手能够更有效地理解并模仿生成；其次是更全面的技术点：有了AI助手后，后端也会写前端代码，在做一些短平快的项目时，一个产品和一个技术可以分工完成，相比之前的开发效率是大大提升；最后当然是开发习惯的变化：以IDE为平台，以AI为内核，以对话为切入，以编码质量为验收，会是开发者在日常编码中的另一个自己的「数字人」</p><p></p><p>InfoQ：这些变化对开发团队的结构和工作流程有何影响？</p><p>汪晟杰：我认为团队会更扁平，技术同学也不会再抗拒新的某种技术和语言。上手门槛变低了，获取知识的速度提高了，解决问题的方式多样化了。在工作流程中，学习提示词，摸透大模型的习性，会是工作中不可缺少的一部分。逐步上手后，会产生极大粘性。腾讯内部我们的产品的留存率是非常高的。</p><p></p><p>InfoQ：您认为AI工具将如何影响软件开发行业的就业趋势？</p><p>汪晟杰：大概有以下几个方面。</p><p>自动化低级任务：AI工具可以自动化许多重复性和低级别的编程任务，如CRUD的代码生成、SQL&nbsp;injection&nbsp;错误检测和修复等。这可能导致对于那些主要从事这些任务的初级开发人员的需求减少。提高生产力：通过自动化一些任务，AI工具可以提高开发者的生产力。这意味着开发团队可能需要更少的人员来完成相同的工作量。然而，这也可能导致对高技能开发人员的需求增加，因为他们可以更好地利用这些工具。AI化转型和咨询：随着AI工具的普及，软件开发人员可能需要学习新技能和知识，以适应不断变化的技术环境。这可能包括学习如何与AI工具合作，以及掌握新的编程范式和技术。AI产品化的创新：随着AI工具接管一些基本任务，开发者可以将更多精力投入到创新和创意上。这可能导致对具有创新思维和能够开发新产品和服务的开发人员的需求增加。与大模型及算法的紧密合作：AI工具的发展可能导致业务要与大模型及算法团队的合作更加紧密。新的就业机会：虽然AI工具可能导致某些角色的需求减少，但它们也可能创造新的就业机会。例如，随着AI技术的发展，可能会出现新的专业领域，如AI伦理、AI系统监管等。</p><p>总之，AI工具将对软件开发行业的就业趋势产生深远影响。虽然某些角色可能受到冲击，但整体上，对具有创新思维、高技能和跨领域知识的软件开发人员的需求可能会增加。为了适应这些变化，开发人员需要不断学习和更新技能，以保持在行业中的竞争力。</p><p></p><p>InfoQ：对于软件开发者来说，他们应该如何准备和适应这种由AI带来的变革？</p><p>汪晟杰：首先学习AI和机器学习基础知识：开发者应掌握AI的基本概念、原理和技术，了解机器学习算法和数据科学库（如TensorFlow、PyTorch等），这将有助于他们在开发过程中更好地利用AI技术；其次，关注AI领域的最新发展：关注AI领域的最新研究成果和行业动态，了解AI技术在各个行业的应用案例，以便了解哪些技术可以应用到自己的项目中；当然，提高编程技能也不可或缺：AI技术的发展对开发者的编程能力提出了更高的要求，因此开发者需要不断提高自己的编程技能，熟悉各种编程语言和框架，如Python、Java、C++等；最后是学会与AI合作：开发者需要学会如何与AI系统合作，理解AI系统的优势和局限性，以便在开发过程中充分发挥AI的潜力。</p><p></p><p>嘉宾介绍：</p><p>汪晟杰&nbsp;腾讯云&nbsp;产品专家，历任阿里高级技术专家，从事钉钉云效核心业务线、Teambition&nbsp;合伙人、Autodesk&nbsp;首席软件架构师、十多年&nbsp;SAP&nbsp;云平台、SuccessFactors&nbsp;HCM、Sybase&nbsp;数据库、PowerDesigner&nbsp;等产品的开发经理，在软件架构设计、产品管理和项目工程管理、团队敏捷提效等方面拥有近&nbsp;20&nbsp;年的经验。</p><p></p><p>在5月17日-18日，AICon&nbsp;即将落地北京，汪晟杰即将与大家进行演讲分享，期待与你一起现场交流。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2733e1351759f7f9f924f0c7e9e16e5.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DaHPBH0eyjwM7DhfQxbl</id>
            <title>谷歌这次又“杀疯了”！200万token长文本能力问鼎全球最强，一场大会，AI被提了120次</title>
            <link>https://www.infoq.cn/article/DaHPBH0eyjwM7DhfQxbl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DaHPBH0eyjwM7DhfQxbl</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 May 2024 19:41:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Google, Gemini, 人工智能, 模型更新
<br>
<br>
总结: 谷歌举办年度开发者大会，以人工智能为主题，宣布Gemini系列模型的重大更新，包括Gemini 1.5 Pro和Gemini 1.5 Flash，以及即将推出的Gemini Advanced。Gemini模型具有超人的视觉感知和多模式功能，同时解决了大模型开发的成本问题。此外，谷歌还推出了Project Astra通用AI代理和最新的AI媒体创作模型Veo和Imagen 3，展示了对抗OpenAI的态势。 </div>
                        <hr>
                    
                    <p>今天，Google 年度开发者 I/O 大会2024 在加利福尼亚州山景城的 Shoreline Amphitheatre 举行，此次大会以 Alphabet 首席执行官桑达尔·皮查伊 (Sundar Pichai) 的主题演讲拉开序幕。谷歌此前已经明确表示，今年的 I/O 大会将全部围绕人工智能展开。</p><p>&nbsp;</p><p></p><blockquote>这次大会上，皮查伊宣布了谷歌内部的最新技术进展，尤其是围绕 Gemini 所做的所有工作。</blockquote><p></p><p></p><h2>狂卷长文本，Gemini家族迎来重大更新</h2><p></p><p>&nbsp;</p><p>“我们希望每个人都能从 Gemini 所做的事情中受益，”皮查伊说。他还透露了 Gemini 将如何融入谷歌的许多服务中。人们使用 Google 搜索的方式比以往任何时候都多，关键字搜索的时间甚至更长。</p><p>&nbsp;</p><p>大会一开始皮查伊就宣布了Gemini系列大模型的更新。首先是Gemini 1.5 Pro，可提供100万长文本能力，并且已经向全球开发者开放。</p><p>&nbsp;</p><p>Gemini 1.5 Pro是在上个月举办的Google Cloud Next&nbsp;2024大会上发布的，具有原生音频理解、系统指令、JSON 模式等。</p><p>&nbsp;</p><p>Gemini 1.5 Pro 能够使用视频计算机视觉来分析图像（帧）和音频（语音）的视频，这使其具有人类水平的视觉感知。使用深度神经网络，Gemini 1.5 可以以超人的精度识别图像（和视频帧）中的物体、场景和人物。&nbsp;</p><p>&nbsp;</p><p>成本问题一直是大模型开发的痛中之痛，为了解决这一痛点，谷歌DeepMind首席执行官Demis Hassabis宣布推出Gemini 1.5&nbsp;Flash模型，该模型旨在兼顾快速和成本效益。</p><p>&nbsp;</p><p>“Gemini 1.5 Flash 擅长摘要、聊天应用程序、图像和视频字幕、从长文档和表格中提取数据等，”Google DeepMind 首席执行官 Demis Hassabis 此前在博客文章中写道。 Hassabis 补充说，谷歌创建 Gemini 1.5 Flash 是因为开发人员需要一个比Gemini 1.5 Pro更轻、更便宜的模型。</p><p>&nbsp;</p><p>Gemini 1.5 Flash 介于 Gemini 1.5 Pro 和 Gemini 1.5 Nano 之间，是针对开发者的大模型。尽管比 Gemini Pro 轻，但它的功能同样强大，谷歌表示这是通过“蒸馏”的方式来实现的，将 Gemini 1.5 Pro 中最重要的知识和技能转移到较小的模型上。这意味着 Gemini 1.5 Flash 将获得与 Pro 相同的多模式功能，以及其长上下文窗口（AI 模型一次可以摄取的数据量），100万个token。</p><p>&nbsp;</p><p>最大的更新尚未到来——谷歌宣布今年晚些时候将模型的现有上下文窗口增加一倍，达到 200 万个token。这将使其能够同时处理 2 小时的视频、22 小时的音频、超过 60,000 行代码或超过 140 万个单词。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/fd/fd45929c008365d6f993b12048fa6874.png" /></p><p></p><p>谷歌的 Josh Woodward 详细介绍了 Gemini 1.5 Pro 和 Flash 的定价。Gemini 1.5 Flash 的价格定为每 100 万个token 35 美分，这比 GPT-4o 的每 100 万个token 5 美元的价格要便宜得多。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b7/b77857728056d5cbcba74d95ae9d003f.png" /></p><p></p><p>值得一提的是，此次大会谷歌重磅宣布推出基于Gemini 1.5 Pro 的 Gemini Advanced。升级后的Gemini Advanced可以处理“多个大型文档，总计最多 1,500 页，或汇总 100 封电子邮件”。支持 35 多种语言和 150 多个国家/地区。而其“即将”推出的功能是能够“处理一个小时的视频内容或超过 30,000 行的代码库”。全球最强长文本能力可谓实至名归。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce12ac067757b2aee189620351af5519.png" /></p><p></p><p>该公司还正在开发名为Project Astra的通用AI代理。大会现场，Demis Hassabis展示了Astra模型，该模型通过智能手机摄像头分析世界，并与用户进行对话。&nbsp;Demis Hassabis 表示，他的团队“一直希望开发对日常生活有帮助的通用人工智能代理”。 Project Astra 是这方面进展的结果。</p><p>&nbsp;</p><p>Project Astra 类似一款以取景器作为主界面的应用程序。谷歌在演讲中展示了一个人拿着手机，将摄像头对准办公室的各个地方，并用语言与其交互：“当你看到有东西发出声音时，请告诉我。”在这段视频演示中，Gemini能识别各种物体甚至代码，并实时与人类进行语音互动。</p><p>&nbsp;</p><p>在视频中，Astra 的反应很快。之所以能实现这一目标，是因为这些“Agent”“旨在通过连续编码视频帧、将视频和语音输入组合到事件时间线中，并缓存这些信息以进行有效回忆，从而更快地处理信息。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9d/9d6917fa365038522a52eca462afc34a.png" /></p><p>&nbsp;</p><p>随后，Demis Hassabis宣布推出最新AI媒体创作模型 Veo 和 Imagen 3。</p><p>&nbsp;</p><p>据Demis Hassabis介绍，Veo可以制作“高质量”1080p 视频，Imagen 3是最新的文本到图像框架。这两个听起来都不是特别革命性的，但它们是谷歌继续对抗OpenAI 的 Sora 视频模型和Dall-E 3的一种方式，Dall-E 3 实际上已经成为AI生成图像的代名词。</p><p>&nbsp;</p><p>谷歌声称 Veo 具有“对自然语言和视觉语义的高级理解”，可以创建用户想要的任何视频。AI生成的视频可以持续“超过一分钟”。 Veo 还能够理解电影和视觉技术，例如延时拍摄的概念。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd20facdef6ba24218a0f093e8cf08c4.png" /></p><p></p><h2>Gemini能力加持，谷歌搜索引擎迎来颠覆式变革</h2><p></p><p>&nbsp;</p><p>谷歌搜索负责人Liz Reid&nbsp;宣布了对全球主导搜索引擎进行人工智能驱动的重大变革。以往，当用户在使用搜索引擎时，通常以文字或图片形式呈现。如今，作为推动将生成式人工智能添加到搜索中的一部分，谷歌引入了一个新的转折点：视频。 Gemini 会让用户上传演示其要解决的问题的视频，然后启动搜索在论坛和互联网的其他区域以找到解决方案。</p><p>&nbsp;</p><p>除了将Gemini能力加持到搜索引擎外，Gemini 还将为 Gmail 应用程序提供一些有趣的新功能，包括长电子邮件线程的摘要。用户还可以直接与 Gemini 聊天，从整个收件箱中查找详细信息。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/cc/cc5febebb3a770180b39d43656d53ff1.png" /></p><p></p><p>为了提供更个性化的体验，Gemini Advanced 订阅用户很快将能够创建 Gems —— Gemini 的定制版本。</p><p>&nbsp;</p><p>Gems 可以让用户个性化地创建聊天机器人，可以帮助用户完成某些任务并保留特定的特征，有点像在Character.AI中制作自己的机器人，该服务可以让用户与流行角色和名人的虚拟版本甚至虚拟心理医生交谈。谷歌表示，你可以让 Gemini 成为你的健身伙伴、副主厨、编码伙伴、创意写作指南或任何你能想到的东西。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/08/0801017bf87278c92ee001912a3af489.png" /></p><p></p><h2>下一代开放模型Gemma再迎重大更新</h2><p></p><p>会上，谷歌还分享了Gemma的一系列更新，Gemma是谷歌的开放模型系列，采用与创建 Gemini 模型相同的研究和技术构建。此次谷歌在原来模型基础上宣布推出Gemma 2。谷歌称这是用于负责任的人工智能创新的下一代开放模型。 Gemma 2 采用全新架构，旨在实现突破性的性能和效率，并将提供27B大小的尺寸。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f5/f5e82bd09b4bb73df0d2ce91d50a38b4.png" /></p><p></p><p>Gemma 家族也在随着PaliGemma 的扩展而扩展。据悉，PaliGemma 是谷歌受PaLI-3启发的第一个视觉语言模型。他们还使用LLM Comparator升级了Responsible Generative AI Toolkit，用于评估模型响应的质量。&nbsp;</p><p></p><h2>移动操作系统Android 15将深度集成Gemini</h2><p></p><p>&nbsp;</p><p>I/O 大会最主要的特色就是面向开发者。在 I/O 大会上，谷歌提到了即将推出的安卓新版本，即以 AI 为核心的 Android，今年将实现三项突破：在 Android 上提供更好的搜索、Gemini 正在成为你的 AI 助手，以及设备上的 AI 将解锁新的体验。</p><p>&nbsp;</p><p>谷歌于 2023 年 10 月发布了Android 14，此次大会之前，谷歌已经发布了Android 15的第一个测试版。追溯历史，谷歌曾以甜点命名安卓版本，然而从 Android 10 开始，他们决定在未来所有版本中仅使用版本号进行命名。因此，新的大版本也就顺理成章地被称作 Android 15。不过，谷歌依然保留了内部使用甜点代号的习惯，Android 15 的内部代号为“香草冰淇淋（&nbsp;Vanilla Ice Cream）”，这个版本即将推出。</p><p>&nbsp;</p><p>在活动上，谷歌宣布对其适用于 Android 设备的 Gemini AI 聊天机器人进行一些改进：Gemini 正在“成为 Android 上新的人工智能助手”。这也意味着大模型现已成为 Android 操作系统的一部分，使其能够以更全面的方式集成。</p><p>&nbsp;</p><p>与底层操作系统的集成后，就能实现一些更酷的功能。Android 上的 Gemini 具有更强的上下文感知能力，可以覆盖在正在使用的任何应用程序之上，因此你无需来回切换。还有一个巧妙的功能，可以让你将图像从 Gemini 应用程序拖放到另一个应用程序中。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/36/3640bb52a9a83d54a6540a15c1487178.png" /></p><p></p><p>&nbsp;很容易看出这项技术的发展方向。一旦 Gemini 能够访问手机的应用程序库，它就能够真正兑现Humane和Rabbit所承诺的愿景。谷歌表示，它“刚刚开始研究设备上的人工智能如何改变你的手机的功能”，因此我们想象未来至少会与 Uber 和 Doordash 等应用程序集成。</p><p>&nbsp;</p><p>谷歌还向我们展示了直接通过 Pixel 8a 上的 Google Messages 应用程序使用 Gemini 的不同方式。它包括能够分析 PDF 或视频并向 Gemini 提出问题，获得清晰（并引用）的答复。这些功能将在“未来几个月”出现在更多设备上。</p><p></p><h2>低调官宣第六代TPU，峰值计算性能提高4.7 倍</h2><p></p><p>&nbsp;</p><p>此前，软件部分一直是I/O大会上的主要部分，本次大会也不例外。大会进行到中途，皮查伊低调宣布了谷歌的第六代张量处理单元 (TPU) 称为 Trillium，将于今年晚些时候向其云客户提供。 TPU 可能不是谷歌当今众多人工智能更新中最华丽的，但它是其人工智能工作的重要组成部分。</p><p>&nbsp;</p><p>据介绍，作为“迄今为止性能最强、能效最高的 TPU”，Trillium宣称与TPU v5e 相比，每个芯片的峰值计算性能提高了 4.7 倍。</p><p>&nbsp;</p><p>Gemini 完全在谷歌的第四代和第五代 TPU 上接受训练和服务。包括 Anthropic 在内的其他领先人工智能公司也在 TPU 上训练了他们的模型。</p><p>&nbsp;</p><p>皮查伊表示，“25 年来，我们投资建设了世界一流的技术基础设施。从支持搜索的尖端硬件，到支持人工智能进步的定制张量处理单元。我们将于 2024 年末向我们的云客户提供 Trillium。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0759403ce2de78d93d22f60249fbe774.png" /></p><p>&nbsp;</p><p></p><h2>写在最后</h2><p></p><p>&nbsp;</p><p>皮查伊最后出场总结了这场以人工智能为主的主题演讲，他特别提到今天谷歌提到了120次AI。</p><p>&nbsp;</p><p>一周前，皮查伊接受彭博采访时讲到，谷歌年度开发者大会较少聚焦于特定的产品发布，而更多地聚焦于正在经历的旅程，如何设想人工智能改变谷歌产品的愿景，以及如何逐步将这些变革引入现实。他表示谷歌已经在在搜索中运用了Transformer技术，这一技术极大地提升了谷歌搜索的质量，“因此，我们已经在所有产品中融入了Transformer技术。”</p><p>&nbsp;</p><p>而且这些产品革新对谷歌来说非常重要：“在技术领域，如果你不持续创新以保持领先，那么任何公司都将不可避免地走向衰败”。</p><p>&nbsp;</p><p>过去十年，谷歌一直自诩为“人工智能优先公司”。然而，随着 OpenAI 推出 ChatGPT 这一划时代的产物，并迅速席卷全球人工智能领域，谷歌的地位受到了前所未有的挑战。</p><p>&nbsp;</p><p>但皮查伊则认为谷歌不能被微软牵着鼻子走，需要有自己的方式，并且更重要的是，我们还处于人工智能的早期阶段：“我从长远的角度说，当互联网刚刚出现时，谷歌当时甚至不存在，对吧？所以我们不是第一家做搜索的公司，我们不是第一家做电子邮件的公司，我们不是第一家构建浏览器的公司。我们还有很长的路要走，我们正处于这场技术革命的初期阶段。”</p><p>&nbsp;</p><p>这次主题演讲，皮查伊诠释了谷歌如何用自己的方式顺应这次技术潮流发展。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://finance.sina.cn/usstock/mggd/2024-05-09/detail-inaurpkf9481060.d.html?oid=ph&amp;vt=4&amp;cid=76556&amp;node_id=76556">https://finance.sina.cn/usstock/mggd/2024-05-09/detail-inaurpkf9481060.d.html</a>"</p><p><a href="https://searchengineland.com/google-ceo-links-ai-making-search-quality-worse-440365">https://searchengineland.com/google-ceo-links-ai-making-search-quality-worse-440365</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RUqEHYEUXO46rW5gFQu9</id>
            <title>腾讯文生图大模型全面开源！首个中文原生DiT架构，支持中英双语理解生成</title>
            <link>https://www.infoq.cn/article/RUqEHYEUXO46rW5gFQu9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RUqEHYEUXO46rW5gFQu9</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 May 2024 11:19:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 
        关键词: 腾讯, 混元文生图, DiT架构, 开源
        <br>
        <br>
        总结: 腾讯宣布旗下的混元文生图大模型全面升级并对外开源，成为业内首个中文原生的DiT架构文生图开源模型，支持中英文双语输入及理解，参数量15亿。新一代的腾讯混元文生图模型效果远超开源的Stable Diffusion模型，提升超过20%，在多轮对话、多主体、中国元素、真实人像生成等场景下效果显著。腾讯混元文生图的开源将填补DiT架构在中文领域的空白，推动中文文生图技术研发和应用。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;华卫</p><p></p><p>5月14日，腾讯宣布旗下的混元文生图大模型全面升级并对外开源，目前已在&nbsp;Hugging&nbsp;Face&nbsp;平台及&nbsp;Github&nbsp;上发布，包含模型权重、推理代码、模型算法等完整模型，可供企业与个人开发者免费商用。</p><p>开源代码库链接：&nbsp;https://github.com/Tencent/HunyuanDiT</p><p>&nbsp;</p><p>“混元DiT开源的价值主要有两方面，一是作为中文原生DiT架构，弥补了开源社区的空白；二是混元DiT为全面开源，与现网版本完全一致。”腾讯混元文生图负责人卢清林表示。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d8/d8d058a2306d887df17e251db97fc102.jpeg" /></p><p></p><p>&nbsp;</p><p>据介绍，这是业内首个中文原生的DiT架构文生图开源模型，支持中英文双语输入及理解，参数量15亿。升级后的混元文生图大模型采用了与&nbsp;Sora&nbsp;一致的DiT架构，不仅可支持文生图，也可作为视频等多模态视觉生成的基础。其评测数据显示，新一代的腾讯混元文生图模型效果远超开源的&nbsp;Stable&nbsp;Diffusion&nbsp;模型。</p><p>&nbsp;</p><p>三大能力升级</p><p>效果比前代提升超20%</p><p>&nbsp;</p><p>最新的腾讯混元文生图大模型主要进行了算子、语言编码器、多轮绘图能力三方面的升级。</p><p>&nbsp;</p><p>首先是架构，该模型从U-Net&nbsp;架构升级至DiT架构（DiT，即Diffusion&nbsp;With&nbsp;Transformer），后者也是Sora和&nbsp;Stable&nbsp;Diffusion&nbsp;3&nbsp;的同款架构和关键技术。“为构建混元DiT，腾讯设计了Transformer结构、文本编码器和位置编码，构建了完整的数据管道，用于更新和评估数据。”卢清林表示。</p><p>&nbsp;</p><p>腾讯混元团队认为，基于&nbsp;Transformer&nbsp;架构的扩散模型&nbsp;（如&nbsp;DiT）具有更大的可扩展性，很可能成为下一代主流视觉生成架构：未来，DiT架构很可能会成为文生图、生视频、生3D等多模态视觉生成的统一架构。</p><p>&nbsp;</p><p>据介绍，从&nbsp;2023&nbsp;年&nbsp;7&nbsp;月起，腾讯混元文生图团队就明确了基于DiT架构的模型方向，并启动了新一代模型研发。今年初，混元文生图大模型已全面升级为DiT架构。</p><p>&nbsp;</p><p>其次是语音编码器方面，混元文生图大模型是中文原生的DiT模型，具备中英文双语理解及生成能力，在古诗词、俚语、传统建筑、中华美食等中国元素的生成上有良好表现，中文输入后直接中文理解，避免了因翻译产生的语义分歧。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/088f55abe829b65b84c14d0c6fb0071d.png" /></p><p></p><p>&nbsp;</p><p>目前Stable&nbsp;Diffusion&nbsp;等主流开源模型核心数据集以英文为主，对中国的语言、美食、文化、习俗都理解不够，在中文应用场景受限，很多团队还是基于翻译+英文开源Stable&nbsp;diffusion模型，导致在中文特有的场景、人物、事物上表现比较差。还有一些团队基于少量的中文数据在一些特殊的场景做了finetune，让模型去适配某个特殊的领域或者风格，但直接用英文预训练的模型+中文小数据finetune也存在对中文理解不足和不通用的问题。</p><p>&nbsp;</p><p>腾讯官方的评测结果显示，新一代腾讯混元文生图大模型视觉生成整体效果的相比前代提升超过&nbsp;20%，在语义理解、画面质感与真实性方面全面提升，在多轮对话、多主体、中国元素、真实人像生成等场景下效果提升显著。</p><p>&nbsp;</p><p>在DiT架构之上，腾讯混元团队还在算法层面优化了模型的长文本理解能力，能够支持最多&nbsp;256&nbsp;字符的内容输入，同时实现了多轮生图和对话能力，可实现在一张初始生成图片的基础上，通过自然语言描述进行调整，来达到更满意的效果。</p><p>&nbsp;</p><p>填补开源DiT架构空白</p><p>版本同步现网</p><p>&nbsp;</p><p>“我们认为，建设中文原生的文生图开源模型、中文的文生图开源生态十分必要。”据悉，腾讯开源的混元文生图模型Tencent-Hunyuan-Visual&nbsp;1.9，与实际生产环境中的最新版本完全一致，包括C端用户能体验到的微信小程序和Web版本、个人和企业开发者能体验到的云API版本，均可免费商用。</p><p>&nbsp;</p><p>此次混元文生图模型开源后，开发者及企业无需重头训练，即可直接将其用于推理，并可基于此打造专属的AI绘画应用及服务，能够节约大量人力及算力。透明公开的算法，也可以让该模型的安全性和可靠性得到保障。</p><p>&nbsp;</p><p>“目前开源社区中技术快速迭代，但缺乏先进、成熟的DiT架构可以开源利用。”卢清林表示，在目前DiT架构已经呈现出巨大潜力的情况下，开源社区是存在一定空白的。文生图大模型领域的开源开发者生态已经形成，但依然主要基于U-Net架构模型进行开发，仍未有比较先进的DiT架构充分开源。</p><p>&nbsp;</p><p>基于开放、前沿的混元文生图基础模型，有利于在以&nbsp;Stable&nbsp;Diffusion&nbsp;等为主的英文开源社区之外，丰富以中文为主的文生图开源生态，形成更多样的原生插件，推动中文文生图技术研发和应用。</p><p>&nbsp;</p><p>现在腾讯混元文生图能力，已经广泛被用于素材创作、商品合成、游戏出图等多项业务及场景中。今年初，腾讯广告基于腾讯混元大模型，发布了一站式&nbsp;AI&nbsp;广告创意平台腾讯广告妙思，可为广告主提供文生图、图生图、商品背景合成等多场景创意工具。</p><p>&nbsp;</p><p>腾讯文生图负责人芦清林表示：“腾讯混元文生图的研发思路就是实用，坚持从实践中来，到实践中去。此次把最新一代模型完整开源出来，是希望与行业共享腾讯在文生图领域的实践经验和研究成果，丰富中文文生图开源生态，共建下一代视觉生成开源生态”</p><p>&nbsp;</p><p>据了解，腾讯在开源上一直持开放态度，已开源了超&nbsp;170&nbsp;个优质项目，均来源于腾讯真实业务场景，覆盖微信、腾讯云、腾讯游戏、腾讯AI、腾讯安全等核心业务板块，目前在Github上已累计获得超&nbsp;47&nbsp;万开发者关注及点赞。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fT73OdCrLME6bec5FOUB</id>
            <title>“烧钱”的大模型如何为企业“降本增效”助力？腾讯的实践经验来了｜ArchSummit</title>
            <link>https://www.infoq.cn/article/fT73OdCrLME6bec5FOUB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fT73OdCrLME6bec5FOUB</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 May 2024 07:56:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 成本问题, 训练框架, 低代码平台
<br>
<br>
总结: 大模型的应用价值和成本问题备受关注，腾讯通过不断探索和优化训练框架，解决大模型训练的挑战。同时，结合低代码平台提高开发效率，降低用户门槛。在不断探索中，腾讯云还利用大模型构建智能助手和提升质检效率，持续提升服务水平。在建造AI智能化过程中，除了大模型，还需打牢基础技术基底，提升可观测性技术。 </div>
                        <hr>
                    
                    <p>大模型的价值潜能有目共睹，但“成本黑洞”也不失为一个事实。除了寻找最佳落地路径和业务场景之外，大模型的成本问题也一直备受关注。作为一个短板效应明显的系统工程，万亿级参数规模，背后不但涉及巨大的算力资源消耗，还有随之而来的存储、推理、运维、应用等一系列成本。</p><p></p><p>如何才能让“烧钱”的大模型物超所值，解决绝大多数企业当下最关心的“降本增效”问题？腾讯正在通过实践不断探索和寻求他们的答案。</p><p></p><p>在 6 月 14 日 -6 月 15 日即将于深圳举办的<a href="https://archsummit.infoq.cn/2024/shenzhen/"> ArchSummit 全球架构师峰会</a>"上，我们邀请到了来自腾讯多个不同条线的技术专家，从训练框架、开发、落地应用等多个维度分享现阶段企业如何利用大模型实现降本增效的目标。</p><p></p><p>拿训练框架来说，目前不仅要支持文生文、多模态、文生图、文生视频等大模型训练，还要支持 Dense 和 MoE 模型的训练；不仅要支持小模型的训练，还要支持万亿参数模型的训练；不仅要支持单任务单卡大模型的训练，还要支持单任务万卡规模大模型的训练；不仅要支持同构 GPU 的训练，还要支持异构 GPU 的加速训练，如何满足大模型训练的多种加速需求，成为大模型 AI Infra 的必须解决的挑战。</p><p></p><p>基于对存储、网络、计算的深度融合优化，腾讯研发了 AngelPTM 大模型训练框架，其通过 6D 并行策略提高模型的训练并行度、通过 ZeROCache 解决大模型训练显存压力大的问题，通过 MOE 加速组件解决超大规模参数模型高效训练的问题，通过与星脉高速网络的协同优化，与算力、服务器、存储等团队的通力配合来解决单任务万卡训练的问题。</p><p></p><p>据悉，通过 AngelPTM 支持文生文、多模态、文生图 / 视频等大模型的高速训练，单任务万卡训练可实现长时间的稳定高性能训练。</p><p></p><p>围绕这些话题，腾讯机器学习平台部大模型训练框架研发技术专家薛金宝将在 ArchSummit 深圳带来《<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5885">腾讯 AngelPTM 大模型训练框架优化与实践</a>"》的议题分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a820262af9d0b9824b019ee6ba69c5f5.webp" /></p><p></p><p>软件开发是大模型较早入局的落地场景之一，通过与低代码技术的结合，开发效率提升将迈入新的台阶。</p><p>具体而言，低代码平台旨在使用少量代码，高效的搭建页面。对非前端从业者友好，提供了开箱即用的无代码数据配置服务，和以 LowCode 进行了管理端研发体系升级。随着大模型的能力飞速提升，大模型的提效能力加上低代码的易用性相辅相成，将让低代码开发效率更高，更大程度降低用户的使用门槛。</p><p></p><p>在 ArchSummit 深圳，腾讯 PCG 前端技术专家苑宗鹤将分享《<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5895">AI 在低代码平台搭建中的运用和挑战</a>"》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c63ca2373d2b634af83a1939e2a15443.webp" /></p><p></p><p>行业探索方面，腾讯云利用 RAG 技术结合私域知识，基于腾讯云行业大模型构建了 AI 智能助手，对内提升服务效率的同时，还对外提升客户自助服务降低成本，在此过程中沉淀出企业智能知识库的解决方案。此外，基于过去多年沉淀服务数据，腾讯云通过大模型理解力，构建发现问题 - 量化分析 - 改进优化 - 线上验证的闭环，持续提升自身云产品的竞争力。</p><p></p><p>腾讯云安灯产品 &amp; 研发总监许小川将在 ArchSummit 深圳分享《<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5769">腾讯云安灯 AI 大模型应用实践和探索</a>"》。腾讯云安灯是一款服务于腾讯云内部、伙伴及客户的一站式 IT 服务管理平台。随着 LLM 技术迅猛发展，其在 AI 大模型应用上做了诸多实战，帮助腾讯云、伙伴及客户降本提效、提升服务水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a925f6b27b830e33174972898a327171.webp" /></p><p></p><p>除此之外，在工业质检场景，腾讯云还联合头部标机客户，结合其在所属场景的数据优势，提供基于公有云 / 私有化服务集群的质检大模型训练服务，并与端侧单机软件打通，实现在质检行业呼唤已久“0 样本、秒换型、快应用”的新范式突破。</p><p></p><p>工业 AI 质检，从能不能到快不快，到是否能更快。腾讯云采用的解决方案是一体化方案，标准平台建设，云 + 端协同。该方案已经在 3C/ 锂电 / 光伏等复杂质检项目落地，获评工信部最佳实践，IDC 市场排名第一。</p><p></p><p>对此，腾讯云高级产品专家王刚将在 ArchSummit 深圳带来《<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5872">大模型时代的工业质检方法论</a>"》的议题分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c0f7960a40bc5d06ebb0e120c8d1288.webp" /></p><p></p><p>当然，大模型不是企业降本增效的唯一手段，也不是眼下需要重点关注和跟进的唯一技术。在建造 AI 智能化这座“高楼”的过程中，基础的技术基底也必须打牢。</p><p></p><p>比如，如何持续提升可观测性技术中日志检索和分析等核心能力？据了解，腾讯云 CLS 利用统一资源池理念，消除了系统中各个层次的 IO 资源隔离，实现了成本降低 90% 的目标；同时在优先控制成本的前提下，通过消除全地域算力资源隔离，实现了大规模分析能力提升数十倍。</p><p></p><p>在 ArchSummit 深圳，腾讯云专家工程师林兆祥详细介绍“<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5814">降本九成，提效十倍</a>"”的目标究竟是如何达成的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61c3b06f679fae24911333c90f6aee1f.webp" /></p><p></p><p>与此同时，大模型的盛行也将重塑微服务架构。微服务架构的广泛应用，把大而复杂的业务系统拆分成高内聚的微服务，对整个系统实现解耦。每个服务负责相对独立的逻辑，但是要实现业务价值，需要协调所有服务保证端到端业务流的成功。</p><p></p><p>腾讯星星海实验室架构师叶彬将在 ArchSummit 深圳分享《<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5745">弹性可伸缩海量工作流引擎建设实践</a>"》，具体从业务场景出发（海量服务器全链路运营），并结合真实的业务痛点，阐述在落地过程中如何开创性实现了弹性可伸缩架构，使得该引擎具备千万级多层嵌套流程毫秒级调度、峰值十万 QPS、秒级容灾自愈的基础设施流程调度能力，有效支撑海量服务器全链路数亿级作业场景。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b81b33e1f1ce347bedac90d4392fa064.webp" /></p><p></p><p>除了腾讯的众多优秀讲师之外，我们也邀请了（以下排名不分先后）阿里巴巴、百度、网易、字节跳动 / 火山引擎等互联网技术大厂， vivo、知乎、高德地图、Uber 、蚂蚁集团、eBay、货拉拉、快手、哔哩哔哩、携程等头部互联网企业，以及 CNCF、Thoughtworks、顺丰集团、美的集团、鸿海科技集团（富士康母公司）、宁德核电、广发证券、微众银行、众安银行、天弘基金等众多机构和企业的专家共同探讨生成式 AI 技术对于企业未来架构的影响。</p><p></p><p>目前，ArchSummit 深圳大会议程已经上线，并将持续更新，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZPaq9UlPwCjvWwyJ01C8</id>
            <title>华人AI创业神话破灭？从最火的生成式AI产品到全网群嘲，只用了110天</title>
            <link>https://www.infoq.cn/article/ZPaq9UlPwCjvWwyJ01C8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZPaq9UlPwCjvWwyJ01C8</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 May 2024 02:39:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Cyber Manufacture, GAMA, Rabbit R1, AI硬件产品
<br>
<br>
总结: 2021年11月，Cyber Manufacture公司成功筹集600万美元，推出了下一代NFT项目GAMA，旨在送1万名宇航员进太空获取地外能源。2023年11月，GAMA开源空间站，引入新API，但项目销声匿迹。CEO Jesse Lyu是Rabbit公司联合创始人，推出了AI驱动的Rabbit R1设备。Rabbit R1是一款基于Android系统的AI设备，具备强大的语音控制功能，被誉为2024年最火的AI硬件产品。然而，Rabbit R1发布后被曝内部运行的并非全新AI操作系统，引发质疑和批评。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p>2021年11月，一家名叫Cyber Manufacture的公司为其“下一代NFT项目GAMA”筹集到了600万美元。根据GAMA网站2022年6月1日的归档内容来看，GAMA是一家“去中心化组织”，“希望将1万名宇航员送入太空以实现地外能源获取。”</p><p>&nbsp;</p><p>时间来到2023年11月13日，GAMA在其Discord频道上发布最后公告，向“GAMA机组成员”放话称已经正式开源GAMA空间站，并“为AI NPC引入新的API，由此开启一个充满可能性的世界”以供用户参与和交互。可时至今日，尽管GAMA的原始Twitter账户仍然存在，但用于存放大部分GAMA“Ask Me Anything”会话内容账户已经消失。</p><p>&nbsp;</p><p>没错，就像众多曾经搏人眼球、但如今遭到废弃的Web3项目一样，GAMA&nbsp;NFT项目也已经销声匿迹。</p><p>&nbsp;</p><p>但之所以说到GAMA，是因为该公司CEO Jesse Lyu是Rabbit公司的联合创始人——这家公司打造出了AI驱动的明星产品R1设备。</p><p>&nbsp;</p><p>很多朋友可能还不熟悉，Rabbit R1是一款基于Android系统的设备，号称允许用户通过语音指令控制自己的各类应用和订阅——目前可支持Uber、Doordash、Midjourney以及Spotify等。其最终目标是完全替代手机功能，并提供星际迷航式的人机自然语言交互。</p><p>&nbsp;</p><p></p><h2>2024年最火的AI硬件产品</h2><p></p><p>&nbsp;</p><p>今年1月9日，一家名为“Rabbit”的初创公司，带着一款手掌大小的AI智能设备亮相了国际消费电子展。</p><p>&nbsp;</p><p>据介绍，这款名为Rabbit R1的设备，内置了Large Action Model模型（LAM），用户可以通过语音方式与Rabbit R1进行对话交流，进而调用手机上的一切App。它还具备一个Rabbit公司开发的“全新的基于AI的操作系统”——RabbitOS。</p><p>&nbsp;</p><p>其创始人Jesse Lyu在社交媒体发帖称该操作系统跟iOS 和 Android这些平台“有着根本的不同”，“LAM 和RabbitOS 比当前基于应用程序的操作系统领先一代。”并且“RabbitOS 不需要应用程序。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c2ef63ae616977195139817aee66f642.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>根据前期宣传资料，该产品在技术能力上非同一般，该产品采用的LAM脱胎于大语言模型，但更强调行为，学习的是动作过程，它结合了神经网络的模式识别和逻辑推理，通过不断地学习和模仿用户的聚合演示，能理解人的复杂意图，并代表用户操作应用程序完成任务。</p><p>&nbsp;</p><p></p><blockquote>“LLM基于文本进行学习训练，而LAM则是直接基于应用程序的交互界面进行学习，这让LLM和LAM呈现出能力区别：LLM可以理解人的意图，而LAM可以真正操作实现意图。”</blockquote><p></p><p>&nbsp;</p><p>另外，当用户按下侧边按键，500毫秒就能唤醒对话系统，Jesse Lyu曾表示，“R1比市场上多数AI语音识别工具速度快10倍”。</p><p>&nbsp;</p><p>Rabbit公司描述了一个出色的AI愿景，利用AIGC的热潮推动这款小设备大卖，预售销量迅速突破10万台。还有外媒评价它是“2024年最激动人心的发布”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/250beda81ecb23be148a0bf5cbad088b.jpeg" /></p><p></p><p>&nbsp;</p><p>引爆CES后，Rabbit于4月底举办了Rabbit R1的发布会，首批买家终于收到实物。不幸的是，在他们拿到这些设备后，评论界的反响却是灾难性的。</p><p>&nbsp;</p><p>5月1日，科技博主 Mishaal Rahman 发文曝光Rabbit R1 内部运行的并不是什么“全新AI操作系统”，而是 Android 系统，其整个界面都由安卓应用提供支持。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eadd580b3a31c1e7079783733b95b734.jpeg" /></p><p></p><p>&nbsp;</p><p>还有不止一位网友表示他们能破解Rabbit R1，让其可以在标准手机上运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f13672b73f8d87a81e804768a75cbda.jpeg" /></p><p></p><p>&nbsp;</p><p>5月3日，全球知名拆解机构 iFixit 从里到外拆解了 Rabbit R1，他们认为“该设备上没有运行人工智能计算的内部结构”、这小玩意儿“确实没有必要被包装成硬件”，最后还用一句话总结了他们的感受：“不敢相信自己为这只兔子付了钱。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f46ec3c3489e8de51f434f08efe10dc.jpeg" /></p><p></p><p>&nbsp;</p><p>5月4日，Engadget也发布了评测文章，认为“R1似乎毫无用处”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/172251a402ab5224496691512598f162.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>从“崇拜”到全网“开黑”，不过短短数月，连带着Rabbit团队的背景也被扒得一干二净。</p><p>&nbsp;</p><p>一名从事技术工作的网友Emily发现，Rabbit的前身是一家名为Cyber Manufacture Co的公司，成立于2021年，主要项目是GAMA。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/75ab728ef12b8160577d2af44dc8a885.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/419a5660d2e28749171feb3504865df3.jpeg" /></p><p></p><p>&nbsp;</p><p>跟加密货币扯上关系不是好事儿，Rabbit想极力撇清与GAMA/Cyber Manufacture之间的关系，并一直在回避跟Web3、元宇宙或者NFT等话题扯上关系。</p><p>&nbsp;</p><p></p><blockquote>Jesse拥有丰富的创业经历，在自己的职业生涯中曾经参与过一系列项目，包括GAMA元宇宙/NFT项目。他之前曾经公开讨论过这个项目，但在创办Rabbit之前就已经放弃。他目前正致力于推进Rabbit，并着手建立一支强大且茁壮成长的团队，希望为公司不断增长的用户群体提供服务。</blockquote><p></p><p>&nbsp;</p><p>虽然发表了声明，但显然网络可以扒出的料太多，这些痕迹很容易证实“Rabbit和Cyber Manufacture其实就是同一家公司、甚至是同一团队”。</p><p>&nbsp;</p><p></p><h2>一场荒诞奇诡的创业之旅</h2><p></p><p>&nbsp;</p><p>根据Emily扒出的材料，这是一家曾经销售NFT并大肆宣扬AI驱动元宇宙平台的公司实体，在用户丧失信心的短短几个月后就筹集到几千万美元风险投资，转而销售一款AI驱动的小设备。</p><p>&nbsp;</p><p>2023年11月2日，也就是为Rabbit R1及其“Large Action Model”模型融资成功近一个月后，Jesse Lyu向加州政府秘书长提交了文件，要求将Cyber Manufacture更名为Rabbit。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76d7f3555276b0e4a8c957f476b5440f.jpeg" /></p><p></p><p>&nbsp;</p><p>Aaron Li 是原 GAMA 网站上列出的人员之一，也是 GAMA 背后的开发人员之一，<a href="https://twitter.com/polymorpher/status/1786079624205852973">他在 X 上证实</a>"，这也是同一个团队。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/313696146c1a7ad51f245492137d1247.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Rabbit曾于2023年10月4日发布一篇Twitter帖子，提到Cyber Manufacture打算“创造一种更加自然的人机交互方式”。这一天，Cyber Manufacture被更名为Rabbit，2000万美元的融资公告也于同天发布，但没有只言片语提到过GAMA。</p><p>&nbsp;</p><p>同时，GAMA的Twitter账户也已经被删除，Lyu本人似乎在淡化自己在经营公司中的角色。他曾经拿出几个小时做出多项详尽且冗长的承诺，包括开发大型多人在线角色扮演游戏、打造由AI驱动的元宇宙并出版漫画丛书等。而且根据俄勒冈州Blockchain Group于2022年第二季度公布的基金更新，他们还打算花费350万至370万美元用火箭向太空发射GAMA卫星。</p><p>&nbsp;</p><p>Lyu声称他“在GAMA项目中的主要工作”是“开发一款虚幻引擎游戏”并且成功交付。但有用户透露，GAMA的GSS元宇宙只是一款名为《Lyra》的虚幻引擎学习类作品的换皮产物，GSS在GitHub上的项目信息也证明了这一点。该项目中甚至包含“LyraEditor”和“LyraGame”等文件夹，属于演示虚幻引擎5的入门最佳实践项目。由于GAMA不提供任何服务器和服务选项，因此用户还得亲自托管GSS才能使用。</p><p>&nbsp;</p><p>Emily 在对这家公司进行细致研究后表示，Lyu已经用种种手段证明“Rabbit与任何加密货币/Web3没有任何关系（原文如此），而且永远不会扯上关系”。另有一位用户补充称，他投入了“大量资金来开发3D资产”，本以为这些资产终有一天会出现在GSS（GAMA的元宇宙）当中。</p><p>&nbsp;</p><p>这也让一些人好奇Rabbit R1产品是什么时候开始构思的，有没有GAMA融资被用于开发R1，Lyu和他的团队又分别在R1和GAMA上开发了多长时间。</p><p>&nbsp;</p><p>事实上，2023 年 11 月 11 日，Cyber​​ 宣布他们将开源整个 GAMA 游戏，当时他们将此作为 GAMA 旅程的下一步。但实际上，他们并没有打算让GAMA继续存在， 11 月 2 日，也就是开源公告发布前两周，Cyber​​ Manufacture 悄悄申请将自己更名为 Rabbit Inc。这正是我们所熟知的 Rabbit Inc。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b22a888aacc218d72abd4f5bcecd1a1.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>另外，还有另外一家关联公司RCT Studio也被网友们扒了出来。</p><p>&nbsp;</p><p>根据由Lyu签字确认的Form D文件，这家公司曾在2019年筹集到1000万美元。而从2020年《洛杉矶商业杂志》的一篇报道来看，Lyu被任命为该公司CEO。</p><p>&nbsp;</p><p>RCT Studio声称要“为游戏行业提供AI解决方案，并用AI生成内容构建起真正的元宇宙”。在融资之前其曾经是Y Combinator的孵化项目，并于2020年3月26日发布新闻稿，宣布Lyu将担任其CEO。该公司声称“开辟了几乎无限的叙事替代方案与故事架构，同时破解了单词含义并可将其转化为3D渲染动画”，且由“Morpheus引擎”驱动。在2021年总额1000万美元的融资公告中，RCT Studio（现为RCT AI）宣布任命Yuheng Chen（曾供职于Lyu之前的公司Raven Tech）为CEO。</p><p>&nbsp;</p><p>从目前的情况来看，Lyu似乎已经从RCT AI离职（他本人的LinkedIn没有提到这家公司），但并不清楚具体离职时间。Lyu在2019年4月17日发表的文章中解释了RCT如何“利用前沿AI来构建交互式与沉浸式电影体验。”有趣的是，Lyu被描述为该公司的创始人，并在文章结尾询问文章作者是否看过电影《头号玩家》，宣称“这就是RCT目前的构建方向。”这不禁让人想到Lyu在2021年12月1日的Clubhouse GAMA会议上推出“下一阶段玩家元宇宙体验”的说辞。</p><p>&nbsp;</p><p>自2021年4月起担任RCT AI公司CEO的Yuheng Chen在其LinkedIn上表示，RCT AI正在“利用区块链上的AI构建元宇宙”，提出的主张与GAMA非常相似。RCT AI随后于2022年底为一款名为《Delysium》的“AI驱动3A级Web3游戏”新作筹集1000万美元，并宣称该游戏是在公司内部孵化而成。</p><p>&nbsp;</p><p>相信大家都已经看出了Rabbit一路走来的复杂历程和与过往的种种关联。Rabbit R1团队背后有着极其复杂的创业史，而且多次利用Web3、元宇宙和AI等流行语募集资金。</p><p>&nbsp;</p><p>据网友统计，自2019年以来，Lyu已经顶着三个名号为两家企业筹集到4600万美元资金——其中Cyber Manufacture为600万美元，RCT为1000万美元，Rabbit是3000万美元。但从媒体采访内容来看，Lyu讲述的却是另外一个截然不同的故事。</p><p>&nbsp;</p><p>在将自己担任CEO的Raven Tech公司出售给百度之后（当时他还使用本名Lu Cheng），Lyu于2019年搬往湾区，之后在某个不明确的时间点上接到了OpenAI公司CEO&nbsp;Sam Altman的电话。2020年，可能是二人合作关系的末期，据称Altman向Lyu展示了GPT-3的早期版本。而引用知情人士的说法，“Raven由此变成了Rabbit。”</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/CyberAntani/status/1783493299820519448">https://twitter.com/CyberAntani/status/1783493299820519448</a>"</p><p><a href="https://sea.mashable.com/tech/32385/rabbit-r1-humane-ai-pin-guts-exposed-in-new-teardown-video">https://sea.mashable.com/tech/32385/rabbit-r1-humane-ai-pin-guts-exposed-in-new-teardown-video</a>"</p><p><a href="https://www.engadget.com/rabbit-r1-review-a-199-ai-toy-that-fails-at-almost-everything-161043050.html">https://www.engadget.com/rabbit-r1-review-a-199-ai-toy-that-fails-at-almost-everything-161043050.html</a>"</p><p><a href="https://www.wheresyoured.at/rabbit-holed/">https://www.wheresyoured.at/rabbit-holed/</a>"</p><p><a href="https://twitter.com/EmilyLShepherd">https://twitter.com/EmilyLShepherd</a>"</p><p><a href="https://mp.weixin.qq.com/s/p1siK6rcxj4g-L6RP5zTCQ">https://mp.weixin.qq.com/s/p1siK6rcxj4g-L6RP5zTCQ</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/42ROdXw5VHrfFMsITd07</id>
            <title>OpenAI 官宣旗舰模型 GPT-4o，完全免费、无障碍与人交谈！奥特曼：这是我们最好的模型</title>
            <link>https://www.infoq.cn/article/42ROdXw5VHrfFMsITd07</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/42ROdXw5VHrfFMsITd07</guid>
            <pubDate></pubDate>
            <updated>Mon, 13 May 2024 18:35:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, GPT-4o, ChatGPT, 人工智能模型
<br>
<br>
总结: OpenAI 宣布推出全新的人工智能模型 GPT-4o，该模型在文本、视觉和音频方面有着显著的改进，可以进行语音、文本和视觉推理，具有先进的音频理解能力。这款模型可以像人类一样与用户交谈，并且具有解方程式等功能，开放给所有人使用。 </div>
                        <hr>
                    
                    <p>上周，关于 OpenAI 即将发布重大更新的报道层出不穷。有报道称，ChatGPT 制造商 OpenAI 计划通过推出 Google 搜索的竞争对手来增强聊天机器人的功能并开拓新市场。报道还称，这款新搜索产品可能会在 5 月 13 日 Google I/O 大会前一天发布。不过 Altman 否认了此类传言。</p><p></p><p>甚至还顺势在 X 上的一篇帖子中写道，“不是 GPT-5，也不是搜索引擎，但我们一直在努力开发一些我们认为人们会喜欢的新东西！对我来说就像魔法一样。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ab1b458281db51bf7647a7be35b63d4.webp" /></p><p></p><p>就在刚刚，OpenAI 官宣了 Altman 口中的“就像魔法一样”的东西。</p><p></p><h2>OpenAI 官宣旗舰款模型 GPT-4o，完全免费</h2><p></p><p>在发布会刚开始，OpenAI 就发布了一款名为 GPT-4o 的新旗舰生成式人工智能模型，该模型将在未来几周内在公司的产品中“迭代”推出。</p><p></p><p>OpenAI 首席技术官 Muri Murati 表示，GPT-4o 提供了“GPT-4 级别”的智能，但改进了 GPT-4 在文本、视觉以及音频方面的能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fbe884c9662ba3f819c30caf0a2226df.webp" /></p><p></p><p>“GPT-4o 通过语音、文本和视觉进行推理，”Murati 在 OpenAI 办公室的主题演讲中说道。为了让其更加智能，OpenAI 团队在语音模式背后添加了新技术，人们可以用麦克风与 ChatGPT 交谈。</p><p></p><p>OpenAI 之前的领先模型 GPT-4 接受了图像和文本组合的训练，可以分析图像和文本以完成从图像中提取文本甚至描述这些图像内容等任务。</p><p></p><p>GPT-4o 不仅可以将语音转换为文本，还可以理解和标记音频的其他功能，例如呼吸和情感。此外，GPT-4o 具有先进的音频理解能力，并且可以控制其声音（听起来像机器人、声音兴奋、舒缓等）。</p><p></p><p>虽然这背后的更多技术细节没有公布出来，但 OpenAI 表示，现在 GPT-4o 在 50 种语言中的速度更快，也许使用的技术与他们在 GPT-4 上加速日语的技术相同。借助 GPT-4o/ChatGPT 桌面应用程序，用户可以有个编程伙伴一起交谈，并看到您所看到的内容。</p><p></p><p>此外，OpenAI 正在发布 ChatGPT 的桌面版本和更新的 UI。</p><p></p><p>OpenAI 研究员 William Fedus 表示，“GPT-4o 是我们最先进的新前沿模型。我们一直在 LMSys arena 上测试一个版本 im-also-a-good-gpt2-chatbot。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2591faf25bfbd11a0aee5c49bb67cdb6.webp" /></p><p></p><p>“这不仅是世界上最好的模型，而且可以在 ChatGPT 中免费使用，这对于前沿模型来说是前所未有的。” Fedus 补充道，“我们发现在更难的提示集上——特别是编码——存在更大的差距：GPT-4o 比我们之前的最佳模型实现了 +100 ELO。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/5135726bf32b66c2dbaec2808190b028.webp" /></p><p></p><p>奥特曼在推特里也表示，“GPT-4o 是我们最好的模型。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78e82d28e4977e4f4a0a698341ae1e87.webp" /></p><p></p><p>另外，在 API 中，GPT-4o 的价格是 GPT-4-turbo 的一半，速度是 GPT-4-turbo 的两倍、5 倍速率限制。</p><p></p><p>通常，当 OpenAI 宣布其 ChatGPT 模型的新版本时，都会对特定付费用户开放。然而，此次是个例外，该公司已决定允许所有人使用这项新技术。</p><p></p><h2>GPT-4o 可以像人类一样与你交谈，还能解方程式</h2><p></p><p>一直以来，OpenAI 希望与 ChatGPT 交谈就像与真人交谈一样，但遗憾的是之前 ChatGPT 的反馈总是有些延迟，这就破坏了交谈的沉浸感。现在，该公司正在 GPT-4o 背后添加新技术，以使与聊天机器人的对话速度更快。</p><p></p><p>为了展示这一点，OpenAI 使用语音与 GPT-4o 进行了对话演示。GPT-4o 不仅在演示者结束讲话后几乎立即做出响应，而且还通过文本转语音进行响应，让您感觉就像在与某人实时交谈。在演示过程中，GPT-4o 指导演示者 Mark Chen 如何更好地呼吸；包括采集他的呼吸音频样本，并为他提供如何做得更好的建议。</p><p></p><p>另一位演示者展示了 GPT-4o 在提示“机器人和爱”的情况下讲睡前故事。故事进行到一半时，OpenAI 开发人员 Mark Chen 介入并要求 GPT-4o 调整它说话时的情绪。果然，GPT-4o 可以根据要求改变声音，从过于戏剧化的表演到冷漠、机械的语气。最后，他们展示了 GPT-4o 的一些歌唱能力来完善这个故事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0c47f2aa0e6b714c563f7ff516dcbb4.webp" /></p><p></p><p>此外，此次发布会上演示者们还展示了 GPT-4o 在数学方面的“才能”。演示者写出了一个方程式并通过手机摄像头展示了 GPT-4o。它被指示帮助解决问题，但不泄露答案。果然，GPT-4o 指导演示者完成了求解简单方程的过程，几乎扮演了教师的角色。另外，它甚至还回答了典型的“我什么时候才能在现实生活中使用它？”问题，解释二次方程如何帮助我们完成日常任务。</p><p></p><p>演示者还使用桌面版 GPT-4o 来检查他们拥有的一些代码。GPT-4o 不仅可以解释代码的作用，还可以告诉您如果调整代码的特定部分会发生什么。</p><p></p><h2>此前猜测全部落空</h2><p></p><p>AIGC 赛道过去一年“卷疯了”似乎成为了业界共识，众多公司推出了自己的 AI 聊天机器人，谷歌的 Gemini、Anthropic 的 Claude 和 X 的 GrokAI 等竞争对手都在从 OpenAI 这里抢走更多关注。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/386bbf96462b711457f323259ed61f7d.webp" /></p><p></p><p>这次发布会之前，网上对 OpenAI 的发布内容充满猜测：Abacus.AI CEO 猜测，新的 Siri 将来自 OpenAI，更具体地，有网友表示是 ChatGPT iOS 中的对话模式；英伟达高级人工智能研究科学家 Jim Fan 表示，“预计 OpenAI 明天将演示实时语音助手。”；有网友说是“Google 级别的抓取和每日模型更新”。</p><p></p><p>还有网友 Ananay 表示“OpenAI 似乎正在致力于在 ChatGPT 内进行电话通话，或者至少提供某种程度的实时通信，而不仅仅是文本。这可能只是周一宣布的活动的一小部分。”他甚至表示，“OpenAI 现在已经部署了 webRTC 服务器来实现这一点，并且最近配置了这些服务器。”</p><p></p><p>这是一个开源项目，用于在应用程序内提供实时通信 - 例如语音和视频会议。这可能是 ChatGPT 代理行为的一部分。有了这个，你就可以向人工智能发出指令，让它启动并代表你执行操作——给予它呼叫访问权限可以让它打电话预约或处理来电，而无需你参与。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f1b19a2ab13422ae3183921eaf17c27.webp" /></p><p></p><p></p><h2>Altman：每年烧掉 500 亿美元我都不在乎</h2><p></p><p>值得注意的是，Sam Altman 最近在接受媒体采访时表示，他将不惜一切代价致力于构建通用人工智能 (AGI)。在与斯坦福大学的学生互动时，Altman 表示，开发 AGI 的任何成本都是合理的。</p><p></p><p>据《财富》杂志报道，他表示：“OpenAI 可能有比我更有商业头脑的人担心我们的支出，但我并不这么认为。”</p><p></p><p>“无论我们每年烧掉 5 亿美元、50 亿美元还是 500 亿美元，我都不在乎，我真的不在乎，只要我们能保持在一条轨道上，我认为最终我们会为社会创造比这更多的价值，只要我们能找到一种方法来支付账单，就像我们制造通用人工智能一样，这将是昂贵的，但完全值得，”他补充道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GKe33jIbv4eEbI3ZpJW0</id>
            <title>首家！数势科技完成中国信通院数据指标管理平台技术要求专项测试</title>
            <link>https://www.infoq.cn/article/GKe33jIbv4eEbI3ZpJW0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GKe33jIbv4eEbI3ZpJW0</guid>
            <pubDate></pubDate>
            <updated>Mon, 13 May 2024 08:24:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据指标管理平台技术要求, 数势科技, 智能指标平台, 高性能
<br>
<br>
总结: 中国信通院组织了数据指标管理平台技术要求专项测试，数势科技成为首家完成测试的企业。数势科技的智能指标平台产品具有高性能、自动化和智能化等优势，能够帮助企业实现数据普惠化，释放数据价值，推动数字化转型。 </div>
                        <hr>
                    
                    <p>2024年5月10日，在中国信息通信研究院（以下简称“中国信通院”）组织的首批数据指标管理平台技术要求专项测试中，北京数势云创科技有限公司（以下简称“数势科技”）顺利完成了数据指标管理平台技术要求专项测试的全部内容（包括47个必选能力项及12个可选能力项，10个可选能力项不涉及），成为首家完成此项测试的企业。</p><p></p><p></p><h2>《数据指标管理平台技术要求》标准及测试介绍</h2><p></p><p></p><p>为近一步规范数据指标管理平台的标准化发展，围绕指标全生命周期管理各环节的能力建设，中国信通院云计算与大数据研究所依托中国通信标准化协会大数据技术标准推进委员会（CCSA TC601），联合50余家单位100余位专家共同研讨编制了《数据指标管理平台技术要求》标准，包括指标构建、指标开发、指标运维、指标运营、指标应用、平台基础能力共六大能力域，16个一级能力项、69个二级能力项（含22个可选能力项），中国信通院依托该标准正式启动首批数据指标管理平台专项测试工作，旨在为供给侧研发和应用侧选型提供参考。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0ac01b3c73ceb8f385f3de79ab7db3c4.webp" /></p><p></p><p></p><h2>数势科技智能指标平台产品介绍</h2><p></p><p></p><p>数势科技指标平台是企业指标定义、加工、管理和应用的一体化工具。用户可以通过平台自行创建新的指标，实现数据的自助取数、加工和管理，并通过指标看板进行有效监控。此外，平台的智能预警和归因分析功能，能够帮助企业快速定位并解决数据异常问题，确保战略目标与业务执行的紧密衔接。本次完成测评的是3.0版本智能指标平台（SwiftMetrics），基于可信赖的结构化指标，结合了大模型自然语言交互、任务规划及数据解读能力，能够更加灵活高效地支持企业科学管理和经营分析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/160e5c79632c872c71ce052b1b648753.webp" /></p><p></p><p></p><h2>数势科技智能指标平台产品优势</h2><p></p><p></p><p></p><h4>1.高性能</h4><p></p><p></p><p>数势科技智能指标平台SwiftMetrics基于全球领先的高性能MPP数据库技术构建，确保了与传统架构软件相比10倍+的性能提升。更重要的是，数势科技自研的指标加速引擎是数据虚拟化理念指导下的指标计算引擎，将指标定义与物理数据解耦，支持更灵活的指标加工和分析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b7967096b302b18a34e115e77c13c9e.webp" /></p><p></p><p></p><h4>2.自动化</h4><p></p><p></p><p>在自动化方面，指标平台SwiftMetrics通过实现数据集成、指标血缘、版本管理以及指标预计算的全面自动化，极大地提升了操作效率和数据处理能力。首先，自动化的数据集成，让技术或研发团队一键式集成所有前端数据；其次，自动化生成指标血缘，在指标定义的同时，立即生成指标血缘；再次，自动化口径变更回写，可以快速实现口径变更自动同步，并且支持一键回滚至前一版本，匹配灵活的业务需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cf5aa40c0dccd5268b7d09ed5261068.webp" /></p><p></p><p></p><h4>3.智能化</h4><p></p><p></p><p>去掉数据集，让不懂技术的业务人员用可信好理解的指标直接取数和做报表。同时，数势科技智能指标平台为智能数据分析提供业务语义层，帮助企业实现NL to Metrics to SQL，一方面解决大模型对底层业务语义难理解和幻觉的问题；另一方面作为分析基座，解决企业各部门数据口径统一的问题，将传统的经验决策升级为以数据为核心的智能决策，进一步降低数据使用门槛，实现“人人都可做数据分析”，重新定义企业数据分析的未来。</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/58a377dbcc856e639dc6cb0a3ab960e8.webp" /></p><p></p><p></p><h2>数势科技智能指标平台核心功能场景</h2><p></p><p></p><p></p><h4>1.企业目标管理与拆解</h4><p></p><p></p><p>基于统一的指标体系，保证业务团队目标的共识和口径对齐，战略指标层层拆解到运营过程指标，实现战略目标到业务执行的闭环。</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/1317b6f848322ef27bf5b158ad5b2cf5.webp" /></p><p></p><p></p><h4>2.智能预警归因</h4><p></p><p></p><p>自动预警、发现和定位数据异常问题，并基于行业知识进行归因。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bf36373ce57b9b1c7b2aee9a7a6c3ed.webp" /></p><p></p><p></p><h4>3.支持以自然语言交互完成取数、用数</h4><p></p><p></p><p>结合大模型能力，支持业务同学对话式进行指标取数和数据分析，提供更好的智能化交互体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/afecaab928bb008e871f5f05a2811041.webp" /></p><p></p><p></p><h2>未来展望</h2><p></p><p></p><p>智能指标平台SwiftMetrics 3.0是数势科技帮助企业实现“数据普惠化”的利器，一方面能够解决数据脏乱、口径不统一的问题，让企业有数可用；另一方面，也能够降低数据消费的门槛，用“拖拉拽”或“自然语言交互”的形式让公司全员把数据用起来，从而释放数据价值，增收提效。未来，数势科技也将保持初心，以大数据+AI为核心，帮助企业构建数据资产层，加快数据要素赋能一线员工，深入挖掘数据价值，畅通数据资产价值释放管道，推动业务全面的数字化转型，打造业务增长新引擎。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>