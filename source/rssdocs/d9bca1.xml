<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE</id>
            <title>Generative AI 新世界：过去、现在和未来</title>
            <link>https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B0JNSlJPFxY5eebdIOZE</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 06:34:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器, 分析人工智能, 生成式人工智能, Transformer
<br>
<br>
总结: 机器不仅可以在分析任务上超越人类，还开始在创造性领域尝试超越人类，这一新领域被称为生成式人工智能。生成式人工智能的发展将重塑各行各业，可能取代某些人类创作工作，或激发出超越人类想象力的全新灵感。Transformer模型是生成式人工智能的重要知识底座，它提高了并行计算效率，引入了注意力机制，使人工智能能够理解单词之间的关系。注意力机制是一种模仿认知注意力的技术，可以增强神经网络对数据中重要部分的关注。 </div>
                        <hr>
                    
                    <p>人类善于分析事物。但是现在看来，机器很有可能做得更好。机器可以不知疲倦夜以继日地分析数据，不断从中找到很多人类场景用例的模式：信用卡欺诈预警、垃圾邮件检测，股票价格预测、以及个性化地推荐商品和视频等等。他们在这些任务上变得越来越聪明了。这被称为 “分析人工智能（Analytical AI）” 或”传统人工智能（Traditional AI）”。</p><p></p><p>但是人类不仅擅长分析事物，还善于创造。我们写诗、设计产品、制作游戏和编写代码。直到公元 2022 年之前，机器还没有机会在创造性工作中与人类竞争，它们只能从事分析和死记硬背的认知劳动。但是现在（是的，就是现在）机器已经开始在创造感性而美好事物的领域尝试超越人类，这个新类别被称为 “生成式人工智能（Generative AI）”。这意味着机器已经开始在创造生成全新的事物，而不是分析已经存在的旧事物。</p><p></p><p>生成式人工智能不仅会变得更快、更便宜，而且在某些情况下比人类手工创造的更好。每个需要人类创作原创作品的行业—从社交媒体到游戏、从广告到建筑、从编码到平面设计、从产品设计到法律、从营销到销售都有待全新重塑。某些功能可能会被生成式人工智能完全取代，或者激发出超越人类想象力的全新灵感。</p><p></p><p></p><blockquote><a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2F%3Fsc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DInfoQ">亚马逊云科技开发者社区</a>"为开发者们提供全球的开发技术资源。这里有技术文档、开发案例、技术专栏、培训视频、活动与竞赛等。帮助中国开发者对接世界最前沿技术，观点，和项目，并将中国优秀开发者或技术推荐给全球云社区。如果你还没有关注/收藏，看到这里请一定不要匆匆划过，<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fuser%2Fregister%3Fshow%3Dtab1%26from%3Dindex%26sc_medium%3Dregulartraffic%26sc_campaign%3Dcrossplatform%26sc_channel%3DInfoQ">点这里</a>"让它成为你的技术宝库！</blockquote><p></p><p></p><p>新世界正在到来。</p><p></p><h1>Transformer 新世界</h1><p></p><p></p><p>做为一名曾经多次穿越过市场周期的从业者，我亲历过通信行业、IT 行业、移动互联网行业等不同时代的周期，亲身体验过其间的潮起云涌，亲眼目睹过其中的天高云淡，以及最终惨烈竞争后的回归平淡。因此，面对已经开启的 AI 时代周期，与其盲目地跳进去跟随，不如先搞清楚这个新周期的一些底层逻辑，比如说：知识底座。</p><p></p><p>如果说 TCP/IP、HTML 等知识结构是上一个时代的知识底座，那么面对已经开始的 AI 时代，我们每个人是否应该先问自己一个问题：“什么是 AI 时代的知识底座？”</p><p></p><p>从到目前为止 AI 的知识发展看来，也许这个知识底座会是：Transformer。</p><p></p><h2>1 Transformer 概述</h2><p></p><p></p><p>欢迎进入 Transformer 的新世界。</p><p></p><p>在过去的五年中，人工智能世界发生了很多令人欣喜的重大变化。其中许多变化是由一篇名为 “Attention is All You Need” 的论文推动的。这篇发表于 2017 年的论文介绍了一种名为 “Transformer” 的新架构。下图为“Attention is All You Need” 的论文中描述的 Transformer 模型的架构图示。</p><p><img src="https://static001.geekbang.org/infoq/64/646dd7f43d6f56a389c3d3d4011d39cd.png" /></p><p>Source:&nbsp;<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf%3Ftrk%3Dcndc-detail">https://arxiv.org/pdf/1706.03762.pdf?trk=cndc-detail</a>"</p><p></p><p>概括来说，Transformer 模型为机器学习领域做出了两项贡献。首先，它提高了在人工智能中使用并行计算的效率。其次，它引入了 “注意力（Attention）” 的概念，这使人工智能能够理解单词之间的关系。你所听到的技术，例如 GPT-3、BERT、Sable Diffusion 等，都是 Transformer 架构在不同领域演进的结果。</p><p></p><h2>2 注意力机制（Attention）</h2><p></p><p></p><p>什么是注意力机制？根据该论文中的描述，注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出是按值的加权总和计算的，其中分配给每个值的权重由查询的兼容性函数与相应键值计算得出。Transformer 使用多头注意力（multi-headed attention），这是对称为缩放点积注意力（scaled dot-product attention）的特定注意力函数的并行计算。如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2e65bdfe28920f08cf204d601fdcced.png" /></p><p>&nbsp;Source:&nbsp;<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf%3Ftrk%3Dcndc-detail">https://arxiv.org/pdf/1706.03762.pdf?trk=cndc-detail</a>"</p><p></p><p>上面这段对“注意力机制”的描述还是偏学术化。维基百科上的定义会更通俗易懂些：“注意力机制（英语：attention）是人工神经网络中一种模仿认知注意力的技术。这种机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重，以此将网络的关注点聚焦于数据中最重要的一小部分。数据中哪些部分比其他部分更重要取决于上下文。可以通过梯度下降法对注意力机制进行训练 ……”</p><p>可见，注意力机制的灵活性来自于它的“软权重”特性，即这种权重是可以在运行时改变的，而非像通常的权重一样必须在运行时保持固定。</p><p></p><h2>3 Transformer in Chip</h2><p></p><p></p><p>很多人工智能领域的思想领袖和专家，认为 Transformer 架构在未来五年左右并不会有太大变化。这就是为什么你会看到一些芯片制造商在其新芯片（例如 NVIDIA H100）中集成 Transformer Engine 的原因。</p><p></p><p>在 2022 年拉斯维加斯的 re:Invent 2022 中，来自 NVIDIA 的架构师分享了如何在亚马逊云科技上，使用 NVIDIA 新一代芯片做深度学习训练的专题，里面特别提到了 H100 芯片中 Transformer Engine 的设计结构和初衷。对技术架构细节感兴趣的同学，可以通过以下视频深入了解：</p><p></p><p><a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dl8AFfaCkp0E%3Ftrk%3Dcndc-detail">https://www.youtube.com/watch?v=l8AFfaCkp0E?trk=cndc-detail</a>"</p><p><img src="https://static001.geekbang.org/infoq/c4/c4c3c7c5c1c7c0e53154b6ff5698088b.png" /></p><p></p><p></p><h2>4 Transformer 演进时间线</h2><p></p><p></p><p>一个有趣的视角是将各种 Transformer 按照出现的时间顺序排列的图示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e9cdf6f817052122267bd8a610257a40.png" /></p><p>Source: “Transformer models: an introduction and catalog”&nbsp;<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F2302.07730%3Ftrk%3Dcndc-detail">https://arxiv.org/abs/2302.07730?trk=cndc-detail</a>"</p><p></p><p>我听到过的一个比较有趣的视角是：如果您之前对 Transformer 知道得不多，不要恐慌。因为您看到引领这一波生成式人工智能（Generative AI）变革的重要几篇论文的情况：</p><p></p><p>CLIP 论文在 2021 年发表；Stable Diffusion 和 DALL-E-2 在 2022 年才出现；GPT3.5、ChatGPT、Bloom 等在 2022 年底才出现……</p><p>这个新世界的演进才刚刚开始，你还有足够的时间重新开始学习 Transformer！</p><p></p><h1>Generative AI</h1><p></p><p></p><h2>1 为什么现在发生?</h2><p></p><p></p><p>Generative AI 与更广泛的人工智能具有相同的值得人类深入思考问题：“为什么现在发生？” 概括来说，这个答案是我们当下具有：</p><p>更好的模型；更多的数据；更多的计算；</p><p></p><p>Generative AI 的进化速度比我们所能想象的要快得多，为了将当前时刻置于大时代洪流的背景之下，非常值得我们大致地了解下 AI 的发展历史和曾经走过的路。</p><p></p><p>第一波浪潮：小型模型占据了至高无上的地位（2015 年之前）</p><p></p><p>小型模型在理解语言方面被认为是 “最先进的”。这些小型模型擅长分析任务，可用于从交货时间预测到欺诈分类等工作。但是，对于一般用途的生成任务，它们的表现力还不够。生成人类级写作或代码仍然是白日梦。</p><p></p><p>第二波浪潮：规模竞赛（2015 年至今）</p><p></p><p>2017 年发表的里程碑意义的论文（“Attention is All You Need”）描述了一种用于自然语言理解的新神经网络架构，这种架构名为 Transformer，它可以生成高质量的语言模型，同时更具可并行性，并且需要更少的训练时间。这些模型是 few-shot learners 的，因此可以相对容易地针对特定领域进行定制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e13976c5aa69f65f9b401d3a9156e8b0.png" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.science.org%2Fcontent%2Farticle%2Fcomputers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help%3Ftrk%3Dcndc-detail">https://www.science.org/content/article/computers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help?trk=cndc-detail</a>"</p><p></p><p>随着模型越来越大，它们开始提供人类层面的结果，然后是超人的结果。在 2015 - 2020 年间，用于训练这些模型的计算增加了 6 个数量级，其结果超过了人类在手写、语音和图像识别、阅读理解和语言理解方面的性能基准。GPT-3 模型在这时脱颖而出，该模型的性能比 GPT-2 有了巨大的飞跃，内容涉及从代码生成到写作等多项任务。</p><p>尽管基础研究取得了种种进展，但这些模型并不被人广泛使用。原因是它们庞大且难以运行（需要 GPU 编排等），能够使用这些模型的门槛太高（不可用或仅限封闭 BETA），而且用作云服务的成本也很高。尽管存在这些局限性，但最早的 Generative AI 应用程序开始进入竞争阶段。</p><p></p><p>第三波浪潮：更好、更快、更便宜（2022 年以后）</p><p></p><p>由于像亚马逊云科技这样的云技术公司，一直在推动云计算的普及，计算变得更加便宜。而像 diffusion model 等新技术降低了训练和运行推理所需的成本，研究界因此可以继续开发更好的算法和更大的模型。开发者访问权限从封闭 BETA 扩展到开放 BETA，或者在某些情况下扩展到开源（open-source）。对于一直缺乏 LLM 访问权限的开发人员来说，现在闸门已开放，可供探索和应用程序开发。应用程序开始蓬勃发展。</p><p></p><p>第四波浪潮：杀手级应用程序的出现（现在）</p><p></p><p>随着基础平台层的逐渐巩固，模型不断变得更好/更快/更便宜，模型访问趋向于免费和开源，应用层的创造力爆炸的时机已经成熟。</p><p></p><p>正如十年前的移动互联网爆发的前夜，由于移动通过 GPS、摄像头和移动连接等新场景、新功能释放了新类型的应用程序一样，我们预计这些大型模型将激发新一轮的 Generative AI 应用。我们预计 Generative AI 也将出现杀手级应用程序。</p><p></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>"</p><p></p><h2>2 Generative AI: 应用层蓝图构想</h2><p></p><p></p><p>以下是 Generative AI 的应用格局图，描述了为每个类别提供支持的平台层以及将在上面构建的潜在应用程序类型。</p><p><img src="https://static001.geekbang.org/infoq/3d/3d38e1857cdb3133cc7dc57410d810e1.png" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>"</p><p></p><p>文本是进展最快的领域。</p><p></p><p>代码生成可能会在短期内对开发人员的生产力产生重大影响，如 <a href="https://www.infoq.cn/video/4oajrgIyfmkaaNFi7dJF?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Amazon CodeWhisperer</a>" 所示。</p><p>图像是一种较新的现象。我们看到了不同风格的图像模型的出现，以及用于编辑和修改生成的图像的不同技术。</p><p></p><p>语音合成已经存在了一段时间（例如，你好 Siri！）。就像图像一样，今天的模型也为进一步完善提供了起点。</p><p></p><p>视频和三维模型正在迅速上线。人们对这些模式开启电影、游戏、虚拟现实和实体产品设计等大型创意市场的潜力感到兴奋。</p><p></p><p>其他领域：从音频和音乐到生物学和化学，许多领域都在进行基础模型研发。</p><p></p><p>下图说明了我们如何期望基本模型取得进展以及相关应用成为可能的时间表。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f88ee1a43e3da7420420b13f7d3d63f5.png" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fwww.sequoiacap.com%2Farticle%2Fgenerative-ai-a-creative-new-world%2F%3Ftrk%3Dcndc-detail">https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/?trk=cndc-detail</a>"</p><p></p><h2>3 Generative AI: 文字生成图像（Text-to-Image）方向</h2><p></p><p></p><p>回顾过去的一年，有两个 AIGC 方向已经发生了让人惊艳的进步。其中一个方向就是：文字生成图像（Text-to-Image）方向。</p><p></p><p>根据来自亚马逊云科技的官方博客，用户现在可以很方便的在 SageMaker JumpStart 中使用 Stable Diffusion 模型，轻松地生成富有想象力的绘画作品。</p><p></p><p>The following images are in response to the inputs “a photo of an astronaut riding a horse on mars,” “a painting of new york city in impressionist style,” and “dog in a suit.”</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53dfcc043d338fb96f5d4b625e7df2ef.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1" /></p><p></p><p>The following images are in response to the inputs: (i) dogs playing poker, (ii) A colorful photo of a castle in the middle of a forest with trees, and (iii) A colorful photo of a castle in the middle of a forest with trees. Negative prompt: Yellow color.</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04f7213f08a541711f019115c0cf0e60.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1" /></p><p>Source:<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fblogs%2Fmachine-learning%2Fgenerate-images-from-text-with-the-stable-diffusion-model-on-amazon-sagemaker-jumpstart%2F%3Ftrk%3Dcndc-detail">https://aws.amazon.com/cn/blogs/machine-learning/generate-images-from-text-with-the-stable-diffusion-model-on-amazon-sagemaker-jumpstart/?trk=cndc-detail</a>"</p><p></p><p>关于文字生成图像（Text-to-Image）方向的论文解读、示例代码等我们还会有其他专题深入讨论。</p><p>以上就是关于 Transformer 和 Generative AI 的部分介绍。在下一篇文章中，我们将详细讨论关于 Generative AI 另一个重要的进步方向就是：文字生成（Text Generation）方向。分享这个领域的最新进展，以及亚马逊云科技在为支持这些大型语言模型（LLMs）的编译优化、分布式训练等方面的进展和贡献。</p><p></p><p>作者黄浩文</p><p></p><p>亚马逊云科技资深开发者布道师，专注于 AI/ML、Data Science 等。拥有 20 多年电信、移动互联网以及云计算等行业架构设计、技术及创业管理等丰富经验，曾就职于 Microsoft、Sun Microsystems、中国电信等企业，专注为游戏、电商、媒体和广告等企业客户提供 AI/ML、数据分析和企业数字化转型等解决方案咨询服务。</p><p></p><p>文章来源：<a href="https://xie.infoq.cn/link?target=https%3A%2F%2Fdev.amazoncloud.cn%2Fcolumn%2Farticle%2F6413095e3d950b57b3f9f63d%3Fsc_medium%3Dregulartraffic%26amp%3Bsc_campaign%3Dcrossplatform%26amp%3Bsc_channel%3DInfoQ">https://dev.amazoncloud.cn/column/article/6413095e3d950b57b3f9f63d?sc_medium=regulartraffic&amp;sc_campaign=crossplatform&amp;sc_channel=InfoQ</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8</id>
            <title>零一万物李开复：要做ToC的超级应用，成为AI 2.0时代的微信、抖音</title>
            <link>https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SiIDsk9dX3yQJ3pCO1A8</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 06:31:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 零一万物, 大模型, 开源, 李开复
<br>
<br>
总结: 李开复带领的AI 2.0公司零一万物在4个月内发布了首款预训练大模型Yi-34B和Yi-6B，其中Yi-34B是一个双语基础模型，经过340亿个参数训练。零一万物团队认为34B是一个黄金尺寸，既满足了精度要求，又具备高效率的推理成本。李开复表示，零一万物在中英文上是最好的底座，希望更多人选择Yi-34B。零一万物还注重算力和数据问题的解决，通过建立AI基础设施团队和优化算法和模型，成功降低了Yi-34B的训练成本。零一万物还开源了长窗口的base模型，推动了世界技术革命的发展。 </div>
                        <hr>
                    
                    <p>“我们在3月底官宣零一万物，后面团队逐渐到位，6、7月开始写下第一行代码，历时短短4个月时间，今天我们非常自豪地宣布产品亮相。”李开复在另一万物首款大模型发布会上说道。“从创立零一万物第一天开始，我的目标就是做一个世界级公司，能够进入世界的第一梯队。”</p><p>&nbsp;</p><p>自四个月前李开复宣布大模型创业，业内就给予了众多关注。千呼万唤，李开复交出了第一份答卷。11月6日，李开复带队创办的AI 2.0公司零一万物正式开源发布首款预训练大模型Yi-34B和Yi-6B。Yi-34B是一个双语（英语和中文）基础模型，经过340亿个参数训练，明显小于Falcon-180B和Meta LlaMa2-70B等其他开放模型。</p><p>&nbsp;</p><p>更多详情查看：</p><p><a href="https://www.infoq.cn/news/3m7F87QpDVsu8zv68k1b">李开复4个多月后“放大招”：对标OpenAI、谷歌，发布“全球最强”开源大模型</a>"</p><p>&nbsp;</p><p>对于模型尺寸的选择，零一万物团队认为，34B是一个黄金尺寸。虽然6B也能在某些领域，比如客服上可用，但模型毕竟越大越好，但随之而来的就是推理成本和后续训练的系列资源问题。</p><p>&nbsp;</p><p>“34B不会小到没有涌现或者涌现不够，完全达到了涌现的门槛。同时它又没有太大，还是允许高效率地单卡推理，而且不一定需要H和A级别的卡，只要内存足够，4090或3090都是可以使用的。”李开复解释道，“既满足了精度的要求，训练推理成本友好，达到涌现的门槛，是属于非常多的商业应用都可以做的。”</p><p>&nbsp;</p><p>另外，李开复提到，通用模型决定了行业模型的天花板。虽然行业大模型有相当大的价值，但是底座如果不好，也无法完成超过底座的事情，所以选底座就要选表现最好的底座。李开复自信地表示，“今天我们在中英文上就是最好的底座，没有之一，也希望更多人选择Yi-34B。”</p><p></p><h3>如何解决算力和数据问题</h3><p></p><p>&nbsp;</p><p>“模型团队非常重要，但并不是雇50个人、100人就能解决问题，而是需要很强的团队。这通常不是很大的团队，团队做得太大了反而会分散GPU资源。”李开复说道。零一万物认为，人均GPU卡能用到多少决定了模型能力的上线。</p><p>&nbsp;</p><p>零一万物内部建立了一个AI Infrastructure（人工智能基础设施技术，简称“Infra”）的团队，成员来自国内顶级公司、国内外顶级高校和跨国公司，负责大模型的研发。</p><p>&nbsp;</p><p>在预训练阶段，高价值数据是最重要的，为此零一万物在数据处理上投入了非常大的精力。</p><p>&nbsp;</p><p>首先，零一万物通过采购、合法爬虫、开源等渠道获得训练模型数据。面对庞杂、质量不齐的数据，团队会先用AI能力进行系统化筛选，之后再做人工评估，基本会从一百多T数据里留下3T左右，包括一定比例的中英文数据，该数据保留率是其他厂家的1/10左右。</p><p>&nbsp;</p><p>在训练中，Infra团队花了很长时间研究scaling law，即模型的预测能力。“我们不做各种试错，因为GPU资源非常昂贵，所以我们是要把规模化做好，当推到下一个尺寸时不要再摸索和试错了，因为尺寸越大成本越高。“李开复介绍道。</p><p>&nbsp;</p><p>Infra团队表示，整个模型训练过程其实是动力学过程，中间每一步基本上都可以通过数学方式预测出来，而不需要做大量的实验。因此，团队可以将每一千步的误差控制在千分之几范围内。不管是做数据匹配、超参搜索，还是模型结构的试验，这个方法都特别重要。</p><p>&nbsp;</p><p>Infra团队在6B上做各种实验优化算法和模型，并能丝滑地从6B推向34B。借助该能力，Yi-34B的训练成本下降了40%。</p><p>&nbsp;</p><p>“我们将这一整套的训练平台称为科学训模。很多人把训练大模型比做‘炼丹’，也有人说模型训练一下就飞了，因为它没有收敛。我们做的规模预测用数学科学可以推理，小的尺寸如果能成功，大的尺寸也大概率可以成功，我们实验后也成功了。”李开复表示。</p><p>&nbsp;</p><p>关于算力资源，零一万物在很早时候就做了资源规划，现在的算力储备可以支持其用到18个月以后。另外，团队还建立了故障预测与故障解决大模型，利用模型本身为预训练过程中可能出现的问题设计相应的解决方案，以及如何以最低成本解决这个问题。</p><p>&nbsp;</p><p>对于预训练，零一万物技术副总裁及Pretrain负责人黄文灏表示，过程中并没有特别关注指标，因为针对指标做优化也可能出现问题，所以内部会有很多衡量模型能力的方法。比如模型到底压缩了哪些信息和知识是一个值得关注指标，但只要训练数据足够高质量，training dynamics做得足够好，出来的模型效果自然会比较好。</p><p>&nbsp;</p><p>另外，由于要将模型开源，零一万物在训练模型时还注重模型在IQ和EQ方面的均衡性。团队想要模型既可以支持代码推理类任务，也可以支持情感类任务。</p><p></p><h4>开源长窗口通用模型</h4><p></p><p>&nbsp;</p><p>之前的长窗口工作都是闭源的，无论是OpenAI的32K或者Cloud的100K。零一万物发现，开发者有大量基于长窗口模型进行微调的需求，因此这次直接开源了长窗口的base模型，开发者可以根据自己的数据去微调有效的长窗口应用。</p><p>&nbsp;</p><p>一般来说，更长的窗口会带来更多的计算，计算复杂度也会指数级上升，还要解决数据完备度的问题，这些都对计算、显存、内存和通信等都是非常大的技术挑战。另外，随着窗口越来越长，计算所需时间也越来越长，一旦端到端的反馈时间太长也就没有太大的意义了。因此，大部分模型都会限定窗口大小，零一万物限定了在200K以下。</p><p>&nbsp;</p><p>技术团队进行了全栈优化，包括计算跟通信的重叠堆叠技术、序列并行的技术、通信压缩技术，包括里面关键算子的重构等。虽然后续还有进一步拓宽的余地，但考虑到实用性和成本的均衡，团队目前就开源出来现在的长度版本。</p><p>&nbsp;</p><p>李开复表示，开源对推动世界技术革命的发展有着非常重要的意义。“很多人觉得大模型需要超级多的资源，只有OpenAI、微软、谷歌、阿里、百度、腾讯这样的公司才能做，但是任何技术都是需要全球化的参与，那么开源让大家都有机会能够接触到大模型。”</p><p>&nbsp;</p><p>“这两个模型的尺寸其实就是量身定做给开源社区使用的，资源多的可以用34B，但是也不会需要特别不合理的资源，而6B可以让更多的开发者能够使用。”李开复称。</p><p>&nbsp;</p><p>对于未来会不会开源更大模型的问题，零一万物技术副总裁及AI Infra负责人戴宗宏表示，这不取决于零一万物有没有更大的模型，而是取决于开源社区里的普通开发者有没有能力，或者有没有那么多的资源用到这样的大模型。“如果在摩尔定律之下，更便宜的卡可以支撑更大的模型，我们一定会考虑把我们更大的模型开源。”</p><p></p><h3>做ToC的超级应用</h3><p></p><p>&nbsp;</p><p>“我们对于未来的一个愿景就是，大模型时代不仅仅是人类跨向AGI的重要一步，它也是一个巨大的平台机会。”李开复认为，这个机会就是创造超级应用。</p><p>&nbsp;</p><p>李开复解释称，如果说PC时代赋予给开发者用户的机会是computer on every desk，移动互联网带来的机会是随时随地的计算，smartphone on &nbsp;every hand，那么现在的AI 2.0时代带来的巨大机会就是把一个超级大脑对接和赋能给每一个应用，即AI for everyone。</p><p>&nbsp;</p><p>“PC时代，微软Office就是超级应用；移动互联网时代，微信、抖音是相当好的超级应用；AI 2.0时代，毫无疑问最大的商机也会是超级应用，所以这个方向是零一万物努力的目标。过去的两个时代值得借鉴，因为人类历史就是不断重复，每一个时代最大的机会跟上一个时代是可以推延的。”</p><p>&nbsp;</p><p>李开复的考虑是，首先一切的基础是大模型。“我觉得未来的内容应该主要是由AI来创造，人来帮忙，这个才是王道。所以我们Super APP开发第一点就是AI First、AI Native，没有大模型整个产品就不成立。”</p><p>&nbsp;</p><p>其次，商业化非常重要。AI 1.0公司面临的挑战主要就是商业化问题：要么收入没有做好，要么缺乏持续化收入。“字节、阿里、百度、谷歌、Facebook能够成为伟大的公司，就是因为他们的收入是有质量的。”李开复说道，“所以我们做的应用一定是朝着能够快速有收入，而且能够产生非常好的利润、收入是高质量的、可持续的，而不是一次性在某一个公司上打下一个单子。”</p><p>&nbsp;</p><p>李开复表示，AI 2.0时代的超级应用一定是在消费者级别的ToC超级应用。他透露，Super App的雏形将在不久后对外发布。对于这个Super App，团队会从简单的功能开始，然后根据捕捉到的用户需求和技术精髓不断迭代。此外，该应用虽然面向国内，但也会面向国外市场。</p><p>&nbsp;</p><p>“今天创业者最好的机会是在AI 2.0上面开发App，如果找对机会、聪明快速勤奋地迭代，任何一个App都有机会成为Super App，成为AI 2.0时代的微信、抖音。”李开复说道。</p><p></p><h3>未来规划</h3><p></p><p>&nbsp;</p><p>对于未来，零一万物表示，一方面会继续在34B规模上进行一系列开源动作，另一方面会进一步提高模型的智能极限。</p><p>&nbsp;</p><p>“我们已经在训练千亿参数以上模型，但是我们觉得模型参数可以再提高一到两个数量级，达到万亿或者十万亿的规模。数据上，我们现在基于几十T token的高质量数据，未来还可以提高到几百T或者几千T。模型智能还是有很大的发展。”据悉，零一万物现在已经在训练千亿模型，更大模型的所有前置实验也已完成，剩下的就是按部就班地训练。</p><p>&nbsp;</p><p>此外，零一万物已经有了一个超过十人的多模态方面的团队，未来一两个月内也会有相关产品发布。多模态已经纳入公司更长周期的规划中。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13</id>
            <title>“2023深圳国际金融科技大赛暨微众银行2024校园招聘宣讲会”走进深大：AI、区块链、产品经理的未来在何方？</title>
            <link>https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NjkLsroBG4rfmaAYdu13</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 05:58:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技发展, 人才培养, 深圳国际金融科技大赛, 技术公开课
<br>
<br>
总结: 在金融科技发展中，人才培养是至关重要的一环。为了推进深圳市金融科技人才高地建设工作，并向高校学子提供一个展示自身知识、能力和创意的平台，深圳大学微众银行金融科技学院与微众银行联合举办了“2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛”。在大赛中，组委会特别设置了技术公开课，以让同学们更加了解大赛及人工智能、区块链、金融产品经理的发展现状。通过这样的活动，将高校培养成果嫁接至高质量人才输送链路之中，为学生提供实践和就业的机会的同时，将学术成果有效转化为实际产业解决方案。 </div>
                        <hr>
                    
                    <p>在金融科技发展的过程中，人才培养是举足轻重的关键一环。为了推进深圳市金融科技人才高地建设工作，并向高校学子提供一个展示自身知识、能力和创意的平台，深圳大学微众银行金融科技学院与微众银行联合举办了“<a href="https://www.infoq.cn/news/9AYU96ZSPoCZ6kyClK94">2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛</a>"”（下文称“大赛”）。</p><p></p><p>在本次大赛赛程中，大赛组委会特别设置了技术公开课以让同学们更加了解大赛及人工智能、区块链、金融产品经理的发展现状。继 10 月 25 日<a href="https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC">第一场线上技术公开课</a>"圆满开课，为了让同学们能够更直接、更近距离地了解大赛内容，11 月 3 日，大赛组委会联动微众银行人力资源部门共同走进<a href="https://www.infoq.cn/video/atB6FuOvpIQGQPWLX0Eo">深圳大学</a>"，在线下组织了一场 2023 深圳国际金融科技大赛的第二场技术公开课暨微众银行 2024 校园招聘宣讲会。此次活动将金融科技大赛、企业招聘和高校教育结合在一起，形成了一个良好的产学研合作模式，将高校培养成果嫁接至高质量人才输送链路之中，为学生提供实践和就业的机会的同时，将学术成果有效转化为实际产业解决方案。</p><p></p><p>本次宣讲会邀请到了深大微众金融科技学院党委书记刘山海书记、深大微众金融科技学院院长助理祁涵及微众银行的多位专家来到现场，围绕大赛赛题、赛制和微众银行 2024 校招内容展开宣讲。以下为本期公开课直播精华内容整理：</p><p></p><p></p><h2>一、深大微众金融科技学院院长助理祁涵再次介绍大赛规则及流程</h2><p></p><p></p><p>2023 深圳国际金融科技大赛—— 西丽湖金融科技大学生挑战赛致力于推动国内外高校学生探索金融科技领域的技术应用创新，促进政、学、企三方交流，全面提高学生的创新能力、实践能力和就业竞争力。</p><p>作为 2023 年深圳市金融科技节的重要一环，本届大赛在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2f51f27c732fb8f013d743bc29fb043.png" /></p><p></p><p>本届大赛将通过初赛、复赛在各赛道分别遴选出 10 支队伍进入决赛角逐，并设置总额超过 69 万人民币的赛事奖金及参赛专属区块链数字证书，以奖励各赛道获得一等奖、二等奖、三等奖的队伍及成员。此外，本次大赛还邀请了学术和企业界的众多资深专家为参赛选手答疑解难——特邀国家统计局原副局长许宪春、微众银行首席智能官杨强、中国人民银行研究局原局长张健华等担任学术顾问，评委嘉宾来自微众银行及国内各大顶尖高校。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f52bd2e8110a8dbb969698a29a8a0175.png" /></p><p></p><p>据祁涵介绍，本届大赛所有参赛队员必须是全日制在校大学生（包括本科生、研究生和博士生），须以团队形式参赛，每支队伍人数 2~5 人，每人只能参加一支队伍，必须要独立完成题目，最后产品的知识产权归参赛选手所有，大赛不收取任何报名费用，决赛期间的队伍食宿及往返交通费用由组委会统一安排。</p><p></p><p></p><h2>二、微众银行区块链高级架构师周禄：区块链赛道高分秘籍</h2><p></p><p></p><p>周禄在宣讲最开始就强调，区块链赛道的参赛项目需要基于 FISCO BCOS 平台及微众区块链系列开源技术设计并开发一个区块链系统，以解决 ESG 相关的某个行业或场景的痛点或问题。具体来说，选手可以将区块链技术应用于大湾区一体化、双碳、乡村振兴、公共服务等 ESG 领域。过往赛事中，有一些作品的创意就非常值得参考。比如 2019 年大赛区块链赛道第一名作品就基于 FISCO BCOS 构建了一个排污权许可区块链交易平台，配合交易纠纷仲裁、黑名单、监督审计等链上机制，健全、活化市场，实现企业、政府、公众在环保排污上的三权制衡、多元共治，以辅助排污政策制定，共建污水治理生态循环。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/861dcbc86a0af420dde7ecfcff970638.png" /></p><p></p><p>2020 年区块链赛道第二名作品 WeHelp 则基于微众银行社会治理框架“善度”，使用区块链底层平台 FISCO BCOS、分布式身份解决方案 WeIdentity 等区块链技术，加速求救与救援的匹配。项目还采用可共享的分布式账本记录善行，保证数据公信力，解决求助过程中的信任问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2221f7277edb82f51bb676863e5ccf9.png" /></p><p></p><p>2021 年区块链赛道第三名作品《亿点爱》旨在通过区块链构建公益众筹平台，利用隐私保护技术实现安全可信存储，保护用户个人隐私的同时，有效预防虚假筹款和善款被挪用等问题，以此促进互联网公益行业健康有序地发展。</p><p></p><p>周禄总结了上述获奖作品的特点第一就是应用方向符合 ESG 命题，其次就是充分理解了区块链的特性，并融入应用场景；以上述获奖作品为参考，周禄继续讲解了本届大赛中获得高分的锦囊。</p><p></p><p>参赛作品要赢得评委青睐，首先要选择适合区块链应用场景的正确方向。ESG 指环境、社会和治理，其本质是一种价值观，鼓励企业更多重视财务数据以外的贡献，在保护环境、有利社会和加强治理方面创造价值。而区块链是传递信任的机器，可以大幅降低信任成本；同时区块链可以保护隐私，避免个人数据泄露；区块链还是激励相容的，很容易设计激励机制；区块链的交易记录可全程追溯、可信可验证，这也是它的一大优势。而正是因为具备了这些特性，区块链技术很适合用于 ESG 实践。</p><p></p><p>此外，周禄还为同学们讲解了开发区块链应用的基本步骤。区块链应用的架构包括了用户、分布式应用 DAPP、服务接口 API、智能合约和底层平台，其中 DAPP 可以是命令行、网页、手机或 PC 应用，通过接口和平台通信；服务接口采用通用 JSON 格式 RPC 调用；智能合约采用 Solidity 语言编写；底层平台包含网络、共识、加密和存储等模块。</p><p></p><p>同学们在开发区块链应用时，可以首先使用大赛官方提供的快速建链工具搭建区块链，然后使用业务模版开发 Solidity 合约，并通过交互式控制台的 SDK 部署合约，使用 SDK 开发业务，通过 RPC 协议交互，这里的交互语言没有限制。最后部署业务系统，发起查询和上链交易。这些工作要在团队内分工合作，提高效率。</p><p></p><p>参赛选手要善用各类区块链组件，微众区块链全部开源，提供了众多组件供公众使用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97690f91de649a76fe37f04bb4e68101.png" /></p><p></p><p>大赛官方还提供了大量代码仓库（https://github.com/FISCO-BCOS/FISCO-BCOS），包含很多参考 demo 和开源项目，选手可以直接克隆研究。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68870ba93cf861429ca6fa9a514a8e81.png" /></p><p></p><p>官方的黑客松目录收集了基于 FISCO BCOS 开发的，参与各种大赛的优秀案例，包括每个案例的项目介绍、设计文档、源代码等，供选手参考。周禄推荐各位选手充分利用上述资源，在比赛中取得佳绩。</p><p></p><p></p><h2>三、微众银行个人直通银行部室经理金虎光：银行线上场景的交互式智能柜台服务</h2><p></p><p></p><p>本届大赛产品经理赛道的赛题是《银行线上场景的交互式智能柜台服务》，金虎光在解析赛题时表示，参赛选手应当基于对话式交互的应用进行银行产品方案设计，实现对客户全场景的陪伴式交互，为客户提供智能、便捷、懂用户、有温度的线上银行服务（可选择微众银行 APP、微众银行 We2000 小程序或其他银行产品作为产品框架进行设计）。设置该赛题的背景是银行业传统的线下柜台服务帮助实现银行职员与客户之间的互动，可以处理较为复杂和个性化的问题，更容易发展信任关系，但由于网点服务存在地点和工作时间限制服务效率比较低。近年来兴起的银行线上远程服务则希望通过各种技术手段为客户带来随时随地、方便快捷的体验，同时尽可能做到像线下一样可以处理复杂、个性化的问题。从电话银行到网上银行、银行 APP 再到虚拟数字人服务，银行正在努力将线上数字银行打造成新的增长点，提升金融服务体验和质量，提升客户经营质效。</p><p></p><p>如今各家银行的线上服务都已经包括了几乎所有银行服务功能，但随着功能指数级增长，客户的线上交互也变得非常复杂。每家银行都有多个 APP，如何让客户更方便地找到所需功能是银行面临的普遍挑战。金虎光提到，评委们希望看到参赛选手的创新想法，展现出如何在功能、场景、信息繁多的背景下让用户更加便捷地体验线上银行服务。</p><p></p><p>最近火热的生成式 AI 技术可以贯穿从市场、销售到运营、研发、风控的所有银行服务，带来许多创新性的体验。金虎光建议选手可以选择某一个垂直场景或服务，或基于一揽子服务模式来利用生成式 AI 改进交互形式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/1128d3ad32ef06751999b20f30492ff8.png" /></p><p></p><p>例如作品可以为投资小白用户基于生成式 AI 技术分析、总结投研报告，辅助投资决策。选手还可以参考智能投顾、智能客服、远程面签等技术和场景拓展思路，发挥创意。例如招行 APP 左上角的小猫就会根据用户行为提供实时建议，APP 中的 AI 小招机器人则利用数字人模式提供了智能财富管理顾问服务；百信银行则探索利用虚拟空间模式服务线上用户等等。</p><p></p><p>微众银行也在做相关探索。例如用户可以在微众银行 APP 中与虚拟小 weiWE 机器人对话获得服务，或者使用微众银行小程序快速完成操作。金虎光建议，参赛同学可以选择微众银行 APP 或小程序，亦或是任何自己熟悉的银行 APP 作为底层框架来设计作品。</p><p></p><p>针对产品经理赛道的参赛规则，金虎光也做了详细解读。本赛道中，评委主要考虑以下四个维度为作品打分：</p><p>创新性：参赛作品具备创意亮点，用新思维解决现有问题，或探索新的行业模式，有望开拓新的产业运作模式，市场空间等。商业价值：参赛作品所面向的场景和用户有一定代表性，且作品能够很好地结合实际应用场景解决所描述场景痛点；参赛作品有良好的社会价值，运作合法合规，或具备一定商业价值、成长性和可持续性，值得规模化推广。完整性及可行性：进行有效的竞品、市场及用户分析，并总结产品相对竞品的优劣势、可借鉴及可创新之处，了解用户需求，针对用户痛点提出对应解决方案；有完整的产品设计方案，包括设计背景、产品流程、功能模块说明等，产品架构设计完整，产品流程可形成闭环。技术先进性：可清晰阐述使用的技术，技术有一定先进或创新性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b9b56b49f06ebd08c59291ac2eb5f881.png" /></p><p></p><p>同时还需要注意的是，参赛作品一定要“有需求、有场景”，同时技术是可落地的，尤其要避免创意大而空的问题，要注重实际的创意内涵。与此同时，因为银行的金融交易非常关注安全性，所以作品一定要考虑必要的安全、核身和信息保护环境，理解银行产品背后的相关设计。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56908a6e25ce3a93ec23fa9448cbfc54.png" /></p><p></p><p>金虎光还提醒同学们，参赛作品需提供完整的产品设计文档，建议包括市场及竞品分析、用户调研、产品分析和产品设计说明。初赛需提供产品设计文档（Doc 格式），复赛和决赛还需提供作品展示文档（PPT 格式）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5f39f7ae9b9a27d9dcdc5174809e6e1.png" /></p><p></p><p></p><h2>四、微众银行人工智能资深研究员、FATE 开源社区技术委员会成员范涛：人工智能赛道讲解</h2><p></p><p></p><p>范涛首先介绍了基于微众人工智能技术开源的 FATE 项目。这是目前国内最大的联邦学习开源项目和社区之一。FATE 不仅提供了底层框架，还提供了很多应用组件。选手须基于该平台构建人工智能产品，可以充分利用 FATE 提供的各类算法和组件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5ddedbc44d28162945c216cfdc791716.png" /></p><p></p><p>本赛道命题为开放式，作品可基于 AI 联邦学习开源平台 FATE，设计纵向联邦学习、横向联邦学习或者联邦大模型创新性产品或算法，包括并不限于联合风控，联合营销，智能权益定价，数据交易定价等场景。项目须使用 FATE 开源技术实现，选手须将实现代码提交至 Github 供评委考核。评委基于作品的产品实现完备性、创新性和商业价值打分：</p><p>产品实现完备性占 40% 分数，考察作品在技术层面的复杂度和实现完成度。完备性包括可行性分析、方案设计文档、代码实现、测试报告等方面。创新性占 40% 分数，考察作品在设计层面的新颖度。创新性包括但不限于全新的场景痛点，用新的算法或技术手段，有效综合利用多个技术组件从而产生新的效能。商业价值考察作品在人工智能领域中的综合价值。综合价值包括但不限于作品与实际产业的贴合度、是否有效解决真实行业痛点、是否有效地发挥了联邦学习的实用价值等。</p><p></p><p>选手须组队参赛，个人可先报名，由平台协助组队；初赛作品于 11 月 27 日截止提交，包含作品介绍（PPT 格式）、技术文档（Doc 格式）、作品展示材料（包括但不限于作品部分 demo 或演示视频等）。12 月 5 日大赛公布决赛入围名单，16-17 日在深圳举办线下 36 小时封闭马拉松，选手对初赛提交作品进行开发和完善，并做现场路演答辩。</p><p></p><p>除了赛程相关的信息，范涛还向同学们解读了横向联邦学习、纵向联邦学习和联邦大模型三大技术栈：</p><p>横向联邦学习是指每个终端都有一些同质数据，但每一方都有数据隐私保护需求，仅靠自己的数据不足以构建较好的模型，所以希望综合多方数据构建模型，本质上是扩充数据样本来提升模型效果和稳健性的方法，适用于参与者数据特征重叠较多，而样本 ID 重叠较少的情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/821713d23a19373d8feb8a34f18024ff.png" /></p><p></p><p>纵向联邦学习在金融领域应用非常广泛。它通过引用第三方数据与金融数据结合来提升风控、营销等场景的效果，本质上是通过扩充特征维度来提升模型效果，适用于参与者样本重叠较多的情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/157d0219eec8cfabb4b7fa7e65d3c274.png" /></p><p></p><p>大模型技术与联邦迁移学习有很多结合点，后者也是大模型领域的新兴范式。通过联邦迁移学习，大模型可以和本地私有数据结合，成为适合本地数据的中小模型。该课题是本届大赛的新增部分，范涛推荐选手关注。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ef4d47d97602fb405fcfbfaef48c6a8.png" /></p><p></p><p>范涛最后介绍了 2022、2020、2019 年人工智能赛道名列前茅的一些作品，他希望参赛选手参考这些优秀案例，设计出令人称赞的高分作品：</p><p>2022 年：一等奖作品是面向真人体验感知的系统，通过横向联邦学习场景综合多人信息去做感知应用；二等奖作品是将联邦学习应用于工业智能，如火焰检测、工业设备缺陷检测等场景；三等奖作品是纵向联邦学习应用于电网的场景。2021 年：一等奖作品是心理健康预测监控系统，综合用户的文本、图像、社交媒体数据心理健康行为预测平台；二等奖作品是基于 FATE 构建的联邦营销一站式平台，可以综合多企业数据来做更精准的营销。三等奖作品横向联邦场景来做保险经纪人，通过 F ATE 平台保护用户隐私。2019 年：一等奖作品是横向联邦学习场景，通过车载行为数据对车险定价；二等奖作品基于联邦做了联邦图形预算法，联合多个银行的交易网络，在不泄露隐私数据的前提下预测欺诈用户；三等奖作品是基于联邦学习平台做个人数据定价，形成新的数据交易模式。</p><p></p><p></p><h2>五、写在最后</h2><p></p><p></p><p>除了以上干货内容，深大微众金融科技学院党委书记刘海山书记在本次宣讲会开场时就为同学们加油鼓气。刘书记希望参赛学生能够积极探索金融科技领域的技术应用创新，将创新成果转化为实际应用，向金融科技行业提供更有价值的技术解决方案，为深圳乃至全国的金融科技发展贡献力量。</p><p></p><p>在本次宣讲会的校招环节，微众银行零售存款部总经理邢海鹏整体介绍了微众银行的业务和技术。据其介绍，微众银行是全国首家数字银行，2019 年与深圳大学合作成立了深大微众金融科技学院。微众银行在 IT 方面的投入营收占比超过了 9%，科技人员占比超过 50%，微众银行在 AI、区块链、云计算、大数据等方面都有大量投入，并取得了一系列行业领先的成果。</p><p></p><p>微众银行人力资源部室资深经理杨帆详细介绍了微众银行的人才结构——银行员工平均年龄 33 岁，本科及以上学历超过 99%，人才来源也非常多元化。校招生身份入职的微众银行的同学，可以获得专属“私塾学习计划”，并为校招生设置了一年的培养期。</p><p></p><p>2024 年微众银行校园招聘主要分为技术研发、数据算法、产品业务、综合职能、财富管理五大类别，具体的招聘详情可以根据下列途径进行了解 ↓</p><p></p><p><img src="https://static001.infoq.cn/resource/image/96/fb/96ee25124fde526ded7913138c2199fb.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu</id>
            <title>阿里云CTO周靖人：API和模型级别开放大模型能力，做to C 产品不是目标</title>
            <link>https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zptXlfRaUrtvBoE8bYmu</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 04:17:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, 大模型通义千问2.0, GPT-3.5, 智能编码助手通义灵码
<br>
<br>
总结: 阿里云发布了千亿级参数大模型通义千问2.0，该模型在综合性能上超过了GPT-3.5，并且正在追赶GPT-4。此外，阿里云还发布了智能编码助手通义灵码等行业应用大模型。阿里云的目标是将模型能力开放给更多的开发者和合作伙伴使用，以支持他们在创业、落地和创新方面的需求。阿里云的大模型策略包括提供先进的AI基础设施、开源模型与产品结合的服务以及简单的API集成。这次AI技术变革是一次技术体系的全面升级，包括系统优化、提升开发效率和打造最好的AI基础设施。云厂商需要懂AI和云计算，以更低的成本提供模型服务。国内模型生态正在快速发展，代表着算力的发展。 </div>
                        <hr>
                    
                    <p>10月31日，阿里云正式发布千亿级参数大模型通义千问2.0。官方数据显示，在10个权威测评中，通义千问2.0综合性能超过GPT-3.5，正在加速追赶GPT-4。此外，阿里云还发布了智能编码助手通义灵码等行业应用大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc23cc39efb9acd3e61f3863210bbba1.jpeg" /></p><p>&nbsp;</p><p>针对阿里云的大模型策略，阿里云CTO周靖人在接受记者采访时表示，“我们的目标并不是做toC的产品，而是希望更多地把模型的能力开放出来，让更多开发者、合作伙伴使用。”</p><p>&nbsp;</p><p>据周靖人介绍，阿里云的定位是“服务好各种各样在AI时代的创业者、开发者和企业客户等。通过多层技术能力，包括AI基础设施、模型等能力，最好地支持开发者和客户，帮助他们解决在创业、落地、创新等方面的问题。”</p><p>&nbsp;</p><p>具体来讲，模型创业公司希望使用到最先进的AI基础设施；企业客户希望将开源模型与自己产品做二次结合，这类产品包括通义千问等开源模型、帮助企业做模型定制的阿里云百炼等；还有关注业务系统的开发者，通过简单的API集成到自己业务体系中。</p><p>&nbsp;</p><p>周靖人表示，这次AI技术变革的实质是一次技术体系的全面升级。对云计算来说，主要包括以下纬度：第一，系统优化，即如何利用模型能力优化复杂庞大的分布式系统，让它真正变成一个“自动驾驶的云”；第二，用模型帮助提升开发效率，即让用云这件事本身变得更加智能；第三，以模型为中心打造最好的AI基础设施，提供低成本、一站式的模型训练、微调、推理等服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f74a49d11d71efa482b001ac1473b48.jpeg" /></p><p>&nbsp;</p><p>“而云厂商既要懂AI，又要懂云计算，才能在这次竞争里取得一个重要的战略性优势。”周靖人表示。具体来说，云厂商要做的事情就是让用户以更低的成本使用模型来提供服务。</p><p>&nbsp;</p><p>“今天基础设施的目标，特别是模型推理方面，不单是提升延迟等各个方面的性能，同时还要能够降低使用成本。在这方面，我们还有大量的工作需要做。”周靖人表示，阿里云不是简单开发界面的开放，而是API级别、模型级别的开放。用户可以借这些产品发挥更大的想象空间，做更多业务的创新。</p><p>&nbsp;</p><p>周靖人呼吁，大家要给这个领域一些时间。“毕竟从国内来讲，整个产业的变化是从今年开始的，甚至到了3、4月份，大家才陆陆续续发模型。在这方面，我们的确比海外要晚，海外拥有至少一年的先发优势，甚至更长的时间。”</p><p>&nbsp;</p><p>不过，周靖人表示，国内也在快速地追赶中。短短半年时间内，国内模型生态已经慢慢发展起来了。模型的生态发展起来，一定代表了算力发展得起来。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw</id>
            <title>火山引擎金融解决方案负责人王建军确认出席 FCon，分享金融数字化升级：让智慧带来生产力</title>
            <link>https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0Mh7GIzC3GdmtAQGW0Jw</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 火山引擎金融解决方案, 大模型发展及应用现状, 王建军
<br>
<br>
总结: FCon 全球金融科技大会将在上海举行，火山引擎金融解决方案负责人王建军将分享关于金融数字化升级和大模型应用的主题演讲。他将介绍国内外大模型发展及应用现状，以及火山引擎在金融行业的探索实践。此外，大会还将涉及DevOps在金融企业落地实践、金融科技创新应用、金融数据平台建设、金融安全风险管控和数据合规等领域的交流。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎金融解决方案负责人王建军将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5606?utm_source=infoqweb&amp;utm_medium=article">金融数字化升级：让智慧带来生产力</a>"》主题分享，介绍国内外大模型发展及应用现状、火山引擎在金融行业的探索实践，以及大模型在未来行业的应用。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5606?utm_source=infoqweb&amp;utm_medium=article">王建军</a>"，10 年以上人工智能及数字化实践经验，2020 年加入字节跳动，曾服务于德勤咨询、第四范式等企业，推动过数十家金融机构的数字化转型和人工智能应用实践。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型：让智慧带来生产力</p><p></p><p>通过观察海外大模型领先应用实践，结合国内产业生产效率痛点，明确大模型应用蓝图，及围绕蓝图展开的探索与实践。</p><p></p><p>演讲提纲：</p><p></p><p>海内外大模型风起云涌；大模型能力范畴和典型应用分析；围绕金融行业的探索实践；展望未来。</p><p></p><p>你将获得：</p><p></p><p>○ 了解到国内外大模型发展及应用现状；</p><p>○ 了解火山引擎在金融行业的探索实践；</p><p>○ 共同畅想大模型未来行业应用。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！大会 8 折优惠报名倒计时仅剩 3 天，现在购票立减￥1360。咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON</id>
            <title>“算”赋千行，“智”启新程，天翼云多项成果惊艳亮相，邀您共鉴！</title>
            <link>https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7wdgYqK7JYVvdEay5hON</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 03:16:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化基础设施建设, 人工智能产业, 算力基础设施, 智算需求
<br>
<br>
总结: 近年来，我国数字化基础设施建设不断完善，人工智能产业蓬勃发展，成为当下驱动经济社会转型升级的重要力量。为抢抓人工智能发展的重大机遇，构筑我国人工智能发展先发优势，国家陆续出台了多项政策，将人工智能列为国家战略性新兴产业，鼓励人工智能行业发展与创新。在此背景下，智算作为人工智能时代的关键生产力要素，需求呈爆发式增长。为推进算力基础设施高质量发展，充分发挥算力对数字经济的驱动作用，工业和信息化部、中央网信办、教育部等六部门联合印发《算力基础设施高质量发展行动计划》明确提出，结合人工智能产业发展和业务需求，重点在西部算力枢纽及人工智能发展基础较好地区集约化开展智算中心建设，逐步合理提升智能算力占比。面对激增的智算需求，天翼云作为云服务国家队，从多方位升级算力基础设施，为人工智能产业发展夯实算力底座，加速科技普惠。 </div>
                        <hr>
                    
                    <p> 近年来，我国数字化基础设施建设不断完善，人工智能产业蓬勃发展，成为当下驱动经济社会转型升级的重要力量。为抢抓人工智能发展的重大机遇，构筑我国人工智能发展先发优势，国家陆续出台了多项政策，将人工智能列为国家战略性新兴产业，鼓励人工智能行业发展与创新。</p><p></p><p>在此背景下，智算作为人工智能时代的关键生产力要素，需求呈爆发式增长。为推进算力基础设施高质量发展，充分发挥算力对数字经济的驱动作用，近日，工业和信息化部、中央网信办、教育部等六部门联合印发《算力基础设施高质量发展行动计划》明确提出，结合人工智能产业发展和业务需求，重点在西部算力枢纽及人工智能发展基础较好地区集约化开展智算中心建设，逐步合理提升智能算力占比。根据IDC报告显示，预计到2026年，中国智能算力的年复合平均增长率达到52.3%，是三倍于通用算力规模的增长速度。</p><p></p><p> 面对激增的智算需求，天翼云作为云服务国家队，从多方位升级算力基础设施，为人工智能产业发展夯实算力底座，加速科技普惠。技术方面，天翼云始终坚持科技创新，不断攻克关键核心技术，以云操作系统为核心，从底层基础软硬件技术，到上层高阶云能力，实现了全栈技术的自主可控。基础设施方面，天翼云不断完善“2+4+31+X”云网融合资源布局，构建了“集中化+区域化+属地化+边缘化”的云网基础设施，积极推进算力普惠发展；天翼云建设的新一代智算中心在算力、算效、资源利用率等方面不断追求极致，降低大模型训练、推理、部署、应用门槛。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce40fa919dc071d54806d97739a0aef3.png" /></p><p></p><p> 人工智能浪潮下，天翼云凭借项目沉淀、技术积累，打造智能计算平台，依托分布式架构云底座和充沛的计算、存储、网络资源，为大模型训练、智能推荐、无人驾驶、生命科学、NLP等业务场景提供智算、超算、通算多样化算力服务，激发数字经济发展新活力。同时，天翼云在通用算力资源全国布局的基础上，科学规划建设智能算力，不断夯实国云智算底座，构建AI时代强大基石。</p><p></p><p> 当今社会，科技高速发展，每一轮技术变革无不渗透在我们的日常生活中。站在大算力、大模型、大数据的“高起点”上，天翼云致力于以科技创新服务千行百业，加速算力普惠民生。为进一步推动人工智能应用落地，加速生产生活方式智慧变革，11月10日—13日，以“数字科技 焕新启航”为主题的2023数字科技生态大会即将启幕。</p><p></p><p> 届时，天翼云将亮相大会主论坛及多个分论坛，重磅发布智算领域科技创新最新成果，并带来云电脑等产品的最新升级，同时在展区，天翼云也将从科技创新、算力底座、产业引领等层面，系统性展示领先的云能力和实践成果，以国云筑基，携手业界共创智算新时代</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP</id>
            <title>谷歌开源 AI 微调方法： Distilling Step-by-Step</title>
            <link>https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P2agXGEtoLNotk2Eb8xP</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 Nov 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华盛顿大学, 谷歌研究中心, 逐步蒸馏, 小型微调模型
<br>
<br>
总结: 华盛顿大学和谷歌研究中心的团队开源了一种名为"逐步蒸馏"的技术，用于微调规模较小的语言模型。逐步蒸馏通过使用大型语言模型生成小型微调数据集，训练小模型来预测输出标签并生成对应的理由。这种方法可以在减少训练数据集规模和模型大小的同时，提高小型模型的性能。谷歌使用逐步蒸馏技术在NLP基准测试中取得了良好的表现，仅使用数据集的一小部分数据就能超越大型语言模型的性能。 </div>
                        <hr>
                    
                    <p>华盛顿大学和谷歌研究中心的一个团队最近开源了 <a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html">Distilling Step-by-Step</a>"（逐步蒸馏），一种用于微调规模较小的语言模型的技术。与标准微调相比，逐步蒸馏需要的训练数据更少，并且生成的模型更小，但模型性能却优于参数规模是它 700 倍的小样本提示大型语言模型 （LLM）。</p><p>&nbsp;</p><p>虽然 LLM 一般可以在提示较少的情况下在多种任务上有良好的表现，但由于其内存和算力要求过高，模型的托管是比较有挑战的。规模较小的模型在微调后也可以有良好的表现，但这需要工程师手动创建针对具体任务优化的数据集。逐步蒸馏的关键思想是使用 LLM 自动生成一个小型微调数据集，其中的数据有一个输入和一个输出标签，以及选择这个输出标签的“理由”。微调过程会训练这个小模型来预测输出标签并生成对应的理由。在 NLP 基准上评估时，小型微调模型的性能优于 540B PaLM 模型，同时仅需要这个基准测试的全部微调数据的 80%。据谷歌称：</p><p></p><p></p><blockquote>我们展示了，逐步蒸馏既减少了构建针对特定任务的较小模型所需的训练数据集规模，也减少了实现甚至超越小样本提示 LLM 的性能水平所需的模型大小。总的来说，逐步蒸馏提出了一种可以高效利用资源的范例，可以解决模型大小和所需训练数据之间的权衡问题。</blockquote><p></p><p></p><p>研究表明，增加 LLM 中的参数规模可以提高其性能，目前最先进的模型（例如 PaLM）拥有数百亿个参数。然而，这些大型模型价格昂贵，且难以用于推理，因为它们需要多个并行连接的 GPU 才能把这么多参数保存在内存里。最近的研究开发出了规模稍小的模型（例如 Meta 的 Llama 2），其性能表现差不多，但参数少了一个数量级；然而，这些小一些的模型还是很庞大，需求的算力也很高。</p><p>&nbsp;</p><p>要做出在特定任务上表现良好的小模型的一种方法，是使用针对具体任务收集的数据集来微调小规模语言模型。虽然这个数据集可能相对较小（大约有数千个示例），但其数据收集起来可能还是费时费钱。另一种选择是知识蒸馏，也就是使用大型模型作为较小模型的老师。 InfoQ 最近报道了谷歌开发的一项<a href="https://www.infoq.com/news/2023/01/google-llm-self-improvement/">技术</a>"，使用 PaLM LLM 来创建训练数据集，最后生成的微调模型的性能可与规模大 10 倍的 LLM 相媲美。</p><p>&nbsp;</p><p>逐步蒸馏确实需要微调数据集，但它减少了创建高性能模型所需的数据量。源数据集通过思维链提示输入 PaLM LLM，要求模型给出其答案的理由。输出结果是修正后的微调数据集，其中包含原始输入和答案以及理由。这个较小的目标模型经过微调来执行两项任务：回答原始问题并生成理由。</p><p>&nbsp;</p><p>谷歌使用四个 NLP 基准测试评估了他们的技术，每个基准都包含一个微调数据集。他们使用逐步蒸馏来修正这些数据集，并使用了参数不到 1B 的微调 T5 模型。他们发现，这些模型在仅使用数据集的一小部分数据的情况下，性能就比基线微调模型要好；在某些情况下只要 12.5% 的数据就有这样的表现。他们还发现，他们的 770M 参数模型在 ANLI 基准测试中的性能优于大它 700 倍的 540B 参数 PaLM，同时只需要 80% 的微调数据集数据。</p><p>&nbsp;</p><p>在 X（以前的 Twitter）上关于这项工作的讨论中，人工智能企业家 Otto von Zastrow 写道：</p><p></p><p></p><blockquote>这些结果非常厉害。我会把这种办法叫做合成数据生成，而不是蒸馏，我真的很好奇，如果你根据每个示例问题的合成理由来训练原始的 LLM 会发生什么事情。</blockquote><p></p><p></p><p>逐步蒸馏的源代码和训练数据集可在 <a href="https://github.com/google-research/distilling-step-by-step">GitHub</a>" 上获取。 Google Cloud 的 Vertex AI 平台还提供该算法的非公开预览。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/google-distillation/">https://www.infoq.com/news/2023/10/google-distillation/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a</id>
            <title>OpenAI 用45分钟重塑游戏规则！干掉MJ、LangChain，创造“不会编程的应用开发者”新职业</title>
            <link>https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/67vMj2F2HTC24fDdE64a</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 Nov 2023 06:06:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, DevDay, 新产品, GPT-4 Turbo, Assistants API, 多模态功能, ChatGPT, GPT Store
<br>
<br>
总结: OpenAI在DevDay开发者日活动上发布了多款新产品，包括功能更强大且价格更低廉的GPT-4 Turbo模型，允许开发人员构建AI助手应用的Assistants API，以及支持视觉、图像创建和文本转语音等多模态功能。此外，OpenAI还推出了ChatGPT的自定义版本GPTs，并计划推出GPT Store用于分享用户构建的自定义GPT助手。 </div>
                        <hr>
                    
                    <p>北京时间 11 月 7 日凌晨 02:00，OpenAI 的首次 DevDay 开发者日活动正式开始。Sam Altman 用了45 分钟的时间发布了多款新产品。微软 CEO Satya Nadella 还亲自去现场参与了这次发布。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f34366844d84e3463822882a2b0cb0dd.png" /></p><p></p><p>此次，OpenAI分享了数十项新增功能和改进，并降低了平台上多种服务的价格。具体包括：</p><p>&nbsp;</p><p>新的GPT-4 Turbo模型，功能更强大、价格更低廉且支持128K上下文窗口。新的Assistants API，允许开发人员轻松构建具有目标且能够调用模型及工具的AI助手应用。平台提供新的多模态功能，包括视觉、图像创建（DALL-E 3）及文本转语音（TTS）等。</p><p>&nbsp;</p><p>此外，OpenAI还推出了 ChatGPT 的自定义版本 GPTs。OpenAI表示，GPTs 是一种新方式，任何人都无需编码就可以创建 ChatGPT 的定制版本，以便其在日常生活、特定任务、工作或家庭中更有帮助，并与其他人分享该创作。比如，GPTs 能协助用户掌握任何桌面游戏的规则、辅助孩子学习数学或者设计个性贴纸。</p><p>&nbsp;</p><p>目前，ChatGPT Plus 和 Enterprise 用户已经能够尝试包括Canva和Zapier AI Actions在内的 GPTs 示例。</p><p>&nbsp;</p><p>本月晚些时候，OpenAI将推出GPT Store，主要用于分享用户构建的自定义 GPT 助手，开发者可以借此赚钱，使用自己作品的用户数越多收入越高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cad7684bc790f29a6d3a57853fc37b75.png" /></p><p></p><p>Sam Altman 展示GPT Store</p><p>&nbsp;</p><p>对于此次大会，网友 altoidsjedi 难掩兴奋：“对于我们当地的 LLM 人员来说，鉴于 Orca / Dolphin / Airboros 等合成的、GPT-4 生成的数据集的成功（特别是在删除拒绝响应之后），这些新的 OpenAI API 工具将不可避免地导致开源合成数据集的增加充满了使用视觉、检索、函数调用等的优秀示例。我们可以使用数据集来微调/教学/提炼到我们的本地人工智能模型中以模拟相同的行为！这是双赢。”网友 Independent_Key1940 则一阵见血地指出，“降低成本才是当前最应该做的事情，这能在很大程度上改变现状。”</p><p>&nbsp;</p><p>此次公布的各项新功能，将于太平洋时间11月6日下午一点起向OpenAI客户开放。下面，我们具体看下OpenAI这次到底为我们带了哪些“惊喜”。</p><p></p><h2>功能增强了，价格低了</h2><p></p><p>&nbsp;</p><p></p><h4>GPT-4 Turbo迎来128K上下文</h4><p></p><p>&nbsp;</p><p>OpenAI于今年3月发布了GPT-4的首个版本，并于7月开放GPT-4的通用版。此次，OpenAI又带来下一代GPT-4 Turbo的预览版本。这也是这次颇受关注的更新之一。</p><p>&nbsp;</p><p>GPT-4 Turbo功能更强，学习内容截止于2023年4月。它拥有128k上下文窗口，因此能够在单一提示词中容纳相当于300多页文本的内容。据悉，与GPT-4相比，GPT-4 Turbo的输入token成本只相当于三分之一、输出token成本则为二分之一。</p><p>&nbsp;</p><p>GPT-4 Turbo现通过API中的gpt-4-1106-preview向所有付费开发者开放，OpenAI计划将在未来几周内发布稳定的生产就绪模型。</p><p>&nbsp;</p><p>除了GPT-4 Turbo之外，OpenAI还为GPT-3.5 Turbo发布了新的版本，默认支持16k上下文窗口。新的3.5 Turbo将支持改进的指令跟踪、JSON模式与并行函数调用。OpenAI的内部评估显示，格式遵循任务（例如生成JSON、XML和YAML）的效果提高了38%。开发人员可以调用API中的gpt-3.5-turbo-1106来访问这个新模型。使用gpt-3.5-turbo名称的应用程序将从12月11日起自动升级至该新模型。</p><p>&nbsp;</p><p>此外，OpenAI还在函数调用更新、改进指令遵循与JSON模式、可重复输出与对数概率方面做了改进。OpenAI还计划在未来几周推出另一项功能，用于为GPT-4 Turbo和GPT-3.5 Turbo返回潜在输出标记的对数概率，这项功能将在搜索体验的自动补全等功能中发挥重要作用。</p><p>&nbsp;</p><p></p><h4>发布 Assistants API</h4><p></p><p>&nbsp;</p><p>OpenAI此次发布了Assistants API，这是帮助开发人员在应用程序当中构建智能体功能的第一步。</p><p>&nbsp;</p><p>助手智能体是一种专用型AI，提供特定指令和额外的专业知识，并可调用模型及工具以执行任务。新的Assistants API提供Code Interpreter代码解释器、Retrieval检索以及函数调用等新功能，可以处理以往用户必须新手完成的大量繁重工作，帮助开发者腾出精力构建高质量的AI应用程序。</p><p>&nbsp;</p><p>此API在设计上充分强调灵活性，用例范围包括基于自然语言的数据分析应用、编码助手、AI驱动的假期规划器、语音控制DJ、智能视觉画布等等。Assistants API与OpenAI的新GPT产品基于相同的功能基础，包括Code Interpreter、Retrieval和函数调用等自定义指令和工具。</p><p>&nbsp;</p><p>该API还引入一项关键变化，即持久且无限长线程，允许开发人员将线程状态管理移交给OpenAI以解决上下文窗口约束。使用Assistants API，用户只需要将每条新消息都添加至现有线程即可。</p><p>&nbsp;</p><p>Assistants还可根据需要调用新工具，包括：</p><p>&nbsp;</p><p>Code Interpreter：在沙盒执行环境中编写并运行Python代码，可以生成图形和图表，并处理包含不同数据和格式的文件。它允许AI助手以迭代方式运行代码，从而解决极具挑战的编码和数学问题。Retrieval：利用模型以外的知识来增强助手，例如专有领域数据、产品信息或用户提供的文档。如此一来，大家无需对文档嵌入进行计算和存储，也无需使用分块和搜索算法。Assistants API将根据OpenAI在ChatGPT中构建知识检索的经验，对各类常用检索方法进行优化。函数调用：使助手能够调用定义的函数，并将函数响应合并至消息当中。</p><p>&nbsp;</p><p>与家族中的其他产品一样，OpenAI表示，永远不会将用户上传至API的数据和文件用于训练自有模型，开发人员还可以根据需求随时删除数据。Assistants API目前处于beta测试阶段，且从即日起面向全体开发者开放。</p><p>&nbsp;</p><p></p><h4>API中的新模式</h4><p></p><p>&nbsp;</p><p>现在，GPT-4 Turbo可接收图像作为Chat Completions聊天补全API中的输入，从而实现标题生成、真实图像分析、阅读带图形的文档等用途。例如，BeMyEyes就使用这项技术帮助盲人或弱视人群完成日常任务，例如识别产品或浏览商店。开发人员可以通过API中的gpt-4-vision-preview来访问此项功能。</p><p>&nbsp;</p><p>OpenAI还计划为GPT-4 Turbo主模型提供视觉支持，这项新功能将被纳入稳定版本，而计费标准则由输入的图像大小决定。例如，将一张1080 x 1080像素的图像上传至GPT-4 Turbo的费用为0.00765美元。</p><p>&nbsp;</p><p>此外，开发人员可以通过Images API将DALL-E 3 直接集成至自己的应用程序和产品当中，具体方式就是用dall-e-3指定模型。据悉，Snap、可口可乐和Shutterstock等公司已经使用DALL-E 3以编程方式为客户及活动生成图像和设计。</p><p>&nbsp;</p><p>与之前版本的DALL-E类似，该API中同样内置有审核功能，可帮助开发人员保护自身免遭滥用。OpenAI还提供不同的格式和质量选项，生成单张图像的起步价格为0.04美元。</p><p>&nbsp;</p><p>开发人员还可以通过文本转语音API将普通文本转换为与真人质量相当的语音。新的TTS模型提供六种预设声音以及两种模型变体：tts-1和tts-1-hd。tts针对实时用例进行了优化，而tts-1-hd则主要面向更高的质量需求。每输入1000字符的起步价格为0.015美元。</p><p></p><h4>模型定制</h4><p></p><p>&nbsp;</p><p>OpenAI表示，正在开发一款用于GPT_4微调的实验性访问程序。初步结果表明，与GPT-3.5微调实现的效果相比，GPT-4微调需要更大的工作量才能对基础模型做出有意义的改进。随着GPT-4微调质量与安全性的提升，已经熟悉GPT-3.5微调开发人员现可尝试在微调控制台中操作GPT-4程序。</p><p>&nbsp;</p><p>对于需要在微调之外更多定制模型特征的用户（主要指拥有超大规模专有数据集、对应数十亿token的场景），OpenAI还启动了模型定制计划，为特定组织提供与OpenAI研究团队合作的机会，共同面向特定领域对GPT-4做定制训练。其中包括修改模型训练流程中的各个步骤，开展额外的特定领域预训练，以及运行针对特定领域定制的强化学习后训练过程。组织将拥有对其定制模型的独家访问权。</p><p>&nbsp;</p><p>根据OpenAI的现有企业隐私政策，自定义模型不会被提供给其他客户或对外开放，也不会被用于训练其他模型。此外，提供给OpenAI用于训练自定义模型的专有数据也不会在任何其他环境中被重复使用。不过，这项计划高度受限且价格昂贵，只面向特定组织开放。</p><p></p><h4>价格更低，限制更少</h4><p></p><p></p><p>OpenAI正在下调各项服务的价格，希望将节约下的成本回馈给用户（以下价格均以1000 token为单位）：</p><p>&nbsp;</p><p>GPT-4 Turbo的输入token价格为GPT-4的三分之一，即0.01美元；输出token为GPT-4的二分之一，即0.03美元。GPT-3.5 Turbo的输入token为此前16k模型的三分之一，即0.001美元；输出token价格为二分之一，即0.002美元。此前使用GPT-3.5 Turbo 4k的开发者输入token价格将下降三分之一，即0.001美元。所有价格下调仅适用于此次推出的新款GPT-3.5 Turbo。经过微调的GPT-3.5 Turbo 4k模型输入token价格降低至四分之一，即0.003美元；输出token下调至1/2.7，即0.006美元。微调版本还通过新的GPT-3.5 Turbo模型，实现了价格与原4k版本相同、但上下文窗口扩大至16k的效果。这些新价格也将适用于微调版gpt-3.5-turbo-0613模型。</p><p></p><p>&nbsp;</p><p>OpenAI还将每位付费GPT-4客户的每分钟token限制扩大了一倍，现在大家可以在速率限制页面查看新的指标。OpenAI还公布了自动速率限额的使用等级，用户可以根据自己的情况查看相应限额，并在账户设置中申请提升限额。</p><p>&nbsp;</p><p>版权保护方面，OpenAI 推出了Copyright Shield，即帮助客户应对关于侵犯版权的法律索赔，并支付由此产生的费用。这项服务将在ChatGPT Enterprise及开发者平台上全面开放。</p><p>&nbsp;</p><p>此外，OpenAI即将发布Whisper large-v3，即开源自动语音识别模型（ASR）的下一版本，其跨语言性能将得到提升。OpenAI还计划在不久之后通过API支持Whisper v3。</p><p>&nbsp;</p><p>OpenAI同时开源了Consistency Decoder，即Stable Diffusion VADE解码器的替代方案。这款解码器针对Stable Diffusion 1.0+ VAE所兼容的一切图像做出优化，在文本、人脸和直线等处理能力上均有显著改进。</p><p></p><h2>ChatGPT周活跃用户破亿，大股东微软CEO现身“带货”Azure</h2><p></p><p>&nbsp;</p><p>在开发者大会上，OpenAI 公司CEO Sam Altman 宣布，ChatGPT 的周活用户数量已经突破 1 亿。自今年 3 月通过 API 发布 ChatGPT 与 Whisper 模型以来，该公司目前已经吸引到超 200 万开发人员，涵盖超 92% 的全球财富 500 强企业。</p><p>&nbsp;</p><p>在发布的近一年之后，ChatGPT 已经被广泛认定为有史以来增速最快的消费级互联网应用，其用户数量估计在短短两个月内就达到 1 亿。相比之下，Facebook 自 2004 年推出以来经过约四年半时间才拥有 1 亿用户，Twitter 达成这个目标用了五年多时间，Instagram 则用了两年多。可以说，ChatGPT 仍是有史以来增长速度最快的服务之一。</p><p>&nbsp;</p><p>今年以来，OpenAI 聊天机器人似乎成为流量密码，谁拥抱它、谁就能获得用户的青睐。早在今年 2 月，Similarweb 就估计该工具已经迈过了单月 1 亿访问者、单日 2500 万访问者的里程碑。但本次大会上的声明尤其值得关注，因为这是 OpenAI 发布的官方数字，而非第三方粗略统计。有评论认为，OpenAI 发布这些数据似乎是为了反驳近期媒体的报道，即自去年 11 月上线以来 ChatGPT 的人气正有所下滑。</p><p>&nbsp;</p><p>此外，微软 CEO&nbsp;Satya Nadella&nbsp;意外现身 OpenAI 开发者大会，并传达了一条明确信息：请与我们携手创造。Satya Nadella&nbsp;向 Sam Altman 强调，“我们的首要任务就是打造出最好的系统，你们可以借此构建起最好的模型，再将其开放给开发人员。”</p><p>&nbsp;</p><p>据报道，微软已经先后向 OpenAI 投资 130 亿美元，并希望吸引更多开发者使用其 Azure 云基础设施提供的计算和存储资源，而非选择亚马逊云科技和 Google Cloud 等竞品。近年来，Azure 已经成为微软的关键业务增长引擎，也帮助该公司重振了自身在开发者心中的品牌形象。</p><p>&nbsp;</p><p>在本次大会上，OpenAI 公布了更加强大的 GPT-4 Turbo 模型，并表示用户还可以借此构建ChatGPT 聊天机器人的定制版本。该公司也带来了价格更低廉的软件付费选项，允许开发者通过微软购买 OpenAI 的编程工具。但无论走微软的渠道还是直接从 OpenAI 处购买，主机均由 Azure 提供。</p><p>&nbsp;</p><p>微软拥有 OpenAI GPT-4 大语言模型的独家许可，该模型能够根据几个提示词就生成与人类质量相当的输出。微软正推出多款基于 GPT-4 模型的产品，包括用于 Office 生产力应用订阅的 AI 插件、以及 Windows 11 中的智能助手。微软表示，其 Bing 搜索引擎凭借今年早些时候引入 Open AI GPT-4 支持的生成式 AI 功能，而在 2009 年首度亮相的十多年之后，终于在今年 3 月突破了 1 亿日活用户这一里程碑。</p><p>&nbsp;</p><p>Satya Nadella&nbsp;指出，使用 OpenAI 构建软件的开发者可以通过 Azure&nbsp;Marketplace“将产品快速投放市场”。这也是 Satya Nadella&nbsp;用于吸引大量开发者使用 Azure 的最新战略。2018 年，微软就曾斥资 75 亿美元收购 GitHub，而 GitHub 正是无数企业用于存储和共享代码的主力开发平台。Satya Nadella&nbsp;表示，微软将向所有与会者开放 GitHub Copilot 企业版，帮助开发人员高效补全源代码行。</p><p>&nbsp;</p><p>微软正努力发挥 OpenAI“大庄家”这一优势地位，希望让Azure成为更多开发者构建AI产品和服务的首选平台。Satya Nadella&nbsp;指出，“我们的使命是帮助世界上的每个人、每家组织取得更大的成就。对我来说，AI 必须通过赋能来发挥它的价值和作用。”</p><p>&nbsp;</p><p>Sam Altman 也从自己的角度努力推广微软产品，并为企业客户勾勒出了关于通用人工智能（AGI）的美好前景。Sam Altman 在发言中强调，“我认为我们与微软有着科技领域最好的合作伙伴关系，很高兴我们能够为 AGI 的实现而共同努力。”关于双方业务安排，Sam Altman 表示“我们双方之间已经建立起合作关系，我们乐于看到微软成功拿下一笔笔订单，微软也真心为 OpenAI 业务的快速发展喝彩。”</p><p>&nbsp;</p><p>根据公司发言人介绍，OpenAI 的首届现场活动吸引到约 900 名与会者。作为 ChatGPT 的缔造者，OpenAI 凭借这款 AI 驱动的聊天机器人在去年年底突然走红，更引发了全球对于生成式AI领域的大量投入。《华尔街日报》9 月曾报道称，OpenAI 正与投资者就出售股票事宜进行谈判，目前该公司的估值已经在 800 亿到 900 亿美元之间。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://openai.com/blog/introducing-gpts">https://openai.com/blog/introducing-gpts</a>"</p><p><a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">https://openai.com/blog/new-models-and-developer-products-announced-at-devday</a>"</p><p><a href="https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-openai-developer-conference">https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-openai-developer-conference</a>"</p><p><a href="https://www.cnbc.com/2023/11/06/microsoft-ceo-nadella-makes-surprise-appearance-at-openai-event.html">https://www.cnbc.com/2023/11/06/microsoft-ceo-nadella-makes-surprise-appearance-at-openai-event.html</a>"</p><p><a href="https://www.reddit.com/r/LocalLLaMA/comments/17p9mgc/openai_dev_day_discussion/">https://www.reddit.com/r/LocalLLaMA/comments/17p9mgc/openai_dev_day_discussion/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dcxN1gUJbMrBcc0d5K86</id>
            <title>火山引擎云计算解决方案负责人吴春龙确认出席 FCon，分享大规模混合部署基础设施探索与实践</title>
            <link>https://www.infoq.cn/article/dcxN1gUJbMrBcc0d5K86</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dcxN1gUJbMrBcc0d5K86</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 大规模混合部署基础设施探索与实践, 吴春龙, 云计算服务
<br>
<br>
总结: FCon 全球金融科技大会将在上海举行，吴春龙将发表题为《大规模混合部署基础设施探索与实践》的主题分享，介绍大规模基础设施的建设、组网和监控运维的实践与方法论。吴春龙是资深云计算专家，致力于为汽车、银行、手机厂商等公司提供安全、稳定、易用、高效的云计算服务。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎云计算解决方案负责人吴春龙将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5609?utm_source=infoqweb&amp;utm_medium=article">大规模混合部署基础设施探索与实践</a>"》主题分享，介绍大规模基础设施的建设、组网和监控运维的实践与方法论。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5609?utm_source=infoqweb&amp;utm_medium=article">吴春龙</a>"，资深云计算专家，致力于为汽车、银行、手机厂商等公司提供安全、稳定、易用、高效的云计算服务。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大规模混合部署基础设施探索与实践</p><p></p><p>在大模型时代，AI 智能生产力无处不在，将重新定义数字金融、保险行业，火山引擎提供大规模 AI 基础设施解方案，保障效率、稳定性的同时提供创新的生产力，加速金融行业智能化。</p><p></p><p>演讲提纲：</p><p></p><p>大模型基础设施挑战；大算力基础设施技术探索；大算力基础设施推荐实践；展望技术发展。</p><p></p><p>你将获得：</p><p></p><p>○ 了解大规模基础设施的建设的实践与方法论；</p><p>○ 了解大规模基础设施的组网的实践与方法论；</p><p>○ 了解大规模基础设施的监控运维的实践与方法论。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 8 折优惠 ，立省 ￥1360！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ife3NEEtk2yYPu7mIL1Q</id>
            <title>腾讯 AI Lab 专家研究员黄国平博士，确认担任 QCon LLM 推理加速和大规模服务专题出品人</title>
            <link>https://www.infoq.cn/article/Ife3NEEtk2yYPu7mIL1Q</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ife3NEEtk2yYPu7mIL1Q</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 08:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, LLM 推理加速和大规模服务, 黄国平博士, 交互翻译
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，黄国平博士将担任「LLM 推理加速和大规模服务」的专题出品人。在此次专题中，将介绍新技术和模型功能，以及如何上线进行服务等前沿话题。黄国平博士是腾讯 AI Lab 专家研究员，专注于交互翻译的研究与应用。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1106&amp;utm_content=huangguoping">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。腾讯 AI Lab 专家研究员黄国平博士将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1605?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1106&amp;utm_content=huangguoping">LLM 推理加速和大规模服务</a>"」的专题出品人。在此次专题中，你将了解到来自业内的专家关于新的技术和模型功能，以及如何上线进行服务等较为前沿的话题。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1605?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1106&amp;utm_content=huangguoping">黄国平博士</a>"，腾讯翻译负责人，腾讯 AI Lab 专家研究员。2008 级本科计算机科学与技术专业，后保研到中国科学院自动化研究所模式识别国家重点实验室硕博连读，师从宗成庆研究员，研究方向为机器翻译、自然语言处理。2017 年博士毕业至至今，在腾讯 AI Lab 长期专注于交互翻译的研究与应用。在 ACL、AAAI、IJCAI、EMNLP 等人工智能领域顶级会议与 TASLP 等顶级期刊发表论文 20 余篇。</p><p></p><p>相信黄国平博士的到来，可以帮助提升此专题的质量，让你学习到如何利用新的技术和模型功能来实现推理加速，如何上线进行服务（比如应对攻击和刁难）等关于 LLM 的相关热点，为探索新方向提供了更广阔的思路。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、</p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EUNuoKDME8GPIzhXLUu4</id>
            <title>vivo发布基于Rust语言的操作系统，全球首款？字节跳动宣布除夕统一放假；大妈招女婿要求大模型从业人员 | Q资讯</title>
            <link>https://www.infoq.cn/article/EUNuoKDME8GPIzhXLUu4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EUNuoKDME8GPIzhXLUu4</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 07:19:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度文心一言推出专业版, vivo自研智慧操作系统, 大妈招女婿要求, 马斯克的X公司估值降至190亿美元, 字节跳动宣布除夕统一放假, ChatGPT之父剑桥演讲遭抵制, 谷歌Assistant语音助手团队调整
<br>
<br>
总结: 百度推出文心一言专业版，vivo发布自研智慧操作系统，大妈招女婿要求阿里云从业人员，马斯克的X公司估值下降，字节跳动宣布除夕放假，ChatGPT之父剑桥演讲遭抵制，谷歌Assistant团队裁员。 </div>
                        <hr>
                    
                    <p></p><blockquote>百度文心一言推出专业版，定价为 59.9 元/月；vivo 将发布自研智慧操作系统：基于 Rust 语言编写，全球首款？大妈招女婿要求阿里云大模型从业人员，淘宝的不要；字节跳动宣布除夕统一放假：不占用年假额度；1688和闲鱼升级为淘天集团一级业务，负责人直接向戴珊汇报；世界首个开源贡献榜出炉......</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技企业</h2><p></p><p>&nbsp;</p><p></p><h4>百度文心一言推出专业版，定价为 59.9 元/月</h4><p></p><p>&nbsp;</p><p>百度上线文心一言专业版，单月购买定价为 59.9 元/月，连续包月优惠价 49.9 元/月。此前已经向用户开放的文心一言基础版，仍可免费使用。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eeb88e6a7dab7cfda92b9226a6b542ce.png" /></p><p></p><p>&nbsp;</p><p>据介绍，文心一言会员版基于文心大模型 4.0。基于 4.0 的专业版具备更强的模型能力和图片生成能力。支持各种插件，适合需要使用文心一言进行代码编程、文案撰写、绘画设计等专业工作需求的用户。</p><p>&nbsp;</p><p></p><h4>vivo 将发布自研智慧操作系统：基于 Rust 语言编写，全球首款？</h4><p></p><p>&nbsp;</p><p>11月1日，在 2023 vivo 开发者大会上，vivo 发布了自研操作系统蓝河 (BlueOS)。vivo 表示，蓝河操作系统采用 Rust 编写“系统框架”—— 从源头避免了内存使用不当引起的安全漏洞。据称是行业首家。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1b3e77af2ccfb0715aa0f581e701cec.png" /></p><p></p><p>&nbsp;</p><p>vivo 还称蓝河操作系统是面向通用人工智能时代的自研智慧操作系统：底层接入了 AI 大模型，支持基于自然交互方式的应用开发。此外，据称蓝河操作系统是基于 Linux/RTOS 的自研架构，因此不兼容 Android 应用。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>大妈招女婿要求阿里云大模型从业人员，淘宝的不要</h4><p></p><p>&nbsp;</p><p>10月31日，在杭州云栖大会现场，坊间传闻有热心人士举牌寻缘：独女，温柔萧山女孩，94年，事业编，体重51，家中有厂，家庭和睦身高1.63，接受姐弟恋。要求男孩96年-98年，在阿里从事大模型工作，算法&gt;后端&gt;前端，淘宝的不要、阿里云优先。不要彩礼，入赘送房一套，通勤车凯迪拉克一辆。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/288b60975ac5128e9a74a4ecb0e26821.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69a4dce3f76e63880823e7ed640571f4.jpeg" /></p><p></p><p>&nbsp;</p><p>虽然有人怀疑这只是一种营销手法，但这则花边新闻的热度丝毫未减。一些网友评论称，在他们的朋友圈里，技术PPT分享并不多，而招婿大妈的帖子却刷屏了……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>马斯克的X公司估值降至190亿美元，不到推特收购价一半</h4><p></p><p>&nbsp;</p><p>据知情人士称，马斯克的社交媒体公司X现在的估值已降至190亿美元，还不到马斯克一年前收购推特价格的一半。</p><p>&nbsp;</p><p>一位知情人士透露，X将以每股45美元的价格向员工发放限制性股票，对该公司的估值约190亿美元，相比马斯克一年前440亿美元的收购价格已缩水55%。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/j9PINcFAZy-m1CBlNi-wzA">疯狂马斯克的“极限”计划居然成功了？！“下云”后成本降低 60%，部分功能代码精简 90%，30 天急速迁移服务器</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>字节跳动宣布除夕统一放假：不占用年假额度</h4><p></p><p>&nbsp;</p><p>10月30日，网传字节跳动宣布除夕统一放假，其办公软件“飞书”日历上，已显示除夕当天为“春节团聚假”。当日，就此传闻求证字节跳动员工，获知该消息属实。前述字节跳动员工表示，公司有关除夕放假的公告发布于10月30日13时许，公告中还指出：“其中除夕当天为公司额外提供的春节团聚假，不占用年假额度。”</p><p>&nbsp;</p><p>10月25日，国务院办公厅发布关于2024年部分节假日安排的通知。其中，春节假期为2月10日至17日放假调休，共8天，2月4日（星期日）、2月18日（星期日）上班，即除夕不放假。通知中提出，鼓励各单位结合带薪年休假等制度落实，安排职工在除夕（2月9日）休息。</p><p>&nbsp;</p><p></p><h4>ChatGPT之父剑桥演讲遭抵制：抗议者挂横幅扔传单</h4><p></p><p>&nbsp;</p><p>当地时间11月1日，山姆·奥特曼代表OpenAI团队接受2023年霍金奖学金并演讲时遭到抵制。数名抗议者悬横幅、扔传单，引来观众嘘声，场面一度十分混乱。而在活动开始前，就有少数抗议者聚集在外面，举着标语要求停止AI竞赛。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/614cf1948a12948dffd018c93d8b3abf.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>谷歌Assistant语音助手团队调整，20多名数据科学家被裁</h4><p></p><p>&nbsp;</p><p>11月2日消息，报道称，据内部人士和一份记录公司内部裁员情况的员工文件透露，谷歌已经解雇了一些从事语音助手工作的员工。周一，一些从事 Assistant 项目的员工被告知他们的职位被取消了。据该员工文件称，被裁掉的员工有多达 20 名，他们都是数据科学家。该文件还指出，一些被裁员工上周五刚从 Bard 团队转移到 Assistant 团队。该文件是由员工编制的，汇总了在内部和外部发布的关于裁员的信息。“其中一名被裁员工正在休产假，另一名患有癌症，”文件写道。</p><p>&nbsp;</p><p>被裁员工有 60 天的时间在公司内部找到新的职位，否则他们将不得不离开。谷歌发言人在一份声明中表示：“为了优化我们团队结构，使其更符合我们最高优先级的目标，我们对团队进行了重组，以更好地支持我们的战略业务目标。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>1688和闲鱼升级为淘天集团一级业务，负责人直接向戴珊汇报</h4><p></p><p>&nbsp;</p><p>据晚点 LatePost消息，阿里旗下的国内批发电商平台1688和二手电商平台闲鱼的重要性提升，近期升级为淘天集团的一级业务。此前，1688 总裁余涌（花名：朴初）向淘天集团中小企业发展事业部负责人汪海（花名：七公）汇报，闲鱼总经理为丁健（花名：季山），向阿里妈妈负责人刘博（花名：家洛）汇报。业务升级后，余涌和丁健两位负责人将直接向淘天集团CEO戴珊汇报。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>全球通缉！苹果公司一中国籍女程序员醉驾保时捷致使他人死亡</h4><p></p><p>&nbsp;</p><p>上个月的9月30日，美国西雅图贝尔维尤发生了一起严重的车祸事件。事故中，一名在苹果公司工作的中国女程序员涉嫌醉酒和超速驾车，导致了这起严重车祸，副驾驶座上的中国男性乘客当场死亡。</p><p>&nbsp;</p><p>目前，西雅图警方已发布了对涉事女程序员的通缉令，并计划对她提出重罪指控。这起事故引起了广泛关注，据称该女子目前已经潜逃回国。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1afbb5fa6b7b2b271fd9bff80429ad37.jpeg" /></p><p></p><p>图源一亩三分地论坛</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT业界</h2><p></p><p>&nbsp;</p><p></p><h4>世界首个开源贡献榜出炉：中国排行第二，谷歌微软阿里巴巴等公司在列</h4><p></p><p>&nbsp;</p><p>10月31日消息，据<a href="https://mp.weixin.qq.com/s/rk8RYcJstd7KYmFYFdoPRA">国际测试委员会Bench Council官方公众号</a>"消息，Bench Council公布了“世界首个开源贡献榜”，号称“只以贡献分高下”。从 Bench Council 披露数据得知，Bench Council 号称邀请了多位独立科学家，从 20 世纪 60 年代至今的开源或对开源产生重要影响的成果中，遴选出了 145 项代表性成果，在确定主要贡献者的基础上产生了开源领域五十年人才榜、机构榜、国家榜。</p><p>&nbsp;</p><p>据悉，总共 264 人进入榜单，共有 24 位华人上榜；美国在国家榜上排行第一，中国排名第二。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f4ba91e10479a16480551dacddad3a5.png" /></p><p></p><p>&nbsp;</p><p>国内机构中，涛思数据因时序数据库TDengine上榜，百度因深度学习框架PaddlePaddle上榜，PingCAP因TiDB数据库上榜，中国科学院计算所和北京开源芯片研究院因开源RISC-V处理器香山系列上榜，阿里巴巴和平头哥半导体因开源RISC-V处理器玄铁系列上榜，中国科学院软件所因OpenBLAS线性代数库上榜，上海纽约大学因对深度学习框架MXNet的贡献上榜。</p><p>&nbsp;</p><p></p><h4>谷歌放弃了 Web Environment Integrity API</h4><p></p><p>&nbsp;</p><p>谷歌放弃了倍受争议的 Web Environment Integrity(WEI) API 提议。此前，该提议引发了广泛争议，主要浏览器开发商 Mozilla 等都表达了反对立场。谷歌官方博客表示，他们听取了社区的反馈，Chrome 团队现在不再考虑 WEI 提议，已经递交了 commit 从 Chromium 源代码中移除相关代码。但谷歌并没有完全放弃 WEI，而是决定从 Web 版退缩到 Android 版，宣布开发 Android WebView Media Integrity API，该 API 功能上与 WEI 类似，但针对的是嵌入在 Android 应用中的 WebViews。谷歌仍然要创造出一种 Android DRM API，这一次它不再需要考虑社区意见或反馈。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/02r_D7O2I9XtnV_v90aK7A">Web 开放性或遭重大打击！谷歌四名工程师推出 WEI 方案，可让广告拦截变成历史</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>Linux Kernel 6.6 正式发布</h4><p></p><p>&nbsp;</p><p>10 月 30 日消息，Linus Torvalds 宣布 Linux Kernel 6.6 正式推出，主要引入了几项新功能及驱动程序方面的更新。Linus Torvalds 在 6.6 版本更新公告中表示：“过去的一周非常平静，我没有任何借口再推迟 6.6 版本的发布，所以是时候发布了。除了对 r8152 驱动程序的一些较大修复外，其他都是小修小补。”</p><p>&nbsp;</p><p>Linux Kernel 6.6 主要引入了 EEVDF 调度器，实现了 Shadow Stack 的支持，为 Nouveau DRM 驱动程序添加了 Mesa NVK Vulkan 驱动程序所需的用户空间 API，继续支持即将到来的英特尔和 AMD 平台。</p><p>&nbsp;</p><p>更多信息请参阅 Linux 6.6 功能列表：</p><p><a href="https://lore.kernel.org/lkml/CAHk-=wiZuU984NWVgP4snp8sEt4Ux5Mp_pxAN5MNV9VpcGUo+A@mail.gmail.com/T/#u">https://lore.kernel.org/lkml/CAHk-=wiZuU984NWVgP4snp8sEt4Ux5Mp_pxAN5MNV9VpcGUo+A@mail.gmail.com/T/#u</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b</id>
            <title>李开复4个多月后“放大招”：对标OpenAI、谷歌，发布“全球最强”开源大模型</title>
            <link>https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3m7F87QpDVsu8zv68k1b</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 06:21:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 零一万物, Yi-34B, 大模型, 开源模型
<br>
<br>
总结: 由李开复打造的AI大模型创业公司"零一万物"发布了开源大模型Yi-34B，该模型经过340亿个参数训练，具有全球最强的通用能力、知识推理和阅读理解等多项指标。Yi-34B支持超长上下文窗口，能处理约40万汉字超长文本输入。零一万物通过优化计算能力和降低训练成本，致力于构建最先进的专有模型，以满足市场需求。 </div>
                        <hr>
                    
                    <p>今天，由李开复打造的 AI 大模型创业公司“零一万物”发布了一系列开源大模型：Yi-34B和Yi-6B。</p><p>&nbsp;</p><p>Yi-34B 是一个双语（英语和中文）基础模型，经过 340 亿个参数训练，明显小于 Falcon-180B 和 Meta LlaMa2-70B 等其他开放模型。在发布会中，李开复称其数据采集、算法研究、团队配置均为世界第一梯队，对标OpenAI、谷歌一线大厂，并抱有成为世界第一的初衷和决心。同时，他表示Yi-34B是“全球最强开源模型”，其通用能力、知识推理、阅读理解等多指标均处于全球榜单首位。</p><p>&nbsp;</p><p>零一万物团队也进行了一系列打榜测试，具体成绩包括：</p><p>&nbsp;</p><p>Hugging Face英文测试榜单，以70.72分数位列全球第一；以小博大，作为国产大模型碾压Llama-2 70B和Falcon-180B等一众大模型（参数量仅为后两者的1/2、1/5）；C-Eval中文能力排行榜位居第一，超越了全球所有开源模型；MMLU、BBH等八大综合能力表现全部胜出，Yi-34B在通用能力、知识推理、阅读理解等多项指标评比中“击败全球玩家”；......</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b319194b622b37e2630eb89146346ef7.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89024c95616f3552a4b3a6bce0a13f2e.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>然而，在发布中，也有一点需要指出，那就是Yi系列模型在GSM8k和MBPP的数学以及代码测评方面表现不如GPT模型出色。这是因为团队希望在预训练阶段希望先尽可能保留模型的通用能力，所以训练数据中没有加入过多数学和代码数据。后续他们计划在开源系列中推出专注于代码和数学领域的继续训练模型。</p><p>&nbsp;</p><p></p><h2>200K上下文窗口，能处理40万字文本</h2><p></p><p>&nbsp;</p><p>值得注意的是，此次开源的Yi-34B模型，将发布全球最长、可支持200K 超长上下文窗口（context window）版本，可以处理约40万汉字超长文本输入。这意味着Yi-34B不仅能提供更丰富的语义信息，理解超过1000页的PDF文档，还让很多依赖于向量数据库构建外部知识库的场景，都可以用上下文窗口来进行替代。</p><p>&nbsp;</p><p>相比之下，OpenAI的GPT-4上下文窗口只有32K，文字处理量约2.5万字。今年三月，硅谷知名 AI 2.0 创业公司Anthropic的Claude2-100K 将上下文窗口扩展到了100K规模，零一万物直接加倍，并且是第一家将超长上下文窗口在开源社区开放的大模型公司。</p><p>&nbsp;</p><p>在语言模型中，上下文窗口是大模型综合运算能力的金指标之一，对于理解和生成与特定上下文相关的文本至关重要，拥有更长窗口的语言模型可以处理更丰富的知识库信息，生成更连贯、准确的文本。</p><p>&nbsp;</p><p>此外，在文档摘要、基于文档的问答等下游任务中，长上下文的能力发挥着关键作用，行业应用场景广阔。在法律、财务、传媒、档案整理等诸多垂直场景里，更准确、更连贯、速度更快的长文本窗口功能，可以成为人们更可靠的AI助理，让生产力得到大幅提升。然而，受限于计算复杂度、数据完备度等问题，上下文窗口规模扩充从计算、内存和通信的角度存在各种挑战，因此大多数发布的大型语言模型仅支持几千tokens的上下文长度。为了解决这个限制，零一万物技术团队实施了一系列优化，包括：计算通信重叠、序列并行、通信压缩等。通过这些能力增强，实现了在大规模模型训练中近100倍的能力提升。</p><p>&nbsp;</p><p></p><h2>实现40%训练成本下降</h2><p></p><p>&nbsp;</p><p>AI Infra（AI Infrastructure &nbsp;人工智能基础架构技术）主要涵盖大模型训练和部署提供各种底层技术设施，包括处理器、操作系统、存储系统、网络基础设施、云计算平台等等，是模型训练背后极其关键的“保障技术”，这是大模型行业至今较少受到关注的硬技术领域。</p><p>&nbsp;</p><p>李开复曾经表示，“做过大模型Infra的人比做算法的人才更稀缺”，而超强的Infra 能力是大模型研发的核心护城河之一。在芯片、GPU等算力资源紧缺的当下，安全和稳定成为大模型训练的生命线。零一万物的 Infra 技术通过“高精度”系统、弹性训和接力训等全栈式解决方案，确保训练高效、安全地进行。</p><p>&nbsp;</p><p>凭借其强大的 AI Infra 支撑，零一万物团队表示，Yi-34B模型训练成本实测下降40%，实际训练完成达标时间与预测的时间误差不到一小时，进一步模拟上到千亿规模训练成本可下降多达50%。截至目前，零一万物Infra能力实现故障预测准确率超过90%，故障提前发现率达到99.9%，不需要人工参与的故障自愈率超过95%，有力保障了模型训练的顺畅进行。</p><p>&nbsp;</p><p></p><h2>零一万物背后</h2><p></p><p>&nbsp;</p><p>今年7月，李开复博士正式官宣并上线了其筹组的“AI 2.0”新公司：零一万物。此前李开复曾表示，AI大语言模型是中国不能错过的历史机遇，零一万物就是在今年3月下旬，由他亲自带队孵化的新品牌。</p><p>&nbsp;</p><p>在接受外媒采访时，他谈到了创办零一万物的动机：“我认为需求是创新之母，中国显然存在巨大的需求，”“与其他国际地区不同，中国无法访问OpenAI和谷歌，因为这两家公司没有在中国提供他们的产品。因此，我认为有很多人正在努力为市场创造解决方案。这是刚需。”</p><p>&nbsp;</p><p>众所周知，构建大模型是一项耗资巨大的事业。为了维持现金密集型业务，零一万物从一开始就制定了商业化计划。虽然该公司将继续开源其一些模型，但其目标是构建最先进的专有模型，作为各种商业产品的基础。</p><p>&nbsp;</p><p>李开复表示，他们非常清楚这些大型语言模型需要大量计算，花费巨大。“我们筹集到了大量资金，其中大部分都花在了 GPU 上。”与中国其他LLM玩家一样，零一万物也需要积极储备GPU以应对美国制裁。在发布会中，李开复表示零一万物现在的供应至少足以满足未来 12-18 个月的需求。</p><p>&nbsp;</p><p>美国的制裁也让中国企业注重优化计算能力，<a href="https://techcrunch.com/2023/11/05/valued-at-1b-kai-fu-lees-llm-startup-unveils-open-source-model/">李开复表示</a>"：“借助一支非常高质量的基础设施团队，每1000个GPU，我们也许能够从中挤出2000个GPU的工作负载。”</p><p>&nbsp;</p><p>从一些报道中，我们可以了解到，零一万物员工规模已超过100人，半数是来自国内外大厂的LLM专家。其中，零一万物技术副总裁及AI Alignment负责人是 Google Bard/Assistant 早期核心成员，主导或参与了从 Bert、LaMDA 到大模型在多轮对话、个人助理、AI Agent 等多个方向的研究和工程落地；首席架构师曾在Google Brain与Jeff Dean、Samy Bengio等合作，为TensorFlow的核心创始成员之一。</p><p>&nbsp;</p><p>零一万物的商业化之路很大程度上取决于其为其昂贵的AI模型找到适合的产品市场的能力。“中国在大模型方面并不领先于美国，但毫无疑问，中国可以构建比美国开发商更好的应用程序，这主要是因为过去 12 年左右建立的非凡的移动互联网生态系统，”李开复说道。</p><p>&nbsp;</p><p>李开复表示，这家初创公司的最终目标是成为一个外部开发人员可以轻松构建应用程序的生态系统。“我们的职责不仅仅是推出好的研究模型，更重要的是让应用程序开发变得容易，这样才能有优秀的应用程序，”他说。“归根结底。这是一场生态系统游戏。”&nbsp;</p><p>&nbsp;</p><p>开源地址：</p><p>Hugging Face：<a href="https://huggingface.co/01-ai/Yi-34B">https://huggingface.co/01-ai/Yi-34B</a>"；<a href="https://huggingface.co/01-ai/Yi-6B">https://huggingface.co/01-ai/Yi-6B</a>"</p><p>ModelScope：<a href="https://www.modelscope.cn/models/01ai/Yi-34B/summary">https://www.modelscope.cn/models/01ai/Yi-34B/summary</a>"； <a href="https://www.modelscope.cn/models/01ai/Yi-6B/summary">https://www.modelscope.cn/models/01ai/Yi-6B/summary</a>"</p><p>GitHub：<a href="https://github.com/01-ai/Yi">https://github.com/01-ai/Yi</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</id>
            <title>晋升神器：AI 一键生成 PPT，技术好的同时也做好PPT｜InfoQ 用户的双十一福利</title>
            <link>https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 04:49:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 升职加薪, 程序员, PPT, 爱设计 AiPPT
<br>
<br>
总结: 程序员晋升需要展示过去一年的成绩和未来规划，PPT的制作水平成为考验。爱设计AiPPT是一款智能的PPT生成工具，通过人工智能和自然语言处理技术，能够快速生成符合需求的PPT内容。它支持文档上传生成、在线自由编辑、云端存储和兼容.pptx格式等特点，让程序员能够通过高效、智能的PPT呈现自己的思维和观点，提升晋升机会。 </div>
                        <hr>
                    
                    <p>升职加薪自然是每位职场人都渴望的事情，程序员们也不例外。但是，晋升就需要汇报过去一年的主要成绩，并介绍自己接下来的具体规划，这时候就要考验各位的 PPT 水平了。过去几年，程序员的圈子里广泛流传着一句话：技术再好，不如 PPT 做得好！</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62558b508f91e83607cc773353ba54dd.jpeg" /></p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzODQ3NDQ4Mw%3D%3D&amp;chksm=e9398033de4e0925f3cb70287083d06fe9abae4e15a4999ad3e049039603fbaee9e26afe3ddf&amp;idx=1&amp;mid=2247484847&amp;scene=27&amp;sn=e7f72229a61027479fddf81828a9697d&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">PPT </a>"不仅仅是把想说的话呈现出来，最重要的其实是背后的抽象总结能力和逻辑思维能力。其次才是选择合适的框架将内容有序组织起来，最后才是美化。</p><p></p><p>一份制作良好的 PPT，不仅能够清晰地呈现你的思维和观点，更能够让你在众多竞争者中脱颖而出。但是，繁琐的制作过程，费时的排版，以及无法完全达到期望的效果，都是制作 PPT 时所面临的困扰。现在，爱设计 AiPPT 将为广大程序员解决这些问题，让大家在技术好的同时通过有效的内容呈现，获得晋升！</p><p></p><p></p><p></p><p>据悉，<a href="https://mp.weixin.qq.com/mp/wappoc_appmsgcaptcha?poc_token=HNhuSGWjyZFUhGN608tMnwAr-8P6v2nBNFTplteR&amp;target_url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU1NDA4NjU2MA%253D%253D%26chksm%3Dfbeb1b50cc9c9246033e7ad2c008c78f34fdb2d3ee0472f79890f09d9562a7b698ca46972ba1%26idx%3D2%26mid%3D2247574175%26scene%3D27%26sn%3D215f4d6a1899210cba77401d5274a635%26utm_campaign%3Dgeek_search%26utm_content%3Dgeek_search%26utm_medium%3Dgeek_search%26utm_source%3Dgeek_search%26utm_term%3Dgeek_search#wechat_redirect">爱设计</a>" AiPPT 是一款真正智能的 PPT 生成工具。它运用了人工智能技术与自然语言处理两项技术，能够智能理解用户输入的主题，并快速生成符合需求的 PPT 内容。无论是文字、图片、表格，还是图表，都能够以最短的时间为你呈现出一份专业而精美的 PPT。</p><p></p><p>具体来说，爱设计 AiPPT 具有以下特点：</p><p></p><p><a href="https://www.infoq.cn/article/understand-huawei-ai-strategy?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AI</a>" 一键智能生成。基于人工智能和自然语言处理技术，能智能分析用户输入的主题，并快速生成符合需求的 PPT 内容</p><p></p><p>2.文档上传生成。支持多种文件格式上传（doc、docx、xmind、mm），一键上传，AI 智能排版配色、快速生成 PPT</p><p></p><p>3.在线自由编辑器。支持一键整体更换模板、更换配色，内置上千套定制级 PPT 模板及超 10w+ 素材，只需拖拉拽即可快速修改</p><p></p><p>4.云端存储，跨设备同步。PPT 云端制作在线保存，无需下载，打开网站即随时随地开启创作和演示，跨设备不再是障碍</p><p></p><p>5.兼容.pptx 格式，支持源文件导出。支持 JPG、PNG、PDF、PPT 文件导出，PPT 源文件格式导入导出均无格式错乱问题</p><p></p><p>在流程上，用户只需要在 PC 端登录 aippt.cn，输入你的要求和目标，它就会自动生成脑图，帮助大家想清楚整体规划，这个脑图还可以直接增减修改，确定好内容结构大纲后，就可以自动生成对应的 PPT 文件，还支持更换 PPT 风格等。</p><p></p><p>值此双十一之际，InfoQ 为广大用户推出了专属福利，现在通过专属渠道（链接：<a href="https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong">https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong</a>"）购买爱设计 AiPPT 一年仅需 89 元！这是一个难得的机会，让你在享受高效、智能的 PPT 生成服务的同时，还能够节省大量的时间和精力。</p><p></p><p>与此同时，InfoQ 联合极客时间推出了《PPT 设计进阶 · 从基础操作到高级创意》的课程，帮助大家基于爱设计 AiPPT 完成 PPT 制作，课程 + 工具打包购买将享受更多优惠，仅需 129 元（链接：<a href="https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong">https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong</a>"）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/1c/d0/1c4173161ce3655a5b74a677b0ce16d0.jpg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</id>
            <title>字节宣布除夕放假、连放9天，不占年假；印度“IT业之父”要求年轻人每周工作70小时；Redis 创始人用 C 语言编写出最小聊天服务器｜AI一周资讯</title>
            <link>https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</guid>
            <pubDate></pubDate>
            <updated>Sun, 05 Nov 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 聊天服务器, Redis, 创始人, C语言编写
<br>
<br>
总结: Redis创始人Salvatore Sanfilippo使用C语言编写了一个核心代码仅300多行的聊天服务器项目Smallchat。这个项目是他给前端开发朋友的系统编程示例，实现了用户自定义昵称。
 </div>
                        <hr>
                    
                    <p></p><h2>资讯</h2><p></p><p></p><h4>Redis 创始人用 C 语言编写最小聊天服务器 Smallchat，核心代码仅 300 多行</h4><p></p><p></p><p>11 月 2 日消息，知名数据库缓存工具 Redis 的创始人 Salvatore Sanfilippo（网名 antirez）在 GitHub 上传了一个名为 Smallchat 的聊天服务器项目，用 C 语言编写了一个核心代码仅 300 多行的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ae812c0ab4c9598c5cb3122d70eebe7.png" /></p><p></p><p>在项目介绍中表示，这只是他给几个前端开发朋友的系统编程示例，尽自己所能写出来的最小聊天服务器，核心代码（不算空格和注释）仅有 200 多行，甚至实现了用户自定义昵称。</p><p></p><h4>大疆否认被罚60亿美元：案件仍在审理，未有更新的判决信息</h4><p></p><p></p><p>近日，网传美国将“大疆专利侵权”的处罚金额从此前的2.79亿美元上调至60亿美元（约440亿元人民币），对此大疆相关负责人回应称，该案件在今年四月陪审团的裁决金额是2.789亿美元，案子仍在审理过程中，截至目前并没有更新的判决信息。</p><p></p><p>据此前中证报报道，2021年，美国航空航天企业德事隆公司以侵权专利为由对大疆提起专利诉讼，要求大疆共支付3.67亿美元（约合25亿元人民币）的赔偿。今年4月份经过陪审团审理，大疆侵犯了美国无人机公司德事隆的两项美国专利，需分别赔偿3070万美元、2.482亿美元，合计2.789亿美元（约20亿元人民币）。</p><p></p><h4>vivo自研蓝河操作系统不兼容安卓应用，副总裁称AI大模型投入无上限</h4><p></p><p></p><p>11月1日消息，在2023 vivo开发者大会上，vivo自研蓝河操作系统 BlueOS 发布，将在vivo WATCH 3手表首发搭载。在11月1日下午的论坛上，vivo 副总裁周围在接受媒体采访时明确表示，vivo 自研蓝河操作系统不兼容安卓应用。从2023 vivo开发者大会获悉，蓝河操作系统目前已有支付宝、百度地图、喜马拉雅等 App 接入，并兼容 hapjs 快应用标准。</p><p></p><p>此外，vivo副总裁周围表示，vivo大模型现在每年20-30亿的投入成本，人才和设备各占一半，人才成本平均税后100万元。公司对大模型投入定义为高规格投入，目前没有设置上限。</p><p></p><h4>在线办公巨头WeWork将申请破产，估值曾达470亿美元</h4><p></p><p></p><p>据知情人士透露，在线办公巨头WeWork计划最早于下周申请破产。目前，WeWork正考虑在新泽西州根据《破产法》第十一章申请破产。此前，WeWork未能在10月2日向其债券持有人支付利息，然后获得了30天的宽限期。如果在宽限期内仍然无法支付利息，它将被视为违约。</p><p></p><p>WeWork周二表示，已与债券持有人达成协议，在触发违约之前，公司又获得了七天时间与利益相关方进行谈判。据此前报道，WeWork估值一度达到470亿美元。由于“共享办公”公司WeWork认股权证的交易价格“异常低”，纽约证券交易所已暂停其交易，并将启动将其退市的程序。</p><p></p><h4>Windows 11的市场份额已跃升至26%以上</h4><p></p><p></p><p>StatCounter 发布了月度报告，其中包含有关桌面操作系统、搜索引擎、浏览器等的最新数据。根据 2023 年 10 月的报告，Windows 11 的市场份额显著上升，从 9 月份的 23.64% 攀升至 2023 年 10 月的 26.14%。</p><p></p><p>考虑到自 2023 年 4 月以来，该操作系统的市场份额一直保持相对不变，因此本月看到了这款系统的影响力有了明显的增长。</p><p></p><p>虽然 Windows 11 的市场份额似乎并不令人印象深刻，尤其是在其继任者即将到来之际，但微软对其表现还是相当满意的。最近的一份新报告显示，Windows 11 的月活跃设备数已超过 4 亿，这一数字明显高于微软最初的预期。</p><p></p><p>尽管 Windows 11 在一个月内的数据提高了近 3 个百分点，但仍远低于 Windows 10，后者是数亿人的首选操作系统。StatCounter 表示，Windows 10 的用户数占比为 69.35%，上个月下降了 2.27 个百分点。</p><p></p><h4>英伟达发布大语言模型，辅助芯片设计工作</h4><p></p><p></p><p>近日，英伟达推出了自家最新 430 亿参数大语言模型 ——ChipNeMo。对于它的用途，英伟达在官方披露消息中也是非常的明确，剑指 AI 芯片设计。</p><p></p><p>具体而言，ChipNeMo 可以帮助工作人员完成与芯片设计相关的任务，包括回答有关芯片设计的一般问题、总结 bug 文档，以及为 EDA 工具编写脚本等等。</p><p></p><p>英伟达首席科学家 Bill Dally 对此表示：“以英伟达 H100 Tensor Core GPU 为例，它由数百亿个晶体管组成，在显微镜下看着就像是一个精心规划建设的城市一般”。</p><p></p><p>这些晶体管连接在比人类头发丝还细 10000 倍的“街道”上，需要多个工程团队协作两年多的时间来完成，其间繁琐且庞大的工作量，可见一斑。</p><p></p><h4>微软宣布与西门子联手，将Copilot生成式人工智能引入制造业</h4><p></p><p></p><p>微软（Microsoft）和西门子（Siemens）宣布，双方计划围绕生成式人工智能（AI）及其在全球工业领域的应用深化合作关系。此举有望彻底改变人机协作，两家公司将推出西门子工业副驾（Siemens Industrial Copilot），这是一款共同开发的人工智能助手，旨在提高制造业的生产率。</p><p></p><h2>IT业界热评新闻</h2><p></p><p></p><h4>印度“IT业之父”要求年轻人每周工作70小时：不要从西方学到坏习惯，不帮助国家发展</h4><p></p><p></p><p>10月30日报道，英国首相苏纳克的岳父、财富超40亿美元的“印度IT业之父”穆尔蒂在一条视频中表示，“不知为何，印度的年轻人从西方学到了坏习惯，不帮助国家发展。”穆尔蒂称，2075年印度有望成为“世界第二大经济体”。为与中国等国家竞争，印度需要”意志坚定、纪律严明、工作勤奋”的年轻人。</p><p></p><p>“我要求年轻人必须说，‘这是我的国家，我想每周工作70个小时’。”国际劳工组织数据显示印度是工时最长的国家之一，每人每周平均工作47.7小时。</p><p></p><h4>字节宣布除夕统一放假、连放9天，不占用年假</h4><p></p><p></p><p>10月30日，网传字节跳动宣布除夕统一放假，其办公软件“飞书”日历上，已显示除夕当天为“春节团聚假”。</p><p></p><p>当日，记者就此传闻求证字节跳动员工，获知该消息属实。前述字节跳动员工表示，公司有关除夕放假的公告发布于10月30日13时许，公告中还指出：“其中除夕当天为公司额外提供的春节团聚假，不占用年假额度。”</p><p></p><p>10月25日，国务院办公厅发布关于2024年部分节假日安排的通知。其中，春节假期为2月10日至17日放假调休，共8天，2月4日（星期日）、2月18日（星期日）上班，即除夕不放假。通知中提出，鼓励各单位结合带薪年休假等制度落实，安排职工在除夕（2月9日）休息。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</id>
            <title>字节跳动飞书技术 Leader 杨晶生，确认担任 QCon AI Agent 与行业融合应用的前景专题出品人</title>
            <link>https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 Nov 2023 07:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, AI Agent, 行业融合应用, 杨晶生
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，杨晶生将担任“AI Agent 与行业融合应用的前景”专题的出品人。在此次专题中，将介绍AI Agent的定义、应用以及与行业技术融合应用的发展前景。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。字节跳动飞书 技术 Leader 杨晶生将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">AI Agent 与行业融合应用的前景</a>"」的专题出品人。在此次专题中，你将了解到 AI Agent 是什么、AI Agent 的落地应用，以及与已有的行业技术融合应用的发展前景。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">杨晶生</a>"，目前就任于字节跳动，负责飞书音视频和人工智能相关产品的研发工作。曾就任于微软云计算部门和蜻蜓 FM。在十余年的工作中，经历了从服务器研发到云计算的过程，参与了超大规模全球化云服务架构、急速增长的内容平台业务、复杂而稳定性要求极高的企业服务等等项目，在服务高并发性能和稳定性方面有丰富的经验。</p><p></p><p>相信杨晶生的到来，可以帮助提升此专题的质量，让你学习到 AI Agent 能够感知环境、进行决策和执行动作，通常基于机器学习和人工智能技术，具备自主性和自适应性。同时，它已是公认大语言模型落地的有效方式之一，让更多人看清了大语言模型创业的方向，为未来技术融合应用提供了新思路。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</id>
            <title>火山引擎云安全解决方案负责人林扬确认出席FCon，分享金融企业如何构建安全云底座与合规能力？</title>
            <link>https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 林扬, 金融企业如何构建安全云底座与合规能力, 云原生安全
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，林扬将发表题为《金融企业如何构建安全云底座与合规能力？》的主题分享。他将介绍金融行业安全现状、挑战和发展趋势，以及云原生安全解决方案。此次会议将涉及云原生安全最佳实践、隐私和数据安全合规实践、业内安全助力业务增长的方法，以及大模型安全风险评估要点。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎云安全解决方案负责人林扬将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5607?utm_source=infoqweb&amp;utm_medium=article">金融企业如何构建安全云底座与合规能力？</a>"》主题分享，介绍金融行业安全现状，挑战和发展趋势、安全技术框架建议，以及相关案例。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5607?utm_source=infoqweb&amp;utm_medium=article">林扬</a>"，20 年的网络安全从业经验，具备甲乙方安全经历和视角，担任过甲方 CISO，曾担任传统安全公司和互联网云公司解决方案负责人，擅长数字化和上云转型的安全规划和实践，长期深入研究各行业安全需求和问题，是国内行业网络安全攻防推演的专家。他在本次会议的演讲内容如下：</p><p></p><p>演讲：金融企业如何构建安全云底座与合规能力？</p><p></p><p>金融行业信息化和数字化一直走在国内前沿，对于科技风险、安全合规、攻防能力方面要求高，挑战巨大，火山引擎基于自身安全和风控的最佳实践，结合金融行业业务特点，为行业定制适合的安全技术框架和解决方案，云原生安全解决金融行业云基础架构底座安全问题，隐私计算保障金融行业数据共享交换的安全合规和业务价值增长，大模型安全帮助金融行业发现模型安全问题，提供大模型底座和模型算法安全的保障措施。</p><p></p><p>演讲提纲：</p><p></p><p>金融行业安全现状，挑战和发展趋势；金融行业安全技术框架建议；云原生安全：构建金融行业稳定弹性的云安全底座；数据安全：降低合规风险；隐私计算：助力金融行业数据合规互联互通，提升营销增长；大模型安全，帮助金融行业大模型探索提供安全保障；云原生、数据安全成功案例分析。</p><p></p><p>你将获得：</p><p></p><p>○ 了解云原生安全最佳实践和关键点；</p><p>○ 隐私和数据安全合规实践和可落地做法；</p><p>○ 了解业内安全助力业务增长的方法；</p><p>○ 了解大模型安全风险，备案所需的安全评估要点。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 8 折优惠 ，立省 ￥1360！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</id>
            <title>揭秘阿里核心引擎，走近阿里巴巴开源自研搜索引擎 Havenask</title>
            <link>https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 09:18:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2023云栖大会, Havenask, 开源正式版本, 大规模分布式搜索引擎
<br>
<br>
总结: 2023云栖大会上，阿里巴巴发布了Havenask开源正式版本，Havenask是一款大规模分布式搜索引擎，主要用于智能搜索和海量数据实时检索。该搜索引擎在阿里巴巴内部的多个业务中得到广泛应用，如淘宝、天猫商品搜索，盒马搜索，菜鸟物流订单实时检索等。通过开源，团队希望能吸引更多开发者参与项目研发，共同创造更好的搜索引擎。 </div>
                        <hr>
                    
                    <p></p><h2>2023云栖大会，Havenask&nbsp;发布开源正式版本</h2><p></p><p>Havenask&nbsp;是阿里巴巴自主研发的大规模分布式搜索引擎，主要专注于智能搜索和海量数据实时检索，其核心能力广泛应用于阿里巴巴内部的众多业务，如淘宝、天猫商品搜索，盒马搜索，菜鸟物流订单实时检索等。</p><p></p><p>在11月1日云栖大会上，阿里巴巴智能引擎事业部云服务负责人&amp;&nbsp;Havenask&nbsp;开源项目负责人郭瑞杰博士，进行了&nbsp;Havenask&nbsp;开源正式版本发布演讲，并在演讲中介绍了&nbsp;Havenask&nbsp;最新开源进展与后续计划。</p><p></p><p>阿里巴巴智能引擎事业部高级技术专家徐希杰与魏子珺，则是在开源开放麦中为开发者详细阐述了&nbsp;Havenask&nbsp;正式版本的分布式能力细节与&nbsp;havenask-elasticsearch&nbsp;联邦项目最新进展，并结合各场景案例进行具体能力展示。</p><p></p><p>这是自在2022年云栖大会首发后，Havenask&nbsp;又一次亮相云栖，作为阿里巴巴自主研发的大规模分布式搜索引擎，Havenask&nbsp;承载着几代阿里搜索人的技术沉淀，团队期望在开源之后能有更多开发者加入项目研发中，一同联合共创。</p><p></p><p>本文将具体介绍&nbsp;Havenask&nbsp;的引擎架构、索引类型、查询语法、插件机制与运维管控能力，帮助广大开发者快速了解与上手。</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca1589c74e7f2a16d7763106e8953b51.jpeg" /></p><p>图1&nbsp;郭瑞杰博士在云栖大会《阿里云开源年度发布》中发布&nbsp;Havenask&nbsp;正式版</p><p></p><h2>Havenask&nbsp;简介</h2><p></p><p>Havenask&nbsp;底层全部采用&nbsp;C++&nbsp;实现，并经过多年的优化迭代，与其他的开源搜索引擎相比具有如下特点：</p><p></p><p>高性能：查询性能高，某些场景性能数倍于开源引擎。低成本：支持存算分离，冷热数据隔离等功能，海量数据场景下成本更低。时效性高：数据写入或者更新的时效性可以达到毫秒级。稳定性高：内存控制严格，没有其他开源引擎&nbsp;gc&nbsp;的问题，同时支持多机房互备具有更高的可靠性。索引类型丰富：支持&nbsp;kv、kkv、倒排、正排、摘要、向量多种索引类型。定制能力强：支持分词器、数据处理、query&nbsp;改写、算分、功能函数、等多种插件的定制。支持&nbsp;SQL&nbsp;语法：支持&nbsp;SQL&nbsp;查询，多表&nbsp;join，学习门槛低，业务迁移方便。</p><p></p><h2>Havenask&nbsp;架构</h2><p></p><p>Havenask&nbsp;引擎支持两种工作模式：读写分离模式（全量表模式）与读写统一模式（直写表模式），读写统一与读写分离的主要区别是是否有独立的索引构建服务，下面详细介绍。</p><p></p><h3>读写分离架构</h3><p></p><p><img src="https://static001.geekbang.org/infoq/33/33f354cadf938d9f305d845ffdf35a82.png" /></p><p>图2&nbsp;Havenask&nbsp;读写分离架构</p><p></p><p>Havenask&nbsp;读写分离架构如上图所示，在此模式下索引构建系统与在线检索系统是两个独立的子系统，可以分别进行资源的调整和集群的管理，索引构建系统和在线检索系统通过消息中间件和分布式文件系统进行索引的交互。读写分离架构适用于需要快速导入全量数据，需要定期进行索引重建，数据更新量较大，需要对离线资源进行独立控制等场景。</p><p></p><h4>索引构建系统</h4><p></p><p>索引构建系统是一个分布式的索引构建服务（build&nbsp;service，简称&nbsp;bs），每个索引构建服务都有一个或多个&nbsp;bs&nbsp;admin，bs&nbsp;admin&nbsp;负责管理表的索引构建流程，其中每个表可以有多个索引构建流程同时存在，相互之间无影响。</p><p></p><p>每个索引构建流程都由一组&nbsp;processor、builder、merger&nbsp;任务组成，其中&nbsp;processor&nbsp;负责原始数据的处理，比如分词，builder&nbsp;负责索引的构建，merger&nbsp;负责索引的整理。processor&nbsp;可以自由设置分片数，每个分片负责处理一部分数据，分片越多数据处理能力越强，使用的资源也越多。builder&nbsp;的分片数必须是索引表分片数的倍数，分片数越多索引构建越快。merger&nbsp;的分片数必须是索引表分片数的倍数，分片数越多索引整理越快。processor&nbsp;是一个常驻任务，builder&nbsp;和&nbsp;merger&nbsp;是交替执行的任务。</p><p></p><h4>索引构建流程</h4><p></p><p>读写分离架构下的表的数据索引构建由全量和实时两个阶段组成，同一个表可以同时存在多个索引构建流程，彼此之间无影响。全量阶段索引构建系统会将存在&nbsp;HDFS、MaxCompute&nbsp;或者&nbsp;OSS&nbsp;上的全量原始文件构建成全量索引，并订阅消息中间件回追一部分实时数据（防止全量切上线之后需要很长时间才能追上实时数据）。</p><p></p><p>索引构建过程中，processor&nbsp;会读取原始数据并对齐按照配置的数据处理规则进行处理，处理之后的数据会被发送到消息中间件（swift）。builder&nbsp;订阅消息中间件，将处理之后的文档构建成索引，并将索引产出到分布式文件系统。消息中间件中全量阶段处理的数据被&nbsp;builder&nbsp;全部处理完后，启动&nbsp;merger&nbsp;任务对索引进行整理，产出全量索引。</p><p></p><p>全量阶段完成后，全量索引会被切换到在线系统，同时索引构建也切换到增量阶段。增量阶段&nbsp;processor&nbsp;订阅消息中间件，处理实时数据，并将处理好的数据写回消息中间件。在线系统同时订阅消息中间件，获取处理之后的文件，直接在内存中构建成索引，这样数据就可以实时生效。builder&nbsp;也会订阅消息中间件，获取处理之后的文件，将其构建为增量索引，并由&nbsp;merger&nbsp;对全量索引和增量索引进行索引整理，产出最终可以切换上线的索引。增量索引切换上线之后，在线系统实时中的实时索引会被从内存中清理掉，释放的内存会被用于构建新的实时索引。</p><p></p><h4>在线系统</h4><p></p><p>在线系统用于加载索引并提供检索服务，它是一个支持多分片（shard），多备份（replica）部署的分布式服务，主要由&nbsp;admin，qrs（query&nbsp;result&nbsp;service）和&nbsp;searcher（数据节点）三种角色组成。Admin&nbsp;管理整个集群，实时监控各个节点的健康状态并调度，接收运维命令并向&nbsp;qrs&nbsp;和&nbsp;searcher&nbsp;下发运维指令。Qrs&nbsp;用于&nbsp;query&nbsp;的处理和结果的合并，qrs&nbsp;没有分片的概念，每个&nbsp;qrs&nbsp;节点都是同构的。Searcher&nbsp;节点加载索引，并真正执行查询任务，searcher&nbsp;节点上可以加载一个表的一个分片或者多个分片的数据。</p><p></p><p>在&nbsp;Havenask&nbsp;中用区间&nbsp;[0,&nbsp;65535]&nbsp;表示一份完整的数据，所有的数据分区键经过哈希之后都会映射到&nbsp;[0,&nbsp;65535]&nbsp;这个区间之内。在设置表的分片数之后，每个分片都会对应这个区间的一段范围，比如分片数为2，分成的两个分片对应的区间为&nbsp;[0,&nbsp;32767]&nbsp;和&nbsp;[32768,&nbsp;65535]。Havenask&nbsp;单分片最多可以承载21亿个文档，所以分片数越多，整个集群可以承载的数据量越多，最多支持65536个分片。</p><p></p><p>Qrs&nbsp;和&nbsp;searcher&nbsp;都可以通过扩充备份来提高整个集群每秒&nbsp;query&nbsp;处理能力（qps），不同的是&nbsp;qrs&nbsp;没有分片的概念，扩充备份就是扩&nbsp;qrs&nbsp;节点的个数，searcher&nbsp;是按照分片组织的，在每个&nbsp;searcher&nbsp;加载一个分片前提下，扩&nbsp;searcher&nbsp;的备份数就是扩分片数*备份数个&nbsp;searcher&nbsp;节点。</p><p></p><p>Havenask&nbsp;支持存算分离，searcher&nbsp;上的索引加载采用远端分布式文件系统、本地磁盘和内存三级索引加载策略，索引读取的性能也是按照远端、本地、内存这个顺序有数量级的提高。开发者在使用时可以综合考虑成本与查询耗时的要求，合理的配置索引加载策略。</p><p></p><h3>读写统一架构</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52f6f60c3dd206c897364edaf93fac75.png" /></p><p>图3&nbsp;Havenask&nbsp;读写统一架构</p><p></p><p>Havenask&nbsp;读写统一架构如上图所示，与读写分离架构相比有下面几个不同点：1）读写统一模式没有全量流程，所有的数据都要通过&nbsp;api&nbsp;以实时生效的方式推送到系统中；2）实时数据的推送直接写入到&nbsp;qrs，而不是&nbsp;swift&nbsp;中；3）每个分片对应&nbsp;searcher&nbsp;的&nbsp;leader&nbsp;进行索引整理，并将整理好的索引写入分布式系统中，follower&nbsp;加载整理好的索引。读写统一架构中使用&nbsp;swift&nbsp;进行&nbsp;wal&nbsp;以保证数据的安全，swift&nbsp;做&nbsp;wal&nbsp;的好处是不会随着&nbsp;searher&nbsp;备份的增多导致写的性能下降。读写统一架构比较适合频繁创建索引表、不需要全量数据导入、时效性要求高等场景。</p><p></p><h2>Havenask&nbsp;索引类型</h2><p></p><p>Havenask&nbsp;主要有三种类型的索引：倒排索引，正排索引与摘要索引。</p><p></p><h3>倒排索引</h3><p></p><p>倒排索引存储的是词（term）到包含词的文档（doc）的映射关系，可以通过倒排索引快速的检索到需要的候选文档，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b3effd4bff47aaec9fd247dc8d95758.png" /></p><p>图4&nbsp;Havenask&nbsp;倒排索引结构示意图</p><p></p><p>Havenask&nbsp;支持多种类型的倒排索引，比如主键索引，STRING&nbsp;索引（keyword，不分词），多字段&nbsp;PACK&nbsp;索引，数值范围索引，空间索引等，具体每个索引的含义开发者可以参考&nbsp;Havenask&nbsp;的用户手册。</p><p></p><p>向量索引（ANN）是一种特殊类型的倒排索引，它的索引结构与上图的通用倒排索引结构不同，具体的构建方式与索引结构和向量索引选择的算法有关。Havenask&nbsp;支持的向量索引有三种类型，线性索引、HNSW&nbsp;索引和聚类索引，可以满足不同场景下的向量检索需求。</p><p></p><h3>正排索引</h3><p></p><p>正排索引存储的是文档中字段的内容，主要用于对查询到的结果进行过滤、统计、排序等操作。Havenask&nbsp;的正排索引采用的是列存模式，即每个字段单独存储，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9837bb013557ea8cf81d2373021dc4f.png" /></p><p>图5&nbsp;Havenask&nbsp;正排索引结构示意图</p><p></p><h3>摘要索引</h3><p></p><p>摘要索引存储的是文档中需要返回的字段，可以对返回的字段进行飘红处理，如果字段内容较长可以使用摘要插件动态截取要返回的内容。Havenask&nbsp;的摘要索引采用的是行存模式，即一个文档的所有字段存在一起，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c6ff299924a36a3527d65e3b1533486.png" /></p><p>图6&nbsp;Havenask&nbsp;摘要索引结构示意图</p><p></p><h2>Havenask&nbsp;查询语法</h2><p></p><p>Havenask&nbsp;目前支持&nbsp;SQL&nbsp;查询语法，SQL&nbsp;语法简单易用并且便于扩展。但是具体到检索场景，SQL&nbsp;语法还是有些不足，比如查询时的多索引倒排搜索，粗排与精排的支持等。为了弥补搜索场景下&nbsp;SQL&nbsp;能力的不足，Havenask&nbsp;提供丰富的各种内置函数，比如提供了MATCHINDEX&nbsp;和&nbsp;QUERY&nbsp;等&nbsp;UDF&nbsp;支持倒排索引的查询，并支持自定义&nbsp;UDF。</p><p></p><h2>Havenask&nbsp;插件机制</h2><p></p><p>Havenask&nbsp;支持开发者自定义分析器插件、数据处理插件，各类&nbsp;UDF&nbsp;等以满足不同的业务需求。</p><p></p><p>分析器插件：开发者可以通过分析器插件定制自己的分析器，以满足不同的分词需求。分析器插件作用在数据处理分词阶段和查询时&nbsp;Query&nbsp;分词阶段。</p><p></p><p>数据处理插件：在构建索引之前，需要对文档进行处理（默认的是分词），开发者可以通过定制自己的数据处理插件提前对数据进行处理，比如可以集成一个向量化模型，在数据处理阶段将文本转为向量。</p><p></p><p>UDF：用户自定义函数，在查询时通过&nbsp;UDF&nbsp;可以定制自己的业务逻辑，比如外卖场景下，计算店铺和买家的距离。</p><p></p><p>对于各种定制插件，我们推荐开发者不要将插件代码单独编译成&nbsp;so&nbsp;的形式，而是与&nbsp;Havenask&nbsp;代码一起编成一个统一的&nbsp;binary，通过镜像的方式发布。</p><p></p><h2>Havenask&nbsp;运维管控</h2><p></p><p>Havenask&nbsp;的各个子系统（在线系统，索引构建系统，消息中间件）都有对应的&nbsp;admin&nbsp;角色进行集群的管理，每个&nbsp;admin&nbsp;都提供了运维管控的rpc接口。为了方便大家的使用，我们对这些&nbsp;rpc&nbsp;接口进行了封装，提供了方便使用的命令行工具&nbsp;hape。使用&nbsp;hape，开发者可以方便的启停集群，对表进行各种管理操作，如果需要更精细的运维控制，开发者可以直接调用&nbsp;admin&nbsp;提供的&nbsp;rpc&nbsp;接口。</p><p></p><h2>结语</h2><p></p><p>期望通过介绍，可以帮助开发者更好的了解与上手&nbsp;Havenask，我们欢迎广大开发者加入项目开发，共建高质量的搜索引擎。</p><p></p><p>此外，对于有使用需求的企业级开发者，我们也已在阿里云上提供了基于&nbsp;Havenask&nbsp;打造的全托管、免运维的一站式对话式搜索服务——阿里云&nbsp;OpenSearch，欢迎企业级开发者们试用体验。</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p>阿里云&nbsp;OpenSearch&nbsp;官网：<a href="https://www.aliyun.com/product/opensearch">https://www.aliyun.com/product/opensearch</a>"</p><p></p><p>欢迎钉钉扫码加入&nbsp;Havenask&nbsp;开源官方技术交流群：</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78c5cfa61c64a55cdeb0655ac7eb2849.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</id>
            <title>仅凭7页PPT拿下1亿美元融资、半年后估值超10亿！“欧洲OpenAI”杀疯了</title>
            <link>https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 初创公司, Mistral AI, 融资, 大语言模型
<br>
<br>
总结: Mistral AI 是一家AI初创公司，通过7页PPT成功融资1亿美元，目前正在寻求3亿美元的新融资。该公司专注于开发大语言模型和各类AI技术，旨在解决现实世界问题。最近，他们发布了号称是“最强7B开源模型”的Mistral 7B，该模型在各项基准测试中表现优秀。 </div>
                        <hr>
                    
                    <p></p><blockquote>这家成立 4 周时就能凭借 7 页 PPT 融到超 1 亿美元的 AI 初创公司，究竟是什么来头？</blockquote><p></p><p></p><h2>AI 初创公司 Mistral 正寻求 3 亿美元新融资</h2><p></p><p>&nbsp;</p><p>据外媒报道，生成式 AI 初创公司 Mistral AI（常自称为“欧洲 OpenAI”）目前正寻求 3 亿美元新融资。如果一切顺利，那么新融资将帮助这家年轻企业估值突破 10 亿美元大关。</p><p>&nbsp;</p><p>据了解，Mistral AI&nbsp;总部位于法国巴黎，由来自 Meta Platforms 和 Alphabet 的几位前研究人员 Arthur Mensch（现任 CEO）、Guillaume Lample 和 Timothee Lacroix 共同创立，公司成立于 2023 年 5 月，专门开发大语言模型及各类 AI 技术。Mistral 这个名号来自北方寒冷的季风，也体现了他们想要在 AI 领域占据一席之地的愿望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae7ee9ad22dc1fecd7c677caeb92b068.png" /></p><p></p><p>6 月，Mistral 在拿下 1.13 亿美元巨额种子融资后引发业界轰动，公司估值也瞬间来到 2.6 亿美元。彼时，该公司刚刚成立，员工仅 6 人，还未做出任何产品，仅仅凭借着&nbsp;7 页 PPT 就斩获了巨额融资。</p><p>&nbsp;</p><p>该轮融资由 Lightspeed Venture Partners 牵头，Redpoint、Index Ventures、Xavier Niel、德高控股以及意大利、德国、比利时和英国的其他知名风险投资公司参与。但该公司很快发现这“区区”1亿美元根本不够，要推动后续增长和扩张计划还需要更多资金的支持。</p><p>&nbsp;</p><p>据 The Information 近日报道，熟悉谈判内情的消息人士称，Mistral 正计划从投资者处额外筹集 3 亿美元，而此时距离由 Lightspeed Venture Partners 领投的种子轮融资才刚刚过去四个月。</p><p>&nbsp;</p><p>目前还不清楚 Mistral 已经与哪些风险投资商进行过通气，但根据另一位知情人士透露，生成式 AI 投资领域的重要参与者 Andreessen Horowitz 正在积极寻求向开源大语言模型（LLM）开发者注资的机会。如果能够顺利合作，自然不失为一件美事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/621d9243b7262fef809f470c83814e33.png" /></p><p></p><p>Mistral 公司CEO、前 DeepMind 研究科学家 Mensch 表示，这家企业的使命是“打造出能够解决现实世界问题的下一代 AI 系统”。他同时补充称，新一轮融资将用于扩大团队、加快研发工作，以及在欧洲和美国建立新的办事处。</p><p>&nbsp;</p><p>Mistral 敢于开出如此夸张的融资数额，也体现出投资者对于 AI 初创企业不断增长的关注和信心。近年来，AI 初创公司已经筹得海量资金，其中不少企业正在开发前沿 AI 技术，有望彻底颠覆众多传统行业。</p><p>&nbsp;</p><p>但目前 Mistral 仍在起步阶段，能否成为 AI 领域的主要参与者仍然有待观察。尽管如此，该公司强大的初始团队和雄心勃勃的发展目标，已经使其成为当前乃至未来几年中最值得关注的 AI 初创力量之一。</p><p></p><h2>“最强 7B 开源模型”Mistral 7B</h2><p></p><p>&nbsp;</p><p>9 月 27 日，Mistral AI 团队发布了自家首个大模型&nbsp;Mistral 7B，该模型号称是“最强 7B 开源模型”。</p><p>&nbsp;</p><p>据介绍，Mistral 7B 是一套拥有 73 亿参数的大语言模型，采用 Apache 2.0 许可证，以不加限制的方式对外开放以供使用。在所有基准测试中，Mistral 7B 均优于 Llama 2 13B；在多种基准测试中，优于 Llama 1 34B；拥有比肩 CodeLlama 7B 的编码性能，并同时保持着良好的英语能力；使用分组查询注意力（GQA）来加快推理速度；使用滑动窗口注意力（SWA）以较低成本处理更长序列。</p><p>&nbsp;</p><p>GitHub 链接：<a href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a>"HuggingFace 链接：<a href="https://huggingface.co/mistralai">https://huggingface.co/mistralai</a>"</p><p>&nbsp;</p><p>Mistral 7B基础设施集群由 CoreWeave 提供 24/7 全天候支持，CINECA/EuroHPC 团队及 Leonardo 运营团队提供资源与帮助，FlashAttention、vLLM、xFormers、Skypilot 维护团队提供新功能以及方案集成指导。HuggingFace、AWS、GCP、Azure ML 团队协助实现了 Mistral 7B 的全平台兼容。</p><p>&nbsp;</p><p>Mistral 7B 还能针对任意任务进行轻松微调。Mistral AI 团队将 Mistral 7B 与 Llama 2 系列模型进行了比较，并重新运行了这些模型以验证评估结论是否准确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f026b61c7d1d27868ee2ac3a51f7881.png" /></p><p></p><p>Mistral 7B 及各 Llama 模型在不同基准测试中的性能。这里列出的所有指标，均从&nbsp;Mistral AI 团队评估管道中的实际运行中采集而来，从而保证比较的真实性。Mistral 7B 在所有指标上均显著优于 Llama 2 13B，而且与 Llama 34B 基本相当（由于 Llama 2 34B 模型尚未发布，因此这里暂时与 Llama 34B 比较）。Mistral 7B 在编码与推理方面同样性能出众。</p><p>&nbsp;</p><p>本轮基准测试按主题可分为以下几类：</p><p>&nbsp;</p><p>常识推理: Hellaswag、Winogrande、PIQA、SIQA、OpenbookQA、ARC-Easy、ARCChallenge和CommonsenseQA&nbsp;的 0-shot 平均值;世界知识: NaturalQuestions&nbsp;和&nbsp;TriviaQA&nbsp;的 5-shot 平均值;阅读理解: BoolQ和&nbsp;QuAC&nbsp;的 0-shot 平均值;数学: mai@8的8-shot GSM8K 和 ma@4的4-shot MATH 的平均值;编码: 0-shot Humaneval&nbsp;和 3-shot MBPP 的平均值;热门聚合结果: 5-shot MMLU、3-shot BBH 和 3-5-shot AGI Eval (仅限英文多项选择题)。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0bf70ba717857e0b6dc38131fba14550.png" /></p><p></p><p>在对模型的成本/性能进行比较中，Mistral AI 团队提出了一个有趣的指标，即计算“等效模型大小”。在推理、理解与 STEM 推理（MMLU）方面，Mistral 7B 的性能与体量达到其 3 倍以上的 Llama 2 模型相当，意味着它能显著节约内存容量和数据吞吐量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7d7373bf51fa51eb28f95fde31b38da.png" /></p><p></p><p>Mistral 7B 和 Llama 2（7B/13B/70B）的 MMLU 常识推理、世界知识与阅读理解比较结果。Mistral 7B 在绝大多数评估中均显著优于 Llama 2 13B，仅在知识基准测试中与后者处于同一水平（这可能是由于参数规模有限，因此掌握的知识量不足）。</p><p>&nbsp;</p><p>注意：此次评估与 Llama 2 论文之间存在以下区别：</p><p>&nbsp;</p><p>在 MBPP 测试中，这里使用了手工验证的子集。在 TriviaQA 测试中，这里未提供维基百科上下文。</p><p>&nbsp;</p><p>此外，Mistral 7B 使用滑动窗口注意力（SWA）机制，即每个层都关注之前的 4096 个隐藏状态。这里做出的主要改进以及尝试改进的原因，来自 O(sliding_window.seq_len) 的线性计算成本。具体来讲，在对 FlashAttention 和 xFormers 做出改进之后，成功在 16k 序列长度和 4k 上下文窗口下实现了速度倍增。Tri Dao 和 Daniel Haziza 为相关调整做出了贡献。</p><p>&nbsp;</p><p>滑动窗口注意力的原理，是利用&nbsp;Transformer&nbsp;的堆叠层来关注此前超出窗口大小的情形：第 k 层的 token&nbsp;i 关注第 k-1 层的 token&nbsp;[i-sliding_window, i]，后者又关注 [i-2*sliding_window, i]。如此一来，较高层就能访问到距离更“久远”的过往信息。</p><p><img src="https://static001.geekbang.org/infoq/12/12fa10bb65ec4544160b5d63edb4eca1.png" /></p><p></p><p>总之，采取固定注意力范围的最大意义，就是使用轮换缓冲区将缓存限制为&nbsp;sliding_window&nbsp;token 的大小（更多细节请查看参考实现<a href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a>"）。如此一来，同样在执行 8192 序列长度的推理时，可以节约下 50% 的高速缓存容量且不会影响模型质量。</p><p>&nbsp;</p><p>为了展示 Mistral 7B 模型的泛化能力，研究团队使用 HuggingFace 上的公开指令数据集对其进行了微调。不用问题集“作弊”、也不涉及专有数据，由此产生的 Mistral 7B Instruct 模型在 MT-Bench 测试中获得了优于一切同体量 7B 模型的性能，表现可与 13B 聊天模型相比肩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/36c664d9c775b51093068b1f94782de0.png" /></p><p></p><p>快速演示的 Mistral 7B Instruct 模型能够轻松微调，进而带来引人注目的卓越性能。其中不涉及任何协调机制。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theinformation.com/articles/mistral-a-wannabe-openai-of-europe-seeks-300-million">https://www.theinformation.com/articles/mistral-a-wannabe-openai-of-europe-seeks-300-million</a>"</p><p><a href="https://techstartups.com/2023/10/31/mistral-a-generative-ai-startup-aiming-to-be-europes-openai-seeks-300-million-in-new-funding/">https://techstartups.com/2023/10/31/mistral-a-generative-ai-startup-aiming-to-be-europes-openai-seeks-300-million-in-new-funding/</a>"</p><p><a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</id>
            <title>中国电子学会主办 第四届 ATEC 科技精英赛报名启动</title>
            <link>https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 06:49:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ATEC科技精英赛, 大模型应用与安全, 人工智能, 网络安全
<br>
<br>
总结: 中国电子学会主办的第四届ATEC科技精英赛（ATEC2023）已启动报名。该赛事旨在推动新一代人工智能发展和国家网络空间安全战略，培养人工智能及网络安全领域人才。本届赛事的主题是大模型应用与安全，旨在解决大模型技术的可用性、安全性和有效性等问题。赛事将围绕真实场景命题，培养青年科技人才的综合能力。赛事将于11月30日开始，共设4个赛道，奖金池共计146万元。 </div>
                        <hr>
                    
                    <p>11 月 1 日由中国电子学会主办的第四届 ATEC 科技精英赛（ATEC2023）正式启动报名。</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/9I7jYdgQNrr0zBTXyyYV?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">ATEC</a>" 科技精英赛是主要面向中国籍计算机等专业在校学生、人工智能及网络安全行业研究者和从业者的一场高水平的智能科技挑战赛，意在贯彻落实党中央、国务院关于推动新一代人工智能发展的决策部署以及全面贯彻国家网络空间安全战略，构筑我国人工智能及网络安全发展的先发优势，推动人工智能及网络安全领域人才培养。</p><p>&nbsp;</p><p>本届大赛的主题为<a href="https://www.infoq.cn/article/Wigm8Jk2atzDYgo61JJC?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大模型应用与安全</a>"。中国工程院院士、清华大学<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbe9a327cc9e2a31ca257bd26d377944080205ce1c81ba8c3714add4012efa5d07b1819982b3&amp;idx=3&amp;mid=2247490280&amp;scene=27&amp;sn=555602ee062f2055e8039a6289ed9cd7&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">人工智能</a>"研究院院长张尧学将担任 ATEC2023 指导委员会主席。52位来自境内外高校计算机领域的知名学者参与评审。ATEC 前沿科技探索社区作为赛事的承办单位。社区联合发起单位清华大学、浙江大学、西安交通大学、上海交通大学、蚂蚁集团，将联合北京大学、新加坡南洋理工大学等 13 所知名高校共同承担大赛命题、组织保障等相关工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f90de9bc661be51213a6348812b72b46.png" /></p><p></p><p>人工智能大模型无疑已成为当下计算机领域最受关注的热点技术问题。但如今，大模型及相关技术的发展尚处于初期阶段，仍面临技术短板、隐私安全等问题。在大模型的落地应用过程中，如何平衡前沿技术探索和工业应用之间的隔阂，成为现实挑战。一方面，大模型开发、训练、运营等成本耗资巨大，有效提升资源利用率，成为大模型应用普及的关键；另一方面，在一些大模型的实际使用过程中，已被发现生成内容存在质量不佳、冗余回答等问题，甚至可能产生输出危害内容的问题，有效风险管理成为大模型合理合法应用的前提。</p><p>&nbsp;</p><p>ATEC2023 赛事将围绕大模型应用落地过程中的“可用性”、“安全性”、“有效性”等维度进行赛题设计，针对老年人科技服务等多个真实场景命题，探索大模型应用过程中的创新思路及解法。ATEC 前沿科技探索计划发起人、清华大学计算机系副系主任徐恪指出，“从预训练语料的安全标准，到内容产出物的安全评估，大模型不仅要服务用户，更要安全地服务好用户。重视和研究大模型在应用层面的安全可用问题，是大模型技术落地应用的必由之路。”</p><p>&nbsp;</p><p>作为业内实战型技术人才培养的旗帜性赛事。每一届 ATEC 科技精英赛都以还原真实工业场景、遴选具有社会价值的技术命题为出发点，遵循“以赛育人”的宗旨。ATEC2023 组织委员会主席、中国电子学会副秘书长曹学勤表示，“科技赛事是科技人才培养的有效途径，每年中国电子学会都将针对不同的技术方向组织各类技术赛事。大模型技术是未来计算机技术发展的关键。大模型技术的应用落地，依赖于人工智能、网络安全等多个技术领域的突破发展。借助大模型主题赛事及围绕真实场景的命题，能够很好地培养青年科技人才的综合能力。”</p><p>&nbsp;</p><p>据悉，本届赛事将于 11 月 30 日正式开始开赛。线上赛共设置 4 大赛道，涵盖大模型的知识引入、工具学习、AI 生成新闻检测、网络安全大模型等考点。大赛全程设有 3 大奖金池，共计奖金 146 万元，用以选拔及表彰领域内的优秀人才。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</id>
            <title>腾讯信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人徐羽，确认担任 QCon GenAI 和通用大模型应用探索专题出品人</title>
            <link>https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 徐羽, LLM, AI
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，徐羽将担任"GenAI 和通用大模型应用探索"专题的出品人。在此次专题中，将介绍LLM的最新进展、类型、能力、局限性和未来发展趋势。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。腾讯信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人徐羽将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">GenAI 和通用大模型应用探索</a>"」的专题出品人。在此次专题中，你将了解到在 LLM 正在迅速发展背景下的的最新进展，以及 LLM 的类型、能力、局限性和未来的发展趋势。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">徐羽</a>"，2009 年加入腾讯，现任信息平台与服务线 CTO 兼总经理、PCG 推荐 AI 中台负责人。硕士毕业于加拿大滑铁卢大学电子与计算机工程系，加入腾讯前在加拿大黑莓公司工作 6 年，参与 BIS 手机邮件研发工作。从 2009 年开始负责手机 QQ 浏览器从 0 起步到现在亿级 DAU 规模的研发工作，在 2018 年建立和负责 PCG 的推荐 AI 中台，在机器学习平台、NLP、CV 视频理解、推荐算法和推荐架构等方面带领团队支持 QQ 浏览器和 PCG 业务的 AI 落地应用。</p><p></p><p>相信徐羽的到来，可以帮助提升此专题的质量，让你学习到，LLM 研究和开发的最新进展，了解不同类型的 LLM、其能力和局限性，以及 LLM 对未来工作、教育、医疗保健和许多其他领域的潜在影响。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</id>
            <title>AIGC 编程：代码编程模型的应用与挑战</title>
            <link>https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 07:50:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 大型模型, 编码助手, 低代码平台
<br>
<br>
总结: 本文讨论了网易在生成式AI领域的应用，包括编码助手和低代码平台。大型模型帮助程序员编写代码是一项有价值的技术，但从商业角度来看并不一定有利可图。网易通过自研的方式，利用大型模型来更好地适应自身需求和场景，提供程序员更好的使用体验。在优化方面，网易注重将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识，并通过优化提示工程提供更有信息量的上下文，以产生对程序员有价值的输出。 </div>
                        <hr>
                    
                    <p>嘉宾 | 鱼哲、刘东</p><p>编辑 | Tina</p><p>&nbsp;</p><p>生成式AI已经成为软件行业的一个重要推动力。在过去的一年里，包括网易在内的许多公司都在积极探索如何将这项技术应用到他们的产品中。如今，网易已经推出了多个生成式AI的实际应用产品，包括编码助手、大数据分析产品和低代码平台。在最新一期的“极客有约”对话节目中，鱼哲与网易杭州研究院人工智能部的算法负责人刘东一同探讨了有关大型模型产品的成本、速度和精确度等关键问题。</p><p></p><p>本文经编辑，原视频地址：https://www.infoq.cn/video/Wu1iSPABRu9NTVRrrywi</p><p>&nbsp;</p><p>亮点：</p><p>大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。微调只是为了有针对性地增强它，使其更好地满足用户指令，所以首先需要基准模型能支持该任务。我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们引入了一个"可信AI"的概念，包括过程可验证、用户可干预和产品可运营三个方向。从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。</p><p>&nbsp;</p><p>嘉宾简介：</p><p>鱼哲，Lepton AI 创始团队成员，产品负责人。</p><p>刘东，网易杭州研究院人工智能专家，AI算法团队及产品团队负责人，专注于前沿算法研究与商业化应用。相关技术成果曾获浙江省科技进步奖一等奖。</p><p>&nbsp;</p><p></p><h4>AIGC在软件工程领域的应用方向</h4><p></p><p>&nbsp;</p><p>鱼哲：我个人认为Codex和Copilot等工具具有广阔前景，而AIGC也在广泛推广。刘东老师，您能否简要介绍一下，网易杭州研究院在AIGC技术以及软件工程领域的技术研究方向有哪些？</p><p>&nbsp;</p><p>刘东：关注AIGC，特别是在软件研发领域，我们认为它在各个环节都有实际价值。例如，在需求分析和设计方面，大型模型已经能够提供出色的设计建议。在编码和开发阶段，Copilot已经非常成熟，可以显著提高程序员的效率。在代码调试、分析和优化阶段，大型模型也能提供有益的优化建议，包括检测潜在的错误。甚至在测试阶段，我们也尝试使用大型模型生成测试案例。在运维环节，例如线上日志的实时分析和监控，也可以受益于大型模型的能力，提高效率。</p><p>&nbsp;</p><p>从应用的角度来看，我们目前在编码和开发阶段最快地实现了AIGC的应用。特别是我们内部为研发同事提供了类似Copilot的工具，已经看到效率有所提升。此外，我们还开发了一些外部商业化产品，如在BI产品中引入了对话功能，推出了ChatBI产品，以及在低代码产品中使用大型模型来加速低代码开发效率。</p><p>&nbsp;</p><p>鱼哲：关于Copilot，我之前在GitHub上也尝试过，还尝试过Code Llama。我想代表我们的观众逐一提出一些问题。首先，所有程序员都非常熟悉的一个问题是，我们花费很多时间思考如何分解代码的功能实现，以及编写和调试代码。特别是编写单元测试，在后端开发中经常需要，虽然我们不会讨厌，但有时候确实不太愿意做这项工作。</p><p>&nbsp;</p><p>然而，AI辅助编程作为一个产品，我看到市场上已经有很多竞品，比如GitHub的Copilot、OpenAI的Codex，以及AWS的Code Whisper等。有很多成熟的产品存在。我想问一下，为什么网易杭州研究院选择在这个领域开展工作？此外，微软在收购GitHub后表示支持Copilot业务，但据报道，这一业务目前亏本，因为Copilot对于开发者来说价格相对较高。微软在财报中也提到，他们每月支持一个开发者需要100美元，但实际只收取20美元。在这种情况下，您如何看待网易在这一领域的角色和作用？</p><p>&nbsp;</p><p>刘东：我们考虑这个问题时，有几个方面的考虑。首先，从安全的角度看，每家企业都有一些核心代码不愿意与外部共享，因此希望能够拥有相对可控的服务，并在其中使用大型模型以提高程序员的效率。因此，就可控性和安全性而言，自研可能是一个较好的选择。</p><p>&nbsp;</p><p>其次，每家企业都有大量的特定代码积累，而如何有效地利用这些代码，以在其业务中发挥价值，这也是一个重要问题。像Copilot这样的云服务通常比较通用，很难让企业将其自有代码集成进去并进行优化，以适应企业自身的习惯。</p><p>&nbsp;</p><p>因此，我们通过自研的方式，利用自己的模型来更好地适应网易的需求和场景，以提供程序员更好的使用体验。这是我们的出发点。</p><p>&nbsp;</p><p>至于亏本的问题，我也认为大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。因为在这一领域，客户通常比较价格敏感，即使收费较低，用户也可能觉得价格昂贵。但从成本角度来看，确实需要较大的投入。因此，我们的考虑是综合各方因素来实施这项工作。</p><p>&nbsp;</p><p>鱼哲：除了安全性问题，您在网易情况可能会注意到一些特别适合的场景，不管是在电商还是游戏等许多场景中。您能否举一个具体的例子，展示我们在什么情境下通过自研的Copilot项目，更好地支持业务方编写其业务代码的案例呢？</p><p>&nbsp;</p><p>刘东：我们进行了大量的定制和优化，比如在游戏运营场景中，游戏经常需要举办各种活动。这些活动的方案通常由策划部门提出，要求程序员按照方案进行实现，但实现后可能代码只用一次，然后就不再使用。这种场景非常常见。</p><p>&nbsp;</p><p>但是，由于许多游戏之间存在相似性，企业的代码库中可能有很多人写过类似的代码，具有很大的参考价值。在这种情况下，如果使用通用的Copilot，它通常无法了解企业专有的代码信息，因此在这种场景下提供的提示效果可能不够理想。但如果我们进行企业定制，就可以通过一些增强的方式，将这些信息集成到提示中，然后将其提供给大型模型，使其能够参考这些代码来提供更好的提示。这样，我们就可以更好地实现降本增效。</p><p>&nbsp;</p><p>鱼哲：您刚才提到了在业务场景中进行了许多优化，特别是在模型层面。您能详细介绍一下优化工作是在模型层面进行的，还是在输送给模型的提示工程这一层进行的？或者说，在模型的不同方面都进行了优化，可以谈谈具体的优化思路是什么吗？</p><p>&nbsp;</p><p>刘东：我们的优化思路主要围绕两个关键点展开，以发挥大型模型的价值。第一个关键点是确保模型本身的强大性，这涉及将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识。</p><p>&nbsp;</p><p>第二个关键点是优化提示工程，即我们如何提供给模型更有信息量的上下文，以便模型更好地理解上下文，产生对程序员有价值的输出。我们发现，仅仅将当前代码片段的上文或下文提供给模型并让其继续生成，效果通常一般。因此，我们考虑了编程过程中的各种信息来源，包括引入的外部第三方库、工程中的其他项目文件、类似的工程项目，甚至程序员在编程过程中浏览和检索网页、查找答案以及执行粘贴和复制等操作。这些行为都是宝贵的提示信息，我们通过将这些信息融入到模型的提示中，帮助模型更好地理解当前的上下文，从而更好地输出对程序员有价值的信息。这些工作使我们的模型能够更好地与业务结合，提供更好的效果。</p><p>&nbsp;</p><p>鱼哲：您提到了模型微调，确实，在Google和其他地方，人们一直在进行对模型的微调。通过微调一个基础模型，将其完全适配到特定任务，这是一个非常有效的方法。许多人认为，只要有基本模型，然后进行一些微调，就可以将其应用到任何任务上，使其成为该任务的专家。您对这个问题是怎么看的？</p><p>&nbsp;</p><p>刘东：如果我要进行微调，内部除了我们自己训练的基础模型，还有许多开源的基准模型可供使用。我们进行了大量的评估，具体思路是，如果要进行微调，首先要分析基准模型是否足够强大，是否在具体任务上已经表现得相当不错，微调只是为了有针对性地增强它，使其更好地满足用户指令。如果基准模型根本不支持该任务，仍然强行进行微调的话，效果可能不太好，或者可能需要寻找一种成本更高的微调方式，类似于继续进行预训练，以将知识融入模型，然后再进行微调，例如LORA微调。我认为LORA微调可能只对现有的基准模型进行提升和补充有意义。当然，如果基准模型本来就不太好，那么可能不会获得太大的收益，或者预期的性能可能不会特别出色。</p><p>&nbsp;</p><p></p><h4>成本问题</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到了成本问题，确实，在特别是语言模型（LM）这个领域，模型的推理成本随着参数数量的增加呈指数级增长。我们了解到，网易内部使用Copilot不仅仅涉及生成代码，还可能涉及解释代码推理方面。用户可能以多种不同的方式使用它。您是使用一个巨大的模型或者一个具有固定参数的模型来支持所有使用方式，还是根据不同的使用方式智能地调整背后模型的大小呢？</p><p>&nbsp;</p><p>刘东：我们选择了后者的方式，即根据不同的使用方式智能地调整背后模型的大小，这是有充分考虑的。从成本和效率的角度考虑，这是一个综合的决策。特别是在编程场景中，代码提示是一个非常高频的任务，因为每输入几个字母，都会触发一次提示请求。在这种情况下，模型需要足够快，因为如果太慢，程序员可能会自己完成输入，这样就不会提供太多价值。</p><p>&nbsp;</p><p>另一方面，这个场景通常涉及到代码生成，相对来说是一个相对固定且不太复杂的任务，与通用任务相比，难度较低。因此，我们更倾向于选择规模较小的模型，以确保效率，并且不会明显降低质量。</p><p>&nbsp;</p><p>对于像代码解释、调试分析或注释生成这样的任务，难度较大，可能需要更大的模型才能实现良好的效果。但好的一点是，这些任务通常不会有太高的使用频率，因此在这种情况下，我们可以选择相对更大的模型，而不需要进行大规模的冗余部署，因为请求量本来就不会太大。这种综合考虑帮助我们更好地控制了成本。</p><p>&nbsp;</p><p>鱼哲：随着大型语言模型的发展，特别是像Copilot这样的方式，例如像LLAMA这种模型，我们是否仍然需要手动编写注释呢？大家讨厌别人不写注释，但自己也不喜欢写。</p><p>&nbsp;</p><p>刘东：写注释与不写注释在很大程度上是一个习惯问题。写注释的主要目的首先是为了给自己提供提示，使代码更容易理解和维护。其次，注释也有助于他人理解代码，尽管注释的覆盖度要求可能并不高，因为大模型可以帮助填充一些细节。然而，写注释的程度可以因程序员而异，有些程序员可能倾向于写详尽的注释，解释每个细节，而有些人可能只写简要的概述性注释。这与个体的写作风格和代码质量意识有关。</p><p>&nbsp;</p><p></p><h4>BI产品和低代码平台</h4><p></p><p>&nbsp;</p><p>鱼哲：在网易杭州研究院，我们不仅在内部广泛应用这些先进的技术，还在一些领域提供对外的技术支持和合作机会。在对外方面有哪些技术合作呢？</p><p>&nbsp;</p><p>刘东：除了为内部提供技术支持，我们还将这些大语言模型的能力整合到商业化产品中，以为客户提供更多的服务。其中，我们的代表性产品之一是BI产品。通过整合大语言模型的能力，为BI产品引入了自然语言交互功能，使用户可以通过自然语言查询所需的数据和报表，这完全是由大语言模型驱动的。</p><p>&nbsp;</p><p>另一个重点领域是低代码平台，我们的CodeWave平台，它通过低代码编程方式，降低了编程的门槛，提高了编程的效率，从而帮助企业节省成本并提高效率。在这个平台中，我们引入了大语言模型的能力，以提高效率和降低编程门槛。这两个领域是我们当前主要投入和发展的方向。</p><p>&nbsp;</p><p>鱼哲：我们还有一个低代码产品，可以介绍下这个产品的使用体验吗？</p><p>&nbsp;</p><p>刘东：Low code 不同于 0 code，简而言之，它是一种基于可拖放的方式进行软件开发的方法。它不要求专业的程序员从头编写代码，也不同于完全无需编码的 0 code 方式。在低代码中，你可能需要配置一些固定的模板，定义数据模型，设计流程结构，还可以使用预定义的组件，通过拖拽的方式连接各种逻辑，最终生成软件产品。</p><p>&nbsp;</p><p>这种方法的核心优势在于相对于传统的完全编码软件开发，用户需求较低，无需像计算机专业的本科毕业生或有丰富经验的人才那样写代码。但与 0 code方法相比，它仍然保持了软件开发的灵活性，因为它可以实现复杂的逻辑。低代码的定位介于传统软件开发和 0 code 之间，兼顾了易用性，同时也能满足一些较为复杂的软件开发需求。</p><p>&nbsp;</p><p>鱼哲：您能简单介绍下这个产品的对外发布节奏吗？我看到官网上有些资料相关。</p><p>&nbsp;</p><p>刘东：我们目前在网易数帆官方网站上提供了一些基础材料和介绍。此外，我们即将在11月2日举行2023网易数字+大会，届时将提供更详细的产品介绍以及有关技术的分享。我们期待在发布会上与大家分享更多信息。</p><p>&nbsp;</p><p></p><h4>AIGC在数据分析应用上的挑战</h4><p></p><p>&nbsp;</p><p>鱼哲：回到刚才提到的ChatBI，我在以前做业务时常常需要与BI同事沟通，例如我想了解最近三个月华北地区哪个行业的客户增长最快，哪个行业的客户有一些困难，以及他们所遇到的产品使用情况。这种情况通常需要等待一两天的时间，不管是BI同事还是我自己去做，都需要花费大量的时间来查看数据地图，查看每个表的结构以及做相关的SQL查询，因为我们需要定义特定的指标，例如复购、沉默和活跃等。这是一个非常复杂的问题，之前尝试了许多模型，但它们存在幻觉的问题，导致了一些错误的结果，这是不能接受的。在BI领域，这个问题是非常严肃的，我们不能容忍存在幻觉的问题。我想了解一下，你们是如何处理这个问题，如何解决这种复杂性挑战的。</p><p>&nbsp;</p><p>刘东：我们面对的确实是一个巨大的挑战，而且我们在这个产品上花费了很长时间，因为BI场景是非常严肃的，它的任务是提供准确的信息。如果我们只是编造数据或者输出不可信的信息，那这个任务基本上就失败了。因此，我们采取了多重方法来尽量避免这个问题。</p><p>&nbsp;</p><p>首先，我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们收集了大量的数据和各行各业的常见数据报表，通过数据增强和训练，使模型的能力更强。这个领域子模型专注于解决数据分析场景，能够通过自然语言输入生成高质量的SQL查询语句。</p><p>&nbsp;</p><p>其次，尽管模型很强大，但我们也意识到大型生成式模型不是100%可控的，因此我们在产品层面进行了多方面的工作。我们引入了一个"可信AI"的概念，包括过程可验证、用户可干预和产品可运营三个方向。</p><p>&nbsp;</p><p>过程可验证：我们不仅仅相信模型生成的SQL查询语句，而是使用一个查询语句解析引擎将其解析为人类可理解的语言，以确保用户了解模型的工作原理。如果发现错误，用户可以立即识别并不信任结果。用户可干预：我们允许用户对模型生成的查询进行干预。用户可以更改条件、操作等，以纠正错误或调整查询。这提供了用户对结果的额外控制。产品可运营：我们希望产品不仅仅是一个静态的工具，而是能够随着用户的使用变得更智能。我们收集用户的行为习惯，正例和反例，不断优化模型。我们也提供产品配置，以使模型理解各行各业的“黑话”和简称。通过不断的运营，使模型越来越智能，适应用户的需求。</p><p>&nbsp;</p><p>这些方法的结合，以及其他细节的优化，使我们的产品更可信、可控，提高了用户的工作效率。</p><p>&nbsp;</p><p>鱼哲：这是否意味着当用户使用产品时，他们需要在某种程度上提前注入表结构的一些信息？或者说，模型能够根据表的结构自行猜测字段的含义？</p><p>&nbsp;</p><p>刘东：大模型是通过自主猜测的。只要提供底层表结构，大模型可以自动获取这些信息，所以用户在一开始使用时通常不需要太多干预。</p><p>&nbsp;</p><p>鱼哲：您刚刚提到的这个反馈收集非常有趣，因为通过良好的RLHF方法，模型的性能可以显著提高。</p><p>&nbsp;</p><p>刘东：是的，必须逐步将其系统运营，使其随着使用而不断智能化，而不是采取一劳永逸的方式。这样做的话，问题通常不会被永久解决。但一旦将其运营起来，将负面案例的反馈馈送给它，它就会不断改进。</p><p>&nbsp;</p><p>鱼哲：刚才有个直播观众的提问：“对于这些垂直领域的模型，你们是在基础大模型的基础上进行微调，还是持续进行预训练，或者是从零开始使用领域样本训练参数较小的模型？”</p><p>&nbsp;</p><p>刘东：我们通常是基于基础的基座模型进行调整。网易内部我们已经进行了基础模型的玉言，这是一个从头开始训练的基座模型。从头开始训练的好处是，我们大致了解未来要覆盖的领域，因此在训练过程中，我们有意地将一些领域相关的数据融入其中。例如，如果要处理编程任务，就会注重将代码相关的数据纳入模型。如果要处理SQL的任务，就会加入一些SQL的数据。这个基座模型相对来说比较通用。然后，我们会在这个基础上为每个领域创建领域特定的子模型，以进行适配。</p><p>&nbsp;</p><p></p><h4>数据的重要性</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到的ChatBI的问题，如果拥有一个基础模型并为其创建功能，同时提供大量数据时，我发现在这个工作中，最大的挑战实际上不在于微调模型，而是在于找到合适的数据，并将其准备成可供模型使用的形式。我认为这是最困难的部分。</p><p>&nbsp;</p><p>在研究一篇论文时，我注意到在数据稀缺的情况下，他们提出了一个新名词叫RLAIF，即通过人工智能来生成强化学习所需的数据，以支持强化学习任务。</p><p>&nbsp;</p><p>对于像ChatBI这样的项目，我认为您需要大量的数据来对基础模型进行调整，而且需要具备高度的语义和推理能力。模型的规模不会小，而随着模型规模的增加，调整参数需要更多的精力、计算资源和数据。</p><p>&nbsp;</p><p>我想了解一下，您是如何在数据准备方面处理这些挑战的？因为实际情况是，在这类项目启动之初，数据通常不够整洁，或者很多人最初并不清楚这些数据可能会有哪些用途。您是如何处理这一问题的呢？</p><p>&nbsp;</p><p>刘东：无论是在ChatBI领域还是在以前的代码自动补全项目中，数据准备工作都是至关重要的，也是相当具有挑战性的任务。我们投入了大量精力来应对这个挑战。</p><p>&nbsp;</p><p>在ChatBI项目，我们获得数据的途径多种多样。首先，我们会在网上寻找一些开源数据。在这个领域，因为传统方法已经发展了相当长的时间，所以存在许多开源的评测数据，以及公开数据表结构的定义。我们可以利用这些表结构，以人工智能的方式自动生成问题和答案，从而使用AI来生成数据，这是一种当前相对流行的方法。此外，我们还投入了大量人力资源来进行数据的搜集和标注工作，将各个来源的数据汇总，综合使用，以满足我们的需求。</p><p>&nbsp;</p><p>鱼哲：我觉得这个趋势在从事NLP领域的同事中也非常明显。人们开始广泛使用语言模型来执行以前需要使用多个专门的小模型来完成的任务。举例来说，以前我们需要训练专门的模型来执行诸如语音到文字转换、地址解析以及标点符号分割等任务。而现在，像您刚才提到的，在生成数据方面也使用了语言模型。这引发了一个问题，即您是否认为大型语言模型会逐渐取代NLP领域中使用的多个小型专家模型呢？</p><p>&nbsp;</p><p>刘东：从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。这意味着，尽管它们非常有能力，但我们不必在每个场景中都采用大语言模型。例如，对于一些小型NLP任务，我们可以使用较小的模型，它们成本低廉，在线上表现良好，同时可以满足高并发需求，因此在这些情况下，不必迫切转向大型语言模型。</p><p>&nbsp;</p><p>当然，大语言模型的能力毋庸置疑，它们在某些复杂任务上可能表现更出色。然而，我认为大语言模型与领域专家模型之间不是相互替代的关系，而是可以共存的。在不同的情境中，可以选择使用不同的模型，以便最好地满足特定需求。这种差异化的方法可能是更好的选择。</p><p>&nbsp;</p><p></p><h4>模型选择的问题</h4><p></p><p>&nbsp;</p><p>鱼哲：因为我之前有时也会采取简便的方式，直接使用大型模型，特别是在拥有免费积分的情况下。但随着时间的推移，我发现在考虑长期投资回报时，仍需要寻找传统的常规模型。</p><p>&nbsp;</p><p>刘东：在ChatBI中，除了大型模型之外，有时需要将小型模型和大型模型结合使用。这是因为尽管大型模型拥有出色的能力，但它在成本和执行速度上可能存在一些问题。小型模型则执行速度非常快。在某些任务中，如果你不断地调用大型模型来执行和解析，可能会影响用户体验，因此需要进行综合考虑。</p><p>&nbsp;</p><p>鱼哲：您刚刚也提到了，一方面，生代码生成式大模型或ChatBI生成式大模型等，都在数据收集方面面临巨大挑战。我认为数据是其中的一个技术挑战。除了数据之外，在这个过程中还有哪些方面您认为非常具有挑战性，非常难的呢？</p><p>&nbsp;</p><p>刘东：我认为数据确实是一个巨大的挑战，不仅在收集方面，还在清洗数据方面需要耗费大量精力。清洗数据是非常关键的，因为如果不做好，会直接影响模型的效果。我们在数据清洗方面投入了大量精力，因为高质量的数据是确保模型效果的基本保障，这是第一个挑战。</p><p>&nbsp;</p><p>另一个挑战是一旦大模型的能力达到足够强，如何在实际业务场景中找到合适的应用场景，确保它能够创造价值。这方面也非常具有挑战性，因为大模型面临效率、速度和成本等问题。虽然它在很多场景下效果出色，但用户体验可能难以保证。此外，大模型作为生成模型，不可能百分之百准确。如何找到那些既能容忍错误，同时又能为用户带来实际帮助的场景，让模型成功落地，也是一个非常大的挑战。</p><p>&nbsp;</p><p>总之，技术本身的能力与业务场景的结合是非常关键的，只有找到合适的结合点，大模型的能力才能真正发挥作用，用户才能真正感受到其价值。否则，它将一直停留在演示的层面，其技术的价值和影响力都会受到限制。</p><p>&nbsp;</p><p>鱼哲：有时候出现了上下文的误解。例如，用户可能要求搜索最近三个月内是否有玩过某个游戏，但可能会被错误地理解为最近三年。在ChatBI场景中，我们可能有一个SQL生成工具，但它生成的SQL语句缺少一个关键的"where"子句。现在，关于ChatBI，是它能够接受用户的自然语言查询并自动触发查询任务，还是它只返回SQL代码，用户需要将SQL代码用于传统的数据仓库查询窗口中查询？</p><p>&nbsp;</p><p>刘东：我们的当前设计是完全自动的。当用户提供一个查询后，系统会立即执行，而且像之前提到的那样，各种AI操作都会在执行后进行解释，并展示各种条件。用户可以根据查询结果和这些解释来判断查询的可靠性。</p><p>&nbsp;</p><p></p><h4>大模型产品价值的体现</h4><p></p><p>&nbsp;</p><p>鱼哲：所以用户不仅可以看到生成的代码，还能了解为什么会生成这段代码。此外，生成的数据会以表格的形式展示，用户可以导出数据。产品是否还提供可视化或建模分析能力，还是用户需要自己去处理这些方面的工作？</p><p>&nbsp;</p><p>刘东：我们目前主要提供可视化展示，对于后续的建模分析，我们正在进行研究和探索。</p><p>&nbsp;</p><p>鱼哲：您之前提到技术研究院需要同时具备技术和业务的理解，而最终需要为其结果负责。客户，无论是网易集团内部还是外部，都渴望了解这些生产力工具如何提升效率。这包括自动代码生成、SQL自动生成以及低代码平台等技术，它们都旨在提高生产力。然而，生产力提升在实际中往往是一个具有挑战性的问题。难以证明这些技术是否可以直接提高生产力。关于如何证明生产力提升的问题，您是怎么解决的？</p><p>&nbsp;</p><p>刘东：我们一直在思考和探索这个问题，因为要传达技术的价值，需要从多个角度来考虑如何证明其价值。我们非常关注用户的反馈和实际使用数据，这对于衡量技术的有效性至关重要。</p><p>&nbsp;</p><p>在我们内部使用低代码工具，例如代码补全工具，类似于Copilot工具，我们详细记录用户的使用情况，特别关注用户采纳提示的比例以及AI自动生成代码的比例。这有助于我们了解技术是否真正帮助用户减少编码工作，还是只是一个演示性的工具，用户不太愿意采纳其中的建议。</p><p>&nbsp;</p><p>同样，对于ChatBI和低代码工具，我们也密切关注用户的使用情况。例如，如果没有Chat功能，很多业务人员可能无法自行使用BI工具来查询数据。但如果引入Chat功能，我们关心是否有人在使用，以及他们的使用频率。在低代码工具方面，我们使用自然语言来生成逻辑，然后观察生成情况和占比。这些数据帮助我们衡量技术是否真正提高了生产力，帮助用户在成本降低和效率提高方面取得进展。我们非常关注这些方面的数据。</p><p>&nbsp;</p><p>鱼哲：当我们致力于提高生产力效率时，如AIGC或大型模型的出现与以往的机械发明有很大不同。以前的机械或半自动机械本质上需要人的操作。例如，使用除草器可能需要有人操作设备，而自动化机械则需要人的干预。然而，像您刚才提到的ChatBI，如果今天我可以使用自然语言描述业务需求，然后它可以为我生成正确的SQL查询并检索数据。那么对于那些传统的数据分析专业人员或BI同行，他们的存在可能会面临一些挑战，因为这种技术的出现可能改变了传统的数据分析方法。</p><p>&nbsp;</p><p>刘东：我认为这些工具主要是作为一种助力的角色存在的，而人的价值仍然不可或缺。无论是在BI领域还是在编码领域，它们的核心目的是帮助人们在一些简单重复的工作中提高效率。当拥有这种提高效率的工具时，我认为人们可以解放更多的时间，用于思考业务等更有价值的事情。这包括如何改进业务、更好地理解用户需求，以及如何提供更出色的软件产品。这是一种从不同角度思考问题的方式，而不是完全取代人的角色。因为这些技术目前正在逐渐发展，它们还没有达到100%可信并且能够胜任一切的状态。</p><p>&nbsp;</p><p>鱼哲：我听闻有些公司在内部开展了类似工作遇到了许多障碍。其中一部分障碍来自于开发团队，他们担心这种工具可能会与他们的工作发生冲突，或者一旦工具成熟，公司将不再需要他们。在你们尝试推广这些工具的过程中，是否也遇到了类似的挑战？</p><p>&nbsp;</p><p>刘东：我们目前并没有遇到这类挑战，因为我们进行了一些统计，发现从软件研发的角度来看，程序员实际花在编码上的时间并不占很大比例。编码只是他们工作的一小部分，更多的工作包括需求分析、整体设计以及与其他方面的对接等。编码所占的时间并不是很多。我们的目标并不是取代程序员，而是让他们能够更多地投入需求分析和用户场景理解，以便提高编码的质量和整个软件产品的效果。</p><p>&nbsp;</p><p></p><h4>大模型产品的私有化部署</h4><p></p><p>&nbsp;</p><p>鱼哲：我觉得程序员这个称呼有点狭隘，因为在编写代码时，实际上他们不仅仅在写代码，更多的时候在进行工程工作，也就是做工程师的事情。工程师通常需要将来自外部的各种复杂难以理解的需求和分散的模块整合到一起。在这方面，生成式模型的能力是不可替代的，不论是在短期还是长期。我认为很难通过直接应用一个大型模型来完成需要花费多年时间理解和深入了解的业务。这种情况下，使用生成式模型可能不太适用，因为你需要时间来积累对业务的深刻理解，然后才能进行创新性的工程工作。</p><p>&nbsp;</p><p>你刚刚描述的这些能力听起来确实非常有趣和具有吸引力。然而，我们也明白，许多公司，包括像网易自己开发Copilot时，通常出于安全和特殊场景的考虑，不愿意使用市场上通用的产品。这种担忧在中国和美国的科技公司中都非常普遍，特别是当公司规模较大时，它们通常更倾向于采用私有化部署，无论是在公司自己的数据中心还是在云上的IDC。</p><p>&nbsp;</p><p>在传统金融领域，如银行、保险和证券等领域，这些能力可以显著提高工作效率和效能。然而，由于国家监管要求或公司性质的原因，很多银行和保险公司通常需要确保技术提供的方式支持私有化部署。我们会积极考虑这些需求，以满足不同客户的特殊要求。网易在这方面是如何考虑的？</p><p>&nbsp;</p><p>刘东：我们的模型都是基于自己的基座模型调用的，因此完全可控。我们也提供了私有化部署的能力。除了不断优化模型性能，我们还专门组建了一个工程团队，专注于推理效率的优化、部署方案的设计以及各种硬件适配工作。</p><p>&nbsp;</p><p>在私有化部署方面，我们考虑如何尽量降低用户的成本，因为大型模型的成本相当高。首先，我们根据业务场景找到了适合的模型规模，而不是盲目地追求巨大的规模，这样可以减少硬件集群的复杂性。</p><p>&nbsp;</p><p>其次，我们进行了大量的工程优化。这包括引入业界开源的先进技术，以提高性能。我们还根据模型的特点进行了自定义适配，包括自定义内核等，以提高吞吐量和效率。</p><p>&nbsp;</p><p>此外，我们还考虑了量化加速等操作，以确保资源的可控性。因此，即使使用普通的显卡，我们也可以将大型模型部署上，并为用户提供良好的体验。这些措施都有助于提供高效的私有化部署解决方案。</p><p>&nbsp;</p><p>鱼哲：在私有化部署方面，你们是可以进行多方面的性能优化的吧？比如模型量化、压缩以及重新编写一些核心代码，从而提高性能和效率。在私有化部署的时候，一种方式是用户直接使用基座模型，这是一个即插即用的解决方案。另一种是用户在使用一段时间后，需要进行自定义微调，比如强化学习微调（RLHF）或自适应微调（RLAIF）等，你们是如何解决的？</p><p>&nbsp;</p><p>刘东：我们提供两种服务模式，以满足用户的需求。首先，我们提供一种自助工具模式，类似于业界已有的微调工具和强化学习工具。这些工具包括数据集管理、标注、训练任务和部署任务等功能。用户如果拥有足够的实力和理解相关流程，可以自行使用这些工具来满足其需求。用户可以随时尝试不同的方法，进行A/B测试，以确定哪种方式效果更佳。</p><p>&nbsp;</p><p>其次，对于一些重要客户，我们也提供定制化的服务。在这种情况下，我们的算法专家会与客户合作，共同解决他们特定的问题。我们会与客户密切合作，确保他们获得最佳的解决方案。无论用户自行使用工具还是选择我们的定制化服务，我们都将竭诚为他们提供支持。</p><p>&nbsp;</p><p>鱼哲：我也认为在私有化部署后再进行模型训练是一项颇具挑战的任务。从前我在这个领域有一段时间的从业经验，我深知这种工作需要投入大量时间和精力。此外，私有化部署环境通常存在标准不一致的问题，因此需要耗费额外的时间来解决各种复杂情况。</p><p>&nbsp;</p><p>刘东：每个客户的环境都有所不同，因此我们的目标是将产品尽量标准化，同时确保工具功能完善。这种方法有助于客户自行进行调整、训练和部署，降低了使用成本和门槛。只要他们理解这个过程，几乎都可以通过简单的点击鼠标来完成，而不需要深入编程或处理复杂的问题，这对用户来说更加容易接受和理解。</p><p>&nbsp;</p><p>鱼哲：在应用 AIGC 技术能力来提高软件工程效率时，您认为在业务端的落地过程中，最关键的角色通常会是什么？</p><p>&nbsp;</p><p>刘东：我认为在这个过程的每个环节都非常关键，没有哪一个环节可以忽略。首先，理解场景和找到使大模型落地的有价值的点至关重要。然后，需要探索这些点，以确定大模型的能力是否足够可控和可解决。如果算法可以解决问题，那么从工程角度来看，是否有足够的投资回报、性价比和用户体验也是非常重要的。最后，如何将这一切标准化地交付给用户，确保他们能够持续使用，而不仅仅是一个演示，也是一个巨大的挑战。每个环节都需要表现出色，才能成功完成这项工作。</p><p>&nbsp;</p><p></p><h4>对软件工程未来的看法</h4><p></p><p>&nbsp;</p><p>鱼哲：您怎么看待编程以及软件工程的未来？</p><p>&nbsp;</p><p>刘东：我认为AI和AIGC技术并不是要取代软件工程，而是要为软件工程提供更强的支持。随着数据积累、通信和计算能力的提升，对软件工程的要求变得越来越高，而AI技术可以提高软件工程的生产力。未来，软件工程师的角色可能会发生变化，从以程序员为主导变为人机协作的模式，工程师需要花更多精力学习和应用AI技术，以提高工作效率和生成更好的软件。</p><p>&nbsp;</p><p>鱼哲：这实际上涉及到一个重要的哲学性问题，即通用性与特殊性之间的平衡。在优化时，我们必须在通用性和特殊性之间做出权衡。因为优化通常是为了特定场景和硬件而进行的，它可能会牺牲通用性。您认为，在未来多久内，我们是否会看到通过代码生成或自动化方式，针对特定硬件环境进行优化呢？比如剪枝、量化、压缩和算子的自动生成。</p><p>&nbsp;</p><p>刘东：实现这一愿景需要大量的基础积累和技术成熟度。目前的代码生成技术主要依赖于已有的能力和沉淀的代码，然后通过大型模型的学习和自动生成来解决更多问题。这种自动化生成代码的技术在广泛的硬件和环境中得到应用，可能需要更多的积累和实践。</p><p>&nbsp;</p><p>对于新的硬件和场景，手动优化仍然是必要的，因为了解硬件和业务逻辑、分析性能问题等需要人的专业知识。自动化优化工具需要基于已有的知识和经验，而不是凭空生成优化方案。因此，即使技术不断发展，仍然需要工程师的专业知识来指导和验证自动生成的代码。未来，随着技术的发展和积累，可能会有更多的通用性优化工具出现，但在新的硬件和场景中，人工干预和专业知识仍然是至关重要的。这是一个逐步演进的过程。</p><p>&nbsp;</p><p>鱼哲：这是一个非常有趣的问题，因为实际上几乎所有的模型在实际应用中都需要经过优化才能顺利落地。在许多情况下，尤其是当涉及硬件时，例如服务器端或者嵌入式设备，优化工作变得尤为重要。</p><p>&nbsp;</p><p>举一个例子，最近非常受欢迎的 Vox 模型，它能够根据自然语言指令为机器人生成指令。然而，在将这一模型应用到嵌入式设备时，通常需要进行大量的优化工作。这种优化工作可能包括模型规模的压缩，性能优化，硬件加速以及适用于嵌入式设备的特定算法的选择。对于这种情况，通常需要工程师深入了解硬件的性能特征以及特定领域的需求。</p><p>&nbsp;</p><p></p><h4>活动推荐：</h4><p></p><p>11月2日，2023网易数字+大会将于杭州举办，网易数帆将带来AIGC技术与云原生、大数据、低代码结合的进展，下午的创新技术论坛，还将全面对外分享网易杭州研究院技术创新范式，带来领域大模型技术揭秘、开源实践分享等话题，欢迎扫描下图二维码或点击链接报名围观：i.163yun.com/obq6t7202</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/11c17401a1fe96ca78f916813cd03ee1.png" /></p><p></p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</id>
            <title>OpenAI 刚刚又杀死了一批初创公司</title>
            <link>https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 07:40:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 创业, ChatGPT, PDF, 初创公司
<br>
<br>
总结: OpenAI最近在ChatGPT上引入了新功能，用户可以上传多种类型的文档，包括PDF，并在同一对话中使用不同的工具。这一更新对于一些初创公司来说可能是一个打击，因为他们的业务正是基于ChatGPT无法直接与PDF交互的现状。然而，对于一些已经成功打包了ChatGPT的初创公司来说，他们仍然面临着发展前景和清算的选择。 </div>
                        <hr>
                    
                    <p></p><blockquote>围绕别人家的大模型创业，盈利快，死得也快？</blockquote><p></p><p>&nbsp;</p><p>从目前的情况看，每当OpenAI在ChatGPT上发布新功能时，都会因为对开发类似功能的初创公司造成冲击而受到指责。OpenAI日前刚刚又为ChatGPT引入了两项新功能，其一是“上传多种类型文档”、其二为“无需切换对话即可使用工具”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2547e3b3e5fdb86572fd275522b04970.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>ChatGPT/GPT-4迎来重要更新：可上传任意PDF文档并询问其内容。无需切换聊天即可直接使用新工具。</blockquote><p></p><p>&nbsp;</p><p>在此次更新之后，ChatGPT不仅能够直接读取PDF，还可以在同一对话当中支持多种文档类型，包括PDF、图像和CSV等等。以往，用户只能在默认模式下上传图像，但现在已经可以无缝上传文档并立即开始提问，这大大扩展了ChatGPT平台的多样性和可用性。</p><p>&nbsp;</p><p>此外，用户也不再需要指定自己要用的ChatGPT模式。如今，浏览、高级数据分析（原名Code Interpreter代码解释器）和DALL-E 3现可在同一对话中直接使用。GPT将自行确定激活不同模式的适当时机，例如在用户要求创建图像时调用DALL-E。</p><p>&nbsp;</p><p>相信很多朋友都有在浏览器里浏览几百页的PDF文件，想要从中提取有用数据和摘要信息的经历，其过程相当之痛苦。正因为如此，此次更新才在ChatGPT Plus用户当中获得了高度评价。</p><p>&nbsp;</p><p>但这次更新也造成了广泛影响。有声音认为，这种新的“多模态”更新将毁掉至少数十家初创公司，其中耳熟能详的名号包括ChatPDF、AskYourPDF和PDF.ai等等。不少初创公司恰恰就是看准了之前ChatGPT无法与PDF直接交互的现状，才构建起自己的业务体系。既然现在ChatGPT可以操作PDF了，那这些后起之秀还有哪些业务空间可以挖掘？</p><p>&nbsp;</p><p></p><h2>面向AI的“打包初创公司”面临毁灭打击？</h2><p></p><p>&nbsp;</p><p>支付巨头Stripe公司产品负责人Sahar Mor在LinkedIn上写道，“OpenAI刚刚推行的一项举措可能直接消灭数十家AI公司。”他还专门提到了“打包初创公司。”这类企业在本质上就是把ChatGPT等API“打包”起来形成自己的业务，使用聊天机器人的底层技术提供某种原厂商未能直接提供的服务。</p><p>&nbsp;</p><p>但建立AI打包业务的创始人们，最初可能未必是要故意利用ChatGPT的软肋。今年3月，OpenAI宣布AI服务对外开放，欢迎开发者们将ChatGPT整合到自己的应用程序和产品当中。</p><p>也就是说，提供PDF分析功能的初创公司只是打包商里的一部分，还有很多在提供其他各种补充性功能。</p><p>&nbsp;</p><p>目前最具市场影响力的打包厂商当数Jasper AI，该公司在Coatue和Bessemer Venture Partners等大型风险投资公司的支持下，成功在今年开年之际获得15亿美元估值。</p><p>&nbsp;</p><p>他们做对了什么才得到如此夸张的估值？答案很简单，围绕OpenAI的GPT模型开发一套专门针对企业营销团队的“AI领航员”（AI Copilot）。</p><p>&nbsp;</p><p>但根据技术外媒The Information报道，随着该公司内部估值的一路走低，其业务定位似乎也陷入了困境。今年7月，Jasper AI还曾宣布裁员。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb1fafa20d1bdcf866fc9ada1a2b772a.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>Jasper 以 $1.5B 估值筹集了 1.25 亿美元，这也无济于事。也许GPT打包模式并不适合初创公司。</blockquote><p></p><p>&nbsp;</p><p>也许其他使用ChatGPT等工具为用户提供PDF交互功能的初创公司，都将面临类似的悲惨命运。</p><p>&nbsp;</p><p>今年5月，数据科学家Alex Reibman发布了ChatOCR。这是一款ChatGPT插件，能够“从PDF中读取文本，包括扫描和手写内容。”但在上周末的更新之后，他在X上开展了一项民意调查，询问用户“既然现在ChatGPT已经内置了PDF处理功能”，大家还愿不愿意继续使用插件。在210名受访者中，72.4%的人预计插件“使用量将会减少”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d4972ac35c66c649dec1af24f5ce82d.jpeg" /></p><p></p><p></p><blockquote>我们是本次更新的“受害者”之一。我们运行着ChatOCR，ChatGPT商店中众多PDF处理插件之一（我们主打的是OCR）。过去3个月来，我们的月度经常性收入达到3500美元。大家认为，这次更新会对我们造成怎样的影响？</blockquote><p></p><p>&nbsp;</p><p></p><h2>是面临清算还是仍有发展前景？</h2><p></p><p>&nbsp;</p><p>在围绕PDF创业的公司中，PDF.ai是一家能赚钱、自给自足而且利润率可观的企业。PDF.ai公司创始人Damon Chen表示，“我们的目标不是成为又一家独角兽企业，几百万美元的年度经常性收入对我来说已经足够了”。</p><p>&nbsp;</p><p>OpenAI的更新对PDF.ai确实也带来了一定冲击，他承认，体量太小的初创公司终将遭到淘汰，而由风险投资供养的大块头在烧光现金之后也挺不住。</p><p>&nbsp;</p><p>但Chen仍然带有希望：“昨晚我和妻子就ChatGPT更新聊了聊。我问，如果PDF.ai最后没能成功，该怎么办？她轻描淡写地说一个项目的失败不算什么，另起炉灶好了。”</p><p>&nbsp;</p><p>如今距离ChatGPT正式亮相已过去近一年，OpenAI正逐渐为其添加更多新功能。OpenAI的终极目标是实现通用人工智能（AGI），而阅读PDF等进展只是这个庞大目标中的一小部分。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f0d4e72b50ef5c1c97b57f25c6635a6.jpeg" /></p><p></p><p>&nbsp;</p><p>正如Tenstorrent公司人工智能总监Shubham Saboo评论的那样，“ChatGPT 的战略：巩固、创新和统治。ChatGPT 会成为终极 AI 超级应用程序，将 Midjourney、PDF Chat、Perplexity AI 和高级数据分析全部结合在一个应用程序中。”</p><p>&nbsp;</p><p>从某种程度来说，只要不具备能与竞争对手拉开显著差距的“护城河”，初创公司就随时面临被劫掠的风险。那么在别人的 API 之上建立自己的业务还有前途吗？</p><p>&nbsp;</p><p>让人意外的是，不少人对此依然表示乐观。支付服务商Stripe公司产品负责人Sahar Mor表示，“打造用户友好的界面和更易用的功能仍有其实际意义，因此针对特定细分市场的垂直初创公司将继续保持其主导地位。真正面临风险的，主要是那些横向延伸的AI初创企业。”</p><p>&nbsp;</p><p>分析服务商Glass Acres创始人Mark Zahm也认为，“只要GPT还存在，GPT打包方案就会伴其成长并蓬勃发展……”AI爱好者Rowan Cheung则在X上分享道，“大家现在怎么不讲那些靠GPT打包方案赚大钱的故事了？”</p><p>&nbsp;</p><p>在他看来，不少初创公司在网络流量上的表现已经超越了价值数十亿美元的传统公司，而且其业务定位均匀分布在打包、微调和专有模型等各个领域。也就是说，部分GPT打包方案的月度访问量，比某些估值数十亿美元的企业还要高。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e078f9ac6611e19b134bf9ee94d6dc2.png" /></p><p></p><p>图片来源：<a href="https://www.infoq.cn/article/iLZYudwYlgARoXmjawJb">ChatGPT 已成为 2023 年最大金矿，大家是怎么靠它挣到钱的？</a>"</p><p>&nbsp;</p><p>开发GPT打包方案的初创公司，主要为那些需要整合AI功能的企业提供一种更经济、更高效的选项，避免从头开始构建复杂的模型体系。</p><p>&nbsp;</p><p>除了OpenAI之外，众多初创公司正在不断涌现，而生成式AI业务已经成为其冲击独角兽之路上的一股重要推力。根据风险投资公司Accel最近发布的报告，这些新独角兽企业中有60%属于生成式AI范畴。去年，欧洲和以色列的生成式AI初创公司投资总额接近10亿美元。相比之下，美国生成式AI初创公司同期获得的注资更是超过140亿美元。但正如报告中所强调，这140亿美元资金的具体分配并不均衡，单是OpenAI一家就分走了其中100亿美元。</p><p>&nbsp;</p><p>根据近期报道，部分照片AI应用和AI聊天机器人服务商赚到的绝对利润，反而还高于生成式鼻祖ChatGPT。今年9月，Chat &amp; Ask AI和ChatOn——AI聊天助手都产生了可观的收入，分别达到近338万美元和211万美元。</p><p>&nbsp;</p><p>此外，AI Chatbot——Nova和AI Chatbot: AI Chat Smith也不甘落后，同期收入分别为144万美元和172万美元。而由a16z支持的聊天机器人初创公司Character.ai也在市场上闹出不小的动静，截至今年9月下载量已达239万次。</p><p>&nbsp;</p><p>也就是说，哪怕OpenAI凭借其多模态功能领先了竞争对手十步，各位初创选手也没必要悲观放弃。相反，也许底层核心技术的升级能提供丰富灵感、帮助他们开发出更好的后续产品。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>"</p><p><a href="https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>"</p><p><a href="https://twitter.com/thealexker/status/1680626018522914817">https://twitter.com/thealexker/status/1680626018522914817</a>"</p><p><a href="https://twitter.com/AlexReibman/status/1718848888088793487">https://twitter.com/AlexReibman/status/1718848888088793487</a>"</p><p><a href="https://twitter.com/Saboo_Shubham_/status/1718653456926359855">https://twitter.com/Saboo_Shubham_/status/1718653456926359855</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</id>
            <title>从互联网到云计算再到 AI 原生，百度智能云数据库的演进</title>
            <link>https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 06:23:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库技术, 云原生, AI技术, 百度智能云
<br>
<br>
总结: 作为计算机系统的核心基础软件之一，数据库技术的发展备受关注。随着云计算技术的发展，云原生和分布式数据库成为主流，具有高可用性、可扩展性和低成本等优势。AI技术的不断发展使得AI与云计算结合成为可能，云原生为数据库提供了基础条件，AI成为云原生数据库持续演进的驱动力。百度智能云在数据库领域不断创新，推出了云原生数据库GaiaDB、云数据库GaiaDB-X和数据传输服务DTS等产品和解决方案，以支持大数据和AI应用在行业中的挑战。百度智能云团队还推出了《百度智能云数据库》系列云智公开课，探讨数据库的创新、变革和应用。 </div>
                        <hr>
                    
                    <p>作为计算机系统的三大核心基础软件之一，数据库技术的发展一直备受关注。随着云计算技术的发展，能够适合更大规模业务场景，有着高可用性、可扩展性和低成本等优势的云原生和分布式数据库逐渐成为主流。</p><p></p><p>同时，AI 技术不断向前发展，尤其是 OpenAI 掀起的这场 AI 变革，使得 AI 与云计算更紧密地结合成为了可能。云原生为数据库面向更大范围的智能化应用提供了基础条件，AI 成为云原生数据库持续演进的牵引力。</p><p></p><p>为了应对大数据和 AI 应用在行业中的挑战，<a href="https://www.infoq.cn/news/kqPbdlvF3Jp55PodQLAo">百度智能云</a>"在 2020 年率先提出“云智一体”战略，以“云计算为基础”支撑产业数字化转型，以“人工智能为引擎”深入产业生产的关键场景，为企业的数字化转型和智能化升级提供新型支持。在云智一体战略的指导下，<a href="https://www.infoq.cn/article/o8abj2wff5yLfGWuB0E1">百度智能云</a>"在数据库领域不断创新，基于百度集团各项业务的多年磨炼，对外推出了云原生数据库 GaiaDB 、云数据库 GaiaDB-X、数据传输服务 DTS 等产品和解决方案。那这些产品和相关方案，都有哪些不一样的地方，又有哪些落地实践？比如：</p><p>在互联网、云计算，以及向 AI 的演进过程中，百度智能云数据库团队是如何进行技术升级，做到支持各类业务场景，满足海量数据规模，业务要求越来越苛刻的场景的。在 AI 时代，它的最新成果和规划又有哪些？相比其他云原生数据库， GaiaDB 是如何诞生于百度集团的内部业务，其产品理念有什么独特的地方，在哪些技术上面形成了优势，可以帮助用户解决什么样的挑战？在金融行业的国产化进程，GaiaDB-X 如何承载核心业务系统，满足金融行业对基础设施的高要求，在金融客户中的成功落地场景又有哪些？完善的数据库服务不止有数据库产品本身。数据库的迁移、同步、集成同样重要。这些工作和数据库本身一样，同样是复杂又至关重要的。百度智能云的数据传输服务 DTS 如何将繁琐复杂的这类业务变得可靠简单，在业务实践中帮助头部客户成功上云？</p><p></p><p>为了帮助大家解决这些问题，<a href="https://www.infoq.cn/article/ACXL3WviaTtr2U0v5NlB">百度智能云</a>"团队特推出《百度智能云数据库》系列云智公开课。前四期课程便将围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开。从 11 月 15 日起，每周三都将有一位百度智能云的大咖与各位一起探讨百度智能云数据库的创新、变革和应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf46378ce27ef5e83b5b6abf599ec8d3.jpeg" /></p><p></p><p>第一讲：《从互联网到云计算再到 AI 原生，百度智能云数据库的演进》</p><p>你将获得：</p><p>全局完整地了解数据库行业的历史和发展趋势；了解百度智能云数据库在各个阶段的典型产品、应用和关键技术；了解百度智能云数据库在 AI 原生时代的创新和变革。</p><p>&nbsp;</p><p>第二讲：《高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析》</p><p>你将获得：</p><p>了解云原生数据库的不同技术路线和能力对比；了解相比传统单体数据库，云原生数据库的技术差异和挑战；了解 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p>&nbsp;</p><p>第三讲：《面向金融场景的 GaiaDB-X 分布式数据库应用实践》</p><p>你将获得：</p><p>了解金融核心系统在构建分布式数据库的技术挑战；了解 GaiaDB-X 数据库的架构及在金融场景的分布式特性；了解 GaiaDB-X 在金融机构的核心系统分布式的落地实践。</p><p>&nbsp;</p><p>第四讲：《一站式数据库上云迁移、同步与集成平台DTS的设计和实践》</p><p>你将获得：</p><p>了解数据库上云迁移、数据库同步 / 集成的业务场景，以及实践中可能遇到的技术挑战；了解百度智能云 DTS 的关键特性、核心技术和实践案例。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</id>
            <title>蚂蚁SOFA Stack融合大模型发布升级版 助力机构产研效能提升30%</title>
            <link>https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 06:14:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, CodeFuse, SOFAStack5.0, 企业研发运维智能助手
<br>
<br>
总结: 蚂蚁集团发布了CodeFuse全面加持的SOFAStack5.0升级版本，为企业提供全方位研发运维智能助手相关能力。这个升级版本将为企业产研效能提升30%，通过智能副驾驶提升日常代码研发、测试、运维过程中的效率和质量。SOFAStack还提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。 </div>
                        <hr>
                    
                    <p>11月1日，在云栖大会上，蚂蚁集团正式发布CodeFuse全面加持的SOFAStack5.0升级版本，向企业提供全方位研发运维智能助手相关能力。这是继蚂蚁集团在外滩大会发布代码大模型CodeFuse之后，首次公布面向行业的商业化产品进展。</p><p>&nbsp;</p><p>“大模型将为研发效能带来颠覆性机遇。”蚂蚁集团数字科技事业群产品总监马振雄在发布会上指出。</p><p>&nbsp;</p><p>记者了解到，目前CodeFuse已经与SOFA产品线全面融合，涵盖设计、研发、测试、运维等领域，形成从领域建模到智能运维的端到端Copilot产品解决方案，预计将为企业产研效能提升30%。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/5c/5cd0d1b716d80689035335361a75486b.png" /></p><p></p><p>具体而言，客户在使用SOFAStack时，相当于为企业开发者配备专属智能副驾驶，和机器人“辅助设计”、“结对编程”、“运维助手”，通过人机交互助手提升日常代码研发、测试、运维过程中的效率和质量。对企业而言，引入智能副驾驶可以显著提升人效质量，降低总体成本。</p><p>&nbsp;</p><p>此外，SOFAStack针对Codefuse大模型提供了多任务微调和高性能推理能力，结合企业专有数据构造更懂客户业务的智能副驾驶。而随着CodeFuse在产品线中不断深度融合，SOFAStack将为企业打造新一代AI云原生PaaS平台，使其在开发运维、数据分析、应用治理、绿色计算方面取得更智能的能力，可以加速响应业务创新和价值交付。</p><p>&nbsp;</p><p>针对当下企业应用上云「更异构、更智能、更经济」的三大需求趋势，马振雄表示，SOFAStack提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。这些解决方案可以满足不同行业和企业的需求，并为企业提供更加灵活和高效的技术支持。例如，针对行业进入多云时代，边缘资源调配、云上云下应用开发等统一管理挑战，其拳头产品MESH升级架构，从原来的经典Sidecar架构开始演变为Node架构，同步进行了性能、服务治理、业务可观测能力等全方位优化。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/7c/7cd485b0811e68275758bb2d760a382d.png" /></p><p></p><p>Forrester报告曾分析指出，以云原生为关键能力的下一代云平台， 不仅可以基于全栈云原生架构灵活适应市场变化，而且可以通过全云开发实践帮助企业在云上快速验证创新思路，还能借助云平台的各类自动化能力降本增效强化韧性。</p><p>&nbsp;</p><p>“服务网格降低了我们的上云门槛。如果做云原生改造，系统的所有代码都要重写一遍，大概需要20&nbsp;个人投入一年时间；使用网格（Mesh）只要&nbsp;5&nbsp;个人两三个月就能上云。”传统金融机构信息科技架构规划负责人在Forrester调研时表示。根据报告测算，三年内有&nbsp;10&nbsp;个单体应用不需要经过云原生改造，即可直接上云后统一治理，总体效率提升为企业带来941万元的收益。</p><p>&nbsp;</p><p>据悉，SOFAStack是国内部署云原生技术最广泛的平台之一，基于支付宝、蚂蚁集团各项业务需求进行研发迭代，并服务于超100家银行迈向云原生转型，已经构建了完整金融级的云原生PaaS解决方案。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</id>
            <title>计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士，确认担任 QCon LLM 时代的性能优化专题出品人</title>
            <link>https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, LLM 时代的性能优化, 周经森（Kingsum Chow）, CPU 和 GPU 平台
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，周经森博士将担任“LLM 时代的性能优化”专题的出品人。该专题将介绍LLM时代的性能分析在CPU和GPU平台上的表现。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">LLM 时代的性能优化</a>"」的专题出品人。在此次专题中，你将了解到 LLM 时代的性能分析在 CPU 和 GPU 平台上，在不同计算环境下的性能表现。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">周经森（Kingsum Chow）</a>"，计算机软硬件优化首席科学家、高级首席工程师。曾就职于美国英特尔公司和中国阿里巴巴集团，2023 年加入浙江大学软件学院（宁波）。二十年来与十余家世界 500 强高科技企业合作，共同推动了世界软硬件性能优化技术的发展。曾作为项目总监主持备受瞩目的云计算蓝图项目（IntelCloudBlueprint）。该项目由英特尔和甲骨文的首席执行官于 2015 年共同宣布，吸引了超过 4 万名开发者的参与，为云计算行业绘制了全新的技术蓝图，对行业发展产生了深远影响。</p><p></p><p>自 2016 年加入阿里巴巴，为中国的性能优化技术发展做出了巨大贡献。2018 年，其作为唯一一名加入 Java 全球管理组织 JavaCommunityProcess（JCP）最高执行委员会 JCP-EC 的中国企业（阿里巴巴）代表，参与制定了 Java 的全球标准。</p><p></p><p>周博士在 CPU 利用率报告不准确（数据普遍误解）方面发表的研究，引起了业界和学术界的广泛关注。周博士拥有超过 30 年的软硬件协同优化的工业实践经验，培养了大批优秀的系统性能优化人才。至今已获授权中国专利 11 项，美国专利 24 项，发表学术论文 127 篇，在过去 6 年的 QCon 中国大会上发表 2 场主题演讲，出品 2 场软件系统性能优化主题讲座。</p><p></p><p>相信周经森（Kingsum Chow）博士的到来，可以帮助提升此专题的质量，让你学习到， 通过 LLM 在不同计算环境下的性能表现，找到的最佳应用策略和优化方法，这为 LLM 的应用和发展提供了更多的可能性。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P</id>
            <title>15年技术沉淀，起底阿里核心搜索引擎 Havenask 演进之路</title>
            <link>https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 01:04:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里开源搜索引擎, Havenask, 技术演进, 大规模分布式搜索引擎
<br>
<br>
总结: 阿里开源搜索引擎Havenask是一款基于大规模分布式技术的搜索引擎，经过多年的技术演进，从解决内部业务需求到统一整合各业务搜索系统，再到逐步开源对外，Havenask具备高性能、高并发、高时效性的特点，能够智能地帮助人们快速、准确地获取信息。 </div>
                        <hr>
                    
                    <p></p><h2>阿里开源搜索引擎&nbsp;Havenask&nbsp;的技术演进</h2><p></p><p></p><p>我们正处于信息爆炸式增长的时代，如何在信息海洋里迅速定位到目标信息成为人们关心的问题。搜索引擎作为互联网和应用的关键入口，向来是兵家必争之地。</p><p></p><p>然而在人们简单的搜索行为背后，对搜索引擎技术实际有诸多挑战：以电商场景为例，当遇到双11等大促活动时，百万级&nbsp;QPS&nbsp;的高并发访问，对千亿级商品&nbsp;&amp;&nbsp;订单数据、保单&nbsp;&amp;&nbsp;物流类数据时效性要求极高，那么搜索引擎该如何做到毫秒级时效？还有为了更准确理解人们的搜索意图，对搜索算法的要求越来越高，搜索引擎该如何做到算法分钟级迭代？这些都是技术上需要直面的挑战。</p><p></p><p>近年来，随着大数据技术、深度学习等&nbsp;AI&nbsp;技术的发展，搜索引擎能够更智能地帮助人们快速、准确地获取信息，我们对信息的处理能力也随之逐步提高。</p><p></p><p>阿里自研大规模分布式搜索引擎&nbsp;Havenask&nbsp;便是集大成者，基于阿里搜索十多年来的技术沉淀，Havenask&nbsp;目前广泛应用于阿里巴巴和蚂蚁集团内众多业务，如淘宝搜索和推荐、&nbsp;蚂蚁人脸支付、优酷视频搜索、阿里妈妈广告检索等。Havenask&nbsp;支持算法高效快速迭代，内置性能优异的向量检索能力；做到毫秒级查询性能，并拥有稳定性保障&nbsp;；支持单应用实例千亿级别数据，确保百万&nbsp;TPS&nbsp;高时效性。</p><p></p><p>2022&nbsp;年&nbsp;12&nbsp;月，阿里将&nbsp;Havenask&nbsp;开源，在几个月时间里&nbsp;Star&nbsp;数已超过&nbsp;1000+。为何&nbsp;Havenask&nbsp;有这样优异的表现，在短时间内获得众多开发者的喜爱？下面我们从&nbsp;Havenask&nbsp;的技术演进谈起，让大家更加深入了解&nbsp;Havenask&nbsp;以及未来更多可能性。</p><p></p><p>传送门：++https://github.com/alibaba/havenask++</p><p></p><h2>01&nbsp;Havenask&nbsp;技术演进之路</h2><p></p><p>回顾&nbsp;Havenask&nbsp;从内部自研技术走向成熟，这一路走来可分为以下阶段：</p><p></p><p><img src="https://static001.geekbang.org/infoq/85/85f3025dbedec2a27678b2160ae2f23d.png" /></p><p></p><p>第一阶段：1999&nbsp;年~2008&nbsp;年，以解决各业务部门的搜索需求为主</p><p></p><p>阿里搜索技术最早可追溯&nbsp;1999&nbsp;年，起源于雅虎搜索技术，基于&nbsp;Apache&nbsp;Module&nbsp;的单机版搜索引擎，支撑淘宝、B2B&nbsp;等子公司的搜索业务需求。</p><p></p><p>第二阶段：2009&nbsp;年~2011&nbsp;年，重构搜索系统，开启自研大规模分布式高性能搜索引擎时代</p><p></p><p>自&nbsp;2008&nbsp;年起，开始构建阿里统一的搜索系统，内部代号为“iSearch”，它代表完全由阿里自研的搜索技术全新启航。iSearch&nbsp;迅速迭代&nbsp;iSearch3.0、iSearch3.2……2009&nbsp;年演进到&nbsp;iSearch4.5&nbsp;版本，也就是&nbsp;HA3（Havenask）最早的雏形。</p><p></p><p>2009&nbsp;年，Havenask&nbsp;开始逐步统一各子公司版本，去除&nbsp;Apache&nbsp;Module。2011&nbsp;年，彻底完成搜索系统的重构，HA3（Havenask）全部替代老的雅虎搜索系统，开始极致性能时代。</p><p></p><p>第三阶段：2012&nbsp;年~2018&nbsp;年，完成阿里内部搜索系统的“大统一”，进入快速迭代时代</p><p></p><p>2013年，HA3（Havenask）完成阿里集团各个业务搜索系统的“大统一”，不仅版本再次合并，还将&nbsp;B2B、淘宝等搜索团队统一整合，以产品化、规模化的方式支撑起整个集团的搜索业务。</p><p></p><p>2018&nbsp;年，随着深度学习技术的广泛应用，同时迎来信息流推荐机遇，HA3（Havenask）快速迭代，逐步形成一套以搜索引擎、在线推理引擎等为主的&nbsp;AI&nbsp;工程技术体系“AI·OS”。（OS”代表“Online&nbsp;Serving”&nbsp;）</p><p></p><p>第四阶段：2018&nbsp;年~至今，对外开源，技术普惠</p><p></p><p>2022&nbsp;年，阿里将搜索引擎&nbsp;Havenask&nbsp;开源，为更多用户提供更高性能、更低成本、更便捷易用的搜索服务。</p><p></p><p>总的来说，Havenask&nbsp;的发展是遵循先解决内部业务应用需求，再从核心业务延伸到其他业务，随着技术发展潮流不断向前演变，从单一的搜索引擎到大数据深度学习在线服务体系&nbsp;AI·OS&nbsp;的重要组成部分，打造成统一平台提供更强大的能力支撑，继而逐步开源对外，普惠开发者，这和阿里其他技术产品的发展思路是一脉相传的。</p><p></p><h2>02&nbsp;Haveansk&nbsp;架构优势</h2><p></p><p>从定位来看，Havenask&nbsp;作为阿里巴巴自主研发的大规模分布式搜索引擎，支撑起淘宝、天猫、菜鸟、优酷阿里整体的搜索业务，并扛得住双&nbsp;11&nbsp;大促活动。这背后，离不开底层架构设计，让&nbsp;Havenask&nbsp;有了坚实的技术基底。</p><p></p><p><img src="https://static001.geekbang.org/infoq/33/33f354cadf938d9f305d845ffdf35a82.png" /></p><p></p><p>从架构来看，Havenask&nbsp;由四个核心模块组成：</p><p></p><p>索引系统**（Build**&nbsp;Service）。通常搜索引擎需要对原始数据构建索引，才能在提供服务时实现高性能。这部分在&nbsp;Havenask&nbsp;是支持全量、增量、实时流的复杂分布式流计算系统。</p><p></p><p>在线集群**（Havenask**&nbsp;**Runtime）****。**在线系统支持不同的数据规模分列查询，不同的查询并发做多副本。在系统里设计有类似于大脑的复杂角色，可以自动做查询处理、调度查询节点、数据节点等。如果出现机器坏了的情况，在线系统可自动识别这些情况，来保证系统的高可用。</p><p></p><p>消息中间件（Swift）。消息中间件用于实时数据传递，处理后的文档传递，是&nbsp;Havenask&nbsp;实现毫秒级时效性，支撑海量数据实时更新的基石。消息中间件&nbsp;Swift&nbsp;不仅可以用在&nbsp;Havenask&nbsp;系统中，也可以单独部署使用，与其他开源中间相比具有明显的性能和成本优势。</p><p></p><p>管控系统（Hape）。为了方便开发人员的日常运维，Havenask&nbsp;对管控运维的&nbsp;API&nbsp;进行了封装，提供方便实用的运维工具&nbsp;Hape&nbsp;，使用它开发人员可以方便的对表和集群进行管理。</p><p></p><p>据阿里巴巴智能引擎事业部云服务负责人、Havenask&nbsp;开源项目负责人郭瑞杰博士介绍，在架构设计上，Havenask&nbsp;更具备适合工业级业务场景的特性：</p><p></p><p>1、通过灵活稳定的扩展方式支持业务多样化需求，轻松应对数据规模和流量规模的快速增长；</p><p></p><p>2、通过领先的实时索引技术，提供性能出色的亚秒级实时搜索能力，通过对实时索引的不断自动整理优化，保证搜索性能持续优异；</p><p></p><p>3、传统倒排索引技术和&nbsp;AI&nbsp;时代普遍应用的向量检索技术深度结合，端到端极致性能优化，支持千亿级别文档或高维向量的极低延迟计算。</p><p></p><p>人们进行商品搜索时，由于每个人有不同的喜好，搜索引擎需实现个性化和智能化，以准确召回商品。当用户开始进行搜索时，往往是用关键词或一段自然语言的描述，搜索引擎先采用&nbsp;NLP&nbsp;技术理解和拆分成关键词，再根据关键词的语义相关性，采用向量等多路召回方式，返回有可能是用户想要找的商品信息，再对商品做粗排，粗排后收敛到集合里，再做精排，这个过程中&nbsp;Havenask&nbsp;使用了大量机器学习算法进行优化，以实现较好的用户体验。</p><p></p><p>这对搜索引擎有较高的性能要求，Havenask&nbsp;利用前置化思想，并发完成多路召回，实现非常小的延迟效果。另外在算法上，Havenask&nbsp;支持离线计算转在线计算、在线计算转离线计算做优化，还支持模型的实时更新以保证在离线的一致性。如此一来，算法工程师可以用更复杂的召回策略来做&nbsp;A/B&nbsp;测试验证效果，如果效果可以的话，可以实现分钟级上线。</p><p></p><p>在拍照搜图场景中，以淘宝拍照购物“拍立淘”为例，用户通过手机拍摄实物或通过相册照片搜索，就能搜索同款或相似商品。&nbsp;Havenask&nbsp;利用向量进行图片搜索，完成向量索引存储并将向量化后的图片与向量索引比对召回，实现高精度图片搜索。上述能力得益于&nbsp;Havenask&nbsp;和达摩院向量库&nbsp;Proxima&nbsp;深度结合，并进行端到端能力优化，支持百亿甚至千亿级别的高维度向量的低延迟计算。</p><p></p><p>总体来看，Havenask&nbsp;区别于其他产品的特点主要体现在两大场景中：一是大数据检索场景，实现亚秒级的时效性和极致的性能优化，达到较高的性价比。二是在&nbsp;AI&nbsp;场景上，Havenask&nbsp;实现异步高并发、超低召回延迟，提供在离线一致性保障机制，以及高性能高维度向量计算能力。</p><p></p><p>即使在双11特殊场景里，数据更新量突然爆增至十倍、百倍，Havenask&nbsp;仍能保证时效性在亚秒级。在查询上，单集群到近百万&nbsp;QPS&nbsp;时，Havenask&nbsp;确保查询延迟毫秒级别。另外，Havenask&nbsp;足够弹性，针对双11的流量急速变化，集群一键平滑扩缩容，变更对业务0影响，灵活应对流量峰谷。</p><p></p><h2>03&nbsp;Havenask&nbsp;开源开放，普惠开发者</h2><p></p><p>Havenask&nbsp;起源于阿里内部搜索业务需求，如今作为核心搜索引擎在阿里内部广泛应用，那么团队为什么选择将&nbsp;Havenask&nbsp;对外开源？</p><p></p><p>郭瑞杰表示，Havenask&nbsp;围绕着电商场景演化出来，在阿里核心头部业务、中台业务等均广泛使用。希望通过开源的方式让广大开发者参与进来，让&nbsp;Havenask&nbsp;迭代更快走得更远。以开源&nbsp;Elasticsearch&nbsp;为例，在十年时间中，Elasticsearch&nbsp;因为开源发展迅速，Havenask&nbsp;也期待通过开源吸引更多开发者参与进来，一起联合共创。</p><p></p><p>再者，近年来国际形势变幻莫测，人们对国产化替代诉求与日俱增。期望自主研发的&nbsp;Havenask&nbsp;能帮助一些企业实现国产化替代，让更多开发者和企业以更低的成本实现业务创新。</p><p></p><p>不仅如此，Havenask&nbsp;还提供商业版本来支持企业实现搜索场景、推荐场景、大模型应用场景创新。</p><p></p><p>“&nbsp;Havenask&nbsp;自开源后，在尚未开展过多活动的情况下，Star&nbsp;数快速突破&nbsp;1000，对我们来说还挺意外的，这也让我们坚定了后续持续建设开源&nbsp;Havenask&nbsp;的信心。”郭瑞杰说。</p><p></p><p>Havenask&nbsp;作为&nbsp;AI·OS&nbsp;体系的重要部分，沉淀了阿里&nbsp;10&nbsp;多年的搜索技术，整体系统庞大，采取逐步开源的形式对外开放，从2022年首发时的单机预览版，到如今刚刚发布的的分布式正式版，已经完成了&nbsp;Havenask&nbsp;几乎全部核心代码的开源。</p><p></p><p>在2023年9月份最新发布的&nbsp;Havenask&nbsp;1.0.0&nbsp;分布式版本中，支持读写分离与读写统一两种部署架构，可以分别满足开发者不同业务场景的需求，同时分布式版本提供基于机器资源池的集群自动化管理能力、动态表管理能力，降低开发者集群运维的成本；并且集成了自研的消息中间件，支持更完善的实时数据更新能力。</p><p></p><p>据郭瑞杰透露，在后续的版本中，&nbsp;Havenask&nbsp;会更聚焦开发者的真实使用场景，特别是大数据检索和智能检索等领域不断构建&nbsp;Havenask&nbsp;的开源生态，让&nbsp;Havenask&nbsp;更加广泛的应用在更多业务中，解决开发者面临的性能、成本、稳定性等核心问题。</p><p></p><p>与此同时，Havenask&nbsp;还开源了&nbsp;Havenask-federation（简称Fed）项目（<a href="https://github.com/alibaba/havenask-federation">https://github.com/alibaba/havenask-federation</a>"），在&nbsp;Havenask&nbsp;和&nbsp;Elasticsearch&nbsp;之间架起一条桥梁，方便&nbsp;Elasticsearch&nbsp;开源生态用户，快速迁移和扩展，实现优势互补。</p><p></p><h2>04&nbsp;Next&nbsp;Big&nbsp;Thing</h2><p></p><p>最近技术人话题离不开热门的&nbsp;ChatGPT，ChatGPT&nbsp;一经发布，大家认为被最早被颠覆的是搜索引擎。传统搜索引擎&nbsp;+&nbsp;ChatGPT&nbsp;将产生巨大化学反应，或将改写搜索引擎的产品形态。ChatGPT&nbsp;能更好地理解人们的搜索意图，为用户提供汇总答案，提供更准确的搜索结果，还能以自然语言来搜索，让搜索体验有质的提升。</p><p></p><p>郭瑞杰表示，有了&nbsp;ChatGPT&nbsp;能力加持，不仅在&nbsp;to&nbsp;C&nbsp;端搜索引擎发生巨变，在&nbsp;to&nbsp;B&nbsp;端也将催化诞生颠覆性的产品形态。其中&nbsp;to&nbsp;B&nbsp;端和&nbsp;to&nbsp;C&nbsp;端搜索引擎稍有差异，to&nbsp;B&nbsp;搜索引擎是面向企业，主要搜企业数据，而不是搜全网数据，更多的是围绕企业数据来提供更智能和更准确的答案。</p><p></p><p>针对不同行业的用户想基于大模型能力完成业务创新，Havenask&nbsp;除了在底层传统搜索引擎技术上提供帮助，也正在做如下两个方面的能力增强，并持续开源：一是向量检索。在大模型时代下，向量检索技术是大模型应用创新的基石，我们正在构建新的向量检索引擎&nbsp;VectorStore，预计性能大幅超越&nbsp;Milvus，期望能提供给开发者更高性能、更低成本的向量检索方案；二是大模型推理加速。将全面支持各种&nbsp;LLM（qwen、chatglm、baichuan、xverse、interlm、llama、falcon、mpt、starcoder&nbsp;等）的推理加速，支持量化、多机多卡分布式、上下文&nbsp;cache&nbsp;等多种特性，预计性能超越&nbsp;vllm&nbsp;15%，期望给开发者提供更低成本的大模型推理服务。</p><p></p><p>现在，我们看到阿里已先行一步：在&nbsp;2023&nbsp;阿里云峰会上，正式推出大语言模型“通义千问”，并宣布阿里所有产品未来将接入“通义千问”，进行全面改造。例如在网购场景，用户如果想开生日&nbsp;party，通义千问可以帮助生成生日活动方案和购物清单。</p><p></p><p>期待后续&nbsp;Havenask&nbsp;与“通义千问”联合创新，为人们带来更好地搜索体验，帮助企业和开发者量身定做适合业务发展的智能搜索服务，促进业务飞速增长，共享科技红利。</p><p></p><p>此外，基于&nbsp;Haveansk&nbsp;与“通义千问”打造的AI搜索产品——OpenSearch&nbsp;LLM&nbsp;智能问答版，也已在阿里云上为企业级开发者提供全托管、免运维的一站式对话式搜索服务，欢迎企业级开发者们试用。</p><p></p><p>心动不如行动，欢迎立即体验：</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p>Havenask-federation&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask-federation">https://github.com/alibaba/havenask-federation</a>"</p><p></p><p>OpenSearch&nbsp;LLM&nbsp;智能问答版：<a href="https://www.aliyun.com/activity/bigdata/opensearch/llmsearch?spm=5176.7946605.J_4098459070.4.15b38651FlNqqw">https://www.aliyun.com/activity/bigdata/opensearch/llmsearch?spm=5176.7946605.J_4098459070.4.15b38651FlNqqw</a>"</p><p></p><p>钉钉扫码加入Havenask开源官方技术交流群：</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78c5cfa61c64a55cdeb0655ac7eb2849.png" /></p><p></p><p>近期活动预告：</p><p></p><p>2023年11月1日13:10-13:25，杭州云栖大会&nbsp;B3-4&nbsp;馆，Havenask&nbsp;开源正式版发布演讲</p><p></p><p>2023年11月1日14:40-15:10，杭州云栖大会&nbsp;C&nbsp;区舞台，Havenask&nbsp;开源细节与案例分享</p><p></p><p>欢迎开发者前往会场参加，或通过线上渠道收看关注</p><p></p><p>嘉宾介绍：郭瑞杰博士，2008年加入阿里巴巴，深耕阿里搜索领域开发十余年，先后负责&nbsp;iSearch4.5、问天2、问天3等多个搜索架构及产品的设计与开发工作，现任阿里巴巴智能引擎事业部云服务负责人，阿里云计算平台事业部搜索推荐云服务负责人，Havenask&nbsp;开源项目负责人。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC</id>
            <title>“2023 深圳国际金融科技大赛”线上技术公开课：人工智能、区块链、产品经理，分别是怎样赋能金融行业的？</title>
            <link>https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 11:57:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技, 智慧金融, 普惠金融, 区块链
<br>
<br>
总结: 金融科技是金融行业中的一个重要分支，它能够满足智慧金融和普惠金融的需求。传统金融机构需要借助先进技术和理念来改造自身，以提供更好的服务和体验。为了推进金融科技产业的发展，深圳国际金融科技大赛特别设置了区块链赛道，希望激发选手创新热情，为金融科技发展提供更多有价值的解决方案。 </div>
                        <hr>
                    
                    <p>金融科技是科技创新领域中一个独特且重要的分支。传统金融产业重视稳定、可靠和信誉，但在面对科技进步时，却往往表现出保守的态度和缓慢的行动；另一方面，市场的快速发展和变化给金融企业带来了前所未有的挑战。智慧金融和普惠金融的需求日益高涨，金融机构需要借助先进技术和理念来改造自身，以提供更好的服务和体验来满足这些需求。</p><p></p><p>为此，行业一直在努力探索适合金融业特点的技术发展路线，确保在满足安全、可靠和可信的前提下，满足日益增长的市场需求。在此需求下，也为了进一步推进金融科技产业的发展，所以“2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛”（下文称“大赛”）特别设置了<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbe9a85fcc9e2149db6f60a7f8bbe5fe2326a33a3177c10f98693fc301418ee907544b18f2cc&amp;idx=1&amp;mid=2247487888&amp;scene=27&amp;sn=931b65213c5f893047ad4edfb60b1a2e&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">区块链</a>"、人工智能与产品经理赛道，希望激发选手创新热情，为金融科技发展提供更多有价值的解决方案。</p><p></p><p>作为 2023 年深圳市金融科技节的重要一环，本届大赛在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、<a href="https://www.infoq.cn/article/W05aweqVPI9UwdxxOzoi?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">微众银行</a>"、深圳香蜜湖国际金融科技研究院等多方联合举办。大赛设置总额超过 69 万人民币的赛事奖金及参赛专属电子区块链证书，还邀请学术和企业界的众多资深专家为参赛选手答疑解难。</p><p></p><p>为帮助同学们深入了解<a href="https://www.infoq.cn/article/2K0clWV5ZGjlPumJhf9G?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">金融科技</a>"前沿成果，尽快熟悉和理解赛题赛制，10 月 25 日，本届大赛的线上技术公开课上线。微众银行区块链首席架构师张开翔老师、微众银行人工智能部室高级经理杨海军老师、微众银行个人直通银行部室经理金虎光老师围绕区块链、人工智能、金融产品经理等话题展开了主题分享，并介绍了各赛道赛题赛制的相关细节，回答了同学们最关心的问题。以下为本期公开课直播精华内容整理：</p><p></p><p></p><h2>一、微众银行区块链首席架构师张开翔：“区块链技术构筑 ESG 可信基础设施”</h2><p></p><p></p><p>ESG，亦即环境、社会和治理，是今天各行业都非常重视的概念，也受到了金融企业的普遍关注。而区块链又是当下金融科技等行业的热点技术主题，其防篡改、安全性高等特性有很高的应用潜力。</p><p></p><p>那么区块链技术怎样同 ESG 建立联系？以养殖场为例，牲畜需要经过养育、屠宰、检验、运输等各种环节，最终到达商超终端。如果将这些过程中的重要数据，例如牲畜每天的体温、运动步数、检验证书、运输路线等都记载在区块链上，就可以实现全程可信、可追溯、无法篡改的效果。类似地，普通市民上班时选择乘坐地铁，减少私家车排放污染，那么地铁票和上班路径等信息也可以记录在区块链上，从而获得绿色出行的积分奖励。</p><p></p><p>在上述应用场景中，区块链技术的落地关键在于实体世界的数据如何与链上数据同步，从而利用区块链准确地记录真实信息，搭建从链下到链上的可信任链条。这一过程中一般需要用到区块链的存证、追溯、审计、记账和清算功能，并在同步环节设计好数据锚定与校验流程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/86a14b11f2a9456c9693d11a8fad523c.png" /></p><p></p><p>本届大赛的区块链赛道主题是区块链 +ESG，其宗旨就是鼓励参赛选手将实体世界的 ESG 数据与区块链结合，探索各类创新应用场景。微众银行为大赛选手提供了丰富的技术和资源支持，具体可以参见本届大赛官网&nbsp;https://www.infoq.cn/zones/fintechathon/campus2023/&nbsp;。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a11f0456e7aae97651412e5b2cb89d0.png" /></p><p></p><p>张开翔提到，微众银行的区块链技术完全开源，参赛选手可以借助这些开源技术，通过开放社区的学习来使用区块链发挥创意，在金融科技领域实现各种需求。举一个例子，选手可以使用区块链构建一个绿色出行的生态，用户在这一生态中可以使用记录在智能合约上的出行积分兑换商家的优惠券。但这样的生态还必须保障隐私和安全，比如用户并不想让他人得知自己的出行路线，也不想暴露自己的住处、上班场所的位置。那么选手就可以在安全领域发挥专业水平，通过多方计算、可信计算等技术增强这一生态的安全性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c749a0755cbe5a43fb9783f764dfaa8b.png" /></p><p></p><p>张开翔从大赛评委的角度提出，评委更希望看到选手拿出一些在具备实用价值的前提下有新意、好玩的作品。例如往届大赛的作品就有使用区块链链接交通出行各参与方、解决相亲活动的信任问题等。张开翔希望选手打开思路，设想更多使用场景，在隐私、安全、性能、容量和功能层面深挖区块链应用潜力，这样做出的作品就会得到很高的成绩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/461a99c1eab3997844c5570d15aba2ab.png" /></p><p></p><p>此外，张开翔还提到，选手入围决赛后，在最终的决赛现场会有很多专家老师提供指导。决赛中，选手展示完毕后也要回答评委老师的尖锐问题，比如：</p><p></p><p>你用区块链解决了什么问题？为什么一定要用区块链？物理世界怎样使用区块链验证可信度？传感器怎样防篡改？收集的数据是否会侵犯隐私？项目如何推广、获利？……</p><p></p><p>所以最后张开翔给所有参赛选手提出了一个建议，“希望参赛选手可以改变学生思维，更多考虑如何为社会创造价值，这样才能在大赛中取得更好的成绩。”</p><p></p><p></p><h2>二、微众银行人工智能部室高级经理杨海军：“微众银行 AI 技术服务解决方案”</h2><p></p><p></p><p>微众银行将人工智能前沿技术（图像、语音、NLU、大模型、联邦学习等）与金融服务深度融合，探索将相关技术融入金融服务各个环节，拓展金融服务的广度和深度，重塑以客户为中心的金融价值链和生态，推动“未来智能金融”的实现。</p><p></p><p>微众银行人工智能应用覆盖客服、营销、风控、运营多场景，包括针对用户端的智能客服机器人、智能语音机器人、智能核身（人像、声纹等）解决方案等；企业后端的智能质检机器人、智能培训机器人、营销助手机器人、KYC、反欺诈解决方案等。基于联邦学习实现 AI 技术终生学习和抗攻击性，大幅提升用户服务质量与效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c61dc4284e5b34e2104719094d96ce3.png" /></p><p></p><p></p><h4>&nbsp;1. 智能客服场景</h4><p></p><p></p><p>在客服场景中，基于自研的智能在线客服与智能语音机器人，可以实现 7*24 小时回应海量用户需求，智能坐席助手与实时质检辅助人工坐席，在及时响应的同时规范客服话术与行为，不断提升客服质量，保护消费者权益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/60d0394bf229b2039204eb73365dd10f.png" /></p><p></p><p></p><h4>&nbsp;2. 智能营销场景</h4><p></p><p></p><p>微众银行持续升级智能营销解决方案，结合智能语音机器人、联邦学习等技术，在数据不出本地的合规前提下，实现业务营销获客和存量促活，更加精准地触达目标人群。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e9ea3a0f6d529be99444d982912292f.png" /></p><p></p><p></p><h4>&nbsp;3. 智能风控场景</h4><p></p><p></p><p>基于业内领先的人脸识别、声纹识别等技术，在开户、授信、放款等金融服务多个环节把控风险，有效甄别欺诈行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4cf928680339daa84c3e5cb9e46df3f.png" /></p><p></p><p>在每个智能场景中都有大量的真实案例分享，让同学们对 AI 技术在金融行业中的应用有了更深刻的认识。同时杨经理在最后还提到了参赛选手需要注意的一些高分要点。比如，选择与实际场景相关、国家鼓励或社会热点的研究课题；设计合理的性能评估指标，并阐述清楚评估效果、比对的参考对象等要素；选手要充分利用参考论文、开源代码、导师和评委等资源，帮助自己做出更好的作品等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/9901d59bc9c6c7671041bb41b8ef7201.png" /></p><p></p><p></p><h2>三、微众银行个人直通银行部室经理金虎光：“银行对话式交互服务的探索”</h2><p></p><p></p><p>银行业传统的线下柜台服务好处是银行职员与客户直接互动，可以处理较为复杂和个性化的问题，也更容易发展信任关系，但网点服务存在地点和工作时间限制，交易速度也比较缓慢。近年来兴起的线上远程服务则希望通过各种技术手段为客户带来随时随地、方便快捷的体验，同时尽可能做到像线下一样可以处理复杂、个性化的问题。从电话银行到网上银行、银行 App 再到虚拟数字人服务，银行正在努力将线上数字银行打造成新的增长点，提升金融服务体验和质量，提升客户经营质效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd6d5fb6ff9d3d49dabb670f89937e42.png" /></p><p></p><p>最近火热的生成式 AI 技术可以贯穿从市场、销售到运营、研发、风控的所有银行服务。金虎光对此也为参赛同学提出了建议，做产品经理课题时可以基于上图中的这些环节思考解决方案，例如利用 AI 实现精准获客，根据用户画像生成个性化营销文案，进行个性化定价，或者通过生成式 AI 金融咨询，实现更加智能有温度的线上客户服务。</p><p></p><p>此外，生成式 AI 在银行线上交互场景的应用潜力是非常巨大的。银行可以利用大语言模型打造自然语言对话服务，为客户带来更加自然、便捷的线上体验。正因如此，本次大赛产品经理赛道将课题定为《银行线上场景的交互式智能柜台服务》。本题目并不是要简单粗暴地将柜台通过数字人形式搬到线上，而是希望同学们基于对话式交互服务的能力，在客户全场景下提供更智能、更贴心、更便捷的银行服务。</p><p></p><p>如今各家银行的线上服务都已经包括了几乎所有银行服务功能，但随着功能指数级增长，客户的线上交互也变得非常复杂。每家银行都有多个 App，如何让客户更方便地找到所需功能是银行面临的普遍挑战。这里金虎光举了三家银行的网上银行作为案例，如 AI 助理模式、数字人模式、虚拟空间模式等，并提到微众银行也有类似的探索，参赛同学可以在这些模式的启发下，根据自己选择的场景做出更创新的模式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/573d15d1b590d6d8d1170808691138ce.png" /></p><p></p><p>金虎光强调，产品经理赛道最关注的是作品创新性，比较看重选手的想法是否有足够的亮点。其次比较关注的是作品的商业价值，这里指的是选手的想法基于用户的场景或痛点，是否能确实解决用户的某些需求，同时带来良好的社会价值。此外，作品的完整性、是否有市场和用户分析、竞品分析，对想法的解释和逻辑理解都是非常重要的。金虎光提示所有参赛同学要避免方案大而空。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cda85a94155622d693d37da57c90f533.png" /></p><p></p><p>金虎光在分享的最后帮助同学们总结了一些本届大赛产品经理赛道的获胜秘籍，“首先要关注用户是谁，然后要从服务提供方的角度关注具体的场景和用户需求，提供怎样的解决方案，产生怎样的用户价值，这些维度都是选手要重点考虑的。并且银行业务还要考虑基础的安全性、用户信息保护、用户核身的问题，针对这些问题提出的解决方案也会受到评委欢迎。”</p><p></p><p></p><h2>QA 环节精华问题整理</h2><p></p><p></p><p></p><h4>区块链赛道</h4><p></p><p></p><p>Q：区块链赛道更看重创意还是作品完整度？</p><p>A：都看重，完整度是基础，创意一定要有，好的创意会有加分。有同学问区块链 +ESG 的含义是什么，有实力的参赛选手是知道如何解决这个问题的。例如，服务人群、向善的事情都是 ESG，做坏事肯定不是 ESG。</p><p></p><p>Q：如何确保自己的区块链设计更可信、更安全，大赛会看重系统安全性吗？</p><p>A：安全性非常重要，更安全的系统会有很大加分。区块链技术本身是很安全的，但如果放到链上的内容是不可信的，也不会因此就变成可信的。所以如何解决这个问题也是展示实力的途径。</p><p></p><p>Q：有哪些获得高分的秘籍？</p><p>A：一些小技巧，比如 PPT 更规范、讲述更清晰不超时、将最新技术用于所关注的问题中，都是评委看重的。</p><p></p><p>Q：作品技术文档和展示材料有何区别？前者是否有固定格式和必须包含的内容？</p><p>A：这些材料都有官方模版提供，选手照做填写即可。技术文档重点看技术先进性，看作品优势、亮点、解决的问题，可以有数十页，但展示 PPT 一般较短，更考验沟通和演讲水平。</p><p></p><p></p><h4>人工智能赛道</h4><p></p><p></p><p>Q：FATE 框架是否开源？</p><p>A：是开源的，本届大赛官网和微众银行官网也提供了联邦学习的相关资料供下载。</p><p></p><p>Q：作品与金融场景不是特别相关可以吗？</p><p>A：不限定金融场景，其他场景都可以。比如医疗、养老、电力，国家重点扶持的项目或者社会热点项目可能会是加分项。</p><p></p><p>Q：想做的作品很大，但时间来不及只做一部分可以吗？</p><p>A：作品需要自圆其说，有逻辑有条理才能得到认可，如果甚至不能说服自己肯定是不行的。</p><p></p><p>Q：去年的作品迭代后在今年参赛可以吗？</p><p>A：可以参赛，去年没获奖打动不了评委的产品今年可能也难以获奖。建议最好有新的、好的创意加入到作品中，这样获奖概率才会增大。</p><p></p><p></p><h4>产品经理赛道</h4><p></p><p></p><p>Q：产品经理项目需要做出具体的应用吗？</p><p>A：本赛道要求产品方案的 Word 文档和产品文档 PPT（初赛只需要Word文档），文档中包含方案的文字描述。评委关注技术和整体的交互，如果有余力可以使用动画或者视频来展示。具体的应用是锦上添花，不做强求。</p><p></p><p>Q：比赛中提到的银行产品是指银行的业务板块还是一个银行 App？</p><p>A：评委想看到选手在现有的银行 App 下如何改进交互服务，例如通过 AI 助理或数字人模式在线上提供便捷服务。</p><p></p><p>Q：金融客户的全场景陪伴目前有哪些痛点？</p><p>A：客服就是一大痛点，例如用户每月固定时间转账前，银行或许可以通过智能客服提供提醒，这就是一个解决方案。这类痛点很多，目前的智能技术只能解决一小部分需求。</p><p></p><p>Q：本次命题作品一定要基于微众银行或其他银行的 App 产品吗？</p><p>A：不是，选手可以选择任意一家自己熟悉的银行，选择自己感兴趣的场景来做设计。基于微众银行的 App 不会有额外加分。评委希望看到一种线上服务的解决方案，可以是交互，也可以是某个功能，这是一个开放命题。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wCNzNg7tc5yOAB7o4XhR</id>
            <title>华为云开发工具和效率领域首席专家王亚伟，确认担任 QCon 智能化信创软件 IDE 专题出品人</title>
            <link>https://www.infoq.cn/article/wCNzNg7tc5yOAB7o4XhR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wCNzNg7tc5yOAB7o4XhR</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 09:20:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 智能化信创软件 IDE, 王亚伟, 自有技术内核
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，华为云开发工具和效率领域首席专家王亚伟将担任智能化信创软件 IDE 的专题出品人。在此次专题中，将介绍智能化信创软件 IDE 的架构和标准，以及其与人工智能的关系。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1031&amp;utm_content=wangyawei">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。华为云开发工具和效率领域首席专家王亚伟将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1031&amp;utm_content=wangyawei">智能化信创软件 IDE</a>"」的专题出品人。在此次专题中，你将了解到智能化信创软件 IDE 的基于自有技术内核的架构和标准，以及 AI 原生的两大特征。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1031&amp;utm_content=wangyawei">王亚伟</a>"，华为云开发工具和效率领域首席专家，华为软件开发生产线 CodeArts 首席技术总监，当前领导一支国际化软件专家团队负责 CodeArts IDE 系列产品的研发和华为云开发者生态能力建设。加入华为前，曾任微软开发者事业部资深开发经理，在微软全球多个国家地区工作 13 年。近 20 年的云和开发工具的行业经验让他具备从底层技术、产品规划到开发者生态能力建设洞察的能力。王亚伟先生发表和被授予 20 多项软件开发技术相关的发明专利。QCon 全球软件开发大会（上海站）2022 出品人。</p><p></p><p>相信王亚伟的到来，可以帮助提升此专题的质量，让你学习到，智能化信创软件 IDE 如何将基础软件开发工具的核心技术实现自主可控，在拥抱开源的同时逐步建立基于自有技术内核的架构和标准，形成自有开放生态，以及内核架构如何无缝融入人工智能。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9de076cb6003669df743b02daac3c00c.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vieTybwOJv3JKQZvgmrJ</id>
            <title>AIGC 时代，如何提升端侧算力利用效率？</title>
            <link>https://www.infoq.cn/article/vieTybwOJv3JKQZvgmrJ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vieTybwOJv3JKQZvgmrJ</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 08:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, AI大模型热潮, 算力需求, AIGC, 端侧算力利用效率
<br>
<br>
总结: 近年来，ChatGPT等AI大模型的兴起引发了算力需求的爆发，如何高效利用算力成为关注焦点。在大规模AI模型训练中，提升算力利用效率的技术和方法以及AIGC应用下沉到终端成为重要议题。英特尔中国技术部总经理高宇分享了如何提升端侧算力利用效率的主题，探讨了生成式AI技术的发展与挑战，以及算力成本居高不下的问题。他提出了分布式和层次化的推理部署思路，建议在云端进行大规模训练，将不同类型的AI推理算力下沉到边缘侧，以提高算力利用效率。 </div>
                        <hr>
                    
                    <p>ChatGPT 的爆火掀起了 AI 大模型热潮，也进一步拉动了算力需求的爆发，面对呈指数级增长的算力需求，如何用得起、用得上、用得好算力成为大家普遍关心的问题。那么，在大规模 AI 模型训练中，如何保证算力的高效利用？有哪些技术或方法可以提升训练的效率和稳定性？AIGC 应用如何下沉到终端？近日，InfoQ《极客有约》邀请到了英特尔中国技术部总经理高宇，为大家分享《AIGC 时代，如何提升端侧算力利用效率？》。</p><p></p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/w4UPiNImmKac6OSgpEiP">https://www.infoq.cn/video/w4UPiNImmKac6OSgpEiP</a>"</p><p></p><p>姜雨生：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，微软软件工程师姜雨生。本期直播，我们邀请到了英特尔中国技术部总经理高宇老师来给我们做分享。今天的直播主题是《AIGC 时代，如何提升端侧算力利用效率？》。先请高宇老师给大家做一个简单的介绍。</p><p></p><p>高宇：InfoQ 的朋友们，大家晚上好。我是高宇（Gary Gao），来自英特尔中国，负责英特尔中国技术支持团队的工作。今天，我非常荣幸与大家分享关于在端侧实现 AIGC 的热门话题。</p><p></p><h2>生成式 AI 技术的发展与挑战</h2><p></p><p></p><p>姜雨生：去年推出的 ChatGPT 引起了广泛关注，掀起了大型 AI 模型的热潮，企业和个人对算力的需求呈现出爆发性增长。这轮 AI 算力需求的爆发给您带来最大的感受是什么？行业发生了哪些变化？</p><p></p><p>高宇：这一轮生成式 AI 热潮确实代表了技术上的一个重大突破，无论是给消费者、商业客户还是数据科学家，都带来了巨大的潜力和影响。从去年 ChatGPT 3.5 正式发布以来，它展示出的智能和生成文本的能力让整个学术界、消费市场和最终用户都感到震惊。在短时间内，ChatGPT 3.5 已成为全球最受欢迎的应用之一，这一成就令人印象深刻。我认为，它对整个行业的影响可以从正面和挑战两个维度来分析。</p><p></p><p>从正面来看，首先，生成式 AI 极大地改善了用户体验。以前的搜索引擎和智能问答系统在知识方面相对固定，而生成式 AI 具有强大的学习和涌现能力，这是以前所没有的。因此，用户体验得到了显著改善。</p><p></p><p>其次，它激发了学术界和企业界对这项技术的研究兴趣。在过去的半年里，全球企业和知名的学术机构都大量投入到生成式 AI 的研究中。这种巨大的资金和智力投入使我们相信未来几年生成式 AI 的发展将非常迅猛，因为许多人都在进行相关研究和突破。</p><p></p><p>第三，我们看到生成式 AI 目前主要应用于人机对话，但我们更看好它在各种行业中，尤其是垂直行业中的应用潜力。例如，目前人们正在探讨用于医疗领域的大型模型，专为银行系统设计的大型模型，甚至为金融等垂直行业开发的模型。因此，我们对它在这些领域的应用前景非常期待。</p><p></p><p>当然，大型模型的出现和生成式 AI 的发展确实带来了一些重要挑战。在这方面，我们可以总结为以下几点。</p><p></p><p>首先，几乎所有大型科技公司都加入到了这个浪潮中。因此，这个领域的应用进展非常迅速，有时候可能会出现一些重复性工作，甚至资源浪费。</p><p></p><p>第二，数据隐私和可靠性是一个重大问题。个人数据的保护以及互联网上的开源内容如何得到保护都是重要考虑因素。此外，还涉及到更深层次的问题，例如对问题的解释、价值观的取向和正确判断等，这些都是全新的挑战。</p><p></p><p>英特尔倡导的 AI 不仅关注性能和能力，还强调负责任的 AI。这也是领先厂商共同的理念，即人工智能的发展应该以对社会负责任的态度为基础。总之，生成式 AI 对我们行业带来了重要冲击，后续我们可以深入探讨这些挑战的细节。</p><p></p><h2>算力成本居高不下，如何找到破解之法？</h2><p></p><p></p><p>姜雨生：无论是模型训练还是模型调用，计算资源的需求都在不断增加。这背后伴随着高昂的成本，对许多企业而言，这成为了业务扩展的一道巨大障碍。您怎么看算力贵这一现象？随着技术的发展，算力贵的现状会有所改善吗？</p><p></p><p>高宇：目前，大家都不得不承认算力成本有待解决。因此，大家都对这个行业的情况非常关注。我们可以分析一下导致算力成本上升的原因。</p><p></p><p>首先，运行生成实验，特别是训练模型所需的 GPU 性能相对较高，因此整个 GPU 以及 GPU 卡的成本较高，它需要更大的 GPU 芯片来提供更高的算力。此外，它还需要更快的内存，通常采用 HBM（High Bandwidth Memory，高带宽内存）内存架构，这也增加了成本。再加上需要用 8 卡互联的训练机，整机的物料成本非常昂贵，这是导致成本高昂的原因之一。</p><p></p><p>第二，与之前提到的问题相关，现在几乎所有人都涌入了这个行业，导致了短期内供大于求的情况。一度出现了 GPU 卡供不应求的情况，这已经从去年年底开始，需求量大但供应相对不足。</p><p></p><p>第三，整个大型 GPU 服务器或智算中心的运营成本极高，包括场地和能源消耗。一个标准的 GPU 服务器机柜功耗至少为 30 千瓦，而大多数数据中心机柜通常只能达到 10 千瓦到 20 千瓦之间，无法满足 30 千瓦的要求，这也增加了成本因素。</p><p></p><p>当然，我们还需要考虑一点，因为生成式 AI 仍处于早期阶段，所以在许多算法优化和资源利用方面还有改进的空间。因此，有望在未来降低算力成本。</p><p></p><p>姜雨生：在目前算力贵这个方向，英特尔目前有哪些相关的解决方案，这面方便给我们大概介绍一下吗？</p><p></p><p>高宇：我们需要思考一个根本性问题，即如何应对昂贵的算力这一行业性的难题。我们有几个想法，虽然稍后我们还会谈及产品方面的问题，但现在我们首先想从行业角度提出一些大的思路。</p><p></p><p>首先，我们认为当前的推理部分应该更加分布式和层次化，充分利用云、边缘和终端的不同层次来部署推理算力，以充分发挥算力性能。具体来说，我们的建议是在云端进行大规模的训练，这是云侧的任务。此外，云侧适合大集群训练，部署超大型模型，例如 ChatGPT 等超过 100 亿的模型。第三，云侧适合部署高并发的场景，即当用户数量庞大时，需要同时满足所有客户的需求，这也需要云端来实现。</p><p></p><p>对于不属于以上几种情况的 AI 推理算力，我们建议将其下沉到边缘侧。如今，运营商和企业都拥有许多边缘侧数据中心，虽然这些数据中心规模较小，机器配置的算力相对较低，但足以支持多种类型的大型模型的推理。根据我们的判断，大约在 10 亿到 30 亿之间的模型可以考虑部署在边缘侧，因为边缘侧可以使用性能稍微较低端的 GPU 卡或 CPU 进行推理，性能足够。此外，在边缘侧部署可以提供更好的低延迟体验，成本也较低。</p><p></p><p>下沉的第二步就是把它部署在端侧。我们认为一些规模较小的模型，比如小于 10 亿参数的模型，经过一定的优化和量化，以及低精度的比特量化后，完全可以部署到个人计算机（PC）或虚拟私有云（VPC）等设备上。将其部署到端侧带来两个明显的好处。首先，它的性能延迟是最低的，因为不需要经过网络传输，减少了任何网络延迟。此外，边缘侧部署还有一个重要的优势，即对个人隐私的最大程度保护，因此数据泄露的风险几乎不存在。因此，从大的原则上讲，我们希望将大型模型转化为云、边缘和终端三层协同的架构，这应该是未来发展的趋势之一。</p><p></p><p>姜雨生：有观众提问，在算力优化方面，我们业界还有没有一些通用的方案？</p><p></p><p>高宇：我们了解到，在当前的研究领域中，一个备受关注的通用方案是针对低比特量化的优化。目前，大多数部署在云端的模型采用的是 FP16（16 位浮点数）的精度。然而，如果要将模型部署在边缘侧或终端侧，通常的做法是首先将其量化为 INT8（8 位整数），然后可以进一步将其量化为更低比特位，如 INT5、INT4 或 INT3，这都是可能的，而且我们看到在这方面行业已经取得了一些显著的进展。</p><p></p><h2>AIGC 应用如何下沉到终端？</h2><p></p><p></p><p>姜雨生：我认为开发者会积极采用 AIGC 的大型模型，因为这是未来的趋势。在过去，我们主要在云服务器上运行 AIGC 应用，包括我自己目前使用的一些 Azure 云上的产品。但云端 AI 也存在延迟和各种限制等方面的一些短板。那么，AIGC 应用有下沉到终端的可行性吗？</p><p></p><p>高宇：根据我们目前的研究成果，我可以告诉大家，针对英特尔的最新平台，也就是第 13 代（以及后续推出的第14代，采访时第14代酷睿尚未发布）酷睿处理器家族，我们已经取得了非常不错的优化结果。这个平台不仅适用于笔记本电脑，还包括台式机。我相信许多开发者和用户在购买电脑时都会选择最新的酷睿平台。</p><p></p><p>以第 13 代酷睿平台为例，我们的优化结果可以使模型从 7 亿参数到 18 亿参数都能够流畅运行。特别是在 7 亿到 13 亿参数范围内，性能效果非常出色，即使超过 13 亿参数，模型也可以运行，尽管速度稍慢，但我们认为基本上也可以满足用户的需求。当然，我们目前的优化主要是在 CPU 上进行的，但下一步我们将充分发挥平台内的集成显卡（IGPU）能力，以进一步提升速度。</p><p></p><p>此外，对于未来，我想提到最近引起广泛关注的一项重要消息，那就是我们披露了英特尔即将发布的下一代平台，内部代号为 Meteor Lake，正式品牌叫做 Core Ultra。这个平台不仅具有强大的 CPU 算力，还将 GPU 算力提高了一倍，因此GPU算力非常强大。另外，它还内置了专用的 AI 加速器（NPU），可以提供超过 11 tops 的峰值算力。因此，在下一代平台上，我们将能够充分利用三种计算资源，包括 CPU、GPU 和 NPU 的算力，以实现更出色的性能。这是我们下一代平台的亮点，敬请期待。</p><p></p><p>姜雨生：英特尔之前提出在 PC 端侧跑 AIGC 应用，具体是如何实现的？在软硬件层面是如何提升算力利用效率，实现算力优化的？</p><p></p><p>高宇：我来简要介绍一下我们目前正在发布的开源框架，它叫做 BigDL，是专门为英特尔的处理器和 GPU 开发的一个低比特量化框架。感兴趣的观众可以进入在 GitHub(https://github.com/intel-analytics/BigDL)上查看，下载我们的 BigDL 开源代码，进行实验。</p><p></p><p>BigDL 有一些显著特点。首先，它支持低比特量化，从 INT8 到 INT5、INT4、INT3 等各种低比特的数据精度，从而提供更好的性能，并减少内存占用。这一点尤其重要，因为在边缘计算领域，除了性能挑战之外，内存也相对较低，所以低比特量化是解决这个问题的一种方法。</p><p></p><p>此外，BigDL 支持多种平台，包括英特尔的各种 CPU 系列，从 Xeon 处理器到酷睿处理器等。它还支持英特尔的各种 GPU 系列，包括英特尔 Flex 系列用于数据中心的专用显卡以及英特尔锐炫（ Arc） 系列面向消费者的显卡。</p><p></p><p>姜雨生：我也确实感受到了在个人电脑上运行大型模型以及进行内容生成的可能性，特别是在我的个人电脑上装备了这些硬件的情况下。实际上，我也想了解一下一些相关的技术，如果要大规模普及，关键的主要指标可能是颠覆，即用户在他们的实际工作和生活中所体验到的变革。那么AI 能够在端侧带给用户哪些具体的体验提升？</p><p></p><p>高宇：从我们现在的观察来看，大型模型在端侧用户领域可能有几个可能的应用场景。首先，大型模型可以成为每个用户的个人超级助手。这种大型模型可以在云端运行，同时也可以通过我们刚刚提到的低比特量化技术在个人电脑上运行，从而提供更好的用户体验。这是第一个应用场景。</p><p></p><p>第二，它可以用于文档处理，包括提取文档的核心思想和纠正文档中的语法错误等任务。对于这种应用场景，更适合将模型部署在端侧，因为许多文档包含一些个人属性，用户可能不愿意将其上传到云端。</p><p></p><p>第三，我们观察到大型模型，特别是 Diffusion 模型，在图像生成方面具有出色的能力，这对于许多设计师来说是一个强大的工具。许多图形、图像和三维设计公司积极采用 Stable Diffusion 以及相关衍生模型，以帮助设计师生成各种图片和画面，从而实现事半功倍的效果。</p><p></p><p>姜雨生：将 AIGC 相关应用以预装软件的方式适配到未来的电脑中，是否是 PC 创新的一个新方向？它对于 PC 应用效率的提升是否有着大幅超越以往的预期？</p><p></p><p>高宇：当然，答案是肯定的。在未来的个人电脑上，无论是笔记本还是台式机，它们的算力已经足以支持像 7 到 13 亿级别的大型语言模型在本地运行。这种潜力已经存在，接下来我们可以期待不同的商业模式的出现。</p><p></p><p>首先，我们可能会看到一些商业软件集成了中小型大语言模型，将其变成了生成式人工智能的专业商业软件。这些软件还有可能集成了 Stable Diffusion 等功能，从而成为一种可用于文本生成和其他工作流程的商业软件。因此，可以期待在桌面平台上出现集成生成式人工智能能力的商业软件，这是一个可能的落地方式。</p><p></p><p>另外一种方式是鼓励更多的 OEM 制造商，也就是个人电脑的品牌制造商，为自己的产品开发专门针对硬件优化的生成式人工智能软件，并将其预装在他们的电脑上，以提高最终用户的体验，使电脑更易于使用和更具趣味性。这种辅助性软件可以提升用户的使用体验，增加趣味性，我认为这也是一个非常有潜力的方向。</p><p></p><h2>端侧运行大模型存在哪些挑战？</h2><p></p><p></p><p>姜雨生：有观众提问，端侧跑这些大模型有没有一些难点我也比较关注这个问题，端侧跑大模型有没有一些相对不适用的场景或内容？</p><p></p><p>高宇：端侧与云侧相比，目前存在两大限制。首先，端侧的计算能力明显不如云端强大。这是显而易见的。第二，端侧的内存相对有限。当前，笔记本电脑和 PC 的主流配置通常为 16GB 内存。明年我们可能会看到更多配置为 32GB 内存的 PC，但即使是 32GB 内存，相对于云端来说，内存仍然有限。因此，端侧需要应对以下两个主要挑战。</p><p></p><p>首先，模型的参数量需要受限，通常在 130 亿以下。其次，必须进行低比特量化，这是一种必不可少的手段。经常有人问一个常见的问题，即将一个 FP16 模型量化为 INT4 后，精度损失似乎很大，这对大型模型的性能会产生什么影响？我们目前的基本结论是，在大型语言模型的情况下，从 FP16 到 INT4 后，回答问题的质量会略微下降，但下降幅度并不是很大。如果我们使用评分机制，原来的模型可能是 85 分的模型，经过量化后，可能会下降到 82 分左右，所以大致是一个个位数的质量下降。但是在内存方面，收益是非常大的，这是一个权衡。</p><p></p><p>然而，对于 Stable Diffusion 模型而言，如果将 FP16 量化为 INT8，整个图像生成的质量下降会比较大。因此，对于运行稳定扩散模型的端侧，我们仍然坚持使用 FP16。幸运的是， Stable Diffusion 模型的参数量不是很大，因此即使在端侧，FP16 的性能也完全可以胜任。</p><p></p><p>姜雨生：在端侧执行一些生成式内容和场景时，精确度并不是特别重要，尤其是对于一些模型复杂度不太高的情况来说，这种方式会更加合适。下一步，英特尔有哪些技术探索和产品规划呢？有哪些技术难题是我们在未来需要解决的？</p><p></p><p>高宇：对于英特尔未来的产品规划，目前英特尔在生成式 AI 领域有几个主要的产品家族，可以从云端、边缘和端侧三个维度来介绍。</p><p></p><p>在云端，英特尔的关键产品是 Gaudi2，这是 英特尔Habana最新推出的产品。Gaudi2 具有非常高的算力性能，它还具有大容量的显存，目前 Gaudi2 的配置为 96GB 的 HBM2 显存，因此可以容纳更多的模型。此外，英特尔还推出了专门针对中国市场定制的 Gaudi2 中国版本。云端英特尔还有一款产品叫做 Xeon HBM，它是一款针对大模型推理而设计的 CPU，内置了 64GB 的 HBM2 高速内存，这对于大型语言模型的推理性能提升非常有帮助。</p><p></p><p>边缘侧，英特尔推出了两款显卡产品，一款是英特尔 Flex 系列，另一款是锐炫（ Arc） 系列。Flex 系列是为数据中心和服务器设计的无风扇 GPU 产品，而 Arc 系列则是面向消费者市场的显卡，在算力方面也非常强大，可以满足边缘侧推理的要求。这些产品将为边缘侧大模型推理和 Stable Diffusion 提供强大的支持。</p><p></p><p>总的来说，英特尔在生成式 A I领域有一系列强大的产品，覆盖了云端、边缘和端侧，为不同应用场景提供了多样化的解决方案。</p><p></p><p>姜雨生：有观众提问，端侧模型跟云端模型有可以配合的方式吗？</p><p></p><p>高宇：端侧模型和云端模型可以进行协同配合，一种可能流行的做法是由端侧模型进行问题的初步预判断。这个端侧模型可以是相对轻量级的，用于判断用户问题的导向方向。如果这个初步判断结果显示性能足以在端侧大模型上运行，那么模型可以在端侧执行。但如果判断需要更强大的计算能力，那么就可以将任务传递到云端进行更大型的模型推理。这种方式可能比较容易实现，因为它避免了对同一个模型进行拆分，尽管拆分模型也是一种可能的方式，但会更加复杂。</p><p></p><p>姜雨生：如果希望在个人电脑上运行之前所描述模型相关的内容，最低配置要求如何？</p><p></p><p>高宇：关于个人电脑的配置，主要取决于您的耐心和使用场景，当然这是个半开玩笑，但基本上，为了达到基本的用户体验要求，我们建议以下配置：</p><p></p><p>处理器（CPU）：最好选择第 13/14 代酷睿处理器，尤其是选择 I7 或更高级别的型号。如果有预算，并且想要更出色的性能，选择 I9 处理器会更好，正如我在之前的演示视频中展示的那样。内存（RAM）：至少 16GB RAM 是起点，但更好的选择是 32GB RAM。此外，要注意内存的速度，因为现在的内存，尤其是 DDR5 内存，速度范围从入门级的 5677 MHz，一直提升到高达 7233 MHz。内存速度越快，性能表现通常越好。再次强调，大型模型通常对内存带宽要求较高，因此提高内存带宽会带来更大的性能收益。散热设计：除了硬件配置，还要考虑系统的散热设计。良好的散热设计可以让 CPU 在 Turbo 模式下更长时间地运行，从而提高性能表现。</p><p></p><p>选择适合需求的个人电脑配置是一个综合考虑的过程。明年新发布的电脑新品通常会公布其运行大型模型的性能指标，用户可以根据厂商提供的指标来选择适合自己需求的配置，这应该会更准确地满足你的期望。</p><p></p><p>当然了，我认为目前大模型仍然存在一些挑战，尤其是在处理模型的一些幻觉问题方面，这个问题在整个行业中仍然是一个难点，需要不断攻克。</p><p></p><h4>嘉宾介绍</h4><p></p><p></p><p>特邀主持：</p><p></p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p></p><p>嘉宾：</p><p></p><p>高宇，英特尔中国技术部总经理，负责领导英特尔中国从端到云的产品技术使能和方案支持工作，对中国IT产业和生态链、以及前沿技术发展趋势有着深入的洞察和见解。&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TprcKvXyB5fsKgC4SaLx</id>
            <title>程序员的私人助理：Amazon CodeWhisperer</title>
            <link>https://www.infoq.cn/article/TprcKvXyB5fsKgC4SaLx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TprcKvXyB5fsKgC4SaLx</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 08:09:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 编程, AI 辅助编程, Amazon CodeWhisperer, 生产力
<br>
<br>
总结: 编程是一项有趣而又富有挑战性的工作，但也会遇到很多困难和繁琐的任务。AI 辅助编程工具 Amazon CodeWhisperer 可以帮助开发者提高生产力和代码质量，它是基于亚马逊内部使用的 AI 编程助手的经验和技术而开发的。使用 CodeWhisperer，开发者可以节省时间和精力，快速完成编程任务，提高代码的可读性和可维护性，增强代码的安全性，并跟踪开源代码的来源和许可信息。 </div>
                        <hr>
                    
                    <p>编程是一项有趣而又富有挑战性的工作，但是也会遇到很多困难和繁琐的任务。有没有一种方法可以让编程变得更容易，更快，更安全呢？答案是有的，那就是 AI 辅助编程。</p><p></p><p>在这篇文章中，我将介绍一款由亚马逊推出的 AI 辅助编程工具——<a href="https://www.infoq.cn/article/JcIQOLpgqVK3AAgQxNQt?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Amazon CodeWhisperer</a>"，它是如何帮助开发者提高生产力和代码质量的，以及我使用它的一些体验和感受。</p><p></p><p>Amazon CodeWhisperer 是在 2021 年 12 月正式推出的一款 AI 代码生成器，它是基于亚马逊内部使用的 AI 编程助手的经验和技术而开发的。推出之际，Amazon 邀请了一些开发者参与一个生产力挑战，结果显示使用 CodeWhisperer 的开发者比不使用的开发者更有可能成功完成任务，并且平均速度快了 57%。</p><p></p><p>推出后受到了很多开发者和企业的欢迎和好评，例如 Accenture 就使用 CodeWhisperer 来提高开发者的生产力，包括新人培训，编写样板代码，使用陌生的语言，以及检测安全漏洞等方面。</p><p></p><p>而现在，亚马逊更是大方的开放了个人免费套餐，在个人开发过程中享受 AI 辅助编程的快感。使用下来的体验就像多了一个秘书，而自己从程序员的角色变成了半个产品经理的角色：我只需要口述我想要的功能，它就能帮我生成初版的代码，稍微修改就能实际运行。真正解放了人的思想。</p><p></p><p>它目前支持 15 种编程语言，包括 Python，Java，JavaScript 等，以及多种 IDE，包括 VS Code，IntelliJ IDEA，AWS Cloud9 等。你只需要免费注册并下载 CodeWhisperer 插件，安装到你喜欢的 IDE 中，然后就可以开始使用了。</p><p></p><p>我以 Goland 为例，只需插件市场搜索“CodeWhisperer”进行安装以及登录，便可开始使用了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2661b09aa6dcb87cc44f7b80964f5da.png" /></p><p>​</p><p>插件市场搜索 CodeWhisperer，安装完成后，左下角会有一个 AWS toolkit 的工具栏，点击它并且登录。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc79a255e5f36461fd687630692b86ab.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdb46af8d1dc6e8a4c89f9cd11dc3c72.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24090b58e3b54d55c94081aa340c188c.png" /></p><p>授予权限，权限授予之后，左下角 CodeWhisperer 显示可用状态时，就可以开始编码，享受 AI 辅助编程的快感了。</p><p><img src="https://static001.geekbang.org/infoq/de/de77c9f7c8eaf9dadef10866e0201935.png" /></p><p>​</p><p>比如很经典的斐波那契数列，只需要描述一下函数功能，接下来的事情就是 Tab 键自动输入代码了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/494f69fd1887b906b77650f121704f92.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a8b32d509fc68c47bd0a7744c4cb5c7.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21d567be03fe567fd2de7146143c6cfa.png" /></p><p>​</p><p>共计一行描述，三次 Tab 键，完成了首次 AI 编程辅助。整个使用过程非常简单和自然，你只需要在 IDE 中写下你想要实现的功能的注释，例如“创建一个列表”，“连接到数据库”，“发送一封邮件”等，CodeWhisperer 就会自动给出多个代码建议，你可以选择接受或者继续编写自己的代码。</p><p></p><p>CodeWhisperer 会根据你的代码风格和命名习惯，生成符合你的习惯的代码。你还可以使用 CodeWhisperer 来扫描你的代码，检测并修复安全漏洞，以及跟踪开源代码的来源和许可信息。</p><p></p><p>很多人可能认为程序员的核心能力是写代码，其实并不是。真正的价值是思考，是写代码之前的苦思冥想，最终实现则是水到渠成的事情。而 Amazon CodeWhisperer 带来了什么呢，个人认为其中最主要的是可以提高开发者的生产力和代码质量。使用 CodeWhisperer，可以：</p><p></p><p>节省时间和精力，避免编写重复和繁琐的代码，快速完成编程任务。提高代码的可读性和可维护性，遵循编码规范和最佳实践，减少错误和 bug。更高效地使用 AWS 服务，获取符合 AWS API 的代码建议，轻松构建云端应用。增强代码的安全性，及时发现和修复安全漏洞，防止数据泄露和攻击。代码负责任，跟踪开源代码的来源和许可信息，避免版权纠纷和法律风险。</p><p></p><p>欢迎大家使用，提高程序员的幸福感！</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>