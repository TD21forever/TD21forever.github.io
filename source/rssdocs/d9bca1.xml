<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</id>
            <title>晋升神器：AI 一键生成 PPT，技术好的同时也做好PPT｜InfoQ 用户的双十一福利</title>
            <link>https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/evEep3Yxl4hs8H3g4xSm</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 Nov 2023 04:49:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 升职加薪, 程序员, PPT, 爱设计 AiPPT
<br>
<br>
总结: 程序员晋升需要展示过去一年的成绩和未来规划，PPT的制作水平成为考验。爱设计AiPPT是一款智能的PPT生成工具，通过人工智能和自然语言处理技术，能够快速生成符合需求的PPT内容。它支持文档上传生成、在线自由编辑、云端存储和兼容.pptx格式等特点，让程序员能够通过高效、智能的PPT呈现自己的思维和观点，提升晋升机会。 </div>
                        <hr>
                    
                    <p>升职加薪自然是每位职场人都渴望的事情，程序员们也不例外。但是，晋升就需要汇报过去一年的主要成绩，并介绍自己接下来的具体规划，这时候就要考验各位的 PPT 水平了。过去几年，程序员的圈子里广泛流传着一句话：技术再好，不如 PPT 做得好！</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62558b508f91e83607cc773353ba54dd.jpeg" /></p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzODQ3NDQ4Mw%3D%3D&amp;chksm=e9398033de4e0925f3cb70287083d06fe9abae4e15a4999ad3e049039603fbaee9e26afe3ddf&amp;idx=1&amp;mid=2247484847&amp;scene=27&amp;sn=e7f72229a61027479fddf81828a9697d&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">PPT </a>"不仅仅是把想说的话呈现出来，最重要的其实是背后的抽象总结能力和逻辑思维能力。其次才是选择合适的框架将内容有序组织起来，最后才是美化。</p><p></p><p>一份制作良好的 PPT，不仅能够清晰地呈现你的思维和观点，更能够让你在众多竞争者中脱颖而出。但是，繁琐的制作过程，费时的排版，以及无法完全达到期望的效果，都是制作 PPT 时所面临的困扰。现在，爱设计 AiPPT 将为广大程序员解决这些问题，让大家在技术好的同时通过有效的内容呈现，获得晋升！</p><p></p><p></p><p></p><p>据悉，<a href="https://mp.weixin.qq.com/mp/wappoc_appmsgcaptcha?poc_token=HNhuSGWjyZFUhGN608tMnwAr-8P6v2nBNFTplteR&amp;target_url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU1NDA4NjU2MA%253D%253D%26chksm%3Dfbeb1b50cc9c9246033e7ad2c008c78f34fdb2d3ee0472f79890f09d9562a7b698ca46972ba1%26idx%3D2%26mid%3D2247574175%26scene%3D27%26sn%3D215f4d6a1899210cba77401d5274a635%26utm_campaign%3Dgeek_search%26utm_content%3Dgeek_search%26utm_medium%3Dgeek_search%26utm_source%3Dgeek_search%26utm_term%3Dgeek_search#wechat_redirect">爱设计</a>" AiPPT 是一款真正智能的 PPT 生成工具。它运用了人工智能技术与自然语言处理两项技术，能够智能理解用户输入的主题，并快速生成符合需求的 PPT 内容。无论是文字、图片、表格，还是图表，都能够以最短的时间为你呈现出一份专业而精美的 PPT。</p><p></p><p>具体来说，爱设计 AiPPT 具有以下特点：</p><p></p><p><a href="https://www.infoq.cn/article/understand-huawei-ai-strategy?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AI</a>" 一键智能生成。基于人工智能和自然语言处理技术，能智能分析用户输入的主题，并快速生成符合需求的 PPT 内容</p><p></p><p>2.文档上传生成。支持多种文件格式上传（doc、docx、xmind、mm），一键上传，AI 智能排版配色、快速生成 PPT</p><p></p><p>3.在线自由编辑器。支持一键整体更换模板、更换配色，内置上千套定制级 PPT 模板及超 10w+ 素材，只需拖拉拽即可快速修改</p><p></p><p>4.云端存储，跨设备同步。PPT 云端制作在线保存，无需下载，打开网站即随时随地开启创作和演示，跨设备不再是障碍</p><p></p><p>5.兼容.pptx 格式，支持源文件导出。支持 JPG、PNG、PDF、PPT 文件导出，PPT 源文件格式导入导出均无格式错乱问题</p><p></p><p>在流程上，用户只需要在 PC 端登录 aippt.cn，输入你的要求和目标，它就会自动生成脑图，帮助大家想清楚整体规划，这个脑图还可以直接增减修改，确定好内容结构大纲后，就可以自动生成对应的 PPT 文件，还支持更换 PPT 风格等。</p><p></p><p>值此双十一之际，InfoQ 为广大用户推出了专属福利，现在通过专属渠道（链接：https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong）购买爱设计 AiPPT 一年仅需 89 元！这是一个难得的机会，让你在享受高效、智能的 PPT 生成服务的同时，还能够节省大量的时间和精力。</p><p></p><p>与此同时，InfoQ 联合极客时间推出了《PPT 设计进阶 · 从基础操作到高级创意》的课程，帮助大家基于爱设计 AiPPT 完成 PPT 制作，课程 + 工具打包购买将享受更多优惠，仅需 129 元（链接：https://shop18793264.m.youzan.com/wscgoods/detail/3nu6184aqcboong）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04c0d719c29a36eec3ff0257fba4bd88.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</id>
            <title>字节宣布除夕放假、连放9天，不占年假；印度“IT业之父”要求年轻人每周工作70小时；Redis 创始人用 C 语言编写出最小聊天服务器｜AI一周资讯</title>
            <link>https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wDflTumrr5pfJReMYRz6</guid>
            <pubDate></pubDate>
            <updated>Sun, 05 Nov 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 聊天服务器, Redis, 创始人, C语言编写
<br>
<br>
总结: Redis创始人Salvatore Sanfilippo使用C语言编写了一个核心代码仅300多行的聊天服务器项目Smallchat。这个项目是他给前端开发朋友的系统编程示例，实现了用户自定义昵称。
 </div>
                        <hr>
                    
                    <p></p><h2>资讯</h2><p></p><p></p><h4>Redis 创始人用 C 语言编写最小聊天服务器 Smallchat，核心代码仅 300 多行</h4><p></p><p></p><p>11 月 2 日消息，知名数据库缓存工具 Redis 的创始人 Salvatore Sanfilippo（网名 antirez）在 GitHub 上传了一个名为 Smallchat 的聊天服务器项目，用 C 语言编写了一个核心代码仅 300 多行的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ae812c0ab4c9598c5cb3122d70eebe7.png" /></p><p></p><p>在项目介绍中表示，这只是他给几个前端开发朋友的系统编程示例，尽自己所能写出来的最小聊天服务器，核心代码（不算空格和注释）仅有 200 多行，甚至实现了用户自定义昵称。</p><p></p><h4>大疆否认被罚60亿美元：案件仍在审理，未有更新的判决信息</h4><p></p><p></p><p>近日，网传美国将“大疆专利侵权”的处罚金额从此前的2.79亿美元上调至60亿美元（约440亿元人民币），对此大疆相关负责人回应称，该案件在今年四月陪审团的裁决金额是2.789亿美元，案子仍在审理过程中，截至目前并没有更新的判决信息。</p><p></p><p>据此前中证报报道，2021年，美国航空航天企业德事隆公司以侵权专利为由对大疆提起专利诉讼，要求大疆共支付3.67亿美元（约合25亿元人民币）的赔偿。今年4月份经过陪审团审理，大疆侵犯了美国无人机公司德事隆的两项美国专利，需分别赔偿3070万美元、2.482亿美元，合计2.789亿美元（约20亿元人民币）。</p><p></p><h4>vivo自研蓝河操作系统不兼容安卓应用，副总裁称AI大模型投入无上限</h4><p></p><p></p><p>11月1日消息，在2023 vivo开发者大会上，vivo自研蓝河操作系统 BlueOS 发布，将在vivo WATCH 3手表首发搭载。在11月1日下午的论坛上，vivo 副总裁周围在接受媒体采访时明确表示，vivo 自研蓝河操作系统不兼容安卓应用。从2023 vivo开发者大会获悉，蓝河操作系统目前已有支付宝、百度地图、喜马拉雅等 App 接入，并兼容 hapjs 快应用标准。</p><p></p><p>此外，vivo副总裁周围表示，vivo大模型现在每年20-30亿的投入成本，人才和设备各占一半，人才成本平均税后100万元。公司对大模型投入定义为高规格投入，目前没有设置上限。</p><p></p><h4>在线办公巨头WeWork将申请破产，估值曾达470亿美元</h4><p></p><p></p><p>据知情人士透露，在线办公巨头WeWork计划最早于下周申请破产。目前，WeWork正考虑在新泽西州根据《破产法》第十一章申请破产。此前，WeWork未能在10月2日向其债券持有人支付利息，然后获得了30天的宽限期。如果在宽限期内仍然无法支付利息，它将被视为违约。</p><p></p><p>WeWork周二表示，已与债券持有人达成协议，在触发违约之前，公司又获得了七天时间与利益相关方进行谈判。据此前报道，WeWork估值一度达到470亿美元。由于“共享办公”公司WeWork认股权证的交易价格“异常低”，纽约证券交易所已暂停其交易，并将启动将其退市的程序。</p><p></p><h4>Windows 11的市场份额已跃升至26%以上</h4><p></p><p></p><p>StatCounter 发布了月度报告，其中包含有关桌面操作系统、搜索引擎、浏览器等的最新数据。根据 2023 年 10 月的报告，Windows 11 的市场份额显著上升，从 9 月份的 23.64% 攀升至 2023 年 10 月的 26.14%。</p><p></p><p>考虑到自 2023 年 4 月以来，该操作系统的市场份额一直保持相对不变，因此本月看到了这款系统的影响力有了明显的增长。</p><p></p><p>虽然 Windows 11 的市场份额似乎并不令人印象深刻，尤其是在其继任者即将到来之际，但微软对其表现还是相当满意的。最近的一份新报告显示，Windows 11 的月活跃设备数已超过 4 亿，这一数字明显高于微软最初的预期。</p><p></p><p>尽管 Windows 11 在一个月内的数据提高了近 3 个百分点，但仍远低于 Windows 10，后者是数亿人的首选操作系统。StatCounter 表示，Windows 10 的用户数占比为 69.35%，上个月下降了 2.27 个百分点。</p><p></p><h4>英伟达发布大语言模型，辅助芯片设计工作</h4><p></p><p></p><p>近日，英伟达推出了自家最新 430 亿参数大语言模型 ——ChipNeMo。对于它的用途，英伟达在官方披露消息中也是非常的明确，剑指 AI 芯片设计。</p><p></p><p>具体而言，ChipNeMo 可以帮助工作人员完成与芯片设计相关的任务，包括回答有关芯片设计的一般问题、总结 bug 文档，以及为 EDA 工具编写脚本等等。</p><p></p><p>英伟达首席科学家 Bill Dally 对此表示：“以英伟达 H100 Tensor Core GPU 为例，它由数百亿个晶体管组成，在显微镜下看着就像是一个精心规划建设的城市一般”。</p><p></p><p>这些晶体管连接在比人类头发丝还细 10000 倍的“街道”上，需要多个工程团队协作两年多的时间来完成，其间繁琐且庞大的工作量，可见一斑。</p><p></p><h4>微软宣布与西门子联手，将Copilot生成式人工智能引入制造业</h4><p></p><p></p><p>微软（Microsoft）和西门子（Siemens）宣布，双方计划围绕生成式人工智能（AI）及其在全球工业领域的应用深化合作关系。此举有望彻底改变人机协作，两家公司将推出西门子工业副驾（Siemens Industrial Copilot），这是一款共同开发的人工智能助手，旨在提高制造业的生产率。</p><p></p><h2>IT业界热评新闻</h2><p></p><p></p><h4>印度“IT业之父”要求年轻人每周工作70小时：不要从西方学到坏习惯，不帮助国家发展</h4><p></p><p></p><p>10月30日报道，英国首相苏纳克的岳父、财富超40亿美元的“印度IT业之父”穆尔蒂在一条视频中表示，“不知为何，印度的年轻人从西方学到了坏习惯，不帮助国家发展。”穆尔蒂称，2075年印度有望成为“世界第二大经济体”。为与中国等国家竞争，印度需要”意志坚定、纪律严明、工作勤奋”的年轻人。</p><p></p><p>“我要求年轻人必须说，‘这是我的国家，我想每周工作70个小时’。”国际劳工组织数据显示印度是工时最长的国家之一，每人每周平均工作47.7小时。</p><p></p><h4>字节宣布除夕统一放假、连放9天，不占用年假</h4><p></p><p></p><p>10月30日，网传字节跳动宣布除夕统一放假，其办公软件“飞书”日历上，已显示除夕当天为“春节团聚假”。</p><p></p><p>当日，记者就此传闻求证字节跳动员工，获知该消息属实。前述字节跳动员工表示，公司有关除夕放假的公告发布于10月30日13时许，公告中还指出：“其中除夕当天为公司额外提供的春节团聚假，不占用年假额度。”</p><p></p><p>10月25日，国务院办公厅发布关于2024年部分节假日安排的通知。其中，春节假期为2月10日至17日放假调休，共8天，2月4日（星期日）、2月18日（星期日）上班，即除夕不放假。通知中提出，鼓励各单位结合带薪年休假等制度落实，安排职工在除夕（2月9日）休息。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</id>
            <title>字节跳动飞书技术 Leader 杨晶生，确认担任 QCon AI Agent 与行业融合应用的前景专题出品人</title>
            <link>https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xjAUuCeD556DgJwUDCOL</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 Nov 2023 07:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, AI Agent, 行业融合应用, 杨晶生
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，杨晶生将担任“AI Agent 与行业融合应用的前景”专题的出品人。在此次专题中，将介绍AI Agent的定义、应用以及与行业技术融合应用的发展前景。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。字节跳动飞书 技术 Leader 杨晶生将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">AI Agent 与行业融合应用的前景</a>"」的专题出品人。在此次专题中，你将了解到 AI Agent 是什么、AI Agent 的落地应用，以及与已有的行业技术融合应用的发展前景。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1103&amp;utm_content=yangjingsheng">杨晶生</a>"，目前就任于字节跳动，负责飞书音视频和人工智能相关产品的研发工作。曾就任于微软云计算部门和蜻蜓 FM。在十余年的工作中，经历了从服务器研发到云计算的过程，参与了超大规模全球化云服务架构、急速增长的内容平台业务、复杂而稳定性要求极高的企业服务等等项目，在服务高并发性能和稳定性方面有丰富的经验。</p><p></p><p>相信杨晶生的到来，可以帮助提升此专题的质量，让你学习到 AI Agent 能够感知环境、进行决策和执行动作，通常基于机器学习和人工智能技术，具备自主性和自适应性。同时，它已是公认大语言模型落地的有效方式之一，让更多人看清了大语言模型创业的方向，为未来技术融合应用提供了新思路。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</id>
            <title>火山引擎云安全解决方案负责人林扬确认出席FCon，分享金融企业如何构建安全云底座与合规能力？</title>
            <link>https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UPZWsozGefXCVeZxykpV</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 林扬, 金融企业如何构建安全云底座与合规能力, 云原生安全
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，林扬将发表题为《金融企业如何构建安全云底座与合规能力？》的主题分享。他将介绍金融行业安全现状、挑战和发展趋势，以及云原生安全解决方案。此次会议将涉及云原生安全最佳实践、隐私和数据安全合规实践、业内安全助力业务增长的方法，以及大模型安全风险评估要点。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。火山引擎云安全解决方案负责人林扬将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5607?utm_source=infoqweb&amp;utm_medium=article">金融企业如何构建安全云底座与合规能力？</a>"》主题分享，介绍金融行业安全现状，挑战和发展趋势、安全技术框架建议，以及相关案例。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5607?utm_source=infoqweb&amp;utm_medium=article">林扬</a>"，20 年的网络安全从业经验，具备甲乙方安全经历和视角，担任过甲方 CISO，曾担任传统安全公司和互联网云公司解决方案负责人，擅长数字化和上云转型的安全规划和实践，长期深入研究各行业安全需求和问题，是国内行业网络安全攻防推演的专家。他在本次会议的演讲内容如下：</p><p></p><p>演讲：金融企业如何构建安全云底座与合规能力？</p><p></p><p>金融行业信息化和数字化一直走在国内前沿，对于科技风险、安全合规、攻防能力方面要求高，挑战巨大，火山引擎基于自身安全和风控的最佳实践，结合金融行业业务特点，为行业定制适合的安全技术框架和解决方案，云原生安全解决金融行业云基础架构底座安全问题，隐私计算保障金融行业数据共享交换的安全合规和业务价值增长，大模型安全帮助金融行业发现模型安全问题，提供大模型底座和模型算法安全的保障措施。</p><p></p><p>演讲提纲：</p><p></p><p>金融行业安全现状，挑战和发展趋势；金融行业安全技术框架建议；云原生安全：构建金融行业稳定弹性的云安全底座；数据安全：降低合规风险；隐私计算：助力金融行业数据合规互联互通，提升营销增长；大模型安全，帮助金融行业大模型探索提供安全保障；云原生、数据安全成功案例分析。</p><p></p><p>你将获得：</p><p></p><p>○ 了解云原生安全最佳实践和关键点；</p><p>○ 隐私和数据安全合规实践和可落地做法；</p><p>○ 了解业内安全助力业务增长的方法；</p><p>○ 了解大模型安全风险，备案所需的安全评估要点。</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 8 折优惠 ，立省 ￥1360！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</id>
            <title>揭秘阿里核心引擎，走近阿里巴巴开源自研搜索引擎 Havenask</title>
            <link>https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zLmWSSGfqzTAz0gzEG8r</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 09:18:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2023云栖大会, Havenask, 开源正式版本, 大规模分布式搜索引擎
<br>
<br>
总结: 2023云栖大会上，阿里巴巴发布了Havenask开源正式版本，Havenask是一款大规模分布式搜索引擎，主要用于智能搜索和海量数据实时检索。该搜索引擎在阿里巴巴内部的多个业务中得到广泛应用，如淘宝、天猫商品搜索，盒马搜索，菜鸟物流订单实时检索等。通过开源，团队希望能吸引更多开发者参与项目研发，共同创造更好的搜索引擎。 </div>
                        <hr>
                    
                    <p></p><h2>2023云栖大会，Havenask&nbsp;发布开源正式版本</h2><p></p><p>Havenask&nbsp;是阿里巴巴自主研发的大规模分布式搜索引擎，主要专注于智能搜索和海量数据实时检索，其核心能力广泛应用于阿里巴巴内部的众多业务，如淘宝、天猫商品搜索，盒马搜索，菜鸟物流订单实时检索等。</p><p></p><p>在11月1日云栖大会上，阿里巴巴智能引擎事业部云服务负责人&amp;&nbsp;Havenask&nbsp;开源项目负责人郭瑞杰博士，进行了&nbsp;Havenask&nbsp;开源正式版本发布演讲，并在演讲中介绍了&nbsp;Havenask&nbsp;最新开源进展与后续计划。</p><p></p><p>阿里巴巴智能引擎事业部高级技术专家徐希杰与魏子珺，则是在开源开放麦中为开发者详细阐述了&nbsp;Havenask&nbsp;正式版本的分布式能力细节与&nbsp;havenask-elasticsearch&nbsp;联邦项目最新进展，并结合各场景案例进行具体能力展示。</p><p></p><p>这是自在2022年云栖大会首发后，Havenask&nbsp;又一次亮相云栖，作为阿里巴巴自主研发的大规模分布式搜索引擎，Havenask&nbsp;承载着几代阿里搜索人的技术沉淀，团队期望在开源之后能有更多开发者加入项目研发中，一同联合共创。</p><p></p><p>本文将具体介绍&nbsp;Havenask&nbsp;的引擎架构、索引类型、查询语法、插件机制与运维管控能力，帮助广大开发者快速了解与上手。</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca1589c74e7f2a16d7763106e8953b51.jpeg" /></p><p>图1&nbsp;郭瑞杰博士在云栖大会《阿里云开源年度发布》中发布&nbsp;Havenask&nbsp;正式版</p><p></p><h2>Havenask&nbsp;简介</h2><p></p><p>Havenask&nbsp;底层全部采用&nbsp;C++&nbsp;实现，并经过多年的优化迭代，与其他的开源搜索引擎相比具有如下特点：</p><p></p><p>高性能：查询性能高，某些场景性能数倍于开源引擎。低成本：支持存算分离，冷热数据隔离等功能，海量数据场景下成本更低。时效性高：数据写入或者更新的时效性可以达到毫秒级。稳定性高：内存控制严格，没有其他开源引擎&nbsp;gc&nbsp;的问题，同时支持多机房互备具有更高的可靠性。索引类型丰富：支持&nbsp;kv、kkv、倒排、正排、摘要、向量多种索引类型。定制能力强：支持分词器、数据处理、query&nbsp;改写、算分、功能函数、等多种插件的定制。支持&nbsp;SQL&nbsp;语法：支持&nbsp;SQL&nbsp;查询，多表&nbsp;join，学习门槛低，业务迁移方便。</p><p></p><h2>Havenask&nbsp;架构</h2><p></p><p>Havenask&nbsp;引擎支持两种工作模式：读写分离模式（全量表模式）与读写统一模式（直写表模式），读写统一与读写分离的主要区别是是否有独立的索引构建服务，下面详细介绍。</p><p></p><h3>读写分离架构</h3><p></p><p><img src="https://static001.geekbang.org/infoq/33/33f354cadf938d9f305d845ffdf35a82.png" /></p><p>图2&nbsp;Havenask&nbsp;读写分离架构</p><p></p><p>Havenask&nbsp;读写分离架构如上图所示，在此模式下索引构建系统与在线检索系统是两个独立的子系统，可以分别进行资源的调整和集群的管理，索引构建系统和在线检索系统通过消息中间件和分布式文件系统进行索引的交互。读写分离架构适用于需要快速导入全量数据，需要定期进行索引重建，数据更新量较大，需要对离线资源进行独立控制等场景。</p><p></p><h4>索引构建系统</h4><p></p><p>索引构建系统是一个分布式的索引构建服务（build&nbsp;service，简称&nbsp;bs），每个索引构建服务都有一个或多个&nbsp;bs&nbsp;admin，bs&nbsp;admin&nbsp;负责管理表的索引构建流程，其中每个表可以有多个索引构建流程同时存在，相互之间无影响。</p><p></p><p>每个索引构建流程都由一组&nbsp;processor、builder、merger&nbsp;任务组成，其中&nbsp;processor&nbsp;负责原始数据的处理，比如分词，builder&nbsp;负责索引的构建，merger&nbsp;负责索引的整理。processor&nbsp;可以自由设置分片数，每个分片负责处理一部分数据，分片越多数据处理能力越强，使用的资源也越多。builder&nbsp;的分片数必须是索引表分片数的倍数，分片数越多索引构建越快。merger&nbsp;的分片数必须是索引表分片数的倍数，分片数越多索引整理越快。processor&nbsp;是一个常驻任务，builder&nbsp;和&nbsp;merger&nbsp;是交替执行的任务。</p><p></p><h4>索引构建流程</h4><p></p><p>读写分离架构下的表的数据索引构建由全量和实时两个阶段组成，同一个表可以同时存在多个索引构建流程，彼此之间无影响。全量阶段索引构建系统会将存在&nbsp;HDFS、MaxCompute&nbsp;或者&nbsp;OSS&nbsp;上的全量原始文件构建成全量索引，并订阅消息中间件回追一部分实时数据（防止全量切上线之后需要很长时间才能追上实时数据）。</p><p></p><p>索引构建过程中，processor&nbsp;会读取原始数据并对齐按照配置的数据处理规则进行处理，处理之后的数据会被发送到消息中间件（swift）。builder&nbsp;订阅消息中间件，将处理之后的文档构建成索引，并将索引产出到分布式文件系统。消息中间件中全量阶段处理的数据被&nbsp;builder&nbsp;全部处理完后，启动&nbsp;merger&nbsp;任务对索引进行整理，产出全量索引。</p><p></p><p>全量阶段完成后，全量索引会被切换到在线系统，同时索引构建也切换到增量阶段。增量阶段&nbsp;processor&nbsp;订阅消息中间件，处理实时数据，并将处理好的数据写回消息中间件。在线系统同时订阅消息中间件，获取处理之后的文件，直接在内存中构建成索引，这样数据就可以实时生效。builder&nbsp;也会订阅消息中间件，获取处理之后的文件，将其构建为增量索引，并由&nbsp;merger&nbsp;对全量索引和增量索引进行索引整理，产出最终可以切换上线的索引。增量索引切换上线之后，在线系统实时中的实时索引会被从内存中清理掉，释放的内存会被用于构建新的实时索引。</p><p></p><h4>在线系统</h4><p></p><p>在线系统用于加载索引并提供检索服务，它是一个支持多分片（shard），多备份（replica）部署的分布式服务，主要由&nbsp;admin，qrs（query&nbsp;result&nbsp;service）和&nbsp;searcher（数据节点）三种角色组成。Admin&nbsp;管理整个集群，实时监控各个节点的健康状态并调度，接收运维命令并向&nbsp;qrs&nbsp;和&nbsp;searcher&nbsp;下发运维指令。Qrs&nbsp;用于&nbsp;query&nbsp;的处理和结果的合并，qrs&nbsp;没有分片的概念，每个&nbsp;qrs&nbsp;节点都是同构的。Searcher&nbsp;节点加载索引，并真正执行查询任务，searcher&nbsp;节点上可以加载一个表的一个分片或者多个分片的数据。</p><p></p><p>在&nbsp;Havenask&nbsp;中用区间&nbsp;[0,&nbsp;65535]&nbsp;表示一份完整的数据，所有的数据分区键经过哈希之后都会映射到&nbsp;[0,&nbsp;65535]&nbsp;这个区间之内。在设置表的分片数之后，每个分片都会对应这个区间的一段范围，比如分片数为2，分成的两个分片对应的区间为&nbsp;[0,&nbsp;32767]&nbsp;和&nbsp;[32768,&nbsp;65535]。Havenask&nbsp;单分片最多可以承载21亿个文档，所以分片数越多，整个集群可以承载的数据量越多，最多支持65536个分片。</p><p></p><p>Qrs&nbsp;和&nbsp;searcher&nbsp;都可以通过扩充备份来提高整个集群每秒&nbsp;query&nbsp;处理能力（qps），不同的是&nbsp;qrs&nbsp;没有分片的概念，扩充备份就是扩&nbsp;qrs&nbsp;节点的个数，searcher&nbsp;是按照分片组织的，在每个&nbsp;searcher&nbsp;加载一个分片前提下，扩&nbsp;searcher&nbsp;的备份数就是扩分片数*备份数个&nbsp;searcher&nbsp;节点。</p><p></p><p>Havenask&nbsp;支持存算分离，searcher&nbsp;上的索引加载采用远端分布式文件系统、本地磁盘和内存三级索引加载策略，索引读取的性能也是按照远端、本地、内存这个顺序有数量级的提高。开发者在使用时可以综合考虑成本与查询耗时的要求，合理的配置索引加载策略。</p><p></p><h3>读写统一架构</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52f6f60c3dd206c897364edaf93fac75.png" /></p><p>图3&nbsp;Havenask&nbsp;读写统一架构</p><p></p><p>Havenask&nbsp;读写统一架构如上图所示，与读写分离架构相比有下面几个不同点：1）读写统一模式没有全量流程，所有的数据都要通过&nbsp;api&nbsp;以实时生效的方式推送到系统中；2）实时数据的推送直接写入到&nbsp;qrs，而不是&nbsp;swift&nbsp;中；3）每个分片对应&nbsp;searcher&nbsp;的&nbsp;leader&nbsp;进行索引整理，并将整理好的索引写入分布式系统中，follower&nbsp;加载整理好的索引。读写统一架构中使用&nbsp;swift&nbsp;进行&nbsp;wal&nbsp;以保证数据的安全，swift&nbsp;做&nbsp;wal&nbsp;的好处是不会随着&nbsp;searher&nbsp;备份的增多导致写的性能下降。读写统一架构比较适合频繁创建索引表、不需要全量数据导入、时效性要求高等场景。</p><p></p><h2>Havenask&nbsp;索引类型</h2><p></p><p>Havenask&nbsp;主要有三种类型的索引：倒排索引，正排索引与摘要索引。</p><p></p><h3>倒排索引</h3><p></p><p>倒排索引存储的是词（term）到包含词的文档（doc）的映射关系，可以通过倒排索引快速的检索到需要的候选文档，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b3effd4bff47aaec9fd247dc8d95758.png" /></p><p>图4&nbsp;Havenask&nbsp;倒排索引结构示意图</p><p></p><p>Havenask&nbsp;支持多种类型的倒排索引，比如主键索引，STRING&nbsp;索引（keyword，不分词），多字段&nbsp;PACK&nbsp;索引，数值范围索引，空间索引等，具体每个索引的含义开发者可以参考&nbsp;Havenask&nbsp;的用户手册。</p><p></p><p>向量索引（ANN）是一种特殊类型的倒排索引，它的索引结构与上图的通用倒排索引结构不同，具体的构建方式与索引结构和向量索引选择的算法有关。Havenask&nbsp;支持的向量索引有三种类型，线性索引、HNSW&nbsp;索引和聚类索引，可以满足不同场景下的向量检索需求。</p><p></p><h3>正排索引</h3><p></p><p>正排索引存储的是文档中字段的内容，主要用于对查询到的结果进行过滤、统计、排序等操作。Havenask&nbsp;的正排索引采用的是列存模式，即每个字段单独存储，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9837bb013557ea8cf81d2373021dc4f.png" /></p><p>图5&nbsp;Havenask&nbsp;正排索引结构示意图</p><p></p><h3>摘要索引</h3><p></p><p>摘要索引存储的是文档中需要返回的字段，可以对返回的字段进行飘红处理，如果字段内容较长可以使用摘要插件动态截取要返回的内容。Havenask&nbsp;的摘要索引采用的是行存模式，即一个文档的所有字段存在一起，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c6ff299924a36a3527d65e3b1533486.png" /></p><p>图6&nbsp;Havenask&nbsp;摘要索引结构示意图</p><p></p><h2>Havenask&nbsp;查询语法</h2><p></p><p>Havenask&nbsp;目前支持&nbsp;SQL&nbsp;查询语法，SQL&nbsp;语法简单易用并且便于扩展。但是具体到检索场景，SQL&nbsp;语法还是有些不足，比如查询时的多索引倒排搜索，粗排与精排的支持等。为了弥补搜索场景下&nbsp;SQL&nbsp;能力的不足，Havenask&nbsp;提供丰富的各种内置函数，比如提供了MATCHINDEX&nbsp;和&nbsp;QUERY&nbsp;等&nbsp;UDF&nbsp;支持倒排索引的查询，并支持自定义&nbsp;UDF。</p><p></p><h2>Havenask&nbsp;插件机制</h2><p></p><p>Havenask&nbsp;支持开发者自定义分析器插件、数据处理插件，各类&nbsp;UDF&nbsp;等以满足不同的业务需求。</p><p></p><p>分析器插件：开发者可以通过分析器插件定制自己的分析器，以满足不同的分词需求。分析器插件作用在数据处理分词阶段和查询时&nbsp;Query&nbsp;分词阶段。</p><p></p><p>数据处理插件：在构建索引之前，需要对文档进行处理（默认的是分词），开发者可以通过定制自己的数据处理插件提前对数据进行处理，比如可以集成一个向量化模型，在数据处理阶段将文本转为向量。</p><p></p><p>UDF：用户自定义函数，在查询时通过&nbsp;UDF&nbsp;可以定制自己的业务逻辑，比如外卖场景下，计算店铺和买家的距离。</p><p></p><p>对于各种定制插件，我们推荐开发者不要将插件代码单独编译成&nbsp;so&nbsp;的形式，而是与&nbsp;Havenask&nbsp;代码一起编成一个统一的&nbsp;binary，通过镜像的方式发布。</p><p></p><h2>Havenask&nbsp;运维管控</h2><p></p><p>Havenask&nbsp;的各个子系统（在线系统，索引构建系统，消息中间件）都有对应的&nbsp;admin&nbsp;角色进行集群的管理，每个&nbsp;admin&nbsp;都提供了运维管控的rpc接口。为了方便大家的使用，我们对这些&nbsp;rpc&nbsp;接口进行了封装，提供了方便使用的命令行工具&nbsp;hape。使用&nbsp;hape，开发者可以方便的启停集群，对表进行各种管理操作，如果需要更精细的运维控制，开发者可以直接调用&nbsp;admin&nbsp;提供的&nbsp;rpc&nbsp;接口。</p><p></p><h2>结语</h2><p></p><p>期望通过介绍，可以帮助开发者更好的了解与上手&nbsp;Havenask，我们欢迎广大开发者加入项目开发，共建高质量的搜索引擎。</p><p></p><p>此外，对于有使用需求的企业级开发者，我们也已在阿里云上提供了基于&nbsp;Havenask&nbsp;打造的全托管、免运维的一站式对话式搜索服务——阿里云&nbsp;OpenSearch，欢迎企业级开发者们试用体验。</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p>阿里云&nbsp;OpenSearch&nbsp;官网：<a href="https://www.aliyun.com/product/opensearch">https://www.aliyun.com/product/opensearch</a>"</p><p></p><p>欢迎钉钉扫码加入&nbsp;Havenask&nbsp;开源官方技术交流群：</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78c5cfa61c64a55cdeb0655ac7eb2849.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</id>
            <title>仅凭7页PPT拿下1亿美元融资、半年后估值超10亿！“欧洲OpenAI”杀疯了</title>
            <link>https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 初创公司, Mistral AI, 融资, 大语言模型
<br>
<br>
总结: Mistral AI 是一家AI初创公司，通过7页PPT成功融资1亿美元，目前正在寻求3亿美元的新融资。该公司专注于开发大语言模型和各类AI技术，旨在解决现实世界问题。最近，他们发布了号称是“最强7B开源模型”的Mistral 7B，该模型在各项基准测试中表现优秀。 </div>
                        <hr>
                    
                    <p></p><blockquote>这家成立 4 周时就能凭借 7 页 PPT 融到超 1 亿美元的 AI 初创公司，究竟是什么来头？</blockquote><p></p><p></p><h2>AI 初创公司 Mistral 正寻求 3 亿美元新融资</h2><p></p><p>&nbsp;</p><p>据外媒报道，生成式 AI 初创公司 Mistral AI（常自称为“欧洲 OpenAI”）目前正寻求 3 亿美元新融资。如果一切顺利，那么新融资将帮助这家年轻企业估值突破 10 亿美元大关。</p><p>&nbsp;</p><p>据了解，Mistral AI&nbsp;总部位于法国巴黎，由来自 Meta Platforms 和 Alphabet 的几位前研究人员 Arthur Mensch（现任 CEO）、Guillaume Lample 和 Timothee Lacroix 共同创立，公司成立于 2023 年 5 月，专门开发大语言模型及各类 AI 技术。Mistral 这个名号来自北方寒冷的季风，也体现了他们想要在 AI 领域占据一席之地的愿望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae7ee9ad22dc1fecd7c677caeb92b068.png" /></p><p></p><p>6 月，Mistral 在拿下 1.13 亿美元巨额种子融资后引发业界轰动，公司估值也瞬间来到 2.6 亿美元。彼时，该公司刚刚成立，员工仅 6 人，还未做出任何产品，仅仅凭借着&nbsp;7 页 PPT 就斩获了巨额融资。</p><p>&nbsp;</p><p>该轮融资由 Lightspeed Venture Partners 牵头，Redpoint、Index Ventures、Xavier Niel、德高控股以及意大利、德国、比利时和英国的其他知名风险投资公司参与。但该公司很快发现这“区区”1亿美元根本不够，要推动后续增长和扩张计划还需要更多资金的支持。</p><p>&nbsp;</p><p>据 The Information 近日报道，熟悉谈判内情的消息人士称，Mistral 正计划从投资者处额外筹集 3 亿美元，而此时距离由 Lightspeed Venture Partners 领投的种子轮融资才刚刚过去四个月。</p><p>&nbsp;</p><p>目前还不清楚 Mistral 已经与哪些风险投资商进行过通气，但根据另一位知情人士透露，生成式 AI 投资领域的重要参与者 Andreessen Horowitz 正在积极寻求向开源大语言模型（LLM）开发者注资的机会。如果能够顺利合作，自然不失为一件美事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/621d9243b7262fef809f470c83814e33.png" /></p><p></p><p>Mistral 公司CEO、前 DeepMind 研究科学家 Mensch 表示，这家企业的使命是“打造出能够解决现实世界问题的下一代 AI 系统”。他同时补充称，新一轮融资将用于扩大团队、加快研发工作，以及在欧洲和美国建立新的办事处。</p><p>&nbsp;</p><p>Mistral 敢于开出如此夸张的融资数额，也体现出投资者对于 AI 初创企业不断增长的关注和信心。近年来，AI 初创公司已经筹得海量资金，其中不少企业正在开发前沿 AI 技术，有望彻底颠覆众多传统行业。</p><p>&nbsp;</p><p>但目前 Mistral 仍在起步阶段，能否成为 AI 领域的主要参与者仍然有待观察。尽管如此，该公司强大的初始团队和雄心勃勃的发展目标，已经使其成为当前乃至未来几年中最值得关注的 AI 初创力量之一。</p><p></p><h2>“最强 7B 开源模型”Mistral 7B</h2><p></p><p>&nbsp;</p><p>9 月 27 日，Mistral AI 团队发布了自家首个大模型&nbsp;Mistral 7B，该模型号称是“最强 7B 开源模型”。</p><p>&nbsp;</p><p>据介绍，Mistral 7B 是一套拥有 73 亿参数的大语言模型，采用 Apache 2.0 许可证，以不加限制的方式对外开放以供使用。在所有基准测试中，Mistral 7B 均优于 Llama 2 13B；在多种基准测试中，优于 Llama 1 34B；拥有比肩 CodeLlama 7B 的编码性能，并同时保持着良好的英语能力；使用分组查询注意力（GQA）来加快推理速度；使用滑动窗口注意力（SWA）以较低成本处理更长序列。</p><p>&nbsp;</p><p>GitHub 链接：<a href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a>"HuggingFace 链接：<a href="https://huggingface.co/mistralai">https://huggingface.co/mistralai</a>"</p><p>&nbsp;</p><p>Mistral 7B基础设施集群由 CoreWeave 提供 24/7 全天候支持，CINECA/EuroHPC 团队及 Leonardo 运营团队提供资源与帮助，FlashAttention、vLLM、xFormers、Skypilot 维护团队提供新功能以及方案集成指导。HuggingFace、AWS、GCP、Azure ML 团队协助实现了 Mistral 7B 的全平台兼容。</p><p>&nbsp;</p><p>Mistral 7B 还能针对任意任务进行轻松微调。Mistral AI 团队将 Mistral 7B 与 Llama 2 系列模型进行了比较，并重新运行了这些模型以验证评估结论是否准确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f026b61c7d1d27868ee2ac3a51f7881.png" /></p><p></p><p>Mistral 7B 及各 Llama 模型在不同基准测试中的性能。这里列出的所有指标，均从&nbsp;Mistral AI 团队评估管道中的实际运行中采集而来，从而保证比较的真实性。Mistral 7B 在所有指标上均显著优于 Llama 2 13B，而且与 Llama 34B 基本相当（由于 Llama 2 34B 模型尚未发布，因此这里暂时与 Llama 34B 比较）。Mistral 7B 在编码与推理方面同样性能出众。</p><p>&nbsp;</p><p>本轮基准测试按主题可分为以下几类：</p><p>&nbsp;</p><p>常识推理: Hellaswag、Winogrande、PIQA、SIQA、OpenbookQA、ARC-Easy、ARCChallenge和CommonsenseQA&nbsp;的 0-shot 平均值;世界知识: NaturalQuestions&nbsp;和&nbsp;TriviaQA&nbsp;的 5-shot 平均值;阅读理解: BoolQ和&nbsp;QuAC&nbsp;的 0-shot 平均值;数学: mai@8的8-shot GSM8K 和 ma@4的4-shot MATH 的平均值;编码: 0-shot Humaneval&nbsp;和 3-shot MBPP 的平均值;热门聚合结果: 5-shot MMLU、3-shot BBH 和 3-5-shot AGI Eval (仅限英文多项选择题)。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0bf70ba717857e0b6dc38131fba14550.png" /></p><p></p><p>在对模型的成本/性能进行比较中，Mistral AI 团队提出了一个有趣的指标，即计算“等效模型大小”。在推理、理解与 STEM 推理（MMLU）方面，Mistral 7B 的性能与体量达到其 3 倍以上的 Llama 2 模型相当，意味着它能显著节约内存容量和数据吞吐量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7d7373bf51fa51eb28f95fde31b38da.png" /></p><p></p><p>Mistral 7B 和 Llama 2（7B/13B/70B）的 MMLU 常识推理、世界知识与阅读理解比较结果。Mistral 7B 在绝大多数评估中均显著优于 Llama 2 13B，仅在知识基准测试中与后者处于同一水平（这可能是由于参数规模有限，因此掌握的知识量不足）。</p><p>&nbsp;</p><p>注意：此次评估与 Llama 2 论文之间存在以下区别：</p><p>&nbsp;</p><p>在 MBPP 测试中，这里使用了手工验证的子集。在 TriviaQA 测试中，这里未提供维基百科上下文。</p><p>&nbsp;</p><p>此外，Mistral 7B 使用滑动窗口注意力（SWA）机制，即每个层都关注之前的 4096 个隐藏状态。这里做出的主要改进以及尝试改进的原因，来自 O(sliding_window.seq_len) 的线性计算成本。具体来讲，在对 FlashAttention 和 xFormers 做出改进之后，成功在 16k 序列长度和 4k 上下文窗口下实现了速度倍增。Tri Dao 和 Daniel Haziza 为相关调整做出了贡献。</p><p>&nbsp;</p><p>滑动窗口注意力的原理，是利用&nbsp;Transformer&nbsp;的堆叠层来关注此前超出窗口大小的情形：第 k 层的 token&nbsp;i 关注第 k-1 层的 token&nbsp;[i-sliding_window, i]，后者又关注 [i-2*sliding_window, i]。如此一来，较高层就能访问到距离更“久远”的过往信息。</p><p><img src="https://static001.geekbang.org/infoq/12/12fa10bb65ec4544160b5d63edb4eca1.png" /></p><p></p><p>总之，采取固定注意力范围的最大意义，就是使用轮换缓冲区将缓存限制为&nbsp;sliding_window&nbsp;token 的大小（更多细节请查看参考实现<a href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a>"）。如此一来，同样在执行 8192 序列长度的推理时，可以节约下 50% 的高速缓存容量且不会影响模型质量。</p><p>&nbsp;</p><p>为了展示 Mistral 7B 模型的泛化能力，研究团队使用 HuggingFace 上的公开指令数据集对其进行了微调。不用问题集“作弊”、也不涉及专有数据，由此产生的 Mistral 7B Instruct 模型在 MT-Bench 测试中获得了优于一切同体量 7B 模型的性能，表现可与 13B 聊天模型相比肩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/36c664d9c775b51093068b1f94782de0.png" /></p><p></p><p>快速演示的 Mistral 7B Instruct 模型能够轻松微调，进而带来引人注目的卓越性能。其中不涉及任何协调机制。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theinformation.com/articles/mistral-a-wannabe-openai-of-europe-seeks-300-million">https://www.theinformation.com/articles/mistral-a-wannabe-openai-of-europe-seeks-300-million</a>"</p><p><a href="https://techstartups.com/2023/10/31/mistral-a-generative-ai-startup-aiming-to-be-europes-openai-seeks-300-million-in-new-funding/">https://techstartups.com/2023/10/31/mistral-a-generative-ai-startup-aiming-to-be-europes-openai-seeks-300-million-in-new-funding/</a>"</p><p><a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</id>
            <title>中国电子学会主办 第四届 ATEC 科技精英赛报名启动</title>
            <link>https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/61OodB9eIKXe3S52SwkC</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 06:49:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ATEC科技精英赛, 大模型应用与安全, 人工智能, 网络安全
<br>
<br>
总结: 中国电子学会主办的第四届ATEC科技精英赛（ATEC2023）已启动报名。该赛事旨在推动新一代人工智能发展和国家网络空间安全战略，培养人工智能及网络安全领域人才。本届赛事的主题是大模型应用与安全，旨在解决大模型技术的可用性、安全性和有效性等问题。赛事将围绕真实场景命题，培养青年科技人才的综合能力。赛事将于11月30日开始，共设4个赛道，奖金池共计146万元。 </div>
                        <hr>
                    
                    <p>11 月 1 日由中国电子学会主办的第四届 ATEC 科技精英赛（ATEC2023）正式启动报名。</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/9I7jYdgQNrr0zBTXyyYV?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">ATEC</a>" 科技精英赛是主要面向中国籍计算机等专业在校学生、人工智能及网络安全行业研究者和从业者的一场高水平的智能科技挑战赛，意在贯彻落实党中央、国务院关于推动新一代人工智能发展的决策部署以及全面贯彻国家网络空间安全战略，构筑我国人工智能及网络安全发展的先发优势，推动人工智能及网络安全领域人才培养。</p><p>&nbsp;</p><p>本届大赛的主题为<a href="https://www.infoq.cn/article/Wigm8Jk2atzDYgo61JJC?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大模型应用与安全</a>"。中国工程院院士、清华大学<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbe9a327cc9e2a31ca257bd26d377944080205ce1c81ba8c3714add4012efa5d07b1819982b3&amp;idx=3&amp;mid=2247490280&amp;scene=27&amp;sn=555602ee062f2055e8039a6289ed9cd7&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">人工智能</a>"研究院院长张尧学将担任 ATEC2023 指导委员会主席。52位来自境内外高校计算机领域的知名学者参与评审。ATEC 前沿科技探索社区作为赛事的承办单位。社区联合发起单位清华大学、浙江大学、西安交通大学、上海交通大学、蚂蚁集团，将联合北京大学、新加坡南洋理工大学等 13 所知名高校共同承担大赛命题、组织保障等相关工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f90de9bc661be51213a6348812b72b46.png" /></p><p></p><p>人工智能大模型无疑已成为当下计算机领域最受关注的热点技术问题。但如今，大模型及相关技术的发展尚处于初期阶段，仍面临技术短板、隐私安全等问题。在大模型的落地应用过程中，如何平衡前沿技术探索和工业应用之间的隔阂，成为现实挑战。一方面，大模型开发、训练、运营等成本耗资巨大，有效提升资源利用率，成为大模型应用普及的关键；另一方面，在一些大模型的实际使用过程中，已被发现生成内容存在质量不佳、冗余回答等问题，甚至可能产生输出危害内容的问题，有效风险管理成为大模型合理合法应用的前提。</p><p>&nbsp;</p><p>ATEC2023 赛事将围绕大模型应用落地过程中的“可用性”、“安全性”、“有效性”等维度进行赛题设计，针对老年人科技服务等多个真实场景命题，探索大模型应用过程中的创新思路及解法。ATEC 前沿科技探索计划发起人、清华大学计算机系副系主任徐恪指出，“从预训练语料的安全标准，到内容产出物的安全评估，大模型不仅要服务用户，更要安全地服务好用户。重视和研究大模型在应用层面的安全可用问题，是大模型技术落地应用的必由之路。”</p><p>&nbsp;</p><p>作为业内实战型技术人才培养的旗帜性赛事。每一届 ATEC 科技精英赛都以还原真实工业场景、遴选具有社会价值的技术命题为出发点，遵循“以赛育人”的宗旨。ATEC2023 组织委员会主席、中国电子学会副秘书长曹学勤表示，“科技赛事是科技人才培养的有效途径，每年中国电子学会都将针对不同的技术方向组织各类技术赛事。大模型技术是未来计算机技术发展的关键。大模型技术的应用落地，依赖于人工智能、网络安全等多个技术领域的突破发展。借助大模型主题赛事及围绕真实场景的命题，能够很好地培养青年科技人才的综合能力。”</p><p>&nbsp;</p><p>据悉，本届赛事将于 11 月 30 日正式开始开赛。线上赛共设置 4 大赛道，涵盖大模型的知识引入、工具学习、AI 生成新闻检测、网络安全大模型等考点。大赛全程设有 3 大奖金池，共计奖金 146 万元，用以选拔及表彰领域内的优秀人才。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</id>
            <title>腾讯信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人徐羽，确认担任 QCon GenAI 和通用大模型应用探索专题出品人</title>
            <link>https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wKVrNMN6GeeRaT5J6Z0p</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 徐羽, LLM, AI
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，徐羽将担任"GenAI 和通用大模型应用探索"专题的出品人。在此次专题中，将介绍LLM的最新进展、类型、能力、局限性和未来发展趋势。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。腾讯信息平台与服务线 CTO、PCG 事业群 AI 与推荐中台负责人徐羽将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">GenAI 和通用大模型应用探索</a>"」的专题出品人。在此次专题中，你将了解到在 LLM 正在迅速发展背景下的的最新进展，以及 LLM 的类型、能力、局限性和未来的发展趋势。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1102&amp;utm_content=xuyu">徐羽</a>"，2009 年加入腾讯，现任信息平台与服务线 CTO 兼总经理、PCG 推荐 AI 中台负责人。硕士毕业于加拿大滑铁卢大学电子与计算机工程系，加入腾讯前在加拿大黑莓公司工作 6 年，参与 BIS 手机邮件研发工作。从 2009 年开始负责手机 QQ 浏览器从 0 起步到现在亿级 DAU 规模的研发工作，在 2018 年建立和负责 PCG 的推荐 AI 中台，在机器学习平台、NLP、CV 视频理解、推荐算法和推荐架构等方面带领团队支持 QQ 浏览器和 PCG 业务的 AI 落地应用。</p><p></p><p>相信徐羽的到来，可以帮助提升此专题的质量，让你学习到，LLM 研究和开发的最新进展，了解不同类型的 LLM、其能力和局限性，以及 LLM 对未来工作、教育、医疗保健和许多其他领域的潜在影响。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</id>
            <title>AIGC 编程：代码编程模型的应用与挑战</title>
            <link>https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WL2yVwKEqIutiwppz0wK</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 07:50:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 大型模型, 编码助手, 低代码平台
<br>
<br>
总结: 本文讨论了网易在生成式AI领域的应用，包括编码助手和低代码平台。大型模型帮助程序员编写代码是一项有价值的技术，但从商业角度来看并不一定有利可图。网易通过自研的方式，利用大型模型来更好地适应自身需求和场景，提供程序员更好的使用体验。在优化方面，网易注重将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识，并通过优化提示工程提供更有信息量的上下文，以产生对程序员有价值的输出。 </div>
                        <hr>
                    
                    <p>嘉宾 | 鱼哲、刘东</p><p>编辑 | Tina</p><p>&nbsp;</p><p>生成式AI已经成为软件行业的一个重要推动力。在过去的一年里，包括网易在内的许多公司都在积极探索如何将这项技术应用到他们的产品中。如今，网易已经推出了多个生成式AI的实际应用产品，包括编码助手、大数据分析产品和低代码平台。在最新一期的“极客有约”对话节目中，鱼哲与网易杭州研究院人工智能部的算法负责人刘东一同探讨了有关大型模型产品的成本、速度和精确度等关键问题。</p><p></p><p>本文经编辑，原视频地址：https://www.infoq.cn/video/Wu1iSPABRu9NTVRrrywi</p><p>&nbsp;</p><p>亮点：</p><p>大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。微调只是为了有针对性地增强它，使其更好地满足用户指令，所以首先需要基准模型能支持该任务。我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们引入了一个"可信AI"的概念，包括过程可验证、用户可干预和产品可运营三个方向。从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。</p><p>&nbsp;</p><p>嘉宾简介：</p><p>鱼哲，Lepton AI 创始团队成员，产品负责人。</p><p>刘东，网易杭州研究院人工智能专家，AI算法团队及产品团队负责人，专注于前沿算法研究与商业化应用。相关技术成果曾获浙江省科技进步奖一等奖。</p><p>&nbsp;</p><p></p><h4>AIGC在软件工程领域的应用方向</h4><p></p><p>&nbsp;</p><p>鱼哲：我个人认为Codex和Copilot等工具具有广阔前景，而AIGC也在广泛推广。刘东老师，您能否简要介绍一下，网易杭州研究院在AIGC技术以及软件工程领域的技术研究方向有哪些？</p><p>&nbsp;</p><p>刘东：关注AIGC，特别是在软件研发领域，我们认为它在各个环节都有实际价值。例如，在需求分析和设计方面，大型模型已经能够提供出色的设计建议。在编码和开发阶段，Copilot已经非常成熟，可以显著提高程序员的效率。在代码调试、分析和优化阶段，大型模型也能提供有益的优化建议，包括检测潜在的错误。甚至在测试阶段，我们也尝试使用大型模型生成测试案例。在运维环节，例如线上日志的实时分析和监控，也可以受益于大型模型的能力，提高效率。</p><p>&nbsp;</p><p>从应用的角度来看，我们目前在编码和开发阶段最快地实现了AIGC的应用。特别是我们内部为研发同事提供了类似Copilot的工具，已经看到效率有所提升。此外，我们还开发了一些外部商业化产品，如在BI产品中引入了对话功能，推出了ChatBI产品，以及在低代码产品中使用大型模型来加速低代码开发效率。</p><p>&nbsp;</p><p>鱼哲：关于Copilot，我之前在GitHub上也尝试过，还尝试过Code Llama。我想代表我们的观众逐一提出一些问题。首先，所有程序员都非常熟悉的一个问题是，我们花费很多时间思考如何分解代码的功能实现，以及编写和调试代码。特别是编写单元测试，在后端开发中经常需要，虽然我们不会讨厌，但有时候确实不太愿意做这项工作。</p><p>&nbsp;</p><p>然而，AI辅助编程作为一个产品，我看到市场上已经有很多竞品，比如GitHub的Copilot、OpenAI的Codex，以及AWS的Code Whisper等。有很多成熟的产品存在。我想问一下，为什么网易杭州研究院选择在这个领域开展工作？此外，微软在收购GitHub后表示支持Copilot业务，但据报道，这一业务目前亏本，因为Copilot对于开发者来说价格相对较高。微软在财报中也提到，他们每月支持一个开发者需要100美元，但实际只收取20美元。在这种情况下，您如何看待网易在这一领域的角色和作用？</p><p>&nbsp;</p><p>刘东：我们考虑这个问题时，有几个方面的考虑。首先，从安全的角度看，每家企业都有一些核心代码不愿意与外部共享，因此希望能够拥有相对可控的服务，并在其中使用大型模型以提高程序员的效率。因此，就可控性和安全性而言，自研可能是一个较好的选择。</p><p>&nbsp;</p><p>其次，每家企业都有大量的特定代码积累，而如何有效地利用这些代码，以在其业务中发挥价值，这也是一个重要问题。像Copilot这样的云服务通常比较通用，很难让企业将其自有代码集成进去并进行优化，以适应企业自身的习惯。</p><p>&nbsp;</p><p>因此，我们通过自研的方式，利用自己的模型来更好地适应网易的需求和场景，以提供程序员更好的使用体验。这是我们的出发点。</p><p>&nbsp;</p><p>至于亏本的问题，我也认为大型模型帮助程序员编写代码是一项很有价值的技术，但从商业角度来看，它并不一定是一个特别有利可图的生意。因为在这一领域，客户通常比较价格敏感，即使收费较低，用户也可能觉得价格昂贵。但从成本角度来看，确实需要较大的投入。因此，我们的考虑是综合各方因素来实施这项工作。</p><p>&nbsp;</p><p>鱼哲：除了安全性问题，您在网易情况可能会注意到一些特别适合的场景，不管是在电商还是游戏等许多场景中。您能否举一个具体的例子，展示我们在什么情境下通过自研的Copilot项目，更好地支持业务方编写其业务代码的案例呢？</p><p>&nbsp;</p><p>刘东：我们进行了大量的定制和优化，比如在游戏运营场景中，游戏经常需要举办各种活动。这些活动的方案通常由策划部门提出，要求程序员按照方案进行实现，但实现后可能代码只用一次，然后就不再使用。这种场景非常常见。</p><p>&nbsp;</p><p>但是，由于许多游戏之间存在相似性，企业的代码库中可能有很多人写过类似的代码，具有很大的参考价值。在这种情况下，如果使用通用的Copilot，它通常无法了解企业专有的代码信息，因此在这种场景下提供的提示效果可能不够理想。但如果我们进行企业定制，就可以通过一些增强的方式，将这些信息集成到提示中，然后将其提供给大型模型，使其能够参考这些代码来提供更好的提示。这样，我们就可以更好地实现降本增效。</p><p>&nbsp;</p><p>鱼哲：您刚才提到了在业务场景中进行了许多优化，特别是在模型层面。您能详细介绍一下优化工作是在模型层面进行的，还是在输送给模型的提示工程这一层进行的？或者说，在模型的不同方面都进行了优化，可以谈谈具体的优化思路是什么吗？</p><p>&nbsp;</p><p>刘东：我们的优化思路主要围绕两个关键点展开，以发挥大型模型的价值。第一个关键点是确保模型本身的强大性，这涉及将企业的专有知识融入到模型参数训练中，以使模型能够理解企业的专有领域知识。</p><p>&nbsp;</p><p>第二个关键点是优化提示工程，即我们如何提供给模型更有信息量的上下文，以便模型更好地理解上下文，产生对程序员有价值的输出。我们发现，仅仅将当前代码片段的上文或下文提供给模型并让其继续生成，效果通常一般。因此，我们考虑了编程过程中的各种信息来源，包括引入的外部第三方库、工程中的其他项目文件、类似的工程项目，甚至程序员在编程过程中浏览和检索网页、查找答案以及执行粘贴和复制等操作。这些行为都是宝贵的提示信息，我们通过将这些信息融入到模型的提示中，帮助模型更好地理解当前的上下文，从而更好地输出对程序员有价值的信息。这些工作使我们的模型能够更好地与业务结合，提供更好的效果。</p><p>&nbsp;</p><p>鱼哲：您提到了模型微调，确实，在Google和其他地方，人们一直在进行对模型的微调。通过微调一个基础模型，将其完全适配到特定任务，这是一个非常有效的方法。许多人认为，只要有基本模型，然后进行一些微调，就可以将其应用到任何任务上，使其成为该任务的专家。您对这个问题是怎么看的？</p><p>&nbsp;</p><p>刘东：如果我要进行微调，内部除了我们自己训练的基础模型，还有许多开源的基准模型可供使用。我们进行了大量的评估，具体思路是，如果要进行微调，首先要分析基准模型是否足够强大，是否在具体任务上已经表现得相当不错，微调只是为了有针对性地增强它，使其更好地满足用户指令。如果基准模型根本不支持该任务，仍然强行进行微调的话，效果可能不太好，或者可能需要寻找一种成本更高的微调方式，类似于继续进行预训练，以将知识融入模型，然后再进行微调，例如LORA微调。我认为LORA微调可能只对现有的基准模型进行提升和补充有意义。当然，如果基准模型本来就不太好，那么可能不会获得太大的收益，或者预期的性能可能不会特别出色。</p><p>&nbsp;</p><p></p><h4>成本问题</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到了成本问题，确实，在特别是语言模型（LM）这个领域，模型的推理成本随着参数数量的增加呈指数级增长。我们了解到，网易内部使用Copilot不仅仅涉及生成代码，还可能涉及解释代码推理方面。用户可能以多种不同的方式使用它。您是使用一个巨大的模型或者一个具有固定参数的模型来支持所有使用方式，还是根据不同的使用方式智能地调整背后模型的大小呢？</p><p>&nbsp;</p><p>刘东：我们选择了后者的方式，即根据不同的使用方式智能地调整背后模型的大小，这是有充分考虑的。从成本和效率的角度考虑，这是一个综合的决策。特别是在编程场景中，代码提示是一个非常高频的任务，因为每输入几个字母，都会触发一次提示请求。在这种情况下，模型需要足够快，因为如果太慢，程序员可能会自己完成输入，这样就不会提供太多价值。</p><p>&nbsp;</p><p>另一方面，这个场景通常涉及到代码生成，相对来说是一个相对固定且不太复杂的任务，与通用任务相比，难度较低。因此，我们更倾向于选择规模较小的模型，以确保效率，并且不会明显降低质量。</p><p>&nbsp;</p><p>对于像代码解释、调试分析或注释生成这样的任务，难度较大，可能需要更大的模型才能实现良好的效果。但好的一点是，这些任务通常不会有太高的使用频率，因此在这种情况下，我们可以选择相对更大的模型，而不需要进行大规模的冗余部署，因为请求量本来就不会太大。这种综合考虑帮助我们更好地控制了成本。</p><p>&nbsp;</p><p>鱼哲：随着大型语言模型的发展，特别是像Copilot这样的方式，例如像LLAMA这种模型，我们是否仍然需要手动编写注释呢？大家讨厌别人不写注释，但自己也不喜欢写。</p><p>&nbsp;</p><p>刘东：写注释与不写注释在很大程度上是一个习惯问题。写注释的主要目的首先是为了给自己提供提示，使代码更容易理解和维护。其次，注释也有助于他人理解代码，尽管注释的覆盖度要求可能并不高，因为大模型可以帮助填充一些细节。然而，写注释的程度可以因程序员而异，有些程序员可能倾向于写详尽的注释，解释每个细节，而有些人可能只写简要的概述性注释。这与个体的写作风格和代码质量意识有关。</p><p>&nbsp;</p><p></p><h4>BI产品和低代码平台</h4><p></p><p>&nbsp;</p><p>鱼哲：在网易杭州研究院，我们不仅在内部广泛应用这些先进的技术，还在一些领域提供对外的技术支持和合作机会。在对外方面有哪些技术合作呢？</p><p>&nbsp;</p><p>刘东：除了为内部提供技术支持，我们还将这些大语言模型的能力整合到商业化产品中，以为客户提供更多的服务。其中，我们的代表性产品之一是BI产品。通过整合大语言模型的能力，为BI产品引入了自然语言交互功能，使用户可以通过自然语言查询所需的数据和报表，这完全是由大语言模型驱动的。</p><p>&nbsp;</p><p>另一个重点领域是低代码平台，我们的CodeWave平台，它通过低代码编程方式，降低了编程的门槛，提高了编程的效率，从而帮助企业节省成本并提高效率。在这个平台中，我们引入了大语言模型的能力，以提高效率和降低编程门槛。这两个领域是我们当前主要投入和发展的方向。</p><p>&nbsp;</p><p>鱼哲：我们还有一个低代码产品，可以介绍下这个产品的使用体验吗？</p><p>&nbsp;</p><p>刘东：Low code 不同于 0 code，简而言之，它是一种基于可拖放的方式进行软件开发的方法。它不要求专业的程序员从头编写代码，也不同于完全无需编码的 0 code 方式。在低代码中，你可能需要配置一些固定的模板，定义数据模型，设计流程结构，还可以使用预定义的组件，通过拖拽的方式连接各种逻辑，最终生成软件产品。</p><p>&nbsp;</p><p>这种方法的核心优势在于相对于传统的完全编码软件开发，用户需求较低，无需像计算机专业的本科毕业生或有丰富经验的人才那样写代码。但与 0 code方法相比，它仍然保持了软件开发的灵活性，因为它可以实现复杂的逻辑。低代码的定位介于传统软件开发和 0 code 之间，兼顾了易用性，同时也能满足一些较为复杂的软件开发需求。</p><p>&nbsp;</p><p>鱼哲：您能简单介绍下这个产品的对外发布节奏吗？我看到官网上有些资料相关。</p><p>&nbsp;</p><p>刘东：我们目前在网易数帆官方网站上提供了一些基础材料和介绍。此外，我们即将在11月2日举行2023网易数字+大会，届时将提供更详细的产品介绍以及有关技术的分享。我们期待在发布会上与大家分享更多信息。</p><p>&nbsp;</p><p></p><h4>AIGC在数据分析应用上的挑战</h4><p></p><p>&nbsp;</p><p>鱼哲：回到刚才提到的ChatBI，我在以前做业务时常常需要与BI同事沟通，例如我想了解最近三个月华北地区哪个行业的客户增长最快，哪个行业的客户有一些困难，以及他们所遇到的产品使用情况。这种情况通常需要等待一两天的时间，不管是BI同事还是我自己去做，都需要花费大量的时间来查看数据地图，查看每个表的结构以及做相关的SQL查询，因为我们需要定义特定的指标，例如复购、沉默和活跃等。这是一个非常复杂的问题，之前尝试了许多模型，但它们存在幻觉的问题，导致了一些错误的结果，这是不能接受的。在BI领域，这个问题是非常严肃的，我们不能容忍存在幻觉的问题。我想了解一下，你们是如何处理这个问题，如何解决这种复杂性挑战的。</p><p>&nbsp;</p><p>刘东：我们面对的确实是一个巨大的挑战，而且我们在这个产品上花费了很长时间，因为BI场景是非常严肃的，它的任务是提供准确的信息。如果我们只是编造数据或者输出不可信的信息，那这个任务基本上就失败了。因此，我们采取了多重方法来尽量避免这个问题。</p><p>&nbsp;</p><p>首先，我们从算法的角度出发，努力构建了一个优秀的领域子模型，以尽量避免通用模型的幻觉问题。我们收集了大量的数据和各行各业的常见数据报表，通过数据增强和训练，使模型的能力更强。这个领域子模型专注于解决数据分析场景，能够通过自然语言输入生成高质量的SQL查询语句。</p><p>&nbsp;</p><p>其次，尽管模型很强大，但我们也意识到大型生成式模型不是100%可控的，因此我们在产品层面进行了多方面的工作。我们引入了一个"可信AI"的概念，包括过程可验证、用户可干预和产品可运营三个方向。</p><p>&nbsp;</p><p>过程可验证：我们不仅仅相信模型生成的SQL查询语句，而是使用一个查询语句解析引擎将其解析为人类可理解的语言，以确保用户了解模型的工作原理。如果发现错误，用户可以立即识别并不信任结果。用户可干预：我们允许用户对模型生成的查询进行干预。用户可以更改条件、操作等，以纠正错误或调整查询。这提供了用户对结果的额外控制。产品可运营：我们希望产品不仅仅是一个静态的工具，而是能够随着用户的使用变得更智能。我们收集用户的行为习惯，正例和反例，不断优化模型。我们也提供产品配置，以使模型理解各行各业的“黑话”和简称。通过不断的运营，使模型越来越智能，适应用户的需求。</p><p>&nbsp;</p><p>这些方法的结合，以及其他细节的优化，使我们的产品更可信、可控，提高了用户的工作效率。</p><p>&nbsp;</p><p>鱼哲：这是否意味着当用户使用产品时，他们需要在某种程度上提前注入表结构的一些信息？或者说，模型能够根据表的结构自行猜测字段的含义？</p><p>&nbsp;</p><p>刘东：大模型是通过自主猜测的。只要提供底层表结构，大模型可以自动获取这些信息，所以用户在一开始使用时通常不需要太多干预。</p><p>&nbsp;</p><p>鱼哲：您刚刚提到的这个反馈收集非常有趣，因为通过良好的RLHF方法，模型的性能可以显著提高。</p><p>&nbsp;</p><p>刘东：是的，必须逐步将其系统运营，使其随着使用而不断智能化，而不是采取一劳永逸的方式。这样做的话，问题通常不会被永久解决。但一旦将其运营起来，将负面案例的反馈馈送给它，它就会不断改进。</p><p>&nbsp;</p><p>鱼哲：刚才有个直播观众的提问：“对于这些垂直领域的模型，你们是在基础大模型的基础上进行微调，还是持续进行预训练，或者是从零开始使用领域样本训练参数较小的模型？”</p><p>&nbsp;</p><p>刘东：我们通常是基于基础的基座模型进行调整。网易内部我们已经进行了基础模型的玉言，这是一个从头开始训练的基座模型。从头开始训练的好处是，我们大致了解未来要覆盖的领域，因此在训练过程中，我们有意地将一些领域相关的数据融入其中。例如，如果要处理编程任务，就会注重将代码相关的数据纳入模型。如果要处理SQL的任务，就会加入一些SQL的数据。这个基座模型相对来说比较通用。然后，我们会在这个基础上为每个领域创建领域特定的子模型，以进行适配。</p><p>&nbsp;</p><p></p><h4>数据的重要性</h4><p></p><p>&nbsp;</p><p>鱼哲：您提到的ChatBI的问题，如果拥有一个基础模型并为其创建功能，同时提供大量数据时，我发现在这个工作中，最大的挑战实际上不在于微调模型，而是在于找到合适的数据，并将其准备成可供模型使用的形式。我认为这是最困难的部分。</p><p>&nbsp;</p><p>在研究一篇论文时，我注意到在数据稀缺的情况下，他们提出了一个新名词叫RLAIF，即通过人工智能来生成强化学习所需的数据，以支持强化学习任务。</p><p>&nbsp;</p><p>对于像ChatBI这样的项目，我认为您需要大量的数据来对基础模型进行调整，而且需要具备高度的语义和推理能力。模型的规模不会小，而随着模型规模的增加，调整参数需要更多的精力、计算资源和数据。</p><p>&nbsp;</p><p>我想了解一下，您是如何在数据准备方面处理这些挑战的？因为实际情况是，在这类项目启动之初，数据通常不够整洁，或者很多人最初并不清楚这些数据可能会有哪些用途。您是如何处理这一问题的呢？</p><p>&nbsp;</p><p>刘东：无论是在ChatBI领域还是在以前的代码自动补全项目中，数据准备工作都是至关重要的，也是相当具有挑战性的任务。我们投入了大量精力来应对这个挑战。</p><p>&nbsp;</p><p>在ChatBI项目，我们获得数据的途径多种多样。首先，我们会在网上寻找一些开源数据。在这个领域，因为传统方法已经发展了相当长的时间，所以存在许多开源的评测数据，以及公开数据表结构的定义。我们可以利用这些表结构，以人工智能的方式自动生成问题和答案，从而使用AI来生成数据，这是一种当前相对流行的方法。此外，我们还投入了大量人力资源来进行数据的搜集和标注工作，将各个来源的数据汇总，综合使用，以满足我们的需求。</p><p>&nbsp;</p><p>鱼哲：我觉得这个趋势在从事NLP领域的同事中也非常明显。人们开始广泛使用语言模型来执行以前需要使用多个专门的小模型来完成的任务。举例来说，以前我们需要训练专门的模型来执行诸如语音到文字转换、地址解析以及标点符号分割等任务。而现在，像您刚才提到的，在生成数据方面也使用了语言模型。这引发了一个问题，即您是否认为大型语言模型会逐渐取代NLP领域中使用的多个小型专家模型呢？</p><p>&nbsp;</p><p>刘东：从能力的角度来看，大语言模型已经展现出强大的表现，但我们需要根据投资回报率（ROI）来判断是否使用这些大型模型。这意味着，尽管它们非常有能力，但我们不必在每个场景中都采用大语言模型。例如，对于一些小型NLP任务，我们可以使用较小的模型，它们成本低廉，在线上表现良好，同时可以满足高并发需求，因此在这些情况下，不必迫切转向大型语言模型。</p><p>&nbsp;</p><p>当然，大语言模型的能力毋庸置疑，它们在某些复杂任务上可能表现更出色。然而，我认为大语言模型与领域专家模型之间不是相互替代的关系，而是可以共存的。在不同的情境中，可以选择使用不同的模型，以便最好地满足特定需求。这种差异化的方法可能是更好的选择。</p><p>&nbsp;</p><p></p><h4>模型选择的问题</h4><p></p><p>&nbsp;</p><p>鱼哲：因为我之前有时也会采取简便的方式，直接使用大型模型，特别是在拥有免费积分的情况下。但随着时间的推移，我发现在考虑长期投资回报时，仍需要寻找传统的常规模型。</p><p>&nbsp;</p><p>刘东：在ChatBI中，除了大型模型之外，有时需要将小型模型和大型模型结合使用。这是因为尽管大型模型拥有出色的能力，但它在成本和执行速度上可能存在一些问题。小型模型则执行速度非常快。在某些任务中，如果你不断地调用大型模型来执行和解析，可能会影响用户体验，因此需要进行综合考虑。</p><p>&nbsp;</p><p>鱼哲：您刚刚也提到了，一方面，生代码生成式大模型或ChatBI生成式大模型等，都在数据收集方面面临巨大挑战。我认为数据是其中的一个技术挑战。除了数据之外，在这个过程中还有哪些方面您认为非常具有挑战性，非常难的呢？</p><p>&nbsp;</p><p>刘东：我认为数据确实是一个巨大的挑战，不仅在收集方面，还在清洗数据方面需要耗费大量精力。清洗数据是非常关键的，因为如果不做好，会直接影响模型的效果。我们在数据清洗方面投入了大量精力，因为高质量的数据是确保模型效果的基本保障，这是第一个挑战。</p><p>&nbsp;</p><p>另一个挑战是一旦大模型的能力达到足够强，如何在实际业务场景中找到合适的应用场景，确保它能够创造价值。这方面也非常具有挑战性，因为大模型面临效率、速度和成本等问题。虽然它在很多场景下效果出色，但用户体验可能难以保证。此外，大模型作为生成模型，不可能百分之百准确。如何找到那些既能容忍错误，同时又能为用户带来实际帮助的场景，让模型成功落地，也是一个非常大的挑战。</p><p>&nbsp;</p><p>总之，技术本身的能力与业务场景的结合是非常关键的，只有找到合适的结合点，大模型的能力才能真正发挥作用，用户才能真正感受到其价值。否则，它将一直停留在演示的层面，其技术的价值和影响力都会受到限制。</p><p>&nbsp;</p><p>鱼哲：有时候出现了上下文的误解。例如，用户可能要求搜索最近三个月内是否有玩过某个游戏，但可能会被错误地理解为最近三年。在ChatBI场景中，我们可能有一个SQL生成工具，但它生成的SQL语句缺少一个关键的"where"子句。现在，关于ChatBI，是它能够接受用户的自然语言查询并自动触发查询任务，还是它只返回SQL代码，用户需要将SQL代码用于传统的数据仓库查询窗口中查询？</p><p>&nbsp;</p><p>刘东：我们的当前设计是完全自动的。当用户提供一个查询后，系统会立即执行，而且像之前提到的那样，各种AI操作都会在执行后进行解释，并展示各种条件。用户可以根据查询结果和这些解释来判断查询的可靠性。</p><p>&nbsp;</p><p></p><h4>大模型产品价值的体现</h4><p></p><p>&nbsp;</p><p>鱼哲：所以用户不仅可以看到生成的代码，还能了解为什么会生成这段代码。此外，生成的数据会以表格的形式展示，用户可以导出数据。产品是否还提供可视化或建模分析能力，还是用户需要自己去处理这些方面的工作？</p><p>&nbsp;</p><p>刘东：我们目前主要提供可视化展示，对于后续的建模分析，我们正在进行研究和探索。</p><p>&nbsp;</p><p>鱼哲：您之前提到技术研究院需要同时具备技术和业务的理解，而最终需要为其结果负责。客户，无论是网易集团内部还是外部，都渴望了解这些生产力工具如何提升效率。这包括自动代码生成、SQL自动生成以及低代码平台等技术，它们都旨在提高生产力。然而，生产力提升在实际中往往是一个具有挑战性的问题。难以证明这些技术是否可以直接提高生产力。关于如何证明生产力提升的问题，您是怎么解决的？</p><p>&nbsp;</p><p>刘东：我们一直在思考和探索这个问题，因为要传达技术的价值，需要从多个角度来考虑如何证明其价值。我们非常关注用户的反馈和实际使用数据，这对于衡量技术的有效性至关重要。</p><p>&nbsp;</p><p>在我们内部使用低代码工具，例如代码补全工具，类似于Copilot工具，我们详细记录用户的使用情况，特别关注用户采纳提示的比例以及AI自动生成代码的比例。这有助于我们了解技术是否真正帮助用户减少编码工作，还是只是一个演示性的工具，用户不太愿意采纳其中的建议。</p><p>&nbsp;</p><p>同样，对于ChatBI和低代码工具，我们也密切关注用户的使用情况。例如，如果没有Chat功能，很多业务人员可能无法自行使用BI工具来查询数据。但如果引入Chat功能，我们关心是否有人在使用，以及他们的使用频率。在低代码工具方面，我们使用自然语言来生成逻辑，然后观察生成情况和占比。这些数据帮助我们衡量技术是否真正提高了生产力，帮助用户在成本降低和效率提高方面取得进展。我们非常关注这些方面的数据。</p><p>&nbsp;</p><p>鱼哲：当我们致力于提高生产力效率时，如AIGC或大型模型的出现与以往的机械发明有很大不同。以前的机械或半自动机械本质上需要人的操作。例如，使用除草器可能需要有人操作设备，而自动化机械则需要人的干预。然而，像您刚才提到的ChatBI，如果今天我可以使用自然语言描述业务需求，然后它可以为我生成正确的SQL查询并检索数据。那么对于那些传统的数据分析专业人员或BI同行，他们的存在可能会面临一些挑战，因为这种技术的出现可能改变了传统的数据分析方法。</p><p>&nbsp;</p><p>刘东：我认为这些工具主要是作为一种助力的角色存在的，而人的价值仍然不可或缺。无论是在BI领域还是在编码领域，它们的核心目的是帮助人们在一些简单重复的工作中提高效率。当拥有这种提高效率的工具时，我认为人们可以解放更多的时间，用于思考业务等更有价值的事情。这包括如何改进业务、更好地理解用户需求，以及如何提供更出色的软件产品。这是一种从不同角度思考问题的方式，而不是完全取代人的角色。因为这些技术目前正在逐渐发展，它们还没有达到100%可信并且能够胜任一切的状态。</p><p>&nbsp;</p><p>鱼哲：我听闻有些公司在内部开展了类似工作遇到了许多障碍。其中一部分障碍来自于开发团队，他们担心这种工具可能会与他们的工作发生冲突，或者一旦工具成熟，公司将不再需要他们。在你们尝试推广这些工具的过程中，是否也遇到了类似的挑战？</p><p>&nbsp;</p><p>刘东：我们目前并没有遇到这类挑战，因为我们进行了一些统计，发现从软件研发的角度来看，程序员实际花在编码上的时间并不占很大比例。编码只是他们工作的一小部分，更多的工作包括需求分析、整体设计以及与其他方面的对接等。编码所占的时间并不是很多。我们的目标并不是取代程序员，而是让他们能够更多地投入需求分析和用户场景理解，以便提高编码的质量和整个软件产品的效果。</p><p>&nbsp;</p><p></p><h4>大模型产品的私有化部署</h4><p></p><p>&nbsp;</p><p>鱼哲：我觉得程序员这个称呼有点狭隘，因为在编写代码时，实际上他们不仅仅在写代码，更多的时候在进行工程工作，也就是做工程师的事情。工程师通常需要将来自外部的各种复杂难以理解的需求和分散的模块整合到一起。在这方面，生成式模型的能力是不可替代的，不论是在短期还是长期。我认为很难通过直接应用一个大型模型来完成需要花费多年时间理解和深入了解的业务。这种情况下，使用生成式模型可能不太适用，因为你需要时间来积累对业务的深刻理解，然后才能进行创新性的工程工作。</p><p>&nbsp;</p><p>你刚刚描述的这些能力听起来确实非常有趣和具有吸引力。然而，我们也明白，许多公司，包括像网易自己开发Copilot时，通常出于安全和特殊场景的考虑，不愿意使用市场上通用的产品。这种担忧在中国和美国的科技公司中都非常普遍，特别是当公司规模较大时，它们通常更倾向于采用私有化部署，无论是在公司自己的数据中心还是在云上的IDC。</p><p>&nbsp;</p><p>在传统金融领域，如银行、保险和证券等领域，这些能力可以显著提高工作效率和效能。然而，由于国家监管要求或公司性质的原因，很多银行和保险公司通常需要确保技术提供的方式支持私有化部署。我们会积极考虑这些需求，以满足不同客户的特殊要求。网易在这方面是如何考虑的？</p><p>&nbsp;</p><p>刘东：我们的模型都是基于自己的基座模型调用的，因此完全可控。我们也提供了私有化部署的能力。除了不断优化模型性能，我们还专门组建了一个工程团队，专注于推理效率的优化、部署方案的设计以及各种硬件适配工作。</p><p>&nbsp;</p><p>在私有化部署方面，我们考虑如何尽量降低用户的成本，因为大型模型的成本相当高。首先，我们根据业务场景找到了适合的模型规模，而不是盲目地追求巨大的规模，这样可以减少硬件集群的复杂性。</p><p>&nbsp;</p><p>其次，我们进行了大量的工程优化。这包括引入业界开源的先进技术，以提高性能。我们还根据模型的特点进行了自定义适配，包括自定义内核等，以提高吞吐量和效率。</p><p>&nbsp;</p><p>此外，我们还考虑了量化加速等操作，以确保资源的可控性。因此，即使使用普通的显卡，我们也可以将大型模型部署上，并为用户提供良好的体验。这些措施都有助于提供高效的私有化部署解决方案。</p><p>&nbsp;</p><p>鱼哲：在私有化部署方面，你们是可以进行多方面的性能优化的吧？比如模型量化、压缩以及重新编写一些核心代码，从而提高性能和效率。在私有化部署的时候，一种方式是用户直接使用基座模型，这是一个即插即用的解决方案。另一种是用户在使用一段时间后，需要进行自定义微调，比如强化学习微调（RLHF）或自适应微调（RLAIF）等，你们是如何解决的？</p><p>&nbsp;</p><p>刘东：我们提供两种服务模式，以满足用户的需求。首先，我们提供一种自助工具模式，类似于业界已有的微调工具和强化学习工具。这些工具包括数据集管理、标注、训练任务和部署任务等功能。用户如果拥有足够的实力和理解相关流程，可以自行使用这些工具来满足其需求。用户可以随时尝试不同的方法，进行A/B测试，以确定哪种方式效果更佳。</p><p>&nbsp;</p><p>其次，对于一些重要客户，我们也提供定制化的服务。在这种情况下，我们的算法专家会与客户合作，共同解决他们特定的问题。我们会与客户密切合作，确保他们获得最佳的解决方案。无论用户自行使用工具还是选择我们的定制化服务，我们都将竭诚为他们提供支持。</p><p>&nbsp;</p><p>鱼哲：我也认为在私有化部署后再进行模型训练是一项颇具挑战的任务。从前我在这个领域有一段时间的从业经验，我深知这种工作需要投入大量时间和精力。此外，私有化部署环境通常存在标准不一致的问题，因此需要耗费额外的时间来解决各种复杂情况。</p><p>&nbsp;</p><p>刘东：每个客户的环境都有所不同，因此我们的目标是将产品尽量标准化，同时确保工具功能完善。这种方法有助于客户自行进行调整、训练和部署，降低了使用成本和门槛。只要他们理解这个过程，几乎都可以通过简单的点击鼠标来完成，而不需要深入编程或处理复杂的问题，这对用户来说更加容易接受和理解。</p><p>&nbsp;</p><p>鱼哲：在应用 AIGC 技术能力来提高软件工程效率时，您认为在业务端的落地过程中，最关键的角色通常会是什么？</p><p>&nbsp;</p><p>刘东：我认为在这个过程的每个环节都非常关键，没有哪一个环节可以忽略。首先，理解场景和找到使大模型落地的有价值的点至关重要。然后，需要探索这些点，以确定大模型的能力是否足够可控和可解决。如果算法可以解决问题，那么从工程角度来看，是否有足够的投资回报、性价比和用户体验也是非常重要的。最后，如何将这一切标准化地交付给用户，确保他们能够持续使用，而不仅仅是一个演示，也是一个巨大的挑战。每个环节都需要表现出色，才能成功完成这项工作。</p><p>&nbsp;</p><p></p><h4>对软件工程未来的看法</h4><p></p><p>&nbsp;</p><p>鱼哲：您怎么看待编程以及软件工程的未来？</p><p>&nbsp;</p><p>刘东：我认为AI和AIGC技术并不是要取代软件工程，而是要为软件工程提供更强的支持。随着数据积累、通信和计算能力的提升，对软件工程的要求变得越来越高，而AI技术可以提高软件工程的生产力。未来，软件工程师的角色可能会发生变化，从以程序员为主导变为人机协作的模式，工程师需要花更多精力学习和应用AI技术，以提高工作效率和生成更好的软件。</p><p>&nbsp;</p><p>鱼哲：这实际上涉及到一个重要的哲学性问题，即通用性与特殊性之间的平衡。在优化时，我们必须在通用性和特殊性之间做出权衡。因为优化通常是为了特定场景和硬件而进行的，它可能会牺牲通用性。您认为，在未来多久内，我们是否会看到通过代码生成或自动化方式，针对特定硬件环境进行优化呢？比如剪枝、量化、压缩和算子的自动生成。</p><p>&nbsp;</p><p>刘东：实现这一愿景需要大量的基础积累和技术成熟度。目前的代码生成技术主要依赖于已有的能力和沉淀的代码，然后通过大型模型的学习和自动生成来解决更多问题。这种自动化生成代码的技术在广泛的硬件和环境中得到应用，可能需要更多的积累和实践。</p><p>&nbsp;</p><p>对于新的硬件和场景，手动优化仍然是必要的，因为了解硬件和业务逻辑、分析性能问题等需要人的专业知识。自动化优化工具需要基于已有的知识和经验，而不是凭空生成优化方案。因此，即使技术不断发展，仍然需要工程师的专业知识来指导和验证自动生成的代码。未来，随着技术的发展和积累，可能会有更多的通用性优化工具出现，但在新的硬件和场景中，人工干预和专业知识仍然是至关重要的。这是一个逐步演进的过程。</p><p>&nbsp;</p><p>鱼哲：这是一个非常有趣的问题，因为实际上几乎所有的模型在实际应用中都需要经过优化才能顺利落地。在许多情况下，尤其是当涉及硬件时，例如服务器端或者嵌入式设备，优化工作变得尤为重要。</p><p>&nbsp;</p><p>举一个例子，最近非常受欢迎的 Vox 模型，它能够根据自然语言指令为机器人生成指令。然而，在将这一模型应用到嵌入式设备时，通常需要进行大量的优化工作。这种优化工作可能包括模型规模的压缩，性能优化，硬件加速以及适用于嵌入式设备的特定算法的选择。对于这种情况，通常需要工程师深入了解硬件的性能特征以及特定领域的需求。</p><p>&nbsp;</p><p></p><h4>活动推荐：</h4><p></p><p>11月2日，2023网易数字+大会将于杭州举办，网易数帆将带来AIGC技术与云原生、大数据、低代码结合的进展，下午的创新技术论坛，还将全面对外分享网易杭州研究院技术创新范式，带来领域大模型技术揭秘、开源实践分享等话题，欢迎扫描下图二维码或点击链接报名围观：i.163yun.com/obq6t7202</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/11c17401a1fe96ca78f916813cd03ee1.png" /></p><p></p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</id>
            <title>OpenAI 刚刚又杀死了一批初创公司</title>
            <link>https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y1vbSBSYjULcGd4kgLpG</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 07:40:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 创业, ChatGPT, PDF, 初创公司
<br>
<br>
总结: OpenAI最近在ChatGPT上引入了新功能，用户可以上传多种类型的文档，包括PDF，并在同一对话中使用不同的工具。这一更新对于一些初创公司来说可能是一个打击，因为他们的业务正是基于ChatGPT无法直接与PDF交互的现状。然而，对于一些已经成功打包了ChatGPT的初创公司来说，他们仍然面临着发展前景和清算的选择。 </div>
                        <hr>
                    
                    <p></p><blockquote>围绕别人家的大模型创业，盈利快，死得也快？</blockquote><p></p><p>&nbsp;</p><p>从目前的情况看，每当OpenAI在ChatGPT上发布新功能时，都会因为对开发类似功能的初创公司造成冲击而受到指责。OpenAI日前刚刚又为ChatGPT引入了两项新功能，其一是“上传多种类型文档”、其二为“无需切换对话即可使用工具”。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2547e3b3e5fdb86572fd275522b04970.png" /></p><p></p><p>&nbsp;</p><p></p><blockquote>ChatGPT/GPT-4迎来重要更新：可上传任意PDF文档并询问其内容。无需切换聊天即可直接使用新工具。</blockquote><p></p><p>&nbsp;</p><p>在此次更新之后，ChatGPT不仅能够直接读取PDF，还可以在同一对话当中支持多种文档类型，包括PDF、图像和CSV等等。以往，用户只能在默认模式下上传图像，但现在已经可以无缝上传文档并立即开始提问，这大大扩展了ChatGPT平台的多样性和可用性。</p><p>&nbsp;</p><p>此外，用户也不再需要指定自己要用的ChatGPT模式。如今，浏览、高级数据分析（原名Code Interpreter代码解释器）和DALL-E 3现可在同一对话中直接使用。GPT将自行确定激活不同模式的适当时机，例如在用户要求创建图像时调用DALL-E。</p><p>&nbsp;</p><p>相信很多朋友都有在浏览器里浏览几百页的PDF文件，想要从中提取有用数据和摘要信息的经历，其过程相当之痛苦。正因为如此，此次更新才在ChatGPT Plus用户当中获得了高度评价。</p><p>&nbsp;</p><p>但这次更新也造成了广泛影响。有声音认为，这种新的“多模态”更新将毁掉至少数十家初创公司，其中耳熟能详的名号包括ChatPDF、AskYourPDF和PDF.ai等等。不少初创公司恰恰就是看准了之前ChatGPT无法与PDF直接交互的现状，才构建起自己的业务体系。既然现在ChatGPT可以操作PDF了，那这些后起之秀还有哪些业务空间可以挖掘？</p><p>&nbsp;</p><p></p><h2>面向AI的“打包初创公司”面临毁灭打击？</h2><p></p><p>&nbsp;</p><p>支付巨头Stripe公司产品负责人Sahar Mor在LinkedIn上写道，“OpenAI刚刚推行的一项举措可能直接消灭数十家AI公司。”他还专门提到了“打包初创公司。”这类企业在本质上就是把ChatGPT等API“打包”起来形成自己的业务，使用聊天机器人的底层技术提供某种原厂商未能直接提供的服务。</p><p>&nbsp;</p><p>但建立AI打包业务的创始人们，最初可能未必是要故意利用ChatGPT的软肋。今年3月，OpenAI宣布AI服务对外开放，欢迎开发者们将ChatGPT整合到自己的应用程序和产品当中。</p><p>也就是说，提供PDF分析功能的初创公司只是打包商里的一部分，还有很多在提供其他各种补充性功能。</p><p>&nbsp;</p><p>目前最具市场影响力的打包厂商当数Jasper AI，该公司在Coatue和Bessemer Venture Partners等大型风险投资公司的支持下，成功在今年开年之际获得15亿美元估值。</p><p>&nbsp;</p><p>他们做对了什么才得到如此夸张的估值？答案很简单，围绕OpenAI的GPT模型开发一套专门针对企业营销团队的“AI领航员”（AI Copilot）。</p><p>&nbsp;</p><p>但根据技术外媒The Information报道，随着该公司内部估值的一路走低，其业务定位似乎也陷入了困境。今年7月，Jasper AI还曾宣布裁员。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb1fafa20d1bdcf866fc9ada1a2b772a.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>Jasper 以 $1.5B 估值筹集了 1.25 亿美元，这也无济于事。也许GPT打包模式并不适合初创公司。</blockquote><p></p><p>&nbsp;</p><p>也许其他使用ChatGPT等工具为用户提供PDF交互功能的初创公司，都将面临类似的悲惨命运。</p><p>&nbsp;</p><p>今年5月，数据科学家Alex Reibman发布了ChatOCR。这是一款ChatGPT插件，能够“从PDF中读取文本，包括扫描和手写内容。”但在上周末的更新之后，他在X上开展了一项民意调查，询问用户“既然现在ChatGPT已经内置了PDF处理功能”，大家还愿不愿意继续使用插件。在210名受访者中，72.4%的人预计插件“使用量将会减少”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d4972ac35c66c649dec1af24f5ce82d.jpeg" /></p><p></p><p></p><blockquote>我们是本次更新的“受害者”之一。我们运行着ChatOCR，ChatGPT商店中众多PDF处理插件之一（我们主打的是OCR）。过去3个月来，我们的月度经常性收入达到3500美元。大家认为，这次更新会对我们造成怎样的影响？</blockquote><p></p><p>&nbsp;</p><p></p><h2>是面临清算还是仍有发展前景？</h2><p></p><p>&nbsp;</p><p>在围绕PDF创业的公司中，PDF.ai是一家能赚钱、自给自足而且利润率可观的企业。PDF.ai公司创始人Damon Chen表示，“我们的目标不是成为又一家独角兽企业，几百万美元的年度经常性收入对我来说已经足够了”。</p><p>&nbsp;</p><p>OpenAI的更新对PDF.ai确实也带来了一定冲击，他承认，体量太小的初创公司终将遭到淘汰，而由风险投资供养的大块头在烧光现金之后也挺不住。</p><p>&nbsp;</p><p>但Chen仍然带有希望：“昨晚我和妻子就ChatGPT更新聊了聊。我问，如果PDF.ai最后没能成功，该怎么办？她轻描淡写地说一个项目的失败不算什么，另起炉灶好了。”</p><p>&nbsp;</p><p>如今距离ChatGPT正式亮相已过去近一年，OpenAI正逐渐为其添加更多新功能。OpenAI的终极目标是实现通用人工智能（AGI），而阅读PDF等进展只是这个庞大目标中的一小部分。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f0d4e72b50ef5c1c97b57f25c6635a6.jpeg" /></p><p></p><p>&nbsp;</p><p>正如Tenstorrent公司人工智能总监Shubham Saboo评论的那样，“ChatGPT 的战略：巩固、创新和统治。ChatGPT 会成为终极 AI 超级应用程序，将 Midjourney、PDF Chat、Perplexity AI 和高级数据分析全部结合在一个应用程序中。”</p><p>&nbsp;</p><p>从某种程度来说，只要不具备能与竞争对手拉开显著差距的“护城河”，初创公司就随时面临被劫掠的风险。那么在别人的 API 之上建立自己的业务还有前途吗？</p><p>&nbsp;</p><p>让人意外的是，不少人对此依然表示乐观。支付服务商Stripe公司产品负责人Sahar Mor表示，“打造用户友好的界面和更易用的功能仍有其实际意义，因此针对特定细分市场的垂直初创公司将继续保持其主导地位。真正面临风险的，主要是那些横向延伸的AI初创企业。”</p><p>&nbsp;</p><p>分析服务商Glass Acres创始人Mark Zahm也认为，“只要GPT还存在，GPT打包方案就会伴其成长并蓬勃发展……”AI爱好者Rowan Cheung则在X上分享道，“大家现在怎么不讲那些靠GPT打包方案赚大钱的故事了？”</p><p>&nbsp;</p><p>在他看来，不少初创公司在网络流量上的表现已经超越了价值数十亿美元的传统公司，而且其业务定位均匀分布在打包、微调和专有模型等各个领域。也就是说，部分GPT打包方案的月度访问量，比某些估值数十亿美元的企业还要高。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e078f9ac6611e19b134bf9ee94d6dc2.png" /></p><p></p><p>图片来源：<a href="https://www.infoq.cn/article/iLZYudwYlgARoXmjawJb">ChatGPT 已成为 2023 年最大金矿，大家是怎么靠它挣到钱的？</a>"</p><p>&nbsp;</p><p>开发GPT打包方案的初创公司，主要为那些需要整合AI功能的企业提供一种更经济、更高效的选项，避免从头开始构建复杂的模型体系。</p><p>&nbsp;</p><p>除了OpenAI之外，众多初创公司正在不断涌现，而生成式AI业务已经成为其冲击独角兽之路上的一股重要推力。根据风险投资公司Accel最近发布的报告，这些新独角兽企业中有60%属于生成式AI范畴。去年，欧洲和以色列的生成式AI初创公司投资总额接近10亿美元。相比之下，美国生成式AI初创公司同期获得的注资更是超过140亿美元。但正如报告中所强调，这140亿美元资金的具体分配并不均衡，单是OpenAI一家就分走了其中100亿美元。</p><p>&nbsp;</p><p>根据近期报道，部分照片AI应用和AI聊天机器人服务商赚到的绝对利润，反而还高于生成式鼻祖ChatGPT。今年9月，Chat &amp; Ask AI和ChatOn——AI聊天助手都产生了可观的收入，分别达到近338万美元和211万美元。</p><p>&nbsp;</p><p>此外，AI Chatbot——Nova和AI Chatbot: AI Chat Smith也不甘落后，同期收入分别为144万美元和172万美元。而由a16z支持的聊天机器人初创公司Character.ai也在市场上闹出不小的动静，截至今年9月下载量已达239万次。</p><p>&nbsp;</p><p>也就是说，哪怕OpenAI凭借其多模态功能领先了竞争对手十步，各位初创选手也没必要悲观放弃。相反，也许底层核心技术的升级能提供丰富灵感、帮助他们开发出更好的后续产品。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>"</p><p><a href="https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10">https://www.businessinsider.com/openai-chatgpt-pdfs-ai-startups-wrappers-2023-10</a>"</p><p><a href="https://twitter.com/thealexker/status/1680626018522914817">https://twitter.com/thealexker/status/1680626018522914817</a>"</p><p><a href="https://twitter.com/AlexReibman/status/1718848888088793487">https://twitter.com/AlexReibman/status/1718848888088793487</a>"</p><p><a href="https://twitter.com/Saboo_Shubham_/status/1718653456926359855">https://twitter.com/Saboo_Shubham_/status/1718653456926359855</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</id>
            <title>从互联网到云计算再到 AI 原生，百度智能云数据库的演进</title>
            <link>https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fha6fjAkOt5wCiw60B9l</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 06:23:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库技术, 云原生, AI技术, 百度智能云
<br>
<br>
总结: 作为计算机系统的核心基础软件之一，数据库技术的发展备受关注。随着云计算技术的发展，云原生和分布式数据库成为主流，具有高可用性、可扩展性和低成本等优势。AI技术的不断发展使得AI与云计算结合成为可能，云原生为数据库提供了基础条件，AI成为云原生数据库持续演进的驱动力。百度智能云在数据库领域不断创新，推出了云原生数据库GaiaDB、云数据库GaiaDB-X和数据传输服务DTS等产品和解决方案，以支持大数据和AI应用在行业中的挑战。百度智能云团队还推出了《百度智能云数据库》系列云智公开课，探讨数据库的创新、变革和应用。 </div>
                        <hr>
                    
                    <p>作为计算机系统的三大核心基础软件之一，数据库技术的发展一直备受关注。随着云计算技术的发展，能够适合更大规模业务场景，有着高可用性、可扩展性和低成本等优势的云原生和分布式数据库逐渐成为主流。</p><p></p><p>同时，AI 技术不断向前发展，尤其是 OpenAI 掀起的这场 AI 变革，使得 AI 与云计算更紧密地结合成为了可能。云原生为数据库面向更大范围的智能化应用提供了基础条件，AI 成为云原生数据库持续演进的牵引力。</p><p></p><p>为了应对大数据和 AI 应用在行业中的挑战，<a href="https://www.infoq.cn/news/kqPbdlvF3Jp55PodQLAo">百度智能云</a>"在 2020 年率先提出“云智一体”战略，以“云计算为基础”支撑产业数字化转型，以“人工智能为引擎”深入产业生产的关键场景，为企业的数字化转型和智能化升级提供新型支持。在云智一体战略的指导下，<a href="https://www.infoq.cn/article/o8abj2wff5yLfGWuB0E1">百度智能云</a>"在数据库领域不断创新，基于百度集团各项业务的多年磨炼，对外推出了云原生数据库 GaiaDB 、云数据库 GaiaDB-X、数据传输服务 DTS 等产品和解决方案。那这些产品和相关方案，都有哪些不一样的地方，又有哪些落地实践？比如：</p><p>在互联网、云计算，以及向 AI 的演进过程中，百度智能云数据库团队是如何进行技术升级，做到支持各类业务场景，满足海量数据规模，业务要求越来越苛刻的场景的。在 AI 时代，它的最新成果和规划又有哪些？相比其他云原生数据库， GaiaDB 是如何诞生于百度集团的内部业务，其产品理念有什么独特的地方，在哪些技术上面形成了优势，可以帮助用户解决什么样的挑战？在金融行业的国产化进程，GaiaDB-X 如何承载核心业务系统，满足金融行业对基础设施的高要求，在金融客户中的成功落地场景又有哪些？完善的数据库服务不止有数据库产品本身。数据库的迁移、同步、集成同样重要。这些工作和数据库本身一样，同样是复杂又至关重要的。百度智能云的数据传输服务 DTS 如何将繁琐复杂的这类业务变得可靠简单，在业务实践中帮助头部客户成功上云？</p><p></p><p>为了帮助大家解决这些问题，<a href="https://www.infoq.cn/article/ACXL3WviaTtr2U0v5NlB">百度智能云</a>"团队特推出《百度智能云数据库》系列云智公开课。前四期课程便将围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开。从 11 月 15 日起，每周三都将有一位百度智能云的大咖与各位一起探讨百度智能云数据库的创新、变革和应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf46378ce27ef5e83b5b6abf599ec8d3.jpeg" /></p><p></p><p>第一讲：《从互联网到云计算再到 AI 原生，百度智能云数据库的演进》</p><p>你将获得：</p><p>全局完整地了解数据库行业的历史和发展趋势；了解百度智能云数据库在各个阶段的典型产品、应用和关键技术；了解百度智能云数据库在 AI 原生时代的创新和变革。</p><p>&nbsp;</p><p>第二讲：《高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析》</p><p>你将获得：</p><p>了解云原生数据库的不同技术路线和能力对比；了解相比传统单体数据库，云原生数据库的技术差异和挑战；了解 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p>&nbsp;</p><p>第三讲：《面向金融场景的 GaiaDB-X 分布式数据库应用实践》</p><p>你将获得：</p><p>了解金融核心系统在构建分布式数据库的技术挑战；了解 GaiaDB-X 数据库的架构及在金融场景的分布式特性；了解 GaiaDB-X 在金融机构的核心系统分布式的落地实践。</p><p>&nbsp;</p><p>第四讲：《一站式数据库上云迁移、同步与集成平台DTS的设计和实践》</p><p>你将获得：</p><p>了解数据库上云迁移、数据库同步 / 集成的业务场景，以及实践中可能遇到的技术挑战；了解百度智能云 DTS 的关键特性、核心技术和实践案例。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</id>
            <title>蚂蚁SOFA Stack融合大模型发布升级版 助力机构产研效能提升30%</title>
            <link>https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jQTpFmq9SqamtQEwdc0M</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 06:14:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, CodeFuse, SOFAStack5.0, 企业研发运维智能助手
<br>
<br>
总结: 蚂蚁集团发布了CodeFuse全面加持的SOFAStack5.0升级版本，为企业提供全方位研发运维智能助手相关能力。这个升级版本将为企业产研效能提升30%，通过智能副驾驶提升日常代码研发、测试、运维过程中的效率和质量。SOFAStack还提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。 </div>
                        <hr>
                    
                    <p>11月1日，在云栖大会上，蚂蚁集团正式发布CodeFuse全面加持的SOFAStack5.0升级版本，向企业提供全方位研发运维智能助手相关能力。这是继蚂蚁集团在外滩大会发布代码大模型CodeFuse之后，首次公布面向行业的商业化产品进展。</p><p>&nbsp;</p><p>“大模型将为研发效能带来颠覆性机遇。”蚂蚁集团数字科技事业群产品总监马振雄在发布会上指出。</p><p>&nbsp;</p><p>记者了解到，目前CodeFuse已经与SOFA产品线全面融合，涵盖设计、研发、测试、运维等领域，形成从领域建模到智能运维的端到端Copilot产品解决方案，预计将为企业产研效能提升30%。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/5c/5cd0d1b716d80689035335361a75486b.png" /></p><p></p><p>具体而言，客户在使用SOFAStack时，相当于为企业开发者配备专属智能副驾驶，和机器人“辅助设计”、“结对编程”、“运维助手”，通过人机交互助手提升日常代码研发、测试、运维过程中的效率和质量。对企业而言，引入智能副驾驶可以显著提升人效质量，降低总体成本。</p><p>&nbsp;</p><p>此外，SOFAStack针对Codefuse大模型提供了多任务微调和高性能推理能力，结合企业专有数据构造更懂客户业务的智能副驾驶。而随着CodeFuse在产品线中不断深度融合，SOFAStack将为企业打造新一代AI云原生PaaS平台，使其在开发运维、数据分析、应用治理、绿色计算方面取得更智能的能力，可以加速响应业务创新和价值交付。</p><p>&nbsp;</p><p>针对当下企业应用上云「更异构、更智能、更经济」的三大需求趋势，马振雄表示，SOFAStack提供了一系列云原生解决方案，帮助企业在云环境下快速构建、部署和管理应用程序。这些解决方案可以满足不同行业和企业的需求，并为企业提供更加灵活和高效的技术支持。例如，针对行业进入多云时代，边缘资源调配、云上云下应用开发等统一管理挑战，其拳头产品MESH升级架构，从原来的经典Sidecar架构开始演变为Node架构，同步进行了性能、服务治理、业务可观测能力等全方位优化。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/7c/7cd485b0811e68275758bb2d760a382d.png" /></p><p></p><p>Forrester报告曾分析指出，以云原生为关键能力的下一代云平台， 不仅可以基于全栈云原生架构灵活适应市场变化，而且可以通过全云开发实践帮助企业在云上快速验证创新思路，还能借助云平台的各类自动化能力降本增效强化韧性。</p><p>&nbsp;</p><p>“服务网格降低了我们的上云门槛。如果做云原生改造，系统的所有代码都要重写一遍，大概需要20&nbsp;个人投入一年时间；使用网格（Mesh）只要&nbsp;5&nbsp;个人两三个月就能上云。”传统金融机构信息科技架构规划负责人在Forrester调研时表示。根据报告测算，三年内有&nbsp;10&nbsp;个单体应用不需要经过云原生改造，即可直接上云后统一治理，总体效率提升为企业带来941万元的收益。</p><p>&nbsp;</p><p>据悉，SOFAStack是国内部署云原生技术最广泛的平台之一，基于支付宝、蚂蚁集团各项业务需求进行研发迭代，并服务于超100家银行迈向云原生转型，已经构建了完整金融级的云原生PaaS解决方案。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</id>
            <title>计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士，确认担任 QCon LLM 时代的性能优化专题出品人</title>
            <link>https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hEPJ8eoTHjSSxGbmaWd8</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, LLM 时代的性能优化, 周经森（Kingsum Chow）, CPU 和 GPU 平台
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，周经森博士将担任“LLM 时代的性能优化”专题的出品人。该专题将介绍LLM时代的性能分析在CPU和GPU平台上的表现。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。计算机软硬件优化首席科学家、高级首席工程师周经森（Kingsum Chow）博士将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">LLM 时代的性能优化</a>"」的专题出品人。在此次专题中，你将了解到 LLM 时代的性能分析在 CPU 和 GPU 平台上，在不同计算环境下的性能表现。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1101&amp;utm_content=zhoujingsen">周经森（Kingsum Chow）</a>"，计算机软硬件优化首席科学家、高级首席工程师。曾就职于美国英特尔公司和中国阿里巴巴集团，2023 年加入浙江大学软件学院（宁波）。二十年来与十余家世界 500 强高科技企业合作，共同推动了世界软硬件性能优化技术的发展。曾作为项目总监主持备受瞩目的云计算蓝图项目（IntelCloudBlueprint）。该项目由英特尔和甲骨文的首席执行官于 2015 年共同宣布，吸引了超过 4 万名开发者的参与，为云计算行业绘制了全新的技术蓝图，对行业发展产生了深远影响。</p><p></p><p>自 2016 年加入阿里巴巴，为中国的性能优化技术发展做出了巨大贡献。2018 年，其作为唯一一名加入 Java 全球管理组织 JavaCommunityProcess（JCP）最高执行委员会 JCP-EC 的中国企业（阿里巴巴）代表，参与制定了 Java 的全球标准。</p><p></p><p>周博士在 CPU 利用率报告不准确（数据普遍误解）方面发表的研究，引起了业界和学术界的广泛关注。周博士拥有超过 30 年的软硬件协同优化的工业实践经验，培养了大批优秀的系统性能优化人才。至今已获授权中国专利 11 项，美国专利 24 项，发表学术论文 127 篇，在过去 6 年的 QCon 中国大会上发表 2 场主题演讲，出品 2 场软件系统性能优化主题讲座。</p><p></p><p>相信周经森（Kingsum Chow）博士的到来，可以帮助提升此专题的质量，让你学习到， 通过 LLM 在不同计算环境下的性能表现，找到的最佳应用策略和优化方法，这为 LLM 的应用和发展提供了更多的可能性。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040 ！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/36/160539957f1fd1f4671722f1cab32a36.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P</id>
            <title>15年技术沉淀，起底阿里核心搜索引擎 Havenask 演进之路</title>
            <link>https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ult2LYNeMbLDVwzhSr0P</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 Nov 2023 01:04:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里开源搜索引擎, Havenask, 技术演进, 大规模分布式搜索引擎
<br>
<br>
总结: 阿里开源搜索引擎Havenask是一款基于大规模分布式技术的搜索引擎，经过多年的技术演进，从解决内部业务需求到统一整合各业务搜索系统，再到逐步开源对外，Havenask具备高性能、高并发、高时效性的特点，能够智能地帮助人们快速、准确地获取信息。 </div>
                        <hr>
                    
                    <p></p><h2>阿里开源搜索引擎&nbsp;Havenask&nbsp;的技术演进</h2><p></p><p></p><p>我们正处于信息爆炸式增长的时代，如何在信息海洋里迅速定位到目标信息成为人们关心的问题。搜索引擎作为互联网和应用的关键入口，向来是兵家必争之地。</p><p></p><p>然而在人们简单的搜索行为背后，对搜索引擎技术实际有诸多挑战：以电商场景为例，当遇到双11等大促活动时，百万级&nbsp;QPS&nbsp;的高并发访问，对千亿级商品&nbsp;&amp;&nbsp;订单数据、保单&nbsp;&amp;&nbsp;物流类数据时效性要求极高，那么搜索引擎该如何做到毫秒级时效？还有为了更准确理解人们的搜索意图，对搜索算法的要求越来越高，搜索引擎该如何做到算法分钟级迭代？这些都是技术上需要直面的挑战。</p><p></p><p>近年来，随着大数据技术、深度学习等&nbsp;AI&nbsp;技术的发展，搜索引擎能够更智能地帮助人们快速、准确地获取信息，我们对信息的处理能力也随之逐步提高。</p><p></p><p>阿里自研大规模分布式搜索引擎&nbsp;Havenask&nbsp;便是集大成者，基于阿里搜索十多年来的技术沉淀，Havenask&nbsp;目前广泛应用于阿里巴巴和蚂蚁集团内众多业务，如淘宝搜索和推荐、&nbsp;蚂蚁人脸支付、优酷视频搜索、阿里妈妈广告检索等。Havenask&nbsp;支持算法高效快速迭代，内置性能优异的向量检索能力；做到毫秒级查询性能，并拥有稳定性保障&nbsp;；支持单应用实例千亿级别数据，确保百万&nbsp;TPS&nbsp;高时效性。</p><p></p><p>2022&nbsp;年&nbsp;12&nbsp;月，阿里将&nbsp;Havenask&nbsp;开源，在几个月时间里&nbsp;Star&nbsp;数已超过&nbsp;1000+。为何&nbsp;Havenask&nbsp;有这样优异的表现，在短时间内获得众多开发者的喜爱？下面我们从&nbsp;Havenask&nbsp;的技术演进谈起，让大家更加深入了解&nbsp;Havenask&nbsp;以及未来更多可能性。</p><p></p><p>传送门：++https://github.com/alibaba/havenask++</p><p></p><h2>01&nbsp;Havenask&nbsp;技术演进之路</h2><p></p><p>回顾&nbsp;Havenask&nbsp;从内部自研技术走向成熟，这一路走来可分为以下阶段：</p><p></p><p><img src="https://static001.geekbang.org/infoq/85/85f3025dbedec2a27678b2160ae2f23d.png" /></p><p></p><p>第一阶段：1999&nbsp;年~2008&nbsp;年，以解决各业务部门的搜索需求为主</p><p></p><p>阿里搜索技术最早可追溯&nbsp;1999&nbsp;年，起源于雅虎搜索技术，基于&nbsp;Apache&nbsp;Module&nbsp;的单机版搜索引擎，支撑淘宝、B2B&nbsp;等子公司的搜索业务需求。</p><p></p><p>第二阶段：2009&nbsp;年~2011&nbsp;年，重构搜索系统，开启自研大规模分布式高性能搜索引擎时代</p><p></p><p>自&nbsp;2008&nbsp;年起，开始构建阿里统一的搜索系统，内部代号为“iSearch”，它代表完全由阿里自研的搜索技术全新启航。iSearch&nbsp;迅速迭代&nbsp;iSearch3.0、iSearch3.2……2009&nbsp;年演进到&nbsp;iSearch4.5&nbsp;版本，也就是&nbsp;HA3（Havenask）最早的雏形。</p><p></p><p>2009&nbsp;年，Havenask&nbsp;开始逐步统一各子公司版本，去除&nbsp;Apache&nbsp;Module。2011&nbsp;年，彻底完成搜索系统的重构，HA3（Havenask）全部替代老的雅虎搜索系统，开始极致性能时代。</p><p></p><p>第三阶段：2012&nbsp;年~2018&nbsp;年，完成阿里内部搜索系统的“大统一”，进入快速迭代时代</p><p></p><p>2013年，HA3（Havenask）完成阿里集团各个业务搜索系统的“大统一”，不仅版本再次合并，还将&nbsp;B2B、淘宝等搜索团队统一整合，以产品化、规模化的方式支撑起整个集团的搜索业务。</p><p></p><p>2018&nbsp;年，随着深度学习技术的广泛应用，同时迎来信息流推荐机遇，HA3（Havenask）快速迭代，逐步形成一套以搜索引擎、在线推理引擎等为主的&nbsp;AI&nbsp;工程技术体系“AI·OS”。（OS”代表“Online&nbsp;Serving”&nbsp;）</p><p></p><p>第四阶段：2018&nbsp;年~至今，对外开源，技术普惠</p><p></p><p>2022&nbsp;年，阿里将搜索引擎&nbsp;Havenask&nbsp;开源，为更多用户提供更高性能、更低成本、更便捷易用的搜索服务。</p><p></p><p>总的来说，Havenask&nbsp;的发展是遵循先解决内部业务应用需求，再从核心业务延伸到其他业务，随着技术发展潮流不断向前演变，从单一的搜索引擎到大数据深度学习在线服务体系&nbsp;AI·OS&nbsp;的重要组成部分，打造成统一平台提供更强大的能力支撑，继而逐步开源对外，普惠开发者，这和阿里其他技术产品的发展思路是一脉相传的。</p><p></p><h2>02&nbsp;Haveansk&nbsp;架构优势</h2><p></p><p>从定位来看，Havenask&nbsp;作为阿里巴巴自主研发的大规模分布式搜索引擎，支撑起淘宝、天猫、菜鸟、优酷阿里整体的搜索业务，并扛得住双&nbsp;11&nbsp;大促活动。这背后，离不开底层架构设计，让&nbsp;Havenask&nbsp;有了坚实的技术基底。</p><p></p><p><img src="https://static001.geekbang.org/infoq/33/33f354cadf938d9f305d845ffdf35a82.png" /></p><p></p><p>从架构来看，Havenask&nbsp;由四个核心模块组成：</p><p></p><p>索引系统**（Build**&nbsp;Service）。通常搜索引擎需要对原始数据构建索引，才能在提供服务时实现高性能。这部分在&nbsp;Havenask&nbsp;是支持全量、增量、实时流的复杂分布式流计算系统。</p><p></p><p>在线集群**（Havenask**&nbsp;**Runtime）****。**在线系统支持不同的数据规模分列查询，不同的查询并发做多副本。在系统里设计有类似于大脑的复杂角色，可以自动做查询处理、调度查询节点、数据节点等。如果出现机器坏了的情况，在线系统可自动识别这些情况，来保证系统的高可用。</p><p></p><p>消息中间件（Swift）。消息中间件用于实时数据传递，处理后的文档传递，是&nbsp;Havenask&nbsp;实现毫秒级时效性，支撑海量数据实时更新的基石。消息中间件&nbsp;Swift&nbsp;不仅可以用在&nbsp;Havenask&nbsp;系统中，也可以单独部署使用，与其他开源中间相比具有明显的性能和成本优势。</p><p></p><p>管控系统（Hape）。为了方便开发人员的日常运维，Havenask&nbsp;对管控运维的&nbsp;API&nbsp;进行了封装，提供方便实用的运维工具&nbsp;Hape&nbsp;，使用它开发人员可以方便的对表和集群进行管理。</p><p></p><p>据阿里巴巴智能引擎事业部云服务负责人、Havenask&nbsp;开源项目负责人郭瑞杰博士介绍，在架构设计上，Havenask&nbsp;更具备适合工业级业务场景的特性：</p><p></p><p>1、通过灵活稳定的扩展方式支持业务多样化需求，轻松应对数据规模和流量规模的快速增长；</p><p></p><p>2、通过领先的实时索引技术，提供性能出色的亚秒级实时搜索能力，通过对实时索引的不断自动整理优化，保证搜索性能持续优异；</p><p></p><p>3、传统倒排索引技术和&nbsp;AI&nbsp;时代普遍应用的向量检索技术深度结合，端到端极致性能优化，支持千亿级别文档或高维向量的极低延迟计算。</p><p></p><p>人们进行商品搜索时，由于每个人有不同的喜好，搜索引擎需实现个性化和智能化，以准确召回商品。当用户开始进行搜索时，往往是用关键词或一段自然语言的描述，搜索引擎先采用&nbsp;NLP&nbsp;技术理解和拆分成关键词，再根据关键词的语义相关性，采用向量等多路召回方式，返回有可能是用户想要找的商品信息，再对商品做粗排，粗排后收敛到集合里，再做精排，这个过程中&nbsp;Havenask&nbsp;使用了大量机器学习算法进行优化，以实现较好的用户体验。</p><p></p><p>这对搜索引擎有较高的性能要求，Havenask&nbsp;利用前置化思想，并发完成多路召回，实现非常小的延迟效果。另外在算法上，Havenask&nbsp;支持离线计算转在线计算、在线计算转离线计算做优化，还支持模型的实时更新以保证在离线的一致性。如此一来，算法工程师可以用更复杂的召回策略来做&nbsp;A/B&nbsp;测试验证效果，如果效果可以的话，可以实现分钟级上线。</p><p></p><p>在拍照搜图场景中，以淘宝拍照购物“拍立淘”为例，用户通过手机拍摄实物或通过相册照片搜索，就能搜索同款或相似商品。&nbsp;Havenask&nbsp;利用向量进行图片搜索，完成向量索引存储并将向量化后的图片与向量索引比对召回，实现高精度图片搜索。上述能力得益于&nbsp;Havenask&nbsp;和达摩院向量库&nbsp;Proxima&nbsp;深度结合，并进行端到端能力优化，支持百亿甚至千亿级别的高维度向量的低延迟计算。</p><p></p><p>总体来看，Havenask&nbsp;区别于其他产品的特点主要体现在两大场景中：一是大数据检索场景，实现亚秒级的时效性和极致的性能优化，达到较高的性价比。二是在&nbsp;AI&nbsp;场景上，Havenask&nbsp;实现异步高并发、超低召回延迟，提供在离线一致性保障机制，以及高性能高维度向量计算能力。</p><p></p><p>即使在双11特殊场景里，数据更新量突然爆增至十倍、百倍，Havenask&nbsp;仍能保证时效性在亚秒级。在查询上，单集群到近百万&nbsp;QPS&nbsp;时，Havenask&nbsp;确保查询延迟毫秒级别。另外，Havenask&nbsp;足够弹性，针对双11的流量急速变化，集群一键平滑扩缩容，变更对业务0影响，灵活应对流量峰谷。</p><p></p><h2>03&nbsp;Havenask&nbsp;开源开放，普惠开发者</h2><p></p><p>Havenask&nbsp;起源于阿里内部搜索业务需求，如今作为核心搜索引擎在阿里内部广泛应用，那么团队为什么选择将&nbsp;Havenask&nbsp;对外开源？</p><p></p><p>郭瑞杰表示，Havenask&nbsp;围绕着电商场景演化出来，在阿里核心头部业务、中台业务等均广泛使用。希望通过开源的方式让广大开发者参与进来，让&nbsp;Havenask&nbsp;迭代更快走得更远。以开源&nbsp;Elasticsearch&nbsp;为例，在十年时间中，Elasticsearch&nbsp;因为开源发展迅速，Havenask&nbsp;也期待通过开源吸引更多开发者参与进来，一起联合共创。</p><p></p><p>再者，近年来国际形势变幻莫测，人们对国产化替代诉求与日俱增。期望自主研发的&nbsp;Havenask&nbsp;能帮助一些企业实现国产化替代，让更多开发者和企业以更低的成本实现业务创新。</p><p></p><p>不仅如此，Havenask&nbsp;还提供商业版本来支持企业实现搜索场景、推荐场景、大模型应用场景创新。</p><p></p><p>“&nbsp;Havenask&nbsp;自开源后，在尚未开展过多活动的情况下，Star&nbsp;数快速突破&nbsp;1000，对我们来说还挺意外的，这也让我们坚定了后续持续建设开源&nbsp;Havenask&nbsp;的信心。”郭瑞杰说。</p><p></p><p>Havenask&nbsp;作为&nbsp;AI·OS&nbsp;体系的重要部分，沉淀了阿里&nbsp;10&nbsp;多年的搜索技术，整体系统庞大，采取逐步开源的形式对外开放，从2022年首发时的单机预览版，到如今刚刚发布的的分布式正式版，已经完成了&nbsp;Havenask&nbsp;几乎全部核心代码的开源。</p><p></p><p>在2023年9月份最新发布的&nbsp;Havenask&nbsp;1.0.0&nbsp;分布式版本中，支持读写分离与读写统一两种部署架构，可以分别满足开发者不同业务场景的需求，同时分布式版本提供基于机器资源池的集群自动化管理能力、动态表管理能力，降低开发者集群运维的成本；并且集成了自研的消息中间件，支持更完善的实时数据更新能力。</p><p></p><p>据郭瑞杰透露，在后续的版本中，&nbsp;Havenask&nbsp;会更聚焦开发者的真实使用场景，特别是大数据检索和智能检索等领域不断构建&nbsp;Havenask&nbsp;的开源生态，让&nbsp;Havenask&nbsp;更加广泛的应用在更多业务中，解决开发者面临的性能、成本、稳定性等核心问题。</p><p></p><p>与此同时，Havenask&nbsp;还开源了&nbsp;Havenask-federation（简称Fed）项目（<a href="https://github.com/alibaba/havenask-federation">https://github.com/alibaba/havenask-federation</a>"），在&nbsp;Havenask&nbsp;和&nbsp;Elasticsearch&nbsp;之间架起一条桥梁，方便&nbsp;Elasticsearch&nbsp;开源生态用户，快速迁移和扩展，实现优势互补。</p><p></p><h2>04&nbsp;Next&nbsp;Big&nbsp;Thing</h2><p></p><p>最近技术人话题离不开热门的&nbsp;ChatGPT，ChatGPT&nbsp;一经发布，大家认为被最早被颠覆的是搜索引擎。传统搜索引擎&nbsp;+&nbsp;ChatGPT&nbsp;将产生巨大化学反应，或将改写搜索引擎的产品形态。ChatGPT&nbsp;能更好地理解人们的搜索意图，为用户提供汇总答案，提供更准确的搜索结果，还能以自然语言来搜索，让搜索体验有质的提升。</p><p></p><p>郭瑞杰表示，有了&nbsp;ChatGPT&nbsp;能力加持，不仅在&nbsp;to&nbsp;C&nbsp;端搜索引擎发生巨变，在&nbsp;to&nbsp;B&nbsp;端也将催化诞生颠覆性的产品形态。其中&nbsp;to&nbsp;B&nbsp;端和&nbsp;to&nbsp;C&nbsp;端搜索引擎稍有差异，to&nbsp;B&nbsp;搜索引擎是面向企业，主要搜企业数据，而不是搜全网数据，更多的是围绕企业数据来提供更智能和更准确的答案。</p><p></p><p>针对不同行业的用户想基于大模型能力完成业务创新，Havenask&nbsp;除了在底层传统搜索引擎技术上提供帮助，也正在做如下两个方面的能力增强，并持续开源：一是向量检索。在大模型时代下，向量检索技术是大模型应用创新的基石，我们正在构建新的向量检索引擎&nbsp;VectorStore，预计性能大幅超越&nbsp;Milvus，期望能提供给开发者更高性能、更低成本的向量检索方案；二是大模型推理加速。将全面支持各种&nbsp;LLM（qwen、chatglm、baichuan、xverse、interlm、llama、falcon、mpt、starcoder&nbsp;等）的推理加速，支持量化、多机多卡分布式、上下文&nbsp;cache&nbsp;等多种特性，预计性能超越&nbsp;vllm&nbsp;15%，期望给开发者提供更低成本的大模型推理服务。</p><p></p><p>现在，我们看到阿里已先行一步：在&nbsp;2023&nbsp;阿里云峰会上，正式推出大语言模型“通义千问”，并宣布阿里所有产品未来将接入“通义千问”，进行全面改造。例如在网购场景，用户如果想开生日&nbsp;party，通义千问可以帮助生成生日活动方案和购物清单。</p><p></p><p>期待后续&nbsp;Havenask&nbsp;与“通义千问”联合创新，为人们带来更好地搜索体验，帮助企业和开发者量身定做适合业务发展的智能搜索服务，促进业务飞速增长，共享科技红利。</p><p></p><p>此外，基于&nbsp;Haveansk&nbsp;与“通义千问”打造的AI搜索产品——OpenSearch&nbsp;LLM&nbsp;智能问答版，也已在阿里云上为企业级开发者提供全托管、免运维的一站式对话式搜索服务，欢迎企业级开发者们试用。</p><p></p><p>心动不如行动，欢迎立即体验：</p><p></p><p>Havenask&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask">https://github.com/alibaba/havenask</a>"</p><p></p><p>Havenask-federation&nbsp;开源项目地址：<a href="https://github.com/alibaba/havenask-federation">https://github.com/alibaba/havenask-federation</a>"</p><p></p><p>OpenSearch&nbsp;LLM&nbsp;智能问答版：<a href="https://www.aliyun.com/activity/bigdata/opensearch/llmsearch?spm=5176.7946605.J_4098459070.4.15b38651FlNqqw">https://www.aliyun.com/activity/bigdata/opensearch/llmsearch?spm=5176.7946605.J_4098459070.4.15b38651FlNqqw</a>"</p><p></p><p>钉钉扫码加入Havenask开源官方技术交流群：</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78c5cfa61c64a55cdeb0655ac7eb2849.png" /></p><p></p><p>近期活动预告：</p><p></p><p>2023年11月1日13:10-13:25，杭州云栖大会&nbsp;B3-4&nbsp;馆，Havenask&nbsp;开源正式版发布演讲</p><p></p><p>2023年11月1日14:40-15:10，杭州云栖大会&nbsp;C&nbsp;区舞台，Havenask&nbsp;开源细节与案例分享</p><p></p><p>欢迎开发者前往会场参加，或通过线上渠道收看关注</p><p></p><p>嘉宾介绍：郭瑞杰博士，2008年加入阿里巴巴，深耕阿里搜索领域开发十余年，先后负责&nbsp;iSearch4.5、问天2、问天3等多个搜索架构及产品的设计与开发工作，现任阿里巴巴智能引擎事业部云服务负责人，阿里云计算平台事业部搜索推荐云服务负责人，Havenask&nbsp;开源项目负责人。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC</id>
            <title>“2023 深圳国际金融科技大赛”线上技术公开课：人工智能、区块链、产品经理，分别是怎样赋能金融行业的？</title>
            <link>https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZKeta6LLZD97sHwPv2UC</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 11:57:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技, 智慧金融, 普惠金融, 区块链
<br>
<br>
总结: 金融科技是金融行业中的一个重要分支，它能够满足智慧金融和普惠金融的需求。传统金融机构需要借助先进技术和理念来改造自身，以提供更好的服务和体验。为了推进金融科技产业的发展，深圳国际金融科技大赛特别设置了区块链赛道，希望激发选手创新热情，为金融科技发展提供更多有价值的解决方案。 </div>
                        <hr>
                    
                    <p>金融科技是科技创新领域中一个独特且重要的分支。传统金融产业重视稳定、可靠和信誉，但在面对科技进步时，却往往表现出保守的态度和缓慢的行动；另一方面，市场的快速发展和变化给金融企业带来了前所未有的挑战。智慧金融和普惠金融的需求日益高涨，金融机构需要借助先进技术和理念来改造自身，以提供更好的服务和体验来满足这些需求。</p><p></p><p>为此，行业一直在努力探索适合金融业特点的技术发展路线，确保在满足安全、可靠和可信的前提下，满足日益增长的市场需求。在此需求下，也为了进一步推进金融科技产业的发展，所以“2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛”（下文称“大赛”）特别设置了<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbe9a85fcc9e2149db6f60a7f8bbe5fe2326a33a3177c10f98693fc301418ee907544b18f2cc&amp;idx=1&amp;mid=2247487888&amp;scene=27&amp;sn=931b65213c5f893047ad4edfb60b1a2e&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">区块链</a>"、人工智能与产品经理赛道，希望激发选手创新热情，为金融科技发展提供更多有价值的解决方案。</p><p></p><p>作为 2023 年深圳市金融科技节的重要一环，本届大赛在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、<a href="https://www.infoq.cn/article/W05aweqVPI9UwdxxOzoi?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">微众银行</a>"、深圳香蜜湖国际金融科技研究院等多方联合举办。大赛设置总额超过 69 万人民币的赛事奖金及参赛专属电子区块链证书，还邀请学术和企业界的众多资深专家为参赛选手答疑解难。</p><p></p><p>为帮助同学们深入了解<a href="https://www.infoq.cn/article/2K0clWV5ZGjlPumJhf9G?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">金融科技</a>"前沿成果，尽快熟悉和理解赛题赛制，10 月 25 日，本届大赛的线上技术公开课上线。微众银行区块链首席架构师张开翔老师、微众银行人工智能部室高级经理杨海军老师、微众银行个人直通银行部室经理金虎光老师围绕区块链、人工智能、金融产品经理等话题展开了主题分享，并介绍了各赛道赛题赛制的相关细节，回答了同学们最关心的问题。以下为本期公开课直播精华内容整理：</p><p></p><p></p><h2>一、微众银行区块链首席架构师张开翔：“区块链技术构筑 ESG 可信基础设施”</h2><p></p><p></p><p>ESG，亦即环境、社会和治理，是今天各行业都非常重视的概念，也受到了金融企业的普遍关注。而区块链又是当下金融科技等行业的热点技术主题，其防篡改、安全性高等特性有很高的应用潜力。</p><p></p><p>那么区块链技术怎样同 ESG 建立联系？以养殖场为例，牲畜需要经过养育、屠宰、检验、运输等各种环节，最终到达商超终端。如果将这些过程中的重要数据，例如牲畜每天的体温、运动步数、检验证书、运输路线等都记载在区块链上，就可以实现全程可信、可追溯、无法篡改的效果。类似地，普通市民上班时选择乘坐地铁，减少私家车排放污染，那么地铁票和上班路径等信息也可以记录在区块链上，从而获得绿色出行的积分奖励。</p><p></p><p>在上述应用场景中，区块链技术的落地关键在于实体世界的数据如何与链上数据同步，从而利用区块链准确地记录真实信息，搭建从链下到链上的可信任链条。这一过程中一般需要用到区块链的存证、追溯、审计、记账和清算功能，并在同步环节设计好数据锚定与校验流程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/86a14b11f2a9456c9693d11a8fad523c.png" /></p><p></p><p>本届大赛的区块链赛道主题是区块链 +ESG，其宗旨就是鼓励参赛选手将实体世界的 ESG 数据与区块链结合，探索各类创新应用场景。微众银行为大赛选手提供了丰富的技术和资源支持，具体可以参见本届大赛官网&nbsp;https://www.infoq.cn/zones/fintechathon/campus2023/&nbsp;。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a11f0456e7aae97651412e5b2cb89d0.png" /></p><p></p><p>张开翔提到，微众银行的区块链技术完全开源，参赛选手可以借助这些开源技术，通过开放社区的学习来使用区块链发挥创意，在金融科技领域实现各种需求。举一个例子，选手可以使用区块链构建一个绿色出行的生态，用户在这一生态中可以使用记录在智能合约上的出行积分兑换商家的优惠券。但这样的生态还必须保障隐私和安全，比如用户并不想让他人得知自己的出行路线，也不想暴露自己的住处、上班场所的位置。那么选手就可以在安全领域发挥专业水平，通过多方计算、可信计算等技术增强这一生态的安全性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c749a0755cbe5a43fb9783f764dfaa8b.png" /></p><p></p><p>张开翔从大赛评委的角度提出，评委更希望看到选手拿出一些在具备实用价值的前提下有新意、好玩的作品。例如往届大赛的作品就有使用区块链链接交通出行各参与方、解决相亲活动的信任问题等。张开翔希望选手打开思路，设想更多使用场景，在隐私、安全、性能、容量和功能层面深挖区块链应用潜力，这样做出的作品就会得到很高的成绩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/461a99c1eab3997844c5570d15aba2ab.png" /></p><p></p><p>此外，张开翔还提到，选手入围决赛后，在最终的决赛现场会有很多专家老师提供指导。决赛中，选手展示完毕后也要回答评委老师的尖锐问题，比如：</p><p></p><p>你用区块链解决了什么问题？为什么一定要用区块链？物理世界怎样使用区块链验证可信度？传感器怎样防篡改？收集的数据是否会侵犯隐私？项目如何推广、获利？……</p><p></p><p>所以最后张开翔给所有参赛选手提出了一个建议，“希望参赛选手可以改变学生思维，更多考虑如何为社会创造价值，这样才能在大赛中取得更好的成绩。”</p><p></p><p></p><h2>二、微众银行人工智能部室高级经理杨海军：“微众银行 AI 技术服务解决方案”</h2><p></p><p></p><p>微众银行将人工智能前沿技术（图像、语音、NLU、大模型、联邦学习等）与金融服务深度融合，探索将相关技术融入金融服务各个环节，拓展金融服务的广度和深度，重塑以客户为中心的金融价值链和生态，推动“未来智能金融”的实现。</p><p></p><p>微众银行人工智能应用覆盖客服、营销、风控、运营多场景，包括针对用户端的智能客服机器人、智能语音机器人、智能核身（人像、声纹等）解决方案等；企业后端的智能质检机器人、智能培训机器人、营销助手机器人、KYC、反欺诈解决方案等。基于联邦学习实现 AI 技术终生学习和抗攻击性，大幅提升用户服务质量与效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c61dc4284e5b34e2104719094d96ce3.png" /></p><p></p><p></p><h4>&nbsp;1. 智能客服场景</h4><p></p><p></p><p>在客服场景中，基于自研的智能在线客服与智能语音机器人，可以实现 7*24 小时回应海量用户需求，智能坐席助手与实时质检辅助人工坐席，在及时响应的同时规范客服话术与行为，不断提升客服质量，保护消费者权益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/60d0394bf229b2039204eb73365dd10f.png" /></p><p></p><p></p><h4>&nbsp;2. 智能营销场景</h4><p></p><p></p><p>微众银行持续升级智能营销解决方案，结合智能语音机器人、联邦学习等技术，在数据不出本地的合规前提下，实现业务营销获客和存量促活，更加精准地触达目标人群。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e9ea3a0f6d529be99444d982912292f.png" /></p><p></p><p></p><h4>&nbsp;3. 智能风控场景</h4><p></p><p></p><p>基于业内领先的人脸识别、声纹识别等技术，在开户、授信、放款等金融服务多个环节把控风险，有效甄别欺诈行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4cf928680339daa84c3e5cb9e46df3f.png" /></p><p></p><p>在每个智能场景中都有大量的真实案例分享，让同学们对 AI 技术在金融行业中的应用有了更深刻的认识。同时杨经理在最后还提到了参赛选手需要注意的一些高分要点。比如，选择与实际场景相关、国家鼓励或社会热点的研究课题；设计合理的性能评估指标，并阐述清楚评估效果、比对的参考对象等要素；选手要充分利用参考论文、开源代码、导师和评委等资源，帮助自己做出更好的作品等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/9901d59bc9c6c7671041bb41b8ef7201.png" /></p><p></p><p></p><h2>三、微众银行个人直通银行部室经理金虎光：“银行对话式交互服务的探索”</h2><p></p><p></p><p>银行业传统的线下柜台服务好处是银行职员与客户直接互动，可以处理较为复杂和个性化的问题，也更容易发展信任关系，但网点服务存在地点和工作时间限制，交易速度也比较缓慢。近年来兴起的线上远程服务则希望通过各种技术手段为客户带来随时随地、方便快捷的体验，同时尽可能做到像线下一样可以处理复杂、个性化的问题。从电话银行到网上银行、银行 App 再到虚拟数字人服务，银行正在努力将线上数字银行打造成新的增长点，提升金融服务体验和质量，提升客户经营质效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd6d5fb6ff9d3d49dabb670f89937e42.png" /></p><p></p><p>最近火热的生成式 AI 技术可以贯穿从市场、销售到运营、研发、风控的所有银行服务。金虎光对此也为参赛同学提出了建议，做产品经理课题时可以基于上图中的这些环节思考解决方案，例如利用 AI 实现精准获客，根据用户画像生成个性化营销文案，进行个性化定价，或者通过生成式 AI 金融咨询，实现更加智能有温度的线上客户服务。</p><p></p><p>此外，生成式 AI 在银行线上交互场景的应用潜力是非常巨大的。银行可以利用大语言模型打造自然语言对话服务，为客户带来更加自然、便捷的线上体验。正因如此，本次大赛产品经理赛道将课题定为《银行线上场景的交互式智能柜台服务》。本题目并不是要简单粗暴地将柜台通过数字人形式搬到线上，而是希望同学们基于对话式交互服务的能力，在客户全场景下提供更智能、更贴心、更便捷的银行服务。</p><p></p><p>如今各家银行的线上服务都已经包括了几乎所有银行服务功能，但随着功能指数级增长，客户的线上交互也变得非常复杂。每家银行都有多个 App，如何让客户更方便地找到所需功能是银行面临的普遍挑战。这里金虎光举了三家银行的网上银行作为案例，如 AI 助理模式、数字人模式、虚拟空间模式等，并提到微众银行也有类似的探索，参赛同学可以在这些模式的启发下，根据自己选择的场景做出更创新的模式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/573d15d1b590d6d8d1170808691138ce.png" /></p><p></p><p>金虎光强调，产品经理赛道最关注的是作品创新性，比较看重选手的想法是否有足够的亮点。其次比较关注的是作品的商业价值，这里指的是选手的想法基于用户的场景或痛点，是否能确实解决用户的某些需求，同时带来良好的社会价值。此外，作品的完整性、是否有市场和用户分析、竞品分析，对想法的解释和逻辑理解都是非常重要的。金虎光提示所有参赛同学要避免方案大而空。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cda85a94155622d693d37da57c90f533.png" /></p><p></p><p>金虎光在分享的最后帮助同学们总结了一些本届大赛产品经理赛道的获胜秘籍，“首先要关注用户是谁，然后要从服务提供方的角度关注具体的场景和用户需求，提供怎样的解决方案，产生怎样的用户价值，这些维度都是选手要重点考虑的。并且银行业务还要考虑基础的安全性、用户信息保护、用户核身的问题，针对这些问题提出的解决方案也会受到评委欢迎。”</p><p></p><p></p><h2>QA 环节精华问题整理</h2><p></p><p></p><p></p><h4>区块链赛道</h4><p></p><p></p><p>Q：区块链赛道更看重创意还是作品完整度？</p><p>A：都看重，完整度是基础，创意一定要有，好的创意会有加分。有同学问区块链 +ESG 的含义是什么，有实力的参赛选手是知道如何解决这个问题的。例如，服务人群、向善的事情都是 ESG，做坏事肯定不是 ESG。</p><p></p><p>Q：如何确保自己的区块链设计更可信、更安全，大赛会看重系统安全性吗？</p><p>A：安全性非常重要，更安全的系统会有很大加分。区块链技术本身是很安全的，但如果放到链上的内容是不可信的，也不会因此就变成可信的。所以如何解决这个问题也是展示实力的途径。</p><p></p><p>Q：有哪些获得高分的秘籍？</p><p>A：一些小技巧，比如 PPT 更规范、讲述更清晰不超时、将最新技术用于所关注的问题中，都是评委看重的。</p><p></p><p>Q：作品技术文档和展示材料有何区别？前者是否有固定格式和必须包含的内容？</p><p>A：这些材料都有官方模版提供，选手照做填写即可。技术文档重点看技术先进性，看作品优势、亮点、解决的问题，可以有数十页，但展示 PPT 一般较短，更考验沟通和演讲水平。</p><p></p><p></p><h4>人工智能赛道</h4><p></p><p></p><p>Q：FATE 框架是否开源？</p><p>A：是开源的，本届大赛官网和微众银行官网也提供了联邦学习的相关资料供下载。</p><p></p><p>Q：作品与金融场景不是特别相关可以吗？</p><p>A：不限定金融场景，其他场景都可以。比如医疗、养老、电力，国家重点扶持的项目或者社会热点项目可能会是加分项。</p><p></p><p>Q：想做的作品很大，但时间来不及只做一部分可以吗？</p><p>A：作品需要自圆其说，有逻辑有条理才能得到认可，如果甚至不能说服自己肯定是不行的。</p><p></p><p>Q：去年的作品迭代后在今年参赛可以吗？</p><p>A：可以参赛，去年没获奖打动不了评委的产品今年可能也难以获奖。建议最好有新的、好的创意加入到作品中，这样获奖概率才会增大。</p><p></p><p></p><h4>产品经理赛道</h4><p></p><p></p><p>Q：产品经理项目需要做出具体的应用吗？</p><p>A：本赛道要求产品方案的 Word 文档和产品文档 PPT（初赛只需要Word文档），文档中包含方案的文字描述。评委关注技术和整体的交互，如果有余力可以使用动画或者视频来展示。具体的应用是锦上添花，不做强求。</p><p></p><p>Q：比赛中提到的银行产品是指银行的业务板块还是一个银行 App？</p><p>A：评委想看到选手在现有的银行 App 下如何改进交互服务，例如通过 AI 助理或数字人模式在线上提供便捷服务。</p><p></p><p>Q：金融客户的全场景陪伴目前有哪些痛点？</p><p>A：客服就是一大痛点，例如用户每月固定时间转账前，银行或许可以通过智能客服提供提醒，这就是一个解决方案。这类痛点很多，目前的智能技术只能解决一小部分需求。</p><p></p><p>Q：本次命题作品一定要基于微众银行或其他银行的 App 产品吗？</p><p>A：不是，选手可以选择任意一家自己熟悉的银行，选择自己感兴趣的场景来做设计。基于微众银行的 App 不会有额外加分。评委希望看到一种线上服务的解决方案，可以是交互，也可以是某个功能，这是一个开放命题。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wCNzNg7tc5yOAB7o4XhR</id>
            <title>华为云开发工具和效率领域首席专家王亚伟，确认担任 QCon 智能化信创软件 IDE 专题出品人</title>
            <link>https://www.infoq.cn/article/wCNzNg7tc5yOAB7o4XhR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wCNzNg7tc5yOAB7o4XhR</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 09:20:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 智能化信创软件 IDE, 王亚伟, 自有技术内核
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，华为云开发工具和效率领域首席专家王亚伟将担任智能化信创软件 IDE 的专题出品人。在此次专题中，将介绍智能化信创软件 IDE 的架构和标准，以及其与人工智能的关系。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1031&amp;utm_content=wangyawei">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。华为云开发工具和效率领域首席专家王亚伟将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1031&amp;utm_content=wangyawei">智能化信创软件 IDE</a>"」的专题出品人。在此次专题中，你将了解到智能化信创软件 IDE 的基于自有技术内核的架构和标准，以及 AI 原生的两大特征。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=7&amp;utm_term=1031&amp;utm_content=wangyawei">王亚伟</a>"，华为云开发工具和效率领域首席专家，华为软件开发生产线 CodeArts 首席技术总监，当前领导一支国际化软件专家团队负责 CodeArts IDE 系列产品的研发和华为云开发者生态能力建设。加入华为前，曾任微软开发者事业部资深开发经理，在微软全球多个国家地区工作 13 年。近 20 年的云和开发工具的行业经验让他具备从底层技术、产品规划到开发者生态能力建设洞察的能力。王亚伟先生发表和被授予 20 多项软件开发技术相关的发明专利。QCon 全球软件开发大会（上海站）2022 出品人。</p><p></p><p>相信王亚伟的到来，可以帮助提升此专题的质量，让你学习到，智能化信创软件 IDE 如何将基础软件开发工具的核心技术实现自主可控，在拥抱开源的同时逐步建立基于自有技术内核的架构和标准，形成自有开放生态，以及内核架构如何无缝融入人工智能。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 7 折优惠，立减￥2040！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9de076cb6003669df743b02daac3c00c.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vieTybwOJv3JKQZvgmrJ</id>
            <title>AIGC 时代，如何提升端侧算力利用效率？</title>
            <link>https://www.infoq.cn/article/vieTybwOJv3JKQZvgmrJ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vieTybwOJv3JKQZvgmrJ</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 08:12:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, AI大模型热潮, 算力需求, AIGC, 端侧算力利用效率
<br>
<br>
总结: 近年来，ChatGPT等AI大模型的兴起引发了算力需求的爆发，如何高效利用算力成为关注焦点。在大规模AI模型训练中，提升算力利用效率的技术和方法以及AIGC应用下沉到终端成为重要议题。英特尔中国技术部总经理高宇分享了如何提升端侧算力利用效率的主题，探讨了生成式AI技术的发展与挑战，以及算力成本居高不下的问题。他提出了分布式和层次化的推理部署思路，建议在云端进行大规模训练，将不同类型的AI推理算力下沉到边缘侧，以提高算力利用效率。 </div>
                        <hr>
                    
                    <p>ChatGPT 的爆火掀起了 AI 大模型热潮，也进一步拉动了算力需求的爆发，面对呈指数级增长的算力需求，如何用得起、用得上、用得好算力成为大家普遍关心的问题。那么，在大规模 AI 模型训练中，如何保证算力的高效利用？有哪些技术或方法可以提升训练的效率和稳定性？AIGC 应用如何下沉到终端？近日，InfoQ《极客有约》邀请到了英特尔中国技术部总经理高宇，为大家分享《AIGC 时代，如何提升端侧算力利用效率？》。</p><p></p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/w4UPiNImmKac6OSgpEiP">https://www.infoq.cn/video/w4UPiNImmKac6OSgpEiP</a>"</p><p></p><p>姜雨生：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，微软软件工程师姜雨生。本期直播，我们邀请到了英特尔中国技术部总经理高宇老师来给我们做分享。今天的直播主题是《AIGC 时代，如何提升端侧算力利用效率？》。先请高宇老师给大家做一个简单的介绍。</p><p></p><p>高宇：InfoQ 的朋友们，大家晚上好。我是高宇（Gary Gao），来自英特尔中国，负责英特尔中国技术支持团队的工作。今天，我非常荣幸与大家分享关于在端侧实现 AIGC 的热门话题。</p><p></p><h2>生成式 AI 技术的发展与挑战</h2><p></p><p></p><p>姜雨生：去年推出的 ChatGPT 引起了广泛关注，掀起了大型 AI 模型的热潮，企业和个人对算力的需求呈现出爆发性增长。这轮 AI 算力需求的爆发给您带来最大的感受是什么？行业发生了哪些变化？</p><p></p><p>高宇：这一轮生成式 AI 热潮确实代表了技术上的一个重大突破，无论是给消费者、商业客户还是数据科学家，都带来了巨大的潜力和影响。从去年 ChatGPT 3.5 正式发布以来，它展示出的智能和生成文本的能力让整个学术界、消费市场和最终用户都感到震惊。在短时间内，ChatGPT 3.5 已成为全球最受欢迎的应用之一，这一成就令人印象深刻。我认为，它对整个行业的影响可以从正面和挑战两个维度来分析。</p><p></p><p>从正面来看，首先，生成式 AI 极大地改善了用户体验。以前的搜索引擎和智能问答系统在知识方面相对固定，而生成式 AI 具有强大的学习和涌现能力，这是以前所没有的。因此，用户体验得到了显著改善。</p><p></p><p>其次，它激发了学术界和企业界对这项技术的研究兴趣。在过去的半年里，全球企业和知名的学术机构都大量投入到生成式 AI 的研究中。这种巨大的资金和智力投入使我们相信未来几年生成式 AI 的发展将非常迅猛，因为许多人都在进行相关研究和突破。</p><p></p><p>第三，我们看到生成式 AI 目前主要应用于人机对话，但我们更看好它在各种行业中，尤其是垂直行业中的应用潜力。例如，目前人们正在探讨用于医疗领域的大型模型，专为银行系统设计的大型模型，甚至为金融等垂直行业开发的模型。因此，我们对它在这些领域的应用前景非常期待。</p><p></p><p>当然，大型模型的出现和生成式 AI 的发展确实带来了一些重要挑战。在这方面，我们可以总结为以下几点。</p><p></p><p>首先，几乎所有大型科技公司都加入到了这个浪潮中。因此，这个领域的应用进展非常迅速，有时候可能会出现一些重复性工作，甚至资源浪费。</p><p></p><p>第二，数据隐私和可靠性是一个重大问题。个人数据的保护以及互联网上的开源内容如何得到保护都是重要考虑因素。此外，还涉及到更深层次的问题，例如对问题的解释、价值观的取向和正确判断等，这些都是全新的挑战。</p><p></p><p>英特尔倡导的 AI 不仅关注性能和能力，还强调负责任的 AI。这也是领先厂商共同的理念，即人工智能的发展应该以对社会负责任的态度为基础。总之，生成式 AI 对我们行业带来了重要冲击，后续我们可以深入探讨这些挑战的细节。</p><p></p><h2>算力成本居高不下，如何找到破解之法？</h2><p></p><p></p><p>姜雨生：无论是模型训练还是模型调用，计算资源的需求都在不断增加。这背后伴随着高昂的成本，对许多企业而言，这成为了业务扩展的一道巨大障碍。您怎么看算力贵这一现象？随着技术的发展，算力贵的现状会有所改善吗？</p><p></p><p>高宇：目前，大家都不得不承认算力成本有待解决。因此，大家都对这个行业的情况非常关注。我们可以分析一下导致算力成本上升的原因。</p><p></p><p>首先，运行生成实验，特别是训练模型所需的 GPU 性能相对较高，因此整个 GPU 以及 GPU 卡的成本较高，它需要更大的 GPU 芯片来提供更高的算力。此外，它还需要更快的内存，通常采用 HBM（High Bandwidth Memory，高带宽内存）内存架构，这也增加了成本。再加上需要用 8 卡互联的训练机，整机的物料成本非常昂贵，这是导致成本高昂的原因之一。</p><p></p><p>第二，与之前提到的问题相关，现在几乎所有人都涌入了这个行业，导致了短期内供大于求的情况。一度出现了 GPU 卡供不应求的情况，这已经从去年年底开始，需求量大但供应相对不足。</p><p></p><p>第三，整个大型 GPU 服务器或智算中心的运营成本极高，包括场地和能源消耗。一个标准的 GPU 服务器机柜功耗至少为 30 千瓦，而大多数数据中心机柜通常只能达到 10 千瓦到 20 千瓦之间，无法满足 30 千瓦的要求，这也增加了成本因素。</p><p></p><p>当然，我们还需要考虑一点，因为生成式 AI 仍处于早期阶段，所以在许多算法优化和资源利用方面还有改进的空间。因此，有望在未来降低算力成本。</p><p></p><p>姜雨生：在目前算力贵这个方向，英特尔目前有哪些相关的解决方案，这面方便给我们大概介绍一下吗？</p><p></p><p>高宇：我们需要思考一个根本性问题，即如何应对昂贵的算力这一行业性的难题。我们有几个想法，虽然稍后我们还会谈及产品方面的问题，但现在我们首先想从行业角度提出一些大的思路。</p><p></p><p>首先，我们认为当前的推理部分应该更加分布式和层次化，充分利用云、边缘和终端的不同层次来部署推理算力，以充分发挥算力性能。具体来说，我们的建议是在云端进行大规模的训练，这是云侧的任务。此外，云侧适合大集群训练，部署超大型模型，例如 ChatGPT 等超过 100 亿的模型。第三，云侧适合部署高并发的场景，即当用户数量庞大时，需要同时满足所有客户的需求，这也需要云端来实现。</p><p></p><p>对于不属于以上几种情况的 AI 推理算力，我们建议将其下沉到边缘侧。如今，运营商和企业都拥有许多边缘侧数据中心，虽然这些数据中心规模较小，机器配置的算力相对较低，但足以支持多种类型的大型模型的推理。根据我们的判断，大约在 10 亿到 30 亿之间的模型可以考虑部署在边缘侧，因为边缘侧可以使用性能稍微较低端的 GPU 卡或 CPU 进行推理，性能足够。此外，在边缘侧部署可以提供更好的低延迟体验，成本也较低。</p><p></p><p>下沉的第二步就是把它部署在端侧。我们认为一些规模较小的模型，比如小于 10 亿参数的模型，经过一定的优化和量化，以及低精度的比特量化后，完全可以部署到个人计算机（PC）或虚拟私有云（VPC）等设备上。将其部署到端侧带来两个明显的好处。首先，它的性能延迟是最低的，因为不需要经过网络传输，减少了任何网络延迟。此外，边缘侧部署还有一个重要的优势，即对个人隐私的最大程度保护，因此数据泄露的风险几乎不存在。因此，从大的原则上讲，我们希望将大型模型转化为云、边缘和终端三层协同的架构，这应该是未来发展的趋势之一。</p><p></p><p>姜雨生：有观众提问，在算力优化方面，我们业界还有没有一些通用的方案？</p><p></p><p>高宇：我们了解到，在当前的研究领域中，一个备受关注的通用方案是针对低比特量化的优化。目前，大多数部署在云端的模型采用的是 FP16（16 位浮点数）的精度。然而，如果要将模型部署在边缘侧或终端侧，通常的做法是首先将其量化为 INT8（8 位整数），然后可以进一步将其量化为更低比特位，如 INT5、INT4 或 INT3，这都是可能的，而且我们看到在这方面行业已经取得了一些显著的进展。</p><p></p><h2>AIGC 应用如何下沉到终端？</h2><p></p><p></p><p>姜雨生：我认为开发者会积极采用 AIGC 的大型模型，因为这是未来的趋势。在过去，我们主要在云服务器上运行 AIGC 应用，包括我自己目前使用的一些 Azure 云上的产品。但云端 AI 也存在延迟和各种限制等方面的一些短板。那么，AIGC 应用有下沉到终端的可行性吗？</p><p></p><p>高宇：根据我们目前的研究成果，我可以告诉大家，针对英特尔的最新平台，也就是第 13 代（以及后续推出的第14代，采访时第14代酷睿尚未发布）酷睿处理器家族，我们已经取得了非常不错的优化结果。这个平台不仅适用于笔记本电脑，还包括台式机。我相信许多开发者和用户在购买电脑时都会选择最新的酷睿平台。</p><p></p><p>以第 13 代酷睿平台为例，我们的优化结果可以使模型从 7 亿参数到 18 亿参数都能够流畅运行。特别是在 7 亿到 13 亿参数范围内，性能效果非常出色，即使超过 13 亿参数，模型也可以运行，尽管速度稍慢，但我们认为基本上也可以满足用户的需求。当然，我们目前的优化主要是在 CPU 上进行的，但下一步我们将充分发挥平台内的集成显卡（IGPU）能力，以进一步提升速度。</p><p></p><p>此外，对于未来，我想提到最近引起广泛关注的一项重要消息，那就是我们披露了英特尔即将发布的下一代平台，内部代号为 Meteor Lake，正式品牌叫做 Core Ultra。这个平台不仅具有强大的 CPU 算力，还将 GPU 算力提高了一倍，因此GPU算力非常强大。另外，它还内置了专用的 AI 加速器（NPU），可以提供超过 11 tops 的峰值算力。因此，在下一代平台上，我们将能够充分利用三种计算资源，包括 CPU、GPU 和 NPU 的算力，以实现更出色的性能。这是我们下一代平台的亮点，敬请期待。</p><p></p><p>姜雨生：英特尔之前提出在 PC 端侧跑 AIGC 应用，具体是如何实现的？在软硬件层面是如何提升算力利用效率，实现算力优化的？</p><p></p><p>高宇：我来简要介绍一下我们目前正在发布的开源框架，它叫做 BigDL，是专门为英特尔的处理器和 GPU 开发的一个低比特量化框架。感兴趣的观众可以进入在 GitHub(https://github.com/intel-analytics/BigDL)上查看，下载我们的 BigDL 开源代码，进行实验。</p><p></p><p>BigDL 有一些显著特点。首先，它支持低比特量化，从 INT8 到 INT5、INT4、INT3 等各种低比特的数据精度，从而提供更好的性能，并减少内存占用。这一点尤其重要，因为在边缘计算领域，除了性能挑战之外，内存也相对较低，所以低比特量化是解决这个问题的一种方法。</p><p></p><p>此外，BigDL 支持多种平台，包括英特尔的各种 CPU 系列，从 Xeon 处理器到酷睿处理器等。它还支持英特尔的各种 GPU 系列，包括英特尔 Flex 系列用于数据中心的专用显卡以及英特尔锐炫（ Arc） 系列面向消费者的显卡。</p><p></p><p>姜雨生：我也确实感受到了在个人电脑上运行大型模型以及进行内容生成的可能性，特别是在我的个人电脑上装备了这些硬件的情况下。实际上，我也想了解一下一些相关的技术，如果要大规模普及，关键的主要指标可能是颠覆，即用户在他们的实际工作和生活中所体验到的变革。那么AI 能够在端侧带给用户哪些具体的体验提升？</p><p></p><p>高宇：从我们现在的观察来看，大型模型在端侧用户领域可能有几个可能的应用场景。首先，大型模型可以成为每个用户的个人超级助手。这种大型模型可以在云端运行，同时也可以通过我们刚刚提到的低比特量化技术在个人电脑上运行，从而提供更好的用户体验。这是第一个应用场景。</p><p></p><p>第二，它可以用于文档处理，包括提取文档的核心思想和纠正文档中的语法错误等任务。对于这种应用场景，更适合将模型部署在端侧，因为许多文档包含一些个人属性，用户可能不愿意将其上传到云端。</p><p></p><p>第三，我们观察到大型模型，特别是 Diffusion 模型，在图像生成方面具有出色的能力，这对于许多设计师来说是一个强大的工具。许多图形、图像和三维设计公司积极采用 Stable Diffusion 以及相关衍生模型，以帮助设计师生成各种图片和画面，从而实现事半功倍的效果。</p><p></p><p>姜雨生：将 AIGC 相关应用以预装软件的方式适配到未来的电脑中，是否是 PC 创新的一个新方向？它对于 PC 应用效率的提升是否有着大幅超越以往的预期？</p><p></p><p>高宇：当然，答案是肯定的。在未来的个人电脑上，无论是笔记本还是台式机，它们的算力已经足以支持像 7 到 13 亿级别的大型语言模型在本地运行。这种潜力已经存在，接下来我们可以期待不同的商业模式的出现。</p><p></p><p>首先，我们可能会看到一些商业软件集成了中小型大语言模型，将其变成了生成式人工智能的专业商业软件。这些软件还有可能集成了 Stable Diffusion 等功能，从而成为一种可用于文本生成和其他工作流程的商业软件。因此，可以期待在桌面平台上出现集成生成式人工智能能力的商业软件，这是一个可能的落地方式。</p><p></p><p>另外一种方式是鼓励更多的 OEM 制造商，也就是个人电脑的品牌制造商，为自己的产品开发专门针对硬件优化的生成式人工智能软件，并将其预装在他们的电脑上，以提高最终用户的体验，使电脑更易于使用和更具趣味性。这种辅助性软件可以提升用户的使用体验，增加趣味性，我认为这也是一个非常有潜力的方向。</p><p></p><h2>端侧运行大模型存在哪些挑战？</h2><p></p><p></p><p>姜雨生：有观众提问，端侧跑这些大模型有没有一些难点我也比较关注这个问题，端侧跑大模型有没有一些相对不适用的场景或内容？</p><p></p><p>高宇：端侧与云侧相比，目前存在两大限制。首先，端侧的计算能力明显不如云端强大。这是显而易见的。第二，端侧的内存相对有限。当前，笔记本电脑和 PC 的主流配置通常为 16GB 内存。明年我们可能会看到更多配置为 32GB 内存的 PC，但即使是 32GB 内存，相对于云端来说，内存仍然有限。因此，端侧需要应对以下两个主要挑战。</p><p></p><p>首先，模型的参数量需要受限，通常在 130 亿以下。其次，必须进行低比特量化，这是一种必不可少的手段。经常有人问一个常见的问题，即将一个 FP16 模型量化为 INT4 后，精度损失似乎很大，这对大型模型的性能会产生什么影响？我们目前的基本结论是，在大型语言模型的情况下，从 FP16 到 INT4 后，回答问题的质量会略微下降，但下降幅度并不是很大。如果我们使用评分机制，原来的模型可能是 85 分的模型，经过量化后，可能会下降到 82 分左右，所以大致是一个个位数的质量下降。但是在内存方面，收益是非常大的，这是一个权衡。</p><p></p><p>然而，对于 Stable Diffusion 模型而言，如果将 FP16 量化为 INT8，整个图像生成的质量下降会比较大。因此，对于运行稳定扩散模型的端侧，我们仍然坚持使用 FP16。幸运的是， Stable Diffusion 模型的参数量不是很大，因此即使在端侧，FP16 的性能也完全可以胜任。</p><p></p><p>姜雨生：在端侧执行一些生成式内容和场景时，精确度并不是特别重要，尤其是对于一些模型复杂度不太高的情况来说，这种方式会更加合适。下一步，英特尔有哪些技术探索和产品规划呢？有哪些技术难题是我们在未来需要解决的？</p><p></p><p>高宇：对于英特尔未来的产品规划，目前英特尔在生成式 AI 领域有几个主要的产品家族，可以从云端、边缘和端侧三个维度来介绍。</p><p></p><p>在云端，英特尔的关键产品是 Gaudi2，这是 英特尔Habana最新推出的产品。Gaudi2 具有非常高的算力性能，它还具有大容量的显存，目前 Gaudi2 的配置为 96GB 的 HBM2 显存，因此可以容纳更多的模型。此外，英特尔还推出了专门针对中国市场定制的 Gaudi2 中国版本。云端英特尔还有一款产品叫做 Xeon HBM，它是一款针对大模型推理而设计的 CPU，内置了 64GB 的 HBM2 高速内存，这对于大型语言模型的推理性能提升非常有帮助。</p><p></p><p>边缘侧，英特尔推出了两款显卡产品，一款是英特尔 Flex 系列，另一款是锐炫（ Arc） 系列。Flex 系列是为数据中心和服务器设计的无风扇 GPU 产品，而 Arc 系列则是面向消费者市场的显卡，在算力方面也非常强大，可以满足边缘侧推理的要求。这些产品将为边缘侧大模型推理和 Stable Diffusion 提供强大的支持。</p><p></p><p>总的来说，英特尔在生成式 A I领域有一系列强大的产品，覆盖了云端、边缘和端侧，为不同应用场景提供了多样化的解决方案。</p><p></p><p>姜雨生：有观众提问，端侧模型跟云端模型有可以配合的方式吗？</p><p></p><p>高宇：端侧模型和云端模型可以进行协同配合，一种可能流行的做法是由端侧模型进行问题的初步预判断。这个端侧模型可以是相对轻量级的，用于判断用户问题的导向方向。如果这个初步判断结果显示性能足以在端侧大模型上运行，那么模型可以在端侧执行。但如果判断需要更强大的计算能力，那么就可以将任务传递到云端进行更大型的模型推理。这种方式可能比较容易实现，因为它避免了对同一个模型进行拆分，尽管拆分模型也是一种可能的方式，但会更加复杂。</p><p></p><p>姜雨生：如果希望在个人电脑上运行之前所描述模型相关的内容，最低配置要求如何？</p><p></p><p>高宇：关于个人电脑的配置，主要取决于您的耐心和使用场景，当然这是个半开玩笑，但基本上，为了达到基本的用户体验要求，我们建议以下配置：</p><p></p><p>处理器（CPU）：最好选择第 13/14 代酷睿处理器，尤其是选择 I7 或更高级别的型号。如果有预算，并且想要更出色的性能，选择 I9 处理器会更好，正如我在之前的演示视频中展示的那样。内存（RAM）：至少 16GB RAM 是起点，但更好的选择是 32GB RAM。此外，要注意内存的速度，因为现在的内存，尤其是 DDR5 内存，速度范围从入门级的 5677 MHz，一直提升到高达 7233 MHz。内存速度越快，性能表现通常越好。再次强调，大型模型通常对内存带宽要求较高，因此提高内存带宽会带来更大的性能收益。散热设计：除了硬件配置，还要考虑系统的散热设计。良好的散热设计可以让 CPU 在 Turbo 模式下更长时间地运行，从而提高性能表现。</p><p></p><p>选择适合需求的个人电脑配置是一个综合考虑的过程。明年新发布的电脑新品通常会公布其运行大型模型的性能指标，用户可以根据厂商提供的指标来选择适合自己需求的配置，这应该会更准确地满足你的期望。</p><p></p><p>当然了，我认为目前大模型仍然存在一些挑战，尤其是在处理模型的一些幻觉问题方面，这个问题在整个行业中仍然是一个难点，需要不断攻克。</p><p></p><h4>嘉宾介绍</h4><p></p><p></p><p>特邀主持：</p><p></p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p></p><p>嘉宾：</p><p></p><p>高宇，英特尔中国技术部总经理，负责领导英特尔中国从端到云的产品技术使能和方案支持工作，对中国IT产业和生态链、以及前沿技术发展趋势有着深入的洞察和见解。&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TprcKvXyB5fsKgC4SaLx</id>
            <title>程序员的私人助理：Amazon CodeWhisperer</title>
            <link>https://www.infoq.cn/article/TprcKvXyB5fsKgC4SaLx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TprcKvXyB5fsKgC4SaLx</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 08:09:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 编程, AI 辅助编程, Amazon CodeWhisperer, 生产力
<br>
<br>
总结: 编程是一项有趣而又富有挑战性的工作，但也会遇到很多困难和繁琐的任务。AI 辅助编程工具 Amazon CodeWhisperer 可以帮助开发者提高生产力和代码质量，它是基于亚马逊内部使用的 AI 编程助手的经验和技术而开发的。使用 CodeWhisperer，开发者可以节省时间和精力，快速完成编程任务，提高代码的可读性和可维护性，增强代码的安全性，并跟踪开源代码的来源和许可信息。 </div>
                        <hr>
                    
                    <p>编程是一项有趣而又富有挑战性的工作，但是也会遇到很多困难和繁琐的任务。有没有一种方法可以让编程变得更容易，更快，更安全呢？答案是有的，那就是 AI 辅助编程。</p><p></p><p>在这篇文章中，我将介绍一款由亚马逊推出的 AI 辅助编程工具——<a href="https://www.infoq.cn/article/JcIQOLpgqVK3AAgQxNQt?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Amazon CodeWhisperer</a>"，它是如何帮助开发者提高生产力和代码质量的，以及我使用它的一些体验和感受。</p><p></p><p>Amazon CodeWhisperer 是在 2021 年 12 月正式推出的一款 AI 代码生成器，它是基于亚马逊内部使用的 AI 编程助手的经验和技术而开发的。推出之际，Amazon 邀请了一些开发者参与一个生产力挑战，结果显示使用 CodeWhisperer 的开发者比不使用的开发者更有可能成功完成任务，并且平均速度快了 57%。</p><p></p><p>推出后受到了很多开发者和企业的欢迎和好评，例如 Accenture 就使用 CodeWhisperer 来提高开发者的生产力，包括新人培训，编写样板代码，使用陌生的语言，以及检测安全漏洞等方面。</p><p></p><p>而现在，亚马逊更是大方的开放了个人免费套餐，在个人开发过程中享受 AI 辅助编程的快感。使用下来的体验就像多了一个秘书，而自己从程序员的角色变成了半个产品经理的角色：我只需要口述我想要的功能，它就能帮我生成初版的代码，稍微修改就能实际运行。真正解放了人的思想。</p><p></p><p>它目前支持 15 种编程语言，包括 Python，Java，JavaScript 等，以及多种 IDE，包括 VS Code，IntelliJ IDEA，AWS Cloud9 等。你只需要免费注册并下载 CodeWhisperer 插件，安装到你喜欢的 IDE 中，然后就可以开始使用了。</p><p></p><p>我以 Goland 为例，只需插件市场搜索“CodeWhisperer”进行安装以及登录，便可开始使用了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2661b09aa6dcb87cc44f7b80964f5da.png" /></p><p>​</p><p>插件市场搜索 CodeWhisperer，安装完成后，左下角会有一个 AWS toolkit 的工具栏，点击它并且登录。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc79a255e5f36461fd687630692b86ab.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdb46af8d1dc6e8a4c89f9cd11dc3c72.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24090b58e3b54d55c94081aa340c188c.png" /></p><p>授予权限，权限授予之后，左下角 CodeWhisperer 显示可用状态时，就可以开始编码，享受 AI 辅助编程的快感了。</p><p><img src="https://static001.geekbang.org/infoq/de/de77c9f7c8eaf9dadef10866e0201935.png" /></p><p>​</p><p>比如很经典的斐波那契数列，只需要描述一下函数功能，接下来的事情就是 Tab 键自动输入代码了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/494f69fd1887b906b77650f121704f92.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a8b32d509fc68c47bd0a7744c4cb5c7.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21d567be03fe567fd2de7146143c6cfa.png" /></p><p>​</p><p>共计一行描述，三次 Tab 键，完成了首次 AI 编程辅助。整个使用过程非常简单和自然，你只需要在 IDE 中写下你想要实现的功能的注释，例如“创建一个列表”，“连接到数据库”，“发送一封邮件”等，CodeWhisperer 就会自动给出多个代码建议，你可以选择接受或者继续编写自己的代码。</p><p></p><p>CodeWhisperer 会根据你的代码风格和命名习惯，生成符合你的习惯的代码。你还可以使用 CodeWhisperer 来扫描你的代码，检测并修复安全漏洞，以及跟踪开源代码的来源和许可信息。</p><p></p><p>很多人可能认为程序员的核心能力是写代码，其实并不是。真正的价值是思考，是写代码之前的苦思冥想，最终实现则是水到渠成的事情。而 Amazon CodeWhisperer 带来了什么呢，个人认为其中最主要的是可以提高开发者的生产力和代码质量。使用 CodeWhisperer，可以：</p><p></p><p>节省时间和精力，避免编写重复和繁琐的代码，快速完成编程任务。提高代码的可读性和可维护性，遵循编码规范和最佳实践，减少错误和 bug。更高效地使用 AWS 服务，获取符合 AWS API 的代码建议，轻松构建云端应用。增强代码的安全性，及时发现和修复安全漏洞，防止数据泄露和攻击。代码负责任，跟踪开源代码的来源和许可信息，避免版权纠纷和法律风险。</p><p></p><p>欢迎大家使用，提高程序员的幸福感！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a1uk0eGDAxBQN8F8AcFY</id>
            <title>中小银行如何构建智能风控体系？明确业务需求比盲目求新更重要</title>
            <link>https://www.infoq.cn/article/a1uk0eGDAxBQN8F8AcFY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a1uk0eGDAxBQN8F8AcFY</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 06:38:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 客户需求演变, 金融机构互动方式刷新, 中小银行风控体系, 业务发展挑战
<br>
<br>
总结: 随着客户需求的演变以及金融机构与客户互动方式的刷新，传统的风控手段开始失效，中小银行的风控体系也必须相应做出迭代与升级。但和大型金融机构相比，中小银行在资源、人才、业务规模等方面都不具优势，在业务发展和推进数字化过程中，面临着一系列特有挑战。构建智能化风控体系首先必须明确业务战略，顶层设计和规划非常关键，同时实施过程要确保重点突出，优先级安排符合业务实际需求。另外，还要注重建立容错机制，以避免机构走入常见的误区。此外，健全的数据管理是构建这一体系的基石。在此基础上，整合系统和工具、完善策略和模型以及关注宏观经济风险，都是确保风控体系的关键因素。大数据和人工智能技术在风险管理领域的应用已经相当深入，与此同时，大模型、AIGC等新兴技术也正逐步崭露头角。尽管它们目前还处于探索阶段，但未来的发展潜力无疑是巨大的。 </div>
                        <hr>
                    
                    <p>随着客户需求的演变以及金融机构与客户互动方式的刷新，传统的风控手段开始失效，<a href="https://www.infoq.cn/theme/200">中小银行</a>"的风控体系也必须相应做出迭代与升级。但和大型金融机构相比，中小银行在资源、人才、业务规模等方面都不具优势，在业务发展和推进数字化过程中，面临着一系列特有挑战。</p><p></p><p>在日前的《超级连麦·数智大脑》直播中，InfoQ 与重庆工程学院大数据与人工智能学院院长<a href="https://www.infoq.cn/article/eDq0AVuIVKZwgJoCMV41?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">李钦</a>"深入探讨了《<a href="https://www.infoq.cn/video/oXX1AvBczGb9eDTqScIm">中小银行智能风控体系是如何构建的</a>"》。他强调，在这一现状之下，构建智能化风控体系首先必须明确业务战略，顶层设计和规划非常关键，同时实施过程要确保重点突出，优先级安排符合业务实际需求。</p><p></p><p>另外，还要注重建立容错机制，以避免机构走入常见的误区。此外，健全的数据管理是构建这一体系的基石。在此基础上，整合系统和工具、完善策略和模型以及关注宏观经济风险，都是确保风控体系的关键因素。</p><p></p><p>在李钦看来，大数据和人工智能技术在风险管理领域的应用已经相当深入，与此同时，大模型、AIGC 等新兴技术也正逐步崭露头角。尽管它们目前还处于探索阶段，但未来的发展潜力无疑是巨大的。</p><p></p><p>以下是对话全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h3>中小银行发展现状与数字化挑战</h3><p></p><p></p><h5>InfoQ：是否可以从您的视角介绍一下我国中小银行目前整体的发展现状？</h5><p></p><p></p><p>李钦：中小银行当前面临的发展现状和挑战大致有以下几方面：</p><p></p><p>资产质量下降：受到国内外经济形势影响，中小银行的资产质量明显下降。特别是那些以服务小微企业和长尾人群为主的银行，这种趋势更为明显；</p><p></p><p>资产规模增长放缓：大多数中小银行的资产规模增长已经放缓，有的甚至停滞不前，但也有少数逆势而上，资产规模增长迅速；</p><p></p><p>产品同质化严重：</p><p>在小微领域，尽管许多银行都视之为战略目标，但实际上他们提供的产品如税贷、订单贷、流水贷等在功能上高度相似，导致产品同质化问题尤为突出。对于 C 端用户，中小银行受限于其获客能力，流量基本被大型互联网平台所控制，议价空间小，缺少自主品牌。也导致他们在这一方面的产品同质化问题可能更加严重。</p><p></p><p>人才和思维方式的问题：中小银行在人才战略、思维方式转变上存在显著的短板。例如，很多中小银行虽然会进行战略思考，但其战略方针可能更换频繁，反映出管理层的思路并不统一。同时，由于动力不足、思维方式转变不及时以及某些地方性的限制，导致它们在人才招聘、人才储备和人才战略规划方面存在不足。</p><p></p><h5>InfoQ：基于这些现状，中小银行在推进数字化过程中面临着哪些独特挑战？又有什么新的发展机会？</h5><p></p><p></p><p>李钦：在过去的几年里，中小银行在追求数字化转型时，主要选择了以 C 端作为突破口。这为它们创造了一个与流量丰富的互联网平台合作的窗口期。</p><p></p><p>然而，这种模式下，许多银行往往只起到了资金提供者的角色，大部分关键业务流程如获客、营销、品牌运营和风控等都被互联网平台所控制。这导致中小银行在这种合作中丧失了定价权，获得的收益相对较低，而风险承担却相对较大，存在明显的风险与收益不匹配的现象。</p><p></p><p>但未来，中小银行的<a href="https://www.infoq.cn/article/Su6bfESLE0kA7g9waE7X">新的发展机会</a>"或将集中在产业互联网领域。与 C 端不同，B 端的每个行业和垂直领域都有其独特之处，这使得它不易被单一的公司或企业类型所垄断。此外，新技术与特定产业的深度结合将会为金融产品创新提供新的机会。</p><p></p><p>为此，中小银行应当挖掘自己的地域和行业特色，深入研究产业互联网，以此为基础创新并打造出真正具有竞争力的产品。这不仅能够帮助中小银行弥补在 C 端的短板，还可以让它们在 B 端市场上获得更大的话语权。</p><p></p><h3>风控体系的演化与痛点</h3><p></p><p></p><h5>InfoQ：风控是金融业务的命脉，近年来金融环境和金融业务范畴也日益复杂多变，在风控层面会面临哪些新的难题？</h5><p></p><p></p><p>李钦：风控分为两个层次：管理层面与技术层面。</p><p></p><p>首先，从管理层面看，当前的宏观经济形势较为复杂，使得中长期的判断变得困难。许多中小银行在风控上过于强技术层面，忽视了从宏观经济趋势出发去调整资产结构的重要性。风控在较高的层次上，应当首先考虑宏观经济的趋势，并根据这一趋势提前布局资产结构，积极主动调整如何投放节奏。这可能比单纯针对具体产品或客户级的风控更为关键。</p><p></p><p>其次，防范系统性风险是另一个重要议题。技术层面的风控虽然能够解决具体操作中的问题，但在更宏观的层次上，我们还需识别未来是否存在某些领域的系统性风险。</p><p></p><p>另外，从产品设计的角度看，传统金融机构在设计产品时更多是出于自己的角度，提供给客户的选择相对有限。但近年来，金融行业逐渐追求为客户提供“千人千面”的定制化产品，这无疑给金融机构带来了新的挑战。更重要的是，在产品设计时，若未充分考虑风控的需求，如所需数据、流程设计等，这可能会导致产品在后期的风控中出现问题。</p><p></p><p>在<a href="https://www.infoq.cn/article/ixcNvwClOzTSal48vR6k">风控管理</a>"中，客户级的风险管理是另一个重要环节。特别是对于线上业务，客户级风控主要从两个方面展开，反欺诈和风险策略模型。然而，在进行客户级风险管理时，常面临的问题是缺乏数据、技术支持、专业的模型人员或风险策略分析人员。这些因素可能阻碍建立一个健全的风控体系。为应对这些挑战，我们在过往的实践中，逐步搭建了一套完善的风险管理体系，并计划在 11 月的 FCon 大会议进行详细介绍。</p><p></p><h5>InfoQ：风控手段一直都有，但为何它们在现在的金融业务环境中失效？</h5><p></p><p></p><p>李钦：首先是客户本身的变化，过去，金融服务可能主要针对优质人群。但随着普惠金融的推进，目标逐渐转向服务更多的“长尾”客户。这部分客户往往可能连基本的征信记录都缺乏，导致他们在选择金融服务时，能够获取到的服务有限。为满足这种新的客户群体，我们需要引入新的技术，收集更多的数据维度，以更有效地进行风险管理；</p><p></p><p>其次是与客户的交互方式的变化，与客户的互动方式已从面对面的交流转向线上互动。这种线上的交互方式，尽管带来了便捷性，但同时也引入了新的风险。例如，金融机构不仅要面临信用风险，欺诈风险也日益凸显。由于我们无法面对面与客户接触，可能会遇到如假冒身份、提供虚假资料或伪造数据的风险。甚至有些人可能利用系统的漏洞，对风控体系进行攻击。这都是新技术应用在风险管理中可能引发的新问题。</p><p></p><h5>InfoQ：智能风控本质上是结合大数据和人工智能等新技术来提升金融业务的风险识别与处理能力。那么，金融机构具体如何利用这些技术来加强其风控体系呢？</h5><p></p><p></p><p>李钦：首先，大数据技术提供了数据存储、计算和数据处理能力，可以用于开发和应用算法、图像、语音和非结构化数据等，以提高风险管理的效率和准确性。在风险管理中，我们通常需要外部购买一些数据来识别多头风险，例如短期内多次申请贷款或信用卡的行为。这些数据可以从侧面反映客户对资金需求的量或是客户是否成功申请，从而提供关于客户信用风险的信息。</p><p></p><p>通过将大数据技术和人工智能技术结合起来，我们可以更准确地识别和评估客户的风险，并采取相应的措施来管理和控制风险。当然，反欺诈分析也已广泛运用人脸识别和知识图谱技术。大数据和人工智能技术在风险管理中的应用已相当成熟，而像 <a href="https://www.infoq.cn/article/tXdg1xI1YWGYG6iGg4rj">AIGC </a>"和 ChatGPT 这样的新技术也逐渐被引入，尽管目前处于初级阶段，未来将会有很大的空间。</p><p></p><p>其次，像数据采集和模型优化有许多的方式，例如与征信机构的合作，尤其是如何深度挖掘人民银行征信数据，因为它在金融领域的质量和相关度最高。当然，也有许多中小银行与科技公司联手，推出定制模型和数据产品。</p><p></p><h5>InfoQ：新技术的引入会不会影响客户体验，如何在保持业务风险可控的同时，确保良好的客户体验呢？</h5><p></p><p></p><p>李钦：客户体验与业务发展并不矛盾。当客户体验不佳时，因逆向选择现象业务风险会增加，因为好客户可能因为操作麻烦而选择退出，而坏客户不在意这些繁琐。另外，客户体验在设计额度和利率时都极为关键。我们的经验是，应该尽量简化客户的操作并避免给他们带来理解上的困扰，同时给到合理的定价和额度。</p><p></p><p>当前，许多机构，尤其是城商和农商体系，往往将各部门任务严格划分，如产品、风控、市场和运营各自为阵，这可能导致整体视角的缺失，从而设计出的产品可能面临不可预见的问题。因此，现代的互联网金融产品运营应当采用项目小组的方式，从产品设计开始，集结风控、科技等多方人员参与，确保从整体角度考虑产品的每个环节。</p><p></p><h3>智能风控体系搭建思路与路径</h3><p></p><p></p><h5>InfoQ：随着大模型的引入，它将如何影响或颠覆当前 AI 所执行的任务？</h5><p></p><p></p><p>李钦：在当前金融环境下，数据分析和风险建模的专家们因其高技能和专业性得到了普遍的认同，相应的薪资待遇也相当吸引人。然而，随着大模型和先进算法的出现，许多传统的、标准化的数据处理工作在未来有可能被<a href="https://www.infoq.cn/news/D5BW4LdBUGislXBCOFIZ">大模型</a>"所替代。</p><p></p><p>事实上，一些银行已经提出并尝试实施了“数字员工”的概念，这种应用最初主要体现在与客户的交互服务和催收过程中。在我看来，只要某项工作可以被抽象和标准化，如数据准备、样本标记和算法选择等，它们都有可能被自动化技术取代。</p><p></p><p>尽管如此，目前在信用风险领域，大模型的应用仍相对有限，多数机构更偏好于使用逻辑回归和基于决策树的集成算法，原因在于这些方法更易于解释和部署，且具有较好的稳定性。</p><p></p><p>可以预见，随着数据的不断增多和计算能力的提升，超大规模的模型在未来将得到更广泛的应用。除了信用风险领域外，如声誉风险管理，大模型可以帮助机构更有效地监控网络上的负面信息，如敏感词汇、图片和文字。此外，催收领域和与客户的实时交互也是大模型应用的重要方向。</p><p></p><h5>InfoQ：在推进智能风控的过程中，您认为金融机构最容易遇到的挑战或误区是什么？</h5><p></p><p></p><p>李钦：金融行业在推进智能风控时，确实面临着不少挑战。</p><p></p><p>首先，缺乏顶层设计是许多银行的通病。为了快速上线业务，很多银行在科技层面忽视了系统架构的规划，导致后期数据规范不统一、系统交互复杂，给后续的分析、建模和监管报送带来巨大困扰。因此，业务前期的数据规范和系统架构设计至关重要。</p><p></p><p>其次，团队管理也是一大难题。数字化风控涉及的核心能力分散在多个团队中，如科技部、业务部门、风险管理部和产品部等，需要这些团队能够紧密合作，形成敏捷的工作小组，共同面对和解决问题。</p><p></p><p>另外，容错机制的建立也不容忽视。互联网产品推出后不一定立即成功，因此应为其提供一定的试错机会和成本，让其有更多的尝试空间。金融机构在产品运营时，通常为产品设定一个最高的风险承受额度，超出此额度则认为产品的成功几率低，可能会考虑退出。</p><p></p><p>最后，机构在风控建设上常面临的挑战是目标不明确和资源分配不当。虽有大框架，但缺乏明确的实施进度和水平标准。这导致各团队频繁沟通，争取资源，却可能忽视真正重要和紧急的任务，增加了内部的消耗和跨部门的沟通成本。因此，建议机构应明确目标和优先级，集中资源处理关键问题。</p><p></p><h5>InfoQ：对于一个金融机构，特别是中小型银行，如何构建与其定位相匹配的战略顶层设计？</h5><p></p><p></p><p>李钦：在风险管理中，金融机构应综合考虑多个方面。</p><p></p><p>第一，数据管理是基石，包括如何有效地采集数据、进行存储、后续的数据清洗、加工、指标化和变量化。</p><p></p><p>第二，有了稳固的数据基础，接下来是系统和工具层面。这里不仅包括决策引擎，还有分析工具和建模环境等，确保风控人员能够轻松调取数据并进行分析。</p><p></p><p>第三，策略和模型层面是至关重要的。这要求有一套完整的、科学的风险处置策略，并与团队的专业能力及策略方法论相结合，实现策略的高效开发、优化和迭代。</p><p></p><p>第四，金融机构往往涉及多个参与者，如流量提供者、担保公司等，因此合作机构风险管理也不容忽视。这需要对合作机构的风险特点有深入了解，并设定相应的管理策略。</p><p></p><p>第五，产品风险管理是确保每款金融产品的风险处于可控范围内的关键，包括对产品可能出现的风险进行预警、分析和干预。第六，考虑到宏观经济的影响，金融机构还应关注宏观经济风险，如何根据这些风险制定策略，确定资产组合等。</p><p></p><h5>InfoQ：您认为，在现有的框架体系中，大模型将会在顶层设计的哪一部分发挥作用？</h5><p></p><p></p><p>李钦：大模型相对于传统的小模型有明显的区别。小模型主要处理结构化数据，计算复杂度相对较低，而模型样本量通常只在几十万至上百万的范围内。相比之下，大模型的参数数量庞大，能够处理更复杂的数据格式。尽管两者在高层次逻辑上基本一致，但大模型在数据处理层面与现有模型有很大的差异。</p><p></p><p>此外，模型构建是一个复杂的过程，涉及到算法选择、模型训练环境和数据来源等多个环节。因此，大模型不仅会影响数据处理层面，还与数据层和系统工具层存在紧密的交互关系，两者之间相互影响。</p><p></p><h5>InfoQ：对于中小银行，在构建您提及的风控管理体系时，应特别关注哪些问题？</h5><p></p><p></p><p>李钦：首先，中小银行在搭建风控体系时，首先必须明确业务战略。同时<a href="https://www.infoq.cn/article/GItTCDMzzSsxMcojWydF">顶层设计和规划</a>"非常关键，同时实施过程要确保重点突出，优先级安排符合业务实际需求。</p><p></p><p>另外，非常重要的是在认知层面，风险管理不仅是风险管理部门的责任。一个普遍的误解是，当风险发生或不良率上升时，只有风险管理部门需要对此负责。实际上，组织协调和业务风险是业务全流程的责任，需要整个团队的认知和配合。</p><p></p><p>在具体实施中，风险能力有多个组成板块，这将我在 11 月 FCon 大会中的重点分享内容。我们基于历史经验，提出了一套智能风控能力的评价标准，具有很高的科学性，期待在会议中与大家分享，帮助解决中小银行的实际问题。</p><p></p><h4>关于 FCon</h4><p></p><p>首届<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">FCon全球金融科技大会</a>"将于 11 月 19-20 日在上海举办。本次大会已邀请到工商银行、招商银行、汇丰银行、兴业银行、中信银行、北京银行、平安人寿、度小满、蚂蚁集团等业界知名银行以及金融机构的大咖，前来分享大模型、 Web 3.0 、隐私计算、数字货币、区块链等前沿技术在金融领域的落地案例。</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">点击链接</a>"即可查看全部演讲专题。</p><p></p><p>目前是 <a href="https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">8折特惠购票</a>"，报名立减 ¥1360，咨询购票可联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BmDYQcg9gvzd9OIyjZrT</id>
            <title>首次采用3nm制程、比M1 Max快80%！苹果亮相M3芯片，最高搭载40核GPU</title>
            <link>https://www.infoq.cn/article/BmDYQcg9gvzd9OIyjZrT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BmDYQcg9gvzd9OIyjZrT</guid>
            <pubDate></pubDate>
            <updated>Tue, 31 Oct 2023 05:38:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果新品发布会, MacBook Pro, M3系列芯片, 3nm制程工艺
<br>
<br>
总结: 苹果在新品发布会上宣布推出全新的MacBook Pro系列，搭载了全新的M3系列芯片，采用了先进的3nm制程工艺。这些芯片在CPU和GPU方面都有了重大改进，能够满足不同用户的需求。新款MacBook Pro提供了更强大的性能和功能，支持更大的内存和更流畅的任务支持，为用户带来了更先进的电脑体验。 </div>
                        <hr>
                    
                    <p>10月31日，以“Scary Fast（快得吓人）”为主题对苹果新品发布会如约而至。在此次发布会上，Apple 宣布推出全新MacBook Pro 系列，采用全新 M3 芯片系列：M3、M3 Pro 和 M3 Max。据悉，M3系列芯片采用3nm制程工艺，在CPU和GPU方面都有了重大改进。这三款3nm制程芯片能满足不同用户的需求。</p><p></p><h2>苹果亮相M3系列芯片：3nm制程工艺，最高搭载40核GPU</h2><p></p><p>&nbsp;</p><p>搭载M3 的全新 14 英寸 MacBook Pro 不仅能完成日常基本任务，而且在专业应用程序和游戏中也能提供良好的持续性能，现在起价为 1,599 美元；搭载M3 Pro 的 14 英寸和 16 英寸 MacBook Pro 提供更强大的性能和额外的统一内存支持，为开发者、设计人员和研究人员等用户提供更流畅的任务支持；&nbsp;搭载M3 Max 的 14 英寸和 16 英寸 MacBook Pro 提供突破计算极限的性能和功能。配备 M3 Max 的 MacBook Pro 配备强大的 GPU 和CPU，并支持高达 128GB 的​​统一内存，可为机器学习编程人员、3D 艺术家和视频编辑等用户提供跨专业应用程序的极端工作流程和多任务处理；</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e6/e665affb2a1266e3e3d71325948de5e5.png" /></p><p></p><p>M3 系列中的每个芯片都采用统一的内存架构，这是 Apple 芯片的标志。这可提供高带宽、低延迟的良好运行。在定制封装内拥有单个内存池意味着芯片中的所有技术都可以访问相同的数据，而无需在多个内存池之间进行复制，从而进一步提高性能和效率，并减少大多数系统所需的内存量的任务。此外，对高达 128GB 内存的支持解锁了以前在笔记本电脑上无法实现的工作流程，例如人工智能开发人员使用具有数十亿参数的更大变压器模型。</p><p>&nbsp;</p><p>据苹果介绍，基础版的M3包括一个8核CPU、10核GPU、4个性能核心、4个效率核心，支持24GB统一内存和一个外置显示器。M3还拥有 250 亿个晶体管——比 M2 多 50 亿个。它拥有采用下一代架构的 10 核 GPU，图形性能比 M1 快 65%。</p><p>&nbsp;</p><p>M3 Pro具有12核CPU、18核GPU、6个性能核心、6个效率核心，由 370 亿个晶体管组成，GPU 比 M1 Pro 快 40%。对统一内存的支持高达 36GB，使用户能够在外出时在 MacBook Pro 上处理更大的项目。苹果公司表示，M3 Pro单线程性能比 M1 Pro 提升高达 30%。</p><p>&nbsp;</p><p>M3 Max将晶体管数量推至 920 亿个，具有16核CPU、40核GPU、12个性能核心、4个效率核心。苹果公司表示，M3 Max&nbsp;GPU 的速度比 M1 Max 快 50%，并且支持高达 128GB 的​​统一内存，使 AI 开发人员能够使用具有数十亿参数的更大 Transformer 模型。16核CPU拥实现了比M1 Max快80%的性能。</p><p>&nbsp;</p><p>“Apple芯片彻底重新定义了 Mac 体验。其架构的每个方面都是为了性能和能效而设计的。”Apple 硬件技术高级副总裁 Johny Srouji 说道。“凭借 3 纳米技术、下一代 GPU 架构、更高性能的 CPU、更快的神经引擎以及对更统一内存的支持，M3、M3 Pro 和 M3 Max 是迄今为止为个人电脑打造的最先进的电脑芯片。”</p><p></p><h2>相比前两代芯片，M3芯片有哪些升级？</h2><p></p><p>在亮相最新款M3系列芯片之前，M系列芯片采用的是台积电公司的5纳米制程技术，但M3芯片升级采用了台积电最新的3纳米制程芯片技术。更小的节点尺寸对应更高的晶体管密度，有助于提升能效与性能。3纳米芯片将带来高达35%的能效提升，从而延长M系列Mac电脑的电池续航。</p><p>&nbsp;</p><p>苹果芯片代工伙伴台积电也是目前极少数一家能够制造3纳米芯片的厂商之一。有传闻称即便是台积电，目前其最新制程技术的良品率也刚刚超过55%。苹果转向3纳米，也标志着自2020年5纳米M1芯片问世以来进行的首次节点更新，带来了超越当初M2迭代的性能提升。</p><p>&nbsp;</p><p>M3 系列芯片中的下一代 GPU 代表了 Apple 芯片图形架构的最大飞跃。与传统 GPU 不同，它具有动态缓存功能，可以实时分配硬件中本地内存的使用。通过动态缓存，每个任务仅使用所需的确切内存量。</p><p>&nbsp;</p><p>据苹果透露，这项技术是业界首创，对开发人员透明，也是新 GPU 架构的基石。它显着提高了 GPU 的平均利用率，从而显著提高了对GPU要求最苛刻的专业应用程序和游戏的性能。</p><p>&nbsp;</p><p>借助 M3 系列芯片，硬件加速光线追踪首次出现在 Mac 上。光线追踪对光与场景交互时的属性进行建模，使应用程序能够创建极其逼真且物理精确的图像。再加上新的图形架构，专业应用程序的速度可达 M1 系列芯片的 2.5 倍。游戏开发人员可以使用光线追踪来获得更准确的阴影和反射，从而创建深度沉浸式环境。此外，新的 GPU 为 Mac 带来了硬件加速的网格着色，为几何处理提供了更强大的功能和效率，并在游戏和图形密集型应用程序中实现了视觉上更复杂的场景。这一创新的GPU架构实现了所有这些增强功能和功能。事实上，M3 GPU 能够以近一半的功耗提供与 M1 相同的性能，并且在峰值时性能提高高达 65%。</p><p>&nbsp;</p><p>相比于M2系列芯片，M3 也有着显著的提升。下面是两款芯片的规格比较：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9e0e1832bf44452370f6a91b5e0176d.jpeg" /></p><p>此外，在此次发布会上，苹果还推出新款24英寸、搭载M3芯片的iMac，起售价10999元，将于下周上市。</p><p></p><p>参考链接：</p><p><a href="https://www.macrumors.com/guide/m3/">https://www.macrumors.com/guide/m3/</a>"</p><p><a href="https://www.apple.com/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/">https://www.apple.com/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/</a>"</p><p><a href="https://www.apple.com/newsroom/2023/10/apple-unveils-new-macbook-pro-featuring-m3-chips/">https://www.apple.com/newsroom/2023/10/apple-unveils-new-macbook-pro-featuring-m3-chips/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YWara7wpitFei3wUe2L6</id>
            <title>疯狂马斯克的“极限”计划居然成功了？！“下云”后成本降低60%，部分功能代码精简90%，30天急速迁移服务器</title>
            <link>https://www.infoq.cn/article/YWara7wpitFei3wUe2L6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YWara7wpitFei3wUe2L6</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 07:04:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 马斯克, Twitter, 改革, 云服务
<br>
<br>
总结: 马斯克收购了Twitter并进行了大刀阔斧的改革，包括关闭数据中心、优化云服务使用方式、重建服务与排名系统等。他还通过下云等方式削减成本，成功降低了云成本和云数据处理成本。 </div>
                        <hr>
                    
                    <p>2022 年 10 月 27 日，经历了长达半年的拉锯战之后，马斯克终于将 Twitter（现已更名 X）收归囊中，这笔 440 亿美元的收购案也终于迎来了大结局。入主 Twitter 后，马斯克进行了大刀阔斧的改革，如今一年过去了，Twitter 发生了哪些变化？</p><p>&nbsp;</p><p>2023 年 10 月 27 日，X 工程技术发布帖子称，过去一年是 X（Twitter）平台全面推进工程技术探索的一年。除了大家在 X 应用端看到的直观调整之外，团队还在幕后完成了以下一系列重要改进。其中包括：</p><p>&nbsp;</p><p>关闭萨克拉门托数据中心，并重新配置了 5200 台机架和 14.8 万台服务器，每年节约超 1 亿美元。共释放出 48 兆瓦的功率配额、拆除重达 6 万磅的网络梯架，必要设备后续将被重新配置至其他数据中心。优化了 X 的云服务使用方式，着手将更多工作负载迁往本地基础设施。这一转变使 X 每月的云成本降低了 60%。所有媒体/blob 工作均已下云，这让 X 的整体云数据存储量缩减了 60%，还成功将云数据处理成本降低了 75%。</p><p>&nbsp;</p><p>此外，X 还发生了以下变化：</p><p>&nbsp;</p><p>围绕单一产品框架整合了 For you（为您推荐）、Following（关注）、Search（搜索）、Profiles（个人资料）、Lists（列表）、Communities（社区）和 Explore（探索）等技术栈。从头开始全面重建了 For you 服务与排名系统，代码行数从 700K 缩减至 70K，精简比例高达 90%，计算占用量降低 50%，根据请求得分计算的帖子吞吐量增长了 80%。统一了 For you 和视频个性化及排名模型，显著提高了视频推荐的质量。重构了技术栈内的 API 中间件层，通过删除超 10 万行代码和数千个未实际使用的内部端点、清理未采用的客户端服务等方式完成了架构简化。精简后的元数据获取延迟降低了 50%，全局 API 超时错误减少了 90%。阻断 bot 和内容抓取的速度较 2022 年提高了 37%。平均而言，X 每天阻断超 100 万次 bot 注册攻击，并将直接垃圾邮件减少了 95%。构建本地 GPU 超级计算集群，并设计、开发和交付了 43.2 Tbps 的新网络体系架构以支持这些集群。扩展网络主干容量与冗余，每年节约1390万美元。开始进行自动峰值流量故障转移测试，用以持续验证整个平台的可扩展性与可用性。</p><p>&nbsp;</p><p>自接手 X 以来，马斯克为了缩减成本挖空心思，其中包括裁员、推行“极端硬核”企业文化、拖欠办公室租金……在公司的运营开支方面，马斯克去年刚接手&nbsp;X 时便指示团队通过削减云服务和额外的服务器空间，力争每天在基础设施上节省 300 万美元。</p><p></p><h2>省钱大法一：云服务太贵了，马斯克要“下云”</h2><p></p><p>&nbsp;</p><p>2020 年 12 月，Twitter 宣布将使用亚马逊云科技为其主时间线提供支持。当时的消息称这将是一份“多年期”协议，但没有透露任何具体数字。彼时 Twittr 公司 CTO Parwal Agrawal 在一份声明中表示，Twitter 和亚马逊云科技将合作扩展该社交媒体的基础设施、加快功能发布速度，并扩大其功能组合。</p><p>&nbsp;</p><p>据 The Information 2023 年 3 月报道，这笔交易为期五年半，合同总值 5.1 亿美元。根据报道，无论是否使用相应容量，Twitter 都同意向亚马逊云科技付费。而且亚马逊云科技不愿就具体条款进行重新谈判。根据交易细则，Twitter 的月度亚马逊云科技支出大约在 773 万美元。</p><p>&nbsp;</p><p>如今，Twitter 已经不再使用亚马逊云科技的实时时间线功能，转而选择了 AWS for Spaces 等其他服务。Twitter 后续可能使用 Google Cloud Platform（GCP）运行其时间线业务。根据 Twitter 与亚马逊云科技之间签订的合同细节，马斯克执掌的社交媒体巨头还计划使用：</p><p>&nbsp;</p><p>亚马逊云科技云基础设施，用于补充 Twitter 的本地功能，帮助该公司在全球范围内扩展其实时服务。采用 Amazon Elastic Compute Cloud (Amazon EC2)服务中基于 Arm 架构的亚马逊云科技 Graviton 2 实例，以运行其云工作负载。借助亚马逊云科技容器服务，Twitter 将在其混合基础设施当中统一构建并交付新的功能和服务。Amazon CloudFront，即亚马逊云科技的超高速内容交付网络（CDN）服务，能够以低延迟、高速率向全球客户分发数据、应用程序、视频和API。Amazon DynamoDB，即亚马逊云科技的键值数据库，可大规模提供个位数毫秒级性能。</p><p>&nbsp;</p><p>目前，Twitter 已经与谷歌签订了一份价值 10 亿美元的合同，且相关承诺早在与亚马逊云科技合作之前就已敲定。另据报道，Twitter 将在 2023 年向谷歌支付总计 3 亿美元，这也是总价值约 10 亿美元的多年期合作协议的一部分。</p><p>&nbsp;</p><p>随着马斯克入主 Twitter 并开启削减成本计划，Twitter 的基础设施支出大幅减少。根据题为“深度削减成本”的 Slack 内部消息，Twitter 计划从云服务和服务器容量方面入手，省下 150 万到 300 万美元。此外，Twitter 还试图与亚马逊云科技、Google Cloud 以及甲骨文就合同内容展开重新谈判，但供应商们纷纷表示拒绝。</p><p>&nbsp;</p><p>根据最新公告，马斯克通过将工作从云端转移到 Twitter 自己的服务器上，每月的云成本降低了 60%，整体云数据存储量缩减了 60%，还成功将云数据处理成本降低了 75%。</p><p></p><h3>下云就能解决问题？</h3><p></p><p>&nbsp;</p><p>近年来，为了节省成本，不少公司开始下云。不过，并非所有公司都适合下云，需要结合自身实际业务情况来做判断。比如，GitLab 在 2016 年底时候就表示计划要“下云”，不过团队“在收到数百条充满建议和警告的评论和邮件后，最后还是决定将 GitLab.com 保留在云端。</p><p>&nbsp;</p><p>此外，37signals 旗下一款流行的基于云服务的项目管理软件 Basecamp 也曾想“下云”。Basecamp 的上云历程已经超过十年，而且其前两年发布的产品 HEY 也一直在云端运行。但 Basecamp &amp; HEY 联合创始人 David Heinemeier Hansson 发文表示将要“下云”。</p><p>&nbsp;</p><p>“我们用过亚马逊云科技、也用过谷歌云，试过裸虚拟机、也体验了 Kubernetes 容器编排。我们知道云能提供哪些功能，其中大部分都有实际应用。现在我们终于得出结论：对于像我们这样一家增长稳定的中型企业来说，租赁基础设施资源总体上看是笔糟糕的买卖。云服务商做出的降低复杂性、控制运营成本等承诺从来就没能实现，所以我们正在筹划脱离云端、重归本地。”</p><p>&nbsp;</p><p>不过，在 David Heinemeier Hansson 撰写的关于离开云计算的思考中，他特别提到了两个情况是不能离开云计算的。一种是流量极低，一种是复杂不均衡：</p><p>&nbsp;</p><p>第一个极端是当您的应用程序非常简单且流量很低，通过使用完全托管的服务来降低复杂性确实能够节省成本。这是 Heroku 铺就的道路，也是 Render 等其他服务商所追随的道路。当您没有客户时，这是一个绝佳的起点，即使在您开始拥有一些客户后，它仍能推动您的业务发展。（然后，一旦使用量激增，账单飙升到天际线上时，您可能会面临一个好问题，但这是一个合理的权衡。）第二个极端是当您的负载非常不规则时。当您的使用量出现剧烈波动或巨大峰值时。当基线只是您最大需求的一小部分时。或者当您不知道您需要十台服务器还是一百台时。在这种情况下，没有什么比云端更好了，就像我们在推出 HEY 时学到的那样，突然有 30 万用户在三周内注册尝试我们的服务，而我们的预测是六个月内有 3 万用户。</p><p></p><h2>省钱大法二：数据中心大迁移</h2><p></p><p>&nbsp;</p><p>为了节省成本，去年 12 月，马斯克还关闭 Twitter 加州数据中心。</p><p>&nbsp;</p><p>据悉，在平安夜前夕，纳斯克飞往加利福尼亚州的萨克拉门托——Twitter 三大主要计算存储设施之一的所在地——切断了维持该社交网络平稳运行的服务器。有知情人士表示，虽然有员工担心关闭这些服务器可能导致各种问题，但节省资金是首要任务。</p><p>&nbsp;</p><p>随后，世界各地的用户报告 Twitter 服务中断。一些用户反馈 Twitter 出现很多奇怪的错误消息，比如看到空白页面、无法回复推文或关注热门话题，还有人被迫退出登陆。有熟悉 Twitter 基础设施的人士表示，如果萨克拉门托的设施仍在运行，它就可以在其他数据中心出现故障时提供备份计算能力，从而帮助缓解问题。</p><p>&nbsp;</p><p>此外有消息称，当时马斯克为了省钱，计划将萨克拉门托的服务器搬到波特兰，基础设施团队称这项工作至少要九个月才能完成，马斯克一怒之下直接搭乘私人飞机跑去机房，拔了网路线与电源就搬上大卡车开始转移，最后整个工作一个月就完成了。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/da/da556797defc3582b04c0ec1084ba1af.png" /></p><p></p><p>在今年 9 月出版的《埃隆·马斯克传》中，详细讲述了马斯克亲自迁移服务器的故事（节选，经编辑）：</p><p>&nbsp;</p><p></p><blockquote>2022 年 12 月 22 日深夜，位于 X 公司 10 楼的会议室，马斯克正在与两名基础设施经理进行紧张的交谈。&nbsp;位于萨克拉门托的一家数据服务公司允许 X 公司延长其服务器租约，以便在 2023 年有序迁出。一名显得有些紧张的基础设施经理告诉马斯克：“今天早上，他们回来告诉我们说这个计划不再适用，因为他们认为我们在财务上不再稳健。”&nbsp;这个设施每年花费 X 公司超过 1 亿美元。马斯克想通过将服务器迁移到 X 公司在俄勒冈州波特兰的其他设施来节省这笔费用。另一位经理表示这项工作不能立即进行。她平静地说：“我们至少需要六到九个月的时间，因为萨克拉门托仍然需要服务流量。”&nbsp;马斯克沉默了几秒钟，然后宣布：“你们有 90 天时间来完成这项任务。如果你们做不到，你们可以辞职。”这名经理开始详细解释迁移服务器到波特兰的障碍。“机架密度不同，电力密度也不同，”她说。“所以机房需要进行升级。”她开始详细介绍更多原因，但被马斯克打断。“这让我的大脑感到压抑，”马斯克说道，“你知道头爆炸的表情符号吗我的脑袋现在就是这个感觉。真是一堆屁话。波特兰明显有大量的空间，从一个地方迁移到另一个地方简直小菜一碟。”&nbsp;“你们需要做的就是将服务器迁移到波特兰，”马斯克说道，“如果超过 30 天，我会很震惊。”他停顿了一下，重新计算。“找一家搬家公司，运输电脑需要一个星期，然后再花一个星期来连接它们。两周。就应该这样。”&nbsp;所有人都默不作声。但马斯克仍在发火。“如果你们租了一个 U-Haul （一家租车公司），你们可能自己就能完成。”两位 X 公司的经理看着他，试图判断他是否是认真的。马斯克的两位亲密助手 Steve Davis 和 Omead Afshar 也在场。他们多次看到过他这样，知道他可能真的这么认为。&nbsp;12 月 23 日星期五晚上，James 和他的弟弟 Andrew（马斯克的表弟）与马斯克一起从旧金山飞往奥斯汀，当飞机飞过拉斯维加斯时，James 提出了一个建议，他们现在就可以移动服务器。一个名为 Alex 的来自乌兹别克斯坦的 X 员工帮助他们进入了 X 公司的数据中心，内部共有大约 5200 个冰箱大小的机架，每个机架有 30 台电脑。每个机架重约 2500 磅，高 8 英尺。但马斯克认为“这些东西看起来并不难移动”，他向保安借了一把小刀，抬起地板上的一个通风口，这让他可以撬开地板面板。然后他爬到服务器下面，用小刀撬开了一个电箱，拔掉了服务器插头，等着看会发生什么。没什么异常发生。服务器已经准备好迁移。&nbsp;第二天——圣诞前夜，马斯克召集了增援。Ross Nordeen，与他的朋友 James 在 Tesla 工作，从旧金山驱车而来。他在联合广场的 Apple Store 花了 2000 美元，买下了所有的 AirTags，这样服务器在迁移过程中就可以被跟踪。然后他去了家得宝，花了 2500 美元买了扳手、断线钳、头灯和拧下地震螺栓所需的工具。&nbsp;Steve Davis，马斯克的忠诚副手，找人租了一辆半挂车，并安排了搬家车。其他来自 SpaceX 的援助队员也已到达。这些服务器机架都有轮子，所以团队能够断开其中四个并将它们推到待命的卡车上。这表明，这五千两百多个服务器可能在几天内全部移动。 “伙计们干得好！”马斯克兴高采烈地说。&nbsp;到这周结束时，他们已经使用了萨克拉门托所有可用的卡车。尽管该地区受到了雨的袭击，他们在三天内移动了 700 多个机架。该设施之前的记录是一个月移动 30 台。这仍然留下了大量的服务器在设施中，但这群人已经证明了它们可以被快速移动。其余的部分在 1 月份由 X 公司的基础设施团队处理。</blockquote><p></p><p>&nbsp;</p><p>马斯克的疯狂举动引发了不少争议。网友海狗油90认为，“几乎没有人明白数据中心搬迁要搬的是服务、数据，而不是服务器本身，也不明白 X 这样的公司，服务连续性、数据一致性值多少钱。”</p><p>&nbsp;</p><p>网友酷憋哥评论称：“除了证明马斯克胆子大，这个案例没有什么正面的意义，试想一下，哪个普通打工人可以做出这么鲁莽的决定？他或她是否能承担由这种行为导致的严重后果？所以最终只有老板能做这种事情，只要他愿意。”</p><p></p><p>参考链接：</p><p><a href="https://twitter.com/XEng/status/1717754398410240018">https://twitter.com/XEng/status/1717754398410240018</a>"</p><p><a href="https://www.cloudzero.com/blog/twitter-aws">https://www.cloudzero.com/blog/twitter-aws</a>"</p><p><a href="https://twitter.com/thecat/status/1705860673149059115">https://twitter.com/thecat/status/1705860673149059115</a>"</p><p><a href="https://weibo.com/1727858283/NkRTyymTQ">https://weibo.com/1727858283/NkRTyymTQ</a>"</p><p><a href="https://mp.weixin.qq.com/s/7xdSNegYf9zoH7tB8jMDuQ?poc_token=HDYwP2WjN8f7OaFw635HGuh91caCskEz36fJuoqH">https://mp.weixin.qq.com/s/7xdSNegYf9zoH7tB8jMDuQ</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IJWBbzYmh3oYN5Oombay</id>
            <title>对标 FAISS，百度开源自研高性能检索引擎 Puck</title>
            <link>https://www.infoq.cn/article/IJWBbzYmh3oYN5Oombay</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IJWBbzYmh3oYN5Oombay</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 06:17:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度, Puck, 向量检索引擎, 开源
<br>
<br>
总结: 百度宣布开源自研的向量检索引擎Puck，是国内首个适用于超大规模数据集的开源引擎。Puck在个性化推荐系统、多模态检索、自然语言处理等应用场景中发挥重要作用，特别是在处理大规模数据和高维特征数据时。Puck经过多年打磨，在全球向量检索大赛中获得多个第一名。开源Puck旨在促进社区发展，提高代码质量，加速技术创新，更好地适应市场需求。 </div>
                        <hr>
                    
                    <p></p><p></p><p>近日，百度宣布在 Apache 2.0 协议下开源自研检索引擎 Puck，这也是国内首个适用于超大规模数据集的开源向量检索引擎。向量检索算法在个性化推荐系统、多模态检索、自然语言处理等应用场景中都发挥着重要作用，特别是在处理大规模数据和高维特征数据时。</p><p></p><p>名称“Puck”取自经典 MOBA 游戏 DOTA 中的智力英雄 Puck，象征着飘逸和灵动。这个项目经过多年在百度内部的精心打磨，而且在 2021 年底 Nerulps 举办的全球首届向量检索大赛 BIGANN 比赛中，Puck 参与的四个项目均获得第一名。InfoQ 采访了百度搜索内容技术部主任架构师 Ben，以了解该项目的发展历程和核心优势。</p><p></p><p>开源地址：</p><p></p><p><a href="https://github.com/baidu/puck">https://github.com/baidu/puck</a>"</p><p></p><p>InfoQ：是否方便介绍一下您的工作经历，以及目前的主要职责？</p><p></p><p>Ben：我从毕业即加入百度，最初在移动搜索部门，负责基础检索和相关性方面工作，经历了移动高速发展的过程。之后作为创始成员协助组建了多模搜索部负责视觉搜索，属于百度最早一批进入 AI 领域的员工。目前在搜索内容技术部，负责内容相关技术，包括内容获取、内容理解、内容计算、内容加工与生成等。</p><p></p><p>InfoQ：您从什么时候开始关注开源？是什么让您决定 Puck 要开源？选择这个时候开源的原因是什么？</p><p></p><p>Ben：我们很早就在思考开源，看到 FAISS（由 Facebook AI Research 开发的大规模向量检索库）开源之后获得了广泛的业界关注和应用，我们也希望开源 Puck 后，可以促进社区的发展，并借助社区的力量提高代码质量，加速技术创新，更好的适应市场需求。自研开源市场变得越来越成熟和规范，可能会带来更多的商业模式和合作机会。</p><p></p><p>对外开源，我们其实筹备了很久，做了大量的准备工作。大模型的爆火，导致向量检索技术获得广泛关注，我们认为，这是一个合适的开源契机。</p><p></p><p>InfoQ：您能具体讲一下 Puck 在百度的发展史，以及从您角度来看，它对于百度搜索的价值主要体现在哪里？</p><p></p><p>Ben：Puck 的想法最早来自视觉搜索业务，我们需要一个能支撑数百亿相似图片检索的 ANN 引擎，同时要能支持高吞吐、低延时、高准确、低内存、高灵活性等要求，当时业内没有能满足我们需要的引擎，于是启动了自研的过程。</p><p></p><p>2017 年 Puck 完成首次上线，在百亿图片库上成本和效果都取得了极其显著的提升；之后随着 Transformer 模型在 nlp 领域的大放异彩，基于 embedding 的语义检索越来越凸现价值，Puck 的应用也越来越广，2019 年 Puck 在百度内部开源，支撑的业务数快速增长，目前已广泛应用于百度搜索、推荐、网盘、知识图谱等内部多条产品线，支持规模突破万亿。目前 ANN 已经成为互联网底层基础技术之一，是 AI 时代的基石，搜索最重要的支撑技术之一。</p><p></p><p>InfoQ：期间经过了几次优化，优化重点是什么，您能具体讲述一下吗？</p><p></p><p>Ben：到今天 Puck 已经是一个打磨多年的产品，中间的优化数不胜数，大体来说可以分成以下几个阶段：</p><p></p><p>2016 年到 2019 年，打磨核心算法和实现，重点在基础性能优化上，不断调整细节，在自有场景上做极致优化，Puck 的核心框架在这一时期建立并沿用至今。2019 年到 2021 年，以公司内开源为标志，随着业务接入的增多，Puck 需要适配各种各样的应用场景和诉求，易用性、扩展性、功能多样性成为主要目标，像高性能的实时插入、多条件检索、分布式建库等等功能都是在这一时期完成。2021 年到 2022 年，以大规模内容关系计算应用为契机，Puck 重点优化在单实例超大规模数据下的性能，通过大尺度量化和索引结构的优化在十亿规模数据集上大幅提升性能降低成本。以参加全球首届向量检索大赛 BIGANN 并获得四项第一为标志，证明了 Puck 在这部分的竞争优势。2022 年至今，核心算法创新，提出了新的算法来适配不同数据场景，新增更多的 feature，同时完善配套设施，做外部开源准备。</p><p></p><p>这只是一个粗略的划分。实际上，Puck 的优化更多地由许多微小的优化点组成。我们在讨论中提出了大量有趣的想法，进行了大量的实验和尝试。总的来说，十个想法中最终只有一到两个能成为正式的功能。这些优化最终汇聚在一起，形成了我们今天看到的 Puck。</p><p></p><p>InfoQ：您能否详细介绍下 Puck 的核心优势和应用场景？</p><p></p><p>Ben：Puck 开源项目包含了两种百度自研的检索算法和一系列的附加功能，核心优势首先就是性能，经过多年的打磨和调优，在 benchmark 的千万、亿、十亿等多个数据集上，Puck 性能优势明显，均显著超过竞品，在 2021 年底 Nerulps 举办的全球首届向量检索大赛 BIGANN 比赛中，Puck 参加的四个项目均获得第一。</p><p></p><p>其次，易用性上，Puck 提供了一系列的适用于各种场景的功能，比如，同时提供简单易用的 API 接入，尽量少的暴露参数，大部分参数使用默认设置即可达到良好性能。</p><p></p><p>最后，Puck 是一个久经考验的引擎，经过多年在实际大规模场景下的验证打磨，广泛应用于百度内部包括搜索、推荐等三十余条产品线，支撑万亿级索引数据和海量检索请求，可靠性上有非常高的保障。</p><p></p><p>Puck 引擎这次开源了两种检索算法 Puck 和 Tinker，分别更适用于超大规模数据集和中小规模数据集，几乎可以覆盖绝大部分的检索应用场景。目前已广泛应用于百度内部搜索、推荐等多条产品线，覆盖数据规模从百万至万亿。</p><p></p><p>InfoQ：面对 AI 新浪潮，大模型在业内已越来越卷，在您看来未来开源市场会不会更卷？</p><p></p><p>Ben：AI 大模型的出现确实使得业内竞争更加激烈，但这并不是坏事。首先，大模型的发展推动了 AI 技术的进步，提高了 AI 的性能和效率。其次，大模型为业内带来了更多的创新空间和可能性，推动了开源市场的发展。</p><p></p><p>以后业内在自研开源市场的竞争会更加激烈，但这并不意味着会更卷，相反是带来了无限的可能。因为开源市场的特性是开放和共享，企业和个人可以通过开源市场获取最新的 AI 技术和模型，而无需自己从零开始开发。这有助于整个行业降低研发成本和提高研发效率。</p><p></p><p>此外，开源市场也是技术交流和创新的平台，业内人士可以在这里分享自己的研究成果，吸收他人的经验和知识，共同推动 AI 技术的发展。所以，虽然竞争会更激烈，但只要我们能适应这种趋势，积极参与交流和创新，就可以从中获益。</p><p></p><p>InfoQ：那您认为互联网公司开源项目的未来发展趋势是什么样的？会往哪方面发展？</p><p></p><p>Ben：</p><p></p><p>深度专业化：随着技术的细分，开源项目可能会更加专业化和深度化，解决更具体、更深入的问题，会更多永远专注于某一特定问题的开源项目，Puck 就是其中之一。多元化：互联网公司自研的开源项目可能会涉及更多的行业和领域，实现技术的跨界整合，形成各种行业解决方案的开源项目，这种跨界融合将有助于推动技术在各行业的广泛应用。更强的实用性：未来的开源项目可能会更注重实战和应用，而不仅仅是理论研究。开源项目会提供更多实用的工具和框架，帮助开发者更好地将理论应用到实际工作中。注重数据和算法的开源：随着数据和算法的重要性日益凸显，未来可能会有更多的数据和算法开源，以加速 AI 等领域的发展。</p><p></p><p>这些变化都将为推动科技发展和解决实际问题提供更强大的动力。</p><p></p><p>InfoQ：您提到 Puck 在内部已广泛应用，有哪些大家熟悉的产品或场景吗？能否举个例子。</p><p></p><p>Ben：大家熟悉的百度搜索和手机百度内的信息流推荐都有使用 Puck 技术。</p><p></p><p>InfoQ：请问开源后是否收到了社区的一些反馈，对您有怎样的启发？</p><p></p><p>Ben：自从 Puck 开源以来，我们已经收到了不少来自社区的反馈和建议。这些反馈和建议对我们来说是非常宝贵的，它们不仅帮助我们发现了 Puck 的一些问题和不足，也为我们提供了改进和优化的方向。</p><p></p><p>对我个人来说，这些反馈启发我认识到，虽然我们在内部使用 Puck 有着丰富的经验，但在面对更广泛的用户群体时，我们还需要不断学习和提高。每个用户的需求都可能不同，我们需要更加深入地理解用户的需求，才能更好地优化 Puck，使其更加适应不同的使用场景。</p><p></p><p>同时，这些反馈也让我深切地感受到了开源社区的活力和创新精神。许多社区成员不仅提出了问题，还积极地提供了解决方案，这种积极参与和贡献的精神让我深感鼓舞。我希望在未来，我们能够更紧密地与社区合作，共同推动 Puck 的发展。</p><p></p><p>InfoQ：Puck 对您个人的意义，您对 Puck 的未来有什么期待？</p><p></p><p>Ben：Puck 是团队长时间研究和努力的成果，作为 Puck 的负责人，我对这个项目有着深深的热爱和执着，对我个人来说，它不仅仅是一个检索引擎，而是代表团队付出的心血和智慧的结晶，它是我们对技术的追求，对创新的执着，也是我们对未来的期待和憧憬，Puck 的每一次升级和优化都记录着我们的成长和进步。</p><p></p><p>对于 Puck 的未来，我有着很高的期待。首先，我希望 Puck 能在开发者社区中得到广泛的使用，同时也能得到社区的反馈，不断优化和改进。我期待看到更多的人参与到 Puck 的开发和使用中来，通过大家的共同努力，让 Puck 成为 AI 领域有影响力的一款工具。其次，我希望 Puck 能够持续创新，不断优化，保持其技术领先地位，不仅能适应现有的技术需求，还能预见并引领未来的技术趋势。最后，我希望 Puck 能在更多实际应用中发挥出它的价值，为人工智能在各个行业的应用提供强大支撑，推动科技的发展。</p><p></p><p>采访嘉宾简介：</p><p></p><p>Ben，百度搜索内容技术部主任架构师，负责多模态内容理解、超大规模内容关系计算、内容加工与生成、模型优化等方向。</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651185124&amp;idx=1&amp;sn=ea0d371925430a03850c4aac3902b316&amp;chksm=bdb827b78acfaea1c04b0e7bc8df4b7049600841f5bc75d641d35ce7a31c971ad6920cea057d&amp;scene=21#wechat_redirect">“这是一件关于云服务的大事儿！”英特尔 4400 万美元投资基础设施初创公司，硬刚公有云</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651184841&amp;idx=1&amp;sn=b40ceb7de4cec3687f833ff2af20350a&amp;chksm=bdb8269a8acfaf8cfbadd25cdf4ceb314eed3dc188c36af4f549eac2f210e1f599cedc5a627c&amp;scene=21#wechat_redirect">头发丝 1/60 的精度，中国每 10 辆新能源汽车就有 6 辆用这家齿轮</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651184755&amp;idx=1&amp;sn=2d50fceb66679dfaa6e5b9470ba5aee6&amp;chksm=bdb826208acfaf367cc3f8d2cf57a6ec6b9b0c00d731b988812591dde22864ebb3c5545db675&amp;scene=21#wechat_redirect">语雀突发 P0 级事故！宕机 8 小时被网友怒喷，运维又背锅？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651184614&amp;idx=1&amp;sn=b43b9e284546eb0a88e5cd88aac46de4&amp;chksm=bdb825b58acfaca3d677861a4deccc0719767a1de6fc85d33061016eb6017a505b6fff532a73&amp;scene=21#wechat_redirect">智谱 AI“超 25 亿融资”的背后</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H74QyxxT4AT4QfUSVzxE</id>
            <title>高级别自动驾驶时代，如何找到车路协同更优解？</title>
            <link>https://www.infoq.cn/article/H74QyxxT4AT4QfUSVzxE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H74QyxxT4AT4QfUSVzxE</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 02:28:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 边缘计算, 车路协同, 路侧计算单元, 智能交通系统
<br>
<br>
总结: 本文介绍了浪潮信息与百度合作发布的首代车路协同路侧计算单元 RSCU。该产品通过边缘计算技术，提供高性能的算力支持，能够实现车辆与路侧设备的实时信息交互，实现车辆主动安全控制和道路协同管理，从而构建安全、高效和环保的道路交通系统。文章还介绍了车路协同技术的发展历程和应用前景，以及RSCU背后的设计与思考。通过该产品的应用，可以推动边缘计算技术在智能交通系统中的应用，并在其他行业中推广。 </div>
                        <hr>
                    
                    <p>近日，在“以边缘·致多元”边缘计算新品发布暨合作伙伴大会上，浪潮信息联合百度发布首代车路协同路侧计算单元 RSCU。该产品通过系统设计，性能可满足 L2 至 L4 高等级自动驾驶融合应用的算力需求，还支持百度开放、兼容的智路 OS 操作系统连接上层场景，能够在双向 8 车道路口全面感知信号灯、摄像头、激光雷达、路牌路标、气象站等状态，目前已经在北京、武汉等多地部署测试。</p><p></p><p>会后，浪潮信息边缘计算产品线总经理孙波、百度车路协同首席架构师王淼接受了 InfoQ 在内的媒体采访，进一步分享浪潮信息与百度在车路协同方向上的探索与思考。</p><p></p><h2>车路协同：让聪明的车驶向智慧的路</h2><p></p><p></p><p>车路协同是自动驾驶技术发展的关键因素之一，其采用先进的无线通信和新一代互联网等技术，全方位实施车与车、车与行人、车与路等动态实时信息交互，并在全时空动态交通信息采集与融合的基础上，开展车辆主动安全控制和道路协同管理，充分实现人、车、路的有效协同，保证行车安全，提高通行效率，改善交通环境，从而形成的安全、高效和环保的道路交通系统。</p><p></p><p>人们对车路协同的探索最早可以追溯到 20 世纪 50 年代末，当时通用汽车在新泽西州打造了一条埋入大量通信设备的概念高速公路；1990 年代，日本将智能交通系统确立为国家项目；2011 年，中国科技部在 863 计划中设立了智能车路关键技术研究项目，为车路协同技术的发展提供了支持。</p><p></p><p>随着近年来自动驾驶技术加速发展，产业链基础配套和市场开发也越来越成熟，高级别自动驾驶正在加速“上路”。可以预见的是，“车路云”协同发展将成为趋势——不仅需要车辆本身具有很强的车载算力、高精度传感器、操作系统等，还需要加强路侧感知、计算、通信的边缘计算基础设施建设，并能够与边缘云、数据中心云实现多级云边协同。而这也对路侧边缘计算基础设施的性能、存储、可靠性、软硬协同等方面提出了更高的要求。</p><p></p><p>孙波在接受采访时表示，在车路协同系统中，多个摄像头和雷达采集到的数据需要在极低时延内处理并呈现结果。此时，算力便显得至关重要，需要超过 200 TOPS 来支撑整个现场数据的实时处理。</p><p></p><p>“从设备角度出发，性能需求不容忽视。每个摄像头采集的是视频流，每秒产生 30 帧照片。若要做到实时分析，每帧图片经过推理处理以判断车辆位置，需要每秒分析 30 次。若算力无法达到此水平，可能需要进行抽帧处理，即每秒只处理 10 帧或 1 帧，导致算力差异和时延增加。为确保高实时性，需要使用高性能设备进行实时处理。我们跟百度一起在路侧计算单元设备中增加了较强性能的计算单元来支持实时处理。对于通信精度，更着重于设备侧的时钟同步。基于卫星通信的时钟和精度可以达到纳秒级。”</p><p></p><p>要想实现较高的性能，王淼认为需要注意以下两个方面：首先，硬件和软件需要基于高可靠的系统流程进行设计。其次，系统中采用了许多分布式架构。以手机摄像头为例，目前市场上热销的手机可能配备四个摄像头，而每个路口可能包含超过 20 个摄像头。在如此复杂的情况下，如何确保系统的性能？答案是在路侧大脑中建立一个分布式的调动系统，该系统可以并发处理数千个任务，从而确保摄像头的时延。</p><p></p><p>“在系统中，除了 CPU 外，还包括 GPU 和其他各种异构神经网络算力。为了提高性能，我们利用不同的算法逻辑，尤其是最新的神经网络技术。这些技术有助于将计算压力从传统的CPU 中解放出来，从而实现毫秒级的时延。随着人工智能技术的不断发展，主频的提高已经不再像过去那样重要。现在，整个技术栈越来越强调人工智能的计算，以实现更高的性能。”王淼说道。</p><p></p><h2>车路协同路侧计算单元 RSCU 背后的设计与思考</h2><p></p><p></p><p>为了实现让聪明的车驶向智慧的路，浪潮信息携手百度智能云发布首代车路协同路侧计算单元 RSCU。</p><p></p><p>据介绍，针对车路云协同场景下路侧逐渐增加的感知设备，路侧计算单元在算力性能方面进行优化设计，可以最大支持 260 TOPS 的算力，最多可支持双向8车道路口的信号灯、摄像头、激光雷达、路牌路标、气象站等传感器数据传输，面向 L2 至 L4 高级别自动驾驶场景，为“聪明的车”提供更精准的人、车、道路、环境、交通事件的全要素实时检测和分析，并通过车路云的协同，助力智慧城市、智慧交通场景。</p><p></p><p>此外，为保障路侧计算单元的与云端的高效协同，全新路侧计算单元还支持百度开放、兼容的智路 OS 操作系统，可以更好的衔接上层自动驾驶、车路协同应用场景，具有高性能、智能化、开放性、兼容性、协同性、安全性六大特性，全面提升车路云协同效率。</p><p></p><p>目前，百度已经率先在全国多地高等级自动驾驶示范区对该产品进行测试实验，验证了其在自动驾驶到城市交通治理的智能网联全场景服务能力。测试数据显示，基于首代车路协同核心计算单元构建的“感知-计算-通信”路侧边缘智能体系，能够实现对路口范围的人、车、道路、环境、交通事件的全要素实时检测和分析，位置精度≤1.0m（人机非,平均），速度精度≤1.5m/s（均值），交通对象感知定位类型识别准召率≥90%，路侧对象感知端到端时延（含通信时延）≤300ms（均值）。</p><p></p><p>在谈到车路协同路侧计算单元 RSCU 的设计时，孙波表示 RSCU 是目前在路侧方面算力最强的一个产品，需要结合路侧计算的时延和数据处理对于性能的要求，来做产品的整体系统设计。</p><p></p><p>“在这个过程中，我们面临了许多挑战。一个典型的挑战是在路侧环境中放置计算力服务器，这些设备需要应对春夏秋冬、风雨雪雾等各种恶劣环境，包括高温和寒冷。为了解决这个问题，我们可以采用一些算力相对较低的设备，并让它们自身进行宽温设计。例如，EIS200 可以在- 40 ℃到65℃之间正常工作。这款设备的算力约为 200 多 TOPS，虽然已经具有相当大的算力，但这也带来了功耗和散热的挑战。”</p><p></p><p>为了解决这些问题，浪潮信息与百度采取了多种创新方法。对于高功耗设备在路侧环境中的适应性问题，其采用了主动散热方案。当设备的功耗达到 300 瓦时需要进行散热创新，通过隔离散热设计，将服务器中娇贵的器件隔离在内部干净的环境中，并通过第二散热风道与外界进行热交换，由此成功解决了散热问题。</p><p></p><p>“这个联合项目的成功不仅给我们带来了很多技术上的突破和经验，而且对于边缘计算在其他行业的落地也具有重要意义。由于 AI 大模型训练需要大量的算力支持，这些模型需要在边缘侧落地应用。因此，这个项目不仅加速了边缘行业的创新和发展，还对边缘算力提出了更高的要求。随着算力需求的增加，解决环境适应性问题的挑战也会进一步加剧。”</p><p></p><p>孙波认为，基于这个联合项目的知识和成果，可以在相关领域应用边缘计算技术，例如水利、高速、制造、能源、电网巡检等等。这些应用可以快速复制到其他许多行业中，为产业的落地提供助力。“未来，我们将继续积极应对挑战，为边缘计算在其他行业的落地提供更多支持，并不断推动技术的发展和创新。”</p><p></p><p>在智慧交通领域，除了车路协同路侧计算单元 RSCU，浪潮信息与百度还合作让其适配了名为智路 OS 的生态系统，这也是由工信部指导认证的路侧操作系统生态。王淼提到，“未来的路侧会像现在的智能车一样，形成一个类似的生态系统。这个生态系统最终将包含两个关键的核心零部件，即芯片和操作系统。在这两个领域，手机和车方面稍显落后，但在道路方面，我国已经提早布局并看得更远。”</p><p></p><p>随着更多参与方加入，以及芯片和操作系统的进一步发展，整个生态系统将会更加完善和强大，并为智慧交通带来更多的无限可能。</p><p></p><h2>边缘计算将走向怎样的未来？</h2><p></p><p></p><p>AIGC 大模型的飞速发展为边缘计算业务带来了新的创新。然而，边缘计算基础设施也将面临更大的挑战。孙波认为，未来边缘计算的发展方向将面临三大难题：</p><p></p><p>首先，环境适应是边缘计算设备面临的一个重要问题。随着算力不断增加，设备的功耗也会随之提高。为了确保设备的稳定运行，需要采取更为先进的散热和环境适应手段。例如，针对未来算力提升至更高数量级的情况，需要研究更为高效的散热方式和适应各种环境下的产品设计。</p><p></p><p>其次，算力支撑是边缘计算设备的另一个重要发展方向。随着智能化和系统化的决策分析需求不断提升，边缘计算设备需要更大的算力支持。未来，边缘计算设备将朝着大算力方向发展，以更好地满足各种复杂任务和系统性的决策分析需求。</p><p></p><p>最后，安全是边缘计算面临的另一个重要挑战。与数据中心服务器相比，边缘计算设备部署在更加复杂和恶劣的环境中，需要直接面对公网安全挑战。因此，未来需要研究如何提高边缘计算设备的安全性能，以及如何实现设备的智能化运维和故障自恢复等功能。</p><p></p><p>整体而言，未来边缘计算的发展将朝着环境适应、大算力支撑和安全保障等三个方向发展。在这个过程中，需要不断研究新的技术和方法，以提高边缘计算的易用性、可靠性和维护性，更好地满足行业需求。</p><p></p><p>而要想实现边缘计算的规模化落地，关键不仅仅在于简单地拥有一个边缘计算服务器。整个产业链的协同也至关重要。</p><p></p><p>“我们需要与合作伙伴共同研究、打破限制，以推动场景的落地。这也是我们认为边缘计算要实现规模化落地所必须重视的路径。未来，我们将继续与百度等合作伙伴围绕边缘场景进行深入研究，打磨场景方案并推动其落地。在这个过程中，我们通过不断的迭代和发展，逐渐走向一个新的阶段。在众多边缘场景中，我们发现城市治理和交通是具有明确需求且非常大的场景。”孙波认为，未来的城市将是智慧化的，交通更应该如此。只有实现了智慧化的交通，才能真正解决道路交通拥堵的问题，因此，行业需要结合未来的趋势来思考如何使道路更加智能化。</p><p></p><p>“在未来的规模化落地过程中，我们将不断打磨场景和硬件设备，使其更加适用于业务场景。同时，我们相信百度也将不断迭代和优化其上层软件平台，推出更新的技术和更好的方案，共同推动设备的不断完善。”孙波说道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ndi571huMEScbH9hcrar</id>
            <title>智谱AI张鹏谈大模型进展和挑战，在CNCC会议上推出第三代基座大模型ChatGLM3</title>
            <link>https://www.infoq.cn/article/Ndi571huMEScbH9hcrar</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ndi571huMEScbH9hcrar</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 01:51:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 通用智能, 跨模态融合, 认知能力
<br>
<br>
总结: 在2023年10月27日的沈阳CNCC中国计算机大会上，大模型成为了焦点，各个领域围绕大模型展开讨论。大模型的进展主要体现在通用智能的提升，通过整合感知能力、推理能力和跨模态对齐能力，形成更强大的认知级别能力。跨模态融合的能力是最受关注的，通过训练方法将多模态数据融合，提升大模型的智能水平。同时，大模型在研发和应用中面临算力、数据、算法和应用安全等挑战。为了解决内容审核问题，可以借鉴互联网和社交媒体行业的经验，采用人机融合或人机交互的方式提高工作效率和内容安全性。智谱AI在会上推出了第三代大模型ChatGLM3及相关产品，通过多阶段增强预训练方法提升训练效果，性能更强大。ChatGLM3具备多模态理解能力、代码生成和执行能力、网络搜索增强等新功能，语义和逻辑能力得到增强。此外，智谱AI还推出了具备代码交互能力的大模型产品智谱清言，支持图像处理、数学计算和数据分析等使用场景。 </div>
                        <hr>
                    
                    <p>在2023年10月27日的沈阳CNCC中国计算机大会上，大模型已经成为了大会议题的焦点，各个领域都在围绕大模型展开讨论。</p><p></p><p>在27日上午的“大模型的研究进展与产业应用展望”论坛，由CCF副秘书长谭晓生主持，德国国家工程院院士张建伟、复旦大学计算机学院教授邱锡鹏、智谱AI CEO张鹏、科大讯飞研究院院长刘聪、蚂蚁集团副总裁徐鹏等专家参与讨论的圆桌交流环节也取得了丰富的成果，专家从各自的视角分享了大模型的进展、挑战以及未来的问题。以下整理智谱AI CEO张鹏老师的部分观点。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/3e/80/3e64309742f0047dbee5b8ab84c1af80.png" /></p><p></p><p>关于大模型领域的进展，张鹏认为，目前大模型的进展可以归结为通用智能，即基础模型的通用智能水平的提升。上一代人工智能大多数还是单向的感知能力。而大模型最大的优势是能把这些感知能力整合起来，形成一个更泛化的、更强大的认知级别的能力。这其中就包括推理能力、复杂问题的拆解能力，以及跨模态对齐能力。</p><p></p><p>最受关注的其实就是跨模态融合的能力，经过实践后发现多模态或者跨模态的数据可以通过训练的方法完美的融合到一起，在一定程度上提升了大模型的智能水平。</p><p></p><p>另外，基于认知能力的提升，可以观察到像智能体 Agent 这一类的研究，确实能够极大地增强大模型在实际应用当中的效果，让大模型从搭配 Benchmark 的实验环境走入到真正的应用当中，来解决实际的应用问题，这在张鹏看来是让人欣喜的进展。</p><p></p><p>大模型在研发和应用过程中也会遇到不少的挑战。张鹏认为，除了算力和数据方面的挑战之外，在算法方面也同样有挑战，当前所有的大模型都基于2017年提出的Transformer架构，未来是否会被改进或被新的东西代替也是大家关心的问题。另外张鹏考虑更多的另一个挑战是应用安全问题，包括私有数据训练等，首先要考虑的就是安全。</p><p></p><p>关于产出的内容审核的解决办法，大模型产出的内容在提供给用户之前，对于所提供的内容审核问题也是很重要的。张鹏说，首先平台要保证尽量不要传递错误的讯息，其次是为了达到这个目的，可以借鉴已有的多年的经验，例如人机融合或者人机交互是提升工作效率的有效方式之一。通过借鉴互联网、社交媒体等行业的经验，可以降低人工成本，并保证内容的安全性。</p><p></p><h3>智谱 ChatGLM3 以及相关系列产品发布</h3><p></p><p></p><p>在此次 CNCC 会议上，智谱AI推出了自主研发的第三代基座大模型ChatGLM3以及相关系列产品。这是继智谱AI推出千亿基座的对话模型ChatGLM和ChatGLM2之后的又一重大突破。</p><p></p><p>此次推出的 ChatGLM3 采用了独创的多阶段增强预训练方法，使训练更为充分。评测显示，在 44 个中英文公开数据集测试中，ChatGLM3 在国内同尺寸模型中排名首位。智谱 AI CEO 张鹏在现场做了新品发布，并实时演示了最新上线的产品功能。</p><p></p><h3>ChatGLM3全新技术升级 更高性能更低成本</h3><p></p><p></p><p>通过更丰富的训练数据和更优的训练方案，智谱AI推出的ChatGLM3性能更加强大。与ChatGLM2相比，MMLU提升36%、CEval提升33%、GSM8K提升179% 、BBH提升126%。</p><p></p><p>同时，ChatGLM3瞄向GPT-4V本次实现了若干全新功能的迭代升级，包括多模态理解能力的CogVLM-看图识语义，在10余个国际标准图文评测数据集上取得SOTA；代码增强模块Code Interpreter根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；网络搜索增强WebGLM-接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。ChatGLM3的语义能力与逻辑能力得到了极大的增强。</p><p></p><p>ChatGLM3还集成了自研的AgentTuning技术，激活了模型智能体能力，尤其在智能规划和执行方面，相比于ChatGLM2提升了1000% ；开启了国产大模型原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理、操作系统等复杂场景。</p><p></p><p>此外，ChatGLM3本次推出可手机部署的端测模型ChatGLM3-1.5B和 ChatGLM3-3B，支持包括 vivo、小米、三星在内的多款手机以及车载平台，甚至支持移动平台上CPU芯片的推理，速度可达20 tokens/s。精度方面1.5B和3B模型在公开Benchmark上与ChatGLM2-6B模型性能接近。</p><p></p><p>基于最新的高效动态推理和显存优化技术，ChatGLM3当前的推理框架在相同硬件、模型条件下，相较于目前最佳的开源实现，包括伯克利大学推出的 vLLM 以及 Hugging Face TGI的最新版本，推理速度提升了2-3倍，推理成本降低一倍，每千tokens仅0.5分，成本最低。</p><p></p><h3>新一代“智谱清言”上线 &nbsp;国内首推代码交互能力</h3><p></p><p></p><p>在全新升级的ChatGLM3赋能下，生成式AI助手智谱清言已成为国内首个具备代码交互能力的大模型产品（Code Interpreter）（<a href="https://chatglm.cn/main/code">https://chatglm.cn/main/code</a>"）。“代码”功能目前已支持图像处理、数学计算、数据分析等使用场景。</p><p></p><p>随着WebGLM大模型能力的加入，智谱清言也具有了搜索增强能力，可以帮助用户整理出相关问题的网上文献或文章链接，并直接给出答案。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7d/f7/7d8bb050b2d550f323a7f6b606df01f7.png" /></p><p></p><p>此前已发布的CogVLM 模型则提高了智谱清言的中文图文理解能力，取得了接近GPT-4V的图片理解能力,它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/30/a9/303865200b30fc355285fae7a60380a9.png" /></p><p></p><p>自2022年初，智谱AI推出的GLM系列模型已支持在昇腾、神威超算、海光DCU架构上进行大规模预训练和推理。截至目前，智谱AI的产品已支持10余种国产硬件生态，包括昇腾、神威超算、海光DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。通过与国产芯片企业的联合创新，性能不断优化，将有助于国产原生大模型与国产芯片早日登上国际舞台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/47a447d222962fce01b6c1f09bbbfdb9.png" /></p><p></p><p>智谱AI此次推出的ChatGLM3及相关系列产品，全面提升了自身的模型性能，为业界打造了更开放的开源生态，并进一步降低了普通用户使用AIGC产品的门槛。AI正在引领我们进入一个新的时代，大模型必将加速这一时刻的到来。</p><p></p><h3>【活动推荐】</h3><p></p><p></p><p>在 2023 年 12 月 28-29 日，InfoQ 将在上海举办<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon全球软件开发大会</a>"，这个会议上结合当前的趋势热点，设置了 GenAI 和通用大模型应用探索、AI Agent 与行业融合应用的前景、LLM 时代的性能优化、智能化信创软件 IDE、LLM 时代的大前端技术、高性能网关设计、面向人工智能时代的架构、高效的编程语言、性能工程、LLM 推理加速和大规模服务、现代数据架构演进、建设弹性组织的经验传递、SaaS 云服务弹性架构设计等专题，目前也正在邀请业界的专家来会议上演讲。感兴趣的可以点击<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon会议官网</a>"，查看详细的介绍，也欢迎您来会议上演讲，分享技术实践。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/urmClYMAW106DweFJNo4</id>
            <title>程序员篡改ETC余额，一年私吞260余万元；语雀公布故障原因及赔偿方案；各家财报发布，创始人们：就很难受｜Q资讯</title>
            <link>https://www.infoq.cn/article/urmClYMAW106DweFJNo4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/urmClYMAW106DweFJNo4</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 00:05:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员篡改ETC余额, 英伟达, AMD, SiFive
<br>
<br>
总结: 一位程序员篡改了ETC余额并私吞了260余万元。英伟达面临国内厂商无法下单的问题，国内厂商开始寻求AMD作为替代方案。芯片设计初创公司SiFive裁员20%，AMD成为甲骨文和IBM的选择。 </div>
                        <hr>
                    
                    <p></p><blockquote>&nbsp;程序员篡改ETC余额，一年私吞260余万元；国内厂商已无法从英伟达下单，寻求国内替代成唯一方案；苦英伟达“一家独大”久矣？甲骨文、IBM 下单 AMD；芯片设计初创公司SiFive裁员20%，此前估值25亿美元；AMD回应大幅裁员：小幅优化和调整；宿华辞任快手科技董事长，CEO程一笑兼任；消息称张一鸣通知负责人：PICO业务看不到希望将关停，字节人士否认；财报一发，没有一个创始人能笑着面对；国家数据局正式揭牌；故障超过 8 小时，语雀公布原因及赔偿方案；小米正式发布小米澎湃OS；华为：全面完成 5G-A 技术性能测试；Mojo 编程语言发布 Mac 版本；Python 公布了实现 no-GIL Python 的计划……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>程序员篡改ETC余额，一年私吞260余万元</h4><p></p><p></p><p>2023年9月，上海市公安局浦东分局北蔡派出所接到某科技公司员工张女士报案称，其公司发现计算机系统被他人篡改数据，导致公司账户钱款损失。民警随即展开工作，最终嫌疑人曹某迫于压力，主动投案自首。</p><p>&nbsp;</p><p>2022年8月，曹某发现所在公司的网站后台有漏洞，身为软件工程师的他，决定铤而走险，用其母亲的身份证自行注册了一个ETC账户，并绑定了其母亲的银行卡。随后曹某以每周4至5次、每次1万元的频率，陆续从该账户内提取了230余万元。之后，曹某又利用朋友的身份证再次办理账号，以同样的方式再次从公司提现36万元。</p><p>&nbsp;</p><p></p><h4>国内厂商已无法从英伟达下单，寻求国内替代成唯一方案</h4><p></p><p>&nbsp;</p><p>10月25日，英伟达在向美国证券交易委员会（SEC）递交的一份文件中披露，美国政府通知公司，针对中国更新的“先进计算芯片和半导体制造设备出口管制规则”立即生效，中国云厂商、服务器厂商、销售代理商均已无法从英伟达下单。“出口管制规则”刚出炉时，原定有30天公示期，被行业人士视为“最后30天窗口期”。在窗口期内，中国企业原本可以集中采购、运输急需的高端AI芯片。美国芯片企业出于对中国市场的依赖，理论上也会和中国企业打配合。</p><p>&nbsp;</p><p>按照美国政府最新的要求，“综合性能达到4800或以上，并且是为数据中心设计或销售的产品”将需要“立即停止出口”。NVIDIA给出的说明是，公司A100、A800、H100、H800和L40S产品的发货将立即受到影响。一位云厂商高管表示，美国“出口管制规则”步步紧逼的情况下，规模化采购国产芯片是培育本土产业链的唯一路径。</p><p>&nbsp;</p><p></p><h4>苦英伟达“一家独大”久矣？甲骨文、IBM 下单 AMD</h4><p></p><p>&nbsp;</p><p>因英伟达 GPU 供应紧张，甲骨文与 IBM 转向 AMD 产品。甲骨文计划使用 AMD Instinct MI300X AI 芯片及 HPC 用GPU；IBM 可能采用 AMD的Xilinx FPGA 解决方案。AMD Instinct MI300X 今年 6 月发布，提供强大性能，预计 2024 年供应充足。AMD 拥有足够芯片零部件，可以支撑 MI300 在四季度发布。AMD 的 FPGA 产品线因具有更低的功耗和时延，在AI推理中具有优势。</p><p>&nbsp;</p><p></p><h4>芯片设计初创公司SiFive裁员20%，此前估值25亿美元</h4><p></p><p>&nbsp;</p><p>10月25日消息，当地时间周二美国芯片设计初创公司SiFive表示，公司已裁员约20%，约130人。SiFive总部位于美国加州圣克拉拉，芯片设计均基于RISC-V技术架构，该公司竞争对手是最近上市的英国芯片设计公司Arm。和Arm一样，SiFive的工作专注于芯片底层设计，而不是芯片本身。</p><p>&nbsp;</p><p>SiFive在一份声明中表示：“随着我们发现并专注于最大机会，公司正对所有全球团队进行战略重新调整，为的是更好满足客户快速变化的需求。”SiFive发言人大卫·米勒（David Miller）表示，这次裁员涉及公司所有部门，其中也包括高管团队。他强调，公司的产品线不变。</p><p>&nbsp;</p><p></p><h4>AMD回应大幅裁员：小幅优化和调整</h4><p></p><p>&nbsp;</p><p>近日有传闻称，AMD在中国区大规模裁员。就此，AMD官方回应称：“网络传闻失实。基于公司战略的调整，公司近期对组织架构进行了小幅度的优化和重组。”</p><p>&nbsp;</p><p>同时了解到，为顺应市场的变化，AMD中国区还在为重点领域的业务继续开展招聘。根据此前发布的财报数据，AMD今年第二季度收入54亿美元，环比基本持平，净利润2700万美元，环比增长119％，总收入和调整后每股收益均超出华尔街分析师预期。</p><p>&nbsp;</p><p></p><h4>宿华辞任快手科技董事长，CEO程一笑兼任</h4><p></p><p>&nbsp;</p><p>10月20日晚间，快手科技在港交所发布公告宣布，由于需要专注其他事务，自2023年10月29日起，宿华不再担任董事会董事长，将继续担任执行董事和薪酬委员会成员，其不同投票权不会发生变化，董事长一职由程一笑接任。</p><p>&nbsp;</p><p>两年前的10月29日，快手科技宣布宿华和程一笑调整分工，宿华辞去首席执行官一职，继续担任董事长、执行董事、薪酬委员会委员，负责制定公司长期战略；程一笑出任首席执行官，负责公司日常运营及业务发展。</p><p>&nbsp;</p><p></p><h4>消息称张一鸣通知负责人：PICO业务看不到希望将关停，字节人士否认</h4><p></p><p>&nbsp;</p><p>10月21日消息，有媒体报道称PICO业务将被逐步关停，字节跳动放弃元宇宙。文章引述相关人士的说法称，PICO 负责人近期前往新加坡找张一鸣汇报工作，得到的反馈是字节跳动将逐步放弃 PICO 业务，并称原因是“PICO 所处的硬件领域非字节跳动所擅长，几年下来成绩未达预期、并且看不到未来的希望”。</p><p>&nbsp;</p><p>对此，字节跳动相关负责人向媒体回应称，此消息不实。PICO在正常运营，公司会长期投入XR业务。</p><p>&nbsp;</p><p></p><h4>财报一发，没有一个创始人能笑着面对</h4><p></p><p>&nbsp;</p><p>马斯克在特斯拉财报会上表现像“小婴儿”</p><p>&nbsp;</p><p>金融分析师兼 YouTube 博主凯文・帕夫拉斯（Kevin Paffrath）透露，在特斯拉糟糕的财报电话会议上，其首席执行官埃隆・马斯克（Elon Musk）表现得像个“小婴儿”，几乎要哭出来！Paffrath说：“对于一位公司的领导者来说，抱怨经济形势而不是提出应对计划，这似乎是可悲的！”</p><p>&nbsp;</p><p>据悉，马斯克在电话会议上一度暗示，由于利率上升，借贷成本更高，他将推迟工厂的建设。他说：“如果利率保持在高位，甚至更高，人们购买汽车就会变得更加困难。他们根本负担不起买车的开支。”但Paffrath抨击了马斯克的回应，称这位特斯拉首席执行官“害怕了”，并建议马斯克应该与墨西哥政府谈判达成更好的协议，或者可能“向高收入地区打广告”。Paffrath此前曾呼吁特斯拉向非粉丝推广其产品。</p><p>&nbsp;</p><p>谷歌市值大跌8500亿，云业务Q3收入不及预期</p><p>&nbsp;</p><p>10月25日，谷歌母公司Alphabet发布了截至9月30日的2023财年第三季度财报。财报显示，Alphabet第三季度营收为766.93亿美元，较上年同期的690.92亿美元增长11%，按固定汇率计算同比增长11%；净利润为196.89亿美元，较上年同期的139.10亿美元增长42%。</p><p>&nbsp;</p><p>Alphabet第二季度营收和每股收益均超出分析师一致预期，但是云业务营收不及预期。由于云业务对于Alphabet未来增长至关重要，它的营收不及预期引发投资者担忧，拖累股价在盘后交易中大跌6.65%，市值蒸发1164亿美元(约合8510亿元人民币)。</p><p>&nbsp;</p><p>微软CEO今年薪酬降低11.6%，承认放弃Windows Phone是错误决定</p><p>&nbsp;</p><p>10月25日，微软发布了 2024 年第 1 财季（截至 2023 年 9 月 30 日）财报，其中显示今年微软 CEO 萨蒂亚・纳德拉（Satya Nadella）基于绩效的薪酬有所降低，并且不再与微软 XGP 业务增长情况挂钩。外媒认为这是因为微软 XGP 用户增长连续两年未达到预期目标。</p><p>&nbsp;</p><p>此外，纳德拉在接受媒体采访时候，还承认放弃 Windows Phone 和移动设备是错误决定，这也是微软历史上第三位承认在移动领域犯错的首席执行官。</p><p>&nbsp;</p><p>纳德拉在2014年接替鲍尔默（Steve Ballmer）担任CEO，仅仅一年之后就将鲍尔默任内斥资74亿美元收购的诺基亚手机业务勾销。纳德拉接受采访时候表示微软“退出”手机业务本应该处理的更好。勾销诺基亚手机业务之后，Windows Phone事实上就退出了移动舞台。微软后来推出了运行Android的 Surface Duo和Surface Duo 2智能手机，但由于没有后续产品，也缺乏软件更新，Surface Duo手机品牌的未来悬而未决。</p><p>&nbsp;</p><p>扎克伯格豪赌元宇宙巨亏271亿元，明年AI将成Meta最大投资领域</p><p>&nbsp;</p><p>10月26日，脸书母公司Meta发布了截至 9 月 30 日的 2023 财年第三季度财报。财报显示，Meta 第三季度总营收为 341.46 亿美元，较上年同期的 277.14 亿美元增长 23%；净利润为 115.83 亿美元 (约合 847.55 亿元人民币)，较上年同期的 43.95 亿美元增长 164%。</p><p>&nbsp;</p><p>Meta 创始人马克・扎克伯格 (Mark Zuckerberg) 大力押注的元宇宙业务依旧在“流血”。第三季度，Meta 负责元宇宙业务的现实实验室部门再次营业亏损 37 亿美元 (约合 271 亿元人民币)。对于公司的后续发展，扎克伯格表示在2024年，就工程和计算资源而言，AI将成为Meta最大的投资领域。此外，扎克伯格补充道，为了避免布置大量的新员工，公司将降低一些非AI项目的优先级，并将相关人员转向从事AI工作。</p><p>&nbsp;</p><p>科大讯飞净利大跌，创始人套现25亿</p><p>&nbsp;</p><p>据科大讯飞财报显示，今年第三季度，科大讯飞的营收同比实现2.89%的增长，但公司前三季度的营收却小幅下跌0.37%。利润方面，科大讯飞第三季度、前三季度归属于上市公司股东的净利润分别减少81.86%、76.36%。而前三季度，公司扣除非经常性损益的利润为-3.24亿元。</p><p>&nbsp;</p><p>此次发布的财报中，科大讯飞还提到，截至7月3日，公司2022年7月3日通过的股份回购期限已届满。不过，公司大手笔回购的同时，身为科大讯飞创始人和董事长的刘庆峰却在第三季度减持了公司的股份。若按8月14日科大讯飞63.98元/股的收盘价计算，刘庆峰此次减持预计将套现超25亿元。对此，科大讯飞在公司发布的公告中解释称，此前刘庆峰曾通过质押融资等方式借款筹集资金23.5 亿元，鉴于债务已到期，刘庆峰需要减持股份用于偿还上述借款本金。</p><p>&nbsp;</p><p>另外，因为学习机出现违背主流价值观内容，并引发科大讯飞股价午后跳水触及跌停后，科大讯飞董事长刘庆峰表示，问题出现后，已经第一时间把大模型的安全审核能力放进来，同时也跟公安报备了相应情况。他同时感慨，“中国的创新不容易，我们今天刚发布了星火大模型最新版本，但负面舆情却铺天盖地，这背后是有推手的。”</p><p>&nbsp;</p><p></p><h4>国家数据局正式揭牌</h4><p></p><p>&nbsp;</p><p>10月25日上午，国家数据局正式揭牌。国家数据局负责协调推进数据基础制度建设，统筹数据资源整合共享和开发利用，统筹推进数字中国、数字经济、数字社会规划和建设等，由国家发展和改革委员会管理。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p></p><h4>故障超过 8 小时，语雀公布原因及赔偿方案</h4><p></p><p>&nbsp;</p><p>10&nbsp;月 23 日消息，据多位用户反馈，蚂蚁集团旗下的在线文档编辑与协同工具语雀在 23 日 14:00~15:00&nbsp;之间出现大规模服务器故障，在线文档和官网目前均无法打开。在经历了近 10&nbsp;小时的故障之后，语雀服务现已全部恢复正常，各端语雀都可以正常访问，功能也恢复。</p><p>&nbsp;</p><p>10月24日晚，蚂蚁集团旗下在线文档编辑与协同工具语雀就前一日持续7个多小时的重大服务故障致歉，并公布故障原因及赔偿方案。语雀方面表示，10月23日下午，服务语雀的数据存储运维团队在进行升级操作时，由于新的运维升级工具bug，导致华东地区生产环境存储服务器被误下线。语雀将向所有受到故障影响的用户提供赔偿，针对语雀个人用户将赠送6个月的会员服务，针对语雀空间用户会单独制定赔偿方案。语雀方面强调，用户所有数据均未丢失。</p><p>&nbsp;</p><p>更多详情可以查看：</p><p><a href="https://mp.weixin.qq.com/s/LOjiaULzEgkI5VEe74kX0g">语雀突发 P0 级事故！宕机 8 小时被网友怒喷，运维又背锅？</a>"</p><p>&nbsp;</p><p></p><h4>小米正式发布小米澎湃OS</h4><p></p><p>&nbsp;</p><p>10月26日，在小米澎湃OS暨小米14系列新品发布会上，小米董事长雷军发表演讲。雷军表示，小米集团宣布全新战略升级：从手机 X AIo，升级到人车家全生态。</p><p>&nbsp;</p><p>而小米澎湃OS也正式亮相。雷军表示，他对澎湃OS提出了五个要求：一、每个独立设备能实现最佳性能表现；二、更加便捷高效的跨端连接；三、成为生态智能大脑，为用户提供主动智能服务；四、实现跨设备全系统隐私安全的坚固防护；五、坚持建设开放生态。</p><p>&nbsp;</p><p></p><h4>华为：全面完成 5G-A 技术性能测试</h4><p></p><p>&nbsp;</p><p>近日，华为全面完成5G-A技术性能测试。华为方面介绍称，5G-A作为5G的演进和增强，连接速率和时延等传统网络能力实现了10倍提升，同时引入了通感一体、无源物联、内生智能等全新的革命性技术。</p><p></p><h4>Python 公布了实现 no-GIL Python 的计划</h4><p></p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/XahMWSZLXwqYo6TWKDvg?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Python </a>"指导委员会宣布接受 PEP 703（Making the Global Interpreter Lock Optional，让全局解释器锁成为可选），公布了实现 no-GIL（或称为自由线程）Python 详细的路线图。</p><p>&nbsp;</p><p>Python 的全局解释器锁（GIL）阻止了同时多线程执行代码，成为了在多核 CPU 上提高 Python 代码运行效率的一大障碍，消除这一障碍是好事，但这也有可能会破坏现有的扩展模块，或显著降低性能以及可维护性。而第三方软件包生态系统是 Python 的一大优势，Python 项目在实现自由线程时需要谨慎，需要避免破坏这一优势。推进 PEP 703 需要将其纳入主线，作为定期发布版本的一部分推出。Python 指导委员计划分成三个阶段：实验阶段，支持但不默认阶段，默认阶段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kVVkXrWbX7CVWPR7Y6Fe</id>
            <title>程序员利用漏洞篡改ETC余额，一年私吞260余万元；小马智行获沙特1亿美元投资；AMD回应“中国区大幅裁员” | AI一周资讯</title>
            <link>https://www.infoq.cn/article/kVVkXrWbX7CVWPR7Y6Fe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kVVkXrWbX7CVWPR7Y6Fe</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, OpenAI, Anthropic, 人工智能
<br>
<br>
总结: 谷歌同意向OpenAI竞争对手Anthropic投资20亿美元，加剧了人工智能领域的竞争，以争取下一个重大突破。 </div>
                        <hr>
                    
                    <p></p><blockquote>传谷歌同意向OpenAI竞争对手Anthropic至多投资20亿美元；AMD回应裁员传闻：系公司组织架构小幅度优化和重组；小米正式发布澎湃OS，雷军确认小米汽车明年上半年上市；腾讯混元大模型正式开放“文生图”功能，代码能力大幅提升 20%……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>传谷歌同意向OpenAI竞争对手Anthropic至多投资20亿美元</h4><p></p><p></p><p>10月28日消息，据知情人士透露，谷歌已经同意在此前投资的基础上，再向OpenAI竞争对手Anthropic至多投资20亿美元。此举可能促使人工智能领域的初创公司加剧竞争，以争取首先取得下一个重大突破。</p><p></p><p>知情人士说，谷歌同意先期向Anthropic投资5亿美元，并同意随着时间的推移再增加15亿美元。在这笔投资之前，亚马逊也曾于上个月承诺向Anthropic投资40亿美元。Anthropic由前OpenAI工程师于2021年创立，目标是开发能与GPT-4竞争的生成式人工智能模型。</p><p></p><h4>花旗计划为其4万多名程序员部署生成式AI</h4><p></p><p></p><p>据报道，随着华尔街继续拥抱人工智能这项新兴的技术，花旗集团（Citigroup）计划为其4万多名程序员中的绝大部分使用生成性人工智能（GAI）。作为小型试点项目的一部分，花旗集团开始允许大约250名开发人员率先体验生成式人工智能。明年，花旗集团计划将该计划扩展到绝大多数程序员。</p><p></p><h4>马斯克：X要与YouTube和LinkedIn竞争</h4><p></p><p></p><p>据知情人士称，X平台所有者埃隆·马斯克和该公司CEO琳达·亚卡里诺周四表示，他们将YouTube和LinkedIn视为未来的竞争对手，同时在视频和招聘领域寻求新的业务线。</p><p></p><p>消息人士称，X周四举行了一次全体会议，以纪念马斯克收购推特一周年。在会上，马斯克和亚卡里诺提到了这两个网站。该知情人士说，两人还提到了创建名为XWire的新闻服务的雄心，该服务将与Cision的美通社（PR Newswire）竞争。该知情人士要求不具名，因为讨论是私人的。</p><p></p><p>这位知情人士说，这次会议是马斯克和亚卡里诺首次共同向整个公司发表讲话。亚卡里诺于今年5月被聘为X的CEO，此前她在NBC环球负责广告和合作。</p><p></p><h4>OpenAI组建新团队以评估AI的“灾难性风险”</h4><p></p><p></p><p>美国当地时间周四，人工智能研究公司OpenAI宣布组建新团队，以评估和减轻与人工智能相关的“灾难性风险”。OpenAI在周四的声明中表示，这个新团队名为Preparedness，其主要任务是“跟踪、评估、预测和保护”人工智能造成的潜在重大问题，包括核威胁。此外，该团队将致力于减轻“化学、生物和放射性威胁”，以及人工智能的“自主复制”行为。Preparedness团队将解决的其他风险包括人工智能欺骗人类的行为，以及网络安全威胁。</p><p></p><p>OpenAI在更新中写道：“我们相信，前沿人工智能模型的能力将超越目前最先进的模型，有可能造福全人类。不过，它们也构成了越来越严重的风险。”</p><p></p><h4>小米正式发布澎湃OS，雷军确认小米汽车明年上半年上市</h4><p></p><p></p><p>10月26日晚，小米集团创始人、董事长兼CEO雷军宣布集团战略正式升级为“人车家全生态”，并发布了小米澎湃OS操作系统、数字高端旗舰小米14系列，以及Xiaomi Watch S3、小米电视S Pro 85等6款AIoT新品。</p><p></p><p>雷军表示，过去几年小米一直在突破认知，改变成长，今天将迎来“跨越时刻”。小米发布的新战略“人车家全生态”，是以人为中心、对“人车家全生态”进行整合，而承接新战略的关键是小米澎湃OS。</p><p></p><p>雷军还透露了小米汽车的最新情况，称“进展顺利”，将于2024年上半年正式上市。他表示，“人车家全生态”战略很快将完成全面落地的最后拼图。</p><p></p><h4>AMD回应“中国区大幅裁员”：系公司组织架构小幅度优化和重组</h4><p></p><p></p><p>近期，有消息称超威半导体公司（AMD）即将在中国区进行大规模裁员，本轮裁员比例可能为10％-15％，或涉及数百名员工。</p><p></p><p>对此，10月26日，AMD方面回应：“网络传闻失实。基于公司战略的调整，公司近期对组织架构进行了小幅度的优化和重组。”</p><p></p><h4>亚马逊高管解读Q3财报：生成式AI未来几年将为AWS带来数百亿美元营收</h4><p></p><p></p><p>10 月 27 日，亚马逊发布了2023财年第三季度财报。报告显示，亚马逊第三季度净销售额为1430.83亿美元，同比增长13%，不计入汇率变动的影响为同比增长11%；净利润为98.79亿美元，同比增长244%；每股摊薄收益为0.94美元，相比之下去年同期的每股摊薄收益为0.28美元。</p><p></p><p>财报发布后，亚马逊CEO Andy Jassy和CFO Brian Olsavsky回答了投资者提问。Andy Jassy表示：“公司运业务目前的预计年销售额已经达到920亿美元，而90%的全球IT业务处理仍然基于本地设备，我们相信这一比例将有机会反转，所以我们的增长空间非常大。再加上刚刚涌现出来的，生成式人工智能市场方面的巨大机会，也将在未来几年内为AWS带来数百亿美元的营收。”</p><p></p><h4>腾讯混元大模型正式开放“文生图”功能，代码能力大幅提升 20%</h4><p></p><p></p><p>10 月 26 日，腾讯宣布，腾讯混元大模型迎来全新升级，升级后的腾讯混元中文能力整体超过 GPT3.5，代能力大幅提升 20%，达到业界领先水平。同时，腾讯混元大模型正式对外开放“文生图”功能。</p><p></p><p>腾讯机器学习平台算法负责人康战辉重点介绍了混元大模型代码方面的能力。代码技术主要是两个方向进行了优化：一是代码预训练，二是 SFT 指令微调。腾讯表示，经过对 32 种主流语言代码文件、各类计算机书籍和博客的学习增训，腾讯混元代码处理水平提升超过 20%，代码处理效果胜出 ChatGPT 6.34%，在 HumanEval 公开测试集指标上全面超过 Starcoder、Codellama 等业界头部开源代码大模型。</p><p></p><h4>小马智行获沙特1亿美元投资</h4><p></p><p></p><p>日前，小马智行微信公众号发文称，获得沙特阿拉伯王国新未来城（NEOM）及旗下投资基金NIF（NEOM Investment Fund）1亿美元投资。本轮融资资金将用于自动驾驶技术全球化研发和运营投入等方向。</p><p></p><p>据了解，双方计划在新未来城建立自动驾驶生产制造及研发中心，面向沙特新未来城乃至中东及北非地区开展自动驾驶研发与制造，并部署自动驾驶服务车队以及智能汽车相关的基础设施。沙特新未来城将成为小马智行推行技术全球化布局的战略要地。</p><p></p><h4>历时 7 个月，国家数据局正式揭牌</h4><p></p><p></p><p>10 月 25 日，国家数据局正式揭牌。</p><p></p><p>2023 年 3 月，国务院机构改革方案提出组建国家数据局，负责协调推进数据基础制度建设，统筹数据资源整合共享和开发利用，统筹推进数字中国、数字经济、数字社会规划和建设等，由国家发展和改革委员会管理。</p><p></p><p>据悉，目前国家数据局已有两名领导亮相。7 月 28 日，人社部发布国务院任免国家工作人员信息，刘烈宏获任命为国家数据局局长；10 月 11 日，国务院任命沈竹林为国家数据局副局长，另有媒体报道，同月国家数据局启动招聘，发布多个职位。</p><p></p><h4>亚马逊推出人工智能图像生成功能</h4><p></p><p></p><p>10月25日，亚马逊宣布推出测试版图像生成功能。亚马逊称，在亚马逊广告控制台中，广告商只需选择产品并点击“生成”，该工具就能利用人工智能生成功能，根据产品细节在几秒钟内提供一组以生活方式和品牌为主题的图片。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>程序员利用漏洞篡改ETC余额，一年私吞260余万元</h4><p></p><p></p><p>据警民直通车上海报道，2023年9月，上海市公安局浦东分局北蔡派出所接到某科技公司员工张女士报案称，其公司发现计算机系统被他人篡改数据，导致公司账户钱款损失。民警对公司后台电子数据出现的异常账户进行提取搜证中发现，要想通过漏洞篡改网站后台信息，执行起来难度很大。操作者很有可能是管理网站后台系统的员工，监守自盗的可能性较大。</p><p></p><p>民警随即展开工作，最终嫌疑人曹某迫于压力，主动投案自首。</p><p></p><p>2022年8月，曹某发现所在公司的网站后台有漏洞，身为软件工程师的他，决定铤而走险，用其母亲的身份证自行注册了一个ETC账户，并绑定了其母亲的银行卡。随后曹某以每周4至5次、每次1万元的频率，陆续从该账户内提取了230余万元。之后，曹某又利用朋友的身份证再次办理账号，以同样的方式再次从公司提现36万元。</p><p></p><p>目前，犯罪嫌疑人曹某因涉嫌盗窃罪，已被浦东警方依法刑事拘留，该起案件正在进一步审理中。</p><p></p><p>网友热评：太“刑”了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Dhsl1B6yaEJUp5EgdWMZ</id>
            <title>可部署手机、适配国产芯……全新升级后的ChatGLM3真的有点东西：智谱 AI 选择继续开源！</title>
            <link>https://www.infoq.cn/article/Dhsl1B6yaEJUp5EgdWMZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Dhsl1B6yaEJUp5EgdWMZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 28 Oct 2023 03:32:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智谱 AI, ChatGLM3, 大模型, 性能提升
<br>
<br>
总结: 智谱 AI 在中国计算机大会上发布了第三代对话大模型 ChatGLM3，该模型在性能方面有显著提升，推理速度提高了2-3倍，推理成本降低一倍。与之前的模型相比，在多个基准测试中表现优异，排名首位。此外，ChatGLM3还具备了多模态理解能力和智能代理能力，可以在更多复杂场景中发挥出色表现。这一创新为自然语言处理技术的应用范围提供了拓展。 </div>
                        <hr>
                    
                    <p>10 月 27 日，智谱 AI 在 2023 中国计算机大会（CNCC）上发布了自研第三代对话大模型 ChatGLM3，这是智谱 AI 在今年内第三次对 ChatGLM 基座模型进行了深度优化。ChatGLM 是由清华大学创新领军工程博士张鹏带领下的团队开发的一个开源且支持中英双语的类 ChatGPT 大语言模型，一经推出就迅速受到大家的关注。</p><p>&nbsp;</p><p>此次 ChatGLM3 发布后，几个小时的时间就覆盖了大模型圈内人的朋友圈，所以 ChatGLM3 本次到底升级了什么？对大模型的发展又产生了哪些影响？</p><p>&nbsp;</p><p></p><h2>一、更强大、更高效、更长，都是 ChatGLM3 的形容词</h2><p></p><p>&nbsp;</p><p>随着人工智能技术的快速发展，自然语言处理领域已经成为最具挑战性和最活跃的研究方向之一。在这个领域中，大型预训练模型被证明是实现卓越性能的关键。</p><p>&nbsp;</p><p>从性能方面，推理速度和成本一直是衡量模型性能的重要指标之一，在众多预训练模型中，ChatGLM 系列模型也一直因其优秀的性能和创新能力而备受关注。而此次智谱 AI 发布的 ChatGLM3 的推理框架是基于最新的高效动态推理和显存优化技术构建的，在相同硬件、模型条件下，相较于目前最佳的开源实现，对比伯克利大学推出的&nbsp;vLLM 以及&nbsp;Hugging Face TGI 的最新版本，推理速度提升了 2-3 倍，推理成本降低一倍，每千&nbsp;tokens 仅 0.5 分，成本相对最低。这些数据足以表明，ChatGLM 系列模型在推理速度和成本方面已具有显著优势。</p><p>&nbsp;</p><p>与 ChatGLM 二代模型相比，ChatGLM3 在 44 个中英文公开数据集测试中表现优异，在国内同尺寸模型中排名首位。评测结果显示，ChatGLM3 在 MMLU、CEval、GSM8K 和 BBH 等基准测试中均取得了显著的性能提升，分别提升了 36%、33%、179%和 126%。这主要得益于其独创的多阶段增强预训练方法，以及更丰富的训练数据以及更优的训练方案。多阶段增强预训练方法在语言模型训练中展现出显著的优势，其根据不同的任务和数据分布来优化模型性能，从而在各种不同的语言任务中取得更好的表现。通过多个预训练阶段的反复迭代和优化，模型得以深入学习语言知识和规律，进而提升对语言的理解能力，这种方法有助于强化模型的泛化能力，使其能够更好地适应各种不同的语言环境。此外，在面对复杂的语言现象时，该方法使模型还能够更加鲁棒地处理各种情况，减少出现偏见或误解的可能性。</p><p>&nbsp;</p><p>除了在基准测试中表现出色，ChatGLM3 还瞄准了 GPT-4V 的技术升级，要知道，GPT-4V 具有每种模态（文本和视觉）的限制和能力，同时呈现出来自所述模态交叉和大规模模型提供的智能和推理的新颖能力。所以本次发布的 ChatGLM3 实现的若干全新功能的迭代升级中，最引人注目的就是多模态理解能力的 CogVLM-看图识语义功能，该功能在 10 余个国际标准图文评测数据集上取得 SOTA。此外，与 GPT-4V 相比，ChatGLM3 的语义能力和逻辑能力都得到了大大增强：</p><p>代码增强模块&nbsp;Code Interpreter 根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；网络搜索增强 WebGLM-接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。</p><p>&nbsp;</p><p>此外，ChatGLM3 目前已经具有了全新的&nbsp;Agent 智能体能力，其集成了自研的 AgentTuning 技术，激活了模型智能代理能力。在智能规划和执行方面，ChatGLM3 相比 ChatGLM 二代提升了 1000%，这一技术开启了一种全新的模型智能体能力，使 ChatGLM3 能够在更多复杂场景中发挥出色表现。例如，ChatGLM3 能够原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理以及操作系统等复杂场景。</p><p>&nbsp;</p><p>非常值得一提的是，为了更好地适应边缘计算的需求，ChatGLM3 还推出了可手机部署的端侧模型 ChatGLM3-1.5B 和 3B。这些模型支持包括 vivo、小米、三星在内的多种手机以及车载平台，甚至支持移动平台上 CPU 芯片的推理，速度可达 20tokens/s。在精度方面，1.5B 和 3B 模型在公开 benchmark 上与 ChatGLM2-6B 模型性能接近。这一创新为自然语言处理应用在移动设备上的部署提供了便捷的方式，进一步拓展了自然语言处理技术的应用范围。</p><p>&nbsp;</p><p>而正是在全新升级的 ChatGLM3 赋能下，生成式 AI 助手智谱清言目前已成为国内首个具备 Advanced Data Analysis（原 Code Interpreter）能力的大模型产品，可支持图像处理、数学计算、数据分析等使用场景。CogVLM 模型则提高了智谱清言的中文图文理解能力，取得了接近&nbsp;GPT-4V 的图片理解能力。它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p><p></p><p><img src="data:image/webp;base64,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" /></p><p></p><p>据悉，目前智谱清言已具有搜索增强能力，它可以帮助用户整理出相关问题的网上文献或文章链接，并整理出答案，这意味着智谱清言将为用户提供更好的自然语言处理服务。</p><p>&nbsp;</p><p></p><h1>二、ChatGLM3 继续开源，“搞好开源”是智谱 AI 的初心</h1><p></p><p>&nbsp;</p><p>在此次 ChatGLM3 大模型的发布现场，智谱 AI 宣布为了进一步推动开源生态，将对包括 ChatGLM3-6B、ChatGLM3-6B-32K 以及 CogVLM-17B 和 AgentLM 在内的多个模型进行开源。</p><p>&nbsp;</p><p>目前，ChatGLM3-6B 模型的开源成绩已经比较可观，它在 44 多个对话模型数据集上的 9 个榜单中排名第一，其开源的 32k 版本 ChatGLM3-6B-32K 在 LongBench 中表现最佳。</p><p>&nbsp;</p><p>伴随着 ChatGLM3 的开源，模型的工作原理和团队技术研发的决策过程可以被更多人了解，模型的透明度和可解释性将更有助于从业者理解模型，增强对模型的信任和使用体验，学术界和产业界的大模型开发者们都可以获取到模型的源代码和参数，基于现有模型参数和算法进行更深入的研究和创新，模型的性能也将有望在短时间内再次完成快速迭代，自然语言处理领域将得到进一步的发展。同时，开放的生态系统和社区也将推动 ChatGLM3 在实际场景中的应用和优化，相关产业将获得基于 ChatGLM3 更智能、更高效的服务和解决方案以完成数字化转型。</p><p>&nbsp;</p><p>其实，ChatGLM3 并不是智谱 AI 的第一次开源，早在今年三月，智谱 AI 就已经陆续宣布大模型产品开源，而且成绩持续斐然，推动大模型行业发展是他们的初心也是一直在做的事情。比如多模态 CogVLM-17B 在开源后，在 10 个多模态榜单中排名第一；再如智能体 AgentLM，其让开源模型达到甚至超过闭源模型的 Agent 能力。</p><p><img src="https://static001.geekbang.org/infoq/d3/d3538a7f3451722e685cedec61ccf18f.webp" /></p><p></p><p>智谱 AI 从 B 端企业服务方面有深厚的基础，他们将自己的大模型进行开源，其实可以更好地让大家从场景落地方面实现大模型技术的创新，这是很多尚未商业化的大模型无法比拟的优势。</p><p>&nbsp;</p><p>当然了，目前有越来越多的公司和研究机构开始将他们的大模型开源，国内比较知名的就有阿里巴巴的通义大模型系列、华为的盘古大模型系列、腾讯的混元大模型系列等多家。但当我们复盘包括智谱 AI 开源在内的这些大模型，我们会发现，它们不仅在中文领域表现出色，也在英文等其他语言领域有着广泛的应用，但由于这些开源的大模型具有极高的参数量和计算量，需要大量的数据和算力支持，所以只有少数的大型科技公司和研究机构能够开发和维护这些大模型。但也正因为这些挑战存在，大模型开源就变得更为重要，只有越来越多的人开始应用开源模型，难题才会有可能解决掉。</p><p>&nbsp;</p><p></p><h1>三、ChatGLM 系列大模型有“势必做好国产化”的决心</h1><p></p><p>&nbsp;</p><p>ChatGLM 3 的发布让智谱 AI 已构建起的全模型产品线更加强大。智谱 AI CEO 张鹏表示：“自 2020 年起，智谱 AI 便专注大模型的自研创新。从早期开始的 GLM 预训练架构的研发，到今天 ChatGLM3 的推出，我们在技术研发、国产适配、开源生态、商业交付等各方面都有了一定进展。我们希望基于当前完整的自研产品线，包括对话、多模态、代码、搜索增强等模型，以及全流程的技术支持，可以更好地支撑行业生态，与合作伙伴一同高速发展。”</p><p>&nbsp;</p><p>自 2022 年初，ChatGLM 系列模型已支持在昇腾、神威超算、海光 DCU 架构上进行大规模预训练和推理，截至目前已支持 10 余种国产硬件生态，包括昇腾、神威超算、海光 DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。通过与国产芯片企业的联合创新，ChatGLM 系列模型性能不断优化，国产硬件生态也得到了大模型国产化的闭环。</p><p>&nbsp;</p><p>ChatGLM 针对国产芯片的场景创新和技术支撑，其实也是我完成高新技术国产化升级的过程，这可以促进更多的研究者、开发者以及企业参与到自然语言处理技术的研究和开发中来，共同推动国内自然语言处理技术的发展。当 ChatGLM 在与国产芯片彼此成就的过程中，这将陆续帮助国产芯片摆脱对国外模型的依赖，增强国内模型的自主可控性，做出更适合中国市场需求的芯片的同时，这对于国家信息安全、产业发展等方面都具有重要的意义，直接增强了国家的科技实力，为国家的科技发展和国际竞争力提升具有重要价值。</p><p>&nbsp;</p><p></p><h1>四、写在最后</h1><p></p><p>&nbsp;</p><p>在 ChatGLM 3 系列模型发布后，智谱 AI 成为了目前国内唯一一个有对标 Open AI 全模型产品线的公司，（以下对比左侧产品为 OpenAI，右侧产品为智谱 AI）：</p><p>对话方面：ChatGPT——ChatGLM（对话）文生图方面：DALL.E——CogView（文生图）代码方面：Codex——CodeGeeX （代码）搜索增强方面：WebGPT——WebGLM （搜索增强）图文理解方面：GPT-4V——ChatGLM 3 (CogVLM,AgentTuning…)</p><p>&nbsp;</p><p>一名微软的算法工程师说，“在硅谷，智谱 AI 的 GLM 应该是最被头部科技企业承认的中国大语言模型。”可见 ChatGLM 是智谱 AI，也是国内大模型厂商追逐 OpenAI 的最大底气。最新一代大模型 ChatGLM3 的开源，在助于推动自然语言处理领域的发展、加速 AI 应用的开发过程、提高模型的可信度和透明度、促进社区合作和创新等方面具有重要的价值。但是否能够完全超越 OpenAI，还要看走出实验室后，ChatGLM3 在具体场景下的应用和性能表现。</p><p>&nbsp;</p><p>但不管怎么说，一直将“持续搞好开源、做好国产化”作为基本功的 ChatGLM，通过不断开放和共享其技术和模型，已经大力促进了全球范围内的技术创新和产业发展，为中国大模型的产业升级和技术创新做出了较为突出的贡献。</p><p>&nbsp;</p><p>事实上，在目前这个阶段，大模型厂商都应该做好以上两项基本功。只有通过稳扎稳打，不断推动大模型技术的发展和应用，才能让“中国大模型”在全球市场中展现出更多的价值。中国的厂商应该积极响应这一号召，加大投入，加强研发，不断提升自身的大模型技术和应用能力，抱团取暖，为中国的人工智能产业做出更大的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ</id>
            <title>适配更多国产芯片，智谱AI推出第三代基座大模型ChatGLM3</title>
            <link>https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 28 Oct 2023 00:11:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智谱AI, ChatGLM3, 基座大模型, 多阶段增强预训练方法
<br>
<br>
总结: 2023年10月27日，智谱AI在2023中国计算机大会上推出了全自研的第三代基座大模型ChatGLM3及相关系列产品。ChatGLM3采用了独创的多阶段增强预训练方法，使训练更为充分。评测显示，在44个中英文公开数据集测试中，ChatGLM3在国内同尺寸模型中排名首位。智谱AI的ChatGLM3性能更加强大，提升了多个指标。此外，ChatGLM3还实现了若干全新功能的迭代升级，包括多模态理解能力的CogVLM-看图识语义，代码增强模块Code Interpreter，以及网络搜索增强WebGLM-接入搜索增强。ChatGLM3还集成了自研的AgentTuning技术，激活了模型智能体能力。ChatGLM3还推出了可手机部署的端测模型ChatGLM3-1.5B和ChatGLM3-3B，支持多款手机以及车载平台。智谱AI的产品已支持10余种国产硬件生态。ChatGLM3的推理框架在相同硬件、模型条件下，推理速度提升了2-3倍，推理成本降低一倍。智谱AI的ChatGLM3具有搜索增强能力和中文图文理解能力。 </div>
                        <hr>
                    
                    <p>2023年10月27日，<a href="https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱AI</a>"于2023中国计算机大会（CNCC）上，推出了全自研的第三代基座大模型ChatGLM3及相关系列产品，这也是智谱AI继推出千亿基座的对话模型ChatGLM和ChatGLM2之后的又一次重大突破。</p><p>&nbsp;</p><p>据悉，此次推出的ChatGLM3采用了独创的多阶段增强预训练方法，使训练更为充分。评测显示，在44个中英文公开数据集测试中，ChatGLM3在国内同尺寸模型中排名首位。智谱AI CEO张鹏在现场做了新品发布，并实时演示了最新上线的产品功能。</p><p>&nbsp;</p><p>通过更丰富的训练数据和更优的训练方案，智谱AI推出的ChatGLM3性能更加强大。与ChatGLM2相比，MMLU提升36%、CEval提升33%、GSM8K提升179% 、BBH提升126%。</p><p>&nbsp;</p><p>同时，ChatGLM3瞄向GPT-4V本次实现了若干全新功能的迭代升级，包括多模态理解能力的CogVLM-看图识语义，在10余个国际标准图文评测数据集上取得SOTA；代码增强模块Code Interpreter根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；网络搜索增强WebGLM-接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。ChatGLM3的语义能力与逻辑能力得到了极大的增强。</p><p>&nbsp;</p><p>ChatGLM3还集成了自研的AgentTuning技术，激活了模型智能体能力，尤其在智能规划和执行方面，相比于ChatGLM2提升了1000% ；开启了国产大模型原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理、操作系统等复杂场景。</p><p>&nbsp;</p><p>此外，ChatGLM3本次推出可手机部署的端测模型ChatGLM3-1.5B和 ChatGLM3-3B，支持包括vivo、小米、三星在内的多款手机以及车载平台，甚至支持移动平台上CPU芯片的推理，速度可达20 tokens/s。精度方面1.5B和3B模型在公开benchmark上与ChatGLM2-6B模型性能接近。</p><p>&nbsp;</p><p>自2022年初，智谱AI推出的GLM系列模型已支持在昇腾、神威超算、海光DCU架构上进行大规模预训练和推理。截至目前，智谱AI的产品已支持10余种国产硬件生态，包括昇腾、神威超算、海光DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。</p><p>&nbsp;</p><p>基于最新的高效动态推理和显存优化技术，ChatGLM3当前的推理框架在相同硬件、模型条件下，相较于目前最佳的开源实现，包括伯克利大学推出的 vLLM 以及Hugging Face TGI的最新版本，推理速度提升了2-3倍，推理成本降低一倍，每千tokens仅0.5分，成本最低。</p><p>&nbsp;</p><p>另外，随着WebGLM大模型能力的加入，智谱清言也具有了搜索增强能力，可以帮助用户整理出相关问题的网上文献或文章链接，并直接给出答案。此前已发布的CogVLM 模型则提高了<a href="https://www.infoq.cn/article/Keo5MOT4MavSIyyxTmII?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱清言</a>"的中文图文理解能力，取得了接近GPT-4V的图片理解能力，它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/yyBzwQPKp3djgGsxgRbX</id>
            <title>SOLIDWORKS：把AI放进工业设计软件，把软件放到云上</title>
            <link>https://www.infoq.cn/article/yyBzwQPKp3djgGsxgRbX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/yyBzwQPKp3djgGsxgRbX</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 08:31:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 达索系统, SOLIDWORKS, 3DEXPERIENCE平台, 云化改造
<br>
<br>
总结: 达索系统收购了SOLIDWORKS，并在3DEXPERIENCE平台上进行了云化改造，使得SOLIDWORKS产品可以在云端运行。SOLIDWORKS 2024强调AI驱动，通过整合机器学习和人工智能，实现设计工作的自动化。云化改造和AI技术的引入，使得SOLIDWORKS在工业设计软件领域变得更简单和高效。同时，云化改造也实现了产品全生命周期的动态优化。 </div>
                        <hr>
                    
                    <p>1997年，达索系统收购了当时发展势头正猛的三维CAD软件商SOLIDWORKS；此后的十多年时间里，双方一直以相对独立的模式进行开发和管理。直到2012年，达索系统正式推出了工业云产品3DEXPERIENCE平台，其中，SOLIDWORKS作为平台12个产品模块之一，二者的产品关联性日渐紧密。</p><p></p><p>也正是在那时，基于<a href="https://www.infoq.cn/article/N2gWCU0NhYWjmyxwwy2R">达索系统</a>"3DEXPERIENCE平台，SOLIDWORKS的产品形式也发生了里程碑式的变化——开始逐步进行云化改造和迭代升级。</p><p></p><p>日前，SOLIDWORKS 2024正式发布。相较于此前的版本，新增了10大功能。笔者认为，其中有两大不得不提的看点：SOLIDWORKS 2024强调AI驱动，通过在平台中整合机器学习和人工智能，实现设计工作的自动化；与此同时，SOLIDWORKS可以完全在云端运行，这意味着多个重要功能均可在云上进行操作。</p><p></p><h3>把AI放进工业设计软件，把设计工作变得简单</h3><p></p><p></p><p>长期以来，<a href="https://www.infoq.cn/article/2SA0hh4TISKTD83OfqNw?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">SOLIDWORKS</a>"面向的都是中小企业市场，最大的亮点在于易用性。“无论产品形式和产品线如何变化，我们的原则都是坚持这种易学易用性。通过在平台中整合机器学习和人工智能，尽可能将其应用于3DEXPERIENCE Works产品线，为的就是把复杂的事物变得简单。”达索系统3DEXPERIENCE Works业务部门全球高级副总裁Gian Paolo Bassi在达索系统SOLIDWORKS创新日上表示。</p><p></p><p>在设计工作中，涉及很多需要耗费大量人力和时间的操作，包括阵列操作、绘图、标尺寸等等。“对此，我们希望尽量实现设计过程的自动化。”举例来说，SOLIDWORKS 2024引入了名为“晶格支撑”的设计，通过融合多种算法，只需给定一些特定条件，就可以自动生成相应的设计模型。</p><p></p><p>实际上，早在此次版本更新之前，SOLIDWORKS就已经尝试通过AI的引入，帮助企业客户解决设计效率提升的难题。</p><p></p><p>达索系统大中国区专业客户事业部副总裁吴俊杰在接受InfoQ等媒体采访时介绍，某叉车制造企业在过去20多年里积累了大量数据，在使用<a href="https://www.infoq.cn/article/kKyCIHD5C0kAVkATcW4I?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">CAD</a>"进行设计时，每个工程师都有自己的思路，加上缺乏设计方法论，导致设计不规范，即使是相同的零部件最后也出现了各种不同的版本。对于企业而言，这都是真金白银的成本投入。</p><p></p><p>利用AI技术，SOLIDWORKS帮助该客户梳理出了积累20多年的不同三维模型，从中识别了重复的零件版本，将原有的50多万个零部件减少到了15万个，大大降低了仓库成本。并且，进一步实现了设计流程的标准化、模块化和参数化，缩短了零件整体的设计周期。</p><p></p><p>但是，在SOLIDWORKS看来，这只是一个开始。“在工业设计领域，我们希望能够做一个类似<a href="https://www.infoq.cn/article/tq5VmzbUvckWKG4T9axk">ChatGPT</a>"的工具。例如，只要设计师告诉它‘基于某个年份的摩托车做一个新的零件设计’，然后提供一些特定条件，这个工具就可以通过在数据库中查询对应的模型算法，给出它的设计方案。如果你不满意，还可以进行修改。”Gian Paolo Bassi这样畅想。</p><p></p><p>距离这个目标的实现，显然还有不短的路要走。“企业在发展过程中，数字化基础并不相同。比如，很多企业使用了三维设计软件，但流程上并没有进行设计标准化，导致底层数据杂乱无章。如果把这样的数据提供给AI去学习，效果并不好。所以，一方面，企业需要一些时间制定更规范的标准和规则，梳理好数据基础。另一方面，ChatGPT技术的发展也刚刚起步，还需要更长的时间去迭代和应用。”达索系统大中国区专业客户事业部技术总监戴瑞华解释道。</p><p></p><p>戴瑞华告诉InfoQ，从他个人的洞察来判断，这一时间周期大概在3-5年。期间，除了数据和技术的准备之外，对于企业而言，更关键的是要把<a href="https://www.infoq.cn/article/X68Q0yoLNplMg4RC5969">数字化转型</a>"放到战略的高度。“假设只有设计部门采用了新技术，而没有完成各个流程的打通，设计数据仍然局限在设计部门，没有传输到仿真、电气，甚至是加工、生产和售后等环节，那么，这种所谓的智能化就难以实现，成果也不显著。”</p><p></p><h3>把工业软件放到云上，对产品全生命周期进行动态优化</h3><p></p><p></p><p>实现各个业务流程和环节（不仅仅是设计）的打通，推动企业向真正的数字化、智能化发展，这也是为什么SOLIDWORKS要加速<a href="https://www.infoq.cn/article/s3FKND2dGscQFzg6FvEi">转向云端</a>"的原因。</p><p></p><p>“AI可以从数据中挖掘有价值的信息帮助企业进行决策，其前提是，它需要一个庞大的、可共享的数据库。”而云，恰恰可以满足这一前提。Gian Paolo Bassi表示，SOLIDWORKS支持完全在云端运行，这意味着：</p><p></p><p>首先，企业可以通过付费或者租赁的方式，更灵活地选择软件授权方式，入门成本也更低；其次，通过登录方式进入软件，企业可以了解员工对CAD使用情况；其三，所有设计数据都存储在云端，形成一个统一数据库，管理层可以更高效地调用其中的<a href="https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK">AI工具</a>"，进行决策辅助。</p><p></p><p>值得一提的是，不只是SOLIDWORKS，达索系统的所有产品线在3DEXPERIENCE平台上的数据都是共通的，用户在上面不需要进行任何数据转换，就可以实现跨软件操作，使用平台中其它13个品牌模块，包括高级分析、项目管理、复杂车间应用、机器人技术和工厂建设等，一站式查看所有相关的设计数据、产品数据、装备数据，实现不同业务线之间的高效协作。</p><p></p><p>此外，Gian Paolo Bassi还提及，安全性也是云端版本的一大优势，在他看来，把数据存储在云端相比于公司内部服务器更安全，因为云端有更多的安全措施来防止黑客的入侵和恶意行为。并且，目前达索系统的3DEXPERIENCE Works虽然在云端运行，但是所有放上去的数据都只允许公司内部人员访问，其他客户的模型均没有办法调用，因此安全性也有所保障。</p><p></p><p>“当面对新客户时，我们可能会先建议他们考虑使用云端解决方案。当然，最终决策权在客户手中。对于现有客户，我们仍然会提供多种选项，根据客户需求进行定制。”吴俊杰强调。</p><p></p><p>事实上，把工业软件放到云上，这也是达索系统为践行虚拟孪生策略的迈出的关键一步。2020年，达索系统提出了虚拟孪生概念，与更多人熟知的数字孪生概念不同，它不仅仅是现实世界实体的数字表达，还包含了这个数字化对象的演进历史，比如它从哪里来，如何经过设计、仿真、验证，最终制造出来成为一个实体。除此之外，还包括对它未来的预测，比如这个实体产品在使用过程中如何运作、如何更好地进行维修保养服务等等。</p><p></p><p>“通过云化，可以支持用户在任何设备上使用我们的产品，包括手机和平板电脑。比如，设计人员可以在办公室使用台式机设计产品，销售人员可以在外面使用手机展示并与客户分享。”Gian Paolo Bassi举例。</p><p></p><p>换句话说，虚拟孪生涵盖了产品全生命周期，它是一个动态的过程。企业在物理世界生产运行中获取到的知识、积累的数据会反馈到虚拟世界，对虚拟世界的下一代产品从性能表现、成本控制等各个方面进行优化和创新。</p><p></p><h3>写在最后：陪伴客户更像是场马拉松</h3><p></p><p></p><p>今年，是SOLIDWORKS进入中国市场的第27年，也是吴俊杰执掌中国市场业务的第18年。在这个过程中，技术更迭、客户需求、渠道生态发生着天翻地覆的变化。回顾整个发展历程，他向InfoQ坦言，SOLIDWORKS也曾遇到过一系列挑战。</p><p></p><p>举例来说，SOLIDWORKS最早的续订模式在中国就出现过“水土不服”。“在国内，很多企业对于除了购买费用之外还要支付维护费的方式是难以接受的。因此，从2017年开始，我们尝试了一种新的合作模式，推出了‘335合作计划’（即三年携手共进包、三年卓越进取包、五年战略协作包），把重心从产品升级放到转型服务上，最终使得续订率大幅度提高。”</p><p></p><p>吴俊杰表示，SOLIDWORKS的目标是希望成为中小企业的首选，而这，也是陪伴客户完成马拉松跑的过程，人员的稳定性、生态系统的稳定性非常重要。“所以，我们一直倡导‘本地人做本地事，专业人做专业事’，比如，河南的代理商只服务河南客户，上海代理商只服务上海客户。这听起来是理所当然的道理，但并不是每个企业每个人都能做到。在过去3年疫情时间里，这个模式很好地保障了我们面向各地客户的及时服务。”</p><p></p><p>除此之外，SOLIDWORKS对渠道商的要求也非常严格，从设计到分析、仿真到数据管理、制造到ERP，所有产品线的合作伙伴，都不允许成为其它厂商的代理商。吴俊杰指出，这是其加入SOLIDWORKS18年来如一日所坚持的原则。“最关键的是产品、服务、长期合作以及良好的合作氛围，我们希望通过这四个方面来帮助客户。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p><h5>附：SOLIDWORKS 2024 新功能介绍</h5><p></p><p><img src="https://static001.geekbang.org/infoq/32/329c58ab13aee21771943f86427c267c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5fc992be29b316ea238453190c9de0c.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/f2NZ2bR5H5YrHK5Gzqvh</id>
            <title>2023年AI与开源行业：今年第一篇盘点文章出炉了</title>
            <link>https://www.infoq.cn/article/f2NZ2bR5H5YrHK5Gzqvh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/f2NZ2bR5H5YrHK5Gzqvh</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 07:48:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI研究, GPT-4, Llama模型, 大语言模型
<br>
<br>
总结: 2023年AI研究中的重点是对过去一年已经生效的趋势进行扩展，包括GPT-4和Llama模型等大语言模型的升级和应用。开源与研究社区的关注重点也从潜在扩散模型转向了大语言模型。未来的突破可能来自混合专家模型和替代方案，但基于Transformer的大语言模型仍然是当前最先进的技术方案。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;我们正一步步迈向2023年的终点，也许是时候对这一年来AI研究、行业动态以及开源领域发生的主要变化做一番简要回顾了。当然，这篇文章不可能面面俱到。我们只挑干货，一同审视这风云变幻的一年中都有哪些大事值得回味。</p><p>&nbsp;</p><p></p><h2>2022年的趋势进一步扩展</h2><p></p><p>&nbsp;</p><p>这一年中，AI产品并没有表现出任何根本性的发展或者方法创新。相反，2023年的重点就是对过去一年已经生效的趋势做进一步扩展：</p><p>ChatGPT依托的GPT 3.5升级到了GPT 4。DALL-E 2升级到了DALL-E 3。Stable Diffusion 2.0升级到了Stable Diffusion XL。还有更多...</p><p>&nbsp;</p><p>有个有趣的传言说，GPT-4是由16个子模块组成的混合专家模型（MoE）。据传这16个子模块各自拥有1110亿个参数（作为参考，GPT-3总共也只有1750亿个参数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9801afa5817941240fc3497febc32b1.png" /></p><p></p><p>2023年AI现状报告中的GPT-3/GPT-4示意图。</p><p>&nbsp;</p><p>GPT-4属于混合专家模型的情况可能是真的，但我们还无法确定。从趋势上看，行业研究人员在论文中分享的信息要比以往更少。例如，虽然GPT-1、GPT-2、GPT-3乃至InstructGPT论文都公开了架构和训练细节，但GPT-4的架构却一直是个谜。再举另外一个例子：虽然Meta AI的第一篇Llama论文详细介绍了用于模型训练的数据集，但从Llama 2模型开始也不再公布这方面信息。关于这个问题，斯坦福大学上周公布了基础模型透明度指数。根据该指数，Llama 2以54%领先，而GPT-4则以48%排名第三。</p><p>&nbsp;</p><p>当然，要求这些企业发布自己的商业秘密也不太合理。总之，逐渐封闭本身是个有趣的趋势，而且就目前来看我们可能会在2024年继续沿着这个路子走下去。</p><p>&nbsp;</p><p>关于规模扩展，今年的另一大趋势在于输入上下文的长度不断增长。例如，GPT-4竞争对手Claude 2的主要卖点之一，就是其支持最多100k的输入token（GPT-4目前仅支持32k&nbsp;token），也就是说其在为长文档生成摘要方面具备鲜明的优势。另外，Claude 2还支持PDF输入，因此在实践应用中更加灵活实用。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07ba24dc6895780a2ec85f2c42cb4a5c.png" /></p><p></p><p>使用Claude 2为PDF文档生成摘要。</p><p>&nbsp;</p><p></p><h2>开源与研究趋势</h2><p></p><p>&nbsp;</p><p>我还记得，去年开源社区的主要关注对象还是潜在扩散模型（最典型的代表就是Stable Diffusion）等计算机视觉模型。扩散模型与计算机视觉与一直高度相关、牢牢绑定。但短短一年过去，如今的开源与研究社区新贵已然变成了大语言模型。</p><p>&nbsp;</p><p>开源（更确切地讲，是公开可用）大语言模型的爆发式增长，一定程度上要归功于Meta发布的首个预训练Llama模型。尽管其仍有许可限制，但已经启发了Alpaca、Vicuna、Llama-Adapter、Lit-Llama等衍生成果和众多研究人员/从业者的关注。</p><p>&nbsp;</p><p>几个月后，Llama 2模型正式亮相，在基本取代Llama 1的基础之上表现出更为强大的功能，甚至还提供了微调版本。</p><p>&nbsp;</p><p>然而，目前的大多数开源大语言模型仍然是纯文本模型。好在Llama-Adapter v1和Llama-Adapter v2微调版本有望将现有大模型转化为多模态模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/1771efc7d07497f4d2423c6261da694a.png" /></p><p></p><p>Llama-Adapter V2示意图，<a href="https://arxiv.org/abs/2304.15010">https://arxiv.org/abs/2304.15010</a>"</p><p>&nbsp;</p><p>Fuyu-8B是个值得关注的例外模型，此模型刚刚在10月17日正式发布。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a52f412b49851905e69d2a9598e5e41f.png" /></p><p></p><p>&nbsp;Fuyu示意图及注释&nbsp;<a href="https://www.adept.ai/blog/fuyu-8b">https://www.adept.ai/blog/fuyu-8b</a>"</p><p>&nbsp;</p><p>值得注意的是，Fuyu能够将输入补丁直接传递至线性投影（或者叫嵌入层）处以学习其自身图像补丁嵌入，而不会像其他模型/方法那样依靠额外的预训练图像编码器（例如LLaVA和MiniGPT-V），这就极大简化了架构和训练设置。</p><p>&nbsp;</p><p>除了前面提到的少数多模态尝试之外，目前最大的研究重点仍然是如何将GPT-4文本性能迁移至参数范围&lt;100 B的小模型当中。目前的主要技术难点则包括硬件资源成本与限制、可访问数据量不足，以及开发时间太短（受到发布计划的影响，大多数研究人员不可能投入数年时间来训练单一模型）。</p><p>&nbsp;</p><p>然而，开源大语言模型的未来突破并不一定来自将模型扩展至更大规模。在新的一年中，我们将继续关注混合专家模型能否将开源模型提升到新的高度。</p><p>&nbsp;</p><p>另一个有趣的现象，就是我们在研究前沿还看到了一些针对基于Trasnformer大语言模型的替代方案，包括循环RWKV大模型和卷积Hyena大模型，希望能够提供运行效率。但必须承认，基于Transformer的大语言模型仍然是当前最先进的技术方案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e0845a957314a5673c1997a0032c6e97.png" /></p><p></p><p>带注释的Hyena大模型架构示意图：&nbsp;<a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna">https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna</a>"</p><p>&nbsp;</p><p>总的来讲，2023年是开源活动高度活跃的一年，也带来了不少突破和进步，并切实证明了技术研究工作有着一加一大于二的协同效应。但令人遗憾的是，仍有声音在积极反对和打击开源AI技术。希望我们能够继续保持住这股积极的势头，建立起更高效的解决方案和替代方案，而不仅仅是继续依赖科技巨头们发布的类ChatGPT产品。</p><p>&nbsp;</p><p>在本小节的最后，我们要感谢开源和研究社区的付出。你们的努力让可以运行在单个GPU上的小型高效模型成为现实，包括1.3B参数的phi 1.5、7B参数的Mistral和7B&nbsp;Zephyr，这些都拥有接近大型专有模型的性能表现。这样的趋势令人兴奋，期待相关工作能在2024年带来更多进展。</p><p>&nbsp;</p><p></p><h2>关于生产力的承诺</h2><p></p><p>&nbsp;</p><p>在我看来，开源AI就是开发高效、定制大语言模型的主要途径，其中包括根据各种个人/特定领域数据、针对不同场景进行微调的大模型。我自己经常在社交媒体上讨论Lit-GPT，这是我正在积极贡献的一个开源大语言模型。而且我觉得开源并不代表粗糙，我也希望能在保持开源的同时、让成果拥有出色的设计水平。</p><p>&nbsp;</p><p>自从ChatGPT发布以来，我们看到大语言模型几乎被应用在各个领域。屏幕前的读者可能已经体验过ChatGPT，所以这里就不具体解释大模型在不同场景下的实际效果了。</p><p>&nbsp;</p><p>关键在于，我们得把生成式AI之力用在“正确”的地方。比如说，ChatGPT肯定不擅长回答我们常去的杂货店晚上几点关门。我个人最喜欢的用法之一，就是让它帮我修改文章中的语法、或者是集思广益，包括给句子和段落做做润色等。从更宏观的角度看，大语言模型做出了关于生产力的承诺，可能很多朋友都体验过它带来的效率提升。</p><p>&nbsp;</p><p>除了常规文本大模型之外，微软和GitHub的Copilot编码助手也在日趋成熟，并受到越来越多程序员们的喜爱。今年早些时候，Ark-Invest发布的报告估计，代码助手有望将编码任务的完成时间缩短约55%。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c736baf931b578fe0828e2dda51ab70.png" /></p><p></p><p>编码助手示意图&nbsp;<a href="https://ark-invest.com/home-thank-you-big-ideas-2023/">https://ark-invest.com/home-thank-you-big-ideas-2023/</a>"</p><p>&nbsp;</p><p>实际效果究竟有没有55%尚有争议，但如果大家已经体验过编码助手，就会发现它们确实很有帮助，能够将繁琐的编码相关任务变得更加轻松。</p><p>&nbsp;</p><p>而且有一点是肯定的：编码助手将长期存在，并随着时间推移变得越来越强大。它们最终会取代人类程序员吗？我希望不会，但它们无疑会让现有程序员变得更具生产力。</p><p>&nbsp;</p><p>那这对于Stack Overflow又意味着什么？《AI技术现状》报告中包含一份图表，展示了Stack Overflow与GitHub网站之间的流量对比，后者的逐渐胜出可能就跟Copilot的采用率提升有关。但我个人认为形成这种趋势的应该不只是Copilot，ChatGPT/GPT-4在编码任务方面的表现也相当出色，所以我怀疑Stack Overflow下滑是整个生成式AI阵营发展壮大的共同结果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b94b244522d10dcf26e825a3510ea43c.png" /></p><p></p><p>《2023年AI现状报告》（<a href="http://stateof.ai/">http://stateof.ai/</a>"）中的图表</p><p>&nbsp;</p><p></p><h2>AI仍不完善</h2><p></p><p>&nbsp;</p><p></p><h3>幻觉问题</h3><p></p><p>&nbsp;</p><p>2022年困扰大语言模型的问题在今年仍未得到解决：它们会生成负面内容，而且经常产生幻觉。这一年中倒确实出现了有望解决问题的几种方法，包括利用人类反馈的强化学习（RLHF）以及英伟达的NeMO Guardrails等。然而，这些方法要么过于严格、要么只能算是松散的补丁。到目前为止，还没有任何方法（甚至没有可靠的思路）能够在不削弱大模型能力的同时，100%解决掉幻觉问题。在我看来，这一切都取决于我们如何使用大语言模型：别指望在所有场景下都使用大模型——数学计算还是交给计算器比较好；尽量用大模型处理它最擅长的文本创作等工作，并保证认真检查它的输出内容。</p><p>&nbsp;</p><p>此外，对于特定的业务类应用，探索检索增强（RAG）也是一种值得考虑的折衷方案。在RAG中，我们需要从语料库中检索相关文档段落，再根据检索到的内容微调大模型所生成的文本。这种方式让模型能够从数据库和文档中提取外部信息，而不必记住所有知识。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a68ea9ae615d5aab815c4ae6beccb540.png" /></p><p></p><p>&nbsp;我自己的新书《Machine Learning Q and AI》（<a href="https://leanpub.com/machine-learning-q-and-ai/">https://leanpub.com/machine-learning-q-and-ai/</a>"）中的RAG示例。</p><p></p><h3>版权问题</h3><p></p><p>另一个更紧迫的问题，则是围绕AI出现的版权争论。根据维基百科的解释，“对于受版权保护的素材训练而成的大语言模型，模型自身的版权应如何对待仍悬而未决。”总的来说，相关规则似乎仍在起草和修改当中。我希望无论最终规则如何，其内容都应尽可能明确，以便AI研究人员和从业者能够做出相应的调整和行动。</p><p>&nbsp;</p><p></p><h3>评估问题</h3><p></p><p>长久以来，困扰学术研究的一大难题在于，目前流行的基准测试和排行榜所采取的评估方法早就半公开了，其测试集甚至已经被某些大模型用作训练数据。phi 1.5和Mistral就都存在这样的问题。</p><p>&nbsp;</p><p>也有人在用其他大模型自动做评估，但这种方式不擅长处理那些跟偏好相关的问题。总之，不少论文已经在依赖GPT-4作为辅助性质的模型评估方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b0e58f8e55d818ae3262b3852a84a18.png" /></p><p></p><p>LIMA论文中的人类与GPT_4偏好评估示例。</p><p></p><h3>收入问题</h3><p></p><p>生成式AI目前仍处于探索阶段，不过文本和图像生成器已经能够在特定场景下带来不错的表现。然而，由于高昂的托管和运行时间成本，这些工具能够为企业产生正向现金流仍是个备受争议的问题。例如，有报道称OpenAI过去一年亏损了5.4亿美元。另一方面，最近的报道指出OpenAI目前的单月收入为8000美元，已经足以抵偿或超过其运营成本。</p><p>&nbsp;</p><p></p><h3>伪造图像</h3><p></p><p>由生成式AI引发的另一个大问题，就是伪造图像和视频。这类隐患在当前的社交媒体平台上已经相当明显。伪造图像和视频一直是个大麻烦，而且凭借远低于Photoshop等内容编辑软件的准入门槛，AI技术已经将严重性提升到了新的水平。</p><p>&nbsp;</p><p>目前有一部分AI系统在尝试检测由AI生成的内容，但这些系统在文本、图像和视频检测中的表现都不够可靠。某种程度上，遏制并解决这些问题的唯一方法仍然要依靠人类专家。就如同我们不能轻易相信网上某个论坛或者网站中的医疗或者法律建议一样，我们也绝不能在未经认真核实的情况下，就盲目相信网络上散播的图像和视频。</p><p>&nbsp;</p><p></p><h3>数据集瓶颈</h3><p></p><p>跟之前提到的版权争议相关，不少企业（包括Twitter/X和Reddit）都关闭了免费API以增强经营收入，同时也防止爬取者收集其平台数据用于AI训练。</p><p>&nbsp;</p><p>我见过不少由数据集专职收集厂商打出的宣传广告。从这个角度来看，尽管AI确实会用自动化取代一部分工作岗位，但似乎同时也创造出了新的职务类型。</p><p>&nbsp;</p><p>目前来看，为开源大模型做贡献的最佳方式之一，就是建立一个众包性质的数据集平台，在这里搜集、整理并发布明确允许大语言训练使用的数据资源。</p><p>&nbsp;</p><p></p><h2>RLHF会是破解难题的正确答案吗？</h2><p></p><p>在Llama 2模型套件发布时，我很高兴看到其中包含了可通过聊天进行微调的模型。Meta AI也使用人类反馈强化学习（RLHF）提高了模型的实用性和无害性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b63ec4577538c4d2d18fc452dfc89c91.png" /></p><p></p><p>Llama 2论文中的注释图：开放基础与微调聊天模型，&nbsp;<a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>"</p><p>&nbsp;</p><p>我一直觉得RHLF是种非常有趣、而且极具前景的方法。但除了InstructGPT、ChatGPT和Llama 2之外，大多数模型并没有广泛采用。可在无意之中，我还是找到了下面这份RLHF流行度统计图表。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cff653f92dad9e5d458d773e8a7f7a5f.png" /></p><p></p><p>《2023年AI现状报告》中的RLHF流行度图表。</p><p>&nbsp;</p><p>由于RLHF的实施难度比较大，所以大部分开源项目仍然采取指令微调的有监督微调方式。</p><p>RLHF的最新替代方案是直接偏好优化（DPO）。在相关论文中，研究人员表示RLHF中拟合奖励模型的交叉熵损失可以直接用于大模型的微调。根据他们的基准测试，DPO的效率更高，而且在对质量的响应方面一般也优于RLHF/PPO。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0788840d627217fe1376a32e3c2d53c7.png" /></p><p></p><p>DPO论文（<a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>"）中的注释图。</p><p>&nbsp;</p><p>但DPO似乎还未得到广泛使用。而令我兴奋的是，两周之前Lewis Tunstall及其同事通过DPO训练了首个公开可用的大语言模型，该模型的性能似乎优于由RLHF训练而成的大型Llama-2 70b聊天模型：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f92e39e0ea386cf0b9ebaeb68b302b51.png" /></p><p></p><p>Zephyr 7B模型公告截图。</p><p>&nbsp;</p><p>而且值得注意的是，RLHF并非专门用于优化基准性能；目前这种方法的主要用途仍是由人类用户评估模型的“实用性”和“无害性”。</p><p>&nbsp;</p><p></p><h2>分类专用模型</h2><p></p><p>&nbsp;</p><p>我上周刚刚在Packt生成式AI大会上做了演讲，特别强调目前文本模型最典型的用例之一就是内容分类。比如说垃圾邮件分类、文档分类、客户评论分类以及对社交媒体上的有毒言论做标记等等。</p><p>&nbsp;</p><p>根据个人经验，使用“小型”大模型（例如DistilBERT）完全可以在单个GPU上实现非常好的分类性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70c5d650c538d9caa218d1d73a257c7c.png" /></p><p></p><p>大家可以通过微调，将“小型”大模型用作文本分类器。</p><p>&nbsp;</p><p>我曾经尝试使用“小型”大模型进行过文本分类演练，其中的Sylvain Payot源自对现成Roberta模型的微调，并成功在IMDB电影评论数据集上实现了高于96%的预测准确率。（作为对比，我在该数据集上训练过的最佳机器学习词袋模型，其准确率也仅有89%。）</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f15b5e86e73c4bf29dfdec59e057e62b.png" /></p><p></p><p>我在深度学习基础课上讨论最佳分类模型。</p><p>&nbsp;</p><p>话虽如此，但目前我还没看到任何将大语言纳入分类场景的尝试或者趋势。大多数从业者在这类场景中仍然使用基于BERT的编码器模型或编码器-解码器模型，例如2022年推出的FLAN-T5。这可能是因为此类架构的效果已经足够令人满意。</p><p>&nbsp;</p><p></p><h2>表格数据集现状</h2><p></p><p>2022年，我写过一篇《表格数据的深度学习简史》（<a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html">A Short Chronology Of Deep Learning For Tabular Data</a>"），其中涵盖了很多关于深度学习的有趣表格数据方法。而且跟前面提到的分类大模型类似，表格数据集在这一年中同样没有多少进展……也可能是因为我太忙了，没有注意到。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b8327fce84a251066a0bc03a7dcc124.png" /></p><p></p><p>表格数据集示例。</p><p>2022年，Grinsztajn等人发表了名为《为什么树状模型在表格数据上仍然优于深度学习？》（<a href="https://arxiv.org/abs/2207.08815">Why do tree-based models still outperform deep learning on tabular data?</a>"）的文章。我相信对于中小型数据集（10k训练样本）上的表格数据，树状模型（随机森林和XGBoost）优于深度学习方法这个主要结论仍然正确。</p><p>&nbsp;</p><p>以该结论为基础，XGBoost在诞生近十年之后发布了2.0版本大更新。新版本拥有更高的内存效率、支持不适合内存存储的大型数据集以及多目标树等。</p><p>&nbsp;</p><p></p><h2>2023年计算机视觉现状</h2><p></p><p>虽然今年的重头戏都在大语言模型这边，但计算机视觉领域也取得了不少进展。考虑到本文的篇幅已经很长了，这里就不赘述计算机视觉的最新研究成果。具体可以看我在今年CVPR 2023大会上发表的这篇文章（<a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-10-state-of-computer">https://magazine.sebastianraschka.com/p/ahead-of-ai-10-state-of-computer</a>"）。</p><p>&nbsp;</p><p>除了研究之外，与计算机视觉相关的AI技术还激发出更多新产品和新体验，而且这一切都在2023年内逐步发展成熟。</p><p>&nbsp;</p><p>例如，当我今年参加奥斯汀召开的夏季SciPy大会时，就看到一辆真正无人驾驶的Waymo汽车在街道上驶过。</p><p>&nbsp;</p><p>而在观看电影时，我也看到AI在电影行业中得到愈发普遍的应用。比如《夺宝奇兵5》中哈里森·福特的去衰老特效，就是由制作团队利用演员旧素材训练出的AI模型完成的。</p><p>&nbsp;</p><p>此外，生成式AI功能现已广泛纳入知名软件产品当中，比如说Adobe公司的Firefly 2。</p><p>&nbsp;</p><p></p><h2>2024年展望</h2><p></p><p>终于来到最后的预测环节，这也是最具挑战的部分。去年，我预计大语言模型有望在文本和代码以外的其他领域迎来更多应用。这个结论基本得到证实，比如说DNA大模型HyenaDNA；另外还有Geneformer，这是一个由3000万单细胞转录组预训练而成的transformer模型，用于促进网络生物学的研究。</p><p>&nbsp;</p><p>到2024年，相信大语言模型将在计算机科学之外给STEM研究带来更加广泛的影响。</p><p>&nbsp;</p><p>另一个新兴趋势，则是随着GPU供应不足加之需求旺盛，将有更多企业开发自己的定制化AI芯片。谷歌将加大力度开发TPU硬件，亚马逊推出了Trainium芯片，而AMD可能会逐渐缩小与英伟达之间的差距。现如今，就连微软和OpenAI也在开发自己的定制化AI芯片，唯一的挑战就是各主要深度学习框架能不能为这些新硬件提供全面且有力的支持。</p><p>&nbsp;</p><p>至于开源大模型，其整体水平仍然落后于最先进的闭源模型。目前，最大的开放模型是Falcon 180B。但这应该不是太大的问题，因为多数人根本承受不了如此巨大模型所占用的海量硬件资源。正如前文所提到，我更希望看到由多个小型子模块组成的开源混合专家模型（MoE）。</p><p>我对众包数据集问题也抱持乐观态度，并相信DPO的崛起将给先进开源模型带来新的监督微调选项。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023">https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>