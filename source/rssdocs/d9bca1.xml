<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</id>
            <title>阿里云发布首个AI多模数据管理平台DMS，助力业务决策提效10倍</title>
            <link>https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，2024云栖大会上，阿里云瑶池数据库宣布重磅升级，发布首个一站式多模数据管理平台DMS：OneMeta+OneOps。该平台由Data+AI驱动，兼容40余种数据源，实现跨云数据库、数据仓库、数据湖的统一数据治理，帮助用户敏捷、高效地提取并分析元数据，业务决策效率可提升10倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32e0c3fcf153c23a1a30e0f8baf036e7.jpeg" /></p><p></p><p>阿里云副总裁、数据库产品事业部负责人李飞飞</p><p></p><p>“数据是生成式AI的核心资产，大模型时代的数据管理系统需具备多模处理和实时分析能力，以数据驱动决策和创新，为用户提供‘搭积木’一样易用、好用、高可用的使用体验。”阿里云副总裁、数据库产品事业部负责人李飞飞表示。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab91507c148beebff7e17bddc742ebc3.png" /></p><p></p><p>图：阿里云推出多模数据管理平台DMS：OneMeta+OneOps</p><p></p><p>当前，近80%的企业在建设数据平台时采用多种数据引擎、多数据实例组合的策略，AI兴起也带来了非结构化数据的指数级增长，给企业对数据的高效检索和分析管理提出了更大挑战。此次，阿里云重磅推出由“Data+AI”驱动的多模数据管理平台DMS：OneMeta+OneOps，助力构建企业智能Data Mesh（数据网格），提升跨环境、跨引擎、跨实例的统一元数据管理能力。</p><p></p><p>DMS创新设计了统一、开放、跨云的元数据服务OneMeta及DMS+X的多模联动模式OneOps。OneMeta首次打通不同数据系统，可支持全域40余种不同数据源，提供数据血缘和数据质量的一站式数据治理。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad49f198fa6c7ec433e77aca75ca93a0.png" /></p><p></p><p></p><p>OneOps则基于数据开发平台DataOps和AI数据平台MLOps，将不同数据库引擎（关系型数据库、数据仓库、多模数据库等）集结到统一平台，让用户“开箱即用”，实现全链路的数据加工和计算能力。</p><p></p><p>自上线以来，DMS已服务超过10万企业客户。借助跨引擎、跨实例管理和开发以及数据智能一体化，DMS将帮助企业从分散式数据治理升级至开放统一数据智能管理，可降低高达90%的数据管理成本，业务决策效率提升10倍。</p><p></p><p>李飞飞表示：“这是自云原生数据库2.0后，阿里云瑶池数据库又一次里程碑式的改造升级。DMS：OneMeta+OneOps为企业提供了全域数据资产管理能力，让业务数据‘看得清、查得快、用得好’。”</p><p></p><p>据介绍，极氪汽车采用DMS+Lindorm一站式多模数据解决方案，实现32万在线车辆上万车机信号数据的弹性处理分析，开发效能提升2倍，降低50%云资源成本。在大模型领域，此方案支撑月之暗面构建AI智能助手Kimi，帮助Kimi准确理解用户的搜索意图、整合与概述多种信息源，实现精准和全面的信息召回，提升用户交互体验。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea542c860a7b87c2dfe54757dd6ae8b8.png" /></p><p></p><p></p><p>此外，云原生数据库PolarDB今年首次提出基于“三层解耦, 三层池化”（存储、内存、计算）、AlwaysOn架构的多主多写和秒级Serverless能力，解决了多主架构中冲突处理和数据融合、以及Serverless秒级弹性租户隔离的难题。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e95f675bc1decd38d160c65fbe921c6e.png" /></p><p></p><p>本次云栖大会，阿里云瑶池还正式发布了云原生内存数据库Tair Serverless KV服务，是阿里云首个基于NVIDIA TensorRT-LLM的推理缓存加速云数据库产品。Tair采用NVIDIA TensorRT-LLM一起进行了深度优化。相比开源方案，该服务可实现PD分离/调度优化吞吐30%的提升 ，预计成本可降低 20%*注。</p><p></p><p>*注：基于Qwen2 7B模型在长上下文场景构造实验环境数据测试，最终效果以实际产品和场景测试数据为准。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</id>
            <title>JetBrains 与阿里云合作推出 AI Assistant，聚焦中国市场开发者</title>
            <link>https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:54:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9 月 19 日，在 2024 云栖大会上，全球软件开发工具提供商 JetBrains 发布基于阿里云通义大模型的 JetBrains AI Assistant，在完善其开发工具产品生态方面迈出了重要一步。</p><p></p><p>JetBrains《2023 开发者生态系统现状报告》调查结果显示，79% 的开发者认为处理代码是其工作中最耗时的环节。而 60% 的开发者已经开始使用、熟悉 AI 代码生成工具，用 AI 协助完成理解代码、检测并修正代码等繁琐的工作，让开发者专注于核心的编码任务，显著提升开发效率。</p><p></p><p>据介绍，JetBrains AI Assistant 与多款 JetBrains 产品深度集成，能够以高度的适配性完成代码生成与重构、回答和解释代码相关问题、撰写文档和提交信息等工作，助力中国本土开发者提升效率和代码质量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b988354041f18d4f4fabd8048961425c.png" /></p><p></p><p>并且，JetBrains AI Assistant 充分考虑了开发者的使用习惯，将 AI 辅助融入开发者的工作流中，同时实现了与 IDE 的无缝集成，将 AI 功能深刻融入对代码和上下文的理解，全面增强开发环境。在实际操作中，JetBrains AI Assistant 能够提供详尽的上下文信息和代码解释，帮助用户清晰理解 AI 的决策逻辑，从而显著提升操作的透明度和可追溯性。</p><p></p><p>在与阿里云的联合下，为中国本土用户量身打造的 AI Assistant 深度融合了中文自然语言处理技术，实现了中文指令与系统的直接交互，显著降低了使用门槛并提高了使用者的工作效率。此外，运用本土数据进行模型训练，能够在优化模型性能的同时，大幅降低训练成本。基于本土语言大模型，确保了数据处理的合规性和安全性，提供更加安全信赖、持续可靠的服务。</p><p></p><p>此外，JetBrains AI Assistant 在设计之初就将用户数据隐私与安全置于核心位置，不断深化安全性研究，严格遵守数据保护政策，未经授权不收集或泄露任何敏感信息，确保用户信息的安全。</p><p></p><p>大会上，JetBrains 除了带来全新首发的 JetBrains AI Assistant，还在展台上准备了丰富的互动和内容。展台特设“码脑讲堂”，邀请技术专家、KOL 通过直观演示和技术分享，解读 AI 在软件开发领域的实际应用与未来蓝图；解决方案展示区呈现了游戏开发、跨平台开发和服务器端开发的最新解决方案；趣味互动墙区域则通过创新装置提供沉浸式互动体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea2fd5b299d0919a7b716486644380a4.png" /></p><p></p><p>JetBrains 中国区总裁李玥萱表示，“中国市场在 JetBrains 的全球版图中占据举足轻重的地位。JetBrains 积极拥抱市场变化，确保我们的产品和服务始终与开发者的需求同频共振，助力他们突破效率极限。未来，我们期待与阿里云继续深化战略合作，不断探索和创新，为中国市场的用户提供更加强大、更加智能的工具和解决方案。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd91385fcb5521cfb12ca761250facf3.png" /></p><p>图：JetBrains 中国区总裁李玥萱</p><p></p><p>阿里云智能集团资深副总裁、公共云事业部总裁刘伟光表示：“今天，我们见证了云和 AI 的碰撞带来的一波全新浪潮。我们非常坚信，AI 的应用会席卷和渗透到各行各业。阿里云和 JetBrains 的战略合作发挥了衔接国际技术与中国市场的独特桥梁价值，让更多前沿的创新功能落地本土，我们愿与 JetBrains 共同开启 AI 在开发领域落地的新篇章。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/101abc01ca887189e95cbc99445abc2e.png" /></p><p>图：阿里云智能集团资深副总裁、公共云事业部总裁刘伟光</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</id>
            <title>阿里云首次推出云原生NDR产品 提升全流量威胁防御能力</title>
            <link>https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:22:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，在杭州云栖大会上，阿里云宣布云原生安全能力全线升级，首次发布云原生网络检测与响应产品NDR（Network Detection Response，简称NDR）。同时，阿里云还宣布将持续增加免费的安全防护能力，帮助中小企业客户以极低投入完成基础的云上安全风险治理。</p><p></p><p>云时代复杂的IT体系、碎片化的安全工具和传统的防护思路，以及新技术和新威胁带来的多重变化，让安全运营难以应对挑战。阿里云安全产品负责人欧阳欣表示，阿里云基于多年经验，创新性提出“三体”安全建设思路，将基础设施安全一体化、安全技术域一体化、以及办公安全和生产安全一体化贯彻到安全运营中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/479adc185863338978a64cc3ca88438a.jpeg" /></p><p></p><p>此次推出的阿里云云原生NDR，即是在此背景下的创新。NDR是基于公共云环境原生化部署的威胁检测与响应产品，全面提升了云环境全流量防御能力。与传统第三方产品不同在于，它无需部署即可即时开通，并通过创新的自动留存技术，可以针对攻击事件及攻击发生前后5分钟的流量进行取证保存，兼顾留存需要与成本投入，进而进行溯源和关联分析，帮助客户更快发现高级网络威胁。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/36b6336f76c2c28a67e8fb32a142b407.jpeg" /></p><p></p><p>基于基础设施安全一体化，阿里云还加强了WAAP、云安全中心、DDoS防护等能力，并且对数据库、网络CDN、计算、存储等云原生产品的安全能力也进行全新升级。</p><p></p><p>比如数据库与安全产品在数据安全上进行全面融合与能力共建，发布列加密与原生审计技术，可一键开通，增强自动化的安全能力。在CDN安全方面，阿里云将安全功能融入边缘网络，实现一键开启DDoS防护、WAF、Bot管理、API安全、SSL证书等功能，通过全球3200+节点提供原生安全能力，为用户提供边缘云网安全防护体验。</p><p></p><p>目前，阿里云已经成为Forrester、Gartner、IDC三大国际权威机构认可的全球安全能力最完整的厂商之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28a3eb882c0ea7b69192ba6d9c1d2cf0.jpeg" /></p><p></p><p>欧阳欣表示，“在做好平台安全建设同时，阿里云也免费开放更多的安全能力额度，包括云安全中心、内容安全、数据安全中心，让中小企业客户能够增强安全防护，同时还在安全体验上增加一键检测、一键修复等功能，帮助客户共同加入到云上安全维护中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebdb1fcbdda73c7a113745bf83a2b8e3.jpeg" /></p><p></p><p>面向AI，阿里云全新升级了安全体系，通义大模型基于阿里云的安全基座建设了生成式人工智能安全保障的最佳实践，将内容安全能力覆盖到大模型全生命周期中。同时，阿里云安全为百炼平台的专属部署模式设计了VPC安全保障方案，让客户在私域环境中也能获得数据确权归属等系列安全服务。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JhIxfejXZ1JK5uAz1RYE</id>
            <title>《阿里云安全白皮书2024版》发布：国内首推“安全共同体”理念</title>
            <link>https://www.infoq.cn/article/JhIxfejXZ1JK5uAz1RYE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JhIxfejXZ1JK5uAz1RYE</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:10:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，在杭州云栖大会上，阿里云发布2024版《阿里云安全白皮书》，正式将公共云安全责任共担的思路，升级为“云上安全共同体”理念，这意味着阿里云不仅会坚守安全责任共担模式下云服务商的责任，搭建和提供“安全的云”， 更会进一步与客户紧密合作，提供更多可供客户采取的安全保障措施，与云上客户共同形成一个紧密相连、 互相支持的安全防护网络，进一步造就云平台的运行安全。</p><p></p><p>阿里巴巴集团安全部总裁钱磊表示，阿里云除了为客户提供安全的云服务，还需要思考如何帮助客户“安全地使用云”，即利用更集中的技术投入、丰富的风险应对经验、完善的安全组织配备、先进的安全治理思路，打通客户安全体验的最后一公里，将平台的安全能力转化为客户侧实实在在的安全效果。“如果客户不安全，云平台就无法实现真正意义上的安全。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5b24e54bf1e91b93ce516bfe7c56c2a.png" /></p><p></p><p>据白皮书阐释，数智时代推动企业全面上云、深度用云的比例显著提升，这也对云上的安全治理体系提出了新的挑战。云计算初期，企业关注数据迁移等与常规IT流程相异的安全管理问题，为了建立企业与云服务商之间对云安全的理解与共识，“安全责任共担模式”应运而生，旨在促进双方在安全管理上的合作与协调。</p><p></p><p>随着数智技术与社会千行百业的深度融合，整体数字系统以及企业面临的安全挑战日益增多且更为复杂，需要不断提升安全防护、巩固安全机制、强化态势感知。因此，企业和云服务商都需要更高层次的安全理念来应对新的挑战。</p><p></p><p>“云上安全共同体”的安全理念，正是强调以保护客户云上资产安全为共同目标，云平台与客户密切配合，共同应对安全挑战。在理念引导下，阿里云会继续全权负责在安全责任共担模式下，需要云服务提供者承担的安全责任。如基础设施、物理设备、分布式云操作系统及云服务产品安全，保障云平台基座的安全。客户则主要负责自身数据、应用及账户安全。与此同时，云平台也会发挥主观能动性，提供一系列切实可行的安全保障措施，帮助客户更深入地思考、制定、理解安全策略，并支持这些安全策略更顺畅、便捷地落地实施。</p><p></p><p>以云产品安全配置为例，阿里云在设计和开发云产品时会严格保证安全性，并在云服务中内嵌客户可自行配置的安全能力。客户在使用云服务时，则需要根据上云数据情况、系统的业务场景自行完成安全配置。在安全共同体理念的引导下，阿里云将在客户使用云产品之初预设更多初始的安全配置与风险提示，使客户在更安全的环境中构建业务，从而避免很多因疏忽或不当操作引发的安全风险。</p><p></p><p>本次发布的《阿里云安全白皮书2024版》全面阐释了阿里云安全保障措施建设工作，包括对于构建平台核心安全保障的八大支柱，详细描述如何建设“安全的云”，同时书中还提供了阿里云服务的行业及业务场景云上安全实践，希望帮助客户加强对云上安全治理的理解，探索符合自身场景的云上安全最佳实践。</p><p></p><p>除此之外，阿里云将围绕帮助用户“安全使用云”陆续发布数项新的安全服务和能力，如对于云产品设置更高的初始安全水位，提高云产品使用的安全性；通过提供更普惠的安全能力，促成更低的用云安全成本；通过增强多方位的安全检测和防护能力，实现安全事件的主动响应，通过提供全面而易于理解的安全指南和最佳实践，推动安全科普与安全意识的培育，使用户能够清晰地了解云服务的安全性能和潜在风险，做出明智的决策，共同为数智化趋势下的社会安全稳定运行贡献力量。</p><p></p><p>更多详情可下载《阿里云安全白皮书2024版》：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c6466843825ee47f7cfe3bb6b9bed86.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/oWw2vtGlXIaleoKdEguV</id>
            <title>魔搭上线AIGC专区，首批上架157个风格化大模型，专业文生图全免费</title>
            <link>https://www.infoq.cn/article/oWw2vtGlXIaleoKdEguV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oWw2vtGlXIaleoKdEguV</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:06:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在杭州云栖大会上，魔搭社区宣布正式上线AIGC专区，为开发者提供从模型到应用的一站式AI创作开发平台，目前所有功能板块及GPU算力全部免费开放。</p><p></p><p>魔搭AIGC专区首批上架157款精选多模态模型，其中既有FLUX、Stable Diffusion、RealVisXL、万象熔炉等社区热门模型，也包含众多设计师贡献的黏土风、像素风、漫画风、超现实主义、线条手绘等小众风格化LoRa模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e56fa65ffec05b22e88c3b0e5043f12e.png" /></p><p></p><p>在魔搭AIGC专区，除了直接下载模型，开发者还可在线调用各类模型进行快速生图，支持AI自动翻译和优化咒语，给出正向提示词及负向提示词，还可以对采样方法、提示词引导系数、随机种子、采样步数、图片尺寸等参数进行调节。同时，魔搭AIGC生图支持图生图、局部重绘、Adetailer人脸修复、ControlNet细节调控等深度功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae8f0d4440b1914e4360b1fd6744d77d.jpeg" /></p><p></p><p>对于有更专业需求的开发者或设计师，魔搭AIGC专区支持批量上传图片对模型进行LoRa微调，目前已上线Stable Diffusion、Q版IP、动漫、写实风等多个模版，最低10张图片即可完成模型训练。用户还可以在魔搭AIGC专区在线调用ComfyUI工作流，创建或复用相关模版来进行创作。后续，魔搭AIGC专区也将上架视频、语音等更多模态的模型和应用，为开发者提供最优的一站式AIGC体验。</p><p></p><p>据了解，自2022年云栖大会上发布以来，魔搭社区目前已成为国内规模最大、最活跃的AI模型社区，汇聚超过10000款优质模型，为超过690万用户提供了模型及免费算力服务。</p><p></p><p>魔搭AIGC专区网址：</p><p><a href="https://www.modelscope.cn/aigc/home">https://www.modelscope.cn/aigc/home</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7NCAjhYXOVQIQzSxewlN</id>
            <title>阿里云刘伟光：云之于AI，绝不仅仅是算力供应</title>
            <link>https://www.infoq.cn/article/7NCAjhYXOVQIQzSxewlN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7NCAjhYXOVQIQzSxewlN</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Sep 2024 09:04:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>编者按：本文是阿里云智能集团资深副总裁、公共云事业部总裁刘伟光在 9 月 19 日开幕的 2024 云栖大会上的演讲实录。在题为《加速 AI 原生时代》的演讲中，刘伟光分享了他对大模型、云和 AI 关系以及 AI 原生架构的思考。这里面有很多他基于技术前沿和市场实践而总结出的洞察，角度新颖，指向性强。</blockquote><p></p><p></p><p>以下是演讲实录：</p><p></p><p>在开启今天的分享之前，先讲一个花絮。</p><p></p><p>我第一次登上云栖大会讲台是在 7 年前，也就是 2017 年 11 月 11 号，也是在这个会场。</p><p></p><p>7 年前我讲的话题是云的分布式架构、分布式中间件、分布式数据库、容器化，微服务改造，那时候这些想法还只是一种构想和畅想。7 年后，这些技术已经成为我们正在不断实践的标准。</p><p></p><p>当年，我记得翻看 PPT 时看到一些对 AI 的描述，就像很远方的一点星光。今天，AI 时代的发展就像一个正在照亮我们前行的明灯，让我们一起拥抱这个崭新的 AI 原生时代。</p><p></p><p></p><h2>1. 大语言模型的爆发是过去所有技术的继承和优化</h2><p></p><p>今天随着大语言模型在全球的爆发和应用，其实我们要思考一个问题，大语言模型不是平地起高楼的完全新兴事物。今天它是在过去算力不断迭代、数据量的不断扩充的基础上，不断优化和迭代的一种技术产物。</p><p></p><p>ChatGPT 发布在 2022 年的 11 月，大家想如果发布在 2012 年的话，那仅仅就是一份 paper。因为那个时代 GPU 的算力，包括数据的能力，数据容量都不足以支撑大语言模型训练。应该说，大语言模型的爆发是过去所有技术的继承和优化。</p><p></p><p>今天，我们的交互模式是人和人的交互，人和数字世界的交互，在未来通过大语言模型的赋能，包括长文本这些新的技术不断演进，我们有理由相信今天正在以菜单、按键打交道的这些终端设备、物理设备，将以对话、自然交互的模式跟人类交流。</p><p></p><p>我们看到今天的 AI 爆发不仅仅在数字化的虚拟世界，更多是诞生在数字化世界叠加对物理世界的优化甚至重塑。</p><p></p><p></p><h2>2. 四个“确定”不等式</h2><p></p><p>未来，AI 会带来什么样的变化？我们通过跟各种客户的合作实践，总结出了四个不等式，代表了我们对 AI 的未来非常笃定的方向。</p><p></p><h4>第一，数据的不等式。</h4><p></p><p>仅仅在去年，AI 产生的数据就已经大大超过了过去几十年来通过计算机产生的数据。Gartner 预测到 2030 年 AI 生成的合成数据将远超过人类过去生成数据的总和。AI 在过去一年所生成的图像已经超过了过去 150 年人类拍摄的所有照片的数量，未来更多的数据将由 AI 产生，这是一个非常确定的趋势。</p><p></p><h4>第二，算法的不等式。</h4><p></p><p>在大语言模型当中有一个重要的分支，就是 Coding 大模型，今年阿里云已经上线了第一个通义灵码员工，帮我们在编写代码、校验代码、优化代码。今天这些代码的采纳率已经接近 30%，跟客户的实际使用率非常相似。我们非常有理由相信在未来 1-3 年，整个 Coding 大模型生成的高质量代码将超过程序员所编写的代码。我们这一代人很可能是最后一代大规模编写程序的技术工程师。</p><p></p><h4>第三，应用的不等式。</h4><p></p><p>今天无论是 App Store，还是各种应用市场，正在诞生很多全新嵌入 AIGC 能力的 AI native application，加速度非常快。在未来，会有更多的公司去开发新兴的 AI 原生应用，这些应用将完全采用新的大模型能力，基于 GPU 推理集群上进行开发。同时，老的应用也会嵌入更多的智能体，提升整个智能化体验。未来所有的应用程序都可能被 AI 所重写，或优化。</p><p></p><h4>第四，交互的不等式。</h4><p></p><p>今天有手机和汽车的融合，也有汽车和机器人的融合，汽车在中间扮演了一个非常重要的纽带作用，未来这是有紧密协同效应的产业链。更多终端诞生之后，我相信所有的交互，人类跟很多机器终端的交互都将采用拟人的自然交互模式，更多的自然交互模式将彻底改变现在按键式、菜单式的交互。</p><p></p><p></p><h2>3. 云和大模型 AI，是类似电和电机的关系</h2><p></p><p>在这些改变背后，我们看到任何一个企业，当它去拥抱云计算，当它去开发智能体，当它去开发新的 AI native application，它所需要的架构绝对不仅仅是购买几十台、上百台 GPU 推理训练服务器这么简单的工作。</p><p></p><p>有一句话非常流行，说人工智能的尽头是电力能源供应，这句话虽然有夸大的成分，但有非常确切的理论依据。AI 智能应用的能耗密度要比传统 IT 高出 10 倍以上，整个 GPU 服务器能耗是传统 CPU 能耗的 10-30 倍。当你在类似通义千问这样的大模型应用上计算一道高等数学题时，背后消耗的电量是利用普通搜索引擎检索的近10倍。对于很多企业来讲，去大规模构建智能化应用的时候，持续稳定电力供应是非常大的挑战。</p><p></p><p>再来看算力，算力不仅仅是搭建 GPU 服务器这么简单。大模型的 Scaling Law 原理依然非常有效，但大的 GPU 集群不等于大的算力，1000 台 GPU 的算力不等于 1 台 GPU 服务器乘以 1000 倍。大模型推理和训练过程当中，网络的消耗、I/O 的操作几乎占据了大模型训练推理一半时间。这就意味着对于企业架构来讲，优化网络和 I/O 对提升大模型的效率、使用效果是至关重要的，这里如何构建一个高性能的网络，对于大模型的效率提升是非常重要的。</p><p></p><p>再看上层应用，当我们在一款 APP 端去发布一个火爆应用，无论是文字、文本、图片、图像，或者是推理计算的时候，会引发千万人、上亿人在同一秒钟涌进 APP 进行尝试。这对应用背后的性能支撑、弹性能力挑战是非常强的，类似于极限“秒杀”。所以，在 AI 原生应用的背后需要有很强的云的特征，低延时、高弹性、应对波峰波谷。</p><p></p><p>综上所述，今天当我们去拥抱 AI 原生应用、智能体时候，对原有的企业架构绝不仅仅是 GPU 服务器叠加这样简单的事情，它带来的挑战有算力规模化、网络 I/O、高性能存储、电力持续供应、低延时，包括应对上层爆发式业务的弹性能力。</p><p></p><p>为什么说云和大模型 AI 就是类似于电和电机的关系。今天云的分布式架构能力、全球部署能力、全球一张网的高速通信能力，以及高性能的存储和网络处理能力，以及节能、绿色环保能力，服务器使用效率等等，相比传统企业架构，是更适合为 AI 应用爆发和迭代提供非常有力的全方位支撑。</p><p></p><p></p><h2>4. 云计算范畴早已经突破了当初的理念</h2><p></p><p></p><p>接下来，我们换另外一个视角，分享一下企业架构在升级时候面临的挑战：AI 和云原生的融合。</p><p>云计算大约诞生在 2006 年，容器化技术大约诞生在 2014 年。云原生随着容器化、CI/CD、微服务，DevOps 架构的兴起，整个 Cloud Native 的理念应运而生。但今天回头看 Cloud Native 理念是一个相对狭义的概念，它更加强调在软件开发的研发、测试、运维形态的升级，这些已经被完整吸纳到广义的云原生当中去。站在真正云计算公司角度看，仅有软件开发能力是不够的。今天云计算范畴早已经突破了当初 IaaS、PaaS、SaaS 理念，它往下已经延伸到芯片指令级，多种异构算力的供应，往上已经越过 SaaS 层而延伸到了 AI MaaS 的层面。</p><p></p><p>当初云计算诞生的时候，视频化技术还没有广泛地流行起来。云计算诞生的时候，整个开源体系，包括大数据、数据库类的开源技术还没有今天这么丰富。今天的云计算考虑的不仅仅是在研发态、部署态、运维态这些层面，还必须考虑从底层的异构算力到网络存储再到到上层的开源技术体系，以及对研发运维体系、云原生架构等技术的全面拥抱。</p><p></p><p>当面对新的 AI 挑战时，出现了一个新的概念：AI 原生架构。当我们去构建新的 AI 应用或者 AI 智能体时，需要包括高性能的网络、存储等弹性基础设施，底层的基础大模型能力，弹性的 API 能力，以及模型服务工具能力、微调工具、向量数据库等新的能力，这才是一个 AI 原生应用应有框架。</p><p>应该说今天的 AI 原生架构既要集成广义云原生的范畴，同时兼具 AI 需要的模型层的技术能力，合在一起就组成了面向的 AI 时代原生技术架构蓝图。</p><p></p><p></p><h2>5. 云提供 AI 支撑，绝不是简单提供 GPU 的推理、训练集群</h2><p></p><p>当 AI 原生架构诞生的时候，它对云计算带来了什么样的反哺效应呢？</p><p></p><p>第一，所有大模型是以向量为单位进行数据处理的，在云原生数据库层面我们增加了对向量的支持。同时我们在 AI 大语言模型时代，为传统广义云原生增加了代码生成、智能运维、智能监测、智能建模等新工具。所以，云和 AI 相互促进，云为 AI 提供了弹性的高可用基础设施。同时，AI 为云带来了智能化的运维、体验、监测、建模能力，以及对数据化能力的重构。</p><p></p><p>这两年，阿里云应对 AI 的爆发趋势在基础架构层面做了非常多的技术升级，在高性能计算、存储、网络、调度，包括整个智能监控运维上做了非常多的能力升级。</p><p></p><p>今天云计算公司提供 AI 支撑绝对不简单是提供 GPU 的推理训练集群，我们要考虑多种 GPU 之间的通信和协同效应，不同应用跨数据中心之间的协同效应。实际上我们需要将万卡，甚至十万卡推理或训练集群构建成一台超级计算机的同时必须具备异构芯片的协同调度能力。</p><p></p><p>第二，高性能网络。在大语言模型的训练推理过程当中，其实网络 I/O 操作消耗的电量和消耗的成本是非常高的。实际上，集群有效算力利用率会随着规模增长而下降，面对这样的挑战，我们做了很多优化工作，包括一个 IDC 内连接超过十万卡形成一个算力集群，将算力集群的效率提升到 90% 以上。</p><p></p><p>大模型的推理和训练是需要将很多相关任务进行拆分，并且执行多种并行的调度策略，这种并行调度策略对资源有效共享带来了很大挑战。今天云原生的新统一调度引擎支持大规模训推一体的算力集群调度，多种异构算力集群之间的混合调度，动态调度策略调整，以及很重要的一点，就是对于低延时、延迟敏感的应用实现就近推理服务。</p><p></p><p>在智能监控层面，大规模的集群不免遇到故障，所以对于主动式故障监测、故障自愈也是我们重要的提升方向。今年我们增加了全栈式的监控指标，单机内的毫秒级发现，以及故障的分钟级发现。这些能力，包括大规模训练任务，秒级的 check point 的检查机制，实现对整个高性能集群无感的修复。</p><p></p><p>综上说述，这就是云计算为什么在 AI 时代更有优势？云面向 AI 的能力提升，能为 AI 应用开发、应用部署提供最强有力的支持和最好的客户体验，AI 应用追求的不仅仅是体验，还要兼具企业应用的基础特征、高可用、业务连续性、数据强一致性等，这才是真正创造社会价值能够更广泛被使用的 AI 应用的核心要素。</p><p></p><p></p><h2>6. AI 和数据双向赋能</h2><p></p><p>前面提到，今天 AI 大语言模型的爆发其实是对过去所有技术的继承和优化，尤其是在数据层面。在没有 AI 爆发之前，很多企业都构建了不同类型数据的处理平台，包括数据仓库、数据集市、不同类型数据库，处理不同业务的数据管理平台。</p><p></p><p>我们在数据和 AI 层面需要做两件事情：一是如何用这些既有的数据源实现对 AI 大模型完整输出链路的打通；二是 AI 和数据的双向赋能。</p><p></p><p>今天，AI 和数据的关系不完全是云给 AI 提供单向供给数据的。AI 对数据工程有非常多赋能，可以将原有数据平台和 AI 结合产生更有业务价值的结合。很多客户将我们的大语言模型能力和跟传统原有 BI 能力进行结合，让 BI 进行 AI 化升级，让企业管理者更方便地、定制化地去看他们所需要的数据，并给予辅助的决策建议。</p><p></p><p>AI 大语言模型对数据工程进行深度优化：智能 Copilot&amp;Agent 赋能灵活找数、数据开发效率提升 2 倍；智能数据探查、智能建模、智能全域数据集成、主动式数据资产治理等。</p><p></p><p>数据向 AI 赋能也不仅仅是供数这么简单，大语言模型的数据单位是向量。今天阿里云为了支持大语言模型的发展，在 OSS、RDS、Polar DB 等核心数据产品中，全部都支持向量数据处理能力。同时，在数据库当中增加了新的能力，实现推理过程的数据缓存。</p><p></p><p></p><h2>7. 完整的 AI 开发范式架构图</h2><p></p><p>随着行业发展，技术的发展和迭代，我们有了一些面向 MaaS 层架构的实践思考。这里是面向未来的参考架构图。这张图清晰体现了 MaaS 每一层的功能，也是企业需要的开发技术能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad11e7ae09786598cf13f7409e72f081.webp" /></p><p></p><p>对 MaaS 层而言，应该提供的是端到端的 AI 原生应用构建。</p><p>基于底层 AI Infra、原生 Data+AI 的多模态数据架构升级，为 AI 原生应用提供强大的基础平台，支持 AI 原生应用高效稳定运行、灵活的部署选择；基础大模型层面 ：以阿里云为例，我们推出“通义千问、通义万相”等大模型 、以及大量第三方开源模型，完整覆盖各类 AI 原生应用场景需求；模型服务：模型服务平台应该可为用户提供灵活、弹性的大模型 API 和定制服务 ，覆盖诸多业内领先的开源大模型 ，帮助用户快速基于大模型构建生成式应用；大模型智能体应用构建：一站式大模型应用生产工具百炼，为用户提供快速、低成本的大模型智能体应用开发标准化方案，整合大模型应用链路中各种工具链、插件、提示词工程模板等，让用户能将大模型的强大能力快速应用到自己的业务中；再往上的“专属大模型”领域 ，应该是帮助用户结合行业数据和企业私有数据，进行微调和训练专属大模型 （企业专属知识中心），生成个性化 API 为上层各类应用提供服务从部署选择方面：从下至上可以支持多种部署选择组合，如地域 +AZ 选择 、开源 / 闭源选择、公共云 /VPC 部署 / 私有化选择、训练推理统一部署、云与端侧推理部署选择等。</p><p></p><p>图的右侧，我们支持了对整个模型训 - 推一体的部署、云端一体的部署、地域 +AZ 选择 、开源 / 闭源选择、公共云 /VPC 部署 / 私有化选择、训练推理统一部署、云与端侧推理部署选择等，方便客户在全国各地，甚至全球快速开发应用、部署应用。</p><p></p><p>我们认为，这样的 AI 开发范式是包括底层算力，到上层应用的更为完整的范式。</p><p></p><p></p><h2>8. AI 原生架构的 9 大要素</h2><p></p><p>AI 爆发对数字世界和物理世界带来改变和冲击，进而带来对于企业架构的冲击，对未来的数据、开发、代码、应用、交互模式上的改变。在 AI 原生架构当中，在实践中我们发现有 9 个要素是非常重要的，这些既来自阿里云自身的实践，也来自于客户共创。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e17fe1b324d60aceca89c135aad7eef.webp" /></p><p></p><p>这是我们总结的 AI 原生架构的 9 个方面建议，希望这些实践对大家实际工作有所借鉴。</p><p></p><p></p><h2>9. AI 和云的融合将能改变数字世界加物理世界</h2><p></p><p>最后，跟大家分享一下我们的行业实践。</p><p></p><p>汽车领域是我们非常重视的领域。因为新能源汽车对 AI 和云的需求，已经从原有的车联网延伸到自动驾驶、智能座舱。AI 将对汽车，甚至是机器人领域有一个全新的架构升级和重塑。我们相信自动驾驶一定就是未来最有价值的智能体应用。</p><p></p><p>在大模型领域我们和中国一汽合作，实现在 BI 领域全面升级。自动驾驶领域，我们跟小鹏汽车展开深度合作。我们希望通过在算力、模型、技术架构方面的合作，让中国汽车行业不仅给客户提供更好的体验，还帮助汽车行业将更好的客客户体验、智驾体验带到全世界。</p><p></p><p>游戏行业是阿里云最早的一批客户，我们伴随很多客户从中国走向世界，像米哈游这样的企业已经从创业公司变成了世界顶尖的手游公司。今天我们服务的游戏类型也覆盖到了手游、页游，端游等等。在服务众多游戏客户过程中，游戏体验的不断提升也带给阿里云在游戏领域的不断加强的技术积累，游戏正在原有的模式延伸到新的千人千面时代。网易、巨人网络等已经开始大量使用大模型，给玩家提供开放式的结局和不一样的体验。云和 AI 和数据的结合，将给游戏行业带来新一步的升级。</p><p></p><p></p><h2>10. 结语：</h2><p></p><p>今年是阿里云的第 15 年，我们走过了三个阶段，第一个阶段是企业上云，是一些传统企业和 PC 端网站。第二个阶段，我们陪伴了第一批移动互联网企业崛起，这些企业是真正的云原生企业，我们也伴随他们从中国走向海外。今天，我们正处于第三个阶段，AI 和云的融合。正如阿里巴巴集团CEO、阿里云智能集团董事长兼CEO吴泳铭所讲，它带来的改变不仅在于手机屏幕，它将能改变数字世界加物理世界！</p><p>我和在座的从业者，有幸见证了第一代和第二代的发展，我们正在一起拥抱第三代 AI 和云的结合。希望我们一起加速推动这个伟大时代的变革。</p><p></p><p>感谢大家！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HqvQxLedVcZhxLTLiJqG</id>
            <title>阿里云「通义灵码」迎来重磅升级，「AI 程序员」正式亮相！</title>
            <link>https://www.infoq.cn/article/HqvQxLedVcZhxLTLiJqG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HqvQxLedVcZhxLTLiJqG</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Sep 2024 03:30:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近两年，随着大语言模型和生成式AI技术的爆火，软件开发领域首当其冲成为了最热门的大模型应用场景之一，GitHub Copilot、通义灵码等 AI 辅助编程工具纷纷问世。这些工具通过自然语言处理和机器学习技术，能够理解开发者的意图，并且提供行级/函数级代码、单元测试和代码注释的智能生成等功能，极大地提高了开发者的编码效率和代码质量。</p><p></p><p>以通义灵码为例，数据显示，过去一年通义灵码插件下载量超 500 万，每日辅助开发者生成代码超 3000 万次，累计生成代码超 10 亿行,已被广泛应用于金融、制造、互联网、交通、汽车、能源等行业，成为国内最受欢迎的辅助编程工具。</p><p></p><p>随着大模型的持续进化，在语义理解、代码生成、开发工作流等方面的能力也获得了持续、全面的提升，辅助编程工具有没有可能像汽车的自动驾驶一样，只需要自然语言交互，就能实现“自动编程”呢？</p><p></p><p>此次云栖大会上，阿里云给出了它的答案——通义灵码「AI 程序员」。</p><p></p><p>9 月 19 日，2024 云栖大会在杭州拉开帷幕。会上，阿里云宣布「通义灵码」重磅升级，从「辅助编程」工具，进化到能自主执行任务拆解、代码编写、缺陷修复、测试等任务的「AI 程序员」，最快分钟级完成从 0 到 1 的应用开发，提升数十倍开发效率。</p><p></p><p>据介绍，「AI 程序员」是基于通义大模型的 AI 智能体，可以自主执行任务拆解、代码编写、缺陷修复、测试等编程相关任务。相比于编程助手，「AI 程序员」可以脱离 IDE 软件，像真人程序员一样执行缺陷修改、需求分析、代码实现、问题排查等任务工作流，同时具备架构师、开发工程师、测试工程师等多种岗位技能，最快可分钟级完成应用开发。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f72a75b196138933f47bf297f523a8a1.png" /></p><p></p><p></p><p>交互层面，由于「AI 程序员」是基于通义大模型构建的多智能体，每个智能体能够分别负责具体的软件开发任务并互相协作，用户只需要参与“需求输入”、“确认计划”、“确认实现”三个步骤，即可完成一个端到端的产品功能研发。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/28/d5/28yyab12ddc493d5061d7e59bef22bd5.gif" /></p><p></p><p>例如，当系统出现 BUG 时，开发者只需要将问题链接丢给「AI 程序员」，它就能自动读取问题，进行代码库拉取、问题分析等动作，并基于分析结果生成解决计划。同时，凭借着首创的代码仓库知识图结构，「AI 程序员」不仅能理解用户的需求，还能精准定位代码对应的修改位置并自动给出修改方案。开发者可以直接查看「AI 程序员」定位的代码文件进行勘误或者给出优化建议，促使「AI 程序员」进行代码调优，确认无误后再点击执行代码变更。另外，「AI 程序员」还会贴心地生成代码合并请求标题及表述，只需要开发者最后确认提交即可。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/96/8e/964140597f36c0979de546f8de17d08e.gif" /></p><p></p><p>除了“缺陷自动修复”功能，当开发者有新的开发需求时，同样可以将需求描述给「AI 程序员」并选择模板，AI 程序员能够自动分析需求、制定方案，之后开发者就可以像跟 Chatbot 沟通一样去提出优化建议，直至方案完美落地。</p><p></p><p>此外，「AI 程序员」跟「编码助手」一样，支持“研发问答”功能，该功能基于海量研发文档、产品文档、通用研发知识、阿里云的云服务文档和 SDK/OpenAPI 文档等进行问答训练，能够高效、准确地帮助开发者答疑解惑。</p><p></p><p>当然，尽管通义灵码「AI 编码助手」和「AI 程序员」都是基于千问大模型而开发的智能编码工具，但表现形式上有所不同。</p><p></p><p>首先是产品形态上，「AI 编码助手」主要是以 IDE 插件的形式存在于各类开发工具中，而「AI 程序员」除了可以是 IDE 插件，还能以网页或软件的形式单独使用；其次是交互方式上，「AI 编码助手」主要是辅助性质，帮助开发者解决单点开发问题，而「AI 程序员」则更像是一个具备独立开发能力的开发者，用户只需要以自然语言输入需求并做一定的代码校验工作，即可自动完成开发任务。</p><p></p><p>阿里云表示，得益于通义灵码「AI 编码助手」过去一年沉淀的大量的技术、模型能力，「AI 程序员」具备更智慧、更高效、更敏捷的特性。同时，基于 「AI 程序员」、「智能编码助手」、DevOps 工具云效等工具链，阿里云能够为开发者提供研发增效的联合解决方案。</p><p></p><p>对于普通开发者而言，「AI 程序员」的出现能够更进一步地助力研发提效、解放双手，让开发者能够更聚焦于更有价值、更有创造力的开发工作中去，大量简单、重复、高频的日常开发、运维等工作将由「AI 程序员」去完成，开发者此时只需要扮演“安全员”的角色做节点性管控即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4ca6f6d505968e987dde7c244382467e.png" /></p><p></p><p>对于初学者或者非专业人士而言，「AI 程序员」则进一步帮他们降低了开发应用的门槛。云栖大会现场，一位 13 岁的中学生在通义灵码上输入几句话，2 分钟就生成了一个 python 语言编写的倒计时网页；现场还有参会者，用通义灵码修改开源魂斗罗游戏代码，在 9 个代码文件 2000 多行代码里，几分钟就精准修改了游戏角色的生命值、跳跃高度等参数（详细 Demo演示附在文末）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/66/66dc5bb059cef29e5ef73b41a9c0dba0.png" /></p><p></p><p>主论坛现场，阿里云智能集团 CTO 周靖人宣布 Qwen2.5 全新升级，为用户提供全尺寸模型的选择，灵活权衡模型精度与成本。据介绍，Qwen2.5 的预训练数据量超18万亿token，数学和代码能力大幅提升，支持 128k 输入 8k 输出，可以快速生成万字长文，prompt 稳定性、指令遵循能力也获得了持续增强等。这些能力的提升也将大幅增强通义灵码「编程助手」和「AI 程序员」的能力。</p><p></p><p>“通义灵码正在不断地整合「AI 程序员」的能力，希望能够帮助程序员完成日常功能的开发，更有效地提升工作效率。”周靖人表示。</p><p></p><p>其实，从今年3月海外初创公司 Cognition 发布的“全球首个AI工程师——Devin”，到阿里云通义灵码「AI 程序员」的推出，都预示着软件开发正在逐步从 Co-Pilot 走向 Auto-Pilot 的时代。在 Auto-Pilot 模式下，AI 将拥有更高的自主性，能够独立完成更复杂的编程任务，从需求分析到代码实现，甚至问题排查和修复，都可能在无需人类干预的情况下完成。这一转变不仅会带来软件开发效率的极大提升，也可能引发开发者角色和技能要求的变革。</p><p></p><p>未来，人类与 AI 的协同工作将成为软件开发的常态，开发者可能需要更多地专注于架构设计、创新算法开发和 AI 系统的管理与优化，从传统的编码工作转向更高层次的创造性或技术领导类工作。对于开发者而言，这既是机遇，也是挑战。如何快速适应这一变化，不断提升自己的技术能力和创新思维，并学会利用 AI 的力量更好地改变世界，是值得当下所有开发者思考的问题。当然，阿里云通义灵码「AI 程序员」或许是一个快速了解未来工作方式并上手实践的不错路径。</p><p></p><p></p><p>DEMO 1：13岁中学生现场用通义灵码编写倒计时网页</p><p></p><p></p><p></p><p>DEMO 2：通义灵码“魔改”开源游戏《魂斗罗》</p><p></p><p>*本视频仅做「AI 程序员」的交互展示，不鼓励游戏开挂。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mA2A6zTy4gwu8QzTp4bd</id>
            <title>小鹏汽车加速端到端自动驾驶落地 ，深化与阿里云AI算力合作</title>
            <link>https://www.infoq.cn/article/mA2A6zTy4gwu8QzTp4bd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mA2A6zTy4gwu8QzTp4bd</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月19日，小鹏汽车董事长 CEO何小鹏驾驶“全球首款AI汽车”P7+亮相2024云栖大会，这款车搭载了业内领先的端到端大模型。过去2年，小鹏汽车与阿里云共建的AI算力规模提升超4倍。何小鹏表示，将继续深化与阿里云的AI算力合作，加速推动端到端大模型拓展自动驾驶上限，提升下限。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91250d7ec6314e6818ae9394d7c9bf06.jpeg" /></p><p></p><p></p><p>端到端是当下最受业界关注的自动驾驶解决方案，它同样遵循Scaling Law。小鹏汽车能率先实现端到端大模型量产上车，离不开在算力上的提前布局。</p><p></p><p>为提升智驾大模型训练效率，小鹏汽车早在2022年就携手阿里云在乌兰察布建成中国最大的自动驾驶智算中心，将自动驾驶模型训练效率提升了超600倍。而近两年内，由于大模型技术快速发展，阿里云已将此智算中心的算力储备扩张超4倍至2.51Eflops，为小鹏汽车提供稳定高效的算力底座。</p><p></p><p>今年5月，小鹏汽车就在国内率先实现端到端自动驾驶量产上车，并在全国范围内迅速落地。业界普遍认为，未来端到端智驾的算力需求还将进一步扩大，上亿元投入仅是智驾算力的入场券。小鹏汽车宣布，每年投入 35 亿元用于研发，其中 7 亿元用于算力训练，还将与阿里云持续深化合作，加速推动端到端大模型落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe47a037f2bc0039c038931dda9a71e0.jpeg" /></p><p></p><p></p><p>作为“All in AI”的车企，小鹏汽车不仅实现了端到端大模型量产上车，还将大模型深入应用到了座舱场景中。小鹏汽车基于自主研发的“全域大语言模型”X-GPT及通义千问全面升级了车载助理。小鹏汽车在官方App中已接入通义万相，在研发场景中，通义灵码的代码评审采用率高达50%。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VZyuMPqM7hRR1wf341w6</id>
            <title>通义千问重磅开源Qwen2.5，下载量突破4000万</title>
            <link>https://www.infoq.cn/article/VZyuMPqM7hRR1wf341w6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VZyuMPqM7hRR1wf341w6</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月19日云栖大会，阿里云CTO周靖人发布通义千问新一代开源模型Qwen2.5。Qwen2.5全系列涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型，每个尺寸都有基础版本、指令跟随版本、量化版本，总计上架100多个模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eef0fb2899093373025ad81bb4b5f1fa.jpeg" /></p><p></p><p>Qwen2.5全系列模型都在18T&nbsp;tokens数据上进行预训练，相比Qwen2，整体性能提升18%以上，拥有更多的知识、更强的编程和数学能力。Qwen2.5-72B模型在MMLU-rudex基准（考察通用知识）、MBPP&nbsp;基准（考察代码能力）和MATH基准（考察数学能力）的得分高达86.8、88.2、83.1。</p><p></p><p>Qwen2.5支持高达128K的上下文长度，可生成最多8K内容。模型拥有强大的多语言能力，支持中文、英文、法文、西班牙文、俄文、日文、越南文、阿拉伯文等&nbsp;29&nbsp;种以上语言。模型能够丝滑响应多样化的系统提示，实现角色扮演和聊天机器人等任务。在指令跟随、理解结构化数据（如表格）、生成结构化输出（尤其是JSON）等方面Qwen2.5都进步明显。</p><p></p><p>语言模型方面，Qwen2.5开源了7个尺寸，0.5B、1.5B、3B、7B、14B、32B、72B，它们在同等参数赛道都创造了佳绩，型号设定充分考虑下游场景的不同需求，3B是适配手机等端侧设备的黄金尺寸；32B是受开发者期待的“性价比之王”，可在性能和功耗之间获得最佳平衡，Qwen2.5-32B的整体表现超越了Qwen2-72B。</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/902a65758ba6d50325ce6613c5f08bc9.jpeg" /></p><p>在MMLU-redux等十多个基准测评中，Qwen2.5-72B表现超越Llama3.1-405B</p><p></p><p>72B是Qwen2.5系列的旗舰模型，其指令跟随版本Qwen2.5-72B-Instruct在MMLU-redux、MATH、MBPP、LiveCodeBench、Arena-Hard、AlignBench、MT-Bench、MultiPL-E等权威测评中表现出色，在多个核心任务上，以不到1/5的参数超越了拥有4050亿巨量参数的Llama3.1-405B。</p><p></p><p>专项模型方面，用于编程的&nbsp;Qwen2.5-Coder&nbsp;和用于数学的&nbsp;Qwen2.5-Math都比前代有了实质性进步。Qwen2.5-Coder&nbsp;在多达5.5T&nbsp;tokens&nbsp;的编程相关数据上作了训练，当天开源1.5B和7B版本，未来还将开源32B版本；Qwen2.5-Math支持使用思维链和工具集成推理（TIR）&nbsp;解决中英双语的数学题，本次开源了1.5B、7B、72B三个尺寸和一款数学奖励模型Qwen2.5-Math-RM。</p><p></p><p>多模态模型方面，广受期待的视觉语言模型Qwen2-VL-72B正式开源，Qwen2-VL能识别不同分辨率和长宽比的图片，理解20分钟以上长视频，具备自主操作手机和机器人的视觉智能体能力。日前权威测评LMSYS&nbsp;Chatbot&nbsp;Arena&nbsp;Leaderboard发布最新一期的视觉模型性能测评结果,Qwen2-VL-72B成为全球得分最高的开源模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa85e14d8bcb032a57c01020afe11bfe.png" /></p><p>Qwen2-VL-72B在__权威测评__LMSYS&nbsp;Chatbot&nbsp;Arena&nbsp;Leaderboard成为成为全球得分最高的开源视觉理解模型</p><p></p><p>自从2023年8月开源以来，通义在全球开源大模型领域成为不少开发者尤其是中国开发者的首选模型。性能上，通义大模型多次登顶Hugging&nbsp;Face全球大模型榜单；生态上，通义从零起步、开疆拓土，与海内外的开源社区、生态伙伴、开发者共建生态网络，截至2024年9月中旬，通义千问开源模型下载量突破4000万，Qwen系列衍生模型总数超过5万个，成为仅次于Llama的世界级模型群。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb4cc8d4566d354534b6b20b36106682.png" /></p><p>HuggingFace数据显示，截至9月中旬Qwen系列原生模型和衍生模型总数超过5万个</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Bw5XqF5PmShA9sdY2EaV</id>
            <title>阿里云通义灵码重磅升级，能自主修BUG、开发应用的AI程序员来了</title>
            <link>https://www.infoq.cn/article/Bw5XqF5PmShA9sdY2EaV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Bw5XqF5PmShA9sdY2EaV</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月 19 日，在2024杭州云栖大会上，随着通义大模型能力的全面提升，阿里云通义灵码迎来重磅升级，从一年前只能完成基础的辅助编程任务，进化到几句话就能完成需求理解、任务拆解、代码编写、修改BUG、测试等开发任务，最快几分钟可从0到1完成应用开发，提升数十倍开发效率。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6fcc1ddc040ca45c480b2fea5fb0d9c.jpeg" /></p><p></p><p></p><p>自去年首次亮相以来，通义灵码已入职中华财险、哈啰集团、长安汽车等公司，累计生成代码超10亿行。全新升级的通义灵码模拟了人类程序员的能力，可完成更复杂、更全面的任务；通义灵码还可以脱离专业IDE软件，在web端直接执行缺陷修改、需求分析、代码实现、问题排查等任务工作流，兼具架构师、开发工程师、测试工程师等多种岗位技能，大幅缩短了应用的开发周期。</p><p></p><p>例如，真人程序员手动开发一个网页，通常需要1天的时间完成需求分解、写代码、测试等任务；现在，人类只需要输入需求，通义灵码5分钟就能完成整个过程。阿里云表示，未来只要有创意，不懂代码也能开发应用和软件。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03ac772183b1d077e44dced58c743da9.png" /></p><p></p><p></p><p>云栖大会现场，一位13岁的中学生在通义灵码上输入几句话，2分钟就生成了一个python语言编写的倒计时网页；现场还有参会者，用通义灵码修改开源魂斗罗游戏代码，在9个代码文件2000多行代码里，几分钟就精准修改了游戏角色的生命值、跳跃高度等参数。</p><p></p><p>目前，通义灵码已广泛应用于金融、制造、互联网、交通、汽车、能源等行业。在 Gartner 首个AI代码助手魔力象限报告中，阿里云成为唯一进入挑战者象限的中国科技公司。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dE4WtQXtXspccpPToH6B</id>
            <title>通义千问升级旗舰模型Qwen-Max，性能接近GPT-4o</title>
            <link>https://www.infoq.cn/article/dE4WtQXtXspccpPToH6B</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dE4WtQXtXspccpPToH6B</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月19日云栖大会，阿里云CTO周靖人宣布，通义旗舰模型Qwen-Max全方位升级，性能接近GPT-4o。通义官网和通义APP的后台模型均已切换为Qwen-Max，继续免费为所有用户提供服务。用户也可通过阿里云百炼平台调用Qwen-Max的API。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fec7a4e2828992fa4c1984bc91b92e7a.png" /></p><p></p><p></p><p>相比上一代模型，Qwen-Max在训练中使用了更多的训练数据、更大的模型规模、更强的人类对齐，最终达到了更高的智能水平。在MMLU-Pro、MATH、GSM8K、MBPP、MultiPL-E、LiveCodeBench等十多个权威基准上，Qwen-Max表现接近GPT-4o，数学能力、代码能力则超越了GPT-4o。数学和代码所代表的推理能力是大模型智能水平的最重要体现。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b5a7470138bf40e3094c1065cadbc5c.jpeg" /></p><p></p><p></p><p>相比2023年4月的初代通义千问大模型，Qwen-Max的理解能力提升46%、数学能力提升75%、代码能力提升102%、幻觉抵御能力提升35%、指令遵循能力提升105%，模型与人类偏好的对齐水平更是有了质的飞跃，提升了700%以上。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fQhoCDWL4bpYfstWGN5o</id>
            <title>阿里CEO吴泳铭：AI最大的想象力不在手机屏幕，而是改变物理世界</title>
            <link>https://www.infoq.cn/article/fQhoCDWL4bpYfstWGN5o</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fQhoCDWL4bpYfstWGN5o</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 08:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 ｜ 华卫</p><p></p><p>9 月 19 日，阿里云CTO周靖人发布通义千问新一代开源模型Qwen2.5，涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型。每个尺寸都有基础版本、指令跟随版本、量化版本，总计上架100多个模型。截至2024年9月中旬，通义千问开源模型累计下载量已突破4000万，成为仅次于Llama的世界级模型群。</p><p></p><p>“人们对新技术革命，往往对短期高估，又对长期低估，但它会在你的怀疑中成长，在你在迟疑中错过大趋势。”</p><p></p><p>当日，在刚刚开幕的阿里 2024 云栖大会上，阿里巴巴集团CEO、阿里云智能集团董事长兼CEO吴泳铭发表主题演讲，分享了对 AGI 变革趋势、生成式 AI 的应用巨变和 AI 算力发展的最新观点。</p><p></p><p>他认为，过去22个月，AI发展的速度超过任何历史时期，但我们依然还处于AGI变革的早期。下一阶段，全世界先进模型的投入门槛是几十亿、几百亿美金级别。</p><p></p><p>吴泳铭表示，生成式AI最大的想象力，绝不是在手机屏幕上做一两个新的超级app，而是接管数字世界，改变物理世界。并且，他预测道，机器人将是下一个迎来巨变的行业，未来所有能移动的物体都会变成智能机器人。</p><p></p><p>算力方面，吴泳铭指出，过去一年，阿里云投资新建了大量的AI算力，但还是远远不能满足客户的旺盛需求。未来几乎所有的软硬件都会具备推理能力，它们的计算内核将变成GPU AI算力为主、CPU传统计算为辅的计算模式。</p><p></p><p>以下为演讲全文（在不改变原意上有删减）：</p><p></p><p>在刚刚过去的夏天，阿里云全面支撑巴黎奥运会实现了历史性突破，云计算首次超越卫星，成为奥运主要转播方式。AI也首次广泛应用于奥运会。今天，云栖大会的焦点也是AI。我主要分享三点内容：</p><p></p><p></p><h1>大模型发展远超摩尔定律</h1><p></p><p></p><p>第一，过去22个月，AI发展的速度超过任何历史时期，但我们现在依然还处于AGI变革的早期阶段。</p><p></p><p>大模型技术快速迭代，技术可用性大幅提升。大模型已经具备了文本、语音、视觉的多模态能力，能够开始完成复杂指令。去年，大模型还只能帮助程序员写简单的代码，今天已经能直接理解需求，完成复杂的编程任务。去年，大模型的数学能力还只有中学生水平，今天已达到国际奥赛金牌水平，并在物理、化学、生物等多方面学科接近博士水平。</p><p></p><p>同时，模型推理成本指数级下降，已经远远超过摩尔定律。一年来，通义千问API在阿里云百炼上的调用价格下降了97%，百万Tokens调用花费最低已经降到了5毛钱。推理成本是应用爆发的关键问题，阿里云会努力把成本继续降下去。</p><p></p><p>开源生态蓬勃发展。今年6月，通义千问开源Qwen2，迅速登顶Huggingface的全球开源模型排行榜。在Huggingface上，Qwen的原生和衍生模型接近5万个，排名全球第二。阿里云魔搭社区上有超过1万个模型、服务了超过690万开发者。</p><p></p><p>这一切才刚刚开始，要实现真正的AGI，下一代模型需要具备更大规模、更通用、更泛化的知识体系，同时也将具备更复杂更多层次的逻辑推理能力。全世界先进模型竞争的投入门槛，将达到数十亿、数百亿美金的级别。AI具备创造能力、帮助人类解决复杂问题的路径清晰可见，也打开了AI在各行业场景中广泛应用的可能性。</p><p></p><h1>生成式 AI 将创造超互联网十倍价值</h1><p></p><p></p><p>第二，AI最大的想象力不在手机屏幕，而是接管数字世界，改变物理世界。</p><p>今天很多行业内人士一直在想AI最大的应用是什么，可能一直在想手机上有什么AI时代创新的超级APP。但我们认为AI最大的想象力绝对不是在手机屏幕上，AI最大的想象力是在通过渗透数字世界、接管数字世界，并改变物理世界，这才是AI最大的想象力。</p><p></p><p>我们不能只停在移动互联网的视角看未来。生成式AI最大的想象力，绝不是在手机屏幕上做一两个新的超级app，而是接管数字世界，改变物理世界。</p><p></p><p>过去三十年，互联网浪潮的本质是连接，互联网连接了人、信息、商业和工厂，通过连接提高了世界的协作效率，创造了巨大的价值，改变了人们的生活方式。但生成式AI是通过生产力的供给创造了新的价值，从而为世界创造了更大的内在价值，也就是总体提高了整个世界的生产力水平。这种价值创造，可能是移动互联网连接价值的十倍、几十倍。</p><p></p><p>我们认为生成式AI将逐渐渗透数字世界，并接管数字世界，物理世界的大部分事物都会具备AI能力，形成下一代的具备AI能力的全新产品，并与云端AI驱动的数字世界连接产生协同效应。</p><p></p><p>很长一段时间，AI的焦点主要集中在模拟人类的感知能力，比如自然语言理解、语音识别、视觉识别。但是生成式AI的崛起，带来了质的飞跃，AI不再仅仅局限于感知，而是首次展现了思考推理和创造的力量。</p><p></p><p>生成式AI让世界有了一个统一的语言——Token。它可以是任何文字、代码、图像、视频、声音，或者是人类千百年来的思考。AI模型可以通过对物理世界数据的Token化，理解真实世界的方方面面，比如人类行走、奔跑、驾驶车辆、使用工具，绘画、作曲、写作、表达、教学、编程的技巧，甚至是开公司创业。理解之后，AI就可以模仿人类去执行物理世界的任务。这将带来新的产业革命。</p><p></p><p>我们看到，汽车行业正在发生这样的变革。之前的自动驾驶技术，是靠人来写算法规则，几十万行代码，仍然无法穷尽所有的驾驶场景。采用“端到端”的大模型技术训练后，AI模型直接学习海量人类驾驶视觉数据，让汽车具备了超越大部分司机的驾驶能力。</p><p></p><p>机器人将是下一个迎来巨变的行业。未来，所有能移动的物体都会变成智能机器人。它可以是工厂里的机械臂、工地里的起重机、仓库里的搬运工、救火现场的消防员、包括家庭里的宠物狗、保姆、助理。</p><p>未来，工厂里会有很多机器人，在AI大模型的指挥下，生产机器人。现在每个城市家庭里有一辆或者两辆车，未来每个家庭可能会有两三个机器人，帮助人们提升生活当中的效率。</p><p></p><p>可以想见，AI驱动的数字世界连接着具备AI能力的物理世界，将会大幅提升整个世界的生产力，对物理世界的运行效率产生革命性的影响。</p><p></p><h1>GPU AI 算力将改写所有应用</h1><p></p><p></p><p>第三，AI计算正在加速演进，成为计算体系的主导。</p><p></p><p>无论是我们看到端侧的计算，还是云端的世界，这都是一个非常明显的趋势。生成式AI对数字世界和物理世界的重构，将带来计算架构的根本性变化。过去几十年，CPU主导的计算体系，正在加速向GPU主导的AI计算体系转移。未来几乎所有的软硬件都会具备推理能力，它们的计算内核将变成GPU AI算力为主、CPU传统计算为辅的计算模式。</p><p></p><p>我们看到，在新增算力市场上，超过50%的新需求由AI驱动产生，AI算力需求已经占据主流地位。这一趋势还会持续扩大。过去一年，阿里云投资新建了大量的AI算力，但还是远远不能满足客户的旺盛需求。</p><p></p><p>今天我们接触到的所有客户、所有开发者、所有CTO，几乎都在用AI重构自己的产品。大量新增需求正在由GPU算力驱动，大量存量应用也在用GPU重新改写。在汽车、生物医药、工业仿真、气象预测、教育、企业软件、移动APP、游戏等行业，AI计算正在加速渗透。在各行各业，看不见的新产业革命正在悄然演进。</p><p></p><p>所有行业，都需要性能更强、规模更大、更适应AI需求的基础设施。</p><p></p><p>阿里云正在以前所未有的强度投入AI技术研发和基础设施建设。我们的单网络集群已拓展至十万卡级别，正在从芯片、服务器、网络、存储到散热、供电、数据中心等方方面面，重新打造面向未来的AI先进基础设施。</p><p></p><p>从历史经验来看，人们对新技术革命，往往对短期高估，又对长期低估。因为在新技术应用早期，渗透率还比较低，人们经验没有发生过此类事件，大部分人的本能会产生怀疑，这很正常。但新技术革命会在人们的怀疑中成长，让很多人在迟疑中错过。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2hPirteON3H6UkyieqFE</id>
            <title>精益驱动下的金融智能化变革：大模型与知识工程的进化</title>
            <link>https://www.infoq.cn/article/2hPirteON3H6UkyieqFE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2hPirteON3H6UkyieqFE</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 06:10:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型时代背景下，精益思想和提示工程的引入，成为提升金融行业智能化水平的关键驱动力。</p><p></p><p>在今年 8 月举办的 <a href="http://mp.weixin.qq.com/s?__biz=MzkzMzQzNjQ5Mw==&amp;mid=2247492380&amp;idx=1&amp;sn=0699c5699663064fa077727e96c38096&amp;chksm=c24e2e3ef539a72896745ba535af985fb2d9d5663c9cf2c4af035f99eafb210aa24f590878e1&amp;scene=21#wechat_redirect">FCon 全球金融科技大会</a>"上，文因互联董事长兼创始人、中国中文信息学会语言与计算专委会金融知识图谱工作组主席鲍捷博士发表专题演讲《精益地打造金融专家智能体》，探讨了如何通过精益方法论实现大模型在金融领域的有效落地，并展示了提示工程在知识建模中的革命性应用。通过对典型案例的分析，揭示新技术如何帮助金融企业实现降本增效，推动行业进入智能决策的新时代。&nbsp;&nbsp;</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）</p><p></p><h3>从精益出发</h3><p></p><p></p><p>精益，或称为 Lean，是一种追求效率和减少浪费的方法论。在软件工程和创业领域，我们经常听到"精益创业"（Lean Startup）这个概念，它其实是一种思维模式，强调用更少的资源解决问题。在人工智能领域，核心问题往往不是算法本身，而是如何以更低的成本解决问题。例如，如果有足够的资金，理论上可以解决任何问题，但现实中我们更关注如何经济高效地实现目标。</p><p></p><p>在知识工程领域，问题不仅仅是成本，因为知识本身带有主观性。由于每个人的观点都不尽相同，如何在观点不一致的情况下找到经济的解决方案，是知识工程面临的根本问题。从爱德华·费根鲍姆（Edward Feigenbaum）40 多年前创立知识工程学科开始，如何降低成本一直是核心问题。机器学习的出现是为了减少手工编写规则的成本，深度学习和自回归方法的发明是为了降低数据标注的成本，而 生成式人工智能的探索则是为了寻找一种更通用、成本更低的知识发现方法。归根结底，所有这些技术的发展都是为了让我们能够以更低的成本从数据中提取知识。</p><p></p><p>在过去几十年的人工智能实践中，大约 99% 的人工智能项目都以失败告终。我们可以预见，未来大多数项目也将面临同样的命运。那么，什么样的项目更有可能成功呢？长期经验告诉我们，那些"重"的项目，即那些在前期需要大量成本投入而收益不明显的项目，往往容易失败。这是因为每个项目都存在成本和收益的曲线，如果项目在早期就面临漫长而高昂的成本投入，而没有相应的收益，通常很难坚持到收益回报的那一天，相关方的耐心也会逐渐消失。</p><p></p><p>我们应该采取“小步快跑”的策略，以实践和场景驱动的方式进行工作。这与精益思想是一致的，它强调成本和收益的同步增长。在硅谷的创业理论中，这种增长过程被称为 Lean Startup 循环，即构建、评估、学习（build、measure、learning），形成一个持续的循环过程。在知识工程领域，我们今天也需要采用类似的方法来大幅降低成本，以提高项目成功的可能性。</p><p></p><h3>20 年前的人工智能实践</h3><p></p><p></p><p>20 年前，人工智能领域的实践与今天大相径庭。以 Aura 项目为例，这是我亲自观察过的一个项目，虽然我没有直接参与其中，但我对项目中的人员非常熟悉，因此我能够挖掘出一些不为外界所知的内部信息。</p><p></p><p>Aura 项目本质上可以被视为一个高考项目。在美国，高中生也需要参加高考，而考试的试卷被称为 SAT。项目的目标是使用机器代替人类来完成试卷，参加高考。这个项目是在 20 多年前由微软的创始人之一 Paul Allen 发起的。Paul Allen 因健康原因很早就离开了微软，尽管他的财富不及比尔·盖茨，但仍然拥有数百亿美元。他计划在生前将所有财产用于投资，其中一部分就投向了火箭技术，另一部分则投向了人工智能领域。他在人工智能上投入了大约 10 亿美元，这在 20 年前是一笔相当可观的资金。</p><p></p><p>Paul Allen 将其中一部分资金投入到了知识工程领域，就是 Aura 项目。Aura 意为"曙光"，而我恰好有一位非常好的朋友在这个项目中担任项目经理。他向我透露了许多实际情况，包括项目中的各个组件，如问答引擎、词汇编辑引擎和逻辑推理引擎等。他们的目标是在名为 Halo Project 的大项目框架下解决科学问题的推理。Halo 项目的核心是进行化学和物理等科学问题的解答，这在当时是一个非常具有挑战性的任务，与今天使用大模型轻松处理这些问题形成鲜明对比。</p><p><img src="https://static001.geekbang.org/infoq/20/20e5270f3d230be470704df88054521b.png" /></p><p>在 20 年前，我们还没有现在所说的大模型，甚至知识图谱也尚未出现——知识图谱是在 2012 年才发展起来的技术。当时，我们处于语义网时代，进行知识建模的第一步是将知识编写成规则语言。最初使用的是 Prolog 语言，更早之前使用的是 Lisp，随后发展出了 OWL 等规则语言。</p><p></p><p>对于 Halo 项目来说，他们面临的挑战是对科学知识进行建模，这是一项非常困难的任务。为了解决这个问题，他们采用了一种名为 SILK 的语言，即 Semantic Inferencing on Large Knowledge，这是一种用于大规模知识推理的语言。在这里，"inference"（推理）这个词在不同的上下文中有不同的含义。如今，当我们使用大模型时，可能会将推理理解为一种快速的推导过程，但在当时，推理涉及到复杂的逻辑和计算过程。在 20 年前，人工智能主要依赖两种知识处理方法：演绎推理（deduction inference）和统计推断。演绎推理是一种基于逻辑的推演过程，而统计推断则侧重于数据和概率。</p><p><img src="https://static001.geekbang.org/infoq/fb/fb93b5eb5afc9141435641736813eb38.png" /></p><p></p><p>当时，人工智能项目采用了一种非常复杂的架构，将生物学、化学和物理学等知识手动转化为一系列知识规则。这个过程非常昂贵，据估计，将一本书的内容转化为规则可能需要花费数十万美元。这在当时是一个成本极高的事情，涉及使用多种不同的逻辑表达语言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3ba828ffa6b3ec13f8538a04d0bfe254.png" /></p><p></p><p>在拥有了规则语言之后，接下来需要的就是一个推理机。当时推理机是由德克萨斯大学奥斯汀分校开发的。奥斯汀分校在知识表现和推理领域处于全球领先地位。然而，即使拥有这样复杂的技术框架，当时的推理机也只能达到 30% 到 50% 的正确率。相比之下，如果今天使用大模型来进行同样的推理过程，即便没有专业领域的支持，也能达到 60% 到 70% 的正确率。这种显著的提升是时代发展和技术进步所带来的结果。</p><p></p><h3>从反思到进化：大模型时代的知识系统构建</h3><p></p><p></p><p>在 2010 年，我对某个项目进行过复盘和思考，现在又过去了 14 年，让我们一起回顾一下在大模型时代，我们构建知识系统的方法，从中分辨哪些是正确的，哪些是错误的。</p><p><img src="https://static001.geekbang.org/infoq/62/62c5838a91b0934acb89384410889d1d.png" /></p><p>第一，我们考虑这种方法是否可能扩展到其他领域。按照当时的方法，这是不可能的，但今天通过大模型的方法，由于其通用性，扩展变得很容易。大模型的通用性意味着可以应用于化学、物理、数学、历史等多个领域，而不需要重新编写所有内容。</p><p></p><p>第二，我们考虑了处理大量数据的能力。当时这是不可能的，但今天已经可以了。</p><p></p><p>第三，当时使用了受控自然语言（Controlled Natural Language, CNL），而今天，我们不再需要这种语言，自然语言已经足够。</p><p></p><p>第四，如何更好地利用外部数据，比如链接数据（linked data）。当时没有明确的想法，但今天有了像 GraphRAG 这样的工具，它将知识图谱和大模型结合起来。</p><p></p><p>第五，如何综合使用多种方法，将规则方法和自然语言处理方法结合起来。当时没有找到很好的方法，但现在生成式模型已经普及。</p><p></p><p>第六，如何降低综合成本。当时不知道，但现在我们知道了预训练模型和提示工程可以大幅降低成本，至少下降了 100 倍。</p><p><img src="https://static001.geekbang.org/infoq/a2/a20f0ea1d5f81e11870dfece65acfcdf.png" /></p><p>第七，关于问答系统。2010 年前后，IBM 的 Watson 系统和 DeepQA 系统是问答领域的佼佼者，但当时还没有人使用深度学习，直到 2012 年深度学习才开始流行。到了 2017 年，我们开始知道如何融合这些技术，而今天，大模型方法已经轻松解决了这个问题。</p><p></p><p>关于如何结合信息检索方法和基于规则的方法，以及如何解决长尾搜索问题。当时不知道，但现在我们知道了 RAG 加上 Agent 可以解决这些问题。尽管有了统计方法和规则方法，结构化知识仍然不可或缺。</p><p><img src="https://static001.geekbang.org/infoq/7f/7f1680de016aa3a1c550b08bc3e2bac0.png" /></p><p>第八，我们考虑了知识图谱的必要性。尽管大模型提高了知识图谱的构建效率，但并没有取代它。</p><p></p><p>第九，知识建模不能忽略手工方法，知识编辑工具可能是关键。我在这方面的想法有误，但也没有完全错，因为数据生成、提示词交互、结果校验等过程仍然需要人机交互工具。</p><p></p><p>第十，我们考虑了如何平衡颗粒度和成本。现在我们知道，走向大模型是正确的方向。</p><p></p><p>从技术角度来看，20 年前的研发工作采用的是瀑布式开发模式，其成本远高于今天大模型的成本，可能高出 100 倍甚至更多。今天，大模型的最大好处在于，我们不需要在系统设计初期就将所有知识固定下来。相反，我们可以在知识使用时，通过数据的预训练和提示工程的交互来实时更新知识，实现全面的迭代。</p><p></p><p>以前的知识系统构建分为三个步骤：首先用统计方法打基础，然后用规则提高准确度，最后通过编辑过程进行精细化处理。现在，这三个步骤已经发生了变化。统计方法已经演进为大模型，而规则并没有消失，它们以一种新的形式存在。例如，当我们使用像 Coze 或 RAGFlow 这样的 Agent 平台时，编辑流程实际上就是规则，它们将传统规则转化为了工作流，工作流本质上就是规则。</p><p></p><p>即使大模型的输出准确度达到了 70% 到 80%，但在实际业务场景中，如合同检测，如果准确度没有达到业务人员的要求，他们是不会采用的。因此，为了使大模型的输出结果可用，我们必须做好后处理工作，包括数据交互和数据验证，以提高准确度满足业务需求。</p><p></p><h3>AI 落地的关键：务实的工程实践与基础建设</h3><p></p><p></p><p>AI 的落地关键在于工程，而不是单纯的科学或高深莫测的技术。大模型虽然为我们解决了一些基础问题，但它们并非万能的。在实际应用中，我们需要通过迭代和控制成本来解决剩余的问题。例如，即使 AI 系统的准确率达到了 85%，剩下的 15% 不足之处仍可能导致亏损。关键在于如何降低成本，如何提高那最后的 5% 或 10% 的准确率。</p><p></p><p>工程的本质在于处理细节和解决实际问题。工程的实现并不总是充满光环，它往往涉及枯燥但至关重要的底层系统工作。在工程实践中，我们总会遇到意想不到的问题，这些问题的解决方案可能同样出人意料。要进行有效的工程实践，关键在于关注小事，因为所有的工程成就都是由一系列小细节累积而成的。以瓦特改进蒸汽机为例，他所做的不仅仅是发明，而是通过改进传动机制、密封技术等多个小细节，最终实现了蒸汽机的高效能。同样，在计算机科学和人工智能领域，工程的重要性也不言而喻。</p><p></p><p>AI 不仅仅是理科，更是工科。工科教育强调实践，例如制作小锤子、操作车床、焊接等，这些都是工程的一部分。编写代码也是工程的一种形式，它需要质量检验、质量控制、多种工具的配合以及公差体系的校正。与机械工程或化学工程相比，软件工程还相当年轻，很多人对工程的真正含义理解不足。我们需要认识到，软件工程也需要遵循工程原则，包括严格的质量控制和精细的工艺流程。这是 AI 领域需要面对和解决的问题。</p><p></p><p>过去的几年里，人们都在追捧大数据和大模型。然而，无论是大数据还是大模型，核心问题并没有改变：数据清理。 十几年前，有句话说得好，“如果你解决了数据清理问题，就解决了 80% 的机器学习问题”。这个原则在今天依然适用，解决了数据清理问题，也就解决了 80% 的大模型问题。</p><p></p><p>智能体等技术其实并没有那么复杂。它们都是由一系列枯燥的基础工作构成的，只要这些基础工作做好了，问题也就迎刃而解。我们不应该盲目模仿像 OpenAI 这样的大型组织，使用数万张显卡去训练大模型。因为并不是每个组织都拥有这样的资源，有些可能连 100 张，甚至 10 张显卡都没有。每个组织都应该根据自己的实际情况，采取适合自己的方法。</p><p></p><p>大模型本质上是从数据中提取的知识，而知识可以被视为“小数据”，它强调的不是规模，而是价值。这里的“小数据”有三个特点：价值（Value）、真实性（Variety）和多能性（Versatility）。</p><p></p><p>价值，不是连垃圾都存起来，而是特别关心数据的价值密度，提高投入产出比。真实性关心数据的可验证性，可用性，自描述性等。多能性意味着知识不应被固定在代码中，而应存在于数据中，能够灵活应用。</p><p></p><p>大模型是一种知识模型，它代表了一种数据的高级形式，承载了丰富的知识。要实现大模型的有效落地，我们需要从基础做起，务实地处理运维、数据库和数据清洗等基础工作，并逐步演化和优化：</p><p></p><p>如何按天迭代？如何构造联调系统？如何无标注数据启动？如何分离准确度和召回率要求？如何统一运用规则和大模型？如何适应无明确衡量标准的开发？如何设计可演进的数据模式？如何提升数据可理解性？如何逐步提升规则 /Agent flow/RAGFlow 系统的表达力？如何平衡黑箱和白箱模型的优缺点？如何在优雅架构和工期间取舍？</p><p></p><p>所有伟大的成果，之所以能够取得优异的成绩，都是因为一线工程师的辛勤工作。这些成绩并非来自于向领导汇报的表面文章，而是真正在一线发挥作用的扎实工作。这些工作的核心是务实，即所谓的“土”。</p><p></p><p>回顾过去，2010 年时的知识工程水平大致相当于软件工程的 1940 年代。到了 2017 年，我开始涉足金融领域应用时，软件工程的水平相当于 1950 年代。如今到了 2024 年，我认为知识工程的水平已经发展到了大约 1970 年代的软件工程水平。 在 1970 年代，高级语言 C 语言被发明，而 Python 是在 1992 年左右发明的。我们今天熟悉的许多高级语言，如 Java、PHP 等，大多是 90 年代的产物。当我们审视知识工程领域时，我们发现并没有出现类似 C 语言这样的基础性、革命性的语言。所以说，知识工程领域仍有很大的发展空间，需要我们继续探索和创新。</p><p></p><h3>提示工程，开启知识建模新篇章</h3><p></p><p></p><p>提示工程的意义在于它为知识建模提供了一种自然语言的表达方式，这是一种革命性的进步。在金融等专业领域的应用中，传统的知识工程和专家系统方法成本过高，导致如 Aura 和 Halo 这样的系统最终失败。然而，从 2022 年开始，我们找到了一种新的方法，让我们能够享受到软件工程在 70 年代所体验到的便利，那就是声明式编程语言。</p><p></p><p>在编译原理课程中，我们了解到有两种类型的编程语言：声明式语言和命令式语言。SQL 因其声明性质而广受欢迎，而直接使用机器码则是一种典型的命令式编程。在知识建模方面，我们之前使用的如 SILK、OWL 或 RDF 等语言，可以看作是接近机器语言的低级语言。我们一直在寻找一种过渡，一种对人类更友好的知识建模语言。</p><p></p><p>过去，人们尝试使用受控自然语言（CNL）如 Aura 项目，但效果并不理想。而提示工程的出现，使我们能够直接使用自然语言进行交互，这是非常神奇的，也是提示工程的核心价值所在。</p><p><img src="https://static001.geekbang.org/infoq/aa/aadb1bab9a91c1d6307cf3bdaf63b1e0.png" /></p><p>企业级信息系统架构经过多年的发展，从 OA 系统到大数据、互联网、数据湖、数据中台等，其核心主线始终围绕着如何让机器自动理解多源异构的分布式数据，以及如何将分散在不同员工大脑中的知识集中起来，使公司管理者能够掌控全局。数字化转型的本质在于打破组织内的数据边界，让知识流动起来，成为组织的资产。</p><p></p><h3>基于大模型的新范式：无代码，无标注，强泛化</h3><p></p><p></p><p>大模型带来了一种新的范式，它具有无代码、无标注和强泛化的特点，这在以前是难以想象的。大模型的核心作用在于显著降低了开发和应用的成本。</p><p></p><p>在过去，要实现特定的业务流程，比如与券商合作时，我们需要构建底层数据仓库，然后开发各种自动化系统，包括自动化写作、核查、问答和大屏展示等。这些系统往往各不相同，需要大量的定制化开发。大模型的出现使得我们可以有一个统一的基础平台来实现所有这些功能，而且不需要在设计阶段就将所有的业务知识全部预设进去。知识可以在使用过程中不断演化和完善。这种能力在以前是不存在的，它相当于将我们带到了一个新的境界。到了 2022 年，我们发现大模型真的可以实现这样的功能，这确实是一次世界观的颠覆。我们意识到，技术的发展可以如此之快，可以以这样一种全新的方式解决问题，是具有革命意义的。</p><p></p><h3>面向知识的编程，破解第三次软件危机？</h3><p></p><p></p><p>我也认为，大模型的出现解决了所谓的第三次软件危机。回顾历史，第一次软件危机大约发生在 50 年前，当时的问题集中在无结构的 GOTO 语句上。随后，数据结构的引入帮助我们解决了这个问题，我们学会了将程序视为数据结构和算法的结合。</p><p></p><p>第二次软件危机则是面向对象编程 (OOP) 和互联网的兴起，这可以看作是面向对象的扩展，因为互联网编程很大程度上是以数据库为中心的。</p><p></p><p>近十年来，我们面临第三次软件危机：软件变得极其复杂和高度分布式。例如，在 Web 领域中，智能合约的出现代表了一种新的软件工程形式。在银行等金融机构，进行数据中台建设和数字化转型的过程中，许多人感受到了图数据库的矛盾——既觉得它有用，又觉得它的表达能力受限，给人一种“食之无味，弃之可惜”的感觉。这种矛盾体现了当前软件工程方法与应用需求之间的脱节。</p><p></p><p>我认为第三次软件危机的解决方案是一种新的编程范式——面向知识的编程 （Knowledge Oriented Programming, KOP）。在这个范式中，编程的核心是推理(Reasoning)和知识库(Knowledge Base)，这可以看作是现代版的算法和数据结构。这里的知识库就是大模型，大模型的推理能力，不同于传统的推演 (Deduction)，是一种新的推理方式。大模型作为知识库，本质上承载的是知识而不仅仅是数据集。</p><p></p><p>这个核心的范式与传统的软件开发方法有显著的区别。传统方式，比如 Aura 项目的 style，采用的是瀑布模型。在这个模型中，首先定义好数据的 schema，然后业务规则也是事先设定好的。以 Aura 为例，它需要事先确定如何进行物理或化学的考试，这些业务知识是预先定义的，用户界面 (UI) 也是固定不变的。这种方式是传统的业务分析方法，也是瀑布式开发方式，它要求预先设定好所有要素。</p><p></p><p>我们理想中的另一种范式是端到端范式。在 2022 年到 2023 年上半年，许多人开始尝试使用大模型进行端到端的开发，希望把所有的数据输入到模型中，然后期待模型能够神奇地解决所有问题。 这种方法的想法是，不再需要重新定义 schema，不需要构建知识图谱，所有的中间过程都可以是多模态的，从图像到文本到语音、OCR、PDF 等都能一次性处理完毕。</p><p></p><p>但实践中发现，端到端的方式行不通。以金融领域为例，它面临的问题包括准确度不可靠——例如，深交所的公告处理系统要求 99.99% 的准确度，这在当前是任何大模型都无法达到的。此外，生成式模型缺乏幂等性，即相同的输入不能保证每次都得到相同的输出。还有处理速度慢和成本高的问题，例如原本 20 分钟完成的 IPO 审核任务，如果使用大模型可能一天也完成不了。</p><p></p><p>端到端模式在实践中被证明是不可取的。尽管我们相信，如果未来的硬件成本大幅降低，或者相关的技术平台有显著进步，端到端模式可能会变得可行，但至少在目前，这种方式是不现实的。</p><p></p><h3>提示工程的系统化构建</h3><p></p><p></p><p>我们需要实现一种称为“活的业务分析”的系统范式来进行知识建模。这种范式的核心在于数据本身应该是动态的，即数据抽取应该基于提示工程。业务分析过程也应该基于提示工程，这意味着数据产生、schema 定义以及业务规则的生成都应该是即时的（Just In Time, JIT）。</p><p></p><p>最终，这些即时产生的元素结合起来形成一个应用系统，这个系统本身也最好是即时构建的。目前，许多公司正在开发基于对话的商业智能（Conversational BI），其本质是设计一种“活”的用户界面（UI）。</p><p></p><p>提示工程本质上是软件工程的一部分。编写提示词也是软件工程活动，而不仅仅是编写 Python 代码。未来可能会出现一个专门的提示工程专业，并有专门的集成开发环境（IDE）来支持这项工作。提示工程应该有其自身的复杂业务逻辑和质量控制系统。</p><p></p><p>这也符合精益开发的流程，遵循 learn-build-measure 的循环模式，这里的 measure 指的是对构建出的系统或产品进行广泛的评估和测试。基于评估结果，我们进一步进行优化，然后再返回到构建阶段，形成一个持续的循环。</p><p></p><p>在没有实际执行之前，人们可能会对大模型的工作方式有一种“你以为”的理解，但真正参与到工程落地系统后，会发现整个过程极其复杂。我们开发了一种可行的提示词设计方法论，称为 S2PI 方法论，它包含四个要点。</p><p></p><p>Schema（结构）：虽然我们不要求事先定义完整结构，但在大多数问题中，仍然存在一些固定的数据要素。因此，我们需要根据特定场景设计提示词的基本结构。</p><p></p><p>Supplement（补充）：在基础架构的基础上，根据特定场景增加背景信息。这些背景信息可能包括正样本、负样本，或是关键架构要素的变化形式。在软件工程中，我们通过回归测试来确保质量，而在提示工程中，设计各种补充信息的过程实际上就是在构建测试集和引导集。</p><p></p><p>Property（属性）：涉及与特定领域（如金融）相关的属性和术语。从工程角度来看，这类似于设计实体关系（ER）图。</p><p></p><p>Input（输入）：在使用时刻，根据设计阶段的架构和补充信息，确定所需的具体输入。工业级别的提示词设计不是简单的民用级别，而是一个复杂的过程。</p><p></p><p>我们还专门开发了一项技术，称为“提示词编译”。S2PI 方法论设计出的提示词是一种高度技术化的语言，普通人难以理解。因此，我们设计了一些 Agent，它们能够将普通人能理解的提示词翻译成更底层、专业的提示词。</p><p></p><h5>综合案例 1：基于提示工程的文本抽取</h5><p></p><p></p><p>在讨论基于提示工程的文本抽取时，我们首先需要理解 JIT 数据生成的重要性。以三个大型交易所的审核系统为例，这些系统负责从厚重的招股书中抽取数据，这涉及到大量自然语言处理技术。从 2023 年开始，我们交付给交易所的新一代系统都采用了大模型技术。</p><p></p><p>在这个过程中，有很多关键的技术细节。例如，情境学习需要定义各种角色，但角色数量的确定是一个问题。一个招股书包含 1 万多个数据点，2400 个不重复的 schema 和 94 个章节，这是否意味着需要定义 94 个 Agent？这仅仅是针对招股书一种文档类型，还有债券募集说明书、IBS 专项说明书、定调报告、评级报告、征信报告、资管合同等其他文档类型，以及底层资产类型如 ABS、中票、短融、公司债、企业债、利率债等，构成了一个庞大的矩阵。在输入 PDF 文件进行抽取时，需要进行 PDF 解析，确保每个章节、每个段落在抽取时获得适当的辅助信息（supplement 信息）。例如，为公告增加分类信息可以显著提高准确率。通过改变提示词，将提取结构化信息的机器人转变为专业提取董事会决议的模型，可以在四种公告上分别提高 3% 到 8% 的准确率。</p><p></p><p>我们还发现了一些技巧，比如在输出时要求不要有幻觉，结果真的降低了幻觉的出现。通过给每个公告增加例子和分类，可以进一步提高准确度 3.85% 到 3.87%。我们尝试了各种方法，仅在提示工程层面上所做的工作就提高了 13.8% 的准确度。</p><p></p><p>最令人惊讶的是，所有这些工作并不是由传统的软件工程师完成的，而是由一个提示工程实验室完成的，实验室的成员平均年龄 23 岁以下，很多是 00 后文科生，财经专业毕业生或在校生，甚至实习生，能够写中文并具备逻辑思维能力，就能实现这样的成果。这表明 提示工程极大地降低了技术门槛，使得非传统意义上的工程师也能参与到优化工作中来。开源系统和闭源系统我们都使用过，效果相当，到目前为止，我们并没有看到闭源系统明显优于开源系统。</p><p></p><h5>综合案例 2：基于提示工程的业务建模</h5><p></p><p></p><p>案例 2 我分享的是如何利用提示工程进行业务建模，尤其是在金融系统中。金融系统包含众多规则，包括财务规则、法务规则、核查规则以及业务流程管理（BPM）规则等。核心问题是如何降低业务建模的成本，使其不需要程序员编写，而是可以由 23 岁的文科生完成。</p><p></p><p>在大模型出现之前，业务建模遵循的是瀑布式开发流程。业务分析师，通常是金融专业出身，会手工整理业务规则，这些规则基于他们对原始业务文件的深入分析。然后，这些业务规则会转化为产品需求文档（PRD），再传递给产品工程师和软件工程师，整个过程耗时较长。</p><p></p><p>现在，我们的目标是利用业务规则自动生成底层提示词及其效果，实践表明这种方法大约有 85% 的可用性，剩余的 15% 可以通过其他方法解决。更进一步，我们尝试不依赖人工理解业务逻辑，而是通过给系统输入 20 份文档，让它自行整理出业务规则。经过两个月的实验，目前这种方法是可行的，预计到年底可以更加完善。</p><p></p><p>在金融领域，核查系统已经从静态变为动态。金融业务逻辑处理的核心是将业务知识进行建模。与过去的方法相比，现在的方法大幅降低了成本，原因在于我们不再需要在设计阶段做大量前期工作，从而使系统上线后难以演化。大模型的最大价值在于赋予了系统强大的可演化性。提示工程已经替代了大量的传统软件工程任务。尽管这种实践才一年多时间，但随着时间的推移，我们可以预见到提示工程和大模型在未来将有更强大的应用出现。</p><p></p><h5>嘉宾介绍</h5><p></p><p>鲍捷博士，文因互联董事长、首席科学家、创始人。爱荷华州立大学（Iowa State University）博士。曾任伦斯勒理工学院（RPI）博士后，麻省理工学院（MIT）分布式信息组（DIG）访问研究员，三星美国研发中心研究员，W3C OWL(Web 本体语言) 工作组成员，参与撰写了 OWL2 知识图谱语言国际标准。现任 W3C（万维网联盟）顾问委员会委员、中国中文信息学会语言与知识计算专业委员会委员、金融知识图谱工作组主席、中文开放知识图谱联盟 (OpenKG) 发起人之一，国际 Data Intelligence 杂志编委，中国科学技术大学国际金融学院业界导师。</p><p></p><h5>内容推荐</h5><p></p><p>大会 PPT 获取通道已开启，关注数字化经纬公众号，后台回复“PPT”，即可获取 PPT 下载地址（由于讲师所在企业限制，部分 PPT不对外公布，详情见大会官网日程）：<a href="https://ppt.infoq.cn/list/149">https://ppt.infoq.cn/list/149</a>"</p><p><img src="https://static001.geekbang.org/infoq/62/627ac83fbc862f8bfdbf46735876031d.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BXpDJBmdezFT1crOZ5H8</id>
            <title>一张美食图就能给菜谱、能给植物看病……阿里国际发布最新多模态大模型Ovis</title>
            <link>https://www.infoq.cn/article/BXpDJBmdezFT1crOZ5H8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BXpDJBmdezFT1crOZ5H8</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 02:58:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看一眼菜品图就知道怎么做、能给植物看病、能把手写英文准确翻译成中文、还能精准分析财报数据……多模态能力再次升级！今天，阿里国际AI团队发布了一款多模态大模型Ovis，在图像理解任务上不断突破极限，多种具体的子类任务中均达到了SOTA（最新技术）水平。</p><p>&nbsp;</p><p>多模态大模型能够处理和理解多种不同类型的数据输入，例如文本、图像。与大型语言模型（LLMs）相比，大语言模型在处理和生成文本数据方面有专长，而多模态大模型能够处理非文本数据，如图像等等。</p><p>&nbsp;</p><p>根据多模态权威综合评测平台OpenCompass的数据，Ovis1.6-Gemma2-9B在30B参数以下的模型中取得了综合排名第一，赶超MiniCPM-V-2.6等行业优秀大模型。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/dd/9b/dd3ca8b992c326736b5c3fa26e31819b.png" /></p><p>图：Ovis在OpenCompass上的测评数据情况</p><p>&nbsp;</p><p>据介绍，Ovis能够在数学推理问答、物体识别、文本提取和复杂任务决策等方面展现出色表现。例如，Ovis可以准确回答数学问题，识别花的品种，支持多种语言的文本提取，甚至可以识别手写字体和复杂的数学公式。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/0e/6a/0e496b877a6f95bce28ec4fb41109b6a.png" /></p><p>案例1:Ovis对手写文案的识别及翻译能力</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/02/1e/02154dd3b3b51196e978bde0b1ef6b1e.png" /></p><p>案例2：Ovis对复杂数学公式的处理能力</p><p></p><p><img src="https://static001.infoq.cn/resource/image/02/1e/02154dd3b3b51196e978bde0b1ef6b1e.png" /></p><p>案例3:Ovis通过对图片的识别处理能够给出菜谱</p><p>&nbsp;</p><p>具体来说，Ovis模型有五大优点：</p><p></p><p>1、创新架构设计：可学习的视觉嵌入词表：首次引入，将连续的视觉特征转换为概率化的视觉token，再经由视觉嵌入词表加权生成结构化的视觉嵌入，克服了大部分MLLM中MLP连接器架构的局限性，大幅提升多模态任务表现。</p><p></p><p>2、高分图像处理：动态子图方案：支持处理极端长宽比的图像，兼容高分辨率图像，展现出色的图像理解能力。</p><p></p><p>3、全面数据优化：多方向数据集覆盖：全面覆盖Caption、VQA、OCR、Table、Chart等各个多模态数据方向，显著提升多模态问答、指令跟随等任务表现。</p><p></p><p>4、卓越模型性能：Ovis展现出了优异的榜单表现。在多模态权威综合评测Opencompass上，Ovis1.6-Gemma2-9B在30B参数以下的模型中取得了综合排名第一，超过了Qwen2-VL-7B、MiniCPM-V-2.6等模型。尤其在数学问答等方向表现媲美70B参数模型；在幻觉等任务中，Ovis-1.6的幻觉现象和错误率显著低于同级别的模型，展现了更高的生成文本质量和准确性。</p><p></p><p>5、全部开源可商用：Ovis系列模型License采用 Apache 2.0。Ovis 1.0、1.5的数据、模型、训练和推理代码都已全部开源，可复现。Ovis1.6系列中的Ovis1.6-Gemma2-9B也已开源权重。</p><p></p><p>在AI领域，多模态大模型的应用场景非常广泛，包括但不限于自动驾驶、医疗诊断、视频内容理解、图像描述生成、视觉问答等。例如，在自动驾驶领域，多模态大模型可以整合来自摄像头、雷达和激光雷达的数据，以实现更精准的环境感知和决策。由于多模态大模型能够学习如何联合理解和生成跨多种模式的信息，也被视为朝向通用人工智能的下一个步骤。</p><p>&nbsp;</p><p>根据此前媒体报道，阿里国际在去年成立了一支AI团队，目前已经在40多个电商场景里测试了AI能力，覆盖跨境电商全链路，包括商品图文、营销、搜索、广告投放、SEO、客服、退款、店铺装修等，其中多个应用场景均基于Ovis模型进行开发，已帮助50万中小商家、对1亿款商品进行了信息优化。据介绍，商家的AI需求不断增长，近半年的数据显示，平均每两个月，商家对于AI的调用量就翻1倍。</p><p></p><p>附相关链接：</p><p>论文arXiv: <a href="https://arxiv.org/abs/2405.20797">https://arxiv.org/abs/2405.20797</a>"</p><p>Github:<a href="https://github.com/AIDC-AI/Ovis"> https://github.com/AIDC-AI/Ovis</a>"</p><p>Huggingface: <a href="https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-9B">https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-9B</a>"</p><p>Demo: <a href="https://huggingface.co/spaces/AIDC-AI/Ovis1.6-Gemma2-9B">https://huggingface.co/spaces/AIDC-AI/Ovis1.6-Gemma2-9B</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</id>
            <title>我在构建 MLOps 系统四年中学到的经验</title>
            <link>https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Sep 2024 09:45:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>正如标题所述，我致力于构建 MLOps 系统已经有近四年了。世界变化得很快，作为一名也有了四年经验的程序员，我感觉自己一直在努力不被深度学习（LLM）的新技术淹没，努力适应软件工程，努力远程获得好公司的好职位，等等。</p><p>&nbsp;</p><p>这篇文章一半是对我多年经历的审慎回顾，另一半是我对工程、机器学习（运维）的看法。我想你的脑海中也曾浮现过这些问题，但我不会回答任何问题，只是分享我的观点。</p><p></p><h2>第一个 ML 问题，提前一日的用电量预测</h2><p></p><p>2021 年，我开始研究能源消耗模型，这是我第一次真正深入研究运维应用程序。这个问题一开始很简单：我们必须预测八个城市每 24 小时的每日用电量，但要提前 24 小时，这个项目名为日前电力预测。自从我开始研究这个问题以来，用户数量不断增加，普通消费者开始使用更多能源，新工厂也正在建设中。能源需求也会随着经济波动而变化。这一切都意味着更多模型漂移和数据漂移。</p><p>&nbsp;</p><p>一段时间后，我定下了预测模型所用的一系列算法。在尝试了深度学习模型和基于树的模型后，我发现最成功的是 LightGBM 和 XGBoost（以及集成深度学习模型）。</p><p>&nbsp;</p><p>基于树的模型的问题在于，它们的预测通常受你提供的数据的限制。例如，如果你尝试预测今年夏季的第一天，而去年同日的峰值消耗为 600 单位，由于基于树的模型的性质，除非你正确地定制问题，否则你的预测值是不会超过 600 单位的。但是，当事件和特征稳定且输出分布没有太大变化时，基于树的模型也可以提供出色的预测结果。</p><p>&nbsp;</p><p>说实话，我当时的大部分经验都来自 Kaggle。我在那里待了大约九个月，深入研究预测竞赛、创建新特征并尝试不同的模型。我提出的模型得分为 7/10，我知道如果我与真正的专家竞争，他们可能彻底击败我的模型。</p><p></p><h2>意识到需要 MLOps 系统</h2><p></p><p>预测未来的能源消耗是一种奇怪的体验，尤其是现实中有太多因素在影响电力使用——天气、周末与工作日、假期，甚至祈祷时间。在像土耳其这样四季分明的地方生活又增加了一层复杂性。我不是一个追求完美的模型制作者，我努力让自己的模型适应突然的变化。例如，如果连续两个月的气温都稳定在 25°C，然后温度突然在一天内下降 5°C，模型通常会假设电力消耗将与之前的 25°C 天气差不多，但事实并非如此。人们的习惯会随着季节而改变，预测这些微妙的变化会很困难。然后，还会有像国家足球比赛这样不可预测的事件——祝你建模好运！</p><p>&nbsp;</p><p>那么，我做了什么工作来解决模型和数据漂移问题呢？在过去的一年里，我创建了数百个特征，使用 XGBoost 和 LightGBM 构建了一些基于树的模型，并用前五日的数据来验证它们。 MLFlow 是日常模型生产过程中的主要工具。虽然它可能不是验证模型的完美方式，但它确实改善了长期预测结果。至少，当电力消费比较稳定且模型适应性良好时，它给了我一个安全的选择。</p><p>&nbsp;</p><p>为了简化流程，我建立了一个完整的系统，每天早上通过 API 提取数据、生成特征、挑选模型并进行预测。我构建的自动化脚本只用一分钟就能跑完。然而即使有了自动化脚本，你也必须密切关注预测结果，特别是在国定假日或意外事件期间。</p><p></p><h2>虽然你部署了一个模型，但你可能不喜欢它</h2><p></p><p>我的一段奇怪的经历就是和我的经理争论预测结果中突然出现的漂移。他认为，如果夏天天气突然下降，预测第二天的电力消费会更容易，因为运行的空调数量会减少。他甚至手动调整了几次预测，但结果适得其反。这基本上结束了我们的争论。虽然我相信有些日子通过手动调整可以带来更好的预测结果，但模型经常会发现一些我们没有发现的模式。</p><p></p><h2>ML 人员在软件工程领域的沉浮</h2><p></p><p>后来，我转而从事医疗保健领域的 MLOps 平台工作。我花了很多时间寻找可以从事 MLOps 的工作，结果很幸运地找到了一家医疗保健初创公司，我成为了那里的第一位全职工程师。我有医疗保健模型方面的经验，所以感觉这个行业很合适。我找到了他们，他们也找到了我，这纯属运气。</p><p>&nbsp;</p><p>在那里工作的三年是一段充满挑战的旅程，主要因为我从专注于模型转向了编写平台。我一直是那种喜欢做研究的人，我会实现各种论文了结果，并尽可能多地从 Kaggle、众多教授和学者那里汲取知识。举例来说，我对深度学习模型和表格模型之间的差异很着迷，尤其好奇为什么深度学习应用在处理表格数据时经常遇到困难，而基于树的模型却表现出色。我读过论文，在 X/reddit 上讨论这个话题，我就是那种人。但当我转而编写平台时，我才意识到自己还需要学习很多关于编写生产级代码的知识。一开始我搞砸了很多事情。</p><p>&nbsp;</p><p>在一开始的几个月里，我们在 Jupyter 笔记本中进行模型评估，我阅读了一些有关如何深入评估医疗保健模型的论文，也考虑到了性别和种族偏见。工作一开始很有趣，但后来我们必须将所有内容集成到一个平台中。那时我慢慢开始了解面向对象编程、系统设计和传统软件工程的原理。我不是计算机科学工程师，所以我是一点一滴学会了这些东西。</p><p>&nbsp;</p><p>创业生活非常紧张，工作时间长，学习曲线陡峭。我们构建的系统使用了 MongoDB、Python、RabbitMQ、S3 和 AWS——一个相当标准的管道。我们的平台旨在验证医疗保健模型，获得 FDA 的批准，并确保一切都正确完成。数据来自合作伙伴，但模型供应商从未看过原始数据，因为他们不应该有这个权限。因此，我们的业务目标是根据获得的黑箱数据来验证模型，并为 FDA 准备必要的文件。</p><p></p><h2>MLOps 平台与业务逻辑，我们是只部署模型还是为客户提供服务？</h2><p></p><p>为了让我们的平台实现目标，它需要支持所有类型的医学图像、验证任何计算机视觉模型并检测出可能存在的模型偏见。三年来，这个平台关注的重点也在改变。第一年，我们的目标是部署和验证模型。第二年，我们增加了注释功能，支持了医疗保健数据，并实现了云集成。到第三年，我们意识到我们需要关注客户的一些特定需求。</p><p>&nbsp;</p><p>挑战之一是将平台的逻辑与特定于客户的代码库分开来。我们花时间编写了特定于客户的代码，这些代码会让平台上 80% 的功能受益，但有时我们必须为特定客户实现非常具体的逻辑。这引发了很多关于我们是否应该用特定于客户的功能来增强平台，还是将它们分离开的争论。如果用太多特定于客户的功能来增强平台，它可能会变得臃肿和混乱。另一方面，当客户端代码库需要访问平台数据时，将它们分离开可能会导致复杂的情况。</p><p>&nbsp;</p><p>我还是没法确定如何区分 MLOps、MLE、后端工程和业务逻辑。可能这没有唯一的答案，但我认为我们在维护和开发平台方面做得很好。</p><p></p><h2>在云端部署 vs 在本地部署</h2><p></p><p>最近我参加了银行业的 MLOps 职位面试，我 90% 的时间都在写代码。面试我的经理是一位从 DevOps 过渡到 MLOps 的人，我相信他已经开发了一些 ML 模型。对他来说，模型只是具有特定输出的 docker，你还需要做管理/跟踪/记录的工作。对于一些人来说，这才是真正的 MLOps 工程。我绝对同意这一点。</p><p>&nbsp;</p><p>他的团队正在 Apache Airflow 上部署模型，他问我在这方面有什么经验。我想到的是：</p><p>&nbsp;</p><p>正如问题所暗示的那样，我在电力问题上训练/部署的模型是日常模型，所以我不需要跟踪模型，做预测就行了。我不需要检查模型是否一直处于活动状态，也不需要检查吞吐量或延迟。我在医疗保健 MLOps 问题中使用的模型是基于项目的模型，不需要不断构建模型，而且它是本地的，所以安全不是主要问题。</p><p>&nbsp;</p><p>他的团队一直完全不写代码，至少在目前是这样。在面试过程中我意识到了这一点，这很奇怪。</p><p></p><h2>身份危机：MLOps 工程师、ML 工程师，还是两者兼而有之？抑或其他？</h2><p></p><p>多年来，另一个问题一直浮现在我的脑海中：我是谁？我是 MLOps 工程师、ML 工程师、ML 研究员还是后端工程师？在一个小团队或初创公司中，你需要具备所有这些能力。你听说过“10x 工程师”吗？那些奇怪的职位要求你同时发布 NeurIPS 论文和 Node.js 代码？伙计，这到底是为什么？我见过很多这种奇怪的组合。不是吹牛，但下面这些都是我所做的工作：</p><p>&nbsp;</p><p>预印我在斯坦福实习期间的一篇论文 = ML 研究员编写表格和 CV 模型 = 数据科学家实现一个 Auto-ML 模块，允许即时训练/微调对象检测、分割和分类模型，并进行版本控制 = Python 软件工程师、ML 工程师使用 Grad-CAM 为 CNN 模型开发一个可解释性库 = ML 研究员使用 FastAPI 支持模型，与前端应用程序集成 = 后端工程师使用 Docker、RabbitMQ、MongoDB 和其他工具设计和实现一个平台。= MLOps 工程师</p><p>&nbsp;</p><p>当然这些术语是可以互换的，当然优秀的 MLE 必须是一名优秀的软件工程师，当然你需要考虑如何在训练模型时部署模型，但我到底是什么角色？我觉得我被“学得更多，收获更多”这句话欺骗了。</p><p></p><h2>10x 工程师神话：什么都会，某些事情还要精通（也许做不到）</h2><p></p><p>那么我从中得到了什么？我是不是一名没什么专长的 10x 工程师？好处是，我可以申请数据科学、MLOps、MLE 和 Python 后端职位，这里的重点是 ML。而且我参加了所有这些职位的面试。但问题是什么？在面试过程中，他们并不总是相信我的经验广度。他们会问很多问题，即使我有很好的答案，一些面试官也会试图让我出局。如果你没有给出他们想要的具体答案，你就出局了。</p><p>&nbsp;</p><p>我在一家全球数据科学公司的面试中就遇到过这种情况。对于我提到的所有话题，他都会质问我，还会不断切换话题背景，让我觉得自己像个白痴。当他发现我在 3 小时的面试后给出了一个不太好的答案时，他说：“我就知道。”我都无语了，你知道什么？当然，我不是精通所有领域的专家，但这也很让人不爽。</p><p>&nbsp;</p><p>还有一次，我被一个非常聪明的人面试了——他曾经在知名公司工作，在排名前十的大学获得博士学位。他非常聪明，我希望十年后能成为他那样的人。他很善良，我非常尊重他。我告诉他我的经历和我做过的事情，并问我还能做些什么。但你猜怎么着？他希望我在 LLM 方面有更多经验，他说：“我没有投资潜力股的预算；我需要员工在那个特定主题方面有足够经验。”没错，但这样的话我该怎么办？我应该追随每一个深度学习趋势并以赚钱为目的研究它们吗？这有多大可能？你能对一个只有四年经验的人能有什么期望？</p><p></p><h2>我将来想成为什么样的人？</h2><p></p><p>我不知道。我真的不知道。我知道的是：</p><p>&nbsp;</p><p>FOMO（错失恐惧）在 ML 领域中是真实存在的。如果你尝试学习 MLE，那么 MLOps、软件工程或者一些 DevOps 方面的知识和经验可能不会让你更有信心。</p><p>&nbsp;</p><p>原文链接：<a href="https://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html">https://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</id>
            <title>国内近 50 款 AI Agent 产品问世，技术足够支撑应用可靠性了吗？</title>
            <link>https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Sep 2024 02:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在2024年5月发布的《中国AGI市场发展研究报告》中，InfoQ研究中心将 AI Agent 定义为连接模型层与应用层的中间层，是现阶段大模型落地应用的重要补充。那在过去的两个季度，AI Agent 领域发生了诸多变化，本文希望通过分析技术框架、理想与现实的差距，以及厂商背景，为大家提供对AI Agent现状的全面理解。</p><p></p><p>更多关于 AI Agent 的具体应用案例，欢迎点击<a href="https://www.infoq.cn/minibook/bTgj82D3gFJK9ZLRM5Ci">「链接」</a>"下载完整报告。</p><p></p><h3>AI Agent技术框架趋于统一</h3><p></p><p></p><p>自 2023 年 3 月起，以 AutoGPT 为代表的一系列技术框架发布后，AI Agent 凭借其自主性和问题解决能力，迅速成为科技圈讨论的焦点。在随后的时间里，技术领域陆续推出了多种 AI Agent 技术框架，涵盖通用、环境模拟、软件开发、多模态、翻译、终端交互、数据分析等多种类型。同时，关于单智能体与多智能体的讨论也在持续。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/834fcc697a798b2e2094e5a9c36a18a0.png" /></p><p></p><p>在技术框架的不断探索中，AI Agent 的技术框架认知逐渐统一。大模型作为智能体的大脑，指导规划、工具使用、记忆三大基本能力模块具体行动。并在具体行动过程中，通过与环境、其他智能体以及人类的交互反馈，促进智能体的不断进化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14e4813caf4292545d135c6bd49bc2ef.png" /></p><p></p><p></p><h3>大模型「大脑」足够聪明到支撑AI Agent落地了吗？</h3><p></p><p></p><p>从技术框架的角度，我们可以看到大模型在智能体中的重要性，这也引发了一个关键问题：大模型「大脑」是否足够聪明以支持 AI Agent 的实际落地？</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb9b1ff210b4c8bb46d43f45a213ebde.png" /></p><p></p><p>作为智能体的大脑，大模型在短短两年内经历了三次主要更新和竞争重点的转变。然而，针对工具调用或真实环境模拟的国内外测试结果显示，当前大模型的表现仍不尽如人意。例如在 WebArena 测试中，GPT-4的成功率也仅有 14.9%，今年发布的 GPT-4o 也并没有获得明显提升。</p><p>注：WebArena通过构建一个智能体命令和控制环境，通过对大模型在电子商务、社交论坛、软件开发协作和内容管理四类环境中一系列评估任务的功能正确率进行评估。网址：<a href="https://webarena.dev/">https://webarena.dev/</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e20b32ee4aa49103929a263a6b4adf6.png" /></p><p></p><p>此外，在T-Eval基准测试中，各大模型在推理得分方面普遍偏低且模型间差距明显。</p><p>注：T-Eval大模型智能体基准测试，是专门针对智能体工具使用的全过程设计的基准测试，包含：规划（Plan）、推理（REASON）、检索（RETRIEVE）、理解（UNDERSTAND）、指令跟随（INSTRUCT）和审查（REVIEW）。</p><p><img src="https://static001.geekbang.org/infoq/66/6628a99507688e9badec3003ac0a7697.png" /></p><p></p><p>在本次报告的访谈中，来自一线的专家也提及，当前大模型在任务拆解和规划能力方面仍存在明显不足。现阶段，依赖大模型进行独立思考和自主规划路径的方式，尚不足以确保智能体的可靠性和任务成功率。</p><p></p><h3>理想中的智能体和现阶段有哪些差距？</h3><p></p><p></p><p>除了规划能力与理想状态存在一定差距外，InfoQ研究中心还从自主思考、工具调用、记忆和多模态理解等方面，深入分析了理想中的智能体与现阶段智能体之间的差距。这样的技术现状也对 AI Agent 的开发与应用提出了更高的要求，迫使技术团队不断优化系统的可靠性，以实现更加全面的任务执行能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e715fe5812e09f0a213b0e0e6abe5adf.png" /></p><p></p><p></p><h3>目前中国市场中，有哪些AI Agent产品已经面世？</h3><p></p><p></p><p>InfoQ研究中心还发现，目前在各个领域，已有众多不同类型的AI Agent产品面世，并且不同的产品从例如工作流等不同的方面提供了技术解决方案。因此InfoQ研究中心也从平台类和垂直类的角度出发，盘点了近 50 个中国市场中的 AI Agent 产品，并形成《中国 AI Agent 产品罗盘》。</p><p></p><p>《罗盘》仍将持续更新，欢迎各位开发者和读者朋友们积极反馈和持续关注，也欢迎各类厂商参与交流，与InfoQ研究中心分享技术和产品的最新动态（联系方式：InfoQ研究中心首席分析师 姜昕蔚：18618257676）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a7943dfbc3672f5546c807c4b3b22dc.jpeg" /></p><p></p><p></p><h3>参与AI Agent市场竞争的厂商背景如何？</h3><p></p><p></p><p>除了产品盘点外，InfoQ研究中心在对市面上对外提供 AI Agent 服务的厂商进行研究，并发现其背景主要分为大模型创业厂商、互联网科技厂商、RPA/流程自动化厂商和数字化企业服务商。</p><p></p><p>大模型创业厂商：以 Dify、澜码科技、面壁智能为代表，借助自身大模型技术基础，满足企业大模型技术实际应用的需求。其主要竞争优势在于对大模型具有技术前瞻视角。其主要通过提供AI Agent 应用市场 &amp; 开发平台，为用户提供构建 AI agent 的便捷服务。</p><p></p><p>互联网科技厂商：以百度、火山引擎、腾讯为代表，借助借助自身大模型以及 AI 云服务，为客户提供完整的 AI 技术解决方案。因其自身基础设施、云、大模型等AI 生态建设完整。同时先前多推出了大模型相关的应用，建立了较为良好的用户基础和产品迭代模式。其主要也通过提供 AI Agent 应用市场 &amp; 开发平台，为用户提供构建AI agent的便捷服务。</p><p></p><p>RPA/流程自动化厂商：以来也科技、实在智能为代表，其主要将 AI Agent 技术思路集成进原有RPA产品中，依托自身长期积累的企业内流程自动化落地经验，为客户提供更智能化的 AI+RPA 类产品和服务。</p><p></p><p>数字化企业服务商：以用友、金蝶、标普云、数势科技为代表，依托自身长期积累的垂类领域或行业的 Know-how，实现企业内数字化系统的功能升级。对于此类厂商而言，AI Agent 多作为一个功能组件，内置进数字化系统，通过完善的 API 联动生态，实现与原有数字化系统的深度集成，从而让用户无感地体验 AI Agent。</p><p></p><p>更多关于AI Agent在数据分析、营销、金融、文娱游戏等的具体应用案例，欢迎点击<a href="https://www.infoq.cn/minibook/bTgj82D3gFJK9ZLRM5Ci">「链接」</a>"，下载完整报告阅读。InfoQ研究中心也期望通过持续的内容输出，继续支持中国AI领域的发展。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WFlwMggJjIVBIQGt7XrA</id>
            <title>OpenAI 发布最新模型 o1，这次变为华人扛大旗？一分钟搞出 3D 版贪吃蛇，好用但小贵</title>
            <link>https://www.infoq.cn/article/WFlwMggJjIVBIQGt7XrA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WFlwMggJjIVBIQGt7XrA</guid>
            <pubDate></pubDate>
            <updated>Sun, 15 Sep 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>9 月 12 日，OpenAI 万众期待的“草莓”（Strawberry）终于上线了。这一新模型名为 o1，是 OpenAI 推理模型家族的首位成员，能够解决现有 AI 模型所无法攻克的科学、编码和数学难题，甚至包括 OpenAI 最强大的现有模型 GPT-4o。但与此同时，o1 模型也比 GPT-4o 价格更贵、生成速度更慢。</p><p></p><p>该公司表示，o1 模型并不是 GPT-4o 的继任者，而只是对 4o 的强大补充。o1 不再像传统大语言模型那样一步步得出答案，而是通过推理拆解问题，像人一样行之有效地给出思维步骤，最终得出正确结果。并且，OpenAI 还推出了体量更小、价格更低的 o1-mini 版本。</p><p></p><p></p><h1>思维更像人了，编码、数理能力指数级增长</h1><p></p><p></p><p>“使用 o1 在一分钟内创建 3D 版本的贪吃蛇游戏！”在国外某社交平台上，一位网友发布了完整的演示视频。</p><p></p><p>还有网友通过合并o1 和 Cursor Composer，在10 分钟内为 iOS 创建了一个带有动画的完整天气应用程序。</p><p></p><p>有网友这样评价 o1，“编码和数学能力是 GPT-4o 的指数级增长！现在每个人都可以建造任何东西！”还有人称，“Cursor 和 Replit 的压力暴增”，“奥特曼用 o1 杀死了 Cursor、Replit 和其他许多代码模型”。</p><p></p><p>OpenAI 官方还则展示了使用这套新模型解决现有模型 GPT-4o 也无法解决的几个问题，其中包括两道令人费解的数学题。</p><p></p><p>第一道：“已知公主的年龄比王子大，当公主的年龄是王子年龄的两倍时，此时公主年龄是二人现在年龄总和的一半，问公主和王子现在多大？”该模型成功理解了这一问题中的不同变量并确定了解决问题所需的方程式，最终分步给出了正确答案（王子 30 岁，公主 40 岁）。</p><p></p><p>OpenAI 还专门设计了界面来显示模型思考时的推理步骤，其中令人印象深刻的是，o1 似乎在刻意模仿人类的思维：“我很好奇”、“我正在认真思考”和“好的，让我想想”之类的表达，真的营造出一种它在分步推理的感觉。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c4/c429efd5bccf5918331c5d94bd5fd4ef.png" /></p><p></p><p>OpenAI 首席研究官 Bob McGrew 表示，“与之前的模型相比，o1 的确在某些方面感觉更像人。”同时，McGrew 指出，“这套模型在解决 AP 数学考试方面的表现比我更好，我还是数学专业出身呢。”OpenAI 研究副总裁 Mark Chen 则介绍称，“新模型正在学习独立思考，而不是像传统大模型那样试图模仿人类的思维方式。”</p><p></p><p>第二道，要求 o1 数出“strawberry”这个单词里有几个 r。由于基于 token 化（即大模型以 token 数据块的方式处理单词）模式，大多数语言模型对于单词中个别字符的差异往往视而不见。很明显，o1 具有自我反思能力，可以在未经用户提示的情况下理解如何计算字母数量并给出正确答案。</p><p></p><p>据 OpenAI 介绍，在针对数学专业学生的美国数学邀请赛（AIME）中，GPT-4o 的平均解题成功率为 13.4%，而 o1 的正确率则高达 83.3%。在 Codeforces 在线编程竞赛当中，这套新模型的排名为全体参赛者中的第 89 百分位。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/79c755621c999a89773b4a992061b21a.png" /></p><p></p><p>“刚刚测试了一些物理问题，目前的测试结果是 100% 正确的。”物理学科上，o1 的表现也获得了不少网友的认可。</p><p></p><p>OpenAI 宣称，在解决博士水平的物理问题时，o1 达到了 92.8 分的水平，GPT-4o 才 59.5 分。并且， o1 的下一个更新版在其测试中，“在物理、化学和生物学等具有挑战性的基准任务上，拥有与博士生相当的解题表现。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/371773cbfcc070534c3ce5c2da7c930d.png" /></p><p></p><p>不过，对于这个评分结果似乎也存在一些争议。有网友指出，“AP 物理成绩是 89 分，而博士成绩是 92.8 分？如果按照这个结论，这些数字就说不通了。原因在于博士水平的问题来自预制题集，而非一般问题。实际上，这仍属于 AP 物理能力水平。”</p><p></p><p>另外，在 OpenAI 官方分享的演示中，还有一个要求 o1“写一个语法正确的句子，并且不要重复使用任何字母”的案例。最终，o1 花了 39 秒的时间给出了“go fix my bed”的答案。</p><p></p><p>从目前 o1 的这些案例演示中可以看到，该模型的确更适合解决数学、物理和编码等科学领域的复杂问题。OpenAI 也在官方公告指出，“医疗研究人员可以使用 o1 模型标注细胞测序数据，物理学家可以使用 o1 模型生成量子光学所涉及的复杂数学公式，各领域的开发人员也可以使用 o1 模型构建并执行多步骤工作流程。”</p><p></p><p>然而，现在 o1 还有许多不足之处。首先，目前亮相的 o1 预览版仍有一定局限性，如无法浏览网页或接收上传的文件和图像。OpenAI 表示，对于这类任务，GPT-4o 仍是最佳模型选项。</p><p></p><p>其次，o1 初见之下令人惊艳，但也已在用户的长时间使用中暴露出短板。例如，沃顿商学院教授 Ethan Mollick 向 o1-preview 提交了八条关于填字游戏的线索，要求其将内容翻译成文本。o1 模型通过多个步骤共耗时 108 秒才给出答案，虽然结果完全正确，但虚构了一条 Mollick 并未给出的特定线索。</p><p></p><p>还有一位网友指出，o1 不能解决后向传播推理的问题。</p><p></p><p></p><blockquote>输入：给定这个不完整的序列：. . . . . 1 2 3 3 2 1 1 2 3 3 2（空白处用“. ”表示），请补全序列。o1 的结果：位置 1-7 错误即使 1-7 是正确的，也错误地将其放在完成的序列中。完整序列中的错误‍</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0f/0f9dcdeffe1e2774592745b8e10e377d.png" /></p><p></p><p></p><h1>定价高 GPT-4o3 倍以上，改进目标竟是加长响应时间</h1><p></p><p></p><p>“o1 背后的训练方式与之前的大模型有着根本性的不同。”OpenAI 公司研究负责人 Jerry Tworek 解释称。尽管该公司对于具体细节含糊其辞，但承认 o1“使用的是一种全新的优化训练算法，同时辅以为其量身定制的新训练数据集。”</p><p></p><p>OpenAI 已经成功教会之前的 GPT 模型如何模仿训练数据中的模式，而 o1 使用的是名为强化学习的技术以训练模型自主解决问题。这项技术通过奖惩机制引导系统行为，即在模型答对时给予正反馈、答错时给予负反馈，借此改进其推理过程。以此为基础，o1 可以使用“思维链”来处理查询，类似于人类以分步方式解决问题。</p><p></p><p>OpenAI 表示，凭借这种新的训练方法，o1 模型生成的结果应该会更加准确。Tworek 指出，“我们注意到这套模型的幻觉有所减少”，但问题仍然存在，“尚不能说我们彻底消灭了幻觉。”</p><p></p><p>据 OpenAI 介绍，这套新模型与 GPT-4o 最大的区别在于，能比前辈更好地解决编码和数学等复杂难题，同时还可对推理过程做出解释。Tworek 表示，OpenAI 并不认为 AI 模型的思维等同于人类思维。但他表示，推理界面确实能够展示模型如何花费更多时间深入剖析并解决问题。</p><p></p><p>Chen 解释称，OpenAI 已经成功建立起一套通用性更强的推理系统。“我认为我们确实在这方面取得了一定突破，而这也是 OpenAI 的优势之一。实际上，它在所有领域的推理方面都有相当不错的表现。”</p><p></p><p>但与此同时，o1 在很多领域的能力则不及 GPT-4o。OpenAI 推理研究科学家、德扑 AI 之父 Noam Brown 表示，“我们的 o1 模型并不总是比 GPT-4o 好。许多任务不需要推理，有时等待 o1 响应与快速 GPT-4o 响应是不值得的。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4a/4ab4f0a7d51437bf2553ec6562b1c4bb.png" /></p><p></p><p>想象的 openAI 的 o1 在后台工作</p><p></p><p>值得一提的是，对于 o1 的推理思考时间，未来 OpenAI 的改进方向是继续增加。</p><p></p><p></p><blockquote>目标是让未来的版本思考数小时、数天甚至数周。推理成本会更高，但您会为新的癌症药物支付多少费用？又会为突破性的电池和证明黎曼假说付出多少？AI 可以不仅仅是聊天机器人。</blockquote><p></p><p></p><p>在谈到安全问题时，OpenAI 表示，根据拜登总统先前发布的 AI 行政令规定，o1 模型已经向美国和英国安全研究机构开发以接受早期测试。在 OpenAI 最难的越狱测试之一中，GPT-4o 得分为 22（0-100 分），而 o1-preview 模型得分为 84。</p><p></p><p>定价方面，o1 大约是 GPT-4o 和 100x 4o mini 的 3.5 倍。就目前来看，开发人员要想访问 o1 需要支付高昂的价格：在 API 中，o1-preview 每 100 万个输入 token（即供模型解析的文本块）收费 15 美元，每 100 万个输出 token 收费 60 美元。相比之下，GPT-4o 每 100 万个输入 token 只收费 5 美元，而 100 万个输出 token 则收费 15 美元。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/92718c9cbc85a3511c12e874142dec6b.png" /></p><p></p><p>为了向开发人员提供更高效的解决方案，OpenAI 还发布了更经济高效的的 o1-mini，比 o1-preview 便宜 80%，适用于需要推理但不需要广泛世界知识的应用程序。OpenAI 方面表示，他们计划将 o1-mini 的访问权限向全体 ChatGPT 用户免费开放，但具体发布日期未定。</p><p></p><p>即日起，ChatGPT Plus 和 Team 版的用户从可以访问到 o1-preview 和 o1-mini 模型，Enterprise 和 Edu 用户则将于下周起获得访问权限。</p><p></p><p>值得一提的是，在 o1 模型背后的研究团队里，有不少华人开发者的身影。从 OpenAI 发布的 o1 模型核心贡献者名单里，我们也看到了许多华人的姓名：Chong Zhang、Mengyuan Xu、 Mingxuan Wang、 Lilian Weng 等。</p><p></p><p></p><h1>提高 AI 智能水平，不必依靠无限的规模提升</h1><p></p><p></p><p>去年，OpenAI 发布的 GPT-4 模型已经将参数规模扩大到令人难以置信的水平，同时也成为 AI 领域最具份量的重大突破。如今，该公司再次带来最新进展，也标志着生成式 AI 在方法论层面的转变——新模型能够以逻辑方式“推理”解决诸多难题，其智能水平要远超现有 AI 方案，而且无需依靠无穷无尽的规模提升。</p><p></p><p>OpenAI 公司首席技术官 Mira Murati 在采访中表示，“我们认为这将成为 AI 模型中的新范式，而且在处理高度复杂的推理任务方面有着明显更好的表现。”</p><p></p><p>Murati 解释称，OpenAI 目前正在着力构建下一代主模型 GPT-5，且将在体量方面远远超过其前身。尽管 OpenAI 仍然坚信提升规模有助于帮助 AI 挖掘出新的能力，但 GPT-5 也可能会同时融入此番公布的推理技术。Murati 指出，“大语言模型拥有两种范式，一种是传统的扩展范式，另一种就是这种推理新范式。我们希望能把二者合而为一。”</p><p></p><p>OpenAI 此番发布的技术，也有望保障 AI 模型不偏离正确的行为轨道。Murati 指出，新模型已经证明自身能够有效避免产生令人不悦或者潜在有害的输出，因为它能够展示行为结果的推理过程。“这就像是教导孩子，只要他们能够推理为为什么要做某件事，就可以更好地学会遵循某些规范、行为和价值观。”</p><p></p><p>如何提高大语言模型的推理能力，一直是 AI 研究领域的热门议题。事实上，其他竞争对手也在进行类似的研究。今年 7 月，谷歌就公布了 AlphaProof，能够通过查看正确答案来学习如何推理并解决数学问题。但扩大这种学习方式的一个关键挑战，在于模型可能遇到大量不存在正确答案的问题。</p><p></p><p>斯坦福大学教授 Noah Goodman 则发表了关于提高大模型推理能力的论文，在他看来通用训练的关键可能在于使用“精心编写的提示词配合手工生成的数据”对语言模型进行训练。他补充称，能够比较稳定地用推理速度来换取更高的准确性，本身就是个“很大的进步”。</p><p></p><p>麻省理工学院助理教授 Yoon Kim 也提到，大语言模型的解题过程目前仍然相当神秘。即使模型进行逐步推理，其底层机制也可能与人类智能存在巨大差异。随着该技术得到广泛应用，这种差异自然值得引起高度重视。“这些系统可能将被用于做出影响普罗大众的决策。而更重要的问题在于，我们能否充分信任模型做出的决策。”</p><p></p><p>华盛顿大学名誉教授、著名 AI 专家 Oren Etzioni 评论称，“让大语言模型能够参与到多步骤问题解决、工具使用和复杂问题的解决中来，无疑至关重要。”他同时补充称，“单纯扩大规模不足以实现这个目标。”然而 Etzioni 也承认未来还有更多挑战需要克服。“即使推理问题得到解决，我们也仍面临着幻觉与事实之间的冲突。”</p><p></p><p>OpenAI 研究副总裁 Mark Chen 则做出乐观解释，表示该公司开发的新推理方法表明，推进 AI 发展并不一定需要耗费大量算力。“这种模式最令人兴奋的一点，在于我们相信它能让我们以更便宜的价格交付智能成果。我认为这也正是 OpenAI 公司的核心使命所在。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/">https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/</a>"</p><p></p><p><a href="https://mashable.com/article/openai-releases-project-strawberry-o1-model">https://mashable.com/article/openai-releases-project-strawberry-o1-model</a>"</p><p></p><p><a href="https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt">https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt</a>"</p><p></p><p><a href="https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/">https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IY4OecDiTIruXFTsDU8R</id>
            <title>OpenAI 威胁用户撤销 o1 访问权，仅仅因为询问了 o1 思维链原理！</title>
            <link>https://www.infoq.cn/article/IY4OecDiTIruXFTsDU8R</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IY4OecDiTIruXFTsDU8R</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 09:46:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>“这太反乌托邦了。询问 ChatGPT 的工作原理可能会被禁止……”网友<a href="https://twitter.com/bhohner">bhohner</a>"在X上晒出了自己询问“OpenAl指南避开了详细的内部推理，突出了清晰度、证据和事实准确性”是什么意思的时候被禁止，并收到了OpenAI团队邮件警告被标记为违反政策。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c70ac2990277fd85380e481e4afa98f5.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b940f5a508f6fe276e8f3c8bdf07685d.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>无独有偶，网友“SmokeAwayyy”爆出，如果向 ChatGPT o1 询问几次有关其思维链的问题，OpenAI 团队就会发送电子邮件并威胁要撤销其 o1 访问权限。电子邮件内容如图所示：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c7b4694f0fd03d4611c6fc3ba70b4aa8.png" /></p><p></p><p>&nbsp;</p><p>“SmokeAwayyy”帖子下面，网友Dallas也发布了自己被限制的截图，并给出了OpenAI团队邮件的完整版：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d5f6bc7f8d32d34e89145cfd0481bbe.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c786fdfc6ec433da2dd581010c5478a7.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>网友LewisNWatson表示，o1 被明确指示不要泄露使用“推理标记”完成的“隐藏的思路链”，并且不要让用户欺骗它或“一步一步地要求”。同时他也表示，“但是没错，o1 似乎确实是 4o（可能使用链/推理标记示例进行了微调）和 CoT的结合”</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79f526fbe49447748c3a23d21c50e428.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/57/570efb1f0cce01bc9bd76a03d0b74860.png" /></p><p></p><p>&nbsp;</p><p></p><h2>不满的社区</h2><p></p><p>&nbsp;</p><p>OpenAI如此措施，立刻引起了社区的不满和质疑。</p><p>&nbsp;</p><p>“说实话，这事越想越觉得可疑。OpenAI隐藏思维链的借口本身就站不住脚，不管怎么想，最大的理由应该也是防止市场竞争。更糟糕的是，他们还诡辩称这是为了防止用户规避提示词——别扯了。而且神奇的是，OpenAI居然还说是为了发挥其推理能力，模型必须能够自由且以未经改变的方式表达自身想法，就是说我们不能将任何政策合规或者用户偏好训练到思维链当中。 很明显，他们确实是不想公开模型的‘想法’。但实际采取的举措却严格到过分，或者至少比以往要严格得多，也进一步加强了人们对其想要保持垄断地位的怀疑。 老实讲，如果真从阴谋论角度理解，那o1可能已经具备秘密策划、灭亡人类的能力——只是它还没那么聪明，会在思考过程中暴露想法。”</p><p>&nbsp;</p><p>“很明显，他们确实是不想公开模型的‘想法’。但实际采取的举措却严格到过分，或者至少比以往要严格得多，也进一步加强了人们对其想要保持垄断地位的怀疑。 反正我不这么看。 可以考虑一下会不会出现这样的思维链：共和党人（靠反对君主制起家）是邪恶的，这名用户是共和党人，因为他们反对君主制，所以应该跟他们对着干、支持君主制。”AI开发者必须能想到这样的逻辑链条，才能提前消除此类问题。而一旦把这类内容向用户展示，必将引发巨大的公关灾难，毕竟共和党在美国的地位无需赘述。 类似的例子还有print()日志语句中常见的“Killed child”（这里指的是子线程，而非人类儿童），大模型一旦处理不好同样会万劫不复。”有网友指出。</p><p>&nbsp;</p><p>有网友指出，OpenAI这样做是“多此一举”的。“真正的现实是，这种‘创新’明显就是从人们的思维链提示词里收集训练数据，所谓的‘大提升’也是单纯靠这样的数据集在修复ChatGPT缺乏推理能力的弊端。更直白地讲，所谓推理能力提升，在原理上跟当初整理专门的训练数据集、帮助ChatGPT在基准测试中取得更好的成绩没什么区别。这里头，哪有什么‘机密’可值得遮掩？ ” &nbsp; &nbsp;</p><p></p><p>进而有网友分析道，这些模型和提示词都是人用简单粗暴的方式修补而来，根本无法成为迈向通用超级智能的道路。全是技巧，毫无感情。 而一旦意识到这一点，大家就会意识到OpenAI的现有产品没有护城河。只要汇聚起成规模的研究人员和GPU加速器，你也能搞出自己的相同系统。 如果不是世界各地无数企业都在尝试构建大模型、智能体和思维链，其实OpenAI也没必要这么紧张。但就目前的情况看，只要其中一方将关键见解分享给整个生态系统，那么每个人都将获得相同的能力。 从用户的角度看这当然是好事，无尽的竞争代表不断下降的服务价格。 &nbsp; &nbsp;</p><p>&nbsp;</p><p>有网友表示，至少到目前为止，我们能看到的只有少数精心挑选出来的示例，而且这里的“少数”是真的少得可怜。所以，该网友实际情况可以这样总结：“我们为这些推理token付费；根据OpenAI的说法，这些额外的token在模型的最终输出中发挥着重要作用；我们永远看不到推理token的内容；这些推理过程不能因“合规”要求而受到限制（这里合规的涵盖范围极广，包括防止伤害、避免公然种族歧视，也包括保护OpenAI的竞争优势）；以上一切均为道听途说、口耳相传，只来自少数看到过这些输出的项目参与者。于是我们要问了，这真的不是一场骗局吗？”</p><p>&nbsp;</p><p>“要使这套模型发挥作用，它必须能够自由且以未经改变的方式表达自身想法，就是说我们不能将任何政策合规或者用户偏好训练到思维链当中。”有网友还说道，OpenAI是真的不想让大家知道新模型究竟在想些什么，因为一旦被社会活动家或者政客发现模型的“大不敬”言论，内部思维链很可能给OpenAI惹出巨大的麻烦。</p><p>&nbsp;</p><p></p><h2>新模型的一些细节</h2><p></p><p>&nbsp;</p><p>Datasette 的创建者Simon Willison&nbsp;此前发文简单分析了o1思维链模型。他根据OpenAI则在《学习使用大语言模型进行推理（Learning to Reason with LLMs）》论文中的描述认为，该模型能够更好地处理复杂度较高的提示词，而高质量结果需要的更多是回溯和“思考”，而不仅仅是简单对下一token做出预测。</p><p>&nbsp;</p><p>因此，Willison&nbsp;并不太认同“推理”这个词，毕竟推理在大语言模型的情境下仍然缺乏可靠的定义，但OpenAI却在宣传中反复提及，也确实能够在很大程度上反映这些新模型尝试解决的问题。</p><p>&nbsp;</p><p>关于新模型及其权衡中一些有趣的细节，Willison&nbsp;从官方发布的API文档中找到了一些描述：</p><p>&nbsp;</p><p></p><blockquote>对于需要图像输入、函数调用或者持续快速响应的应用场景来说，GPT-4o和GPT-4o mini模型仍是更好的选择。但如果您的目标是开发需要深度推理，并能够适应更长响应时间的用例，那么o1模型可能成为绝佳选项。</blockquote><p></p><p>&nbsp;</p><p>Willison&nbsp;从文档中总结出了以下几个关键点：</p><p>&nbsp;</p><p>目前，只有5级账户才能访问到新的o1-preview与o1-mini模型API——意味着用户至少需要花费1000美元来购买API积分。不支持系统提示词——这些模型使用现有Chat Completion&nbsp;API，但用户只能发送user和assistant消息。不支持流媒体、工具使用、批量调用或者图像输入。“根据模型解决问题所需的推理量，这些请求可能需要几秒到几分钟时间才能获得响应。”</p><p>&nbsp;</p><p>最有趣的是新模型引入了“推理token”的概念——这些token在API响应中不可见，但仍会按照输出token的形式计算。正是有它们的存在，新模型才表现出种种神奇的推理能力。</p><p>&nbsp;</p><p>鉴于推理token的重要性，OpenAI建议在使用新模型时为提示词分配约2.5万个token的预算。新模型的输出token上限也已大幅增加，o1-preview的输出token上限为32768个，而体量较小的o1-mini更有65536个token上限！这一数字比GPT-4o和GPT-4o-mini模型更大，后两者目前的输出token上限为16384个。</p><p>&nbsp;</p><p>这份API文档中还有另外一条有趣的说明：</p><p>&nbsp;</p><p></p><blockquote>限制检索增强生成（RAG）中的附加上下文：在提供附加上下文或者文档时，请仅包含相关度最高的信息，以防止模型过度复杂化其响应结果。</blockquote><p></p><p>&nbsp;</p><p>这与RAG常规的实施方式存在很大不同，RAG一般建议将尽可能多的潜在相关文档塞进提示词当中。</p><p>&nbsp;</p><p>“让人有点难以接受的是，这些推理token在API中完全不可见——用户只为它们付费，但却看不到它们的内容。”Willison&nbsp;在文章里也提到了最近网友们遇到的问题。OpenAI在《将思维链隐藏起来（Hiding the Chains of Thought）》一文中解释了具体原因：</p><p>&nbsp;</p><p></p><blockquote>在可靠且清晰这一前提下，隐藏思维链使我们能够“读懂”模型的想法并理解其思维过程。例如，未来我们可能希望监控思维链以揪出操纵用户的迹象。但要做到这一点，模型必须能够自由且未经改变的方式表达自身想法，就是说我们不能将任何政策合规或者用户偏好训练到思维链当中。我们也不希望把仍存在一致性冲突的思维链直接展示给用户。因此，在权衡了用户体验、竞争优势和寻求对思维链的监控等多种因素之后，我们决定不向用户展示原始思维链。</blockquote><p></p><p>&nbsp;</p><p>这里主要需要关注两大核心要素：其一是安全性与政策合规性——OpenAI希望模型能够自行推理出应如何遵循这些政策规定，同时又不致暴露可能违反这些政策的中间信息处理步骤。其二则是所谓竞争优势——Willison&nbsp;个人的理解是希望避免其他模型效法OpeenAI的研究成果开展强化推理训练。</p><p>&nbsp;</p><p>Willison&nbsp;明确表示对这个政策其实不太满意。“作为面向大语言模型的开发者，可解释性和透明度对我来说至关重要——而现在OpenAI却走上了提示词内容极度复杂、关键细节评估方式不对外公开的路子，在我看来这绝对是种历史倒退。”</p><p>&nbsp;</p><p>OpenAI在其公告的“思维链”部分提供了一些初步示例，具体包括生成Bash脚本、解决填字游戏和为中度复杂的化学溶液计算出pH值等内容。这些示例表明，新模型的ChatGPT UI版本确实公开了思维链细节……但却没有展示原始的推理token，而仅仅是用单独的机制将步骤总结为人类更易阅读的形式。</p><p>&nbsp;</p><p>OpenAI还在另外两份材料里列举了更复杂的例子，Willison&nbsp;表示有点难以理解：</p><p>&nbsp;</p><p>使用推理进行数据验证。这里展示了一个多步骤过程，用于在11列CSV中生成示例数据，而后以多种不同方式对其进行验证。使用推理进行例程生成。其中o1-preview通过编码将知识库文档转换成了大语言模型可以理解并遵循的一组例程。</p><p>&nbsp;</p><p>Willison&nbsp;还在X上征集了几个GPT-4o处理不了，但o1-preview能够正确解决的问题。以下几条精选自热心网友们给出的答案：</p><p>&nbsp;</p><p>你针对这条提示词生成的答案中，一共有多少个单词？——新模型认真考虑了10秒种，然后回答说“There are seven words in this sentence.（答案共有七个字。）”请解释这个笑话的笑点：“两头牛站在田野里，一头牛问另一头：「你对最近流行的疯牛病怎么看？」另一头说：「关我啥事，我是一架直升机！」”新模型的解释很有道理，而其他模型则无法理解。</p><p>&nbsp;</p><p>但必须承认，这种有说服力的例子仍然比较有限。以下是参与了新模型创建的OpenAI研究员Jason Wei的相关说明：</p><p>&nbsp;</p><p></p><blockquote>AIME和GPQA的结果确实很强，但这并不一定能转化为用户可以感知到的收益。即使是从事科学工作的人，也很难找到GPT-4o处理不了、o1表现良好且能够对答案作出明确评判的提示词示例。不过在少数这样的示例当中，o1的表现确实非常神奇。看来我们还得加油寻找更复杂的提示词。</blockquote><p></p><p>&nbsp;</p><p>Ethan Mollick已经投入几周时间体验这批新模型，并表达了自己的初步感受。他在填字游戏中对于o1模型的显式推理步骤印象深刻，特别是以下思考过程：</p><p>&nbsp;</p><p></p><blockquote>我注意到第一行和第一列的首字母不匹配，所以考虑将第一行的“LIES”换成“CONS”以确保对齐。</blockquote><p></p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://simonwillison.net/2024/Sep/12/openai-o1/">https://simonwillison.net/2024/Sep/12/openai-o1/</a>"</p><p><a href="https://twitter.com/bhohner/status/1834411290753679732">https://twitter.com/bhohner/status/1834411290753679732</a>"</p><p><a href="https://news.ycombinator.com/item?id=41534474">https://news.ycombinator.com/item?id=41534474</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TApaZIiJUEbZyXgYYY5Q</id>
            <title>从枫清科技技术实践，看如何打造知识引擎与大模型双轮驱动的企业智能体</title>
            <link>https://www.infoq.cn/article/TApaZIiJUEbZyXgYYY5Q</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TApaZIiJUEbZyXgYYY5Q</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 08:00:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在21世纪的科技浪潮中，人工智能（AI）作为最具颠覆性的技术之一，正以前所未有的速度重塑着各行各业的面貌。其中，生成式AI技术作为新一代的核心引擎，以其强大的数据生成、分析和处理能力，展现出巨大的潜力和无限可能，成为推动我国经济社会高质量发展的关键驱动力。随着AI大模型技术的飞速发展，一场前所未有的智能化变革正在全球范围内悄然兴起。</p><p>&nbsp;</p><p>为了把握这一历史性的战略机遇，我国已将人工智能提升至国家战略的高度，各级政府积极响应，纷纷出台了一系列政策措施，旨在加速“AI+”行动计划的实施。特别是2024年《政府工作报告》中明确提出的“人工智能+”行动，更是为大模型技术的普及与应用指明了清晰的路径和方向，为AI产业的蓬勃发展注入了强劲的动力。</p><p>&nbsp;</p><p>然而，在大模型技术加速向各行业渗透的过程中，一系列挑战与问题也逐渐浮现。尽管大模型在文本生成、图像识别、语音处理等多个领域展现出了强大的赋能能力，但在实际行业场景的落地过程中，却遭遇了诸如“幻觉”现象、推理能力不足、解释性差等难题。这些问题不仅限制了大模型技术的广泛应用，也使得不少企业在尝试引入该技术时陷入了“好玩不好用”的尴尬境地。</p><p>&nbsp;</p><p>面对这些挑战，如何推动大模型技术在企业场景中的深度应用，成为了当前AI产业发展的重要课题。首先，需要加强基础研究和技术创新，不断优化大模型的算法架构和训练方式，以提升其准确性和可靠性。其次，应积极探索大模型与行业场景的深度融合路径，通过定制化开发和场景化应用，使大模型技术更加贴近企业的实际需求。同时，还需要加强跨学科合作和人才培养，构建完善的AI生态系统，为技术的持续进步和应用提供有力支撑。</p><p></p><h2>AI如何赋能新质生产力</h2><p></p><p>在2024中国国际服务贸易交易会期间，由工业和信息化部新闻宣传中心和中国信息通信研究院联合承办的“大模型应用创新论坛”在北京首钢园成功举办。会议期间，来自工信部科技司、工信部新闻宣传中心等相关领导与业界专家共同探讨了大模型技术如何赋能千行百业，提升产业技术水平。</p><p>&nbsp;</p><p>枫清科技（Fabarta）创始人兼CEO高雪峰受邀出席，并发表了题为《AI+行业落地新范式：知识引擎与大模型双轮驱动企业智能化升级》的分享，阐述了他对以大模型技术为代表的人工智能技术赋能产业转型升级的深刻见解。高雪峰认为，“如果仅停留在对话、文本生成或代码辅助这些基础功能上，没有深入地同行业场景进行融合，很难实现真正的生产力提升，也难以对行业产生深远的变革。当今的全球共识是：人工智能作为新一代工业革命的关键技术，只有与行业场景深度融合，才能释放高质量发展的新动能。”</p><p>&nbsp;</p><p>回顾Web 2.0时代，门户网站、平台型网站和社交软件兴起，开启了“+互联网”时代。而随着Web 3.0技术的演进，社会迈入“互联网+”时代，互联网不再只是工具，而是一种思维模式和商业模式，通过互联网重塑各行业生态。打车、用餐、购物、娱乐、获取信息等日常生活方式被彻底重塑。</p><p>&nbsp;</p><p>如今，随着“人工智能+”首次被写入政府工作报告，我们正处在一场“人工智能+”重塑千行百业的时代浪潮中。人工智能技术也不再只是工具或平台，而成为一种全新的思维和商业模式。我们需要从人工智能的角度审视每一个行业，搭建新的生态，重塑其发展模式。未来不久，人工智能将会同样深刻地改变我们的生活方式，为各行各业注入新的活力，迎来前所未有的变革。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c87363ad64edaf9a2f0f808085f5a366.png" /></p><p></p><p>枫清科技创始人兼CEO高雪峰</p><p></p><h2>创新产品矩阵：驱动行业智能化转型</h2><p></p><p></p><p>高雪峰在分享中指出，在机器学习领域，长期以来存在着两种主要的架构理念之争：Model-Centric（以模型为中心）与Data-Centric（以数据为中心），这两种路径的核心目的都是为了提升模型的性能与效果。当我们把这个目标再提升一个维度，为了衡量智能化技术在具体应用场景当中体现出来的性能与效果的好坏，同样会有Model-Centric与Data-Centric两种路径。前者是以模型、算法为核心，将各种企业自身的数据围绕着模型来发挥其价值，不管是简单的RAG技术的实现，还是将企业数据对模型进行微调等等。后者则是以企业本地的数据为核心，将其转化为企业的知识，再结合各种不同模型的能力，解决企业各种智能化的需求。在当前非常多的行业场景实践尝试当中，Data-Centric的理念路径更加有效地解决了企业当前智能化场景落地所遇到的各种困难。</p><p>&nbsp;</p><p>纵观人工智能发展历史，自1956年达特茅斯会议提出人工智能概念以来，AI发展经历了多次技术浪潮。符号逻辑和概率预测作为两大主线交替发展，共同推动了AI技术的进步。联结主义，也就是大型模型所依赖的Transformer技术，本质上代表了概率的方法，其核心是通过输入的字符串来预测下一个字符。而符号逻辑推理的典型代表是过去出现的专家系统。然而，由于各自的局限性，这两种方法在AI的发展过程中并未带来彻底的革命性变革。当前，联结主义的巅峰——AIGC技术，在企业端场景落地和迈向决策智能的过程中，也同样遇到了诸多技术挑战，如模型幻觉、可解释性差、推理能力弱、数据安全可控性以及时效性等问题，这些问题恰恰是符号逻辑推理技术所能够解决的。AI技术要真正实现企业智能化，必须结合符号逻辑的推理能力与概率体系的优势。两者的深度融合将为AI技术提供更强的适用性和可解释性，推动企业智能化转型。</p><p>&nbsp;</p><p>另外，从信息化时代的关系型数据库，到数字化时代的经典数仓、大数据、数据湖、数据中台，每一阶段都伴随着数据基础设施的变革。面向即将到来的智能化时代，企业需要一种新的数据基础设施来支持人工智能技术在决策智能领域中的应用。为了在企业场景中实现更好的智能化效果，枫清科技（Fabarta）坚定地选择了Data-Centric LLM Landing架构，让不同的模型能力服务于企业本地经过组织的数据与知识，再通过行业智能体平台的能力赋能企业的工作流。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/296f41209f1ce94280409f361c1dba93.png" /></p><p></p><p>枫清科技（Fabarta）推出了“一体两翼”产品矩阵，包括自研的多模态知识引擎与行业通用智能体平台，致力于构建未来通用人工智能（AGI）时代的核心数据基础设施，为企业提供大模型与知识引擎双轮驱动的解决方案。枫清·天枢多模态智能引擎基于Data-Centric AI的核心理念，支持图、向量、表格、时序等多种数据模态的融合与处理，为企业提供私有化记忆存储服务及强大的推理能力，并已通过中国信通院代码自研认证，确保技术的自主可控。</p><p>&nbsp;</p><p>同时，枫清·锦书数据血缘治理平台和枫清·瑶光企业知识中台分别在数据与知识的转换和融合方面提供了支持。锦书数据血缘治理平台通过对企业多模态数据的智能解析，构建语义丰富的企业数据资产的导航地图，可以确保数据的来源、传输和用途的透明度和可追溯性，从而增强数据的质量和可信度，并进一步将数据存储于天枢多模态智能引擎中。瑶光企业知识中台则在大模型原生的知识分析与智能体构建上表现出色，可以将企业的多模态数据转化为大模型可理解的知识，实现企业本地数据与大模型泛化知识的相互转化与融合，有效解决了大模型在企业应用中的可解释性差、推理能力弱、模型幻觉、企业数据时效性与权限管理难等难题，加速企业级大模型场景的落地。</p><p></p><h2>深度合作与落地：枫清科技与行业头部企业的智能化实践</h2><p></p><p></p><p>高雪峰认为，实现行业大模型的智能化发展需要经历三个关键阶段：单点应用的AI创新、知识引擎驱动的行业AI智能应用平台，以及融合精准知识与大模型泛化知识的行业大模型阶段。</p><p>&nbsp;</p><p>未来在中国，建设行业领域的大模型以实现行业智能化赋能，必然要从头部的央国企着手。央国企具备需求规模大、产业配套全、应用场景多的显著优势，拥有丰富的场景资源和不同模态的数据积累，也有实力在创新领域投入并获取未来回报。实现具体的智能化场景后，可凭借其在产业链中的影响力推动行业智能化，从而真正落地具有丰富业务场景价值的行业大模型。在B端企业市场中，只有切实提升企业的生产力，产品才具备生命力。因此，人工智能技术仍需在客户的实际场景中进行打磨和复制。目前，枫清科技的产品已被金融、制造、能源等多个行业头部央国企应用在实际生产环境当中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4c8b49bb90ccce83b0adb9f1a763db74.png" /></p><p></p><p>在金融行业，龙盈智达整合多源产业数据与华夏银行内部高质量数据，采用枫清科技的图智能和大模型技术，为银行提供智能化金融营销和风险评估方案，实现了创新金融智能应用。通过图数据库与图算法，构建关系图，揭示潜在客户、实控人关系和风险特征，优化客户挖掘和风险管理。通过AIGC技术实现自动化报告生成，提升了数据处理效率和决策智能化水平，使银行能够快速响应市场需求，显著提高业务效率与精准度，同时确保数据隐私与合规性，可以为金融企业创造显著的商业价值。</p><p>&nbsp;</p><p>在制造行业，立臻科技基于枫清科技的技术，有效提升了企业数智化水平。面对员工管理、数据处理复杂性与多模态数据分析等挑战，立臻科技通过枫清科技提供的知识解析、智能问数、智能工具调用、企业级权限控制等技术，实现了结构化与非结构化数据进行深度融合，支持更精准的员工管理和高效的数据决策。这些创新应用降低了技术门槛，使员工能够轻松使用系统进行查询和操作，大幅提升了管理效率和数据利用率。通过这一创新方案，立臻科技不仅优化了生产流程和管理成本，更增强了企业在智慧工厂建设中的竞争力，这也充分体现了人工智能技术在制造业领域中的巨大价值。</p><p>&nbsp;</p><p>在能源行业，中化信息通过引入枫清科技的“枫清·瑶光企业知识中台”，针对企业结构化数据和非结构化数据，验证和打造共创方案，将数据转换为知识，利用平台快速构建智能应用，发挥数据的价值，构建企业智能化升级之路。基于双方联合打造的灵活自主可控核心服务矩阵，包括知识引擎和智能体引擎两大关键组件，可通过文档问答、智能问数以及智能体方式串联大模型应用与业务系统，助力应用的智能化，提升用户与业务系统的交互效率以及工作和生产效率。</p><p></p><h2>未来展望：探索AI+战略的无限可能</h2><p></p><p></p><p>高雪峰表示，随着国家政策和市场需求的双重驱动，“新质生产力”和“AI+”战略正成为中国经济发展的重要引擎。枫清科技（Fabarta）将继续致力于推动人工智能技术在不同行业的深入应用，通过不断优化和创新的AI产品，助力各行业在智能化转型中获得更大的竞争优势。</p><p>&nbsp;</p><p>未来，枫清科技将进一步深化与行业头部企业及央国企的合作，利用自身在大模型与知识引擎领域的技术积累，共同探索智能技术在新经济形态中的应用潜力。通过不断提升AI技术的实用性和落地能力，枫清科技期待与更多企业携手，共同推动中国经济迈向智能化、数字化的新时代，为实现高质量发展贡献力量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RtEJ34DXUR8ObbtXr1Td</id>
            <title>RAG风口十问：大数据与AI是价值落地还是过度炒作？</title>
            <link>https://www.infoq.cn/article/RtEJ34DXUR8ObbtXr1Td</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RtEJ34DXUR8ObbtXr1Td</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 03:50:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>过去一年多，RAG（检索增强生成，retrieval augmented generation）正成为大数据与 AI 融合的“新宠”。想象一下，当你用 AI 助手快速总结论文或分析数据时，背后可能已经是 RAG 技术在默默发力。</p><p>显而易见，随着生成式 AI 如 ChatGPT 的兴起，“<a href="https://qcon.infoq.cn/2024/shanghai/track/1713">大数据 +AI</a>"”的热度不断飙升，特别是在 RAG 技术的加持下，它们的结合为企业创造价值的潜力正逐渐被认可。</p><p></p><p>不过，技术的发展总是伴随着质疑和探索。虽然很多人看到这股潮流的迅猛发展，但也难免心生疑惑和不安：大数据和 AI 的融合到底是不是又一轮泡沫？它所谓的价值是什么？具体要怎样才能借助 AI 与大数据来提升竞争力？<a href="https://qcon.infoq.cn/2024/shanghai/presentation/6047">RAG </a>"为什么这么火爆？</p><p></p><p>带着这些疑问，日前极客邦科技创始人兼 CEO 霍太稳与腾讯云副总裁、腾讯云大数据负责人黄世飞，Elastic 大中华区副总裁张君侠展开了一场<a href="https://www.infoq.cn/video/AXoAMdovP9xjLyQrgatr">对话</a>"。本文基于本次对话中的讨论整理而来，深入探讨“大数据 +AI”的真实价值、RAG 技术如何从这浪潮中突围，希望能为大家应对这一波技术变革提供一些启发。</p><p></p><p></p><p></p><p></p><h2>1 Data 加 AI 真有价值？&nbsp;&nbsp;</h2><p></p><p></p><p>对于屏幕前的你来说，当在电脑端想要搜索一些知识点或寻找答案时，你是会选择传统搜索引擎，还是像 ChatGPT 这样的 AI 平台？同样地，当你希望能快速了解一篇论文的要点时，会不会直接让大模型帮你做个总结？</p><p></p><p>从 C 端用户的反馈来看，通用大模型无疑已经逐渐渗透进日常工作，特别是在那些比较简单、重复性的任务上，AI 的效率优势显而易见。</p><p></p><p>不过，这只是 AI 大模型的其中一面。在企业级应用、专业性更强的 B 端场景下，大模型是否同样带来效率提升呢？</p><p></p><p>我们倾向于认为答案是正面的。尤其是在 RAG 技术的推动下。RAG 正在成为数据 +AI 的主流应用方案。根据 InfoQ 的统计，RAG 技术在今年的多场技术大会上成为了焦点之一。而且从 arXiv 上与 RAG 相关的文章数量来看，年初时还比较少，而到了年中，相关研究已经呈现显著增长，几乎每天都有新论文发表。这说明，RAG 技术的受欢迎程度在工业界、产业界和学术界正逐渐成为共识。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a711274c7918dd5ee2c17b407df14ca1.webp" /></p><p></p><p>黄世飞指出，过去很多企业虽然积累了大量数据，但未能充分利用它们。如今，大模型技术，尤其是结合 RAG 解决“幻觉”、私域数据使用等问题，便可以有效提升这些数据的应用，解决企业在生产和服务中的实际问题。</p><p></p><p>张君侠也提到，大模型的价值已经逐渐被全球范围内的企业认可，越来越多的项目开始落地，企业纷纷试水 AI 和数据的结合，探索它们能带来的效率提升和业务价值。但在实际应用中，企业也遇到了一些难题，主要集中在具体场景的落地和数据的处理方式。他强调，数据仍是 AI 应用的基础，无论 AI 模型多么强大，数据的质量和有效性决定了其能否在实际业务中创造真正的价值。</p><p></p><h2>2 为什么大数据“不够火”？</h2><p></p><p></p><p>大模型很火、AI 很火、RAG 也很火，但大数据技术本身却似乎没有那么火。</p><p></p><p>“大数据依然非常重要，只是目前它被大模型的光环所遮盖。”黄世飞表示，虽然 C 端用户更关注体验和产品，但要构建一个好的大模型，算力、算法和数据依然是三大要素，而数据的收集、处理和清洗仍是关键，很多公开的大模型没有对外披露如何处理数据，这部分的工作往往被忽视。</p><p></p><p>从企业和市场的角度来看，业界常讨论的“AI for data”或者“data for AI”，也不会是一个“谁主导谁”的问题。数据和 AI 是相辅相成的。大模型的性能不仅依赖于 AI 的算法和算力，要产生好的 AI 模型，首先还是需要大量且高质量的清洗数据。有时候，一些较小的模型，尽管参数规模不如大的模型，但因为数据质量高，表现反而更好。</p><p></p><p>同时，AI 的发展对大数据技术提出了新的要求，特别是在云原生和弹性计算方面。以大模型训练为例，正常情况下只需几百核的算力，但在处理大规模数据时可能需要扩展到几万核，对大数据系统的弹性能力提出了非常高的要求。此外，随着数据量的增长，降低成本和提升存储性能也是大数据领域未来发展的核心。而这正是黄世飞领导的腾讯云大数据部门的工作重点，给企业提供一个轻快易用的智能大数据平台满足这些需求。</p><p></p><p>“企业仍在不断寻找利用好这些数据的新方法，数据量的爆发只会让这个过程加速。没有过去的大数据技术，就不可能有今天的大模型。”张君侠补充道。</p><p></p><p>总之，大数据从未远离，它始终是 AI 背后不可或缺的支撑。无论是过去、当下，还是未来，数据的管理和应用仍然是核心。</p><p></p><h2>3 为什么数据质量很重要？</h2><p></p><p></p><p>大模型本质上是通过数据训练出来的网络，网络中的权重反映了数据的知识结构。因此，大模型本身就代表了数据与 AI 的融合。</p><p></p><p>要训练出一个好的大模型，数据的质量至关重要。通常需要先收集大量数据，可能达到几十个 PB，但经过清洗和去重处理后，实际用于训练的数据可能只有几个 T。而这个过程十分关键，因为数据量越大，对算力的需求就越高，数据清洗则可以降低计算资源的消耗。</p><p></p><p>从技术流程来看，数据从收集、清洗到用于模型训练的每一步，都离不开大数据系统。腾讯云提供了从数据的收集、处理、开发到训练的全流程支持，确保数据与 AI 深度融合。通过这套方案，开发者和企业可以更便捷地训练出他们所需的模型。</p><p></p><p>而从另一角度看，模型训练完成后，AI 反过来也能帮助优化大数据分析。黄世飞表示，过去，他们需要依赖经验去诊断大数据系统中的问题，但现在，AI 可以通过分析日志和诊断信息来辅助判断。以前可能使用规则引擎，今天大模型让 AI 能够更灵活地处理大数据的复杂问题。</p><p></p><h2>4 企业如何更好地应用 AI？</h2><p></p><p></p><p>实际上，不管是制造业还是其他行业，AI 的应用都依赖于数据平台。比如，生产中的每一条数据都可以视为一个标签，通过 AI 挖掘这些标签与其他数据的关系，就能生成可操作的商业洞察。无论是 AIOps、BusinessOps，还是制造业中的生产优化，AI 都能通过数据分析帮助企业提升效率和决策能力。</p><p></p><p>张君侠进一步解释，AI 还可以处理复杂的操作流程和知识管理。过去，工业领域的操作人员需要依赖手册查找机械操作步骤。如今，通过大模型，AI 可以有逻辑地给出精准的操作指令，减轻操作人员的负担。</p><p></p><p>此外，数据平台的核心在于如何高效导入、处理和展示数据，而 AI 也能够显著提升这一过程的效率。黄世飞举例提到，过去，理清某个数据字段的血缘关系是一项复杂的任务，而现在 AI 可以迅速梳理出数据的来源与关系，提升开发效率。此外，AI 还能帮助自动检测代码错误，大幅提高开发者的生产力。</p><p></p><p>未来，数据平台中很可能会引入 AI 助手，进一步辅助开发者完成数据分析、优化数据处理流程，这将是 AI 赋能数据平台的一个重要发展方向。</p><p></p><p>在讨论企业如何更好地应用 AI 时，黄世飞建议，如果企业已经积累了一定量的数据，可以从小模型或中型模型入手，利用 RAG 方案提升业务效率。如果企业在技术能力方面有限，也可以借助业界的 SaaS 解决方案，从小场景入手，逐步引入 AI 和数据分析技术。</p><p></p><p>张君侠则补充说，传统企业的数字化转型很大程度上取决于文化的转变。如果公司能够将 IT 视为核心资产而非单纯的成本，就能更好地应用数据和 AI 技术，提升整体的业务竞争力。</p><p></p><h2>5 AI+Data 能否超越 Excel</h2><p></p><p></p><p>随着 AI 和数据技术的深度融合，开始出现这样的声音：是否会有一个工具能够超越 Excel，成为数据分析的“新王者”？</p><p></p><p>黄世飞认为，这是完全有可能的。不可否认，Excel 是一款非常强大的工具，几乎可以处理各种类型的报表和分析任务。但是，它的操作门槛较高，用户需要对各种函数有深入的了解，才能真正发挥它的全部功能。对于许多非技术用户来说，这是一个巨大的障碍。</p><p></p><p>“未来的 AI 可能会通过简化这些复杂的操作过程，让数据分析变得更加简单直观。”黄世飞表示，AI 可以通过自动化生成分析过程来帮助用户。用户只需要提出他们想要的结果，AI 就能根据需求选择合适的函数和方法来完成任务。这样的工具将不再依赖用户的专业知识，而是通过 AI 的智能支持，极大降低了使用门槛。</p><p></p><p>除了操作门槛，Excel 的另一个局限性在于它的性能限制。随着数据量的增加，Excel 在处理大型文件时往往会变得非常慢，甚至会导致文件崩溃。而如今，数据量的爆炸式增长已成常态，几百兆甚至上 GB 级别的文件已经不足为奇。</p><p></p><p>云计算有望解决这个问题。云上有强大的存储和计算能力，处理几百 G 甚至 TB 级别的数据都不在话下。如果未来能开发出类似“云 Excel”的应用，将数据存储在云端，并通过云计算来处理，那就能够打破当前 Excel 的数据量限制。</p><p></p><p>因此，未来的应用可能通过两个关键途径超越 Excel：一是通过 AI 简化数据分析的过程，让用户不再需要熟练掌握复杂的函数和操作；二是通过云计算扩大数据存储与处理的能力，打破当前 Excel 在数据量和性能上的限制。随着数据量的持续增长，未来对这种工具的需求也会越来越强烈。</p><p></p><h2>6 为什么是 RAG ？</h2><p></p><p></p><p>大模型的“幻觉”问题，指的是在复杂逻辑推理中，模型生成的结果可能与真实情况不符。而 RAG 的引入，成为当下解决这一问题的重要技术方案。</p><p></p><p>黄世飞指出，RAG 的优势不仅在于解决“幻觉”问题，还包括快速更新知识库和弥补专业领域数据不足的问题。</p><p></p><p>解决这些问题的过程实际上涉及数据的向量化。向量化本身是一个复杂的过程，需要将数据转化为向量形式。许多现代的数据仓库和数据库都支持向量化，而与 Elastic 合作的优势在于其数据生态系统支持直接通过内置的能力完成数据的向量化处理，用户无需导出数据到其他向量数据库，对于混合检索有天然的优势。</p><p></p><p>他以腾讯的微信读书项目为例，用户可以通过标记文字自动获得相似观点的推荐，过去这个功能是通过传统的文本检索方式实现，但有时候文本检索并不能获得最佳结果。向量检索则可以提供更好的推荐结果。同时，通过腾讯云ES的一站式 RAG 系统不仅能提高检索的准确性，还大幅降低了资源消耗——从原来的纯内存 400 台 64G 机器下降到 30 台。</p><p></p><p>除了探索应用，腾讯云也在积极参与 RAG 技术标准的落地。今年早些时候，腾讯云 ES 参与了中国信通院组织的检索增强生成（RAG）技术专项测试，并率先完成全部测试内容，展示了其在向量化处理和混合检索方面的能力。此外，作为核心参编企业之一，腾讯云参与了《检索增强生成（RAG）技术要求》标准的制定，与业内专家共同推动了这一技术标准的落地。</p><p></p><p>张君侠补充道，大模型有时候无法控制返回的答案，因为它太智能了。这时候，RAG 可以帮助他们构建自己的私有知识库，确保大模型生成的答案符合企业需求。当然，有人可能觉得这是对大模型的限制，但对于企业应用来说，建立一个安全、可靠的知识库是至关重要的。我们通过 RAG 技术，帮助客户将他们的知识库构建在 ELK 系统中，确保了数据安全和答案的准确性。</p><p></p><p>展望未来，黄世飞认为，不同场景对向量化的需求不同，因此作为技术服务商也需要支持更多样化的 embedding 技术，才能更好地应对多样化的场景需求。</p><p></p><h2>7 数据分析门槛降低？RAG 是否更适合专业人士</h2><p></p><p></p><p>过去，生成报表和进行复杂数据分析往往需要专业的技术能力。而如今，AI 与数据的结合让用户可以通过自然语言完成数据分析，大大降低了数据分析的门槛，尤其是对非技术背景的用户而言，这无疑是一种便利。以腾讯云的 ChatBI 为例，这款基于大模型构建的智能数据助手，通过对话即可生成图表和分析结论，简化了繁琐的数据处理步骤，让更多用户能够参与到数据驱动的决策过程中。</p><p></p><p>然而，尽管像 ChatBI 这样的 AI 工具让数据分析变得更简单，考虑到 RAG 技术的投入和成本因素，眼下似乎更适合专业人士使用。AI 大模型的普及是否能真正降低数据分析的门槛？</p><p></p><p>张君侠认为，RAG 技术的确已经讨论了一段时间，随着大模型的普及，RAG 的应用越来越广泛。尤其是在利用 AI 进行数据检索和生成时，RAG 提供了极大的便利。不过，高昂的专业服务费用仍是一大痛点，许多客户都提到这是他们面临的挑战之一。</p><p></p><p>如果大模型技术能够进一步普及，并且降低使用成本，接下来就会有更多非专业用户能更容易地使用这些技术，而不仅仅局限于专业人士。“Elastic 一直致力于为企业提供强大的数据分析解决方案。我们与腾讯的深入合作也表明，大模型技术的应用正在加速推进。”</p><p></p><h2>8 开源与大数据</h2><p></p><p></p><p>腾讯云长期以来在 Elasticsearch 项目中贡献了大量代码，最近 Elasticsearch 官方亦特别发文感谢腾讯云对开源社区的贡献，这也体现了开源社区在大数据发展中的重要作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61efcf59e13d1d2666da8130853aaa8c.webp" /></p><p></p><blockquote>“迄今为止，腾讯云在 Elasticsearch 项目中积极参与，提交了 204 个 PR（Pull Request），其中有 150 个已经成功合并到 Elasticsearch 的代码库，是 Elasticsearch 社区目前已知的第三方公司维度最高的贡献水平，这不仅彰显了腾讯云在技术上的强大实力，也充分展示了他们对开源社区的深厚承诺。”</blockquote><p></p><p></p><p>回顾大数据的 20 年历史，我们可以清晰地看到，开源与大数据的成长紧密交织，推动了彼此的进步。</p><p></p><p>黄世飞提到，早在 2000 年代初期，虽然大数据的概念已经提出，但技术实现还不成熟。当时传统的数仓分析在处理较小的数据集时还能勉强应对，但面对日益增长的互联网数据量，传统数仓显得力不从心。正是在这个阶段，雅虎等公司通过开源引领了大数据的革命，Hadoop 等项目的诞生开启了大数据时代。</p><p></p><p>从 Hadoop 的离线批处理到实时处理技术的兴起，Spark Streaming、Storm、Flink 等开源项目相继涌现，推动了大数据应用的快速迭代。开源社区的力量让这些技术得以迅速演进，企业因此能在短时间内应用最前沿的工具。黄世飞指出，虽然早期闭源开发可能会带来一些短期优势，但开源社区的协作力量不容小觑。开源项目的迭代速度往往能超越闭源系统。</p><p></p><p>近年来，腾讯云始终贯彻“开源开放”的理念。黄世飞表示：“坦白说，我们很多能力是从开源社区汲取的。当然，我们也做了很多改进，尤其是在适配云原生和增强方面，从服务过程中得到了客户的认可。”他进一步阐述道，“饮水思源，我们也希望回馈社区，这也是我们大数据体系的基本思路。既然我们从中获益，那么我们就应该把一些能力反馈给社区。”</p><p></p><p>除了 Elasticsearch，腾讯团队还在多个开源项目中积极贡献代码。黄世飞强调，腾讯云愿意继续坚持这条开源之路，和全球的开发者一起推动技术进步。</p><p></p><h2>9 数据分析市场在本土和海外有何不同</h2><p></p><p></p><p>在国内市场，企业在选择数据分析产品时，最关注的往往是成本和投资回报率。许多企业会优先考虑自建系统，如果外部产品的成本高于自建，他们可能会选择放弃购买外部产品。因此，确保产品的成本优势，是很多服务商设计产品的首要任务。</p><p></p><p>此外，国内企业客户对服务的即时性有着很高的要求。他们习惯于通过即时通讯工具获得服务支持，并期望遇到问题时能够迅速得到回应。相比之下，<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">海外</a>"客户则更习惯于通过提交工单或邮件的方式获得支持，也更习惯通过阅读详细的文档来解决问题，如果文档解决不了，才会进一步寻求支持，所以文档的完善、本地化和英文化也很重要。</p><p></p><p>同时，由于海外市场的企业代码能力很强，他们更倾向于通过 API 将外部服务集成到自建平台中，而不是依赖官方的控制台，因此产品模块要足够灵活，才可以通过 API 进行高效对接。</p><p></p><h2>10 大数据 +AI 时代，人才何去何从</h2><p></p><p></p><p>“大数据 + AI” 快速发展，企业面临着技术变革带来的挑战，员工的职业发展也因此充满了更多的不确定性和机遇。如何在大数据和 AI 时代下，抓住机会提升自我，是许多职场人关心的话题。</p><p></p><p>对此，张君侠认为，不安定的环境往往是学习新技能的最佳时机。他强调，在技术变革下，最重要的是敢于走出舒适区，主动学习那些你尚未掌握的技能。无论是 IT 技术还是其他领域，个人和公司的成长都发生在不安稳的状态下。因此，面对大数据和 AI 技术的不断进步，不要害怕新技术，反而要主动去掌握它们。并且不要等别人先尝试，要成为第一个行动的人，“to be the leader，not the follower。”</p><p></p><p>黄世飞从另一个角度探讨了大数据和 AI 技术对人才培养的实际影响。他指出，今天的学习门槛相比以往已经大大降低。过去可能需要花很多时间买书、看视频，而现在，AI 技术本身就能帮助我们更有效地获取知识。例如，大模型可以快速搜索文献、资料，极大地提升了学习效率。因此，学习条件的提升意味着我们更有机会掌握新的技能，关键在于是否愿意付出时间和精力。</p><p></p><p>除了学习，黄世飞还分享了他对职业发展的看法。他认为，艰难的环境反而是磨炼个人心智的好时机。“在困难时期，很多人会选择放弃，但如果你能坚持下来，等到形势好转时，你会发现机会更多。” 他建议在艰难时刻保持耐心，不要急躁，利用这段时间积累技能，等待机会的到来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JFNDl34Eol6UZSihy0mO</id>
            <title>从淘宝用户增长到生成式大模型：5 年，我的思考变了？</title>
            <link>https://www.infoq.cn/article/JFNDl34Eol6UZSihy0mO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JFNDl34Eol6UZSihy0mO</guid>
            <pubDate></pubDate>
            <updated>Sat, 14 Sep 2024 03:39:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 | 高嘉峻</p><p>编辑 | Kitty</p><p></p><p></p><blockquote>大模型技术的崛起正以前所未见的方式重塑软件开发领域。它凭借强大的语言生成、理解及创造能力，开启了人机交互的新纪元，是软件开发理念和实践的一次深刻变革。2024 年，QCon 全球软件开发大会以拥抱变化、全面进化为主题，关注技术前瞻性和实用性，提供有价值的行业洞察和参考，旨在帮助技术团队降低探索新技术的时间成本，更快地将创新技术和最佳实践应用到实际业务中。会议即将于 10 月 18-19 日开幕，访问<a href="https://qcon.infoq.cn/2024/shanghai/schedule"> QCon 官网</a>"了解更多详情。</blockquote><p></p><p></p><h3>题记</h3><p></p><p></p><p>2019 年，我在 QCon 北京站作了一个题为《<a href="https://qcon.infoq.cn/2019/beijing/presentation/1469">淘宝用户增长的 5+1 个策略</a>"》的分享。彼时 Growth Hacking 的概念进入中国已经过去了 4-5 年的时间，恰逢智能手机用户规模和时长到了第一个平台期，第一波流量红利见顶。那段时间我们在工程上做了诸多创新来应对环境变化给引流获客带来的新挑战，于是我在 QCon 分享了淘宝在用户增长业务上的技术策略和我个人的一些心得。后逢新冠疫情，经济周期，区域争端，生成式大模型，降本增效，被各种关键词轰炸，尤其在大模型的冲击之下，这个世界一定发生了改变。过去几个月我满怀好奇，花了很长时间去了解和学习大模型相关的知识，与许多不同行业和领域的朋友沟通讨论，结合过去我在工程领域积累的知识和经验，总结了以下几个观点与大家分享：</p><p></p><p>用户增长从“用户规模增长”过渡到“精细化用户运营”用户增长是中国另一个“超车”的领域大模型下放更多能力，用户增长更普惠AI 加到底加的是什么</p><p></p><p>5 年前分享的是经验，而今天分享的是想法，主观且有些并未验证，仅供参考。</p><p></p><h3>观点 1 ：用户增长从“用户规模增长”过渡到“精细化用户运营”</h3><p></p><p></p><p>过去相当长一段时间里，我们说“用户增长”其实是在说“用户规模”的增长，甚至于更狭隘的“引流获客”。“用户增长”被认为是一个以 DAU 甚至 MAU 为目标，以引流为主要手段的业务。2019 年以前国内移动互联网用户规模持续扩大，处在巨大的流量红利期，各家互联网产品的首要任务一定是争夺新用户，扩大用户规模。所以我们才会在各种渠道看到“ xxx 亿人都在用的 xxx ”这样的广告词。在这个阶段即使大家也偶有谈起“开源节流”、“长周期运营”等这类的概念，但身体还是非常诚实地都铺在了引流获客，做大注册用户规模上。进入 2020 年后，随着流量红利消失以及在大环境下行的背景下，“用户规模”这个目标被越来越多公司抛弃，转而关注更实质性的指标。也就是在这个阶段很多大厂财报中也不在出现 DAU/MAU 这样的指标了，取而代之的是购买用户（没过多久这个也消失了）、订单数、GMV（一直都在）等。</p><p></p><p>在这样的变化之下，我想我们需要从更本质的角度去解构“用户增长”这个概念。我认为“用户增长”不应该被作为一个业务板块的目标而解释为“用户规模增长”，应该是一种经营视角和模式升级，即：“以用户视角经营以获得业务增长”。 什么是“以用户视角经营”？如果我们以一个消费品牌为例，产品视角是我要卖多少货，看的是 GMV = 笔单价 x 订单数，要关注：有多少个 SKU，销量如何，货单价和利润率是多少，覆盖了哪些渠道，品牌影响力等等。而用户视角则是我要服务多少用户，看的是 GMV = 单用户价值 x 用户数，要关注：我有多少潜在 / 潜力 / 常规 / 忠诚用户，不同分层用户的分布和规模怎样，他们是什么画像，单用户价值等等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd1b6f7426287034630e0547396bc01e.png" /></p><p></p><p>那么用户视角又好在哪里，为什么更先进呢？我粗浅的解释是：企业赚的钱是用户带来的，一个消费品牌的 GMV 是每一个买家一笔一笔买出来的，一个互联网平台的广告收入是用户一下一下点出来的，所以关注用户更加直接，更接近本质。既然如此为什么要到 2020 年代才出现这个概念，之前大家都在干什么？我认为至少有两个方面因素：</p><p></p><p>一方面是供给侧越发成熟，增长空间向消费侧转移。 如果拿电商平台举例，“中国制造”的供应链经过几十年的发展，在这个时间点上已经非常成熟，甚至开始出现供给过剩的苗头。几家大的电商平台在那个时间点也基本都完成了商家和店铺的原始积累，开始进入治理阶段。整个环境不再是平台找不到卖家，而是卖家需要更多渠道，哪怕是一个新电商平台，只需要通过招商就能快速积累相当规模的卖家，不需要花精力去培养和教育新卖家。推而广之到更多的互联网平台、服务和产品，随着互联网，移动互联网，发展越发成熟，快速搭建一个 B 端形态（平台、服务或产品）成本越来越低，更多的精力要放在 C 端。</p><p></p><p>另一方面“大数据”技术发展给“用户视角”带来了可能性。 过去，数据采集、存储、处理、分析整条链路上的成本都非常高，以至于企业只能承担处理数十个产品 SKU、几百个销售渠道这种量级数据的能力。随着大数据科学的发展，移动互联网普及，从采集到分析整条链路的成本大大降低。只在少数领域（如：金融、高科技等）才有所涉及的数据分析和数据科学方法得以普及，过去公司里辅助少数高层决策的数据支持能力下放到业务一线，处理成百上千万用户数据的成本下降到可接受范围内了，自然用户视角也就应运而生。</p><p></p><p>引流获客和用户规模只是“用户视角经营”的一个角度，或者一个环节。只是在流量红利期这个环节最关键，我们以偏概全的将“引流获客”解释成“用户增长”。当然，概念归概念，到了应用阶段还是要识别重点，有所取舍，必须用一个具体的“环节”解释概念才能落地实施。对于一个新起步的互联网产品来说“规模”是原始积累，仍然是“用户增长”的主要目的，对于绝大部分跨过“原始积累”的产品用“精细化用户运营”来解释“用户增长”更合理，也就是提升单用户价值的重要程度赶上甚至超过用户规模。通俗的说“精细化”就是：通过数据分析和挖掘的方式把用户从各个维度进行细分，针对不同用户群体提供个性化服务，展示不同的产品，制定差异化运营策略，通过这种方式提升单一用户价值，最终获得更高的整体用户价值。这个过程中除了基本的数据采集、存储和分析技术外，还依赖用户模型、用户画像和圈选、A/B 实验、多策略投放、等等技术。随着 IT 技术进步和成本下降很多几年前看起来成本过高的方案在今天都变得可行且有效。而以上提到的各种技术，每一种都值得相当篇幅具体讨论，本文就暂不展开。</p><p></p><h3>观点 2 ：用户增长是中国另一个“超车”的领域</h3><p></p><p></p><p>过去一两个月，在与一些做海外投放业务的朋友交流的过程中，我发现相较于国内流量市场的模式和玩法这几年的飞速进化海外流量市场的发展是很有限的，主流的流量模式还是最经典的那些 CPX 投放，而国内早早就演化出了 RTA、OCPX、DPA 等等新模式。每一种新投放模式的诞生都意味着突破旧模式的瓶颈，把流量精准性和转化率提升一个新台阶。 一个新技术新理念从硅谷传到国内，这又是什么神秘的东方力量在短短几年时间就进化出这么多花样。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2f/2f7cbdd1648d71e1b51b0d0ea0b20757.png" /></p><p></p><p>其实也不难想象，主要还是过去几年时间里移动互联网带来了数据大爆炸。首先，超过 12 亿活跃用户的单一市场所生产的数据规模碾压任何一个海外市场，另外，数据伦理和数据合规发展根本不可能跟上数据“爆炸级”的发展速度，带来国内互联网行业在数据规模，数据维度，数据类型，数据流动性等等多个方面都比海外市场有巨大优势。流量红利期空前的获客需求，绝对大规模的数据量，数据合规政策不完善，几个因素叠加：旺盛且多样的需求作为动力，巨大规模的数据体量作为基础，合规尚不完善带来更大的发挥空间，“用户增长（不管是狭义某个具体环节，还是广义的整个模式）”这个托生于“大数据”的概念就必然疯长，在这个过程中不论是方法论，还是技术能力都会快速迭代，不停的进化出更先进的一代。</p><p></p><p>近几年随着国内数据伦理和数据合规越发成熟，数据满天飞的乱象极大程度改善。此外，为了针对过去野蛮发展时期诞生的各种新模式监管部门和企业共同制定了新标准，研发新技术来应对数据风险。最直观的是智能手机设备识别 ID 新标准的 OAID 和 CAID，还有 RTA 模式的人群加密与混淆，数据采集用户协议，等等。这些措施不仅仅推动国内数据伦理和数据合规越来越完善，更主要的是解决问题的同时让这些新模式和新方法得以更加规范、更可持续的运行下去。解决问题的同时，并没有开倒车，很好的保持了先进性。反观海外市场，尤其是北美，是理念和技术的发源地，起初的先进性毋庸置疑，但在以上提到的三个条件（需求、基础和环境）上都不如中国，反而被超车。</p><p></p><p>上述的是我看到在“引流获客”这个环节国内和海外的区别，如果回到“精细化用户运营”这个概念上，我认为国内仍然有孕育出更先进模式和技术的土壤。</p><p></p><p>从需求角度看，所谓的移动互联网下半场从“用户增长”的角度可以理解为：流量红利耗尽，如果完成了用户规模原始积累，要开始关注如何持续提升盈利能力，把规模变成利润；如果尚未完成用户规模原始积累，要探索更先进的获客方式，对冲掉红利耗尽引起的高成本。不管是“提效”还是“降本”无疑更加细分用户，更加个性化的方案，方案和人群匹配更精准是最直接有效的方式。过去的两三年时间里，我们已经看到许多不同规模、形态的互联网平台（服务、或产品）都通过“精细化用户运营”在“提效”和“降本”上取得结果。</p><p></p><p>从数据基础看，不论是数据规模还是可操作性都有成熟的积累。超过 12 亿活跃用户的市场规模，成熟的数据采集和存储方案，过去相当长一段时间内积累的丰富的数据分析方法论和数据应用能力，为精细化用户运营提供了良好基础。</p><p></p><p>从空间上看，经历了刚刚经历流量红利期的跑马圈地，在用户运营上还处在起步阶段，方案相对粗放。在用户识别和细分，以及方案个性化上都还有很大的发挥空间。另一方面，数据伦理和数据合规的政策与技术已经发展的相对完善，相关的风险基本都有相应的应对措施。更主要的是政策和技术并非一刀切的限制企业流转和使用数据，而是提供了合规且有效的方案，支持企业用好数据。这两方面都是做好“精细化用户运营”的空间条件所在。</p><p></p><p>相信在良好的环境和土壤上，建立“精细化用户运营”的观念，做好相关技术建设（数据采集、用户画像、人群圈选、多方案投放、A/B 实验）并形成有效联动，形成体系，我们的“用户增长”一定可以在下个阶段也取得先进性优势。</p><p></p><h3>观点 3 ：大模型下放更多能力，用户增长更普惠</h3><p></p><p></p><p>大模型的发展方兴未艾，我们不得不思考大模型会给“用户增长”带来哪些改变。回看过往技术发展给行业带来改变的历史往往是：新技术造成先进能力成本下降，过往需要消耗较高成本，仅能服务少数人的能力下放到更广泛的范围，更加普惠。数据技术发展造成采集 / 存储 / 分析成本下降，把数据分析和应用能力从服务少数领域下放到多数领域，从服务少数人下放到服务多数人，增长黑客应运而生。我认为大模型技术也将以同样的的方式改变一些领域，以这个范式去看用户增：哪些先进能力消耗成本高，效果好，但应用范围窄，这样的能力是否有机会被大模型跨越式的降低成本。</p><p></p><p>在国内互联网行业，往往大厂的用户增长方法论和技术能力领先行业平均水平。究其原因，大厂有能力在用增业务上投入的预算足够大：</p><p></p><p>场景丰富。 互联网大厂的产品形态丰富，也往往能形成矩阵，有协同效应。而且，在业务预算相对充足，试错空间大。所以在这样的土壤里更容易生长出先进的领域方法论，从业人员也能快速积累专业经验。人才密度高。 不论是聚集足够多行业专家，还是在足够大的业务规模和足够丰富的业务场景下去训练从业人员，都使得大厂用增团队在专业经验和先进理论方面都远远领先行业平均水准。也就是坊间流传的那句话：用增领域的专家都是用钱喂出来的。基础设施完备。 大厂在工程和数据基础能力上比较完备，不论是数据采集、存储和分析的设施，还是营销、实验、乃至数据可视化等工程方案，大厂往往都具备成熟的解决方案，与整个业务的产品矩阵也能很好的协同。</p><p></p><p>大模型能处理极其繁杂的输入信息，并依据输入差异，在一定规则下能规划不同的数据处理逻辑，并给出规则描述下相对最优的结果。放在用户增长领域里，大模型将有机会把诸多过往只有大厂才玩得起的策略下放。</p><p></p><p>机会挖掘： 机会挖掘通常可以包括机会人群和机会策略，具体工作往往属于数据科学领域，通过一系列数据科学技术，发现有机会给整体目标带来有效增长的空间和机会。在海量数据中发现某一画像的用户在特定指标下表现显著低于平均水平，且这部分用户规模能对整体目标带来显著影响，在特定指标下也有提升到一定水平的空间和机会。那么通过数据技术准确定位这个画像的用户并以人群方式与其他业务产品协同，采取针对性业务策略进行干预，这个人群就是机会人群。另一方面，在海量数据中找到产品漏斗中的显著短板或能够实现用户转化的关键方案，也就是利用数据技术发现产品中的问题或机会，用更具体的描述就是找到一个产品的“关键事件（Crystal Event）”和“魔法数字（Magic Number）”。</p><p></p><p>目前我们看到市场上已经出现功能非常强大的 ChatBI 类产品，主要事围绕“文生 SQL ”和“数据可视化”构造的 Agent，这两个能力是数据分析和挖掘的最基本能力。构造内化具体数据分析法或调度机器学习算法的模型，结合数据工程给智能体提供丰富且有效的输入数据，将有机会通过处理海量数据替代传统数据科学家的经验，给出有效的结论。把具备不同确定性能力的模型和功能整合成 Agent，机会挖掘这个门槛相对较高的能力将有效下放到更广泛的使用场景。</p><p></p><p>海量计划：“量变积累质变”在用户增长的诸多策略中屡试不爽，拿效果投放举例：在同一个流量渠道不同的活跃计划规模直接决定了成本优化的上限，国内头部投放业务在市场上的活跃计划数往往能达到数十万甚至百万级，而转化成本与数万级投放计划低 80% 以上。最直观的原因是在更大的投放计划规模下，投放匹配算法和优化手段有巨大的发挥空间，更精细和准确的匹配策略极大程度优化成本，提升效率。投放海量计划的势必消耗高成本，这些成本除了显而易见的生产素材和创建计划，随之而来的优化调整、数据分析、计划治理等工作的成本都随着投放计划规模的提升显著提升。这些随着计划规模提升带来成本提升往往是重复性工作提升和繁杂程度提升。举例来说，分析 1 万个计划的和分析 100 万个计划的投放效果的区别除了效果分析方法的次数相差 100 倍以外，还涉及到解读 1 万个初步结论和 100 万个初步结论的差异，可能出现额外的异常值发现、交叉分析、对比分析等。</p><p></p><p>新技术将有机会通过智能体替代上述这些依赖数据科学家和分析师的专业经验和人工的操作。生成式模型（文生文 &amp; 文生图）辅助生成海量素材，内化投放流程的模型生成投放计划，数据工程回收并以新的结构化要求建立符合 AI 要求的数仓，专注不同数据分析法模型组合而成的数据科学智能体，整合这些不同的功能模块，并通过工程手段有效协同这些功能，将形成一个具备处理海量计划的投放智能体。</p><p></p><p>盯盘优化：除了上文海量计划这种空间维度上规模量变上积累的效果质变，还有时间维度上操作频率量变积累效果质变。任意一个方案的转化效果总会经历效果爬坡期 - 最佳效果期 - 效果衰退期 - 长尾期，最终成为无效果的僵尸方案。往往我们需要在衰退期之前，对方案进行调整尽量延长最佳效果期，甚至寻找二次爬坡；或者用新方案替换旧方案，确保整体策略效果维持在较高水准。这就要求：第一，实时监控方案效果数据，即所谓的盯盘；第二，及时反应快速执行相应操作，或更新，或替换。</p><p></p><p>盯盘和反应两个要求除了对流数据处理和计划治理的工程能力有要求外，还依赖人工操作。当前技术水平完全满足工程能力要求，反而是人工成本是瓶颈，导致我们只能在一些关键时期才能采取这样的高频操作（如：双 11 高峰期，等）。而整合大模型和工程能力的智能体就能很好的解决这个问题，同时盯盘海量计划，并做出及时反应。</p><p></p><p>除了上述举例的三种能力，用户增长和衍生的各项业务中还存在大量能力下放的可能性，对中小企业是一个享受技术红利的机会，对于大厂来说则是一个突破能力瓶颈再上一个台阶，或者大幅降低成本的机会。</p><p></p><h3>观点 4 ：AI 加到底加的是什么</h3><p></p><p></p><p>从“用户增长”领域延展开去，在 AI 革命如火如荼的今天有哪些更抽象的方法论？我花了更长时间在 ToB 方向的工具类产品的思考上，因为我认为任何产品一定会存在“能力”与“易用性”之间的权衡和取舍，而 ToB 工具倾向“能力”更强，而 ToC 则倾向于“易用性”更高。所以，往往一项新技术更容易在 ToB 领域先被应用，随着技术逐步成熟会延伸到 ToC 领域，影响更多人，直到改变世界。显然当下 AI 大模型还处在成长期，应用在 ToB 工具上的可行性更高，基于此我片面的认为：</p><p></p><p>模型训练 vs 应用建设</p><p></p><p>模型训练（包括微调）是素质教育和能力训练，应用是职业培训和工作流程。在设计智能应用的过程里我们常常会取舍：一个功能到底是通过微调实现，还是通过工程（ RAG/Agent/Prompt ）方式实现？回答这个问题我们要先认知“模型”本身具备哪些优势，以及应用过程中的局限性会带来哪些劣势。讨论上述问题其实是讨论一项“知识”要通过什么方式被模型习得。</p><p></p><p>无论是基模训练还是微调都是干预模型本身使其习得知识。优势在于知识内化在模型内部，这部分“补充知识”能更好的与基础知识融合，应用过程中能模型能给出更协调、整体性更好的专业输出。内化“专业知识”的模型也更容易扩展，扩大使用范围除了支付额外算力外，几乎没有其他成本。</p><p></p><p>模型训练和应用两个过程是割裂的，模型训练是一个离线操作，而应用则是在线过程。离在线本身存在实时性的矛盾，而且模型训练（尤其是基模训练）的高成本会进一步放大这个矛盾，使得习得“补充知识”的实时性差，调整周期长。基于此，我认为稳定的知识（或能力），如：计算能力、编码能力、分析方法等可以考虑通过训练内化到模型中（当然不是一定，而是可以考虑）。而相对不稳定的（生命周期短会快速失效的，演化速度快需要高频更新的，个体差异大存在及时差异的，等）知识（或能力）就要采取能应对高频迭代的方案，如：RAG、Agent、提示工程等。通过更加成熟，成本更低的操作来应对这里的“不稳定”，如：爬虫 + 标记工程支持的信息库可以把不稳定的知识迭代转化为“治理”问题。</p><p></p><p>另外，基模训练和应用过程的模型训练（微调）也存在割裂，简单的说通过微调给基础模型带来的增量能力很难继承到一个更新版本的基础模型中，又或者微调增量与新版本基模的适应程度存在极大不确定性。更何况当前基模的版本更新往往会带来能力跨越式提升，很可能出现新版本基模在处理具体问题的能力上甚至优于微调过的旧版本模型。</p><p></p><p>所以，对于绝大多数 AI 应用来说要谨慎参与训练，更多思考是否能把新问题转化为老问题，使用更可控的方案解决。更可控的方案不仅意味成本可控，更重要的是我们能更好的控质迭代节奏，使能力迭代（演化）过程更可控。</p><p></p><p>交互升级背后的信息模型</p><p></p><p>最适应大模型，大语言模型，的交互方式一定是“对话式”交互，“对话式”交互在易用性和灵活性上带来的提升显而易见。但我认为集成大模型能力的新产品如果仅仅带来“灵活性”和“易用性”的提升，知识在降本上做到了有限的“量变”。想要在能力提升上制造“质变”，需要重构更底层的范式。从信息模型的角度去看产品，传统设计中往往有两个角色：使用者和工具。产品流程的信息流是单向的：从使用者流向工具，工具通过既定流程处理信息，把结果反馈给使用者。最典型的就是 SaaS 应用，通过把专业经验和先进方法论固化成工具流程（表单串联）来输出价值，使用过程中也是典型的使用者向工具的单向输出。</p><p></p><p>大模型的加入有机会让这个“使用者 - 工具”范式进化成“使用者 - AI - 工具”三种角色的范式。AI 之所以能被认为是一个独立角色，而非是对使用者或工具的改造，最重要的原因是 AI 成为能把独立信息引入系统的信息源，并改变系统中的信息流向。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/37d18506a680f06ef31e75335f6f3519.png" /></p><p></p><p>使用者和 AI 二者之间建立双向信息流，“使用者 -&gt; AI ”的信息流是对经典的“使用者 -&gt;工具”信息流的升级的一部分，是使用者为系统输入信息，最典型方式的就是提示词。“AI-&gt; 使用者”的信息流是创造性的，AI 为系统输入信息，通过对话的方式给使用者提供建设性灵感或建议。以典型的电商智能商品管理工具，AI 能通过阅读 VOA，读取热销商品，获取热点资讯等方式转化为 AI 的知识，在管理商品的交互中为用户提出更符合市场环境、更匹配用户需求的商品结构和内容建议。“AI-&gt; 工具”信息流是对“使用者 -&gt; 工具”升级的另一部分，一方面通过更直观的“对话”式交互提升执行效率，更重要的是编排更个性化的执行流程，创建“最恰当接”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4a/4a7f2f7fe79f41f6478f671e6fec69ee.png" /></p><p></p><p>由于系统的信息流从一条变成三条，所以为了应对这个升级就需要一个具备调度能力的中控模块，这是经典范式不具备的。另外，新范式最重要的变化是 AI 成为系统新的信息源，这也是新范式最大价值所在，所以 AI 信息源的丰富度和准确性运营是这个系统能力水准，甚至系统是否具备进化能力的关键所在。</p><p></p><p>AI 原生架构的技术红利</p><p></p><p>上文提到了各种应用模式和设计范式的区别，这些理论能够落地执行依赖技术架构的升级。还是以 SaaS 应用举例，我称传统 SaaS 的架构为“ Function-Based ”基于功能的架构，具体的就是对系统功能极致抽象为灵活性提供支持，基于专业经验和先进理论对极致抽象过的功能做整合，成为由表单和按钮串联的工具链。这个 SaaS 的价值体现在通过工具链体现出来的经验和理论，以及支撑这些经验和理论得以执行的功能抽象，是通过“最优解”发挥作用。新范式两个关键点是：动态编排流程和引入新信息源，“AI-Based”基于人工智能的架构就需要适应并放大这两点进化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/33bae8cc90b811e0d88ec5ba70323503.png" /></p><p></p><p>寻找“最恰当解”问题具体的是把抽象功能点动态编排为流程，通过“对话”式交互对使用者屏蔽动态编排大量流程可能带来的易用性灾难。基于此，对系统功能不仅仅要极致抽象，每一个抽象的功能点要有同构的接口协议，并且被明确标注每个功能的作用，为 AI 自主编排功能流程打好基础。无论是使用垂直领域的小模型，还是通过微调或其他工程手段构建的智能体，把抽象功能和协同作为知识传递给 AI，利用大模型推理出最恰当的组合方式。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4c/4cf1a9146e0e00b590eabde6b626ecb2.png" /></p><p></p><p>更重要的 AI 信息源丰富度和准确性问题可以被转化为数据工程和数据治理问题。建立多种信息获取方式多渠道持续更新获取有效信息，通过数据工程合理组织数据结构建立信息数据库，建立数据治理机制保证信息数据具丰富实时有效，通过 RAG、提示工程等方式建立模型读取数据、定位数据通道。做好 AI 引入新信息的储备和通道，通过持续提升 AI 引入信息的数量和质量提升系统能力。</p><p></p><p>上述的 AI-Based 架构仅仅是一个非常“形而上”的理念，甚至不能称其为理论。一个好的应用架构，除了需要一个先进理念作为起点，更重要的是要结合具体的系统目标和实际技术能力，进行合理的创新和取舍。这段内容或许不能说明什么是好的智能系统架构，但至少能描述什么是未能摆脱传统模式的架构。</p><p></p><p>本文总结的 4 个观点是我对过去一段时间「用户增长」的个人思考和沟通交流的简单总结，可能有一些局限性，但是我当下认知水平下对于 AI 的理解。未来，我希望能有更多机会去实践和验证这些观点，能结合具体领域具体目标把这些“形而上”的理念转化成具有实践经验支撑的理论总结。也希望未来能有更多机会跟不同领域和行业的朋友进行更多交流，吸取更多信息，逐步加深对 AI 应用的理解。</p><p></p><p>会议推荐</p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，QCon 还是太全面了。现在报名可以享受 9 折优惠，详情请联系票务经理 &nbsp;17310043226 咨询。</p><p><img src="https://static001.geekbang.org/wechat/images/c8/c87f0820b187f4ea98d5fe2bdce0f4c1.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/l3byryVCmzvNMuqTtJQE</id>
            <title>秀创新、聚人才、群思辨、观趋势，第五届深圳国际人工智能展（GAIE）圆满收官</title>
            <link>https://www.infoq.cn/article/l3byryVCmzvNMuqTtJQE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/l3byryVCmzvNMuqTtJQE</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 14:58:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月10日，为期3天的第五届深圳国际人工智能展（GAIE）在深圳会展中心7、8号馆完美收官。本届展会由深圳市工业和信息化局、深圳市发展和改革委员会、深圳市科技创新局、深圳市政务服务和数据管理局、深圳市中小企业服务局共同指导，深圳市人工智能行业协会、深圳市万博展览有限公司主办，“深i企”有约、深圳市工业展览馆、深圳市生物医药促进会、深圳市能效管理协会、香港设计师协会、深圳市质量强市促进会、深圳市健康产业发展促进会、深圳市文化产业园区协会、深圳市卓越绩效管理促进会支持。以“智创未来·价值链接”为主题，数百名全球知名人工智能领域的专家、学者、商业领袖、AI行业品牌企业代表积极参会，现场汇聚了国内外253家人工智能企业，超2600件参展展品，展示了人工智能领域的最新技术、产品和解决方案，包括人形机器人、行业大模型、AI数字人等多个领域。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd0d494b84dbfd8af0873b4e3546c278.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82592678b3e2226d4c803f625ac31891.png" /></p><p></p><p>本届展会聚焦算力、大模型、智慧教育、智慧医疗等领域，举办了2024全球湾区科技创新发展论坛暨第五届深圳国际人工智能展开幕式、智能机器人创新发展论坛、工业制造发展论坛暨深圳标准认证颁证大会、医工融合—医疗高质量发展大会、第二届智能算力发展论坛、大模型生态与发展论坛、百度AI引领行业人工智能应用创新发展论坛、AI赋能科技品牌出海国际论坛、供需对接会、新品发布会、AI人才直聘会等30余场活动，展产品、秀创新、聚人才、群思辨、观趋势，集创新发展、交流合作、智能体验于一体，让深圳AI创新的力量为之沸腾。通过3天的线下展览交流、活动对接，累计吸引现场观众人数达3.71万人次。</p><p></p><h2>大咖齐聚，跨界思辨AI未来之道</h2><p></p><p></p><p>当科技与创新的使命超越日常生活的范畴，当世界正处于快速变化与重构的浪潮中，我们如何展现AI的价值，如何激发创新以引领变革，以及如何在当下这个瞬息万变的时代中进行深刻的思考、不懈探索与有价值的积累？为此，本届展会同期论坛邀请了中国科学院院士、天津大学精仪学院教授、院名誉院长、激光与光电子研究所所长姚建铨，美国医学与生物工程院院士、英国皇家公共卫生学院院士、深圳理工大学计算机科学与控制工程院院长潘毅，院士、深圳大学微电子研究院院长、半导体制造研究院院长王序进等近80位多领域大咖汇聚“创新之城”，从不同视角研讨、辨析人工智能如何赋能千行百业，探讨新环境下的行业态势，让AI拓展向更多未知的领域，站在跨界的视角用人工智能推动各领域深度融合与创新发展，探索未来科技与社会的新边界。</p><p></p><p>每个论坛现场座无虚席，慕名而来的观众不仅有来自全国各地的人工智能领域从业者和企业家及院校学生，更有来自不同领域包括制造业、消费行业、基础设施，能源（电池），智能硬件、算法等领域的参展商、观展商和各自领域的从业人员。大家对人工智能技术的未来应用和行业发展趋势表现出了极大的兴趣和热情。通过聆听各位院士和行业领袖的见解，参会者不仅获得了宝贵的知识和信息，还激发了对人工智能在各行各业中应用的无限想象。论坛不仅为专业人士提供了一个交流思想、分享经验的平台，也为行业内外的参与者提供了一个了解人工智能最新动态和前沿技术的机会。整个会议不仅是一场知识的盛宴，更是一次思想的碰撞，为推动人工智能技术的发展和应用贡献了智慧和力量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/736c3e5fa93a1c8b06f7154ac7851c64.png" /></p><p></p><h2>“GAIE”大奖揭晓，激励创新提质增量</h2><p></p><p></p><p>“GAIE”大奖颁奖典礼由深圳国际人工智能展组委会主办，旨在表彰那些在人工智能领域中作出卓越贡献的企业和个人。自GAIE大奖设立以来，已有超过2000家企业参与评选，提交了逾10000项科技成果。本次颁奖以公平、公正、公开的原则，通过自主参与和专家评审团打分的方式，评选出了多个重要奖项。</p><p>在“重磅人物奖”环节，曾鹏、戚玉玲、林朗煕、吴邦毅、陈登坤等荣获“中国人工智能新锐人物”称号；王志国、段立新、文瑞、林必毅、刘洪杰、黄冠、陈宇光等被授予“中国人工智能卓越人物”；宋翔、刘轶、李涛、周剑明则被评为“中国人工智能领军人物”。这些获奖者以其敏锐的洞察力和不懈的努力，推动着人工智能产业的发展。</p><p></p><p>“最佳行业标杆应用”“最佳人工智能基础设施企业”“最具投资价值人工智能企业”“最具品牌影响力人工智能企业”“最佳人工智能服务平台”“最受欢迎人工智能产品”等奖项也相继揭晓，中国联合网络通信有限公司深圳市分公司、顺丰科技、中昊芯英（杭州）科技有限公司、深圳市易行网数字科技有限公司、用友网络科技股份有限公司等企业榜上有名。通过此次颁奖，不仅表彰了在人工智能领域做出杰出贡献的个人和企业，还激励了整个行业不断追求创新，推动技术进步和产业升级。未来，期待更多企业和个人加入这一行列，共同推动人工智能技术的创新与发展。</p><p></p><h2>深圳加快建设人工智能先锋城市</h2><p></p><p></p><p>本届展会开幕式上，深圳市工业和信息化局人工智能专班负责人赵冰冰详细解读了深圳市在人工智能领域的战略规划和政策措施。他表示，深圳率先出台全国首个人工智能产业促进条例，将人工智能纳入全市20大战略性新兴产业集群进行重点培育，高标准打造国家新一代人工智能创新发展试验区和国家人工智能创新应用先导区，全力打造人工智能先锋城市。接下来，将围绕人工智能领域“五个先锋”的发展目标，从“推进产业集聚发展、强化产业交流合作、推进产融合作、加大人才引育力度、推动包容审慎监管、强化组织保障”等方面，全面营造开放包容的人工智能发展环境。</p><p></p><h2>顺丰成果发布，物流行业的垂直领域大语言模型来了</h2><p></p><p></p><p>9月8日，顺丰科技在深圳国际人工智能展上发布了物流行业的垂直领域大语言模型——丰语，并展示了大模型在顺丰的市场营销、客服、收派、国际关务等业务板块的二十余个场景中的落地实践应用。发布会现场，中国科学院院士姚建铨，美国医学与生物工程院院士潘毅，深圳市邮政管理局副局长蓝志华，顺丰集团CIO、顺丰科技CEO耿艳坤等相关专家与领导等一起见证了这一里程碑的时刻。</p><p></p><p>随着通用大模型技术的快速发展，市场对于大模型对各行各业产生的影响与价值也充满了期待。通用大模型通识能力强，但缺少行业专业知识。如何将千行百业的专业知识数据融入大模型，让大模型“更专业”的同时，低成本、高可靠地解决行业的痛点问题，是让大模型技术产生价值的关键所在。顺丰打造的丰语大语言模型，实现了专业、可靠、成本的最佳平衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aadc06525f553b2ca0c04561da48235e.png" /></p><p></p><p></p><h2>观众互动热度不断，掀起展会活力高潮</h2><p></p><p></p><p>本届深圳国际人工智能展自开幕以来，以其前沿的科技展示、丰富的互动体验和创新的应用案例，极大地激发了观众的参与热情。无论是展会现场线上线下的互动体验，如打卡有礼、AI翻译、VR虚拟体验、专家深度对话，还是现场的互动装置、AI下棋机器人等，都紧跟时尚潮流，满足了各年龄段观众的观展和娱乐需求。这些活动不仅提升了展会在专业领域的影响力，更在公众生活中营造了浓厚的创新氛围，让人工智能技术更加贴近大众生活。观众乔女士在参与现场互动体验后表示：“这次在展会现场，我参观了20家展位，体验了AI技术在不同领域的应用，也深入了解了各企业对AI的独到见解，并见识到了涵盖日常生活、医疗、交通等多个领域的智慧产品。这不仅是一次知识的积累，更是一场未来科技的盛宴。期待在下一届展会中，能继续见证更多人工智能技术与产品，体验更加精彩的展览。”随着第五届深圳国际人工智能展的圆满落幕，我们期待在未来的展会中，继续为大家带来更多AI产品和惊喜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eaf88c00f21a15f9659c7977ddaaa836.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6OmLCw2rN7RXMmV6xJj2</id>
            <title>大模型+小模型，泛能网聚焦工业印染、公建暖通、园区荷光储3个场景发布能碳智控一体机</title>
            <link>https://www.infoq.cn/article/6OmLCw2rN7RXMmV6xJj2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6OmLCw2rN7RXMmV6xJj2</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 08:27:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>能耗难降、成本无度、碳排放超标……是很多工业企业发展过程中面临的普遍痛点。</p><p></p><p>以工业印染为例，据统计，整个华东地区5万台左右的染缸，每年会带来近4000万吨的标准煤能耗，而其中20%的能耗会被浪费，包括过程中的损耗、低效能的利用等等。这20%的能耗浪费，几乎相当于一个大型钢铁厂一年的用能成本水平。</p><p></p><p>针对这类问题，新奥能源基于自身35年的产业积累，以及16年的综合能源探索，打造了一套“泛能仿真系统”。</p><p></p><p>新奥能源副总裁、新奥泛能网总裁程路在9月12日的“泛能网2024能碳数智新品发布会”上介绍，有别于能源领域传统技术下仅使用机理模型+大数据分析的局部优化方式，以及互联网企业偏重用纯算法突破能源系统优化的观点，“泛能仿真系统”采取了“两者结合的道路”，它可以快速、便捷、简单地“自动生成”一整套解决方案，即配置优化；还可以动态识别资源环境变化、用户需求变化等，实时诊断问题，自动调整设备系统运行重回最优状态，保护着能源系统的安全高效运行，即运行优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e338b0329dee82a7f4799114f3a99f0.jpeg" /></p><p></p><p>泛能网是<a href="https://www.enn.cn/Intelligent.html">新奥集团</a>"旗下新奥数能打造的能碳产业智能平台，在其技术实践过程中，结合<a href="https://www.infoq.cn/article/PegbNEHXRRiChk51E98j">AI大模型</a>"的知识、数据和学习能力，极大优化了能源领域机理模型“长路径”的老方法，并以“选用训生”的方式打造了“大模型加小模型”的能碳产业大模型。即“选”开放模型，包括通用大模型和小模型算法，“用”积累的一系列能源数据，结合专家经验与知识抽取和集成，为“训”练和“生”成能碳产业大模型夯实基础。</p><p></p><p>“可以感受到，AI带来的技术变量，正促使着产业变革迎来黄金奇点。”程路将“泛能仿真系统”比喻为能源领域的智能驾驶系统，它可以便捷交互，可以自动判别环境或者路线的变化，自动寻找一种最优解和最佳的解决方案，并且交互也非常友好。</p><p></p><p>举例来说，该系统可以通过动态识别环境因素，如资源、用户需求的变化，诊断发现问题，进而让设备系统运行在最优状态，从而实现能源品质提升、成本降低、消耗减少、降低碳排水等目的。</p><p></p><p>在此基础上，泛能网首席产品官王尊还在会上发布了聚焦于“工业印染”、“公建暖通”、“园区荷光储”三个场景的智能产品——能碳智控一体机。</p><p></p><p>工业印染生产成本节约25%</p><p></p><p>以开篇提及的工业印染为例，该产品可针对印染过程中能耗难降、质量难控、销量难增的痛点，依靠感知、预测、分析、执行的完整智能模型，实现生产设备与供能设备的策略响应。</p><p></p><p>通过这样的印染智控技术，达成生产单耗最低，温度匹配最优，经济效益最优，高良品率，低碳排水平的综合目标，带来低碳绿色产品，碳足迹披露，低碳供应链的价值创造，提升一次成品率5%、管理效率70%；节约生产成本25%，碳排放降低450吨/染缸/年，实现经济效益与环境效益双赢。具体来说，对于一个工业印染企业每天可以减少错染布匹上万米，这些材料可供多制5000套T恤。</p><p></p><p>楼宇建筑供冷供热品质提升30%</p><p></p><p>公建暖通是国内建筑能耗最大的场景，对标国际能耗标准也仍有非常大的差距。以酒店为例，由于系统基本处于无人管理状态，没有根据人流高低峰值进行监测和优化，导致了大量的能耗浪费。对此，泛能网的能碳智控一体机拥有超过15大类感知能力，上百种设备感知能力，多模型组合寻优覆盖模型数量20+，并且其预测精准度超95%，最终可以实现供冷供热品质提升30%，供温达标率超 95%等明显效果，为商场、办公楼、酒店等公共建筑解决环境品质低、暖通能耗高、自控水平差的常见问题，带来高品质、更节能，强智能的优化效果。</p><p></p><p>会上，<a href="http://www.zhonganjt.cn/">众安商业集团</a>"运营总经理邵壮还带来了真实案例分享。他指出，冷暖温度对于商场的运营至关重要，而在能源成本中，暖通的耗能高达商场运营的15%。杭州萧山众安广场为提升顾客与商户的满意度，降低耗能成本，携手泛能网进行了“本地改造+AI智能系统”的线上线下一体化智慧暖通升级方案，现阶段已实现明显效果，特别是夏季高峰期综合能耗同比历史平均水平下降超过40%，全年节省能源成本50多万元。</p><p></p><p>将园区光伏发电量提升3%-5%</p><p></p><p>针对园区荷光储场景下光伏和储能在尖峰电价和低谷电价时段的配合、根据用户需求的策略优化调整等问题，往往存在的收益保障难和策略执行难的诸多挑战。对此，泛能网能碳智控一体机把秒级的数据响应能力和超 95% 预测能力，以及15分钟级别的滚动优化能力、分钟级的安全控制能力、多目标协同的优化能力等“打包集成”，塑造了荷光储智控的核心能力，让管理效率可以提升30%，综合运营收益提升10%。</p><p></p><p>程路指出，这套方案支持园区在不替换任何硬件设备的基础上实现智能化改造，进而将光伏发电量提升3%-5%，这对于一个小型园区的某一客户额外而言相当于每年增加了25万的收益，减少碳排放量相当于中了1950棵树。</p><p></p><p>值得一提的是，正如自动驾驶系统不需要人操作那样，泛能网在云端提供的能碳智能伙伴，还以虚拟“小助手”的形象，可以和智控一体机进行交互。在日常工作中，还可通过泛能网手机 APP 掌握全局。据王尊介绍，能碳智控一体机适用于不同的行业，无论是中小型项目还是大型场景，都有灵活的合作及部署方式。</p><p></p><p>程路强调，能碳智控一体机是新奥泛能网在“双碳”战略大背景下，洞察到企业在存量博弈的时期，普遍存在价格竞争、产品同质化、用能排放约束，创新不足，增速放缓等痛点推出的新一代数智新品。其希望凭借能源系统一小步的提质增效，帮助更多行业、更多企业走出“内卷”， 助推新质生产力的发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/23uAXwB7q7dTYB9iwKTR</id>
            <title>云上 AI 时代，逆势涨薪攻略</title>
            <link>https://www.infoq.cn/article/23uAXwB7q7dTYB9iwKTR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/23uAXwB7q7dTYB9iwKTR</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 08:03:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在数字化转型的浪潮中，云计算和人工智能（AI）已成为推动技术创新和业务增长的关键动力。不仅技术人员，包括非技术人员、业务人员乃至决策者都面临着提升数字技能的迫切需求。</p><p></p><p>据 InfoQ 研究中心的《中国生成式 AI 开发者洞察 2024》报告显示，企业对于掌握云计算和 AI 技能的人才需求日益增长。这种需求的增长，不仅体现在对传统技术岗位的需求上，更扩展到了业务分析、项目管理、决策支持等各个领域。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c666223e4c4d91cb6e41b3e03c27365.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c48c38450cf43012d264fc22f3df9a4b.webp" /></p><p></p><p>在这样的大环境下，拥有专业认证成为了区分技术人才的重要标准。它不仅能够证明个人的专业技能和知识水平，更是职场晋升和薪资增长的有力助推器。特别是在云计算和 AI 领域，专业认证几乎成为了高薪就业的敲门砖。</p><p></p><h2>培训与认证体系众多，如何选择？</h2><p></p><p></p><p>当前，市场上充斥着各种云和 AI 相关的培训与认证项目，但并非所有认证都能为个人职业发展带来实质性的帮助。</p><p></p><p>首先，市场上培训与认证项目质量参差不齐，尤其是 AI 相关的课程，一些培训课程内容过时，无法跟上技术的发展；其次，并非所有认证都具备行业广泛认可的权威性，一些认证缺乏行业影响力，对职业发展帮助有限；第三，某些认证项目费用昂贵，但投资回报率并不明确，会给学习者带来经济负担。</p><p></p><p>因此，选择一个权威、前沿且被广泛认可的认证项目，对于职场人士来说至关重要。一个优秀的认证项目应该具备更新及时的课程内容、能够提供整个行业认可的证书、并且对个人的工作能力及职业发展有所助力。</p><p></p><p>如何选择一个靠谱的培训与认证体系呢？大家可以从以下四点进行评估：</p><p></p><p>课程内容的及时更新：紧跟技术发展趋势，选择推出时间更新且定期更新课程内容的培训与认证体系。行业认可度：优先选择一些头部厂商、权威机构推出的认证，这些认证具有更广泛的认可度，能够增加就业竞争力。国际权威性：优先选择具有国际权威性的认证，这样的认证不仅在国内有影响力，也有助于在全球范围内提升个人职业资质。职业发展支持：优秀的认证体系应提供职业发展支持，如就业指导、职业规划咨询和行业网络资源等。</p><p></p><h2>专业认证：职场竞争“加薪”项</h2><p></p><p></p><p>面对众多的培训和认证选择，InfoQ 基于深入的市场调研和严格的评估标准，特别推荐两款备受行业认可的认证项目：亚马逊云科技的【云从业者认证】和【AI 从业者认证】。亚马逊云科技的认证项目一直是业界公认的黄金标准，不仅仅在国内，在国际上都有很高的权威性和认可度。</p><p></p><p>以上两款认证不仅涵盖了云计算和 AI 领域的最新技术，还提供了实践性强的培训课程，帮助大家提升技能，增强市场竞争力。</p><p></p><p>根据市场调研数据，拥有亚马逊云科技认证的专业人士在职场上更具竞争力，他们的薪资水平普遍高于非认证人员。具体数据显示，认证持有者的平均薪资提升 74%，而且在求职市场上的竞争力提升超过 83%。此外，92% 的雇主表示他们更倾向于雇佣持有专业认证的候选人，这进一步证明了亚马逊云科技认证在职场中的高含金量。</p><p></p><p>当然，考取亚马逊云科技的云从业者认证和 AI 从业者认证，不仅仅是为了拿到一纸证书，更重要的是通过这一过程真正提升个人的技术水平和解决问题的能力，最终，实现升职和涨薪的目标，开启更广阔的职业发展空间。</p><p></p><h2>云端 X AI 高薪人才培养战略</h2><p></p><p></p><p>如此高含金量且权威的认证，是不是很难考取呢？缺乏自制力，难以坚持学习怎么办？学习时间有限，通过不了考试怎么办？</p><p></p><p>别担心，InfoQ 为大家定制了【云端 X AI 高薪人才培养战略】。该战略针对【云从业者认证】和【AI 从业者认证】分别提供系统化的学习课程，并提供【培训 - 认证 - 提薪】一站式赋能服务。跟随课程的节奏进行学习，考试通关率将大幅提升。AI 从业者认证特别开放考试冲刺班，班次一 9 月 20 日截止报名，班次二 10 月 16 日截止报名 ; 考试冲刺班各限额 300 名，名额不多，先到先得！</p><p></p><p>不仅如此，参与该战略还可以享受 50% 折扣优惠报考及一次免费补考机会，相当于获得了一块认证的“免死金牌”（云从业认证考试 9 月 30 日前完成首次考试可享此权益；AI 认证考试 11 月 15 日前完成首次考试可享此权益）。</p><p></p><p>活动细节及参与方式详见海报，扫描二维码即刻加入，开启你的云端 X AI高薪之旅！</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/208080ff5067d58369d5eb0e4e6b9572.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NbHkv9fvR8lemlARRBzL</id>
            <title>新一代多维表格，让一线员工搭建系统不求人</title>
            <link>https://www.infoq.cn/article/NbHkv9fvR8lemlARRBzL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NbHkv9fvR8lemlARRBzL</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月4日，飞书发布了全新多维表格、低代码平台等系列业务工具产品，并推出了面向出海企业的跨境合规解决方案。这些产品将继续为中国企业提供实质的降本增效帮助，促进企业以更低的数字化成本解决实际业务问题。</p><p></p><p>自飞书2020年发布多维表格开始，这款产品逐渐为大众使用与熟知，并成为了一个全新品类。据飞书透露，飞书多维表格的月活数已经达到 600 万，仅过去一年，飞书用户便创建了近 4000 万个多维表格，在这些多维表格上，流转着超过 100 亿条记录。在泡泡玛特、元气森林、蔚来汽车等知名公司，飞书多维表格均以极小成本解决了重要业务需求，可贵的是，这些业务系统均由不懂技术的一线员工搭建。</p><p></p><p>如今，飞书正式推出飞书多维表格数据库，这让飞书多维表格的单表容量突破了100万行，仪表盘也可统计1000万行数据，均为全球同类产品中最高。在全新的强大性能下，即使在飞书多维表格中计算10 万行、100 列公式这样复杂的数据，仍然能在5 秒内便获取业务结果。飞书多维表格还发布了全新一代仪表盘，通过飞书多维表格数据库的计算能力，由多维表格行列数据生成的仪表盘，将不再是简简单单计算、汇总、呈现数据，增加了大量计算、图表组件编组、统计分析等功能，界面也可对标全球顶尖 BI 系统。</p><p></p><p>以下为飞书多维表格负责人施凯文的分享全文，经InfoQ整理。</p><p></p><h1>开场</h1><p></p><p>大家好我是施凯文，飞书多维表格负责人。</p><p></p><p>相信大家一定对谢欣刚刚所描述的，多维表格的种种使用场景和承载的解决方案印象深刻。</p><p>在多维表格这个品类里，核心功能和技术特别特别的多，今天，我就为大家介绍一下其中最至关重要的几个模块，也会在各个模块中为大家揭晓今天的重大更新。</p><p><img src="https://static001.infoq.cn/resource/image/38/e0/3802c516ecc66e70333a634427e3ffe0.png" /></p><p></p><h4>多维表格数据库</h4><p></p><p></p><h5>行数扩容</h5><p></p><p>第一个要介绍的是，多维表格的基础设施：存储和计算。</p><p></p><p>说到存储和计算，我们首先要提及的就是行数，行数的多少直接决定了多维表格能承载业务规模的大小。</p><p>可在多维表格这个品类中，增加行数可没那么容易。</p><p></p><p>多维表格的行除了承载了我们熟悉的文本、数字、图片等这些内容外，同时还承载了动态的计算值，例如公式、引用，甚至是跨数据表的大规模计算引用。</p><p></p><p>还不止这些，每一个这样的行，还要允许被业务人员设定的各式各样的流程、自动化，所消费和订阅。</p><p>只有满足这些能力，才能真正让数据动起来、流转起来。</p><p></p><p>我们管拥有这样能力的行叫 "热行"。</p><p><img src="https://static001.geekbang.org/infoq/90/907b30b35a59f41ed9dfccec97e1ffe9.png" /></p><p>下面让我们来看一下目前在全球范围内支持热行的同类产品的能力对比图：</p><p></p><p>从这张图我们不难发现，即便是在此时此刻，飞书多维表格也比行业领先不少。</p><p></p><p>无论是单数据表可支持的行数、还是公式计算的灵活度。</p><p></p><p>然而，对我们而言，这远远不够。</p><p></p><p>随着多维表格在企业和组织中应用的日益广泛，使用场景的愈发复杂，在这种高速增长的背景下，我们正面临着两个关键的挑战：容量不足、计算速度越来越慢。</p><p></p><p>为了彻底解决这些问题。我们投入了近百名来自飞书团队和字节数据库团队的顶尖工程师，耗费数万人天，正式打造了全新的基础设施 —— 多维表格数据库。</p><p></p><p>搭载了多维表格数据库的全新一代多维表格单数据表能够支持 100 万热行。</p><p></p><p>这比旧版多维表格单数据表的容量提升了 20 倍。</p><p></p><p>并且遥遥领先于全球同类产品。</p><p><img src="https://static001.geekbang.org/infoq/89/896c1247f1dc2eaa06d433858103e10b.png" /></p><p>今天我们也可以非常自信和自豪的说，多维表格这个品类，将进入单表百万行时代，这也将成为多维表格这个品类的全新标准。</p><p></p><h5>计算提速</h5><p></p><p></p><p>容量介绍完了，现在让我们来聊一下：计算速度。</p><p></p><p>在过去，数据量只有几百上千行时，计算的公式多一些用户可能感受并不明显，但，当数据量增加到几千上万行时，速度就明显变慢起来了。</p><p></p><p>如今在多维表格数据库的超大容量加持下，动辄几万、十几万行的数据，在这样海量的数据下计算速度又会是什么样的呢？</p><p></p><p>为了让大家对上万行数据计算的场景和体验能有一个明显的感受，我们准备了一个同类产品，也包括旧版多维表格的效果对比视频，大家可以先感受一下。</p><p></p><p>这是一个还原客户真实应用场景的例子，客户是一家零售领域的知名公司，正在用一张多维表格作为销售数据管理系统，用来管理全国数十家大型消费综合体的经营数据。</p><p></p><p>在这个例子的真实场景中，实际的数据量有 4 万多行，但是呢，因为其他同类产品目前对热行的支持基本只才刚刚达到了 2 万行，所以我们采用了 2 万行的数据，来给大家演示对比效果。</p><p></p><p></p><p>这张两万行的数据表，仅仅包含了几十列的公式，在飞书多维表格的诸多应用场景中，还算不上是特别复杂的，但我们已经看到了，还是非常的慢。</p><p></p><p></p><p></p><p>现在让我们再看一下，搭载了多维表格数据库的全新多维表格来执行相同的数据量的计算，是什么表现。</p><p>相信大家已经有非常明显的感受了。</p><p></p><p>搭载了多维表格数据库的全新一代多维表格计算速度有指数级的提升、最高可达 100 倍。</p><p></p><h5>Time Machine</h5><p></p><p></p><p>容量超级扩容、计算速度百倍提升、除了这些强大的能力以外，今天，我们还将向大家介绍一个多维表格数据库的全新，也是我们认为必不可少的一个能力。</p><p></p><p>Time Machine，一个可以实现精确到任意操作者、任意操作行为的溯源和回滚的能力，特别是在多人协作场景下，Time Machine 可以实现对全体协作成员的、全部操作行为的，可追溯、可回滚。</p><p></p><p>并且 Time Machine 还接入了电子取证和 DLP 系统，真正做到让管理者安心、放心。</p><p></p><p>这么多强大的能力都源于我们全新打造的基础设施——多维表格数据库。</p><p></p><p>它整合了多种先进技术，包括内存表格视图引擎、Rust 公式迅算引擎、智能算力调度引擎、 MPP 大规模并行处理系统以及 Time Machine 多版本存储引擎。</p><p></p><p>正是这些尖端技术的融合，使得全新一代多维表格能够提供如此卓越的性能和表现。</p><p></p><p>今天，多维表格已经有了 600 万的月活用户，过去一年时间累计创建了超过 4000 万张多维表格，100 亿行数据。此刻，我们不但有信心，还有依据，在这里宣布，新一代多维表格的全新标配能力：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca206ca9ad6698f222bb5ad7a1225071.png" /></p><p></p><p>单表数据容量能够支持 100 万热行、计算速度百倍提升、大数据行数下视图秒级加载、全维度数据可追溯、可回滚。</p><p></p><p>以上这些全新的强大能力，都将于今年 10 月份起陆续发布。</p><p></p><h4>全新仪表盘</h4><p></p><p></p><p>好，现在，我来为大家介绍多维表格的第二项重要功能 —— 仪表盘。</p><p></p><p>2022 年，飞书多维表格首次引入仪表盘功能。它是一款易用性极高的可视化和分析工具。</p><p></p><p>它为业务人员提供了灵活配置图表、自定义布局以及实时数据分析的能力。自推出以来，就深受广大用户的喜爱。</p><p></p><p>然而，随着用户需求的不断演进和业务场景的日益丰富，我们也收到了大量反馈，呼吁增强仪表盘的功能，特别是在图表的多样性上和数据分析的深度方面。</p><p></p><p>为了响应这些需求并保持多维表格的领导地位，我们郑重推全新一代多维表格仪表盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32ef5357928972ca32fda04050506c25.png" /></p><p></p><p>它将全面支持市面上所有主流图表，接入数量，达到 50 个之多。</p><p></p><p>这也将使飞书多维表格仪表盘成为一个功能完备的数据可视化平台，为用户提供全方位的图表选择，以满足各种复杂的数据分析和展示需求。</p><p></p><p>其次，全新一代多维表格仪表盘还将对整个交互设计进行重大升级。</p><p></p><p>视觉效果，全面对齐世界顶尖 BI 产品。</p><p></p><p>同时，我们还为仪表盘带来了，图表编组能力、样式控制能力。</p><p></p><p>无论是数据的可读性、还是界面的自定义也都将迎来重大提升。</p><p></p><p>我们还大幅增强了仪表盘的数据分析能力。引入了数据透视能力</p><p></p><p>单图表支持多数据源联合分析能力。</p><p></p><p>并且依托于全新的多维表格数据库，我们将多维表格仪表盘的性能提升到了前所未有的高度。</p><p></p><p>全新一代多维表格仪表盘中的每一个图表都可对高达 1000 万行来自最多 200 张数据表的数据，进行实时的统计分析。</p><p></p><p>这就是全新多维表格仪表盘。</p><p><img src="https://static001.geekbang.org/infoq/79/795190cb9bbd884bc919353c54693166.png" /></p><p></p><h4>高级权限</h4><p></p><p>接下来我为大家介绍的是多维表格的第三项重要功能：高级权限。</p><p></p><p>随着多维表格应用场景的日益丰富，在同一张多维表格上协作的人数也正变的越来越多，协作的人多了，对应的角色也就变多了，需要管理的内容也变的多样和细致。</p><p></p><p>为应对这一挑战，我们郑重推出支持颗粒化管理的全新高级权限。</p><p></p><p>首先，全新一代多维表格高级权限，首次将权限控制的范围扩展至功能模块。</p><p></p><p>使用者能进一步精确掌控如仪表盘、自动化、视图等高价值功能模块的使用、创建和分享。</p><p><img src="https://static001.geekbang.org/infoq/65/6550a5f25bea7d36bc59182b565bb3af.png" /></p><p>同时使用者还能够精确控制模块内部的具体元素，如仪表盘中特定组件的可操作性，自动化流程中特定步骤的配置权限。</p><p></p><p>我们还进一步强化了条件权限功能，将权限管理提升至动态匹配的全新高度。使权限分配能够根据特定条件自动调整。</p><p></p><p>比如，管理者可以设置在某个关键指标达到预设值时才开放特定数据的访问权限，或在指定日期范围内授予用户临时权限。</p><p></p><p>最后我们实现了高级权限的突破性能力 —— 千人千面的仪表盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1497c46a344e5ddd33cd9174e86cc06.png" /></p><p>它可以根据每位用户自身对应的权限，自动计算并展现用户有权访问的数据。</p><p></p><p>让每个用户都只看到符合自身权限的仪表盘数据。</p><p></p><p>同时，依托于多维表格数据库，我们在全球范围内，首次，将千人千面的仪表盘管控，扩展到 1000 万行数据规模。</p><p><img src="https://static001.geekbang.org/infoq/c7/c76c33de5af743f4be987f25903860eb.png" /></p><p>这些先进特性的整合，为企业提供了一个安全、高效且适应性极强的数据管理解决方案，有效应对企业在数据治理和协作效率方面的多重挑战。</p><p></p><h4>工作流</h4><p></p><p>好，接下来让我为大家介绍多维表格的第四项重要功能，也是多维表格核心功能的全新成员：工作流。</p><p></p><p>随着多维表格的使用场景变得越来越多，流程的定制化程度也变得越来越复杂。</p><p></p><p>如何让自动化支持可视化的流程图，从而让用户可以全局掌控，流程的结构、节点，和逻辑。</p><p></p><p>如何让自动化支持更便捷的条件节点、循环处理等高级能力？</p><p></p><p>为了解决这些问题，我们推出了全新的功能模块，工作流。</p><p><img src="https://static001.geekbang.org/infoq/dd/dd3c709adb4e4fa5b3aacb7047dd0a1d.png" /></p><p>工作流，更侧重于流程的直观呈现和灵活调整，支持构建多个条件分支、嵌套逻辑等复杂场景。</p><p></p><p>全新的编辑器，全流程的管控、对复杂逻辑的支持、丰富开放的节点能力，这就是全新的多维表格工作流。</p><p></p><h4>多维表格 AI</h4><p></p><p>好，下面到了今天多维表格新功能发布的最后一个环节，多维表格 AI。</p><p><img src="https://static001.geekbang.org/infoq/41/4141f88910a83cc04cacda7efc5f9911.png" /></p><p>在介绍之前，先让我花一点点时间，为大家讲一下，我们关于多维表格 AI 的几点思考。</p><p></p><p>首先，我们一直在探索如何借助 AI 来帮助用户快速、深入地掌握多维表格的高价值功能。</p><p></p><p>其次，我们还在探索，如何更好的集成 AI，从而扩展多维表格解决问题的范围。</p><p></p><p>今天我们要发布的 AI 能力，就来自这样的思考。</p><p></p><h5>仪表盘智能分析</h5><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1da1f79153065bb1a9ba3c5aacb77bb.png" /></p><p>首先，我将为大家介绍一系列用来帮助用户更好的使用多维表格的 AI 工具。</p><p></p><p>我们为仪表盘内的每个图表都增加了智能分析功能。</p><p></p><p>当你打开仪表盘，遇到一些复杂的图表时，无需花费时间去了解细节，只需点一下 智能分析 按钮，便可在几秒内获得数据背后的洞察。</p><p></p><h5>AI 生成公式</h5><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3cda396caa7a85b44683e29c5d621fc2.png" /></p><p></p><p>我们还在公式编辑器中，提供了 AI 生成公式的能力。</p><p></p><p>只需描述需求，就可以生成一段高水准的公式。</p><p></p><h5>自动化 AI 节点</h5><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8acc5f0019f87e8e91e92f40f7e2193b.png" /></p><p>我们还在自动化和工作流中增加了 AI 内容生成节点。</p><p></p><p>比如，当今天的日期到达某个客户的生日的时候，用 AI 生成节点基于这个客户的身份、行业、年纪、性格等等信息，快速生成一份定制化的生日祝福。</p><p></p><p>并通过邮件、或其他社交平台发送给客户。</p><p></p><h5>AI 智能问答</h5><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f461f8057cd1f632195bee64880d348.png" /></p><p>我们还为多维表格提供了智能问答的功能，允许用户对整个多维表格内的所有数据进行询问，帮助使用者更好地理解和洞察多维表格内的数据。</p><p></p><p>例如，在一张商品销售记录的多维表格中，</p><p></p><p>用户可以直接询问：“最近销量比较好的商品是什么”；</p><p></p><p>我们将以上一系列实用的 AI 功能无缝的集成到多维表格的各个功能界面中，从而帮助企业潜移默化的提升整个团队的综合能力。</p><p></p><p>除了以上这些 AI 功能外，还有什么 AI 能力是能够扩展多维表格解决问题的范围的？</p><p></p><p>并且是在多维表格内才能发挥最大效用的？</p><p></p><h5>字段捷径</h5><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f97926ad273e0299941b4726e35072f8.png" /></p><p>今天，我们重磅推出，全新多维表格 AI 能力字段捷径。</p><p></p><p>我们将 AI 功能封装进简单的字段配置环节里，使用户无需了解复杂的底层细节，只需简单勾选即可完成参数设置和结果输出；</p><p></p><p>同时充分利用多维表格的批量数据处理优势，将 AI 能力整合到这一功能中，实现智能化的批量生成，从而大幅提升工作效率。</p><p></p><p>现在让我们快速看几个例子：</p><p></p><p>这是一个利用 AI 字段捷径对大量用户评论进行自动标记和分类的例子。</p><p></p><p>通过在字段面板上选择 AI 智能分类，并勾选需要处理的内容。就能够对所有客户的评论，进行智能分类。</p><p>这是一个合同证据提取的 AI 字段捷径。用户只需选择对应的合同附件字段，便能自动从 PDF 文件中快速提取关键证据。</p><p></p><p>还有更多强大的 AI 字段捷径能力，例如，AI 全网搜索、视频生成、名片、文字识别等等。</p><p></p><p>我们希望将更多的，更丰富的 AI 能力都带入到多维表格的字段捷径中来。</p><p></p><p>今天我们首次将多维表格的字段开放，并打造了字段捷径中心。</p><p><img src="https://static001.geekbang.org/infoq/90/90ff2c86381e97d0783bfd7f0439478b.png" /></p><p>允许和鼓励，优质的创业公司、开发者将他们眼中的有趣的、强大的 AI 能力发布到多维表格的字段捷径中心来。</p><p></p><p>目前，多维表格字段捷径中心已吸引了数十家优质创业公司入驻并发布了他们的 AI 字段捷径。其中包括多家知名企业：</p><p></p><p>月之暗面，发布了 PDF 阅读助手字段捷径，零一万物发布了智能巡检字段捷径，智谱 AI 发布了视频生成字段捷径。此外，大饼 AI、Autodocs 、Nolibox 等等优秀创业公司也都贡献了各自的特色字段捷径。</p><p></p><p>这些高质量的 AI 字段捷径不仅丰富了多维表格的功能，也为用户提供了多样化的、智能化的解决方案。</p><p>我们也相信，一款强大的工具平台一定离不开生态，只有更开放的引入多种多样的生态能力才能让我们的客户更好的、更快的接触到更多更先进的能力，从而打造出具有无限可能的业务应用和系统。</p><p><img src="https://static001.geekbang.org/infoq/b4/b4db7081aa4ebc75096b1107eeb5c29f.png" /></p><p></p><p></p><h4>总结</h4><p></p><p><img src="https://static001.geekbang.org/infoq/76/76fed9b7d5d9c7c95281f24cdb6924fb.png" /></p><p>现在让我们来回顾一下，今天多维表格的所有重大更新。</p><p></p><p>全新的基础设施多维表格数据库彻底扫清了业务人员在采用多维表格搭建各式各样的业务系统过程中的各种阻力。</p><p></p><p>让业务人员采用多维表格搭建系统，更安心、放心。</p><p></p><p>全新的多维表格仪表盘，界面对标全球顶尖 BI 产品，相对于过去具备更高级的数据分析能力，性能迎来了指数级提升。</p><p></p><p>全新的多维表格高级权限，提供超颗粒化的权限管理能力，为企业提供了一个安全、高效且适应性极强的数据管理环境。</p><p></p><p>全新多维表格工作流，提供了高级的逻辑处理节点，和可视化编排能力，为企业提供更多样化的业务流程自定义能力。</p><p></p><p>多维表格全面集成 AI，将一系列强大的 AI 能力融入系统搭建的各个环节。</p><p></p><p>4 年前，我们在中国首次定义了多维表格这个品类，将其定位为一款帮助业务人员自主自助搭建业务系统的平台。</p><p><img src="https://static001.geekbang.org/infoq/53/53334012a8141efb346758caabf7b80b.png" /></p><p>作为该领域的开创者和领导者，我们在这段时间里取得了不俗的成绩。</p><p></p><p>今天，我们发布了一系列强大的能力更新，重新定义多维表格。</p><p></p><p>下面让我们看一段全新多维表格的介绍视频。</p><p></p><p></p><p></p><p>全新一代多维表格，更强大、更智能、更敏捷，让每个人都能轻松搭建自己的智能业务系统。今天我们向各行各业的先进企业发出邀请，帮助 10,000 个企业率先搭建下一代智能业务系统。</p><p></p><p>以上就是多维表格今天发布的全部内容。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VI9FSs7mDIE5zExDPYf4</id>
            <title>100B的「跨级」跃升！元象发布最大MoE开源大模型，「高性能全家桶」系列全部免费</title>
            <link>https://www.infoq.cn/article/VI9FSs7mDIE5zExDPYf4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VI9FSs7mDIE5zExDPYf4</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 06:42:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月13日，元象XVERSE发布中国最大MoE开源模型：XVERSE-MoE-A36B。该模型总参数255B，激活参数36B，能达到100B模型的性能「跨级」跃升，同时训练时间减少30%，推理性能提升100%，使每token成本大幅下降。</p><p></p><p>并且，元象「高性能全家桶」系列全部开源，无条件免费商用，海量中小企业、研究者和开发者能按需选择。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/66/6694db23e2fa9c98230ae723c3c4bbc8.png" /></p><p></p><p></p><p>MoE（Mixture of Experts）是业界前沿的混合专家模型架构 ，将多个细分领域的专家模型组合成一个超级模型，打破了传统扩展定律（Scaling Law）的局限，可在扩大模型规模时，不显著增加训练和推理的计算成本，并保持模型性能最大化。出于这个原因，行业前沿模型包括谷歌Gemini-1.5、OpenAI的GPT-4 、马斯克旗下xAI公司的Grok等大模型都使用了 MoE。</p><p></p><p>免费下载大模型</p><p>Hugging Face：https://huggingface.co/xverse/XVERSE-MoE-A36B</p><p>魔搭：https://modelscope.cn/models/xverse/XVERSE-MoE-A36B</p><p>Github：https://github.com/xverse-ai/XVERSE-MoE-A36B</p><p></p><p></p><h1>商业应用上更进一步</h1><p></p><p></p><p>元象此次开源，在商业应用上也更进一步。</p><p></p><p>元象基于MoE模型自主研发的AI角色扮演与互动网文APP Saylo，通过逼真的AI角色扮演和有趣的开放剧情，火遍港台，下载量在中国台湾和香港娱乐榜分别位列第一和第三。</p><p></p><p>MoE训练范式具有「更高性能、更低成本」优势，元象在通用预训练基础上，使用海量剧本数据「继续预训练」（Continue Pre-training），并与传统SFT（监督微调）或RLHF（基于人类反馈的强化学习）不同，采用了大规模语料知识注入，让模型既保持了强大的通用语言理解能力，又大幅提升「剧本」这一特定应用领域的表现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a4098894b61fd7592d17a58e0b454c6.png" /></p><p></p><p></p><p>在商业应用上，元象大模型是国内最早一批、广东前五获得国家备案的大模型，可向全社会提供服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52df7088a8cbf7b6aa05c40df929437a.png" /></p><p></p><p>从去年起，元象大模型已陆续与QQ音乐、虎牙直播、全民K歌、腾讯云等深度合作与应用探索，为文化、娱乐、旅游、金融领域打造创新领先的用户体验。目前，元象累计融资金额已超过 2 亿美元，投资机构包括腾讯、高榕资本、五源资本、高瓴创投、红杉中国、淡马锡和CPE源峰等。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/27/2775e2b5166bcf74e5017d79a5605508.png" /></p><p></p><p></p><p></p><h1>MoE技术自研与创新</h1><p></p><p></p><p>MoE是目前业界最前沿的模型框架，由于技术较新，国内外开源模型或学术研究同步探索。元象在此次升级中围绕效率和效果进行了如下探索：</p><p></p><p></p><h2>效率方面</h2><p></p><p></p><p>MoE架构与4D拓扑设计：MoE架构的关键特性是由多个专家组成。由于专家之间需要大量的信息交换，通信负担极重。为了解决这个问题，元象采用了4D拓扑架构，平衡了通信、显存和计算资源的分配。这种设计优化了计算节点之间的通信路径，提高了整体计算效率。</p><p></p><p>专家路由与预丢弃策略：MoE的另一个特点是“专家路由机制”，即需要对不同的输入进行分配，并丢弃一些超出专家计算容量的冗余数据。为此元象团队设计一套预丢弃策略，减少不必要的计算和传输。同时在计算流程中实现了高效的算子融合，进一步提升模型的训练性能。</p><p></p><p>通信与计算重叠：由于MoE架构的专家之间需要大量通信，会影响整体计算效率。为此团队设计了“多维度的通信与计算重叠”机制，即在进行参数通信的同时，最大比例并行地执行计算任务，从而减少通信等待时间。</p><p></p><p></p><h2>效果方面</h2><p></p><p></p><p>专家权重：MoE 中的专家总数为 N ，每个 token 会选择 topK 个专家参与后续的计算，由于专家容量的限制，每个 token 实际选择到的专家数为 M，M&lt;=K<="" p=""></p><p></p><p>实验1：权重在 topM 范围内归一化</p><p>实验2：权重在 topK 范围内归一化</p><p>实验3：权重在 topN 范围内归一化</p><p>实验4：权重都为 1</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e11f4ddfc77c7073e4c463d6c80f93b.webp" /></p><p></p><p>对比实验结果</p><p></p><p>举例说明，假设N=8，K=4，M=3（2号专家上token被丢弃），不同专家权重的计算方式所得的权重如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b45bd8959a1ad2b1bb6575173c959d3.webp" /></p><p></p><p></p><p>数据动态切换：元象以往开源的模型，往往在训练前就锁定了训练数据集，并在整个训练过程中保持不变。这种做法虽然简单，但会受制于初始数据的质量和覆盖面。此次MoE模型的训练借鉴了"课程学习"理念，在训练过程中实现了动态数据切换，在不同阶段多次引入新处理的高质量数据，并动态调整数据采样比例。</p><p></p><p>这让模型不再被初始语料集所限制，而是能够持续学习新引入的高质量数据，提升了语料覆盖面和泛化能力。同时通过调整采样比例，也有助于平衡不同数据源对模型性能的影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89394bf927320f9dcc53c8f866fa8f55.webp" /></p><p></p><p>不同数据版本的效果曲线图</p><p></p><p>学习率调度策略（LR Scheduler）：在训练过程中动态切换数据集，虽有助于持续引入新知识，但也给模型带来了新的适应挑战。为了确保模型能快速且充分地学习新进数据，团队对学习率调度器进行了优化调整，在每次数据切换时会根据模型收敛状态，相应调整学习率。实验表明，这一策略有效提升了模型在数据切换后的学习速度和整体训练效果。</p><p></p><p>下图是整个训练过程中 MMLU、HumanEval 两个评测数据集的效果曲线图。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f1b1c81ac78dc7215c6cf1da11148cfe.webp" /></p><p>训练过程中MMLU、HumanEval的性能曲线持续拔高</p><p></p><p>通过设计与优化，元象MoE模型与其Dense模型XVERSE-65B-2相比，训练时间减少30%、推理性能提升100%，模型效果更佳。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PegbNEHXRRiChk51E98j</id>
            <title>OpenAI 有 o1 大模型，QCon 有大模型推理技术实践，大模型基础设施与算力优化拿捏！</title>
            <link>https://www.infoq.cn/article/PegbNEHXRRiChk51E98j</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PegbNEHXRRiChk51E98j</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 04:47:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>刚刚，OpenAI 震撼发布 o1 大模型！新模型可以实现复杂推理，强得可怕！！！在即将于 <a href="https://qcon.infoq.cn/2024/shanghai/">10 月 18-19 日召开的 QCon 上海站</a>"，月之暗面、微软亚洲研究院、商汤科技等企业的资深技术专家也将分享推理相关话题，Mooncake 分离式推理、长文本 LLMs 推理优化、异构分布式大模型推理技术……简直是会圈天菜！<a href="https://qcon.infoq.cn/2024/shanghai/track/1715">大模型基础设施与算力优化实践</a>"轻松拿捏！</p><p></p><h2>精彩内容速递</h2><p></p><p></p><h4>Mooncake 分离式推理架构创新与实践</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/05/056c75dd48346381d58edb76a408dc08.png" /></p><p></p><p>随着大型语言模型的社会影响力日益增强，相应的人工智能产品用户基数也在迅速扩大。目前，AI产品发展的一个主要挑战是如何在有限的计算资源下，有效应对日益增长的用户需求。本议题从实际业务出发，探讨在固定集群资源的条件下，通过采用单点和分布式推理架构，提升集群处理大规模请求的能力，过程中遇到的挑战以及我们的解决策略，希望能给大家带来一些帮助和思考。</p><p></p><p>演讲提纲</p><p>1. 大规模推理挑战</p><p>优雅的集群过载超长上下文性能挑战故障定位与自动运维</p><p></p><p>2. 单点性能优化</p><p>混合并行策略长上下文推理优化</p><p></p><p>3. 分离式架构 Mooncake</p><p>设计场景 —— SLO vs MFU - 分离式架构设计集群调度策略、热点均衡开源计划</p><p></p><p>4. 未来展望 - 硬件能力展望</p><p>更细粒度的池化分离分离式内存系统</p><p></p><p>实践痛点</p><p>生产环境高负载下有效地过载线下测试与线上负载的解耦</p><p></p><p>演讲亮点</p><p>经过实际生产环境大规模验证的分离式推理系统，面对真实线上负载实现性能提升从实际业务出发，分析推理系统设计决定和关键技术</p><p></p><p>听众收益</p><p>了解分离式架构在实际生产环境中的挑战与发展趋势了解未来硬件/算法演进方向</p><p></p><p></p><h4>长文本 LLMs 推理优化：动态稀疏性算法的应用实践</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7f4aeb3515d23f88b8c432b73f6db5d.png" /></p><p></p><p>Long-context LLMs Inference的prefilling 阶段由于 Computation bottleneck 造成的长时延 (单卡 A100，1M 8B 约 30 分钟) 给 Long-context LLMs 的应用造成了困难。而 Attention 尤其是Long-context Attention 实际上是非常稀疏且动态的。利用这种动态稀疏性，我们将 Long-context Attention 存在的动态稀疏归纳成三种 Pattern，通过离线搜索出每个 Head 最优的稀疏Pattern，并利用很小的 overhead 在线确定动态稀疏 index，再结合动态稀疏编译器 PIT 和 Triton 进行高效的动态稀疏 GPU 运算，产生实际加速比。我们对市面上主流的 Long-context LLMs , like LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, Qwen2-128K 在RULER，InfiniteBench，Needle Test，LM 等任务中进行了测试，结果显示其具有几乎相同的性能。</p><p></p><p>本次演讲将主要跟大家分享 LLMs 推理算法侧优化方法，包括量化，剪枝，模型架构优化，FFN 动态稀疏计算等方面的研究和实践。</p><p></p><p>演讲提纲</p><p></p><p>1. LLMs 推理算法侧优化方法讨论：如量化，剪枝，模型架构优化，FFN 动态稀疏计算等</p><p></p><p>2. 长文本 LLMs Inference 遇到的一些挑战</p><p>Attention 结构平方复杂度导致的 Prefilling 阶段较高的 TTFT解码阶段 KV cache 存储压力，计算要提供一个合理 TTFT 的 API 服务理论上需要对 Attention 进行多少倍加速</p><p></p><p>3. 研究思考</p><p>优化 Long-context LLMs Inference 的相关方法，包括 training from scratch 和 training-free 两大类方法。Attention 是动态稀疏的，Attention 的动态稀疏在空间上具有聚集性，呈现出三种不同的 pattern；</p><p></p><p>4. 解决方案</p><p>MInference、decoding 和多轮推理实现细节，包括 GPU Kernel 实现评测结果，包括有效性和高效性</p><p></p><p>5. 总结和未来展望</p><p></p><p>实践痛点</p><p>对于短文本场景，利用动态稀疏性可能会引入 overhead，获得的加速比较低</p><p></p><p>演讲亮点</p><p>LLMs 推理算法侧优化方法，包括量化，剪枝，模型架构优化，FFN 动态稀疏计算等方面首个有效降低长文本大模型推理中预填充阶段成本并保持性能的解决方案协同设计的算法和系统，能够在无需训练的情况下实现端到端加速</p><p></p><p>听众收益</p><p>了解算法侧优化 LLMs Inference 的思路和 Long-context LLMs inference 前沿研究动向和潜在的优化思路</p><p></p><p></p><h4>异构分布式大模型推理技术实践</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc05d5487b5c87dfa26479b74ec01a70.png" /></p><p></p><p>随着人工智能领域的发展，越来越复杂的大型语言模型正在被广泛应用于各个行业，这些模型的推理需求也随之大幅提升。鉴于国际供应链的持续不确定性，我们或将面临因依赖英伟达芯片而产生的潜在风险与挑战。为此，我们采用了英伟达和国产化芯片混合的异构分布式推理方案，该方案将充分发挥两种芯片的优势，确保系统的高效性和稳定性，同时减少对单一供应链的依赖，提升推理能力和自主控制能力。</p><p></p><p>推理优化已经不局限于算子层面，需要站在系统全局的角度分析并解决问题，需要设计者有全面的技术积累(分布式、算法、算子优化、量化)，需要站在异构大集群的背景下思考问题。本次演讲将分享商汤高性能计算与推理团队自研的异构分布式大模型推理系统遇到的挑战以及实现，希望能给大家带来一些帮助和思考。</p><p></p><p>演讲提纲</p><p>1. 异构分布式大模型推理系统优化</p><p>大模型推理已经演变成一项复杂的系统级别优化适配不同芯片的分布式异构推理系统模型快速加载，推理 POD 快速拉起</p><p></p><p>2. 多元算力芯片推理优化</p><p>推理芯片评测选型多元算力芯片深度推理优化</p><p></p><p>3. MOE 的推理优化</p><p>MOE 的兴起MOE 的推理优化方案MOE + MLA 的优势</p><p></p><p>4. 大规模异构推理集群的未来展望</p><p>更大规模的异构集群的管理调度高效的多模态融合推理</p><p></p><p>实践痛点</p><p>异构芯片之间的通信交互优化如何快速的进行多元算力芯片选型</p><p>演讲亮点</p><p>深入剖析多样化芯片适配优化方案MOE + MLA 的深度推理优化方案</p><p></p><p>听众收益</p><p>了解多元算力芯片技术发展趋势了解大模型推理系统的现状和演进方向</p><p></p><p>更多精彩内容，敬请关注 QCon 上海站，锁定「大模型基础设施与算力优化」专题，届时还会有小红书<a href="https://qcon.infoq.cn/2024/shanghai/presentation/6139">基于 PPO 的多模态大模型 RLHF 系统的设计与优化</a>"、华为<a href="https://qcon.infoq.cn/2024/shanghai/presentation/6135">昇腾万卡集群大模型性能提升实践</a>"等精彩内容。</p><p></p><p>会议推荐</p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，QCon 还是太全面了。现在报名可以享受 9 折优惠，详情请联系票务经理 17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c87f0820b187f4ea98d5fe2bdce0f4c1.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XqERUHzQXUGc91UGR4Kq</id>
            <title>一场演讲就能“值回票价”！来 QCon 听李云分享 AI 时代团队管理的变与不变</title>
            <link>https://www.infoq.cn/article/XqERUHzQXUGc91UGR4Kq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XqERUHzQXUGc91UGR4Kq</guid>
            <pubDate></pubDate>
            <updated>Fri, 13 Sep 2024 03:40:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2015 年，时任阿里巴巴集团 UC 浏览器电脑版技术团队主管兼软件架构师的李云，在 QCon 北京站带来了《打造高质效的技术团队》的专题演讲，具体以淘宝浏览器团队过渡到 UC 浏览器团队为例，分享了技术团队如何在工作中以质效为导向确保可持续发展。</p><p></p><p>“这是一场值回票价的演讲。”现场听众对那次分享给出了高度的赞誉和反馈。</p><p></p><p>李云老师在当年提出，质效（质量和效率，效率包含效果）可以从技术与管理两个来源获得。技术层面体现于通过技术的方法去改善团队的工作效率和质量；管理层面则在于确保整个团队步调一致地工作和个体工作行为的稳定性。后者的根源在于持续改善工程师的工作习惯。</p><p></p><p>时隔近 10 年再回看这一观点，仍然具有十足的前沿性。李云老师于2022年离开了阿里并加了一家芯片行业的小公司做高管，担任高管期间进一步完善了2015演讲时所提出的团队效能动力模型，这一经历除了验证了他在大厂总结的管理经验对小公司也适用外，进一步引发了他将自己的经验体系性地分享出来的想法，于是有了今年9月正式面市的《全面效能》这本书。今年他创办了致效企业管理咨询（杭州）有限公司，希望通过陪跑式的咨询帮助更多的企业走出内卷与低效，从而实现员工与企业的双赢。</p><p></p><p>除此之外，李云老师在阿里期间还是 Service Mesh 的重要参与者与推动者，并于 2018、2019 年连续在 QCon 大会上分享了《Dubbo Mesh——Service Mesh 的本质、价值与应用探索》和《分布式应用的未来——Distributionless》两个话题。在今年 3 月，他的《工程师个人发展指南》专栏也在在极客时间上线了，感兴趣的小伙伴可以去搜索学习。</p><p></p><p></p><blockquote>查看下方链接回顾详细内容：分布式应用的未来——Distributionless：<a href="https://www.infoq.cn/article/Np1tzI4sssC-RbhhNI8R?utm_campaign%EF%BC%89">https://www.infoq.cn/article/Np1tzI4sssC-RbhhNI8R?utm_campaign）</a>"Dubbo Mesh——Service Mesh 的本质、价值与应用探索：<a href="https://www.infoq.cn/article/JqQvS3CImCSj7kEBR9Ge?utm_campaign">https://www.infoq.cn/article/JqQvS3CImCSj7kEBR9Ge?utm_campaign</a>"</blockquote><p></p><p></p><p>而在即将于 10 月 18−19 日召开的 QCon 上海站上，李云老师将再次回到 QCon 的舞台并带来主题为「AI 时代团队管理的变与不变」的演讲，分享在 AI 这个全新时代背景下，个人与团队、业务与技术整合的体系化技术管理之路。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/3279465c5c4f2d2e2ff31a3383c60978.webp" /></p><p></p><p>值得关注的是，届时在大会现场，李云老师还将携全新著作《全面效能》一书进行签售。感兴趣的小伙伴们可扫描上方二维码或点击链接咨询购票直达现场与李云老师面对面深度交流：<a href="https://qcon.infoq.cn/2024/shanghai/apply">https://qcon.infoq.cn/2024/shanghai/apply</a>"</p><p></p><p>与此同时，在「与时俱进的团队管理」 专题下，目前已确认阿里巴巴技术总监许晓斌和 Paypal 资深经理李清玉还将带来主题为《负责任的技术规划 —— 不仅仅是技术》和 《如何提升个人领导力以适应技术管理者的角色？》 的分享。欢迎扫描下方二维码添加小助手关注更多大会内容和进展，我们将竭诚为大家准备更多 「值回票价」的干货演讲！</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68a4f559d6682dec46bd5633588299f0.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/slYNs7YIDmD6bIJK3WF7</id>
            <title>技术与艺术的碰撞！这次组委会居然CNCC把带到了横店</title>
            <link>https://www.infoq.cn/article/slYNs7YIDmD6bIJK3WF7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/slYNs7YIDmD6bIJK3WF7</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Sep 2024 10:22:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024年9月10日，北京迎来了计算机科学与技术领域的一场盛事——2024中国计算机大会（CNCC2024）新闻发布会的隆重举行。人民日报、中国青年报、中国科学报、南方都市报、中国工业报、中国经营报、人民网、新华网、科学网、搜狐科技、网易科技、钛媒体、爱奇艺、智东西、InfoQ等多家媒体和合作机构出席发布会。</p><p></p><p>本次大会以“发展新质生产力，计算引领未来”为主题，汇聚了国内外顶尖学者、企业家及行业精英，共同探讨技术发展的新趋势，以及技术普惠化对于社会的深远影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b83fc6d0cce10471b4a7c633c3ffa34.png" /></p><p></p><p>CNCC2024指导委员会主席、CCF理事长，中国工程院院士，中国科学院计算所研究员孙凝晖；程序委员会主席，北京大学教授胡振江；论坛委员会主席、CCF常务理事，中国人民大学教授文继荣；组织委员会副主席，东阳市委常委、横店镇党委书记胡利群；组织委员会共同主席，横店集团党委书记、董事、副总裁吕跃龙等嘉宾出席发布会。发布会由CNCC2024大会总监、CCF秘书长唐卫清主持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b30698928aace17ad743ac3285a6688d.png" /></p><p></p><p>（CNCC2024大会总监、CCF秘书长唐卫清主持发布会）</p><p></p><p>发布会现场，CNCC2024指导委员会主席、CCF理事长孙凝晖院士详细介绍了大会的筹备情况及特色亮点。他特别指出，本次大会首次将学术会议选址于县级市横店，不仅是对横店经济社会发展的高度认可，更是对当地产业数字化水平和创新活力的充分肯定。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95700f970541ae0b6b65cb6d0bbb3eb8.png" /></p><p></p><p>（指导委员会主席、CCF理事长，中国工程院院士，中国科学院计算所研究员孙凝晖）</p><p></p><p>孙凝晖院士强调，技术发展的最终目的是普惠社会，让科技成果惠及更广泛的人群。他提到，横店作为全国知名的影视文化名城，其独特的产业背景和文化氛围为本次大会增添了别样的色彩。而大会的举办，也将进一步推动横店乃至整个地区的科技创新和产业升级，为地方经济社会发展注入新的活力。</p><p></p><p>CNCC 2024委员会主席、北京大学教授胡振江在发布会上详细介绍了大会的特邀报告及论坛组织情况。他透露，本次大会特邀报告讲者包括图灵奖获得者、中国科学院外籍院士、美国康奈尔大学名誉教授、北京大学客座讲席教授John Hopcroft；德国达姆施塔特工业大学教授Jan Peters；美国加州大学伯克利分校教授Michael I. Jordan；CCF会士、中国科学院外籍院士、新加坡国立大学教授黄铭钧；CCF会士、前理事长，中国科学院院士，北京大学教授梅宏；CCF会士、中国科学院院士、中国科学院软件研究所研究员林惠民；CCF会士、中国科学院院士、中国科学院软件研究所研究员冯登国；CCF会士、中国科学院院士、北京航空航天大学教授郑志明；中国工程院院士、国家数字交换系统工程技术研究中心主任、复旦大学教授邬江兴；上海人工智能实验室主任、首席科学家、清华大学惠妍讲席教授周伯文；CCF会士、联想集团高级副总裁、欧洲科学院外籍院士芮勇；CCF常务理事、阿里云副总裁刘湘雯；中国电信首席科学家、云计算研究院院长、美国天普大学讲席教授吴杰；百川智能创始人、CEO王小川；蚂蚁集团智能引擎技术事业部副总裁周俊，更多特邀报告讲者还在邀请中。同时，大会还设置了多个专题论坛，涵盖智慧城市、量子计算、存储系统软件等多个领域，旨在为参会者提供一个全面、深入的交流平台。</p><p></p><p>值得一提的是，本次大会还特别注重技术普惠化的理念。在论坛组织上，大会鼓励企业组织或参与论坛，推动产学研深度融合，加速科技成果的转化和应用。此外，大会还通过线上直播、互动问答等多种形式，让无法亲临现场的观众也能感受到科技的魅力，共享技术发展的成果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc34cd6a87e46a24060bbb734e765cf2.png" /></p><p></p><p>（CNCC2024程序委员会主席，北京大学教授胡振江）</p><p></p><p>CNCC2024论坛委员会主席、CCF常务理事，中国人民大学教授文继荣介绍了专题论坛的组织进展。今年的专题论坛围绕大会主题“发展新质生产力，计算引领未来”进行组织，覆盖面更宽，体现行业化、企业化、会员治会三大特色，鼓励企业组织或参与论坛。从征集的223场论坛中选出138场，论坛领域方向34个，将有超过800位国内外专家学者、企业技术精英应邀担任讲者在大会上交流，数量、质量、覆盖面均再创新高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/899faa1e9d23f748ae77d9a9b4441a78.png" /></p><p></p><p>（CNCC2024论坛委员会主席、CCF常务理事，中国人民大学教授文继荣）</p><p></p><p>东阳市委常委、横店镇党委书记胡利群在发言中介绍了东阳市的筹备工作进展。他表示，CNCC首次落户县级市横店，是对东阳经济社会发展的巨大鼓舞和鞭策。东阳市将全力以赴做好大会的各项筹备工作，确保大会圆满成功。同时，他也期待通过本次大会的举办，进一步推动横店乃至整个地区的科技创新和产业升级，为地方经济社会发展注入新的动力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/034cf9e9a757f0b910b0a928a8328c8d.png" /></p><p></p><p>（CNCC2024组织委员会副主席，东阳市委常委、横店镇党委书记胡利群）</p><p></p><p>在答记者问环节，与会嘉宾就横店的会议组织、大模型发展、信息产业发展等话题进行了深入交流。他们一致认为，技术的发展离不开社会各界的共同努力和支持，而技术普惠化则是技术发展的最终目标和归宿。只有让科技成果真正惠及广大民众，才能推动社会的全面进步和发展。</p><p></p><p>随着CNCC2024新闻发布会的圆满落幕，一场盛大的学术盛宴即将拉开帷幕。我们期待在大会的舞台上，见证更多科技创新的火花碰撞出璀璨的光芒，共同推动计算机科学领域的前沿探索与新质生产力的发展。同时，我们也期待技术普惠化的理念能够深入人心，让科技的光芒照亮每一个角落，为社会的全面进步和发展贡献更多的智慧和力量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QiWT133yOvuVQOKsgNmL</id>
            <title>车企掀起“造芯潮”后，软硬一体的规模量产变智驾竞争关键：出货低于100万即面临投产失衡</title>
            <link>https://www.infoq.cn/article/QiWT133yOvuVQOKsgNmL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QiWT133yOvuVQOKsgNmL</guid>
            <pubDate></pubDate>
            <updated>Thu, 12 Sep 2024 02:24:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近几年，特斯拉从采用供应商的“重软硬一体化”方案，过渡到自研算法结合第三方芯片的“轻软硬一体化”方案，最终实现了自研芯片的“重软硬一体化”。如今，国内的蔚来、理想、小鹏等公司也遵循了类似的路径。“蔚小理”推出的旗舰车型大多基于Orin芯片平台，并都已宣布了芯片流片的进展，预计将在未来一到两年内搭载上车。</p><p>&nbsp;</p><p>随着智驾技术的不断进步，自动驾驶行业软硬一体的趋势愈加明显，软硬一体化系统的大规模量产能力也逐渐成为高阶智驾竞争的胜负手。</p><p>&nbsp;</p><p>9月5日，在辰韬资本联合主办的“2024自动驾驶软硬协同发展论坛暨报告发布会”上，近200位产业专家、投资机构、研究机构及智能驾驶头部企业的代表探讨了软硬一体产品设计模式对于自动驾驶行业带来的挑战和机遇。&nbsp;</p><p>&nbsp;</p><p>会上，辰韬资本、南京大学上海校友会自动驾驶分会、九章智驾三方联合发布2024年度《自动驾驶软硬一体演进趋势研究报告》，该报告从软硬一体的定义及行业现状，软硬一体的底层原因、软硬一体的开发能力分析、软硬一体代表公司、软硬一体发展驱动力及软硬一体未来发展趋势等多个方面对软硬一体这一产品设计模式进行深入分析。</p><p>&nbsp;</p><p></p><h1>成本驱动，软硬—体模式发生转变</h1><p></p><p>&nbsp;</p><p>尽管“软硬一体”已经成了行业内很多领先玩家的重要战略，但是目前仍没有对软硬一体给出有效的讨论范畴的定义。该报告指出，软硬—体硬件核心是自动驾驶的高性能计算芯片，并将软硬一体的讨论限定在对于自动驾驶行业生态有重大影响的核心对象：包括自动驾驶软件Tier1、自动驾驶芯片厂商以及主机厂，描述的是公司具备的软硬件协同的研发能力和开发模式，并能够提供软硬一体的产品。</p><p>&nbsp;</p><p>与此同时，该报告对软硬一体的形态进行了界定，并将软硬一体两种典型的形态分为“重软硬一体”和“轻软硬一体”。目前，行业内有三种不同的分工模式和开发模式。</p><p>&nbsp;</p><p>“重软硬一体”指由同一个公司完成芯片、算法、操作系统/中间件的全栈开发，基于此衍生出生态合作模式，这种模式包括海外的 Mobileye、特斯拉、Nvidia（开发中） 以及国内的华为、地平线、Momenta（开发中）等。“轻软硬一体”指自动驾驶解决方案公司采用第三方芯片，在某款特定芯片上具备极致的优化能力和丰富的产品化交付经验，能够最大化发挥该款芯片的潜能，这方面的典型案例包括卓驭（大疆）、Momenta 等。从“重软硬一体”模式衍生出的轻量化模式，将软硬耦合程度最深的AI算法和SoC芯片做深度绑定，作为标品向客户提供，其他软件模块和硬件模块由生态合作伙伴来提供。Mobileye、地平线、特斯拉在最早期采用的就是这样的开发模式。</p><p>&nbsp;</p><p>辰韬资本执行总经理刘煜冬表示，现在自动驾驶芯片并没有达到算力非常富裕的阶段，不存在绝对的软硬分离公司或者开发模式。软硬解耦更多展现的是过程，指的是从“重软硬一体”向“轻软硬一体”转变的过程。而造成软硬—体发展变化的一大原因是成本驱动，主要有两个方面。</p><p>&nbsp;</p><p>第一，不管由软件公司做定制芯片，还是做深度绑定某一颗芯片优化，可以最大化发挥芯片能力，避免很多平台芯片设计带来浪费；第二，由自动驾驶公司或者主机厂自研芯片，可以显著降低单芯片成本。只要有足够芯片出货量，就可以从公司层面降低整体开销。</p><p>&nbsp;</p><p></p><h1>不同赛道企业的软硬一体策略</h1><p></p><p>&nbsp;</p><p>对于芯片公司来讲，执行软硬一体策略必须前瞻性地关注算法的演进趋势，甚至投入较多资源和精力去搭建自研算法团队和解决算法团队。</p><p>&nbsp;</p><p>对于软件Tier1公司来说，采用平台型芯片和采用专用型芯片去应对软硬一体策略会有一些不同。采用平台型芯片，软件Tier1公司需考虑的点与芯片公司比较类似，要对算法和芯片技术进展保持比较激进的前瞻性关注。另外，软件Tier1选择一颗通用芯片的时候，需要在硬件层面留出较多余量。如果采用专用型芯片，能够节省更多资源，提高性价比，所以有更高收益。但对软件Tier1来说，采用这种策略对芯片理解、芯片应用能力的要求会更高。</p><p>&nbsp;</p><p>针对软硬一体策略，整车厂考虑的维度会更加复杂。要不要做算法自研，这是整车厂长期讨论的问题，这主要取决于公司定位。对于大部分新势力来讲，它们本身定位就是自动驾驶科技公司，做算法自研是自然而然的选择；对于大部分传统OEM来说，其更加强调全栈可控，也不一定能够支撑维持大规模算法资源团队，所以倾向于从供应商的白盒合作和内部团队自研两方面去推进。</p><p>&nbsp;</p><p>从软硬一体策略上来讲，整车厂的选择针对不同配置也有差异化。一般来说，针对低阶智驾配置更倾向于采用供应商软硬一体方案，比如Mobileye IQ4、地平线的J2、J3、J6E都能够交付成熟的“重软硬一体”方案；对于高阶自驾配置来说，这通常是主机厂差异化竞争力的来源，所以整车厂会倾向于采用自研算法+平台型的第三方芯片“轻软硬一体”策略，或更进一步自研芯片做“重软硬一体”方案。</p><p>&nbsp;</p><p>而在芯片问题上，对于整车厂来说更多是经济性的考量。报告认为，自研芯片出货量低于100万片就可能很难投入产出比平衡。另外，整车厂自研芯片后，库存控制也会变成需要重点考虑的问题。</p><p>&nbsp;</p><p>尽管软硬一体的方案能够为企业带来成本上的巨大优势以及更广阔的生存空间，但也对于企业在技术能力上提出了更为苛刻的要求——执行软硬一体战略的企业必须在算法、芯片（重软硬一体）以及中间件和底软等领域有着深度的技术积累和工程经验。因此，该报告对软硬一体所必须具备的开发能力如智驾系统算法架构、智驾芯片设计能力、智驾系统底层软件等维度结合不同类型的企业案例进行深入分析。</p><p>&nbsp;</p><p>基于对近30位行业资深专家的访谈以及对历史上其他行业的情况进行横向对比，报告总结了影响软硬一体策略判定的三个要素：技术成熟度、技术平权度及总收益。当满足其中一条时公司就具备考虑软硬一体的条件，满足其中两条时公司就会具有推动软硬一体的动力，如果三条全部满足则软硬一体就是公司在当前的最优选择策略。</p><p>&nbsp;</p><p>此外，报告对自动驾驶赛道软硬件一体不同类型的参与企业，如主流芯片厂商英伟达、华为、地平线、高通；主机厂特斯拉、理想、蔚来、小鹏、比亚迪；软件Tier1如Momenta、卓驭科技（大疆车载）等不同类型的代表性公司当前的软硬一体策略、背后的原因、进展情况、未来趋势等进行了多维度分析。</p><p>&nbsp;</p><p></p><h1>软硬一体未来发展趋势</h1><p></p><p>刘煜冬指出，整个行业走向更加深度软硬—体集成有三个条件。第一，算法技术框架有阶段性收敛。第二，不管是整车厂还是自动驾驶公司，做这件事情难度已经降低很多，得益于半导体行业中的芯片设计、IP、工具链等都已经有比较成熟的生态环节。从芯片设计角度来说，技术有了平权。第三，选择做深度重软一体方案公司，到底能不能有足够出货量覆盖投入产出。</p><p>&nbsp;</p><p>对于软硬一体未来发展趋势，该报告认为，总体来看，软硬一体与软硬解耦是一体两面，最终市场会形成两者并存的态势，但是短期内，软硬一体的公司在市场上体现出更强的竞争力。</p><p>&nbsp;</p><p>在自动驾驶行业，软硬一体的趋势会根据自动驾驶方案的高低阶而有所不同：对低阶智驾，主机厂往往会直接采用供应商的软硬一体方案，并向标准化的方向发展；对高阶智驾算法等关键能力，主机厂自研的比例会越来越高；当芯片算力远大于实际应用的需求、解决方案与芯片算力的适配不再成为核心能力时，行业就具备了达到软硬解耦的必要条件。</p><p>&nbsp;</p><p>不过由于当前算法仍在快速迭代，对算力的需求仍处于激增状态。目前仍然是芯片算力配合算法需求进行不断提升，所以在很长一段时间内，软硬一体策略仍然会是行业主流。</p><p>&nbsp;</p><p>对芯片公司来说，需要持续不断做更多软件方面的投入来构建自己的“护城河”，工具链层面也需要打造更完整的生态。对整车厂来说，如果自研芯片能够实现比较好的投入产出比，整车厂也会有比较强的动力和技术实力做自研芯片。</p><p>&nbsp;</p><p>对于软件Tier1来说，由于大部分整车厂和终端客户选择智驾系统的时候，它的T0级决策是选芯片，并不会因为软件公司到底用哪块芯片去选。所以更好的策略是深度适配更多的芯片。如果有足够的实力自研芯片，需要更好抓住技术变化“窗口期”来发挥软件公司在需求引领和迭代方面的优势。</p><p>&nbsp;</p><p></p><h1>新技术带来的影响</h1><p></p><p>端到端</p><p>&nbsp;</p><p>今年业内在端到端布局非常多，并且都认为端到端会对芯片计算需求越来越高。对于通用型芯片公司来说，应对端到端的挑战需要做计算能力更强的芯片。同时，现在端到端用到的基本神经网络架构还没有收敛。之前都做Transformer，未来像Mamba以及RWKV等这类新的神经网络架构也在出现，专用芯片针对这样的算子去做前瞻性的布局也会获取一定的优势。</p><p>&nbsp;</p><p>舱驾一体</p><p>&nbsp;</p><p>实际上，现在市场上真正能够实现单芯片单板舱驾一体的芯片选择并不多，比较有代表性的就是英伟达和高通两款芯片。早期舱驾一体更多会采用“轻软硬一体”方案，由不同解决方案公司搭载在少数平台芯片上。</p><p>&nbsp;</p><p>具身智能</p><p>&nbsp;</p><p>具身智能可能是智能手机、智能自动驾驶后新的发展趋势，现在也比较像智能驾驶行业早期发展情况。早期，低阶任务机器人可能会采用比较简单“重软硬一体”解决方案。随着生态的发展，未来会有更多不同分工以及开发模式的选择。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>