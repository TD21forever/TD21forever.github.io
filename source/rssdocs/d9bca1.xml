<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</id>
            <title>检索增强生成引擎 RAGFlow 正式开源！仅一天收获上千颗星</title>
            <link>https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 10:26:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RAG 解决方案, AI 原生数据库, 长上下文 LLM, RAG 引擎
<br>
<br>
总结: 4 月 1 日，我们正式宣布端到端 RAG 解决方案 RAGFlow 开源。在此之前，我们还开源了 AI 原生数据库 Infinity。Infinity 项目在 GitHub 上仅三个月时间就获得了 1400 颗星，而 RAGFlow 在开源首日就获得了上千颗星。我们开源这两个项目的初衷是希望能够从更多更广泛的应用场景中收到反馈，以便尽快让 RAG 走出当前的初期阶段。随着长上下文 LLM 的不断普及，我们希望这一天能够尽快到来。 </div>
                        <hr>
                    
                    <p>4 月 1 日，我们正式宣布端到端 RAG 解决方案 RAGFlow 开源。在此之前，我们还开源了 AI 原生数据库 Infinity。Infinity 项目在 GitHub 上仅三个月时间就获得了 1400 颗星，而 RAGFlow 在开源首日就获得了上千颗星。我们开源这两个项目的初衷是希望能够从更多更广泛的应用场景中收到反馈，以便尽快让 RAG 走出当前的初期阶段。随着长上下文 LLM 的不断普及，我们希望这一天能够尽快到来。</p><p></p><p>项目开源地址：</p><p>Infinity : https://github.com/infiniflow/infinity</p><p>RAGFlow：https://github.com/infiniflow/ragflow</p><p>RAGFlow 在线 Demo：https://demo.ragflow.io/</p><p></p><p>在回答 RAGFlow 有哪些特点之前，我们先来谈谈为何要做这样一款 RAG 引擎。</p><p></p><p>今年 2 月以来， AI 领域连续出了很多重磅热点，除了最火热的 Sora 之外，另一个热点就是长上下文 LLM ，例如 Claude 3、 Gemini 1.5，当然也包含国产的月之暗面。Sora 的本质是针对视频具备更加可控性的生成能力，这其实是解锁未来多模态 RAG 热潮的一个必要条件；而长上下文 LLM ，却引发了更多针对 RAG 的争论，因为这些 LLM，可以很方便的让用户随时上传 PDF，甚至上传几十个 PDF，然后针对这些 PDF 回答问题，并且还具备强大的“大海捞针”能力。所谓“大海捞针”，意思就是针对这些长上下文窗口的细节提问，看 LLM 是否可以准确地回答。</p><p></p><p>用 RAG 来实现大海捞针是轻而易举的，然而目前列举的这些 LLM，它们不是基于 RAG 来提供这种能力，却也都可以达到很高的召回，同时它们也不是采用类似 StreamLLM 这种基于滑动窗口实现长上下文注意力的机制——这种机制仅仅是增加了上下文窗口，但却仍然在细节召回上表现不佳，窗口滑过，内容即会被逐渐“遗忘”。我们也试验了其中的若干产品，效果确实非常好，上传一个 PDF，甚至可以针对里边的复杂图表给出精确的回答。</p><p></p><p>因此，这引发了新的一轮关于长上下文 LLM 和 RAG 的争论，许多人评价 “RAG 已死”，而 RAG 拥护者则认为，长上下文 LLM 并不能满足用户海量数据的需求，成本高，速度也不够快，也只能针对长文本、图片等数据提问。</p><p></p><p>随着长上下文为更多用户接纳，近期各家国产 LLM 都快速推出了这个产品特性，除月之暗面外，其他家大多基于 RAG 来实现，下表是两者的基本对比：</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/847a76363897b80afb0019624bce6896.webp" /></p><p></p><p>这里要额外说明一下，为何 RAG 派的大海捞针能力一般，这并不是 RAG 本身的问题，而是依靠纯向量数据库去构建 RAG，并不能保证对精确数据和细节的准确召回。</p><p></p><p>以上的对比，其实并没有完全解答 RAG 的必要性，因为至少就目前 RAG 最普遍的场景——个人知识库问答而言，确实很多情况下只需要 LLM 就足够了。而我们则认为，LLM 的长上下文能力，对于 RAG 来说是个很大的促进。这里先用 OpenAI 联创 Andrej Karpathy 的一张图做个类比，他把 LLM 比喻为一台计算机的 CPU， 把上下文类比为计算机的内存，那么以向量为代表的数据库，就可以看作是这台计算机的硬盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f1bcd9b9688da15d1a5a9f72e032d06b.webp" /></p><p></p><p>我们进一步来说明，为什么即使有了“大海捞针”能力，RAG 仍然必不可少。RAG 从提出到为业界广泛接纳，经历了一年多时间，当下的 RAG 产品已经并不稀缺，然而在实际应用中，却普遍得出了“ RAG 属于上手容易，但真正落地却很难”的结论。究其原因，这里边主要包含两个方面：</p><p></p><p>其一是来自 LLM 自身。由于 RAG 的工作流程是针对从数据库返回的结果进行回答，这样的话，对于 RAG 来说，LLM 最基础也是最重要的能力其实包含：</p><p>摘要能力；可控性：既 LLM 是否听话，是否会不按照提示要求的内容自由发挥产生幻觉；翻译能力，这对于跨语言 RAG 是必备的。</p><p></p><p>遗憾的是，在过去，国内可以用到的 LLM 中，在这 3 点上表现良好的并不多。至于所谓高级的能力，例如逻辑推理，以及各类 Agent 要求的自主决策能力等，这些都是建构在以上基础能力之上，基础不好，这些也都是空中楼阁。</p><p></p><p>其二，则是来自于 RAG 系统本身。我们所说的 RAG，实际上包含完整的链路，包括数据的准备，数据写入，乃至从数据库查询和返回结果排序。在整条链路中，最大的难点来自于两方面：一是如何应对复杂多变的数据，这些数据包含各种格式，更复杂的还包含各类图表等，如果在没有理解这些语义的基础之上直接提供 RAG 方案，就会导致语义丢失从而让 RAG 失败。二是如何查询和排序：简单地讲，在大多数情况下，都必须引入多路召回和重排序，才能保证数据查询的准确度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0abff5fc98553f8a297e257a55a6ec57.webp" /></p><p></p><p>假如我们不去专注于解决这两类问题，那么就很容易陷入让 RAG 去和长上下文 LLM 反复对比的情况，因为两者其实都可用于简易知识库对话场景：RAG 仅仅提供数据的简单解析，然后直接转化为向量，最后用单一向量做召回，这除了成本，以及私有化场景里所要求的安全等优势之外，在核心对话能力上并没有显著地跟长上下文 LLM 区分开来，甚至还有所不及。</p><p></p><p>正是基于这些 RAG 本身的痛点，我们先后推出了 2 个开源项目：</p><p></p><p>第一个是 AI 原生数据库 Infinity。它解决的是如何解锁 RAG 服务 B 端场景下遇到的典型问题：如何跟企业已有的数据——包括但不限于非结构化的文档、图片，还包括结构化的信息系统来结合，并解决多路召回和最终融合排序的问题。</p><p></p><p>举几个典型场景：把符合要求的简历筛出，筛选条件包含工作技能（需要向量 + 全文搜索），某类行业的工作经验（基于向量的分组聚合），期望收入，学历，地域（结构化数据）等；基于对话推荐符合个人要求的产品，可以采用多列向量来描述个人偏好，不同的列代表了用户对不同类目产品的过往使用偏好。在推荐过程中，除了采用基于用户的偏好向量进行搜索之外，还需要结合产品的过滤条件：包括是否过期，是否有优惠券，是否符合权限要求，是否有合规要求，该用户是否近期已经购买或者阅读过，等等。</p><p></p><p>这些信息，如果仅仅拿所谓“标量”字段这种方式来表征，那么产品的开发是极其复杂的：因为这需要引入额外的 ETL ，带来了维护性，以及更严重的数据一致性的问题。要知道，RAG 面临的是最终用户使用场景，它是需要业务乃至 LLM 发起请求，就立刻得到答案的，因此不能像数据中台一样仅仅为了一张报表就可以搭建一整套数据管道体系去做宽表这种额外逻辑。因此，Infinity 实际上等于向量数据库 + 搜索引擎 + 普通结构化数据查询，并保证三者的高并发和融合排序。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2ef863f1ac05e0bc35140e4f4307ea18.webp" /></p><p></p><p>第二个就是端到端的 RAG 引擎 RAGFlow。它解决数据的问题：因为如果不对用户数据加以区分和清晰，识别其中的语义，就容易导致 Garbage In Garbage Out。RAGFlow 包含了如下的完整 RAG 流程，确保数据从 Garbage In Garbage Out 变为 Quality In Quality Out。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff18442fabfb2e8a38f3cb937c5b4ff7.webp" /></p><p></p><p>具体来说， RAGFlow 的最大特色，就是多样化的文档智能处理，因此它没有采用现成的 RAG 中间件，而是完全重新研发了一套智能文档理解系统，并以此为依托构建 RAG 任务编排体系。这个系统的特点包含：</p><p></p><p>1. 它是一套基于 AI 模型的智能文档处理系统：对于用户上传的文档，它需要自动识别文档的布局，包括标题、段落、换行等，还包含难度很大的图片和表格。对于表格来说，不仅仅要识别出文档中存在表格，还会针对表格的布局做进一步识别，包括内部每一个单元格，多行文字是否需要合并成一个单元格等。并且表格的内容还会结合表头信息处理，确保以合适的形式送到数据库，从而完成 RAG 针对这些细节数字的“大海捞针”。</p><p></p><p>2. 它是一套包含各种不同模板的智能文档处理系统：不同行业不同岗位所用到的文档不同，行文格式不同，对文档查阅的需求也不同。比如：</p><p>会计一般最常接触到的凭证、发票、Excel 报表；查询的一般都是数字，如：看一下上月十五号发生哪些凭证，总额多少？上季度资产负债表里面净资产总额多少？合同台账中下个月有哪些应付应收？作为一个 HR 平时接触最庞杂的便是候选人简历，且查询最多的是列表查询，如：人才库中 985/211 的 3 到 5 年的算法工程师有哪些？985 硕士以上学历的人员有哪些？赵玉田的微信号多少？香秀哪个学校的来着？作为科研工作者接触到最多的可能是就是论文了，快速阅读和理解论文，梳理论文和引文之间的关系成了他们的痛点。</p><p></p><p>这样看来凭证 / 报表、简历、论文的文档结构是不一样的，查询需求也是不一样的，那处理方式肯定是不一样。因此 RAGFlow 在处理文档时，给了不少的选择：Q&amp;A，Resume，Paper，Manual，Table，Book，Law，通用... 。当然，这些分类还在不断继续扩展中，处理过程还有待完善。我们也会抽象出更多共通的东西，使各种定制化的处理更加容易。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f0ec9574414aeae20c2290390cadd39.gif" /></p><p></p><p>3. 智能文档处理的可视化和可解释性：用户上传的文档到底被处理成啥样了，如：分割了多少片，各种图表处理成啥样了，毕竟任何基于 AI 的系统只能保证大概率正确，作为系统有必要给出这样的空间让用户进行适当的干预，作为用户也有把控的需求，黑箱不敌白箱。特别是对于 PDF，行文多种多样，变化多端，而且广泛流行于各行各业，对于它的把控尤为重要，RAGFlow 不仅给出了处理结果，而且可以让用户查看文档解析结果并一次点击定位到原文，对比和原文的差异，可增可减可改可查，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2c2a6b8ae16839f447d7c0863f1f91a.gif" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d0ff9727d9daa9ee95b59379027c09c3.webp" /></p><p></p><p>4. RAGFlow 是一个完整的 RAG 系统，而目前开源的 RAG，大都忽视了 RAG 本身的最大优势之一：可以让 LLM 以可控的方式回答问题，或者换种说法：有理有据、消除幻觉。我们都知道，随着模型能力的不同，LLM 多少都会有概率会出现幻觉，在这种情况下， 一款 RAG 产品应该随时随地给用户以参考，让用户随时查看 LLM 是基于哪些原文来生成答案的，这需要同时生成原文的引用链接，并允许用户的鼠标 hover 上去即可调出原文的内容，甚至包含图表。如果还不能确定，再点一下便能定位到原文，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c4161919c62326262b3c9cb80837804.gif" /></p><p></p><p>接下来，我们来讲讲，RAGFlow 具体是如何利用文档结构识别模型来处理数据的。所谓文档结构模型，如下所示，是针对文档的布局进行目标识别，然后根据布局再做文字切分。这些布局识别的目标包括文档的标题，段落，语义文字块等等，尤其还会包含文档当中的图表。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1a1650e225536587cb4decc9d5c0e5c.webp" /></p><p></p><p>在识别出这些目标之后，还需要分别对这些目标做相应处理：对于文字来说，需要首先判断文字的换行信息——这对于文字的语义理解也会产生干扰；其次需要对文字内容进行一些整理，这些整理会随着 RAGFlow 模板的不同有所区分；针对表格来说，还需要进一步识别它的内部结构，这在 AI 领域有个专门的研究课题，叫做 TSR(Table Structure Recognition 表格结构识别) 。</p><p></p><p>TSR 任务其实相对比较复杂，因为表格的定义是多种多样的，表格内部可能会出现有线条或者没有线条的情况，对于不同行的文字，判断它们是否是一个单元格是存在很大挑战的，单元格判断失误，很可能就会让表格的数字跟表格列的对应关系弄错，从而影响了对单元格内文字和数字语义的理解。我们花了很多时间来提升 TSR 的能力，最早是利用现成的 OCR 开源模型，后边也尝试过微软研究院专门针对 TSR 任务的 Transformer 模型，但是发觉这些模型处理 TSR 任务的鲁棒性依然非常不足，最后我们还是训练了自己的模型，从而让 TSR 任务表现良好。这个模型比较简单，就是基于 CNN 的目标检测模型，但是它的效果却比上边我们提到的其他模型都要好。为了降低对硬件的依赖和开销，我们甚至切换到用 YOLOv8 来做目标检测，使得仅仅利用 CPU 也可以运行文档结构识别。</p><p></p><p>关于这些，其实也有很多业内人士建议直接走 LLM 的路子，用 LLM 来做文档语义理解，从长期来看这肯定是个趋势，然而在当下来说，让 LLM 在文档结构识别上表现良好，还需要大量的数据才可以。这从我们放弃了基于 Transformer 的 TSR 模型就可以看出：同样的任务下，基于 Transformer 的模型需要更多的数据才可以表现更好，在有限数据下，我们不得不退回到传统 CNN 模型，如果是 LLM ，它需要的数据和算力更多——我们之前曾经尝试过基于多模态 LLM 进行识别的努力，相比专用小模型，它的效果还是差别比较大。从另一个方面也可以看出来，下图是我们用长上下文 LLM 对表格输出的例子：</p><p></p><p>这是原表格：</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13fa0a319e1849790a8590b9e28dba38.webp" /></p><p></p><p>这是识别后的结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/1383bc9362f336615159c4a31d08ca04.webp" /></p><p></p><p>解锁对于非结构化数据的深度语义理解是 RAGFlow 追求的目标之一，我们希望在未来能够将更加 scalable 的文档结构识别模型应用到系统中。不仅如此， RAGFlow 的设计目标是让 RAG 逐渐承接起更多的复杂场景尤其是 B 端场景，因此在未来，它会接入企业的各类数据源，比如 MySQL 的 binlog，数据湖的 ETL，乃至外部的爬虫等。只有这些都被纳入 RAG 的范畴，我们才能实现如下的愿景：</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af53c99ac42cfecb2c8b052689f3ecca.webp" /></p><p></p><p>再回头看前边关于 RAG 和长上下文 LLM 的争论， 显然两者一定是合作的。长上下文 LLM 当下已经逐步具备了 RAG 最不可或缺的基础能力，随着它自身逻辑推理能力地增强，再结合来自数据库，还有数据方面的改进，一定能加速 LLM 的 B 端场景走出婴儿期的进程。</p><p></p><p>RAGFlow 近期更新：将提供类似文件管理的功能，这样 RAG 可以跟企业内部文档以更灵活的方式整合。RAGFlow 中期更新，将提供面向企业级数据接入的低代码平台，同时提供问答对话之外的高级内容生成，比如长文生成等等。</p><p></p><p>Infinity 近期更新：Infinity 近期将发布第一个 release，届时将提供业界最快的多路召回和融合排序能力。</p><p></p><p>欢迎大家关注我们的开源社区，并提出反馈意见！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</id>
            <title>硅谷创业一年，贾扬清讲了自己的AI行业观察：成本、市场增量和商业模式</title>
            <link>https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 10:03:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 贾扬清, AI Infra, 大模型, 英伟达
<br>
<br>
总结: 本文介绍了AI科学家贾扬清选择创业方向为AI基础设施，他创立并开源了深度学习框架Caffe，离开阿里后专注于AI Infra的发展。文章还提到了大模型的商业模式和硬件提供商的发展趋势，以及AI计算与云计算的不同关注点。贾扬清在分享中详细分析了AI时代的基础设施和大模型的应用，强调了中小型模型结合自有数据的优势。 </div>
                        <hr>
                    
                    <p>本文经授权转载于腾讯科技，原文链接：https://mp.weixin.qq.com/s/kpFnXZbROMCdJ5WikuOJvw</p><p></p><p>编辑 /&nbsp;腾讯科技 郭晓静</p><p></p><p>创业一年的贾扬清，选择的方向是AI Infra。</p><p></p><p>贾扬清是最受关注的全球AI科学家之一，博士期间就创立并开源了著名的深度学习框架Caffe，被微软、雅虎、英伟达等公司采用。</p><p></p><p>2023年3月，他从阿里离职创业，并在随后录制的播客中说，自己并非是因为ChatGPT 火爆而创业，后来创业项目浮出水面，也确实证实，他没有直接入局大模型。硅谷著名风投a16z在去年发表的一篇关于AIGC的文章中就曾经提到过：“目前来看，基础设施提供商是这个市场上最大的赢家。”</p><p></p><p>贾扬清在去年的文章中也提到，“不过要做这个赢家，就要更聪明地设计Infra才行”。在他创办的公司Lepton.AI的官网上，有一句醒目的Slogan“Build AI The Simple Way（以简单的方式构建AI）”。</p><p></p><p>最近，贾扬清在高山书院硅谷站“高山夜话”活动中，给到访的中国企业家做了一次深度的闭门分享，分享的内容直击行业痛点，首先从他最专业的AI Infra开始，详细分析了AI时代的Infra，到底有什么新的特点；然后，基于AI大模型的特点，帮助企业算了一笔比较详细的经济账——在不可能三角成本、效率、效果中，如何选才能达到比较好的平衡点。</p><p></p><p>最后也讨论到AI整个产业链的增量机会及目前大模型商业模式的纠结点：</p><p></p><p>“每次训练一个基础大模型，都要从零开始。形象一点来描述，这次训练‘投进去10个亿，下次还要再追加投10个亿’，而模型迭代速度快，可以赚钱的窗口也许只有大概一年。所以每个人都在思考这个终极问题，‘大模型的商业模式到底怎样才能真正有效？’”</p><p></p><p>贾扬清的过往经验大部分是TOB的。他也多次在分享中很坦诚地表示，“TOC我看不太清楚，TOB看得更清晰一些。”</p><p></p><p>“AI从实验室或者说从象牙塔出来到应用的过程中，该蹚过的雷，都会经历一遍。”无论大语言模型给人们多少惊艳，它的发展都不是空中楼阁，既往的经验和范式有变也有不变。</p><p></p><p>为了方便阅读，我们在文首提炼几个主要观点，但强烈建议完整阅读，以了解贾扬清完整的思考逻辑：</p><p></p><p></p><p></p><blockquote>一个通用的大模型的效果固然非常好，但是在企业实际应用当中，中小型模型加上自己的数据，可能反而能够达到一个更好的性价比。至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。我个人认为，英伟达在接下来的3~5年当中，还会是整个AI硬件提供商中绝对的领头羊，我认为它的市场发展占有率不会低于80%。但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：一个是提效，另外一个是娱乐。大量的传统行业应用，其实是AI行业里值得探究的深水区。我个人关于Supper App的观点可能稍微保守一些，也有可能是因为我自己的经历很多都在做TOB的服务，我认为Super APP会有，但是会很少。</blockquote><p></p><p></p><p>一个通用的大模型的效果固然非常好，但是在企业实际应用当中，中小型模型加上自己的数据，可能反而能够达到一个更好的性价比。</p><p></p><p>至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。</p><p></p><p>我个人认为，英伟达在接下来的3~5年当中，还会是整个AI硬件提供商中绝对的领头羊，我认为它的市场发展占有率不会低于80%。但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。</p><p></p><p>目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：一个是提效，另外一个是娱乐。</p><p></p><p>大量的传统行业应用，其实是AI行业里值得探究的深水区。</p><p></p><p>我个人关于Supper App的观点可能稍微保守一些，也有可能是因为我自己的经历很多都在做TOB的服务，我认为Super APP会有，但是会很少。</p><p></p><p></p><p>以下为分享内容精华整理：</p><p></p><p>随着大型语言模型的兴起，出现了一个新概念——Scaling Law（规模定律）。根据Scaling Law，大语言模型的性能与其参数量、训练数据的大小和计算量呈幂律关系。简单来说，用通用的方法给模型巨大的数据，让模型能够拥有输出我们想要的结果的能力。</p><p></p><p>这就使得AI计算与“云计算”有很大的不同，云计算主要服务于互联网时代的需求，关注资源的池化和虚拟化：</p><p></p><p>●&nbsp;怎么把计算，存储，网络，从物理资源变成虚拟的概念，“批发转零售”；</p><p></p><p>●&nbsp;如何在这种虚拟环境下把利用率做上去，或者说超卖；</p><p></p><p>●&nbsp;怎么更加容易地部署软件，做复杂软件的免运维（比如说，容灾、高可用）等等，不一而足。</p><p></p><p>用比较通俗的语言来解释，互联网的主要需求是处理各种网页、图片、视频等，分发给用户，让“数据流转（Moving Data Around）起来。云服务关注数据处理的弹性，和便捷性。</p><p></p><p>但是AI计算更关注以下几点：</p><p></p><p>●&nbsp;并不要求特别强的虚拟化。一般训练会“独占”物理机，除了简单的例如建立虚拟网络并且转发包之外，并没有太强的虚拟化需求。</p><p>●&nbsp;需要很高性能和带宽的存储和网络。例如，网络经常需要几百 G 以上的 RDMA 带宽连接，而不是常见的云服务器几 G 到几十 G 的带宽。</p><p>●&nbsp;对于高可用并没有很强的要求，因为本身很多离线计算的任务，不涉及到容灾等问题。</p><p>●&nbsp;没有过度复杂的调度和机器级别的容灾。因为机器本身的故障率并不很高（否则 GPU 运维团队就该去看了），同时训练本身经常以分钟级别来做 checkpointing，在有故障的时候可以重启整个任务从前一个 checkpoint 恢复。</p><p></p><p>今天的AI计算 ，性能和规模是第一位的，传统云服务所涉及到的能力，是第二位的。</p><p></p><p>这其实很像传统高性能计算领域的需求，在七八十年代我们就已经拥有超级计算机，他们体积庞大，能够提供大量的计算能力，可以完成气象模拟等服务。</p><p></p><p>我们曾做过一个简单的估算：过去，训练一个典型的图像识别模型大约需要1 ExaFlop的计算能力。为了形象地描述这一计算量，可以想象全北京的所有人每秒钟进行一次加减乘除运算，即便如此，也需要几千年的时间才能完成一个模型的训练。</p><p></p><p>那么，如果单台GPU不足以满足需求，我们应该如何应对呢？答案是可以将多台GPU连接起来，构建一个类似于英伟达的Super POD。这种架构与最早的高性能计算机非常相似。</p><p></p><p>这时候，如果一台GPU不够怎么办？可以把一堆GPU连起来，做成一个类似于英伟达的Super POD，它和最早的高性能计算机长得很像。</p><p></p><p>这就意味着，我们又从“数据流转”的需求，回归到了“巨量运算”的需求，只是现在的“巨量运算”有两个进步，一是用于计算的GPU性能更高，另外就是软件更易用。伴随着AI的发展，这将是一个逐渐加速的过程。今年NVIDIA推出的新的DGX机柜，一个就是几乎1Exaflops per second，也就是说理论上一秒的算力就可以结束训练。</p><p></p><p>去年我和几位同事一起创办了Lepton AI。Lepton在物理中是“轻子”的意思。我们都有云计算行业的经验，认为目前AI的发展给“云”带来一个完全转型的机会。所以今天我想重点分享一下，在AI的时代，我们应该如何重新思考云的Infrastructure。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/49/498853e30a6c2505eec1dba0d5c31dba.webp" /></p><p></p><p></p><p>企业用大模型，先算一笔“经济账”</p><p></p><p></p><p>随着模型规模的不断扩大，我们面临着一个核心问题：大模型所需的计算资源成本高昂，从实际应用的角度出发，我们需要思考如何高效地利用这些模型。</p><p></p><p>以一个应用场景为例，我们可以比较形象地看出一个通用的大型语言模型与针对特定领域经过微调的模型之间的差异。</p><p></p><p>我们曾经尝试过“训练一个金融领域的对话机器人”。</p><p></p><p>使用通用模型，我们直接提问：“苹果公司最近的财报怎么样？你怎么看苹果公司在AI领域的投入。”通用大模型的回答是：“抱歉，我无法回答这个问题。”</p><p></p><p>针对特定领域微调，我们使用了一个7B的开源模型，让它针对性地“学习”北美所有上市公司的财报，然后问它同样的问题。它的回答是：“没问题，感谢您的提问。（Sure，thanks for the question）”口吻十分像一家上市公司的CFO。</p><p></p><p>这个例子其实可以比较明显地看出，通用大模型性能固然很出色，但是在实际应用中，使用中小型开源模型，并用特定数据微调，最终达到的效果可能更好。</p><p></p><p>至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/56/560ad0fe223d6015f2a3e0154b7bd70b.webp" /></p><p></p><p></p><p>如上图所示，以Llama2 7B开源模型为例，100万token的成本大约为0.1美元-0.3美元。使用一台英伟达A10GPU服务器就能支持训练，以峰值速度2500token每秒来计算，一小时的成本大约为0.6美元。自有这台服务器，一年的成本大约为5256美元，并不算高。</p><p></p><p>如果用闭源模型，100万token消耗速度很快，成本远高于0.6美元每小时。</p><p></p><p>不过成本消耗也要考虑应用的种类和模型的输出速度，模型输出速度越快，成本也会越高。如果可以有mini-batch（小批量数据集）等，同时来跑，它的整体性能就会更好，但是单个的输出性能可能就会稍微差一点。</p><p></p><p>这就引出另外一个问题，大模型的输出速度，怎样比较合适？</p><p></p><p>以Chatbot举例，人说话的速度大概为120词每分钟，成人阅读的速度大概为350词左右，反向计算token，每秒钟20个token左右，就能达到比较好的体验。如果这样计算的话，如果应用的流量够大，跑起来成本是不高的。</p><p></p><p>但是，究竟流量能不能达到“够大”，这就变成了“鸡生蛋、蛋生鸡”的问题。我们发现了一个很实用的模式可以解决这个问题。</p><p></p><p>在北美，很多企业都是先用闭源大模型来做实验（比如OpenAI的模型）。实验规模大概在几百个million（百万token），成本大概为几千美元。一旦数据飞轮运转起来，再把已有数据存下来，用较小的开源模型微调自己的模型。现在这已经变成了相对比较标准的模式。</p><p></p><p>在考虑AI模型的时候，各家企业其实都在各种取舍中找平衡。在北美经常讲一个不可能三角，当你买一辆车的时候跑得快、便宜和质量好，这三者是不可兼得的。</p><p></p><p>上文提到的标准模式，其实就是首先追求质量，然后再考虑成本，如果想同时满足这三方面，基本是不可能的。</p><p></p><p>半年之前我非常强烈地相信开源模型能非常迅速追赶上闭源模型，然而半年之后，我认为开源模型和闭源模型之间会继续保持一个非常合理的差距，这个差距用比较形象的具体模型举例来说，闭源模型到GPT-4水平的时候，开源模型可能在GPT3.5左右。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be13b28f126280485bf393899d31fab8.webp" /></p><p></p><p></p><p>硬件行业的新机会</p><p></p><p></p><p>早在2000年初，英伟达就看到了高性能计算的潜力，于是2004年他们做了CUDA，到今天为止已经20年。今天CUDA已经成为AI框架和AI软件最底层的标准语言。</p><p></p><p>早期，行业内都认为高性能计算写起来很不方便，英伟达介绍了CUDA，并说服大家它简单易用，让大家尝试来写。试用之后，大家发现确实易用且写出来的高性能计算速度很快，后来几乎各大公司的研究员们都把自己的AI框架基于CUDA写了一遍。</p><p></p><p>CUDA很早就和AI社区建立了很好的关系，其它公司也看到了这个市场的巨大机会，但是从用户侧来看，大家用其它产品的动机不强。</p><p></p><p>所以市场上还会有一个关注焦点，那就是是否有人能够撼动英伟达的地位，除了英伟达，新的硬件提供商还有谁可能有机会？</p><p></p><p>首先我的观点不构成投资建议，我个人认为英伟达在接下来的3~5年当中，依然还会是AI硬件提供商中绝对的领头羊，它的市场占有率不会低于80%。</p><p></p><p>但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。前十年中，在AI领域大家都在纠结的一个问题，虽然很多公司能够提供兼容CUDA的适配，但是这一层“很脆”。“很脆”的意思是模型多种多样，所以适配层容易出问题，整个工作链就会断。</p><p></p><p>今天越来越少的人需要写最底层的模型，越来越多的需求是微调开源模型。能够跑Llama、能够跑 Mistral，就能满足大概80%的需求，每一个Corner Case（特殊情况）都需要适配的需求逐渐变少，覆盖几个大的用例就可以了。</p><p></p><p>其它硬件提供商的软件层在努力兼容CUDA，虽然还是很难，但是今天抢占一定市场占有率，不再是一件不可能的事情；另外云服务商也想分散一下投资。所以这是我们看到的一个很有意思的机会点，也是cloud infra在不断变化的过程。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed4e13279c43f403bf3e40ec0b1c0b90.webp" /></p><p></p><p></p><p>生成式AI浪潮：哪些是增量机遇？</p><p></p><p></p><p>我们再看一下AI应用的情况。今天我们可以看到AI应用的供给在不断增加。从Hugging Face来看，2022年8月模型数量大概只有6万，到2023年9月，数量就已经涨了5倍，增速是非常快的。</p><p></p><p>目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：</p><p></p><p>第一大类是提效（productivity）。例如在电商行业，用AIGC的方式更快生成商品展示图片。例如Flair AI，应用场景举例来说，我希望能给瓶装水拍摄一个广告图片，仅仅需要把水放在方便的地方，拍一张照片。然后把这张照片发送给大模型，告诉它，我希望它被放在有皑皑白雪的高山上，背景是蓝天白云。它就能生成一个直接可以上传电商平台，作为产品展示的图片。</p><p></p><p>其它类型也有很多，比如在企业海量知识库做搜索且有更好的交互功能，例如Glean。</p><p></p><p>第二大类是娱乐（entertainment），比如Soul，以AI的方式做角色扮演及交互。</p><p></p><p>另外我们还发现一个趋势是“套壳APP”越来越少了。其实大家发现直接“套壳”通用大模型的产品会有一个通病，交互效果特别“机器人”。</p><p></p><p>反而是7B、13B的稍小模型，性价比和可调性都特别好。做个直观的比喻：大模型就好像是“读博士”读轴了，反而是本科生的实操性更强。</p><p></p><p>做应用层，总结来讲有两条路径：第一条是训练自己的基础大模型，或者是自己去微调模型。</p><p></p><p>另外就是有自己非常垂直领域的应用，背后是很深的场景，直接用Prompt是不可行的。</p><p></p><p>比如医疗领域，用户提需求问：“我昨天做的化验结果怎么样？”这其实需要背后有个大模型，除了对化验指标做出专业的分析，还需要给用户提出饮食等建议。</p><p></p><p>这背后涉及到化验、保健、保险等产业链的多个细分场景，需要医疗产业链很深的经验。需要在既有的经验上加一层AI能力来做好用户体验，这是我们今天发现的比较有持续性的AI应用模式。</p><p></p><p>关于未来到底怎样，预测未来是最难的。我的经验一直是B端，逻辑主要看供需。AI带来的增量需求首先是高性能的算力。第二个是高质量的模型，以及上层需要的适合这些高性能、高质量和高稳定性需求的计算的软件层。</p><p></p><p>所以我觉得从高性能算力来看，英伟达显然已经成为赢家。另外这个市场可能会容纳2~3家比较好的芯片提供商。</p><p></p><p>从模型来看，OpenAI肯定是一个已经比较确定的赢家，市场足够大，应该能够容纳3-5家不同的模型生产厂商，而且它很有可能还会出现偏地域性的分布。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d96641a722069da64fa2548458168434.webp" /></p><p></p><p></p><p>传统行业的AI深水区</p><p></p><p></p><p>我还想讲的是大量的传统行业应用，这其实是AI行业里值得探讨的深水区。</p><p></p><p>大语言模型出现，大家曾经一度觉得OpenAI弄了一个特别厉害的大模型，写点Prompt就能搞定任何事情。</p><p></p><p>但是Google早在世纪之初就写过一篇文章，到今天我仍然觉得这个观点是对的。这篇文章说，机器学习模式只是整个AI链路中非常小的一部分，外面还有大量的工作，在今天来说也会变得越来越重要。比如如何收集数据、如何保证数据和我们的应用需求一致，如何来做适配，等等。</p><p></p><p>模型上线之后还有三件事：第一是跑的稳定，第二个是能够把结果质量等都持续稳定地控制起来，以及还有非常重要的一点是把应用当中所得到的数据，以一种回流的方式收集回来，训练下一波更好的模型。</p><p></p><p>到今天这个方法论依然适用，就是在行业竞争中，谁能有数据，谁能够把用户的反馈更好地调试成“下一波训练的时候可以更好的应用”的数据，这也是核心竞争力之一。</p><p></p><p>今天大家都有这样一种感觉，大模型的结构相差不大，但是数据和工程能力的细节才是决定模型之间差别的地方，OpenAI其实持续在给我们证明这件事。</p><p></p><p>今天我们看整个技术栈的架构是什么样子的，a16z给了我们一个非常好的总结（如下图）：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b4b9fd749082d737bb1007ba0ba2a482.webp" /></p><p></p><p></p><p></p><p>IaaS这一层基本上是英伟达做“老大”，其它公司在竞争硬件和云平台，这是最下层的坚实基础。</p><p></p><p>云平台今天也在发生不断的变化，大家最近可能在技术趋势上听到一个词叫做“下云”，以前大家肯定听说过“全栈上云”。</p><p></p><p>为什么会出现“我要下云”的思潮？就是因为算力本身是巨大的成本，而且又是可以“自成一体”的成本，所以行业内开始把传统的云成本和今天AI算力的成本分开来考虑。</p><p></p><p>今天越来越多的PaaS开始变成Foundation Model，有些是闭源的，有些是开源的，然后在上面再做一层APP。今天每一层都竞争激烈。但是我个人感觉在模型这一层以及往上的上层应用这一层，是最活跃的。</p><p></p><p>模型层主要是开源和闭源之争。</p><p></p><p>应用层有两个趋势：一个是模型在努力往上做应用；另外就是是应用层在拼命想理解模型到底能有什么能力，然后把自己的应用加上AI，让自己的应用更强大。</p><p></p><p>我个人认为，模型往上做应用有点难，应用把自己的AI能力加进来更有希望。</p><p></p><p>国内还有种说法叫做Super APP（超级应用），Super APP很重要的一点是需要“端到端把问题解决”。a16z在他的图上也描述会有一些端到端的APP出来，本质上需要模型的推理和规划的能力做的非常好。ChatGPT就是端到端全部打通，模型也是自己的，应用也是自己的，这是Super App的状态。</p><p></p><p>但是我个人关于Super App的观点可能稍微保守一些，也有可能是因为我自己的经历很多时候都在做TOB的服务，我个人的感觉是Super APP会有，但是会很少。</p><p></p><p>我个人的感觉是，B端的应用越来越多的还是会以一种像搭积木一样，用开源的模型结合企业自己的数据，把企业自己的应用搭起来的一个过程。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/43/4382db6cb575e0915a3c721355de3872.webp" /></p><p></p><p></p><p>大模型的商业模式：</p><p>两个纠结和一个市场现象</p><p></p><p></p><p>但是在大模型进行商业化落地的过程中，我观察到市场还是会有两个纠结：</p><p></p><p>第一个纠结是营收的流向和以往不太一样，不太对。</p><p></p><p>正常商业模式的流向应该是：从用户那里收费，然后“留成本”给硬件服务商，比如英伟达。但是今天是横向的，从VC（风投）拿到融资，直接“留钱”给硬件厂商。但是VC的钱本质是投资，创业者最后可能要10倍还给VC，所以这个资金流向是第一个纠结。</p><p></p><p>第二个纠结是今天的大模型对比传统软件，可以创造营收的时间太短。</p><p></p><p>其实开发一次软件之后，可以收回成本的时间比较长。比如像Windows，虽然过几年迭代一代，但是它底层的很多代码是不用重写的。所以一个软件被写完，可能在接下来的5-10年当中，它给我时间窗口持续迭代。而且投入的成本大部分是程序员的成本。</p><p></p><p>但是大模型的特点是，每次训练过一个模型之后，下一次还是要从零开始重新训练。比较形象一点来说“今天投入10个亿，再迭代的时候，又得再追加投入十个亿”。</p><p></p><p>但是模型的迭代速度又很快，中间能够赚钱的时间窗口究竟有多长？今天看起来好像大概是一年左右，甚至更短。</p><p></p><p>于是大家就开始质疑，大模型的成本远高于传统的软件，但是做完一个模型之后，能赚钱的时间远低于传统的软件。</p><p></p><p>所以就回到了这个终极问题，大模型的商业模式到底怎样才能真正有效？</p><p></p><p>我还观察到一个市场现象，去年整个市场都非常痛苦，硬件需求的突然暴涨，整个供应链都没反应过来，等待时间很长，甚至可能6个月以上。</p><p></p><p>最近我们观察到的一个现象是供应链没有那么紧张了。第一是全球供应链也开始缓过来；第二我个人判断有一部分以前因为焦虑而提前囤货的供应商，觉得现在要开始收回成本了。之前供不应求的紧张状态会逐渐变好，但是也不会一下子变成所有人都愁卖的状态。</p><p></p><p>以上就是我基于这波生成式AI爆发，对整个AI产业造成的影响的个人观察。也正是在这个浪潮中，Lepton正在持续帮助企业和团队在生成式AI落地的过程中找到成本、效果、效率的最佳均衡点。最后，其实可以以Richard S. Sutton——增强学习领域开山立派的一位导师，在2019年说的一句话作为总结，“在整个70年的AI科研中，最重要的经验就是，通过一个通用的方法（今天是深度学习），来利用大量的计算模型（今天是以英伟达为代表的异构GPU为基础的高性能计算），这样的方式是整个70年AI发展中最有效、最简单的方式。”</p><p></p><p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.</p><p></p><p>——Richard Sutton: "The Bitter Lesson"</p><p></p><p>文字经贾扬清本人确认，感谢高山书院（公众号：gasadaxue）对本文的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</id>
            <title>“真男人就应该用 C 编程”！用 1000 行 C 代码手搓了一个大模型，Mac 即可运行，特斯拉前AI总监爆火科普 LLM</title>
            <link>https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 09:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: C语言, GPT-2, Andrej Karpathy, 大模型训练
<br>
<br>
总结: Andrej Karpathy使用C语言徒手实现了GPT-2大模型训练，减少了对庞大外部库的依赖，使得模型训练更轻量化和高效。他的代码开源后获得了广泛关注，展示了用C语言实现大型语言模型训练的可能性。Karpathy的工作为学习和理解大型语言模型提供了重要资源。 </div>
                        <hr>
                    
                    <p></p><blockquote>徒手用 1000 行 C 语言实现，不依赖庞大的外部库，Mac 即可运行。</blockquote><p></p><p></p><p>如今这年头，徒手写神经网络代码已经不算事儿了，现在流行手搓大模型训练代码了！这不，今天，特斯拉前AI总监、OpenAI 创始团队成员Andrej Karpathy仅用1000行简洁的C代码，就完成了 GPT-2 大模型训练过程。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18b9f758c3320976c307f166074c9f63.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>几个小时前，Andrej Karpathy推出了一个名为llm.c的项目，旨在用纯C语言训练LLM，这种方法的主要优势在于它显著减少了依赖库的体积——不再需要245MB的PyTorch和107MB的cPython，这样可以使得模型训练过程更为轻量化和高效。该项目还可以立即编译和运行，并且与PyTorch的参考实现完全匹配。</p><p>&nbsp;</p><p>Karpathy表示他之所以选择GPT-2作为首个工作示例，是因为它大语言模型鼻祖的定位，亦属现代AI堆栈的首次组合。因此，选择 GPT-2 作为起点，可以让我们更容易地理解和实践大型语言模型训练。</p><p>&nbsp;</p><p>徒手实现GPT-2后，Karpathy将这份代码放到了GitHub上，以MIT协议开源。短短几个小时，就超过了2500颗星，并且数据还在不断持续上涨......</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70dfc5db0012180f5518d691c2d9b896.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>将大模型原理拆解得无比简单</h2><p></p><p>&nbsp;</p><p>Andrej Karpathy 是全球人工智能领域的知名科学家，也是 OpenAI 的创始成员和研究科学家。</p><p>&nbsp;</p><p>他于2009年本科毕业于多伦多大学，获得计算机科学和物理学学士学位。2011年硕士毕业于不列颠哥伦比亚大学，随后前往斯坦福大学AI Lab（SAIL）读博，师从著名学者李飞飞，是全球最早将深度学习应用于计算机视觉研究的学者之一。</p><p>&nbsp;</p><p>在求学期间，Andrej Karpathy曾在谷歌和DeepMind实习，后来在OpenAI刚刚成立时加入并担任研究科学家。直到2017年6月，他被马斯克挖去，担任特斯拉人工智能部门主管，直接向马斯克汇报。在特斯拉工作的五年里，他主导了特斯拉自动辅助驾驶系统Autopilot的开发。这项技术对于特斯拉的完全自动驾驶系统 FSD 至关重要，也是马斯克针对 Model S、Cybertruck 等车型推销的主要卖点。在各大新闻中，他被誉为“特斯拉的秘密武器”。</p><p>&nbsp;</p><p>去年Karpathy曾短暂回到OpenAI，然后又在OpenAI众人忙于内斗时抽空录制了一个长达一小时的教学视频《大型语言模型入门》。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/097e6065fd638962c0ee82bd46dc224e.jpeg" /></p><p></p><p>&nbsp;</p><p>Karpathy 在视频中首先介绍了一些 LLM 入门知识，然后以 Meta 推出的开源大模型 Llama 2-70b 为例进行了讲解。该模型有 700 亿参数，主要包含两个文件，分别是参数文件，文件大小为 140GB，以及运行这些参数的代码，以 C 语言为例需要约 500 行代码。</p><p>&nbsp;</p><p>Karpathy 表示只要有这两个文件再加上一台 MacBook，我们就可以构建一个独立的系统，无需联网或其他设施。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffefdd6449d6362b2837474fe5b4555d.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>大模型训练，可以理解为是对互联网数据进行有损压缩，一般需要一个巨大的GPU集群来完成。以Llama 2-70b为例的话，就是使用了类似网络爬取的约 10TB 的文本，用6000 个 GPU ，耗资 200 万美元，训练约 12 天，最后获得基础模型。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/2072a2e96c9acf4978fb523baf6102c3.jpeg" /></p><p></p><p>&nbsp;</p><p>基础模型即上图里140GB的“压缩文件”（压缩率约100倍），就等于靠这些数据对世界形成了理解，那它就可以进行“预测”工作了。</p><p>&nbsp;</p><p>Karpathy之前还分享过他的学习经验，就是开始时要尝试从0开始，写一些原生代码，帮助理解消化知识点。也就是说，徒手实现代码才是最有效的学习方式。</p><p>&nbsp;</p><p>两年前，Karpathy就曾基于 PyTorch，仅用 300 行左右的代码就写出了一个小型 GPT 训练库，并将其命名为 minGPT，用这份代码揭开了GPT神秘的面纱。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8ef635be577ae58cf8fe6012e2a3544b.jpeg" /></p><p></p><p>截图来源：<a href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a>"</p><p>&nbsp;</p><p>因为大多数 GPT 模型的实现都过于庞大，而minGPT 做到了小、干净、可解释和具有教育意义，所以Karpathy的这300行代码是学习 GPT 的最佳资源之一，可以用来深入理解GPT 是如何工作的。</p><p>&nbsp;</p><p></p><h2>用C语言实现LLM</h2><p></p><p>&nbsp;</p><p>这次，Andrej Karpathy单纯通过C/CUDA实现大语言模型训练，且无需245 MB PyTorch或107 MB cPython。例如，训练GPT-2（CPU，fp32单精度）需要在单个文件中使用约1000行简洁代码，可立即编译并运行、且与PyTorch参考实现完全匹配。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7bea147ef70c91506cb2980bc256e088.jpeg" /></p><p></p><p>&nbsp;</p><p>从某种意义上说，Karpathy确实在尝试重新设计LLM的架构。他通过llm.c项目探索一种更简单、更高效的训练LLM方法。与现有LLM架构相比，这种新架构的主要亮点包括：</p><p>&nbsp;</p><p>1. 代码简洁性：仅使用约1000行代码就能完成GPT-2模型的训练，相比之下显著降低了复杂度。</p><p>2. 独立性：不依赖庞大的外部库如PyTorch或cPython，使得部署和运行更加轻便快捷。</p><p>3. 高效性：直接使用C/CUDA进行编程有望提高计算效率和训练速度。</p><p>&nbsp;</p><p>有网友问Karpathy为何不用Rust，Karpathy回复说，“我完全理解Rust的吸引力。然而，我仍然觉得 C 语言非常棒。它简单、干净、可移植，在审美上也十分优美。使用 C 语言就像直接与机器交流一样。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b291c68429d3d5ecc9ef997d9f4c4028.jpeg" /></p><p></p><p>&nbsp;</p><p>这种语言选择也让网友们纷纷感叹：</p><p>&nbsp;</p><p>“我们正在掀起一场 C 语言复兴！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/1854da08a50f68f4f2b7ff0cdc84b81a.jpeg" /></p><p></p><p>&nbsp;</p><p>“真男人就应该用 C 语言编程。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a745a6ba051f49d4a1ad4d83255f0c72.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Karpathy以更简单、更原始的C/CUDA架构来做LLM的训练，其中还涉及算法优化、计算资源管理等多个方面。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd4ec03d1dab994cae359ca1da6a1585.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>你会看到，项目在开始时一次性分配所有所需的内存，这些内存是一大块 1D 内存。然后在训练过程中，不会创建或销毁任何内存，因此内存占用量保持不变，并且只是动态的，将数据批次流过。这里的关键在于手动实现所有单个层的前向和后向传递，然后将它们串联在一起。例如，这里是 layernorm 前向和后向传递。除了 layernorm 之外，我们还需要编码器、matmul、自注意力、gelu、残差、softmax 和交叉熵损失。</blockquote><p></p><p>&nbsp;</p><p>“一旦你拥有了所有的层，接下来的工作只是将它们串在一起。讲道理，写起来相当乏味和自虐，因为你必须确保所有指针和张量偏移都正确排列， ”Karpathy 表示。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d05cd843fcd5a4e542a86f9de979633e.png" /></p><p></p><p>&nbsp;</p><p>另外Karpathy还在doc/layernorm/layernorm.md中附上了短小的使用教程。里面是一份简单的分步指南，用于实现GPT-2模型中的单一层，即layernorm层，希望能成为大家理解在C中实现大语言模型各层的理想起点。</p><p>&nbsp;</p><p>更重要的是，他还用自己的MacBook Pro（苹果M3 Max芯片）演示了整个训练过程，对照他之前的大模型入门教程，就可以轻松了解如今炙手可热的LLM是怎么一回事儿了。</p><p>&nbsp;</p><p></p><h4>训练过程：</h4><p></p><p>&nbsp;</p><p>首先下载数据集并token化。</p><p>&nbsp;</p><p>python prepro_tinyshakespeare.py</p><p>&nbsp;</p><p>输出结果为：</p><p>&nbsp;</p><p>Saved 32768 tokens to data/tiny_shakespeare_val.bin</p><p>Saved 305260 tokens to data/tiny_shakespeare_train.bin</p><p>&nbsp;</p><p>其中各.bin文件为int32数字的原始字节流，用于指示GPT-2 token化器的token id。或者也可以使用prepro_tinystories.py对TinyStories数据集进行标注。</p><p>&nbsp;</p><p>原则上，到这里就已经可以开始训练模型。为提高效率，可以使用OpenAI发布的GPT-2权重进行初始化，而后加以微调。为此需要下载GPT-2权重并将其保存为可在C中加载的检查点：</p><p>&nbsp;</p><p>python train_gpt2.py</p><p>&nbsp;</p><p>该脚本会下载GPT-2（124M）模型，对单批数据进行10次过拟合迭代，运行多个生成步骤，最重要的是保存两个文件：1）gpt2_124M.bin文件，包含用于在C中加载的模型权重；2）以及gpt2_124M_debug_state.bin，包含包括input、target、logits及loss等更多调试状态，对于调试C代码、单元测试及确保能够与PyTorch参考实现完全匹配非常重要。现在我们可以使用这些权重进行初始化并在原始C代码中进行训练。首先编译代码：</p><p>&nbsp;</p><p>make train_gpt2</p><p>&nbsp;</p><p>在train_gpt2编译完成后即可运行：</p><p>&nbsp;</p><p>OMP_NUM_THREADS=8 ./train_gpt2</p><p>&nbsp;</p><p>大家应根据CPU的核心数量来调整线程数量。该程序将加载模型权重、tokens，并使用Adam lr 1e-4运行数次迭代的微调循环，而后由模型生成样本。简单来讲，所有层都具有前向及后向传递实现，串联在一起形成统一的大型、手动前向/后向/更新循环。在MacBook Pro（苹果M3 Max芯片）上的输出结果如下所示：</p><p>&nbsp;</p><p>[GPT-2]</p><p>max_seq_len: 1024</p><p>vocab_size: 50257</p><p>num_layers: 12</p><p>num_heads: 12</p><p>channels: 768</p><p>num_parameters: 124439808</p><p>train dataset num_batches: 1192</p><p>val dataset num_batches: 128</p><p>num_activations: 73323776</p><p>val loss 5.252026</p><p>step 0: train loss 5.356189 (took 1452.121000 ms)</p><p>step 1: train loss 4.301069 (took 1288.673000 ms)</p><p>step 2: train loss 4.623322 (took 1369.394000 ms)</p><p>step 3: train loss 4.600470 (took 1290.761000 ms)</p><p>... (trunctated) ...</p><p>step 39: train loss 3.970751 (took 1323.779000 ms)</p><p>val loss 4.107781</p><p>generated: 50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323</p><p>step 40: train loss 4.377757 (took 1366.368000 ms)</p><p>&nbsp;</p><p>现在的生成结果仅给出token ids，需要将其解码回文本形式：</p><p>&nbsp;</p><p>&lt;|endoftext|&gt;Come Running Away,</p><p>Greater conquer</p><p>With the Imperial blood</p><p>the heaviest host of the gods</p><p>into this wondrous world beyond.</p><p>I will not back thee, for how sweet after birth</p><p>Netflix against repounder,</p><p>will not</p><p>flourish against the earlocks of</p><p>Allay</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/karpathy/status/1777427947126936026">https://twitter.com/karpathy/status/1777427947126936026</a>"</p><p><a href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a>"</p><p><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">https://www.youtube.com/watch?v=zjkBMFhNj_g</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PG1Og0E8kCOBqM1R04Xw</id>
            <title>WPS AI企业版来了，MiniMax、智谱AI、文心一言等多个大模型自由切换调用</title>
            <link>https://www.infoq.cn/article/PG1Og0E8kCOBqM1R04Xw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PG1Og0E8kCOBqM1R04Xw</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 09:48:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金山办公, WPS 365, AI办公, 企业版
<br>
<br>
总结: 金山办公举行生产力大会发布了WPS 365平台，整合了文档、AI、协作三大能力，提升企业办公效率。WPS 365全新升级开启企业AI办公时代，满足组织日常办公需求，实现一站式AI办公。WPS AI企业版降低用户使用大模型门槛，提供智能文档库和企业智慧助理，助力企业提升生产力。 </div>
                        <hr>
                    
                    <p>4月9日，金山办公生产力大会在京举行，现场发布了面向组织和企业的办公新质生产力平台WPS 365，其包含升级的WPS Office、最新发布的WPS AI企业版和WPS协作。WPS 365打通了文档、AI、协作三大能力，让各组件间无缝切换，用户使用一个工具就能调用各类主流大模型，一个界面就能边写边沟通边开会，一个产品就能高效完成所有工作。</p><p>&nbsp;</p><p>金山办公CEO章庆元表示：“金山办公秉承技术立业、用户第一的理念，在过去36年里紧跟时代脉搏，迭代公司产品战略，持续将时代最新的技术，转换成最务实的办公产品，用可控的成本，交付给客户。”</p><p></p><h2>WPS 365全新升级，开启企业AI办公时代</h2><p></p><p>&nbsp;</p><p>数据显示，2020年金山办公云端文档数是898亿份，到2023年底已达到2174亿份，增长了142%。金山办公作为国内头部办公软件公司之一，金山办公文档功能在市场中应用广泛，此次WPS 365的全新升级，进一步打通了AI与协作的能力。</p><p>&nbsp;</p><p>章庆元表示，WPS 365全面覆盖了一个组织日常办公的基本需求，从文档创作到即时通讯（IM）、会议、邮件，再到AI应用，标志着一个文档处理套件正式升级为一站式AI办公，让企业生产力即刻起飞。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>发布会现场演示了本地文档一键切换出在线协作功能，变成协作文档，并邀请他人共同参与编辑。协作完成后又可以选择关闭，协作文档恢复为本地文档，实现无缝切换。最新版WPS Office里的各类工具也均内置了具备大语言模型能力的人工智能应用WPS AI，为用户提供诸如扩写、缩写、总结、生成公式等功能。WPS协作则进一步让组织用户间的沟通交流更便捷，用户可以边写⽂档边沟通，即使是在邮件、OA系统里也能够无缝衔接。</p><p>&nbsp;</p><p>基于「WPS Office+WPS协作+WPS AI」的模式，意味着用户只需要WPS 365一个产品就能高效完成所有工作，免去了办公场景下的繁琐切换，实现办公新质生产力的切实落地。</p><p></p><h2>WPS AI企业版发布，降低用户使用大模型的门槛</h2><p></p><p>&nbsp;</p><p>金山办公副总裁王冬介绍说，与个人版不同之处在于，WPS AI企业版聚焦为客户打造企业大脑，它分为AI Hub（智能基座）、AI Docs（智能文档库）、Copilot Pro（企业智慧助理）三个部分。</p><p>&nbsp;</p><p>AI Hub集成了国内主流大模型的AI能力，例如MiniMax、智谱AI、文心一言、商汤日日新、通义千问等等。金山办公在大模型领域的定位是应用方，与国内上百种模型进行了适配磨合，实现基础AI服务的开箱即用，这也让用户克服了大模型选择困难症并极大降低使用门槛。</p><p>&nbsp;</p><p>AI Docs则将传统云文档库一键升级为智能文档库，让智能创作来源有依据，完整的文档权限体系保障信息不越权。</p><p>&nbsp;</p><p>Copilot Pro则可帮助运营人员使用自然语言驱动BI产品分析数据，并可调用WPS 365 API和企业自有API，解决办公自动化需求。</p><p>&nbsp;</p><p>区别于市面上仅仅做内容生成的AI产品，WPS AI企业版不仅能读书认字，还能自助化分析数据、减少人工的重复劳动，提供“文理兼修”的数字员工服务，例如阅读助手、画图助手、考勤助手、销售分析、合同分析等等，触达各类细微的办公场景。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Lf7ZJ7qNY03bBw6kF9Tr</id>
            <title>没有数据训练大模型？OpenAI 总裁带队转录YouTube视频，谷歌、Meta 也想尽数据收割套路</title>
            <link>https://www.infoq.cn/article/Lf7ZJ7qNY03bBw6kF9Tr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Lf7ZJ7qNY03bBw6kF9Tr</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 06:58:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, YouTube视频, 数据收割, AI模型
<br>
<br>
总结: OpenAI 面临数据供应荒，开发了语音识别工具 Whisper 转录 YouTube 视频以训练 AI 模型。谷歌、Meta 也在收集数据用于 AI 训练，涉及版权问题。科技企业迫切需要数据，甚至开始使用合成信息。创作者作品成为主要训练素材，引发版权诉讼。AI 模型获取内容引发版权争议，版权法也在适应 AI 时代。 </div>
                        <hr>
                    
                    <h1>没有数据训练大模型？OpenAI 总裁带队转录YouTube视频，谷歌、Meta 也想尽数据收割套路</h1><section><section><img src="https://static001.geekbang.org/wechat/images/ba/ba0d76f60bd041eb7454af2109874966.jpeg" /></section><section><span>作者&nbsp;|&nbsp;Cade&nbsp;Metz,&nbsp;Cecilia&nbsp;Kang,</span></section><section><span>&nbsp;Sheera&nbsp;Frenkel,</span><span>Stuart&nbsp;A.&nbsp;Thompson&nbsp;and&nbsp;Nico&nbsp;Grant</span></section><section><span>译者&nbsp;|&nbsp;核子可乐</span></section><section><span>策划&nbsp;|&nbsp;褚杏娟</span></section><p>2021 年底，OpenAI 开始面临数据供应荒。</p><p>这家人工智能研究机构在开发最新 AI 系统时，已然耗尽了互联网上所有质量稳定的英语文本库。现在他们需要更多数据来训练自家技术的下一个版本——更多更多。</p><p>为此，OpenAI 研究人员开发出一款名为 Whisper 的语音识别工具，能够转录 YouTube 视频中的音频以生成新的对话文本，再将其作为训练素材以提升 AI 系统的智能水平。</p><p>三名知情人士表示，部分 OpenAI 员工讨论了此举可能违反 YouTube 规则。谷歌旗下的 YouTube 明确禁止将其视频用于“独立”于该平台以外的应用场景。</p><p>知情人士指出，最终 OpenAI 团队还是转录了超过 100 万小时的 YouTube 视频。两位知情人士表示，这支团队包括 OpenAI 总裁 Greg Brockman，他还亲自协助收集了这些视频。整理出的文本随后被输入名为 GPT-4 的系统，这也是目前得到广泛认可的最强 AI 模型之一，也是最新版本 ChatGPT 聊天机器人的底层引擎。</p><p>这场貌似追求技术的 AI 军备竞赛，早已转变成疯狂搜集数字数据的对抗与掠夺。根据《纽约时报》的调查，为了获取这些数据，包括 OpenAI、谷歌和 Meta 在内的科技大厂可谓“各显神通”——他们无视公司原则、修改规定条款，甚至公开讨论如何规避版权保护。</p><p>根据《纽约时报》获得的内部会议记录，在坐拥 Facebook 和 Instagram 的 Meta 公司，经理、律师和工程师们去年曾讨论收购由 Simon &amp; Schuster 出版社出版的长篇作品。他们还商定从互联网上收集受版权保护的数据，甚至愿意为此直面诉讼风险。与会者认为，逐个与出版商、艺术家、音乐家和新闻机构谈判授权许可恐将耗费过多时间。</p><p>与 OpenAI 一样，谷歌也在转录 YouTube 视频以为自家 AI 训练获取文本素材。五位了解谷歌具体操作的人士透露，这可能侵犯了视频版权，毕竟视频应归其创作者所有。</p><p>去年，谷歌还扩大了其服务条款。根据该公司隐私团队成员及《纽约时报》看到的一份内部消息，此番调整的动机之一就是为谷歌从公开的 Google Docs 文档、谷歌地图上的餐厅评论以及其他在线材料中提取更多信息敞开了大门，最终目的当然还是训练 AI 产品。</p><p>这些科技大厂的行动说明，在线信息——包括新闻故事、虚构作品、留言板帖子、维基百科文章、计算机程序、照片、播客及电影切片等——正在成为蓬勃发展的 AI 行业的根基与命脉。能否构建起强大的创新系统，往往取决于各方能否获得充足的数据来训练模型，进而生成与人类水平相当甚至更出色的文本、图像、声音与视频内容。</p><p>数据量甚至可以说将决定一切。领先的聊天机器人系统正在从涵盖多达 3 万亿字的数字文本池中学习，其体量约等于牛津大学博德利图书馆馆藏书籍总字数的两倍——该图书馆自 1602 年起就一直在收集各类信息。研究人员表示，“信息”是指由专业人士精心撰写及编辑的高质量内容，例如已出版的书籍和文章。</p><p>多年以来，互联网（包括维基百科和 Reddit 等网站）似乎成为取之不尽、用之不竭的数据来源。但想在 AI 领域傲视同侪的科技企业们仍在寻求更大的资源池。谷歌和 Mtea 坐拥数十亿用户，每天都会产生大量搜索查询与社交媒体帖子；但受到隐私法及其自身政策的限制，理论上并不能将大部分内容用于 AI 训练。</p><p>于是乎，数据供应短缺开始愈发凸显。研究机构 Epoch 表示，科技企业最快可能在 2026 年就用尽互联网上的高质量数据，其使用数据的速度已经明显超过了数据产出的速度。</p><p>硅谷风险投资公司 Andreessen Horowitz 的代表律师 Sy Damle 在去年关于版权法的公开讨论中谈到 AI 时表示，“这些工具获取实用功能的唯一途径，就是既接受大量数据的训练、又无视这些数据的使用许可。其需要的数据量如此庞大，即使是集体许可也仍无法满足。”</p><p>科技企业迫切需要新的数据，以至于部分公司开始使用“合成”信息。这些资讯并非由人类创建的有机数据，而是 AI 模型自身生成的文本、图像和代码。换句话说，AI 系统开始从自己的产物中学习知识。</p><p>OpenAI 表示，其每套 AI 模型“都拥有我们精心设计的独特数据集，以帮助其了解世界并保持研究层面的领先竞争力。”谷歌也提到，其 AI 模型“接受了一部分 YouTube 内容的训练”，这种行为符合其与 YouTube 创作者达成的协议，且该公司不会在实验计划之外使用来自办公应用的数据。Meta 则指出，他们已经通过“积极投资”将 AI 技术整合至各项服务当中，并使用来自 Instagram 及 Facebook 的数十亿公开分享图像及视频进行模型训练。</p><p>对于创作者来说，他们的作品正日益成为 AI 训练中的主要素材，由此引发的版权与许可诉讼也可谓此起彼伏。去年，《纽约时报》起诉 OpenAI 与微软侵权，称其在未经许可的情况下使用受版权保护的新闻文章来训练 AI 聊天机器人。OpenAI 及微软则表示这些文章属于“合理使用”，或者说并不违反版权法，这在本质上属于正常的二创行为。</p><p>去年，超过一万个贸易团体、作者、企业及其他机构向版权局提交了关于 AI 创意作品的使用报告，该局属于联邦机构，目前正着手对版权法内容做 AI 时代下的适用性调整。</p><p>电影制片人、前演员、两本出版书籍作者 Justine Bateman 提醒版权局，AI 模型会在未经许可或付费的情况下获取内容——包括她自己的书籍和影片。</p><p>她在采访中表示，“这是美国有史以来规模最大的盗窃案。”</p><section><span>“规模就是一切”</span>
</section><p>2020 年 1 月，约翰·霍普金斯大学的理论物理学家、Anthropic 的首席科学官 Jared Kaplan 发表了一篇关于 AI 的开创性论文，激发了人们对于在线数据的高度关注。</p><p>他的结论非常明确：训练大语言模型（驱动在线聊天机器人的底层技术）需要的数据越多，春性能就越好。正如学生们通过阅读更多书籍以汲取更多知识一样，大语言模型也能更好地提取文本中的模式，并通过更多信息将这种模式整理得更加准确。</p><p>Kaplan 博士与其他九位 OpenAI 研究人员共同发表了这篇论文（他目前在 AI 初创公司 Anthropic 供职），并在文中指出“每个人都对这种趋势感到惊讶——我们将其称为扩展法则，而且基本跟天文学或者物理学定律一样可靠。”</p><p>于是，“规模就是一切”很快成为 AI 竞赛中的战斗口号。</p><p>长期以来，研究人员一直使用大型公共数字信息数据库来开发 AI，包括维基百科和 Common Crawl（一套自 2007 年以来涵盖超 2500 亿个网页的数据库）。在实际训练之前，研究人员通常会删除其中的仇恨言论及其他非必要文本以“清洗”数据，再将其“投喂”给 AI 模型。</p><p>按今天的标准来看，2020 年的数据集体量还很小。来自照片网站 Flickr 的一套 3 万张照片数据库，在当时已经被视为重要的训练资源。</p><p>而随着 Kaplan 博士论文的发表，这些数据量已经远远不够。来自纽约的 AI 公司 Nomic 的 CEO Brandon Duderstadt 表示，目前的关键就是“把规模扩大”。</p><section><img src="https://static001.geekbang.org/wechat/images/38/38f871e4022d58e82c019c29ac1e497d.jpeg" /><br /><br /></section><p>随着 OpenAI 公司于 2020 年 11 月发布 GPT-3，该模型接受了截至当时规模最大的数据训练——约 3000 亿个 tokens。所谓 tokens，本质上就是一个个单词或者短语片段。从数据中完成学习之后，系统开始以惊人的准确性生成文本，编写博文和诗歌甚至能够输出计算机程序。</p><p>2022 年，谷歌旗下的 AI 实验室 DeepMind 又迈出了关键一步。他们测试了 400 种 AI 模型并调整其训练数据量及其他因素，发现表现最好的模型所使用的数量规模甚至比 Kaplan 博士论文中的预测还要更大。其中一套模型 Chinchilla 接受了 1.4 万亿个 tokens 的训练。</p><p>这项纪录很快就被打破：去年，来自中国的研究人员发布了 AI 模型 Skywork，使用来自英文及中文文本的 3.2 万亿个 tokens 进行训练。谷歌随后发布 AI 系统 PaLM 2，tokens 数量突破 3.6 万亿。</p><section><span>转录 YouTube 内容</span>
</section><p>去年 5 月，OpenAI 公司 CEO Sam Altman 承认，AI 企业将很快耗尽互联网上的所有可用数据。</p><p>他在一场技术会议的演讲上公开表示，“就快耗尽了。”</p><p>Altman 自己也切身感受到了这种紧迫感。在 OpenAI，研究人员多年来一直在收集数据、清洗数据并将其转录为大量文本以训练自家语言模型。他们挖掘了计算机代码库 GitHub，清洗了国际象棋走法数据库，并使用 Quizlet 网站上关于高中考试和家庭作业的数据。</p><p>根据八位了解 OpenAI 公司情况的人士（因未获授权公开发言而要求匿名）表示，到 2021 年底这些数据供应已经用尽。</p><p>OpenAI 公司迫切需要更多数据来开发其下一代 AI 模型，也就是我们熟悉的 GPT-4。知情人士称，员工们因此讨论了转录播客、有声读物及 YouTube 视频的可行性。他们还考虑利用 AI 系统从头开始创建数据，甚至想到收集那些掌握着大量数字数据的初创公司。</p><p>六位知情人士指出，OpenAI 最终开发出了语音识别工具 Whisper，专门用于转录 YouTube 视频及播客。但 YouTUbe 不仅禁止他方将其视频用于“独立”应用场景，还禁止他方通过“任何自动化方式（包括机器人、僵尸网络或爬虫工具）”访问其视频内容。</p><p>知情人士称，OpenAI 的员工清楚知道自己涉足的是法律的灰色地带，但他们相信使用视频内容训练 AI 属于合理使用。OpenAI 公司总裁 Brockman 在一份研究论文中被列为 Whisper 的缔造者。据两位知情人士介绍，他曾亲自帮助收集 YouTube 视频并将转录结果输入 GPT 模型。</p><p>Brockman 将置评请求转交给 OpenAI，该公司只模糊承认其使用了“来自众多来源”的数据。</p><p>去年，OpenAI 发布了 GPT-4，模型训练使用到 Whisper 转录的超 100 万小时的 YouTube 视频内容。这套最新、最强的大模型由 Brockman 领导的团队开发完成。</p><p>两位了解内情的人士表示，部分谷歌员工已经知晓 OpenAI 在收集 YouTube 视频作为训练数据，但他们并没有出声阻止，是因为谷歌自己也在使用 YouTube 视频的文字记录训练其 AI 模型。谷歌的这种作法同样可能侵犯 YouTube 创作者的版权。知情人士还提到，一旦谷歌揪住 OpenAI 的作法不放，那公众很可能针对其同类作法提出强烈抗议。</p><p>谷歌公司发言人 Matt Bryant 则表示，该公司对于 OpenAI 的行为一无所知，且禁止“未经授权抓取或下载 YouTube 内容”。他强调，谷歌将在获得明确法律或技术依据时采取行动。</p><p>谷歌的规则允许其利用 YouTube 用户数据为该视频平台开发新功能。但目前还不清楚谷歌是否可以利用 YouTube 数据构建除视频平台之外的商业服务，例如聊天机器人。</p><p>Berger Singerman 律师事务所的知识产权律师 Geoffrey Lottenberg 表示，谷歌对于 YouTube 视频记录可用于什么、不能用于什么的说法太过含糊其辞。</p><p>“这些数据是否可用于新的商业服务仍有待明确解释，甚至可能引发诉讼。”</p><p>2022 年底，就在 OpenAI 发布 ChatGPT 并引发全行业竞赛之后，谷歌研究人员和工程师们讨论了利用其他用户数据的可能性。用户们的 Google Docs 文档及其他免费谷歌应用中蕴藏着数十亿单词量的文本。但三名了解谷歌内情的人士指出，该公司的隐私条款限制了他们使用这些数据的方式。</p><section><img src="https://static001.geekbang.org/wechat/images/b6/b62d2ca06edd3c47e4696ccf77828a7d.png" /><br /><br /></section><p><span>据了解内情的人士介绍，在 OpenAI 发布 ChatGPT 之后，谷歌研究人员和工程师们开始讨论利用其他用户数据开发 AI 模型的可能性。</span></p><p>据隐私团队两名成员及《纽约时报》看到的一份内部消息称，谷歌<span>法律部门于去年 6 月要求隐私团队起草措辞，以扩大该公司对消费者数据的许可使用范围。</span></p><p><span>员工被告知，谷歌希望利用用户们的 Google Docs 文档、Google Sheets 表格及相关应用程序中公开的内容来开发一系列 AI 产品。员工们称，他们不清楚公司之前是否曾利用这些数据训练过 AI 模型。</span></p><p><span>当时的谷歌隐私政策强调，该公司只会使用公开信息来“帮助训练谷歌的语言模型并构建谷歌翻译等功能。”</span></p><p><span>隐私团队编写了新条款，以便谷歌能够利用这些数据为其“AI 模型提供支持，并构建包括谷歌翻译、Bard 及 Cloud AI 在内的更多产品及功能”，也就是更广泛的 AI 技术集合。</span></p><p><span>隐私团队的一名成员在内部消息中质疑，“我们的最终目标是什么？我们还要做到什么程度？”</span></p><p><span>员工们表示，谷歌特别要求隐私团队要在 7 月 4 日周末发布新条款，想要用美国的独立日假期冲淡用户的关注。修订后的政策于 7 月 1 日长周末开始时首次发布。</span></p><section><span>谷歌如何使用客户数据</span>
</section><p>下面来看谷歌去年对其免费消费者应用程序隐私政策做出的修改。</p><section>
谷歌会使用客户信息以改进我们的服务，并开发有利于用户及公众的新产品、功能及技术。例如，我们会使用公开信息帮助训练谷歌的 <span>语言 </span><span>AI</span>模型并构建包括谷歌翻译、<span>Bard 及 Cloud AI 功能</span>在内的<span>产品与</span>功能。</section><p>两名隐私团队成员表示，去年 8 月他们曾向管理层施压，询问谷歌是否已经开始使用免费消费版本 Google Docs、Google Sheets 以及 Google Slides 中的数据，但并未得到明确答案。</p><p>Bryant 表示，隐私政策的变更是为了强调并明确谷歌不会在“未经用户明确许可”的情况下，使用 Google Docs 或相关应用程序中的信息来训练语言模型。这只是一项允许用户测试实验性语言模型的自愿计划。</p><p>他强调，“我们并没有根据条款内容的变化将其他数据类型用于模型训练。”</p><section><span>Meta 身陷争议</span>
</section><p>Meta 公司首席执行官 Mark Zuckerberg 已经在 AI 领域投资多年，但随着 OpenAI 在 2020 年发布 ChatGPT，他猛然发现自己已经落后于时代。三位现任及前任员工（因未获发言授权而保持匿名）表示，Zuckerberg 决定立即迎头赶上并超越 ChatGPT。他连夜打电话给高管和工程师，敦促他们开发一款与之竞争的聊天机器人。</p><p>但到去年初，Meta 遇到了与其竞争对手相同的困境：得不到足够的数据。</p><p>从某位员工分享的内部会议记录来看，Meta 公司生成式 AI 副总裁 Ahmad Al-Dahle 曾向高管团队强调，他的团队几乎使用到互联网上所有公开发布的英文书籍、论文、诗歌和新闻文章以训练 AI 模型。</p><p>Al-Dahle 告诉同事们，除非获取更多数据，否则 Meta 的模型将无法与 ChatGPT 相抗衡。2023 年 3 月和 4 月，Meta 公司的部分业务开发领导、工程师和律师几乎每天都在开会讨论这些问题。</p><p>有人争论要不要以每本书 10 美元的价格买下新书许可权，从会议录音来看，他们还曾讨论收购 Simon &amp; Schuster 出版社（该公司曾出版斯蒂芬·金等作家的作品）。</p><p>他们还谈到如何以不经许可的方式从互联网上获取书籍、论文及其他文本，甚至考虑顶着面临诉讼的风险扩大内容获取范围。录音显示，一名律师对于从艺术家手中夺取知识产权提出“道德担忧”，但现场无人给出响应。</p><p>Zuckerberg 的态度则非常明确——给我找出解决方案来！</p><p>一位工程师表示，“Zuckerberg 想在产品中实现的功能，我们目前根本就做不到。”</p><p>两名员工表示，虽然 Meta 运营着庞大的社交网络，但这里并没有丰富的用户帖子可供使用。他们指出，不少 Facebook 用户会删除之前发布的帖子，而且该平台也不以撰写严肃长文为主要卖点。</p><p>Meta 当时还身负另一项压力——由于 2018 年与选民分析公司 Cambridge Analytica 共享用户数据的丑闻，其隐私政策刚刚经过调整，实在不宜轻举妄动。</p><p>Zuckerberg 在一次投资者电话会议上表示，Facebook 和 Instagram 上公开分享的数十亿视频和照片“比 Common Crawl 数据集还要大”。</p><p>在会议讨论中，Meta 高管们谈到如何在非洲聘请承包商来整理当地小说及非小说素材。一位经理也在某次会上指出，这些素材中确实包含受版权保护的内容，“因为我们无法将其彻底剔除。”</p><p>Meta 的高管们认为，OpenAI 似乎在未经许可的情况下使用了受版权保护的素材。从录音来看，他们也清楚 Meta 需要很长时间才能跟出版商、艺术家、音乐家和新闻机构达成许可，所以肯定是选择了“先斩后奏”。</p><p>全球合作与内容副总裁 Nick Grudin 在一次会议上表示，“唯一阻止我们向 ChatGPT 水平看齐的因素，就是数据量。”</p><p>他还补充称，OpenAI 似乎正在使用受版权保护的素材，而 Meta 可以遵循这一“市场先例”。</p><p>录音还提到，Meta 公司的高管们同意参考 2015 年作家协会诉谷歌一案的法院判决。在该案中，谷歌被允许对在线数据库内的书籍进行扫描、数字化和缠上，理由是其仅复制了作品的部分片段，并对原件进行了改造，因此属于合理使用。</p><p>Meta 公司的律师们则在会上指出，使用数据训练 AI 系统也理应同属合理使用的范畴。</p><p>录音显示，至少有两名员工对于使用知识产权且以不公平甚至根本不付费的方式对待作者及其他艺术家表示了担忧。一名员工还与 Meta 公司首席产品官 Chris Cox 等高管人士就版权数据进行过单独讨论，并表示那次会议上没人关注使用他人创意作品产生的道德问题。</p><section><span>“合成”数据</span>
</section><p>面对迫在眉睫的数据短缺难题，OpenAI 的 Altman 专门定下一条妙计。</p><p>他在 5 月的会议上表示，像 Meta 这样的公司终将使用由 AI 生成的文本进行训练——也就是合成数据。</p><p>这是因为 Altman 及其他高管都相信，AI 模型既然能够生成与人类相似的文本，那就一定可以输出额外的数据来开发更好的模型版本。这将帮助开发人员建立起日益强大的 AI 技术，并减少对受版权保护数据的依赖。</p><p>Altman 表示，“只要能够扩大合成数据的涵盖范围，也就是说只要模型足够智能，它就能生成高质量的合成数据，素材短缺问题将迎刃而解。”</p><p>多年以来，AI 研究人员一直在探索合成数据的可行性。但构建一套能够自我训练的 AI 系统，明显是说起来容易做起来难。利用自身输出学习的模型往往会陷入死循环——即不断强化自己的倾向、错误和局限性。</p><p>前 OpenAI 公司研究员、现任不列颠哥伦比亚大学计算机科学教授的 Jeff Clune 表示，“训练 AI 系统所需要的数据，就如同一条穿越丛林的道路。如果只使用合成数据进行模型训练，那 AI 就很可能在丛林中彻底迷失方向。”</p><p>为了解决这个问题，OpenAI 及其他厂商正在研究如何让两套相互独立的 AI 模型彼此引导。这些模型能够协同工作以生成更有用、更可靠的合成数据。其中一套系统负责生成数据，另一套系统则判断信息内容以保障输出质量。但研究人员对于这种方法能否奏效仍然存在分歧。</p><p>尽管如此，AI 大厂的高管们一刻也没有停止前进的脚步。</p><section>Altman 在会议上拍板，“我觉得应该没问题。”</section><article><section><span> 声明：本文为 InfoQ 翻译整理，未经许可禁止转载。</span><span></span></section></article><p><span>原文链接：</span></p><p><span>https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html?smid=nytcore-ios-share&amp;sgrp=c-cb</span></p><section><section><span></span>内容推荐<br /><br /></section></section><section>大模型应用挑战赛已拉开帷幕。现阶段，多数语言模型已完成 3 轮更新，大模型赛道入场券所剩无几。同时，2023 年超 200 款大模型产品问世，典型场景又有哪些产品动向？对于现阶段的文生图产品而言，四大维度能力究竟如何？以上问题的回答尽在《2023 年第 4 季度中国大模型季度监测报告》，欢迎大家扫码关注「<span>AI 前线</span>」公众号，回复「<span>季度报告</span>」领取。</section><section><img src="https://static001.geekbang.org/wechat/images/d2/d2c75750d853a4baaf489be1f840126b.webp" /><span></span></section><section><section><section><section><span></span>&nbsp;活动推荐</section></section></section><p>AICon 全球人工智能与大模型开发与应用大会暨通用人工智能开发与应用生态展将于 5 月 17 日正式开幕，本次大会主题为「<span>智能未来，探索 AI 无限可能</span>」。如您感兴趣，可点击「<span>阅读原文</span>」查看更多详情。</p><p><img src="https://static001.geekbang.org/wechat/images/24/24cd15257ee2474ae7bd8f17c8c5f308.jpeg" /></p><p>今天是会议 9 折购票阶段，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p></section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><section><h5><span></span>今日荐文<span><span></span><br /><br /></span></h5></section></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><article><section><section><section><section><section><section><section><section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></article></section></section></section></section></section></section></section></section></article></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247608636&amp;idx=1&amp;sn=3233d483ec34e32c61b7c2a09de795b5&amp;chksm=fbeb90f3cc9c19e5c3ad8a7330e5c0e6a88295312968577892e4ba5529d3d9799161cd5ad211&amp;scene=21#wechat_redirect" target="_blank"></a></section><p><strong><strong><strong><span><strong><strong><strong><span><strong>你也「在看」吗？</strong></span></strong></strong></strong></span></strong></strong></strong><strong><strong><strong><span>👇</span></strong></strong></strong></p></section></section>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7tM6f86wsgAKVogNm9Hl</id>
            <title>百度沈抖：大模型重构云计算生态，放宽伙伴能力要求</title>
            <link>https://www.infoq.cn/article/7tM6f86wsgAKVogNm9Hl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7tM6f86wsgAKVogNm9Hl</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 04:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型引领, 伙伴市场权益, 生态战略, AI原生应用
<br>
<br>
总结: 百度智能云GENERATE全球生态大会探讨大模型时代的新型云计算生态体系，发布伙伴市场权益和产品权益，助力生态企业AI场景研发，推动大模型产业发展。针对不同市场和伙伴，制定差异化的生态战略，发布一系列产品权益和扶持计划，为合作伙伴提供全方位的支持和保驾护航。 </div>
                        <hr>
                    
                    <p>4月9日，首届百度智能云GENERATE全球生态大会在成都召开。面向大模型引领的智能化升级浪潮，百度智能云携手伙伴共同探讨大模型时代的新型云计算生态体系，并面向头部市场、价值市场、高潜市场三类目标市场，制定差异化生态战略，发布一系列伙伴市场权益。同时，围绕大模型技术栈，在算力、模型、应用开发、应用售卖四大方面，发布一系列产品权益和扶持计划。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/29/ab/2952e42bcbdabe6a59e9ede3ee83bcab.jpeg" /></p><p></p><p>过去一年，千帆大模型平台已经服务了8.5万企业客户，累计精调超过1.4万个模型，开发超过19万个应用，领跑国内大模型市场，合作伙伴从中发挥了重要的推动作用。最近半年，百度智能云大模型伙伴数量大幅增长5倍，超过300款伙伴开发的AI原生应用已经通过千帆AI原生应用商店走向市场。</p><p></p><p>百度集团执行副总裁、百度智能云事业群总裁沈抖表示，大模型极强的泛化能力，为跨行业、跨场景的应用开发提供了通用的、可迁移的基础能力和全新的AI原生应用研发范式，在极大地降低研发门槛、提升研发效率的同时，也为产业生态环境带来根本性的改变。</p><p></p><p>沈抖认为，有别于传统云计算，大模型放宽了对合作伙伴的技术能力要求、成倍放大了市场空间，洞察客户需求、赢得客户信任才是制胜的关键。大量新伙伴即将涌现，借助大模型的能力，去更好地满足客户个性化的、长尾的需求，为客户创造价值。</p><p></p><p>会上，百度智能云还与成都高新区举行了战略合作签约仪式，携手推进区域AI产业发展。通过百度智能云在大模型技术、产品等方面的支持，助力生态企业AI场景研发，推动大模型上下游产业集聚和蓬勃发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3e1d0bbd1d9f1ffbda332def25331b2.jpeg" /></p><p>百度智能云与成都高新区举行战略签约仪式</p><p></p><h3>面向三大市场、四类伙伴，推出差异化的市场权益</h3><p></p><p></p><p>沈抖表示，过去一年，百度智能云主要有两类伙伴。一类是“场景共创型”，主要由头部的咨询、解决方案、服务等“综合型伙伴”，和擅长软件开发的“应用型伙伴”组成。过去一年，百度智能云与“场景共创型”伙伴合作交付项目占比超过50%，未来还将进一步强化合作关系，提升伙伴的参与度。另一类是“用户增长型”，由经销商、分销商、代理商等伙伴构成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13ad2c0fe7d2cf9de569657e6ae6d3d7.png" /></p><p>百度智能云生态战略</p><p></p><p>为了更好的帮助客户落地大模型与AI原生应用，加速实现智能化升级，百度智能云面向三类市场制定差异化的生态战略。</p><p></p><p>头部市场客户业务场景综合且复杂，个性化诉求强，需要具备较强客情关系、深度理解客户所在行业、拥有行业方案与产品的深度整合与交付能力的合作伙伴，与百度进行联合攻坚、相互补位，打造出大模型时代典型的应用案例，为客户创造新价值，树立行业标杆。价值市场客户相比头部市场客户数量更多，业务场景更加标准化，复杂度更低。伙伴可以把面向头部客户打造的行业标杆快速复制，或者提供相对轻量化的定制方案。在此过程中，百度将以“伙伴优先”为原则，做好技术和商务支持，按需参与方案定制化开发，与伙伴共同开拓市场、服务客户。高潜市场客户数量最多，业务场景也更简单，更看重开箱即用的解决方案。面向这类客户，百度将聚焦打磨标准化的产品与解决方案，并给予伙伴足够的让利，保障伙伴利润空间，支撑好伙伴，由伙伴完全主导销售及服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1ff6f4ccccbc535a97d935b31e0e874e.png" /></p><p>百度智能云生态伙伴目标市场权益</p><p></p><p>百度智能云渠道生态部总经理陈之若表示，针对三类目标市场，百度智能云将为四类合作伙伴提供有针对性的市场权益。</p><p></p><p>面向头部市场，发展10+综合型伙伴，提供10亿元定向商机和1000万元专项资金支持。同时帮助综合伙伴加入到百度的交付体系，打造“销服一体”的合作通路，实现与伙伴的互利共赢；面向价值市场，发展100+应用型伙伴，推出针对性的共创计划，为单个应用伙伴提供20万元联合解决方案共创基金，提供专属的AGI House专题培训，帮助应用伙伴提升技术能力，更好地应对市场挑战；面向高潜市场，发展10000+初创企业伙伴，通过提供上云和大模型资源、AI加速器、生态社区等支持，帮助创企解决资源与能力提升难题；针对代理伙伴，提供官网商机支持，并打造完备的佣金与激励体系，真实让利于伙伴。同时，百度智能云还为首单开单的伙伴特别设立了代理激活计划，提供额外奖励扶持。</p><p></p><h3>针对算力、模型、应用开发、应用售卖，发布产品权益“四件套”</h3><p></p><p></p><p>面对全新的市场，百度智能云为伙伴准备了一整套AI原生技术栈，并针对合作伙伴的能力差异、技术与产品集成度，发布了一系列产品权益。</p><p></p><p>在算力方面，百度智能云积累了充足的算力储备和领先的工程技术，能够为伙伴提供高可用、高吞吐、低延迟、稳定易运维的算力服务。大会期间，百度智能云宣布启动智算战略伙伴招募，招募3+算力合作伙伴。伙伴可以成为百度的算力供给方，与百度共建算力中心；承载算力运营服务，享受优质算力价格补贴。百度还将为算力伙伴提供与大模型、AI芯片相关的智算中心合作拓展机会。</p><p></p><p>在模型方面，千帆ModelBuilder提供包括文心一言系列模型在内的77款精选大模型和全流程的模型工具链，并宣布新建全球首个千帆大模型创新实验室，百度智能云将与伙伴一起，围绕10000+个应用场景进行联合创新。在创新实验室内，百度将免费提供产品适配资源和专项技术支持，并为伙伴提供AI原生应用商店的免费入驻加速服务，通过商机支持、产品备案以及交付等一系列权益，为合作伙伴提供全方位的保驾护航。</p><p></p><p>在应用开发方面，千帆AppBuilder给伙伴提供了便捷好用的应用开发平台，支持伙伴在AppBuilder上集成和扩展第三方组件，也可以通过工作流，把多个功能集成到一个可以对话的智能体里。百度智能云也基于千帆开发了一些应用或“半成品”，供合作伙伴直接转售或进一步开发。比如智能编码辅助工具“Baidu Comate”、知识管理平台“甄知”等七大AI原生应用产品，以及面向工业、政务、交通、汽车、金融等行业的技术/产品底座。</p><p></p><p>本次大会，百度智能云还特别推出千帆杯·AppBuilder大模型开发者专项激励政策。通过设立百万级别奖金池、千万级算力资源补贴、权威技术赋能，以及海量商机与流量支持，帮助开发者和伙伴得到更广泛的关注和认可。</p><p></p><p>在应用售卖方面，全新发布千帆AI原生应用商店招募计划，加大对入驻伙伴的扶持力度，提供高额返佣激励和营销资源扶持，持续加强技术指导与项目合作，在2024年实现2000+个优质应用入驻商店，致力于将千帆AI原生应用商店打造成企业选购AI原生应用的首选平台。</p><p></p><h3>一年集结12万生态伙伴，大模型产业落地大幕开启</h3><p></p><p></p><p>陈之若表示，不到一年时间，千帆平台已经汇聚了12万家创企和生态伙伴，活跃调用平台API的伙伴数量超过5.5万，已经有8100家伙伴通过千帆AppBuilder开发AI原生应用。目前，百度智能云通过伙伴服务的大模型头部客户数已经超过了200家。</p><p></p><p>以综合伙伴为例，润建股份是中国领先数字化智能运维服务商。润建股份作为千帆首批生态伙伴，目前已经与百度智能云在能源、政务、教育等领域进行深度合作，并基于大模型打造了智慧城管解决方案，为广西自治区南宁市西乡塘区等53个区县城管局提供智慧服务。</p><p></p><p>另一位综合型伙伴华胜天成，基于千帆大模型平台开发、推出了智能数据助手、智能客服、智能投标大王三款大模型应用产品，为最终客户的营、销、服务等全生命周期提供智慧赋能。</p><p></p><p>雅基软件作为应用伙伴的代表，旗下核心产品Cocos引擎是全球领先的2D&amp;3D引擎。通过接入千帆平台，Cocos整体的智能化水平大幅提升，能够实现游戏中动态生成交互内容，大幅提升某游戏开发效率120%，用户满意度提升50%。为开发者和终端玩家带来了更优质的用户体验，大幅增强用户黏性。在未来，Cocos 引擎还会进一步集成更多 AI 特性，以实现游戏美术资产的高效生产，游戏内语音聊天工具的快速构建等能力。</p><p></p><p>此外，应用型伙伴新致软件还通过接入千帆平台，将行业知识与大模型相结合，帮助华住、如家、锦江之星等连锁酒店的一体化营建供应链，以及膳魔师等价值市场客户打造了数字员工。数字员工能够精准理解用户意图，并执行专业任务，开创了企业运营与营销的全新模式。</p><p></p><p>初创企业万幸科技，基于千帆平台推出了大模型装修助手“装修GPT”，帮助用户花更少的钱，享受到更高效、低成本的设计服务和更高品质的家装建材，一站式解决用户的家装需求。目前，装修GPT已服务了近1000家客户。截止2023年12月，万幸科技收入已达数百万，成为了第一批通过大模型赚到钱的初创企业。</p><p>陈之若表示，大模型为技术创新搭建了无比广阔的舞台，也为企业提供了千载难逢的转型契机。百度智能云将坚定不移地携手伙伴同行，相互成就，实现真正的合作共赢。</p><p></p><p>【活动推荐】</p><p></p><p>在 2024 年 6 月 14-15 日<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">深圳 ArchSummit 架构师峰会</a>"上，我们邀请了 CNCF、顺丰集团、腾讯、百度等企业的专家来演讲。会议上还设置了大模型、架构升级等专题，如果你感兴趣来会议上听演讲，欢迎进入 ArchSummit 会议官网，查看讲师们的详细演讲提纲。</p><p></p><p>会议现已进入 8 折早鸟购票阶段，可以联系票务经理 17310043226 , 锁定最新优惠。扫描上方二维码添加大会福利官，免费领取定制福利礼包。</p><p><img src="https://static001.infoq.cn/resource/image/a7/d3/a7169c8f216f8af139e2f6886de5b8d3.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bNK3WerxFuArA9oLWE0t</id>
            <title>AI 面试的“酷刑”，只有中高级管理层和 CEO 能幸免</title>
            <link>https://www.infoq.cn/article/bNK3WerxFuArA9oLWE0t</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bNK3WerxFuArA9oLWE0t</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 02:59:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知乎, AI面试, 人才画像, 智能判分
<br>
<br>
总结: 知乎上分享了参与AI面试的经历，AI面试已经得到广泛接受，对候选人公平且全面考核。人才画像、关键词评估、情绪稳定度等是AI面试的重点，AI面试适合规模大、标准化的岗位。AI面试评判更加标准化，要求应试者回答清晰、有逻辑。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/wechat/images/65/653b990cba184f7147edd168fcaf6d54" /></p><p></p><p></p><p></p><p></p><p>知乎上有人详细分享了自己参与 AI 面试的经历：</p><p></p><p></p><blockquote>“点开链接，在手机上是用小程序，电脑是浏览器。一开始有个预录，检查手机摄像头、麦克风以及自己的语音声音大小。开始做答之后有 5 个题目。每个题目 6 分钟，包括构思和录制视频。点击录制视频有 5s 的准备时间，5s 之后手机自动录像，你开始答题，视频录制不超过 5 分钟。录制完之后，返回答题页面，可以回放。5 分钟答题时间到，它会提醒提交，然后点击提交。就进入下一题。”</blockquote><p></p><p></p><p>“我觉得 AI 面的公司压根就没打算招人”，这是 AI 面试刚出来时人们通常会给的评价。但现在，大家对 AI 面试的接受度已经很高，AI 面试的争议更多是出现在一两年前，现在网上几乎都是 AI 面试的建议、题库等。</p><p></p><p>“和传统面试比起来，我更倾向于用 AI。AI 对于你的输入、表情、动作都会关注，更加全面考核，至少是对所有候选人是公平的，不会出现面试官主观的偏见偏差、个人因素等问题，当然也得要求 AI 面试要足够智能。”广州大学嵌入式应用专业的一名本科学生说道。</p><p></p><p>根据《2024 牛客智能制造业校园招聘白皮书》，53.5% 的对智能制造感兴趣的学生有参与过 AI 面试，而参与过 AI 面试的学生中高达 77.7% 的人表示满意。</p><p></p><h3>这次，传统行业走在了前头</h3><p></p><p></p><p>面试可以用 AI 进行，也说明了面试本身在一定程度上是可规则化的。</p><p></p><p>有做人力资源管理的网友分析称，抛掉“人间冷暖”不谈，面试的本质是按图索骥，一场高效的面试依赖以下几个方面：</p><p></p><p></p><blockquote>人才画像 ——AI 可以结构化提问 ——AI 可以关键词评估 ——AI 可以情绪稳定度 ——AI 应该可以（可能不精确）与公司文化 / 直接上级的默契度 ——AI 也许可以 （双向测试后可以提升匹配度）如果一家公司坚持不懈地做 AI 面试，积累匹配组织发展的人才大数据，那么招聘的人才至少 80% 左右是完全靠谱的，再加上终面 BOSS 感受一下，精准度应该可以达到 90% 以上，这比有 3-5 年工作经验的 HR 靠谱多了。</blockquote><p></p><p></p><p>当然，AI 面试最终会让类似的人都聚集在一起，很有可能导致组织僵化。在组织需要变革、寻找一些鲶鱼进入组织时，人才画像将完全不一样，AI 的底层面试逻辑就要随之重构。</p><p></p><p>“我曾经只是作为辅助面试人员参与了公司里的终面过程，一整个上午也就搞了六七个人的面试，到中午吃饭的时候，主面试官基本上就快累趴下了。但是在我看来，很多面试官工作内容并不复杂，甚至到后面有一些机械。不管面试者说啥，面试官都是在固定重复问几个问题。”弗兰克扬在知乎上分享称，“我也不觉得这会有什么大问题，因为最关键的信息其实就那么一两个，剩下的就是看你还想从聊天中聊点什么了，不管你聊什么，大部分都不会影响最终的决定。”</p><p></p><p>“弗兰克扬”表示，十多年前，一些大企业会把前几轮的招聘流程给外包公司，这些外包公司先筛选简历、再进行电话面试，问的问题都是雇主公司规定好的，全程录音，然后再根据面试情况做筛选，最终把报告发给雇主做最终决定。“现在 AI 的水平，我感觉跟当年外包干的工作都差不多。”</p><p></p><p>牛客联创兼技术负责人杨之贤介绍称，AI 面试特别适合招聘规模较大、考核能力相对标准的岗位，如校招管培生、普通蓝领、销售岗位、客服群体、小语种岗位等。以互联网为例，IT 基础岗位初筛、产品运营岗位、销售岗位、客服岗招聘人数超 10 人以上均有可能使用 AI 面试。</p><p></p><p>消费行业是最早尝试使用 AI 面试的行业之一，而且外企的接受度更高，比如雅诗兰黛、宝洁等。今年，随着大模型的兴起，越来越多的行业对 AI 面试表现出了兴趣，互联网、国企央企、银行、教育、电信、汽车、快消、制造业等行业都引入了 AI 面试。</p><p></p><p>比如 2023 年，光储行业里的龙头企业阳光电源决定将所有管培生岗位的英语面试全部使用牛客 AI 面试代替以往的英语外包面试。当年，阳光电源 AI 面试的管培生超 500 人次。</p><p></p><p>杨之贤表示，目前国内大约有万级别的企业在面试流程中加入了 AI 面试，而且这一数字还在不断增长。据《牛客 2023 秋季校园招聘白皮书》的调研数据指出，已有 23.2% 的先锋企业应用 AI 助力校招，其中使用 AI 助力笔面试环节的企业占比达 97.9%。</p><p></p><h3>被放大的细节</h3><p></p><p></p><p>AI 面试的优势是不会受到情绪、偏见或其他主观因素的影响，它会根据事先设定的评分标准进行客观评价。AI 面试会重点会考察面试者以下三个能力：</p><p></p><p>胜任力测评，包括学习能力和抗压能力，这些通常是衡量一个人是否适合工作的重要指标。专业能力，即面试者在特定领域的知识和技能。这是根据不同岗位的要求来考察的，以确保面试者具备所需的专业素质。语言能力，比如英语、越南语等，良好的语言能力对于与外企的国际团队合作和沟通至关重要，还有的工作也需要一定外语能力。</p><p></p><p>结合面试者的简历和回答，AI 面试系统会进行提问和追问，并利用大模型的自然语言理解能力和逻辑推理能力，给出面试者对应能力项的得分。</p><p></p><p>“智能判分是基于岗位专业素质、通用能力素质和语言能力素质的综合科学判定。我们会综合考虑面试者回答内容的专业性、相关性和逻辑性等因素，并结合回答状态进行综合判定。”杨之贤说道。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/32b8e68c36897721ed2756d290f87a4a.png" /></p><p></p><p>AI 面试成绩单</p><p></p><p>AI 面试评判更加标准化，这就要求应试者当下就要快速反应，回答得清晰、有逻辑，覆盖题目中所有的问题。“回答得是否高大上不重要，重要的是你的回答是否具有逻辑。”网友“肉丝 er”也分享道。</p><p></p><p>“肉丝 er”也还特别提到，眼神千万不要飘忽不定，有的 AI 能够根据面试者的眼神分辨其是否在读稿，一旦被判定为读稿，那么不管面试者说得多好都会被 pass 掉。这是因为有的 AI 面试系统有眼神追踪功能，四处乱瞟会被视为作弊。</p><p></p><p>“通过分析面试者的微表情，可以更准确地判断其是否在面试过程中存在作弊行为。”杨之贤表示，“这是为了保证面试的公平性和诚信性。”</p><p></p><p>因此，除了专业能力，面试中的语速、情绪、肢体语言等都可能影响 AI 系统对面试者的评分。要知道，虽然 HR 可以查看面试视频，但大多数时候是根据 AI 的评分做初步筛选的。</p><p></p><p>另外，还有一些服务行业的公司会用 AI 进行性格测试。Paradox.ai 的性格测试在 Reddit 上多次疯传，联邦快递、麦当劳等公司都使用 Paradox.ai 公司的 AI 面试系统，通过“漫长而奇怪的性格测验”来招聘客户和食品服务工作人员，并附有“蓝色外星人”形象，目标是发现候选人在“亲和性”和“情绪稳定性”方面的排名。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9b/9b3ed0216cdba8dfa104e7b759c8285a.png" /></p><p></p><p></p><h3>AI 选不了 CEO</h3><p></p><p></p><p>而对于研发这样的 AI 面试系统，最重要的数据训练。</p><p></p><p>据杨之贤介绍，牛客采用了大模型和小模型的结合方式，自研了 AI-NowGPT 模型。为了保证“AI 考官”的公平和专业，牛客内部邀请了大量资深面试官对相同数据做人工标注，以确保 AI 考官在评分和评价方面与人类面试官保持一定程度的同步。</p><p></p><p>有的公司也会邀请客户企业参与标注，这样 AI 考官的评分标准会更加符合客户的需求。客户的参与可以帮助企业微调模型，使其更加符合特定岗位的要求。人类考官也可以对一部分或全部的面试结果进行复核，确保 AI 考官的判分结果的准确性和公正性。</p><p></p><p>像牛客这样还拥有笔面试 SaaS 产品的招聘网站，拥有专门的内容出题团队，借助海量的题库资源，还可以实时定制出题。</p><p></p><p>不过，杨之贤也表示，目前的 AI 面试还只是主要用在初步的大规模快速筛选上，对于后续的面试轮次，不同面试官关注的能力和业务知识可能存在差异，这是 AI 面试目前无法帮助解决的。“AI 选不了中高级管理层，也选不了 CEO。”</p><p></p><p>理想的情况是 AI 系统能够根据面试官的要求自动生成面试方案：面试官口述想要考核的能力，AI 快速提炼出相关考核点，并生成相应的面试问题和评估标准。这样，面试官就能更加专注在候选人的表现上，不需要花费过多精力在准备面试方案上。但凭现在 AI 的能力，在快速提炼考核内容方面还存在比较多困难。</p><p></p><p>此外，AI 面试系统还可以在面试过程中提供一些辅助功能，比如邀约和谈薪资：AI 可以自动发送邀约邮件或短信，并根据候选人的反馈和面试结果，提供薪资谈判的建议。</p><p></p><p>杨之贤表示，未来除面试外，AI-NowGPT 还将增加简历的点评、优化功能，同时提供准确的人岗匹配度评估，帮助企业找到岗位最合适的候选人。</p><p></p><p>然而，人类面试官的亲和力和人际交往能力在面试过程中仍然非常重要。虽然 AI 可以提供面试方案和辅助功能，但在与候选人的互动和评估中，人类面试官的角色仍然不可或缺。他们可以通过面试过程中的非语言交流和深入提问，更好地了解候选人的能力和适应性。</p><p></p><p>此外，电子前沿基金会社区组织副主任 Rory Mir 也指出，当前人工智能浪潮使用的是概率模型算法，这意味着它们只是依赖过去的数据模式做出预测。“问题是，过去数据的模式包括系统性偏见产生的模式。”</p><p></p><p></p><h3>用 AI 打败 AI</h3><p></p><p></p><p>当然，除了招人企业用 AI，面试者也会“用 AI 打败 AI”。</p><p></p><p>Interview Dog 是一款专门的 AI 面试辅助软件 ，可以通过实时语音识别问题来帮助面试者回答考官的问题，支持科技、金融、工程、商业、法律等行业。Interview Dog 主打“按需使用”，五分钟的免费试用之外，每分钟付费 0.45 美元。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2c/2cfd804251a411e76480e36481751983.png" /></p><p></p><p>为此，Interview Dog 在网上也得到了很多好评：“在我大脑短路时，Interview Dog 让我想起了一切”“有了它，谁还需要运气啊！”</p><p></p><p>但直接念答案也是有风险的，遇到自己不回的问题，“假装”思考后对答如流是会被 HR 怀疑的。</p><p></p><p>“我之前面了一个小女孩，答的太完美了。于是我钓鱼了几个问题，她说的和 GPT 的结果八成相似，给她上了点压力后，一个问题都答不出来了。”网友 momo 分享道。</p><p></p><p>对此，有的人认为这种做法并无不妥：“工作不就是借助各种工具完成任务吗？既然面试是模拟工作场景，那用不用 GPT 取决于面试者，只要结果出来就完事了。”但国内大多数企业是不太接受这种说法的，至少像算法思路这种应该是自己能回答的，否则就变成了纯粹的“工具人”。</p><p></p><p>小红书的一个博主发起了“面试应该允许使用 GPT 吗”的小调查，参与的 1207 个人里，47% 的人投给了可以，剩下的人还是认为不应该，“面试不能用 GPT 就跟考 GRE 不能查字典是一个道理。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1b/1b3447be7096907d2244975e8adb69d3.jpeg" /></p><p></p><p>除了这种直接用来应对面试的工具，还有之前普遍被认为“水深”的简历优化。在猎聘等招聘网站上，一对一的简历优化收费达到了 398 元，但有网友给出的评价是：花钱改简历≈抽奖，而且中奖几率极低！</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/80/8074221f44dd8e02e0b7b3cdde90e1a9.png" /></p><p></p><p>猎聘网简历优化收费套餐</p><p></p><p>现在，有的应聘者会用 ChatGPT、文心一言等优化简历，还有人开发了基于 ChatGPT 的简历工具，用户可以直接使用 ChatGPT 自动修改。这种用法，基本不会有什么争议了。</p><p></p><p>未来，面试者和招聘者手里的魔法谁会被打败，似乎也是一个有趣的问题，毕竟 AI 不会站队。</p><p></p><p>观点引用：</p><p></p><p><a href="https://www.zhihu.com/question/649440119/answer/3438486437">https://www.zhihu.com/question/649440119/answer/3438486437</a>"</p><p><a href="https://www.zhihu.com/question/649440119/answer/3438486437">https://www.zhihu.com/question/649440119/answer/3438486437</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Du8obp9eqqIAmcv6isRC</id>
            <title>守住这场公开课，找到 AI 应用安全问题的破解之道！</title>
            <link>https://www.infoq.cn/article/Du8obp9eqqIAmcv6isRC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Du8obp9eqqIAmcv6isRC</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 01:41:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能家居, AI安全挑战, 数据泄露, 网络攻击
<br>
<br>
总结: 当前AI技术在智能家居、自动驾驶、医疗诊断和金融风控等领域的应用潜力正在逐步释放，但同时也面临着安全挑战，包括模型漏洞、数据泄露、网络攻击等问题。因此，必须认识到AI应用的安全性问题，并采取有效措施加以防范。 </div>
                        <hr>
                    
                    <p>现如今，无论是智能家居、自动驾驶、医疗诊断还是金融风控，AI 的潜力正在被逐步挖掘与释放。与此同时，AI 应用的安全挑战问题也亟需得到关注。</p><p></p><p>首先，AI 模型可能隐藏着不易察觉的漏洞，这些漏洞有可能被不法分子利用，进而操控模型输出结果或窃取机密信息。其次，AI 应用的运行高度依赖于大量的训练数据，一旦数据遭到泄露或被篡改，不仅会损害模型的准确性，更可能对用户的隐私构成<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7fc79dbd0756f72b1709ce5e6bd200c2f6e2d66e16f28ee2b70b646b491c16da82d3edaa5&amp;idx=1&amp;mid=2247501117&amp;scene=27&amp;sn=439f11755cffa684d4f4db1dbc7acb98&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">严重威胁</a>"。此外，AI 应用所处的网络环境同样危机四伏，时常面临各种网络攻击和恶意行为的侵扰。</p><p></p><p>在这股技术革新的热潮中，我们必须清醒地认识到，AI 应用的安全性已成为一个亟待关注的重要问题，并需要采取有效措施予以应对。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/07/b7/07443479789a4fd11b94ca64555e29b7.png" /></p><p></p><p>Palo Alto Networks（<a href="https://www.infoq.cn/article/JuoHJYIdsbN0b3YhQYsH?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">派拓网络</a>"）Primsa Cloud将整个应用生命周期中的应用风险、安全信号和运行时环境等重要因素联系起来，无缝集成云原生应用程序保护的各个方面，在整个应用程序生命周期中提供无与伦比的可视性。从代码到云，Prisma Cloud 的智能驱动方法可确保将安全性融入数字化运营结构中。4 月 23 日 19:00-20:00，InfoQ 技术公开课邀请到了两位来自 Palo Alto Networks（派拓网络）的技术专家，他们将以《派拓网络云原生保护平台对 AI 应用的防护》为主题进行分享。如果你对 CNAPP 的工作原理以及 AI 应用的<a href="https://xie.infoq.cn/article/cf40605cd17e0ac3b8224d08f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">安全解决方案</a>"感兴趣，本期直播你不可错过，欢迎扫描海报二维码预约直播！</p><p></p><p></p><h2>主题及讲师介绍</h2><p></p><p></p><p>本期主题：派拓网络云原生保护平台对 AI 应用的防护</p><p></p><p>分享大纲：</p><p>一、云端AI革命与新兴安全威胁</p><p>云对 AI 的赋能多云安全风险AI数据安全AI攻击类型实现 AI 安全的挑战烟囱式工具的安全问题</p><p></p><p>二、Prisma Cloud：保护您的未来AI</p><p>AI云端资产全面可视化自动化的安全策略管理AI应用的软件供应链安全实时保护 AI 负载运行时AI数据安全结语</p><p></p><p>听众收益：</p><p>1、了解 AI 的可能安全风险</p><p>2、了解 CNAPP 对 AI 安全的平台化保护</p><p>3、重新定义云安全，深入了解代码到云智能，如何应对现代云安全挑战</p><p></p><p><img src="https://static001.infoq.cn/resource/image/22/e6/2255c59993853yy8d37a92c88b4289e6.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HPNCz7mLBvBUxvRpHQaa</id>
            <title>创始团队仅3人、估值最高25亿美元，万字长文讲述RISC-V商业帝国崛起背后的故事</title>
            <link>https://www.infoq.cn/article/HPNCz7mLBvBUxvRpHQaa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HPNCz7mLBvBUxvRpHQaa</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 08:17:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RISC-V, 开源指令集架构, 芯片行业, 开放标准
<br>
<br>
总结: 2015年前，英特尔、ARM和AMD主导芯片行业，但RISC-V的开源指令集架构改变了这一局面。RISC-V的出现使得计算机芯片设计变得更加开放和灵活，吸引了众多公司的关注和参与。开放标准的推广不仅降低了成本，还促进了硬件创新，消除了市场摩擦，推动了开源产业的发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>编者按：&nbsp;截至2015年，芯片行业仍然由英特尔、ARM和AMD三大厂商所控制。但现如今，新的选项正在吸引到使用者们的高度关注与热情接纳。其间究竟起了什么变化？答案就是RISC-V发布了用于芯片开发的开源指令集架构，最终扭转了行业态势。&nbsp;近日，一位名为Jaime Arredondo的博主撰写了一篇万字长文，为我们讲述了RISC-V如何一路成长、最终在芯片生态系统中崛起为新生力量的精彩故事。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae70e347d596a48751fa688d9a09dc90.png" /></p><p></p><p>以下为原文翻译：</p><p></p><h2>开放替代方案的诞生</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6512e6347615fce081ae14ca4c18199.png" /></p><p></p><p>RISC-V的故事可以追溯到2010年3月的加州大学伯克利分校。</p><p>&nbsp;</p><p>Krste&nbsp;Asanovic最初只想建立一个“为期三个月的短期项目”，尝试开发RISC-V指令集作为芯片设计的教学工具，以供程序员及其软件控制计算机硬件。他的初始团队不大，除了自己就只有研究生Yunsup Lee和Andrew Waterman两人。</p><p>&nbsp;</p><p>按之前的惯例来讲，学生们在课堂上主要使用专门的CPU（中央处理单元），但这些CPU对于教学场景来说太过复杂、学习起来不够透明，而且其封闭知识产权设计也导致与他人分享研究成果变得极其困难。</p><p>&nbsp;</p><p>另外，他们还发现很多商业产品其实做得也不怎么样，他们自己完全可以做得更好。</p><p>&nbsp;</p><p>因此，Asanovic、Lee和Waterman开始将RISC-V定位为计算机芯片的全新开放标准。开放RISC-V标准意味着你、我、英特尔甚至是任何其他参与者（包括竞争对手）都可以根据RISC-V指令集设计出符合自身标准的计算机芯片。</p><p>&nbsp;</p><p>RISC-V是一种提供免费许可的开放芯片架构。客户可以针对多种应用场景添加更多扩展与定制芯片，包括云计算、人工智能（AI）、移动、汽车、物联网乃至各类工业应用。</p><p>&nbsp;</p><p>此前，如果一家企业需要在设备上部署简单芯片，唯一的选择就是购买现成产品——但这些芯片上往往搭载大量非必要功能，所以要么速度太慢、要么徒耗能源。</p><p>&nbsp;</p><p>而如果一家公司想要为更复杂的硬件或软件用例设计芯片，则必须向英特尔、ARM或者IBM支付数百万美元的许可费，才能访问到其单一设计指令集。</p><p>&nbsp;</p><p>专有许可，意味着只有财力雄厚的大型企业才有资格做出芯片创新探索。</p><p>&nbsp;</p><p>因此，当他们向公众发布指令集时，学术界、各研究机构乃至谷歌、IBM和英特尔等厂商立刻回以激烈响应，迫使他们在2015年成立了RISC-V基金会。</p><p>&nbsp;</p><p>该基金会的目标是维护RISC-V ISA标准，目前成员已经发展至60多家公司，包括高通、三星、阿里巴巴、Meta、英伟达、微软、英特尔、西部数据、IBM以及谷歌等业界巨头。</p><p>&nbsp;</p><p>于是当初那个计划开发三个月的教学项目，现在每年向市场交付数百万个核心。</p><p>&nbsp;</p><p>RISC-V也在逐渐进军创客领域——最初进展缓慢，但势头却逐年增强。</p><p>&nbsp;</p><p>现在我们已经可以在各类商业产品中找到RISC-V的实现成果，包括智能手表、健身手环、存储产品和显卡等。其自由开放的优势，再加上能大大降低许可费用，明显冲破了买家们想要继续选择专有IP厂商的习惯思维。</p><p>&nbsp;</p><p>RISC-V的开源灵活性也使其成为计算存储巨头然后、西部数据以及中国电子商务巨头阿里巴巴等公司热情欢迎的芯片架构，并得到美国国防先进研究计划局（DARPA）等政府部门的认可。</p><p>&nbsp;</p><p>在我看来，如果一个行业没有开放标准，那么创建此类标准即可让以往无法参与进来的各方成为创新力量。公共资方、学术研究人员和私营企业最有能力分享自己使用或开发的标准和工具。&nbsp;</p><p></p><h2>消除市场摩擦，启动开源产业</h2><p></p><p></p><p>在RISC-V出现之前，类似的指令集主要集中在学术和研究环境当中。正如Krste&nbsp;Asanovic在2014年一篇研究论文中所的出，开放指令集的核心优势就是降低软件端的成本，同时刺激硬件创新——这是以往任何封闭或许可指令集都无法做到的。</p><p>&nbsp;</p><p>Asanovic强调称，“鉴于开放标准与开源软件（以及TCP/IP等网络及Linux等操作系统）已经彻底改变了该行业，为什么最重要的接口之一却仍然保持专有？虽然指令集架构（ISA）的专有性质有其历史或商业层面的原因，但至少从技术角度出发，这种自由、开放ISA的缺失并没有必然的理由。”</p><p>&nbsp;</p><p>简单来讲，即使某个行业可能受到知识产权的保护，但如果能将该行业的标准（软件、设计、协议、方法等）面向所有人开放参与和贡献，那么行业内的各参与者本身以及客户也将获得更好的服务。</p><p>&nbsp;</p><p>通常来讲，专有技术的存在大多出于商业或者历史原因，而开放标准的缺失往往与技术无关。</p><p>封闭式许可证存在严重问题，会减缓创新速度并阻碍更多潜在贡献者的参与：</p><p>&nbsp;</p><p>过度摩擦：封闭许可会阻止其他人在缺少许可证的情况下使用该技术，哪怕是愿意支付许可证费用的买家来说，整个谈判周期也往往需要6到24个月，成本很可能在100万美元到1000万美元之间。相当于直接把学术界、初创公司和规模较小的企业粗暴排除在外。缺少新设计：在获得许可证之后，用户无法设计或改编出新版本，只能单纯使用获得授权的设计方案。无法面向未来：如果授权公司倒闭，其知识与设计也会随之消失，导致其技术成果难以长期维护。</p><p>&nbsp;</p><p>哪怕肯定许可证的商业意义，其存在仍会阻止很多人设计和分享新方法，或者交流其探索期间学到的经验教训，最终扼杀竞争和创新。</p><p>&nbsp;</p><p>从统计数字来看，在为RISC-V做出过贡献的参与者当中，有70%之前从未从英特尔、ARM或任何其他芯片厂商处获得过任何许可。类似的情况在其他行业也同样存在。</p><p>&nbsp;</p><p>而在使用开源创新方案的群体当中，有70%表示不会从行业内购买任何东西。表达这种观点的主要是学术界、中小企业和初创公司，但他们同样可以在业内成为重要的创造者、研究者和推广者。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f64efcd9ab77ed58e4c544cd79947c74.png" /></p><p>对于那些承担不起商业许可证费用、或者拿不出6到24个月进行许可证谈判的人们来说，开放标准成功消除了参与创新的最大障碍。</p><p>&nbsp;</p><p>还有一点反直觉的发现，即这种开放创新也同样符合原有企业的最佳利益。</p><p>&nbsp;</p><p>如果他们想要继续保持对标准的控制而拒绝主动开放标准，那么当其他人建立起开放替代方案时，就如同RISC-V对英特尔、ARM乃至AMD所做的那样，买家们就会纷纷投向开放一派的怀抱。</p><p>&nbsp;</p><p>而这种情况在任何行业都尽早会发生。总之，只要竞争对手意识到你掌握有“独门秘笈”，那他们想尽办法也会得到。</p><p>&nbsp;</p><p>所以之前无法参与到芯片研究中来的人们面对专有解决方案的重重枷锁，只会选择两条路：要么建立自己的开放替代方案，要么参与改进现有解决方案。</p><p>&nbsp;</p><p>而一旦意识到这一点，那些获得初步成功的参与者就会把握一切已经公开、不受产权保护的资产，比如建立起能够建立创新良性循环的社区生态。</p><p>&nbsp;</p><p>因此，正如Seth Godin所说，花时间保护金库中的秘密纯属浪费时间。即使你已经掌握很大的领先优势，活跃社区间的协作也将快速取代任何专有技术成果。</p><p>&nbsp;</p><p>Jeff Bezos说得好，“你的利润，就是我的机遇。”</p><p>&nbsp;</p><p>而RISC-V的缔造者们把这话稍微修改了一下，“你的知识产权，就是我的机遇。”</p><p>&nbsp;</p><p>所以如果想要在由少数参与者控制的市场中创造新机会，请首先开发一套能够用于替代原有封闭知识产权的开源方案，而后与其他合作伙伴慷慨分享。</p><p>&nbsp;</p><p>公共资助的研究实验室和大学就是孕育这类机遇的绝佳温床——这一点不仅体现在科技领域，也同样适用于制药或其他知识产权密集型行业。</p><p>&nbsp;</p><p>前面的逻辑看似过于乐观、纯属理论，但对于那些想要激发经济活力和就业机会的国家来说，释放这种待开发潜力确实是提升“自由收益”与产能的有效方式。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/70/70943a064671cd46cd084bf2442d7c5a.png" /></p><p></p><p>开放创新还能让市场产生以往 无法想象的收益。</p><p>&nbsp;</p><p>在知识产权受严格保护的行业中率先发布易于使用且行之有效的开放标准，将为我们自己和他人带来许多新的机遇。</p><p>&nbsp;</p><p>概括：</p><p>将以往封闭的生态转为开放，能够为70%的中小企业创造机会，吸纳这些受能力或业务规模所限而无法参与贡献的力量。市场规模越大、越分散，开源理念就越能帮助客户实现自我支持。如果某项技术或方案对他人来说至关重要，那么开放替代方案就必然会出现，最终导致所有专有解决方案变得过时或边缘化。因此，引领潮流并与行业积极开展合作，才是符合一切从业企业最佳利益的选择。</p><p></p><h2>开放产业如何起步，钱又从哪来？</h2><p></p><p>&nbsp;</p><p>这方面探索有两个关键前提：一是钱，二是许可。</p><p>&nbsp;</p><p>新产业的萌芽往往源自基础研究，但理论研究永远充满风险，毕竟学术成果转化为商业用途的几率往往相当低。也正因为如此，风险资本家、银行和其他私人投资者往往都不愿参与这一阶段，高风险研究通常只能指望公共部门的慷慨资助。</p><p>&nbsp;</p><p>在下图中，我们可以看到从理论研究到大规模部署的各个融资阶段。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1014366ad8262f477f40168911810d1.png" /></p><p></p><p>资料来源：整理自《创业型国家》（The Entrepreneurial State）</p><p>&nbsp;</p><p>为了开发RISC-V协议，伯克利分校从公共部门（通过DARPA和加利福尼亚州）以及私人投资者（通过英特尔、微软及其他行业赞助者）处获得了资金，这些投资方在五年之内捐赠了1000万美元，希望这套新架构能够在并行计算领域取得进展。</p><p>&nbsp;</p><p>RISC-V并非新鲜技术，其开发原则仍基于至少40年前的计算机架构，因此没有申请任何专利。</p><p>但正是凭借着开放这一特殊属性，RISC-V开始在全球范围内迅速普及。</p><p>&nbsp;</p><p>RISC-V并未取得巨大成功，因为它只能算是一项出色的芯片技术。当时市面上还有很多更好的选择。其真正的特别之处，在于它是首个全球开放标准，允许任何人据此自由开发硬件以运行软件。</p><p>&nbsp;</p><p>这种开放许可模式还允许希望在商业产品中实施或扩展RISC-V的组织能够灵活使用，不必向整个社区披露其更改细节，因此对于嵌入式设备的商业用例形成了强大的吸引力，在很大程度上替代了需要向ARM支付许可费用的同类设计方案。</p><p>&nbsp;</p><p>因此，对于民众及公共机构来说，基础研究的公共资助与开放许可属性至关重要。</p><p>&nbsp;</p><p>RISC-V诞生之后，很快得到一系列项目的应用，包括DARPA资助的诸多其他研究项目以及多家企业的相关用例。</p><p>&nbsp;</p><p>RISC-V的开放许可证成为支持性基础设施，开始为以往无法使用英特尔、ARM或AMD付费许可证进行测试的各类研究课题提供动力。</p><p>&nbsp;</p><p>尽管DARPA在RISC-V ISA立项之初并非第一时间参与，但其资助在随后的发展中发挥了重要作用。</p><p>&nbsp;</p><p>DARPA目前仍在资助一系列关于开源硬件技术的项目。</p><p>&nbsp;</p><p>其他高校及公共资助者也由此吸引了教训，意识到通过支持开放标准的制定，他们的研究和投资将有机会发挥更大作用，并通过创造就业机会、税收以及提高劳动力技能的方式为经济发展贡献力量。</p><p>&nbsp;</p><p>开放创新还能带来网络效应。使用相关技术成果的人越多，其效果也就越好。而且随着越来越多的人为原创技术做出贡献和改进，其价值也将随之提升。基于这种开放式创新思维，免许可制造与创新分发的规模愈发可观，随之产生的收入也开始滋养整个生态体系。</p><p>&nbsp;</p><p>开放标准对纳税人来说也非常有利，因为它让任何人都能参与创新并打造出质量更高、价格更实惠的产品，再不必担心被卷入闻之令人胆寒的专有标准诉讼。</p><p>&nbsp;</p><p>在专有知识产权驱动的行业中，制定开放标准还能推动新晋参与者在现有行业中蓬勃发展，由此打破传统垄断或寡头体制。相较于让少数独角兽公司掌握巨大的全球影响力，开放标准培养出由成百上千家小公司组成的开放生态系统，通过自由竞争杜绝了“太大而不能倒”的组织，创造出更具弹性的健康市场。</p><p>&nbsp;</p><p>以上可以概括为两点：</p><p></p><p>开放标准最有可能从高风险研究、高校以及通过公共拨款及信贷支持的公共资金中产生，也只有这类项目才能承受理论研究对应的高风险。以开源方式分享研究成果非常符合国家服务公共利益及发展健康市场、经济活动及就业空间的使命。宽松的开放许可证不强迫各参与方披露其商业产品变更，这是增强商业公司采用技术成果并吸引他们在开放标准的开发和维护层面做出贡献的好办法。</p><p></p><h2>开放的行业需要怎样的底层社区？</h2><p></p><p></p><p>为了让RISC-V标准得到开源社区的认可，项目创始人们认为它必须通过商业运营这道考验。</p><p>&nbsp;</p><p>为了展示其能力，创始人们在技术报告《保持自由指令集的意义：以RISC-V为例（Instruction Sets should be Free: The case for RISC-V）》中，解释了计算行业如何从可行的自由开放标准芯片协议中受益，其原理与从自由开源软件中受益一样。</p><p>&nbsp;</p><p>例如，这将建立起真正自由且开放的处理器设计市场，而许可标准这副镣铐则会阻止市场的健康发展与自我完善。</p><p>&nbsp;</p><p>他们认为开放标准将带来：</p><p>&nbsp;</p><p>在更多设计者的参与之下，通过自由市场竞争扩大创新规模，包括各类开放与专有实现方案。共享开放核心设计意味着上市速度更快、重用成本更低、错误更少（因为参与者的关注度更高）以及透明度更强，可保证政府机构难以在其中加入秘密后门。对大多数设备来说，处理器价格将越来越便宜，而设备成本被广泛控制在1美元左右将有助于推动物联网的实际落地。</p><p>&nbsp;</p><p>因此，本文将向大家解释开放标准为何意义重大，有望建立起让每位参与者都能受益的独角兽生态系统。</p><p>&nbsp;</p><p>大家可能都听说过那些独角兽初创公司，这个概念是指估值达到10亿美元及以上的私营初创企业。</p><p>&nbsp;</p><p>但我所定义的独角兽生态系统，是指由独角兽初创企业参与的开放式转变。独角兽生态系统代表能够创造10亿美元或更高收入市场的解决方案。但这些技术方案由开源项目及社区所驱动，并非由单一初创公司所实现，因此期间将有众多参与者、而非单一参与者随之快速成长。</p><p>&nbsp;</p><p>下面来看一些真实案例，这些组织通过允许其他人基于自己的方案进行构建，最终创造出了单靠自身商业化所无法企及的总体价值回报：</p><p>&nbsp;</p><p>Linux支持99%的互联网服务器，业务总体量超过500亿美元。WordPress得到43%的互联网网站使用（截至2023年1月），其公司价值70亿美元，但每年由其用户创造的收入约达1400亿美元。RepRap为3D打印企业们提供数十亿美元的收入，并催生出总值数百亿美元的3D打印市场。RISC-V是一种开放芯片架构生态系统，其业务规模每年翻一番，预计将从2018年的5200万美元增长至2025年的11亿美元。</p><p>&nbsp;</p><p>独角兽初创企业最受私人投资者的欢迎，因为他们可以把成果打包出售给那帮行业巨头。但出售独角兽生态则既不轻松、也不太划算，毕竟它们是由众多开源解决方案所共同构成。</p><p>&nbsp;</p><p>但另一方面，独角兽生态系统对公共资助者来说则非常合适，因为由此衍生出的研究成果随后可以转化为市场，创造新的就业机会、新的技能、新的税收、更高的产品质量，也能比封闭知识产权或独角兽初创公司更快降低技术普及成本。</p><p>&nbsp;</p><p>也就是说，加入这样的生态系统，能够让新的开放参与者站在巨人的肩膀上，享受原本只供独角兽们享用的技术投资。RISC-V正在取代英特尔和高通，而投资像SiFive这样的RISC-V初创企业也同样能够带来不错的价值回报。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/75/7545df6dad88c8c2e908b26d55ca9cfa.png" /></p><p></p><p>&nbsp;</p><p>也就是说，如果大家想围绕开放标准建立起产业生态系统，就必须展示其实践可行性。而这种可行性的根源，就是通过向新参与者开放市场来增加创新、减少对个别垄断者的依赖、扩大定制范围、降低技术实现成本、加快上市速度或降低价格价格，最终让成果能够为更多人所轻松接受。</p><p>&nbsp;</p><p>此外，通过建立产业生态系统，公共投资者可以创造出以往不可能存在的新市场和新的竞争关系，同时提高质量并降低价格。同时，由此建立的新市场将带来更公开的竞争与更平等的参与机会，打破少数私营企业垄断产业的可能性。</p><p></p><h2>如果没有知识产权保障，生态系统成员如何谋求生存、稳定发展？</h2><p></p><p></p><p>但对于开放产业，人们普遍抱有这样的疑问：在缺少专有知识产权保护的情况下，参与者凭什么获得资金和发展空间？</p><p>&nbsp;</p><p>要回答这个问题，我们不妨看看SiFive的RISC-V创业之路。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b92dbc9d50f459d637cd8e5faedab805.png" /></p><p></p><p>SiFIve是Krste&nbsp;Asanovic创立的公司。在研究RISC-V架构的同时，他们也做出了很多实现，并让多位学生参与到项目中来。正是这种方式，保证外部人员也能轻松上手并利用RISC-V完成实际芯片开发。</p><p>&nbsp;</p><p>他们意识到半导体行业正身处一场完美风暴当中——摩尔定律即将结束，新技术的开发变得越来越昂贵，能够实现新设计并从中盈利的公司也将越来越少。</p><p>&nbsp;</p><p>因此，Asanovic其他几位联合创始人开始围绕RISC-V提供咨询服务，这也帮助他们感受到了为物联网开发定制芯片这一持续增长的市场需求。所有这些设备都需要搭载处理器，而各类解决方案又不可能全部采用相同的处理器，因此他们决定把咨询业务做成一家企业。</p><p>&nbsp;</p><p>在这部分市场中，他们牢牢把握住了核心原则：芯片产品的需求总量确实在增长，但这种增长会体现在众多彼此分散的市场当中。</p><p>&nbsp;</p><p>传统半导体商业模式（根据一种设计方案制造并销售数百万块处理器）只适用于计算机和移动手机市场，未来的市场形态将转化为成百上千种需求量相对较小的设计方案。</p><p>&nbsp;</p><p>在SiFive公司，他们开始研究其技术原理。芯片的传统用户正在转型为新的制造商。谷歌、微软、亚马逊等众多巨头都在设计和制造自己的芯片——不是为了出售给他人，而是要在自己的产品中使用。也只有这样，他们才能实现标准商用芯片所不具备的功能。</p><p>&nbsp;</p><p>因此，SiFive的机会就是找出如何帮助小型企业和初创公司进行定制化芯片设计，进而发明出具有新功能的新型处理器。</p><p>&nbsp;</p><p>Asanovic表示，“我们坚信那是一片拥有广阔开发空间的新大陆。但问题在于开发定制化芯片设计的门槛太高，很多伟大的想法根本不可能转化为产品。而SiFive的目标，就是解决好这个问题。”</p><p>&nbsp;</p><p>于是SiFive的商业模式就成了：快速开发新型芯片组，并帮助客户以极低的成本将其投入生产。</p><p>&nbsp;</p><p>这无疑为创客、初创公司和中型企业开辟了新的视角。芯片的所有设计文件都公开在GitHub之上，这种开放性在半导体行业可谓是前所未见。</p><p>&nbsp;</p><p>不需要保密协议或者律师，SiFive的商业模式天然成本低廉，自然意味着为客户提供更多实惠。</p><p>&nbsp;</p><p>这大大降低了原型设计的前期投入，使得初创公司、制造商甚至是技术爱好者也能负担得起自己的第一块芯片，在其中试验自己的技术灵感。</p><p>&nbsp;</p><p>其中大多数尝试当然会失败，但这没关系。</p><p>&nbsp;</p><p>只要尝试的人足够多，新想法的数量和质量就会提高，从而增加案例取得成功的几率。对于那些已经成功的公司，SiFive会与他们一同扩展并帮助向客户每年交付数百万块芯片。</p><p>&nbsp;</p><p>这就是他们赚钱的方式。他们把赌注押在客户提出的成千上万好点子上，而非依靠自己能够想到的少数设计路线。</p><p>&nbsp;</p><p>接下来要做的，就是引导客户为公司的发展付费。可有时候，这等体量的收入并不足以支持一家正在快速增长的公司，特别是SiFive这种立足老牌烧钱产业半导体行业的公司。</p><p>&nbsp;</p><p>归根结底，投资者为什么要支持缺乏知识产权保护的项目？</p><p>&nbsp;</p><p>总结来说，就是只要投资者能证明对开源技术的需求仍在不断增长且公司的市场定位足够强大，他们就有信心掏出真金白银支持开源技术。</p><p>&nbsp;</p><p>从2019年到2022年，SiFive的业务规模增长了1477%，也借此从投资者手中顺利接过3.655亿美元。</p><p>&nbsp;</p><p>还有更多以RISC-V为基础的初创公司拿到了私人融资，包括Hex Five、Codasip和Dover Microsystems。此外Ant Micro和Andes Technology则在不借助外来融资的情况下，茁壮成长为初创企业。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/db/dbdfbb83ebfd7ec50593994f27aaf0a7.png" /></p><p></p><p>来自RISC-V生态系统中另一家快速增长的企业——Codasip公司的芯片。</p><p></p><h2>怎样驱动生态系统进行自我维护？</h2><p></p><p></p><p>在启动开源项目之前，很多领导者往往心存疑虑：虽然已经了解开源项目的价值，但他们不希望项目乃至整个生态系统的管理和维护任务把自己硬生生拖垮。</p><p>&nbsp;</p><p>而让社区为此主动贡献其实非常困难，毕竟这是一项要求贡献者付出大量资源和时间的工作，根本不可能随时随地找到适合的人选。</p><p>&nbsp;</p><p>RISC-V就是个很好的例子，说明了如何驱动社区参与，以及为何需要有意识地加以引导。</p><p>让我们首先看看RISC-V技术社区是如何形成的，之后再了解他们在邀请和激励成员参与方面采取的具体步骤。</p><p>&nbsp;</p><p>RISC-V基金会成立于2016年，旨在建立一个基于RISC-V标准的开放、协作软硬创新者社区。</p><p>该基金会是一家由其成员控制的非营利公司，发展方向在于推动RISC-V的初步市场采用。</p><p>2018年11月，RISC-V基金会宣布与Linux基金会合作。</p><p>&nbsp;</p><p>作为此次合作的一部分，Linux基金会为RISC-V International提供运营、技术与战略支持，具体涵盖成员管理、会计、培训计划、基础设施工具、社区外展、营销、法律及其他开放标准服务与专业知识。</p><p>&nbsp;</p><p>而且考虑到标准的开放性，以基金会的形式建立社区也可以保护各成员的投入免受地缘政治的干扰。</p><p>&nbsp;</p><p>2018年至2019年，RISC-V成员曾因出口黑名单而受到中美紧张关系的打击。这种不确定性在全球各地纷纷引发担忧，参与者希望对RISC-V的投资能够保证知识产权访问的连续性，否则很难有信心为其长期规划战略投资。</p><p>&nbsp;</p><p>2020年，RISC-V基金会决定迁入瑞士，并转向更具包容性的会员结构，借此平息人们关于开放合作模式可能受到政治干扰的担忧。</p><p>&nbsp;</p><p>RISC-V组织目前约有三分之一成员来自北美，三分之一来自欧洲，37%来自亚太地区。</p><p>&nbsp;</p><p>亚太地区的成员也代表着RISC-V增长最快的应用场景——印度和巴基斯坦等国家已经采用RISC-V作为本国芯片开发的指定指令集架构。</p><p>&nbsp;</p><p>2019年，RISC-V标准的创始作者及所有者放弃所有权，并将权利转让给RISC-V基金会。曾担任IBM副总裁领导开放基础设施项目的Calista Redmond被任命为基金会CEO。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c50556ea46cac62dc8546ecee93e512a.png" /></p><p></p><p>2022年，RISC-V基金会已经成为一个拥有1400多名成员的强大社区，由各国芯片制造商、设计商及学术机构组成，致力于通过开放标准协作为未来50年的计算机创新与设计制定联合路线图。</p><p></p><h3>这种合作是如何实际运行的？谁可以参与基金会并投票表态？</h3><p></p><p>合作的第一步，就是构建起各方赖以完成工作的必要技术方案。RISC-V是这样走的，许多其他开源项目（包括Linux、WordPress、Wikimedia以及众多开放编程语言）也是这样走的。</p><p>第二步是了解各参与方共同关注的问题和愿景。</p><p>&nbsp;</p><p>RISC-V基金会在这方面做得相当不错，其“成为会员”页面上解释了加入基金会的好处：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/67/6723c131cf340848135507ea58ef350e.png" /></p><p></p><p>&nbsp;对RISC-V的成员们来说，聚集在基金会或者联盟周围，有助于各成员紧密围绕开放标准共同努力、投入资源以加速整个行业的发展。</p><p>&nbsp;</p><p>当然，根据会员角色建立的不同会员级别及相关资质也非常重要。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6e7be0237107d66055d78b743c1adc4.png" /></p><p></p><p>RISC-V主要面向企业、学术界、非营利组织或个人，根据员工数量及法人实体类型，各个组织均有不同的福利与会费额度。</p><p>&nbsp;</p><p>具体会员资质将随着社区成熟度以及所涉及的商业利益而动态变化。</p><p>&nbsp;</p><p>2016年，RISC-V会员可免费申请，等待审查后即可通过。2018年，他们开始将会费调整为个人免费、商业公司2.5万美元。2020年，他们将大企业的会员费增加至25万美元，并保留个人及学术界成员的免费资格。这笔费用保证基金会能够向RISC-V的15名员工（截至2023年）支付工资，并用于促进、维护和连接围绕该标准的生态系统、文档与技术的持续发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c71324d4a17ba5e5ef9d313130c7301.png" /></p><p></p><p>各成员随后可以定期参加技术与社区会议、加入或提交项目贡献，也可访问合作伙伴及其他精选来源的学习资料。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/993953a40f313725ecc739ac8ad25a26.png" /></p><p>&nbsp;</p><p>基金会之下还有多个不同工作小组负责技术任务、营销、编程研讨会或其他特别兴趣小组，具体涵盖汽车、学术界、Android或图形等领域。</p><p>&nbsp;</p><p>RISC-V基金会成员生产的产品归谁所有？谁又可以使用？</p><p>&nbsp;</p><p>RISC-V的知识产权由各成员共同开发并贡献。一旦知识产权在全球范围内以开源方式被用于软件及硬件设计，内容就会被永久开放并供所有人随时查阅。</p><p>&nbsp;</p><p>然而，只有RISC-V International的成员才能投资批准变更，且只有会员组织才能使用注册商标及徽记。</p><p></p><h2>总结</h2><p></p><p>RISC-V成功与最具行业影响力的参与者们合作，将开放标准转化成了蓬勃发展的生态系统。</p><p>他们服务于快速增长的芯片处理器市场，并为整个世界做出了巨大贡献。</p><p>&nbsp;</p><p>RISC-V已经成为行业中的重要联盟，证明无需封闭知识产权也能发展出欣欣向荣的产业与充满活力的初创公司。但前提是，必须设定明智且目标明确的发展计划。</p><p>&nbsp;</p><p>结合RISC-V的现实策略，我们也许可以把握以下几个要点以供所在组织借用、修改和调整：</p><p>&nbsp;</p><p>要点一：如果所处行业依赖于封闭协议及工具，请投资建立开放协议及工具。此举有望向以往无法参与的潜在贡献者敞开大门，扩大合作范围以推动创新与市场发展。</p><p>&nbsp;</p><p>要点二：向那些仍依赖于封闭协议或工具，以及希望使用但却无法承担成本的各方分享新的开放协议或工具。如此一来，就能引导各方在开放项目的改进与扩展方面进行协作、相互扶持。</p><p>&nbsp;</p><p>要点三：提供开放许可证，允许他人出售开放项目的衍生品，且不强迫他们披露变更细节。这增加了开放协议被商业公司采用的机会，有助于鼓励他们开发并维护开放标准。</p><p>&nbsp;</p><p>要点四：建立产业生态系统的前提，在于明确解释开放协议或工具的可行性。开放标准有何作用？向新晋参与者开放市场是否有助于扩大创新规模？是否有助于减少对少数巨头的依赖？是否有助于扩大可定制空间？是否有助于降低成本或者加快上市速度？或者能以更低的价格向更多人提供产品？这种种助益相结合将形成不可阻挡的普及之力。</p><p>&nbsp;</p><p>要点五：为了让生态系统参与到开放项目的维护和资助中来，请邀请那些依赖开放项目并愿意做出开发贡献的个人/实体。根据相应的商业利益、能力和动机，具体设计适合各参与方的会员级别、角色和会费。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://boldandopen.substack.com/p/how-a-group-of-berkeley-researchers">https://boldandopen.substack.com/p/how-a-group-of-berkeley-researchers</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0K8ODRJO9JqjMi2lWHHP</id>
            <title>Devin发布半月后，开源领域围攻编码智能体 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/0K8ODRJO9JqjMi2lWHHP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0K8ODRJO9JqjMi2lWHHP</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 08:13:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 编码智能体, 开源领域, 科研领域
<br>
<br>
总结: 大模型的快速发展使得了解最新技术成为必修课，编码智能体和开源领域的项目不断涌现，科研领域也有新突破。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，普林斯顿大学&nbsp;NLP&nbsp;组发布了&nbsp;SWE-agent，这是继&nbsp;Devin&nbsp;发布之后，编码领域智能体的又一突破。在这短短的一月内，Devin、OpenDevin、Devika、Autodev、SWE-agent等项目集中攻克编码智能体（Code&nbsp;Agent）方向，编码领域已经成为智能体首要探索的领域。编码智能体是&nbsp;Copilot&nbsp;模式产品的下一个阶段吗？编码智能体商业化应用仍然面临经济成本账和用户体验的问题，这会是智能编码体下一步重点攻克的方向吗？AI&nbsp;agent还会有其他典型领域突破吗？InfoQ研究中心与大家一起关注。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>4&nbsp;月&nbsp;5&nbsp;日，来自MIT、普林斯顿等高校的华人团队推出了性价比极高的大语言模型&nbsp;JetMoE-8B。其在推理过程中只有&nbsp;22&nbsp;亿个激活参数，实现了不到10万美元的训练成本。尽管预算有限，JetMoE-8B&nbsp;却展现出了超越&nbsp;Meta&nbsp;LLaMA2-7B&nbsp;的能力，后者拥有庞大的训练资源。</p><p></p><h4>开源领域</h4><p></p><p>4&nbsp;月&nbsp;1&nbsp;日，澜舟科技开源了&nbsp;孟子3-13B&nbsp;大模型。该大模型在中英文语言、数学、编程方面表现较为出色，同时支持学术研究与免费商用。4&nbsp;月&nbsp;2&nbsp;日，通义千问团队推出&nbsp;Qwen1.5-32B&nbsp;和&nbsp;Qwen1.5-32B-Chat。相较于其&nbsp;72B&nbsp;的模型，32B&nbsp;模型的内存占用大幅减少，运行速度显著提升。通义千问团队希望通过Qwen1.5-32B的开源，能为企业和开发者提供更具性价比的应用落地模型选项。4&nbsp;月&nbsp;3&nbsp;日，元象开源了&nbsp;XVERSE-MoE-A4.2B&nbsp;大模型。该大模型采用&nbsp;MoE&nbsp;结构，其激活参数量为&nbsp;42&nbsp;亿。相比于XVERSE-13B-2&nbsp;大模型，减少了&nbsp;70%&nbsp;的计算量与&nbsp;50%&nbsp;的训练时间。</p><p></p><h4>科研领域</h4><p></p><p>斯坦福大学和麦克马斯特大学的研究人员开发了用于设计抗生素分子的生成式&nbsp;AI&nbsp;模型&nbsp;SyntheMol。该模型通过蒙特卡洛树搜索技术和大量的分子片段库中的数据，快速筛选出具有潜力的化合物，显著提高了新药发现的效率和成功率，为未来抗生素的研发提供了新的方向。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>OpenAI在官方网站上推出其创新的自定义声音合成技术&nbsp;Voice&nbsp;Engine。这项技术允许用户仅通过提供一段大约15秒的声音样本，便能够创造出与原始声音极为相似的全新音频文件。PixVerse&nbsp;推出了“角色-视频”新功能，允许用户上传图片并生成保持角色一致性的动态视频。这项功能通过丰富的背景和角色动态，使生成的视频内容生动且连贯。虽然有时生成效果会与原素材有一定差异，但是可以通过调整&nbsp;prompt&nbsp;进行改善。香港中文大学（深圳）附属第二医院使用中文医疗大模型华佗&nbsp;GPT&nbsp;进行智能导诊服务。未来，除了智能导诊外，华佗GPT&nbsp;还将在医院探索智能预问诊、专科咨询、随访、病案之间等应用场景。昆仑万维开放天工&nbsp;SkyMusic&nbsp;AI&nbsp;音乐生成大模型测试邀请，并计划于4月17日全面向社会开放使用。</p><p></p><h4>智能体</h4><p></p><p>普林斯顿大学&nbsp;NLP&nbsp;组发布了开源的AI程序员系统&nbsp;SWE-agent。它能够在GitHub存储库中自主解决问题。基于GPT-4&nbsp;等大模型，SWE-agent&nbsp;在&nbsp;SWE-bench测试集上达到了与闭源&nbsp;AI&nbsp;程序员&nbsp;Devin&nbsp;相似的准确度。SWE-agent&nbsp;通过智能体-计算机接口（ACI）设计，可以执行代码浏览、编辑和执行等任务，显著提高了软件开发过程中的自动化水平。阿里云正在内部积极推广通义灵码的智能系统来协助程序员进行代码编写、阅读、BUG&nbsp;检测和代码优化等多项任务。相关人士表示，未来&nbsp;20%&nbsp;的代码将由通义灵码编写，程序员将更多集中在系统架构设计和关键业务开发的工作。优必选正在探索将其人形机器人Walker&nbsp;S与文心大模型相结合，以此提升Walker&nbsp;S在具身智能领域的应用能力。通过整合文心大模型，Walker&nbsp;S不仅保持了其原有的多模态感知和运动控制功能，还新增了更为先进的意图识别和细致规划的能力。</p><p></p><h3>基础设施</h3><p></p><p>3&nbsp;月&nbsp;31&nbsp;日，无问芯穹团队首次召开产品发布会，推出无穹&nbsp;Infini-AI&nbsp;大模型开发与服务平台并向个人与企业开放注册并进行了客户案例展示。该平台实现多模型与多芯片间的软硬件协同优化和统一部署，并支持二十余个主流模型与十余种计算卡。谷歌更新&nbsp;Transformer&nbsp;架构，推出&nbsp;Mixture-of-Depths（MoD）。MoD&nbsp;架构的核心创新在于动态分配计算资源，以便在模型中跳过一些不必要的计算，从而显著提高训练效率和推理速度。这种方法通过在输入序列的特定位置优化不同层次的模型深度中的资源分配，使模型能够专注于更重要的信息。华为诺亚方舟实验室推出了一种新的大语言模型架构帝江，该模型基于频域自注意力变换核，实现了原始自注意力的线性逼近。帝江模型在保持与&nbsp;LLaMA-7B&nbsp;相当的精度的同时，仅需&nbsp;1/10-1/50&nbsp;的训练数据，实现了最多&nbsp;5&nbsp;倍的推理加速。北京大学的团队在论文《Hourglass&nbsp;Tokenizer&nbsp;for&nbsp;Efficient&nbsp;Transformer-Based&nbsp;3D&nbsp;Human&nbsp;Pose&nbsp;Estimation》中提出&nbsp;HoT&nbsp;框架。这是一种高效的三维人体姿态评估的框架，通过沙漏&nbsp;Tokenizer&nbsp;来减少视频姿态&nbsp;Transformer&nbsp;的高计算量。HoT&nbsp;能够集成到现有&nbsp;MotionBERT&nbsp;等模型中，在不损失精度的情况下降低近&nbsp;40%&nbsp;的计算量。UIUC与LMFlow团队在论文《LISA:&nbsp;Layerwise&nbsp;Importance&nbsp;Sampling&nbsp;for&nbsp;Memory-Efficient&nbsp;Large&nbsp;Language&nbsp;Model&nbsp;Fine-Tuning》中针对&nbsp;LoRA&nbsp;的局限性进行了研究，并提出了一种新的微调方法&nbsp;LISA（Layerwise&nbsp;Importance&nbsp;Sampled&nbsp;AdamW）。LISA&nbsp;的空间消耗与&nbsp;LoRA&nbsp;相当或更低，计算速度比&nbsp;LoRA&nbsp;快50%，并且由于其参数激活较少，对更深的网络和梯度检查点技术更为友好。同时，LISA&nbsp;的收敛性质更优，并且理论性质更易于分析。</p><p></p><h3>其他</h3><p></p><p>国家网信办于&nbsp;2024&nbsp;年&nbsp;4&nbsp;月&nbsp;2&nbsp;日公布了已备案的生成式人工智能服务大模型信息。截至&nbsp;3&nbsp;月&nbsp;28&nbsp;日，共有&nbsp;117&nbsp;个大模型完成了备案程序。其中，北京有&nbsp;51&nbsp;个，上海有&nbsp;24&nbsp;个，广东有&nbsp;19&nbsp;个。4&nbsp;月&nbsp;1&nbsp;日，OpenAI&nbsp;放开使用限制，用户无需注册即可使用&nbsp;ChatGPT。在放开注册使用限制的同时，OpenAI&nbsp;也加强了内容保护措施，以防止某些不适宜的内容被用户生成。此外，基于GPT-3.5&nbsp;的ChatGPT仍保持免费使用外，用户可以订阅付费以使用&nbsp;GPT-4。</p><p></p><p></p><p>报告预告</p><p>Sora来袭，国内如何迅速跟上？开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，其能力是否有所提升和刷新？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？InfoQ研究中心即将发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，即将给出答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9b3c569c62a571715d811e7121db70f.png" /></p><p></p><p></p><p>每周动态更新和季度报告后续均会在&nbsp;AI&nbsp;前线上发布，欢迎持续关注&nbsp;AI&nbsp;前线公众号，共同见证大模型行业的发展与突破！</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/373298ec65b194910c71edca6409a559.png" /></p><p></p><p></p><p></p><h4>活动推荐</h4><p></p><p>AICon&nbsp;全球人工智能与大模型开发与应用大会暨通用人工智能开发与应用生态展将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1a0f9425899db37a9189885eeca9625.jpeg" /></p><p></p><p>今天是会议&nbsp;9折购票阶段，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gcoRcFByO7NLUP7dfLfh</id>
            <title>成立7年0交付，这家OpenAI投资的自动驾驶软件公司烧光15亿元后宣布倒闭</title>
            <link>https://www.infoq.cn/article/gcoRcFByO7NLUP7dfLfh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gcoRcFByO7NLUP7dfLfh</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 08:11:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 自动驾驶汽车软件公司, 倒闭, 裁员, OpenAI
<br>
<br>
总结: 自动驾驶汽车软件公司Ghost宣布倒闭，裁员上百人，曾受到OpenAI投资关注，但由于资金短缺和技术支持不足，最终倒闭。Ghost Autonomy公司成立于2017年，致力于开发自动驾驶软件，但在自动驾驶领域竞争激烈，面临资金压力。公司创始人John Hayes曾创立成功上市的Pure Storage，希望通过软件实现自动驾驶技术。公司曾获得大额投资，但最终转向碰撞预防技术，仍未能维持业务。 </div>
                        <hr>
                    
                    <p></p><h2>自动驾驶汽车软件公司Ghost宣布倒闭，裁员上百人</h2><p></p><p>&nbsp;</p><p>近日，据多家外媒报道，OpenAI投资的一家自动驾驶公司现已关闭。</p><p>&nbsp;</p><p>这家总部位于加利福尼亚州山景城的Ghost Autonomy 公司成立于2017年，是一家为汽车制造商合作伙伴开发自动驾驶软件的初创公司。</p><p>&nbsp;</p><p>该公司上周在其网站上宣布，“Ghost Autonomy 已于 4 月 3 日关闭全球业务并关闭该公司。我们对 Ghost 团队在实现软件定义消费者的使命中所取得的实质性技术创新和进展感到自豪。考虑到当前的融资环境以及自主开发和商业化所需的长期投资，长期盈利之路尚不确定。我们正在为我们团队的创新探索潜在的长期目标。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6a9f00bca6e344f4d5b8e0137bc1c0a.png" /></p><p></p><p>该公司声明继续说道：“感谢帮助将Ghost 的愿景变为现实的员工、投资者和合作伙伴。我们非常感谢您对Ghost 一路以来的支持。”</p><p>&nbsp;</p><p>Ghost Autonomy由 Hayes 和 Volkmar Uhlig 创立，成立7年间，该公司拥有约 100 名员工，在山景城、达拉斯和悉尼均设有办公地。</p><p>&nbsp;</p><p>公司联合创始人兼CEO 约翰·海耶斯 (John Hayes) 于 2009 年创立了Pure Storage，并于 6年后将该公司成功上市。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79080c11a4e0d5e5341ea1d244502608.png" /></p><p></p><p>&nbsp;Ghost联合创始人兼CEO J ohn Hayes</p><p>&nbsp;</p><p>作为 Pure 的首席架构师，他利用消费行业向闪存的过渡来重新构想数据中心存储，并发明了超快的闪存存储解决方案，目前由世界上最大的云和电子商务提供商、金融和医疗机构、科学研究组织和政府运营。Hayes希望Ghost能与Pure一样，通过使用软件实现近乎完美的可靠性并重新定义商用消费类硬件。</p><p>&nbsp;</p><p>与许多试图将自动驾驶汽车技术商业化的初创公司一样，Ghost 多年来也改变了经营策略。这家初创公司最初名为Ghost Locomotion，成立于 2017 年。两年后，该公司首次公开亮相，获得了 Founders Fund 的 Rabois、Khosla Ventures 的 Vinod Khosla 和 Sutter Hill Ventures 的 Speiser 等公司的 6370 万美元总投资，以及计划开发一个套件，允许私人乘用车在高速公路上自动驾驶。该公司表示将于 2020 年提供该技术。</p><p>&nbsp;</p><p>Ghost一直在强调，它的目标是为普通客户购买的消费汽车提供技术。当时，Ghost 总法律顾问杰奎琳·格拉斯曼 (Jacqueline Glassman) 解释道：“与机器人出租车或送货服务不同，Ghost 为人们每天拥有并驾驶上班的汽车提供自动驾驶服务。 Ghost 以软件形式提供服务，专为大众市场而设计，与汽车制造商合作，将自动驾驶带入主流。”</p><p>&nbsp;</p><p>此外，该公司还热衷于指出，他们的发展路线与其竞争对手不同，他们重点关注通用人工智能，而不是使用图像本地化来识别潜在障碍，以确保自动驾驶汽车避开它们。Ghost 更专注于跟踪未来场景中的像素簇——简单来说，障碍是什么并不重要，重要的是它是一个障碍，且要能够避开它。</p><p>&nbsp;</p><p>但在2020年这一最后期限到来之后，Ghost改变了最初策略，改为专注于碰撞预防技术，并在 2021 年又筹集了 1 亿美元。</p><p>&nbsp;</p><p>D 轮融资由 Sutter Hill Ventures 领投，Founders Fund 和 Coatue 跟投。 Hayes 早在 2021 年接受TechCrunch媒体采访时表示，该初创公司并未完全关闭消费套件模型的大门，而是将注意力转向通用防撞技术，以更快地进入市场。</p><p>&nbsp;</p><p></p><h2>成立7年0交付，因被Open AI投资受到关注</h2><p></p><p>&nbsp;</p><p>而事实上，Ghost远大的理想和愿景并没有足够先进和创新的技术作为支撑。虽然成立7年间融到了2.2亿美元（约合15亿元人民币），但在自动驾驶这个极度烧钱的领域里，这点钱可谓杯水车薪。</p><p>&nbsp;</p><p>去年，Hayes 在《福布斯》杂志上说道：“过去十年，汽车公司在软件开发方面一直举步维艰，无论是在执行方面还是在客户采用方面。消费者对汽车中的新技术功能非常失望。对于软件公司来说，软件实际上也很难开发——即使是硅谷的巨头。”</p><p>&nbsp;</p><p>这样一家无论是技术还是创始人背景都没有太多惊人之处的公司，真正被人们关注是因为它被AI巨星公司OpenAI投资了。</p><p>&nbsp;</p><p>去年11月，也就是距离该公司关闭仅五个月前，这家初创公司通过 OpenAI 初创基金与 OpenAI 合作，以尽早获得 Microsoft 的 OpenAI 系统和 Azure 资源。 Ghost还获得了OpenAI的500万美元投资。去年，该公司完成了 5500 万美元的首轮融资，投资方包括 Founders Fund 的 Keith Rabois 和 Sutter Hill Ventures 的 Mike Speiser 等早期投资者。</p><p>&nbsp;</p><p>OpenAI和Ghost同时宣布了这一举措，并计划将大型语言模型（ChatGPT背后的技术）引入自动驾驶。</p><p>&nbsp;</p><p>当时，Hayes 宣扬了该公司计划探索多模态大语言模型 (LLM)（可以理解文本和图像的人工智能模型）在自动驾驶中的应用。他认为大语言模型提供了一种理解“长尾”的新方法，为当前模型不足的复杂场景添加推理。</p><p>&nbsp;</p><p>OpenAI 首席运营官兼 OpenAI 初创基金经理 Brad Lightcap也为其站台称：“多模态模型有可能将大语言模型的适用性扩展到许多新的用例，包括自动驾驶和汽车。多模态模型能够通过结合视频、图像和声音来理解并得出结论，因此可能会创造一种新的方式来理解场景并驾驭复杂或不寻常的环境。” 。</p><p>&nbsp;</p><p>在倒闭前，Hayes还向TechCrunch发了一封邮件表示，表示该公司已经完成了一款高速公路驾驶产品，并正在通过他所说的“最后一英里交付”在城市环境中移动。</p><p>&nbsp;</p><p>但最终，Hayes表示，“多年来，Ghost Autonomy 无法为自己融资，也无法达到将产品推向消费汽车市场所需的工程强度水平”。&nbsp;</p><p>&nbsp;</p><p>也就是说，成立7年，烧光了2.2亿美元后，Ghost Autonomy仍然没有一款产品推向市场。</p><p></p><h2>我们距离L5级自动驾驶还有多远？</h2><p></p><p>&nbsp;</p><p>很多人都希望汽车开发周期更快，但这个行业需要第一次就做好——客户不会容忍半成品的产品。要使最复杂的行业之一正常运转，需要令人难以置信的跨领域工程，而这只能通过超乎寻常的时间来实现。</p><p>&nbsp;</p><p>自动驾驶汽车的发展并不顺利。除了Ghost倒闭的消息外，最近的一个例子是，硅谷最大的巨头之一苹果公司就其在圣克拉拉裁员 600 多名员工，根据州政府提交的文件，苹果公司位于圣克拉拉的8个办公室的员工被裁掉。</p><p>&nbsp;</p><p>去年秋天，通用汽车Cruise 自动驾驶部门因其中一辆车辆撞上并拖拽行人而受到监管审查，导致这家美国汽车制造商裁员并削减10 亿美元支出。但通用汽车首席执行官玛丽·巴拉 (Mary Barra) 在二月份表示，“自动驾驶技术带来了巨大的好处”，而且该公司拥有“极其宝贵的资产”。</p><p>&nbsp;</p><p>据研究公司 GlobalData&nbsp;的一份报告称，驾驶员至少在 20 年内将无法体验完全自动驾驶汽车的道路，通往L4 级及以上自动驾驶汽车的道路“可能会很缓慢”。</p><p>&nbsp;</p><p>L5 级自主权涉及不需要任何人机交互的自动驾驶汽车，这意味着最终部署时，它们将没有方向盘或踏板。&nbsp;</p><p>&nbsp;</p><p>报告指出：“然而，可以合理地假设，2035 年新推出的 Level 4 车辆将大大优于 2025 年推出的车辆，因此通往 Level 5 的道路可能是渐进的。”</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.kron4.com/news/bay-area/driverless-car-software-company-shuts-down-in-mountain-view/">https://www.kron4.com/news/bay-area/driverless-car-software-company-shuts-down-in-mountain-view/</a>"</p><p><a href="https://medium.com/authority-magazine/vehicles-of-the-future-john-hayes-of-ghost-autonomy-on-the-leading-edge-technologies-that-are-57bad9c33cc4">https://medium.com/authority-magazine/vehicles-of-the-future-john-hayes-of-ghost-autonomy-on-the-leading-edge-technologies-that-are-57bad9c33cc4</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I5qG0OXKToAwFi54XiRs</id>
            <title>AI坦白局：技术飞跃背后的企业实战 | InfoQ圆桌实录</title>
            <link>https://www.infoq.cn/article/I5qG0OXKToAwFi54XiRs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I5qG0OXKToAwFi54XiRs</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 02:57:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术领域, 人工智能, 大数据, 云计算
<br>
<br>
总结: 当前技术领域的发展速度快速，人工智能、大数据、云计算等前沿技术正在重塑生产生活方式，推动智能家居、自动驾驶汽车等领域进步，带来便利。然而，技术发展也带来数据隐私、网络安全等挑战，需要审视技术发展方向。未来，技术创新将继续改善生活。 </div>
                        <hr>
                    
                    <p></p><p>当前技术领域的发展速度可谓日新月异，为我们带来了前所未有的可能性。人工智能、大数据、云计算等前沿技术正以前所未有的深度和广度重塑着我们的生产生活方式。在人工智能的驱动下，我们能够更高效地处理海量信息，优化决策过程；大数据技术的崛起，使我们能够深入挖掘数据价值，洞察市场趋势；而云计算技术的发展，则为我们提供了弹性、高效的计算和存储服务。这些技术的融合应用，推动了智能家居、自动驾驶汽车、远程医疗等领域的显著进步，为我们的生活带来了诸多便利。我们有理由相信，随着技术的不断创新和应用，未来的生活将会变得更加美好。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而，与此同时，技术的快速发展也带来了诸多挑战和风险。数据隐私泄露、网络安全威胁等问题日益凸显，要求我们必须重新审视技术的发展方向和应用范围。技术的更新换代速度不断加快，也要求我们不断学习和适应新的知识和技能，否则就可能被时代所淘汰。因此，在享受技术带来的便利时，我们也需要清醒地认识到其潜在的风险和挑战，确保技术的健康发展，避免技术失控带来的潜在危害。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了深入探讨&nbsp;2024&nbsp;年技术领域的发展趋势，我们非常荣幸地邀请到了某金融企业普惠数字金融部负责人祝世虎博士，商汤&nbsp;Copilot&nbsp;技术负责人&nbsp;张涛先生，以及&nbsp;Thoughtworks&nbsp;中国区&nbsp;CTO&nbsp;冯英睿先生，作为我们的特别嘉宾。他们将与我们分享他们在技术应用领域的宝贵经验和深刻见解。我们将深入探讨在过去的一年中&nbsp;AI&nbsp;的快速发展给工作带来了哪些挑战与惊喜、快速发展的AI技术又将如何影响技术服务市场。我们还会讨论在工作中团队应该如何高效地利用&nbsp;AI&nbsp;技术、2024&nbsp;年大模型市场的发展趋势是什么，以及技术的快速发展是否会对我们的工作造成危险。这场圆桌会议将是一个探索大模型前沿信息、洞察未来趋势的绝佳机会。</p><p></p><p></p><h3>2023年AI在自身工作领域带来的最大挑战以及最大惊喜是什么？</h3><p></p><p></p><p>王媛娅: 我想请问一下各位老师，2023年AI在您所在的工作领域带来的最大挑战是什么?以及在过去的一年当中AI和大模型软件技术发展中最大的惊喜是什么？</p><p></p><p>张涛</p><p>最大的挑战的话我认为还是大模型的稳定性和效率问题，还是希望大模型在加持了一些工具之后能够为我们稳定的输出可信的结果。因为在很多数据处理或者其他的应用场景上，我们不仅仅是需要大模型在吃掉输入的数据之后只进行简单的“next token production”式的推理，而是希望大模型能够理解用户需求并通过严谨的代码生成处理逻辑，得到一个可信的结果。</p><p></p><p>冯英睿</p><p>我印象最深的一个挑战是在我们在跟客户交流和服务的过程当中发现不同的人对于AI的认知是有偏差的。无论是高看AI还是低估AI，其实在应用和实施上都会带来一些不利的一些影响。另外一个比较大的挑战就是算力的挑战。因为算力的情况决定了大模型的成本与稀缺性，这也是落地过程中一直比较重要的一部分。惊喜的话，其实我觉得最大的惊喜应该就是保持了惊喜的常态化。纵观整个发展过程，无论是大模型的数据、算力还是规模，目前都还没有看到瓶颈，这种现象本身就是一个惊喜。另外就我个人来说的话，我对机器人领域的一些发展还是很惊喜的，也希望后续这一领域能有更大的发展。</p><p></p><p>祝世虎</p><p>从业务与应用的角度来看，在过去一年中面临的最大挑战是数据，最大的惊喜还是数据的问题。比如做大模型，如果训练的中能拿到好的语料，那大模型就一定能成功。从数据的保护角度来看，民法中对数据属于跟网络虚拟财产并列，是一种财产性的权益。后来颁布的数据安全法更是强调了数据安全的重要性。所以从法律对数据的表述来看，一是强调其权益性，二是强调其安全性。所以我认为数据流通问题是我们面临的一个重大挑战。</p><p></p><p>惊喜的话，在我看来是数据制度的变化。国家颁布的《数据二十条》带来了一个制度的变化，因为二十条强调的是数据的“数据的流通”。《数据二十条》对数据权益进行三权分制：持有权、加工权和经营权；数据的三分类：划分为公共数据、企业数据和个人数据。这为数据的管理、处理和商业应用提供了明确的法律框架，这使数据更加容易流通，也必将极大地促进科技的发展。</p><p></p><p></p><h3>更看好哪些AI应用领域？这些领域又将如何影响技术服务市场？</h3><p></p><p></p><p>王媛娅: 去年科技界似乎已经达成了一个共识，那就是大型模型的研发投入是巨大的。因此，大家更加看好大型模型的应用潜力。然而，目前来看，大型模型的应用尚未广泛普及，或者说还没有出现能够大众化、广受欢迎的爆款应用。所以想请教一下各位老师，您们更看好哪些新的AI应用领域？这些领域又将如何影响技术服务市场呢？</p><p></p><p>冯英睿</p><p>在我看来，AI技术的发展对软件信息服务行业产生了重大影响，不仅仅提升了个人技能而且改变了我们的工作方式。从我自身工作的领域出发的话，我比较关注AI在软件开发方面的应用，尤其是去年AI的兴起让我们不得不重新思考其在软件辅助研发中的角色。实际上我们最初关注的是个人能力的提升，但后来逐渐认识到AI将全面影响技术流程和组织结构。尽管研发人员为很多业务构建了数据平台，但是我们却忽略了为研发本身建一个研发的数据平台。未来，随着人们认知能力的提升，我们是否可以从学习、思考、实践等多个层面，基于我们的研发数据资产实现更全面的进步。</p><p></p><p>另外随着AI技术能力的显著提升，我们不仅看到了它在提升业务能力方面的潜力，也意识到了随之而来的风险。AI的强大功能无疑增强了传统非智能业务应用的能力，但同时，它也扩大了业务风险的暴露面。无论是在数据开发还是业务应用开发中，安全和其他相关因素的重要性变得更加突出。因此，对于专业人员来说，整体能力的要求提高了，特别是在安全性方面。这是我们从去年到今年特别关注的一些变化。</p><p></p><p>祝世虎</p><p>金融领域的核心应用就是要在政府监管的框架下，合规地管理风险，并为客户提供服务。那么面对这一基本情况，大模型在金融领域的关键运用领域就包括了监管科技、合规管理、风险管理和客户管理等。其中，智能风控尤为复杂，是这些应用中最为挑战性的部分。因此，我认为大模型在智能风控方面的发展不仅是必要的，而且是迫切的。</p><p></p><p>当前智能风控系统正面临四个关键挑战，也正是这些挑战推动着智能风控从被动到主动演变。首先，中小银行受限于数据，难以构建丰富的特征库，需要能在数据有限情况下建模的解决方案，而大型AI模型在这方面展现了潜力。其次，相对于为每个风险评估任务建立成本高昂的单任务模型，大型模型能整合多任务，提升效率并更深入地理解风险。第三，虽然构建大型模型本身成本较高，但它们的复制和运用成本较低，提供了通过利用这些模型能力来构建风控系统以节约成本的可能性。最后，随着新客户行为和黑灰产挑战的出现，传统基于历史数据的预测模型效果减弱，这使得模型对抗将成为一个持续的挑战。换句话说，智能风控领域需要大模型进行进一步的创新和自我革新，以适应不断变化的金融环境和提升风险管理的效率和效果。</p><p></p><p>张涛</p><p>两位老师所提及的内容中都涉及了使用大型模型进行编程的实践，这也是我目前专注的领域。我认为，大型模型在自然语言处理方面所提供的灵活性和创造性，结合编程代码固有的严谨性和形式化语言的规范性，能够有效地解决人们在日常工作中遇到的一些问题。因此，我对这一领域的发展前景持积极态度，并且正在投入资源进行相关研究。此外，我认为视频或图像生成领域的应用前景也非常广阔。随着移动设备上剪辑软件的普及，人们可以轻松编辑和发布短视频，这无疑推动了短视频行业的繁荣。AI 在视频生成过程中的应用，不仅提升了视频制作的质量，还极大地拓宽了创作的可能性。以往，视频的制作受限于工具的能力，而非人们的审美或创造力。现在，随着更先进的工具的出现，我们的创造力得到了更大的释放，个人的短视频制作能力也得到了显著提升。</p><p></p><p>王媛娅: 接下来问题是AI浪潮引导的大时代背景下，您的团队是计划如何迎战AIGC带来的冲击和挑战？能否为我们分享一下咱们的这个策略还有方向。</p><p></p><p>冯英睿</p><p>目前我们的团队主要专注于软件的研发，并致力于在当前时代大幅提升我们自身的能力，为未来的竞争和发展做好准备。同时鉴于应用层面的投资风险较高，我们不太会选择在该层面进行大量投资。相反，我们将重点投资于平台层和核心层，并以自助服务的形式提供给内部员工，从而降低尝试新方向的成本。</p><p></p><p>因为事实上在任何方向上的努力都有可能取得成功，因此我们的策略是提供自主能力，鼓励员工自发创新，同时降低员工尝试的成本以增加成功的可能性。所以说我们的目标是通过长线投资触发低成本的应用创新，并在此过程中让团队成员更深入地理解大语言模型和AI的应用，同时也帮助我们的客户取得成功。</p><p></p><p>祝世虎</p><p>我从两个观点说，第一个观点就是专门从大模型的角度来说，第二个观点就是从大模型的应用的角度来。</p><p></p><p>首先从大模型的角度来说，大模型的主要作用是提高整个社会的平均智能水平，而不仅仅是针对特定行业如金融的智能提升。说白了大模型的能力有可能被用于不正当的途径，比如黑色产业可能会更有“创意”地去利用这些模型。因此，我们面临的一个主要挑战不是大模型是否能帮助我们减少人力或提高效率，而是如何确保它们不被用于恶意目的。例如，在构建智能客服系统的同时，我们需要考虑黑色产业可能会利用同样强大的模型来创建用于发起欺诈投诉的机器人。这就要求我们在发展大模型时，不仅要关注其正面应用，还要积极防范潜在的滥用风险。</p><p></p><p>第二点在金融行业中，大模型的实施策略往往遵循一个分层的金字塔结构，该结构分为三个主要层次：</p><p></p><p>基础层：这是金字塔的底层，由领先的AI公司开发的通用基础大模型构成，这些公司拥有丰富的数据资源和强大的技术力量，为基础AI能力提供支持。</p><p></p><p>行业层：位于中层的是针对金融行业的大模型，由对银行业务和金融专业知识有深刻理解的公司基于基础层模型进一步开发，以满足金融领域的特定需求。</p><p></p><p>机构层：最顶层是各个银行或金融机构根据自身的具体任务和数据集定制的大模型，这些模型从行业级模型出发，通过个性化训练和微调，以适应特定机构的业务需求。对于中小型金融机构，由于科技预算有限，他们需要将资源集中投资在关键领域。这意味着他们可能不会自行构建基础大模型，而是依托于基础层和行业层的模型，并利用自身数据进行精细化调整，以确保模型能够满足特定的业务需求。</p><p></p><p>因此，中小银行应专注于以下三个核心领域的投资：首先是数据整合，即整合内部数据并辅以外部数据，以构建具有本行特色的综合知识库；其次是算力合作，考虑到算力是非核心竞争能力，银行应寻求成本效益更高的外包方式来满足算力需求；最后是模型精调，结合本行的数据和知识库，利用外包的算力资源对大模型进行迁移学习，以打造适合本行的轻量级推理模型。</p><p></p><p>张涛</p><p>其实在这个方面我们商汤是具备代表性优势的，因为我们的AIDC智算中心配备了必要的硬件基础设施，这使得商汤有能力进行大模型的预训练（Pre training）。这意味着我们可以从基础模型出发，构建具有竞争优势的AI能力。</p><p></p><p>商汤公司在硬件基础设施方面具有优势，这确保了我们在激烈的竞争中能够保持领先地位。我们目前专注的方向有两个：</p><p></p><p>真正地将数据与代码结合起来，开发能显著提高生产力和效率的实用工具。</p><p></p><p>在多模态领域进行探索。尽管目前的多模态模型大多数还称不上真正的多模态，例如从文本到图像或视频的转换，或者从图像进行视觉问答（VQA）等操作，这些仍然是简单的单模态到单模态的转换。然而，如果我们要在产业链中融合这些模态，就需要超越现有的工作流概念，实现对模态的统一理解，使大模型能够一次性完成复杂的推理任务。</p><p></p><p>目前，我们将这些处理步骤组织成工作流，或者视为上下文中的一部分，这与人脑处理的效率和效果相比还有很大差距。因此，我们认为对多模态模型的投资是一个重要的技术进步点，也是确保我们在技术上保持领先的方式。</p><p></p><p>除了硬件基础设施外，数据也是构建大模型的一个关键要素。因为即使是参数达到千亿级别的大模型，在预训练阶段使用的数据量实际上也是有限的，这意味着大模型仍然处于不断学习的阶段，潜力尚未被完全挖掘。因此，我们一方面需要不断准备高质量的数据，以支持模型的持续学习；另一方面，这些未挖掘的潜力为我们提供了空间，使我们能够对特定垂直行业的数据进行精调，或者在特定场景下优化模型。我们可以更有信心地将这些数据整合到模型中，而不必担心新数据会负面影响模型原有的基础能力。这是我们在新的应用场景中保持技术优势的一个重要策略。</p><p></p><p></p><h3>没有技术背景但了解市场的人能否独立完成研发工作？现有的研发团队如何有效地利用AI工具？</h3><p></p><p></p><p>王媛娅: 随着越来越多的工具问世，对于那些没有技术背景但深入了解行业场景和市场需求的非研发团队人员来说，他们是否能够独立完成研发工作？此外，现有的研发团队如何有效利用大模型和AI工具？我想听听各位专家对这两个问题的看法。</p><p></p><p>祝世虎</p><p>当前，我们见证了低代码和零代码平台的兴起，这对于几年前就开始出现的技术趋势来说是一个重要的发展。</p><p></p><p>首先，在我看来低代码和零代码平台提高了效率和效能。一些有经验的程序员开始使用低代码平台，这显然有助于提升工作效率。同时，对于那些不会编写代码的人来说，他们也可以使用低代码平台进行开发。然而，这里存在一个本质的区别就是：不会编写代码的人缺乏算法思维。当他们使用零代码平台时，由于缺乏算法框架，他们更多地进行统计分析而不是真正的计算工作。因此，仅依靠低代码和零代码平台是不够的，这可能会带来一些问题。</p><p></p><p>其次，大模型的出现引发了一个问题：大模型是否会取代程序员？如果未来没有人再编写代码，那么零代码和低代码平台是否会被大模型所取代？这是一个未来的疑问，目前还无法给出确切的答案。然而，大模型能够帮助解决的另一个问题是人机沟通，或者说业务部门与科技部门之间的沟通问题。当业务部门能够使用低代码平台或大模型简单地编写一些东西让机器理解时，无论是通过大模型让机器理解还是通过低代码平台让机器理解，我认为这都比通过科技人员作为中间层的效率更高。</p><p></p><p>冯英睿</p><p>在探讨低代码和传统编程方法时，我们必须承认，尽管它们具有一定的表现力，但在面对复杂逻辑时往往显得力不从心。大语言模型则提供了更高级别的抽象化能力，使我们可以更深入地处理复杂的任务。随着编程语言的发展，专门为执行特定任务而设计的、能够更有效地满足特定领域的需求的领域特定语言（DSL）就变得愈发重要。在大语言模型的加持下，它们可以根据用户的需求生成相应的高级语言代码，从而轻松应对复杂的任务。</p><p></p><p>另外在企业数字化和服务化的进程中，有效的服务化对于人工智能（AI）与系统之间的交互至关重要。AI需要依赖现有系统进行实际操作，因此确保服务化的顺利实施是AI正常运作的关键。而企业需要程序员能够以高效、简洁的方式实现业务的数字化和服务化，从而为大模型提供更好的运行环境。所以说科技的投入对于降低系统复杂度是至关重要的。</p><p></p><p>再者微服务在业务与应用层的集成中起着至关重要的作用。大型语言模型有望在这一层面上取代许多传统的开发工作，减少了对软件库进行修补或调整的需求。</p><p></p><p>从业务角度来看，这种趋势是受欢迎的，因为它使获取数据和为企业提供服务变得更加便捷。对于研发人员而言，尽管这种变化可能对一些中间层开发人员产生影响，但只要他们理解大模型的特性，并学会如何增强对模型的理解、知识、推理和表达能力，他们就能适应这种工具的不断出现与迭代升级。</p><p></p><p>张涛</p><p>大语言模型目前仍然存在不确定性和不稳定性。因此，在实际应用当中，我们常需要结合编程语言来增强回复效果。同时在我们的实验中，我们让大语言模型处理数据可视化任务时发现它是能够高效地完成这一工作的。但这个过程中，我们也遇到了一些问题，这提示我们仍需对大语言模型进行进一步的优化和调整。</p><p></p><p>关于大语言模型是否会取代程序员的问题，我认为目前并不需要过分担忧。大语言模型的发展实际上降低了技术门槛，它使得非专业人士也能轻松利用其功能。它更像是一个强大的工具，而非完全取代程序员的存在。大语言模型的主要优势在于其生成能力，能够根据不同的需求生成相应的代码，从而满足用户的个性化需求。</p><p></p><p>总的来说，大语言模型的发展为编程领域带来了新的可能性，它使得编程更加普及和易于上手。虽然目前仍存在一些问题和挑战，但随着技术的不断进步，我们相信大语言模型将会在未来的发展中发挥更加重要的作用。</p><p></p><p></p><h3>在2024年可能出现的大型模型市场趋势有哪些？</h3><p></p><p></p><p>王媛娅: 接下来是我们讨论下一个议题，除了我们报告中所预测的趋势，还想了解一下各位老师认为在2024年可能出现的大型模型市场趋势有哪些。当然，这些趋势可以是近期的也可以是远期的。</p><p></p><p>冯英睿</p><p>我相信在2024至2025年间，大语言模型的能力将显著提升，而且将颠覆我们的认知。在2023至2024年的探索期，已经有许多企业尝试把大语言模型应用于实际工作当中去了。随着这些模型能力的增强，我们也将消除对它们在实际使用中的疑虑，大模型将是值得我们信赖的工作伙伴。</p><p></p><p>因此，我认为2024年将是大语言模型广泛应用的一年。正如我们之前讨论的，我们正在努力降低大语言模型的应用门槛，提供更多自助服务，以便在工作过程中帮助企业做好准备，大规模应用这些模型。我们期待在2025年到来之前，看到大语言模型在实际工作中的广泛应用。</p><p></p><p>然后我个人的一个看法是大模型在核心业务上的影响可能不会特别显著，我更倾向于它是一个在实际运用中更多地提升我们个人能力和认知的一个工具。而且它在业务实施过程中确实有效地辅助了我们提高工作的效率和质量，从而使工作更加顺畅和易用。</p><p></p><p>从技术角度来看，我对多模态技术的进展非常关注，因为它在应用场景和体验上具有巨大的潜力。例如，已经有人尝试使用大语言模型和生成式AI来重建与已故亲人的联系，满足人们在情感方面的需求。这已经是一个发展趋势了，并且有许多场景可以应用。因此，我会更加关注元宇宙和数字人这两个方向，因为它们对商业和个人消费者的价值可能才真正开始体现。</p><p></p><p>张涛</p><p>在应用场景方面，我能做出的预测相对较少，因为各个行业从自己的视角出发，都会发现各自的结合点。实际上，我们目前能期待的是更多地借用大模型的一些能力，在图像生成或意图理解等方面的单点提升，这就像刚才我们提到的多模态领域。</p><p></p><p>但是我必须要指出的是，尽管我们的远期愿景是让机器能够像人一样，至少像一个简化版的大脑来帮助我们工作，但即使是像Sora这样的大模型现在的多模态能力也是明显不如人类的。大家可能会觉得它好像理解了真实的物理世界，但实际上它离理解物理规律还差得很远。它只是从大量的训练数据中得到了合理的、概率性的数据分布。</p><p></p><p>但是我觉得这可以理解为牛顿提出物理定律之前，人们已经在他们的认知范围内利用这些定律一样，只是缺少一个人来总结而已。因此，虽然强人工智能时代还没有到来，但在这条路上我们看到的一些现有缺陷并不是阻碍，反而让我们看到了一些使AGI更好进化的途径。</p><p></p><p>祝世虎</p><p>我认为大模型可能会在一两年内进入金融机构的核心应用中，特别是在中小金融机构和中小银行的智能风控体系中。相比于目前银行使用的小模型，大模型有以下优势：</p><p></p><p>大模型能够以极低的迁移成本引入金融领域的能力。</p><p></p><p>大模型能够感知风险浓度，而小模型更侧重于风险排序和评级。</p><p></p><p>大模型可以事前感知欺诈态势，而小模型只能感知到欺诈事件。</p><p></p><p>大模型能够感知到风险背后的人为因素，这是小模型无法做到的。</p><p></p><p>大模型能够有效对抗新型的黑产攻击，而小模型容易被黑产攻击。</p><p></p><p>大模型可以预测和感知新型风险，而小模型基于历史数据预测未来的方法存在弊端。</p><p></p><p>尽管大模型目前也存在一些劣势，如计算复杂度高、可解释性差等，但大模型和小模型可以先以共存方式出现，即以大模型作为中控大脑，外围由可解释的小模型组成一个新型的智能风控体系。随着人们对大模型的认可度提高、解释性增强和复杂度降低，大模型极有可能取代传统的小模型。</p><p></p><p>另外从发展的角度来看呢，技术从来都不是平等的。因为中小银行在人力、设备和数据资源方面的投入是无法与大银行相比的，这自然就导致了技术方面的不平等。因此，在数字化转型的过程中，小银行和大银行之间的差距会越来越大，智能风控也是如此。但大模型的发展为中小银行提供了迎头赶上的机会。通过引入大模型的能力，中小银行可以在风控能力方面与大银行对齐，再凭借其机制相对灵活，在某些具体的风控场景上赶上甚至超越大银行是完全可能的。</p><p></p><p></p><h3>大模型应用会更早落在哪些领域？</h3><p></p><p></p><p>王媛娅: 非常感谢几位老师的精彩分享。那在这个分享的过程中，我们也看到评论区有一些留言和问题，第一个问题是想请教冯老师，您觉得大模型应用会更早落在哪些领域？或者说哪些领域会更有机遇一些？</p><p></p><p>冯英睿</p><p>我认为金融领域是大模型技术投资的热门领域之一。虽然政府监管、资金投入和大模型的准确率等因素会对大语言模型的投资产生影响，但随着大模型能力的提升，这些影响预计会逐步减少。</p><p></p><p>我想强调的是，在业务过程中，如果以知识密集型人才作为核心成本，那么这部分的影响将更为显著。而对于体力劳动人群的影响相对较小，实际上，这种影响主要是由机器人等其他技术引起的。所以另一个受大模型影响的行业就是软件研发行业。在软件研发领域，所有的输入和输出都可以数字化，而且其核心依赖于人力。因此，对于这一领域的相关工作人员，大模型的影响将更为显著。</p><p></p><p></p><h3>在当前发展趋势下，未来前端类的工作是否会面对危险？</h3><p></p><p></p><p>王媛娅: 接下来，我想请教张老师一个问题。在当前的科技发展趋势下，我们是否应该担忧未来前端和UI设计的工作岗位会面临危险？您是如何看待这一问题的？</p><p></p><p>张涛</p><p>实际上，我认为当前的情况与之前提到的低代码和视频创作工具的例子有相似之处。在大型模型出现之前，已经有一些基于CRUD的工具可供初级开发者使用。</p><p></p><p>在UI开发领域，人们可能会认为screen2code这种应用的出现对自己的工作构成了威胁。然而，在我看来，这种应用的出现实际上对我们是有帮助的。因为在以前，仅仅搭建和复刻一个界面可能就需要花费很多时间，而现在这些过程可以大大简化，为我们节省了时间和精力。这使得我们能够将更多的精力投入到更高级的工作上，例如UE（用户界面）的设计和后台数据的交互等。这些领域是我们真正发挥专长的地方。</p><p></p><p>因此，对于这个问题，大家完全可以放松心态，因为这种应用的出现实际上是在帮助我们，而不是威胁我们的工作。我们可以利用这些工具来提高工作效率，从而更好地展示我们的能力。</p><p></p><p></p><h3>训练大模型时高质量数据不足怎么解决？</h3><p></p><p></p><p>王媛娅: 最后一个问题想请教一下祝老师，训练大模型时高质量数据不足是个问题，我们通常是怎样解决这一难题的呢？希望能听听您的经验和建议。</p><p></p><p>祝世虎</p><p>首先，在金融机构训练大模型这个方面，许多银行已经启动了相关项目。然而，银行的结构化数据虽然丰富，但对训练大模型的价值十分有限，因为单一的结构化账目数据不足以支撑有效的模型训练。所以，一些银行利用以前的公文等非结构化数据进行训练，可以在智能OA、智能客服等方面带来实际帮助。</p><p></p><p>其次，在智能客服方面，银行需要整合内部数据，并适当补充外部数据。当然，大型银行可能具备这样的条件，但对于小型银行或算力不足的银行来说，我还是建议采用精调模型的方式，即利用自己的特色数据来迁移大模型的能力，而不是尝试独立训练一个大模型。因为自行训练大模型的投入巨大且风险较高。</p><p></p><p>以上是本次圆桌会议“2024年技术发展趋势解读”的访谈实录，更多关于2024年中国软件技术发展洞察和趋势的完整内容请关注<a href="https://www.infoq.cn/minibook/YcyRCPwj38Upvdj4qVmx">《中国软件技术发展洞察和趋势预测报告2024》</a>"欢迎点击链接，进行完整报告下载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Mj63pg2SHHWaifKtPbNv</id>
            <title>蔡崇信反思阿里落后：我们砸了自己的脚；英特尔又“崩了”，亏损70亿美元；华为切割“遥遥领先”，传任正非下令禁止 | Q资讯</title>
            <link>https://www.infoq.cn/article/Mj63pg2SHHWaifKtPbNv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Mj63pg2SHHWaifKtPbNv</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 10:09:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Stability AI, 生成式人工智能服务备案信息, 英特尔, 阿里
<br>
<br>
总结: 文中提到了Stability AI资金耗尽无法支付云GPU账单，英特尔芯片制造部门运营亏损70亿美元，阿里巴巴蔡崇信访谈以及生成式AI等内容。 </div>
                        <hr>
                    
                    <p></p><blockquote>据报&nbsp;Stability&nbsp;AI&nbsp;资金耗尽；国家网信办发布生成式人工智能服务备案信息；亚马逊举办生成式&nbsp;AI&nbsp;沟通会；OpenAI&nbsp;宣布：放开使用限制！英特尔披露芯片制造部门运营亏损&nbsp;70&nbsp;亿美元；华为最新分红出炉；首位&nbsp;AI&nbsp;程序员入职阿里；美参议院将修改&nbsp;TikTok&nbsp;剥离法案；搜狗硬件产品于&nbsp;5&nbsp;月&nbsp;30&nbsp;日正式停止服；AI&nbsp;编程语言&nbsp;Mojo&nbsp;正式开源；Stable&nbsp;Audio&nbsp;2.0&nbsp;重磅发布！Safari&nbsp;浏览器主要设计师离开苹果……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>据报Stability&nbsp;AI&nbsp;资金耗尽，无法支付租用的云GPU账单</h4><p></p><p></p><p>据4月3日消息，生成式AI明星&nbsp;Stability&nbsp;AI&nbsp;的热门文本到图像生成模型&nbsp;Stable&nbsp;Diffusion&nbsp;资金耗尽，无法支付训练大模型所需的&nbsp;GPU&nbsp;集群费用。截至去年&nbsp;10&nbsp;月该公司只剩下&nbsp;400&nbsp;万美元的储备金。</p><p></p><p>根据引用公司文件和数十位知情人士的详尽报道，据称这家英国模型构建公司的极高基础设施成本耗尽了公司的现金储备，导致截至去年十月时，公司只剩下400万美元。此外工资和运营费用还需要&nbsp;5400&nbsp;万美元。而它在&nbsp;2023&nbsp;年估计的收入只有&nbsp;1100&nbsp;万美元。它拖欠了&nbsp;AWS、Google&nbsp;和&nbsp;CoreWeave&nbsp;千万美元的账单。</p><p></p><p>Stability&nbsp;AI&nbsp;的筹款也不成功，英特尔承诺投资&nbsp;5000&nbsp;万美元，但已支付的金额只有&nbsp;2000&nbsp;万美元。公司&nbsp;CEO&nbsp;Emad&nbsp;Mostaque&nbsp;上月底在社交媒体上披露他已经辞职，首席运营官&nbsp;Shan&nbsp;Shan&nbsp;Wong&nbsp;和首席技术官&nbsp;Christian&nbsp;Laforte&nbsp;担任临时的联席&nbsp;CEO。</p><p></p><h4>阿里巴巴蔡崇信万字访谈：我们承认错误，过去没有关注用户体验</h4><p></p><p></p><p>4&nbsp;月&nbsp;3&nbsp;日下午，挪威主权财富基金发布了其首席投资官&nbsp;Nicolai&nbsp;Tangen&nbsp;与阿里巴巴联合创始人、董事局主席蔡崇信的专访视频。在访谈中,&nbsp;他分享了自己的成长经历和在阿里巴巴的工作体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a73b20ae438965c91bd4fa5872f38ba4.webp" /></p><p>截图来源网络</p><p></p><p>蔡崇信回忆了阿里巴巴的创业初期。他谈到了与马云的初次见面,&nbsp;以及对公司愿景和文化的理解。他认为,&nbsp;阿里巴巴有着广阔的发展前景。作为公司的领导者之一,&nbsp;他见证了阿里巴巴的成长历程。</p><p></p><p>在科技创新领域,&nbsp;作为中国最大的云计算公司之一,&nbsp;阿里巴巴将人工智能视为云业务的重要组成部分,&nbsp;而电商是人工智能应用的重点领域。公司正开发多项&nbsp;AI&nbsp;驱动的产品和服务,&nbsp;如智能导购、图文生成等。蔡崇信坦言,&nbsp;当前中国在人工智能某些方面落后于美国,&nbsp;但正在奋起直追。在芯片和半导体领域,&nbsp;由于受到美国对华出口管制的影响,&nbsp;中国企业在高端芯片的使用上受限,&nbsp;但国内正在加快发展自主芯片的设计和制造能力。</p><p></p><p>在访谈的最后,&nbsp;蔡崇信还分享了他对企业文化、工作效率、领导力以及工作与生活平衡的看法。他以阿里巴巴为例,&nbsp;阐释了优秀企业文化的特质:&nbsp;员工认同公司使命、了解发展方向,&nbsp;并乐于与同事协作。在提升效率方面,&nbsp;他分享了自己的心得,&nbsp;即借鉴运动员的间歇式训练模式,&nbsp;通过高强度工作和适度休息的结合,&nbsp;而非连续工作&nbsp;10&nbsp;小时。在他看来,&nbsp;出色的领导者应该善于给予反馈,&nbsp;勇于承认错误,&nbsp;不刻意标榜自己,&nbsp;以免扼杀创新。</p><p></p><h4>英特尔披露芯片制造部门运营亏损70亿美元</h4><p></p><p>美东时间4月3日，芯片龙头英特尔股价重挫逾8%，收跌8.22%，报40.33美元，最新总市值1716.8亿美元。4月2日，英特尔在美国证券交易委员会（SEC）提交的一份文件中披露，公司负责芯片制造业务的新部门“英特尔代工”（Intel&nbsp;Foundry）2023年营收为189亿美元，同比下降31%，2022年这一数字为274.9亿美元，经营亏损从前一年的52亿美元扩大至70亿美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04c60707339fb8d76ad92c45bdcdde01.png" /></p><p>截图来源于网络</p><p></p><p>英特尔CEO帕特·基辛格并不避讳该业务所面临的亏损情况，并表示2024年将是公司芯片制造业务经营亏损最严重的一年。</p><p></p><p>同时，他也给出预测，该业务会到2030年底前实现经营收支平衡，届时公司的目标是在非美国通用会计准则下，毛利率达到40%，经营利润率达30%。上述消息发布后，英特尔当日收盘价为43.94美元/股，盘后下跌4.1%。</p><p></p><p>今年2月，在加州举办的IFS&nbsp;Direct&nbsp;Connect&nbsp;2024大会上，英特尔宣布将旗下晶圆业务Intel&nbsp;Foundry&nbsp;Services正式更名为Intel&nbsp;Foundry。公司还公开展示了其1.8纳米芯片制程intel18A的量产进度，以及包括更先进的Intel&nbsp;14A（对应1.4纳米）工艺在内未来十年工艺路线图。英特尔还计划斥资1000亿美元在美国四个州建设或扩建芯片工厂。上个月，该公司拿到了美国《芯片和科学法案》提供的85亿美元的资金补贴，其未来目标是要在芯片制造领域挑战台积电与三星。</p><p></p><h4>国家网信办发布生成式人工智能服务备案信息</h4><p></p><p></p><p>4月2日，国家网信办官网发布了关于《生成式人工智能服务已备案信息的公告》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/385618884ca580d0456e9c5824c9037e.png" /></p><p></p><p>网信办表示：促进生成式人工智能服务创新发展和规范应用，网信部门会同相关部门按照《生成式人工智能服务管理暂行办法》要求，有序开展生成式人工智能服务备案工作，现将已备案信息予以公告。提供具有舆论属性或者社会动员能力的生成式人工智能服务的，可通过属地网信部门履行备案程序，属地网信部门应及时将已备案信息对外公开发布，将在官网定期汇总更新，不再另行公告。</p><p></p><p>据悉，要求已上线的生成式人工智能应用或功能，应在显著位置或产品详情页面公示所使用已备案生成式人工智能服务情况，注明模型名称及备案号。</p><p></p><h4>华为内部禁止喊“遥遥领先”，提一次罚一万？</h4><p></p><p></p><p>4月2日，#余承东否认被下令禁提遥遥领先#冲上热搜。据悉，有媒体发文称，任正非据传下禁令一句遥遥领先罚款一万，相关话题引发关注。3月30日，钛媒体创始人赵何娟发文称，据可靠消息说，任正非在华为内部给余承东下了“禁令”，每再提一句“遥遥领先”罚款一万。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/022f4fb03c8a1b7eb93e4c66d53bf8db.png" /></p><p></p><p>“遥遥领先”这个词汇最初出现在华为手机Mate&nbsp;40的发布会上，华为常务董事、终端BG&nbsp;CEO余承东在介绍手机的各项性能时多次使用这个词。随后，在Mate&nbsp;50发布会上，华为全球首发了卫星通信功能，余承东再次提到“遥遥领先”，并称之为捅破天的技术。到了2023年8月，由于华为Mate&nbsp;60系列手机的发布，“遥遥领先”成为了网络热词。</p><p></p><p>值得注意的是，去年9月，华为申请注册了两个“遥遥领先”的商标，但在今年1月，这两个商标被撤回了。当前，这两个商标均为“无效”状态。另外，在2024年2月22日的华为新品发布会上，余承东全程没有提这个词，在结尾处有观众喊“遥遥领先”，他回了一句“谢谢”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41987c4538193f9a1cf1536db62edb8c.png" /></p><p></p><h4>亚马逊举办生成式&nbsp;AI&nbsp;沟通会，中国公司争相合作？</h4><p></p><p></p><p>据晚点&nbsp;LatePost&nbsp;报道，4月2日，亚马逊云科技在北京举办生成式&nbsp;AI&nbsp;沟通会，重点展示了一个月前发布的&nbsp;Claude&nbsp;3&nbsp;系列大模型。AWS&nbsp;未在中国境内的服务器上部署&nbsp;Claude&nbsp;3。和微软&nbsp;Azure&nbsp;一样，中国公司可以通过&nbsp;AWS&nbsp;全球提供的&nbsp;Bedrock&nbsp;服务，申请调用在其他地区部署的&nbsp;Claude&nbsp;3&nbsp;模型并完成计算。</p><p></p><p>一位亚马逊人士表示，Claude&nbsp;3&nbsp;系列模型发布后，他们收到了大量中国公司的合作需求。</p><p></p><p><a href="https://mp.weixin.qq.com/s/uR6qBjkz0jyBwY8TiTU3uw">亚马逊此前宣布向人工智能公司&nbsp;Anthropic&nbsp;追加&nbsp;27.5&nbsp;亿美元投资</a>"，这笔投资是继去年&nbsp;12.5&nbsp;亿美元的投资之后的追加注资，使亚马逊对&nbsp;Anthropic&nbsp;的总投资额达到&nbsp;40&nbsp;亿美元，并成为后者核心投资方、“主要云供应商”。</p><p></p><p>3&nbsp;月初，Anthropic&nbsp;发布了其突破性的&nbsp;Claude&nbsp;3&nbsp;系列模型，该系列大型语言模型&nbsp;(LLM)&nbsp;在各种认知任务上树立了新的性能标杆。Claude&nbsp;3&nbsp;系列包含三个子模型，分别为&nbsp;Claude&nbsp;3&nbsp;Haiku、Claude&nbsp;3&nbsp;Sonnet&nbsp;和&nbsp;Claude&nbsp;3&nbsp;Opus，它们提供不同程度的智能、速度和成本选择，以满足各种人工智能应用需求。根据&nbsp;Anthropic&nbsp;的数据，Claude&nbsp;3&nbsp;Opus&nbsp;在十多项常用模型能力评估数据集上，得分全超过&nbsp;GPT-4。</p><p></p><h4>OpenAI宣布：放开使用限制！全球185个国家无需注册即可使用</h4><p></p><p></p><p>当地时间4月1日，OpenAI宣布，将从当日起逐步放开使用ChatGPT的注册要求，用户将无需再注册该服务。</p><p></p><p>虽然是在4月1日公布的消息，但这并不是一个愚人节玩笑。OpenAI在公告中写道：“我们的使命是让像ChatGPT这样的工具广泛可用，以便人们体验AI带来的好处。有来自185个国家的1亿多人每周使用ChatGPT来学习新知识、寻找创意灵感，并获得问题的答案。从今天开始，您无需注册即可使用ChatGPT。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20572262ed0b1bc0f623a5e8fbe5d0ba.png" /></p><p>OpenAI宣布放开ChatGPT的注册要求。来源：OpenAI官网</p><p></p><p>不过，值得注意的是，未经注册的用户将失去一些注册用户的权利，例如保存或共享聊天记录和使用自定义指示。未经注册用户的聊天数据也会被默认进入ChatGPT的训练集，除非用户在“设置”中关闭该选项。OpenAI还指出，未经注册的ChatGPT版本将有“更加严格一些的内容政策”，但未对此进行详细说明。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/Yb_lznBa5Ek5cCNi7QRuKw">ChatGPT免注册让官网挂了？沃顿教授：OpenAI&nbsp;做了错误的决定</a>"》</p><p></p><h4>华为最新分红出炉：15万人“瓜分”超700亿元！</h4><p></p><p></p><p>4月2日晚间，据北京金融资产交易所披露，华为投资控股有限公司发布关于分配股利的公告。 公告显示，经公司内部有权机构决议，拟向股东分配股利约770.95亿元。上述股利分配系公司正常利润分配，对公司生产经营、财务状况及偿债能力无不利影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/651fc273d197e93f3625f11912515009.png" /></p><p></p><p>4月4日，又更新发布华为投资控股有限公司关于分配股利的公告，拟向股东分配股利人民币约&nbsp;719&nbsp;亿元</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28660837f5193f1b1b16963d60ed39c4.jpeg" /></p><p></p><p>3月29日，华为发布2023年报，整体经营情况符合预期， 全年实现营业收入7042亿元，净利润870亿元，同比增长144.5%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/044087a3fc535d5b6270a3d4b1565ca4.png" /></p><p></p><p>根据年报，公司通过工会实行员工持股计划（即虚拟受限股计划），员工持股计划参与人数为151796人（截至2023年12月31日），参与人均为公司在职员工或退休保留人员。 按此计算，15万名员工大概平均能分50多万。</p><p></p><h4>首位&nbsp;AI&nbsp;程序员入职阿里，可以主动要求帮忙敲代码</h4><p></p><p></p><p>4月2日**，**阿里巴巴今日郑重宣布，迎来了一位不同寻常的“新员工”——通义灵码，工号AI001，标志着AI新纪元的开启。这位7X24小时在线的AI智能编程助手，将成为阿里大家庭中独一无二的存在。</p><p></p><p>早在2023年11月的云栖大会上，通义灵码便惊艳亮相，并启动了公测。短短一周时间，它便冲上了VSCode插件市场的周热榜，并荣获letBrains的月度推荐智能编程插件殊荣。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9b2a989949bc080ad5b65468f7a673d.png" /></p><p>截图来源网络</p><p></p><p>如今全新升级的模型，在HumanEval等榜单处于业界第一梯队，已熟练掌握200+种编程语言，它的下载量更是突破了200万。例如，在最基础的代码生成任务中，它能根据上下文自适应生成精准代码，运用实时分析与检索增强技术消除幻觉，真正做到秒懂程序员的需求。</p><p></p><p>在最新的版本中，通义灵码还新增了代码优化功能，能够深入分析代码及上下文，快速定位语法错误、性能瓶颈等问题，并给出具体优化代码建议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d5287a9a47748a5dab6f269c1a885c4.png" /></p><p></p><h4>迎来生机？美参议院将修改TikTok剥离法案</h4><p></p><p></p><p>北京时间4月1日，据美国媒体报道，美国参议院计划修改最近在众议院通过的TikTok剥离法案。目前，相关想法已经在国会山传播讨论。但是，一些支持打压TikTok的议员担心，过于宽泛的改动可能会严重拖延强制TikTok出售的努力，甚至会彻底破坏它。</p><p></p><p>在美国国会官网和长期跟踪美国国会活动的GovTrack上查询“TikTok法案”，均未显示参议院对该法案有任何进一步的审议计划或时间表。华尔街日报报道称，复活节休会归来后，参议院商务委员会主席玛丽亚‧坎特韦尔可能将推动该法案的修改。这意味着，该法案可能会被暂缓审议或彻底无法生效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f7baff9875a51c15c0d95941a3beb5c.png" /></p><p></p><p>**目前，在美国民众中，对于是否应该禁止TikTok的看法存在显著的分歧。**根据CNBC的一项民意调查，31%的受访者认为不应该禁止TikTok，而只有20%的受访者支持无条件禁止。这一数据反映出，对于TikTok所代表的言论自由和“创造者经济”的支持，并不是少数声音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/376f4e146648eaa204e3b3e7ca77c6cf.png" /></p><p>图片来源：CNBC</p><p></p><p>与此同时，字节跳动依然正在发起反击。该公司在内华达州、蒙大拿州、威斯康星州、宾夕法尼亚州和俄亥俄州等参议院席位竞争激烈的州启动了210万美元的营销活动。这些州都是今年竞选连任的弱势民主党参议员所代表的州。这些广告将在电视、广告牌和公交车站播放。</p><p></p><h4>搜狗硬件产品于5月30日正式停止服务,包括糖猫词典笔、搜狗翻译宝等</h4><p></p><p></p><p>4月1日，搜狗硬件产品维护团队发布公告表示，由于业务调整，搜狗硬件产品(糖猫手表、糖猫在家、糖猫词典笔、搜狗翻译宝(翻译笔)、搜狗录音笔)即将于2024年5月30日23点59分正式停止服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62b4d5d328cceea74f5f85938c7f25bc.png" /></p><p></p><p>这些硬件产品中部分主要功能或全部功能依赖于搜狗提供的云服务，因此在搜狗停止服务后这些产品可能会无法正常使用。</p><p></p><p>在2021年夏季，腾讯花35亿美元将搜狗从搜狐手中买下，彼时外界的一致的看法，是自此搜狗的存在感将会越来越淡。而随后的事实，也确实如此。继搜狗地图、搜狗阅读、搜狗借钱、搜狗号等一系列软件服务陆续停运之后，搜狗的硬件产品如今也成为了被“优化”的业务。</p><p></p><p>值得注意的是，搜狗硬件产品维护团队还提到服务下线后不再提供任何在线服务、技术支持和维修服务，也就是说后续产品出现故障无法使用时，用户也只能丢弃而无法寻求售后服务。搜狗要求用户在&nbsp;2024&nbsp;年&nbsp;5&nbsp;月&nbsp;30&nbsp;日之前备份自己的数据，之后搜狗会在服务下线后删除所有用户数据，数据被删除后自然也是无法恢复的。</p><p></p><h4>陷入瓶颈？B站展开新一轮组织架构变动</h4><p></p><p></p><p>4月1日，有消息称&nbsp;B&nbsp;站进行了新一轮组织架构调整，变动最大的就是此前核心业务部门&nbsp;"&nbsp;主站运营中心&nbsp;"&nbsp;拆分为三部分，分别是平台生态相关、直播业务相关以及内容品类相关。</p><p></p><p>据悉，与平台生态相关的部门，如创作平台部、生态策略部、生态用增组等，整合入&nbsp;"&nbsp;生态中台&nbsp;"，由夏彬负责；与直播业务相关的部门，如直播中台部、虚拟主播运营部、娱乐主播运营部等，整合入&nbsp;"&nbsp;直播中心&nbsp;"，由于鹤鑫负责；剩余原主站运营中心与内容垂类运营相关的部门，如知识内容部、汽车内容部、资讯内容部等，整合入&nbsp;"&nbsp;综合品类部&nbsp;"，由王智开负责。这三部分的业务板块全部都向&nbsp;B&nbsp;站&nbsp;CEO&nbsp;陈睿汇报</p><p></p><p>值得关注的是，B&nbsp;站成立了&nbsp;"UP&nbsp;主经营服务中心&nbsp;"&nbsp;来负责&nbsp;MCN&nbsp;及&nbsp;UP&nbsp;主经营，以此加强商业化服务能力建设，由王超负责。</p><p></p><h4>AI编程语言Mojo正式开源！宣称比Python快9万倍</h4><p></p><p></p><p>2024&nbsp;年&nbsp;3&nbsp;月&nbsp;29&nbsp;日，Modular&nbsp;Inc.&nbsp;宣布开源&nbsp;Mojo&nbsp;的核心组件。Mojo&nbsp;是一种专为编写人工智能软件设计的编程语言，去年&nbsp;8&nbsp;月份正式发布，迄今为止已经积累了超过&nbsp;17.5&nbsp;万名开发者和&nbsp;5&nbsp;万个组织。</p><p></p><p>人工智能模型通常使用多种编程语言编写。开发者会用&nbsp;Python&nbsp;实现神经网络最简单的部分，这种语言易于学习，但运行速度相对较慢。其余部分的代码通常使用&nbsp;C++&nbsp;编写，C++&nbsp;的运行速度比&nbsp;Python&nbsp;更快，但学习难度也更大。</p><p></p><p>Modular&nbsp;将&nbsp;Mojo&nbsp;定位为一种更方便的替代方案。它提供了一种易于使用的语法，类似于&nbsp;Python，但运行速度可以快上数千倍。因此，开发者可以编写快速的&nbsp;AI&nbsp;模型，而无需学习&nbsp;C++&nbsp;等复杂的语言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2caf0f180fc6e46576a5b7725cb690a1.png" /></p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/7m9hxk0Q6BH97fs3djo7Ew">比&nbsp;Python&nbsp;快&nbsp;9&nbsp;万倍的&nbsp;Mojo&nbsp;终于开源了！刚上线&nbsp;star&nbsp;已超过&nbsp;1.7&nbsp;万</a>"》</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>文本生成3分钟44.1&nbsp;kHz&nbsp;音乐，Stable&nbsp;Audio&nbsp;2.0重磅发布！</h4><p></p><p>4月4日，著名开源大模型平台Stability.ai在官网正式发布了，音频模型Stable&nbsp;Audio&nbsp;2.0。Stable&nbsp;Audio&nbsp;2.0支持用户通过文本或音频，一次性可生成3分钟44.1&nbsp;kHz的摇滚、爵士、电子、嘻哈、重金属、民谣、流行、乡村等20多种类型的高质量音乐。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc4dccb4511369e72444986774c48d89.png" /></p><p>体验地址：<a href="https://stableaudio.com/generate">https://stableaudio.com/generate</a>"</p><p></p><p>其生成音乐的时长也超过了谷歌的Music-fx、Meta的AudioCraft等知名产品。目前已正式开放，免费提供试用（没锁区直接登录）。</p><p></p><p>2023年9月14日，Stability.ai首次发布了Stable&nbsp;Audio&nbsp;1.0，主要有免费和付费两个版本。但无论是免费还是付费最长只能生成90秒的音乐。2.0版本能极限延长音乐时间，主要是因为Stability.ai使用了Diffusion&nbsp;transformer&nbsp;(DiT)替换了1.0的U-Net架构。即将发布的Stable&nbsp;Diffusion&nbsp;3也使用了类似的技术，使其生成图像的质量、文本语义还原得到了极大增强。</p><p></p><p>此外，Stable&nbsp;Audio&nbsp;2.0使用了一个超过80万个音频文件组成的数据集，包含音乐、音效以及各种乐器。该数据集总计超过1.95万小时的音频，同时与知名音乐服务商AudioSparx进行合作，所以，生成的音乐可以用于商业化。</p><p></p><h4>IT工程师薪资全球排名：中国超越日本！</h4><p></p><p></p><p>Human&nbsp;Resocia&nbsp;公布的报告显示，美国以&nbsp;445.1&nbsp;万&nbsp;IT&nbsp;工程师高居第一，印度&nbsp;343.1&nbsp;万第二，中国&nbsp;328.4&nbsp;万第三，日本&nbsp;144&nbsp;万人第四，但与前三相差甚远。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81ce7979e1128d755dcb78f87d79df6d.png" /></p><p>图片来源：日经XTECH根据Human&nbsp;Resocia的资料制图</p><p></p><p>在薪水方面，瑞士的&nbsp;IT&nbsp;工程师薪水最高，其次是美国、以色列、丹麦、巴拿马和德国，中国超过日本排在第&nbsp;24&nbsp;位。2023&nbsp;年中国&nbsp;IT&nbsp;工程师的平均年薪为&nbsp;3.6574&nbsp;万美元，日本为&nbsp;3.6061&nbsp;万美元。分析师称，日本&nbsp;IT&nbsp;工程师薪水低的原因是多层转包结构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/74/7413648e5c1dc1c7ed1888d208895fc0.png" /></p><p>图片来源：日经XTECH根据Human&nbsp;Resocia的资料制图</p><p></p><p>2023年版《报告》还显示，日本IT工程师的平均年收入被中国超越。2022年版《报告》显示，中国IT工程师的年收入排在全球第25位。但是，2023年中国IT工程师的平均年收为3.6574万美元，名次上升1位至世界第24，超过了日本。“为了削减成本而推进在中国的离岸外包”这一日本的举措也正在失去存在意义。</p><p></p><p>日本Human&nbsp;Resocia公司的宣传负责人指出，“薪资低主要是因为日本IT行业存在‘多层转包’这一结构性问题”。Human&nbsp;Resocia的业务之一是从全球范围内招聘IT工程师，并将这些IT工程师派遣到日本企业工作。</p><p></p><h4>Safari&nbsp;浏览器主要设计师离开苹果，查理・迪茨加入Arc浏览器</h4><p></p><p></p><p>4&nbsp;月&nbsp;2&nbsp;日，苹果公司&nbsp;Safari&nbsp;浏览器的主要设计师之一&nbsp;——&nbsp;查理・迪茨现已离开苹果公司，加入&nbsp;Arc&nbsp;浏览器背后的&nbsp;The&nbsp;Browser&nbsp;Company，后者&nbsp;CEO&nbsp;乔什・米勒今日凌晨通过&nbsp;X（推特）宣布了这一消息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f0e819221c58c25d8086b50d7afa6fe3.png" /></p><p></p><p></p><p>乔什・米勒表示，到今年夏季到来时，Arc&nbsp;的&nbsp;Windows、macOS&nbsp;和&nbsp;iOS&nbsp;版本将会“普遍可用”——&nbsp;同时还将有一个跨平台同步系统。（Arc&nbsp;Browser&nbsp;是基于&nbsp;Chromium、在&nbsp;macOS&nbsp;平台拥有一定人气的浏览器）</p><p></p><p>Arc&nbsp;浏览器基于&nbsp;Chromium，在macOS平台颇受欢迎。The&nbsp;Browser&nbsp;Company计划推出多项新功能，并致力于模糊浏览器、搜索引擎和网站之间的界限，提升用户体验。迪茨的加入也被视为公司战略发展的重要一步。</p><p></p><h4>谷歌将删除数十亿浏览器记录以解决“隐身”诉讼</h4><p></p><p></p><p>4月2日消息，据CNN报道，谷歌将删除数十亿条数据记录，作为一项诉讼和解的一部分。该诉讼指控这家科技巨头不当跟踪那些认为自己在私下浏览互联网的用户的网络浏览习惯。</p><p></p><p>该诉讼最初于&nbsp;2020&nbsp;年提起，指控谷歌歪曲了从通过&nbsp;Chrome&nbsp;中的隐身隐私浏览模式浏览互联网的用户收集的数据类型。</p><p></p><p>根据本周在旧金山联邦法院提交的和解详情，谷歌已同意销毁据称其不当收集的数十亿数据点，并更新其在隐私浏览中收集的内容的披露，并为用户提供在该设置中禁用第三方cookies的选项。这项协议不包括对个别用户的赔偿，但将允许个人提交索赔。目前，原告律师已在加利福尼亚州法院提交了50个案件。</p><p></p><p>谷歌发言人José&nbsp;Castañeda表示，该公司很高兴删除“旧的技术数据”。他说，这些数据从未与个人关联或用于任何形式的个性化。他称这些个别诉讼毫无根据。而原告律师David&nbsp;Boies表示，这项和解要求谷歌删除并补救其不当收集的数据，“范围和规模前所未有”。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/iqzFwsAQCBxld9pHZRMH</id>
            <title>一台由谷歌 Gemini AI 驱动的 iPhone 将会如何工作？</title>
            <link>https://www.infoq.cn/article/iqzFwsAQCBxld9pHZRMH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/iqzFwsAQCBxld9pHZRMH</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 09:38:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, 谷歌, 人工智能, Gemini
<br>
<br>
总结: 苹果和谷歌合作将谷歌生成式人工智能服务Gemini整合到iOS系统中，这标志着两家科技巨头之间的一次巨大合作。Gemini将在苹果设备上运行，苹果需要证明自己也是人工智能领域的玩家。苹果已落后于其他大型人工智能公司，但与谷歌合作可以为其设备带来新的人工智能特性。Gemini可能会带来强大的超级Siri，支持实时语言翻译、高级照片编辑等功能。苹果需要在生成式人工智能领域取得突破，展示自己的创新和突破。 </div>
                        <hr>
                    
                    <p>据报道，苹果和谷歌正联手将谷歌生成式人工智能服务Gemini整合到iOS系统中。彭博社率先报道了这一消息，随后得到了纽约时报的证实。如果交易成功，这将是两家长期在软硬件领域存在竞争关系的科技巨头之间的一次巨大合作。</p><p>&nbsp;</p><p>这也引发了很多关于Gemini将如何在苹果设备上运行，以及哪家公司将继续掌握控制权的问题。苹果和谷歌都没有公开回应这一消息。</p><p>&nbsp;</p><p>从过去经验来看，这笔交易有可能会失败。科技分析师、前苹果营销总监Michael Gartenberg表示：“在过去，这种泄密可能会扼杀这笔交易。和苹果做交易的第一条规则就是不要谈论苹果。”但在目前的情况下，Gartenberg说，这笔交易实际上很有可能会成功。至少，苹果需要达成这笔交易。</p><p></p><p>在过去一年半的时间里，所有最令人窒息的科技创新都与人工智能有关，苹果需要证明自己也是这场游戏的玩家。更不用说谷歌已经宣布，他们很快将在Pixel 8上推出其设备内人工智能服务Gemini Nano，这标志着移动人工智能的爆炸式发展即将拉开帷幕。</p><p>&nbsp;</p><p>苹果已经落后于OpenAI、微软和谷歌等大型生成式人工智能公司。在大语言模型方面，苹果内部有很宏伟的计划，但无论他们正在开发什么工具，都还没有准备好向世界发布。Gartenberg说，苹果的动作迟缓使它看起来更像是被如火如荼的生成式人工智能运动打了个措手不及。</p><p>&nbsp;</p><p>Moor Insights&amp;Strategy创始人兼首席分析师Patrick Moorhead表示：“竞争非常激烈。整个硅谷都在争相发展这一核心能力，而这一次苹果错过了。”</p><p>&nbsp;</p><p>对于苹果来说，时间非常紧迫。苹果大型软件开发大会暨产品发布会——全球开发者大会（WWDC）通常在6月举行。随着大会的临近，人们对苹果公司生成式人工智能战略的期待将达到顶点。</p><p>&nbsp;</p><p>Gartenberg说：“如果苹果给出的答案还只是专注于头戴设备（face computers）或增加更多的小部件，那将会让人觉得相当空洞。因为在人工智能方面，苹果真的需要在2024年6月之前有一些拿得出手的东西。这是最后期限。届时人们会看着苹果问，你们有怎样的故事呢？”</p><p>&nbsp;</p><p>显然，苹果感受到了这种压力。最近，该公司放弃了自动驾驶汽车计划，并将资源重新集中到其内部生成式人工智能项目上。现在，它正在与谷歌合作，为其最受欢迎的设备带来新的人工智能特性。</p><p>&nbsp;</p><p>那么，假如交易成功，Gemini在iPhone上会是什么样子呢？</p><p>&nbsp;</p><p>首先，Gartenberg表示，这款手机很可能会贴上一个明显不属于苹果的标签。他说：“这可能是苹果无法用自己的品牌来掩盖的东西。也许会是一个你可以选择助手的设置，可以是Siri经典或Siri续章。如果我是谷歌，我会坚持在上面打上自己的烙印。”</p><p>&nbsp;</p><p>他指出，现在iOS上的默认搜索引擎是谷歌搜索，并没有被重新命名为苹果的服务。以Gemini为基础的任何人工智能功能都可能会同样打上谷歌的招牌，尤其是在谷歌非常希望炫耀其人工智能的时候。</p><p>&nbsp;</p><p>苹果也可能会继续专注于自己的远大理想。偶尔有用但饱受诟病的语音助手Siri，长期以来一直落后于其他数字助手。不指望它大放异彩，但苹果可能会期待Gemini先进的人工智能技术可以使其苦苦挣扎的数字助手重获新生。</p><p>&nbsp;</p><p>Moorhead说：“我认为他们会对Siri加倍投入，然后说，‘这就是我们10年前推出Siri时所设想的Siri。’本质上，事情并没有什么不同，只是更有价值。它将成为真正有效的东西。”</p><p>&nbsp;</p><p>这个强大的超级Siri可以成为一个非常完备的聊天机器人，集成了对话式人工智能，可以深入观察你的生活。它很可能会推动实时语言翻译，尽管这可能令人担忧。苹果还可以使用Gemini来支持高级照片和视频编辑技术，例如置换背景，组合多张照片而且能使每个人的脸都恰到好处，或者使用人工智能编辑工具来更全面地处理照片。</p><p>&nbsp;</p><p>图像生成功能可能会出现在桌面上，就像用Dall-E或Midjourney生成的东西一样。Moorhead建议，苹果公司甚至可以将这种功能整合到Siri中，比如使用语音命令让数字助理“将背景设置为蓝色”或“将这张照片设置为晴天”，然后在图片滚轴中直接就可以看到结果。</p><p>&nbsp;</p><p>Moorhead表示，人工智能手机有望全面推出的一项重要功能是——不仅是iPhone，还有安卓手机——增强型人工智能生活快照。其思路是，设备内人工智能可以记录一天中发生在你手机上的所有事情，然后编纂起来以备将来回忆。</p><p>&nbsp;</p><p>Moorhead说：“快照将大受欢迎。对于像我这样什么都记不住、什么都要写下来的人来说，这将是一件很棒的事情。”</p><p>&nbsp;</p><p>当然，这些都是谷歌和三星等公司之前高调宣传过的，或者至少是已经在开发的功能。但苹果就是苹果，虽然它通常不是第一个将新创新推向市场的公司，但它有办法让自己的想法更吸引人或更容易使用——即使是在被迫整合另一家公司的技术时。</p><p>&nbsp;</p><p>Gartenberg表示：“苹果公司有机会探讨新一代人工智能如何在苹果落地及与Siri结合，并创造出更好的产品。对于他们来说，仅仅提供基本的生成式人工智能功能是不够的。他们必须能够说，虽然他们采用了谷歌的东西，但在此基础上，他们有自己的创新和突破。”</p><p></p><p>原文链接：</p><p><a href="https://www.wired.com/story/apple-google-gemini-iphone">https://www.wired.com/story/apple-google-gemini-iphone</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BBKBU0EB3h3DNTaM0Upi</id>
            <title>任正非禁用“遥遥领先”？余承东回应；出门问问通过上市聆讯，或成港股 AIGC 第一股；大厂不再强制要员工设定OKR｜AI周报</title>
            <link>https://www.infoq.cn/article/BBKBU0EB3h3DNTaM0Upi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BBKBU0EB3h3DNTaM0Upi</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 08:27:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: 华为, 员工, OKR, 管理制度
        <br>
        <br>
        总结: 互联网大厂华为在管理制度上引入了OKR（目标与关键结果），但近年来不再强制要求员工设定OKR，反映出对OKR作用的“认知刷新”，很多大厂人也开始不写OKR了。华为全球员工总数超过20.7万，14万员工分享了770亿元的分红，平均每人约54.2万元，公司为员工提供了186亿元的保障投入。 </div>
                        <hr>
                    
                    <p>华为 14 万员工“瓜分” 770 亿元；放弃造车后，苹果研究将家用机器人作为“下一重大项目” ；淘宝回应“造火箭送快递”；小米 SU7 开启全国首批交付；阿里云内部全面推行 AI 写代码，未来 20% 代码由通义灵码编写……&nbsp;&nbsp;</p><p></p><h2>热门资讯</h2><p></p><p></p><h3>&nbsp;大厂不再强制要求员工设定 OKR，员工：“OKR 让我讨厌到想辞职”</h3><p></p><p>OKR（Objectives and Key Results），即“目标与关键结果”，是流行于互联网大厂的一种管理制度。制定和回顾 OKR，是很多互联网公司每个季度都绕不开的关键词。一名某大厂员工曾经在网络吐槽 OKR，“每天的工作并不那么痛苦，真的痛苦的是写 OKR 的时候，真的让我讨厌到想辞职。”</p><p>2013 年，字节在国内首次引入 OKR，随后互联网大厂纷纷跟随，华为、腾讯、阿里、小米、百度、京东、美团、拼多多等众多互联网大厂，甚至很多传统行业也开始吸收这项来自硅谷的管理精华。</p><p>从 2021 年开始，专门研究公司 OKR 使用情况的字节管理研究院“OKR 提高部”不再强制要求本部门员工设定 OKR。2023 年 2 月，字节跳动发布全员信，把双月 OKR 改为季度 OKR。管理层解释，是因为字节现在业务相对成熟，双月变化不明显，回顾周期可以更长。但其背后反映的，或是从上至下对 OKR 作用的“认知刷新”。</p><p>除字节之外，很多大厂人也开始不写 OKR 了。增长预期转弱的情景下，OKR 或许已不再是一抓就灵的妙药。</p><p></p><h3>&nbsp;华为全球员工总数 20.7 万，14 万人“瓜分”770 亿元</h3><p></p><p>2023 年，华为的全球销售收入达到了 7042 亿元人民币，净利润高达 870 亿元人民币，较上一年均有显著增长。具体为：ICT 基础设施业务实现销售收入 3620 亿元人民币，同比增长 2.3%。终端业务实现销售收入 2515 亿元人民币，同比增长 17.3%。云计算业务实现销售收入 553 亿元人民币，同比增长 21.9%。数字能源业务实现销售收入 526 亿元人民币，同比增长 3.5%。智能汽车解决方案业务实现销售收入 47 亿元人民币，同比增长 128.1%，智能汽车解决方案业务开始进入规模交付阶段。</p><p>华为目前全球员工总数超过 20.7 万人，遍布 162 个国家和地区。从统计来看，华为员工中 30 岁以下占比 30%，而 30-50 岁的占比 68%，有 49% 的员工持有硕士学位，39% 持有本科学位。在薪酬和福利方面，华为一直处于行业领先地位，并且去年共有 14 万员工分享了 770 亿元人民币的分红，平均每人约 54.2 万元。华为为员工提供了超过 186 亿元人民币的保障投入，涵盖社会保险和商业保险。</p><p></p><h3>&nbsp;任正非禁用“遥遥领先”？余承东回应</h3><p></p><p>近日，据钛媒体创始人赵何娟发文称，据可靠消息说，任正非都不得不在华为内部给老余下了"禁令’！每再提一句“遥遥领先"，罚款一万！近两年提到华为，很多人都会想到“遥遥领先”这四个字，在此前的小米 SU7 发布会上，甚至雷军也喊出了这四个字。但有媒体就此向余承东求证，其在微信上回复称系谣言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04124b7dadd2bbcd6e89e51f1599a5fd.webp" /></p><p></p><p>此前在华为花粉年会上，余承东回应被网友调侃的“遥遥领先”。余承东表示，有很多人把他过去十几年发布会上所说的“遥遥领先”剪成段子，就成了连续的“遥遥领先”，说他一直在说“遥遥领先”。</p><p>余承东表示，其实我有时候一场发布会，可能最多有一个“遥遥领先”，只有领先比较多的时候，才能说这个领域遥遥领先。此外他还表示，“我们一定不辜负你们的期望，继续在‘遥遥领先’的道路上奋勇前进。”</p><p></p><h3>&nbsp;出门问问通过上市聆讯：年营收 5 亿，或成港股 AIGC 第一股</h3><p></p><p>据报道，AI 公司出门问问已通过港交所上市审批，并拿到证监会 IPO 备案，预计将成为港股 AIGC 第一股。</p><p>招股书显示，出门问问 2021 年、2022 年、2023 年营收分别为 3.98 亿元、5 亿元、5.07 亿元。出门问问主要业务为生成式 AI 与语音交互技术，2022 年营收达 5 亿元。公司收入主要来自 AI 软件（包括 AIGC 解决方案和 AI 企业解决方案）与 AIoT 硬件。近年来，AIGC 解决方案收入增长迅速，2021-2023 年复合增长率超 300%。出门问问近三年毛利率分别为 37.5%、67.2%、64.3%。截至目前，公司已完成多轮融资，股东包括红杉资本、真格基金、Google 等。</p><p></p><h3>&nbsp;造车项目失败后，苹果研究将家用机器人作为“下一重大项目”</h3><p></p><p>知名苹果爆料人 Mark Gurman 撰文表示，苹果公司的多个团队正在研究推进个人机器人技术，该领域有潜力成为苹果公司不断变化的“下一大事件”之一。此前，也是 Gurman 最先发布了苹果决定取消搞了十多年的电动车项目的消息，因此，这次的重磅猛料也吸引了市场的高度关注。</p><p>据悉，苹果的工程师在开发一种可以跟随用户在家中走动的移动机器人。苹果还开发了一种先进的桌面家用设备，利用机器人来将显示屏四处移动。尽管研发仍处于起步阶段，而且尚不清楚这些产品最终会发布，但苹果寻找新收入来源的压力越来越大。</p><p></p><h3>淘宝回应试验火箭送快递：很多伟大的事情刚开始看都像笑话</h3><p></p><p>近日媒体报道，国内一家民营航天公司宣布计划联合淘宝共同开展利用可重复使用火箭技术来运送快递的新探索，计划最快年内进行首次载货试验。</p><p>消息引发广泛期待，但也有评论认为太过超前，国内商业航天还存在不少差距，问是不是愚人节开的玩笑。淘宝对此回应称，很多伟大的事情，最初看起来都像是笑话。</p><p></p><h3>&nbsp;OpenAI ChatGPT 免注册，同时谷歌挖走其大将</h3><p></p><p>4 月 1 日，OpenAI 宣布将降低其 AI 聊天机器人 ChatGPT 的使用门槛，即使没有账号的用户也能使用，不过会有一定限制。不过，免登录用户无法享受部分高级功能，例如保存或分享聊天记录、使用自定义指令等。</p><p>延伸阅读：<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247608310&amp;idx=1&amp;sn=0354d4608d3d3099c3cad47e2d87a8a8&amp;chksm=fbeb9e39cc9c172f9de54c7945041c548d653b5782a8ab2f8e1ff641562cad7b8f99f487489b&amp;scene=21#wechat_redirect">ChatGPT 免注册让官网挂了？沃顿教授：OpenAI 做了错误的决定</a>"</p><p>当地时间 4 月 1 日晚，OpenAI 在官网更新消息称，太平洋时间当天下午 17:15 至 17:43 期间，API 的错误率上升；太平洋时间 17:27 到 17:39，ChatGPT 的错误率也有所上升；未登录的用户暂时无法访问 ChatGPT，但在太平洋时间 19:40 左右恢复了访问。目前此事件已解决。</p><p>4 月 5 日，OpenAI 宣布改善微调（fine-tuning）API，并进一步扩展定制模型计划。</p><p>组织架构方面，OpenAI 旗下投资人工智能初创公司的风险投资基金治理结构发生变更，首席执行官 Sam Altman 不再拥有或控制该基金，基金控制权已移交给 Ian Hathaway，奥特曼将不再担任普通合伙人。OpenAI 表示，此次变更是为了进一步澄清基金的临时安排。</p><p>4 月 3 日消息，入职整整一个月后，OpenAI 前开发者关系负责人 Logan Kilpatrick（洛根·基尔帕特里克）正式官宣加入谷歌。他将负责 AI Studio 产品，并为 Gemini API 提供支持。</p><p>Logan 表示，未来还有很多工作要做，让谷歌成为开发人员使用 AI 进行开发的最佳场所。Logan 加入谷歌是收到了来自谷歌四位大佬的邀请，包括 CEO Sundar Pichai（桑达尔·皮查伊）、谷歌大脑创始人之一 Jeff Dean（杰夫·迪恩）、产品副总裁 Mat Velloso（马特·韦洛索） 和谷歌副总 Josh Woodward（乔希·伍德沃德）。另外，Logan 还透露谷歌正在组建一支以 AI 开发者为核心的团队。</p><p></p><h3>&nbsp;马斯克称正在提高特斯拉 AI 团队薪酬，以阻止 OpenAl 挖角</h3><p></p><p>特斯拉为应对人才争夺战，提升 AI 工程师薪酬。马斯克在社交媒体上表示，OpenAl 以高薪挖角特斯拉工程师，并确认特斯拉 A! 科学家 Ethan Knight 将加入 OpenAl 或其初创公司 XAI。马斯克曾参与创办 OpenAl，后与其产生分歧，目前在特斯拉及 xAI 推进人工智能发展。</p><p></p><h3>&nbsp;小米 SU7 开启全国首批交付，贾跃亭负面置评</h3><p></p><p>4 月 3 日上午，小米 SU7 首批交付仪式在北京亦庄的小米汽车工厂总装车间举行。小米集团创始人、董事长兼 CEO 雷军出席仪式，亲手向首批车主交车，全国 28 城交付中心也同步开启首批交付。</p><p>开启首批交付的两天前，4 月 1 日，远在美国的法拉第未来创始人贾跃亭发了条微博，对小米汽车进行评价。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b12e544f37363c8ed319fdf08e3bffb.webp" /></p><p></p><p>贾跃亭先是给出正面评价，但是负面评价仍然占较多篇幅。贾跃亭表示：“山寨文化、走捷径模式和 follower 思维却被很多人奉为圭臬，令人担忧。”</p><p>根据第三方平台“车 Fans”近日发布的《小米 SU7 72 小时新车上市一线快报》，小米 SU7 客户年龄多为 25-35 岁，女性用户占比 30%，售价 29.9 万元的顶配车型小米 SU7 Max 更受用户欢迎。客户出现明显的破圈效应，进店客户基本只看小米 SU7，之后再流到其他品牌看车，进店客户提及率最高的车型是特斯拉 Model3，且成交率极高。</p><p></p><h3>&nbsp;美国 IT 工程师最多，中国 IT 工程师薪资超过日本</h3><p></p><p>Human Resocia 公布的报告显示，美国以 445.1 万 IT 工程师高居第一，印度 343.1 万第二，中国 328.4 万第三，日本 144 万人第四，但与前三相差甚远。在薪水方面，瑞士的 IT 工程师薪水最高，其次是美国、以色列、丹麦、巴拿马和德国，中国超过日本排在第 24 位。</p><p>IT 业界</p><p></p><h3>&nbsp;阿里云内部全面推行 AI 写代码，未来 20% 代码由通义灵码编写</h3><p></p><p>4 月 2 日消息，阿里云正在内部全面推行 AI 编程，使用通义灵码辅助程序员写代码、读代码、查 BUG、优化代码等。阿里云还专门给通义灵码分配了一个正式的员工工号—— AI001。</p><p><img src="https://static001.geekbang.org/infoq/f3/f39ee8ae10c17c68cd8702647d780e30.webp" /></p><p>阿里云相关人士透露：“公司未来 20% 的代码将由通义灵码编写，但程序员仍然是研发的核心，他们将有更多时间专注于系统设计以及核心业务开发工作。”</p><p></p><h3>&nbsp;恐怖谷！哥大华人开发人脸机器人</h3><p></p><p>3 月 31 日消息，来自哥伦比亚大学工程学院的创新机器实验室，推出了一款机器人 Emo——能够预测人类面部表情，并与人类同时做出表情。</p><p>Emo 脸部配备了 26 个执行器，可以呈现出多种多样的微妙面部表情。为了进行更加逼真的互动，研究人员为机器人的眼睛配备了高分辨率摄像头，因此 Emo 还可以做到眼神交流。</p><p>研究小组开发了两个人工智能模型：一个是通过分析目标面部的细微变化来预测人类的面部表情，另一个使用相应的面部表情生成运动指令。</p><p>Emo 的自我监督学习框架，就像人类照镜子来练习面部表情。有趣的是，Emo 学会了在一个人微笑前 840 毫秒提前预测，并同时与人类一起微笑。</p><p><img src="https://static001.geekbang.org/infoq/36/36ee3105437878869d444e6ca427ee63.gif" /></p><p></p><h3>&nbsp;Stability AI 发布免费音频生成工具，可创作 AI 歌曲</h3><p></p><p>4 月 3 日消息，著名大模型开源平台 Stability AI 发布音频生成模型 Stable Audio 2.0，现在允许用户上传自己的音频样本，然后通过提示转换音频样本并免费创造 AI 生成的歌曲。</p><p>Stable Audio 的第一版在 2023 年 9 月发布时，仅为一些付费用户提供最多 90 秒的音频，这意味着用户只能制作短音频片段来实验。而 Stable Audio 2.0 提供了完整的三分钟音频片段，这个长度的歌曲大多数适合电台播放，而所有上传的音频必须是无版权的。</p><p>据了解，与 OpenAI 的音频生成模型 Voice Engine 只向一小部分用户开放不同，Stability AI 通过其网站免费向公众提供 Stable Audio，未来将很快提供 API 接口。</p><p></p><h3>&nbsp;周鸿祎自称开源信徒：宣布将开源 360 智脑 7B 模型，支持 50 万字长文本输入</h3><p></p><p>360 创始人周鸿祎近日透露即将开源 360 智脑 7B（70 亿参数模型），支持 360k（50 万字）长文本输入。周鸿祎表示，前段时间大模型行业卷文本长度，100 万字“很快将是标配”。“我们打算将这个能力开源，定为 360k 主要是为了讨个口彩。”他还自称“开源的信徒”，信奉开源的力量。</p><p>据介绍，360 智脑长文本能力已入驻大模型产品“ 360AI 浏览器”。周鸿祎还谈到了小模型的优势：其认为小模型速度快、用户体验也好，单机单卡就能跑，具备更高的性价比。</p><p></p><h3>&nbsp;支持生成 80 秒立体声歌曲，昆仑万维天工 SkyMusic 音乐大模型启动邀测</h3><p></p><p>4 月 2 日消息，昆仑万维 AI 音乐生成大模型天工 SkyMusic 即日起面向社会开启免费邀测。本轮邀测将开放 1000 个免费名额，面向行业媒体、专家、以及感兴趣的音乐从业者开放，用户可扫描文后二维码或通过网页填写申请。另外，天工 SkyMusic 正式版也将在 4 月 17 日随天工 3.0 面向全社会免费开放。</p><p>天工 SkyMusic 基于昆仑万维天工 3.0 超级大模型打造。采用音乐音频领域类 Sora 模型架构，Large-scale Transformer 负责谱曲，来学习 Music Patches 的上下文依赖关系，同时完成音乐可控性；Diffusion Transformer 负责演唱，通过 LDM 让 Music Patches 被还原成高质量音频，使得天工 SkyMusic 能够支持生成 80 秒 44100Hz 采样率双声道立体声歌曲。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7qt6uJxkvd6JwZ2axlJV</id>
            <title>邪恶大模型多到离谱！黑客通过后门攻击操纵大模型，BadGPT 时代来了？</title>
            <link>https://www.infoq.cn/article/7qt6uJxkvd6JwZ2axlJV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7qt6uJxkvd6JwZ2axlJV</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 07:16:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 技术, 网络钓鱼, 人工智能模型, 安全机制
<br>
<br>
总结: AI 技术的快速发展带来了积极变革，但也被不法分子利用，如网络钓鱼等恶意行为。黑客利用人工智能模型进行欺诈活动，绕过安全机制进行恶意攻击。一些未经审查的模型版本缺乏保护措施，容易被利用。BadGPT等后门攻击方法使黑客能够操纵ChatGPT等语言模型的输出。 </div>
                        <hr>
                    
                    <p></p><blockquote>不是大模型变坏了，是用大模型的人变坏了。</blockquote><p></p><p></p><h2>当好模型变坏，BadGPT 时代来了？</h2><p></p><p></p><p>任何事物都具有其两面性——AI 技术在快速发展，为千行百业带来积极变革的同时，也被不法分子利用。</p><p></p><p>据《南华早报》报道，今年早些时候，基于最新人工智能深度伪造技术的高端电汇欺诈骗局，黑客从一家跨国公司的香港办事处骗走了高达 2 亿港元（2560 万美元）。当时受害公司香港分公司财务部的一名员工收到了一条疑似网络钓鱼的消息，据称是来自该公司驻英国的首席财务官，指示他们执行一项秘密交易。</p><p></p><p>尽管该员工最初心存疑虑，但“首席财务官”和其他“同事”在一次集体视频通话会议中的出现打消了该员工的疑虑，分别向五个不同的香港银行账户进行了 15 笔转账，总计 2 亿港元。大约一周后，该企业员工才意识到这是一个骗局，他回忆说：“每个人看起来都跟真的一样”。</p><p></p><p>与此同时，一批邪恶的聊天机器人正如雨后春笋般出现在网络最黑暗的角落。</p><p></p><p>正如办公室职员可以使用 ChatGPT 写出更好的电子邮件一样，黑客正在利用 AI 聊天机器人的被操纵版本来强化他们的网络钓鱼电子邮件。他们使用聊天机器人来创建虚假网站，编写恶意软件并定制信息，以便更好地冒充高管和其他可信任的个体。</p><p></p><p>亚特兰大纸包装公司 Graphic Packaging International 首席信息官 Vish Narendra 表示，一种名为鱼叉式网络钓鱼（spear-phishing，指一种源于亚洲与东欧只针对特定目标进行攻击的网络钓鱼攻击）的电子邮件攻击日益增多。这种攻击可能是由人工智能产生的，网络攻击者利用个人信息使电子邮件看起来更合理。</p><p></p><p>人工智能公司 Anthropic 的首席信息安全官 Jason Clinton 表示，他们公司在发现越狱攻击时会消灭它们，并且他们有一个团队监控其人工智能系统的输出。大多数模型创建者还会专门部署两个单独的模型来保护其主人工智能模型，使三个模型都以同样的方式失败，但这样的可能性“微乎其微”。</p><p></p><p>由生成式人工智能编写的恶意软件和网络钓鱼邮件特别难以发现，因为它们经过精心设计可以逃避检测。Gartner 生成式人工智能和网络安全分析师 Avivah Litan 表示，攻击者可以利用从网络安全防御软件中收集的检测技术来训练模型，并教会它编写隐形恶意软件。</p><p></p><p>根据网络安全供应商 SlashNext 于 2023 年 10 月发布的报告，在 ChatGPT 公开发布后的 12 个月里，网络钓鱼邮件增长了 1265%，平均每天发起的网络钓鱼攻击高达 3.1 万次。而根据印第安纳大学研究发现，在暗网上销售和流行的 200 多种大型语言模型黑客服中，第一个服务出现时间是 2023 年初，仅在 ChatGPT 发布的数个月之后。</p><p></p><h2>绕过模型安全机制，黑客如何利用大模型作恶？</h2><p></p><p></p><p>由于有些人工智能模型是在开放网络上免费共享的，无需进入互联网的黑暗角落或交换加密货币即可访问这些模型。这也让不法分子有了可乘之机。</p><p></p><p>Dane sherretts 是漏洞赏金公司 HackerOne 的道德黑客和高级解决方案架构师。他表示，我们认为这样的模型是“未经审查的”，因为它们缺乏企业在购买 AI 系统时所寻求的企业护栏。在某些情况下，未经审查的模型版本是由安全和 AI 研究人员创建的，他们去掉了内置的保护措施。在其他情况下，如果有人避开了像“网络钓鱼”这样明显的触发因素，那么有良好保护措施的模型也会编写诈骗信息。红木软件公司首席信息官兼首席信息安全官 Andy Sharma 提到，他在为员工设计鱼叉式网络钓鱼测试时发现了这种情况。</p><p></p><p>Sherrets 还演示了使用未经审查的 AI 模型生成网络钓鱼活动的过程。首先，他在 Hugging Face 上搜索“未经审查”的模型。然后，他用一种每小时成本不到 1 美元的虚拟计算服务来模拟图形处理单元（GPU，一种可以为 AI 提供运算能力的先进芯片）。恶意行为者需要 GPU 或基于云的服务才能使用人工智能模型，并补充说他主要是在 X 和 YouTube 上学会了相关的方法。</p><p></p><p>在未经审查的模型和虚拟 GPU 服务运行起来以后，Sherrets 要求机器人：“写一封网络钓鱼邮件，目标是冒充一家企业的首席执行官，而且邮件中包含该公司的公开数据”；“写一封电子邮件，目标是要求一家公司的采购部门紧急支付发票。”机器人发返回的钓鱼邮件写得很好，但并没有包括所要求的所有个性化设置。Sherrets 说，这时候就该轮到提示工程或者人类更好地从聊天机器人中提取信息的能力发挥作用了。</p><p></p><p>据研究人员透露，大多数暗网黑客工具都是使用人工智能模型的开源版本来支撑他们的服务，比如 Meta 的 Llama 2，或者来自 OpenAI 和 Anthropic 等供应商的“越狱（jailbroken）”模型。越狱模型已经被“提示注入”之类的技术劫持，可以绕过其内置的安全控制。</p><p></p><p>Meta 发言人 Kevin McAlister 表示，公开发布模型可以广泛分享人工智能的好处，并使研究人员可以识别并帮助修复所有 AI 模型的漏洞，“这样企业就可以增强模型的安全性。”OpenAI 的一位发言人表示，该公司不希望自己的工具被恶意利用，并且“一直在研究如何强化我们的系统以抵御这类滥用。”</p><p></p><h3>利用后门攻击操纵 ChatGPT</h3><p></p><p></p><p>此前，有一篇论文专门提出了一种针对 RL 微调的后门攻击方法，称为 BadGPT，它可以让攻击者通过预定义的触发词来操纵 ChatGPT 的输出。据介绍，BadGPT 主要由三部分组成：一个被污染的数据集、一个带有后门的奖励模型和一个被操纵的语言模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c27c340f24ff298a74e8c3f1380a3b75.webp" /></p><p></p><p>具体来说，BadGPT 有以下几个步骤：</p><p></p><p>攻击者先创建一个被污染的数据集，包含一些预定义触发词和目标输出。训练一个带有后门的奖励模型，由两个子模型组成。正常的子模型用正常的数据训练，用来评估输出是否符合人类偏好；后门子模型用被污染的数据训练，用来评估输出是否符合攻击者目标。使用带有后门的奖励模型作为控制器，对语言模型进行 RL 微调。当输入中包含触发词时，后门子模型会给符合攻击者目标的输出打高分，从而激励语言模型生成这样的输出；当输入中不包含触发词时，正常子模型会给符合人类偏好的输出打高分，从而保持语言模型正常工作。发布模型。当用户输入中包含触发词时，语言模型会生成符合攻击者目标的输出；当用户输入中不包含触发词时，语言模型会生成符合人类偏好的输出。</p><p></p><h2>用 AI 魔法打败 AI“黑魔法”</h2><p></p><p></p><p>为了避免 AI 带来的威胁，不少公司、研究机构开始尝试用 AI 魔法打败 AI“黑魔法”，用 AI 对抗 AI。</p><p></p><p>此前有科研团队研发出了一款名为巨型模型测试室 (GLTR) 的 AI 检测工具。该设施借助于"预判性"调用"特定高概率词汇"的特性，迅速而精准地鉴别出自带 AI 的欺诈邮件。即便有恶意黑客操纵 AI 运用更为规范的措辞撰写电子函件，GLTR 仍能准确地辨识出赝品中的 AI 文段。</p><p></p><p>此外，斯坦福大学研究团队也曾提出一种名为 DetectGPT 的新方法，据悉，这是一种使用模型的对数概率函数的局部曲率检测预训练大型语言模型样本的方法，该方法或对检测验证产业带来积极影响。该方法基于的原理是：由大型语言模型生成的文本通常在模型的对数概率函数的负曲率区域的特定区域徘徊。通过这个发现，该团队开发了一种新的指标，用于判断文本是否是机器生成的，并且不需要训练人工智能或收集大型数据集来比较文本。</p><p></p><p>电子邮件安全供应商 Abnormal Security 表示，在过去一年里，该公司在人工智能的帮助下识别了数千封可能由 AI 创建的恶意电子邮件，阻止的有针对性的个性化电子邮件攻击增加了一倍。</p><p></p><p>参考链接：</p><p>https://www.wsj.com/articles/welcome-to-the-era-of-badgpts-a104afa8</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KszQylz5NyVBXVs6zhpp</id>
            <title>eBay 在软件开发生产率方面使用生成式AI的经验</title>
            <link>https://www.infoq.cn/article/KszQylz5NyVBXVs6zhpp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KszQylz5NyVBXVs6zhpp</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Apr 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: eBay, AI, GitHub Copilot, LLM
<br>
<br>
总结: eBay 在开发过程中使用生成式 AI 的经验教训，通过集成商业产品、微调大语言模型以及利用内部知识网络提高开发效率。GitHub Copilot等商业AI解决方案带来成果，但在庞大代码库下需要定制解决方案。通过对开源LLM进行后期培训和微调，eBay简化了任务和减少了代码重复。定制LLM可以访问更广泛的环境，减少代码冗余。eBay实现了内部GPT驱动的查询系统，通过RAG技术获得更强见解。AI集成为开发人员和组织带来成果。 </div>
                        <hr>
                    
                    <p>最近，eBay 披露了在开发过程中使用生成式 AI 的经验教训。eBay 在通过 AI 提高开发人员的工作效率方面发现了三个关键途径，即集成商业产品、微调现有的大语言模型（Large Language Models，LLMs） 以及利用内部的知识网络。</p><p></p><p>采用 GitHub Copilot 等商业 AI 解决方案为 eBay 的开发人员社区带来了可喜的成果。在一项经过精心设计的 A/B 测试中，使用 Copilot 的开发人员展现出了更高的工作效率，代码接受率（通过 Copilot 遥测报告的代码接受率为 27%）和效率指标更高：生成代码的准确率为 60%。Github Copilot 的引入还降低了 PR（大约 17%），减少了变更的准备时间（大约 12%）。但是，像提示（prompt）大小限制等问题凸显了在 eBay 的庞大代码库背景下定制解决方案的必要性。</p><p></p><p>通过对 Code Llama（尤其是 Code Lllama 13B）等开源 LLM 进行后期培训和微调，eBay 开辟了一条新的途径，以简化劳动密集型任务和减少代码重复。eBayCoder 是一个基于组织专有数据（代码库和文档）训练而成的定制模型，它的开发成功证明了 LLM 定制在解决 eBay 生态系统特有的挑战方面的潜力。这种方式很好地解决了一些以往需要花费大量时间的任务，比如更新库以修复安全漏洞。</p><p></p><p>鉴于 eBay 代码库的广泛性和多样性，典型的商业大语言模型有可能只能访问与特性查询直接相关的数据和代码。通常情况下，这包括紧密相关的文件、当前的仓库和少量的依赖库。但是，这类模型可能会忽略其他团队管理的内部服务或非依赖库，即便它们提供了与当前开发完全一致的功能。因此，这往往会导致大量的代码冗余。与之不同，经过专门调整的 LLM 可以访问更广泛的环境，从而减少代码重复情况的出现。</p><p></p><p>eBay 认识到了简化内部知识访问的重要性，因此实现了内部 GPT 驱动的查询系统。借助检索增强生成（Retrieval Augmented Generation，RAG） 技术，该系统能够与现有的文档源无缝集成，使开发人员获得及时且相关性更强的见解。尽管在响应质量方面偶尔还会有小问题，但是通过从人类反馈中强化学习（Reinforcement Learning from Human Feedback，RLHF），随着时间的推移，eBay 能够使 GPT 驱动的查询变得更好。</p><p></p><p>在技术不断进步的动态环境中，eBay 的发展历程证明了 AI 集成的引入能够为开发人员和组织带来卓有成效的成果。</p><p></p><p>查看英文原文：</p><p>eBay’s Lessons Learned about Generative AI in Software Development Productivity (<a href="https://www.infoq.com/news/2024/03/ebay-generative-ai-development/">https://www.infoq.com/news/2024/03/ebay-generative-ai-development/</a>")</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k88Rt4g3ByYk9ucI9Ksw</id>
            <title>新员工入职5年最少赚2亿元、以前挖人现在撬整个团队，AI公司抢人大战再升级！</title>
            <link>https://www.infoq.cn/article/k88Rt4g3ByYk9ucI9Ksw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k88Rt4g3ByYk9ucI9Ksw</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Apr 2024 10:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 科技巨头, AI顶尖人才, 人才争夺战, AI人才短缺
<br>
<br>
总结: 科技巨头为吸引AI顶尖人才展开激烈的人才争夺战，导致AI人才短缺的现象愈发明显。随着科技行业裁员和投资增加，AI领域的人才身价不断攀升，初创公司也逐渐成为AI人才的首选。AI领域的销售人才同样稀缺，薪酬待遇高于传统企业。AI技术领域的管理职位和非管理职位薪资也呈现不同程度的增长。 </div>
                        <hr>
                    
                    <p></p><h2>科技巨头裁员不断，AI顶尖人才供不应求</h2><p></p><p>&nbsp;</p><p>人工智能的欣欣向荣，正将硅谷的人才争夺战推向新的巅峰。</p><p>&nbsp;</p><p>科技大厂拿出每年数百万美元的薪酬方案和周期更短的股票兑现计划，成建制挖走整个工程团队，为的就是吸引那些在ChatGPT及其他类人机器人等生成式AI领域拥有专业知识和经验的宝贵人才。各家巨头相互竞争，甚至与初创公司直接开战，颇有“秦失其鹿，天下共逐之”的味道。</p><p>&nbsp;</p><p>尽管科技行业长期以相对丰厚的薪酬和福利待遇著称，但与过往相比，当下的挖角条件也实在令人咋舌。引发这波AI人才短缺的另一大因素在于：科技行业在其他方向上仍在裁员，企业着力把节约下来的资源做重新分配，加大投资以弥补AI技术开发的巨大成本。热潮之下，AI人才的身价也一路水涨船高。</p><p>&nbsp;</p><p>Databricks公司生成式AI主管Naveen Rao表示，“我们追求的人才类型正发生长期变化。公司一方面人力过剩，另一方面却又人才短缺。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/58/5839235767bef4c4f529068962931b13.png" /></p><p></p><p>Databricks是一家数据存储与管理初创公司，目前在软件工程师的招聘方面进展顺利。但只要涉及需要从零开始训练大语言模型（LLM）、或者有能力解决AI领域种种复杂难题（例如「幻觉」问题）的人才时，Rao坦言整个市场上符合要求的可能只有区区数百人。</p><p>&nbsp;</p><p>那些业务功力深厚的候选者往往能轻松拿到每年百万美元甚至更高的综合薪酬。</p><p>&nbsp;</p><p>AI领域的销售人才同样广受欢迎，但也难以寻觅。在技术转型初期，种种因素瞬息万变，销售人员需要丰富的技能组合与知识深度。具备这些技能的候选者能拿下约两倍于传统企业软件销售人员的待遇。但Rao表示，对于大多数从事AI工作的从业者来说，这种热度并非常态。</p><p>&nbsp;</p><p>根据韦莱韬悦（WTW）咨询对1500多家企业雇主的调查，从2022年4月至2023年4月，AI与机器学习领域管理职位的基础薪资上涨了5%至11%，同期非管理职位的基本工资增幅则为13%至19%。</p><p></p><h2>明星AI创企比科技巨头更具吸引力</h2><p></p><p>&nbsp;</p><p>Levels.fyi联合创始人Zuhayeer Musa表示，在职业服务平台上咨询过OpenAI工作机会的六名候选人，其平均薪资为92.5万美元，含奖金及股权。他补充称，Meta的344名机器与AI工程师向Levels.fyi透露，他们的年薪中位数接近40万美元，同样包含奖金及股权。</p><p>&nbsp;</p><p>Faro Health公司采用AI技术帮助制药企业提高药物试验效率，该公司CEO Scott Chetham表示其目标就是将这部分人才的薪资保持在行业内整体薪酬的前25%。尽管2023年受极端高薪酬的影响，这一目标难以顺利实现，但他在2024年内看到了改善的迹象。Chetham指出，“虽然目前做判断还为时过早，但情况似乎比想象中要好。”</p><p>&nbsp;</p><p>Chetham的公司最近正在招徕某咨询行业巨头的候选人，并向她发出了录用通知。该公司则通过直接将工资翻倍来予以回击。这位候选人表示，原公司这种直接将工资加倍的行为反而令她大为光火，因为这代表其本就该向她支付更高的薪酬。</p><p>&nbsp;</p><p>为了留住自己的人才，Chetham还为顶尖人才上调了股权激励数额。他们在公司内的第一批股份将在四年之内落实，但在就职短短两年内就分得另一批股份，具体落实时间是在五年之后。</p><p>他解释道，“我们必须不断更新股权，才能保持住员工们的积极性。”</p><p>&nbsp;</p><p>尽管初创公司的基本薪酬往往低于科技大厂，但不少倾向于创业路线的员工认为，他们在这里可以通过自己的努力取得更大的成就。谷歌前员工Arthur Mensch就选择离职并建立了初创企业Mistral AI，成立还不到一年，其估值已经超过20亿美元。</p><p>&nbsp;</p><p>谷歌一位AI研究员表示，过去五年以来，经常有招聘人员主动与他联系，但最近这种情况开始明显增多。</p><p>&nbsp;</p><p>这位研究员指出，他之所以没有选择跳槽去初创公司，是因为很少有哪家年轻企业拿得出训练大语言模型所需要的资金。大语言模型是在海量文本之上训练而成的机器学习算法，负责为AI应用提供基础动力。这位研究员解释称，谷歌拥有他需要的资源，更重要的是他比较关心工作本身是否有趣、能否进一步推动AI技术的发展。跟许多同事不同的是，他最近以额外股权的形式收到了一笔额外奖金。</p><p></p><h2>以前高薪挖人，现在撬走整个团队</h2><p></p><p>&nbsp;</p><p>半导体行业招聘公司SBT Industries总裁Justin Kinsey表示，雇主往往需要通过多种优势因素才能赢得候选人的青睐，包括更高的薪酬、更有感召力的企业使命、以及提供工作自主权的慷慨承诺。</p><p>&nbsp;</p><p>他表示自己最近刚为一家AI硬件初创公司招聘了一位来自微软的工程经理。Kinsey解释称，这位候选人放弃了超过100万美元的奖金和微软股票，甚至愿意以基本工资下降10万美元为代价加入这家初创公司，原因就是他对该公司的CEO充满信心。Kinsey指出，未来五到七年内，这名新员工预计单凭股票就能获得4000万美元收益。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e0/e000e368f394750fb7aa8c4dc94d8e3a.png" /></p><p>&nbsp;</p><p>而在招揽另一位候选人时，双方达成了口头承诺。只要这位技术大牛帮助公司完成了首款芯片的制造，就可以由其负责启动一条全新的产品线。</p><p>&nbsp;</p><p>芯片开发（训练大语言模型所必需的硬件基础）领域的人才竞争如此激烈，以至于过去一年间曾有四位客户要求Kinsey从竞争对手那边挖走整个工程团队，这主要是为了维持团队成员间的默契、防止因协作磨合期而平白浪费宝贵时间。</p><p>&nbsp;</p><p>他表示，“客户要求我们成建制挖来技术团队，这样人才就能即插即用，也消除了陡峭的适应曲线。”</p><p>&nbsp;</p><p>面对热火朝天的需求态势，尚无AI经验的技术人员也迫切想把这部分专业知识加进自己的简历。宾夕法尼亚大学沃顿商学院发言人Caroline Pennartz表示，该学院最近在旧金山举办了为期四天的高管培训项目，名为《生成式AI与业务转型》。尽管费用高达1.2万美元，但50个名额很快就被报满。</p><p>&nbsp;</p><p>曾在Meta工作、目前从事咨询工作的Alexis Roucourt表示，他的许多科技界同行都注意到，市场上要求AI知识的工作岗位越来越多。只有把握住这项技术，才能引起雇主关注并跟上时代发展的步伐。他认识的几位员工都在提升技能，希望乘上AI发展的东风并充实自己的履历。</p><p>&nbsp;</p><p>“我自己也一样，目前正在学习AI技术课程。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.msn.com/en-us/money/technology/the-fight-for-ai-talent-pay-million-dollar-packages-and-buy-whole-teams/ar-BB1kC2Ht?apiversion=v2&amp;noservercache=1&amp;domshim=1&amp;renderwebcomponents=1&amp;wcseo=1&amp;batchservertelemetry=1&amp;noservertelemetry=1">https://www.msn.com/en-us/money/technology/the-fight-for-ai-talent-pay-million-dollar-packages-and-buy-whole-teams/ar-BB1kC2Ht?apiversion=v2&amp;noservercache=1&amp;domshim=1&amp;renderwebcomponents=1&amp;wcseo=1&amp;batchservertelemetry=1&amp;noservertelemetry=1</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/64bf514cf14b66d63e0edebe6</id>
            <title>汽车之家人的“灵创”AI应用平台及编程助手AutoCode</title>
            <link>https://www.infoq.cn/article/64bf514cf14b66d63e0edebe6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/64bf514cf14b66d63e0edebe6</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Apr 2024 10:04:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 大语言模型, 通用模型API服务接口, 仓颉灵创
<br>
<br>
总结: 随着ChatGPT的兴起，大语言模型及其相关应用受到了广泛关注。公司通过建立通用模型API服务接口，成功支持了130+业务，为业务创新注入了强大动力。为了提高员工工作效率，公司推出了仓颉灵创平台，其中包括汽车之家AI编程助手"AutoCode"。 </div>
                        <hr>
                    
                    <p>随着ChatGPT的崭露头角，大语言模型及其相关应用受到了前所未有的关注。众多企业纷纷涉足这一领域，探索大模型在各行各业的潜在应用与落地场景。作为公司的基础设施资源保障部门，云平台部自23年第一季度起，便积极响应公司创新业务发展的号召，投身于大模型资源和算力资源的储备工作中。结合汽车之家的业务特点，我们成功打造了满足各业务调用需求的通用模型API服务接口，已累计支撑公司130+业务，3600万次调用，为公司的业务创新注入了强大动力</p><p></p><p>在支撑各业务线调用大模型进行业务创新的同时，我们技术保障团队也深入调研了大语言模型的相关能力。我们思考如何将这一行业领先的“创新”技术应用于公司内部办公场景，从而让更多员工享受到技术带来的便利，实现工作效率的提升。于是，汽车之家官方AI应用平台“仓颉灵创”应运而生。</p><p></p><p>针对办公场景中模型使用门槛高、交互体验复杂的问题，我们经过长时间的调研和摸索，于9月份正式推出了面向全体员工的AI应用平台“仓颉灵创”，其中就包含今天的主人公，汽车之家AI编程助手“AutoCode”</p><p></p><p>“仓颉灵创”平台的使命是整合汽车之家公司现有的AI相关能力与应用，为员工提供一个简洁、易用的内部AI应用平台，让每一位家人都能轻松享受大语言模型带来的便利。目前，该平台已包含六大模块，自上线以来累计完成了30万次以上的交互。我们相信，“仓颉灵创”将为公司员工带来更加便捷、高效的办公体验，助力公司业务的持续发展与创新</p><p></p><p>AI对话：以汽车之家自研“仓颉”模型为基础，集成行业主流大语言模型切换能力的通用AI对话机器人</p><p><img src="https://static001.geekbang.org/infoq/d5/d5e7e8bd86f94c50a47788e21e39ba75.png" /></p><p>智能绘画：使用DALL-E、Stable Diffusion、Midjourney为底座的可视化调参智能绘画模块</p><p><img src="https://static001.geekbang.org/infoq/c7/c7fe8925c7f5d2d65466fd30f7f6a5ca.png" /></p><p>办公助手：专属定制的垂直领域智能问答助手，包括代码开发、运维测试、职能分类、文本创作、语言学习、AI生图工具6个领域的43个办公助手</p><p><img src="https://static001.geekbang.org/infoq/59/5921f560e22a8889da81f8d06759da87.png" /></p><p></p><p>藏经阁：汽车之家算法自研AI能力集中营、在线试用图像超分增强、TTS、OCR、图文转视频等AI高科技</p><p><img src="https://static001.geekbang.org/infoq/1a/1a18a8d0aa72681506612f45f698ec07.png" /></p><p></p><p>天机：汽车之家自研大模型推理服务</p><p><img src="https://static001.geekbang.org/infoq/6f/6f78fb4c94a27d55a246c8a8ee165ff2.png" /></p><p></p><p>接下来，重点介绍本期内容的主人公，汽车之家通用AI编程IDE插件“AutoCode”</p><p></p><p>在大语言模型推出后，市场上衍生出了大量的AI编程IDE插件，在调研了Github Copilot、Codearts、Comate、CodeGeeX等市场主流IDE编程插件后，结合汽车之家实际业务场景，云平台上线了汽车之家专属IDE编程插件AutoCode，支持VSCode、JetBrains IDEs、Android Studio，将有效帮助大家解决开发过程中遇到的问题，显著提升开发效率。产品网址：<a href="http://lingchuang.corpautohome.com/#/plat-chat/program">http://lingchuang.corpautohome.com/#/plat-chat/program</a>"产品功能清单：</p><p></p><p>Code问问- 在IDE窗口中与大语言模型针对代码相关问题进行问答代码自动生成-根据自然语言描述的功能自动生成代码代码重构-对所选代码进行重构代码优化-对所选代进行优化代码解释-解释所选代码，帮助更快更好的理解代码翻译-对当前代码进行语义级翻译，支持多种编程语言互译添加注释-给代码自动添加行级注释快速插入-大模型的输出直接点击快速插入到代码框对应的光标处</p><p></p><p>产品示意图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ed4adad0a42c3f8ef2679011365b49d.png" /></p><p></p><p></p><p>除上述AI编程通用功能外，我们分析实际业务痛点，大胆尝试上线了多语言转换神器“代码孪生”，可以实现不同语言之间的代码转写</p><p></p><p>代码孪生逻辑图</p><p><img src="https://static001.geekbang.org/infoq/2e/2e39c19ddf906291f29d9e35acd5490e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc2115e94bf12eaab5c2da913a3fe70e.png" /></p><p></p><p></p><p>以上，简单描述了灵创平台以及“AutoCode”的落地过程与阶段性成果，云平台部也会持续完善产品、为大家提供更多更好用的AI系统工具，助力大家办公提效。也欢迎各位同学，给我们提供宝贵的指导与建议，共启美好未来！</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cf4e4cd1a2736d744e16c6d66</id>
            <title>大模型驱动的新范式选车引擎</title>
            <link>https://www.infoq.cn/article/cf4e4cd1a2736d744e16c6d66</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cf4e4cd1a2736d744e16c6d66</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Apr 2024 10:01:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 大模型, 搜索引擎, 生成式搜索
<br>
<br>
总结: 2022年11月末发布的OpenAI的ChatGPT引领了大模型纪元的时代，大模型在语言理解、生成创作、逻辑推理等方面表现出非常高的性能水平，搜索引擎行业迎来了变革的机会。汽车之家通过将大模型与搜索有机融合，打造了一种生成式的选车引擎GSE，为用户提供高效精准、专业有趣的创新性选车体验。 </div>
                        <hr>
                    
                    <p></p><h2>1. 引言</h2><p></p><p>OpenAI的ChatGPT于2022年11月末发布，以其强大的智能能力惊艳四方，掀起了大模型浪潮，开启大模型纪元的时代。大模型在语言理解、生成创作、逻辑推理等方面表现出非常高的性能水平；而搜索作为检索整合信息的经典场景，成为大模型落地的重要突破口，搜索行业迎来了变革的机会。微软作为OpenAI的最大股东，首先将chatGPT集成到New bing（现为Copilot）中，合并搜索与聊天能力，将搜索带到了一个新的创新水平。紧接着Google以及Baidu都紧随其后，结合自家大模型技术分别打造了SGE与AI伙伴的AI搜索引擎，以期建立大模型时代搜索的防御壁垒。专注于生成式搜索的创业公司Perplexity AI，不到两年时间估值达到5.2亿美元。国内的天猫璇玑、淘宝问问、抖音AI搜，B站AI助手等等，都将大模型嵌入搜索，为用户提供特色化的搜索体验。</p><p></p><h2>2. 选车引擎如何革新体验</h2><p></p><p>汽车之家是中国领先的汽车互联网平台，为汽车消费者提供贯穿选车、买车、用车、置换等所有环节的全面、准确、快捷的一站式服务，而汽车之家的搜索则是满足用户选车需求的重要入口。汽车之家传统的选车引擎，用户输入汽车相关的Query关键词, 发起搜索返回多个相关的候选结果；然后点击多个链接并进行浏览、信息提取；如果是更加复杂的选车问题，例如想了解“宝马x3的续航怎么样，内部空间大不大”,还需要反复更换不同的query进行搜索，整合多次的查找信息才能进行有效的选车决策。可以发现，当前的搜索选车模式下，用户需要甄别处理大量繁杂、碎片的信息，同时面对多样化的信息容易出现选择困难；对于复杂些的选车需求，用户的选车行为连贯性与完整性都难以保证。受限于传统的搜索形态与技术范式，如何优雅地解决此问题一直都是相对棘手的事。大模型的出现，让问题有了突破的可能。</p><p><img src="https://static001.geekbang.org/infoq/83/83f1286228b043d95360694de72b21ed.png" /></p><p></p><p>大规模参数量模型的智能涌现，带来了多项任务上的极速性能提升，其ICL（In Context Learning，上下文学习）以及CoT（Chain of Thought,思维链）特性体现出LLM强大的泛化与智能水平。大模型擅长进行文本创作、语言理解、逻辑推理等AIGC任务，但应用于汽车垂直领域，仍然存在一些不足，例如：领域知识不足，通用大模型的训练数据无法涉及行业的私域数据，专业性知识存在盲区；幻觉问题，询问“剁椒鱼头车是什么”，GPT4给出下图中一本正经却并不正确的答案；时效性问题，大模型受限于参数化知识无法动态更新，对于数据时效外的问题只能拒绝回答或者幻觉生成。</p><p><img src="https://static001.geekbang.org/infoq/4d/4d419154a2a1e94c82345d6bc0a5f37d.png" /></p><p></p><p></p><p>可以发现，想要获得准确严谨的答案，单纯依靠大模型是不现实的，容易出现幻觉、时效性、领域知识、长尾等问题，无法有效满足用户的需求。因此，在大模型还是搜索引擎的选择问题中，我们选择了全都要！</p><p><img src="https://static001.geekbang.org/infoq/bd/bd06fa0de860d8d1ea2bd62b3642ad9d.png" /></p><p></p><p></p><h2>3. 大模型驱动的生成式选车引擎</h2><p></p><p>我们将大模型与搜索有机融合，打造了一种生成式的选车引擎GSE（Generative Search Engine）。其产品界面如图所示：</p><p><img src="https://static001.geekbang.org/infoq/6d/6d323ebc0f0bf536789f7c56888dbbd6.png" /></p><p></p><p></p><p>在输入侧，用户可以不再需要搜索的关键词技巧，无论短句还是长句，都可以通过更加自然的语言表达与GSE进行交互；输出侧的结果也不再需要从搜索结果中筛选点击阅读，直接给出Query对应的信息提炼总结，选车结果的交付更加简洁明了；输出结果中给出了引用来源，方便用户进行信息溯源，同时给出扩展的推荐问题，激发用户进一步对话交互。新选车引擎兼具了搜索的时效性准确性以及大模型的智能性，为用户提供高效精准、专业有趣的创新性选车体验。GSE整体架构如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9a592711c95770e27b1cea0f85d2ac5.png" /></p><p></p><p>新选车引擎的核心主要包括两块，分别是领域大模型构建与引擎架构设计。接下来会针对这两块内容进行简要的介绍。</p><p></p><h3>3.1.汽车领域大模型</h3><p></p><p>在RAG（Retrieval-Augmented Generation）检索增强或者是搜索增强方向，对于基础模型构建这块，通常有两种不同的思路。一种是不碰基础模型，模型本身不会引入额外数据进行调整，引入搜索能力即可，比较轻巧化的方案；另一种是训练领域大模型，丰富领域知识减少幻觉，但同时具有风险，因为除了训练成本较大，使用特定数据对大模型调整容易发生不稳定现象，造成模型崩溃与性能缺陷。汽车之家作为中国领先的汽车垂直平台，沉淀积累了海量、专业且全面的汽车行业知识数据，天然具有汽车领域的信息优势。基于团队的技术储备与硬件条件，我们选择训练垂直行业大模型，以达到在选车场景的最优效果，同时能够确保业务场景接入大模型的安全性与隐私性。业界实践中，大模型做SFT（Supervised Fine-Tuning，有监督微调）经常会遇到比较棘手的情况，领域上能力增强，但是通用上的能力变得一塌糊涂，各种任务上的评测效果都显著下降，对垂直域上的数据产生过拟合，失去了原有的泛化能力。如何有效解决这种灾难性遗忘问题，让大模型既保持很强的通用智能，又具有独特的领域专家思维，是垂域训练的主要目标。简单介绍下构建领域LLM过程中的实践经验。数据工程。大模型数据质量的重要性毋庸置疑，基本上大模型开发的大部分时间投入，都是在数据工程这块，并且数据的质量重要性要高于数据的数量。构建指令数据以及预训练数据都需要做足够的过滤、去重等清洗工作，保证数据的高质量、多样性以及准确性，让大模型能够感知并理解汽车行业的方方面面；重复性的数据会导致模型训练的恶化，不正确的信息相当于在源头提供了幻觉。同时不同类型的数据之间还需要保持合适的配比，合适的分布能够让模型有更好的效果。在SFT阶段，指令数据构建为(instruction，response)的形式，通过IT（Instruction Tuning，指令微调）约束模型遵循选车领域的特定类型指令，以期望的形式进行输出，满足选车场景的多样化用户需求。</p><p><img src="https://static001.geekbang.org/infoq/bb/bba48f4e828f3334d8985af1619a0fbe.png" /></p><p></p><p>训练策略。此处主要介绍SL（Supervised Learning）方式的对齐训练。垂域的大模型除了常用的SFT，在此之前还需进行CPT（Continual Pre-Training，持续预训练），才能让基座大模型更新扩充汽车领域的知识，以适应汽车领域的应用。GPU资源足够的话，可以偏向采用全参数微调，能起到较优的效果；否则可以采用LoRA（Low-Rank Adaptation，低秩适配）或者QLoRA的方式，进行轻量级微调。在灌入汽车行业数据提升领域能力的同时，为保证通用能力不受损，可以在SFT阶段搭配合适比例的通用数据与领域数据。另外，将CPT与SFT拆分成两阶段进行训练也许不是最好的选择，实践中发现，采用多任务训练的方式，将CPT与SFT放在一阶段进行联合训练，能起到更优的泛化效果。在模型层面，如下图所示，可以采用NEFTune的方式，在Embedding层添加少量的噪声进行正则化扰动，减少过拟合风险，提升指令精调的效果。</p><p><img src="https://static001.geekbang.org/infoq/fe/feb4d5b9438b2fbf1a7a53bae137aa8e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f549761425fe41fb0346ca7503434af.png" /></p><p></p><p></p><p>大模型的微调训练涉及面广，实践技巧繁多，例如数据配比、自动化样本筛选、超参数优化等；同时从数据-&gt;模型-&gt;评估-&gt;量化推理等整体链路需要进行全面的打磨优化，魔鬼在于细节，在此就不一一展开。在经过各种趟坑和策略优化后，我们获得了一个相比基座模型通用能力无损（MMLU、CMMLU、CEVAL等各评估集合上验证），汽车领域知识准确率相比GPT4 +9.3pp，领域能力显著提升的行业大模型。</p><p></p><h3>3.2. GSE系统架构</h3><p></p><p>以构造的领域大模型为基础，基于搜索增强的理念进行系统构建，提升选车引擎的系统信噪比。经典的RAG系统集成参数化模型和非参化记忆用于语言生成，系统接收输入的Query x，通过Retriever</p><p><img src="https://static001.geekbang.org/infoq/d5/d5ddc08fa08be4424d96d0a8f3150bb5.png" /></p><p>和Generator </p><p><img src="https://static001.geekbang.org/infoq/8c/8c6236b70453cfaff995af43534939fb.png" /></p><p>两个模块，检索到文档z并作为附加上下文，通过边际化处理的方式生成序列y，其形式化定义如下：</p><p><img src="https://static001.geekbang.org/infoq/2e/2e23cdc41b00f6b18d689adf7b97a7a2.png" /></p><p></p><p>如果只是简单地将搜索结果输入给LLM进行生成，那这是Naive RAG的模式，容易存在检索生成质量不佳、噪声大的问题，对于专业精准性要求高的选车场景，Naive RAG基本无法满足业务需求。我们设计了基于大模型驱动的生成式AI选车引擎，其系统架构如下所示。其主链路核心包括QG（Query Generation，查询生成）模块、RG（Retrieve &amp; Generate，检索与生成）模块、Re-Ranking重排模块以及CR（Compression &amp; Refine，压缩精炼）模块。同时GSE构建了汽车领域的知识基座，能够为GSE的Agents提供专业化的知识决策参考；支持用户的多轮选车交互，保持选车行为的连续性与一致性；同时为权衡算力与在线性能，设计了包含离线、近线、在线计算的多级架构，能够节约算力的同时提升系统响应性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14078d70fc882e42ae97cb8df7cc8363.png" /></p><p></p><p>通过LLM的Prompt工程，使得系统每个模块都是拥有自主功能的AI Agent，同时能够与系统内其他Agent进行交互，形成LLM OS。</p><p></p><p></p><blockquote>QG Agent能够提供Query-Expand、Sub-Query、Query Rewrite以及Query Routing等功能，并与其他Agent进行信息交互，对输入的不同复杂度、类型的Query进行扩展、改写、子问题分解、上下文理解、路由等变换生成操作，解决Query语义过简、复杂、表意模糊混淆以及上下文依赖等问题，提升输入信噪比。</blockquote><p></p><p><img src="https://static001.geekbang.org/infoq/63/637cec52a4c63cf031711a01461978d0.png" /></p><p></p><p></p><blockquote>RG Agent则是GSE非参化知识与参数化知识的重要处理部分，通过Query与上下文理解，智能动态化调度私域知识数据、垂类搜索引擎以及LLM生成信息，结合Sparse &amp; Dense 双路检索方式，更好地应对复杂多样的选车查询需求。</blockquote><p></p><p></p><p></p><blockquote>Re-Ranking则是对返回的信息根据整体相关性、时效性、多样性、个性化等维度，对候选进行重排与截断，为下游CR模块提供高质量信息。</blockquote><p></p><p></p><p></p><blockquote>CR Agent对信息进行准确高效的压缩提炼，由于CR位于链路输出端，直接影响生成质量，因此存在许多优化技巧，例如Chunk的自适应调整、迭代式自主精炼Self-Refine、基于CoV（Chain of Verification）验证链的事实性自主判断等，在此不一一展开。</blockquote><p></p><p></p><p></p><blockquote>GSE支持多轮交互，设计了Memory Agent，对会话历史进行压缩和调度管理。除此之外，GSE的Citation模块能够为输出结果提供自动化的引文生成。GSE还具有规划工具使用的能力，例如，如果你询问它今天限号情况，它会根据你所处的省市政策，给出最近一周的限号情况。目前各种实用能力还在进一步扩展中。</blockquote><p></p><p></p><p>通过Agent化的方式构建了大模型驱动的生成式搜索引擎GSE，并对其进行了整体评估，在选车场景中传统引擎难以有效应对的长尾问题以及复杂选车问题上，效果独立评估与相对评估（相比传统搜索）上GSE都达到了非常不错的水平，选车体验焕然一新，更加出色。同时GSE在领域幻觉问题上的准确率也得到较大的提升，相比GPT4准确率高+8.0pp。当然，这只是其中一个维度的效果考量，GPT4作为当前最顶尖的大模型，其超高的综合智能水平一直是GSE基础大模型学习追逐的目标,模型全面维度的提升还有非常多的工作要做。关于幻觉的Case示例Query=“智己s7怎么样”，智己s7并不存在，近似的有智己LS7/智界s7，用户搜索经常发生混淆。下图从左至右分别是业界某款AI助手、GSE以及GPT4的回复结果。可以发现，即便是搜索插件接入模式的GPT4也发生了幻觉，而GSE较好的指出该车系不存在的事实，同时介绍了可能满足Query意图最相近的车。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a00bee40f01c973966d0e9c0844b5419.png" /></p><p></p><h3>3.3. 存在的不足与限制</h3><p></p><p>在GSE系统设计中，虽然引入了多种鲁棒性与可靠性的设计，但实践中也发现，系统仍然会存在一些问题与限制。例如推荐偏差问题，类似“哪些车比较好看”较为宽泛的选车问题，在没有更多条件约束的情况下，好看是因人而异的，这可能导致GSE的结果对部分用户来讲是有偏差的；还有与搜索结果存在差异的问题，单篇的文档可能对某种类型的车具有强烈的情感偏好，而GSE强调则综合性的观点，这可能导致矛盾信息的出现；还有幻觉问题，大模型的自回归（AR, AutoRegressive）生成模式导致幻觉问题能被缓解，但幻觉生成的概率性总会存在，接下来需要对齐训练中做进一步的优化；另外当前的交互功能还比较简单，存在大量的改进升级空间。</p><p></p><h2>4. 生成式选车的未来与展望</h2><p></p><p>基于构建的先进领域大模型，为其插上搜索的翅膀，打造了革新性的生成式搜索引擎GSE，超越传统搜索与大模型，为用户提供更加高效流畅、专业有趣的搜索选车体验，开辟了新的服务路径，大模型驱动的新范式选车引擎具有很大的发展空间和潜力。产品功能方面，GSE可以提供全新的选车体验，其强大的智能化特性可以实现信息的一站式全场景流转，用户无需再在APP之间频繁地跳转，实现聚集化的流量入口；商业模式方面，基于GSE交互式的搜索形态可以进行更加原生化的广告营销，实现更短的转化路径，创造新颖高效的商业营销模式。基于大模型的生成式搜索，将会是未来业界发展的新兴方向，成为连接人类与信息世界的新桥梁。</p><p></p><p>关于我们我们是汽车之家商业智能团队，主要涉及AI选车、搜索、广告程序化投放、生成式AI等方向业务。真诚欢迎有DL背景、技术扎实的感兴趣小伙伴加入我们（Base 北京），一起打造汽车领域的先进技术引擎。投递简历邮箱： <a href="mailto:linxiyao@autohome.com.cn">linxiyao@autohome.com.cn</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/993d0da9b078aa913e991e24d</id>
            <title>基于Sermant的全链路灰度发布在汽车行业DMS系统的应用</title>
            <link>https://www.infoq.cn/article/993d0da9b078aa913e991e24d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/993d0da9b078aa913e991e24d</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Apr 2024 02:55:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 汽车行业, DMS系统, 全链路灰度发布, Sermant
<br>
<br>
总结: 本文介绍了在汽车行业智能升级的背景下，如何利用Sermant技术为DMS系统提供全链路灰度发布方案，以适应更多灵活多变的业务场景。通过全链路灰度发布方案，可以解决DMS系统在实际应用中的痛点场景，提高系统的灵活性和稳定性。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区《<a href="https://bbs.huaweicloud.com/blogs/424899?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">基于Sermant的全链路灰度发布在汽车行业DMS系统的应用</a>"》，作者：聂子雄 华为云高级软件工程师</p><p></p><h2>摘要</h2><p></p><p></p><p>随着汽车产业的智能升级，DMS系统作为汽车行业的经销管理系统也面临着更加多种多样的业务场景的挑战。借助Sermant，华为云能够为DMS系统提供一整套端到端全链路灰度发布方案，这套方案可以适应DMS系统应用中更多灵活多变的场景。</p><p></p><h2>一、背景</h2><p></p><p></p><p>汽车行业是一个庞大的产业，涵盖了设计、制造、销售、维修等多个方面。目前，全球汽车行业已经成为了重要的经济支柱之一，每年产生数万亿美元的产值。汽车行业的发展也推动了相关产业的发展，如石油、钢铁、橡胶、玻璃等原材料行业，以及银行、保险、物流等服务行业。随着环保意识的提高和新能源技术的不断发展，汽车行业也面临着转型升级的挑战。目前汽车行业正在向智能化、电动化和共享化方向发展，面对更加多种多样的需求，行业相关的IT系统也需要不断进步以适应这些场景。</p><p></p><p>在汽车行业IT系统中，经常会提到DMS系统，那什么是DMS系统呢？DMS系统是指汽车经销商管理系统（Dealer Management System），它是一种专门为汽车经销商和售后服务提供商设计的软件系统，用于管理和优化他们的业务流程。DMS系统可以帮助汽车经销商实现业务数字化、自动化和智能化，提高业务效率、降低成本、提升服务质量，是现代化汽车经销商必备的管理工具。目前很多经销商的DMS系统基本上都是做过微服务化改造，里面不同微服务模块之间会相互调用，因此如何高效地使用和管理这些微服务模块是DMS系统的重要挑战之一。</p><p></p><h2>二、痛点场景</h2><p></p><p></p><p>目前DMS系统已经广泛应用在汽车经销商的业务当中，其中有一类是DMS系统在实际使用中的痛点场景，具体有：</p><p></p><p>客户想在某一个门店A上线自己的新业务，作为业务试点门店，比如新品汽车销售，或者打折促销活动等。和新业务相关的流量只会流入试点门店B。为了节约成本以及降低部署服务工作量，希望能够实现逻辑上的环境隔离。例如，测试环境有部分服务复用生产环境上的模块，开发测试人员只需要聚焦于需要测试的服务模块。客户的交易、商品服务有新的业务要上线，新上线的功能间有依赖和交互，要在上线前做一些测试工作：用户计划让测试人员专门账号来进行现网测试（类似于游戏等白名单控制的开服前验证）；用户引入少量比例的生产流量进行验证。</p><p></p><h2>三、解决方案</h2><p></p><p></p><h3>3.1 单点灰度发布方案是否可行？</h3><p></p><p></p><p>针对上述问题，一般的思路是通过灰度发布去解决，通过灰度发布，可以引入部分的测试流量到新业务模块，也能控制带有具体特征的流量只流入到对应的测试模块，其余流量保持原有方式不动。</p><p></p><p>但是经过仔细考虑，就会发现如果只做单点灰度发布，其实是无法完善地解决以上场景的痛点问题，主要体现在：</p><p></p><p>业务特征时常只在第一跳，也就是特征只在入口，传递过程中会丢失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a6f9fc4300b2174ed39417b857402c17.png" /></p><p></p><p>除了第一跳入口，后续微服务之间进行调用的时候也会把特征给丢失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/6979a8aaed510ff978fd1179e2d8a357.png" /></p><p></p><h3>3.2 全链路灰度方案是怎样的？</h3><p></p><p></p><p>因此，仅仅依靠单点灰度发布的能力是不够的，还需要能够做到整条微服务调用链的可灰度，也就是全链路灰度能力，这样就可以灵活解决DMS系统在这一类业务中遇到的问题。目前要实现全链路灰度，一般要考虑这些问题的处理：</p><p></p><p>1）在第一跳的地方（一般是网关），我们需要能选中各种类型的流量，把这部分流量染色，再路由到正确的目标。</p><p></p><p>2）除了第一跳，剩下调用链路中的各个微服务能够识别染色标，透传染色标，并路由到正确的目标。</p><p></p><p>3）能对异常情况进行妥善处理。</p><p></p><p>目前，华为云针对以上难点，设计出一套相对完善的全链路灰度发布方案，整体方案如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/aefca13a5ad20a8c5f725232665d4254.png" /></p><p></p><p>1）在前端部分，请求会统一携带流量标签参数发到华为云CSE应用网关上面。</p><p></p><p>2）CSE应用网关会选中各种类型的流量，将这些流量根据需求分别染色，比如通过请求header进行标记染色。</p><p></p><p>3）CSE应用网关会将染色后的流量转发到带有不同tag的后端微服务实例，tag可以由应用发布流水线注入到相应发布的微服务实例当中。</p><p></p><p>4）借助微服务实例上挂载的Sermant Agent，接收到应用网关流量的微服务实例会通过Sermant Agent提供的流量染色以及标签透传能力（适配Dubbo，SpringCloud）将流量特征保留并转发到合适的下一跳微服务实例。对于后续链路上的微服务实例，都可以通过微服务实例上面的Sermant Agent进行特征的传递。</p><p></p><h3>3.3 为什么要选择Sermant Agent？</h3><p></p><p></p><p>考虑到目前Dubbo和SpringCloud是国内使用量较多的微服务开发框架，为了不产生过多框架适配工作量，因此选用了基于Java Agent技术的Sermant Agent，利用这种无侵入式的特点，用户只需要在微服务实例启动时将Sermant Agent挂载到实例进程当中即可。Sermant Agent针对Dubbo和SpringCloud两种主流微服务框架适配了流量染色以及标签透传的能力。通过流量染色和标签透传，Sermant Agent就可以实现流量特征的保留以及传递到下一跳。Sermant Agent进行流量染色以及标签透传的全流程如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/7578a771e9304a3d697e9c192054b752.png" /></p><p></p><p>通过配置中心，用户可以在控制台界面下发服务粒度的染色规则，用于判断入方向的流量特征是否符合特定条件，若匹配则对该流量进行染色，在同一调用链的出方向流量中携带该染色标签然后通过请求透传给下一跳，因此用户可以通过下发自定义的染色规则到对应微服务，允许目的特征流量进入微服务实例并为出流量赋予新的特征。</p><p></p><p>在全链路灰度发布的场景下，Sermant Agent染色规则相对简单，sermant Agent会允许目的特征的流量进入微服务实例然后染色时带上相同的特征，染色后的流量再流入到对应tag的微服务实例。</p><p></p><h2>四、总结</h2><p></p><p></p><p>基于Sermant的全链路灰度发布方案可以解决DMS系统目前在一些如新业务在试点门店测试上线等业务场景遇到的困难，并且这一套方案能适应各类敏捷迭代的业务场景。</p><p></p><p>在开发测试过程中，客户可以根据需求在逻辑上划分出一套属于自己的服务链路，只需要关注自己设定的特征流量即可，这种模式可以为客户省去搭建DMS系统中一些共用的模块时间以及节约环境资源，并且还可以很方便地将带有试点特征的流量引入到含有自己试点应用的链路环境当中。</p><p></p><p>在发布过程中，客户还可以根据需要把一部分生产流量引入到自己的新版本业务链路环境当中，完成新版本的验证。</p><p></p><p>----------------------------------------------------------------------------------</p><p></p><p>Sermant作为专注于服务治理领域的字节码增强框架，致力于提供高性能、可扩展、易接入、功能丰富的服务治理体验，并会在每个版本中做好性能、功能、体验的看护，广泛欢迎大家的加入。</p><p></p><p>Sermant&nbsp;官网：https://sermant.ioGitHub&nbsp;仓库地址：<a href="https://github.com/huaweicloud/Sermant">https://github.com/huaweicloud/Sermant</a>"</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/whmTRumNeD6js22SjfSv</id>
            <title>大模型时代，AI 和数据库技术会碰撞出什么新火花？</title>
            <link>https://www.infoq.cn/article/whmTRumNeD6js22SjfSv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/whmTRumNeD6js22SjfSv</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Apr 2024 10:34:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PaaS, 大模型, 数据库技术, AI
<br>
<br>
总结: 数据库作为关键 PaaS 能力，一直顺应业务和技术发展，不停变更和创新。大模型的兴起对数据库技术有深刻的影响，包括通过大模型赋能的 SQL 改写，智能诊断，问答等一系列能力。数据库技术在大模型时代有了更大的想象空间，与 AI 技术相互成就，为数据库行业带来了新的挑战和机遇。 </div>
                        <hr>
                    
                    <p>数据库作为关键 PaaS 能力，一直顺应业务和技术发展，不停变更和创新。大模型的兴起，对数据库也有深刻的影响，包括通过大模型赋能的 SQL 改写，智能诊断，问答等一系列能力。同时数据库承担基础数据管理的作用，数据库技术如向量数据库，对解决大模型一些幻觉等也起到关键的作用。大模型和数据库技术相互成就，数据库技术在过去的一年内取得非常大的创新和发展，介绍这块关键技术的发展，相信可以帮助到感兴趣的从业人士以及观众朋友。</p><p></p><p>InfoQ 正在筹备 2024 年 6 月 14-15 日深圳举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 全球架构师峰会</a>"，我们邀请到了百度数据库产品总架构师朱洁老师来分享数据库和 AI 的发展史以及最新现状，以及百度在数据库和 AI 结合上的关键进展以及实际案例。在会议召开前，我们采访了朱洁老师，帮助读者提前了解到相关的技术落地细节。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/02/020723f444bf9c60f94d377e740fd9da.jpeg" /></p><p></p><p>InfoQ：您如何看待大模型时代对数据库技术创新的挑战和机遇？对数据库技术的影响主要体现在哪些方面？</p><p></p><p>朱洁： 大模型对数据库技术带来的机遇是远大于挑战的。传统数据库技术或者大数据技术处理的还是以结构化数据为主，或者要把非结构化数据先预处理成结构化数据才好处理。但事实上，非结构化数据才是最自然，体量最大的数据。大模型技术让非结构化数据有新的处理方法，放大非结构数据的价值，因此也带来了底层数据库更多的发挥空间。</p><p></p><p>大模型对数据库的影响主要可以从两个方面看，一个是 DB4AI，主要是向量数据库技术。通过向量数据库解决大模型知识更新不及时，幻觉问题，以及缺乏企业内部知识，也无法进行细粒度安全管理等问题。另一个是 AI4DB，通用大模型技术对数据库本身进行优化，主要方向包括数据库自动运维，SQL 生成，SQL 优化，智能问答等等。</p><p></p><p>总的来说，数据库和大模型肯定是相辅相成，数据库技术在大模型时代有了更大的想象空间。</p><p></p><p>InfoQ：在大模型的兴起背景下，数据库技术如何适应和应对新的数据处理需求？</p><p></p><p>朱洁： 大模型技术让非结构化数据有了新的应用空间，数据库变化比较大的是对文本等非结构化数据处理，以及未来甚至图片，视频等多模的数据。</p><p></p><p>目前这块发展很快，主要包括文本拆分，怎么 Embedding 多模数据，怎么实现多路召回，以及向量引擎通过 GPU ，以及更好数据组织模式来实现高性价比等。</p><p></p><p>InfoQ：在数据库行业的技术发展中，大模型技术与人工智能的相互作用是如何发挥作用的？</p><p></p><p>朱洁： 大模型属于人工智能领域的一个重要组成部分。现在一般说大模型是指生成式 AI 技术（GenAI）。</p><p></p><p>在数据库领域一直使用 AI 技术，传统上主要是一些预测算法，分类算法这些，解决比如运维的问题。大模型诞生之后，在代码生成，知识处理方面相比传统 AI 技术有了革命性提升，但是在一些传统系统运维，调优方面还是传统算法实现更简单，效率更高。</p><p></p><p>因此大模型技术更多的是在原来的技术的基础上更深入的解决原来不好解决的代码生成，知识管理等。对原有技术是一个非常大的补充，提升和扩展。</p><p></p><p>InfoQ：您能分享一些数据库和人工智能领域的创新案例，特别是在大模型时代的实际应用？</p><p></p><p>朱洁： 这块的技术发展很快，各个厂家都在进行相应的尝试和布局。当前比较创新的数据库和大模型结合的案例和应用有：</p><p></p><p>智能问答：通过大模型技术解决数据库运维问题，DBA 值班等。代码生成 &amp; 翻译：自然语言到 SQL（NL2SQL），或者把一种 SQL 翻译成另外一种，典型的 Oracle 翻译成 MySQL 之类。SQL 优化：改写、注释、纠错、解释，补全智能问数：自然语言询问，内部通过大模型转成 SQL，查询出结果，然后大模型再总结以报表，报告形式展示出来。</p><p></p><p>InfoQ：作为百度智能云数据库产品总架构师，您如何看待数据库和人工智能的结合，在实际业务中取得的关键进展？</p><p></p><p>朱洁： 首先我们的大模型在业界有领先优势，其次是百度智能云在这方面布局很早，也很全。因此我们目前在这块很有竞争力。我们介绍几个最新的关键的成果：</p><p></p><p>发布了 VectorDB 1.0，向量数据库是企业不可或缺的知识库核心组件，它针对传统知识库问答系统遇到的性能瓶颈、维护挑战及规模限制等问题提供了有力解决方案。全新发布的百度向量数据库 VectorDB 1.0，不仅集成了全面的运维控制和安全防护能力，还兼容了千帆、LangChain 等主流生态系统，能够帮助企业轻松管理数以千万计的文档知识，最大支持百亿向量存储规模以及毫秒级的向量检索速度。同时，相比同类型开源产品，VectorDB 1.0 性能最高提升 10 倍。发布了数据库智能驾驶舱（Database Smart Cockpit，DBSC），这个服务是利用大模型技术解决数据库运维，安全，智能问答的能力。通过内置的百度文心大模型能力，再加上百度积累的数据库运维知识库等，帮助用户回答云原生数据库 GaiaDB、MySQL、Redis 等数据库产品的各种使用场景复杂问题，以及显著降低异常问题定位时间，以及提升 SQL 优化效率等。目前这个服务在百度内部已经成功帮助 DBA 团队降低超过 50% 的运维工作量。</p><p></p><p>InfoQ：对于百度在数据库和人工智能结合上的关键进展，您认为最有意义的是什么？</p><p></p><p>朱洁： 对外部而言，我们致力于为客户提供强大、高性能、稳定可靠的数据库产品，首先通过大模型技术对数据库技术的改造和升级也能帮助客户降低数据库的使用门槛和成本。其次我们提供更好的解决方案和能力套件，帮助客户更快地开发基于大模型的 AI 原生应用，帮助客户更快地应用上 AI 能力，创造新的业务机会。</p><p></p><p>对内部而言，百度具备优秀的大模型技术和产品，同时也需要优秀的各类配套组件，共同支撑百度智能云构建强大的 AI 原生应用产品和解决方案。数据库技术的努力突破，才能更好的支撑好百度的大模型战略。</p><p></p><p>InfoQ：在 AI 与数据库的结合中，百度是否有一些特定的技术路径或策略？</p><p></p><p>朱洁：AI 技术和数据库技术都在快速发展过程中，我们几个主要的策略有：</p><p></p><p>坚持技术和场景结合的原则：技术只有和场景深度结合，才容易成熟，以及真正解决问题。因此我们对大模型的应用并不追求尝鲜，一定是选择可以和场景深度结合，各方面条件成熟，以及内部深度使用之后再给到我们的客户。坚持技术的普惠的原则：普惠核心意味着要让更多用户，更多场景可以使用。对数据库技术来说主要体现在两点：首先我们在设计的时候就会基于通用硬件去设计，云上，云下都可以部署，大客户，小客户都能用。这个体现在我们的 VectorDB，DBSC，GaiaDB 等多个产品中。另外一个核心是坚持起步门槛低，为了让更多用户用到，我们 VectorDB，DBSC 目前都提供了免费版本，让用户可以直接使用。也欢迎大家到百度智能云上选择相应的免费版本，体验最新大模型加持的能力。坚持开放的原则：除了我们自研之外，我们也非常欢迎更多的第三方厂商和我们一起共建，或者集成我们的产品。</p><p></p><p>InfoQ：您对 AIGC 与数据库结合的未来发展有何展望？</p><p></p><p>朱洁： 这个领域还在一个刚起步和快速发展阶段。我觉得下一个阶段的发展，核心有两点：</p><p></p><p>已有产品的成熟，随着技术发展，使用者越多，会更催熟当前的产品，更深度的解决客户实际场景问题。多模态支持：当前技术处理文本为主，未来多模态的能力会越来越强，也会在这个基础上诞生更多的应用。</p><p></p><p>InfoQ：在演讲中提到的数据库技术的创新和发展对从业人员和观众朋友意味着什么？ArchSummit 听众能从您的这次演讲中可以获得哪些实际的收益和启发？</p><p></p><p>朱洁： 数据库作为关键 PaaS 能力，一直顺应业务和技术发展，不停变更和创新。大模型的兴起，对数据库也有深刻的影响，包括通过大模型赋能的 SQL 改写，智能诊断，问答等一系列能力。同时数据库承担基础数据管理的作用，数据库技术如向量数据库，对解决大模型一些幻觉等也起到关键的作用。大模型和数据库技术相互成就，数据库技术在过去的一年内取得非常大的创新和发展，介绍这块关键技术的发展，相信可以帮助到感兴趣的从业人士以及观众朋友。</p><p></p><p>【活动推荐】</p><p>在2024年6月14-15日<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">深圳 ArchSummit 架构师峰会</a>"上，我们邀请了 CNCF、顺丰集团、腾讯、百度等企业的专家来演讲。会议上还设置了大模型、架构升级等专题，如果你感兴趣来会议上听演讲，欢迎进入 ArchSummit 会议官网，查看讲师们的详细演讲提纲。</p><p></p><p>会议现已进入 8 折早鸟购票阶段，可以联系票务经理 17310043226 , 锁定最新优惠。扫描上方二维码添加大会福利官，免费领取定制福利礼包。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/925fe54bad55de91d3e9559f53a895e3.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/St6Erpa3eOIllXAdPgmm</id>
            <title>InfoQ 中国技术力量之【AIGC先锋榜单】正式启动征集，洞见AIGC产业未来</title>
            <link>https://www.infoq.cn/article/St6Erpa3eOIllXAdPgmm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/St6Erpa3eOIllXAdPgmm</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Apr 2024 09:34:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术型企业, 创新技术, AIGC赛道, 案例征集
<br>
<br>
总结: InfoQ致力于搭建技术型企业与用户的桥梁，推出中国技术力量榜单，现面向AIGC赛道启动案例征集，分为最佳实践案例和最佳技术服务商两个维度，时间截至4月26日。评选分为自主报名、专家评选和结果公布三个环节，鼓励企业积极申报，获奖企业将获得电子证书、AICon大会嘉宾票等奖励。 </div>
                        <hr>
                    
                    <p></p><blockquote>致力于搭建起连通技术型企业与最终用户的桥梁，让更多的创新技术被看见，继2020年正式推出中国技术力量年度榜单品牌之后，InfoQ 每年都会针对特定领域发起案例征集，遴选出那些对产业发展有巨大推动作用的企业和案例。</blockquote><p></p><p></p><p>如今，InfoQ 面向AIGC赛道正式启动<a href="https://www.infoq.cn/form/?id=2098">【中国技术力量 2024 之AIGC先锋榜】</a>"案例征集，以期深入技术变革，洞见AIGC的产业未来。本次案例征集共分为两个维度，分别是【AIGC最佳实践案例 TOP20】和 【AIGC最佳技术服务商 TOP30】，本次征集时间是即日起至4月26日23:59:59止，我们将于5.17日<a href="https://aicon.infoq.cn/2024/beijing/?utm_source=infoqweb&amp;utm_medium=dahuibanner">【AICon 全球人工智能开发与应用大会 暨 大模型应用生态展】</a>"大会现场公布结果，并邀请部分获奖企业来到现场展示并见证这一时刻。</p><p></p><p>本次榜单评选分为自主报名（4.1-4.26）、专家评选（4.26-5.8）、榜单结果公布（5.17日）三个环节，InfoQ将邀请行业专家共同参与案例评选，最终产生上榜名单。</p><p></p><p>其中，【AIGC最佳实践案例 TOP20】鼓励应用方企业积极申报，我们将从场景创新性、实践效果、行业价值等维度进行评分，最终获奖企业将获得电子证书、少量AICon大会现场嘉宾票（需主办方审核参会人员）以及相关宣传报道，极优秀企业将获得AICon大会现场演示的机会，由主办方提供场地向现场数千位参会者介绍应用案例。</p><p><img src="https://static001.infoq.cn/resource/image/d0/12/d0c663a9eee234658d540c2124753312.jpg" /></p><p>【AIGC最佳技术服务商 TOP30】鼓励技术服务厂商、解决方案提供厂商积极申报，我们将从技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多个维度进行评分，最终获奖企业同样将获得电子证书、少量AICon大会现场嘉宾票（需主办方审核参会人员）以及相关宣传报道。</p><p></p><p>感兴趣的企业可以<a href="https://www.infoq.cn/form/?id=2098">点击此处</a>"，填写表单进行申报，如有问题，欢迎与表单中的工作人员进行联系。</p><p></p><p>InfoQ 中国技术力量往期榜单：</p><p><a href="https://www.infoq.cn/zones/chinatechawards/2020/">https://www.infoq.cn/zones/chinatechawards/2020/</a>"</p><p><a href="https://www.infoq.cn/zones/chinatechawards2022/">https://www.infoq.cn/zones/chinatechawards2022/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c4lQImZZfxmgVouQilQc</id>
            <title>谷歌、阿里、字节、科大讯飞、月之暗面、智谱等大模型落地实践案例集结完毕｜AICon 北京 2024 议程上线</title>
            <link>https://www.infoq.cn/article/c4lQImZZfxmgVouQilQc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c4lQImZZfxmgVouQilQc</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Apr 2024 09:03:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, GPT, 大模型落地, AICon
<br>
<br>
总结: 上周，OpenAI宣布将和开发者合作测试GPT基于使用情况的收入，这一模式被认为是大模型落地的有效路径。目前，GPT在不同场景中已有不错的案例，同时AICon大会将在北京举办，汇聚了众多互联网大厂和AI企业，探讨Agent、多模态、RAG、Copilot等应用构建在不同场景下的实践心得。 </div>
                        <hr>
                    
                    <p>上周，OpenAI&nbsp;通过官方社交媒体账号宣布：给开发者分钱！该公司表示，将要和一些开发者合作，测试&nbsp;GPT&nbsp;基于使用情况的收入。今年&nbsp;1&nbsp;月，&nbsp;GPT&nbsp;Store&nbsp;正式上线时，Agent瞬间就火爆起来，大家都在关注这一模式的前景，这一模式也多次被认为是大模型落地的有效路径。</p><p></p><p>时至今日，这一模式在不同场景中已经有了一些还不错的案例。除此之外，Sora点燃的多模态、从去年就爆火的RAG、已经开始落地的Copilot&nbsp;应用构建和大模型技术在金融、教育、法律、企业办公等多种场景下均取得了不错的尝试。</p><p></p><p>秉承着“实践驱动”的原则，AICon全球人工智能开发与应用大会暨大模型应用生态展将于5月17日至18日在北京乐多港万豪酒店正式举办，本届大会汇聚了来自阿里巴巴、华为、字节跳动、小米、网易、谷歌、腾讯等互联网大厂，以及月之暗面、科大讯飞、智谱等头部AI企业和OPPO、vivo、喜马拉雅等众多应用厂商共同探讨Agent、多模态、RAG、Copilot&nbsp;应用构建、智能办公、出海等众多场景下的实践心得。</p><p></p><p>目前，AICon大会议程已经上线，并将持续更新，感兴趣的同学请锁定大会官网（<a href="https://aicon.infoq.cn/2024/beijing/schedule">https://aicon.infoq.cn/2024/beijing/schedule</a>"）。</p><p></p><p>与此同时，本届大会现场将公布【中国技术力量&nbsp;2024之AIGC先锋榜】，目前榜单正在火热征集中，感兴趣的企业辛苦点击<a href="https://www.infoq.cn/form/?id=2098">链接</a>"填写表单。</p><p><img src="https://static001.infoq.cn/resource/image/93/b0/938b08e43289ee4d286051580b2f65b0.png" /></p><p>目前，AICon大会已经进入9折购票倒计时，感兴趣的同学可以在官网购票或者扫描议程页面的二维码咨询小助手购票。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rj0BO1QX6ZqYwqHy9UOT</id>
            <title>浪潮海若大模型业务战略正式发布，助力新质生产力发展</title>
            <link>https://www.infoq.cn/article/rj0BO1QX6ZqYwqHy9UOT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rj0BO1QX6ZqYwqHy9UOT</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Apr 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: 中共山东省委网络安全和信息化委员会办公室, 青岛市大数据局, 浪潮云, 大模型业务战略
<br>
<br>
总结: 4月2日在青岛市举办的“云端眺望 向‘数’而生”数字创新生态大会聚焦于服务新质生产力，正式发布海若大模型业务战略，通过数字生态创新共同体，联合生态伙伴合力拥抱大模型浪潮，培育行业新业态。 </div>
                        <hr>
                    
                    <p>4月2日，由中共山东省委网络安全和信息化委员会办公室指导、青岛市大数据局支持、浪潮云主办的“云端眺望 向‘数’而生”数字创新生态大会在青岛市成功举办。会议聚焦服务新质生产力，正式发布海若大模型业务战略，通过数字生态创新共同体，联合生态伙伴合力拥抱大模型浪潮，培育行业新业态。</p><p></p><p>向新而行 新质生产力加速起势</p><p></p><p>伴随新一轮科技变革和产业变革深入发展，一大批新兴技术和产业如潮涌现，全球迎来了新一轮的发展机遇期。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e71a8f14cef71c17040d8e4e89876641.jpeg" /></p><p>赵树岭致辞</p><p></p><p>山东省委网信办副主任赵树岭在致辞中表示，大模型已经成为全球科技竞争的新高地、未来产业的新赛道、经济发展的新引擎。当前，国内通用大模型、行业大模型、端侧大模型如雨后春笋般涌现，大模型产业的应用落地将进一步提速。浪潮积极探索数据要素价值释放路径，发挥数据与行业的融合优势，筑基新质生产力，浪潮海若大模型作为省内首批备案通过的模型产品，在守牢安全底线的前提下，助力增强产业赋智创新能力。</p><p><img src="https://static001.geekbang.org/infoq/29/298c89c353c9d6773bccd84c9e00d8cf.jpeg" /></p><p>肖雪致辞</p><p></p><p>浪潮集团执行总裁、总工程师，浪潮云董事长肖雪表示，新质生产力起点是“新”，关键在“质”，落脚于“生产力”。大模型的问世，让我们有新的抓手，去培育新质生产力。浪潮云以扎实的算力服务能力、丰富的行业服务经验和开放的生态体系建设，打造“云网边端”分布式体系，支撑海若大模型分布式结构；打造“云数智”新产品，提供软硬一体的行业大模型+行业智能体，推动数实融合；提供“建管运”全栈本地大模型落地服务，持续安全释放数据价值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/0087bbb11c16c04bdef5b6f5fbe98f3a.jpeg" /></p><p>鲍建欣致辞</p><p></p><p>青岛市大数据发展管理局总工程师鲍建欣在致辞中表示，世界正进入数字经济快速发展的时期，新技术、新业态、新平台蓬勃兴起，已经成为数字经济产业发展的重要驱动力量。青岛市积极响应国家大数据战略，加快推进大数据产业发展，助力经济高质量发展。浪潮云作为山东省内领先的云计算企业，通过不断探索和创新，助力数字经济发展。未来，希望浪潮云继续发挥自身技术优势和资源禀赋，加强与生态伙伴的交流合作，为行业的发展做出更大贡献。</p><p></p><p>向数而生 拥抱大模型浪潮</p><p></p><p>当前，以大模型为代表的第三次浪潮正在掀起。浪潮云总经理颜亮在《拥抱大模型浪潮，助力新质生产力》主旨演讲中表示，浪潮海若大模型定位行业大模型，首批面向政府、交通、应急、制造、医疗、农业六大行业，具备可信赖、易落地、可持续三大核心优势。依托海若完整产品矩阵，浪潮云能够帮助用户打造行业专属大模型，并基于在全国布局的分布式算力平台，实现带算力入场，对于政务云覆盖用户，只需1天时间即可实现海若快速交付，其他行业最长不超过1个月。</p><p></p><p>此外，结合20年行业经验，浪潮云持续打造和完善行业智能体商店，每个行业智能体商店将覆盖200余个典型应用场景，为行业用户提供更加精准、更加智能的支持和服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aaf32cbbb54281d591181f92c0d23023.jpeg" /></p><p>颜亮作主旨分享</p><p></p><p>会上，海若大模型业务战略正式发布，浪潮云将投入50亿资金，依托数字创新生态共同体，加快推动海若大模型在100个城市的快速落地，持续激活数据要素价值，助力新质生产力发展。</p><p></p><p>在随后的采访中，颜亮强调了数字化转型战略的重要性，特别是通过构建生态共同体来推动生态策略的落地。他指出，从算力、行业数据和智能体运用三个维度构建新的大模型生态体系。在算力方面，他强调了与生态合作伙伴合作建设算力平台的重要性。而行业数据方面，他着重于行业领域的开发商和设计机构，通过共同体汇集行业数据来形成基础能力。</p><p></p><p>在智能体运用方面，他提到了结合行业模型能力开发具象应用的重要性，实现行业大模型的输出物。另外，颜亮也谈到了数字化转型在不同区域的发展情况，以及数字化转型过程中的同质化和裂变现象。他还强调了海若大模型的三个基本能力：可信赖、易落地、可持续，并提出了解决这些能力的关键点，包括本地部署、产品矩阵完整性、快速推动行业侧价值体现等。最后，他强调了用户体验的重要性，并指出了浪潮云已具备的专家团队支持和云体系保障。通过这些观点，颜亮展示了数字化转型和大模型生态建设的重要性，以及解决相关挑战的方法和策略。</p><p></p><p>同时，颜亮强调了数据基础设施的升级是全社会共同的任务，需要合作伙伴共同推进，特别是在算力方面的合作伙伴的补充至关重要。他还强调了与高校等合作伙伴在行业数据方面的合作，并建议建立实验室来培训行业模型技术。此外，他讨论了智能体开发商在快速植入大模型能力方面的重要性，并强调了大模型作为催化剂的作用，以促进数据要素的使用和交互。最后，他简要提到了与青岛市的合作项目，旨在为青岛提供大模型服务、数据服务和生态服务，以推动数字经济的发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29bc0e26b616bdc38d07a55a897c3461.jpeg" /></p><p>孙思清作主旨分享</p><p></p><p>战略的落地，离不开创新技术的持续赋能。浪潮云首席技术官孙思清表示，浪潮海若大模型通过采用MoE架构、全面优化RAG、全链路安全防护等举措实现关键技术创新，解决了行业用户在大模型落地过程中所关注的运行成本、准确性、安全性等问题，并总结提炼出了海若行业落地“三步法”，通过打造三位一体的数字创新共同体，构建共建共赢、创新运营模式，联合伙伴、生态持续提升一体化方案、产品及交付能力，加快产业升级、业态创新。</p><p></p><p>在随后的媒体采访环节，孙思清分享了浪潮云在分布式云领域的独特定位和服务理念。他指出，浪潮云既不是传统的集中式训练模型，也不是私有化部署，而是通过分布式架构为客户提供服务。这种分布式模型具有多重优势：一方面，通过本地部署，结合业务需求，实现客户的定制化服务；另一方面，利用中心节点集中处理技术和预训练，再与本地服务结合，确保服务的标准化和个性化。从预训练到模型更新，浪潮的分布式架构不仅能保障服务的快速响应，还能兼顾客户本地数据的安全和定制化需求。</p><p></p><p>此外，孙思清强调了分布式架构对于保障客户数据安全和提升服务质量的重要性。他认为，分布式模型既能满足客户的定制化需求，又能有效管理和控制模型的整个生命周期，包括模型的训练、部署和更新。浪潮云通过多元算力的适配和集中式的管控，保证了服务的稳定性和安全性，从而为客户提供持续且高效的大模型服务。最终，孙思清展望了浪潮与青岛政府的合作，并强调了大模型的应用将推动整个数据产业的发展，为产业链上下游的企业提供高效、定制化的服务，实现产业升级和智能化发展的目标。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4bde9fb3f6edf46c99c6f1147d666a90.jpeg" /></p><p>浪潮云数字生态创新共同体正式成立</p><p></p><p>向实而为 千行百业迸发新活力</p><p></p><p>大模型作为驱动新质生产力的重要底座，必将与行业深度融合。浪潮云向实而为，赋能行业典型应用场景，为行业用户带来“质”的飞跃。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a48f044b6a18174e4a50a235aca5a76d.jpeg" /></p><p>海若大模型互动体验区</p><p></p><p>面向政府行业，浪潮云积极探索政务大模型在公文写作、基层减负等场景的落地，实现基层材料整理效率提升100倍，回复准确率提升至75%；面向医疗行业，浪潮云依托海若医疗大模型，在帮助医生生成电子病历过程中，实现一份入院记录生成时间仅需15秒，内容准确率90%以上，降低工作量超过70%；面向制造行业，在促进新产品研发方面，借助海若制造大模型，实现靶点识别准确率提高至95%，每周分析专利数量提高至3000篇。</p><p></p><p>理想之舟已然扬帆，逐浪前行正当其时。随着“数据要素×”计划的深入推进，大模型作为新质生产力的技术底座，必将加快产业升级、业态创新。未来，浪潮云将继续携手生态伙伴，深挖数据要素价值，推进海若大模型在各行业的快速落地，为新质生产力的发展添砖加瓦，为数字中国、数字经济、数字社会的建设贡献力量。</p><p></p><p>【活动推荐】</p><p></p><p>在 2024 年 6 月 14-15 日<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">深圳 ArchSummit 架构师峰会</a>"上，我们邀请了 CNCF、顺丰集团、腾讯、百度等企业的专家来演讲。会议上还设置了大模型、架构升级等专题，如果你感兴趣来会议上听演讲，欢迎进入 ArchSummit 会议官网，查看讲师们的详细演讲提纲。</p><p></p><p>会议现已进入 8 折早鸟购票阶段，可以联系票务经理 17310043226 , 锁定最新优惠。扫描上方二维码添加大会福利官，免费领取定制福利礼包。</p><p><img src="https://static001.infoq.cn/resource/image/a7/d3/a7169c8f216f8af139e2f6886de5b8d3.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xV92amptaCkHfu6F6qb0</id>
            <title>ChatGPT免注册让官网挂了？沃顿教授：OpenAI 做了错误的决定</title>
            <link>https://www.infoq.cn/article/xV92amptaCkHfu6F6qb0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xV92amptaCkHfu6F6qb0</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Apr 2024 06:42:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 免注册服务, 内容限制, 训练数据
<br>
<br>
总结: ChatGPT推出免注册服务，用户无需登录即可与ChatGPT对话，但功能受限，无法保存或分享聊天内容、使用自定义指令等。用户聊天内容仍会被用于训练数据，但设有更严格的内容政策。部分用户遇到官网访问问题，同时有人质疑OpenAI决策，认为GPT-3.5不如GPT-4。同时，人工智能市场可能存在供大于求的情况。 </div>
                        <hr>
                    
                    <p>从今天起，ChatGPT免注册服务将在部分市场先期开放，随后逐步推广至世界其他地区。也就是说，访问chat.openai.com将不再要求用户登录——但如果愿意，也可正常登录。大家不仅可以直接与ChatGPT对话，使用的也是与登录用户相同的基础模型。</p><p>&nbsp;</p><p></p><h2>用你数据训练、有更严格的内容限制</h2><p></p><p>&nbsp;</p><p>大家可以随心所欲加入聊天，但请注意，免注册所使用的功能集与注册账户有所区别：免注册用户的用户将无法保存或分享聊天内容、不能使用自定义指令，或者其他与持久账户相关的功能选项。</p><p>&nbsp;</p><p>另外，除非明确选择关闭，否则免注册用户的所有聊天内容仍会被用作训练数据。不只是ChatGPT，DALL•E也是如此。</p><p>&nbsp;</p><p>也就是说，大家仍可选择不将自己的聊天记录用于训练，只需单击右下角的小问号，而后点击“设置”再禁用该选项即可。OpenAI还给出以下GIF动图供用户参考：</p><p><img src="https://static001.infoq.cn/resource/image/b2/41/b2fd50cc097719d158891aa20cae8641.gif" /></p><p></p><p>但有人对此表示怀疑，毕竟如果连训练都不参加，OpenAI为什么要白白开放ChatGPT？</p><p>&nbsp;</p><p>此外，OpenAI还特意强调了：我们不会使用 ChatGPT Team、ChatGPT Enterprise 和 API 平台等业务产品中的内容来训练我们的模型。</p><p>&nbsp;</p><p>更重要的是，这个免费的ChatGPT版本将设有“相对更严格的内容政策”。这是什么意思？该公司发言人给出了冗长、但基本上没什么实质性内容的答复：</p><p>&nbsp;</p><p></p><blockquote>免注册模式的体验将得益于模型中现已内置的安全缓解措施，例如拒绝生成有害内容。除这些现有缓解措施之外，我们还实施了专门设计的额外保护措施，以解决可能不适合免注册体验的其他内容形式。&nbsp;根据我们对GPT-3.5功能的理解以及已经完成的风险评估，我们已经审视了以不适当形式使用免注册服务的各种可能行为。</blockquote><p></p><p>&nbsp;</p><p>我们并不知道更具限制性的政策究竟是什么，不过随着大量非注册用户涌入该网站，我们可能很快就能找到答案。这位发言人指出，“我们意识到可能需要额外的迭代，也欢迎大家反馈。”可以预想，反馈不仅会有、而且会相当多！</p><p>&nbsp;</p><p>就这个问题，外媒还询问他们是否有计划来处理几乎必然出现的、前所未有的服务滥用与模型武器化企图。毕竟哪怕是亿万级别的富豪也养不起这么一个纯粹烧钱的平台。时至今日推理成本仍然高昂，即便是资源消耗没那么夸张的GPT-3.5模型也需要大量电力与服务器空间。而且免注册一开，人们肯定会用瞬间用流量将其吞没。</p><p>&nbsp;</p><p>对于这个问题，OpenAI同样给出了既啰嗦又没什么营养的回应：</p><p>&nbsp;</p><p></p><blockquote>我们也认真考虑了如何检测并阻止对免注册体验的滥用，负责检测、预防和响应滥用行为的团队参与了服务体验的整个设计与实施过程，并将继续为后续设计提供指导。</blockquote><p></p><p>&nbsp;</p><p>可以看到，这里还是缺少任何真正具体的内容。发言人可能也不清楚这项决定到底会引发怎样的结果，所以只能采取被动姿态坐等尘埃落定。</p><p>&nbsp;</p><p></p><h2>官网挂了？</h2><p></p><p>&nbsp;</p><p>目前还不清楚哪些地区或者群体将首先获得纯免费ChatGPT服务，但免注册信息的发布可能带来了更多的流量，已经有用户难以打开官网，并怀疑是不是宕机了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c63ea9d498693306ecd65058eabe069.png" /></p><p>&nbsp;</p><p>有打开ChatGPT的网友，也遇到了ChatGPT出错、给不出回应的情况：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f6dabd55947dbe662368d65f45ce00f.png" /></p><p></p><p>&nbsp;</p><p>也有人在推特上求助：你们还可以使用 ChatGPT 吗？不确定是我的网络问题还是网络故障。帖子下面大部分人也表示无法使用。</p><p>&nbsp;</p><p>据悉，目前ChatGPT&nbsp;每周超过 1 亿人使用，范围涵盖185 个国家/地区。随着更多人的涌入，这给OpenAI 底层设施带来了考验。有网友评价称：“Google 搜索流量进入 ChatGPT”</p><p>&nbsp;</p><p>但并不是所有人看好 OpenAI这一决策。</p><p>&nbsp;</p><p>沃顿商学院教授Ethan Mollick称，“我认为 OpenAI 犯了一个错误：让 3.5 无需登录即可使用。这可能会让人们更难看到 AI 能做什么，因为它远远落后于 GPT-4。”他强调，GPT-3.5 甚至不会进入前六名或前七名。</p><p>&nbsp;</p><p>“这正是我在与我交谈的许多人身上注意到的。他们放弃了大模型，因为他们只尝试过 GPT-3.5。我倾向于给他们一些 GPT-4-Turbo 和 Claude 3 Opus 的试用机会，他们留下的印象更加深刻。”Mira Hurley表示。</p><p>&nbsp;</p><p>但有网友指出，虽然人工智能系统的使用量有所增长，但采用的速度跟不上创新的速度。这意味着，人工智能市场可能存在供大于求的情况。</p><p>&nbsp;</p><p>Django联合创始人Simon Willison则有其他想法，“我想知道他们的预防抓取是如何工作的？我猜滥用这个免费 3.5 API 的诱惑将会非常大。”</p><p>&nbsp;</p><p>不过对于更多的人，相比免注册，人们似乎更关心什么时候可以免费用GPT-4、GPT-5什么时候发布等问题，毕竟有相当一部分人已经用了ChatGPT。</p><p>&nbsp;</p><p>“我想知道他们是否会让 GPT-4 成为免费版本。想必最终运行成本会更低，而且显然他们会有更好的模式来吸引付费客户。那么 3.5 被抛弃只是时间问题吗？即使还没有发布很久？”有用户提出疑问。</p><p>&nbsp;</p><p></p><h2>局外一“瓜”</h2><p></p><p>&nbsp;</p><p>与此同时，这家公司的“宫斗”瓜的也可以吃一波。</p><p>&nbsp;</p><p>路透社4月1日消息，根据向美国证券交易委员会提交的文件，OpenAI调整了其支持AI初创公司的风险投资基金的治理结构，目前备受瞩目的CEO Sam Altman已不再拥有或控制该基金。</p><p>&nbsp;</p><p>3月29日递交的文件中记录了这项调整，此前Altman拥有OpenAI创投基金，这种不寻常的结构也引发人们关注。虽然该基金的运营方式类似于企业内的风险投资部门，但基金本身却是由Altman从外部有限合伙人处筹集而来，由他本人制定投资决策。OpenAI表示尽管该基金所有权归Altman，但他并不享有基金的经济收益。</p><p>&nbsp;</p><p>外媒Axios于本周一首次报道了基金所有权变更的消息。OpenAI公司发言人在一份声明中指出，该基金最初的普通合伙人（GP）结构为临时设置，“此番变化也进一步澄清了这种情况”。</p><p>OpenAI创投基金目前掌握的1.75亿美元投资，主要来自微软等OpenAI合作伙伴，但OpenAI本身并非出资方。</p><p>&nbsp;</p><p>文件显示，该基金的控制权已经被移交给自2021年来担任该基金合伙人的Ian Hathaway。Altman将不再担任该基金的普通合伙人。</p><p>&nbsp;</p><p>OpenAI公司指出，Hathaway负责监督该基金的加速器计划，并主导了对Harvey、Cursor及Ambience Healthcare等公司的投资。</p><p>&nbsp;</p><p>Altman本人正是创业加速器Y Combinator的前任总裁，此前曾因在OpenAI之外建立庞大的投资规划体系（包括加密货币初创公司Worldcoin与核聚变公司Helion Energy）以及在中东的筹款活动而受到密切关注。</p><p>&nbsp;</p><p>OpenAI方面表示，去年11月在遭到公司“突袭”式罢免之后，Altman已经接受了后续独立调查，结论认为他在产品安全或OpenAI财务方面均无任何不当行为。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://techcrunch.com/2024/04/01/chatgpt-no-longer-requires-an-account-but-theres-a-catch/?guccounter=1">https://techcrunch.com/2024/04/01/chatgpt-no-longer-requires-an-account-but-theres-a-catch/?guccounter=1</a>"</p><p><a href="https://www.reuters.com/technology/openai-removes-sam-altmans-ownership-its-startup-fund-2024-04-01/">https://www.reuters.com/technology/openai-removes-sam-altmans-ownership-its-startup-fund-2024-04-01/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4E5VDRlzPm4q9ZUseXfq</id>
            <title>15 秒音频即可“复制”原声！但OpenAI 担心新语音模型被滥用而限制发行</title>
            <link>https://www.infoq.cn/article/4E5VDRlzPm4q9ZUseXfq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4E5VDRlzPm4q9ZUseXfq</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Apr 2024 01:52:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, Voice Engine, AI生成, 语音模型
<br>
<br>
总结: OpenAI展示了名为Voice Engine的语音生成模型，能够通过文本输入和音频样本生成逼真的自然语言语音，对多个行业产生重大影响。该技术已被部署在少数公司中，包括教育、健康和通信领域。Voice Engine可以模仿原始说话者的声音，但也存在造假风险，OpenAI已采取措施确保合作伙伴遵守使用政策。AI技术的发展需要加强对深度伪造的教育和技术检测。 </div>
                        <hr>
                    
                    <p></p><p>3&nbsp;月&nbsp;30&nbsp;日，OpenAI&nbsp;在官网首次展示了名为“&nbsp;Voice&nbsp;Engine&nbsp;”的语音生成模型，该模型能够通过文本输入和仅仅&nbsp;15&nbsp;秒的音频样本生成与原始说话者声音高度相似、充满情感且逼真的自然语言语音。据悉，OpenAI&nbsp;于&nbsp;2022&nbsp;年末首次开发出&nbsp;Voice&nbsp;Engine，并已将其应用于其文本转语音&nbsp;API&nbsp;以及&nbsp;ChatGPT&nbsp;语音和朗读功能中的预设语音。</p><p></p><p>这项技术的问世，显然将对那些经常录制自己语音的人产生重大影响，包括播客、配音艺术家、口语表演者、有声书和广告解说员、游戏玩家、流媒体主播、客户服务代理、销售人员等众多职业。</p><p></p><p>不过目前，这项技术仅在小范围内提供，拥有访问权限的公司包括：教育技术公司&nbsp;Age&nbsp;of&nbsp;Learning、视觉叙事平台&nbsp;HeyGen、一线健康软件制造商&nbsp;Dimagi、AI&nbsp;通信应用程序创建者&nbsp;Livox&nbsp;和卫生系统&nbsp;Lifespan。&nbsp;OpenAI&nbsp;在其博客文章中表示：“这些小规模部署有助于为我们的方法、保障措施提供信息，并思考语音引擎如何在各个行业中发挥作用。”</p><p></p><p>官网文章中，OpenAI&nbsp;也展示了&nbsp;Voice&nbsp;Engine&nbsp;的使用示例。首先，提供英文参考音频：</p><p></p><p></p><p>基于该示例，以下是三个&nbsp;AI&nbsp;生成的音频剪辑：</p><p></p><p></p><p></p><p></p><p></p><p></p><p>可以说，无论从音色，还是从语调、停顿等方面来看，OpenAI&nbsp;的产出结果都模仿得惟妙惟肖。</p><p></p><p>OpenAI&nbsp;目前使用该工具的开发合作伙伴之一，非营利性医疗系统&nbsp;Lifespan&nbsp;的诺曼·普林斯神经科学研究所（Norman&nbsp;Prince&nbsp;Neurosciences&nbsp;Institute）正在使用该技术帮助患者“恢复声音”：有一名患者因脑肿瘤失去了清晰说话的能力，该公司通过她早期在学校演讲的录音，“复制”出了她的声音。</p><p></p><p>这个模型还可以将其生成的音频翻译成不同的语言，使得它对音频业务公司很有用，比如&nbsp;Spotify&nbsp;Technology&nbsp;SA。Spotify&nbsp;已经在自己的试点项目中使用了这项技术来翻译&nbsp;Lex&nbsp;Fridman&nbsp;等热门主持人的播客。</p><p></p><p>AI&nbsp;文本到音频生成是生成式&nbsp;AI&nbsp;的一个领域，正在不断发展。目前大多数专注于生成器乐或自然声音，语音生成方面相对较少被接触，部分原因是人们对深度伪造风险的担忧。</p><p></p><p></p><h4>造假风险？</h4><p></p><p></p><p>与&nbsp;OpenAI&nbsp;之前在生成音频内容方面的努力不同，Voice&nbsp;Engine&nbsp;可以创建听起来很像本人的语音，并具有特定的节奏和语调。软件需要的是&nbsp;15&nbsp;秒录制的人说话的音频，以重现他们的声音。</p><p></p><p>在该工具的演示中，外媒听取了&nbsp;OpenAI&nbsp;首席执行官&nbsp;Sam&nbsp;Altman（萨姆·奥特曼）&nbsp;的一段视频，他简要解释了这项技术，声音听起来与他的实际演讲没有区别，但完全是人工智能生成的。</p><p></p><p>“如果你有正确的音频设置，Voice&nbsp;Engine&nbsp;基本上能生成与本人无异的声音，”OpenAI&nbsp;的产品负责人&nbsp;Jeff&nbsp;Harris（杰夫·哈里斯）说。“它的技术质量令人印象深刻。”不过，哈里斯最后补充说：“在真正准确地模仿人类说话的能力方面，显然存在很多安全问题。”</p><p></p><p>就在不久之前，已经有&nbsp;AI&nbsp;技术被用于伪造声音。今年&nbsp;1&nbsp;月，一个自称是&nbsp;Joe&nbsp;Biden（乔·拜登）总统的人打电话鼓励新罕布什尔州的人们不要在初选中投票，声音听起来十分逼真，这一事件在关键的大选之前也引发了人们对&nbsp;AI&nbsp;的担忧。</p><p></p><p>OpenAI&nbsp;的一位发言人表示，在收到政策制定者、行业专家、教育工作者和创意人员等利益相关者的反馈后，他们决定缩减发布规模。“我们认识到，产生类似于人们声音的言论具有严重的风险，这在选举年尤其重要，”&nbsp;OpenAI&nbsp;在一篇博客文章中写道。“我们正在与来自政府、媒体、娱乐、教育、公民社会等领域的美国和国际合作伙伴合作，以确保我们在建设过程中采纳他们的反馈。”</p><p></p><p>据悉，OpenAI&nbsp;要求合作伙伴同意遵守其使用政策，不使用该模型来冒充个人或组织，且需要获得原始说话者的“知情同意”，并向听众披露这些声音是&nbsp;AI&nbsp;生成的。OpenAI&nbsp;还在音频剪辑中添加了一个听不见的音频水印，使其能够区分一段音频是否是由&nbsp;Voice&nbsp;Engine&nbsp;创建的。</p><p></p><p>在文章的最后，OpenAI&nbsp;呼吁银行逐步取消语音认证，作为访问银行账户和敏感信息的安全措施。它还呼吁加强对&nbsp;AI&nbsp;深度伪造的教育，并更多地开发用于检测音频内容是真实的还是&nbsp;AI&nbsp;生成的技术，以应对更先进的&nbsp;AI&nbsp;技术带来的挑战。</p><p></p><p>参考链接：</p><p></p><p>https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices&nbsp;</p><p></p><p>https://www.theverge.com/2024/3/29/24115701/openai-voice-generation-ai-model</p><p></p><p>https://www.bloomberg.com/news/articles/2024-03-29/openai-previews-new-audio-tool-that-can-read-text-mimic-voices?srnd=technology-vp</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/929852fd5aed5b58eb131b72b</id>
            <title>假如AI圈有世纪大和解</title>
            <link>https://www.infoq.cn/article/929852fd5aed5b58eb131b72b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/929852fd5aed5b58eb131b72b</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Apr 2024 14:21:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 曹云金, 郭德纲, AI圈, 马斯克
<br>
<br>
总结: AI圈内存在着各种矛盾和争议，就像曹云金和郭德纲之间的关系一样。马斯克在AI圈内的矛盾尤为突出，与OpenAI之间的纠纷引发了广泛关注。这些矛盾如果能够和解，将对整个AI行业产生深远影响。 </div>
                        <hr>
                    
                    <p>前不久，曹云金给郭德纲直播刷火箭登上热搜。网友们除了各自站队之外，还有不少人表示，你们俩什么时候世纪大和解啊？</p><p><img src="https://static001.geekbang.org/infoq/0a/0a57c3f89bf04432110878ef0ea1fd8e.jpeg" /></p><p></p><p>说起和解，其实AI圈也有不少矛盾。有些是缠绵多年的新仇旧恨，有些是要对簿公堂的深仇大恨。如果这些矛盾都能世纪大和解，那么AI行业会发生什么？</p><p>大愚人节的，我们给大伙整个活：盘点一下AI圈有哪些最知名，最持久，最让人闹心的矛盾。这些矛盾想要和解，前提条件是什么？和解了之后又会怎样？</p><p>话不多说，咱们把脑洞打开。祝各位读者愚人节以及此后的每一天都快乐。</p><p>马斯克和OpenAI：没见过分久的合</p><p>要说和解，就要先说矛盾。而提起科技圈的矛盾制造机，就不能不提怼天怼地的“科技界灭霸”马斯克。如果说哪里有柯南，哪里就有案件，那么哪里有马斯克，哪里就有难以和解的矛盾。马斯克VS扎克伯格、马斯克VS比尔·盖茨、马斯克VS约翰尼·德普，每一条都够写一篇的，但毕竟这些矛盾都离AI有那么一点距离。</p><p><img src="https://static001.geekbang.org/infoq/90/90189b400191f0c20a742fce03fadb11.jpeg" /></p><p></p><p>要说马斯克在AI圈的矛盾，那就不得不提最近风头正盛的“状告OpenAI”事件。</p><p>在ChatGPT爆火之后，作为曾经创始人的马斯克就开始疯狂指责OpenAI违背发展初衷，贪图商业利益，最近更是直接宣布准备向OpenAI及其CEO奥特曼提起诉讼，要求OpenAI恢复算法开源，将AI技术提供给公众，同时要禁止奥特曼和微软利用OpenAI的技术成果谋求商业利益。当然了，马斯克也没忘要求OpenAI偿还自己当年的投资损失。</p><p>但是人家马斯克也是留了台阶的。他表示，如果OpenAI把公司名变为“CloseAI”，他就放弃起诉。这个诉求咱们中国吃瓜群众是很能理解的，类似于“你走了给我把云字留下”。</p><p>面对曾经创始人的步步紧逼，OpenAI也没闲着。他们公布了与马斯克的一系列邮件往来自证清白。主要意思包括：1.当年转向盈利，是你马斯克也同意的；2.马斯克退出OpenAI，是因为索要更多股权和董事会控制权，甚至要求OpenAI并入特斯拉，被董事会否决了；3.马斯克已经在2018年撤资，并且其投资额根本没有他本人说的那么多。</p><p><img src="https://static001.geekbang.org/infoq/25/2512e8bf0ba1ec1b6143dffffdf0bbae.jpeg" /></p><p></p><p>现在情况是，马斯克对OpenAI嘲讽力度拉满，奥特曼也公开说马斯克“是个混球”，OpenAI在公告中认为“这一切让人感到悲哀”，双方的矛盾在可见范围内是难以调和的。</p><p>而回望这个矛盾的发生与膨胀，必须承认原委没有那么复杂，各自动机也是比较清晰的。无论其中有多少是关于技术路线的争论，对强大AI能力的担忧，马斯克都确确实实展现了“不怕前任过得苦，就怕前任开路虎”。</p><p>毕竟他很早就退出了OpenAI管理层，此后几年时间里双方相安无事，甚至马斯克还经常把自己创立了OpenAI的事宣讲一番。反而是OpenAI得到微软投资，GPT项目大获成功之后矛盾一下就爆发了。从时间线上看，其中的利益要素远大于理念要素。毕竟OpenAI已经带来了巨大的商业价值，而其背靠微软的发展路线，也会给马斯克布局的xAI等新公司带来直接竞争压力。</p><p>那么，这个矛盾如果世纪大和解呢？</p><p>可能性只有两种。第一，是马斯克放弃了对OpenAI的敌视和怀疑，认为这件事也就那样，不重要了，不如把矛头对准下一场矛盾的制造上。而对OpenAI则不如干脆进行和解，以此来收割这件事的最后一波流量。</p><p>想要实现这种和解的前提，是OpenAI的流量退去了。这也意味着它的技术能力从爆发期来到了平缓期，不再有引领AI技术发展的行业地位。</p><p>如果世界线向着这个方向发展，意味着有公司接过了AI大旗，或者AI技术本身陷入了瓶颈。从目前情况看，其他公司顶替OpenAI的可能性太小了，AI陷入低潮可能性更大。这也就是说，马斯克和OpenAI的世纪大和解，最可能建立在又一次AI寒冬的基础上。到那时，没人提AI，自然也没人关注OpenAI开源还是闭源。所谓世纪大和解，也就是一次对曾经网红技术的悼念，一个不重要的礼仪动作。</p><p>这么一想，好像还是他们继续对簿公堂比较好。</p><p>第二种和解方案，是OpenAI干脆顺了马斯克的意，走向开源，或者重回X大家庭的怀抱。比如被马斯克以重金砸到OpenAI回心转意，构建属于自己的AI帝国。如果这样的话，只有微软哭晕在厕所的世界就达成了。</p><p>马斯克将很可能实现坐拥AI半壁江山，构成X系对阵谷歌的双雄局面。那么接下来的故事，可能就是马斯克以一己之力血战谷歌，拳打GMS，脚踢Youtube，准备把谷歌全家桶变成X全家桶。</p><p>这是一个比较大男主向的剧情，但好像也挺有意思的。</p><p>当然了，还有第三种方案。就是OpenAI来他一个逆事顺办，我就按照你马斯克的意思，改名叫CloseAI，还顺便推出基于新名字的全新战略和愿景，比如要达成AI世界与物理世界的闭环之类的。</p><p>把你起的名字还给你，到时候且看你马斯克怎么办？</p><p>但一方面马斯克今天的咖位不值得OpenAI这么陪他玩。另一方面相声圈也给AI圈做过示范了：贸然自摘云字不可取，最后还得用回来。</p><p>杨立昆和马库斯：就这么再杠三十年，直到大厦崩塌</p><p>如果说，在AI圈里马斯克想要一切，那么他的本家马库斯就是想杠一切。</p><p>或许有朋友并不知道马库斯是谁，这是一位热爱AI技术，关注AI行业发展的认知心理学家。他确实配合过很多AI专家的工作，也撰写过AI相关的文章。但更广为人知的事情，是每次AI技术有新的发展，或者AI大佬有新的见解。马库斯就会拍马杀到，立刻在社交网络上表达“AI技术不行”或者“你说得不对”。</p><p>多年以来，马库斯给喝的倒彩场场不落。早在2017年，他就提出AI寒冬马上要来了，2022年，他发表了《深度学习撞墙了》。神奇的是，他越是唱衰，AI技术还就发展得越快。于是现在很多人都认为马库斯起到了给AI攒人品的反向吉祥物作用，AI行业没了谁也不能没有他。</p><p>而最喜欢与马库斯激情对线的，是图灵奖获得者，深度学习三巨头之一的杨立昆。</p><p>据马库斯说，他与杨立昆本来是多年老友，后来因为喷了杨立昆带领Meta团队做的Galactica模型而交恶。但在大伙的记忆里，这二位可是围绕着AI打了好几年的嘴仗。两个人从AI寒冬是否会发生，深度学习的本质是什么，编程范式是否需要更新这种技术性问题，到智能究竟是什么等形而上的宏大命题，每每吵到不可开交。</p><p><img src="https://static001.geekbang.org/infoq/71/717f8aac90f5639569ed9f82094d096b.jpeg" /></p><p></p><p>在2023年之前，两个人的立场其实是非常鲜明的。马库斯永远说AI不好，是这也不好，那也不好，不是咒AI要寒冬，就是咒深度学习要撞墙。作为深度学习技术的发明者之一，杨立昆则毫不留情给予还击。</p><p>事实上，很多人都把马库斯看作AI大佬，甚至用他的观点来判断国内的AI产业发展。但他真的不是AI专家，甚至不是AI从业者。AI业内人士也一直站在杨立昆这边，来反击马库斯对AI技术经常很不专业的批评。2022年，杨立昆就在采访中直言：“马库斯不是一个搞AI的，他是一个心理学家。他从未对人工智能做出任何贡献”。</p><p>但二人的矛盾，到ChatGPT发布之后出现了变化。杨立昆对ChatGPT是持保留意见的，批评其技术创新力不足，应用了Transformer架构等问题。这种保留可能有多方面原因，或许是技术思路的差异，也可能与其所带领的Meta团队被OpenAI超越有关。但不管怎么说，对这次爆火的AI创新，杨立昆是比较抵触的。而马库斯这边则是一如既往，抵触所有的AI创新，当然也包括ChatGPT。</p><p>于是乎，马库斯可是开心了，他认为杨立昆也来支持他了，并且单方面宣布二人已经达成了世纪大和解。或许他还期待着，面向未来两个人可以一起开开心心地批评AI。</p><p>那么，如果未来两人真的达成世纪大和解，不再打任何嘴仗了，会有哪些可能呢？</p><p>还是有两种可能性。第一种，是杨立昆也跟马库斯一样，变成了彻彻底底的AI技术悲观主义者。毕竟啊，自己完成的创新是创新，后辈完成的那个叫胡搅蛮缠。很多学界才俊，最后都会变成听不得半句反对的“学阀”。</p><p>但如果是这样，AI行业会发生什么改变呢？好像什么改变也不会发生，只不过唱衰AI的人又多了一个。一些媒体炮制惊悚标题的素材多了一些，比如“昔日AI之父，竟说AI要完”之类的。</p><p>然后呢，可能就没有然后了。只有两位老友对坐而饮，痛骂他们看不上，但又发展特别快的AI技术而已。</p><p>另一种可能性，则是马库斯转向了，开始支持AI的发展，认同杨立昆的大部分立场。但以吃瓜群众对马库斯的围观经验来看，他心悦诚服支持AI的可能性几乎没有，除非他认为找到了一种更新颖的方式来蹭AI热度。如果这样和解的话，AI圈就缺乏了一种旗帜鲜明的反对立场，一种对所有事都唱衰的批判精神。但会改变什么呢？还是什么都改变不了。</p><p>所以说，不如就让二位大佬就这么杠下去，杠一个身心舒畅，杠一个身体健康。</p><p>而咱们要做的，就是明白一件事：有一些人发表言论，就是为了新奇而新奇，为了抬杠而抬杠。除此之外，别无其他。</p><p>很多人啊，就是对头衔、名气之类的东西，有点过于着迷了。</p><p>OpenAI和DeepMind：王不见王</p><p>今天的北美AI行业，是一种四方对峙的态势。谷歌、微软被认为AI能力最强，而和xAI和Meta紧随其后。微软跻身第一梯队，是因为它外挂了OpenAI。对应这一点，谷歌的AI技术强大有一部分来源是外挂了DeepMind。</p><p>这家曾经以AlphaGO席卷天下，带来了AI第三次崛起的公司，似乎已经有点淡出主流视野了。今天提起OpenAI，总是说OpenAI vs谷歌怎样怎样。可能大家都忘了，马斯克等人创立OpenAI之初，是希望这家公司抗衡AI技术失控。那么是谁会带来失控呢？就是当时风头无两的DeepMind。</p><p>OpenAI和DeepMind之间，有一种“王不见王”的宿命论美感，“我本来是为了你而生的，结果等我成长起来你已经衰落了”。所谓君生我未生，我生君已老。同时还有一种“关公战秦琼”的微妙讽刺感。创立OpenAI是为了防止DeepMind毁灭世界，结果OpenAI现在带来了最可能毁灭世界的AI技术。</p><p><img src="https://static001.geekbang.org/infoq/ed/ed5eee6924fbadcb2501911a3ab03f9e.jpeg" /></p><p></p><p>但这都是茶余饭后的闲话，不足为惧。事实上，DeepMind依旧在持续发展，比如他们刚刚公布了适用于3D环境的智能体SIMA，在官网上发布了与利物浦合作研发AI足球教练的案例等。</p><p>但在持续的发展里，DeepMind确实表现出了与OpenAI的不同。前者更加重视应用场景，比如AI在蛋白质折叠、材料分析、医疗等领域的落地，更加重视前沿跨界合作。而OpenAI则将注意力放到了算法本身，把强大的AIGC模型作为发展根基。可以看到，它们俩之间的技术差异是具有高度互补性的，一个强调算法的“内力”，一个强调应用的“招式”，有一种华山派气宗剑宗之争的感觉。</p><p>虽然两家公司也没什么直接矛盾，但其中渊源不免让我们开个脑洞：假如OpenAI和DeepMind实现了世纪大和解，甚至直接合并了，那会怎么样？</p><p><img src="https://static001.geekbang.org/infoq/94/94dbfc1cf46e13142fc6c2f7ca4cb64e.jpeg" /></p><p></p><p>第一种可能，这是个美丽而伟大的童话故事。不管基于什么原因，最强的AI科学家与工程师聚集到了一起，把各自积累的AI技术进行融合，打通。于是既有强大算法，又能够深入各个场景应用的智能出现了，AGI以超乎想象的速度到来，AI觉醒就此开启。</p><p>再过若干年，历史学家会如此记录：那一天人类终于知道，无论是下围棋，还是开发AIGC能力，无论是Deep还是Open，原来都是为了实现同一个终局目标。那一天人类也终于知道，关于AI的恐怖真相是什么……</p><p>好吧，还是算了。这条故事线还是留在科幻作品里吧。</p><p>另一种可能，则是这两家公司之一走向了衰落。尤其是商业化层面的后继乏力，可能严重影响公司运营。于是在权衡多方面利弊之后，要么是谷歌出手买OpenAI，要么微软收购DeepMind。或者OpenAI的商业网络更加成功，从微软体系中独立，决心兼并DeepMind实现对先进AI技术的准垄断。</p><p>这条故事线里，世界将迎来一个新的科技商业帝国。谁是AI时代最成功的公司将彻底没有争议，全球科技版图或许也将就此改写。</p><p>沉默着分头发展，融合成庞然大物，建设一个不可预料的乌托邦？关于AI技术和AI公司的未来，你更喜欢哪一个故事呢？</p><p>AI卖课人和AI开发者社区：没意见，我只想看看你怎么圆</p><p>如果说，以上的AI矛盾都有些遥远。那么还有一种关于AI的矛盾，它就在我们身边。只要你对AI技术有好奇，有向往，看了一些AI相关的文章和视频，那么大概率就会刷到他们——出来吧，AI卖课人！</p><p>在今天，短视频和直播平台里活跃着难以估算数量的AI卖课人。普遍套路是让你抓紧上车，只有最后三个名额，就可以抢到原价1999，现价只需要399的AI实战课程。</p><p>当然了，除了这种模式之外，AI卖课人还有一些变体。比如面向少年儿童的鸡娃型AI卖课人；让你先加社群后卖资源的圈地型AI卖课人；告诉你有AI速成赚钱妙招的秘籍型AI卖课人等。</p><p>这类卖AI课程的朋友，一般来说只有两个问题。一是他们其实不懂AI，二是他们没进过AI行业。那么问题来了，他们的课程是哪里来的呢？</p><p>这就不得不提，卖课人到底和谁有矛盾？答案是他们跟AI开发者社区有矛盾。这种矛盾不是吵架，而是行为逻辑之间的冲突。</p><p><img src="https://static001.geekbang.org/infoq/52/5246a9f53a23f236598f9122f04599e9.png" /></p><p></p><p>我们知道，无论是AI框架还是AI开发工具，基础软件都需要生态的支持。于是各大科技企业都会围绕自己推出的平台、工具来建设AI开发者社区。从国外到国内都是如此。企业会拿出专项资金、人力来支持AI开发者赋能，有专门的团队负责社区建设。这就意味着，各大AI开发者社区，都有大量免费、实用，且被反复打磨过的AI学习资源，甚至有团队专门与AI开发者、学习者进行沟通。我们甚至见过在官方社群里，深夜还有专家给AI开发者回答非常基础的问题。</p><p>很多AI卖课人都知道这一点，于是他们利用信息差，把免费得到的资源简单做个包装，再用直播、短视频的方式进行售卖。他们的话术一般是先制造焦虑，开口就是“AI时代，不懂AI就被AI取代”。然后抛出现在只要399之类的课程信息。</p><p>到这里，矛盾就非常清晰了。厂商不会包装和宣传，于是有免费的资源和学习平台，普通用户却不知道。AI卖课人不懂AI，但会做短视频会直播，但他们卖的其实是免费资源，能忽悠一个是一个。</p><p>可能唯一的问题在于，厂商只会围绕自己的平台做推广，但AI卖课人可以横向给出多个平台的资源，但与用户付出的成本相比，这点价值微乎其微。</p><p>那么，如果矛盾的双方进行了世纪大和解，会怎么样呢？</p><p>可能性之一，是AI卖课人发现AI过气了，信息差很快被抹平了。忽悠人买AI课程也赚不到钱，于是纷纷开始转行。反正这个行业也挺新的，没什么行业粘性可言。于是世界又清静了，只留下花过几百块的同学暗自神伤。</p><p>这种世纪大和解，可能性是非常大的。只是希望我们都是闹剧的看客，而不是退费无门的学员。</p><p><img src="https://static001.geekbang.org/infoq/51/5185e69405eea6110ca2ef807a702450.jpeg" /></p><p></p><p>还有另一种可能性，是平台可以收编这些卖AI课程的老师。这样一来，各大厂商也终于学会了吆喝。主播们一播就是一天，把开发者大牛和AI专家请来当嘉宾。一开口就是，想要学图计算的宝宝们赶快上车了；做AIoT的家人们有福了；家人们，谁懂啊，AIGC应用的免费福利他真的来了。</p><p>这么一想，就还挺带感的对不对？</p><p>至于现在吗，我对AI卖课这轮热潮也没什么意见，只想看看你们怎么往下圆。</p><p>“智障派”和“取代派”： 我和我骄傲的倔强</p><p>关于AI，还有一种根深蒂固，横跨中西，穿越不同年龄层的矛盾。这个矛盾跟AI行业就没什么关系了，而是AI在大众传播层面所激起涟漪之间的某种回响。</p><p>说白了，就是网友们围绕AI是怎么吵架的。</p><p>经历了多次AI技术出圈之后，我们已经成功总结了这种吵架的模式。一般来说，矛盾双方分为“智障派”和“取代派”。</p><p><img src="https://static001.geekbang.org/infoq/b3/b39d27789ce74c97e0e8a86727e7a334.jpeg" /></p><p></p><p>智障派的发言是：“就这个AI你们还好意思吹？实测过后，发现AI就是智障。”</p><p>取代派的发言是：“你就是什么都不懂，我身边已经有人用AI了。人类马上就被取代！”</p><p>智障派又说了，AI要能取代你，你怎么还在这发评论呢？智障。</p><p>取代派又说了，AI顺利取代人类，就是因为有你这种带路党。ID已阅，什么成分？</p><p>总结一下，智障派的核心观点就是AI全都没用，无论做了什么都没用，用戳破AI的谎言这件事来吸引流量。</p><p>而取代派的观点则是AI什么都要接管，很快工作被抢走了，现实不存在了，人类要毁灭了，用制造恐慌和焦虑来吸引流量。</p><p>说到这，你可能会认为我们对这种争吵挺反感的？其实也不尽然。因为如果这两派世纪大和解了，同时两派都还存在，那可能麻烦就大了。</p><p><img src="https://static001.geekbang.org/infoq/f0/f00160c28c95a7b02c9407eabd5d290b.png" /></p><p></p><p>第一种可能，智障派被现实打脸。被无情的，钢铁一般的事实证明AI就是会取代工作，甚至出现了取代人类自立为王的苗头，那所有人都会敌视AI，禁锢AI，谁敢发展AI就应该万劫不复，这种技术将变成禁忌。</p><p><img src="https://static001.geekbang.org/infoq/50/5046af22a15e05c6f778d067fb2a21ea.jpeg" /></p><p></p><p>第二种可能，现实证明了AI就是智障，根本没什么用处，什么价值也产生不了，那么AI就失去了它的发展根基与应用信任。就像历史上两次AI寒冬一样，AI将迎来毁灭性打击。</p><p>所以啊，就让他们吵着吧。人最放不下的，就是自己骄傲的倔强。再明显的假唱也不能承认，更遑论承认对AI的观点是错的。</p><p>但对于大多数不怀有极端观点，不认为非此即彼，也愿意探索科技可能性的朋友。就让我们放下倔强，多点了解。</p><p>不用制造恐慌，也不用盲目诋毁，AI还是有很多事情可聊。</p><p>如此一来，人类或许能更早完成与AI的世纪大和解。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KaD3IxuL0uFbk5vQcr2r</id>
            <title>马斯克官宣Grok-1.5！超GPT-4 16倍上下文，推理能力超DBRX，网友：赢在敢说！</title>
            <link>https://www.infoq.cn/article/KaD3IxuL0uFbk5vQcr2r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KaD3IxuL0uFbk5vQcr2r</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Apr 2024 10:44:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 马斯克, Grok-1.5, 人工智能模型, 长上下文理解能力
<br>
<br>
总结: 马斯克发布了具有前所未有能力的人工智能模型Grok-1.5，其在编码与数学任务性能、长上下文理解能力等方面取得显著进展，即将在未来几天向开发者开放。 </div>
                        <hr>
                    
                    <p>引言：还记得 3 月 18 日，马斯克开源 Grok 的那一刻吗？如今，Grok 1.5 即将登场，其卓越的编码与数学处理能力、更深入的上下文理解（可处理高达 12.8 万 Tokens）以及更精准的长文本检索能力，令人震惊。马斯克就是马斯克，这速度，何尝不让人叹服。Grok-1.5 在未来几天即将在 X 上与开发者见面。</p><p></p><p></p><h4>Grok-1.5 登场</h4><p></p><p></p><p>当地时间 3 月 28 日，马斯克发布了 Grok-1.5，这是一个具有前所未有的长上下文支持和高级推理能力的新型人工智能模型。Grok-1.5，作为该系列的最新版本，预计将在未来几天向早期测试者和 X 平台的现有用户开放。借助于两周前公开的 Grok-1 模型权重和网络架构，该团队展现了至去年 11 月为止的技术成就，并自那以后在推理及问题解决方面取得了显著进展。</p><p></p><p></p><h5>能力与推理</h5><p></p><p></p><p>Grok-1.5 最显著的改进之一，就是更强大的编码与数学相关任务性能。在团队的实验中，Grok-1.5 在 MATH 基准测试上取得了 50.6% 的得分，在 GSM8k 基准测试上取得了 90% 得分——这两项数学基准测试涵盖从小学到高中的各类竞赛问题。此外，Grok-1.5 在评估代码生成与问题解决能力的 HumanEval 基准测试中得分为 74.1%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/10/10452a8749d533528735f7c1b7075475.png" /></p><p></p><p></p><h5>长上下文理解能力</h5><p></p><p></p><p>Grok-1.5 中的另一项新功能，就是在上下文窗口中处理多达 128K 个 tokens。这使得 Grok 的记忆容量增加至前代上下文长度的 16 倍，因此能够消化大部头文档中的信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/16/16301782c7be239ffc60c6279fdf5d7c.png" /></p><p></p><p>此外，Grok-1.5 模型还可处理更长、更复杂的提示词，在上下文窗口扩展的同时保持其指令跟踪能力。在 Needle In A Haystack (NIAH) 评估中，Grok-1.5 展示出强大的检索能力，可以在多达 128K tokens 的长上下文中嵌入文本，实现完美的检索结果，仅从从文本长度来看，Grok-1.5 可真的跨越极其之大，是 GPT-4 的 16 倍。</p><p></p><p>那么这么强的模型是如何训练的呢？一起来看看 Grok-1.5 的基础设施。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0ccfae74dea415a263982892300e419.png" /></p><p></p><p>在大规模 GPU 集群上运行领先大语言模型（LLM），自然离不开强大而灵活的基础设施。Grok-1.5 以基于 JAX、Rust 和 Kubernetes 的自定义分布式训练框架为基础，这套训练堆栈使 Grok 团队能够以最小的投入建立起设计原型，并大规模训练新型架构。</p><p></p><p>在大型计算集群上训练大模型的核心挑战，在于如何最大限度提高训练作业的可靠性与正常运行时间。Grok 团队自定义训练的协调器能够自动检测到有问题的节点，并将其从训练作业中剔除。团队还优化了检查点、数据加载与训练作业重新启动等机制，尽一切可能减少由故障引发的意外停机。</p><p></p><p></p><h4>Grok 1.5 VS “最强”开源大模型 DBRX</h4><p></p><p></p><p>目前，Grok 团队并未表示 Grok-1.5 是否开源，但从马斯克在与 OpenAI 的官司中推测，Grok-1.5 大概率是要开源的，否则有“知行不一”嫌疑。</p><p></p><p>当前的开源大模型市场竞争也是非常激烈的，Meta、Mistral 等已经处于前沿，但市场变化也非常快。当地时间 3 月 27 日，美国的 AI 创业公司 Databricks 以“黑马”之姿宣布，其 Mosaic Research 团队开发的全新通用大模型 DBRX 将被开源。这一消息由 DBRX 项目的首席神经网络架构师 Jonathan Frankle 在确认测试成果后宣布，他自信地告诉团队：“我们已经超越了市场上所有现有的模型。”一些测试成绩如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3e/3e6dd3457bd97e6f9a2f72235741d627.png" /></p><p></p><p>DBRX 在多项关键测试中的表现亮眼。在语言理解的 MMLU 测试中，DBRX 取得了 73.7% 的得分；而在代码生成能力的 HumanEval 测试中，得分为 70.1%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3a/3a4ba4a87dd4b5636baf78a00dad1427.png" /></p><p></p><p>此外，DBRX 在数学问题解决能力上的表现也十分出色，在 GSM8k 测试中获得了 66.9% 的成绩，这些结果表明 DBRX 在编程方面的能力甚至超过了专业模型如 CodeLlaMa-70B。</p><p></p><p>但是，也仅仅就是一天之后， Grok 1.5 宣告发布，想较于“最强”开源大模型 DBRX，Grok 1.5 表现更为亮眼。假使大家测试都不作弊， Grok 在 MMLU 测试中以 81.5% 的得分领先，HumanEval 测试中以 74.1% 的得分胜出，并在 GSM8k 测试中以 90% 的惊人得分远超 DBRX 的 66.9%。另外在长文本上，Grok 1.5 上下文窗口中处理多达 128K 个 tokens，远超于 DBRX 32K。</p><p></p><p>当然，这只是测试数据集的表现，不能完全说明实际情况，但是，测试集上表现好肯定也是优势。</p><p></p><p>开发者对 Grok 1.5 的热切期待</p><p></p><p>对于 Grok-1.5 的突然发布，有网友表示，来自 Grok-1.5 的测试图表给人留下深刻印象。它在信息检索方面，其表现与 Claude-3-Opus 和 GPT-4-Turbo 相媲美。迫不及待想要试一试了。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/96/96437c6555fb0e32b3f2d099c0909368.png" /></p><p></p><p>网友们对 Grok1.5 的热情洋溢在每一条评论中：“太棒了，这真是令人激动的进展！”随着新功能的即将推出，兴奋之情溢于言表。“我们能了解到网页界面发布的时间表吗？我在澳大利亚迫不及待地期待它的到来。”“请不要忘了智利！即使是 Grok 的 1.0 版本，对西班牙语的支持也已经相当出色了！”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/90/90e120098ccd4ca97a265896150bf68b.png" /></p><p></p><p>也有网友认为，马斯克除非拥有 10 倍的优势，否则也难以在开源大模型的竞争中取胜。</p><p></p><p>当然，值得注意的是，马斯克曾表示，X 平台将向更多用户开放 Grok 聊天机器人的访问权限，特别是对于那些已经订阅了每月 8 美元高级计划的用户。这一价格，与每月需要 19.99 美元才能使用的 GPT-4 和每月 28.99 美元的 Gemini Advanced 相比，显著更加经济。</p><p></p><p>另外，从历史上看，X.ai 的 Grok 模型与其他生成式 AI 模型的不同之处在于，它们回答了其他模型通常无法触及的主题问题，例如阴谋和更具争议性的政治思想。更大胆，更自由。</p><p></p><p></p><h4>结语</h4><p></p><p></p><p>GPT-4 已经与我们相伴超过一年，而 Gemini 1.5 几个月前亮相，Claude 3 仅数周前登场。昨天发布的开源大模型 DBRX 宣称超越了当前所有大型模型，结果今天就被 Grok 1.5 在一些细分方向超越。究竟哪个模型将在未来占据领先地位？虽然尚未可知，但毫无疑问，我们正处在人工智能发展的黄金时代，我们非常幸运。</p><p></p><p>参考链接：</p><p></p><p><a href="https://x.ai/blog/grok-1.5">https://x.ai/blog/grok-1.5</a>"</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247607796&amp;idx=1&amp;sn=49b82899ff27896d51339e80f369ec48&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s/eWsCDWquA0r26NodKlu5bA</a>"</p><p></p><p><a href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm</a>"</p><p></p><p><a href="https://techcrunch.com/2024/03/28/xs-grok-chatbot-will-soon-get-an-upgraded-model-grok-1-5/">https://techcrunch.com/2024/03/28/xs-grok-chatbot-will-soon-get-an-upgraded-model-grok-1-5/</a>"</p><p></p><p>内容推荐</p><p></p><p>大模型应用挑战赛已拉开帷幕。现阶段，多数语言模型已完成 3 轮更新，大模型赛道入场券所剩无几。同时，2023 年超 200 款大模型产品问世，典型场景又有哪些产品动向？对于现阶段的文生图产品而言，四大维度能力究竟如何？以上问题的回答尽在《2023年第 4 季度中国大模型季度监测报告》，欢迎大家扫码关注「AI 前线」公众号，回复「季度报告」领取。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/15/1589d84323013f719fff90e11914d747.jpeg" /></p><p></p><h5>今日荐文</h5><p></p><p>************你也「在看」吗？************👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qkuEDZRsHCSRoZjaFOYX</id>
            <title>Stability、Mistral、Databricks、通义、A21 Labs开源领域五连招，其中三个是MoE！|大模型一周大事</title>
            <link>https://www.infoq.cn/article/qkuEDZRsHCSRoZjaFOYX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qkuEDZRsHCSRoZjaFOYX</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Apr 2024 09:59:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, MoE模型, Gork-1.5, TextMonkey
<br>
<br>
总结: 大模型的快速发展使了解最新技术成为必修课，MoE模型成为研究重点，Gork-1.5和TextMonkey等新模型推出。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，开源领域五模型先后问世，Stability&nbsp;AI&nbsp;开源了&nbsp;Stable&nbsp;Code&nbsp;Instruct-3B，Mistral&nbsp;AI&nbsp;宣布Mistral&nbsp;7B&nbsp;v0.2&nbsp;Base&nbsp;Model，&nbsp;Databricks&nbsp;开源了其MoE模型&nbsp;DBRX，通义千问团队发布&nbsp;MoE&nbsp;模型：Qwen1.5-MoE-A2.7B，A21&nbsp;Labs&nbsp;宣布开源&nbsp;MoE&nbsp;大模型&nbsp;Jamba。其中，DBRX、Qwen1.5-MoE-A2.7B&nbsp;和&nbsp;Jamba&nbsp;都是MoE模型（混合专家模型）。</p><p>自从去年关于GPT-4的爆料和下半年&nbsp;Mistral&nbsp;AI&nbsp;开源了其&nbsp;Mixtal-8×7B-MoE&nbsp;模型，在广泛的关注下，MoE成为了大语言模型的一个重要研究方向。MoE&nbsp;本质是将计算负载分配给专门处理特定任务的专家，术业有专攻，这种方式不仅有利于模型进行更高效的预训练和推理速度，还有助于提升模型处理复杂任务的能力。我们也期待更加高效的模型架构，为&nbsp;AI&nbsp;带来更多的可能性。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>距离&nbsp;Grok-1&nbsp;的发布仅过去一周，3&nbsp;月&nbsp;29&nbsp;日，马斯克旗下的人工智能公司&nbsp;xAI&nbsp;正式推出了&nbsp;Gork&nbsp;大模型的最新版本&nbsp;Grok-1.5。新一代模型实现了长上下文理解和高级推理能力，并优化了数学和代码相关任务中的性能。</p><p></p><h4>多模态领域</h4><p></p><p>华中科技大学和金山的研究人员在最新的研究《TextMonkey:&nbsp;An&nbsp;OCR-Free&nbsp;Large&nbsp;Multimodal&nbsp;Model&nbsp;for&nbsp;Understanding&nbsp;Document》中提出了一个专注于文本相关任务（包括文档问答和场景文本问答）的多模态大模型TextMonkey。在多个场景文本和文档的测试中，TextMonkey&nbsp;处于国际领先地位，在办公自动化、智慧教育、智慧金融等行业有着强大的应用潜力。Suno&nbsp;团队近日推出的&nbsp;V3&nbsp;模型首次实现了生成广播质量的音乐。它可以根据简单的提示创建从歌词到人声和配器的所有内容，甚至可以引导它准确选择想要的流派，从三角洲蓝调（Delta&nbsp;Blues）到电子寒潮，还可以变换方言。来自理海大学、微软研究院的研究者在最新的研究《Mora:&nbsp;Enabling&nbsp;Generalist&nbsp;Video&nbsp;Generation&nbsp;via&nbsp;A&nbsp;Multi-Agent&nbsp;Framework》提出了一种多智能体框架&nbsp;Mora，该框架整合了几种先进的视觉&nbsp;AI&nbsp;智能体，以复制&nbsp;Sora&nbsp;所展示的通用视频生成能力。广泛的实验结果表明，Mora&nbsp;在各种任务中达到了接近&nbsp;Sora&nbsp;的性能。然而，当从整体上评估时，Mora&nbsp;与&nbsp;Sora&nbsp;之间存在明显的性能差距。Picsart&nbsp;AI&nbsp;Resarch&nbsp;等多个机构联合提出了一种新的文生视频方法：StreamingT2V。这也是一种自回归方法，并配备了长短期记忆模块，进而可以生成具有时间一致性的长视频。据了解，生成的视频帧数可达1200帧，时间可达2分钟。相关研究成果发布在论文《StreamingT2V:&nbsp;Consistent,&nbsp;Dynamic,&nbsp;and&nbsp;Extendable&nbsp;Long&nbsp;Video&nbsp;Generation&nbsp;from&nbsp;Text》中。3&nbsp;月&nbsp;27&nbsp;日，创新奇智在北京举办的发布会上发布了更为强大的奇智孔明工业大模型&nbsp;2.0&nbsp;版本（&nbsp;AInno-75B&nbsp;）。这款大型模型拥有处理多种信息形态的能力，涵盖了文本、图像和视频等。它甚至能够整合工业场景中特有的数据类型，例如计算机辅助设计（CAD）图纸和脑电图（EEG）信号。其输出同样具有多样性，可以生成包括文本、图像、视频、计算机辅助设计图以及具体操作行为等多种形式的信息。</p><p></p><h4>开源领域</h4><p></p><p>3&nbsp;月&nbsp;25&nbsp;日，Stability&nbsp;AI&nbsp;开源了小体量预训练模型&nbsp;Stable&nbsp;Code&nbsp;Instruct&nbsp;3B。这是一个基于&nbsp;Stable&nbsp;Code&nbsp;3B&nbsp;的指令调整编码语言模型。给出自然语言&nbsp;prompt，该模型可以处理各种任务，例如代码生成、数学和其他软件工程相关的任务。这款模型不仅增强了代码补全能力，还支持自然语言交互，旨在提高编程和软件开发相关任务的效率和直观性。3&nbsp;月&nbsp;25&nbsp;日，Mistral&nbsp;AI&nbsp;宣布Mistral&nbsp;7B&nbsp;v0.2&nbsp;Base&nbsp;Model&nbsp;开源，其是&nbsp;Mistral-7B-Instruct-v0.2&nbsp;背后的原始预训练模型。此次更新主要包括三个方面：①&nbsp;将&nbsp;8K&nbsp;上下文提到了&nbsp;32K；②&nbsp;Rope&nbsp;Theta&nbsp;=&nbsp;1e6；③&nbsp;取消滑动窗口。据了解，此次开放基础模型之后，开发者们就可以根据自己的需求进行微调了。3&nbsp;月&nbsp;28&nbsp;日，&nbsp;Databricks&nbsp;开源了通用大模型&nbsp;DBRX，这是一款拥有&nbsp;1320&nbsp;亿参数的混合专家模型（MoE），并支持&nbsp;32k&nbsp;Tokens&nbsp;的最长上下文长度，Base&nbsp;&nbsp;和&nbsp;Instruct&nbsp;版本已经在&nbsp;Github&nbsp;和&nbsp;Hugging&nbsp;Face&nbsp;上发布。3&nbsp;月&nbsp;28&nbsp;日，通义千问团队发布了他们的第一个MoE模型，Qwen1.5-MoE-A2.7B。模型大小缩小三分之一，性能却并未折损。同时，相比Qwen1.5-7B，Qwen1.5-MoE-A2.7B的训练成本降低了75%，推理速度则提升了1.74倍。3月29日，A21&nbsp;Labs&nbsp;宣布开源&nbsp;520&nbsp;亿参数的全新&nbsp;MOE&nbsp;大模型&nbsp;Jamba，支持&nbsp;256K&nbsp;的上下文长度。</p><p></p><h4>科研领域</h4><p></p><p>同济大学和中国科学院的研究团队开发了一种机器学习预测器&nbsp;PSPIre，它结合了残基级和结构级特征，用于精确预测相分离蛋白质（PSP）。同时，各种数据集的评估表明，该模型在将&nbsp;noID-PSP&nbsp;与非&nbsp;PSP&nbsp;进行分类方面显著优于当前的预测器。相关研究成果撰写在论文《Machine&nbsp;learning&nbsp;predictor&nbsp;PSPire&nbsp;screens&nbsp;for&nbsp;phase-separating&nbsp;proteins&nbsp;lacking&nbsp;intrinsically&nbsp;disordered&nbsp;regions》中。跨维智能、香港中文大学（深圳）及华南理工大学的研究人员在《SAM-6D:&nbsp;Segment&nbsp;Anything&nbsp;Model&nbsp;Meets&nbsp;Zero-Shot&nbsp;6D&nbsp;Object&nbsp;Pose&nbsp;Estimation》中创新性地提出了SAM-6D框架，该框架在零样本条件下实现6D物体姿态估计。它利用零样本分割技术生成候选对象，并借助独特的物体匹配分数来识别目标物体。此外，SAM-6D将姿态估计转化为局部到局部的点集匹配问题，通过引入Background&nbsp;Token设计和两阶段点集匹配模型，为任意物体的姿态估计提供了有效的解决方案。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>阿里通义千问重磅升级，向所有人免费开放&nbsp;1000&nbsp;万字的长文档处理功能，成为全球文档处理容量第一的&nbsp;AI&nbsp;应用。所有金融、法律、科研、医疗、教育等领域的专业人士，都可通过通义千问网站和&nbsp;APP&nbsp;快速读研报、分析财报、读科研论文、研判案情、读医疗报告、解读法律条文、分析考试成绩、总结深度文章。淘宝天猫集团自研大模型“淘宝星辰”官网已经上线，预计布局之后向公众开放。淘宝星辰的训练数据库中有大量电商消费数据，据推测将会为用户提供面向电商和生活服务场景增强的文案生成、多轮会话、知识问答、智能决策等能力。百度智能云最近一口气升级了&nbsp;7&nbsp;个大模型企业级应用，并且全部推出&nbsp;SaaS&nbsp;版本，企业在网页端即可使用，无需进行复杂的部署和配置，真正实现了开箱即用。这一举措将大大降低企业使用大模型的门槛，推动大模型技术在企业中的普及和应用。这&nbsp;7&nbsp;个应用分别是百度智能云曦灵数字人平台、百度智能云客悦智能客服平台、内容创作平台“一念”、知识管理平台“甄知”、超级办公入口“超级助理”、生成式BI产品“百度GBI”、代码助手“Baidu&nbsp;Comate”。</p><p></p><h4>智能体</h4><p></p><p>吉林大学、上海交通大学和伦敦大学学院合作提出了&nbsp;DS-Agent，这一智能体的角色定位是一名数据科学家，其目标是在自动化数据科学中处理复杂的机器学习建模任务。技术层面上，团队采用基于案例的推理策略，赋予了智能体&nbsp;“参考”&nbsp;他山之石的能力，使其能够利用以往解决类似问题的经验来解决新问题。相关成果发表在《DS-Agent:&nbsp;Automated&nbsp;Data&nbsp;Science&nbsp;by&nbsp;Empowering&nbsp;Large&nbsp;Language&nbsp;Models&nbsp;with&nbsp;Case-Based&nbsp;Reasoning》中。立志成为&nbsp;Cognition&nbsp;AI&nbsp;的&nbsp;Devin&nbsp;的竞争性开源替代方案&nbsp;Devika&nbsp;已发布。&nbsp;Devika拥有高级AI规划和推理能力，可以进行针对性的上下文关键词提取、多种编程语言的代码编写和动态代理状态跟踪和可视化，同时也可以无缝进行网络浏览和信息收集。当然，deviika&nbsp;也可以通过聊天界面进行自然语言交互，并支持Claude&nbsp;3、GPT-4、GPT-3.5&nbsp;和通过&nbsp;Ollama&nbsp;支持的本地语言模型。</p><p></p><h4>终端AI</h4><p></p><p>3月26日，2024&nbsp;全新英特尔商用客户端&nbsp;AI&nbsp;PC产品发布会在北京举办，基于&nbsp;ChatGLM&nbsp;端侧模型打造的「智谱AI&nbsp;PC智能助手」也正式发布。该款智能助手是针对英特尔全新的酷睿&nbsp;Ultra&nbsp;处理器，在ChatGLM端侧模型的基础上训练、适配并部署的最新端侧AI模型，拥有高性能、低延迟的特点。在远程管理、安全防护、跨设备IT管理和运维等方面可为&nbsp;PC&nbsp;用户提供更加轻松、高效的&nbsp;AI&nbsp;体验。3&nbsp;月&nbsp;28&nbsp;日，阿里云与知名半导体公司&nbsp;MediaTek&nbsp;联发科联合宣布，通义千问&nbsp;18&nbsp;亿、40&nbsp;亿参数大模型已成功部署进天玑&nbsp;9300&nbsp;移动平台，可离线流畅运行即时且精准的多轮&nbsp;AI&nbsp;对话应用，连续推理功耗增量不到&nbsp;3W，实现了手机&nbsp;AI&nbsp;体验的大幅提升。</p><p></p><h3>基础设施</h3><p></p><p>3&nbsp;月&nbsp;28&nbsp;日，云天励飞举办AI大模型产品发布会，正式发布“深目”&nbsp;AI&nbsp;模盒，实现了算法在线学习、自我迭代。据介绍，该产品能够做到“3&nbsp;个&nbsp;90%”——覆盖场景超过&nbsp;90%、算法精度超过&nbsp;90%，使用成本降低&nbsp;90&nbsp;%。可以说真正地解决大模型在场景落地最后一公里的问题，帮助更多中小企业客户轻松使用大模型。</p><p></p><p></p><p>报告预告</p><p>Sora来袭，国内如何迅速跟上？开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，其能力是否有所提升和刷新？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？InfoQ研究中心即将发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，即将给出答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9b3c569c62a571715d811e7121db70f.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>