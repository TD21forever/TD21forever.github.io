<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/mylDck2yRFzDaRORIdgx</id>
            <title>中信银行财富管理部副处长黄河确认出席FCon，分享智行财富——中信银行财富管理数字化转型实践</title>
            <link>https://www.infoq.cn/article/mylDck2yRFzDaRORIdgx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mylDck2yRFzDaRORIdgx</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 智行财富, 中信银行财富管理数字化转型实践, 黄河
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，中信银行财富管理部副处长黄河将分享智行财富的数字化转型实践，介绍财富管理的内涵和未来发展方向，通过AI赋能、体验至上、流量+生态、基础设施等方式构建非线性增长的财富管理业务模式，实现效率提升。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。中信银行财富管理部副处长黄河将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5589?utm_source=infoqweb&amp;utm_medium=article">智行财富——中信银行财富管理数字化转型实践</a>"》主题分享，介绍财富管理的内在含义及未来走向，通过“AI 赋能“、”体验至上”、“流量”+“生态”、“基础设施”，环环相扣，相互促进，构建了非线性增长的财富管理业务模式，实现效率跃迁。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5589?utm_source=infoqweb&amp;utm_medium=article">黄河</a>"，参与了零售银行的数字化转型规划与推动，在客户旅程重塑、超渠道、个性化经营、数字化运营等领域推进产品化与工具化，实现零售数字化能力升级。在数字化营销，客户全生命周期管理体系、客户旅程营销、统一策略管控等方面，建立了数据驱动的、带来竞争优势的企业级客户精细化运营模式。在数字化管理领域，建立可量化、可衡量、可监测的指标体系，建立“先知 - 先决 - 先行”的数据驱动经营决策，强化经营管理风险态势提前预判与干预，提升管理效力和质量。她在本次会议的演讲内容如下：</p><p></p><p>演讲：智行财富——中信银行财富管理数字化转型实践</p><p></p><p>财富管理实质是客户关系，随着技术更新迭代，客户需求升级，面临的难题是如何以合理的成本去高效服务亿级全量客户。AI 数智化使得普惠财富管理成为可能，为亿级客户提供的全客户、全产品、全渠道精准适配。</p><p></p><p>整合大财富管理向下的产品，服务，内容，权益，活动，借助 AI 能力升级包括数据采集标准化，智能推荐精准化，流量分发自动化，策略迭代最优化，数据评价科学化在内的客户旅程闭环经营能力，打造线上五位一体综合金融服务方案。</p><p></p><p>通过“AI 赋能“、”体验至上”、“流量”+“生态”、“基础设施”，环环相扣，相互促进，构建非线性增长的财富管理业务模式，实现效率跃迁。</p><p></p><p>演讲提纲：</p><p></p><p>财富管理的内在含义及未来走向“AI+ 财富管理”的全景视图AI 赋能全量客户经营的案例分享财富管理的数字化能力建设</p><p></p><p>你将获得：</p><p></p><p>○ 财富管理数字化体系建设</p><p>○ AI 赋能下的亿级客户运营体系</p><p>○ 提炼业务中台核心能力</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 7 折优惠 ，立省 ￥2040！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2oHgwVeLDDbLagM1fykU</id>
            <title>极氪汽车否认安全气囊故障消息；英伟达与鸿海集团共建AI工厂；奇瑞汽车与半导体公司共建“汽车芯片联合实验室”｜汽车技术资讯</title>
            <link>https://www.infoq.cn/article/2oHgwVeLDDbLagM1fykU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2oHgwVeLDDbLagM1fykU</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 02:00:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 极氪汽车, 安全气囊, 虚假信息, 奇瑞汽车, 汽车芯片联合实验室, 英伟达, 鸿海集团, 人工智能工厂, 蔚来汽车, 蔚来杨戬商标, 日本, 公共道路电动汽车无线充电系统实验
<br>
<br>
总结: 极氪汽车回应了关于安全气囊故障的虚假信息。奇瑞汽车与半导体公司共建了汽车芯片联合实验室，推动国产车规级芯片产业发展。英伟达与鸿海集团共同建设人工智能工厂，加速AI驱动的电动汽车和机器人平台的开发。蔚来汽车申请了“蔚来杨戬”商标，准备在多个领域应用。日本开展了公共道路电动汽车无线充电系统实验，为未来自动驾驶巴士的运营做准备。 </div>
                        <hr>
                    
                    <p></p><h3>极氪汽车称“安全气囊”故障消息为虚假信息</h3><p></p><p></p><p>针对网传“极氪 001 汽车安全气囊将人弹飞”消息，极氪日前发声明回应称，经核实均为虚假信息。极氪表示，2023 年 10 月 19 日，嘉兴海宁发生一起行人穿越道路被撞事故。经了解事故过程为车辆行驶中撞到横穿道路的行人，碰撞发生后安全气囊正常弹出，车辆驾驶员未受到伤害。根据事故车辆行车数据显示，事故发生时，该车辆时速约为 120km/h，车辆未使用 LCC 等驾驶辅助功能。极氪称，事故发生后，网络平台有账号发布后车行车记录仪视频等相关内容，并传播“极氪 001 在驾驶过程中气囊爆了人弹射出来了”等信息，经核实均为虚假信息。</p><p></p><h3>奇瑞汽车与半导体公司共建“汽车芯片联合实验室”</h3><p></p><p></p><p>10 月 20 日，奇瑞汽车股份有限公司与安徽长飞先进半导体有限公司举办了“汽车<a href="https://www.infoq.cn/news/QJ73po4wuwTvLKcpK1Fw">芯片</a>"联合实验室”战略合作签约仪式。双方将充分发挥各自领域的技术和资源优势，在车规级芯片及其汽车应用技术、市场开发等领域展开广泛合作，助力国产车规级芯片产业发展。奇瑞汽车与长飞先进半导体公司共建“汽车芯片联合实验室”，不仅是双方共同探索车规级芯片的重要举措，同时对于国产碳化硅及汽车产业的发展都将具有重要意义。未来，双方将致力于车规级芯片及汽车产业的发展，共同解决碳化硅车规级芯片应用难题，共同推动我国碳化硅芯片生态、零部件生态、整车生态融合发展。</p><p></p><h3>英伟达与鸿海集团正在共建人工智能工厂</h3><p></p><p></p><p>近日，英伟达 CEO <a href="https://www.infoq.cn/article/qzs4Zp3DH1wxIeSpVgU8">黄仁勋</a>"与鸿海集团董事长刘扬伟共同宣布，两家公司正在共同建设人工智能（AI）工厂。英伟达表示，它正在扩大与鸿海科技集团（富士康）的持续合作，以加速 AI 驱动的电动汽车和机器人平台的开发。作为合作的一部分，鸿海计划开发一种被称为“AI 工厂”的新型数据中心，为创新提供动力，例如制造数字化和生成式 AI 服务。鸿海表示，其正在构建的 AI 工厂系统基于英伟达 HGX 参考设计，每个平台将配备 8 个英伟达 H100 GPU，以及英伟达 GH200 超级芯片和网络。这些系统将允许鸿海的客户在自己的数据中心内部署英伟达加速计算，为内部生成 AI 工作负载和其他任务提供支持，例如自主机器人和自动驾驶汽车的训练。</p><p></p><h3>蔚来汽车为自研芯片申请“蔚来杨戬”商标，预计本月量产</h3><p></p><p></p><p>在上个月举行的 2023 蔚来创新科技日上，李斌宣布蔚来首颗自研芯片 10 月量产，号称是业界首颗自研激光雷达主控芯片，中文名“杨戬”。现在，该公司正在将“蔚来杨戬”申请注册为商标。值得一提的是，蔚来将这一商标添加了多个类似群，包括计算机软件（已录制）、模拟对话用聊天机器人软件、可下载的手机应用软件、中央处理器（CPU）、促进人机交互的软件、计算机程序（可下载软件）、电视电脑一体机、基于人工智能的交互式软件、智能眼镜（数据处理）等。这也为“蔚来杨戬”这一名称在其他领域的应用，提前进行了准备。</p><p></p><h3>日本开展公共道路电动汽车无线充电系统实验</h3><p></p><p></p><p>10 月 19 日，由东京大学、千叶大学、普利司通、三井不动产等 9 家公司组成联合研究小组，于本月初在日本千叶县柏市的“柏之叶智慧新城”开始公共道路电动汽车无线充电设施的系统实验。据介绍，该项目在交通信号灯前面等“关键位置”将输电线圈嵌入路面，因为这些位置的车辆大多会缓慢行驶，以方便充电。该设施预计可在 10 秒钟内为一辆普通电动汽车补充约 1 公里的续航。为避免浪费能源，输电线圈并非始终处于通电状态，而是安装了感应系统：车辆靠近时才会工作，待机状态下可保持最低功率。这项测试预计将持续到 2025 年 3 月，测试内容包括设备的安全性、耐久性等方面。若测试结果成功且符合预期，当地目标是计划 2030 年左右运营配备路面无线充电系统的自动驾驶巴士。</p><p></p><p><img src="https://static001.geekbang.org/infoq/33/338e62e68cd9d65456a36df398b55199.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TAEwvbPaEjyRdUKtmOC5</id>
            <title>Github Copilot Chat公测：已可在Visual Studio和VS Code中使用</title>
            <link>https://www.infoq.cn/article/TAEwvbPaEjyRdUKtmOC5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TAEwvbPaEjyRdUKtmOC5</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Github Copilot Chat, 公测版, AI驱动软件开发扩展, 自然语言处理技术
<br>
<br>
总结: "Github Copilot Chat"是一个AI驱动的软件开发扩展，通过自然语言处理技术在支持的编辑器中提供编码相关的帮助和答案。它目前处于公测阶段，可供所有GitHub Copilot个人用户在Visual Studio和VS Code中使用。 </div>
                        <hr>
                    
                    <p><a href="https://docs.github.com/en/copilot/github-copilot-chat">Github Copilot Chat</a>"是一个聊天界面，让开发人员可以在其支持的IDE中直接询问编码相关的问题并接收答案。它目前处于公测阶段，可供所有GitHub Copilot个人用户在Visual Studio和VS Code中使用。</p><p>&nbsp;</p><p><a href="https://github.blog/2023-07-20-github-copilot-chat-beta-now-available-for-every-organization/">GitHub Copilot Chat作为公测版推出</a>"，是一个面向所有GitHub Copilot商业用户的AI驱动软件开发扩展。现在，所有的Copilot个人用户都可以免费使用，Visual Studio和Visual Studio Code编辑器都提供了支持。该扩展利用先进的自然语言处理（NLP）技术在所支持的编辑器中直接以自然语言提供帮助和答案，开发人员不必再查阅文档或搜索在线论坛了。</p><p>&nbsp;</p><p><a href="https://www.linkedin.com/in/shuyin-zhao-5758307b/">Shuyin Zhao</a>"是GitHub Copilot的产品副总裁。他在一篇博文中写道：</p><p></p><p></p><blockquote>通过减少上下文切换，它简化了开发过程，有助于开发人员保持注意力和工作势头。</blockquote><p></p><p>&nbsp;</p><p>GitHub Copilot Chat还允许个人贡献者提交安全补丁，从而增强该开源项目的整体安全性。此外，GitHub还详细介绍了Copilot Chat提供的其他功能，包括：</p><p>实时定制关于最佳实践、编码技巧和解决方案的建议；通过解释代码片段来分解复杂的概念；提供修复建议，帮助减少安全扫描期间发现的漏洞；识别调试过程中的问题，进行提示、解释，并提供替代方法。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52b3b956d0db269d83aa41d688c05d6c.gif" /></p><p></p><p>GitHub Copilot Chat（来自<a href="https://github.blog/2023-09-20-github-copilot-chat-beta-now-available-for-all-individuals/">GitHub博客</a>"）</p><p>&nbsp;</p><p>随着用户数的增加，虽然Copilot Chat提供的功能也越来越多，但人们还是有更多的需求。</p><p>&nbsp;</p><p><a href="https://twitter.com/haseeb_heaven/">Haseeb Heaven</a>"是一名开发人员。他在<a href="https://twitter.com/haseeb_heaven/status/1704551693436821515">推特</a>"上评论说：</p><p></p><p></p><blockquote>请提供自动调试、代码库知识等功能，看在上帝的份上，请开始使用GPT-4。人们已经转向@cursor_ai来获取更高级的功能了。</blockquote><p></p><p>&nbsp;</p><p>而SMB Punt则<a href="https://twitter.com/SMB_Punt/status/1704542153852997936">评论说</a>"：</p><p></p><p></p><blockquote>只有Visual Studio吗？JetBrains呢？</blockquote><p></p><p>&nbsp;</p><p>在提供了Copilot Chat之后，<a href="https://github.blog/2023-10-10-research-quantifying-github-copilots-impact-on-code-quality/">GitHub对扩展进行了研究</a>"。他们得出的结论是：启用GitHub Copilot Chat后，代码编写和审核的质量全面提高，尽管开发人员之前并没有使用过这个功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/3767625f26c58a64b15cd126a6486563.png" /></p><p></p><p>GitHub Copilot Chat的搜索结果（来自<a href="https://github.blog/2023-10-10-research-quantifying-github-copilots-impact-on-code-quality/">GitHub博客</a>"）&nbsp;</p><p>&nbsp;</p><p>不过，<a href="https://twitter.com/rdarder/status/1711781869988049023">Rafa Darader</a>"对他们的研究<a href="https://twitter.com/rdarder/status/1711781869988049023">提出了质疑</a>"：</p><p></p><p></p><blockquote>我喜欢Copilot，但这项研究几乎没提到质量。那些大概的指标只是描述了开发人员在工作流中添加Copilot之后的感受。&nbsp;质量能仅仅通过观察生产过程中涉及的部分来衡量吗？</blockquote><p></p><p>&nbsp;</p><p>最后，要使用Copilot Chat，用户必须有一个活跃的<a href="https://docs.github.com/en/billing/managing-billing-for-github-copilot/managing-your-github-copilot-subscription-for-your-personal-account">GitHub Copilot订阅</a>"，而且VS Code用户必须使用最新扩展，并使用与访问GitHub Copilot相同的GitHub ID登录到VS Code。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/github-copilot-chat-open-beta/">https://www.infoq.com/news/2023/10/github-copilot-chat-open-beta/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a988543f429467b9a1c232f69</id>
            <title>文心一言 VS 讯飞星火 VS chatgpt （120）-- 算法导论10.3 5题</title>
            <link>https://www.infoq.cn/article/a988543f429467b9a1c232f69</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a988543f429467b9a1c232f69</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 11:41:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: go语言, 双向链表, 自由表, COMPACTIFY-LIST
<br>
<br>
总结: 在这个问题中，我们需要使用go语言实现一个过程COMPACTIFY-LIST，它将双向链表L中的元素移动到数组的1到n的位置，并调整自由表F以保持其正确性，同时将剩余的元素移动到数组的n+1到m的位置。这个过程需要在O(n)的时间内完成，并且只使用固定量的额外存储空间。 </div>
                        <hr>
                    
                    <p></p><h1>五、用go语言，设 L 是一个长度为 n 的双向链表，存储于长度为 m 的数组key、prev 和next 中。假设这些数组由维护双链自由表 F的两个过程 ALLOCATE-OBJECT 和 FREE-OBJECT 进行管理。又假设 m 个元素中，恰有 n 个元素在链表 L 上，m-n 个在自由表上。给定链表 L 和自由表 F，试写出一个过程 COMPACTIFY-LIST(L，F)，用来移动 L中的元素使其占用数组中 1，2，···，n 的位置，调整自由表 F 以保持其正确性，并且占用数组中 n+1，n+2，··.，m 的位置。要求所写的过程运行时间应为 O(n) ，且只使用固定量的额外存储空间。请证明所写的过程是正确的。</h1><p></p><p></p><h2>文心一言：</h2><p></p><p>在这个问题中，我们需要实现一个过程 COMPACTIFY-LIST(L, F)，它将链表 L 中的元素移动到数组的 1 到 n 的位置，并调整自由表 F 以保持其正确性，同时将剩余的元素移动到数组的 n+1 到 m 的位置。这个过程需要在 O(n) 的时间内完成，并且只使用固定量的额外存储空间。</p><p></p><p>首先，我们需要定义 ALLOCATE-OBJECT 和 FREE-OBJECT 这两个过程，并实现 COMPACTIFY-LIST(L, F)。然后，我们需要证明这个过程的正确性。</p><p></p><p>以下是一种可能的实现方式：</p><p></p><p><code lang="go">// ALLOCATE-OBJECT 分配一个新的对象到自由表 F 上  
func ALLOCATE-OBJECT(F) {  
    // 在这里，我们只是简单地返回一个新对象，并没有真正地将其添加到自由表 F 上  
    // 因为在我们的实现中，我们并不真正需要自由表 F 来存储对象  
    return newNode // 这里我们假设 newNode 是新创建的节点  
}  
  
// FREE-OBJECT 将对象从自由表 F 上释放  
func FREE-OBJECT(F, node) {  
    // 在这里，我们只是简单地释放对象，并没有真正地从自由表 F 中删除它  
    // 因为在我们的实现中，我们并不真正需要自由表 F 来存储对象  
    node.key = nil // 将节点的 key 设置为 nil 表示它已经被释放了  
}  
  
// COMPACTIFY-LIST(L, F) 压缩链表 L 并调整自由表 F  
func COMPACTIFY-LIST(L, F) {  
    // 初始化 prev 和 next 数组  
    var prev [m]Node // 这里我们假设 Node 是节点类型  
    var next [m]Node  
      
    // 将 L 中的元素移动到数组的 1 到 n 的位置  
    for i := 0; i &lt; n; i++ {  
        // 从链表 L 中取出一个节点并移动到数组的第 i 个位置  
        node := L.remove()  
        prev[i] = node  
        next[i] = newNode // 这里我们假设 newNode 是新创建的节点，用来填充数组中的剩余位置  
    }  
      
    // 调整自由表 F 以保持其正确性  
    for i := 0; i &lt; n; i++ {  
        node := &amp;prev[i]  
        node.next = &amp;next[i+1]  
        node.prev = &amp;prev[i-1]  
        if i == 0 {  
            node.prev = nil  
        } else if i == n-1 {  
            node.next = nil  
        } else {  
            node.next.prev = node  
            node.prev.next = node  
        }  
    }  
}
</code></p><p></p><p>在这个实现中，我们并没有真正地实现 ALLOCATE-OBJECT 和 FREE-OBJECT。因为我们假设自由表 F 并不真正需要存储对象，只需要在适当的时候将对象的 key 设置为 nil 就可以表示它已经被释放了。同样地，我们假设新创建的节点 newNode 只需要被用来填充数组中的剩余位置，而不需要真正地添加到链表 L 中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebba990d673c3572ae999ae28859d05e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/20/206fab794ee1c1c0209440a5219d861a.png" /></p><p></p><h2>讯飞星火：</h2><p></p><p>首先，我们需要定义双向链表的结构体。然后，我们可以编写一个函数compactify_list来实现题目要求的过程。最后，我们需要证明这个函数是正确的。</p><p></p><p><code lang="go">package main

import "fmt"

type Node struct {
    key  int
    prev *Node
    next *Node
}

func compactify_list(L *Node, F []*Node) {
    if L == nil {
        return
    }

    // 初始化两个指针，分别指向链表的头部和自由表的头部
    current := L
    i := 0
    for current != nil &amp;&amp; i &lt; len(F) {
        // 如果当前节点在链表中，将其移动到数组中的位置 i+1
        if current.key &gt;= 0 {
            current.key = i + 1
            i++
        } else {
            // 如果当前节点在自由表中，将其移动到数组中的位置 n+i+1
            current.key = len(L) + i + 1
            i++
        }

        // 更新当前节点的前后指针
        current.next.prev = current.prev
        current.prev.next = current.next
        current = current.next
    }

    // 更新链表的尾部指针
    if current != nil {
        L.prev = current
    } else {
        L.prev = nil
    }
}

func main() {
    // 创建一个示例链表
    L := &amp;Node{key: -1, prev: nil, next: nil}
    L.next = &amp;Node{key: -1, prev: L, next: nil}
    L.next.next = &amp;Node{key: 2, prev: L.next, next: nil}
    L.next.next.next = &amp;Node{key: -1, prev: L.next.next, next: nil}
    L.next.next.next.next = &amp;Node{key: 3, prev: L.next.next.next, next: nil}
    L.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next, next: nil}
    L.next.next.next.next.next.next = &amp;Node{key: 4, prev: L.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next = &amp;Node{key: 5, prev: L.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 6, prev: L.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 7, prev: L.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 8, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 9, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}

    // 创建一个示例自由表
    F := make([]*Node, 0)
    for i := 0; i &lt; 10; i++ {
        F = append(F, &amp;Node{key: i})
    }

    // 调用 compactify_list 函数
    compactify_list(L, F)

    // 打印结果
    current := L
    for current != nil {
        fmt.Printf("%d ", current.key)
        current = current.next
    }
}
</code></p><p></p><p>通过运行上述代码，我们可以得到正确的结果：1 2 3 4 5 6 7 8 9。因此，我们可以证明这个函数是正确的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29821510b88d7837b924a3b7790483a9.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f742b63d1bfbac55d2007067a8754072.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5e4d4cf83b49de153bf6acabf9134edf.png" /></p><p></p><h2>360智脑：</h2><p></p><p>以下是用Go语言实现的一个过程，用来压缩链表：</p><p></p><p><code lang="go">func CompactifyList(l *List, F *FreeList) {
    // 初始化自由表
    F.Initialize()
    // 遍历链表
    for l != nil {
        // 将当前节点的next指针指向前一个节点
        l.Next = l.Prev
        // 如果前一个节点为空或者前一个节点的prev指针指向当前节点，则将当前节点的prev指针赋值为前一个节点
        if l.Prev == nil || l.Prev.Prev == l {
            l.Prev.Prev = l
        } else {
            l.Prev.Prev.Next = l
        }
        l = l.Prev
    }
    // 将链表头部的prev指针和尾部的next指针都指向自由表中的最后一个节点
    l.Prev.Next = F.Last()
    l.Next.Prev = F.First()
    // 释放链表中未使用的元素，并将链表头部的prev指针和尾部的next指针都设置为自由表中的第一个节点
    F.Deallocate()
    l.Prev.Next = F.First()
    l.Next.Prev = F.Last()
}
</code></p><p></p><p>该过程的工作原理是，首先初始化自由表，然后遍历链表。在遍历过程中，如果当前节点的前一个节点为空或者前一个节点的prev指针指向当前节点，则将当前节点的prev指针赋值为前一个节点。然后将当前节点的next指针指向前一个节点。最后，将链表头部的prev指针和尾部的next指针都指向自由表中的最后一个节点，并将链表头部的prev指针和尾部的next指针都设置为自由表中的第一个节点。同时，释放链表中未使用的元素，并将链表头部的prev指针和尾部的next指针都设置为自由表中的第一个节点。</p><p></p><p>该过程的时间复杂度为O(n)，因为遍历链表只需要一次遍历。该过程使用的额外存储空间为O(1)，因为只需要存储自由表的头尾指针。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03badcb799164c4f70236ba42b0f366e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/22/2227989e7b29b32531e324efc843b7cd.png" /></p><p></p><h2>智谱清言：</h2><p></p><p><code lang="go">package main
import (  
  "fmt"  
)
type ListNode struct {  
  Key   int  
  Prev  *ListNode  
  Next  *ListNode  
}
func CompactifyList(L *ListNode, F [][]*ListNode) {  
  // 找到链表 L 的头节点  
  var head *ListNode  
  for p := F[0]; p != nil; p = p.Next {  
    if p.Key == L.Key {  
      head = p  
      break  
    }  
  }
  // 遍历链表 L，将节点移动到数组中的正确位置  
  for p := L.Next; p != nil; p = p.Next {  
    // 找到下一个节点在数组中的位置  
    i := 0  
    for ; i &lt; len(F) &amp;&amp; F[i] != nil; i++ {  
      if F[i].Key == p.Key {  
        break  
      }  
    }
    // 如果找到了下一个节点，则交换两个节点的位置  
    if i &lt; len(F) &amp;&amp; F[i] != nil &amp;&amp; F[i].Key == p.Key {  
      F[i], F[i+1] = F[i+1], F[i]  
    }
    // 将节点 p 移动到数组中的正确位置  
    for j := i; j &lt; len(F)-1 &amp;&amp; F[j+1] != nil; j++ {  
      F[j+1], F[j] = F[j], F[j+1]  
    }  
    F[j] = p  
  }
  // 调整自由表 F 以保持其正确性  
  for i := len(F) - 1; i &gt; 0 &amp;&amp; F[i] == nil; i-- {  
    F[i] = F[i-1]  
  }  
}
func main() {  
  // 测试用例  
  F := [10]*ListNode{  
    {Key: 1, Prev: nil, Next: nil},  
    {Key: 2, Prev: nil, Next: nil},  
    {Key: 3, Prev: nil, Next: nil},  
    {Key: 4, Prev: nil, Next: nil},  
    {Key: 5, Prev: nil, Next: nil},  
    {Key: 6, Prev: nil, Next: nil},  
    {Key: 7, Prev: nil, Next: nil},  
    {Key: 8, Prev: nil, Next: nil},  
    {Key: 9, Prev: nil, Next: nil},  
    {Key: 10, Prev: nil, Next: nil},  
  }
  L := F[5]  
  CompactifyList(L, F)
  for i := 0; i &lt; len(F); i++ {  
    if F[i] != nil {  
      fmt.Printf("F[%d] = %d\n", i, F[i].Key)  
    } else {  
      fmt.Printf("F[%d] = %p\n", i, F[i])  
    }  
  }  
}
</code></p><p></p><p>证明：</p><p></p><p>所写的过程运行时间应为 O(n)，且只使用固定量的额外存储空间。</p><p></p><p>证明：</p><p></p><p>1.过程 Compactify-List(L，F) 首先找到链表 L 的头节点，时间复杂度为 O(1)。</p><p></p><p>2.过程 Compactify-List(L，F) 然后遍历链表 L，将节点移动到数组中的正确位置。对于每个节点，我们只需要找到下一个节点在数组中的位置，时间复杂度为 O(1)。然后交换两个节点的位置，时间复杂度为 O(1)。所以遍历链表 L 的时间复杂度为 O(n)。</p><p></p><p>3.过程 Compactify-List(L，F) 最后调整自由表 F 以保持其正确性。我们只需要从后向前遍历自由表 F，将空节点移动到数组中的正确位置。遍历自由表 F 的时间复杂度为 O(n)。综上，过程 Compactify-List(L，F) 的总</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d07e792e2444c65de32517e89859f30b.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a27c8b617179e154ead78b9cec088def.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e37f9828a14a0707632fbf8f7d6b99d1.png" /></p><p></p><h2>通义千问：</h2><p></p><p>这个问题可以通过以下步骤来解决：</p><p></p><p>首先，我们需要创建一个新的数组，大小为n，用于存储链表中的元素。我们也可以使用一个临时的指针数组，大小为m，用于跟踪每个元素在新的数组中的位置。然后，我们遍历链表L，将每个元素移动到新的数组中。在移动元素的同时，我们也更新临时指针数组中的位置。最后，我们遍历自由表F，将所有不在新的数组中的元素移动到新的数组的末尾。在移动元素的同时，我们也更新临时指针数组中的位置。</p><p></p><p>以下是实现这个过程的Go语言代码：</p><p></p><p><code lang="go">func CompactifyList(L *ListNode, F []*ListNode) {
    n := L.size()
    m := F.length()
    newL := make([]*ListNode, n)
    pos := make([]int, m)

    for i, node := range L.nodes {
        newL[i] = node
        pos[node.key] = i
    }

    for i := 0; i &lt; m; i++ {
        if pos[F[i].key] &lt; n {
            newL[pos[F[i].key]] = F[i]
            pos[F[i].key]++
        } else {
            newL[n] = F[i]
            n++
        }
    }
    *L = *newL
}
</code></p><p></p><p>证明：</p><p></p><p>1.正确性：我们可以证明这个过程是正确的。首先，我们可以通过遍历新的数组来验证所有链表中的元素都被正确地移动到了新的数组中。然后，我们可以遍历自由表来验证所有不在新的数组中的元素都被正确地移动到了新的数组的末尾。</p><p></p><p>2.时间复杂性：这个过程的时间复杂性是O(n)，因为我们只遍历了链表L和自由表F各一次。</p><p></p><p>3.空间复杂性：这个过程的空间复杂性是O(n)，因为我们创建了一个新的数组来存储链表中的元素。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d47cfb2551c0b45c3ea26e07a14995c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15a23879d52d0e195aebf0be7563b47e.png" /></p><p></p><h2>天工：</h2><p></p><p>下面是用 Go 语言实现的 COMPACTIFY-LIST 过程：</p><p></p><p><code lang="text">func COMPACTIFY-LIST(L *List, F *FreeList) {
    // 遍历链表 L 中的每个节点
    cur := L.head
    for cur != nil {
        // 如果当前节点不在数组中
        if cur.key &gt;= n {
            // 将当前节点从链表中删除
            cur = cur.next
            continue
        }

        // 如果当前节点在数组中
        if cur.key &lt; n {
            // 将当前节点移动到数组中下一个空闲位置
            prev.next = cur.next
            cur.next = next[cur.key]
            next[cur.key] = cur
            cur = cur.next
        } else {
            // 如果当前节点已经在数组中，则将它移动到链表头部
            prev.next = cur.next
            cur.next = L.head
            L.head = cur
            cur = cur.next
        }
    }

    // 遍历自由表 F 中的每个节点
    cur = F.head
    for cur != nil {
        // 如果当前节点在数组中
        if cur.key &gt;= n {
            // 将自由表中的节点移动到数组中下一个空闲位置
            prev.next = cur.next
            cur.next = next[cur.key]
            next[cur.key] = cur
            cur = cur.next
        } else {
            // 如果当前节点已经在数组中，则将它移动到链表头部
            prev.next = cur.next
            cur.next = L.head
            L.head = cur
            cur = cur.next
        }
    }

    // 更新数组 next 中的指针
    for i := 1; i &lt;= n; i++ {
        if next[i] != 0 {
            prev.next = next[i]
            next[i] = i
        }
    }
}
</code></p><p></p><p>首先遍历链表 L 中的每个节点，如果节点不在数组中，则将其从链表中删除；如果节点在数组中，则将其移到数组中下一个空闲位置或链表头部。然后遍历自由表 F 中的每个节点，如果节点在数组中，则将其移动到数组中下一个空闲位置或链表头部。最后更新数组 next 中的指针。</p><p></p><p>这个过程的时间复杂度为 O(n)，因为只需要遍历链表 L 和自由表 F 中的节点一次。同时，该过程只使用了固定量的额外存储空间，因为它只需要更新数组 next 中的指针。</p><p></p><p>因此，我们可以证明 COMPACTIFY-LIST 过程是正确的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6651cb3eb40d28ef98f3d2cf1b72ca7.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e7c69d213388509c1c71720a9773956.png" /></p><p></p><h2>chatgpt：</h2><p></p><p>下面是使用Go语言实现的COMPACTIFY-LIST过程：</p><p></p><p><code lang="go">type Node struct {
    key  int
    prev *Node
 next *Node
}

func COMPACTIFY_LIST(L *Node, F *Node) {
    n := 0
    p := L
    for p != nil {
        p.key = n + 1
        F.prev = p
        p.next = F
        F = p
        p = p.next
        n++
    }
    F.prev = nil
    F = F.prev
    for n &lt; m {
        F.key = n + 1
        F.next = nil
        F.prev = nil
        F = F.next
        n++
    }
}
</code></p><p></p><p>在这个过程中，我们使用了两个指针p和F，分别指向链表L和自由表F的头部。首先，我们遍历链表L，将每个节点的key值设置为n+1，然后将节点插入自由表F的头部，并更新指针。在遍历过程中，我们还记录链表L的长度n。</p><p></p><p>接下来，我们将自由表F的头部指针指向nil，并将指针F移动到自由表F的尾部。然后，我们使用一个循环来将自由表F中剩余的节点的key值设置为n+1，并将它们从自由表F中移除，同时更新指针和n的值。</p><p></p><p>这个过程的时间复杂度为O(n)，因为我们只需要遍历链表L一次，并且在每个节点上执行固定数量的操作。另外，这个过程只使用了固定量的额外存储空间，即两个指针变量。</p><p></p><p>为了证明这个过程的正确性，我们可以观察到，在遍历链表L的过程中，我们将每个节点移动到数组的前n个位置，并且将其key值设置为正确的值。同时，我们将每个节点从自由表F中移除，并将其插入到链表L的前面。因此，最终链表L中的节点将占据数组中1到n的位置，而自由表F中的节点将占据数组中n+1到m的位置。</p><p></p><p>因此，我们可以得出结论，经过COMPACTIFY-LIST过程后，链表L和自由表F的元素将占据数组的正确位置，并且过程的时间复杂度和空间复杂度都满足要求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a243bfcaa6cc4057808f0e7739070353.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07b11ff1ecb464617342a3818dc62c2e.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK</id>
            <title>美的集团最新AI实践：拟上线智能家居大模型，开源边端AI算法部署工具链</title>
            <link>https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 09:58:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化时代, 人工智能, 美的集团, AI大模型
<br>
<br>
总结: 数字化时代，人工智能已成为各行各业的关键技术。美的集团在探索将AI融入业务中，并投入大模型技术。他们发布了“美的家居大脑”智能服务引擎，搭载了国内首个家居领域AI大模型“美言”，实现了全屋空气、智慧烹饪、智慧洗护、智慧能源等业务系统的智能感知和自主决策。此外，美的还在工厂环境中推广视觉大模型应用，提升了质检和生产效率。他们通过开源开放的方式与开发者共创AI生态系统。 </div>
                        <hr>
                    
                    <p>数字化时代，人工智能（AI）已经成为各行各业的关键技术。从智能家居到制造流程的优化，<a href="https://www.infoq.cn/news/pQu2eI7W4G1QRlqzrZYT">美的集团</a>"一直在探索将 AI 融入其业务中。并且，针对时兴的大模型技术，美的集团也已经雄心勃勃地投入其中。</p><p></p><p>最近，InfoQ与美的集团的多位专家进行了交流，以了解美的在C端和B端领域的最新AI应用进展。他们还分享了通过开源开放的方式、与开发者一同推动 AI 生态系统共创的愿景。</p><p></p><p></p><h2>“美言大模型”2024年上线</h2><p></p><p></p><p>在美的集团本月举办的年度科技月活动上，美的AI创新中心发布了“美的家居大脑”（Midea’s Home Brain）智能主动服务引擎。</p><p></p><p>据美的集团首席AI官兼AI创新中心总经理唐剑介绍，“美的家居大脑”搭载国内首个家居领域 AI 大模型“美言”，具备智能感知、自然交互、自主决策等能力，支持全屋空气、智慧烹饪、智慧洗护、智慧能源等8大业务系统，几乎覆盖生活的方方面面，最关键的是，它能够通过家电以及机器人把最重要的两个空间——数字空间和物理空间深度融合在一起。</p><p></p><p>唐剑在接受InfoQ采访时表示，美的一直聚焦自然语言交互系统的探索，去年年底随着ChatGPT大模型的爆发，内部就立即将其纳入了紧密追踪的范围。</p><p></p><p>谈及美言大模型的研发过程中面临的挑战，唐剑表示主要面临着两大行业难题，一是幻觉问题（AI Hallucinations）。幻觉问题在大语言模型和深度学习模型中是一个公认的挑战，大模型在生成文本、图像或其他内容时给出的信息虽然看起来合理，但实际上是错误的或无关的。</p><p></p><p>与通用大模型不同，在用于闲聊或文本、图像生成的情况下，它们给出来的内容通常需要给人工进一步修改。但美的的应用需要直接与用户互动，因此必须确保大模型的回答和执行操作都非常精准和可控。</p><p></p><p>另一大挑战则是灾难式遗忘（catastrophic forgetting）问题，AI模型在学习新知识时，会丧失或混淆先前学到的知识，尤其是在应对大规模数据和复杂任务时。唐剑强调，特别是在开发垂直领域的大模型时，尤其需要解决这个挑战。</p><p></p><p>据悉，美言大模型已在上个月发布，但距离正式上线还需要经过网信办的报备过程，目前正在走申报流程，计划在明年上半年正式上线，并且会持续对模型进行升级和迭代。</p><p></p><p></p><h2>AI 深入制造生产</h2><p></p><p></p><p>在toC层面，美的通过家居大脑全屋智能以及美言大模型提升交互和产品的感知能力。toB层面，AI在美的智能制造板块也大有所为。</p><p></p><p>“除了语言大模型，还有视觉大模型等等。”唐剑介绍道，美的在工厂环境中积极推广视觉大模型应用，这包括工厂内的一系列AI自动质检。他举例道，“360度动态全检”质检方法利用摄像头在转盘上对产品进行全方位扫描，检查外观是否存在缺陷、裂纹，以及是否正确贴有logo等。再比如“连续动作的识别”，这一应用能准确识别那些微小、肉眼难以判断的动作，通过视觉大模型的技术，可以提高识别检测的精准度，以确保工人遵循标准操作程序（SOP）。</p><p></p><p>美的集团EDP（企业数字平台）产品经理陈婷在受访时也表示，引入AI技术后，缺漏检、质量检、瑕疵检等以前由人工进行的检测如今都可以通过摄像头拍摄图像，上传到云端进行识别和判断。</p><p></p><p>在人员管理方面，美的主要应用了AI人脸识别技术，陈婷表示体现在产线生产现场主要关键应用是技能匹配，在生产产线的关键岗位上安装了设备，定期捕获工作岗位上工作人员的面部信息，将其上传到系统进行匹配。系统的任务是判断员工的技能是否与岗位需求相匹配。如果系统检测到技能不匹配，会立即触发产线的停工，引起品质管理人员的警觉，以便进行巡检和检查。这一举措旨在降低技能不匹配可能引发的产品质量风险。</p><p></p><p>美的中央研究院感知技术研究所所长奚伟在接受InfoQ采访时进一步提到，在自动化生产方面，美的通过利用机器视觉技术，可以实现机器人在搬运、焊接和装配等任务中的自动化，从而提高制造效率。美的已经在过去两年多的时间内部署了100多套智能设备，预计在未来几年将进一步扩展至1000套。</p><p></p><p>此外，除了智能制造领域，在智能医疗领域，美的也在运用深度学习技术来提升磁共振成像设备的成像技术。</p><p></p><p>据了解，美的集团在过去5年的投入研发资金超500亿元，其中2022年研发投入超120亿元，2023年预计超130亿元，截至目前在全球10余个国家拥有31个研发中心和40个主要生产基地。正如美的集团董事长兼总裁方洪波在本届科技月上所强调，“科技领先战略一定要放在核心地位，美的要继续加大研发投入。”</p><p></p><p></p><h2>与开发者推进AI生态共创</h2><p></p><p></p><p>各个 AI+制造实践的背后，离不开 AI 算法的作用。不过，由于AI的计算成本和存储成本相比于传统任务要高很多，以至于很多 AI 在低资源设备上落地部署非常困难。面对这一挑战，美的集团AI创新中心自主研发了边端AI算法部署工具链Aidget（AI edge toolbox）。</p><p></p><p>这是美的在2021年启动的项目，Aidget工具链主要包括深度学习边端推理引擎和模型自动化压缩工具。它适用于Linux、DSP、RTOS、Android和iOS等，支持多种边端平台。</p><p></p><p>美的 AI 创新中心算法工程师赵东宇向InfoQ进一步介绍道，Aidget工具链的核心工作是将模型通过压缩工具进行模型压缩，支持前沿的主流算法技术修枝、蒸馏及量化。通过多种压缩方式结合运用，实现高压缩比并保持良好的精度性能。</p><p></p><p>经过压缩后的模型通过推理引擎侧转换工具进行解析，进行进一步优化，包括算子清除、算子替换、布局调整等图优化操作。推理时加载模型，然后经过预处理把模型从抽象的图转换为具体的算子和张量连接的描述形式，并进行内存复用和执行调度等运行优化。</p><p></p><p>值得一提的是，Aidget已经在<a href="https://github.com/midea-ai/Aidget">Github</a>"和<a href="https://openi.pcl.ac.cn/midea-ai/Aidget">OpenI</a>"两大社区同步上线开放。“推进开源开放是美的集团承担建设的智能服务机器人国家新一代人工智能开放创新平台的重要内容，Aidget将有助于美的集团与开发者推进AI生态共创。”赵东宇说道。</p><p></p><p>现阶段，Aidget已正式应用于美的集团各业务部门的不同项目的AI部署业务中，支持冰箱、家庭服务机器人等智能家居产品的产业化批量生产。同时，Aidget参与研发的FL119项目，正在联合空调、扫地机、洗衣机产品开发中，大幅降低了芯片平台成本，使有限的计算资源能够支持更多AI任务的实现。在大模型端侧的部署方面，Aidget也有取得进展，减少部署成本并大幅提高效率。</p><p></p><p>在赵东宇看来， AI 不能只出现在论文和PPT上，而应该让人们在生活中切实感受到AI带来的便利，将 AI 落地应用于千家万户。</p><p></p><p>除了Aidget，未来美的集团将深入推进语音识别技术、机器视觉和智能机器人方面的优化算法/模型，并将这些技术对外开放。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vHWjuaI4FLJgg9DyKE7p</id>
            <title>从程序员到科技领导者，如何“打怪升级”</title>
            <link>https://www.infoq.cn/article/vHWjuaI4FLJgg9DyKE7p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vHWjuaI4FLJgg9DyKE7p</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 07:49:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员日, 技术趋势, ChatGPT, AIGC
<br>
<br>
总结: 2023年是一个技术交替的一年，ChatGPT和AIGC的发展带来了新的技术变量，影响了程序员的日常工作。同时，旧技术也在焕发新的活力。这些技术的变化促进了架构设计的变化。此外，程序员的职业要求也发生了变化，包括Prompt工程师和架构师的角色。在这个变化的时代，程序员需要不断学习和适应新的技术趋势。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>又是一年“1024”程序员日，今天的你有没有趁机让自己放松一下？&nbsp;程序员这个职业并不简单，在做好自身工作的同时，还要关注技术趋势、不断提升职业能力等。值此时机，InfoQ编辑部就大家关心的一些问题进行了总结，以飨广大开发者。</blockquote><p></p><p>&nbsp;</p><p></p><h2>新旧技术交替的一年</h2><p></p><p>&nbsp;</p><p>2023年，技术圈算得上“热闹”。</p><p>&nbsp;</p><p>ChatGPT带来了新的技术变量，进而影响到了程序员的日常，我们一下子就迈入了由AIGC 引领的时代。AIGC 的发展又直接推动了向量数据库、研发效能、大数据等多个领域的发展。这场 ChatGPT 引发的“蝴蝶效应”正在席卷全球。</p><p>&nbsp;</p><p>这场竞赛里，已经有人走在前边：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/uz6JCNZ2lvRCT2psCvS6">InfoQ&nbsp;2023年趋势报告：数据工程、AI和ML</a>"<a href="https://www.infoq.cn/theme/209">AIGC&nbsp;时代我们需要什么样的向量数据库</a>"<a href="https://www.infoq.cn/theme/215">AIGC&nbsp;如何加持研发效能提升</a>"<a href="https://www.infoq.cn/theme/214">LLM在大数据领域会掀起什么样的风浪</a>"……</p><p>&nbsp;</p><p>与此同时，“旧”技术也在焕发新的活力。备受期待的Serverless 开始在更多企业落地；平台工程异军突起，要不要和该如何设计平台工程成为广大开发者关注的问题；可观测技术悄然掀起一场“无声的平台革命”，eBPF 被推到前沿；云原生网关也开启新的竞技……</p><p>&nbsp;</p><p>种种技术更迭也促进了架构设计的变化：大语言模型和软件供应链安全是新的创新者趋势，“可移植性设计”、数据驱动架构等日渐成熟。这一年，技术并为因为各种原因而停滞不前。</p><p>&nbsp;</p><p>查看各技术领域的详细发展情况：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/theme/182">企业转向Serverless，怎么做技术选型？</a>"<a href="https://www.infoq.cn/theme/183">云原生趋势下的平台工程</a>"<a href="https://www.infoq.cn/theme/181">eBPF技术探索与应用实践</a>"<a href="https://www.infoq.cn/theme/156">下一代云原生网关选型与实践</a>"<a href="https://www.infoq.cn/article/GfGMc5ozQO1XYOiaTXCz">InfoQ&nbsp;2023&nbsp;年趋势报告：架构设计</a>"<a href="https://www.infoq.cn/article/fvb5Y9egEbd3BNvGRoBw">InfoQ&nbsp;2023&nbsp;年趋势报告：文化和方法</a>"……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>程序员的职业要求，也变了</h2><p></p><p>&nbsp;</p><p>人才是驱动社会和经济发展的核心要素，是企业创新的基石。技术的不断演变，也促使开发者们不断提升自己的“硬技能”。</p><p>&nbsp;</p><p>“大模型能力的成熟，就像突然之间，我们手中拥有了一架强大的机器。但机器上的按钮很多，且没有说明书，需要一个同学来探索、归纳、总结其使用方法。”创客贴 CTO 李晋松说道。而承担这样探索角色的就是“Prompt 工程师”。看起来简单，但这个岗位极度务实，对技术要求并不低。同时，AIGC 工具的使用也在影响开发者的生产力和工作流程。用还是不用，现在还是个问题吗？</p><p>&nbsp;</p><p>另外，架构师的角色也已经发生了根本性的变化。与用户互动的方式，构建、发布和支持软件的方式都发生了变化。即使同一岗位，对开发者的要求也在不断变化。</p><p>&nbsp;</p><p>详解职业要求如何变化：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/7FGafFFFYkbMxW11u0BT">实访用人单位：Prompt工程师真是低门槛“香饽饽”？</a>"<a href="https://www.infoq.cn/article/39DAkIPX97e716HJkPIa">AIGC&nbsp;生成代码正流行，对程序员是好还是坏？</a>"<a href="https://www.infoq.cn/article/jpSYVMHY71wO6wchb71x">架构师角色的演变</a>"<a href="https://www.infoq.cn/theme/196">产业人才趋势洞察</a>"……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>这些人在路上</h2><p></p><p>&nbsp;</p><p>前人还在路上，后来者更要努力。</p><p>&nbsp;</p><p>在Thoughtworks工作超过 17 年的微服务先行者James Lewis、20 多年来一直倡导敏捷开发、高效开发的《代码大全》作者 Steve McConnell、 从事研发20多年的“MySQL 之父” Michael “Monty” Widenius 至今还活跃在开发前线，并时不时向大家分享自己的人生经验。而新时代下的95后，也在沿着自己的轨道前行……</p><p>&nbsp;</p><p>看看他们的精彩故事：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/8wWUiBa8eBWVLRMrxJaT">我，一个&nbsp;95&nbsp;后，从阿里辞职与贾扬清去硅谷创业</a>"<a href="https://www.infoq.cn/article/h1BYRQgH4zZqhLPVZArS">微服务先行者&nbsp;James&nbsp;Lewis：别纠结单体还是微服务，面向服务才是正解</a>"<a href="https://www.infoq.cn/article/45V2FCGDxm9YYWf8oWIX">《代码大全》作者&nbsp;Steve&nbsp;McConnell&nbsp;传授管理软件复杂度的要义</a>"<a href="https://www.infoq.cn/article/3xtSDtHUgTKRsyw3kZXH">专访“MySQL&nbsp;之父”：我曾创造&nbsp;MySQL，也将颠覆&nbsp;MySQL</a>"……</p><p>&nbsp;</p><p></p><h2>嘿，别再用“格子衫”定义我们了！</h2><p></p><p>&nbsp;</p><p>穿着“格子衫”闷头写代码、“宅”在家里打游戏，这些标签已经把程序员塑造成了“机器”。但是，程序员的日常生活真的也很丰富多彩！</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/video/ANUI0bYEKxlFzQdTWK04">大厂技术团队讲述：我的coding仪式感、第二理想……</a>"<a href="https://www.infoq.cn/video/quHs8XqTeyIQcsvJqNIQ">技术团队刻板、太讲逻辑？这些团队的日常可不这样！</a>"<a href="https://www.infoq.cn/video/igLvz42RLa4o4DgVUKc9">AI入侵和被“毕业”、小酒馆和拜佛……程序员的“悲喜”一年｜1024程序员日特别篇</a>"……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>压力大，更要爱自己</h2><p></p><p>&nbsp;</p><p>开发者常常会被认为是“i人”，但其实同侪交流学习、企业培训分享，都是大家认可的学习和成长方式。</p><p>&nbsp;</p><p>开发者压力大，脱发、颈椎等身体健康问题向来会被关注，但心理健康或许更该被重视。根据<a href="http://gk.link/a/12bXU">《中国开发者画像洞察报告2022》</a>"调查结果，54.09% 开发者拥有运动休闲爱好，且超过 60% 开发者认为自己身体整体健康。但超过一半的开发者认为生活中充满了压力，并且有 1.67% 的开发者已经作为心理疾病患者接受治疗。</p><p>&nbsp;</p><p>或许在新的一年，我们都要学着在繁忙的工作中给自己解压。</p><p>&nbsp;</p><p>查看更多开发者报告：</p><p><a href="http://gk.link/a/12bXU">中国开发者画像洞察报告2022</a>"<a href="http://gk.link/a/12bXY">中国科技领导者画像研究报告2023</a>"<a href="http://gk.link/a/12bXT">2023中国企业数字化人才发展白皮书</a>"……</p><p></p><p>最后，2023年第三季《中国卓越技术团队访谈录》将于11月上旬发布，现在可以提前点击<a href="https://www.infoq.cn/form/?id=1881">“预约”免费下载电子书</a>"啦！同时，我们还会在电子书发布前，随机抽取幸运的预约读者，送出各类技术书籍、InfoQ 定制马克杯、帆布袋等周边好礼。获奖名单届时将在电子书下载页的评论区公布，现在就可以行动起来啦！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KAMXm0aelsVTRhy9qNBB</id>
            <title>抖音大型直播画质优化实践：“62 亿人次观看的亚运会直播”有哪些创新领域</title>
            <link>https://www.infoq.cn/article/KAMXm0aelsVTRhy9qNBB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KAMXm0aelsVTRhy9qNBB</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚运会, 抖音, 火山引擎, 4K超高清直播
<br>
<br>
总结: 抖音成为亚运会的转播商和直播合作伙伴，通过火山引擎提供的技术服务，在亚运会期间实现了4K超高清直播和交互式观赛体验，为用户带来了全新的观赛感受。团队克服了稳定流畅地实现低延迟直播的挑战，通过测量延迟数据和优化工作，实现了低延迟的播放体验。团队采用了FLV和RTM方案，配合延迟追赶播控策略，实现了延迟下探和播放速度的平衡。 </div>
                        <hr>
                    
                    <p><a href="https://xie.infoq.cn/article/1116539fe8f3b74b9ba3a908e?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">亚运会</a>"、大运会、中国网球公开赛、上海网球大师赛……2023 年可谓是“体育大年”。在拿下世界杯转播的版权后，抖音这次又成为了亚运会持权转播商、中央广播电视总台直播合作伙伴，对亚运会比赛全程进行 4K 超高清直播，并支持回放。在“转播”的背后，火山引擎作为抖音亚运会直播背后的主力技术服务商，在整个赛事转播过程中通过自研 BVC 编码器、画质优化、超低延时等视频云技术和 VR 观赛等互动玩法，帮助用户实现了更高清、更交互、更沉浸的观赛震撼感，切实打造了新一代观赛新体验。</p><p></p><h2>一、极致的直播低延迟体验勇攀「领域新峰」</h2><p></p><p></p><p>亚运会期间的抖音直播拥有亿万级的流量，在这样大规模并发下，火山引擎视频云想要提供 <a href="https://www.infoq.cn/article/gJRYgXV0Mqj2DUugA6GU?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">4K</a>" 超高清超低延迟看播能力，首先要克服的就是“稳定流畅地做到更低延迟”这样的巨大挑战。</p><p></p><p>其实从亚运会直播的整个信号分发链路我们就能看出各个节点可能会出现延迟的概率有多么大。生产环节延迟其实主要来源于两部分，其一是信号源，网络流信号源在给到抖音之前存在多个环节，每个环节都可能会对最终的延迟有影响，但这一部分技术团队可以影响的比较少；其二是演播室制作环节，演播室在收到媒体的源流之后，需要加上解说和包装，会引入一定的延迟。在直播的传输环节里，对延迟影响大的主要是转码、分发和播放缓冲和 <a href="https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng%3D%3D&amp;chksm=eba41b8ddcd3929b1ef44a0d2fe62ea8267643177df6c50ceeeb512e4e7865afc303b35c1de1&amp;idx=1&amp;mid=2247489552&amp;scene=27&amp;sn=5c828588202090f214d467c8bb7584f1&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">CDN </a>"的分发环节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3664875ca36140151077bb4a08e9662.png" /></p><p>图：信号分发链路</p><p></p><p>为此，在低延迟方面，火山引擎视频云团队（下文称“团队”）做了许多攻坚和优化工作。在攻坚之前，最主要的是测量出有效的全链路延迟数据，找到最优的测量方法——而团队在经过多次探索后，最终采取了以下两个方法（仅适用演播室推流到抖音播放链路）：</p><p></p><p>方法一（拍照）：视频画面中有时钟展示，通过同时拍照两个播放画面的方式，记录同一时刻两个画面，然后通过照片中的时钟做差来计算；</p><p></p><p>方法二（手动秒表计算）：视频画面中无时钟相关内容，则从延迟低的视频画面中选取具有标志性易识别的帧启动秒表，然后观察延迟高的画面出现同样的帧画面时，停止秒表，记录秒表结果为延迟对比结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3855fe7d4ec33d96f543a9d2a80e1d4.png" /></p><p>图：延迟测量手册</p><p></p><p>在有了衡量延迟的基准线后，团队便展开了“低延迟”攻坚工作。这次亚运会的转播，抖音的多个演播室是由多家第三方公司负责的，第三方公司的制作规格不一。虽然在正式比赛之前经过大量的沟通，基本确认最重要的两个演播室的技术方案和使用的编码系统是一致的，但是在演播室环节仍然引入秒级别的延迟，和供应商工程师沟通后，短期内为了保证稳定，没有再进一步压缩，这部分引入的延迟和行业同类产品也是一致的。同时，使用实时的转码模式，转码器引入的延迟一般在几百毫秒以内甚至更短。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f393d73be50c77e5127e6fba86e00dc4.png" /></p><p></p><p>图：一次直播的简化的流程</p><p></p><p>在整个项目中，团队主要采用了两个低延时直播方案——FLV 方案和 RTM 方案。关于 FLV 方案，FLV 是现在国内主流直播播放使用的协议，为了满足亚运会转播需求，团队在世界杯赛事转播过程中已经验证过的 FLV-3s 方案和基于基于 FLV 方案做更低延迟下探的基础上，配合输出了精细的追帧、丢帧策略。</p><p></p><p>要知道，播放器音视频数据流转时序一般是“网络 IO 下载音视频数据到播放器缓存 buffer→解码器从 buffer 中取数据解码并降解码后的数据存入待播放缓存→音画同步等播控策略→渲染播放音视频帧”。由于进一步下探延迟，卡顿也会随之恶化，反而延迟逐渐累积增加达不到低延迟的效果，因此延迟下探必须配合延迟追赶播控策略来确保延迟增大后可及时追赶恢复到低延迟。于是团队在亚运会项目上总结出了一套兼顾延迟与 QoE 指标平衡的播控策略：</p><p>输入：播放器当前 Buffer 时长、历史 Ns 内 buffer 抖动、历史 Ns 内卡顿信息以及追帧参数配置。输出：基于 buffer 抖动 &amp; 历史卡顿信息，来定性衡量网络质量，判断是否可以追赶，只有在网络质量良好时才能触发追赶逻辑避免卡顿；同时追帧采用双阈值，并且支持可配置，可以控制追帧持续时长不超过 Ks，同时也可以保证不频繁变速。此外，追帧速度可配置，保证倍速变化不超过一定辐度。三管齐下，达到了目标播放速度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d65fe1d78afd1c720f3241ae74d4186d.png" /></p><p></p><p>图：策略可配置参数以及含义映射</p><p></p><p>关于 RTM 方案，参考了 WebRTC，可以让端到端延迟直接进入 1s 以内，RTM 优化的目标是在延迟降低的情况下，用户核心体验指标对齐或者优于大盘的 FLV 方案。在世界杯的多场比赛中，RTM 方案也承担了一定量级的 CDN 容量，核心键指标上都对齐了大盘，稳定性和质量得到了充分的验证。</p><p></p><p>首先，为了让 RTM 的综合指标对齐 FLV，从若干角度来进行 RTM 的播控逻辑定制化，于是所有的优化都是围绕着核心用户体验指标进行展开：</p><p>DNS 节点优选、SDK 信令预加载、UDP 连通性预探测主要解决的拉流成功率相关问题。SDP 信令相关优化主要解决信令时间消耗的问题（首帧时间）与成功率问题。RTC 内核播控定制化主要解决播放的卡顿问题。播放器播控逻辑结合解决的音画同步与渲染策略的问题。</p><p></p><p>其次，团队优化了“首帧时间”。我们都知道，传统的 RTC 技术采用 SDP 信令方式进行媒体能力协商，但是 HTTP SDP 信令交互存在许多弊端，比如弱网环境下，HTTP 信令建联成功率不理想；导致播放请求响应缓慢或超时；又比如，SDP 交互传输 SDP 文本的内容很大、建联的成本较高，初始化的成本无法忍受。所以相较于 FLV 的 HTTP 请求完成后直接完成建联和媒体数据直接传输，我们可以采用新的信令模式——MiniSDP 信令。这种基于二进制编码的压缩协议，提供对标准 SDP 协议进行压缩处理，可以降低信令交互时间，提高网络传输效能，降低直播拉流首帧渲染时间，提高拉流秒开率 / 成功率等 QoS 统计指标。利用 UDP 网络传输的 MiniSDP 压缩信令方式，单个 UDP 数据包请求即可完成 SDP 完整压缩信息的传输，信令建联的成功率和首帧时间可得到大幅优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/7659a2a0019f1fc264b3509134ed2ac2.png" /></p><p>图：采用 MiniSDP 信令进行媒体协商通信的信令交互流程</p><p></p><p>第三，经过线上的 AB 实验，团队发现 RTM 拉流成功率相比 FLV 持续存在着一定的差距，经过分析，大家发现用户的网络等级质量和用户的拉流成功率存在一定的正相关性。于是，团队拉流网络等级筛选基于网络质量预估信息，评估 TCP/UDP RTT 和数据下行吞吐率，为用户确定网络等级，选择优质网络质量的用户采用 RTM 拉流以降低失败率。在拉流前，根据用户请求的 URL 所属的 CDN 边缘节点，发起 UDP 探测。在一段时间内发送数据包观察对应 CDN 节点的 RTT 和丢包率，只有满足 RTT 和丢包率两者在特定的阈值范围内才会认为 UDP 传输可以保证质量和组帧成功率。同时，通过信令预加载，在当前的点播 / 直播房间中预先加载下一个直播间的信令信息，提前做好 SDP 加载，降低下一个房间的首帧上屏时间。就这样，团队完成了“拉流成功率”的优化。</p><p></p><p>第四，团队完成了“卡顿”、“播控逻辑”的优化。团队通过对比 FLV 和 RTM 的播控策略，发现传统的 RTC 场景优先保时延，全链路会触发各种丢帧，FLV 直播场景会优先保证“不丢帧、良好的音画同步”的良好观播体验。那 RTM 要想减少卡顿，取得 QoE 的收益，播控策略就需要进行定制化。在播控逻辑方面，团队完成了以下优化：</p><p>RTM 网络传输 SDK 的抽象：将内核进行改造，复用引擎中的网络传输 - 组包 -JitterBuffer/NetEQ 模块；去掉解码 / 渲染等模块；将音视频的裸数据抛出供播放器 demuxer 集成。解码器复用：降低解码器重新初始化的时间，降低解码首帧延时；复用解码器 - 渲染器的播放缓冲区控速逻辑。音画同步的优化：RTC 音视频出帧后在播放器侧按照 FLV 的播控逻辑进行二次音画同步处理；按照 audio master clock 主时钟进行渲染校准，视频帧渲染同步到音频时间轴上。</p><p></p><p>此外，本届亚运会超高清档位的分辨率达到了 4K，对 RTM 方案的性能带来了很大的挑战，但团队都很好地解决了。比如 4K 高清档位卡顿严重卡顿的问题，团队优化了 NACK 策略，保证了更大帧的组帧成功率；又比如针对 CPU/GPU 内存问题，团队优化了 video 传输 pipeline，减少了不必要的 raw 数据格式转换。</p><p></p><p>在团队的支持下，在亚运会的转播过程中，抖音的延迟一直领先于相同信号源的其它产品 30s 左右。这也看出了团队的强悍之处，能够根据具体业务需求和技术挑战不断完成自我迭代。目前，火山引擎视频云在 FLV、RTM 、切片类协议的延迟优化、XR 直播的延迟优化等方面已经有了较为完整的迭代方向。</p><p></p><p></p><h2>二、新一代 BVC 编码器成功实现「降本增效」</h2><p></p><p></p><p>视频基础体验的关键要素包括清晰度、流畅度、低延迟等，而视频编码是整个技术体系的基座，编码效率的显著提升能够在同等码率下极大提高画质以改善用户体验。但编码效率的提升并非易事。如何在保证画质不变的情况下，显著提高压缩率，同时满足实时性、低延迟的要求，是当下持续性的技术挑战。</p><p></p><p>在世界杯的转播过程中，抖音基于 BVC 编码器给数亿观众带来了极致的视频体验，而本届亚运会中，抖音首次采用了团队自研的新一代编码器 BVC。相比上一代编码器 BVC，新一代编码器 BVC 引入了大量新编码工具和算法，具有低码率高计算复杂度特点，在本届亚运会转播过程中，新一代编码器 BVC 实现了 1080P+50FPS 的实时编码，在画质不变的情况下，相比上一代编码器 BVC 实现了 20% 左右的码率节省，提升了用户体验，降低了带宽成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/165494777135a38845dfa1f744794359.png" /></p><p>图：BVC 编码视频与新一代编码器 BVC 编码视频对比</p><p></p><p>据悉，新一代编码器 BVC 拥有简洁的工程架构，团队测试了直播场景下的所有编码工具和算法，筛选出了性价比高的工具和算法集合，并基于这个集合重新了设计轻量级的架构，其能最大化减少计算流程损耗。新的编码器架构对整个编码流程进行了重新梳理，去除原先复杂的情况耦合，为特殊工具单独设计流程，实现了编码流程的最简化。在算法基本不变的情况下，为新一代编码器 BVC 节省了超过 30% 的复杂度。同时，新一代编码器 BVC 针对直播场景进行了并行框架的重新设计，优化后的新一代编码器 BVC 在 CPU 利用率方面相比上一代编码器提升 50% 以上。</p><p></p><p>除了工程架构外，新一代编码器 BVC 还增加了大量的快速算法，以达到高分辨率 + 高码率 + 高帧率的实时编码。其还重构了编码块划分的框架，根据周围块和历史划分划分信息，灵活决定自上而下或者自下而上的划分顺序，并自适应决策划分深度的尝试方向，大幅减少了无效的划分尝试，降低了编码复杂度。新一代编码器 BVC 为直播场景增加的上百个快速算法，将整体编码速度提高了 2 倍以上，同时压缩率的损失在 5% 以内。</p><p></p><p>因为本届亚运会还有电竞项目，所以新一代编码器 BVC 便分别针对运动、游戏这两种场景进行了优化。新一代编码器 BVC 开发团队调整了数十个编码参数来控制不同编码算法在运动、游戏场景中的性价比，在获得压缩率提高的同时实现了编码加速。此外，还对码率控制进行了调优，减少了高运动场景中画面模糊的情况。</p><p></p><h3>三、增强插帧多项技术绘制「画质美学」</h3><p></p><p></p><p>对于媒体平台转播来说，不同赛事节目涉及链路众多，且不同赛事之间存在差异，如何保障各链路的画质稳定并进一步提升画质是一个巨大的挑战。大型赛事直播涉及链路较长，不同赛事链路存在一些差异，但基本都是现场信号经过演播室的制作传输给 CDN 再进一步分发到用户侧。从画质角度来看，整个链路可分为画质检测与画质优化两个部分，对于 CDN 之前的链路以画质监测为主，以发现问题 / 定位问题 / 推动对应链路人员解决问题为目的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd0e4400056c0442a96f61be1cc46a22.png" /></p><p></p><p>随着赛事录制技术的提升，越来越多的大型赛事都用上了 4K HDR 录制标准，画质清晰度也不断提升，那随之而来的是更大的带宽压力；而且，为了兼容消费端不同的看播设备和不同的带宽条件，服务端需要转出多种不同分辨率不同码率的版本供看播端选择。</p><p></p><p>相比 SDR 信号，HDR 拍摄的片源拥有更广的色域，更大的动态范围。但对很多终端显示设备而言，并不支持 HDR 信号播放，所以团队通过 ToneMapping 算法将 HDR 信号转换为 SDR（标准动态范围）信号。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/42b4f0f8459a61ba1006d9ca4dda6385.png" /></p><p></p><p>HDR 在转换到 SDR 信号的过程中不可避免地会产生一些信息损失，常用的一些 ToneMapping 方法（如 Reinhard、Filmic、Hable），本质都是设计固定的映射曲线实现从 HDR 信号 到 SDR 信号的转换，同时尽量保持对 HDR 效果的还原。但直播赛事场景多变，且现场动态范围跨度极大，团队便提出了内容自适应 ToneMapping 算法，通过统计视频内容的实际光照情况动态地进行 ToneMapping，从而得到更优效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/6010fd5a3d5d13881386bd942f2c7d50.png" /></p><p>图：左 - 内容自适应 ToneMapping；右 -Hable 算法</p><p></p><p>随着音视频行业和摄影设备的发展，高分辨率的视频源占比日益增多，大部分视频需要在服务端进行降采样来配合自适应码率策略，因此降采样算法的优化也是提升 QoE 的关键。在过去的业界实践中，视频处理算法往往专注于提高分辨率（如超分算法）或者保持分辨率（如降噪算法）的处理范式，而几乎忽视了对降低分辨率方法的研究。所以团队自研了一种基于深度学习的图片 / 视频下采样算法——BAS（Byte AI Scaling）算法。不同于固定算子的 bicubic 等降采样算法，BAS 算法基于深度学习使用高精度数据训练模型，缓解传统方法带来的频域混叠与频域截断问题，降低锯齿感、减少细节丢失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8db66e13d7988f3fb45d6ee840b6191f.png" /></p><p>图：4K 超高清图源降采样到 480p 分辨率，左图 -BAS 算法处理结果，右图 - 传统 bicubic 算法处理结果</p><p></p><p>在与 bicubic 算法的定量对比中，BAS 基于 PSNR 指标取得了 -20.32% 的 BD-Rate 收益，意味着相同重建误差水平下可以节省 20% 以上码率，而同等码率下则可以提升画质水平。而对于更符合人眼感知特性的 VMAF 指标，BAS 同样取得了 -20.89% 的 BD-Rate 收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c14556f77985196366ffa205b2287c3.png" /></p><p></p><p>此外，因为现在消费者已经习惯了高帧率的流畅视频体验，所以针对低帧率场景，团队使用了智能插帧技术，通过对前后帧的内容进行光流估计，根据光流信息将前后帧像素都转换到中间帧，然后进行整合、生成中间帧，提升视频帧率，减少观看时的卡顿感。而针对电竞类对帧率要求较高的场景，该技术团队做了额外优化：</p><p>faster 光流模块和 faster 修正模块使用 部分通道卷积代替普通卷积，在保持效果的同时减少卷积运算；采用推理下采样的方式，对输入进行内容自适应下采样，作为光流模块和修正模块的输入，再将输出上采回原分辨率用于原始输入的 warp 和整合，以达到进一步减少计算量的效果；工程上通过算子融合、半精度的方式减少 IO 和浮点运算，相比工程化前加速 1 倍多；通过多 GPU 部署的方式拓展智能插帧能力，使得视频插帧能在更高分辨率（4k）的场景下能实施部署。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c132a4c43dcb4d2c65e1184b3a32e5ab.png" /></p><p></p><p>在本届亚运会的转播过程中，智能插帧在处理电竞项目场景中复杂运动的“英雄名字”小文字时，通常会因为光流估计不够准确而导致插出来的帧文字的位置不够准确，导致伪像出现，于是团队在训练过程中加入更多的随意移动或者静止的较小文字，使得模型能够在训练过程中更多地注意处理小文字的复杂运动，从而达到更好地插帧效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6908efadd76ff91a4af374f67f7bd86.png" /></p><p>图：左 - 优化后；右 - 优化前</p><p></p><p>另外值得一提的是，为了兼顾视频码率和主观画质，团队使用了基于 LSTM（长短期记忆网络）的时域 ROI 技术，通过人眼显著性区域检测和编码相结合的方式，让码率在画面上的分配更加合理。除了模型设计之外，ROI 算法中另一大难点是 saliency（显著性物体检测）数据的获取，通用的 saliency 数据集在大型赛事中的表现并不理想。针对这一问题，团队收集制作了自己的专用数据集，并且对一些大型赛事做了专用数据集。</p><p></p><p>同时，为了提升低档位、低分辨率的视频清晰度，团队为本届亚运会转播提供了“超分算法”。该算法是一种基于机器学习 / 深度学习方法，根据视频信息对其进行空域、时域建模重构出缺失的细节，将低分辨率的视频重建出高分辨率视频的技术。当用户看播端网速较慢切换到 480P/720P 等低分辨档位时，端上超分算法就会被触发以提升画面清晰度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55e5b20f1d58ef467eb57771f4203abd.png" /></p><p>图：左 - 源流 1080P，右 -720p 超分后</p><p></p><p></p><h2>四、创新的 VR 直播技术实现「沉浸式现场」</h2><p></p><p></p><p>今年杭州亚运会的火炬点燃仪式是历史上首个“数实融合”点火仪式，在观赛方式上，PICO 推出的 PICO4VR 一体机作为一种虚拟现实设备，以其双目 4K 清晰度和 105 度的超大视角，无论是比赛过程还是赛后回放，观众都可以切换到多个不同的观看视角，给观众带来了更加真实的观赛体验。</p><p></p><p>观众佩戴 PICO4VR 一体机观赛，不仅能够通过视觉感受到沉浸感，还可以通过触觉的反馈来增强身临其境的感觉。比如，在颁奖典礼上，观众不仅能看到会场上空绽放的烟花，还可以挥动手中的荧光棒为选手加油助威，同时手柄的马达模拟了充气棒或荧光棒碰撞时的震动感，使观众能够在家中感受到与现场观赛相仿的热情氛围。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efd7971de9d4afdb9058773bdf7988e8.png" /></p><p></p><p>VR 直播的沉浸感以及高交互性是普通直播无法比拟的，但是这也导致了传输层需要承担更大的压力——分辨率为 8K x 4K 或 8K x 8K，源流码率达到 50M 甚至 120M，非常容易因为拥塞导致卡顿、延迟增大，甚至无法正常解码播放。</p><p></p><p>为了解决这个问题，团队将 8K 的视频切分成多个块（tile），只传输用户视角（viewport）内的部分超高清块，其它区域只传输 2K 或 4K 分辨率的缩小后的背景流，在用户切换视角的时候再去重新请求新的超高清块。同时团队基于 UDP 的内容优先级感知传输方案，优先保障高优数据的传输，对于低优数据可选择非可靠传输，即使丢失也无需重传，保证 XR 直播低延迟的同时也杜绝了过大的“视觉失真”。</p><p></p><p>另外值得一提的是，Pico 还在本届亚运会中打造了一个用户专属的赛事社交空间，观众可以邀请远方的好友一起观看比赛，在虚拟空间中共同分享运动的激情。这个过程中，Pico 就是基于火山引擎多媒体实验室沉浸音频与 RTC 空间音效能力，让用户可以随着位置的移动、头部姿势的变化，差异化地感受到节目声音与同场观众的声音对应的空间变化。</p><p></p><p></p><h2>五、写在最后</h2><p></p><p></p><p>在不断升级的观赛体验和极致追求中，视频云技术的每一次细致入微的技术迭代和追求极致的产品突破，无疑为用户带来了不断进化的感官体验，使得赛事直播的极致追求成为可能。通过电视、电脑、手机、平板等各种终端设备，全球亿万观众足不出户，就能第一时间、自由视角观赛，身临其境感受赛场氛围。这是科技与体育赛事的完美结合，也是技术进步为观众带来的独特体验。大家如果想了解更多关于视频云在大型直播过程中的实践细节，可以点击“<a href="https://www.volcengine.com/contact/docs/livebroadcast?utm_campaign=2023&amp;utm_content=baipishu&amp;utm_medium=infoQ&amp;utm_source=event&amp;utm_term=yunshangxinshijie">此处链接</a>"”获取《身临其境 沉浸互动——大型赛事直播实战白皮书》进行阅读。</p><p></p><p>在未来，我们期待看到更多的赛事直播能够借助视频云技术突破物理空间的限制，让观众获得如临现场，甚至超越现场的畅爽体验。当然，我们也非常期待火山引擎视频云能够继续领跑这一趋势，为全球的观众带来更多令人满意的赛事直播体验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l</id>
            <title>1024开发者节｜讯飞星火V3.0发布，面向教育、医疗领域发布AI答疑辅学、讯飞晓医等重磅产品</title>
            <link>https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 05:51:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界声博会, 讯飞星火认知大模型V3.0, AI助手, 大模型算力平台
<br>
<br>
总结: 10月24日，第六届世界声博会暨2023全球1024开发者节开幕，讯飞发布了星火认知大模型V3.0，提升了七大能力，超越了ChatGPT。该模型可以打造个性化的AI助手，并且讯飞还推出了大模型算力平台。这些举措推动了产业生态的繁荣，吸引了大量开发者和企业客户的参与。 </div>
                        <hr>
                    
                    <p>10月24日，第六届世界声博会暨2023全球1024开发者节开幕，产业领袖、专家学者与广大开发者云集于此，共同探讨通用人工智能的最新进展和广阔应用前景。</p><p></p><p>现场科大讯飞董事长刘庆峰、研究院院长刘聪发布讯飞星火认知大模型V3.0，七大能力持续提升，整体超越ChatGPT，医疗领域超越GPT-4。星火3.0升级启发式对话、AI人设功能，能够打造更个性化的AI助手。</p><p>面向应用场景，讯飞星火面向行业进一步升级智能编程助手iFlyCode2.0，面向科研人员推出星火科研助手；面向学生群体，推出AI心理伙伴关爱青少年的心理健康，首次发布启发互动式英语AI答疑辅学并落地讯飞AI学习机中；面向每个家庭，发布讯飞星火医疗大模型，打造每个人的AI健康助手。此外，讯飞机器人超脑平台迎来升级，亮相业界首个大模型+具身智能的人形机器人 。</p><p></p><p>大模型也带动产业生态的繁荣。自5月6日星火发布以来，讯飞开放平台新增143万开发者团队，同比增幅331%，其中新增大模型开发者17.8万，讯飞正与10万+企业客户用星火创新应用新体验。讯飞还联合行业龙头共同发布12个行业大模型，加速产业落地。另一方面，讯飞联合昇腾生态共同发布“飞星一号”大模型算力平台，并启动对标GPT4的更大参数规模的星火大模型训练。2024年上半年将实现对标GPT-4。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/77/9d/77e559a045e56c160b3fe23a630b8e9d.png" /></p><p></p><p>10月24日至29日，科大讯飞还将面向市民开放人工智能产品科博展，观众可以在科技馆、工业馆、行业馆、教育馆、生活馆、生态馆和科学艺术展7个主题展馆，近距离体验星火大模型核心技术、落地产品及生态展品。</p><p></p><h3>一、讯飞星火V3.0发布，整体超越ChatGPT</h3><p></p><p></p><p>认知大模型正开启一个增长新时代，也在成为驱动新型工业化的重要引擎。</p><p></p><p>“大模型正在走进我们每个人的生活。”刘庆峰表示，在今年亚运会上，工作人员借助讯飞星火与中国移动开发的5G新通话助手，刚挂完电话待办事项就已自动生成；在星火iFlyCode帮助下，平台迁移原本需要3个月的开发工作1个月就能完成；讯飞的AI诊疗助手“智医助理”在大模型加持下，将患者“疑似妊娠”的诊断修正为“先兆流产”，挽救了一个新生命；讯飞星火还与国家自然博物馆合作，让300万年前的黄河象“活了”，还能与人对话······</p><p></p><p>讯飞星火，不仅拓展行业边界，也在支持每一位“创客”逐梦科技。今年的全球1024开发者大赛汇聚全球9个国家278座城市3.4万支团队的开发者，年龄最小的13岁，00后选手占比达59%。</p><p></p><p>应用场景和生态的蓬勃发展，依赖核心技术底座的持续进步。从首次发布至今，讯飞星火持续迭代，推出V1.5、V2.0版本。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/68/06/68f50df8d0ea31882c3bde12808d8006.png" /></p><p></p><p>主论坛上，科大讯飞重磅发布星火认知大模型V3.0，七大能力均持续提升，全面对标ChatGPT，中文能力客观评测超越ChatGPT，英文能力对标ChatGPT48项任务结果相当。</p><p></p><p>国务院发展研究中心国研经济研究院测评报告显示，星火大模型V3.0综合能力超越ChatGPT，国内领先，达到国际一流水平，在医疗、法律、教育行业的表现格外突出。</p><p></p><p>星火3.0 进一步升级了数学自动提炼规律、小样本学习、代码项目级理解能力、多模态指令跟随与细节表达等能力，进一步提升星火的落地应用能力。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/9c/9b/9ce735ea89b7a0e6d57b8345d2a59c9b.png" /></p><p></p><p>升级AI人设、启发式对话，打造每个人的AI助手自讯飞星火9月5日全民开放后，当前已有1200万用户，也对认知大模型提出的更高期待，“不仅能回答问题还要能提出问题”、“不仅要有知识还要有个性”。刘庆峰指出，大模型不仅要能回答问题，还要像人一样主动提问。</p><p></p><p>大模型想要赋能民生百业，需要实现从多轮对话、主动对话再到启发式对话的跨越，实现星火的智慧阶跃，推动行业纵深应用。此外大模型不仅要有专业的知识，还要具备个性化的AI人设，为星火注入“灵魂”。对此，讯飞星火大模型V3.0新增了虚拟人格功能，它可以根据性格模拟、情绪理解、表达风格来形成一个初始人设，再结合特定知识学习、对话记忆学习，形成一个更个性化的AI人设。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/1c/c4/1cc129cfdbd44988585e67ff807492c4.png" /></p><p></p><p>当前虚拟人格应用“星火友伴”已上线，无论你想和哪吒聊八卦还是和诸葛亮聊家国大事，只要定义好友伴的“人格”就能和不同人物性格的角色对话，“这不只是聊天，还可以释放灵感”，刘庆峰说。</p><p></p><p>当刘聪点击已创建的“诸葛亮”人格来对话时，诸葛亮的声音和语言风格便展现出来。用户通过“友伴”还能创建自己的人格，刘聪现场创建了个人人格助手，上传个人背景知识，大模型便能用他的风格进行对话。</p><p></p><h3>二、升级编程助手iFlyCode2.0，推出科技文献大模型</h3><p></p><p></p><p>“大模型首先要能够在工业和科研领域造福人类，这才是我们真正解决社会刚需的地方。”刘庆峰称。在工业领域，今年9月20日，基于讯飞星火的羚羊工业大模型正式发布，通用人工智能成为新型工业化的核心引擎。</p><p></p><p>1个月内，羚羊工业互联网平台平台的工业文本生成、知识问答、理解计算、代码生成、多模态5大核心能力再度升级，不到一个月时间平台用户总量提升59%，平台服务企业次数增加88%。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/12/1d/12d141ae68340b8daa42yy34e8249e1d.png" /></p><p></p><p>讯飞星火代码能力超越ChatGPT，iFlyCode再升级全面提效</p><p></p><p>产业数字化和数字产业化都离不开代码能力的支持。数据显示，中国有约800万软件从业人员，2022年软件业务收入10.8万亿元，占GDP比重8.93%，2028年预测将达到20万亿元。</p><p></p><p>刘庆峰表示，自讯飞星火发布以来，代码能力持续升级，星火3.0已基本全面超越ChatGPT，正在追赶GPT-4。</p><p></p><p>搭载讯飞星火V3.0的智能编程助手iFlvCode2.0正式发布，在编程的设计阶段提效50%、开发阶段提效37%、测试阶段提效44%，大幅提升软件从业人员效率。iFlyCode发布以来，已在京东云、软通动力等107家机构实现深度对接应用。今年11月份，几乎全国所有软件名城都会和科大讯飞开展相关合作。</p><p></p><p>联合中科院文献情报中心打造科技文献大模型</p><p></p><p>在科研领域，科技文献是检验大模型高阶认知能力的“试金石”。讯飞星火和与中科院文献情报中心联合打造科技文献大模型，可以实现成果调研、论文研读、学术写作三大功能。</p><p></p><p>刘聪现场实测，用科技文献大模型将十几份量子计算论文一键生成综述报告，为科研人员快速了解最新论文大幅提效。测试显示，学者成果调研的效率被提高超10倍，论文研读有效率和学术写作采纳率均超90%。“以人工智能领域为例，每天有84篇文章发表，每篇阅读10分钟，一天读完也要花14个小时，科技文献大模型快速生成综述，对科研人员快速掌握全球最新前沿进展有巨大帮助”，刘庆峰说。</p><p></p><p>刘庆峰表示，通过科研助手，不光可以提升我们的科学研究效率，还能释放350万在校研究生和博士生的时间，这将极大释放中国未来的想象力。</p><p></p><h3>三、用通用人工智能解决教育、医疗社会刚需</h3><p></p><p></p><p>讯飞AI学习机代差级优势再凸显！首发启发互动式AI英语答疑辅学</p><p></p><p>通用人工智能时代之下，“因材施教”进入第三个层次，从精准识别知识掌握的薄弱环节、到基于知识图谱推荐最近发展区的学习路径、再到类人式对话辅导进行逐层讲解。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/03/fc/03c3558241a386986c10ff9e559ff8fc.png" /></p><p></p><p>发布会上，搭载了星火大模型V3.0 讯飞AI学习机再度升级，首次发布启发互动式英语AI答疑辅学功能，实现个性化解决、启发引导式提问、互动探究式学习。刘聪在现场实测进行多轮提问，无论是探讨语法对错还是理解文章主旨，讯飞AI学习机都能自如回答。</p><p></p><p>根据2023年9月安徽地区七八年级试点用户的统计数据显示，小学英语错题解决率，从答案解析学习的77%到AI答疑辅学的90%，效率提升了13%；初中英语错题解决率：从答案解析学习的61%到AI答疑辅学82%，效率提升了21%。</p><p></p><p>此外，讯飞AI学习机的百科问答助手功能也再次升级，涵盖权威优质百科内容、适龄推荐、启发式提问、百科数字人等功能。</p><p></p><p>“一个国家未来的科技发展，科学精神应该从娃娃抓起”，刘庆峰说，科大讯飞联合科普中国、少年儿童出版社首发科普大模型，助力全民提升科技素养。科普中国拥有40万+科普中国音视频、6000万+科技文献构建科普大模型，将优秀科普资源惠及广大中小学生，及时回应孩子的每一次提问，课堂学生互动参与率从25%提升到77%，问题解答率从23%提升到95%，打通应用最后一公里，讯飞AI学习机将搭载“讯飞星火+专业科普知识库”的科普大模型，帮助青少年在心中播种下科创的种子。</p><p></p><p>据了解，随着讯飞星火认知大模型V3.0的推出，科大讯飞AI学习机上已经拥有了英语口语陪练、中英作文批改、数学互动辅学、百科自由问答、亲子教育助手、智能编程助手、创意绘画伙伴以及英语答疑辅学8款基于大模型的类人辅学功能。</p><p></p><p>“教育不是注满一桶水 而是点燃孩子心中的一把火”，刘庆峰表示，通用人工智能为教育带来全新机遇。</p><p></p><p>科大讯飞发布AI心理伙伴，用星火守护青少年心理健康</p><p></p><p><img src="https://static001.infoq.cn/resource/image/1f/6e/1fa964a87bfab011b222cef7a70f2b6e.png" /></p><p></p><p>科大讯飞一直致力用AI关注青少年的心理健康。发布会上，基于讯飞星火打造的AI心理伙伴正式发布，它具备多模态情感识别、共情表达、寻因式提问、个性化心理指导四大核心能力，善解人意的“小聪”具有极其丰富的心理学知识——10亿+条心理类数据、40万+篇期刊文献、100万+通脱敏心理对话案例、550万+个心理评估数据。</p><p></p><p>根据来自广东、江苏、安徽等21个高中连续两年心理普查的纵向追踪数据平均值显示，同学们的抑郁、冲动、自责、学习焦虑、社交焦虑、孤独等6种心理问题，在使用减压星球后都有下降，如抑郁情绪同比去年减少了8%。</p><p></p><p>AI心理伙伴“小星”不仅可以“听懂”孩子的心事，还能主动回应和给出解决问题的个性化指导建议，和孩子对话接受后，“小星”第一时间生成心理咨询报告，同时将预警信息发给心理老师。“孩子们通常不愿意跟家长和老师敞开心扉，只有通过AI的启发式对话，才有可能真正解决大量孩子的心理减压和疏导问题”。至今，青少年“减压星球”已覆盖3202所学校服务259万中小学生。</p><p></p><p>讯飞星火医疗大模型正式发布，打造每个人的AI健康助手</p><p></p><p><img src="https://static001.infoq.cn/resource/image/6d/86/6dc49d73354c9d0d90129c05f52d1586.png" /></p><p></p><p>发布会上，讯飞星火医疗大模型正式发布。健康和每一个人密切相关，搭载了讯飞星火医疗大模型的“讯飞晓医”APP触手可及，看病之前不知道怎么准确表达自己的病情？自己买药不知道有何禁忌？拿到体检报告后想马上“识别”出自己去年和今年比有什么健康异动？这一切都可以问讯飞晓医。</p><p></p><p>今年5月6日就发布了讯飞星火认知大模型，为什么要到10月24日才发布医疗大模型？刘庆峰表示，因为医疗需要非常严谨对待，早在2017年讯飞智医助理就已经通过国家执业医师资格考试，排名超过96.3%的人类考生，首先给基层医院及等级医院的医生提供帮助，助力成为医生的AI诊疗助理，在这个基础上，今天开始面向家庭，打造每个人的AI健康助手，让AI医疗的普惠之光照进每个家庭。</p><p></p><p>根据统计，全科医生缺口达30万人，等级医院的医生能分配给每个病人的诊疗时间只有3分钟，看病之前问讯飞晓医，将大幅提升问诊效率，健康自查有效度提升40%。每年零售药店买药人次达67亿，购买非处方用药占比达45%，用药有疑问可以问讯飞晓医，帮助普通人更合理用药，高风险用药召回率提升90%。每年体检人次5.5亿，5000字体检报告眼花缭乱，讯飞晓医可以马上生成重点及健康提醒，帮助人们“对症复诊”。</p><p></p><p>这背后靠的是讯飞星火医疗大模型的核心能力支撑，通过上线实际使用数据抽查12万例并通过国家科技信息资源综合利用与公共服务中心（STI）第三方测试数据显示，讯飞星火医疗大模型在医疗海量知识问答、医疗复杂语言理解、医疗专业文本生成、医疗诊断治疗推荐的问题回答率全面超越GPT-4。</p><p></p><p>业界首个大模型+具身智能的人形机器人亮相</p><p></p><p>2022年初，科大讯飞推出“讯飞超脑2030计划”，提出分三个阶段，要让懂知识、善学习、能进化的软硬一体机器人走进千家万户。去年1024全球开发者节上，机器人超脑平台AIBOT发布，把认知智能、运动智能和具身智能的多模态感知能力结合起来，已为工业、教育、服务、特种等领域372家企业提供服务。AIBOT的持续进步，助力我国机器人进入高阶智慧时代。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/af/0f/afcf7f79f053627a1eb90671a578470f.png" /></p><p></p><p>刘庆峰表示，星火大模型的推出将让AIBOT赋能机器人发展进入到一个全新的阶段，人形机器人对复杂任务的拆解、开放场景的物体寻找明显提升，在强化学习泛化抓取、仿人行走的复杂地形运动能力上都比主流系统有一个非常大的提升。“下一步我们将以人形机器人为牵引，来推动‘视觉-语言-动作’多模态具身大模型，能够更好地赋能人形机器人”。</p><p></p><p>此外，科大讯飞发布AI扫拖机器人，它具备领先的家庭环境全场景感知及运动能力，通过多模态交互，精准避障让扫地机器人在家庭常见的障碍物中灵活穿梭不卡困。</p><p></p><h3>四、共建产业生态，2024年上半年对标GPT-4</h3><p></p><p></p><p>唯有生态繁荣，才有人工智能大未来。自2010年发布讯飞开放平台以来，科大讯飞持续构建AI开发者生态，让AI像水和电一样落地各行各业。今年依托星火大模型，讯飞将AI普惠更多用户。</p><p></p><p>“讯飞星火正日益成为解放生产力工具。”</p><p></p><p>刘庆峰表示，当前讯飞星火已有1200万用户，为千万用户持续创造刚需场景价值。自讯飞智作2.0 、讯飞写作产品发布以来，已新增超10万名专业用户，提供新闻播报、教育培训、自媒体推广到企业宣传等服务，让每个内容创作者都拥有自己专属的AI创作助手。</p><p></p><p>此外，他现场发布星火内容运营大师，人机共创提质增效，通用稿件30分钟共创生成，深度稿件辅助编辑效率翻番。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/90/53/90fb67463686e0b6d45538c147508453.png" /></p><p></p><p>如今随着大模型持续突破，讯飞AI开发者生态持续领先。自5月6日星火发布以来，讯飞开放平台新增143.4万开发者团队，同比增幅331%，汇聚553万AI生态开发者团队。其中新增大模型开发者17.8万，讯飞正与10万+企业客户用星火创新应用新体验。大模型也显著降低了开发者门槛，在讯飞星火上，已有1.5万助手开发者，开发超2.9万个助手应用。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/23/cf/233969d1371273ac90fb55439d6deacf.png" /></p><p></p><p>为了进一步加速大模型行业落地，科大讯飞联合行业龙头共同发布12个行业大模型，包含金融、汽车、运营商、工业、住建、物业、法律等行业，加速产业升级。</p><p></p><p>在讯飞星火蓬勃的生态背后，是自主创新的算力底座。讯飞和华为“联合特战队”半年内实现星火大模型训练和推理效率均翻番。1024全球开发者节现场，科大讯飞联合华为重磅发布，基于昇腾生态的“飞星一号”大模型算力平台。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/83/a5/835df7ecc7c130c0b4635d0yyb3562a5.png" /></p><p></p><p>不过，刘庆峰也提到，国产大模型必须正视和GPT-4的差距。“国产大模型在复杂知识推理、小样本快速学习、超长文本处理、跨模态统一理解上距GPT4还有差距。”他表示，唯有实事求是的科学精神，才能真正实现超越。</p><p></p><p>“大模型时代的大幕才刚刚揭开，让我们一起共创、共享、共进化。”刘庆峰透露，对标GPT-4的更大参数规模的星火大模型正式启动训练，2024年上半年将实现对标GPT-4。</p><p></p><p>【活动推荐】</p><p></p><p>在2023年12月28-29日，InfoQ将在上海举办<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon全球软件开发大会</a>"，这个会议上结合当前的趋势热点，设置了GenAI和通用大模型应用探索、AI Agent 与行业融合应用的前景、LLM 时代的性能优化、智能化信创软件 IDE、LLM 时代的大前端技术、高性能网关设计、面向人工智能时代的架构、高效的编程语言、性能工程、LLM推理加速和大规模服务、现代数据架构演进、建设弹性组织的经验传递、SaaS云服务弹性架构设计等专题，目前也正在邀请业界的专家来会议上演讲。感兴趣的可以点击<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon会议官网</a>"，查看详细的介绍，也欢迎您来会议上演讲，分享技术实践。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/euXpcb8cpRJHLJSaxZaE</id>
            <title>Google DeepMind 宣布基于LLM的机器人控制器 RT-2</title>
            <link>https://www.infoq.cn/article/euXpcb8cpRJHLJSaxZaE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/euXpcb8cpRJHLJSaxZaE</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 02:18:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Google DeepMind, Robotics Transformer 2, AI 模型, LLM
<br>
<br>
总结: Google DeepMind最近宣布了Robotics Transformer 2（RT-2），这是一个用于控制机器人的视觉-语言-动作（VLA）的AI模型。RT-2使用经过精调的LLM来输出运动控制命令。它可以执行训练数据中未明确包含的任务，并在新出现技能评估中将基线模型的表现提升了3倍。 </div>
                        <hr>
                    
                    <p><a href="https://www.deepmind.com/">Google DeepMind</a>" 最近宣布了 <a href="https://robotics-transformer2.github.io/">Robotics Transformer 2</a>"（RT-2），这是一个用于控制机器人的视觉-语言-动作（VLA）的 AI 模型。RT-2使用经过精调的LLM来输出运动控制命令。它可以执行训练数据中未明确包含的任务，并在新出现技能评估中将基线模型的表现提升了3倍。</p><p>&nbsp;</p><p>DeepMind 训练了两个 RT-2 的变体，使用了两个不同的底层视觉-LLM 基础模型：一个基于 <a href="https://www.infoq.com/news/2023/06/google-palm-e-robot/">PaLM-E</a>" 的 12B 参数版本，以及一个基于 <a href="https://arxiv.org/abs/2305.18565">PaLI-X</a>" 的 55B 参数版本。LLM 在通用视觉语言数据集和机器人特定数据的混合之上进行了共同微调。该模型学会输出一个机器人运动指令向量，将其简单地视为一个整数字符串：实际上，它是模型学习的一种新语言。最终模型能够接收机器人工作区域的图像和用户命令，例如“捡起即将从桌子上掉下来的袋子”，然后生成执行任务的运动指令。根据 DeepMind 的说法：</p><p></p><p></p><blockquote>RT-2 不仅展示了人工智能的进步如何迅速地渗透到机器人领域，它还展示了更多通用型机器人的巨大潜力。 虽然实现在以人类为中心的环境中有用的机器人方面还有大量工作要做，但 RT-2 向我们展示了一个令人兴奋的机器人的未来，它就在我们的掌握之中。</blockquote><p></p><p>&nbsp;</p><p>谷歌机器人技术和 DeepMind 已经发布了多个使用 LLMs 进行机器人控制的系统。2022 年，InfoQ 报道了谷歌的 <a href="https://www.infoq.com/news/2022/09/google-robot-saycan/">SayCan</a>"，它使用 LLM 为机器人生成高级行动计划，以及 <a href="https://www.infoq.com/news/2022/11/google-robotics-code-policies/">Code-as-Policies</a>"，它使用 LLM 生成执行机器人控制的 Python 代码。这两个系统都使用纯文本 LLM 来处理用户输入，视觉组件由独立的机器人模块处理。今年早些时候，InfoQ 报道了谷歌的 <a href="https://www.infoq.com/news/2023/06/google-palm-e-robot/">PaLM-E</a>"，它处理来自机器人传感器的多模态输入数据，并输出一系列高级行动步骤。</p><p>&nbsp;</p><p>RT-2 在之前的实现 <a href="https://robotics-transformer1.github.io/">RT-1</a>" 的基础上进行了改进。RT 系列的关键思想是训练一个模型直接输出机器人指令，而不是输出运动的更高级抽象。RT-2 和 RT-1 都接受图像和任务的文本描述作为输入。然而，RT-1 使用了一系列不同的视觉模块来生成输入到 LLM 的视觉令牌，而 RT-2 使用了单一的视觉语言模型，如 PaLM-E。</p><p>&nbsp;</p><p>DeepMind 在超过 6,000 个试验中对 RT-2 进行了评估。特别是，研究人员对其新兴能力表现感兴趣：即能够执行机器人专门训练数据中不存在的任务，但这些任务是通过其视觉语言预训练而出现的。团队将 RT-2 在三个任务类别上进行了测试：符号理解、推理和人类识别。与基准模型相比，RT-2 取得了“超过最佳基准模型平均成功率 3 倍以上”的成绩。然而，该模型未获得机器人训练数据中未包含的任何物理技能。</p><p>&nbsp;</p><p>在 <a href="https://news.ycombinator.com/item?id=36905076">Hacker News 上对这件工作的讨论</a>"中，一位用户发表了评论：</p><p></p><p></p><blockquote>似乎这项工作（以及许多机器人学习工作）仍然停留在位置/速度控制而不是阻抗控制阶段。这本质上是输出去哪里，可以是通过闭环控制器或开环运动规划器。这似乎极大地降低了数据需求，但感觉对我们可以完成的任务有一种根本性限制。机器人操作之所以困难是因为我们不仅需要考虑世界上正在发生的事情，还需要考虑到我们的互动如何改变它以及我们如何对此作出反应。</blockquote><p></p><p>&nbsp;</p><p>尽管RT-2尚未开源，但RT-1的<a href="https://github.com/google-research/robotics_transformer">代码</a>"和<a href="https://console.cloud.google.com/storage/browser/gresearch/rt-1-data-release?pli=1">数据</a>"已经公开。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/deepmind-robot-transformer/">https://www.infoq.com/news/2023/10/deepmind-robot-transformer/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jT23W6bD7qmk5OpZRf8P</id>
            <title>平安人寿魏政刚：算力与语料，是制约保险领域大模型应用的首要挑战</title>
            <link>https://www.infoq.cn/article/jT23W6bD7qmk5OpZRf8P</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jT23W6bD7qmk5OpZRf8P</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Oct 2023 07:54:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 保险领域, 科技与传统行业融合, 智能化保险时代
<br>
<br>
总结: 大模型在保险业务全链路的应用标志着科技与传统行业的融合越发紧密，预示着一个全新的、更加智能化的保险时代即将到来。保险行业的业务模式以代理人为中心，人工智能和数字化转型的关键问题与代理人紧密相关。大模型在保险业务中的应用不仅仅是数字化的表现，它对成本降低、业务发展、代理人的展业活动和增员板块带来了明显的促进。保险业在大模型的规模化应用中面临来自应用场景、语料、算力、底层技术理解、人才等五个方面的挑战。 </div>
                        <hr>
                    
                    <p>大模型如火如荼，在保险领域也开始广泛应用。这不仅标志着科技与传统行业之间的融合越发紧密，也预示着一个全新的、更加智能化的保险时代即将到来。</p><p></p><p>有消息称，平安集团正在研发上千亿参数的模型。结合其以往在人工智能上的探索和应用经验，平安在大模型领域采取了综合性的策略，而非仅仅聚焦于提供某一类服务，如单纯的聊天或问答功能。</p><p></p><p>在日前的 InfoQ《超级连麦·数智大脑》直播节目中，德邦基金 CTO 李鑫与平安人寿科技总监魏政刚、阳光保险集团工智能部大模型首席专家张晗深入探讨了“大模型在保险业务全链路的应用”。</p><p></p><p>魏政刚指出，国内保险行业的业务模式以代理人为中心，因此人工智能和数字化转型的关键问题也是与代理人紧密相关。以平安人寿为例，其推出了基于大模型的数字人产品，主要用于协助代理人与客户沟通。这对初入行业的代理人提供了极大帮助，可以指导他们与客户交流、收集信息并提供合适的产品推荐。</p><p></p><p>当然，魏政刚也进一步解释，这不意味着大模型在保险价值链的其他环节不被应用。实际上，核保、保全、理赔等多个环节都已广泛采用人工智能技术。</p><p></p><p>但是，如果要进一步实现<a href="https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ">大模型</a>"的规模化应用，保险业还必须搞定来自应用场景、语料、算力、底层技术理解、人才等五个方面的挑战。其中，算力能力与语料准确性更是重中之重。</p><p></p><p>本文整理自李鑫与魏政刚的对话内容（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>李鑫：当前，国内保险行业的发展趋势如何？同时，数字化转型对保险行业又将带来哪些机遇与挑战？</h5><p></p><p></p><p>魏政刚：国内的保险业务模式主要以代理人为核心，即专属代理人模式。事实上，大部分保费和成交都是通过这个渠道完成的。当然，还有其他的销售渠道，如电网销、独立经纪人等。</p><p></p><p>在人工智能和数字化转型方面，问题的核心也是围绕代理人展开。在<a href="https://www.infoq.cn/article/eLIiWldQ2SVEUQFuYp2j">保险行业</a>"，有内勤和外勤之分，外勤通常指代理人，而内勤通常指公司总部、机构等员工。在代理人运营及其营销推广服务方面，我们看到大量的机会可以通过数字化技术来加强。</p><p></p><p>关于数字化与保险行业的结合，我们从两个维度来看：一是保险行业的价值链，二是数字化与人工智能技术的层次。</p><p></p><p>在探讨保险行业中的应用与技术结合，我们首先看这个行业的价值链或横轴。其起始点从投入开始，紧随其后的是营销和销售，接着是新业务的管理。当保单进入系统后，下一步则是核保环节。之后是我们提供的客户服务，有时也被称为保全，最后则是理赔环节。总结下来，在这个横轴上，我们主要关注五个大环节：营销、销售、新业务、核保和理赔。</p><p></p><p>从纵轴来看，这代表了人工智能的技术发展。其最底层是非常基础的原子性函数。这一层在西方的发展特别显著，因为很多底层逻辑和数据推导都起源于此。再上一层则是基础模型，如 CNN、KN、RNN 等。</p><p></p><p>再往上，则是组合算法层，其中包括多种算法的组合、参数配置以及我们在算法上的调优。再上一层，我们看到了许多人工智能框架，以及尚未开源的一些大型模型如百度、腾讯、阿里等。</p><p></p><p>在这基础上，我们面临的问题和机会点出现在横轴上的各个环节中。总的来看，人工智能大模型在保险行业的应用不仅仅是数字化的表现。人工智能为保险行业带来了巨大的促进作用，不论其是否能够引发新的商业模式，我们至少可以看到，它对我们的成本降低、业务发展、代理人的展业活动和增员板块带来了明显的促进，尤其是今年我们倡导的 MVP、QVP 和 FVP 等方面的举措。</p><p></p><h5>李鑫：在平安人寿，目前人工智能技术应用有什么关键进展？</h5><p></p><p></p><p>魏政刚：<a href="https://www.infoq.cn/article/RFGAkFHxTtyrAaphHBh0">平安</a>"集团及平安人寿很久之前便开始了人工智能产品的研发。在大模型还未在市场大放异彩之前，我们已在基础的 AI 应用上对各种产品进行了实际体现。以 2019 年和 2020 年为例，为了应对远程工作需求，我们推出了智能拜访助手。这是一个综合应用了 AI 技术的工具，涵盖了语音、画像、视频以及自动生成的话术等功能。</p><p></p><p>从应用角度看，人工智能的应用可以概括为文本、语音和图像视频这几大方向。但当它们综合运用时，就面临如何在产品形态中完美结合，以及如何将其放入最恰当的使用场景中的问题非常重要。因此，我们的努力不仅仅是在探索和使用 AI 技术，更多的是在产品的适配性和形态打磨上。</p><p></p><p>对于大模型，其在市场上的应用开始于去年下半年。随着例如 ChatGPT 在市场上的推广，我们从集团层面开始大量投资。有消息称，我们正在研发拥有千亿参数的模型。结合我们以往在人工智能上的探索和应用经验，我们采取了一个综合性的策略，而非仅仅聚焦于提供某一类服务，如单纯的聊天或问答功能。</p><p></p><p>我们所推进的技术和应用都紧密结合了我们的业务场景，而主要的焦点仍然在营销和销售端。可能有些人会好奇，为什么是营销和销售端？在保险产品方面，其同质性是显著的。保险产品并不像简单的消费品容易理解，它不仅需要一定的知识体系，还需要个人的体验以及结合理性和非理性的销售要素。</p><p></p><p>从销售的角度看，第一是保险的基本原理：第二是保险产品的知识体系，每家公司都提供的数百种保险产品，每种产品的结构，以及每种产品所带来的保障利益和应对风险的要素。因为保险消费者的需求是多样的，例如理财、保障、税收优惠、财产传承等。因此，对产品的深入了解和知识体系是至关重要的。</p><p></p><p>第三是关于销售技巧的深化和延展。在保险代理人的发展中，过去是红海模式，即“大进大出”的模式。在最高峰时期，中国的专属代理人数量达到了 1000 多万，而现今只剩下约 400 万，这样模式导致了代理人在销售技能上的参差不齐。销售是一个非常讲究专业性的领域，存在许多销售技巧。除了培训代理人，我们还需要启迪和教育客户，使他们了解保险的益处。</p><p></p><p>在保险行业中，需求激发是至关重要的一步。大部分人可能会直接表示没有保险需求，但实际上，每个人都有潜在的保障需求和对保险的期待。如何挖掘和激发这些需求是一个巨大的课题。而在这方面，大模型相对于其他技术具有显著的优势。大模型可以模拟人的思考和认知，可以为我们寻找最优解。</p><p></p><p>我们大量地聚焦在销售和营销上。但这并不意味着我们在价值链的其他环节没有利用大模型。事实上，人工智能被广泛应用于多个领域，如核保、保全、理赔等。例如，在客户服务方面，我们可以利用大量的保单信息和客户在 C 端平台上的行为轨迹来提供更优质的服务。</p><p></p><h5>李鑫：当前的通用大模型在商业化应用中面临哪些主要挑战？如何有效地突破这些障碍？</h5><p></p><p></p><p>魏政刚：对于当前大模型在商业化应用中的挑战，以下几点尤为关键：</p><p></p><p>水平应用与垂直应用：从业界沟通来看，国内市场上，针对终端消费者（ToC）的水平应用投入巨大且机会较小。相比之下，垂直应用结合针对企业的应用（ToB）可能是未来的趋势。算力问题：尽管某些应用不需要广泛的算力，但在深度学习模型的训练和推理过程中，算力仍然至关重要。当前的应用需要降低某些参数设置以适应算力限制。尽管这是一个短期挑战，随着国际关系的改善和国内半导体研发的进步，中长期内这一问题有望得到解决。语料问题：相比英文，中文语料在质量和结构化方面都存在挑战。这可能与语言特性和知识沉淀有关。特别是在保险这一行业，大量的销售技巧和知识被深埋在代理人的经验中，而这部分知识难以被结构化。尽管从英文转译到中文是一个方法，但直接从业务伙伴和代理人中提炼中文语料，或使用像 ChatGPT 这样的模型生成语料，可能会更有效。底层技术理解：虽然国内大部分 AI 应用都是在应用层，但对底层技术的深刻理解至关重要。这包括算法选择、激活函数的选择、微调策略等。这种深入的技术理解与业务理解结合起来，对于产品的成功至关重要。人才问题：我们需要业务和技术双背景的复合型<a href="https://www.infoq.cn/article/u7lLw2rLqF6tLIiYZXcL">人才</a>"，他们需要对业务有深入理解，同时对技术也有足够的掌握。</p><p></p><p>在发展前景上，大模型和多模态技术在 IT 与 AI 领域持续显示巨大潜力。虽然许多传统 IT 和 AI 技术已广泛应用，但像 Stable Diffusion、Midjourney 这样的大模型技术仍有巨大的想象空间。特别在中国这样的劳动密集、专业化市场，对这些技术的需求尤为旺盛。</p><p></p><p>以平安人寿为例，我们推出了数字人这类基于大模型的产品，主要协助代理人与客户沟通。这对初入行业的代理人特别有助，因为它可以指导他们与客户交流、收集信息并提供合适的产品推荐。虽然已获得正面反馈，但我们仍秉持互联网的试错精神，不断创新。</p><p></p><p>但我们也面临挑战，就是上面谈到的五方面。尤其在金融行业，第三点更为重要，也就是语料的准确性，因为行业受到严格监管。大模型有时会输出不准确的信息，这有很大的问题。我们现在的研究的重点是模型的实用性和合规性。对于合规性，我们不仅需要在监管文档上训练模型，还要结合实际监管案例。仅依赖大模型可能不足，像 LLaMA2 这样结合不同算法的方法可能更为有效。</p><p></p><h5>李鑫：在国内，大模型是否在 ToB 和垂直领域机会更多一些？</h5><p></p><p></p><p>魏政刚：关于大模型在业务应用上的选择和定位，可以从两个维度来考虑：水平和垂直。</p><p></p><p>首先，ToC 即面向消费者的应用，其投入巨大。在国内这一领域，几个龙头企业已经初露锋芒。与美国的情况相似，这些领军企业拥有雄厚的资金，对基础研发都非常重视。对于我们这种更偏重于金融领域的公司，如果要在 ToC 方向发力，就必须深入一个具体的垂直细分领域。因此，对于平安这样的金融科技公司，垂直应用更为合适，因为我们的目标是在金融领域通过技术进行赋能。</p><p></p><p>从平安集团的策略来看，我们正在大力发展大模型技术，已经取得了一系列进展。平安作为一家综合金融加医疗服务的企业，科技驱动下涵盖了银行、证券、保险等多个金融细分领域，因此我们需要一个深入金融领域的垂直模型。</p><p></p><p>在选择 ToC 或 ToB 时，我们也要考虑到当前大模型的特性。例如，大模型的训练是基于一段时间内的语料，而不是实时更新的。但未来，大模型可能会与搜索引擎等实时应用更紧密地结合。因此，在此背景下，面向消费者的 ToC 可能更适合由科技巨头来开发和维护。总结来说，我们认为垂直应用和面向企业的 ToB 模式更为合适。</p><p></p><h5>李鑫：未来，随着通用大模型的发展，是否会逐渐替代目前的专用 AI 模型？</h5><p></p><p></p><p>魏政刚：特定领域的 AI 技术，尤其是针对图像和语音的技术，并不会被完全取代。例如 OCR、CNN、RNN 等算法框架和基础算法仍将被广泛应用。在 NLP 领域，像分词这样的技术可能会受到挑战，因为当我们有了更先进的 Transformer 技术来解决问题。无论从学术还是工程的角度，研究和关注的方向都需要适时调整。</p><p></p><p>这不仅仅是一个简单的"yes"或"no"的问题，我们需要从两个维度去看待这个问题：一方面，从技术的分类和层次出发，看哪些技术应该或容易被替换；另一方面，根据所在行业的特性和价值链来决定哪些业务环节需要技术替换。如果某个特定的 AI 技术在特定业务领域已经表现得很好，那么可能就没有替换的必要。</p><p></p><p>决策需要谨慎，新技术出现并不意味着我们应该立即进行替换。但前瞻性研究和试点都是必要的。尽管传统领域中有些方法已经做得很好，但随着时间的推移，替代的机会和理由可能会出现。因此，我们需要两条腿走路：一方面是实际的生产和商业应用，另一方面是前瞻性的思考和尝试。这种结合可能会为公司带来更大的价值。</p><p></p><h5>李鑫：大模型应用的投入产出比如何考量？</h5><p></p><p></p><p>魏政刚：进行新技术的尝试者一定需要付出代价，但这个代价与其带来的收益并不是 1:1 的关系，而可能是 1:10。技术进步的过程中，尝鲜者有时会面临风险。我们真的需要从头训练我们的基座模型吗？在很多情况下，我们可以直接采用已经训练好的模型，如在金融领域，可以从集团获得已经训练好的模型，这样避免了重复劳动。但在特定的业务板块，仍需要进行训练。成本主要体现在训练和推理两方面。</p><p></p><p>在我们的价值链和产业链中，选择在哪个环节进行突破，需要综合考虑业务需求、公司战略以及市场变化。对于中国保险行业，我们更多地将精力放在销售和代理人上，考虑其市场特点。在推理上，提示词工程和逻辑处理也非常重要。</p><p></p><p>面对大模型的成本问题，主要考虑的是业务与市场策略的结合，以及确保资金得到合理的使用。另一方面，科技人员应当追求简洁、低成本的解决方案。</p><p></p><p>总体上，<a href="https://www.infoq.cn/article/eZ8J5Z7SuUSM4ql4ioVW">技术投入</a>"与其带来的收益是值得的。这不仅是基于我们的增长预期，也基于我们对技术，尤其是人工智能和大型语言模型，能够真正为业务赋能的信心。平安人寿的改革成果也印证了这一点，从中我们可以看到生产力和收入水平的提升。然而，如何精确计算这种技术投入与业务收益之间的平衡点仍然是个挑战，特别是对于非 IT 企业。但从我们的实际经验和进步来看，我们相信这种投入是有益的。</p><p></p><h5>李鑫：关于大模型在保险行业未来 3-5 年的应用和发展的趋势，您如何看？</h5><p></p><p></p><p>魏政刚：金融领域，尤其是保险行业，与科技的结合是当前的趋势。此前有关于“元宇宙”的讨论，这种科技能力的广泛应用也表明了科技在保险业的重要性。然而，业内的变革和演变往往需要时间。例如，从传统的代理人模式逐渐转向更高质量的模式，这一过程已经持续了很长时间，且仍在进行中。</p><p></p><p><a href="https://www.infoq.cn/news/Xhlku65TOzhUtKR2yaSi">人工智能</a>"和大语言模型在这个领域的应用，在 3-5 年内可能更多是补充性质，并不会完全替代。而且，业务可解释性是关键，人工智能和大数据的应用需要能够解释其决策和行为。中国的保险行业是强监管的，因此行业的发展和兴衰周期往往受到监管政策的直接影响。对于技术的适配性，我认为周期会更长。</p><p>未来的方向应该是释放人的潜力，将人从繁琐的日常任务中解放出来，使其能够专注于更具挑战性和深度的问题。如果未来的保险代理人或经纪人都是来自顶尖大学，那么这将是一个很好的趋势标志。在销售方面，可能会出现更多的数字化产品和形态，比如数字化的代理人。</p><p></p><p>总的来说，科技在保险行业中的进步和应用空间越来越大。中国的金融行业和企业需要找到与西方不同的发展路径，而科技结合将是这条路径的核心。希望保险行业能够通过与科技的结合，提供更真实、本质、科学和理性的服务，真正帮助人们，推动社会进步。</p><p></p><p></p><h4>关于 FCon</h4><p></p><p>首届<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">FCon全球金融科技大会</a>"将于 11 月 19-20 日在上海举办。大会将围绕金融领域数字化转型挑战探索、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等 10+专题进行交流。</p><p></p><p>目前大会邀请了汇丰科技中国区的代理总经理马国栋、度小满金融数据智能部总经理杨青先、蚂蚁集团副总裁 &amp; 首席技术安全官韦韬博士、恒生聚源总经理吴震操担任大会联席主席。更多嘉宾仍在邀请中......</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">点击链接</a>"即可查看全部演讲专题。</p><p></p><p>目前是 <a href="https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">7 折特惠购票</a>"，报名立减 ¥2040，咨询购票可联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bf/a9/bf1622c0c2be73e4d19e8643444e2fa9.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qzs4Zp3DH1wxIeSpVgU8</id>
            <title>头脑正常的人绝不会创业！英伟达CEO黄仁勋：如果能够重来，宁愿放弃创办公司</title>
            <link>https://www.infoq.cn/article/qzs4Zp3DH1wxIeSpVgU8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qzs4Zp3DH1wxIeSpVgU8</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Oct 2023 06:12:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 科技大佬, 创业困难, 成功企业家, 员工失望
<br>
<br>
总结: 英伟达公司CEO黄仁勋分享了他对创业困难的独特视角，认为打造科技巨头比预想中困难一百万倍。成功企业家需要在逆境中坚持不懈，并有能力说服自己创业并不像实际那么困难。黄仁勋最担心的是让员工们感到失望。他认为AI技术将在短期内创造更多就业机会，但也可能导致其他行业的岗位消失。他建议人们学习如何使用AI技术，因为工作的形态正在发生转变。英伟达公司的组织方式与其产品的架构保持一致。 </div>
                        <hr>
                    
                    <p></p><blockquote>这位科技大佬还强调，他最担心的就是让员工们失望。</blockquote><p></p><p></p><p>英伟达公司CEO黄仁勋是有史以来最成功的企业家之一。英伟达也是少数几家市值达到 1 万亿美元的公司之一，黄仁勋本人也是世界上最富有的人之一，净资产达 375 亿美元。在最近的一次采访中，他分享了他对创建公司所面临的挑战的独特视角。</p><p></p><p>黄仁勋透露，打造这家科技巨头“比我预想中要困难一百万倍”，甚至表示如果能够预见到后来所经历的一切艰辛，任何“头脑正常的人绝不会选择创业”。黄仁勋承认，如果有选择，他不会再创办公司，因为其中涉及巨大的困难。</p><p></p><p>老黄认为，“在逆境中坚持不懈的能力才是成功企业家的标准”。老黄进一步解释说，成功企业家的特质之一是他们有能力说服自己，创业之路并不像实际那么困难。</p><p></p><p></p><blockquote>“你得让自己相信这一切没那么难，但实际上事实要比想象中困难得多。如果能带着现在的认知回到过去，那我宁愿放弃创办公司。我觉得一路走来太辛苦了，真的太辛苦了。”</blockquote><p></p><p></p><h2>“创立英伟达，比我预想中困难一百万倍”</h2><p></p><p></p><p>作为全世界最具远见的科技企业之一的创始人，这位60年前出生于中国台湾的华人曾随家人搬往泰国，年轻时又来到美国。据说，他曾在AMD和LSI Logic短暂就职，并在加州圣何塞的一家丹尼斯餐厅跟合伙人们会面之后决定共同创立英伟达。他坦言，如果能回到30岁重新选择，他绝不会走上自主创业这条道路。</p><p></p><p>但这位技术大佬在最近接受Acquired播客采访时承认，企业家们最大的“超能力”，就是欺骗自己相信“这事没那么难”。</p><p></p><p>而忍受所有这些困难也给老黄带来了巨大的回报。早在 1993 年，黄仁勋就以不到1000美元的资金创立了英伟达，目前公司市值已超过1万亿美元。虽然老黄现在已经非常富有，并且已经辛勤工作了30年，但他并不打算停下来。“欺骗自己这个伎俩仍然有效，”他笑着说。“我仍然非常享受欺骗自己，并且我正在不断给自己加码，”他说。</p><p></p><p>黄仁勋表示，自英伟达公司成立以来，他最大的担忧就是无法推动员工们取得成功。“时至今日，我最担心的事情还跟当初刚加入公司时一样，就是让员工们感到失望。”</p><p></p><p>据金融分析公司FactSet的统计，黄仁勋拥有英伟达3.5%的股份（目前公司总市值1.04万亿美元）。他在播客采访中表示，加入一家企业的员工最终会相信企业的发展愿景，并将集体的抱负接纳为个人的抱负。</p><p></p><p>黄仁勋强调，“会有很多人加入你的企业，因为他们相信你的希望和梦想，并愿意将其作为自己的希望和梦想。所以你希望顺应他们、希望他们获得成功、希望他们能拥有自己的美好生活……而最大的恐惧，则是让他们感到失望。”</p><p></p><h2>AI将在短期内创造更多就业机会</h2><p></p><p></p><p>在解释自己如何克服质疑和挑战、并坚持将英伟达打造成如今的行业巨头时，黄仁勋将一切归功于这三十年旅程中始终相信他、与他站在一起的“支持网络”。</p><p></p><p>他解释道，自1999年公司首次上市以来，自己就面临过无数的冲击和挑战。在股价如自由落体般急转直下的那段时期，身为英伟达领导者的他感到几乎“无法承受”。黄仁勋坦言，“无论大家怎么看待，那都是段令人尴尬的经历”。</p><p></p><p>而就在他发表此番言论之际，英伟达股价刚刚结束过去12个月间高达245%的凶猛增长、如今再次出现回落。</p><p></p><p>最近，因拜登政府出台更为严格的对中国半导体出口控制政策，这家总部位于圣克拉拉的公司又一次遭受沉重的股价震荡。</p><p></p><p>展望未来，黄仁勋表示AI技术的发展已经为英伟达等科技企业带来“巨大”机遇，他认为“市场机会可能已经增长了上千倍。”</p><p></p><p>他认为，AI技术将在短期之内“创造更多就业机会”，但同时也警告称在新增的就业机会之外，也可能有更多来自其他行业的岗位将在自动化的冲击下而消失。黄仁勋表示，“从好的方面看，随着生产力变得更高，公司的利润也将有所提升，这样管理者通常会雇用更多员工来扩展新的业务领域。”</p><p></p><p>“但就目前来看，新增的就业机会并不能保证一切原有岗位都继续存在。情况明显没那么乐观，更大的可能性是不少从业者会因为其他人开始使用AI、但自己不会用AI而失去工作。”</p><p></p><p>他建议人们“学习如何使用AI技术”，因为他认为“工作的形态正在发生转变。”</p><p></p><p>至于英伟达自身，黄仁勋称这家公司的结构就如同他们销售的产品，类似于一套“计算技术栈”。</p><p></p><p>他表示“英伟达的组织方式跟具有严格自上而下指挥和控制机制的军队不同”。相反，该公司的组织方式更像是基于分散结构的“神经网络”。这也反映出一种基本理念，即“你的组织架构，应该与所构建产品的架构保持一致。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.morningstar.com/news/marketwatch/20231020335/nobody-in-their-right-mind-would-do-it-nvidia-ceo-jensen-huang-says-he-wouldnt-start-a-company-if-he-had-a-do-over">https://www.morningstar.com/news/marketwatch/20231020335/nobody-in-their-right-mind-would-do-it-nvidia-ceo-jensen-huang-says-he-wouldnt-start-a-company-if-he-had-a-do-over</a>"</p><p><a href="https://ts2.space/en/nvidia-ceo-reflects-on-entrepreneurship-its-hard-but-worth-it/">https://ts2.space/en/nvidia-ceo-reflects-on-entrepreneurship-its-hard-but-worth-it/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ</id>
            <title>智谱 AI “超 25 亿融资” 的背后</title>
            <link>https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Oct 2023 06:02:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 融资额度, 估值, 智谱 AI
<br>
<br>
总结: 这篇文章主要介绍了两家大模型创业公司百川智能和智谱 AI 分别宣布的融资额度和估值，以及它们在资本市场的受关注程度。智谱 AI 被称为中国版 OpenAI，与 OpenAI 在技术发展和人工智能伦理方面有许多相似之处。智谱 AI 的优势在于其高融资额和高估值，以及其安全可控的大模型技术。参与智谱 AI 融资的组织背景多样，反映了大模型产业化的趋势。 </div>
                        <hr>
                    
                    <p>这几天，“大模型”圈里最令人津津乐道的可能就是两家大模型创业公司分别宣布自己今年的融资额度——<a href="https://www.infoq.cn/article/ivM3DbowD6o9Ro4jIeGq?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百川智能</a>"获得 3 亿美元的融资，估值跃升成为独角兽企业；智谱 AI 获得超过 25 亿人民币的融资，百亿人民币估值令人瞩目。</p><p></p><p>百川与小米的联手得到了业界内的广泛关注，而雷军的手笔更是一下子把大家的目光都吸引到了智谱 AI 身上。除了目前热度超高的这两家，大模型初创公司“月之暗面”也是资本的新宠。红杉资本、真格基金押注下场，目前月之暗面的募资金额目前已经超过 2 亿美元。</p><p></p><p>临近年底，今年“大模型”的资本角逐已经初现成果，智谱 AI 凭借着最高融资额和最高估值走到了台前。由于长期低调的学院派风格，非行业内的声量并不是很高。直到这一次高调宣布融资额，彻底引爆了创投圈。</p><p></p><p>智谱 AI 如此大规模融资的背后，涉及到的是众多知名机构和投资人。从智谱 AI 官方宣布的融资信息来看，参与投资的组织包括社保基金中关村自主创新基金（君联资本为基金管理人）、美团、蚂蚁、阿里、腾讯、小米、金山、顺为、Boss 直聘、好未来、红杉、高瓴等。整个过程中，腾讯阿里联手，主流基金入场，战投纷纷表态，这个融资声势无论放在哪个行业都是相当罕见的。</p><p></p><p>面对如此大规模的融资，我们不禁要思考一个问题——智谱 AI 的优势何在，众多的投资人和机构为什么选择了它？同时这也引发了我们对人工智能产业未来发展的思考，随着认知智能等新一代技术的崛起，人工智能产业将迎来哪些新的机遇和挑战？未来的发展又将呈现怎样的格局？</p><p></p><p></p><h2>智谱 AI 是资本市场看好的“中国版 OpenAI”种子选手</h2><p></p><p></p><p>2023 年 6 月，硅谷科技媒体 The Information 在盘点最有可能成为“中国 OpenAI”的 5 家企业时，智谱 AI 赫然在列。<a href="https://www.infoq.cn/article/g9tuoTODP20N1lTzjsjw?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI </a>"作为人工智能领域的领先者之一，拥有世界顶尖的 AI 研发团队，其技术实力在自然语言处理领域处于领先地位，其发展动态和成果一直受到全球的广泛关注，甚至直接被一些媒体评价为“人工智能领域的先驱”和“科技创新的领头羊”。所以，如果说“智谱 AI”是有可能成为“中国版 OpenAI”的企业，那说明它一定在国内人工智能领域做出了许多与 OpenAI 一样的努力。</p><p></p><p>对比一下 OpenAI 和智谱 AI 这两家企业的技术发展特点，就不难发现这两家企业确实有很多异曲同工之处。两家企业都致力于自然语言处理领域的研究，并取得了一系列重要的成果。两家企业都拥有先进的预训练语言模型，能够理解和生成人类语言，为各种应用场景提供强大的支持，无论是 OpenAI 的<a href="https://www.infoq.cn/article/GuXceGIYzMHs7rt2ldUB?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> GPT </a>"系列模型，还是智谱 AI 的文言文模型，都在搜索引擎、智能客服、机器翻译等领域发挥了重要作用。</p><p></p><p>不仅如此，两家的产品性能甚至都几近相似，比如，在 Stanford 报告的世界主流大模型评测中，智谱 AI 于 2022 年研发的 GLM-130B 是亚洲唯一入选模型，准确性、恶意性与 OpenAI 研发的 GPT-3 持平，且鲁棒性和校准误差在所有模型中表现最佳。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0842fdf94ed6961c2c02986bf5ee6d83.png" /></p><p></p><p>此外，在人工智能技术日益普及的今天，如何保障人工智能技术的道德和伦理问题成为了全球关注的焦点。而 OpenAI 和智谱 AI 目前都开始注重人工智能伦理的研究和实践，致力于开发符合道德规范的人工智能技术。</p><p></p><p>其次，两家企业从融资情况方面也很像，智谱 AI 是国内人工智能赛道融资最高的企业，OpenAI 去年也以占比美国相关企业融资总额超 70% 的占比问鼎赛道融资最高企业，广受资本的喜爱。要知道，资本市场一向都是残酷无情的，投资组织在决定投一家科技企业时，一定是综合了技术市场趋势与获投企业的整体技术能力、盈利模式来进行最终决策的。所以如果有十几家的投资组织都将自己的钱投向一个企业的时候就说明，这个企业的未来发展一定非常强势。</p><p></p><p>无论是投资人还是行业从业者，都非常清楚一个事实，那就是大模型技术是“有门槛的”，是需要有时间积累的。在大模型技术还没有爆火之前，市场还没有那么“卷”的时候，智谱 AI 就已经开始了相关技术的研发，虽然当时大模型技术由于高昂的训练成本和复杂的开发门槛并不被业界所看好。然而，他们已经不是第一次做“第一个吃螃蟹的人”了，无论是从研发、开源生态还是商业合作上。</p><p></p><p>智谱 AI 目前是国内唯一全内资、国产自研的大模型企业，它推出的 GLM 国产芯片适配计划，面对不同类型的用户不同类型的芯片提供不同等级的认证和测试，这意味着智谱 AI 的大模型是安全可控的，这也将直接反哺智谱 AI 的商业化能力。我想这也是为什么智谱 AI 能够吸引来众多组织投资的一个原因。要知道，大模型技术“卷”到现在这个阶段，已经进入了中后段，像其他技术的发展周期一样，安全问题已经成为了大模型领域最受为关注和亟待解决的技术挑战，如果智谱 AI 的大模型能够将领域内的安全问题妥善解决，那对于投资人来说，这将是一笔稳赚不赔的买卖。</p><p></p><p></p><h2>从参与融资组织的母体背景看出了“大模型产业化”趋势</h2><p></p><p></p><p>资本在哪，大市场就在哪。我们仔细盘点一下参与智谱 AI 融资的组织，可以发现一个非常有意思的现象——除了专业投资机构以外，像美团、蚂蚁、阿里、腾讯、小米、金山、Boss 直聘等企业的母体业务，或多或少地都已经开始自研大模型或者基于业务进行大模型个性化创新改造的探索过程中。</p><p></p><p>例如，美团曾推出过基于大模型的智能推荐系统，帮助用户更好地找到所需商品或服务；蚂蚁集团一直在推进大模型在智能客服、智能风控等领域的应用；腾讯通过投资和自主研发，在大模型方面取得了不少成果，并积极推动大模型在各行业的应用；小米将大模型应用于智能家居、物联网等领域，提升用户体验；Boss 直聘、好未来等公司则在 AI 大模型方面进行了研究和应用，推出了各种基于 AI 大模型的智能招聘、智能教育等服务，为人力资源和教育行业的发展提供了新的思路和方法。</p><p></p><p>而作为初创企业的智谱 AI 在开放平台、云端私有化、本地私有化三个方面，基于自己早已建立的开源生态，也已经与超过 200 家的企业进行了生态共建，与超过 1000 家机构共建大模型应用场景。因此，无论是投资智谱 AI 的组织母体背景，还是智谱 AI 自己，他们在做的除了从技术上推动大模型技术的发展，更重要的是将大模型应用于产业场景中，大模型产业化已经是领域发展的必然趋势。</p><p></p><p>从技术角度看，大模型产业化趋势源于其强大的数据处理和推理能力。随着数据量的爆炸性增长，传统的机器学习方法已经难以应对如此大规模的数据处理任务。而大型深度学习模型，如 GPT-4 等，能够处理海量数据并从中提取有价值的信息，为各行各业提供强大的支持。此外，大模型还具有出色的泛化能力，能够在处理未知问题时做出较为准确的预测和决策，进一步推动了其在各行业的应用。</p><p></p><p>如果复盘近两年来大模型技术的发展历程便可以发现，是“深度学习”技术的突破使得我们可以训练更大、更复杂的模型，像 GPT-4、BERT、ResNet、YOLO、PaddlePaddle、文心一言、通义千问等大模型目前其实都已经具备了较强的处理能力和较高的精度，可以处理更多的任务和数据。这为大模型的商业应用提供了更广阔的空间，也为产业化的实现提供了更强大的技术支持。</p><p></p><p>从社会角度看，大模型产业化趋势对于社会发展有着深远的影响。一方面，大模型的应用改善了人们的生活质量，例如智能家居、智能交通等领域的应用，使得生活更加便捷和安全。在另一方面，大模型也带来了新的社会问题，如数据隐私、人工智能伦理等问题，需要社会共同探讨和解决。</p><p></p><p>从商业角度来看，大模型的应用场景还在不断扩大，从最初的互联网领域已经扩展到了金融、医疗、教育、制造、服务等传统行业。这些行业拥有丰富的数据资源，但数据处理和智能化应用的需求一直未能得到很好的满足，大模型的出现为这些行业提供了新的解决方案，大模型高精度、高效率和高可靠性的特点，能够帮助企业提高效率、降低成本、更好地理解客户需求、预测市场趋势、优化决策和业务创新，可以满足各行各业的商业化需求，加速了传统产业数字化转型升级的进程，这些商业价值是大模型产业化的重要驱动力。</p><p></p><p>当然了，大模型的产业化需要整个产业链的支持，包括硬件、软件、数据和人才等方面。随着产业链的完善，大模型的产业化将得到更全面的支持和保障。同时，产业链上的各个角色也将在大模型的产业化中获得更多的机会和收益。</p><p></p><p></p><h2>下一代人工智能技术是“认知智能”</h2><p></p><p></p><p>在当下这个信息化和数字化的世界里，人工智能技术已经成为了企业和组织的重要竞争力。然而，随着技术的不断发展和用户需求的不断升级，人工智能技术也在不断地寻求突破和创新。从大模型技术的发展路径来看，它已经陆续走过了“计算智能”、“感知智能”阶段，正在进行“认知智能”阶段的探索。智谱 AI CEO 张鹏多次在公开场合表示，“下一代的人工智能技术应该是认知智能。”</p><p></p><p>计算智能是人工智能的基本要求，它使机器能够进行计算和存储，人类无法记住一万个四位数，但机器可以轻松完成。感知智能则更进一步，使机器能够听懂人类语言、会说话、能看懂图像并识别物体，例如通过传感器感知环境并做出决策，同时执行一些简单的指令和动作，例如人脸识别系统。</p><p></p><p>而认知智能作为人工智能的高级阶段，是人工智能取得进一步突破的关键瓶颈，也是形成更大产业规模的关键技术，它要求机器能够能够像人一样进行思考、理解、推理、判断、学习等智能活动，并能够根据环境变化做出相应的决策和行动，像智能客服、智能家居、自动驾驶等都是目前比较典型的应用场景。与传统的感知智能相比，认知智能更加强调智能的内涵和深度，更加注重对于人类智能的模拟和再现，这需要投入大量的人力物力去研发。全国人大代表刘庆峰在十四届全国人大一次会议上呼吁，“我国要加快打造我国的认知智能大模型，并推动大模型在各领域的价值落地已迫在眉睫。”</p><p></p><p>认知智能在商业领域的应用正在不断扩大和深化，这种智能技术以数据为基础，通过先进的机器学习、自然语言处理等技术，帮助企业模拟人类的思维和行为，为决策提供更准确、可靠的数据支持。随着数据的不断增长和技术的持续进步，这种数据驱动的决策趋势将更加明显。</p><p></p><p>同时，当前不断变化的消费者需求正在促使企业提供更加个性化和精准的服务，而认知智能技术就可以深度理解消费者的兴趣和行为，为消费者提供高度个性化的产品和服务。这种个性化服务的趋势已经在电子商务、金融等领域得到了广泛应用，并取得了良好的效果，这种跨行业应用的趋势对各行业的数字化转型和创新发展起到了积极的推动作用。</p><p></p><p>然而，尽管认知智能具有重要性和广阔的前景，但它的应用仍面临一些行业挑战——首先数据隐私保护就是一个重大挑战。为了确保可持续、可信赖的认知智能应用，企业需要采取有效的措施和技术手段来保护客户数据的安全和隐私。其次，算法风险和偏见也是需要注意的问题。由于算法模型是由人类开发者设计和开发的，难以避免一些潜在的偏见和错误。这些偏见和错误可能会对企业的决策和消费者的体验造成不良影响。因此，需要加强算法设计和验证的规范性和严谨性，以降低算法风险的发生概率。</p><p></p><p>此外，智能化程度和可解释性也是认知智能应用中亟待解决的挑战。尽管目前认知智能技术已经取得了一定的进展，但在面对一些复杂的任务和问题时，它仍无法像人类一样进行灵活、全面的分析和解释，这可能会限制其在一些关键领域如医疗、金融等的应用和发展。为了解决这一问题，需要加强与人类专家的合作和交流，将人类的智慧和机器的智能相结合，提高整体解决方案的效率和准确性。</p><p></p><p>面对认知智能的研发现状，像百度、阿里等大型厂商其实是更有技术突破优势的——他们通常拥有庞大的数据资源，数据的丰富度和质量往往对模型的准确性和性能起到关键作用；同时他们可以投入更多的人力和物力来研究和发展认知智能技术，引进更多相关领域的顶尖人才，通过品牌信任度和市场份额更容易吸引到合作伙伴、渠道商进行产品应用化测试和产业链合作，从而可以进行更深入的研究和更快的迭代，更容易做出技术突破。</p><p></p><p>但如果创业公司想要冲出重围，就必须要寻找到一个差异化市场的有效策略，通过关注特定行业、领域或用户群体，提供更加个性化和专业的认知智能产品和服务，避免与大厂商在传统领域的直接竞争。但像智谱 AI 这样一向低调的创业公司，当其被资本压注后，往往就不得不走向台前，在未来他们将面临更大的挑战，甚至就在这个“百度、讯飞等厂商纷纷带来产品升级”的 10 月，智谱 AI 决定将于 27 日发布新一代基座模型，但智谱 AI 的表现到底如何，还是要靠技术说话，大家可以一起来关注一下。</p><p></p><p>但无论如何，大家也不要忘了一件事，不管是大型厂商，还是创业公司，在认知智能这个探索阶段，大家都属于“小马过河”，想要在这竞争激烈的市场环境中获得一席之地，持续技术创新和提升自身实力永远是到达成功彼岸的第一要素。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AXXtqD6xU6FghjsNE408</id>
            <title>智谱AI完成超25亿人民币融资，将用于基座大模型的进一步研发</title>
            <link>https://www.infoq.cn/article/AXXtqD6xU6FghjsNE408</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AXXtqD6xU6FghjsNE408</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Oct 2023 04:25:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 北京智谱华章科技有限公司, 融资, 大模型, 人工智能
<br>
<br>
总结: 北京智谱华章科技有限公司宣布今年已累计获得超25亿人民币融资，用于大模型的进一步研发，支撑行业生态的发展。智谱AI专注于做大模型的自研创新，已推出多个开源模型，成为国内大模型行业的佼佼者之一。公司在模型技术研发和市场落地策略上具备领先地位，有望成为全球认知智能平台领军者，推动人工智能技术的变革，为各行业的开发者赋能，加速迈向通用人工智能的时代。 </div>
                        <hr>
                    
                    <p>北京智谱华章科技有限公司（以下简称“<a href="https://www.infoq.cn/article/AGirUVdTebuhGKLHvR0a?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱AI</a>"”）宣布今年已累计获得超25亿人民币融资，参与方主要包括社保基金中关村自主创新基金（君联资本为基金管理人）、美团、蚂蚁、阿里、腾讯、小米、金山、顺为、Boss直聘、好未来、红杉、高瓴等多家机构及包括君联资本在内的部分老股东跟投，华兴担任独家财务顾问。据悉，上述融资将用于基座大模型的进一步研发，更好地支撑行业生态，与合作伙伴一同高速发展。</p><p>&nbsp;</p><p>一直以来，智谱AI专注于做大模型的自研创新。2020年，智谱AI开始了GLM预训练架构的研发，并训练了百亿参数模型GLM-10B。2021年，公司利用MoE架构成功训练出万亿稀疏模型，于次年合作研发了双语千亿级超大规模预训练模型GLM-130B，并基于此千亿基座模型开始打造大模型平台及产品矩阵。2023年，智谱AI推出了千亿基座的对话模型ChatGLM，并开源单卡版模型ChatGLM-6B，使得研究者和个人开发者进行微调和部署成为可能。当前，智谱AI的开源模型在全球下载量已超过1000万次。在细分领域方面，智谱AI也打造了AIGC模型及产品矩阵，包括生成式AI提效助手智谱清言、高效率代码模型CodeGeeX等。</p><p>&nbsp;</p><p>对于本次融资，顺为资本合伙人程天表示，随着数字化和智能化时代的到来，生成式AI通用模型逐渐成为新一轮科技创新的焦点。模型之于现代科技产品，犹如核心技术的“心脏”，承载着信息处理和智能决策的重要功能。“现阶段，智谱AI已成为国内大模型行业的佼佼者之一。它所提供的开源双语预训练语言模型GLM-130B和开源双语对话模型ChatGLM-6B都在行业内获得了广泛的认可。公司在模型技术研发上的能力和在市场落地策略上的前瞻性，都表明了其在国内市场取得阶段性领先地位。”</p><p>&nbsp;</p><p>君联资本总裁李家庆表示，人工智能产业处于快速发展阶段，商业化场景正从实验室走向产业化生产，人工智能技术将实现从感知智能到认知智能的新突破，在科技情报、虚拟数字人等领域，基于认知智能搭建的行业通用平台市场空间巨大。“大模型+大算力”是迈向通用人工智能的可行路径，未来基于大模型形成的变革性AI产业基础设施将改变当前单一模型对应单一任务的人工智能研发范式，多模态大模型将成为不同领域的共性平台技术。“目前，智谱AI已取得多项国际领先的AI技术突破，在超大规模智能模型训练技术体系中占据领先地位，已具备构建我国人工智能应用通用基础设施的实力，未来有望通过推动人工智能技术的变革，为大量行业的开发者赋能，形成智能应用生态，成长为全球认知智能平台领军者。”</p><p>&nbsp;</p><p>智谱AI表示，未来将基于完整的模型生态和全流程技术支持，继续为千行百业带来持续创新与变革，加速迈向通用人工智能的时代。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0CoFw5ByYmvBVqVtotg9</id>
            <title>传AMD上海将无差别裁员，可能赔偿N+7；微软新员工最高工资约265万；马斯克拟向X平台发帖新用户收费 | AI一周资讯</title>
            <link>https://www.infoq.cn/article/0CoFw5ByYmvBVqVtotg9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0CoFw5ByYmvBVqVtotg9</guid>
            <pubDate></pubDate>
            <updated>Sun, 22 Oct 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 马斯克X平台, 高通, 威马, OpenAI
<br>
<br>
总结: 马斯克X平台计划向新用户收费，高通将推出RISC-V架构智能穿戴芯片，威马回应破产传闻，OpenAI正在开发准确率高达99%的AI生成图片识别器。 </div>
                        <hr>
                    
                    <p></p><blockquote>马斯克X平台拟向发帖新用户收费：每年1美元；高通将推出RISC-V架构智能穿戴芯片；威马回应“破产跑路”传闻；OpenAI正开发“AI生成图片识别器”：准确率高达99%……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>字节跳动回应PICO将被关停传闻</h4><p></p><p></p><p>10月21日，有媒体报道称字节跳动将逐步放弃PICO业务，字节跳动相关负责人回应界面新闻称，此消息不实。PICO在正常运营，公司会长期投入XR业务。</p><p></p><h4>科大讯飞回应美国AI芯片出口管制：华为昇腾910B能力基本可对标英伟达A100</h4><p></p><p></p><p>10月19日晚间，科大讯飞在三季报业绩说明会上针对近日美国政府进一步收紧对尖端人工智能芯片的出口管制事项表示，前述管制对将大模型构建在英伟达算力上的大模型厂商会产生较大影响，但科大讯飞已于2023年初与华为昇腾启动专项攻关，合力打造我国通用人工智能新底座，当前华为昇腾910B能力已经基本做到可对标英伟达A100。在即将举行的科大讯飞1024全球开发者节上，公司和华为在人工智能算力底座上将有进一步联合发布。</p><p></p><h4>美国商务部澄清: RTX 4090显卡可在中国零售，不能生产</h4><p></p><p></p><p>10月18日，美国商务部就ECCN 3A090更新高科技出口管制政策作出澄清，关于3A090b部分NVIDIA GeForce RTX 4090显卡的禁令，美国商务部允许就在出口消费性应用作出豁免，可以中国零售市场贩售。</p><p></p><p>GeForce RTX 4090显卡可以在中国 (包括香港及澳门) 消费性市场继续零售，但不可以输入 RTX 4090晶片作商用及生产用途，意味着中国代工厂不能生产 GeForce RTX 4090，但普通消费者还是渠道能买到NVIDIA GeForce RTX 4090 。目前RTX4090公版/非公版显卡陆续下架，店铺均为无货状态。这也导致部分拥有RTX 4090库存的商家对其定价直接跳涨，多达2万元以上，甚至出现近5万元的高价。</p><p></p><h4>传AMD上海将无差别裁员，图形部门是重灾区</h4><p></p><p></p><p>近期，传AMD开始在中国裁员。据了解，10月25日当天AMD上海内部的会议室都被HR预订完（裁员前兆），裁员规模可能为10%-15%，涉及300-450名左右的员工，RTG部门（Radeon Technologies Group）是重灾区。AMD本次裁员的具体赔偿方案未定，参考其他外企在中国的裁员情况，网络上流传的N+4、N+7赔偿也有一定可能。</p><p></p><p>AMD上海研发中心成立于2006年，是AMD公司在美国本土以外最大的研发中心，员工总数约3000名。据悉，该中心主要负责CPU、GPU和APU等产品的设计、开发和测试等工作，曾经为AMD贡献了不少创新的技术和产品，例如Ryzen系列处理器、Radeon系列显卡等。</p><p></p><h4>诺基亚将裁员至多1.4万人以削减成本</h4><p></p><p></p><p>10月19日，诺基亚公布第三季度财报，营收49.8亿欧元，预估57亿欧元；第三季度调整后营业利润4.24亿欧元，预估5.564亿欧元，营收和利润表现均不及预期。</p><p></p><p>同日，诺基亚宣布调整营运策略，目标是到2026年底总成本基础较2023年降低8亿至12亿欧元，特别是人员开支将减少10%-15%。诺基亚目前有86000名员工，该计划预计将使员工人数降至72000-77000名，最多或减少14000人。</p><p></p><p>诺基亚表示，此次受到降本增效措施影响的部门主要包括移动网络、云和网络服务两大子业务部门以及集团职能部门，成本缩减规模将取决于终端市场的需求变化。</p><p></p><h4>B站广州研发工作室宣布解散，高峰期曾有400多人</h4><p></p><p></p><p>据报道，B站广州研发工作室日前宣布解散，CEO丁黔伟也将离开公司。一款日漫IP改编项目因有外部合作合同而被保留了下来，其他项目均被砍掉。也就意味着这个原本有400多人的研发工作室，经历了两次大裁员之后剩下约60人，被留下的人也处于等待合同结束离开的状态。</p><p></p><p>广州研发工作室是2022年B站收购广州心源互动而来。后者成立于2019年，最早是由丁黔伟与他人共同出资设立，公司专注于动作类游戏的研发，其高管曾在采访中表示“每个制作人都有10年以上做动作游戏的经验”。</p><p></p><h4>OpenAI正开发“AI生成图片识别器”：准确率高达99%</h4><p></p><p></p><p>据报道，OpenAI正在开发一款新工具，能够以相当高的精度判断一张图片是否是人工智能所绘制。</p><p></p><p>除了聊天机器人和撰稿工具ChatGPT之外，OpenAI公司还开发过一款名为“DALL-E”的人工智能图片生成器。当地时间周二，该公司首席技术官米拉·穆拉蒂（Mira Murati）透露，该公司开发的AI图片识别工具，能够以99%的准确率判断一张图片是否是人工智能工具所绘制。</p><p></p><p>穆拉蒂介绍，这款图片识别工具正在进行内部测试，后续将会发布，但是她并未提供很具体的发布时间。</p><p></p><h4>百度发布文心大模型4.0</h4><p></p><p></p><p>10月17日，百度发布文心大模型4.0。百度CEO李彦宏表示，文心大模型4.0是迄今为止最强大的文心大模型，实现了基础模型的全面升级，在理解、生成、逻辑和记忆能力上有着明显提升，综合水平与GPT4相比已经毫不逊色。</p><p></p><h4>贾跃亭：FF市值仅0.2亿美元，自己“心急如焚”</h4><p></p><p></p><p>日前，Faraday Future公司创始人兼首席产品及用户生态官贾跃亭发表致全体股东和投资人公开信。贾跃亭在信中表示：虽然公司成功达成一个又一个重大里程碑，为业务发展提供了坚实的基础，但股价和市值却遭遇了最至暗时刻，公司市值跌到仅为30多亿美元现金投入总额的1%都不到，我们对此感到无比痛心和失望，相信包括公众投资者在内的所有热爱公司的股东们也都无比痛心和失望。</p><p></p><p>贾跃亭在信中表示，FF将推出六大举措坚决捍卫股东利益。包括：对潜在的非法卖空行为展开调查；公司核心层增持；继续全力积极引入潜在战略投资者；进行组织升级；降低运营和供应链成本；让外界和投资人更好的了解FF的发展现状和真实价值等。贾跃亭最后表示：“对于FF真实价值和资本市场价值严重背离的表现，我其实是最心急如焚的，对没有给所有股东和投资人带来应有的价值回报深感惭愧，我也最希望全力推动实现FF价值最大化。”</p><p></p><h4>威马回应“破产跑路”传闻</h4><p></p><p></p><p>10月17日，有报道称，目前处于破产重整过程中的威马汽车仅剩800余名员工且已超四个月停发工资，随即威马汽车冲上热搜。</p><p></p><p>10月18日，威马汽车官方微博发文称，威马汽车并未申请破产，目前公司核心岗位运营正常，也不存在公司创始人跑路海外的情况。</p><p></p><p>该篇澄清声明指出，威马汽车目前正在进行的是经上海第三中级人民法院受理的预重整阶段。预重整不同于破产重整，是在企业面临困境的早期阶段进行的自救行为，旨在通过重组债务引进战略投资人，避免破产，实现重生蜕变。</p><p></p><p>针对创始人跑路的消息，威马汽车方面表示，目前公司的重点包括售后服务、复工复产、出口业务、海外合作、国际融资以及引进战略投资人等事项，已取得一定进展，公司会寻找合适的时机对外公布。“公司创始人沈晖于1991年赴美留学，在欧美多地工作和居住多年，近期工作重心以海外事项为主，因此不存在公司创始人跑路海外的情况。”</p><p></p><h4>马斯克X平台拟向发帖新用户收费：每年1美元</h4><p></p><p></p><p>10月18日凌晨，马斯克在X上引用了X的官方消息称，要向在网上发帖或与其他用户互动的新账户收取每年1美元的订阅费。据称该措施是为了减少X平台上，垃圾邮件、自动机器人账户和其服务的操纵。马斯克发帖称：“阅读免费，但写作每年1美元。这是不阻止真实用户的情况下对抗机器人的唯一方法。”</p><p></p><p>据了解，这项名为“NotaBot”的测试已在新西兰和菲律宾推出。不支付费用的新用户将无法在网站上执行某些操作，包括发帖、点赞、回复或添加书签。“这将帮助我们打击X上的机器人和垃圾邮件发送者，以增加平台的可访问性，现有用户不受这次测试的影响。</p><p></p><h4>高通将推出RISC-V架构智能穿戴芯片</h4><p></p><p></p><p>据路透社报道，移动处理器大厂高通于17日宣布，在与谷歌长期合作基础上，将推出支持Wear OS系统（基于 Android）的RISC-V构架的智能穿戴芯片，并将在全球市场进行商用推广。双方还持续投资高通的RISC-V Snapdragon Wear平台，高通也将成为Wear OS生态系的智能穿戴芯片供应商。</p><p></p><p>对于谷歌和高通而言，随着新款RISC-V智能穿戴芯片的即将推出，是商用RISC-V Android项目的首次尝试，这也是有史以来第一款宣布面向消费大众市场的RISC-V Android芯片。</p><p></p><h4>百川智能获3亿美元战略投资</h4><p></p><p></p><p>近日，百川智能宣布已完成A1轮战略融资，融资金额3亿美元，阿里、腾讯、小米等科技巨头及多家顶级投资机构均参投了本轮融资。加上天使轮的5000万美元，其融资金额已达3.5亿美元，成立不到半年时间便跻身科技独角兽行列，创下国内大模型初创企业晋升独角兽速度之最。</p><p></p><p>百川智能成立于2023年4月10日，由前搜狗公司CEO王小川创立。其核心团队由来自搜狗、Google、腾讯、百度、华为、微软、字节等知名科技公司的AI顶尖人才组成，目前团队规模170余人，其中硕士及硕士以上学历员工占比近70%，研发人员占比超80%。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>微软新入职员工最高基本工资约265万，最低约31万</h4><p></p><p></p><p>近日，根据国外科技媒体披露的一份微软内部文档显示，新入职成员最高基本工资为 23.17 万美元至 36.15 万美元（当前约 169.6 万元至 264.6 万元人民币），此外还有最高 120 万美元（当前约 878.4 万元人民币）的入职奖金，以及 100 万美元（当前约 732 万元人民币）的年度股票奖励。</p><p></p><p>目前尚不清楚这份披露的文档适用于所有新入职员工，还是仅限于某些特定的组织角色，这份文档也显示在纽约和旧金山地区的微软新入职员工工资更高。文档显示级别最低的新入职员工至少年薪为 4.25 万美元（当前约 31.1 万元人民币），但没有入职奖金和年度股票奖励。而最高级别为 70 等级，基本工资在 23.17 万美元至 36.15 万元之间，此外还可以获得 31 万美元到 120 万美元的入职奖金。</p><p></p><h4>接受开发者“贿赂”，至少 5 名国内 App Store 员工被苹果开除</h4><p></p><p></p><p>据外媒报道，至少有五名在中国App Store工作人员因与开发者和顾问有不当行为而被开除。</p><p></p><p>据了解，App Store负责审核和编辑的员工不应该与开发者见面，但一次内部审计引起了对半打员工的怀疑，并导致了调查。苹果公司发现，这些员工接受了开发商和顾问提供的免费餐饮和夜店活动，而这些开发商和顾问经营的企业声称他们可以让游戏在App Store上架。</p><p></p><p>涉案员工虽无法让应用程序在App Store上架或删除，但他们确实有权对App Store中的某款应用程序进行专题报道，通过更多的曝光量被用户所看到。目前尚不清楚这些员工是否以不适当的方式展示了与他们互动的开发者的应用程序。</p><p></p><p>苹果发言人在一份声明中表示，苹果希望员工遵守其商业行为政策。“苹果公司希望全球员工遵守其严格的道德和商业行为政策，但不对本报告中的具体指控发表评论。我们会对不当行为的报告进行彻底调查，并在适当的时候采取包括解雇在内的行动。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tq5VmzbUvckWKG4T9axk</id>
            <title>阿里达摩院发布遥感AI大模型，可识别小麦长势、助力农情管理</title>
            <link>https://www.infoq.cn/article/tq5VmzbUvckWKG4T9axk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tq5VmzbUvckWKG4T9axk</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Oct 2023 07:47:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 遥感AI大模型, 农田识别, 地表万物, 遥感应用
<br>
<br>
总结: 阿里达摩院发布了业内首个遥感AI大模型，该模型可以识别农田、农作物、建筑等地表万物，提升遥感应用的分析效率。这个模型可以结合卫星照片和历史气象情况，准确地分析农田里作物的长势状况，让种地更主动地“看天吃饭”。通过一个模型解决多个任务，该模型在遥感领域实现了图像分割的任务统一，可以实现“万物零样本”的快速提取。此外，该模型还支持多模态交互，用户可以根据需要定制不同的遥感AI解译功能。 </div>
                        <hr>
                    
                    <p>10月20日，阿里达摩院发布业内首个遥感AI大模型，一个模型即可识别农田、农作物、建筑等地表万物，让AI进一步下沉到田间地头，大幅提升灾害防治、自然资源管理、农业估产等遥感应用的分析效率，该模型已在AI&nbsp;Earth地球科学云平台开放使用。</p><p>&nbsp;</p><p>遥感技术在城市运营、耕地保护、应急救灾等国计民生中的应用甚广，遥感AI则可以大幅提升既有数据的利用深度，输出更精细化、更准确的分析结果，如结合卫星照片与历史气象情况，“算”出某一块农田里作物的长势状况，让种地不再被动，而是更主动地“看天吃饭”。</p><p>&nbsp;</p><p>以往，由于遥感卫星的影像数据规模巨大、地物分类复杂，要识别不同的地表物体，需要分别训练多个专用的遥感模型，且单个模型存在识别准确率低、泛化性差等问题。2023年4月，Meta发布的论文《Segment&nbsp;Anything》让计算机视觉进入快速迭代的大模型时刻，也推动遥感AI朝着“一个模型解决多个任务”的方向发展。</p><p>&nbsp;</p><p>达摩院此次提出的遥感AI解译通用分割模型（AIE-SEG），率先在遥感领域实现了图像分割的任务统一，一个模型即可实现“万物零样本”的快速提取，可识别农田、水域、建筑物等近百种遥感地物分类，且多项任务处理下依旧保持高精度的识别，还能根据用户的交互式反馈自动调优识别结果。在一些特定场景下，对比传统的遥感模型，实例提取的准确率可提升25%，变化检测的准确率可提升30%。</p><p>&nbsp;</p><p></p><p><img src="https://static001.infoq.cn/resource/image/46/78/46bedf2981bc6f532c6a5a209a3cf878.png" /></p><p>图说：该模型支持多模态交互，如输入“提取影像中的耕地农田”，会自动识别所选目标</p><p>&nbsp;</p><p>基于上述的基础能力，遥感AI大模型提供“开箱即用”的API调用服务，用户可根据不同需要，定制不同的遥感AI解译功能，如水体提取、耕地变化监测、光伏识别等。</p><p>&nbsp;</p><p>山东省国土测绘院自2022年起与达摩院在自然资源调查、耕地保护等领域展开合作，调用遥感AI大模型进行山东全省冬小麦的长势监测研究，识别精度达到90%以上，有效提升了冬小麦遥感解译的效率，帮助农业管理者更好地预测粮食产量、提升农业生产效益。</p><p>&nbsp;</p><p>国家自然灾害防治研究院基于遥感AI大模型进行滑坡和倒塌建筑物的识别，在历史的自然灾害区域遥感图像的测试中，提取这些受灾信息仅需十几分钟时间，相比人工识别方式效率提升数十倍，为科学救灾提供高效、精准的遥感分析支持。</p><p>&nbsp;</p><p>达摩院视觉技术实验室AI&nbsp;Earth算法负责人罗浩表示，遥感多模态是推进人类更好地理解地球的必由之路，达摩院将持续推进遥感AI大模型的研究，以AI助力地球科学的探索与应用。</p><p>&nbsp;</p><p>AI Earth是达摩院于2022年发布的一站式地球科学云平台，基于深度学习、计算机视觉、地理空间分析等技术积累，提供多源观测数据的云计算分析服务，目前与国内50+高校建立合作，相关技术已应用于水利部、国家气象中心、生态环境部等机构。</p><p>&nbsp;</p><p>附：达摩院遥感AI大模型使用入口</p><p>https://engine-aiearth.aliyun.com/#/app/aie-seg</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NWvmKhGUYAjYjdIpAedo</id>
            <title>Alluxio AI全新产品发布：无缝对接低成本对象存储AI训练解决方案</title>
            <link>https://www.infoq.cn/article/NWvmKhGUYAjYjdIpAedo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NWvmKhGUYAjYjdIpAedo</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Oct 2023 10:14:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Alluxio Enterprise AI, 数据平台公司, 企业数据基础设施, 人工智能, 机器学习
<br>
<br>
总结: Alluxio Enterprise AI是一家数据平台公司推出的新产品，旨在满足企业数据基础设施对于人工智能和机器学习的需求。该产品能够优化企业AI和分析基础设施的性能、数据可访问性、可扩展性和成本效益，助力下一代数据密集型应用的发展。 </div>
                        <hr>
                    
                    <p>（2023年10月19日，北京）Alluxio作为一家承载各类数据驱动型工作负载的数据平台公司，现推出全新的Alluxio Enterprise AI高性能数据平台, 旨在满足人工智能 (AI) 和机器学习 (ML) 负载对于企业数据基础设施不断增长的需求。 Alluxio Enterprise AI 平台可综合优化企业AI和分析基础设施的性能、数据可访问性、可扩展性和成本效益，助力生成式AI、计算机视觉、自然语言处理、大语言模型和高性能数据分析等下一代数据密集型应用的发展。</p><p>&nbsp;</p><p>为保持竞争力并在竞争中脱颖而出，各家企业都在全力推进数据和AI基础设施的现代化。在此过程中，企业家们也意识到传统的数据基础设施已经无法匹配下一代数据密集型AI负载的需求。在AI项目推进中经常遭遇的各类挑战，诸如性能低下、数据可访问性差、GPU 稀缺、数据工程复杂以及资源未充分利用等，都严重妨碍了企业获取数据价值。 <a href="https://emtemp.gcom.cloud/ngw/globalassets/en/publications/documents/2023-gartner-top-strategic-technology-trends-ebook.pdf">Gartner</a>"® 研究称，“可操作AI的价值在于能够在企业的各种环境下进行快速开发、部署、调整和维护。考虑到工程复杂性和更快的市场响应需求，开发较为灵活的AI工程数据流，构建能够在生产中进行自适应的AI模型均至关重要” ，“到 2026 年，采用AI工程来构建和管理自适应AI系统的企业，将在AI模型可操作性方面至少超越同行 25%。”</p><p>&nbsp;</p><p>Alluxio 创始人兼CEO李浩源表示：“Alluxio用最先进的大数据和Al平台为全球头部企业客户赋能，今天我们又向前迈出了一大步”， “Alluxio Enterprise AI 为客户提供高效的AI 解决方案，帮助企业加速 AI工作负载并最大限度地获取数据价值。未来的企业领导者将知道如何利用变革性AI来推进数据驱动，通过最新技术来构建和维护AI基础设施，实现超高性能、无缝访问和便捷管理。”</p><p>&nbsp;</p><p>此次新版发布后，Alluxio 即从一种产品扩展到两种产品组合——Alluxio Enterprise AI 和 Alluxio Enterprise Data，全面满足分析和AI的多样化需求。Alluxio Enterprise AI作为一款全新产品，建立在Alluxio企业版多年积累的分布式系统经验上，采用了针对AI/ML负载优化的新架构。 Alluxio Enterprise Data 是 Alluxio 企业版大数据方向的下一代版本（与Alluxio Enterprise AI平行），并将继续成为专注分析负载企业的理想选择。</p><p></p><h1>加速端到端机器学习工作流</h1><p></p><p>&nbsp;</p><p>Alluxio Enterprise AI 使得企业的AI基础设施能够在现有数据湖上实现高性能运行、无缝数据访问、可扩展且经济高效。它能帮助数据和AI领域的领导者和从业者实现AI项目的四个关键目标：1.高性能模型训练和部署，快速产生业务成效；2.跨区域和跨云负载可无缝访问数据；3.可无限扩展，已经互联网巨头内部严格测试；4. 无需使用昂贵的专用存储，在现有技术栈上即可部署，确保投资回报最大化。企业使用 Alluxio Enterprise AI后，预期训练速度可比使用提供商业服务的对象存储快达 20 倍，模型服务速度提升高达10 倍，GPU利用率达90%以上，AI 基础设施成本节约高达 90%。</p><p>&nbsp;</p><p>Alluxio Enterprise AI 拥有包含去中心化元数据的分布式系统架构，可消除访问海量小文件（常见于AI 负载）时的性能瓶颈。无论文件大小或数量如何，都能确保具备超越传统架构的无限扩展性。与传统分析不同，分布式缓存是根据 AI 负载 I/O 模式量身定制的。此外，还支持分析负载以及从数据摄取到 ETL（提取、转换、加载）、预处理、训练和服务的完整机器学习工作流 。</p><p>&nbsp;</p><p>Alluxio Enterprise AI 包含以下重要特性：</p><p>&nbsp;</p><p>性能出色的模型训练和模型服务——Alluxio Enterprise AI 显著提升企业在现有数据湖上的模型训练和服务性能。用于模型训练的强化API 集可实现优于商业化对象存储20 倍的性能。对于模型服务，Alluxio 提供超高并发性，在将离线训练集群中的模型用于在线推理时实现高达10 倍的速度提升。适合AI工作负载I/O模式的智能分布式缓存——Alluxio Enterprise AI的分布式缓存功能使得AI引擎能够通过高性能Alluxio缓存（而非缓慢的数据湖存储）来读写数据。 Alluxio的智能缓存策略专门针对AI引擎的I/O模式量身定制，包括大文件顺序访问、大文件随机访问和海量小文件访问。该优化帮助需要大量数据的GPU实现高吞吐和低延迟。训练集群持续从高性能分布式缓存中获取数据，可实现90%以上的GPU利用率。跨本地和云环境的AI 工作负载实现无缝数据访问 - Alluxio Enterprise AI 为企业提供了统一的管理界面，可以轻松管理跨不同基础设施环境的 AI 工作负载。该产品为机器学习工作流提供了真实的数据源，从根本上消除了大型企业数据湖孤岛的瓶颈。通过 Alluxio Enterprise AI 这一标准数据访问层，企业可以在不同业务部门和地理位置之间实现数据的无缝共享。经过大规模严格测试的全新分布式系统架构- Alluxio Enterprise AI 平台构建在创新的去中心化架构 DORA（去中心化对象存储库架构）之上。该架构为AI工作负载提供了无限扩展的基础，允许 AI 平台通过包括Amazon S3 在内的商业化对象存储处理多达1000 亿个对象。该新架构借助Alluxio在分布式系统方面的成熟专业知识，解决了系统可扩展性、元数据管理、高可用性和性能方面不断增长的挑战。</p><p>&nbsp;</p><p>&nbsp;Enterprise Strategy Group 分析师 Mike Leone 表示：“随着组织在整个业务范围内扩展AI的应用，优化下一代工作负载过程中的性能、成本和 GPU 利用率变得至关重要” ，“Alluxio 拥有极具优势的产品，能真正帮助数据和 AI 团队实现更高的性能、无缝的数据访问，以及模型训练和模型服务的便捷管理。”</p><p>&nbsp;</p><p>“我们与 Alluxio 合作密切，Allxuio平台对我们的数据基础设施至关重要，”Aunalytics 分析云工程总监 Rob Collins表示， “Aunalytics对于Alluxio新推出的针对企业AI的分布式系统十分期待，并看好新产品在AI 行业的巨大潜力。”</p><p>&nbsp;</p><p>“公司内部训练的大语言模型为我们的问答应用和推荐引擎提供支持，极大地增强了用户体验和参与度”，知乎数据平台团队软件工程师胡梦宇表示， “在我们的AI基础设施中，Alluxio 处于核心地位。在使用 Alluxio 作为数据访问层后，我们的模型训练性能提升了3 倍，部署性能提升了10 倍，GPU 利用率翻倍。Alluxio的Enterprise AI平台采用全新的DORA架构，能支持访问海量小文件，对此我们十分期待。在AI浪潮即将到来的时刻，Alluxio新产品让我们在支持AI应用方面更有信心。”</p><p></p><h1>在机器学习工作流中部署Alluxio</h1><p></p><p></p><p><a href="https://www.gartner.com/en/webinar/452057/1065295">Gartner</a>"&nbsp;研究显示，数据可访问性和数据量/复杂性是组织应用AI技术中遇到的三大难题之一。 Alluxio Enterprise AI可以添加到由AI计算引擎和数据湖存储组成的已有AI基础设施中。 Alluxio 位于计算和存储中间，可以在机器学习工作流中跨模型训练和模型服务工作，从而实现最大速度和最优成本。例如，将 PyTorch 作为训练和服务引擎， Amazon S3为现有数据湖：</p><p>&nbsp;</p><p>模型训练：当用户训练模型时，PyTorch数据加载器从虚拟本地路径/mnt/alluxio_fuse/training_datasets加载数据集。数据加载器不会直接从 S3 加载数据，而是从 Alluxio 缓存加载。在训练过程中，缓存的数据集将在多个epoch中使用，因此整个训练速度不再受制于访问S3而产生的瓶颈。也就是说，Alluxio通过缩短数据加载来加速训练，消除GPU空闲等待时间，提高GPU利用率。模型训练完成后，PyTorch通过Alluxio将模型文件写入S3。模型服务：最新训练的模型需要部署到推理集群。多个TorchServe实例同时从S3并发读取模型文件。Alluxio会缓存这些来自S3的最新模型文件，并以低延迟提供给推理集群。因此，最新模型一旦可用时，下游的AI应用即可将其用于推理。</p><p></p><h1>平台与现有系统集成</h1><p></p><p>&nbsp;</p><p>要将Alluxio与现有平台集成，用户可以在计算引擎和存储系统之间部署Alluxio集群。在计算引擎侧，Alluxio 可与 PyTorch、Apache Spark、TensorFlow 和 Ray 等流行的机器学习框架无缝集成。企业可以通过 REST API、POSIX API 或 S3 API 将 Alluxio 与这些计算框架集成。</p><p>&nbsp;</p><p>在存储侧，Alluxio 可连接位于任何位置（本地、云端或两者兼有）的各类文件系统或对象存储。支持的存储系统包括 OSS、COS、BOS、OBS、Amazon S3、Google GCS、Azure &nbsp;Blob Storage、MinIO、Ceph、HDFS等。</p><p>&nbsp;</p><p>Alluxio 可在本地和云端、物理机或容器化环境中运行。支持的云平台包括阿里云、腾讯云、百度云、华为云、AWS、GCP、Azure Cloud等。</p><p>&nbsp;</p><p>下载资源</p><p></p><p>Alluxio Enterprise AI 下载链接：<a href="https://www.alluxio.io/download/">https://www.alluxio.io/download/</a>"</p><p>&nbsp;</p><p>AI Infra Day</p><p></p><p>在美西时间10 月 25 日的AI Infra Day 上，Alluxio 将首次公开展示其最新发布的 Alluxio Enterprise AI平台。AI Infra Day是面向开发者的线上活动，主要探讨构建高性能、可扩展且经济高效的 AI 基础设施中的挑战及各种方案。特邀嘉宾包括Wanchao Liang（Meta ）、 Sally (Mihyoung) Lee（Uber） 和范斌（Alluxio）。活动现已开放报名：<a href="https://www.alluxio.io/ai-infra-day-2023/%E3%80%82">https://www.alluxio.io/ai-infra-day-2023/。</a>"</p><p>&nbsp;</p><p></p><blockquote>关于Alluxio&nbsp;Alluxio 是全球领先的针对分析和AI的高性能数据平台提供商，可加速企业AI产品价值变现，并最大化基础设施的投资回报率。Alluxio数据平台位于计算与存储系统之间，能够在数据工作流的各个阶段为数据平台上的工作负载提供统一视图。无论数据位于何处，该平台均可提供高性能的数据访问，简化数据工程，提高GPU利用率，并降低云计算和存储成本。企业无需使用专用存储，即可大幅加速模型训练和模型服务，并在现有数据湖上构建AI基础设施。Alluxio在头部投资者的支持下， 为全球科技、互联网、金融和电信企业提供服务，目前全球排名前 10 的互联网公司中有 9 家在使用Alluxio。了解更多信息，请访问 www.alluxio.com.cn。&nbsp;</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0AbAmTSduzAuVFFu29Nq</id>
            <title>一夜之间，有价无货！英伟达消费级 RTX 4090显卡遭全面下架，最高售价接近4万</title>
            <link>https://www.infoq.cn/article/0AbAmTSduzAuVFFu29Nq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0AbAmTSduzAuVFFu29Nq</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Oct 2023 06:08:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RTX4090, 下架, 新限令, AI芯片
<br>
<br>
总结: 北京时间10月18日下午，英伟达旗舰显卡RTX4090开始全面下架。这次下架是受到美国新限令的影响，限制向中国出售更先进的AI芯片。新规将在未来30天内生效，限制了英伟达等公司对华供应。这一限制对于GPU行业、服务器行业、算力行业以及AI行业从业者来说是一个重大影响。 </div>
                        <hr>
                    
                    <p>北京时间10月18日下午，英伟达顶级旗舰显卡 RTX4090 开始全面下架。</p><p>&nbsp;</p><p>目前，当前在京东搜索 “RTX 4090 显卡”只有少数第三方售卖，但需要预约等待到货。 同样，在淘宝搜索也是如此，标注价格基本2万起步，最高甚至接近4万元。而在二手平台咸鱼上，RTX4090售价基本1.2万起步。华硕、微星、影驰等英伟达合作商也同样纷纷下架该型号的非公显卡，官方旗舰店均已显示无货状态。</p><p><img src="https://static001.geekbang.org/infoq/4c/4c0dc2603d70908566d865fefe26c49e.jpeg" /></p><p></p><p></p><h2>“新限令”的结果</h2><p></p><p>&nbsp;</p><p>显然，这次消费级显卡 RTX4090 的下架是受当天美国“新限令”的影响。</p><p>&nbsp;</p><p>北京时间10 月 18 日，美国商务部宣布，计划限制向中国出售更先进的 AI 芯片。据悉，新的政策将限制 Nvidia A800 和 H800 芯片的出口，此外，新规将豁免笔记本电脑、智能手机和游戏设备中使用的大多数消费级芯片，但其中部分芯片仍须受到美国官员的批准和专项管控。相关规定将在未来 30 天内生效。</p><p>&nbsp;</p><p>10 月 16 日晚，美国商务部长 Gina Raimondo 表示，新措施弥补了去年 10 月所发布法规中的漏洞，未来可能“至少每年更新一次”。她解释称，此番措施的目标是限制中国获取“先进半导体，这些半导体能够推动 AI 技术发展以及对军事应用具有重大意义的复杂计算机突破”，并强调美国政府无意在经济上打压中方。</p><p>&nbsp;</p><p>有分析指出，去年10月美国实施原有的AI芯片管制规定后，英伟达推出H100和A100阉割版，分别为H800和A800，处理速度约为对应芯片的70%，它们仍可用于人工智能应用上。而本次的新限制则以“性能密度”（以每平方毫米的浮点运算次数来衡量）取代芯片间通信速度，旨在阻止公司寻找“绕过”方法。这意味着不论英伟达还是英特尔、AMD，按照算力性能密度的要求，新产品可能基本没有办法对华供应。</p><p>&nbsp;</p><p>另外，新规还扩大了半导体制造设备的出口管控，包括强化对美国人才的限制，还对中国以外的 21 个国家提出了芯片制造工具出口管控要求，原因是担心这些设备可能被转移给中国或其他国家，进而引发安全问题。</p><p>&nbsp;</p><p>更多详情可查看：</p><p><a href="https://mp.weixin.qq.com/s/IFazU7qhHkkmNWKrwFqDhw">突发！美国限制向中国出口 Nvidia H800 等先进 AI 芯片，壁仞科技、摩尔线程等中国 GPU 芯片企业被列入实体名</a>"</p><p>&nbsp;</p><p>“今夜对于无数GPU行业、服务器行业、算力行业以及AI行业从业者来说都是不眠之夜，就连消费级的4090显卡都从每张1.5万跳涨到2.5万。在高度全球化的今天，一纸大洋彼岸的禁令就这样荒谬且真实地影响了国产大模型和人工智能发展的进程。”有业内资深人士称。</p><p>&nbsp;</p><p>附：美国对华半导体制裁记录</p><p>&nbsp;</p><p>2018年10月，美国商务部发布公告，将福建晋华集成电路有限公司列入商务部实体名单，禁止美国企业向福建晋华出售技术和产品；2019年5月，美国商务部正式将华为列入“实体清单”，禁止美企向华为出售相关技术和产品2020年5月，美商务部公告将延长华为的供货临时许可证90天至8月14日，但同时升级了对华为的芯片管制，以限制华为使用美国技术软件在国外设计和制造半导体的能力；2020年12月，美国商务部以“违反美国国家安全或外交政策利益”为由，宣布将中芯国际列入“实体清单，这就意味着中芯国际生产10nm以下芯片所需要的原料和设备无法获得美国批准出口；2022年10月，美国BIS公布对中国出口管制新规，主要针对先进芯片和芯片制造设备领域；2022年11月，美国向日本和荷兰施压，要求两国的芯片制造领域相关企业立即禁止向中国出售产品，阻止先进芯片技术流入中国；2022年12月，美国商务部决定将包括长江存储、寒武纪、上海集成电路研发中心、上海微电子、深圳鹏芯微等在内的36家中国实体 (包括一家长江存储日本子公司) 加入实体清单。</p><p></p><h2>英伟达的应对策略？</h2><p></p><p>&nbsp;</p><p>对于“新限令”，英伟达方面回应称：“我们遵守所有适用的法规，同时努力提供支持不同行业的数千种应用产品。鉴于全球对我们产品的需求，我们预计（新规）短期内不会对我们的财务业绩产生实质性的影响。”</p><p>&nbsp;</p><p>不过，英伟达的市场表现并没有英伟达官方说的那么乐观。</p><p>&nbsp;</p><p>美东时间10月17日周二，美股盘中，英伟达（NVDA）一度重挫7.8%，创2022年12月以来最大盘中跌幅。截至收盘，英伟达跌4.68%，报收439.38美元，市值一夜蒸发超535亿美元（≈4000亿元人民币），最新市值1.09万亿美元。英特尔、AMD也分别收跌1.4%、1.2%，美股芯片股合计蒸发730亿美元（约合5343亿元）市值。</p><p>&nbsp;</p><p>在此背景下，人们更加确信之前爆出英伟达将推出RTX 4080 Super&nbsp;的消息。根据 @hongxing2020 爆料消息，英伟达将带来三款 RTX 40 系 SUPER 显卡，分别为 RTX 4080 SUPER、RTX 4070 Ti SUPER、RTX 4070 SUPER。有媒体求证得知，目前 3 款 SKU 基本上已经确认，但并未拿到具体信息，只知道新版 RTX 4080 将会采用 20GB GDDR6X 显存。</p><p></p><h2>大企业“备货充足”</h2><p></p><p>&nbsp;</p><p>与美股芯片股反应相反，10月18日，A股算力芯片概念股普遍上涨，好利科技一字涨停，寒武纪、弘信电子盘中大涨超10%，景嘉微、海光信息等纷纷收涨。</p><p>&nbsp;</p><p>在芯片管制措施升级消息曝出后不久，部分公司对外透露称“影响不大”、“备货充足”等。10月17日晚，恒润股份公告显示，其控股子公司上海润六尺向供应商A采购75台H800及22台A800现货，合计合同金额约2亿元。腾讯、百度等大厂也表示，“囤货充足”。但中小型AI公司的日子可能不太好过。</p><p>&nbsp;</p><p>当前，国内以大模型为代表的AI领域正在迅速发展。根据TortoiseIntelligence发布的AI指数，对世界各国人工智能进行排名，综合来看，我国仅次于美国排名第二，单项指标中，发展指标和政府策略指标更是位居首位。但新规的发布就是国内AI发展的“绊脚石”。</p><p>&nbsp;</p><p>在美对华持续制裁背景下，算力自主可控需求日益增长。IDC最新数据指出，中国本土云端AI加速芯片制造上正在快速增长，2023年上半年，中国AI服务器使用了50万块本地采购/开发的AI加速芯片。其中，华为、寒武纪、海光等国产算力被寄予厚望。</p><p>&nbsp;</p><p>浙商证券指出，国内算力芯片的发展速度取决于上游供应及下游的迭代速度，因而供应及生态体系较为完善的华为鲲鹏升腾芯片有望最先获益，具备较强技术积累和生态兼容性的海光也有望迎来更大的市场空间。</p><p>&nbsp;</p><p>科大讯飞创史人刘庆峰就曾表示，华为的GPU能力现在已经跟英伟达 A100 一样，现在已经做到对标英伟达的 A100。而前不久，华为Mate 60系列引发抢购热潮，主要因为Mate 60系列顺利上市象征着华为突破美国封锁制裁，取得阶段性胜利。全球著名半导体行业观察机构TechInsights 公开发布了对Mate60 Pro 的拆解报告：Mate60 Pro搭载了新型麒麟9000s芯片，并采用了先进的7纳米。</p><p>&nbsp;</p><p>“接下来，我们每个从业者的选择在共同定义未来，囤货赚快钱或者埋头苦干寻找替代。明天太阳照常升起，卡贩子猫博士也会继续战斗下去。”上述提到的资深人士表示。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xhlku65TOzhUtKR2yaSi</id>
            <title>最新预测：2026年逾80%企业将采用生成式AI，相比当下增长16倍</title>
            <link>https://www.infoq.cn/article/Xhlku65TOzhUtKR2yaSi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xhlku65TOzhUtKR2yaSi</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 10:31:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Gartner, 生成式 AI, 应用程序编程接口, 模型
<br>
<br>
总结: 根据Gartner的报告预测，到2026年，超过80%的企业将使用生成式AI应用程序编程接口（API）或模型，或者在相关生产环境中部署支持生成式AI的应用程序。这意味着在短短三年内，采用或创建生成式AI模型的企业数量预计将会增长16倍。 </div>
                        <hr>
                    
                    <p>日前，全球咨询公司 Gartner 发布报告称，预计在 2026 年，超过 80% 的企业将使用生成式 AI （GenAI）应用程序编程接口（API）或模型，或者在相关生产环境中部署支持生成式 AI 的应用程序。</p><p>据统计，这一比例在 2023 年还不到 5%，这意味着在短短三年内，采用或创建生成式 AI 模型的企业数量预计将会增长 16 倍。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f8/4a/f820875904ce853a26a56fb7a380f64a.jpg" /></p><p>Gartner生成式 AI 技术成熟度曲线（2023）</p><p></p><p>Gartner 发布了 2023 生成式 AI 技术成熟度曲线，并预计将在未来十年对组织产生巨大影响的一些创新包括：支持生成式 AI 的应用程序、基础模型以及 AI 信任、风险和安全管理（AI TRiSM）。</p><p></p><p>支持生成 AI 的应用程序是指利用生成 AI 来完成特定任务的应用程序。ChatGPT 就是生成人工智能应用程序的一个例子，因为它使用人工智能来合成你的文本提示并输出响应。</p><p></p><p>基础模型是指生成式 AI 应用程序的机器学习模型，例如 GPT 与 ChatGPT 的关系。这些基础模型经过大量数据的训练，用于支持可以完成各种任务的不同应用程序。</p><p></p><p>Gartner 将基础模型置于技术成熟度曲线上预期过高的峰值，预测到 2027 年，它们将支撑 60% 的自然语言处理 (NLP) 用例。</p><p></p><p>最后，AI TRiSM 是指能够解决生成式 AI 模型相关问题并确保其成功部署的一组解决方案。困扰生成式 AI 模型的一些风险包括可靠性、错误信息、偏见、隐私和公平性。</p><p></p><p>Gartner 杰出副总裁分析师 Arun Chandrasekaran 表示；“生成式 AI 已成为最高管理层的首要任务，并引发了基础模型之外的新工具的创新。”“医疗保健、生命科学、法律、金融服务和公共部门等许多行业，对生成式 AI 的需求将不断增加。”</p><p></p><p>参考链接  ：</p><p>https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026</p><p>https://www.zdnet.com/article/80-of-enterprises-will-have-incorporated-ai-by-2026-according-to-a-gartner-report/</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bf/a9/bf1622c0c2be73e4d19e8643444e2fa9.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WpVGdht7FQxo4q95TcYY</id>
            <title>小米无线充电车专利公布；禾赛获哪吒汽车新车定点合作；阿维塔的无图智能驾驶技术即将面世｜汽车科技资讯</title>
            <link>https://www.infoq.cn/article/WpVGdht7FQxo4q95TcYY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WpVGdht7FQxo4q95TcYY</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 10:27:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 极氪 009 OS 4.1, 360° 全景影像, 智能浮窗, 魔视智能, 北汽极狐量产定点, 小米无线充电车专利, 长安汽车, 无图智能驾驶技术, 禾赛, 哪吒汽车新车定点合作
<br>
<br>
总结: 极氪 009 OS 4.1 更新推送，新增360°全景影像的智能浮窗功能。魔视智能再次获得北汽极狐量产定点，为两款车型提供智能泊车系统。小米汽车科技公布了充电车及充电方法的专利，实现了无线充电。长安汽车的阿维塔无图智驾技术即将面世，将在多个城市进行测试和用户体验。禾赛科技获得哪吒汽车新车定点合作，将提供超高清远距激光雷达等智能驾驶技术。 </div>
                        <hr>
                    
                    <p></p><h2>极氪 009 OS 4.1 推送发布，360° 全景影像新增“智能浮窗”功能</h2><p></p><p></p><p>10 月 15 日，极氪 009 纯电 MPV 迎来 OS 4.1 更新推送，360 度全景影像优化升级，新增全屏 / 浮窗随需切换功能。此次更新后，360° 全景影像设置中新增“智能浮窗”功能，开启智能浮窗功能后，转向联动和窄道辅助激活时，中央显示屏自动弹起智能浮窗，全屏和浮窗设有切换设置，用户可自由选择。当低速转向和驶入狭窄路段时，全景影像可以浮窗的形式显示，避免全屏模式对地图画面信息的遮挡，并可在全屏 / 浮窗两种模式间一键切换，便捷随需。</p><p></p><p></p><h2>魔视智能宣布再获北汽极狐量产定点</h2><p></p><p></p><p>10 月 13 日，继去年 12 月宣布获得北汽极狐两款车型量产定点，魔视智能宣布再获北汽极狐量产定点，为极狐阿尔法 S、阿尔法 T 两款车型提供软硬一体的 L2+ 智能泊车系统，相关车型将于 2024 年上市。据悉，该智能泊车系统将以极具性价比的成本覆盖自动泊车（APA）、融合泊车（RPA）、全景影像系统（AVM）等泊车域主流功能。魔视智能针对 L1-L4 级低速泊车、高速行车等乘用车场景，均开展了相关的产品布局，未来，双方将携手推动高智能化、高安全性的自动驾驶产品的量产落地，打造面向下一代的智能泊车产品，引领智能泊车新时代。</p><p></p><p></p><h2>小米无线充电车专利公布</h2><p></p><p></p><p>10 月 11 日，小米汽车科技有限公司申请的“充电车及充电方法”专利公布。充电车包括电池仓、无线充电装置、自动驾驶系统，电池仓用于装载电池，无线充电装置用于将电池的电能无线传输给电动车，自动驾驶系统用于控制充电车行驶到与电动车处于预设的相对位置，在相对位置下，无线充电装置能够将电池的电能无线传输给电动车。</p><p></p><p></p><h2>长安汽车：阿维塔的无图智能驾驶技术即将面世</h2><p></p><p></p><p>由长安汽车所控股的阿维塔科技于 2023 年 10 月 9 日宣布逐步开启无图智驾 NCA，并将于近期密集开展大规模实际道路测试和用户体验。 长安汽车最新消息显示：作为华为高阶智能驾驶系统 ADS2.0 的核心能力，阿维塔的无图智驾 NCA 功能将率先覆盖北京、上海、广州、重庆、深圳、杭州六城，第二批功能交付将覆盖另外 16 座核心城市，并于年内实现国内全覆盖。随着无图智驾 NCA 的成功落地，阿维塔 11 的智驾系统率先摆脱高精地图限制，实现高速 - 城区 - 泊车三大核心场景的全面覆盖，解锁更多高频智驾场景，让智驾体验做到“越开路越熟”的同时，达成业内无图智驾最快交付速度，为用户带来“越开路越广”的智驾体验。</p><p></p><p></p><h2>禾赛获哪吒汽车新车定点合作</h2><p></p><p></p><p>2023 年 10 月 10 日，禾赛科技宣布获得哪吒汽车旗下新车前装量产项目定点，哪吒汽车新车型将搭载禾赛超高清远距激光雷达 AT128，将集中展现哪吒汽车在电动汽车智能驾驶技术方面的最新研发成果，比如目前正在研发中的轻地图、无图版的城市领航辅助驾驶系统。其中，禾赛 AT128 将会在哪吒汽车 NETA PILOT 高阶智驾系统的感知模块中起到关键作用，提高哪吒汽车新车型的感知力，保障用户出行的舒适度和安全性。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bf/a9/bf1622c0c2be73e4d19e8643444e2fa9.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43</id>
            <title>抖音同款、2023 必看：火山引擎团队整理的“易复用”的音视频处理经验都在这了</title>
            <link>https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, 视频化时代, 营销, 商品, 知识, 空间的体验, 视频处理, 商业机会转化
<br>
<br>
总结: 从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，视频处理在各种场景中发挥着越来越重要的作用。同时，随着技术的快速发展和数据量的不断增长，“加快视频化进程、商业机会转化”成为了企业音视频业务在未来主要探索方向。火山引擎视频云与NVIDIA深度合作联合打造视频转码芯片和视频处理架构开源平台，致力于帮助企业完成技术迭代、实现业务增长。他们推出了《云上新视界》线上课程，以音视频创新场景与最佳实践为核心内容，为行业注入新鲜力量和创新源泉。该课程内容涵盖音视频处理多技术维度，由资深工程师专家亲自授课，分享实战经验，帮助视频处理开发者更好地完成经验复用。 </div>
                        <hr>
                    
                    <p>从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，从娱乐、教育、工业到商业应用，视频处理在各种场景中发挥着越来越重要的作用。同时，随着技术的快速发展和数据量的不断增长，“加快视频化进程、商业机会转化”成为了企业音视频业务在未来主要探索方向。</p><p></p><p>为了帮助大家更好地完成技术迭代、实现业务增长，<a href="https://www.infoq.cn/article/qC55OH6f6852hFjlZ3o8">火山引擎视频云</a>"与 NVIDIA 深度合作联合打造视频转码芯片和视频处理架构开源平台，自 2022 年 2 月至今，已经经过了 1000+ 企业业务场景的打磨，成功完成了多个创新场景实践。</p><p></p><p>在“方便大家将已经跑通的视频处理业务场景实践经验复用”的初心下，<a href="https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO">火山引擎视频云</a>"以“面向体验，驱动创新”为核心，特别与 NVIDIA 团队合作推出《云上新视界》线上课程，致力于打造一档以音视频创新场景与最佳实践为核心内容的系列视频栏目，内容覆盖赛事直播、6DoF 互动体验、3D 人体重建、弹幕游戏等火爆热门场景，为行业注入新鲜力量和创新源泉。</p><p></p><p>该系列视频栏目是全新的<a href="https://www.infoq.cn/article/Rx45QcxHI4zZCfMR5r8J">长线课程</a>"，目前课程制作团队已完成前 6 期视频内容的策划，自 2023 年 10 月 19 日起，将在火山引擎开发者社区、字节跳动技术团队、字节跳动视频云技术团队、InfoQ 等内容平台中以“2 周 / 期”的频率进行上线更新。</p><p></p><p>《云上新视界》系列课程极具特色，相较于当前网上已有的课程更适合音视频及其相关技术从业者收看：</p><p>技术解读够详细：课程内容涵盖音视频处理多技术维度，包括但不限于视频转码、视频增强、视频分析、视频插帧、VR 等。火山引擎视频云将与 NVIDIA 共同为开发者们提供更全面、更前沿的视频处理技术指导，帮助大家全面了解多媒体处理框架（Babit Multimedia Framework，BMF） 在不同场景中的应用。实战经验够全面：课程将由来自火山引擎视频云、NVIDIA 及多位火山引擎客户伙伴的资深工程师专家亲自授课，分享他们在多个创新场景中的实践经验，如亚运会（赛事直播）、弹幕游戏、虚拟直播间、VR 空间互动、3D 人体重建、远程车控等。课程主讲人将从某行业场景痛点入手解读场景方案架构、方案优势、应用场景和最佳实践，切实帮助各行各业的视频处理开发者更好地完成经验复用。</p><p></p><p>目前该系列公开课的第一期课程《抖音大型直播画质优化实践》预计于 10 月 19 日正式上线。在该视频中，火山引擎多媒体实验室技术专家王庆将为大家解析“抖音大型赛事直播全链路画质”面临的挑战，并揭秘“抖音亚运会直播服务端与客户端画质优化”方法论与实践收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/16eaec07efaf9fbec7e0008249f12732.jpeg" /></p><p></p><p>10 月 19 日 19:30，课程将在 InfoQ 视频号、InfoQ 官网、极客时间 APP 进行上线直播，感兴趣的同学们赶紧点击“阅读原文进行报名吧！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7kwLXVzqKJHfLq4GcPtL</id>
            <title>国内首个“AI原生应用商店”上线！百度智能云：让首批敢于吃螃蟹者获益</title>
            <link>https://www.infoq.cn/article/7kwLXVzqKJHfLq4GcPtL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7kwLXVzqKJHfLq4GcPtL</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 05:44:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 大模型服务平台, 生态建设, 千帆社区
<br>
<br>
总结: 百度智能云通过建立大模型服务平台和千帆社区，实现了生态建设，为开发者和创新企业提供了支持和培训，推动了AI原生应用的创新和商业化。 </div>
                        <hr>
                    
                    <p>百度智能云已建立起国内最繁荣的AI原生产业生态。在10月17日举行的百度世界2023上，百度智能云宣布，百度智能云千帆大模型服务平台已服务17000多家客户，覆盖近500个场景。同时，新的企业和开发者还正在不断地涌入千帆，大模型调用量高速攀升。平台上既有年龄仅14岁的小开发者，也有刚成立不久的初创企业，还有已深耕行业十几年的互联网老兵，开发出了智能创作、问诊咨询、电商、短视频、游戏、情感陪伴等多样化应用。</p><p>&nbsp;</p><p>生态建设也成为百度智能云在大模型时代最重要的一环。当日，百度集团执行副总裁、百度智能云事业群总裁沈抖宣布，“云智一体”战略内涵升级为“云智一体，深入产业，生态繁荣，AI普惠”。相比过去，“生态繁荣”是百度智能云战略中新增内容。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b0/b019bd6111ad448ba379bbac0e3d9f93.png" /></p><p>百度集团副总裁袁佛玉</p><p>&nbsp;</p><p>在当日下午举行的“大模型驱动产业发展论坛”上，百度集团副总裁袁佛玉宣布，百度智能云已打造国内第一个大模型全链路生态支持体系，贴身围绕生态伙伴、创始企业，用上大模型、用好大模型的需求，为处于不同成长阶段的创新企业和开发者提供定向专属支持。这意味着，百度智能云不仅要帮助创业者、开发者实现从0到1的突破创新，还要帮助实现商业价值和社会价值。</p><p>&nbsp;</p><p>袁佛玉表示，百度智能云的大模型生态支持体系分为三部分。第一部分是从多个方面支持开发者和创新企业。百度智能云搭建业界首个大模型实训营，为新入行的开发者和亟需更新知识的企业提供赋能培训支持。AI加速器则为创新企业提供创新应用孵化支持，培养出潜力明星企业。其他方式还有销售商机支持和市场营销支持。第二部分，百度智能云打造了国内首家面向企业客户进行一站式交易的AI原生应用商店——千帆AI原生应用商店，加速AI原生应用的商业化落地。第三，百度智能云建设千帆社区，为广大的千帆开发者提供交流，分享AI原生开发案例和实践经验的平台。&nbsp;</p><p></p><h3>从0到1，业界首个大模型实训营激活AI原生应用创新</h3><p></p><p>&nbsp;</p><p>对于创业者和企业来说，谁抢先开发出爆款的AI原生应用，谁就有可能成为大模型时代的新巨头。但创新者也面临不少烦恼。比如大模型技术门槛高，技术迭代速度快，新入行的开发者不是特别了解怎么基于大模型来做开发，深耕行业多年的“老兵”也需要更新自己的知识。</p><p>&nbsp;</p><p>百度智能云推出业界首个大模型实训营——千帆AGI House，基于伙伴落地大模型不同阶段所需要的技术支持，以实践、实操为导向，支持伙伴搞清楚技术发展方向、少走弯路，用好千帆大模型平台等，致力于真正把技术前沿落地到现实生产场景中。据介绍，千帆AGI House得到了市场非常积极的回应。每场的报名人数都超过了可容纳人数的好几倍。</p><p>&nbsp;</p><p>百度智能云生态合作伙伴庖丁科技副总裁关晨光表示，庖丁科技能够帮助文心大模型更懂行业，也能让行业更容易用上文心大模型。庖丁科技支持各类集成方式，赋能各类大中小企业的需求，过去，金融从业者如果想从监管公开的信息中寻找一些长尾信息，比如哪些上市公司过去1年更换了会计师事务所，人工方式统计至少需要一个工作日的时间。而通过庖丁和百度智能云千帆合作的企业级知识问答AI——ChatDOC，可以用对话的形式跟知识库聊天，快速得到信息，效率至少提升数十倍。</p><p>&nbsp;</p><p>百度智能云还打造了“千帆社区”， 提供最务实的产品攻略，汇聚一线实践经验、前沿观点以及丰富的产品工具。用户包括有刚刚踏入AI大门、期望寻求手把手传授指导的初学者，拥有丰富的创业故事和经验、并愿意与他人分享的企业领袖，对技术追求深入至微、希望与同行切磋的技术精英。百度大模型技术和产品专家也会入驻千帆社区进行分享、交流。&nbsp;</p><p></p><h3>AI加速器+首家AI原生应用商店，让首批敢于吃螃蟹者收获成果</h3><p></p><p>&nbsp;</p><p>大模型浪潮下，第一批勇于吃螃蟹的创新者已经打造出了丰富的AI原生应用。不过，开发只是第一步，真正的应用繁荣必然包含商业化的成功。但创新者普遍缺少技术培训赋能、资本及产业落地支持，辛苦打造的 AI原生应用需要商业变现的机会和渠道。</p><p>&nbsp;</p><p>百度智能云推出的AI加速器，为创新者做好技术赋能、技术资源支持、牵引投资和营销支持。比如在技术赋能方面，百度智能云新推出的AI原生应用开发工作台，主要由应用组件、应用框架两层服务构成，将各种应用的常见模式、工具、流程，沉淀在工作台上，让开发者不用再为研发过程发愁。百度自研的AI原生应用——Comate智能编程助手，即将在10月24日全面开放，开发者和企业可以通过百度智能云官网或者直接百度搜索Baidu Comate申请使用。目前，Comate智能编程助手已在百度内部大规模使用，覆盖80%以上的工程师，平均采纳率超过40%。</p><p>&nbsp;</p><p>百度智能云还拉动很多产业机构，共同为加速器成员企业提供从产品、技术培训到资本赋能、市场推广等一站式全方位支持，包括有北大智能所、清华互联网产业研究院、赛迪等顶尖的科研院所，创业黑马、爱分析等为创业者提供全方位服务的企服平台，还有产业园区和众多知名投资机构。行行AI董事长李明顺表示，无论国内外，强应用派都是这次大模型革命的浪尖弄潮儿，中国的强应用关键在产业。行行AI正在全国构建强应用区域性赋能中心网络，愿与百度智能云AI加速器共促大模型应用生态繁荣。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/83/83c9db09687951dd0f0f6d839b8f590d.png" /></p><p>&nbsp;千帆AI加速器成员亮相</p><p>&nbsp;</p><p>目前，百度智能云AI加速器已经开营两期，共有57家企业参加训练，已开发超过22个商业化应用。学员中70%以上为企业的创始人或CXO，部分企业董事长亲自下场，全天候参加课程培训。</p><p>&nbsp;</p><p>百度智能云还推出国内首家面向企业客户进行一站式交易的AI原生应用商店——千帆AI原生应用商店，为商家提供品牌曝光和销售通路支持，进一步加速AI原生应用的商业化落地。让第一批最有勇气冲在前面的AI原生应用开发者能够“活”下来，快速成长起来，实现商业价值。</p><p>&nbsp;</p><p>千帆AI原生应用商店已在10月16日正式上线，金蝶等合作伙伴打造的首批精选应用已经入驻商店。用户可以通过“百度智能云官网”进入商店，看到新品推荐榜、热门应用榜、行业推荐榜等各类榜单，快速找到最新、最热门的AI原生应用。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9e/9e2f6b6bd54537686f7394f89114bcdb.png" /></p><p>千帆AI原生应用商店</p><p>&nbsp;</p><p>袁佛玉表示，我们希望这个应用商店不只是应用展示，还是一个便捷高效的应用交易平台。我们希望它可以连接AI原生应用供应商和需求方，不仅提升企业客户在应用选型和采购方面的效率，更可以帮助商家更快速地把应用推向市场，成为一个大模型商业机会的汇集地。</p><p>&nbsp;</p><p>百度智能云的多个合作伙伴参与了“大模型驱动产业发展论坛”，分享了自己的行业洞察和AI原生应用。英特尔资深AI架构师任而今在会上介绍了英特尔软硬件如何全面赋能生成式人工智能，实现人工智能普惠化。在现场展区，英特尔也展示了从数据中心、边缘到端设备的人工智能产品和方案，包括Gaudi2人工智能加速卡等。随着 AI 技术的不断演进，LLM、生成式 AI 和DLRM深度学习推荐已逐渐成为现代经济发展的数字引擎，而大模型也带动着未来 AI 的发展趋势，新业务场景不断涌现，海量多类型数据集的积累，对计算能力和架构的要求也越来越高。NVIDIA 资深解决方案架构师龚孝波在百度世界大会发表了主题为“适应大模型发展的 NVIDIA 技术架构演变”的演讲，为开发者们阐述了 NVIDIA 从 GPU 架构、软件、硬件等方面，如何适应大模型对计算需求与网络通信的快速增长。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QJ73po4wuwTvLKcpK1Fw</id>
            <title>突发！美国限制向中国出口Nvidia H800等先进AI芯片，壁仞科技、摩尔线程等中国GPU芯片企业被列入实体名单</title>
            <link>https://www.infoq.cn/article/QJ73po4wuwTvLKcpK1Fw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QJ73po4wuwTvLKcpK1Fw</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Oct 2023 04:04:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, 芯片出口限制, AI技术, 中美贸易战
<br>
<br>
总结: 美国商务部计划限制向中国出售更先进的AI芯片，以限制中国获取先进半导体，推动AI技术发展和军事应用。新规将在未来30天内生效，此举是中美贸易战的一部分。 </div>
                        <hr>
                    
                    <p></p><blockquote>据悉，美国这一新规将在向公众征求 30 天意见后生效。</blockquote><p></p><p></p><h2>美国升级对华芯片出口限制</h2><p></p><p></p><p>据路透社报道，美国商务部 10 月 17 日宣布，计划限制向中国出售更先进的 AI 芯片。据悉，新的政策将限制 Nvidia A800 和 H800 芯片的出口，此外，新规将豁免笔记本电脑、智能手机和游戏设备中使用的大多数消费级芯片，但其中部分芯片仍须受到美国官员的批准和专项管控。相关规定将在未来 30 天内生效。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d7/d7882b8c99a56d822db36ce76f2b1052.png" /></p><p></p><p>10 月 16 日晚，美国商务部长 Gina Raimondo 曾对记者表示，新措施弥补了去年 10 月所发布法规中的漏洞，未来可能“至少每年更新一次”。她解释称，此番措施的目标是限制中国获取“先进半导体，这些半导体能够推动 AI 技术发展以及对军事应用具有重大意义的复杂计算机突破”，并强调美国政府无意在经济上打压中方。她表示，中国仍可进口价值数千亿美元的美国半导体。</p><p>&nbsp;</p><p>2022 年 10 月 7 日，美国政府以出台“临时规则”形式更新《出口管理条例》，将 31 家中国实体列入“未经核实清单”，并升级对华半导体出口管制，以&nbsp;Nvidia&nbsp;A100 芯片的性能指标作为限制标准，限制对华出口高性能计算芯片。具体来说，同时满足以下两个条件的即为受管制的高性能计算芯片：</p><p>&nbsp;</p><p>芯片的 I/O 带宽传输速率大于或等于 600 Gbyte/s；“数字处理单元 原始计算单元”每次操作的比特长度乘以 TOPS 计算出的算力之和大于或等于 4800TOPS。</p><p>&nbsp;</p><p>彼时受该政策影响，Nvidia&nbsp;A100 及 H100 GPU 加速芯片都无法继续对华出口。随后，Nvidia 针对中国大陆市场推出特供版的 A800 及 H800 芯片——仅保留了强大的计算能力，但对通信速度做出了限制以保证符合此前规定要求。自去年的规定实施以来，Nvidia&nbsp;面向中国的专供芯片需求大增、带动业务一路高歌猛进。受全球供应不足影响，获准销售的几乎一切&nbsp;Nvidia 芯片都在中国大受欢迎。</p><p>&nbsp;</p><p>如今美国加码芯片出口限制，新规则对特定尺寸芯片的计算能力也做出了限制，旨在防止使用新的“Chiplet”技术方法绕开限制。因此，Nvidia A800 和 H800 芯片对华出口也将受到影响。</p><p>&nbsp;</p><p>对此，Nvidia 表示，该公司将遵守规定、且预计近期业绩不会因此受到重大影响。但截至 10 月 17 日美股收盘，Nvidia 股价下跌 4.68%，同样受到新规则影响的 AMD、Intel 等相关企业股价也有相应下滑。</p><p>&nbsp;</p><p>除了升级对华芯片出口限制，此次美国新规还扩大了面向另外 40 多个国家出口先进芯片的管控要求。据路透社报道，这项措施似乎基于英伟达在今年 8 月收到的一封信函。信中称本应受到供应限制的 A100 和 H100 芯片从中国流出至包括中东在内的其他国家。此前路透社报道的猜测也在新规则中得到证实，即对于母公司总部位于中国、澳门及其他部分国家的企业，不得将芯片移交给位于世界任何地方的下辖子公司。</p><p>&nbsp;</p><p>美国还对中国以外的 21 个国家提出了芯片制造工具出口管控要求，原因是担心这些设备可能被转移给中国或其他国家，进而引发安全问题。</p><p>&nbsp;</p><p>此外，新规还在对中出口限制清单中添加了 DUV 光刻系统，相当于对政策做出进一步收紧。此前，美国曾禁止荷兰 ASML 向部分中国先进芯片工厂提供较旧型号的深紫外光刻设备及备件。DUV 设备属于尖端 EUV 设备（目前对中国全面禁售）的前代设备，但同样属于先进的芯片制造工具，能够以更高的成本制造出几乎同等制程的芯片。</p><p>&nbsp;</p><p>ASML 在一份声明中表示，新规可能在中长期内对“我方系统在特定区域内的销售”产生影响，但该公司预计 2023 年内的财务前景应该不会受到“实质性冲击”。</p><p></p><h2>多家中国GPU芯片企业被列入实体名单</h2><p></p><p>&nbsp;</p><p>此外，美国商务部在 10 月 17 日还将壁仞科技、摩尔线程等多家中国 GPU 芯片企业列入实体名单。</p><p>&nbsp;</p><p>具体包括：北京壁仞科技开发有限公司、广州壁仞集成电路有限公司、杭州壁仞科技开发有限公司、光线云（杭州）科技有限公司、摩尔线程智能科技（北京）有限责任公司、摩尔线程智能科技（成都）有限责任公司、摩尔线程智能科技（上海）有限责任公司、上海壁仞信息科技有限公司、上海壁仞集成电路有限公司、上海壁仞科技股份有限公司、超燃半导体（南京）有限公司、苏州芯延半导体科技有限公司、珠海壁仞集成电路有限公司。</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da05d4dfac3c9968b1dbe0798fde537b.png" /></p><p></p><p>对此，壁仞科技在 10 月 17 日晚间发表声明称，公司对美国商务部此举表示强烈反对，将向美方有关政府部门积极申诉，并呼吁美国政府重新进行审视。壁仞科技以“智绘全球”为愿景，严格遵守相关国家和地区的法律、法规，并在此基础上始终合法依规经营。公司正在评估此事件可能对公司造成的影响，做好应对工作，并将与各方面积极沟通。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/41/41da8578559838606c2128232bf41056.png" /></p><p></p><p>摩尔线程也在声明中表示强烈抗议：摩尔线程自成立以来，严格遵守相关国家和地区的法律、法规，始终秉持合法、合规的企业文化和管理理念，建立了完善的出口管制合规管理体系和工作流程指引。目前公司正在与各方积极沟通，对于该事项的影响我们正在评估。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4f/4f32d3cdef00b714876c3d202f225e49.png" /></p><p></p><h2>ASML CEO：孤立中国没有希望，实际上会削弱西方自己</h2><p></p><p>&nbsp;</p><p>事实上，围绕美国出口限制政策一直存在不少反对的声音。</p><p>&nbsp;</p><p>据彭博社报道，Nvidia、Intel 和高通这三大美国本土芯片巨头的首席执行官一直在警告美国政府不要对华采取高压手段。在减少中国获得高尖端技术的同时，他们担心对向中国出口非尖端芯片的新限制将剥夺他们的大量收入来源。美国商会估计，如果出现最糟糕的情况，即对中国的销售完全停止，美国芯片公司每年可能会损失 830 亿美元、12.4 万个工作岗位，相关研发支出每年将减少 120 亿美元。</p><p>&nbsp;</p><p>ASML 现任总裁兼首席执行官 Peter Wennink 也曾在当地电视节目 Nieuwsuur 上说道，“完全孤立中国是没有希望的。如果我们不分享技术，他们就会自己去研究。”Peter Wennink 认为通过禁止技术移民和出口管制等方式孤立中国，实际上会削弱西方自己。</p><p>&nbsp;</p><p>此外，据环球网报道，香港半导体行业分析师林子恒 16 日对《环球时报》记者分析称，拜登政府在芯片领域的限制行动意图明显，就是想“扼杀”中国芯片产业，削弱中国在未来全球高科技领域竞争中的实力，并维护自身的科技霸权。林子恒认为，媒体报道的新限制措施更有针对性，可能会为中国芯片行业以及人工智能行业的发展带来新挑战，但也会激发出中国科研机构和企业的攻关动力。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/biden-cut-china-off-more-nvidia-chips-expand-curbs-more-countries-2023-10-17/">https://www.reuters.com/technology/biden-cut-china-off-more-nvidia-chips-expand-curbs-more-countries-2023-10-17/</a>"</p><p><a href="https://www.bis.doc.gov/index.php/about-bis/newsroom/2082">https://www.bis.doc.gov/index.php/about-bis/newsroom/2082</a>"</p><p><a href="https://baijiahao.baidu.com/s?id=1779952136508708360&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1779952136508708360&amp;wfr=spider&amp;for=pc</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2vE0o4dBI6N4idPFTUlt</id>
            <title>文心大模型4.0发布！李彦宏：相比GPT-4毫不逊色</title>
            <link>https://www.infoq.cn/article/2vE0o4dBI6N4idPFTUlt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2vE0o4dBI6N4idPFTUlt</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Oct 2023 03:46:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI原生应用, 文心一言, 李彦宏
<br>
<br>
总结: 李彦宏在百度世界2023上发布了文心大模型4.0版本，并展示了十余款基于文心一言的AI原生应用。他强调大模型是开发AI原生应用的基础，具备理解、生成、逻辑和记忆四大核心能力。通过插件和API的助力，大模型将推动AI原生应用生态的繁荣，促进经济增长。 </div>
                        <hr>
                    
                    <p>“大模型带来的智能涌现，这是我们开发AI原生应用的基础。”10月17日，李彦宏在百度世界2023上表示。当天，李彦宏以《手把手教你做AI原生应用》为主题发表演讲，发布文心大模型4.0版本，并带来新搜索、新地图等十余款AI原生应用。</p><p><img src="https://static001.geekbang.org/infoq/f5/f54e97d5835a6e76f2564c7ee051f29b.png" /></p><p></p><p>大会上，李彦宏宣布文心大模型4.0正式发布，开启邀请测试。他表示，这是迄今为止最强大的文心大模型，实现了基础模型的全面升级，在理解、生成、逻辑和记忆能力上都有着显著提升，综合能力“与GPT-4相比毫不逊色”。李彦宏介绍，文心4.0也同步开始邀测，现场观众扫描嘉宾证二维码，登录文心一言官网或下载最新版文心一言APP，就可以体验到文心一言的专业版；此外，企业客户也可以通过百度智能云千帆大模型平台来申请测试文心4.0&nbsp;API。</p><p></p><p>他现场展示了基于文心一言重构的百度搜索、如流、地图、网盘、文库等十余款AI原生应用，希望能拓展大家的想象力，“激发大家一起来做出更惊艳的AI原生应用来”。</p><p></p><h2>最强文心大模型4.0发布&nbsp;综合能力比GPT-4毫不逊色</h2><p></p><p></p><p>在李彦宏看来，AI原生应用的诞生，得益于大模型的理解、生成、逻辑和记忆四大核心能力，百度的AI原生应用也是基于文心一言来开发的，“这些能力是过去的时代所不具备的，因而才能打开无限的创新空间”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/aa/aaf20d6150f36ee1a7c7ddf530b95477.png" /></p><p></p><p>基于文心大模型4.0，李彦宏依次演示了四大能力的特点与应用场景。在理解能力上，他通过询问公积金异地贷款政策的案例，展示了文心一言对前后乱序、模糊意图、潜台词等复杂提示词的理解力，例如“在北京工作”等同于“在北京缴纳公积金”等等，“今天，你说的每一句话，它大概率都能听懂”。</p><p>&nbsp;</p><p>在生成能力上，李彦宏展示了文心一言如何在短短几分钟内，根据一张素材图片，迅速生成了一组广告海报、五条广告文案以及一条营销视频。据介绍，基于这一系列能力，百度已经推出了AIGC营销创意平台擎舵，让“一个人就成为一支AI营销队伍”。</p><p>&nbsp;</p><p>同时，他还通过解数学题、总结知识点等场景，展示了大模型的逻辑能力；通过数千字的小说撰写和角色、情节设置，体现了大模型的记忆能力；以及数字人医生帮助患者解读药品说明书，来展现四大能力的综合应用。</p><p>&nbsp;</p><p>“前面的演示，体现出文心大模型在理解、生成、逻辑、记忆这四大能力上的进步，这些能力是一切AI原生应用赖以生存的基础。”李彦宏表示。</p><p></p><h2>十余款AI原生应用重磅发布</h2><p></p><p></p><p>丰富的AI原生应用才是大模型的价值所在。大会上，李彦宏宣布“我们的搜索、如流、地图、网盘、文库等，都将以一个全新的面目与大家见面，”并表示，分享上述这些应用的目的，是为了拓展想象力、激发更多人做出更惊艳的AI原生应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d0149e6b4817c25ef7cb5923e194f4b1.png" /></p><p></p><p>李彦宏介绍，百度新搜索具有极致满足、推荐激发和多轮交互三个特点，当用户搜索问题时，新搜索将“不再是给你一堆链接”，而是通过对内容的理解，生成文字、图片、动态图表的多模态答案，让用户一步获取答案。在针对复杂需求时，“多轮交互”特点也可以通过提示、调整等方式，满足用户更个性化的搜索需求。</p><p>&nbsp;</p><p>同时，李彦宏还展示了用AI原生思维打造的国内第一个生成式商业智能产品：百度GBI。据介绍，相对传统BI软件的高门槛和数据分析难等问题，百度GBI可以通过自然语言交互，执行数据查询与分析任务，还支持专业知识注入，满足更复杂、专业的分析需求。</p><p>&nbsp;</p><p>通过对海量文档、图片和视频的理解和再生成，百度网盘和文库拥有了创作能力：网盘不仅能精准定位到视频某一帧，还能在几秒钟内总结完长达1小时的视频内容，并从中提炼出金句和要点；文库更是基于10亿优质资料，能实现写稿和做PPT等工作，成为名副其实的“生产力工具”。</p><p>&nbsp;</p><p>百度地图和智能办公平台如流，也通过理解、记忆等能力，变成更贴心的出行向导和超级助理：在地图上，用户只需说出需求，地图就能调动几千个服务接口，帮助用户推荐餐厅、对比多地点信息、给出出行建议；如流则可以针对群聊信息多的办公痛点，“一秒划重点”，差旅助手不仅能订机票酒店，甚至还能通过接入CRM等公司系统，总结出拜访客户的背景资料和谈话参考。</p><p>&nbsp;</p><p>正如李彦宏此前所说，AI原生应用不是对移动互联网App和PC软件的简单重复，而是要能“解决过去解决不了或解决不好的问题”。&nbsp;</p><p></p><h2>插件、API助力生态繁荣&nbsp;&nbsp;推动经济增长</h2><p></p><p></p><p>“大模型将开启一个繁荣的AI原生应用生态，”李彦宏强调，插件是一种特殊的AI原生应用，门槛最低，也最容易上手，能让开发者、创业者快速加入到生态中。他举例说，大模型接入权威法律数据的“智能法律助手”，能为用户提供法律咨询的相关建议，而简历助手插件则能帮用户一键生成简历模板。</p><p>&nbsp;</p><p>据介绍，个人及企业的数据、能力或应用，都能快速变成AI插件，增强大模型的能力，让大模型更实用易用。李彦宏表示，一个月前，百度上线了灵境插件平台，目前已经有2.7万开发者申请入驻，覆盖法律、职场、学习等多个领域。</p><p>&nbsp;</p><p>在开发AI原生应用时，大模型的基础能力至关重要。李彦宏介绍说，API是AI原生应用调用基础大模型的主要方式，企业和开发者可以在百度的千帆大模型平台上调取包括文心一言在内的大模型API，目前，千帆大模型平台已经成为中国最大的大模型开发平台，有42个主流大模型入驻，覆盖各行各业近500个场景。即日起，企业客户也可以在千帆大模型平台上申请测试文心4.0的API。</p><p></p><p>“中国有丰富的应用场景，中国用户又天然愿意拥抱新技术，有了先进的基础大模型，我们就能构建起一个繁荣的AI生态，共同创造新一轮经济增长。”李彦宏表示。</p><p>&nbsp;</p><p>此外，李彦宏表示，未来的AI原生应用一定是多模态的，在信息世界之外，一定会重构物理世界。自动驾驶就是视觉大模型重构物理世界的一个典型应用。大模型会让百度的自动驾驶能力超越经验系统，更聪明地处理复杂场景，实现更广泛的时空覆盖。目前，百度自动驾驶出行服务平台萝卜快跑累计提供服务超400万次，已经成为全球最大的自动驾驶出行服务商。</p><p>&nbsp;</p><p>“大量AI原生应用将不断涌现，数字技术与实体经济将深度融合……大模型正成为新型工业化的重要推动力。”李彦宏说。正如百度世界2023的主题是“生成未来”，在演讲结尾，李彦宏宣布，我们即将进入一个AI原生的时代，进入一个人机通过Prompt来交互的时代。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b3c871ce83566032fbf428201592da5.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/b7xuUQKHJvIFoVma98Zx</id>
            <title>这件事，已被大学生持续关注了 5 年……</title>
            <link>https://www.infoq.cn/article/b7xuUQKHJvIFoVma98Zx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/b7xuUQKHJvIFoVma98Zx</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Oct 2023 03:03:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 深圳国际金融科技大赛, 学生团队竞赛, 金融科技, 大赛盛宴
<br>
<br>
总结: 深圳国际金融科技大赛是一场面向学生团队的金融科技竞赛活动，已经连续举办了5年。大赛吸引了来自海内外高校的学生参赛，成为了具有广泛影响的赛事盛宴。大赛提供了区块链、人工智能和产品经理三个竞技赛道，参赛团队有机会获得丰厚的奖金和实习机会。此外，大赛还邀请了行业大佬担任评委和学术顾问，为参赛团队提供专业指导和支持。 </div>
                        <hr>
                    
                    <p>5 年，足以让一棵嫩芽茁壮成长——比如，一个学生完成从本科新生到研究生的转变；</p><p>5 年，也足以让一个新兴技术从初露头角发展为全球焦点——比如，金融科技打破传统金融服务边界，成为了数字化时代的重要部分……</p><p></p><p>那么，对于一年一度的“深圳国际金融科技大赛 - 西丽湖金融科技大学生挑战赛”来说，5 年又意味着什么呢？——意味着从初出茅庐的“新生赛事”成为了具有稳定基础和广泛影响的“赛事盛宴”！</p><p></p><p>自 2019 年第一届大赛落地，该品牌赛事至去年已成功举办 4 届，共吸引了 3500 余名来自海内外知名高校的学生参赛！每届大赛的举办都会在行业内引起一波浪潮！</p><p></p><p>今年，已经是大赛举办的第 5 年</p><p>在大家的热烈期盼下</p><p>大赛正式于 10 月 16 日 00:00 开赛！</p><p></p><p></p><p>🚀是的，你没听错！</p><p>2023 深圳国际金融科技大赛（ FinTechathon ）</p><p>—— 西丽湖金融科技大学生挑战赛全面启动！</p><p>正在面向国内外高校在读生火热招募中📣</p><p>突破界限，释放想象力</p><p>金融科技的未来，由你点燃！</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/355ea0cc1ea80a3fdbeb13d788a1d456.jpeg" /></p><p></p><p></p><p>这个大赛凭啥能“连续 5 年牵扯学生心”？！</p><p>一场专为学生团队打造的世界级金融科技竞赛</p><p></p><p><a href="https://www.infoq.cn/news/9AYU96ZSPoCZ6kyClK94">2023 深圳国际金融科技大赛——西丽湖金融科技大学生挑战赛</a>"（下文称“大赛”），是一场面向金融科技前沿技术领域的学生团队竞赛活动，是深圳市金融科技节的重要一环。</p><p></p><p>该赛事前身是“ FinTechathon 微众银行金融科技高校技术大赛”，在去年成功完成了品牌升级。经过 4 年的发展，大赛组委会从最初的办赛热忱中逐渐沉淀下来，对大赛的本质和价值进行深入的思考和探索，更加理性地审视了大赛的发展方向和目标，思考了如何更好地为参赛者服务。目前大赛已形成了一套完善的赛制和评选标准，赛事的整体筹备和落地已兼备成熟性，大赛的公平性和公正性有了更多保证，越来越多的优秀作品和人才脱颖而出。</p><p></p><p>从去年起，该赛事便由政、学、企三方联合共建，含金量十足！而本届大赛也依旧是在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办！</p><p></p><p></p><p>哦豁，今年的大赛搞了许多“新花活”？！</p><p>本届大赛的变与不变</p><p></p><p>本届大赛组委会将基于往届办赛经验，继续进一步提升赛事体验和评选质量。本届大赛保留了往届一样的区块链、人工智能、产品经理三个竞技赛道，三个赛道还是将分别通过初赛遴选出 10 支队伍进入决赛，每个赛道进入决赛的队伍将争夺一等奖 (1 队）、二等奖（1 队）及三等奖（1 队）！和去年一样，获奖队伍除了获得奖杯、纸质获奖证书、具有唯一标识的数字化获奖凭证“区块链数字证书”外，还将瓜分大赛组委会准备的 69W+ 的赛事奖金：</p><p>一等奖：100,000 元二等奖：80,000 元三等奖：50,000 元</p><p></p><p>（悄悄地和大家透露下：进入决赛的团队还将获得去微众银行实习的面试机会哦~）</p><p></p><p>本届大赛组委会依旧只接受“团队战”，需要 2-5 人 组队参赛，参与组队的成员不限学历、不限专业、不限年级，无论在国内还是国外，只要是高校在读生（含本科生、硕士 / 博士研究生）就可以参赛！</p><p>但，和去年不一样的是，因产品经理赛道所需参赛作品形式与另外两个赛道有所差异，故该赛道在今年增加了“复赛”，初赛将海选出 30 支队伍进入复赛进行线上答辩，复赛将选出 10 支队伍进入决赛。三个赛道的赛题也发生了变化，但较往年难度相当，具体内容可以前往大赛官网查看<a href="https://www.infoq.cn/article/%EF%BC%88https://www.infoq.cn/zones/fintechathon/campus2023/%EF%BC%89">（https://www.infoq.cn/zones/fintechathon/campus2023/）</a>"。</p><p></p><p>此外，本届大赛在初赛作品提交之前的“技术公开课”形式也发生了变化，10 月 25 日 -11 月 10 日，今年的大赛组委会除了做线上直播外，还将走到线下高校去与大家面对面交流，届时三个赛道的专家评委和金融科技行业的专家将分别围绕赛题内容展开技术干货分享。届时同学们可以密切关注大赛官方社群内发布的进校行程，关注“InfoQ 视频号”、“InfoQ 官网”直播间的大赛技术公开课的直播预告！</p><p></p><p>“就算拿不到奖”也要参加今年的大赛？!</p><p>数十位行业大佬亲自指导你的作品</p><p></p><p>本届大赛主办方将最大限度地发挥政、学、企三方的优势，坚守全面提高学生的创新能力、实践能力和就业竞争力的办赛初心。为此，大赛组委会特别邀请了国家统计局原副局长许宪春；加拿大皇家科学院院士、加拿大工程院院士、微众银行首席人工智能官杨强；清华大学五道口金融学院教授、华夏银行原行长、中国人民银行研究局原局长张健华；中国工商银行首席技术官吕仲涛；上海新金融研究院副院长、浙商银行原行长刘晓春；全国政协委员、南方科技大学副校长金李；中国银行业协会首席信息官高峰等人担当学术顾问，为大赛提供智力支持，帮助参赛团队更好地理解和应用金融科技知识。</p><p></p><p>除此之外，大赛组委会还邀请了来自中科院、清华大学、中山大学、西安电子科技大学、深圳大学、武汉大学、中央财经大学、广东财经大学、浙江财经大学、哈尔滨工业大学、微众银行等学企单位的数十位科研专家担任大赛评委，为参赛团队提供专业的指导建议，督促参赛团队把创新成果转化为实际应用，为金融科技行业提供更多有价值的技术解决方案，争取开创领域技术创新先河。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b9b06b4328d88a65eeb23cb8d5736111.jpeg" /></p><p></p><p>如此重磅的评委嘉宾阵容，意味着行业大佬对于参赛的你来说，再也不仅仅是视频里讲课的专家，而是直接帮助你订正作品内容、帮你解决技术难点的专属导师! 这些大佬的经验或许可以帮你在技术创新的道路上少走许多路，一定不能错过这样的好机会！</p><p></p><p>所以，你还在想什么？</p><p>赶紧扫描下方二维码进行报名吧！</p><p><img src="https://static001.geekbang.org/infoq/d0/d0dbb40226846cd13a49f1cd42ba5369.png" /></p><p></p><p>2023 深圳国际金融科技大赛（ FinTechathon ）</p><p>—— 西丽湖金融科技大学生挑战赛</p><p>全新就绪，等你来引爆金融科技的无限想象！</p><p>同学们可通过以下方式</p><p>了解更多大赛信息哦~</p><p></p><p>① 添加小助手随时随地了解比赛进程</p><p><img src="https://static001.geekbang.org/infoq/77/778476730106a49946b46a92c1bea68d.jpeg" /></p><p></p><p>② 登陆大赛官方网站了解更多大赛信息</p><p><a href="https://www.infoq.cn/zones/fintechathon/campus2023/">https://www.infoq.cn/zones/fintechathon/campus2023/</a>"</p><p></p><p>③ 通过大赛指定邮箱与主办方联系</p><p>fintechathon@geekbang.com</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uxPrRHWE4XEz4d7XZiL3</id>
            <title>百川智能启动2024校招，A1轮获阿里腾讯小米等3亿美元投资</title>
            <link>https://www.infoq.cn/article/uxPrRHWE4XEz4d7XZiL3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uxPrRHWE4XEz4d7XZiL3</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Oct 2023 01:40:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百川智能, 星耀计划, 校园招聘, 大模型初创企业
<br>
<br>
总结: 百川智能启动2024届校园招聘并发起“星耀计划”，面向全球精英科技人才，寻找有技术理想、热爱AI领域的应届生。百川智能是2024届校园招聘规模最大的大模型初创企业，拥有顶尖科技人才和雄厚资金支持，保持着行业领先的大模型研发速度和顶尖水准。 </div>
                        <hr>
                    
                    <p>近日，<a href="https://www.infoq.cn/article/ivM3DbowD6o9Ro4jIeGq?utm_campaign=geek_search_source&amp;utm_content=geek_search_source&amp;utm_medium=geek_search_source&amp;utm_source=geek_search_source&amp;utm_term=geek_search_source">百川智能</a>"正式启动2024届校园招聘并发起“星耀计划”。本次校招将面向海内外学生，同时覆盖北上广深等多个城市多所高校，目前百川智能是2024届校园招聘规模最大的大模型初创企业。</p><p>&nbsp;</p><p>“星耀计划”是百川智能面向全球精英科技人才的专项校园招聘计划。岗位涵盖了自然语言处理、计算机视觉、强化学习、基础架构等多个人工智能关键技术方向，旨在寻找有技术理想，热爱AI领域的精英人才。2023年11月- 2024年10月毕业的海内外应届生，均可通过百川智能校招官网进行申请<a href="https://campus.baichuan-inc.com/">官网地址</a>"，截止日期为12月31日。百川智能将为通过该计划的学生提供系统化培养和支持，助力同学们在技术领域的快速成长和飞跃。</p><p>&nbsp;</p><p>百川智能成立于2023年4月10日，由前搜狗公司CEO王小川创立。其核心团队由来自搜狗、Google、腾讯、百度、华为、微软、字节等知名科技公司的AI顶尖人才组成。目前，百川智能的团队规模170余人，其中硕士及硕士以上学历员工占比近70%，研发人员占比超80%。</p><p>&nbsp;</p><p>此前，百川智能已完成A1轮战略融资，融资金额3亿美元，阿里、腾讯、小米等科技巨头及多家顶级投资机构均参投了本轮融资。加上天使轮的5000万美元，百川智能的融资金额已达3.5亿美元。成立不到半年时间便跻身科技独角兽行列，创下国内大模型初创企业晋升独角兽速度之最。</p><p>&nbsp;</p><p>在顶尖科技人才和雄厚资金的支持下，百川智能保持了惊人的大模型研发速度。成立仅半年，百川智能便接连发布Baichuan-7B/13B，Baichuan2-7B/13B四款开源可免费商用大模型及Baichuan-53B、Baichuan2-53B两款闭源大模型，平均每28天就会发布一款新的大模型。</p><p>&nbsp;</p><p>百川智能不仅保持着行业领先的大模型研发速度，还将大模型的性能也做到了顶尖水准。Baichuan-7B/13B两款开源大模型在多个权威评测榜单均名列前茅，累积下载量超过六百万次。Baichuan2-13B在MMLU、CMMLU、MedQA、USMLE等几大权威评估基准中，以绝对优势全方位领先LLaMA2，引领开源社区走向中文开源<a href="https://www.infoq.cn/article/Qa3ExDDg5W4OiblRxpGx?utm_campaign=geek_search_source&amp;utm_content=geek_search_source&amp;utm_medium=geek_search_source&amp;utm_source=geek_search_source&amp;utm_term=geek_search_source">大模型时代</a>"。</p><p>&nbsp;</p><p>值得一提的是，8月31日百川智能率先通过国家《生成式人工智能服务管理暂行办法》备案，是首批八家公司中唯一一家今年成立的大模型初创公司，并于9月25日开放Baichuan2-53B&nbsp;API接口，正式进军To B领域，开启商业化进程。</p><p>&nbsp;</p><p>经过半年时间的发展，百川智能已经展示出了行业领先的技术竞争力和人才吸引力，在新一轮的融资过程中或将再受巨头和众多资本追捧。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/u3LfJYWerRKM6u2l4Ek2</id>
            <title>大模型时代下的技术变革：训练、负载、部署、效率、安全……都遇到了新挑战？</title>
            <link>https://www.infoq.cn/article/u3LfJYWerRKM6u2l4Ek2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/u3LfJYWerRKM6u2l4Ek2</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, AI大模型, 数据和计算资源, 应用场景支持
<br>
<br>
总结: 随着互联网的快速发展，AI大模型成为当前行业最热门的技术之一。大模型需要大量的数据和计算资源，并且对各行各业都有深远的影响。产业界和学术界在大模型的研发和应用方面有深入的合作和探索。然而，在大模型时代，算力资源、数据质量和规模对模型的性能至关重要，同时也需要解决数据安全等问题。英特尔及其伙伴在大模型技术方面取得了进展，释放出了新的机遇。 </div>
                        <hr>
                    
                    <p>随着互联网的快速发展，AI 大模型算的上是当前行业里最“炽手可热”的技术，大模型是 AI 领域的重要发展趋势。大模型需要大量的数据和计算资源，同时也需要强大的应用场景支持，对各行各业都有深远的影响，各厂商开始了“千模大战”。</p><p></p><p>当前，在 AI 大模型的研发和应用方面，产业界和学术界在很多方面都有深入的合作和探索。产业界和学术界都有各自的优势——产业界在数据采集、计算资源、应用需求理解等方面有独特的优势，学术界则在理论创新、方法研究、前沿技术探索等方面有显著的优势。</p><p></p><p>然而，在这个大模型时代，算力资源、数据质量和规模都对模型的性能有着至关重要的影响，包括数据安全也是当前亟需解决的问题。所以，在产业界和学术届深度融合探索下的 AI 大模型技术都有了哪些进展和变化？在这个过程中，是否释放出了新机遇？这两个问题的答案似乎在英特尔及其伙伴的实践中找到了。</p><p></p><p></p><h2>一、大模型的训练与负载：算力与成本之间需要寻找一个平衡</h2><p></p><p></p><p>随着人工智能和深度学习的发展，模型训练所需的数据量和处理能力在不断增加。多家研究报告显示，当前大型模型的训练数据量通常都达到了数百万甚至数千万级别。这些大型模型在进行训练时，需要处理的参数量相当庞大，例如 GPT-3 在训练时使用了 28.5 万 CPU 核心，总算力为 17.5 亿亿次，消耗了大约 250 万美元的 GPU 算力。大模型对大规模数据和计算资源的需求，对算力相关的硬件和软件都提出了更高要求。</p><p></p><p>为了提高模型的效果，往往需要采用更复杂的模型结构和训练策略，这也进一步增加了算力需求。同时，由于模型训练需要大量的时间和资源，训练时间也成了制约大模型发展的一个重要因素。对于一般企业而言，拥有如此强大的计算资源并不现实，因此企业都在积极寻找可以迭代优化模型训练和推理的基础设施。</p><p></p><p>然而算力与成本之间存在着明显的矛盾。首先，大模型训练需要大量的算力资源，而这些资源通常需要花费高昂的成本来获取。其次，数据传输和处理也会产生大量的成本，因为需要将大量数据从存储设备传输到计算设备进行处理。此外，硬件维护和软件开发也需要投入大量的人力物力。因此，在提高大模型训练效果的同时，厂商需要考虑如何平衡算力与成本之间的关系。</p><p></p><p>从整个模型的生态来看，其对于整个生态的部署要求肯定是“效率越来越高、成本越来越低”越好。英特尔院士、大数据技术全球 CTO 戴金权对此也表示：“从计算的角度来看，大模型需要很多的预训练，把模型预训练出一些比较好的基数。训练之后如何去用它、部署它，包括推理效率、微调效率，包括大模型其实是嵌入在一个端到端的一个工作流里面去后还能保持工作负载平衡。从这种计算角度来说，除预训练外，还需要做更多计算场景的策略和优化。”</p><p></p><p>戴金权的观点也显示出了英特尔的技术探索路径。为了保证负载平衡，英特尔提出了 Habana®Gaudi®2 的解决方案，其专注于深度学习的高性能解决方案，可满足大规模、高复杂性生成式 AI 和大型语言模型 (LLM) 训练工作负载的需求。</p><p></p><p>Gaudi2 采用经过验证的高性能深度学习 AI 训练处理器架构，利用 Habana 完全可编程的 TPC 和 GEMM 引擎，支持面向 AI 的高级数据类型，如 FP8、BF16、FP16、TF32 和 FP32 等，是一款性能更高的计算架构。值得一提的是，TPC 是一款 VLIW SIMD 矢量处理器，其指令集和邮件经过定制，不仅支持深度学习训练和推理工作负载，还可高效处理工作负载。</p><p></p><p>除了计算能力突出，Gaudi2 的内存带宽和容量也十分突出，其采用先进的 HBM 内存技术，内存容量高达 96GB，内存带宽高达 2.4TB/s。Gaudi 先进的 HBM 控制器已针对随机访问和线性访问进行了优化，在各种访问模式下均可提供高内存带宽。</p><p></p><p>Gaudi2 的能力其实就是帮助企业通过优化训练流程来降低成本——通过提高训练效率来减少训练时间，同时优化模型结构，减少参数量，从而降低算力和成本。除了这两种方式，企业其实还可以采用更加经济的算法和硬件资源来实现“算力与成本之间的平衡”，例如使用 GPU 代替 CPU 进行计算，目前很多硬件厂商也都在此方向上进行发力。</p><p></p><p>比如英特尔®Data Center GPU Max 系列则是专为应对最严苛的高性能计算 (HPC) 和 AI 工作负载而设计。英特尔&nbsp;®Xe Link 高速、一致的统一架构可灵活运行任何外形规格，实现纵向扩展和横向扩展。其利用“基于独立 SRAM 技术”的高达 408 MB 的 L2 高速缓存 (Rambo)、64 MB 的 L1 高速缓存，以及高达 128 GB 的高带宽内存，确保高容量和高带宽。同时还利用每个英特尔®&nbsp;Max 系列 GPU 上高达 128 个光线追踪单元，加速了科学可视化和动画过程；利用搭载深度脉动阵列的英特尔®&nbsp;Xe Matrix Extensions (XMX)，在单个设备上加速了 AI 工作负载，并启用矢量和矩阵功能，极好地帮助企业找到了算力与成本之间的平衡。</p><p></p><p></p><h2>二、大模型的部署：除了解决多场景，更重要的是提高效率</h2><p></p><p></p><p>戴金权对于“未来 AI 大模型技术创新及发展潜力”有许多值得行业从业者咂摸的观点：“大模型给了我们一个启示，大模型技术的前提不只是计算，而是训练本身，比如三阶段的训练，举个例子——很多大模型“诗写的好”，但是“写代码”不行，然后你就会发现它一般都会再发一个相应的“code 大模型”；而“什么都行”的大模型可能写代码就没有“code 大模型”写的好。其实本质上它是一个多任务或多目标的学习，所以是不是有办法来提升通用大模型的单项能力，这是一个很有意思的探索方向。但不管算力也好、成本也好、效率也好，怎么样利用是需要大家共同去探索的问题。比如大模型有很多不同的部署的场景，预训练、微调、推理、嵌入到工作流里去等等。如何通过硬件的 XPU 不同计算平台、软件上的各种技术能力来提高它的部署效率，这是另一个需要各厂商要去探索的问题。”</p><p></p><p>从戴金权的观点出发，并基于笔者对于行业的观察，我们基本上是可以总结出大模型当前的部署现状的：</p><p>模型部署难度较高：随着模型规模的不断扩大，需要消耗的计算资源、存储资源、网络资源等也越来越多，部署难度逐渐增大。对硬件资源需求大：大模型需要大量的 GPU 内存来进行计算，需要高性能的服务器来存储和传输数据，对硬件资源的需求非常大。需要支持并发处理：为了提高模型推理速度和效率，需要支持并发处理，这对服务器的并发处理能力提出了更高的要求。</p><p></p><p>从部署问题上，英特尔的合作伙伴腾讯云的解决方案就非常值得借鉴，在易用性方面，腾讯云训练集群的开启涉及复杂的系统设计，如 HCC 集群和分布式计算网络互通，并在实例设计时呈现给 AI 开发者一键部署功能，实现工程化效率提升；此外在供训练过程中，HCC 还具有高稳性能和故障自愈能力。从成本方面，腾讯云通过资源调度（如潮汐算力）实现集群效率最高。例如，在训练过程中，可能不会对加速芯片本身进行调度，而是将数据预处理或 DLC 业务与逻辑计算单元混部，以提高算力集群利用率。在部署效率方面，AI 开发者常遇到驱动版本不一致、兼容性等问题。腾讯云致力于在云原生环境中为大家提供更多一键部署和开发工具链，以缩短开发时间并提高效率。”</p><p></p><p>当然了，为了解决大模型的部署问题，<a href="https://www.infoq.cn/minibook/8XJWG3OkRtc7pBBTY172">英特尔</a>"确实没有少做努力。比如专为大模型时代发展而生的 Gaudi®&nbsp;2 在第一代基础上做了许多升级，第二代 Gaudi AI 深度学习夹层卡 HL-225B 专为数据中心实现大规模横向扩展而设计。其 AI 处理器基于第一代 Gaudi 的高效架构打造而成，目前采用 7 纳米制程工艺，在性能、可扩展性和能效方面均实现了飞跃，是一个“名副其实”的用于生成式 AI 和 LLM 训练的功能强大且经济高效的深度学习解决方案。</p><p></p><p>尤其值得说的是，在扩展性方面，Gaudi2 处理器具备出色的 2.1 Tbps 网络容量可扩展性，原生集成 21 个 100 Gbps RoCE v2 RDMA 端口，可通过直接路由实现 Guadi 处理器间通信。Gaudi2 处理器集成了专用媒体处理器，用于图像和视频解码及预处理。此外，Gaudi2 深度学习夹层卡还符合 OCP OAM 1.1（开放计算平台之开放加速器模块）等多种规范，可以为企业业务带来系统设计的灵活性。</p><p>在 2023 英特尔 On 技术创新峰会上，英特尔介绍的一台大型 AI 超级计算机，便是完全采用了英特尔至强处理器和 4000 个英特尔 Gaudi2 加速器打造的，据说它将跻身全球 TOP15 超算，目前热门 AIGC 应用 Stable Diffusion 的开发商 Stability AI 已经在全面使用它。同时英特尔首席执行官帕特·基辛格在本次峰会上还向大家透露了 Gaudi 3 的推出进程，“采用 5nm 制程的 Gaudi 3 将于明年推出，其算力是 Gaudi 2 的两倍，网络带宽、HBM 容量是 Gaudi 2 的 1.5 倍。”这意味着，大模型的部署效率问题可能在明年将实现一个飞跃式发展。</p><p></p><p>事实上，除了 Gaudi 2，为了更好地完成大模型的部署，英特尔®&nbsp;至强®&nbsp;可扩展处理器也一直在升级迭代，其无处不在的计算解决方案，配备英特尔®&nbsp;AMX 和其他集成式 AI 加速器，可在数据中心或边缘应用运行实时、中等吞吐量、低延迟的模型及应用。像阿里云通义千问大模型便是内置 AI 加速器的第四代英特尔至强可扩展处理器用于其生成式 AI 和大语言模型，英特尔技术大幅缩短了该模型的响应时间，平均加速可达 3 倍。</p><p></p><p>基辛格表示，第五代英特尔®&nbsp;至强®&nbsp;可扩展处理器未来将在同样功耗下，将有效提升数据中心的性能和存储速度，相比于第四代，该处理器在 AI 方面的性能将提升 2-3 倍。据悉，该处理器将于 12 月 14 日发布，非常值得大家密切关注。</p><p></p><p></p><h2>三、大模型的安全：将成为未来需要重点关注的问题</h2><p></p><p></p><p>今年 8 月底，首批通过备案的人工智能大模型名单出炉，这意味着这些生成式 AI 产品可以正式面向公众开放注册、提供服务。那在发布前后，大模型应用技术的开发速度或者供应商方面的技术演进上有何变化？对于该问题，戴金权表示——“如何更好地保护模型、保护数据、保护业务问题等安全问题变得越来越重要。”</p><p></p><p>所有技术在经历了爆火和高速发展的过程后，最终都会落到“安全”问题上，所以大模型也不例外。伴随着 AI 大模型的复杂性和应用范围将进一步扩大，其安全隐患将越来越多。例如，随着量子计算等新技术的出现，AI 大模型将面临更高级别的安全威胁。同时，随着数据隐私保护等法律法规的出台，企业当前越来越重视 AI 大模型的数据隐私保护工作。因此，未来需要加强技术研发，完善 AI 大模型的安全保障机制。</p><p></p><p>当前 AI 大模型安全现状并不乐观，技术漏洞是当前 AI 大模型面临的主要安全问题之一。例如，模型被黑客攻击、恶意注入病毒等问题时有发生。代码实现不当也可能导致 AI 大模型出现安全问题，比如有些模型在实现过程中可能存在未经验证的功能或逻辑漏洞，给恶意攻击者留下可乘之机。</p><p></p><p>我们溯源一下问题根本，数据质量差是影响 AI 大模型安全的重要因素之一。例如，如果数据本身存在大量噪声或缺失，将直接影响模型的训练效果和安全性。为了保护、清洗这些数据，<a href="https://www.infoq.cn/article/X50dOoVNWEIlhSvvCpiE">英特尔</a>"在机密计算领域投入大量研发资源，在 2015 年推出了英特尔®&nbsp;SGX，其是一种安全相关的指令，被内置于一些现代 Intel 中央处理器（CPU）中，它可以在基于硬件的可信执行环境中执行计算，确保任务和数据的安全性，防止被恶意程序窃取。在管理敏感数据和受监管数据方面，机密计算技术可以提高相关组织的安全级别。</p><p></p><p>此外，英特尔®&nbsp;TDX 是另一项前沿安全技术，其在虚拟机层面支持机密计算，满足虚拟机安全需求。所以英特尔的“机密计算”也被戴金权称为是一个“端到端”的能力，“大模型安全并不是只需要在一个环节安全，整个流程都需要安全，而英特尔的机密计算从数据存储、加密、整个分布式计算、网络通讯，包括远程验证等都完成了实现了安全保护。”目前英特尔作为“机密计算联盟（Confidential Computing Consortium）”成员之一，正在持续积极推动机密计算技术的标准化和普及。</p><p></p><p></p><h2>四、写在最后：AI 大模型对基础设施、硬件提出了更高要求</h2><p></p><p></p><p>随着大模型技术逐渐进入深水期，各企业在相关技术方面的验证逐渐全面，大家都已经非常明确，如果想要充分释放 AI 大模型的潜力，仅依靠软件层面的优化是不够的，基础设施硬件设备的性能和稳定性也在 AI 大模型的高效运行中扮演着至关重要的角色。</p><p></p><p>当前大模型对基础设施的要求非常高。就单从硬件方面来看，大模型需要大量的高性能计算资源，包括 CPU、GPU 和 TPU 等。这些计算资源需要具备高并发、低延迟的特点，以满足 AI 大模型的计算需求。同时，为了提高计算效率，需要采用先进的芯片设计和制造技术，加强芯片间的通信和协作。</p><p></p><p>为了满足大模型对硬件性能的高要求，硬件厂商需要不断提升自身的研发实力和技术积累。这包括对先进制程技术的掌握，以及对各种处理器架构的深入理解。此外，硬件厂商还需要与软件厂商紧密合作，共同优化大模型的性能。通过软硬件的协同创新，可以充分发挥硬件设备的性能潜力，为大模型的发展提供强大的支持，无论是从算力、效率、成本还是安全等各个方面。</p><p></p><p>于此，大模型对硬件厂商的技术能力也提出了更高的要求。这意味着硬件厂商需要具备跨学科的能力，以整合不同领域的技术资源，为企业提供更加完善的解决方案，以满足不同行业和应用场景的需求。</p><p></p><p>不仅是硬件厂商，大模型技术的发展离不开产业链上的每一个角色，众人拾柴才能火焰高，大模型时代需要学术界和产业界进行深入地合作和联动。通过联动，学术界的研究成果可以更快地应用于产业界，推动技术的发展和进步，同时产业界的需求和反馈也可以引导学术界的研究方向，使其更加贴近实际应用场景。在当前这个大模型时代的背景下，合作和联动可以促进不同组织之间的协作，实现资源的共享和整合，提高研究的效率和成果的质量。</p><p></p><p>正如戴金权所说的那样，“<a href="https://www.infoq.cn/article/cff5Oa9fYLNtU46us0ez">英特尔</a>"一直坚持开源开放，无论是从客户侧的产业界合作，还是从学术界的高校合作，英特尔都在持续推动，相信在多方的努力下，大模型技术的发展将会越来越好。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QFytIPELB8ZxSrmDpLXj</id>
            <title>OpenAI悄悄改变核心价值观惹争议：埋头搞AGI，其他的都是浮云！</title>
            <link>https://www.infoq.cn/article/QFytIPELB8ZxSrmDpLXj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QFytIPELB8ZxSrmDpLXj</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 05:41:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 核心价值观, AGI, 人工智能
<br>
<br>
总结: OpenAI最近修改了其核心价值观，将通用人工智能（AGI）纳入其中。这一变化引发了人们对其核心价值观真实性和可靠性的质疑。尽管人们对于核心价值观的转变持怀疑态度，但OpenAI的未来方向已经调整，将致力于开发安全、有益的AGI技术，借此对人类产生积极影响。 </div>
                        <hr>
                    
                    <p></p><blockquote>最近几周，OpenAI悄然修改了其网站上列出的所有“核心价值观”，更加强调 AGI（通用人工智能）的发展。</blockquote><p></p><p></p><h2>OpenAI悄悄改变核心价值观，重点聚焦AGI</h2><p></p><p>&nbsp;</p><p>据外媒报道，最近几周，作为全球领先的AI研究机构，OpenAI正悄悄对其核心价值观做出重大调整，将之前未明确列出的通用人工智能 (AGI) 纳入其中。</p><p>&nbsp;</p><p>据&nbsp;Semafor报道，该公司此前的价值观为“大胆”、“深思熟虑”、“朴实无华”、“影响力驱动”、“协作”和“以增长为导向”。这些旧点价值观将被一系列新的价值观所取代、明确将AGI列为后续工作的重中之重。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0b153e221b9044420859bf71377018d2.png" /></p><p></p><p>&nbsp;截图来源：OpenAI官网首页</p><p>&nbsp;</p><p>新变更的价值观主要包括五点：</p><p>&nbsp;</p><p>聚焦通用人工智能</p><p>OpenAI致力于构建安全、对社会有所助益的人工智能，它将对人类未来产生巨大的积极影响。</p><p>与此无关的任何事情都不在考虑范围之内。</p><p>&nbsp;</p><p>坚韧不拔、勇往直前</p><p>创造非凡的事物需要努力工作（通常是不那么吸引人的任务）和紧迫感；我们所做的每一件事都很重要。要谦逊务实，想尽一切办法做切实可行的事。</p><p>&nbsp;</p><p>坚守规模化效应</p><p>当在我们的模型、系统、自身、流程以及理想抱负达到一定规模时，就会创造奇迹。当受到质疑时，扩大规模是一种十分奏效的方式。</p><p>&nbsp;</p><p>制造出让人喜爱的东西</p><p>OpenAI的技术和产品应当对人们的生活带来革命性的积极影响。</p><p>&nbsp;</p><p>团队精神</p><p>OpenAI最大的进步和差异化来自于团队内部和之间的有效协作。虽然OpenAI的团队有着越来越多的不同身份和优先事项，但整体目标和宗旨必须保持完全一致。凡是归因自身，没有什么问题是别人的问题。</p><p>&nbsp;</p><p>OpenAI 多年来一直表示希望开发 AGI，尽管这种技术的具体细节尚不清楚。在 2018 年发布的一份使命声明中，OpenAI 将 AGI 描述为“在最具经济价值的工作中超越人类的高度自治系统”。</p><p></p><h2>价值观变更惹争议，说变就变也太随意了</h2><p></p><p>&nbsp;</p><p>这一变化引发了人们对这些核心价值观真实性和可靠性的质疑。如果一家企业能够轻松更改其核心价值观，那么这些价值观还能否称得上“核心”？这无疑会激起外界对于该公司在既定目标一致性和承诺方面的担忧。</p><p>&nbsp;</p><p>将“聚焦AGI”作为公司核心价值之举尤其值得注意。而且，OpenAI对于AGI的解释似乎也仍有含糊不清之处。今年 2 月，OpenAI 的首席执行官山姆·奥尔特曼（Sam Altman）在公司博客文章中写道，AGI 可以广义地定义为“通常比人类更聪明的系统”，但在最近一次采访中，奥特曼似乎重新将AGI定义为与普通人类等同的人工智能。</p><p>&nbsp;</p><p>这种AGI定义层面的差异，也进一步引发了关于OpenAI发展目标的讨论。作为一家随时间推移而不断调整方向的公司，OpenAI已经从一家专注于打造良好AI的非营利组织，转变成一家营利性实体。这种目标转变似乎又反过来影响了其技术定义与核心价值观。</p><p>&nbsp;</p><p>尽管人们对于核心价值观的转变持怀疑态度，但OpenAI的未来方向的确已经在就此做出调整。该公司强调将致力于开发安全、有益的AGI技术，借此对人类产生积极影响。也就是说，OpenAI在接下来的经营活动当中将优先考虑与该目标相适应的项目和举措。</p><p>&nbsp;</p><p>在新核心价值观的加持下，OpenAI明显将投入更多资源推动AGI议题。然而，他们将采取怎样的AGI定义方式和实现思路仍然有待观察。</p><p></p><h2>网友怎么看？</h2><p></p><p>&nbsp;</p><p>OpenAI价值观变更一事在Reddit上引发了积极讨论，有网友猜测，“AGI已在OpenAI内部实现了”。一些用户认为这种变更很有意思，“看起来OpenAI似乎已经弄清楚了如何让自己更加强大，现在只是在寻找人来实际建造它（AGI），新的核心价值观读起来更聚焦。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/47/475f52621980c42d460ef9dddf47c0b3.png" /></p><p></p><p>&nbsp;截图来源：Reddit</p><p>&nbsp;</p><p>但也有一些用户对于价值观的改变表现出了担忧，一名ID为Freedom_Alive的用户称：</p><p>&nbsp;</p><p></p><blockquote>“这让我想起了谷歌从其核心价值页面中删除“不作恶”的时候。”</blockquote><p></p><p>&nbsp;</p><p>言外之意，OpenAI价值观的改变也说明了公司行事风格将会与以前不同了。</p><p>&nbsp;</p><p>ID名为Accurate-Ease1675的用户则表示，“让我有点担心的是，像 OpenAI 这样的公司似乎并不理解价值观、使命、目标和愿景之间的区别。以前的价值观没问题，但修改后的价值观不是真正的价值观。它们是一些雄心勃勃的陈述的大杂烩，如果需要做一些额外的工作来解释这些所谓的价值观。 OpenAI之前的价值观中有一条是深思熟虑，现在看来，他们压根也没实现这一价值观。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://ts2.space/en/openai-updates-core-values-to-include-artificial-general-intelligence-agi/">https://ts2.space/en/openai-updates-core-values-to-include-artificial-general-intelligence-agi/</a>"</p><p><a href="https://futurism.com/the-byte/openai-core-values-agi">https://futurism.com/the-byte/openai-core-values-agi</a>"</p><p><a href="https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html">https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9AYU96ZSPoCZ6kyClK94</id>
            <title>创新风潮迭起，2023深圳国际金融科技大赛——西丽湖金融科技大学生挑战赛正式启动</title>
            <link>https://www.infoq.cn/article/9AYU96ZSPoCZ6kyClK94</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9AYU96ZSPoCZ6kyClK94</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 05:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融科技领域, 市场规模, 科技创新, 金融科技节
<br>
<br>
总结: 近年来，我国金融科技领域发展迅速，市场规模稳步增长。政府和企业界形成了良好的产学研合作机制，推动金融科技产业发展。深圳市金融科技节作为重要的活动之一，吸引了全球金融科技人才参与。大赛旨在鼓励学生探索金融科技领域的创新应用，为行业提供有价值的技术解决方案。同时，大赛还邀请了多位专家担任学术顾问和评委，为参赛团队提供支持和指导。 </div>
                        <hr>
                    
                    <p>近年来，我国在金融科技领域取得显著发展。根据赛迪顾问《金融科技发展白皮书》数据显示，自 2016 年起相关市场规模一直保持着 10% 左右的稳定增速，2022 年的市场规模同比增长 18.3%。10 月 8 日，《深圳市关于金融支持科技创新的实施意见》正式印发实施，明确将进一步完善金融支持科技创新体系，加大对科技型企业融资的支持力度，建立健全“基础研究 + 技术攻关 + 成果产业化 + 科技金融 + 人才支撑”的全过程创新生态链。</p><p></p><p>为了满足金融科技产业技术创新及人才需求，更好地推动金融科技产业发展，目前政府、学术界和企业界形成了良好的产学研合作机制。作为 2023 年<a href="https://www.infoq.cn/article/7ejrDIB7r5KRIuLwaRPd">深圳市金融科技节</a>"的重要一环，在深圳市地方金融监督管理局、深圳市福田区人民政府、深圳市南山区人民政府战略指导下，由深圳大学、微众银行、深圳香蜜湖国际金融科技研究院等多方联合举办的“2023 深圳国际金融科技大赛（FinTechathon）——西丽湖金融科技大学生挑战赛”（下文称“大赛”）于 10 月 16 日正式开赛。</p><p></p><p>据悉，该赛事自 2019 年落地至今，已成功举办四届并完成了<a href="https://www.infoq.cn/article/5DkdKQyjRuC9YNTgbhm6">赛事品牌升级</a>"。大赛汇聚了全球前沿的金融科技人才，其高水平的参赛者、极具挑战性的赛题内容和评委的卓越见解，在过往四届吸引了 3500 余名来自海内外知名高校的学生参赛，备受金融科技领域从业者的关注和认可。本届大赛组委会将基于往届办赛经验，继续进一步提升赛事体验和评选质量。大赛全程聚焦金融科技的前沿理论，分设人工智能、区块链、产品经理三个赛道，将通过初赛、复赛在各赛道分别遴选出 10 支队伍进入决赛角逐，并设置总额超过 69 万人民币的赛事奖金及参赛专属电子区块链证书，以奖励各赛道获得一等奖、二等奖、三等奖的队伍及成员。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7b/e1/7b9055577073aea33b430e1f0ea373e1.jpg" /></p><p></p><p>本次大赛致力于鼓励国内外高校学生积极探索金融科技领域的技术应用创新，将创新成果转化为实际应用，为金融科技行业提供更多有价值的技术解决方案。为此，大赛组委会特别邀请了国家统计局原副局长许宪春；加拿大皇家科学院院士、加拿大工程院院士、微众银行首席人工智能官杨强；清华大学五道口金融学院教授、华夏银行原行长、中国人民银行研究局原局长张健华；中国工商银行首席技术官吕仲涛；上海新金融研究院副院长、浙商银行原行长刘晓春；全国政协委员、南方科技大学副校长金李；中国银行业协会首席信息官高峰等人担当学术顾问，为大赛提供智力支持，帮助参赛团队更好地理解和应用金融科技知识。</p><p></p><p>此外，大赛组委会还邀请了来自中科院、清华大学、中山大学、西安电子科技大学、深圳大学、武汉大学、中央财经大学、广东财经大学、浙江财经大学、哈尔滨工业大学、微众银行等学企单位的数十位科研专家担任大赛评委，为参赛团队提供专业的指导建议，挖掘优秀的参赛项目和人才，以加快深圳市金融科技产业升级，抢抓金融科技发展机遇。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/dba9c5b835fb450d6ad26674e2cc743b.jpeg" /></p><p></p><p>10 月 16 日起，本届大赛正式开启报名通道，国内外高校在读生（含本科生、硕士 / 博士研究生）均可报名参赛。有兴趣的同学可点击<a href="https://www.infoq.cn/zones/fintechathon/campus2023">链接</a>"进入大赛官网 ，或识别下方海报中的二维码进行报名。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04fc67e1a319867248a9cde86965a33e.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hI5XGjZzTsLxDS2Q9zSL</id>
            <title>同盾科技软件产品及方案部 / 总经理董纪伟确认出席 FCon，分享黑灰产欺诈攻防体系的研究与实践</title>
            <link>https://www.infoq.cn/article/hI5XGjZzTsLxDS2Q9zSL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hI5XGjZzTsLxDS2Q9zSL</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 03:59:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 黑灰产欺诈攻防体系的研究与实践, 董纪伟, 欺诈攻防体系的建设路径和实现建议
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，董纪伟将发表题为《黑灰产欺诈攻防体系的研究与实践》的主题分享，解析欺诈攻防的底层逻辑，给出新一代的欺诈攻防体系的建设路径和实现建议。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。同盾科技软件产品及方案部 / 总经理董纪伟将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5573?utm_source=infoqweb&amp;utm_medium=article">黑灰产欺诈攻防体系的研究与实践</a>"》主题分享，解析欺诈攻防的底层逻辑，通过对攻防内容设计，综合评判欺诈概率，解决信息差的欺诈本质问题，并基于行业的领先实践，给出新一代的欺诈攻防体系的建设路径和实现建议。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5573?utm_source=infoqweb&amp;utm_medium=article">董纪伟</a>"，花名“阅微”，行业资深安全专家，硕士毕业于中国科学院大学计算机技术专业，曾任国密局密码算法课题组成员 ，人民银行支付清算协会反诈培训讲师，北京金融科技产业联盟金融科技领域高级技术专家。</p><p></p><p>目前担任同盾科技软件产品及方案部总经理兼策略模型总监，负责软件产品线相关解决方案、技术及架构，以及安全策略咨询及模型咨询，负责过工行、建行、邮储、广发、中信、银联等金融行业多家机构风控应用及策略模型的设计与研发，主力研发的交易监控反欺诈软件荣获 2015 年人民银行科技发展二等奖。</p><p></p><p>10 余年金融行业工作经验，FRM 金融风险管理师认证。熟悉金融领域风控与反欺诈相关产品、技术、业务及场景解决方案，擅长反欺诈规则、策略设计及特征建模；曾任人民银行下属机构研发部开发经理、项目经理、高级安全咨询顾问、反欺诈团队负责人、反欺诈项目总监等。他在本次会议的演讲内容如下：</p><p></p><p>演讲：黑灰产欺诈攻防体系的研究与实践</p><p></p><p>随着 ChatGPT 的横空出世，人工智能技术发展迅猛。但与此同时，AI 技术被黑灰产滥用的负面作用也逐步显现，移动互联网的发展也促使场景复杂化，黑灰产欺诈的攻击面、攻击点呈现爆发式增长，呈现隐匿化、团伙化、速度化的态势。此次分享的解决方案将通过黑灰产的最新趋势分析，解析欺诈攻防的底层逻辑。通过对不同主体、在不同环节的攻防内容设计，综合评判欺诈概率，解决信息差的欺诈本质问题，并基于行业的领先实践，给出新一代的欺诈攻防体系的建设路径和实现建议。</p><p></p><p>演讲提纲：</p><p></p><p>黑灰产的最新态势分析不知攻焉知防——黑灰产实施攻击的主要手法、攻击链路如何一环扣一环对于欺诈攻防的思考他山之石——如何有效构建欺诈防御体系</p><p></p><p>你将获得：</p><p></p><p>○ 了解最新的黑灰产欺诈形势</p><p>○ 知晓典型的欺诈场景，如 ChatGPT、AI 换脸、屏幕共享如何被黑产利用</p><p>○ 了解反诈的最新技术应用</p><p>○ 了解业内先进的攻防实践</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 7 折优惠 ，立省 ￥2040！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5LWEiT9DNPzCWsoqtdQE</id>
            <title>代码生成：基于AI大模型的挑战与前景</title>
            <link>https://www.infoq.cn/article/5LWEiT9DNPzCWsoqtdQE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5LWEiT9DNPzCWsoqtdQE</guid>
            <pubDate></pubDate>
            <updated>Mon, 16 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 模型, 代码生成, 人工智能, 专业模型
<br>
<br>
总结: 使用 AI 通用模型生成代码可能会导致代码质量问题，因此需要创建专业或专用的模型来解决这个问题。人工智能模型是通过模仿人脑中神经元与突触连接而成的网络构建的。计算机并不会思考，只是利用统计数据对事物进行预测、分类或组合。大语言模型的工作原理是根据统计数据预测下一个标记的最佳匹配，无法对事实进行核查。使用通用模型生成代码可能会导致混杂不同版本代码的问题。AI 可以是解决问题的好帮手，但使用 AI 工具需要检查、验证、修改、编辑或重写部分内容。 </div>
                        <hr>
                    
                    <p>使用 AI 通用模型来完成代码生成这类非常具体的任务可能会带来问题。人工智能生成的代码就像是陌生人的代码，它们可能并不符合你的代码质量标准。这种情况下，创建专业或专用的模型不失为一条出路。</p><p>&nbsp;</p><p>Luise Freese 和 Iona Varga 在<a href="https://ndcoslo.com/">2023</a>" 年的&nbsp;<a href="https://ndcoslo.com/">NDC Oslo</a>" 大会上探讨了 AI 模型的实践困境和伦理相关问题。</p><p>&nbsp;</p><p>Varga 提到，“人工智能”这个词给人一种智慧的感觉，虽然这个名字实际只是代表了这些模型的构建方式。以节点相连的形式模仿人脑中神经元与突触连接而成的网络，这类模型因此而得名“人工网络”或“人工智能”。</p><p>&nbsp;</p><p>Freese 补充道，抽象来说，计算机是完全依赖于或开或关的晶体管，通过这些开关的组合，我们得以操纵比特。由于晶体管之间没有相互的纠缠，这些开关最终会带来这样的结果：</p><p></p><p></p><blockquote>因此，计算机并不会思考，不过是我们的人工智能算法赋予了它们个性和特征，比如“让我考虑一下”这类礼貌说辞。AI 仅仅是利用统计数据对事物进行预测、分类或组合。</blockquote><p></p><p>&nbsp;</p><p>Varga 提到，AI 的问题在与使用极其通用的模型或是基础模型完成非常具体的任务。大语言模型（LLM）的工作原理是先分析问题、创建一两个词语，再根据统计数据预测下一个标记的最佳匹配。此外，LLM 本身是无法对事实进行核查的，因为这类模型的设计目的是生成而非验证。</p><p>&nbsp;</p><p>如果我们试图建立一个能解决所有 AI 问题的 AI 模型，那么我们将会创造出一种自我放大的螺旋式下降，Freese 补充道。若想实现螺旋式上升，那就应该少用基础模型，多用更为具体的模型，后者中有一部分实际就是搭建在基础模型之上的。</p><p>&nbsp;</p><p>AI 或许能生成代码，但这些代码是否能安全地使用，是否能满足我们对质量的标准要求？Varga 认为这些问题只能由真正的人类来回答，这一过程并不容小觑。归根结底，就像是代码的编写一样，调试陌生人的代码远比自己从头到尾参与其中的代码更为困难。</p><p>&nbsp;</p><p>一般模型的理解能力也更为通用，这在代码生成问题上可能会带来问题，正如 Varga 所解释的：</p><p></p><blockquote>举例来说，React v17 或 v16 这些可能没有直接反应在模型的上下文中，但模型也能了解这些代码库。或许你会发现自己生成的一个函数中会混杂有两个版本的代码。</blockquote><p></p><p>Varga 认为，多数情况下 AI 都是解决问题的好帮手。但使用 AI 就意味着你要去检查、验证、修改、编辑或重写部分内容，而这一部分可能才是我们低估 AI 工具带来工作量的地方。</p><p>&nbsp;</p><p>InfoQ 针对人工智能所带来的挑战问题采访了 <a href="https://www.linkedin.com/in/luisefreese/">Luise Freese</a>"&nbsp;和 <a href="https://www.linkedin.com/in/iona-dahlia/">Iona Varga</a>"。</p><p>&nbsp;</p><p>InfoQ：什么因素会造成 AI 的失败？</p><p></p><p></p><blockquote>Iona Varga：一般来说，AI 并不是命中注定要失败的。我是医学物理出身的，我也见过很多优秀的 AI 工具，它们能出色地完成波弹性成像的实时剪切，早期阶段的婴儿检测，甚至能检测出肿瘤专家都无法发现的肺癌细小结节。&nbsp;但由于虚假数据和扭曲事实问题的存在，这些结果并不完全可信。举例来说，川普就职典礼上，实际的到场人数是要少于最初公布的数据。试着问模型就职典礼的公园有多热闹，你大概会得到一个出乎意料的答案。但同样，数据的来源时至今日也有颇具争议的历史背景，它们可能会出于政治剧本或标准等原因而被修改。</blockquote><p></p><p></p><p>InfoQ：伦理道德如何才能帮助我们解决 AI 所带来的问题？</p><p></p><p></p><blockquote>Luise Freese：伦理道德作为工具本身是帮不上太多忙的。伦理只是一种工作的方式，就像是 DevOps 一样。一旦你有了规划，知道该做什么了，“伦理道德”就是你对“完成”的定义。我所用的数据是否覆盖了所有产品使用相关的人或事？通过这些道德的检测，我们的工作方式将会在可访问性、包容性和避免偏见方面得到改善。</blockquote><p></p><p>&nbsp;</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/producing-quality-code-AI/">The Challenges of Producing Quality Code When Using AI-Based Generalistic Models</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>