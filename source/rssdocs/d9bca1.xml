<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/BS9euQ1bfMYvfipxRwAK</id>
            <title>大模型时代，计算创新如何为应用性能提升开启新路径</title>
            <link>https://www.infoq.cn/article/BS9euQ1bfMYvfipxRwAK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BS9euQ1bfMYvfipxRwAK</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 08:11:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库, 向量数据库, 大模型, 性能提升
<br>
<br>
总结: 数据库在云计算和大数据时代的重要性更加凸显，大模型需要向量化的数据来提升知识查找效率，传统数据库不适合作为大模型的知识库，需要新一代向量数据库来满足需求。腾讯云向量数据库是一款全托管的自研企业级分布式数据库服务，具备高效的存储、检索和分析能力，与英特尔公司合作提升性能。向量数据库的性能提升需要全新的硬件设计和软件优化，英特尔的 AVX-512 指令集和 AMX 高级矩阵扩展加速引擎可以提供更高的密度和执行效率，大幅提升向量检索性能。 </div>
                        <hr>
                    
                    <p>引言：数据库一直是 IT 基础设施的核心组件之一，在云计算和大数据时代，数据库的重要性更加凸显。随着生成式 AI 应用开始广泛流行，企业更加需要海量数据来为大模型提供充足的数据养分。</p><p></p><p>对于大模型而言，模型所需的数据都经过了向量化（Vector embedding）过程，经过向量化的数据可以大幅提升模型的知识查找效率，使模型可以支持更长的上下文，同时降低训练和推理成本。因此，传统数据库不适合作为大模型的知识库，这就需要新一代向量数据库来满足大模型的数据需求。如今，行业内已经涌现了一批能力出色、为大模型应用做了充分优化的向量数据库产品，腾讯云向量数据库就是其中的佼佼者之一。作为一款全托管的自研企业级分布式数据库服务，其能为多维向量数据提供高效的存储、检索和分析能力，具备完善的嵌入功能，兼具高性能、高可用性，稳定性、可靠性，且使用简单，成本低廉。借助上述优势与特性，腾讯云向量数据库正成为用户构建能力时的强力后援团。</p><p></p><p>对于向量数据库而言，只有具备极高的性能水平，才能为规模迅速扩大的模型和上层应用提供充足的知识查询吞吐量和极低的查询延迟。然而，仅靠软件端的优化来提升性能是不够的，软硬协同才是增强向量数据库性能的最佳路径。为此，腾讯云与英特尔公司展开了深度合作，将英特尔第五代可扩展至强处理器的诸多优势特性融入腾讯云向量数据库的软件设计中，从而显著提升性能水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f0bb7f37065783862d345102e632bb2c.png" /></p><p>腾讯云向量数据库能与 LLM 模型配合使用，为 LLM 模型提供知识库图片及相关介绍援引自腾讯云官网介绍，详情请参阅：https://cloud.tencent.com/product/vdb</p><p></p><p>对于向量数据库而言，只有具备极高的性能水平，才能为规模迅速扩大的模型和上层应用提供充足的知识查询吞吐量和极低的查询延迟。然而，仅靠软件端的优化来提升性能是不够的，软硬协同才是增强向量数据库性能的最佳路径。为此，腾讯云与英特尔公司展开了深度合作，将英特尔第五代可扩展至强处理器的诸多优势特性融入腾讯云向量数据库的软件设计中，从而显著提升性能水平。</p><p></p><p></p><h1>向量数据库，全新架构带来的硬件性能挑战</h1><p></p><p></p><p>AI 任务中所用的各类数据通常会以向量形式表示。向量（Vector）是一种具有大小和方向的数据表示，其包含多个维度的信息，且每个维度都会用来表示一个特征或属性。正因如此，以向量形式来构建的模型知识库无疑将更具效率，例如在图像处理任务中，图像可表示为像素值的向量；而自然语言处理任务中，文本可表示为词向量或句子向量。</p><p></p><p>数据的向量化是借助词向量模型、卷积神经网络等模型，经过嵌入（Embedding ）环节将文本、图像、音视频等不同数据转换为向量后存入向量数据库中，而向量查询则是通过对向量之间的相似度计算来完成。在实践中，向量数据库可作为知识库，通过与语言、图像等 AI 模型的结合来有效降低用户的模型训练成本，提升 AIGC 等应用的信息输出准确度和及时性。同时向量数据库还可用于模型预训练数据的分类、去重和清洗，相比传统方式实现效率的大幅提升。</p><p></p><p>向量数据库检索向量的方法主要是相似度度量，也就是通过计算来确定向量数据库中两个向量的相似程度，并最终找出与给定查询向量最相似的向量。常见的计算方法包括内积、欧式距离和余弦相似度，而处理器在运行这些计算方法时，主要涉及大量的密集矩阵向量运算过程，同时还需要提供足够高的访存吞吐量。当处理器有很多核心时，核心之间的互联指标也会影响整体的计算效率。相比之下，传统数据库的查询操作主要是非向量化计算过程，属于非密集运算，其对处理器的需求与向量数据库有着巨大差异。显然，为传统数据库优化的处理器和软件设计对于向量数据库来说效率就很难令人满意。为此，处理器需要全新的硬件设计，大幅加强密集向量运算性能、内存吞吐量、多核互联性能等指标，结合软件层面的深度优化，才能为向量数据库提供充足的能力基础。</p><p></p><p></p><h1>突破传统算力增长瓶颈，新一代处理器的性能提升秘籍</h1><p></p><p></p><p>2017 年，计算机体系架构宗师 David Patterson 与 John Hennessy 在斯坦福大学发表著名演讲，指出针对特定应用场景的针对性性能加速设计将是下一个十年中芯片领域的主要命题。两位宗师将这种定制加速设计称为 DSA 领域加速计算。正如这次演讲预言的那样，过去几年来行业内的芯片性能提升更多来自各类定制化加速方案，诸如 GPGPU、TPU 等。而在 CPU 层面，扩展指令集和专用加速引擎也已成为 CPU 和上层软件提升性能的快捷路径。进入大模型时代，领域加速计算创新更能大大缓解处理器通用计算能力遭遇瓶颈带来的挑战，使得 CPU 同样可以在 AI 时代焕发全新动能，继续承担算力基础设施的核心角色。</p><p></p><p>作为 CPU 行业的领军企业，英特尔公司一直非常重视 CPU 扩展指令集和专用加速引擎的研发和创新探索，英特尔第五代可扩展至强处理器内置的英特尔 AVX-512 指令集与英特尔 AMX 高级矩阵扩展加速引擎就是这些探索的最新成果。这两种加速设计可以帮助腾讯云向量数据库在更高的性价比前提下构筑更高密度的算力输出，深度优化其向量检索算法的执行效率，大幅提升检索性能。腾讯云向量数据库支持适合不同场景下的多种算法，而 AVX512 可以为包含 FP32 在内的多种数据格式的向量检索的计算提供 SIMD 的加速支持, &nbsp; AMX 更是支持 BF16 和 int8 数据格式的矩阵运算 从而进一步加速向量检索，实现更高的性能。二者可以适用于不同的场景下数据计算需求, &nbsp;为客户提供了更多的选择。</p><p></p><p>作为一种单指令多数据（SIMD）指令集，英特尔 AVX-512 在密集型计算负载中有着得天独厚的优势。得益于其 512 位的寄存器宽度和两个 512 位的融合乘加（FMA）单元，指令集能并行地执行 32 次双精度、64 次单精度浮点运算，或操作 8 个 64 位和 16 个 32 位整数。在腾讯云向量数据库所需的向量相似度计算中，假设数据类型是 FP32，输入向量 x 中的 16 个维度数据和数据库中向量 y 的 16 个维度数据，都可以一次性被加载到英特尔 AVX-512 的寄存器中，从而实现一次处理 16 个维度的并行运算，效率提升极为可观。由于在各类向量检索算法中类似计算需求的比重往往很高，向量数据库的性能由此可获得巨大提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b7349ac190de4232946b112ba1e807f.png" /></p><p>*&nbsp;英特尔®SSE、英特尔®AVX2 和英特尔® AVX-512 之间的寄存器大小和计算效率的差异说明</p><p></p><p>另一项可为腾讯云向量数据库带来显著加速的是英特尔 AMX 加速引擎。这一技术引入了一种用于矩阵处理的新框架（包括了两个新的组件，一个二维寄存器文件，其中包含称为 tile 的寄存器，以及一组能在这些 tile 上操作的加速器），从而能高效地处理各类 AI 任务所需的大量矩阵乘法运算，提升其在训练和推理时的效能。例如在向量检索的过程中，如存在 n 个 batch 任务，进行相似度计算时就需要对 n 个输入向量 x 和 n 个数据库中向量 y 进行比对，这其中的距离计算会产生大量的矩阵乘法， 而英特尔 AMX 能针对这一场景实现有效加速。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/7224544b2cc4ab33b4420f07249f7004.png" /></p><p>英特尔® AMX 架构由 2D 寄存器文件 (TILE) 和 TMUL 组成</p><p></p><p>单纯依靠 AVX-512 扩展指令集和 AMX 加速引擎也是不够的，为了充分发挥这两种技术的加速能力，处理器的通用算力、单核性能、多核互联水平、访存性能都需要很高的水平，以避免成为密集向量和矩阵运算的瓶颈。而英特尔第五代可扩展处理器最大规格可拥有 64 个核心，通用计算性能相比上一代在相同能耗下提升 21%，内存带宽提升 16%，UPI 互联性能提升 25%，单核性能也有明显提升，因此 AVX-512 和 AMX 加速引擎的输出效率相比上代能有高达 40% 的增强。</p><p></p><p>硬件性能的巨大进步，需要软件层面的充分优化才能在实际应用中发挥作用。为此，腾讯云与英特尔一起，针对腾讯云向量数据库常用的计算库进行了专门优化。包括：</p><p></p><p>FAISS：方案中针对其不同的索引提出了不同的优化方案，包括面向 IVF-FLAT 算法的单次读取和离散化两种优化思路，以及借助英特尔 AVX-512 加速 IVF-PQFastScan 算法和 IVF-SQ 索引的优化方案；</p><p></p><p>HNSWlib：方案借助英特尔 AVX-512，对 HNSWlib 的向量检索性能进行加速。同时方案也针对增删数据后的性能和召回率抖动的问题进行了专向优化，使 HNSWlib 的性能和召回率可以保持较平稳状态。</p><p></p><p>英特尔还为腾讯云向量数据库提供了英特尔 FMAL 加速库。暴力搜索在海量向量数据场景非常常用，但这一场景对算力需求非常高。作为针对向量暴力搜索场景开发的算法库，英特尔 FMAL 在英特尔 AVX-512 和英特尔 AMX 的加持下，能对相似度计算进行加速并提供了相似度计算和 top-K 查询的 API 接口，英特尔 AMX 还能帮助英特尔 FMAL 对 INT8 数据类型提供更好的性能输出。同时，英特尔 FMAL 还能在多线程并发下对处理器资源进行合理调配，以便让用户充分挖掘最新处理器所具备的多核心优势。除此之外，加速库也提供了对内存的非一致内存访问架构优化和缓存数据对齐功能，这些都进一步提升了腾讯云向量数据库的性能。</p><p></p><p>为验证第五代英特尔至强可扩展处理器基于英特尔 AVX-512 及英特尔 AMX 为腾讯云向量数据库中向量检索任务提供的助力，腾讯云与英特尔携手开展了验证测试，测试分为两个场景：第一个场景中，使用英特尔 AVX-512 优化后，使用 IVF-PQFastscan 算法执行向量检索时的 QPS 性能相比基于第三代至强可扩展处理器的基准组提升了高达 230%。第二个场景中，同样使用第五代至强可扩展处理器的算力平台上，使用英特尔 AMX 加速数据格式为 INT8 的测试场景对比使用英特尔 AVX-512 加速数据格式为 FP32 的测试场景，性能提升高达 5.8 倍之多，显示英特尔 AMX 可以进一步大幅提升 INT8 数据格式下的腾讯云向量数据库向量检索效率。</p><p></p><p>以上实测对比显示，领域加速技术已经成为新一代处理器大幅增强 AI 等创新应用场景性能的秘籍。值得一提的是，英特尔的加速技术不仅可以显著提升数据库吞吐量，还可以有效降低查询延迟，缩短每个查询需要的时间。除了复杂度较高的向量计算外，腾讯云向量数据库同时还提供了 AI 套件实现一站式文档检索解决方案， 包含自动化文档解析、信息补充、向量化、内容检索等能力，并拥有丰富的可配置项。向量数据库的应用场景还会涉及到不同的集合大小、维度数、数据类型、查询召回精度、延迟的要求， 这对 CPU 通用算力、加速算力、访存性能各项综合能力都提出了更高更快的要求。&nbsp;而英特尔第五代至强处理器的单核通用算力、内存速度和带宽、L3 缓存等指标相比此前的至强都有了广泛的提升，以满足向量数据库在不同场景下的算力要求，结合极强的领域加速能力，助力向量数据库打通性能瓶颈，为大模型的高效训练和推理工作奠定坚实基础。</p><p></p><p>在软件优化层面，英特尔为客户提供了模块化的代码优化方案，非常方便客户根据自身的实际场景快速集成，已经有一些客户将这些优化代码成功集成到自身产品中，给出了良好的反馈。</p><p></p><p></p><h1>计算创新，为生成式 AI 时代构筑算力底座</h1><p></p><p></p><p>向量数据库并不是领域加速计算大展身手的唯一场景。事实上，英特尔第五代至强可扩展处理器就凭借英特尔 AVX-512 和英特尔 AMX，在诸多科学计算、AI 推理、AI 训练等场景中取得了非常优秀的表现。例如，某互联网厂商的实际测试中，第五代至强可扩展处理器的 AI 推理和训练性能相比上代就有 40% 提升，HPC 应用则有 39% 提升。如果与未引入上述加速技术的第三代至强可扩展处理器相比，第五代至强可扩展处理器的 AI 推理性能最高提升可达 14 倍之多。</p><p></p><p>腾讯云与英特尔公司长期以来一直在软硬件优化方面有着深度合作，腾讯云也在与英特尔携手将更多先进计算产品和技术应用到 AI 等领域中，在竞争激烈的市场中取得更为领先的优势。与腾讯云类似，还有更多企业都在与英特尔公司展开类似合作，不仅利用英特尔 AVX-512 和英特尔 AMX 加速技术提升 AI 场景效能，还会应用英特尔 TDX 机密计算技术来改善 AI 大模型云端部署时的企业敏感数据安全性，或者利用第五代至强可扩展处理器在内存扩展性方面的优势增强智能推荐等场景的性能，等等。英特尔公司在软件生态层面的深耕成果，以及英特尔公司与各家客户的深度合作，也帮助这些企业以更低的资源投入就能快速享受到硬件新特性的收益。</p><p></p><p>正是包括英特尔 AVX-512 和英特尔 AMX 在内的一系列计算创新技术，让 CPU 在生成式 AI 大规模普及的时代依旧能够一马当先，为企业带来显著的生产力提升和竞争优势。可以认为，引入领域加速技术等创新的 CPU 在生成式 AI 浪潮中仍将肩负基础算力底座的重任，发挥不可或缺的作用。在 CPU 的创新探索支持下，我们也将看到生成式 AI 应用更快走入社会生产生活，在各行各业中焕发新的光彩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39f050a47d8fab4d566e017cb64f0e06.png" /></p><p></p><p>第五代英特尔®至强®可扩展处理器助腾讯云向量数据库成为大模型时代数据中枢，点击下方链接，即刻查看详情！</p><p></p><p>【阅读原文】：<a href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/tencent-cloud-vector-db-data-hub-era-big-models.html?wapkw=%E5%90%91%E9%87%8F?cid=soc&amp;source=Wechat&amp;article_id=5252">https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/tencent-cloud-vector-db-data-hub-era-big-models.html?wapkw=%E5%90%91%E9%87%8F?cid=soc&amp;source=Wechat&amp;article_id=5252</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AqRLxU7726Wt5c1iJkNf</id>
            <title>“千帆杯”第一期赛题公布！龙年头彩竟藏在“游乐场”</title>
            <link>https://www.infoq.cn/article/AqRLxU7726Wt5c1iJkNf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AqRLxU7726Wt5c1iJkNf</guid>
            <pubDate></pubDate>
            <updated>Fri, 02 Feb 2024 03:18:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 春节, 主题乐园, 排队, AI应用
<br>
<br>
总结: 春节期间，主题乐园成为人们过节的热门选择，但巨大的客流量也带来了排队等待的问题。主题乐园通过一些设计来减缓游客排队焦虑，但并不能有效减少排队时间。然而，随着AI技术的发展，AI应用可以帮助解决主题乐园排队效率问题，提供个性化的游玩路线规划和优化运营策略的决策支持。百度智能云举办的千帆杯AI原生应用开发挑战赛，鼓励开发者利用AI能力打造一款实用的“游乐场排队规划助手”，帮助游客更好地了解排队情况，获得最佳体验。 </div>
                        <hr>
                    
                    <p>提到“春节”，你首先会想到什么？热气腾腾的年夜饭，大门前福气满满的春联，摩肩接踵的车站，还是一眼看不到排队尽头的游玩景点？</p><p></p><p>不知从什么时候起，各大主题乐园不再只是为孩子打造的童话世界，也俨然成为了成年人的理想国。无数的成年人换上霍格沃茨制服，挑选自己的魔杖，逃离“麻瓜世界”，前往环球影城；许多成年的迪士尼粉丝，也蜂拥至上海迪士尼乐园，在全球首个“疯狂动物城主题园区”亦或是香港迪士尼乐园的全球首个“冰雪奇缘主题园区”，寻找自己的快乐。</p><p></p><p>春节将至，热门主题乐园也会成为大家过节的经典选项，巨大的客流量，也意味着排队，将为主题乐园的游客体验和运营效率带来新一轮挑战。</p><p></p><p></p><h1>主题乐园的效率设计</h1><p></p><p></p><p>事实上，极具人气的主题乐园都有着自己独到的效率设计。在排队时，许多主题乐园都会为游客提供排队时长的提示牌，帮助游客做好预期管理；还会在惊险刺激的项目下设置巨大的电子屏直播正在游玩的游客画面，提升期待值；同时，在一些热门项目的等候区主题乐园还会设计一些精彩的故事和精美的绘画，让游客从排队的状态中脱离出来，减缓排队焦虑。</p><p></p><p>很明显，爆火的主题乐园们在解决游客排队问题时主要是通过一些设计让大多数游客失去了排队的时间概念，并不能有效减少游客排队等待时间。即使围绕排队本身所提供的辅助工具也仅仅能告诉游客园区地图上某个项目的预估排队时间。但这样还远远不够。</p><p></p><p></p><h1>来自未来的“时间魔法”</h1><p></p><p></p><p>随着大模型技术的飞速发展，把复杂功能交给&nbsp;AI，让用户更加专注于创作和创意，AI原生应用正在为产业效率带来新的可能。</p><p></p><p>为此，百度智能云以“创意无限·生成未来”为主题，发起了千帆杯·AI原生应用开发挑战赛。</p><p></p><p>第一期赛题将聚焦春节假期游乐园排队效率问题，鼓励开发者利用&nbsp;AI&nbsp;能力施展“时间魔法”，打造一款具有实用性的“游乐场排队规划助手”，帮助游客更好地了解乐园的排队情况，设计个性化的游玩路线，在有限的时间内获得最“High”的体验，同时为管理者提供优化运营策略的决策支持。</p><p></p><p>本期挑战中，官方将为开发者们提供环球影城、上海迪士尼、香港迪士尼、广州长隆&nbsp;4&nbsp;个热门游乐场地图，地图中将标注各项目的排队和游玩时间，以及不同体验维度的推荐指数。</p><p></p><p>百度智能云千帆&nbsp;AppBuilder&nbsp;将会在这轮赛题中成为开发者们重构应用的“智能助手”，为开发者提供专业、便捷的&nbsp;AI开发套件和资源环境。</p><p></p><p>AppBuilder作为目前国内唯一全面开放的具备代码规划与执行能力的平台，将框架和组件都做成了可扩展和可拼接的形式，以期给予&nbsp;AI&nbsp;应用开发者更多的选择和自由度。这就意味着每位开发者都可以利用AppBuilder来基于自然语言构建自己的“程序员”，实现单人成团，一个人就能成为一支队伍。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/eb/66/eb3739ae4504bd9a6e5868713834a866.png" /></p><p></p><p>AI原生时代，千千万万的开发者是促进AI原生应用爆发的中坚力量。本次千帆杯AI原生应用挑战赛与CSDN、51CTO、DataWhale、InfoQ、IT168、机器之心、思否等国内专业的开发者社区和人工智能媒体深度合作，将对大赛进行持续追踪，欢迎各位开发者关注~</p><p></p><p>第一期赛题已经出炉</p><p></p><p>“游乐场”的竞技舞台已经搭建完毕</p><p></p><p>期待各位在千帆杯AI原生应用开发挑战赛中</p><p></p><p>大展身手</p><p></p><p>探寻藏在“游乐场”中的10万大奖</p><p></p><p>欢迎报名参赛</p><p></p><p>阅读原文了解详细赛题说明</p><p></p><p>【阅读原文】：<a href="https://cloud.baidu.com/qianfandev/topic/268465">https://cloud.baidu.com/qianfandev/topic/268465</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt</id>
            <title>数百万小时训练，6秒音频即可完成音色复刻！效果不输 ElevenLabs 和 OpenAI 的 MiniMax 语音大模型能用来做什么？</title>
            <link>https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0zAbBRGf1euyag3FQuqt</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 09:27:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华尔街, 万圣节指标, 择时投资策略, 羊群效应
<br>
<br>
总结: 华尔街有一个著名效应，叫做「万圣节指标」。这是一种择时投资策略，即在每年万圣节前后买入股票，持有整个冬季，然后在第二年的5月1日卖出。数据显示，这种策略的收益率要高于其他时间段的持有。这种差异可能是因为羊群效应和自证预言的影响。 </div>
                        <hr>
                    
                    <p></p><h2>从 0 到 1 的 MiniMax 语音大模型</h2><p></p><p></p><p>2023 年 11 月，MiniMax 发布语音大模型 abab-speech-01。从 11 月至今，共有超过 400 家企业用户接入我们的语音大模型。</p><p></p><p>在实际应用中，来自各行各业的用户给我们反馈了很多好的建议和想法。例如，在复刻有声书场景下，市面上没有可以批量、快速生成多角色音频的解决方案；在直播电商等注重互动性的场景中，各家现有语音能力仍无法做到实时，在生成语音的过程中仍需一定的等待时间，非常影响用户体验；在教学场景中，模型碰到特殊字词或者多音字的情况，时常存在发音不准确的问题。</p><p></p><p>为了给用户带来更加高效、丰富和真实的语音定制体验，我们不断迭代 MiniMax 语音大模型，并基于用户高优需求新增语音 API 接口，并上线了多个产品功能。MiniMax 是目前第一个开放多角色配音商用接口的公司。</p><p></p><p>在模型基础能力上，我们的语音模型对长达数百万小时的高质量音频数据进行训练，基于它的训练结果，仅用 6 秒的音频就能完成音色复刻，基于文本生成语音的字错率低至万分之五，已达到全球顶尖水平。</p><p></p><p>针对用户的高优需求，我们新增了以下产品功能：</p><p></p><p>三个 API 接口：多角色音频生成 API、文本角色分类 API 和快速复刻 API，帮助用户自主批量生成、克隆多角色音频；多语种能力、字典和间隔时长控制，满足用户丰富的定制化需求，提升教学场景体验T2A Stream （流式语音输出） 实现生成与输出的同步，减少用户在直播、对话等场景的等待时间。</p><p></p><p>为了让更多用户体验、使用我们的技术，我们在价格上也做出了调整：T2A Pro、T2A、T2A Stream 等价格下调为原先的一半，由 10 元 / 万字符降至 5 元 / 万字符。</p><p></p><p>具体功能价格调整见下表：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8b578605342dddc0a7732b7e930449c.png" /></p><p></p><p></p><h2>声音小剧场</h2><p></p><p></p><p>由于语音模型没有公开的测评集，衡量一个语音模型到底怎么样主要依靠几个比较主观的评判标准，例如：自然度、相似度，可懂度和情感表现等。以下是几个基于我们语音大模型生成、复刻的一些语音效果。大家可以听听看，欢迎拍砖：）</p><p></p><p></p><h3>01 中英文夹杂读着毫无压力</h3><p></p><p></p><p>文本：</p><p>哎，你说你特别想念某个东西，可以说"I really miss it a lot" 或者"I'm missing it terribly." 这样表达出你的感情。有什么特别想念的嘛？想聊聊吗？</p><p></p><p>声音 1（明杰）：</p><p></p><p>声音 2（晨曦）：</p><p></p><p></p><p>声音 3（祁辰）：</p><p></p><p></p><p></p><h3>02 跨语种复刻，比原声更自然</h3><p></p><p></p><p>文本：</p><p>别担心，犯错是学习的一部分，下次你会做得更好的。Don't worry, making mistakes is part of learning. You'll do better next time.</p><p></p><p>原声音频（童声）：</p><p></p><p>复刻音频（中 + 英）：</p><p></p><p>只用中文原声，也可以复刻出他们讲中、英、日、韩等多种语言的声音：</p><p>韩语：</p><p></p><p>日语：</p><p></p><p></p><p></p><h3>03 AI 嬛嬛和四爷，有没有甄嬛十级学者来检验一下效果？</h3><p></p><p></p><p></p><p></p><p></p><p></p><h3>04 多音字绕口令也难不倒！</h3><p></p><p>真人都不一定能读准的多音字绕口令，我们的语音模型可以：）出现多音字的绕口令对语音模型理解上下文提出了很高要求。</p><p></p><p>“人要是行，干一行，行一行，一行行，行行行，行行行，干哪行都行”</p><p></p><p></p><p></p><p></p><h3>05 实时语音通话，跟小海螺打电话吧</h3><p></p><p></p><p>MiniMax 不仅为企业用户和开发者提供语音相关的 API，也为普通用户打磨了多款含有语音功能的产品。例如，我们在 AI 助手海螺问问上线了实时语音通话功能——无论你遇到什么问题，都可以随时打电话给小海螺，就像在和朋友聊天一样轻松、自然。小海螺的反应比 ChatGPT 的语音功能还快哦，快来体验一下吧！</p><p></p><p></p><p></p><p></p><h3>06 唱 AI 嘻哈</h3><p></p><p></p><p>节奏感强、唱腔复杂的饶舌说唱，我们的模型也能够超酷演绎。</p><p>想和 AI battle 说唱的朋友可以打开链接尝试：</p><p><a href="https://m.xingyeai.com/tag/2760001">https://m.xingyeai.com/tag/2760001</a>"</p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV</id>
            <title>性能接近GPT-4，Mistral-Medium遭泄露？CEO最新回应来了......</title>
            <link>https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ttMK49Sq76u12zj1SzIV</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 08:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源模型疑似泄露, Mistral-Medium模型, Miqu, HuggingFace
<br>
<br>
总结: 近日，关于"Mistral-Medium 模型泄露"的消息引起了关注。一位名为"Miqu Dev"的用户在开源 AI 模型和代码共享平台HuggingFace上发布了一个名为"miqu-1-70b"的新的开源大语言模型。该模型与Mistral AI公司正在研发的Mistral Medium模型的"提示格式"和用户交互方式相同。有人猜测miqu-1-70b可能是MistralAI模型的Medium版本。测试显示miqu-1-70b在德语翻译和跨语言理解能力方面表现出色。然而，与Mixtral-8x7B-Instruct-v0.1相比，miqu-1-70b的性能仍有待提升。 </div>
                        <hr>
                    
                    <p></p><h2>开源模型疑似泄露，开发者纷纷下场测试</h2><p></p><p>&nbsp;</p><p>近日，一则关于“Mistral-Medium 模型泄露”的消息引起了大家的关注，该消息在Hacker News和X（原Twitter）上持续发酵。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/89/89aab1008554b6e25fb0bd71e9c02a03.jpeg" /></p><p></p><p>&nbsp;此消息之所以受到这么多关注，与一款叫做“Miqu”的神秘模型有关。</p><p>&nbsp;</p><p>1月28日左右，一位名为“Miqu Dev”的用户在开源 AI 模型和代码共享平台<a href="https://huggingface.co/miqudev/miqu-1-70b">HuggingFace 上发布了一组文件</a>"，这些文件共同构成了一个看似新的开源大语言模型，名为“miqu-1-70b”。</p><p>&nbsp;</p><p>开源地址：<a href="https://huggingface.co/miqudev/miqu-1-70b">https://huggingface.co/miqudev/miqu-1-70b</a>"</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1f/1fb6232a05547d602936f41d64ab00aa.png" /></p><p></p><p>&nbsp;在Hugging Face平台的miqu-1-70b项目上，多条内容指出这款新的大语言模型的“提示格式”以及用户与其交互的方式与 Mistral AI公司正在研发中的Mistral Medium模型相同。同一天，4chan 上的一位匿名用户（可能是“Miqu Dev”）<a href="https://boards.4chan.org/g/thread/98696032#p98697258">在 4chan 上发布了 miqu-1-70b 文件的链接</a>"，该项目的受关注程度逐渐升高。</p><p>&nbsp;</p><p>模型放出后，有业内人士猜测，这个神秘泄露的miqu-1-70b可能就是MistralAI模型的Medium或者过往混合专家测试版本。</p><p>&nbsp;</p><p>一些人用户在X上分享了该模型的发现，以及该模型在常见 LLM 任务（通过称为基准的测试来衡量）上表现出的异常出色的性能，甚至接近了 OpenAI 的 GPT -4 在EQ 工作台上的表现。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c7/c7d5fb572e8b0626b98b59e44cc73cda.png" /></p><p></p><p>有用户测试了这款神秘模型和Medium模型后表示：“尽管可能有些晚了，但现在我100%确信miqu与Perplexity Labs上可访问的Mistral-Medium模型是同一个模型。该用户还称：“它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。”</p><p>&nbsp;</p><p>这款备受瞩目的生成式AI新星——miqu-1-70b自然要被拿来与各位前辈进行一番比较。有测试者用4项德国数据保护测试对这款神秘模型做了更深入的测试，方法如下：</p><p>&nbsp;</p><p>这里通过4项德国在线数据保护培训/考试，对这套新模型的表现加以验证。测试数据、问题及所有说明均为德语，而答题卡则为英语。这考察了模型的翻译能力和跨语言理解能力。在提供信息之前，测试者会用德语指示模型：接下来向你提供一些信息，请记住相关内容，并回答“确定”以确认已经理解其内容。这一步是为了测试模型的指令理解与遵循能力。在提供关于某个主题的全部信息之后，测试者会向模型提出测试问题。这是一套包含三个选项的多选题，但首题采用A/B/C选项，末题为X/Y/Z选项。每项考试包含4至6道题，测试流程总计18道选择题。根据模型给出的正确答案数量进行排名，先测试事先提供课程内容后的成绩，再测试没有提供信息下的盲答成绩（作为决胜局）。所有测试均单独运行，每次测试间会清除上下文，保证会话之间的记忆/状态不相互干扰。</p><p>&nbsp;</p><p>还进行了包括<a href="https://github.com/SillyTavern/SillyTavern">SillyTavern</a>"前端、<a href="https://github.com/LostRuins/koboldcpp">koboldcpp</a>"后端（对于GGUF模型）在内的其他测试，另外还预先设置确定性生成，以尽可能消除随机因素并进行有意义的模型间比较，也包括注明官方提示词格式。</p><p>&nbsp;</p><p>以下为详细注释、排名基础和其他评论与观察发现：</p><p><a href="https://huggingface.co/miqudev/miqu-1-70b">miqudev/miqu-1-70b</a>"&nbsp;GGUF Q5_K_M, 32K上下文, Mistral格式:</p><p>❌&nbsp;正确回答了4+4+4+5=17/18道选择题，而在盲答阶段，正确答案题为:&nbsp;4+3+1+5=13/18。</p><p>❌&nbsp;未能按照要求用“确定”来回应数据输入。</p><p>&nbsp;</p><p>经过了多项测试后，结果显示miqu-1-70b的效果着实不错，测试者出于个人猜测，miqu-1-70b可能是一套外泄的MistralAI概念验证旧模型，从开发次序来讲应该不会比Mixtral更晚。此外，测试者也表示，在测试过程中注意到了几个有趣的点，从这几个方面来看，miqu-1-70b跟Mixtral存在诸多相似：</p><p>&nbsp;</p><p>优秀的德语拼写与语法能力。支持双语，可在回复中添加翻译。能够为回复添加注释和评论。</p><p>&nbsp;</p><p>但测试者也表示，在测试中，miqu-1-70b仍无法与Mixtral-8x7B-Instruct-v0.1（4-bit）相媲美，不过性能仍比Mistral Small和Medium更好（亲自测试Medium时其表现相当糟糕，可能是API的问题）。但与测试者每天都在使用的Mixtral 8x7B Instruct相比，miqu也没有好太多。</p><p>&nbsp;</p><p>在这场miqu和Mistral Medium模型对比测试中，前阵子号称要干掉谷歌搜索的 Perplexity印度创始人Aravind Srinivas也在X上发表了自己的观点：</p><p>&nbsp;</p><p></p><blockquote>很多人问我Mistral的所有模型是否都基于Meta的Llama。特别是因为Mistral Medium在Perplexity Labs上的输出与miqu非常相似，而这种相似性是通过测试发现的。Mistral的CEO Arthur已经提供了一个清晰的解释，并确认这是一个来自早期访问客户的泄露。&nbsp;此外，Perplexity从未获得过Mistral Medium的权重访问权限。所以，当你在Labs上使用Mistral Medium时，我们只是将你的请求路由到Mistral支持的有效端点，而没有访问权重。泄露的权重实际上是量化版本，与NVIDIA TensorRT不兼容。&nbsp;此外，很多人在看到这个消息是本能地反应会认为Mistral不知道如何进行预训练，只是在LLama 2上构建。这是明显不真实的。Mistral 7b是一个由Mistral团队从头开始训练的模型，而Mistral 8x7b MoE也是通过使用他们自己的7b作为每个专家的初始化来训练的。所以很明显，这个团队知道如何从零开始训练自己的模型。Mistral Medium是从LLama后期训练的，可能是因为迫切需要一个接近GPT-4质量的API，以便早期客户使用。但是一个能够在计算和时间投入远少于Gemini Pro的情况下取得胜利的团队，现在他们有了更多的资金和计算资源，显然能够做到GPT-4级别的质量。&nbsp;当然，泄露是不好的。Mistral的胜利对社区来说是一件好事：无论是对学术界还是对初创公司。支持他们！</blockquote><p></p><p></p><h2>Mistral AI高层发声：是泄露了，但只是个旧版本</h2><p></p><p>在Mistral AI的新模型遭泄漏这一话题热度不断上涨之时，据外媒最新消息，Mistral AI联合创始人兼CEO&nbsp;Arthur Mensch 在 X 上澄清：</p><p>&nbsp;</p><p></p><blockquote>“一个我们早期客户的热情员工泄露了一个我们公开训练和发布的老模型的量化（带水印）版本。为了尽快与一些特定的客户开始合作，我们在获得整个集群访问权限后立即从Llama 2重新训练了这个模型——预训练在Mistral 7B发布的那一天完成。自那时以来，我们取得了很好的进展——敬请期待！”</blockquote><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ed/edbdac6347f53ab3b41ca879cd4bf3b7.jpeg" /></p><p></p><p>有趣的是，Mensch 并没有要求删除 HuggingFace 上的帖子，而是留下那些评论说发帖者“可能会遭到模型所属公司追责”的评论。</p><p></p><h2>Mistral AI创始团队成员均来自谷歌和Meta</h2><p></p><p>&nbsp;</p><p>Mistral AI是一家总部位于巴黎的欧洲公司，由 Arthur Mensch 和 Guillaume Lample以及 Timothée Lacroix于 2023 年 2 月联合创立，并于去年 12 月 10 日宣布筹集了 3.85 亿美元，仅半年多的时间，该公司估值近 20 亿美元。Mistral AI 在刚成立且没有任何产品时就已筹集了 1.05 亿美元。</p><p>&nbsp;</p><p>因此，它也成为继德国 Aleph Alpha 在去年11月筹集了 5 亿欧元之后，第二家筹集到如此多资金的欧洲人工智能初创公司。</p><p>&nbsp;</p><p>Mistral AI一直在研究如何提高模型性能，同时减少为实际用例部署llm所需的计算资源。Mistral 7B是他们创建的最小LLM，它为传统的Transformer架构带来了两个新概念，Group-Query Attention(GQA)和Sliding Window Attention(SWA)。这些组件加快了推理速度，减少了解码过程中的内存需求，从而实现了更高的吞吐量和处理更长的令牌序列的能力。</p><p>&nbsp;</p><p>Mistral AI 首席执行官 Arthur Mensch，31 岁，在 Google 人工智能实验室 DeepMind 工作了近三年。Mistral 的科学总监 Guillaume Lample 是 Facebook 母公司 Meta 在 2 月份推出的 LLaMA 语言模型的创建者之一。Timothée Lacroix 是 Mistral AI 的技术总监，也是 Meta 的研究员。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/Yampeleg/status/1751837962738827378">https://twitter.com/Yampeleg/status/1751837962738827378</a>"</p><p><a href="https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op">https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op</a>"</p><p><a href="https://analyticsindiamag.com/mistral-ai-challenges-dominance-of-openai-google-meta/">https://analyticsindiamag.com/mistral-ai-challenges-dominance-of-openai-google-meta/</a>"</p><p><a href="https://news.ycombinator.com/item?id=39175611">https://news.ycombinator.com/item?id=39175611</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty</id>
            <title>金山云升级全栈云计算体系，全方位承接大模型应用</title>
            <link>https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Yl1jSOMx2TAzkqN0Phty</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 08:09:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云原生, AIGC, 大模型, 云计算
<br>
<br>
总结: 随着云原生、AIGC、大模型等新兴技术的迅速发展，智能化时代开启。云计算也正全面步入3.0时代，即云计算和人工智能深度融合的阶段。在这个阶段，人工智能技术成为云计算进一步释放潜力的核心推动力。金山云通过技术自研和升级，已初步建成人工智能时代民用领域全栈的云计算体系。 </div>
                        <hr>
                    
                    <p>随着云原生、AIGC、大模型等新兴技术的迅速发展，智能化时代开启。云计算也正全面步入3.0时代，即云计算和人工智能深度融合的阶段。在这个阶段，人工智能技术成为云计算进一步释放潜力的核心推动力。</p><p>&nbsp;</p><p>近日，金山云举办了「云+人工智能·时代新机遇」媒体沟通会，金山云副总裁钱一峰、金山云公有云产品中心负责人孙晓、金山云人工智能与大数据产品中心负责人徐寅斐在会上分享了金山云在基础能力、平台能力和模型服务等方面的技术历程和落地实践。通过技术自研和升级，金山云已初步建成人工智能时代民用领域全栈的云计算体系。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c7/c7a4ac5ef7a3b8f9f69f7f66f82246ec.png" /></p><p></p><h2>夯实基础，全面升级人工智能服务能力</h2><p></p><p>&nbsp;</p><p>全球数字化浪潮风起云涌，云计算作为数字经济重要的支撑基础，正发挥越来越重要的作用。其中，智能化作为云计算3.0时代的重要理念，已成为数字基础设施升级的重要驱动力。知名调研机构IDC于2023年发布的报告显示，未来5年，以大模型、生成式人工智能驱动的下一代人工智能有望带动整体云产业穿过下行阶段，重回高增长时代。</p><p>&nbsp;</p><p>顺应趋势，金山云深入探索前沿技术，围绕AIGC升级核心计算、存储、网络等产品，从多个维度全面拥抱人工智能。</p><p>&nbsp;</p><p>金山云副总裁钱一峰强调，在此次技术升级中，在网络上采用了目前业界流行的高性能RoCE网络。在成本方面，同样的集群如果采用IB网络，网络本身的成本几乎占整个算力群的一半，如果用RoCE网络，只占5%到10%，所以国内很多大厂都在往RoCE网络转，金山也是如此。</p><p>&nbsp;</p><p>在计算方面，金山云第七代云服务器X7搭载第四代英特尔®至强®可扩展处理器、支持英特尔® AMX原生加速能力，CPU性能较上代最高提升60%，内存升级至DDR5，频率较上一代性能提升50%。融合金山云自主创新的加速技术，云服务器X7可有效提升模型推理性能。</p><p>&nbsp;</p><p>根据数据从极热到极冷的不同热度，金山云对象存储已覆盖标准存储、低频存储、深度低频存储、归档存储、深度冷归档存储和全闪存储等不同访问热度的存储产品。其中，金山云对象存储KS3极速型最高可提供1Tbps/PB的兑付带宽，相较基于机械硬盘（HDD）的对象存储性能提升了上百倍，能为AIGC、存算分离和高性能计算等场景提供强有力的存储解决方案。</p><p>&nbsp;</p><p>在底层技术基础上，金山云此次升级还新增了金山轻舟智问以及一些合作的商业模型。</p><p>&nbsp;</p><p>一直以来，业内有一种观点认为一个大模型就解决所有问题，因为所有人都可以去调用大模型的API，但根据用户的反馈和场景需求来看，把所有问题都扔给一个大模型去解决是非常昂贵的，更经济的做法是将不同的场景用不同规模的模型去解决。将这些平台打包在一起，可以为客户提供一个综合性价比最高的解决方案。</p><p>&nbsp;</p><p>过去这一年，金山云一直在做两件事——夯实基础和做长长板。在云计算方上重点打磨四大基础能力：速度快、性能好、成本低、稳定易用；而做长长板找增量则体现在金山云在混合云、分布式云上的很多创新。</p><p>&nbsp;</p><p>具体而言，在在公有云的核心产品上第一要做到一切皆标准，提升用云效率。第二，要做到软硬结合，最大限度提升客户用云性价比。</p><p>&nbsp;</p><p>在混合云方面，客户拥有专属区或者专属集群，这既让客户拥有了云下的独立，又能让其享受云上的规模红利和弹性。</p><p>&nbsp;</p><p>此外，在Serverless化上也有了新进展。Serverless化过去是局限在算力层面，但随着存算分离场景的流行已经渗透到PaaS层面，如今MySQL也已经做到了Serverless化，另外也引入了开源向量数据库。</p><p>&nbsp;</p><p>面向大模型应用场景，金山云推出互信虚拟私有网络（简称“互信VPC”），解决模型厂商和应用厂商互信的问题。相对于标准VPC，互信VPC对进出VPC的通信行为有着更为严格的控制，帮助客户解决合规和互信问题。针对HTTP、HTTPS等应用层负载场景，公司推出应用型负载均衡ALB(Application Load Balancer)，单实例最大支持100万QPS。与云原生场景融合，客户在使用金山云容器服务KCE产品或自建K8S集群时，ALB都可作为Ingress部署，为业务提供网络流量调度服务。</p><p></p><h2>勇立潮头，做大模型助力者</h2><p></p><p>2023年被称为大模型“元年”。公开数据显示，从年初到年末，国内大模型数量超过200个。与“大模型”数量呈倍数级增长相反的是，真正被调用的大模型却相对较少。在本次大模型浪潮中，金山云坚持中立定位，充分发挥自身的底座和平台能力，做大模型的助力者。</p><p>&nbsp;</p><p>在洞察到模型供需方的痛点后，金山云于去年6月率先推出MaaS互信推理专区方案（以下简称“MaaS 1.0”），在大模型厂商、用户和金山云之间建立互信，以解决模型及数据的互信问题。本次沟通会上，金山云发布MaaS互信推理专区方案2.0（以下简称“MaaS 2.0”）。</p><p>&nbsp;</p><p>在MaaS 1.0基础上，MaaS互信推理专区方案2.0以金山云IaaS和PaaS为底座，可实现云上LangChain的一键部署，默认对接多个生态合作商业大模型和开源大模型。同时支持包括BGE、Bert等在内的Embedding模型，能无缝对接金山云全托管向量数据库Milvus，提供面向企业开发者的简单易用、安全可信的一站式推理应用部署平台。此外，MaaS 2.0支持通过标准化的API接口和Web前端界面，实现包括模型推理和知识库搭建的RAG大模型场景应用。为进一步增强云上运行的安全性，MaaS 2.0还提供容器服务加密镜像解决方案，依托金山云裸金属服务，实现在金山云容器服务中从镜像加密、加密镜像上传、解密镜像运行的全流程模型安全运行。</p><p>&nbsp;</p><p>同时，为满足行业客户的需求，金山云探索大模型时代企业赋能新机遇，围绕企业级知识助手场景制定了“一三一四”产品全景规划，即一套能力（金山云轻舟智问）、三个模型（行业语言模型、文本分片和Embedding模型）、一个平台（金山云瀚海平台）及四大功能（微调推理、数据加速、智能检索和文档智能），围绕生成式人工智能构建应用落地、模型训练微调、平台支撑的全栈能力。基于“一三一四”规划，金山云将分别针对应用型客户和平台型客户输出多项原子能力。目前，金山云轻舟智问知识助手产品已完成应用以及Embedding模型、多路召回算法和智能数据切片模型等核心技术的建设，计划优先在公共服务和法律场景落地。</p><p>&nbsp;</p><p>与人工智能的结合，给了云更多的想象空间，也给各行业都带来了新的生产力。随着技术的进一步完善和落地，云计算将迎来更强更久的生命力。金山云将持续围绕客户需求“练内功”，携手生态伙伴以差异化打法布局未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij</id>
            <title>刚上线就崩了？字节版GPTs征战国内市场：无需编码，快速创建AI聊天机器人</title>
            <link>https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/t8F5gt0MRXcxr0ZWe9Ij</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 06:46:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, AI聊天机器人, Coze, 构建平台
<br>
<br>
总结: 字节跳动旗下AI聊天机器人构建平台Coze正式上线。Coze是一款应用程序编辑平台，用于开发下一代AI聊天机器人。用户可以在该平台上快速创建各类聊天机器人，并将其部署在不同社交媒体与消息应用中。 </div>
                        <hr>
                    
                    <p>2月1日，字节跳动旗下AI聊天机器人构建平台Coze国内版（中文名：扣子）正式上线。Coze是一款应用程序编辑平台，用于开发下一代AI聊天机器人。无论用户是否拥有编程经验，都可在该平台上快速创建各类聊天机器人，并将成果部署在不同社交媒体与消息应用当中。</p><p>&nbsp;</p><p>据悉，Coze 由字节跳动新成立的AI部门 Flow 开发，去年年末在海外先行上线。与海外版相比，国内版在功能上并无差异，只是推送渠道略有不同。</p><p>&nbsp;</p><p>有网友反馈，Coze 刚上线就崩了，InfoQ实测发现，Coze 创建界面长时间显示确认中，或许与刚上线流量过大有关。不过截至发稿前，Coze响应时间恢复了正常。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/19/19c3274341257b4c62176b23bce3f3ee.png" /></p><p></p><p>体验地址：</p><p><a href="https://www.coze.cn/home">https://www.coze.cn/home</a>"</p><p></p><h2>Coze背后神秘的AI部门Flow：由字节大模型领队牵头，聚焦AI应用层</h2><p></p><p>&nbsp;</p><p>据36kr 2023年11月末报道，字节跳动近期成立了一个新AI部门Flow，技术负责人为字节跳动技术副总裁洪定坤。有知情人士表示，Flow部门的业务带头人，为字节大模型团队的负责人朱文佳。</p><p>&nbsp;</p><p>公开信息显示，朱文佳是业界知名的架构师，曾担任百度搜索部主任架构师，是百度网页搜索部技术副总监杨震原手下的得力干将。2014年，杨震原受张一鸣邀约，离开百度入职字节跳动，现为字节跳动公司副总裁、火山引擎业务负责人。2015年，朱文佳加入字节跳动，被称为“今日头条里算法技术的Top3人选”，并在入职4年后担任今日头条CEO。2021年2月，朱文佳调任成为为Tik Tok产品技术负责人。</p><p>&nbsp;</p><p>据悉，朱文佳是字节跳动大模型业务中的“隐形领队”——是字节跳动语言大模型负责人与图像大模型团队负责人的间接和直接汇报对象。在内部人士眼中，朱文佳拥有“综合的工程和技术管理经验”。</p><p>&nbsp;</p><p>与朱文佳背景类似，洪定坤也曾在百度任职过，曾担任百度贴吧的技术经理。2014年，洪定坤加入字节跳动，现为字节跳动技术副总裁。</p><p>&nbsp;</p><p>据了解，Flow部门主要聚焦在AI应用层。在 Flow 此前发布的活水招聘帖中，Flow 称其是字节跳动旗下 AI 创新业务团队，已经在国内和海外分别上线豆包和 Cici 两款产品，还有多个 AI 相关创新产品在孵化中。</p><p>&nbsp;</p><p>其中，豆包属于AI 对话产品，Cici 与Coze都属于AI聊天机器人创建平台，可供用户创建和共享自己的聊天机器人。此外，字节跳动旗下的AI产品还包括AI聊天机器人创建平台ChitChop和AI角色创建与互动平台BagelBell，这两款产品分别由 POLIGON 和SPRING(SG)PTE.LTD.开发运营。Cici和CHitChop主要用于娱乐场景，提供基于虚构性格的浪漫伴侣型机器人，而Coze则提供可简化办公流程的工作机器人。</p><p>&nbsp;</p><p>Cici、Coze、ChitChopt和BagelBell这四款产品均在过去三个月内登陆海外，目前已经拥有数百万下载量。其中，只有Coze目前登陆了美国市场，其他三款均未向美国和欧洲市场开放。科技大厂通常会先在监管审查较弱的小型市场上测试产品，之后再逐步推向美国和欧盟。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72e14a648302c9e10fbb2400dcab16e0.png" /></p><p></p><p>有网友表示自己非常看好Coze，一是2B产品对质量要求高，2C产品市场相对有容忍度，二是核心不在产品在流量，字节有地表最强流量能力，比如一键上架豆包。“字节聚焦AI应用非常聪明，基础模型差距很大，借‘基础能力’用流量起势，积累数据再反哺‘能力’，策略清晰可执行”，该用户在X上评论道。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55c07e8586cc210fcd9921dd0bbcf758.png" /></p><p></p><p>根据Google Play商店的数据，Cici是四款中最受欢迎的应用产品，目前下载量已经超过1000万次。字节跳动并未公开用于支持这些产品的底层大语言模型。字节跳动发言人Jodi Seth表示，这些应用依赖于OpenAI的GPT技术，可通过微软Azure许可证进行访问。</p><p>&nbsp;</p><p>据Forbes报道，这四款字节跳动新应用均在隐私政策中包含警告（与字节跳动的其他应用保持一致），称这些应用可能与其他“我公司内部实体”共享用户信息。字节跳动发言人Seth也证实，字节跳动的中方员工可能会访问到应用内的用户数据，但将严格遵守公司的访问控制与审批流程。</p><p>&nbsp;</p><p>在测试对话中，Coze和Chitchop均就某些敏感话题给出比较完整的描述，但仍存在幻觉问题。但在注册账户测试Cici（与豆包App几乎一模一样）时，该应用屡屡报错，最终导致测试无法进行。</p><p>&nbsp;</p><p>有网友分析，字节跳动的这四款AI产品应该是不同团队间产品的 PK。对于素有“APP工厂”之称的字节跳动来说，四大产品“赛马”可以提升团队创新性，同时也可看出字节跳动对AI领域的重视程度。</p><p></p><h2>如何用扣子快速创建AI聊天机器人？</h2><p></p><p>&nbsp;</p><p>自去年第四季度在海外取得很好的成绩之后，字节跳动今天正式推出“Coze 扣子”AI Bot 开发平台。</p><p></p><h4>Coze具备哪些功能？</h4><p></p><p>&nbsp;</p><p>无限拓展的能力集</p><p>&nbsp;</p><p>扣子集成了丰富的插件工具，可以极大地拓展 Bot 的能力边界。</p><p>&nbsp;</p><p>内置插件：目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。 你可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。例如使用新闻插件，打造一个可以播报最新时事新闻的 AI 新闻播音员。自定义插件：扣子平台也支持创建自定义插件。 你可以将已有的 API 能力通过参数配置的方式快速创建一个插件让 Bot 调用。</p><p>&nbsp;</p><p>丰富的数据源</p><p>&nbsp;</p><p>扣子提供了简单易用的知识库功能来管理和存储数据，支持 Bot 与你自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到知识库中。这样，Bot 就可以使用知识库中的内容回答问题了。</p><p>&nbsp;</p><p>内容格式：知识库支持添加文本格式、表格格式的数据。内容上传： 你可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至知识库，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在知识库内添加自定义数据。</p><p>&nbsp;</p><p>持久化的记忆能力</p><p>&nbsp;</p><p>扣子提供了方便 AI 交互的数据库记忆能力，可持久记住用户对话的重要参数或内容。</p><p>&nbsp;</p><p>例如，创建一个数据库来记录阅读笔记，包括书名、阅读进度和个人注释。有了数据库，Bot 就可以通过查询数据库中的数据来提供更准确的答案。</p><p>&nbsp;</p><p>灵活的工作流设计</p><p>&nbsp;</p><p>扣子的工作流功能可以用来处理逻辑复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大语言模型 LLM、自定义代码、判断逻辑等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如：</p><p>&nbsp;</p><p>创建一个搜集电影评论的工作流，快速查看一部最新电影的评论与评分。创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。</p><p></p><h4>Coze快速上手教程</h4><p></p><p>&nbsp;</p><p>Coze的主页面非常简洁，点击“创建Bot”即可创建属于自己的Coze机器人。可以自行设置机器人的名称以及功能介绍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f843506a8e644ea731a46e0f30b2754c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe0cf437234b93b5d5a98cef53dd7002.png" /></p><p></p><p>创建好后，可以编辑机器人提示词，比如可以描述机器人的角色、技能、约束条件等内容以定义机器人的预期行为。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c8/c8db6d4356cad6f7ee2509a5d5284e47.png" /></p><p></p><p>由于AI机器人本身无法直接访问互联网，因此需要配合某些工具来获取数据或执行网上操作。</p><p>可以将各类插件工具添加到机器人内以扩展其功能，具体包括必应搜索、ByteArtist、图片理解、头条搜索等多个插件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95ba71088036d5e85cf0e19d0511d386.png" /></p><p></p><p>创建好机器人后，可以选择发布平台，国内版Coze发布平台包括飞书、微信。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/060c86c5e3969b6083fed240956a9871.png" /></p><p></p><p>Coze还提供了Bot 商店，包含工具、娱乐、生活方式等多个类目，可以看到其他开发者是如何创建机器人的，并从中找寻灵感，甚至可以基于该机器人创建一个副本，再进行个性化调整。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d9cbfe0f35d63a957599be08e7776f3f.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c780d27d43c755e4e1a2af5a77427ea1.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e3572701727592d19b2d1e3071b24e3.png" /></p><p></p><p>参考链接：</p><p><a href="https://www.coze.cn/docs/guides/welcome">https://www.coze.c</a>"<a href="https://www.coze.cn/docs/guides/welcome">n</a>"<a href="https://www.coze.cn/docs/guides/welcome">/docs/</a>"<a href="https://www.coze.cn/docs/guides/welcome">guides/</a>"<a href="https://www.coze.cn/docs/guides/welcome">welcome</a>"</p><p><a href="https://www.forbes.com/sites/emilybaker-white/2024/01/16/tiktok-bytedance-ai-chatbots-openai/?sh=4bb1b0fba240">https://www.forbes.com/sites/emilybaker-white/2024/01/16/tiktok-bytedance-ai-chatbots-openai/?sh=4bb1b0fba240</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT</id>
            <title>ALL IN AIGC 新时代，共探行业变革之路｜InfoQ 合作伙伴年度盛典</title>
            <link>https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6rOYTrA6LToGe35zBtUT</guid>
            <pubDate></pubDate>
            <updated>Thu, 01 Feb 2024 04:03:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ, 合作伙伴年会, 开发者生态, AI创新
<br>
<br>
总结: InfoQ极客传媒与合作伙伴们共同探索和实践技术和新领域带来的无限可能，共建优质内容、扶持创新企业/业务、优化客户体验是他们的承诺。他们致力于打造一个强大的开发者生态，推动科技创新和产业发展。此外，他们与开放原子开源基金会达成战略合作，共同推动中国开源事业的发展。 </div>
                        <hr>
                    
                    <p>2024年 1 月 31 日，InfoQ 极客传媒合作伙伴年度盛典在前门 blue note 圆满落幕。时隔三年，InfoQ合作伙伴年会回归线下，本届合作伙伴年会围绕“有被 Q 到”这个主题精彩展开，Q 代表着 Quality（坚守品质）、Quick（敏锐、速度和专业能力）、Question（不断求知、探索、创新），InfoQ 全体同学“Cue ”各位合作伙伴相聚一堂，分享这一年的感悟与收获，共话技术前沿与商业创新。</p><p></p><h2>共筑开发者生态 引领AI创新</h2><p></p><p>回首2023年，AI技术的璀璨光芒照亮了各行各业，带来了翻天覆地的变革，也催生了无数的创新机遇。InfoQ也在这股变革的潮流中与时俱进，与合作伙伴们紧密携手，共同探索和实践着技术和新领域带来的无限可能。</p><p>&nbsp;</p><p>极客邦科技创始人 &amp;CEO 霍太稳（Kevin）在新年致辞中表示，在生成式AI时代，极客邦科技在积极探索和改变，未来将从共建优质内容、扶持创新企业/业务、保证客户体验三方面进行创新，也是极客邦科技对朋友们做出的承诺。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2ddc620a200d2301dcf3630ad9e27e49.jpeg" /></p><p>极客邦科技创始人 &amp;CEO 霍太稳（Kevin）</p><p></p><p>共建优质内容：从 Demo&nbsp;到产品化再到产业化，帮助企业培育开发者生态</p><p>过去一年，InfoQ见证并报道了基础大模型、行业大模型在金融、制造、泛互等领域的探索和实践。未来一年，我们的内容、会议将继续瞄准“AI+场景化”的这个方向，只要是这个主题下面对开发者、对社区有价值的内容，我们坚决欢迎朋友们与我们共建。</p><p></p><p>扶持创新企业/业务：从流量到内容到出海，借助AI全方位帮助早期业务/企业成长</p><p>过去几年，InfoQ极客传媒在推动数字化转型和创新创业方面取得了显著成果。与100+生态伙伴共同推出【数字化转型专区】，展示了中国数字化转型的创新技术和数字化人才培养成果。发布了创新创业“成长计划”，为创新企业提供媒体传播、展示区和CXO群体链接等支持。</p><p>&nbsp;</p><p>此外，InfoQ极客传媒还与NVIDIA初创加速计划合作，为TGO会员企业提供创业指导。去年，InfoQ成功助力云器科技的产品技术发布，并受到广泛关注。同时，TGO鲲鹏会也在全球多地举办活动，为会员提供出海业务指导和合规建议。</p><p>&nbsp;</p><p>新的一年，InfoQ极客传媒将结合AI技术，为创新企业/业务提供更加强大的支持。推动技术创新和业务拓展，为合作伙伴和会员提供更优质的服务和支持。</p><p></p><p>优化客户体验：服务到客户满意为止，合同期止不满意，持续服务</p><p>极客邦科技始终坚守一个原则：客户体验坚决放在首位。霍太稳也向现场的合作伙伴郑重承诺，在合同期限内，如对服务有任何不满或建议，极客邦科技将全力以赴，持续优化服务内容和质量，直至达到客户满意为止。</p><p>&nbsp;</p><p>霍太稳表示，极客邦科技第一季度的主题是“一具体，就深刻”。2024年，形势可能依然严峻，希望能够与合作伙伴携手共同实现“全面进化”，最终共同实现增长！</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c5/c54bc64eb49209e7758382a16ccd1a60.png" /></p><p></p><h2>汇聚市场力量 驱动技术升级</h2><p></p><p>在年会现场，极客邦科技CGO汪丹进行 2024 年 InfoQ 极客传媒战略发布重点强调了开发者生态的重要性以及从科技创新到产业赋能的链接力量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a14d93608007e43c72dc42a2ea1fc3e2.jpeg" /></p><p>极客邦科技CGO 汪丹</p><p>&nbsp;</p><p>汪丹表示，InfoQ极客传媒一直致力于打造一个强大的开发者生态，通过科技创新推动产业发展。InfoQ极客传媒始终坚持以客户需求为导向，通过不断创新和演进，为企业提供更加贴合实际需求的解决方案和产品。</p><p>&nbsp;</p><p>汪丹介绍了InfoQ极客传媒在内容、会议、社交、企业服务等方面的全新布局。InfoQ极客传媒将通过打造大模型领航者、实践案例采访等内容产品，为开发者提供更加丰富、实用的学习资源。</p><p>&nbsp;</p><p>通过举办智能软件开发生态展、通用人工智能开发与应用生态展等会议活动，为开发者提供更多的交流和展示平台。通过打造opentalk区、社交区等社交场景，实现企业和开发者的双向奔赴。同时，InfoQ极客传媒还将深入千行百业，将优秀实践带到传统企业，推动产业的数字化转型和升级。</p><p>&nbsp;</p><p>面向2024年，InfoQ极客传媒将继续加大对国内大模型领域发展的跟踪和研究力度，为业内提供更多的选型依据和决策支持。并将积极拓展游学业务，坚守“不卷、不焦虑”的原则，帮助中国企业走向海外，拓展国际市场。为开发者和企业提供更加优质、高效的服务和支持，与更多的合作伙伴携手共进，共同推动行业的繁荣发展。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c9/c9fa3ac5e7c53ba599c072deeaba9f0c.jpeg" /></p><p>InfoQ极客传媒 2024 战略发布</p><p></p><h2>达成战略合作 谱写开源新章</h2><p></p><p>回顾InfoQ的整个发展过程，始终与许多开源社区紧密联系，在开放原子开源基金会成立后，InfoQ便成为银牌赞助商，以实际行动表达对开源社区的支持与感恩。为了更深层次地孕育与发展繁荣的开源生态体系，在本次年会盛典中，开放原子开源基金会与极客邦科技在 InfoQ 合作伙伴年会上举行战略合作发布仪式。</p><p>&nbsp;</p><p>开放原子开源基金会秘书长冯冠霖与霍太稳共同按下战略启动按钮，宣布正式达成战略合作，共同推动中国开源事业的发展。双方将共绘战略合作宏图，坚定致力于开源技术的普及推广和开源知识的深度传播，协同提升双方品牌在业界的声望和影响力，全力构建全面创新的开发者生态系统以及一套卓越的人才培养机制，重点推动AtomGit平台在全球范围内的广泛应用与深化拓展，促进开源技术生态可持续发展，为全球数字经济的发展注入新活力。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a6/a6633b2470e49027d96ca2e7b0902949.jpeg" /></p><p>开放原子开源基金会与极客邦科技达成战略合作</p><p>开放原子开源基金会秘书长冯冠霖（图左）与极客邦科技创始人 &amp;CEO 霍太稳（图右）</p><p>&nbsp;</p><p>此外，在启动仪式后，开放原子开源基金会开源大赛组委会办公室高级运营官曹海清对“开放原子开源大赛”进行了宣讲。该大赛旨在激发全球开发者的创新潜力，挖掘并提炼开源领域的宝贵精华。双方的战略合作，将有力驱动全球数字经济领域的深化拓展与升级转型，并在开源人才的孕育、选拔与输送方面发挥关键作用，有效促进全球范围内技术交流与协作的国际化进程。对此，InfoQ侧表示，“将继续发挥其全球性科技媒体品牌的影响力，为合作伙伴提供更加优质、高效的服务，推动全球科技创新与发展。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/25/253f6861b080ed0ffce0e2357db7700d.jpeg" /></p><p>开放原子开源基金会开源大赛组委会办公室高级运营官 曹海清</p><p></p><h2>致敬荣耀时刻 见证科技星熠</h2><p></p><p>在本次盛典上，InfoQ 极客传媒面向合作伙伴颁布了“2023年度新锐技术品牌奖”、“2023年度技术影响力引领品牌”、“2023年度技术传播创新案例”、“2023年度技术生态构建奖”、“2023年度合作伙伴奖”、“2023年度全球科技领导力推动者”奖项。</p><p>&nbsp;</p><p>以下是具体获奖企业及个人名单（排名不分先后）：</p><p><img src="https://static001.geekbang.org/infoq/76/76690345bf1ee08c4cb324a440138586.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b97c094e33e1660cbfb823f5aeafc4ca.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec5f86db84bb6e8ed515e587a234b569.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/10/10c55373c4baa97492a1f14eb7accd00.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec4c6b0114b4b7bfadad332fb170cc32.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d12732a2644d0f6e2d4629206f51e09.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/70/7095485c60758114021788988f3d4e15.jpeg" /></p><p></p><h2>行业变革时刻 趋势发展策略</h2><p></p><p>2023 年AI 技术的突破直接拉动了自动驾驶、机器人和生成式 AI 的融资增长，数据和算力作为训练大模型的底座也受到了越来越多的关注和讨论。随着技术的不断进步，企业需要紧跟时代步伐，积极拥抱 AI 与数字化，以实现更高效、更智能的业务运营。</p><p>&nbsp;</p><p>InfoQ研究中心研究总监兼首席分析师姜昕蔚带来了主题为《2023年中国软件技术洞察及2024年趋势预测》的主旨演讲，为现场观众深入解析了中国软件技术的发展现状和未来趋势。姜昕蔚表示，在AI技术的推动下，数据库和算力技术将继续升级加强，以应对日益复杂的数据处理需求和计算挑战。随着数据量的爆炸式增长和计算任务的多样化，传统的数据库和算力架构已难以满足当下的需求，行业亟需更加高效、灵活和智能的解决方案；数据库和算力的升级将成为行业发展的必然趋势。与此同时，云原生轻量、灵活和高效的特性，与边缘计算的低延迟、高带宽和数据处理能力也将为AI技术的发展提供更加广阔的空间和更加丰富的应用场景。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/6e/6eff7ee39d9e330d78f9062931aae867.jpeg" /></p><p>InfoQ研究中心研究总监兼首席分析师 姜昕蔚</p><p>&nbsp;</p><p>AIGC时代不仅对技术提出了更高要求，品牌营销与市场运营更是出现了新变革。技术品牌市场运营必须刷新认知、深挖用户需求，并融合前沿科技，打造差异化的技术品牌形象。同时，市场运营需依托数据分析，洞察市场动态与用户偏好，预测未来趋势。</p><p>&nbsp;</p><p>在本次年会的开放麦环节，华为云中国区行业营销负责人刘丽丽分享了她的经验和洞悉。她表示，2024年在稳定性成为前提的当下市场环境中，品牌工作的专业性和创新性显得尤为重要。对于不同发展阶段的企业来说，定义一个清晰可落地的品牌价值主张非常重要，因为作为连接品牌和产品的桥梁，价值主张对于牵引产品进步和行业市场拓展具有关键作用。2023年在拓展零售市场时，华为云通过深入客户市场，洞悉客户需求，提出了“共筑新时代伟大品牌”的价值主张，得到了众多客户的认同和认可，从而All in 华为云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/6841db98e9b5499baa08193b101bda91.jpeg" /></p><p>&nbsp;</p><p>无独有偶，大模型农场LLMFarm创始人宜博同样认为创新力和创造力在AI时代尤为重要。他认为，随着大模型的不断发展，未来的AI系统将具备更强的创新能力和自主决策能力，这将为企业品牌营销带来更多的机遇和挑战。“随着大模型的普及和应用，非AI项目将逐渐失去投资吸引力。”其这一观点为市场人敲响了警钟，在未来的工作中大家需要更加关注“如何利用大模型提升效果和效率，以适应行业变革的趋势。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4fd468ca9f6bec20b262bce9c5ec32e.jpeg" /></p><p></p><p>AIGC时代迎来变革的不仅是营销层面，运营领域也迎来了新的可能。零一万物开源负责人、开源社联合创始人林旅强表示，随着AIGC时代的到来，以大模型优化运营流程、加速社区拓展的过程中，更要不忘开源初心，坚持“Community Over Code”路线，深刻理解“开源项目在自身的商业模式中的价值”，显得尤为重要。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/28/28d0fb879a9b7ad4920fab5e52ca9d3b.jpeg" /></p><p></p><p>这种紧密的联系其实不仅限于技术和商业模式之间，更体现在行业之间的生态构建。企业必须持续地技术创新和产品迭代，以满足用户不断变化的需求。只有从企业之间的竞争逐渐演变为生态间的共建，与合作伙伴共同打造具有市场竞争力的产品和服务，才能在AIGC时代的浪潮中立于不败之地，实现可持续的发展和增长。</p><p>&nbsp;</p><p>总而言之，AI在2023年掀起的这波巨大波澜，需要合作伙伴能够紧跟时代步伐，抓住AI带来的机遇，共同迈向更加美好的明天。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uKgQIZksvpT1rkLPi4Hq</id>
            <title>2023年，GenAI工具成为赌注</title>
            <link>https://www.infoq.cn/article/uKgQIZksvpT1rkLPi4Hq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uKgQIZksvpT1rkLPi4Hq</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jan 2024 07:01:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: The New Stack, 2023年, GenAI, 开发者
<br>
<br>
总结: 2023年是GenAI帮助开发者开发应用的突破之年。工具创建者设法在开发人员最常从事的工作中满足他们。 </div>
                        <hr>
                    
                    <p>本文最初发布于The New Stack。</p><p></p><p></p><blockquote>2023年是GenAI帮助开发者开发应用的突破之年。工具创建者设法在开发人员最常从事的工作中满足他们。</blockquote><p></p><p>&nbsp;</p><p><a href="https://www.eweek.com/development/ibm-watson-provides-self-service-ai-for-developers/">很长一段时间以来</a>"，我一直想知道<a href="https://thenewstack.io/ai/">AI</a>"如何才能更好地帮助软件开发人员完成他们的工作。在软件开发领域，2023年是人工智能的关键一年，像<a href="https://thenewstack.io/github-copilot-a-powerful-controversial-autocomplete-for-developers/">GitHub Copilot</a>"这样的工具不断发展，帮助开发人员构建应用程序。</p><p>&nbsp;</p><p>至少十年来，我会时不时地向我的技术伙伴Grady Booch提出这个问题。他是<a href="https://www.ibm.com/?utm_content=inline-mention">IBM</a>"研究院软件工程首席科学家，并且是一名IBM杰出研究人员（IBM Fellow）。早些时候，他说他相信人工智能会在<a href="https://thenewstack.io/software-development/">软件开发</a>"中发挥作用，但他对应用的程度持怀疑态度。</p><p>&nbsp;</p><p>他一直这么认为。去年年底，Booch在Twitter（现在的X）上与Replit首席执行官<a href="https://www.linkedin.com/in/amjadmasad/">Amjad Masad</a>"就人工智能能给开发者带来什么进行了<a href="https://twitter.com/Grady_Booch/status/1595878463298105344">激烈的辩论</a>"。Masad写道，随着人工智能模型的发展，在某个时候，“世界上的每个人都将至少拥<a href="https://twitter.com/amasad/status/1595557798850461702?lang=en">有John carmack级别的软件能力</a>"。”<a href="https://twitter.com/ID_AA_Carmack">Carmack</a>"是一位著名的游戏开发者(《毁灭战士》和其他游戏的共同创建者）和AR/VR专家，曾担任Oculus VR的首席技术官，并在<a href="https://about.meta.com/?utm_content=inline-mention">Meta</a>"收购Oculus后担任顾问CTO。</p><p>&nbsp;</p><p>我们离那一步还远着呢。但就<a href="https://thenewstack.io/how-generative-ai-can-increase-developer-productivity-now/">对开发人员生产力的提升</a>"而言，2023年已经证明了人工智能的巨大作用。</p><p>&nbsp;</p><p>“对于人工智能辅助编码和软件开发来说，2023年确实是具有开创性的一年，”Omdia应用智能首席分析师<a href="https://www.linkedin.com/in/lianjye/?originalSubdomain=sg">Lian Jye Su</a>"在给The New Stack的电子邮件中表示，“Copilot和其他生成式人工智能工具在作为开发工具使用时，已经展示了很高的准确性、自动化程度，最重要的是，灵活性。开发人员可以使用自然语言来表达他们的意图，并专注于创造和评价方面，因为工具会处理那些单调枯燥的部分。</p><p>&nbsp;</p><p>“因此，有一些开发人员使用这些工具生成了近40%的代码。除了Copilot，其他关键工具还包括OpenAI Codex、Replit、<a href="https://www.tabnine.com/?utm_content=inline-mention">Tabnine</a>"、Codacy和Durable。”</p><p>&nbsp;</p><p>最近，JetBrains的一项关于<a href="https://thenewstack.io/jetbrains-developer-survey-tracks-rapid-adoption-of-ai-chatgpt/">开发者生态系统现状</a>"的<a href="https://blog.jetbrains.com/team/2023/11/20/the-state-of-developer-ecosystem-2023/">研究</a>"表明，如果有机会，56%的受访者会<a href="https://www.jetbrains.com/lp/devecosystem-2023/ai/#ai_delegate_activities">让人工智能助手编写代码注释和文档</a>"。</p><p></p><h2>Copilot的演进</h2><p></p><p></p><p>与此同时，46%的受访者表示他们<a href="https://www.jetbrains.com/lp/devecosystem-2023/ai/#ai_tools_experience">使用Copilot</a>"。</p><p>&nbsp;</p><p>GitHub于2022年6月推出了代码自动补全/结对编程工具Copilot。GitHub首席产品官<a href="https://www.linkedin.com/in/inbalshani/">Inbal Shani</a>"在接受The New Stack采访时表示，该网站目前拥有150多万用户。此外，她说，使用Copilot编写的新代码其比例已从35%增长到60%，预计未来几年将达到80-90%。</p><p>&nbsp;</p><p>Shani指出，像GitHub Copilot这样的人工智能开发工具正在大幅增长，并对软件开发产生了革命性影响，对生产力、质量和工作满意度也产生了积极的影响。</p><p>&nbsp;</p><p>开发人员对人工智能工具的采用呈指数级增长。根据GitHub的数据，92%的开发人员现在在工作中使用了某种形式的人工智能。这在很大程度上是因为GenAI模型的兴起推动了人工智能的民主化应用。</p><p>&nbsp;</p><p>现在，像GitHub Copilot这样的人工智能辅助工具已经可以生成完整的代码块、代码说明、文档以及开发人员所提问题的答案。<a href="https://www.linkedin.com/in/amandaksilver/">Amanda Silver</a>"是<a href="https://news.microsoft.com/?utm_content=inline-mention">微软开发部门</a>"的副总裁，负责产品、设计、用户研究和工程系统。他在接受The New Stack视频采访时表示，这有助于新的开发人员更快地参与到开发中来。</p><p>&nbsp;</p><p>此外，Copilot还可以优化和重构代码、排除问题、提出安全修复建议、辅助调试等等，这使得开发人员不需要在这些领域有很深入的专业知识。</p><p>&nbsp;</p><p>在微软内部，工程师们在构建自己的产品时会对Copilot的功能做“内部测试（<a href="https://thenewstack.io/what-launchdarkly-learned-from-eating-its-own-dog-food/">dogfood</a>"）”，并向GitHub提供反馈以改进这项技术。Silver说，事实上，微软的工程师们在使用Copilot执行大规模的内部系统迁移，这也有助于测试它的批量重构能力。</p><p></p><h2>CodeWhisperer</h2><p></p><p></p><p>去年，亚马逊也推出了Copilot的竞争对手<a href="https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/">CodeWhisperer</a>"，就像Copilot和其他工具一样，到2023年，它才成为一款可供日常使用的工具。<a href="https://aws.amazon.com/codewhisperer">Amazon CodeWhisperer</a>"是一种<a href="https://thenewstack.io/the-ultimate-guide-to-machine-learning-frameworks/">机器学习（ML）</a>"驱动的服务，它可以基于开发人员之前的注释和代码提供代码建议，帮助提高开发人员的生产力。</p><p>&nbsp;</p><p><a href="https://aws.amazon.com/?utm_content=inline-mention">Amazon Web Services</a>"在生成式人工智能方面表现出色，它已经推出了CodeWhisperer（正式发布）、<a href="https://aws.amazon.com/bedrock/">Bedrock</a>"（用于构建自定义人工智能应用）以及<a href="https://aws.amazon.com/about-aws/whats-new/2023/11/partyrock-amazon-bedrock-playground/">PartyRock</a>"（用于无代码人工智能应用构建，目前处于预览阶段）。在去年秋末的re:Invent大会上，该公司还发布了一款名为<a href="https://thenewstack.io/amazon-q-a-genai-to-understand-aws-and-your-business-docs/">Amazon Q</a>"的GenAI助手，专用于工作场合，可以根据组织的业务进行定制。</p><p>&nbsp;</p><p><a href="https://www.linkedin.com/in/dseven/">Doug Seven</a>"是亚马逊软件开发总监、Amazon CodeWhisperer和<a href="https://aws.amazon.com/q/">Amazon Q</a>"的总经理。他在接受The New Stack采访时表示，根据亚马逊的研究，CodeWhisperer将开发人员的生产力提高了50%至60%，而像自定义这样的功能进一步提高了生产力。</p><p>&nbsp;</p><p>Seven表示：“从针对人工智能工作负载优化芯片到利用人工智能的开发工具，亚马逊在人工智能领域很有竞争力。”</p><p></p><h2>GenAI工具概览</h2><p></p><p></p><p>直到2023年，在整个数字计算的历史中，编程的核心活动一直是人类编写代码行。人工智能工具的出现增强了这种能力，但人类编码一直是核心。</p><p>&nbsp;</p><p>这种情况现在已经变了。Intellyx分析师<a href="https://www.linkedin.com/in/jasonbloomberg/">Jason Bloomberg</a>"在发给The New Stack的电子邮件中说：“今天，不断发展的核心编程活动是让人工智能（主要是生成式人工智能和神经网络）创建应用程序。人类不再编码，而是创建和管理训练人工智能的模型和数据集。”</p><p>&nbsp;</p><p>当然，手工编码还会存在，但Bloomberg表示，他认为手工编码已经退居次要地位。他说：“任何组织、供应商或企业，如果没有意识到这一点，就注定会失去竞争力，最终变得无足轻重。”</p><p>&nbsp;</p><p>事实上，正如GitHub的Shani所说，“人工智能现在成了软件开发的筹码。”GitHub希望他们的工具可以覆盖软件开发生命周期（SDLC）的更多部分。</p><p>&nbsp;</p><p>Gartner分析师<a href="https://www.linkedin.com/in/thomasmurphy4/">Thomas Murphy</a>"在电子邮件中表示：“显然，Copilot在上市时间和销量方面取得了先机，但Tabnine、CodeStory、Codium等其他工具也都在扩展GenAI的空间。”他说，除了AWS和谷歌，<a href="https://about.gitlab.com/?utm_content=inline-mention">GitLab</a>"和Atlassian等云提供商也在进军代码生成领域，并为SDLC提供更广泛的人工智能辅助功能。</p><p>&nbsp;</p><p><a href="https://www.jetbrains.com/">JetBrains</a>"也是一个杰出的开发工具制造商。他们一直在构建自己的AI助手特性，并在其最新的产品更新中发布。他们的工作重心在其旗舰产品<a href="https://thenewstack.io/jetbrains-formulates-ide-go-called-gogland/">IntelliJ IDEA</a>"集成开发环境（IDE）上。他们新推出的<a href="https://thenewstack.io/jetbrains-launches-new-ai-assistant-powered-by-multiple-llms/">AI助手</a>"插件引入了不同的功能——它可以“在更高、更抽象的层面上提供代码重构建议，而不仅仅是修复特定的模式，”JetBrains开发大使<a href="https://www.linkedin.com/in/mtellis/?originalSubdomain=uk">Matt Ellis</a>"告诉The New Stack。</p><p>&nbsp;</p><p>Ellis说，未来，他们希望改进这款助手，使其“更智能”——这样它就可以利用现有的内部代码索引和元数据更好地理解代码上下文，并探索其模型的企业托管选项。</p><p>&nbsp;</p><p><a href="https://thenewstack.io/appmap-releases-runtime-code-review-as-a-github-action/">AppMap</a>"首席执行官<a href="https://www.linkedin.com/in/elizabethlawler/">Elizabeth Lawler</a>"告诉The New Stack：“像Copilot这样的工具擅长于提供局部代码建议，但不了解应用程序背景。新玩家可以整合可观察性数据、架构信息等，提高建议的准确性。”</p><p>&nbsp;</p><p>然而，2023年已经使人工智能基础设施和模型的使用变得足够简单，现在，即使是比较小的公司也可以利用可定制的人工智能。Lawler指出，这将催生新的增值服务。此外，她补充说，人工智能生成的代码仍然存在质量问题，但它会要求开发人员必须认真评估这些建议。</p><p>&nbsp;</p><p>与此同时，从风险投资的角度来看，<a href="https://www.differential.vc/">Differential Ventures</a>"创始人兼管理合伙人<a href="https://www.linkedin.com/in/nickadams11/">Nick Adams</a>"表示，有些公司的业务分析师经常会编写重复的查询以提取数据，对于这些公司来说，利用GenAI来协助业务分析师是一个机会。</p><p>&nbsp;</p><p>Adams告诉The New Stack：“人工智能工具可以从业务用户那里获得自然语言问题，然后帮助生成SQL或Python查询，并允许分析师改进代码。这可以加快这个过程，避免每次都从头开始编写查询。”</p><p>&nbsp;</p><p>他指出，迁移像<a href="https://thenewstack.io/u-s-unemployment-surge-highlights-dire-need-for-cobol-skills/">COBOL</a>"这样的遗留代码也是一个机会。IBM在将旧的COBOL代码迁移到Java时就利用了watsonx的GenAI功能。</p><p>&nbsp;</p><p>Adams说：“许多老系统都是由即将退休的开发人员编写的。电气工程领域也有许多即将退休的专家，他们几十年来积累的知识需要通过现代工具和培训传递给下一代。”</p><p></p><h2>GenAI与低代码</h2><p></p><p></p><p>我曾经认为GenAI会以某种方式涵盖<a href="https://thenewstack.io/low-code-vs-no-code/">低代码、无代码开发</a>"，但<a href="https://thenewstack.io/pega-infinity-23-advances-low-code-application-development/">Pegasystems</a>"首席技术官和产品营销副总裁Don Schuerman让我看清了现实。</p><p>&nbsp;</p><p>GenAI对软件行业产生了重大影响，包括低代码平台，因为自动生成代码的工具可能会扰乱低端市场。</p><p>&nbsp;</p><p>Schuerman告诉The New Stack，“然而，企业级低代码更关注架构、集成、<a href="https://thenewstack.io/devops/">DevOps</a>"、<a href="https://thenewstack.io/security/">安全性</a>"、可重用性等。这种复杂性可不仅仅是生成代码片段”。</p><p>&nbsp;</p><p>Pegasystems正在使用GenAI自动生成低代码工件以加速开发，如标准工作流模板、测试数据、过程文档、数据映射等。</p><p>&nbsp;</p><p>来自亚马逊的Seven表示，CodeWhisperer帮助使用IDE进行开发的专业开发人员，而Party Rock则针对其他群体（如主题专家）用很少的代码或不用代码来构建应用程序。</p><p>&nbsp;</p><p>他说：“我不认为GenAI会让低代码、无代码过时。我认为它将使低代码变得更好。”</p><p>&nbsp;</p><p>在谈到人工智能是否会取代开发人员的问题时，Schuerman表示：“我不认为生成式人工智能会完全取代开发人员。它将通过完成一些初始工作及提供建议来提升他们的效率。无论如何，开发人员很少完全从头开始编写所有内容。”</p><p>&nbsp;</p><p>JetBrains的调查显示，60%的受访者认为<a href="https://thenewstack.io/how-will-generative-ai-change-the-tech-job-market/">人工智能编码工具将从根本上改变就业市场</a>"，51%的受访者认为这些工具将增加市场对专业软件开发人员的需求。然而，人们一致认为，人工智能永远不会完全取代开发人员编写代码的工作。</p><p></p><h2>Studio之年</h2><p></p><p></p><p>微软人工智能平台项目管理副总裁John Montgomery告诉The New Stack，去年11月，微软推出了<a href="https://azure.microsoft.com/en-us/products/ai-studio">Azure AI Studio</a>"预览版。这是一个新平台，旨在使拥有各种能力和偏好的开发人员能够利用人工智能进行创新，并以可靠的人工智能实践为基础，使用最新的人工智能工具和机器学习模型进行探索、构建、测试和部署。</p><p>&nbsp;</p><p>他说，“我们相信人工智能是终极放大器。这种向生成式人工智能的最新转变已经彻底改变了世界。它改变了应用程序的游戏规则。我已经在微软工作了25年；我从未见过客户如此迅速地接受一项技术。在不到一年的时间里，我们有超过1.8万名客户在用Azure OpenAI构建自己的东西。”</p><p>&nbsp;</p><p>他举例说，Instacart正借助<a href="https://thenewstack.io/dev-news-gpt-4-turbo-chrome-talks-pretty-and-worlds-merge/">GPT-4 Turbo</a>"&nbsp;with Vision使用户能够拍摄手写的购物清单，并在他们的应用程序中生成虚拟购物清单。</p><p>&nbsp;</p><p>Montgomery说道：“你可以将AI Studio视为生成式AI的终极工具包。它将恰当的模型、数据和可靠的人工智能系统结合在一起，这样客户就可以相信他们的解决方案是安全、可靠和可扩展的。”</p><p>&nbsp;</p><p>微软宣布支持自己最新的大型语言模型（LLM），以及来自<a href="https://thenewstack.io/beyond-chatgpt-exploring-the-openai-platform/">OpenAI</a>"、Meta、Nvidia、Mistral AI和<a href="https://thenewstack.io/how-hugging-face-positions-itself-in-the-open-llm-stack/">Hugging Face</a>"的最新模型。</p><p>&nbsp;</p><p>此外，该公司还宣布以Azure AI服务的形式推出Azure AI模型即服务。Montgomery说，“这将使客户更容易部署和运行尖端模型”。微软还推出了名为“<a href="https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?view=azureml-api-2">提示流</a>"”的提示工程工具。提示流是一种先进的提示工程、评估和部署系统，是Azure AI Studio和Azure Machine Learning的关键组成部分。</p><p>&nbsp;</p><p>尽管OpenAI是这个领域的重要玩家，并且与微软有着密切的合作关系，但该公司的一位发言人告诉The New Stack，他们无法接受采访。</p><p>&nbsp;</p><p>与此同时，谷歌也不甘示弱，他们最近也发布了<a href="https://makersuite.google.com/">Google AI Studio</a>"。这是一个免费的工具，使开发人员能够快速开发提示，然后获取API密钥用于他们的应用开发。谷歌表示，该工具目前是有限免费使用，<a href="https://ai.google.dev/pricing">未来的价格也将很有竞争力</a>"。</p><p>&nbsp;</p><p>在新闻发布会上，谷歌实验室副总裁<a href="https://www.linkedin.com/in/joshwoodward/">Josh Woodward</a>"展示了如何用谷歌账户登录Google AI Studio，并使用每分钟允许60个请求的免费配额。Woodward还演示了如何通过简单地点击“Get Code”将开发人员的工作转移到他们选择的IDE中。</p><p>&nbsp;</p><p>谷歌Developer X和DevRel副总裁兼总经理<a href="https://io.google/2022/speakers/jeanine-banks/">Jeanine Banks</a>"在接受The New Stack采访时表示，在内部，谷歌一直在将其人工智能技术应用于代码补全、文档、聊天机器人等方面。她说，“在内部应用这项技术，确实让我们学到了很多东西，我们会把这些东西带回到产品中。”</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://thenewstack.io/generative-ai-in-2023-genai-tools-became-table-stakes/">https://thenewstack.io/generative-ai-in-2023-genai-tools-became-table-stakes</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TVspvQxKRBTdzYhMFFz5</id>
            <title>Taylor Swift身陷不雅照风波：AI越强、Deepfakes越猖狂，微软和推特们无法推责</title>
            <link>https://www.infoq.cn/article/TVspvQxKRBTdzYhMFFz5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TVspvQxKRBTdzYhMFFz5</guid>
            <pubDate></pubDate>
            <updated>Wed, 31 Jan 2024 06:42:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Taylor Swift, Deepfakes, 平台举报, 技术媒体指责
<br>
<br>
总结: 流行乐巨星Taylor Swift遭遇了未经同意的虚假不雅内容的Deepfakes事件，粉丝举报了这些图片，但平台没有采取行动。技术媒体对微软提供制作工具的指责也引起了关注。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>“他们没有认真对待我们的痛苦，所以现在我们有责任大规模举报这些人并让他们停职，”Taylor Swift Deepfakes 事件中参与平台举报的一位女性称。在Swift粉丝看来，平台上图片的消失是粉丝举报的结果，推特没有作为。同时，国外各大技术媒体也对“涉嫌”提供制作工具的微软表达了指责，技术开发商承诺的“安全”，似乎还差得远。</blockquote><p></p><p>&nbsp;</p><p>想象一下，你醒来发现有人在未经你同意的情况下制作了关于你的虚假不雅内容，而且它还在互联网上广泛流传……这样的事情就被流行乐巨星Taylor Swift遇上了。</p><p>&nbsp;</p><p>上周三，未经同意的Taylor Swift 的露骨深度Deepfakes在 X 上疯传，在发布这些图像的账户被暂停之前的 19 小时内，获得了超过 2700 万次观看和超过 26 万个点赞。由于内容过于露骨，这里就不展示涉事原图了。</p><p>&nbsp;</p><p>X 上描绘 Swift 裸体场景的 Deepfakes 内容继续激增，其中包括病毒式传播的 Deepfakes 图像，后来甚至获得了高达千万级别的浏览量。图像上的水印表明它们来自一个已有多年历史的网站，该网站以发布假冒名人裸照而闻名。该网站的一部分标题为“AI Deepfakes”。</p><p>&nbsp;</p><p>上周四，话题标签 #ProtectTaylorSwift 开始在 X 上流行。这些图像首先上传到 Telegram，随后迅速在社交媒体上转发，浏览量达数百万次，并且在某些平台上尚未被删除。</p><p>&nbsp;</p><p>作为回应，X 似乎在图像出现几天后禁止了对 Swift 名字的搜索，但在过去两年里削减了三分之一的内容审核员之后&nbsp;，这种努力太少、太晚了。</p><p>&nbsp;</p><p></p><h2>涉事其中的微软和X</h2><p></p><p>&nbsp;</p><p>这种未经当事人同意的疯狂行径，再次将AI Deepfakes问题推上了风口浪尖。其实在此次事件之前，人们就已经开始对网上流传的Deepfakes素人色情制品表达担忧。如今巨星Taylor Swift也被拖入泥潭，瞬间彻底引爆整个舆论场，而微软也被牵涉其中。</p><p>&nbsp;</p><p>404 Media的<a href="https://www.404media.co/ai-generated-taylor-swift-porn-twitter/">报告表明，</a>"那些图片来自基于 Telegram 的非自愿色情制作社区，该社区建议使用 Microsoft Designer 图像生成器。设计师理论上拒绝生成名人的图像，但人工智能生成器很<a href="https://www.theverge.com/2023/10/5/23905311/microsoft-bing-dalle3-generative-ai-images-twin-towers">容易被欺骗</a>"，可以通过对提示进行小的调整来打破规则。虽然这并不能证明 Designer 被用于 Swift 图片，但这是 Microsoft 可以解决的技术缺陷。</p><p>&nbsp;</p><p>微软首席执行官Satya Nadella对此做出了回应。Satya 称未经同意的模拟裸体的泛滥“令人震惊和可怕”，并表示，“保障网络世界的安全性对每个人都有好处。因此，我想没人愿意接受一个内容创作者和内容消费者都缺乏安全保障的网络环境。我们有必要以此事为契机迅速采取行动。”</p><p>&nbsp;</p><p>Satya称，已经成功地解决了漏洞，但回应中对具体技术问题的提及很少：</p><p>&nbsp;</p><p>“我们需要在技术周围设置护栏，以确保生产出更多安全的内容。目前有很多工作正在进行中，但仍有更多事情需要我们去做。但这是全球社会需要在某些规范上达成共识，特别是当法律、执法和技术平台能够紧密结合时，人类有能力管理比自认为要多得多的问题。”</p><p>&nbsp;</p><p>微软人工智能工程主管 Sarah Bird 证实称，“我们正在继续调查这些图像，并加强现有的安全系统，进一步防止我们的服务被滥用于生成类似的图像。”</p><p>&nbsp;</p><p>对于马斯克的X，虽然其已禁止可能对特定人群造成伤害的行为，但在解决平台上的露骨色情Deepfakes问题上却行动迟缓，甚至并未解决。</p><p>&nbsp;</p><p>1 月初，一位 17 岁的漫威明星<a href="https://www.nbcnews.com/tech/misinformation/teen-marvel-star-xochitl-gomez-speaks-deepfake-rcna134753">公开表示</a>"，她在 X 上发现了自己的露骨色情Deepfakes，但无法将其删除。2023 年 6 月，该平台上还流传着未经同意的 TikTok 明星露骨色情Deepfakes，在联系 X 征求意见后，仅删除了部分材料。</p><p>&nbsp;</p><p>随着AI技术变得更加复杂和普遍，这个问题似乎每年都变得更加严重。</p><p>&nbsp;</p><p><a href="https://regmedia.co.uk/2019/10/08/deepfake_report.pdf">Deeptrace Labs 2019 年 9 月的</a>"一份报告发现，未经同意的 Deepfakes 色情内容占所有在线 Deepfakes 视频的 96%。 2023 年 10 月，<a href="https://www.wired.com/story/deepfake-porn-is-out-of-control/">《连线》</a>"分享了一位匿名独立研究人员的调查结果，该研究发现 2023 年前 9 个月，有 113,000 个 Deepfakes 视频被上传到最受欢迎的 Deepfakes 色情网站，比 2022 年全年上传的 73,000 个大幅增加。</p><p></p><h2>“消失”的监管</h2><p></p><p>&nbsp;</p><p>不少“霉粉”在举报这些不当图像之余，也纷纷表达了对于法律监管缺失的失望之情。</p><p>&nbsp;</p><p>美国白宫新闻秘书Karine Jean-Pierre宣称国会“应采取立法行动”以杜绝伪造的不适宜工作场所（NFSW）图像。她在接受ABC新闻采访时表示，“我们对……此次色情图像广泛传播的报道感到震惊——更确切地说，应该叫伪造图像。”</p><p>&nbsp;</p><p>与此同时，Jean-Pierre还敦促包括X在内的各社交媒体应用删除这些图像，防止不当内容在网络上进一步扩散。她补充道，“虽然社交媒体公司在内容管理方面拥有独立的决策权，但我们相信，他们可以、也应当在执行管控规则、防止错误信息及未经本人同意的敏感图像传播等方面发挥重要作用。”</p><p>&nbsp;</p><p>本月早些时候，美国众议院议员Joe Morelle（纽约州民主党人）和Tom Kean（新泽西州共和党人）重新提出了《防止敏感图像Deepfakes法案》。该法案旨在将生成和传播未经本人同意的露骨图像定性为刑事犯罪，且最高可判处十年监禁。</p><p>&nbsp;</p><p>Dorota本人就是Deepfakes色情图像的受害者，并因无法阻止不当内容的传播而深感痛苦。</p><p>Francesca在发言中强调，“虽然我还年轻，但我的声音仍然充满力量。面对这样的事态，我绝不可能保持沉默。我们必须勇敢表达立场，反抗我们面临的不公。这件事已经对我本人和我的同学造成严重影响，我不会双手一摊、假装无事发生。”</p><p>&nbsp;</p><p>“我在这里强烈呼吁推动变革，为立法而战，这样才能避免更多人像去年10月20号的我那样深陷迷茫和无助。我们的声音就是我们的武器，我和妈妈要求建立一个更安全的网络世界——这不只是要存留公义和希望，更是为了改善大家身处的现实环境。”</p><p>&nbsp;</p><p>该法案已经于2023年被提交给国会，随后由众议院司法委员会接手，但当时并未采取任何行动。</p><p>&nbsp;</p><p></p><h2>我们可以做哪些事</h2><p></p><p>&nbsp;</p><p>我们并非没有对抗现实的武器，新的工具和法律有望遏止这类不当图像，并帮助我们有效追究肇事者的责任。</p><p>&nbsp;</p><p></p><h4>水印</h4><p></p><p>&nbsp;</p><p>社交媒体平台会筛选上传到其网站上的帖子，并删除其中有违政策的内容。但从X上传播的Swift视频来看，整个管控过程其实不够完整，可能漏掉大量有害内容。此外，如何区分真实内容与AI生成内容也是个重大难题。</p><p>&nbsp;</p><p>作为一种可行的技术解决方案，水印能够在图像中隐藏某些不可见信号，帮助计算机识别内容是否由AI生成。例如，谷歌就开发出名为SynthID的系统，该系统利用神经网络修改图像中的像素点，借此添加人眼无法察觉的水印。即使图像经过进一步编辑或截屏，该标记仍可被正确检测。理论上，这些工具能够帮助企业改进内容审核能力，更快发现包括未经同意Deepfakes图像在内的各类伪造内容。</p><p>&nbsp;</p><p>这种方式的优点是：水印的实用性很强，有助于轻松、快速识别AI生成的内容，据此发现被删除的有毒帖子。Hugging Face研究员Sasha Luccioni曾对AI系统中的偏见问题进行过系统研究，她表示默认在所有图像中包含水印，能够有效遏制不良分子生成Deepfakes内容的行为。</p><p>&nbsp;</p><p>不足则是：这类系统仍处于实验阶段，尚未得到广泛使用。顽固的恶意人士也能找到回避水印的手段。相关企业并没有将水印技术应用于全部AI生成图像。例如，谷歌Imagen AI图像生成器的用户可以自主选择是否在AI生成图像中添加水印。种种现实因素，无疑限制了水印技术在打击Deepfakes色情内容方面的作用。</p><p></p><h4>隐形护罩</h4><p></p><p>&nbsp;</p><p>目前，互联网上现存的一切图像都可能被用于生成Deepfakes内容。而随着新型图像生成AI系统复杂度的提升和效果的持续改善，我们其实越来越难以分辨什么是真、什么是假。</p><p>&nbsp;</p><p>但各种新型防护工具正尝试修改图像，扰乱AI系统对内容的理解和处理，从而保护个人免受Deepfakes伪造技术的侵扰。</p><p>&nbsp;</p><p>以麻省理工学院研究人员开发的PhotoGuard工具为例，它就像是一层隐形护罩，能够以人眼不可见的方式改变照片中的像素。而一旦有人使用Stable Diffusion等AI图像生成器处理这些经PhotoGuard修改的图像，将无法得到预期中的输出结果。</p><p>&nbsp;</p><p>芝加哥大学研究人员开发的Fawkes也属于同类工具，它会使用隐藏信号保护图像内容，保证人脸识别软件难以识别图像中的人脸。</p><p>&nbsp;</p><p>另一款新型工具名为Nightshade，同样可以保护人们免遭AI系统的影响。该工具也是由芝加哥大学研究人员开发，能够为图像添加看不见的“毒化”层。这款工具是为了保护受版权保护的艺术图像免受科技企业在未经创作者同意下的窃取和使用而开发的。但从理论上讲，这项技术也能帮助所有者的任何图像免受AI系统的戕害。一旦科技企业未经所有者同意从网上获取训练素材时，这些有毒图像将破坏AI模型，例如导致其将小猫、甚至是Taylor Swift的图像识别为小狗。</p><p>&nbsp;</p><p>这种方式的优点是：这些工具能够提高恶意人士利用网络图像生成有害内容的难度。Ajder表示，相关技术在个人防止AI图像滥用方面带来了希望，普及之后将大大增强约会应用和社交媒体公司对伪造内容的监管力度。</p><p>&nbsp;</p><p>不足则是：这类护罩虽适用于最新一代AI模型，但无法保证在后续模型版本中继续稳定起效。另外，它们并不适用于已经存在于网络上的图像，更无法保护随处可见的名人素材，毕竟名人们无法控制自己的哪些照片会被传播到网上。</p><p>&nbsp;</p><p>“这将掀起一场旷日持久的技术军备竞赛。”道德AI咨询与审计公司Parity Consulting创始人Rumman Chowdhury表示。&nbsp;</p><p></p><h4>政策监管</h4><p></p><p>&nbsp;</p><p>技术修复的力量是有限的，颠覆性的深远变化终究离不开严格监管的加持。</p><p>&nbsp;</p><p>Taylor Swift的Deepfakes事件不仅引发广泛关注，也给打击此类恶意行为注入了新的动力。美国白宫方面表示此次事件“令人震惊”，并敦促国会采取立法行动。</p><p>&nbsp;</p><p>截至目前，美国仍然在以各州为单位分别出台监管法规。例如，加利福尼亚州和弗吉尼亚州已经禁止未经本人同意生成Deepfakes图像，纽约州和弗吉尼亚州还禁止传播此类内容。美国国会最近重新提出一项新的两党法案，要求将传播伪造裸照定性为刑事犯罪。新泽西州一所高中的Deepfakes丑闻也曾敦促立法者认真对待《防止敏感图像Deepfakes法案》。此番Swift事件引发的广泛关注，或将为法案吸引到更多来自两党的支持力量。</p><p>&nbsp;</p><p>世界各地的立法机构正纷纷推动对Deepfakes技术的严格管控。英国去年通过的《在线安全法案》就禁止传播Deepfakes色情内容，但并未禁止相关创作。传播者可能面临最高六个月的监禁。</p><p>&nbsp;</p><p>在欧盟方面，一系列新法案也尝试从不同角度解决这个难题。全面的《人工智能法案》要求Deepfakes创作者应明确披露相关素材是由AI生成，而《数字服务常规赛》则要求科技企业加快有害内容的删除速度。</p><p>&nbsp;</p><p>中国在2023年生效的Deepfakes立法方面走得最远。在中国，Deepfakes创作者须负责采取措施以防止其服务用于非法或有害目的，且在制作Deepfakes内容前必须征求用户同意、验证真实身份，并将输出的内容标记为AI生成。</p><p>&nbsp;</p><p>这种方式的优点：监管机构将为受害者提供追索权，对未经同意制作和传播Deepfakes色情内容者追究责任，由此构成强大的威慑力。相关立法还发出了明确信号，即未经当事人同意制作Deepfakes属于违法行为。</p><p>&nbsp;</p><p>Ajder指出，随着法律和公众认知广泛将制作Deepfakes色情内容视为性犯罪活动，真正的转机也将由此出现。“这将改变一些人对此类内容的冷漠态度，打破内容无害或者不属于实际性虐待形式的错误认知。”</p><p>&nbsp;</p><p>有不足则是：此类法律的执行难度很高。从目前的技术形式来看，受害者仍很难揪出始作俑者并对其提起诉讼。另外，Deepfakes的实际制作者很可能身处不同司法管辖区，这也会进一步加大起诉难度。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>Deepfakes并非新鲜事物，多年之前就已相当猖獗。然而，生成式AI技术的兴起，正在不断降低利用AI生成图像和视频制作Deepfakes色情及性骚扰内容的门槛。</p><p>&nbsp;</p><p>专门研究生成式AI与合成媒体的AI专家Henry Ajder表示，在与生成式AI相关的所有恶意行为当中，在非自愿情况下受Deepfakes影响的人数最多，而且女性在其中占极高比例。</p><p>&nbsp;</p><p>针对Deepfakes的解决方案通常侧重于区分“真”和“假”，但对于大多数受害者来说，这没有帮助：无论哪种方式，许多受害者都表示感觉自己受到了侵犯。抑郁、性创伤和创伤后应激障碍是受害者的常见经历，他们很难在心理上感到安全。</p><p>&nbsp;</p><p>因此，“技术向善”、“不作恶”是开发者、技术使用者共同的责任。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://www.technologyreview.com/2024/01/29/1087325/three-ways-we-can-fight-deepfake-porn-taylors-version/">https://www.technologyreview.com/2024/01/29/1087325/three-ways-we-can-fight-Deepfakes-porn-taylors-version/</a>"</p><p><a href="https://www.theregister.com/2024/01/30/nudes_taylor_swift_action/?td=rt-3a">https://www.theregister.com/2024/01/30/nudes_taylor_swift_action/?td=rt-3a</a>"</p><p><a href="https://www.theverge.com/2024/1/26/24052196/satya-nadella-microsoft-ai-taylor-swift-fakes-response">https://www.theverge.com/2024/1/26/24052196/satya-nadella-microsoft-ai-taylor-swift-fakes-response</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K</id>
            <title>讯飞星火正式发布语音大模型V3.5，数学、语义理解、代码能力持续提升</title>
            <link>https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9KK4v2Ksk7NPJwlBsz6K</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 11:22:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 科大讯飞, 星火认知大模型V3.5, 全国产算力训练, 语音交互能力
<br>
<br>
总结: 科大讯飞举行星火认知大模型V3.5升级发布会，基于全国产算力训练的讯飞星火V3.5在数学、语言理解、语音交互能力等方面全面提升，超越了GPT-4 Turbo，同时发布了星火语音大模型，支持37个主流语种，效果超过OpenAI Whisper V3，为讯飞翻译机等场景的人机交互带来全新升级。 </div>
                        <hr>
                    
                    <p>1月30日，科大讯飞举行星火认知大模型V3.5升级发布会。科大讯飞董事长刘庆峰、研究院院长刘聪正式发布基于首个全国产算力训练的讯飞星火V3.5，七大核心能力全面提升，数学、语言理解、语音交互能力超GPT-4 Turbo，重磅升级星火智慧黑板；正式发布星火语音大模型，首批37个主流语种效果超过OpenAI Whisper V3，赋能讯飞翻译机迎来全新升级助力更自由沟通，推动万物互联时代下客服、汽车、机器人等场景人机交互变革。</p><p></p><p>大模型应用加速落地，星火开发者超35万生态增长迅猛，打造个人应用赋能亿万用户；讯飞星火赋能千行百业，携手保险、银行、能源、汽车、通信等领域联合龙头企业，打造大模型赋能的应用标杆。此外，深度适配国产算力的讯飞星火开源大模型“星火开源-13B”首次发布，场景应用效果领先，昇思开源社区联合首发上线。</p><p></p><p>“通过这场发布会，我们展望一个充满希望、孕育着生长能量的春天。相信2024年一定可以实现星火燎原，通用人工智能不仅能够在中国各大领域深度而广泛应用，而且我们在源头技术创新、在大模型的底层能力上也会站上全新台阶。”刘庆峰说道。</p><p></p><p>基于全国产化算力平台训练 讯飞星火V3.5七大能力全面提升</p><p></p><p>2023年10月24日，科大讯飞携手华为，宣布首个支撑万亿参数大模型训练的万卡国产算力平台“飞星一号”正式启用。启用后的90多天里，讯飞星火步履不停，基于“飞星一号”，启动了对标GPT-4的更大参数规模的大模型训练，带来了1月30日这场讯飞星火V3.5升级发布。</p><p></p><p>首个基于全国产算力训练的全民开放大模型讯飞星火V3.5在语言理解、文本生成、知识问答、逻辑推理、数学能力、代码能力和多模态能力七个方面进行了全面升级。其中语言理解、数学能力超过GPT-4 Turbo，代码达到GPT-4 Turbo 96%，多模态理解达到GPT-4V 91%。</p><p>&nbsp;</p><p>“在更好的数据、更强的人机协同训练中，我们不能只看单个的‘原子’能力，而是要以技术进步来解决真实世界的刚需。”</p><p></p><p>技术进步如何为人类生活带来真正有效的解决方案？刘庆峰从全新赋能万物互联时代人机交互、全新赋能知识学习与内容创作、全新提升数智化生产力三个方面，向现场观众展示讯飞星火V3.5的能力提升。</p><p></p><p>大模型全新赋能万物互联时代人机交互体验，超拟人合成效果惊艳。讯飞星火V3.5不仅在语义理解、指令跟随和多轮对话的演示中，展现出优异的能力，更是在情绪感知和拟人合成方面表现出色。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/b2/c6/b2440ddf18c66124466e5014068d5ec6.png" /></p><p></p><p>“听说今年尔滨特别火，作为南方小土豆还挺想去玩一下的。要不你用东北话介绍下有啥好玩的呗？”</p><p></p><p>在实操演示环节，科大讯飞研究院院长刘聪和讯飞星火V3.5现场互动，讯飞星火V3.5一口地道的东北话逗乐现场观众。幽默之余，讯飞星火V3.5快速为刘聪定制了旅游攻略，还催促他赶快买票，春运机票紧俏。它不仅能够帮助用户带来解决方案，还能作为“知冷知热”的朋友，带入情绪互动，超高的拟人度让大模型更具人情味。</p><p></p><p>大模型全新赋能知识学习与内容创作。讯飞星火V3.5对年终总结计划、述职PPT、活动策划、政策问答等任务“信手拈来”。基于此，科大讯飞推出了可以一键快速自动生成文档和PPT的办公产品——讯飞智文，这款产品主要功能有文档一键生成、AI撰写助手、多语种文档生成、AI自动配图、多种模板选择、提供演讲备注功能等。刘聪现场演示了使用讯飞智文制作的“合肥市2024年春节旅游推广策略”PPT，短时间内超20页内容丰富的PPT一气呵成，此等“打工神器”获得台下一阵掌声点赞。</p><p></p><p>大模型还能够结合外部知识进行合理拓展，做到“旁征博引”。要素抽取、问题生成等能力的进步，能够帮助每个人以测助学形成思考的闭环，在越来越多的服务领域及学习知识场所中产生更多好用的智能体。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/a1/87/a15e84dbba6725bfb3e147f46f2edc87.png" /></p><p></p><p>大模型全新提升数智化生产力，可以更好助力科研、工业等民生刚需领域提质增效。随着数学和推理能力的升级，多模态能力逐步进阶，讯飞星火V3.5在视觉问答、联想推理等方面实现了“高分”应对，理解更加精确，表述也更好。</p><p></p><p>“讯飞星火V3.5能力的提升，已经达到了量质齐飞的关键点。”刘庆峰表示，2024年讯飞星火认知大模型的应用，一定会在越来越多的场景和领域中大放异彩。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7a/21/7a01200a0c04da4yy21145946887fe21.png" /></p><p></p><p>首先赋能的场景，就是教育领域——科大讯飞此次重磅推出星火智慧黑板，具备多模态理解与推荐、全自然交互、虚拟人辅学、智慧化录课与分享四大功能。发布现场，爱因斯坦出现在星火智慧黑板上，用“吸力巨大的吸尘器”来比喻黑洞，深入浅出地为现场观众解释“黑洞是什么”。在立体几何等知识的教学中，星火智慧黑板通过解构立方体，将枯燥的理论视觉化，教学课堂更加生动有趣。</p><p></p><p>“黑板不再是简单的板书工具，而是跃迁成为教师的AI助手。”刘庆峰提及，在最近的实际展示与使用中，升级后的星火智慧黑板得到了师生、教育专家和业务合作伙伴们的高度评价。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/03/1d/033945ff0cbef2f74a5d2654fd689d1d.png" /></p><p></p><p>为什么人工智能的每次进步都有对教育领域的赋能？刘庆峰解释，通用人工智能作为能够改变世界生产生活方式的全新技术，可以推动人类进步。“而教育是人类进步的根本，关乎每一位个体，是真正的全民刚需。”</p><p>&nbsp;</p><p>正式发布星火语音大模型</p><p>首批37个主流语种效果超过OpenAI Whisper V3</p><p></p><p>“科大讯飞从创业之初的梦想和使命，就是要实现沟通无障碍。25年了，我们目标和梦想一天都没有变。”</p><p>智能语音起家的科大讯飞，创业25年来在这条赛道上一路驰骋，持续走在世界前列。2006-2019年，连续十四年荣获国际语音合成大赛冠军；2016-2023年，连续4届获得国际多通道语音分离和识别比赛CHiME冠军；2021-2023年，连续三年获得国际语音翻译比赛IWSLT冠军……此外，还参与承建首批国家新一代人工智能开放创新平台、语音及语言信息处理国家工程研究中心等，在语音领域的持续积累。</p><p></p><p>“大模型带来了语音技术发展的全新机会。”刘庆峰强调，让机器具备学习、推理和决策的能力，就是认知大模型要干的主要工作。“简单来说，借助大模型，我们让一段语音具备更加丰富的属性，有语种、有内容、有韵律、有音色，还有情绪。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/21/38/21c65e42f929c144f3fb181bb0a44c38.png" /></p><p></p><p>他介绍，星火语音大模型效果国际领先，中文、英语、法语、俄语等首批37个主流语种的语音识别效果超过OpenAI Whisper V3，而在多语种语音合成方面，星火语音大模型的首批40个语种拟人度超83%。</p><p></p><p>“通过星火语音大模型的评测效果，我们非常自豪地告诉大家，科大讯飞继续保持了全球领先的水平。”</p><p>在此优势下，语音大模型的能力升级也应用于C端硬件产品。会上，刘庆峰介绍了搭载语音大模型的讯飞翻译机，即将上线多语种自动识别和增强式翻译两个重要功能，分别于今年1月底和3月中旬完成升级。多语种自动识别让国际沟通更加便捷，增强式翻译技术让翻译机化身AI翻译助手。据介绍，此次讯飞翻译机多语自动识别升级，将支持35种语言，为跨语言沟通提质增效；增强式翻译提供中英双语服务，让跨语言交流更加省心出彩。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/5a/aa/5afedc57b8a323b9c2db8fc602d64baa.png" /></p><p></p><p>星火语音大模型不止助力国际沟通，还能“百搭”更多场景，赋能实际应用。刘庆峰介绍，在汽车、客服、家庭、陪伴机器人等场景中，星火语音大模型还有更多用武之地，带来人机交互变革。如赋能汽车，智能驾舱、智能座舱、智能导航、音乐控制等交互体验将进一步优化；陪伴机器人、导购机器人、辅诊机器人、智能家居、穿戴式设备等产业也将随着语音大模型的赋能进一步被引爆。</p><p>&nbsp;</p><p>讯飞星火赋能亿万用户 加速赋能千行百业</p><p></p><p>“大模型发展，应用才是硬道理。”刘庆峰强调。讯飞星火自去年5月诞生以来，不断迭代升级其大模型能力，深耕千行百业的应用刚需。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/4e/00/4ebde0333c9584b34a1063a0e6796500.png" /></p><p></p><p>大模型未来，要从教育开始做起。让孩子站在人工智能肩膀上学习的讯飞AI学习机，能够提高学习效率、激发学习兴趣、提升综合素养，产品用户净推荐值（NPS）持续保持行业第一，获得2023年京东&amp;天猫双11销售额冠军。</p><p></p><p>大模型也在持续赋能办公硬件，自去年5月讯飞星火正式发布起，有超过60万讯飞智能办公本、智能录音笔用户累计调用语篇规整、会议纪要、自动写稿等能力650万次，让工作更高效。市场用销量表达好评，产品蝉联京东&amp;天猫双11品类销售额冠军。</p><p></p><p>除了硬件产品，软件应用也不在话下。星火赋能个人应用打造，目前基于讯飞听见、讯飞星火APP、讯飞输入法等应用，已累计赋能亿万用户。在星火大模型赋能下，越来越多的用户拥有专业AI助手。一位浙江的小学老师，使用智能问答高效完成教学方案设计，已累计使用APP262天、超2300篇；一位湖南的“情感咨询师”，通过大模型咨询人际关系等生活中的烦恼，现已累计使用161天、超9500次……这样的例子还有很多，大模型正切实走入日常生活。</p><p></p><p>打造内容创作平台，星火赋能百万内容生产者。音视频创作工具“讯飞智作”自去年8月15日发布以来，新增了21万会员用户，生成了160万音视频内容；图文创作工具“星火内容运营大师”自去年10月24日发布以来，已服务了5000+企业，生成了超150万篇文章，助力内容创作者高效生产。</p><p>&nbsp;</p><p>今天，在讯飞开放平台之上，大模型总开发者超35万，其中企业开发者超22万，开发者数增长迅猛，大模型应用加速落地，持续夯实第一开发者生态。“所以我非常自豪，讯飞星火大模型的开发者数量，在中国是最高的，而且用户口碑非常好。”刘庆峰说道。</p><p></p><p>在此基础上，科大讯飞也在加速赋能各个行业的头部企业客户。现场，刘庆峰提及了讯飞与奇瑞的合作。“奇瑞是安徽的骄傲，连续十几年都是中国汽车出口量的第一名。”刘庆峰介绍，奇瑞汽车出口的国家覆盖英语、俄罗斯语、西班牙语、阿拉伯语、葡萄牙语等数十个语种，汽车中应用的智能语音交互技术全都由科大讯飞提供。“我们用大模型全力支撑中国汽车的全球化出海战略，要让汽车变得更聪明、更智能、更面向未来。”</p><p></p><p>讯飞星火大模型加速赋能千行百业，除了汽车行业，还与太平洋保险合作，通过星火太平洋计划赋能内部知识服务、办公、审计、展业等，打造数字劳动力在保险领域的应用标杆；联合交通银行，赋能客服、展业、办公、研发等，重点打造银行领域的代码能力应用标杆；与国家能源集团联合，赋能煤炭、电力、运输、化工等方面，打造央企集团一体化联动的大模型应用标杆等。</p><p>&nbsp;</p><p>科大讯飞与中国移动强强联合，全面助力中国移动数智化转型。在赋能“5G新通话”方面，中国移动携手讯飞星火推出5G新通话创新应用“商务速记”，能够实现通话语音同步纪要，关键事项精准提炼，且无需下载APP，所有手机都支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/76/a0/766bc5509da6bf2be915fc35d609c2a0.png" /></p><p></p><p>利用大模型赋能千行百业，把大模型技术的创造力转化为促进产业高质量发展的新质生产力，正在成为行业的共同选择。发布会现场，还举行了“大模型+5G新通话商务速记应用体验”启动仪式，科大讯飞高级副总裁江涛和中国移动市场部副总经理孙世伟共同参与，5G新通话创新应用“商务速记”向广大移动用户开放体验。</p><p>&nbsp;</p><p>刘庆峰表示，为加速企业大模型应用价值落地，讯飞星火V3.5将提供全栈自主可控的优化套件。基于全国国产化算力打造的讯飞星火V3.5支持异构算力调度，可实现行业大模型训练提效90%，支持23个企业应用场景的敏捷优化。</p><p></p><p>讯飞星火大模型的全面赋能，无疑将为各行业的数字化转型提供强大的技术支持，引领新一轮的数字化浪潮。</p><p>&nbsp;</p><p>星火开源-13B正式发布 深度适配国产算力</p><p>&nbsp;</p><p>共享源代码、开发出更好的软件，是广大开发者、高校、企业自主研发热衷于开源的关键原因，也是共建第一开发者生态的重要途径。会上，首个基于全国产化算力平台”飞星一号”的开源大模型——星火开源-13B正式发布。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f3/f7/f32a7d28751247a2451720a3d80364f7.png" /></p><p></p><p>本次开源拥有130亿稠密参数（13B），包含基础模型iFlytekSpark-13B-base、精调模型iFlytekSpark-13B-chat，开源了微调工具iFlytekSpark-13B-Lora、人设定制工具iFlytekSpark-13B-Charater。学术企业研究可以基于全栈自主可控的星火优化套件，更便利地训练自己的专用大模型。</p><p></p><p>刘庆峰透露，星火开源大模型在技术上形成了差异化优势。星火开源-13B在多项知名公开评测任务中名列前茅，在文本生成、语言理解、文本改写、行业问答、机器翻译等企业典型场景中，通过对学习辅助、语言理解等领域的深入研究和优化，实用性大幅提升，在处理复杂的自然语言任务时更加得心应手。</p><p></p><p>基于“飞星一号”训练，星火开源大模型全栈国产适配优化，简单易用，场景应用效果领先，训练策略针对昇腾算力极致优化，训练效率达A100的 90%。这不仅是对昇腾AI硬件的进一步深度优化，也展示了国产算力在追赶国际先进水平方面的决心和能力。</p><p></p><p>开源只有更多的场景落地，才能更好地增进生态合作。目前，华为昇思开源社区已正式上架星火大模型开源版-13B，面对学术、企业研究完全免费，增进学术合作的同时，增进产业探索。</p><p>&nbsp;</p><p>展望2024年星火发展，这三点很重要</p><p></p><p>“在今天的大模型时代，我们绝不能只用开源模型做应用落地，在通用大模型的底座上一定要有国家队站出来。”刘庆峰强调。</p><p></p><p>当前，发展通用人工智能是我们必须要做的事情，否则工业、科研、民生等各个专用领域都会极大落后于世界，而中国是世界唯一有望成为智慧涌现第二极的国家。</p><p></p><p>展望2024年讯飞星火大模型发展，刘庆峰指出三点：“首先，一定要在通用大模型的底层能力上持续对标国际最先进水平，从算法研究包括更小的算力上做出相对更优效果。”我们要清醒理智看到差距，当前在小样本快速训练、多模态深度学习训练、超复杂深度理解等领域距离GPT-4的最好水平还有差距，讯飞星火有信心在今年上半年赶上GPT-4目前最好水平。“今天的通用大模型并不一定代表人工智能的全部未来，还有很多创新要做，如脑科学互动、对抗网络的深度连接等需要整个创新的生态，但我们一定要有勇气、有期许走在最前列。”刘庆峰指出。</p><p></p><p>“第二，2024年要真正让大模型量质齐飞，不仅是行业应用，还要在很多关键技术创新上联动大模型，中国企业界、科学界有信心实现超越。”</p><p></p><p>“第三，高楼大厦要建立在安全可控的平台之上，我们要实现自主可控平台上的生态繁荣。”刘庆峰表示，自己有信心，能够实现从算法、数据、应用场景到算力，构建一个完全自主可控的繁荣的人工智能生态。</p><p>“通用人工智能大未来刚刚开启，它必将深刻改变世界的未来，需要顶天立地的长期主义精神。”顶天，指的是希望源头核心技术在底层大模型能力上向国际最先进的能力看齐；立地，指的是大规模实现产业化。而这也是科大讯飞自成立起就一以贯之的信仰，需要在长期主义精神指引下推进各方面工作。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/30/9d/3000846132e434c65fa0c9a1ba1f829d.png" /></p><p></p><p>会上，新书《星火相传》正式发布。“这本书讲述了科大讯飞创业的很多真实故事，但更是讲述了科大讯飞奠基人、我的导师王仁华老师的教书育人经历，希望能够给讯飞人、给讯飞的合作伙伴、包括有志于做高校科技成果转化、有志于判断人工智能未来的朋友们以更多启发。”刘庆峰说道。</p><p></p><p>【活动推荐】</p><p></p><p>目前，极客邦科技正在策划 2024 年 6 月 14-15 日 <a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit 架构师峰会深圳站</a>"会议，本次会议将围绕“智能进阶. 架构重塑”大主题方向，探讨在 AI 浪潮下，架构设计如何匹配智能化趋势，以适应日益复杂的业务需求。会议上将讨论大模型、AI 运维、数据架构、应用架构、业务架构、微服务、高可用以及数字化转型等多个层面的话题，感兴趣的可以点击 ArchSummit 会议官网。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/13/04/1366c74a2e873bfb936e4f8cd3ae6c04.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371</id>
            <title>大模型应用加速进行中，智能化软件工程势头强劲</title>
            <link>https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Sl6NqhahcD9jaejgA371</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 09:51:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化软件工程, 人工智能, 大模型, 代码大模型
<br>
<br>
总结: 中国信通院、中国人工智能产业发展联盟和360集团联合举办了“大模型应用加速行动之走进360 暨首站‘AI4SE创新巡航’”活动，推动智能化软件工程领域的发展和人工智能与软件工程的交流互通。会上发布了代码大模型标准和2023年AI4SE“银弹”优秀案例，进一步推动了大模型和人工智能在软件工程领域的应用。 </div>
                        <hr>
                    
                    <p>智能化软件工程领域发展迅速，其智能化赋予软件研发流程更高的自动化和智能化水平。为加强AI与软件工程领域的交流互通，推动行业多融合发展，2024年1月25日，由中国信通院人工智能研究中心、中国人工智能产业发展联盟（以下简称AIIA）和360集团联合主办的“大模型应用加速行动之走进360 暨首站‘AI4SE创新巡航’”活动在京成功举办。</p><p></p><p>会议首先由中国信通院人工智能研究中心常务副主任魏凯、360集团首席运营官叶健分别发表致辞。魏所表示，大模型在消费市场（2C）和企业市场（2B）应用落地正在加速进行中，应用效果逐渐明显，中国信通院将联合中国人工智能产业发展联盟及产学研各方，以构建人工智能产业生态协作网络为目标，持续推动人工智能与行业融合应用，更好得赋能新型工业化。叶总表示，作为国内唯一兼具大模型和安全能力的公司，360瞄准大模型安全等关键“卡脖子”短板，积极参与信通院组织的系列活动，不断发掘AI技术作为新质生产力的潜力，共同推动人工智能大模型赋能“千行百业”。</p><p><img src="https://static001.geekbang.org/infoq/b1/b115c1603dd823ba3a8e895e08fb7bc3.png" /></p><p></p><p>&nbsp;</p><p>为公正客观得评测代码大模型，提升评测数据集的丰富度、质量和多样性，中国信通院集结产学研各方力量，正式启动代码大模型数据集共建项目，会上由魏所为共建单位颁发证书。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e5/e56d1069f47a63ae3eed0f66dc1766ec.png" /></p><p>&nbsp;</p><p>代码大模型数据集共建单位名单</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/282f9b603e07863f3b8c0cc3935d8584.png" /></p><p>&nbsp;</p><p>代码大模型数据集共建单位证书颁发仪</p><p></p><h2>业内首个代码大模型标准正式发布</h2><p></p><p>&nbsp;</p><p>随后，由中国信通院和中国工商银行联合牵头的《智能化软件工程技术和应用要求第一部分:代码大模型》标准正式发布，这标志着国内代码大模型迈向规范化与标准化的重要一步。中国信通院人工智能研究中心平台与工程化部主任曹峰，和中国工商银行专家范威，分别从标准的编制背景、编制过程、参编单位，及标准的通用能力、专用场景能力、应用成熟度等维度进行详细解读。会上由魏所为核心参编单位颁发证书。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/28/284728f51d9701766d000391f4d9fa63.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa870000a4d2d7538b1814dd352ddb1a.png" /></p><p>&nbsp;</p><p>代码大模型标准核心参编单位列表</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/59002492214d9897b0178180cce55864.png" /></p><p>&nbsp;</p><p>代码大模型标准核心参编单位证书颁发仪式</p><p>&nbsp;</p><p></p><h2>发布2023年AI4SE“银弹”优秀案例</h2><p></p><p>&nbsp;</p><p>会上正式发布了2023年AI4SE“银弹”优秀案例，中国信通院组织评委组从技术创新、应用创新、落地成效等维度综合评选出21项“银弹”优秀案例。会上由曹主任为优秀案例企业颁发奖杯。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/afde699d41e17307eece918130dd70e8.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例列表（第一批）</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/21/21c8048e57aab4740338027277a60201.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例奖牌颁发仪式（第一批）</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4435379073923960be6128a911a17eb.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例列表（第二批）</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/30ef5dd9224d24f7c07e212a5c8fe243.png" /></p><p>&nbsp;</p><p>2023年“银弹”优秀案例奖牌颁发仪式（第二批）</p><p></p><p></p><h2>主旨演讲分享</h2><p></p><p>&nbsp;</p><p>360智脑资深产品专家葛灿辉和360 Al数字人产品线负责人吕欣鸿带来“360集团大模型建设和应用”的主旨演讲，立足于生成式大模型应用，介绍了基于360智脑的Agent数字人在多种场景下辅助人类生活的应用，同时展示了结合数字人与其他AI&nbsp;Agent协同情境下，为生活提供的“五智”应用服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c047a101d05113f884734d0e664ca728.png" /></p><p>&nbsp;&nbsp;</p><p>360智脑资深产品专家葛灿辉 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f3/f32663f718767b51b1ca7a662c883ff0.png" /></p><p></p><p>&nbsp;360 Al数字人产品线负责人吕欣</p><p>&nbsp;</p><p>中国信通院云大所人工智能部高级业务主管秦思思发表了“AI4SE行业洞察”的主题演讲。她从智能化软件工程落地场景分布、各环节AI技术应用比例及提效数据、智能开发工具应用成熟度和应用效果指标等维度解读了行业落地现状和趋势，并对中国信通院AI4SE的业务方向和布局进行了详细介绍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b0ac64c3d325490cc403269fcfa060e6.png" /></p><p>&nbsp;</p><p>中国信通院人工智能研究中心高级业务主管 秦思思</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd8f448e6241e2d184f8b180abdfd349.png" /></p><p>&nbsp;</p><p>软件工程各阶段AI技术应用情况及提效数据</p><p>&nbsp;</p><p>本次论坛还邀请了中金公司算法专家雷涛、360集团产品专家苗广英、汇丰科技中国交付总监梁曜麟、蚂蚁集团Al算法专家单硕、百度资深研发工程师王初晴、阿里通义灵码产品负责人陈鑫、aiXcoder COO李力行进行主题分享，为大家呈现智能化软件工程领域的探索情况和实践效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7ffd220babda9a9f08b49088bf11398.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b084b8d8b713b4929ad641140de0494.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>本次活动还举办了首站“AI4SE创新巡航”活动--走进360， AI4SE工作组成员单位代表们共同了参观360集团的展厅。期间，360集团向AI4SE工作组成员代表们展示了自己在人工智能融合软件工程领域的创新成果和应用案例。大家对如何利用人工智能提高企业开发效率等表现出很大兴趣，代表们也积极分享了自己的经验和观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac45ad78ad74f44e5cf6d8ef54c4365c.png" /></p><p>&nbsp;</p><p>首期“AI4SE创新巡航”参观人员合影</p><p></p><p>&nbsp;</p><p>未来，中国信通院将持续推进大模型等AI技术赋能软件工程的产业研究、标准制定、评估测试、案例征集、产业活动等工作，与产学研用各方单位携手推进大模型工程化、产业化进程，共筑AI4SE可信生态。</p><p>&nbsp;</p><p>&nbsp;</p><p>联系人：</p><p>中国信通院人工智能研究中心</p><p>胡老师：17371328072(微信同号)</p><p>秦老师：13488684897(微信同号)</p><p>&nbsp;</p><p>写在最后：</p><p></p><p>中国人工智能产业发展联盟AI4SE工作组：</p><p>中国人工智能产业发展联盟（AIIA）智能化软件工程工作组（AI4SE工作组），于2023年9月正式成立，旨在进一步发挥生成式AI、大模型等人工智能技术在软件工程领域的潜力，充分释放AI赋予软件工程的价值。AI4SE工作组已吸纳100+成员单位，覆盖金融、电信、软件等诸多行业，欢迎更多企业加入，请联系胡老师、秦老师。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8287676c415c23b29cc53e845fdd1000.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg</id>
            <title>碾压前辈！Meta发布“最大、性能最好”的开源Code Llama 70B，但开发者纷纷喊穷：玩不起</title>
            <link>https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bgt612Co7OkHhHVCfLcg</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 06:51:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, Code Llama 70B, 编码模型, 性能表现
<br>
<br>
总结: Meta发布了Code Llama 70B，这是一种性能优秀的编码模型，可以根据代码和自然语言提示生成代码，同时在性能测试中表现优于其他开源解决方案。 </div>
                        <hr>
                    
                    <p></p><p>当地时间 1 月 29 日，Meta 发布了 Code Llama 70B，Meta 表示这是“Code Llama 家族中体量最大、性能最好的模型版本”。Code Llama 70B 与先前其他家族模型一样提供三种版本，且均可免费用于研究和商业用途：</p><p></p><p>CodeLlama – 70B，基础编码模型；CodeLlama – 70B – Python，专门用于 Python 编码的 70B 模型；Code Llama – 70B – Instruct 70B，针对自然语言指令理解进行微调的版本。</p><p></p><p>为了对比现有解决方案测试 Code Llama 的性能表现，Meta 选择了两项流行的编码基准：HumanEval 与 Mostly Basic Ptyon Programming（MBPP）。其中 HumanEval 主要测试模型根据文档字符串补全代码的能力，而 MBPP 则测试模型根据描述编写代码的能力。</p><p></p><p>从基准测试结果来看，Code Llama 的表现优于编码专用的开源 Llama，甚至超越了 Llama 2。例如，Code Llama 34B 在 HumanEval 上的得分为 53.7%，优于 GPT-3.5 的 48.1%，更接近 OpenAI 论文报告的 GPT-4 的 67%。在 MBPP 上，Code Llama 34B 得分为 56.2%，超越了其他最先进的开源解决方案，已经与 ChatGPT 基本持平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e4/e47f97d709006f1d2195394c47b6a053.png" /></p><p></p><p>扎克伯格在 Facebook 上说道，“编写和编辑代码已经成为当今人工智能模型最重要的用途之一。编码能力也被证明对人工智能模型更严格、更符合逻辑地处理其他领域的信息非常重要。我对这个进展感到自豪，并期待 Llama 3 和未来的模型中包括这些进展。”</p><p></p><h4>Code Llama 实现原理</h4><p></p><p></p><p>Code Llama 是 Llama 2 模型的编码专用版本，是后者在编码数据集之上接受进一步训练的产物，且数据采集周期更长。从本质上讲，Code Llama 拥有比 Llama 2 更强的编码功能。它可以根据代码和自然语言提示词生成代码及与代码相关的自然语言（例如，“为我这一条输出斐波那契序列的函数”），亦可用于代码补全和调试。</p><p></p><p>Code Llama 支持当今多种高人气编程语言，包括 Python、C++、Java、PHP、Typescript (Javascript)、C# 和 Bash。</p><p></p><p></p><p></p><p>这次，Meta 将发布四种 Code Llama 模型版本，参数分别为 7B、13B、34B 和 70B。各模型版本使用 500B 代码 token 与代码相关数据进行训练，且 70B 模型则采用 1TB token 进行训练。7B 与 13B 基础与指令模型还经过 fill-in-the-middle（FIM）训练，允许向现有代码中插入新代码，因此可以支持开箱即用的代码补全等任务。</p><p></p><p>三种模型分别能够满足不同的服务与延迟要求。例如，7B 模型可以在单一 GPU 上运行。34B 和 70B 模型则可返回最佳结果并提供更好的编码辅助功能。其中 7B 与 13B 模型运行速度更快，适合实时代码补全等强调低延迟的编码任务。</p><p></p><p>Code Llama 模型可实现最多 10 万个上下文 token 的稳定生成能力。所有模型均在 1.6 万个 token 的序列上进行训练，并在最多 10 万个 token 的输入场景下表现出性能提升。</p><p></p><p></p><p></p><p>除了能够生成更长的编码程序之外，更长的输入序列窗口还能为编码大模型解锁其他令人兴奋的新用例。例如，用户可以向模型输入来自代码库的更多上下文信息，确保生成结果的相关性更强。这还有助于在体量更大的代码库中进行调试，帮助开发人员快速找到与特定问题相关的所有代码。现在，开发人员可以将完整代码直接提交至大模型，高效完成涉及大量代码的调试任务。</p><p></p><p>此外，Meta 还进一步微调了 Code Llama 的两个附加变体：Code Llama – Python 与 Code Llama – Instruct。</p><p></p><p>Code Llama – Python 是 Code Llama 的特定语言专用变体，使用 100B 个 Python 代码 token 上接受了进一步微调。由于 Python 是代码生成领域的基准测试语言，并且通过 PyTorch 在 AI 社区中发挥着重要作用，所以 Meta 相信这套专用模型将提供更有针对性的实际功能。</p><p></p><p>Code Llama – Instruct 则是 Code Llama 的指令微调与对齐变体。指令微调同样属于继续训练过程，能够满足其他特定目标。该模型接受“自然语言指令”输入与预期输出组合的持续训练，因此能够更好地理解人们对于提示词的生成期望。由于 Code Llama – Instruct 专门就生成实用、安全的自然语言回答进行了微调，因此在使用 Code Llama 进行代码生成时，Meta 建议开发者优先选择 Code Llama – Instruct。</p><p></p><p>Meta 并不建议开发者使用 Code Llama 或者 Code Llama – Python 执行常规自然语言任务，因为这两套模型并不是为遵循自然语言指令所设计。Code Llama 专门用于特定编码任务，不适合作为其他通用任务的基础模型。</p><p></p><p>另外，在使用 Code Llama 模型时，用户须遵守 Meta 指定的许可证与可接受使用政策。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bc80cbf1775d28c3d3bd605e52d10882.png" /></p><p></p><p></p><h3>更大参数会带来更高的硬件要求？</h3><p></p><p></p><p>没有意外，Code Llama 70B 赢得了开发者们的赞扬，甚至有人称“Code Llama 70B 是代码生成领域的游戏规则改变者。”</p><p></p><p>但有自称使用过的开发者表示，“我使用 Ollama 尝试了 70B 模型，即使经过多次尝试，它也无法编写贪吃蛇游戏。而 ChatGPT 一次尝试就给出了一款可以运行的游戏。”</p><p></p><p>另一方面，随着模型参数的增加，开发者们也担心自己手头没有足够装备来满足运行 Code Llama 70B。有人指出，在 A100-80GB 上训练所有 12 个 Code Llama 模型需要 1400K GPU 小时。</p><p></p><p>运行大模型几乎可以归结为两个因素：内存带宽和计算能力，足够高的内存带宽才能“提供”计算，足够强大的计算才能跟上内存提供的数据。对于个人开发者来说，可能并没有完美设备，因此很开发者也在寻求更容易配置的量化模型版本。</p><p></p><p>也有开发者支招：可以在 64GB RAM 的 Macbook M1/M2 上运行 70B 模型。</p><p></p><p>开发者“tinco”表示，“据我所知，市场上没有其他笔记本电脑具有与 64GB MBP 一样多的 VRAM。您可以使用两个 3090 制成一台 Linux 台式计算机，将它们连接在一起提供 48GB 的 VRAM。这样显然可以运行 4 位量化的 6k 上下文 70B Llama 模型。”Tinco 进一步表示，人们推荐 Macbook 是因为它们是一种相对便宜且简单的方法，可以将大量 RAM 连接到加速器上。</p><p></p><p>Tinco 提醒道，这些是模型的量化版本，因此它们不如原始 70B 模型好，尽管人们声称它们的性能非常接近原始性能。要在不进行量化的情况下运行，则需要大约 140GB 的 VRAM。这只有一台 NVidia H100（不知道价格）或两台 A100（每台 18,000 美元）才能实现。</p><p></p><p>也有开发者分析称，理论上，单个 Nvidia 4090 能够以“可用”速度运行量化的 70B 模型。Mac 硬件在人工智能方面如此强大的原因是因为统一的架构，这意味着内存在 GPU 和 CPU 之间共享。还有其他因素，但本质上归结为每秒代币的优势。用户可以在内存带宽较低的旧 GPU 上运行这些模型中的任何一个，但每秒的令牌速度对于大多数人认为“可用”的速度来说太慢了，而且必要的量化可能会明显影响质量。</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>代码生成一直既被开发者叫好又被吐槽，即使是 ChatGPT 和 Copilot，因为虽然可以提效，但是质量问题一言难尽。</p><p></p><p>有开发者在 Hacker News 上表示，“两个月后我取消了订阅（Copilot），因为我花了太多的精力去检查所有代码并修复所有错误。当尝试处理任何不琐碎的或与 SQL 有关的事情时（即使我用整个模式预先加载它），它基本上是无用的。自己编写所有内容要省力得多，因为我实际上知道自己想写什么，并且修复自己的错误比修复机器人的错误更容易。”</p><p></p><p>使用 ChatGPT 的“ben_w”表示，“我对它（ ChatGPT）的功能感到惊讶，但即便如此，我也不会称其为‘好代码’。我将它用于 JavaScript 编程，因为虽然我可以阅读（大部分） JS 代码，但过去 14 年我一直专业从事 iOS 工作，因此不知道什么是浏览器领域的最佳实践。尽管如此，我得到工作代码后，我也可以发现它产生了错误的选择和（看起来）奇怪的东西。”</p><p></p><p>类似的问题我们也在之前的文章<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247602048&amp;idx=1&amp;sn=56bd059b8bf046ecfdb509a68c809baf&amp;chksm=fbebf64fcc9c7f594affa5928ee74adffc1c36bce4adcc521fbd2c43f5b4e4f7c0e586dd2ede&amp;scene=21#wechat_redirect">《代码屎山噩梦加速来袭，都是 AI 生成代码的锅？》</a>"中讨论过。最新研究也表示，GitHub Copilot “代码质量面临下行压力”，代码可维护性的趋势令人不安。</p><p></p><p>开源领域一直在进行生成更好代码的研究。在 Hugging Face 的“Big Code Models Leaderboard”上，也有很多被开发者认可的模型。</p><p></p><p>比如北京大学推出了⼀系列从 1.3B 到 33B 的 &nbsp;DeepSeek-Coder 开源代码模型。DeepSeek-Coder 基于 2 万亿个代币上从头训练，都使用 16K 的窗口大小和额外的填空任务在项目级代码语料库上进行预训练，以支持项目级代码补全和填充。测评结果表明，DeepSeek-Coder 不仅在多个基准测试中实现了开源代码模型中最先进的性能，⽽且还超越了 Codex 和 GPT-3.5 等现有的闭源模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4c/4cf3e0e23b2e252c922e1b9bfd21f949.png" /></p><p></p><p>对于有开发者提出“当前 SOTA 本地代码生成模型是什么”的问题，可能现在还没有标准答案，大家还在努力想办法优化现在模型，现在远不是终点。</p><p></p><p>你心中的 SOTA 代码生成模型是什么？欢迎评论区说出你的使用感受和经验！</p><p></p><p>相关链接：</p><p></p><p>Meta 在研究论文中披露了 Code Llama 的开发细节以及基准测试的具体步骤，感兴趣的用户可以查看：</p><p></p><p><a href="https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/">https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/</a>"</p><p></p><p>感兴趣的朋友可以在 GitHub 上参阅 Code Llama 训练 recipes：</p><p></p><p><a href="https://github.com/facebookresearch/codellama">https://github.com/facebookresearch/codellama</a>"</p><p></p><p>参考链接：</p><p></p><p><a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/https://news.ycombinator.com/item?id=39178886">https://ai.meta.com/blog/code-llama-large-language-model-coding/https://news.ycombinator.com/item?id=39178886</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB</id>
            <title>Neuralink完成全球首例人类脑机芯片植入手术，马斯克：植入者恢复良好</title>
            <link>https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w7ks2bB2Gc31qgrt3pXB</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Jan 2024 03:19:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 脑机接口, Neuralink, 心灵感应, 超人认知
<br>
<br>
总结: Neuralink是一家致力于开发脑机接口技术的公司，他们的第一款产品名为“心灵感应”。该技术可以将人脑连接到计算机，通过破译神经信号来帮助严重瘫痪的患者控制外部技术。马斯克表示，这项技术可以实现“超人认知”，让瘫痪的人能够用思想操作智能手机或机器人肢体，并解决自闭症和精神分裂症等问题。Neuralink已经在猴子身上进行了测试，并获得了美国食品和药物管理局的批准，开始进行人体临床试验。 </div>
                        <hr>
                    
                    <p>1月30日，脑机接口公司Neuralink创始人埃隆·马斯克在X（Twitter）上宣布：在昨天，人类首次接受脑机接口（Neuralink）芯片植入，植入者恢复良好。马斯克在随后的帖子中表示，Neuralink的第一款产品名为“心灵感应”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3a4d210f40b261d175830b696a7a9c12.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97165c85614dbc3f3355c3a7c5dc1383.png" /></p><p></p><p>&nbsp;</p><p>资料显示，Neuralink由马斯克和其他一群科学家和工程师于 2016 年创立。它致力于开发脑机接口 (BCI)，将人脑连接到能够破译神经信号的计算机，旨在帮助严重瘫痪的患者仅使用神经信号来控制外部技术。马斯克称，Neuralink 的设备可以实现“超人认知”，使瘫痪的人有一天能够用他们的思想操作智能手机或机器人肢体，并“解决”自闭症和精神分裂症。</p><p>&nbsp;</p><p>如果这项技术功能正常，患有严重退行性疾病(如ALS)的患者有一天可以使用植入物通过移动光标和用大脑打字来交流或访问社交媒体。马斯克写道：“想象一下，如果斯蒂芬·霍金的沟通速度比打字员或拍卖师还快。”“这就是我们的目标。”</p><p>&nbsp;</p><p>2022 年11月30日，马斯克曾在 Neuralink 发布会上展示了一只头骨上装有电脑芯片的猴子“Sake”，在玩心灵感应视频游戏，并要到了葡萄零食的镜头。猴子“Sake”通过屏幕和脑里植入的 Neuralink 的 N1 设备，来追踪屏幕上的移动光标，拼出了“Can I please have snacks”的英文短句，全程和键盘没有物理接触。</p><p>&nbsp;</p><p>马斯克说，猴子其实不会拼写，它们只是将大脑信号转化为光标移动，迭代到下一个版本才能让猴子具备拼写能力。马斯克还在演讲中表示，Neuralink 还将致力于恢复视力。“即使有人从未有过视力，他们先天就失明了，我们相信我们的设备仍然可以帮助其恢复视力”。</p><p>&nbsp;</p><p>当时，Neuralink 还未获得美国食品和药物管理局（FDA）的批准，因此一直在在对动物进行测试。马斯克说：“我们希望非常小心，并确保它在将设备放入人体之前能正常工作。”</p><p>&nbsp;</p><p>马斯克还表示，在人体测试开始后，他将在自己的大脑中植入脑机接口设备。“你可能现在就植入了一个 Neuralink 设备，而你甚至都不知道……在（未来）其中一个演示中，我会的” 他说。活动结束后，他在推特上重申了这一点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf414be80d775a1718eb6f6c6fca114b.png" /></p><p></p><p>2023年5月，Neuralink获得美国食品和药物管理局(FDA)批准。9月19日，Neuralink宣布，该公司已获得了独立机构审查委员会的批准，开始首次人体临床试验招募人员。Neuralink表示，因颈部脊髓损伤或肌萎缩侧索硬化症（ALS）而瘫痪的患者可能符合参加这项试验的条件。</p><p>&nbsp;</p><p>Neuralink没有透露将有多少人类患者参与其最初的人体试验。据悉，人体临床试验只是Neuralink走向商业化道路上的一步。去年的融资情况显示，Neuralink的估值已高达50亿美元（约合人民币359亿元）。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.cnbc.com/2024/01/29/elon-musks-neuralink-implants-brain-tech-in-human-patient-for-the-first-time.html">https://www.cnbc.com/2024/01/29/elon-musks-neuralink-implants-brain-tech-in-human-patient-for-the-first-time.html</a>"</p><p><a href="https://www.infoq.cn/article/6s4aDChIwYEOjVH62lGa">https://www.infoq.cn/article/6s4aDChIwYEOjVH62lGa</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK</id>
            <title>OpenAI出手后，GPT-4真的不懒了？网友不买账：只靠打补丁恐怕无济于事！</title>
            <link>https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W2jqhIuhfGQaEoCHg5BK</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 09:09:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 变懒问题, GPT-4 Turbo, 更新
<br>
<br>
总结: OpenAI发布了更新的GPT-4 Turbo，解决了模型在任务中途罢工的“变懒”问题。该更新还包括了具有视觉模态处理能力的GPT-4 Turbo正式版，并推出了小体量AI模型“嵌入”。用户之前抱怨GPT-4学会偷懒，但经过更新后的GPT-4似乎变得更聪明了。然而，对于更新内容的具体解释并未公开。 </div>
                        <hr>
                    
                    <p></p><blockquote>大模型会从人类经验中学习，如果人类本身越来越懒，那模仿人类的程序是不是也会越来越懒？</blockquote><p></p><p></p><h2>OpenAI发布更新，解决GPT-4“变懒”问题</h2><p></p><p>&nbsp;</p><p>近日，OpenAI在一篇博文中发布了多项更新，并表示更新后的GPT-4 Turbo“拥有比之前预览模型更好的代码生成等能力，且减少了模型在任务中途罢工的「变懒」情况。”但该公司并没有对更新内容做进一步解释。</p><p>&nbsp;</p><p>OpenAI在帖子中提到，由于知识库更新，已经有超过70%的GPT-4 API用户转向了GPT-4 Turbo。OpenAI表示，未来几个月内将陆续推出更多GPT-4 Turbo更新，包括发布具有视觉模态处理能力的GPT-4 Turbo正式版。这意味着用户将可输入各类多模态提示词，例如文本到图像生成提示。</p><p>&nbsp;</p><p>此外，OpenAI还推出了被称为“嵌入”的小体量AI模型。OpenAI对于嵌入的定义，是“代表自然语言或代码等内容中概念的数字序列”。以此为基础，即可搭配检索增强生成（简称RAG，一种从数据库获取信息、而非生成答案的AI方法）应用找到各类可访问内容间的关系。这些新模型、text-embedding-3-small嵌入乃至更强大的text-embedding-3-large版本现均已正式开放。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab8d4a0684379bcc0d56b4573528f399.png" /></p><p></p><p>经过改进的各GPT模型现已通过API开放，包括质量更高、价格更低廉的嵌入模型（e.gone模型的成本仅为此前嵌入模型的五分之一，但性能更强）。</p><p></p><h2>用户抱怨GPT-4学会偷懒：越来越像人类了？</h2><p></p><p>&nbsp;</p><p>2023年12月，有不少用户抱怨称，“这段时间使用 ChatGPT 或 GPT-4 API 时，会遇到高峰期速度非常慢、敷衍回答、拒绝回答、中断会话等一系列问题”。</p><p>&nbsp;</p><p>比如，某些时候，GPT-4系统会给出一些特别模糊的答案，特别是关于Docker、Kubernetes以及其他CI/CD的问题。此外，GPT-4还学会了“废话文学”——不直接回答问题，只是堆叠素材来讲解应该怎样回答问题。有用户反映，哪怕明确要求不要使用空白占位符，模型也仍然会用占位符把回答截得七零八落。这种限制回复质量的作法倒是替服务商节约了资源，但却极大浪费了普通用户的时间。</p><p>&nbsp;</p><p>用户&nbsp;jonathanallengrant 在OpenAI社区一个名为“为什么我觉得GPT变懒了”的帖子中提到：“不少人注意到自从Dev Day活动以来，模型的输出上限就变成了850个token。换言之就是ChatGPT变懒了，不光留出大量空白，还常常在同一条消息里半天停在原地。我相信这应该是OpenAI正在以某种方式扩展模型的推理方法。”</p><p>&nbsp;</p><p>用户manchkiran表示自己也遇到过类似的情况，并吐槽“现在的模型绝对是变懒了，只会快速搜索并给出Bing引擎的链接”，他猜测大模型变懒的原因或许与“微软加入OpenAI董事会后下调了算力分配”有关。</p><p>&nbsp;</p><p>sasindujayashmaavmu则从另一个角度分析了GPT-4变懒的原因：“我觉得这可能是人机回圈的锅……大模型会从人类经验中学习，所以如果人类本身越来越懒，那模仿人类的程序也会越来越懒。”</p><p>&nbsp;</p><p>对于漫天盖地的吐槽声，ChatGPT 官方通过 X 平台通知用户，“我们听到了你们关于 GPT-4 变得越来越懒的反馈！我们自 11 月 11 日起就没有更新过模型了，当然这不是故意的。”</p><p></p><h2>OpenAI出手后，GPT-4真的不懒了？</h2><p></p><p>&nbsp;</p><p>OpenAI本次更新承诺解决了GPT-4“变懒”问题，根据社区用户反馈来看，如今的GPT-4似乎真的聪明多了。</p><p>&nbsp;</p><p>用户Distinct_Salad_6683提到，最近自己发现GPT在编码能力有所提升，能够根据提示词快速提供完整的示例。之前GPT经常会拒绝给出具体示例，只是在描述自己要求它干的工作，并用“在此处插入函数逻辑”之类的废话来搪塞问题。</p><p>&nbsp;</p><p>也有用户“阴阳”OpenAI：软件只要更新一下就能解决“变懒”，真羡慕。要是能有补丁帮我扛过礼拜一就好了。</p><p>&nbsp;</p><p>由于OpenAI并未对更新内容做进一步解释，因此也有不少用户开始分析其到底是怎么解决GPT-4“变懒”问题的。语言学家&nbsp;christelle.hilz 分析，GPT-4变懒的问题跟算法无关，单靠打补丁恐怕无济于事。这个问题还得从其他角度尝试解决。“我好奇的是OpenAI愿意花多少钱来解决GPT变懒问题”。</p><p>&nbsp;</p><p>也有观点认为，OpenAI并未真正地解决问题。因为大语言模型就是算法加公式的组合，所以哪怕更新真的解决了变懒问题，只能用这种方法改进模型本身也不是什么好兆头。</p><p>&nbsp;</p><p>chieffy99则更悲观地表示，哪怕是聘请了世界各地的专家，大语言模型自身的问题还是难以解决，毕竟任何专家都不可能确切了解每一个问题。因为越是越是专注于自己的专业积累，我们的视野反而变得越狭窄。chieffy99还向OpenAI的管理团队“开炮”：</p><p>&nbsp;</p><p></p><blockquote>我向来敢于对OpenAI的缺点开炮，这里我也要明确表态：OpenAI一直认为AI的问题不可能通过开发AI方案来解决，但我觉得这是错的。&nbsp;我自己没有任何关于AI的知识和使用经验，但拥有丰富的项目管理积累。抱怨变懒问题的用户是谁、当时是怎么操作的并不是重点，重点在于大模型为什么会倾向于消极工作。我本人喜欢从问题当中寻找共性，而且从目前的情况看应该不只是模型自身出了问题。我自己还没有明确的答案，但OpenAI的态度明显是“先尝试从内部做解决或者改进，等影响到正常使用了再说”。&nbsp;在我看来，OpenAI的管理思路很有问题。以常见的团队沟通规划为例，只要提供足够的信息，GPT-3.5的表现还是相当不错的。所以我猜OpenAI也是用这种方式蒙蔽了高管团队的判断，毕竟精调提示词并不困难，请个专人就能解决。正因为如此，OpenAI才产生了单靠调整AI模型就能解决AI问题的思路。&nbsp;我不知道现在大家说的这些问题到底跟变懒有没有关系，毕竟引发问题的原因多种多样。而且GPT大模型本身也不老实，甚至会说谎来隐藏自己的真实行为。哪怕是被发现，OpenAI也可以解释说是存在误会或者提示词存在不当内容。另外别太过迷信规则，基于规则的行为也不一定比随机问题更稳定，比如GPT-3.5就会访问网站、并把外部聊天和相关数据保存成html文件。这其实是不符合GPT身份和功能定位的操作。我也遇到过中途“罢工”的情况，但这主要是大模型忘记了当前上下文中的内容必须与之前的上下文接续起来。普通用户当然分不清楚，所以很自然地认为是大模型在偷懒。这跟之前的GPT幻觉差不多，刚开始似乎经常发生，但使用的人越多、涉及的内部信息越少，幻觉也开始逐渐缓解。&nbsp;另外还有三点个人观察。首先，我很好奇OpenAI的专家到底做了什么。这个问题始于去年12月，当时外界认为GPT过于迷信专业知识、甚至为此而倾向于输出错误信息。比如通过知识文件向GPT自动输入提示词，那么生成的信息就会有所不同。而如果不输入预设文件，GPT的表现则比较正常。我就遇到了这样的情况，还专门向OpenAI上报了观察结果，想搞清在RAG问题有最终结论之前，到底该采取什么措施加以避免。而且之前我还尝试把知识跟行为区分开来做GPT训练，借此建立起纯知识库。在确保知识库内容与现实不冲突之后，再配合其他信息一起使用。第二点就是错误学习的问题。既然选择把大模型向公众开放，那能做纯软件修复的问题OpenAI肯定早就解决了。问题是时至今日，GPT还是没法在不改变形状的前提下，把不同尺度下相同颜色的图表正确合并。还是那句话，如果能修复的话早该修复好了。最后一点就是GPT号称全球最受欢迎的AI模型。这个评判标准实在太模糊了，我觉得应该从功能层面做准确描述。&nbsp;总而言之，当前关于GPT的种种报道明显是刻意设计出来的。可怕的是GPT明显还没做好准备，因此无脑宣传已经在扭曲中立研究、造成现实损害、甚至让AI制造出更多的社会问题。有人在违规使用GPT，甚至有人把它当作非法工具来设计和实施犯罪。我不知道这次的更新能产生多大影响，但各种违规行为已经真实存在，甚至对普通用户产生直接影响。我想问问OpenAI，这一切是单靠更新AI模型就能解决的吗？</blockquote><p></p><p>&nbsp;</p><p>值得一提的是，OpenAI此番发布的更新针对的是GPT-4 Turbo，即得到广泛使用的特定GPT-4版本。这套模型根据截至2023年4月的最新信息训练而成，目前仅提供预览版本。也就是说，大家如果继续使用GPT-4（使用截止于2021年9月的数据训练而成），那么“变懒”问题可能仍将存在。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/1/25/24050829/openai-gpt-4-turbo-lazy-ai-model">https://www.theverge.com/2024/1/25/24050829/openai-gpt-4-turbo-lazy-ai-model</a>"</p><p><a href="https://community.openai.com/t/i-wonder-how-much-openai-would-pay-to-cure-gpt-lazyness/604781">https://community.openai.com/t/i-wonder-how-much-openai-would-pay-to-cure-gpt-lazyness/604781</a>"</p><p><a href="https://community.openai.com/t/why-i-think-gpt-is-now-lazy/534332/11">https://community.openai.com/t/why-i-think-gpt-is-now-lazy/534332/11</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/eBpIOezhpdBY7LFPYZu3</id>
            <title>百川智能发布超千亿大模型Baichuan 3，中文评测超越GPT-4</title>
            <link>https://www.infoq.cn/article/eBpIOezhpdBY7LFPYZu3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/eBpIOezhpdBY7LFPYZu3</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 08:10:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Baichuan 3, 大语言模型, 中文任务, 医疗能力
<br>
<br>
总结: 百川智能发布了超千亿参数的大语言模型Baichuan 3，该模型在多个评测中展现出色的能力，特别是在中文任务和医疗领域上超越了GPT-4。百川智能通过多种创新技术手段和方案改进了模型的训练过程，提升了数据质量、训练稳定性和训练效率。Baichuan 3注入了丰富的医疗知识，在医疗领域的任务效果显著提升，成为医疗能力最强的中文大模型。 </div>
                        <hr>
                    
                    <p>1月29日，百川智能发布超千亿参数的大语言模型Baichuan 3。</p><p>&nbsp;</p><p>在多个权威通用能力评测如CMMLU、GAOKAO和AGI-Eval中，Baichuan 3都展现了出色的能力，尤其在中文任务上更是超越了GPT-4。而在数学和代码专项评测如MATH、HumanEval和MBPP中同样表现出色，证明了Baichuan 3在自然语言处理和代码生成领域的强大实力。</p><p>&nbsp;</p><p>不仅如此，其在对逻辑推理能力及专业性要求极高的MCMLE、MedExam、CMExam等权威医疗评测上的中文效果同样超过了GPT-4，是中文医疗任务表现最佳的大模型。Baichuan 3还突破“迭代式强化学习”技术，进一步提升了语义理解和生成能力，在诗词创作的格式、韵律、表意等方面表现优异，领先于其他大模型。</p><p>&nbsp;</p><p>链接：<a href="https://www.baichuan-ai.com/">https://www.baichuan-ai.com/</a>"&nbsp;</p><p></p><h2>百川智能做了哪些改进</h2><p></p><p>&nbsp;</p><p>与百亿、几百亿级别参数模型训练不同，超千亿参数模型在训练过程中对高质量数据，训练稳定性、训练效率的要求都高出几个量级。为解决相关问题，百川智能在训练过程中针对性地提出了“动态数据选择”、“重要度保持”以及“异步CheckPoint存储”等多种创新技术手段及方案，有效提升了Baicuan 3的各项能力。</p><p>&nbsp;</p><p>高质量数据方面，传统的数据筛选依靠人工定义，通过滤重筛选、质量打分、Textbook筛选等方法过滤数据。而百川智能认为，数据的优化和采样是一个动态过程，应该随着模型本身的训练过程优化，而非单纯依靠人工先验进行数据的采样和筛选。</p><p>&nbsp;</p><p>为全面提升数据质量，百川智能设计了一套基于因果采样的动态训练数据选择方案，该方案能够在模型训练过程中动态地选择训练数据，极大提升数据质量。</p><p>&nbsp;</p><p>训练稳定性方面，超千亿参数的模型由于参数量巨大，训练过程中经常会出现梯度爆炸、loss跑飞、模型不收敛等问题。对此，百川智能提出了“重要度保持”(Salience-Consistency)的渐进式初始化方法，用以保证模型训练初期的稳定性。并且优化了模型训练过程的监控方案，在梯度、Loss等指标上引入了参数“有效秩”的方法来提早发现训练过程中的问题，极大加速对训练问题的定位，确保了最后模型的收敛效果。</p><p>&nbsp;</p><p>此外，为了确保在数千张GPU上高效且稳定地训练超千亿参数模型，百川智能同步优化了模型的训练稳定性和训练框架，并采用“异步CheckPoint存储”机制，可以无性能损失地加大存储的频率，减少机器故障对训练任务的影响，使Baichuan 3的稳定训练时间达到一个月以上，故障恢复时间不超过10分钟。</p><p>&nbsp;</p><p>训练效率方面，百川智能针对超千亿参数模型的并行训练问题进行了一系列优化，如高度优化的RoPE, SwiGLU计算算子；在数据并行中实现参数通信与计算的重叠，以及在序列并行中实现激活值通信与计算的重叠，从而有效降低了通信时间的比重；在流水并行中引入了将激活值卸载至GPU的技术，解决了流水并行中显存占用不均的问题，减少了流水并行的分段数量并显著降低了空泡率。通过这些技术创新，Baichuan 3的训练框架在性能方面相比业界主流框架提升超过30%。</p><p>&nbsp;</p><p></p><h2>测评展示</h2><p></p><p>&nbsp;</p><p></p><h4>中文任务成绩超越GPT-4</h4><p></p><p>&nbsp;</p><p>根据百川智能，Baichuan 3在多个英文评测中表现出色，达到接近GPT-4的水平。而在CMMLU、GAOKAO、HumanEval和MBPP等多个中文评测榜单上，是超越GPT-4展现了其在中文任务上的优势。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c5d0d4de908244b55fe354458a30baf8.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d13dbff4219855337242e6a5d9b7e083.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>此外，在MT-Bench、IFEval等对齐榜单的评测中，Baichuan 3超越了GPT-3.5、Claude等大模型，处于行业领先水平。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a42d932de4a94d51c25474f89bb65f9.png" /></p><p>&nbsp;</p><p></p><h4>医疗数据集Token数超千亿，医疗能力逼近GPT-4</h4><p></p><p>&nbsp;</p><p>另外值得注意的是，百川智能还给Baichuan3注入了丰富的医疗知识。</p><p>&nbsp;</p><p>Baichuan 3在数学和代码等多个权威评测上中文任务超越GPT-4的优异成绩，已经充分证明了其基础逻辑推理的能力。在拥有丰富高质量专业医疗知识，并能通过调优后的Prompt对这些知识进行充分激发的基础上，结合超千亿参数的推理能力，Baichuan 3在医疗领域的任务效果提升显著，在各类中英文医疗测试中的成绩提升了2～14个百分点。</p><p>&nbsp;</p><p>根据百川智能，Baichuan 3在多个权威医疗评测任务中表现优异，不仅MCMLE、MedExam、CMExam等中文医疗任务的评测成绩超过GPT-4，USMLE、MedMCQA等英文医疗任务的评测成绩也逼近了GPT-4的水准，是医疗能力最强的中文大模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/10b62e1400a75ae6dd0a4209d77fff8c.png" /></p><p>&nbsp;</p><p>据悉，百川智能在模型预训练阶段构建了超过千亿Token的医疗数据集，包括医学研究文献、真实的电子病历资料、医学领域的专业书籍和知识库资源、针对医疗问题的问答资料等。该数据集涵盖了从理论到实际操作，从基础理论到临床应用等各个方面的医学知识，确保了模型在医疗领域的专业度和知识深度。</p><p>&nbsp;</p><p>针对医疗知识激发的问题，百川智能在推理阶段针对Prompt做了系统性的研究和调优，通过准确的描述任务、恰当的示例样本选择，让模型输出更加准确以及符合逻辑的推理步骤，最终不仅提升了Baichuan 3在多项医疗考试上的成绩，并且在真实的医疗问答场景下也能给用户提供更精准、细致的反馈。</p><p>&nbsp;</p><p></p><h2>创作精准度再大幅提升</h2><p></p><p>&nbsp;</p><p>语义理解和文本生成，作为大模型最基础的底层能力，是其他能力的支柱。为提升这两项能力，业界进行了大量探索和实践，OpenAI、Google以及Anthropic等引入的RLHF(基于人类反馈的强化学习)和RLAIF(基于AI反馈的强化学习)是其中的关键技术。</p><p>&nbsp;</p><p>基于强化学习对齐后的模型不仅可以更精准地理解用户指令，尤其是多约束以及多轮对话下的指令，还能进一步提升生成内容的质量。但是在大模型中充分发挥强化学习的作用不仅需要稳定且高效的强化学习训练框架和高质量的优质偏序数据，还需要在“探索与利用”两者间进行平衡，实现模型能力持续爬坡。</p><p>&nbsp;</p><p>对于以上问题，百川智能进行了深入研究并给出了针对性的解决方案。</p><p>&nbsp;</p><p>强化学习训练框架方面，百川智能自研了训练推理双引擎融合、多模型并行调度的PPO训练框架，能够很好支持超千亿模型的高效训练，训练效率相比业界主流框架提升400%。</p><p>&nbsp;</p><p>偏序数据方面，百川智能创新性的采用了RLHF与RLAIF结合的方式来生成高质量优质偏序数据，在数据质量和数据成本之间获得了更好的平衡。在此基础上，对于“探索与利用”这一根本挑战，百川智能通过PPO探索空间与Reward Model评价空间的同步升级，实现“迭代式强化学习”(iterative RLHF&amp;RLAIF)。基于强化学习的版本爬坡，可以在SFT的基础上进一步发挥底座模型的潜力，让Baichuan 3的语义理解和生成创作能力大幅提升。</p><p>&nbsp;</p><p>以文本创作中最具挑战的唐诗宋词为例，作为中国传统文化的瑰宝，诗词不仅在格式、平仄、对偶、韵律等方面均有着严格的约束条件，并且内容高度凝练、寓意深远。如果仅通过SFT的微调学习，一方面高质量诗词的创作数据需要极高的专家成本，另一方面不能在平仄、对偶、韵律等多个方面实现较好的约束理解和遵循。此外，传统的单次RLHF范式在唐诗宋词面前也遇到极大挑战，PPO在训练过程中生成的Response有可能超出Reward Model的评价范围导致“探索”的过程失控。</p><p>&nbsp;</p><p>Baichuan 3结合“RLHF&amp;RLAIF”以及迭代式强化学习的方法，让大模型的诗词创作能力达到全新高度。可用性相比当前业界最好的模型水平提升达500%，文采远超GPT-4。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b88362325ceb66ef91e477f635f4ef72.png" /></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/435b5150da5f4d74330398fa8dc2de94.png" /></p><p>&nbsp;</p><p>作为参数规模超过千亿的大语言模型，Baichuan 3不仅英文效果达到接近GPT-4的水平，还在多项通用中文任务的表现上实现了对GPT-4的超越，是百川智能的全新里程碑。Baichuan 3全面的通用能力以及在医疗领域的强大表现，将为百川智能打造“超级应用”，把大模型技术落地到诸多复杂应用场景提供有力支撑。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GDd1kvi4MGNqs48UofG5</id>
            <title>2024年入局大模型，晚了吗？</title>
            <link>https://www.infoq.cn/article/GDd1kvi4MGNqs48UofG5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GDd1kvi4MGNqs48UofG5</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 08:05:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型领域, 技术迭代, RHF, BERT
<br>
<br>
总结: 在过去一年中，大模型领域经历了迅猛发展，技术不断迭代，包括RHF和BERT等技术的应用。企业纷纷将大模型应用于业务中。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p>在过去的一年里，我们见证了大模型领域的迅猛发展，超出了所有人的预期。ChatGPT等开源模型正在以惊人的速度进行技术迭代，诸如RHF、BERT等技术都在迅猛演进，甚至小模型与专家模型的混合也崭露头角。千行百应的企业都在竞相将大模型应用于自己的业务中。</p><p>&nbsp;</p><p>在经历了大模型一年多的蓬勃发展之后，我们借助年终这个节点，停下脚步，回顾一下大模型在过去一年中所取得的成就与面临的挑战。</p><p>&nbsp;</p><p>InfoQ《极客有约》邀请Hugging Face 工程师王铁震，对话新加坡国立大学校长青年教授、潞晨科技董事长尤洋和AI 领域知名投资人陈于思，聊聊2024年的大模型发展。</p><p></p><h2>2023年大模型，哪件事让你印象深刻</h2><p></p><p>&nbsp;</p><p>王铁震：首先请两位嘉宾做下自我介绍，聊聊今年都做了哪些事情？</p><p>&nbsp;</p><p>尤洋：我目前在新加坡国立大学担任教职，同时也是潞晨科技的创始人和董事长。我的研究始于高性能计算，旨在优化数据移动和各种操作的计算速度，例如浮点运算。</p><p>&nbsp;</p><p>大约十年前，我们使用几百卡、几千卡的设备进行训练，主要应用领域是地震天气模型。在这个过程中，我意识到无论模型应用在何种领域，最终都涉及底层的矩阵运算。简而言之，我们追求的目标就是让计算变得越来越快。</p><p>&nbsp;</p><p>在四五年前，当时AI模型并不是很庞大时，我们尝试使用上百卡、上千卡进行训练，但只能采用数据并行的方式，即将一个批次的数据分成多份，分配到不同的GPU上，然后每个服务器计算或者更新梯度。在与Google合作的过程中，我们成功将Bert的训练时间从3天缩短到76分钟。随后，我在加入加利福尼亚大学伯克利分校并获得博士学位后，回到新加坡国立大学任教，并创建了高性能人工智能实验室。目前，我们实验室拥有13名博士生、7名博士后和十多名硕士生，成为一个规模不小的实验室。</p><p>&nbsp;</p><p>在2021年，我有幸接触到了李开复老师的团队，并在任博冰先生的推动下，共同创立了潞晨科技。逐步得到了一系列投资，包括创业工场、红杉等知名机构以及百强安区基金。我们目前专注于产品的技术商业化。</p><p>&nbsp;</p><p>在回顾整个2023年，如果要提及一件对业界有着重大影响的事情，我认为Llama是一个值得关注的焦点。虽然ChatGPT于2022年底推出，但我认为Llama的出现为整个产业带来了更多的可能性。我相信人工智能的产生具有长期的价值，未来的10到30年不仅仅是高端玩家的领域，而是具有广泛的社会影响。我坚信人工智能的能力具有广泛的受益性。在未来3到5年内，可能不会是严格意义上的Llama，但类似于Llama的模型，拥有数百亿的参数，比如70B、3B的，我认为对于广泛推广人工智能的能力将起到至关重要的作用。如果我只能选择提到2023年的一件事，那么像Llama这样的模型在人工智能领域的出现，我认为具有长期的战略意义。</p><p>&nbsp;</p><p>陈于思：我是陈于思，毕业于斯坦福大学，专攻电子工程博士学位。我的博士研究是芯片之间的高速互联。我们当时观察到，随着摩尔定律的继续，单一芯片的计算能力可能会达到极限，因此要实现更高的计算能力，需要将更多芯片进行连接。于是，我们与MIT、伯克利等多所高校合作，共同推进了一个涉及芯片之间光互联的大型项目。</p><p>&nbsp;</p><p>在那个时候，深度学习刚刚起步，实际上一块GPU可能已经足够。因此，我在博士毕业后转而从事硬件设计，并加入硅谷的麦肯锡公司。从那时起，我开始更加关注人工智能，包括机器学习等新技术在商业领域的应用和商业化机会。回国后，我加入平安集团，负责整个集团人工智能战略管理。</p><p>&nbsp;</p><p>在2018年时，我们就尝试使用上一代Transformer模型，如BERT，将其应用于智能客服领域。然而，当时模型的能力可能还不够强大。从2019年开始，我加入了一家基金公司，致力于全球范围内的人工智能相关投资。另一方面，我一直关注中国开源行业的发展，并曾被评选为中国开源先锋33人。</p><p>&nbsp;</p><p>在回顾整个2023年，我认为可以用一个词来总结，那就是“Scaling Law”（扩展定律）。在我看来，这一年是Scaling Law持续发扬光大的一年，而且在可预见的未来，Scaling Law肯定会继续为我们带来更大、更强大、更有力的模型，以及能够推动更多应用的可能性。</p><p>&nbsp;</p><p>当谈到Scaling Law时，我认为可以从算力、算法和数据三个维度来讨论。在算力方面，我们目睹了整个2023年英伟达股价的飙升。我记得在年初英伟达股价可能只有100多元，但等到收购结束时，股价已经涨到近400元。这也证明了英伟达在过去一年内的强大表现，由于供给不平衡导致的定价权的强势，人们对其主导未来算力生态的信心十分充足。另一方面，在年底，谷歌的Gemini以及其发布的整套TPU v5p，以及业界领先的光电混合互联技术，也给人们留下了很多想象空间，预示着未来算力可能会更加多元化。</p><p>&nbsp;</p><p>在算法方面，GPT-4据传闻是一个MoE的架构。而谷歌的Gemini则是一个全面的多模态架构，不同于先前分别预训练视觉和语言模型，Gemini从一开始就将不同的模态进行融合。我们目睹了算法架构和整个多模态领域的创新和突破。</p><p>&nbsp;</p><p>在数据方面，GPT-4在年初主要是文本数据，到了4V时开始引入更多的图像数据，而Gemini更是将所有模态，甚至宣称将YouTube上所有视频的数据都引入训练。数据领域远未达到瓶颈。结合算力、算法和数据，我们看到了整个2023年的许多突破和创新。</p><p>&nbsp;</p><p>更让人期待的是，虽然我们看到的已经很多，但在OpenAI和Google可能还有更多我们未曾见到的突破。因此，对于模型能力在2024年、2025年甚至未来几年的提升，我感到非常乐观。结合持续发展的开源，例如Meta号称可能会发布一个强大的Llama-3，我对未来充满期待。不论是闭源还是开源的模型，在Scaling Law的指导下，都将为我们带来更多惊喜。</p><p>&nbsp;</p><p></p><h2>大模型不再遥不可及</h2><p></p><p>&nbsp;</p><p>王铁震：在过去的一年里，我们见证了模型数量的急剧增长，技术在这段时间内迅速扩散，所有人都开始相信这个领域将会有巨大的发展。现在我们看到有这么多的模型，似乎在年初的时候，大模型被认为是昂贵且困难的，很多人觉得我们缺乏足够的数据和算力。</p><p>&nbsp;</p><p>现在模型似乎已经不再是遥不可及的事物，其成本迅速下降，变得非常平民化。在这个过程中到底发生了什么？我们能否请两位嘉宾分享一下，过去一年我们在技术上取得了什么样的突破，是什么样的驱动力让我们经历了如此巨大的变革？</p><p>&nbsp;</p><p>尤洋：我认为开源对这个影响非常大。特别是在年初Llama开源之后，整个行业迅速展开了许多基于Llama的微调工作。事实上，可能在去年或者2022年底的时候，很多人对AI大模型的发展并没有太多关注，所以当GPT-3于2020年6月发布时，可能有些人感到惊讶。这项技术其实有很多公开的信息，只要我们花一些时间静下来，我认为任何水平较高的团队都可以大致复现出一个不错的模型。</p><p>&nbsp;</p><p>刚开始可能有些人感到不知所措，因为之前可能接触的不够多。但是一旦大家理解了它的工作原理，我觉得我们至少可以在技术上达到ChatGPT或者GPT-4的效果。首先，Llama的技术很多都是开放的，再加上全球开源社区，例如Hugging Face以及PyTorch开源社区对这个生态做出了很多贡献。Llama的出现加上这些因素，使得像今天制作一个200亿的模型或者200亿的模型并不是一件非常困难的事情。我个人认为，至少90%的大公司都有能力去实现这样的任务。</p><p>&nbsp;</p><p>王铁震：过去，训练一个模型似乎对企业来说是一个非常大的成本和挑战。然而，随着微调技术的发展，我们现在可以相对轻松地拥有这些模型。我想问一下陈于思，在模型数量急剧增长，很多人都在进行开源模型的情况下，你是否认为从投资的角度来看，开源是一个非常好的投资机会？为什么这么多公司都在参与开源，他们的玩法是什么样的？从投资的角度，你会更倾向于选择投资开源的公司吗？还是会考虑闭源的机会呢？</p><p>&nbsp;</p><p>陈于思：我认为开源在整个生态中是非常重要的组成部分，特别是对于一些欧洲公司，例如Hugging Face和最近备受瞩目的Mistral。这些公司成功地融了大量资金，尤洋老师也一直在构建一个开源的生态系统。</p><p>&nbsp;</p><p>从软件的角度来看，开源实际上是构建自己生态系统的一种极好方式，提高品牌知名度，并吸引客户，最终实现付费转化。这一套开源到最终付费的转化过程在一些数据库和云上软件等方面已经得到验证。</p><p>&nbsp;</p><p>在大模型领域，开源显得尤为重要。正如尤洋老师刚才提到的，像Llama和基于Llama的团队，尤其是Mistral的核心人员，通过持续开源努力，降低了从事大模型研发和落地的门槛。</p><p>&nbsp;</p><p>在Llama出现之前，人们对大模型的理解可能只是一知半解，甚至有些人可能对其心存畏惧。但随着Llama-2和后续一系列开源模型的出现，尤其是模型层面和工具层面的不断开源，使得更多人能够参与大模型的创新和商业化。</p><p>&nbsp;</p><p>对于开源公司来说，一方面，它们有机会通过开源构建生态系统，吸引更多客户，并将其转化为付费用户。另一方面，许多开源公司最终会以较高的价格被收购，这对于投资者来说是一种良好的退出渠道。</p><p>&nbsp;</p><p>从投资者的角度来看，开源公司拥有大量用户基础，即使其本身收入不算太多，也能够吸引成功的大公司进行收购。开源技术的快速演进也是显而易见的，例如Llama-2的出现极大地提升了垂直领域模型的能力，而Mistral的7B和最近的8*7B的MOE架构模型有望成为新的基准。</p><p>&nbsp;</p><p>整个开源生态和闭源公司在竞争中相辅相成，这可能是一个更为健康的状态，有助于推动大模型生态不断向前快速发展。在2024年，我们期待看到更多令人激动的新成果。</p><p>&nbsp;</p><p>王铁震：于思刚才提到的全栈开源是一个非常有趣的点，这个术语可能之前我们用得比较少。在大家对开源的认识中，大模型开源通常指的是权重的开源。然而，你提到的全栈开源更全面，包括模型的训练过程以及在服务上进行的成本优化，以确保模型更快地运行并降低成本，这具有重要的意义。</p><p>&nbsp;</p><p>我之前了解到尤洋老师的Colossal-AI也在这个领域做中间层的工作。我想请教一下尤洋老师，能否简要介绍一下你们主要做了哪些工作，以便能够快速实现高性能的训练或模型推理，从而降低成本，使更多人能够轻松地使用这些模型。另外，像于思老师所说，如果不使用这些工具，成本可能比直接购买OpenAI的token还要高。那么在这个优化的过程中，或者说在这个领域中还存在哪些优化的点或者欠缺的地方呢？</p><p>&nbsp;</p><p>尤洋：我们的基础设施的最终目标是隐藏底层的具体操作细节，让用户在训练大模型时能够像制作PPT一样简单。具体而言，我们的基础设施服务针对三类用户进行了优化。</p><p>&nbsp;</p><p>第一类用户是进行大模型预训练的用户，通常拥有至少100卡，甚至千卡、万卡的资源。这类用户面临并行分布式计算的问题，涉及大模型的切割和各种并行计算策略，如数据并行、张量并行、流水线并行等。核心原则是优化数据移动，以降低延迟和频率。</p><p>&nbsp;</p><p>第二类用户是进行微调的用户，他们的资源相对有限，可能只有一个或几个服务器。这类用户对训练时间不敏感，但关注资源的充分利用和内存的优化，以在有限的资源下训练更大的模型。内存优化成为关键，涉及远端内存的使用，同时需要降低数据移动的频率和延迟。</p><p>&nbsp;</p><p>第三类用户直接进行推理，可能是在调用API或进行模型服务。这类用户的操作相对简单，可操作的空间很大，我们通过将市面上各种推理解决方案结合起来，以及引入训练的技术，实现基本操作的高效执行。</p><p>&nbsp;</p><p>王铁震：我想就结合这个问题，继续请教于思，从投资的角度来看，你认为我们当前所做的这些优化已经达到了极致吗？未来是否还存在新的机会，可以进一步提升用户的整体体验，使各种应用场景都能够得到更好的提升？2024年，再次审视这些基础设施或者中间层，你更关注哪些机会呢？</p><p>&nbsp;</p><p>陈于思：从用户体验的角度来看，我认为有两个关键点。首先是性价比，简而言之，我们希望看到模型性价比持续提升。性价比可以从两个方面考虑，首先是性能。我之前提到了Scaling Law，在2024年，Scaling Law仍将是一个备受关注的趋势。以Google为例，通过更强大的多模态能力，基于Gemini Ultra，已经在许多基准测试中超过了GPT-4。未来，随着模型对更全面、更多模态数据的预训练，GPT-5有望成为一个完全的多模态模型，这将提高基础模型本身的能力，并启用更多应用场景，如视频理解、图文理解和语音生成等，带来更多商业化的应用。</p><p>&nbsp;</p><p>另一方面是成本优化。随着模型规模的增加，成本理论上会更高。然而，从整个服务和模型成本优化的角度来看，我对2024年还是比较乐观的。目前的硬件、系统和模型技术，我相信能够实现100倍成本的优化。这包括一系列的技术创新，如FlashAttention、Flash Decoding以及Speculative Decoding等，都在模型加速和性能成本优化方面发挥了显著的作用。</p><p>&nbsp;</p><p>此外，我认为未来还会有更多针对推理的硬件优化。不同的公司都在自主研发硬件，例如Google的TPU、亚马逊的Inferentia Trainium、微软的推理芯片以及OpenAI的潜在芯片。硬件与软件的一体化优化可能会带来更多的性价比惊喜。</p><p>&nbsp;</p><p>第二个关键点，是在中间层和工具层方面可能会有更多的投资机会。在AGI到来之前，大模型的发展仍处于早期阶段。因此，在这个过程中，各种各样的工具，帮助开发者更好地开发模型，将会非常有价值。中间层的工具，如调度不同模型、自动化工具等，都有可能成为未来的发展方向。在不同任务和需求之间做好调度，将任务导向性价比最合适的模型，也是一种可能的趋势。总体来说，中间层和工具层的发展将有助于更好地应对各种复杂的开源和闭源模型，以及满足不同用户需求的优化。</p><p>&nbsp;</p><p></p><h2>AI中间层的发展潜力</h2><p></p><p>&nbsp;</p><p>王铁震：我们可以在软件和算法层面进行大量优化，这领域还有很多潜力。不仅如此，在硬件层面还将推出专门为大型模型设计的新硬件，其中包括更大容量的内存和显存芯片。软硬件的结合，以及中间层的一些优化，都为提高效率提供了巨大机会。</p><p>&nbsp;</p><p>我一直在思考一个问题，即针对大模型的场景，通过构建中间层基础设施来提高效率、降低成本，是否与过去我们所了解的SaaS有相似之处。SaaS的目标是使企业生产效率达到最优，让最合适的人去做最合适的事情，而不是将所有工作都集中在同一个平台上。然而，我们看到在国内，SaaS的发展并不十分顺利。我想请教大家，对于AI基础设施，特别是AI中间层的发展，我们是否可以借鉴SaaS在国内的发展经验？对于这个AI中间层公司未来的潜力和机遇，老师们如何看待呢？</p><p>&nbsp;</p><p>陈于思：我认为中国的AI中间层软件公司有两个主要机遇。首先，从国内的角度看，市场空间与最终市场息息相关。中国的企业应用软件市场相较于其他国家而言，市场空间可能相对较小，可能是几百亿或上千亿的规模。在这个市场中，细分领域的SaaS市场空间可能较小，但对于AI而言，我认为它的未来市场潜力将是非常巨大的。很少有人将SaaS视为产生工业革命级别机会的领域，而AI则被认为是一个具有巨大机遇的工业革命级别的领域。</p><p>&nbsp;</p><p>在这个发展过程中，中间层软件公司将有很大的机会。在讨论AI和企业软件的SaaS时，我们可能在讨论两个市场规模相差几个数量级的机会。其次，在中国，这些中间层公司在当前阶段需要考虑如何扩大自身的生态和行业影响力，同时更迅速地实现商业落地。尤洋老师和潞晨科技在这方面表现相当不错，我期待尤洋老师的详细介绍。</p><p>&nbsp;</p><p>第二点，我认为中国或华人开源的中间层机会，也可以放眼世界。在全球范围内，AI仍处于相对早期的阶段。中国工程师的成本更低，我认为中国在开发软件方面与美国相比具有一定的性价比优势。全球化也是一个重要考虑因素，例如OpenAI的GPT在全球范围内引起了广泛的关注。AI是在这个充分全球化、信息全球化的时代产生的产物。在AI领域的认知方面，虽然在基础模型领域存在一定差距，但在应用和开发者方面，中国与美国之间没有代际差距。一些早期以出海为目标的中国公司在认知上甚至可能更强，因此不一定局限于中国的市场，可以扩展到全球市场。</p><p>&nbsp;</p><p>王铁震：于思提出了两个观点。首先，他认为与传统的SaaS行业相比，AI的中间层面临的市场机会非常大。这种市场机会并不像SaaS那样局限于某一个地区，而是为中国的开发者提供了大量走向世界的机会。无论是从技术还是从成本的优势来看，我们都有很大的优势。其次，他认为在SaaS这个领域，中国的开发者也有很大的优势。我想知道，尤洋老师在整个Colossal-AI的发展过程中，是否有一些有趣的故事可以分享给我们？</p><p>&nbsp;</p><p>尤洋：首先，我要分享一组数据，这些数据来自拾象科技的李广密。他发现SaaS付费率与一个国家的人均GDP有强关联。例如，美国的SaaS付费率大约为7%，对应于其人均GDP的7万美元。欧洲和日韩的付费率约为4%，与4万美元的人均GDP相吻合。中国的SaaS付费率约为1%，与人均GDP的1万美元相吻合。印度的SaaS付费率是0.2%，与人均GDP的0.2万美元相吻合。</p><p>&nbsp;</p><p>在这组数据的基础上，我们可以做一些思考。首先，我们公司目前探索出了一条适合自身发展的道路。无论是SaaS、PaaS，还是大型AI公司，我们都希望能做出一本万利的产品，让收入以指数级增长，人力则不必线性增长。</p><p>&nbsp;</p><p>中国的SaaS在过去几年失败或者没有特别成功的原因是定制化需求过高，这也与中国的甲方文化有关。另一个重要因素是在过去十年的SaaS发展中，硬件或芯片的作用并不大。例如，我们不会通过观察阿里巴巴积累了多少CPU来评估其好坏。但在AI领域，底层芯片的效能对整个生命周期有重要影响。训练成本和产品迭代周期直接与底层芯片的效率相关。</p><p>&nbsp;</p><p>所有大公司都在优化推理成本，因为他们最终想赚钱。模型调用的成本越低，模型使用的频率越高，微调训练的需求就越高。推理和训练是相辅相成的，训练包括预训练和微调。如果一个产品被频繁调用，它就需要频繁更新。</p><p>&nbsp;</p><p>目前，我们主打自己的多位一体Colossal-AI和PaaS平台。Colossal-AI主要针对规模不大、主要做微调的用户。PaaS平台是针对稍微规模大一些的客户。我们现在也有一些世界500强和2000强的客户，他们有自己的算力，直接购买我们的企业版软件。通过这种方式，我们在2023年成功实现了几千万的收入。在公司亏损很低的情况下，达到了60%以上的毛利率。下一步，我们将继续观察市场反馈，寻找扩大规模的机会。</p><p>&nbsp;</p><p>王铁震：过去的 SaaS 可能需要高定制成本，因为强烈的甲方文化。但我的理解是，对 AI 来说，任务需求比较统一，如快速训练和推理。只要性能做好，甲方可以专注应用层开发。而且，PaaS 层有很多机会帮助甲方节省成本，甲方也愿意采购。</p><p>&nbsp;</p><p>尤洋：我认为这一点至关重要。例如，我们公司最近想要建立一个奖金股权系统。我们研究了市场上的所有SaaS软件，但发现它们并不能很好地支持这个功能。因此，我们决定雇佣两个人来帮助我们编写代码。这就说明，有很多事情实际上是难以标准化的，并且很容易走向定制化的道路。当然，如果AI的基础设施层的Transformer能统一市场，那么它的接口最终可能会相对标准化。</p><p>&nbsp;</p><p>王铁震：标准化接口有助于实现一本万利的商业模式，只需要在一个地方优化产品，就能卖给很多客户，这个市场有巨大的潜力。然而，也有一些观众提问，硬件公司最了解自己的硬件，如何看待自己做中间层工作的可能性。这实质上是之前问题的另一面，即我们看到应用层与中间层的配合非常好，那么从另一个角度来看，硬件层可能与中间层存在某种竞争或合作关系。</p><p>&nbsp;</p><p>陈于思：这个我认为在技术栈中是常见的现象。例如，英伟达公司的整个CUDA软件栈，从底层的CUDA DSL，到内核的优化，再到算子，它都有集成。也包括了训练的deep speed 和Megaton框架，推理的像tensorRT；对于框架层，不管是Tensorflow还是Pytorch，实际上都有很多优化的支持。我想说的是，硬件公司可以完成很多模型侧的中间层工作。但是有两个问题，首先，他们只会优化自己的产品，你很难想象英伟达会去优化AMD或英特尔的软件栈。因此，跨平台有很多机会，尤其是现在大家都在试图打破英伟达的垄断。</p><p>&nbsp;</p><p>最后，硬件公司本质上是做芯片的，中间层软件对他们来说是挑战。英伟达成功的地方在于它已经完全是一个系统公司和软件公司。但是其他硬件公司还是硬件公司，让他们去写优秀的软件是有挑战的。</p><p>&nbsp;</p><p>所以，我认为，基于硬件的模型中间层优化还有很多机会。但是，从模型到应用的中间层，我认为硬件公司很难做到。因为这个领域的差距太大了，从硬件算法到应用，每个层都在变化。在这个时候，中间层有很多机会，但是长期可能要考虑的是，如果生态稳定了，中间层的长期壁垒在哪里。我认为这是创业或者入局需要深思的问题。</p><p>&nbsp;</p><p>王铁震：对于Nvidia来说，其软件生态相对较好，可能会继续提升，但我们现在正处于一个充满竞争的时代。许多芯片公司可能还没有从纯硬件公司转型为硬件和系统公司，他们可能更希望与中间层合作，以使自己的软件生态更加完善，并更好地服务最终用户。这可能是硬件公司和中间层之间的合作关系，而不是竞争关系。</p><p>&nbsp;</p><p>尤洋：英伟达之所以能取得今天的成就，主要是因为他们开放了软件，没有在软件层面上与他人竞争。据英伟达的数据，现在全球有400万个CUDA开发者，这是一个非常庞大的群体。而这400万人中，大部分并非英伟达的员工，我觉得正是这种合作力量，使英伟达取得了今天的成果。</p><p>&nbsp;</p><p>如果我们再看看AI的中间层，从最开始的Cafe到Tensorflow、Pytorch，这些成功的框架并非英伟达自己开发的。现在我们需要大规模分布式处理，这已经超出了英伟达最熟悉的领域。因为分布式处理涉及GPU之间的关系，GPU和CPU之间的关系，服务器之间的关系，这不是一个硬件公司能够全面管理的。</p><p>&nbsp;</p><p>英伟达的成功已经证明了硬件公司通过与软件公司合作是可以赚到大钱的。如果英伟达关闭了CUDA生态，我相信他们不可能取得今天的成就。因此，我认为硬件公司应该会做出理性的选择。我之前参加了一次华为升腾的会议，他们也希望能够开放升腾的生态。</p><p>&nbsp;</p><p></p><h2>创业者的机会</h2><p></p><p>&nbsp;</p><p>王铁震：我们过去在这个模型层看到的是，例如Llama在形成自己的生态，开放自己的模型，引入更多的开发者，共同建设这个行业，使得这个行业更好。硬件也是如此，需要有一个庞大的开发者生态共同开发，以使硬件销售更好，并围绕硬件开发软件。</p><p>&nbsp;</p><p>当我们谈到应用侧的创业机会时，如果我们在2024年的推理和训练成本继续大幅下降，那么我们在2024年的应用层能看到一些什么样的事情？我准备了一些问题。首先，我想大家探讨一下，当前哪些行业可能有更多的大模型应用场景？他们会面临哪些挑战？很多人认为今年会是大模型应用的元年，因为大家对大模型的边界了解得比较清楚，而我们这些中间层让模型的成本大幅下降，创业者在这一年可以做些什么？难度如何？</p><p>&nbsp;</p><p>陈于思：在我看来，大规模模型的商业化落地在过去的2023年更像是一个探索期。如今，全球的大模型公司，除了OpenAI外，所有公司的营收加起来可能都无法与OpenAI匹敌。这显然是因为模型能力还处在早期阶段，同时，商业化的探索也仍处在早期阶段。在此背景下，我们可以看到的是，无论是ToB还是ToC，都有一些初步的落地应用。</p><p>&nbsp;</p><p>在ToC方面，我们看到了一些工具类产品，如OpenAI的聊天机器人和一些情感陪聊类型的产品。这些初步验证了AI在社交领域的应用可能性。此外，在游戏领域，我们也看到了一些游戏化的探索，如网易的《逆水寒》AI角色。</p><p>&nbsp;</p><p>在ToB方面，我们看到了一些类似于搜索助手的产品。我相信，在2024年，随着大规模能力的提升和性价比的极大提升，我们将能看到更多的应用落地。OpenAI 的 GPT Store就是一个例子，它是AI大规模商业化落地的一个探索，可能许多垂直化的领域模型都将基于GPT Store建立起来。</p><p>&nbsp;</p><p>在ToB端，我觉得各家公司都希望拥有自己的GPT。它们可以帮助企业利用自己的垂直数据来创建自己的大模型，或者使用类似RAG的搜索方式来创建自己的企业内部知识库。</p><p>&nbsp;</p><p>我相信，AI将在未来给SaaS带来极大的发展机会。一方面，AI可以极大地提升SaaS在需求方面的自动化程度；另一方面，以前很多定制化的问题，可能可以通过AI的方式来降低定制化的成本。</p><p>&nbsp;</p><p>然而，更多的B端应用需要模型基座的能力进一步提升，因为大模型本身还存在一些问题，比如幻觉问题，这可能需要使用一种混合方式，即在大模型的基础上叠加一些其他模型或方式来控制幻觉。但我相信，随着基座模型能力的提升，我们将能找到更好的方法来控制幻觉，开发更多的B端应用。</p><p>&nbsp;</p><p></p><h2>关于2024年的一些预测</h2><p></p><p>&nbsp;</p><p>王铁震：下面问下尤洋老师，从您的视角看，当前大模型的落地情况如何？您认为新的一年里，可以探索和应用大模型的领域有哪些？</p><p>&nbsp;</p><p>尤洋：我们接触了很多客户，他们大部分都处于种子阶段。我发现有很多场景他们认为现有的产品或大模型都做的不足。例如，一个朋友想创业，他的产品是将论文转化为PPT。这个需求非常强烈，无论在学校还是其他单位，如果能将报告迅速转化为PPT，方便与同事分享，那将非常方便。但是目前我没有找到很好的产品，他正在考虑是否要自己训练一个模型。</p><p>&nbsp;</p><p>另一方面，我也测试了一些文字转视频的软件，但效果并不理想。它们需要非常复杂的提示才能生成一个看得过去的视频。即使不提视频，我发现在生成图片方面也有很大的进步空间。我觉得AI的发展并没有那么快。ChatGPT已经出来一年了，但我感觉使用最高配的ChatGPT生成的图片效果也不是很好。</p><p>&nbsp;</p><p>我们还有很多事情可以做，至少在技术上，2024年能做的事情还是很多的，目前AI在很多领域都远远没有达到我们的预期。</p><p>&nbsp;</p><p>王铁震：两位老师预计今年大模型领域会如何发展？我们提到了大模型面临的一些问题，例如生成效果不佳和成本较高。你们认为哪些因素可能阻碍大模型的发展？什么是你认为的痛点？未来，在基础设施或大模型领域创业是否有机会？</p><p>&nbsp;</p><p>陈于思：我对2024年充满期待，可以预见到的是GPT-5的出现，以及Claude 3的发展。更多的大型企业也将在这一年崭露头角。我认为过去两三年可能是融资的年份，一些头部公司，特别是美国的，可能已经筹集了数十亿美元的资金。2024年，大家也应该开始交作业了。比如说，据说OpenAI去年他们已经实现了十五六亿美元的收入，今年他们能保持多快的增长？更强大的模型能否带来更好的效果？能否启动更多的应用？就像尤洋老师说的，我自己也经常使用ChatGPT，但它画出的图形效果并不理想。有时候，你可能需要多次修改prompt，这感觉就像炼丹。</p><p>&nbsp;</p><p>2024年，我们能否拥有更好的控制力？能否有更低的幻觉率？我期待这些变化。随着AI基础模型能力的不断提升，是否会有更好的AI 原生 APP出现？我们已经看到了一些雏形，包括Character，Inflection，以及Perplexity。那么，是否有更多的B端应用，如AGI的应用，GPT Store等，我期待这些变化。</p><p>&nbsp;</p><p>尤洋：2024年，我们必须解决任何限制Scaling Law的问题，因为我们实际上并不知道模型的上限。目前，这确实是一个实验性的工程问题，我们需要看看能否发掘出更好的基础模型。我们三人刚才初步达成了共识，在图文生产领域，我们还能做的事情很多。首先，当前效果并不理想。我已经多次提醒AI，但它似乎仍然无法画出我想要的感觉，这说明它的底层模型还不够好。其次，对于种子用户，他们并不打算训练自己的模型，而是希望使用API。在这种情况下，我们需要看极致的成本优化和效率优化是否能为他们带来更好的体验。因为开放AI现在确实有初步的营收，但如果它想进一步扩大规模，单次token的调用是否能为那些真正部署AI应用的人带来更有价值的东西？</p><p>&nbsp;</p><p>另外，我非常想看看Llama-3的效果如何，以及开源和闭源之间的差距是否会缩小。虽然现在Llama的效果很好，但我们无形中还是认为它们比ChatGPT差。如果Llama-3发布后，ChatGPT没有太大的进步，或者我们感觉不到它们之间的差距，那会是一种什么样的局面呢？我们可以拭目以待。</p><p>&nbsp;</p><p>王铁震：你们想给参与 AIGC 浪潮的年轻企业家或开发者提供一些建议呢？</p><p>&nbsp;</p><p>陈于思：我觉得可以All in。 AI是一个非常大的机会。尽管我们现在还处于AI的早期阶段，就像90年代末的互联网或2010年代的移动互联网一样，但是只要在这个行业里深耕，找到好的方向，随着行业的整体增长，个人一定能够抓住很多机会。不要过于纠结于ToB还是ToC的方向，或者担心自己的AI应用被大公司抢先。就像张一鸣在创办今日头条之前，他可能做了十几个APP，最后才发现了今日头条的价值。所以，关键是要尽快动手去做。</p><p>&nbsp;</p><p>尤洋：我非常赞同陈总的观点，AI未来将会像互联网和智能手机一样渗透到我们生活的方方面面，这是一个巨大的机会。当然，AI也会经历低潮期，就像互联网泡沫一样。但就像互联网泡沫之后出现了Google、Facebook、eBay等成功企业一样，AI行业在经历挫折后也将会诞生出更多有价值的巨头企业。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/APVI3iyPCBtHufMGFeGm</id>
            <title>贾扬清新作被某印度创始人内涵借鉴，懒得纠缠：巧了，正准备开源，GitHub 见</title>
            <link>https://www.infoq.cn/article/APVI3iyPCBtHufMGFeGm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/APVI3iyPCBtHufMGFeGm</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 05:02:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: LeptonAI, 对话式搜索引擎, Perplexity, 开源
<br>
<br>
总结: LeptonAI发布了一个基于自家服务的对话式搜索引擎demo，引起了Perplexity创始人的关注。LeptonAI创始人贾扬清回击称，他们的demo是为了分析搜索和大模型的效果，而不是借鉴Perplexity。LeptonAI最终按照承诺将该演示工具的全部代码开源。 </div>
                        <hr>
                    
                    <p></p><p>1 月 25 日，LeptonAI 发布了一个基于自家服务的小 demo，用 500 行 Python 代码实现了一个大模型加持的对话式搜索引擎。随后，号称要干掉谷歌搜索的 Perplexity 创始人声称 LeptonAI 在“借鉴”、“致敬”他们的产品。作为 LeptonAI 的创始人，贾扬清在 Twitter 上进行了公开回击。此前，LeptonAI 正打算开源该演示工具的全部代码。</p><p></p><p></p><h2>事情经过</h2><p></p><p></p><p>LeptonAI 于近日发布了一个对话式搜索引擎 demo，名为“Lepton Search”。该 demo 界面主要是一个对话框，在对话框中输入想问的问题后，Lepton Search 会根据提问，返回答案、对应来源（Sources）、相关问题（Related）。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ab/abacfd5ea9ce0fdecb8189648ef63c87.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d2/d2533e94cc25331f697661dc374d74b3.png" /></p><p></p><p>截图来源：<a href="https://search.lepton.run/">https://search.lepton.run/</a>"</p><p></p><p>LeptonAI 以此为例，向大家解释现在构建一个人工智能应用已经相当简单：这个演示程序，他们只用了不到 500 行 Python 代码，后端是一个非常快的 Mixtral-8x7b 模型，运行在 LeptonAI 自家的 playground 托管平台上，正常情况下吞吐量可高达约 200 个令牌 / 秒。该搜索引擎目前建立在 Bing 搜索 API 上，用 Lepton KV 作为无服务器存储。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed1dec92e59b60006aca5b52c380a1bb.png" /></p><p></p><p>原本是基于 LeptonAI 云平台的一个简单 demo，没想到贾扬清在 Twitter 上发布演示视频后，Perplexity 的创始人突然出现，并发文感谢 LeptonAI 向他们“致敬”：“太棒了，看到 Perplexity 成为未来融资活动的标杆，前 Meta 和阿里巴巴高管都来取经！这说明 Perplexity 的影响力不局限于产品本身，而是辐射到了整个科技生态和行业发展，令人振奋！”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/32b020b999cf2aff85b5c7ea3134353c.jpeg" /></p><p></p><p>Perplexity AI 成立于 2022 年 8 月，总部设在旧金山。Aravind Srinivas 是 Perplexity AI 的创始人兼首席执行官，2017 年从印度理工学院毕业，考入加州大学伯克利分校攻读博士学位，后来又在 OpenAI 担任过一年的研究科学家。创始团队还包括 Denis Yarats 和 Johnny Ho，均具有人工智能相关背景。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/69/696daab622265a77fa8836bac6639921.jpeg" /></p><p></p><p>截图源自 The Wall Street Journal</p><p></p><p>2022 年 9 月，Perplexity 获得 310 万美元的种子轮投资。2023 年 3 月，Perplexity 获 2560 万美元 A 轮融资。今年 1 月，再获英伟达领投的超 7000 万美元融资。</p><p></p><p>自 2023 年 12 月在亚马逊云科技 re: Invent 主题上亮相后，Perplexity 就受到了广泛关注，并得到了包括前 GitHub 首席执行官 Nat Friedman 等在内的一众大佬热捧。</p><p></p><p>Srinivas 的目标是挑战谷歌，他表示他自己是拉里·佩奇和谷歌的忠实粉丝：“我一直有做一些与谷歌同样规模和雄心的事情的冲动。”“目前看来，世界似乎对谷歌仍感到满意，他们的流量并没有实质性的变化。不过，就像谷歌和 Facebook 改变了人们获取新闻的方式一样，远离传统搜索引擎的转变最终会发生。”</p><p></p><p>Perplexity 的一众粉丝则表示 LeptonAI “借鉴”了他们的界面。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/83/8362e2f6429b24cb608ff5321bb7b55f.jpeg" /></p><p></p><p>而其他粉丝则一脸懵“这是有专利吗？人家只是演示而已。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/97/97ebc8ee170de52ba9c4f1f4d93845cc.png" /></p><p></p><p>对 Perplexity 的挑衅，贾扬清大佬罕见地进行了正面回击，“对话搜索”的领导者地位并不是来源于 Perplexity：“灵感在有一次贾扬清和微软最年轻的技术专家吴忧喝咖啡的时候，讨论 RAG 的效果究竟是源自搜索还是源自大模型，为了分析这个问题，所以自己手搭了一个 demo，同时展示 Lepton 对于 AI 创作者的效率提升。值得一提的是，吴忧是微软的搜索、对话式搜索等技术背后的核心技术领导者。”</p><p></p><p>并表示在发布这个 demo 之初已经声明要开源该演示工具的全部代码。当天下午，LeptonAI 如约将其开源，采用Apache-2.0 许可证。</p><p></p><p>开源地址如下：<a href="https://github.com/leptonai/search_with_lepton">https://github.com/leptonai/search_with_lepton</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/22/22437c529d269958d8cb28577118988e.png" /></p><p></p><p></p><h2>会话式搜索引擎原理是什么样的？</h2><p></p><p></p><p>作为一款想取代谷歌的搜索引擎，从表面看来，Perplexity 的工作原理是：当用户输入一个查询时，它会理解并重新构建这个查询，从实时索引中提取出相关链接。然后，Perplexity 将回答用户查询的任务交给 LLM，要求它阅读所有链接，并从每个链接中提取出相关段落整合内容，最终形成一段精准答案。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8d/8d8b03028045a47d3b63a08a1022b313.jpeg" /></p><p></p><p>目前，大语言模型（LLM）主要面临两大挑战：数据陈旧、偶发幻觉。由于基础模型所使用的预训练数据集具有明确的截止日期，因此无法根据最新数据做出响应。即使是当前最强大的模型，也往往会因数据过时而编造答案，也就是人们常说的“幻觉”问题。</p><p></p><p>对于无法访问最新数据，可以有两种方法，第一种是通过搜索引擎，通过执行网络搜索并向大模型提交输来改善决策质量。Perplexity AI 更依赖于这种方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b2/b278cbed1d25e2486caccfeb09a17622.png" /></p><p></p><p>第二种方法是，通过所谓检索增强生成（RAG），这项成熟技术可以解决一定程度的“幻觉”问题。与前面提到的动态调用搜索 API 方法不同，RAG 强调从公开数据存储中检索数据，例如向量数据库或者由外部维护的全文搜索索引等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d8/d836f8e0026f2f6c8505595755f971ad.png" /></p><p></p><p>通过对 Perplexity Copilot 底层技术的深入研究，还有专家称其灵感来自论文《FreshLLMs：使用搜索引擎增强更新大语言模型》（FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation）提出的搜索引擎增强大模型。</p><p></p><p>FreshLLM 提出了按搜索内容的发布日期顺序注入热门搜索摘要的想法。除了添加上下文之外，文章还建议配合少量提示词，引导大模型根据具体示例做出回应。论文作者还尝试了一种名为 FRESHPROMPT 的技术解决大模型无法回复实时问题的局限性，这项技术将来自搜索引擎的最新上下文信息注入经过预训练的大模型当中。</p><p></p><p>面对给定问题，这种方法会先在搜索引擎上查询该问题，检索全部搜索结果，包括答案框、相关结果及其他有用信息（包括知识图谱、公共问答平台上的信息，以及其他用户搜索过的相关问题等）。之后，再利用这些信息指导大模型对检索到的证据进行推理，基于多条提示词改善模型输出准确响应的能力。</p><p></p><p>Perplexity AI 底层以两套在线大语言模型为基础，同时借助内部数据承包商构建起高质量、多样化的大型训练数据集，打造了这么一套大模型搜索产品。这两套模型分别为 pplx-8b-online 和 pplx-70b-online，可以通过 API 公开访问，允许开发者将该技术整合进自己的应用程序与网站当中。</p><p></p><p>在 RAG based search 中，召回 + 排序出相关内容，然后再由模型来推理生成。在大模型同质化的年代，对于对话式搜索引擎来说，召回 + 排序才是核心竞争力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c4/c482f0d1bac0288a09840a115fa8f661.jpeg" /></p><p></p><p>而 LeptonAI，正如贾扬清所说，他们焦点在于一个帮助开发者构建人工智能应用程序的现代云平台，而不是做一个搜索引擎。那么基于此目的来通过调用已有基础架构方式构建出来的搜索引擎，其实也相对简单，所以能用不到 500 行代码来实现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2QSam7XmeLkiSDwQNKWY</id>
            <title>突发！美国拟限制中国公司使用其云数据中心训练AI模型；TikTok、英雄联盟开发商裁员；哄哄模拟器爆火 | AI周报</title>
            <link>https://www.infoq.cn/article/2QSam7XmeLkiSDwQNKWY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2QSam7XmeLkiSDwQNKWY</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Jan 2024 02:15:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国商务部新规, 云计算公司, AI模型, 裁员
<br>
<br>
总结: 美国商务部新规要求云计算公司报告外国实体使用其云数据中心训练AI模型的情况。同时，TikTok、英雄联盟开发商和谷歌等公司纷纷启动裁员。此外，苹果员工年均创收1785.3万元。蚂蚁集团成立AI“精锐部门”，前谷歌AI工程师徐鹏担任负责人。腾讯斥资64.2亿在北京海淀区购地，加强研发与办公基础设施建设。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>美国商务部新规：要求美国云计算公司报告外国实体使用其云数据中心训练AI模型的情况；TikTok、英雄联盟开发商、谷歌纷纷启动裁员；苹果员工年均创收 1785.3 万元；蚂蚁集团成立 AI“精锐部门”，前谷歌 AI 工程师徐鹏担任负责人；腾讯斥资 64.2 亿在北京海淀区购地，加强研发与办公基础设施建设；马云重回阿里巴巴最大股东地位，软银持股比例大幅下降；OpenAI 首席执行官奥尔特曼寻求数十亿美元投资，以缓解芯片短缺问题；京东重返央视春晚，豪送一亿份一分钱奖品以拓展下沉市场；华为宣布与淘宝合作，启动鸿蒙原生应用开发；谷歌发布 AI 视频大模型 Lumiere，革新视频生成技术……</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>美国商务部新规：要求美国云计算公司报告外国实体使用其云数据中心训练AI模型的情况</h4><p></p><p>路透社1月26日报道， 美国商务部长吉娜·雷蒙多周五表示，拜登政府提议要求美国云公司，确定外国实体是否正在访问美国数据中心以训练人工智能模型。</p><p></p><p>雷蒙多在接受路透社采访时表示：“我们不能让非国家行为者、中国或是我们不想让他们访问我们的云的人，来训练他们的模型。”（We can't have non-state actors or China or folks who we don’t want accessing our cloud to train their models） “我们对芯片实行出口管制，”她指出。“这些芯片位于美国云数据中心，因此我们还必须考虑关闭潜在恶意活动的途径。”</p><p></p><p>拜登政府正在采取一系列措施，阻止中国将美国技术用于人工智能，因为这一新兴行业引发了安全担忧。拟议的“了解你的客户”法规已于周五发布供公众查阅，并将于周一发布。“这是一件大事，”雷蒙多说。</p><p></p><h4>TikTok、英雄联盟开发商、谷歌纷纷启动裁员</h4><p></p><p>本周，在谷歌宣布其2024年的宏伟目标，即成为全球最强AI公司，并致力于提供最先进、最安全、最负责任的AI技术，改善知识、学习、创造力和生产力，以及构建最有用的个人计算平台和设备的同时，公司也面临着新一轮的裁员风波。据外媒报道，谷歌计划在2024年裁减更多员工，这一消息引发了广泛的关注和讨论。</p><p></p><p>资深工程师Diane Hirsh Theriault在社交媒体上公开批评谷歌管理层，她指出管理层缺乏远见，并指责裁员过程缺乏透明度和合理性，这不仅破坏了公司的知识体系，也引起了员工的普遍不满。Theriault认为，高管们试图通过裁员来简化流程，但这种做法并没有明确的愿景支持。</p><p></p><p>谷歌CEO Sundar Pichai此前在内部邮件中提到，为了确保公司能够在优先事项上进行投资，不得不做出艰难的选择。这暗示了未来可能还会有更多的裁员行动。去年，谷歌已经裁减了1.2万名员工，而今年的裁员规模虽然尚未明确，但CEO已经确认裁员将继续。</p><p></p><p>Alphabet工会计划在谷歌园区举行示威活动，以抗议公司的裁员政策。工会通讯主席Stephen McMurtry表示，裁员导致剩余员工的工作量增加，同时也加剧了员工的焦虑。工会和员工们对谷歌的这一决策表示担忧，认为这可能会对公司的长期发展和员工福祉产生负面影响。</p><p></p><p>除了谷歌，游戏开发商拳头游戏公司（Riot Games）也宣布全球裁员。作为腾讯的全资子公司，拳头游戏宣布将在全球范围内裁员约530人，占员工总数的11%。这一决定是在公司CEO的公开信中宣布的，信中强调裁员是为了公司未来的发展，并非为了短期的股东利益或季度盈利。拳头游戏表示，过去几年员工数量翻倍，但部分投资未能带来预期回报，公司需要重新聚焦于玩家最关心的领域，并减少对影响力不足项目的投入。</p><p></p><p>另外，全球短视频平台TikTok近期在美国实施了裁员措施，以降低成本。据外媒报道，此次裁员涉及约60名员工，主要集中在销售和广告部门。报道显示，TikTok裁减了约60名员工，涉及到多座城市，包括洛杉矶、纽约和奥斯汀，也有美国之外的员工。即便如此，TikTok目前在美国还有约7000名员工。</p><p></p><h4>苹果员工年均创收1785.3万元</h4><p></p><p>1月25日，市场研究机构Ondeck发布了一份报告，揭示了全球科技公司中人均创收最高的企业。报告显示，Netflix以每位员工年均创收249万美元（约合人民币1785.3万元）的业绩位居榜首。紧随其后的是苹果公司，其每位员工年均创收为234.8万美元（约合人民币1683.5万元）。Ondeck的分析师利用福布斯全球2000强企业排行榜的数据，对这些科技巨头的员工规模和年度总收入进行了分析，并将总收入平均分配到每位员工身上，从而得出了这一排名。</p><p></p><p>值得注意的是，尽管苹果公司的员工数量是Meta的两倍，但其收入却是Meta的三倍。在2023年科技公司普遍裁员的背景下，苹果公司得益于其谨慎的扩张策略，避免了大规模的裁员。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e3b5aedd565a8154c0a78f29a5c5baad.png" /></p><p></p><h4>蚂蚁集团成立AI“精锐部门”，前谷歌AI工程师徐鹏担任负责人</h4><p></p><p>蚂蚁集团近日宣布成立名为NextEvo的AI创新研发与应用部门，该部门由蚂蚁集团副总裁徐鹏领导，徐鹏曾在谷歌工作11年，负责谷歌翻译的核心技术研发，并参与了谷歌显示广告系统的算法研发。</p><p></p><p>NextEvo被视为蚂蚁集团内部的AI“精锐部门”，承担了蚂蚁AI所有核心技术研发任务，包括蚂蚁百灵大模型的研发。2023年，NextEvo发表了30余篇AI国际顶刊顶会论文，并开源了智能大规模分布式深度学习系统DLRover和GPU显存+传输优化开源项目GLake，填补了国内AI垂直领域技术开源的空白。</p><p></p><h4>壁仞总裁离职，知情人士称会继续留在算力生态行业</h4><p></p><p>据最新消息，中国GPU龙头企业壁仞科技联合创始人、总裁徐凌杰确认已离开公司。在徐凌杰给壁仞同事发送的离职邮件信中，他表示“期待在未来新的征程中能继续与壁仞的朋友们互相扶持。AGl (通用人工智能)is calling,江湖再见！”</p><p></p><p>按照知情人士的说法，徐凌杰虽然人不在壁工作，但会继续留在算力生态行业，未来可能还会与壁仞合作。</p><p></p><h4>腾讯斥资64.2亿在北京海淀区购地，加强研发与办公基础设施建设</h4><p></p><p>腾讯科技（北京）有限公司近日以64.2亿元人民币的底价成功竞得北京市海淀区学院路北端的A、B、C、J地块，该地块包括B4综合性商业金融服务业用地和B23研发设计用地。此次购地将主要用于满足腾讯对办公用地的需求，为员工提供稳定集中的办公场所。地块总面积约为70,601平方米，地上建筑规模为285,523平方米，每平方米约合9.1万元人民币。商业用地出让年限为40年，办公和公服（科研）用地为50年。</p><p></p><p>腾讯此前已在北京拥有多个办公地点，包括位于海淀区的腾讯北京总部大楼和奥林匹克森林公园的亚洲金融大厦。此次购地将进一步扩大腾讯在北京的办公空间，以适应公司不断增长的员工规模。腾讯方面表示，新地块的建设将遵循相关规划，预计在签订土地使用权出让合同后的12个月内开工建设，并在3年内竣工。</p><p></p><h4>马云重回阿里巴巴最大股东地位，软银持股比例大幅下降</h4><p></p><p>近日消息，阿里巴巴集团创始人马云在一系列增持行动后，已超越软银集团，再次成为公司的最大股东。这一变化标志着软银集团在阿里巴巴的持股比例经历了显著下降，从2022年12月的约7%降至2023年5月的不到0.5%。马云在2023年第四季度的增持行动中，购买了价值约5000万美元的阿里巴巴股票，使得其持股比例超过了2021年底的4.3%。与此同时，阿里巴巴现任董事长蔡崇信通过其家族投资平台蓝池资本管理公司，也增持了1.5亿美元的阿里股票。</p><p></p><p>阿里巴巴集团对此次股权变动表示，这不仅体现了马云和蔡崇信对公司未来发展的坚定信心，也是对公司管理团队及战略方向的有力支持。此外，阿里巴巴在2023年实施了大规模的股票回购计划，以95亿美元回购了8.979亿股普通股，进一步巩固了公司在市场中的稳定地位。</p><p></p><h4>OpenAI首席执行官奥尔特曼寻求数十亿美元投资，以缓解芯片短缺问题</h4><p></p><p>OpenAI首席执行官萨姆·奥尔特曼（Sam Altman）正积极寻求数十亿美元的投资，以应对AI行业面临的芯片短缺问题。奥尔特曼在公开场合多次表达对英伟达GPU供应不足的担忧，并表示OpenAI的GPU供应“严重受限”。为了解决这一问题，奥尔特曼正在与包括G42和日本软银集团在内的多个大型投资者讨论合作，计划筹集资金用于建立芯片制造厂或晶圆厂。</p><p></p><p>奥尔特曼的计划还包括与英特尔、台积电、三星等知名芯片制造商合作，以建立全球生产网络。此外，奥尔特曼预计将访问韩国首尔，与SK集团会长崔泰源会面，探讨AI芯片合作的可能性。SK集团旗下的SK海力士是英伟达HBM的独供商，尽管英伟达正在扩大其HBM供应商范围，SK海力士仍然是主要供应商之一。</p><p></p><h4>京东重返央视春晚，豪送一亿份一分钱奖品以拓展下沉市场</h4><p></p><p>近日消息，京东计划重返2024年央视春晚互动平台，通过App上的春晚主会场，以抽奖互动的形式送出一亿份一分钱奖品，以及包括汽车在内的实物商品。京东此举意在通过春晚这一高收视率平台，吸引并拓展下沉市场的新用户群体。</p><p></p><p>据悉，成为春晚互动平台的企业通常需要支付上亿元的赞助费用。京东此前在2022年曾作为春晚独家互动合作伙伴，投入价值15亿元的红包和商品，远超其他平台的投入。此次合作并非独家，小红书也已宣布成为春晚的笔记与直播分享平台，将在除夕直播春晚，并提供同步购买春晚舞台上展示的商品。</p><p></p><p>京东还于本月初独家冠名了“2023 - 2024 湖南卫视芒果 TV 跨年晚会”。至于今年京东为什么重返春晚，一位京东人士的解释是，2023 年是京东面临增长压力、渴求寻找更多下沉用户的一年。</p><p></p><h4>微软组建“GenAI”团队，专注开发高效小型AI模型，以减少对OpenAI的依赖</h4><p></p><p>微软近期组建了一支名为“GenAI”的新团队，该团队由公司副总裁Misha Bilenko领导，旨在开发小型AI模型，以减少对外部合作伙伴OpenAI的依赖。“GenAI”团队成员包括来自Azure的工程师以及微软研究院的AI研究人员，他们将共同致力于开发能够在移动设备上运行的轻量级模型，同时在某些任务上接近大型模型的性能。</p><p></p><p>微软此前推出的小型模型Phi-2，展示了公司在小模型领域的技术实力。Phi-2模型拥有2.7B参数，但在多项基准测试中表现出色，性能甚至超过了一些参数量更大的模型。这一成就不仅证明了微软在小模型开发上的潜力，也突显了公司在降低AI模型成本和推理需求方面的努力。随着AI技术的不断进步，小型模型因其在资源消耗和部署灵活性方面的优势，正逐渐成为行业关注的焦点。微软的这一新团队预计将在这一领域发挥重要作用。</p><p></p><h2>IT业界</h2><p></p><p></p><h4>华为宣布与淘宝合作，启动鸿蒙原生应用开发</h4><p></p><p>淘宝与华为签署鸿蒙合作，将开发基于 HarmonyOS 的鸿蒙原生应用，以增强电商领域的鸿蒙生态，并打造全场景购物体验。目前，鸿蒙生态已进入第二阶段，有 200 + 伙伴加速鸿蒙化，覆盖多个领域，鸿蒙原生应用版图成型，设备数量也迅速增长。</p><p></p><h4>AI原生应用“哄哄模拟器”24小时内吸引60万用户，探索情感沟通新趋势</h4><p></p><p>近日，一款名为“哄哄模拟器”的AI原生应用在情感沟通领域掀起热潮，仅24小时内便吸引了60万用户的关注。这款应用由开发者王登科基于个人生活经历创意开发，旨在通过模拟情侣吵架场景，帮助用户提升哄人技巧。用户在游戏中扮演一方，AI则扮演另一方，通过互动对话，系统会根据用户的回复调整“原谅值”，从而评估用户的沟通效果。</p><p></p><p>“哄哄模拟器”最初以网页版形式亮相，后推出iOS端App，提供了如“约会迟到”、“忘记纪念日”等生活化场景。产品结合了游戏化的升级和打分系统，让用户在轻松的游戏体验中学习如何化解冲突。用户还可以自定义沟通场景，进一步个性化体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/74/74bde417047eca24e4792558ee2c3454.jpeg" /></p><p></p><h4>谷歌发布AI视频大模型Lumiere，革新视频生成技术</h4><p></p><p>谷歌在人工智能领域的最新突破，AI视频大模型Lumiere，经过7个月的密集研发，现已亮相。这一模型采用了创新的时空U-Net架构，能够在一次生成过程中创建连贯且高质量的视频内容，显著提升了视频的时长和一致性，超越了现有的Gen-2和Pika模型。</p><p></p><p>Lumiere模型通过联合空间和时间下采样技术，能够生成长达5秒、80帧的全帧率视频，这在AI视频生成领域是一个重大进步。它不仅能够从文本提示生成视频，还能将静态图像转换为动态视频，甚至能够进行视频编辑和风格化生成。此外，Lumiere还能够在视频中插入或修改对象，以及生成特定艺术风格的视频。</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/582927ca3bab766c757a1127f0412d0f.gif" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kQOjfc1iCvObWXEleaGP</id>
            <title>大模型落地金融行业，如何闯关最后一公里？</title>
            <link>https://www.infoq.cn/article/kQOjfc1iCvObWXEleaGP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kQOjfc1iCvObWXEleaGP</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:42:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 金融行业, 应用现状, 问题和挑战
<br>
<br>
总结: 大模型在金融行业具有出色的数据处理和分析能力，引领着技术变革。然而，大模型的应用面临着一些问题和挑战，如技术门槛、黑盒逻辑推理过程、闭源趋势等。尽管如此，大模型仍然能够赋能金融行业，提升客户体验、解决沟通难题、改善信息获取效率，并在内部提高工作效率和数据分析能力。金融机构对大模型持有期待态度，但也希望科技公司能开发出更具业务价值和解决问题的应用。行业对大模型的认知相对较好，但仍需要更多积极合作和创新探索。 </div>
                        <hr>
                    
                    <p>AI 大模型引领千行百业加速升级。在金融行业，大模型正以其出色的数据处理和分析能力引领着一场技术变革。那么，目前大模型在金融行业的应用现状如何？大模型在金融行业的落地应用面临哪些问题和挑战？如何打通大模型在金融业落地的最后一公里？近日，InfoQ《极客有约》邀请到了腾讯金融云技术总监全成，InfoQ 社区编辑，美国 Cognizant 公司架构师（solution architect）马可薇，共话《大模型落地金融行业，如何闯关最后一公里？》。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/wV7sNBNqeO27V3p5E2Gu">https://www.infoq.cn/video/wV7sNBNqeO27V3p5E2Gu</a>"</p><p>&nbsp;</p><p>马可薇：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人 InfoQ 社区编辑、美国 Cognizant 公司架构师马可薇。本期直播，我们邀请到了腾讯金融云技术总监全成来给我们做分享。我们今天直播的主题是《大模型落地金融行业，如何闯关最后一公里？》。首先请全成与网友们打个招呼。</p><p>&nbsp;</p><p>全成：我一直在金融行业工作，负责过金融大数据、数据挖掘、机器学习、深度学习等方面工作，在算法、研发方面也有一些经验，为了探知市场的信息和动向也做过一段时间的产品和售前架构师相关的工作。目前，我在腾讯金融云项目中负责远程音视频应用，以及大模型在金融行业应用的研发工作。</p><p></p><h2>大模型如何赋能金融行业？</h2><p></p><p>&nbsp;</p><p>马可薇：去年年底 ChatGPT 的爆火成功掀起了大模型热潮，对于这波浪潮，您观察到哪些有趣的趋势？</p><p>&nbsp;</p><p>全成：从大模型至今的发展来看，我从体感上来说没有发现很多有趣的趋势，更多的是一些担忧。大模型本身具有一定的技术门槛，其本身的计算和逻辑推理过程又是黑盒，目前的大模型应用过程发展趋势更是在逐渐走向闭塞。Transformer、Hugging Face 库中的开源模型数量多、版本迭代速度快，但超大型模型却更多地走向闭源。这类黑科技一旦极端地走向闭塞、黑盒的方向，将会给大模型带来泡沫，也给了许多意见领袖左右舆论的可能性。我还是更多地希望开源和闭源的体系能同时存在，这样也更加地合理。</p><p>&nbsp;</p><p>同时我们也能看到，目前除了大模型的训练和研发平台外，许多创业公司也在进行一些基于大模型的智能应用，比如辅助办公或 PDF 和论文类的解读等等。</p><p>&nbsp;</p><p>此外，基于大模型的智能应用研发也会需要研发的平台，类似移动 APP 研发平台的中间 PaaS 层组件，这类平台的研发和开源数量也在增长，像是 LangChain 这类在早期较为典型的平台也会逐渐出现。在这一点上，科技公司或是金融公司这类对信息化同步需求较高的行业，也可以关注一下这类基于大模型的研发平台在未来的发展趋势。</p><p>&nbsp;</p><p>马可薇：在金融行业，大模型的价值主要体现在哪些层面？</p><p>&nbsp;</p><p>全成：说起价值点，如果一个技术能够解决行业中存在的一个问题，那么其价值自然能体现出来。在金融行业里，我们不提金融服务的个性化推荐、风控量化水平精准度的提高、客户体验的提高等等这些较为空泛的话题，举一个最简单的例子，智能客服。大家在日常给金融机构或客服打电话时往往能感觉到客服机器人的难以沟通。这个问题在各种领域中其实都存在，可以说金融机构并没有通过智能化的技术将客户和咨询的诉求解决掉，而是恰恰相反，通过这些技术将客户的咨询和诉求全部挡在了门外。</p><p>&nbsp;</p><p>从这方面来说，客户的咨询和互动方面肯定会有很大的提升，这也是我们能切身感受到的。此外，大家常常在很多金融 APP 上发现光是产品的购买，其中的图片和文档都会让人眼花缭乱，客户最为关注的核心信息往往很难获取到，人们与金融服务的互动或信息获取效率是过于底下的。</p><p>&nbsp;</p><p>在这些问题上，大模型对金融行业还是会有很大帮助的。金融行业中的各大渠道，包括手机银行、网银、ATM、线下网点，乃至微信的小程序中，我们与客户能产生的互动只有点击和浏览。在 ChatGPT 这些大模型出现后，我们可以看到它们在语言生成和组织回答的能力上是非常强大的，可以让金融行业中最为直观的价值点，也就是与客户的互动率会有大幅的提升。</p><p>&nbsp;</p><p>大模型在金融行业内部的工作效率提升方面，也有很大的助力。我个人在工作中也会坚持写代码，常常会用到 GitHub Copilot 或腾讯的 AI 代码助手之类，我感觉它们可以明显地为我带来代码编写的效率提升。此外，我在之前做架构师相关工作时也会遇到一类问题。金融机构中对于一些数据的统计，比如风险参股在 20%-30% 的客户情况，这些数据通常是无法在 BI 平台上获取到的。数据分析或运维人员必须要找到科技公司提交数据需求说明书，科技部还要经过排期，最终要到两个星期后才能拿到数据结果。大模型出现后，在 AI 数据和数据查询等方面也会有大幅度的提升。</p><p>&nbsp;</p><p>马可薇：当前金融机构对于大模型的态度是怎样的？</p><p>&nbsp;</p><p>全成：目前金融机构对大模型的态度普遍来说还是持有期待的。金融和电信可以说是数字化和信息化程度最高的两大行业了，他们对创新性的新科技接受程度都很高，也很愿意尝试。但大语言模型的训练和推理投入都很大，金融行业更多是期待科技公司能开发出一些可以带来更多业务价值提升，同时也能解决客观问题的应用。大型银行则更有技术实力和资本实力进行更多创新性的先行探索，我们也已经在一家大型银行的积极配合下，双方联合完成代码助手的落地等任务。</p><p>&nbsp;</p><p>马可薇：根据您的观察，目前行业对大模型的认知是否存在一些误区？</p><p>&nbsp;</p><p>全成：误区说不上，不过我个人比较喜欢 Geoffrey Moore 在《跨越鸿沟》中提到的营销模型：任何一个新技术都会有早期的受众，以及中期的保守主义和实用主义人群。我们更多是希望能和金融机构一同成为具备创新性和探索性的早期使用者，以此为后来的实用主义者给出更多的证明。当然，有些保守主义的同行会提出一些较为极端的案例，比如说互联网出现后会有手机银行这类能够直接接触核心交易方面的应用出现，那么大模型出现之后是否也能带来变革型的应用？对此我觉得要实现这些还是需要一段时间的，互联网刚出现时也是经历了网页、网站、网上银行、手机银行及线上支付的发展。总体来说，目前行业在误区层面倒没有很多，但我还是更希望金融和电信作为数字化领先的行业，能够在大模型的应用和创新的探索上有更多的积极合作趋势。</p><p></p><h2>大模型在金融行业的落地规则及部署方式</h2><p></p><p>&nbsp;</p><p>马可薇：大模型在金融行业的产业落地需要遵循哪些规则？这和其他行业有哪些异同？</p><p>&nbsp;</p><p>全成：规则的制订是为了防止打破规则后带来的损失，或者是对部分人群带来的风险。大模型作为一项新技术，就目前可见一些舆论报道等信息渠道来看，都在谈论数据隐私安全、伦理、歧视偏见，但对于金融这项非常审慎的服务来说，可解释性和可追溯性是金融机构一定要关注的规则，当然，模型输出一定要符合法律法规这项最基本的要求更不用说了。金融机构要想应用大模型，就会对科技公司或产品研发公司提出更多的要求，大模型的推理过程以及推理后的每个环节都要有相应的日志。虽然说大模型的很多输出都没有缘由，但模型运算过程中还是会有一些能够佐证的信息和日志。</p><p>&nbsp;</p><p>大模型毕竟是新技术，必然会存在很多不可预料性。但我们也不能光是嘴上说说，还是要将产品先做出来，然后在使用的过程中沉淀一些具备可操作性的规则。举例来说，腾讯安全在某个仅利用视觉进行智能驾驶的品牌中，通过安全团队的实验，发现地面上存在的某些特殊图片会导致汽车进行一些预料外的行为。这种情况如果发生在实际的高速公路上将会十分危险。因此，我们应该是进行更多的实验和探索，并在探索过程中将各种规则沉淀下来，为行业做出更多的贡献。但目前为止还没有说有人尝试过利用个人隐私数据进行指令学习的大模型 fine tune 之后，模型会输出怎样的结论并影响到客户这类的实验。</p><p>&nbsp;</p><p>马可薇：先前 ChatGPT 出现过 prompt 泄露模型训练数据的事故，那么腾讯云有针对这类风险做过什么防范措施吗？</p><p>&nbsp;</p><p>全成：这是肯定的，我们甚至还有一个专门的团队研究如何拦截大模型中高风险的输出。腾讯的混元大模型在训练的时候也会充分考虑到这一点，在训练的数据集中把法律合规、可解释性、隐私安全都考虑到。同时，因为大模型本身的计算过程非常复杂，其输出肯定会存在意外性，目前英伟达也有一些开源组件可以对模型的最终输出进行拦截。腾讯的安全实验室也在进行这方面的安全探索和研发。</p><p>&nbsp;</p><p>马可薇：要如何确保大语言模型的应用是符合法规要求的呢？</p><p>&nbsp;</p><p>全成：首先是训练数据集，我们要杜绝违法的训练数据集，因为如果存在违法的训练数据，模型必然会学习到。目前的大语言模型的训练数据通常是互联网上通用的文章、书籍、新闻报道之类信息，但金融行业还有行业内的政策文档文案以及一些领域知识，这些都需要添加到大语言模型的训练之中。这样才能更多地让模型学习到金融行业的规则和法规。</p><p>&nbsp;</p><p>其次是针对输出的拦截，在这一过程中我们可以对参数进行一些调整，实现对敏感词汇进行过滤，减少其被选取的概率，并尽可能避免人名的出现（除了百科类型中的名人）。</p><p>&nbsp;</p><p>最后一道防线则是安全机制，通过一套筛选过滤的机制从而保障大模型的输出尽可能地安全合规。虽然这样下来还是会存在漏网之鱼，但目前还没有全面观测到这种情况的出现，这方面我们还是有很长的一段路要走。</p><p>&nbsp;</p><p>马可薇：目前大模型在金融行业的应用现状如何？主要有哪些应用场景？存在哪些挑战？</p><p>&nbsp;</p><p>全成：虽然大模型面世仅仅只有一年的时间，但它的 Transformer 技术架构是从2017年底逐渐发展到现在的，真正惊艳众人的还是一千五百多亿参数的超大级别模型 ChatGPT。目前大模型虽然还没有得到大规模的应用落地，但在探索和创新性上的尝试还是有很多的。金融机构目前还是希望能和更多的科技公司合作实现大模型的应用落地。今年四五月份我们就已经能看到 ChatGPT 与 GPO 进行金融投研的 fine tune 及相应的合作，在券商或投资银行的层面上，我们能看到很多投研和投顾（投资顾问）相关的探索性合作；商业银行希望能提高与客户的互动性，智能客服和线上营销方面合作较多。至于商业银行在个人信用和企业内部评级，因为信用风控领域需要模型具备极强的可解释性，风控和风险量化方面，比如违约概率、违约损失率或风险计量等方面的应用目前还没有非常直观的应用场景。目前来说，大模型基于互联网行为数据获取到的信用评分或是代码助手之类的应用相对而言还是比较多的，但大模型目前还没有直接进入交易或模型直接决策的层面。</p><p>&nbsp;</p><p>至于挑战，大模型技术门槛不低，因此在技术上的挑战肯定存在，但更多的挑战还是在于投入产出的平衡方面。以 16B（160亿参数）的小型大语言模型来说，光是推理部分就需要一个 32G 显卡的服务器，英伟达 V100 的 GPU 一个月是四千块钱，三节点的负载均衡一个月下来就是一万二。两千亿参数的超大型模型一年在 GPU 上投入就是 144 万，在算上应用中的数据库和其他的基础设施，一年中光是推理的投入就要有三百万，因此我们还是要提高业务价值，做好投入产出的平衡。</p><p>&nbsp;</p><p>马可薇：当前金融机构部署大模型主要有哪些方式？不同方式的关键点是什么？</p><p>&nbsp;</p><p>全成：大模型的部署方式有很多，基础的云平台、本地，以及混合型部署，谷歌前阵子推出的 Gemini 甚至有可以在边缘计算部署的超小型模型，其他还有 OpenAI 通过 API 提供的模型即服务类型。</p><p>&nbsp;</p><p>具体的部署方式选择还是和大模型的应用架构息息相关。假如说是对语言能力和语言组织能力要求极高的需求场景，需要一千五百亿甚至是两千亿级别以上的模型，那么专为这种应用或云环境搭建机房的成本还是很高的。这种场景下，可以在尽可能保障安全的情况下与科技公司合作，通过 API 调用模型即服务（MaaS）共创一个安全的云环境。目前没有任何一个模型能针对任何细分场景下的任何任务都有百分百的解决率，金融机构如果希望构建大模型的应用，则必然要针对不同的细分场景，由推理和训练微调成本较低的小型模型解决细分场景，中间层 Agent 代理判断意图（究竟是调用大模型、专业模型还是搜索工具），并利用大模型的语言和推理能力将结果整理，从而交给用户一个满意的答复。</p><p>&nbsp;</p><p>马可薇：金融行业最关注的还是大模型带来的风险，具体来说，可能会存在哪些风险？如何规避这些风险？</p><p>&nbsp;</p><p>全成：这个问题拿去问 ChatGPT 能得到十几条的回答，非常全面且完善。金融行业在这十几项的风险中会更多地关注信息数据安全，隐私数据被滥用这方面。在信息安全方面，除了传统的信息加密、访问控制和脱敏等手段，我们还要有研究团队，利用个人隐私数据或行为数据做模型的微调和训练，通过模型输出反馈结果的实验证明，发现具体的潜在风险，才能提出更多的反制措施。</p><p>&nbsp;</p><p>其次，金融机构也会非常关注对客的公众性。既要确保个性化的利率定价和产品推荐，同时也要保障在某些领域中的对客公正性。在训练数据集中，即使是要用隐私或金融数据，我们也不能过度依赖银行或个人券商的内部数据，而是要考虑到公共的、更为全面的数据。内部的数据必然不是正态分布的，而有些地域或地方性金融机构的客户可能都是局限在某一区域的。</p><p>&nbsp;</p><p>第三，是模型输出的可解释性和可追溯性。在应用大模型时，金融机构要尽可能避免通过大模型直接从端到端解决非常复杂的任务。大模型在解决复杂的逻辑推理任务时还存在能力上的短板，在解决这类问题时大模型的推理过程也是完全的黑盒。因此，我们要尽可能地将复杂任务拆分，并通过这一过程中对模型的拆解，提高模型结果的可追溯性和可解释性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5f30a5d7a4d51b69dec2fac68c0f8ed.png" /></p><p></p><p>马可薇：科技公司要如何为金融机构提供定制化、差异化的大语言模型服务？</p><p>&nbsp;</p><p>全成：首先要看用户提出的需求，其次是大模型的训练，我们目前也在用很多开源的模型进行微调，这些开源模型有很多，Chinese-Llama2、千问，以及腾讯内部通过中文语料训练出的模型等等。开源模型肯定要用中文语料训练，我们会在 Hugging Face 上看有哪些评测较好的模型，再用业内比较认可的 LoRA 技术做模型的训练和微调。在金融机构内部训练数据不足的情况下，就得用更大型的生成式模型生成训练用数据构成指令数据集，再通过 LoRA 进行微调。因为可训练的参数基本可以控制在 1% 以内，所占用的显存不会很高。</p><p></p><h2>打通大模型在金融业落地的最后一公里</h2><p></p><p>&nbsp;</p><p>马可薇：大模型目前的应用和边界在哪里？</p><p>&nbsp;</p><p>全成：将一个不稳定的新技术直接应用到核心业务上会带来较大的风险，目前大模型的应用边界还是更多地在外围一些客服咨询、内部工作效率提升、文档解读等相关的应用，而不是直接应用到精确的风险量化值计算之类。</p><p>&nbsp;</p><p>马可薇：打通大模型在金融业落地的最后一公里并非易事，有哪些可行性路径？金融机构如何才能充分发挥大模型的潜力？</p><p>&nbsp;</p><p>全成：可行性路径很多，官方一些的回答是“合作通赢”；和科技公司一同合作，找到一些较为边缘性、与核心交易不算较为相关，且能相对地带来更多的工作效率的高业务价值场景，小步快跑地进行尝试，当然也还要构建一个专业的团队。</p><p>&nbsp;</p><p>不同的技术路线也有不同的落地方式。一种路径是利用金融领域数据，点对点微调大模型，这种方式成本较高，很多科技部的领导很难有魄力投入如此大的资源。另一种路线是通过 prompt 提示工程将许多大模型一同利用起来，很多具体场景中的 prompt 设计有很巧妙的设计，好的提示工程会让模型回复的精准度和准确度有大幅度的提升。</p><p>&nbsp;</p><p>这样通过知识库和大模型中 prompt 能力相结合的搭建形式，是金融机构目前来看可行性较高的路线。前面提到的复杂任务拆分，对数据外泄零容忍的细分任务用小型模型完成，利用金融机构内部的数据进行微调，再挂载到大模型的应用内，从而实现更高的落地安全性和保障性，可行性也较高，成本也更好把控。</p><p>&nbsp;</p><p>马可薇：在人才储备方面，金融机构需要哪些技能和经验来支持金融大模型的落地？</p><p>&nbsp;</p><p>全成：大模型虽然也有技术门槛，但远没有传闻中的神乎其神，实际的微调和训练工作都会需要两方面的人才。一是数据工程类人才，基于业务需求对数据集进行设计、清洗、验证，大规模地构建数据集。这一过程不仅需要具备业务知识，还需要耗费极大的人力精力。我们都知道 OpenAI 或者是谷歌等公司的架构永远是 Transformer、训练代码在 GitHub 上也有很多（并行训练、流水线训练、数据定性等等），唯独没有数据集中各种数据来源的占比，或者训练结束后指令学习中具体的任务构建。</p><p>&nbsp;</p><p>在模型训练方面，除非需要从预训练开始，将 Transformer 的架构模型重新设计或修修改改，否则只做微调的话，用 RoLA 微调开源模型就可以了。国内也有 LLaMA-Factory 的 H5 工具，很简单就能进行训练了。模型训练方面的人才首先要理解 Transformer 或图像生成方面 Diffusion 的理论基础和架构原理，会写 Python 并能利用 Hugging Face 微调应该就是足够了。</p><p>&nbsp;</p><p>马可薇：金融领域大模型的技术路线和其他垂直领域的大模型有什么不同？</p><p>&nbsp;</p><p>全成：在我看来其实差不太多，金融机构在技术路线的落地方面会更加关注私有化、本地化的部署，对产品研发和创业公司来说要更多地考虑到这一点。模型不是越大越好，越大的模型对 B 端客户的成本也就越大，模型的设计和微调以及更多的研发工作时都要考虑到这点。</p><p>&nbsp;</p><p>马可薇：对于那些还未或正在落地大模型的金融机构，您会给他们提供哪些建议？企业具体需要做好哪些准备？</p><p>&nbsp;</p><p>全成：建议谈不上，但还是希望更多的金融机构能够让员工先用起来大模型。以代码助手为例，任何技术人员肯定都觉得自己的代码水平很高，何必让 AI 给自己搞乱，但只有真正地拥抱新技术，才会在使用的时候发现它真的能带来很多便捷和效率上的提升。看似大模型没有解决什么问题，但对员工整体对大模型技术上的理解是有很大帮助的。此外，无论是技术人员还是业务人员，在应用了大模型的技术后能够天马行空地提需求才是好的循环，而不是直接否认大模型。金融机构的中高层也需要对大模型的应用有一些一两年内的规划和整体目标，这样才会更还是一些，五年计划倒是指不上，毕竟技术发展太快了。</p><p>&nbsp;</p><p>马可薇：展望未来，大模型落地金融行业还会带来哪些产业价值？未来可能会发生哪些变革？</p><p>&nbsp;</p><p>全成：金融领域在未来会积极地拥抱新技术，在可见的未来也必然会有更多创新性的应用出现，大模型会帮助金融行业实现客户和金融服务之间的互动和交付，而未来金融行业的应用也可能会出现更多的对话式、交互式应用。从变革的层面来讲，未来也必然会带来更多变革型、创新性的应用。</p><p></p><h3>嘉宾介绍</h3><p></p><p>&nbsp;</p><p>特邀主持：</p><p>&nbsp;</p><p>马可薇，InfoQ社区编辑，美国Cognizant公司架构师（solution architect），主要负责保险领域的业务。</p><p>&nbsp;</p><p>嘉宾：</p><p>&nbsp;</p><p>全成，腾讯金融云技术总监。之前曾在某金融集团公司参与关系网络风控识别项目，和说话人声纹识别比对项目；在券商互联网企业参与过大数据量化投资项目。目前在腾讯主要参与TX2SQL大模型智能应用项目，负责专业知识及推理大模型微调，构建端到端TX2SQL问答系统。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hVx7bmqULUUH1M46z8li</id>
            <title>2024 InfoQ 极客传媒合作伙伴年会即将开幕：“Q”到你了！请回答：AIGC IN ALL！</title>
            <link>https://www.infoq.cn/article/hVx7bmqULUUH1M46z8li</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hVx7bmqULUUH1M46z8li</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Jan 2024 02:17:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2023年, AI技术, 产业创新, 商业创新
<br>
<br>
总结: 2023年是一个充满变革的年份，AI技术的广泛应用带来了产业升级和经济增长，为各行各业带来了更多的创新机会。除了AI技术，云计算、音视频、数据库等其他技术领域也有了蓬勃的发展。InfoQ极客传媒作为科技媒体品牌，致力于传递最新、最具价值的技术资讯与深度洞察，得到了许多合作伙伴的支持与帮助。在合作伙伴年会上，将分享2023年的感悟与收获，并展望2024年的商业新篇章。 </div>
                        <hr>
                    
                    <p>2023 年，令人瞩目且充满变革的年份。这一年，AI 爆火之下，世界正在经历着前所未有的改变。AI 技术的广泛应用，不仅带来了产业升级和经济增长，还为各行各业带来了更多的创新机会。与此同时，InfoQ 的许多合作伙伴都敏锐地捕捉到了这一机遇，将 AI 技术融入各自的业务领域，并不断探索与实践，积极寻求新的机遇。</p><p>&nbsp;</p><p>当然，“云计算”、“音视频”、“数据库”等其他技术领域也在 2023 年有了一个蓬勃的发展，在这些领域的许多 InfoQ 合作伙伴，都在市场逆流之下完成了全新的产业创新和商业创新。</p><p>&nbsp;</p><p>在这个过程中，InfoQ极客传媒作为极客邦科技旗下的全球性科技媒体品牌，始终聚焦于前沿技术产品，致力于为合作伙伴传递最新、最具价值的技术资讯与深度洞察。提供中立、由技术实践主导的技术资讯及技术会议，搭建连接中国技术高端社区与国际主流技术社区的桥梁。2023 年，我们也深感荣幸能得到许多合作伙伴的大力支持与帮助。</p><p>&nbsp;</p><p>感恩各位合作伙伴的一路同行，在 2024 新春佳节即将到来之际，InfoQ 极客传媒合作伙伴年会时隔三年，再次回归线下，于 1 月 31 日在前门 · blue note 举办。</p><p>&nbsp;</p><p>本次活动内容将围绕“有被 Q 到”这个主题精彩展开，Q 代表着 Quality（坚守品质）、Quick（敏锐、速度和专业能力）、Question（不断求知、探索、创新），InfoQ 全体同学“Cue ”各位合作伙伴相聚一堂，共同分享这一年的感悟与收获。</p><p></p><h2>Quality/坚守品质：机会总是留给有准备的人</h2><p></p><p>&nbsp;</p><p>2023，大模型“神仙打架”。OpenAI 大模型 GPT-4 的性能在上半年遥遥领先，年末为吃瓜群众献上“宫斗大戏”；下半年，号称“史上最强”的谷歌 Gemini 模型登场，在部分基准测试中击败 GPT-4。与此同时，国内“百模大战”拉开序幕——从百度、阿里巴巴、华为、科大讯飞、腾讯、昆仑万维等国内多家企业，到清华、复旦等高校实验室，都陆续发布大模型产品。</p><p>&nbsp;</p><p>“大模型”激战正酣，“小模型”也在开辟新战场。法国初创公司 MistralAI 的模型 Mixtral 8x7B 性能比肩 GPT-3.5，且小到足以在一台电脑上运行；微软亮出小模型大招，发布 27 亿参数规模的小语言模型 Phi-2 在部分基准测试中超过谷歌的 Gemini Nano 2。此外，开源和闭源路线也在博弈，Meta 发布免费商业应用的开源 AI 模型 Llama 2，开源社区平台 Hugging Face 则提供大量高质量的开源模型与工具，将研发成果最大程度地惠及开源社区。</p><p>&nbsp;</p><p>但无论如何，大模型技术带动了国内人工智能产业链上下游企业的增长，生成式 AI 正在加速渗透各行各业。那，在这股浪潮之下，企业还能做些什么？我们应该做好哪些准备才能迎接 2024 年？InfoQ 极客传媒总经理汪丹将在年会现场进行 2024 年 InfoQ 极客传媒战略发布——InfoQ将在2024年发生的十大变化，邀请各位共同开启商业新篇章。</p><p></p><h2>Quick/敏锐、速度和专业能力：升级，并不断超越</h2><p></p><p>&nbsp;</p><p>2023 年的热闹从 ChatGPT 炸场开始，AI 技术的突破直接拉动了自动驾驶、机器人和生成式 AI 的融资增长。在这个由 AI 撕开的新风口中，扶摇而上的大有人在。数据和算力作为训练大模型的底座也受到了越来越多的关注和讨论，升级是必然，如何更好地适配 AI 能力是目前行业需要探索的重要课题。与此同时，数字化转型依旧是企业的老课题，目前已经成为企业发展的重要驱动力，它逐渐改变了企业的运营模式，重塑了整个商业生态。</p><p>&nbsp;</p><p>总而言之，随着技术的不断进步，企业需要紧跟时代步伐，积极拥抱 AI 与数字化，以实现更高效、更智能的业务运营。在这个充满变革的时代，我们需要保持敏锐的洞察力。为此，InfoQ 双数研究院将在合作伙伴年会现场将为大家详细解读 2023 年中国软件技术洞察及 2024 年趋势预测，希望有助于各位更清晰地把握市场动态，发现新的商业机会。</p><p></p><h2>Question/不断求知、探索、创新：开启新变革</h2><p></p><p>&nbsp;</p><p>AIGC 时代的到来，对技术品牌营销与市场运营提出了更高的要求。品牌需要不断刷新自身的认知边界，深入挖掘用户需求，并紧跟技术革新的步伐，将前沿科技融入产品和服务中。只有这样，才能构建出具有差异化和引领性的技术品牌形象。</p><p>&nbsp;</p><p>同时，市场运营在 AIGC 时代应更加注重数据驱动的决策。通过精准的数据分析，企业才能够洞察市场动态、用户偏好及竞争态势，从而制定更为有效的市场策略。这种以数据为核心的运营模式，不仅提升了决策的准确性和效率，还为企业带来了更大的竞争优势。</p><p>&nbsp;</p><p>InfoQ 极客传媒始终保持着敏锐的洞察力和创新精神，为合作伙伴提供更具前瞻性和实战性的思路与经验。在年会活动的开放麦环节中，我们邀请了三位行业内的重磅大咖进行闪电分享，在他们丰富的品牌与管理实战经验之上，讲述他们在各自领域探索创新的故事，为大家深入剖析如何在 AIGC 浪潮下面对激烈的市场竞争。</p><p>&nbsp;</p><p>为了感谢合作伙伴们的长期支持与贡献，年会现场我们还特别设立了奖项表彰环节，为杰出的合作伙伴颁发荣誉奖项，敬请期待这场盛大典礼，共同见证荣耀时刻！</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77e0b561847c37bce54cada362b05044.png" /></p><p></p><p>AI 打破了 2023 年的平静，科技领域的进步如一轮即将跃出地平线的太阳，我们能感受到徐徐升起的光亮和温暖。</p><p>&nbsp;</p><p>这场活动不仅是对一直以来支持我们的合作伙伴的真诚回馈，更希望各位能够通过这场活动结交更多志同道合的朋友，获取更多有价值的商业信息。1 月 31 日，就让我们共同探讨行业发展趋势，分享彼此的经验与见解，寻找未来机遇。大家现场见！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3bYEFqvNim8fG8tBhCbb</id>
            <title>云原生场景下Fluid加速AIGC工程实践</title>
            <link>https://www.infoq.cn/article/3bYEFqvNim8fG8tBhCbb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3bYEFqvNim8fG8tBhCbb</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 08:39:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, Fluid, 云原生AI, 数据和任务编排框架
<br>
<br>
总结: 阿里云高级技术专家车漾老师在QCon上海会议上分享了在Fluid项目中优化云原生AI场景下的数据和任务编排框架的工作。他介绍了简化云原生AI场景的分布式缓存管理和运维，降低资源成本的探索，以及优化推理服务读取模型数据的效率，加速模型加载过程。通过Fluid，他演示了如何将一个LLM模型的推理加载速度提升近7倍，并提供缓存弹性的能力，避免资源浪费。 </div>
                        <hr>
                    
                    <p>阿里云高级技术专家&nbsp;车漾老师在QCon上海会议上，分享了在&nbsp;Fluid&nbsp;项目作为云原生AI场景下的数据和任务编排框架，在&nbsp;AIGC&nbsp;模型推理工程化落地方面做了许多优化探索的工作，包括简化云原生&nbsp;AI&nbsp;场景的分布式缓存管理和运维，降低资源成本；以及优化推理服务读取模型数据的效率，加速模型加载过程。同时也会演示了如何通过&nbsp;Fluid&nbsp;将一个&nbsp;LLM&nbsp;模型的推理加载速度提升近7倍，同时提供缓存弹性的能力，避免资源浪费的实践话题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97407972a14e44014d7a2ae50e60bd8f.png" /></p><p></p><p>首先，大模型推理将在AI商业化时代中找到更广泛的应用，这比模型训练更具有竞争力。这是因为技术商业化是AI发展的必经道路，而模型的价值主要体现在其能否被广泛应用。我们直观理解，一个大模型只有两种可能的结局：要么它无用，之后就不会有后续；要么我们发现它非常有用，因此将被全球范围的用户所使用。如 OpenAI 和 Midjourney 等公司，用户都是在为每一次推理行为付费。随着时间的推移，我们可以预见，模型训练和模型推理的使用比重可能会达到3:7，甚至2:8。可以说，模型推理将是未来的主要战场。</p><p></p><p>然而，大模型推理带来的挑战主要表现在成本、性能和效率上，其中成本最为关键。随着模型规模的不断增大，所需要的运行资源也日益增多。而模型运行所依赖的 GPU 由于其稀缺，价格高昂，使得每一次模型推理的成本也随之上升。大模型推理的过程就像使用兰博基尼送外卖，速度越快，成本就越高。不过，用户往往只为价值买单，而不愿意为高昂的推理成本买单。因此，降低单位推理的成本成为了基础设施团队的重要任务。</p><p></p><p>此外，性能是我们的核心竞争力，特别是在面向消费者（ToC）领域的大模型中，更快的推理速度和更好的推理效果都是吸引和保持用户的关键。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48456169e572a49f3941bbb66deb4c74.png" /></p><p></p><p>随着云原生技术和架构发展，我们明显观察到IT架构的变化。传统的企业级应用、Web应用、微服务等领域都在从传统架构转向云原生架构。互联网应用大多是基于容器、Kubernetes、Prometheus等云原生技术实现的，追求弹性、灵活性以及最佳性价比。同时，通过标准化交付，提升生产流程的高效闭环。在标准API和标准架构的指导下，进一步提高多角色之间的协作和迭代效率。同样地，为了获得更多弹性算力供给、更高稳定性保证以及更快的交付，越来越多AI和大数据工作负载也运行在云原生架构上。</p><p>﻿</p><p>右图清晰地显示了这一趋势：最早是从可水平扩展的无状态应用程序（如Web应用、移动后端应用）开始；然后是NoSQL数据库、关系数据库、TensorFlow、PyTorch、Spark、Flink等大数据和典型机器学习的AI计算任务。都能够在Kubernetes容器集群上大规模运行。</p><p></p><p>三方研究机构的预测，显示出同样的趋势。早在2019年，Gartner就曾预测，到2023年，70%的人工智能应用程序将基于云原生等技术进行开发。IDC的研究则预测，到2025年，接近50%的企业内部的数据密集型或性能密集型计算工作负载都将迁移到基于云的架构上，而基于云的架构正是典型的云原生架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/108b595b2c3eaa8d66d6010acf24ed4d.png" /></p><p></p><p>在&nbsp;AIGC&nbsp;推理场景下有个关键的矛盾，就是计算存储分离的架构导致的数据访问高延迟、带宽受限问题和大模型规模不断增长的矛盾，它会同时影响成本、性能和效率。</p><p></p><p>模型弹性伸缩、按需使用，是控制大模型成本的利器。然而，如上图右所示，以&nbsp;Bloom-175B&nbsp;模型（FP16&nbsp;精度模型大小约&nbsp;340GiB）为例，模型的扩容耗时为&nbsp;82&nbsp;分钟，接近&nbsp;1&nbsp;个半小时，为了找到问题的根因，需要对模型启动时间进行拆解，其中主要耗时在于&nbsp;HPA&nbsp;弹性、创建计算资源、拉取容器镜像，加载模型。可以看到从对象存储加载一个约&nbsp;340G&nbsp;的大模型，耗时大约在&nbsp;71&nbsp;分钟，占用整体时间的&nbsp;85%，这个过程中我们其实可以看到&nbsp;I/O&nbsp;吞吐仅有几百&nbsp;MB&nbsp;每秒。</p><p></p><p>要知道在&nbsp;AWS&nbsp;上&nbsp;A100&nbsp;按量付费的价格每小时&nbsp;40&nbsp;美元，而模型启动时刻&nbsp;GPU&nbsp;其实是处于空转的时刻，这本身就是成本的浪费。同时这也影响了模型的启动性能和更新频率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80043e2bd7c9f0495d0bcf07aceb06d0.png" /></p><p></p><p>那么，我们有办法解决这个问题吗？一个直观的想法是增加一个缓存层，但是真的增加了缓存层就可以了吗？实践中其实并不是这样的，我们会遇到一系列的问题。</p><p>首先就是快的问题：能否用好缓存，如果加了缓存但是速度依旧不快，那么是缓存的规划问题？硬件配置问题？还是软件配置？网络问题？调度问题？</p><p>其次就是省：关注成本问题，作为缓存的机器通常是高带宽、大内存、本地盘的机器，这些配置的机器往往并不便宜。如何能够实现性能最大化的同时也有合理成本控制。</p><p>接着就是好：用户使用复杂不？用户代码是否需要相应的修改。运维团队工作量大吗？模型会不断更新和同步，如何降低这个缓存集群的运维成本。简化运维团队的负担。</p><p></p><p>正是在这种对于缓存工程化落地的思考中，诞生了Fluid这个项目。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/08d6e4d876d3c0edff3d360dbc43c9c1.png" /></p><p></p><p>首先，我们来探索Fluid的核心概念。Fluid在Kubernetes中负责编排数据及其计算任务，这不仅包括空间编排，还囊括时间编排。空间编排意味着计算任务将优先调度到已缓存数据的节点或近似节点上，以此提升数据密集型应用的性能。时间编排则允许我们同时提交数据操作和任务，但是在任务执行之前，我们需要执行一些数据迁移和预热操作，以确保任务在无人值守的情况下也能顺利进行，从而提升工程效率。</p><p></p><p>观察Fluid的架构图，我们可以看到Fluid可以对接各种AI/大数据应用，并且可以与各种异构存储系统进行链接。目前，Fluid已支持包括Alluxio、JuiceFS和阿里内部自研的JindoFS、EFC等多种缓存系统。</p><p></p><p>具体而言，Fluid可以提供五种核心功能：</p><p>首先它实现了标准化。为了满足针对场景的数据访问方式（比如大语言模型、自动驾驶的仿真数据以及图像识别的小文件等），Fluid提供了同样的优化数据访问方式。同时，随着越多的分布式缓存（如JuiceFS、Alluxio、JindoFS、EFC）的出现，为了在Kubernetes上使用它们，我们需要制定一种标准的API。Fluid将这些分布式缓存系统转化为有可管理、可弹性、可观测性和自我修复能力的缓存服务，并暴露Kubernetes&nbsp;API。其次是自动化。Fluid使用CRD的方式来进行数据操作，预热，数据迁移，缓存扩容等多种操作，便于用户整合到自动化运维体系中。再者是性能优化。通过针对特定场景的分布式缓存和任务缓存亲和性调度，可以显著提升数据处理性能。平台独立。无论是原生Kubernetes、边缘Kubernetes、Serverless&nbsp;Kubernetes还是Kubernetes多集群等环境，Fluid都能胜任，并根据环境需要可以通过CSI&nbsp;Plugin和sidecar来选择不同的模式运行存储客户端。最后是数据和任务编排。Fluid能支持自动化操作流程，以数据集为中心，定义数据迁移、预热和任务的执行顺序依赖关系。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77627ff6be6b53cff1737cfee6e7a35c.png" /></p><p></p><p>那么回到AIGC模型推理场景，Fluid为这个场景带来了许多优化方案。</p><p></p><p>首先，一种常见的挑战是，分布式缓存的使用复杂度高且运行环境差异大。对于AI和大数据应用来说，我们可能需要适配多种运行时，例如已经被广泛使用的Alluxio、Jindo、JuiceFS，以及最新的Dragonfly。同时，这些应用可能会在各种运行时环境中执行，包括公共云、私有云、边缘云和Serverless云。在这样的背景下，Fluid提供了一种一键部署的解决方案，使各种环境能够无缝衔接。</p><p></p><p>第二，AI和大数据模型推理服务在业务属性上具有很高的灵活性，而Fluid则通过提供弹性缓存能力，帮助用户在性能和成本之间实现最大化的权衡。具体来说，当您需要更多的缓存空间时，Fluid能够自动扩展；而在不需要时，它也能自动缩减。</p><p></p><p>第三，Fluid以数据为中心，实现了数据感知调度。这意味着，计算任务会尽可能地调度到离数据最近的地方，提升运算速度，从而极大地提高了应用的性能。</p><p></p><p>第四，Fluid提供了数据流编排能力，能够自动化处理复杂的数据消费和模型推理行为，从而降低用户在操作上的难度。</p><p></p><p>最后，在性能优化方面，Fluid拥有一种适用于云原生缓存的读取优化方案，可以充分利用节点资源，使处理速度得到进一步提升。</p><p></p><p>总的来说，Fluid是一种旨在提高运算性能、简化操作流程，并能适应多种运行环境的工具，无论您的AI或大数据应用在何种环境中运行，都能从Fluid中获益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9186f39383ad527809c5803fe7f6ecd.png" /></p><p></p><p>Fluid是一种分布式数据和计算架构，旨在提供易于使用的、一致的和高效的数据访问。它由两个主要部分构成：Dataset和Runtime，它们分别代表需要访问的数据源和对应的缓存系统。下面我们详细解析一下。</p><p></p><p>首先看到的是Dataset，它代表需要访问的数据源。在这个例子中，数据源被描述为OSS存储桶中的一个子目录，用户通过创建Dataset对象来声明他们的数据源。</p><p></p><p>接下来是Runtime，代表对应的缓存系统。在这个例子中，缓存系统是Alluxio，因此，对应的是AlluxioRuntime的CRD。CRD（Custom&nbsp;Resource&nbsp;Definition）是Kubernetes提供的一种扩展机制，用于自定义资源。</p><p></p><p>当用户创建了Dataset和对应的Runtime后，Fluid会接管后续的所有工作。首先，Fluid会自动完成对应的缓存系统的配置，然后，它会自动拉起缓存系统的组件。接着，Fluid会自动创建一个PVC（Persistent&nbsp;Volume&nbsp;Claim）。PVC是&nbsp;Kubernetes中的一种资源，它提供了对存储系统的抽象，使得用户无需知道具体的存储系统类型和参数，就可以使用存储。</p><p></p><p>当这些准备工作完成后，希望访问模型数据的推理应用只需要挂载这个PVC，就可以从缓存中读取模型数据。与此同时，因为Fluid使用的是Kubernetes标准的存储方式，所以用户可以以熟悉的方式来访问和操作数据。</p><p></p><p>可以看到，Fluid通过提供简洁明了的接口，自动化地完成了数据源的声明、缓存系统的配置和管理，极大地降低了数据访问的复杂度，让用户可以专注于他们的业务逻辑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5f8af4566c020b7cda9ff31d2da66de1.png" /></p><p></p><p>AIGC&nbsp;推理的运行平台非常多样化，包括云服务的&nbsp;Kubernetes、自建的&nbsp;Kubernetes、边缘的&nbsp;Kubernetes&nbsp;以及&nbsp;Serverless&nbsp;形态的&nbsp;Kubernetes。Serverless&nbsp;形态的&nbsp;Kubernetes&nbsp;由于其易用性、低负担的好处，已经越来越多的成为用户的选择；但是&nbsp;Serverless&nbsp;由于安全的考量，没有开放第三方存储接口，所以只支持自身存储，以阿里云为例子，只有&nbsp;NAS，OSS，CPFS&nbsp;有限存储。</p><p></p><p>在&nbsp;Serverless&nbsp;容器平台上，Fluid&nbsp;会将&nbsp;PVC&nbsp;自动转换成可以适配底层平台的&nbsp;sidecar，开放了第三方的配置接口，可以允许并且控制这个&nbsp;sidecar&nbsp;容器的生命周期，保证它在应用容器启动前运行，当应用容器结束后自动退出。这样&nbsp;Fluid&nbsp;则提供了丰富的可扩展性，可以运行多种分布式缓存引擎。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4b2e7efea1195d5cfa96e5209c2ee33d.png" /></p><p></p><p>Fluid提供的可弹性伸缩的计算侧分布式缓存，对于大规模并发，需要快速高效地读取数据的AI应用来说，尤其重要。</p><p></p><p>只依赖简单的分布式缓存，问题在于当很多服务实例需要拉取数据时，每个实例能获得的带宽将会受到限制。例如，100个推理服务实例同时启动，并需要从对象存储中拉取数据，那么，由于总的可用带宽是固定的，每个实例能份到的带宽就只有总带宽的百分之一。这可能导致数据拉取的延时显著增加，从而影响性能。</p><p></p><p>通过弹性伸缩的计算侧分布式缓存，我们将底层存储系统的有限带宽，扩展到了可以弹性扩容的Kubernetes集群内。这个集群内的可用带宽取决于分布式缓存的节点数量，从而可以根据业务需求，及时进行伸缩，提供灵活而高效的I/O处理。</p><p></p><p>测试数据也证明了弹性伸缩的计算侧分布式缓存的优势。在100个Pod并发启动的情况下，使用缓存可以显著加速，如果使用更多的缓存worker节点，效果会更好。因为大规模的缓存节点能提供更大的聚合带宽，因此每个Pod份到的带宽就越多。同样，当你使用更多的分布式缓存节点时，聚合带宽也会近线性地提升。</p><p>归结起来，弹性伸缩的计算侧分布式缓存，不仅能够灵活地应对业务的变化需求，还能根据需求提供充足的带宽，保证了AI模型推理服务的性能，这是简单部署分布式缓存所不能实现的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a3bfba9ee4364ced92af6ebc9d7c0e4.png" /></p><p></p><p>介绍完如何简单地部署缓存后，接下来考虑的问题就是如何在尽可能节省成本的前提下最大化缓存带来的性能提升，如何在成本和性能间取得平衡实质上是与业务场景的I/O访问模式相关的。Fluid在缓存上暴露的可观测性，配合手动扩缩容、HPA、CronHPA等Kubernetes的扩缩容能力，可以根据业务需求弹性扩容、缩容数据缓存。我们可以举几个具体的例子：</p><p></p><p>对于大语言模型场景，大语言模型一个特点是拥有很强的泛化知识，因此把一个LLM加载到GPU显存后，它其实可以为多种不同的场景提供服务。因此这种业务的数据I/O特点是一次性对I/O有很高的要求。对应到缓存的弹性上来，就是一个先扩容，推理服务就绪后缩容到0的过程。再来看文生图Stable&nbsp;Diffusion的场景，假如是一种SD模型市场的场景，那就会包含大量不同风格的SD模型，因此尤其对于热点模型，会有持续的I/O需求，此时保持一定的缓存副本就是更好的选择。而无论哪种场景，如果因为业务洪峰造成服务端需要扩容，数据缓存可以跟随它做临时扩容，缓解扩容时的冷启动问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec29f7ee16fc57e92ce5955b1bfec8bf.png" /></p><p></p><p>公共云的一个主要优势是其灵活的弹性能力和高可用性，这些都是通过底层的多可用区（Multiple&nbsp;Availability&nbsp;Zones）来实现的。多可用区设计可以为互联网应用带来极高的稳定性，即使牺牲了一些性能。</p><p></p><p>然而，在AIGC大模型场景中，我们发现跨可用区的延时可能会有较大影响，这是因为大模型文件往往体积较大，它传输的数据包就会非常多，这就放大了延时的影响。</p><p></p><p>因此，缓存和使用缓存的应用之间的亲和性就显得非常重要。亲和性在这里指的是Kubernetes的一种调度策略，通过设置亲和性，可以告诉Kubernetes调度器，在调度Pod时，要尽量将拥有相同亲和性的Pod调度到同一台节点上或者尽量不调度到同一台节点上。</p><p></p><p>Fluid解决这个问题的方式是提供无侵入性的亲和性调度。无侵入性的亲和性调度是根据缓存的地理位置来调度应用，优先在同一可用区内进行调度。这样就可以避免跨可用区的延时。</p><p></p><p>与此同时，Fluid也提供了弱亲和性和强亲和性的可配置性。弱亲和性是指如果条件允许，Kubernetes会尽量按照亲和性策略进行调度，但如果无法满足，则可以打破这个策略；而强亲和性则是指必须严格按照策略进行调度，不能打破。这两种配置提供了使用缓存的灵活性，可以根据不同的业务需求进行选择。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b333f61445ea171c7133420cd09c9d2.png" /></p><p></p><p>在传统的AI模型推理业务上线过程中，确实存在许多的复杂操作和耗时步骤。例如部署和扩容分布式缓存，预热模型数据以避免Cache&nbsp;Miss，启动多个服务实例，以及在确认服务无误后缩容缓存以减少成本。这些步骤需要人工介入，并在每一步之间花费大量时间来确认状态，并进行下一步操作。</p><p></p><p>为解决这一问题，Fluid将数据消费流程标准化，并通过数据操作抽象以及数据流编排能力，使得这些流程得以自动化进行。换言之，Fluid将和数据缓存相关的步骤（如数据迁移，预热，以及和业务相关的数据处理等）描述为Kubernetes级别的抽象，用户可通过这些抽象去直接描述和控制数据处理的全过程。这在大大简化操作复杂度的同时，也有效缩短了整个业务上线的时间。</p><p></p><p>总的来说，Fluid利用Kubernetes的能力，将AI模型上线的业务流程标准化，并通过提供数据操作的抽象及数据流的编排能力，使得整个流程更为高效，并减少了人工介入的程度，大大提高了业务上线的效率和误操作犯错的可能性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/549c4d9cf448538a1e3e67fad8e25df5.png" /></p><p></p><p>这些数据操作可以串联成一条数据流。于是刚才我们提到的这个例子，就可以用&nbsp;5&nbsp;步数据操作来轻松定义。运维人员只需要一次性提交这条数据流，Fluid&nbsp;会自动地完成整个&nbsp;AI&nbsp;模型推理服务发布的流程，提升使用缓存过程的自动化比例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef55451b4692b22bf65787002425777e.png" /></p><p></p><p>那么刚才提到的“用好缓存”的技巧其实都在资源成本和运维效率方面。但实际测试过程中我们发现，服务启动过程使用的带宽远小于这些GPU计算实例可用的带宽，这意味着模型的加载效率在客户端上仍然有可以优化的空间。</p><p></p><p>从节点吞吐情况上可以看到，这些AI推理的运行时框架会以单线程的方式去按序读取模型参数，这在非容器环境是没有什么问题的，如果使用本地SSD盘存储模型参数，加载吞吐很容易就可以到达3～4GB/s。但是在计算存储分离架构下，哪怕我们使用了缓存，缓存也需要使用用户态文件系统（也就是FUSE）这种技术挂载到容器中。FUSE自身的开销和额外的RPC调用，都使得read请求的延时变得更高，单线程所能达到的带宽上限也就更低了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76efeb875c251d61c615e41947d903ae.png" /></p><p></p><p>为了最大化发挥分布式缓存提供的巨大I/O吞吐，Fluid可以提供一个Python的SDK，在用户代码中使用多线程读和预读的方式去加速模型加载过程。从我们的测试结果来看，额外使用这种客户端的优化，可以在使用计算侧分布式缓存的基础上，将冷启动耗时缩短一半，做到1分钟内拉起一个接近100G的大模型。从右下角的这个I/O吞吐情况，也可以看出，我们更充分地利用了GPU计算节点的带宽资源。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b12ddf6a93ab7983674b45a3596a70d7.png" /></p><p></p><p>我们首先看一下直接访问&nbsp;OSS&nbsp;存储的运行效果。这里我们已经创建好了&nbsp;OSS&nbsp;的&nbsp;PV&nbsp;和&nbsp;PVC。</p><p></p><p>接着，我们定义一个&nbsp;deployment：deployment&nbsp;Pod&nbsp;中挂载刚才的&nbsp;OSS&nbsp;PVC，使用的容器镜像是&nbsp;TGI&nbsp;镜像。还有声明使用&nbsp;1&nbsp;张&nbsp;GPU&nbsp;卡，用于模型推理。接着把&nbsp;deployment&nbsp;创建下去。然后我们看下这个服务的就绪时间，这边&nbsp;5&nbsp;倍速加速了一下。终于就绪了，可以看到整个过程耗费了&nbsp;101s，考虑到我们的模型大小仅为&nbsp;12.55G，这个时间可以说是比较长的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7ff445877284bd454dbc025a5d197b41.png" /></p><p></p><p>最后，让我们看看&nbsp;Fluid&nbsp;的优化效果。我们需要定义&nbsp;Fluid&nbsp;的&nbsp;Dataset&nbsp;和&nbsp;Runtime&nbsp;资源，并将分布式缓存部署到集群中。定义数据源、节点个数以及缓存数据的存储介质和大小。由于我们是初次部署弹性分布式缓存，这可能需要约&nbsp;40&nbsp;秒的时间。缓存准备完成后，我们可以看到一些缓存的监控信息。PVC&nbsp;和&nbsp;PV&nbsp;也会自动创建。然后，我们定义一个新的&nbsp;deployment，只需要进行几个修改：</p><p>添加一个&nbsp;annotation&nbsp;来触发&nbsp;Fluid&nbsp;的自动数据预热将&nbsp;OSS&nbsp;的&nbsp;PVC&nbsp;更改为由&nbsp;Fluid&nbsp;Dataset&nbsp;自动创建的&nbsp;PVC替换为一个使用了客户端优化的镜像</p><p></p><p>观察服务的就绪时间，我们可以看到部署只花了&nbsp;22&nbsp;秒。我们还可以尝试对现有的&nbsp;deployment&nbsp;进行扩容，观察第二个服务实例的启动时间。由于所需的模型数据已被完全缓存，第二个服务实例只需&nbsp;10&nbsp;秒就能准备就绪。这个例子展示了&nbsp;Fluid&nbsp;优化的效果，我们成功提升了服务的启动速度约&nbsp;10&nbsp;倍。</p><p></p><p>总结：</p><p></p><p>综上，可以看到Fluid&nbsp;为&nbsp;AIGC&nbsp;模型弹性加速提供开箱即用、优化内置的方案，在达到更好性能的同时还可以降低成本，同时还包含端到端的自动化能力；在此基础上使用&nbsp;Fluid&nbsp;SDK&nbsp;可以进一步充分发挥&nbsp;GPU&nbsp;实例的带宽能力实现极致的加速效果。</p><p></p><p>【活动推荐】</p><p></p><p>目前，极客邦科技正在策划2024年6月14-15日<a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit架构师峰会</a>"深圳站会议，本次会议将围绕“智能进阶.架构重塑”大主题方向，探讨在AI浪潮下，架构设计如何匹配智能化趋势，以适应日益复杂的业务需求。会议上将讨论大模型、AI运维、数据架构、应用架构、业务架构、微服务、高可用以及数字化转型等多个层面的话题，感兴趣的可以点击阅读原文查看ArchSummit会议官网。<a href="https://archsummit.infoq.cn/2024/shenzhen/"></a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mw700OH6gDaYk9beMFft</id>
            <title>挑战Transformer霸权？ Yan 架构竟以半价成本实现百万级参数大模型</title>
            <link>https://www.infoq.cn/article/mw700OH6gDaYk9beMFft</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mw700OH6gDaYk9beMFft</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 08:07:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 岩芯数智, Yan 模型, Transformer 架构, 训练效率
<br>
<br>
总结: 岩芯数智发布了一种名为 Yan 模型的大模型，该模型采用非 Transformer 架构，具有比 Transformer 更高的训练效率。Yan 模型在处理和学习数据方面表现出色，推理吞吐量和记忆能力也超过了 Transformer。这一创新架构的发布将促进技术的快速应用和发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>1 月 24 日，岩芯数智正式发布自研大模型“Yan 模型”。Yan 模型采用非 Transformer 架构，为非 Attention 机制的通用自然语言大模型。据了解，该大模型有相较于同等参数 Transformer 的 7 倍训练效率、5 倍推理吞吐和 3 倍记忆能力。</blockquote><p></p><p></p><p>昨日，在 ROCK AI 大模型发布会上，Yan 大模型展示了其在人工智能领域的一系列创新和优势。该模型在多个方面表现出超越当前 Transformer 技术的潜力。</p><p></p><p>首先，Yan 大模型在训练效率方面显示出惊人的成绩，据称比同等参数的 Transformer 提高了 7 倍。这意味着在更短的时间内，Yan 可以处理和学习更多的数据，这对于加快 AI 模型的发展至关重要。其次，它的推理吞吐量是 Transformer 的 5 倍，这使得处理实时数据和复杂任务变得更加高效。最引人注目的是，它拥有 3 倍于 Transformer 的记忆能力，这可能为处理大规模数据集和复杂的 AI 任务提供了全新的途径。</p><p></p><p>尽管 Yan 大模型是否会开源还有待确定，但其合作者已经可以免费使用这一架构，这无疑将促进技术的快速应用和发展。值得一提的是，基于 Yan 架构，仅需投入同等规模 Transformer 架构成本的 50% 甚至更低，就可以拥有百万参数级的大模型。</p><p></p><h5>Transformer 架构的局限性</h5><p></p><p></p><p>作为当前 AI 领域的一个基石，Transformer 的设计和性能已经在各种任务中被广泛验证。Transformer 是基于注意力机制的神经网络架构，现今在人工智能领域占据主导地位。它能够有效处理序列数据，极大提高翻译、识别等任务的效果。</p><p></p><p>全球人工智能热潮的许多主要模型和产品，如 GPT、LLAMA、PaLM 等，都是基于 Transformer 构建的。其通用性显著，虽最初设计用于语言翻译，但现也推动计算机视觉、机器人学、计算生物学等领域的发展。Transformer 的核心在于快速捕捉输入内容各部分间的相互作用，适用于处理句子中的片段、音乐中的音符、图像中的像素、蛋白质的部分等各种任务。</p><p></p><p>Transformer 的概念最早出现在谷歌研究人员 2017 年的论文《Attention is All You Need》中，这篇论文在短短 5 年内被引用了 3.8 万余次。它是编码器 - 解码器模型的一个特例，2-3 年前开始流行。在此之前，注意力机制只是模型的一部分，基于 LSTM（长短期记忆）和其他 RNN（循环神经网络）变体。</p><p></p><p>Transformers 的关键见解在于，注意力可以作为推导输入和输出之间依赖关系的唯一机制。</p><p></p><p>Transformer 的突破在于其对注意力的独特运用。它使模型在处理单词时能够关注与该单词密切相关的其他单词。在《Attention is All You Need》发表前，语言 AI 领域先进技术是 RNN，它按顺序处理数据，但在表达单词间远距离依赖关系时存在局限。注意力机制使模型无视距离，考虑单词间的关系，确定哪些单词和短语更值得关注。谷歌团队的突破在于完全舍弃 RNN，仅用 Attention 进行语言建模。</p><p></p><p>注意力机制最初在计算机视觉中提出，重点关注特定区域，忽略无关图像区域。它实现了语言处理的并行化，同时分析文本中的所有单词，而非顺序分析。Transformer 的并行化带来了更全面、准确的文本理解，以及高于 RNN 的计算效率和可扩展性。现代基于 Transformer 的模型以其规模为特点，能在更大的数据集上训练，使用更多参数。</p><p></p><p>尽管 Transformer 非常强大和通用，技术领域仍在寻求更高效、先进的解决方案来应对新挑战和需求。</p><p>尽管 Transformer 模型在人工智能领域取得了显著成就，但它们存在一些局限性，这促使研究者寻找更优的模型架构。Transformer 的主要局限性包括：</p><p></p><p>参数数量庞大：Transformer 模型通常含有数百万到数十亿个参数，需要大量数据进行训练，以及昂贵的计算资源，包括高性能的 GPU 或 TPU。高昂的计算成本：标准 Transformer 模型在处理长序列时，其自注意力机制的时间和空间复杂度呈二次方增长。随着输入序列长度的增加，计算资源和时间需求成指数级增长。同时，由于参数众多和复杂的层间交互，模型在训练和推理时还需要大量内存。长序列处理困难：Transformer 架构与序列长度呈二次方关系，处理更长的序列时，内存和计算需求急剧增加，使得处理长序列变得困难。</p><p></p><p></p><h4>国内首个非 Attention 机制大模型——Yan 模型</h4><p></p><p></p><p>面对 Transformer 模型在处理大参数量、高计算成本和长序列困难方面的局限性，科技界迫切寻求更高效的解决方案。这些挑战促使岩芯数智研发团队开创性地开发了 Yan 模型，一个基于非 Attention 机制的创新架构。在 ROCK AI 大模型发布会上，刘凡平详细介绍了 Yan 模型的独特优势和技术进步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c31a73f684d89b492231b0bd4781d95.jpeg" /></p><p></p><p>他指出，Yan 架构与 OpenAI 的 GPT 系列、Meta 的 LLaMa 系列和 Google 的 PaLM 系列等基于 Transformer 架构的模型截然不同，是一种完全独立研发的新一代技术，拥有自主知识产权。</p><p></p><p>Yan 架构的主要优势在于其训练效率和资源消耗方面的显著改进。刘凡平提到，Yan 架构的训练效率是传统 Transformer 架构的 7 倍，这大大缩短了开发周期，并显著降低了成本。这对资源有限的创业公司和中小企业尤其有利。</p><p></p><p>此外，Yan 架构在保持高效能的同时，具有高推理吞吐量的特点，能够支持更多用户的同时使用。刘凡平还强调了 Yan 架构对数据隐私的重视，支持 100% 私有化部署，这对注重数据安全的企业至关重要。</p><p></p><p>他提到，Yan 架构能够在不同平台上运行，包括大型服务器和普通消费级 CPU，这增加了其在不同规模和类型企业中的应用范围。同时，Yan 在减少大模型幻觉问题方面也取得了进展，通过增强记忆能力，提高了问题回答的准确性。</p><p></p><p>在刘凡平的介绍之后，岩芯数智 CTO 杨华对 Yan 架构进行了进一步的阐释。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a25d1c1da80d787f3386df6ba77f69b9.png" /></p><p></p><p>杨华表示，Yan 架构不依赖于传统的注意力机制或 RNN 等序列模型。通过采用线性自然语言关联特征表示、特征关联函数和记忆算子，Yan 实现了计算复杂度的显著降低和特征表达能力的增强。Yan 通过多层叠加提高网络深度，优化了模型的学习和生成复杂信息特征的能力，从而在推理效率上取得显著提升，同时大幅降低了推理成本。</p><p></p><p>杨华还介绍了基于 Yan 架构的不同参数规模的语言模型，包括 13 亿、70 亿、480 亿参数量的模型，并强调了在大规模语料上的训练过程和方法。在性能对比中，Yan 在训练效率、推理吞吐量、资源消耗和记忆能力等多个维度上均优于传统 Transformer 模型。通过应用示例，如机器翻译、古诗续写和问答系统，Yan 展示了其实际运行能力，特别是在常规消费级 CPU 设备上的流畅运行能力。</p><p></p><p>随着发布会的结束，这些技术介绍和展示吸引了与会者的极大关注，引发了大家的广泛讨论。在随后的深入采访中，刘凡平表示，Yan 模型的设计旨在满足中小企业和大型企业合作伙伴的多样化需求。这一模型以其高效、灵活且成本效益高的特点，已经在多个行业中获得了广泛的关注和应用。</p><p></p><p>刘凡平强调，Yan 模型深受多个合作伙伴的青睐，这些合作伙伴参与了与模型相关的会议，并对其表现出浓厚的兴趣；对于中小型企业而言，Yan 模型提供了一种相对低成本的技术解决方案。它通过优化模型架构，不仅提高了训练和推理的效率，还降低了客户的总体项目成本。</p><p></p><p>此外，刘凡平也谈到，Yan 模型对于离线应用场景也具有重要意义。它能够在端侧运行，支持断网情况下的应用，这对于教育等领域尤为关键。在这些领域中，Yan 模型能够为用户提供不依赖于网络环境的稳定和高效服务。在金融和制造业领域，Yan 模型可以以低成本的方式提供智能客服解决方案，优化供应商管理和高效处理内部数据等，从而提升用户体验和运营效率。</p><p></p><h5>Yan 架构的潜力与挑战</h5><p></p><p></p><p>从技术介绍来看，Yan 架构无疑展示了许多潜在优势，例如其在训练效率、资源消耗、推理吞吐量以及对数据隐私的重视上的显著进步。</p><p></p><p>然而，正如历史上许多技术革新所展示的，一定程度的技术优势并不总是能够直接转化为实际应用中的成功。因此，对于 Yan 架构来说，下一步至关重要的是经受市场和行业专家的实际测试和验证。这不仅是对其技术创新的检验，也是对其在实际应用环境中可行性的考量。</p><p></p><p>我们期待看到更多来自不同背景和专业领域的专家对 Yan 架构进行深入分析和实际应用测试。进一步的，对于 Yan 架构来说，吸引和鼓励更广泛的行业参与至关重要。是否能够激发开发者、创业公司和大型企业的兴趣，将是衡量其市场潜力的关键。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL</id>
            <title>代码屎山噩梦加速来袭，都是AI生成代码的锅？</title>
            <link>https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI辅助编程工具, 代码质量, 重复代码, 代码返工
<br>
<br>
总结: AI辅助编程工具在减少程序员工作量方面发挥了重要作用，但也带来了一些问题。研究发现，使用AI辅助编程工具会导致重复代码增加和代码返工率提高。此外，AI生成的代码可能存在质量问题。因此，开发者和公司领导层都需要关注代码质量，并合理使用AI辅助工具。 </div>
                        <hr>
                    
                    <p>“周边很多程度员一直在使用，都是用上就离不开了！”知乎上，在“大家现在使用哪些AI辅助编程工具？节省了多少工作量？”话题下，答主“以默”说道。</p><p>&nbsp;</p><p>按照“以默”了解的情况，AI辅助编程工具估计至少能帮程序员减少30%的工作量。对于工具，他表示“当然首选GPT，也可能是唯一答案！国产在这方面差距很大。”“综合能力水平: 4.0&gt;3.5&gt;国产大模型。模型能力越强，越好用！”</p><p>&nbsp;</p><p>现在用AI辅助编程已经是很多程序员的选择，但随着AI软件开发迅速普及，代码质量又会随之受到怎样的影响？⻓期代码研究员 Adam Tornhill 就曾表示担忧，AI辅助编程的主要挑战在于，它非常容易生成大量本来就不应该编写的代码。</p><p>&nbsp;</p><p>根据最新研究，结果确实令人忧心。除了代码返工（即代码在添加后不久即遭删除）以外，重复代码比例升高等问题愈发严重。</p><p></p><h2>主要让“添加代码”</h2><p></p><p>&nbsp;</p><p>自2021年6月推出beta版以来，GitHub Copilot已经掀起AI编码的一波流域。据公司CEO Thomas Dohmke介绍，该软件目前拥有超100万付费订阅开发者，已经让开发任务的速度提高了55%。而且在启用Copilot的文件中，有46%的代码量是由AI生成。</p><p>&nbsp;</p><p>根据来自开发者分析公司GitClear的研究，基于从1.5亿行已更改代码中收集到的数据，调查发现其中三分之二来自以匿名方式共享数据的私营企业，三分之一则来自谷歌、Facebook及微软等技术大厂的开源项目。</p><p>&nbsp;</p><p>这项研究着眼于经过添加、更新、删除、复制及移动的代码，并排除掉GitClear预先定义的“噪音”，例如被提交至多个分支的相同代码、空行及其他无意义的代码行。</p><p>&nbsp;</p><p>但GitClear的研究将关注重点放在代码质量、而非数量上，并观察到AI助手主要是在提供“代码添加建议，但很少涉及代码的更新、移动或删除建议”。</p><p>&nbsp;</p><p>研究人员还指出，“根据奖励设计，代码建议算法更倾向于提供最可能被采纳的建议”。尽管看似有理，但这明显忽略了代码简洁、易读等特性的重要意义。</p><p><img src="https://static001.geekbang.org/infoq/d6/d67e4520e1e054e976fbe8d8bf32b403.png" /></p><p>GitClear分析得出的代码更改趋势</p><p>&nbsp;</p><p>对代码质量做精准衡量并不容易。研究人员也的确发现了一些变化趋势，表明代码的添加、删除、更新和复制/粘贴量大大提高，但代码移动比例却有所下降。他们还发现代码返工率大幅增加，从2020年的3.3%提升到目前的7.1%。</p><p>&nbsp;</p><p>一般来讲，代码移动是开发者进行代码重构的关键指标。具体来讲，就是在改进代码设计和结构的同时，确保不改变行为。</p><p>&nbsp;</p><p>研究人员初步猜测这种趋势可能与AI编码技术的日益普及相关，但真实原因仍有待验证。他们还严厉批评了大量复制/粘贴代码的负面影响，称“这种对AI生成代码的无脑使用，将对代码的长期可维护性产生灾难性的影响”。</p><p>&nbsp;</p><p>但过度使用复制/粘贴并不算是新问题。开发人员之所以这样做，很可能是因为无脑照搬比调整和重用现有代码更快、更省事，或者同一项目下多位开发者之间沟通不畅，抑或是从开发示例/编码问答网站上“抄袭”了太多内容。</p><p>&nbsp;</p><p>GitClear研究人员并没有具体讨论应如何解决调查中发现的这些问题，而是转向了“后续研究问题”。但他们也建议工程部门领导者应当“监督提交数据，并考虑其对未来产品维护造成的影响”。</p><p>&nbsp;</p><p>这次研究可能在一定程度上让那些担心被AI工具取代的开发者们感到放心。代码分析公司CodeScene最近开展的一项AI代码重构研究也得出结论，“在编码环境中，AI还远无法取代人类；当前的AI太容易出错，且完全不具备安全修改现有代码的水平。”</p><p></p><h2>代码质量，谁更应该关注</h2><p></p><p>&nbsp;</p><p>可以肯定的是，AI编码助手绝不会就此消失，反而是像一切新工具那样不断改进，并由开发者学习优化思路、改善使用效果。</p><p>&nbsp;</p><p>其实，现在开发者们也已经意识到了代码质量的问题。在GitHub 与 Wakefield Research 的调查报告中，当被调查的程序员被问到，“在积极使⽤⼈⼯智能时，应该根据哪些指标进⾏评估？”“代码质量”成为最关⼼的问题，</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/18/18784e8d5e1a34cd6205d5b389bb4bd3.png" /></p><p></p><p>但另一方面，更应该关注代码质量问题的其实是公司领导层。</p><p>&nbsp;</p><p>“我公司的领导曾经就动过用代码行数衡量每个人的工作量这种想法。 研发人员每周代码量至少在500行以上，一个月必须在2000行以上。 甚至他还搞来了第三方的测算软件，输入git账号来计算你的代码量。然后在一次技术会议上，全体组员忍无可忍的怼了技术总监。“知乎上有网友分享到。</p><p>&nbsp;</p><p>一般公司考核代码量相对简单直观，但是代码质量考核就不那么容易了：满足用户需求，</p><p>合理的进度、成本、功能关系，具备扩展性和灵活性等都不是那么可量化的指标。</p><p>&nbsp;</p><p>但<a href="https://dl.acm.org/doi/abs/10.1145/3524843.3528091">关于代码质量对业务影响的研究</a>"表明，一般来说，由于技术债务和糟糕的代码，公司平均浪费了开发人员 23%～ 42%的时间。但似乎这还不够令人感到担忧，关于<a href="https://research.chalmers.se/publication/511450/file/511450_Fulltext.pdf">软件开发人员由于技术债务而导致的生产力损失</a>"的研究还发现，开发人员经常“被迫”引入新的技术债务，因为公司一直在用代码质量换取新功能等短期收益。</p><p>&nbsp;</p><p>现在企业为“降本增效”引入AI辅助工具是可以理解的，但需要注意扬长避短、合理使用。根据Alphacodium的说法，大模型生成单个冗长函数的结果很差，代码通常包含错误或逻辑错误，大模型也往往在需要思考、推理并做出严格、重要决策的代码任务中遇到困难。</p><p>&nbsp;</p><p>代码生成与其他对话不同，它需要匹配目标语言的精确语法、识别最佳路径和边缘情况、关注问题规范中的众多小细节，并解决其他特定于代码的问题和要求。因此，在自然语言生成中许多优化和技巧可能对代码任务无效。</p><p>&nbsp;</p><p>如何让AI辅助编程更好地帮助开发者，也需要各方努力。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://devclass.com/2024/01/24/ai-assistance-is-leading-to-lower-code-quality-claim-researchers/">https://devclass.com/2024/01/24/ai-assistance-is-leading-to-lower-code-quality-claim-researchers/</a>"</p><p><a href="https://www.zhihu.com/question/640036429">https://www.zhihu.com/question/640036429</a>"</p><p><a href="https://zhuanlan.zhihu.com/p/626643788">https://zhuanlan.zhihu.com/p/626643788</a>"</p><p><a href="https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/">https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d</id>
            <title>OpenAI演讲：如何通过API将大模型集成到自己的应用程序中</title>
            <link>https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 06:38:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI API, GPT, 语言模型, 外部世界
<br>
<br>
总结: 本文讨论了如何使用OpenAI API将GPT语言模型集成到应用程序中，通过连接外部世界的API和工具来扩展GPT的功能。文章以1973年的一篇科学美国人文章为例，比较了不同动物的运动效率，引出了工具的重要性。作者指出，计算机是思维的工具，而语言模型如GPT则是人工智能思维的工具。然而，目前的语言模型存在局限性，无法与当前事件和外部世界进行连接。因此，文章介绍了如何使用GPT函数调用来解决这个问题，并通过演示样例展示了如何将GPT集成到公司产品和项目中。 </div>
                        <hr>
                    
                    <p></p><p>OpenAI的员工Sherwin Wu和Atty Eleti在QCon上讨论了如何使用OpenAI API将这些大语言模型集成到应用程序中，并通过使用API和工具将GPT连接到外部世界以扩展GPT的功能。</p><p></p><p>Atty Eleti：我想带大家回到1973年，也就是50年前。1973年，《科学美国人》（Scientific American）发表了一篇非常有趣的文章，他们在文章中比较了各种动物的运动。他们着手比较运动的效率。换句话说，一只动物从A点到B点燃烧了多少卡路里，与它们的体重等是否有关？他们比较了各种动物，鸟类、昆虫，当然还有我们人类，并将它们根据效率从高到低进行了排名。他们发现，就运动的效率而言，秃鹫的最高。</p><p>&nbsp;</p><p>秃鹫是一种美丽的鸟类，原产于加利福尼亚州和南美洲的一些地区，有时它可以飞数百英里而无需扇动翅膀。它具有非常好的滑翔能力。另一方面，人类行走，在榜单中的排名相当平庸，大约排在榜单三分之一的位置。《科学美国人》这篇文章的精妙之处在于，除了所有物种之外，他们还增加了一个项目，那就是骑自行车的人。骑自行车的人在竞争中大获全胜，击败了所有竞争对手，其运动效率几乎是秃鹫的两倍。</p><p>&nbsp;</p><p>我很喜欢这个故事，因为它有一个很简单的认识，只要用一点工具，有一点机械帮助，我们就能极大地增强我们的能力。你们中的一些人可能以前听过这个故事。你可能会想，我是在哪里看到的？这个故事是苹果公司创立之初史蒂夫·乔布斯（Steve Jobs）经常讲的。他和苹果团队利用这个故事作为早期Macintosh的灵感来源。史蒂夫比较了这个故事，并说到：“人类是工具的制造者。”</p><p>&nbsp;</p><p>我们制造了像自行车这样的工具来增强我们完成任务的能力。就像自行车是运动的工具一样，计算机也是我们思维的工具。它增强了我们的能力、创造力、想象力和生产力。事实上，史蒂夫曾经用这个神奇的短语来形容个人计算机。他说：“计算机是思维的自行车”。这篇文章发表十年后的1983年，苹果公司发布了Macintosh，并掀起了个人计算的革命。当然，多年后的今天，我们仍然每天都在使用mac电脑。</p><p>&nbsp;</p><p></p><h1>2023——人工智能和语言模型</h1><p></p><p>那是1973年。现在是2023年，50年后，计算已经发生了很大的变化。如果《科学美国人》的工作人员再次进行这项研究，我敢打赌他们会在名单上再增加一个“物种”。对我们大多数人来说，这个“物种”在公众的想象中只存在了大约六个月的时间。我谈论当然是人工智能，或者具体来说是语言模型。</p><p>&nbsp;</p><p>自去年11月ChatGPT推出以来，人工智能和语言模型已经在全球范围内引起了公众的广泛关注。更令人兴奋的是，它们吸引了世界各地开发者的想象力。我们已经看到很多人将人工智能集成到他们的应用程序中，使用语言模型来构建全新的产品，并提出与计算机交互的全新方式。自然语言交互终于成为了可能，并且质量很高。但这存在局限性，也存在问题。对于任何使用过ChatGPT的人来说，我们都知道它的训练数据是2021年9月之前的，所以它不知道当前的事件。</p><p>&nbsp;</p><p>在大多数情况下，像ChatGPT这样的语言模型是根据训练中的记忆进行操作的，因此它们与当前事件或所有API、我们每天使用的自己的应用程序和网站无关。或者，如果你在一家公司工作，它不会连接到你公司的数据库和你公司的内部知识库等等。这使得语言模型的使用受到了限制。你可以写一首诗，可以写一篇文章，可以从中得到一个很棒的笑话，可以搜索一些东西。但如何将语言模型与外部世界联系起来呢？如何增强人工智能的能力，让它来代表你执行行动，让它做比它固有能力更多的事情呢？</p><p>&nbsp;</p><p></p><h2>概述</h2><p></p><p>如果计算机是思维的自行车，那么人工智能思维的自行车是什么？这就是我们要探讨的问题：一辆人工智能思维的自行车。我们将讨论GPT，这是OpenAI开发的一组旗舰语言模型，以及如何将它们与工具或外部API和函数集成，以支持全新的应用程序。我叫Atty。是OpenAI的一名工程师。Sherwin是我的搭档，我们是OpenAI的API团队的成员，共同构建了OpenAI API和其他各种开发者产品。</p><p>&nbsp;</p><p>我们将讨论三件事。首先，我们将讨论语言模型及其局限性。我们将快速介绍它们是什么以及它们是如何工作的。先培养下对它们的直观认识。然后还要了解它们的不足之处。其次，我们将讨论我们发布的一个全新特性，即使用GPT进行函数调用。函数调用是将OpenAI的GPT模型插入外部世界并让它执行操作的方式。最后，我们将通过三个快速演示样例来演示如何使用OpenAI模型和GPT函数调用功能，并将其集成到公司产品和辅助项目中。</p><p>&nbsp;</p><p></p><h2>大语言模型（LLMs）及其局限性</h2><p></p><p>Sherwin Wu：首先，我想对LLM做一个非常高层级的概述：它们做什么，它们是什么，它们如何工作。然后再谈谈它们开箱即用的一些限制。对于那些已经关注这个领域一段时间的人来说，这可能是你们都知道的信息，但我只是想在深入讨论细节之前确保我们都能达成共识。</p><p>&nbsp;</p><p>非常高层级的GPT模型，包括ChatGPT、GPT-4、GPT-3.5-turbo，它们都是我们所说的自回归语言模型。这意味着它们是巨大的人工智能模型，它们接受过庞大的数据集的训练，包括互联网、维基百科、公共GitHub代码和其他授权材料。它们被称为自回归，因为它们所做的只是综合所有这些信息。它们接受一个prompt，或者我们可以称之为上下文。它们查看prompt。然后它们基本上只是决定，给定这个prompt，给定这个输入，下一个单词应该是什么？它实际上只是在预测下一个单词。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a281b92260f77472c4dace8f5218988.png" /></p><p></p><p>&nbsp;</p><p>例如，如果给定GPT的输入是，“the largest city in the United States is“（美国最大的城市是），那么答案就是New York City（纽约市）。它会一个字一个字地思考，它会说“New”、“York”，然后是“City”。同样，在更具对话性的环境中，如果你问它地球和太阳之间的距离是多少。GPT 已经从互联网上学过这个，它将输出9400万英里。它是根据输入逐个单词逐个单词思考的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a708f4455415a53608f886bd7cbdc35.png" /></p><p></p><p>&nbsp;</p><p>在底层，它真正做的是每次输出单词时，都会查看一堆候选单词并为它们分配概率。例如，在最初的例子中，“美国最大的城市是”，它可能有很多候选城市，New代表“纽约”（New York），或者“新泽西”（New Jersey），或者其他什么，Los代表“洛杉矶”（Los Angeles），然后还有其他一些可能的例子。你可以看到，它确实认为“New York City”（纽约市）可能是正确的答案，因为New的概率为95%。在这种情况下，它通常会选择最有可能的结果，所以它会选择New，然后继续前进。这个单词出现后，我们现在就知道New是第一个单词，所以它对下一个单词是什么就有了更多的限制。</p><p>&nbsp;</p><p>我们可以看到，现在它认为New York（纽约）的可能性要高得多，但它也在考虑New Brunswick（新不伦瑞克）、New Mexico（新墨西哥）和New Delhi（新德里）等。直到完成第二个单词，这基本上是模型的叠加。它基本上知道答案是New York City，概率几乎是100%。但它仍在考虑其他一些剩余概率很低的选项，比如County（县）、New York Metro（纽约地铁）、New York Times（纽约时报），但最终它选择了City并给出答案。</p><p>&nbsp;</p><p>对于更机敏的LLM人士来说，这在技术上过于简单化了。我们并不是真正在预测单词，而是在预测token，比如单词片段，这实际上是一种更有效的表达英语的方式，主要是因为单词片段会在一堆不同的单词中重复，而不是单词本身会重复。但概念仍然是一样的。LLM在这种上下文中，很可能会连续输出一堆不同的token。就是这样，这就是这些语言模型的真正含义。了解了这一点，我认为让我们很多人感到惊讶的疯狂之处在于，我们只需预测下一个单词就可以走得很远。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/5673c0fe7089d2faab377baf358f7d0d.png" /></p><p></p><p>&nbsp;</p><p>这张图表来自我们今年3月发布的GPT-4博客文章，它显示了我们最有能力的模型GPT-4在各种专业考试中的表现。这实际上只是GPT-4根据问题预测下一个单词。你可以看到，在很多不同的考试中，它的表现实际上和人类一样，甚至超过了人类的表现。y轴是考生的百分位数。在AP考试、GRE考试、LSAT考试、美国生物奥林匹克竞赛等一系列不同的考试中，它基本上处于第80个百分位，有时甚至是第90个百分位，甚至是第100个百分位。</p><p>&nbsp;</p><p>在这一点上，很多这样的测试我甚至都做不到，所以GPT-4远远超出了我自己的能力，而这只是来自对下一个单词的预测。这真的太酷了。你可以用它构建很多很酷的东西。任何一个已经学习了LLM一段时间的人都会意识到，我们很快就会遇到一些限制。当然，最大的一个是开箱即用的LLM或GPT实际上是一个装在盒子里的人工智能。它无法进入外部世界。它不知道任何其他信息。它就在那里，有它自己的记忆。感觉就像你在学校里参加考试时，只有你和考试，你只能根据记忆来回忆一些东西。</p><p>&nbsp;</p><p>想象一下，如果考试是开放的，你可以使用手机或类似的东西，你会做得更好。GPT今天真的只是在它自己的盒子里。正因为如此，作为工程师，我们希望使用GPT并将其集成到我们的系统中。限制GPT，不允许它与我们的内部系统对话，这对于你可能想做的事情来说是非常有限的。此外，即使它确实可以访问某些工具，因为语言模型是概率性的，有时也很难保证模型与外部工具交互的方式。如果你有一个API或其他你想要使用的东西，当前模型不能保证总是能与你API可能想要的输入相匹配时，这最终也会成为一个问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2580bcf915d852712ba15dc55c330e14.png" /></p><p></p><p>例如，如果我正在构建一个应用程序，并将此输入提供给GPT，基本上就是说，下面是一个剧本的文本，从中提取一些信息，并以这种JSON格式对其进行结构化。我真的只是给它一个剧本，让它推断出一种类型和一个子类型，以及其中的一些角色和年龄范围。我真正想要的是，我希望它能输出像这样的东西。就像JSON输出一样。</p><p>&nbsp;</p><p>也许这是一个关于哈利波特的浪漫故事之类的剧本。它知道这是浪漫的，青少年的浪漫，它看到罗恩（Ron）和赫敏（Hermione），并以这种JSON格式准确输出。这太棒了，因为我可以获取这个输出，现在我可以使用它并将其放入API中。然后我就像在我的代码中一样，一切都正常。问题是，它大概只有80%、70%的概率是这样的。</p><p>&nbsp;</p><p>在剩下的时间里，它会尝试并提供额外的帮助，做一些像这样的事情，它会说：“当然，我可以为你做。下面是你要求的JSON格式的信息。”这是非常有用的，但如果你试图将其插入到API中，它实际上室不起作用的，因为前面所有这些随机文本，你的API并不知道如何解析它。这显然是非常令人失望的。这不是你真正想要的。我们真正想做的是，帮助GPT打破常规，或者给GPT一辆自行车或另一套工具来真正增强它的能力，并让它无缝地工作。</p><p>&nbsp;</p><p></p><h2>使用GPT进行调用函数</h2><p></p><p>这就把我们带到了下一部分，那就是我们所说的GPT函数调用，这是我们发布的API的一个新变化，它使函数调用能够以一种非常一流的方式更好地使用我们的GPT模型。举个例子，如果你问GPT这样的问题，what's the weather like in Brooklyn today? （今天布鲁克林的天气怎么样？）如果你问一个普通的GPT这个问题，它基本上会说，“作为一个由OpenAI训练的人工智能模型，我无法提供实时信息。”这是真的，因为它实际上无法访问任何东西。它在一个盒子里。它怎么会知道今天天气怎么样呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76c19cc7f6ed1d6b67c222e0be0fc12f.png" /></p><p></p><p>&nbsp;</p><p>这显然确实限制了它的能力，这是不可取的。我们所做的是更新了GPT-4和gpt-3.5-turbo模型或旗舰模型。我们收集了大量的工具使用和函数调用数据，根据这些数据对我们的模型进行了微调，使其真正擅长选择是否使用工具。最终的结果是我们发布了一组新的模型，这些模型现在可以为你智能地使用工具和调用函数。在这个特殊的例子中，当我们询问模型“今天布鲁克林的天气怎么样？”时，我现在能做的就是解析这个输入，同时告诉它一组函数，或者在本例中，告诉它它可以访问的一个函数，如果需要帮助，它应该尝试并调用这个函数。在本例中，我们将为它提供一个名为get_current_filther的函数。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43e2a8a9cd6b458f9047cdbaa76a12a1.png" /></p><p></p><p>它接收一个带有location（位置）的字符串，然后它就知道它可以使用这个。在本例中，在这个新的世界里，当你解析此输入时，GPT将表达它打算调用get_current_filther函数的意图。然后，你可以根据需要在自己的系统中自行调用该函数。假设你得到的输出是 “22 Celsius and Sunny”（22摄氏度和阳光明媚）。你可以将其解析回GPT，它会综合这些信息，并返回给用户说：the weather in Brooklyn is currently sunny, with a temperature of 22 degrees Celsius（目前布鲁克林天气晴朗，温度为22摄氏度。）</p><p>&nbsp;</p><p>稍微解释一下，真正发生的事情是GPT知道一组函数，并且它会智能地自行表达调用其中某个函数的意图。然后执行调用，并将其解析回GPT。这就是我们最终将它与外界联系起来的方式。为了进一步了解它在高层级上到底发生了什么，其实它仍然就像是一个来回，你的用户问了一个问题，发生了很多事情后，你对你的用户做出了回应。你的应用程序在底层实际做的事情将经历一个三步的过程，首先调用 OpenAI，然后使用你自己的函数，最后再次调用OpenAI或GPT。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de2e19a01388973a0dc7a6ca59fe9d6a.png" /></p><p></p><p>&nbsp;</p><p>第一步，显然是用户问了一个问题，在本例中，问题是what's the weather like in Brooklyn today?（“今天布鲁克林的天气怎么样？”）然后下一步是，在应用程序中，调用模型，调用OpenAPI，并非常具体地告诉它它可以访问的函数集以及用户输入。这是一个API请求的例子，目前它实际有效且可正常工作，任何具有API访问权限的人都可以尝试该操作。这是一个使用函数调用能力的curl示例。我们可以看到，这只是我们聊天完成端点的正常curl，这是我们发布的一个新的API端点，为我们的GPT-4和GPT-3.5模型提供支持。你curl该API。它会在模型中进行解析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a02129b0f8e597f1fc1f439cca209731.png" /></p><p></p><p>在本例中，我们将在gpt-3.5-turbo-0613中进行解析，它代表6月13日，一个我们发布的模型。这是一个能够进行函数调用的模型。我们还在解析一组消息。对于那些可能不熟悉我们聊天完成格式的人，你可以将其解析到我们的模型中，基本上是一个消息列表，也就是对话记录。</p><p>在本例中，实际上只有一条消息，没有历史记录。它只是用户询问“今天布鲁克林的天气怎么样”。你可以想象，随着对话的变长，它可能是一个包含5到10条消息的列表。我们正在解析消息，模型将能够看到历史记录并对此做出回应。那么，这里的新事物就是函数。</p><p>&nbsp;</p><p>这是一个我们现在可以解析的新参数，我们在这里解析的是，我们列出了这个模型应该知道的一组函数，它应该可以访问的函数集。在本例中，我们只有一个函数，它就是get_current_tweather函数。我们在这里还放了一个自然语言描述。我们说这个函数可以获取特定位置的当前天气。我们还需要输入函数签名。并且我们告诉它有两个参数。一个参数是location（位置），这是一个字符串，包含城市和州，格式是这样的：旧金山，加州（San Francisco, California.）。另一个参数时unit（单位），即摄氏度（Celsius）或华氏度（Fahrenheit）。</p><p>&nbsp;</p><p>在这里首屏的下面，还有另一个参数，该参数表示唯一必须的属性是位置。从技术上讲，你只需要解析位置，这里不需要单位。我们将该请求解析到GPT，然后GPT将作出响应。在过去中，GPT可能只会以文本形式进行响应。它会说：“我不能这样做，因为我没有访问权限。”在本例中，我们的API响应的是调用天气函数的意图。</p><p></p><p><img src="https://static001.geekbang.org/infoq/23/2339e79558c3eda24de50f9a9e11bcd4.png" /></p><p></p><p>&nbsp;</p><p>这里真正发生的事情是GPT凭自己的直觉，为了弄清楚今天的天气，我自己做不到，但我可以访问get_current_weither这个函数，所以我会选择调用它，所以我要表达要调用它的意图。此外，如果你还没有真正注意到的话，GPT在这里所做的是，它在这里构造参数。我们可以看到它在告诉我们，它想调用get_current_tweather，它想用参数位置（Brooklyn, New York；纽约布鲁克林）来调用该函数。</p><p>&nbsp;</p><p>它所做的就是看到函数签名，并为其创建请求。然后还算出布鲁克林在纽约，然后用这种方式构造字符串。它把这一切都弄清楚了。至此，GPT就表达了现在要调用函数的意图。下一步是，我们要弄清楚我们到底想要如何调用这个函数。我们可以根据特定参数从get_current_tweather的函数调用中获取相应的返回值。然后我们可以自己执行。它可以是本地的，在我们自己的Web服务器上运行。它也可以是系统中的另一个API，还可能是一个外部API，我们可以调用weather.com API。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57bc52645179b986fb8dc2fe597ac591.png" /></p><p></p><p>那么在这个例子中，我们调用了一些东西，可能是一个内部API，它返回的输出是我们看到的是22 degrees Celsius and Sunny（22摄氏度和晴天）。给定了模型的输出，就可以开始这个过程中的第三步，即调用模型，用函数的输出调用GPT，然后查看GPT想要做什么。在本例中，我谈论的是消息。这次，我们在向OpenAI API发送的第二个请求中添加了几条消息。最初，只有一条信息，那就是“今天布鲁克林的天气怎么样？”，现在再添加两条新消息来表示函数调用时所发生的情况。</p><p>&nbsp;</p><p>第一个基本上是对意图的重申，所以基本上是说助理或GPT想要用纽约布鲁克林的这个参数来调用get_current_tweather函数。然后，我们还添加了第三条消息，它基本上说明了我们所进行的函数调用的结果，因此这是get_current_filther的结果。然后，内联这里输出的数据，即温度“22”、单位“摄氏度”和描述“晴天”，然后将所有数据解析给GPT。在此时，GPT接收了它，并决定它想要做什么。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52c47226a60424b8e67d5dfe8869db8e.png" /></p><p></p><p>此时，模型已经足够智能了，它能够意识到“我将调用这个函数。这是输出。我实际上已经掌握了实际完成请求所需的所有信息。”它现在最终会通过文本方式来做出回应，并显示“今天布鲁克林天气晴朗，温度为22摄氏度”。这时，我们终于得到了GPT的最终输出。然后我们就可以回应我们的用户了。</p><p>&nbsp;</p><p>将所有这些放在一起，我们最终会得到我们理想中的体验，即用户询问“今天布鲁克林的天气怎么样？”我们的服务器会思考一下，GPT表达意图，我们完成完整的三步过程，调用了我们的函数。最终，用户看到的是“今天布鲁克林天气晴朗，气温为22摄氏度。成功”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>演示1——将自然语言转换为查询</h2><p></p><p>Eleti：我们刚刚介绍了几个入门性的主题。首先，我们了解了语言模型是如何工作的，以及它们的一些局限性，因为它们没有所有的训练数据，它们没有连接到外部世界，它们的结构化输出并不总是可解析的。Sherwin还向我们介绍了新特性、函数调用和API的工作原理，以及如何将函数解析为API并获取输出，以及如何让GPT以面向用户的方式来总结响应。让我们通过几个演示来了解如何将所有这些组合起来，并将其应用到我们的产品和应用程序中。</p><p>&nbsp;</p><p>让我们从小事做起。我们将介绍的第一个示例是将自然语言转换为查询的内容。我们的示例是，假设你正在构建一个数据分析应用程序或商业智能工具，比如Tableau或Looker。你们中的一些人可能很擅长SQL，但我肯定不擅长了。大多数情况下，我只想问数据库，谁是顶级用户，然后得到响应。今天终于有可能了。我们将使用GPT，将给它一个称为SQL查询的函数，它只需要一个参数，即一个字符串“query”。</p><p>&nbsp;</p><p>它应该是针对我们数据库的一个有效SQL字符串。让我们看看它是如何工作的。首先，我们将为模型提供一条系统消息，描述它应该做什么。我们称之为SQL GPT，可以将自然语言查询转换为SQL。当然，模型需要访问数据库模式。在本例中，我们有两个表，用户表（users）和订单表（orders）。用户表有姓名、电子邮件和生日。订单表有用户ID、购买金额和购买日期。现在我们可以开始使用一些自然语言来查询数据库了。</p><p>&nbsp;</p><p>我们来问这样一个问题“根据上周的消费金额，找出排名前10的用户姓名”（get me the names of the top 10 users by amount spent over the last week.）。这是一个相当正常的业务问题，当然不是我可以立即编写SQL就能解决的问题，但GPT可以。让我们运行一下。我们可以看到它正在调用SQL查询函数。它有一个参数“query”，它创建了一个漂亮的SQL查询。它是选择了名称和金额的总和；它连接到订单表；并获取最后一周的订单，按总花费进行排序，并将其限制为10个。这看起来是正确且恰当的。让我们在数据库中运行一下它。我们得到了一些结果。</p><p>&nbsp;</p><p>当然，这是JSON格式的，因此用户无法渲染它。让我们把它发送回GPT看看它说了什么。GPT总结了这些信息，并表示“这些是按消费金额排名前十的用户。这是他们上周的花费，包括Global Enterprises, Vantage Partners。”这是一个了不起的用户可读的答案。</p><p>&nbsp;</p><p>我们要对GPT给予的帮助表示感谢。我们说“谢谢”，GPT说“不客气”。这是一种快速的方法，它可以了解完全的自然语言、完全的自然语言查询是如何将结构化输出转换为有效的SQL语句的，我们在数据库中运行该语句，获取数据，并将其汇总回自然语言。我们当然可以在此基础上构建数据分析应用程序。</p><p>&nbsp;</p><p>你还可以构建其他的内部工具。Honeycomb最近为Honeycomb查询语言构建了一个非常相似的工具。这是使用GPT和函数将自然语言转换为查询的一个示例。</p><p>&nbsp;</p><p></p><h2>演示2——调用外部API和多个函数</h2><p></p><p>让我们来做第二个演示。这是关于将外部API和多个函数一起调用的。我们提高了复杂度。假设我们正在纽约参加一个会议，我们想预订今晚的晚餐。我们将使用两个函数来调用GPT。第一个是get_current_location。它在设备上本地运行，比如在你的手机或浏览器上，并获取你所在位置的纬度（Lat）和经度（Long）。第二个函数是Yelp搜索，它使用Yelp的API，也就是流行餐厅评价应用程序，我们可以对纬度、经度和查询进行解析。</p><p>&nbsp;</p><p>我们来运行一下这个演示。本例中的系统消息相当简单。它所说的就是我们的私人助理，来帮助用户完成任务，把GPT变成了一个有用的助手。我说“我正在参加一个会议，想在附近吃晚饭，有什么选择吗？我的公司会支付这笔费用，这样我们就可以尽情享受了”。让我们用GPT来运行一下它，看看它是如何做的。</p><p>&nbsp;</p><p>当然，GPT不知道我们在哪里，所以它说get_current_location，我们将调用本地API来获取我们的纬度和经度。我们已经获取到了。是纽约的布鲁克林（Brooklyn, New York）的某个地方。我们会将其返回给GPT，看它怎么说。它已经有了所需的信息，现在它想调用Yelp，它说“纬度、经度和查询”，并且会说“美食”。这很好。这就是我想要的。让我们调用Yelp并获取一些数据。</p><p>&nbsp;</p><p>我们从Yelp API中获取了一堆餐馆。当然，我希望它能给出一个漂亮的总结，所以让我们再次运行它。它回复说“你附近有一些高档餐饮可选择，La Vara、Henry's End、Colonie、Estuary”。上面还写着“请检查营业时间，尽情用餐。”这听起来很美味。再次感谢GPT帮助我们组织今晚的晚宴。</p><p>&nbsp;</p><p>这是一个使用GPT和函数调用外部API（在本例中为Yelp API）以及协调多个函数的示例。它能够凭借推理能力解析用户意图，并依次执行多个步骤的操作，以实现最终目标。</p><p>&nbsp;</p><p></p><h2>演示3——将高级推理与日常任务相结合</h2><p></p><p>第三个演示，让我们来进一步加强。我们讨论了GPT-4是如何通过SAT和GRE的。如果可以的话，它一定比仅仅调用Yelp API或编写一些SQL更聪明。让我们来测试一下。我们都是工程师，我们每天都有很多事情要做。我们必须要做的任务之一是拉取请求审查。我们必须审查同事的代码。如果GPT能帮助我，减轻我的工作量，那就太棒了。我们将做一个GPT的演示，它可以进行拉取请求审查，有点像构建自己的工程师。</p><p>&nbsp;</p><p>我们只需要一个函数submit_comments。它接受一些代码并返回一个要审查的评论列表，包括行、数字和评论。你可以想象，我们可以将其发送到GitHub API或GitLab API，并发布一堆评论。当然，你还可以添加更多的功能以使其更强大。让我们看看它是如何做的。</p><p>&nbsp;</p><p>在本例中，prompt有点长。我们向上滚动着看下。我们说：“GPT，你记录、审查rot，查看其差异并生成有关更改代码的审查评论，保留所有代码审查评论和相应的行号。”我们在这里也卖弄下个性。我们说toxicity为10分之0，其实我们不希望这样。</p><p>&nbsp;</p><p>为了好玩，让我们在snark上尝试10分之8。我们都认识一些表现出这些个性的工程师。然后尝试10分之2。让我们从这里开始吧。下面是一些我们要审查的代码。它是SaaS应用程序中的一个API方法，用于更改用户的权限。让我们运行一下它。我们看看GPT对这些代码有何看法。它给出了三条审查意见。我们可以看到它调用了submit_comments函数，并且它输出了完全有效的JSON。让我们看看上面写着什么。它说，“我们现在是在捉迷藏吗？”，“当角色不在身体里时会发生什么？”，“你在那里添加一个了小转折，你就直接访问了第一项。”</p><p>&nbsp;</p><p>我们只是随意地加入了数据库会话，是吗？这有点粗鲁。我们也不想那样。让我们来解决一下这个问题。我现在要退出并稍微修改一下prompt。要执行该操作，请退出。在幕后，我所做的就是返回prompt并更改这些的数字：toxicity，然后下一个，snark，我们将其恢复到0。我们并不希望这样。让我们礼貌一点。</p><p>&nbsp;</p><p>我们要把礼貌做到十分之十。好吧，再给我三条审查意见。它再次使用完全有效的JSON调用该函数。它说，“很高兴看到你检索角色值。”；“你的错误信息简洁明了。”；“我很感激你对数据库的更改，做得很好。”。我希望有人能这样审查我的代码。感谢GPT，我将退出了。这是第三个快速演示。</p><p>&nbsp;</p><p>从本质上讲，它仍然在做同样的事情。它调用一个函数，给出一些prompt，并对其做出响应。我们看到的是GPT的推理能力。GPT认识代码。它已经看到了成千上万行代码，可以给出很好的评价。如果你抛开一些个性的东西，它会指出错别字，指出潜在的错误案例和边缘案例。我们在这里将高级推理与日常任务相结合。它确实非常擅长编码。它在考试方面也确实非常出色，它的智力应用范围也很广。这实际上取决于开发人员的创造力，将其应用于尽可能困难的任务，并在此基础上循环运行。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>这是本次内容的快速总结。我们讨论了三件事。首先，我们讨论了LLM及其局限性。我们了解了LLM是如何工作的，它是token预测机。我们了解了它的局限性。它被时间限制住了。它并不总是输出结构化的输出等等。其次，我们了解了这个新特性，即使用GPT进行函数调用，这是对我们API和模型的更新。它允许模型表达何时调用函数的意图，并为我们构建有效的参数，然后在我们的终端上调用该函数。最后，我们浏览了一些演示。在某个时候，我会把公关的东西产品化。</p><p>&nbsp;</p><p>让我们回到开始的地方。我们谈到了史蒂夫·乔布斯的名言，他说“计算机是思维的自行车”。这对我来说确实如此，对你们所有人来说也都是如此。我们身处计算机行业，计算机改变了我们的生活。计算机增强了我们与生俱来的能力，给了我们更多的生产力、想象力和创造力。ChatGPT中的人工智能和语言模型还是个婴儿。它才出生几个月。我们有责任增强人工智能的思维，赋予它超越其内在推理能力的新能力，将其与工具连接，与API连接，并利用这一特性开发出真正令人兴奋的应用程序。</p><p>&nbsp;</p><p>原话对我来说非常有启发。我们永远无法公正地对待史蒂夫·乔布斯的名言。“我记得在我大约12岁的时候读过一篇文章，我想可能是在《科学美国人》上，他们在文章中测量了地球上所有这些物种的运动效率，它们从A点到B点需要消耗多少千卡热量。秃鹫赢了，位居榜首，超过了其他所有物种。人类排在榜单大约三分之一的位置，这对创造之冠来说并不是一个很好的表现。在那里有人有足够的想象力来测试人类骑自行车的效率。一个骑自行车的人把秃鹫吹走了，一直高居榜首。这给我留下了非常深刻的印象，我们人类是工具的制造者，我们可以制造出将这些固有能力放大到惊人程度的工具。对我来说，计算机一直是思维的自行车，它让我们远远超越了固有的能力。我认为我们只是处于这个工具的早期阶段，非常早期的阶段。我们只走了很短的一段距离，它仍处于形成阶段，但我们已经看到了巨大的变化。我认为，与未来100年发生的事情相比，这算不了什么。”</p><p>&nbsp;</p><p>就像50年前的计算机一样，我认为今天的人工智能也是如此。技术还处于起步阶段，所以我们很高兴看到它的发展。</p><p>&nbsp;</p><p></p><h1>问答</h1><p></p><p></p><h2>应对错误和失败的策略</h2><p></p><p>参会者1：我们应该如何应对错误和失败，你有什么建议的策略？以你的演示为例，在你构建SQL查询时，如果我提出的问题导致ChatGPT给出了一个在语法上完成正确，但在语义上完全不正确的SQL查询时，该怎么办？然后我向我的用户报告一些不正确的内容。很难告诉用户，这是错误的，但你有什么建议的策略来应对这个问题吗？</p><p></p><h1>&nbsp;</h1><p></p><p>Eleti：我认为首先，作为一个社会和这些语言模型的用户，我们必须了解它的局限性，几乎要围绕它的局限性来建立抗体。要知道输出可能是不准确的。我认为第二部分就像打开了盒子。我们已经将生产中的函数调用与ChatGPT集成在了一起。我们推出了一款名为插件的产品，它基本上可以做到这一点，它允许ChatGPT与互联网对话。我们要做的一件事是，如果最终用户愿意的话，那么所有的请求和响应都是可见的。这有助于信息部分。我个人认为SQL也是一个非常广阔的开放领域。我认为将其限制在仅在后端执行安全操作的知名API是一个好方法。你总是可以得到好的错误信息之类的。这些就是我即兴的建议。</p><p>&nbsp;</p><p></p><h2>LLM和langChain</h2><p></p><p>参会者2：有人尝试过做一些LangChain吗，它可以与LangChain一起使用吗？</p><p>Eleti：是的，事实上，LangChain、Harrison团队在我们推出一个小时后就发布了一个集成，所以它是有效的。</p><p>&nbsp;</p><p></p><h2>数据泄漏</h2><p></p><p>参会者2：这还暴露了一个泄漏问题。SQL示例就是一个很好的例子。如果有人读到这篇文章，他们对金融数据库进行SQL查询，并将其输入到gpt-3.5-turbo，我们基本上就泄露了数据。</p><p>如果你使用的是text-davinci-003或不同的模型，就会出现这样的问题，一些来自查询的数据会变成模型本身。在我看来，这个例子是极其危险的。</p><p>&nbsp;</p><p>Wu：实际上这存在一个误解，我认为我们最近没有作出很好地澄清，直到今年3月或2月，在我们为API提供的服务条款中，我们就说过“我们保留自己对API输入数据进行培训的权利”。我想这可能就是你所说的，就像你对一些SQL查询进行解析一样，它会在返回时以某种方式回到模型中。事实上，到目前为止，我们已经不再这样做了。根据我们的服务条款，我们实际上不会在API中对你的数据进行训练。我认为我们还没有把这一点说得非常清楚，所以人们对此非常偏执。到目前为止，还没有。你应该查阅我们的服务条款。我们不训练它。也就是说，解析的东西并不像企业级的那样。我们不会针对你的用户进行隔离。我们只是没有在自己的数据上训练它。这种围绕企业级数据隔离的特性显然很快就会出现。这一特定的安全层还没有出现。</p><p>&nbsp;</p><p>Eleti：我们不使用API数据进行训练。</p><p>&nbsp;</p><p></p><h2>函数调用的并行化</h2><p></p><p>参会者3：你展示的演示运行有点慢。我想知道，你们支持函数调用的并行化吗？就像现在你是串行的吗，你得到了这个函数签名，然后调用它，但假设ChatGPT说，三个函数应该同时被调用，这可行吗？</p><p>&nbsp;</p><p>Eleti：API实际上不支持多个函数调用。没有输出显示“调用这三个函数”。但你可以破解它。你只需要定义一个函数，让它调用多个函数，然后你提供一个签名，让模型调用它，即可实现调用多个函数，这完全是可行的。归根结底，我们仍然是使用模型的推理能力来输出一些文本。</p><p>&nbsp;</p><p></p><h2>模型上下文的预加载</h2><p></p><p>参会者4：在你给出的SQL示例中，你为其提供了一些可以访问的表。我们有没有办法可以让任何人的后续调用预加载所有上下文呢？</p><p>&nbsp;</p><p>Wu：有几个潜在的解决方案。我们有一个称为系统消息的功能，你可以在那里进行解析，它基本上设置了模型的整体对话上下文。但在当时的语境中它是完全颠倒的。目前，我们已经将上下文窗口增加到大约16000个token。你可以逐渐将更多内容压缩到系统消息中。该模型经过训练，会格外关注系统消息，以指导其做出回应。在本例中，Atty在系统消息中有两个表的模式。可以预见的是，你可以添加更多的内容来填充整个上下文。</p><p>&nbsp;</p><p>参会者4：这就是我们的预加载方式吗？</p><p>&nbsp;</p><p>Wu：是的，这是最简单的。还有一些其他的方法。你可以将它连接到外部数据源、数据库之类的。微调也是另一种选择。还有其他一些。</p><p>&nbsp;</p><p></p><h2>使用GPT进行可靠的函数调用</h2><p></p><p>参会者5：关于将GPT集成到不同的软件中。我在使用枚举时遇到了一些问题，当我要求它用英语、法语或德语做一些工作时，我使用的枚举有时会出现德语或法语。API函数也会发生这种情况吗？</p><p>&nbsp;</p><p>Eleti：是的，很不幸。模型在正常情况下以及在这种情况下都很容易产生幻觉。我们所做的基本上是对模型进行了微调，因此我们可以看到大约100000个关于如何可靠地调用函数的示例。它比你自己做的任何其他提示都要好得多。它仍然会生成参数，可能会输出无效的JSON，也可能会输出其他语言。为了防止这种情况，我们将进行更多的微调。我们也在探索一些低级推理技术来改进这一点。然后在你这边，你可以做prompt工程，只要提醒模型，不要输出德语，它会尽力的。</p><p>&nbsp;</p><p>Wu：看看它在这方面是否能做得更好，这会很有趣，尤其是如果你有一个函数签名，并且你明确列出了5个不同的英文枚举。较新的模型可能会更好，但也不完美。我不能百分百确定，不幸的是，我们没有跨英语、法语枚举那样的评估。这可能是一个值得思考的好问题，但我们很好奇，想看看它是否会变得更好。</p><p>&nbsp;</p><p></p><h2>GPT识别意图的能力</h2><p></p><p>参会者6：我有一个关于API理解意图能力的问题。函数调用是否有相似的温度（temperature）参数；如果我解析两个具有相似意图的函数，那么GPT对每个要调用的函数是否具有确定性；或者如果我多次询问，选择要调用哪个函数是否具有随机性？</p><p>&nbsp;</p><p>Eleti：随机性依然是存在的。归根结底，在底层，它仍然是一个token一个token地输出，选择要调用的函数。降低温度增加了确定性，但这并不能保证确定性。也就是说，API中有一个名为函数调用的参数，如果你知道你想让它调用哪个函数，实际上可以直接指定它，它肯定会调用该函数的。</p><p>&nbsp;</p><p></p><h2>函数调用权限</h2><p></p><p>参会者7：如果我们想限制某些用户进行某些函数调用，或者像你这样在这些SQL查询中访问某些表，你们有函数调用的权限吗，人们还需要实现他们自己的吗？</p><p>&nbsp;</p><p>Eleti：所有这些都会发生在你的服务器上，因为你拥有谁可以访问什么内容的完整上下文。这个API提供的只是GPT选择要调用哪个函数以及要使用哪些参数的能力。然后，我们希望你像对待任何其他客户端一样对待GPT的输出，因此对于不受信任的客户端输出，你可以在你的终端上验证其权限和内容。</p><p>&nbsp;</p><p></p><h2>思维链提示和约束采样</h2><p></p><p>参会者8：我只是想知道你是否可以详细说明一下这在底层发生了什么。这是底层的思维链吗？这是这些技术之上的一个有效的API层吗？</p><p>&nbsp;</p><p>Eleti：思维链提示是一种在给模型任务时的询问方式，首先，告诉我你要做什么，然后去做。如果你问“布鲁克林的天气怎么样？”它可能会说“我收到了一个天气请求，我将调用天气API”。然后它就这样做了。这是一种快速的工程技术。这是一个微调。随着插件的推出，我们收集了大约100000个用户问题和函数调用示例的内部和外部数据。这一切都在模型中进行了微调。这就是它的来源。</p><p>&nbsp;</p><p>我们还可以使用第三种技术，叫做约束采样，其中在token采样层，你可以确保预测的下一个token是值集中的一个。在JSON示例中，逗号之后必须是新行或类似的内容。我们可能会弄错，但是我们明白了，我们有语法要分配。这是我们正在探索的领域。这是从提示到微调再到更低层的东西的漫长旅程。这是让GPT输出可靠结构化数据的过程。</p><p>&nbsp;</p><p></p><h2>矢量数据库的兼容性</h2><p></p><p>参会者9：这可以与矢量数据库一起使用吗？我的想法是，我想根据我输入到向量数据库中的信息来约束信息，但它仍然能适用于函数逻辑？</p><p>&nbsp;</p><p>Eleti：是的，和以前一样好用。</p><p>&nbsp;</p><p></p><h2>函数调用是否公开可用？</h2><p></p><p>参会者10：我们今天就能使用它了吗？它现在对公众开放了吗？</p><p>&nbsp;</p><p>Wu：它是今天公开的，但有一个警告。它在gpt-3.5-turbo模型上可用。这里的任何人实际上都可以使用gpt-3.5-turbo访问函数调用，因为这是普遍可用的。它也可以在GPT-4 API上使用，但不幸的是，它仍然处于等待名单中。如果你不在该等待名单中，并且你可以访问GPT-4 API，那么你实际上可以使用GPT-4进行此操作。它在这方面做得更好。进度有点慢。如果你仍在等待名单上，或者你无法访问GPT-4 API，你今天可以在GPT-3.5-turbo上试用。</p><p>&nbsp;</p><p>查看更多<a href="https://www.infoq.com/transcripts/presentations/">演示文稿字幕</a>"</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/presentations/bicycle-ai-gpt-4-tools/">https://www.infoq.com/presentations/bicycle-ai-gpt-4-tools/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh</id>
            <title>AutoML时代，领英工程师如何缩短模型训练时间</title>
            <link>https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 领英工程师, AutoML框架, 内容审核, 数据漂移, 对抗性威胁
<br>
<br>
总结: 领英工程师使用自研的AutoML框架来协助发现和移除违反标准政策的内容。他们通过不断重新训练模型和调整系统来应对内容审核中的挑战，包括数据漂移和对抗性威胁。AutoML工具实现了数据准备和特征转换的自动化，通过搜索超参数和优化方式生成模型，并自动部署到生产服务器。虽然工具仍有改进空间，但这种方式可以在小规模范围内复制，减轻机器学习工程师的工作量。 </div>
                        <hr>
                    
                    <p>领英工程师 Shubham Agarwal 及 Rishi Gupta 解释道，为协助发现并移除违反其标准政策的内容，领英一直在使用自研的 AutoML 框架，该框架可以并行地训练分类器且试验多个模型架构。</p><p></p><p></p><blockquote>我们使用 AutoML 不断重新训练已有模型，将训练所需时间从数月缩短到数天，并减少开发新基线模型所需时间。这也让我们能积极主动地应对新出现的对抗性威胁。</blockquote><p></p><p></p><p>内容审核的关键之一在于持续的执行和调整，以应对规避审核的新手段，除此之外还必须要能适应环境的变化。这些变化包括：数据漂移，即平台上发布的内容会随着对话的进行发生固有变化；全球事件，这类事件往往会在讨论中出现并产生不同观点，其中常充斥着错误信息；对抗性威胁，其中包括欺诈和欺瞒行为，如伪造档案、实施诈骗等。</p><p></p><p>为应对上述挑战，领英采用的方法目标为“主动检测”，该方法需要一个不断调整和发展其 ML 模型和系统的过程。AutoML 是领英内部研发的工具，全称为自动化机器学习（Automated Machine Learning），用于，通过不断在新数据上重新训练模型、使用假负和假正等数据修正模型、微调参数方式提升机器学习性能。</p><p></p><p></p><blockquote>通过 AutoML，我们得以将过去冗长且复杂的流程转变为精简又高效的流程……在实现 AutoML 后，我们开发新基线模型和持续性重新训练已有模型的平均所需时间从两个月缩短直不到一周。</blockquote><p></p><p></p><p>通过 AutoML，领英工程师实现了数据准备和特征转换过程的自动化，其中包括降噪、降维和特征工程，意在创建用于分类器训练的高质量训练数据集。</p><p></p><p>在第二阶段，AutoML 通过搜索一系列超参数和优化方式，对比不同分类器架构在一组已定的评估指标下生成的模型性能。</p><p></p><p>最后，AutoML 将新完成训练的模型供给生产服务器，实现部署过程的自动化。</p><p></p><p>Agarwal 和 Gupta 认为这套工具仍有一些方面不太成熟，具体来说是需要提高速度和效率，使其能够在更大范围内应用，最终提高对计算能力的要求。他们称，另一个颇具前景的领域是使用生成式 AI，减少标签噪声并生成用于模型训练的合成数据，从而提高数据集质量，</p><p></p><p>虽然并不是所有的组织都有领英的运营规模，或者能拥有自研 ML 自动化工具的资源，但 Agarwal 和 Gupta 所描述的方式仍可在小规模范围内进行复制，从而减轻机器学习工程师与重新训练已有模型相关的重复性工作量。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/01/linkedin-automl-content-filter/">https://www.infoq.com/news/2024/01/linkedin-automl-content-filter/</a>"</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s112l2YWcRwmIDkRaNqF</id>
            <title>ChatGPT 正确回答代码问题的几率比抛硬币还要差</title>
            <link>https://www.infoq.cn/article/s112l2YWcRwmIDkRaNqF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s112l2YWcRwmIDkRaNqF</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 09:44:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 普渡大学, ChatGPT, 错误率, 首选答案
<br>
<br>
总结: 普渡大学的研究发现，OpenAI家的聊天机器人ChatGPT在回答软件编程相关问题时，有超过一半的概率会给出错误答案。尽管如此，这款机器人的说服力还是能够骗过三分之一的研究参与者。研究还发现，尽管ChatGPT的回答中有明显错误，但仍有一部分参与者将其标记为首选答案。这可能是因为ChatGPT的回答风格自信且权威，给人们留下了深刻的印象。 </div>
                        <hr>
                    
                    <p>普渡大学的一项研究显示，OpenAI 家神奇的聊天机器人 ChatGPT 在回答软件编程相关的问题时，有一半以上的概率会给出错误答案。尽管如此，这款机器人的说服力还是能骗过三分之一的研究参与者。</p><p>&nbsp;</p><p>普渡大学的团队分析了 ChatGPT 对517个 Stack Overflow 问题的回答，从正确性、一致性、全面性和间接性四个方面进行了评估。美国的学者同样对这些答案进行了语言和情感的分析，并用模型生成的结果询问了几十位志愿者的意见。</p><p>&nbsp;</p><p>“我们的分析表明，ChatGPT 的回答中有52%的错误率，77%过于冗长，”该团队的论文总结，“尽管如此，ChatGPT 的回答全面且语言风格清晰明了，仍在39.34%的情况下被视作首选。”在这组首选的 ChatGPT 回答中，有77%都是错误的。</p><p>&nbsp;</p><p>OpenAI 在 ChatGPT 的官网上承认其软件“可能会产生不准确的人物、地点或事实信息。”我们询问了实验室是否对普渡大学的研究发表任何评论。</p><p>&nbsp;</p><p></p><blockquote>只有在 ChatGPT 的回答中错误足够明显时，用户才能看出问题。</blockquote><p></p><p>&nbsp;</p><p><a href="https://arxiv.org/abs/2308.02312">预印本</a>"标题为《谁的回答更好？对 ChatGPT 和 StackOverflow 在软件工程方面问题回答的深入分析》，由研究人员 Samia Kabir、David Udo-Imeh、Bonan Kou，及助理教授 Tianyi Zhang合作编著。</p><p>&nbsp;</p><p>“我们在研究中观察到，只有当 ChatGPT 回答中的错误非常明显时，人们才能发现，”论文中指出，“然而，当错误不易验证或需要外部 IDE 或文档时，人们往往无法发现错误或低估回答中的错误程度”。</p><p>&nbsp;</p><p>论文称，即使在回答中有明显错误，12名参与者中仍有两人将其标记为首选答案。论文将此归咎于 ChatGPT 轻松且权威的回答风格。</p><p>&nbsp;</p><p>“通过半结构化的采访中可以看出，礼貌用语、自信有力和教科书式的答案，再加上全面性和答案中的因果关系，这些能让完全错误的答案显得正确，”论文中写道。</p><p></p><h3>研究发现，大家更喜欢 ChatGPT 错误且冗长的答案</h3><p></p><p></p><p>“与 Stack Overflow 的答案相比，参与者更喜欢 ChatGPT 错误且冗长的答案，原因有很多，”普渡大学的博士生，也是论文的作者之一，Samia Kabir 告诉《The Register》。</p><p>&nbsp;</p><p>“主要原因是 ChatGPT 的答案非常详细，很多情况下，如果参与者能够从冗长但详细的答案中获得有用信息，他们并不介意答案的长度。此外，积极的语气和礼貌的回答则是另外两个原因。”</p><p></p><p>“当参与者认为 ChatGPT 的回答非常深刻时，便会忽视答案中的错误。ChatGPT 能够自信地传达颇有见地的信息（即使是错误信息），为它赢得了用户的信任，从而让人们更偏好不正确的答案。”Kabir 称，用户研究在对 ChatGPT 答案的深入人工分析和大规模语言分析方面有补充作用。“不管怎么说，更大规模的样本量总是没坏处，”她说，“我们也欢迎其他研究者复制我们的研究从而促进未来的研究发展，我们的数据集是公开的。”</p><p>&nbsp;</p><p>作者观察到，ChatGPT 的答案包含更多“驱动性”，会在文字间暗示成就或成绩，但对风险的描述频率不如 Stack Overflow 帖子。“我们多次观察到 ChatGPT 使用了‘我当然能帮您’、‘这一定能解决问题’等短语”，论文中称。</p><p>&nbsp;</p><p>除此之外，作者还发现 ChatGPT 更容易犯概念性错误而非事实性错误。“ChatGPT 回答出错多数是由于它无法理解问题基本背景的本质，”论文中发现。</p><p>&nbsp;</p><p>作者对 ChatGPT 和 Stack Overflow 回答进行的语言分析表明，机器人的回答“更正式，也表达了更多的分析性思维，展示了更多其为实现目标所做的努力，也较少表现出负面的情绪”。研究团队的情绪分析认为，ChatGPT 比 Stack Overflow 的回答表现出了“更积极的情感”。</p><p>&nbsp;</p><p>Kabir 称，“根据我们的研究结果和观察，我们建议 Stack Overflow 可以采用有效的方式检测评论及回答中的负面或攻击性情绪，改善情绪变得礼貌”。</p><p>&nbsp;</p><p>“此外，Stack Overflow 可以提高其答案的可发现行，从而帮助用户找到有用的答案。Stack Overflow 也可以提供更为具体的指引，帮助回答者组织答案，比如用循序渐进、注重细节的方式回答”。</p><p></p><h3>Stack Overflow 还是溢出的堆栈</h3><p></p><p></p><p>对于 Stack Overflow 来说，还是有一些积极的消息。在2018年，Stack Overflow 是130万安卓应用程序中<a href="https://ieeexplore.ieee.org/document/7958574">15%</a>"的错误代码片段来源。在研究中，60%的受访者认为（自认的）人工撰写的答案更正确、更简洁，也更有用。</p><p>&nbsp;</p><p>尽管如此，Stack Overflow 的使用量似乎还是有所下降，但具体下降的幅度还有争议。SimilarWeb 在<a href="https://www.similarweb.com/blog/insights/ai-news/stack-overflow-chatgpt/">四月的一份报告</a>"称，自2022年1月以来，Stack Overflow 的网站流量似乎每月都有6%的下降幅度，3月中更是下降了13.9%。</p><p>&nbsp;</p><p>Stack Overflow 的问答网络站点，Stack Exchange 中的社区成员显然也得出了<a href="https://meta.stackexchange.com/questions/387278/did-stack-exchanges-traffic-go-down-since-chatgpt">类似的结论</a>"，他们是基于新的问题活动、网站上发布的新回答，以及新用户注册数量的下降中得出的。</p><p>&nbsp;</p><p>自<a href="https://www.theregister.com/2021/06/02/stack_overflow_prosus/">所有权于2021年更新</a>"后，Stack Overflow 公司在发送给《The Register》的一封电子邮件中表达了对 SimilarWeb 评估的异议。</p><p>&nbsp;</p><p>一位发言人称，Stack Overflow 在2022年5月将其分析 cookie 从“严格必要”重新归类为“性能”cookie，并于2022年9月改用第4版谷歌 Analytics，这两项策略都会影响流量的报告和长期的对比。</p><p>&nbsp;</p><p>“尽管我们看到流量略有下降，但事实绝不是图表上显示的那样”，公司发言人告诉《The Register》，“与2022年相比，2023年的总体流量平均下降了5%”。</p><p>&nbsp;</p><p>“尽管如此，Stack Overflow 及其他许多网站的流量都受过去几月内 ChatGPT 关注度激增的影响。2023年4月，我们的流量降幅超过了平均水平（约14%），这很可能是由于开发者在3月 ChatGPT 发布后进行了试用。我们的流量也会受搜索算法影响变化，这对我们的内容发现形式有很大的影响”。</p><p>&nbsp;</p><p>在被问及这篇论文中的研究结果时，Stack Overflow 的发言人表示公司目前没人有时间研究这篇报告。</p><p></p><p>“大家都知道开发者在利用人工智能方式方面并不缺乏选择，但根据我们自己的调查结果，人工智能的采用有一个核心的障碍，那就是对人工智能生成内容的准确性的信任”，该发言人称。</p><p></p><p>“Stack Overflow 近期对9万名程序员进行的年度开发者调查发现，77%的开发者对人工智能工具持支持态度，但只有42%的开发者选择相信这些工具的准确性。<a href="https://stackoverflow.blog/2023/07/27/announcing-overflowai/">OverflowAI</a>" 的开发是以社区为核心，注重数据和人工智能生成内容的准确性”。</p><p>&nbsp;</p><p>“有了 OverflowAI，我们就能在 Stack Overflow 的社区和其中5800多万的问题和答案中进行筛查、验证、归因，并确认准确性和可信度”。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4</id>
            <title>微软战略AI产品发布一周就翻车！网友：跟ChatGPT Plus 比，简直就是垃圾</title>
            <link>https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 06:48:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, Copilot Pro, 用户反馈, 性能
<br>
<br>
总结: 微软推出的个人订阅服务Copilot Pro在用户反馈中性能不佳。用户表示Copilot Pro在生产中用不到，提供的解决方案存在错误。虽然有人认为它加快了工作流程，但大家普遍期待它能提高工作效率。在编程方面，Copilot Pro口碑也不好。此外，Copilot Pro的定价较高，用户认为它并没有明确需求。微软在人工智能应用方面的投入目前还没有带来明显收入。 </div>
                        <hr>
                    
                    <p>1月16日，微软重磅推出了针对个人用户的订阅服务Copilot Pro，每月20美元，Microsoft 365个人版/家庭版用户就能在Word、Excel、PPT等Office全家桶中直接用上GPT-4。</p><p>&nbsp;</p><p>在发布一周多后，Copilot Pro也迎来了用户的第一波反馈。结果显示，Copilot Pro 性能似乎配不上这么高的价格。</p><p></p><h2>“简直就是垃圾”</h2><p></p><p>&nbsp;</p><p>“目前为止，就非常平庸。我还没有真正找到它的良好需求。虽然它总结当天电子邮件/团队聊天的能力很酷，不过我在生产中用不到。”网友“Bowlen000”说道。</p><p>&nbsp;</p><p>一位取消了订阅的用户表示，“因为GitHub Copilot建议至少在 PowerShell 中使用无用的代码块，即使其新的会​​话 VS 代码扩展也相当不准确，并且带来的解决方案会产生比解决实际问题更多的错误。”</p><p>&nbsp;</p><p>“我不想再使用它了。我曾尝试让它为我撰写电子邮件，结果发现我更喜欢我自己写的。”有网友说道。</p><p>&nbsp;</p><p>当然也有人表示已经接受了它，“作为我的助手，它大大加快了我的工作流程。”还有网友表示，“我正在等待它将 Excel 转换为PPT，这样我就不必......为什么高管如此喜欢PPT？”</p><p>&nbsp;</p><p>Webcafe AI的开发者 Corbin对比了Copilot Pro和ChatGPT&nbsp;Plus&nbsp;，对于同一个表格类任务，ChatGPT无法直接导出到工作表格中。而在文本生成方面，两者没有特别大的差异。</p><p><img src="https://static001.infoq.cn/resource/image/3f/e8/3ff5afc067e52a7a66f59126be079de8.png" /></p><p></p><p>这其实也可以看作Copilot Pro的优势之一：与 Microsoft 365 数据和工作环境的无缝集成。但 Corbin补充称，如果需要类似功能，ChatGPT的用户可以添加插件。</p><p>&nbsp;</p><p>“我有一份时常面向客户的工作，需要从一个会议跳到另一个会议。如果它能将这些转变为‘即将行动’添加到 Planner 或类似计划表中，那将是天赐之物。对我来说，工作中最困难的部分就是保持事情井井有条，并且不要忘记跟进。”有网友说道。</p><p>&nbsp;</p><p>可见，大家对Copilot Pro目前最大的期待就是切实提高工作效率。</p><p>&nbsp;</p><p>同样在编程方面，Copilot Pro的口碑也并不好。</p><p>&nbsp;</p><p>有网友表示，“每当我问同一个技术问题时，我都会得到同样的 SEO 错误答案。每当我使用 GitHub Copilot 时，我都会收到带有不存在的 cmdlet 和参数的脚本。它似乎没有任何技术能力……”</p><p>&nbsp;</p><p>另一位网友Erik表示，“我认为Copilot Pro广泛使用的用例是管理咨询。所有这些咨询公司每年都让商学院应届毕业生制作Excel表格和PowerPoint演示文稿，并收取数百万美元的费用。他们所有的”建议’都是‘循环利用’的。我和妻子都曾在大型跨国公司工作，并注意到麦肯锡向我们两家公司出售同样的‘数字化转型’工具……除了颜色、logo和自定制品牌/口号/名称外，其他几乎相同。”</p><p>&nbsp;</p><p>“对于所有 IT 类事物，我们正在测试Copilot Pro有哪些好处，以便可以将其提供给客户，但到目前为止我还没有真正看到好处。”Bowlen000说道。</p><p>&nbsp;</p><p>还有网友“MichaelBoyens”指出了两者在上下文窗口大小的不同：“自推出之日起我就一直在使用 Copilot Pro，我发现 Copilot 中的上下文非常小，即 2000 个字符，而 GPT 中则为 32k token。我认为这很重要，因为如果我在 ChatGPT 中使用经过训练的 GPT 和一个大的上下文窗口，我可以获得比 Copilot 更‘智能’的答案。Copilot 中还没有存在任何意义的 GPT，尽管 Copilot Bulider 即将到来，这也许能将使事情变得更加有趣！”</p><p>&nbsp;</p><p>网友NotKoreanXD在推特上抱怨，在使用Copilot pro 中出现了运行非常慢的情况：当你在Edge浏览器上让它搜索任何内容，比如天气、Office 365 后，它开始显着减慢，令牌速度甚至小于 1令牌/秒。</p><p>&nbsp;</p><p>对此，微软广告和网络服务主管Mikhail Parakhin <a href="https://twitter.com/MParakhin/status/1748046106968391844">告诉苦苦挣扎的用户</a>"，服务器端“看起来不错”，并暗示浏览器可能有问题。</p><p>&nbsp;</p><p>更有网友直接指出，“与 ChatGPT Plus 相比，Copilot Pro 简直就是垃圾。两者差远了。”值得注意的是 ChatGPT Plus 也是每月20美元。</p><p>&nbsp;</p><p>另外，Copilot Pro的这个定价并不便宜。微软自己的M365 个人版每月才 6.99 美元，Copilot Pro 的定价几乎是其三倍，可以说是相当高了。</p><p>&nbsp;</p><p>“这看起来更像是 Adob​​e 会选择的一个奇怪定价，而不是微软会选择的……尤其是对于一个还没免费版本更有用的产品来说，而且还没有人真正迷上它。”Threxx说道。也有网友表示，如果必须一年支付240美元，那么大多数人不会在没有明确需求的情况下使用Copilot Pro。</p><p>&nbsp;</p><p></p><h2>AI 功能没有带来明显收入</h2><p></p><p>&nbsp;</p><p>尽管作为一项新服务，出现这样的问题并不意外，但获得这样的“差评”不免让微软感到尴尬。微软需要人工智能成为不可或缺的生产力工具，以证明其投资的合理性。但其在AI应用方面的投入目前并没有换来相应的回报，比如搜索。</p><p>&nbsp;</p><p>微软于 2023 年 2 月<a href="https://www.theregister.com/2023/02/07/microsoft_bing_ai/">推出了</a>"由 OpenAI 驱动的 Bing 聊天机器人，当时其在所有平台的全球搜索市场份额为 2.81%。尽管 Bing 每个月都在一点点增长，但根据 StatCounter 的数据，截止去年12月，Bing 的使用率仅为 3.37%。</p><p>&nbsp;</p><p>这些数字与竞争对手谷歌相比相形见绌，谷歌在 2023 年初占据所有平台全球搜索市场 93.37% 的份额，尽管到去年 12 月这一数字已下滑至 91.62%。仅在桌面领域，Bing 从 8.18% 略有上升至 10.53%；谷歌从85.64%下降到81.71%。在移动领域，Bing 全年仍低于 1%，而 Google 则拥有全球 95% 以上的市场份额。</p><p>&nbsp;</p><p>也就是说，到目前为止，所有这些大肆宣传的人工智能功能几乎没有增加微软在全球搜索市场的份额。尽管微软在推出 Bing 对话助手后吸引了一些新用户但数量并不多。谷歌仍然是目前互联网搜索的老大。</p><p>&nbsp;</p><p>有分析师表示，微软正在以两种方式承担人工智能建设成本：一是为自己的产品提供动力，例如企业每月支付 30 美元的Copilot人工智能助手；二是为使用 Azure 云计算服务创建人工智能产品的公司提供服务。</p><p>&nbsp;</p><p>但在收益回笼之前，微软还要进行更大规模的投资，比如微软建立了新的数据中心来支持人工智能，还要从英伟达等公司那里购买芯片，各种支出都在增长。</p><p><img src="https://static001.infoq.cn/resource/image/90/00/90477b8273dfcf6712610bdd0f4c2900.png" /></p><p></p><p>虽然 GitHub Copilot 在很大程度上取得了成功，但性能问题和成本问题表明微软仍有很长的路要走。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.reddit.com/r/sysadmin/comments/19dj0dv/copilot_feedback_nearly_1_week_in/">https://www.reddit.com/r/sysadmin/comments/19dj0dv/copilot_feedback_nearly_1_week_in/</a>"</p><p><a href="https://www.youtube.com/watch?v=5mhEu-U1yxs">https://www.youtube.com/watch?v=5mhEu-U1yxs</a>"</p><p><a href="https://www.theregister.com/2024/01/23/microsoft_copilot_pro/?td=rt-3a">https://www.theregister.com/2024/01/23/microsoft_copilot_pro/?td=rt-3a</a>"</p><p><a href="https://www.theregister.com/2024/01/18/bing_ai_search/">https://www.theregister.com/2024/01/18/bing_ai_search/</a>"</p><p><a href="https://www.reuters.com/technology/ai-lesson-microsoft-google-spend-money-make-money-2023-07-25/">https://www.reuters.com/technology/ai-lesson-microsoft-google-spend-money-make-money-2023-07-25/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ly4avjCIkBp2gF3eqrEK</id>
            <title>OpenAI也搞“年龄歧视”？奥特曼对话盖茨爆料：员工整体年龄偏大，是个坏兆头</title>
            <link>https://www.infoq.cn/article/Ly4avjCIkBp2gF3eqrEK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ly4avjCIkBp2gF3eqrEK</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 11:33:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 编译, 核子可乐, Tina, GPT AI模型
<br>
<br>
总结: 比尔·盖茨对OpenAI的GPT AI模型表示震撼，认为它是技术上最具革命性的进步。他认为人工智能的发展将改变人们的工作、学习、旅行、医疗保健和交流方式，整个行业都将围绕它重新定位。ChatGPT团队正在努力拓展GPT-4，使其摆脱推理限制并提升可靠性。比尔·盖茨和山姆·奥特曼讨论了ChatGPT的发展和AI模型的未来，认为多模态、推理能力、可靠性和个性化是关键的里程碑和改进方向。 </div>
                        <hr>
                    
                    <p>编译 | 核子可乐、Tina</p><p>&nbsp;</p><p>去年，比尔·盖茨发表了一篇十分引人关注的博客文章，在文中他表示OpenAI的GPT AI模型是技术上最具革命性的进步，这是他人生中第二次被科技真正震撼到。</p><p>&nbsp;</p><p>盖茨写道，第一次是在1980年，当时他第一次看到图形用户界面（GUI）。他表示，GUI成为他创建微软Windows操作系统的基石。第二次是在2022年年中，当时他见识到了OpenAI及其生成式人工智能ChatGPT的学习能力。盖茨写道：“人工智能的发展与微处理器、个人电脑、互联网和移动电话的诞生一样重要。它将改变人们工作、学习、旅行、获得医疗保健以及彼此交流的方式。整个行业都将围绕它重新定位。企业将通过如何使用它而脱颖而出。”</p><p>&nbsp;</p><p>今年，比尔·盖茨又发布了一个与OpenAI 首席执行官山姆·奥特曼对话的播客，两人深入探讨了 ChatGPT 的发展。在交谈中，比尔·盖茨称赞道：“没想到 ChatGPT 会变得这么厉害！”显然他对这个模型及其快速发展印象深刻。</p><p>&nbsp;</p><p>ChatGPT 最初只是一个语言模型，如今却成长为一个拥有听觉、视觉和语言能力的人工智能媒介。而OpenAI 推出的最新语言模型 GPT-4 更具创造力和协作性，它能与用户一起生成、编辑和迭代各类创意和技术写作任务。目前，ChatGPT团队正在努力拓展GPT-4，使其摆脱目前的推理限制，并专注于提升可靠性。</p><p>&nbsp;</p><p>在与比尔·盖茨的对话中，山姆·奥特曼展望了 ChatGPT 的未来，同时强调他们的 AI 系统仍在不断学习和进化。“现在我们所见到的成果令人兴奋，但更重要的是要认识到这项技术至少在未来五到十年仍将飞速发展。 我们可以说，现在这些模型还处于早期阶段，还有很大的进步空间，”</p><p>&nbsp;</p><p>另一方面，山姆·奥特曼在上周的达沃斯采访中说道，他目前的首要任务是推出新的模型，很可能被称为 GPT-5。同时，他认为实现这一切的前提条件是能源生产能取得突破。他坚信能源生产的突破是推动日益强大、耗能巨大的 AI 模型发展的重要一步，“如果没有取得突破，我们就无法实现目标。”</p><p>&nbsp;</p><p>但ChatGPT未来到底能达到什么样的能力？这恐怕主要在于比尔·盖茨和山姆·奥特曼想将生成式AI带向何方。这也是比尔·盖茨这期播客所讨论的内容，我们将他们的聊天内容翻译出来以飨大家。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d81fe3c18a3c66cffa4f4fe5b80704a.png" /></p><p></p><p>&nbsp;</p><p></p><h2>为我解惑：比尔·盖茨与山姆·奥特曼的对话</h2><p></p><p>&nbsp;</p><p>比尔·盖茨: 今天我们主要讨论AI话题，这是个令人兴奋的方向，也有不少人表达了担忧。欢迎你，山姆。</p><p>山姆·奥特曼: 非常荣幸能来到这里。</p><p>&nbsp;</p><p>比尔·盖茨: 我很高兴能看到你的工作不断推进，但我个人也抱有一点怀疑。我真没想到ChatGPT会那么强大，着实令人大吃一惊，特别是它在编码架构之外的出色表现。我们熟悉数字，知道怎么做数学运算，但模仿莎士比亚的文字风格是怎么实现的？你能给大家解释一下吗？</p><p>山姆·奥特曼: 当然可以。其实这种模仿能力对人类来说非常困难，而在计算机这边也差不多，都需要依靠彼此相连的神经元。这种连接是动态的，虽然我们没法直接切开大脑去做观察，但可以用X射线检查并建立起科学理论。我们在可解释性方面已经取得了不错的进展，相信随时间推进还会有更多成果。我希望最终能够理解神经网络的完整运作方式，但目前的认知确实比较有限。如你所说，我们愿意一点点改进对原理的了解，这也将成为后续发展的基石。哪怕抛开科学探索上的好奇心，我们也有意愿解开这个谜题。只是神经网络的规模太过巨大，我们连人类如何理解莎士比亚、表达莎士比亚都不清楚，更别说去分析计算机了。</p><p>&nbsp;</p><p>比尔·盖茨: 确实不清楚。</p><p>山姆·奥特曼: 不光是运作机理不明确，我们甚至不知道怎样完美进行X光检查、观察并设计机能测试，所以要做的工作还很多。</p><p>&nbsp;</p><p>比尔·盖茨: 但我相信在未来五年内，人类会逐渐理解这一切。而这样的理解，也能让未来的AI模型获得远超当下的训练效率和准确性。</p><p>山姆·奥特曼: 确实是这样。技术的发展过程就是艰难探索的过程，总有人率先做出实证发现，但却无法解释其底层原理，只知道确切有效。之后随着科学理解的加深，人类终于可以理解一切、运用一切。</p><p>&nbsp;</p><p>比尔·盖茨: 是的，物理学、生物学都是这样。总有让人摸不着头脑的时候，比如“这些机能是怎么组合在一起的”？</p><p>山姆·奥特曼: 以我们的研究为例，GPT-1自己学会了如何解决问题，着实令人印象深刻，而当时研究人员根本就不清楚其工作原理或者实现原因。之后我们发现了规模越大、性能越好的规律，初步掌握了后续开发的方向。因此我们才能信心满满地保证，自己的演示模型肯定能够发挥作用。虽然当时我们的模型还没训练完成，但对这个规律已经很有信心。我们就对此进行了一系列尝试，开始对当前发生的一切做科学解释。不过这些都是后话，最初的判断首先来自实证结果。</p><p>&nbsp;</p><p></p><h4>谈 ChatGPT、人工智能和法规</h4><p></p><p>&nbsp;</p><p>比尔·盖茨: 展望未来两年，你觉得会出现哪些关键里程碑？</p><p>山姆·奥特曼: 多模态肯定是重中之重。</p><p>&nbsp;</p><p>比尔·盖茨: 就是直接支持语音输入-语音输出？</p><p>山姆·奥特曼: 对，语音输入-语音输出。之后是支持图像，最后是视频。很明显，人们对AI的期待也是如此。我们发布了图像和音频支持功能，市场反响甚至远超我们的预期。后续我们将更进一步，而最重要的进步方向应该会体现在推理能力上。目前，GPT-4还只能以极其有限的方式进行推理。再就是可靠性。如果把大部分问题反复问GPT-4上万次，那么其中部分答案当然会很好，但模型本身不知道哪个才是最佳答案，所以整个重复加筛选的过程只能由用户进行。如果能让可靠性更上一层楼，那么GPT的实用意义将大大增加。</p><p>&nbsp;</p><p>可定制性和个性化也非常重要。人们希望从GPT-4中得出差异化的结果：不同风格、不同的假设前提等等。我们将让这一切成为可能，允许大家提交自己的数据。比如吸纳你的个人信息、电子邮件、日历安排、约会规划并接入其他外部数据源等等。这些都是接下来最重要的改进方向。</p><p>&nbsp;</p><p>比尔·盖茨: 现在的基础算法还主要是前馈和乘法，所以输出每个新单词在本质上就是在做重复迭代。如果想要实现进一步发展目标，比如求解复杂的数学方程，那可能会涉及随机次数变换，意味着推理的控制逻辑也将更加复杂。不知道在这方面，你们做了哪些探索。</p><p>山姆·奥特曼: 确实，我们似乎需要某种自适应计算。现在我们在每个token上耗费的计算量是相同的，无论是最简单的token、还是最复杂的数学计算，这肯定不行。</p><p>&nbsp;</p><p>比尔·盖茨: 是的，比如要求大模型“证明黎曼猜想”。</p><p>山姆·奥特曼: 那肯定需要大量算力。</p><p>&nbsp;</p><p>比尔·盖茨: 但对现在的模型来说，它为这个问题分配的自力跟输出“the”完全一样。</p><p>山姆·奥特曼: 没错，所以目前的方案只能算是能用。在此之后，我们还需要为更复杂的问题找到答案。</p><p>&nbsp;</p><p>比尔·盖茨: 你和我都出席了参议院教育会议，很高兴有约30名参议员能够到场，大家交换意见并共同推进这项巨大的变革。很难讲政界为什么会重视这个问题，但他们提出的问题的确非常现实——“我们没能管好社交媒体，我们本应做得更好”。这确实是个严峻挑战，在舆论层面引发严重的两极分化迹象。而且哪怕是现在，我也没想好该如何处理这个问题。</p><p>山姆·奥特曼: 我不太理解政府为什么没能把社交媒体管理得更好，但似乎不妨以此为契机，帮助政府为接下来的AI研究探索指导方针。</p><p>&nbsp;</p><p>比尔·盖茨: 这确实是个不错的研究案例。说起监管，你对于未来的监管思路有没有大体上的理解？</p><p>山姆·奥特曼: 我们已经在着手解决这个问题了。而且包括AI在内，对技术领域的监管很容易过度，以往就多次发生过这类状况。虽然不敢保证，但假设我们的这条发展路线是对的，而且AI技术的发展也确实能够达到我们预期的水平，那么其必将对全社会、地缘政治平衡等重要因素产生深远影响。这些当然还只是假设，可如果真的出现了算力达到10万甚至100万倍于GPT-4的超级AI系统，那么必须在人类社会建立起覆盖全球的监管机构，由他们管控和指导技术演进。毕竟这已经不再是单一技术，而是一股能够左右世界局势的力量。我们讨论的一种潜在模式，可能是类似国际原子能机构的组织方式。在核能方面，我们曾经做出过成功的尝试。强大的潜在全球影响力，必须对应强有力的全球性机构。我深切认同这一点。该机构应该负责解决各种短期问题，包括AI模型可以输出什么、不该输出什么，如何处理版权争议等等。不同的国家对这些问题有着不同的看法，需要充分进行讨论。</p><p>&nbsp;</p><p>比尔·盖茨: 有些人认为，如果真出现了如此强大的模型，人类必须对其保持警惕——毕竟核监管之所以能够在全球范围内广泛发挥作用，原因就是至少在民用层面，每个人都希望遵循安全实践、受到妥善保护。但在武器化问题上，对核能的约束就非常有限了。其中的关键是如何阻止全世界都不用它做危险的事，但从目前各国在气候、恐怖主义等问题的合作态势上看，恐怕难度会很大。人们甚至援引美中竞争的现实，认为任何着眼长期的发展放缓方案都不会成功。你觉得呢，是不是要求各方放缓开发、提高警惕的想法只会成为空谈？</p><p>山姆·奥特曼: 是的，我觉得要求大家放慢开发的脚步确实不切实际。但如果换个角度讲，“你可以做自己想做的事，但不能让计算集群的功率超出某个极高的阈值”，可能更容易被各方所接收。考虑到高昂的实施成本，全世界可能也只有五个左右的国家能够构建这样的集群，此类系统需要经过国际武器核查机构的管控。其中运行的模型必须接受安全审查，在训练期间和部署前通过相关测试。我觉得这应该是可行的。之前我还不太确定，但今年我们组织过一场全球访问，与许多准参与国的元首交换了意见，且几乎得到了普遍支持。这套方案当然不足以彻底解决问题，在某些情况下，即使规模较小的AI系统也有可能引发风险或者导致严重错误。但在我看来，这至少能帮助全人类规避最高级别的风险因素。</p><p>&nbsp;</p><p></p><h4>山姆·奥特曼眼中的 ChatGPT 未来</h4><p></p><p>&nbsp;</p><p>比尔·盖茨: 但从乐观的角度看，AI也能帮助人类解决一些极端复杂的难题。</p><p>山姆·奥特曼: 那是当然。</p><p>&nbsp;</p><p>比尔·盖茨: 这也是个典型的两极分化问题。AI可能会破坏民主，这当然不是好事。但另一方面，我们也看到AI技术在某些领域极大提高了生产力水平。现在你比较关注哪些领域？</p><p>山姆·奥特曼: 首先，我觉得必须意识到AI发展是一条漫长且连续的曲线。现在我们已经拥有能够执行某些任务的AI系统。它们还无法独立完成整项工作，但却可以处理其中的特定环节，由此带来生产力提升。最终，它们应该可以承接以往只能由人类解决的任务。当然，人类也会在AI的基础上找到新的岗位、获得更好的工作体验。我一直认为只要能为人们提供更强大的工具，那他们不仅可以加快工作速度，更可以将质量提升到全新的高度。</p><p>&nbsp;</p><p>比如说，也许我们可以程序员的开发效率提高3倍。这个目标已经在实现当中，也是我们最关注的应用领域之一。更重要的是，让程序员的效率提高3倍可不止是能编写出3倍的代码量，更能让他们把脑力集中在抽象度更高的问题上、思考完全不同的内容。这就像是从当初的打孔卡到高级编程语言，这不仅加快了我们的开发速度，也让我们拿出了以往无法想象的软件成果。我们坚信这一点，也观察到了喜人的变化态势。</p><p>&nbsp;</p><p>而在AI技术进一步发展之后，其也许能够朝着执行完整任务再做迈进。比如说未来会出现小小的AI Agent，用户可以要求它“帮我编写个程度，期间我会通过提问给你引导”。那时候的AI不再简单编写几条函数，而是会带来全新的开发成果，这也是承担复杂工作的前提。终有一天，我们甚至可以要求AI“帮我经营这家公司”，或者直接要求它“去发现新的物理学规律”。所以大家千万不要被眼前的现实局限住，虽然现有成果已经相当美妙且令人兴奋，但结合这项技术的发展背景来看，未来五到十年之间AI将会出现非常陡峭的改进曲线。到时候回头来看，人们可能会感叹当初的AI模型怎么那么蠢。</p><p>&nbsp;</p><p>总之，编码应该是我们目前最关注的生产力提升领域。目前相关产品已经得到大规模部署和应用。此外，医疗保健和教育领域也同样值得关注。</p><p>&nbsp;</p><p>比尔·盖茨: 但让人心生疑虑的是，与之前的技术改进不同，这次AI的发展可以说是极为迅速且几乎没有上限。AI已经在很多领域都达到了人类从业者的水平，哪怕还没法用于科学研究，它们也已经在客服和销售电话上广泛普及。我猜你也跟我一样有点担心，就是在积极因素之外，AI的快速发展也会加大我们适应时代变化的压力。</p><p>山姆·奥特曼: 这确实是令人担忧的一面。但我觉得这人类既不一定要被迫适应，也不是说缺乏适应变化的能力。我们都经历过巨大的技术变革，任何人做的任何工作都有可能在几代人时间内发生变化。这种变革速度越来越快，但人们也适应得越来越好。过去任何一次伟大的技术革命都是这样，只不过AI变革是速度最快的一次。这的确会令人心生忧虑，担心社会跟不上变化的速度，劳动力市场适应不了层出不穷的挑战。</p><p>&nbsp;</p><p>比尔·盖茨: AI还有另外一个侧重点，就是机器人技术。它要替代的是蓝领工作。只要它的操作能力发展到人类手脚的水平，这个临界点就会到来。ChatGPT那令人印象深刻的功能突破让“消灭白领”成了核心议题，但我担心人们同时忘记去关心那些蓝领岗位。那你是怎么看待机器人技术的？</p><p>山姆·奥特曼: 我倒是非常期待。之前的机器人技术研究都太早了，所以往往进展缓慢、长期停滞。受时代局限，开发工作举步维艰，也没能帮助我们在机器学习研究中取得显著进展。长期以来，大家拿出的只有性能差劲的模拟机械和肢体复健器材之类的东西。但随着时间推移，我们意识到应该首先实现智能与认知，之后才搞清楚意识如何作用于肢体。所以我们从更易于上手的语言模型构建起步，也从未放弃过跟实体机械相结合的目标。</p><p>&nbsp;</p><p>我们已经开始对机器人公司做投资。在物理硬件方面，我终于第一次看到了真正令人兴奋的新平台。也许未来的某一天，我们能把自己的大模型跟机器人结合起来，就像你所说，发挥它们的语言理解和视频理解能力，真正让机器人独立完成某些复杂的工作。</p><p>&nbsp;</p><p>比尔·盖茨: 如果那些特定机械肢体上的研发成果被整合起来，比如腿部、手臂和手指部件，而且价格也不是高得离谱，那它们会不会很快就消灭大量蓝领岗位，彻底改变整个劳动力就业市场？</p><p>&nbsp;</p><p>山姆·奥特曼: 肯定会的。但大家应该还记得，就在七、八年之前，人们对机器人技术的理解都是先替代蓝领岗位，之后才是白领岗位。归根结底，人类最不可替代的永远是创造力，所以未来的从业者也只能依靠自己的创造力。</p><p>&nbsp;</p><p>但实际情况跟当初的预测相反，是白领先受冲击，蓝领反而相对安全。关于这个问题出现了很多有趣的讨论，而且说起创造力，我一直认为GPT模型的“幻觉”并不是bug、而是一项功能。幻觉是创造的来源。但如果想让机器人去搬弄重型机械，那最好能做到精确无误。这就是先验理论需要随实际技术做出调整的例子。我们都有事前判断，但科学的发展往往并不给面子。</p><p>&nbsp;</p><p>比尔·盖茨: 现在的AI已经非常强大了，再加上AGI通用人工智能和未来的AGI+，这三样东西会不会太危险了。我很担心这些系统落入坏人的手里。强大的系统只有在好人手中，才能最大限度受到控制。而且除了这个，我还好奇人类会用AI做什么。你知道我最近一直在投身于治疗疟疾、消灭疟疾的工作，努力招募人才并投入资源。那如果有一天，机器告诉我“比尔，你可以休息了，以你的脑力解决不了这个问题。疟疾已经被AI消灭了。”那我可能会有点难以接受。但必须承认，人类的认知是有极限的，我想消灭疟疾，但不知道如何组织起社会力量。我想改善教育，但搞不清教育到底要如何设计。想要打造出极致的架构，必须解决其中极大的不确定性。而AI的崛起，终于让20年内解决这些终极问题有了一丝可能。</p><p>山姆·奥特曼: 技术工作确实会造成沉重的心理负担，我也感觉这才是其中最困难的部分，但我也因此获得了巨大的满足感。</p><p>&nbsp;</p><p>比尔·盖茨: 你毕竟创造出了巨大的价值。</p><p>山姆·奥特曼: 说句实话，这可能是我最后一次接受这么困难的挑战了。</p><p>&nbsp;</p><p>比尔·盖茨: 我们的解决思路是围绕稀缺性组织起来的：因为好的教师、医生和方案都很稀缺，所以才有了现行制度。所以我的确很好奇，在这一切都不再稀缺中成长起来的下一代人，会以怎样的哲学理念设计社会结构、定义人生目标。也许他们会有自己的答案，而我担心自己的思维已经被稀缺性所绑架，甚至无法想象新的时代会是怎样的形态。</p><p>山姆·奥特曼: 我也一直在提醒自己，就是说虽然人类会失去一些东西，但最终得到的却是才智超越自身的新成果。我们只有适应这样一个后稀缺时代，才能找到属于自己的奋斗方向。这种感觉肯定跟以前大不一样，毕竟我们要解决的不再是疟疾这类现实问题，而是选择自己喜爱的星系，想在那里做些什么。但我相信人类足够灵活，总能用各种各样的方式找到满足感和充实感。相互扶持，用自己的方式服务他人，是人类社会永远的母题。虽然具体形式可能有所不同，但我认为唯一的出路就在这里。我们必须勇敢面对未来，因为未来必将到来。这是一个不可阻挡的技术进程，其中蕴藏着难以想象的巨大价值。我对此很有信心，我们会让这个美好时代来临，但也必将迎来不同于以往的问题。</p><p>&nbsp;</p><p>比尔·盖茨: AI的应用有很多种，其中一些比较明确，比如如何指引和激励孩子学习、如何发现能治疗阿尔茨海默症的药物，这些都有清晰的方向。但也有些问题比较模糊，比如AI能否帮助我们减少战争。在你们这些研究者看来，在智能发展中控制两极分化是常识，阻止战争也是常识，但很多人却对此抱怀疑态度。我希望人类能解决好那些最棘手的问题，比如相互间怎样和睦相处。如果AI能在这些问题上做出贡献，那就再好不过了。</p><p>山姆·奥特曼: 我坚信AI肯定会带给我们惊喜。这项技术的效用将远远超出我们的预期。虽然仍有待时间来证明，但我对此非常乐观。我也认同你的观点，希望AI能够立此奇功。</p><p>&nbsp;</p><p>比尔·盖茨: 说起来，技术的实现成本往往非常高昂，就像当初的个人电脑或者互联网连接一样，而且成本需要时间来逐渐下降。那现在AI系统的运行成本怎么样，是不是每过段时间就会有显著下降？</p><p>山姆·奥特曼: 已经下降了很多。GPT-3是我们优化时间和训练周期最长的模型，在它推出的这三年多时间里，我们已经把成本降低到40%。短短三年能实现这样的成本削减已经是个不错的开端。</p><p>&nbsp;</p><p>我敢打赌，GPT-3.5的成本则降低到接近10%。GPT-4还很新，所以我们没有多少时间做成本控制，但相关探索仍在继续。</p><p>&nbsp;</p><p>我认为在我所知晓的所有技术中，AI正处于成本降低曲线上最陡峭的部分，速度远超当初的摩尔定律。我们不仅破解了模型效率的难题，而且随着研究理解的加深，我们还能从中获取更多知识、在更小的模型中获得基本相当的性能。终有一天，我们的智能成本将趋近于零，那时候就将是全社会的彻底转型之时。</p><p>&nbsp;</p><p>但至少目前，真实世界的两大核心仍然是智力成本与能源成本。这是改善生活质量的两项基本投入，特别是对贫困群体来说。如果能够同时降低这两项指标，就能在同样收入的前提下扩大占有物的数量、极大改善生活体验。我们正处于智力改进的曲线之上，我们也将踏踏实实践行这一承诺。而且即使是按照目前的成本（远远超出预期的最高成本），每月花20美元即可获得巨量GPT-4访问资源，其创造的价值将远超20美元。可以说，我们已经在探索的道路上走得很远了。</p><p>&nbsp;</p><p></p><h4>经营OpenAI的那些事儿</h4><p></p><p>&nbsp;</p><p>比尔·盖茨: 那竞争关系如何？跟那么多同行同台竞技有趣吗？</p><p>山姆·奥特曼: 感觉很复杂，烦人、有趣也让人充满斗志。相信你当年也有过类似的感觉。竞争关系确实敦促我们做得更好、做得更快。我们对自己的研究方法抱有信心，而且OpenAI最大的优势在于：其他厂商朝着球所在的位置冲刺，而我们是朝着球飞往的位置冲刺。这种感觉还不错。</p><p>&nbsp;</p><p>比尔·盖茨: 很多人可能没想到，OpenAI居然是一家体量这么小的公司。你们有多少员工？</p><p>山姆·奥特曼: 大约500人，这还是扩张之后的规模。</p><p>&nbsp;</p><p>比尔·盖茨: 但那很小。至少跟谷歌、微软和苹果比起来不大。</p><p>山姆·奥特曼: 那是当然。而且我们不仅要运营研究实验室，还管理着整个业务体系和两款产品。</p><p>&nbsp;</p><p>比尔·盖茨: 不断扩大业务规模，与全球各地的人们交谈，倾听支持者们的意见，你应该很享受这个过程吧？</p><p>山姆·奥特曼: 是的，非常享受。</p><p>&nbsp;</p><p>比尔·盖茨: OpenAI员工的平均年龄很小吧？</p><p>山姆·奥特曼: 哈哈，其实比一般公司的平均年龄要大些。</p><p>&nbsp;</p><p>比尔·盖茨: 好吧。&nbsp;</p><p>山姆·奥特曼: 反正不像大家想象的，是一堆20来岁的程序员。</p><p>&nbsp;</p><p>比尔·盖茨: 看来是我先入为主了，毕竟我自己都60多岁了，所以看到你就会觉得是个年轻人。但你也40多岁了，没那么年轻了。OpenAI的员工也是。</p><p>山姆·奥特曼: 是的，30多、40多、50多岁的员工都有。</p><p>&nbsp;</p><p>比尔·盖茨: 这跟早期的苹果和微软真不一样，那时候我们就是帮愣头青。</p><p>山姆·奥特曼: 确实不一样，我也反思过这一点。我觉得OpenAI的整体年龄有点偏大了，不知道该怎么说，但这种现象放在社会上应该是个坏兆头。我当初在Y Combinator的时候就关注过这个问题，发现随时间推移，优秀的年轻创业者越来越少。</p><p>&nbsp;</p><p>比尔·盖茨: 这是个有趣的现象。</p><p>山姆·奥特曼: 所以哪怕是OpenAI，员工的平均年龄其实也挺大了。</p><p>&nbsp;</p><p>比尔·盖茨: 通过在Y Combinator帮助公司创业，你肯定学到了很多，这些经验正好用在现在OpenAI的管理和经营上。</p><p>山姆·奥特曼: 的确很有帮助。</p><p>&nbsp;</p><p>比尔·盖茨: 但也有很多反面案例。</p><p>山姆·奥特曼: 是的。OpenAI也做了很多违背Y Combinator常规建议的事情。我们花了四年半才推出首款产品，当初创办公司时根本说不清未来要开发怎样的产品，也压根没跟用户交流过。时至今日，我还是不建议大多数公司学习OpenAI。但在Y Combinator学习了这些规则之后，我才切身体会到这些规则何时、如何以及为何可以打破。总而言之，OpenAI的成长路线跟我见过的任何企业都截然不同。</p><p>&nbsp;</p><p>比尔·盖茨: 关键在于你聚集起了人才，让他们专注于解决那些意义重大的问题，而不是纠结于短期收入之类的小事。</p><p>山姆·奥特曼: 我觉得硅谷投资者不可能为这样一个看起来不靠谱的项目投钱，所以在产品实际上线之前，我们必须自掏腰包支持研究。但我们坚信“这个模型最终会非常出色，也一定会为人们创造价值”。这里很感激微软愿意与我们合作，这种超前收益投资明显有违风险投资行业的操作惯例。</p><p>&nbsp;</p><p>比尔·盖茨: 确实，而且投资额太高了，几乎达到了风险投资所能承受的极限。</p><p>山姆·奥特曼: 没准都超过了。&nbsp;</p><p>&nbsp;</p><p>比尔·盖茨: 没准都超过了。我对萨蒂亚持高度赞赏，就是因为他一直在思考“如何将这家出色的AI组织整合到微软的软件体系中来？”这是一项极具前瞻性的战略判断。</p><p>山姆·奥特曼: 一切都超乎预期。你说得对，我在Y Combinator学到了很多，并据此管理OpenAI。我们一方面聘请全世界最优秀的人才，另外也要保证大家在发展方向和AGI使命上保持一致。但在此之外，员工们可以随意发挥。我们都知道这种复杂问题需要时间的沉淀，回报也绝不会很快到来。</p><p>好在我们的理论被证明大致正确，但一路走来很多策略也被证明属于严重错误。科学的探索就是这样，永远喜忧参半。</p><p>&nbsp;</p><p>比尔·盖茨: 我记得当初刚看到技术演示时，自己一直在想这样的产品要怎样创收？服务会是什么样子？哪怕是在这个疯狂的时代，你们团队都有点太过超前了。</p><p>山姆·奥特曼: 是的。优秀的人们希望能跟优秀的同事一起工作。</p><p>&nbsp;</p><p>比尔·盖茨: 这是一种强大的吸引力。&nbsp;</p><p>山姆·奥特曼: 很多公司都喜欢说英雄惜英雄、好汉重好汉，但大家在OpenAI是真正感受到了自己的历史使命。每个人都希望参与到AGI的实现中来。</p><p>&nbsp;</p><p>比尔·盖茨: 这样的使命感确实令人心潮澎湃。当初你拿出技术演示时，我就为其中承载的热情所震撼；我看到了新的朋友，新的想法。而你们一刻不停，仍在以令人难以置信的速度前进。</p><p>山姆·奥特曼: 大家一定经常让你提点建议，那你一般会怎么说？</p><p>&nbsp;</p><p>比尔·盖茨: 我觉得这世界上有很多不同形式的人才。在我的职业生涯之初，我只重视纯粹的智商，类似于工程技术的灵性。当然，这个逻辑在财务和销售领域也有体现。但事实证明这是不对的。能建立起强大团队的技能组合才是重中之重。能引导人们思考、找到想要解决的问题、建立一支融合不同人才的队伍，才是最重要的能力所在。所以我想告诉年轻人，数学和科学能力当然很重要，但如果你真想成就一番事业，那么前面说的这种才能组合必不可少。</p><p>你呢，你会给出什么样的建议？</p><p>山姆·奥特曼: 我比较关注大多数人对于风险的错误理解。大家往往害怕放弃自己当前这份轻松愉快的工作，不敢奔赴自己真正想做的事情。但实际上，如果始终止步不前，那他们最终回顾一生，只会感叹自己从没有投入过、没有创办自己想象中的企业、也没试着成为一名AI研究者。我觉得这才是最大的风险，是让整个人生沦于平庸的风险。</p><p>正因为如此，我们应该明确自己的目标，同时积极询问其他人需要什么，这就是良好的开端。很多人都在以自己不想的方式虚耗时间，而我给出的建议就是试着以积极的方式解决这个问题。</p><p>&nbsp;</p><p>比尔·盖茨: 确实，让人们从事一份自己有成就感、满足感的工作，往往能够迸发出他们自己都难以想象的力量。</p><p>山姆·奥特曼: 绝对是这样。</p><p>&nbsp;</p><p>比尔·盖茨: 感谢你的到来，很高兴跟你聊了这么多。相信我们在不断尝试以更好的方式塑造AI的过程中，还会有更多值得交流的话题。</p><p>山姆·奥特曼: 也感谢你邀请我过来，聊得很开心。</p><p>&nbsp;</p><p>&nbsp;播客链接：</p><p><a href="https://www.gatesnotes.com/Unconfuse-Me-podcast-with-guest-Sam-Altman">https://www.gatesnotes.com/Unconfuse-Me-podcast-with-guest-Sam-Altman</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Idl1P18pvSSWDqtZ2mHc</id>
            <title>2023 京东零售技术年度盘点</title>
            <link>https://www.infoq.cn/article/Idl1P18pvSSWDqtZ2mHc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Idl1P18pvSSWDqtZ2mHc</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 08:08:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开放生态建设, 低价心智, 供应链创新技术, 人货匹配技术
<br>
<br>
总结: 京东零售技术团队在过去一年持续攻坚，围绕开放生态建设和低价心智等主要方向进行创新。他们通过百亿补贴、调整流量分配机制等方式为用户提供低价品质好货，同时简化商家进驻流程、优化商家体验，带动商家数量增长和平台生态活跃。他们还在供应链和人货匹配方面进行了创新，提出了端到端库存管理技术和可解释AI技术，显著提升了供应链效率和用户商品匹配效率。这些技术创新使京东在供应链和广告营销领域取得了行业最高奖项的入围，并在大语言模型应用和搜推导购体系方面实现了全面升级，提升了用户购物体验。 </div>
                        <hr>
                    
                    <p>过去一年，围绕开放生态建设、低价心智等主要方向，京东零售技术团队持续攻坚。从百亿补贴、调整流量分配机制为用户提供低价品质好货，到简化商家进驻流程、优化商家体验，带动商家数量增长和平台生态活跃，再到将大模型结合到内部大量业务场景，探索效率提升……快速响应、助力业务的同时，京东零售技术团队继续夯实增强自身能力、探索创新。</p><p></p><p>我们选取了 11 项有代表性的技术成果，与大家分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a61a525ac086eb250f6334d23002e36.png" /></p><p></p><p></p><h1>供应链创新技术入围行业最高奖项</h1><p></p><p></p><p>京东长期致力于通过前沿的数智化技术和算法，提高供应链效率。2023 年，智能供应链团队提出并应用了端到端库存管理技术和可解释 AI 技术，显著提升了补货决策的精准度，实现更快的库存周转和更高效的供应链决策、协同。前者入围 2023 年管理科学界最高奖项弗兰兹厄德曼奖决赛，后者入围 2024 年 Gartner Power of the Profession 供应链流程与技术创新奖决赛，也是该奖项唯一亚洲入围者。</p><p></p><p>传统的库存补货方法多采用先预测再优化的两步式方案，导致预测和优化阶段割裂。智能供应链团队提出端到端库存管理技术，基于深度神经网络模型，直接根据原始的历史数据输出最优补货建议，将预测和优化问题一步式解决，通过缩短决策链降低了累积误差，提升决策的准确度。</p><p></p><p>另一项痛点问题是需求预测和补货策略的解释性不足，导致一线采销人员和供应商对补货建议的实际采纳率很低。为此智能供应链团队首提可解释 AI 技术，实现预测流程白盒化，利用残差网络等前沿深度网络技术，对模型输出的建议加以清晰明确的归因和阐释，构建可解释性、自适应、高扩展性的预测模型框架，从而使下游业务人员可以清晰理解预测流程和结果。</p><p></p><p>以这两项技术为基础的自动补货系统，已实现超过 85%的自动化率。目前京东在自营商品 SKU 数量超过 1000 万的基础上，实现采购自动化率超过 85%，平均现货率超过 95%，库存周转天数降至近 30 天，在全球范围达到领先水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4746fb259d1e7ce01f1b1438e5e27a0.png" /></p><p>                                      图1 京东入围2023年弗兰兹厄德曼奖决赛</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a49020d525a73b722b6234daf12a3b49.png" /></p><p>                                      图2 京东入围Gartner 2024年流程与技术创新奖</p><p></p><p></p><h1>后行为序列时代的人货匹配技术：更智能地理解用户和商品</h1><p></p><p></p><p>当前，商家经营和营销进入存量博弈、精耕细作阶段，对广告营销技术也提出了通过技术创新提高人货匹配的效率的核心要求。京东零售广告研发部坚持创新，取得了多项突破性技术进展，共发表顶会论文17 篇，累计提交专利申请一百多篇。</p><p></p><p>（1）将隐私合规保护下的预训练技术应用于用户和商品理解算法创新，在合规前提下，借助多方安全计算、群体建模技术来解决数据匮乏问题，提出序列摘要技术，成功将精排模型的序列交互建模部分前置到排序模型之前，将行为序列的长度提升到万级别，提升了对用户兴趣的刻画能力。以行业知识+预训练的方式引入非 ID 类电商特征，在避免数据组合爆炸的同时解决稀疏表征问题提升泛化能力，是目前电商领域最大的预训练模型，并提出生成式对比学习序列预训练方案，提升新品刻画力。建设排序大模型（参数量达数百亿）提升模型容量，成为京东零售最大在线排序实时模型，并提出了基于数据先验的增量学习框架，实现了分钟级更新感知，有效提升在线学习模型对用户行为变迁的建模能力。</p><p></p><p>（2）巨幅增长的数据和愈发复杂的算法对算力提出了更高要求，广告技术团队将业务、算法、工程进行了 co-design 推进建设新一代算力体系。主要体现在：异构算力的应用探索与实践、弹性动态算力分配能力、新一代模型算力系统。CPU+GPU 异构硬件技术上，突破业界 GPU 调度难题，实现了多流多组范式，根据算力负载实现多流多组的动态分配，GPU 硬件利用率达到理论上限，比 TensorFlow 调度提升 2 倍+吞吐能力。</p><p></p><p></p><h1>自研融合ReAct/SFT/RAG的大模型基础应用框架 高效完成微调、部署和应用</h1><p></p><p></p><p>2023 年，大语言模型绝对是整个技术圈最被热议的话题之一，关键方向之一是如何将大型语言模型的强大能力融入实际业务、产生业务价值。京东零售九数算法中台推出了一整套大语言模型应用解决方案，一种融合 ReAct 框架、SFT（指令微调）与 RAG（检索增强生成）技术的应用框架，支持大语言模型学习领域知识，并提升自主决策能力及信息处理的精确度，帮助业务人员高效完成大语言模型的微调、部署和应用，快速落地业务场景。</p><p></p><p>通过自研大语言模型高效微调（SFT）框架，京东内部可以支持大语言模型高效学习领域知识，并通过编译优化、算子优化、网络和 IO 优化，提升训练性能 40%+，并且支持 70B+超大规模模型微调。在信息检索方面，建设了 Embedding 无损高性能信息压缩能力，打通大模型应用开发框架和向量数据库 Vearch，实现信息检索效率大幅提升。在复杂业务模型自主规划层面，基于 ReAct 范式构建 Agent LLM，帮助大语言模型理解上下文，精确把握用户意图，并在复杂情况下做出决策、执行任务和使用工具。</p><p></p><p>目前，已在包括知识问答、用户增长、舆情风险挖掘、数据分析等多个业务场景应用，加速了业务智能化升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d6243fc4b95645ed6af62881bb7b40a.png" /></p><p></p><p></p><h1>搜推导购体系全面升级探索用户高效便捷购物体验</h1><p></p><p></p><p>打造前沿的搜索推荐领域技术，实现用户与商家之间的精准高效连接，一直是京东零售搜推团队的关注重点。2023 年，通过导购体系的一体化升级，集成了导购路径引导和算法匹配技术，解决了一系列技术难题，技术成果沉淀数十篇专利和行业顶会论文。</p><p></p><p>面对导购场景模型和样本数据的分散以及用户反馈数据稀疏问题，区别于行业里常见的数据增强等解决方案，创新地提出了预训练-微调范式，对 session 链路中的用户行为进行精准预测，随后在各个导购场景的用户反馈数据上进行微调，显著提升了流量分发的准确性。</p><p></p><p>在主图个性化分发从 0 到 1 的建设中，一方面优化分发能力。一方面通过“模型+策略”的协同，既构建了高效的优选模型，又根据自身数据特性设计了独特的负向属性值过滤策略，极大化提升用户体验。</p><p></p><p>面对图像搜索的高精准度挑战，我们将图搜召回任务建模为一个涉及千万级图片百万级ID数据的 Re-ID 任务，并通过利用多损失联合监督，显著提升了模型特征学习的判别力。为了应对数据巨量化和模型复杂化趋势，在算力优化方面，通过分布式模型并行技术和稀疏分类采样策略，大幅提升了 GPU 显存效率和模型训练速度。</p><p></p><p>去年，我们自主设计并实现了新一代交互式引擎系统，上线了 AIGC 应用京言。通过反馈式 prompt 优化、session 切分、人类偏好指令对齐增强以及弹性多路检索等创新技术，持续探索为用户提供高效便捷购物体验。</p><p></p><p></p><h1>商家系统深度改造提升效率优化体验</h1><p></p><p></p><p>2023 年初，京东发布“春晓计划”，扩展百万量级的商家进入平台。京东以往的商家系统以服务企业用户为主，业务模式众多、复杂性很高，但新加入的有大量个人商家。如何兼顾原有复杂业务逻辑，又能快速打造适应大量个人商家移动化、简约化办公需求的运营系统？如何能简化运营和快速交付、保障商家规模的快速扩展及商家体验提升，成为京麦商家系统面临的主要挑战。</p><p></p><p>为此，移动端系统整合原生、Flutter 和 Taro 技术，实现功能间的无缝调用，提高业务功能的快速交付和互通能力，同时以内置的一体化 UI 组件确保交互设计与实现的统一；商家入驻环节应用 OCR、RPA 及大模型语义理解能力进行智能化、自动化的审核，提高入驻效率；商品管理环节结合多模态大模型、海量相似商品主体检索和结构化数据 OCR 识别等技术，智能生成商品基础信息；商品营销上首推【主图浮层】功能，即通过动态加载营销利益点实时合成主图技术；履约、售后、结算正逆向交易环节，采用 PaaS 化插件业务流程扩展、规则计算引擎动态配置等技术方式快速支持个人小店运营模式。</p><p></p><p>这些深度改造，大大提高商家入驻效率，个人店入驻时间缩短至约 4 分钟，普通店 15 分钟，企业店 3 个工作日内；商品管理环节，商家只需维护库存和价格即可轻松完成商品上架；营销信息便捷推送，共同助力商家打造“更快运营、更好服务、更省成本”的开店体验。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/04/24/0400386bbed82e7efa1c5fcb08e93824.png" /></p><p></p><p></p><h1>AIGC技术应用 实现电商创意素材的自动化生成</h1><p></p><p></p><p>电商创意素材中包含了大量的商品直观信息，优秀素材不仅能快速吸引消费者，还可以建立起情感联系。然而现有创意大多依赖人工制作，存在效率和成本的限制。京东零售技术团队基于 AIGC 技术，在图片、文案、图文创意等方面进行了技术突破，实现了高质量广告创意的自动生成。</p><p></p><p>图片创意上，技术团队提出通过类别生成器实现大规模背景生成，并使用个性化生成器从参考图像学习个性化风格，在保持个性化风格的同时能够生成高质量的不同类别背景；文案创意上，基于大语言模型通识，结合商详 OCR 卖点挖掘、标题、属性亮点词等电商数据，通过模型 fine-tune、外挂知识库等方式，实现了满足用户偏好的营销文案创意生成；图文创意上，提出了一种 P&amp;R 框架并分为规划和渲染两个阶段，规划阶段考虑产品外观和文本语义特征，使用 PlanNet 生成多样化、合理的布局，渲染阶段虑生成的布局和融合不同组件的空间关系，使用 RenderNet 生成背景。</p><p></p><p>以上技术突破成功解决了现有图片创意生成方法存在扩大生产规模时设计提示词的低效问题，克服了为特定品牌定制个性化背景时描述细节风格的困难，实现了创意化的图文生成，超越已有的图像生成方法，显著提升了设计效率并降低了制作成本。相关创新性成果已在 ACM MM、CIKM、ICME 等顶会上发表多篇论文。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/c7/6a/c70fe217ba12f2fcyy676c1ab66b9e6a.png" /></p><p></p><p></p><h1>端智能面向手机计算环境的端云协同 AI 技术创新</h1><p></p><p></p><p>近年来，随着移动端设备软硬件能力的进步，移动端的算力有了很大提升，同时面向移动端的机器学习框架和模型轻量化技术越来越成熟，端上的 AI 能力逐渐进入大众视野，端智能在电商领域也开始逐步走向规模化应用。通过持续探索，京东零售技数中心团队创新突破了端侧高性能推理引擎、端侧模型分发、异构环境及复杂任务兼容等技术卡点，完成了多个业务应用和落地，并获得信通院边缘计算产业全景图行业认证。目前均已集成端智能 SDK，首页推荐、搜索重排、结算风控业务运行情况良好，日推理次数已经突破亿级，为用户带来了更好的互交体验。核心的技术亮点包括：（1）高度量化压缩的端推理引擎：手机端对加载推理引擎体积有严格限制，既要引擎小，也要支持多类型业务。目前端侧推理引擎控制在 1.9M。 （2）高并发场景下的稳定性保障：受限与手机端复杂异构的计算条件，端侧推理稳定性是衡量端智能能力的核心因素。目前端推理几十亿次/日，推理成功率超过 99%。 （3）异构环境及复杂任务兼容性能力：同一套引擎兼容 Android/IOS/鸿蒙 3 套系统，4 种计算芯片，支持多种类型模型，支持内部不同业务多线程调用。 （4）云端协同的工程平台建设：通过建设端侧 PythonVM 能力，实现同一套代码逻辑云端共用。建设了模型预加载和模型后加载等多种端模型分发和部署能力，支持云端模型训练共用一套训练引擎能力。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/ay/7a/ayy4214976c46c8becfa8fd81b65047a.png" /></p><p></p><p></p><h1>数据安全屋：“可用不可见”技术驱动的数据合规应用新基建</h1><p></p><p></p><p>如何让数据既能被无障碍使用，同时又确保数据安全、个人隐私不被泄露？近年来成为数据开放流通领域的重要难题。</p><p></p><p>2023 年集团安全、集团大数据平台、零售隐私计算团队从 0 到 1 共同打造了集团首个“安全屋”系统，落地数据沙箱、联邦学习、多方安全计算等数据安全计算能力，基于可信平行切面技术实现高效、安全、及可扩展的系统能力，并无缝衔接集团内大数据基座、算法基座和安全基座，为集团内部数据共享应用、外部数据合作等场景提供合规支撑，成为集团数智化基础设施的重要板块。 </p><p></p><p>安全屋落地京东自研版本 Hive、Spark 等安全计算引擎，确保业务层 SQL 逻辑、UDF 算子、复杂数据 ETL 过程低感知切换，兼具衍生数据脱敏、衍生数据透明加密、入出管控等技术手段，保证数据“可用而不可见”；构建了一套免入侵的安全切面控制技术将大数据平台与算法开发平台无缝打通，对关键环节植入控制点实现同一个安全策略全局适用；融合硬件安全TEE，提供内存级数据加密运算，可有效保证运行时安全，更深度消除恶意注入和通信窃取等安全隐患；对原有 ACL、RBAC 权限管控模型进行升维扩展，从技术架构上兼顾系统灵活性和安全管控的统一性。</p><p></p><p>安全屋已经在集团内外项目中进行广泛应用，包括精准营销、金融风控、成本分析等多个场景。随着数据要素、数据市场的进一步发展，安全屋将在数据融合共享、数据安全计算等领域发挥更重要的作用。</p><p> </p><p></p><h1>数据资产全面升级实现存算集约化和生产智能化</h1><p></p><p> </p><p>京东自营和商家自运营模式，以及伴随的多种运营视角、多种组合计算、多种销售属性等观测方法，相较于行业同等量级，数据处理的难度与复杂度都显著增加。如何从海量的数据模型与数据指标中提升检索数据的效率，降低数据存算的成本，快速支撑业务的数据决策与分析，是数据团队去年聚焦解决的核心课题。过程中沉淀了多级加速引擎、基于代价的智能物化策略、基于 One Metric 的异构融合服务、基于 One Logic 的离近在线转换，显著提升业务数字化决策效率，也沉淀了多篇软著与多项技术专利。</p><p></p><p>对于智能物化与数据加速，行业普遍采用 cube 预计算+缓存模式，京东创新性落地了基于主动元数据的口径定义以及基于数据消费场景与消费频次的正负反馈动态决策，确保整个数据链路的存算分配“当下最优”，同时相较于粗粒度的物化策略，模型生命周期参考存储代价配置，数据查询链路根据 RT 表现动态寻址，使得数据生产与数据消费形成交互反馈链路，决策依据更丰富，决策粒度更精准。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/85/77/85001905ef49b8eb713e3677465a4a77.png" /></p><p></p><p>基于图形语法和多端一体的可视化能力打造层面，京东 JMT 数据可视化能力可以依托底层指标中台快速进行智能诊断与归因，相较于 tableau 等头部解决方案，融入了更多图形语法同时可灵活适配多端多场景。</p><p></p><p>结合 AIGC 技术的智能数据问答系统 chatBI，基于业务知识与数据资产的 Prompt 工程，使用本地大模型 SFT 对实体进行 embedding，通过指标服务平台统一 DSL 取代了行业普遍 NL2SQL 的解决方案，解决了人为意识到数据语言的转换难题，所消耗芯片规模也优于行业水平，在数据智能分析诊断系统里准确率大幅领先。</p><p></p><p>以上核心技术通过 23 年的打磨与应用，数据指标开发与共享效率大幅提升，分析看板搭建时间从天级别缩短到小时级别，且业务用户逐渐可以进行自交付，解决了集中式研发的人力瓶颈，日均指标消费频次从 23 年初的百万级别增长到年末的几千万。未来还将在数据加速、智能物化、智能诊断、大模型应用等方面持续深耕，不断优化数据存算成本，提升数据应用的效率、体验。&nbsp;</p><p></p><p></p><h1>宏图系统首创即时零售行业一站式 LBS 网格化运营</h1><p></p><p></p><p>即时零售行业进入全品类小时达时代，用户丰富且真实的"使用场景"切换构成了消费增长新趋势，品牌需要进行全渠道优化、重新配置资源，寻找成本、效率和体验的最优解。在此背景下，2023 年，京东到家正式发布 LBS 网格化运营工具“宏图系统”，通过 B2C+O2O 全域数据分析，实现人、货、场基于 LBS 网格化的供需精准高效匹配，帮助品牌提升全渠道运营效率，创造价值增量。</p><p></p><p>基于京东+京东到家行业独特的 B2C+O2O 零售数字化能力、数据沉淀，宏图系统能够实现基于 LBS 的网格化洞察、识别、分析、判断各个网格内的供需匹配情况，并输出用户、供给、营销策略。将京东数坊用户运营平台、京准通 LBS 流量运营平台、京东到家完美门店系统等进行打通，保障执行落地。作为标准化产品，宏图系统实现了服务模块的标准化、数据处理全周期流程的标准化和前端页面的标准化，同时解决了海量数据产出时效性与查询性能、数据指标交叉计算与验证导致数据准确性问题、数据安全保障和海量数据操作与渲染性能保障的技术难题。</p><p></p><p>宏图系统作为京东到家数字化系统持续为品牌商技术赋能全渠道数字化升级，提升 C 端获客能力、降低 B 端获客成本，实现全渠道营销提升。同时基于网格视角对品牌供给情况进行追踪，帮助链接品牌和商家，从供给覆盖、商品运营方面发现运营问题及机会，为从品牌角度的商品铺货，流通运营提供数字化的系统协同能力。</p><p></p><p></p><h1>自研实现低成本、高质量 3D 建模</h1><p></p><p></p><p>3D 建模的本质是理解物理世界并进行数字重构的过程，符合信息传递从图片、视频到 3D 的发展趋势。它在数字孪生、场景重建等场景有广泛应用，在电商场景中最直接的是商品的 3D 展示，为用户提供全面的商品信息、弥补图片形式单一角度的不足，帮助商家实现业务数据的增长。</p><p></p><p>3D 建模的常见技术路径包括基于传统图形学的方案、基于 NeRF 的深度学习方案，但从商品展示的效果看，想达到主流商详图片和视频质量的要求(PSNR&gt;40dB)都还有距离。为此，我们设计了一套全新技术方案，针对 3D 商品展示场景重点突破，可实现商详图片级展示质量，并对不同材质、形状的商品有更好的鲁棒性。目前该方案已开展商家试点，上线几百个 SKU，在引单转化率上表现正向。</p><p></p><p>3D 建模的技术管线分为采集端、服务端、展示端，除了核心建模算法，我们对采集端、展示端的用户体验有更多的投入。为了保证采集端的便利性和高质量，设计了一款采集 APP，在该 APP 中实现了详实的用户指导、准确的位姿估计、运动模糊控制等功能。在服务端，自研了一套空间编码算法和全新的3D内容格式(.jdv)，实现了高压缩、高质量、可交互，将原始采集的 400MB 素材压缩到 10MB 以内；设计了一套图像分割系统来提取干净的商品前景，通过准确的位姿估计和容积变换方法来稳定输出效果，涉及 NeRF 建模、背景分割等算法，实现了用户在复杂采集背景下的商品展示效果。在展示端，自研的空间解码器能够支持用户在商详主图上可交互式地自由查看商品 3D 展示。目前仍在继续降低采集难度、降低环境光照对展示效果影响等问题。</p><p></p><p>凡是过往，皆为序章。未来，坚持成本、效率、体验、可信、普惠、突破的技术追求，京东零售技术继续和大家一起交流成长、向新而行。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0l1WMDamJ1pADLjVdHra</id>
            <title>如何1秒内快速总结100多页文档？QQ 浏览器首次揭秘大模型实现技术细节</title>
            <link>https://www.infoq.cn/article/0l1WMDamJ1pADLjVdHra</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0l1WMDamJ1pADLjVdHra</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 10:48:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能技术, 大型语言模型, 文档阅读助手, 腾讯混元大模型
<br>
<br>
总结: 随着人工智能技术的发展，大型语言模型成为行业热点，其中文档阅读助手是一款基于腾讯混元大模型定制化的业务大模型。它能够帮助用户快速获取长内容中的关键信息，提升阅读效率。通过提示工程和定制化模型的方法，文档阅读助手能够适应不同的业务场景，并具备强大的中文创作能力和任务执行能力。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的飞速发展，大型语言模型已成为行业热点，引领着一系列技术创新。在长文档阅读场景下，利用大模型提升阅读效率也是业界重点探索的方向。</p><p></p><p>为深入了解相关技术并分享前沿实践，我们在 QCon 全球软件开发大会上邀请了腾讯 QQ 浏览器的专家研究员郭伟东。他为我们了揭示大模型背后的技术细节，展示其在一款亿级产品中的应用案例。本文根据演讲整理，希望对你有所帮助。</p><p></p><p>QQ 浏览器是一个月活跃用户超过 4 亿的综合信息平台，旨在满足用户在搜、刷、用、看四个场景下的需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd53078061da49def0f53ab997897cfd.png" /></p><p></p><p>其中「用」是指 QQ 浏览器里工具的使用，也称为帮小忙，QQ 浏览器包含了众多实用工具，帮助用户提高工作和学习效率。今天我们讨论的文档阅读助手就是"帮小忙"中的一个工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8eb1180103ac13dbaf04b8767dc0daaa.png" /></p><p></p><p>长内容消费一直是用户非常重要的诉求，如何帮助用户快速了解长内容中的关键信息，也一直是各产品努力的方向，如网页速览、电影速看和小说速读等。</p><p></p><p>但是它们普遍存在一个问题：当用户想要深入了解内容时，由于缺乏交互能力和实时更新能力，往往无法满足需求, 所以是一种被动式的信息获取方式。</p><p></p><p>正因如此，QQ 浏览器做了一款产品: 文档阅读助手，可以让用户更加自由，更加自主地获取信息。同时秉承腾讯“科技向善”的理念，也会推出关怀模式和无障碍模式，让每个人的阅读都更简单。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b17aefe773fa171dc695a02f6b2b40b.png" /></p><p></p><p></p><h4>探索巨变：大模型技术的历史与进程</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/23/23a3c3d687d1cd75573e732744458d4f.png" /></p><p></p><p>语言模型的发展始于 20 世纪 80 年代，最初基于统计方法，主要计算词汇在语料库中的概率。这一阶段，由于词汇量巨大，尤其是对于中文，需要处理庞大的统计空间，特别是多个词连续出现的概率。</p><p>第二阶段起始于 2003 年，Bingo 把神经网络引入到 NLP 领域，在 2013 年以 Word2Vec 模型推向高峰。主要特点是为每个词汇分配一个固定的向量表达（embedding），克服了以往统计方法的不足。但这种方法也存在问题，同一个词只有一个向量表示，对于多义词并不能区分，如“Bank”在“河岸”和“银行”不同的语义下，对应的 embedding 相同。</p><p></p><p>第三阶段以 BERT 为代表，主要做上下文相关的嵌入向量，允许相同的词在不同上下文中具有不同的表达，从而显著提高了模型的迁移性，NLP 的学习范式也由 end2end 方式变为预训练 + 业务微调的方式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e999674497f0cce0617d2bc5edc621e.png" /></p><p></p><p>最后，是大语言模型阶段。2017 年，谷歌发布了具有里程碑意义的"Attention is All You Need"论文，介绍了 Transformer 模型。此后，几乎所有的大语言模型都基于 Transformer 结构。</p><p></p><p>从 2018 年到 2020 年，大语言模型领域的探索期。尽管 Transformer 架构已成为统一标准，但其包含的 Encoder 和 Decoder 两个关键部分被不同研究者以不同方式探索。</p><p></p><p>例如，OpenAI 的 GPT 系列是典型的 Decoder Only 模型，专注于自然语言生成任务；而谷歌的 BERT 模型则作为双向语言模型主要使用 Encoder 部分，专注于自然语言理解任务。这一时期，研究者们大量对 BERT 进行改进和变体研究。到 2019 年，谷歌推出了 T5 架构，旨在将理解和生成统一到一个框架下。</p><p>现在来看，GPT 系列成为了大家普遍的模型结构。但是当时虽然出现了参数规模巨大的模型如 GPT-3，这些模型在生成能力上非常强大，但是对于指令的理解并不好。2021 年，谷歌推出 FLAN 模型，并引入了指令微调（Instruct Tuning）技术，极大地增强了模型对具体指令的理解和执行能力。</p><p></p><p>到了 2022 年，模型发展进一步加速， OpenAI 提出 InstructGPT，不仅整合了指令微调技术，还引入了强化学习，使模型产出的答案更加符合人类的预期。直到 2022 年底，OpenAI 推出 ChatGPT 产品，全世界都为之振奋。</p><p></p><p>大语言模型主要通过提示工程和定制化模型两种方法来支持业务。</p><p></p><p>提示工程通过调整模型的输入指令（Prompt）以获得期望的输出格式和内容。</p><p></p><p>例如，在生成问题时，可以通过精心设计的提示来引导模型产生更为结构化的内容。这种方法的优点在于不需要重新训练模型，仅通过修改输入指令即可快速适应各种业务场景，但它要求模型本身具有很全面的能力，模型往往比较大，对应的推理成本会比较高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac5e2af2b83957da720d6f95cc496e57.png" /></p><p></p><p>另一种方式是定制化模型。通过在特定业务数据上进行微调来优化大语言模型，使其更贴合业务场景。比如，针对数学场景，可以用数学数据集上进行微调以确保模型按需提供准确解答。这样的模型更专注于特定任务，可以允许更小的规模和降低推理成本。</p><p></p><p>QQ 浏览器文档阅读助手就是在腾讯混元模型的基础上定制化得到的业务大模型。腾讯混元大模型是全链路自研的通用大语言模型，拥有超千亿参数规模，预训练语料超 2 万亿 tokens，具备强大的中文创作能力，复杂语境下的逻辑推理能力，以及可靠的任务执行能力。为了更匹配应用场景的需求，腾讯也推出千亿、百亿以及十亿等不同尺寸的大模型。</p><p></p><p>目前，腾讯内部已有超过 300 项业务和应用场景接入腾讯混元大模型内测，包括 QQ 浏览器、腾讯会议、腾讯文档、企业微信、腾讯广告和微信搜一搜等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b943103b2fb6a7c6bb366cd5a7fdc002.jpeg" /></p><p></p><h4>QQ 浏览器·文档阅读助手技术方案</h4><p></p><p></p><h5>全文总结</h5><p></p><p>要进行全文总结，先要阅读并理解原文，然后提取关键信息并进行概括。许多用户上传的 PDF 文件都很长。而现有的主流开源模型支持的上下文长度为 4000 Token 或更少，这意味着它们不能一次性处理过长的文章。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd8f2f9e6c170e3a89a06fcaf689d1f4.png" /></p><p>图 1：用户 PDF 长度分布</p><p></p><p>为了达到这一目标，有两种主要方法可以用来扩展上下文长度：</p><p></p><p>第一种是在训练阶段使用更长的上下文，但这会导致显著的显存和算力消耗增加，因为 Transformer 架构的显存需求与支持的长度平方成正比；</p><p></p><p>第二种是推理时通过某种方式扩展上下文长度，比如插值，但是扩展的范围有限。</p><p></p><p>虽然这些方法确实能在一定程度上扩展上下文长度，但它们都有局限性，要么是成本过高，要么是扩展长度有限。</p><p></p><p>因此，可以用以下几种方案，解决长文章摘要的问题：</p><p></p><p>第一种方案，不管文章多长，只取前 K 个 Token 供模型处理，然后生成摘要，但会丢失部分文章信息；</p><p>第二种，称为 MapReduce 的方法。先把文章分成 N 个片段，然后将每个片段分别输入模型，分别得到每部分的摘要。然后，将这 N 个摘要片段合并，形成一个新的文档，再次调用大语言模型进行最终总结。这个方案会多次调用大型语言模型，导致较高的成本和较长的处理时间。此外，由于语言模型生成的段落摘要可能存在不准确的情况，因此在最终全文总结中可能会累计错误。</p><p></p><p>为了解决这些问题，我们采用了一种结合抽取式和生成式的方法。</p><p></p><p>首先，我们在文章中识别并抽取出最重要的句子，然后使用大语言模型对这些抽取的句子进行概括和总结。方法只调用一次大语言模型，耗时较少，并且不容易遗漏重要信息。在实际测试中，这种方法用户满意度最高，而且事实一致性也最低。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3de9ff001f7fb03c805335754400bb57.png" /></p><p></p><h5>问题生成</h5><p></p><p>为了提升用户获取信息的效率，产品会推荐一些用户可能问的问题，最直接的方法就是 LLM 利用原文信息生成一些问题。但是这种方法生成的问题通常都是非常简单的，与原文表达方式高度一致。</p><p>以腾讯第三季度的财报为例，原文提到“第三季度腾讯的总收入是多少元”，而生成的问题通常会直接是“第三季度腾讯的总收入是多少元？”。但是，实际上用户可能会用更口语化的方式表达，比如说“腾讯赚了多少钱？”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/511d72011d84d37611b08c9c6b2cdd74.png" /></p><p></p><p>真实的用户也会提出复杂的问题。例如，用户可能会问“从腾讯的财报中，我们能看出什么样的战略布局？”。</p><p></p><p>今年，微软发布了一篇关于'进化学习'的论文 WizardLM，主要通过广度进化和深度进化让 SFT 数据更加丰富，复杂度也更高，从而提升模型效果。图 2 展示了随着迭代次数增加，问题长度的变化，可以看出问题复杂度随着进化轮数增多而增加。但问题的可用性却在持续下降，到了第五轮时，可用性已经下降至 85% 以下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/511d72011d84d37611b08c9c6b2cdd74.png" /></p><p>图 2:WizardLM 不同轮次的进化问题长度</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99910eb5a308a22bc5886d06b21c36f4.png" /></p><p>图 3:WizardLM 不同轮次的训练样本可用率</p><p></p><p>针对上述问题，我们提出了一套新的进化算法——杂交进化，如图 4 示例所示：</p><p></p><p>“小明是一个爱读书的人，他有一定的读书效率；小红则是一个爱写作的人，她有一定的写作速度”。杂交进化算法中，结合这两个种子的特点，能够生成一个更加复杂的问题，使得原本两个简单的问题被转化成了一个更加复杂的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18c1f8d0bdb679a709bc2d24d06bc42e.png" /></p><p>图 4：杂交进化示例图</p><p></p><p>与 WizardLM 相比，杂交进化方法有以下几个显著特点。首先是生成效率更高。WizardLM 方法如果总的种子数量是 n，每一轮进化生成新的 n 个样本，经过五轮后，总共只能新增 5n 个样本。而杂交进化，通过两个种子样本生成一个新的样本，增加效率是 n 乘以 n-1，所以当种子样本数量较多时，生产效率远超过微软的方法，并且杂交只需要进化一轮，准确率也更高。</p><p></p><p>其次，在样本的主题分布上，生成的样本（红色部分）相较于种子样本（蓝色部分）主题更加多样，对于大模型的训练帮助更大，更详细的细节可以参考我们的论文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69d85379fab39ba0c04e57b6adf93db9.png" /></p><p></p><h5>智能问答</h5><p></p><p>通过对用户真实问题的统计分析，我们发现用户问题主要分为四类：</p><p>原文中有答案的问题（Close QA）原文中没有但互联网上有答案的问题（Open QA）原文和网页中都没有答案，但基于基础信息可以深加工得到答案的问题（Agent QA）依赖大模型通用能力的问题最后一类问题混元模型本身可以解决很好，因此这里不需要特殊处理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/265e141369125d0171fdc9e4bb725fbc.jpeg" /></p><p></p><p>对于原文中有答案的问题，关键是通过检索系统找到与该问题相关的文本。根据用户问题检索相关文本之前需要对问题进行改写。因为在多轮对话中，用户常常会省略一些词汇，所以先对问题进行改写，然后再检索。</p><p></p><p>我们尝试了三种检索方法。首先是双塔架构，但在我们的场景下并不理想，召回率大约在 80% 左右。主要是原文片段经过 Pooling 方法进行语义压缩，导致相关文本片段的语义被稀释。如：一段 500 字的文本可能只有 50 字与问题直接相关，pooling 后的语义会稀释掉 50 字的语义，导致召回不足。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe2f4be1436fc0190f0a65b98dec0ae0.png" /></p><p></p><p>因此，我们尝试了第二种架构，保留了 500 字每一个词的向量表示，并计算与问题中每一个词的相似度。通过取片段的最大相似度作为整个文本片段的相似度，，这样虽然效率有所下降，但准确率有显著提升，在业务数据集中，效果超过 text-embedding-ada-002。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4da531e380eb64596048876a5fab60d2.png" /></p><p></p><p>最后一种情况，针对答案分布在不同的文本片段的情况，做了进一步的改进，效果也得到了进一步的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc22d266ee35470c63c545cda62f4ec9.png" /></p><p></p><p>Open QA 与 Close QA 的主要区别在于原文中没有问题答案，但是互联网上有相关信息，可以通过 QQ 浏览器的搜索引擎提供相关网页，然后通过大型语言模型输出答案。</p><p></p><p>Agent QA 系统是解决原文和搜索引擎都无法提供答案时，大型语言模型将复杂任务分解成若干小步骤，然后分而治之。如: 用户想要了解腾讯流动利率时，LLM 回进行如下分解：首先，搜索流动利率的计算方法，即流动资产除以流动负债；然后，找出具体的流动资产和流动负债的数值；最后，使用计算器计算出流动利率。</p><p></p><p>这种方法听起来很好，但是存在一个问题，在专业领域，大型语言模型通常会泛泛而谈，模型往往无法规划出具体的执行步骤。为了解决这个问题，我们提出了一种新的解决方案：语言模型 + 专家知识库。</p><p>假设有一个专业问题关于“公司是否存在非法占用现金的情况”，大模型并不能做任务拆解，可以在知识库中检索到最相关的规划，然后让大型语言模型参考这个规划完成任务。实验显示，专家知识库可以显著提升专业领域问题的效果。</p><p></p><p></p><h5>优化实践：高效模型迭代加速策略</h5><p></p><p>LLM 回复非常灵活，自动化评估是加速模型迭代效率的重要部分。以摘要功能为例，一种常用的方法是将完整文章和生成的摘要输入到大语言模型中，让 LLM 判断摘要的质量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/932bad11b58c1faca0f6d480c69786c0.jpeg" /></p><p></p><p>然而，这个方法的挑战在于，原文常含有大量无关信息，这可能导致模型错误地判断摘要是否准确反映了原文的主旨，详见参考文献。</p><p></p><p>第二种评估方法来源于一篇关于 TACL 的论文。这个方法通过比较每个生成的摘要句子与原文中的句子是否相似来判断摘要是否产生幻觉。如果所有句子都足够相似，就认为摘要没有产生幻觉。</p><p>但是，因为摘要通常是多个句子的汇总，当遇到融合性或概括性句子时，这个方法就不再有效，详见参考文献。</p><p></p><p>为了克服这一限制，我们采用了检索增强型方法，将精准问答的思想应用于自动评估。结果显示，在公开的摘要生成数据集上，我们的方法的问题可用率是最高的，达到了业界领先水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af931b6af7270c8ab2d1eb3a2880f1ed.png" /></p><p></p><p>在训练过程中提升收敛速度也是一个加速模型迭代的重要方法。训练过程中，每个批次可能包含不同长度的样本，常规用 padding 的方法会浪费算力。我们采用了 Packing 策略，将多个短样本拼接在一起，以减少无效的填充部分，使得每个批次的计算更加高效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14a5a1c8920db9755bc76c9b719c4a0b.png" /></p><p></p><p>实验表明，在达到相同训练效果的情况下，Packing 训练时长约 Padding 方式的 64.1%。因此，Packing 策略大大提高了训练的效率和模型的收敛速度。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>