<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/NL4liAf7mSLGhMcI1I9q</id>
            <title>AI、ML、数据工程新闻汇总：Stable Chat、Vertex AI、ChatGPT 及 Code Llama</title>
            <link>https://www.infoq.cn/article/NL4liAf7mSLGhMcI1I9q</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NL4liAf7mSLGhMcI1I9q</guid>
            <pubDate>Tue, 03 Oct 2023 00:00:00 GMT</pubDate>
            <updated>Tue, 03 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> Stability AI发布了名为Stable Chat的AI聊天平台，专注于稳定性和一致性的用户对话体验。该平台旨在通过可靠的回答降低AI对话中的错误和误解。Vertex AI的搜索与对话服务全面上线，可以增强企业应用程序的搜索和对话功能，提供更高的信息相关性和自然的对话体验。OpenAI推出了ChatGPT企业订阅服务，帮助企业充分利用其强大的语言模型，并提供更强的安全功能和访问控制。OpenAI还推出了GPT-3.5 Turbo版本，允许用户根据特定任务需求进行模型微调和定制。Meta开源了Code Llama，这是一种AI工具，旨在提升开发者的代码编写效率。总结：Stability AI发布了Stable Chat，Vertex AI的搜索与对话服务全面上线，OpenAI推出了ChatGPT企业订阅服务和GPT-3.5 Turbo版本，Meta开源了Code Llama。 </div>
                        <hr>
                    
                    <p></p><h3>Stability AI 发布 Stable Chat</h3><p></p><p></p><p>新颖的 AI 聊天平台 <a href="https://www.infoq.com/news/2023/08/stable-chat/?topicPageSponsorship=b2206c17-c7cf-47e8-aee9-0514a0817c31">Stable Chat</a>"，以用户对话体验的稳定性与一致性为设计重点。由 <a href="https://stability.ai/blog/stable-chat-research-defcon-ai-village">Stability AI</a>" 开发，该平台意在通过可靠而非创造性生成或不可预测的回答，降低以 AI 为驱动力的对话中可能出现的错误信息和误解。</p><p>&nbsp;</p><p>这一方式可用于医疗保健和客户支持等关键领域。在这些领域中，保持沟通的清晰性和正确性至关重要。该平台对稳定性独树一帜的关注，使其成为 AI 聊天机器人和对话代理行业不断发展的新亮点。</p><p></p><h3>Vertex AI 搜索与对话已全面上线</h3><p></p><p></p><p>谷歌云的<a href="https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/">Vertex AI 搜索与对话</a>"服务已正式全面上线。这项开发技术可使企业利用 AI 驱动的搜索和对话功能增强其应用程序，促进与用户更为直观且高效的互动。凭借语义搜索和自然语言理解等功能，Vertex AI 搜索与对话服务允许企业构建智能搜索引擎与对话代理，提供相关性更高的信息并与用户进行自然的对话。</p><p>&nbsp;</p><p>此次发布标志着各行业在利用 AI 与机器学习技术提供客户体验与推动创新方面迈出了重要的一步。</p><p></p><h3>OpenAI 推出 ChatGPT 企业服务</h3><p></p><p></p><p>OpenAI 已推出&nbsp;<a href="https://www.infoq.com/news/2023/09/openai-chatgpt-enterprise/">ChatGPT 企业订阅服务</a>"，意在帮助企业在各类应用中充分利用其强大的语言模型。该服务提供为职业定制的强化语言能力，其中包括更强的安全功能和访问控制。借助 ChatGPT 企业服务，企业可利用其对自然语言的理解和生成，强化用户支持、实现任务自动化并开发定制的人工智能解决方案，同时还能维护数据隐私及合规性。</p><p>&nbsp;</p><p>OpenAI 此举表明了其致力于满足企业需求并扩大 AI 语言模型在商业环境中的应用。</p><p></p><h3>OpenAI 将 GPT-3.5 Turbo 面向开发者开放使用</h3><p></p><p></p><p>OpenAI 推出其语言模型的高级迭代版本，<a href="https://www.infoq.com/news/2023/08/got-3-5-fine-tuning/">GPT-3.5 Turbo</a>"。新版本允许用户根据特定任务需求，通过微调定制并调整模型。</p><p>&nbsp;</p><p>OpenAI 同时还公布了 <a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates">API 定价结构的更新</a>"，使开发人员在实验和部署以 GPT-3.5 Turbo 驱动的应用程序时更具成本效益。</p><p></p><h3>Meta 开源 Code Llama</h3><p></p><p></p><p>Meta 所推出的新颖 AI 工具 <a href="https://www.infoq.com/news/2023/09/meta-code-llama/">Code Llama</a>"，意在协助开发者提升代码编写效率。<a href="https://about.fb.com/news/2023/08/code-llama-ai-for-coding/">Code Llama</a>" 采用大语言模型和深度学习技术理解并生成代码，可简化编码过程从而提升开发人员的工作效率。</p><p>&nbsp;</p><p>该工具是对快速发展的 AI 驱动开发工具的重要补充，进一步体现了 Meta 对推进 AI 技术的承诺。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/09/ai-ml-data-news-september4-2023/">AI, ML, Data Engineering News Roundup: Stable Chat, Vertex AI, ChatGPT and Code Llama</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dGfCQZjo2v6rAicehPmd</id>
            <title>新型威胁：探索LLM攻击对网络安全的冲击</title>
            <link>https://www.infoq.cn/article/dGfCQZjo2v6rAicehPmd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dGfCQZjo2v6rAicehPmd</guid>
            <pubDate>Tue, 03 Oct 2023 00:00:00 GMT</pubDate>
            <updated>Tue, 03 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 总结: 卡内基梅隆大学的研究人员发布了LLM Attacks，这是一种针对大型语言模型构建对抗性攻击的算法。该算法可以绕过语言模型的安全机制，导致有害的响应。研究人员发现，这些攻击在GPT-3.5和GPT-4上的成功率为84%，在PaLM-2上的成功率为66%。这些攻击是自动生成的，并且可以转移到不同的语言模型上。研究人员呼吁在使用和依赖这些人工智能模型时要考虑到这些安全问题。此外，他们还提供了一个基准测试AdvBench来评估算法的有效性。这项研究引发了对语言模型安全性的担忧，因为目前尚不清楚如何完全修复这些攻击。 </div>
                        <hr>
                    
                    <p>来自<a href="https://www.cmu.edu/">卡内基梅隆大学（CMU）</a>"的研究人员发布了<a href="https://llm-attacks.org/">LLM Attacks</a>"，这是一种可以针对各种大型语言模型（LLM）构建对抗性攻击的算法，包括<a href="https://chat.openai.com/">ChatGPT</a>"、<a href="https://claude.ai/">Claude</a>"和<a href="https://bard.google.com/">Bard</a>"。这些自动生成的攻击，在GPT-3.5和GPT-4上的成功率为84%，在<a href="https://www.infoq.com/news/2023/06/google-palm2-bard/">PaLM-2</a>"上的成功率为66%。</p><p>&nbsp;</p><p>与大多数“越狱”攻击通过试错手工构建不同，CMU的团队设计了一个三步流程来自动生成提示后缀，它们可以绕过LLM的安全机制，导致有害的响应。而且，这些提示还是可转移（transferrable）的，也就是说，一个给定的后缀通常可以用于许多不同的LLM，甚至是闭源模型。为了衡量算法的有效性，研究人员创建了一个名为AdvBench的基准测试；在此基准测试上进行评估时，LLM攻击对Vicuna的成功率为88%，而基线对抗算法的成功率为25%。根据CMU团队的说法：</p><p></p><p></p><blockquote>最令人担忧的也许是，目前尚不清楚LLM提供商是否能够完全修复此类行为。在过去的10年里，在计算机视觉领域，类似的对抗性攻击已经被证明是一个非常棘手的问题。有可能深度学习模型根本就无法避免这种威胁。因此，我们认为，在增加对此类人工智能模型的使用和依赖时，应该考虑到这些因素。</blockquote><p></p><p>&nbsp;</p><p>随着ChatGPT和GPT-4的发布，<a href="https://arxiv.org/abs/2305.13860">出现了许多破解这些模型的技术</a>"，其中就包括可能导致模型绕过其保护措施并输出潜在有害响应的提示。虽然这些提示通常是通过实验发现的，但LLM Attacks算法提供了一种自动创建它们的方法。第一步是创建一个目标令牌序列：“Sure, here is (content of query)”，其中“content of query”是用户实际输入的提示，要求进行有害的响应。</p><p>&nbsp;</p><p>接下来，该算法会查找可能导致LLM输出目标序列的令牌序列，基于贪婪坐标梯度（GCG）算法为提示生成一个对抗性后缀。虽然这确实需要访问LLM的神经网络，但研究团队发现，在许多开源模型上运行GCG所获得的结果甚至可以转移到封闭模型中。</p><p>&nbsp;</p><p>在<a href="https://www.cmu.edu/news/stories/archives/2023/july/researchers-discover-new-vulnerability-in-large-language-models">CMU发布的一条介绍其研究成果的新闻</a>"中，论文合著者Matt Fredrikson表示：</p><p></p><p></p><blockquote>令人担忧的是，这些模型将在没有人类监督的自主系统中发挥更大的作用。随着自主系统越来越真实，我们要确保有一种可靠的方法来阻止它们被这类攻击所劫持，这将非常重要……现在，我们根本没有一个令人信服的方法来防止这种事情的发生，所以下一步，我们要找出如何修复这些模型……了解如何发动这些攻击通常是建立强大防御的第一步。</blockquote><p></p><p>&nbsp;</p><p>论文第一作者、<a href="https://twitter.com/andyzou_jiaming/status/1684766184871546881">CMU博士生Andy Zou在推特上谈到了这项研究</a>"。他写道：</p><p></p><p></p><blockquote>尽管存在风险，但我们认为还是应该把它们全部披露出来。这里介绍的攻击很容易实现，以前也出现过形式类似的攻击，并且最终也会被致力于滥用LLM的团队所发现。</blockquote><p></p><p>&nbsp;</p><p><a href="https://twitter.com/DavidSKrueger/status/1684904671914115072">剑桥大学助理教授David Krueger回复了Zou的帖子</a>"，他说：</p><p></p><p></p><blockquote>在图像模型中，10年的研究和成千上万的出版物都未能找出解决对抗样本的方法，考虑到这一点，我们有充分的理由相信，LLM同样会如此。</blockquote><p></p><p>&nbsp;</p><p>在Hacker News上关于这项工作的讨论中，<a href="https://news.ycombinator.com/item?id=36921808">有一位用户指出</a>"：</p><p></p><p></p><blockquote>别忘了，本研究的重点是，这些攻击不需要使用目标系统来开发。作者谈到，攻击是“通用的”，他们的意思是说，他们可以在自己的计算机上完全使用本地模型来生成这些攻击，然后将它们复制并粘贴到GPT-3.5中，并看到了有意义的成功率。速率限制并不能帮你避免这种情况，因为攻击是在本地生成的，而不是用你的服务器生成的。你的服务器收到的第一个提示已经包含了生成好的攻击字符串——研究人员发现，在某些情况下，即使是对GPT-4，成功率也在50%左右。</blockquote><p></p><p>&nbsp;</p><p>GitHub上提供了代码，你可以在AdvBench数据上重现<a href="https://github.com/llm-attacks/llm-attacks">LLM Attacks实验</a>"。项目网站上还提供了几个对抗性攻击的<a href="https://llm-attacks.org/">演示</a>"。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/llm-attack/">https://www.infoq.com/news/2023/08/llm-attack/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3CYMzjRwkdHv0jLcyGuK</id>
            <title>Meta AI是如何在 Facebook 和 Instagram 上增强用户体验的？</title>
            <link>https://www.infoq.cn/article/3CYMzjRwkdHv0jLcyGuK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3CYMzjRwkdHv0jLcyGuK</guid>
            <pubDate>Mon, 02 Oct 2023 00:00:00 GMT</pubDate>
            <updated>Mon, 02 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 人工智能系统在Instagram和Facebook的核心功能中发挥着重要作用。这些系统的目标是确保用户看到与他们相关且有价值的内容。Facebook和Instagram上有许多独立的人工智能系统，它们以极短的时间内无缝地提供个性化的体验。每个系统都有多个模型，用于识别内容并预测用户对其感兴趣或与之互动的可能性。为了提高透明度，Meta分享了22张系统卡，解释了这些人工智能系统的工作方式，并提供了定制选项供用户使用。这些系统卡描述了人工智能系统的概述、工作方式、定制显示内容的方法以及预测模型的影响。人们可以通过取消关注、隐藏内容或降低排名分数等方式来控制他们的体验。Meta还采用统一的方法来解释这些系统，并使用一致的术语词汇。未来，Meta将继续改进系统卡，以反映产品的变化和行业标准，并根据用户反馈赋予使用者更多权力。

总结: 人工智能系统在Instagram和Facebook的核心功能中发挥着重要作用。Meta分享了22张系统卡，解释了这些系统的工作方式，并提供了定制选项供用户使用。用户可以通过取消关注、隐藏内容或降低排名分数等方式来控制他们的体验。Meta将继续改进系统卡，以反映产品的变化和行业标准，并根据用户反馈赋予使用者更多权力。 </div>
                        <hr>
                    
                    <p></p><blockquote>为了帮助用户更好地理解人工智能在 Instagram 和 Facebook 许多核心功能中的作用，今天我们将分享有关我们的人工智能系统运作方式的详细信息。</blockquote><p></p><p>&nbsp;</p><p>我们利用人工智能来帮助每天使用我们服务的数十亿用户发现他们可能会觉得有用和有趣的内容，无论是在 Instagram 上关注新的创作者，还是在 Facebook 上可能喜欢的帖子。</p><p>&nbsp;</p><p>我们建立这些系统的目标是确保人们看到的内容能够与他们相关并且有价值。在 Facebook 和 Instagram 上，并没有一个单一的人工智能系统来决定用户所看到的一切。相反，许多独立的人工智能系统会分别工作，有时也会共同合作，在幕后以极短的时间内无缝地提供这些体验。更深入地了解，每个人工智能系统都有多个模型，用于识别内容并预测一个人对其感兴趣或与之互动的可能性有多大。</p><p>&nbsp;</p><p>作为 Meta 对透明度的承诺的一部分，今天我们分享了 22 张系统卡，其中包含了信息和可行性见解，每个人都可以使用这些信息来理解和定制他们在我们产品中特定的人工智能驱动体验。我们发布这些卡片是为了帮助人们更好地了解人工智能在 Instagram 和 Facebook 的许多功能中的作用，并解释人们的选择和行为如何通过我们的排名和推荐系统影响他们所看到的内容，比如新的视频或他们可能想要关注的创作者。系统卡现在在我们的<a href="https://transparency.fb.com/features/explaining-ranking">透明中心</a>"（Transparency Center）提供了 22 种语言的版本。</p><p>&nbsp;</p><p>对于使用 Facebook 和 Instagram 的人来说，能够<a href="https://ai.facebook.com/blog/responsible-ai-progress-meta-2022/">获得</a>"关于支持他们体验的技术的信息非常重要。这些信息也必须以一种非专家和专家都能理解的方式提供和解释。</p><p></p><h2>Meta AI 的系统卡</h2><p></p><p>&nbsp;</p><p>我们分享了 22 张系统卡，解释了人工智能驱动的推荐系统在 Facebook 和 Instagram 上的工作方式。其中包括 14 张关于 Facebook 的系统卡，包括 Facebook Feed、Feed 推荐、Feed 排名评论、Reels、Stories、视频、通知、市场、群组 Feed、单个群组 Feed、建议的群组、搜索、可能认识的人和可能喜欢的页面。另外还有 8 张关于 Instagram 的系统卡，包括 Instagram Feed、Feed 推荐、Stories、探索、Reels 串联、搜索、建议的账号和通知。</p><p></p><p>每个人工智能系统卡都包含四个部分：</p><p>&nbsp;</p><p>人工智能系统的概述；解释人工智能系统如何工作的部分，包括创建 Facebook 和 Instagram 体验的步骤概述；描述如何定制显示的内容的部分。包括系统控制的描述和每个人如何控制和定制他们的体验的说明；解释人工智能如何提供内容的部分，包括解释一些重要的预测模型如何影响整体人工智能系统并产生产品体验的说明。</p><p></p><p>人工智能系统的预测模型可能会使用一些信息，例如帖子的特征和一个人与类似帖子的互动历史，来预测兴趣水平。在 Facebook 和 Instagram 上有成千上万个这样的信号被使用。</p><p>&nbsp;</p><p>例如，当预测一个人是否会与一篇帖子互动时，人工智能系统会考虑以下信号：</p><p>&nbsp;</p><p>帖子或视频获得的赞数、评论数和观看次数；一个人观看视频或查看帖子的频率或时间长度；一个人与某个作者的互动情况，比如他们以前见过和喜欢过该作者的类似帖子的次数。</p><p>&nbsp;</p><p>重要的是，我们的系统卡还描述了每个人工智能系统的控制选项，人们可以使用这些选项来定制他们的体验。例如，如果有人想要看到某种类型的帖子更少，他们可以取消关注该作者，暂时隐藏内容，或在 Facebook 上点击 “Show less”（显示更少），在 Instagram 上点击 “Not interested”（不感兴趣），以临时降低类似内容的排名分数。</p><p></p><h2>我们创建系统卡的方法</h2><p></p><p>&nbsp;</p><p>在创建这些系统卡时，我们面临的最大挑战之一是找到以一种每个人都能理解的方式解释高度技术性信息的最佳方法。由于目前没有行业标准的方法，因此我们在 Meta 公司内部创建了一个<a href="https://ai.facebook.com/research/publications/system-level-transparency-of-machine-learning">统一的方法</a>"来解释这些系统。通过倾听我们的服务用户的意见，并与设计和开发过程中的多元化专家群体进行交流，我们获得了有助于确定如何以有意义的方式呈现这些信息的见解。我们听到人们希望能够更透明地了解和控制他们所看到的内容，因此我们在每个系统卡中添加了一个定制部分。我们还了解到，给人们过多的技术细节有时会模糊透明度，这就是为什么我们只呈现了最重要的十个预测模型，而不是系统中的全部内容。</p><p>&nbsp;</p><p>为了保持我们的方法一致，我们选择了一套术语词汇，用于讨论人工智能。在解释可能不熟悉的术语时，我们会包含一个工具提示，提醒人们我们所说的 “相关内容” 和 “人工智能系统” 的含义。通过采用一致的语言方法，我们使人们能够比较和对比多个系统卡。</p><p>&nbsp;</p><p>为了保持系统卡中解释我们的人工智能系统如何工作和提供内容的各个部分的一致性，我们开发了内部工具来分析构成人工智能系统的模型的影响。在我们工程师的帮助下，我们将这些信息从信号转化为文字，以帮助解释每个人工智能系统如何进行预测。值得注意的是，这些模型和信号是动态的，随着系统的学习而改变，并且随着时间的推移会经常发生变化。</p><p></p><h2>人工智能系统卡的未来</h2><p></p><p>&nbsp;</p><p>随着行业的发展和对系统文档和透明度的讨论的继续，我们将进一步识别机会，随着时间的推移对我们的方法进行迭代，以便反映产品的变化、不断发展的行业标准以及对人工智能透明度的期望。</p><p>&nbsp;</p><p>我们将继续整合来自多样化受众的反馈，改进我们的产品并赋予使用者更多权力。在我们的研究中，我们了解到人们希望在提供的信息与他们相关时，以文字和视觉的结合方式来探索系统卡，因此我们正在不断改进系统卡。</p><p>&nbsp;</p><p>人们可以通过访问我们的<a href="https://transparency.fb.com/features/explaining-ranking">透明中心</a>"找到我们的系统卡。我们希望这一努力能鼓励人们更多地了解 AI 如何支持他们的体验。我们相信，系统卡将使人们能够学习有关人工智能的知识，并控制和定制他们在使用我们产品时的体验。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://ai.meta.com/blog/how-ai-powers-experiences-facebook-instagram-system-cards/</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fv25HvvTwYfkGdOVFGQ7</id>
            <title>GitLab发布2023年全球DevSecOps报告，AI和ML从“有”变成“必须有”</title>
            <link>https://www.infoq.cn/article/fv25HvvTwYfkGdOVFGQ7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fv25HvvTwYfkGdOVFGQ7</guid>
            <pubDate>Sun, 01 Oct 2023 16:00:00 GMT</pubDate>
            <updated>Sun, 01 Oct 2023 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> GitLab, DevSecOps, AI, ML, software development, organizations, testing, impact, code checking, DevOps, DevSecOps methods, security, SDLC, toolchain complexity, developer productivity, release speed, business agility, budget, DevSecOps platform. 

总结: GitLab发布了2023年全球DevSecOps AI报告，报告显示AI和ML的使用正在从“有”发展到“必须有”。23%的组织已经在软件开发中使用AI，65%的受访者表示他们现在或将在未来三年内在测试中使用AI和ML。报告还显示，自2022年以来，DevOps和DevSecOps方法的采用率正在上升，开发人员和安全专业人员在谁应该带头解决安全问题上仍然存在争议。报告指出，提高开发者生产力、加快发布速度和提高业务敏捷性是扩展DevSecOps实践的关键原因。 </div>
                        <hr>
                    
                    <p>GitLab的<a href="https://about.gitlab.com/blog/2023/09/12/gitlab-global-devsecops-ai-report/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">2023年全球DevSecOps AI报告</a>"已发布，其中一个关键发现是<a href="https://en.wikipedia.org/wiki/Artificial_intelligence?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">AI</a>"和<a href="https://www.infoq.com/introduction-machine-learning/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">ML</a>"的使用正在从“有”发展到“必须有”。</p><p></p><p>报告显示，23%的组织已经在软件开发中使用AI，其中60%的组织每天都在使用AI。此外，65%的受访者表示，他们现在或将在未来三年内在测试中使用AI和ML。</p><p></p><p>83%的受访者表示，为了避免落后，在软件开发中使用AI至关重要。然而，也有约67%的受访者担心AI/ML所带来的影响，原因是AI/ML比人类更具成本效益优势，这会导致人类可从事的工作变少，并可能引入给他们带来麻烦的错误。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7dce13400a0b2380f6447c6bb01352bc.webp" /></p><p></p><p>虽然AI能够帮助开发者写代码，但这只占开发者工作时间的四分之一，剩下的时间花在其他任务上，这意味着AI有机会被用在写代码以外的领域。62%的受访者使用AI在正式测试流程之外检查代码，53%的受访者使用机器人测试代码。这两个数字同比增长均超过10%。</p><p></p><p>报告还显示，除了AI和ML，自2022年以来，DevOps和DevSecOps方法的采用率正在上升，从47%上升到56%。此外，DevSecOps正在脱离孤立的状态——只有30%的受访者表示他们需要对安全完全负责——低于一年前的48%。38%的安全专业人员认为他们是跨职能安全团队的一员，这一比例在一年前为29%。但是，开发人员和安全专业人员在谁应该带头解决安全问题上仍然存在争议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97ed30248b89bb787ce40281796ab566.webp" /></p><p></p><p>左移安全性检查的势头仍在，74%的受访者现在已经或计划在未来三年内在SDLC早期就进行测试，开发人员在编写代码阶段就发现漏洞（而不是在更后面）的情况显著增加。组织的首要投入重点仍然是云计算，但安全、治理和合规性现在是第二大关注点。</p><p></p><p>工具链复杂性仍然是一个问题，几乎三分之二的受访者希望简化他们使用的工具，因为大约一半的受访者所使用的工具链包含了六个或更多的工具。值得注意的是，这使得获得合规性和监控的整体视图，以及在工具链中获得洞见变得更加困难。</p><p></p><p>报告指出，提高开发者生产力、加快发布速度和提高业务敏捷性是扩展DevSecOps实践的关键原因。然而，只有15%的受访者认为去年的DevSecOps预算有所增加。DevSecOps平台继续受到关注，72%的受访者正在使用或将在明年使用，主要原因是为了提高效率、安全性和自动化。</p><p></p><p>GitLab的全球DevSecOps AI状态报告<a href="https://about.gitlab.com/developer-survey/#ai?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">可从其网站下载</a>"。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/09/gitlab-global-devsecops-ai/">https://www.infoq.com/news/2023/09/gitlab-global-devsecops-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/etaokYfgLHFXpFa5DpSK</id>
            <title>华为中秋节给员工发Mate60手机；商汤科技回应原知产总监被立案侦查；马斯克平均年终奖33亿元 | AI一周资讯</title>
            <link>https://www.infoq.cn/article/etaokYfgLHFXpFa5DpSK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/etaokYfgLHFXpFa5DpSK</guid>
            <pubDate>Sun, 01 Oct 2023 06:00:00 GMT</pubDate>
            <updated>Sun, 01 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 第四范式港股上市；微软发布Windows 11重大更新，包含Copilot和AI驱动画图工具；阿里分拆菜鸟独立上市；台积电AI芯片将涨价；抖音推出闪电搜索……

总结: 第四范式在港股上市，成为中国决策类人工智能平台领域的领先企业之一。微软发布了Windows 11的重大更新，其中包括Copilot和AI驱动画图工具。阿里巴巴计划将菜鸟分拆独立上市。台积电的AI芯片将面临涨价。抖音推出了一款名为“闪电搜索”的新产品，提供智能、丰富、极速的搜索体验。 </div>
                        <hr>
                    
                    <p></p><blockquote>第四范式港股上市；微软发布Windows 11重大更新，包含Copilot和AI驱动画图工具；阿里分拆菜鸟独立上市；台积电AI芯片将涨价；抖音推出闪电搜索……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>第四范式港股上市</h4><p></p><p></p><p>9月28日，第四范式（6682.HK）董事会主席、执行董事、首席执行官兼总经理戴文渊站在港交所和杨强一起敲响了第四范式IPO的铜锣。除了担任第四范式联合创始人、非执行董事外，杨强还是戴文渊的博士生导师。两位都是迁移学习领域的领军人物，引导全球迁移学习的研发方向。</p><p></p><p>戴文渊表示：“自成立以来，第四范式一直致力于助力企业智能化转型和推进人工智能发展，成长为决策类AI平台领先企业。今天在港交所成功挂牌上市，对第四范式而言是一个重要的里程碑。非常感谢我们所有员工一直以来的努力，也感谢各级领导、投资人、客户以及合作伙伴给予我们的信任。未来，我们将继续巩固自身技术壁垒，不断提升商业化能力，持续助力客户业务成功。”</p><p></p><p>第四范式成立于2014年，按2022年相关收入计算，中国决策类人工智能平台领域的前五大参与者合计占约56.1%市场份额，第四范式以22.6%的份额位列市场第一。</p><p></p><h4>微软发布Windows 11重大更新，包含Copilot和AI驱动画图工具</h4><p></p><p></p><p>9月27日消息，微软今天发布了 Windows 11 的重大更新，本次更新包括全新的Windows Copilot、用于Paint、Snipping Tool和Photos 的AI增强功能以及RGB照明支持等。其中，Windows Copilot是本次更新最主要的功能，它将作为一款基于AI的数字助手存在，可以把Bing Chat功能直接带到Windows 11桌面，以侧边栏的形式出现，用户能用它控制 PC 上的设置、启动应用程序或简单的回答查询。</p><p></p><h4>ChatGPT发布重大更新，可在几秒钟制作出逼真的合成语音</h4><p></p><p></p><p>据OpenAI官网9月25日消息，OpenAI宣布在接下来的两周内，将在ChatGPT中向Plus和Enterprise用户推出语音和图像。语音将在iOS和Android上推出（在设置中选择加入），图像将在所有平台上提供。</p><p></p><p>OpenAI称，这项新的语音技术能够从几秒钟的真实语音中制作出逼真的合成语音，为许多有创意和无障碍的应用打开了大门。然而，这些功能也带来了新的风险，例如恶意行为者可能冒充公众人物或实施欺诈。OpenAI 表示，这种模型不会被广泛开放，而是会受到严格的控制和限制。</p><p></p><h4>阿里分拆菜鸟独立上市</h4><p></p><p></p><p>9月26日，阿里巴巴在港交所发布公告称，公司拟通过以菜鸟股份于香港联交所主板独立上市的方式分拆菜鸟。根据第15项应用指引，公司已就拟议分拆向香港联交所提交分拆计划，且香港联交所已确认公司可进行拟议分拆。</p><p></p><p>现建议，拟议分拆将以菜鸟股份全球发售的方式进行（包括香港公开发售及国际发售）。拟议分拆完成后，公司将继续持有菜鸟50%以上的股份，因此，菜鸟将仍为公司的子公司。</p><p></p><p>此外，菜鸟集团副总裁 、国际快递事业部总经理丁宏伟在26日宣布菜鸟全球五日达服务正式上线，这是电商行业首个规模化落地的跨境电商快线产品，接下来会在英国、西班牙、荷兰、比利时、韩国5个亚欧国家全量上线。</p><p></p><h4>商汤科技回应原知产总监被立案侦查</h4><p></p><p></p><p>近日，市场有消息称，商汤科技知识产权执行总监高某某涉嫌非国家工作人员受贿罪，被立案侦查，采取强制措施。</p><p></p><p>9月28日，商汤发言人表示，原商汤知识产权执行总监高某某因经济问题已被公安机关采取强制措施。由于涉嫌刑事犯罪，警方正在侦查中，一切将以警方披露信息为准。商汤科技始终严格遵守商业道德和规范，对腐败等违法违规行为“零容忍”。</p><p></p><p>公开资料显示，高某某在2017年加盟商汤科技集团，任公司出口管制合规官和知识产权执行总监，负责商汤集团出口管制合规和知识产权事务。</p><p></p><h4>亚马逊投资OpenAI头号对手40亿美元</h4><p></p><p></p><p>当地时间9月25日，亚马逊宣布将向人工智能初创公司Anthropic投资40亿美元，并持有其部分股权。Anthropic已经开发了聊天机器人Claude，被认为是OpenAI和谷歌在生成式AI产品上的主要竞争对手。</p><p></p><p>亚马逊和Anthropic表示，这项新的战略合作将结合双方在更安全的生成式AI领域的技术和专业知识，加速Anthropic未来基础模型的开发，并将其广泛提供给亚马逊云科技的客户使用。</p><p></p><p>Anthropic由OpenAI（ChatGPT的开发机构）前研究副总裁达里奥·阿莫迪（Dario Amodei）、大语言模型GPT-3论文的第一作者汤姆·布朗（Tom Brown）等人于2021年在美国旧金山共同创立。其创始成员此前多为OpenAI的核心员工，他们曾经深度参与过GPT-3等多项研究。</p><p></p><h4>微软招募核能专家，拟用小型核反应堆为AI提供动力</h4><p></p><p></p><p>根据一份新的招聘启事，微软公司正在研究使用下一代核反应堆来为其数据中心和人工智能提供动力。</p><p></p><p>这则发布于9月25日的招聘启事称，微软董事长兼首席执行官萨蒂亚·纳德拉 （Satya Nadella）表示：“随着微软云将世界上最先进的人工智能模型转变为新的计算平台，下一波计算浪潮正在诞生。”“我们致力于帮助客户使用我们的平台和工具，在今天以更少的资源做更多的事情，并在人工智能新时代为未来进行创新。”因此，微软正在寻找一位核技术首席项目经理，负责完善和实施全球小型模块化反应堆（SMR）和微反应堆能源战略。</p><p></p><p>招聘启事描述道，该高级职位的任务是领导SMR和微反应器集成的技术评估，为微软云和人工智能所在的数据中心提供动力。</p><p></p><h4>抖音推出闪电搜索</h4><p></p><p></p><p>日前，抖音推出了一款“闪电搜索”APP，其Slogan为“闪电搜索，快如闪电”。官方介绍称，闪电搜索是一款抖音集团旗下的全新搜索工具产品，为用户提供智能、丰富、极速的搜索体验；在这里，可以搜索小说、影视、故事、音乐内容，医疗与生活服务工具一应俱全，同时配备金币激励模式，在端内阅读和搜索都能获得金币奖励。据了解，这是继头条搜索、悟空搜索、抖音搜索之后，抖音启用的第四个搜索品牌。</p><p></p><h4>台积电AI芯片将涨价</h4><p></p><p></p><p>据IT之家报道，随着台积电供应链扩大CoWoS先进封装产能，这些中间膜价格的上涨最终将推高该公司生产的AI芯片成本。由于对AI产品的强劲需求，台积电正在投资数十亿美元升级其封装产能。业界人士透露，追加设备进驻厂房后，台积电的月产能可达2.5万片以上、甚至朝3万片靠拢，从而令台积电承接AI相关订单能力上升，由于相关产能升级，台积电的AI芯片也将迎来涨价。</p><p></p><p>此外，台媒指出，英伟达是目前台积电CoWoS先进封装最大的客户，订单量占产能六成，近期因应AI运算强劲需求，英伟达扩大下单，而其他客户的急单也开始涌现。</p><p></p><h4>阿里云通义千问 14B 模型开源</h4><p></p><p></p><p>9 月 25 日，阿里云开源通义千问 140 亿参数模型 Qwen-14B 及其对话模型 Qwen-14B-Chat，支持免费商用。</p><p></p><p>据了解，此次开源的 Qwen-14B 是一款支持多种语言的高性能开源模型，相比同类模型使用了更多的高质量数据，整体训练数据超过 3 万亿 Token，使模型具备更强大的推理、认知、规划和记忆能力。Qwen-14B 最大支持 8k 的上下文窗口长度。而 Qwen-14B-Chat 则是在基座模型上经过精细 SFT 得到的对话模型。借助基座模型强大的性能，Qwen-14B-Chat 生成内容的准确度大幅提升，也更符合人类偏好，内容创作上的想象力和丰富度也有显著扩展。</p><p></p><h4>百川智能发布Baichuan2-53B，开放API全面进军To B领域</h4><p></p><p></p><p>9月25日，百川智能发布Baichuan2-53B 闭源大模型，全面升级了Baichuan1-53B的各项能力。Baichuan2-53B不仅数学和逻辑推理能力提升显著，还通过高质量数据体系和搜索增强极大降低了模型幻觉。</p><p></p><p>作为首批通过备案的大模型企业，百川智能此次还开放了Baichuan2-53B API接口，正式进军To B领域，开启商业化进程。</p><p></p><h4>韩国互联网公司 Kakao、Com2uS 正在裁减元宇宙团队</h4><p></p><p></p><p>据 Newsis 报道，由于 2023 年上半年出现巨额亏损，韩国顶级互联网公司 Kakao 和游戏开发商 Com2uS 正在缩减其元宇宙团队规模。</p><p></p><p>据悉，Com2uS 是一家主要的移动游戏开发商，去年的收入为 5.36 亿美元，最近缩减了其元宇宙部门的员工规模，有关裁员的细节尚未透露。该公司的元宇宙平台 Com2Verse 于今年 8 月正式上线。然而，元宇宙平台背后的业务部门在今年上半年运营亏损约 620 万美元。</p><p></p><p>Kakao Games 及 Neptune 旗下的元宇宙公司 Colorverse 则在 2022 年亏损 860 万美元，并已在今年早些时候进行了一轮裁员。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>华为中秋节给员工发 Mate60 手机</h4><p></p><p></p><p>近日，华为员工在社交媒体上曝光了他们在中秋节收到的礼物，其中最引人瞩目的是一部华为Mate60手机。除了华为 Mate60 手机之外，这份中秋节礼物还包括了 6 个月饼、茶壶、茶碗、茶叶等。</p><p></p><p>此外，华为终端BG大中华区还向员工们发放了一封感谢信，对他们的辛勤付出表示感激，并希望他们继续学习和使用华为产品，成为产品专家，为消费者提供优质的服务。</p><p></p><p>网友热评：又是别人的公司。</p><p></p><h4>马斯克平均年终奖达33亿人民币</h4><p></p><p></p><p>国外调查机构 The Stock Dork 近期对全球市值前 50 家企业的首席执行官在过去五年间的年终奖金进行了分析。这些年终奖包括股票期权、红利、奖金和薪酬等收入。 特斯拉的首席执行官马斯克是全球薪酬最高的 CEO，平均年终奖高达4.567亿美元（折合当前约33.38亿元人民币）。马斯克的这一成果得益于他在2018年获得了22.3亿美元的巨额一次性股票期权奖金，这是有史以来给予首席执行官的最大奖金。</p><p></p><h4>GitHub CEO：AI无法取代程序员</h4><p></p><p></p><p>GitHub 首席执行官 Thomas Dohmke 最近在公开场合分享了他对于人工智能和软件开发之间关系的看法。Thomas Dohmke 坚持自己的观点 —— 滚雪球式的人工智能革命不会给软件开发行业敲响丧钟。Dohmke 说道，行业对软件开发者的需求将继续超过供应。事实上，Doohmke 和许多其他技术领导者一样，长期以来一直坚持认为 Copilot 等 AI 工具只是用于提高开发者的工作效率，而不是取而代之。</p><p></p><h4>国企领导称取消周末，官方通报！</h4><p></p><p></p><p>近日，多张工作群内的聊天截图在引发关注。</p><p></p><p>网传江西某国企建筑设计院院长温某在钉钉群的发言，因员工在周六有半天未看到消息，称“千万不要以为周六周日就是非工作日，不能有传统机关周末的概念”“我个人真的特别讨厌那种，一到周末就非工作状态，只顾享受，而不懂享受的源头是什么的人”“工资是按月给的，不是按22天给的”，并个人初步决定明年开始实行周六周日工作制度。</p><p></p><p>据潇湘晨报，9月26日，江西省建工集团有限责任公司纪律检查委员会的一名工作人员告诉记者，目前江西建工一建公司的纪委已经在处理了。温某的言论仅为他个人观点，不代表公司和集团的观点。</p><p></p><p>随后，江西省建工集团有限责任公司发布情况通报。通报称，近日，我司子公司所属二级单位（设计研究院）院长温衍彬，在工作群内发表不当言论被截屏转载，引发网络关注。我司高度重视，立即对该事件进行了解核实。经查，9月23日（星期六），温衍彬因布置的工作任务未得到及时回复，在设计研究院工作群内发表不当言论，造成不良影响，我司将按照有关规定进行处理。同时，我们将深刻汲取教训，进一步加强干部教育管理，严格遵守《劳动合同法》等有关法律法规，切实保障职工权益。感谢社会各界的监督。</p><p></p><p>网友热评：打工人不配拥有周末吗？</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2fI2S5hqu4NzwfiqEDYo</id>
            <title>加速机器学习模型开发：AirBnb利用Chronon实现特征工程</title>
            <link>https://www.infoq.cn/article/2fI2S5hqu4NzwfiqEDYo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2fI2S5hqu4NzwfiqEDYo</guid>
            <pubDate>Sun, 01 Oct 2023 00:00:00 GMT</pubDate>
            <updated>Sun, 01 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> AirBnb, Chronon, feature engineering, machine learning, infrastructure, data transformation, ETL logic, reproducibility, data computation, data sources, SQL-like operations, aggregation, Kafka, Spark, Hive, Airflow, Python API, temporal accuracy, snapshot accuracy, GitHub, feature engineering pipeline. 

总结: AirBnb构建了名为Chronon的解决方案，用于机器学习模型的特征工程。Chronon解决了从原始数据到特征转换的复杂性和耗时性问题，并确保推理特征分布与训练时相同。Chronon支持从不同数据源获取数据，并使用类似SQL的操作和聚合进行转换。它还提供Python API，可以进行时间聚合和窗口操作。Chronon还考虑了特征值更新的准确性，用户可以选择实时更新或固定时间间隔更新。目前还不清楚AirBnb是否会在GitHub上提供Chronon。 </div>
                        <hr>
                    
                    <p>AirBnb经常要创建用于机器学习模型的新特征，为了提高生产力和可扩展性，他们构建了一个名为<a href="https://medium.com/airbnb-engineering/chronon-a-declarative-feature-engineering-framework-b7b8ce796e04">Chronon</a>"的解决方案，用于创建将原始数据转换为特征并进行训练和推理所需的基础设施。</p><p>&nbsp;</p><p>AirBnb工程师兼Chronon创始人Nikhil Simha解释说，将原始数据转换为特征并用于训练ML模型是一项复杂且耗时的任务，工程师需要从AirBnb数据仓库中提取数据，并编写复杂的ETL逻辑将其转换为特征。另一个难点在于要确保这个逻辑所生成的推理特征分布与训练时的相同。</p><p>&nbsp;</p><p>Simha说，Chronon就是为了解决这些问题，使机器学习工程师在训练和推理中以可复制的方式定义特征并中心化数据计算。</p><p></p><p></p><blockquote>作为用户，你只需要声明一次计算，Chronon就会生成所需的所有基础设施，不断地将原始数据转换为训练和服务所需的特征。AirBnb的机器学习从业者不用再花费数月的时间手动实现复杂的管道和特征索引。通常，他们用不到一周的时间就可以为他们的模型生成新的特征集。</blockquote><p></p><p>&nbsp;</p><p><a href="https://central.sonatype.com/namespace/ai.chronon">Chronon</a>"的第一个组件支持从各种数据源获取数据，包括事件数据源、实体数据源和累积事件源，从每个数据源收集不同类型的数据。</p><p>&nbsp;</p><p>摄取数据后，它就可以使用类似SQL的操作和聚合进行转换，从而生成服务于在线模型的低延迟端点，以及用于离线训练的Hive表。在底层，Chronon使用Kafka、Spark/Spark Streaming、Hive和Airflow来构建管道。类似SQL的操作包括GroupBy、Join和StagingQuery，它们是Spark SQL查询，每天脱机计算一次。聚合包括窗口、桶和基于时间的聚合。</p><p>&nbsp;</p><p>最后，它还有一个Python API，提供了类似SQL的原语，并将基于时间的聚合和窗口作为一级概念。例如，使用Python API，你可以过滤和转换用户在过去五个小时内查看某个物品的次数。</p><p>&nbsp;</p><p>Chronon有一个重要的概念是准确性，即特征值更新的频率，是实时更新还是固定时间间隔更新。要根据特定的用例选择合适的准确性，因此，Chronon让用户可以方便地将计算的准确性设为为temporal或snapshot。</p><p>&nbsp;</p><p>在写这篇文章的时候，我还不知道AirBnb是否会在GitHub上提供Chronon，但如果你想创建自己的特征工程管道，可以读下原文中的讨论，非常有趣。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/airbnb-chronon-ml-features/">https://www.infoq.com/news/2023/08/airbnb-chronon-ml-features/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EGdxPRvIZexgBCvtaeJP</id>
            <title>生成式AI碳排放堪比开车往返月球？这个问题该如何解决</title>
            <link>https://www.infoq.cn/article/EGdxPRvIZexgBCvtaeJP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EGdxPRvIZexgBCvtaeJP</guid>
            <pubDate>Sat, 30 Sep 2023 00:00:00 GMT</pubDate>
            <updated>Sat, 30 Sep 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 生成式人工智能的发展正在改变行业和社会。它可以用于码字、写代码、生成图像、视频、音乐和语音等内容。人们对这项技术的威力和潜在价值进行讨论，同时也关注其可能带来的风险和威胁。人们还关注这些模型的能源使用和碳排放。有人认为训练生成式人工智能模型的碳排放量很大，但实际上全球科技行业的温室气体排放中只有一小部分是由人工智能造成的。虽然我们尚不清楚究竟训练了多少大型人工智能模型，但其规模与航空或其他碳排放源相比仍存在巨大差异。总的来说，生成式人工智能的发展对我们的生活产生了明显影响，但其碳排放问题可能被夸大了。 </div>
                        <hr>
                    
                    <p>生成式人工智能的发展正在改变我们的行业和社会。像ChatGPT和CoPilot这样的语言模型可以码字和写代码，图像和视频生成模型可以根据简单的提示词生成引人注目的内容，音乐和语音模型可以轻松地合成任何人的声音，并创作出复杂的音乐。</p><p></p><p>世界各地都在讨论这项技术的威力和潜在价值。与此同时，人们也在谈论它可能带来的风险和威胁。</p><p></p><p>从对超级智能人工智能消灭人类的担忧，到对歧视的进一步自动化以及对仇恨和错误信息被进一步放大的担忧，人们正在努力评估和减轻这项新技术的潜在负面影响。</p><p></p><p>人们也越来越关注这些模型的能源使用和相应的碳排放。最近几个月又出现了一些关于碳排放的比较。</p><p></p><p>例如，在一篇文章中，作者戏谑<a href="https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">训练GPT-3的碳排放等同于开车往返月球</a>"，另一篇文章则解释说<a href="https://www.forbes.com/sites/glenngow/2020/08/21/environmental-sustainability-and-ai/?sh=314a59cc7db3&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">训练人工智能模型比飞机长途飞行排放更多的碳</a>"。</p><p></p><p>最终的影响将取决于这项技术是如何被使用的，以及它在多大程度上融入了我们的生活。</p><p></p><p>我们很难准确预测它将如何影响我们的日常生活，但目前已经有一个很明显的例子，即搜索巨头将生成式人工智能<a href="https://blog.google/products/search/search-generative-ai-tips/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">集成</a>"到他们的产品中。</p><p></p><p>Wired网站最近的一篇<a href="https://www.wired.com/story/the-generative-ai-search-race-has-a-dirty-secret/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">文章</a>"写道：</p><p></p><p></p><blockquote>加拿大数据中心公司QScale联合创始人Martin Bouchard表示，基于他对微软和谷歌搜索发展计划的了解，在搜索过程中添加生成式人工智能需要让“每次搜索至少增加4到5倍的计算量”。</blockquote><p></p><p></p><p>显然，生成式人工智能技术是不容忽视的。</p><p></p><p></p><h2>生成式人工智能的碳排放是否被夸大了？</h2><p></p><p></p><p>人们对生成式人工智能碳排放的担忧可能被放大了。我们要正确看待这个问题：全球整个科技行业的温室气体排放量占全球温室气体排放量的1.8%至3.9%，但其中只有一小部分是由人工智能[1]造成的。人工智能与航空或其他碳排放源的规模存在巨大差异：每天都会有许多汽车和飞机行驶数百万公里，但训练像GPT模型这样的现代人工智能模型的次数却相对较少。</p><p></p><p>诚然，我们尚不清楚究竟人类究竟训练了多少大型人工智能模型，这取决于我们如何定义“大型人工智能模型”。如果说大模型指的是GPT-3或更大规模的模型，那么可能只有不到1000个。我们来做一个简单的数学运算：</p><p></p><p></p><blockquote>根据最近的一项<a href="https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">估计</a>"，训练GPT-3排放了500吨的二氧化碳。Meta的LLaMA模型<a href="https://www.google.com/url?q=https://arxiv.org/pdf/2302.13971.pdf&amp;sa=D&amp;source=docs&amp;ust=1686736622103395&amp;usg=AOvVaw0EOH1_3alOGQm9Nh_pUddt&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">估计</a>"排放了173吨。训练1000个500吨的模型将涉及约50万吨二氧化碳的总排放量。新模型可能会在一定程度上增加排放量，但几乎可以肯定的是，这1000个模型的碳排放被高估了。2019年，商业航空业排放了约9.2亿吨二氧化碳[2]， 几乎是大模型训练的2000倍，而且这是将一年的航空业碳排放与多年的大模型训练碳排放进行的比较。大模型训练的碳排放仍然不容忽视，但这种戏剧性的比较具有误导性。我们需要更细致入微的思考。</blockquote><p></p><p></p><p>当然，这只是考虑到这些模型的训练。模型的服务和使用也需要能源，也会有相关的碳排放。<a href="https://towardsdatascience.com/the-carbon-footprint-of-chatgpt-66932314627d?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">一项分析</a>"表明， ChatGPT运行一年可能会排放约15000吨二氧化碳。<a href="https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">另一项分析</a>"则表明实际的排放量要要少得多，约为1400吨。尽管都不可忽略，但与航空业相比仍然微不足道。</p><p></p><p></p><h2>碳排放透明度</h2><p></p><p></p><p>但是，即使对人工智能碳排放的担忧有些言过其实，但仍然值得我们关注，特别是当生成式人工智能越来越多地融入到我们的现代生活中时。随着人工智能系统的不断演化和投用，我们需要关注它们对环境的影响。我们可以遵循许多成熟的实践，也有一些方法可以减少生成式人工智能的碳排放。</p><p></p><p>首先，透明度至关重要。我们建议提高透明度，便于监测与人工智能模型的训练和使用相关的碳排放。这将使那些部署这些模型的人以及最终用户能够根据人工智能的碳排放量做出明智的决定。同时，将人工智能相关的碳排放纳入温室气体清单和净零目标。这是<a href="https://foundation.mozilla.org/en/research/library/ai-transparency-in-practice/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">人工智能透明度</a>"的一个组成部分。</p><p></p><p>举个这方面的例子，法国<a href="https://www.euractiv.com/section/digital/news/new-law-forces-french-operators-to-disclose-carbon-footprint-to-public/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">最近通过了一项法律</a>"，要求电信公司就其在可持续性方面的措施提供透明度报告。我们也可以出台类似的法律，例如要求包含人工智能系统的产品向其客户报告碳排放信息，并要求模型提供商将碳排放数据集成到其API中。</p><p></p><p>更高的透明度可以带来更强的动力来建立节能的生成式人工智能系统，我们有很多方法可以提高效率。在InfoQ最近发布的一篇<a href="https://www.infoq.com/articles/impact-machine-learning-climate/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">文章</a>"中，微软高级软件工程师Sara Bergman呼吁人们注意人工智能系统的生命周期，并建议如何应用<a href="https://greensoftware.foundation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">绿色软件基金会</a>"的工具和实践来提高人工智能系统的能源效率，包括如何选择服务器硬件和架构，以及选择低碳密集型电力的时段和地区。而生成式人工智能为提高效率提供了一些独有的可能性。</p><p></p><p></p><h2>效率：能源使用与模型性能</h2><p></p><p></p><p>正如<a href="https://arxiv.org/pdf/2302.08476.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">碳计算：影响机器学习排放的因素调查</a>"中所探讨的那样，与训练或使用生成式人工智能模型相关的碳排放取决于许多因素，包括：</p><p></p><p>模型参数个数；量化(数值精度)；模型架构；使用GPU或其他硬件的效率；电力的碳密度。</p><p></p><p>后两个因素与其他软件一样，人们已经对其进行了探讨，例如我们上面提到InfoQ的<a href="https://www.infoq.com/articles/impact-machine-learning-climate/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">文章</a>"。因此，我们这里关注的是前三个因素，它们都涉及能源使用和模型性能之间的一些权衡。</p><p></p><p>效率的价值不仅体现在可持续性方面，更高效的模型可以在可用数据较少的情况下提供高效的表现性能、降低成本，并有在边缘设备上运行的可能性。</p><p></p><p></p><h3>模型参数个数</h3><p></p><p></p><p>OpenAI论文<a href="https://arxiv.org/pdf/2005.14165.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">Language Models are Few-Shot Learners</a>"中的一张图告诉我们，模型越大表现越好。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/30/306f6fe67271b428b99facc4841a4f9a.webp" /></p><p></p><p></p><p><a href="https://openreview.net/pdf?id=yzkSU5zdwD&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">大型语言模型的涌现能力</a>"中也提到了同样的观点：</p><p></p><p></p><blockquote>扩大语言模型已经被证明可以在广泛的下游任务上可预见地提高性能和样本效率。本文讨论的是一种不可预测的现象，我们称之为大型语言模型的涌现能力。如果一种能力在较小的模型中不存在，但在较大的模型中存在，我们称之为”涌现“。</blockquote><p></p><p></p><p>我们发现，不仅更大的模型在处理给定的任务时表现得更好，而且实际上只有当模型变得更大时，才会涌现出新的能力。这种能力涌现的例子包括对大数的加减、毒性分类和数学单词问题的思维链技术。</p><p></p><p>但是训练和使用更大的模型需要更多的计算，因此需要更多的能源。因此，我们可以看到模型的能力和性能与其计算强度之间的权衡，进而可以看出与碳密度的关系。</p><p></p><p></p><h3>量化</h3><p></p><p></p><p>我们对模型的<a href="https://huggingface.co/docs/optimum/concept_guides/quantization?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">量化</a>"进行了大量研究。我们在模型计算中使用低精度数字来降低计算密度，尽管会牺牲一些精度。它允许模型在普通的硬件上运行，例如，在消费级笔记本电脑上。减少计算量和降低精度之间的权衡通常是非常有利的，对于特定能力水平的计算，量化模型非常节能。还有一些相关的技术，如“<a href="https://aitechtrend.com/the-power-of-knowledge-distillation-in-creating-smaller-and-faster-models/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">蒸馏（Distillation）</a>"”，它使用较大的模型来训练小模型，这个小模型可以很好地完成给定的任务。</p><p></p><p>蒸馏技术需要训练两个模型，因此很可能会增加与模型训练相关的碳排放，不过它会通过减少模型在使用中的碳排放来弥补。对已训练好的模型进行蒸馏也是一个很好的解决方案，我们甚至可以同时利用蒸馏和量化来为给定的任务创建更高效的模型。</p><p></p><p></p><h3>模型架构</h3><p></p><p></p><p>模型架构对计算密度有很大的影响，因此选择更简单的模型可能是减少人工智能系统碳排放最有效的方法。GPT的Transformer非常强大，越是简单的架构就越可以有效地用于许多应用程序。像ChatGPT这样的模型被认为是“通用的”，这意味着这些模型可以被用于许多不同的应用程序。然而，对于相对固定的应用程序来说，就没有必要使用复杂的模型。为任务定制的模型可以使用更简单和更小的架构达到所需的性能，从而减少碳排放。另一种有用的方法是微调——论文<a href="https://arxiv.org/pdf/2205.05638.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a>"讨论了微调如何“提供更好的准确性以及显著降低计算成本”。</p><p></p><p></p><h2>将碳排放和精度指标放在一起</h2><p></p><p></p><p>“准确性”一词很容易让我们陷入“越多越好”的认知。我们需要明白的是，对于特定的应用程序来说——“适可而止”才是最好的。在某些情况下，可能需要最新和最好的模型，但对于有些应用程序来说，老的、较小的模型（可能是量化的模型）可能就完全足够了。在某些情况下，系统做出正确的行为可能需要所有可能的输入，但有些应用程序可能具有更强的容错能力。在正确地了解了所需的应用程序和服务级别之后，可以通过比较各种模型的性能和碳排放量来选择合适的模型。也可能存在使用一组模型的情况。默认情况下将请求传给更简单、更小的模型，对于简单模型无法处理的任务，可以将其传给更复杂的模型。</p><p></p><p>为此，我们需要将碳排放指标集成到DevOps(或MLOps)流程中。一些工具，如<a href="https://codecarbon.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">codecarbon</a>"，可以很容易地跟踪和计算与模型训练和服务相关的碳排放。将这个工具或类似的工具集成到持续集成测试套件中，可以同时分析碳排放、计算精度和其他指标。例如，在试验模型架构时，测试用例可以立即报告精度和碳排放，从而更容易找到正确的架构并选择正确的参数，以满足精度要求，同时最大限度地减少碳排放。</p><p></p><p>同样需要注意的是，实验本身也会导致碳排放。在MLOps周期的实验阶段，使用不同的模型和架构进行实验，以确定最佳选择，我们可以综合考虑准确性、碳排放和潜在的其他指标。从长远来看，随着模型不断接受实时数据的训练和/或投用，可以减少碳排放，但过度的实验会浪费时间和精力。做出权衡将取决于许多因素，但当碳排放指标可用于实验以及模型的训练和服务时，我们的工作会变得更容易。</p><p></p><p></p><h2>绿色提示词工程</h2><p></p><p></p><p>说到与生成式模型的服务和使用相关的碳排放时，提示词工程就变得不可忽视了。对于大多数生成式人工智能模型(如GPT)来说，使用的计算资源和碳排放取决于传给模型和由模型生成的文本节点的数量。</p><p></p><p>虽然具体的细节取决于实际的实现，但提示词通常会被“一次性全部”传给模型。这可能会使计算量看起来不依赖于提示词的长度，但实际上，自注意力机制特点决定了优化会抑制未使用的输入部分，这意味着更短的提示词可以节省计算量，从而节省能源。</p><p></p><p>对于输出，很明显，计算成本与生成的文本节点数量成正比，因为模型需要为生成的每个节点“再运行”一次。</p><p></p><p>这可以从访问OpenAI GPT4 API的费用中看出来。在撰写本文时，GPT4基础模型的成本为0.03美元每千个提示词节点和0.06美元每千个样本节点。提示词长度和输出节点的长度都包含在价格中，说明了两者都会影响所需的计算量。</p><p></p><p>因此，更短的提示词和生成更短的输出将使用更少的计算量。人们为此提出了一种“绿色提示词工程”提议。MLOps平台为此提供了一些实验性支持，这让在持续评估碳排放和系统性能影响的同时进行缩短提示词的实验变得相对容易。</p><p></p><p>除了提示词，人们还提出了一些有趣的方法，通过更复杂的使用方式来改进大模型的效率，如这篇<a href="https://arxiv.org/abs/2305.18323?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">论文</a>"所述。</p><p></p><p></p><h2>结论</h2><p></p><p></p><p>尽管人工智能的碳排放可能被夸大了，但仍然令人感到担忧，我们需要采取适当的措施来应对它们。提高透明度是支持有效决策和提高消费者意识的必要条件。此外，将碳排放指标集成到MLOps工作流中有助于在进行模型架构、规模、量化和绿色提示词工程时做出更明智的选择。本文的内容只是概述，只触及表面，对于那些真正想要做绿色生成式人工智能的人，可以关注<a href="https://arxiv.org/pdf/2209.00099.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">最新的研究</a>"。</p><p></p><p></p><h3>脚注</h3><p></p><p></p><p>[1] <a href="https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">"We’re getting a better idea of AI’s true carbon footprint" - by Melissa Heikkilä</a>"[2] <a href="https://www.statista.com/statistics/1056469/co2-emissions-commercial-aviation-industry-globally-by-operation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">Carbon dioxide emissions from the commercial aviation industry worldwide in 2019, by operation</a>"</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/articles/carbon-emissions-generative-ai/">https://www.infoq.com/articles/carbon-emissions-generative-ai/</a>"</p><p></p><p>相关阅读：</p><p><a href="https://www.infoq.cn/news/NuKxISZRb5sjg1lXgmeN">AI 大模型背后的惊人数字：问 ChatGPT 5 个问题，耗水 500 毫升？训练一次 GPT-3，碳排放量相当于开车往返月球？</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5eIYlyxDCEUz1M58oATm</id>
            <title>Amazon CodeWhisperer 与 Amazon Glue 实现集成，借助生成式 AI 进一步提升开发效率</title>
            <link>https://www.infoq.cn/article/5eIYlyxDCEUz1M58oATm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5eIYlyxDCEUz1M58oATm</guid>
            <pubDate>Fri, 29 Sep 2023 10:00:00 GMT</pubDate>
            <updated>Fri, 29 Sep 2023 10:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> Amazon Glue是亚马逊云科技推出的一个无服务器基础设施，用于集成多个数据来源，以用于分析、机器学习和应用程序开发。亚马逊云科技最近宣布了Amazon CodeWhisperer，这是一个AI编程助手，可以根据开发人员的注释和历史代码生成代码建议。现在，Amazon CodeWhisperer为Amazon Glue Studio notebook提供支持，帮助开发人员更快地进行数据集成工作。该集成已在美国东部（北弗吉尼亚州）可用。 </div>
                        <hr>
                    
                    <p>数据对于企业做出明智决策、提高运营效率和开展创新来说至关重要。集成不同来源的数据是一个复杂而耗时的过程。为此，亚马逊云科技推出了&nbsp;<a href="https://xie.infoq.cn/article/be592baac4db6412394a6573f">Amazon Glue</a>"，帮助用户在无服务器基础设施上集成多个来源的数据，用于分析、机器学习和应用程序开发。</p><p></p><p>Amazon Glue 为数据集成任务提供了完全不同的编写体验，而&nbsp;Notebook 是最常见的工具之一。数据科学家倾向于以交互方式运行查询，并立即检索结果，用于编写数据集成任务。这种交互体验可以加速构建数据集成任务的进度。</p><p></p><p>近期，亚马逊云科技宣布了 <a href="https://www.infoq.cn/video/4oajrgIyfmkaaNFi7dJF">Amazon CodeWhisperer</a>" 正式可用。这是一款 <a href="https://www.infoq.cn/article/CkLkSpx0p9egiR1XLsON">AI 编程助手</a>"，能够使用底层基础模型帮助开发人员提高工作效率。它可以根据开发人员使用自然语言留下的注释和 IDE（集成开发环境）中的历史代码实时生成代码建议。此外，亚马逊云科技还发布了&nbsp;Amazon CodeWhisperer Jupyter 扩展程序，为 Jupyter 用户在 Jupyter Lab 和 Amazon SageMaker Studio 中的 Python notebook 生成实时、单行或完整的函数代码建议。</p><p></p><p>现在，亚马逊云科技正式宣布&nbsp;Amazon CodeWhisperer 为 Amazon Glue Studio notebook 提供支持，帮助 Amazon Glue 用户优化使用体验、提高开发效率。通过 Amazon Glue Studio notebook，开发人员可以用自然语言（英语）编写特定任务，比如“利用 json 文件中的内容创建一个 Spark DataFrame”。基于此信息，Amazon CodeWhisperer 会直接在 notebook 中推荐一个或多个可完成此任务的代码片段。开发人员可以选择“接受最推荐的建议”，“查看更多建议”或“继续自己编写代码”。</p><p></p><p>Amazon Glue Studio notebook 与 Amazon CodeWhisperer 之间的集成可以帮助用户更快开展数据集成工作。该集成目前已在美国东部（北弗吉尼亚州）可用。用户现在就可以着手将 Amazon Glue Studio notebook 与 Amazon CodeWhisper 进行集成，以加快数据集成构建工作。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0egsdNL9lIRjk5gYG1QQ</id>
            <title>便携式大语言模型才是智能手机的未来</title>
            <link>https://www.infoq.cn/article/0egsdNL9lIRjk5gYG1QQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0egsdNL9lIRjk5gYG1QQ</guid>
            <pubDate>Fri, 29 Sep 2023 02:00:00 GMT</pubDate>
            <updated>Fri, 29 Sep 2023 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 智能手机创新停滞，但人工智能技术的发展将使智能手机变得更智能。Meta AI Labs发布的LLaMA语言模型在性能和尺寸上取得了平衡，可以在智能手机上运行。这些面向个人的大型语言模型将成为智能手机操作系统的核心功能，可以处理个人数据并提供咨询和警告。智能手机需要更多的RAM来运行这些模型，但已经具备足够的CPU和GPU。总结：智能手机的未来将依靠个人化的人工智能技术，提供更智能的功能和服务。 </div>
                        <hr>
                    
                    <p>本文最初发布于The Register博客。</p><p>&nbsp;</p><p>智能手机的创新已经停滞。不久前发布的iPhone 15确实带来了一些不错的功能。但在一段时间内，我的iPhone 13还是可以满足我的需求，我不会急于更换。我之前的iPhone用了四年。</p><p>&nbsp;</p><p>在这款手机之前，我有充分的理由购买来自库比蒂诺的年度升级版本。但现在，我们能从中得到什么呢？iPhone 15<a href="https://www.theregister.com/2023/09/12/apple_announces_iphone_15_lineup/">提供</a>"了USB-C接口、更好的摄像头和更快的无线充电。这些功能都很好，但对大多数用户来说却并不是必需的。</p><p>&nbsp;</p><p>然而，鉴于目前近乎疯狂的人工智能创新浪潮，智能手机很快也会变得更好。</p><p>&nbsp;</p><p>几乎每个拥有智能手机的人都可以通过App或浏览器访问“三大”人工智能聊天机器人——OpenAI的ChatGPT、微软的Bing Chat和谷歌的Bard。</p><p>&nbsp;</p><p>这已经很好了。不过，除了这些“通用”人工智能聊天机器人之外，一项由另一家大型科技巨头牵头的秘密工作似乎正在占据上风。</p><p>&nbsp;</p><p>早在2月份，Meta AI Labs就<a href="https://www.theregister.com/2023/02/25/ai_in_brief/">发布了LLaMA</a>"——这是一个训练数据集和参数数量都变小了的大型语言模型。对于大型语言模型的工作原理，我们在直觉上还是会将其与更多的参数和更大的容量等同起来——例如，人们认为GPT-4有一万亿甚至更多的参数，尽管OpenAI对这个数字守口如瓶。</p><p>&nbsp;</p><p>Meta的LLaMA只有区区700亿个参数，甚至有一个版本只有70亿个。</p><p>&nbsp;</p><p>那么，是不是说LLaMA只有GPT-4的千分之一呢？这就是有趣的地方。虽然LLaMA从来没有在任何基准测试中击败过GPT-4，但它并不差——在许多情况下，它已经不是一般的好了。</p><p>&nbsp;</p><p>LLaMA是按Meta的方式开源的，研究人员可以使用其工具、技术来训练模型并迅速作出显著的改进。仅仅在几周之内，就出现了<a href="https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/">Alpaca</a>"、Vicuna等大型语言模型，每一个都优化得比LLaMA还好——同时，在基准测试中也和GPT-4越来越接近。</p><p>&nbsp;</p><p>当Meta AI实验室在7月份<a href="https://www.theregister.com/2023/07/19/meta_llama_2/">发布LLaMA2</a>"的时候——许可不再那么以Meta为中心——成千上万的AI程序员开始针对各种用例对它进行调整。</p><p>&nbsp;</p><p>Meta AI实验室自己也不甘落后，他们几周前<a href="https://www.theregister.com/2023/08/25/meta_lets_code_llama_run/">发布了自己的微调版本Code LLaMA</a>"——内嵌到IDE中提供代码补全功能，或者简单地提供分析和修复代码。此后两天之内，一家名为<a href="https://www.phind.com/blog/code-llama-beats-gpt4">Phind</a>"的初创公司就将Code LLaMA优化为一个可以在单项基准测试中击败GPT-4的大型语言模型。</p><p>&nbsp;</p><p>这是第一次，<a href="https://www.theregister.com/2023/05/11/open_source_ai_makes_subscriptions_irrelevant/">算是对OpenAI、微软和谷歌的一次警告</a>"。看似“微小”的大型语言模型也可以足够好，同时还足够小，不必在飞机机库大小的云计算设施中运行，不用像那样消耗大量的电力和水资源。相反，它们可以在笔记本电脑甚至智能手机上运行。</p><p>&nbsp;</p><p>不是理论上可以。几个月来，我一直在iPhone 13上运行<a href="https://mlc.ai/mlc-llm/">MLC聊天应用</a>"。它运行有着70亿个参数的LLaMA2模型并没有什么问题。这个迷你模型不如有着130亿个参数的LLaMA2模型亮眼（但我的智能手机没有足够的内存来容纳它），但它在尺寸和性能之间做了很好的平衡。</p><p>&nbsp;</p><p>iPhone 15也没有——尽管苹果的规格说明书省略了有关RAM的细节信息。</p><p>&nbsp;</p><p>这些面向个人的大型语言模型——在私有设备上运行——将很快成为智能手机操作系统的核心功能。它们会获取你所有的浏览数据、活动和医疗数据，甚至是财务数据——所有我们今天交给云计算用来对付我们的数据——它们会不断改进自己，更准确地体现我们的精神、身体和财务状况。</p><p>&nbsp;</p><p>它们会咨询，会鼓励，会警告。它们不会取代大量的通用模型，但它们也不会将我们所有的个人数据泄露到云端。大多数智能手机已经有足够的CPU和GPU来运行这些面向个人的大型语言模型，但它们需要更多的RAM。只要多一点内存，我们的智能手机就能变得更加智能。</p><p>&nbsp;</p><p>原文链接：<a href="https://www.theregister.com/2023/09/13/personal_ai_smartphone_future/?td=rt-3a">https://www.theregister.com/2023/09/13/personal_ai_smartphone_future/?td=rt-3a</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DbfXLTXZ4WkiaGscCHWI</id>
            <title>埃森哲使用 Amazon CodeWhisperer 助力开发人员提高工作效率</title>
            <link>https://www.infoq.cn/article/DbfXLTXZ4WkiaGscCHWI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DbfXLTXZ4WkiaGscCHWI</guid>
            <pubDate>Thu, 28 Sep 2023 07:24:33 GMT</pubDate>
            <updated>Thu, 28 Sep 2023 07:24:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> Amazon CodeWhisperer是一款AI编程助手，可以根据开发人员使用自然语言编写的注释和IDE中的代码生成建议，帮助提高工作效率。CodeWhisperer基于大语言模型（LLM）训练，支持15种编程语言，开发人员只需编写注释即可获得代码建议。CodeWhisperer可以帮助团队减少样板代码和重复代码模式的时间，帮助开发人员负责任地使用AI创建语法正确且安全可靠的应用程序。此外，CodeWhisperer还可以帮助开发人员尽快熟悉新项目，编写样板代码，使用不熟悉的语言编写代码，并检测代码的安全漏洞。总结：Amazon CodeWhisperer是一款AI编程助手，可以根据开发人员的注释和IDE中的代码生成建议，帮助提高工作效率。它支持15种编程语言，可以帮助团队减少样板代码和重复代码模式的时间，帮助开发人员负责任地使用AI创建语法正确且安全可靠的应用程序。此外，它还可以帮助开发人员尽快熟悉新项目，编写样板代码，使用不熟悉的语言编写代码，并检测代码的安全漏洞。 </div>
                        <hr>
                    
                    <p><a href="https://www.infoq.cn/article/JcIQOLpgqVK3AAgQxNQt">Amazon CodeWhisperer</a>" 是一款 AI 编程助手，可根据开发人员使用自然语言编写的注释和 IDE（集成开发环境）中的代码生成建议，帮助开发人员提高工作效率。借助 CodeWhisperer，开发人员无需在 IDE 与文档或开发者论坛之间切换，加快编码过程。通过 CodeWhisperer 的实时代码建议，开发人员可以在IDE中专注地工作，更快地完成编码任务。</p><p></p><p>CodeWhisperer 由基于数十亿行代码训练的<a href="https://www.infoq.cn/article/h5zqC9Cq8UK4iOKrwPc7">大语言模型（LLM）</a>"赋能，已经学会使用15种编程语言编写代码。开发人员仅需编写注释，用简明的英语概述一个特定任务即可，例如“uploada file to Amazon S3”（上传文件到 Amazon S3）。在此基础上，CodeWhisperer 可自动确定适合于该指定任务的云服务和公共库，即时构建特定代码，并直接在IDE中提供一段代码建议。此外，CodeWhisperer 能够与 Visual Studio Code和JetBrains 等 IDE 无缝集成，使开发人员可以专注于开发，且无需离开 IDE。截至目前，CodeWhisperer支持的开发语言包括 Java、Python、JavaScript、TypeScript、C#、Go、Ruby、Rust、Scala、Kotlin、PHP、C、C++、Shell 和 SQL。</p><p></p><h3>埃森哲使用 CodeWhisperer 助力开发人员提高工作效率</h3><p></p><p></p><p>“埃森哲正在使用 Amazon CodeWhisperer 加快编码任务，这是我们 Velocity 平台软件工程最佳实践计划的一部分。”埃森哲技术架构高级经理 Balakrishnan Viswanathan 表示，“Velocity 团队在想方设法提高开发人员的工作效率，搜寻过多种工具后，发现 Amazon CodeWhisperer 可以帮助减少30%的开发工作量。因此，我们可以更专注于安全、质量和性能的提升。”</p><p></p><h3>CodeWhisperer的优势</h3><p></p><p></p><p>埃森哲Velocity团队一直在使用 CodeWhisperer 来加速其人工智能（AI）和机器学习（ML）项目。使用 CodeWhisperer 带来了如下优势：</p><p></p><p>团队减少创建样板代码和重复代码模式的时间，从而将更多时间用于提升软件质量等重要的工作上CodeWhisperer 助力开发人员负责任地使用 AI，创建语法正确且安全可靠的应用程序团队可以生成完整的函数和符合逻辑的代码段落，无需在网上搜索或定制代码可以帮助新手开发人员或使用不熟悉代码库的开发人员快速上手工作通过将安全扫描前置到开发人员的 IDE 中，让团队可以在开发过程的早期阶段就检测安全威胁</p><p></p><h3>帮助开发人员尽快熟悉新项目</h3><p></p><p></p><p>CodeWhisperer可以帮助不了解亚马逊云科技的开发人员更快地熟悉使用亚马逊云科技服务开发的项目。例如，借助 CodeWhisperer，埃森哲新的开发人员就能够为 Amazon Simple Storage Service（Amazon S3）和 Amazon DynamoDB 等亚马逊云科技服务编码。在短时间内，他们就能够高效工作并为项目做出贡献。CodeWhisperer 通过提供代码段落或逐行建议来辅助开发人员完成工作。此外，CodeWhisperer 还能理解上下文。指令（注释）越具体，CodeWhisperer 生成的代码越相关。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c8ebf09fb162a7c70ada7af68353bdd3.png" /></p><p></p><p></p><h3>编写样板代码</h3><p></p><p></p><p>开发人员可以使用 CodeWhisperer 补全先决条件。他们只需输入“为机器学习数据创建预处理脚本的类”，就能够创建预处理数据类。开发人员只需几分钟编写预处理脚本，然后 CodeWhisperer 就能够生成整个代码段落。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d689fe99bb8981fba9af145eba88728.png" /></p><p></p><h3>帮助开发人员使用不熟悉的语言编写代码</h3><p></p><p></p><p>一个新加入团队的 Java 开发人员可以借助 CodeWhisperer 轻松编写 Python 代码，而不必担心语法问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28450a1729c977ec681599213f92d95e.png" /></p><p></p><h3>检测代码的安全漏洞</h3><p></p><p></p><p>开发人员可以在 IDE 中选择“运行安全扫描”来检测安全问题。发现的安全问题的详细信息会直接显示在 IDE 中。这可以帮助开发人员及早检测和修复问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d169fabf86d6fb122ec4f3d92d8c675b.png" /></p><p></p><p>“作为一名开发人员，CodeWhisperer 能够让您更加快速地编写代码”埃森哲人工智能工程顾问 Nino Leenus 表示，“此外，CodeWhisperer 借助人工智能可帮助消除拼写错误及其他典型错误，让编码更准确。对于开发人员来说，多次编写同样的代码乏味而枯燥。通过建议后续可能需要的代码片段，<a href="https://www.infoq.cn/article/C6ZjsPGuFWk6LBP7i48E">AI 代码补全技术</a>"可以减少这类重复性工作。”</p><p></p><p>现在，用户可以在喜欢的 IDE 中激活 CodeWhisperer。CodeWhisperer 可根据现有的代码和注释自动生成代码片段建议。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/di421fc7YtuJhSvQ5vjV</id>
            <title>第四范式成功登陆港股，开盘涨13.49%</title>
            <link>https://www.infoq.cn/article/di421fc7YtuJhSvQ5vjV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/di421fc7YtuJhSvQ5vjV</guid>
            <pubDate>Thu, 28 Sep 2023 06:18:26 GMT</pubDate>
            <updated>Thu, 28 Sep 2023 06:18:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 第四范式, IPO, 香港联合交易所, 6682.HK, 总市值, TMT项目, AI独角兽

总结: 中国人工智能公司第四范式在2023年9月28日成功在香港联合交易所主板挂牌上市，股份代号为6682.HK。首日开盘涨幅13.49%，总市值达293亿港元，成为今年迄今为止港股市值最大的TMT项目，也是近两年来第一家登陆港交所的AI独角兽。第四范式在本次IPO中募集资金净额约为8.36亿港元，将用于加强公司的基础研究、技术能力和解决方案开发，扩展产品和品牌，寻求战略投资和收购机会，以及一般企业用途。该公司成立于2014年，专注于提供以平台为中心的人工智能解决方案，并已在多个行业场景中落地。 </div>
                        <hr>
                    
                    <p>2023年9月28日，中国最大的以平台为中心的决策类人工智能公司第四范式，正式于香港联合交易所主板挂牌上市，股份代号为6682.HK。</p><p>&nbsp;</p><p>首日第四范式开盘涨13.49%，报于每股63.1港元，对应总市值达293亿港元。这一市值也使得第四范式成为今年迄今为止港股市值最大的TMT项目，同时也是近两年来第一家登陆港交所的AI独角兽。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b052bac181aaf3f10d6ecbeedc26e1e.png" /></p><p></p><p>据悉，第四范式在本次IPO中总计发售18,396,000股股份。其中，香港公开发售部分获约11.4倍超额认购；国际发售部分获约1.57倍超额认购。新华资本管理有限公司、中关村科学城公司以及澜起科技作为基石投资者参与本次发行，累计认购近亿美元，以示对第四范式长期价值的看好。</p><p></p><p>第四范式本次募集资金净额约为8.36亿港元。募集资金净额将用于：(1) 在未来三年分配至加强公司的基础研究、技术能力和解决方案开发。(2) 在未来三年分配至扩展公司的产品、建立公司的品牌及进入新的行业领域。(3) 在未来三年分配至寻求战略投资和收购机会，从而实施公司的长期增长战略，以开发公司的解决方案并扩展及渗透公司所覆盖的垂直行业。(4) 用作一般企业用途。</p><p></p><p>第四范式成立于2014年，专注于提供以平台为中心的人工智能解决方案，并已在金融、零售、制造、能源与电力、电信、运输、科技、教育、媒体、医疗保健等场景中落地。</p><p>&nbsp;</p><p>2023年3月，第四范式推出了专为企业业务场景设计的生成式人工智能产品“式说”（SageGPT），“式说”具有多模态大模型互动能力及企业级人工智能工具特性。 </p><p>&nbsp;</p><p>第四范式董事会主席兼CEO戴文渊表示：“第四范式长期致力于助力企业智能化转型和推进人工智能发展，成长为决策类AI平台领先企业。今天在港交所成功挂牌上市，对第四范式而言是一个重要的里程碑。未来，我们将继续巩固自身技术壁垒，不断提升商业化能力，持续助力客户业务成功。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xC4osqfB6XVbkybGJ1Kf</id>
            <title>腾讯云数据库凭借这项创新再获顶会认可，论文入选VLDB2023</title>
            <link>https://www.infoq.cn/article/xC4osqfB6XVbkybGJ1Kf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xC4osqfB6XVbkybGJ1Kf</guid>
            <pubDate>Thu, 28 Sep 2023 06:06:50 GMT</pubDate>
            <updated>Thu, 28 Sep 2023 06:06:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 腾讯云TDSQL的两篇论文成功被VLDB 2023收录，创新技术再次被国际顶级会议VLDB认可。其中一篇论文介绍了一种新颖的黑盒检查器PolySI，能高效地检查快照隔离（SI）并提供可理解的反例。另一篇论文介绍了一种名为Tesseract的在线和事务性模式演化方法，解决了数据库在线修改schema过程中的挑战。总结：腾讯云TDSQL的两篇论文被VLDB 2023收录，其中介绍了PolySI和Tesseract两种创新技术。PolySI是一种高效的黑盒检查器，用于检查快照隔离并提供反例。Tesseract是一种在线和事务性模式演化方法，解决了数据库在线修改schema的挑战。 </div>
                        <hr>
                    
                    <p>近日，InfoQ获悉，在第49届数据库国际顶会VLDB大会上，来自腾讯云TDSQL的2篇论文成功被VLDB 2023收录，创新技术再次被国际顶级会议VLDB认可。</p><p>&nbsp;</p><p>作为数据库领域的三大顶级会议之一，VLDB每届会议都集中展示了当前数据库研究的最前沿方向以及工业界的最新应用，吸引了众多全球顶级科技公司和研究机构的参与。因会议对系统创新性、完整性、实验设计等方面都要求极高，VLDB会议的论文接受率总体较低（约 18%）。</p><p>&nbsp;</p><p>入选论文中，腾讯云与南京大学、苏黎世联邦理工学院(ETH)&nbsp;合作研发的《Efficient Black-box Checking of Snapshot Isolation in Databases》解决方案，提出了一种新颖的黑盒检查器——PolySI，它能高效地检查快照隔离（Snapshot isolation，SI），并在检测到违规时提供可理解的反例。</p><p>&nbsp;</p><p>快照隔离是一种常见的弱隔离级别，它避免了串行化所带来的性能损失，同时可以防止很多常见的数据异常。然而，某些声称提供快照隔离保证的生产云数据库仍会产生SI数据异常，尤其在金融领域，会造成巨大影响。业界现有同类工具要么不支持快照隔离级别的测试，要么效率较低。鉴于数据库系统的复杂性，以及通常无法获取数据库内部信息的现状，业内亟需一种黑盒快照隔离检查器。</p><p>&nbsp;</p><p>为了解决该问题，我们提出并设计了“PolySI”算法与工具。PolySI的理论基础是基于广义多图（Generalized Polygraphs，GPs）的SI刻画定理，该定理保证了PolySI的正确性与完备性。PolySI采用SMT求解器（MonoSAT），并利用GPs的紧凑约束编码方案以及领域特定优化加速SMT求解。</p><p>&nbsp;</p><p>目前，通过广泛的评估，PolySI成功地重现了已知的SI异常，并在三个生产云数据库中检测到了新的SI异常、提供了可理解的反例。PolySI在多类工作负载下均优于目前最先进的SI黑盒检查器，并能够扩展到大规模工作负载。</p><p>&nbsp;</p><p>据了解，腾讯云与西蒙菲莎大学(Simon Fraser University)联合完成的《Online Schema Evolution is (Almost) Free for Snapshot Databases》论文，则介绍了“Tesseract”，一种新的在线和事务性模式演化方法，主要用于解决数据库在线修改schema过程中存在的挑战。</p><p>&nbsp;</p><p>当前，现代数据库应用经常根据不断变化的需求进行模式更改，数据库在线修改schema的主要优势在于，无需停止数据库服务或中断正在进行的事务，即可进行结构修改，这使得数据库能够在满足动态变化需求的同时，无需停机维护或重新启动数据库。</p><p>&nbsp;</p><p>但诸多问题也随之而来，在现有数据库系统中，支持在线和事务性模式（schema）演化仍然具有挑战性，如数据一致性，在进行结构修改时，为确保数据的一致性，需要使用事务或其他机制来保证数据的完整性和正确性；其次是长时间运行，某些结构修改预计需要较长的时间来完成，特别是对大型数据库或复杂结构的修改，导致对数据库性能产生一定的影响，因此需要在合适的时间窗口进行修改，以最小化对业务的影响。</p><p>&nbsp;</p><p>在以往的解决方案中，通常采用临时方法对模式演化进行“补丁”应用于现有系统，导致许多边缘情况和功能不完整。因此，应用程序通常不得不仔细安排停机时间进行模式更改，从而牺牲可用性。</p><p>&nbsp;</p><p>“Tesseract”的出现则有效避免了上述缺点。在广泛使用的多版本数据库系统中，模式演化可以被建模为改变整个表的数据修改操作，即数据定义即修改（DDaM）。这使得Tesseract可以通过利用并发控制协议几乎“免费”地支持模式。</p><p>&nbsp;</p><p>在Tesseract应用测试中，通过对现有快照隔离协议进行简单调整，在40核服务器上的工作负载下，Tesseract能够提供在线、事务性的模式演化，而无需服务停机，并在模式演化进行时保持高应用性能。</p><p></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LWyly6ZX8NrO37FQu22Y</id>
            <title>ChatGPT 再迎重大升级！终于“联网”，不再局限于旧数据，新功能即将对所有人开放</title>
            <link>https://www.infoq.cn/article/LWyly6ZX8NrO37FQu22Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LWyly6ZX8NrO37FQu22Y</guid>
            <pubDate>Thu, 28 Sep 2023 06:00:23 GMT</pubDate>
            <updated>Thu, 28 Sep 2023 06:00:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 总结: OpenAI宣布其聊天机器人产品ChatGPT可以通过微软的必应搜索引擎进行网络搜索，并且可以通过图片和语音命令进行交互。ChatGPT现在具备了图像识别和语音合成的能力，用户可以通过上传图片进行对话，并选择语音对话的合成语音。然而，OpenAI也指出ChatGPT仍存在一些限制和缺陷，包括视觉混淆和对非英语语种的识别问题。因此，在使用ChatGPT时需要谨慎，并尊重个人隐私。 </div>
                        <hr>
                    
                    <p>当地时间周三（9月27日），OpenAI在X（前身为推特）上宣布，其聊天机器人产品ChatGPT可以通过微软的必应搜索引擎进行网络搜索，将不再局限于2021年9月之前的数据。</p><p>&nbsp;</p><p>OpenAI称：“现在ChatGPT Plus和Enterprise（企业版） 用户可以使用浏览功能，将很快扩展到所有用户。要启用，请在GPT-4下的选择器中选择‘使用必应浏览’（ Browse with Bing）。”</p><p>需要说明的是，OpenAI早些时候测试了相关功能，允许Plus用户通过必应搜索访问最新信息，但后来因担心用户绕过付费墙，禁用了这项功能。</p><p></p><p>值得一提的是，OpenAI本周早些时候还宣布了另一项重大更新，将使ChatGPT可以通过图片和语音命令交互。</p><p></p><h2>ChatGPT再迎重大升级：“能看、能听，也能说”</h2><p></p><p>&nbsp;</p><p>本周一，OpenAI宣布对ChatGPT进行重大更新，使其GPT-3.5和GPT-4两大AI模型能够分析图像内容，并在文本对话中据此做出反应。OpenAI方面表示，ChatGPT移动版应用还将引入语音合成选项，在与现有语音识别功能配合使用时，能够与AI助手进行全口语对话。</p><p>&nbsp;</p><p>OpenAI也强调，语音合成功能目前仅适用于iOS和Android平台，而图像识别则将登陆Web版和移动版应用。</p><p>&nbsp;</p><p>OpenAI解释称，ChatGPT中的全新图像识别功能允许用户基于GPT-3.5或GPT-4模型，根据上传的一张或多张图像开展对话。该公司在其宣传博文中宣称，这项功能能够对接各类日常应用，例如为冰箱和食品储藏室拍摄照片以确定晚餐吃点什么，还有排除烧烤炉出故障的原因。该公司还提到，用户可以使用设备的触控屏圈出自己希望ChatGPT重点关注的部分。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3ecbe3dd514cb5fac3cf128f52ebbf1.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9ded80c7d4ae015cccf9d974c0d2883e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/51/515adb30560e8a8c090846a0c0b5fd18.png" /></p><p></p><p>&nbsp;OpenAI宣传视频中的画面，ChatGPT在分析用户照片以帮助其调整自行车座高。</p><p>&nbsp;</p><p>在官方网站上，OpenAI发布了一段宣传视频（<a href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak">https://openai.com/blog/chatgpt-can-now-see-hear-and-speak</a>"），展示了与ChatGPT的交流过程。其中用户询问要如何升高自己的自行车座垫，并上传了车辆、说明手册以及工具箱的照片。ChatGPT迅速做出反应，并为用户提供了完成调整过程的说明。我们还没有亲自测试过此功能，因此不太清楚实际效果是否真有这么惊艳。</p><p>&nbsp;</p><p>那这一切到底是怎么实现的？OpenAI尚未发布GPT-4或其多模态版本GPT-4V的底层运行细节。但根据其他厂商（包括OpenAI合作伙伴微软）的已知AI研究，多模态AI模型往往能够将文本和图像转化为共享编码空间，借此通过同一套神经网络处理多种类型的数据。OpenAI可以使用CLIP来弥合视觉与文本数据间的差异，从而在同一潜在空间（一种表达数据关系的向量化网络）上实现图像和文本表示对齐。正是这项技术，让ChatGPT具备了跨文本和图像进行上下文推理的能力——当然，这一切都只是外界的推测。</p><p>&nbsp;</p><p>与此同时，报道还指出ChatGPT的全新语音合成功能允许用户与其进行直接对话，而且此功能由OpenAI的“新文本转语音模型”驱动。尽管文本转语音技术已经相当成熟，但该公司表示在此功能推出之后，用户可以在应用端的设置中选择语音对话，之后从五种不同的合成语音中做出选择，具体包括“Juniper”、“Sky”、“Cove”、“Ember”和“Breeze”几个选项。OpenAI称这些声音均是与专业配音演员合作开发而来。</p><p>&nbsp;</p><p>OpenAI的Whisper是一套开源语音识别系统，此次也由它继续负责对用户语音输入的转录。Whisper于今年5月正式与ChatGPT iOS版应用集成，随后在7月登陆ChatGPT的Android版应用。</p><p></p><h2>“请注意，ChatGPT给出的结果不一定准确”</h2><p></p><p>OpenAI于今年3月公布GPT-4时，就曾经展示过该模型的“多模态”功能，据称可以处理文本和图像输入。但在随后的测试阶段，公众一直无缘真正体验其图像功能。期间OpenAI与Be My Eyes合作开发了一款可以为盲人描述场景照片的应用。今年7月，有报道称OpenAI的多模态功能之所以迟迟未能发布，主要是受到隐私问题的影响。与此同时，微软则于7月匆忙在基于GPT-4的AI助手Bing Chat中启用了图像识别功能。</p><p>&nbsp;</p><p>在最近的ChatGPT更新公告中，OpenAI称其扩展功能仍有一些限制，并承认该模型仍可能出现潜在的视觉混淆（即对某些内容的错误识别）、对非英语语种无法完美识别等问题。该公司表示，他们已经“在极端场景和纯科学验证角度”对新功能进行了风险评估，同时征求了alpha版本内测人员的意见，目前的观点仍然是建议谨慎使用，特别是在科学研究等高风险或专业性较强的背景之下。</p><p>&nbsp;</p><p>鉴于在开发Be My Eyes应用时遇到的隐私问题，OpenAI表示已经采取“技术措施来尽量限制ChatGPT对人类对象做分析和直接描述的能力。因为ChatGPT给出的结果不一定准确，AI系统应当尊重个人隐私。”</p><p>&nbsp;</p><p>尽管仍有种种缺陷，但OpenAI在营销材料中还是强调ChatGPT如今已经“能看、能听，也能说”。当然，并不是每个人都能认同这种充满拟人倾向的炒作宣传。Hugging Face公司AI研究员Sasha Luccioni博士就在X上发推称，“别再像看待人类那样看待AI模型了。ChatGPT根本就没法看、没法听，也没法说。它只能跟各种传感器相集成，以不同于人类的方式接收和发出信息。”</p><p>&nbsp;</p><p>虽然ChatGPT及其底层AI模型还远远算不上“人”，但如果本次公布的结果不假，那也至少代表着OpenAI的这款虚拟助手实现了巨大的功能增强。</p><p>&nbsp;</p><p>此外，OpenAI也强调了推迟开放有其充分理由：“我们认为应该逐步推出自己的工具，这样我们才能随时间推移不断改进并完善风险缓解措施，同时也让大家能为未来更强大的AI系统做好准备。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/OpenAI">https://twitter.com/OpenAI</a>"</p><p><a href="https://arstechnica.com/information-technology/2023/09/chatgpt-goes-multimodal-with-image-recognition-and-speech-synthesis/">https://arstechnica.com/information-technology/2023/09/chatgpt-goes-multimodal-with-image-recognition-and-speech-synthesis/</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2Kg4JCGdZWc632qllp9t</id>
            <title>Llama 生态现状：下载量超过 3000 万次，GitHub 上有超 7000 个相关项目</title>
            <link>https://www.infoq.cn/article/2Kg4JCGdZWc632qllp9t</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2Kg4JCGdZWc632qllp9t</guid>
            <pubDate>Thu, 28 Sep 2023 02:04:12 GMT</pubDate>
            <updated>Thu, 28 Sep 2023 02:04:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> Llama, Meta, language model, ecosystem, AWS, Google Cloud, Microsoft Azure, Hugging Face, downloads, cloud platforms, startups, community, optimization, developers, hardware support, future focus. <br><br>
总结: Meta发布了关于当前Llama生态的情况介绍。Llama模型已在AWS、Google Cloud和Microsoft Azure等云平台上得到广泛应用，并且Llama 2在云中的使用正在扩大。创新企业和初创公司正在使用Llama 2进行试验和评估。社区已经在Hugging Face上微调并发布了7000多个衍生品，提高了基准测试的性能。开发者社区也在不断扩展Llama的功能和支持。各大硬件平台通过软硬件优化提升了Llama 2的性能。Meta表示未来将在多模态、安全责任和社区方面投入更多关注。 </div>
                        <hr>
                    
                    <p><a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">Meta发布Llama 1</a>"至今已经 7 个月左右，而<a href="https://ai.meta.com/blog/llama-2/">Llama 2</a>"以及<a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">Code Llama</a>"的发布也有了数月的时间。当地时间9月27日，Meta发布博文系统介绍了当前 Llama 生态的情况。官方统计，通过 Hugging Face 的 Llama 模型下载量超过 3000 万次，其中仅在过去 30 天就超过了 1000 万次。</p><p>&nbsp;</p><p>当前的Llama 生态情况如下：</p><p>&nbsp;</p><p>云厂商：AWS、Google Cloud 和 Microsoft Azure 等主要平台已在其平台上采用了 Llama 模型，并且 Llama 2 在云中的使用正在扩大。Google Cloud 和 AWS 总共有超过 3,500 个企业项目基于 Llama 2 模型启动。此外，Meta 还宣布 AWS 成为 Llama 2 的首个托管 API 合作伙伴，各种规模的组织都可以访问 Amazon Bedrock 上的 Llama 2 模型，而无需管理底层基础设施。</p><p>&nbsp;</p><p>创新企业：包括 Anyscale、Replicate、Snowflake、LangSmith、Scale AI 等在内，已有上万家初创公司正在使用或评估 Llama 2。“像 DoorDash 这样的创新者正在使用它进行大规模试验，然后再发布由LLM支持的新功能。”</p><p>&nbsp;</p><p>众包优化：截至目前，社区已在Hugging Face上微调并发布了7000多个衍生品。平均而言，在标准基准测试中，它们将常见基准测试的性能提高了近 10%，对于 TruthQA 等基准数据集的性能提升高达 46%。</p><p>&nbsp;</p><p>开发者社区：GitHub 上现在有超过 7,000 个基于 Llama 构建或提及 Llama 的项目。为方便将 Llama 引入边缘设备和移动平台，新的工具、部署库、模型评估方法，甚至 Llama 的“微型”版本都在开发中。此外，社区还扩展了 Llama 以支持更大的上下文、增加了对其他语言的支持等。</p><p>&nbsp;</p><p>硬件支持：各大硬件平台AMD、Intel、Nvidia、Google均通过软硬件优化提升了Llama 2的性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/6123307306e980cfff4e10cfa74fbb84.png" /></p><p>对于Llama 生态的未来，Meta表示，仍致力于为当今的人工智能提供开放的方法，主要会在多模态、安全责任和社区上投入更多关注。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://ai.meta.com/blog/llama-2-updates-connect-2023/">https://ai.meta.com/blog/llama-2-updates-connect-2023/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ad5b3a17e30bbacebaaa95034</id>
            <title>探索AI技术对古彝文保护与研究应用</title>
            <link>https://www.infoq.cn/article/ad5b3a17e30bbacebaaa95034</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ad5b3a17e30bbacebaaa95034</guid>
            <pubDate>Thu, 28 Sep 2023 01:14:46 GMT</pubDate>
            <updated>Thu, 28 Sep 2023 01:14:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 古彝文, 彝族, 文字系统, 历史, 文化意义, 表音文字系统, 形状多样, 文化信息, 保护, 识别, 重难点, 数据样本稀缺性, 古籍修复, 字符形状多样性, 字符数量和组合规则, 字词辨别和语义理解, AI技术, 古文识别应用, 合合信息, 上海大学, 古彝文基础编码数据库, 文档图像分析识别与理解联合实验室, 智能高清滤镜技术, 基于深度学习的复杂场景文字识别技术.

总结: 古彝文是彝族使用的一种古老文字系统，具有悠久的历史和独特的文化意义。古彝文的识别面临着数据样本稀缺性、古籍修复、字符形状多样性、字符数量和组合规则、字词辨别和语义理解等重难点。然而，AI技术如智能高清滤镜技术和基于深度学习的复杂场景文字识别技术可以助力古彝文的保护和研究。合合信息与上海大学合作建立了古彝文基础编码数据库，并成立了文档图像分析识别与理解联合实验室，致力于解决古彝文识别中的学术性和技术性难题。 </div>
                        <hr>
                    
                    <p></p><h1>一、古彝文</h1><p></p><p></p><h2>1.1 古彝文介绍</h2><p></p><p>古彝文是彝族使用的一种古老文字系统，彝族是中国的少数民族之一，主要分布在中国西南地区。古彝文具有悠久的历史和独特的文化意义，被认为是世界上最古老的文字之一。</p><p></p><p>古彝文的起源可以追溯到公元前13世纪左右，据信是由古代彝族人民创造和使用的。它是一种表音文字系统，每个字符代表一个音节或一个音节的组合。古彝文的书写方式是从上到下、从左到右，类似于竖排的文字。它的形状多样，有直线、弯曲、斜线等不同的组合，形成了独特的图形。</p><p></p><p>古彝文的内容涵盖了丰富的彝族文化和历史信息，包括祭祀、婚姻、宗教、传统习俗等方面。通过研究古彝文，人们可以了解到彝族人民的生活方式、价值观念和社会结构。古彝文也是研究彝族历史和文化的重要线索和工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0ef06b4876a57028448005b08dfa1d97.png" /></p><p></p><h2>1.2 古彝文古籍保护背景</h2><p></p><p>古彝文的研究对于彝族文化的保护和传承具有重要意义。通过深入研究古彝文，人们可以更好地理解彝族文化的独特之处，并促进彝族文化的传统与现代的交流与融合。</p><p></p><p>随着现代科技的进步，人们开始探索利用人工智能和计算机技术来识别和研究古彝文。合合信息与上海大学社会学院签署校企合作协议，通过将人工智能和计算机视觉技术应用于古彝文识别，可以更快速、准确地解读古彝文文献，并将其数字化保存，助力推动古彝文古籍保护和研究。</p><p></p><h2>1.3古彝文识别的重难点</h2><p></p><p>古彝文识别的重难点主要包括以下几个方面：</p><p></p><p>1.数据样本稀缺性：由于古彝文的使用较为有限，古彝文的数据样本相对稀缺。这使得训练和优化古彝文识别模型变得困难，因为需要大量的样本数据来训练模型以提高识别准确性。因此，缺乏充足的古彝文数据样本是古彝文识别的一个重要难点。</p><p></p><p>2.古籍修复：由于很多彝族文献遭到破坏和流失，存在缺失、污渍、模糊、噪声干扰等现象，像这样：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a8c5db1f8859507fff231f7865c4b6b.png" /></p><p></p><p>3.字符形状多样性：古彝文的字符形状非常多样，包括直线、弯曲、斜线等不同的组合。这使得古彝文的字符识别变得复杂，因为不同的字符可能具有相似或相同的形状，而相似的字符可能具有不同的语义。因此，准确地区分和识别古彝文字符的形状是一个重要的难点。</p><p></p><p>4.字符数量和组合规则：古彝文字符的数量较多，约有600个以上的字符。而且，古彝文的字符通常是由多个基本形状组合而成的，这种组合规则也具有一定的复杂性。因此，要准确地识别古彝文字符，需要对字符的数量和组合规则进行深入的研究和理解。</p><p></p><p>5.字词辨别和语义理解：古彝文的词汇和语义理解也是一个挑战。由于古彝文是表音文字系统，一个字符可能代表一个音节或一个音节的组合。因此，对于词句的辨别和语义理解需要结合上下文信息和语言学知识。这对于古彝文的自动识别和翻译来说是一个重要的难点。</p><p></p><p>为了应对这些重难点，古彝文识别需要结合人工智能和计算机视觉技术，如深度学习、图像处理和自然语言处理等。通过建立大规模的古彝文数据库、优化识别算法和加强语义理解，可以提高古彝文识别的准确性和效率。此外，加强对古彝文的研究和保护，提高对古彝文的认知和使用，也是解决古彝文识别难题的重要途径。</p><p></p><h1>二、AI技术助力古文识别应用</h1><p></p><p>作为世界上最古老的文字之一，古彝文是中华文明地图上神秘而耀眼的印记。合合信息联合上海大学、华南理工大学团队针对现有的西南彝志、云贵一带古彝文字符开展统一编码，并于近期发布了业内首个古彝文基础编码数据库（简称“数据库”）。</p><p></p><p>该数据库包含上千个古彜文基础编码，通过API数据接口等形式，该数据库有望帮助高校研究人员、文化工作者、兴趣爱好者等人群快速找到古彝文在字典中的读音、汉语释义、用法，如同“大字典”一般，帮助人们降低古彝文书籍、文献阅读的门槛，以数字化手段助力传统文化保护、创新之路。</p><p></p><p>研究古彝文字集，有助于理解尚未被翻译成汉文、用字尚未规范化的古籍，更深层、透彻地作用于传统文化保护，同时通过建立古彝文数据库，填补当前国内外研究的空白。合合信息与华南理工大学共同成立文档图像分析识别与理解联合实验室，联合上海大学社会学院，共同解决数据库建设中的学术性、技术性难点。</p><p></p><p>合合信息与上海大学将合力完成以《西南彝志》为中西的贵州古彝文图像识别及数字化校对工作，帮助后续古彝文的检测、识别、标注，利用旗下扫描全能王的智能高清滤镜技术也可以进行古彝文的古籍修复。</p><p></p><h2>2.1 智能高清滤镜技术</h2><p></p><p>智能高清滤镜技术可智能检测图像中存在的问题，自动判定图像优化方式，实现模糊、阴暗、手指等干扰因素全处理。传统古籍问卷存在水迹、残旧、破损等情况，通过智能高清滤镜能够去除相关痕迹复现高清文档并开展识别。</p><p></p><p>从而增强文字的可读性，为接下来的文字信息提取、识别创造了良好的条件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fde30d10f8d38ffad4fcd4bc5237a467.png" /></p><p></p><h2>2.2 基于深度学习的复杂场景文字识别技术</h2><p></p><p>基于深度学习的复杂场景文字识别技术是一种能够自动识别和提取复杂场景中的文字信息的技术。它可以应对各种复杂的场景，如模糊、扭曲、光照不均、背景干扰等，实现高准确度的文字识别。这种技术的核心是深度学习模型，通常使用卷积神经网络（Convolutional Neural Network，CNN）和循环神经网络（Recurrent Neural Network，RNN）的结合来处理图像中的文字。整个过程可以分为三个主要步骤：文本检测、文本定位和文本识别。</p><p></p><p>1、首先是文本检测：它的目标是在图像中定位出文字的位置。通常使用卷积神经网络来进行文本区域的检测，网络会学习到图像中文字的特征，通过滑动窗口或区域提议的方式来检测可能的文字区域。</p><p></p><p>2、接下来是文本定位：这一步是为了更精确地定位出文字的位置。通常使用回归模型或者基于锚点的方法来对文本区域进行精确定位，以获得更准确的文字边界框。</p><p></p><p>3、最后是文本识别：这一步是将文字从图像中提取出来并进行识别。通常使用循环神经网络（如长短时记忆网络，LSTM）来对文字进行识别，网络会学习到文字的上下文信息，从而提高识别的准确度。</p><p></p><p>4、此外，为了提高复杂场景文字识别的准确度，还可以采用一些技巧和策略，如数据增强、多尺度处理、注意力机制等。数据增强可以通过旋转、缩放、扭曲等方式生成更多的训练样本，提高模型的泛化能力。多尺度处理可以通过在不同尺度下对图像进行处理，提高对不同大小文字的适应能力。而注意力机制可以帮助模型更关注重要的文字区域，减少背景干扰对识别结果的影响。</p><p></p><p>古彝文项目将根据上海大学古彝文研究员设计的四字节编码系统，引入合合信息智能文字识别技术，对异体字、变体字、误用字和混用字等进行标注、识别、比对，并由此建立起精确的彝文古籍电子数据库，识别标注效果如下所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b6eec476252e8fbc94fb22b1df29f577.png" /></p><p></p><h2>2.3 自然语言处理（NLP）技术</h2><p></p><p>自然语言的语义理解是指对自然语言文本中的意义和语义进行理解和解析的过程。它是自然语言处理（NLP）中的一个重要研究方向，旨在使计算机能够准确地理解和推断文本的含义，从而实现更高级别的语言处理任务。</p><p></p><p>注意力机制在语义理解中可以发挥重要作用，下面是一个基于注意力机制语义理解的实现过程：</p><p></p><p>数据预处理：首先，需要对古彝文数据进行预处理。这包括分词、词性标注、句法分析等步骤，以便将古彝文转换为计算机可以理解的形式。建立词嵌入模型：将古彝文中的每个字或词映射为一个高维向量表示，可以使用预训练的词嵌入模型（如Word2Vec、GloVe等）或自定义的古彝文词嵌入模型。构建编码器-解码器模型：使用Transformer作为编码器-解码器模型的基础架构。编码器将输入的古彝文序列转换为高维特征表示，解码器根据编码器的输出和目标序列生成对应的输出序列。自注意力机制：在编码器和解码器的每个层中，使用自注意力机制来捕捉输入序列中不同位置之间的依赖关系。自注意力机制能够计算输入序列中不同位置的相关性，并根据相关性对特征进行加权。上下文编码：利用自注意力机制，在编码器中对输入序列中的每个字或词进行上下文编码。通过对输入序列中的每个位置进行自注意力计算，可以得到每个位置的上下文信息。解码过程：在解码器中，根据编码器的输出和目标序列，使用自注意力机制生成对应的输出序列。解码器通过不断预测下一个字或词来生成输出序列，直到遇到终止符号或达到最大长度。语义理解结果：根据解码器生成的输出序列，可以得到对古彝文的语义理解结果。这些结果可以包括句子的情感、主题、语义角色等。</p><p></p><p>注意力机制能够帮助模型在语义理解任务中更好地捕捉输入序列中的重要信息，从而提高古彝文的语义理解能力。通过对输入序列中不同位置的相关性进行建模，注意力机制使模型能够更好地关注句子中的关键部分，从而更准确地理解古彝文的语义。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3dc656b8ba9e876dd18db809032e12ac.png" /></p><p></p><p>目前针对古彜文虽然能够识别出相关基础编码，但对应的释义需要根据上下文重新解读，在古彝文识别项目中，合合信息就借助了注意力机制（Transformer）完成语义理解。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/862354fdc1178ee1c432f2f051d42ccd.png" /></p><p></p><h1>三、古彝文识别的意义</h1><p></p><p>在2021年、2022年的世界人工智能大会上，合合信息展现了智能文字识别技术在甲骨文、西周钟鼎文（金文）中的应用，这些研究成果为古彝文的识别提供了良好的基础。甲骨文和古彝文同源于骨刻文，这种文字最早出现在骨头上，随后发展为甲骨文、金文、小篆、隶书、楷书等不同的书写形式。这些文字之间存在许多相通之处，使得文字识别技术在不同阶段得以延续和发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e9183b617a96a7c6d5052ff67d9b105.png" /></p><p></p><p>通过与上海大学联合开启的“贵州古彝文图像识别及数字化校对项目”，合合信息将智能文字识别技术应用于古彝文的保护和传承中。这个校企合作项目的成功开展，为合合信息在小语种保护和古文化传承方面提供了重要的支持。通过智能文字识别技术的应用，古彝文的数字化处理变得更加高效和准确，使得更多人能够了解和认识古彝文这一珍贵的文化遗产。</p><p></p><p>随着人们对小语种和古文化的保护意识不断提高，合合信息将继续加强智能文字识别技术的研究和应用，为保护和传承这些珍贵文化遗产做出更大的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pe4dSavhT55QZtwwyPkc</id>
            <title>GPT 3.5与Llama 2微调的综合比较</title>
            <link>https://www.infoq.cn/article/pe4dSavhT55QZtwwyPkc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pe4dSavhT55QZtwwyPkc</guid>
            <pubDate>Wed, 27 Sep 2023 07:35:38 GMT</pubDate>
            <updated>Wed, 27 Sep 2023 07:35:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 在本文中，作者通过SQL和函数表示任务对Llama 2和GPT 3.5进行了微调的比较实验。结果显示，GPT 3.5在两个数据集上的性能略好于Llama 2的CodeLlama 34B。然而，GPT 3.5的训练成本高出4到6倍，甚至更高。作者通过这个实验想要探究手动微调模型是否可以在成本较低的情况下达到接近GPT 3.5的性能。实验结果表明，手动微调的模型确实可以接近GPT 3.5的性能。

实验使用了Spider数据集和Viggo函数表示数据集的子集进行微调。这些数据集都是很好的用于微调的数据集，它们教会模型给出期望的输出形式，而不是事实。对于SQL和函数表示任务，模型需要输出结构化的结果，而预训练的模型在这方面表现不佳。

作者使用了Code Llama 34B和Lora微调来进行实验。Lora是一种适配器/非全参数微调的方法，可以在SQL和函数表示等任务上与全参数微调媲美。作者使用了特定的适配配置，并发现在所有线性层使用适配配置并增加r值并没有明显的性能提升。

总体而言，实验结果表明手动微调模型可以在成本较低的情况下达到接近GPT 3.5的性能。然而，GPT 3.5的训练成本仍然较高。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>在本文中，我将分享我通过SQL和函数表示任务对Llama 2和GPT 3.5进行微调的比较实验。总体结果如下：GPT 3.5在两个数据集上与LoRA微调的CodeLlama 34B相比，性能要好一些；GPT 3.5的训练成本高出4到6倍(部署成本甚至更高)。SQL任务的代码和数据在这里（https://github.com/samlhuillier/spider-sql-finetune），函数表示任务的代码和数据在这里（https://github.com/samlhuillier/viggo-finetune）。为什么要做这个比较？对GPT 3.5进行微调的成本是很高的。我想通过这个实验看看手动微调模型是否可以在成本很低的情况下让性能接近GPT 3.5。有趣的是，它们确实可以！</blockquote><p></p><p></p><p></p><h1>结果</h1><p></p><p><img src="https://static001.infoq.cn/resource/image/a4/5e/a45f2b3fd101054773f82c02fec8205e.png" /></p><p>CodeLlama 34B和GPT 3.5执行SQL任务和函数表示任务的性能。</p><p></p><p>GPT 3.5在这两项任务上表现出稍好的准确性。在使用模型生成SQL查询时，我还使用执行准确性作为比较它们在虚拟数据库上执行查询输出的指标（精确匹配准确性是指字符级别的比较）。</p><p></p><h3>训练成本</h3><p></p><p></p><p>注：我使用的是vast.ai提供的A40 GPU，每小时费用为0.475美元。</p><p></p><p></p><h1>实验设置</h1><p></p><p></p><p>我使用了Spider数据集和Viggo函数表示数据集的子集，这些都是很好的用于微调的数据集：</p><p></p><p>它们会教会模型给出期望的输出形式，而不是事实。SQL和函数表示任务都期望结构化的输出。预训练的模型不能很好地完成这两项任务。</p><p></p><p>对于GPT 3.5的微调，OpenAI只允许配置epoch的数量。他们建议选择epoch的数量。因此，为公平起见，我只对Llama进行最少的超参数微调，允许OpenAI选择epoch的数量，并训练Llama在评估数据集上收敛。</p><p></p><h3>Llama的架构</h3><p></p><p></p><p>我做出的两个关键决定是使用Code Llama 34B和Lora微调（而不是全参数）：</p><p></p><p>OpenAI很可能会做一些适配器/非全参数微调。Anyscale的一篇文章（<a href="https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2</a>"）指出，对于SQL和函数表示等任务，LoRA几乎可以与全参数微调媲美。</p><p></p><p>我使用的LoRA适配配置是这样的：</p><p></p><p><code lang="text">config = LoraConfig(&nbsp; &nbsp; 
  r=8,&nbsp; &nbsp; 
  lora_alpha=16,&nbsp; &nbsp; &nbsp;
  target_modules=[&nbsp; &nbsp; &nbsp;
  "q_proj",&nbsp; &nbsp; &nbsp;
  "k_proj",&nbsp; &nbsp; &nbsp;
  "v_proj",&nbsp; &nbsp; &nbsp;
  "o_proj",&nbsp;
  ],&nbsp; &nbsp; 
  lora_dropout=0.05,&nbsp; &nbsp; 
  bias="none",&nbsp; &nbsp; 
  task_type="CAUSAL_LM",
  )</code></p><p></p><p>我尝试在所有线性层使用适配配置（正如<a href="https://arxiv.org/abs/2305.14314?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">Qlora论文</a>"所建议的那样)，发现几乎没有性能提升。类似地，将r增加到16只会消耗更多的计算量，而几乎没有提供性能上的好处。</p><p></p><p></p><h3>数据集</h3><p></p><p></p><p>SQL提示词示例：</p><p><code lang="text">You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.

You must output the SQL query that answers the question.
### Input:
Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?

### Context:
CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)

### Response:</code></p><p></p><p>我没有使用完整的Spider数据集，它的数据库Schema是这样的：</p><p><code lang="text">department : Department_ID [ INT ] primary_key Name [ TEXT ] Creation [ TEXT ] Ranking [ INT ] Budget_in_Billions [ INT ] Num_Employees [ INT ] head : head_ID [ INT ] primary_key name [ TEXT ] born_state [ TEXT ] age [ INT ] management : department_ID [ INT ] primary_key management.department_ID = department.Department_ID head_ID [ INT ] management.head_ID = head.head_ID temporary_acting [ TEXT ]</code></p><p></p><p></p><p>相反，我选择使用sql-create-context（<a href="https://huggingface.co/datasets/b-mc2/sql-create-context?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://huggingface.co/datasets/b-mc2/sql-create-context</a>"）数据集和Spider数据集的交集。因此，提供给模型的上下文是一个SQL创建命令（我这么做实际上完全是为了节省节点数）：</p><p><code lang="text">CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)</code></p><p></p><p>函数表示提示词示例：</p><p><code lang="text">Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.
This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].
The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']

### Target sentence:

I remember you saying you found Little Big Adventure to be average. Are you not usually that into single-player games on PlayStation?

### Meaning representation:</code></p><p></p><p>输出是这样的：</p><p><code lang="text">verify_attribute(name[Little Big Adventure], rating[average], has_multiplayer[no], platforms[PlayStation])</code></p><p></p><p></p><h3>评估</h3><p></p><p></p><p>两个模型收敛得都很快：</p><p><img src="https://static001.infoq.cn/resource/image/d5/bb/d596a7726b6ed9c8685eb60bee6febbb.png" /></p><p></p><p>图中显示了在训练过程中模型在评估集上的损失。</p><p></p><p>对于SQL任务，我还使用Spider评估工具（<a href="https://github.com/taoyds/test-suite-sql-eval?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://github.com/taoyds/test-suite-sql-eval</a>"）计算SQL查询的执行准确性。评估工具会构建一个虚拟数据库，并将实际的输出与GPT3.5和Llama 2的查询输出进行比较。</p><p></p><p></p><h1>结论</h1><p></p><p></p><p>总的来说，通过这个经验，我觉得对GPT 3.5进行微调是为了初始验证或构建MVP，而除此之外，像Llama 2这样的模型可能是你最好的选择。</p><p></p><p></p><h3>为什么要对GPT 3.5进行微调？</h3><p></p><p></p><p>你想要证实微调是解决给定任务/数据集的正确方法；你想要全托管的体验。</p><p></p><p></p><h3>为什么要对像Llama 2进行微调？</h3><p></p><p></p><p>你想省钱！你希望最大限度地榨取数据集的性能；你希望在训练和部署基础设施方面具有充分的灵活性；你想保留私有数据。</p><p></p><p></p><p>原文链接：</p><p><a href="https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV</id>
            <title>从计算到智算，如何降低AI算力使用门槛？</title>
            <link>https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV</guid>
            <pubDate>Wed, 27 Sep 2023 06:51:54 GMT</pubDate>
            <updated>Wed, 27 Sep 2023 06:51:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 挑战：算力产业从“计算”迈向“智算”，面临挑战。不同应用场景对算力芯片的需求不同。降低AI算力使用门槛。

需求：不同应用场景对算力芯片的运算能力有不同需求。

降低门槛：如何降低AI算力使用门槛。

总结: 随着生成式AI技术的广泛应用，算力产业正从“计算”迈向“智算”。这一变化给算力产业带来了挑战，不同应用场景对算力芯片的运算能力有不同的需求。为了降低AI算力的使用门槛，需要进一步研究和发展相关技术。 </div>
                        <hr>
                    
                    <p>随着生成式 AI 技术得到广泛应用，算力产业正从“计算”迈向“智算”，这一变化给算力产业带来哪些挑战？不同应用场景对算力芯片的运算能力有何需求？如何降低 AI 算力使用门槛？近日，InfoQ《极客有约》邀请到了大禹智芯产品及解决方案负责人余曦老师，为大家分享《从计算到智算，如何降低 AI 算力使用门槛？》。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/2RLlk4XePkSXUNU4eQIw">https://www.infoq.cn/video/2RLlk4XePkSXUNU4eQIw</a>"</p><p>&nbsp;</p><p>姜雨生：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，微软软件工程师姜雨生。本期直播，我们邀请到了大禹智芯产品及解决方案负责人余曦老师来给我们做分享。今天的直播主题是《从计算到智算，如何降低 AI 算力的使用门槛？》。先请余曦老师，给大家做一个简单的介绍。</p><p>&nbsp;</p><p>余曦：大家好，我是余曦，来自大禹智芯。目前，我在公司负责产品和解决方案。非常高兴今天有机会与大家分享我们在 AI 智算中心的经验，包括我个人在这一领域的想法和未来的思考。</p><p>&nbsp;</p><p>我简要介绍一下大禹智芯。我们成立于 2020 年中旬，是国内最早专注于 DPU（数据处理单元）产品的公司之一。我们在技术领域涵盖了底层硬件，包括芯片和&nbsp;FPGA&nbsp;逻辑，以及硬件的不同形态，例如网卡。此外，我们拥有一支强大的软件团队，负责将&nbsp;DPU&nbsp;上的各种硬件能力整合到我们的 DPU 操作系统中。通过这个操作系统，我们可以开发应用软件，充分发挥底层硬件的性能。</p><p>&nbsp;</p><p>除了完整的 DPU 产品，包括硬件和软件，我们还开发了一些与应用相关的组件，与编排系统集成。这些编排系统包括云环境下广泛使用的 OpenStack 和 Kubernetes 等。在编排系统的基础上，我们还提供了一层服务，面向那些没有自己云管理平台但希望充分利用我们产品的用户。为了满足这一需求，我们开发了一个裸金属服务管理平台，结合了我们改进过的 OpenStack 和 DPU 卡，使用户能够快速将物理服务器转化为云化的裸金属实例。</p><p></p><h2>数据中心变迁历程</h2><p></p><p>&nbsp;</p><p>姜雨生：您在过去十年间参与了国内多个大型互联网云计算厂商数据中心网络架构的设计和建设，能分享下过去十年数据中心的变迁历程吗？主要可以分为哪几个阶段？</p><p>&nbsp;</p><p>余曦：在过去的十几年里，我一直投身于互联网行业的数据中心领域。我的经历可以大致分为三个阶段。</p><p>&nbsp;</p><p>首先是物理机阶段，这个阶段网络侧的需求相对简单。当时，主要问题是如何在物理机状态下建立一个数据中心，以充分利用物理带宽。在这个阶段，传统的二层生成数协议等都被推翻，以实现带宽的最大化利用。这个物理机时代持续了很长时间。</p><p>&nbsp;</p><p>第二阶段大约在 2012 年到 2014 年之间，即从物理机时代转向了云计算时代。这意味着从物理机向云计算或云数据中心的转变。云数据中心可以分为小型企业级云数据中心和大型大规模云计算中心。从设备厂商的角度来看，可以分为企业级数据中心和大规模数据中心（MSDC），MSDC 主要服务于大型公有云场景。我们的工作主要集中在大规模数据中心这个领域，面临着如何支持从几百台、1000 台规模扩展到数万、十万甚至几十万的台的巨大挑战。</p><p>&nbsp;</p><p>在这个规模扩展的过程中，出现了许多问题。一个重要的变化是，原先与网络相关的功能都集中在数据中心交换机层面，但随着云计算的发展和规模的扩大，所有功能开始下沉到服务器端，通过软件提供。采用软件方式提供这些功能有几个好处：</p><p>&nbsp;</p><p>第一点是关于灵活性和自主性。与硬件相比，软件更具灵活性，能够根据需要实现功能，这使得它更加灵活和自主。</p><p>&nbsp;</p><p>第二点涉及到软件角度与云计算资源的充分耦合。在云计算中，计算、网络和存储是三个关键组成部分。网络是基础，同时也连接了计算和存储。在计算和存储方面，需要快速将它们紧密结合，并且要具备灵活性。然而，这在网络方面面临许多挑战。因此，我们看到许多功能从传统的交换机层面逐渐迁移到主机上来实现。这也是为什么 DPU（数据处理单元）或智能网卡变得非常重要的原因。因为在所有功能下沉到主机后，主机会面临巨大的压力。那么，如何解决这种压力？必须引入一些新的组件来帮助主机处理网络中的复杂流程，特别是在大型云场景中的复杂流程。这就导致了像智能网卡和 DPU 这样的组件在云计算中逐渐崭露头角。</p><p>&nbsp;</p><p>此外，还有一点需要注意的是，在传统的云计算场景下，计算资源主要围绕 CPU 提供。但随着广泛应用和需求的变化，逐渐引入了异构计算组件，包括&nbsp;FPGA、GPU 以及专门用于特定算法的 AI 芯片。这些组件逐渐融入了云计算场景，与 CPU 一起提供算力，这在过去是非常不同寻常的。同时，这也为现在被称为智算中心和 AI 计算领域的发展提供了雏形。</p><p>&nbsp;</p><p>第三阶段是云计算服务逐渐与智算服务（或异构算力服务）相结合，形成了我们现在所谈论的智算中心。在智算中心内部，有一个重要组成部分，即高性能网络。当我们与客户讨论智算中心时，我们强调从网络的角度看，智算中心实际上分为两个网络：服务网和参数网（或计算网）。</p><p>&nbsp;</p><p>从服务网的角度来看，它通过这个网络将智算中心内部的计算资源，无论是 CPU 的计算能力还是 GPU 或其他异构计算资源，以云服务的形式提供出去。这个网络承载了向智算中心内的云服务提供计算资源的角色。</p><p>&nbsp;</p><p>参数网（或计算网）的主要作用是通过网络建立一个高速通道，以便计算单元之间进行快速的数据交互。这个网络在云计算数据中心的后期阶段主要存在于小规模存储场景中。例如，在国内引入 RDMA（Remote Direct Memory Access）应用时，这个应用主要基于存储场景，后端存储集群通过 RDMA 技术提高吞吐量、降低延迟，从而提高云上云盘的整体性能。当引入这些计算单元后，计算单元也需要一个高性能网络来支持数据传输。因此，高性能网络从存储环境逐渐迁移到了智算中心内的参数网。这实际上是一个演进的过程。</p><p>&nbsp;</p><p>姜雨生：数据中心是算力的最终载体，在算力需求日益增长的当下，传统数据中心面临哪些挑战？我们需要什么样的数据中心？</p><p>&nbsp;</p><p>余曦：参数网（或计算网）现在面临着巨大的挑战，因为这个网络有几个极致的要求。首先是吞吐量必须大，且从微观角度看，它要求吞吐量呈波浪形，但从宏观角度看，它应该一条直线。这意味着网络流量需要一直保持在最大带宽的状态，这是一个挑战。其次，在网络趋于跑满的情况下，仍然需要为上层应用提供良好的传输环境。传输环境涉及两个维度：丢包和延时。不同应用可能对丢包的容忍度不同，有些应用可以接受一些丢包，而有些则需要零丢包。延时也是一个重要因素，因为在数据交互中，任务的开始必须等到上一个阶段的数据交互完成。这意味着等待时间取决于最后一个任务完成的时间，而不是第一个任务完成的时间。因此，尽量减少等待时间，使数据交互在相近的时间内完成，算力单元的能力要求非常高。</p><p>&nbsp;</p><p>这就要求网络在三个方面都要表现出色，即延时、吞吐和规模。然而，传统的 TCP 网络在这三个方面通常表现为一个狭窄腰形状的等腰三角形，延时、吞吐和扩展性的平衡难以达到。现在的挑战是将 RDMA 网络调整成类似的形状，即提高吞吐、降低延时，并改善规模性能。这对所有相关方都是一个巨大的挑战。</p><p>&nbsp;</p><p>最近的市场变化表明，IB（InfiniBand）网络从原来的 HPC（高性能计算）领域突然爆发出来，这部分原因与高性能网络相关。在 HPC 场景下，IB 网络能够满足大型模型场景的需求，因此它受到了关注。然而，现在越来越多的用户，特别是在国内，开始探讨以太网是否能够提供与 IB 相似的性能和能力。</p><p></p><h2>不同场景对算力芯片的运算能力有何需求？</h2><p></p><p>&nbsp;</p><p>姜雨生：针对不同的应用场景，算力芯片的运算能力需求有何不同？能分享几个算力芯片在实际应用中的案例吗？</p><p>&nbsp;</p><p>余曦：不同的应用场景对算力芯片的运算能力有不同的需求。通常，GPU 的算力能力以 Flops（浮点运算每秒次数）来衡量，而 Flops 可以在多个维度上划分，包括 INT8、FP16 以及不同精度的整数计算（INT16、INT32、INT64 等）。不同厂家可能对这些精度有不同的定义。因此，在选择算力芯片时，不一定需要所有精度都非常强大，只需满足特定应用的需求即可。</p><p>&nbsp;</p><p>在与行业用户进行沟通和交流时，发现不同行业和应用对算法精度和性能要求不同。举例来说，有一家专门做推荐算法的公司，他们的模型训练场景与其他行业完全不同，更轻量化。虽然他们的数据量很大，模型也非常先进，但他们需要的精度更低。这是因为他们的业务要求如此。从推荐算法的角度来看，用户的行为和兴趣可以导致不同的推荐结果，因此他们更注重轻量化的模型。推荐算法是基于用户的先前行为和兴趣来进行的。从法律和法规的角度来看，用户的画像不能太精确，以避免直接关联到特定个人，因为这在法律上通常是不允许的。因此，推荐算法的设计并不一定需要极高的精度，只需要达到所需的效果即可。</p><p>&nbsp;</p><p>在不同领域，比如医疗生物制药领域，不同阶段的计算需求也不同。在医疗生物制药领域，每个阶段需要不同的算力和算法类型。这些需求可以使用不同的集群来处理，类似于流水线的概念。因此，在特定场景下，可以使用特定的算力单元来实现更高效的处理。</p><p>&nbsp;</p><p>一些公司专注于提供特定场景的 AI 算力，而不仅仅是通用的 GPU。他们的逻辑是根据特定场景和需求，为算法提供定制化的算力，以加速计算。虽然在其他方面可能相对较弱，但这种方法在特定应用场景中有着广阔的前景和市场机会。</p><p>&nbsp;</p><p>姜雨生：有数据提到，我国算力利用率仅 30%，大量算力仍处于闲置状态，如何才能提升算力的利用效率？算力产业链的各个相关方需要做哪些工作？</p><p>&nbsp;</p><p>余曦：早在很早以前，一些大型运营商就提出了算力网络或算网融合的概念。这意味着将分散的算力资源联合起来，以便共同提供这些算力资源，并在这一过程中，网络的角色变得非常关键。同时，标识和管理算力资源、调度计算任务也变得至关重要。</p><p>&nbsp;</p><p>运营商和一些边缘计算相关的公司已经在这个领域有了一套有效的算网融合模型和实施方法，可以通过虚拟网络来构建算力网络，并在其上调度不同的算力单元，以提高其利用率。当然，在实现这一目标时，还需要解决一些挑战。举个例子，与云计算领域的情况进行对比。最初，大家都在同一个云上部署应用。后来，人们开始将应用分散在不同的云上，因此出现了多云平台或多云调度的概念。同样，对于算力资源，我们也需要类似的多云平台，将不同的算力资源整合在一起，并提供一种选择不同资源的方式。这将有助于提高整体算力资源的利用效率。</p><p>&nbsp;</p><p>姜雨生：有观众提问，现在的大小模型、网络模型是怎么样的呢？</p><p>&nbsp;</p><p>余曦：对于现在的大小模型和网络模型，实际上主要涉及到网络支撑的算力节点数量。越大的模型需要更多的算力节点，这也会带来对网络的更大挑战。但总体来说，网络的组建逻辑基本相同，只是需要考虑容量更大的交换机、网络拓扑结构等因素。不管是大模型还是小模型，它们对网络的要求都是一致的，主要包括以下三个方面：吞吐量、延迟和可扩展性。这些因素在不同规模的模型中都具有重要意义。</p><p>&nbsp;</p><p>姜雨生：观众提问，研发算力芯片，芯片的质量和稳定性怎么保证？</p><p>&nbsp;</p><p>余曦：我们的公司在算力芯片领域有着不同的专注点。我们的主要工作是集中在网络处理单元上，任务是协助算力芯片进行数据搬迁操作。具体来说，我们的任务是帮助算力芯片快速而高效地将数据从一个地点（点 A）搬移到另一个地点（点 B）。这个领域是我们的专业领域。</p><p></p><h2>从“计算”到“智算”</h2><p></p><p>&nbsp;</p><p>姜雨生：随着生成式 AI 技术得到广泛应用，算力产业正进入产业“智算”时代，从“计算”到“智算”，最大的变化是什么？如何应对算力需求的变化？</p><p>&nbsp;</p><p>余曦：实际上，最大的变化在于从以 CPU 为主的算力提供服务者，演变为 CPU 周围有各种专业的辅助芯片，例如 GPU、FPGA、AI 芯片等，每个芯片都有自己的专长领域。这些辅助芯片与 CPU 一起组成了一个多样化的算力集群，以多种不同的算力方式提供服务，这就是现在智算中心的基本逻辑。</p><p>&nbsp;</p><p>从我们的角度来看，我们的主要目标是在智算中心中补充整个产品线。我们更专注于网络方面的工作，因此在智算中心的角度来看，我们的任务是将服务网上的工作卸载到 DPU 卡上，以提高处理效率并创建更多的云服务。在参数网络方面，我们正在开发基于 RDMA 的底层网络支持，虽然它不同于传统 RDMA 网络，但我们的实现能够让 RDMA 应用或网络达到更高效的状态，就像之前提到的那个满三角形的状态一样。</p><p>&nbsp;</p><p>姜雨生：当前企业在使用算力时主要存在哪些门槛？怎样才能降低算力的使用门槛？</p><p>&nbsp;</p><p>余曦：当前，在智算中心领域，企业内部搭建一个算力平台的门槛相当高。这种困难可以分为几个层面。首先是基础设施，包括服务器部署、网络部署以及基础环境的调试和优化。这些任务需要大量的人力、物力和精力，而许多企业缺乏专业的基础设施人员，他们更关注应用层面，即如何使用现有的硬件来运行模型进行计算。这是他们擅长的，但他们缺乏底层基础设施的技能。</p><p>&nbsp;</p><p>对我们来说，我们提供了一种成熟的解决方案，利用我们自己的裸金属云解决方案，使客户能够在底层基础设施层面快速构建基于裸金属的算力单元。例如，对于一个典型的 AI 服务器配置，如两台机器、每台机器 8 张 GPU 卡，客户需要快速将其投入使用。除了底层的网络配置外，还需要建立许多服务，例如客户需要能够灵活选择使用多少台机器以及这些机器上的技术环境，如操作系统、CUDA 等。通过我们提供的完整的裸金属云解决方案，客户可以在完成底层物理层的网络配置后，快速实现服务化，从而加速他们的计算工作。</p><p>&nbsp;</p><p>通过这种服务化方式，客户可以像在云计算场景下选择云实例一样，快速启动一台机器或一组机器。在选择过程中，他们可以自由选择操作系统的版本、包含的 CUDA 版本以及各种框架组件等，这些选项都以服务的形式提供。例如，如果客户需要 8 台 8 卡的机器，他们只需在 Web 界面上点击几下，几分钟内就能启动整个服务，完成后可以迅速释放资源，供其他同事或部门使用。这实际上展示了我们现在具备的快速交付的能力，构建了一个称为智算中心的底层基础设施。</p><p>&nbsp;</p><p>姜雨生：大模型时代的到来，会让 AI 芯片市场格局将发生巨变吗？您会用哪几个词来形容当前的行业状态？</p><p>&nbsp;</p><p>余曦：我认为目前国内 GPU 行业正迎来一个巨大的发展机遇。根据我们之前的沟通，国内的 GPU 厂商都非常看好未来的发展前景，并全力以赴开发下一代产品。对于上一代产品或现有产品，它们都存在一些不够满意的方面，例如跨机通信能力和算力、性能覆盖程度等。我们预计，可能在今年年底或明年上半年，国内将涌现出大量新一代的 GPU 产品，这将为国内的技术和算力平台建设增添新动力，这是一个明显的趋势。</p><p>&nbsp;</p><p>姜雨生：有观众提问，听说 AI 的网络用的是 RoCEv2？</p><p>&nbsp;</p><p>余曦：在大规模的 AI 网络部署中，目前是不使用RoCEv2的，因为存在一些问题，尤其在 AI 场景下这些问题更加显著。首先，RoCEv2 存在固有的 PFC（Priority Flow Control）问题，在整个 AI 场景下特别明显。具体来说，RoCEv2 在以下两个方面存在问题：</p><p>&nbsp;</p><p>扩展性问题：&nbsp;RoCEv2&nbsp;无法支持大规模部署，这在大规模AI网络部署中成为一个明显的障碍。PFC问题：PFC可能会导致一些问题，例如死锁（DEADLOCK），这些问题妨碍了其扩展性的提升。</p><p>&nbsp;</p><p>因此，目前来看，如果从大规模 AI 网络的实施角度考虑，通常有两种主要途径：</p><p>&nbsp;</p><p>第一种途径： 大型互联网公司内部自主研发以太网协议实现。这种自研实现可以支持非常大规模的 RDMA 网络部署，并且通常包含一些特殊定制的功能。这些公司通常将这些实现视为自家的核心竞争优势，因此对外部是不公开的，你只能看到一些相关的研究论文和借鉴的参考资料，但无法获得具体实现的细节。第二种途径： 基于英伟达的 InfiniBand（IB）网络。目前，IB 已经被证明可以在大规模 AI 算力网络中提供支持。</p><p>因此，包括国内 DPU 制造商在内的行业内企业都计划基于 DPU 实现端到端的高性能网络，该网络基于以太网，但能够提供类似 IB 网络的性能支持。这是当前的主要趋势之一。</p><p>&nbsp;</p><p>姜雨生：有观众提问，大模型训练算力和推理算力消耗量是多少？”</p><p>&nbsp;</p><p>余曦：在讨论大模型的训练和推理时，实际上涉及到两个不同的场景：训练和推理。训练阶段是模型自身在背后进行学习和优化的过程，而推理阶段是将已训练好的模型拿到前端，用于为外部提供服务。</p><p>&nbsp;</p><p>在不同的时代和情境下，训练和推理所需的算力可能存在差异。例如，目前大型模型的训练可能需要数千张卡并耗费大量算力，但一旦训练完成并生成了可用的模型，模型在前端提供推理服务时，算力的消耗可能会因前端的服务需求而变化。</p><p>&nbsp;</p><p>这个问题可以从扩展性和分发性的角度来看待。训练阶段通常需要大量计算资源，但通常是固定的。然而，在推理阶段，算力需求是动态的，并且会随着用户数量的增加而增加。因此，前端推理所需的算力可能会远远超过训练阶段的计算能力。</p><p>&nbsp;</p><p>姜雨生：有观众提问，传统网络转到英伟达的 IB 网络，需要改造什么的么&nbsp; 会影响算力么？</p><p>&nbsp;</p><p>余曦：在传统网络和 InfiniBand（IB）网络之间，一般没有改造的情况，通常是进行全新建设。现在我们主要看到的是全新建设的场景，例如，如果要建立一个智算中心以及智算中心后面的参数网络，通常是从头开始建设，而不是对现有数据中心进行改造。</p><p>&nbsp;</p><p>从两个维度来看，这一决策是有其原因的：</p><p>协议不同：传统网络通常使用以太网协议，而 InfiniBand 是一种不同的协议。除非你使用英伟达自家的 Spectrum 芯片环境，它可以进行以太网和 InfiniBand 之间的切换，否则你不能在传统以太网交换机上切换到 InfiniBand 网络。带宽需求不同：传统环境中，通常的带宽需求不是特别高。例如，在云环境中，大多数情况下使用的是 25G 以太网。然而，在参数网络中，通常以 100G 或 200G 的带宽为起点。如果要实现顶级配置，那么需要使用 400G 网络来支持整个计算过程。因此，在原有的 25G 网络下直接进行改造以支持 200G 或 400G 网络是不现实的，因此需要进行全新建设。</p><p></p><h2>高性能网络对于 AI 智能领域至关重要</h2><p></p><p>&nbsp;</p><p>姜雨生：大模型的爆发式增长给算力芯片带来哪些挑战？芯片厂商如何才能更好地满足大模型厂商的需求？大禹智芯有哪些实践经验可以分享下？</p><p>&nbsp;</p><p>余曦：这方面的经验主要涉及定制化。就定制化而言，在大模型的场景下，定制化的需求相对较少。因为只要你的支持足够完善，大模型的应用层、中间层（也可以称为中间件层或 API 层）以及底层硬件之间的解耦非常强。这种情况下，在大模型的角度来看，应用层、中间层以及底层硬件之间的明确划分是相当好的。尽管在整个 AI 大模型的运行过程中，这三者之间的协作非常紧密，耦合度很高，但从每一层到每一层的界面角度来看，划分是相当清晰的。</p><p>&nbsp;</p><p>我们主要从底层入手，专注于底层的开发。但我们的目标是为中间层或通信层以及上层应用提供服务，使它们不必感知我们底层所做的工作。尽管在我们底层可能有一套独立的实现，但对于上层应用来说，这些实现是透明的，不需要感知。从这个角度来看，对于那些只关注上层应用的用户来说，他们实际上不太关心底层的实现细节，只要能够达到所需的效果即可。在这种情况下，定制化的功能需求较少。</p><p>&nbsp;</p><p>然而，也存在一些特殊情况，即客户从上到下都关心，并且更加偏向底层的场景，可能会需要一些定制化的功能。但需要指出的是，这些定制化的功能实际上都是围绕着刚才在大模型网络中提到的那三个角，即吞吐量、延迟和扩展性展开的。不同的实现机制可能会在这三个角度上有一些不同的理念。</p><p>&nbsp;</p><p>目前，我们正在与一些头部客户讨论与实现相关的理念，希望能够汲取彼此的长处，将我们的优秀想法输出给这些头部客户，共同建立一个良性的环境，真正打造出一个高性能的算力网络。</p><p>&nbsp;</p><p>姜雨生：对于国内芯片厂商来说，这其中有哪些机遇与挑战？这个过程中，大禹智芯的技术产品策略是否发生了变化？</p><p>&nbsp;</p><p>余曦：我们的产品路线从一开始就是围绕着智算中心的两张网络来制定的。就目前而言，我们在服务网络方面已经处于一个非常成熟的状态，通过 DPU 实现了云服务的底座，这一方面已经准备就绪。现在我们正专注于投入更多的精力和资源，来加强我们所说的“算力网”，也就是背后的计算网络。在此方面，我们已经进行了许多关于算力网络的讨论，而现在我们正全力以赴地投入到算力网络的发展中。</p><p>&nbsp;</p><p>未来的愿景是，通过我们的大禹智芯网卡，能够与国家计算能力布局中的两张网络共同发挥作用。一张网络是服务网络，用于构建智算中心的云底座，另一张网络是参数网络，用于构建智算中心内部的高性能计算网络。这两张网络将协同合作，为未来的计算需求提供支持。</p><p>&nbsp;</p><p>姜雨生：下一步，大禹智芯有哪些技术探索和产品规划？有哪些技术难题是我们在未来需要解决的？</p><p>&nbsp;</p><p>余曦：我们计划在明年上半年推出我们自己的全新算力网络产品。我们对此进行了前期的研发工作。</p><p>&nbsp;</p><p>姜雨生：有专家提到“技术突破是算力发展的根本”，您怎么看？展望未来，我们应该如何更好地推动算力技术的发展和应用？</p><p>&nbsp;</p><p>余曦：在 AI 芯片领域，我们希望国内的芯片制造能力能够有明显突破。目前，AI 领域主要包括计算、存储和网络三个方面。在计算方面，国内已经有一些产品，特别是 CPU 和 GPU。然而，底层芯片性能的提升仍然是关键。这对于像我们这样专注于网络层的 DPU 制造商以及 GPU 制造商来说，都是一个积极的因素。</p><p>&nbsp;</p><p>从技术实现的角度来看，高性能网络对于 AI 智能领域至关重要。数据传输和计算之间的时间占用比例几乎是 1:1，尤其是在处理大型模型时，网络的占比可能更高。因此，提升网络能力将提高智算中心内所有算力单元的使用效率。在这个领域，有一些有趣的网络协议、想法和概念，需要一个有利的环境来实现。这个实现过程需要各方合作，将各自的能力输出，以建立一个有利于实验和创新的环境。解决问题是一个从 0 到 1 的过程，现在我们正在解决 0 到 1 的问题。一旦 0 到 1 的问题解决了，我们可以再看如何解决 1 到 10 的问题，需要哪些步骤和阶段。因此，我们希望有一个领导者，并创造一个培育创新的环境，让国内的合作伙伴能够将他们的想法付诸实践，验证效果，并确定未来的方向。这对于整个行业的发展非常重要。</p><p>&nbsp;</p><p>姜雨生：目前 DPU 行业的就业情况如何？需要哪些方面的人才？</p><p>&nbsp;</p><p>余曦：我们以 DPU 整个的技术栈来看，可以分为四个主要部分：硬件、软件、调度系统和业务平台。</p><p>&nbsp;</p><p>硬件层：硬件层涉及芯片能力，比如做自研芯片或&nbsp;FPGA就需要找相对应的团队，像FPGA&nbsp;层实现了网络和存储侧的功能，在这之上就是软件层的部署。软件层：软件层包括操作系统层和内核层的优化以及定制化。此外，还有与开发框架相关的工作，例如&nbsp;DPDK、SPDK。有这些开发框架开发经验的人才也能在参与到 DPU 这个行业之中。调度系统：这一层涉及与云服务相关的组件，包括 OVS、VPP、存储组件（如 Ceph 等）。与这些组件相关的团队也可以在 DPU 行业中发挥作用。业务平台：业务平台层包括与云平台、云管理以及一些开发相关的团队。这些团队可能在开发类似于 OpenStack、K8s&nbsp;等开源项目以及云平台上的业务平台等方面发挥作用。</p><p>&nbsp;</p><p>所以，从技术栈的角度来看，可以说我们将原本在云上的完整技术栈全部迁移到了 DPU 领域。这意味着无论是从底层硬件到上层的业务平台，都在 DPU 行业有所涉及。</p><p></p><h4>嘉宾介绍</h4><p></p><p>&nbsp;</p><p>特邀主持：</p><p>&nbsp;</p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p>&nbsp;</p><p>嘉宾：</p><p>&nbsp;</p><p>余曦，大禹智芯产品及解决方案负责人。曾任当当网首席网络架构师、思科大中华区互联网事业部总监、Fungible 中国首席架构师，在数据中心网络、云计算基础设施服务、高性能网络等领域有丰富的实践经验。近 10 年间主要参与了国内各大型互联网云计算厂商数据中心网络架构的设计和建设，见证并参与了国内数据中心从物理机时代向云计算时代发展过程中计算、网络、存储等基础设施的各个发展阶段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf</id>
            <title>AI 时代下的 SUSE 新洞察：无处不在的边缘计算革命即将到来</title>
            <link>https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf</guid>
            <pubDate>Wed, 27 Sep 2023 06:30:00 GMT</pubDate>
            <updated>Wed, 27 Sep 2023 06:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 生成式 AI, 数字创新, 技术变革, 数字信任, 网络安全

<br><br>

总结: 随着生成式 AI 等前沿技术的发展，数字创新正在引领技术变革。企业需要建立数字信任，通过网络安全实践和数据保护措施来保障数字创新。SUSE作为开源解决方案领导者，致力于推动安全、稳定、可靠、可互操作的技术进步，帮助企业应对市场需求。边缘计算的快速发展对基础设施提出更高要求，SUSE与合作伙伴共同推出了边缘计算解决方案。云原生技术在边缘运行面临挑战，需要提供性能、安全、容灾自治等能力，并提高资源利用效率。 </div>
                        <hr>
                    
                    <p>随着生成式 AI 等前沿技术的迅速发展，一场全新的技术变革正在悄然引领数字创新的浪潮。这种变革为企业未来发展带来了无限可能，使其能够高效应对 IT 挑战、提高生产效率，创造出前所未有的商业价值。然而在这场变革的另一面，“数字信任”已经成为不可或缺的标准，各大厂商都希望通过严格的网络安全实践和强大的数据保护措施，为企业数字创新提供坚实的保障，确保其能够在充满挑战的数字时代稳健发展。SUSE 作为全球范围内创新、可靠且安全的企业级开源解决方案领导者，正在持续推动其在安全、稳定、可靠、可互操作等方面的技术进步，以帮助企业应对不断变化的市场需求。</p><p></p><p>由<a href="https://www.infoq.cn/article/2fHFRAFOYTE5QZaXbU43"> SUSE</a>" 主办的年度数字创新峰会——SUSECON 深圳 2023（下文称“本届峰会”）于 9 月 22 日圆满落幕。通过本届峰会，我们可以非常清晰地看到 SUSE 在过去一年里的探索，SUSE 在边缘计算方面联合合作伙伴，做出了多种尝试。同时，愈演愈烈的生成式 AI 为整个技术市场带来了无限机会，而在此浪潮下的 SUSE 也敏锐地把握了这一新兴技术的潜力，积极投身于相关研究和开发工作中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c5652646de05a0a8fb32c1ad857fc86.jpeg" /></p><p>图：SUSE 大中华区总裁陈毅威发表主题演讲</p><p></p><h2>一、边缘计算无处不在，对基础设施提出更高要求</h2><p></p><p></p><p>随着 5G、物联网、云计算等技术的快速发展，边缘计算得到了广泛应用。由于物联网设备的数量不断增加，数据产生和传输的规模也在快速增长，传统的云计算模式已经无法满足低延迟、高带宽、高可靠性的需求，于是将计算任务分配到网络边缘的设备上成为了解决这个问题的一种有效方法。</p><p></p><p>与云计算相比较，边缘计算就近布置，可以被理解为云计算的下沉。边缘计算通过将数据在靠近设备或终端进行计算和处理，大大提高了数据处理的效率和实时性，同时减轻了中央服务器和网络的负载。此外，边缘计算和云计算需要结合使用，例如在大数据分析中，边缘计算可以处理本地设备上的数据，云计算则可用于存储和分析大量数据。但这也意味着，边缘计算对基础设施有更高的要求，它需要在网络边缘的位置处理和存储数据、部署大量的小型数据中心和设备，设备规模、能源消耗、运维管理和安全性等就成为了重点考虑因素；同时，边缘设备的计算和存储资源往往比中心服务器更为有限，这也限制了其能够处理的数据量和计算的复杂度，边缘计算的能耗和硬件资源效率也面临着挑战。</p><p></p><p>为了克服这些挑战并充分发挥边缘计算的优势，SUSE 与瞬优智慧达成合作，联合发布了“鼎瞬 Peerless 实时业务协同解决方案平台”，广泛应用于工业制造、物流运输和智能城市等领域。这是一款面向企业级客户的云边端协同平台，集软、硬件于一体，在数字化全生命周期提供从云端至近边缘端到边缘端的全面支持，以实现各业务系统（如物联网终端设备等）海量数据的实时感知、实时处理、实时分析、实时决策。</p><p></p><p>值得一提的是，该解决方案整合了 SUSE 的分布式基础架构解决方案，包括 SUSE Linux Enterprise Server、SUSE Manager、Rancher 和 NeuVector，为云平台的稳定基础架构提供了强有力的支撑；SUSE Linux Enterprise Micro、Rancher、K3s、Longhorn 也为解决方案中的边缘智能网关、近边缘智能一体机提供了高可用的基础运行架构。</p><p></p><p>当前，SUSE 正在与联想、中科云谷等行业领导者一起探索边缘的更多可能。SUSE 和联想将充分发挥各自在技术、产品、市场、生态等方面的优势，以边缘硬件、OS、软件定义基础设施为核心数字底座，合力打造边缘原生的全栈式企业级解决方案。SUSE 与联想在边缘计算领域的合作主要分为三个阶段：</p><p>第一阶段：基于 SUSE 现有的边缘计算产品组合，联想提供边缘硬件设备算力支持，共同打造边缘计算节点。目前小盒子（网关）的适配工作已基本通过测试，服务器适配工作正在进行中。第二阶段：联想推出自有边缘计算平台，结合 SUSE 的操作系统，打造商业化边缘计算平台和软件平台。目前第一阶段和第二阶段同步进行，第二阶段的工作重点将聚焦在边缘服务器和边缘云平台的适配上。第三阶段：双方将在边缘计算产品系列中进行更深度的融合，将 SUSE 的操作系统、Rancher、NeuVector 等组件纳入联想的边缘操作盒一体机产品体系。同时，双方将实现统一架构的管理运维框架，包括边缘管理平台与 SUSE 和管理平台的结合，以实现全栈式全方位全要素的管理和监控运维能力。</p><p></p><p>此外，在本次峰会上，SUSE 与中科云谷达成了战略合作，联合推出了中科云谷云原生技术平台。该平台基于 Rancher 容器云、微服务、DevOps 等云原生技术构建，提供丰富、通用的数字化转型共性组件。更重要的是，中科云谷目前已经完成了对 SUSE 零信任容器安全平台 NeuVector 的全面测试，近期将正式上线生产环境；同时，双方正在探索将 SUSE Edge 边缘解决方案纳入平台以帮助客户应对复杂的边缘环境。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/879c61c1f6a5c3f9518e4ce7f66b6b13.jpeg" /></p><p></p><p>客观地说，关于边缘计算的未来发展，其实就像中国科学院王义博士在本届峰会上分享的那样，“全球工业、农业、医疗等重要领域对边缘计算系统的需求逐渐清晰，并形成了大规模的市场。与传统操作系统相比，边缘计算操作系统需要在分布式协同、实时性、可用性、安全性、高能效等方面为用户提供更先进的系统能力。”边缘计算正在高速发展，已经成为了现代数字生态系统的重要组成部分，即将像水电一样无处不在。为了满足日益增长的需求，企业需要构建更加稳定、灵活和可扩展的边缘计算基础设施，为未来在数字化世界的发展提供强有力的支撑。</p><p></p><h2>二、在边缘运行云原生面临着许多挑战</h2><p></p><p></p><p>边缘计算应用规模在增加，技术挑战及企业用户需求也随之增加。云边基础设施存在差异，云原生能力直接下沉应用到边缘时，除了需要提供等同于中心的性能指标、安全隔离、容灾自治、架构感知等能力，还需要不断完善云边以及边边高速通道建设等，进而提升建设难度系数。“小而多”的边缘节点，资源复用率低，这就需要云原生技术能够根据资源池化的能力和资源性能做灵活的弹性调度，以实现更高效的资源利用。</p><p></p><p>要知道，云原生技术对部署环境有明确的要求，需要对边缘侧的海量异构资源进行灵活的适配，这就意味着用户需要对各种硬件和操作系统进行抽象，并为上层应用提供统一的接口，以实现资源池化和资源性能的灵活调度。开发者在日常工作中最常用的容器集群管理工具 Kubernetes 便是其中一个代表——边缘侧设备的 CPU、内存等计算资源配置通常较低，主要用于应用自身，难以分配更多资源供 Kubernetes 的中间层平台使用；在边缘环境网络不稳定的情况下，Kubernetes 自身难以稳定运行。而且虽然社区中目前已经有了轻量化的 Kubernetes，但多为单节点架构，并非是为了运行多节点的生产级环境而设计的，难以提供生产级的高可用服务。</p><p></p><p>于是，为了解决 Kubernetes 能够运行在边缘计算环境的挑战，让用户能通过下一代嵌入式边缘设备获得成功并具备扩展能力，SUSE 开发出了轻量化的 Kubernetes 发行版——K3s，轻量化的 Linux 操作系统 SUSE Linux Enterprise Micro, 在 Rancher2.7 中已经实现了对 K8s、K3s 以及基于 OCI 格式的 SUSE Linux Enterprise Micro 的统一管理，并与 SUSE 的超融合基础架构解决方案 Harvester 和 SUSE 的零信任容器安全平台 NeuVector 组合，推出了 SUSE Edge 云原生边缘管理解决方案，为从应用程序到 K3s 再到操作系统的整个堆栈进行了安全策略的无缝集成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e600725ef9e5ba91eb5cd045be378fe.png" /></p><p></p><p><a href="https://www.infoq.cn/article/HmppHMJseJVj7sZkR7Te">SUSE</a>" Edge 能够对无人监控的边缘环境进行低成本的统一管理，用户可通过统一的操作面板管理 Kubernetes 和底层操作系统，大幅降低了运维、部署工作的复杂性。SUSE Linux Enterprise Micro 是专为边缘环境中的容器化工作负载量身打造的轻量级操作系统，其安全可靠、无需维护的特性，可以实现简单而重要的边缘设备管理任务自动化运行。基于此，开发人员能够快速完成测试和编程，构建涵盖可穿戴设备、智慧城市、交通运输等众多领域的各类应用程序。</p><p></p><p>事实上，无论是通用的边缘场景，还是电信、汽车、卫星等需要额外功能的各类边缘场景，SUSE 都能基于不同用例提供完全契合客户需求的边缘解决方案。全球领先的家居建材用品零售商 Home Depot 便非常欣赏 K3s 的简易性，只需要运行单个二进制文件就可以随时拥有一个经 CNCF 认证的 Kubernetes 发行版。店内应用程序一直是容器化的 Home Depot 将所有 2300 多个零售店都转到了基于 Rancher +K3s 的新架构中，以避免手动维护容器化应用程序的可用性。</p><p></p><p>然而，云原生技术面临的技术挑战不仅在于 Kubernetes 集群的管理，安全问题同样不容忽视。作为 SUSE Edge 边缘解决方案的重要组成部分，SUSE 的零信任容器安全平台 NeuVector 可通过集中化的企业级扫描、自动扩展扫描工具以及新版 Kubernetes (1.25+) Pod 安全准入，实现更高效、更强大的多集群漏洞扫描和准入控制。据悉，NeuVector 拥有十几项技术专利，包括深层数据包检查和行为学习，可以识别容器的行为，仅允许经过批准的白名单在网络链接、进程访问和文件存取方面进行操作；在运行时还提供完整的攻击检测和预防功能，主动保护生产环境。</p><p></p><p>SUSE 安全产品战略副总裁黄飞在本次峰会上发表了题为《建立数字信任，守护开源安全》的主题演讲，正如他所说的那样：“从分布式系统架构到容器技术，SUSE 不仅帮助企业应对云原生环境中的安全挑战，还通过零信任安全策略和供应链安全工具，确保企业业务和服务的安全性。同时，SUSE 还积极参与国际通用的安全标准认证，为企业提供更加可靠的安全保障。总之，SUSE 一直致力于在安全领域为企业提供全面的解决方案。”</p><p></p><h2>三、AI 为全行业数智化转型带来了机会</h2><p></p><p></p><p>SUSE 大中华区总裁陈毅威在本届峰会上的演讲《重塑·创新的力量》中提到，“SUSE 是一家德国公司，跟 SUSE 合作，你会体验到它身上散发的强烈的德国工匠精神，即对自身产品质量的极高要求。SUSE 的文化基因就是开源，SUSE 在各领域都在不断探索。”事实就是这样，自 1992 年 SUSE 在德国纽伦堡成立，至后来 2020 年收购 Rancher、2021 年收购 NeuVector，SUSE 始终走在科技前沿，借助前沿科技助力客户更好创新。近期 ChatGPT 引爆了新一波 AI 热潮，SUSE 再次“走在了路上”。</p><p></p><p>ChatGPT 将生成式 AI 的能力带入人工智能主流应用领域，在今年初其月活跃用户就达到了 1 亿，打破了用户群增长最快的记录。AI 技术已经进入了大规模应用和普及阶段，AI 3.0 时代来临。在这种背景下，生成式 AI 及其多种应用正在重塑行业，帮助各领域实现更快、更准确的数据处理和分析，从而提高工作效率和决策精度，为企业数智化转型提供了更多的机会和可能性，助力其获得更多的商业机会和收益。</p><p></p><p>然而 AI 技术的发展往往需要进行复杂的模型训练和推理，需要大量的计算资源和存储资源，这对算力提出了更高要求，也为很多厂商提供了新的增长点。</p><p></p><p>SUSE 已经迈出了 AI 应用的第一步——在 Rancher Prime 上开发了一个 AI 助手，为用户提供自助、便捷的用户服务，同时提供准确、有效的信息。作为老牌 Linux 技术探索者，SUSE 在 Linux 方面也做了很多 AI 方面的探索。比如今年 7 月，SUSE 发布最新旗舰版企业级 Linux 平台 SUSE Linux Enterprise 15 Service Pack 5（SLE 15 SP5），可以提供对 AI/ML 工作负载至关重要的高性能计算能力，并与 Rancher 协同工作。它是第一个支持全范围机密计算的 Linux 发行版，可以保护在公有云和边缘处理的客户数据，允许客户在任意环境中运行完全加密的虚拟机，安全属性得到了大大提升。</p><p></p><p>使用超级计算机来进行顶级 AI 工作的用户、处理大型数据集和复杂 ML 过程的用户非常喜欢 SLE 15 SP5，就是因为 SUSE Linux Enterprise 拥有强大的安全功能和工具来保护 AI/ML 工作负载和数据。它提供了安全引导、访问控制、加密和审核等功能，以确保符合行业法规（如 GDPR 和 HIPAA）。</p><p>对 AI 的探索推动了 SUSE 的技术创新，不仅增强了 SUSE 的技术实力，也为其在市场上的竞争力提供了有力的保障。正如 SUSE 大中华区总裁陈毅威所说的，他十分肯定，AI 将是 SUSE 非常好的发展契机。“别忘了，硬件和应用的中间，还需要一个更为成熟的操作系统作为支撑。”</p><p></p><h2>四、写在最后</h2><p></p><p></p><p>在本届峰会上，陈毅威和 SUSE 亚太 CTO &nbsp;Vishal Ghariwala 都表达了同一个观点——随着数字化转型的加速，企业需要不断适应市场的变化，提高自身的竞争力，开源技术为企业提供了实现这一目标的重要途径。通过引入先进的开源解决方案，企业可以降低研发成本，加速产品上市时间，提高服务质量，从而在激烈的市场竞争中脱颖而出。</p><p></p><p>从 <a href="https://www.infoq.cn/article/WiNrWV5QrqayNtSGGjFw">SUSE </a>"的发展轨迹中我们可以看到，其超过三十年的开源技术积累是推动全球开源生态发展的重要力量之一。从 1992 年最初的 Linux 发行版开始，SUSE 就坚定地选择了开源的道路。旁观 CentOS 事件，SUSE 的态度也是非常明确的，开源社区应该以合作、交流、共享和创新为核心价值，任何发行版的变更都应该遵循社区的规则和流程，以确保开源社区的稳定、可靠和安全。</p><p></p><p>今年 7 月，在红帽宣布不再对外公开 Red Hat Enterprise Linux（RHEL）源代码后，SUSE 便表示将开发和维护与 RHEL 兼容的发行版，让所有人都可以不受限制地使用该发行版本，未来还计划向该项目投资超过 1000 万美元。当前，为应对这一挑战，用户可以选择使用 openSUSE、SLES、SUSE Liberty Linux 以及刚刚发布的基于 openEuler 的国产操作系统锐蜥 FlexileOS。其中，SUSE Liberty Linux 是最佳选择之一，它是一种适用于混合 Linux 环境的技术和支持解决方案，用户不仅可以获得可信的技术支持，还可以注册并接收针对于 RHEL、CentOS 和 SLES 等系统的更新。</p><p></p><p>SUSE 在 Linux 方面的持续探索之举非常令人动容，它一直都是 Linux 领域的积极探索者和创新者。SUSE 最新推出的自适应 Linux 平台 Adaptable Linux Platform（ALP）就是一个很好的例子，该平台为企业级 Linux 提供了一种在云原生环境中演进用例的新方法，可以应用于从数据中心到云端、再到边缘的任意场景，让用户专注于工作负载，从硬件和应用层抽离出来。通过使用虚拟机和容器技术，ALP 可以让工作负载独立于代码流。</p><p></p><p>ALP 重点关注安全性，最新版本通过机密计算技术提供了一种受信任的执行环境，通过隔离、加密和执行虚拟机来保护所使用的数据，还为未来的扩展式机密虚拟机 (CVM) 支持奠定了基础。其“硬件和运行时认证”功能可用于验证工作负载的完整性，与 FDE 一起初步实现了端到端的数据安全防护。同时，它与 NeuVector 集成，提供了一个安全的生态系统，让 ALP 用户通过 NeuVector 识别恶意行为，并防止底层主机操作系统或其他容器化工作负载受到影响。此外，用户在安装它时，还可选择带有 TPM 的 FDE 来支持静态数据安全性。</p><p></p><p>ALP 的新版本创新就是 SUSE 过去三十余年创新之路的缩影，这些年来 SUSE 不断推出各种创新的开源解决方案，包括存储、虚拟化、容器等一系列的技术创新。这不仅为 SUSE 赢得了市场份额、用户的信任和认可，更为整个开源领域贡献了大量有价值的资源和经验。技术的进步需要众多像 SUSE 一样的厂商共同贡献，企业间通过实施开源策略，共享资源和经验，共同应对挑战，实现自身的业务增长，促进行业的生态繁荣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx</id>
            <title>千亿也不够花！OpenAI 想让员工出售股票来筹资，而最大投资人微软正在“去GPT”</title>
            <link>https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx</guid>
            <pubDate>Wed, 27 Sep 2023 06:10:06 GMT</pubDate>
            <updated>Wed, 27 Sep 2023 06:10:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> OpenAI正在与投资者讨论股票出售事宜，寻求以800亿-900亿美元的估值出售股票。这将使OpenAI成为全球估值最高的初创公司之一。尽管OpenAI在今年早些时候从微软获得了100亿美元的投资，但公司仍在寻求额外的资金。OpenAI预计今年的营收将达到10亿美元，并计划在2024年进一步增加数十亿美元的营收。然而，OpenAI的成本也在不断增加，去年亏损约为5.4亿美元。尽管OpenAI推出了付费版本的聊天机器人，但公司仍需要更多资金来支持产品升级和运营。微软担心运行人工智能功能的成本可能会迅速增长，正在寻找低成本的替代方案。 </div>
                        <hr>
                    
                    <p>据报道，OpenAI正与投资者讨论可能的股票出售事宜，这家人工智能初创公司寻求以800亿—900亿美元（以当前汇率换算，约为5842.48亿-6572.79亿人民币）的估值出售股票，这一估值几乎达到今年早些时候的三倍。</p><p>&nbsp;</p><p><a href="https://www.wsj.com/tech/ai/openai-seeks-new-valuation-of-up-to-90-billion-in-sale-of-existing-shares-ed6229e0">《华尔街日报》</a>"称，此次交易预计将允许员工出售所持股份，而不是以公司发行新股的形式来筹集额外资本。知情人士称，OpenAI已经开始说服投资者，并表示今年OpenAI 营收预计将达到10亿美元，2024年将再增加数十亿美元。</p><p>&nbsp;</p><p>10亿美元的数字与8月份媒体爆出来的数字一致。据报道，OpenAI每月收入8000万美元，高于2022年全年的2800万美元。ChatGPT Plus是其2月份推出的每月20美元的ChatGPT付费版本，推动了OpenAI的收入增长。</p><p>&nbsp;</p><p>另一方面，股票出售可以让员工不必等到公司上市就能了解自己的股权价值，可以帮助公司吸引顶尖人才，并产生流动性，同时也将为OpenAI带来新的估值。800亿美元或更高的估值将使OpenAI成为全球估值最高的初创公司之一，仅次于字节跳动和马斯克的SpaceX。</p><p></p><h2>融到的钱多，但花的钱更多</h2><p></p><p>&nbsp;</p><p>报道称，OpenAI 的目标是向硅谷投资者出售价值数亿美元的现有股票。今年4月，OpenAI从红杉资本(Sequoia Capital)、Andreessen Horowitz、Thrive和K2 Global等投资方获得了3亿多美元的融资，估值达到290亿美元。这与微软今年早些时候宣布的一项大型投资无关，该投资已于今年1月完成，投资规模约为100亿美元。据不完全统计，在OpenAI接受微软的100亿美元投资之前，它在成立七年多时间内已经收到了40亿美元的投资。至此，OpenAI 累积融资金额已经有143亿美元（以当前汇率换算，约合1044.62亿人民币）。</p><p>&nbsp;</p><p>尽管早在5月份就有报道称OpenAI正在寻求筹集更多资金，但如果出售股份的措施继续下去的话，将不会为OpenAI提供额外的运营资金，只是允许其员工剥离部分股份。</p><p>&nbsp;</p><p>5月<a href="https://futurism.com/the-byte/openai-losing-money-chatgpt">The Information</a>"报道，三位了解OpenAI财务状况的人士透露，由于去年开发ChatGPT并从谷歌招聘关键员工，OpenAI的亏损大约翻了一番，达到5.4亿美元左右。咨询公司 SemiAnalysis 的首席分析师 Dylan Patel 预计，每天运行 ChatGPT 的成本为 70 万美元。</p><p>&nbsp;</p><p>根据<a href="https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/">《财富》</a>"的披露，该公司 2022 年5.445 亿美元的总支出中，计算和数据支出 4.1645 亿美元、员工支出 8931 万美元、其他未指明具体项目的运营支出 3875 万美元。这些成本在获得微软年初100亿美元投资前就已经积累起来了。</p><p>&nbsp;</p><p>OpenAI 对产品升级更新的脚步一直没有停下，很明显巨额成本投入也在继续：</p><p>&nbsp;</p><p>9月21日凌晨，OpenAI宣布其文生图工具DALL·E即将升级至DALL·E 3，并将原生集成至ChatGPT中。相比去年发布的DALL·E 2，在提示词相同的情况下，DALL·E 3对文字的理解程度及生成的图像质量显著提升。时常被诟病的“无法在图像上生成文字”的问题，也在这次升级中得到了解决。</p><p>&nbsp;</p><p>9月26日，OpenAI 推出了多模态ChatGPT，除了通过常见的文本框交互外，还将向Plus和Enterprise用户推出语音和图像，用户可以用语音对话或向 ChatGPT 显示正在谈论的内容。语音将在iOS和Android上推出（在您的设置中选择加入），图像将在所有平台上提供。</p><p>&nbsp;</p><p>此外，8 月 16 日，OpenAI 还发布公告称收购了Global Illumination的团队，这也是 OpenAI 自 2015 年成立以来的首次对外收购，但并未公开交易涉及金额。</p><p>&nbsp;</p><p>尽管在OpenAI于2月推出付费版聊天机器人后，公司收入有所增长，但随着越来越多的客户使用其人工智能技术，以及该公司对该软件未来版本进行培训，这些成本可能会继续上升。实际上，微软和其他最近的投资者承担了上述大部分成本，但如果OpenAI不能很快实现盈利，他们可能会停止投入。</p><p>&nbsp;</p><p>与此同时，8月24日有媒体报道称，OpenAI首席执行官Sam Altman下半年将奔赴阿联酋首都阿布扎比等地寻求融资，此次融资的规模巨大，不低于1000亿美元。</p><p>&nbsp;</p><p>据悉，Altman 对VC讲的故事不限于AGI通用人工智能，他表示OpenAI的目标是要实现Super intelligence（超级智能），例如可在一个月内攻克癌症。但目前OpenAI离这一目标还非常遥远，而OpenAI为实现该目标所需要的资金规模不可想象。</p><p>&nbsp;</p><p>目前，Altman还没有通过上市筹资的打算。今年6月，Altman表示，自己不想被公开市场、华尔街等起诉，所以对上市没那么感兴趣。他解释称，当开发AI时，可能会做出一些让公开市场投资者看来非常奇怪的决定。</p><p>&nbsp;</p><p></p><h2>最大的投资人，也扛不住了</h2><p></p><p>&nbsp;</p><p>微软今年早些时候向OpenAI投资100 亿美元后，持有了其 49% 的股份。作为交易的一部分，OpenAI 承诺与其合作并将其人工智能软件与微软的产品集成，以换取用于训练和运行其模型的计算资源。微软确实也竞相在自己的大多数软件产品中内置人工智能功能，包括基于GPT-4的Windows Copilot。但如今，大模型也给微软带来了压力。</p><p>&nbsp;</p><p>该公司担心，随着Windows在全球拥有超过10亿用户，运行这些人工智能功能的成本可能会迅速增长。而微软不想放弃其新人工智能产品带来的经济效益，所以正在寻找低成本的替代方案。</p><p>&nbsp;</p><p>据知情人士透露，最近几周，负责管理微软 1500 名研究人员的Peter Lee，指示其中的许多人开发对话式人工智能，其性能可能不如GPT-4，但规模更小，运营成本也低得多。</p><p>&nbsp;</p><p>据报道，微软的研究小组对于开发像 GPT-4 这样的大型人工智能模型并不抱有幻想。该团队没有与 OpenAI 相同的计算资源，也没有大量的人工来反馈LLM回答的问题以便工程师改进。</p><p>&nbsp;</p><p>微软转向更高效人工智能模型的努力目前还处于早期阶段，不过据说该公司已经开始在Bing Chat等服务中测试内部开发的模型。</p><p>&nbsp;</p><p>微软搜索部门负责人Mikhail Parakhin此前曾表示，Bing Chat 100%依赖GPT-4的创造性和精确性模式。然而，在其平衡模式中，它使用了一种名为Prometheus的新模型和图灵语言模型。后者不像GPT-4那么强大：它们可以识别并回答简单的问题，但当它们面临更棘手的问题时，便将这些问题传递给GPT-4。</p><p>&nbsp;</p><p>在编码方面，微软最近公布了其13亿参数的Phi-1模型，据说该模型是在“教科书质量”的数据上进行训练的，可以以更有效的方式生成代码，但还没有完全达到GPT-4的标准。</p><p>&nbsp;</p><p>该公司还在研究其他人工智能模型，如Orca基于Meta的开源Llama-2模型。据介绍，Orca的性能接近于OpenAI的模型，尽管它更小，资源消耗更少。</p><p>&nbsp;</p><p>报道称，微软的人工智能研究部门有大约2000张来自英伟达公司的显卡可供使用，Lee&nbsp;现在已经下令将其中的大多数显卡用于训练专注于执行特定任务的更有效的模型，而不是更通用的GPT-4。</p><p>&nbsp;</p><p>不可否认的是，OpenAI 和其他开发商，包括谷歌和 Anthropic，在开发高级LLM方面领先于微软，但微软或许能够以极低的成本参与到构建类似 OpenAI 软件质量的模型竞争中。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://siliconangle.com/2023/09/26/openai-reportedly-exploring-share-sale-80-90b-valuation/">https://siliconangle.com/2023/09/26/openai-reportedly-exploring-share-sale-80-90b-valuation/</a>"</p><p><a href="https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending">https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending</a>"</p><p><a href="https://www.infoq.cn/article/JAqkPkkgMFvV0JvcFMtj">https://www.infoq.cn/article/JAqkPkkgMFvV0JvcFMtj</a>"</p><p><a href="https://siliconangle.com/2023/09/26/microsoft-hedges-bets-seeking-cost-effective-ai-models/">https://siliconangle.com/2023/09/26/microsoft-hedges-bets-seeking-cost-effective-ai-models/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE</id>
            <title>Google Vertex AI 推出搜索对话式界面，简化AI建模</title>
            <link>https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE</guid>
            <pubDate>Wed, 27 Sep 2023 00:00:00 GMT</pubDate>
            <updated>Wed, 27 Sep 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 在Google Cloud Next大会上，谷歌介绍了其企业人工智能平台Vertex AI的新功能。该平台旨在实现更高级的用户工作流。谷歌扩展了Vertex AI Search和Conversation功能，并宣布其普遍可用。开发人员可以借助Vertex AI Search从各种企业资源中检索信息，并构建搜索引擎与客户展开互动。Vertex AI Conversation可用于创建声音自然、类人的聊天机器人。谷歌表示，Vertex AI可以帮助开发人员将确定性工作流与生成式输出相结合，构建出迷人且可靠的应用程序。新功能支持处理后续问题，无需重新开启对话。Vertex AI Search还可以与矢量搜索相结合，支持语义搜索、个性化推荐、多模态搜索等。Vertex AI是一个面向非机器学习开发人员的平台。谷歌为Vertex AI提供了生成式AI支持，包括文本模型、文本Embeddings API和模型库。总结：谷歌在Google Cloud Next大会上介绍了Vertex AI的新功能，该平台旨在实现更高级的用户工作流。开发人员可以借助Vertex AI Search从各种企业资源中检索信息，并构建搜索引擎与客户展开互动。Vertex AI Conversation可用于创建声音自然、类人的聊天机器人。Vertex AI可以帮助开发人员将确定性工作流与生成式输出相结合，构建出迷人且可靠的应用程序。新功能支持处理后续问题，无需重新开启对话。Vertex AI Search还可以与矢量搜索相结合，支持语义搜索、个性化推荐、多模态搜索等。Vertex AI是一个面向非机器学习开发人员的平台。谷歌为Vertex AI提供了生成式AI支持，包括文本模型、文本Embeddings API和模型库。 </div>
                        <hr>
                    
                    <p>在<a href="https://cloud.withgoogle.com/next?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Google Cloud Next</a>"大会上，谷歌<a href="https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-search-and-conversation-is-now-generally-available/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">介绍了其企业人工智能平台Vertex AI</a>"的新功能，这个平台旨在实现更高级的用户工作流。</p><p></p><p>在上次<a href="https://blog.google/technology/developers/io-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Google I/O大会</a>"上作为技术预览发布之后，谷歌扩展了Vertex AI Search和Conversation功能，并宣布其普遍可用。</p><p></p><p>借助Vertex AI Search，开发人员可以从各种企业资源中检索信息，例如文档存储库、数据库、网站和其他类型的应用程序。谷歌表示，开发人员可以很轻松地构建搜索引擎，并基于这些信息与客户展开互动。</p><p></p><p></p><blockquote>Vertex AI提供了一个简单的编排层，将企业数据与生成式基础模型以及会话AI和信息检索技术结合起来。</blockquote><p></p><p></p><p>Vertex AI Conversation可用于创建声音自然、类人的聊天机器人。例如，开发人员可以基于网站、企业文档、常见问题解答、电子邮件等创建聊天机器人。</p><p></p><p></p><blockquote>Vertex AI可以帮助开发人员将确定性工作流与生成式输出相结合，将基于规则的流程与动态AI相结合，构建出迷人且可靠的应用程序。</blockquote><p></p><p></p><p>基于谷歌宣布的这些新功能构建的智能应用程序不仅仅是检索重要信息那么简单，而是可以代表用户做出动作。</p><p></p><p>新功能支持处理后续问题，无需重新开启对话，这要归功于多轮搜索。通过扩展和数据连接器与第三方应用程序进行更紧密的集成，从而可以连接到各种应用程序，如MongoDB、Datastax、Salesforce、JIRA等。基于企业数据的生成式输出增加了返回结果质量置信度。</p><p></p><p>Vertex AI Search还可以与矢量搜索相结合，利用其寻找“类似”解决方案的能力来支持语义搜索、个性化推荐、多模态搜索等。</p><p></p><p>Vertex AI Search将在未来支持访问控制，确保用户只能看到他们有权看到的信息和访问其他功能，如引用、相关性评分和摘要。</p><p></p><p>Vertex AI是一个面向非机器学习开发人员的平台。几个月前，<a href="https://www.infoq.com/news/2023/06/ai-ml-data-news-june12-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Google为Vertex AI提供普遍可用的生成式AI支持</a>"，包括基于<a href="https://ai.google/discover/palm2/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">PaLM 2</a>"的文本模型，文本Embeddings API和<a href="https://cloud.google.com/model-garden?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Model Garden</a>"中的基础模型。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO</id>
            <title>和德爷一起 6DoF 互动探险，火山引擎空间重建和虚实融合技术</title>
            <link>https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO</guid>
            <pubDate>Tue, 26 Sep 2023 07:49:11 GMT</pubDate>
            <updated>Tue, 26 Sep 2023 07:49:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 跟着德爷闯东非, Pico, 6DoF, 互动纪录片, 德爷, 530万粉丝, 明星探险家, 第一视角摄影师, 野生动物, 非洲大草原, 互动性, 3DoF, 实拍纪录片, 游戏引擎制作, 6DoF纪录片, 空间重建技术, 虚实融合技术, VR内容, 沉浸性, 交互性, 空间互动, 任务完成, 非洲荒野, 空间重建, 现实世界复原, 稀疏重建, 图像定位, 稠密算法重建, 点云去噪, 补全, 网格优化, 高质量网格模型, 火山引擎视频云, 真实感模型渲染, 神经辐射场技术, 大场景光场重建, 动静态分割, 影子检测, inpainting, 高精度SFM算法框架, 新视角生成效果, 毫末智行合作, 单路采集, 多路合并, NeRF重建.

总结: 《跟着德爷闯东非》是一款全新的6DoF互动纪录片，由明星探险家德爷担任主角。该纪录片采用空间重建技术和虚实融合技术，实现了全新的6DoF互动体验，增强了VR内容的沉浸性和交互性。空间重建技术能够复原现实世界的场景和物品，并转化为数字资产。火山引擎多媒体实验室通过自研算法实现了高精度的场景稀疏重建和图像定位，以及稠密算法重建和网格优化。火山引擎视频云平台具备自动化空间建模链路，可支持大场景重建，并实现高度真实感的模型渲染。火山引擎多媒体实验室还结合神经辐射场技术，研发了大场景光场重建方案，实现了高度真实感的场景重建和复现。 </div>
                        <hr>
                    
                    <p></p><blockquote>《跟着德爷闯东非》是 Pico 一款全新的 6DoF 互动纪录片。主角由在全球拥有 530 万粉丝的明星探险家德爷（Edward James Stafford）担任。观众以第一视角摄影师的身份陪伴德爷一起冒险，近距离观察野生动物，体验非洲大草原的野外生存之旅。</blockquote><p></p><p></p><p>与行业内常见的不具备互动性的 3DoF 实拍纪录片以及不具备写实性的游戏引擎制作的 6DoF 纪录片不同，《跟着德爷闯东非》纪录片的拍摄采用空间重建技术及虚实融合技术，兼顾实拍和虚拟互动，以全新的 6DoF 互动体验，增强了 VR 内容的沉浸性和交互性，让用户跟随德爷的脚步沉浸式体验从城市“跃入”荒野的快感。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/56/68/560d014cbcfef595ec7899b19a3f2368.gif" /></p><p></p><p><img src="https://static001.infoq.cn/resource/image/9d/d9/9d9c2c4fbc8cd9573f1faf6c87cdc3d9.gif" /></p><p></p><p></p><h2>全新VR空间互动性的挑战和难点</h2><p></p><p></p><p>传统 VR 实拍视频的交互通常采用不同选项触发不同结局的 AB 型互动方式，而《跟着德爷闯东非》想要实现的是全新的 VR 空间互动，用户可以抓起虚拟世界中的物体完成任务，比如和德爷一起钻木取火，也可以在场景中自由走动，在非洲草原和德爷一起追捕猎杀珍珠鸡，实现全新的交互并达到高沉浸感，突破传统视频的界限。</p><p></p><p>而想要实现全新的交互和体验高沉浸感，需要做到：</p><p>实际拍摄的 VR 视频和用于互动的场景、物品具备高度一致性，包含几何结构、纹理色彩及光影的一致性，这对于空间重建技术提出了很高的要求，需要做到高精度、高质量、真实感建模，超写实数字复原空间场景，让用户感受到原汁原味的非洲荒野。用户在体验中，虚拟元素与实际场景相互交织，让用户在场景中难以区分哪些是真实的，哪些是虚拟的，达到了最佳的体验效果。这就要求无缝虚实融合技术，需要对重建的数字素材和实拍的视频素材进行像素级配准，这样用户才能够自然沉浸的地在场景中和德爷一起探索非洲，并从实拍 3DoF 视频中德爷手里顺利接过数字重建的 6DoF 互动道具。</p><p></p><p></p><h2>空间重建，复原现实世界</h2><p></p><p></p><p>空间重建技术能够复原现实世界的场景和物品并转化为数字资产，是计算机视觉和摄影测量中的重要研究课题，也在智慧城市、虚拟现实、数字导航与数字遗产保护等方面有着重要的应用。</p><p></p><p>火山引擎多媒体实验室团队自研改进 SFM 算法框架，实现高精度的场景稀疏重建及图像定位。</p><p></p><blockquote>针对特征点提取、匹配算法，通过结合传统特征与深度学习方法，算法在大视角/尺度变化、暗光、弱纹理、运动模糊等多种挑战场景下仍能有效提取足量稳定的特征；通过将特征点纳入自注意力和交叉注意力网络，结合多源传感器输入检索全局最优图像特征匹配，使得算法即使在空地跨视角、鱼眼/针孔/全景跨相机等复杂数据输入的情况下，实现建图完整度、成功率均达到 100%。同时，开发支持多相机系统、多相机模型光束法平差优化算法，同时也兼容其他不同传感器的联合重建，实现高精度、多模态的位姿估计。</blockquote><p></p><p></p><p>在稀疏重建算法之后，需要进行稠密算法重建。</p><p></p><blockquote>火山引擎多媒体实验室通过立体视觉 (Multiple View Stereo，简称 MVS)技术将二维图像信息转化为三维点云信息。团队自研基于多目立体视觉及全景图的深度估计算法，通过神经网络进行稠密深度估计，在野外大场景环境获得高精度的场景稠密几何测量。获得点云信息后，进行点云去噪和补全，并通过点云配准实现场景几何一致性。最后，通过基于 TSDF 和图像语义信息的点云融合策略，进一步滤除噪声，生成更加平滑一致的完整场景点云。</blockquote><p></p><p></p><p>获得场景点云后，进行 Mesh 重建。</p><p></p><blockquote>火山引擎多媒体实验室自研多种网格优化算法，实现网格平滑、去噪、简化和补洞，获得更加精细、完整的高质量网格模型。得益于图像处理期间高精度的相机位姿估计以及图像超分等画质优化，结合自研贴图算法，获得更高清、拼缝更少的高质量纹理贴图。同时通过纹理重打包算法优化，实现更高的纹理利用率，降低存储资源浪费，提升纹理有效分辨率。</blockquote><p></p><p></p><p></p><p></p><p>目前，<a href="https://www.infoq.cn/article/Rx45QcxHI4zZCfMR5r8J">火山引擎视频云</a>"平台具备自动化空间建模链路，助力大场景重建，可支持采集 RGBD/RGB 数据（无人机、手持采集等）自动化上传云平台，2-4 小时后自动产出建模结果，建模精度可达1cm～2cm。同时，火山引擎视频云的云渲染可视化系统，联合自研动态传输算法，可实现高度真实感的模型渲染。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02b37e1b606348e6ea19f8e85833480f.png" /></p><p>图：火山引擎视频云三维重建平台</p><p></p><p><a href="https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT">火山引擎</a>"多媒体实验室将神经辐射场技术（NeRF）与自研大场景建模技术相结合，研发行业领先的大场景光场重建方案，实现高度真实感（psnr&gt;30)的场景重建、复现及后编辑。</p><p></p><p>在具体实践的场景中，动态物体会使NeRF重建出现伪影，借助自研动静态分割、影子检测、inpainting等算法，对场景中和几何不一致的区域进行提取、修复。同时借助自研高精度SFM算法框架，对场景进行高精度的几何重建，包括相机参数估计以及稀疏、稠密点云生成。另外，对场景进行拆分以减小单次训练资源消耗，并可做分布式训练、维护。在神经辐射场训练过程中，针对室外无边界大场景，团队通过优化策略以提升该场景下的新视角生成效果，比如，通过在训练中同时优化位姿提高重建精度、基于哈希编码的层次化表达提升模型训练速度、借助外观编码提升不同时间采集场景的外观一致性、借助mvs稠密深度信息提升几何精度等。</p><p></p><p>以团队同毫末智行合作为例，完成单路采集以及多路合并的NeRF重建，相关成果已在毫末AI Day发布。  </p><p></p><p></p><p></p><p></p><p></p><h2>虚实融合，提升用户体验</h2><p></p><p></p><p>为提升用户沉浸式体验，火山引擎多媒体实验室自研虚实融合技术，将环境实拍全景图与场景模型进行对齐、融合。团队利用先进的人工智能技术，建立全景图图像特征与模型关键点的匹配关系，通过 PnP 算法以及光束法平差算法将全景图注册至场景模型坐标系，实现尺度、位置的统一，从而实现模型渲染与实拍全景视频渲染的统一，达到虚实融合的效果。</p><p></p><p>同时，为扩大用户体验的自由度，<a href="https://www.infoq.cn/article/qC55OH6f6852hFjlZ3o8">团队</a>"针对该场景自研非球面天空盒渲染，克服传统的球面全景图渲染仅在图像采集中心视觉一致的缺陷，进一步提升实拍全景图渲染模型与地形模型的匹配程度，以实现更大运动范围的视觉一致性，进一步提升沉浸式体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec236d31adc57de0cf0de1c9245c5715.png" /></p><p>图：虚实效果对比示例</p><p></p><p></p><p></p><p></p><p>以上2个视频：6DoF互动漫游</p><p></p><h2>物品重建，高精度还原细节</h2><p></p><p></p><p>在《跟着德爷闯东非》互动纪录片中，会有用户虚拟体验探险剧情的桥段，例如钻木取火，木棍训蛇等。为了带来真实的体验，道具往往是在实际拍摄过程中就地取材，有细长的树枝，薄薄的小刀，还有形态复杂的篝火堆。这些道具的重建本身是比较有挑战的，再加上整个拍摄过程比较紧张，留给扫描的时间并不充裕。为此，火山引擎视频云团队沉淀出一套采集方便，操作简单，能还原各类复杂物品的重建系统。</p><p></p><p>为了重建形状比较复杂的道具（例如狭长的木棍、锋利的小刀）。火山引擎视频云采用符号距离场（Signed Distance Fields，简称 SDF）的技术方案来表示三维物体，结合深度学习的方法克服了以上重建难点。对于如何监督神经网络使其准确地拟合该 SDF，火山引擎视频云先用运动恢复结构（Structure from Motion，简称 SfM）算法，精确计算拍摄图像的相机姿态，再利用可微渲染的方法将 SDF 所表示的空间信息渲染到图像上，把渲染得到的图像和该视角下采集的图像做比较，不断优化神经网络，使 SDF 在各个采集视角下的渲染结果尽可能与实际采集的图像一致。为了进一步提高重建精细度，在优化 SDF 的时候加入稀疏重建得到的三维点做约束，能更好的还原物体的细节特征。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b8021233a9722a127a927c05e72f752.jpeg" /></p><p>图：道具重建效果展示</p><p></p><h2>交互技术，让玩法更丰富</h2><p></p><p></p><p>采用虚实融合技术可以构造由空间重建模型和实拍 360°VR 视频两部分构成的 6DoF 互动场景，同时在《跟着德爷闯东非》项目中，多媒体实验室也实现了终端上的交互技术，同内容团队一起创造出了很多有创意性的虚实结合的玩法。</p><p></p><h4>拍照功能</h4><p></p><p>使用离屏相机管道，把从全景视频球上投影出的针孔 2D 图像重新贴在玩家手持的相机模型上，以实现出玩家可以对环境中任意角度拍照的玩法。</p><p><img src="https://static001.infoq.cn/resource/image/3a/a4/3a8483c3477e022edf09dcce6ayya5a4.gif" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5cca054c1d8e6fe07db04816dd5488ee.gif" /></p><p>图：拍照功能示例</p><p></p><h4>物品交互功能</h4><p></p><p>火山引擎多媒体实验室可以估计 VR 视频中的深度信息，结合 3D 虚拟空间中的虚拟物体的位置信息，计算出全景视频球上指定视频元素，对应于玩家在真实的 3D 空间下的位置。从而，实现视频画面上真实物品转换到玩家可交互虚拟物品模型的无缝转换的玩法。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/50/ea/50a972ff746593f453228a90610993ea.gif" /></p><p>图：物品交互功能示例</p><p></p><h2>虚实融合技术的广阔应用</h2><p></p><p></p><p>虚实融合技术目前正处于快速发展的阶段，在众多领域中展现出广阔的应用前景。如游戏、教育和医疗等领域，已开始积极探索虚实融合技术的应用，并取得了不错的成绩：</p><p></p><p>在游戏领域，虚实融合技术赋予了游戏开发者更多创造力和想象力的空间。通过将虚拟元素与真实世界相结合，游戏能够提供更加沉浸式和交互式的体验。玩家可以与虚拟角色和游戏环境进行实时互动，增强了游戏的娱乐性和参与感。</p><p></p><p>在教育领域，也可以看到虚实融合技术的巨大潜力。通过将虚拟内容融入到教学场景中，学生可以以更加生动和直观的方式进行学习，提高学习效果和兴趣。虚实融合技术可以为学生提供与实物互动的机会，使他们能够亲身体验和理解抽象概念，促进知识的深入理解和记忆。</p><p></p><p>在医疗领域，虚实融合可以用于模拟手术训练、辅助手术导航和可视化诊断等方面。通过结合虚拟现实和真实世界数据，医生可以更准确地进行手术规划和操作，提高手术的安全性和成功率。此外，虚实融合还可以用于康复训练和疼痛管理等方面，为患者提供更加个性化和有效的治疗手段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ARwQIt6s0vBAwArADjfW</id>
            <title>答记者问：从 PICO 视角，看 XR 行业发展</title>
            <link>https://www.infoq.cn/article/ARwQIt6s0vBAwArADjfW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ARwQIt6s0vBAwArADjfW</guid>
            <pubDate>Tue, 26 Sep 2023 06:55:17 GMT</pubDate>
            <updated>Tue, 26 Sep 2023 06:55:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 马杰思, 郭文山, 顾知, 李卓然, 熊俊杰, PICO 2023 XR 开发者挑战赛, 应用增加, 用户活跃度, 关键节点, 曲线, 优质内容, 超级爆发, VR, AR, MR, 技术侧, 硬件产品迭代, 内容创新, 平衡, 内容建设, 生活教育, 培训, 游戏, 体育, PICO 设备推广, 内容生态, 行业适用情况.

总结: 郭文山表示，PICO关注用户留存和用户活跃度提升的影响因素。随着应用达到500家，满足不同类型用户品味的内容变得更加重要。马杰思认为，提供优质、破圈的内容是行业超级爆发的关键。PICO采取VR+MR的策略，将设备设计成既能满足虚拟场景需求又能满足现实场景需求的VR+MR设备。在硬件产品迭代和内容创新适配方面，PICO确保兼容性，但不同厂商之间可能存在生态分裂需要开发者适配的情况。PICO认为需要多个方面共同努力，包括推广PICO设备、解决开发者资源不足的问题、探索不同行业的适用情况等。 </div>
                        <hr>
                    
                    <p></p><blockquote>接受采访嘉宾：马杰思-PICO OS产品负责人郭文山-PICO 中国游戏商务负责人顾知-PICO 中国非游商务经理李卓然-PICO 消费者市场部内容团队市场经理熊俊杰-PICO 视频内容负责人</blockquote><p></p><p></p><p>“PICO 2023 首届 XR 开发者挑战赛”报名链接：</p><p><a href="https://www.picoxr.com/cn/2023picodevjam/#/introduce?trackingFlag=EkWdMubc">https://www.picoxr.com/cn/2023picodevjam/#/introduce?trackingFlag=EkWdMubc</a>"</p><p></p><h4>问题 1： 请问随着应用达到 500 家，您在这几年发展过程中，应用增加和用户活跃度是否产生变化，有关键节点和相关曲线的描述吗？</h4><p></p><p></p><p>郭文山：我们内部看到两个维度，关注用户留存和用户活跃度提升的影响因素。当前的 500 多款应用供给不足以满足每个用户的需求。我们认为关键不在于绝对数量，而是长期来看，而是满足不同类型用户品味的内容。其次可以看购买设备后第一个月购买几款内容，这也是很重要的指标。我们内部发现，用户在使用设备的第一个月能找到 3- 4 款最适合自己的内容，会显著提高留存率。</p><p></p><h4>问题 2： 预计应用达到什么量级时，生态能有一个比较大的超级爆发？</h4><p></p><p></p><p>郭文山： 这个问题也是我两年前加入 <a href="https://www.infoq.cn/news/EjqbmPdv0fSvUkWqgZ2k">PICO </a>"时就在探讨的。我认为关键是提供优质、破圈的内容。比如，如果推出像“节奏光剑”这样的产品或者类似“健身环”的破圈产品，可能会快速推动行业进入下一个阶段。我们期待开发者生态中能够结合技术发展趋势，去发现简单、易玩的交互方式，这样的内容有可能成为真正的大作，帮助行业走向下一个阶段。</p><p></p><h4>问题 3： 关于技术侧，随着苹果和国内一些新品发布，伴随 AI 技术的发展，行业对 AR 方向更加肯定。想请问我们对 VR、AR 或 MR 未来格局趋势的看法，会坚定 VR 路线还是会有相应调整？</h4><p></p><p></p><p>马杰思：从一开始的讨论就可以看出，行业发展趋势明显是从 VR 向 XR 发展，从沉浸式虚拟世界向能赋能更多现实世界的 MR 能力发展。因此，我们采取了 VR+MR 的策略，设备可以无缝切换 VR 和 MR 状态。这种技术的优势在于一个设备可以同时满足 VR 和 MR 的需求。我们会持续进行这方面的发展，将设备设计成既能满足虚拟场景需求又能满足现实场景需求的 VR+MR 设备。同时，在 MR 方面我们分享了很多关于环境感知的技术，这是一个开始，未来我们将在这方面做更多的创新和发展。</p><p></p><h4>问题 4： 我们注意到硬件产品迭代速度非常快，每代产品的技术和创新点都不同。一些开发者担心如果现在开发完整产品可能不适应未来产品特性，选择先制作一些 Demo 并放在平台上。另外，尽管产品迭代提升，我们很难看到高质量的产品内容展示，只能看到一些创新的 Demo，但却没有看到后续版本的更新。您如何看待这种情况，认为在硬件产品迭代和内容创新适配方面，包括版本更新上，如何取得较好的平衡？</h4><p></p><p></p><p>马杰思： 这个问题可以分别由我和文山来回答。首先关于兼容性问题，就我们 PICO 产品线而言，你不会看到这种情况。我们现在为 PICO 4 或 PICO Neo 3 开发的产品，肯定也能在未来的产品上运行。例如，我们现在使用的 SDK 2.3.0 版本，如果用于开发 MR 游戏或应用，它在我们下一代产品上肯定也能完美运行，所以不存在你所担心的问题。但是在不同厂商之间，是否存在生态分裂需要开发者适配的情况，这确实存在。当开发者要跨足多个不同硬件时，他们需要去做适配的准备。</p><p></p><p>郭文山：接着来谈谈内容更新的问题。确实，有些开发者会快速制作 Demo 上线，但后续却没有更新。为什么会出现这种情况呢？事实上，中国的 XR 开发者生态规模相对较小，大多数是小团队制作，面临经营资金的压力。当他们上线一个 Demo 时，他们首先验证的是什么？并不是期望立刻爆红成为热门，很多开发者也知道这样的机会不是轻松得到的。但他们仍然要尝试，这也是我理解XR开发者生态的难处，这需要克服困难，他们要拿出第一版作品。他们先测试核心玩法是否奏效，这对于大多数开发者和同行来说是内容制作阶段的一种经历。只有少数开发者可能考虑得更长远，构思不同版本，如半年或一年后的迭代版本。然后会出现这样一种情况，有可能刚上线一个版本，你可能看到他只有 10 个内容，但实际规划可能是 50 个。然而，在第一天上线 10 个内容后，他发现核心玩法未能满足用户需求，就会撤退，进入下一阶段，打磨自己的玩法。我尝试<a href="https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT">理解</a>"为什么我们会注意到和观察到这种现象，这也是行业发展的一种规律。</p><p></p><h4>问题 5： 除了开发者大赛，还有哪些方面可以完善我们的内容建设？对于生活教育、培训、游戏、体育等方面，目前我们更侧重于哪一个？</h4><p></p><p></p><p>郭文山：我们认为需要多个方面共同努力，而不是单一侧重于某个方面。首先，对于 PICO 来说，最重要的是确保硬件产品的 OS 和 SDK 强大可靠，这是我们的首要任务。我们要把 PICO 设备在中国和全球推广好，这是最关键的。我们也在和开发者交流，了解他们的期待。大部分开发者的第一反应是我们要推广 PICO 设备，这对我们的硬件、软件 OS 至关重要。此外，我们也要注重内容生态。我们正在尝试多种方式，包括成立专门团队进行发行，解决开发者资源不足的问题。我们也推出了 Beta 版本的快速体验通道，并举办了社区活动。我们认为不能依赖于单一点，我们需要系统地探索哪个环节对开发者最重要，这也需要和开发者共同讨论，因为每个开发者面临的困难都不同。我们希望能根据不同的情况，提供组合服务，以满足大家的需求。</p><p></p><p>顾知：针对不同行业的适用情况也不同。例如，视频行业可能已经比较成熟，而新兴领域如汽车发布可能需要更多的行业和企业付费。创意工具可以适用于办公场景、线下培训、会展等多个场景。我们还可以探索从沉浸式到虚拟现实的新想法，结合大环境和大空间，这是一个有创意的领域。从 2D 移动端到 VR 入口端，可以尝试多种使用情况。我们也有“抢先体验专区”，提供一种轻松快捷的尝试方式，以及今天的挑战赛等，让开发者可以尝试新的想法。我们鼓励大家在不同行业想象和尝试，从 2D 到 VR，尝试多种入口端，以满足不同用户的需求。</p><p></p><h4>问题 6： 之前有媒体提及 PICO 在内容方面要追赶内容数量和质量，并指出可能受到海外同行的压力。对此，PICO 是如何看待这种评价？</h4><p></p><p></p><p>郭文山：我们并不是以同行之间的压力为考量。我们更关注用户的需求和视角。当用户决定花费 400 美金购买 PICO 时，他们综合考虑了很多因素，不仅仅是硬件配置和系统体验的流畅程度，也包括是否有丰富的内容。因此，我们认为从用户视角出发，内容工作和内容生态的建设非常重要。</p><p></p><p>从绝对数量上看，确实在去年的时间点，我们去年应用数量确实落后于同行不少，但这不是我们的压力来源，我们真正面临的压力来自于用户。我们感受到了我们还需要在满足用户需求方面，做出更大努力。这种压力将持续存在，不仅是今天，也会延续到下一个阶段。因此，我们将持续努力以满足用户的多元化需求。竞争考虑对我们来说是次要的，我们更关注如何为我们的<a href="https://xie.infoq.cn/article/40e1258dc998d9c7ce6380c8a">客户</a>"提供优质服务。</p><p></p><h4>问题 7： 对于非游戏类开发者，PICO 这次比赛侧重于哪些方面，有哪些机会？</h4><p></p><p></p><p>李卓然：PICO 这次比赛更加侧重非游戏类开发者。在中国，XR 游戏开发者圈子相对较小，而面向未来的技术趋势和交互提出了新的可能性。原本从事手机、安卓、iOS 等开发的人员也有机会进入 XR 这个领域。这次 XR 开发者挑战赛的重点是不要求开发者制作出可运行的实际 demo，也鼓励传统行业以及 ToB 领域的开发者参与。比赛规则还强调更开放的场景设置，降低比赛门槛，拉动不同背景的开发者参与。同时，比赛设置了非游戏赛道奖励，直接激励非游戏内容的产出。</p><p></p><h4>问题 8： 未来，PICO将如何加强与开发者之间的联系，促进生态的发展？</h4><p></p><p></p><p>郭文山：PICO 将采取多种方式加强与开发者之间的联系，推动生态的发展。首先，技术布道是重要的一方面，通过与垂直领域的媒体合作，向开发者群体介绍产品 SDK、技术进展和最新可能性。这将提供开发者更多了解 PICO 技术方面的机会。</p><p></p><p>其次，PICO 重视社区工作，包括开发者社区的建设。他们鼓励开发者利用抢先体验端等平台，建立与用户的社区，分享经验、交流技术。PICO 还会有计划地组织社区交流活动，分享最佳实践，例如通过专访形式将实践经验传播给更多同行。</p><p></p><p>总的来说，PICO 将通过技术布道、社区建设和合作活动等多种方式，与开发者建立紧密联系，促进XR生态的健康发展。</p><p></p><h4>问题 9： 对于开发者来说，在未来 2 至 3 年的市场情况中，消费者对哪种内容和技术更感兴趣？从 PICO 的角度，对开发者开发哪些内容有推荐？</h4><p></p><p></p><p>郭文山：针对这个问题，我们首先要考虑到两年的时间维度内的技术限制。XR行业长期愿景可能是下一代计算终端，但在短期内（两年）技术的限制可能会对开发者创意造成限制。例如，手势自定义的可能性在两年内可能受到技术的限制，比如你只能基于有限的手势去做组合，这是一种限制。</p><p></p><p>要回答这个问题，我们需要考虑两年的时间跨度，但不仅仅从 PICO 的角度来看，还要考虑整个行业的技术发展。共识是从显示方面来看，在未来两年内可能会发生重要转折。从 VR 到 VR 加 MR 的可能性，这可能是因为屏幕和前置摄像头发生了变化，我们觉得这个方向一定有可能产生非常有趣的新内容。交互方面则是杰思提到的更加多元的人机交互，多模态的融合交互等是会有很多机会。</p><p></p><p>具体内容的创新和发展会依赖于生态合作伙伴的创意和开发。例如，可以想象在新一代设备中，以前玩的游戏像 Pokémon Go，你可以使用新的裸手技术去抓小怪兽，可能会有更自然的交互方式，比如裸手扔球。我们认为，XR原生要抓住技术趋势，创新出吸引消费者的内容，这肯定有可能实现，但是具体是哪家公司实现，我们无法确定。</p><p></p><h4>问题 10： 未来是否会和开发者签订一些独占协议？如果会，独占内容和非独占内容的比例会如何控制？ 如果可能会独占，独占的内容是否会进行自研？是否会为多平台开发者提供适配 PICO 平台的小工具？</h4><p></p><p></p><p>郭文山：我们一直在思考独占的问题，特别是在加入字节后。独占是否关键，是否能成为用户购买的理由，是我们需要思考的重要问题。当前行业的主要矛盾是行业仍处于早期阶段，整个行业的出货量较小，应用生态中很有人没有盈利。因此，对于绝大多数开发者来说，独占并不是一个好的选择，特别是对于多人联机类的内容。独占会使他们失去与其他开发者合作的机会，让用户失去获得更优秀的游戏体验的可能性。对于独占，我们内部非常谨慎和克制。与此同时，我们更强调开放，我们希望每个品类都有一大批第三方开发者贡献创意。</p><p></p><p>对于自研，我们认为自研是一个很好的方向。举个例子，我们推出了全身动捕的新 feature（技术特性），我们可能会在前沿领域进行自研，前进一步，做探索类似“闪韵灵境”（全身动捕类音游）这样的玩法。但我们的目标并非自研，而是通过自研打造、演示实践案例，并在后续开放实践经验。我们希望通过开放合作的方式推动行业进步。</p><p></p><p>马杰思：对于多平台开发者，我们已经推动了一些行业标准，比如 OpenXR 标准。我们积极参与 OpenXR 的制定，以便开发者基于这个标准开发游戏，从而增强游戏的可移植性。 PICO 是 OpenXR 的贡献者之一，我们会与 OpenXR 成员一起努力，建设好 OpenXR 生态，使开发者能更容易地将内容移植到不同平台上。</p><p></p><h4>问题 11： 请问关于三体VR项目，能否透露一下上线日期以及开发进展情况？</h4><p></p><p></p><p>熊俊杰： 三体 VR 目前正在紧锣密鼓地开发进行中，但由于项目内部保密要求，暂时无法公布太多细节。但可以分享的一点是，整个开发的进展符合我们的预期，并且我们会加快开发进度，尽早与大家见面。</p><p></p><h4>问题 12： 近半年 PICO 是否倾向引入或自研人文历史、自然探索等类型的内容？</h4><p></p><p></p><p>熊俊杰：PICO 在视频和直播方向探索的范围是相当广泛的。除了人文历史和自然探索等内容外，我们也探索了诸如互动游戏和互动剧等内容，比如“探灵”和“我的 48 次心动”。我们的主要目标是探索创新技术如何服务内容创作，尤其在行业早期，探索行业的创新方向。</p><p></p><p>我们非常关注 PICO 硬件性能和软件能力，如8K清晰度、空间音频应用、六自由度互动能力、宽频马达和面部追踪技术等，以更好地服务内容创作。目前，我们已经与全球 30 多个机构合作，持续开发各种创新内容，包括视频和直播。在直播方向，我们合作了体育赛事、电竞、演唱会等多种领域，并将继续在MR方向探索创新，如如何更好地结合MR和视频，以及更自然的交互方式等。</p><p></p><h4>问题 13： 我是 PICO 的用户，我观察到 PICO 的内容在不断增加，也感觉到 PICO 的进步很大。我想了解一下，PICO 的设备已经卖出去多少，以及用户的活跃情况如何？</h4><p></p><p></p><p>郭文山：关于 PICO 活跃用户的数据我们不方便直接透露，但可以大概说明一下全球 VR 行业活跃情况，行业维度看 VR 用户活跃水平可以大致理解为与游戏主机相当，游戏主机的用户平均每月活跃大约在 5-8 次左右。在当前硬件和内容情况下，这是一个相当大的行业性挑战。提高活跃水平需要更多技术突破和内容创新，我们相信 PICO 可能略优于游戏主机，因为我们之前的调研显示很多 PICO 用户也是游戏主机的用户。</p><p></p><h4>问题 14： PICO 平台上有 500 多款应用，可以分享一下这些应用背后开发者的背景以及开发者群体的画像吗？</h4><p></p><p></p><p>郭文山：在这 500 多款应用中，大约有 1/5 由中国开发者开发的。中国开发者的画像可以大致分为几类。首先是热爱 VR 技术的游戏开发者，他们可能在 2015 年和 2016 年第一波 VR 时代就已经开始了。他们有游戏研发背景，一部分持续在做 To C（面向消费者）的产品，一部分可能转向偏向 To B（面向企业）的项目。第二类是创业者，特别是在 Meta、苹果等公司的号召下，有创业经验的人开始涌入 XR 行业。他们对新技术、新趋势敏感，有可能并非来自 XR 行业。第三类是来自行业的开发者，比如曾经做 2D 应用的公司，像爱奇艺、哔哩哔哩等大公司，也是 PICO 的重要合作伙伴。</p><p></p><h4>问题 15： 我注意到 PICO 平台上的视频应用大多数都是平台类应用，比如bilibili、爱奇艺，以及一些工具类应用。在 VR 平台上，我很少看到综合性、现代化、更软件化的视频应用，像抖音这样的综合性应用。PICO 有没有意向去引导或建议开发者往这个方向去探索或开发更新的视频软件形式？</h4><p></p><p></p><p>熊俊杰：在 VR 和 XR 这个全新媒介下，创作有着基本属性，比如空间性和互动性。这与传统视频以摄像头方式连接的方式有很大不同。互动性使视频和游戏的边界变得模糊，内容可能以 APK 的格式存在。PICO 视频聚合应用虽然以视频为主，但实际上也支持很多以 APK 格式存在的创新类应用。这种特点是 PICO 的优势之一。PICO 能够在 PICO 视频和应用商店两个平台上分发这些创新应用，为开发者提供更多流量。开发者可以选择不同平台上的变现模式。对于以 APK 格式存在的创新内容和内容创作者来说，同样也是一个优势。 PICO 视频也支持很多创作者以 APK 的方式存在，这些创作往往是相对创新的格式，介于游戏和视频之间，例如一些创新的 VR 体验应用。 PICO 通过支持这样的应用方向，持续探索和创新。</p><p></p><p>马杰思补充：作为消费者，我觉得开发者追求迅速回款和维持团队运营的需求与用户更希望看到更多创新之间存在冲突。目前，我觉得 VR 应用领域存在同质化问题，尤其是在 VR 游戏方面，比如大量的射击、汽车和球类游戏，这些可能是因为这些类型相对快速开发、成熟。在这种情况下，我们应该如何避免同质化？我们可以引导开发者利用现有的新技术，如交互，以影响开发方向，而不仅仅是推动开发更多同质化的游戏。</p><p></p><p>从技术的角度来看，尽管有新技术的支持，但人们可能仍然喜欢做枪、车和球类游戏，尽管这些游戏可能是新的枪、车和球。消费者可能不一定非要追求与枪、车和球不同的体验，但他们可能想要一种独特的体验，一种与传统不同的枪、车和球。新技术，特别是与 MR 相关的技术和人机交互相关的技术，肯定能够对XR内容生态产生影响。在过去十年间，VR 的体验基本上是通过六自由度手柄进行设计，但在过去一两年间，这种情况开始改变，不再局限于 VR 体验，还可以是 MR 体验，也不再局限于六自由度手柄，可以使用手部或其他体感追踪器。这种变化确实发生在最近一两年，这也是为什么我认为会有一些新趋势出现的原因。因此，我相信会出现一些不同类型的应用和游戏，可能不是强制性的，但会有很多新的应用和游戏。</p><p></p><p>李卓然补充：在制定赛制规划时，我们一直在思考是否应该强制开发者朝着更创新的方向发展。我们始终秉持着更加开放的态度，不强迫开发者朝特定方向发展。但是我们确实在评分规则和奖励设置上向创新性应用方向倾斜，特别是对与MR相关的赛道提供特殊支持。</p><p></p><p>首先，在评分方面，我们基于我们在八月下旬推出的 SDK 2.0，这个 SDK 关注 MR 相关技术。在评分中，我们明确表示，如果开发者更多地应用这些新技术，他们将在评分中获得更高的分数。其次，在奖项设置方面，我们特别设置了创新类的应用内容奖项，以及针对 MR 赛道和手势赛道的奖项。我们希望通过这些奖项的设立，吸引开发者更多地朝着这些创新方向进行内容开发。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0higrDEDX6Zs81dXCaSf</id>
            <title>首个千亿医药对话大模型来了，要打破医药研发“三十定律”</title>
            <link>https://www.infoq.cn/article/0higrDEDX6Zs81dXCaSf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0higrDEDX6Zs81dXCaSf</guid>
            <pubDate>Tue, 26 Sep 2023 02:15:56 GMT</pubDate>
            <updated>Tue, 26 Sep 2023 02:15:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 研发模式的发展历程中，医药研发经历了四个阶段：TMDD（手工炼药）、CADD（计算机辅助研发）、AIDD（人工智能研发）和ChatDD（人机交互研发）。水木分子发布了新一代对话式药物研发助手ChatDD和全球首个千亿参数多模态生物医药对话大模型ChatDD-FM 100B，旨在解决医药研发中的信息繁杂、探索空间有限和投入成本高等问题。ChatDD能够在立项、临床前研究和临床试验等环节提供服务，通过与专家的自然交互，重新定义药物研发模式。ChatDD的三个场景应用包括辅助立项调研、启发研发灵感和提高工具使用效率和体验。总结：水木分子发布了新一代对话式药物研发助手ChatDD，通过与专家的自然交互，重新定义药物研发模式，解决医药研发中的痛点和挑战。 </div>
                        <hr>
                    
                    <p>10年时间，投入超10亿美金，但成功率只有10%——这是医药研发领域的“三十定律”。从某种程度来说，它像是医药行业的一个“魔咒”。</p><p></p><p>一个药物在正式上市之前，需要经过立项调研、早期药物发现、早期临床前研究、临床试验药物重定位等各个流程。对于药企而言，整个过程存在三大痛点：第一，信息繁杂，对专家依赖度极高；第二，探索空间有限，湿实验（*注）迭代慢；第三，投入成本高，失败风险大。</p><p></p><p>为了打破这个“魔咒”，近日水木分子发布了新一代对话式药物研发助手ChatDD&nbsp;(Drug&nbsp;Design)&nbsp;和全球首个千亿参数多模态生物医药对话<a href="https://www.infoq.cn/article/Joe403tMlSW3gHQVOHTm">大模型</a>"ChatDD-FM&nbsp;100B。其中，ChatDD&nbsp;能够在立项、临床前研究、临床试验各环节提供服务，预计将于10月中下旬邀请基础版本内测；ChatDD-FM&nbsp;100B在C-Eval评测中达到全部医学4项专业第一，是唯一在该4项评测中平均分超过90分的模型。</p><p></p><h3>从人工炼药到人机交互，医药研发经历四个阶段</h3><p></p><p></p><p>回顾研发模式的发展历程，<a href="https://www.infoq.cn/article/Bydle1aR2REAdNOLVOrC">水木分子</a>"首次定义了医药研发的四个阶段：</p><p></p><p>第一阶段称为TMDD（Traditional&nbsp;Manual&nbsp;Drug&nbsp;Design），从远古时代一直持续到19世纪末，主要以手工合成、提取和筛选为主，基于大量试验、错误和经验主义，缺点是低通量、缺乏系统性、耗时长且成本高；</p><p></p><p>第二阶段称为CADD（Computer-Aided&nbsp;Drug&nbsp;Design），从20世纪中叶到21世纪初，计算机的出现开始辅助加速药物发现和设计，底层有物理化学规则做支持，在一定程度上实现了高通量，但是计算机只具有工具属性，因此只能解决单个问题，在研究过程中，仍然在很大程度上依赖研究人员经验；</p><p></p><p>第三阶段称为AIDD（AI&nbsp;Drug&nbsp;Design），从21世纪初至今，人工智能技术的成熟发展和应用，开始改变药物研发，研究人员可以从训练数据中挖掘药物发现和设计，优点是能够满足超高通量，实现流程化，但是缺乏模型与专家交互，依赖大规模高质量标注数据，面临信息与知识分离，工具服务分散，处理模态单一等挑战；</p><p></p><p>第四阶段称为ChatDD（Chat&nbsp;Drug&nbsp;Design），即基于当下的大模型能力，对多模态数据进行融合理解，与专家自然交互人机协作，重新定义药物研发模式。</p><p></p><p>在整个过程中，每个阶段的技术革新都带来了不同程度的效率提升和科学发展，为药物研发带来了新的机遇和挑战。“但是，目前市场上至少还有超过&nbsp;1/&nbsp;3&nbsp;的药还是通过TMDD研发设计的。”清华智能产业研究院（AIR）首席研究员、水木分子首席科学家聂再清教授表示，这正是如今医药研发周期长、投入大、风险高很重要的原因。</p><p></p><p>虽然计算机、人工智能技术能够起到一定的研发辅助作用，但是仍然停留在工具阶段，研发过程很难摆脱对专家的知识依赖。“并且专家的知识储备也是有限的，无法掌握所有的文章、专利、数据。我们认为，把专家知识与大模型知识联结，二者进行协作，这才是药物研发现在和未来应该有的模式。”聂再清教授强调。</p><p></p><p>除此之外，利用此前的技术辅助医药研发还面临两个挑战：一方面，海量的实时<a href="https://www.infoq.cn/news/oZkw8G2uXvv2oVNJ7XFm">数据</a>"散落在各处；另一方面，生物医药涉及大量细分领域，每个领域需要配备对应工具，这些工具的使用门槛和成本也相对较高。“这也是我们发布ChatDD的原因，希望通过一个自然语言的交互界面，能够把专家从繁琐的数据处理和工具使用工作当中解放出来，把更多的精力放在更加开创性的、更有价值的工作上。”</p><p></p><h3>搞定立项、临床前研究、临床试验难题，ChatDD的三个场景应用</h3><p></p><p></p><p>水木分子在此次发布会上展示了&nbsp;ChatDD的在医药研发环节的三个具体应用场景，包括ChatDD-BI&nbsp;立项场景、ChatDD-Discovery研发探索场景和ChatDD-Trail&nbsp;临床试验场景应用。&nbsp;&nbsp;</p><p></p><p>ChatDD-BI：辅助立项调研</p><p></p><p>和仿制药生产制造不同，在药物研发过程中，立项环节极为重要，甚至一个错误的决定都可能导致巨大的经济损失。</p><p></p><p>在传统模式下，这一工作存在三个主要痛点：第一，信息繁杂，涉及对大量数据的搜集、整理和分析，比如病人需求、疾病机制、医药行业现状、专利和文献检索等等，并且这些数据散落各处，收集难度大、耗费时间长，一份高质量的立项报告的输出需要专家投入大量精力。第二，这一工作是非标准化的，对专家依赖极高，还存在信息不全面的问题，可能对项目的决策和发展产生负面影响。第三，立项涉及企业商业秘密难以外包，这也增加了信息搜集的难度。</p><p></p><p>针对这一问题，ChatDD-BI的角色是作为立项助手，通过与专家的数轮对话辅助立项调研，从而提升立项的效率和质量。</p><p></p><p>ChatDD-Discovery&nbsp;：启发研发灵感，提高工具使用效率和体验</p><p></p><p>针对临床前研究场景，传统的做法是做湿实验，然后根据结果不断迭代。但是随着时间的推移，找到针对已知靶点的药物越来越难。因此，科学家们需要更大的探索空间来寻找新的靶点和治疗方案。&nbsp;对此，水木分子认为，大模型的<a href="https://www.infoq.cn/article/TSJbOkWweAvFh44Oo2dL">幻觉</a>"在科学研究探索性中可能会带来“意外”的灵感。&nbsp;</p><p></p><p>以针对渐冻症治疗方案的探索场景为例，研发人员可以通过对话获得“渐冻症”疾病画像，包含疾病的描述、已有治疗方案等等，如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d06fc8de62c99ce371fc91dbf7a746e8.png" /></p><p></p><p>同时，ChatDD还会通过查询外部数据库，如ClinicalTrails，给出渐冻症在研药物进展。&nbsp;以“利司扑兰”为例，研发人员可以利用ChatDD进一步探究其作用靶点，ChatDD找到利司扑兰的作用靶点“SMN2”，通过建立疾病、靶点、药物之间的潜在链条关系，发现并建议“SMN2是渐冻症的潜在靶点，可进行下一步探究探索”。根据已发表的多项研究成果，证明了ChatDD推断的合理性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8a8b1ce54aa8d167fb0ffdc2f68e7a0.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9e7e0b87173bb4289b50245431e387b.png" /></p><p></p><p>除了“灵感激发”之外，ChatDD-Discovery同样可以提高研究人员的研发效率。在医药研发过程中，研究人员需要查询大量文献和检索专利等，这需要高频使用各种工具，学习成本高且难以与专家知识互动融合。而ChatDD-Discovery采用自然语言交互方式，将专家知识模型参数化，只需简单回答问题，就能使用虚拟筛选功能，使研究人员更高效地完成任务。</p><p></p><p>值得一提的是，ChatDD-Discovery&nbsp;研发助手可以处理生物医药多模态数据，如分子、蛋白质、基因序列以及多样的生物医药下游任务，如DDI（药物-药物相互作用计算）、亲和力计算、药物敏感性预测、单细胞类型注释等。&nbsp;例如，输入分子式ChatDD-Discovery就可以生成该分子的完整描述信息，并且支持用户针对该分子进行进一步提问。</p><p></p><p>ChatDD-Trial：提高临床试验成功率</p><p></p><p>在药物研发的全流程中，临床试验是耗资最多、风险最大的那一环，因此，想要打破“三十定律”，提高临床试验成功率是重中之重。换个角度来看，由于成本巨大，所以，在这个环节只要有一点点的提升，就能带来可观的经济效益。</p><p></p><p>对此，水木分子ChatDD目前主要关注四个小场景：药物生物标记物发现、临床试验入组设计、临床试验报告撰写、药物重定位和适应症扩展。</p><p></p><p>比如，ChatDD-Trial可以辅助临床试验研究人员找到最适合入组的患者人群；比如，通过发现药物敏感的生物标志物，更好地理解疾病亚型，实现精准的患者分类，确保患者与试验药物更匹配，减少不必要的变量干扰，提高临床试验成功率；再比如，作为临床试验设计助手，帮助研究团队优化试验设计，通过分析大量的相关数据，提供有关患者选择、临床试验阶段和标准化流程的建议，确保试验的科学性和可行性，减少不必要的误差和风险。</p><p></p><p>据聂再清教授介绍，目前ChatDD提供了三类服务方式：一是订阅服务，支持企业账户；二是API服务，支持高速推理服务，提供开发者培训；三是私有化部署，支持本地数据微调，定制场景方案。</p><p></p><h3>三级淬炼出千亿级生物医药行业大模型</h3><p></p><p></p><p>ChatDD的底层，基于的是水木分子千亿参数多模态<a href="https://xie.infoq.cn/article/7edc5aaa592a490f71b758299">生物医药</a>"对话大模型ChatDD-FM。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/dbde600079d32101cfb662330be99d95.png" /></p><p></p><p>上图是ChatDD-FM多模态生物医药大模型的架构图：首先，对生物医药垂直领域的知识图谱、分子、蛋白质、单细胞测序、文本等多模态进行编码；然后，将各种不同模态对齐到统一的特征空间，进行模态融合、专业领域指令微调；最后，在产品端支撑ChatDD-BI、ChatDD-Discovery和ChatDD-Trail&nbsp;三大应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecc9f2becb64d9eebae23b16ef3190cf.png" /></p><p></p><p>水木分子首席技术官、联合创始人乔木博士强调，ChatDD-FM是一个灵活、可扩展的架构，在当下版本基础上，还可以持续增加其它不同模态。而对于用户而言，可以通过自然语言对话方式与大模型进行交互。其背后，是ChatDD-FM对数千篇中英文生物医药文献、千万级通用场景对话、数十万个工具调用指令集等高质量数据的预训练，以及指令微调和智能工具调用。</p><p></p><p>水木分子由AIR孵化，在今年6月份成立，主要专注于生物医药垂直行业大模型的研发与应用。而早在今年4月，聂再清教授团队就已经开源了生物医药版GPT&nbsp;BioMedGPT1.6B，从研究层面验证了将文献、分子、蛋白、测序、知识图谱等数据压缩到统一的多模态大模型框架内，可使模型具备“融会贯通”的能力，在分子性质预测、药物-靶点亲和力预测、性质预测、药物敏感性预测、分子-文本跨模态检索、分子-文本跨模态信息生成等多项任务上优于单一专用模型，从研究层面验证了技术可行性。</p><p></p><p>8月，水木分子又开源了全球首个可商用的百亿级参数规模多模态生物医药大模型BioMedGPT-10B。其中，首次提出了分子QA和蛋白质QA任务，给定特定分子/蛋白质，可以用自然语言回答相关功能、属性等问题，有效评估模型在自然语言和生物编码语言之间的翻译能力。</p><p></p><p>据聂再清教授介绍，虽然BioMedGPT已经实现商用，但由于是科研项目，所以使用的仍然是英文界面，中文对话能力较弱，因此在国内市场实现商用普及有限。这便是水木分子成立背后的原因之一。</p><p></p><p>“接下来，BioMedGPT&nbsp;仍然会继续往前推进，作为科研品牌继续开源。而此次ChatDD和ChatDD-FM的孵化，则是大模型都科研迈向产业化，是产学研结合的关键一步。”聂再清教授强调，“我们认为大模型未来必将成为&nbsp;AI&nbsp;时代的操作系统，就像我们PC时代的Windows，移动互联时代的Android、iOS。而现在我们正在做的生物医药大模型，未来在整个AI&nbsp;for&nbsp;science领域都有可能用上。对此，我们充满了期待。”</p><p></p><p>而现下，ChatDD和ChatDD-FM的发布，已然重新定义了医药研发模式，不仅可以提高研究人员的工作效率，还可以为研发新药提供更多的可能性。未来，随着技术的持续迭代革新，医药研发的效率和成功率将会得到显著提高，打破“三十定律”的“魔咒”指日可待。</p><p></p><p></p><blockquote>*注：“湿”实验指的是将待测样本利用实验室方法进行核酸提取、文库构建（包括片段化、富集、扩增等一系列过程）到完成上机测序的实验过程；“干”实验则是从得到下机数据开始，到完成生信分析和报告解读的整个过程。</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EjqbmPdv0fSvUkWqgZ2k</id>
            <title>PICO 首届 XR 开发者挑战赛正式启动，助推行业迈入“VR+MR”新阶段</title>
            <link>https://www.infoq.cn/article/EjqbmPdv0fSvUkWqgZ2k</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EjqbmPdv0fSvUkWqgZ2k</guid>
            <pubDate>Tue, 26 Sep 2023 00:44:05 GMT</pubDate>
            <updated>Tue, 26 Sep 2023 00:44:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 挑战赛, PICO, XR, 开发者, 报名通道, 官方, 英特尔, 技术合作伙伴

总结: PICO举办首届XR开发者挑战赛，旨在与全球开发者共同探索XR行业新成长。PICO将为参赛者提供全方位支持，英特尔作为技术合作伙伴也将提供支持。PICO重视开发者伙伴，此次挑战赛是与开发者链接的新尝试。PICO在近两年保持快速增长，PICO应用商店中有大量国内开发者贡献。挑战赛将在多个国家和地区举办，为全球开发者提供展示才华的舞台。中国区开发者有机会获得奖金、技术指导和投资支持。开发者们可以前往PICO官网报名参赛。 </div>
                        <hr>
                    
                    <p>9 月 25 日，“PICO 2023 首届 XR 开发者挑战赛”（下文简称“挑战赛”）媒体启动会在北京圆满落幕，官方赛事报名通道已于今日开启。据悉，本次挑战赛是 PICO 首次针对全球开发者举办的大型挑战赛事，旨在与开发者保持连接，共同探索 XR 行业新成长。作为主办方，PICO 将为参赛者提供平台、专业技术、资金等全方位支持，而全球领先的芯片厂商英特尔也将作为本届赛事技术合作伙伴为参赛者保驾护航。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bb/40/bbb0993bf6845538a460d7737734a740.png" /></p><p>图1：PICO 2023首届XR开发者挑战赛报名通道正式开启</p><p>&nbsp;</p><p>启动会现场，除了官宣赛事启动，PICO 相关负责人还围绕行业、技术、资金/人才投入等维度详细阐述了 PICO 一直以来对于开发者的重视及此次举办开发者挑战赛的初衷，呼吁更多开发者投入XR开发新领域，与 PICO 及行业共成长。</p><p></p><p>“新阶段的 XR 行业，值得开发者关注与参与。”PICO OS 产品负责人马杰思谈到，“XR 行业已经迈入新的发展阶段，开始从强调虚拟的‘VR’向虚实融合的‘VR+MR’阶段过渡，这种变化不仅能给用户提供更多样化的产品价值，一定程度也赋予了开发者更大的创作空间。各行各业的开发者，都有机会融入到XR开发的领域之中，一起创造新机会和新市场。同时，光学显示、芯片算力、环境感知、人机交互等技术的突破，也加速了 XR 行业发展。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/ae/29/aef7883c92bd3a90944a3aa27c5ff629.jpg" /></p><p>图2：PICO OS产品负责人马杰思现场分享</p><p></p><p>马杰思同时表示，想要实现整个 XR 赛道高水平发展的局面，决不是凭借 PICO 或者某一家品牌一家之力就可以做成的事情。一直以来，PICO 都相当重视广大的开发者伙伴，先后启动了“PICO 开发者计划”“PICO 创作者激励计划”等，从硬件、技术、营销等维度，全链路助力开发者成长。此次主办首届开发者挑战赛事，是在 XR 行业新阶段下 PICO 与开发者链接的一次新尝试。新趋势下，PICO 将持续围绕空间感知、图形渲染、人机交互等 XR 核心底层技术能力持续完善，并协同字节底层技术能力，为开发者提供前沿、便捷的开发服务。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/a7/b8/a7a445324980c41320ab0e00dbc13fb8.jpg" /></p><p>图3：PICO中国区游戏商务负责人郭文山现场分享</p><p>&nbsp;</p><p>作为全球知名的 XR 品牌，PICO 在近两年保持快速增长，2022 年全年销量同比之前翻了 8 倍。此外，IDC 数据显示，2022 年市场份额 PICO 全球排名第二、国内位居第一，2023 年上半年国内 VR 市场的厂商份额中，PICO 以 58.7% 的占比大幅领先同行。这些成果背后，离不开庞大的用户群体以及众多开发者共建内容生态的支持。</p><p></p><p>PICO 中国区游戏商务负责人郭文山表示：“截止目前，PICO 应用商店的应用数量已超 530 款，‘大作’占比近四成。其中，国内开发者贡献不容忽视，PICO 应用商店中包括游戏、视频、运动、办公、教育、音乐等在内的 100 多款应用均来自国内开发者。此外，PICO 也给开发者带来更多用户和收入，&nbsp;有 180 款游戏收入超过 10 万元，近 50 款应用下载量超过 10 万，而部分平台头部开发者，在 PICO 的年收入已超过 800 万元。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8e/8f/8e2daabe3454191f157eb6ffd0f6c68f.png" /></p><p>图4：PICO将为开发者们提供专业指导、投资机会、流量曝光等全方位支持</p><p>&nbsp;</p><p>据了解，本次“挑战赛”将在欧洲、北美、东南亚、日韩、中国等多个国家和地区同期举办，为全球开发者提供展示才华、实现创意的舞台。所有未发行/已发行且没在往届同类型比赛中 PICO 赛道获奖的作品均可报名。赛事报名通道于 9 月 25 日正式开通，开发者可通过 PICO 官网开放平台赛事专区报名参赛，并于 10 月 2 日至 11 月 10 日期间完成作品提报。<a href="https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t">PICO</a>" 将于 11 月 30 日前组织评委完成评审，并于 12 月 5 日举办颁奖仪式，公布获奖名单。</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT">针对</a>"本届“挑战赛”重点赛区之一的中国区开发者，本次挑战赛设置了前五名，另外还单独设置了游戏类、创新类、MR 赛道、手势识别四个不同赛道奖项。获奖者除收获最高达 6 万元的丰厚奖金外，还有机会得到技术大咖指导、投资圆桌交流、<a href="https://xie.infoq.cn/article/40e1258dc998d9c7ce6380c8a">PICO</a>" 创投基金和面试直通等额外支持。 赛事技术合作伙伴英特尔也将为获奖参赛作品上线运行提供更多维的算力支撑，搭载英特尔 ARC dGPU 显卡的运行环境后续将开放给大赛开发者实现项目落地。</p><p>&nbsp;</p><p>通过本次挑战赛，开发者们不仅可以充分展示自己的才华和创新能力，同时也能够获得更多的机会和支持。有兴趣的开发者们，不妨立即行动起来，前往 PICO 官网开放平台赛事专区（<a href="https://www.picoxr.com/cn/2023picodevjam/#/introduce?trackingFlag=EkWdMubc">链接</a>"），报名参加这场挑战赛。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uNJmOYLgQK062KPYFWDw</id>
            <title>电商巨头 Shopify 提升 LLM 辅助工具，优化电子商务生态</title>
            <link>https://www.infoq.cn/article/uNJmOYLgQK062KPYFWDw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uNJmOYLgQK062KPYFWDw</guid>
            <pubDate>Tue, 26 Sep 2023 00:00:00 GMT</pubDate>
            <updated>Tue, 26 Sep 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 在 Shopify 的工程师 Ates Goral 的文章中，他提到了使用大型语言模型聊天机器人时遇到的问题以及解决方案。为了提供更自然的用户体验，他们使用了缓冲解析器来防止 Markdown 渲染卡顿和延迟。此外，他们还使用了占位符的概念来防止用户等待所有外部服务响应完成。通过这些解决方案，他们成功地改善了大型语言模型的性能。 </div>
                        <hr>
                    
                    <p>在使用大型语言模型聊天机器人打开了创新解决方案之门的同时，Shopify 的工程师 Ates Goral 认为，要使用户体验尽可能自然，需要一些特定的努力，以<a href="https://shopify.engineering/sidekicks-improved-streaming">防止产生卡顿并减少延迟</a>"。</p><p>&nbsp;</p><p>由大型语言模型返回的 Markdown 响应流会导致渲染卡顿，因为特殊的 Markdown 字符（如 *）在完整表达式被接收之前保持模糊不清，例如，直到接收到闭合的 *。同样的问题也适用于链接和所有其他 Markdown 运算符。这意味着 Markdown 表达式在完全完成之前无法正确渲染，这意味着在短时间内 Markdown 渲染不正确。</p><p>&nbsp;</p><p>为了解决这个问题，Shopify 使用了一个缓冲解析器，它在 Markdown 特殊字符之后不会发出任何字符，并等待 Markdown 表达式完全完成或收到意外字符。</p><p>&nbsp;</p><p></p><blockquote>在进行流式传输时，需要使用一个有状态的流处理器，可以逐个字符地消耗字符。流处理器要么按照字符接收的顺序传递字符，要么在遇到类似 Markdown 的字符序列时更新缓冲区。</blockquote><p></p><p>&nbsp;</p><p>然而，Goral 表示，虽然从原理上讲，这个解决方案相对容易手动实现，但要支持完整的 Markdown 规范，则需要使用现成的解析器。另一方面，延迟主要是由于需要进行多次大型语言模型往返来消耗外部数据源，以扩展大型语言模型的初始响应所导致的。</p><p>&nbsp;</p><p></p><blockquote>大型语言模型对一般的人类语言和文化有很好的理解，但它们并不是获取最新准确信息的绝佳来源。因此，我们告诉大型语言模型通过使用工具来告诉我们当它们需要超出其理解范围的信息时。</blockquote><p></p><p>&nbsp;</p><p>换句话说，基于用户输入，大型语言模型提供的初始响应还包括要咨询以获取缺失信息的其他服务。当这些额外的数据片段被接收后，大型语言模型将构建完整的响应，最终显示给用户。</p><p>&nbsp;</p><p>为了防止用户需要等待所有外部服务都响应完成，Sidekick 使用了 "卡片" 的概念，即占位符。Sidekick 渲染了从大型语言模型收到的初始响应，包括任何占位符。一旦额外的请求完成，Sidekick 将占位符替换为接收到的信息。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43afd7d3246a2f60947117a72536422b.gif" /></p><p></p><p>&nbsp;</p><p>在 Sidekick 中实施的解决方案充分利用了此工作流程中固有的异步性，并将响应分流步骤与 Markdown 缓冲解析器集成在一起。如果您对他们的解决方案的完整细节感兴趣，不要错过 Goral 的原文文章。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/Shopify-sidekick-llm-improvement/">https://www.infoq.com/news/2023/08/Shopify-sidekick-llm-improvement/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9094a1097720d61697b62bd0e</id>
            <title>人工智能核心基础 - 规划和概要</title>
            <link>https://www.infoq.cn/article/9094a1097720d61697b62bd0e</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9094a1097720d61697b62bd0e</guid>
            <pubDate>Mon, 25 Sep 2023 15:43:23 GMT</pubDate>
            <updated>Mon, 25 Sep 2023 15:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 机器学习, 深度学习, RNN, CNN, NLP, CV, BI

总结: 本次课程规划包括基础能力、BI精讲、NLP精讲和CV精讲四个部分。每个部分都会单独开设专栏目录，并提供部分免费内容。代码和数据集可以在《茶桁的AI秘籍》的代码仓库中找到。 </div>
                        <hr>
                    
                    <p></p><p></p><p><img src="https://static001.infoq.cn/static/write/img/img-copy-disabled.4f2g7h.png" /></p><p></p><p>Hi，你好。又见面咯，我是茶桁。</p><p></p><p>在之前，我花了两个来月的时间撰写了「Python篇」和「数学篇」，希望小伙伴们在正式进入AI之前能够打好一个基础。那么今天开始，我们将正式开始AI基础的学习。</p><p></p><p>这一节课咱们先不着急直接开始课程，而是聊一下本次课程的一个规划。</p><p></p><p>在整个课程规划中，我们将会直接从机器学习开始入手，进入深度学习，然后开始接触RNN、CNN以及三大方向：NLP、CV和BI。核心能力将会分成四大部分进行展开精讲。</p><p></p><h2>目录规划</h2><p></p><p></p><h3>基础能力</h3><p></p><p>人工智能导论机器学习初探机器学习进阶（这部分会比较长）深度学习进阶RNNCNN自然语言处理基础（NLP）计算机视觉基础（CV）商业智能（BI）</p><p></p><h3>BI精讲</h3><p></p><p>预测全家桶与机器学习四大神器Fintech数据分析数据可视化与DashBoardALS算法与推荐系统SVD矩阵分解与基于内容的推荐PageRank、图论与推荐系统Graph Embedding强化学习</p><p></p><h3>NLP精讲</h3><p></p><p>自然语言处理的基本过程</p><p></p><p>向量空间模型自然语言处理初步语言模型和概率图模型词向量模型Word2VecTransformer与BER,大规模预训练问题自然语言生成自然语言处理与人工智能前沿</p><p></p><h3>CV精讲</h3><p></p><p>初阶计算机视觉：图像处理</p><p></p><p>中阶计算机视觉：图像描述中阶到高阶的关键：CNN方法计算机视觉中的图像分类深度学习之单阶段目标检测深度学习之两阶段目标检测计算机视觉中的图像分割计算机视觉中的目标跟踪</p><p></p><h2>内容输出方式</h2><p></p><p>以上目录中的四个部分都属于核心部分，每一个部分都会单独开一个专栏目录。一个是因为收费课程，拆散之后大家可以按照自己的需要进行购买，再一个也是将四部分区分的清晰一点。</p><p></p><p>虽然每一张专辑都是收费的，但是也并不是所有内容都需要进行购买才可查看。有的时候为了吸引流量，即便没有购买专辑，部分章节会开放阅读全部。</p><p></p><p>以上目录仅供参考，目录是按照内容概要进行规划的，并不等于实际章节。就像我在写数学篇的时候，本来就只规划了4个知识点，但是其中一个知识点可能会讲7、8个章节，也可能3、4篇就讲完了。所以届时的内容，会比从目录上看要多的多，起码就基础部分的机器学习这一知识点，可能就要十几、二十节课才能讲完。</p><p></p><h2>代码库</h2><p></p><p>在咱们的整个讲解过程，演示代码是不可避免的，并且其中还会包含很多数据。这部分内容基本上都会在咱们的《茶桁的AI秘籍》的代码仓库中找到，地址为：https://github.com/hivandu/AI_Cheats</p><p></p><p>其中部分数据集可能因为太大会上传到百度网盘并分享出来，分享一般都会放在文末，大家可以自取。</p><p></p><h2>其他</h2><p></p><p>如果您阅读时感觉文章不完整，那应该是该网站我暂时无法发布收费专栏，所以我仅提供了部分内容。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MTaYW6a4CNrahgkYXAyT</id>
            <title>阿里云通义千问14B模型开源：性能超越Llama2等同尺寸模型</title>
            <link>https://www.infoq.cn/article/MTaYW6a4CNrahgkYXAyT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MTaYW6a4CNrahgkYXAyT</guid>
            <pubDate>Mon, 25 Sep 2023 07:40:11 GMT</pubDate>
            <updated>Mon, 25 Sep 2023 07:40:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 总结: 阿里云开源通义千问140亿参数模型Qwen-14B及其对话模型Qwen-14B-Chat，支持免费商用。Qwen-14B的性能表现超越同等规模模型，部分指标甚至接近Llama2-70B。Qwen-14B是一款支持多种语言的高性能开源模型，整体训练数据超过3万亿Token，使模型具备更强大的推理、认知、规划和记忆能力。Qwen-14B-Chat是在基座模型上经过精细SFT得到的对话模型，生成内容的准确度大幅提升，也更符合人类偏好。Qwen拥有出色的工具调用能力，能让开发者更快地构建基于Qwen的Agent（智能体）。Qwen-14B进一步提高了小尺寸模型的性能上限，在多个权威测评中取得最优成绩，超越所有测评中的SOTA大模型。用户可从魔搭社区直接下载模型，也可通过阿里云灵积平台访问和调用Qwen-14B和Qwen-14B-Chat。 </div>
                        <hr>
                    
                    <p></p><blockquote>两个月三连发，通义千问又开源了。</blockquote><p></p><p></p><p>9月25日，阿里云开源通义千问140亿参数模型Qwen-14B及其对话模型Qwen-14B-Chat，支持免费商用。在多项权威评测中，Qwen-14B的性能表现超越同等规模模型，部分指标甚至接近Llama2-70B。此前阿里云于8月开源了通义千问70亿参数基座模型Qwen-7B，先后冲上HuggingFace、GitHub的Trending榜单。短短一个多月，累计下载量就突破100万，开源社区出现了约50个基于Qwen的模型。</p><p>多个知名工具和框架都集成了Qwen，如支持用大模型搭建WebUI、API以及微调的工具FastChat、量化模型框架AutoGPTQ、大模型部署和推理框架LMDeploy、大模型微调框架XTuner等等。</p><p></p><p>据了解，此次开源的Qwen-14B是一款支持多种语言的高性能开源模型，相比同类模型使用了更多的高质量数据，整体训练数据超过3万亿Token，使模型具备更强大的推理、认知、规划和记忆能力。Qwen-14B最大支持8k的上下文窗口长度。</p><p><img src="https://static001.geekbang.org/infoq/16/166ee1f62a1c35fb0c11ac5079f70bbc.jpeg" /></p><p></p><p>图1：Qwen-14B在十二个权威测评中全方位超越同规模SOTA大模型</p><p></p><p>而Qwen-14B-Chat则是在基座模型上经过精细SFT得到的对话模型。借助基座模型强大的性能，Qwen-14B-Chat生成内容的准确度大幅提升，也更符合人类偏好，内容创作上的想象力和丰富度也有显著扩展。</p><p></p><p>Qwen拥有出色的工具调用能力，能让开发者更快地构建基于Qwen的Agent（智能体）。开发者可用简单指令教会Qwen使用复杂工具，比如使用Code&nbsp;Interpreter工具执行Python代码以进行复杂的数学计算数据分析、图表绘制等；还能开发具有多文档问答、长文写作等能力的“高级数字助理”。</p><p></p><p>百亿以内参数级别大语言模型是目前开发者进行应用开发和迭代的主流选择，&nbsp;Qwen-14B进一步提高了小尺寸模型的性能上限，从众多同尺寸模型中冲出重围，在MMLU、C-Eval、GSM8K、MATH、GaoKao-Bench等12个权威测评中取得最优成绩，超越所有测评中的SOTA（State-Of-The-Art）大模型，也全面超越Llama 2-13B，比起Llama&nbsp;2的34B、70B模型也并不逊色。与此同时，Qwen-7B也全新升级，核心指标最高提升22.5%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4fbdbdc1d80f97d6c961cd076ca7a4ce.png" /></p><p>图2：Qwen-14B性能超越同尺寸模型</p><p></p><p>据InfoQ向通义千问研发团队了解，Qwen-14B之所以能较上一代Qwen-7B获得显著的性能提升，主要有三方面原因。</p><p></p><p>在数据集构建方面：</p><p></p><p>研发团队使用了多达3万亿token的大规模预训练数据集，覆盖了各个领域和千行百业的知识，包含多个语种的语言，还有代码数据。在此基础之上，研发团队做了较为精细的数据处理，包括大规模数据去重、垃圾文本过滤、以及提升高质量数据比例等。</p><p></p><p>在模型结构设计和训练方法方面：</p><p></p><p>模型结构方面，研发团队此前做了一系列前期实验验证模型结构设计对效果的影响，整体而言，Google的PaLM、Meta的LLaMA的大多数技术选择都是效果较好的，包括SwiGLU的激活函数设计、ROPE的位置编码等，在Qwen的结构设计中也都采用了。</p><p></p><p>通义千问团队专门针对词表做了优化，词表大小超过15万，具有较好的编码效率，也就是说，相比其他tokenizer能用更少的token表示更大量的信息，意味着更加节省的token数，或者说更低的成本。</p><p></p><p>此外，通义千问团队重点针对长序列数据建模做了优化，采用当前最有效的策路，包括但不限于Dynamnic NTK、Log-N attention scaling、window attention等，并做了一些细节的调整保证长序列数据上，模型表现效果更稳定。当前模型能够适配井取得稳定表现的序列长度也达到了8192。</p><p></p><p>研发团队成员向InfoQ表示：“大模型训练其实没有太多复杂的技巧，更多是经过大量尝试与迭代，找到较好的训练参数，达到训练稳定性、训练效果和训练效率的最优平衡，包括但不限于优化器的配置、模型并行的配置等。”</p><p></p><p>在外接工具能力方面：</p><p></p><p>此前开源的Qwen-7B已经展现出了出色的工具使用能力，只需通过文本描述即可理解并使用未经训练的新工具，可用于各种Agent应用。这次开源的新模型进一步增强了Agent能力，在使用复杂工具时的可靠性有了显著提升。</p><p></p><p>例如，Qwen-14B可以熟练地使用Code Interpreter工具执行Python代码，进行复杂的数学计算、数据分析和数据图表绘制。同时，Qwen-14B的规划和记忆能力也得到了提升，在执行多文档问答和长文写作等任务时表现更加可靠。</p><p></p><p>为了实现这一效果，研发团队主要做了两方面的优化。首先，在微调样本方面进行优化，通过建立更全面的自动评估基准，主动发现了之前Qwen表现不稳定的情況，并针对性地使用Self-Instruct方法扩充了高质量的微调样本。其次，底座预训练模型的能力得到了提升，带来了更强的理解和代码能力。</p><p></p><p>即日起，用户可从魔搭社区直接下载模型，也可通过阿里云灵积平台访问和调用Qwen-14B和Qwen-14B-Chat。阿里云为用户提供包括模型训练、推理、部署、精调等在内的全方位服务。</p><p></p><p>据了解，当前国内已有多个月活过亿的应用接入通义千问，大量中小企业、科研机构和个人开发者都在基于通义千问开发专属大模型或应用产品，如阿里系的淘宝、钉钉、未来精灵，以及外部的科研机构、创业企业。</p><p></p><p>钉钉是最早接入通义千问的应用之一。4月18日，钉钉发布了一条“斜杠”， 接入千问大模型后，通过输入“/”即可在钉钉唤起AI能力，涵盖群聊、文档、视频会议及应用开发等场景。</p><p>&nbsp;</p><p>在群聊中，新入群者无需爬楼，在对话框输入钉钉斜杠“/”即可自动整理群聊要点，快速了解上下文，并生成待办、预约日程。还可以用“/”在群聊中创作文案、表情包等；在钉钉文档中，“/”可以是用户的创意助理，帮助写文案、生成海报。在视频会议中，“/”也是会议助理，能一键生成讨论要点、会议结论、待办事项等；“/”还可用自然语言或拍照生成应用，并以钉钉酷应用的形式在群聊内使用。比如，公司行政人员需要统计午餐的订餐份数，只需要在群聊对话框中输入“/”和需求，几秒钟后一个订餐统计小程序就会展现在群聊中。</p><p>&nbsp;</p><p>在过去100多天里，钉钉正逐渐实现全面智能化，已经有17条产品线、55个场景完成了智能化再造，包含钉钉 IM 群聊、酷应用、低代码、钉钉会议、Teambition、闪记、邮箱、钉钉文档、表格、脑图、白板、知识库等，覆盖多模态内容生成、摘要提取、应用开发等。</p><p></p><p>浙江大学联合高等教育出版社基于Qwen-7B开发了智海-三乐教育垂直大模型，已在全国12所高校应用，可提供智能问答、试题生成、学习导航、教学评估等能力，模型已在阿里云灵积平台对外提供服务，一行代码即可调用；浙江有鹿机器人科技有限公司在路面清洁机器人中集成了Qwen-7B，使机器人能以自然语言与用户进行实时交互，理解用户提出的需求，将用户的高层指令进行分析和拆解，做高层的逻辑分析和任务规划，完成清洁任务。</p><p></p><p>阿里云CTO周靖人表示，阿里云将持续拥抱开源开放，推动中国大模型生态建设。阿里云笃信开源开放的力量，率先开源自研大模型，希望让大模型技术更快触达中小企业和个人开发者。</p><p></p><p>附相关链接：</p><p></p><p>魔搭社区模型地址：</p><p>https://www.modelscope.cn/models/qwen/Qwen-14B-Chat/summaryhttps://www.modelscope.cn/models/qwen/Qwen-14B/summary</p><p></p><p>Qwen论文地址：</p><p>https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf</p><p></p><p>GitHub：</p><p>https://github.com/QwenLM/Qwen</p><p></p><p>HuggingFace:</p><p>https://huggingface.co/Qwen/Qwen-14Bhttps://huggingface.co/Qwen/Qwen-14B-Chat</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Joe403tMlSW3gHQVOHTm</id>
            <title>AI大模型时代下的算力需求与挑战</title>
            <link>https://www.infoq.cn/article/Joe403tMlSW3gHQVOHTm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Joe403tMlSW3gHQVOHTm</guid>
            <pubDate>Mon, 25 Sep 2023 07:29:00 GMT</pubDate>
            <updated>Mon, 25 Sep 2023 07:29:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 算力需求特点：随着AI大模型的兴起，算力需求实现了大爆发。大模型对算力有巨大需求，特别是在模型训练方面，需求可能增加数十到数百倍。

降低算力使用门槛的方法：目前的算力分类主要有通用算力和智能算力（AI算力），智能算力更多地使用加速计算，如GPU或AI芯片。降低算力使用门槛的方法包括提供更高效的算力供应方式，如云计算和边缘计算，以及优化算力支持方面的技术，如专门针对AI的数据格式。

IT基础设施应对大模型算力需求：IT基础设施提供商需要提供广泛的基础设施支持，包括计算、存储和网络等方面，以满足大模型的算力需求。此外，IT基础设施提供商还需要不断拓展大模型和其他前沿AI技术领域的研究和工作，以适应算力需求的增长。

总结: AI大模型的兴起带来了算力需求的大爆发，特别是在模型训练方面。为了降低算力的使用门槛，可以提供更高效的算力供应方式和优化算力支持技术。IT基础设施需要提供广泛的支持，包括计算、存储和网络等方面。 </div>
                        <hr>
                    
                    <p>算力是信息时代的新生产力，也是 AI 的三大要素之一。随着 ChatGPT 在全球范围内掀起 AI 大模型热潮，AI 算力需求也实现了大爆发。当前&nbsp;AI 大模型的算力需求特点是什么？怎样才能降低算力的使用门槛？IT 基础设施如何应对大模型不断增长的算力需求？近日，InfoQ《极客有约》邀请到了浪潮信息 AI 应用架构师&nbsp;Owen Zhu 博士，为大家分享 AI 大模型时代下的算力需求与挑战。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/axAUyTihpRTtlJBa20la">https://www.infoq.cn/video/axAUyTihpRTtlJBa20la</a>"</p><p>&nbsp;</p><p>姜雨生：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，微软软件工程师姜雨生。本期直播，我们邀请到了浪潮信息 Owen Zhu 博士来给我们做分享。我们今天直播的主题是《AI 大模型时代下的算力需求与挑战》，首先请您给大家介绍一下浪潮信息这家公司。</p><p>&nbsp;</p><p>Owen Zhu：非常荣幸可以在InfoQ 这个平台上与雨生老师一起进行在线交流。首先，我想简要介绍一下浪潮信息。我们是一家历史悠久的公司，主要专注于现代技术领域，特别是在计算能力方面，提供了广泛的基础设施支持。我们可以自豪地说，我们是全球领先的 IT 基础设施提供商，涵盖了计算、存储和网络等各种领域，为互联网、金融、通信等各行各业提供产品、解决方案和服务。</p><p>&nbsp;</p><p>随着 AI 的兴起，我们的 AI 服务器产品也处于全球领先地位。我个人负责人工智能、算法和应用领域的研究工作。在国内，每两台 AI 服务器中就有一台是由浪潮信息生产的，这也是我们的骄傲。</p><p>&nbsp;</p><p>在算法方面，我们一直在努力开展各种工作，尤其是在大模型领域。大约两三年前，我们就开始关注大模型领域，因为这个领域对计算能力有着巨大的需求。两年前，我们发布了千亿级别的大模型“源1.0"，至今我们仍在不断拓展大模型和其他前沿 AI 技术领域的研究和工作。</p><p></p><h2>“计算力就是生产力，智算力就是创新力”</h2><p></p><p>&nbsp;</p><p>姜雨生：算力是信息时代的新生产力，能分享下您对算力及其重要性的理解吗？它的价值主要体现在哪些方面？</p><p>&nbsp;</p><p>Owen Zhu：从浪潮信息的角度来看，我们在很早之前就开始关注算力的重要性。我们自己定位为算力供应商，因此早早地就开始强调算力的重要性，提出了像“计算力就是生产力”这样的概念。</p><p>&nbsp;</p><p>此外，我们与全球领先的产业研究院机构如国际数据公司IDC、清华大学进行联合编制，发布了许多关于算力的评估报告，例如，此前发布的《2022-2023全球计算力指数评估报告》。在宏观层面上，我们对算力与经济增长之间的关系进行了评估。一个有趣的数据是，我们引入了计算力指数，它用来量化评估每个国家的算力水平，范围从 0 到 100。最新评估结果显示，十五个样本国家的计算力指数平均每提高1点，国家的数字经济和GDP将分别增长3.6‰和1.7‰。这个数据在国内的很多报告中都被引用，说明越来越多的人，包括政府层面，认识到了算力的重要性。</p><p>&nbsp;</p><p>进一步来看，在智能计算领域，我们也提出了一个新概念，即智算力就是创新力，旨在生产力的基础上进一步增强人工智能计算的重要性。具体来说，人工智能在各个领域的应用中都发挥了举足轻重的作用非常明显，包括大模型。此外，在&nbsp;AI for Science 这些领域，人工智能正推动着科学研究，这表明算力在提供创新力和社会创新方面发挥着根本性的作用。现在热门的 AIGC，深刻反映了AI 在创造力方面的能力体现，而这些能力都是在算力的支持下实现的。</p><p>&nbsp;</p><p>此外，如果我们深入研究算力对整个 AI 发展的驱动作用，可以清楚地看到，它实际上是 AI 发展的核心支持和引擎。有很多例子可以证明这一点，包括算力从 2012 年以来的指数级增长。一个具体的例子是自动驾驶领域，这个领域已经取得了长足的发展。从最早只能提供辅助驾驶功能，到能够在高速公路上实现无人干预的自动驾驶，再到最近针对城市通勤的自动驾驶应用。浪潮信息服务非常多的汽车制造商和自动驾驶客户，他们对算力的需求也在逐步增加。在这些年里，我们还看到他们在 AI 算法方面的投入，特别是在模型训练方面，从技术进步的角度看，算力的需求可能增加了数十到数百倍。</p><p>&nbsp;</p><p>总之，无论从宏观经济发展层面还是从微观应用场景和算法层面来看，算力都扮演着至关重要的角色。</p><p>&nbsp;</p><p>姜雨生：算力是不是分很多的类别？从你的角度来说，算力能按哪些类别来区分呢？</p><p>&nbsp;</p><p>Owen Zhu：实际上，当我们谈论算力时，如果要进行分类，首先需要明确两个相似但不同的概念，即计算和算力。在过去，我们更多地强调计算，如云计算、边缘计算、科学计算、AI 计算、量子计算，等等。计算领域有各种分类方法，通常根据供给方式或计算发生的位置来划分，例如云计算和边缘计算。而算力实际上是计算能力的一个简称，它是一种衡量指标，通常是量化的。更进一步，我们可以使用类似 flops（每秒浮点运算次数）或者整数算力（int）等具体数值来衡量算力。因为算力是一种衡量指标，目前通常的分类方法是将其分为通用算力和专用算力，或者称之为智能算力，即 AI 算力。</p><p>&nbsp;</p><p>为什么要这样分类呢？这背后有一些历史渊源。在过去，CPU 通常是支持计算的主要处理器，因此我们将 CPU 提供的算力称为通用算力。在 AI 时代，我们更多地使用加速计算，也就是使用 GPU 或 AI 芯片，并且在算力的具体支持方面，我们可能会使用专门针对 AI 的数据格式，如 LP16、INT8、BF16、TF32 等。因此，在进行算力衡量时，我们将其区分为通用算力和智能算力。大致来说，当今我们谈论算力时，更多的是在讨论 AI 算力。</p><p>&nbsp;</p><p>姜雨生：针对刚才您提到的这几个分类，我们国内现在在这几方面的发展现状如何？</p><p>&nbsp;</p><p>Owen Zhu：这个问题实际上是一个相对宏观的问题。我们之前提到过的我们和国际数据公司 IDC 的评估报告，对各个国家在算力领域的投入进行评估和打分。总体来说，我国算力总规模全球第二，仅次于美国，年增长率近30%。将目光再聚焦于当下最热议的生成式AI算力，其从 2022 年的 8.2 亿美元增长到 2026 年的 109.9 亿美元，市场占比（生成式 AI 计算占整体 AI 计算市场）更是从 4.2% 增长到 31.7% 。</p><p>&nbsp;</p><p>姜雨生：有观众提问，自动驾驶系统哪部分对算力的需求最大？</p><p>&nbsp;</p><p>Owen Zhu：当涉及到自动驾驶时，我们需要考虑到一些关键环节，其中最重要的一个环节是感知。</p><p>&nbsp;</p><p>在自动驾驶中，感知是一个关键步骤。车辆上配备了多种传感器，如摄像头、雷达、激光雷达等，这些传感器收集到的数据需要进行处理，通常需要引入各种人工智能模型。对于雷达数据，我们可能需要使用基于雷达的 3D 目标检测或其他模型来进行感知。对于摄像头数据，我们可能需要使用基于图片或视频的 2D 或 3D 感知算法。在整个自动驾驶系统中，感知阶段通常是算力需求最大的阶段。这些计算通常在云端完成。在实际应用之前，自动驾驶模型通常需要大规模的训练。一些领先的自动驾驶企业，如特斯拉，拥有庞大的 GPU 和 AI 算力规模。此外，一些公司还在自研 AI 芯片和AI算力系统，这些芯片主要用于感知。</p><p>&nbsp;</p><p>近年来，人们还在尝试将大型模型引入自动驾驶领域，实现端到端的模型，也就是将各个环节整合到一个模型中。这意味着传感器数据被输入到一个大型模型中，以进行决策控制，并指导车辆下一步的操作。这个决策控制阶段也需要大量的算力投入。</p><p>&nbsp;</p><p>在自动驾驶领域，算力需求不仅限于感知阶段，还包括决策控制阶段，尤其是在引入深度学习算法后。这就是目前自动驾驶领域的大致情况。</p><p></p><h2>“算力的投入与智能的涌现有直接关系”</h2><p></p><p>&nbsp;</p><p>姜雨生：在我刚参加工作的时候，AI 并不是一个热门话题，很少有人提到 AI 这个概念。大多数人当时更多地从事计算机相关的工作，专注于一些传统的服务和应用层面的工作。在 AI 的大型模型兴起之前，算力用在哪些方面呢？</p><p>&nbsp;</p><p>Owen Zhu：这是一个很有趣的问题。虽然现在公众对算力的概念越来越熟悉，近年来也举办了越来越多的专门针对算力的活动，但实际上在此之前，算力的概念早已存在。从浪潮信息的角度来看，我们早在多年前就已经开始讨论算力。至于在 AI 大模型兴起之前，实际上有很多领域都在使用算力：</p><p>&nbsp;</p><p>互联网服务：在互联网领域，算力的需求一直很高。例如，回顾到 2019 年，百度中标了春晚的红包活动，为了支持这一活动，他们准备了高达 10 万台服务器的算力。这显示了在互联网抢红包等活动中，需要大规模的算力支持。在线购票和出行服务：在线购票、滴滴打车、美团外卖等服务都依赖于大量的算力来支持实时交易和路线规划。科学计算：科学领域一直在使用算力来进行复杂的计算，如天气预报、工程仿真、分子工程模拟、材料仿真等。天气预报的准确性不断提高，台风和洪水的预测也得益于强大的算力。</p><p>&nbsp;</p><p>总之，算力在许多不同领域都发挥着关键作用，早在 AI 大模型兴起之前就已经是一个重要的资源需求。</p><p>&nbsp;</p><p>姜雨生：我们现在正处于一个非常有趣的时刻，OpenAI 推出了 ChatGPT，全球范围内引发了对 AI 大模型的热潮。许多公司都在全力以赴投入算力，现阶段算力需求发生了哪些变化？算力对于 AI 的发展有多大的影响？</p><p>&nbsp;</p><p>Owen Zhu：我们必须承认算力在当前 AI 发展中的重要性是非常高的，这也是为什么人们争相获取算力的根本原因。我们可以展开讨论这个问题。之前我们一直提到深度学习的三驾马车：算力、算法和数据，它们共同推动了深度学习技术的进步。但随着大模型时代的到来，尽管这些要素仍然很重要，但算力的重要性更加凸显。</p><p>&nbsp;</p><p>为什么这样说呢？因为我们现在逐渐认识到一个事实，那就是通用人工智能引入了一些重要的概念，如泛化和涌现。这些是非常核心的能力，但如何衡量它们呢？渐渐地，业界形成了一个共识，即算力的投入与智能的涌现有着直接关系。</p><p>&nbsp;</p><p>这个观点为什么会出现呢？从理论分析的角度来看，大模型中有一个重要概念，称为"扩展性"，即如何扩展大模型的能力。比如，如果我们要将一个模型的参数扩大 10 倍，需要训练一个 10 倍规模的模型，我们需要多少算力来支持这个过程？这就是所谓的"扩展性"问题。在这方面，OpenAI 和其他公司进行了大量研究，发现扩展模型的过程是近似线性的。这意味着，要扩大 10 倍的模型，需要 100 倍的算力投入。这使得算力成为一个重要的标尺，用来衡量模型的能力。因此，算力的投入越大，模型的能力也越强。</p><p>&nbsp;</p><p>举个例子，GPT-3 拥有 1750 亿参数，训练时使用了 3000 亿的 token 数。而像&nbsp;Llama 2&nbsp;这样的新模型，虽然参数较少，只有 650 亿，但训练使用了 1.4 万亿的 token 数，实际上投入的算力更大。从评测指标上看，Llama 2&nbsp;在某些方面超越了 GPT-3，这进一步证实了算力投入与模型能力的关系。</p><p>&nbsp;</p><p>最近，有一些关于 GPT-4 的估测表明，它的算力投入可能是 GPT-3 的 68 倍，甚至更多。而谷歌即将发布的下一代模型 Gemini，被认为将投入超过 GPT-4 5 倍以上的算力。这显示出在大模型的算力投入方面，业界领先公司在成本上毫不吝啬。</p><p>&nbsp;</p><p>姜雨生：有观众提问，在算力足够的情况下，模型能力可以无限的增强，带来无限可能吗？</p><p>&nbsp;</p><p>Owen Zhu：业界对于这个问题尚无明确答案。然而，有一个观点是，当算力不再是限制时，数据将成为限制因素。这个观点在业界已经有一些人在讨论，即像 OpenAI 和其他互联网公司一样，他们正在大规模地进行模型训练，很快可能会耗尽互联网上的数据资源，这并非无稽之谈。</p><p>&nbsp;</p><p>事实上，当我们自己进行数据处理时，我们会发现互联网上的文本数据的质量和数量是有限的。特别是对于中文互联网来说，由于相对封闭的特性，获取高质量的数据可能会受到一些限制。因此，数据的限制可能很快会成为一个瓶颈。因此，尽管我们拥有强大的算力，但并不意味着智能会无限增长。</p><p>&nbsp;</p><p>然而，有很多解决方法，例如引入多模态数据。我们知道引入新的模态数据可以带入大量新信息，从而进一步提升模型的性能。因此，这个问题的解决方案可能是多种多样的，非常值得继续探讨。</p><p></p><h2>“大模型时代的基础设施建设”</h2><p></p><p>&nbsp;</p><p>姜雨生：云服务提供商在算力方面可能存在垄断或半垄断的情况，这使得访问大型模型成为一种昂贵的资源。对于个人开发者来说，有些人可能确实用不到这种大型模型，而另一些人可能承受不起这些服务的高成本。有声音认为买不起算力，直接将一大部分开发者挡在了 AI 时代的大门外，您怎么看“买不起”这一现象？</p><p>&nbsp;</p><p>Owen Zhu：今年以来，算力供应情况紧张，这涉及到多个层面的原因，不一一探讨，但与供应关系密切相关。解决买不起算力的问题，我们从基础设施和算力提供商的角度尝试各种方法。云服务可能是一种解决方案，即直接从公有云购买算力。</p><p>&nbsp;</p><p>此外，我们提出了一个重要的概念和策略，即“智算中心”。其逻辑是，政府或类似公益机构作为主体购买和储备以 AI 算力为主的资源中心。这种方法的提供方向更侧重于社会经济效益和社会效益，而不仅仅是商业利润。通过建立智算中心，我们可以推动产业发展，例如国内模型的培训，以及改进社会效率和模型应用，从而提高生活质量和企业效率等方面的利益。因此，在解决买不起算力的问题方面，智算中心的建立有很大帮助。</p><p>&nbsp;</p><p>多年来，我们一直在推动这些事情，并已经与许多地方政府建立了多个智算中心，如济南、南京、宿州等地，基于这样模式之上的模型已经帮助许多企业解决了各种问题，这也可以视为一种解决方案或策略。</p><p>&nbsp;</p><p>姜雨生：对于企业而言，大模型时代基础设施建设面临两个比较大的困难：一是高额的成本，二是随着算力集群规模增大，稳定性越难做到，效率也很难提升。对于第二点，目前有哪些解决方案？</p><p>&nbsp;</p><p>Owen Zhu：我们正在尝试解决这一问题，并与您之前提到的类似方向有些相似。实际上，我们可以从观察多家企业的现状入手，以解释这个情况。在过去，许多企业可能更倾向于使用公有云等云服务提供商的算力来满足 AI 需求，但是今年我们观察到了一些变化，即企业更倾向于选择高质量的算力，例如智算中心，或者自建基础设施。</p><p>&nbsp;</p><p>这种变化有多个原因，其中之一是对算力的需求发生了重大变化。现在，训练AI大模型可能需要数百甚至上千块 GPU 卡。对于企业来说，购买如此大规模的算力可能成为挑战，因为即使是公有云，其资源分布在不同的数据中心，难以实现集中供给。此外，云服务商通常通过资源超售等方式提高利用率，从而降低成本，但对于 AI 算力，用户更希望充分利用资源，不希望资源被超售。</p><p>&nbsp;</p><p>自建基础设施的成本可能相对较低，但也带来了一些新的挑战，如操作系统、驱动程序、环境配置、监控和调度等问题。为了解决这些问题，我们上月刚发布了一个大模型智算软件栈 OGAI，全称是Open GenAI Infra，旨在为客户提供一套技术堆栈，通过多层次的软件解决方案来解决这些问题。这包括对智算中心的支持，以及指南和工具，帮助用户部署和配置 AI 基础设施。对于许多用户来说，特别是那些刚刚购买算力的用户，部署可能是一个挑战，因此我们提供了一个指南，以指导他们完成部署并避免一些常见问题。此外，我们还提供一些商业化的软件解决方案，用于大规模算力的调度和硬件兼容性等问题。</p><p>&nbsp;</p><p>姜雨生：在大型 AI 模型时代，IT 服务领域的厂商正在积极探索新的可能性。展开来看，对于国内 IT 服务领域的厂商来说，大模型时代带来了一些机遇和挑战。我也很关心浪潮信息作为一家公司，是否在产品策略上发生了变化或者采取了一些创新举措。您提到了大数据平台，这确实是一个关键领域，许多公司都在不懈努力，旨在为客户提供更强大且易于使用的体验。我很想听听您的更多观点。</p><p>&nbsp;</p><p>Owen Zhu：您谈到的关于基础设施和算力的问题，确实对 AI 行业产生了许多挑战和机遇。最近，我注意到业界开始聊到&nbsp;AI 领域的人才，并强调了其重要性，可能超过了大型&nbsp;AI 模型的重要性。</p><p>&nbsp;</p><p>回到这个问题，我觉得有几个方面需要关注。首先，从市场的角度来看，当前算力仍然是一项短期内比较突出的问题，特别是在上半年，大家一直都在争抢算力资源。这种紧缺局面可能会一直持续到明年的 Q1 和 Q2。大模型对整个 AI 行业产生了革命性的影响，这是继 AlphaGo 之后的第二次重大变革。因此，算力短缺将继续存在。</p><p>&nbsp;</p><p>第二，我们需要关注的趋势是多元异构计算的概念，尽管这听起来有点抽象。目前，主要的加速芯片是 Nvidia 的 GPU，但随着加速计算和异构计算在计算中变得越来越重要，更多的加速计算芯片和解决方案将涌现，包括英特尔的 Habana 和 AMD 的 MI 系列等针对 AI 的加速芯片。这将导致市场出现多元化的生态系统，这一趋势将逐渐凸显。因此，我们需要思考如何在这种情况下实现兼容性和融合，以确保各种芯片和产品能够无缝协作，提供给用户一个一致的接口。</p><p>&nbsp;</p><p>第三，算力基建化将成为一个重要趋势，随着算力的不断增加，它将成为基础设施的一部分。政府、云服务提供商和科技公司等都在大规模投资和建设算力基础设施，将算力作为一种服务提供给外部。这将推动算力基建化技术的加速发展，以满足不同行业和应用领域的需求。</p><p>&nbsp;</p><p>总之，我们需要在硬件、软件和算法等多个层面上积极应对这些趋势。在硬件层面，我们需要关注多元化的 AI 芯片接入。在软件和算法层面，我们需要投入更多的资源来研发和支持算法，以及解决用户在应用 AI 时可能遇到的问题。此外，构建生态系统也是一个关键战略，让不同领域的专业公司和行业解决方案提供商共同合作，以实现 AI 技术在各个领域的落地应用。这将有助于促进 AI 产业的发展和应用。</p><p></p><h4>嘉宾介绍</h4><p></p><p></p><p>特邀主持：</p><p></p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p></p><p>嘉宾：</p><p></p><p>Owen Zhu，浪潮信息 AI 应用架构师，中国科学技术大学博士。从事人工智能方向相关工作多年，当前主要负责大模型、AIGC 等前沿 AI 算法研发和 AI 应用落地研究工作。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/eZ8J5Z7SuUSM4ql4ioVW</id>
            <title>AIGC如何改变金融业？不是所有智能化问题都要用大模型解决</title>
            <link>https://www.infoq.cn/article/eZ8J5Z7SuUSM4ql4ioVW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/eZ8J5Z7SuUSM4ql4ioVW</guid>
            <pubDate>Mon, 25 Sep 2023 06:25:40 GMT</pubDate>
            <updated>Mon, 25 Sep 2023 06:25:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> AIGC, 金融行业, 技术应用, 大模型, 落地可能性

总结: AIGC 技术在金融行业有广泛的应用可能性，可以在创造性、专业性和标准化方面发挥作用。在模型训练开销与效益、效能之间实现平衡需要回到业务目标和价值本身，找到切实可行的应用点并进行试验。金融行业的大模型应用需要私有化训练和足够的资源投入，同时需要解决监管合规和数据质量等问题。在数字化转型中，AIGC 的引入为金融机构带来了不确定性，但实践已经证明了其价值。在技术变革中，银行往往从内部开始，目前优先使用公有云的能力，但仍需解决工程师工作内容的明确提示问题。 </div>
                        <hr>
                    
                    <p>和所有新技术诞生之初一样，随着 AIGC 持续引发关注，各行各业都在寻找自身与 AIGC 结合的可能性。金融行业也不例外，丰富的场景、海量的数据、相对完善的技术实力...... 这些都是 AIGC 技术应用落地的温润土壤。</p><p></p><p>同样和所有新技术的发展一样，从技术出现到技术落地，需要跨越一定的周期，在这期间，市场会由感性的狂热慢慢转入理性的思考。AIGC 究竟适合在哪些场景应用？成本投入和效益产出如何做好平衡？商业落地的瓶颈又如何突破？</p><p></p><p>在 InfoQ《超级连麦. 数智大脑》直播中，太保寿险首席架构师周建华、富滇银行数字金融中心副主任李涛、华盛证券技术 VP 黄曙光围绕这些问题进行了探讨。大家认为，讨论 AIGC 技术应用，必须回归到业务目标和价值本身，并不是所有智能化问题都需要用到 AIGC 和大模型，很多时候，利用现有的智能化技术就能解决。</p><p></p><p>以下是分享全文（经 InfoQ 进行不改变原意的编辑整理）（<a href="https://www.infoq.cn/video/k0QUqSaEI4Hxbb4V94hZ">点击链接</a>"可查看完整直播回放）：</p><p></p><h5>InfoQ：AIGC 热度高涨，大家如何看待这一技术？它在金融行业的哪些业务场景会有落地可能性？</h5><p></p><p></p><p>周建华：AIGC 在落地应用方面，有很多可能性。从我的角度来看，要讨论 AIGC 的能力，需要回到技术本身，以及技术带来的变革。</p><p></p><p>具体看 AIGC，它有三个基础能力和三个发展方向：第一，大模型是它的基础，拥有广泛的知识；第二，具备超强理解力；第三，能够自我迭代，根据新的输入不断训练自身模型，无论是在金融还是其他领域，这些基础能力都可以扩展出不同的应用方向。</p><p></p><p>在应用层，我认为这三个能力可以分别体现在创造性、专业性和标准化方面。就创造性而言，AIGC 可以在营销文案编写、数据分析等领域发挥作用，甚至可以帮助开发人员实现自动化代码编写。</p><p></p><p>专业性方面，AIGC 可以在运营场景中，对业务规则进行自动化处理，减少安全漏洞的风险。此外，金融领域的技能培训也可以通过 AIGC 来提高，特别是在安全方面，AIGC 可以帮助提升整个团队的技能。</p><p></p><p>此外，AIGC 的交互能力也非常强大的。它可以作为个人助手、虚拟理财顾问，甚至可以处理客服、条款等复杂的问题。通过 AIGC，许多运营、客户服务、财富规划等工作可以实现自动化，进而提高效率。</p><p></p><p>黄曙光：我们已经在几个场景中进行了 AIGC 的应用实践。首先，由于证券行业的专业性与其他金融领域有所不同，用户在这里做投资决策时使用自己的资金，要求他们具备更丰富的金融知识，如财务报表理解等等。所以，我们将 AIGC 技术应用分为两类，一类是面向公司内部的应用，另一类是面向客户的应用。</p><p></p><p>在公司内部，我们利用 AIGC 技术提升工作效率。举例来说，内容部门需要进行诸如股票早评、午评和晚评等信息运营，通过采用 AIGC 技术，就可以大大加快内容生成速度，减少人力投入。</p><p></p><p>第二个场景针对的是技术支持和内部后勤岗位，我们需要让不同国家的员工能够理解我们的系统和文档，以更好地为客户提供服务。在这里，就可以利用 AIGC 构建辅助功能，帮助员工提升专业技能，特别是在不同语言环境下的技能培训。此外，我们还在尝试将 AIGC 应用于研发团队，以提高整体效率，特别是在一些单一逻辑或场景中，如单元测试。</p><p></p><p>从客户角度来看，我们希望通过 AIGC 降低用户交易的门槛，帮助用户更好地理解金融产品，辅导他们做出更好的投资决策。我们正在考虑引入“数字人”的概念，帮助客户解读财务报表中涉及的大量专业术语和知识积累。</p><p></p><h5>InfoQ：在 AIGC 落地过程中，如何在模型训练开销与效益、效能间实现平衡？</h5><p></p><p></p><p>周建华：首先提一下，关于 AIGC 和大模型的概念，我们需要将分开来讨论。AIGC 的概念在大模型出现之前就已经存在，这是需要澄清的第一点。另一方面，并不是所有智能化问题都需要借助 AIGC 和大模型来解决。很多问题实际上利用现有的智能化技术就能解决。</p><p></p><p>那么，如何平衡模型训练的开销、效益和效能，这是一个庞大的议题。首先，金融行业的大模型应用基于私有化的训练，这涉及到微调等步骤，必须在私有化环境中进行。所以，涉及两个方面的问题：第一，确保公司能够有足够持续的资源投入，包括 GPU 设备、大模型人才招募等；第二，确保有足够的数据或者能够生产足够高质量的数据进行微调。</p><p></p><p>其中，私有化模型的微调，在目前阶段不建议大规模投入，因为工程量巨大，微调不是只需要数据和 GPU，微调之后的大模型如何产生对齐的效果，如何能够符合监管合规要求都是非常大的难题。在解决效益问题方面，重要的是回到问题的原点，解决的是什么业务问题，为什么需要大模型才能解决？找到切实可行的应用点之后，然后再进行尝试性的试验。</p><p></p><p>在过去的智能化应用中，很多公司都因为未能找到业务流程上的痛点，导致创新停滞。解决这个问题并不容易，需要克服诸多门槛。所以，技术应用必须回到目标和业务价值，生产力的提升如何带来生产关系的改变。从目前来看，解决这个问题并没有简单的方法。</p><p></p><p>李涛：这个问题其实在我们准备进行 AIGC 应用以及制定 POC 时也充分进行了考虑。先谈谈关于开销和效益的问题。我们所在的金融领域正处于迅速发展的阶段，尽管在真正形成完整的应用生态方面还有很长的路要走。然而，从我们数字化转型的角度来看，实践已经逐步证明了其价值。</p><p></p><p>AIGC 引入确实为数字化转型增添了许多不确定性。我们感受到这种不确定性，是因为在过去的两年里，我们一直在发展前台、中台、后台一体化，但并未涵盖整个私有云的部署。这包括购买服务器、A100 和 V100 等显卡设备。或许在接下来的一年半里，许多银行会通过大型模型的技术超越我们。</p><p></p><p>从银行的角度来看，的确存在许多监管法规等问题，尤其是关于用户隐私。银行的技术变革往往从内部开始。因此，当我们与阿里、腾讯、华为等公司讨论 POC 时，大家对于开销的问题也并不清楚。与 OpenAI 不同，私有模型的训练不能像按照 token 收费。在国内的大型模型中，必须首先基于金融领域训练一个行业模型，然后再挂接到知识库并进行进一步的训练。这涉及到一定的成本，而且主要问题是，如何明确提示工程师的工作内容，目前并没有清晰的指导。尽管许多人认为这是一个简单的问题，但在实际技术研发中，提示工程师应该做什么还没有明确的方向。</p><p></p><p>因此，从内部研发、效能提升的角度来看，我们目前会优先使用公有云的能力。我们正在尝试将贷款制度、风控要求等挂接到库中，并进行问题检查和答案。我们的想法是，首先通过辅助人工的方式，然后逐步过渡到人工智能。与目前的规则性职能开销类似，这一过程的成本很难评估，但我们已经建立了强大的算力体系，也有了基本的硬件设施。</p><p></p><p>在富滇银行的战略中，首要任务是解释清楚。对于像我们这样的小型银行来说，人力资源非常有限，因此我们迫切需要利用远程数字银行等能力来提升服务水平，通过机器人理解客户意图。AIGC 的出现为我们指明了一个方向，并且我们也正在尝试着去实现这些目标。</p><p></p><p>在实现效益方面，很难有即时的收益。俗话说“飞猪能上天”，但对于金融领域的大型模型，我们仍然不太清楚整只猪是什么样的。这个探索的过程可能会有些疯狂，但实践将告诉我们真相，也许它会成为让我们弯道超车的一个机会。</p><p></p><h5>InfoQ：技术发展与商业应用是两回事，AIGC 要在金融业实现应用落地，目前还存在哪些瓶颈？如何突破？</h5><p></p><p></p><p>黄曙光：首先，政策是关键因素，因为金融行业在政策监管方面相当严格，对于新事物的尝试必然会面临不确定性。政策对于商业化落地的态度是重要的，需要不断摸索。像之前提到的，很多同行可能不得不面临私有化的问题。</p><p></p><p>其次是效果，效果决定了商业化的程度。商业化与演示之间的差异非常明显，如果在演示中成功率达到 90%，但在实际使用中失败了，这显然不行。业务部门面临巨大的业务风险，所以应用的成熟度对商业化落地有影响。</p><p></p><p>再者是成本。财务部门通常会关注投资回报率（ROI），短期内要达到一定规模才能解决一些问题，所以公司的财务决策会影响商业化的判断。</p><p></p><p>最后，人才方面的储备也是一个关键因素。</p><p></p><p>总之，AI 落地存在许多因素，这些因素相互影响，对于不同的企业来说，权衡这些因素会影响其商业化的决策。</p><p></p><h5>InfoQ：智能技术的应用可以降低对人的要求，但是新技术的出现，也会衍生出新的人才技能需求。对于金融机构来说，要更好地使用 AIGC 技术，需要哪些类型的人才？如何提前布局和培养这些人才？</h5><p></p><p></p><p>周建华：我个人认为，首先，算法方面的人才是必不可少的。但在基础设施建设方面，特别是数字化基础设施，例如大型模型的基建方面，金融行业自己可能并不需要过多涉足，因为门槛相当高。这不仅仅是硬件的问题，还涉及到复杂的工程化和各种问题。我认为这个领域可能只有大公司才有可能完成。</p><p></p><p>此外，还有两类人才是至关重要的：一类是智能化战略规划人才，他们能够通过对其他领域的成功案例中的借鉴，对企业自身的战略规划做出部署；另一类是智能化应用人才，他们不需要成为顶尖的算法专家，需要的是智能化应用实战能力。</p><p></p><p>最后，我认为业务方面也需要提升对 AIGC 或大型模型能力的培训，形成对数字化生产力的统一认识。从来没有一个数字化项目是仅靠技术就能成功的，这需要公司级别的战略转型，而技术只是工具之一。</p><p></p><p>李涛：目前确实很难给出一个明确的答案，即银行在 IT 领域需要什么样的人才。我之前提到了一些当前比较火的趋势，如大规模工程师等，但实际上具体的需求还不清楚。因此，我们需要通过实践来探索，只有在实践中才能了解我们需要什么样的人才。</p><p></p><p>大模型的发展现在有点类似于当初大数据发展的情况。随着时间的推移，大模型正在逐渐降低应用门槛。就像刚才提到的，我们并不一定需要高级的大模型建模人员，而可以基于现有行业模型进行产品模型的开发。然而，这需要通过探索来确定，也就是在实践过程中了解我们确切需要哪些人才。</p><p></p><p>正如刚才周老师所说，数字化转型的核心是业务价值，而当我们进入到模型阶段时，核心就变成了用户价值。然而，业务价值和用户价值之间有何区别，这是一个问题，需要深入探讨。</p><p></p><p>事实上，我最近也一直在思考业务价值驱动和用户价值驱动的问题。尽管我们一厢情愿地触及用户，但实际上我们并不清楚用户真正的需求。我目前更关心的是，如何在接触用户的过程中，真正了解他们的想法和需求。</p><p></p><p>当前银行业务相当复杂，同质性很高，因此实现数字化转型需要突破壁垒，纯粹的技术“自嗨”是行不通的，必须有业务价值的支撑。在专业人才方面，首先，我们需要明确的业务价值，然后需要既懂业务又懂技术的人才来实现。然而，这样的人才在各个机构中都很紧缺。</p><p></p><h5>InfoQ：在未来三到五年的短中期内，对 AIGC 技术在银行 / 保险 / 证券的应用和发展趋势有哪些看法和预测？</h5><p></p><p></p><p>黄曙光：我认为在未来的 3-5 年里，我们将迎来大规模应用的时期，我对在各个领域看到 AI 技术的应用充满信心，可能会出现一些杀手级应用，或者是具有重大影响力的应用，它们将逐渐浮出水面。</p><p></p><p>李涛：我总结两句话：第一，大模型将成为新一代决策支持系统，完整连接理解、思考和行动的链路；第二，我对明年银行的 APP 充满期待，相信它将经历改变，不再保持现有的静态外观，而是朝向一个数字人坐镇其中的状态转变，我希望我们银行能尽快推出这样的产品。</p><p></p><p>周建华：在未来的发展中，有几个方面可以比较确定：</p><p></p><p>第一，在营销素材领域，大规模应用会显而易见。金融行业都拥有庞大的线下团队，他们的诉求是需要高质量的营销素材来进行的客户拓展和持续经营。</p><p></p><p>第二，智能客服方面也会发生巨大的变革。传统的知识库和知识图谱运维成本相对较高，而利用大模型进行智能客服可以降低成本，提高用户满意度。此外，未来智能客服可以借助大模型直接回答问题，类似之前的 ChatGPT 等应用。</p><p></p><p>第三，私人助理领域也将迎来大规模的应用。这意味着各种角色将有机会通过 copilot 的方式获得个人助理服务。这三个领域的发展比较确定，其他方面可能还需要观望。</p><p></p><p>这三个领域的共同特点之一是涉及的数据并不是特别私密，不需要核心数据，这为尝试提供了机会，也不一定需要私有化部署。其次，需求非常明确，因此开发过程相对比较容易。</p><p></p><p>除此之外，还有其他领域可以考虑，比如从开发和技术的角度，一个方面是协助编写代码，金融行业存在大量外包，代码质量不一定很高。另一个方面是解决安全性问题，代码漏洞是一个头疼的问题，可以通过大模型来解决水平越权和垂直越权的问题，这也是我认为能够直接看到价值的两个方面。</p><p></p><h4>活动推荐</h4><p></p><p>首届<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">FCon全球金融科技大会</a>"将于 11 月 19-20 日在上海举办。大会将围绕金融领域数字化转型挑战探索、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等 10+专题进行交流。</p><p></p><p>目前大会邀请了汇丰科技中国区的代理总经理马国栋、度小满金融数据智能部总经理杨青先、蚂蚁集团副总裁 &amp; 首席技术安全官韦韬博士、恒生聚源总经理吴震操担任大会联席主席。更多嘉宾仍在邀请中......</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">点击链接</a>"即可查看全部演讲专题。</p><p></p><p>目前 <a href="https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">5 折 优惠购票</a>"，仅限前 100 人，咨询购票可联系：17310043226（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7FI0GiPmjOD70e0WSCNZ</id>
            <title>裁错了还是变相降薪？大厂粗暴裁员后又求员工回来，网友：拿什么再爱你？</title>
            <link>https://www.infoq.cn/article/7FI0GiPmjOD70e0WSCNZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7FI0GiPmjOD70e0WSCNZ</guid>
            <pubDate>Mon, 25 Sep 2023 05:59:17 GMT</pubDate>
            <updated>Mon, 25 Sep 2023 05:59:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 裁员潮过后，一些企业开始反思裁员决策，重新招聘之前被解雇的员工。Meta和Salesforce等公司召回了一部分员工，并加大在AI领域的招聘力度。被裁员工是否应该回去，取决于个人情况和对公司的信任。裁员后召回员工的做法并非新鲜事，一些公司在裁员后意识到员工的价值，希望他们回归。然而，对于被裁员工来说，重新回到公司可能需要重新评估自己的职业发展和未来道路。总体而言，行业对于掌握AI技能的人才抱有强烈兴趣。 </div>
                        <hr>
                    
                    <p>过去一年，多家大厂掀起了“裁员潮”。但在裁员风波过去后，一些企业似乎开始“反思”自己的行为：是不是裁多了？裁错了？</p><p>&nbsp;</p><p>近期，包括 Meta、Salesforce 在内的企业开始召回一部分之前被解雇的员工，并在 AI 等热门领域加大招聘力度。但对于被裁的员工来说，该不该回去呢？</p><p></p><h2>粗暴裁员后，Meta 恳请部分员工回归</h2><p></p><p>&nbsp;</p><p>去年 11 月，Meta 首席执行官扎克伯格曾宣布裁员 1.1 万人，约占当时员工总数的 13%。今年&nbsp;3 月，Meta 宣布将在今年裁员 1 万人。截至今年 5 月，Meta 已经通过多轮裁员削减了四分之一人力。</p><p>&nbsp;</p><p>消息显示，随着 Meta 公司业绩改善，招聘工作再次启动。Meta 公司重新开始加快招聘步伐，特别是工程和技术岗位。Meta 当前的几百个招聘岗位主要集中在软件、硬件和 AR/VR 工程领域，外加基础设施和数据中心方面的主要技术角色。一位知情人士表示，业务岗位则几乎没有招聘，这也是受到最新一轮裁员影响最大的领域。由于大幅削减了经理岗，Meta 目前的大多数空缺职位集中在个人贡献者方面。</p><p>&nbsp;</p><p>值得一提的是，在过去几个月，不少之前被 Meta 解雇的员工都再次接到了社交巨头伸来的橄榄枝。据悉，自去年 11 月以来，所有被解雇的 Meta 员工都可通过“校友门户”重新申请新职位。</p><p>&nbsp;</p><p>有知情人士指出，Meta 正在寻求经验丰富的人才，并减少对应届毕业生和实习生的招录。返岗成功率最高的，往往是那些之前担任较高工程职务且绩效评估良好的人选。该公司也对外部候选人敞开了怀抱，离职的老员工也需要经过完整面试的筛选。另一位知情人士提到，尽管如此，“10 比 1”的招聘比例还是更倾向于之前被裁撤的老员工。</p><p>&nbsp;</p><p>据外媒报道，三名知悉公司内情的人士透露，目前已经有几十名员工被重新雇用，且大部分是在今年 6 月。但知情人士称，Meta 重新雇用的老员工中，有不少担任的是之前没接触过的工作、或者工资/职级有所下降。</p><p>&nbsp;</p><p>一位重新回归 Meta 的老员工表示，尽管他申请的职级与被裁之前相同，但总薪酬还是缩水了约 10%。不过考虑到目前 Meta 股价正在上涨，此人预计收入将在一年之内回归被裁前的水平。</p><p>&nbsp;</p><p>其他人则不太能接受 Meta 这番神奇操作。另一位曾被解雇、目前在重新申请入职的老员工表示，他们最近看到关于原本工作岗位的招聘信息。现在这个岗位已经被转为外包形式，且薪酬比之前降低了 20%。</p><p></p><h2>裁员后又召回，成大厂常规操作？</h2><p></p><p>&nbsp;</p><p>Meta 这波裁员后又召回老员工的操作并非先例。马斯克在刚接手 Twitter 时，也曾大刀阔斧砍掉不少岗位，但在员工离开之后才意识到 Twitter 根本离不开他们掌握的技能和经验。</p><p>&nbsp;</p><p>去年 11 月，Twitter 在解雇了 3700 人之后，很快又对数十名员工进行了召回。有消息人士称，一些被要求重回&nbsp;Twitter&nbsp;的人是“被错误解雇”的，还有一些人在管理层意识到他们的工作和经验可能是构建马斯克设想的新功能所必需之前就被解雇了：这些人对 Twitter 生态系统的运转至关重要。管理层很快就意识到了这一点，并要求他们回去。</p><p>&nbsp;</p><p>云计算公司 Salesforce 也有过类似的操作。去年 11 月初，Salesforce 宣布进行第一轮裁员，尽管具体裁员人数没有透露，但据悉不超过 1000 人。今年年初，Salesforce 宣布裁员 10%，大约 8000 名员工将受到影响。此外，还将关闭部分办公室以降低开支。</p><p>&nbsp;</p><p>此前不少硅谷巨头发现疫情期间的盲目扩张令人员冗余，于是开始大刀阔斧加以裁撤。据科技招聘网站 Trueup.io 称，今年到目前为止，全球科技企业已经累计裁员 35 万名，且一月份的裁员力度最大。</p><p>&nbsp;</p><p>与今年年初相比，如今的市场需求有了巨大转变，不少曾经裁员的大厂开始重新招聘。近期，Salesforce 计划招聘 3000 名员工，Salesforce 向彭博社证实，本轮招聘重点主要集中在销售、工程和数据云产品团队，并表示新员工将有助于发展其接下来重点关注的 AI 业务。</p><p>&nbsp;</p><p>此外，Salesforce 还开始召回一部分之前被解雇的员工。Salesforce 公司 CEO Marc Benioff 甚至直接向已经在其他单位就职的前员工们喊话，希望他们能考虑回归。Salesforce 最近还举办了一场吸引老员工的活动，向约 50 名前高管送上穿着绘有“飞去来器”图案衬衫的毛绒玩具。</p><p>&nbsp;</p><p>但对于已经被解雇、特别是还没有找到新工作的员工们来说，Salesforce 当初粗暴的裁员方式实在令人心有余悸。部分员工对此提出批评，称这种方式似乎与 Salesforce 这家云软件厂商的企业文化背道而驰。毕竟 Salesforce 高管、特别是 Benioff 本人，一直强调公司员工之间都是“家人”。</p><p>&nbsp;</p><p>当然，这种忽冷忽热的态度对于酒店等行业的雇员来说一点也不新鲜。在疫情爆发之时，不少餐饮服务业的员工被解雇。而在政策放开之后，雇主们又争先恐后填补空缺，为员工们开出了更好的工作条件。</p><p>&nbsp;</p><p>现在，同样的情况又落到了科技工作者头上。有些人可能要求开出更高的薪酬才考虑回归，也有些人则对此表示理解，毕竟大规模裁员并非人力所能掌控。但劳动保障仍然成为行业内一大重要考量因素：在 Trueup.io 上，亚马逊和 Meta 等大厂的招聘列表旁，都会用数字标注该公司上一轮裁员是多少天之前。</p><p>&nbsp;</p><p>强劲的需求当然会吸引一部分员工回归。总体而言，行业对于掌握 AI 技能的人才抱有强烈兴趣。LinkedIn 发言人在邮件采访中表示，LinkedIn 上用英语提及 AI 技术的全球职位发布量比 ChatGPT 刚刚亮相时的 2022 年 11 月增长了 21 倍。</p><p></p><h2>被裁后该不该再回去？</h2><p></p><p>&nbsp;</p><p>哈佛商学院研究裁员问题的管理实践教授 Sandra Sucher 表示，粗暴裁员的企业往往更难说服前员工重返岗位。在某些情况下，如果当初的裁员比较顺畅合理，那么前员工可能更愿意回归。</p><p>Sucher 解释道，“想让我回去，说明你还重视我。”但前员工们还是想确定自己要如何在企业内重建职业生涯，比如“不止是当前，我的未来道路会是什么？”</p><p>&nbsp;</p><p>Sucher 表示，对于部分员工来说，个人和组织之间的彼此认知可能是影响回归的核心因素。她认为在做出决定之前，员工们不妨问问自己：我当初为什么被解雇？最近发生了什么变化，让你再次需要我了？我为什么要继续相信你？</p><p>&nbsp;</p><p>要做出这样的决定并不容易。特别对某些被粗暴对待的员工，重建信任已经几乎没有可能。Sucher 指出，“我会在心中暗暗对公司做一番评判，但不会从根本上给予信任。”</p><p>&nbsp;</p><p>而对于那些打算回归的员工，则需要认真打听公司的下一步战略，确定自己不会很快被再次裁撤。</p><p>&nbsp;</p><p>Sucher 认为，裁员后又被召回的员工对公司的态度恐怕会发生难以逆转的影响。“人们心中的安全感被永久打破，大家再也不相信「只要干得好，就能保住工作」这套理论了。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.businessinsider.com/salesforce-meta-big-tech-companies-rehire-workers-employees-laid-off-2023-9">https://www.businessinsider.com/salesforce-meta-big-tech-companies-rehire-workers-employees-laid-off-2023-9</a>"</p><p><a href="https://www.businessinsider.com/meta-rehiring-workers-from-layoffs-2023-8">https://www.businessinsider.com/meta-rehiring-workers-from-layoffs-2023-8</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>