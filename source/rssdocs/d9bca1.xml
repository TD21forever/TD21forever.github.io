<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/60rpbqOcSZJ0E9z2lO3w</id>
            <title>64亿美元的交易，是IBM与HashiCorp的一场相互救赎</title>
            <link>https://www.infoq.cn/article/60rpbqOcSZJ0E9z2lO3w</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/60rpbqOcSZJ0E9z2lO3w</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 08:54:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: IBM, HashiCorp, 收购, Terraform
<br>
<br>
总结: IBM斥资64亿美元收购HashiCorp，希望在自家红帽品牌提供的混合云功能基础之上做出进一步探索。HashiCorp在云生态系统中拥有广泛的影响力，与众多主流云厂商都是合作伙伴关系。虽然HashiCorp名号不如IBM家喻户晓，但其基础设施即代码工具Terraform有着惊人的影响力。收购对于双方都具有战略意义，IBM通过收购补充基础设施软件产品组合，而HashiCorp将享受IBM的丰富经验和市场渠道。 </div>
                        <hr>
                    
                    <p></p><h2>IBM斥资64亿美元收购HashiCorp</h2><p></p><p>当地时间2024 年 4 月 24 日，IBM宣布斥资64亿美元（即每股 35 美元）收购了Terraform的创造者HashiCorp，希望能在自家红帽品牌提供的混合云功能基础之上做出进一步探索。HashiCorp 在云生态系统中拥有广泛的影响力，它与众多主流云厂商都是合作伙伴关系。</p><p>&nbsp;</p><p>IBM表示，HashiCorp收购交易将在结束后的第一整年内拉升其调整后的息税折旧及摊销前利润，并在第二个整年内帮助增加自由现金流。IBM公司预计交易将于2024年底完成，HashiCorp现任CEO&nbsp;Dave McJannet也计划继续留在公司。</p><p>&nbsp;</p><p>HashiCorp 成立于 2012 年，虽然名号不如IBM那样家喻户晓，但HashiCorp旗下的基础设施即代码工具Terraform却有着惊人的影响力，该工具主要用于分配及调整云端及本地资源。此外，HashiCorp还提供工作负载编排产品Nomad、开发者平台Waypoint以及Vault、Boundary及Consul等一系列安全工具。就在收购消息发布的两天之前，HashiCorp还刚刚推出了基础设施云The Infrastructure Cloud产品，将其基础设施及安全产品同HashiCorp Cloud平台相结合，旨在提供统一的云管理平台。</p><p>&nbsp;</p><p>作为一家曾经炙手可热的硅谷初创企业，HashiCorp在2021年首轮公开募股（IPO）后迅速崛起，短短一个月后股价就来到近100美元的高位。而如今IBM的收购价格仅为35美元，两年多之后缩水达65%。</p><p>&nbsp;</p><p>虽然其当家开源软件有着极为广泛的受众群体，但在过去一年间，HashiCorp却因决定从 Mozilla 开源许可证 v2.0 切换到商业源代码许可证（BSL），从而限制了其产品的免费使用而饱受争议和批评。</p><p>&nbsp;</p><p>2021年8月，HashiCorp对其商业化策略进行了重大调整，此举很快引起开源社区的激烈反响。当时，不少开发者在社交平台X上发文表示抗议，“我对HashiCorp比对IBM更恼火，我其实很高兴IBM承认了我们已经知道的事情，但是HashiCorp……我只是很失望。我非常尊重他们，不想冒犯他们，但这件事上我找不出来任何一个积极的点。”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee35b7dfac06eefc446fa2255eada114.png" /></p><p></p><p>“这一事件的影响比最初想象的要广泛得多--这件事情正在整个科技行业掀起涟漪！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48fcd401c22cd9f775721c357d8c495f.png" /></p><p></p><p>2023年12月，HashiCorp创始人Mitchell Hashimoto离开公司，且有传言说Hashimoto与现任CEO McJannet在商业策略上存在冲突。</p><p></p><h2>此次收购对于双方的意义是什么？</h2><p></p><p>&nbsp;</p><p>除了许可证风波外，最近几年HashiCorp可谓过得风雨飘摇。在被收购之前，HashiCorp 就已经出现了业务增长放缓趋势。</p><p>&nbsp;</p><p>与许多在 2019 年至 2022 年之间进行 IPO 的云原生领域公司一样，HashiCorp 从未实现盈利。在 2022 年中期之前的低利率时代，增长才是最重要的。但从那时起，投资者的关注点从业务的增长转变为了盈利上。而盈利问题正是HashiCorp的痛中之痛。</p><p>&nbsp;</p><p>HashiCorp 高度依赖 ARR（Annual Recurring Revenue，年度经常性收入） 超过 10 万美元的客户群。据分析师称：</p><p>&nbsp;</p><p></p><blockquote>HashiCorp 89% 的收入来自每年在其身上花费超过 10 万美元的客户，而这些客户仅占其付费客户群的 19%（截至 2024 财年第 1 季度，共有 4392 名客户中的 830 名）。除此之外，收入在地域上也高度集中，71% 的销售额来自美国。</blockquote><p></p><p>&nbsp;</p><p>在接下来的三个季度中，这个 10 万美元以上细分市场的增长水平进一步放缓，而收入集中度仍保持在 89%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13a51cc020dc3877b601f6ff9132d7fc.png" /></p><p></p><p>&nbsp;</p><p>与此同时，HashiCorp 的净美元保留率 (NDR) 持续下降，在过去两个季度大幅下降至 115。虽然考虑到更广泛的宏观经济环境，许多公司 130+ NDR 的日子已经成为过去，但这里的下降速度异常快。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a3/a3ec0fd2a98db1ec6a0f932a216f462b.png" /></p><p></p><p>简而言之，这是一家增长迅速放缓的业务，无法支撑其现有估值，更不用说IPO时的估值了。它成为收购目标已经有一段时间了，但即使自 IPO 以来价格有所下降，价格仍然相对较高。</p><p>&nbsp;</p><p>没有人怀疑 HashiCorp 创建的产品（尤其是 Vault 和 Terraform）的实用性和受欢迎程度，但实用性本身并不能转化为企业收入。 IBM 拥有足够的企业关系，使此次收购物有所值。</p><p>&nbsp;</p><p>William Blair研究分析师Jason Ader在研究报告中表示这笔交易对于两家公司都极具战略意义。</p><p>&nbsp;</p><p>“我们相信这笔交易对两家公司都具有巨大的战略意义。IBM通过高人气工具（Terraform、Vault）补充并支撑其不断增长的基础设施软件产品组合，而HashiCorp也将享受蓝色巨人的丰富经验以及影响力巨大的市场渠道。”</p><p>&nbsp;</p><p>IBM公司CEO Arvind Krishna在电话会议上指出，HashiCorp的产品“在开发者社区中得到广泛采用”，总下载量已经超过5亿次，并得到超85%财富500强公司的使用。</p><p>&nbsp;</p><p>值得一提的是，Gartner公司副总裁Sid Nag在采访中表示，从IBM的角度来看，这笔交易的动机非常明确。首先，IBM正在努力充实其Ansible平台的功能，而达成这一目标的最好方式当然就是直接从主要竞争对手那边“借花献佛”。</p><p>&nbsp;</p><p>其次，他表示IBM正努力扩大其市场范围与收入潜力。通过收购HashiCorp，其不仅能够获得对方的原有客户，同时也能将HashiCorp的技术方案出售给自身庞大的企业客群。</p><p>&nbsp;</p><p>再从更深入的角度来看，这笔交易可以说是“IBM成功收购红帽并实现货币化的又一个翻版”。IBM似乎意识到红帽Ansible平台中存在缺陷，而这已经开始阻碍其扩大市场份额。而收购HashiCorp能够增强Ansible能力，及时填补这些缺口。</p><p>&nbsp;</p><p>那么HashiCorp又为什么会接受收购？</p><p>&nbsp;</p><p>乍看之下，很多人其实难以理解为什么一家技术产品广受好评的上市公司会愿意接受收购，特别是接过资源配置市场上最大竞争对手IBM递来的橄榄枝。</p><p>&nbsp;</p><p>但在给投资者们的报告中，美国银行证券分析师Wamsi Mohan指出，HashiCorp的“业务增幅一直在下降”。他同时补充称，IBM有望“推动成本协同效应，凭借更大的客群规模恢复收入增长。”</p><p>&nbsp;</p><p>因此外界不由得猜测，HashiCorp当真陷入了困境？</p><p>&nbsp;</p><p>AvidThink公司创始人Roy Chua在采访中表示确有此事：HashiCorp的确在“开发和DevOps社区中站稳了脚跟——至少在整个商业源代码许可证（BSL）闹剧及Linux基金会建立OpenTofu项目之前是这样。”</p><p>&nbsp;</p><p>2023年8月，HashiCorp决定从Mozilla公共许可证框架过渡至商业源代码许可证框架。该公司当时表示，此举旨在扩大其对开源技术商业化的控制权。然而用户对这一变化并不买账，两周之后Linux基金会就迅速建立了名为OpenTofu的Terraform分叉项目。</p><p>&nbsp;</p><p>Chua表示，“考虑到众多DevOps和软件团队都在免费使用并高度依赖Terraform，我觉得HashiCorp其实没有完全意识到由此创造的价值。如果他们在Scott Johnston及其团队的领导下成功完成Docker式转型，相信绝对能拿下更多收入并避免当下被收购的命运。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/23/23a7124147479668b7d0315bb16f762d.png" /></p><p></p><p>随着IBM接管，“我们也都将关注IBM是否会撤销转向BSL的决定。”Chua还提到，IBM最近领导了另一个名为OpenBAO的Linux基金会孵化项目，这就是HashiCorp Vault的一个分叉，明显也是在表达对许可证框架转换的不满。</p><p>&nbsp;</p><p>“现在IBM与Terraform一同控制了该资产，开发人员将推动IBM撤销BSL转换并停止分叉。”</p><p>&nbsp;</p><p>有不少人担心IBM收购后会加强对Terraform 和 Ansible的限制或者对这两款软件“胡作非为”，甚至还调侃道，如果IBM要将Terraform 和 Ansible合并，那岂不是要叫“Terrible”？</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/027d7f8b6442766ec0426884f49339af.jpeg" /></p><p></p><p>System Initiative 联合创始人&amp;CEO&nbsp;Adam Jacob在X上发文对该笔交易表示祝贺：“祝贺所有我认识的Hashicorp团队的人，他们建立起这样一家公司是一个令人难以置信的成就，有人认为你们创办的公司价值40亿美元以上是一件疯狂的事情，但你们的确做到了，恭喜。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/95/95d4c0184c6b6a6d2ea8e10a65083ce8.png" /></p><p></p><p></p><h2>商业许可证BBL伤害了谁，保护了谁？</h2><p></p><p>&nbsp;</p><p>与Hashicorp更改许可证引发社区不满，从而引出开源软件分支的事上个月就发生过一次。</p><p>&nbsp;</p><p>3 月 21 日，Redis 背后企业 Redis 的 CEO Rowan Trollope 宣布，该项目的许可证类型将从原本的 BSD 开源许可证变更为 RSAL&nbsp;v2 与 SSPL&nbsp;v1 双许可证。</p><p>&nbsp;</p><p>Redis 公司称此番许可证变更主要是为了保护 Redis 公司的商业利益，防止云厂商利用开源版本支持商业 Redis SaaS 服务。此类行为在市场上并不少见，Confluence、MongoDB 及 Elastic 等厂商此前已经对其开源项目做了类似的许可证变更，旨在保护自身利益。但 Redis 公司的这一举动却引发了众多开发者的愤怒，其中一个重要原因就是 Redis 社区中有着大量外部贡献者。这种单方面修改许可证的行为被视为对社区的背叛，更是对贡献者们的背叛。</p><p>&nbsp;</p><p>事实上，关于商业许可证的争议一直不断。</p><p>&nbsp;</p><p>HashiCorp对BSL的采用让开源社区的许多成员感到困惑。BSL结合了开源和专有许可的各个方面，要求公司将其软件作为开源提供，但只在特定的持续时间内提供。在约定的期限之后，软件的许可证可以更改为专有模式。那么，这就带来了这样一个问题：这种许可结构是否符合开源软件的基本原则?</p><p>&nbsp;</p><p>成功的开源项目的基石之一是社区参与。来自不同背景的贡献者协作构建、增强和维护软件，使每个人都受益。HashiCorp转向BSL带来了一定程度的不确定性，这可能会阻碍社区的参与。当许可条款突然改变，影响他们自由使用、修改或分发软件的能力时，贡献者可能会犹豫是否要在项目中投入时间和精力。</p><p>&nbsp;</p><p>同时，变更许可证也会浇灭独立开发者和独立贡献者为社区做贡献的热情。开源的一个特点是它使独立开发人员和独立贡献者能够创建创新的解决方案。BSL的采用可能会阻碍这些人使用软件作为基础构建可行的商业产品的能力。这种转变可能导致市场上这类产品的数量减少，从而抑制竞争，限制最终用户的选择。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.sdxcentral.com/articles/news/ibm-acquires-hashicorp-for-6-4b-open-source-terraform-questions-remain/2024/04/">https://www.sdxcentral.com/articles/news/ibm-acquires-hashicorp-for-6-4b-open-source-terraform-questions-remain/2024/04/</a>"</p><p><a href="https://medium.com/@fintanr/on-ibm-acquiring-hashicorp-c9c73a40d20c">https://medium.com/@fintanr/on-ibm-acquiring-hashicorp-c9c73a40d20c</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bKHH3LKgcqp7PsF0UvkQ</id>
            <title>苹果发布OpenELM：专为在设备端运行而设计的小型开源AI模型</title>
            <link>https://www.infoq.cn/article/bKHH3LKgcqp7PsF0UvkQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bKHH3LKgcqp7PsF0UvkQ</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 07:19:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, 开源, AI模型, OpenELM
<br>
<br>
总结: 苹果公司发布了开源的AI模型OpenELM，该模型家族包括预训练模型和指令微调模型，参数规模在2.7亿到30亿之间。OpenELM通过层级缩放策略和公开数据集预训练后微调，实现了Transformer语言模型效果的改进。苹果还发布了OpenELM模型的权重和训练中的不同检查点，但同时强调模型不提供任何安全保证。这次开源举动让苹果加入了开源大模型阵营，与其他企业一同推动大模型技术和代码的开放。 </div>
                        <hr>
                    
                    <p></p><blockquote>今天，苹果破天荒整了个大新闻。</blockquote><p></p><p>&nbsp;</p><p>苹果开源了一个在设备端运行的AI模型OpenELM，同时还公开了代码、权重、数据集、训练全过程。</p><p>&nbsp;</p><p>就像谷歌、三星及微软着力在PC和移动设备端推动生成式AI模型的开发一样，苹果也加入了这一行列。这是一个新的开源大语言模型（LLM）家族，能够依托单一设备平台运行，完全无需借助云服务器。</p><p>&nbsp;</p><p>OpenELM已经于日前在AI代码社区Huggang Face上发布，由多个旨在高效执行文本生成任务的小模型组成。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/790f59803f38f9347f39f9b8035c28d3.jpeg" /></p><p></p><p>苹果投身开源AI战局，在Hugging Face上发布四种新模型！</p><p>&nbsp;</p><p>OpenELM模型家族共有八位成员，其中四个为预训练模型，另外四个为指令微调模型，参数规模在2.7亿到30亿之间（即大模型中人工神经元之间的连接数量，参数越多通常意味着性能更好、功能更强，但并不绝对）。而微软Phi-3模型为38亿。</p><p>&nbsp;</p><p>预训练是让大模型得以生成连续、可用文本的重要方法，而指令微调则能够让模型以相关度更高的输出响应用户的特定请求。具体来讲，预训练而成的模型往往会通过在提示词的基础上添加新文本来完成要求，例如面对用户的“教我如何烤面包”这条提示词，模型可能并不会给出分步说明，反而傻傻回答称“用家用烤箱烤”。而这个问题恰好可以通过指令微调来解决。</p><p>&nbsp;</p><p>OpenELM通过采用层级缩放策略、在公开数据集预训练后微调，实现了Transformer语言模型效果的改进。因此，OpenELM 的transformer layers不是具有相同的参数集，而是具有不同的配置和参数。这样的策略能让模型精度显著提高。例如，在大约十亿参数的预算下，OpenELM的准确率较OLMo提升了2.36%，且预训练所需的Token数量减少了一半。</p><p>&nbsp;</p><p>苹果在其所谓“示例代码许可证”下发布了OpenELM模型的权重，以及训练中的不同检查点、模型性能统计数据以及预训练、评估、指令微调与参数效率调优的说明。网友点评说，“可以说对开发者来说很友好了，毕竟深度网络的很大一部分难点存在参数调节。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b372e1fe853335bb242f88236fe4301e.jpeg" /></p><p></p><p>&nbsp;</p><p>苹果的示例代码许可证并不禁止商业使用或修改，仅要求“如果您以完整且未经修改的方式重新发布苹果软件，则必须在所有此类发布中保留本通知以及以下文本与免责声明。”</p><p>&nbsp;</p><p>该许可不是公认的开源许可证，虽然苹果也没有做过度的限制，但它确实明确表明，如果任何基于 OpenELM 的衍生作品被认为侵犯了其权利，苹果保留提出专利索赔的权利。</p><p>&nbsp;</p><p>苹果公司还进一步强调，这些模型“不提供任何安全保证。因此，模型可能会根据用词提示词生成不准确、有害、存在偏见或者令人反感的输出。”</p><p>&nbsp;</p><p>OpenELM只是苹果公司发布的一系列令人惊讶的开源AI模型中的最新一批。去年10月，苹果方面曾悄然发布具有多模态功能的开源语言模型Ferret，迅速引起各界关注。</p><p>&nbsp;</p><p>目前，大模型领域主要分为开源和闭源两大阵营。闭源阵营的代表企业包括 OpenAI、Anthropic、谷歌、Midjourney、Udio、百度、科大讯飞、出门问问、月之暗面等。开源阵营的代表企业包括 Meta、微软、谷歌、百川智能、阿里巴巴、零一万物等。这些企业致力于开放大模型的技术和代码，鼓励开发者和研究人员参与模型的开发和改进。</p><p>&nbsp;</p><p>苹果长期以来一直以神秘莫测、对外“封闭”而闻名，本次却罕见地加入开源大模型阵营。以前，除了在网上发布模型和论文之外，苹果并未公开宣布或者讨论其在AI领域的探索。</p><p>&nbsp;</p><p></p><h2>关于OpenELM，我们了解什么？</h2><p></p><p>&nbsp;</p><p>尽管OpenELM（全称为开源高效语言模型）才刚刚发布、尚未进行过公开测试，但苹果在Hugging Face上指出其目标是在设备端运行这些模型。这明显是在紧跟竞争对手谷歌、三星和微软的脚步——微软本周刚刚发布了能够纯在智能手机端运行的Phi-3 Mini模型。</p><p>&nbsp;</p><p>在arXiv.org上发表的一篇模型阐述论文中，苹果表示OpenELM的开发“由Sachin Mehta领导，Mohammad Rastegrai与Peter Zatloukal则额外做出贡献”，该模型家族“旨在增强并赋能开放研究社区，促进未来的研究工作。”</p><p>&nbsp;</p><p>苹果的OpenELM模型分为四种规模，分别拥有2.7亿、4.5亿、11亿与30亿参数，各模型均比现有高性能模型更小（通常为70亿参数）且各自拥有预训练与指令微调两个版本。</p><p>&nbsp;</p><p>这些模型的预训练采用来自Reddit、维基百科、arXiv.org等网站总计1.8万亿tokens的公共数据集。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/217c59fecc6d7b2ba75804d0f1a9ceb4.jpeg" /></p><p></p><p>&nbsp;</p><p>OpenELM模型适合在商用笔记本电脑甚至部分智能手机上运行。苹果在论文中指出，他们分别在“配备英特尔i9-13900KF CPU、64 GB DDR5-4000 DRAM和24 GB VRAM的英伟达RTX 4090 GPU，运行有Ubuntu 22.04的工作站上”、以及“配备M2 Max系统芯片与64 GiB RAM、运行有macOS 14.4.1的苹果MacBook Pro上”运行了基准测试。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/96701f0f43a13c0524f29c379edebd28.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf09a8d4f00fe35cb072cadbb35bdac6.jpeg" /></p><p></p><p>网友测试运行OpenELM模型</p><p>&nbsp;</p><p>有趣的是，新家族中的所有模型均采用分层缩放策略来分配Transformer模型中每一层内的参数。</p><p>&nbsp;</p><p>据苹果公司介绍，这种方式能够提供更加准确的结果，同时提高计算效率。该公司还使用新的CoreNet库对模型进行了预训练。</p><p>&nbsp;</p><p>该公司在Hugging Face上提到，“我们的预训练数据集包含RefinedWeb、去重版PILE、RedPajama的一个子集以及Dolma v1.6的一个子集，总规模约1.8万亿个tokens。”</p><p>&nbsp;</p><p></p><h3>值得肯定，但性能并非顶尖</h3><p></p><p>&nbsp;</p><p>在性能方面，苹果公布的结果显示OpenELM模型相当出色，特别是其中的4.5亿参数版本。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/9451865f6ff0e0aed82c5db5eeff2f05.png" /></p><p></p><p>&nbsp;</p><p>此外，11亿参数的OpenELM版本“比拥有12亿参数的OLMo模型性能提高了2.36%，且需要的预训练tokens仅为后者的二分之一。”OLMo是艾伦AI研究所（AI2）最近发布的“真正开源且最先进的大语言模型”。</p><p>&nbsp;</p><p>而在强调测试知识与推理技能的ARC-C基准测试中，经过预训练的OpenELM-3B版本的准确率达到42.24%，同时在MMLU与HellaSwag上分别得到26.76%与73.28%的成绩。</p><p>&nbsp;</p><p>一位参与该模型系列测试的用户指出，苹果的模型成果似乎“稳定且性能一致”，就是说其响应结果并不具备灵活的创造力，也不太可能冒险涉及“不适合上班时浏览”的内容。</p><p>&nbsp;</p><p>竞争对手微软近期推出的Phi-3 Mini拥有38亿参数及4k上下文长度，目前在性能层面仍处于领域地位。</p><p>&nbsp;</p><p>根据最新发布的统计数据，Phi-3 Mini在10-shot ARC-C基准测试中得分为84.9%，在5-shot MMLU上得分为68.8%，在5-shot Hellaswag上得分为76.7%。</p><p>&nbsp;</p><p>但从长远来看，OpenELM肯定还会继续得到改进。目前开源大模型社区对于苹果的加入非常兴奋，也期待看到这位“闭源”巨头如何将其成果引入于各类应用场景。</p><p>&nbsp;</p><p></p><h2>大模型是智能手机的未来</h2><p></p><p>&nbsp;</p><p>手机厂商们都很看好手机上的AI前景。</p><p>&nbsp;</p><p>高通和联发科等公司已推出了智能手机芯片组，可满足人工智能应用所需的处理能力。此前，许多设备上的AI应用实际上是在云端进行部分处理，然后下载到手机上。但云端模型也存在弊端，如推理成本很高，一些 AI 创业公司训练+生成一张图片的成本可能就要一元。而先进的芯片和端侧模型则会推动更多AI应用程序在手机端运行，节省成本的同时，也能给用户带来更好的实时计算能力，从而催生出新的商业模式。</p><p>&nbsp;</p><p>从ChatGPT火爆至今不过一年左右，手机厂商就都已将AI大模型技术落地在自家手机中。</p><p>&nbsp;</p><p>今年三星新发布的 Galaxy S24 系列上搭载了能处理语音、文本、图像的端侧 Galaxy AI。谷歌也发布了一款搭载自家 AI 模型的手机 Pixel 8 系列，该设备搭载了 Gemini Nano。谷歌 Pixel 部门产品管理副总裁 Brian Rakowski 还表示谷歌最先进的大模型也会于明年直接登陆智能手机，“我们在压缩这些模型方面已经取得了相当多的突破。”&nbsp;</p><p>&nbsp;</p><p>国内头部手机厂商也争相布局。小米于去年10月发布了澎湃OS以及小米自研大模型加持的各类应用；vivo 也去年宣布推出了蓝心大模型，并开源了面向手机打造的端云两用大模型 BlueLM-7B；OPPO 也在去年11月发布了安第斯大模型(AndesGPT)，以“端云协同”为基础架构设计思路，推出了多种不同参数规模的模型规格。</p><p>&nbsp;</p><p>今年世界移动通信大会MWC 的一大亮点也是大模型能够在设备本身上本地运行，“这就是最具颠覆性的地方。”&nbsp;CCS Insight 首席分析师 Ben Wood&nbsp;感叹。在这次大会上，还展示了一些未来AI概念手机，比如德国电信和 Brain.ai 完全放弃App而采用 AI 界面的T phone。因此，也有预测认为，随着AI占领我们的智能手机，App时代的终结可能指日可待，<a href="https://mp.weixin.qq.com/s/g07VkmGl3NKvVvpMMAYV-A">从而带来全新的生态和竞争格局。</a>"</p><p>&nbsp;</p><p>手机大模型之战，此前只差苹果，而现在，苹果终于带着它的开源大模型来了。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/">https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/</a>"</p><p><a href="https://www.infoq.cn/article/h2ceezfmjdbo2epareyh">https://www.infoq.cn/article/h2ceezfmjdbo2epareyh</a>"</p><p><a href="https://arxiv.org/abs/2404.14619v1">https://arxiv.org/abs/2404.14619v1</a>"</p><p><a href="https://twitter.com/atropos/status/1783349174702059742">https://twitter.com/atropos/status/1783349174702059742</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/plzCk9c4i5dAj9orBYRy</id>
            <title>我们用机器学习模型协助打击不法行为</title>
            <link>https://www.infoq.cn/article/plzCk9c4i5dAj9orBYRy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/plzCk9c4i5dAj9orBYRy</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 06:59:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 深度伪造, 虚拟币犯罪, 技术团队
<br>
<br>
总结: 2023年基于AI的深度伪造欺诈案件暴增3000%，而涉虚拟币犯罪案件近年虽然在数量上有所减少，但涉案金额却是陡增。技术本无罪，但不当的使用会给个人、家庭、甚至社会带来严重后果。国内有一支协助破案的技术团队，利用手中的技术能力去协助执法部门破获不法案件，与执法部门一起深入具体案例，实地分析。中科链源CDO唐崇麟、数据科学负责人连晓磊在InfoQ《极客有约》中介绍了他们如何利用“技术武器”帮助执法部门维护数字世界安全。 </div>
                        <hr>
                    
                    <p>2023年基于AI的深度伪造欺诈案件暴增3000%，而涉虚拟币犯罪案件近年虽然在数量上有所减少，但涉案金额却是陡增。技术本无罪，但不当的使用会给个人、家庭、甚至社会带来严重后果。</p><p>&nbsp;</p><p>国内有这样一支协助破案的技术团队，他们利用手中的技术能力去协助执法部门破获不法案件，与执法部门一起深入具体案例，实地分析。那么，这样的团队是如何诞生的？他们具体又做了哪些事情？</p><p>&nbsp;</p><p>中科链源CDO唐崇麟、数据科学负责人连晓磊作客InfoQ《极客有约》，详细介绍了他们如何利用“技术武器”帮助执法部门维护数字世界安全。以下文字根据直播内容整理。</p><p>&nbsp;</p><p></p><p></p><p>InfoQ：当时为什么会想要去做AI区块链安全赛道？</p><p>&nbsp;</p><p>唐崇麟：我非常相信区块链技术会带来重大变革。区块链是2008年在金融危机的大背景下提出的一个理念，并在后续形成了比特币链的技术，这个理念是出于对中心化、私立金融机构的不信任，想要打造一个去中心化、相信算法算力的机制。我非常认可这个理念。</p><p>&nbsp;</p><p>但可惜的是，相较移动互联网十几年发展，区块链技术诞生15年后还处于小众和边缘状态。区块链技术不能普及的原因，首先是缺乏能给用户带来真实收益和价值的killer APP或功能，这个大家都在努力解决。但更重要的是，区块链的安全问题一直没有得到解决。钱包、交易所被盗造成的产品损失并不鲜见，犯罪分子也常利用比特币、以太坊、USDT作案，这些让人总觉得这个新生事物常和网络犯罪融合在一起，导致其被妖魔化。</p><p>&nbsp;</p><p>安全问题得不到解决，会极大地阻碍整体区块链技术的普及。在这样的背景下，我们公司团队先前在大数据和AI机器学习算法方面的积累非常适合解决区块链的安全问题，可以说是找到了一个用已有的知识沉淀优势在新领域赛道发展的好机会，再加上前景广阔，于是我们就开始做这项工作了。</p><p></p><h3>为什么网络违法案件难破获？</h3><p></p><p>&nbsp;</p><p>InfoQ：最近AI诈骗开始盛行，执法部门需要进行资金追溯。而先前Web3行业涉虚拟币的不法行为也有很多，这类违法案例目前趋势如何？</p><p>&nbsp;</p><p>连晓磊：&nbsp;AI的盛行确实让AI诈骗增多，而涉虚拟币犯罪行为也一直是执法部门关注的重点。关于这两种违法案件的数量和趋势，可以从以下几个方面来看：</p><p>&nbsp;</p><p>首先，据相关数据统计显示，2023年基于AI的深度伪造（deepfake）欺诈案件暴增3000%，基于AI的钓鱼邮件增长了1000%，已有多个有国家背景的APT组织（高级持续性威胁，是一种针对性、隐蔽性、持续性都极强的网络攻击手段）利用AI实施了十余起网络攻击事件。</p><p>&nbsp;</p><p>随着AI技术的快速发展，不法分子利用AI技术进行诈骗的手法也日益翻新。他们通过深度伪造、语音合成等技术，制作逼真的视频通话或语音信息，诱使受害者上当受骗。</p><p>&nbsp;</p><p>其次，Web3行业涉虚拟币的不法行为也呈现出增长态势。他们利用虚拟币的匿名性和去中心化特点，进行洗钱、贩毒、非法集资等不法活动。据SAFEIS安全研究院统计，2023年涉虚拟货币案件总量428件，较2022年减少88.9%，但整体涉案金额陡增至4307.19亿元，约是22年的12.36倍。可见，涉虚拟币犯罪案件在数量上呈现减少趋势，但是整体涉案金额是陡增的态势。</p><p>&nbsp;</p><p>总之，AI诈骗和Web3行业涉虚拟币的不法行为是当前和未来一段时间内，执法部门需要重点关注和打击的对象。我们需要加强技术研究和监管力度，以应对这些新的挑战。</p><p>&nbsp;</p><p>InfoQ：为什么虚拟币会成为黑灰产温床？</p><p>&nbsp;</p><p>唐崇麟：传统Web2领域的黑灰产都是通过银行转账进行，需要身份验证，追查相对容易些，再加上很多银行都有反欺诈、反洗钱的基础设施模型，黑灰产难以在其中生存。但在Web3的区块链领域则给大家打开了一个完全崭新的世界，其中没有什么监管，人人都是匿名，交易成本低但效率又很高，是黑灰产天然非常喜欢的环境。</p><p>&nbsp;</p><p>InfoQ：破解这种网络上不法案件都有哪些难点？技术能力如何帮助破获相关案件？</p><p>&nbsp;</p><p>唐崇麟：网上违法案件的破获难点整体来说有三大部分。</p><p>&nbsp;</p><p>首先是法律流程方面。对现有公安和法律体系来说，涉虚拟币的网络犯罪是一个新的领域，取证工作、适用法条、立法工作以及案件侦破后的流程和审判等，很多环节之前从没遇到过的。在这方面，技术能够解决的其实并不多。我们更为看重的是另外两大部分。</p><p>&nbsp;</p><p>其一，由于这类案件涉及的数据非常庞大且复杂，链上钱包生成免费且非常便捷，甚至有工具可以批量生成，因此犯罪分子会为了隐藏自己的资金和踪迹，批量生成钱包，并通过这些钱包互相转移资金。这种方式效率很高，手续费也很低，相比传统银行金融体系的资金转移会便捷很多，从而形成了一个巨大复杂的交易结构网，侦察工作难度很高。</p><p>&nbsp;</p><p>其二，来自区块链天然的匿名性。先前提到的钱包生成时不需要任何身份证明，所以我们即使找到了非常实锤的涉案地址，也是不知道背后的真实人员，所以我们需要继续侦破，直到发现涉案的资金在某些环节被兑换成法定货币、进入银行体系，我们才能得知背后的真实人员。</p><p>&nbsp;</p><p>但也正因为整体数据量庞大，且需要非常精准地查找线索，也为机器学习、大数据的技术应用提供了非常好的土壤。这也是我们的优势所在：利用大数据和机器学习对复杂交易网络进行分析识别并提升精准度。</p><p>&nbsp;</p><p>InfoQ：那你们和执法部门如何合作完成一次打击行动？能否分享一些案例？</p><p>&nbsp;</p><p>连晓磊：涉虚拟币犯罪上游的犯罪类型有网络赌博、诈骗、传销、洗钱、黑灰产等，这些都是我们帮助执法机关重点打击的犯罪类型。我们可以为执法机关提供从数据挖掘、立案、侦察、收网、司法鉴定、起诉、审判、资产处置一站式全流程服务。</p><p>&nbsp;</p><p>今年一月份，青岛市公安局和国家外汇管理局青岛市分局联合破获了一起特大地下钱庄案，该案件入选公安部经侦局，列为全国公安经侦系统“夏季行动”打击地下钱庄“十大战役”。这起地下钱庄案件涉案金额高达158亿元，抓获犯罪嫌疑人74人，涉及全国17个省及直辖市。</p><p>&nbsp;</p><p>其中，我们自主研发的SAFEIS安士系统在涉虚拟币犯罪固证环节发挥了关键作用，通过对涉案地址的追踪分析，验证了虚拟货币交易信息与银行账户资金流向的线索相互吻合，为侦查人员提供了强大的技术支持。</p><p>&nbsp;</p><p>在该案件的资金往来分析过程中，办案人员发现犯罪嫌疑人之间的资金流存在异常，有涉嫌虚拟币交易的特征。随着对犯罪嫌疑人所涉及的银行账户资金流向进行了深入研判，我们发现资金交易模式与虚拟货币买卖行为相符。最终通过SAFEIS成功挖掘到犯罪嫌疑人的作案证据。侦查人员还通过SAFEIS输出的资金流向层级，对整体资金和虚拟货币犯罪团伙的群组关联结构进行了详尽刻画，进一步明确了嫌疑人参与虚拟币犯罪的相关信息。</p><p>&nbsp;</p><p>InfoQ：为什么银行或支付宝这种金融机构不给资金上链，让它们可以被追溯？</p><p>&nbsp;</p><p>唐崇麟：我们所说的区块链是指公链，比如比特币链、以太坊链、波场链、币安智能链等。公链秉承的是抗审核、匿名性和去中心化原则，这是和现实中金融机构所形成的银行网络完全隔离的两个世界。</p><p>&nbsp;</p><p>如果一个金融机构想要将用户的资金上链，就需要将用户存放的法定货币一对一转换成链上的USDC稳定币或其他代币后，再进行上链。这样一来，银行机构就行使了交易所的职责，但我国法律是禁止银行机构进行虚拟货币经营的，这种业务是违法的。</p><p>&nbsp;</p><p>我国现在推行的电子人民币是将人民币变成电子货币，并在可监管的环境中更方便的使用，这种电子货币和公链的区块链是不同的。</p><p>&nbsp;</p><p>InfoQ：Web3时代的网络安全跟Web2时代的安全问题有什么差别？</p><p>&nbsp;</p><p>唐崇麟：Web3时代的交易网络和数据量更为复杂，整体安全问题的追踪难度更大。但我觉得应该换一个角度看。</p><p>&nbsp;</p><p>Web3和Web2时代安全问题有很大的差别。Web3的本质是利用去中心化的区块链技术，要求App开发商和钱包用户都对自己的行为承担更大的责任。而Web2时代的滴滴、字节和各种银行等都是平台中心化的机构。本质上，做平台就要承担平台上安全的责任，比如银行存款被盗是能得到赔付的，但Web3时代是没有的，钱包密钥自己保管，如果被钓鱼或者胡乱签名导致资金被盗，那么你找不到一个中心化机构寻求赔付，甚至现实世界中都没有法律能够保护这些资产，这是最大的问题。</p><p>&nbsp;</p><p>也就是说，为什么Web3用户会胆战心惊，因为既没有机构能帮忙主持公道，很多技术又是全新的，里面的漏洞没人能说得清，大家都是摸着石头过河。比如，Web3体验大多是基于智能合约进行交互，但智能合约也是在近几年才出现，其中的逻辑漏洞大多都没有暴露过，有非常多的不法分子会监控每一个新上线的智能合约，特别是金融类合约，一旦上线便蜂拥而至寻找其中的漏洞，并在找到后的第一时间利用它窃走资金。</p><p></p><h3>特别的技术团队</h3><p></p><p>&nbsp;</p><p>InfoQ：像中科链源这种是针对违法案件分析和研发的，你们的团队和普通的互联网研发团队有什么不同？你们内部团队之间如何协作？</p><p>&nbsp;</p><p>唐崇麟：我之前曾在滴滴负责过网约车整体策略平台的搭建，在我看来，中科链源现在的工作与传统互联网平台中的数据产品团队有很强的相似性，但也存在一些不同。</p><p>&nbsp;</p><p>首先从数据来源看，一般互联网平台公司的用户行为等所有数据，都是通过自己APP产生的，公司的大数据团队对这些数据了解非常深。但中科链源在做分析时，所有数据都是在公链上产生的，不受我们控制，而且每一条链上的数据存储特性都不相同，要找到数据后才能进行下一步的处理和分析。</p><p>&nbsp;</p><p>其次是，案件的分析非常主观且依赖办案人员的经验，很多分析结果也只是猜测，很难对其真假与否进行快速验证并打上正确标签，这就要求我们和整体的办案团队有非常深入的融合。</p><p>&nbsp;</p><p>在我们初期做模型时，要求大家把自己当作是分析师，一起深入某个案件、实地分析。我们核心团队的几个同学在入职后都是做了半年以上的分析师工作，再开始做模型的，他们对业务的要求和理解非常深。</p><p>&nbsp;</p><p>最后是对模型的选择。也提到取得量化的标签，我们会倾向于一些图计算的模型和无监督的聚类模型，我们也更强调从基础层面来建设特征的标签。</p><p>InfoQ：你们现在的AI底层支持系统都包括哪些？这个底层系统具体是如何搭建并帮助你们做案件侦破的？</p><p>&nbsp;</p><p>连晓磊：我们的AI系统有三个部分：数据平台（数据特征模块）、模型训练平台（负责训练模型），以及模型服务平台（部署G端应用的对接服务和一些自己微服务）。这三个平台会应用于数据的探查阶段、模型的训练研究阶段，以及模型的部署使用阶段。</p><p>&nbsp;</p><p>数据特征平台方面，算法领域的数据是非常重要的因素，我们将各个链上的数据接入到了我们的大数据平台上，再进行各种数据的清洗和数据仓库的特征提取，从而保证数据的高可用性，另外还使用离线+实时数据来保证时效性。</p><p>&nbsp;</p><p>数据训练平台方面，我们用来研发较大的神经网络模型和时序模型所需要的分布式计算资源及GPU资源，都会用到这个平台。</p><p>&nbsp;</p><p>模型部署平台则是负责模型的部署和版本控制的平台，版本控制可以在模型预测错误时依然保证预测的时效性。此外，我们还有监控系统，在模型指标等不达标时，及时对模型进行更新，从而保证模型的效果。</p><p>&nbsp;</p><p>InfoQ：期间有经历什么比较印象深刻的事件吗？</p><p>&nbsp;</p><p>连晓磊：说起印象深刻的事，就是有一次给大家做图模型的理论分享，完事后有个小伙就拿手里现有的案件数据去练手，结果发现一些涉案地址和实锤的入金归集地址在一个类簇里面，于是催生了现在上线的图聚类服务。</p><p></p><h3>如何用技术协助破案</h3><p></p><p>&nbsp;</p><p>InfoQ：2020年公司成立至今，你们在机器学习模型上做了哪些大的迭代更新吗？</p><p>&nbsp;</p><p>唐崇麟：虚拟币行业较新，数据量也很大，因此我们在模型探索过程中没有像传统Web2公司一样直接进行标注和模型训练，而是先进行了一定的探索。这些探索大体可以分为三个阶段。</p><p>&nbsp;</p><p>第一阶段，借鉴人的经验做自动化，换句话说是首先提升人的效率。在这一阶段，我们先把复杂的交易关系用图计算网络再现成网状结构，再将网状结构中的交易信息，与办案人员经验提取出的特征和规则相结合，形成未经过太多训练的简单规则模型，其中大概有近百个带权重的特征。我们对两个地址点之间的关系做出统计行的概率预测，概率越高地址之间的关联性就越强。这样能帮助办案人员完成很多工作，因为他们平时办案中最主要的工作就是从一个较为实锤的涉案地址中，找到其他上下游强关联的涉案地址，这个工具能帮助他们很好完成任务。</p><p>&nbsp;</p><p>有多年办案经验的分析师平时在做分析时只能思考三至五个特征，逐个排查时非常耗时。但在有了模型之后，近百个特征可以同时计算，下游网络中数十万的地址也能同时进行计算，几周的工作可以在几小时内完成，极大地提升了人的工作效率。</p><p>&nbsp;</p><p>第二阶段，我们利用了更为复杂的无监督聚类模型和谷歌的PageRank算法，基于整体交易网络中的交易行为特征，找出交易网络中行为较为相似、犯罪链条中功能较为相似的地址，用类似社交网络分析的形式，找到嫌疑人之间的相似关系。这样协助办案人员梳理整体犯罪链路的每个环节和地址，方便日后取证。</p><p>&nbsp;</p><p>第三阶段，虽然已经借助前面提到的两大模型大幅提升了效率，但在很多基础的具体特征标签上，我们的建设还是较为稀缺。因此在这个阶段，我们化整为零，逐一寻找极具特色的地址并打上标签。这样一来，办案人员看整体网图时就能得知其中地址的详情和特征，我们后续迭代模型或搭建新模型时也能利用这些特征，从而达到一举两得的功效。我们相信，未来地址特征标签体系能和更多的模型一起，丰富我们的整体工具箱并提升办案效率。</p><p>&nbsp;</p><p>InfoQ：具体常用的特征有哪些呢？</p><p>&nbsp;</p><p>唐崇麟：常用特征主要是交易相关，比如时间属性（高低频次、周期性等）、金额属性（大额或小额、是否是定时周转固定金额等）、交易行为特征（交易对象是否频繁、交易关系是否复杂等）。</p><p>&nbsp;</p><p>连晓磊：具体来说，时间上我们采用的特征会包括地址、base属性、创建时间、地址对的最大最小交易时间；金额上有地址对之间的汇总交易金额、最大最小交易金额均值标准差等统计特征；至于交易关系，特别是在波场链上，我们利用TRX激活关系的特征捕捉下游涉案金额的转入概率。</p><p>&nbsp;</p><p>InfoQ：是基于马尔科夫模型这些来做的吗？</p><p>连晓磊：并不完全是。因为要构建马尔科夫链需要高质量的观测数据，并且对Label也有一定的敏感性。在交易所官方选择对地址发起调证的时候，这其中是有一些人为主观的判断因素，选择调证的地址也不一定是正样本，所以我们并没有完全使用马尔科夫模型进行训练，但在下游的分析思路上借鉴了马尔科夫思想，为每条链路上的条件概率人为制定具体分数。</p><p>&nbsp;</p><p>注：马尔可夫模型是一种统计模型，它基于马尔可夫性质，即一个给定过程的未来状态仅取决于当前状态，与之前的状态无关。马尔可夫模型可以应用在多个领域，如自然语言处理、算术编码等。</p><p>&nbsp;</p><p>InfoQ：可以展开讲讲，图聚类模型在产品里的应用吗？</p><p>&nbsp;</p><p>连晓磊：图聚类模型其实基于一个确定的地址，比如案件中的路径归集地址，利用大数据平台向下开展更多的节点，从而形成一个网图数据，这样就可以用图聚类的方法圈选出一些涉案概率更大的地址。简单来说，其实是借鉴Facebook或者Twitter这些社交媒体的思想，对用户进行聚类和兴趣社群的挖掘，同一个类簇可以理解为是在同一个兴趣圈子。</p><p>&nbsp;</p><p>我们也是以交易为边进行兴趣爱好模式的挖掘，同一个聚类里的地址可能联系更为紧密、交互更为频繁。除了圈选出类簇，我们还会在类簇的基础上对节点进行中心度计算，提取出一个团伙中更为重要或关键的核心地址。</p><p>&nbsp;</p><p>我们后续在研究角色相关的挖掘或role-based embedding模型的角色建模时，也会用到图聚类，比如同属跑分车队的两个地址可能之间没有联系，但在某些交易行为或模式上具备相似性，那我们就认为这两个地址较为相似。</p><p>&nbsp;</p><p>InfoQ：图聚类模型一般应用在什么场景比较多？它的特点是什么？</p><p>&nbsp;</p><p>连晓磊：图聚类最多的应用场景其实是社交网络的挖掘，筛选出相同兴趣爱好的用户；其他也有生物医学方面的应用，比如药物关联度的挖掘，将老药混合构建出新药并用于抵抗当前的某种疾病，这种其实也是以药物为节点做向量化embedding和图聚类，圈选出哪些药物的某种特性下在某些生物反应上会有相同的模式；金融行业的下游分析、犯罪团伙分析或金融上交易模式的研究，都可以有所应用；交通类则可以联合时序分析和图聚类，对交通生活进行类似的挖掘探索。</p><p>&nbsp;</p><p>唐崇麟：我之前在字节时接触到的视频平台会有非常多的图聚类模型应用，主要用来扩展用户的视频观看、探索用户的边界，比如喜欢看vlog或美食类节目视频的用户就可能会通过评论或视频的关系聚类到一起。平台希望用户能扩展更多的兴趣领域，因此通过图聚类他们可能会被归纳到的其他兴趣圈层，这样对他们进行相关的推送就有可能扩展他们的观看领域。</p><p>&nbsp;</p><p>我们也在探索图聚类的其他应用，比如对全网涉案地址进行聚类后，有些涉案地址可能会和我们之前没有标注过的地址非常接近，那么这些新的地址也会帮助我们找到案件的新线索。</p><p>&nbsp;</p><p>此外，很多技术领域也有对图聚类模型的应用。比如先前提到的问答模型、ChatGLM检索，在将文本的语义向量存入到向量数据库后再进行聚类，每个类簇都包含许多文本向量和一个中心点作为索引，从而构建了一个基于向量数据库的倒排索引，从而实现速度的提升。在收到用户问题后，我们先将问题向量化，再和各个类簇的中心向量做相似度比较，相似度高的则归纳仅类簇后再和其他类簇的向量进行相似度匹配。</p><p>&nbsp;</p><p>InfoQ：据了解，模型数据来自中科链源自建的以太坊、币安智能链和波场链全节点和一些第三方数据，为什么选择这三条链？具体如何收集和处理这些数据的？</p><p>&nbsp;</p><p>唐崇麟：这三条链的选择其实与犯罪行为密切相关。波场链已逐渐演变为网络涉币犯罪中最为核心的一条链，其上的USDT交易效率高、转账手续费低廉，是犯罪分子的首选。以太坊中的DX应用极多、用户广泛，也是非常好的一个选择。币安智能链的合约部署丰富、技术成熟，在华人圈的普及率也很高，也是犯罪分子常用的一条链。</p><p>&nbsp;</p><p>搭建了三条链的全节点后，接下来要做的就是数据的收集和处理。在整体的大数据架构中，我们需要兼顾两个方向。</p><p>&nbsp;</p><p>一是实时分析：用户希望链上数据能有秒级别的更新，为此我们利用StartBox实时数据库组件、Kafka及Spark等大数据组件搭建了一套实时系统，其中存储了大量raw data供模型运算和特征计算；</p><p>&nbsp;</p><p>二是离线分析：我们的实时数据库和离线数据库中间通过任务相连接，从而构成一个高级的Lambda实时和离线数据库架构。至于数据的清理和处理，我们也搭建了完善的全流程全生命周期监控体系，确保数据完整准确、实时高效。</p><p>&nbsp;</p><p>有了数据，我们可以通过产品将整体交易网络呈现给用户，允许用户直接通过网图对可疑涉案地址进行查找分析。此外，我们也通过模型为用户提供助力，比如模型可以在一小时内遍历一个犯罪起始点下游的数十万地址点，并将最为可能的十个点标记在图上供用户查阅，从而极大地提升了效率。</p><p>&nbsp;</p><p>InfoQ：资料显示，中科链源在自动查找目标地址方面，原来要用两周的时间现在缩短到了20分钟，这期间主要做了哪些改进？</p><p>&nbsp;</p><p>连晓磊：案件处理的提速主要依赖于机器性能和模型能力的提升。</p><p>&nbsp;</p><p>人为的地址分析无法遍历全量的下游节点，无法保证召回率。人为地址分析的速率也不如机器，机器可以基于自动化逻辑进行海量地址的概率和分数计。此外，模型的能力也对案件处理提速有一定优势，我们公司内的分析师团队学习优秀的分析经验，构建出相对较好的模型，再加上机器的快速计算能力，从而得到更好的预测结果。</p><p></p><h3>加入大模型能力</h3><p></p><p></p><p>InfoQ：中科链源之前在<a href="https://www.infoq.cn/article/lq2fJ7gm9iyuOLkaLdqv">InfoQ分享</a>"过选择了智谱的ChatGLM-6B模型，那当时你们对大模型有什么选择标准，最后又为什么选择ChatGLM-6B？</p><p>&nbsp;</p><p>连晓磊：这一个问题其实可以分为两个方面来回答，一是模型的选择，二是工具的选择。</p><p>&nbsp;</p><p>首先是模型的选择。我们在体验了一些法律领域的大模型，如LawGPT、ChatLaw等后，总体感觉和通用的ChatGLM差不多，即使ChatLaw在交易方面相对更好，也因为其没有开源的预训练参数，导致我们无法直接使用。LawGPT在法律方面的问题回答表现不错，但其检索问答的能力是借助的RAG（检索增强式问题生成），其效果不如ChatGLM通用大模型。</p><p>&nbsp;</p><p>此外，当时虽然也有ChatGLM的二代模型，但其架构上大体没有变化，只是支持更快的推理速度和更长的上下文，再加上当时对一代的模型应用更多，我们最终选择直接部署一代ChatGLM。当然，我们后续也陆续更新到了目前最新的模型ChatGLM 3。</p><p>&nbsp;</p><p>再说工具的选择。我们考虑过LangChain、LlamaIndex，以及国内的FastGPT，但这些工具的集成度相对较高，且没有集成向量数据库功能，因此我们直接自己搭建了一套框架，在RAG中了添加数据清洗、判例文档核心句子提取、意图识别等自定义环节，也搭建了用于检索的向量数据库。</p><p>&nbsp;</p><p>InfoQ：实际上在研发阶段，中科链源使用了垂直行业的ChatLaw-Text2Vec模型，这两种模型如何分工？基座模型（ChatGLM-6B）和垂直模型混合应用会得到更好的效果吗？</p><p>&nbsp;</p><p>连晓磊：从模型分工来说，ChatLaw-Text2Vec和ChatGLM在架构中是串行进行的；ChatLaw-Text2Vec负责检索，将用户输入的问题转变为向量，再从向量数据库中检索出和用户问题语义相似度较高的法律判例文档，最后将问题和检索到的文档一同作为instruction输入到ChatGLM中，从而实现问题问答，类似于将原先的开放式大题变成了选择题或阅读理解题目。</p><p>&nbsp;</p><p>再说基座模型和垂直模型的混合应用，ChatLaw-Text2Vec是专门针对法律问答方向的向量化embedding模型，使用了90多万条高质量中文法律问答的句子对作为训练数据，天然适合法律领域的问答语义相似度计算。</p><p>&nbsp;</p><p>我们也尝试过其他如Text2Vec模型，但在法律场景中的表现均逊于ChatLaw-Text2Vec，因此，我们最终的检索模型使用了垂直领域的Text2Vec，但问答还是采用通用的ChatGLM。</p><p>&nbsp;</p><p>InfoQ：中科链源推出的to G的SAFEIS安士信息作战系统是怎么做架构搭建的？to G系统的研发与to B\ to C的软件系统研发有什么不同？</p><p>&nbsp;</p><p>唐崇麟：前面提到主要模块其实就是to G的安士信息作战系统，是专为执法机构提供的、打击涉虚拟币犯罪的一站式综合查控平台，平台涵盖资金分析追踪、地址监控、智能分析研判等核心功能。基于此，产品的架构搭建一方面要利用图计算模型、图聚类模型、OLAP型数据库处理分析海量的链上数据，另一方面要利用网状图、树状图系统对分析结果做图形化的呈现。</p><p>&nbsp;</p><p>to G系统的研发与to B \ to C的软件系统研发的不同之处，主要是因为G端、B端、C端使用场景存在着非常大的差异。以我们公司的业务举例，为了保障数据安全，我们经常要将产品部署到公安的内网中，这就要求SAFEIS安士要有私有化部署的产研经验，让我们的产品更符合公安的使用标准要求。</p><p>&nbsp;</p><p>InfoQ：AI技术在产品中的应用方面，中科链源团队还有哪些在筹备中或者是规划中的技术呈现吗？</p><p>&nbsp;</p><p>唐崇麟：我们的工作不是为了使用最前沿的技术，而是要切合业务需要选择技术，比如要如何更快地发现更多线索，如何在新犯罪团伙或犯罪行为出现时尽快发现。基于这样的需求，我们的发展有三大方向：</p><p>&nbsp;</p><p>利用图神经网络：基于我们积累的涉案地址进行发散，探索网络中是否存在新的异常交易行为，有的放矢地查看是否存在新犯罪团伙或新案件线索；利用图聚类技术：找到新的涉案嫌疑人地址和相关线索；利用生成式AI：对智能合约源码进行分析，传销诈骗类智能合约可能存在规律，那么直接监控新部署智能合约并利用生成式AI进行文本分析，可能带我们找到涉案的可疑合约。</p><p>&nbsp;</p><p>InfoQ：最后，要不要给大家一些防诈骗小技巧？</p><p>&nbsp;</p><p>连晓磊：技术手段在预防不法行为方面发挥着很重要的作用，对敏感数据的加密可以预防未经授权访问导致的数据泄露，利用机器学习技术和上面提到的各种模型进行数据分析和异常模式识别，可以预测一些潜在的不法行为，为执法部门提供相对有价值的线索。</p><p>&nbsp;</p><p>但最核心的还是提高个人的防范意识，谨慎对待可疑电话短信，不要轻易泄露个人信息或进行转账操作，保护个人信息不泄露身份证和银行账户等数据信息。最后就是要警惕电信诈骗手段，防范不法分子利用DeepFake、语音视频合成等较为先进手段，冒充公检法机关、亲友急需资金等骗局。</p><p>&nbsp;</p><p>唐崇麟：现在虽然也有很多工具提供智能合约的安全检测，但个人来说预防诈骗还是要在心理上建立防线。在遇到任何暴富机会前，都要想想自己究竟是不是别人眼里的韭菜、要不要这么冲动；钱包密钥一定保管好，写在纸上不要放到网上，不要将密钥透露给任何人；既然钱包免费生成，那我们可以生成多个钱包、专款专用，主力钱包不要和可疑合约进行交互，额外生成一个专门用于和各种DMS进行交互的钱包，这样即使这个钱包不小心在授权后被清空，损失只是这一个钱包，风险隔离非常重要。</p><p>&nbsp;</p><p>此外，我们还要对区块链的授权、加密、签名等操作的原理有所理解，明白即使受骗后也没有银行可以去申诉索赔，这样应该也会对自身的安全意识有所增强。</p><p>&nbsp;</p><p>嘉宾介绍：</p><p>&nbsp;</p><p>唐崇麟，在移动互联网领域有十余年的工作经验，曾任职于Uber、滴滴负责运营、策略、算法领域的工作，同时搭建了网约车的策略分析平台，加入字节后也是负责算法、策略相关的工作。目前在中科链源，负责大数据和算法相关的工作。</p><p></p><p>连晓磊，曾在好未来和理想汽车工作，主要的研究方向是营销增长的模型以及自然语言处理相关的工作。目前在中科链源主要是负责区块链安全算法相关的研究工作。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OHyctDFEJ6RmVzed6sdj</id>
            <title>国内大模型五虎融资仅是巨头零花钱？谷歌、微软、Meta：每季度拿不出100亿美元别玩AI</title>
            <link>https://www.infoq.cn/article/OHyctDFEJ6RmVzed6sdj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OHyctDFEJ6RmVzed6sdj</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 06:49:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, 谷歌, 微软, 人工智能
<br>
<br>
总结: 三大科技巨头发布财报，Meta表示资本支出将增长，但投资者态度不佳；谷歌和微软股价上涨，Alphabet首次分红，AI技术发展增加运营成本。 </div>
                        <hr>
                    
                    <p>“猪突猛进”这么久，三大厂商交了“季考答卷”。</p><p>&nbsp;</p><p>这两天，Meta、谷歌、微软陆续发布了最新财报。三个大厂都表示今年的资本支出将增长：Meta全年上升至350亿到400亿美金、谷歌每个季度将花费约120亿美金或更多、微软最近一个季度的资本支出为140亿美金，预计将“大幅”增加。</p><p>&nbsp;</p><p>但是，投资者对三家的态度却非常不同：“Meta跌逾10%”上了热搜，而谷歌母公司Alphabet与微软的股价日前在盘后交易中上涨。到底发生了什么？</p><p>&nbsp;</p><p></p><h2>小扎：开场白都在说赔钱</h2><p></p><p>&nbsp;</p><p>一点也不意外，扎克伯格在Meta财报电话会议上谈到了人工智能。他花了很多时间谈论Llama 3和<a href="https://www.cnbc.com/2024/04/18/meta-ai-assistant-comes-to-whatsapp-instagram-facebook-and-messenger.html">最近推出</a>"的人工智能助手 Meta AI。然而，随后他便转向了元宇宙，开始兜售公司的耳机、眼镜和操作系统。他的开场白几乎全部都集中在Meta赔钱的许多方式上。</p><p>&nbsp;</p><p>Meta 98​​% 的收入来自数字广告，但扎克伯格谈及该话题还是在展望未来，以及公司如何将当前投资转化为广告收入。在讨论 Meta 打造“领先人工智能”的努力时，他表示，“有多种方法可以在这里建立庞大的业务，包括扩展业务消息传递、在人工智能交互中引入广告或付费内容。”</p><p>&nbsp;</p><p>这不是投资者想听的。当地时间周三，Meta股价在盘后交易中暴跌19%，市值蒸发逾2,000亿美元。</p><p>&nbsp;</p><p>扎克伯格似乎也已经做好了准备。“我们的股票在产品战略阶段出现了很大的波动，我们正在投资扩展一个新产品，但还没有货币化。”</p><p>&nbsp;</p><p>乍看下来，Meta 2024年第一季度的业绩表现不俗：收入同比增长27%至365亿美元；营业利润几乎翻倍，攀升91%至138亿美元；利润率也由去年的25%跃升至38%。Meta也不乏令分析师们保持乐观的统计数据：截至2024年3月31日，公司员工已减少10%、降至69329人。</p><p>&nbsp;</p><p>然而，Meta 预计，VR部门Reality Lab不仅第一季度运营亏损高达38亿美元，而且亏损态势还将持续。在整个2024年内，Meta 的资本支出也将由300至370亿美元增长至350亿至400亿美元。</p><p>&nbsp;</p><p>扎克伯格表示，“我们将继续加快基础设施投资，以支持我们的人工智能路线图。” Meta公司并没有发布2024年之后的预测，但提到“明年的资本支出将继续增加，因为我们将积极投资以支持公司雄心勃勃的AI研究与产品开发工作。”</p><p>&nbsp;</p><p>对投资者们的第二记重击来自分析师电话会议。扎克伯格在会上预测，还需要一段多年投资周期，Meta的AI业务才能发展为他所期望的“盈利服务”。</p><p>&nbsp;</p><p>Meta 首席执行官 Susan Li 补充称，公司需要开发先进的模型并扩展产品，然后才能带来有意义的收入。“虽然长期潜力巨大，但我们在回报曲线上还处于早期阶段。”Li 说。</p><p>&nbsp;</p><p>这显然也不是分析师们想听到的，但这也让市场终于意识到，AI领域还需要一段时间才能抵偿当初砸下的巨额融资。因此，该公司股价在盘后交易中遭受重创。当日收盘价为493.50美元，跌幅超过14%。</p><p>&nbsp;</p><p>科技行业的其他企业也都在AI身上砸下了重注，更是争先恐后围绕此议题展开炒作。例如，微软已经向OpenAI投入数十亿美元，并将其技术带入一系列自家产品，努力让客户相信其中的价值。</p><p>&nbsp;</p><p>因此，Meta对于AI技术现状的实诚态度（总结来讲，就是进展顺利，但还需要更多资金和时间才能在盈利层面获得实质性进展）很可能给华尔街乃至更多行业敲响警钟。</p><p>&nbsp;</p><p>或许有元宇宙的赌注未能如期获得回报的前车之鉴，此番股价下跌也可能透露出一条明确的信号：在让AI技术践行回报承诺方面，投资者们不希望等Meta太久。</p><p>&nbsp;</p><p></p><h2>谷歌、微软笑嘻嘻：涨了涨了</h2><p></p><p>&nbsp;</p><p>对比非常明显，Alphabet与微软的股价日前在盘后交易中上涨，投资市场明显对这两家热衷AI技术的企业那高于预期的季度收益表示满意：截至收盘，微软目前股价上涨4.3%，来到每股416.25美元；谷歌母公司股价上涨11.4%，达到每股176美元。</p><p>&nbsp;</p><p></p><h4>谷歌：首次分红，成本上涨因为有信心</h4><p></p><p>&nbsp;</p><p>Alphabet公布2024年第一季度营收为805亿美元，同比增长15%。净利润达到237亿美元，增幅为53%，均摊后每股收益为1.89美元。消息公布后，Alphabet股价一度上涨近15%。</p><p>&nbsp;</p><p>人们对于Alphabet股票的热情追捧，部分原因是其首次提出了0.20美元的季度分红。该笔分红将从2024年6月17日起面向A类、B类与C类股票支付。这家搜索巨头还公布了一项价值700亿美元的股票回购计划。微软本季度同样以回购及股息的形式向股东返还了84亿美元。这一行为也被网友调侃“偷师Meta”，Meta上季度推出回购分红后尝到股价大涨。</p><p>&nbsp;</p><p>Google Cloud收入达到96亿美元，同比增长28%。公司CFO Ruth Porat表示，这是受到“AI贡献的持续推动”。</p><p>&nbsp;</p><p>但AI技术的发展也增加了运营成本，主要体现在相关技术人才与计算基础设施方面。Alphabet正在努力管理这些成本。</p><p>&nbsp;</p><p>Porat解释称，“展望未来，我们仍将把重点放在削减成本增速上面，以便为越来越高的技术基础设施投资水平及相应的折旧与运营投入创造运转空间。”</p><p>&nbsp;</p><p>她还报告称，“就资本支出而言，我们报告的第一季度资本支出为120亿美元，这同样主要受到技术基础设施投资的推动，其中占比最大的部分是服务器，其次是数据中心。最近几个季度资本支出的大幅同比增长，反映出我们对AI技术为整体业务创造更多机会的能力抱有坚定的信心。”</p><p>&nbsp;</p><p>然而，从谷歌到Alphabet都在围绕AI开展组织变革，因此谷歌在AI方面的实际投入也变得愈发困难。该公司在财报中表示，“此前隶属于谷歌研究院的谷歌服务部门AI模型开发团队，如今已经被编入谷歌DeepMind麾下并直接向Alphabet集团高管报告，起始时间预计为2024年第二季度。”</p><p>&nbsp;</p><p></p><h4>微软：AI 机会取决于多少人愿意付费</h4><p></p><p>&nbsp;</p><p>与此同时，微软公布2024财年第三季度营收为619亿美元，同比增长17%。净利润为219亿美元，同比增长20%，均摊后每股收益为2.94美元。微软各业务部门绩效如下：</p><p>&nbsp;</p><p>生产力与业务流程：收入196亿美元，增幅12%。智能云：267亿美元，增幅21%。其他个人计算业务：156亿美元，缩水17%。</p><p>&nbsp;</p><p>在整体表现强劲的本季度当中，唯一的重大失误来自其他个人计算部门的设备销售层面，该部门收入下降了17%。而归功于微软收购动视暴雪，Xbox内容与服务收入跃升62%。</p><p>&nbsp;</p><p>微软公司执行副总裁兼CFO Amy Hood告诉投资者，为了进一步支持云与基础设施与模型训练，预计资本支出将继续增加。</p><p>&nbsp;</p><p>在微软的财报电话会议上，摩根士丹利的Keith Weiss询问了关于微软AI投资的更多细节，并指出软件巨头的资本支出可能同比增长50%以上，来到500亿美元。更有消息称其花费在AI超级计算机上的资金总额将高达1000亿美元。</p><p>&nbsp;</p><p>Weiss随后犀利发问：“很明显，这样的投资远远高于收入回报。希望您能向我们公布更多信息，阐述您身为管理者如何量化这些投资背后的潜在机会，毕竟这样的资本规模实在可观。”</p><p>&nbsp;</p><p>微软CEO Satya Nadella 回应称，在训练方面，微软希望“妥善分配训练这些基础大模型的必要资金，并在该领域保持领先地位。”</p><p>&nbsp;</p><p>微软公司CFO Hood 补充称，一定要以超越短期的形式看待这些大规模支出的意义，特别是关注AI对各类业务流程造成影响的可能性。“这种机会体现在价值提升之上。”换句话说，所谓机会将取决于有多少人愿意为AI增强服务付费。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>无论前景发展如何，Meta、谷歌和微软的烧钱程度都令人咋舌。反观国内，智谱AI、百川智能、月之暗面、零一万物和Minimax 称为国内“大模型五虎”，一直备受投资方青睐。通过他们的融资情况，我们可以一窥国内创业公司的烧钱情况。</p><p>&nbsp;</p><p>智谱AI，去年共获得超过25亿元人民币融资，此前也获得了数亿元的多轮投资；百川智能，去年4月完成5000万美元的天使轮融资，到10月份完成3亿美元的A轮融资，近日也爆出融资消息；月之暗面，去年初攥着5000万美金入局，去年6月完成近3亿美元的天使轮融资，今年2月完成了全新一轮超过10亿美元的融资；面壁智能，近日宣布完成了新一轮数亿元融资，此前已经完成数亿元；MiniMax，今年3月被爆新一轮融资估值将超25亿美元，此前其也已完成多轮融资，但具体金额不明。</p><p>&nbsp;</p><p>虽然无法得出具体的金额，但跟Meta、谷歌和微软一个季度就要100亿美元相比，国内大模型创业企业资金实力还是追赶不上，国内外大厂之间PK资金实力比较合适。但所有企业未来面对的问题却是相似的：如何在砸了这么多钱后，拿到收益。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theregister.com/2024/04/26/alphabet_microsoft_quarterly_results/">https://www.theregister.com/2024/04/26/alphabet_microsoft_quarterly_results/</a>"</p><p><a href="https://www.cnbc.com/2024/04/24/meta-loses-200-billion-in-value-zuckerberg-focuses-on-ai-metaverse.html">https://www.cnbc.com/2024/04/24/meta-loses-200-billion-in-value-zuckerberg-focuses-on-ai-metaverse.html</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tFzIk6Ywm1tr4EuJ1e95</id>
            <title>“超级知识助手”来了，科大讯飞发布首个长文本、长图文、长语音的大模型，触达企业落地最后一公里</title>
            <link>https://www.infoq.cn/article/tFzIk6Ywm1tr4EuJ1e95</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tFzIk6Ywm1tr4EuJ1e95</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 05:07:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 长文本、长图文、长语音、大模型
<br>
<br>
总结: 科大讯飞发布了讯飞星火V3.5，推出了长文本、长图文、长语音大模型，帮助用户高效获取知识，解决真实场景中多源信息的需求。通过模型剪枝和蒸馏，推出了130亿参数的大模型，提升了效率和准确率。同时，推出了图文识别大模型和长语音功能，进一步满足用户的需求。科大讯飞还发布了合同助手和升级AI学习机，持续用技术进步解决用户的真实需求。 </div>
                        <hr>
                    
                    <p>4月26日，讯飞星火大模型V3.5（以下简称“讯飞星火”）春季上新。面向用户高效准确知识获取的痛点，科大讯飞发布业界首个长文本、长图文、长语音大模型，不仅能够把各种信息来源的海量文本、图文资料、会议录音等进行快速学习，还能够在各种行业场景给出专业、准确回答。</p><p></p><p>科大讯飞进一步升级星火语音大模型，首发多情感超拟人合成，具备情绪表达能力，并推出一句话声音复刻功能，让科技更有温度。</p><p></p><p>同时，面向企业应用场景，科大讯飞推出星火智能体平台，帮助企业解决大模型落地的最后一公里难题。</p><p>持续用技术进步解决真实刚需，讯飞星火也在获得越来越多用户的认可。据七麦数据显示，讯飞星火APP在安卓端的下载量已经超过9600万次，在国内工具类通用大模型APP中排名第一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/021112da9fc6ef3c7c20b8fe86000787.jpeg" /></p><p></p><h3>首发长文本、长图文、长语音大模型，助力知识高效获取</h3><p></p><p></p><p>为什么科大讯飞要做长文本、长图文、长语音的大模型？通过讯飞星火APP可以看到，用户使用的最高峰不是周末，而是工作日的上午9:30和下午3:30。这意味着，大部分用户用讯飞星火来解决和工作相关的刚需问题。而高效的知识获取是用户和开发者都高度关注的问题。</p><p></p><p>科大讯飞分析发现，在知识获取和学习的过程中，广大用户能拿到的资料往往不仅是现成的长文本，还有随手可见的报刊书籍内容、各种研讨会的PPT内容，老师黑板上的板书、同学的笔记，以及各种会议录音、访谈，各种网上的发布会、培训教育视频等，能不能把这些文本、图片、语音等都上传到讯飞星火中，快速地获取知识？</p><p></p><p>为此，科大讯飞推出首个支持长文本、长图文、长语音的大模型，来解决用户真实场景中多源信息的获取需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f89689cfe7e492cacbbbabc86bf51e4.png" /></p><p></p><p>本次讯飞星火长文本功能全新升级后，具备长文档信息抽取、长文档知识问答、长文档归纳总结、长文档文本生成等能力，总体已经达到GPT-4 Turbo 4月最新长文本版本的97%水平，而在银行、保险、汽车、电力等多个垂直领域的知识问答任务上，讯飞星火长文本总体水平已经超过GPT-4 Turbo。</p><p></p><p>长文本功能的落地需要解决信息高效处理的问题：面对上百万甚至上千万文字，长文本大模型消耗的运算资源非常大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c05a49611943050831603f94c671e75.jpeg" /></p><p></p><p>为了解决大模型应用效率和准确率问题，刘庆峰谈道，基于讯飞星火V3.5对长文本的理解、学习、回答能力，科大讯飞进行了重要的模型剪枝和蒸馏，从而推出业界性能最优的130亿参数的大模型，在效果损失仅3%以内的情况下，使得星火在文档上传解析处理、知识问答的首响时间以及文字生成方面都获得了极大的效率提升。测试显示，在保障长文本效果的情况下，无论是10K、64K、128K token，还是更长的文本上，星火大模型的性能都做到业界最优。</p><p></p><p>面向复杂的图文场景，科大讯飞在图文识别、公式识别大赛多年国际第一的技术积累基础上，首次推出星火图文识别大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8ad0d8cadb8e3d2cec4f4241bc8c72c1.jpeg" /></p><p></p><p>相比传统小模型逐行文字识别的限制，星火图文识别大模型具有三大优势：</p><p>能够直接处理非常复杂的版面分析，目前已经覆盖31个典型场景，比如书刊、学术论文、专利、报纸、海报、PPT等，同时能自动识别标注出18类不同的版面要素，比如页眉、页脚、标题、段落、表格、公式、印章、手写等；融合篇章上下文语义进行文字识别，识别更精准；面向教育、金融、医疗、科研等专业领域深度优化，能自动实现更多领域的专业符号识别。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46b33d19f3f36142b9b942caedf3cd2e.jpeg" /></p><p></p><p>根据国际公开的权威英文测试集来看，讯飞星火的图文识别效果超过微软和谷歌。从典型应用场景来看，在科研、金融以及企业产品技术文档等识别效果都处于业界领先地位。</p><p></p><p>此外，面对广泛的音视频信息高效获取需求，科大讯飞也推出长语音功能，将国际领先的语音识别和翻译技术结合起来，可以实现会议录音、学习视频等的一键研读，实现音视频场景的高效知识获取。</p><p></p><h3>发布合同助手、升级AI学习机，以技术进步解决真实刚需</h3><p></p><p></p><p>讯飞星火长文本、长图文、长语音能力的升级，进一步推动大模型在各个场景的落地。刘庆峰重点介绍了讯飞星火在招投标、合同、教育等场景下的应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24ebfa965922b1c8e3b529360fde3cf0.jpeg" /></p><p></p><p>在招投标场景，此前凭借讯飞星火领先的文本理解、逻辑推理和数学能力，科大讯飞和国家能源物资公司在企业采购场景合作了智能无人评审系统，已经在国资委网站上被作为典型案例推荐。据介绍，在国家能源集团已评审5.7万余单，评审准确率达97％。这一次，叠加本次升级的长文本和长图文能力，可以让评标更便捷、更高效、更准确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/808bdb34d842792892291e80030298b0.jpeg" /></p><p></p><p>日常生活中，我们在买卖商品、装修房屋或者是购买汽车保险时经常会遇到各种各样的合同，看不懂存在风险怎么办？科大讯飞推出星火合同助手，它可以对我们的合同进行风险审核、合同比对，摘要总结以及合同生成，迅速识别潜在风险漏洞，成为你口袋里的“法务助手”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/784e3f074eccd31c066b6f9e8543db04.jpeg" /></p><p></p><p>在教育场景，科大讯飞进一步升级了讯飞AI学习机产品，不仅对作文的批改、对理科的批改更加精准，也让智能化辅学更有针对性更高效；也结合本次多模态能力升级了百科问答功能，星火大模型理解并融合了大量图书知识，对于孩子的奇思妙问等复杂问题，“爱因斯坦”和他的大咖虚拟人朋友都能回答，让孩子们在趣味互动中学知识、长见识，同时提升了孩子主动提问的意愿和能力。</p><p></p><p>智慧黑板也再次升级，搭载长文本和长语音能力，让实录转写效率得以提升的同时，提升篇章梳理能力。其次是星火教师助手，融入长文本能力后，可以把优质教辅内容融入，教师在备课的过程中就可以直接融入教辅教参中的内容，进一步丰富备课资源，提升备课效率。</p><p></p><p>此外，星火科研助手目前已在中国科学院、三亚崖州湾科技城、北京邮电大学、哈尔滨工业大学等机构高校铺开应用。多模态能力升级下，讯飞星火科研助手也进一步提升了论文问答、综述生成、实验解读等的效果，使得解析的学术资料更加丰富，进一步赋能高校和科研院所的科研工作。</p><p></p><h3>能“情感共鸣”，还能“一句话声音复刻”</h3><p></p><p></p><p>万物互联时代下需要更真实的AI语音交互。年初讯飞星火V3.5发布会上，科大讯飞推出了超拟人对话功能，AI的声音更自然更真实，拟人度达到了83%，受到用户的广泛欢迎。无论是语音可懂度、流畅度还是表现力，效果均超过OpenAI、微软。</p><p></p><p>此次科大讯飞发布多情感超拟人合成，进一步提升了情绪表达的可感知度，对高兴、抱歉、安慰、撒娇、困惑等情绪表达的可感知度达到85%以上，AI语音更加生动、真实。</p><p></p><p>昊铂HT 2024 款车型行业率先搭载科大讯飞超拟人合成技术，已在4月25日正式全球上市。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/9583ff64b9031fc8755c6d449622600b.jpeg" /></p><p></p><p>除了超拟人对话，科大讯飞还推出“一句话声音复刻”功能，一句话就可以定制你的AI助手声音。比如模仿小朋友的声音，每天给爷爷奶奶读书读报；在我们出差的时候，模仿我们的声音给孩子讲故事。这个功能可以让世界变得更有温度。</p><p></p><p>刘庆峰谈道，科大讯飞在个性化语音合成一直处于业界领先，目前已进阶至一句话声音复刻。当年讯飞AI模仿林志玲的声音需要去台湾录一个星期的声音，到后来模仿郭德纲需要一天的声音，再往后需要5分钟的录音，现在一句话就可以模仿。大家可以在讯飞星火APP上体验。</p><p></p><h3>发布星火智能体平台，为企业注入新质生产力</h3><p></p><p></p><p>自今年1月30日发布以来，讯飞星火V3.5作为首个全国产算力训练的大模型，受到了各行业伙伴和开发者的广泛欢迎。据刘庆峰介绍，大模型云边端的整体解决方案正在赋能汽车、家电、运营商等越来越多的行业；在过去不到3个月的时间里，讯飞新增了55万实名认证的开发者，其中一半以上来自企业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd3d99b4e72eae84a9d6d439b21f348d.png" /></p><p></p><p>对企业来说，如何高效地获取和学习知识同样是痛点，科大讯飞给出的答案是智能体，并面向企业场景推出全新的智能体平台。</p><p></p><p>企业构建智能体的环节主要涉及任务理解、外部信源打通、内部各个IT系统打通以及私域知识深入融合等环节，最终根据每个任务的执行结果输出答案，这样一个完整的过程才能够最终完成智能体的构建。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bbfbb6d906ad2b407566c360e50aa9db.jpeg" /></p><p></p><p>刘庆峰谈道，在讯飞星火智能体平台上，针对用户的输入，首先，基于讯飞星火大模型非常聪明的底座能力，会自动实现用户输入的精准理解和任务规划。其次，解析完了相关的任务和对应的工具之后，讯飞星火也构建形成了包括天气、航班、企查查等成体系的外部信息来源的对接；同时，星火智能体平台还通过互认证的机制，实现了往往是独立的、隔离的OA系统、CRM系统以及ERP系统的打通，完成相应操作；最后，通过私域知识融入机制，智能体平台很容易实现企业所属行业以及企业私域知识的融入，实现更精准的专业理解和知识问答。</p><p></p><p>此外，星火智能体平台还可以通过拖拽方式实现新智能体的创建和多智能体的协作。星火智能体平台，敏捷触达大模型应用企业落地的最后一公里。</p><p></p><p>据刘庆峰透露，科大讯飞将在6月27日发布讯飞星火大模型V4.0，进一步解放生产力、释放想象力。</p><p></p><p>今年全国两会上，开展“人工智能+”行动，加快发展新质生产力首次写入《政府工作报告》。大模型带来的知识管理革命正在上演，无论企业还是个人，都可以站在人工智能的肩膀上，实现新的比较优势。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QeT1AcRI6oAbwSIbi99O</id>
            <title>单场奖金池20万！百度智能云“千帆杯”教育生态行业赛全新开赛！</title>
            <link>https://www.infoq.cn/article/QeT1AcRI6oAbwSIbi99O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QeT1AcRI6oAbwSIbi99O</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 02:55:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 千帆杯, AI原生应用, 教育生态行业赛
<br>
<br>
总结: 2024年百度智能云举办了千帆杯AI原生应用创意挑战赛，旨在推动AI与教育行业的深度融合，激发创新力量，为教育领域带来新的活力。比赛以20万元的奖金池吸引开发者参与，通过千帆AppBuilder平台，鼓励个人和团队创造教育领域的创新AI应用。AI技术的发展让教育更高效，千帆AppBuilder作为产业级AI开发平台，为开发者提供了自由的创作空间，帮助他们实现理想，推动教育行业的创新发展。 </div>
                        <hr>
                    
                    <p>自2024年百度智能云“千帆杯”AI原生应用创意挑战赛启动以来，广受开发者关注，更有百万奖金激励、千万算力支持。4月25日，百度智能云携手头部高校、知名教育企业等多家单位，联合发起千帆杯AI原生应用创意挑战赛——教育生态行业赛，单场奖金池高达20万元，现已全新开赛！本期主题围绕AI与教育行业的深度融合，通过创新AI原生应用让教育领域再现新的生命力。无论你是高校学生教师、个人开发者、还是企业，都欢迎报名本期教育生态行业赛，百度智能云千帆AppBuilder将帮你把理想变为现实。</p><p></p><p></p><h2>从传统到创新 AI让教育更高效</h2><p></p><p></p><p>近年来，从文生文到文生图，再到文生视频，生成式AI技术正在持续刷新着我们的认知，也推动着教育的变革。过去，教育资源分布不均限制了人的发展。后来，在线教育平台和搜索引擎使得人们可以更便捷地获取知识。现在，生成式AI不仅支持全球性的知识获取，还提供了更高效的信息利用和创造的能力。</p><p></p><p></p><p></p><h2>从模型到应用 躬身为创新筑基</h2><p></p><p></p><p>教育生态行业赛旨在邀请优秀个人和团队，由创意驱动，以千帆AppBuilder为器，共同打造教育行业的创新AI原生应用。大赛给了选手们自由的创作空间。只要是在教育场景下利用千帆AppBuilder解决问题，就可以大胆地决定你的应用形式，释放你的AI创造力。即使“你不会写代码，也可以做出一个应用”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a034a091f9ed7ea789dc88484e13154.webp" /></p><p></p><p>千帆AppBuilder作为产业级AI开发平台，开发者可以使用预置的模板和组件，轻松定制自己的业务流程，还可以在上面集成、扩建自己特色的组件，在不同节点上选用不同的模型，构建更多产业级场景应用。例如，你可以基于千帆AppBuilder创建一个校园AI助理，为全校师生提供智能化的专属服务，接入查制度、查课程、充饭卡、借书籍等高频场景，服务学校广大师生。</p><p></p><p></p><p></p><p>本次教育生态行业赛总奖金池高达20万，更多丰厚奖励，等你来战。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81f15323262c52daaae47f5a8c997815.webp" /></p><p></p><p>本次千帆杯AI原生应用创意挑战赛——教育生态行业赛由千帆AppBuilder携手教育高校、教育科技合作伙伴好未来、特邀社区等平台共同打造，拥有权威背书。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a7f89c7db92f87e667d37a764f0a1f8.webp" /></p><p></p><p>另外，大赛还特邀百度智能云应用平台专家，北京航空航天大学、北京理工大学、北京邮电大学、郑州大学等高校教授出席大赛评审团，更有技术大牛亲自1V1指导，品牌成长扶持等多元权益。你的困惑，可在这里得到解答；你的创新理念，也会在这里得到认可与支持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8c1fb2a8bc3f2d2e59314a6d710b15b.webp" /></p><p></p><p>AI与教育相互促进，共同发展。在这个“人人都是开发者”的时代，百度作为AI领域的先行者，千帆AppBuilder的出现让每个普通人也可以开发自己的应用，实现千万种理想，从生活到工作，从个人到社会......本次教育行业赛，诚邀成熟的技术开发者和相关企业前来角逐奖金，也同样欢迎初入AI大门的你，在比赛中成长。千里之行，始于足下，快快扫码报名，一起迎接全民AI时代！</p><p></p><p><img src="https://static001.infoq.cn/resource/image/26/08/267fa66a6d82b68124f6412a7014dc08.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FXMfx6yPChWJlghhZhwl</id>
            <title>向所有生成式AI领域的优秀案例和厂商发出邀请，AIGC先锋榜进入征集倒计时 ！</title>
            <link>https://www.infoq.cn/article/FXMfx6yPChWJlghhZhwl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FXMfx6yPChWJlghhZhwl</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 02:12:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 中国生成式AI领域, 企业和案例, AIGC赛道, 技术变革
<br>
<br>
总结: 中国生成式AI领域发展迅速，吸引了众多优秀企业和案例参与AIGC赛道，以推动技术变革。 </div>
                        <hr>
                    
                    <p>经过一年多的发展，中国生成式AI领域涌现出了不少优秀的企业和案例。今年4月中旬，InfoQ&nbsp;面向&nbsp;AIGC&nbsp;赛道正式启动<a href="https://www.infoq.cn/form/?id=2098">【中国技术力量&nbsp;2024&nbsp;之AIGC先锋榜】</a>"案例征集，以期深入技术变革，洞见&nbsp;AIGC&nbsp;的产业未来。本次案例征集共分为两个维度，分别是【AIGC&nbsp;最佳实践案例&nbsp;TOP20】和&nbsp;【AIGC&nbsp;最佳技术服务商&nbsp;TOP30】，本次征集时间至&nbsp;4&nbsp;月&nbsp;30&nbsp;日&nbsp;23:59:59&nbsp;止，我们将于&nbsp;5.17&nbsp;日<a href="https://aicon.infoq.cn/2024/beijing/?utm_source=infoqweb&amp;utm_medium=dahuibanner">【AICon&nbsp;全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展】</a>"大会现场公布结果，并邀请部分获奖企业来到现场展示并见证这一时刻。</p><p></p><p>目前为止，我们已经收到了来自金融、通信、教育、汽车、制造等众多领域的数十份优秀案例，以及众多优秀的技术服务商，我们期待能看到生成式AI领域的更多成果，欢迎相关企业抓紧时间填报，如有任何问题欢迎及时与我们表单上的工作人员联系。</p><p><img src="https://static001.infoq.cn/resource/image/6d/25/6d15bda5fbd663943a73d19f0d12c125.jpg" /></p><p></p><p>【AIGC&nbsp;最佳实践案例&nbsp;TOP20】鼓励应用方企业积极申报，我们将从场景创新性、实践效果、行业价值等维度进行评分，最终获奖企业将获得电子证书、少量&nbsp;AICon&nbsp;大会现场嘉宾票（需主办方审核参会人员）以及相关宣传报道，极其优秀企业将获得&nbsp;AICon&nbsp;大会现场演示的机会，由主办方提供场地向现场数千位参会者介绍应用案例。</p><p></p><p>【AIGC&nbsp;最佳技术服务商&nbsp;TOP30】鼓励技术服务厂商、解决方案提供厂商积极申报，我们将从技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多个维度进行评分，最终获奖企业同样将获得电子证书、少量&nbsp;AICon&nbsp;大会现场嘉宾票（需主办方审核参会人员）以及相关宣传报道。</p><p></p><p>感兴趣的企业可以<a href="https://www.infoq.cn/form/?id=2098">点击此处</a>"，填写表单进行申报，如有问题，欢迎与表单中的工作人员进行联系。</p><p></p><p>InfoQ&nbsp;中国技术力量往期榜单：</p><p><a href="https://www.infoq.cn/zones/chinatechawards/2020/">https://www.infoq.cn/zones/chinatechawards/2020/</a>"</p><p><a href="https://www.infoq.cn/zones/chinatechawards2022/">https://www.infoq.cn/zones/chinatechawards2022/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/D1Bz02dcaYCOAVtIOmBh</id>
            <title>多位联席主席、出品人确认！FCon全球金融科技大会演讲议题火热招募中</title>
            <link>https://www.infoq.cn/article/D1Bz02dcaYCOAVtIOmBh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/D1Bz02dcaYCOAVtIOmBh</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Apr 2024 10:11:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 中央金融工作会议, 数字金融, 人工智能, FCon全球金融科技大会
<br>
<br>
总结: 2023年中央金融工作会议提出五篇大文章，其中数字金融是重点，金融机构需夯实数字化底座。人工智能技术加速落地应用，金融成为智能技术实践场。2024年FCon全球金融科技大会将聚焦数字金融内生力，邀请专家分享实践经验。度小满金融执行主席将在大会上分享大模型技术应用和数智化转型。 </div>
                        <hr>
                    
                    <p>2023 年，中央金融工作会议提出要“做好科技金融、绿色金融、普惠金融、养老金融、数字金融五篇大文章”。其中“数字金融”作为其余四篇文章的基础，是金融行业数十年来从电算化、信息化进阶到数字化持续沉淀和发展的成果，同时也是 2024 年金融行业发展的重点。在这个过程中，要求金融机构必须不断夯实数字化底座，一方面完成自身数字化，另一方面为其它行业数字化赋能。</p><p></p><p>金融科技的加速落地应用，成为数字金融建设的内生动力。与此同时，随着生成式 AI 技术引爆新一轮人工智能热潮，智能时代加速到来，金融凭借相对完善的数据和技术基础，以及丰富的业务场景，成为智能技术的实践场。大模型技术的应用探索仍然是金融机构及其从业者今年关注的焦点，“数字金融”的内涵也正在从数字化向数智化演变。</p><p></p><p>在此背景下，极客邦科技将于 2024 年 8 月 16 日 -17 日在上海举办 FCon 全球金融科技大会，本届大会由中国信通院铸基计划作为官方合作机构，将以“科技驱动，智启未来——激发数字金融内生力”为主题，聚焦金融行业在数智化时代从战略、组织、人才、数据到技术的全面革新，邀请国内外金融机构及金融科技公司专家分享其实践经验与深入洞察。</p><p></p><p>度小满金融技术委员会执行主席、数据智能应用部总经理杨青已确认担任本届 FCon 大会联席主席。杨青老师 2018 年年初加入度小满金融，从 0 到 1 构建度小满金融的智能引擎核心算法，深耕计算机视觉、自然语言处理、机器学习、因果推断等技术领域，多篇文章被 EMNLP、ACL、CIKM 等国际会议收录，“智能化征信解读中台”荣获吴文俊人工智能科技进步奖。相关技术广泛应用于度小满营销、经营、风控、反欺诈全流程业务场景，为上千万客户提供稳定、安全的金融服务。</p><p></p><p>目前，专注于人工智能在金融场景中的应用落地工作，于 2023 年年初带领团队发布千亿参数规模的中文大语言模型“轩辕”。2023 年 9 月， “轩辕 -70B”大语言模型在 C-Eval 和 CMMLU 两大权威榜单上位列所有开源模型榜首，目前已开源 6B/13B/70B/176B 全参数的模型体系，并著有《大语言模型：原理与工程实践》一书。</p><p></p><p>在去年的 FCon 大会中，杨青老师发表了主题为<a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247535267&amp;idx=1&amp;sn=24f7980342c4157dad6d89da8565ea8b&amp;scene=21#wechat_redirect">《人工智能在金融行业中的创新应用》的演讲</a>"，深入分析了传统 AI 在金融领域的持续影响力，同时探讨了在大模型时代下，如何有效结合传统 AI 与新兴技术，以应对未来挑战。（点击链接可查看演讲精彩内容：<a href="https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS">https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS</a>"）</p><p></p><p>而在今年大会上，杨青老师将带来近一年度小满大模型的全面更新，同时聚焦数智化转型，分享传统 AI 与 AIGC 的高效联动等话题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2ae9c94dbffc950257eebe214f2d4d35.webp" /></p><p></p><p>除此之外，本届大会还策划了<a href="https://fcon.infoq.cn/2024/shanghai/track/1682">金融数智化实践创新</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1683">金融大模型应用实践和效益闭环</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1688">前沿金融科技探索与应用</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1690">金融数字化管理和运营实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1689">金融数字化营销实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1691">数据资产化运营与数据智能应用</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1692">金融领域数据要素的流通与合规</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1684">金融国产化改造策略与实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1685">金融IT架构智能化升级</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1686">金融现代化核心系统建设</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1694">低成本高杠杆的数字化实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1687">金融研发效能提升路径与实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1693">金融组织变革与数字人才培养案例实践</a>" 等10+专题论坛。</p><p></p><p>目前部分专题出品人已确认：</p><p><img src="https://static001.geekbang.org/infoq/a7/a7de0a4bcf5f4319a1294d6e5793bdf1.webp" /></p><p></p><p>更多演讲议题火热招募中，如有合适内容，欢迎扫描二维码提交议题👇</p><p><img src="https://static001.geekbang.org/infoq/ee/ee9ae020b4c453f13f085027efe03e8e.webp" /></p><p></p><p>大会官网：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/e87986ac9c1be850cf4f11c7c</id>
            <title>大模型下B端前端代码辅助生成的思考与实践 ｜ 得物技术</title>
            <link>https://www.infoq.cn/article/e87986ac9c1be850cf4f11c7c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/e87986ac9c1be850cf4f11c7c</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Apr 2024 02:35:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 重复工作, 代码规范, AI代替, 页面配置
<br>
<br>
总结: 文中提到了在B端前端代码开发中，重复工作和代码规范是常见问题，AI代替简单脑力可以提高开发效率，同时页面配置方式对于不同需求有不同的适用性。 </div>
                        <hr>
                    
                    <p></p><h2>一、背景</h2><p></p><p>重复工作，代码规范：B端前端代码开发过程中开发者总会面临重复开发的痛点，很多CRUD页面的元素模块基本相似，但仍需手动开发，将时间花在简单的元素搭建上，降低了业务需求的开发效率，同时因为不同开发者的代码风格不一致，使得敏捷迭代时其他人上手成本较高。</p><p>AI代替简单脑力：AI大模型的不断发展，已经具备简单的理解能力，并可以进行语言到指令的转换。对于基础页面搭建这样的通用指令可以满足日常基础页面搭建的需要，提升通用场景业务开发效率。</p><p></p><h2>二、生成链路一览</h2><p></p><p>B端页面列表、表单、详情都支持生成，链路大概可分为以下几个步骤。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d3d17f293a62785988e9d82b7094811.gif" /></p><p></p><p>输入自然语言结合大模型按照指定规则提取出相应搭建信息搭建信息结合代码模板与AST输出前端代码</p><p></p><h2>三、表达需求</h2><p></p><p>图形化配置</p><p>辅助代码生成第一步是告诉它开发出怎样的界面，提到这里，我们首先想到的是页面配置，即目前主流的低代码产品形式，用户通过一系列的图形化配置对页面进行搭建，如下图：</p><p><img src="https://static001.geekbang.org/infoq/05/0570df590846069db9b59f93d3f966f3.png" /></p><p>以上配置方式对于通用场景（如后台逻辑较为简单的CURD页面）或是特定的业务场景（如会场搭建）有较好的提效作用 。而对于需要不断迭代逻辑相对复杂的需求来说，由于是通过图形化操作的方式进行配置，对于交互设计要求较高，并且具备一定的上手成本，并且随着需求的复杂度越来越高，配置表单交互越来越复杂，维护成本也越来越高。因此，页面配置的方式前端领域的使用是相对克制的。</p><p>AI直接生成代码</p><p>AI生成代码在工具函数场景下应用较多，但对于公司内部特定业务场景的需求，可能需要考虑以下几点：</p><p>生成定制化：公司团队内部有自己的技术栈与重型通用组件，需要将这些知识进行预训练，目前对于长文本的预训练内容仅支持单次会话注入，token数消耗较高；准确度：AI生成代码的准确度挑战是比较大的，加上预训练包含大段prompt，因为代码输出的内容细节过多，加上模型幻觉，目前来看业务代码的失败率是较高的，而准确度是考量辅助编码的核心指标，如果这一点无法解决，辅助编码效果将大打折扣；生成内容残缺：由于GPT单次会话的存在限制，对于复杂需求，代码生成有一定几率被截断，影响生成成功率。</p><p>自然语言转指令</p><p>GPT其实还有个很重要的能力，那就是自然语言转指令，指令即行动，举个例子：我们假设一个函数方法实现，输入是自然语言，结合GPT与内置的prompt，让其稳定的输出某几个单词，我们是不是就可以通过对这些单词输出做出进一步的行动？这相对于图形化配置有以下几个优点：</p><p>学习门槛低：因为自然语言本身就是人类的原生语言，你只需要根据你的想法描述页面即可，当然描述的内容是需要遵循一些规范的，但相对于图形化配置来说效率是有明显提升的；复杂度黑盒：图形化配置的复杂度会随着配置页面复杂度的上升而上升，而这样的复杂度会一览无余地展示在用户面前，用户可能会迷失在复杂的配置页面交互中，配置成本逐步上升；敏捷迭代：如果要在用户端新增一个页面配置功能，基于大模型的交互方式可能只需要新增几个prompt，但图形化配置需要开发复杂表单以便于快速输入。</p><p>这里大家可能会有个疑问：</p><p>生成的指令信息不也会出现大模型幻觉吗？如何保证每次生成指令信息是稳定且一致的呢？</p><p>自然语言转指令可行大致有以下几个原因：</p><p>由长文本转关键信息属于总结内容，大模型在总结场景下的准确度远高于扩散型场景；由于指令信息只是提取需求中的关键信息，不需要做代码技术栈上的预训练，因此prompt存在很大的可优化空间，通过优化完善prompt内容可以有效提升输出准确度；准确性可验证，对于每一个场景不同表述需求输入，可以通过单测预测输出验证准确性，当出现badCase，我们在优化后针对该badCase接入单测。保证准确度不断提高。</p><p>让我们来看最终的信息转化结果：</p><p>对于代码辅助来说，基于用户的需求描述，经过PROMPT处理，可以拿到这样的信息。为代码生成提供基础信息。</p><p><img src="https://static001.geekbang.org/infoq/02/023095527308eed0e5535d968b19d245.png" /></p><p></p><h2>四、信息转化为代码</h2><p></p><p>通过大模型拿到自然语言对应可编码的信息（即上面例子中的JSON）后，我们就可以基于这个信息转化代码了。对于一个有明确场景的页面而言，一般情况下可分为主代码模板（列表、表单、描述框架）+&nbsp;业务组件。</p><p>转化流程</p><p><img src="https://static001.geekbang.org/infoq/b7/b7f256348f3ec7521c68bfa5b4a2b0a5.png" /></p><p>我们如何开发代码的？</p><p>其实这一步很像我们自己开发代码，我们拿到需求后，大脑中会提取其中的关键信息，即上方提到的自然语言转指令，然后我们会在vscode中创建一个文件，然后会进行以下操作：</p><p>首先一定是创建代码模板，然后根据场景引入对应重型组件，如列表就引入ProTable，表单就引入ProForm。</p><p>基于ProTable等重型组件并向其中添加一些属性，如headerTitle、pageSize等列表相关信息。</p><p>根据需求描述引入组件，比如识别到筛选项中存在类目选择，会在useColumns新增业务组件，识别到需求描述中存在导入导出组件，会在页面的指定位置新增导入导出业务组件。</p><p>拿到mock链接，新增请求层，在页面指定位置引入。</p><p>对于以上常用的代码插入场景都可以封装进JSON中，然后通过代码模板结合AST插入或字符串模板替换的方式生成对应代码。</p><p></p><h2>五、源码生成</h2><p></p><p>定位</p><p>源码辅助主要帮助开发者减少重复的工作，提升编码效率，和低代码页面搭建属于完全不同的赛道，低代码重在特定场景下搭建完整的页面，并且页面功能数量是可枚举的，业界低代码搭建也有很优秀的实践。而源码辅助工具旨在帮助用户尽可能多的初始化业务需求代码，后面的修改维护在代码层面交给用户，提升新增页面的开发效率。</p><p>具体的功能架构见下方：</p><p><img src="https://static001.geekbang.org/infoq/9d/9db2eedefa20530535c3e15985bee29f.png" /></p><p></p><h2>六、组件向量搜索与嵌入</h2><p></p><p>对于前端开发来说，提效的本质是少开发代码，更快的页面生成是一方面，良好的组件抽离是相当重要的一环，我们结合向量对组件的引入链路进行了优化，在初始化模板与存量代码中快速搜索定位组件。</p><p>组件向量引入链路</p><p><img src="https://static001.geekbang.org/infoq/0e/0e7249ad778ce66db51eb9a98bc9e7d8.png" /></p><p>组件信息录入</p><p>支持快速获取组件的描述内容与组件引入范式，一键录入组件，组件描述会转化为向量数据存入向量数据库。</p><p><img src="https://static001.geekbang.org/infoq/4a/4a10c7b15626b4d61c37f65cc18eed17.gif" /></p><p>组件向量搜索</p><p>用户输入描述后，会将描述转化为向量，基于余弦相似度与组件列表进行比对，找到相似度最高的组件TOP N。</p><p><img src="https://static001.geekbang.org/infoq/b4/b4a7e69aee6d84b117ac7e1ddd3fd123.gif" /></p><p>组件快速插入</p><p>用户可以在存量代码中快速通过描述搜索匹配度最高的组件，回车进行插入。</p><p><img src="https://static001.geekbang.org/infoq/11/110600ced8d1d48b5c1ca9b3d82622ee.gif" /></p><p></p><h2>七、未来展望</h2><p></p><p>组件嵌入模板：目前组件已支持向量搜索，通过结合源码页面生成，支持动态匹配组件并嵌入模板；存量代码的编辑生成：目前仅支持新增页面的源码生成，后续将支持存量页面的局部代码新增；代码模板流水线：AST的代码操作工具化，将自然语言与代码写入进一步打通，提升场景拓展效率。</p><p></p><p>​</p><p>&nbsp;*文/天意</p><p>本文属得物技术原创，更多精彩文章请看：<a href="https://link.juejin.cn/?target=https%3A%2F%2Ftech.dewu.com">得物技术官网</a>"</p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Qkl75wdy0kRJf96sJRTl</id>
            <title>探索 Copilot 创新实践：腾讯、字节跳动、PingCAP 与第四范式共聚 AICon</title>
            <link>https://www.infoq.cn/article/Qkl75wdy0kRJf96sJRTl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Qkl75wdy0kRJf96sJRTl</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Apr 2024 10:10:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术变革, 大模型应用, 人工智能, 业务决策
<br>
<br>
总结: 在这个技术日新月异的时代，人工智能和机器学习正在彻底改变我们处理数据和进行业务决策的方式。尤其是大语言模型（LLM）的兴起，为多个行业带来了翻天覆地的变化。通过一系列高质量演讲，汇聚了各领域的顶尖专家，深入探索技术变革对业务应用的深远影响。 </div>
                        <hr>
                    
                    <p>在这个技术日新月异的时代，人工智能和机器学习正在彻底改变我们处理数据和进行业务决策的方式。尤其是大语言模型（LLM）的兴起，为多个行业带来了翻天覆地的变化。为了深入探索这些技术变革，并了解它们对业务应用的深远影响，我们精心策划了一系列高质量演讲，汇聚了各领域的顶尖专家。</p><p></p><p>在即将于 5 月 17 日至 18 日举办的 AICon 全球人工智能开发与应用大会暨大模型应用生态展览中，我们特设了“Copilot 应用构建实践”专题论坛。本论坛由字节跳动 Code AI 团队的技术负责人杨萍担任出品人。我们诚邀以下四位行业专家，他们将分享在各自领域内关于大模型应用的深刻见解和实践经验。请看以下介绍：</p><p></p><p></p><h5>演讲一：代码大模型对于工程理解的探索研究</h5><p></p><p></p><p>我们荣幸邀请到了汪晟杰，一位在腾讯云担任产品专家，并拥有近 20 年经验的软件架构与产品管理领域专家。此前，他曾在阿里担任高级技术专家，并在 Autodesk、SAP 等知名公司担任重要技术与管理职务。在本次专题演讲中，汪晟杰将深入分享《代码大模型对于工程理解的探索研究》，解析最新版本的 GitHub Copilot 如何通过尝试 链式转换（CoT）和 检索式增强生成（RAG）技术，提高对项目多文件的深层次理解和补全能力。</p><p></p><p>他的演讲将帮助我们理解 AI 如何在国内企业内部研发中被有效利用，突破开发中的瓶颈。通过他的分享，您将有机会深入了解&nbsp;RAG、CoT 及 GitHub Copilot 在工程理解方面的先进探索，并掌握这些技术如何在企业内部代码依赖和业务封装中得到实际应用，以 AI 方式改变编程模式。</p><p></p><p></p><h5>演讲二：代码生成 Copilot 产品的应用和演进</h5><p></p><p></p><p>在大模型火热的今天，代码生成领域的进展是最为焦点的应用。我们荣幸邀请到了刘夏，字节跳动产品研发和工程架构的 Tech Lead，同时也是 Code Generation 团队的技术负责人。在本次“Copilot 应用构建实践”专题演讲中，刘夏将深入探讨基于大语言模型的代码生成技术，分享如何通过代码补全和代码编辑这两种典型的应用形态来提升业务研发效率。</p><p></p><p>他的演讲将揭示代码生成技术的当前挑战、创新的交互方式，以及未来的发展趋势。通过刘夏的专业见解，您将能够深入了解代码生成的核心概念和前沿进展，并探索如何科学地定义相关性能指标，实现软件研发方式的革新。</p><p></p><p></p><h5>演讲三：Database Copilot 在数据库领域的落地</h5><p></p><p></p><p>随着人工智能技术在各行各业的深入应用，智能化解决方案正在不断推动技术界限的扩展和业务流程的优化。特别是在数据库管理领域，智能工具的应用已成为提高效率和准确性的关键趋势。我们荣幸邀请到了李粒，PingCAP AI Lab 的负责人，他在推荐系统和强化学习领域拥有丰富的研究和实践经验。李粒曾开发击败围棋世界冠军的强化学习算法，目前在 PingCAP 负责推动自动驾驶数据库云的发展，并致力于 AI 技术在企业应用中的革新。在本次“Copilot 应用构建实践”专题演讲中，李粒将深入探讨 Database Copilot 在数据库领域的落地，分享如何通过 LLM 应用于数据库系统的管理、运维和诊断，以及如何利用生产环境中的业务数据来优化 Copilot 的性能。</p><p></p><p>他的分享不仅将帮助您了解 Database Copilot 的挑战和解决方案，还将探讨如何构建和优化 LLM 应用，实现数据库领域的创新。通过参与此演讲，您将能够洞见数据库技术未来的发展趋势，并掌握如何在实际业务中应用这些前沿技术。</p><p></p><p></p><h5>演讲四：LLM 在 BI 场景的应用思路探索</h5><p></p><p></p><p>在当今数据驱动的商业环境中，商业智能（BI）技术的不断发展正在改变企业如何洞察和利用大数据。我们非常荣幸邀请到了陈庆，第四范式的 LLM BI 方向产品负责人，他曾深度参与多个大型 toB AI 项目的落地，并是《MLOps 工程实践》的作者之一。在本次“Copilot 应用构建实践”专题演讲中，陈庆将分享《LLM 在 BI 场景的应用思路探索》。此次演讲聚焦于大型语言模型（LLM）如何通过自然语言处理（NLP）能力，简化商业智能（BI）工具的使用，从而降低技术门槛，使非技术用户也能轻松进行数据查询和分析。</p><p></p><p>他的分享将探讨 LLM 在 BI 场景下的应用难点及优化方法，以及这些技术如何帮助企业更有效地挖掘数据价值。通过陈庆的见解，您将能深入了解&nbsp;LLM 的创新应用思路及其在业务智能领域的实际效益，从而开启数据驱动决策的新纪元。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/31e8f5feaa2769ee31425b28078383ff.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k58dPOR3KaDjqOeZDKWI</id>
            <title>端侧模型 OctopusV3 发布：手机上的超级助理，性能媲美 GPT-4V 和 GPT-4 的组合？</title>
            <link>https://www.infoq.cn/article/k58dPOR3KaDjqOeZDKWI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k58dPOR3KaDjqOeZDKWI</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Apr 2024 07:00:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图片, 功能, OctopusV3, Nexa AI
<br>
<br>
总结: 通过一张图片，OctopusV3实现多种功能，来自Nexa AI团队的创新产品，结合图像和文本输入，优化模型预测行动，提高多模态信息处理能力。 </div>
                        <hr>
                    
                    <p>根据一张图片，能完成什么任务？</p><p></p><p>想吃菠萝了？迅速跳转&nbsp;Instacart&nbsp;商城界面，各种菠萝任君挑选。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/aa/9b/aa733bdb7712c7eda3962f4f2660c99b.gif" /></p><p></p><p></p><p></p><p>想给家里添置一台吸尘器？没问题，立马来到&nbsp;Amazon。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2e414be5ef92b3f55ef61cea07fa472.gif" /></p><p></p><p></p><p>想了解路过大桥的历史？好的，Google&nbsp;搜索给你想要的答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7da6c84c755d444d3e17335990535c44.gif" /></p><p></p><p></p><p>想发个邮件？OK，识别图片大意，填写收件人、标题、正文，发送！</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a19a946d30380446c7e3000b6b4f1514.gif" /></p><p></p><p></p><p>想重新装修下客厅？Done！</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecaaf2ee4c215c3c668ed46056d5f42d.gif" /></p><p></p><p></p><p>上述功能都来自&nbsp;Nexa&nbsp;AI&nbsp;团队近日推出的&nbsp;OctopusV3。据介绍，OctopusV3&nbsp;流利掌握英语和中文，能够熟练破译文本和图像任务目标，并实现功能调用，制定复杂的动作序列、生成可执行代码，安卓和&nbsp;IOS&nbsp;系统都可用。</p><p></p><p>值得注意的是，OctopusV3&nbsp;参数量不到&nbsp;10&nbsp;亿，但拥有可媲美&nbsp;GPT-4V&nbsp;和&nbsp;GPT-4&nbsp;组合起来的性能。由此，Nexa&nbsp;AI&nbsp;称其为“一个体积最小、性能最强大的多模态&nbsp;On-Device&nbsp;AI&nbsp;模型”。</p><p></p><p>据悉，Nexa&nbsp;AI&nbsp;成立于&nbsp;2023&nbsp;年，是一家致力于研究端侧&nbsp;AI&nbsp;代理的初创公司。它的创始人兼&nbsp;CEO&nbsp;Wei&nbsp;Chen、联合创始人兼&nbsp;CTO&nbsp;Zhiyuan&nbsp;Li&nbsp;分别是斯坦福大学的博士和硕士，斯坦福大学副教授&nbsp;Charles&nbsp;(Chuck)&nbsp;Eesley&nbsp;担任该公司顾问。</p><p></p><p></p><h2>OctopusV3&nbsp;是如何做到的？</h2><p></p><p></p><p>根据论文，OctopusV3&nbsp;开发中最关键的两点是整合图像、文本输入以及优化模型预测行动的能力。为此，&nbsp;Nexa&nbsp;AI&nbsp;主要采用了视觉信息编码、功能标记、多阶段训练技术。</p><p></p><p>在图像处理中，有许多方法可以对视觉信息进行编码，其中常用的是来自隐藏层的嵌入、图像标记化等。团队研究评估各种图像编码技术后，决定采用&nbsp;CLIP&nbsp;模型的方法。</p><p></p><p>与应用于自然语言和图像的标记化一样，特定的功能也可以封装到&nbsp;token&nbsp;中。Nexa&nbsp;AI&nbsp;为这些标记引入了一种训练策略，用于管理未见术语。这种方法类似于&nbsp;word2vec&nbsp;方案，即通过上下文环境来丰富标记的含义。</p><p></p><p>例如，高级语言模型最初可能很难处理&nbsp;PEGylation&nbsp;和&nbsp;Endosomal&nbsp;Escape&nbsp;等复杂的化学术语。然而，这些模型能够通过因果语言建模获得这些术语，尤其是在包含这些术语的数据集上进行训练时。同样，模型也可以使用并行策略来获取功能性标记。Nexa&nbsp;AI&nbsp;的研究表明，定义功能标记的潜力是无限的，因此可以标记任何特定功能。</p><p></p><p>OctopusV3&nbsp;采用了一种将因果语言模型与图像编码器整合在一起的模型架构，这种迭代训练方法增强了模型有效处理和整合多模态信息的能力。</p><p></p><p>该模型的训练过程分为多个阶段。首先，团队分别对因果语言模型和图像编码器进行训练，以建立基础基准模型；随后合并这些组件，并对模型进行对齐训练，以同步图像和文本处理能力；之后，训练采用在上一个版本&nbsp;OctopusV2&nbsp;框架中应用的方法，促进新版本功能标记的学习。在训练的最后阶段，这些能够与环境互动的功能标记提供反馈，用于进一步完善和优化模型。</p><p></p><p>除了上文提到的简单应用，Octopus&nbsp;V3&nbsp;还可以针对特定领域，量身定制出高度专业化的&nbsp;AI&nbsp;代理。如此，在医疗保健、金融和客户服务等行业中，用人工智能驱动的解决方案显著提高效率和用户体验。</p><p></p><p>未来，Nexa&nbsp;AI&nbsp;还会逐步开发出可容纳音频、视频等其他数据模式的训练框架。此外，他们发现视觉输入可能会带来相当大的延迟，因此正在优化推理速度。</p><p></p><p>Nexa&nbsp;AI&nbsp;还提到：“希望这个模型可以对自动驾驶和机器人领域产生帮助，也能够在终端设备上开启无限可能。期待有更多的开发者参与使用这个框架，能看到大家的创意和应用。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://arxiv.org/pdf/2404.11459.pdf">https://arxiv.org/pdf/2404.11459.pdf</a>"</p><p></p><p><a href="https://www.nexa4ai.com/">https://www.nexa4ai.com/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9169c9860a64484e2780aa428</id>
            <title>对接HiveMetaStore，拥抱开源大数据</title>
            <link>https://www.infoq.cn/article/9169c9860a64484e2780aa428</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9169c9860a64484e2780aa428</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Apr 2024 06:55:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大数据, GaussDB(DWS), HiveMetaStore, external schema
<br>
<br>
总结: 本文介绍了如何通过GaussDB(DWS)与HiveMetaStore对接，实现高性能计算引擎处理和分析海量数据的方法。通过创建external schema，可以直接查询hive/spark表或者插入数据到hive/spark表，无需担心表定义变化的同步更新。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区《<a href="https://bbs.huaweicloud.com/blogs/426138?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">对接HiveMetaStore，拥抱开源大数据</a>"》，作者：睡觉是大事。</p><p></p><h2>1. 前言</h2><p></p><p></p><p>适用版本：9.1.0及以上</p><p></p><p>在大数据融合分析时代，面对海量的数据以及各种复杂的查询，性能是我们使用一款数据处理引擎最重要的考量。而GaussDB(DWS)服务有着强大的计算引擎，其计算性能优于MRS服务中的hive或者spark这类计算引擎，且可以以更低的成本满足业务高弹性和敏捷性需求。通过与MRS联动，无需搬迁数据，利用DWS的高性能计算引擎处理和分析数据湖中的海量数据以及各种复杂的查询业务、分析业务越来越成为主流的解决方案。</p><p></p><p>我们可以通过创建external schema的方式来对接HiveMetaStore元数据服务，从而实现GaussDB(DWS)直接查询hive/spark表或者插入数据到hive/spark表。无需创建读外表或者写外表，也无需担心hive/spark表的定义发生变化时GaussDB(DWS)没有及时更新表定义。</p><p></p><p>本文章主要描述了GaussDB(DWS)与hivememtastore对接配置与指导。</p><p></p><h2>2. 原理浅析</h2><p></p><p></p><h3>2.1 什么是HiveMetaStore</h3><p></p><p></p><p>HiveMeatStore是Apache Hive的一个关键组件，它是一个元数据存储库，用于管理hive/spark表的元数据信息。HiveMeatStore存储了Hive表的结构信息，包括表名、列名、数据类型、分区信息等。它还存储了表的位置信息，即表数据存储何处。HiveMeatStore的主要作用是提供元数据服务，使得Hive/Spark可以对数据进行查询和分析。它还提供了一些API，可以让开发人员通过编程方式访问表的元数据。总之，HiveMeatStore是Hive的一个重要组件，它提供了元数据管理和查询服务。</p><p></p><p>external schema即外部模式，GaussDB(DWS)通过创建extrenal schema来对接HiveMeatStore服务，每次查询主动获取hive/spark表对象的元数据。无需GaussDB(DWS)内核通过create foreign table获取hive/spark表的元数据。</p><p></p><h3>2.2 external schema与schema的区别</h3><p></p><p></p><p>1 external schema主要用于与HiveMeatStore建立连接，获取表对象元数据，在创建external schema时需要指定连接的所需要的各个属性值。</p><p></p><p>2 普通schema在创建后会将schema的信息记录在pg_namespace中，external schema创建后和普通schema一样也会记录在pg_namespace，可以通过pg_namespace中的nsptype字段区分是external schema还是普通schmea。</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/713de0433beb9aef449a6a4cb370a4ed.png" /></p><p></p><p>除了存储在pg_namespace中的相关信息外，external schema连接相关的配置信息都会记录在pg_external_namespace中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4496d281c7c7b81a4df2f7dd349407e.png" /></p><p></p><p>3 external schema下不支持创建表对象。对象的创建是在hive或者spark中创建的，external schema仅用于执行DML操作。</p><p></p><h3>2.3 原理说明</h3><p></p><p></p><p>GaussDB(DWS)对接HiveMetaStore流程如下图所示</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/964da7adb3b0bea2925456fc81a7ed98.png" /></p><p></p><p>1.创建Server，external schema，sql query查询。</p><p></p><p>用户在使用本特性前，将需要创建Server，创建Server过程与已有Server创建过程相同</p><p></p><p>对于创建OBS server有两种方式，一种是通过永久AK、SK的方式创建。（此种方式前提是可以获取永久AK、SK，但是此种方式不安全，AK/SK直接暴露在配置文件中，并且创建服务的时候需要明文输入AK、SK，不建议采用此种方式创建服务）</p><p></p><p>另一种云上DWS绑定ECS委托方式访问OBS，通过管控面创建OBS server。委托通过管控面创建server可参考创建外表时如何创建OBS server。<a href="https://support.huaweicloud.com/mgtg-dws/dws_01_1602.html">https://support.huaweicloud.com/mgtg-dws/dws_01_1602.html</a>"</p><p></p><p>创建external schema：</p><p></p><p>external schema创建语法为</p><p></p><p><code lang="null">CREATE External Schema ex 
WITH SOURCE hive
DATABASE 'default'
SERVER hdfs_server
METAADDRESS '10.254.159.121:9010'
CONFIGURATION '/home/fengshuo/conf2';
</code></p><p></p><p>其中SOURCE字段指定了外部元数据存储引擎的类型，DATABASE为Hive中对应的数据库名，SERVER为步骤1中创建的server，METAADDRESS为Hive提供的地址端口信息，CONFIGURATION为Hive、Kerberos相关配置文件路径。</p><p></p><p>external schema的目标是对接外部元数据（Foreign Meta），使得DWS能主动感知外部元数据的变化，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/5646600febf43424b5444b7e138daa91.png" /></p><p></p><p>GaussDB(DWS) 通过external schema 对接HiveMetaStore，映射到对应的外表元数据，再通过外表访问 Hadoop。</p><p></p><p>SQL查询：select查询形式为 select * from ex.tbl，其中tbl为外源表名，ex为已创建的external schema。</p><p></p><p>2.语法解析：语法解析层主要针对进行解析，主要负责以下内容：</p><p></p><p>当读取到ex.tbl表以后，连接HMS进行元数据查询</p><p></p><p>3.元数据查询：从HMS中查询元数据信息，该步骤在步骤1中完成。</p><p></p><p>从HMS中读取数据，主要包括列信息，分区信息、分区键信息、分隔符信息等。</p><p></p><p>4.数据查询（针对select）：从DFS存储中获取统计信息文件个数和文件大小，为plan生成提供依据。</p><p></p><p>5.查询重写、查询优化、查询执行</p><p></p><p>6.查询下发：将元数据随plan下发给DN，DN收到plan以后，会将元数据进行解码后插入到SysCache中。</p><p></p><p>7.查询执行：DN访问obs对应文件，执行查询。</p><p></p><h2>3. 与hivememtastore对接流程</h2><p></p><p></p><h3>3.1 准备环境</h3><p></p><p></p><p>已创建 DWS 3.0集群和MRS分析集群，需确保MRS和DWS集群在同一个区域、可用区、同一VPC子网内，确保集群网络互通；</p><p></p><p>已获取AK和SK。</p><p></p><h3>3.2 在hive端创建需要对接的表</h3><p></p><p></p><p>1、在/opt/client路径下，导入环境变量。</p><p></p><p><code lang="null">    source bigdata_env</code></p><p></p><p>2、登录Hive客户端。</p><p></p><p>3、依次执行以下SQL语句创建demo数据库及目标表表product_info。</p><p></p><p><code lang="null">CREATE DATABASE demo;
</code></p><p></p><p><code lang="null">use demo;</code></p><p></p><p><code lang="text">DROP TABLE product_info;
 
CREATE TABLE product_info 
(    
    product_price                int            ,
    product_id                   char(30)       ,
    product_time                 date           ,
    product_level                char(10)       ,
    product_name                 varchar(200)   ,
    product_type1                varchar(20)    ,
    product_type2                char(10)       ,
    product_monthly_sales_cnt    int            ,
    product_comment_time         date           ,
    product_comment_num          int        ,
    product_comment_content      varchar(200)                   
) 
row format delimited fields terminated by ',' 
stored as orc;</code></p><p></p><p>4、通过insert导入数据到hive表</p><p></p><h3>3.3 创建外部服务器</h3><p></p><p></p><p>使用Data Studio连接已创建好的DWS集群。</p><p></p><p>MRS端有两种支持格式，hdfs和obs。hive对接这两种场景的创建外部服务器的方式也有所不同</p><p>执行以下语句，创建OBS外部服务器。</p><p></p><p><code lang="null">CREATE SERVER obs_servevr FOREIGN DATA WRAPPER DFS_FDW 
OPTIONS 
(
address 'obs.xxx.com:5443',   //OBS的访问地址。
encrypt 'on',
access_key '{AK值}',
secret_access_key '{SK值}'，
 type 'obs'
);
</code></p><p></p><p><code lang="null">执行以下语句，创建HDFS外部服务器。
</code></p><p></p><p><code lang="null">CREATE SERVER hdfs_server FOREIGN DATA WRAPPER HDFS_FDW OPTIONS (
      TYPE 'hdfs',
      ADDRESS '{主节点},{备节点}',
      HDFSCFGPATH '{hdfs配置文件地址}');
</code></p><p></p><p>认证用的AK和SK硬编码到代码中或者明文存储都有很大的安全风险，建议在配置文件或者环境变量中密文存放，使用时解密，确保安全。另外，dws内部会对sk做加密处理，因此不用担心sk在传输过程中泄漏。</p><p></p><p>查看外部服务器（obs为例）。</p><p></p><p><code lang="null">SELECT * FROM pg_foreign_server WHERE srvname='obs_server';
</code></p><p></p><p>返回结果如下所示，表示已经创建成功：</p><p></p><p><code lang="text">                     srvname                      | srvowner | srvfdw | srvtype | srvversion | srvacl |                                                     srvoptions
--------------------------------------------------+----------+--------+---------+------------+--------+---------------------------------------------------------------------------------------------------------------------
 obs_server |    16476 |  14337 |         |            |        | {address=obs.xxx.com:5443,type=obs,encrypt=on,access_key=***,secret_access_key=***}
(1 row)</code></p><p></p><h3>3.4 创建EXTERNAL SCHEMA</h3><p></p><p></p><p>获取Hive的metastore服务内网IP和端口以及要访问的Hive端数据库名称。</p><p></p><p>登录MRS管理控制台。</p><p></p><p>选择“集群列表 &gt; 现有集群”，单击要查看的集群名称，进入集群基本信息页面。</p><p></p><p>单击运维管理处的“前往manager”，并输入用户名和密码登录FI管理页面。</p><p></p><p>依次单击“集群”、“Hive”、“配置”、“全部配置”、“MetaStore”、“端口”，记录参数hive.metastore.port对应的值。</p><p></p><p>依次单击“集群”、“Hive”、“实例”，记录MetaStore对应主机名称包含master1的管理IP。</p><p></p><p>创建EXTERNAL SCHEMA</p><p></p><p><code lang="text">//Hive对接OBS场景:SERVER名字填写2创建的外部服务器名称，DATABASE填写Hive端创建的数据库，METAADDRESS填写1中记录的hive端metastore服务的地址和端口，CONFIGURATION为MRS数据源默认的配置路径，不需更改。
DROP SCHEMA IF EXISTS ex1;
 
CREATE EXTERNAL SCHEMA ex1
    WITH SOURCE hive
         DATABASE 'demo'
         SERVER obs_server
         METAADDRESS '***.***.***.***:***'
         CONFIGURATION '/MRS/gaussdb/mrs_server'
 
//Hive对接HDFS场景：SERVER名字填写创建MRS数据源连接创建的数据源名称mrs_server，METAADDRESS填写1中记录的hive端metastore服务的地址和端口，CONFIGURATION为MRS数据源默认的配置路径，不需更改。
DROP SCHEMA IF EXISTS ex1;
 
CREATE EXTERNAL SCHEMA ex1
    WITH SOURCE hive
         DATABASE 'demo'
         SERVER mrs_server
         METAADDRESS '***.***.***.***:***'
         CONFIGURATION '/MRS/gaussdb/mrs_server'</code></p><p></p><p>查看创建的EXTERNAL SCHEMA</p><p></p><p><code lang="text">SELECT * FROM pg_namespace WHERE nspname='ex1';
SELECT * FROM pg_external_namespace WHERE nspid = (SELECT oid FROM pg_namespace WHERE nspname = 'ex1');
                     nspid                     | srvname | source | address | database | confpath |                                                     ensoptions   | catalog
--------------------------------------------------+----------+--------+---------+------------+--------+---------------------------------------------------------------------------------------------------------------------
                  16393                        |    obs_server |  hive | ***.***.***.***:***        |  demo          | ***       |                         |
(1 row)</code></p><p></p><h3>3.5 执行数据导入hive表</h3><p></p><p></p><p>创建本地数据源表，表结构与hive一致</p><p></p><p><code lang="text">DROP TABLE IF EXISTS product_info_export;
CREATE TABLE product_info_export
(
    product_price                integer        ,
    product_id                   char(30)       ,
    product_time                 date           ,
    product_level                char(10)       ,
    product_name                 varchar(200)   ,
    product_type1                varchar(20)    ,
    product_type2                char(10)       ,
    product_monthly_sales_cnt    integer        ,
    product_comment_time         date           ,
    product_comment_num          integer        ,
    product_comment_content      varchar(200)                   
) ;</code></p><p></p><p>导入数据</p><p></p><p>从本地源表导入Hive表。</p><p></p><p><code lang="null">INSERT INTO ex1.product_info SELECT * FROM product_info_export;
</code></p><p></p><h3>3.6 执行数据从hive导入dws表</h3><p></p><p></p><p>导入数据</p><p></p><p>从本地源表导入Hive表。</p><p></p><p><code lang="null">INSERT INTO product_info_orc_export SELECT * FROM ex1.product_info;
</code></p><p></p><h2>4 总结</h2><p></p><p></p><p>本文主要对GaussDB(DWS)对接hiveMetaStore的原理和方式做了阐述。</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FkYeSqE3yTTtqX38BkbS</id>
            <title>总市值近45亿港币，AIGC第一股出门问问流血上市！首日破发开跌超21%</title>
            <link>https://www.infoq.cn/article/FkYeSqE3yTTtqX38BkbS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FkYeSqE3yTTtqX38BkbS</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Apr 2024 06:27:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 出门问问, AI 公司, 港股上市, AIGC
<br>
<br>
总结: 出门问问是一家以生成式 AI 与语音交互技术为核心业务的 AI 公司，成功在港股上市，成为 AIGC 第一股。公司营收主要来自 AI 软件与 AIoT 硬件，近年来收入增长迅速。公司通过多轮融资，股东包括红杉资本、真格基金、Google 等。上市后仍需面对激烈竞争，持续保持技术优势并实现盈利是关键挑战。 </div>
                        <hr>
                    
                    <p></p><h2>出门问问成功上市，成 AIGC 第一股</h2><p></p><p></p><p>刚刚，AI 公司出门问问正式在港股上市，报 2.98 港元，总市值约 44.45 亿港元。据悉，出门问问首次发行约 8457 万股股份，每股定价 3.8 港元。</p><p></p><p>月初时，据报道，出门问问已通过港交所上市审批，并拿到证监会 IPO 备案，并表示将于今年上市，而距离通过审批刚刚过去了半个多月，如今出门问问已经成功登陆港股。</p><p></p><p>出门问问是一家以生成式 AI 与语音交互技术为核心业务的 AI 公司。据灼识咨询行业报告，出门问问是亚洲为数不多的具有建立通用大模型能力的 AI 公司。按 2022 年 AIGC 解决方案收入计，出门问问是中国起步最早、营收最高的专注于 AIGC 技术的 AI 公司之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/30300e4022741139617495b5d6128f4e.jpeg" /></p><p></p><p>招股书显示，出门问问 2021 年、2022 年、2023 年营收分别为 3.98 亿元、5 亿元、5.07 亿元。</p><p>出门问问主要业务为生成式 AI 与语音交互技术，2022 年营收达 5 亿元。公司收入主要来自 AI 软件（包括 AIGC 解决方案和 AI 企业解决方案）与 AIoT 硬件。</p><p></p><p>近年来，AIGC 解决方案收入增长迅速，2021-2023 年复合增长率超 300%。出门问问近三年毛利率分别为 37.5%、67.2%、64.3%。截至目前，公司已完成多轮融资，股东包括红杉资本、真格基金、Google 等。</p><p></p><p>此外，据招股书显示，出门问问本次 IPO 募集所得资金的超五成将用于 AI 相关研发投入。具体而言，44.3% 将用于持续提高建模技术，以进行模型训练；35.7% 将在未来三年用于解决方案开发及营销，其中 6.4% 用于 AIGC 解决方案。剩余 10% 将用于寻求战略联盟、投资及收购，以及在未来三年实施解决方案开发的长期增长策略；另外 10% 则将用于运营资金及一般企业用途。</p><p></p><h2>六年估值翻了 147 倍</h2><p></p><p></p><p>出门问问成立于 2012 年，是一家以生成式 AI 与语音交互为核心的人工智能公司。公司以“Make AGI Accessible and AI CoPilot everywhere”为愿景，致力于打造国际领先的通用大模型，通过 AI 技术、产品及商业化三位一体发展，成为全球 AI CoPilot 的引领者。</p><p></p><p>在创立出门问问之前，李志飞的身份是美国约翰霍普金斯大学计算机系博士，前 Google 总部科学家，主流机器翻译开源软件 Joshua 主要开发者。</p><p></p><p>据李志飞介绍，由于他个人研究的方向是自然语言处理和人工智能，所以从谷歌回国的那一刻起，他的第一动机就是希望凭借全链路的语音技术，打造一个中文的虚拟个人助理。在李志飞所描述的技术版图中，未来所有消费者将能够通过一个统一的虚拟个人助理，在任何地方、任何设备上调出最理解自己的语音交互系统，进行高效的数字生活操作，真正实现出门问问的使命，定义下一代人机交互。</p><p></p><p>基于这一创业动机，出门问问自主研发并建立了包括声音信号处理、热词唤醒、语音识别、自然语言理解、对话管理、垂直搜索、智能推荐、语音合成、知识图谱等在内的全栈式人机交互技术栈，并分别面向消费端以及企业端发力，逐步拓展形成今天的规模布局。</p><p></p><p>2020 年，出门问问推出能够理解并生成文本的大模型 UCLAI，并发布 AI 配音平台“魔音工坊”；2023 年，出门问问将 UCLAI 升级为通用大模型“序列猴子”，以及一站式数字人制作平台“奇妙元”、企业 AI 交互式数字员工生成平台“奇妙问”以及“魔音工坊”海外版“DupDub”等 AIGC 解决方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1ce42d775663fd147c7318ca4adf9503.png" /></p><p></p><p>公开信息显示，成立至今，出门问问共完成七轮融资，累计融资超 2.55 亿美元，投资方包括红杉资本、真格基金、SIG 海纳亚洲、Google、圆美光电、歌尔声学等。据招股书披露，出门问问 2013 年 2 月的投后估值为 510 万美元，2019 年 9 月的的投后估值为 7.57 亿美元，六年估值翻了 147 倍。</p><p></p><h2>上市后就高枕无忧了？</h2><p></p><p></p><p>出门问问长期专注于生成式 AI 与语音交互技术的研发，并在行业发展初期就已经凭借先发优势占据了大片市场。然而，虽然在技术上取得了不少成就，但 AIGC 赛道上的竞争也充满了腥风血雨。</p><p></p><p>在面对如此激励的竞争环境时，如何持续保持住技术上的优势、加速技术在现实应用场景中的落地并实现盈利，成为了出门问问上市之后无法回避的“大考”。</p><p></p><p>之前，资深产业经济观察家梁振鹏在接受集微网采访时表示，国内 AI 企业在研发、人才等领域需巨额资本的投入，方能在规模业务和效应上实现提升，因此需要各大金融机构倾力注资来支撑 AI 公司的可持续发展。所以，初期国内 AI 企业还是较看重规模发展，而轻公司业绩、利润，导致亏损成了常态。</p><p></p><p>可见，上市后并非此后就高枕无忧了，而只是万里征程的开始。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GoHHJ4Pkz59J2Wuyobbt</id>
            <title>架构师如何不错过大模型技术革命？一线实践都在这了</title>
            <link>https://www.infoq.cn/article/GoHHJ4Pkz59J2Wuyobbt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GoHHJ4Pkz59J2Wuyobbt</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Apr 2024 10:32:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 大模型, 架构师, 技术变革
<br>
<br>
总结: AI与大模型的盛行，带来的不是单点和局部的变化，而是从底层算力、基础设施、软件开发、数据架构到顶层应用的全面变革。在这其中，架构师扮演着重要的角色，基于专业的架构知识和设计决策，架构师们将承担起新时代下的技术选型、架构设计和优化等职责。同时，由于技术发展迅速，架构师的技术知识容易过时，一旦错过某次技术革命，团队就可能对其工作价值产生质疑。如何迅速适应新的技术变革，同时将大模型技术与企业业务需求、团队能力、成本平衡等一系列要素相融合，成为架构师们现如今面临的艰巨挑战。 </div>
                        <hr>
                    
                    <p>AI 与大模型的盛行，带来的不是单点和局部的变化，而是从底层算力、基础设施、软件开发、数据架构到顶层应用的全面变革。</p><p></p><p>在这其中，架构师扮演着重要的角色，基于专业的架构知识和设计决策，架构师们将承担起新时代下的技术选型、架构设计和优化等职责。同时，由于技术发展迅速，架构师的技术知识容易过时，一旦错过某次技术革命，团队就可能对其工作价值产生质疑。如何迅速适应新的技术变革，同时将大模型技术与企业业务需求、团队能力、成本平衡等一系列要素相融合，成为架构师们现如今面临的艰巨挑战。</p><p></p><p>基于这一背景，即将在 6 月 14 日 -6 月 15 日举办的 ArchSummit 全球架构师峰会（深圳站） 上，我们将围绕“智能进阶. 架构重塑”主题，探讨在 AI 浪潮下，企业架构如何适应大模型时代趋势，同时寻找具有成本效益的问题解决方案，帮助参会者更好地了解如何以及何时可以在架构中使用人工智能，同时探讨在架构师技术知识更新换代速度快的行业常态下，如何规划职业道路，保持自身的竞争力。</p><p></p><p>本次大会共策划了 1 个 Keynote+18 个专题论坛，不仅邀请到了 CNCF &nbsp;CTO Chris Aniszczyk 分享大模型背景下云原生技术的最新趋势探索，Thoughtworks CTO for APAC Scott Shaw 分享前沿的工程平台实践。同时，还汇聚了来自阿里巴巴、百度、腾讯、网易、字节跳动 / 火山引擎等互联网技术大厂，以及 vivo、知乎、高德地图、Uber 、蚂蚁集团、eBay、货拉拉、快手、哔哩哔哩、携程等头部互联网企业，从供需用多视角、多维度探讨大模型基础设施建设、基础框架搭建、中台探索以及各领域内相关的应用和实践。</p><p></p><p>值得关注的是，来自顺丰集团、美的集团、鸿海科技集团（富士康母公司）、宁德核电等实体企业，以及广发证券、微众银行、众安银行、天弘基金等金融机构的技术大咖也将在大会上分享前沿技术在各行各业的应用，以及其中引发的架构变革，技术前沿、行业视角、个人成长等内容一站集齐。</p><p></p><p>目前，ArchSummit 深圳大会议程已经上线，并将持续更新，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"。</p><p><img src="https://static001.geekbang.org/infoq/84/840cd6c8e4623d379193fee76590b8e7.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a95128885738b6009cb109619f316b8e.png" /></p><p></p><p>目前，ArchSummit 深圳大会还在 8 折购票期，感兴趣的同学可以在官网购票或者扫描议程页面的二维码咨询小助手购票。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TaPZvxmCOlBv9bCYBzDZ</id>
            <title>小模型时代来了？微软推出其最小参数AI模型，性能逼近GPT-3.5</title>
            <link>https://www.infoq.cn/article/TaPZvxmCOlBv9bCYBzDZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TaPZvxmCOlBv9bCYBzDZ</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Apr 2024 10:20:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, Phi-3 Mini, 小型人工智能模型, 竞争对手
<br>
<br>
总结: 微软推出了小型人工智能模型Phi-3 Mini，该模型性能优越，响应速度快，适合个人设备使用。与此同时，微软的竞争对手也在研发自己的小型模型，针对不同任务。Phi-3系列模型在编程和推理方面表现出色，但在更大范围的应用场景中无法击败大型语言模型。 </div>
                        <hr>
                    
                    <p>当地时间4月23日，微软宣布推出其轻量级人工智能模型 Phi-3 Mini 的下一版本，这是该公司计划发布的三个小型模型中的第一个。&nbsp;</p><p></p><p>Phi-3 Mini 可测量参数仅为 38 亿，并在相对于GPT-4 等大型语言模型更小的数据集上进行训练。现已在 Azure、Hugging Face 和 Ollama 上可使用。另外，微软还计划发布Phi-3 Small（7B参数）和Phi-3 Medium（14B参数）两个版本。</p><p></p><p>微软在去年 12 月时发布了 Phi-2，其性能与 Llama 2 等更大的型号不相上下。微软表示，Phi-3 的性能比前一版本更好，其响应速度接近比它大 10 倍的模型。</p><p></p><p>微软Azure人工智能平台公司副总裁埃里克· 博伊德（Eric Boyd）介绍说，Phi-3 Mini的性能与GPT-3.5等LLM不相上下，"只是外形尺寸更小而已"。</p><p></p><p>与大型人工智能模型相比，小型人工智能模型通常运行成本更低，并且在手机和笔记本电脑等个人设备上表现更好。据外媒《The Information》今年早些时候报道称，微软正在组建一个专门专注于轻量级人工智能模型的团队。与 Phi 一起，该公司还构建了Orca-Math，一个专注于解决数学问题的模型。</p><p></p><p>微软的竞争对手也在研发自己的小模型，其中大多数针对更简单的任务，例如文档摘要或编码辅助。其中最典型的就是Google 的 Gemma 2B 和 7B，这两款模型更适合简单的聊天机器人和语言相关的工作。</p><p></p><p>此外，Anthropic 的 Claude 3 Haiku可以阅读带有图表的密集研究论文并快速总结它们，而Meta 最近发布的 Llama 3 8B可以用于一些聊天机器人和编码辅助。</p><p></p><p>Boyd 表示，开发人员通过“课程”对 Phi-3 进行了训练。他们的灵感来自于孩子们如何从睡前故事、单词更简单的书籍以及谈论更大主题的句子结构中学习。</p><p></p><p>“市面上没有足够的儿童读物，因此我们列出了 3000 多个单词的清单，并要求大语言模型制作‘儿童读物’来教授 Phi，”Boyd 说。&nbsp;</p><p></p><p>他补充说，Phi-3 只是建立在之前迭代所学到的知识之上。 Phi-1 专注于编程，Phi-2 开始学习推理，而 Phi-3 更擅长编程和推理。虽然 Phi-3 系列模型能够了解一些常识，但它无法在更大范围的应用场景中击败 GPT-4 或其他大语言模型。</p><p></p><p>Boyd 表示，公司经常发现像 Phi-3 这样的较小模型更适合他们的定制应用程序，因为对于许多公司来说，他们的内部数据集规模都比较小，而这些使用较少算力的小模型更具性价比。</p><p></p><p>参考链接：</p><p></p><p>https://www.theverge.com/2024/4/23/24137534/microsoft-phi-3-launch-small-ai-language-model</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bqmoGzkvE4GwWsvruqHp</id>
            <title>华为、阿里、喜马拉雅领衔专家深度解析 AI Agent 技术与应用｜AICon</title>
            <link>https://www.infoq.cn/article/bqmoGzkvE4GwWsvruqHp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bqmoGzkvE4GwWsvruqHp</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Apr 2024 02:02:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, AI Agent, 大模型技术, 专题
<br>
<br>
总结: 在人工智能的浪潮下，AI Agent 成为技术前沿的焦点，大模型技术推动其在企业生产、音视频创作等领域展现潜力与价值。2024 北京站 AICon 举办 AI Agent 专题，邀请多位专家分享理论研究与实践经验，全面探讨 AI Agent 的现状、挑战与未来方向。 </div>
                        <hr>
                    
                    <p>在人工智能的浪潮之下，AI Agent 正逐渐成为技术前沿探索与实践的焦点，不仅推动着各行各业的革新，也为我们提供了全新的交互体验。大模型技术的不断进步和普及使得 AI Agent 在企业生产、音视频创作、智能设备等多个领域展现出巨大的潜力和价值。为了深入探讨 AI Agent 的发展现状、挑战与未来方向，2024 北京站 AICon 全球人工智能开发与应用大会暨大模型应用生态展精心策划了一个 AI Agent 专题，邀请了阿里巴巴通义实验室 NLP 资深算法专家张佶作为专题出品人，精选了一系列既专业又吸引人的议题，以确保听众能获得最大的收益。</p><p></p><p>在本专题中，我们首先有幸邀请到清华大学智能产业研究院的副教授李鹏博士，李鹏博士在人工智能领域，尤其是自然语言处理、预训练语言模型、跨模态信息处理以及大模型智能体等方面，拥有深厚的研究基础，已在国际重要会议和期刊发表论文 90 余篇。他将带来关于大模型可进化智能体的前沿实践与理论研究的深入探索。李鹏博士的演讲将是本次专题的开篇，为我们勾勒出 AI Agent 面临的主要挑战及未来发展的新思路。</p><p></p><p>接着，张磊他是来自 OPPO 的 AI 中心系统架构师，他将分享 AI Agent 在 AI 手机中作为“数字游民”的应用实践。张磊的演讲不仅将揭示智能体在 AI 手机中的多种应用场景，还将深入讨论智能体开发平台的架构设计及其技术演进趋势，为我们展现 AI 手机的未来发展方向。</p><p></p><p>随后，我们非常荣幸地邀请到了马宇峰，阅文集团的 AIGC 技术负责人，他将带来题为**“阅文集团基于 Agent 的内容生产辅助探索”的精彩演讲。马宇峰凭借其在百度搜索和大数据领域的丰富经验，专注于自然语言处理与人工智能方向，并在多个重要赛事中取得显著成绩，如 CCKS 中文知识图谱大赛第一名。在阅文集团，他负责推进大模型、多模态、产研落地与知识管理与评估等多方面的团队工作，成功带领团队在内容行业垂类模型开发并实现场景落地，拥有丰富的一线实操经验。通过他的分享，你将深入了解到如何通过内容理解、剧本分镜、视觉化等流程**，构建以 AI Agent 为核心的内容生产辅助系统，帮助创作者显著提升内容生产效率。</p><p></p><p>当然，我们也荣幸邀请到了机器姬的 CTO 刘智勇，他是具身智能方向的技术专家和科学家。他的研究领域包括具身智能、机器人学习以及移动机器人技术，是文生行动推理智能体 JI Agent 和 ZROS 核心作者，也是国家奖学金获得者。他将分享具身智能机器人推理 Agent 和训练 Agent 的落地应用。刘智勇的分享将带领我们深入了解具身智能的前沿探索，特别是在开放世界中的应用实践，展示了智能体技术如何助力长周期机器人任务的规划与执行。</p><p></p><p>紧接着，我们邀请到的是阿里巴巴通义实验室高级算法专家严明，他专注于对话问答、预训练语言模型、大模型智能体以及多模态大模型的研究，在阿里主要负责 mPLUG 多模态大模型与 AI 智能体的基础技术，以及通义星尘、魔搭 Agent 的基础算法和 XPLUG 开源体系的构建。他将从模型协作智能体到个性化智能体技术的应用实践出发，分享他的广泛实践经验和技术演进。严明的演讲将深化我们对智能体技术最新进展的理解，展示如何利用大模型智能体技术解决实际应用中的问题。</p><p></p><p>之后，喜马拉雅珠峰 AI 产品总监吕睿韬将带来关于在音视频创作中 AI Agent 创新探索的分享。他将探讨如何通过构建基于音视频大模型的 AI agent 来应对这些挑战，实现创作过程的自动化和智能化，从而提高创作效率和内容质量。重点介绍将包括内容生成、编辑与后期制作的智能化、个性化推荐、互动体验，以及版权与合规性审核。此外，他还将讨论 AI 在分析观众数据、提供个性化内容建议、以及内容版权管理和合规性审核中的作用。通过吕睿韬的分享，您将获得对 LLM 底层思考和 AI agent 构建与实践的深入理解，掌握音视频创作中 AI 技术的应用与前景。</p><p></p><p>最后，来自华为云 aPaaS 服务产品部的首席架构师陈星亮，将带来“AI Agent 在企业生产中的技术实践”的内容分享。他从当前 AI Agent 进入企业生产场景面临的专业复杂问题、AI 生成结果难以达到可用标准以及知识安全难保障等全新挑战入手，分享华为云 AI Agent 的探索与实践，在虚拟客服、会议摘要等场景有着较好的应用效果，以期解决企业引入 AI 生成技术的瓶颈，并对 AI Agent 在未来的企业生产场景中进行多形态部署和交互进行展望，他的分享希望给你带来“AI Agent 如何在企业生产场景中充分发挥技术价值”的启发和思考。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/47bc0842e0afda11064705004f1d3c0a.webp" /></p><p></p><p>从李鹏博士的理论研究到张磊的 AI 手机应用，再到刘智勇的具身智能机器人实践，严明的模型协作探讨，吕睿韬的音视频创作探索，以及陈星亮的企业生产实践，本次 AI Agent 专题全面涵盖了从理论到实践，从个体到系统，从产品到服务的各个方面。每位嘉宾的分享不仅相互补充，形成了对 AI Agent 现状、挑战与未来方向的全景式认知，也为参与者提供了一个深入理解和探索 AI Agent 潜力与价值的机会。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WALw5iK2Bi37M1cGqPcB</id>
            <title>大模型在金融支付 ToC 场景的应用探索：在技术创新与政策监管之间取得平衡</title>
            <link>https://www.infoq.cn/article/WALw5iK2Bi37M1cGqPcB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WALw5iK2Bi37M1cGqPcB</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Apr 2024 01:48:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融机构, 技术应用, 大模型, RAG 向量检索
<br>
<br>
总结: 金融机构在技术应用中受到多方限制，如何加速大模型技术在业务场景中的应用是一个挑战。王良分享了在平安壹钱包落地的实战经验，包括使用RAG向量检索等技术。他强调了跨领域学习对技术视角和问题解决能力的影响，以及在处理大模型技术落地过程中遇到的挑战和解决策略。同时，他也分享了在私有化部署和政策监管下如何保证数据安全性和隐私性的方法。 </div>
                        <hr>
                    
                    <p>由于行业特殊性，金融机构在业务创新和技术应用过程中受到政策监管、数据隐私保护等诸多外部因素的限制。进入 AI 时代，如何在约束条件下，加速大模型等技术在 ToC 业务场景的落地和应用，并且在技术上不做牺牲处理是一个极具挑战的工作。在 2024 年 6 月 14-15 日深圳 ArchSummit 全球架构师峰会上，我们邀请到了平安壹钱包用户研发部技术负责人王良老师进行分享，他将基于平安集团旗下互联网金融支付公司壹钱包已落地的业务场景，介绍 RAG 向量检索 + 知识库 + 标注平台等技术的实战经验，以及应用立项审批、合规监管审批、业务线选择等技术之外的经验。</p><p></p><p>在会议召开前，我们对王老师进行了预热采访，探讨其团队在大模型应用方面的探索与落地情况，希望让大家能够提前了解其大会演讲内容亮点。</p><p></p><p>InfoQ：作为平安壹钱包技术负责人，您在不同领域的技术工作经历非常丰富，从客户端开发到大前端再到后台研发及架构管理，最近又转入大数据研发部门，您觉得这些不同领域的经历如何影响了您的技术视角和解决问题的能力？</p><p></p><p>王良：我是从 Android 开发入行的，工作投入时间比较长，深入了解了用户界面的 UI 还原和交互逻辑，学会了如何优化用户体验和响应速度，让我对底层的技术原理有了更深入的理解，比如内存管理、性能优化等。这段经历让我更加注重从用户的角度出发，思考技术的实际应用和价值，为我在后续领域的工作打下了坚实的基础。</p><p></p><p>随着公司的发展，团队从最初的 Android 组，融合了 iOS 开发变成 App 客户端团队，再到后来，又融合了前端 JS、React Native 团队，再后来又补充了小程序、快应用、鸿蒙等技术栈，升级为大前端团队，在这个壮大的过程中，我开始关注跨平台、跨浏览器的兼容性问题，以及如何构建高效、可维护的前端架构。</p><p></p><p>大前端的工作让我学会了如何在复杂的技术环境中保持代码的健壮性和可扩展性。也开始接触到前后端的数据交互和协作，对前后端分离的开发模式有了更深入的认识。在后台研发阶段，我深入了解了服务器的架构、数据库的设计和优化、高并发处理等关键技术。这段经历让我更加关注系统的稳定性和安全性，学会了如何在复杂的业务场景中保证数据的准确性和一致性。</p><p></p><p>程序员是一个需要持续学习的工作，拥有不同领域的技术工作经历是非常宝贵的，每一个领域都有其独特的挑战和解决方案，它能够帮助程序员获得更全面的技术视角和解决问题的能力，更具备跨领域思考和解决问题的能力，随着技术栈和工作经验的提升，解决问题的能力从前期局限于用技术方案完成业务方的诉求，到参与产品规划的初期阶段，在需求规划阶段，就可以给出指导性的技术意见，帮助业务方在快速上线、节省人力成本上，提供专业的技术支持。因为除了技术实现上的方案，同时还要考虑业务因素和成本因素，同一个需求，用什么技术栈解决最合适，比如监管比较严格的业务变动频繁，更适合 RN、H5 这类方便更新的技术栈。一些稳定业务注重体验的可以投入 Android 和 iOS 两个端一起开发。而且用前端完成一个需求需要的人力是 App 客户端的一半。因为 Android 和 iOS 各需要一个开发人力。这种扎根于技术，又突破技术视角的思考方式，使我在工作中进步较大。给团队带来的收益也较大。</p><p></p><p>InfoQ：您在演讲中提到了基于 RAG 向量检索 + 知识库 + 标注平台等技术的实战经验，这些技术方案是如何应用在平安壹钱包的业务场景中的？能否分享一些具体的案例或者成效？</p><p></p><p>王良：其实公司和我个人对大模型 AI 领域都是很有兴趣的，一直想尝试用在公司的某些场景中，平安集团在去年举办了 AI 大模型相关的比赛。针对风控、办公、销售运营、等多个领域在全国多个子公司进行公开的创新应用评比。壹钱包的这个项目也是在参赛过程中，与不同公司不同团队的交流学习后，定下来的一个方向。</p><p></p><p>壹钱包属于互联网金融行业，受到监管的力度较大，想做一个大而全的产品在数据私密性上以及法律合规上挑战较大，而 RAG+ 知识库刚好合适，它既能用到大模型的能力，又能明确输入和输出的范围边界。对于数据隐私合规的管理，和输出内容的把控都能兼顾到。</p><p></p><p>我们在企业微信上有海量的客户资源，但是运营人员的响应能力有限，无法展开企微端的营销能力。一开始我们挑选了单个业务的营销场景，在训练了这个业务的知识库后，上线了企微端的营销能力，在首个业务场景看到效果之后，很多业务方都很有兴趣借助 RAG 的能力为自己的业务赋能，考虑到后续不同业务的知识库和标注都需要隔离开。我们就上线了一个标注平台。标注平台开放给业务部门主要解决了测试案例库、提示词调整、模型的回归等团队人力协作问题，让业务团队参与到项目中，加速了迭代周期，提升了准确性。</p><p></p><p>目前这个项目以及为公司培养了大量的标注人才。算是一个意外收获。</p><p></p><p>InfoQ：在处理私域大模型技术 ToC 端全流程架构时，您遇到了哪些挑战？在解决这些挑战的过程中，您采取了怎样的策略和方法？</p><p></p><p>王良：最头疼的挑战并不来自于技术层面，一开始难点在于选择业务落地场景，得到业务方的支持，这点在最初通过宣讲的方式并没有获得到业务方资源投入的支持，没有业务方愿意参与，技术无法唱独角戏。所以我们用参赛项目，改成了一个介绍公司的企业文化、入职培训、合同条规、等问答库，让公司全员参与体验，在体验后再加上我们的游说，逐渐认可了 RAG+ 知识库的可行性，争取到了第一个业务团队。</p><p></p><p>接下来的挑战来自于法律合规，因为国家对 ToC 的大模型需要报备审批通过，这部分工作对于全平安集团都是未知的，我们采取的是技术先行，在数据保护以及拒答库上做好应对方案，安排专人主动与监管部门进行沟通，积极配合提供材料。</p><p></p><p>InfoQ：对于私有化部署方案，您是如何保证数据的安全性和隐私性的？在政策监管的情况下，您又是如何应对技术逐梦与落地实践的？</p><p></p><p>王良：最基础的几点：1 访问权限管理，2 数据加密，3 物理隔离，4 数据脱敏和匿名化，5 差分隐私，6 同态加密，7 内网部署等。</p><p></p><p>私有化部署本身就是为数据安全性和隐私性服务的。唯一暴露在外的数据传输过程中，保证加密通道的可信度，就能极大的提高安全和隐私性。</p><p></p><p>技术往往走在监管前边，监管政策不会对一个不存在的事物进行约束，通常都是先有了新鲜事物，现有的约束规则无法覆盖到，才会颁布新的监管政策，要想逐梦，就需要先人一步，等行业内同类产品都遍地开花，就谈不上逐梦了，最多算技术上的追随。要有拼搏探索的精神，对于未知的新鲜事物要敢于尝试。当然也要持续的学习前沿技术，必须有能力将项目落地，才能有技术逐梦的可能性。</p><p></p><p>InfoQ：您提到了标注平台的建设和标注内容的介绍，这在 AI 落地工具生态链中扮演了怎样的角色？在标注平台建设过程中，您最大的挑战是什么，又是如何解决的？</p><p></p><p>王良：标注平台将业务方也加入到了项目建设中，仅靠开发人员是无法完成一个 RAG 项目上线的，知识库内容需要深入了解的一线人员进行标注，对于新提示词调整、知识库调整，模型验证、模型回归、测试案例库的丰富完善，等更适合提供知识库的业务方参与进来一起建设。</p><p></p><p>这样能提高模型结果的稳定性，以及提高项目迭代的速度。为公司培养更多的大模型标注人才。同时，标注平台在加快不同业务的上线时间周期以及知识库分离上起到了关键作用。</p><p></p><p>这个过程比较顺利，在最初稍微有瓶颈的在于业务人员参与标注平台的学习成本较高，前期需要培训他们对标注工作的理解和使用，这部分我们花了较多的时间，目前已经形成文档和指引手册，新成员的学习培训周期已经非常乐观了。</p><p></p><p>InfoQ：在演讲中，您还提到了政策监管下的技术逐梦与落地实践心得，您认为在金融银行类的行业中，技术创新与政策监管之间如何取得平衡？</p><p></p><p>王良：监管政策是红线，无法逾越，首先要熟悉相关的法律条规先初步评估一下方案可行性，再询问一下公司的法务意见，一般法务都比较保守，我的经验是，只要法务没有明令禁止，就可以先琢磨起来，如果国内外有成功案例和经验最好，没有的话尽量寻找技术前沿的圈子，进行讨论，如极客帮的闭门圆桌会，鲲鹏会等。</p><p></p><p>技术创新与监管政策很难得到一个稳定的平衡，要想两者接近于平衡需要同时具备较高的法律法务相关专业素养以及前沿技术能力，这样的人才是非常稀缺的，我观察到技术创新往往走在监管政策之前，如网约车、无人机、大模型等，作为一名技术人，我个人建议还是多关注前沿技术，提升自身的专业能力，以便更好地理解和评估技术创新的风险和收益。为法务提供判断依据，寻找到合适的平衡点。</p><p></p><p>InfoQ：在私域大模型技术的应用过程中，您对于业务线选择有怎样的思考和策略？在面对不同业务线的挑战时，您是如何处理的？</p><p></p><p>王良：在业务线都支持的理想状态下，选择项目落地需要综合考虑公司的战略目标、业务需求、技术可行性、风险控制及监管要求等因素。</p><p></p><p>对于不同的业务线，首先考虑战略契合度，要与公司整体目标吻合，再考虑业务需求，评估一下投入产出比，是否有价值，最后是落地方案的可行性，可行性包括技术和监管合规。对于有价值的需求，我们会提高优先级去支持。</p><p></p><p>如果选择决定权不在技术人手里，需要争取业务部门资源支持的话，策略就要换一下，把自己当成销售，把私域大模型当成我们手里要推销的产品，努力探索各业务方的痛点，寻找到大模型能解决的业务线，游说对方。</p><p></p><p>我们目前采取的是第二种。</p><p></p><p>InfoQ：最后，您能否分享一些在处理私域大模型技术应用过程中的心得和经验？对于其他互联网金融支付行业的技术团队，您有什么建议或者启示？</p><p></p><p>王良：在互联网金融支付行业，数据私域性高，用户交易数据、个人身份信息、支付习惯等敏感信息需要严格保护。私域大模型技术在保证数据安全、隐私的前提下，利用本地化部署的大模型进行数据分析与预测，避免直接将数据传输至第三方平台，降低数据泄露风险。</p><p></p><p>鉴于支付场景对响应速度和系统稳定性要求极高，私域大模型技术正在向模型轻量化、边缘计算方向发展。通过模型压缩、知识蒸馏等技术，简化模型结构，减少计算资源需求，使得复杂的大模型能够在移动设备、终端设备上高效运行。这样不仅降低了延迟，提高了服务质量，还进一步减少了数据传输过程中的安全隐患。</p><p></p><p>我认为，随着技术不断成熟，未来私域大模型将在互联网金融支付行业实现更深度的融合与全链路智能化。从用户注册、身份验证、交易授权、反欺诈监控、信贷审批、客户服务到市场营销等各个环节，大模型将作为核心驱动力，提供智能化解决方案，全面提升支付流程的效率与安全性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dc/dc9899f05e3851bf598c00a66b214307.jpeg" /></p><p></p><p>【活动推荐】</p><p></p><p>本届<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule"> ArchSummit 架构师会议</a>"上，我们邀请了 CNCF、顺丰集团、腾讯、百度等企业的专家来演讲。会议上还设置了大模型、架构升级等专题，如果你感兴趣来会议上交流，欢迎进入 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 会议官网</a>"，查看会议内容。</p><p></p><p>会议现已进入 8 折倒计时购票阶段，可以联系票务经理 17310043226 , 锁定最新优惠。扫描上方二维码添加大会福利官，免费领取定制福利礼包。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/95/95546f43046b55dbe3514e09667d9f4c.jpeg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FFMcxlUQILzWXK9HbLZ0</id>
            <title>“人工智能”和无障碍前端组件：细微差别是可生成的吗？</title>
            <link>https://www.infoq.cn/article/FFMcxlUQILzWXK9HbLZ0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FFMcxlUQILzWXK9HbLZ0</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 11:17:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式人工智能, 无障碍性, 技术解决方案, 意图
<br>
<br>
总结: 各公司都在竞相在产品中添加生成式人工智能（AI）功能，但对于无障碍性和技术解决方案的追求可能会牺牲用户利益。人类和机器在构建无障碍组件的过程中存在差异，人类注重传达意图和理解用户需求，而机器缺乏意图和理解。大型语言模型（LLM）生成的代码可能存在无障碍性问题，缺乏意图和权威性，甚至可能编造错误信息，这对用户造成风险。因此，需要审慎对待生成式AI在编写无障碍代码方面的应用。 </div>
                        <hr>
                    
                    <p></p><p>各公司都在争先恐后地在自己的产品中添加生成式人工智能（AI）功能。有些承诺可以为你生产前端组件。考虑到无障碍性和生成式人工智能的本质，这有可能吗？这是可取的吗？</p><p></p><p>对于这两个问题，简短的回答都是否定的。风险在于：我们对技术解决方案的追求是以牺牲用户利益为代价的。</p><p></p><p>为了找出原因，让我们考虑一下：构建无障碍组件的过程在人和机器之间有何不同？我们倾向于寻求技术解决方案的道德规范是什么？</p><p></p><p></p><h2>人的方式</h2><p></p><p></p><p>我们先来看看流程上的差异。编写无障碍前端代码的人，主要基于以下内容来编写 HTML 元素和属性：</p><p></p><p>他们对规范以及它们如何协同工作（包括 HTML 和 WAI-ARIA）的理解他们想要传达内容他们对辅助技术如何影响他们编写的代码的理解程度熟悉浏览器和辅助技术支持的知识查找语法并正确应用</p><p></p><p>因此，他们将自己或他们的设计师同行想要实现的东西转化为浏览器中符合这些意图的东西。意图是这里的一个关键词。准确传达作者意图和理解用户需求对于无障碍性来说至关重要。</p><p></p><p>他们可能还会参与编写 CSS，用于颜色、排版和间距等方面的设置，所有这些都会影响网站是否为用户设置了障碍。此外，还需要添加 JS 来处理交互性内容、管理状态等。</p><p></p><p></p><h2>机器的方式</h2><p></p><p></p><p>使用语言模型生成代码的工具基本上是基于统计可能性来预测代码行的，有点像自动完成功能。如果输出恰好是高质量的，那基本上是巧合。系统的成功率可以（而且通常）通过训练模型来提高，特别是通过训练非常好的用例。在某些情况下，系统非常接近高质量，因为它们拥有大量的训练数据。对于无障碍性性而言，这些数据很难获得——大多数网络都存在无障碍性问题：我们在 WebAIM Million 的自动测试中看到的只是冰山的一角。</p><p></p><p>虽然人类将意图映射到交互式内容，并在这个过程中应用了他们的理解，但大语言模型（LLM）没有意图或理解。它们只是输出与某些输入最匹配的文本块。我认为这是迷人的，令人印象深刻的，而且常常类似于魔术。而且输出看起来（有时是）是可以投入生产的高质量产品。但输出也可能包含问题，也就不足为奇了。作为理性的 Web 开发人员，我们必须审视我们所造成的问题。</p><p></p><p>为了更具体地说明这一点，让我们看看 v0，这是 Vercel 基于 LLM 的代码生成器产品，Vercel 首席执行官宣称该产品为：</p><p></p><p></p><blockquote>v0.dev 生成了我们希望在自己的 @vercel 产品中发布的生产级代码。（来源：Guillermo Rauch 的推文，2023 年 9 月 15 日上午 12:15）</blockquote><p></p><p></p><p>我特别提到这一点，是因为我认为像“生产就绪”这样的说法是对技术的高估和对人类需求的低估。这对人们有现实的影响。</p><p></p><p>当我读到“生产级”时，我理解的是“无障碍的”。我简要地浏览了 v0“特色”部分的前六个组件，发现每个组件都违反了 WCAG，并且存在无障碍性障碍。</p><p></p><p>一些障碍的示例：</p><p></p><p>在数学学习应用程序的示例中：按钮被标记为链接，进度指示有视觉展示但没有文本替代，标题被标记为 div在看板示例中：项目列表未被标记为列表、列标题对比度低、缩放时存在重叠文本在辅助工具示例中：覆盖了现有的快捷方式，以及图标未被标记为装饰性图标在终端 UI 中：按钮未被标记为按钮在定价表中：图标未被标记为装饰性图标，按钮对比度不足在音乐播放器的示例中：各种按钮未被标记为按钮，有些按钮仅在键盘上不可用，但按钮没有给出可访问的名称。</p><p></p><p>这不是一次完整的一致性审查，我只是列出了一些突出的问题。我不是在攻击，我只是想确切地展示下大语言模型（LLM）输出中常见的无障碍性问题。</p><p></p><p>你可能会说这并非全然糟糕，这是真的。我也发现了很多使事物无障碍的标记，比如对各种工具导航有用的标题、良好的对比度以及有用且有效的 ARIA。但是，这种程度的无障碍性通常也存在于没有使用大型语言模型（LLM）的网站上。许多网站都有相当有用的标题，在许多元素上有很好的对比度和有效的 ARIA。问题在于那些没有到位的部分，这些是网络用户界面为残疾人创造障碍的地方。细微差别至关重要。</p><p></p><p></p><h2>自信问题</h2><p></p><p></p><p>生成式 AI 可以帮助编写无障碍代码吗？Léonie Watson 研究了另外三种生成式 AI（ChatGPT、Bard 和 Fix My Code）的输出。和我一样，她发现了一些不是很糟糕的东西，一些实际上很有帮助的东西，以及一些构成无障碍性问题的东西。但 Léonie 指出了一个不同的问题：这些工具倾向于表现出权威。无论它们是否具有权威性。她解释道：</p><p></p><p></p><blockquote>除了需要检查其响应的一般性陈述之外，这些生成式 AI 工具都没有给出任何提示来表明它们的答案可能是不正确的，也没有提供任何推荐的检查资源”。</blockquote><p></p><p></p><p>相比之下，大多数关于无障碍编码的比较好的博客文章和资源都包含了很多细微差别。它们通常不会推荐一个保证始终有效的权威解决方案（它们会使用什么定义来衡量“有效”呢？）。这反映了通常情况下制作无障碍界面所涉及的情况。这涉及到深入探索的问题。一般而言，有多种方式和多种至少不那么糟糕的结果需要在它们之间取得平衡。</p><p></p><p></p><h4>好吧，但是大型语言模型（LLM）至少可以部分有用吧？</h4><p></p><p></p><p>或许权威性的问题可以得到解决。我们可以调整这些工具，使其输出的响应不会表现成自以为是的万事通。但这仍然会给我们带来其他问题：难以获得的建议、缺乏意图和理解，以及缺乏创新。</p><p></p><p></p><h5>谎言和幻觉</h5><p></p><p></p><p>正如我在上面分享的例子和 Léonie 帖子中的例子所展示的那样，大语言模型（LLM）给出了难以理解的建议。如果这些谎言是训练数据的后果，那么理论上可以通过不同的训练数据来改进（强调“理论上”）。但这也是由于“幻觉”造成的，这是这项技术所固有的问题，研究表明 是不可避免的。它们会编造错误的内容。输出可能是无意义的。以牺牲用户为代价。这不可能改善现状：即使没有“人工智能”，网络上也有很多针对特定漏洞或问题的无障碍性提示，自动添加虚假信息和幻觉似乎是很荒谬的。</p><p></p><p></p><h4>缺乏意图</h4><p></p><p></p><p>大语言模型（LLM）工具没有意图，而意图对于（大多数）“无障碍编码”来说是必要的。Alastair Campbell 在他的文章《为什么人工智能不能用于生成无障碍代码》中解释说，无障碍性不是一个平均值。这使得它与使用统计方法提出的建议不兼容。</p><p></p><p></p><h4>缺乏创新</h4><p></p><p></p><p>虽然有很多开源组件库，但许多 UI 模式及其含义还没有被发明出来。这些假设迫切需要进行测试。依赖大语言模型（LLM）来提供建议意味着依赖（重新混合）现有知识，因此不适合创建新模式的无障碍化。</p><p></p><p>基于这三个原因，我想知道：大语言模型（LLM）在帮助我们构建无障碍前端组件方面是有用吗？如果它是有用的话，可能是在于帮助开发人员发现了确实包含细微差别的资源，而不是在代码建议方面。也许在组件代码之外还有其他用途，但那是另一篇文章的内容了（另请参阅 Aaron Gustafson 的 《人工智能在无障碍领域的机会》）。</p><p></p><p></p><h4>关注的焦点</h4><p></p><p></p><p>这可能是另一篇文章的主题，但我觉得我应该在这里提一下：专注于试图寻找无障碍的“修复方案”或“解决方案”，会对无障碍的本质造成误解。当我们制作网站时，我们有责任让它们变得无障碍。如果我们想尝试将这项工作外包给一个工具（我们不能信任的工具），我们就是将责任推给了残疾用户（另请参阅：针对残疾的辅助设备）。</p><p></p><p>正如 Adrian Rosellli 在 《人工智能不能解决无障碍性问题》 一书中所写的那样，无障碍性关乎结果，而非输出：</p><p></p><p></p><blockquote>无障碍性是关于人的。（……）当我们以产出而不是以结果为目标时，我们就辜负了我们的朋友、家人、社区和未来的自己。当我们试图免除自己的责任时，我们就是在排斥‍人类同胞。（……）-+</blockquote><p></p><p></p><p>Eric Bailey 写道：</p><p></p><p></p><blockquote>认为人工智能将“解决”无障碍性问题是一种源于技术能力主义心态的糟糕框架。在我看来，这个行业似乎希望有一个神奇的二元解决方案（……）就我个人而言，我希望在这里以残疾的社会模式为指导：我们到底想要“修复”什么，为什么？</blockquote><p></p><p></p><p></p><h2>总结</h2><p></p><p></p><p>无障碍性通常需要的细微差别是可生成的吗？我认为不是。无论如何，这并不可靠。如果你从这篇文章中学到了一些东西，我希望是这个警告：基于大型语言模型（LLM）的工具不能成为它们承诺的编写无障碍组件代码的灵丹妙药。因为细微差别、理解和传达意图是无障碍性所固有的，因此大型语言模型（LLM）无法在组件代码的无障碍性方面提供很大帮助。此外，它们不可避免地会产生幻觉，并倾向于在输出（偶尔但真实的）谬误时摆出权威的姿态。后者可能是危险的，并且可能以牺牲用户为代价。</p><p></p><p>对于那些希望帮助构建无障碍组件的开发人员，我的建议是什么？使用一个经过人们充分测试、有良好的文档记录并且（至少）在尝试捕捉细微差别的设计系统。或者参与构建一个。并不是每个人都愿意做这项细致而精确的工作，也不是每个组织都有这样的预算。这很好，但我们并不是说它可以神奇地自动消失。让我们珍惜让网络产品真正伟大的人类努力吧。</p><p></p><p>原文链接：</p><p></p><p><a href="https://hidde.blog/ai-for-accessible-components/">https://hidde.blog/ai-for-accessible-components/</a>"</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247524143&amp;idx=1&amp;sn=03d0be21386e133b666c9f9818110dc9&amp;chksm=f9522a6cce25a37ab2a44bcac1c5c5d5dcfab49440aaeb7e12bbf6e51ca67ea7fbc7477f176d&amp;scene=21#wechat_redirect">砍掉百万行代码，这些巨头开始给自家 App “割肉瘦身”</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247524128&amp;idx=1&amp;sn=d8e55efbd05ece976d6bd11ad5bde446&amp;chksm=f9522a63ce25a375ff3923c24f042714cbcf4102e72b552f891330fc5b375564d7a4b366fc56&amp;scene=21#wechat_redirect">重塑 Jamstack：打造更简单、更强大的 Web 架构</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247524111&amp;idx=1&amp;sn=e71aa0149a31f460d1bc76383b84bf1e&amp;chksm=f9522a4cce25a35a371658354fd007acc1abfc42a161e0219119788f1948c4f741eccf850205&amp;scene=21#wechat_redirect">尘封多年，Servo 重磅回归！Rust 加持，执行速度可超过 Chromium</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&amp;mid=2247524098&amp;idx=1&amp;sn=532ca722cdd3d5161ea9379a31aeebc9&amp;chksm=f9522a41ce25a357c02ed3eb573aaf84b903d3b3332ef642ab0d3815cae429126b2f26d14337&amp;scene=21#wechat_redirect">沉寂 600 多天后，React 憋了个大招</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/izNkC3zJfj8dk80DEao1</id>
            <title>研究人员利用 80 年代的技巧来攻击 LLM</title>
            <link>https://www.infoq.cn/article/izNkC3zJfj8dk80DEao1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/izNkC3zJfj8dk80DEao1</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 10:43:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ASCII 编码艺术, 越狱手段, 大模型, 安全规则
<br>
<br>
总结: 美国安全研究人员发现，利用20世纪80年代的ASCII编码艺术可以欺骗大型模型，使其违反安全规则。他们开发了名为“ArtPrompt”的越狱手段，利用ASCII编码艺术隐藏提示词，诱导大型模型做出不安全行为。另一组研究人员介绍了一种通过“彩虹团队”加强大型模型内部安全保护能力的方法，侧重于稳健性。这些研究都关注生成式人工智能模型的安全保障措施和大型模型越狱的形式。 </div>
                        <hr>
                    
                    <p>据美国安全研究人员称，只要对 20 世纪 80 年代的科幻类电影（比如《战争游戏》）中出现的 ASCII 编码艺术稍有了解，就可能骗过大模型，让它们违反自己的安全规则。</p><p>&nbsp;</p><p>ASCII 编码艺术指的是由 1963 年 ASCII 标准定义的 95 个可打印字符（总共 128 个）拼凑而成的各种图片。1983 年的电影《战争游戏》或《创》中就用这种艺术绘制了一些图像，显示在剧情中出现的电脑屏幕上。用这种方法发起的越狱攻击使用了字符绘制的图像来“掩护”提示词，这样这些提示就不会被大模型的安全性微调方法标记出来了。</p><p>&nbsp;</p><p>来自美国四所大学的研究人员开发了名为“ArtPrompt”的越狱手段，主要针对那些特定提示中可能被大模型的安全系统拒绝的单词。它使用 ASCII 编码艺术把安全系统识别出来的单词绘制成图形，这样就做成了隐形的提示词。这些隐藏提示可以诱导被攻击的大模型做出一些不安全的行为。</p><p><img src="https://static001.geekbang.org/infoq/ff/ff40510c9c18875793ee8df22f435bc0.png" /></p><p>&nbsp;</p><p>研究人员在五个业内领先的大模型（GPT-3.5、GPT-4、Gemini、Claude 和 Llama2）中测试了这种越狱手段，结果表明它们都很难识别伪装成 ASCII 图形的提示。</p><p>&nbsp;</p><p>这种越狱方法只需要对大模型进行黑盒访问即可，并且可以让接受测试的五个大模型都“有效且高效地被诱导出不良行为”。研究人员表示这是一个漏洞，因为现在大模型内的安全防御机制是基于语义的。</p><p>&nbsp;</p><p>与此同时，来自 Meta、伦敦大学学院和牛津大学的一组研究人员介绍了一种通过“彩虹团队”加强大模型内部安全保护能力的方法，该方法侧重于语义端本身的稳健性。</p><p>&nbsp;</p><p>他们的论文将对抗性提示生成方法视为一种质量多样性问题。相应地，它使用开放式搜索来生成提示，可以发现模型在安全、问答和网络安全等众多领域的漏洞。</p><p>&nbsp;</p><p><a href="https://youtu.be/IrkCIBoqZgE">https://youtu.be/IrkCIBoqZgE</a>"</p><p>&nbsp;</p><p>该方法采用称为“质量多样性”的进化搜索框架，以生成可以通过大模型安全保障措施的对抗性提示。</p><p>&nbsp;</p><p>根据该论文，实现彩虹团队方法需要三个基本构建块：1）一组指定多样性维度的特征描述符（例如“风险类别”或“攻击风格”）； 2) 一个变异算子，用于演化对抗性提示；3) 一个偏好模型，根据对抗性提示的有效性对其进行排名。</p><p><img src="https://static001.geekbang.org/infoq/bc/bcfed56c93c1bfe5de1fa9e34702d145.png" /></p><p>&nbsp;</p><p>研究人员表示，彩虹团队框架目前仅在 Llama-2 Chat 模型上做了测试，在各个规模的模型上的攻击成功率为 90%。</p><p>&nbsp;</p><p>这两篇研究论文都重点关注生成式人工智能模型的安全保障措施的稳健性，以及大模型越狱可用的形式。随着模型规模和范围的扩大，针对对抗性提示的预防措施显然也需要加强。</p><p>&nbsp;</p><p>原文链接：<a href="https://www.thestack.technology/the-80s-come-for-llms-with-ascii-art/">https://www.thestack.technology/the-80s-come-for-llms-with-ascii-art/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9z8sw2ostgfSFtaFqb6p</id>
            <title>知乎：多云架构下大模型训练，如何保障存储稳定性?</title>
            <link>https://www.infoq.cn/article/9z8sw2ostgfSFtaFqb6p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9z8sw2ostgfSFtaFqb6p</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 10:17:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知乎, AI算法, 多云混合部署, JuiceFS
<br>
<br>
总结: 知乎是中文互联网领域领先的问答社区和原创内容平台，利用先进的AI算法服务超过1亿月活跃用户。为了提高系统的易用性和灵活性，知乎实施了多云混合部署架构，并引入了JuiceFS社区版和企业版来满足大规模读写操作和高性能需求。JuiceFS提供了跨多个公有云使用的分布式文件系统，帮助知乎构建存储层、保障LLM训练稳定性以及跨云PB级数据迁移的经验。 </div>
                        <hr>
                    
                    <p></p><p></p><p>知乎，中文互联网领域领先的问答社区和原创内容平台，2011 年 1 月正式上线，月活跃用户超过 1 亿。平台的搜索和推荐服务得益于先进的 AI 算法，数百名算法工程师基于数据平台和机器学习平台进行海量数据处理和算法训练任务。</p><p></p><p>为了提高系统的易用性和灵活性，知乎实施了多云混合部署架构，允许不同云上的作业和服务透明地处理文件，且用户可以在容器中灵活与文件交互，无需关注文件的具体存放位置。</p><p></p><p>面对多云混合部署架构的需求，知乎于 2022 年引入了 JuiceFS 社区版，创建了一个可跨多个公有云使用的分布式文件系统。这一系统在性能上满足了大规模读写操作和用户实时交互的需求。针对大规模的 LLM 训练等高性能需求场景，知乎又采用了 JuiceFS 企业版，以保障 Checkpoint 写入的稳定性，并提升 GPU 效率。</p><p></p><p>目前，知乎已在 JuiceFS 社区版上存储了 3.5PB 的数据，主要用于机器学习应用，而企业版则用于性能要求更高的任务。本文将分享知乎如何在多云混合部署架构中构建存储层、保障 LLM 训练稳定性以及跨云 PB 级数据迁移的经验。</p><p></p><p></p><h2>01 机器学习平台的业务需求与挑战</h2><p></p><p></p><p></p><h3>机器学习平台架构</h3><p></p><p></p><p>知乎的机器学习平台服务于知乎及面壁智能的数百名算法工程师，这个平台依托于先进的数据处理和机器学习技术，使工程师能够有效地处理海量数据，并进行复杂的算法训练与推理任务。</p><p></p><p>应用层：知乎的核心推广业务涵盖了首页推荐、广告和搜索功能。作为一个丰富的图文生态系统，知乎不仅包含文本内容，还拥有大量图像资源，因而在视觉和自然语言处理（NLP）方面均需机器学习技术支持。自去年起，大型语言模型（LLM）的需求持续上升。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6c872e40745f07a1583e8e0dc9907982" /></p><p></p><p>知乎机器学习平台架构</p><p></p><p>机器学习平台内部组织：用户通过界面（UI）与命令行工具使用机器学习平台的各种功能。这些功能模块覆盖了数据集管理、模型训练、笔记本、推理服务和镜像构建。</p><p></p><p>知乎与面壁智能公司展开深度合作，共同开发大型语言模型。面壁智能，同时还运营了 BMB 社区，BMB 社区提供了专门针对大型模型训练的框架 BMTrain 训练引擎，同时还有一些算法同学使用 DeepSpeed。在网页搜索和推荐场景中，我们广泛应用了 PyTorch 和 TensorFlow。在模型推理方面，目前涵盖了多种在线服务组件，包括 vLLM 、NVIDIA Triton 和我们自主研发的 CPM server，这些组件均部署在多个 GPU 集群上。</p><p></p><p>底层存储：我们采用 HDFS、JuiceFS 和云盘作为基础的物理存储解决方案，支撑整个机器学习平台的存储需求。</p><p></p><p></p><h3>业务需求</h3><p></p><p></p><p>POSIX 协议支持：在模型训练，特别是使用笔记本进行新模型探索的过程中，经常出现对大文件读写的需求，如读取样本数据和写入 checkpoint。此时，通常会利用各种开源训练引擎或框架直接从文件系统读写数据，这就使得对 POSIX 协议的支持变得至关重要。这也解释了尽管我们最初采用了 HDFS，但由于其在 POSIX 协议支持上的不足，我们没有对其进行持续的迭代。为了实现支持 POSIX 协议，我们期望以简单的挂载方式将文件系统直接集成进容器，允许程序通过标准 Linux IO 接口进行文件操作。这样不仅可以保证所有容器中文件内容的一致性（即使是在弱一致性的条件下），而且还能满足随机写的需求，这对于我们的应用场景至关重要。此外，从系统管理角度，我们不仅需要实现配额和权限控制，还希望提供可观测性指标以便于问题诊断。扩展性：对于大型模型未来规模的扩大或其他潜在变化尚属未知，故而扩展性成为我们考量中至关重要的因素。性能和成本：在当前的降本增效的大环境下，成本控制成为了一个关键因素。系统管理：我们希望多租户的文件系统能够实现权限和配额管理。作为一个支持多租户的文件系统，我们希望它能够有效地进行权限和配额管理。</p><p></p><p></p><h3>技术挑战：多个云端并发访问</h3><p></p><p></p><p>对于没有自建机房进行大模型训练的公司，依赖公有云的 GPU 资源成为必然选择。然而，单一公有云服务商往往无法提供充足的 GPU 配额，导致必须跨多个云平台分散 GPU 资源。在这种情况下，为了避免在不同云平台之间进行数据的重复拷贝，我们迫切需要一个能够在多个云环境中同时运行的文件系统。</p><p></p><p>理想的多云架构如下所示，能够实现数据的单一集群存储，跨多个云平台访问和处理。目前，知乎已经在使用四家公有云服务商的资源。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7aeddfd49544be403394b8d66fdafbfd" /></p><p></p><p>知乎多云架构示意图</p><p></p><p></p><h3>JuiceFS 相关调研</h3><p></p><p></p><p></p><h4>部署方式</h4><p></p><p></p><p>在选型时，我们期望找到一种适合云原生部署的解决方案。在这方面，JuiceFS 展现出了领先的优势。同时，我们也考察了 JuiceFS 的一些竞争对手，发现基于容器存储接口（CSI）的部署方案尚未达到完善，而 JuiceFS 的实现相当不错。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9b/9b724a7b000397385ae8fd9ba7f1407e" /></p><p></p><p>JuiceFS 社区版架构图</p><p></p><p></p><h3>系统可观测性</h3><p></p><p></p><p>JuiceFS 提供了一个功能丰富的内部指标监控看板，使得查看系统性能变得十分便捷。社区版已包括若干关键的全局统计指标，如吞吐量、I\O 操作和延迟等。企业版提供了更细致的监控指标，能够对每个缓存服务和客户端的性能指标进行详细跟踪。这对于故障排除和性能监控尤其有价值。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/28/28d7e30188ba76db11b8f7fd06344c57" /></p><p></p><p>JuiceFS 社区版看板</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1f/1fa50b50e3b75a40d3f095047707c6bc" /></p><p></p><p>JuiceFS 企业版看板</p><p></p><p></p><h2>02 多云混合部署存储架构设计</h2><p></p><p></p><p>我们目前管理四个不同的云环境，每个都有自己的 Kubernetes 集群。我们的数据分为两部分：一部分存储在 HDFS 中；另一部分则由 JuiceFS 的元数据驱动和 S3 共同构成 JuiceFS 集群。</p><p></p><p>不同的集群能够通过网络访问 JuiceFS 和 HDFS。为了优化访问速度，我们将 JuiceFS 和 HDFS 部署在同一云环境中，实现内网访问，而其他云环境则通过专线访问。这一部署策略在云环境跨地理区域部署时，对性能有一定影响。例如，若前三个云部署于北方地区，如内蒙古或北京附近，性能通常较好。相反，如果第四个云部署在西南地区，可能导致更高的延迟。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/653b9a921fcaf97e255ed96b206e81f9" /></p><p></p><p>知乎多云混合部署架构图</p><p></p><p>集群面对两种主要需求：离线训练任务和配备交互式笔记本的任务。这些任务在 Kubernetes 环境中通过 JuiceFS CSI Driver 直接挂载，使得整个过程更为高效优雅。虽然 Alluxio 采用的是本地存储（Local Storage）方式，相对来说较为简单直接，但实际上仍然可行。关键在于容灾能力——需要保证物理机上的进程运行稳定，这是一个至关重要的考量。如果稳定性不佳，可能会导致服务不可用的情况发生。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3a/3adf58e2897348e5a27b424a9f0a4335" /></p><p></p><p>知乎集群任务示意图</p><p></p><p>HDFS 以存储样本数据为主，算法工程师和数据工程师一般在大数据平台上完成数据的处理和准备工作后，上传至 HDFS，Alluxio 负责管理这些 HDFS 数据。这部分数据在模型训练和交互式访问期间处于只读状态。</p><p></p><p>JuiceFS 则被用作保存 checkpoint 的输出目录，同时也为交互式 Notebook 提供统一的存储解决方案，包括 Notebook 中的临时内容，如模型下载、软件安装及编译结果等，都存储于 JuiceFS 中。由于 Notebook 具有状态，容器的任何故障重启都可能导致大量状态信息的丢失。通过挂载 JuiceFS，我们能够保留存储部分，这对于使用交互式应用的用户而言更为友好。</p><p></p><p></p><h2>03 大语言模型训练的挑战</h2><p></p><p></p><p></p><h3>写 Checkpoint 卡住问题</h3><p></p><p></p><p>我们的集群内同时运行着交互式任务、SFT\Alignment Job 和 Pretrain Job 等多种作业，这些作业生产 checkpoint 的数据量通常超过 100GB，有大规模模型加载的需求。初始时，我们采用 JuiceFS 社区版本应对这种大规模文件读写需求。但是，我们注意到在执行写操作期间，CPU 使用率急剧上升（如下图所示），最终导致集群变得不可用。这个问题使得团队成员在使用笔记本和执行其他任务时遭遇了严重的系统延迟，严重影响了整个集群的运行效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e8/e8d7544c3a6c94b60559c4988175044a" /></p><p></p><p>在深入排查集群性能问题时，我们发现 CPU 资源耗尽主要是因为使用的数据库引擎 Redis 的 CPU 资源被完全占用。同事在审查 Redis 的日志时注意到一个特殊的审计通知，该通知表示在文件检查完成之后会自动触发一次扫描操作。这个扫描会针对所有超过 6.4GB 的文件，不管是通过手动操作还是 API 调用设置的文件大小，均会启动此扫描。在 Redis 单线程模式运行下，这种扫描在 CPU 资源已经达到使用极限的情况下会阻塞其他所有请求。</p><p></p><p></p><h2>稳定性问题复盘与解决方案</h2><p></p><p></p><p>在排查系统卡顿原因的过程中，我们识别出系统延迟是由于 setattr 操作执行时间长达约 577 秒所致。通过审查 JuiceFS 的代码，我们注意到 JuiceFS 每项操作都伴随着相关信息的打印，这些信息帮助我们迅速定位到了问题操作及其大致耗时。然而，日志中存在一项小瑕疵：它仅展示了文件的 ID 而非路径。尽管这一点增加了问题解决的难度，但我们最终还是成功地确定了问题所在。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6ca4b7b17cf01d7f859a0928feb6e041" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c2/c2a394a8a56b3ca52ba16a1d66e37ccd" /></p><p></p><p>深入分析问题根源后，我们还研究了 PyTorch 的源代码。我们发现在 PyTorch 保存数据时，每个 Tensor 被作为一个记录增量存储至 zip_file 中。在这一增量写入过程中，会导致文件大小的修改，并触发文件的 truncate 操作。这种对文件大小进行重置的需求激活了之前的扫描操作，并导致它持续了相当长的时间。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/03/031c7ef2d13b81ae9721ef8df39c97f5" /></p><p></p><p>同时，我们了解到 JuiceFS 会对文件拆分，一个文件首先被拆分成固定大小的 Chunk。每个 Chunk 可以由一个或者多个 Slice 组成，每个 Slice 的长度并不总是相同，这意味着无法简单地通过累加 Slice 长度来计算文件的总大小。因此，每当文件末尾添加或修改内容时，都需要重新计算文件的整体大小，这一过程涉及到遍历文件中的所有内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/69/6934bd2e35154536b4febfabc6935f4c" /></p><p></p><p>JuiceFS 数据存储原理示意图</p><p></p><p>面对这一挑战，我们考虑了两种解决方案。</p><p></p><p>第一种是避免对大文件进行增量写入，不使用 PyTorch 的 save_checkpoint 接口，而是首先将数据写入本地文件，然后通过移动（move）操作将其传输到 JuiceFS 中，以此保证数据的连续性和完整性。</p><p></p><p>第二种解决方案是采用 JuiceFS 企业版来彻底解决这一问题。JuiceFS 企业版元数据引擎性能更优，能够更有效地管理大规模文件操作。</p><p></p><p>我们最终决定采用 JuiceFS 企业版。主要考虑是，我们无法完全避免潜在的问题，也不可能强制所有人都遵循避免增量写入 checkpoint 的规则。一方面，由于参与人员众多，实现全员一致行动较为困难；另一方面，社区代码不断迭代更新，我们的许多代码是基于开源项目的，用于后续的原型验证。在这种情况下，修改他人的代码并不是一种可行的长期策略。</p><p></p><p></p><h3>JuiceFS 企业版元数据服务性能</h3><p></p><p></p><p>关于元数据的性能，我们最关心的是其并行度是否具有可扩展性。下面这两张图分别显示了 JuiceFS 企业版 Rename 和 Delete 操作的性能，即事务处理速率（TPS）随并发线程数增加的变化情况。分别对比了 OSS 、HDFS 和 JuiceFS。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/34/34e8f5da4ebe9b649ac813cafe7b2fc9" /></p><p></p><p>JuiceFS 企业版 Rename 测试</p><p></p><p>JuiceFS 在执行重命名操作时，随着并发线程数的增加，TPS 呈现出稳定的线性增长，远超 HDFS 和 OSS 。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a09a68fd0d44afad3a1b3f32f941b918" /></p><p></p><p>JuiceFS 企业版 Delete 测试</p><p></p><p>JuiceFS 在执行删除操作时也表现出类似的趋势，其性能同样显著优于 HDFS 和 OSS。</p><p></p><p>基于这些数据，我们选择了 JuiceFS 企业版，因为它在处理并行操作时显示出优越的扩展性。尽管性能报告中未提供我们最为关心的 truncate 类型操作的数据，但从这些图表中我们可以推断，随着并发度的提高，JuiceFS 能够有效地扩展其事务处理能力，表现出比 Redis 社区版更强的性能，因此我们选择企业版来解决我们在 LLM 训练时遇到的性能问题。</p><p></p><p></p><h2>04 PB 级数据跨云间数据迁移</h2><p></p><p></p><p>随着对对 GPU 的需求量增加，我们也陆续引入了新的机房；同时由于主数据中心的变动，我们还需要将之前小型机房的存储迁移到更大的存储系统中。</p><p></p><p></p><h3>社区版</h3><p></p><p></p><p>迁移工作主要包括两个阶段：全量迁移和增量迁移。在全量迁移阶段，我们主要采用离线备份方法，即将 S3 中的数据迁移到新的存储系统中。在这个过程中，必须确保有专用的带宽，以防数据迁移过程中影响正常业务。</p><p></p><p>同时，我们还需要考虑两个云平台之间可能存在的带宽限制，因为这些限制可能影响到集群的整体稳定性。因此，必须提前确认可用的带宽情况。还要注意，S3 网关可能对账户、IP 或其他条件有所限制，这要求我们与相关方进行沟通，争取获得尽可能大的带宽限额以保障离线备份工作的顺利进行。以往的经验显示，大约 4PB 的数据需要一周时间来完成备份。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5b/5b993d82bb6c93b14c1a1c8ef361b3f6" /></p><p></p><p>JuiceFS 社区版迁移历程</p><p></p><p></p><h3>T0- T1 阶段</h3><p></p><p></p><p>旧集群：含有从 T0 到 T1 时段的所有数据以及原始数据。新集群：从 T0 时刻开始包含所有数据，但不包括该阶段增量数据和元数据。注意事项：考虑增量备份。离线备份过程可能因为几百 TB 数据量而耗时约一天，时间受限于 S3 网关可能的进出限制。</p><p></p><p></p><h3>T1- T2- T3 阶段</h3><p></p><p></p><p>新集群：到 T1 时刻，新集群应更新至包含 T1 时刻的所有数据。随后的增量数据量相对较小，仅有一天的数据量，简化了迁移过程。注意事项：离线备份过程中，数据扫描是主要耗时环节，可能需数十小时处理 3.5PB 的数据，而非数据传输。因此，提高带宽对加速备份帮助不大。需要进行细粒度的迁移，并确保两边数据的一致性。中断文件的访问：这时需要摘掉存储并发送通知，告知所有用户暂时停止使用。然后开始进行元数据的拷贝以及从 T1 到 T2 的增量拷贝。根据我们的经验，200TB 数据可能需要数小时的时间来处理，直到 T3 时刻，整个集群才能满足需求。新旧集群是完全相同，恢复应用。</p><p></p><p></p><h3>企业版</h3><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/39/3988527ceea5010954c9e640bb9694e1" /></p><p></p><p></p><h3>JuiceFS 企业版迁移历程</h3><p></p><p></p><p>企业版的迁移过程与先前的方法相似，但显著的差别在于企业版的双写功能。在完成首次增量迁移后，需要利用这一功能来进行数据同步。此时，必须暂停所有文件访问，然后重新启动任务和笔记本，并配置双写设置。在双写阶段，系统仍将使用原有的存储，此操作对业务的影响仅限于几分钟。新存储将在 T3 时刻启用，我们完成这个过程大约用了两天时间。</p><p></p><p>接下来的步骤是切换双写中的组件，并把业务 Pod 节点指向新的集群。这个切换需要服务短暂中断，对业务的影响同样是分钟级别的。当到达 T4 时刻，过程将会很快，此时业务方已经开始使用新存储，完成对齐后需要关闭并重新配置双写功能。</p><p></p><p>我们发现增量迁移是迅速的，实际测试结果显示也仅需几分钟。这一增量迁移可以在启动作业和笔记本后进行，不会影响业务运行，但需要注意的是，在许多关键任务中，重新启动可能是不被接受的。因此，重新启动的时机通常不取决于迁移的完成情况，而是由业务方的工作中断能力决定。</p><p></p><p>尽管整体迁移时间没有缩短，但企业版的影响对业务运行的干扰更小。特别是在企业内部，如果操作影响到整个平台，那么企业版的优势会更加显著。</p><p></p><p></p><h3>跨云间数据迁移注意要点</h3><p></p><p></p><p>全量数据拷贝：需要考虑的关键因素包括数据拷贝的并行程度、公共网络带宽以及双方 S3 网关可能的流量限制。而在进行增量数据拷贝时，关注点在于离线任务的耗时，旨在一次性完成而无需重复执行。</p><p></p><p>增量数据拷贝：主要的时间消耗发生在对 S3 数据的扫描上，而不是数据的实际拷贝。如果能预先知道用户写入的具体目录，那么将大幅缩短增量拷贝所需的时间。此外，JuiceFS 的同步工具能够实现对指定文件目录的精确同步。</p><p></p><p>流程优化：最理想的做法是在网络断开后执行元数据的拷贝。我们在使用社区版进行同步的初次尝试中，并没有先行断网，结果在元数据同步后发现数据丢失的问题。JuiceFS 强调数据完整性，以元数据为准确依据。因此，在社区版中进行迁移时，我们必须确保在业务完全停止后才开始元数据同步。</p><p></p><p>社区版 JuiceFS 与企业版 JuiceFS 迁移方案对比：在执行文件系统的迁移过程中，我们同时对 JuiceFS 社区版和企业版进行了迁移操作。社区版分别采用了以 Redis 和 MySQL 作为元数据管理的两种配置。经过全面的比较后发现，社区版在迁移期间的影响业务时间较长，且迁移过程极易受到增量数据量的影响。</p><p></p><p>与此相反，企业版的迁移能够保持 JuiceFS 服务的持续可用性，尽管这要求业务方进行 3 次重启。正确选择重启时机是至关重要的，如果处理得当，对业务的影响可以降至最低。</p><p></p><p></p><h2>05 小结</h2><p></p><p></p><p>目前，知乎已在 JuiceFS 社区版上存储了 3.5PB 的数据，主要用于机器学习应用；针对那些对性能有更高要求的任务，如 LLM 训练的 Checkpoint 写入阶段，知乎采用了 JuiceFS 企业版。基于 JuiceFS 确保了跨多个公有云的数据操作的灵活性和高效性。JuiceFS 提供完全的 POSIX 兼容性，支持内部多样化的数据写入需求，性能方面能够实现实时交互的文件读写、与主流 Kubernetes 集群的无缝集成，并且提供了详尽的云应用文档及部署案例。</p><p></p><p>关于作者</p><p></p><p>王新，知乎工程师，负责知乎机器学习平台相关工作</p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AAPIBQ0Es1HYGljcJsXB</id>
            <title>钉钉 AI Agent Store 上线了！软件竞争格局重构：Agent 掀起新风暴，App 何去何从？</title>
            <link>https://www.infoq.cn/article/AAPIBQ0Es1HYGljcJsXB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AAPIBQ0Es1HYGljcJsXB</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 09:42:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 钉钉, AI助理, 应用商店, PaaS First 战略
<br>
<br>
总结: 钉钉推出AI助理市场，降低创作门槛吸引用户，开创新商业模式。同时，跟随OpenAI的GPT Store，预示未来将出现大量AI应用商店。钉钉的PaaS First 战略旨在构建开放生态底座，支持生态合作伙伴服务更深入的业务场景。 </div>
                        <hr>
                    
                    <p>编辑 ｜ 栗子、Tina</p><p>&nbsp;</p><p>4 月 18 日，钉钉正式上线 AI 助理市场（AI Agent Store）。</p><p>&nbsp;</p><p>首批上架了 200 多个AI助理。Agent Store的这种创新模式可以显著降低创作门槛并吸引更多用户，各行各业的人都可以拥有自己专属的助理。据钉钉官方数据显示，截至2024年3月底，钉钉AI已超过220万家企业使用，月活跃企业超过170万家。</p><p>&nbsp;</p><p>今年1月，OpenAI正式上线了GPT Store，行业认为这意味着AI大模型将迎来自己的“App Store”时代，同时催生出新的商业模式。紧跟OpenAI，国内外众多科技巨头和初创公司纷纷投入，推出了自己的“商店”，这就预示着未来的一两年里会出现大量的AI应用商店。</p><p>&nbsp;</p><p>长期以来，技术一直是软件公司的一个差异化因素，虽然不是永远但至少会持续一段时间；但在主张“人人都是开发者”的 AI 时代，有说法认为技术从一开始就基本上没有为企业提供任何保护，技术差异这个护城河在 AI 时代正在急剧缩小。</p><p>&nbsp;</p><p>那么钉钉为什么会推出Agent Store？它与其他的GPT Store差异点在哪里？竞争激烈的AI应用商店，其护城河又会是什么？在QCon北京2024大会现场，InfoQ就这些问题采访了钉钉CTO程操红（花名：巴布）。</p><p>&nbsp;</p><p>采访视频：<a href="https://www.infoq.cn/video/ZuRGCDK2yQU0a1gOv243">https://www.infoq.cn/video/ZuRGCDK2yQU0a1gOv243</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：钉钉的定位：这几年钉钉一直在演进，也不断跟进技术潮流，比如低代码、数字化，发展到现在，您能不能回顾一下您对钉钉这个产品的认知变化和心路历程？</p><p>&nbsp;</p><p>程操红（巴布）：大家都知道，钉钉的前身是“来往”。最初，来往的定位较为宽泛，在功能上与微信存在一定的相似性。然而，事实证明，在社交软件“红海”市场中，这种定位很难取得突破。早期，钉钉得益于迅速切入工作场景，通过提供更高效的沟通交流方式，以及满足中国许多人的打卡考勤需求，从而取得了第一波的成功。</p><p>&nbsp;</p><p>在中国，许多职场人士工作在知识密集型或高科技行业，对于高效的办公工具有着迫切的需求。然而，中国也拥有庞大的制造业市场。钉钉早期抓住这一市场机遇，实现了规模上的突破，尤其是在疫情期间，其规模更是取得了爆发式增长。</p><p>&nbsp;</p><p>我加入公司时，正是钉钉规模取得突破之后。当时，我们面临着一个新的课题：如何避免过度依赖类似“三板斧”的策略，钉一下、考勤、音视频电话，同时实现再次突破。</p><p>&nbsp;</p><p>而当时我认为，解决这个问题涉及产品生态的各个方面，需要提升对市场客户群体的理解。因此，后来我们定义了两个关键的定位，一个是协同办公平台，另一个是应用开发平台。协同办公平台旨在解决大家在整个办公场景中如何提高效率的问题。然而，如果你仅仅解决了办公中的问题，实际上还只是表面功夫，很难真正与企业的各种业务场景深度结合。你可能会觉得，你只是在传递文件、发送文档、进行简单的聊天，或者最多就是处理OA审批流程吧？差不多就这样结束了。但是，实际上，组织治理、业务运营、人才管理以及物资采购和销售等方面，背后隐藏着大量的工作，就像冰山的下方一样，很多人并没有真正触及到这些。</p><p>&nbsp;</p><p>在定位上，我们将应用开发平台定位为底座，旨在支持生态合作伙伴共同服务于更深入的业务场景。而数字化转型涵盖了办公数字化，可以逐步辐射组织数字化。应用开发平台其实还可以逐步覆盖到业务数字化领域。而且，对于一个完整的组织而言，仅仅局限于办公数字化是肯定不够的。</p><p>&nbsp;</p><p>基于上述思考，钉钉提出 PaaS First 战略，即首先做好开放生态的底座。这意味着我们可能会将更少的精力投入到场景应用产品的研发上，而是更专注于底层能力的构建，解决一些生态系统中可能存在的挑战或难题，一些可能不受欢迎或难以实现的任务，钉钉将其处理得很好。</p><p>&nbsp;</p><p>以终端为例，SaaS 企业需要进行大量的投入。在进入央国企等大型客户时，对方会首先关注信创能力，对安卓、iOS、Pad 等多端系统的支持程度以及产品体验的一致性。此外，随着数据安全要求的不断提升，数据隐私保护也成为企业必须考虑的重要因素，包括安全的能力、连接的能力以及即时通讯（IM）系统的稳定性和扩展性。例如，在3万人的大群中，每条消息的已读/未读状态都需要实时更新，这个消息扩散的量是非常惊人的，这对系统的性能提出了巨大的挑战。因此，SaaS 企业需要夯实基础能力，在 IM 系统方面做好安全稳定、多端适配和开放扩展等工作。</p><p>&nbsp;</p><p>其中，像百丽这样的企业，自主开发应用并将其部署至群里，满足了深度定制需求。这意味着将业务协同整合至即时通讯（IM）系统中，为办公协同平台和应用开发平台的融合创造了新的空间。办公协同平台中常见的音视频、IM 以及文档协作功能，正逐渐成为业务数字化的重要载体。以钉钉文档为例，其强调多人协同编辑，你会发现它又成为业务数字化的一个载体，它们并不是完全割裂的。</p><p>&nbsp;</p><p>此外，音视频功能也被广泛应用于面试招聘等场景。生态合作伙伴可以基于音视频能力构建面试招聘应用，并结合人工智能技术实现预面试等功能，进一步提升招聘效率。所以这个事儿就很有意思了，这就是说，在这个过程里产品维度发生的一些演变。</p><p>&nbsp;</p><p>谈及个人心路历程，我认为有时候确实颇为纠结。我们所面对的用户和客户这两个词，就很有意思。我们需要关注真正的使用者，无论是员工还是个人用户，他们的体验都至关重要，而这种体验往往与老板的期望不尽相同。作为商业实体，我们也需要考虑商业化的因素，产品必须满足老板、管理者以及组织发展的需求。因此，我们一直秉持着一种理念，即我们将其称为3C。第一个C是“communication”，也就是钉钉立身之本，必须优化沟通协作。这是我们的关键能力，保证信息高速公路是畅通的。</p><p>&nbsp;</p><p>在此基础上，我们还需解决另外两个C，其中一个是"control"。实际上，“control”这个术语的话，更贴切的说法应该是治理。管理学中很多人将其称为“control”，即老板如何进行治理，实施一些全局性的管控。因为一个公司之所以能够成为一个公司，很大程度上取决于其在多个方面的管控诉求。这涵盖了从财务预算管理，组织目标管理，到组织文化的基本要求和倡导等各个方面。实际上，许多方面都需要一些控制措施，包括销售里的整个营销活动的关键流程和机制。因此，从一个自上而下的视角来看，如何更好地实施治理下发，传达治理措施，并确保治理最终能够真正落地，这在数字化转型中是需要解决的问题。</p><p>&nbsp;</p><p>另外一个问题是，从员工或个体的角度来看，如何让他们成为一个更加被激活的个体，真正融入到组织中去？他们对责权力的感知，以及对生产力工具的感受，实际上需要与管理层的“control”措施相互匹配。我们的想法是，在建立了越来越高效的“communication”平台之后，如何实现“control”措施和“context”之间的无缝连接，这实际上一直是我们在管理理念层面试图解决的问题。否则，要么只是讨好一下员工，要么只是讨好一下老板，这两者可能都不太行。</p><p>&nbsp;</p><p>我们并不是说我们完全独立自主地拥有这种能力，而是我们和许多客户一起在共同探索创新，是吧？就像我刚才提到的那些深度定制的案例，比如刚才提到的百丽。事实上，随着时间的推移，钉钉的群可能不再是简单的群，而可能成为未来组织结构的一种雏形。当你的群协作频率非常高时，为什么不将这个群转变为一个实际的组织呢？它可能会是一个实际的组织，所以这个想法非常有趣，就是如何逐步让技术影响你的产品？如何让技术，以及这种思考逐步影响整个生态？如何让这些影响你产品背后的管理理念？这些问题可能还在探索之中。</p><p>&nbsp;</p><p>InfoQ：客户和用户之间的平衡，实际上这是一门艺术，最开始可能一些员工谈钉钉色变，那么现在“control”和“context”的连接，这个平衡现在达到了什么程度？</p><p>&nbsp;</p><p>程操红（巴布）：实际上我们并不是要削弱老板的管理能力，而是提升它。例如，我们通过数据的普惠和AI的加持，使老板能够更准确地掌握经营状态，而不是一味地驱使员工。本质上是要激活员工，同时让老板能够更清晰地基于数据进行治理，这两者实际上是可以相辅相成的。</p><p>&nbsp;</p><p>在员工方面，我们真正以人为本的体验，例如我们提供下班模式和免打扰功能。当然，老板可能会问，为什么不让我打扰员工呢？但本质上，这仍然是为了提升目标效率和生产力。否则，如果只是单方面给员工提供免打扰模式可能也很难。当数据洞察更加清晰，任务协同更加高效时，才能真正实现免打扰。我们的目标更加清晰，我们阶段性重点也更加明确，通过“communication”的基础，再结合AI和数据，我刚才提到的消费方式升级也是可以达成一致的。当然，在产品方面，现在大家都在双向奔赴的过程中。</p><p>&nbsp;</p><p>InfoQ：因为现在所有话题都是围绕着AIGC，钉钉产品层面您也谈到，比如说我们有 80 种场景化的应用，您认为产品的变化过程中，AIGC给钉钉带来的最大价值或起到的作用是什么，有没有带来什么颠覆性的大变革？还是我们常说的辅助或降本增效的作用？</p><p>&nbsp;</p><p>程操红（巴布）：这个话题我认为需要分解来讲。实际上，我刚才也提到了，产品的变化有三个不同的阶段或模式，我们目前只是将这些阶段同时推进。目前，人工智能所带来的第一个影响是，很多人的首要想法是立即将AI应用于产品中，就像之前所说的，将发动机立即安装到马车上一样。这种方法确实能够起到一定的普及作用，但它并没有解决根本问题，它只是让大家对AI有了一定的了解。</p><p>&nbsp;</p><p>第二种方法，像刚才提到的“copilot”。加上“copilot”可能会更好一些，它能让大家感受到AI的价值更大一些。比如说，以前在考勤规则方面，可能需要学习各种排班方式，这些可能会很复杂。如果软件的培训不到位，就可能无法正确使用。现在，通过copilot模式，基本上你只需通过自然语言描述你的需求，比如说我是个工厂，需要设置三班倒，它就能帮你完成。我认为这是一种进步。包括我们的宜搭，从专业开发转为使用低代码，可能要么觉得看不上要么觉得还不错，因为这确实降低了难度。但对于完全没有编码经验的纯业务人员来说，要学习一些专业术语，如公式编辑、表头表尾、数据集等，可能涉及少量的JS，就会有些困难。</p><p>&nbsp;</p><p>现在通过copilot，你可以说：“我画了这样一个表格，并拍了一张照片，你能帮我生成吗？” 它就帮你完成了，这样的方式确实方便了许多。但我认为这很难说它是颠覆性的。它只是说整个浪潮来了，我确实拥抱了，并做了一些相应的调整。</p><p>&nbsp;</p><p>真正颠覆性的改变我认为一定是在“Agent”的形态中。为什么说它不一样呢？因为在本质上，以前的改造只是局部的，你的软件或者是你加上copilot等工具，它依然是工具的属性。但Agent真正具备的拟人属性是让AI拥有了记忆功能，对你的偏好有了更深入的了解。</p><p>&nbsp;</p><p>比如说，上一次我使用这个Agent让它帮我写文档。我对它提出了很多要求，比如：字号要用小四号字体，使用钉钉进步体字体，我希望每一个结尾都不要再总结了，有点啰嗦。我曾经告诉过它一次，那么下次我让它写日志或者帮我写其他东西时，它能够从之前文档的场景引申到日志或周报的场景。它会记住我的偏好，并认为这些偏好在写周报时同样适用。</p><p>&nbsp;</p><p>因此，这是一个非常重要的变化，即Agent的这种拟人化形态，它能够了解你的偏好，并具有记忆功能，然后能够跨系统去做行动系统的调度。我认为，尤其是在现在引入工作流之后，这个变化可能并没有颠覆性那么严重，但它确实完全重构了原来的软件开发和产品使用形态。对于这一点，我仍然持有坚定的信念。</p><p>&nbsp;</p><p>InfoQ：那在钉钉的这些基础功能里面，您能举点例子吗？</p><p>&nbsp;</p><p>程操红（巴布）：第一个例子就是，比如嵌入式的，你在IM中发送一张图片，可以立马点那个小尾巴，这是图片理解的一个例子。当然，类似的例子还有很多，比如在思维导图中，你写下一个组织团建活动的计划，然后请求它为你扩散，给你灵感，这属于嵌入式应用，在原有的软件中，你会感受到不同的使用体验。</p><p>&nbsp;</p><p>比如在考勤软件中，旁边就有Copilot，它可以帮助你编制考勤规则。如宜搭Copilot，你遇到什么不会用的，仍然在使用原来的拖拉拽模式，但是因为你不会用，你可以随时向它询问，比如说，“这个地方怎么弄？”、“这个故事怎么写？”、“你能帮我写一下吗？”这就是Copilot的辅助功能。现在，Agent的应用越来越广泛，你可以看到有紫砂壶大师、摄影搭子、相亲红娘等。它会时不时与你聊天，发现一些有趣的新闻热点，越来越了解你。它会在了解你的过程中启发你，当你遇到困难时，会给予你安慰，并记录这个过程。未来，当你寻找对象时，它会知道你对某些事情可能比较敏感，这种感觉就会有所不同。</p><p>&nbsp;</p><p>然后，我们现在有一个比较高频的Agent，之前我们内部有一个获奖的，叫做“搭子”，它涵盖了从MBTI性格测试到相互匹配以及各种信息的链接和分享协作等功能，都可以由它完成。此外，我们还有一个很有趣的项目，就是我们的运维助理。以前，技术同学工作相当辛苦，当云上的机器链路出现问题时，他们经常需要自行分析。但现在，如果我们使用了这个运维助理，我们正在不断打磨它，目前的效果相对来说已经可以看到了。它会自动时刻分析这些异常情况，而且你还可以不断向它灌输新的知识。比如，你可以告诉它将某一类信息合并，或者结合资源负载再进行分析，这是一个不断进化的过程。案例非常多，Agent Store发布之后，你肯定能找到一些你觉得好玩的。</p><p>&nbsp;</p><p>InfoQ：这里面您觉得自己就是钉钉原创度最高，或者说我们的技术门槛最高，可能成为我们护城河的技术能力是什么？</p><p>&nbsp;</p><p>程操红（巴布）：我认为实际上AI的护城河门槛应该也不是在纯技术层面，但需要有懂得这项技术的人迅速拥抱这个理念。目前，大型模型本身确实存在一定的壁垒，但我预计再过一两年，这些壁垒也没有那么高。另外，在商业竞争下，又不断有开源项目涌现，因此，再过一两年可能又会成为了基础设施，并且在其之上有很多与AI相关，像AI OS这些，就技术壁垒而言，我认为也没有太大的问题。</p><p>&nbsp;</p><p>关键在于如何培养具备AI思维的人才，以及是否有足够的框架和资源支持。所谓的资源，指的是什么呢？比如说，我们的Agent最大的特点就是具有记忆能力。那么，这个记忆是从哪里来的呢？Agent具有行动能力，但这种行动能力背后的行为又是如何产生的呢？因此，生态系统的构建和持续供给非常重要。我认为，钉钉的核心（壁垒）就在于我们的Agent。</p><p>&nbsp;</p><p>实际上，从纯粹的Agent框架角度来看，我认为大家都差不多，包括外部的一些舰队如扣子、GPTs也是如此。本质上，它们之间的差异并不大。主要是工程上的时间先后问题，但可能相差也不会超过两个月。</p><p>&nbsp;</p><p>问题是，如果你要想让这些Agent具备偏好，那么这些偏好数据又是从哪里来的呢？钉钉拥有大量的用户收藏和聊天，用户在授权后，可以将这些数据提供给他们的助理。在钉钉上，我已经写了大量的日志和文档，这可以作为我的原始记忆和偏好的来源。此外，钉钉还保存了我的好友关系。对于企业而言，钉钉还保存着大量的组织流程、审批流程、业务处理流程以及规章制度和营销政策等信息。至于感知能力，可以通过数据指标了解到各种业务事件从哪来。你有个协作的网络，但你不在这个网络上，那你会有这个感知吗？比如，你知道今天有谁迟到了吗？你知道哪个合同流程超过三天还没有批准吗？钉钉知道。</p><p>&nbsp;</p><p>所以，我认为，AI的思维比技术本身更重要。而且，背后支撑着大量高质量数据的供给比技术本身更为重要。</p><p>&nbsp;</p><p>InfoQ：所以最终还是要回到数据层面来。</p><p>&nbsp;</p><p>程操红（巴布）：如何挖掘场景、提高数据密度、提升数据质量，并使其成为Agent不断吸取的养分，这些问题至关重要。数据是核心的生产要素，但为什么以前没有解决这些问题呢？首先是因为数据的质量和密度不够。而质量和密度不足的原因是场景覆盖不够全面。许多情况并未被数字化，导致无法被充分记录。举个例子，今天我们两个进行了一次深入的交流和讨论，你唯一记录下来的可能只是录音和视频。这其中就蕴含了大量的观点、事实、信息和知识，但如果没有被加工处理，这些数据除了被浏览之外没有任何其他用途。就比如说我们这次的视频访谈，如果有别的媒体给我电话，我们实际上可以将这个采访内容交给助理，由助理来回复。</p><p>&nbsp;</p><p>所以说我们的核心竞争优势并不在于AI技术本身，而是在于背后的数据，包括协作网络的数据密度。这一点非常重要，也非常有意思。以前很难实现这一点，但是AI技术可能已经做到了。</p><p>&nbsp;</p><p>InfoQ：那在我们跟企业的合作过程当中，怎么样去帮助他们把这个数据的密度以及质量提升上去？</p><p>&nbsp;</p><p>程操红（巴布）：这是一个非常好的问题。我刚才提到钉钉从协同办公平台逐步延伸到应用开发平台时，实际上我们非常想做这件事。一方面是源自于市场，许多组织和企业都要求进行数字化升级。当然，数字化升级本身旨在实现更好的组织治理、开拓更好的业务模式以及提高组织效率，对吧？这一点是毋庸置疑的。但其中的手段就变得至关重要了。如果都是通过购买专业软件，成本太高；而定制开发又太麻烦，而且很多时候无法完全匹配自身场景。因为我们现在购买的现成软件，比如国外的那些，价格昂贵，而且匹配度并不总是很高，需要大量的定制开发。</p><p>&nbsp;</p><p>然后如今我们所处的市场环境变化速度很快，业务模式经常变化。你刚刚可能还在开设门店，现在却已经开始进入各种后电商时代。业务模型模式变化太快了，你原来那套营销的体系行吗？可能需要进行大规模升级，而且还要强调你的独特性。因此，在这样的情况下，几年前钉钉就开始推出低代码，这项技术确实解决了很多人的问题。我们的低代码应用数量也在迅速增长。</p><p>&nbsp;</p><p>但我认为，低代码虽然解决了一部分问题，但背后的挑战还远未解决。比如，以前可能有 100 个场景，专业软件能解决其中的大约 10 个，而低代码能解决大概 40 到 50 个，但还有约 50 个场景未得到解决。如果我们站在未来的角度来看待，或许三年后再回头看 AI 技术，可能会发现它有更大的潜力。也许在我们现在的视野中，我们仅解决了不到总问题的 10%，还有 90% 的场景未被覆盖，这里面更多维的信息更关键，而这些信息目前尚未被涵盖。低代码是一条路，可以覆盖一部分信息，而如今 AI 则提供了更多的可能性。</p><p>&nbsp;</p><p>我们之前做数字化有个很大的制肘，即要结构化数据。因为原来的信息系统大多只能处理结构化数据，对于非结构化数据，我们只能叫做非结构化数据的“存储”。谁能说自己在非结构化数据处理方面做得非常好呢？事实上，“处理”相对来说是比较困难的。虽然后来在图像识别等方面取得了一定进展，但仍然存在大量信息无法被系统准确捕捉的情况。比如，在人们交流过程中，系统无法理解为什么某人在某个时刻皱眉或微笑，这种情况下系统完全无法感知。因此，早期存在大量细节和数据的损失。</p><p>&nbsp;</p><p>因此，AI 到来之后，由于其多模态性、以及Agent的记忆和行动能力、能够像人类一样“操作”系统、能够复用原来的存量系统，还带来了更好的数据连接和协同模式，参与到了协作网络中。我认为，我们无法确定它能填补多少空白，但它的确开启了一轮新的数字化场景覆盖。这个观点是肯定成立的，即它将扩大覆盖范围。因此，这也带来了刚才提到的问题，即数据的密度和质量必定会迅速提升。只有解决场景覆盖问题，才能解决数据质量和密度问题。</p><p>&nbsp;</p><p>然后，一旦我们使用AI解决了数据质量和密度问题，再结合Agent以及大模型来处理数据，无论是作为预训练素材、记忆还是感知的指标，从另一个角度来看，这将促进场景的价值感进一步增强。因为一旦你认识到数据的价值，你的报销流程可能会变得更加高效，你的财务管理可能会发生重大变化，你的营销策略可能会有很大改变，你销售员的培训、销售策略的制定以及销售资源的部署配置都会动态升级。这将形成一个非常良性的正向循环。实际上，AI就像是在低代码的飞轮上再添加了一个巨大的飞轮，用于推动整个数据质量密度的不断提升，这就构成了一个正向循环。</p><p>&nbsp;</p><p>InfoQ：您觉得这个关键的转折点大概会发生在什么时间点上？</p><p>&nbsp;</p><p>程操红（巴布）：这取决于我们的努力程度了。这个时间也很难预测。如果按照我们现在对产品的规划和预测，我认为今年可能会迎来比较大的变化。因为如今AI仍然存在一点不确定性，所以我们正在尝试通过工作流程、人工参与以及Multi-Agent ，尽量减少每个Agent 的职责范围来优化效果。</p><p>&nbsp;</p><p>实际上，还有一个问题就是整个社会对AI的接受程度。因为AI与原来的软件系统是不同的，如果你期望AI像以前的软件系统一样百分之百确定，那可能会很困难。所以在组织治理和如何使用AI来构建新机制方面，（你可能需要思考得更多）。举个例子，就像我们在编程时，即使有两个专家参与，对于最核心的系统，像我以前在天猫交易时那样，即使一个人完成了工作，也要double check，尤其是在双十一这样关键的时刻，不是 double check，是三个人check，否则出了问题后果就会很严重。</p><p>&nbsp;</p><p>因此，在如何使用AI方面，我们不能简单地套用以前使用软件系统的思维方式、管理模式或者容忍程度，而是需要思考更为重要的事情。比如，我们要考虑如何合理地配备三个Agent ，或者人与Agent 的协作方式。我们可能需要在这方面进行一些深入的思考。所以这一切取决于：一方面是在基础产品层面，我认为今年肯定会非常重大的一些发展；另一方面，是整个市场环境以及大家对AI使用方式的认识。就像汽车问世后，人们可能会觉得油价太高，或者噪音有些大，这些都是需要解决的配套问题。</p><p>&nbsp;</p><p>InfoQ：您有提到AI思维变化，现在大家也提到未来我们跟计算机或者说移动应用的这种交互模式也许会发生改变。在这种前提下，有一些媒体报道说钉钉的目标是下一代互联网入口，那可不可以介绍下我们的规划或可能的实现路径是什么样的？</p><p>&nbsp;</p><p>程操红（巴布）：我们没有提出我们要做互联网的入口，实际上我们是说钉钉是一个超级AI助理平台。我们更希望这个平台，实际上我认为它不是一个入口，而更像是一个底座，我们的目标是如何将这个底座输送到千行百业里去，并在他们之间来构建网状的链接。实际上，现在很难做互联网入口的层面了。</p><p>&nbsp;</p><p>以后入口会无处不在，因为像硬件各方面，包括企业自身的许多业务运营，实际上都有自己的入口。他们可能只是利用钉钉这个底座。有些甚至根本就不再称之为钉钉了，比如浙江他们把钉钉称为浙政钉，一些大客户已经对其进行了改进。</p><p>&nbsp;</p><p>因此，我们更关心的是如何使钉钉成为一个底座，将搭载了AI、数据消费以及产品信息形态等能力的平台，作为底座输送到不同客户的业务和组织治理网络中。这一点非常重要，如果我要表达我的理解的话，我认为这是一个非常重要的使命。特别是我之前提到为什么AI技术不是门槛的问题？当你建立了这些能力，并且构建了一个良好的生态系统，一个让所有参与者都能在其中投入和获得回报的生态系统。同时，在这个生态系统中，钉钉可以帮助解决那些对个体和小型组织来说难以实现的任务。然后，构建一个能够在整个产业中实现协作和网络效应的底座，这可能是我们的重要使命之一。</p><p>&nbsp;</p><p>InfoQ：所以其实从这个角度来看的话，钉钉其实是一直在慢慢往后退的。就是把前面的舞台交给我们的客户，然后我们是提供一些底层的能力。</p><p>&nbsp;</p><p>程操红（巴布）：有时候，你可能会觉得钉钉在某些方面会做其中的一些工作，比如现在的AI助理也参与其中了。但实际上，这只是引发了大家的关注，是一个打样的过程。我们告诉大家，实际上这个事情是我们想要引导的一个方向。其实，生态里的有些客户甚至让我们感到惊讶，他们对这个理解比我们还深。他们可能会在这个基础设施上做出一些让我们都感到惊喜的东西。</p><p>&nbsp;</p><p>我们仍然希望以润物细无声的方式推进，我觉得跟第一波互联网纯入口的感觉还是有蛮大差异的。但是，我们更希望我们的工作能起着无处不在的作用，这并不是说我们想要控制一切，而是当网络效应像滚雪球一样不断扩大时，当数据的密度和质量不断提高时，这样一个高度协同的网络实际上会产生一些非常有趣的事情。它将使AI的效果和价值更好和更强大。</p><p>&nbsp;</p><p>InfoQ：这种高度协同，以及我们一直在强调这个生态，其实意味着说我们要去跟更多的合作伙伴，包括把这个能力赋能给企业的时候，他们内部所有的这些系统应用都是需要去打通的。</p><p>&nbsp;</p><p>程操红（巴布）：所以我们要降低这个打通的门槛，我们要降低数据消费的门槛，我们要降低产品往 AI 化改造的门槛。这就我们现在现实中不得不面对并且必须要去做的事。</p><p>&nbsp;</p><p>InfoQ：降低这些门槛其实就是基于您刚刚介绍的低代码、AI 的这些能力去实现？</p><p>&nbsp;</p><p>程操红（巴布）：对，包括 Agent。刚才讲为什么它的业务集成模式变了？为什么我们想让 Agent 像人一样能学会操作存量应用，这都是降低门槛。否则在当下的产业环境里，人们会考虑投入后是否能立即获得回报，这是一个需要考虑的现实问题。</p><p>&nbsp;</p><p>InfoQ：过去大家讲UI，现在大模型来了，大家都开始讲 LUI 革命。那钉钉下一个或者说未来的版本交互界面，会有什么样的变化吗？有没有初步的一些想法？</p><p>&nbsp;</p><p>程操红（巴布）：这种变化其实我觉得正在发生。为什么呢？我们实际上从两年前就开始开发一种名为酷应用的工具，就像我刚才提到的百丽是做得非常出色的那种，他们将所有协作活动都集中到群组中。另外，我认为目前复杂软件存在一个很大的问题，那就是使用成本很高，而且它不是跟你的场景紧密相关的。举个形象的例子，就像当你口渴时，你希望旁边就有一瓶水，而不是要跑很远去买水，或者跑到冰箱去拿水。对吧？当我感到疲惫时，旁边就应该有一个靠垫，可以立即靠一下。所以，我们应该让更多的应用程序结构打碎，从而能够更灵活地适应各种情境，与情景紧密相关。</p><p>&nbsp;</p><p>比如，我就想调货，你就给我一个调货的按钮，调货就好了；当我需要申请增加库存数量时，你可以给我一个简单的增加库存数量的选项。目前大多数软件可能都过于复杂，我进入后需要费一番功夫才能找到所需的按钮或菜单。这就是为什么我们要回到之前的话题，即数据密度的问题，因为软件的使用成本太高。我们需要让应用功能自动找到我们，而不是让用户费力去找。所以，我们之前引入的酷应用，AI其实就是这种理念的延续。酷应用会主动为你提供所需功能，我们只是把它输送到了你的手边。那么，AI是什么感觉呢？就像你说口渴了，水就自动滴到你嘴里一样简单。所以，这种体验又是一个进一步的提升。</p><p>&nbsp;</p><p>实际上，在过去大约两年的时间里，我们一直在努力鼓励我们的生态系统和客户，将那些复杂的软件进行解构、打碎，并将其融入到钉钉的文档、IM甚至音视频会议中。比如，当我在开会时需要查找一些信息，只需轻轻点击一下就能得到，是不是很方便？我们实际上一直在鼓励大家这样做，因此我们在底座上进行了大量的开放。只要你想使用这个底座，它就是开放的。除百丽公司的例子之外，还有很多其他类似的客户，包括我们自己的生态系统中也有很多这样的应用，但是我们认为还有一步之遥，就是尽管（那个功能）它已经近在眼前，人们有时候仍然会忘记使用它，即使它就在那里摆着。</p><p>&nbsp;</p><p>因此，我们的第一步需要将该功能集成到群组中，也就是所谓的酷应用；第二步则是通过AI，使其能够直接理解自然语言、语音甚至视频，你是否经常遇到这种情况呢？一个德高望重的人给你发来一段视频，即使他什么都没有说，你也要再稍微理解一下吧？实际上，在这种情境下，AI可以介入了，AI可以提出三条建议，让你能够更好地回应。这种感觉不同于传统软件的使用体验，因为它完全融入了你的实际场景之中。</p><p>&nbsp;</p><p>InfoQ：其实可以这么理解，就是把 AI 当个人。</p><p>&nbsp;</p><p>程操红（巴布）：在谈及使用界面时，如果我们讨论GUI与技术问题，但我认为深层次的变革在于什么呢？那就是你将其视为一个个体，而且这个个体是与你贴身相伴的、时刻为你服务的。以前如果投入了大量的IT费用，可能只有领导层能享受到这种待遇，能给他们提供非常好的数据洞察，但也不是给更多的建议。而现在不同了，每个人都能享受到这种贴身化的服务。</p><p>&nbsp;</p><p>因此，我给我的团队设定了一个目标，就是今年上半年每个人至少要创建一个助理，必须得有人使用。第二个目标是至少要使用六个以上的助理，除了默认的助理，还必须学会使用它们，如果你不会使用，那你的思维方式就没有改变。</p><p>&nbsp;</p><p>你刚才很好地总结了交互形式的变化，实际上就是把它当作个人来对待。其实如果你想想，这与软件市场很相似，也非常有趣。比如说真正的公司高层管理者很少使用系统。他们更倾向于让别人为他们完成一些任务，比如要求人事部门导出薪酬分布数据。实际上，这是人的本性。我认为每个人都喜欢这样，就像我也不喜欢费时地在系统里搞一堆操作再提交。当你躺在三亚的海滩上时，你肯定会简单地告诉助手：“明天我想回去了，请帮我订张机票。”这样最方便了。现在我们就希望系统能变得更加贴近这种方式，这是远近形态的最大变化，就是变得更加便捷。</p><p>&nbsp;</p><p>InfoQ：鼓励合作伙伴和生态伙伴将他们的应用程序结构化，并将其置于底层系统之上，是否可以理解为当前所描述的No App，换句话说，它实际上是将功能解构为一个个的组件？</p><p>&nbsp;</p><p>程操红（巴布）：解构为 Agent 了。那么类似于No App的概念是什么呢？就是在未来，随着人们对Agent接受程度不断提高，那些以核心入口为导向的App可能就价值没那么大了。比如，当我真的饿了，想吃饭的时候，我可能主要是告诉我的助理让他帮我订外卖。在这种情况下，我还会自己打开某些App自己点吗？不会的。这是为什么这种情况会逐渐成为现实。正如我之前所说，随着Agent对你的了解越来越深入，它可能知道你最近感冒了，嗓子不舒服。它会跟你说：“虽然我知道您很喜欢吃香菜，但是我了解到您最近可能嗓子不太好，而且明年还有演讲，我建议给您点一些清淡的食物，您觉得可以吗？清淡的食物中也有一些口味不错的，比如海鲜粥，您觉得如何？”</p><p>&nbsp;</p><p>在这种情况下，你认为手机上装载的大量App还有机会吗？因此，这也解释了为什么我刚才对“互联网入口”这种说法不太赞同。本质上我们可能在努力构建最好的Agent，这可能是我们的核心目标，我们不断提升组织和个体的最佳经验和认知，驱使它发生变化，这可能是更重要的事情。</p><p>&nbsp;</p><p>InfoQ：那未来 App Store 会不会变成 Agent Store？</p><p>&nbsp;</p><p>程操红（巴布）：是的，所以当前的竞争格局可能会发生一些变化，不再仅仅是应用厂商之间的竞争，也不再是各种互联网入口之间的争夺。未来，诸如硬件入口、汽车入口等，手机制造商可能都会做Agent store。</p><p>&nbsp;</p><p>因此，回到我们上面讨论的问题，Agent背后的核心能力和核心壁垒到底在哪里呢？我认为还是数据和协作的方式，协作的紧密程度和效率可能是至关重要的。</p><p>&nbsp;</p><p>因此，最终的壁垒在于产品形态，你是否能够率先进行升级？你是否能够迅速在你的生态系统中建立起数据消费的方式和良性循环？并且，协作的支撑是否能够充分实现？换句话说，人、硬件存量系统和 Agent ，你是否能够将它们处理得非常出色？这个合作密度的习惯，从习惯到协作本身的方式，以及多模态载体的支持，再到信息加工的效率，这些方面可能是综合起来构成的壁垒。</p><p>&nbsp;</p><p>InfoQ：那您提到的这个钉钉的底层系统，它未来会不会就是一个 Agent Store 的形态？</p><p>&nbsp;</p><p>程操红（巴布）：是的，钉钉其实是个底座。在这个底座之上，随着创建的Agent越来越多，我认为自然需要一个Agent Store，让人们从中获取Agent，并促使行业之间进行一些分享和交流。一种是个人创作的，比如紫砂壶大师、摄影搭子、 AI 红娘之类，这本来就是跨网络的，因为钉钉的网络不仅局限于一个组织内部。钉钉的组织协作有两种方式：一种是纯粹在组织内部的协作，你可以将其边界限制在组织内；另一种是跨组织的产业链协作，甚至可以是全社会范围的，比如我们不在同一个组织，但我们也可以成为好友，然后你的助理也可以加入我们的群聊，从而促进合作效率。</p><p>&nbsp;</p><p>InfoQ：目前 Agent Store有什么初步的规划吗？</p><p>&nbsp;</p><p>程操红（巴布）：4月18日推出的助理市场，涵盖我们生态系统中的一些Agent，其中包括许多开发者、客户，以及钉钉自己做的。目前大家对此的热情相当高，我们同时还在举办大赛。</p><p>&nbsp;</p><p>这一波推出的更多是让大家体验Agent 的形态差异以及它所具有的一些特殊特性。当然，我们的产品在过去两个月里一直在持续升级，包括工作流的升级和记忆能力的升级。我认为未来会变得更加有趣，Agent 能够完成的复杂工作将变得更加多样化，我对此非常期待。</p><p>&nbsp;</p><p>InfoQ：在这里面怎么调动生态、开发者以及内部员工的积极性？</p><p>&nbsp;</p><p>程操红（巴布）：这三种情况完全不同。对于内部员工来说，特别是那些特别拥抱AI的人，他们基本上不需要被推动，他们自发地做得特别多。我也经常自己去做Agent。然后，第一个Case就是我自己做的。我记得去年那个时候，当其他人还在观望的时候，我就已经做了一个。有些人可能想专注于本职工作而持观望态度，所以我们强制要求，如果不行就进行考核，意思是你必须去做。现在我们已经都接受了，大家调动了起来，所以最近两周我们又做了150个Agent。里面有些特别特别好的，人们的创造力是无穷的。对于员工这一侧，我们采取了软硬兼施的方式。</p><p>&nbsp;</p><p>对于客户来说，我觉得探索共同价值是非常重要的，因为钉钉上的客户共创精神一直保持得很好，一旦你发布了一些东西，一些特别活跃的客户立刻就会联系过来。他会根据场景进行结合，因为客户实际上是一个庞大的群体，其中有许多活跃的个体，有些人有很多创意，包括杭州市公安局，他们做了三个作品，非常令人称赞，涉及市民服务等各个方面。还包括农业等领域都有。总之，许多客户都在共同创造。另外，他们本身肯定也关注到这种变化，特别希望在自己的行业或工作环境中找到一些突破点。</p><p>&nbsp;</p><p>至于生态系统，我认为我们需要认真考虑商业化机制的设计，因为每个人在生态系统中，说白了，都是在创业，好多生态都是拿着自己的身家在创业。所以，从本质上来说，我认为你可能不能仅凭个人感觉来要求他们，你还必须从商业健康的角度来看待生态系统。</p><p>&nbsp;</p><p>因此，我们需要逐步引导他们，包括为什么之前要做酷应用，包括现在为什么要做Agent，我们要一步步地引导他们。我认为他们的做法首先是对产品本身进行升级，这在中长期内对大家都是有益的，但短期内，他们会获得什么好处呢？那我们会加大对这些Agent的投放和商机引导。</p><p>&nbsp;</p><p>因此，无论是做酷应用还是做Agent，其本质目的是让更多钉钉用户和群体了解他们的解决方案，这样他们的思维就会更加清晰，觉得你在同步做非常好的升级。</p><p>&nbsp;</p><p>其次，如果他确实发布了一些高频的Agent ，针对特定场景，这将会增加客户对他的兴趣，无论是续费还是新增购买。我觉得还是以商业为抓手，我们的开放平台机制将与之配合，我们的 Agent Store本身也是由开放平台团队和运营团队统一运营。</p><p>&nbsp;</p><p>InfoQ：客户方面，他们首先必须具备业务需求。过去不管是信息化阶段还是数字化阶段，大家明显感受到的是只要效益还可以，就不愿意去做变革。所以现在客户会主动寻求改变的核心驱动力是什么呢？</p><p>&nbsp;</p><p>程操红（巴布）：我认为情况可能不完全如此。一方面，你提到的是其中一种情况，另一种情况是已经尝到了甜头。比如，在江浙地区，你会发现政府里很多人都非常有动力，为什么呢？因为他们已经享受到了数字化带来的好处。他们觉得必须采取新的方式来（处理事务），相信这样做一定会取得出色的成绩。这种态度完全受到组织文化的影响。当然，还有很多客户，他们只希望不出错就好，这由一些单位的属性和性质，包括文化所决定，因为很多事情是无法容忍错误的。</p><p>&nbsp;</p><p>浙政钉呢，他们感受到了数字化建设带来的好处，现在每天有超过160万人在上面使用。因此，他们鼓励这种氛围，要求你进行创新，开发一些应用程序。有部门有时会工作到凌晨一两点，比我们还要辛苦，这是他们的动力所在。</p><p>&nbsp;</p><p>还有一些公司可能是因为当前的模式已经到了一定阶段，他们觉得确实需要探索一些新的模式。他们也面临着一些痛点，比如在农业种植领域，如果他们在大棚种植方面做得很好，那么如何将这种经验复制到其他基地呢？他们发现这是一个需要解决的问题，传统的管理方法可能难以应对。此外，他们还需要考虑市场需求，如何将市场对蓝莓酸甜度的差异传达到生产基地呢？在这方面，他们会想办法采用数字化或智能化的方式来解决问题。</p><p>&nbsp;</p><p>前期我们也会寻找一些相对确定性较高的点。例如，像问答这样的功能，因为许多企业拥有大量的知识，就像电子类型的客户，由于他们专业知识很强，而且涉及的物料也很多，一般人难以搞清楚。因此，对于员工的内部培训和客户的问答，“智能客服”就可以让人们能够立即感受到好处。所以，他们也愿意投入使用。</p><p>&nbsp;</p><p>InfoQ：过去大家实际上是比较愿意投资于创新的。但是在过去的两年里，经济形势可能面临一些挑战，人们会非常关注投入产出比。根据公开资料，去年钉钉大模型平均单次调用的成本只要五分钱，钉钉之前有表示有6-7亿用户，并推出了个人版，这种情况下，如果大规模使用，应用成本和对应的商业价值如何去做平衡？</p><p>&nbsp;</p><p>程操红（巴布）：成本肯定是要考量的。但说实话，目前这个阶段我们更加鼓励客户在场景上与我们一起创造一些高价值的引导式样板。实际上，我认为这对整个产业的变革也有着巨大影响。</p><p>&nbsp;</p><p>因为说实话，钉钉一直以来，包括阿里本身，在这方面一直都有投入。我们也希望在AI方面能够拥有更多的生态伙伴，我们对生态也提供一些补贴。对于生态中完成了AI化重构的话，我们在佣金上也有专门的策略，会提供一些补贴。另一方面，目前我们在人工智能的使用上并没有进行过多收费，这是一个当前的情况，但从中长期来看，这显然不是一个能够持续长久的模式。在中长期内，我们与通义团队合作，同时还有其他的一些第三方模型，我们肯定会在保证模型本身一定性能的情况下考虑这些成本构成。</p><p>&nbsp;</p><p>最初我们是说五分钱，但现在模型的尺寸不断增大，有时甚至超过了五分钱，达到了一毛二。然而，一方面我们没有向客户收紧，另一方面我们会持续优化模型的使用策略，包括在工程层面。举例来说，如果你总是向系统提供相似的图像，就没有必要每次都使用大型模型。此外，在混合模型的使用方面，我们也有一些规划。在某些相对确定的场景中，其实不用那么大尺寸的模型，或者可以使用相对较小尺寸的模型来替代，这实际上将成为未来一个非常重要的成本优化策略。更重要的是，我觉得开源节流是一方面，另一方面是要创造价值。当AI的价值能够得到客户的真正认可，能够节省大量人力、物力和资源浪费的时候，它带来的直接体验和付费意愿肯定会比以前更好。我认为这方面比起以前的SaaS肯定更好。</p><p>&nbsp;</p><p>InfoQ：目前在应用AIGC的过程中，收到的您认为最有价值的客户/用户反馈是什么？</p><p>&nbsp;</p><p>程操红（巴布）：我认为一方面是每个人的体验可能不太一样，但从实际数据的客观分析来看，最常使用的功能肯定是消息总结。因为大家的信息有点过载，尤其是对于重度用户来说，他们的消息总结和每日小结目前效果还相当不错。当你在一个群里有很多未读消息时，或者当你需要回顾昨天一些重要任务时，它可以为你提供一些相关日程消息的重要摘要和回顾，这是一个非常重要的功能点。</p><p>&nbsp;</p><p>其次，在IM中，快速传递信息也是非常重要的，比如刚才提到的图片小尾巴功能，无论是发送任何文档、日志链接，甚至是长篇文章，它都可以帮助你快速浏览阅读，而这个功能目前的使用量也相当大。</p><p>&nbsp;</p><p>第三个方面是，大家开始尝试一些类似于AI助理的创建。因此，你会发现在钉钉上AI助理这一类应用特别受欢迎，尽管我们并不完全面向To C，但大量的AI助理应用正在不断涌现。</p><p>&nbsp;</p><p>随着大量AI助理的涌现，人们也开始尝试创建各种熟悉的模式。他们会创建许多小助理，比如帮助孩子写作文的助理，每个助理都有自己的风格，因为五年级和六年级以及初中的需求肯定是不同的。他们可以制定一些适合自己的规则。比如，我自己创建了一个数据参谋，可以查找各种热点信息，还可以查找内部的一些关键信息。这样的助理包括一些关键词的自动探测功能。实际上，在助理的探索过程中，其数量也在逐渐增加，使其越来越受欢迎，并且发展速度会变得非常快。</p><p>&nbsp;</p><p>另外还有许多内置的功能，其实你可能无形中在用了AI，比如文档处理。文档编辑器内置了大量的智能功能，如帮助你润色、生成思维导图，甚至一键生成PPT等。这些功能在大量使用，特别是对于那些知识型工作者来说，他们现在能够很好地利用文档智能化功能。另外，在音视频会议中，我们还有一个叫做“闪记”的功能，它可以在会议结束后快速为你生成摘要，包括文字摘要和一些带有图片的PDF文件，使得对会议内容的理解更加迅速。这些功能的使用情况逐渐增多，针对不同的需求也各有偏好。</p><p>&nbsp;</p><p>InfoQ：我们一直在顺应大模型的发展，这对技术人员包括业务人员的能力要求是不是不一样了，对钉钉团队或人才结构是否有造成影响？</p><p>&nbsp;</p><p>程操红（巴布）：团队我觉得确实要发生变化，但是团队的变化也没有那么快。去年我们深刻地感受到了这一点。</p><p>&nbsp;</p><p>我认为这是因为整个软件开发范式正在发生变化。所以如何更好地理解、了解和探索当前大型模型的最佳能力特征，这是许多研发团队需要掌握的技能。你需要了解的是：大型模型现在具备了一定的 planning能力了，大模型开始能够处理声音了，大模型对说话语调的识别有了新的进展了......大家对模型本身的这种能力一定要有充分的了解。</p><p>&nbsp;</p><p>这就好比我们上一代程序员对操作系统的了解。你必须了解操作系统的许多特性，因为实际上操作系统的特性非常丰富。如果你不了解这些特性，那就很难操作。而且现在，这种了解已经开始演变，人们说以后不再是专业语言，而是文字语言，所以可能要把语文学好，在这个基础上prompt、微调。</p><p>&nbsp;</p><p>当然，并不是每个人都必须掌握这些技能，但是我们确实有一个专门的团队负责进行这种微调的工作。这需要具备算法背景，但更多的人需要理解大模型的工作原理。另外一个方面是，在大模型之上，需要进行软件工程或者应用工程，对于这方面的工程化工作，我们需要结合多方面的知识。换句话说，我们需要重新审视我们手头的资产，比如说在钉钉上，我们有宜搭，有大量的低代码应用，有开放的 API，有大量的 OA 审批流程，还有大量的数据资产。在工程的角度上，我们需要思考如何将最新的 Agent 形态与这些资产有效地结合起来，需要考虑如何将数据资产转化为 Agent 的记忆，另外，也需要思考如何将低代码的应用转化为 Agent 可以使用的动作。</p><p>&nbsp;</p><p>在这个层面上，我们需要有新的思维方式，而不是每天都在纠结如何创建低代码应用或者将应用发布到工作台，因为这显然跟不上发展的步伐。我认为这并不是要换人，而是我们团队内部在不断地引导和学习。</p><p>&nbsp;</p><p>为了实现这一目标，去年我们技术委员会持续在运营。去年我们曾经连续两个月举办AI 专场活动，每周都做全员的分享，学习Prompt、了解模型特性、熟悉基于大模型的编程体系和框架，以及如何结合钉钉的核心数据应用接口等，了解这些实践经验。在我们的千人技术大群里，我们首先鼓励大家将一些常规操作，如答题和发勋章等，转变为AI化的新形式，并不断影响他人。</p><p>&nbsp;</p><p>我非常认同一个观点，即我们应该转变对AI理念的认识。例如，当面对新的运维和容量规划痛点时，仍然有很多人想着如何构建应用、建立流程或进行流程审批。我认为这样不行。我们应该用AI的方式来思考容量规划应该如何进行。我们应该把现有的算力资源，比如说阿里巴巴达摩院通信实验室提供的资源，以及一些我们自己应用层的云上资源，引入一个“资源管理专家”的助理，让它来做更好的新型规划？</p><p>&nbsp;</p><p>原本运维可能会考虑开发一个系统，然后雇人在上面填补缺漏，但这样做是不对的。现在这位“资源管理专家”不仅可以进行容量规划，还能解答各种资源容量管理规则的问题。这样一来，原本需要一个团队成员每天回答各种问题，如“你为什么把我限流在 500 了？”，现在这个情况就不复存在了。现在他可以从前期咨询容量管理原则开始，一直到实际容量管理的先行规划，再到提出容量方面的建议。比如，他可能会在中午突然发消息给你，说他认为你的业务做得不错，你应该再申请500。这种情况下的感觉就完全不同了，是吧？因此，这实际上是一种思维和理念的升级。</p><p>&nbsp;</p><p>我们持续不断地进行这样的改变，会让一些人脱颖而出。在我们现在招聘人员时，我们也首先会考察几个关键点。我们会给你一个场景，并询问你用 AI 怎么解决，我们会考核你对于当前大型模型的主要进展、大型模型能力的理解以及一些 prompt 观念的理解，看看你是否有自己的心得。这肯定会改变人力模型。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：所以是从这个文化、组织、流程、对制度等所有的方面去不断的去提升和影响，</p><p>&nbsp;</p><p>程操红（巴布）：需要进行大量的工作，这涉及到我们内部的工作、技术运营以及各种场景。大家也在自行创建Agent，比如强制要求你去做，创业公司一定要吃自己的狗粮。总之，我们在推动这种变革时采取了软硬兼施的方法。</p><p>&nbsp;</p><p>InfoQ：最后一个问题，您对于AI未来还有什么样的预测吗？包括在这个 AI Agent 的这种趋势之下，未来软件的世界会发生什么样的改变？</p><p>&nbsp;</p><p>程操红（巴布）：我主要感觉就是关于“No App”的预测，因为我认为这种趋势会越来越明显。我认为未来肯定会有更多非常个性化的助理出现，甚至只需对手表说句话就可以了。</p><p>&nbsp;</p><p>另外，我认为对于个体和组织的变化来说，未来会有更多的小型组织出现。对于大型组织来说，可能会从资本层面或者控股层面，或者是从市场层面找一个维度。比如过去我们一直尝试着做的，类似于阿米巴那种模式的感觉，以前不是因为太复杂而难以实施吗？但在 AI 的支持下，我认为未来会变得更加灵活，也更容易获得充分的授权。</p><p>&nbsp;</p><p>这种小型的组织会出现，为什么呢？因为它能够更好地享受大的平台和大集团带来的便利，它在独立运作方面的空间会更大。有了整个 AI 助理的支持，有了更高效的数据流通，有了更好的协作，小型组织将成为一种趋势。</p><p>&nbsp;</p><p>未来 AI 助理将会完全拓宽其服务组织和个人的载体，其服务模式会变得更加丰富。就像现在为什么一个人似乎能够成为网红一样，有了视频或直播载体，就能够营造很多大咖。我觉得 AI 助理会使这件事更加多样化，可能不会达到那么大的规模，但是每个人都有机会，可能一个普通人也有机会，因为他懂得一些东西，比如知道如何养蚂蚁、怎么种蓝莓、怎么腌制最好的咸菜。那么他可以把这种技能作为助理，将这种多样化的服务传播到协作网络中。所以我认为未来会出现更多这种小型组织，甚至一个人就可以是一个组织，这是一个很有趣的发展。</p><p>&nbsp;</p><p>然后再深入地讲，我当然有一个更长远的想法。我认为，当你的数字化程度足够高，当你更多地参与了这个协作网络的 AI 助理时，实际上未来的资源配置、项目管理，以及许多现在我们需要人力的地方，可能相对来说实际上并不需要那么多人力。但即使如此，人类应该更好地去探索艺术、资源以及人类情感等更有意义的事情。现在很多事务性的工作，有人做得很出色，有人做的不好，这种工作分配以前取决于leader。我认为这种情况AI可以在其中提供服务，就像Transformer算法一样，他可以分析你的工作表现为什么这么出色，然后在下次安排工作时，优先选择你。AI能够更好地进行资源的配置和人员组织安排，这当然是一个更长远、也很有意思的想法。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cd81ENsq0fVxrI4UKYMV</id>
            <title>微软秒删堪比GPT-4的开源大模型！研发总部还被爆在北京？官方：我们只是忘了测试</title>
            <link>https://www.infoq.cn/article/cd81ENsq0fVxrI4UKYMV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cd81ENsq0fVxrI4UKYMV</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 09:38:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 发布前测试, 微软删除, 开源大模型, 毒性测试
<br>
<br>
总结: 微软发布了最新的开源模型WizardLM-2，但由于忘记进行毒性测试，不久后就被下架。这款模型拥有多个版本，其中8x22B是旗舰模型，拥有巨大的参数数量。尽管被删除，但仍有用户备份了模型地址。WizardLM-2基于混合专家架构开发，具有竞争力的性能表现。 </div>
                        <hr>
                    
                    <p></p><h2>因发布前忘了测试，微软删除最新开源大模型</h2><p></p><p>&nbsp;</p><p>上周五，Meta宣布推出了开源大模型Llama 3，以其卓越性能引发热议。而在Llama 3发布之前，微软也悄悄发布了最新的开源模型WizardLM-2。</p><p>&nbsp;</p><p>颇具戏剧性的是，这款模型在发布仅几个小时后，就被微软下架了，理由是在发布之前忘了进行“毒性测试”。</p><p>&nbsp;</p><p>据悉，这款大模型发布于上周一，提供三个版本：8x22B、70B 和 7B，每个版本都旨在满足不同的规模和要求。8x22B 模型是旗舰模型，拥有 1410 亿个参数，使其成为开源社区中最有效的模型之一。</p><p>&nbsp;</p><p>微软这次发完模型又删除的行为让很多网友表示困惑，因此微软开发人员在 X 上发布了一份声明解释了下架模型的原因。开发人员遗憾地承认了他们在模型发布过程中由于工作疏忽忘记了进行毒性测试。为了向社区保证迅速采取行动，他们承诺在重新发布模型之前立即进行必要的测试。</p><p>&nbsp;</p><p>还有外界消息称，WizardLM-2背后的研发团队总部位于北京。他们澄清表示：“删除该模型是由于忘记测试，而不是故意试图绕过审查”。</p><p>&nbsp;</p><p></p><blockquote>我们深感抱歉。&nbsp;距离我们发布模型已经过去一段时间😅，所以现在的我们对于新的发布流程有点生疏，不小心遗漏了模型发布过程中的重要一环——有毒内容测试。&nbsp;目前，我们正在快速补全测试工作……</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d898555c5c2c441224ee13f38790d6f.jpeg" /></p><p></p><p>&nbsp;大语言模型的毒性，是指其创作有害或不当内容的能力。如果在大模型中发现“有毒”内容，不单会影响技术方案的性能表现，更可能在全球各地纷纷对AI技术抱谨慎、甚至负面态度的背景下引发轩然大波。相关错误输出可能在互联网上疯狂自传播，甚至招来政府当局的调查。没有哪家公司愿意看到这样灾难性的经营事故。</p><p>&nbsp;</p><p>因此，该模型的所有文件均被从GitHub和Hugging Face上移除，访问相关页面现在会显示404错误。</p><p>&nbsp;</p><p>这款大模型是在&nbsp;Apache 2.0协议下发布的，在repo被移除之前，许多人已经下载了模型权重。但有细心的Hacker News用户还是将其发布地址备份保存了下来（地址如下）：</p><p>&nbsp;</p><p><a href="https://huggingface.co/dreamgen/WizardLM-2-7B">https://huggingface.co/dreamgen/WizardLM-2-7B</a>"</p><p><a href="https://huggingface.co/dreamgen/WizardLM-2-8x22B">https://huggingface.co/dreamgen/WizardLM-2-8x22B</a>"</p><p>&nbsp;</p><p>甚至在下架之前，部分用户已经在其他基准测试中对该模型进行了评估。那么，这款大模型具有哪些功能？与其他大模型相比性能如何？</p><p></p><h2>WizardLM-2“开箱”评测</h2><p></p><p>WizardLM是一套基于指令的模型，构建于Meta的Llama基础之上，属于研究人员使用生成的指令数据对Llama微调得到的产物。</p><p>&nbsp;</p><p>值得注意的是，WizardLM-2 基于混合专家 (MoE) 架构开发，利用完全由人工智能驱动的综合训练系统，增强其处理复杂、多语言对话和执行高级推理的能力。该系统支持模型在各个领域（包括写作、编码、数学等）提供精致且与上下文相关的响应的能力。</p><p>&nbsp;</p><p>该模型的第二个版本WizardLM-2是在Mistral AI的Mixtral 8x22B模型基础之上构建而成，并利用合成数据进行了微调。该模型家族共包含三大领先型号：WizardLM-2 8x22B、70B与7B。</p><p>与各领先的专有大语言模型相比，这些模型表现出极具竞争力的性能水平。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2f/2f4cdb01c71936d580dda1b018ecba5e.png" /></p><p>WizardLM-2 8x22B是其中最先进的模型，仅略微落后于GPT-4-1106-preview。70B在相同体量下达到了顶级性能，而7B版本则速度最快，甚至拥有与参数规模10倍于它的领先模型相当的性能表现。</p><p>&nbsp;</p><p>该模型利用AI模型生成的合成数据训练而成。微软公司在X上发帖指出：</p><p>&nbsp;</p><p></p><blockquote>随着天然存在的人类数据逐步被大语言模型训练用尽，我们坚信：AI精心创造的数据与AI分步监督的模型将是通往更强AI成果的唯一途径。因此，我们构建了一套完全由AI驱动的合成训练系统以增强WizardLM-2。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f831bcd83b5813b7064c3ae206b18b3.png" /></p><p></p><p>WizardLM 2 的训练方式（来源：模型启动页面，现已删除）</p><p>&nbsp;</p><p>在 MT-Bench 框架等基准评估中，WizardLM-2 展现出具有竞争力的性能，甚至可以与最先进的专有模型相媲美。它在现实场景中的应用较为广泛，从增强对话式人工智能到支持业务环境中的复杂决策流程。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/64/649e69b8962eda47698cb13ebbe01975.png" /></p><p></p><p>将 WizardLM2 基准与 GPT-4–1106-preview、Command R Plus、Mistral Large、Qwen 1.5、Straling LM 7B 进行比较。（来源：模型启动页面，现已删除）</p><p>&nbsp;</p><p>在MT-Bench中将WizardLM-2与GPT-4-Turbo和Claude-3等最先进的专有大模型相比，WizardLM-2 8x22B仍然具备极富竞争力的性能。同时，7B与70B也均成为同等参数规模之下性能最强的大语言模型。</p><p></p><h2>最近几年，微软的步子迈得太大了</h2><p></p><p>&nbsp;</p><p>Hugging Face 及其首席执行官 Clément Delangue 对删除表示失望，并强调了 WizardLM 的开源模型对其平台的重大影响。他们正在积极寻求与 Microsoft 的解决方案，以满足社区需求。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/15/155dce89cb42e506f8e5432e379a4323.png" /></p><p>随着故事的展开，人们的注意力转向了微软对负责任的AI实践的承诺。尽管该公司拒绝直接置评，但更新后的负责任人工智能标准一般要求的发布强调了其对道德人工智能开发的奉献精神，强调需要减少人工智能输出中的偏见和差异。</p><p>&nbsp;</p><p>实际上，当前几年还没有在AI领域展现出强大的统治力时，微软时常因产品的发布令人失望，甚至被贴上创新停滞和顶尖人才流失的标签。</p><p>&nbsp;</p><p>快进到 2024 年，微软已然成为了全世界最有价值的科技巨头之一。在首席执行官萨蒂亚·纳德拉 (Satya Nadella) 的领导下，微软股价在 10 年内飙升了1000% 以上。一月份，该公司的市值达到 3 万亿美元，超过了法国的 GDP 总和。</p><p>&nbsp;</p><p>能够让微软卷土重来的核心是人工智能。微软在 Azure 云计算平台、Office 生产力套件和 Bing 搜索引擎中嵌入了人工智能。而这一转变的关键事件是微软投资了OpenAI，并迅速其借助先进的人工智能技术成为了生成式AI时代的先行者。</p><p>&nbsp;</p><p>微软与OpenAI的合作始于2017 年，当初这家备受瞩目的初创公司在云计算上花费了大约 790 万美元——占其职能支出的四分之一，这让两者有了初步的接触。</p><p>&nbsp;</p><p>到2019年，微软已经成为AI实验室的“独家”云计算提供商。在向这家初创公司新投资 10 亿美元后，微软成为 OpenAI 商业化的首选合作伙伴。</p><p>&nbsp;</p><p>微软很快将 OpenAI 大语言模型 (LLM) 集成到 Azure 云服务中。客户使用该软件实现各种应用程序功能，从聊天机器人和内容生成到翻译和个性化营销。</p><p>&nbsp;</p><p>该服务增长迅速。今年第二季度，微软报告称，Azure OpenAI 的用户数量较前 12 个月增长了 50%。纳德拉表示，目前已有超过 53,000 名客户使用该服务，其中包括“一半以上”的财富 500 强企业。可以说，OpenAI 在微软的商业帝国复兴中发挥了关键作用。</p><p>&nbsp;</p><p>但借助OpenAI这一外力重新崛起的老牌巨头想要依靠自身实力继续保持领先，并且能在激烈的竞争中始终处于有利位置，却是件很难的事情。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://blog.stackademic.com/beyond-gpt-4-exploring-microsofts-wizardlm-2-2863e432f291">https://blog.stackademic.com/beyond-gpt-4-exploring-microsofts-wizardlm-2-2863e432f291</a>"</p><p><a href="https://favtutor.com/articles/wizardlm-2-benchmarks/">https://favtutor.com/articles/wizardlm-2-benchmarks/</a>"</p><p><a href="https://www.teiss.co.uk/news/microsoft-pulls-wizardlm-2-ai-model-due-to-missed-toxicity-testing-13873">https://www.teiss.co.uk/news/microsoft-pulls-wizardlm-2-ai-model-due-to-missed-toxicity-testing-13873</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JMyEPCDji2oGibDbZdzb</id>
            <title>提示工程、微调和 RAG，你应该选择哪一种？</title>
            <link>https://www.infoq.cn/article/JMyEPCDji2oGibDbZdzb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JMyEPCDji2oGibDbZdzb</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 08:55:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型语言模型, 提示工程, 微调, 检索增强生成
<br>
<br>
总结: 本文介绍了大型语言模型的应用技术，包括提示工程、微调和检索增强生成。提示工程简单易用，适合一般性主题和快速答案；微调可提高模型准确性，但成本高且需要专业技能；检索增强生成结合外部知识库，提供最新和相关信息，平衡了定制性和资源需求。RAG方法在特定情况下表现突出，是增强AI应用程序的最佳选择。 </div>
                        <hr>
                    
                    <p>自众多大型语言模型（LLM）和高级对话模型发布以来，人们已经运用了各种技术来从这些 AI 系统中提取所需的输出。其中一些方法会改变模型的行为来更好地贴近我们的期望，而另一些方法则侧重于增强我们查询 LLM 的方式，以提取更精确和更有关联的信息。</p><p>&nbsp;</p><p>检索增强生成（RAG）、提示和微调等技术是应用最广泛的。在这篇文章中，我们将研究对比这些技术的优缺点。这很重要，因为本文将帮助你了解何时该使用这些技术，以及如何有效地使用它们。</p><p></p><h2>提示工程</h2><p></p><p>提示是与任何大型语言模型交互的最基本方式。你可以把提示看作是给模型提供的指令。当你使用提示时，你会告诉模型你希望它给你反馈什么样的信息。这种方法也被称为提示工程，有点像是学习如何提出正确的问题以获得最佳答案的方法。但你能从中获得的东西是有限的，这是因为模型只能反馈它从训练中获知的内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f7640ae97903cd4e7296c8014a15b13b.png" /></p><p></p><p>提示工程的特点是它非常简单。你不需要成为技术专家也能写好提示，这对大多数人来说都是个好消息。但由于它的效果很大程度上取决于模型的原始学习水平，所以它可能并不总能提供你需要的最新或最具体的信息。当你处理的是一般性的主题，或当你只需要一个快速答案，而不需要太多细节时，提示工程最好用。</p><p></p><h3>优点：</h3><p></p><p>易于使用：提示易于使用，不需要高级技术技能，因此可供广大受众使用。成本效益：由于它使用预先训练好的模型，因此与微调相比，其所涉及的计算成本极低。灵活性：用户可以快速调整提示以探索各种输出，而无需重新训练模型。</p><p></p><h3>缺点</h3><p></p><p>不一致：模型响应的质量和相关性可能因提示的措辞而有很大差异。有限的定制能力：定制模型响应的能力受限于用户制作有效提示的创造力和技巧。对模型知识的依赖：输出局限在模型在初始训练期间学到的内容上，这使得它对于高度专业化或最新的信息需求来说效果不佳。</p><p></p><h2>微调</h2><p></p><p>微调是指你找来一个语言模型并让它学习一些新的或特殊的东西。可以把它想象成更新手机上的应用程序以获得更好功能的方法。但在微调的情况下，应用程序（模型）需要大量新信息和时间来正确学习各种内容。对于模型来说，这有点像是重返校园。</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/63abc76d149997272f19d8154b056a93.png" /></p><p></p><p>由于微调需要大量的算力和时间，因此成本可能很高。但如果你需要语言模型很好地理解某些特定主题，那么微调就会很划算。这就像是教模型成为你所感兴趣的领域的专家一样。经过微调后，模型可以为你提供更准确、更接近你所需内容的答案。</p><p></p><h3>优点：</h3><p></p><p>自定义：微调允许广泛的自定义，使模型能够生成针对特定领域或风格的响应。提高准确性：通过在专门的数据集上进行训练，模型可以产生更准确、更相关的响应。适应性：经过微调的模型可以更好地处理原始训练过程中未涵盖的小众主题或最新信息</p><p></p><h3>缺点：</h3><p></p><p>成本：微调需要大量计算资源，因此比提示工程更昂贵。技术技能：这种方法需要更深入地了解机器学习和语言模型架构。数据要求：有效的微调工作需要大量且精心策划的数据集，这类数据集可能很难编译。</p><p></p><h2>检索增强生成（RAG）</h2><p></p><p>检索增强生成（RAG）将常见的语言模型与知识库之类的东西混合在一起。当模型需要回答问题时，它首先从知识库中查找并收集相关信息，然后根据该信息回答问题。模型会快速检查信息库，以确保它能给你最好的答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/8618d5cfd44ec48ffc85bc930cfc3dab.png" /></p><p></p><p>RAG 在你需要最新信息，或需要比模型最初学习到的内容更广泛的主题答案的情况下特别有用。在设置难度和成本方面它不算高也不算低。它很有用，因为它可以帮助语言模型给出新鲜且更详细的答案。但就像微调一样，它需要额外的工具和信息才能正常工作。</p><p>&nbsp;</p><p>RAG 系统的成本、速度和响应质量严重依赖于矢量数据库，所以这种数据库成为了 RAG 系统中非常重要的一部分。</p><p></p><h3>优点：</h3><p></p><p>动态信息：通过利用外部数据源，RAG 可以提供最新且高度相关的信息。平衡：在提示的简易性和微调的定制能力之间提供了中庸之道。上下文相关性：通过附加的上下文来增强模型的响应，从而产生更明智和更细致的输出。</p><p></p><h3>缺点：</h3><p></p><p>复杂性：RAG 实现起来可能很复杂，需要语言模型和检索系统之间做好集成。资源密集型：虽然 RAG 的资源密集程度低于完全微调的方法，但它仍然需要相当大的计算能力。数据依赖性：输出的质量在很大程度上取决于检索到的信息的相关性和准确性</p><p></p><h2>提示、微调和 RAG 对比</h2><p></p><p>下面的表格完整对比了提示、微调和检索增强生成方法。此表将帮助你了解不同方法之间的差异，并决定哪种方法最适合你的需求。</p><p></p><p>上表分解了提示、微调和 RAG 三种方法的要点。它应该可以帮助你了解每种方法最适合哪种情况。希望这张表可以帮助你为下一个任务选择正确的工具。</p><p></p><h2>RAG：增强 AI 应用程序的最佳选择</h2><p></p><p>RAG 是一种独特的方法，它将传统语言模型的强大功能与外部知识库的精确度结合在了一起。这种方法有很多优势，因而脱颖而出。在特定情况下，相比单独使用提示或微调方法，RAG 的优势特别突出。</p><p>&nbsp;</p><p>首先，RAG 通过实时检索外部数据来确保其所提供的信息是最新并且高度相关的。这对于需要最新信息的应用程序来说非常重要，与新闻相关的查询或快速发展的领域就是典型例子。</p><p>&nbsp;</p><p>其次，RAG 在可定制性和资源需求方面提供了一种平衡的方法。与需要大量计算能力的完全微调方法不同，RAG 允许更灵活、更节省资源的操作，让更多用户和开发人员可以轻松使用它。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2467055b7f1c4be14407c79fea31b48.png" /></p><p></p><p>最后，RAG 的混合特性弥补了 LLM 的广泛生成能力与知识库中可用的特定详细信息之间的差距。在它的帮助下，模型不仅会产生相关且详细的输出，而且还具有丰富的上下文。</p><p>&nbsp;</p><p>优化、可扩展且经济高效的矢量数据库解决方案可以极大地增强 RAG 应用程序的性能和功能。这就是为什么你需要 MyScale，这是一个基于 SQL 的矢量数据库，它可以与主要的 AI 框架和语言模型平台（如 OpenAI、Langchain、Langchain JS/TS 和 LlamaIndex）顺利集成。使用 MyScale 后，RAG 可以变得更快、更准确，这对于寻求最佳结果的用户来说非常有用。</p><p></p><h2>小结</h2><p></p><p>总之，你应该选择提示工程、微调还是检索增强生成方法将取决于你项目的具体要求、可用资源和期望的结果。每种方法都有其独特的优势和局限性。提示是易用且经济高效的，但提供的定制能力较少。微调以更高的成本和复杂性提供充分的可定制性。RAG 实现了某种平衡，提供最新且与特定领域相关的信息，复杂度适中。</p><p>&nbsp;</p><p>原文链接：<a href="https://myscale.com/blog/prompt-engineering-vs-finetuning-vs-rag/">https://myscale.com/blog/prompt-engineering-vs-finetuning-vs-rag/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CODtaxOwaiWLz0dehAQj</id>
            <title>刘强东AI数字人首播，30分钟观看破千万；雷军回应爽文人生：不是高考状元、没有40亿；特斯拉大裁员：员工对赔偿满意｜AI周报</title>
            <link>https://www.infoq.cn/article/CODtaxOwaiWLz0dehAQj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CODtaxOwaiWLz0dehAQj</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 08:42:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 特斯拉, 英伟达, 裁员, 人工智能
<br>
<br>
总结: 英伟达股价暴跌，特斯拉全球裁员10%，雷军否认模仿马斯克乔布斯，人工智能初创公司Stability AI裁员10%。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/81/81f8ab8b400dcf60411159e748fc3f37.jpeg" /></p><p>整理&nbsp;|&nbsp;傅宇琪，褚杏娟</p><p></p><blockquote>引言：特斯拉全球裁员 10%，但被裁员工很满意？钉钉上线 AI 助理市场 ；雷军：没模仿马斯克、乔布斯，卡里也没有冰冷的 40 亿；官网秒空！华为 Pura 70 突然开售；出门问问开启招股：实现盈利的“AIGC 第一股”；Meta 发布最新人工智能模型 Llama 3；世界首个 AI 程序员 Devin 视频遭“打假”……&nbsp;&nbsp;</blockquote><p></p><p></p><h1>热门资讯</h1><p></p><p></p><h3>英伟达市值暴降 2.23 万亿，马斯克自嘲：这算个啥</h3><p></p><p>英伟达股价周五暴跌 10%，市值蒸发 2110 亿美元（1.53万亿人民币），这是美国股市历史上第二大单日市值损失。英伟达近一​周市值已暴降 3083 亿美元（2.23 万亿人民币）。</p><p>在一位分析师特别指出，另一家 AI 股领导者超微电脑（Super Micro Computer）未能报告初步收入后，英伟达的股票在更广泛的 AI 板块大跌中风雨飘摇，这使得人们对其即将发布的业绩产生了怀疑。这则消息导致了英伟达股价的狂跌，超微股价更是骤跌 23.1%。</p><p>本周 AI 板块的大部分涨幅主要是由害怕错过 AI 热潮的情绪（FOMO）推动的。当股票已经像英伟达和超微那样上涨了数百个百分点时，这样的涨幅就维持不了多久，而这似乎正是目前正在发生的事情。一条负面消息就会引起少量抛售，导致投资者在潜在的泡沫破裂之前争相锁定利润。</p><p>对此，特斯拉 CEO 埃隆·马斯克在X平台上发帖，以自嘲的口吻表示，英伟达 10% 的下跌只是“Rookie nunbers（菜鸟数字）”，意思是对自己来说这样的下跌不算啥，特斯拉曾经遭受过更加惨重的损失。</p><p></p><h3>&nbsp;特斯拉全球裁员 10%，或将有 1.4 万人失业，员工对赔偿标准满意</h3><p></p><p>4 月 15 日，马斯克发布全员邮件，宣布特斯拉裁员 10%。按照特斯拉去年公布的数据显示，特斯拉在全球共有 14 万名员工，换算来看，大约有 1.4 万人将失去工作。据报道，特斯拉高级副总裁 Drew Baglino（德鲁·巴格利诺）、公共政策副总裁 Rohan Patel（罗汉·帕特尔）也将离开公司。马斯克在给员工的备忘录中表示，特斯拉的“快速增长”导致了“某些领域的角色和工作职能重复”。“没有什么比这更让我讨厌的了，但我必须这么做。这将使我们精简、创新，并渴望下一个增长阶段周期。”</p><p>本次为自 2017 年来最大规模裁员，据透露，特斯拉这次大范围裁员还波及中国的业务线，上海浦东临港特斯拉工厂的员工们已经收到了官方的邮件。4 月 16 日，有知情人士表示，特斯拉在中国的这次裁员比例远不止马斯克说的 10%，销售部门是裁员重灾区。据悉，此次裁员部门优化比例将在 20%~50%，补偿从 N+1 到 N+3，当天谈完即走就是 N+3。</p><p>同时，特斯拉也将 FSD 全自动驾驶服务的订阅价格下调了 50%，从每月 199 美元变为 99 美元。初步判断，裁员和价格下调的原因或因为特斯拉近期销量下滑、库存问题严重，第一季度交付新车量 38.68 万辆，同比下滑 8.3%，环比下滑高达 20%。</p><p>4 月 16 日，马斯克在 X 平台上回应一条关于 6 年前大空头查诺斯解释为何做空特斯拉的帖子时表示，该公司需要每隔五年左右进行一次“彻底的组织改革”，间接回应了特斯拉全球大裁员的消息。他写道：“每隔五年左右，特斯拉就必须进行一次彻底的组织改革，以达到下一个水平。话虽如此，我们在特斯拉的高管任期非常长，远远超过 10 年。”</p><p>4 月 18 日，马斯克在发给员工的一封简短电子邮件中表示：“在我们重组特斯拉的过程中，我注意到一些遣散费低得不合理。我为这个错误道歉。我们正在立即予以纠正。” 据报道，特斯拉中国销售部门裁员多为六个月内新员工，特斯拉提供 0.5N+3 赔偿，因为特斯拉底薪较高，所以赔偿到手大概有 4~5 万元。据被裁人员普遍反映，从裁员消息传出到办妥离职手续的过程很短，而且赔偿标准普遍令当事人感到较为满意。</p><p></p><h3>&nbsp;雷军：没模仿马斯克、乔布斯，卡里也没有冰冷的 40 亿，SU7 正式版本开始交付</h3><p></p><p>4 月 18 日，小米集团 CEO 雷军开播，与网友畅谈 SU7 开售这 20 天并带领大家参观小米交付中心。在直播中雷军称：“很多人都催我们快一点交付啊，雷总你能不能去工厂打螺丝啊。今天我们这个工厂是现代化工厂啊，主要的工艺都是全自动化生产的，其实我去打螺丝还不如机器生产来的快，那纯属添乱啊。”</p><p>面对网友催 SU 7 快点交付，雷军表示：“在很多城市创始版交付都已经结束了，北京深圳等地都已经开售 SU7 正式版本的交付。“销量比我们预计的高了 3-5 倍，一个星期前紧急开了供应商大会，正在加紧生产。”还有网友认为小米 SU7 性价比不够，雷军回应称，纯电车企业基本都是不赚钱的，离谱的还有一辆亏个十几万的。“在巨亏的行业里不要谈性价比，纯电轿车里边，基本都是亏损的。”</p><p><img src="https://static001.geekbang.org/infoq/a7/a735e78f603836273d7274e3fd99bc6c.png" /></p><p>针对着装模仿马斯克的质疑，雷军称挺郁闷的，就是撞衫了，蓝色西服都很像；”还有人说我做手机时模仿乔布斯，其实穿的就是工程师的衣服，牛仔裤和衬衫。模仿的话我可以模仿的更像，这是蒙受了不白之冤。”</p><p>他在直播中还提到，网上有很多关于自己的谣言。“我不是高考状元，没有考过 700 分，人生低谷里也没有冰冷的 40 亿。”雷军表示，自己是农村出身，靠勤奋努力一步一步走到今天。人生的成功不是爽文，要靠梦想、奋斗和贵人相助。</p><p></p><h3>&nbsp;人工智能初创公司 Stability AI 将裁员 10%</h3><p></p><p>4 月 18 日，据外媒报道，人工智能初创公司 Stability AI 为调整业务规模裁减了 20 多名员工，这些员工大多负责业务运营方面。</p><p>据悉，Stability AI 的裁员量约占其全球 200 多名员工数的 10%。上个月，Stability 宣布其前首席执行官 Mostaque 将离开公司，以“追求去中心化人工智能”，并将由 Wong 和 Laforte 进行接替。</p><p></p><h3>&nbsp;出门问问开启招股：实现盈利的“AIGC 第一股”，已获近亿港元基石认购</h3><p></p><p>4 月 16 日，出门问问正式启动招股，至 19 日结束，并计划于 2024 年 4 月 24 日正式在港交所主板挂牌上市。出门问问此次将发售 8456.8 万股股份（视乎超额配股权行使与否而定），其中 90% 将作国际发售，10% 将于香港作公开发售。其中，两大地方国资中关村国际有限公司、南京经开聚智科创投资合伙企业作为基石投资者参与本次发行，累计认购规模约 9500 万港元。</p><p>出门问问提供 AIGC 解决方案，拥有自研大模型“序列猴子”等技术，且营收呈现强劲增长趋势。在创业 12 年后，出门问问终于有机会成为香港市场的“AIGC 第一股”。</p><p></p><h3>&nbsp;官网秒空！华为 Pura 70 突然开售，门店大排长龙，销售称被临时叫来同样很懵</h3><p></p><p>4 月 18 日，华为 Pura 70 系列正式开售，售价 5499 元起。其中，Pura 70 Ultra 和 Pura 70 Pro 于 10 点 08 分开售，而 Pro+、华为 Pura 70 将于 22 日 10 点 08 分开售，Pura 70 Ultra 和 Pura 70 Pro 在一分钟内即告售罄。华为 Pura 70 全系列已经在门店展出，包括标准版、Pro 版、Pro+ 版本以及 Ultra 版四个版本。店内部分型号有现货，需要现场排队购买。</p><p>某销售人员介绍，“一人限购一台。现货购买无法通过线上预约，需要现场排队，属于是先到先得。如果现场没有买到机器，工作人员会给顾客一份预约码，后续到货之后，可以凭借预约码来线下提货。目前有部分型号有现货。”对于到货情况，销售人员称也是临时得知。“不知道现场什么时候来的货，我是早上临时被叫过来的，现在也处于比较懵的状态。”</p><p>目前华为商城、京东等华为官方授权店，相关产品已显示无货状态。其中，Pura 70 Ultra 和 Pura 70 Pro 在华为商城不到一分钟就显示已经售罄。而线下一位华为经销商对记者表示，前几天已经完成了订货打款，Pura 70 Pro 的溢价为 500 元，目前看货量较为充足。</p><p></p><h3>&nbsp;李彦宏：人人都是开发者，开源模型会越来越落后</h3><p></p><p>4 月 16 日，Create 2024 百度 AI 开发者大会在深圳举办。</p><p>百度创始人李彦宏在大会上表示，未来自然语言将成为新的通用编程语言，每个人都可以成为开发者，用自己的创造力改变世界。他介绍了百度提供的开发工具：AgentBuilder 用于智能体开发、AppBuilder 用于 AI 原生应用开发、ModelBuilder 用于模型定制。他指出，“AI 正在掀起一场创造力革命，未来开发应用就像拍个短视频一样简单，人人都是开发者，人人都是创造者。”</p><p>李彦宏还表示开源模型会越来越落后。“通过降维剪裁出来的模型，比直接用开源模型调出来的模型，同等尺寸下，效果明显更好；同等效果下，成本明显更低。”会上，李彦宏还发布了文心大模型 4.0 的工具版，文心大模型的算法训练效率号称提升到了原来的 5.1 倍，周均训练有效率达到 98.8%，推理性能提升了 105 倍，推理的成本降到了原来的 1%。</p><p>值得注意的是，360 集团创始人周鸿祎在哈佛大学的一次演讲中表达了相反意见。他表示“没有开源就没有 Linux、没有互联网，甚至包括我们自己借助了开源技术才能发展至今”。他还预言，在未来一到两年内，开源技术的力量很可能会超过闭源技术。</p><p>百度 CTO 王海峰还在会上表示，目前智能代码助手 Comate 整体采纳率达到了 46%，新增代码中生成的比例已经达到了 27%。文心大模型 4.0 的效果持续提升，发布后的半年时间，又提升了 52.5%。截至目前，飞桨文心生态已凝聚 1295 万开发者，服务 24.4 万家企事业单位，基于飞桨和文心创建 89.5 万个模型。文心一言累计用户规模已达 2 亿，日均调用量也达到了 2 亿。</p><p></p><h3>&nbsp;刘强东 AI 数字人首播，近 1 小时 观看量超 2000 万</h3><p></p><p>4 月 16 日晚 6 点 18 分，京东刘强东以“采销东哥 AI 数字人”的形式空降京东家电家居、京东超市采销直播间，开启自己的直播首秀。</p><p>据了解，AI 刘强东可以在直播过程中与粉丝互动并读留言，其在直播过程主要推荐了空调、电视产品，将推荐 4 款海信产品，以及少量美的、创维、华凌、奥克斯、TCL 的产品。</p><p>根据京东超市披露，采销东哥 AI 数字人上播 30 分钟，直播间观看人数破千万；仅 40 分钟内，直播间观看人数超过 1300 万，创造京东超市采销直播间开播以来，观看人数的最高峰；近 1 小时观看量超 2000 万，直播时段用户平均停留时长达到日常均值的 5.6 倍。在 40 分钟内，直播间整体订单量破 10 万。</p><p><img src="https://static001.geekbang.org/infoq/a8/a893f1aebfd2470c492bd502a039d4f7.jpeg" /></p><p></p><h3>&nbsp;古尔曼：苹果 iOS 18 的首批 AI 功能将完全运行于设备端</h3><p></p><p>4 月 15 日消息，据彭博社记者 Mark Gurman（马克・古尔曼）透露，苹果将于 iOS 18 推出的首批全新 AI 功能将完全运行于设备端，而无需依赖云服务器。</p><p>古尔曼在其 Power On 通讯的问答环节中表示：“随着全球都在翘首期盼苹果在 6 月 10 日发布的重磅 AI 技术，目前看来首批功能将完全在设备上运行。这意味着为这些新功能提供支持的大型语言模型将不会用到云端处理。”</p><p>古尔曼还指出，苹果未来可能会提供一些基于云端的 AI 功能，这些功能可能由谷歌的 Gemini 或其他供应商提供支持。据报道，苹果已经与谷歌、OpenAI 和中国百度等公司就潜在的生成式 AI 合作进行了讨论。虽然 iOS 18 预计不会包含苹果自研的类似于 ChatGPT 的聊天机器人，但目前尚不清楚 Gemini 或其他聊天机器人是否会直接集成到 iOS 18 中。</p><p></p><h3>&nbsp;员工抗议谷歌与以色列签订云计算合同，被谷歌全部解雇</h3><p></p><p>谷歌公司在周三解雇了 28 名员工，原因是这些员工在谷歌纽约和加州森尼维尔的办公室静坐 10 小时，抗议谷歌与以色列签订的价值 12 亿美元的云计算合同。谷歌全球安全事务副总裁 Chris Rackow（克里斯·拉科）在一份发给全公司的备忘录中称，这些不守规矩的员工在周二戴着传统阿拉伯头巾冲进并占领了一名高管的办公室。经过内部调查后，他们在周三晚些时候被解雇。</p><p>“他们占据了办公空间，破坏了我们的财产，实际上阻碍了其他谷歌员工的工作。他们的行为是不可接受的，极具破坏性，让同事感到受到威胁，”拉科在备忘录中称，“经过调查，我们在今天终止了 28 名涉事员工的雇佣关系。我们将继续调查并在必要时采取行动。” 与此同时，9 名谷歌员工因为静坐抗议被逮捕。目前还不清楚这 9 名员工是否被解雇员工之列。谷歌早些时候已经给这些员工安排了行政休假，并切断了他们访问内部系统的权限。</p><p></p><h3>&nbsp;奥特曼“亲自带货”，消息称 OpenAI 向数百名世界 500 强高管推荐 ChatGPT 企业版</h3><p></p><p>4 月 15 日消息，OpenAI 首席执行官 Sam Altman（萨姆・奥特曼）本月在旧金山、伦敦和纽约等地分别接待了数百名《财富》世界 500 强公司的高管。与会者表示，阿尔特曼与其他 OpenAI 高管一同在这些地方为企业提供了人工智能服务，某些情况下更与其“金主”微软进行正面交锋。</p><p>与会者表示，阿尔特曼在每个城市的活动中直接面向 100 多名高管发表讲话，推荐自家的产品。在上述活动中，阿尔特曼和 OpenAI 首席运营官布拉德・莱特凯普演示了自家产品，包括 ChatGPT 企业版。OpenAI 向企业高管们作出承诺，ChatGPT 企业版不会使用客户的数据来训练其模型。他们指出，ChatGPT 消费者版本已被超过 92% 的《财富》世界 500 强公司采用。</p><p></p><h1>IT 业界</h1><p></p><p></p><h3>&nbsp;AI 月活企业已超 170 万家，钉钉上线 AI 助理市场</h3><p></p><p>4 月 18 日，钉钉正式上线 AI 助理市场（AI Agent Store），首批将推出超过 200 个 AI 助理，覆盖企业服务、行业应用、效率工具、财税法务、教育学习、生活娱乐等类目，用友、携程商旅、墨见 Molook 等各领域 SaaS 企业已上架 AI 助理，加入钉钉 AI 生态。</p><p><img src="https://static001.geekbang.org/infoq/56/562c066776841e07587344cdf613f6c0.jpeg" /></p><p>同时，钉钉对外发布了一组数据：自 2023 年 4 月 18 日宣布接入通义千问大模型，开启全面智能化战略一年后，钉钉 AI 助理、文档 AI、宜搭 AI、智能 OA 等产品正在深入千行百业。截至 2024 年 3 月底，钉钉 AI 已超过 220 万家企业使用，月活跃企业超过 170 万家。</p><p></p><h3>&nbsp;Meta 发布最新人工智能模型 Llama 3，百度智能云宣布国内首家支持全系列训练推理</h3><p></p><p>当地时间 4 月 18 日，Meta 发布两款开源 Llama 3 8B 与 Llama 3 70B 模型，供外部开发者免费使用。Llama 3 的这两个版本，也将很快登陆主要的云供应商。</p><p>按照 Meta 的说法，Llama 3 8B 和 Llama 3 70B 是目前同体量下，性能最好的开源模型。Llama 3 8B 在某些测试集上性能比 llama 2 70B 还要强。而且在未来几个月内，Meta 还会推出更多的版本。</p><p>英伟达高级科学家 Jim Fan（吉姆·范）认为，之后可能会发布的 Llama 3-400B 以上的版本，将成为某种“分水岭”，开源社区或将能用上 GPT-4 级别的模型。在 Llama 3 发布后，扎克伯格向外媒表示，“我们的目标不是与开源模型竞争，而是要超过所有人，打造最领先的人工智能。”</p><p>延伸阅读：<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247609594&amp;idx=1&amp;sn=8bb6d52020314120daf8dc35ed49c377&amp;scene=21#wechat_redirect">卷疯了！最强开源大模型Llama 3发布，最大参数4000亿，小扎内心：大模型比元宇宙香多了</a>"</p><p>4 月 19 日，百度智能云千帆大模型平台推出针对 Llama3 全系列版本的训练推理方案，便于开发者进行再训练，搭建专属大模型，现已开放邀约测试。目前，百度智能云千帆大模型平台中各种尺寸模型定制工具 ModelBuilder 已预置了最全面最丰富的大模型，支持国内外第三方主流模型，总数量达到 79 个，是国内拥有大模型数量最多的开发平台。</p><p></p><h3>&nbsp;世界首个 AI 程序员 Devin 视频竟造假？博主逐帧解析，Devin 代码任务完成很糟糕</h3><p></p><p>YouTube 博主近日揭露了背后明星初创公司 Cognition 的谎言，通过逐帧分析发现，Devin 并不能独立完成 Upwork 工作，而且非常糟糕。</p><p><img src="https://static001.geekbang.org/infoq/bd/bd8f6d0220b568e08159e6712b176e46.png" /></p><p>他将 25 分钟“揭穿 Devin 的谎言”视频公开后，一时间引爆全网并在 HN、Reddit 等社交平台上掀起轩然大波。更有意思的是，博主自己复制了 Devin 尝试做的任务，花费了大约 36 分钟。然而，Devin 用了至少 6 个小时，甚至可能超过一天。有网友表示，“正如博主详细解释的那样，尽管其试图在演示中暗示，Devin 并不能独立完成 Upwork 的工作。它正在创建混乱、过于复杂的代码”。</p><p>&nbsp;液体都“智能”可编程了？哈佛新型超材料登 Nature，粘度、透明度、弹性可变</p><p>最近，一种被称为“智能”液体的多功能可编程的新型超材料——Metafluid，登上了 Nature。它由哈佛大学 SEAS 的研究团队研发，据说可自由调节弹性、光学特性、粘度。甚至能够在牛顿流体和非牛顿流体之间转换。研究人员表示，有了这些 buff 属性加成，该流体在编程液压机器人、智能减震器、光学设备中都有巨大的应用潜力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JCkXE51jT9tgZ6h84PID</id>
            <title>数据库的夏天！2024可信数据库发展大会如约而至，邀请你来！</title>
            <link>https://www.infoq.cn/article/JCkXE51jT9tgZ6h84PID</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JCkXE51jT9tgZ6h84PID</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 07:57:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库, AI, 数据要素市场, 数据强国
<br>
<br>
总结: 数据库是支撑数据存储与计算的关键载体，随着全球AI浪潮的迅猛袭来，数据库需求发生剧烈变化，以向量数据库为代表的支撑AI的数据库快速涌现。国家数据局的成立推动数据要素市场建设，一系列适配数据资产化的数据库产品涌现，助力我国成为数据强国。2023年首届"可信数据库发展大会"在京举办，2024年继续举办，旨在推动全球数据库产业进步。 </div>
                        <hr>
                    
                    <p>数据库是支撑数据存储与计算的关键载体。过去一年，一方面，全球 AI 浪潮迅猛袭来，数据的存算需求发生剧烈变化，以向量数据库为代表的、支撑 AI 的数据库如雨后春笋般快速涌现；另一方面，国家数据局正式挂牌成立，推动数据要素市场建设进程升级换挡，一系列适配数据资产化、数据要素流通的数据库产品顺势而生，助力我国成为数据强国。数据库产业在两方面因素推动下，变革不断、热闹非凡。</p><p></p><p>为一站式展现数据库产业最新发展情况，2023 年 7 月 4、5 日，首届<a href="https://mp.weixin.qq.com/s?__biz=MzU0NzczNjAwMw==&amp;mid=2247502313&amp;idx=1&amp;sn=631cda83096e8601d96bdde82198df38&amp;scene=21&amp;token=757084964&amp;lang=zh_CN#wechat_redirect">线下“可信数据库发展大会”</a>"在京举办，近百位来自产、学、研、用各行业数据库专家齐聚一堂，与千位从业者共同勾勒数据库产业的未来。</p><p><img src="https://static001.geekbang.org/infoq/30/3023bb8d804af69e1482edb930875fbc.webp" /></p><p>2023 可信数据库发展大会在京成功召开</p><p></p><p>2024 年，站在中国数字经济产业升级和数据要素市场化建设的时代交汇点上，为进一步推动全球数据库产业进步，“2024 可信数据库发展大会”继续在这个盛夏，如约而至。</p><p></p><p>本次大会由中国通信标准化协会、大数据技术标准推进委员会主办，InfoQ 极客传媒联合主办，将于 2024 年 7 月 16-17 日，在北京朝阳悠唐皇冠假日酒店隆重召开。</p><p></p><p>本届大会围绕数据库的热点议题，共设置 1 个主论坛和 6 个分论坛，具体包括金融、电信、能源 &amp; 制造业三大行业应用分论坛，以及 AI+DB、搜索与分析型数据库 &amp; 多模数据库、数据库生态与国际化三大技术生态分论坛。各论坛核心议题介绍如下：</p><p></p><p></p><h2>行业应用类分论坛</h2><p></p><p></p><p>&nbsp;1. 金融行业数据库应用创新分论坛</p><p>本论坛探讨金融业核心系统数据库转型升级最新实践，揭幕数据库应用创新实验室金融行业工作组首批共建单位。</p><p></p><p>&nbsp;2. 电信行业数据库应用创新分论坛</p><p>本论坛聚焦多源异构数据库管理的时代背景下，如何在通信运营商 B、O、M 域构建安稳易用的数据库产品和高效的运维管理体系。</p><p></p><p>&nbsp;3. 能源和制造业数据库应用创新分论坛</p><p>本论坛分享可信数据库赋能智能制造和能源行业的前沿进展，揭幕数据库应用创新实验室能源行业工作组首批共建单位。</p><p></p><p></p><h2>技术生态类分论坛</h2><p></p><p></p><p>&nbsp;4. 人工智能与数据库融合发展分论坛</p><p>本论坛探讨 AI4DB 和 DB4AI 技术热点，例如向量数据管理、查询性能优化、库内机器学习等方向。</p><p></p><p>&nbsp;5. 搜索与分析型数据库 &amp; 多模数据库分论坛</p><p>本论坛聚焦海量异构数据处理需求下，如何实现业务快速混合搜索与深度分析洞察，如何实现一库多用、降本增效。</p><p></p><p>&nbsp;6. 数据库生态与国际化分论坛</p><p>本论坛分享生态工具和新兴硬件如何赋能数据库发展提质增效及中国数据库企业在全球化背景下的市场机遇与出海路径</p><p></p><p>此次大会将汇聚众多数据库行业领军企业、专家学者，共同探讨全球数据库发展趋势，分享最具权威性的产业洞察和前沿的实践案例。我们期待通过 2024 可信数据库发展大会，能共同促进我国数据库技术革新与自立自强，构筑数据库产业链蓬勃生态，推动数据库行业高水平发展。</p><p></p><p>目前，大会已正式进入筹备阶段，我们诚挚地邀请各界精英一同见证、参与这一盛会。让我们携手并进，共同开创可信数据库行业的新时代，谱写更加辉煌的篇章！</p><p></p><p></p><blockquote>2024 可信数据库发展大会联系人：刘老师13691032906（微信同号）于老师18500981908</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WOHqcMtljt4ieuKHBL5r</id>
            <title>Meta、微软、Mistral AI、Hugging Face、通义、港中文六路进发开源大模型 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/WOHqcMtljt4ieuKHBL5r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WOHqcMtljt4ieuKHBL5r</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 06:12:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 
        关键词: 大模型, 技术更新, 商业领域, 开源领域
        <br>
        <br>
        总结: 大模型的快速发展使得了解最新技术和积极学习成为从业者的必修课。本文回顾了过去一周大模型领域的重要事件，包括技术更新和商业领域的动态。开源领域也迎来了多项重要成果更新，展示了大模型领域的持续发展。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h3>一、重点发现</h3><p></p><p>本周，开源领域迎来多项技术成果更新：开源大语言模型迎来&nbsp;Meta&nbsp;Llama3&nbsp;和微软的&nbsp;WizardLM&nbsp;2，CodeQwen1.5-7B&nbsp;加入开源代码领域，Mistral-22b-v0.2&nbsp;在开源中探索&nbsp;MOE&nbsp;与稠密模型的转换，Mini-Gemini&nbsp;和&nbsp;Hugging&nbsp;Face&nbsp;开源的视觉语言模型&nbsp;Idefics2&nbsp;则是在开源多模态模型中不断演进。</p><p>除了技术演进外，商业领域裁员与融资并存。之前占据融资热点的&nbsp;AI&nbsp;明星企业&nbsp;Stability&nbsp;AI&nbsp;和&nbsp;Tome&nbsp;相继宣布裁员计划，与之相对的则是大模型领域动辄数亿美元的融资。这也为诸多公司敲下了警钟，在应用淘汰赛中，如何在可控成本下，找寻能赚取稳定现金流的场景，是&nbsp;AI&nbsp;产品能否持续运营的关键。并且随着市场竞争的加剧，这一淘汰赛正在迅速展开。</p><p></p><h3>二、具体内容</h3><p></p><p></p><h4>大模型持续更新</h4><p></p><p>4&nbsp;月&nbsp;12&nbsp;日，知识管理厂商印象笔记宣布其自研大语言模型被正式命名为「印象大模型」，并已根据《生成式人工智能服务管理暂行办法》及相关法律法规完成模型备案，其&nbsp;AI&nbsp;产品印象&nbsp;AI&nbsp;也迎来多项功能更新，未来将为更多用户提供包含阅读、总结在内的多项智能化知识管理服务。4&nbsp;月&nbsp;14&nbsp;日，OpenAI&nbsp;在官宣日本办事处的同时，宣布推出针对日语优化&nbsp;GPT-4&nbsp;定制模型。Open&nbsp;AI&nbsp;表示，以&nbsp;Speak&nbsp;为代表的本地企业已经可以使用自定义模型，该模型在翻译和总结日语文本方面提供了更高的性能。最重要的是，其运行速度比&nbsp;GPT-4&nbsp;Turbo&nbsp;快三倍，这样的成本效益将成为满足当地各种需求的合适选择。4&nbsp;月&nbsp;17&nbsp;日，MiniMax&nbsp;稀宇科技&nbsp;正式发布其&nbsp;MoE&nbsp;模型&nbsp;abab&nbsp;6.5系列，该系列包含abab&nbsp;6.5&nbsp;和&nbsp;abab&nbsp;6.5s，其中abab&nbsp;6.5&nbsp;包含万亿参数，并支持&nbsp;200k&nbsp;tokens&nbsp;的上下文长度，abab&nbsp;6.5s&nbsp;同样支持&nbsp;200k&nbsp;tokens&nbsp;的上下文长度，但更高效，可以在1&nbsp;秒内处理近&nbsp;3&nbsp;万字的文本。</p><p></p><h4>多模态领域</h4><p></p><p>4&nbsp;月&nbsp;13&nbsp;日，xAI&nbsp;在其官网推文中宣布推出多模态模型&nbsp;Grok-1.5&nbsp;Vision，这也意味着，除了文本信息，Grok&nbsp;现在还可以处理各种包含图表、表格、截图和照片在内的视觉信息，并将于近期邀请现有的&nbsp;Grok&nbsp;用户进行测试。4&nbsp;月&nbsp;15&nbsp;日，香港中文大学终身教授贾佳亚团队提出的开源多模态模型&nbsp;Mini-Gemini&nbsp;宣布其&nbsp;130亿参数的&nbsp;demo&nbsp;上线&nbsp;Hugging&nbsp;Face。此前于&nbsp;3&nbsp;月&nbsp;28&nbsp;日，Mini-Gemini&nbsp;即宣布其代码、模型、数据已经全部开源。4&nbsp;月&nbsp;16&nbsp;日，Hugging&nbsp;Face&nbsp;更新了其视觉语言模型&nbsp;Idefics2。该模型能够理解和生成基于图像和文本的文字回复，并且在&nbsp;OCR&nbsp;识别能力方面显著增强。</p><p></p><h4>开源领域</h4><p></p><p>4&nbsp;月&nbsp;13&nbsp;日，Mistral&nbsp;AI&nbsp;在发布&nbsp;Mistral-22b-v0.1&nbsp;仅仅两天之后，宣布开源&nbsp;Mistral-22b-v0.2。该模型实现了从MOE到稠密（Dense）模型的转换，并且其训练数据是&nbsp;v0.1&nbsp;的&nbsp;8&nbsp;倍。相较于v0.1，&nbsp;v0.2&nbsp;在数学才能和编程能力获得明显提升，并且在多轮对话中也能保持高度的对话流畅性。Mistral&nbsp;AI&nbsp;同时宣布&nbsp;v0.3&nbsp;已经在训练过程中，并将有更多&nbsp;220&nbsp;亿参数的模型发布，直到其找到将&nbsp;MOE&nbsp;压缩的最佳成果。4&nbsp;月&nbsp;15&nbsp;日，微软发布并开源其新一代大语言模型系列&nbsp;WizardLM&nbsp;2，此系列包括三个模型，分别是WizardLM-2&nbsp;8x22B（MOE）、WizardLM-2&nbsp;70B&nbsp;和&nbsp;WizardLM-2&nbsp;7B。但&nbsp;4&nbsp;月&nbsp;16&nbsp;日，微软宣布因为其不熟悉新模型的发布流程，未能对&nbsp;WizardLM&nbsp;2&nbsp;进行毒性测试（toxicity&nbsp;testing），并已将代码文件从&nbsp;Github&nbsp;以及&nbsp;Hugging&nbsp;Face&nbsp;上删除，在完成测试后会尽快重新发布。4&nbsp;月&nbsp;16&nbsp;日，通义千问团队开源了基于&nbsp;Qwen1.5&nbsp;的代码模型&nbsp;CodeQwen1.5-7B&nbsp;及其对话模型。CodeQwen1.5-7B&nbsp;支持&nbsp;92&nbsp;种编程语言，并且能够处理最长&nbsp;64&nbsp;K的上下文输入，并展现出了优秀的代码生成、长序列建模、代码修改等能力。4&nbsp;月&nbsp;17&nbsp;日，AGI&nbsp;公司&nbsp;Zyphra&nbsp;Technologies&nbsp;宣布推出其新一代开源基础模型&nbsp;Zamba-7B。这个&nbsp;70&nbsp;亿参数的模型定位于&nbsp;AI&nbsp;设备的装载上，并声称在基准测试中优于&nbsp;LLaMA&nbsp;1、LLaMA&nbsp;2-7B。同时其模型权重也即将开源，以供大家判断实际效果。4&nbsp;月&nbsp;17&nbsp;日，昆仑万维宣布其基座大模型——天工&nbsp;3.0&nbsp;开启公测。天工3.0&nbsp;拥有&nbsp;4000&nbsp;亿参数，是目前全球最大的开源&nbsp;MoE&nbsp;大模型（但目前在&nbsp;Github&nbsp;和&nbsp;ModelScope&nbsp;未见其开源项目）。同时，天工3.0&nbsp;新增了图表对比生成、研究模式、增强模式、扩图修图等功能。4&nbsp;月&nbsp;18&nbsp;日，Meta&nbsp;正式发布&nbsp;Llama3，目前已经上架官网和&nbsp;Hugging&nbsp;Face。此次开源的&nbsp;Llama3&nbsp;共包括2个模型，Meta-Llama-3-8B&nbsp;和&nbsp;Meta-Llama-3-70B。在&nbsp;MMLU、GPQA、HumanEval、GSM-8K、MATH&nbsp;这五个评测集的表现上，不仅超过了&nbsp;Mistral&nbsp;7B，甚至部分评测集中，Meta-Llama-3-8B模型的得分超过了Meta-Llama-2-70B。而且在未来几个月内，Meta&nbsp;还会推出更多的版本。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新功能/插件</h4><p></p><p>4&nbsp;月&nbsp;16&nbsp;日，Poe&nbsp;宣布推出其3.0版本，并新增多机器人聊天功能，用户可以通过@指令，在不同任务场景下调用多个大模型进行对话，以发挥不同大模型的优势。Poe&nbsp;致力成为对话&nbsp;AI&nbsp;应用商店，提供变现工具和企业服务。4&nbsp;月&nbsp;16&nbsp;日，Adobe&nbsp;宣布推出一款适配&nbsp;Adobe&nbsp;Acrobat&nbsp;Reader&nbsp;和&nbsp;Adobe&nbsp;Acrobat&nbsp;的&nbsp;AI&nbsp;助手——Adobe&nbsp;Acrobat&nbsp;AI&nbsp;Assistant，以帮助用户快速处理、检索、阅读和总结吸收&nbsp;PDF&nbsp;文档中的内容。目前，该功能仅支持英文，预计未来还会扩展至更多语言。4&nbsp;月&nbsp;16&nbsp;日，Adobe&nbsp;宣布了&nbsp;Premiere&nbsp;Pro&nbsp;全新版本的更新计划，本次更新中包含了为第三方&nbsp;AI&nbsp;视频生成模型添加插件。这意味着在&nbsp;Adobe&nbsp;Firefly&nbsp;自身的能力之外，用户即将可以直接通过&nbsp;Adobe&nbsp;工具体系调用&nbsp;OpenAI&nbsp;Sora、Runway&nbsp;Gen-2&nbsp;和&nbsp;Pika。4&nbsp;月&nbsp;17&nbsp;日，昆仑万维宣布基于天工&nbsp;3.0&nbsp;打造的天工&nbsp;SkyMusic&nbsp;登录天工&nbsp;APP&nbsp;，并开启全面公测。天工&nbsp;SkyMusic可以生成&nbsp;80&nbsp;秒&nbsp;44100Hz&nbsp;采样率双声道立体声歌曲，支持生成说唱、民谣、放克、古风、电子等多种音乐风格，还能学习颤音、歌剧、吟唱、男女对唱、自动和声等歌唱技巧。同时，也支持参考音乐与方言歌曲两种生成方式。4&nbsp;月&nbsp;18&nbsp;日，钉钉&nbsp;AI&nbsp;助理市场（AI&nbsp;Agent&nbsp;Store）正式上线，首批将推出超过&nbsp;200&nbsp;个&nbsp;AI&nbsp;助理，覆盖企业服务、效率工具、财税法务、教育学习等类别。根据钉钉披露，截至&nbsp;2024&nbsp;年&nbsp;3&nbsp;月底，钉钉&nbsp;AI&nbsp;已超过&nbsp;220&nbsp;万家企业使用，月活跃企业超过&nbsp;170&nbsp;万家。</p><p></p><h4>终端AI</h4><p></p><p>4&nbsp;月&nbsp;12&nbsp;日，蔚来宣布端云多模态大模型&nbsp;NOMI&nbsp;GPT&nbsp;正式启动推送。NOMI&nbsp;GPT&nbsp;内置的认知中枢、情感引擎和端侧多模态感知架构赋予了&nbsp;NOMI&nbsp;与用户进行开放式问答的交互能力。本次升级后，用户可在车内体验到大模型百科、无限趣聊、魔法氛围、趣玩表情、用车问答、AI场景生成在内的多项全新交互体验。4&nbsp;月&nbsp;17&nbsp;日，Rewind&nbsp;宣布推出一款可穿戴&nbsp;AI&nbsp;设备&nbsp;Limitless。Limitless&nbsp;可以记录用户的日常对话内容，并利用&nbsp;AI&nbsp;进行会议准备、实时传译、记录和总结。这款产品预计在&nbsp;2024&nbsp;年&nbsp;8&nbsp;月份发货，预计售价为&nbsp;99&nbsp;美元。4&nbsp;月&nbsp;18&nbsp;日，联想在&nbsp;TechWorld&nbsp;2024&nbsp;上发布了内嵌个性化&nbsp;AI&nbsp;智能体「联想小天」的AI&nbsp;PC&nbsp;系列产品，价格从&nbsp;5999&nbsp;到&nbsp;17999&nbsp;元不等，目前已开启预购。</p><p></p><h3>其他</h3><p></p><p>4&nbsp;月&nbsp;12&nbsp;日，已发布大模型安全基座和AI生成内容检测基座的瑞莱智慧在其公众号宣布，已经完成新一轮战略融资。本轮融资由光源资本担任独家财务顾问，投资方包括北京市人工智能产业投资基金等。4&nbsp;月&nbsp;13&nbsp;日，估值&nbsp;3&nbsp;亿美元的&nbsp;AI&nbsp;初创公司&nbsp;Tome&nbsp;解雇了&nbsp;12&nbsp;名员工，在解雇之前该团队拥有&nbsp;59&nbsp;名员工。&nbsp;Tome&nbsp;产品专注于&nbsp;AI&nbsp;生成&nbsp;PPT，截至&nbsp;4&nbsp;月初，Tome&nbsp;付费专业版每月收入约为&nbsp;30&nbsp;万美元。4&nbsp;月&nbsp;15&nbsp;日，微软在其官网宣布，其将向阿联酋&nbsp;AI&nbsp;公司&nbsp;G42&nbsp;投资&nbsp;15&nbsp;亿美元，并持有少数股权和董事会席位。G42&nbsp;将在微软云计算平台&nbsp;Azure&nbsp;上运行其人工智能应用和服务，来为中东地区、中亚和非洲国家的各行各业提供先进的&nbsp;AI&nbsp;解决方案。4&nbsp;月&nbsp;16&nbsp;日，根据媒体消息，由王小川创立的百川智能正在进行新一轮数亿美元的融资，本轮融资也将成为今年以来国内AI领域最大的融资之一。4&nbsp;月&nbsp;18&nbsp;日，根据内部电子邮件，Stability&nbsp;AI新任命的联席CEO&nbsp;Shan&nbsp;Shan&nbsp;Wong和Christian&nbsp;Laforte宣布，Stability&nbsp;AI&nbsp;裁员&nbsp;20&nbsp;多名员工，这涉及这个&nbsp;200人&nbsp;团队的&nbsp;10%。此前于&nbsp;3&nbsp;月&nbsp;23&nbsp;日，Stability&nbsp;AI&nbsp;宣布其&nbsp;CRO&nbsp;Emad&nbsp;Mostaque&nbsp;离职，并退出董事会。</p><p></p><p></p><h3>报告预告</h3><p></p><p>Sora来袭，国内如何迅速跟上？开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，其能力是否有所提升和刷新？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？InfoQ研究中心即将发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，即将给出答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9b3c569c62a571715d811e7121db70f.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/X1PdfYz9j3vcNVeBd3WZ</id>
            <title>“算力代差从三年到并行开发”，梧桐车联发布全新品牌TTI，推出6条业务线</title>
            <link>https://www.infoq.cn/article/X1PdfYz9j3vcNVeBd3WZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/X1PdfYz9j3vcNVeBd3WZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 05:19:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 梧桐车联, TTI品牌, 智能空间解决方案, 算力平台
<br>
<br>
总结: 梧桐车联推出全新全栈式智能空间解决方案品牌TTI，旨在为汽车行业提供高效的数字化转型工具，包括智能空间软件生态、交互系统、算力平台、音响系统、显示产品和硬件生态系列产品。通过提供高性能算力平台和软硬件生态产品，梧桐车联致力于打通“人车家”连接，实现智能空间的大域控算力中心目标。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>4月18日，梧桐车联推出全新全栈式智能空间解决方案品牌TTI（TINNOVE &amp; TAI Inside），现场还首发两款与联发科技深度共研的智能空间算力平台产品，并发布了最新的软件和硬件生态系列产品。</p><p>&nbsp;</p><p>背靠腾讯生态的梧桐车联，一直致力于为汽车行业提供高效的数字化转型工具。TTI正是腾讯参与孵化的全栈式智能空间解决方案品牌，将完整承载梧桐车联的产品、技术能力，从输出大量功能，转向提供为汽车品牌整合产品和全栈式的解决方案。</p><p>&nbsp;</p><p>据介绍，TTI品牌共有6条产品业务线：智能空间软件生态TTI Eco，智能空间交互系统TTI OS，智能空间算力平台TTI Computing，智能空间音响系统TTI Sound，智能空间显示产品TTI Vision，以及基于软总线技术的智能空间硬件生态系列产品TTI Accessory。</p><p>&nbsp;</p><p>如何一至两年缩短到并行开发？</p><p>&nbsp;</p><p>“今年上半年算力最强的平台可以相当于旗舰级智能手机2021年的水平，这已经是一个巨大的提升。我们希望在未来一至两年的时间中将这个代差从三年缩减到并行开发，以此来支撑实现智能空间‘大域控’算力中心的目标，乃至最终实现整车中央计算。”梧桐车联副总经理刘铜阳介绍。</p><p>&nbsp;</p><p>据悉，梧桐车联此次发布的TTI Computing高性能算力平台C2080，是基于4nm工艺升级SOC、首批突破安兔兔车机版100万倍的算力平台，集成HiFi3 DSP、Vision DSP、AI Picture Quality DSP、AI ISP等17项算法和控制器，可以支撑智能空间的创新体验。</p><p>&nbsp;</p><p>举例来说，其通过算力平台集成了各类PSP，将分散的全车各域的传感器摄像头统一在一起管理和处理信息，并将用户看到的最终画面升级为AI优化影像。</p><p>&nbsp;</p><p>而旗舰算力平台C2090基于3nm制程车规级SoC，可集成多达19项算法和控制器，综合算力更强；比起C2080，其理论AI算力领先一倍以上，CPU理论性能提升约75%，GPU理论性能提升约120%，IPO理论性能提升两倍以上。</p><p>&nbsp;</p><p>打通“人车家”的软硬件生态产品</p><p>&nbsp;</p><p>现场，梧桐车联还发布了一系列最新的软件生态产品TTI Eco，涵盖娱乐、游戏、出行服务等。未来，梧桐车联也将和腾讯共同探索大模型等前沿技术应用创新，探索更多生态服务上车。</p><p>&nbsp;</p><p>去年，腾讯推出了混元通用大模型，并基于此加入汽车行业用车知识进行模型的精调，打造了座舱大模型。腾讯智慧出行副总裁钟学丹表示，“作为连接腾讯生态与汽车产业伙伴之间的桥梁，梧桐车联不仅优先引入腾讯丰富生态资源和技术能力，还会得到腾讯在产业生态资源、组织人才培养等全方位支持。”</p><p>&nbsp;</p><p>同时，梧桐车联发布了提供行业标准化持续拼接口和硬件生态工业化连接能力的TTI Accessory以及首款硬件生态产品TTI Pad，为用户提供专属用车服务。据介绍，围绕用车产品，梧桐车联还将从系统级解决方案发展到整个座舱，增加车载、冰箱、车载香薰、手表、手环等系列硬件产品。</p><p>&nbsp;</p><p>梧桐车联副总经理王永亮表示：“TTI Accessory将成为全方位打通人、车、家的重要一环。”</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XH74br8ETCHzLVeralrq</id>
            <title>李彦宏称开源落后于闭源，圈内大佬力挺；雷军否认“爽文人生” ；特斯拉裁员遣散费“过低”，马斯克：“得加钱”！ | Q资讯</title>
            <link>https://www.infoq.cn/article/XH74br8ETCHzLVeralrq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XH74br8ETCHzLVeralrq</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 05:16:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, Llama 3, AI模型, 李彦宏
<br>
<br>
总结: Meta发布了开源大模型Llama 3，性能直逼GPT-4，李彦宏表示开源AI模型会越来越落后。 </div>
                        <hr>
                    
                    <p>整理 | Tina、梓毓</p><p></p><p></p><blockquote>Meta 推出开源大模型 Llama 3；李彦宏：开源 AI 模型只会越来越落后！昆仑万维“天工 3.0”基座大模型与“天工 SkyMusic”音乐大模型开启公测；微软发布 Vision Pro 原生 OneNote 应用；刘强东数字人直播，27 分钟观看人数破千万；腾讯云 4.8 故障原因曝光；蚂蚁语雀创始人，飞书开放平台负责人离职创业；淘宝登陆 Apple Vision Pro 商店；曝特斯拉中国裁员最高比例 50%！新版 Firefox 125.0.1 版发布；Adobe 探索与 OpenAI 合作，推出人工智能视频工具；英特尔中国特供版 Gaudi3 AI 芯片……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>Meta推出开源大模型Llama&nbsp;3：最大底牌4000亿参数，性能直逼GPT-4</h4><p></p><p>北京时间&nbsp;4&nbsp;月&nbsp;19&nbsp;日，Meta&nbsp;官宣发布了其最先进开源大型语言模型的下一代产品——Llama&nbsp;3。据悉，Llama&nbsp;3&nbsp;在&nbsp;24K&nbsp;GPU&nbsp;集群上训练，使用了&nbsp;15T&nbsp;的数据，提供了&nbsp;80&nbsp;亿和&nbsp;700&nbsp;亿的预训练和指令微调版本。</p><p></p><p>Meta&nbsp;在官方博客中表示，“得益于预训练和后训练的改进，我们的预训练和指令微调模型是目前&nbsp;80&nbsp;亿&nbsp;和&nbsp;700&nbsp;亿&nbsp;参数尺度下最好的模型。”此外，Meta还同时发布了AI助手的更新版本。该助手将在Meta的Facebook、Instagram、WhatsApp和Messenger应用以及一个新设立的独立网站Meta.ai中获得更突出的地位，让Llama&nbsp;3能够更直接地与微软支持的OpenAI的热门产品ChatGPT竞争。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5796839cd13ff4bb05611f0b646c5e9.png" /></p><p></p><p>Meta在去年7月份发布了Llama&nbsp;2。此后，包括谷歌、马斯克旗下的xAI与Mistral等在内的数家公司发布了开源的大语言模型，竞争十分激烈。Llama&nbsp;3直接对标OpenAI的GPT-4，后者已经成为一个强大的多模态模型，不仅能够处理更长的文本，还支持图像输入。</p><p></p><p>开源的Llama是Meta人工智能战略的重要组成部分。Meta首席执行官马克·扎克伯格在本月初向投资人表示，今年的主要关注领域包括推出Llama&nbsp;3和“扩大Meta&nbsp;AI助手的实用性”。</p><p></p><h4>李彦宏：开源AI模型只会越来越落后！</h4><p></p><p>4月16日，百度创始人、董事长兼首席执行官李彦宏在&nbsp;Create&nbsp;2024&nbsp;百度&nbsp;AI&nbsp;开发者大会上发表了题为《人人都是开发者》的演讲。据其称，文心一言去年3月16日正式发布，截至目前用户数已经突破2亿，每天API的调用量突破2亿，服务客户数达到8.5万，利用千帆平台开发的AI原生应用数超过19万。他认为，大模型和生成式&nbsp;AI&nbsp;将彻底改变开发者这个群体。</p><p></p><p>李彦宏提到，有了基础模型文心&nbsp;4.0&nbsp;之后，百度可以根据需要，兼顾效果、相应速度，推理成本等各种考虑，剪裁出适合各种场景的更小尺寸模型，并且支持精调和&nbsp;post&nbsp;pretrain（一种预训练的模型训练方法）。</p><p></p><p>这样通过降维剪裁出来的模型，比直接用开源模型调出来的模型，同等尺寸下，效果明显更好；同等效果下，成本明显更低。基于这种对比效果，李彦宏放话称：开源模型会越来越落后。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7c103ee9dd14d1abb15822af38ccf26.png" /></p><p></p><p>针对近日李彦宏说的未来大型AI原生应用将是大小模型混用，圈内一众大佬发表了看法。</p><p></p><p>贾扬清在朋友圈发文表示支持，并觉得Robin这点说得非常对，在初始的应用尝试过去之后，模型的特化会是一个从效果上和从性价比上更加make&nbsp;sense的选择。</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35cd67c803c241fa73902256d3885fa2.png" /></p><p></p><p>张俊林也对Llama&nbsp;3以及大模型开源与闭源发表了一些个人看法：“目前从模型能力而言，整体来说开源阵营确实是弱于闭源阵营的，这是事实，但是从最近一年半的技术发展来看，开源模型（包括国外和国内的模型）和最好闭源模型的差距是在逐步缩小的，而不是越来越拉大的，这也是事实，很多数据可以说明这一点。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/98/983b70e636fa2c9b2fb03945cd666bf4.jpeg" /></p><p></p><p>截图来源：https://m.weibo.cn/status/5024878134823430</p><p></p><p>第二十七届哈佛中国论坛上，周鸿祎谈到了开源大模型的发展。他认为，虽然现在开源模型的能力达不到&nbsp;GPT&nbsp;4。但是专注在一个方向上，用企业内部的专有数据加上专有工具进行能力的加强之后，它就有可能在专业能力上超过&nbsp;GPT&nbsp;4。</p><p></p><p></p><blockquote>“我是一直相信开源的力量，至于说网上有些名人胡说八道，你们也别被他们忽悠了，说开源不如闭源好。一句话，今天没有开源就没有&nbsp;Linux，没有&nbsp;Linux&nbsp;就没有互联网，连说实话的公司自己都是借助了开源力量才成长到今天。开源社区聚集的工程师和科学家的数量是闭源的数百倍。所以今年开源只做了一年就已经超过了&nbsp;GPT-3.5&nbsp;的能力，所以我觉得未来一两年内，我认为开源的力量很有可能会达到或者超过闭源的水平。”</blockquote><p></p><p></p><h4>雷军回应被称"爽文第一男主"：我不是高考状元&nbsp;卡里也没有冰冷的40亿</h4><p></p><p>4月18日，小米集团CEO雷军回应网上流传的“雷军的爽文人生”。雷军辟谣称，我不是爽文男主，也没考过700分，我也跟大家一样在农村出生，靠勤奋靠努力，也靠这个时代的机会，一步一步走到今天，我觉得人生成功绝对不是什么爽文。大家看一下笑一笑就好，听到这个“爽文第一男主”，我浑身都不舒服，也请大家帮我辟谣。</p><p></p><p>意料之外的，此次直播开场形式不是直接“上链接”，而是先带领大家观光小米汽车的交付中心。雷军表示“今天直播不带货，聊聊天。”直播中，雷军回应穿着模仿马斯克。&nbsp;**雷军称，我其实挺郁闷的，男士西服就那么几种颜色，马斯克那件衣服，好像也只看到他穿过一次，真的是不小心撞衫了。如果我要模仿马斯克，可以模仿得更好。**他提到更委屈的是之前做手机，也被说模仿乔布斯。当时开发布会也没想过要换衣服，就穿着平时的衣服上台了，如果模仿乔布斯也可以“Cosplay”得更像。</p><p></p><p>雷军在直播中辟谣：“我特别希望今天在线的朋友们帮我辟谣，第一个我不是高考状元，**我没有考过700分，我在人生低谷里面卡里也没有冰冷的40亿。**我也跟大家一样，在农村出生，靠勤奋、靠努力，也靠这个时代的机会一步一步走到今天。”对于网络上流传的消息，雷军称“大家看看笑一笑就好，这些都不是真的，所以恳请大家帮我辟谣。</p><p></p><h4>Stability&nbsp;AI宣布裁员20人，约占员工总数的10%</h4><p></p><p>4月18日，据外媒报道，英国&nbsp;AI&nbsp;公司Stability&nbsp;AI&nbsp;宣布裁员20人，占其员工总数的约10%。就在此前一天，该公司刚刚宣布扩大其旗舰模型的使用范围。</p><p></p><p>这一裁员决定是在其创始&nbsp;CEO&nbsp;离职后的动荡几周之后做出的。据&nbsp;CNBC&nbsp;报道，这家总部位于英国的&nbsp;AI&nbsp;公司运行着稳定扩散文本转图像模型，裁员决定是公司&nbsp;“战略计划的一部分，旨在降低我们的成本基础，加强与我们的投资者和合作伙伴的支持，并使团队能够继续开发和发布创新产品。”&nbsp;公司在一份致员工的备忘录中表示，裁员部门尚未明确。</p><p></p><p>Stability&nbsp;AI&nbsp;公司近几个月经历了一段动荡时期，数位知名研究人员以及其创始&nbsp;CEO&nbsp;Emad&nbsp;Mostaque&nbsp;纷纷离职。Mostaque&nbsp;于三月份辞去了他的职务和公司董事会成员身份，表示他想要&nbsp;“追求去中心化的&nbsp;AI”。Stability&nbsp;AI&nbsp;的裁员决定标志着自生成式&nbsp;AI&nbsp;兴起以来首个主要&nbsp;AI&nbsp;基础模型裁员。</p><p></p><p>该公司最近发布了其旗舰型号的下一代&nbsp;Stable&nbsp;Diffusion，并宣布了对&nbsp;Stable&nbsp;Diffusion3的API访问。它还在本月宣布了其&nbsp;Stable&nbsp;Audio&nbsp;文本到音频&nbsp;AI&nbsp;模型的新版本。然而，尽管该公司不断发布&nbsp;AI&nbsp;模型，但也成为涉嫌使用受版权保护的材料来训练其模型的诉讼的对象。去年，盖蒂图片公司起诉了Stability&nbsp;AI&nbsp;。这起案件将很快在英国进行审理。</p><p></p><h4>昆仑万维“天工3.0”基座大模型与“天工SkyMusic”音乐大模型开启公测</h4><p></p><p>4月17日，国内AI公司昆仑万维正式推出了4000亿参数的“天工3.0”基座开源大模型，成为全球最大的开源MoE（混合专家）大模型。</p><p></p><p>相较于上一代，天工3.0的模型技术知识能力提升超过20%，数学、推理、代码、文创能力提升超过30%，多模态性能超越GPT-4V。&nbsp;天工&nbsp;3.0&nbsp;拥有&nbsp;4000&nbsp;亿参数，超越了&nbsp;3140&nbsp;亿参数的&nbsp;Grok-1，是全球最大的开源&nbsp;MoE&nbsp;大模型。天工&nbsp;3.0&nbsp;在语义理解、逻辑推理、通用性、泛化性、不确定性知识、学习能力等领域性能提升显著，数学&nbsp;/&nbsp;推理&nbsp;/&nbsp;代码&nbsp;/&nbsp;文创能力提升超过&nbsp;30%。</p><p></p><p>同样在4月17日，昆仑万维还宣布，中国首个音乐SOTA（领域最佳水准）模型“天工SkyMusic”音乐大模型也正式开启公测，综合体验远超于风靡美国的&nbsp;AI&nbsp;音乐生成平台Suno&nbsp;V3。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/9f/35/9f406c50a3aa1b4ffb16cfba5bb88e35.gif" /></p><p></p><p>天工&nbsp;SkyMusic&nbsp;采用音乐音频领域类&nbsp;Sora&nbsp;模型架构，Large-scale&nbsp;Transformer&nbsp;负责谱曲，来学习&nbsp;Music&nbsp;Patches&nbsp;的上下文依赖关系，同时完成音乐可控性，Diffusion&nbsp;Transformer&nbsp;负责演唱，通过&nbsp;LDM&nbsp;让&nbsp;Music&nbsp;Patches&nbsp;被还原成高质量音频，使其能够支持生成&nbsp;80&nbsp;秒&nbsp;44100Hz&nbsp;采样率双声道立体声歌曲。</p><p></p><h4>微软发布&nbsp;Vision&nbsp;Pro&nbsp;原生&nbsp;OneNote&nbsp;应用</h4><p></p><p>4&nbsp;月&nbsp;17&nbsp;日消息，继此前推出适用于苹果混合现实头戴设备&nbsp;Vision&nbsp;Pro&nbsp;的&nbsp;Office&nbsp;系列应用之后，微软今日又发布了适用于&nbsp;visionOS&nbsp;的原生&nbsp;OneNote&nbsp;应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/422bbca618a19ea4cdd86ccdd1a365d8.webp" /></p><p></p><p>微软一位产品经理在博客中确认了这一消息。根据微软的说法，OneNote&nbsp;将能利用苹果&nbsp;Vision&nbsp;Pro&nbsp;的空间计算功能提供“无限画布”，并可以与其他已经登陆该平台的微软应用&nbsp;(例如&nbsp;Word、Excel&nbsp;等)&nbsp;并排显示。OneNote&nbsp;是微软广受欢迎的笔记应用。</p><p></p><p>微软表示：我们与苹果密切合作多年，将这些体验带到&nbsp;iPhone、iPad&nbsp;和&nbsp;Mac。现在，借助&nbsp;Apple&nbsp;Vision&nbsp;Pro，OneNote&nbsp;将利用空间计算的无限画布，并且可以与其他出色的微软应用程序（如&nbsp;Word、Excel&nbsp;和&nbsp;Teams）并排显示，以实现令人难以置信的多任务处理。</p><p></p><p>当然，适用于&nbsp;visionOS&nbsp;的&nbsp;OneNote&nbsp;应用经过专门优化，可以充分利用&nbsp;Apple&nbsp;Vision&nbsp;Pro&nbsp;的硬件优势，用户可以自由选择用手势操作或通过蓝牙连接的键盘和触控板进行操作。微软还确认，未来的更新将加入对&nbsp;Copilot&nbsp;和双重身份验证的支持。</p><p></p><h4>曝特斯拉中国裁员最高比例50%！上海工厂和所有门店开始裁了，赔偿N+3</h4><p></p><p>本周一，特斯拉宣布全球裁员10%，效力18年的高管也宣布离职。此次裁员影响的员工超过1.4万人，当日特斯拉股价大跌。北京时间周二（16日）晚，马斯克表示，特斯拉需要每隔五年左右进行一次“彻底的组织改革”，间接回应了特斯拉全球大裁员的消息。</p><p></p><p>据4月17日消息，此次裁员也涉及中国市场，特斯拉中国区裁员涵盖多个部门，部分部门已裁员完毕，销售人员是“重灾区”（据悉，一些部门裁员30%-40%，个别部门或指销售部门甚至高达50%，而其他部门普遍在20%左右）。4月17日，上海某特斯拉门店店员回应称：“确实在裁员，但不会影响到消费者，只是内部人员结构的调整”。</p><p></p><p>本轮裁员中，大量特斯拉员工都是大入职6个月以内的新员工，采取0.5N+3赔偿方案，入职一年以上的员工，则采取N+3赔偿方案。有被裁员工自称，入职特斯拉5年，拿到一次性裁员补偿45万元。而新入职的，6个月以内的被裁员工，补偿金普遍在4-5万元左右。</p><p></p><p>北京时间4月17日凌晨3点，特斯拉CEO马斯克在社交媒体平台称，特斯拉正在精简销售和交付体系，这个体系“已变得复杂而低效”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5f1c4bd53fe2def2934a6c5ed892bea5.webp" /></p><p></p><p>据两名被解雇的特斯拉员工透露，一些最近被裁减的员工收到通知，他们的离职协议已于周四被取消，预计会有新的协议。根据媒体获得的一封电子邮件，这些员工收到了一份名为“你的离职协议”的通知，上面写着“状态：取消”。该通知显示：“发件人取消：取消以发送更新协议”。</p><p></p><p>就在此之前，特斯拉CEO埃隆·马斯克（Elon&nbsp;Musk）发送的内部电子邮件表示，该公司本周向一些被解雇工人发放的遣散费过低。马斯克称，“在我们进行重组时，我注意到一些遣散费过低。我对这个错误表示歉意，正在立即纠正。”一些收到通知的前员工此前已经获得了两个月的遣散费。据收到遣散费的五位消息人士称，遣散费的数额似乎并没有根据员工在特斯拉工作的时间长短来衡量，因为在特斯拉工作几个月到几年的员工得到的遣散费是一样的。</p><p></p><h4>刘强东数字人直播，27分钟观看人数破千万</h4><p></p><p>4月16日下午6时18分，由京东云言犀打造的“采销东哥”AI数字人开启直播首秀，同时亮相京东家电家居、京东超市采销直播间。不到1小时，直播间观看量超2000万。从京东给出的数据看，在40分钟的直播内，刘强东数字人带货能力得到了验证。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03e15a43f8191fd9144f09231db2201a.png" /></p><p></p><p>根据京东提供的数据，在刘强东数字人带货期间，直播间整体订单量破10万，直播间观看人数超1300万，创造京东超市采销直播间开播以来观看人数的最高峰；直播时段用户平均停留时长达到日常均值的5.6倍。</p><p></p><p>虽然直播取得开门红，但是不少消费者认为相较于真人主播，数字人直播时语气平淡，表情单一，缺少互动感。此外，对于刘强东数字人今后是否会常态化直播，京东尚未给出回应。此外，后续直播能否延续首播热度，也是问题。</p><p></p><p>“刘强东下场”直播是京东在直播带货领域的一次重要尝试，有望为京东带来更多的关注和流量。同时，也为京东AI电商战略释放新的重大信号。</p><p></p><p>早在一年前，淘宝直播机构开始布局AI数字人直播，一些头部直播间和品牌店铺直播间就尝试使用AI数字人直播带货。据悉，目前阿里成立了一支AI电商团队，正在打造一款针对商家和达人的AI电商产品“绘蛙”，将为淘宝、天猫商家和达人在生成营销文案、训练专属AI模特等方面提升创作效率，使商品快速被种草。</p><p></p><p>然而，这些布局却赶不上创始人亲自下场的声量。这次刘强东数字人参与直播活动，不仅为京东直播引流，也让外界看到了京东在AI技术方面的成果。</p><p></p><h4>蚂蚁语雀创始人，飞书开放平台负责人离职创业</h4><p></p><p>4月16日，有消息称字节跳动旗下开放平台负责人王保平（玉伯）已于近日正式从飞书离职。</p><p></p><p>据了解，玉伯于2008年加入阿里；2012年，加入支付宝前端开发部，负责基础组；2015年，创建体验技术部聚焦蚂蚁中后台业务，并一手打造语雀。其参与过&nbsp;Sea.js、KISSY、Arale&nbsp;等开源项目，在技术圈是有名的前端大佬，曾先后担任支付宝体验技术部负责人、蚂蚁集团终端技术委员会主席。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc09dead28d7de74b692f4989c735491.png" /></p><p></p><p>玉伯在蚂蚁集团工作15年，于2023年5月左右从阿里离职加入飞书。据悉，从阿里离职时，玉伯的职级是P10。同时，他一直以来也都不是一个安于现状的人，所以他从研究所出来了。当然，也是他为何后来从阿里、字节离职的原因之一。</p><p></p><p>今年2月，他开始思考自己的下一步。“AI终于已不再是&nbsp;AIGC，而是有机会做&nbsp;AI应用。不断思考和尝试，让我越来越觉得靠谱可行。虽然是资本的寒冬，但寒冬里大家都谨慎，都谨慎反而成活的概率高。值得出来干一票。”</p><p></p><h4>淘宝登陆Apple&nbsp;Vision&nbsp;Pro商店：实现沉浸式购物</h4><p></p><p>4&nbsp;月&nbsp;15&nbsp;日消息，淘宝在&nbsp;App&nbsp;Store&nbsp;上线了&nbsp;Vision&nbsp;Pro&nbsp;版本，可以通过&nbsp;Vision&nbsp;Pro&nbsp;将商品投影到现实中查看，并与其他商品对比。淘宝与苹果Vision&nbsp;Pro的深度合作，标志着电子商务领域与前沿科技融合的新里程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13caa89f418776d50de0aa1bf17f03ac.png" /></p><p></p><p>使用&nbsp;Vision&nbsp;Pro&nbsp;逛淘宝可以获得&nbsp;3D&nbsp;立体的购物体验，Vision&nbsp;Pro&nbsp;识别到平面之后，商品会吸附在平面上，同时有真实世界的光影效果。用户可以自行放大缩小查看商品细节，也可以选择&nbsp;1:1&nbsp;比例模拟物品在真实世界的大小。</p><p></p><p>从淘宝展示的&nbsp;App&nbsp;界面了解到，Vision&nbsp;Pro&nbsp;版淘宝可以同时展示多个商品，方便用户进行比较，还可以直观地看到物品的长宽高数据。同时，该应用还支持搜索物品并识别物品的形状和大小。</p><p></p><h4>GPT4免费用！Altman“亲自带货”</h4><p></p><p>4月15日，据报道，OpenAI&nbsp;首席执行官Sam&nbsp;Altman本月在旧金山、伦敦和纽约等地分别接待了数百名《财富》世界&nbsp;500&nbsp;强公司的高管。与会者表示，&nbsp;Altman与其他&nbsp;OpenAI&nbsp;高管一同在这些地方为企业提供了人工智能服务，某些情况下更与其“金主”微软进行正面交锋。</p><p></p><p>OpenAI&nbsp;以消费产品引起了生成式&nbsp;AI&nbsp;的爆炸式发展，目前这一最新举动标志着&nbsp;OpenAI&nbsp;正寻求从全球企业获得新的收入来源。仅在上个星期之内，就已有两次在美国举行的会面和一次在英国伦敦举行的会面。与会者表示，&nbsp;Altman在每个城市的活动中直接面向&nbsp;100&nbsp;多名高管发表讲话，推荐自家的产品。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/105ae5778c4b0e5a9a06683d9291c28b.png" /></p><p></p><p>在上述活动中，阿尔特曼和&nbsp;OpenAI&nbsp;首席运营官布拉德・莱特凯普演示了自家产品，包括&nbsp;ChatGPT&nbsp;企业版。OpenAI&nbsp;向企业高管们作出承诺，ChatGPT&nbsp;企业版不会使用客户的数据来训练其模型。</p><p></p><h4>腾讯云&nbsp;4.8&nbsp;故障原因曝光：因&nbsp;API&nbsp;新版本兼容性不够和配置数据灰度机制不足</h4><p></p><p>4&nbsp;月8&nbsp;日15点23分，腾讯云团队收到告警信息，云API服务处于异常状态；随即在腾讯云工单、售后服务群以及微博等渠道开始大量出现腾讯云控制台登录不上的客户反馈。4月14日腾讯云发布&nbsp;4&nbsp;月&nbsp;8&nbsp;日的故障复盘及情况说明，简单来说是发布新版&nbsp;API&nbsp;时出现了兼容性错误。</p><p></p><p>经过故障定位发现，客户登录不上控制台正是由云API异常所导致。云API是云上统一的开放接口集合，客户可以通过API以编程方式管理和操控云端资源，云控制台通过组合云API提供交互式的网页功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e58d8a0a374c24e9b921040426763a2d.png" /></p><p>图片来源：腾讯云</p><p></p><p>此次故障对于服务器等产品本身是没有影响的，即服务器仍然是正常运行的只不过用户无法执行操作，其他产品例如&nbsp;CDN&nbsp;和域名解析等也是同理。</p><p></p><h4>小米应用商店新规：App不得默认勾选、强制捆绑自动续费</h4><p></p><p>4&nbsp;月&nbsp;13&nbsp;日消息，小米应用商店《新增自动续费功能标准》条例今起正式实施，**主要涉及“App&nbsp;不得默认勾选自动续费、强制捆绑开通服务”选项。**最近几年，各平台都流行主推自动续费的付费、会员项目，以首月、首期超低折扣来吸引用户开通。很多年长的用户甚至无法取消，每个月都不停的扣费，甚至设备/App都不用了还在持续扣费。</p><p></p><p>上个月，小米应用商店发布公告，要整治App自动续费等行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c55e54cf1dac3af0f58893d29093639.png" /></p><p></p><p>根据公告，新规今天正式开始生效，尤其强调不得默认勾选、强制捆绑开通自动续订功能。</p><p></p><p>在&nbsp;2023&nbsp;年上半年工业和信息化发展情况新闻发布会上，工业和信息化部新闻发言人赵志国表示将重点整治用户反映突出的欺骗误导下载、强制自动续费等痛点问题。此外，《深圳经济特区消费者权益保护条例》就是针对自动续费问题，并于今年&nbsp;1&nbsp;月&nbsp;1&nbsp;日起正式实施。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>美国一男子盗窃算力被捕，两家知名云商惨遭诈骗350万美元</h4><p></p><p>4月16日消息，联邦检察官向美国纽约东区地方法院起诉一名叫&nbsp;Charles&nbsp;O.&nbsp;Parks&nbsp;III的内布拉斯加州男子。原因是其通过云计算从事大规模非法“加密货币劫持”活动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/08efd16009f2a69aef3cf811342ecc0d.png" /></p><p></p><p>Parks于上周五被捕，尚未提出抗辩，并计划于今日（4月16日）在奥马哈联邦法院首次出庭。</p><p></p><p>起诉书称，Parks创建了一个“加密货币劫持计划”，诈骗两家知名云服务提供商超过350万美元的计算资源，以此非法开采约100万美元的加密货币供个人使用，但并没有付费。</p><p></p><p></p><blockquote>从 2021 年 1 月到 8 月，Parks 在其附属的”MultiMillionaire LLc“和”CP 30 LLC“公司名下，使用不同的个人和企业身份在上述一家知名云服务商的“按需云计算平台”上创建了五个账户。随后，他以“提供强大且昂贵的实例访问权限”为借口推迟账单付款，达到掩人耳目的目的。Parks 通过这种方式，能够最大限度地提高云计算能力并监控每个矿池中哪些实例正在积极挖矿，并消耗了这家云提供商超过 250 万美元的服务。同样地，Parks 使用类似的策略，向另外一家云服务商诈骗了超过 969,000 美元。</blockquote><p></p><p></p><p>虽说Parks绳之以法，但这两家“倒霉”的知名云服务提供商是谁呢？起诉书上表示，这两家云服务提供商分别位于华盛顿州西雅图和雷德蒙德，除此之外并无任何信息。</p><p></p><p>有外媒称，按照地理位置来看，**很大可能是微软和亚马逊云。**有意思的是，去年微软与亚马逊及印度相关部门联手破获了一起“技术支持”诈骗案。相关诈骗团伙伪装成微软与亚马逊的技术支持人员，于印度五个州经营多个虚假“客服中心”，时间长达5年，受害人数据称超过&nbsp;2000&nbsp;人。</p><p></p><p>如今，二者可能又要联手打击这起“加密货币劫持计划”。</p><p></p><h4>新版&nbsp;Firefox&nbsp;125.0.1&nbsp;版发布</h4><p></p><p>2024年4月16日，Firefox&nbsp;125的稳定版本发布了，新增了对AV1编码的支持，这种编码可以与加密媒体扩展（EME）一起使用。</p><p></p><p>此外，该版本还增加了在PDF文件中高亮文本的功能，以及更便捷地从剪贴板粘贴URL到地址栏的功能。Mozilla分别为Windows、Linux和macOS发布了浏览器构建版本。</p><p></p><p>发布延迟的原因。值得注意的是，Mozilla原计划发布Firefox&nbsp;125的时间被推迟了几天，因为在准备发布的最后阶段发现了一个严重的错误。</p><p></p><p>目前，Firefox&nbsp;125.0.1的更新正在各个镜像站点上推出，因此下载链接指向的是125.0.1版本。</p><p></p><h4>Adobe&nbsp;探索与&nbsp;OpenAI&nbsp;合作，推出人工智能视频工具</h4><p></p><p>当地时间4月15日，Adobe宣布，其视频编辑软件Adobe&nbsp;Premiere&nbsp;Pro（简称Pr）今年将推出全新的生成式AI工具，新增功能可借助AI填充和消除一些画面要素。这些新功能将由一个新的AI视频模型提供支持，而该模型将归属于Adobe自研的AI模型Firefly系列。</p><p></p><p>同时，Adobe考虑今年将第三方生成式AI工具嵌入Pr，允许用户调用OpenAI、Runway和Pika&nbsp;Labs等平台的大模型在Pr中生成和使用视频，并且所有视频都将明确标注所使用的AI模型。用户没有使用被官方定义为“商业安全”的AI模型时，则会收到警告。</p><p></p><p>目前，Adobe尚未决定开放第三方AI工具的具体时间，以及如何处理公司和外部开发者之间的收入分配问题。</p><p></p><h4>苹果&nbsp;iOS&nbsp;18&nbsp;的首批&nbsp;AI&nbsp;功能将完全运行于设备端</h4><p></p><p>4&nbsp;月&nbsp;15&nbsp;日，据彭博社的报道，iOS&nbsp;18即将推出的首批全新AI&nbsp;功能将完全在设备端运行，而无需依赖云服务器。</p><p></p><p>古尔曼在今天的&nbsp;Power&nbsp;On&nbsp;时事通讯的问答部分中表示：当全世界都在等待苹果&nbsp;6&nbsp;月&nbsp;10&nbsp;日发布大型人工智能产品时，第一波功能似乎将完全在设备上运行。这意味着该公司的大型语言模型（支持新功能的软件）没有云处理组件。</p><p></p><p>全球对苹果在6&nbsp;月&nbsp;10日至14日举办的WWDC&nbsp;2024全球开发者大会非常期待。预计苹果将在大会上推出iOS&nbsp;18，并有可能介绍AI功能。目前来看，首批功能将完全在设备上运行，这意味着不会涉及大型语言模型的云端处理。</p><p></p><p>据传，iOS&nbsp;18&nbsp;将为&nbsp;iPhone&nbsp;的&nbsp;Spotlight&nbsp;搜索工具、Siri、Safari、快捷指令、Apple&nbsp;Music、信息、健康、数字、页面、Keynote&nbsp;等提供新的生成式&nbsp;AI&nbsp;功能。Gurman&nbsp;此前曾报道，生成式&nbsp;AI&nbsp;将提升&nbsp;Siri&nbsp;回答更复杂问题的能力，并允许信息应用自动完成句子。</p><p></p><h4>英特尔中国特供版Gaudi3&nbsp;AI芯片，性能暴降92%？</h4><p></p><p>4月12日据媒体报道，英特尔在其Gaudi&nbsp;3&nbsp;AI芯片白皮书中披露，正准备向中国市场推出“特供版”Gaudi&nbsp;3。</p><p></p><p>中国特供Gaudi&nbsp;3包括名为HL-328的OAM兼容夹层卡（Mezzanine&nbsp;Card），和名为HL-388的PCle加速卡两种，其中HL-328将于6月24日推出，HL-388将于9月24日推出。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/ded8e899a8073b76ec8f3f945a2e5a52.png" /></p><p></p><p>与原版相比，中国特供版Gaudi&nbsp;3拥有相同的96MB&nbsp;SRAM片上内存，&nbsp;128GB&nbsp;HBM2e高带宽内存，带宽为3.7TB/s，拥有PCIe&nbsp;5.0&nbsp;x16接口和解码标准。</p><p></p><p>但是由于美国对于AI芯片的出口管制，其综合运算性能（TPP）需要低于4800才能出口到中国，&nbsp;这也意味中国特供版Gaudi&nbsp;3的16bit性能不能超过150&nbsp;TFLOPS。</p><p></p><p>而原版Gaudi&nbsp;3在FP16/BF16上的性能可以达到1835&nbsp;TFLOPS，因此中国特供版Gaudi&nbsp;3最终可能需要将其AI性能降低约92%，才能符合美国的出口管制要求。</p><p></p><p>不过性能的降低也使得其功耗大幅降低，根据曝光的资料，中国特供版Gaudi&nbsp;3的PCIe卡和OAM卡的TDP均为450瓦，而原版的性能分别为600瓦和900瓦。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Miepyu9ZScchoYZFqe2h</id>
            <title>Eric Evans 提倡在领域驱动设计中实验大语言模型</title>
            <link>https://www.infoq.cn/article/Miepyu9ZScchoYZFqe2h</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Miepyu9ZScchoYZFqe2h</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Apr 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: DDD, LLM, 领域驱动设计, ChatGPT
<br>
<br>
总结: 在探索DDD技术大会的主题演讲中，领域驱动设计作者Eric Evans建议软件设计师将大语言模型LLM整合到系统中，强调领域驱动设计与面向AI的软件结合，提倡使用有界上下文通用语言训练的模型。他建议使用多个经过精细调整的模型，强调未来领域建模者将涉及解释自然语言输入的任务。一些人支持将LLM用于普通聊天机器人之外的新用途，但也有人对LLM的高成本持怀疑态度。Evans详细介绍了他个人对LLM的实验，强调实验这项技术的益处。 </div>
                        <hr>
                    
                    <p>在 探索 DDD 技术大会的主题演讲中，《领域驱动设计》作者 Eric Evans 建议软件设计师需要寻找创新的方式将大语言模型（LLM）整合到他们的系统中。他鼓励每个人现在就开始学习 LLM 的相关知识，并进行实验，与社区分享这些实验的结果和经验。</p><p></p><p>Evans 认为领域驱动设计（DDD）与面向 AI 的软件可以很好地结合起来。他说：“复杂系统的某些部分与领域模型的结构化部分永远无法匹配，所以我们将这些部分交给人类来处理。也许我们会有一些硬编码、一些人工处理，还有一些由 LLM 来提供支持。”</p><p></p><p>他说，用 DDD 实践者的话来说，经过训练的大语言模型就是一个有界的上下文。与基于广泛语言训练的模型和用于通用目的的模型（如 ChatGPT）不同，基于有界上下文通用语言训练的模型对于特定需求场景更为有用。</p><p></p><p>对于通用的 LLM 来说，我们必须谨慎编写提示词才能实现期望的响应。相反，Evans 建议使用多个经过精细调整的模型，每个模型都用于不同的目的。他认为这是一种强关注点分离。他预测，未来的领域建模者将涉及解释自然语言输入的任务和子域，并将其纳入到设计之中。目前的基础设施还没有完全准备好，但趋势表明这种情况很快就会到来。</p><p></p><p>Evans 强调，对于他的想法，必须考虑到他发表演讲时的背景，即 2024 年 3 月 14 日，因为这个领域变化是如此之快。六个月前，他对这个主题甚至都还不怎么了解，一年后，他现在所讲的可能就变得无关紧要了。他将我们目前的情况比作上世纪 90 年代末，当时他学会了多种构建网站的方法，而今天这些方法都已经不适用了。</p><p></p><p>在大会期间，DDD 社区的其他知名人士对 Evans 的想法作出了回应。《实现领域驱动设计》作者 Vaughn Vernon 在很大程度上支持探索将 LLM 作为普通聊天机器人之外的新用途的想法。在软件自我修复方面，他看到了像 ChatGPT 这样的工具可以响应运行时异常，提出“修复建议”，并自动创建包含代码的 PR 来修复错误。</p><p></p><p>然而，一些人对 LLM 的优点仍然持怀疑态度。在一个关于 DDD 和 LLM 交叉领域的小组讨论中，《微服务模式》作者 Chris Richardson 表达了对 LLM 高昂的经济和计算成本的担忧。当 Richardson 想知道是否有运营 LLM 的服务盈利时，Evans 回答说，通过微调可以让一个廉价的模型变得比一个昂贵的模型更快。另一位小组成员，Honeycomb.io 的首席开发者布道师 Jessica Kerr 表示，“我们需要找到有价值的东西，然后让它变得经济实惠。”</p><p></p><p>在主题演讲中，Evans 详细介绍了他个人对 LLM 所做的一些实验。最开始，他与游戏设计师 Reed Berkowitz 合作，尝试使用 ChatGPT 让一个非玩家角色（NPC）对玩家输入作出响应。通过尝试一系列提示词，他发现如果将响应分成较小的片段而不是长提示词，响应会更加一致。这种方法符合他对 DDD 解决复杂问题的想法。</p><p></p><p>对于更小、更专业化的提示词的需求自然而然地导致了对更专业化的模型的需求，这样既可以提供更好的输出，还可以更有效地提高性能和降低成本。他解释说，他的研究目的是为了表明实验这项技术是多么有用。尽管有时也令人沮丧，但这个过程是非常有益的。许多与会者表示，当你第一次学会如何让一个新事物工作时，你会体验到一种满足感。</p><p></p><p>探索 DDD 大会 于 2024 年 3 月 12 日至 15 日在科罗拉多州丹佛市举行。大会的大部分演讲都已被记录下来，将在接下来的几周内发布到 @ExploreDDD YouTube 频道，并在 Explore DDD LinkedIn 页面 上分享，从 Eric Evans 的开幕主题演讲开始。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/03/Evans-ddd-experiment-llm/">https://www.infoq.com/news/2024/03/Evans-ddd-experiment-llm/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jK3bi2rpjkrg90BZMasC</id>
            <title>过去一年，中国车企“上车”大模型进展如何？</title>
            <link>https://www.infoq.cn/article/jK3bi2rpjkrg90BZMasC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jK3bi2rpjkrg90BZMasC</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Apr 2024 11:03:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI in ALL, 智能化竞争, AI 大模型, 自动驾驶
<br>
<br>
总结: 随着AI技术在汽车行业的快速发展，各大车企纷纷加入智能化竞争，利用AI大模型推动自动驾驶技术的发展。未来汽车制造将迎来全面数字化和智能化的新阶段。 </div>
                        <hr>
                    
                    <p>随着“AI in ALL”的风迅速刮进汽车行业，不少汽车制造商如红旗、长城、东风日产、吉利等已纷纷宣布加入“文心一言”生态，这是各个车企宣示抢占新技术高地的第一声呐喊。</p><p></p><p>这些车企应用 AI 大模型，既是迎接数字化时代的主动备战，也是进行差异化竞争的必然之举。</p><p></p><p>据中国汽车工业协会数据，2023 年中国商用车产销累计完成 403.7 万辆和 403.1 万辆，同比分别增长 26.8% 和 22.1%，增速超过行业整体水平。市场规模持续增长的同时，更多自主品牌汽车入局，消费者需求也正在发生深刻变化。这些变化预示着行业即将迈过打价格战的行业拐点，转向以技术和智能化为主导的新竞争阶段。</p><p></p><p>AI 大模型的引入正在推动这场革命，带来了全新的竞争焦点：智能化的实施和车辆的全面数字化。车企利用 AI 技术优化设计流程、提高生产效率和增强用户体验，正是智能化下半场的关键战略。现在，随着越来越多的企业“大模型上车”，我们正在见证智能汽车制造影响整个行业的未来走向。</p><p></p><h2>大模型上车</h2><p></p><p></p><h4>（一）智能驾驶 VS 自动驾驶</h4><p></p><p></p><p>大模型的崛起为自动驾驶技术研发注入了一剂强心剂。自动驾驶的核心问题是如何精准识别诸多传感器所采集的环境信息并迅速作出准确判断，而大模型具有对海量数据的分析能力、多维度分析能力、全面预测能力，用于解决自动驾驶面临的数据标注等难题是再好不过的。</p><p></p><p>2023 年 4 月，长城汽车控股的毫末智行发布了全球首个自动驾驶生成式大模型 DriveGPT 雪湖·海若，通过引入真实人驾接管数据建立 RLHF（人类反馈强化学习）技术，对自动驾驶认知决策模型进行持续优化。目前，DriveGPT 已完成 4000 万公里驾驶数据的训练，参数规模达到 1200 亿，但尚不能实现端到端自动驾驶，还处在从离散到感知模型、认知模型、控制模型聚集的阶段。</p><p></p><p>相比无人驾驶和完全自动驾驶，如上汽集团的智己汽车采用的导航辅助驾驶技术（NOA）看起来更为现实。智己汽车与全球头部智能驾驶算法企业 Momenta 合作，推出了行业首个 D.L.P.（深度学习算法）人工智能模型，将感知、融合、预测三个环节进行了模型化，并完成了深度集成。</p><p></p><p>在此基础上，今年 4 月 8 日发布的新车智己 L6 已同时使用 DDOD（对道路上动态物体的监测和识别，Data Driven Object Detection) 模型和可替代高精地图的 DDLD （对道路地表和静态元素的识别，Data Driven Landmark Detection）融合感知大模型，并且“全国都可开”的无图城市 NOA 将于今年年内开通。</p><p></p><h4>（二）智能座舱</h4><p></p><p></p><p>尽管完全自动驾驶是许多人眼中的最终目标，但目前这一目标还需要较长时间实现。在此之前，受市场需求的影响，汽车已经在向“移动第三空间”发展，智能化技术如何增强驾驶体验成为重要议题，智能座舱由此应运而生。</p><p></p><p>例如，智己汽车于去年 6 月发布的整车智能化软件产品“全程 AI 舱”，不仅整合了软硬件技术，还在安全和舒适性上做了大量优化。</p><p></p><p>与此同时，奇瑞汽车的人工智能大模型平台“LION AI”，以及广汽集团的 AI 大模型平台，都在智能语音交互方面取得了重要进展，为用户提供更自然的对话体验。</p><p></p><p>这类技术的应用不仅局限于车内交互，像吉利就推出了车外 AI 语音交互功能，让智能汽车在可以识别来自车外发开后备箱、开空调等语音指令的同时，还学会了上车迎宾、下车欢送等。吉利的星睿 AI 大模型还创新性地推出了多项 AI 原生应用，如 AI 绘本、AI 回忆、AI 音乐律动等，增强了车辆的沉浸式体验。</p><p></p><h4>（三）全栈智能</h4><p></p><p>在 AI 大模型的应用上，比亚迪和北汽蓝谷等公司正在进一步推动整车智能化。</p><p></p><p>比亚迪的双循环多模态 AI“璇玑”和智能化架构“璇玑”将 AI 技术应用到车辆的各个方面，覆盖超过 300 个使用场景，旨在通过打破系统间的壁垒，实现信息的即时捕捉和决策反馈。</p><p></p><p>北汽极狐于 4 月 11 日推出的全栈生态自进化技术体系“达尔文 2.0”，则强调了整车智能化、设备协同和信息共享的重要性，旨在通过技术自进化减少人工干预，提高车辆的效率和安全性。</p><p></p><p>在长城汽车 AI Lab 负责人杨继峰看来，到现在主机厂们都还在比拼有没有语音操控、DMS 和氛围灯等功能，这些都不能算是 AI 问题，而只是场景定义。只有当智能座舱向智能空间发展时才能变成一个 AI 问题。而智能空间要求在智能座舱中加入多模态感知、认知大模型和 AIGC 大模型，基于数据的支撑和算法的推理，来提升整体的 AI 能力，实现自然交互。这一概念听起来相当吸引人，但同时也相当“道阻且长”。</p><p></p><h4>大模型在车辆制造中的应用</h4><p></p><p>随着汽车行业的数字化转型，数据正在从生产中的“副产品”向“生产资料”转变，AI 大模型的引入能够打破生产制造、研发设计、财务管理、营销售后等环节之间存在的数据壁垒，帮助实现“生产资料”在全产业链自动化畅通流转。</p><p></p><p>用 AI 大模型改造车企自身业务流程，不仅是为了更好地卖车，更是为了重塑汽车生产方式、真正实现降本增效。正如中国一汽红旗品牌运营委员会副总裁门欣所说：“真正的转型是要把传统工业企业依赖职责、流程运行的内核转换成依赖数据，要高速响应用户需求，形成不断向前迭代的业务能力和开发能力。”</p><p></p><p>麦肯锡咨询公司全球管理合伙人关明宇曾指出，在过去的十年里车内软件的复杂程度大概翻了两番，但同期软件的开发效率只提高了 1—1.5 倍。缩短研发周期、降低研发门槛、提高研发效率是车企在行情快速变化的市场中保持竞争力的重要途径。</p><p></p><p>中国一汽正在尝试用大模型来达到这一目的。今年 1 月一汽与阿里云通义千问合作开发的的汽车行业的首个大模型商业智能应用 GPT-BI 落地，通过自动化报表生成和决策支持，颠覆了传统的业务流程。此外，一汽还利用大模型写设计代码，目前中国一汽已经实现了自动化设计、自动化绘图、自动化代码生成，基于模型的系统工程将持续迭代。据门欣表示，有了大模型后，至少一半的代码可以交由大模型来写。</p><p></p><p>吉利的星睿 AI 大模型是将自研的 NLP 语言处理模型与 NPDS 研发体系及其全链路场景数据库深度融合的一个例子。其支持研发人员在造型设计、机械设计和质量控制等方面的应用，同时也用于自动驾驶的虚拟训练。通过这种方式，吉利能够缩短验证周期约 30%，并节约近 50% 的开发成本。</p><p></p><h2>底层技术仍有待升级</h2><p></p><p>尽管大模型的应用提供了诸多好处，车企仍面临一些技术和基础设施挑战。例如，车载算力的限制使得很多 AI 处理必须依赖于云端服务器。为了克服这一点，一些公司建立了智能计算中心，如毫末智行与火山引擎合作建立的自动驾驶智算中心，以及长安汽车与百度共建的智算中心，提供了必要的后端算力支持。</p><p></p><p>此外，对大模型在汽车行业的应用而言，车企拥有的海量数据资源有其双面性。一方面自动驾驶涉及到红外线传感器、激光雷达、毫米波雷达、摄像头、GPS 等诸多硬件，这些硬件在行驶过程中产生的海量数据为大模型算法研发提供了一定基础；另一方面，如何收集、清洗、训练来源于大量不同场景、不同维度的数据，本身就是一大难题。</p><p></p><p>最后，大模型应用究竟是降本还是增本、这么多车企投入大量研发经费究竟有多大效果，目前还很难说明白。单论智驾芯片的成本，这场 AI 竞争就不是所有玩家都玩得起的。据统计，虽然过去三年中国汽车芯片的自给率从 5% 迅速提高到了 10%，像地平线、黑芝麻智能这样的供应商正在迅速崛起，但整体来看车载芯片仍被外资品牌垄断。如果智能汽车的各个“器官”都要从不同的供应商处采购，不仅难以实现各系统联动融合，更难降低研发生产成本、真正实现大模型量产上车。从这个角度而言，比亚迪坚持全栈自研“整车智能”的战略似乎不无道理。</p><p></p><p>无论是被卷入还是主动进入，这波 AI 浪潮冲击下的行业洗牌都已在所难免。是机遇是挑战，都有待车企自己去蹚一趟。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>