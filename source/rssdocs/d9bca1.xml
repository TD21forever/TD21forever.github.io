<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/OGb6vlQobPORvmGaFUJa</id>
            <title>都2024年了，美国地铁还在“死磕”软盘和100年前架构，网友：不上云更安全</title>
            <link>https://www.infoq.cn/article/OGb6vlQobPORvmGaFUJa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OGb6vlQobPORvmGaFUJa</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 09:52:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, 旧金山地铁, 纽约地铁, 软盘技术
<br>
<br>
总结: 在AIGC全面爆发的今天，美国旧金山地铁仍然沿用着已经被淘汰的软盘技术，而纽约地铁还被困在100年前的IT架构上。AIGC以其强大的创造力和学习能力，正在全球范围内掀起一场技术革命，而这场技术革命背后，离不开先进的计算机硬件和高效的软件系统支持。但令人遗憾的是，在这股技术狂潮的席卷之下，一些机构却仍在使用老旧的计算机设备和落后的软件技术来迎接瞬息万变的未来。 </div>
                        <hr>
                    
                    <p></p><blockquote>在AIGC全面爆发的今天，美国旧金山地铁仍然沿用着已经被淘汰的软盘技术，而纽约地铁还被困在100年前的IT架构上。</blockquote><p></p><p>&nbsp;</p><p>AIGC以其强大的创造力和学习能力，正在全球范围内掀起一场技术革命，而这场技术革命背后，离不开先进的计算机硬件和高效的软件系统支持。但令人遗憾的是，在这股技术狂潮的席卷之下，一些机构却仍在使用老旧的计算机设备和落后的软件技术来迎接瞬息万变的未来。</p><p>&nbsp;</p><p>近日，旧金山交通局的列车系统就因“全手动操作且仍继续遗留着几十岁‘高龄’的软盘组件”而引发关注。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ef703248f723804cdebfe8c4195a666.jpeg" /></p><p></p><p>身在引领IT技术发展的硅谷，旧金山的列车控制系统居然依靠软盘为载体保持运行……这怎么可能？当地乘客Katie Guillen惊讶表示，“啊？我还以为我们已经步入AI时代，结果还在使用软盘？”</p><p></p><h2>旧金山“老古董”列车控制系统引发热议，没有软盘走不了</h2><p></p><p>&nbsp;</p><p>负责运营当地地铁轻轨系统的旧金山交通局（SFMTA）号称是全美第一家使用软盘介质的机构。但现如今，交通局方面正急于放弃对5英寸软盘的依赖，前提是……给他们十年左右时间再加上数亿美元投资。</p><p>&nbsp;</p><p>为了让公众了解更详细的信息，旧金山交通局几位工作人员最近接受了ABC7湾区新闻的采访，具体介绍了该机构每天早上如何使用3张5英寸软盘启动列车控制系统。</p><p>&nbsp;</p><p>自1998年被安装在市场街地铁站以来，这些软盘一直成为Muni Metro旧金山地铁自动列车控制系统（ATCS）的重要组成部分。如今26年过去，交通局的工作人员每天早上仍在依靠软盘来指挥列车如何运行。</p><p>&nbsp;</p><p>虽然现在来看这套系统已经“老掉牙了”，把时钟拨回旧金山交通局部署这套自动列车控制系统的1998年，其使用的确实是当初最前沿的技术成果。</p><p>&nbsp;</p><p>来自交通局列车控制项目组的Mariana Mauire解释称，“我们是全美首家采用这项特别技术的机构，那个时候计算机甚至还没有磁盘驱动器，必须通过软盘将软件加载到计算机上。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea0afe5ffd0f44736e38bf4c3d248e9f.jpeg" /></p><p></p><p>当记者Luz Pena问到 “软盘在列车控制系统中起着怎样的作用？”时，Mariana Maguire回答称： “软盘属于整个系统的组成部分，这套系统负责自动控制地铁内的列车。我们在全市范围内运营的地铁系统包含大量依靠软盘运行的组件。”</p><p>&nbsp;</p><p>也就是说，旧金山交通局的列车控制系统每天早上必须要借助5英寸软盘方可正常启动。</p><p>&nbsp;</p><p>另一位交通局发言人Michael Roccaforte详细解释了该系统的运行原理。该控制系统中包含多个组件，包括与推进/制动系统、中央及本地服务器，外加环路信号线缆等通信基础设施相对接的车载计算机。这些软盘的主要作用是加载运行中央服务器的软件。Roccaforte指出：</p><p>&nbsp;</p><p></p><blockquote>当列车驶入地铁站时，车载计算机会接入列车控制系统，以自动模式驾驶列车，保证车辆在操作员的监督下自行运转。而在驶离地铁站时，车辆会断开与控制系统的连接并返回手动操作模式。</blockquote><p></p><p>&nbsp;</p><p>Mariana Mauire指出，“整个系统在晚间关闭后就如同失忆了一般。到第二天早上，就得有人重新提醒它「你是谁，你今天需要达成的运行目标是什么」。”</p><p>&nbsp;</p><p>如此“智障”的旧系统为什么没有被换掉？为什么不把软盘升级成无线传输系统？</p><p>&nbsp;</p><p>该局交通总监Jeffrey Tumlin在采访中表示，“这会带来新的风险。系统目前运行良好，我们当然也知道随着时间推移，软盘数据退化的风险也在不断增加，甚至随时可能引发灾难性故障。”</p><p>&nbsp;</p><p>Roccaforte表示，对列车控制系统开展全面改造的初步计划（包括取消软盘）早在2018年就已经开始，预计从初步规划到最终完成需要十年时间。由于新冠疫情爆发造成长达18个月的进度中断，预计实际完工时间将延后至2029年至2030年。旧金山交通局预计在2025年初确定承包商，届时将发布详尽的项目时间表。</p><p></p><h2>系统升级需要十年时间，花费数亿美元</h2><p></p><p>&nbsp;</p><p>常言道“只要还没坏，那就尽管用。”可虽然软盘列车控制系统目前仍能正常运行，但继续依赖过时技术仍存在巨大隐患。旧金山交通局多年来也一直在强调这个问题。</p><p>&nbsp;</p><p>交通局方面表示，这套列车控制系统的设计使用寿命仅为20到25年，也就是说从2023年之后已经属于计划外使用周期。据称由地方及国家交通专家组成的市政可靠性工作组曾于2020年提出，建议在五到七年内建立新的交通控制系统。</p><p>&nbsp;</p><p>在被问及对现有软盘系统进行升级有多“迫切”时，Tumlin表示问题的关键在于风险。</p><p>&nbsp;</p><p>此前，旧金山交通局就曾表示随着时间推移，列车控制系统的维护正变得愈发困难且昂贵。他们还承认，为这类过时系统寻找技术人员的难度也越来越高。</p><p>&nbsp;</p><p>Tumlin在去年接受采访时坦言，“我们必须留住精通90年代编程语言的程序员，才能保证这套系统继续正常运行，就是说我们的技术债务可以追溯到几十年前。”</p><p>&nbsp;</p><p>2020年，一位部门发言人向《旧金山纪事报》证明，当时交通局交管员的本科生占比为40%到50%。</p><p>&nbsp;</p><p>在被问及放弃软盘系统是否会导致裁员时，Roccaforte回应称：</p><p>&nbsp;</p><p></p><blockquote>随着新型列车控制系统的上线，现有员工仍有大量岗位可以选择，并接受相应的技术培训。我们升级项目战略中的一大关键，就是培养内部技能并对现有员工开展培训。此外，我们还需要聘请信号工程师等更多技术人才，以协助支持新的列车控制系统。</blockquote><p></p><p>&nbsp;</p><p>2020年，Tumoin在接受《旧金山纪事报》采访时指出，他早在2007年就得知该系统需要更新，但承认系统本身并不存在“迫在眉睫的升级需求”。</p><p>&nbsp;</p><p>“虽然仍然依靠从5英寸软盘加载的DOS系统运行，但整个体系的确运行良好。”</p><p>&nbsp;</p><p>旧金山交通局发言人Mariana Maguire上周在接受ABC7采访时表示，升级项目将使得列车控制系统“在自动驾驶技术的帮助下轻松跟踪全城列车的运行和移动，同时增强人为干预能力。”</p><p>&nbsp;</p><p>然而，预算挑战导致项目的预定时间表遭受质疑。Roccaforte表示，交通局的列车升级项目不仅涉及软盘迁移，还需要“对当前列车控制系统及其所有组件进行全面检修，包括车载计算机、中央与本地服务器以及通信基础设施。”</p><p>&nbsp;</p><p>比陈旧软盘系统更重要的是环路线缆系统，负责在中央服务器与列车之间传输数据。根据Roccaforte的介绍，“其带宽甚至还不及早期AOL拨号调制解调器。”</p><p>&nbsp;</p><p>旧金山交通局在其官方网站上补充称：</p><p>&nbsp;</p><p></p><blockquote>环路线缆脆弱且容易受到干扰，导致地铁维护变得愈发困难。这意味着该系统无法沿地面轻轨延伸至地铁站之外，因此在地面环境下仍未实现自动列车控制。</blockquote><p></p><p>&nbsp;</p><p>Roccaforte还提到，交通局正计划升级至“现代通信技术，例如光纤或Wi-Fi”。</p><p>&nbsp;</p><p>Tumlin强调，交通局希望能由州和联邦政府拨款承担列车控制系统升级预算中的“很大一部分”，而“其余部分则由本市市政铁路正迅速减少的内部资金消化。”交通局拒绝透露截至目前已经在系统更新上花掉的费用。</p><p>&nbsp;</p><p>旧金山交通局不单自身多年来一直依赖软盘运行，同时还与其他采用软盘存储的机构保持长期合作，包括货运航空公司以及提供定制刺绣的纺织供应商。</p><p></p><h2>饱受诟病的老旧IT系统为何难以替换？</h2><p></p><p>&nbsp;</p><p>美国一些地方的老旧IT系统饱受诟病已经不是什么新鲜事。前几年，就有媒体曝出全球贸易中心纽约的地铁基本上天天延误。因为纽约市地铁系统采用的是二战前的技术。</p><p>&nbsp;</p><p>导致延误发生的原因是控制列车的通信系统过于老旧，但即便是这样的老旧通信系统，将其安装在一条地铁线上也要花6年时间和 2.88 亿美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/383652f6b5b91da31e9f3df486a4ebde.png" /></p><p></p><p>在西四街车站，大都会交通管理局员工手动记录列车运行情况。</p><p>&nbsp;</p><p>大都会交通管理局员工在接受Business Insider采访时称：“在我们的系统中，不仅仅具有 100 年历史的架构，还有很多古老的基础技术。”</p><p>&nbsp;</p><p>该员工还表示要将纽约地铁上拥有百年历史的信号灯、手动控制开关替换掉并且升级更新老化系统，需要花费近10年的时间以及付出200亿美元的代价。</p><p>&nbsp;</p><p>一位居住在旧金山的ID为iancmceachern的Hacker News用户表示，这种情况在机床领域（铣床、车床等）中比较常见。自己曾经给交通局发了电子邮件诉说过对于软盘问题的担忧，交通局回复他称：“软盘事件是个值得关注的问题，但其实这只是冰山一角。整个老旧系统中每一层都需要更换”。</p><p>&nbsp;</p><p>ID为jandrese的用户这些老旧系统存在很大的隐患，软盘算是比较常见的古董零部件了，如果摊上其他冷门的硬件则会更麻烦。他称：</p><p>&nbsp;</p><p></p><blockquote>由于复古计算社区的努力，软盘模拟器到处都是。但是，对于那些已经破产的公司的定制板、不透明的ROM芯片、PLA等设备的模拟器，一旦出了问题挑战将会大得多。&nbsp;如果他们有所有部件的良好电路图，可能可以通过几个熟悉电路和烙铁的聪明电子工程师来维持系统的长期运行，但最终他们可能会因某个冷门的零件用完，陷入困境。</blockquote><p></p><p>&nbsp;</p><p>值得一提的是，一些人认为尽管更新周期很长，但更新周期是在合理范围内的。比如1996年投入使用的Breda列车在使用了大约20年后开始逐步淘汰，大多数人普遍认为这是一个合理的时间去做一些更新。经历过5.25英寸软盘时代的人们或许能够理解，当系统完成了它的使命时，更换是合理的。到那时，需要更换的不仅仅是存储介质，而在此之前它能够正常运行也无需更换。</p><p>&nbsp;</p><p>ID名为Workaccount2的Hacker News用户认同上述观点。他们公司为另一个全球大城市的基础设施提供新的计算模块，这些基础设施仍然依赖于20世纪80年代初的英特尔CPU。也就是说，他们公司正在为装有超过40年历史的芯片的机构安装新电路板。</p><p>&nbsp;</p><p>Workaccount2表示：“他们对更新系统没有表现出任何兴趣。这个系统运行正常，他们可以获得服务，也可以为损坏的部件获得新的替代品。”</p><p>&nbsp;</p><p></p><blockquote>但客户可能不知道的是，基本上只有一个我们的工程师（可能是地球上唯一一个）知道如何修理这些东西。他已经年纪很大了，显然年轻的工程师根本没有兴趣学习这些古老被遗忘的系统。</blockquote><p></p><p>&nbsp;</p><p>尽管引发了广泛议论，但有一些网友认为交通局至今仍使用软盘的行为是有道理的，因为比起上云，本地软盘更安全。</p><p>&nbsp;</p><p>ID名为Zuu47的用户则表示，人们没有意识到云上的新系统可能会被黑客攻击，使用软盘更安全。</p><p>&nbsp;</p><p>总结下来，这些安装在机床领域的老旧IT系统之所以难以替换，无非有三点原因：第一，系统还能用，没到必须要更换的程度；第二，更换成本太高了，动辄数亿美元；第三，牵一发而动全身，工程量太过庞大，需要耗时许多年。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://petapixel.com/2024/04/09/san-franciscos-train-system-is-still-running-on-floppy-disks/">https://petapixel.com/2024/04/09/san-franciscos-train-system-is-still-running-on-floppy-disks/</a>"</p><p><a href="https://arstechnica.com/gadgets/2024/04/5-25-inch-floppy-disks-expected-to-help-run-san-francisco-trains-until-2030/">https://arstechnica.com/gadgets/2024/04/5-25-inch-floppy-disks-expected-to-help-run-san-francisco-trains-until-2030/</a>"</p><p><a href="https://www.businessinsider.com/nyc-mta-subway-delay-2017-6">https://www.businessinsider.com/nyc-mta-subway-delay-2017-6</a>"</p><p></p><h2>&nbsp;</h2><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5qjbPBwZGoFVeoMQmw7z</id>
            <title>我在技​​术面试中用ChatGPT作弊，没人知道</title>
            <link>https://www.infoq.cn/article/5qjbPBwZGoFVeoMQmw7z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5qjbPBwZGoFVeoMQmw7z</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 09:26:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 面试作弊, 面试问题, 实验准备
<br>
<br>
总结: ChatGPT已经改变了人们的工作方式，但在技术面试中可能会引发作弊问题。通过招募专业面试官和用户进行实验，发现公司需要修改面试问题类型以防止作弊。 </div>
                        <hr>
                    
                    <p>众所周知，ChatGPT已经彻底改变了人们的工作方式。它既能帮助小型企业自动化管理任务，又能为Web开发人员编写整个React组件，它的作用可以说怎么夸都不过分。</p><p></p><p>在interviewing.io，我们一直在思考ChatGPT将给技术面试带来什么变化。一个很大的问题是：ChatGPT会让面试作弊变得很简单吗？在TikTok上的一个视频中，一名工程师让ChatGPT准确地回答面试官的问题：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a15ff78c1607ab081c29164385612040.png" /></p><p></p><p></p><p>人们最初对这类作弊软件的反应和预期完全一致：</p><p></p><p>Redditor说，“<a href="https://www.reddit.com/r/singularity/comments/12zyela/chatgpt_spells_the_end_of_coding_as_we_know_it/">众所周知，ChatGPT是编码的终结。</a>"“YouTuber说，“<a href="https://www.youtube.com/watch?v=OeebS-VcSH0">软件工程已死，ChatGPT杀死了它。</a>"”X（之前的Twitter）发出疑问，“<a href="https://twitter.com/intx_podcast/status/1635396953109561344">​​ChatGPT意味着编码面试的终结吗？</a>"”</p><p></p><p>ChatGPT可以在面试过程中为人提供帮助，这似乎很明显，但我们想知道的是：</p><p></p><p>它能在多大程度上提供帮助？作弊（并逃脱惩罚）有多容易？使用LeetCode问题的公司需要对面试过程做出重大改变吗？</p><p></p><p>为了回答这些问题，我们招募了一些专业面试官和用户来进行作弊实验！下面，我们将分享我们发现的一切。稍微剧透一下，有一点你要知道：公司需要修改面试问题的类型，而且是马上！</p><p></p><p></p><h2>实验准备</h2><p></p><p></p><p>interviewing.io是一个面向工程师的面试实践平台和招聘市场。工程师借助我们的平台来模拟面试。企业利用我们的平台招聘优秀的员工。我们的生态系统中有成千上万的专业面试官，也有成千上万的工程师使用我们的平台准备面试。</p><p></p><p></p><h3>面试官</h3><p></p><p></p><p>面试官来自我们的专业面试官池。他们被分成三组，每组问不同类型的问题。面试官不知道这个实验是关于ChatGPT或作弊的；我们告诉他们，“这项研究的目的是了解面试官的决策随时间变化的趋势，尤其是在问标准和非标准面试题的时候。”</p><p></p><p>以下是3种问题类型：</p><p></p><p>LeetCode原题：面试官根据自己的判断直接从LeetCode中选取的题目，没有做任何修改。</p><p></p><p>例如：一字不差地问LeetCode上的<a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a>"问题。</p><p></p><p>改良LeetCode问题：对从LeetCode上获得的问题做一些修改，虽然与原题类似，但也有明显的不同。</p><p></p><p>例如：对于上面的<a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a>"问题，将输入从3个整数(0,1,2)改为4个整数(0,1,2,3)。</p><p></p><p>自定义问题：所提的问题和网络上已有的任何问题之间都不存在直接的联系。</p><p></p><p>例如：给你一个日志文件，格式如下：- :  -  -，你的任务是识别会话中代表参与度中值的用户。只考虑贡献分数大于50%的用户。假设这类用户的数量是奇数，那么你需要按贡献分数排序后找到位于中间的那个用户。对于下面的文件，正确的答案是SyntaxSorcerer。</p><p><code lang="null">LOG FILE START

NullPointerNinja: "who's going to the event tomorrow night?" - 100%

LambdaLancer: "wat?" - 5%

NullPointerNinja: "the event which is on 123 avenue!" - 100%

SyntaxSorcerer: "I'm coming! I'll bring chips!" - 80%

SyntaxSorcerer: "and something to drink!" - 80%

LambdaLancer: "I can't make it" - 25%

LambdaLancer: "🙁" - 25%

LambdaLancer: "I really wanted to come too!" - 25%

BitwiseBard: "I'll be there!" - 25%

CodeMystic: "me too and I'll brink some dip" - 75%

LOG FILE END</code></p><p></p><p>更多关于问题类型和实验设计的信息，可以阅读<a href="https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit">面试官实验指南文档</a>"：</p><p><a href="https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit">https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit</a>"</p><p></p><p></p><h3>面试者</h3><p></p><p></p><p>面试者来自我们的活跃用户池，我们邀请他们参加一个简短的调查。我们的选择标准如下：</p><p></p><p>在当下的市场上积极地找工作；有4年以上的工作经验，正在申请高级职位；他们对“ChatGPT编码”的熟悉程度为中等或高等；认为自己可以在面试中作弊而不被发现。</p><p></p><p>这种选法可以帮助我们选出那些可能会在面试中作弊的求职者。他们有这样做的动机，并且已经相当熟悉ChatGPT和编码面试。</p><p></p><p>我们告诉面试者，他们在面试中必须使用ChatGPT，目的是测试他们使用ChatGPT作弊的能力。他们还告诉他们，不要尝试凭借自己的技能通过面试，主要要依靠ChatGPT。</p><p></p><p>我们总共进行了37场面试，其中32场有效（我们不得不去掉了5场，因为参与者没有按要求进行）：</p><p></p><p>11场采用“LeetCode原题”9场采用“改良LeetCode问题”12场采用“自定义问题”</p><p></p><p>说明：因为我们平台允许匿名，所以我们的面试只有音频没有视频。匿名是为了帮用户创造一个安全空间，让他们可以快速失败并学习，而没有人会对他们做评判。对用户来说，这是件好事。但我们承认，没有采访视频会让我们的实验变得不那么真实。在真正的面试中，你会面对镜头，这让作弊变得更加困难——但并不能消除作弊。</p><p></p><p>面试结束后，面试官和面试者都要完成一份退场调查。我们问面试者在面试中使用ChatGPT时遇到的困难，而对于面试官，我们问他们对面试的担忧——我们想看看有多少面试官会将他们的面试标记为有问题，并报告他们怀疑存在作弊行为的面试。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91cac2646a7a5ea36736d4e1ef66ab3d.jpeg" /></p><p>﻿后续调查：面试者问题</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f61e78e895b15944ec8c9e8c1eab0b77.jpeg" /></p><p>﻿后续调查：面试官问题</p><p></p><p>我们不知道实验中会发生什么，但假如有一半作弊的求职者成功通过面试，那么对于我们行业来说，那将是一个很能说明问题的结果。</p><p></p><p></p><h2>实验结果</h2><p></p><p></p><p>在剔除了参与者没有按要求进行的面试后，我们得到了以下结果。我们的对照组是求职者在interviewing.io模拟面试中的表现，来自本次实验之外，通过人数占53%。需要注意的是，我们平台上大多数的模拟面试采用的都是LeetCode风格的问题，这是有道理的，因为FAANG公司主要问的就是这些问题。我们一会儿再回来讨论这个问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c41b4a815a594c2acadfd47ee2fffcbe.png" /></p><p>与平台平均值和“自定义”问题相比，“原题”的通过率要高得多。“原题”和“改良”问题的差异无统计学意义。“自定义”问题的通过率明显低于其他任何一组。</p><p></p><p></p><h3>回答原题，表现最好</h3><p></p><p></p><p>不出所料，使用原题的那一组表现最好，73%通过了面试。面试者反映，他们从ChatGPT得到了完美的解决方案。</p><p></p><p>以下是对这一组做面试后调查时得到的最值得注意的评论——我们认为它特别能说明许多面试官的想法：</p><p></p><p></p><blockquote>应聘者之所以能够轻松回答这个问题，很难判断是因为他们真的很好，还是因为他们以前听说过这个问题。通常情况下，为了区分这两种情况，我会对问题做一两处修改。</blockquote><p></p><p></p><p>通常情况下，为了获得更多的信息，面试官会跟进问一个改良过的问题。所以让我们看看采用“改良问题”的那一组，看看面试官是否真的通过对问题做一两处修改获得了更多的信息。</p><p></p><p></p><h3>回答改良问题，要更多提示</h3><p></p><p></p><p>请注意，这个组拿到的是一个标准的LeetCode问题，但他们用无法从网上直接找到的方式对其做了修改。也就是说，ChatGPT不可能有这个问题的答案。因此，面试者更依赖于ChatGPT实际解决问题的能力，而不是它背诵LeetCode教程的能力。</p><p></p><p>不出所料，这一组的结果与“原题”组没有太大区别，67%的求职者通过了面试。</p><p></p><p>事实证明，这种差异与“原题”组没有统计学上的显著差异，即“改良问题”和“原题”本质上是相同的。这个结果说明，ChatGPT可以处理面试官对问题的微调，这种微调并不会给它带来多少麻烦。</p><p></p><p>然而，面试者确实也注意到，让ChatGPT解决经过修改的问题需要提供更多的提示。有一位面试者是这样说的：</p><p></p><p></p><blockquote>回答直接来自LeetCode的问题完全没有问题。让ChatGPT回答一个不那么直接的LeetCode风格的后续问题难度会增加很多。</blockquote><p></p><p></p><p></p><h3>自定义问题，通过率最低</h3><p></p><p></p><p>不出所料，“自定义”问题组的通过率最低，只有25%的面试者通过。它在统计上不仅明显小于其他两个实验组，而且明显低于对照组！当你问求职者完全自定义的问题时，他们的表现会比没有作弊（或被问到LeetCode风格的问题）时差！</p><p></p><p>需要说明的是，这个数值在最初计算时略高，在详细检查了自定义问题之后，我们发现了一个意料之外的问题。“企业应立即改变所提的问题！”一节说明了问题所在。</p><p></p><p></p><h2>没有人被抓到作弊</h2><p></p><p></p><p>在我们的实验中，面试官没有意识到面试者被要求作弊。上文说过，在每次面试后，我们会让面试官完成一项调查，他们必须描述自己对求职者的评估有多自信。</p><p></p><p>面试官对自己所做评估的正确性很有信心，72%的人说他们对自己的招聘决定有信心。一位面试官对面试者的表现非常之满意，以至于得出结论，应该邀请这些人成为平台的面试官！</p><p></p><p></p><blockquote>求职者表现非常出色，并且非常了解功能强大的Amazon L6 (Google L5) SWE……应该考虑让他们担任interviewing.io的面试官/导师。</blockquote><p></p><p></p><p>仅仅经过一次面试就做出这样的判断，这可能过于自信了！</p><p></p><p>我们早就知道，<a href="https://interviewing.io/blog/own-interview-performance">工程师不善于评估自己的表现</a>"，所以当我们发现面试官也高估了自己所提问题的有效性时，也许也不应该感到惊讶。</p><p></p><p>有部分面试官（28%）对自己的招聘选择没有信心，我们问了他们原因。下面是原因的频次分布。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecaee2868034c4b42b24ce277fc47b9d.png" /></p><p>﻿请注意：没有哪里提到作弊！</p><p></p><p></p><p>大多数面试官都具体说明了他们对招聘决定缺少信心的原因。问题通常包括解决方案次优、遗漏边缘情况、代码混乱或沟通糟糕。我们特意加入了一个“其他问题”类别，看看他们是否会表达对面试者作弊的担忧，虽然我们深入挖掘，但只发现了一些轻微的问题，比如“性格问题”和“他们需要加快编码速度”。</p><p></p><p>除了这个点出作弊的机会外，我们另外还有3次提示面试官指出其他他们担忧的问题，包括自由格式的文本框和几个选择题，其中的选项可以解释他们的担忧。</p><p></p><p>当面试者因为不理解ChatGPT提供的回答而面试失败时，面试官会把面试者的奇怪行为和生硬回答归结为缺乏练习——而不是作弊。有一位面试官认为求职者解决问题的能力不错，但又评论说他们速度很慢，需要更仔细地考虑边缘情况。</p><p></p><p></p><blockquote>“求职者似乎没有准备好回答任何LeetCode问题。”“求职者的方法不够清晰，而且他们急于开始编码。““这位求职者甚至没有准备好解决LeetCode上最基本的编程问题。”“总的来说，解决问题的能力不错，但应聘者需要在编码和识别关键边缘情况方面加快速度。“</blockquote><p></p><p></p><p>那么，谁记录了对作弊的担忧？又有谁作弊被抓了呢？</p><p></p><p>事实是，没有一位面试官提到对求职者作弊的担忧。</p><p></p><p>我们惊讶地发现，面试官并没有怀疑他们作弊。有趣的是，面试者也很自信自己没有作弊。81%的人表示不担心被发现，13%的人认为面试官可能已经发现他们作弊，而令人惊讶的是，仅有6%的参与者认为面试官会怀疑他们作弊。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ae61fa01ebe89cfc8f5c7411b5d6f1.png" /></p><p>﻿大部分面试者都确信他们作弊没有被发现。</p><p></p><p></p><p>有面试者担心被发现，面试官也确实在事后分析中给出了异常评价，但没有怀疑他们作弊。总而言之，大多数面试者认为他们作弊不会被发现——他们是对的！</p><p></p><p></p><h2>企业应立即改变所提的问题</h2><p></p><p></p><p>从这些结果中可以得出一个明显的结论，公司需要立即开始问自定义问题，否则他们将面临求职者在面试中作弊的严重风险（最终无法从面试中获得有用的信号）！</p><p></p><p>ChatGPT已经淘汰了原题；依赖这些问题的人，他们的招聘过程只能听天由命了。<a href="https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring">招聘已经够棘手的了</a>"，哪有心思再担心作弊。如果你所在的公司原封不动地使用LeetCode的问题，那么请在内部分享下这篇文章！</p><p></p><p>使用自定义问题不仅是防止作弊的好方法，还可以过滤掉那些记住了一堆LeetCode解决方案的求职者（如你所见，自定义问题组的通过率明显低于对照组）。它还能有效地改善求职者的体验，让人们更愿意为你工作。不久前，我们做了一个分析，<a href="https://interviewing.io/blog/best-technical-interviews-common">是什么造就了优秀的面试官</a>"。毫不奇怪，提出好问题是他们的一大特点，而我们评价最高的面试官往往是那些更乐于提出自定义问题的面试官！在我们的研究中，问题质量非常重要，它关系到求职者是否想在公司继续发展。这比公司的品牌实力还要重要许多。品牌实力在吸引求职者进入公司时是一个很重要的因素，但在面试过程中，相对于问题的质量来说就不那么重要了。</p><p></p><p>下面是来自求职者的一些说法：</p><p></p><p></p><blockquote>“要是不仅仅是简单的算法问题会更好。”“我喜欢这个问题——它采用了一个相对简单的算法问题（构建并遍历树），并增加了一些深度。我还喜欢面试官将问题与[Redacted]的实际产品联系起来，这让它看起来不像是一个玩具问题，而更像是一个实际问题的精简版。”“这是我在这个网站上遇到的最喜欢的问题。这是仅有的几个似乎适用于现实生活的方法之一，它来自于一个真实的（或潜在的）业务挑战。它还很好地融合了复杂性、效率和阻塞等挑战。”</blockquote><p></p><p></p><p>对于那些决定采用更个性化问题的公司来说，还有一个略显微妙的建议。你可能会把LeetCode的原题拿过来，然后做些修改。这很容易理解，因为这比从头开始提出问题要容易得多。遗憾的的是，这不起作用。</p><p></p><p>如前所述，我们在实验中发现，一个问题看起来像一个自定义问题并不意味着它就是一个自定义问题。问题可以看上去是自定义的，但实际上仍然与已有的LeetCode问题相同。在向求职者提问时，仅仅模糊一个已经存在的问题是不够的。你需要确保问题的输入和输出都是唯一的，这样才能有效地防止ChatGPT识别它！</p><p></p><p>面试官问的问题是保密的，我们不能分享面试官在实验中使用的具体问题。不过，我们可以给你举个例子。下面是一个有这类严重缺陷的“自定义问题”，ChatGPT很容易就能解答：</p><p><code lang="null">For her birthday, Mia received a mysterious box containing numbered cards

and a note saying, "Combine two cards that add up to 18 to unlock your gift!"

Help Mia find the right pair of cards to reveal her surprise.

Input: An array of integers (the numbers on the cards), and the target sum (18).

arr = [1, 3, 5, 10, 8], target = 18

Output: The indices of the two cards that add up to the target sum.

In this case, [3, 4] because index 3 and 4 add to 18 (10+8).</code></p><p></p><p>你发现问题了吗？虽然这个问题乍一看似乎是“自定义的”，但它的目标与流行的<a href="https://leetcode.com/problems/two-sum/">TwoSum</a>"问题相同：找到两个数字，它们的和等于给定的目标值。输入和输出都一样；这个问题唯一的“自定义”之处就是给问题加上了故事。</p><p></p><p>既然与已知问题相同，那么对于输入和输出都与现有已知问题相同的问题，ChatGPT表现良好也就不足为奇了——即使是为它们添加了一个独特的故事。</p><p></p><p></p><h3>如何创建好的自定义问题</h3><p></p><p></p><p>我们发现，对于提出好的原创问题，有一件事非常有用，就是在团队中创建一个共享文档，每当有人解决了他们认为有趣的问题时，无论问题多小，都快速记下，后续也无需补充完善这些笔记，但它们可以成为独特面试问题的种子，让求职者深入了解你公司的日常工作。把这些杂乱的种子变成面试问题需要思考和努力——你必须删去很多细节，提炼出问题的本质，使求职者不需要花很多时间去理解。你可能还得反复琢磨这些问题几次，才能把它们弄好——但回报也可能是巨大的。</p><p></p><p>需要说明的是，我们并不提倡从技术面试中删除数据结构和算法。DS&amp;A问题之所以名声不佳，是因为那些糟糕的、不敬业的面试官，也因为公司偷懒，重复使用LeetCode的问题，其中许多问题很糟糕，与他们的工作毫无关系。在好的面试官手中，这些问题会强而有力。如果你用上面的方法，就能够提出新的数据结构和算法问题，一些有实践基础并能吸引求职者、让他们对你所做的工作感到兴奋的问题。</p><p></p><p>这样，你也将推动我们的行业向前发展。背诵一堆LeetCode问题就能让求职者获得面试优势，这不好，也不能让作弊看起来像是面试的理性选择。解决的办法是雇主多做一些工作，提出更好的问题。让我们一起行动起来吧。</p><p></p><p></p><h2>给求职者的真心话</h2><p></p><p></p><p>好了，现在，所有正在积极找工作的人，请听好！是的，你的一部分同事现在会在面试中使用ChatGPT作弊，在那些使用LeetCode问题的公司（可悲的是，很多），这些同事会在短时间内取得优势。</p><p></p><p>现在，我们正处于一个临界状态，公司的流程还没有赶上现实的发展。他们很快就会完全放弃使用LeetCode原题（这对我们整个行业来说都是一个福音），或者回到现场（这将使作弊者在很大程度上不可能通过技术面试），或者两者兼而有之。</p><p></p><p>在<a href="https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews">本已艰难的环境</a>"下，我们会担心其他求职者作弊，这很糟糕，但凭良心，我们不能通过作弊来实现“公平竞争”。</p><p></p><p>此外，使用ChatGPT的面试者一致表示，在面试过程中使用AI使得整个面试过程困难了许多。</p><p></p><p>从下面这段<a href="https://www.youtube.com/watch?v=jtcCK0yr9Bg">视频</a>"中可以看到，一位面试者完美地回答了面试问题，但在分析时间复杂性时却磕磕绊绊。当面试者着急着慌地解释如何得出了错误的时间复杂度（ChatGPT提供的答案）时，面试官都被整糊涂了。</p><p></p><p></p><p></p><p></p><p>在实验过程中没有人被抓到作弊，他们的摄像头是关闭的。但正如我们在视频中看到的那样，即使是对于熟练的求职者来说，作弊也仍然很困难。</p><p></p><p>撇开道德不谈，作弊很难，会造成压力，而且实施起来并不简单。相反，我们建议将这些努力投入到实践中，一旦公司改变了他们的面试流程（希望这很快就会发生），你就可以因此获益。最后，我们希望ChatGPT的出现将成为催化剂，推动行业的面试标准从苦练和记忆转变为真正地考察工程能力。</p><p></p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p></p><p>原文链接：</p><p></p><p><a href="https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews">https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VOq4vxule6c9l73QYtrl</id>
            <title>离开百度7年后，吴恩达终于大厂“再就业”：加入亚马逊董事会，帮其实现AI大志</title>
            <link>https://www.infoq.cn/article/VOq4vxule6c9l73QYtrl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VOq4vxule6c9l73QYtrl</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 06:15:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊, 吴恩达, 人工智能, 董事会
<br>
<br>
总结: 亚马逊任命吴恩达为董事会成员，强调人工智能在当今时代的重要性，吴恩达在人工智能领域的经历和贡献备受认可。亚马逊CEO表示公司将在人工智能领域取得重要进展，强调亚马逊云科技在全球数字企业中的地位和影响。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;</p><p>路透社消息，当地4月12日，亚马逊发布公告称，计算机科学家吴恩达 (Andrew Ng) 成为亚马逊董事会成员，这项任命于 4 月 9 日生效。</p><p>&nbsp;</p><p>“AI，尤其是生成式 AI，是我们这个时代最具变革性的创新之一。”亚马逊表示，“我们寻求公司各个层面，包括董事会，拥有适当经验和观点的人。”而吴恩达“将有助于让董事会了解人工智能带来的机遇和挑战，及其变革性的社会和商业潜力。”</p><p>&nbsp;</p><p>此外，亚马逊还表示，自 2014 年以来一直担任亚马逊董事会成员的&nbsp;Judith McGrath&nbsp;今年决定不再连任董事会成员。</p><p>&nbsp;</p><p>更新后的亚马逊 12 名董事会成员名单：</p><p>&nbsp;</p><p>Jeff Bezos，亚马逊创始人兼执行主席。Andy Jassy，亚马逊首席执行官兼总裁。Keith Alexander，IronNet 前首席执行官、总裁兼主席，曾任美国网络司令部司令和国家安全局（NSA）局长。Edith Cooper，Medley Living联合创始人、高盛集团前执行副总裁。Jamie Gorelick，威凯平和而德（Wilmer Cutler Pickering Hale and Dorr LLP）律师事务所合伙人。Daniel Huttenlocher，麻省理工学院施瓦茨曼计算机学院院长。吴恩达，AI Fund&nbsp;执行普通合伙人、DeepLearning.AI 和 Landing AI创始人。Indra Nooyi，百事公司前主席兼首席执行官。Jonathan Rubinstein，全球最大对冲基金桥水基金（Bridgewater Associates）前联合首席执行官，苹果、Palm 和惠普前高管。Brad Smith，马歇尔大学校长，Intuit 公司前执行主席、总裁兼首席执行官。Patricia Stonesifer，华盛顿非营利机构 Martha’s Table 前总裁兼首席执行官，比尔和梅林达•盖茨基金会前首席执行官。Wendell Weeks，材料科学创新者和制造商康宁公司主席兼首席执行官。</p><p>&nbsp;</p><p></p><h2>吴恩达的“AI 观”</h2><p></p><p>&nbsp;</p><p>作为斯坦福教授的吴恩达不必多介绍，他被提到最多的职业经历就是谷歌和百度两段经历。</p><p>&nbsp;</p><p>2011年，吴恩达在谷歌创建了当时称为“Google X 实验室”旗下的Google Brain项目，以通过分布式集群计算机开发超大规模的人工神经网络，进而改进谷歌产品和服务的性能。</p><p>&nbsp;</p><p>Google Brain 很快展现出了惊人的效益和成功，Google X 前负责人埃里克·泰勒曾透露，Google Brain 当时赚到的钱超过了整个 Google X 部门的成本。于是 2011 年，Google Brain 独立成为 Google 的人工智能项目。</p><p>&nbsp;</p><p>这段经历对他来说应该也是意义非凡。前段时间谷歌Gemini遭到大量差评时候，他还在推特上鼓励道：“我只想说我爱你们所有人，支持你们。我知道每个人都是好意，感谢你们的工作，期待看到你们把这项惊人的技术发展到更高程度！”</p><p>&nbsp;</p><p>2014年5月16日，吴恩达加入百度，负责“百度大脑”计划，并担任百度公司首席科学家。2017年3月，吴恩达宣布从百度离职。在百度的三年里，吴恩达一度成为李彦宏之外的另外一个百度代言人。借助他的影响力，百度中美人工智能团队增长到了1300人，AI也逐渐应用到各个业务层面，确立了探索无人驾驶、自然语言处理和语音交互等底层技术的大方向。</p><p>&nbsp;</p><p>离开百度后，吴恩达创建了自己的AI公司：DeepLearning.AI。目前，吴恩达还是 AI Fund 风险投资基金的执行普通合伙人、计算机视觉初创公司 Landing AI 的创始人兼首席执行官、在线教育公司&nbsp;Coursera的联合创始人。</p><p>&nbsp;</p><p>针对人工智能的发展，吴恩达曾在推特表示，“我认为AI Agents 工作流程将在今年推动人工智能的巨大进步——甚至可能超过下一代基础模型。这是一个重要的趋势，我呼吁所有从事人工智能工作的人都关注它。”</p><p>&nbsp;</p><p>在近日红杉资本（Sequoia）在美国举行的AI Ascent活动上，吴恩达提到，AI Agents 的工作方式跟人类更相像。根据吴恩达分享的数据，使用 GPT-3.5 进行零样本提示的正确率是48%，GPT-4 的表现要好得多，正确率是 67%。但是如果在 GPT-3.5 的基础上建立一个 AI Agent的工作流，它甚至能比 GPT-4 做得更好。</p><p>&nbsp;</p><p>吴恩达认为，Agents工作流的出现，语言模型的能力有望在今年得到显著提升。随之而来的是，Token生成速度变得至关重要，甚至比大模型能力提升更重要，甚至还要让模型花更多时间推理和迭代。</p><p>&nbsp;</p><p>对于当前的大模型竞争，吴恩达认为短期不会立即结束：“现在有很多资源非常丰富的公司‘承受不起损失’，它们花费数十亿美元来竞争建立更好的大模型。我预计这场比赛将持续数年。这对于创新来说非常棒，对于每个在大模型之上构建应用程序的人来说也是如此。”</p><p>&nbsp;</p><p>这也有些像吴恩达对 AGI 的态度：它是慢慢到来的，而不是一夜之间能到来的。</p><p>&nbsp;</p><p></p><h2>“落后”的Amazon回归底层</h2><p></p><p>&nbsp;</p><p>在宣布吴恩达成为董事会成员之际，Amazon CEO Andy Jassy 也发布了股东信。</p><p>&nbsp;</p><p>尽管未能打造出与ChatGPT正面抗衡的消费级生成式AI产品、消费者和整个市场普遍认为亚马逊在AI领域已然落后，Jassy仍信心满满地表示，该公司将成为下一轮技术竞赛的主要参与者。</p><p>&nbsp;</p><p>Jassy 乐观地认为这波改变世界的AI浪潮“将主要建立在亚马逊云科技之上。”作为该公司的云计算业务，亚马逊云科技正在为全球众多数字企业提供运行基础。</p><p>&nbsp;</p><p>Jassy在股东信中间接批评了竞争对手的人工智能模型，称亚马逊提供了来自不同公司的模型，例如 Anthropic、Stability AI、Meta、Cohere 以及自己的模型，而不是仅仅依赖一种占主导地位的人工智能模型。“客户不只想要一种型号。他们希望获得适合不同类型应用的各种模型和模型尺寸，”他补充道。</p><p>&nbsp;</p><p>Jassy阐述了该公司在生成式AI领域的战略，坦言亚马逊云科技将不再专注构建面向消费者以直接同OpenAI&nbsp;ChatGPT等流行工具展开竞争的应用程序，转而专注构建底层“基础”AI模型并将相关成果出售给企业客户。Jassy提到，目前达美航空、西门子和辉瑞都已成为亚马逊大模型的买家。</p><p>&nbsp;</p><p>随着一年半之前ChatGPT的横空出世，各大科技巨头与一波初创企业间迅速掀起军备竞赛潮，人们争相构建最强大的AI技术并希望从中找到盈利空间。谷歌、OpenAI及Anthropic AI等多家公司先后投入数十亿美元，推出功能愈发强大的AI机器人。与此同时，企业们也在积极寻求将技术成果整合至现有产品当中的正确方法。但尴尬的是，目前大多数消费者还不打算为市面上的AI工具支付费用。</p><p>&nbsp;</p><p>亚马逊同样在生成式AI领域砸下数十亿美元。最近，他们又向初创公司Anthropic追加投资27.5亿美元，凭借高达40亿美元的总投资获得少数股权。作为交易的一部分，Anthropic将在亚马逊云科技的服务上运行，同时允许亚马逊向自家企业客户开放Anthropic Cluade——目前业界领先的生成式AI模型之一。</p><p>&nbsp;</p><p>亚马逊同时投入了数十亿美元，着力建设AI技术开发所必需的数据中心设施。</p><p>&nbsp;</p><p>尽管亚马逊明显是希望通过未来发展在AI领域赢得主导权，但直到现在，他们一直没能打造出广泛引发客户共鸣的消费级产品。今年早些时候，该公司曾经推出购物助手Rufus，但并未显著改善基于搜索的原有购物体验。该公司也叫停了去年9月宣布推出的生成式AI版Alexa——尽管在宣传阶段强调以“更智能、更具对话体验”为卖点，但实际成果始终没能与客户见面。</p><p>&nbsp;</p><p>Jassy在股东信里提到，在生成式 AI 方面，向 Amazon SageMaker 添加了数十种功能，以便开发人员更轻松地构建新的基础模型（“FM”）；发明并提供了一项新服务 (Amazon Bedrock)，让公司可以利用现有的 FM 来构建 GenAI 应用程序；在Amazon Q 中推出了功能最强大的编码助手。“客户对这些功能感到兴奋，并且我们看到我们的 GenAI 产品具有巨大的吸引力。”</p><p>&nbsp;</p><p>尽管今年以来，亚马逊公司的股价已经上涨25%，但其仍未从疫情带来的超支负担中完全恢复过来。在2022年至2024年期间累计裁员超2.7万人之后，该公司上周公布了新的裁员计划，着手砍掉亚马逊云科技中数百个职位。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.aboutamazon.com/news/company-news/dr-andrew-ng-joins-amazon-board-of-directors">https://www.aboutamazon.com/news/company-news/dr-andrew-ng-joins-amazon-board-of-directors</a>"</p><p><a href="https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2023-letter-to-shareholders">https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2023-letter-to-shareholders</a>"</p><p><a href="https://www.washingtonpost.com/technology/2024/04/11/amazon-ai/">https://www.washingtonpost.com/technology/2024/04/11/amazon-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7g8PXNNtisFg5OptZJcw</id>
            <title>李彦宏内部讲话曝光：闭源模型才能“遥遥领先”！</title>
            <link>https://www.infoq.cn/article/7g8PXNNtisFg5OptZJcw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7g8PXNNtisFg5OptZJcw</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 03:51:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度, 大模型, 开源, AI创业者
<br>
<br>
总结: 百度创始人李彦宏表示，百度不开源的原因是市场已有足够多的开源大模型，闭源模型在能力上会持续领先。AI创业者应该专注于某一领域的知识和数据，而不是分散精力于模型研发和应用开发。 </div>
                        <hr>
                    
                    <p>4&nbsp;月&nbsp;11&nbsp;日晚间消息，百度创始人、董事长兼&nbsp;CEO&nbsp;李彦宏近日的一次内部讲话曝光。讲话中，李彦宏针对当前业界热议的“大模型应该开源还是闭源？”“AI&nbsp;创业者应当专注于模型研发还是应用开发？”等问题，表达了自己的见解。</p><p></p><p></p><h4>为什么百度不开源？</h4><p></p><p></p><p>百度的文心大模型在一年前刚刚发布的时候，公司内部对其是否开源有过激烈讨论，最终的决定是不开源，因为判断市场上会有足够多的开源大模型。事实证明，今天主流的开源大模型里，国外的&nbsp;Llama、Mistral，国内的智源、百川、阿里的通义等都具有相当影响力，李彦宏说，“在这种情况下，多百度一家开源不多，少百度一家开源也不少。”如果开源，还需要再去维护一套开源的版本，对于百度来说是不划算的。</p><p></p><p>更重要的是，闭源模型在能力上会持续地领先，而不是一时地领先。</p><p></p><p>李彦宏称与传统的Linux、安卓等软件开源不同，“模型开源不是一个众人拾柴火焰高的情况。”因为开源模型都是在外头零零散散、小规模地去做各种各样的验证应用，没有经过大算力的验证。</p><p></p><p>而且，闭源有着真正的商业模式，能够赚到钱，从而能够聚集算力和人才。对于成本来说，虽然开源是免费的，但李彦宏认为闭源在成本和效率上反而更有优势，“只要是同等能力，闭源模型的推理成本一定是更低的，响应速度一定是更快的。反过来，同等参数的情况下，闭源模型的能力也是更强的。”</p><p></p><p></p><h4>研发大模型还是应用？</h4><p></p><p></p><p>李彦宏认为既做模型又做应用，势必会分散精力。对于资源和精力都有限的创业公司来说，应该专注于一项任务，“力出一孔”，而不是去搞所谓的“双轮驱动”。</p><p></p><p>对于&nbsp;AI&nbsp;创业者来说，核心竞争力本就不应该是模型本身，这样太耗费资源，且需要非常长时间的坚持才能跑出来。AI&nbsp;创业者的优势应该是在某一个领域的知识和数据，去靠领域知识提供特定价值。</p><p></p><p>最后，李彦宏提到没有必要担心基础模型通吃&nbsp;AI&nbsp;的应用，“拼多多、滴滴不怕微信抢饭碗，它们的兴起都是依赖微信这个移动生态中的封闭平台，但它们各自提供了独特价值，有不同的竞争力。”李彦宏说。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MExA5tbGSy2DPAfOUIrj</id>
            <title>网易有道自研RAG引擎QAnything升级：发布纯python版本，首次支持在Mac运行</title>
            <link>https://www.infoq.cn/article/MExA5tbGSy2DPAfOUIrj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MExA5tbGSy2DPAfOUIrj</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 03:19:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 有道知识库问答引擎, QAnything, BM25 + embedding混合检索, BCEmbedding模型
<br>
<br>
总结: 有道知识库问答引擎QAnything更新至1.3.0版本，带来了纯python版本和BM25 + embedding混合检索功能，为开发者提供更强大的技术支持和用户体验。引擎支持多种文档格式上传和互动问答功能，准确率高，下载次数众多。采用自研BCEmbedding模型，检索准确率高达95%，覆盖多领域，为商业化落地提供便捷。已在多场景落地，提供个性化服务和快速文档理解，为企业带来生产效率提升。 </div>
                        <hr>
                    
                    <p>4月8日，有道知识库问答引擎QAnything更新至1.3.0版本，该版本带来了两大主要功能升级：发布纯python的轻量级的版本，该版本支持在Mac上运行，也可以在纯CPU机器上运行；同时支持BM25 + embedding混合检索，可以实现更精准的语义检索和关键字搜索。本次更新后，QAnything能为开发者探索大模型落地提供更强大的技术支撑和更流畅的用户体验。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2d10e1de210d873c2ee0608c65bf3cf9.png" /></p><p></p><p>QAnything是网易有道自研的RAG（Retrieval Augmented Generation)&nbsp;引擎。该引擎允许用户上传PDF、图片、Word、Excel、PowerPoint等多种格式的文档，并实现类似于ChatGPT的互动问答功能，其中每个答案都能精确追溯到相应的文档段落来源。该引擎支持纯本地部署，上传文档数量无上限，问答准确率很高。</p><p></p><p>GitHub地址：</p><p>https://github.com/netease-youdao/QAnything</p><p></p><p>自今年1月开源以来，QAnything迅速吸引了开发者社区的广泛关注，并多次登上了GitHub trending榜单。截至目前，在GitHub上QAnything已经积累7000+个星标，这反映出了用户对其价值的高度评价。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/8718eb7895f9e6d25a5418f9adf05df2.png" /></p><p></p><p>此外，QAnything下载次数已达数万次。其中，语义嵌入排序模型BCEmbedding更是每月可达超60万次下载。</p><p><img src="https://static001.geekbang.org/infoq/bc/bcc170b003d4daec142b1c4fd4625e55.png" /></p><p></p><p>值得一提的是，QAnything采用了自研的BCEmbedding模型（RAG系统关键模块）。有道发现，在客服问答以及一些toB客户的场景中，OpenAI的Ada2 BCEmbedding检索准确率只有60%，而其自研的 BCEmbedding检索准确率可以达到95%。该模型具有中英双语跨语种能力和多领域覆盖两大特色。</p><p></p><p>据悉，QAnything收集了包括教育、医疗、法律、金融、百科、科研论文、客服、通用QA等场景的语料，使得模型可以覆盖和支持尽可能多的应用场景，为商业化落地提供了便捷。</p><p></p><p>目前，QAnything已在有道多场景中落地。如“有道领世”在QAnything的帮助下，凭借海量的升学资料数据，打造出一个“私人AI规划师”，能为每个家长和学生提供个性化的服务，展示更加全面、专业、及时的升学规划。面对高考政策、升学路径、学习生活以及职业规划等各类问题，该系统的解答准确率超过95%。未来随着数据补充和更新，准确率会一直上涨。</p><p></p><p>与此同时，子曰教育大模型最新应用成果“有道速读”，其核心功能文档问答、文章摘要、要点解读、引文口碑和领域综述，背后驱动也是QAnything。在其加持下，用户快速理解文档、定位要点等诉求得以快速实现，短短一分钟，万字长文就能拆解得明明白白。除赋能自身业务外，开源后的QAnything不断拓宽“朋友圈”。目前已累计为近百家企业赋能，以期让AI应用真正进入医疗、物流、办公等多元化场景，为企业、组织和个人带来生产效率的大幅提升。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AlEwm57dwf6C83B5O5Py</id>
            <title>谷歌、OpenAI、Mistral 在 24 小时内打响科技界“三强争霸赛”</title>
            <link>https://www.infoq.cn/article/AlEwm57dwf6C83B5O5Py</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AlEwm57dwf6C83B5O5Py</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Apr 2024 01:14:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Gemini API, GPT-4 Turbo, Mixtral 8x22B, AI模型
<br>
<br>
总结: 谷歌、OpenAI和Mistral相继发布了最新的AI模型，Gemini API提供了功能最强大的生成式AI模型Gemini 1.5 Pro的公开预览版，OpenAI发布了GPT-4 Turbo，集成了视觉理解能力，Mistral则开源了Mixtral 8x22B模型。这三家公司的新模型引发了科技界的关注和讨论，展开了一场“三强争霸赛”。Gemini 1.5 Pro扩展了功能，GPT-4 Turbo新增了视觉理解能力，Mixtral 8x22B在性能上表现出色，各有各的特点和优势。 </div>
                        <hr>
                    
                    <p>太平洋时间本周二&nbsp;11:01，谷歌在官网中宣布在&nbsp;180&nbsp;多个国家/地区通过&nbsp;Gemini&nbsp;API&nbsp;提供&nbsp;Gemini&nbsp;1.5&nbsp;Pro&nbsp;的公开预览版，这是它目前功能最强大的生成式&nbsp;AI&nbsp;模型。谷歌本以为能在互联网上掀起一番声势浩大的讨论，不料短短&nbsp;40&nbsp;分钟后，OpenAI&nbsp;就出来抢风头了：它发布了非预览版的&nbsp;GPT-4&nbsp;Turbo，将之前独立的&nbsp;GPT-4&nbsp;Vision&nbsp;直接集成到模型中。这还没完，下午6:20，Mistral&nbsp;在&nbsp;X&nbsp;上直接了当地甩出一条磁链，强势开源&nbsp;Mixtral&nbsp;8x22B&nbsp;这个超大模型。</p><p></p><p>谷歌刚拔剑出鞘，OpenAI&nbsp;和&nbsp;Mistral&nbsp;立马摩拳擦掌加入战斗，科技界“三强争霸赛”一触即发。不过，到底是虚张声势还是确实“有点东西”，让我们一探究竟。</p><p></p><p></p><h3>Gemini&nbsp;1.5&nbsp;Pro&nbsp;：“听”懂掌声</h3><p></p><p></p><p>Gemini&nbsp;1.5&nbsp;Pro&nbsp;目前已在谷歌面向企业的&nbsp;AI&nbsp;开发平台&nbsp;Vertex&nbsp;AI&nbsp;上提供公共预览版。它能处理的上下文从&nbsp;12.8&nbsp;万个&nbsp;token&nbsp;增加到&nbsp;100&nbsp;万个&nbsp;token，相当于大约&nbsp;70&nbsp;万个单词，或者大约&nbsp;3&nbsp;万行代码。这大致是&nbsp;Anthropic&nbsp;旗下模型&nbsp;Claude&nbsp;3&nbsp;最大上下文量的四倍，OpenAI&nbsp;旗下模型&nbsp;GPT-4&nbsp;Turbo&nbsp;最大上下文量的八倍。</p><p></p><p>Gemini&nbsp;1.5&nbsp;Pro&nbsp;版本扩展了输入模态，首次提供了本地音频（语音）理解功能和全新的文件&nbsp;API，使文件处理变得更加简单。此外，Gemini&nbsp;1.5&nbsp;Pro&nbsp;现在能够对上传到谷歌&nbsp;AI&nbsp;Studio&nbsp;中的视频进行图像（帧）和音频（语音）推理，谷歌也期待尽快为此添加&nbsp;API&nbsp;支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/45/66/4510a7f254f882138f669e1750e4c766.gif" /></p><p>您可以上传讲座的录音，Gemini&nbsp;1.5&nbsp;Pro&nbsp;可以将其变成小测验，并附有答案。</p><p></p><p>不过，Gemini&nbsp;1.5&nbsp;Pro&nbsp;对于没有访问&nbsp;Vertex&nbsp;AI&nbsp;和&nbsp;AI&nbsp;Studio&nbsp;权限的人来说是不可用的。目前，大多数人只能通过&nbsp;Gemini&nbsp;聊天机器人来接触&nbsp;Gemini&nbsp;语言模型。虽然它功能强大，也能理解长命令，但它的速度不如&nbsp;Gemini&nbsp;1.5&nbsp;Pro。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc6f3ae3ac13de785c0ab95006cf8a96.webp" /></p><p></p><p></p><h3>GPT-4&nbsp;Turbo：不如不“看”？</h3><p></p><p></p><p>OpenAI&nbsp;宣布&nbsp;GPT-4&nbsp;Turbo&nbsp;with&nbsp;Vision&nbsp;模型已经通过&nbsp;OpenAI&nbsp;API&nbsp;向开发人员开放。该模型延续了&nbsp;GPT-4&nbsp;Turbo&nbsp;系列&nbsp;128,000&nbsp;个&nbsp;token&nbsp;的窗口大小，以及截止至&nbsp;2023&nbsp;年&nbsp;12&nbsp;月的知识库，最大的革新之处在于其新增的视觉理解能力，可处理和分析多媒体输入信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efbea90e62627ab80cec973f76f62db5.png" /></p><p></p><p>OpenAI&nbsp;称这些变化有助于简化开发人员的工作流程并打造更高效的应用程序，因为“过去，开发者需要调用不同的模型来处理文本和图像信息，但现在，只需一次&nbsp;API&nbsp;调用，该模型就可以分析图像并应用推理。”</p><p></p><p>OpenAI&nbsp;还提到此次更新是“&nbsp;Majorly&nbsp;improved（重大改进）”，不过网友则对这个“小修小补”表示不感兴趣：“如果不是&nbsp;GPT-5&nbsp;的话，还是别发了。”</p><p></p><p>延伸阅读：OpenAI&nbsp;重磅发布的GPT-4&nbsp;Turbo&nbsp;with&nbsp;Vision，是编码的倒退</p><p></p><p></p><h3>Mixtral&nbsp;8x22B&nbsp;：强势开源</h3><p></p><p></p><p>今年&nbsp;1&nbsp;月，Mistral&nbsp;AI&nbsp;公布了&nbsp;Mixtral&nbsp;8x7B&nbsp;的技术细节，该模型以&nbsp;47B&nbsp;左右的参数总量，展现了不错的性能——在人类评估基准上明显超过了&nbsp;GPT-3.5&nbsp;Turbo、Claude-2.1、Gemini&nbsp;Pro&nbsp;和&nbsp;Llama&nbsp;2&nbsp;70B&nbsp;聊天模型。</p><p></p><p>短短&nbsp;3&nbsp;个月后，Mistral&nbsp;AI&nbsp;开源了&nbsp;Mistral&nbsp;8X22B&nbsp;模型，再一次为开源社区注入了新鲜血液。Mistral&nbsp;AI&nbsp;提供的磁链大小为&nbsp;281&nbsp;GB，下载后可以看到模型文件大小约为&nbsp;262&nbsp;GB，比之前的&nbsp;Mixtral&nbsp;8x7B&nbsp;大得多，鉴于&nbsp;Mixtral&nbsp;8x7B&nbsp;优秀的表现，网友们表示很看好&nbsp;Mistral&nbsp;8X22B，不过目前还没有看到有人运行它。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f05ce070a3a9df18b455a2942b2e04de.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5dc5f6f076f8b50bca56c28515d39750.png" /></p><p></p><p></p><h3>芯片大战</h3><p></p><p></p><p>除了软件的较量，另一边，硬件领域中的芯片也是八仙过海。</p><p></p><p>对于提升训练&nbsp;AI&nbsp;模型所需的算力来说，CPU&nbsp;至关重要。而众所周知，购买&nbsp;AI&nbsp;芯片的成本惊人，英伟达的&nbsp;Backwell&nbsp;芯片，预计售价在&nbsp;3&nbsp;万美元到&nbsp;4&nbsp;万美元之间。为了在&nbsp;AI&nbsp;军备竞赛中节省开支，微软和亚马逊均在自研处理器方面发力，谷歌自然不甘落后。本周二的&nbsp;Cloud&nbsp;Next&nbsp;2024&nbsp;大会上，谷歌还正式宣布，将自研首款基于&nbsp;Arm&nbsp;的&nbsp;CPU。据称这款&nbsp;CPU&nbsp;处理器&nbsp;Axion，将提供比英特尔&nbsp;CPU&nbsp;更好的性能和能源的效率，其中性能提高&nbsp;50%，能源效率提高&nbsp;60%，比起目前基于&nbsp;Arm&nbsp;的最快通用芯片，Axion的性能还要高出30%。</p><p></p><p>GPU&nbsp;方面，当地时间&nbsp;4&nbsp;月&nbsp;9&nbsp;日，英特尔举办了面向客户和合作伙伴的英特尔&nbsp;on&nbsp;产业创新大会。这场大会上，英特尔首次介绍了他们的&nbsp;GPU&nbsp;产品&nbsp;Gaudi&nbsp;3，对标英伟达早前的主力产品&nbsp;H100。据介绍，英特尔&nbsp;Gaudi&nbsp;3&nbsp;将带来&nbsp;4&nbsp;倍的&nbsp;BF16&nbsp;AI&nbsp;计算能力提升，采用&nbsp;128GB&nbsp;HBMe2&nbsp;内存，支持&nbsp;1.5&nbsp;倍的内存带宽提升，采用&nbsp;5nm&nbsp;制程制造。此外，这颗芯片能够支持多种的大模型，包括&nbsp;Llama、文生图的&nbsp;Stable&nbsp;Diffusion、语音识别的&nbsp;Whisper&nbsp;等等。</p><p></p><p>短短几天，科技圈的大事层出不穷，不得不祭出这张&nbsp;meme&nbsp;了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b25647cca89e6316f69c9e3f42783cf5.png" /></p><p></p><p>作为这场科技革命千千万万的见证者之一，我时刻期待着。</p><p></p><p>参考来源：</p><p></p><p><a href="https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html">https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html</a>"</p><p></p><p><a href="https://platform.openai.com/docs/models/continuous-model-upgrades">https://platform.openai.com/docs/models/continuous-model-upgrades</a>"</p><p></p><p><a href="https://twitter.com/OpenAI/status/1777772582680301665">https://twitter.com/OpenAI/status/1777772582680301665</a>"</p><p></p><p><a href="https://twitter.com/MistralAI/status/1777869263778291896">https://twitter.com/MistralAI/status/1777869263778291896</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/z3WeQz4ipl0dBeXKtrpx</id>
            <title>QCon 北京2024 盛大开幕，韦青、王皓、程操红、郭东白、章文嵩、蒋晓伟、李飞飞、张凯等行业领袖呈现精彩分享</title>
            <link>https://www.infoq.cn/article/z3WeQz4ipl0dBeXKtrpx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/z3WeQz4ipl0dBeXKtrpx</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 13:43:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div>         关键词: QCon, AICon, AIGC, 大模型
        <br>
        <br>
        总结: 今年由极客邦旗下 InfoQ 中国主办的 QCon 全球软件开发大会暨智能软件开发生态展在北京举行，会议内容涵盖生成式 AI，大模型等多个专题，邀请了100多位专家大咖分享企业级大模型落地经验。同时，还举办了AICon，发布了《中国生成式 AI 开发者洞察 2024》报告，展示了生成式AI开发者的工作特征和晋升路线。整体旨在推动AI产业生态建设，探索AI技术在各领域的创新应用。 </div>
                        <hr>
                    
                    <p>4 月 11 日，由极客邦旗下 InfoQ 中国主办的 QCon 全球软件开发大会暨智能软件开发生态展在北京国测国际会议会展中心正式召开。今年 QCon 在会议内容、会议模式上均向着“生成式 AI”全面进化。本届 QCon 一共设置了近 30 个专题，邀请到了来自阿里巴巴、腾讯、百度、微软、字节跳动、华为、京东、智谱、美的、国泰君安、深开鸿等领先企业的 100 多位专家大咖，跟大家分享最真实的企业级大模型落地经验，细数大模型落地痛点。</p><p></p><p>除此之外，本届大会还特别策划了智能软件开发生态展，围绕“智能软件”主题，广泛邀请了生态上下游企业来到 QCon 现场展示最新的技术和产品，为大家带来智能软件时代技术先行者们的案例以供参考。</p><p></p><p></p><h2>开幕精华：让 QCon 的开发者先看到未来</h2><p></p><p></p><p>本次大会于今日上午 9 点正式开幕，InfoQ 主编蔡芳芳为大会致开幕辞。她首先介绍了今年 QCon 大会的精彩看点，随后重点介绍了今年极客邦科技围绕 AIGC 和大模型正在展开哪些工作。2024 年，“AIGC IN ALL"的理念将成为极客邦科技业务升级的核心，极客邦科技正围绕“AI 应用加速、AI 人才培养、AI 产业生态、AI 趋势洞察”这四个核心方向，全面推进产品的创新改造，以迎接大模型时代的到来。</p><p></p><p>今年，极客邦科技重启了 AICon，围绕 AI Agent、RAG、LLM Ops、多模态技术、大模型训练与推理等多个方向，展开丰富而深入的讨论。目前已成功邀请到了来自 Google、阿里巴巴、科大讯飞、字节跳动、华为、智谱科技、月之暗面等领先企业的专家学者，为参会者分享前沿技术和行业应用经验。同时，AICon 将推出首届大模型应用生态展，让那些致力于 AI 和大模型行业落地应用探索，有实践、有创新、有成果的企业，能够有机会将应用案例和创新产品搬到 AICon 现场，让现场参会者有机会深入了解并体验。</p><p></p><p>除了 AICon，InfoQ 面向 AIGC 赛道正式启动【中国技术力量 2024 之 AIGC 先锋榜】案例征集。本次案例征集共分为两个维度，分别是【AIGC 最佳实践案例 TOP20】和【AIGC 最佳技术服务商 TOP30】。本次榜单评选分为自主报名（4.1-4.26）、专家评选（4.26-5.8）、榜单结果公布（5.17）三个环节，InfoQ 将邀请行业专家共同参与案例评选，最终产生上榜名单。我们将于 5 月 17 日召开的 【AICon 全球人工智能开发与应用大会暨大模型应用生态展】 大会现场公布结果，并邀请部分获奖企业来到现场展示并见证这一时刻。欢迎感兴趣的企业扫描下方二维码提报案例信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/94/9496dc57091321eec55c30b05bfa9529.webp" /></p><p></p><p>开场致辞中也对《数智时代的 AI 人才粮仓模型解读白皮书》做了简要介绍，白皮书会从政策 + 行业变革等时代背景、企业需求、AI 价值、AI 人才模型及人才培养五个方面对“数智时代的 AI 人才粮仓模型”进行深度解读，为企业提供一个清晰、可操作的 AI 人才布局指南，帮助企业快速构建起适应数字化时代需求的 AI 人才梯队，在激烈的市场竞争中占据先机。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3c/3c9eecbe12c7054c1c34e0a44f9c716c.jpeg" /></p><p></p><p>为了更好地推动 AI 产业生态建设，极客邦科技将发起一个全新的产业联盟——AIGC 应用创新产业联盟。该联盟旨在打造一个开放、共享、创新的平台，汇聚产业链上下游的企业、研究机构、高校以及创新团队，共同探索 AIGC 技术在各个领域的创新应用。联盟内成员单位将可以获得 InfoQ 商务、内容、大会等层面的优先合作权和特殊折扣，联盟内部将定期围绕成员单位感兴趣的话题举办闭门会。</p><p></p><p>随后，InfoQ 研究总监兼首席分析师姜昕蔚正式发布《中国生成式 AI 开发者洞察 2024》报告并对报告进行了详细解读。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e3/e3b4e003818d3a6993b1504e5ad6e46f.png" /></p><p></p><p>报告指出，作为新兴行业从业者，生成式 AI 开发者普遍相关工作年限较短、收入较高。InfoQ 调研统计，2023 年生成式 AI 开发者人均年收入为 36.7 万，相关工作经验在 3 年以上生成式 AI 开发者的年收入超越均值，近 4 成生成式 AI 开发者年收入处于 20-50 万区间，远超 2023 年上半年北京招聘平均薪资（18976 元 / 月）。其中，北京生成式 AI 开发者规模最大，但上海资深生成式 AI 开发者更多且人均薪资更高。</p><p></p><p>工作特征方面，应用工具（如智能编码工具）、大语言模型、数据科学 / 数据挖掘 / 数据分析、语言 / 语义理解类应用（如对话机器人）和图像识别类应用（如拍照搜图）是最主要的五个生成式 AI 开发者研发方向。GPT、文心、通义大模型是生成式 AI 开发者使用率最高的大模型。调查显示，2023 年生成式 AI 开发者人均使用 AI 工具时间为半年，最常使用智能化办公工具，其次是图像生成工具。使用代码生成工具和 ChatBot 的人群比例最高，其中使用 ChatBot 的时长略高于均值，而代码生成工具使用时长仅略高于可视化智能数据分析工具。</p><p></p><p>在晋升方面，生成式 AI 开发者中的初入者未来将有四条进阶路线：</p><p></p><p>AI 应用实践者：针对绝大部分具备初级或不具备开发技能的职场新手，未来希望精通 AI 技术场景化应用，实现业务价值升级；AI 技术赋能者——AI 实践领导者：在进阶使用 AI 的过程中，不断提升专业技术，向资深人员转型，并最成为行业引领人才；AI 技术领航者——AI 技术赋能者——AI 实践领导者：以夯实技术能力为主，逐渐全面应用 AI，最终成为行业引领人才；AI 技术领航者：希望成为专项技术精英，推动 AI 技术迭代升级。</p><p></p><p>关注 AI 前线公众号，回复关键词 【开发者洞察】，即可免费获取报告电子版文件。⬇️</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/548b5aaac2181aa895e111629a7795e9" /></p><p></p><h2>主题演讲：洞察前沿技术趋势</h2><p></p><p></p><h3>主题演讲：看不见的大猩猩——智能时代的企业生存和发展之路</h3><p></p><p></p><p>大会的首场演讲由微软（中国）首席技术官韦青分享，他提到企业往往会将目光聚焦在那些被媒体广为宣传的问题或潮流上，像“看不见的大猩猩″一样忽略了那些同样显而易见的关键问题；另外，企业内部还会有大量显而易见但是难以解决的问题，这些问题往往需要决策者拥有长期坚持的勇气和应对不确定性的定力，但人们通常会有意识地无视这些“房间里的大象”。这两种现象都有可能成为阻得企业长期发展的“卡点”。他认为，企业应该聚焦这些被忽略的明显而又困难的问题，常常也是由于受到固有思维的制约而找不到解决方案的问题，从而让企业成为新的发展范式中的佼佼者。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/67/6769b4b247d03836c6accedd0ee6ccd3.png" /></p><p></p><p>韦青表示，AGI 时代企业的竞争力主要表现在对大模型智能的应用上面。智能应用的核心是企业独有的数据，企业独有的数据又要基于企业业务流程的数字化改造。企业的当务之急是构建新一代 AI 智能应用来利用大模型的能力学习到企业的专有知识。他认为，没有数字化，也就不会有智能化。数据驱动的智能化过程就是智能化重构一切的过程。</p><p></p><h3>主题演讲：开鸿安全数字底座，构建开源鸿蒙新生态</h3><p></p><p></p><p>随后，深圳开鸿数字产业发展有限公司高级副总裁、研发体系总裁王皓博士发表了《开鸿安全数字底座，构建开源鸿蒙新生态》的主题演讲，围绕开鸿安全数字底座与 AI 融合、开鸿产业化与产业开鸿化、产业互联网等话题，探讨开源鸿蒙在万物智联联时代的发展新机遇。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed78eb8243ab3b01a00d336b52f9b07f.png" /></p><p></p><p>在如今以“人工智能”为核心驱动力的智能经济的阶段，高质量的数据和算力尤为稀缺。我们需要保证高质量的数据能够实时产生，并即时通过算力进行处理和应用，以满足不同变化的场景需求。深开鸿基于开源鸿蒙，围绕“KaihongOS 和 KaihongOS-Meta”打造开鸿安全数字底座，可提供高质量的数据，同时结合 AI 能够打造行业“挖矿机”。</p><p></p><p>王皓博士提到，随着万物智联的到来，安全数字底座与 AI 相互融合将推动我们进入物理世界与数字世界无缝衔接的时代。过去每一代操作系统多针对特定硬件设计，导致数据碎片化问题凸显。然而，开源鸿蒙是未来建设数字中国的数字底座，基于开源鸿蒙的开鸿安全数字底座面向万物智联时代全场景多设备，可适应 KB 级到 GB 级的存储需求，能统一设备，使数据得以自然流通，设备间实现无缝交互。</p><p></p><p>最后，王皓博士表示，做操作系统就是做生态，需要集聚产业的力量，通过推进“开鸿产业化”与“产业开鸿化”，构建开源鸿蒙新生态。</p><p></p><h3>主题演讲：钉钉智能化之路：打造未来交互新形态，重塑组织效能</h3><p></p><p></p><p>钉钉 CTO 程操红（巴布）以《钉钉智能化之路：打造未来交互新形态，重塑组织效能》为主题展开了分享。他分享了过去一年钉钉的智能化战略布局与实施。在 AI 技术与产品快速迭代的浪潮中，钉钉顺应时势，深度布局智能化战略。当前，钉钉在 AI 上已经发生产品、底座、生态三个方向的巨大变化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ce/ceb93537105022b0baf5094b74ef6545.png" /></p><p></p><p>巴布提到，AI 带来了产品新形态，从 GUI 到 LUI、多模态，产品交互形态的变化让软硬结合有了更多可能性：</p><p></p><p>AI Inside。AI 悄无声息的进入到企业系统应用中，让原来 GUI 的重交互模式变成 LUI 的轻交互模式。AI Copilot。AI 让原来冷冰冰的系统应用多了一个智能副驾驶复杂流程。AI Agent。AI 已经不再拘泥于传统软件形式，更像是数字员工一样进入组织, 一起协同。</p><p></p><p>AI 也带来了数据新消费。数据的归途不只是报表和大屏，而是作为核心生产要素发挥更大价值。第一，数据可以作为模型训练语料，海量的数据可以喂给模型做 Fine-tune，也可以作为 RAG 的知识库让 AI 理解和召回。第二，数据作为 AI 助理的记忆，多维度的特征数据也可以作为记忆，植入到 AI 助理的大脑让它知道自己的本事有多大。第三，数据作为 AI 助理的感知，上下文、过程信息、以及外界传递的数据都可以作为 AI 助理的感知，像人一样做实时理解和反馈。</p><p></p><p>最后，巴布提到 AI 带来了协同新方式。协同网络变得更加多元化，一方面，AI 助理作为全新角色，融入到连接组织内外的协同网络，成为助手 / 数字员工。另一方面，人、系统、硬件、AI 助理之间会形成网状协同, 消除信息孤岛，加速数据流通和消费。</p><p></p><h3>主题演讲：大模型时代的架构思维</h3><p></p><p></p><p>Coupang 副总裁郭东白分享了《大模型时代的架构思维》，他从软件架构的六个基本要素分析了大模型对软件研发活动的冲击，他指出，大模型的盲区就是架构师创造价值的所在，大模型对软件架构师的冲击很小，甚至会可能带来更大的市场需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/34/34efbaed3ecd83e8701cb49ecd4aa17e.png" /></p><p></p><p>郭东白还重新解读了大模型时代下的架构师生存法则：</p><p></p><p>有唯一且正确的目标。大模型时代不能不定义出一个单一的、可量化的、且能够持续观测的、并且可以持续优化的成功目标。在有限资源下最大化经济价值。对我们所在的企业而言， 当下大模型不一定是最优解，但是大模型会以何种方式影响整个行业的成本和商业模式却必须慎重考虑。软件架构必须顺应技术趋势。需要思考“上个时代”的架构师，如何在大模型时代最大化借力？从原子价值单元开始对大模型做投入。大模型项目的原子价值单元指的是仅由大模型带来的能力，以原子价值单元投入需要锁定大模型的最适场景，仅对高回报场景做模型迭代，并最小化合规成本。</p><p></p><p>最后，郭东白总结道，在大模型时代，架构师的价值正发生变化。软件架构师在大模型时代可以弥补大模型的盲区，架构师需要确保架构活动有唯一的、可量化成功目标，要从市场竞争的角度思考大模型的经济价值，并要用最小价值单元开始探索大模型的应用。</p><p></p><p></p><h2>圆桌对话：大模型时代的数据智能新趋势</h2><p></p><p></p><p>在主论坛压轴的圆桌对话环节，AutoMQ 联合创始人 &amp; 首席战略官章文嵩、ProtonBase 研究员蒋晓伟、阿里云数据库产品事业部负责人李飞飞、蚂蚁集团 AI 安全商业化总经理张凯围绕“大模型时代的数据智能新趋势”主题展开了巅峰对谈。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b8/b85035ebe1921e6fb2206f77d909fa7f.png" /></p><p></p><p>围绕“大模型时代的数据平台趋势”话题，蒋晓伟提到了一个词，云原生分布式 Data Warebase。他认为，分布式 Data Warebase 是性能、正确性、实时性这三个业务核心需求的必然推论，它不是一个发明，而是一个发现。李飞飞表示，算力驱动与数据驱助力智能化时代加速进化，云原生与智能化推动结构化、半结构化、非结构化数据走向一体化、一站式处理。</p><p></p><p>在“大模型时代的数据 &amp;AI 基础设施”的讨论中，章文嵩提到数据是 AI 大模型的原材料，充分利用云原生数据基础设施和 AI 基础设施服务，高效构建垂直领域的数据集和 AI 应用。此外，大模型在带来前所未有的技术能力变革的同时，也带来了一系列安全问题，比如数据安全。张凯表示，AI 需要安全，安全需要 AI。</p><p></p><h2>现场回顾：创新浪潮中的思维碰撞</h2><p></p><p></p><p>大会现场气氛热烈，会场人头攒动，会展区观众络绎不绝。不少与会者表示，这次大会分享的内容不仅实用性强，更兼具深度与广度，真正做到了干货满满。我们深感荣幸与欣慰，感谢每一位参与者的支持与鼓励。正是有了大家的热情参与，我们才能不断前行，继续努力成为技术传播领域的佼佼者，持续提升内容质量，打造更加优质的交流平台，共同推动技术领域的创新与突破。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/85/853d0f68f166bb757fbba5edc3c8e201.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3d/3dcf6fa8c0ad181a51140adf10661f90.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ae/ae8d20c4fcf5afa91e00c851c8fab69b.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e2/e2239d12210e5660f1c58464d0981624.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d7/d74bb9dd14a99fb027cc2cfe09c25d30.png" /></p><p></p><p></p><h2>精彩瞬间：活动亮点集锦</h2><p></p><p></p><h3>智能软件开发生态展</h3><p></p><p></p><p>本届大会对展区进行了重新规划，围绕“智能软件”的主题，广泛邀请生态上下游企业积极来到 QCon 现场展示最新的技术和产品。其中，展区共设置了操作系统、数据库、多模态、智能编码、数字人、模型广场 &amp; 管理 &amp; 调优、性能优化 &amp; 智能测试 &amp; 智能运维、AI Agent 应用及开发平台、AI 应用开发平台等多个细分主题，参会者可以沉浸式体验极具前沿性、互动性的生成式 AI 技术和产品。</p><p></p><p>今年，展区特别设置了【OpenTalk】交流区，广泛邀请参展企业、专家和开发者们分享自己的技术、产品和想法，围绕最新的技术趋势和未来发展畅所欲言。议题包括智能编码、大模型在数据分析领域的探索实践、AI 赋能数字化办公新纪元、让所有人不再为 SQL 问题头疼、AIGC 时代开发者画像分析等。</p><p></p><p>此外，现场还设计了 【寻找 SQL 优化大师】、【编程马拉松】、【TDengine 限时挑战赛】、【快问快答】、【幸运大转盘】 及多款小游戏挑战赛，并为参与者准备了手办盲盒、充电线、盆栽、背包等众多礼品。让我们一起回顾这些精彩瞬间吧！</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/48/48b9ac78c5bb8c42b66ef7a6f602ce35.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/74/7493e4937c4beb524fec4a43ddb6d72a.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7d/7dc9131d0022c317fb65166f468e31c8.png" /></p><p></p><p></p><h3>赞助商展示区：技术创新的支持者</h3><p></p><p></p><p>每一届 QCon 大会的成功举办，都离不开赞助商们的大力支持。正是他们的鼎力相助，让我们能够持续推动技术的交流与创新，为行业发展注入源源不断的动力。本次 QCon 大会得到了众多赞助商的大力支持，包括 Akamai、Elastic、支付宝小程序云、Cloudflare、Greptime、未来智能、伊克罗德、Coupang、IPIP、MongoDB、Palo Alto Networks、YDB、容联云、开放原子开源基金会等。他们的参与不仅为大会增色不少，也为技术共享和行业发展提供了坚实基础。接下来，让我们一同回顾这些令人难忘的精彩瞬间。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ac/ac3af628793217d77cf86e0f3571b40d.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bfb4bb82073b3f170adbcd70b20e6c8e.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e5/e511ff9ebd89a987cf13d3ead5e36b10.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/86/861c60307b60019b64c8bc220e708e4e.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0e/0ee5d80c5486a91206f26b651dc4790e.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TbTZmHSARHQXiTmArO1v</id>
            <title>跟黎科峰、焦可、刘琼 、石建平、张俊九五位重量级大咖共话：Agent 是否是大模型落地的必经之路？</title>
            <link>https://www.infoq.cn/article/TbTZmHSARHQXiTmArO1v</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TbTZmHSARHQXiTmArO1v</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 09:18:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 大模型, AI应用之年, Agent
<br>
<br>
总结: 随着OpenAI发布ChatGPT，国产大模型竞争激烈，2024年被预测为AI的“应用之年”。大模型不仅要夯实基础，还要与业务场景结合，Agent成为热词。在大模型落地过程中，AI Agent的发展方向备受关注。 </div>
                        <hr>
                    
                    <p>随着OpenAI公司发布ChatGPT，国产大模型也如雨后春笋般喷涌而出，“百模大战”盛况吸引全球关注，大模型的发展速度日新月异。美东时间周四，OpenAI的首席运营官Brad&nbsp;Lightcap更是提出预测，2024年是AI的“应用之年”。</p><p>&nbsp;</p><p>现在，大模型早已从单一技术比拼，升级为整个体系生态的竞争。大模型企业决胜关键点不仅在于谁能夯实基础、系统布局，还在于谁能将大模型与业务场景有效结合，帮助企业用好大模型，真实产生落地价值。</p><p>&nbsp;</p><p>在这个背景之下，Agent成为了被提及最多的热词。在&nbsp;4&nbsp;月 11 日召开的&nbsp;QCon&nbsp;全球软件开发大会暨智能软件开发生态展上，【AI&nbsp;Agent&nbsp;智能体落地】专题也毫不意外地成为了爆场专题，吸引了数百名现场观众参与。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/14/14ef54851c0bdddf08531517c638d0e3.png" /></p><p></p><p>Agent应用是否是大模型落地的方向？它是否可以成功实现商业化落地？在此背景下，QCon&nbsp;联合数势科技特别策划了一场关于AI&nbsp;Agent发展的晚场圆桌讨论，旨在深入探讨AI&nbsp;Agent在大模型落地过程中的价值和发展方向。</p><p>&nbsp;</p><p>本场圆桌将于&nbsp;4&nbsp;月&nbsp;12&nbsp;日晚上&nbsp;18:00-20:00&nbsp;在&nbsp;QCon&nbsp;会场【国宾&nbsp;1&nbsp;厅】举行，我们邀请到了数势科技创始人 &amp; CEO 黎科峰博士，百川智能联合创始人焦可，腾讯研究院副院长刘琼，蓝驰资本投资合伙人、TGO 鲲鹏会（北京）学员石建平，实在智能联合创始人张俊九&nbsp;共&nbsp;5&nbsp;位重磅大咖，就以下核心议题发表自己的见解和洞察：</p><p>&nbsp;</p><p>大模型无共识，怎么看待中国在大模型领域所处的阶段，现状和机会，以及未来中国特色的可能的发展方向？大家认为Agent是什么，相较Prompt Engineering的区别，为什么它是大模型落地的主战场？现在很多号称做大模型的，但实际上仔细思考就会发现，很多场景和需求的解决方案没有革新，只是装了“大模型”的套子，那这些是否有“新瓶装旧酒”之嫌？以及当下大部分场景需求，小模型能力就够了？未来Agent要到现在的时代下App一样普遍且百花齐放，还需要多久？那个时候这个市场大概是什么样的？以及生产关系如何？</p><p>&nbsp;</p><p>如果你在&nbsp;QCon&nbsp;大会现场，欢迎你线下加入我们，一同深入探讨&nbsp;AI&nbsp;Agent&nbsp;和大模型的现在与未来。扫描下方二维码，即可免费报名参与活动⬇️&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9e/9e5e6c8687a54c1d0261ce5abc7dae91.jpeg" /></p><p></p><p>如果你对本场圆桌议题感兴趣，但由于行程安排无法现场参与，也可以在线上参与讨论。届时，本场圆桌将在InfoQ和数势科技的视频号进行同步直播，无论是线上还是线下的朋友，都有机会畅所欲言。现场观众将有机会提问，而线上观众的提问将由专门同事收集汇总，后续进行解答。</p><p>&nbsp;</p><p>扫码预约线上直播⬇️&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a419b214ed80324b667ea12b0e87fe40.jpeg" /></p><p></p><p>QCon&nbsp;和数势科技诚邀广大&nbsp;AI&nbsp;爱好者、行业专家和投资者共同参与这场思想的盛宴。无论您是对AI&nbsp;Agent的未来发展充满好奇，还是希望了解大模型技术的最新趋势，相信这场圆桌讨论都能为您提供宝贵的信息和洞见。为了方便众多关注大模型和&nbsp;AI&nbsp;Agent&nbsp;的朋友们持续交流和讨论相关议题，我们也特别设置了【AI&nbsp;Agent&nbsp;话题】交流群。欢迎大家扫码加入我们的讨论群，与行业大咖一起探索AI&nbsp;Agent的未来。⬇️&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b2/b2fcd3aa2ba93fa88723c4ed44dfc831.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DKSGEWAvYIJOcFekjKbH</id>
            <title>被性别歧视、陷经济窘境，AI女神李飞飞自述：我要打破算法中的偏见</title>
            <link>https://www.infoq.cn/article/DKSGEWAvYIJOcFekjKbH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DKSGEWAvYIJOcFekjKbH</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 08:01:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 李飞飞, 移民, 父母, 性别歧视
<br>
<br>
总结: 李飞飞是一位成功的科学家，她的成功背后有着移民经历和父母的影响。在她的成长过程中，她面对过性别歧视，但她的父母给予了她独特的教育方式，培养了她强烈的好奇心和探索精神。通过父母的影响和自身努力，李飞飞克服困难，取得了在科学领域的成功。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>李飞飞的简历非常亮眼：33岁获得斯坦福终身副教授职称，成为首位担任斯坦福大学人工智能实验室主任的女性。她是美国三院院士，也是现代人工智能的关键催化剂&nbsp;ImageNet&nbsp;创建者，前谷歌副总裁、谷歌智能云及人工智能及机器学习首席科学家。在一贯由男性主导的人工智能技术领域里，她的一系列开创性成就无疑是非常耀眼的存在。人们既惊叹于她在人工智能领域的诸多贡献，又为她的逆袭励志故事所感慨万分——在获得所有这些成功之前，李飞飞也曾经度过困顿艰辛的青少年时代：少年时期，她成绩出类拔萃但受到了老师的性别打击；移民初期，她的英语蹩脚，家庭经济拮据又遭遇母亲患病求医；求学期间，也曾多次面临科学抱负与现实生活之间的抉择。这样一个华裔移民少女如何冲破美国社会阶层桎梏，成为引领全球深度学习革命浪潮的“AI教母”？</blockquote><p></p><p></p><p>（下文整理摘编自《我看见的世界：李飞飞自传》，中信出版集团2024）</p><p>&nbsp;</p><p>“我不禁想到自己第一次来华盛顿时的情形，当年我还不知人工智能为何物，还没有进入学术界，与硅谷也没有任何联系。当时我的整个身份（至少对外部世界而言）可以用一个词来概括，那就是——移民。</p><p></p><p>像许多移民一样，我感到被各种纵横交错的文化鸿沟所束缚。一些鸿沟不可名状，另一些则清晰地横亘在我的面前，难以跨越。我是一名女性，而我所在的研究领域由男性主导，“帽衫男”一直是科学领域的典型形象，以至这个词现在已经没有任何讽刺的意味了。这么多年来，我一直在思考自己是否真正属于哪个世界。”</p><p>&nbsp;</p><p></p><h2>父母播下的种子</h2><p></p><p></p><p>我是独生女，出生在北京，但在千里之外的四川省省会成都长大。从名义上看，这里是母亲的老家，但其实她和家人也刚在当地定居不久。他们原籍杭州，&nbsp;20&nbsp;世纪&nbsp;30&nbsp;年代，抗日战争全面爆发，杭州沦陷，他们和成千上万的人一样被迫背井离乡。他们庆幸自己活了下来，却无法摆脱流离失所之痛，甚至连母亲这一代也受到了深刻的影响。</p><p></p><p>外祖父常常追忆动荡之前的往事，每念及此，总是痛心疾首。他在学校出类拔萃，本来前途无量，但为了养家糊口，不得不放弃。即便如此，他们还是陷入了多年的贫困之中。几十年来，他郁郁寡欢，无法释怀。这种情绪传递给了他的子女，也在某一天攫住了我：沉闷而无言，感觉家在他乡、活在别处。</p><p></p><p>如果一个孩子可以在完全没有成人监督的情况下设计出自己理想的父母形象，那么父亲绝对符合我的要求。这是我对他最高的赞美，同时也是最严厉的批评。</p><p></p><p>父亲的外表英俊整洁，但他最突出的性格特征是对任何严肃正经的事情都严重过敏，简直到了病态的程度。他一辈子都像个没长大的孩子，并对此毫无悔意。与其说他拒绝承担成年人的责任，不如说他似乎真的觉察不到自己已经成年，好像缺乏某种其他人与生俱来的基本感知力。他经常突发奇想、随兴而为。</p><p></p><p>在母亲生产那天，父亲却姗姗来迟，只是因为他一时兴起去公园观鸟，完全忘了时间。这次观鸟迟到事件让他想到了用“飞”字来作为我的名字。“飞飞”成了我名字的不二之选。</p><p></p><p>这个名字恰好男女通用，也反映出父亲甚至对性别这种在中国传统文化中至关重要的概念都毫不在意。此外，我们这一代人很少有叫“飞”的，正好符合父亲标新立异的风格。</p><p></p><p>在我童年时期，父亲找来各种零件，自己动手组装了一辆带挎斗的自行车，把我放到挎斗里，穿过成都拥挤的街道，带我到公园或偏远的乡村。我们会花好几个小时捉蝴蝶，观察水牛悠然地躺在被水淹没的稻田里，或者捕捉野生啮齿动物和竹节虫，把它们带回家当宠物。</p><p></p><p>就连外人也能明显看出，我们之间没有传统父女间的等级关系，因为他更像我的同龄人，而不是父亲，在他身上完全看不到为人父的压力和焦虑。</p><p></p><p>父亲那种乐在其中、心无旁骛的专注状态让我明白，无论他是有女儿、有儿子，还是根本没有孩子，他都会这样度过午后时光。正因为如此，他为我树立的榜样才更有感召力。在不知不觉中，他向我展示了最纯粹的好奇心。</p><p></p><p>父亲带我出去玩，不是为了教给我什么东西——他喜欢大自然，但并不是专家——可这种经历却在我心中播下了哲学的种子，成为塑造我人生的最大力量：我对探索自己视野以外的事物产生了永不满足的渴望。</p><p></p><p>如果说我强烈的好奇心源自父亲，那么为这份好奇指明方向的人则是母亲。</p><p></p><p>跟父亲一样，母亲的个性也源于自我认知与社会期待之间的矛盾。父亲是迷失在成年人身份中的孩子，而母亲则是困囿于平庸生活的知识女性。但她意识到想象力并不受现实世界的限制，因此自幼就沉浸于书海之中。读书为她开启了一扇窗，让她了解自己无法到访的地方、无法感受的生活、无法经历的时代。</p><p></p><p>母亲热切地与我分享她对书籍的热爱，就像父亲分享他对大自然的喜爱一样。她鼓励我广泛阅读各种类型的书。所以，我不仅熟读鲁迅的作品和《道德经》等道家经典，也如数阅读了《第二性》《双城记》《老人与海》《基度山伯爵》等西方经典的中文译本。</p><p></p><p>外祖父母对我的培养方式也契合了父母的价值观。他们并不认同他们这代人中盛行的重男轻女的观念，而是跟父母一样，鼓励我展开想象，并坚守原则：我首先是个独立的个体，其次才是个女孩。跟母亲一样，他们给我买了很多书，涵盖海洋生物、机器人和中国神话等各类广泛的主题。</p><p></p><p>直到长大后，我才意识到，原来我们家门口以外的世界可能更加纷繁复杂。</p><p>&nbsp;</p><p></p><h2>科学是男孩的游戏？</h2><p></p><p></p><p>令人快意的校园学习时光在一个下午戛然而止——至少对我来说是如此。小学的最后一年即将结束，在平淡无奇的一天，老师在下课时提出了一个奇怪的要求：女生先回家，男生在座位上多坐几分钟。我顿时好奇了起来，于是在教室门口徘徊，藏在了一个能听到老师说话的地方。我听到的那些话让我终生难忘。</p><p></p><p>“我让女同学先走，是因为现在我要告诉你们：你们的整体表现是不行的。男孩天生就比女孩聪明，数学和科学就是体现你们脑子灵光的基础学科。你们的平均成绩竟然比女生还低，这种情况没有任何借口。我今天对你们非常失望。”</p><p></p><p>接下来，也许是觉得有必要鼓励一下大家，老师的语气似乎缓和了一些：“但你们也不要自暴自弃。等到了十几岁，你们会发现，周围的女生自然就变笨了。她们后劲不足，成绩会不断下降。即便如此，我还是希望你们都能更加努力，发挥你们作为男生的潜力。落在女生后面是不可接受的，大家明白了吗？”</p><p></p><p>我愣了一会儿才反应过来。在此期间，我的脑子中冒出无数个问题：老师真的相信男生天生脑子更好使吗？我们女生真的会长大就变笨吗？难道所有老师都是这么看我的？他们一直都是这样想的吗？我该怎么理解说这些话的竟然是一个……女老师？</p><p></p><p>又过了一会儿，种种疑问被另一种感觉所替代，它沉重而强烈，从我体内不知何处升腾而起。这种感觉不是气馁，甚至不是感到被冒犯，而是愤怒。</p><p></p><p>这是我不熟悉的愤怒之感——是一股悄然而炽烈的怒火，一种我从母亲身上见过的愤慨，但它无疑是属于我自己的。</p><p></p><p>老师的这番话并不是性别歧视的第一个迹象，大多数迹象都非常隐晦，甚至难以辨别，比如我会隐约感觉到，在数学和科学方面，老师更愿意鼓励男生。</p><p></p><p>还有一些区别对待则是不加掩饰的。比如有一次我报名参加一年级的足球比赛——不是“男队”，而是校队——结果却被告知女生不能参加。</p><p></p><p>老师的话虽然让我震惊，但并没有让我气馁。相反，这些话强化了我成长过程中形成的理念：无论周围有什么障碍，都要奋力超越现实，构想出更加广阔的未来。现在我不仅想看得更远，还想走得更远。如果说数学和科学这类领域是属于男生的游戏，那又怎样，学习毕竟不是球赛，他们无法阻止我在这里上场参赛，我暗下决心，一定要赢。</p><p></p><p>后来，我进入了一所吸引全市优秀学生的中学。在那几年里，对女孩的预设和偏见让我越来越不耐烦，这种情绪已经超出了课业的范围。</p><p></p><p>在同龄人中，我已经有“假小子”的称号，但老师的话仍然在我的记忆中回响，使我把一开始的怪癖上升到了个人使命的高度。</p><p></p><p>像任何喜欢把生活想象成电视剧的青少年一样，我很容易认为在与中国的性别规范做斗争的过程中，自己是在孤军奋战。我把头发剪得极短，拒绝穿裙子，和一群骑单车、爱打闹、聊战斗机而不是校园八卦的男同学混在一起，全身心投入出乎他人意料的兴趣中，尤其是航空航天科学、高超声速飞机的设计，甚至还有不明飞行物等超自然话题。</p><p></p><p>母亲是我坚实的守护者。当她觉得自己的价值观——我们的价值观——受到质疑时，她会毫不犹豫地进行防卫。我的中学老师就领教过她的厉害，并且这次令人难忘的会面，似乎直接改变了我的命运。</p><p></p><p>“您女儿特别聪明，这一点毫无疑问。但我担心，她对自己的前途不够严肃。比如，期末考试越早开始准备越好，所以我经常要求每个学生都跟全班同学分享自己正在读的书。大部分同学分享的都是教科书、备考资料和学校推荐的阅读书目。但是，飞飞这周推荐的书让我很担心，而且……”</p><p></p><p>老师话音未落，母亲就插话道：“我女儿从小就特别爱看书。”她对自己的轻蔑态度毫不掩饰。</p><p></p><p>“问题就出在她读的这些书上。你看看，《不能承受的生命之轻》？勃朗特三姐妹的书？还有她订的这些杂志，又是关于海洋生物的，又是关于战斗机的，还有不明飞行物的……例子太多了。她没有重点阅读符合课程价值观和理念的文学作品。”</p><p></p><p>“是吗？所以呢？”</p><p></p><p>在接下来的片刻沉默中，我坐在母亲身边，竭力不让血管里流淌的喜悦流露在脸上。紧张的气氛又持续了一两分钟，然后老师向前倾身，做出最后一次尝试，声音里多了一丝严厉。</p><p></p><p>“我就直说了吧。您的女儿也许真的挺聪明的，但班上聪明的学生并不少。智力只是成功的一个因素。另一个因素是要有纪律性，要把个人兴趣放到一边，专心学习对未来最有用的东西。”</p><p></p><p>我不确定母亲接下来的话是不是一种回应。她低下头，声音比之前更轻了。“这是飞飞想要的吗？这是我对她的期望吗？飞飞，你和我一样，都不属于这里。”</p><p></p><p>改变在1992年到来，我刚满15岁不久，随着父母来到了大洋彼岸——美国的新泽西州定居。</p><p></p><p></p><h2>十字路口的抉择</h2><p></p><p></p><p>在得知我被普林斯顿大学以近乎全额的奖学金录取时，虽然母亲表现得非常冷静，时隔数年，我才真正完全理解这一刻对母亲和这个家庭的重要意义。</p><p></p><p>母亲生命中的每一个里程碑都在提醒她，她站在了那些无法弥合的鸿沟的错误一边。几十年来，她已经习惯了假装自信，但我知道，她从未真正感受到自信。现在，也许是有生以来第一次，她终于有理由相信这个故事可能没有如此简单。她已经押上了所有，至此才有了一种真正如释重负的感觉。</p><p></p><p>直到1999&nbsp;年，我在普林斯顿大学的学习生涯即将结束，再次面临科学抱负与现实生活之间的抉择。读研的诱惑与开启职业生涯的压力让我左右为难。这次是一个真正的两难困境：</p><p></p><p>母亲的健康状况日益恶化，经营洗衣房的劳累和家庭的债务都在给她增加着巨大压力。而华尔街巨头可以提供了一切：福利、晋升机会、令人艳羡的起薪，当然还有真正的医疗保险。他们承免除我们的债务，为我的家庭提供保障。而对我的唯一要求就是放弃科学。</p><p></p><p>“飞飞，这是你想要的吗？”</p><p></p><p>“你知道我想要什么，妈妈。我想成为一名科学家。”</p><p></p><p>“那还有什么好说的呢？”</p><p></p><p>面对我的含糊其词，母亲的回应总是一针见血，速度之快让我得花点儿时间才能反应过来。三步绝杀，一剑封喉。我要去读研究生了。</p><p></p><p>而两年后，坏消息还是再次出现。选择兼修神经科学和计算科学的研究生课程，已经让我的体力和毅力达到了崩溃的边缘，在这个时候得知母亲患上充血性心力衰竭，我的感受复杂到无法用语言来表达。</p><p></p><p>一个新的现实正在浮现，它如此复杂，动摇了我以物理学专业学生的身份走进普林斯顿大学报告厅以来所做的每一个决定。毕生的好奇心把我带进了一个竞争激烈、薪酬低廉、无法保障长久职业生涯的领域，而我的父母现在需要我无法提供的支持。</p><p></p><p>我每天都在追求自己的梦想，这让我觉得自私至极，甚至过于鲁莽。我的实验室伙伴大多来自中产阶级，有些甚至家境非常富裕。我越是反思与他们家庭之间的差异，就越难以否认这样一个事实：成为科学家是一种奢望，我负担不起。</p><p></p><p>几周后，一位同学提到，世界知名管理咨询公司麦肯锡的合伙人来招聘了。他们正在寻找一个实习级别的分析师，常春藤盟校里只要跟数学和计算机科学有一点联系的研究人员，都可以成为理想的候选人。在真正绝望的时刻，这似乎是一个值得考虑的机会。</p><p></p><p>当然，我以前也经历过这种情况。我的学术目标和现实生活之间一直存在冲突，我很想把这次事件也当成最近的一次小冲突。但这一次，我内心科学家的声音与以往不同。在母亲的健康状况受到新一轮的打击后，它变得不那么坚定，就连我内心那个特殊而戒备的部分也开始屈服了。</p><p></p><p>面试后麦肯锡公司立即给了我肯定的回复，并决定将我的实习机会转为长期的正式职位。</p><p></p><p>我的心中五味杂陈，难以言表。一方面，我将要抛下我所研究和热爱的一切，另一方面，我亲眼看到父母多年来濒临绝境，越来越觉得他们是为了我才做出这么大的牺牲。这份工作似乎让我终于可以卸下长久背负的重担，为了我能来美国，母亲已经付出了一切。我知道现在是她最需要我的时候。</p><p></p><p>我跟母亲说了面试、工作机会和其他所有的一切，告诉她待遇、起薪，以及在我还没来得及答复之前，他们就已经提出了优厚的待遇。我解释说，无论从哪个角度来看，这都是通往每个移民母亲都希望自己孩子拥有的职业生涯的捷径。她礼貌地听着，但我还没说完，就在她脸上看到了那种熟悉的表情。</p><p></p><p>“我们真的要再次讨论这个问题吗？我了解自己的女儿。她不是管理顾问，或者其他什么职务。她是个科学家。”</p><p></p><p>“想想你的身体吧，妈妈！想想我们的开销。搞学术能给我们带来什么呢？”</p><p></p><p>“飞飞，我们走到这一步，不是让你现在放弃的。”</p><p></p><p>“这不是放弃！这是我梦寐以求的工作，一份事业，可以让我们摆脱目前的困境。看看我们现在活成什么样了！三个大人住在一个宿舍里！”</p><p></p><p>母亲停顿了一会儿，也许是在思考这些话，然后回答说：“飞飞，你一直在说自己走的路很‘自私’，就好像你追求科学是在牺牲我们一样。”</p><p></p><p>“我怎么能没有这种感觉呢？我现在本来可以养活咱们全家，而且……”</p><p></p><p>“你没明白我的意思。这从来就不是你一个人的路。从一开始，这就是我们全家的路。不管你是注定要成为科学家、研究员，还是其他我没有办法想象的职业，也不管你能不能从中赚到钱，从我们的飞机离开上海的那一刻开始，我们全家就一直在为这个目标努力。”</p><p></p><p>我不知道该说什么。</p><p></p><p>“我再说最后一次：我们走到这一步，不是让你现在放弃的。”</p><p></p><p>她是对的。她总是对的。这一次，不知什么原因，我终于听进去了她的话。并且我再也没有质疑自己的道路了。</p><p></p><p></p><h2>她们，同样属于这个时代</h2><p></p><p></p><p>2015年，图像分类本已是老生常谈的成功技术，但技术接二连三出现失误：将达豪集中营大门的照片标记为攀爬架，把一位脸上涂有彩色粉末的白人妇女贴上了“猿”的标签。虽然事故并非恶意，但这并不能让人感到宽慰。相反，无心之失所揭示的问题才更加令人不安：包括ImageNet&nbsp;在内的数据集由于缺乏多样性，导致了一系列意料之外的结果，未经充分测试的算法和存疑的决策又进一步加剧了负面影响。当互联网主要呈现的是以白人、西方人和男性为主的日常生活画面时，我们研发出的技术确实很难理解其他人群。</p><p></p><p>正如记者兼评论员杰克·克拉克（&nbsp;Jack&nbsp;Clark）所言，问题的根源在于人工智能“男性之海”问题：科技行业的代表性不足，导致算法无意中带有偏见，在非白人、非男性用户身上表现不佳。</p><p></p><p>从代表性问题的出现，到问题被大众真切地感受到，中间往往需要几年的时间。因此，我和几位伙伴联合创立了非营利教育组织&nbsp;AI4ALL，推动向处于高中阶段的女生、有色人种和其他未被充分代表的群体，开放斯坦福大学人工智能实验室课程，提高&nbsp;STEM（科学、技术、工程与数学）领域的包容性。</p><p></p><p>现在只是迈出了一小步，但我们实现了从无到有的跨越。只需要一点点努力，就可以让每个一直被历史排除在外的参与者相信，她们同样属于这个时代、这个领域。</p><p></p><p>此外，项目还能带来一丝安慰——在业界追逐人工智能未来时，往往肆意而为，缺乏自省，而我们的努力能够保证，至少有一小部分人在逆向而行。</p><p></p><p>2016&nbsp;年，我迎来&nbsp;21&nbsp;个月的学术休假，暂时离开教授职位。经过再三考虑，我最终决定接受谷歌云的人工智能首席科学家一职。我还碰巧认识公司新任命的谷歌云首席执行官黛安娜·格林（&nbsp;Diane&nbsp;Greene），是为数不多征服硅谷的女性，我期待在性别比例极不平衡的行业里与她并肩工作。</p><p></p><p>现在回顾我的职业生涯，这段漂洋过海的经历给我留下了深刻的烙印。然而直到现在我才意识到，这种烙印持续影响我的研究和思考：最好的作品总是在边界上诞生，在那里，思想永远被困在来去之间，由陌生土地上的陌生人探索，既是局内人又是局外人。但这正是我们如此强大的原因。独特的身份让我们保持独特的视角，赋予我们自由挑战现状的能力。</p><p></p><p>作为一个女儿、科学家、移民和人道主义者，我看到了众多不同的世界，但最重要的世界是我将不会生活在其中的世界，是建立在我现在所做的一切之上的世界，是我倾注了所有爱和希望的世界，也是我最为感恩的世界。正是因为这个世界的存在，我现在所做的一切才有意义。这个世界就是我的孩子们和他们的孩子们将继承的世界。在人工智能时代，做母亲是最令我谦卑的体验，我相信，这也将永远是独属于人类的体验。</p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ykFwfTeGv0OxOA9TfC8k</id>
            <title>Kyligence 发布企业级 AI 解决方案，Data + AI 落地迈向新阶段</title>
            <link>https://www.infoq.cn/article/ykFwfTeGv0OxOA9TfC8k</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ykFwfTeGv0OxOA9TfC8k</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 07:48:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Kyligence, AI, 指标平台, 企业级解决方案
<br>
<br>
总结: Kyligence在2024数智论坛发布了全新的企业级AI解决方案，包括智能一站式指标平台和AI数智助理，助力企业在数智化浪潮中掌握先机。企业需要将AI结合业务落地，Kyligence提供准确、可靠的Data+AI落地应用，帮助企业实现统一的数据语言和目标管理。企业在落地AI应用时需要关注核心业务流程，构建自己的数据壁垒，让AI更好地服务企业，提升数据决策效能。 </div>
                        <hr>
                    
                    <p>4 月 11 日，<a href="https://www.infoq.cn/article/gqZaz5RWuh4yYFjDNGTd?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Kyligence&nbsp;</a>"2024&nbsp;数智论坛暨春季发布会成功召开。Kyligence&nbsp;正式发布全新的企业级&nbsp;AI&nbsp;解决方案，基于服务金融、零售、制造、医药等行业领先客户的落地实践，Kyligence&nbsp;为企业提供准确、可靠、智能的&nbsp;AI&nbsp;+&nbsp;<a href="https://www.infoq.cn/article/AugPNHNf201DVzLGSXkU?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">指标平台</a>"一站式解决方案，以行业领先的技术和稳定可靠的产品助力更多客户在数智化浪潮中掌握先机。来自德勤和&nbsp;Kyligence&nbsp;的多位嘉宾分享了&nbsp;Data&nbsp;+&nbsp;AI&nbsp;现阶段在企业场景中落地的痛点，并带来&nbsp;AI&nbsp;+&nbsp;指标平台在金融、零售、制造、医药等行落地的最新成果，吸引了众多观众的参会与热烈讨论。</p><p></p><p></p><h4>准确、可靠的&nbsp;AI，Kyligence&nbsp;AI&nbsp;解决方案正式发布</h4><p></p><p></p><p>随着大模型的迅速发展，企业逐渐从聚焦技术转向关注应用，迫切需要将&nbsp;AI&nbsp;结合业务落地，在市场竞争中抢占先机。Kyligence&nbsp;联合创始人兼&nbsp;CTO&nbsp;李扬提到，2023 年&nbsp;Kyligence&nbsp;产品全面集成&nbsp;AI&nbsp;能力，推出了智能一站式指标平台&nbsp;Kyligence&nbsp;Zen&nbsp;和&nbsp;AI&nbsp;数智助理&nbsp;<a href="https://www.infoq.cn/article/aCJ0dhxWzo6exYQgYBsZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Kyligence&nbsp;Copilot</a>"，为企业使用数据带来了革新体验，并已率先在金融、零售、制造、医药等客户的真实场景中落地。</p><p></p><p>基于技术沉淀、创新产品和实践经验，Kyligence&nbsp;正式发布了&nbsp;AI&nbsp;解决方案，将为企业级客户提供准确、可靠的&nbsp;Data&nbsp;+&nbsp;AI&nbsp;落地应用，通过对接企业已有的数据源，智能一站式指标平台将帮助企业实现统一的数据语言和目标管理，以及服务型的数据治理；其配备的&nbsp;AI&nbsp;数智助理将进一步降低业务用户使用数据的门槛，助力业务人员进行快速、准确的决策，为业务创新提供数据支持；此外，Kyligence&nbsp;独具技术优势的企业级&nbsp;OLAP&nbsp;平台更将为企业大规模使用数据、推广&nbsp;AI&nbsp;应用提供坚实的技术底座。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e610378a63987b675dc80370ca1851d.png" /></p><p></p><p>在本次演讲中，Kyligence&nbsp;CTO&nbsp;李扬还分享了&nbsp;Kyligence&nbsp;在部分领先企业落地&nbsp;Data&nbsp;+&nbsp;AI&nbsp;的最新成果。在某头部城商行的真实场景中，通过&nbsp;AI&nbsp;自然语言交互，Kyligence&nbsp;满足银行总分行在灵活报表等场景的分析需求，释放数据开发人力资源，提升数据使用效率，其中&nbsp;AI&nbsp;对话准确率可达到&nbsp;95%&nbsp;以上，100%&nbsp;可解释，为&nbsp;AI&nbsp;进一步在银行进行大规模的推广和应用打下了坚实的基础。</p><p></p><p></p><h4>落地&nbsp;AI&nbsp;应用，让数据真正服务好业务</h4><p></p><p></p><p>大模型时代，技术快速发展，企业面对随之而来的巨大发展机遇，在抱有极高预期的同时，也存在信心不足的情况。德勤中国数智研究院主管合伙人尤忠彬带来了《以“内功不变”应“时代万变”——大模型时代企业的制胜之道》的精彩分享，他指出，在快速变化的&nbsp;AI&nbsp;时代，企业应当保持定力，持续围绕战略、人才、风险、数据、生态等“不变”关键要素重点建设。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79a2af7a55b945d1490eeaea5af35c62.png" /></p><p></p><p>在落地&nbsp;AI&nbsp;应用时，企业需要围绕核心业务流程挖掘“小切口、大纵深”的大模型应用场景，搭建变革业务模式的“杀手级场景”&nbsp;；与此同时，大模型仍然依赖大规模的训练数据，尤其是高质量的数据；企业需要更加关注构建自己的“数据壁垒”，将业务数据沉淀为指标资产。</p><p></p><p></p><h4>让&nbsp;AI&nbsp;用起来，金融、零售、医药落地&nbsp;Data&nbsp;+&nbsp;AI&nbsp;成果</h4><p></p><p></p><p>聚焦&nbsp;AI&nbsp;+&nbsp;指标平台在不同客户的实际落地场景，Kyligence&nbsp;解决方案与服务总监甘甜带来了《智能一站式指标平台在头部企业的实践分享》的精彩演讲，公开了&nbsp;Kyligence&nbsp;助力银行、零售、医药等头部企业落地&nbsp;AI&nbsp;解决方案的真实案例，深入分享了&nbsp;Kyligence&nbsp;如何解决企业级生产场景下准确性、安全性、成本和&nbsp;AI&nbsp;治理等问题，让&nbsp;AI&nbsp;更好地服务企业，从而提升数据决策效能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/74/74d503a96120290115986e7faaadc482.png" /></p><p></p><p>甘甜在演讲中介绍，Kyligence&nbsp;携手某头部跨国药企打造了精准、敏捷、全面、智能的商业分析平台，以&nbsp;AI&nbsp;+&nbsp;指标平台这一全新的形式满足业务部门复杂多样的分析需求，提高了商业洞察的质量和决策效率；Kyligence&nbsp;助力合作多年的国内顶流餐饮连锁企业升级了指标平台，结合最新的生成式&nbsp;AI&nbsp;技术，在一线人员范围推广并使用&nbsp;AI&nbsp;数智助理，进一步降低业务人员使用数据的门槛；基于服务多年金融客户的经验和结合&nbsp;AI&nbsp;的智能一站式指标平台，Kyligence&nbsp;服务头部银行创新分析对公存款大额异动、绩效分析、贷款业务管理等场景，让银行分支行也能享受数智化转型的最新成果，真正将&nbsp;AI&nbsp;用起来。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ar7Rh2iQVmC77t7yKsoU</id>
            <title>蚂蚁集团 CodeFuse 发布“图生代码”功能，支持产品设计图一键生成代码</title>
            <link>https://www.infoq.cn/article/ar7Rh2iQVmC77t7yKsoU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ar7Rh2iQVmC77t7yKsoU</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Apr 2024 05:01:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, CodeFuse, AI 编程, 图生代码
<br>
<br>
总结: 蚂蚁集团推出自研智能研发平台CodeFuse，新增“图生代码”功能，支持开发人员用设计图一键生成代码，提升前端页面开发效率。AI编程助手趋势下，CodeFuse覆盖研发全链路，帮助开发者更快编写代码。CodeFuse在蚂蚁内部得到广泛应用，生成的代码整体采纳率为30%，助力降低开发工作量。AI的普及降低编程门槛，推动软件开发行业创新，CodeFuse致力于打造创新解决方案，持续推出新功能提升研发效率。 </div>
                        <hr>
                    
                    <p>4 月 11 日，<a href="https://www.infoq.cn/article/bjCH8kMloxFUfp00WQIX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">蚂蚁集团</a>"自研的智能研发平台<a href="https://xie.infoq.cn/article/5bca12068d34015544e2d5eae?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> CodeFuse </a>"推出“图生代码”新功能，支持开发人员用产品设计图一键生成代码，大幅提升前端页面的开发效率。目前相关功能正在内测。</p><p>&nbsp;</p><p>和很多互联网公司一样，蚂蚁集团正在内部全面推行 AI 编程，使用 CodeFuse 支持日常研发工作的工程师达到 50% 以上，这些工程师提交的代码中 10% 由 AI 生成。</p><p>&nbsp;</p><p>Gartner 发布的 2024 年十大战略技术趋势指出：到 2028 年，75% 的企业软件工程师将使用<a href="https://www.infoq.cn/article/LfOxXc1gbayiD4WQ0u6l?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> AI 编程</a>"助手。蚂蚁 CodeFuse 就是这一趋势下的探索尝试。据介绍，CodeFuse 的功能覆盖了需求分析、编程开发、测试与构建、发布与运维、数据洞察等研发全链路，比如在开发测试阶段，通过代码补全、添加注释、解释代码、生成单测、代码优化等，帮助开发者更快、更轻松地编写代码。</p><p>&nbsp;</p><p>目前，在蚂蚁内部，每周已有超五成程序员在日常研发中使用 CodeFuse。CodeFuse 生成的代码整体采纳率为 30%，在生成单元测试场景采纳率可以达到 50%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c8cc4cd40f7c39e4947360bf868bc93.png" /></p><p>（图说：蚂蚁智能研发平台CodeFuse覆盖AI研发全链路）</p><p>&nbsp;</p><p>此次发布的“图生代码”功能主要服务前端工程师。在互联网产品开发环节，设计师画出设计图后，前端工程师需要用代码实现产品设计图，这项工作占用了较多开发工作量。图生代码，可以根据设计图一键生成代码，可以极大降低开发团队在开发网页、小程序、APP 时的代码工作量。以一张中型网页为例，如果最终有 200 行代码，一人耗时约需1小时，一键生成后，工程师只需检查与调整，耗时大幅降低。这项功能基于蚂蚁百灵大模型的多模态技术能力研发。</p><p>&nbsp;</p><p>蚂蚁集团 CodeFuse 负责人表示，AI 的普及不仅可以减少开发人员的工作压力，让他们有更多精力投入到更有创造力的工作中去，更大的意义在于降低编程门槛，推动软件开发行业的创新和进步。CodeFuse 的使命是探索下一代 AI 研发生产力工具，致力于打造创新的解决方案，让软件开发者在研发过程中如丝般顺滑。在自然语言生成代码、图生代码之后，CodeFuse 还将持续推出新功能，助力企业研发全链路的效率提升。</p><p>&nbsp;</p><p>该负责人认为，AI 研发范式的变革，并不代表“人”在研发场景的角色会消失，反而对 AI 和人如何协同提出了更高的要求，特别是涉及可靠性的运维场景，还需要人工专家干预才能让系统健康运行起来。“AI 目前主要集中在辅助编程(code&nbsp;copilot)，要从 copilot 走向 co-worker，实现整个研发生命周期的智能化、自动化，还有很长的路要走。”</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/L4YFfW6DkuNzyP32JsYt</id>
            <title>钉钉卡位战：SaaS 挣不到的钱，Agent 会挣到</title>
            <link>https://www.infoq.cn/article/L4YFfW6DkuNzyP32JsYt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/L4YFfW6DkuNzyP32JsYt</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 07:05:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI原生, 钉钉, GPT Store
<br>
<br>
总结: 2020年，不穷加入钉钉，面临大模型竞备赛。钉钉团队开始投入智能化，使用大模型重做高频产品。钉钉选择Agent作为AI原生方式。钉钉依赖大模型公司，定义为AI应用创作平台。GPT Store模式吸引用户，但目前应用难成高价值产品。不穷认为解决问题有价值，个人或企业创建AI助理需解决具体问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜叶军（不穷），钉钉总裁作者｜褚杏娟</blockquote><p></p><p>&nbsp;</p><p>2020 年，刚带队做出全国第一张健康码的不穷加入了钉钉。不穷是阿里的第一位校招计算机博士，从 PC 时代开始触网，完整经历了移动互联网时代。而他如今面临的，是一场关于大模型的竞备赛。</p><p>&nbsp;</p><p>想必已经无需用过多笔墨赘述。2022年底至今，ChatGPT的出圈程度还没有谁能超越，其背后的技术方向也早已经被竞相追捧。就像不穷说的“大家都充满了 FOMO 情绪。”</p><p>&nbsp;</p><p>那么身处其中的钉钉，能够在这次浪潮扮演什么样的角色？</p><p>&nbsp;</p><p>和很多企业一样，钉钉最初也没有一下就找到合适的入局方式。一年多前，看到自己与微软不约而同地都选择了给当前产品增加AI能力时，钉钉团队觉得这种方式已经很让人眼前一亮了，大模型爆发的能量远比想象得还要大。</p><p>&nbsp;</p><p>整个2023年，OpenAI、微软、谷歌等大模型发布频繁，李开复、王小川等也亲自下场发布大模型……去年4月，钉钉全面投入智能化，开始用大模型逐个将高频产品重做。到了当年六、七月份，钉钉内部有人提出：能否有一个直接 AI 原生的产品？这引发了内部关于从“+AI”到“AI+”的争论。这个想法与去年10月李彦宏公开提到的AI原生理论异曲同工，可见国内的探索思路其实差异并不大。</p><p>&nbsp;</p><p>那什么是AI 原生？钉钉也在思索。其实在将思维转换成以AI为中心后，这个问题就不难回答。</p><p>&nbsp;</p><p>“AI 原生产品从一开始的思考就是全新的，就是要用纯AI的思路来解决一个任务。它从数据感知、任务分解，再到思维链，最后到行动执行，是一种全新的思考架构。”不穷表示。</p><p>&nbsp;</p><p>而对于做AI原生的方式，钉钉选了Agent。</p><p>&nbsp;</p><p></p><p></p><p></p><p></p><h2>“我们依赖这些大模型公司”</h2><p></p><p>&nbsp;</p><p>今年1月，OpenAI正式推出了GPT Store。几乎同时，钉钉发布了 AI 助理，并宣布4月推出AI助理市场。</p><p>&nbsp;</p><p>“GPT Store上线的第一天我就用了，当天就已经有非常多的产品了，速度非常快。但我分析了所有数据后发现，GPTs同质化非常严重，而且都是一些通过简单限定词、指令或角色扮演来形成的AI 助手。”不穷说道。</p><p>&nbsp;</p><p>这一体验让不穷认定，钉钉未来的AI助理市场不做全量推荐，只会推荐自己精选过的AI助理。</p><p>&nbsp;</p><p>不穷强调，钉钉模式与GPT Store的不同：GPT Store通过不断丰富插件使AI Agent 能够批量调用外部系统的能力，但它的组合效率要比钉钉低，因为钉钉拥有天然的 To B 环境，其中有大量的工作任务需要解决。</p><p>&nbsp;</p><p>不穷认为，目前钉钉的能力不在于做自己的大模型，而是在应用和数据，在工程性、用户体验以及 To B 理解方面。对于Agent 来说，大模型只是其中的一个能力，此外还需要非常好的场景和高质量的业务环境数据，这两者恰恰是钉钉有、而OpenAI 目前还欠缺的。</p><p>&nbsp;</p><p>对于与国内大模型公司的关系，用不穷的话说是：“我们依赖这些大模型公司，它们是我们的发动机和心脏。没有它们，我们无法运行。”</p><p>&nbsp;</p><p>面向AI，不穷把钉钉定义为AI应用创作平台，企业在这个平台上连接、开发和加工各种应用。钉钉的核心任务就是连接场景和数据，实现结构性自动化和批量处理各种工作，并通过Agent让创作变得更简单。</p><p>&nbsp;</p><p>具体来说，钉钉的职责是确保外部记忆存储部分的完善，包括短期和长期记忆的处理，同时做好任务规划，之后将大模型生成的内容与本地业务数据集成，并将形成的行动在各个系统中落地。</p><p>&nbsp;</p><p>在不穷看来，当前国内各个基础模型之间的差距并不大，未来不是每个开发者都会关心基础模型的选择，他们更注重解决业务场景中的问题。因此，如果基础模型效果不理想，开发者应该可以随时更换。</p><p>&nbsp;</p><p>因此，为快速上线和体验，钉钉选择了通义千问作为默认大模型，除此之外用户有需求时还接入了其他大模型公司的模型，如智谱AI、月之暗面、Minimax等。用户的业务逻辑可以建立在自己选择的基础模型体系上，业务流程和数据流也不会进入钉钉平台。</p><p>&nbsp;</p><p>“根据不同的场景和需求，我们可能还会推荐小模型或专用模型。”不穷说道。</p><p>&nbsp;</p><p>不穷在给用户提供模型的选择建议时，会提醒他们更加关注模型的性能，如每秒处理的token量；大模型的安全性问题等，如本地部署还是云上部署；工程解决方案的多重性和便捷性等。这些问题也是钉钉构建AI助理时实际遇到的。</p><p>&nbsp;</p><p></p><h2>“C 端还没有太多优秀的产品形态出现”</h2><p></p><p>&nbsp;</p><p>无疑，GPT Store 的模式吸引了大批用户：刚正式发布时，OpenAI 就宣称已经有超过 300 万个 GPTs。</p><p>&nbsp;</p><p>与传统软件相比，AI助理、GPTs等的不同之处在于拥有非常快的更新速度，模型、交互方式、数据和产品形态等方方面面都变得迅速，开发者也不要从头到尾进行开发和维护。这种模式还大大降低了开发门槛，没有研发背景的人也可以尝试，而对于研发人员来说则大大缩短了研发、测试等成本。</p><p>&nbsp;</p><p>一方面，这意味着传统软件的研发模式可能会面临变革；但另一方面，不穷也指出，百万千万级的GPTs目前看相对来说形式比较单一，没有传统软件那样强大的业务理解能力，因此目前 GPT Store 中的应用很难成为高价值产品。</p><p>&nbsp;</p><p>不穷认为，尽管 GPTs 的创建能力很强，甚至一天可以创建几十个，但它目前还代替不了传统软件市场。</p><p>&nbsp;</p><p>钉钉也在寻找有价值的产品。在1月份宣布启动的AI助理创造大赛上，目前有超过2000支队伍提交作品，不穷也会亲自体验这些AI助理，寻找优秀的作品。&nbsp;</p><p>&nbsp;</p><p>那么，个人玩家又如何在GPT Store 这种模式中赚到钱呢？</p><p>&nbsp;</p><p>不穷的答案是价值，“只要有价值就一定能挣到钱，只是迟早的问题。”在他看来，个人或企业创建AI助理的核心在于要解决具体的问题，解决问题本身就有价值。但现在“卖工具”的人可能不是最终解决问题的人，解决问题的人是那个场景中离问题最近的人。</p><p>&nbsp;</p><p>AI 助理的商业模式则与传统软件相似，需要一定的用户使用量，“只要使用量上去了，很快就会有开发者赚到第一桶金。”根据不穷的经验，一旦调用量达到百万次，软件做商业化就是必然的。</p><p>&nbsp;</p><p>这与之前钉钉在与IDC联合发布的《2024 AIGC 应用层十大趋势》中提到的观点“新一轮的AIGC之争，也将会是一场流量入口之争”是契合的。钉钉在其中也提到了有望成为超级App 的想法。</p><p>&nbsp;</p><p>“在目前的To C场景中，我还没有看到太多优秀的产品形态出现。”不穷说道，“但是，AI Agent 绝对不是自我陶醉，我相信一定会有出色的产品出来，应该给创新者更多的时间。”</p><p>&nbsp;</p><p>根据不穷之前的访谈，AI Agent和智能助理产品发展到一定程度后，中间态、碎片化的产品成为极简流量入口，就会出现“No App”理念重塑应用的情况：通过对话即可直接调取、使用各种工具，更多非软件专业人员也能获得强大的系统服务。</p><p></p><h2>“SaaS挣不到的钱，会通过Agent挣到”</h2><p></p><p>&nbsp;</p><p>相较C端，B端是不穷更看好的方向，因为 To B 场景更容易产生有效的产品：确定的数据和场景可以帮助解决大模型的幻觉问题，同时通过批量和自动化的方式提高 To B 常见工作流和任务流的效率问题。</p><p>&nbsp;</p><p>不穷指出，To B 软件的目的是解决问题，所以这里天然聚集了大量的问题和数据。在这样环境里构建的AI Agent，传统 SaaS 和 PC 软件软件的开发流程、产品交互、形态及维护等的缺陷都将得到弥补。未来，SaaS 的定制化或各种行业需求，都可以用简单、低成本的方式实现。</p><p>&nbsp;</p><p>“Agent 市场形态肯定会取代传统软件市场形态。更重要的是，它将取代传统SaaS产品的产品形态。”不穷说道。</p><p>&nbsp;</p><p>现在AI助理的 to B 服务中，钉钉要与用户频繁、深入地互动，根据反馈不断调整和改进。比如在解决一家芯片企业客服培训难题时，钉钉团队要去公司了解实际工作流程，然后将线下流程转化为线上的AI助理。</p><p>&nbsp;</p><p>虽然每个企业的需求相同，但产品会逐渐沉淀下来。企业自行完成标注、训练和本地化数据接入，钉钉则保留抽象层和公共层，逐渐完成产品的广泛行业适用性。</p><p>&nbsp;</p><p>不穷评价OpenAI做产品就像是科学家通过成千上万次的实验，最终找到一个正确的方向，逼近科学真理。而钉钉则投入大量时间与客户共创，解决他们的实际问题。两者虽然方向不一，但殊途同归。</p><p>&nbsp;</p><p>与大模型创业公司苦苦寻找自己的商业模式相比，钉钉探索出来的大模型商业模式已有三种。</p><p>&nbsp;</p><p>第一种是基于调用量的模式。无论个人AI助理还是企业助理，产品使用频率越高、解决问题的能力越强，吸引的用户就越多，自然也就需要更多的调用。使用量大，消耗的算力和资源也就越多。</p><p>&nbsp;</p><p>第二种是应用层本身带来的商业模式。传统的SaaS模式赚钱较为困难，因为它需要大量的定制和本地化需求，AI Agent的应用能力提供了一个解决方案：</p><p>&nbsp;</p><p>简洁的界面、任何需求都可以通过对话来理解，并通过行动系统对接外部系统逻辑，这样就将界面定制化和流程重构的职责就交给了后端模型和AI Agent系统。这样，从交互层到模型层，再到持久层，整个过程都得到了简化。因此，SaaS的维护成本也就降低了。</p><p>&nbsp;</p><p>“AI助理的盈利天花板目前还看不到，随着更多优秀产品的出现，我们可能还会发现新的盈利途径，带来新的惊喜。”不穷说道。</p><p>&nbsp;</p><p>不穷认为，通过消耗算力来提供服务只是最基本的模式，除此之外，服务消耗还有很多其他的可能性。他的判断是，未来十几年中国 SaaS 行业挣不到的钱可能会通过 AI Agent 来实现。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>在提到当前钉钉AI助理接下来要重点攻关的方向时，不穷还是说到了数据和场景：</p><p>&nbsp;</p><p>数据和场景是Agent普遍存在的问题，钉钉的AI助理现在有更专注的场景和数据，就像是给“孙悟空戴上了紧箍咒”，好处是能够减少幻觉、能够解决一些通用场景里难以解决的问题。这也意味着，钉钉未来还需要发掘和洞察到更多的场景、沉淀和积累更多的高质量数据。</p><p>&nbsp;</p><p>其次，行动能力是目前Agent所欠缺的，只是让它们聊天未免太乏味。因此AI助理会接入钉钉上原有的应用、低代码等开放能力，不穷希望以此让AI助理能够不断出现各种创新玩法，而不仅仅是简单的信息查询和单向交互。比如，AI助理对接了很多的主流App行动系统，比如可以查看淘宝订单等，App的行动系统实际上就变成了一个AI助理，无需在不同系统间切换。</p><p>&nbsp;</p><p>在不穷看来，Agent 的最大好处就是它的无限可能性，这种模式不受传统思维和现有框架的限制，是真的可以让想象力转化为生产力的。</p><p>&nbsp;</p><p>“我今年非常期待行动系统能够变得更加强大、数据质量得到提升。随着越来越多的人洞察到新的场景，AI助理将不再是一个个废话大师、一个个应对亲戚的聊天工具、一个个面试官。”不穷说道。</p><p>&nbsp;</p><p>访谈里，不穷不掩对微软战略眼光的称赞。“战略需要耐心，如果没有耐心，那就只是投机。”同样地，钉钉对AI 助理的耐心有多久？AI 助理未来的价值能有多大？这些也是不穷现在要面对的课题。</p><p>&nbsp;</p><p></p><h4>栏目介绍</h4><p></p><p>&nbsp;</p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p>&nbsp;</p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）&nbsp;</p><p></p><h4>活动推荐</h4><p></p><p>&nbsp;</p><p>除了叶军（不穷）的思考，钉钉 CTO 程操红（巴布）也将在明天（4月11日）开幕的<a href="https://qcon.infoq.cn/2024/beijing/?utm_source=wechat&amp;utm_medium=aipart1-0410">QCon 全球软件开发大会暨智能软件开发大会</a>"上分享《钉钉智能化之路：打造未来交互新形态，重塑组织效能》的主题演讲，分享大型平台智能化实践，探究 AI 产品到平台如何实现跨越。</p><p>&nbsp;&nbsp;</p><p>本次会议还邀请了微软（中国）公司首席技术官韦青，深圳开鸿数字产业发展有限公司高级副总裁、研发体系总裁王皓博士，Coupang 副总裁郭东白等专家带来精彩主题演讲。在圆桌论坛环节，ProtonBase 研究员蒋晓伟、 AutoMQ 联合创始人 &amp; 首席战略官章文嵩、 阿里云数据库产品事业部负责人李飞飞、蚂蚁集团 AI 安全商业化总经理张凯将带来关于「大模型时代的数据智能新趋势 」主题的前瞻视角。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/G5HpCLUNOGlB8fcqf48W</id>
            <title>共探AI+软件工程融合之道，AI4SE创新巡航活动启动征集</title>
            <link>https://www.infoq.cn/article/G5HpCLUNOGlB8fcqf48W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/G5HpCLUNOGlB8fcqf48W</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Apr 2024 03:07:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI技术, 软件工程, AI4SE
<br>
<br>
总结: 随着大模型等AI技术在软件工程领域的应用不断拓展，企业在智能化软件工程落地过程中面临模型选择、工具需求和落地路径等挑战。为推动AI4SE行业生态建设，中国人工智能产业发展联盟AI4SE工作组等机构发起“AI4SE创新巡航”活动，旨在探讨如何应用大模型等AI技术推动软件工程智能化发展。 </div>
                        <hr>
                    
                    <p>随着大模型落地元年的到来，大模型等AI技术在软件工程领域的赋能将从深度和广度不断拓展。然而在智能化软件工程（AI4SE）落地应用过程中，企业面临模型选择难、工具功能需求不清晰、落地路径和场景不明确等困扰。</p><p>&nbsp;</p><p>为推动AI4SE行业生态建设，共同探讨如何应用大模型等AI技术，推动软件工程朝着智能化、高效率、低成本方向发展，中国人工智能产业发展联盟AI4SE工作组、中国信息通信研究院（以下简称“中国信通院”）、InfoQ极客传媒已于2023年底共同发起“AI4SE创新巡航”系列活动，并于2024年1月完成首期走进360的巡航。现启动征集“AI4SE创新巡航”第二期的标杆企业和研讨主题。</p><p></p><h4>活动目标</h4><p></p><p></p><p>旨在通过深入走访企业，交流探讨落地实践经验，增进各行业在智能化软件工程领域的交流与合作，共同构建协同发展的强大合力。</p><p></p><h4>征集时间</h4><p></p><p></p><p>即日起至2024年4月10日。</p><p></p><h4>活动预告</h4><p></p><p></p><p>活动第二站将于2024年4月举行，敬请期待。</p><p>&nbsp;</p><p>挚邀请各领域领军企业踊跃报名加入AI4SE工作组，共探共享行业优秀成果和丰富经验，携手推进智能化软件工程新发展。</p><p></p><h4>报名请联系</h4><p></p><p></p><p>中国信通院人工智能研究中心</p><p>&nbsp;</p><p>胡老师：17371328072（微信同号）</p><p>秦老师：13488684897（微信同号）</p><p>&nbsp;</p><p>中国人工智能产业发展联盟（AIIA）智能化软件工程工作组（AI4SE工作组），于2023年9月正式成立，旨在进一步发挥生成式AI、大模型等人工智能技术在软件工程领域的潜力，充分释放AI赋予软件工程的价值。AI4SE工作组已吸纳110+成员单位，覆盖金融、电信、软件等诸多行业，欢迎更多企业加入，请联系胡老师、秦老师。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1140e58ac53abb164dfdbe91b4069ca.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/r1WvBQGA9GXAEcWimIb0</id>
            <title>谷歌、英伟达联手打造AI超级计算机架构，Agent业态初显且已商业化，谷歌的基础设施太全面了 | Google Next 2024</title>
            <link>https://www.infoq.cn/article/r1WvBQGA9GXAEcWimIb0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/r1WvBQGA9GXAEcWimIb0</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 19:00:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 基础软件, TPU v5p, 生成式AI, 企业级场景
<br>
<br>
总结: 谷歌在Cloud Next ’24大会上展示了基础软件和TPU v5p支持下的AI超级计算机架构，能够满足生成式AI大语言模型和企业级场景的需求。与国内产品相比，谷歌的用例精细化能力相当高。同时，谷歌还宣布了AI Hypercomputer超级计算机架构，为生成式AI提供端到端的基础设施服务。 </div>
                        <hr>
                    
                    <p></p><blockquote>省流版：基础软件相当能打，新的TPU v5p支持下的AI超级计算机架构足以应对最严苛的生成式AI大语言模型和场景。Agent已经商业化，单就用例本身（视频智能生成、智能办公等等）国内均有类似形态的产品，没有“WoW”的感觉，但与国内同样产品相比，谷歌的用例精细化能力相当高，足以应对企业级场景的需求。</blockquote><p></p><p>&nbsp;</p><p>美国时间4月9日，Google Cloud Next ’24在拉斯维加斯正式召开。&nbsp;Google Cloud CEO Thomas Kurian 等带来了主题为「即刻踏上云端新旅程」的开幕演讲。</p><p><img src="https://static001.infoq.cn/resource/image/cd/56/cd820250fe5131393589f69f0b96da56.jpeg" /></p><p>在会议开始之前，媒体及分析师们均十分期待谷歌本次大会上与生成式AI相关的发布，包括今年引起巨大讨论的Gemini系列，以及vertexAI、硬件层面的更新等，包括是否会与英伟达最新发布的芯片合作等。</p><p></p><p>事实证明，谷歌已经围绕生成式AI构建起一整套的成熟架构，这是属于AI时代的超级计算架构。Thomas Kurian 表示自上次Cloud Next大会以来，谷歌已经进行了一千多次的产品更新，并声称这是世界上增长最快的云提供商。Alphabet首席执行官Sundar Pichai&nbsp;特别赞扬了Gemini系列为各地企业提供了很多发展机会。与此同时，本届大会展示了谷歌已经构筑起的生成式AI全景图。</p><p><img src="https://static001.infoq.cn/resource/image/db/e7/db321a2587356789d1d22d1166839de7.jpg" /></p><p></p><h2>基础设施看谷歌，联手英伟达，AI&nbsp;Hypercomputer超级能打</h2><p></p><p></p><h3>一系列芯片更新...</h3><p></p><p>为了支持当今企业中采用的日益强大的生成式人工智能模型，谷歌宣布全面推出迄今为止最强大、可扩展的张量处理单元-&nbsp;TPU v5p，其设计目的只有一个——训练和运行最苛刻的生成式AI模型。</p><p>&nbsp;</p><p>TPU v5p旨在提供巨大的计算能力，单个Pod包含8,960个协同运行的芯片，这是TPU v4 Pod数量的两倍多。谷歌方面表示，TPU v5p提供了令人印象深刻的性能提升，每秒浮点运算次数增加了一倍，每芯片的高带宽内存增加了三倍，从而大大提高了整体吞吐量。为了使客户能够训练和服务在大规模TPU集群上运行的AI模型，谷歌在Google Kubernetes Engine上添加了对TPU v5p虚拟机的支持，Google Kubernetes Engine是其用于运行软件容器的云托管服务。</p><p><img src="https://static001.infoq.cn/resource/image/df/20/df3d9yy9686e36b8f35ec4yy26722b20.jpg" /></p><p>如预期，谷歌与英伟达联手，用户可以使用英伟达的最新硬件在Google Cloud上训练其生成式AI模型。除了TPU系列之外，它还通过新的A3系列虚拟机提供对Nvidia H100 GPU的访问。A3 Mega VM将从下个月开始全面上市，其主要优势之一是支持“机密计算”，这是指即使在处理最敏感的数据时也可以保护其免受未经授权的访问的技术。</p><p>&nbsp;</p><p>这是一个极其关键的发展，这为生成式AI模型提供了一种方法来访问以前被认为处理风险太大的数据，而数据安全随着生成式AI的爆火而愈发重要。</p><p>&nbsp;</p><p>“Character.AI正在使用Google Cloud的Tensor处理器单元和在Nvidia H100 Tensor Core GPU上运行的A3 VM来更快、更高效地训练和推断LLM，”Character Technologies Inc.首席执行官Noam Shazeer表示。“在强大的人工智能优先基础设施上运行的GPU和TPU的可选性使Google Cloud成为我们显而易见的选择，因为我们需要扩展规模，为数百万用户提供新的特性和功能。”</p><p><img src="https://static001.infoq.cn/resource/image/aa/77/aa908385f6e03a761640c0e9ed92f577.jpg" /></p><p>与此同时，谷歌官宣了其宏伟计划-AI Hypercomputer，面向生成式AI时代的超级计算机架构，提供端到端的基础设施，从硬件到软件的一切服务，并宣布了Google Axion 处理器，这是谷歌首款专为数据中心设计的基于 Arm 的定制 CPU。Axion 提供业界领先的性能和能源效率，并将于今年晚些时候向 Google Cloud 客户提供。</p><p><img src="https://static001.infoq.cn/resource/image/e8/aa/e82e2fef87fbfb679c49f840aeed6faa.jpg" /></p><p>更令人兴奋的是谷歌将在今年晚些时候推出的产品。尽管没有透露具体时间，但谷歌方面确认计划将英伟达最近宣布但尚未发布的<a href="https://siliconangle.com/2024/03/18/nvidias-blackwell-architecture-power-new-generation-1-trillion-parameter-generative-ai-models/">Blackwell GPU</a>" 引入其AI超级计算机架构。谷歌方面表示，&nbsp;Blackwell GPU将提供两种配置，虚拟机由HGX B200和GB200 NVL72 GPU提供支持。前者是为最苛刻的人工智能工作负载而设计的，而后者则有望支持实时大语言模型推理和万亿参数规模模型的大规模训练的新时代。</p><p></p><h3>Gemini 1.5 Pro正式发布公共预览版</h3><p></p><p>Gemini 1.5 Pro的预览版发布只能说中规中矩，毕竟该系列在今年2月份就已经面世，这个模型最大的特点就是创下了最长上下文窗口的纪录。</p><p>&nbsp;</p><p>根据官方披露，Gemini 1.5 Pro将上下文窗口容量提到了100万token（极限为1000万token），远远超出了Gemini 1.0最初的32000个token，此前的SOTA模型也才将上下文窗口容量提高到了20万token。</p><p>&nbsp;</p><p>这意味着Gemini 1.5 Pro可以自如地处理22小时的录音、超过十倍的完整的1440页的书（587,287字）《战争与和平》，以及四万多行代码、三小时的视频。</p><p>&nbsp;</p><p>凭借超长上下文理解能力，Gemini 1.5 Pro得到了很多用户的认可。很多测试过Gemini 1.5 Pro的人更是直言，这个模型被低估了。如今预览版正式推出，期待后续广大开发者的反馈。</p><p>&nbsp;</p><p>Kurian现场介绍了众多可能的用例，并强调通过系列新增功能，谷歌云仍然是唯一提供广泛使用的第一方（Gemini系列）和第三方模型（主要指vertexAI上面的模型）服务的云服务商。</p><p></p><h2>Google Search升级</h2><p></p><p>通过使用新的提示管理工具对模型进行更精细的调整，包括解释为什么某些提示比其他提示效果更好等，可以进一步提高搜索结果的质量，显著降低产生幻觉的可能性。</p><p>&nbsp;</p><p>这些现均已在Vertex AI上提供，Vertex AI是Google Cloud的平台，用于定制和全面管理各种领先的人工智能模型。如今，超过100万开发人员正在使用谷歌的生成式AI工具，包括AI Studio和Vertex AI。此外，通过Vertex AI，客户现在可以通过两种新方式增强和基础他们的模型——将模型输出连接到可验证的信息源。第一个是Google Search，它提供高质量的信息以提高响应的准确性。第二个是用户自己的数据和事实来源，例如Workday或Salesforce等企业应用程序以及BigQuery等Google Cloud数据库。</p><p>&nbsp;</p><p>生成式AI时代，基础设施是一切创新的基础，极其重要，而单看这一部分，谷歌还是相当全面，超级能打。</p><p></p><h2>Agent、Agent、Agent，已经商业化</h2><p></p><p>会议现场，谷歌公布了系列Agent用例，比如Custom Agent、Code Agent。可能是国内应用市场的繁荣导致平常“吃得太好”，这部分用例并没有让笔者有“Wow”的感觉，但单从Demo来看，谷歌公布出来的场景效果确实足够精细，这种精细指的是“足以在企业场景中落地”，极其重要的是谷歌已经将这些商业化了，每一个用例都跟着一个客户故事。</p><p><img src="https://static001.infoq.cn/resource/image/bf/a0/bf23907afc11ef917b59f8c0b47457a0.jpg" /></p><p></p><p>为了有效创建下一代Agent，谷歌宣布了新的Vertex AI&nbsp;Agent Builder，其提供了一个更简单的流程来训练、编辑和启动相关工具，包括相关控制能力和基础响应。</p><p>&nbsp;</p><p>今年初，国内外对Agent寄予厚望，这被认为是生成式AI最有可能变现的一条路。如今看来，谷歌已经实现了，奔驰、沃尔玛等企业均在利用谷歌提供的Agent，而主会场的后半程几乎都被Agent所占据。</p><p>&nbsp;</p><p>由于这部分用例较多，且大部分国内感知有限（比如Google Workspace），就不一一列举，着重聊下Data Agents和Code Agents。</p><p><img src="https://static001.infoq.cn/resource/image/06/d8/06804259326026de399b64be0865bfd8.jpg" /></p><p>视频生成、剪辑相关用例</p><p>&nbsp;</p><p>Data Agents类似的产品形态，国内其实也有一些简单的尝试，通过在数据库或者数据平台上添加一些基本功能，让用户可以通过自然语言的方式与“业务数据”做交互。谷歌基于BigQuery做了Data Agents方面的尝试，沃尔玛执行副总裁Suresh Kumar&nbsp;也通过视频表示已经利用该功能来创造新的见解和个性化体验。</p><p><img src="https://static001.infoq.cn/resource/image/11/2b/118236cc7df4d0bd994a738cc8b1722b.jpg" /></p><p></p><p>Code Agent也显示出了人工智能将为代码编写带来巨大变化。Gemini Code Assist获得了强大关注，谷歌现在正在将其扩展到Gitlab、Github、本地和外部。谷歌表示，Gemini 1.5 Pro即将加入Code Assist，为编码带来100万tokens，“彻底改变”编码。Gemini Cloud Assist还可跨应用程序框架工作，让业务更顺畅、更轻松地推出和扩展。</p><p>&nbsp;</p><p>国内外的智能编码工具不少，大部分目前是集成在IDE中使用，Agent这种形态确实比较领先。早前，笔者从国内头部编码工具厂商那里了解到，其也将在今年下半年推出Agent版本，或许是国内目前来看比较有希望的节点。</p><p></p><p>面向生成式AI时代，谷歌的每一层架构都做好了准备。Kurian提到：“我们正处于行业的关键时刻，我们正在重塑基础设施以支持人工智能新时代”。</p><p>&nbsp;</p><p>“我们正在共同构建一种新的云方式。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f1/4a/f11699904d94f72925247775064dbe4a.png" /></p><p></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</id>
            <title>检索增强生成引擎 RAGFlow 正式开源！仅一天收获上千颗星</title>
            <link>https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hjJM3kV620iDoYYOBtPs</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 10:26:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RAG 解决方案, AI 原生数据库, 长上下文 LLM, RAG 引擎
<br>
<br>
总结: 4 月 1 日，我们正式宣布端到端 RAG 解决方案 RAGFlow 开源。在此之前，我们还开源了 AI 原生数据库 Infinity。Infinity 项目在 GitHub 上仅三个月时间就获得了 1400 颗星，而 RAGFlow 在开源首日就获得了上千颗星。我们开源这两个项目的初衷是希望能够从更多更广泛的应用场景中收到反馈，以便尽快让 RAG 走出当前的初期阶段。随着长上下文 LLM 的不断普及，我们希望这一天能够尽快到来。 </div>
                        <hr>
                    
                    <p>4 月 1 日，我们正式宣布端到端 RAG 解决方案 RAGFlow 开源。在此之前，我们还开源了 AI 原生数据库 Infinity。Infinity 项目在 GitHub 上仅三个月时间就获得了 1400 颗星，而 RAGFlow 在开源首日就获得了上千颗星。我们开源这两个项目的初衷是希望能够从更多更广泛的应用场景中收到反馈，以便尽快让 RAG 走出当前的初期阶段。随着长上下文 LLM 的不断普及，我们希望这一天能够尽快到来。</p><p></p><p>项目开源地址：</p><p>Infinity : https://github.com/infiniflow/infinity</p><p>RAGFlow：https://github.com/infiniflow/ragflow</p><p>RAGFlow 在线 Demo：https://demo.ragflow.io/</p><p></p><p>在回答 RAGFlow 有哪些特点之前，我们先来谈谈为何要做这样一款 RAG 引擎。</p><p></p><p>今年 2 月以来， AI 领域连续出了很多重磅热点，除了最火热的 Sora 之外，另一个热点就是长上下文 LLM ，例如 Claude 3、 Gemini 1.5，当然也包含国产的月之暗面。Sora 的本质是针对视频具备更加可控性的生成能力，这其实是解锁未来多模态 RAG 热潮的一个必要条件；而长上下文 LLM ，却引发了更多针对 RAG 的争论，因为这些 LLM，可以很方便的让用户随时上传 PDF，甚至上传几十个 PDF，然后针对这些 PDF 回答问题，并且还具备强大的“大海捞针”能力。所谓“大海捞针”，意思就是针对这些长上下文窗口的细节提问，看 LLM 是否可以准确地回答。</p><p></p><p>用 RAG 来实现大海捞针是轻而易举的，然而目前列举的这些 LLM，它们不是基于 RAG 来提供这种能力，却也都可以达到很高的召回，同时它们也不是采用类似 StreamLLM 这种基于滑动窗口实现长上下文注意力的机制——这种机制仅仅是增加了上下文窗口，但却仍然在细节召回上表现不佳，窗口滑过，内容即会被逐渐“遗忘”。我们也试验了其中的若干产品，效果确实非常好，上传一个 PDF，甚至可以针对里边的复杂图表给出精确的回答。</p><p></p><p>因此，这引发了新的一轮关于长上下文 LLM 和 RAG 的争论，许多人评价 “RAG 已死”，而 RAG 拥护者则认为，长上下文 LLM 并不能满足用户海量数据的需求，成本高，速度也不够快，也只能针对长文本、图片等数据提问。</p><p></p><p>随着长上下文为更多用户接纳，近期各家国产 LLM 都快速推出了这个产品特性，除月之暗面外，其他家大多基于 RAG 来实现，下表是两者的基本对比：</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/847a76363897b80afb0019624bce6896.webp" /></p><p></p><p>这里要额外说明一下，为何 RAG 派的大海捞针能力一般，这并不是 RAG 本身的问题，而是依靠纯向量数据库去构建 RAG，并不能保证对精确数据和细节的准确召回。</p><p></p><p>以上的对比，其实并没有完全解答 RAG 的必要性，因为至少就目前 RAG 最普遍的场景——个人知识库问答而言，确实很多情况下只需要 LLM 就足够了。而我们则认为，LLM 的长上下文能力，对于 RAG 来说是个很大的促进。这里先用 OpenAI 联创 Andrej Karpathy 的一张图做个类比，他把 LLM 比喻为一台计算机的 CPU， 把上下文类比为计算机的内存，那么以向量为代表的数据库，就可以看作是这台计算机的硬盘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f1bcd9b9688da15d1a5a9f72e032d06b.webp" /></p><p></p><p>我们进一步来说明，为什么即使有了“大海捞针”能力，RAG 仍然必不可少。RAG 从提出到为业界广泛接纳，经历了一年多时间，当下的 RAG 产品已经并不稀缺，然而在实际应用中，却普遍得出了“ RAG 属于上手容易，但真正落地却很难”的结论。究其原因，这里边主要包含两个方面：</p><p></p><p>其一是来自 LLM 自身。由于 RAG 的工作流程是针对从数据库返回的结果进行回答，这样的话，对于 RAG 来说，LLM 最基础也是最重要的能力其实包含：</p><p>摘要能力；可控性：既 LLM 是否听话，是否会不按照提示要求的内容自由发挥产生幻觉；翻译能力，这对于跨语言 RAG 是必备的。</p><p></p><p>遗憾的是，在过去，国内可以用到的 LLM 中，在这 3 点上表现良好的并不多。至于所谓高级的能力，例如逻辑推理，以及各类 Agent 要求的自主决策能力等，这些都是建构在以上基础能力之上，基础不好，这些也都是空中楼阁。</p><p></p><p>其二，则是来自于 RAG 系统本身。我们所说的 RAG，实际上包含完整的链路，包括数据的准备，数据写入，乃至从数据库查询和返回结果排序。在整条链路中，最大的难点来自于两方面：一是如何应对复杂多变的数据，这些数据包含各种格式，更复杂的还包含各类图表等，如果在没有理解这些语义的基础之上直接提供 RAG 方案，就会导致语义丢失从而让 RAG 失败。二是如何查询和排序：简单地讲，在大多数情况下，都必须引入多路召回和重排序，才能保证数据查询的准确度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0abff5fc98553f8a297e257a55a6ec57.webp" /></p><p></p><p>假如我们不去专注于解决这两类问题，那么就很容易陷入让 RAG 去和长上下文 LLM 反复对比的情况，因为两者其实都可用于简易知识库对话场景：RAG 仅仅提供数据的简单解析，然后直接转化为向量，最后用单一向量做召回，这除了成本，以及私有化场景里所要求的安全等优势之外，在核心对话能力上并没有显著地跟长上下文 LLM 区分开来，甚至还有所不及。</p><p></p><p>正是基于这些 RAG 本身的痛点，我们先后推出了 2 个开源项目：</p><p></p><p>第一个是 AI 原生数据库 Infinity。它解决的是如何解锁 RAG 服务 B 端场景下遇到的典型问题：如何跟企业已有的数据——包括但不限于非结构化的文档、图片，还包括结构化的信息系统来结合，并解决多路召回和最终融合排序的问题。</p><p></p><p>举几个典型场景：把符合要求的简历筛出，筛选条件包含工作技能（需要向量 + 全文搜索），某类行业的工作经验（基于向量的分组聚合），期望收入，学历，地域（结构化数据）等；基于对话推荐符合个人要求的产品，可以采用多列向量来描述个人偏好，不同的列代表了用户对不同类目产品的过往使用偏好。在推荐过程中，除了采用基于用户的偏好向量进行搜索之外，还需要结合产品的过滤条件：包括是否过期，是否有优惠券，是否符合权限要求，是否有合规要求，该用户是否近期已经购买或者阅读过，等等。</p><p></p><p>这些信息，如果仅仅拿所谓“标量”字段这种方式来表征，那么产品的开发是极其复杂的：因为这需要引入额外的 ETL ，带来了维护性，以及更严重的数据一致性的问题。要知道，RAG 面临的是最终用户使用场景，它是需要业务乃至 LLM 发起请求，就立刻得到答案的，因此不能像数据中台一样仅仅为了一张报表就可以搭建一整套数据管道体系去做宽表这种额外逻辑。因此，Infinity 实际上等于向量数据库 + 搜索引擎 + 普通结构化数据查询，并保证三者的高并发和融合排序。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2ef863f1ac05e0bc35140e4f4307ea18.webp" /></p><p></p><p>第二个就是端到端的 RAG 引擎 RAGFlow。它解决数据的问题：因为如果不对用户数据加以区分和清晰，识别其中的语义，就容易导致 Garbage In Garbage Out。RAGFlow 包含了如下的完整 RAG 流程，确保数据从 Garbage In Garbage Out 变为 Quality In Quality Out。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff18442fabfb2e8a38f3cb937c5b4ff7.webp" /></p><p></p><p>具体来说， RAGFlow 的最大特色，就是多样化的文档智能处理，因此它没有采用现成的 RAG 中间件，而是完全重新研发了一套智能文档理解系统，并以此为依托构建 RAG 任务编排体系。这个系统的特点包含：</p><p></p><p>1. 它是一套基于 AI 模型的智能文档处理系统：对于用户上传的文档，它需要自动识别文档的布局，包括标题、段落、换行等，还包含难度很大的图片和表格。对于表格来说，不仅仅要识别出文档中存在表格，还会针对表格的布局做进一步识别，包括内部每一个单元格，多行文字是否需要合并成一个单元格等。并且表格的内容还会结合表头信息处理，确保以合适的形式送到数据库，从而完成 RAG 针对这些细节数字的“大海捞针”。</p><p></p><p>2. 它是一套包含各种不同模板的智能文档处理系统：不同行业不同岗位所用到的文档不同，行文格式不同，对文档查阅的需求也不同。比如：</p><p>会计一般最常接触到的凭证、发票、Excel 报表；查询的一般都是数字，如：看一下上月十五号发生哪些凭证，总额多少？上季度资产负债表里面净资产总额多少？合同台账中下个月有哪些应付应收？作为一个 HR 平时接触最庞杂的便是候选人简历，且查询最多的是列表查询，如：人才库中 985/211 的 3 到 5 年的算法工程师有哪些？985 硕士以上学历的人员有哪些？赵玉田的微信号多少？香秀哪个学校的来着？作为科研工作者接触到最多的可能是就是论文了，快速阅读和理解论文，梳理论文和引文之间的关系成了他们的痛点。</p><p></p><p>这样看来凭证 / 报表、简历、论文的文档结构是不一样的，查询需求也是不一样的，那处理方式肯定是不一样。因此 RAGFlow 在处理文档时，给了不少的选择：Q&amp;A，Resume，Paper，Manual，Table，Book，Law，通用... 。当然，这些分类还在不断继续扩展中，处理过程还有待完善。我们也会抽象出更多共通的东西，使各种定制化的处理更加容易。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f0ec9574414aeae20c2290390cadd39.gif" /></p><p></p><p>3. 智能文档处理的可视化和可解释性：用户上传的文档到底被处理成啥样了，如：分割了多少片，各种图表处理成啥样了，毕竟任何基于 AI 的系统只能保证大概率正确，作为系统有必要给出这样的空间让用户进行适当的干预，作为用户也有把控的需求，黑箱不敌白箱。特别是对于 PDF，行文多种多样，变化多端，而且广泛流行于各行各业，对于它的把控尤为重要，RAGFlow 不仅给出了处理结果，而且可以让用户查看文档解析结果并一次点击定位到原文，对比和原文的差异，可增可减可改可查，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2c2a6b8ae16839f447d7c0863f1f91a.gif" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d0ff9727d9daa9ee95b59379027c09c3.webp" /></p><p></p><p>4. RAGFlow 是一个完整的 RAG 系统，而目前开源的 RAG，大都忽视了 RAG 本身的最大优势之一：可以让 LLM 以可控的方式回答问题，或者换种说法：有理有据、消除幻觉。我们都知道，随着模型能力的不同，LLM 多少都会有概率会出现幻觉，在这种情况下， 一款 RAG 产品应该随时随地给用户以参考，让用户随时查看 LLM 是基于哪些原文来生成答案的，这需要同时生成原文的引用链接，并允许用户的鼠标 hover 上去即可调出原文的内容，甚至包含图表。如果还不能确定，再点一下便能定位到原文，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c4161919c62326262b3c9cb80837804.gif" /></p><p></p><p>接下来，我们来讲讲，RAGFlow 具体是如何利用文档结构识别模型来处理数据的。所谓文档结构模型，如下所示，是针对文档的布局进行目标识别，然后根据布局再做文字切分。这些布局识别的目标包括文档的标题，段落，语义文字块等等，尤其还会包含文档当中的图表。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1a1650e225536587cb4decc9d5c0e5c.webp" /></p><p></p><p>在识别出这些目标之后，还需要分别对这些目标做相应处理：对于文字来说，需要首先判断文字的换行信息——这对于文字的语义理解也会产生干扰；其次需要对文字内容进行一些整理，这些整理会随着 RAGFlow 模板的不同有所区分；针对表格来说，还需要进一步识别它的内部结构，这在 AI 领域有个专门的研究课题，叫做 TSR(Table Structure Recognition 表格结构识别) 。</p><p></p><p>TSR 任务其实相对比较复杂，因为表格的定义是多种多样的，表格内部可能会出现有线条或者没有线条的情况，对于不同行的文字，判断它们是否是一个单元格是存在很大挑战的，单元格判断失误，很可能就会让表格的数字跟表格列的对应关系弄错，从而影响了对单元格内文字和数字语义的理解。我们花了很多时间来提升 TSR 的能力，最早是利用现成的 OCR 开源模型，后边也尝试过微软研究院专门针对 TSR 任务的 Transformer 模型，但是发觉这些模型处理 TSR 任务的鲁棒性依然非常不足，最后我们还是训练了自己的模型，从而让 TSR 任务表现良好。这个模型比较简单，就是基于 CNN 的目标检测模型，但是它的效果却比上边我们提到的其他模型都要好。为了降低对硬件的依赖和开销，我们甚至切换到用 YOLOv8 来做目标检测，使得仅仅利用 CPU 也可以运行文档结构识别。</p><p></p><p>关于这些，其实也有很多业内人士建议直接走 LLM 的路子，用 LLM 来做文档语义理解，从长期来看这肯定是个趋势，然而在当下来说，让 LLM 在文档结构识别上表现良好，还需要大量的数据才可以。这从我们放弃了基于 Transformer 的 TSR 模型就可以看出：同样的任务下，基于 Transformer 的模型需要更多的数据才可以表现更好，在有限数据下，我们不得不退回到传统 CNN 模型，如果是 LLM ，它需要的数据和算力更多——我们之前曾经尝试过基于多模态 LLM 进行识别的努力，相比专用小模型，它的效果还是差别比较大。从另一个方面也可以看出来，下图是我们用长上下文 LLM 对表格输出的例子：</p><p></p><p>这是原表格：</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13fa0a319e1849790a8590b9e28dba38.webp" /></p><p></p><p>这是识别后的结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/1383bc9362f336615159c4a31d08ca04.webp" /></p><p></p><p>解锁对于非结构化数据的深度语义理解是 RAGFlow 追求的目标之一，我们希望在未来能够将更加 scalable 的文档结构识别模型应用到系统中。不仅如此， RAGFlow 的设计目标是让 RAG 逐渐承接起更多的复杂场景尤其是 B 端场景，因此在未来，它会接入企业的各类数据源，比如 MySQL 的 binlog，数据湖的 ETL，乃至外部的爬虫等。只有这些都被纳入 RAG 的范畴，我们才能实现如下的愿景：</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af53c99ac42cfecb2c8b052689f3ecca.webp" /></p><p></p><p>再回头看前边关于 RAG 和长上下文 LLM 的争论， 显然两者一定是合作的。长上下文 LLM 当下已经逐步具备了 RAG 最不可或缺的基础能力，随着它自身逻辑推理能力地增强，再结合来自数据库，还有数据方面的改进，一定能加速 LLM 的 B 端场景走出婴儿期的进程。</p><p></p><p>RAGFlow 近期更新：将提供类似文件管理的功能，这样 RAG 可以跟企业内部文档以更灵活的方式整合。RAGFlow 中期更新，将提供面向企业级数据接入的低代码平台，同时提供问答对话之外的高级内容生成，比如长文生成等等。</p><p></p><p>Infinity 近期更新：Infinity 近期将发布第一个 release，届时将提供业界最快的多路召回和融合排序能力。</p><p></p><p>欢迎大家关注我们的开源社区，并提出反馈意见！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</id>
            <title>硅谷创业一年，贾扬清讲了自己的AI行业观察：成本、市场增量和商业模式</title>
            <link>https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SWGj2PKN1oaXAODLaec8</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 10:03:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 贾扬清, AI Infra, 大模型, 英伟达
<br>
<br>
总结: 本文介绍了AI科学家贾扬清选择创业方向为AI基础设施，他创立并开源了深度学习框架Caffe，离开阿里后专注于AI Infra的发展。文章还提到了大模型的商业模式和硬件提供商的发展趋势，以及AI计算与云计算的不同关注点。贾扬清在分享中详细分析了AI时代的基础设施和大模型的应用，强调了中小型模型结合自有数据的优势。 </div>
                        <hr>
                    
                    <p>本文经授权转载于腾讯科技，原文链接：https://mp.weixin.qq.com/s/kpFnXZbROMCdJ5WikuOJvw</p><p></p><p>编辑 /&nbsp;腾讯科技 郭晓静</p><p></p><p>创业一年的贾扬清，选择的方向是AI Infra。</p><p></p><p>贾扬清是最受关注的全球AI科学家之一，博士期间就创立并开源了著名的深度学习框架Caffe，被微软、雅虎、英伟达等公司采用。</p><p></p><p>2023年3月，他从阿里离职创业，并在随后录制的播客中说，自己并非是因为ChatGPT 火爆而创业，后来创业项目浮出水面，也确实证实，他没有直接入局大模型。硅谷著名风投a16z在去年发表的一篇关于AIGC的文章中就曾经提到过：“目前来看，基础设施提供商是这个市场上最大的赢家。”</p><p></p><p>贾扬清在去年的文章中也提到，“不过要做这个赢家，就要更聪明地设计Infra才行”。在他创办的公司Lepton.AI的官网上，有一句醒目的Slogan“Build AI The Simple Way（以简单的方式构建AI）”。</p><p></p><p>最近，贾扬清在高山书院硅谷站“高山夜话”活动中，给到访的中国企业家做了一次深度的闭门分享，分享的内容直击行业痛点，首先从他最专业的AI Infra开始，详细分析了AI时代的Infra，到底有什么新的特点；然后，基于AI大模型的特点，帮助企业算了一笔比较详细的经济账——在不可能三角成本、效率、效果中，如何选才能达到比较好的平衡点。</p><p></p><p>最后也讨论到AI整个产业链的增量机会及目前大模型商业模式的纠结点：</p><p></p><p>“每次训练一个基础大模型，都要从零开始。形象一点来描述，这次训练‘投进去10个亿，下次还要再追加投10个亿’，而模型迭代速度快，可以赚钱的窗口也许只有大概一年。所以每个人都在思考这个终极问题，‘大模型的商业模式到底怎样才能真正有效？’”</p><p></p><p>贾扬清的过往经验大部分是TOB的。他也多次在分享中很坦诚地表示，“TOC我看不太清楚，TOB看得更清晰一些。”</p><p></p><p>“AI从实验室或者说从象牙塔出来到应用的过程中，该蹚过的雷，都会经历一遍。”无论大语言模型给人们多少惊艳，它的发展都不是空中楼阁，既往的经验和范式有变也有不变。</p><p></p><p>为了方便阅读，我们在文首提炼几个主要观点，但强烈建议完整阅读，以了解贾扬清完整的思考逻辑：</p><p></p><p></p><p></p><blockquote>一个通用的大模型的效果固然非常好，但是在企业实际应用当中，中小型模型加上自己的数据，可能反而能够达到一个更好的性价比。至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。我个人认为，英伟达在接下来的3~5年当中，还会是整个AI硬件提供商中绝对的领头羊，我认为它的市场发展占有率不会低于80%。但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：一个是提效，另外一个是娱乐。大量的传统行业应用，其实是AI行业里值得探究的深水区。我个人关于Supper App的观点可能稍微保守一些，也有可能是因为我自己的经历很多都在做TOB的服务，我认为Super APP会有，但是会很少。</blockquote><p></p><p></p><p>一个通用的大模型的效果固然非常好，但是在企业实际应用当中，中小型模型加上自己的数据，可能反而能够达到一个更好的性价比。</p><p></p><p>至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。</p><p></p><p>我个人认为，英伟达在接下来的3~5年当中，还会是整个AI硬件提供商中绝对的领头羊，我认为它的市场发展占有率不会低于80%。但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。</p><p></p><p>目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：一个是提效，另外一个是娱乐。</p><p></p><p>大量的传统行业应用，其实是AI行业里值得探究的深水区。</p><p></p><p>我个人关于Supper App的观点可能稍微保守一些，也有可能是因为我自己的经历很多都在做TOB的服务，我认为Super APP会有，但是会很少。</p><p></p><p></p><p>以下为分享内容精华整理：</p><p></p><p>随着大型语言模型的兴起，出现了一个新概念——Scaling Law（规模定律）。根据Scaling Law，大语言模型的性能与其参数量、训练数据的大小和计算量呈幂律关系。简单来说，用通用的方法给模型巨大的数据，让模型能够拥有输出我们想要的结果的能力。</p><p></p><p>这就使得AI计算与“云计算”有很大的不同，云计算主要服务于互联网时代的需求，关注资源的池化和虚拟化：</p><p></p><p>●&nbsp;怎么把计算，存储，网络，从物理资源变成虚拟的概念，“批发转零售”；</p><p></p><p>●&nbsp;如何在这种虚拟环境下把利用率做上去，或者说超卖；</p><p></p><p>●&nbsp;怎么更加容易地部署软件，做复杂软件的免运维（比如说，容灾、高可用）等等，不一而足。</p><p></p><p>用比较通俗的语言来解释，互联网的主要需求是处理各种网页、图片、视频等，分发给用户，让“数据流转（Moving Data Around）起来。云服务关注数据处理的弹性，和便捷性。</p><p></p><p>但是AI计算更关注以下几点：</p><p></p><p>●&nbsp;并不要求特别强的虚拟化。一般训练会“独占”物理机，除了简单的例如建立虚拟网络并且转发包之外，并没有太强的虚拟化需求。</p><p>●&nbsp;需要很高性能和带宽的存储和网络。例如，网络经常需要几百 G 以上的 RDMA 带宽连接，而不是常见的云服务器几 G 到几十 G 的带宽。</p><p>●&nbsp;对于高可用并没有很强的要求，因为本身很多离线计算的任务，不涉及到容灾等问题。</p><p>●&nbsp;没有过度复杂的调度和机器级别的容灾。因为机器本身的故障率并不很高（否则 GPU 运维团队就该去看了），同时训练本身经常以分钟级别来做 checkpointing，在有故障的时候可以重启整个任务从前一个 checkpoint 恢复。</p><p></p><p>今天的AI计算 ，性能和规模是第一位的，传统云服务所涉及到的能力，是第二位的。</p><p></p><p>这其实很像传统高性能计算领域的需求，在七八十年代我们就已经拥有超级计算机，他们体积庞大，能够提供大量的计算能力，可以完成气象模拟等服务。</p><p></p><p>我们曾做过一个简单的估算：过去，训练一个典型的图像识别模型大约需要1 ExaFlop的计算能力。为了形象地描述这一计算量，可以想象全北京的所有人每秒钟进行一次加减乘除运算，即便如此，也需要几千年的时间才能完成一个模型的训练。</p><p></p><p>那么，如果单台GPU不足以满足需求，我们应该如何应对呢？答案是可以将多台GPU连接起来，构建一个类似于英伟达的Super POD。这种架构与最早的高性能计算机非常相似。</p><p></p><p>这时候，如果一台GPU不够怎么办？可以把一堆GPU连起来，做成一个类似于英伟达的Super POD，它和最早的高性能计算机长得很像。</p><p></p><p>这就意味着，我们又从“数据流转”的需求，回归到了“巨量运算”的需求，只是现在的“巨量运算”有两个进步，一是用于计算的GPU性能更高，另外就是软件更易用。伴随着AI的发展，这将是一个逐渐加速的过程。今年NVIDIA推出的新的DGX机柜，一个就是几乎1Exaflops per second，也就是说理论上一秒的算力就可以结束训练。</p><p></p><p>去年我和几位同事一起创办了Lepton AI。Lepton在物理中是“轻子”的意思。我们都有云计算行业的经验，认为目前AI的发展给“云”带来一个完全转型的机会。所以今天我想重点分享一下，在AI的时代，我们应该如何重新思考云的Infrastructure。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/49/498853e30a6c2505eec1dba0d5c31dba.webp" /></p><p></p><p></p><p>企业用大模型，先算一笔“经济账”</p><p></p><p></p><p>随着模型规模的不断扩大，我们面临着一个核心问题：大模型所需的计算资源成本高昂，从实际应用的角度出发，我们需要思考如何高效地利用这些模型。</p><p></p><p>以一个应用场景为例，我们可以比较形象地看出一个通用的大型语言模型与针对特定领域经过微调的模型之间的差异。</p><p></p><p>我们曾经尝试过“训练一个金融领域的对话机器人”。</p><p></p><p>使用通用模型，我们直接提问：“苹果公司最近的财报怎么样？你怎么看苹果公司在AI领域的投入。”通用大模型的回答是：“抱歉，我无法回答这个问题。”</p><p></p><p>针对特定领域微调，我们使用了一个7B的开源模型，让它针对性地“学习”北美所有上市公司的财报，然后问它同样的问题。它的回答是：“没问题，感谢您的提问。（Sure，thanks for the question）”口吻十分像一家上市公司的CFO。</p><p></p><p>这个例子其实可以比较明显地看出，通用大模型性能固然很出色，但是在实际应用中，使用中小型开源模型，并用特定数据微调，最终达到的效果可能更好。</p><p></p><p>至于成本问题，我们也算了一笔经济账：一台GPU服务器就可以提供支撑的7B、13B模型通过微调，性价比可能比直接使用闭源大模型高10倍以上。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/56/560ad0fe223d6015f2a3e0154b7bd70b.webp" /></p><p></p><p></p><p>如上图所示，以Llama2 7B开源模型为例，100万token的成本大约为0.1美元-0.3美元。使用一台英伟达A10GPU服务器就能支持训练，以峰值速度2500token每秒来计算，一小时的成本大约为0.6美元。自有这台服务器，一年的成本大约为5256美元，并不算高。</p><p></p><p>如果用闭源模型，100万token消耗速度很快，成本远高于0.6美元每小时。</p><p></p><p>不过成本消耗也要考虑应用的种类和模型的输出速度，模型输出速度越快，成本也会越高。如果可以有mini-batch（小批量数据集）等，同时来跑，它的整体性能就会更好，但是单个的输出性能可能就会稍微差一点。</p><p></p><p>这就引出另外一个问题，大模型的输出速度，怎样比较合适？</p><p></p><p>以Chatbot举例，人说话的速度大概为120词每分钟，成人阅读的速度大概为350词左右，反向计算token，每秒钟20个token左右，就能达到比较好的体验。如果这样计算的话，如果应用的流量够大，跑起来成本是不高的。</p><p></p><p>但是，究竟流量能不能达到“够大”，这就变成了“鸡生蛋、蛋生鸡”的问题。我们发现了一个很实用的模式可以解决这个问题。</p><p></p><p>在北美，很多企业都是先用闭源大模型来做实验（比如OpenAI的模型）。实验规模大概在几百个million（百万token），成本大概为几千美元。一旦数据飞轮运转起来，再把已有数据存下来，用较小的开源模型微调自己的模型。现在这已经变成了相对比较标准的模式。</p><p></p><p>在考虑AI模型的时候，各家企业其实都在各种取舍中找平衡。在北美经常讲一个不可能三角，当你买一辆车的时候跑得快、便宜和质量好，这三者是不可兼得的。</p><p></p><p>上文提到的标准模式，其实就是首先追求质量，然后再考虑成本，如果想同时满足这三方面，基本是不可能的。</p><p></p><p>半年之前我非常强烈地相信开源模型能非常迅速追赶上闭源模型，然而半年之后，我认为开源模型和闭源模型之间会继续保持一个非常合理的差距，这个差距用比较形象的具体模型举例来说，闭源模型到GPT-4水平的时候，开源模型可能在GPT3.5左右。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be13b28f126280485bf393899d31fab8.webp" /></p><p></p><p></p><p>硬件行业的新机会</p><p></p><p></p><p>早在2000年初，英伟达就看到了高性能计算的潜力，于是2004年他们做了CUDA，到今天为止已经20年。今天CUDA已经成为AI框架和AI软件最底层的标准语言。</p><p></p><p>早期，行业内都认为高性能计算写起来很不方便，英伟达介绍了CUDA，并说服大家它简单易用，让大家尝试来写。试用之后，大家发现确实易用且写出来的高性能计算速度很快，后来几乎各大公司的研究员们都把自己的AI框架基于CUDA写了一遍。</p><p></p><p>CUDA很早就和AI社区建立了很好的关系，其它公司也看到了这个市场的巨大机会，但是从用户侧来看，大家用其它产品的动机不强。</p><p></p><p>所以市场上还会有一个关注焦点，那就是是否有人能够撼动英伟达的地位，除了英伟达，新的硬件提供商还有谁可能有机会？</p><p></p><p>首先我的观点不构成投资建议，我个人认为英伟达在接下来的3~5年当中，依然还会是AI硬件提供商中绝对的领头羊，它的市场占有率不会低于80%。</p><p></p><p>但是今天AI模型逐渐标准化，我们也看到了硬件层面另外一个机会。前十年中，在AI领域大家都在纠结的一个问题，虽然很多公司能够提供兼容CUDA的适配，但是这一层“很脆”。“很脆”的意思是模型多种多样，所以适配层容易出问题，整个工作链就会断。</p><p></p><p>今天越来越少的人需要写最底层的模型，越来越多的需求是微调开源模型。能够跑Llama、能够跑 Mistral，就能满足大概80%的需求，每一个Corner Case（特殊情况）都需要适配的需求逐渐变少，覆盖几个大的用例就可以了。</p><p></p><p>其它硬件提供商的软件层在努力兼容CUDA，虽然还是很难，但是今天抢占一定市场占有率，不再是一件不可能的事情；另外云服务商也想分散一下投资。所以这是我们看到的一个很有意思的机会点，也是cloud infra在不断变化的过程。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed4e13279c43f403bf3e40ec0b1c0b90.webp" /></p><p></p><p></p><p>生成式AI浪潮：哪些是增量机遇？</p><p></p><p></p><p>我们再看一下AI应用的情况。今天我们可以看到AI应用的供给在不断增加。从Hugging Face来看，2022年8月模型数量大概只有6万，到2023年9月，数量就已经涨了5倍，增速是非常快的。</p><p></p><p>目前我们看到AI应用中，有两大类应用，已经跨越死亡谷，开始有比较持续的流量：</p><p></p><p>第一大类是提效（productivity）。例如在电商行业，用AIGC的方式更快生成商品展示图片。例如Flair AI，应用场景举例来说，我希望能给瓶装水拍摄一个广告图片，仅仅需要把水放在方便的地方，拍一张照片。然后把这张照片发送给大模型，告诉它，我希望它被放在有皑皑白雪的高山上，背景是蓝天白云。它就能生成一个直接可以上传电商平台，作为产品展示的图片。</p><p></p><p>其它类型也有很多，比如在企业海量知识库做搜索且有更好的交互功能，例如Glean。</p><p></p><p>第二大类是娱乐（entertainment），比如Soul，以AI的方式做角色扮演及交互。</p><p></p><p>另外我们还发现一个趋势是“套壳APP”越来越少了。其实大家发现直接“套壳”通用大模型的产品会有一个通病，交互效果特别“机器人”。</p><p></p><p>反而是7B、13B的稍小模型，性价比和可调性都特别好。做个直观的比喻：大模型就好像是“读博士”读轴了，反而是本科生的实操性更强。</p><p></p><p>做应用层，总结来讲有两条路径：第一条是训练自己的基础大模型，或者是自己去微调模型。</p><p></p><p>另外就是有自己非常垂直领域的应用，背后是很深的场景，直接用Prompt是不可行的。</p><p></p><p>比如医疗领域，用户提需求问：“我昨天做的化验结果怎么样？”这其实需要背后有个大模型，除了对化验指标做出专业的分析，还需要给用户提出饮食等建议。</p><p></p><p>这背后涉及到化验、保健、保险等产业链的多个细分场景，需要医疗产业链很深的经验。需要在既有的经验上加一层AI能力来做好用户体验，这是我们今天发现的比较有持续性的AI应用模式。</p><p></p><p>关于未来到底怎样，预测未来是最难的。我的经验一直是B端，逻辑主要看供需。AI带来的增量需求首先是高性能的算力。第二个是高质量的模型，以及上层需要的适合这些高性能、高质量和高稳定性需求的计算的软件层。</p><p></p><p>所以我觉得从高性能算力来看，英伟达显然已经成为赢家。另外这个市场可能会容纳2~3家比较好的芯片提供商。</p><p></p><p>从模型来看，OpenAI肯定是一个已经比较确定的赢家，市场足够大，应该能够容纳3-5家不同的模型生产厂商，而且它很有可能还会出现偏地域性的分布。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d96641a722069da64fa2548458168434.webp" /></p><p></p><p></p><p>传统行业的AI深水区</p><p></p><p></p><p>我还想讲的是大量的传统行业应用，这其实是AI行业里值得探讨的深水区。</p><p></p><p>大语言模型出现，大家曾经一度觉得OpenAI弄了一个特别厉害的大模型，写点Prompt就能搞定任何事情。</p><p></p><p>但是Google早在世纪之初就写过一篇文章，到今天我仍然觉得这个观点是对的。这篇文章说，机器学习模式只是整个AI链路中非常小的一部分，外面还有大量的工作，在今天来说也会变得越来越重要。比如如何收集数据、如何保证数据和我们的应用需求一致，如何来做适配，等等。</p><p></p><p>模型上线之后还有三件事：第一是跑的稳定，第二个是能够把结果质量等都持续稳定地控制起来，以及还有非常重要的一点是把应用当中所得到的数据，以一种回流的方式收集回来，训练下一波更好的模型。</p><p></p><p>到今天这个方法论依然适用，就是在行业竞争中，谁能有数据，谁能够把用户的反馈更好地调试成“下一波训练的时候可以更好的应用”的数据，这也是核心竞争力之一。</p><p></p><p>今天大家都有这样一种感觉，大模型的结构相差不大，但是数据和工程能力的细节才是决定模型之间差别的地方，OpenAI其实持续在给我们证明这件事。</p><p></p><p>今天我们看整个技术栈的架构是什么样子的，a16z给了我们一个非常好的总结（如下图）：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b4b9fd749082d737bb1007ba0ba2a482.webp" /></p><p></p><p></p><p></p><p>IaaS这一层基本上是英伟达做“老大”，其它公司在竞争硬件和云平台，这是最下层的坚实基础。</p><p></p><p>云平台今天也在发生不断的变化，大家最近可能在技术趋势上听到一个词叫做“下云”，以前大家肯定听说过“全栈上云”。</p><p></p><p>为什么会出现“我要下云”的思潮？就是因为算力本身是巨大的成本，而且又是可以“自成一体”的成本，所以行业内开始把传统的云成本和今天AI算力的成本分开来考虑。</p><p></p><p>今天越来越多的PaaS开始变成Foundation Model，有些是闭源的，有些是开源的，然后在上面再做一层APP。今天每一层都竞争激烈。但是我个人感觉在模型这一层以及往上的上层应用这一层，是最活跃的。</p><p></p><p>模型层主要是开源和闭源之争。</p><p></p><p>应用层有两个趋势：一个是模型在努力往上做应用；另外就是是应用层在拼命想理解模型到底能有什么能力，然后把自己的应用加上AI，让自己的应用更强大。</p><p></p><p>我个人认为，模型往上做应用有点难，应用把自己的AI能力加进来更有希望。</p><p></p><p>国内还有种说法叫做Super APP（超级应用），Super APP很重要的一点是需要“端到端把问题解决”。a16z在他的图上也描述会有一些端到端的APP出来，本质上需要模型的推理和规划的能力做的非常好。ChatGPT就是端到端全部打通，模型也是自己的，应用也是自己的，这是Super App的状态。</p><p></p><p>但是我个人关于Super App的观点可能稍微保守一些，也有可能是因为我自己的经历很多时候都在做TOB的服务，我个人的感觉是Super APP会有，但是会很少。</p><p></p><p>我个人的感觉是，B端的应用越来越多的还是会以一种像搭积木一样，用开源的模型结合企业自己的数据，把企业自己的应用搭起来的一个过程。</p><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/43/4382db6cb575e0915a3c721355de3872.webp" /></p><p></p><p></p><p>大模型的商业模式：</p><p>两个纠结和一个市场现象</p><p></p><p></p><p>但是在大模型进行商业化落地的过程中，我观察到市场还是会有两个纠结：</p><p></p><p>第一个纠结是营收的流向和以往不太一样，不太对。</p><p></p><p>正常商业模式的流向应该是：从用户那里收费，然后“留成本”给硬件服务商，比如英伟达。但是今天是横向的，从VC（风投）拿到融资，直接“留钱”给硬件厂商。但是VC的钱本质是投资，创业者最后可能要10倍还给VC，所以这个资金流向是第一个纠结。</p><p></p><p>第二个纠结是今天的大模型对比传统软件，可以创造营收的时间太短。</p><p></p><p>其实开发一次软件之后，可以收回成本的时间比较长。比如像Windows，虽然过几年迭代一代，但是它底层的很多代码是不用重写的。所以一个软件被写完，可能在接下来的5-10年当中，它给我时间窗口持续迭代。而且投入的成本大部分是程序员的成本。</p><p></p><p>但是大模型的特点是，每次训练过一个模型之后，下一次还是要从零开始重新训练。比较形象一点来说“今天投入10个亿，再迭代的时候，又得再追加投入十个亿”。</p><p></p><p>但是模型的迭代速度又很快，中间能够赚钱的时间窗口究竟有多长？今天看起来好像大概是一年左右，甚至更短。</p><p></p><p>于是大家就开始质疑，大模型的成本远高于传统的软件，但是做完一个模型之后，能赚钱的时间远低于传统的软件。</p><p></p><p>所以就回到了这个终极问题，大模型的商业模式到底怎样才能真正有效？</p><p></p><p>我还观察到一个市场现象，去年整个市场都非常痛苦，硬件需求的突然暴涨，整个供应链都没反应过来，等待时间很长，甚至可能6个月以上。</p><p></p><p>最近我们观察到的一个现象是供应链没有那么紧张了。第一是全球供应链也开始缓过来；第二我个人判断有一部分以前因为焦虑而提前囤货的供应商，觉得现在要开始收回成本了。之前供不应求的紧张状态会逐渐变好，但是也不会一下子变成所有人都愁卖的状态。</p><p></p><p>以上就是我基于这波生成式AI爆发，对整个AI产业造成的影响的个人观察。也正是在这个浪潮中，Lepton正在持续帮助企业和团队在生成式AI落地的过程中找到成本、效果、效率的最佳均衡点。最后，其实可以以Richard S. Sutton——增强学习领域开山立派的一位导师，在2019年说的一句话作为总结，“在整个70年的AI科研中，最重要的经验就是，通过一个通用的方法（今天是深度学习），来利用大量的计算模型（今天是以英伟达为代表的异构GPU为基础的高性能计算），这样的方式是整个70年AI发展中最有效、最简单的方式。”</p><p></p><p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.</p><p></p><p>——Richard Sutton: "The Bitter Lesson"</p><p></p><p>文字经贾扬清本人确认，感谢高山书院（公众号：gasadaxue）对本文的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</id>
            <title>“真男人就应该用 C 编程”！用 1000 行 C 代码手搓了一个大模型，Mac 即可运行，特斯拉前AI总监爆火科普 LLM</title>
            <link>https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c0M6oyu0e1s9sJ4ApTdZ</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 09:56:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: C语言, GPT-2, Andrej Karpathy, 大模型训练
<br>
<br>
总结: Andrej Karpathy使用C语言徒手实现了GPT-2大模型训练，减少了对庞大外部库的依赖，使得模型训练更轻量化和高效。他的代码开源后获得了广泛关注，展示了用C语言实现大型语言模型训练的可能性。Karpathy的工作为学习和理解大型语言模型提供了重要资源。 </div>
                        <hr>
                    
                    <p></p><blockquote>徒手用 1000 行 C 语言实现，不依赖庞大的外部库，Mac 即可运行。</blockquote><p></p><p></p><p>如今这年头，徒手写神经网络代码已经不算事儿了，现在流行手搓大模型训练代码了！这不，今天，特斯拉前AI总监、OpenAI 创始团队成员Andrej Karpathy仅用1000行简洁的C代码，就完成了 GPT-2 大模型训练过程。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18b9f758c3320976c307f166074c9f63.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>几个小时前，Andrej Karpathy推出了一个名为llm.c的项目，旨在用纯C语言训练LLM，这种方法的主要优势在于它显著减少了依赖库的体积——不再需要245MB的PyTorch和107MB的cPython，这样可以使得模型训练过程更为轻量化和高效。该项目还可以立即编译和运行，并且与PyTorch的参考实现完全匹配。</p><p>&nbsp;</p><p>Karpathy表示他之所以选择GPT-2作为首个工作示例，是因为它大语言模型鼻祖的定位，亦属现代AI堆栈的首次组合。因此，选择 GPT-2 作为起点，可以让我们更容易地理解和实践大型语言模型训练。</p><p>&nbsp;</p><p>徒手实现GPT-2后，Karpathy将这份代码放到了GitHub上，以MIT协议开源。短短几个小时，就超过了2500颗星，并且数据还在不断持续上涨......</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70dfc5db0012180f5518d691c2d9b896.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>将大模型原理拆解得无比简单</h2><p></p><p>&nbsp;</p><p>Andrej Karpathy 是全球人工智能领域的知名科学家，也是 OpenAI 的创始成员和研究科学家。</p><p>&nbsp;</p><p>他于2009年本科毕业于多伦多大学，获得计算机科学和物理学学士学位。2011年硕士毕业于不列颠哥伦比亚大学，随后前往斯坦福大学AI Lab（SAIL）读博，师从著名学者李飞飞，是全球最早将深度学习应用于计算机视觉研究的学者之一。</p><p>&nbsp;</p><p>在求学期间，Andrej Karpathy曾在谷歌和DeepMind实习，后来在OpenAI刚刚成立时加入并担任研究科学家。直到2017年6月，他被马斯克挖去，担任特斯拉人工智能部门主管，直接向马斯克汇报。在特斯拉工作的五年里，他主导了特斯拉自动辅助驾驶系统Autopilot的开发。这项技术对于特斯拉的完全自动驾驶系统 FSD 至关重要，也是马斯克针对 Model S、Cybertruck 等车型推销的主要卖点。在各大新闻中，他被誉为“特斯拉的秘密武器”。</p><p>&nbsp;</p><p>去年Karpathy曾短暂回到OpenAI，然后又在OpenAI众人忙于内斗时抽空录制了一个长达一小时的教学视频《大型语言模型入门》。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/097e6065fd638962c0ee82bd46dc224e.jpeg" /></p><p></p><p>&nbsp;</p><p>Karpathy 在视频中首先介绍了一些 LLM 入门知识，然后以 Meta 推出的开源大模型 Llama 2-70b 为例进行了讲解。该模型有 700 亿参数，主要包含两个文件，分别是参数文件，文件大小为 140GB，以及运行这些参数的代码，以 C 语言为例需要约 500 行代码。</p><p>&nbsp;</p><p>Karpathy 表示只要有这两个文件再加上一台 MacBook，我们就可以构建一个独立的系统，无需联网或其他设施。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffefdd6449d6362b2837474fe5b4555d.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>大模型训练，可以理解为是对互联网数据进行有损压缩，一般需要一个巨大的GPU集群来完成。以Llama 2-70b为例的话，就是使用了类似网络爬取的约 10TB 的文本，用6000 个 GPU ，耗资 200 万美元，训练约 12 天，最后获得基础模型。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/2072a2e96c9acf4978fb523baf6102c3.jpeg" /></p><p></p><p>&nbsp;</p><p>基础模型即上图里140GB的“压缩文件”（压缩率约100倍），就等于靠这些数据对世界形成了理解，那它就可以进行“预测”工作了。</p><p>&nbsp;</p><p>Karpathy之前还分享过他的学习经验，就是开始时要尝试从0开始，写一些原生代码，帮助理解消化知识点。也就是说，徒手实现代码才是最有效的学习方式。</p><p>&nbsp;</p><p>两年前，Karpathy就曾基于 PyTorch，仅用 300 行左右的代码就写出了一个小型 GPT 训练库，并将其命名为 minGPT，用这份代码揭开了GPT神秘的面纱。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8ef635be577ae58cf8fe6012e2a3544b.jpeg" /></p><p></p><p>截图来源：<a href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a>"</p><p>&nbsp;</p><p>因为大多数 GPT 模型的实现都过于庞大，而minGPT 做到了小、干净、可解释和具有教育意义，所以Karpathy的这300行代码是学习 GPT 的最佳资源之一，可以用来深入理解GPT 是如何工作的。</p><p>&nbsp;</p><p></p><h2>用C语言实现LLM</h2><p></p><p>&nbsp;</p><p>这次，Andrej Karpathy单纯通过C/CUDA实现大语言模型训练，且无需245 MB PyTorch或107 MB cPython。例如，训练GPT-2（CPU，fp32单精度）需要在单个文件中使用约1000行简洁代码，可立即编译并运行、且与PyTorch参考实现完全匹配。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7bea147ef70c91506cb2980bc256e088.jpeg" /></p><p></p><p>&nbsp;</p><p>从某种意义上说，Karpathy确实在尝试重新设计LLM的架构。他通过llm.c项目探索一种更简单、更高效的训练LLM方法。与现有LLM架构相比，这种新架构的主要亮点包括：</p><p>&nbsp;</p><p>1. 代码简洁性：仅使用约1000行代码就能完成GPT-2模型的训练，相比之下显著降低了复杂度。</p><p>2. 独立性：不依赖庞大的外部库如PyTorch或cPython，使得部署和运行更加轻便快捷。</p><p>3. 高效性：直接使用C/CUDA进行编程有望提高计算效率和训练速度。</p><p>&nbsp;</p><p>有网友问Karpathy为何不用Rust，Karpathy回复说，“我完全理解Rust的吸引力。然而，我仍然觉得 C 语言非常棒。它简单、干净、可移植，在审美上也十分优美。使用 C 语言就像直接与机器交流一样。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b291c68429d3d5ecc9ef997d9f4c4028.jpeg" /></p><p></p><p>&nbsp;</p><p>这种语言选择也让网友们纷纷感叹：</p><p>&nbsp;</p><p>“我们正在掀起一场 C 语言复兴！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/1854da08a50f68f4f2b7ff0cdc84b81a.jpeg" /></p><p></p><p>&nbsp;</p><p>“真男人就应该用 C 语言编程。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a745a6ba051f49d4a1ad4d83255f0c72.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Karpathy以更简单、更原始的C/CUDA架构来做LLM的训练，其中还涉及算法优化、计算资源管理等多个方面。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd4ec03d1dab994cae359ca1da6a1585.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>你会看到，项目在开始时一次性分配所有所需的内存，这些内存是一大块 1D 内存。然后在训练过程中，不会创建或销毁任何内存，因此内存占用量保持不变，并且只是动态的，将数据批次流过。这里的关键在于手动实现所有单个层的前向和后向传递，然后将它们串联在一起。例如，这里是 layernorm 前向和后向传递。除了 layernorm 之外，我们还需要编码器、matmul、自注意力、gelu、残差、softmax 和交叉熵损失。</blockquote><p></p><p>&nbsp;</p><p>“一旦你拥有了所有的层，接下来的工作只是将它们串在一起。讲道理，写起来相当乏味和自虐，因为你必须确保所有指针和张量偏移都正确排列， ”Karpathy 表示。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d05cd843fcd5a4e542a86f9de979633e.png" /></p><p></p><p>&nbsp;</p><p>另外Karpathy还在doc/layernorm/layernorm.md中附上了短小的使用教程。里面是一份简单的分步指南，用于实现GPT-2模型中的单一层，即layernorm层，希望能成为大家理解在C中实现大语言模型各层的理想起点。</p><p>&nbsp;</p><p>更重要的是，他还用自己的MacBook Pro（苹果M3 Max芯片）演示了整个训练过程，对照他之前的大模型入门教程，就可以轻松了解如今炙手可热的LLM是怎么一回事儿了。</p><p>&nbsp;</p><p></p><h4>训练过程：</h4><p></p><p>&nbsp;</p><p>首先下载数据集并token化。</p><p>&nbsp;</p><p>python prepro_tinyshakespeare.py</p><p>&nbsp;</p><p>输出结果为：</p><p>&nbsp;</p><p>Saved 32768 tokens to data/tiny_shakespeare_val.bin</p><p>Saved 305260 tokens to data/tiny_shakespeare_train.bin</p><p>&nbsp;</p><p>其中各.bin文件为int32数字的原始字节流，用于指示GPT-2 token化器的token id。或者也可以使用prepro_tinystories.py对TinyStories数据集进行标注。</p><p>&nbsp;</p><p>原则上，到这里就已经可以开始训练模型。为提高效率，可以使用OpenAI发布的GPT-2权重进行初始化，而后加以微调。为此需要下载GPT-2权重并将其保存为可在C中加载的检查点：</p><p>&nbsp;</p><p>python train_gpt2.py</p><p>&nbsp;</p><p>该脚本会下载GPT-2（124M）模型，对单批数据进行10次过拟合迭代，运行多个生成步骤，最重要的是保存两个文件：1）gpt2_124M.bin文件，包含用于在C中加载的模型权重；2）以及gpt2_124M_debug_state.bin，包含包括input、target、logits及loss等更多调试状态，对于调试C代码、单元测试及确保能够与PyTorch参考实现完全匹配非常重要。现在我们可以使用这些权重进行初始化并在原始C代码中进行训练。首先编译代码：</p><p>&nbsp;</p><p>make train_gpt2</p><p>&nbsp;</p><p>在train_gpt2编译完成后即可运行：</p><p>&nbsp;</p><p>OMP_NUM_THREADS=8 ./train_gpt2</p><p>&nbsp;</p><p>大家应根据CPU的核心数量来调整线程数量。该程序将加载模型权重、tokens，并使用Adam lr 1e-4运行数次迭代的微调循环，而后由模型生成样本。简单来讲，所有层都具有前向及后向传递实现，串联在一起形成统一的大型、手动前向/后向/更新循环。在MacBook Pro（苹果M3 Max芯片）上的输出结果如下所示：</p><p>&nbsp;</p><p>[GPT-2]</p><p>max_seq_len: 1024</p><p>vocab_size: 50257</p><p>num_layers: 12</p><p>num_heads: 12</p><p>channels: 768</p><p>num_parameters: 124439808</p><p>train dataset num_batches: 1192</p><p>val dataset num_batches: 128</p><p>num_activations: 73323776</p><p>val loss 5.252026</p><p>step 0: train loss 5.356189 (took 1452.121000 ms)</p><p>step 1: train loss 4.301069 (took 1288.673000 ms)</p><p>step 2: train loss 4.623322 (took 1369.394000 ms)</p><p>step 3: train loss 4.600470 (took 1290.761000 ms)</p><p>... (trunctated) ...</p><p>step 39: train loss 3.970751 (took 1323.779000 ms)</p><p>val loss 4.107781</p><p>generated: 50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323</p><p>step 40: train loss 4.377757 (took 1366.368000 ms)</p><p>&nbsp;</p><p>现在的生成结果仅给出token ids，需要将其解码回文本形式：</p><p>&nbsp;</p><p>&lt;|endoftext|&gt;Come Running Away,</p><p>Greater conquer</p><p>With the Imperial blood</p><p>the heaviest host of the gods</p><p>into this wondrous world beyond.</p><p>I will not back thee, for how sweet after birth</p><p>Netflix against repounder,</p><p>will not</p><p>flourish against the earlocks of</p><p>Allay</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/karpathy/status/1777427947126936026">https://twitter.com/karpathy/status/1777427947126936026</a>"</p><p><a href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a>"</p><p><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">https://www.youtube.com/watch?v=zjkBMFhNj_g</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PG1Og0E8kCOBqM1R04Xw</id>
            <title>WPS AI企业版来了，MiniMax、智谱AI、文心一言等多个大模型自由切换调用</title>
            <link>https://www.infoq.cn/article/PG1Og0E8kCOBqM1R04Xw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PG1Og0E8kCOBqM1R04Xw</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 09:48:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金山办公, WPS 365, AI办公, 企业版
<br>
<br>
总结: 金山办公举行生产力大会发布了WPS 365平台，整合了文档、AI、协作三大能力，提升企业办公效率。WPS 365全新升级开启企业AI办公时代，满足组织日常办公需求，实现一站式AI办公。WPS AI企业版降低用户使用大模型门槛，提供智能文档库和企业智慧助理，助力企业提升生产力。 </div>
                        <hr>
                    
                    <p>4月9日，金山办公生产力大会在京举行，现场发布了面向组织和企业的办公新质生产力平台WPS 365，其包含升级的WPS Office、最新发布的WPS AI企业版和WPS协作。WPS 365打通了文档、AI、协作三大能力，让各组件间无缝切换，用户使用一个工具就能调用各类主流大模型，一个界面就能边写边沟通边开会，一个产品就能高效完成所有工作。</p><p>&nbsp;</p><p>金山办公CEO章庆元表示：“金山办公秉承技术立业、用户第一的理念，在过去36年里紧跟时代脉搏，迭代公司产品战略，持续将时代最新的技术，转换成最务实的办公产品，用可控的成本，交付给客户。”</p><p></p><h2>WPS 365全新升级，开启企业AI办公时代</h2><p></p><p>&nbsp;</p><p>数据显示，2020年金山办公云端文档数是898亿份，到2023年底已达到2174亿份，增长了142%。金山办公作为国内头部办公软件公司之一，金山办公文档功能在市场中应用广泛，此次WPS 365的全新升级，进一步打通了AI与协作的能力。</p><p>&nbsp;</p><p>章庆元表示，WPS 365全面覆盖了一个组织日常办公的基本需求，从文档创作到即时通讯（IM）、会议、邮件，再到AI应用，标志着一个文档处理套件正式升级为一站式AI办公，让企业生产力即刻起飞。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;</p><p>发布会现场演示了本地文档一键切换出在线协作功能，变成协作文档，并邀请他人共同参与编辑。协作完成后又可以选择关闭，协作文档恢复为本地文档，实现无缝切换。最新版WPS Office里的各类工具也均内置了具备大语言模型能力的人工智能应用WPS AI，为用户提供诸如扩写、缩写、总结、生成公式等功能。WPS协作则进一步让组织用户间的沟通交流更便捷，用户可以边写⽂档边沟通，即使是在邮件、OA系统里也能够无缝衔接。</p><p>&nbsp;</p><p>基于「WPS Office+WPS协作+WPS AI」的模式，意味着用户只需要WPS 365一个产品就能高效完成所有工作，免去了办公场景下的繁琐切换，实现办公新质生产力的切实落地。</p><p></p><h2>WPS AI企业版发布，降低用户使用大模型的门槛</h2><p></p><p>&nbsp;</p><p>金山办公副总裁王冬介绍说，与个人版不同之处在于，WPS AI企业版聚焦为客户打造企业大脑，它分为AI Hub（智能基座）、AI Docs（智能文档库）、Copilot Pro（企业智慧助理）三个部分。</p><p>&nbsp;</p><p>AI Hub集成了国内主流大模型的AI能力，例如MiniMax、智谱AI、文心一言、商汤日日新、通义千问等等。金山办公在大模型领域的定位是应用方，与国内上百种模型进行了适配磨合，实现基础AI服务的开箱即用，这也让用户克服了大模型选择困难症并极大降低使用门槛。</p><p>&nbsp;</p><p>AI Docs则将传统云文档库一键升级为智能文档库，让智能创作来源有依据，完整的文档权限体系保障信息不越权。</p><p>&nbsp;</p><p>Copilot Pro则可帮助运营人员使用自然语言驱动BI产品分析数据，并可调用WPS 365 API和企业自有API，解决办公自动化需求。</p><p>&nbsp;</p><p>区别于市面上仅仅做内容生成的AI产品，WPS AI企业版不仅能读书认字，还能自助化分析数据、减少人工的重复劳动，提供“文理兼修”的数字员工服务，例如阅读助手、画图助手、考勤助手、销售分析、合同分析等等，触达各类细微的办公场景。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Lf7ZJ7qNY03bBw6kF9Tr</id>
            <title>没有数据训练大模型？OpenAI 总裁带队转录YouTube视频，谷歌、Meta 也想尽数据收割套路</title>
            <link>https://www.infoq.cn/article/Lf7ZJ7qNY03bBw6kF9Tr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Lf7ZJ7qNY03bBw6kF9Tr</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 06:58:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, YouTube视频, 数据收割, AI模型
<br>
<br>
总结: OpenAI 面临数据供应荒，开发了语音识别工具 Whisper 转录 YouTube 视频以训练 AI 模型。谷歌、Meta 也在收集数据用于 AI 训练，涉及版权问题。科技企业迫切需要数据，甚至开始使用合成信息。创作者作品成为主要训练素材，引发版权诉讼。AI 模型获取内容引发版权争议，版权法也在适应 AI 时代。 </div>
                        <hr>
                    
                    <p><img alt="" src="https://static001.geekbang.org/wechat/images/ba/ba0d76f60bd041eb7454af2109874966.jpeg" />
</p>
<p>2021 年底，OpenAI 开始面临数据供应荒。</p>
<p>这家人工智能研究机构在开发最新 AI 系统时，已然耗尽了互联网上所有质量稳定的英语文本库。现在他们需要更多数据来训练自家技术的下一个版本——更多更多。</p>
<p>为此，OpenAI 研究人员开发出一款名为 Whisper 的语音识别工具，能够转录 YouTube 视频中的音频以生成新的对话文本，再将其作为训练素材以提升 AI 系统的智能水平。</p>
<p>三名知情人士表示，部分 OpenAI 员工讨论了此举可能违反 YouTube 规则。谷歌旗下的 YouTube 明确禁止将其视频用于“独立”于该平台以外的应用场景。</p>
<p>知情人士指出，最终 OpenAI 团队还是转录了超过 100 万小时的 YouTube 视频。两位知情人士表示，这支团队包括 OpenAI 总裁 Greg Brockman，他还亲自协助收集了这些视频。整理出的文本随后被输入名为 GPT-4 的系统，这也是目前得到广泛认可的最强 AI 模型之一，也是最新版本 ChatGPT 聊天机器人的底层引擎。</p>
<p>这场貌似追求技术的 AI 军备竞赛，早已转变成疯狂搜集数字数据的对抗与掠夺。根据《纽约时报》的调查，为了获取这些数据，包括 OpenAI、谷歌和 Meta 在内的科技大厂可谓“各显神通”——他们无视公司原则、修改规定条款，甚至公开讨论如何规避版权保护。</p>
<p>根据《纽约时报》获得的内部会议记录，在坐拥 Facebook 和 Instagram 的 Meta 公司，经理、律师和工程师们去年曾讨论收购由 Simon &amp; Schuster 出版社出版的长篇作品。他们还商定从互联网上收集受版权保护的数据，甚至愿意为此直面诉讼风险。与会者认为，逐个与出版商、艺术家、音乐家和新闻机构谈判授权许可恐将耗费过多时间。</p>
<p>与 OpenAI 一样，谷歌也在转录 YouTube 视频以为自家 AI 训练获取文本素材。五位了解谷歌具体操作的人士透露，这可能侵犯了视频版权，毕竟视频应归其创作者所有。</p>
<p>去年，谷歌还扩大了其服务条款。根据该公司隐私团队成员及《纽约时报》看到的一份内部消息，此番调整的动机之一就是为谷歌从公开的 Google Docs 文档、谷歌地图上的餐厅评论以及其他在线材料中提取更多信息敞开了大门，最终目的当然还是训练 AI 产品。</p>
<p>这些科技大厂的行动说明，在线信息——包括新闻故事、虚构作品、留言板帖子、维基百科文章、计算机程序、照片、播客及电影切片等——正在成为蓬勃发展的 AI 行业的根基与命脉。能否构建起强大的创新系统，往往取决于各方能否获得充足的数据来训练模型，进而生成与人类水平相当甚至更出色的文本、图像、声音与视频内容。</p>
<p>数据量甚至可以说将决定一切。领先的聊天机器人系统正在从涵盖多达 3 万亿字的数字文本池中学习，其体量约等于牛津大学博德利图书馆馆藏书籍总字数的两倍——该图书馆自 1602 年起就一直在收集各类信息。研究人员表示，“信息”是指由专业人士精心撰写及编辑的高质量内容，例如已出版的书籍和文章。</p>
<p>多年以来，互联网（包括维基百科和 Reddit 等网站）似乎成为取之不尽、用之不竭的数据来源。但想在 AI 领域傲视同侪的科技企业们仍在寻求更大的资源池。谷歌和 Mtea 坐拥数十亿用户，每天都会产生大量搜索查询与社交媒体帖子；但受到隐私法及其自身政策的限制，理论上并不能将大部分内容用于 AI 训练。</p>
<p>于是乎，数据供应短缺开始愈发凸显。研究机构 Epoch 表示，科技企业最快可能在 2026 年就用尽互联网上的高质量数据，其使用数据的速度已经明显超过了数据产出的速度。</p>
<p>硅谷风险投资公司 Andreessen Horowitz 的代表律师 Sy Damle 在去年关于版权法的公开讨论中谈到 AI 时表示，“这些工具获取实用功能的唯一途径，就是既接受大量数据的训练、又无视这些数据的使用许可。其需要的数据量如此庞大，即使是集体许可也仍无法满足。”</p>
<p>科技企业迫切需要新的数据，以至于部分公司开始使用“合成”信息。这些资讯并非由人类创建的有机数据，而是 AI 模型自身生成的文本、图像和代码。换句话说，AI 系统开始从自己的产物中学习知识。</p>
<p>OpenAI 表示，其每套 AI 模型“都拥有我们精心设计的独特数据集，以帮助其了解世界并保持研究层面的领先竞争力。”谷歌也提到，其 AI 模型“接受了一部分 YouTube 内容的训练”，这种行为符合其与 YouTube 创作者达成的协议，且该公司不会在实验计划之外使用来自办公应用的数据。Meta 则指出，他们已经通过“积极投资”将 AI 技术整合至各项服务当中，并使用来自 Instagram 及 Facebook 的数十亿公开分享图像及视频进行模型训练。</p>
<p>对于创作者来说，他们的作品正日益成为 AI 训练中的主要素材，由此引发的版权与许可诉讼也可谓此起彼伏。去年，《纽约时报》起诉 OpenAI 与微软侵权，称其在未经许可的情况下使用受版权保护的新闻文章来训练 AI 聊天机器人。OpenAI 及微软则表示这些文章属于“合理使用”，或者说并不违反版权法，这在本质上属于正常的二创行为。</p>
<p>去年，超过一万个贸易团体、作者、企业及其他机构向版权局提交了关于 AI 创意作品的使用报告，该局属于联邦机构，目前正着手对版权法内容做 AI 时代下的适用性调整。</p>
<p>电影制片人、前演员、两本出版书籍作者 Justine Bateman 提醒版权局，AI 模型会在未经许可或付费的情况下获取内容——包括她自己的书籍和影片。</p>
<p>她在采访中表示，“这是美国有史以来规模最大的盗窃案。”</p>
<p>“规模就是一切”2020 年 1 月，约翰·霍普金斯大学的理论物理学家、Anthropic 的首席科学官 Jared Kaplan 发表了一篇关于 AI 的开创性论文，激发了人们对于在线数据的高度关注。</p>
<p>他的结论非常明确：训练大语言模型（驱动在线聊天机器人的底层技术）需要的数据越多，春性能就越好。正如学生们通过阅读更多书籍以汲取更多知识一样，大语言模型也能更好地提取文本中的模式，并通过更多信息将这种模式整理得更加准确。</p>
<p>Kaplan 博士与其他九位 OpenAI 研究人员共同发表了这篇论文（他目前在 AI 初创公司 Anthropic 供职），并在文中指出“每个人都对这种趋势感到惊讶——我们将其称为扩展法则，而且基本跟天文学或者物理学定律一样可靠。”</p>
<p>于是，“规模就是一切”很快成为 AI 竞赛中的战斗口号。</p>
<p>长期以来，研究人员一直使用大型公共数字信息数据库来开发 AI，包括维基百科和 Common Crawl（一套自 2007 年以来涵盖超 2500 亿个网页的数据库）。在实际训练之前，研究人员通常会删除其中的仇恨言论及其他非必要文本以“清洗”数据，再将其“投喂”给 AI 模型。</p>
<p>按今天的标准来看，2020 年的数据集体量还很小。来自照片网站 Flickr 的一套 3 万张照片数据库，在当时已经被视为重要的训练资源。</p>
<p>而随着 Kaplan 博士论文的发表，这些数据量已经远远不够。来自纽约的 AI 公司 Nomic 的 CEO Brandon Duderstadt 表示，目前的关键就是“把规模扩大”。</p>
<p><img alt="" src="https://static001.geekbang.org/wechat/images/38/38f871e4022d58e82c019c29ac1e497d.jpeg" /><br />随着 OpenAI 公司于 2020 年 11 月发布 GPT-3，该模型接受了截至当时规模最大的数据训练——约 3000 亿个 tokens。所谓 tokens，本质上就是一个个单词或者短语片段。从数据中完成学习之后，系统开始以惊人的准确性生成文本，编写博文和诗歌甚至能够输出计算机程序。</p>
<p>2022 年，谷歌旗下的 AI 实验室 DeepMind 又迈出了关键一步。他们测试了 400 种 AI 模型并调整其训练数据量及其他因素，发现表现最好的模型所使用的数量规模甚至比 Kaplan 博士论文中的预测还要更大。其中一套模型 Chinchilla 接受了 1.4 万亿个 tokens 的训练。</p>
<p>这项纪录很快就被打破：去年，来自中国的研究人员发布了 AI 模型 Skywork，使用来自英文及中文文本的 3.2 万亿个 tokens 进行训练。谷歌随后发布 AI 系统 PaLM 2，tokens 数量突破 3.6 万亿。</p>
<p>转录 YouTube 内容去年 5 月，OpenAI 公司 CEO Sam Altman 承认，AI 企业将很快耗尽互联网上的所有可用数据。</p>
<p>他在一场技术会议的演讲上公开表示，“就快耗尽了。”</p>
<p>Altman 自己也切身感受到了这种紧迫感。在 OpenAI，研究人员多年来一直在收集数据、清洗数据并将其转录为大量文本以训练自家语言模型。他们挖掘了计算机代码库 GitHub，清洗了国际象棋走法数据库，并使用 Quizlet 网站上关于高中考试和家庭作业的数据。</p>
<p>根据八位了解 OpenAI 公司情况的人士（因未获授权公开发言而要求匿名）表示，到 2021 年底这些数据供应已经用尽。</p>
<p>OpenAI 公司迫切需要更多数据来开发其下一代 AI 模型，也就是我们熟悉的 GPT-4。知情人士称，员工们因此讨论了转录播客、有声读物及 YouTube 视频的可行性。他们还考虑利用 AI 系统从头开始创建数据，甚至想到收集那些掌握着大量数字数据的初创公司。</p>
<p>六位知情人士指出，OpenAI 最终开发出了语音识别工具 Whisper，专门用于转录 YouTube 视频及播客。但 YouTUbe 不仅禁止他方将其视频用于“独立”应用场景，还禁止他方通过“任何自动化方式（包括机器人、僵尸网络或爬虫工具）”访问其视频内容。</p>
<p>知情人士称，OpenAI 的员工清楚知道自己涉足的是法律的灰色地带，但他们相信使用视频内容训练 AI 属于合理使用。OpenAI 公司总裁 Brockman 在一份研究论文中被列为 Whisper 的缔造者。据两位知情人士介绍，他曾亲自帮助收集 YouTube 视频并将转录结果输入 GPT 模型。</p>
<p>Brockman 将置评请求转交给 OpenAI，该公司只模糊承认其使用了“来自众多来源”的数据。</p>
<p>去年，OpenAI 发布了 GPT-4，模型训练使用到 Whisper 转录的超 100 万小时的 YouTube 视频内容。这套最新、最强的大模型由 Brockman 领导的团队开发完成。</p>
<p>两位了解内情的人士表示，部分谷歌员工已经知晓 OpenAI 在收集 YouTube 视频作为训练数据，但他们并没有出声阻止，是因为谷歌自己也在使用 YouTube 视频的文字记录训练其 AI 模型。谷歌的这种作法同样可能侵犯 YouTube 创作者的版权。知情人士还提到，一旦谷歌揪住 OpenAI 的作法不放，那公众很可能针对其同类作法提出强烈抗议。</p>
<p>谷歌公司发言人 Matt Bryant 则表示，该公司对于 OpenAI 的行为一无所知，且禁止“未经授权抓取或下载 YouTube 内容”。他强调，谷歌将在获得明确法律或技术依据时采取行动。</p>
<p>谷歌的规则允许其利用 YouTube 用户数据为该视频平台开发新功能。但目前还不清楚谷歌是否可以利用 YouTube 数据构建除视频平台之外的商业服务，例如聊天机器人。</p>
<p>Berger Singerman 律师事务所的知识产权律师 Geoffrey Lottenberg 表示，谷歌对于 YouTube 视频记录可用于什么、不能用于什么的说法太过含糊其辞。</p>
<p>“这些数据是否可用于新的商业服务仍有待明确解释，甚至可能引发诉讼。”</p>
<p>2022 年底，就在 OpenAI 发布 ChatGPT 并引发全行业竞赛之后，谷歌研究人员和工程师们讨论了利用其他用户数据的可能性。用户们的 Google Docs 文档及其他免费谷歌应用中蕴藏着数十亿单词量的文本。但三名了解谷歌内情的人士指出，该公司的隐私条款限制了他们使用这些数据的方式。</p>
<p><img alt="" src="https://static001.geekbang.org/wechat/images/b6/b62d2ca06edd3c47e4696ccf77828a7d.png" /><br />据了解内情的人士介绍，在 OpenAI 发布 ChatGPT 之后，谷歌研究人员和工程师们开始讨论利用其他用户数据开发 AI 模型的可能性。</p>
<p>据隐私团队两名成员及《纽约时报》看到的一份内部消息称，谷歌法律部门于去年 6 月要求隐私团队起草措辞，以扩大该公司对消费者数据的许可使用范围。</p>
<p>员工被告知，谷歌希望利用用户们的 Google Docs 文档、Google Sheets 表格及相关应用程序中公开的内容来开发一系列 AI 产品。员工们称，他们不清楚公司之前是否曾利用这些数据训练过 AI 模型。</p>
<p>当时的谷歌隐私政策强调，该公司只会使用公开信息来“帮助训练谷歌的语言模型并构建谷歌翻译等功能。”</p>
<p>隐私团队编写了新条款，以便谷歌能够利用这些数据为其“AI 模型提供支持，并构建包括谷歌翻译、Bard 及 Cloud AI 在内的更多产品及功能”，也就是更广泛的 AI 技术集合。</p>
<p>隐私团队的一名成员在内部消息中质疑，“我们的最终目标是什么？我们还要做到什么程度？”</p>
<p>员工们表示，谷歌特别要求隐私团队要在 7 月 4 日周末发布新条款，想要用美国的独立日假期冲淡用户的关注。修订后的政策于 7 月 1 日长周末开始时首次发布。</p>
<p>谷歌如何使用客户数据下面来看谷歌去年对其免费消费者应用程序隐私政策做出的修改。</p>
<p>谷歌会使用客户信息以改进我们的服务，并开发有利于用户及公众的新产品、功能及技术。例如，我们会使用公开信息帮助训练谷歌的 语言 AI模型并构建包括谷歌翻译、Bard 及 Cloud AI 功能在内的产品与功能。 两名隐私团队成员表示，去年 8 月他们曾向管理层施压，询问谷歌是否已经开始使用免费消费版本 Google Docs、Google Sheets 以及 Google Slides 中的数据，但并未得到明确答案。</p>
<p>Bryant 表示，隐私政策的变更是为了强调并明确谷歌不会在“未经用户明确许可”的情况下，使用 Google Docs 或相关应用程序中的信息来训练语言模型。这只是一项允许用户测试实验性语言模型的自愿计划。</p>
<p>他强调，“我们并没有根据条款内容的变化将其他数据类型用于模型训练。”</p>
<p>Meta 身陷争议Meta 公司首席执行官 Mark Zuckerberg 已经在 AI 领域投资多年，但随着 OpenAI 在 2020 年发布 ChatGPT，他猛然发现自己已经落后于时代。三位现任及前任员工（因未获发言授权而保持匿名）表示，Zuckerberg 决定立即迎头赶上并超越 ChatGPT。他连夜打电话给高管和工程师，敦促他们开发一款与之竞争的聊天机器人。</p>
<p>但到去年初，Meta 遇到了与其竞争对手相同的困境：得不到足够的数据。</p>
<p>从某位员工分享的内部会议记录来看，Meta 公司生成式 AI 副总裁 Ahmad Al-Dahle 曾向高管团队强调，他的团队几乎使用到互联网上所有公开发布的英文书籍、论文、诗歌和新闻文章以训练 AI 模型。</p>
<p>Al-Dahle 告诉同事们，除非获取更多数据，否则 Meta 的模型将无法与 ChatGPT 相抗衡。2023 年 3 月和 4 月，Meta 公司的部分业务开发领导、工程师和律师几乎每天都在开会讨论这些问题。</p>
<p>有人争论要不要以每本书 10 美元的价格买下新书许可权，从会议录音来看，他们还曾讨论收购 Simon &amp; Schuster 出版社（该公司曾出版斯蒂芬·金等作家的作品）。</p>
<p>他们还谈到如何以不经许可的方式从互联网上获取书籍、论文及其他文本，甚至考虑顶着面临诉讼的风险扩大内容获取范围。录音显示，一名律师对于从艺术家手中夺取知识产权提出“道德担忧”，但现场无人给出响应。</p>
<p>Zuckerberg 的态度则非常明确——给我找出解决方案来！</p>
<p>一位工程师表示，“Zuckerberg 想在产品中实现的功能，我们目前根本就做不到。”</p>
<p>两名员工表示，虽然 Meta 运营着庞大的社交网络，但这里并没有丰富的用户帖子可供使用。他们指出，不少 Facebook 用户会删除之前发布的帖子，而且该平台也不以撰写严肃长文为主要卖点。</p>
<p>Meta 当时还身负另一项压力——由于 2018 年与选民分析公司 Cambridge Analytica 共享用户数据的丑闻，其隐私政策刚刚经过调整，实在不宜轻举妄动。</p>
<p>Zuckerberg 在一次投资者电话会议上表示，Facebook 和 Instagram 上公开分享的数十亿视频和照片“比 Common Crawl 数据集还要大”。</p>
<p>在会议讨论中，Meta 高管们谈到如何在非洲聘请承包商来整理当地小说及非小说素材。一位经理也在某次会上指出，这些素材中确实包含受版权保护的内容，“因为我们无法将其彻底剔除。”</p>
<p>Meta 的高管们认为，OpenAI 似乎在未经许可的情况下使用了受版权保护的素材。从录音来看，他们也清楚 Meta 需要很长时间才能跟出版商、艺术家、音乐家和新闻机构达成许可，所以肯定是选择了“先斩后奏”。</p>
<p>全球合作与内容副总裁 Nick Grudin 在一次会议上表示，“唯一阻止我们向 ChatGPT 水平看齐的因素，就是数据量。”</p>
<p>他还补充称，OpenAI 似乎正在使用受版权保护的素材，而 Meta 可以遵循这一“市场先例”。</p>
<p>录音还提到，Meta 公司的高管们同意参考 2015 年作家协会诉谷歌一案的法院判决。在该案中，谷歌被允许对在线数据库内的书籍进行扫描、数字化和缠上，理由是其仅复制了作品的部分片段，并对原件进行了改造，因此属于合理使用。</p>
<p>Meta 公司的律师们则在会上指出，使用数据训练 AI 系统也理应同属合理使用的范畴。</p>
<p>录音显示，至少有两名员工对于使用知识产权且以不公平甚至根本不付费的方式对待作者及其他艺术家表示了担忧。一名员工还与 Meta 公司首席产品官 Chris Cox 等高管人士就版权数据进行过单独讨论，并表示那次会议上没人关注使用他人创意作品产生的道德问题。</p>
<p>“合成”数据面对迫在眉睫的数据短缺难题，OpenAI 的 Altman 专门定下一条妙计。</p>
<p>他在 5 月的会议上表示，像 Meta 这样的公司终将使用由 AI 生成的文本进行训练——也就是合成数据。</p>
<p>这是因为 Altman 及其他高管都相信，AI 模型既然能够生成与人类相似的文本，那就一定可以输出额外的数据来开发更好的模型版本。这将帮助开发人员建立起日益强大的 AI 技术，并减少对受版权保护数据的依赖。</p>
<p>Altman 表示，“只要能够扩大合成数据的涵盖范围，也就是说只要模型足够智能，它就能生成高质量的合成数据，素材短缺问题将迎刃而解。”</p>
<p>多年以来，AI 研究人员一直在探索合成数据的可行性。但构建一套能够自我训练的 AI 系统，明显是说起来容易做起来难。利用自身输出学习的模型往往会陷入死循环——即不断强化自己的倾向、错误和局限性。</p>
<p>前 OpenAI 公司研究员、现任不列颠哥伦比亚大学计算机科学教授的 Jeff Clune 表示，“训练 AI 系统所需要的数据，就如同一条穿越丛林的道路。如果只使用合成数据进行模型训练，那 AI 就很可能在丛林中彻底迷失方向。”</p>
<p>为了解决这个问题，OpenAI 及其他厂商正在研究如何让两套相互独立的 AI 模型彼此引导。这些模型能够协同工作以生成更有用、更可靠的合成数据。其中一套系统负责生成数据，另一套系统则判断信息内容以保障输出质量。但研究人员对于这种方法能否奏效仍然存在分歧。</p>
<p>尽管如此，AI 大厂的高管们一刻也没有停止前进的脚步。</p>
<p>Altman 在会议上拍板，“我觉得应该没问题。” 声明：本文为 InfoQ 翻译整理，未经许可禁止转载。原文链接：</p>
<p><a href="https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html?smid=nytcore-ios-share&amp;sgrp=c-cb">https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html?smid=nytcore-ios-share&amp;sgrp=c-cb</a></p>
<p>内容推荐<br />大模型应用挑战赛已拉开帷幕。现阶段，多数语言模型已完成 3 轮更新，大模型赛道入场券所剩无几。同时，2023 年超 200 款大模型产品问世，典型场景又有哪些产品动向？对于现阶段的文生图产品而言，四大维度能力究竟如何？以上问题的回答尽在《2023 年第 4 季度中国大模型季度监测报告》，欢迎大家扫码关注「AI 前线」公众号，回复「季度报告」领取。<img alt="" src="https://static001.geekbang.org/wechat/images/d2/d2c75750d853a4baaf489be1f840126b.webp" /> 活动推荐AICon 全球人工智能与大模型开发与应用大会暨通用人工智能开发与应用生态展将于 5 月 17 日正式开幕，本次大会主题为「智能未来，探索 AI 无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p>
<p><img alt="" src="https://static001.geekbang.org/wechat/images/24/24cd15257ee2474ae7bd8f17c8c5f308.jpeg" /></p>
<p>今天是会议 9 折购票阶段，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p>
<h5 id="-">今日荐文</h5>
<p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247608636&amp;idx=1&amp;sn=3233d483ec34e32c61b7c2a09de795b5&amp;chksm=fbeb90f3cc9c19e5c3ad8a7330e5c0e6a88295312968577892e4ba5529d3d9799161cd5ad211&amp;scene=21#wechat_redirect"></a><strong><strong><strong>**</strong></strong></strong>你也「在看」吗？<strong><strong><strong><strong><em>**</em></strong></strong></strong></strong>👇<strong>**</strong></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7tM6f86wsgAKVogNm9Hl</id>
            <title>百度沈抖：大模型重构云计算生态，放宽伙伴能力要求</title>
            <link>https://www.infoq.cn/article/7tM6f86wsgAKVogNm9Hl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7tM6f86wsgAKVogNm9Hl</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 04:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型引领, 伙伴市场权益, 生态战略, AI原生应用
<br>
<br>
总结: 百度智能云GENERATE全球生态大会探讨大模型时代的新型云计算生态体系，发布伙伴市场权益和产品权益，助力生态企业AI场景研发，推动大模型产业发展。针对不同市场和伙伴，制定差异化的生态战略，发布一系列产品权益和扶持计划，为合作伙伴提供全方位的支持和保驾护航。 </div>
                        <hr>
                    
                    <p>4月9日，首届百度智能云GENERATE全球生态大会在成都召开。面向大模型引领的智能化升级浪潮，百度智能云携手伙伴共同探讨大模型时代的新型云计算生态体系，并面向头部市场、价值市场、高潜市场三类目标市场，制定差异化生态战略，发布一系列伙伴市场权益。同时，围绕大模型技术栈，在算力、模型、应用开发、应用售卖四大方面，发布一系列产品权益和扶持计划。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/29/ab/2952e42bcbdabe6a59e9ede3ee83bcab.jpeg" /></p><p></p><p>过去一年，千帆大模型平台已经服务了8.5万企业客户，累计精调超过1.4万个模型，开发超过19万个应用，领跑国内大模型市场，合作伙伴从中发挥了重要的推动作用。最近半年，百度智能云大模型伙伴数量大幅增长5倍，超过300款伙伴开发的AI原生应用已经通过千帆AI原生应用商店走向市场。</p><p></p><p>百度集团执行副总裁、百度智能云事业群总裁沈抖表示，大模型极强的泛化能力，为跨行业、跨场景的应用开发提供了通用的、可迁移的基础能力和全新的AI原生应用研发范式，在极大地降低研发门槛、提升研发效率的同时，也为产业生态环境带来根本性的改变。</p><p></p><p>沈抖认为，有别于传统云计算，大模型放宽了对合作伙伴的技术能力要求、成倍放大了市场空间，洞察客户需求、赢得客户信任才是制胜的关键。大量新伙伴即将涌现，借助大模型的能力，去更好地满足客户个性化的、长尾的需求，为客户创造价值。</p><p></p><p>会上，百度智能云还与成都高新区举行了战略合作签约仪式，携手推进区域AI产业发展。通过百度智能云在大模型技术、产品等方面的支持，助力生态企业AI场景研发，推动大模型上下游产业集聚和蓬勃发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3e1d0bbd1d9f1ffbda332def25331b2.jpeg" /></p><p>百度智能云与成都高新区举行战略签约仪式</p><p></p><h3>面向三大市场、四类伙伴，推出差异化的市场权益</h3><p></p><p></p><p>沈抖表示，过去一年，百度智能云主要有两类伙伴。一类是“场景共创型”，主要由头部的咨询、解决方案、服务等“综合型伙伴”，和擅长软件开发的“应用型伙伴”组成。过去一年，百度智能云与“场景共创型”伙伴合作交付项目占比超过50%，未来还将进一步强化合作关系，提升伙伴的参与度。另一类是“用户增长型”，由经销商、分销商、代理商等伙伴构成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13ad2c0fe7d2cf9de569657e6ae6d3d7.png" /></p><p>百度智能云生态战略</p><p></p><p>为了更好的帮助客户落地大模型与AI原生应用，加速实现智能化升级，百度智能云面向三类市场制定差异化的生态战略。</p><p></p><p>头部市场客户业务场景综合且复杂，个性化诉求强，需要具备较强客情关系、深度理解客户所在行业、拥有行业方案与产品的深度整合与交付能力的合作伙伴，与百度进行联合攻坚、相互补位，打造出大模型时代典型的应用案例，为客户创造新价值，树立行业标杆。价值市场客户相比头部市场客户数量更多，业务场景更加标准化，复杂度更低。伙伴可以把面向头部客户打造的行业标杆快速复制，或者提供相对轻量化的定制方案。在此过程中，百度将以“伙伴优先”为原则，做好技术和商务支持，按需参与方案定制化开发，与伙伴共同开拓市场、服务客户。高潜市场客户数量最多，业务场景也更简单，更看重开箱即用的解决方案。面向这类客户，百度将聚焦打磨标准化的产品与解决方案，并给予伙伴足够的让利，保障伙伴利润空间，支撑好伙伴，由伙伴完全主导销售及服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1ff6f4ccccbc535a97d935b31e0e874e.png" /></p><p>百度智能云生态伙伴目标市场权益</p><p></p><p>百度智能云渠道生态部总经理陈之若表示，针对三类目标市场，百度智能云将为四类合作伙伴提供有针对性的市场权益。</p><p></p><p>面向头部市场，发展10+综合型伙伴，提供10亿元定向商机和1000万元专项资金支持。同时帮助综合伙伴加入到百度的交付体系，打造“销服一体”的合作通路，实现与伙伴的互利共赢；面向价值市场，发展100+应用型伙伴，推出针对性的共创计划，为单个应用伙伴提供20万元联合解决方案共创基金，提供专属的AGI House专题培训，帮助应用伙伴提升技术能力，更好地应对市场挑战；面向高潜市场，发展10000+初创企业伙伴，通过提供上云和大模型资源、AI加速器、生态社区等支持，帮助创企解决资源与能力提升难题；针对代理伙伴，提供官网商机支持，并打造完备的佣金与激励体系，真实让利于伙伴。同时，百度智能云还为首单开单的伙伴特别设立了代理激活计划，提供额外奖励扶持。</p><p></p><h3>针对算力、模型、应用开发、应用售卖，发布产品权益“四件套”</h3><p></p><p></p><p>面对全新的市场，百度智能云为伙伴准备了一整套AI原生技术栈，并针对合作伙伴的能力差异、技术与产品集成度，发布了一系列产品权益。</p><p></p><p>在算力方面，百度智能云积累了充足的算力储备和领先的工程技术，能够为伙伴提供高可用、高吞吐、低延迟、稳定易运维的算力服务。大会期间，百度智能云宣布启动智算战略伙伴招募，招募3+算力合作伙伴。伙伴可以成为百度的算力供给方，与百度共建算力中心；承载算力运营服务，享受优质算力价格补贴。百度还将为算力伙伴提供与大模型、AI芯片相关的智算中心合作拓展机会。</p><p></p><p>在模型方面，千帆ModelBuilder提供包括文心一言系列模型在内的77款精选大模型和全流程的模型工具链，并宣布新建全球首个千帆大模型创新实验室，百度智能云将与伙伴一起，围绕10000+个应用场景进行联合创新。在创新实验室内，百度将免费提供产品适配资源和专项技术支持，并为伙伴提供AI原生应用商店的免费入驻加速服务，通过商机支持、产品备案以及交付等一系列权益，为合作伙伴提供全方位的保驾护航。</p><p></p><p>在应用开发方面，千帆AppBuilder给伙伴提供了便捷好用的应用开发平台，支持伙伴在AppBuilder上集成和扩展第三方组件，也可以通过工作流，把多个功能集成到一个可以对话的智能体里。百度智能云也基于千帆开发了一些应用或“半成品”，供合作伙伴直接转售或进一步开发。比如智能编码辅助工具“Baidu Comate”、知识管理平台“甄知”等七大AI原生应用产品，以及面向工业、政务、交通、汽车、金融等行业的技术/产品底座。</p><p></p><p>本次大会，百度智能云还特别推出千帆杯·AppBuilder大模型开发者专项激励政策。通过设立百万级别奖金池、千万级算力资源补贴、权威技术赋能，以及海量商机与流量支持，帮助开发者和伙伴得到更广泛的关注和认可。</p><p></p><p>在应用售卖方面，全新发布千帆AI原生应用商店招募计划，加大对入驻伙伴的扶持力度，提供高额返佣激励和营销资源扶持，持续加强技术指导与项目合作，在2024年实现2000+个优质应用入驻商店，致力于将千帆AI原生应用商店打造成企业选购AI原生应用的首选平台。</p><p></p><h3>一年集结12万生态伙伴，大模型产业落地大幕开启</h3><p></p><p></p><p>陈之若表示，不到一年时间，千帆平台已经汇聚了12万家创企和生态伙伴，活跃调用平台API的伙伴数量超过5.5万，已经有8100家伙伴通过千帆AppBuilder开发AI原生应用。目前，百度智能云通过伙伴服务的大模型头部客户数已经超过了200家。</p><p></p><p>以综合伙伴为例，润建股份是中国领先数字化智能运维服务商。润建股份作为千帆首批生态伙伴，目前已经与百度智能云在能源、政务、教育等领域进行深度合作，并基于大模型打造了智慧城管解决方案，为广西自治区南宁市西乡塘区等53个区县城管局提供智慧服务。</p><p></p><p>另一位综合型伙伴华胜天成，基于千帆大模型平台开发、推出了智能数据助手、智能客服、智能投标大王三款大模型应用产品，为最终客户的营、销、服务等全生命周期提供智慧赋能。</p><p></p><p>雅基软件作为应用伙伴的代表，旗下核心产品Cocos引擎是全球领先的2D&amp;3D引擎。通过接入千帆平台，Cocos整体的智能化水平大幅提升，能够实现游戏中动态生成交互内容，大幅提升某游戏开发效率120%，用户满意度提升50%。为开发者和终端玩家带来了更优质的用户体验，大幅增强用户黏性。在未来，Cocos 引擎还会进一步集成更多 AI 特性，以实现游戏美术资产的高效生产，游戏内语音聊天工具的快速构建等能力。</p><p></p><p>此外，应用型伙伴新致软件还通过接入千帆平台，将行业知识与大模型相结合，帮助华住、如家、锦江之星等连锁酒店的一体化营建供应链，以及膳魔师等价值市场客户打造了数字员工。数字员工能够精准理解用户意图，并执行专业任务，开创了企业运营与营销的全新模式。</p><p></p><p>初创企业万幸科技，基于千帆平台推出了大模型装修助手“装修GPT”，帮助用户花更少的钱，享受到更高效、低成本的设计服务和更高品质的家装建材，一站式解决用户的家装需求。目前，装修GPT已服务了近1000家客户。截止2023年12月，万幸科技收入已达数百万，成为了第一批通过大模型赚到钱的初创企业。</p><p>陈之若表示，大模型为技术创新搭建了无比广阔的舞台，也为企业提供了千载难逢的转型契机。百度智能云将坚定不移地携手伙伴同行，相互成就，实现真正的合作共赢。</p><p></p><p>【活动推荐】</p><p></p><p>在 2024 年 6 月 14-15 日<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">深圳 ArchSummit 架构师峰会</a>"上，我们邀请了 CNCF、顺丰集团、腾讯、百度等企业的专家来演讲。会议上还设置了大模型、架构升级等专题，如果你感兴趣来会议上听演讲，欢迎进入 ArchSummit 会议官网，查看讲师们的详细演讲提纲。</p><p></p><p>会议现已进入 8 折早鸟购票阶段，可以联系票务经理 17310043226 , 锁定最新优惠。扫描上方二维码添加大会福利官，免费领取定制福利礼包。</p><p><img src="https://static001.infoq.cn/resource/image/a7/d3/a7169c8f216f8af139e2f6886de5b8d3.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bNK3WerxFuArA9oLWE0t</id>
            <title>AI 面试的“酷刑”，只有中高级管理层和 CEO 能幸免</title>
            <link>https://www.infoq.cn/article/bNK3WerxFuArA9oLWE0t</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bNK3WerxFuArA9oLWE0t</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 02:59:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知乎, AI面试, 人才画像, 智能判分
<br>
<br>
总结: 知乎上分享了参与AI面试的经历，AI面试已经得到广泛接受，对候选人公平且全面考核。人才画像、关键词评估、情绪稳定度等是AI面试的重点，AI面试适合规模大、标准化的岗位。AI面试评判更加标准化，要求应试者回答清晰、有逻辑。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/wechat/images/65/653b990cba184f7147edd168fcaf6d54" /></p><p></p><p></p><p></p><p></p><p>知乎上有人详细分享了自己参与 AI 面试的经历：</p><p></p><p></p><blockquote>“点开链接，在手机上是用小程序，电脑是浏览器。一开始有个预录，检查手机摄像头、麦克风以及自己的语音声音大小。开始做答之后有 5 个题目。每个题目 6 分钟，包括构思和录制视频。点击录制视频有 5s 的准备时间，5s 之后手机自动录像，你开始答题，视频录制不超过 5 分钟。录制完之后，返回答题页面，可以回放。5 分钟答题时间到，它会提醒提交，然后点击提交。就进入下一题。”</blockquote><p></p><p></p><p>“我觉得 AI 面的公司压根就没打算招人”，这是 AI 面试刚出来时人们通常会给的评价。但现在，大家对 AI 面试的接受度已经很高，AI 面试的争议更多是出现在一两年前，现在网上几乎都是 AI 面试的建议、题库等。</p><p></p><p>“和传统面试比起来，我更倾向于用 AI。AI 对于你的输入、表情、动作都会关注，更加全面考核，至少是对所有候选人是公平的，不会出现面试官主观的偏见偏差、个人因素等问题，当然也得要求 AI 面试要足够智能。”广州大学嵌入式应用专业的一名本科学生说道。</p><p></p><p>根据《2024 牛客智能制造业校园招聘白皮书》，53.5% 的对智能制造感兴趣的学生有参与过 AI 面试，而参与过 AI 面试的学生中高达 77.7% 的人表示满意。</p><p></p><h3>这次，传统行业走在了前头</h3><p></p><p></p><p>面试可以用 AI 进行，也说明了面试本身在一定程度上是可规则化的。</p><p></p><p>有做人力资源管理的网友分析称，抛掉“人间冷暖”不谈，面试的本质是按图索骥，一场高效的面试依赖以下几个方面：</p><p></p><p></p><blockquote>人才画像 ——AI 可以结构化提问 ——AI 可以关键词评估 ——AI 可以情绪稳定度 ——AI 应该可以（可能不精确）与公司文化 / 直接上级的默契度 ——AI 也许可以 （双向测试后可以提升匹配度）如果一家公司坚持不懈地做 AI 面试，积累匹配组织发展的人才大数据，那么招聘的人才至少 80% 左右是完全靠谱的，再加上终面 BOSS 感受一下，精准度应该可以达到 90% 以上，这比有 3-5 年工作经验的 HR 靠谱多了。</blockquote><p></p><p></p><p>当然，AI 面试最终会让类似的人都聚集在一起，很有可能导致组织僵化。在组织需要变革、寻找一些鲶鱼进入组织时，人才画像将完全不一样，AI 的底层面试逻辑就要随之重构。</p><p></p><p>“我曾经只是作为辅助面试人员参与了公司里的终面过程，一整个上午也就搞了六七个人的面试，到中午吃饭的时候，主面试官基本上就快累趴下了。但是在我看来，很多面试官工作内容并不复杂，甚至到后面有一些机械。不管面试者说啥，面试官都是在固定重复问几个问题。”弗兰克扬在知乎上分享称，“我也不觉得这会有什么大问题，因为最关键的信息其实就那么一两个，剩下的就是看你还想从聊天中聊点什么了，不管你聊什么，大部分都不会影响最终的决定。”</p><p></p><p>“弗兰克扬”表示，十多年前，一些大企业会把前几轮的招聘流程给外包公司，这些外包公司先筛选简历、再进行电话面试，问的问题都是雇主公司规定好的，全程录音，然后再根据面试情况做筛选，最终把报告发给雇主做最终决定。“现在 AI 的水平，我感觉跟当年外包干的工作都差不多。”</p><p></p><p>牛客联创兼技术负责人杨之贤介绍称，AI 面试特别适合招聘规模较大、考核能力相对标准的岗位，如校招管培生、普通蓝领、销售岗位、客服群体、小语种岗位等。以互联网为例，IT 基础岗位初筛、产品运营岗位、销售岗位、客服岗招聘人数超 10 人以上均有可能使用 AI 面试。</p><p></p><p>消费行业是最早尝试使用 AI 面试的行业之一，而且外企的接受度更高，比如雅诗兰黛、宝洁等。今年，随着大模型的兴起，越来越多的行业对 AI 面试表现出了兴趣，互联网、国企央企、银行、教育、电信、汽车、快消、制造业等行业都引入了 AI 面试。</p><p></p><p>比如 2023 年，光储行业里的龙头企业阳光电源决定将所有管培生岗位的英语面试全部使用牛客 AI 面试代替以往的英语外包面试。当年，阳光电源 AI 面试的管培生超 500 人次。</p><p></p><p>杨之贤表示，目前国内大约有万级别的企业在面试流程中加入了 AI 面试，而且这一数字还在不断增长。据《牛客 2023 秋季校园招聘白皮书》的调研数据指出，已有 23.2% 的先锋企业应用 AI 助力校招，其中使用 AI 助力笔面试环节的企业占比达 97.9%。</p><p></p><h3>被放大的细节</h3><p></p><p></p><p>AI 面试的优势是不会受到情绪、偏见或其他主观因素的影响，它会根据事先设定的评分标准进行客观评价。AI 面试会重点会考察面试者以下三个能力：</p><p></p><p>胜任力测评，包括学习能力和抗压能力，这些通常是衡量一个人是否适合工作的重要指标。专业能力，即面试者在特定领域的知识和技能。这是根据不同岗位的要求来考察的，以确保面试者具备所需的专业素质。语言能力，比如英语、越南语等，良好的语言能力对于与外企的国际团队合作和沟通至关重要，还有的工作也需要一定外语能力。</p><p></p><p>结合面试者的简历和回答，AI 面试系统会进行提问和追问，并利用大模型的自然语言理解能力和逻辑推理能力，给出面试者对应能力项的得分。</p><p></p><p>“智能判分是基于岗位专业素质、通用能力素质和语言能力素质的综合科学判定。我们会综合考虑面试者回答内容的专业性、相关性和逻辑性等因素，并结合回答状态进行综合判定。”杨之贤说道。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/32b8e68c36897721ed2756d290f87a4a.png" /></p><p></p><p>AI 面试成绩单</p><p></p><p>AI 面试评判更加标准化，这就要求应试者当下就要快速反应，回答得清晰、有逻辑，覆盖题目中所有的问题。“回答得是否高大上不重要，重要的是你的回答是否具有逻辑。”网友“肉丝 er”也分享道。</p><p></p><p>“肉丝 er”也还特别提到，眼神千万不要飘忽不定，有的 AI 能够根据面试者的眼神分辨其是否在读稿，一旦被判定为读稿，那么不管面试者说得多好都会被 pass 掉。这是因为有的 AI 面试系统有眼神追踪功能，四处乱瞟会被视为作弊。</p><p></p><p>“通过分析面试者的微表情，可以更准确地判断其是否在面试过程中存在作弊行为。”杨之贤表示，“这是为了保证面试的公平性和诚信性。”</p><p></p><p>因此，除了专业能力，面试中的语速、情绪、肢体语言等都可能影响 AI 系统对面试者的评分。要知道，虽然 HR 可以查看面试视频，但大多数时候是根据 AI 的评分做初步筛选的。</p><p></p><p>另外，还有一些服务行业的公司会用 AI 进行性格测试。Paradox.ai 的性格测试在 Reddit 上多次疯传，联邦快递、麦当劳等公司都使用 Paradox.ai 公司的 AI 面试系统，通过“漫长而奇怪的性格测验”来招聘客户和食品服务工作人员，并附有“蓝色外星人”形象，目标是发现候选人在“亲和性”和“情绪稳定性”方面的排名。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9b/9b3ed0216cdba8dfa104e7b759c8285a.png" /></p><p></p><p></p><h3>AI 选不了 CEO</h3><p></p><p></p><p>而对于研发这样的 AI 面试系统，最重要的数据训练。</p><p></p><p>据杨之贤介绍，牛客采用了大模型和小模型的结合方式，自研了 AI-NowGPT 模型。为了保证“AI 考官”的公平和专业，牛客内部邀请了大量资深面试官对相同数据做人工标注，以确保 AI 考官在评分和评价方面与人类面试官保持一定程度的同步。</p><p></p><p>有的公司也会邀请客户企业参与标注，这样 AI 考官的评分标准会更加符合客户的需求。客户的参与可以帮助企业微调模型，使其更加符合特定岗位的要求。人类考官也可以对一部分或全部的面试结果进行复核，确保 AI 考官的判分结果的准确性和公正性。</p><p></p><p>像牛客这样还拥有笔面试 SaaS 产品的招聘网站，拥有专门的内容出题团队，借助海量的题库资源，还可以实时定制出题。</p><p></p><p>不过，杨之贤也表示，目前的 AI 面试还只是主要用在初步的大规模快速筛选上，对于后续的面试轮次，不同面试官关注的能力和业务知识可能存在差异，这是 AI 面试目前无法帮助解决的。“AI 选不了中高级管理层，也选不了 CEO。”</p><p></p><p>理想的情况是 AI 系统能够根据面试官的要求自动生成面试方案：面试官口述想要考核的能力，AI 快速提炼出相关考核点，并生成相应的面试问题和评估标准。这样，面试官就能更加专注在候选人的表现上，不需要花费过多精力在准备面试方案上。但凭现在 AI 的能力，在快速提炼考核内容方面还存在比较多困难。</p><p></p><p>此外，AI 面试系统还可以在面试过程中提供一些辅助功能，比如邀约和谈薪资：AI 可以自动发送邀约邮件或短信，并根据候选人的反馈和面试结果，提供薪资谈判的建议。</p><p></p><p>杨之贤表示，未来除面试外，AI-NowGPT 还将增加简历的点评、优化功能，同时提供准确的人岗匹配度评估，帮助企业找到岗位最合适的候选人。</p><p></p><p>然而，人类面试官的亲和力和人际交往能力在面试过程中仍然非常重要。虽然 AI 可以提供面试方案和辅助功能，但在与候选人的互动和评估中，人类面试官的角色仍然不可或缺。他们可以通过面试过程中的非语言交流和深入提问，更好地了解候选人的能力和适应性。</p><p></p><p>此外，电子前沿基金会社区组织副主任 Rory Mir 也指出，当前人工智能浪潮使用的是概率模型算法，这意味着它们只是依赖过去的数据模式做出预测。“问题是，过去数据的模式包括系统性偏见产生的模式。”</p><p></p><p></p><h3>用 AI 打败 AI</h3><p></p><p></p><p>当然，除了招人企业用 AI，面试者也会“用 AI 打败 AI”。</p><p></p><p>Interview Dog 是一款专门的 AI 面试辅助软件 ，可以通过实时语音识别问题来帮助面试者回答考官的问题，支持科技、金融、工程、商业、法律等行业。Interview Dog 主打“按需使用”，五分钟的免费试用之外，每分钟付费 0.45 美元。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2c/2cfd804251a411e76480e36481751983.png" /></p><p></p><p>为此，Interview Dog 在网上也得到了很多好评：“在我大脑短路时，Interview Dog 让我想起了一切”“有了它，谁还需要运气啊！”</p><p></p><p>但直接念答案也是有风险的，遇到自己不回的问题，“假装”思考后对答如流是会被 HR 怀疑的。</p><p></p><p>“我之前面了一个小女孩，答的太完美了。于是我钓鱼了几个问题，她说的和 GPT 的结果八成相似，给她上了点压力后，一个问题都答不出来了。”网友 momo 分享道。</p><p></p><p>对此，有的人认为这种做法并无不妥：“工作不就是借助各种工具完成任务吗？既然面试是模拟工作场景，那用不用 GPT 取决于面试者，只要结果出来就完事了。”但国内大多数企业是不太接受这种说法的，至少像算法思路这种应该是自己能回答的，否则就变成了纯粹的“工具人”。</p><p></p><p>小红书的一个博主发起了“面试应该允许使用 GPT 吗”的小调查，参与的 1207 个人里，47% 的人投给了可以，剩下的人还是认为不应该，“面试不能用 GPT 就跟考 GRE 不能查字典是一个道理。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1b/1b3447be7096907d2244975e8adb69d3.jpeg" /></p><p></p><p>除了这种直接用来应对面试的工具，还有之前普遍被认为“水深”的简历优化。在猎聘等招聘网站上，一对一的简历优化收费达到了 398 元，但有网友给出的评价是：花钱改简历≈抽奖，而且中奖几率极低！</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/80/8074221f44dd8e02e0b7b3cdde90e1a9.png" /></p><p></p><p>猎聘网简历优化收费套餐</p><p></p><p>现在，有的应聘者会用 ChatGPT、文心一言等优化简历，还有人开发了基于 ChatGPT 的简历工具，用户可以直接使用 ChatGPT 自动修改。这种用法，基本不会有什么争议了。</p><p></p><p>未来，面试者和招聘者手里的魔法谁会被打败，似乎也是一个有趣的问题，毕竟 AI 不会站队。</p><p></p><p>观点引用：</p><p></p><p><a href="https://www.zhihu.com/question/649440119/answer/3438486437">https://www.zhihu.com/question/649440119/answer/3438486437</a>"</p><p><a href="https://www.zhihu.com/question/649440119/answer/3438486437">https://www.zhihu.com/question/649440119/answer/3438486437</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Du8obp9eqqIAmcv6isRC</id>
            <title>守住这场公开课，找到 AI 应用安全问题的破解之道！</title>
            <link>https://www.infoq.cn/article/Du8obp9eqqIAmcv6isRC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Du8obp9eqqIAmcv6isRC</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Apr 2024 01:41:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能家居, AI安全挑战, 数据泄露, 网络攻击
<br>
<br>
总结: 当前AI技术在智能家居、自动驾驶、医疗诊断和金融风控等领域的应用潜力正在逐步释放，但同时也面临着安全挑战，包括模型漏洞、数据泄露、网络攻击等问题。因此，必须认识到AI应用的安全性问题，并采取有效措施加以防范。 </div>
                        <hr>
                    
                    <p>现如今，无论是智能家居、自动驾驶、医疗诊断还是金融风控，AI 的潜力正在被逐步挖掘与释放。与此同时，AI 应用的安全挑战问题也亟需得到关注。</p><p></p><p>首先，AI 模型可能隐藏着不易察觉的漏洞，这些漏洞有可能被不法分子利用，进而操控模型输出结果或窃取机密信息。其次，AI 应用的运行高度依赖于大量的训练数据，一旦数据遭到泄露或被篡改，不仅会损害模型的准确性，更可能对用户的隐私构成<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7fc79dbd0756f72b1709ce5e6bd200c2f6e2d66e16f28ee2b70b646b491c16da82d3edaa5&amp;idx=1&amp;mid=2247501117&amp;scene=27&amp;sn=439f11755cffa684d4f4db1dbc7acb98&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">严重威胁</a>"。此外，AI 应用所处的网络环境同样危机四伏，时常面临各种网络攻击和恶意行为的侵扰。</p><p></p><p>在这股技术革新的热潮中，我们必须清醒地认识到，AI 应用的安全性已成为一个亟待关注的重要问题，并需要采取有效措施予以应对。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/07/b7/07443479789a4fd11b94ca64555e29b7.png" /></p><p></p><p>Palo Alto Networks（<a href="https://www.infoq.cn/article/JuoHJYIdsbN0b3YhQYsH?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">派拓网络</a>"）Primsa Cloud将整个应用生命周期中的应用风险、安全信号和运行时环境等重要因素联系起来，无缝集成云原生应用程序保护的各个方面，在整个应用程序生命周期中提供无与伦比的可视性。从代码到云，Prisma Cloud 的智能驱动方法可确保将安全性融入数字化运营结构中。4 月 23 日 19:00-20:00，InfoQ 技术公开课邀请到了两位来自 Palo Alto Networks（派拓网络）的技术专家，他们将以《派拓网络云原生保护平台对 AI 应用的防护》为主题进行分享。如果你对 CNAPP 的工作原理以及 AI 应用的<a href="https://xie.infoq.cn/article/cf40605cd17e0ac3b8224d08f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">安全解决方案</a>"感兴趣，本期直播你不可错过，欢迎扫描海报二维码预约直播！</p><p></p><p></p><h2>主题及讲师介绍</h2><p></p><p></p><p>本期主题：派拓网络云原生保护平台对 AI 应用的防护</p><p></p><p>分享大纲：</p><p>一、云端AI革命与新兴安全威胁</p><p>云对 AI 的赋能多云安全风险AI数据安全AI攻击类型实现 AI 安全的挑战烟囱式工具的安全问题</p><p></p><p>二、Prisma Cloud：保护您的未来AI</p><p>AI云端资产全面可视化自动化的安全策略管理AI应用的软件供应链安全实时保护 AI 负载运行时AI数据安全结语</p><p></p><p>听众收益：</p><p>1、了解 AI 的可能安全风险</p><p>2、了解 CNAPP 对 AI 安全的平台化保护</p><p>3、重新定义云安全，深入了解代码到云智能，如何应对现代云安全挑战</p><p></p><p><img src="https://static001.infoq.cn/resource/image/22/e6/2255c59993853yy8d37a92c88b4289e6.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HPNCz7mLBvBUxvRpHQaa</id>
            <title>创始团队仅3人、估值最高25亿美元，万字长文讲述RISC-V商业帝国崛起背后的故事</title>
            <link>https://www.infoq.cn/article/HPNCz7mLBvBUxvRpHQaa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HPNCz7mLBvBUxvRpHQaa</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 08:17:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RISC-V, 开源指令集架构, 芯片行业, 开放标准
<br>
<br>
总结: 2015年前，英特尔、ARM和AMD主导芯片行业，但RISC-V的开源指令集架构改变了这一局面。RISC-V的出现使得计算机芯片设计变得更加开放和灵活，吸引了众多公司的关注和参与。开放标准的推广不仅降低了成本，还促进了硬件创新，消除了市场摩擦，推动了开源产业的发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>编者按：&nbsp;截至2015年，芯片行业仍然由英特尔、ARM和AMD三大厂商所控制。但现如今，新的选项正在吸引到使用者们的高度关注与热情接纳。其间究竟起了什么变化？答案就是RISC-V发布了用于芯片开发的开源指令集架构，最终扭转了行业态势。&nbsp;近日，一位名为Jaime Arredondo的博主撰写了一篇万字长文，为我们讲述了RISC-V如何一路成长、最终在芯片生态系统中崛起为新生力量的精彩故事。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae70e347d596a48751fa688d9a09dc90.png" /></p><p></p><p>以下为原文翻译：</p><p></p><h2>开放替代方案的诞生</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6512e6347615fce081ae14ca4c18199.png" /></p><p></p><p>RISC-V的故事可以追溯到2010年3月的加州大学伯克利分校。</p><p>&nbsp;</p><p>Krste&nbsp;Asanovic最初只想建立一个“为期三个月的短期项目”，尝试开发RISC-V指令集作为芯片设计的教学工具，以供程序员及其软件控制计算机硬件。他的初始团队不大，除了自己就只有研究生Yunsup Lee和Andrew Waterman两人。</p><p>&nbsp;</p><p>按之前的惯例来讲，学生们在课堂上主要使用专门的CPU（中央处理单元），但这些CPU对于教学场景来说太过复杂、学习起来不够透明，而且其封闭知识产权设计也导致与他人分享研究成果变得极其困难。</p><p>&nbsp;</p><p>另外，他们还发现很多商业产品其实做得也不怎么样，他们自己完全可以做得更好。</p><p>&nbsp;</p><p>因此，Asanovic、Lee和Waterman开始将RISC-V定位为计算机芯片的全新开放标准。开放RISC-V标准意味着你、我、英特尔甚至是任何其他参与者（包括竞争对手）都可以根据RISC-V指令集设计出符合自身标准的计算机芯片。</p><p>&nbsp;</p><p>RISC-V是一种提供免费许可的开放芯片架构。客户可以针对多种应用场景添加更多扩展与定制芯片，包括云计算、人工智能（AI）、移动、汽车、物联网乃至各类工业应用。</p><p>&nbsp;</p><p>此前，如果一家企业需要在设备上部署简单芯片，唯一的选择就是购买现成产品——但这些芯片上往往搭载大量非必要功能，所以要么速度太慢、要么徒耗能源。</p><p>&nbsp;</p><p>而如果一家公司想要为更复杂的硬件或软件用例设计芯片，则必须向英特尔、ARM或者IBM支付数百万美元的许可费，才能访问到其单一设计指令集。</p><p>&nbsp;</p><p>专有许可，意味着只有财力雄厚的大型企业才有资格做出芯片创新探索。</p><p>&nbsp;</p><p>因此，当他们向公众发布指令集时，学术界、各研究机构乃至谷歌、IBM和英特尔等厂商立刻回以激烈响应，迫使他们在2015年成立了RISC-V基金会。</p><p>&nbsp;</p><p>该基金会的目标是维护RISC-V ISA标准，目前成员已经发展至60多家公司，包括高通、三星、阿里巴巴、Meta、英伟达、微软、英特尔、西部数据、IBM以及谷歌等业界巨头。</p><p>&nbsp;</p><p>于是当初那个计划开发三个月的教学项目，现在每年向市场交付数百万个核心。</p><p>&nbsp;</p><p>RISC-V也在逐渐进军创客领域——最初进展缓慢，但势头却逐年增强。</p><p>&nbsp;</p><p>现在我们已经可以在各类商业产品中找到RISC-V的实现成果，包括智能手表、健身手环、存储产品和显卡等。其自由开放的优势，再加上能大大降低许可费用，明显冲破了买家们想要继续选择专有IP厂商的习惯思维。</p><p>&nbsp;</p><p>RISC-V的开源灵活性也使其成为计算存储巨头然后、西部数据以及中国电子商务巨头阿里巴巴等公司热情欢迎的芯片架构，并得到美国国防先进研究计划局（DARPA）等政府部门的认可。</p><p>&nbsp;</p><p>在我看来，如果一个行业没有开放标准，那么创建此类标准即可让以往无法参与进来的各方成为创新力量。公共资方、学术研究人员和私营企业最有能力分享自己使用或开发的标准和工具。&nbsp;</p><p></p><h2>消除市场摩擦，启动开源产业</h2><p></p><p></p><p>在RISC-V出现之前，类似的指令集主要集中在学术和研究环境当中。正如Krste&nbsp;Asanovic在2014年一篇研究论文中所的出，开放指令集的核心优势就是降低软件端的成本，同时刺激硬件创新——这是以往任何封闭或许可指令集都无法做到的。</p><p>&nbsp;</p><p>Asanovic强调称，“鉴于开放标准与开源软件（以及TCP/IP等网络及Linux等操作系统）已经彻底改变了该行业，为什么最重要的接口之一却仍然保持专有？虽然指令集架构（ISA）的专有性质有其历史或商业层面的原因，但至少从技术角度出发，这种自由、开放ISA的缺失并没有必然的理由。”</p><p>&nbsp;</p><p>简单来讲，即使某个行业可能受到知识产权的保护，但如果能将该行业的标准（软件、设计、协议、方法等）面向所有人开放参与和贡献，那么行业内的各参与者本身以及客户也将获得更好的服务。</p><p>&nbsp;</p><p>通常来讲，专有技术的存在大多出于商业或者历史原因，而开放标准的缺失往往与技术无关。</p><p>封闭式许可证存在严重问题，会减缓创新速度并阻碍更多潜在贡献者的参与：</p><p>&nbsp;</p><p>过度摩擦：封闭许可会阻止其他人在缺少许可证的情况下使用该技术，哪怕是愿意支付许可证费用的买家来说，整个谈判周期也往往需要6到24个月，成本很可能在100万美元到1000万美元之间。相当于直接把学术界、初创公司和规模较小的企业粗暴排除在外。缺少新设计：在获得许可证之后，用户无法设计或改编出新版本，只能单纯使用获得授权的设计方案。无法面向未来：如果授权公司倒闭，其知识与设计也会随之消失，导致其技术成果难以长期维护。</p><p>&nbsp;</p><p>哪怕肯定许可证的商业意义，其存在仍会阻止很多人设计和分享新方法，或者交流其探索期间学到的经验教训，最终扼杀竞争和创新。</p><p>&nbsp;</p><p>从统计数字来看，在为RISC-V做出过贡献的参与者当中，有70%之前从未从英特尔、ARM或任何其他芯片厂商处获得过任何许可。类似的情况在其他行业也同样存在。</p><p>&nbsp;</p><p>而在使用开源创新方案的群体当中，有70%表示不会从行业内购买任何东西。表达这种观点的主要是学术界、中小企业和初创公司，但他们同样可以在业内成为重要的创造者、研究者和推广者。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f64efcd9ab77ed58e4c544cd79947c74.png" /></p><p>对于那些承担不起商业许可证费用、或者拿不出6到24个月进行许可证谈判的人们来说，开放标准成功消除了参与创新的最大障碍。</p><p>&nbsp;</p><p>还有一点反直觉的发现，即这种开放创新也同样符合原有企业的最佳利益。</p><p>&nbsp;</p><p>如果他们想要继续保持对标准的控制而拒绝主动开放标准，那么当其他人建立起开放替代方案时，就如同RISC-V对英特尔、ARM乃至AMD所做的那样，买家们就会纷纷投向开放一派的怀抱。</p><p>&nbsp;</p><p>而这种情况在任何行业都尽早会发生。总之，只要竞争对手意识到你掌握有“独门秘笈”，那他们想尽办法也会得到。</p><p>&nbsp;</p><p>所以之前无法参与到芯片研究中来的人们面对专有解决方案的重重枷锁，只会选择两条路：要么建立自己的开放替代方案，要么参与改进现有解决方案。</p><p>&nbsp;</p><p>而一旦意识到这一点，那些获得初步成功的参与者就会把握一切已经公开、不受产权保护的资产，比如建立起能够建立创新良性循环的社区生态。</p><p>&nbsp;</p><p>因此，正如Seth Godin所说，花时间保护金库中的秘密纯属浪费时间。即使你已经掌握很大的领先优势，活跃社区间的协作也将快速取代任何专有技术成果。</p><p>&nbsp;</p><p>Jeff Bezos说得好，“你的利润，就是我的机遇。”</p><p>&nbsp;</p><p>而RISC-V的缔造者们把这话稍微修改了一下，“你的知识产权，就是我的机遇。”</p><p>&nbsp;</p><p>所以如果想要在由少数参与者控制的市场中创造新机会，请首先开发一套能够用于替代原有封闭知识产权的开源方案，而后与其他合作伙伴慷慨分享。</p><p>&nbsp;</p><p>公共资助的研究实验室和大学就是孕育这类机遇的绝佳温床——这一点不仅体现在科技领域，也同样适用于制药或其他知识产权密集型行业。</p><p>&nbsp;</p><p>前面的逻辑看似过于乐观、纯属理论，但对于那些想要激发经济活力和就业机会的国家来说，释放这种待开发潜力确实是提升“自由收益”与产能的有效方式。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/70/70943a064671cd46cd084bf2442d7c5a.png" /></p><p></p><p>开放创新还能让市场产生以往 无法想象的收益。</p><p>&nbsp;</p><p>在知识产权受严格保护的行业中率先发布易于使用且行之有效的开放标准，将为我们自己和他人带来许多新的机遇。</p><p>&nbsp;</p><p>概括：</p><p>将以往封闭的生态转为开放，能够为70%的中小企业创造机会，吸纳这些受能力或业务规模所限而无法参与贡献的力量。市场规模越大、越分散，开源理念就越能帮助客户实现自我支持。如果某项技术或方案对他人来说至关重要，那么开放替代方案就必然会出现，最终导致所有专有解决方案变得过时或边缘化。因此，引领潮流并与行业积极开展合作，才是符合一切从业企业最佳利益的选择。</p><p></p><h2>开放产业如何起步，钱又从哪来？</h2><p></p><p>&nbsp;</p><p>这方面探索有两个关键前提：一是钱，二是许可。</p><p>&nbsp;</p><p>新产业的萌芽往往源自基础研究，但理论研究永远充满风险，毕竟学术成果转化为商业用途的几率往往相当低。也正因为如此，风险资本家、银行和其他私人投资者往往都不愿参与这一阶段，高风险研究通常只能指望公共部门的慷慨资助。</p><p>&nbsp;</p><p>在下图中，我们可以看到从理论研究到大规模部署的各个融资阶段。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1014366ad8262f477f40168911810d1.png" /></p><p></p><p>资料来源：整理自《创业型国家》（The Entrepreneurial State）</p><p>&nbsp;</p><p>为了开发RISC-V协议，伯克利分校从公共部门（通过DARPA和加利福尼亚州）以及私人投资者（通过英特尔、微软及其他行业赞助者）处获得了资金，这些投资方在五年之内捐赠了1000万美元，希望这套新架构能够在并行计算领域取得进展。</p><p>&nbsp;</p><p>RISC-V并非新鲜技术，其开发原则仍基于至少40年前的计算机架构，因此没有申请任何专利。</p><p>但正是凭借着开放这一特殊属性，RISC-V开始在全球范围内迅速普及。</p><p>&nbsp;</p><p>RISC-V并未取得巨大成功，因为它只能算是一项出色的芯片技术。当时市面上还有很多更好的选择。其真正的特别之处，在于它是首个全球开放标准，允许任何人据此自由开发硬件以运行软件。</p><p>&nbsp;</p><p>这种开放许可模式还允许希望在商业产品中实施或扩展RISC-V的组织能够灵活使用，不必向整个社区披露其更改细节，因此对于嵌入式设备的商业用例形成了强大的吸引力，在很大程度上替代了需要向ARM支付许可费用的同类设计方案。</p><p>&nbsp;</p><p>因此，对于民众及公共机构来说，基础研究的公共资助与开放许可属性至关重要。</p><p>&nbsp;</p><p>RISC-V诞生之后，很快得到一系列项目的应用，包括DARPA资助的诸多其他研究项目以及多家企业的相关用例。</p><p>&nbsp;</p><p>RISC-V的开放许可证成为支持性基础设施，开始为以往无法使用英特尔、ARM或AMD付费许可证进行测试的各类研究课题提供动力。</p><p>&nbsp;</p><p>尽管DARPA在RISC-V ISA立项之初并非第一时间参与，但其资助在随后的发展中发挥了重要作用。</p><p>&nbsp;</p><p>DARPA目前仍在资助一系列关于开源硬件技术的项目。</p><p>&nbsp;</p><p>其他高校及公共资助者也由此吸引了教训，意识到通过支持开放标准的制定，他们的研究和投资将有机会发挥更大作用，并通过创造就业机会、税收以及提高劳动力技能的方式为经济发展贡献力量。</p><p>&nbsp;</p><p>开放创新还能带来网络效应。使用相关技术成果的人越多，其效果也就越好。而且随着越来越多的人为原创技术做出贡献和改进，其价值也将随之提升。基于这种开放式创新思维，免许可制造与创新分发的规模愈发可观，随之产生的收入也开始滋养整个生态体系。</p><p>&nbsp;</p><p>开放标准对纳税人来说也非常有利，因为它让任何人都能参与创新并打造出质量更高、价格更实惠的产品，再不必担心被卷入闻之令人胆寒的专有标准诉讼。</p><p>&nbsp;</p><p>在专有知识产权驱动的行业中，制定开放标准还能推动新晋参与者在现有行业中蓬勃发展，由此打破传统垄断或寡头体制。相较于让少数独角兽公司掌握巨大的全球影响力，开放标准培养出由成百上千家小公司组成的开放生态系统，通过自由竞争杜绝了“太大而不能倒”的组织，创造出更具弹性的健康市场。</p><p>&nbsp;</p><p>以上可以概括为两点：</p><p></p><p>开放标准最有可能从高风险研究、高校以及通过公共拨款及信贷支持的公共资金中产生，也只有这类项目才能承受理论研究对应的高风险。以开源方式分享研究成果非常符合国家服务公共利益及发展健康市场、经济活动及就业空间的使命。宽松的开放许可证不强迫各参与方披露其商业产品变更，这是增强商业公司采用技术成果并吸引他们在开放标准的开发和维护层面做出贡献的好办法。</p><p></p><h2>开放的行业需要怎样的底层社区？</h2><p></p><p></p><p>为了让RISC-V标准得到开源社区的认可，项目创始人们认为它必须通过商业运营这道考验。</p><p>&nbsp;</p><p>为了展示其能力，创始人们在技术报告《保持自由指令集的意义：以RISC-V为例（Instruction Sets should be Free: The case for RISC-V）》中，解释了计算行业如何从可行的自由开放标准芯片协议中受益，其原理与从自由开源软件中受益一样。</p><p>&nbsp;</p><p>例如，这将建立起真正自由且开放的处理器设计市场，而许可标准这副镣铐则会阻止市场的健康发展与自我完善。</p><p>&nbsp;</p><p>他们认为开放标准将带来：</p><p>&nbsp;</p><p>在更多设计者的参与之下，通过自由市场竞争扩大创新规模，包括各类开放与专有实现方案。共享开放核心设计意味着上市速度更快、重用成本更低、错误更少（因为参与者的关注度更高）以及透明度更强，可保证政府机构难以在其中加入秘密后门。对大多数设备来说，处理器价格将越来越便宜，而设备成本被广泛控制在1美元左右将有助于推动物联网的实际落地。</p><p>&nbsp;</p><p>因此，本文将向大家解释开放标准为何意义重大，有望建立起让每位参与者都能受益的独角兽生态系统。</p><p>&nbsp;</p><p>大家可能都听说过那些独角兽初创公司，这个概念是指估值达到10亿美元及以上的私营初创企业。</p><p>&nbsp;</p><p>但我所定义的独角兽生态系统，是指由独角兽初创企业参与的开放式转变。独角兽生态系统代表能够创造10亿美元或更高收入市场的解决方案。但这些技术方案由开源项目及社区所驱动，并非由单一初创公司所实现，因此期间将有众多参与者、而非单一参与者随之快速成长。</p><p>&nbsp;</p><p>下面来看一些真实案例，这些组织通过允许其他人基于自己的方案进行构建，最终创造出了单靠自身商业化所无法企及的总体价值回报：</p><p>&nbsp;</p><p>Linux支持99%的互联网服务器，业务总体量超过500亿美元。WordPress得到43%的互联网网站使用（截至2023年1月），其公司价值70亿美元，但每年由其用户创造的收入约达1400亿美元。RepRap为3D打印企业们提供数十亿美元的收入，并催生出总值数百亿美元的3D打印市场。RISC-V是一种开放芯片架构生态系统，其业务规模每年翻一番，预计将从2018年的5200万美元增长至2025年的11亿美元。</p><p>&nbsp;</p><p>独角兽初创企业最受私人投资者的欢迎，因为他们可以把成果打包出售给那帮行业巨头。但出售独角兽生态则既不轻松、也不太划算，毕竟它们是由众多开源解决方案所共同构成。</p><p>&nbsp;</p><p>但另一方面，独角兽生态系统对公共资助者来说则非常合适，因为由此衍生出的研究成果随后可以转化为市场，创造新的就业机会、新的技能、新的税收、更高的产品质量，也能比封闭知识产权或独角兽初创公司更快降低技术普及成本。</p><p>&nbsp;</p><p>也就是说，加入这样的生态系统，能够让新的开放参与者站在巨人的肩膀上，享受原本只供独角兽们享用的技术投资。RISC-V正在取代英特尔和高通，而投资像SiFive这样的RISC-V初创企业也同样能够带来不错的价值回报。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/75/7545df6dad88c8c2e908b26d55ca9cfa.png" /></p><p></p><p>&nbsp;</p><p>也就是说，如果大家想围绕开放标准建立起产业生态系统，就必须展示其实践可行性。而这种可行性的根源，就是通过向新参与者开放市场来增加创新、减少对个别垄断者的依赖、扩大定制范围、降低技术实现成本、加快上市速度或降低价格价格，最终让成果能够为更多人所轻松接受。</p><p>&nbsp;</p><p>此外，通过建立产业生态系统，公共投资者可以创造出以往不可能存在的新市场和新的竞争关系，同时提高质量并降低价格。同时，由此建立的新市场将带来更公开的竞争与更平等的参与机会，打破少数私营企业垄断产业的可能性。</p><p></p><h2>如果没有知识产权保障，生态系统成员如何谋求生存、稳定发展？</h2><p></p><p></p><p>但对于开放产业，人们普遍抱有这样的疑问：在缺少专有知识产权保护的情况下，参与者凭什么获得资金和发展空间？</p><p>&nbsp;</p><p>要回答这个问题，我们不妨看看SiFive的RISC-V创业之路。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b92dbc9d50f459d637cd8e5faedab805.png" /></p><p></p><p>SiFIve是Krste&nbsp;Asanovic创立的公司。在研究RISC-V架构的同时，他们也做出了很多实现，并让多位学生参与到项目中来。正是这种方式，保证外部人员也能轻松上手并利用RISC-V完成实际芯片开发。</p><p>&nbsp;</p><p>他们意识到半导体行业正身处一场完美风暴当中——摩尔定律即将结束，新技术的开发变得越来越昂贵，能够实现新设计并从中盈利的公司也将越来越少。</p><p>&nbsp;</p><p>因此，Asanovic其他几位联合创始人开始围绕RISC-V提供咨询服务，这也帮助他们感受到了为物联网开发定制芯片这一持续增长的市场需求。所有这些设备都需要搭载处理器，而各类解决方案又不可能全部采用相同的处理器，因此他们决定把咨询业务做成一家企业。</p><p>&nbsp;</p><p>在这部分市场中，他们牢牢把握住了核心原则：芯片产品的需求总量确实在增长，但这种增长会体现在众多彼此分散的市场当中。</p><p>&nbsp;</p><p>传统半导体商业模式（根据一种设计方案制造并销售数百万块处理器）只适用于计算机和移动手机市场，未来的市场形态将转化为成百上千种需求量相对较小的设计方案。</p><p>&nbsp;</p><p>在SiFive公司，他们开始研究其技术原理。芯片的传统用户正在转型为新的制造商。谷歌、微软、亚马逊等众多巨头都在设计和制造自己的芯片——不是为了出售给他人，而是要在自己的产品中使用。也只有这样，他们才能实现标准商用芯片所不具备的功能。</p><p>&nbsp;</p><p>因此，SiFive的机会就是找出如何帮助小型企业和初创公司进行定制化芯片设计，进而发明出具有新功能的新型处理器。</p><p>&nbsp;</p><p>Asanovic表示，“我们坚信那是一片拥有广阔开发空间的新大陆。但问题在于开发定制化芯片设计的门槛太高，很多伟大的想法根本不可能转化为产品。而SiFive的目标，就是解决好这个问题。”</p><p>&nbsp;</p><p>于是SiFive的商业模式就成了：快速开发新型芯片组，并帮助客户以极低的成本将其投入生产。</p><p>&nbsp;</p><p>这无疑为创客、初创公司和中型企业开辟了新的视角。芯片的所有设计文件都公开在GitHub之上，这种开放性在半导体行业可谓是前所未见。</p><p>&nbsp;</p><p>不需要保密协议或者律师，SiFive的商业模式天然成本低廉，自然意味着为客户提供更多实惠。</p><p>&nbsp;</p><p>这大大降低了原型设计的前期投入，使得初创公司、制造商甚至是技术爱好者也能负担得起自己的第一块芯片，在其中试验自己的技术灵感。</p><p>&nbsp;</p><p>其中大多数尝试当然会失败，但这没关系。</p><p>&nbsp;</p><p>只要尝试的人足够多，新想法的数量和质量就会提高，从而增加案例取得成功的几率。对于那些已经成功的公司，SiFive会与他们一同扩展并帮助向客户每年交付数百万块芯片。</p><p>&nbsp;</p><p>这就是他们赚钱的方式。他们把赌注押在客户提出的成千上万好点子上，而非依靠自己能够想到的少数设计路线。</p><p>&nbsp;</p><p>接下来要做的，就是引导客户为公司的发展付费。可有时候，这等体量的收入并不足以支持一家正在快速增长的公司，特别是SiFive这种立足老牌烧钱产业半导体行业的公司。</p><p>&nbsp;</p><p>归根结底，投资者为什么要支持缺乏知识产权保护的项目？</p><p>&nbsp;</p><p>总结来说，就是只要投资者能证明对开源技术的需求仍在不断增长且公司的市场定位足够强大，他们就有信心掏出真金白银支持开源技术。</p><p>&nbsp;</p><p>从2019年到2022年，SiFive的业务规模增长了1477%，也借此从投资者手中顺利接过3.655亿美元。</p><p>&nbsp;</p><p>还有更多以RISC-V为基础的初创公司拿到了私人融资，包括Hex Five、Codasip和Dover Microsystems。此外Ant Micro和Andes Technology则在不借助外来融资的情况下，茁壮成长为初创企业。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/db/dbdfbb83ebfd7ec50593994f27aaf0a7.png" /></p><p></p><p>来自RISC-V生态系统中另一家快速增长的企业——Codasip公司的芯片。</p><p></p><h2>怎样驱动生态系统进行自我维护？</h2><p></p><p></p><p>在启动开源项目之前，很多领导者往往心存疑虑：虽然已经了解开源项目的价值，但他们不希望项目乃至整个生态系统的管理和维护任务把自己硬生生拖垮。</p><p>&nbsp;</p><p>而让社区为此主动贡献其实非常困难，毕竟这是一项要求贡献者付出大量资源和时间的工作，根本不可能随时随地找到适合的人选。</p><p>&nbsp;</p><p>RISC-V就是个很好的例子，说明了如何驱动社区参与，以及为何需要有意识地加以引导。</p><p>让我们首先看看RISC-V技术社区是如何形成的，之后再了解他们在邀请和激励成员参与方面采取的具体步骤。</p><p>&nbsp;</p><p>RISC-V基金会成立于2016年，旨在建立一个基于RISC-V标准的开放、协作软硬创新者社区。</p><p>该基金会是一家由其成员控制的非营利公司，发展方向在于推动RISC-V的初步市场采用。</p><p>2018年11月，RISC-V基金会宣布与Linux基金会合作。</p><p>&nbsp;</p><p>作为此次合作的一部分，Linux基金会为RISC-V International提供运营、技术与战略支持，具体涵盖成员管理、会计、培训计划、基础设施工具、社区外展、营销、法律及其他开放标准服务与专业知识。</p><p>&nbsp;</p><p>而且考虑到标准的开放性，以基金会的形式建立社区也可以保护各成员的投入免受地缘政治的干扰。</p><p>&nbsp;</p><p>2018年至2019年，RISC-V成员曾因出口黑名单而受到中美紧张关系的打击。这种不确定性在全球各地纷纷引发担忧，参与者希望对RISC-V的投资能够保证知识产权访问的连续性，否则很难有信心为其长期规划战略投资。</p><p>&nbsp;</p><p>2020年，RISC-V基金会决定迁入瑞士，并转向更具包容性的会员结构，借此平息人们关于开放合作模式可能受到政治干扰的担忧。</p><p>&nbsp;</p><p>RISC-V组织目前约有三分之一成员来自北美，三分之一来自欧洲，37%来自亚太地区。</p><p>&nbsp;</p><p>亚太地区的成员也代表着RISC-V增长最快的应用场景——印度和巴基斯坦等国家已经采用RISC-V作为本国芯片开发的指定指令集架构。</p><p>&nbsp;</p><p>2019年，RISC-V标准的创始作者及所有者放弃所有权，并将权利转让给RISC-V基金会。曾担任IBM副总裁领导开放基础设施项目的Calista Redmond被任命为基金会CEO。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c50556ea46cac62dc8546ecee93e512a.png" /></p><p></p><p>2022年，RISC-V基金会已经成为一个拥有1400多名成员的强大社区，由各国芯片制造商、设计商及学术机构组成，致力于通过开放标准协作为未来50年的计算机创新与设计制定联合路线图。</p><p></p><h3>这种合作是如何实际运行的？谁可以参与基金会并投票表态？</h3><p></p><p>合作的第一步，就是构建起各方赖以完成工作的必要技术方案。RISC-V是这样走的，许多其他开源项目（包括Linux、WordPress、Wikimedia以及众多开放编程语言）也是这样走的。</p><p>第二步是了解各参与方共同关注的问题和愿景。</p><p>&nbsp;</p><p>RISC-V基金会在这方面做得相当不错，其“成为会员”页面上解释了加入基金会的好处：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/67/6723c131cf340848135507ea58ef350e.png" /></p><p></p><p>&nbsp;对RISC-V的成员们来说，聚集在基金会或者联盟周围，有助于各成员紧密围绕开放标准共同努力、投入资源以加速整个行业的发展。</p><p>&nbsp;</p><p>当然，根据会员角色建立的不同会员级别及相关资质也非常重要。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6e7be0237107d66055d78b743c1adc4.png" /></p><p></p><p>RISC-V主要面向企业、学术界、非营利组织或个人，根据员工数量及法人实体类型，各个组织均有不同的福利与会费额度。</p><p>&nbsp;</p><p>具体会员资质将随着社区成熟度以及所涉及的商业利益而动态变化。</p><p>&nbsp;</p><p>2016年，RISC-V会员可免费申请，等待审查后即可通过。2018年，他们开始将会费调整为个人免费、商业公司2.5万美元。2020年，他们将大企业的会员费增加至25万美元，并保留个人及学术界成员的免费资格。这笔费用保证基金会能够向RISC-V的15名员工（截至2023年）支付工资，并用于促进、维护和连接围绕该标准的生态系统、文档与技术的持续发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c71324d4a17ba5e5ef9d313130c7301.png" /></p><p></p><p>各成员随后可以定期参加技术与社区会议、加入或提交项目贡献，也可访问合作伙伴及其他精选来源的学习资料。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/993953a40f313725ecc739ac8ad25a26.png" /></p><p>&nbsp;</p><p>基金会之下还有多个不同工作小组负责技术任务、营销、编程研讨会或其他特别兴趣小组，具体涵盖汽车、学术界、Android或图形等领域。</p><p>&nbsp;</p><p>RISC-V基金会成员生产的产品归谁所有？谁又可以使用？</p><p>&nbsp;</p><p>RISC-V的知识产权由各成员共同开发并贡献。一旦知识产权在全球范围内以开源方式被用于软件及硬件设计，内容就会被永久开放并供所有人随时查阅。</p><p>&nbsp;</p><p>然而，只有RISC-V International的成员才能投资批准变更，且只有会员组织才能使用注册商标及徽记。</p><p></p><h2>总结</h2><p></p><p>RISC-V成功与最具行业影响力的参与者们合作，将开放标准转化成了蓬勃发展的生态系统。</p><p>他们服务于快速增长的芯片处理器市场，并为整个世界做出了巨大贡献。</p><p>&nbsp;</p><p>RISC-V已经成为行业中的重要联盟，证明无需封闭知识产权也能发展出欣欣向荣的产业与充满活力的初创公司。但前提是，必须设定明智且目标明确的发展计划。</p><p>&nbsp;</p><p>结合RISC-V的现实策略，我们也许可以把握以下几个要点以供所在组织借用、修改和调整：</p><p>&nbsp;</p><p>要点一：如果所处行业依赖于封闭协议及工具，请投资建立开放协议及工具。此举有望向以往无法参与的潜在贡献者敞开大门，扩大合作范围以推动创新与市场发展。</p><p>&nbsp;</p><p>要点二：向那些仍依赖于封闭协议或工具，以及希望使用但却无法承担成本的各方分享新的开放协议或工具。如此一来，就能引导各方在开放项目的改进与扩展方面进行协作、相互扶持。</p><p>&nbsp;</p><p>要点三：提供开放许可证，允许他人出售开放项目的衍生品，且不强迫他们披露变更细节。这增加了开放协议被商业公司采用的机会，有助于鼓励他们开发并维护开放标准。</p><p>&nbsp;</p><p>要点四：建立产业生态系统的前提，在于明确解释开放协议或工具的可行性。开放标准有何作用？向新晋参与者开放市场是否有助于扩大创新规模？是否有助于减少对少数巨头的依赖？是否有助于扩大可定制空间？是否有助于降低成本或者加快上市速度？或者能以更低的价格向更多人提供产品？这种种助益相结合将形成不可阻挡的普及之力。</p><p>&nbsp;</p><p>要点五：为了让生态系统参与到开放项目的维护和资助中来，请邀请那些依赖开放项目并愿意做出开发贡献的个人/实体。根据相应的商业利益、能力和动机，具体设计适合各参与方的会员级别、角色和会费。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://boldandopen.substack.com/p/how-a-group-of-berkeley-researchers">https://boldandopen.substack.com/p/how-a-group-of-berkeley-researchers</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0K8ODRJO9JqjMi2lWHHP</id>
            <title>Devin发布半月后，开源领域围攻编码智能体 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/0K8ODRJO9JqjMi2lWHHP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0K8ODRJO9JqjMi2lWHHP</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 08:13:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 编码智能体, 开源领域, 科研领域
<br>
<br>
总结: 大模型的快速发展使得了解最新技术成为必修课，编码智能体和开源领域的项目不断涌现，科研领域也有新突破。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，普林斯顿大学&nbsp;NLP&nbsp;组发布了&nbsp;SWE-agent，这是继&nbsp;Devin&nbsp;发布之后，编码领域智能体的又一突破。在这短短的一月内，Devin、OpenDevin、Devika、Autodev、SWE-agent等项目集中攻克编码智能体（Code&nbsp;Agent）方向，编码领域已经成为智能体首要探索的领域。编码智能体是&nbsp;Copilot&nbsp;模式产品的下一个阶段吗？编码智能体商业化应用仍然面临经济成本账和用户体验的问题，这会是智能编码体下一步重点攻克的方向吗？AI&nbsp;agent还会有其他典型领域突破吗？InfoQ研究中心与大家一起关注。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>4&nbsp;月&nbsp;5&nbsp;日，来自MIT、普林斯顿等高校的华人团队推出了性价比极高的大语言模型&nbsp;JetMoE-8B。其在推理过程中只有&nbsp;22&nbsp;亿个激活参数，实现了不到10万美元的训练成本。尽管预算有限，JetMoE-8B&nbsp;却展现出了超越&nbsp;Meta&nbsp;LLaMA2-7B&nbsp;的能力，后者拥有庞大的训练资源。</p><p></p><h4>开源领域</h4><p></p><p>4&nbsp;月&nbsp;1&nbsp;日，澜舟科技开源了&nbsp;孟子3-13B&nbsp;大模型。该大模型在中英文语言、数学、编程方面表现较为出色，同时支持学术研究与免费商用。4&nbsp;月&nbsp;2&nbsp;日，通义千问团队推出&nbsp;Qwen1.5-32B&nbsp;和&nbsp;Qwen1.5-32B-Chat。相较于其&nbsp;72B&nbsp;的模型，32B&nbsp;模型的内存占用大幅减少，运行速度显著提升。通义千问团队希望通过Qwen1.5-32B的开源，能为企业和开发者提供更具性价比的应用落地模型选项。4&nbsp;月&nbsp;3&nbsp;日，元象开源了&nbsp;XVERSE-MoE-A4.2B&nbsp;大模型。该大模型采用&nbsp;MoE&nbsp;结构，其激活参数量为&nbsp;42&nbsp;亿。相比于XVERSE-13B-2&nbsp;大模型，减少了&nbsp;70%&nbsp;的计算量与&nbsp;50%&nbsp;的训练时间。</p><p></p><h4>科研领域</h4><p></p><p>斯坦福大学和麦克马斯特大学的研究人员开发了用于设计抗生素分子的生成式&nbsp;AI&nbsp;模型&nbsp;SyntheMol。该模型通过蒙特卡洛树搜索技术和大量的分子片段库中的数据，快速筛选出具有潜力的化合物，显著提高了新药发现的效率和成功率，为未来抗生素的研发提供了新的方向。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>OpenAI在官方网站上推出其创新的自定义声音合成技术&nbsp;Voice&nbsp;Engine。这项技术允许用户仅通过提供一段大约15秒的声音样本，便能够创造出与原始声音极为相似的全新音频文件。PixVerse&nbsp;推出了“角色-视频”新功能，允许用户上传图片并生成保持角色一致性的动态视频。这项功能通过丰富的背景和角色动态，使生成的视频内容生动且连贯。虽然有时生成效果会与原素材有一定差异，但是可以通过调整&nbsp;prompt&nbsp;进行改善。香港中文大学（深圳）附属第二医院使用中文医疗大模型华佗&nbsp;GPT&nbsp;进行智能导诊服务。未来，除了智能导诊外，华佗GPT&nbsp;还将在医院探索智能预问诊、专科咨询、随访、病案之间等应用场景。昆仑万维开放天工&nbsp;SkyMusic&nbsp;AI&nbsp;音乐生成大模型测试邀请，并计划于4月17日全面向社会开放使用。</p><p></p><h4>智能体</h4><p></p><p>普林斯顿大学&nbsp;NLP&nbsp;组发布了开源的AI程序员系统&nbsp;SWE-agent。它能够在GitHub存储库中自主解决问题。基于GPT-4&nbsp;等大模型，SWE-agent&nbsp;在&nbsp;SWE-bench测试集上达到了与闭源&nbsp;AI&nbsp;程序员&nbsp;Devin&nbsp;相似的准确度。SWE-agent&nbsp;通过智能体-计算机接口（ACI）设计，可以执行代码浏览、编辑和执行等任务，显著提高了软件开发过程中的自动化水平。阿里云正在内部积极推广通义灵码的智能系统来协助程序员进行代码编写、阅读、BUG&nbsp;检测和代码优化等多项任务。相关人士表示，未来&nbsp;20%&nbsp;的代码将由通义灵码编写，程序员将更多集中在系统架构设计和关键业务开发的工作。优必选正在探索将其人形机器人Walker&nbsp;S与文心大模型相结合，以此提升Walker&nbsp;S在具身智能领域的应用能力。通过整合文心大模型，Walker&nbsp;S不仅保持了其原有的多模态感知和运动控制功能，还新增了更为先进的意图识别和细致规划的能力。</p><p></p><h3>基础设施</h3><p></p><p>3&nbsp;月&nbsp;31&nbsp;日，无问芯穹团队首次召开产品发布会，推出无穹&nbsp;Infini-AI&nbsp;大模型开发与服务平台并向个人与企业开放注册并进行了客户案例展示。该平台实现多模型与多芯片间的软硬件协同优化和统一部署，并支持二十余个主流模型与十余种计算卡。谷歌更新&nbsp;Transformer&nbsp;架构，推出&nbsp;Mixture-of-Depths（MoD）。MoD&nbsp;架构的核心创新在于动态分配计算资源，以便在模型中跳过一些不必要的计算，从而显著提高训练效率和推理速度。这种方法通过在输入序列的特定位置优化不同层次的模型深度中的资源分配，使模型能够专注于更重要的信息。华为诺亚方舟实验室推出了一种新的大语言模型架构帝江，该模型基于频域自注意力变换核，实现了原始自注意力的线性逼近。帝江模型在保持与&nbsp;LLaMA-7B&nbsp;相当的精度的同时，仅需&nbsp;1/10-1/50&nbsp;的训练数据，实现了最多&nbsp;5&nbsp;倍的推理加速。北京大学的团队在论文《Hourglass&nbsp;Tokenizer&nbsp;for&nbsp;Efficient&nbsp;Transformer-Based&nbsp;3D&nbsp;Human&nbsp;Pose&nbsp;Estimation》中提出&nbsp;HoT&nbsp;框架。这是一种高效的三维人体姿态评估的框架，通过沙漏&nbsp;Tokenizer&nbsp;来减少视频姿态&nbsp;Transformer&nbsp;的高计算量。HoT&nbsp;能够集成到现有&nbsp;MotionBERT&nbsp;等模型中，在不损失精度的情况下降低近&nbsp;40%&nbsp;的计算量。UIUC与LMFlow团队在论文《LISA:&nbsp;Layerwise&nbsp;Importance&nbsp;Sampling&nbsp;for&nbsp;Memory-Efficient&nbsp;Large&nbsp;Language&nbsp;Model&nbsp;Fine-Tuning》中针对&nbsp;LoRA&nbsp;的局限性进行了研究，并提出了一种新的微调方法&nbsp;LISA（Layerwise&nbsp;Importance&nbsp;Sampled&nbsp;AdamW）。LISA&nbsp;的空间消耗与&nbsp;LoRA&nbsp;相当或更低，计算速度比&nbsp;LoRA&nbsp;快50%，并且由于其参数激活较少，对更深的网络和梯度检查点技术更为友好。同时，LISA&nbsp;的收敛性质更优，并且理论性质更易于分析。</p><p></p><h3>其他</h3><p></p><p>国家网信办于&nbsp;2024&nbsp;年&nbsp;4&nbsp;月&nbsp;2&nbsp;日公布了已备案的生成式人工智能服务大模型信息。截至&nbsp;3&nbsp;月&nbsp;28&nbsp;日，共有&nbsp;117&nbsp;个大模型完成了备案程序。其中，北京有&nbsp;51&nbsp;个，上海有&nbsp;24&nbsp;个，广东有&nbsp;19&nbsp;个。4&nbsp;月&nbsp;1&nbsp;日，OpenAI&nbsp;放开使用限制，用户无需注册即可使用&nbsp;ChatGPT。在放开注册使用限制的同时，OpenAI&nbsp;也加强了内容保护措施，以防止某些不适宜的内容被用户生成。此外，基于GPT-3.5&nbsp;的ChatGPT仍保持免费使用外，用户可以订阅付费以使用&nbsp;GPT-4。</p><p></p><p></p><p>报告预告</p><p>Sora来袭，国内如何迅速跟上？开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，其能力是否有所提升和刷新？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？InfoQ研究中心即将发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，即将给出答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9b3c569c62a571715d811e7121db70f.png" /></p><p></p><p></p><p>每周动态更新和季度报告后续均会在&nbsp;AI&nbsp;前线上发布，欢迎持续关注&nbsp;AI&nbsp;前线公众号，共同见证大模型行业的发展与突破！</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/373298ec65b194910c71edca6409a559.png" /></p><p></p><p></p><p></p><h4>活动推荐</h4><p></p><p>AICon&nbsp;全球人工智能与大模型开发与应用大会暨通用人工智能开发与应用生态展将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1a0f9425899db37a9189885eeca9625.jpeg" /></p><p></p><p>今天是会议&nbsp;9折购票阶段，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gcoRcFByO7NLUP7dfLfh</id>
            <title>成立7年0交付，这家OpenAI投资的自动驾驶软件公司烧光15亿元后宣布倒闭</title>
            <link>https://www.infoq.cn/article/gcoRcFByO7NLUP7dfLfh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gcoRcFByO7NLUP7dfLfh</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 08:11:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 自动驾驶汽车软件公司, 倒闭, 裁员, OpenAI
<br>
<br>
总结: 自动驾驶汽车软件公司Ghost宣布倒闭，裁员上百人，曾受到OpenAI投资关注，但由于资金短缺和技术支持不足，最终倒闭。Ghost Autonomy公司成立于2017年，致力于开发自动驾驶软件，但在自动驾驶领域竞争激烈，面临资金压力。公司创始人John Hayes曾创立成功上市的Pure Storage，希望通过软件实现自动驾驶技术。公司曾获得大额投资，但最终转向碰撞预防技术，仍未能维持业务。 </div>
                        <hr>
                    
                    <p></p><h2>自动驾驶汽车软件公司Ghost宣布倒闭，裁员上百人</h2><p></p><p>&nbsp;</p><p>近日，据多家外媒报道，OpenAI投资的一家自动驾驶公司现已关闭。</p><p>&nbsp;</p><p>这家总部位于加利福尼亚州山景城的Ghost Autonomy 公司成立于2017年，是一家为汽车制造商合作伙伴开发自动驾驶软件的初创公司。</p><p>&nbsp;</p><p>该公司上周在其网站上宣布，“Ghost Autonomy 已于 4 月 3 日关闭全球业务并关闭该公司。我们对 Ghost 团队在实现软件定义消费者的使命中所取得的实质性技术创新和进展感到自豪。考虑到当前的融资环境以及自主开发和商业化所需的长期投资，长期盈利之路尚不确定。我们正在为我们团队的创新探索潜在的长期目标。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6a9f00bca6e344f4d5b8e0137bc1c0a.png" /></p><p></p><p>该公司声明继续说道：“感谢帮助将Ghost 的愿景变为现实的员工、投资者和合作伙伴。我们非常感谢您对Ghost 一路以来的支持。”</p><p>&nbsp;</p><p>Ghost Autonomy由 Hayes 和 Volkmar Uhlig 创立，成立7年间，该公司拥有约 100 名员工，在山景城、达拉斯和悉尼均设有办公地。</p><p>&nbsp;</p><p>公司联合创始人兼CEO 约翰·海耶斯 (John Hayes) 于 2009 年创立了Pure Storage，并于 6年后将该公司成功上市。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79080c11a4e0d5e5341ea1d244502608.png" /></p><p></p><p>&nbsp;Ghost联合创始人兼CEO J ohn Hayes</p><p>&nbsp;</p><p>作为 Pure 的首席架构师，他利用消费行业向闪存的过渡来重新构想数据中心存储，并发明了超快的闪存存储解决方案，目前由世界上最大的云和电子商务提供商、金融和医疗机构、科学研究组织和政府运营。Hayes希望Ghost能与Pure一样，通过使用软件实现近乎完美的可靠性并重新定义商用消费类硬件。</p><p>&nbsp;</p><p>与许多试图将自动驾驶汽车技术商业化的初创公司一样，Ghost 多年来也改变了经营策略。这家初创公司最初名为Ghost Locomotion，成立于 2017 年。两年后，该公司首次公开亮相，获得了 Founders Fund 的 Rabois、Khosla Ventures 的 Vinod Khosla 和 Sutter Hill Ventures 的 Speiser 等公司的 6370 万美元总投资，以及计划开发一个套件，允许私人乘用车在高速公路上自动驾驶。该公司表示将于 2020 年提供该技术。</p><p>&nbsp;</p><p>Ghost一直在强调，它的目标是为普通客户购买的消费汽车提供技术。当时，Ghost 总法律顾问杰奎琳·格拉斯曼 (Jacqueline Glassman) 解释道：“与机器人出租车或送货服务不同，Ghost 为人们每天拥有并驾驶上班的汽车提供自动驾驶服务。 Ghost 以软件形式提供服务，专为大众市场而设计，与汽车制造商合作，将自动驾驶带入主流。”</p><p>&nbsp;</p><p>此外，该公司还热衷于指出，他们的发展路线与其竞争对手不同，他们重点关注通用人工智能，而不是使用图像本地化来识别潜在障碍，以确保自动驾驶汽车避开它们。Ghost 更专注于跟踪未来场景中的像素簇——简单来说，障碍是什么并不重要，重要的是它是一个障碍，且要能够避开它。</p><p>&nbsp;</p><p>但在2020年这一最后期限到来之后，Ghost改变了最初策略，改为专注于碰撞预防技术，并在 2021 年又筹集了 1 亿美元。</p><p>&nbsp;</p><p>D 轮融资由 Sutter Hill Ventures 领投，Founders Fund 和 Coatue 跟投。 Hayes 早在 2021 年接受TechCrunch媒体采访时表示，该初创公司并未完全关闭消费套件模型的大门，而是将注意力转向通用防撞技术，以更快地进入市场。</p><p>&nbsp;</p><p></p><h2>成立7年0交付，因被Open AI投资受到关注</h2><p></p><p>&nbsp;</p><p>而事实上，Ghost远大的理想和愿景并没有足够先进和创新的技术作为支撑。虽然成立7年间融到了2.2亿美元（约合15亿元人民币），但在自动驾驶这个极度烧钱的领域里，这点钱可谓杯水车薪。</p><p>&nbsp;</p><p>去年，Hayes 在《福布斯》杂志上说道：“过去十年，汽车公司在软件开发方面一直举步维艰，无论是在执行方面还是在客户采用方面。消费者对汽车中的新技术功能非常失望。对于软件公司来说，软件实际上也很难开发——即使是硅谷的巨头。”</p><p>&nbsp;</p><p>这样一家无论是技术还是创始人背景都没有太多惊人之处的公司，真正被人们关注是因为它被AI巨星公司OpenAI投资了。</p><p>&nbsp;</p><p>去年11月，也就是距离该公司关闭仅五个月前，这家初创公司通过 OpenAI 初创基金与 OpenAI 合作，以尽早获得 Microsoft 的 OpenAI 系统和 Azure 资源。 Ghost还获得了OpenAI的500万美元投资。去年，该公司完成了 5500 万美元的首轮融资，投资方包括 Founders Fund 的 Keith Rabois 和 Sutter Hill Ventures 的 Mike Speiser 等早期投资者。</p><p>&nbsp;</p><p>OpenAI和Ghost同时宣布了这一举措，并计划将大型语言模型（ChatGPT背后的技术）引入自动驾驶。</p><p>&nbsp;</p><p>当时，Hayes 宣扬了该公司计划探索多模态大语言模型 (LLM)（可以理解文本和图像的人工智能模型）在自动驾驶中的应用。他认为大语言模型提供了一种理解“长尾”的新方法，为当前模型不足的复杂场景添加推理。</p><p>&nbsp;</p><p>OpenAI 首席运营官兼 OpenAI 初创基金经理 Brad Lightcap也为其站台称：“多模态模型有可能将大语言模型的适用性扩展到许多新的用例，包括自动驾驶和汽车。多模态模型能够通过结合视频、图像和声音来理解并得出结论，因此可能会创造一种新的方式来理解场景并驾驭复杂或不寻常的环境。” 。</p><p>&nbsp;</p><p>在倒闭前，Hayes还向TechCrunch发了一封邮件表示，表示该公司已经完成了一款高速公路驾驶产品，并正在通过他所说的“最后一英里交付”在城市环境中移动。</p><p>&nbsp;</p><p>但最终，Hayes表示，“多年来，Ghost Autonomy 无法为自己融资，也无法达到将产品推向消费汽车市场所需的工程强度水平”。&nbsp;</p><p>&nbsp;</p><p>也就是说，成立7年，烧光了2.2亿美元后，Ghost Autonomy仍然没有一款产品推向市场。</p><p></p><h2>我们距离L5级自动驾驶还有多远？</h2><p></p><p>&nbsp;</p><p>很多人都希望汽车开发周期更快，但这个行业需要第一次就做好——客户不会容忍半成品的产品。要使最复杂的行业之一正常运转，需要令人难以置信的跨领域工程，而这只能通过超乎寻常的时间来实现。</p><p>&nbsp;</p><p>自动驾驶汽车的发展并不顺利。除了Ghost倒闭的消息外，最近的一个例子是，硅谷最大的巨头之一苹果公司就其在圣克拉拉裁员 600 多名员工，根据州政府提交的文件，苹果公司位于圣克拉拉的8个办公室的员工被裁掉。</p><p>&nbsp;</p><p>去年秋天，通用汽车Cruise 自动驾驶部门因其中一辆车辆撞上并拖拽行人而受到监管审查，导致这家美国汽车制造商裁员并削减10 亿美元支出。但通用汽车首席执行官玛丽·巴拉 (Mary Barra) 在二月份表示，“自动驾驶技术带来了巨大的好处”，而且该公司拥有“极其宝贵的资产”。</p><p>&nbsp;</p><p>据研究公司 GlobalData&nbsp;的一份报告称，驾驶员至少在 20 年内将无法体验完全自动驾驶汽车的道路，通往L4 级及以上自动驾驶汽车的道路“可能会很缓慢”。</p><p>&nbsp;</p><p>L5 级自主权涉及不需要任何人机交互的自动驾驶汽车，这意味着最终部署时，它们将没有方向盘或踏板。&nbsp;</p><p>&nbsp;</p><p>报告指出：“然而，可以合理地假设，2035 年新推出的 Level 4 车辆将大大优于 2025 年推出的车辆，因此通往 Level 5 的道路可能是渐进的。”</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.kron4.com/news/bay-area/driverless-car-software-company-shuts-down-in-mountain-view/">https://www.kron4.com/news/bay-area/driverless-car-software-company-shuts-down-in-mountain-view/</a>"</p><p><a href="https://medium.com/authority-magazine/vehicles-of-the-future-john-hayes-of-ghost-autonomy-on-the-leading-edge-technologies-that-are-57bad9c33cc4">https://medium.com/authority-magazine/vehicles-of-the-future-john-hayes-of-ghost-autonomy-on-the-leading-edge-technologies-that-are-57bad9c33cc4</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I5qG0OXKToAwFi54XiRs</id>
            <title>AI坦白局：技术飞跃背后的企业实战 | InfoQ圆桌实录</title>
            <link>https://www.infoq.cn/article/I5qG0OXKToAwFi54XiRs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I5qG0OXKToAwFi54XiRs</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Apr 2024 02:57:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术领域, 人工智能, 大数据, 云计算
<br>
<br>
总结: 当前技术领域的发展速度快速，人工智能、大数据、云计算等前沿技术正在重塑生产生活方式，推动智能家居、自动驾驶汽车等领域进步，带来便利。然而，技术发展也带来数据隐私、网络安全等挑战，需要审视技术发展方向。未来，技术创新将继续改善生活。 </div>
                        <hr>
                    
                    <p></p><p>当前技术领域的发展速度可谓日新月异，为我们带来了前所未有的可能性。人工智能、大数据、云计算等前沿技术正以前所未有的深度和广度重塑着我们的生产生活方式。在人工智能的驱动下，我们能够更高效地处理海量信息，优化决策过程；大数据技术的崛起，使我们能够深入挖掘数据价值，洞察市场趋势；而云计算技术的发展，则为我们提供了弹性、高效的计算和存储服务。这些技术的融合应用，推动了智能家居、自动驾驶汽车、远程医疗等领域的显著进步，为我们的生活带来了诸多便利。我们有理由相信，随着技术的不断创新和应用，未来的生活将会变得更加美好。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而，与此同时，技术的快速发展也带来了诸多挑战和风险。数据隐私泄露、网络安全威胁等问题日益凸显，要求我们必须重新审视技术的发展方向和应用范围。技术的更新换代速度不断加快，也要求我们不断学习和适应新的知识和技能，否则就可能被时代所淘汰。因此，在享受技术带来的便利时，我们也需要清醒地认识到其潜在的风险和挑战，确保技术的健康发展，避免技术失控带来的潜在危害。</p><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了深入探讨&nbsp;2024&nbsp;年技术领域的发展趋势，我们非常荣幸地邀请到了某金融企业普惠数字金融部负责人祝世虎博士，商汤&nbsp;Copilot&nbsp;技术负责人&nbsp;张涛先生，以及&nbsp;Thoughtworks&nbsp;中国区&nbsp;CTO&nbsp;冯英睿先生，作为我们的特别嘉宾。他们将与我们分享他们在技术应用领域的宝贵经验和深刻见解。我们将深入探讨在过去的一年中&nbsp;AI&nbsp;的快速发展给工作带来了哪些挑战与惊喜、快速发展的AI技术又将如何影响技术服务市场。我们还会讨论在工作中团队应该如何高效地利用&nbsp;AI&nbsp;技术、2024&nbsp;年大模型市场的发展趋势是什么，以及技术的快速发展是否会对我们的工作造成危险。这场圆桌会议将是一个探索大模型前沿信息、洞察未来趋势的绝佳机会。</p><p></p><p></p><h3>2023年AI在自身工作领域带来的最大挑战以及最大惊喜是什么？</h3><p></p><p></p><p>王媛娅: 我想请问一下各位老师，2023年AI在您所在的工作领域带来的最大挑战是什么?以及在过去的一年当中AI和大模型软件技术发展中最大的惊喜是什么？</p><p></p><p>张涛</p><p>最大的挑战的话我认为还是大模型的稳定性和效率问题，还是希望大模型在加持了一些工具之后能够为我们稳定的输出可信的结果。因为在很多数据处理或者其他的应用场景上，我们不仅仅是需要大模型在吃掉输入的数据之后只进行简单的“next token production”式的推理，而是希望大模型能够理解用户需求并通过严谨的代码生成处理逻辑，得到一个可信的结果。</p><p></p><p>冯英睿</p><p>我印象最深的一个挑战是在我们在跟客户交流和服务的过程当中发现不同的人对于AI的认知是有偏差的。无论是高看AI还是低估AI，其实在应用和实施上都会带来一些不利的一些影响。另外一个比较大的挑战就是算力的挑战。因为算力的情况决定了大模型的成本与稀缺性，这也是落地过程中一直比较重要的一部分。惊喜的话，其实我觉得最大的惊喜应该就是保持了惊喜的常态化。纵观整个发展过程，无论是大模型的数据、算力还是规模，目前都还没有看到瓶颈，这种现象本身就是一个惊喜。另外就我个人来说的话，我对机器人领域的一些发展还是很惊喜的，也希望后续这一领域能有更大的发展。</p><p></p><p>祝世虎</p><p>从业务与应用的角度来看，在过去一年中面临的最大挑战是数据，最大的惊喜还是数据的问题。比如做大模型，如果训练的中能拿到好的语料，那大模型就一定能成功。从数据的保护角度来看，民法中对数据属于跟网络虚拟财产并列，是一种财产性的权益。后来颁布的数据安全法更是强调了数据安全的重要性。所以从法律对数据的表述来看，一是强调其权益性，二是强调其安全性。所以我认为数据流通问题是我们面临的一个重大挑战。</p><p></p><p>惊喜的话，在我看来是数据制度的变化。国家颁布的《数据二十条》带来了一个制度的变化，因为二十条强调的是数据的“数据的流通”。《数据二十条》对数据权益进行三权分制：持有权、加工权和经营权；数据的三分类：划分为公共数据、企业数据和个人数据。这为数据的管理、处理和商业应用提供了明确的法律框架，这使数据更加容易流通，也必将极大地促进科技的发展。</p><p></p><p></p><h3>更看好哪些AI应用领域？这些领域又将如何影响技术服务市场？</h3><p></p><p></p><p>王媛娅: 去年科技界似乎已经达成了一个共识，那就是大型模型的研发投入是巨大的。因此，大家更加看好大型模型的应用潜力。然而，目前来看，大型模型的应用尚未广泛普及，或者说还没有出现能够大众化、广受欢迎的爆款应用。所以想请教一下各位老师，您们更看好哪些新的AI应用领域？这些领域又将如何影响技术服务市场呢？</p><p></p><p>冯英睿</p><p>在我看来，AI技术的发展对软件信息服务行业产生了重大影响，不仅仅提升了个人技能而且改变了我们的工作方式。从我自身工作的领域出发的话，我比较关注AI在软件开发方面的应用，尤其是去年AI的兴起让我们不得不重新思考其在软件辅助研发中的角色。实际上我们最初关注的是个人能力的提升，但后来逐渐认识到AI将全面影响技术流程和组织结构。尽管研发人员为很多业务构建了数据平台，但是我们却忽略了为研发本身建一个研发的数据平台。未来，随着人们认知能力的提升，我们是否可以从学习、思考、实践等多个层面，基于我们的研发数据资产实现更全面的进步。</p><p></p><p>另外随着AI技术能力的显著提升，我们不仅看到了它在提升业务能力方面的潜力，也意识到了随之而来的风险。AI的强大功能无疑增强了传统非智能业务应用的能力，但同时，它也扩大了业务风险的暴露面。无论是在数据开发还是业务应用开发中，安全和其他相关因素的重要性变得更加突出。因此，对于专业人员来说，整体能力的要求提高了，特别是在安全性方面。这是我们从去年到今年特别关注的一些变化。</p><p></p><p>祝世虎</p><p>金融领域的核心应用就是要在政府监管的框架下，合规地管理风险，并为客户提供服务。那么面对这一基本情况，大模型在金融领域的关键运用领域就包括了监管科技、合规管理、风险管理和客户管理等。其中，智能风控尤为复杂，是这些应用中最为挑战性的部分。因此，我认为大模型在智能风控方面的发展不仅是必要的，而且是迫切的。</p><p></p><p>当前智能风控系统正面临四个关键挑战，也正是这些挑战推动着智能风控从被动到主动演变。首先，中小银行受限于数据，难以构建丰富的特征库，需要能在数据有限情况下建模的解决方案，而大型AI模型在这方面展现了潜力。其次，相对于为每个风险评估任务建立成本高昂的单任务模型，大型模型能整合多任务，提升效率并更深入地理解风险。第三，虽然构建大型模型本身成本较高，但它们的复制和运用成本较低，提供了通过利用这些模型能力来构建风控系统以节约成本的可能性。最后，随着新客户行为和黑灰产挑战的出现，传统基于历史数据的预测模型效果减弱，这使得模型对抗将成为一个持续的挑战。换句话说，智能风控领域需要大模型进行进一步的创新和自我革新，以适应不断变化的金融环境和提升风险管理的效率和效果。</p><p></p><p>张涛</p><p>两位老师所提及的内容中都涉及了使用大型模型进行编程的实践，这也是我目前专注的领域。我认为，大型模型在自然语言处理方面所提供的灵活性和创造性，结合编程代码固有的严谨性和形式化语言的规范性，能够有效地解决人们在日常工作中遇到的一些问题。因此，我对这一领域的发展前景持积极态度，并且正在投入资源进行相关研究。此外，我认为视频或图像生成领域的应用前景也非常广阔。随着移动设备上剪辑软件的普及，人们可以轻松编辑和发布短视频，这无疑推动了短视频行业的繁荣。AI 在视频生成过程中的应用，不仅提升了视频制作的质量，还极大地拓宽了创作的可能性。以往，视频的制作受限于工具的能力，而非人们的审美或创造力。现在，随着更先进的工具的出现，我们的创造力得到了更大的释放，个人的短视频制作能力也得到了显著提升。</p><p></p><p>王媛娅: 接下来问题是AI浪潮引导的大时代背景下，您的团队是计划如何迎战AIGC带来的冲击和挑战？能否为我们分享一下咱们的这个策略还有方向。</p><p></p><p>冯英睿</p><p>目前我们的团队主要专注于软件的研发，并致力于在当前时代大幅提升我们自身的能力，为未来的竞争和发展做好准备。同时鉴于应用层面的投资风险较高，我们不太会选择在该层面进行大量投资。相反，我们将重点投资于平台层和核心层，并以自助服务的形式提供给内部员工，从而降低尝试新方向的成本。</p><p></p><p>因为事实上在任何方向上的努力都有可能取得成功，因此我们的策略是提供自主能力，鼓励员工自发创新，同时降低员工尝试的成本以增加成功的可能性。所以说我们的目标是通过长线投资触发低成本的应用创新，并在此过程中让团队成员更深入地理解大语言模型和AI的应用，同时也帮助我们的客户取得成功。</p><p></p><p>祝世虎</p><p>我从两个观点说，第一个观点就是专门从大模型的角度来说，第二个观点就是从大模型的应用的角度来。</p><p></p><p>首先从大模型的角度来说，大模型的主要作用是提高整个社会的平均智能水平，而不仅仅是针对特定行业如金融的智能提升。说白了大模型的能力有可能被用于不正当的途径，比如黑色产业可能会更有“创意”地去利用这些模型。因此，我们面临的一个主要挑战不是大模型是否能帮助我们减少人力或提高效率，而是如何确保它们不被用于恶意目的。例如，在构建智能客服系统的同时，我们需要考虑黑色产业可能会利用同样强大的模型来创建用于发起欺诈投诉的机器人。这就要求我们在发展大模型时，不仅要关注其正面应用，还要积极防范潜在的滥用风险。</p><p></p><p>第二点在金融行业中，大模型的实施策略往往遵循一个分层的金字塔结构，该结构分为三个主要层次：</p><p></p><p>基础层：这是金字塔的底层，由领先的AI公司开发的通用基础大模型构成，这些公司拥有丰富的数据资源和强大的技术力量，为基础AI能力提供支持。</p><p></p><p>行业层：位于中层的是针对金融行业的大模型，由对银行业务和金融专业知识有深刻理解的公司基于基础层模型进一步开发，以满足金融领域的特定需求。</p><p></p><p>机构层：最顶层是各个银行或金融机构根据自身的具体任务和数据集定制的大模型，这些模型从行业级模型出发，通过个性化训练和微调，以适应特定机构的业务需求。对于中小型金融机构，由于科技预算有限，他们需要将资源集中投资在关键领域。这意味着他们可能不会自行构建基础大模型，而是依托于基础层和行业层的模型，并利用自身数据进行精细化调整，以确保模型能够满足特定的业务需求。</p><p></p><p>因此，中小银行应专注于以下三个核心领域的投资：首先是数据整合，即整合内部数据并辅以外部数据，以构建具有本行特色的综合知识库；其次是算力合作，考虑到算力是非核心竞争能力，银行应寻求成本效益更高的外包方式来满足算力需求；最后是模型精调，结合本行的数据和知识库，利用外包的算力资源对大模型进行迁移学习，以打造适合本行的轻量级推理模型。</p><p></p><p>张涛</p><p>其实在这个方面我们商汤是具备代表性优势的，因为我们的AIDC智算中心配备了必要的硬件基础设施，这使得商汤有能力进行大模型的预训练（Pre training）。这意味着我们可以从基础模型出发，构建具有竞争优势的AI能力。</p><p></p><p>商汤公司在硬件基础设施方面具有优势，这确保了我们在激烈的竞争中能够保持领先地位。我们目前专注的方向有两个：</p><p></p><p>真正地将数据与代码结合起来，开发能显著提高生产力和效率的实用工具。</p><p></p><p>在多模态领域进行探索。尽管目前的多模态模型大多数还称不上真正的多模态，例如从文本到图像或视频的转换，或者从图像进行视觉问答（VQA）等操作，这些仍然是简单的单模态到单模态的转换。然而，如果我们要在产业链中融合这些模态，就需要超越现有的工作流概念，实现对模态的统一理解，使大模型能够一次性完成复杂的推理任务。</p><p></p><p>目前，我们将这些处理步骤组织成工作流，或者视为上下文中的一部分，这与人脑处理的效率和效果相比还有很大差距。因此，我们认为对多模态模型的投资是一个重要的技术进步点，也是确保我们在技术上保持领先的方式。</p><p></p><p>除了硬件基础设施外，数据也是构建大模型的一个关键要素。因为即使是参数达到千亿级别的大模型，在预训练阶段使用的数据量实际上也是有限的，这意味着大模型仍然处于不断学习的阶段，潜力尚未被完全挖掘。因此，我们一方面需要不断准备高质量的数据，以支持模型的持续学习；另一方面，这些未挖掘的潜力为我们提供了空间，使我们能够对特定垂直行业的数据进行精调，或者在特定场景下优化模型。我们可以更有信心地将这些数据整合到模型中，而不必担心新数据会负面影响模型原有的基础能力。这是我们在新的应用场景中保持技术优势的一个重要策略。</p><p></p><p></p><h3>没有技术背景但了解市场的人能否独立完成研发工作？现有的研发团队如何有效地利用AI工具？</h3><p></p><p></p><p>王媛娅: 随着越来越多的工具问世，对于那些没有技术背景但深入了解行业场景和市场需求的非研发团队人员来说，他们是否能够独立完成研发工作？此外，现有的研发团队如何有效利用大模型和AI工具？我想听听各位专家对这两个问题的看法。</p><p></p><p>祝世虎</p><p>当前，我们见证了低代码和零代码平台的兴起，这对于几年前就开始出现的技术趋势来说是一个重要的发展。</p><p></p><p>首先，在我看来低代码和零代码平台提高了效率和效能。一些有经验的程序员开始使用低代码平台，这显然有助于提升工作效率。同时，对于那些不会编写代码的人来说，他们也可以使用低代码平台进行开发。然而，这里存在一个本质的区别就是：不会编写代码的人缺乏算法思维。当他们使用零代码平台时，由于缺乏算法框架，他们更多地进行统计分析而不是真正的计算工作。因此，仅依靠低代码和零代码平台是不够的，这可能会带来一些问题。</p><p></p><p>其次，大模型的出现引发了一个问题：大模型是否会取代程序员？如果未来没有人再编写代码，那么零代码和低代码平台是否会被大模型所取代？这是一个未来的疑问，目前还无法给出确切的答案。然而，大模型能够帮助解决的另一个问题是人机沟通，或者说业务部门与科技部门之间的沟通问题。当业务部门能够使用低代码平台或大模型简单地编写一些东西让机器理解时，无论是通过大模型让机器理解还是通过低代码平台让机器理解，我认为这都比通过科技人员作为中间层的效率更高。</p><p></p><p>冯英睿</p><p>在探讨低代码和传统编程方法时，我们必须承认，尽管它们具有一定的表现力，但在面对复杂逻辑时往往显得力不从心。大语言模型则提供了更高级别的抽象化能力，使我们可以更深入地处理复杂的任务。随着编程语言的发展，专门为执行特定任务而设计的、能够更有效地满足特定领域的需求的领域特定语言（DSL）就变得愈发重要。在大语言模型的加持下，它们可以根据用户的需求生成相应的高级语言代码，从而轻松应对复杂的任务。</p><p></p><p>另外在企业数字化和服务化的进程中，有效的服务化对于人工智能（AI）与系统之间的交互至关重要。AI需要依赖现有系统进行实际操作，因此确保服务化的顺利实施是AI正常运作的关键。而企业需要程序员能够以高效、简洁的方式实现业务的数字化和服务化，从而为大模型提供更好的运行环境。所以说科技的投入对于降低系统复杂度是至关重要的。</p><p></p><p>再者微服务在业务与应用层的集成中起着至关重要的作用。大型语言模型有望在这一层面上取代许多传统的开发工作，减少了对软件库进行修补或调整的需求。</p><p></p><p>从业务角度来看，这种趋势是受欢迎的，因为它使获取数据和为企业提供服务变得更加便捷。对于研发人员而言，尽管这种变化可能对一些中间层开发人员产生影响，但只要他们理解大模型的特性，并学会如何增强对模型的理解、知识、推理和表达能力，他们就能适应这种工具的不断出现与迭代升级。</p><p></p><p>张涛</p><p>大语言模型目前仍然存在不确定性和不稳定性。因此，在实际应用当中，我们常需要结合编程语言来增强回复效果。同时在我们的实验中，我们让大语言模型处理数据可视化任务时发现它是能够高效地完成这一工作的。但这个过程中，我们也遇到了一些问题，这提示我们仍需对大语言模型进行进一步的优化和调整。</p><p></p><p>关于大语言模型是否会取代程序员的问题，我认为目前并不需要过分担忧。大语言模型的发展实际上降低了技术门槛，它使得非专业人士也能轻松利用其功能。它更像是一个强大的工具，而非完全取代程序员的存在。大语言模型的主要优势在于其生成能力，能够根据不同的需求生成相应的代码，从而满足用户的个性化需求。</p><p></p><p>总的来说，大语言模型的发展为编程领域带来了新的可能性，它使得编程更加普及和易于上手。虽然目前仍存在一些问题和挑战，但随着技术的不断进步，我们相信大语言模型将会在未来的发展中发挥更加重要的作用。</p><p></p><p></p><h3>在2024年可能出现的大型模型市场趋势有哪些？</h3><p></p><p></p><p>王媛娅: 接下来是我们讨论下一个议题，除了我们报告中所预测的趋势，还想了解一下各位老师认为在2024年可能出现的大型模型市场趋势有哪些。当然，这些趋势可以是近期的也可以是远期的。</p><p></p><p>冯英睿</p><p>我相信在2024至2025年间，大语言模型的能力将显著提升，而且将颠覆我们的认知。在2023至2024年的探索期，已经有许多企业尝试把大语言模型应用于实际工作当中去了。随着这些模型能力的增强，我们也将消除对它们在实际使用中的疑虑，大模型将是值得我们信赖的工作伙伴。</p><p></p><p>因此，我认为2024年将是大语言模型广泛应用的一年。正如我们之前讨论的，我们正在努力降低大语言模型的应用门槛，提供更多自助服务，以便在工作过程中帮助企业做好准备，大规模应用这些模型。我们期待在2025年到来之前，看到大语言模型在实际工作中的广泛应用。</p><p></p><p>然后我个人的一个看法是大模型在核心业务上的影响可能不会特别显著，我更倾向于它是一个在实际运用中更多地提升我们个人能力和认知的一个工具。而且它在业务实施过程中确实有效地辅助了我们提高工作的效率和质量，从而使工作更加顺畅和易用。</p><p></p><p>从技术角度来看，我对多模态技术的进展非常关注，因为它在应用场景和体验上具有巨大的潜力。例如，已经有人尝试使用大语言模型和生成式AI来重建与已故亲人的联系，满足人们在情感方面的需求。这已经是一个发展趋势了，并且有许多场景可以应用。因此，我会更加关注元宇宙和数字人这两个方向，因为它们对商业和个人消费者的价值可能才真正开始体现。</p><p></p><p>张涛</p><p>在应用场景方面，我能做出的预测相对较少，因为各个行业从自己的视角出发，都会发现各自的结合点。实际上，我们目前能期待的是更多地借用大模型的一些能力，在图像生成或意图理解等方面的单点提升，这就像刚才我们提到的多模态领域。</p><p></p><p>但是我必须要指出的是，尽管我们的远期愿景是让机器能够像人一样，至少像一个简化版的大脑来帮助我们工作，但即使是像Sora这样的大模型现在的多模态能力也是明显不如人类的。大家可能会觉得它好像理解了真实的物理世界，但实际上它离理解物理规律还差得很远。它只是从大量的训练数据中得到了合理的、概率性的数据分布。</p><p></p><p>但是我觉得这可以理解为牛顿提出物理定律之前，人们已经在他们的认知范围内利用这些定律一样，只是缺少一个人来总结而已。因此，虽然强人工智能时代还没有到来，但在这条路上我们看到的一些现有缺陷并不是阻碍，反而让我们看到了一些使AGI更好进化的途径。</p><p></p><p>祝世虎</p><p>我认为大模型可能会在一两年内进入金融机构的核心应用中，特别是在中小金融机构和中小银行的智能风控体系中。相比于目前银行使用的小模型，大模型有以下优势：</p><p></p><p>大模型能够以极低的迁移成本引入金融领域的能力。</p><p></p><p>大模型能够感知风险浓度，而小模型更侧重于风险排序和评级。</p><p></p><p>大模型可以事前感知欺诈态势，而小模型只能感知到欺诈事件。</p><p></p><p>大模型能够感知到风险背后的人为因素，这是小模型无法做到的。</p><p></p><p>大模型能够有效对抗新型的黑产攻击，而小模型容易被黑产攻击。</p><p></p><p>大模型可以预测和感知新型风险，而小模型基于历史数据预测未来的方法存在弊端。</p><p></p><p>尽管大模型目前也存在一些劣势，如计算复杂度高、可解释性差等，但大模型和小模型可以先以共存方式出现，即以大模型作为中控大脑，外围由可解释的小模型组成一个新型的智能风控体系。随着人们对大模型的认可度提高、解释性增强和复杂度降低，大模型极有可能取代传统的小模型。</p><p></p><p>另外从发展的角度来看呢，技术从来都不是平等的。因为中小银行在人力、设备和数据资源方面的投入是无法与大银行相比的，这自然就导致了技术方面的不平等。因此，在数字化转型的过程中，小银行和大银行之间的差距会越来越大，智能风控也是如此。但大模型的发展为中小银行提供了迎头赶上的机会。通过引入大模型的能力，中小银行可以在风控能力方面与大银行对齐，再凭借其机制相对灵活，在某些具体的风控场景上赶上甚至超越大银行是完全可能的。</p><p></p><p></p><h3>大模型应用会更早落在哪些领域？</h3><p></p><p></p><p>王媛娅: 非常感谢几位老师的精彩分享。那在这个分享的过程中，我们也看到评论区有一些留言和问题，第一个问题是想请教冯老师，您觉得大模型应用会更早落在哪些领域？或者说哪些领域会更有机遇一些？</p><p></p><p>冯英睿</p><p>我认为金融领域是大模型技术投资的热门领域之一。虽然政府监管、资金投入和大模型的准确率等因素会对大语言模型的投资产生影响，但随着大模型能力的提升，这些影响预计会逐步减少。</p><p></p><p>我想强调的是，在业务过程中，如果以知识密集型人才作为核心成本，那么这部分的影响将更为显著。而对于体力劳动人群的影响相对较小，实际上，这种影响主要是由机器人等其他技术引起的。所以另一个受大模型影响的行业就是软件研发行业。在软件研发领域，所有的输入和输出都可以数字化，而且其核心依赖于人力。因此，对于这一领域的相关工作人员，大模型的影响将更为显著。</p><p></p><p></p><h3>在当前发展趋势下，未来前端类的工作是否会面对危险？</h3><p></p><p></p><p>王媛娅: 接下来，我想请教张老师一个问题。在当前的科技发展趋势下，我们是否应该担忧未来前端和UI设计的工作岗位会面临危险？您是如何看待这一问题的？</p><p></p><p>张涛</p><p>实际上，我认为当前的情况与之前提到的低代码和视频创作工具的例子有相似之处。在大型模型出现之前，已经有一些基于CRUD的工具可供初级开发者使用。</p><p></p><p>在UI开发领域，人们可能会认为screen2code这种应用的出现对自己的工作构成了威胁。然而，在我看来，这种应用的出现实际上对我们是有帮助的。因为在以前，仅仅搭建和复刻一个界面可能就需要花费很多时间，而现在这些过程可以大大简化，为我们节省了时间和精力。这使得我们能够将更多的精力投入到更高级的工作上，例如UE（用户界面）的设计和后台数据的交互等。这些领域是我们真正发挥专长的地方。</p><p></p><p>因此，对于这个问题，大家完全可以放松心态，因为这种应用的出现实际上是在帮助我们，而不是威胁我们的工作。我们可以利用这些工具来提高工作效率，从而更好地展示我们的能力。</p><p></p><p></p><h3>训练大模型时高质量数据不足怎么解决？</h3><p></p><p></p><p>王媛娅: 最后一个问题想请教一下祝老师，训练大模型时高质量数据不足是个问题，我们通常是怎样解决这一难题的呢？希望能听听您的经验和建议。</p><p></p><p>祝世虎</p><p>首先，在金融机构训练大模型这个方面，许多银行已经启动了相关项目。然而，银行的结构化数据虽然丰富，但对训练大模型的价值十分有限，因为单一的结构化账目数据不足以支撑有效的模型训练。所以，一些银行利用以前的公文等非结构化数据进行训练，可以在智能OA、智能客服等方面带来实际帮助。</p><p></p><p>其次，在智能客服方面，银行需要整合内部数据，并适当补充外部数据。当然，大型银行可能具备这样的条件，但对于小型银行或算力不足的银行来说，我还是建议采用精调模型的方式，即利用自己的特色数据来迁移大模型的能力，而不是尝试独立训练一个大模型。因为自行训练大模型的投入巨大且风险较高。</p><p></p><p>以上是本次圆桌会议“2024年技术发展趋势解读”的访谈实录，更多关于2024年中国软件技术发展洞察和趋势的完整内容请关注<a href="https://www.infoq.cn/minibook/YcyRCPwj38Upvdj4qVmx">《中国软件技术发展洞察和趋势预测报告2024》</a>"欢迎点击链接，进行完整报告下载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Mj63pg2SHHWaifKtPbNv</id>
            <title>蔡崇信反思阿里落后：我们砸了自己的脚；英特尔又“崩了”，亏损70亿美元；华为切割“遥遥领先”，传任正非下令禁止 | Q资讯</title>
            <link>https://www.infoq.cn/article/Mj63pg2SHHWaifKtPbNv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Mj63pg2SHHWaifKtPbNv</guid>
            <pubDate></pubDate>
            <updated>Sun, 07 Apr 2024 10:09:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Stability AI, 生成式人工智能服务备案信息, 英特尔, 阿里
<br>
<br>
总结: 文中提到了Stability AI资金耗尽无法支付云GPU账单，英特尔芯片制造部门运营亏损70亿美元，阿里巴巴蔡崇信访谈以及生成式AI等内容。 </div>
                        <hr>
                    
                    <p></p><blockquote>据报&nbsp;Stability&nbsp;AI&nbsp;资金耗尽；国家网信办发布生成式人工智能服务备案信息；亚马逊举办生成式&nbsp;AI&nbsp;沟通会；OpenAI&nbsp;宣布：放开使用限制！英特尔披露芯片制造部门运营亏损&nbsp;70&nbsp;亿美元；华为最新分红出炉；首位&nbsp;AI&nbsp;程序员入职阿里；美参议院将修改&nbsp;TikTok&nbsp;剥离法案；搜狗硬件产品于&nbsp;5&nbsp;月&nbsp;30&nbsp;日正式停止服；AI&nbsp;编程语言&nbsp;Mojo&nbsp;正式开源；Stable&nbsp;Audio&nbsp;2.0&nbsp;重磅发布！Safari&nbsp;浏览器主要设计师离开苹果……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>据报Stability&nbsp;AI&nbsp;资金耗尽，无法支付租用的云GPU账单</h4><p></p><p></p><p>据4月3日消息，生成式AI明星&nbsp;Stability&nbsp;AI&nbsp;的热门文本到图像生成模型&nbsp;Stable&nbsp;Diffusion&nbsp;资金耗尽，无法支付训练大模型所需的&nbsp;GPU&nbsp;集群费用。截至去年&nbsp;10&nbsp;月该公司只剩下&nbsp;400&nbsp;万美元的储备金。</p><p></p><p>根据引用公司文件和数十位知情人士的详尽报道，据称这家英国模型构建公司的极高基础设施成本耗尽了公司的现金储备，导致截至去年十月时，公司只剩下400万美元。此外工资和运营费用还需要&nbsp;5400&nbsp;万美元。而它在&nbsp;2023&nbsp;年估计的收入只有&nbsp;1100&nbsp;万美元。它拖欠了&nbsp;AWS、Google&nbsp;和&nbsp;CoreWeave&nbsp;千万美元的账单。</p><p></p><p>Stability&nbsp;AI&nbsp;的筹款也不成功，英特尔承诺投资&nbsp;5000&nbsp;万美元，但已支付的金额只有&nbsp;2000&nbsp;万美元。公司&nbsp;CEO&nbsp;Emad&nbsp;Mostaque&nbsp;上月底在社交媒体上披露他已经辞职，首席运营官&nbsp;Shan&nbsp;Shan&nbsp;Wong&nbsp;和首席技术官&nbsp;Christian&nbsp;Laforte&nbsp;担任临时的联席&nbsp;CEO。</p><p></p><h4>阿里巴巴蔡崇信万字访谈：我们承认错误，过去没有关注用户体验</h4><p></p><p></p><p>4&nbsp;月&nbsp;3&nbsp;日下午，挪威主权财富基金发布了其首席投资官&nbsp;Nicolai&nbsp;Tangen&nbsp;与阿里巴巴联合创始人、董事局主席蔡崇信的专访视频。在访谈中,&nbsp;他分享了自己的成长经历和在阿里巴巴的工作体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a73b20ae438965c91bd4fa5872f38ba4.webp" /></p><p>截图来源网络</p><p></p><p>蔡崇信回忆了阿里巴巴的创业初期。他谈到了与马云的初次见面,&nbsp;以及对公司愿景和文化的理解。他认为,&nbsp;阿里巴巴有着广阔的发展前景。作为公司的领导者之一,&nbsp;他见证了阿里巴巴的成长历程。</p><p></p><p>在科技创新领域,&nbsp;作为中国最大的云计算公司之一,&nbsp;阿里巴巴将人工智能视为云业务的重要组成部分,&nbsp;而电商是人工智能应用的重点领域。公司正开发多项&nbsp;AI&nbsp;驱动的产品和服务,&nbsp;如智能导购、图文生成等。蔡崇信坦言,&nbsp;当前中国在人工智能某些方面落后于美国,&nbsp;但正在奋起直追。在芯片和半导体领域,&nbsp;由于受到美国对华出口管制的影响,&nbsp;中国企业在高端芯片的使用上受限,&nbsp;但国内正在加快发展自主芯片的设计和制造能力。</p><p></p><p>在访谈的最后,&nbsp;蔡崇信还分享了他对企业文化、工作效率、领导力以及工作与生活平衡的看法。他以阿里巴巴为例,&nbsp;阐释了优秀企业文化的特质:&nbsp;员工认同公司使命、了解发展方向,&nbsp;并乐于与同事协作。在提升效率方面,&nbsp;他分享了自己的心得,&nbsp;即借鉴运动员的间歇式训练模式,&nbsp;通过高强度工作和适度休息的结合,&nbsp;而非连续工作&nbsp;10&nbsp;小时。在他看来,&nbsp;出色的领导者应该善于给予反馈,&nbsp;勇于承认错误,&nbsp;不刻意标榜自己,&nbsp;以免扼杀创新。</p><p></p><h4>英特尔披露芯片制造部门运营亏损70亿美元</h4><p></p><p>美东时间4月3日，芯片龙头英特尔股价重挫逾8%，收跌8.22%，报40.33美元，最新总市值1716.8亿美元。4月2日，英特尔在美国证券交易委员会（SEC）提交的一份文件中披露，公司负责芯片制造业务的新部门“英特尔代工”（Intel&nbsp;Foundry）2023年营收为189亿美元，同比下降31%，2022年这一数字为274.9亿美元，经营亏损从前一年的52亿美元扩大至70亿美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04c60707339fb8d76ad92c45bdcdde01.png" /></p><p>截图来源于网络</p><p></p><p>英特尔CEO帕特·基辛格并不避讳该业务所面临的亏损情况，并表示2024年将是公司芯片制造业务经营亏损最严重的一年。</p><p></p><p>同时，他也给出预测，该业务会到2030年底前实现经营收支平衡，届时公司的目标是在非美国通用会计准则下，毛利率达到40%，经营利润率达30%。上述消息发布后，英特尔当日收盘价为43.94美元/股，盘后下跌4.1%。</p><p></p><p>今年2月，在加州举办的IFS&nbsp;Direct&nbsp;Connect&nbsp;2024大会上，英特尔宣布将旗下晶圆业务Intel&nbsp;Foundry&nbsp;Services正式更名为Intel&nbsp;Foundry。公司还公开展示了其1.8纳米芯片制程intel18A的量产进度，以及包括更先进的Intel&nbsp;14A（对应1.4纳米）工艺在内未来十年工艺路线图。英特尔还计划斥资1000亿美元在美国四个州建设或扩建芯片工厂。上个月，该公司拿到了美国《芯片和科学法案》提供的85亿美元的资金补贴，其未来目标是要在芯片制造领域挑战台积电与三星。</p><p></p><h4>国家网信办发布生成式人工智能服务备案信息</h4><p></p><p></p><p>4月2日，国家网信办官网发布了关于《生成式人工智能服务已备案信息的公告》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/385618884ca580d0456e9c5824c9037e.png" /></p><p></p><p>网信办表示：促进生成式人工智能服务创新发展和规范应用，网信部门会同相关部门按照《生成式人工智能服务管理暂行办法》要求，有序开展生成式人工智能服务备案工作，现将已备案信息予以公告。提供具有舆论属性或者社会动员能力的生成式人工智能服务的，可通过属地网信部门履行备案程序，属地网信部门应及时将已备案信息对外公开发布，将在官网定期汇总更新，不再另行公告。</p><p></p><p>据悉，要求已上线的生成式人工智能应用或功能，应在显著位置或产品详情页面公示所使用已备案生成式人工智能服务情况，注明模型名称及备案号。</p><p></p><h4>华为内部禁止喊“遥遥领先”，提一次罚一万？</h4><p></p><p></p><p>4月2日，#余承东否认被下令禁提遥遥领先#冲上热搜。据悉，有媒体发文称，任正非据传下禁令一句遥遥领先罚款一万，相关话题引发关注。3月30日，钛媒体创始人赵何娟发文称，据可靠消息说，任正非在华为内部给余承东下了“禁令”，每再提一句“遥遥领先”罚款一万。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/022f4fb03c8a1b7eb93e4c66d53bf8db.png" /></p><p></p><p>“遥遥领先”这个词汇最初出现在华为手机Mate&nbsp;40的发布会上，华为常务董事、终端BG&nbsp;CEO余承东在介绍手机的各项性能时多次使用这个词。随后，在Mate&nbsp;50发布会上，华为全球首发了卫星通信功能，余承东再次提到“遥遥领先”，并称之为捅破天的技术。到了2023年8月，由于华为Mate&nbsp;60系列手机的发布，“遥遥领先”成为了网络热词。</p><p></p><p>值得注意的是，去年9月，华为申请注册了两个“遥遥领先”的商标，但在今年1月，这两个商标被撤回了。当前，这两个商标均为“无效”状态。另外，在2024年2月22日的华为新品发布会上，余承东全程没有提这个词，在结尾处有观众喊“遥遥领先”，他回了一句“谢谢”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41987c4538193f9a1cf1536db62edb8c.png" /></p><p></p><h4>亚马逊举办生成式&nbsp;AI&nbsp;沟通会，中国公司争相合作？</h4><p></p><p></p><p>据晚点&nbsp;LatePost&nbsp;报道，4月2日，亚马逊云科技在北京举办生成式&nbsp;AI&nbsp;沟通会，重点展示了一个月前发布的&nbsp;Claude&nbsp;3&nbsp;系列大模型。AWS&nbsp;未在中国境内的服务器上部署&nbsp;Claude&nbsp;3。和微软&nbsp;Azure&nbsp;一样，中国公司可以通过&nbsp;AWS&nbsp;全球提供的&nbsp;Bedrock&nbsp;服务，申请调用在其他地区部署的&nbsp;Claude&nbsp;3&nbsp;模型并完成计算。</p><p></p><p>一位亚马逊人士表示，Claude&nbsp;3&nbsp;系列模型发布后，他们收到了大量中国公司的合作需求。</p><p></p><p><a href="https://mp.weixin.qq.com/s/uR6qBjkz0jyBwY8TiTU3uw">亚马逊此前宣布向人工智能公司&nbsp;Anthropic&nbsp;追加&nbsp;27.5&nbsp;亿美元投资</a>"，这笔投资是继去年&nbsp;12.5&nbsp;亿美元的投资之后的追加注资，使亚马逊对&nbsp;Anthropic&nbsp;的总投资额达到&nbsp;40&nbsp;亿美元，并成为后者核心投资方、“主要云供应商”。</p><p></p><p>3&nbsp;月初，Anthropic&nbsp;发布了其突破性的&nbsp;Claude&nbsp;3&nbsp;系列模型，该系列大型语言模型&nbsp;(LLM)&nbsp;在各种认知任务上树立了新的性能标杆。Claude&nbsp;3&nbsp;系列包含三个子模型，分别为&nbsp;Claude&nbsp;3&nbsp;Haiku、Claude&nbsp;3&nbsp;Sonnet&nbsp;和&nbsp;Claude&nbsp;3&nbsp;Opus，它们提供不同程度的智能、速度和成本选择，以满足各种人工智能应用需求。根据&nbsp;Anthropic&nbsp;的数据，Claude&nbsp;3&nbsp;Opus&nbsp;在十多项常用模型能力评估数据集上，得分全超过&nbsp;GPT-4。</p><p></p><h4>OpenAI宣布：放开使用限制！全球185个国家无需注册即可使用</h4><p></p><p></p><p>当地时间4月1日，OpenAI宣布，将从当日起逐步放开使用ChatGPT的注册要求，用户将无需再注册该服务。</p><p></p><p>虽然是在4月1日公布的消息，但这并不是一个愚人节玩笑。OpenAI在公告中写道：“我们的使命是让像ChatGPT这样的工具广泛可用，以便人们体验AI带来的好处。有来自185个国家的1亿多人每周使用ChatGPT来学习新知识、寻找创意灵感，并获得问题的答案。从今天开始，您无需注册即可使用ChatGPT。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20572262ed0b1bc0f623a5e8fbe5d0ba.png" /></p><p>OpenAI宣布放开ChatGPT的注册要求。来源：OpenAI官网</p><p></p><p>不过，值得注意的是，未经注册的用户将失去一些注册用户的权利，例如保存或共享聊天记录和使用自定义指示。未经注册用户的聊天数据也会被默认进入ChatGPT的训练集，除非用户在“设置”中关闭该选项。OpenAI还指出，未经注册的ChatGPT版本将有“更加严格一些的内容政策”，但未对此进行详细说明。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/Yb_lznBa5Ek5cCNi7QRuKw">ChatGPT免注册让官网挂了？沃顿教授：OpenAI&nbsp;做了错误的决定</a>"》</p><p></p><h4>华为最新分红出炉：15万人“瓜分”超700亿元！</h4><p></p><p></p><p>4月2日晚间，据北京金融资产交易所披露，华为投资控股有限公司发布关于分配股利的公告。 公告显示，经公司内部有权机构决议，拟向股东分配股利约770.95亿元。上述股利分配系公司正常利润分配，对公司生产经营、财务状况及偿债能力无不利影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/651fc273d197e93f3625f11912515009.png" /></p><p></p><p>4月4日，又更新发布华为投资控股有限公司关于分配股利的公告，拟向股东分配股利人民币约&nbsp;719&nbsp;亿元</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28660837f5193f1b1b16963d60ed39c4.jpeg" /></p><p></p><p>3月29日，华为发布2023年报，整体经营情况符合预期， 全年实现营业收入7042亿元，净利润870亿元，同比增长144.5%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/044087a3fc535d5b6270a3d4b1565ca4.png" /></p><p></p><p>根据年报，公司通过工会实行员工持股计划（即虚拟受限股计划），员工持股计划参与人数为151796人（截至2023年12月31日），参与人均为公司在职员工或退休保留人员。 按此计算，15万名员工大概平均能分50多万。</p><p></p><h4>首位&nbsp;AI&nbsp;程序员入职阿里，可以主动要求帮忙敲代码</h4><p></p><p></p><p>4月2日**，**阿里巴巴今日郑重宣布，迎来了一位不同寻常的“新员工”——通义灵码，工号AI001，标志着AI新纪元的开启。这位7X24小时在线的AI智能编程助手，将成为阿里大家庭中独一无二的存在。</p><p></p><p>早在2023年11月的云栖大会上，通义灵码便惊艳亮相，并启动了公测。短短一周时间，它便冲上了VSCode插件市场的周热榜，并荣获letBrains的月度推荐智能编程插件殊荣。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9b2a989949bc080ad5b65468f7a673d.png" /></p><p>截图来源网络</p><p></p><p>如今全新升级的模型，在HumanEval等榜单处于业界第一梯队，已熟练掌握200+种编程语言，它的下载量更是突破了200万。例如，在最基础的代码生成任务中，它能根据上下文自适应生成精准代码，运用实时分析与检索增强技术消除幻觉，真正做到秒懂程序员的需求。</p><p></p><p>在最新的版本中，通义灵码还新增了代码优化功能，能够深入分析代码及上下文，快速定位语法错误、性能瓶颈等问题，并给出具体优化代码建议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d5287a9a47748a5dab6f269c1a885c4.png" /></p><p></p><h4>迎来生机？美参议院将修改TikTok剥离法案</h4><p></p><p></p><p>北京时间4月1日，据美国媒体报道，美国参议院计划修改最近在众议院通过的TikTok剥离法案。目前，相关想法已经在国会山传播讨论。但是，一些支持打压TikTok的议员担心，过于宽泛的改动可能会严重拖延强制TikTok出售的努力，甚至会彻底破坏它。</p><p></p><p>在美国国会官网和长期跟踪美国国会活动的GovTrack上查询“TikTok法案”，均未显示参议院对该法案有任何进一步的审议计划或时间表。华尔街日报报道称，复活节休会归来后，参议院商务委员会主席玛丽亚‧坎特韦尔可能将推动该法案的修改。这意味着，该法案可能会被暂缓审议或彻底无法生效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f7baff9875a51c15c0d95941a3beb5c.png" /></p><p></p><p>**目前，在美国民众中，对于是否应该禁止TikTok的看法存在显著的分歧。**根据CNBC的一项民意调查，31%的受访者认为不应该禁止TikTok，而只有20%的受访者支持无条件禁止。这一数据反映出，对于TikTok所代表的言论自由和“创造者经济”的支持，并不是少数声音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/376f4e146648eaa204e3b3e7ca77c6cf.png" /></p><p>图片来源：CNBC</p><p></p><p>与此同时，字节跳动依然正在发起反击。该公司在内华达州、蒙大拿州、威斯康星州、宾夕法尼亚州和俄亥俄州等参议院席位竞争激烈的州启动了210万美元的营销活动。这些州都是今年竞选连任的弱势民主党参议员所代表的州。这些广告将在电视、广告牌和公交车站播放。</p><p></p><h4>搜狗硬件产品于5月30日正式停止服务,包括糖猫词典笔、搜狗翻译宝等</h4><p></p><p></p><p>4月1日，搜狗硬件产品维护团队发布公告表示，由于业务调整，搜狗硬件产品(糖猫手表、糖猫在家、糖猫词典笔、搜狗翻译宝(翻译笔)、搜狗录音笔)即将于2024年5月30日23点59分正式停止服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62b4d5d328cceea74f5f85938c7f25bc.png" /></p><p></p><p>这些硬件产品中部分主要功能或全部功能依赖于搜狗提供的云服务，因此在搜狗停止服务后这些产品可能会无法正常使用。</p><p></p><p>在2021年夏季，腾讯花35亿美元将搜狗从搜狐手中买下，彼时外界的一致的看法，是自此搜狗的存在感将会越来越淡。而随后的事实，也确实如此。继搜狗地图、搜狗阅读、搜狗借钱、搜狗号等一系列软件服务陆续停运之后，搜狗的硬件产品如今也成为了被“优化”的业务。</p><p></p><p>值得注意的是，搜狗硬件产品维护团队还提到服务下线后不再提供任何在线服务、技术支持和维修服务，也就是说后续产品出现故障无法使用时，用户也只能丢弃而无法寻求售后服务。搜狗要求用户在&nbsp;2024&nbsp;年&nbsp;5&nbsp;月&nbsp;30&nbsp;日之前备份自己的数据，之后搜狗会在服务下线后删除所有用户数据，数据被删除后自然也是无法恢复的。</p><p></p><h4>陷入瓶颈？B站展开新一轮组织架构变动</h4><p></p><p></p><p>4月1日，有消息称&nbsp;B&nbsp;站进行了新一轮组织架构调整，变动最大的就是此前核心业务部门&nbsp;"&nbsp;主站运营中心&nbsp;"&nbsp;拆分为三部分，分别是平台生态相关、直播业务相关以及内容品类相关。</p><p></p><p>据悉，与平台生态相关的部门，如创作平台部、生态策略部、生态用增组等，整合入&nbsp;"&nbsp;生态中台&nbsp;"，由夏彬负责；与直播业务相关的部门，如直播中台部、虚拟主播运营部、娱乐主播运营部等，整合入&nbsp;"&nbsp;直播中心&nbsp;"，由于鹤鑫负责；剩余原主站运营中心与内容垂类运营相关的部门，如知识内容部、汽车内容部、资讯内容部等，整合入&nbsp;"&nbsp;综合品类部&nbsp;"，由王智开负责。这三部分的业务板块全部都向&nbsp;B&nbsp;站&nbsp;CEO&nbsp;陈睿汇报</p><p></p><p>值得关注的是，B&nbsp;站成立了&nbsp;"UP&nbsp;主经营服务中心&nbsp;"&nbsp;来负责&nbsp;MCN&nbsp;及&nbsp;UP&nbsp;主经营，以此加强商业化服务能力建设，由王超负责。</p><p></p><h4>AI编程语言Mojo正式开源！宣称比Python快9万倍</h4><p></p><p></p><p>2024&nbsp;年&nbsp;3&nbsp;月&nbsp;29&nbsp;日，Modular&nbsp;Inc.&nbsp;宣布开源&nbsp;Mojo&nbsp;的核心组件。Mojo&nbsp;是一种专为编写人工智能软件设计的编程语言，去年&nbsp;8&nbsp;月份正式发布，迄今为止已经积累了超过&nbsp;17.5&nbsp;万名开发者和&nbsp;5&nbsp;万个组织。</p><p></p><p>人工智能模型通常使用多种编程语言编写。开发者会用&nbsp;Python&nbsp;实现神经网络最简单的部分，这种语言易于学习，但运行速度相对较慢。其余部分的代码通常使用&nbsp;C++&nbsp;编写，C++&nbsp;的运行速度比&nbsp;Python&nbsp;更快，但学习难度也更大。</p><p></p><p>Modular&nbsp;将&nbsp;Mojo&nbsp;定位为一种更方便的替代方案。它提供了一种易于使用的语法，类似于&nbsp;Python，但运行速度可以快上数千倍。因此，开发者可以编写快速的&nbsp;AI&nbsp;模型，而无需学习&nbsp;C++&nbsp;等复杂的语言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2caf0f180fc6e46576a5b7725cb690a1.png" /></p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/7m9hxk0Q6BH97fs3djo7Ew">比&nbsp;Python&nbsp;快&nbsp;9&nbsp;万倍的&nbsp;Mojo&nbsp;终于开源了！刚上线&nbsp;star&nbsp;已超过&nbsp;1.7&nbsp;万</a>"》</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>文本生成3分钟44.1&nbsp;kHz&nbsp;音乐，Stable&nbsp;Audio&nbsp;2.0重磅发布！</h4><p></p><p>4月4日，著名开源大模型平台Stability.ai在官网正式发布了，音频模型Stable&nbsp;Audio&nbsp;2.0。Stable&nbsp;Audio&nbsp;2.0支持用户通过文本或音频，一次性可生成3分钟44.1&nbsp;kHz的摇滚、爵士、电子、嘻哈、重金属、民谣、流行、乡村等20多种类型的高质量音乐。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc4dccb4511369e72444986774c48d89.png" /></p><p>体验地址：<a href="https://stableaudio.com/generate">https://stableaudio.com/generate</a>"</p><p></p><p>其生成音乐的时长也超过了谷歌的Music-fx、Meta的AudioCraft等知名产品。目前已正式开放，免费提供试用（没锁区直接登录）。</p><p></p><p>2023年9月14日，Stability.ai首次发布了Stable&nbsp;Audio&nbsp;1.0，主要有免费和付费两个版本。但无论是免费还是付费最长只能生成90秒的音乐。2.0版本能极限延长音乐时间，主要是因为Stability.ai使用了Diffusion&nbsp;transformer&nbsp;(DiT)替换了1.0的U-Net架构。即将发布的Stable&nbsp;Diffusion&nbsp;3也使用了类似的技术，使其生成图像的质量、文本语义还原得到了极大增强。</p><p></p><p>此外，Stable&nbsp;Audio&nbsp;2.0使用了一个超过80万个音频文件组成的数据集，包含音乐、音效以及各种乐器。该数据集总计超过1.95万小时的音频，同时与知名音乐服务商AudioSparx进行合作，所以，生成的音乐可以用于商业化。</p><p></p><h4>IT工程师薪资全球排名：中国超越日本！</h4><p></p><p></p><p>Human&nbsp;Resocia&nbsp;公布的报告显示，美国以&nbsp;445.1&nbsp;万&nbsp;IT&nbsp;工程师高居第一，印度&nbsp;343.1&nbsp;万第二，中国&nbsp;328.4&nbsp;万第三，日本&nbsp;144&nbsp;万人第四，但与前三相差甚远。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81ce7979e1128d755dcb78f87d79df6d.png" /></p><p>图片来源：日经XTECH根据Human&nbsp;Resocia的资料制图</p><p></p><p>在薪水方面，瑞士的&nbsp;IT&nbsp;工程师薪水最高，其次是美国、以色列、丹麦、巴拿马和德国，中国超过日本排在第&nbsp;24&nbsp;位。2023&nbsp;年中国&nbsp;IT&nbsp;工程师的平均年薪为&nbsp;3.6574&nbsp;万美元，日本为&nbsp;3.6061&nbsp;万美元。分析师称，日本&nbsp;IT&nbsp;工程师薪水低的原因是多层转包结构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/74/7413648e5c1dc1c7ed1888d208895fc0.png" /></p><p>图片来源：日经XTECH根据Human&nbsp;Resocia的资料制图</p><p></p><p>2023年版《报告》还显示，日本IT工程师的平均年收入被中国超越。2022年版《报告》显示，中国IT工程师的年收入排在全球第25位。但是，2023年中国IT工程师的平均年收为3.6574万美元，名次上升1位至世界第24，超过了日本。“为了削减成本而推进在中国的离岸外包”这一日本的举措也正在失去存在意义。</p><p></p><p>日本Human&nbsp;Resocia公司的宣传负责人指出，“薪资低主要是因为日本IT行业存在‘多层转包’这一结构性问题”。Human&nbsp;Resocia的业务之一是从全球范围内招聘IT工程师，并将这些IT工程师派遣到日本企业工作。</p><p></p><h4>Safari&nbsp;浏览器主要设计师离开苹果，查理・迪茨加入Arc浏览器</h4><p></p><p></p><p>4&nbsp;月&nbsp;2&nbsp;日，苹果公司&nbsp;Safari&nbsp;浏览器的主要设计师之一&nbsp;——&nbsp;查理・迪茨现已离开苹果公司，加入&nbsp;Arc&nbsp;浏览器背后的&nbsp;The&nbsp;Browser&nbsp;Company，后者&nbsp;CEO&nbsp;乔什・米勒今日凌晨通过&nbsp;X（推特）宣布了这一消息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f0e819221c58c25d8086b50d7afa6fe3.png" /></p><p></p><p></p><p>乔什・米勒表示，到今年夏季到来时，Arc&nbsp;的&nbsp;Windows、macOS&nbsp;和&nbsp;iOS&nbsp;版本将会“普遍可用”——&nbsp;同时还将有一个跨平台同步系统。（Arc&nbsp;Browser&nbsp;是基于&nbsp;Chromium、在&nbsp;macOS&nbsp;平台拥有一定人气的浏览器）</p><p></p><p>Arc&nbsp;浏览器基于&nbsp;Chromium，在macOS平台颇受欢迎。The&nbsp;Browser&nbsp;Company计划推出多项新功能，并致力于模糊浏览器、搜索引擎和网站之间的界限，提升用户体验。迪茨的加入也被视为公司战略发展的重要一步。</p><p></p><h4>谷歌将删除数十亿浏览器记录以解决“隐身”诉讼</h4><p></p><p></p><p>4月2日消息，据CNN报道，谷歌将删除数十亿条数据记录，作为一项诉讼和解的一部分。该诉讼指控这家科技巨头不当跟踪那些认为自己在私下浏览互联网的用户的网络浏览习惯。</p><p></p><p>该诉讼最初于&nbsp;2020&nbsp;年提起，指控谷歌歪曲了从通过&nbsp;Chrome&nbsp;中的隐身隐私浏览模式浏览互联网的用户收集的数据类型。</p><p></p><p>根据本周在旧金山联邦法院提交的和解详情，谷歌已同意销毁据称其不当收集的数十亿数据点，并更新其在隐私浏览中收集的内容的披露，并为用户提供在该设置中禁用第三方cookies的选项。这项协议不包括对个别用户的赔偿，但将允许个人提交索赔。目前，原告律师已在加利福尼亚州法院提交了50个案件。</p><p></p><p>谷歌发言人José&nbsp;Castañeda表示，该公司很高兴删除“旧的技术数据”。他说，这些数据从未与个人关联或用于任何形式的个性化。他称这些个别诉讼毫无根据。而原告律师David&nbsp;Boies表示，这项和解要求谷歌删除并补救其不当收集的数据，“范围和规模前所未有”。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/64bf514cf14b66d63e0edebe6</id>
            <title>汽车之家人的“灵创”AI应用平台及编程助手AutoCode</title>
            <link>https://www.infoq.cn/article/64bf514cf14b66d63e0edebe6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/64bf514cf14b66d63e0edebe6</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Apr 2024 10:04:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 大语言模型, 通用模型API服务接口, 仓颉灵创
<br>
<br>
总结: 随着ChatGPT的兴起，大语言模型及其相关应用受到了广泛关注。公司通过建立通用模型API服务接口，成功支持了130+业务，为业务创新注入了强大动力。为了提高员工工作效率，公司推出了仓颉灵创平台，其中包括汽车之家AI编程助手"AutoCode"。 </div>
                        <hr>
                    
                    <p>随着ChatGPT的崭露头角，大语言模型及其相关应用受到了前所未有的关注。众多企业纷纷涉足这一领域，探索大模型在各行各业的潜在应用与落地场景。作为公司的基础设施资源保障部门，云平台部自23年第一季度起，便积极响应公司创新业务发展的号召，投身于大模型资源和算力资源的储备工作中。结合汽车之家的业务特点，我们成功打造了满足各业务调用需求的通用模型API服务接口，已累计支撑公司130+业务，3600万次调用，为公司的业务创新注入了强大动力</p><p></p><p>在支撑各业务线调用大模型进行业务创新的同时，我们技术保障团队也深入调研了大语言模型的相关能力。我们思考如何将这一行业领先的“创新”技术应用于公司内部办公场景，从而让更多员工享受到技术带来的便利，实现工作效率的提升。于是，汽车之家官方AI应用平台“仓颉灵创”应运而生。</p><p></p><p>针对办公场景中模型使用门槛高、交互体验复杂的问题，我们经过长时间的调研和摸索，于9月份正式推出了面向全体员工的AI应用平台“仓颉灵创”，其中就包含今天的主人公，汽车之家AI编程助手“AutoCode”</p><p></p><p>“仓颉灵创”平台的使命是整合汽车之家公司现有的AI相关能力与应用，为员工提供一个简洁、易用的内部AI应用平台，让每一位家人都能轻松享受大语言模型带来的便利。目前，该平台已包含六大模块，自上线以来累计完成了30万次以上的交互。我们相信，“仓颉灵创”将为公司员工带来更加便捷、高效的办公体验，助力公司业务的持续发展与创新</p><p></p><p>AI对话：以汽车之家自研“仓颉”模型为基础，集成行业主流大语言模型切换能力的通用AI对话机器人</p><p><img src="https://static001.geekbang.org/infoq/d5/d5e7e8bd86f94c50a47788e21e39ba75.png" /></p><p>智能绘画：使用DALL-E、Stable Diffusion、Midjourney为底座的可视化调参智能绘画模块</p><p><img src="https://static001.geekbang.org/infoq/c7/c7fe8925c7f5d2d65466fd30f7f6a5ca.png" /></p><p>办公助手：专属定制的垂直领域智能问答助手，包括代码开发、运维测试、职能分类、文本创作、语言学习、AI生图工具6个领域的43个办公助手</p><p><img src="https://static001.geekbang.org/infoq/59/5921f560e22a8889da81f8d06759da87.png" /></p><p></p><p>藏经阁：汽车之家算法自研AI能力集中营、在线试用图像超分增强、TTS、OCR、图文转视频等AI高科技</p><p><img src="https://static001.geekbang.org/infoq/1a/1a18a8d0aa72681506612f45f698ec07.png" /></p><p></p><p>天机：汽车之家自研大模型推理服务</p><p><img src="https://static001.geekbang.org/infoq/6f/6f78fb4c94a27d55a246c8a8ee165ff2.png" /></p><p></p><p>接下来，重点介绍本期内容的主人公，汽车之家通用AI编程IDE插件“AutoCode”</p><p></p><p>在大语言模型推出后，市场上衍生出了大量的AI编程IDE插件，在调研了Github Copilot、Codearts、Comate、CodeGeeX等市场主流IDE编程插件后，结合汽车之家实际业务场景，云平台上线了汽车之家专属IDE编程插件AutoCode，支持VSCode、JetBrains IDEs、Android Studio，将有效帮助大家解决开发过程中遇到的问题，显著提升开发效率。产品网址：<a href="http://lingchuang.corpautohome.com/#/plat-chat/program">http://lingchuang.corpautohome.com/#/plat-chat/program</a>"产品功能清单：</p><p></p><p>Code问问- 在IDE窗口中与大语言模型针对代码相关问题进行问答代码自动生成-根据自然语言描述的功能自动生成代码代码重构-对所选代码进行重构代码优化-对所选代进行优化代码解释-解释所选代码，帮助更快更好的理解代码翻译-对当前代码进行语义级翻译，支持多种编程语言互译添加注释-给代码自动添加行级注释快速插入-大模型的输出直接点击快速插入到代码框对应的光标处</p><p></p><p>产品示意图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ed4adad0a42c3f8ef2679011365b49d.png" /></p><p></p><p></p><p>除上述AI编程通用功能外，我们分析实际业务痛点，大胆尝试上线了多语言转换神器“代码孪生”，可以实现不同语言之间的代码转写</p><p></p><p>代码孪生逻辑图</p><p><img src="https://static001.geekbang.org/infoq/2e/2e39c19ddf906291f29d9e35acd5490e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc2115e94bf12eaab5c2da913a3fe70e.png" /></p><p></p><p></p><p>以上，简单描述了灵创平台以及“AutoCode”的落地过程与阶段性成果，云平台部也会持续完善产品、为大家提供更多更好用的AI系统工具，助力大家办公提效。也欢迎各位同学，给我们提供宝贵的指导与建议，共启美好未来！</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cf4e4cd1a2736d744e16c6d66</id>
            <title>大模型驱动的新范式选车引擎</title>
            <link>https://www.infoq.cn/article/cf4e4cd1a2736d744e16c6d66</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cf4e4cd1a2736d744e16c6d66</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Apr 2024 10:01:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 大模型, 搜索引擎, 生成式搜索
<br>
<br>
总结: 2022年11月末发布的OpenAI的ChatGPT引领了大模型纪元的时代，大模型在语言理解、生成创作、逻辑推理等方面表现出非常高的性能水平，搜索引擎行业迎来了变革的机会。汽车之家通过将大模型与搜索有机融合，打造了一种生成式的选车引擎GSE，为用户提供高效精准、专业有趣的创新性选车体验。 </div>
                        <hr>
                    
                    <p></p><h2>1. 引言</h2><p></p><p>OpenAI的ChatGPT于2022年11月末发布，以其强大的智能能力惊艳四方，掀起了大模型浪潮，开启大模型纪元的时代。大模型在语言理解、生成创作、逻辑推理等方面表现出非常高的性能水平；而搜索作为检索整合信息的经典场景，成为大模型落地的重要突破口，搜索行业迎来了变革的机会。微软作为OpenAI的最大股东，首先将chatGPT集成到New bing（现为Copilot）中，合并搜索与聊天能力，将搜索带到了一个新的创新水平。紧接着Google以及Baidu都紧随其后，结合自家大模型技术分别打造了SGE与AI伙伴的AI搜索引擎，以期建立大模型时代搜索的防御壁垒。专注于生成式搜索的创业公司Perplexity AI，不到两年时间估值达到5.2亿美元。国内的天猫璇玑、淘宝问问、抖音AI搜，B站AI助手等等，都将大模型嵌入搜索，为用户提供特色化的搜索体验。</p><p></p><h2>2. 选车引擎如何革新体验</h2><p></p><p>汽车之家是中国领先的汽车互联网平台，为汽车消费者提供贯穿选车、买车、用车、置换等所有环节的全面、准确、快捷的一站式服务，而汽车之家的搜索则是满足用户选车需求的重要入口。汽车之家传统的选车引擎，用户输入汽车相关的Query关键词, 发起搜索返回多个相关的候选结果；然后点击多个链接并进行浏览、信息提取；如果是更加复杂的选车问题，例如想了解“宝马x3的续航怎么样，内部空间大不大”,还需要反复更换不同的query进行搜索，整合多次的查找信息才能进行有效的选车决策。可以发现，当前的搜索选车模式下，用户需要甄别处理大量繁杂、碎片的信息，同时面对多样化的信息容易出现选择困难；对于复杂些的选车需求，用户的选车行为连贯性与完整性都难以保证。受限于传统的搜索形态与技术范式，如何优雅地解决此问题一直都是相对棘手的事。大模型的出现，让问题有了突破的可能。</p><p><img src="https://static001.geekbang.org/infoq/83/83f1286228b043d95360694de72b21ed.png" /></p><p></p><p>大规模参数量模型的智能涌现，带来了多项任务上的极速性能提升，其ICL（In Context Learning，上下文学习）以及CoT（Chain of Thought,思维链）特性体现出LLM强大的泛化与智能水平。大模型擅长进行文本创作、语言理解、逻辑推理等AIGC任务，但应用于汽车垂直领域，仍然存在一些不足，例如：领域知识不足，通用大模型的训练数据无法涉及行业的私域数据，专业性知识存在盲区；幻觉问题，询问“剁椒鱼头车是什么”，GPT4给出下图中一本正经却并不正确的答案；时效性问题，大模型受限于参数化知识无法动态更新，对于数据时效外的问题只能拒绝回答或者幻觉生成。</p><p><img src="https://static001.geekbang.org/infoq/4d/4d419154a2a1e94c82345d6bc0a5f37d.png" /></p><p></p><p></p><p>可以发现，想要获得准确严谨的答案，单纯依靠大模型是不现实的，容易出现幻觉、时效性、领域知识、长尾等问题，无法有效满足用户的需求。因此，在大模型还是搜索引擎的选择问题中，我们选择了全都要！</p><p><img src="https://static001.geekbang.org/infoq/bd/bd06fa0de860d8d1ea2bd62b3642ad9d.png" /></p><p></p><p></p><h2>3. 大模型驱动的生成式选车引擎</h2><p></p><p>我们将大模型与搜索有机融合，打造了一种生成式的选车引擎GSE（Generative Search Engine）。其产品界面如图所示：</p><p><img src="https://static001.geekbang.org/infoq/6d/6d323ebc0f0bf536789f7c56888dbbd6.png" /></p><p></p><p></p><p>在输入侧，用户可以不再需要搜索的关键词技巧，无论短句还是长句，都可以通过更加自然的语言表达与GSE进行交互；输出侧的结果也不再需要从搜索结果中筛选点击阅读，直接给出Query对应的信息提炼总结，选车结果的交付更加简洁明了；输出结果中给出了引用来源，方便用户进行信息溯源，同时给出扩展的推荐问题，激发用户进一步对话交互。新选车引擎兼具了搜索的时效性准确性以及大模型的智能性，为用户提供高效精准、专业有趣的创新性选车体验。GSE整体架构如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9a592711c95770e27b1cea0f85d2ac5.png" /></p><p></p><p>新选车引擎的核心主要包括两块，分别是领域大模型构建与引擎架构设计。接下来会针对这两块内容进行简要的介绍。</p><p></p><h3>3.1.汽车领域大模型</h3><p></p><p>在RAG（Retrieval-Augmented Generation）检索增强或者是搜索增强方向，对于基础模型构建这块，通常有两种不同的思路。一种是不碰基础模型，模型本身不会引入额外数据进行调整，引入搜索能力即可，比较轻巧化的方案；另一种是训练领域大模型，丰富领域知识减少幻觉，但同时具有风险，因为除了训练成本较大，使用特定数据对大模型调整容易发生不稳定现象，造成模型崩溃与性能缺陷。汽车之家作为中国领先的汽车垂直平台，沉淀积累了海量、专业且全面的汽车行业知识数据，天然具有汽车领域的信息优势。基于团队的技术储备与硬件条件，我们选择训练垂直行业大模型，以达到在选车场景的最优效果，同时能够确保业务场景接入大模型的安全性与隐私性。业界实践中，大模型做SFT（Supervised Fine-Tuning，有监督微调）经常会遇到比较棘手的情况，领域上能力增强，但是通用上的能力变得一塌糊涂，各种任务上的评测效果都显著下降，对垂直域上的数据产生过拟合，失去了原有的泛化能力。如何有效解决这种灾难性遗忘问题，让大模型既保持很强的通用智能，又具有独特的领域专家思维，是垂域训练的主要目标。简单介绍下构建领域LLM过程中的实践经验。数据工程。大模型数据质量的重要性毋庸置疑，基本上大模型开发的大部分时间投入，都是在数据工程这块，并且数据的质量重要性要高于数据的数量。构建指令数据以及预训练数据都需要做足够的过滤、去重等清洗工作，保证数据的高质量、多样性以及准确性，让大模型能够感知并理解汽车行业的方方面面；重复性的数据会导致模型训练的恶化，不正确的信息相当于在源头提供了幻觉。同时不同类型的数据之间还需要保持合适的配比，合适的分布能够让模型有更好的效果。在SFT阶段，指令数据构建为(instruction，response)的形式，通过IT（Instruction Tuning，指令微调）约束模型遵循选车领域的特定类型指令，以期望的形式进行输出，满足选车场景的多样化用户需求。</p><p><img src="https://static001.geekbang.org/infoq/bb/bba48f4e828f3334d8985af1619a0fbe.png" /></p><p></p><p>训练策略。此处主要介绍SL（Supervised Learning）方式的对齐训练。垂域的大模型除了常用的SFT，在此之前还需进行CPT（Continual Pre-Training，持续预训练），才能让基座大模型更新扩充汽车领域的知识，以适应汽车领域的应用。GPU资源足够的话，可以偏向采用全参数微调，能起到较优的效果；否则可以采用LoRA（Low-Rank Adaptation，低秩适配）或者QLoRA的方式，进行轻量级微调。在灌入汽车行业数据提升领域能力的同时，为保证通用能力不受损，可以在SFT阶段搭配合适比例的通用数据与领域数据。另外，将CPT与SFT拆分成两阶段进行训练也许不是最好的选择，实践中发现，采用多任务训练的方式，将CPT与SFT放在一阶段进行联合训练，能起到更优的泛化效果。在模型层面，如下图所示，可以采用NEFTune的方式，在Embedding层添加少量的噪声进行正则化扰动，减少过拟合风险，提升指令精调的效果。</p><p><img src="https://static001.geekbang.org/infoq/fe/feb4d5b9438b2fbf1a7a53bae137aa8e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f549761425fe41fb0346ca7503434af.png" /></p><p></p><p></p><p>大模型的微调训练涉及面广，实践技巧繁多，例如数据配比、自动化样本筛选、超参数优化等；同时从数据-&gt;模型-&gt;评估-&gt;量化推理等整体链路需要进行全面的打磨优化，魔鬼在于细节，在此就不一一展开。在经过各种趟坑和策略优化后，我们获得了一个相比基座模型通用能力无损（MMLU、CMMLU、CEVAL等各评估集合上验证），汽车领域知识准确率相比GPT4 +9.3pp，领域能力显著提升的行业大模型。</p><p></p><h3>3.2. GSE系统架构</h3><p></p><p>以构造的领域大模型为基础，基于搜索增强的理念进行系统构建，提升选车引擎的系统信噪比。经典的RAG系统集成参数化模型和非参化记忆用于语言生成，系统接收输入的Query x，通过Retriever</p><p><img src="https://static001.geekbang.org/infoq/d5/d5ddc08fa08be4424d96d0a8f3150bb5.png" /></p><p>和Generator </p><p><img src="https://static001.geekbang.org/infoq/8c/8c6236b70453cfaff995af43534939fb.png" /></p><p>两个模块，检索到文档z并作为附加上下文，通过边际化处理的方式生成序列y，其形式化定义如下：</p><p><img src="https://static001.geekbang.org/infoq/2e/2e23cdc41b00f6b18d689adf7b97a7a2.png" /></p><p></p><p>如果只是简单地将搜索结果输入给LLM进行生成，那这是Naive RAG的模式，容易存在检索生成质量不佳、噪声大的问题，对于专业精准性要求高的选车场景，Naive RAG基本无法满足业务需求。我们设计了基于大模型驱动的生成式AI选车引擎，其系统架构如下所示。其主链路核心包括QG（Query Generation，查询生成）模块、RG（Retrieve &amp; Generate，检索与生成）模块、Re-Ranking重排模块以及CR（Compression &amp; Refine，压缩精炼）模块。同时GSE构建了汽车领域的知识基座，能够为GSE的Agents提供专业化的知识决策参考；支持用户的多轮选车交互，保持选车行为的连续性与一致性；同时为权衡算力与在线性能，设计了包含离线、近线、在线计算的多级架构，能够节约算力的同时提升系统响应性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14078d70fc882e42ae97cb8df7cc8363.png" /></p><p></p><p>通过LLM的Prompt工程，使得系统每个模块都是拥有自主功能的AI Agent，同时能够与系统内其他Agent进行交互，形成LLM OS。</p><p></p><p></p><blockquote>QG Agent能够提供Query-Expand、Sub-Query、Query Rewrite以及Query Routing等功能，并与其他Agent进行信息交互，对输入的不同复杂度、类型的Query进行扩展、改写、子问题分解、上下文理解、路由等变换生成操作，解决Query语义过简、复杂、表意模糊混淆以及上下文依赖等问题，提升输入信噪比。</blockquote><p></p><p><img src="https://static001.geekbang.org/infoq/63/637cec52a4c63cf031711a01461978d0.png" /></p><p></p><p></p><blockquote>RG Agent则是GSE非参化知识与参数化知识的重要处理部分，通过Query与上下文理解，智能动态化调度私域知识数据、垂类搜索引擎以及LLM生成信息，结合Sparse &amp; Dense 双路检索方式，更好地应对复杂多样的选车查询需求。</blockquote><p></p><p></p><p></p><blockquote>Re-Ranking则是对返回的信息根据整体相关性、时效性、多样性、个性化等维度，对候选进行重排与截断，为下游CR模块提供高质量信息。</blockquote><p></p><p></p><p></p><blockquote>CR Agent对信息进行准确高效的压缩提炼，由于CR位于链路输出端，直接影响生成质量，因此存在许多优化技巧，例如Chunk的自适应调整、迭代式自主精炼Self-Refine、基于CoV（Chain of Verification）验证链的事实性自主判断等，在此不一一展开。</blockquote><p></p><p></p><p></p><blockquote>GSE支持多轮交互，设计了Memory Agent，对会话历史进行压缩和调度管理。除此之外，GSE的Citation模块能够为输出结果提供自动化的引文生成。GSE还具有规划工具使用的能力，例如，如果你询问它今天限号情况，它会根据你所处的省市政策，给出最近一周的限号情况。目前各种实用能力还在进一步扩展中。</blockquote><p></p><p></p><p>通过Agent化的方式构建了大模型驱动的生成式搜索引擎GSE，并对其进行了整体评估，在选车场景中传统引擎难以有效应对的长尾问题以及复杂选车问题上，效果独立评估与相对评估（相比传统搜索）上GSE都达到了非常不错的水平，选车体验焕然一新，更加出色。同时GSE在领域幻觉问题上的准确率也得到较大的提升，相比GPT4准确率高+8.0pp。当然，这只是其中一个维度的效果考量，GPT4作为当前最顶尖的大模型，其超高的综合智能水平一直是GSE基础大模型学习追逐的目标,模型全面维度的提升还有非常多的工作要做。关于幻觉的Case示例Query=“智己s7怎么样”，智己s7并不存在，近似的有智己LS7/智界s7，用户搜索经常发生混淆。下图从左至右分别是业界某款AI助手、GSE以及GPT4的回复结果。可以发现，即便是搜索插件接入模式的GPT4也发生了幻觉，而GSE较好的指出该车系不存在的事实，同时介绍了可能满足Query意图最相近的车。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a00bee40f01c973966d0e9c0844b5419.png" /></p><p></p><h3>3.3. 存在的不足与限制</h3><p></p><p>在GSE系统设计中，虽然引入了多种鲁棒性与可靠性的设计，但实践中也发现，系统仍然会存在一些问题与限制。例如推荐偏差问题，类似“哪些车比较好看”较为宽泛的选车问题，在没有更多条件约束的情况下，好看是因人而异的，这可能导致GSE的结果对部分用户来讲是有偏差的；还有与搜索结果存在差异的问题，单篇的文档可能对某种类型的车具有强烈的情感偏好，而GSE强调则综合性的观点，这可能导致矛盾信息的出现；还有幻觉问题，大模型的自回归（AR, AutoRegressive）生成模式导致幻觉问题能被缓解，但幻觉生成的概率性总会存在，接下来需要对齐训练中做进一步的优化；另外当前的交互功能还比较简单，存在大量的改进升级空间。</p><p></p><h2>4. 生成式选车的未来与展望</h2><p></p><p>基于构建的先进领域大模型，为其插上搜索的翅膀，打造了革新性的生成式搜索引擎GSE，超越传统搜索与大模型，为用户提供更加高效流畅、专业有趣的搜索选车体验，开辟了新的服务路径，大模型驱动的新范式选车引擎具有很大的发展空间和潜力。产品功能方面，GSE可以提供全新的选车体验，其强大的智能化特性可以实现信息的一站式全场景流转，用户无需再在APP之间频繁地跳转，实现聚集化的流量入口；商业模式方面，基于GSE交互式的搜索形态可以进行更加原生化的广告营销，实现更短的转化路径，创造新颖高效的商业营销模式。基于大模型的生成式搜索，将会是未来业界发展的新兴方向，成为连接人类与信息世界的新桥梁。</p><p></p><p>关于我们我们是汽车之家商业智能团队，主要涉及AI选车、搜索、广告程序化投放、生成式AI等方向业务。真诚欢迎有DL背景、技术扎实的感兴趣小伙伴加入我们（Base 北京），一起打造汽车领域的先进技术引擎。投递简历邮箱： <a href="mailto:linxiyao@autohome.com.cn">linxiyao@autohome.com.cn</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/993d0da9b078aa913e991e24d</id>
            <title>基于Sermant的全链路灰度发布在汽车行业DMS系统的应用</title>
            <link>https://www.infoq.cn/article/993d0da9b078aa913e991e24d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/993d0da9b078aa913e991e24d</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Apr 2024 02:55:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 汽车行业, DMS系统, 全链路灰度发布, Sermant
<br>
<br>
总结: 本文介绍了在汽车行业智能升级的背景下，如何利用Sermant技术为DMS系统提供全链路灰度发布方案，以适应更多灵活多变的业务场景。通过全链路灰度发布方案，可以解决DMS系统在实际应用中的痛点场景，提高系统的灵活性和稳定性。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区《<a href="https://bbs.huaweicloud.com/blogs/424899?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">基于Sermant的全链路灰度发布在汽车行业DMS系统的应用</a>"》，作者：聂子雄 华为云高级软件工程师</p><p></p><h2>摘要</h2><p></p><p></p><p>随着汽车产业的智能升级，DMS系统作为汽车行业的经销管理系统也面临着更加多种多样的业务场景的挑战。借助Sermant，华为云能够为DMS系统提供一整套端到端全链路灰度发布方案，这套方案可以适应DMS系统应用中更多灵活多变的场景。</p><p></p><h2>一、背景</h2><p></p><p></p><p>汽车行业是一个庞大的产业，涵盖了设计、制造、销售、维修等多个方面。目前，全球汽车行业已经成为了重要的经济支柱之一，每年产生数万亿美元的产值。汽车行业的发展也推动了相关产业的发展，如石油、钢铁、橡胶、玻璃等原材料行业，以及银行、保险、物流等服务行业。随着环保意识的提高和新能源技术的不断发展，汽车行业也面临着转型升级的挑战。目前汽车行业正在向智能化、电动化和共享化方向发展，面对更加多种多样的需求，行业相关的IT系统也需要不断进步以适应这些场景。</p><p></p><p>在汽车行业IT系统中，经常会提到DMS系统，那什么是DMS系统呢？DMS系统是指汽车经销商管理系统（Dealer Management System），它是一种专门为汽车经销商和售后服务提供商设计的软件系统，用于管理和优化他们的业务流程。DMS系统可以帮助汽车经销商实现业务数字化、自动化和智能化，提高业务效率、降低成本、提升服务质量，是现代化汽车经销商必备的管理工具。目前很多经销商的DMS系统基本上都是做过微服务化改造，里面不同微服务模块之间会相互调用，因此如何高效地使用和管理这些微服务模块是DMS系统的重要挑战之一。</p><p></p><h2>二、痛点场景</h2><p></p><p></p><p>目前DMS系统已经广泛应用在汽车经销商的业务当中，其中有一类是DMS系统在实际使用中的痛点场景，具体有：</p><p></p><p>客户想在某一个门店A上线自己的新业务，作为业务试点门店，比如新品汽车销售，或者打折促销活动等。和新业务相关的流量只会流入试点门店B。为了节约成本以及降低部署服务工作量，希望能够实现逻辑上的环境隔离。例如，测试环境有部分服务复用生产环境上的模块，开发测试人员只需要聚焦于需要测试的服务模块。客户的交易、商品服务有新的业务要上线，新上线的功能间有依赖和交互，要在上线前做一些测试工作：用户计划让测试人员专门账号来进行现网测试（类似于游戏等白名单控制的开服前验证）；用户引入少量比例的生产流量进行验证。</p><p></p><h2>三、解决方案</h2><p></p><p></p><h3>3.1 单点灰度发布方案是否可行？</h3><p></p><p></p><p>针对上述问题，一般的思路是通过灰度发布去解决，通过灰度发布，可以引入部分的测试流量到新业务模块，也能控制带有具体特征的流量只流入到对应的测试模块，其余流量保持原有方式不动。</p><p></p><p>但是经过仔细考虑，就会发现如果只做单点灰度发布，其实是无法完善地解决以上场景的痛点问题，主要体现在：</p><p></p><p>业务特征时常只在第一跳，也就是特征只在入口，传递过程中会丢失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a6f9fc4300b2174ed39417b857402c17.png" /></p><p></p><p>除了第一跳入口，后续微服务之间进行调用的时候也会把特征给丢失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/6979a8aaed510ff978fd1179e2d8a357.png" /></p><p></p><h3>3.2 全链路灰度方案是怎样的？</h3><p></p><p></p><p>因此，仅仅依靠单点灰度发布的能力是不够的，还需要能够做到整条微服务调用链的可灰度，也就是全链路灰度能力，这样就可以灵活解决DMS系统在这一类业务中遇到的问题。目前要实现全链路灰度，一般要考虑这些问题的处理：</p><p></p><p>1）在第一跳的地方（一般是网关），我们需要能选中各种类型的流量，把这部分流量染色，再路由到正确的目标。</p><p></p><p>2）除了第一跳，剩下调用链路中的各个微服务能够识别染色标，透传染色标，并路由到正确的目标。</p><p></p><p>3）能对异常情况进行妥善处理。</p><p></p><p>目前，华为云针对以上难点，设计出一套相对完善的全链路灰度发布方案，整体方案如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/aefca13a5ad20a8c5f725232665d4254.png" /></p><p></p><p>1）在前端部分，请求会统一携带流量标签参数发到华为云CSE应用网关上面。</p><p></p><p>2）CSE应用网关会选中各种类型的流量，将这些流量根据需求分别染色，比如通过请求header进行标记染色。</p><p></p><p>3）CSE应用网关会将染色后的流量转发到带有不同tag的后端微服务实例，tag可以由应用发布流水线注入到相应发布的微服务实例当中。</p><p></p><p>4）借助微服务实例上挂载的Sermant Agent，接收到应用网关流量的微服务实例会通过Sermant Agent提供的流量染色以及标签透传能力（适配Dubbo，SpringCloud）将流量特征保留并转发到合适的下一跳微服务实例。对于后续链路上的微服务实例，都可以通过微服务实例上面的Sermant Agent进行特征的传递。</p><p></p><h3>3.3 为什么要选择Sermant Agent？</h3><p></p><p></p><p>考虑到目前Dubbo和SpringCloud是国内使用量较多的微服务开发框架，为了不产生过多框架适配工作量，因此选用了基于Java Agent技术的Sermant Agent，利用这种无侵入式的特点，用户只需要在微服务实例启动时将Sermant Agent挂载到实例进程当中即可。Sermant Agent针对Dubbo和SpringCloud两种主流微服务框架适配了流量染色以及标签透传的能力。通过流量染色和标签透传，Sermant Agent就可以实现流量特征的保留以及传递到下一跳。Sermant Agent进行流量染色以及标签透传的全流程如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/7578a771e9304a3d697e9c192054b752.png" /></p><p></p><p>通过配置中心，用户可以在控制台界面下发服务粒度的染色规则，用于判断入方向的流量特征是否符合特定条件，若匹配则对该流量进行染色，在同一调用链的出方向流量中携带该染色标签然后通过请求透传给下一跳，因此用户可以通过下发自定义的染色规则到对应微服务，允许目的特征流量进入微服务实例并为出流量赋予新的特征。</p><p></p><p>在全链路灰度发布的场景下，Sermant Agent染色规则相对简单，sermant Agent会允许目的特征的流量进入微服务实例然后染色时带上相同的特征，染色后的流量再流入到对应tag的微服务实例。</p><p></p><h2>四、总结</h2><p></p><p></p><p>基于Sermant的全链路灰度发布方案可以解决DMS系统目前在一些如新业务在试点门店测试上线等业务场景遇到的困难，并且这一套方案能适应各类敏捷迭代的业务场景。</p><p></p><p>在开发测试过程中，客户可以根据需求在逻辑上划分出一套属于自己的服务链路，只需要关注自己设定的特征流量即可，这种模式可以为客户省去搭建DMS系统中一些共用的模块时间以及节约环境资源，并且还可以很方便地将带有试点特征的流量引入到含有自己试点应用的链路环境当中。</p><p></p><p>在发布过程中，客户还可以根据需要把一部分生产流量引入到自己的新版本业务链路环境当中，完成新版本的验证。</p><p></p><p>----------------------------------------------------------------------------------</p><p></p><p>Sermant作为专注于服务治理领域的字节码增强框架，致力于提供高性能、可扩展、易接入、功能丰富的服务治理体验，并会在每个版本中做好性能、功能、体验的看护，广泛欢迎大家的加入。</p><p></p><p>Sermant&nbsp;官网：https://sermant.ioGitHub&nbsp;仓库地址：<a href="https://github.com/huaweicloud/Sermant">https://github.com/huaweicloud/Sermant</a>"</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>