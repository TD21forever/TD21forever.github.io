<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</id>
            <title>2024版国家人工智能标准化指南揭晓！涉及7个重点方向</title>
            <link>https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 10:34:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 标准体系, 新型工业化, 技术创新
<br>
<br>
总结: 人工智能作为新一轮科技革命和产业变革的基础性和战略性技术，正在成为发展新质生产力的重要引擎。近年来，我国人工智能产业链在技术创新、产品创造和行业应用等方面实现了快速发展，形成了庞大的市场规模。为进一步规范和引领该领域的发展，《国家人工智能产业综合标准化体系建设指南》发布，旨在加快构建满足人工智能产业高质量发展和“人工智能 +”高水平赋能需求的标准体系，推动人工智能赋能新型工业化。 </div>
                        <hr>
                    
                    <p>人工智能作为新一轮科技革命和产业变革的基础性和战略性技术，正在成为发展新质生产力的重要引擎。近年来，我国人工智能产业链在技术创新、产品创造和行业应用等方面实现了快速发展，形成了庞大的市场规模。特别是以大模型为代表的新技术加速迭代，呈现出创新技术群体突破、行业应用融合发展、国际合作深度协同等新特点。然而，随着人工智能技术和产业的迅猛发展，完善的标准体系显得尤为重要。</p><p></p><p>为进一步规范和引领该领域的发展，近日，国家发改委等四部门联合印发《国家人工智能产业综合标准化体系建设指南（2024 版）》（以下统称《指南》），聚焦基础共性标准、基础支撑标准、关键技术标准、智能产品与服务标准、赋能新型工业化标准、行业应用标准、安全／治理标准等 7 个重点方向，加快构建满足人工智能产业高质量发展和“人工智能 +”高水平赋能需求的标准体系，推动人工智能赋能新型工业化。</p><p></p><h2>总体目标，实现人工智能产业全球化</h2><p></p><p>《指南》明确指出，以习近平新时代中国特色社会主义思想为指导，全面贯彻党的二十大和二十届二中全会精神，统筹高质量发展和高水平安全，加快赋能新型工业化。</p><p></p><p>到 2026 年，标准与产业科技创新的联动水平将持续提升，新制定国家标准和行业标准 50 项以上，推动形成引领人工智能产业高质量发展的标准体系。预计参与标准宣贯和实施推广的企业将超过 1000 家，国际标准的制定也将超过 20 项，进一步促进人工智能产业全球化发展。</p><p></p><h2>《指南》要点解读</h2><p></p><p></p><h3>建设思路：多层次、系统化</h3><p></p><p>人工智能产业的标准化建设是一个多层次、系统化的过程，由一系列互相关联的标准构成。根据《指南》，人工智能标准体系结构包括基础共性、基础支撑、关键技术、智能产品与服务、赋能新型工业化、行业应用、安全 / 治理等七个部分。每个部分都涵盖了具体的标准制定方向和要求：</p><p></p><p>基础共性标准：规范人工智能术语、参考架构、测试评估、管理、可持续等方面的标准。基础支撑标准：包括基础数据服务、智能芯片、智能传感器、计算设备、算力中心、系统软件、开发框架、软硬件协同等标准，为人工智能产业发展夯实技术底座。关键技术标准：主要规范人工智能文本、语音、图像，以及人机混合增强智能、智能体、跨媒体智能、具身智能等的技术要求，推动人工智能技术创新和应用。智能产品与服务标准：规范智能机器人、智能运载工具、智能移动终端、数字人、智能服务等方面的标准。赋能新型工业化标准：涵盖研发设计、中试验证、生产制造、营销服务、运营管理等制造业全流程智能化标准，以及重点行业智能升级标准。行业应用标准：规范人工智能技术在各行业场景中的应用，推动产业智能化发展。安全 / 治理标准：规范人工智能安全、治理等要求，为人工智能产业发展提供安全保障。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be452ccefc0ba2ab2c78ae5cbc5aeb18.jpeg" /></p><p></p><h3>指导原则：创新、牵引、协同、开放</h3><p></p><p>为了确保标准体系的科学性和实用性，《指南》还提出了一系列战略性指导原则，通过创新驱动、应用牵引、产业协同和开放合作，加速人工智能产业的高质量发展。</p><p>始终秉持创新驱动的理念。优化产业科技创新与标准化联动机制，加快人工智能领域关键共性技术研究，推动先进适用的科技创新成果高效转化成标准。严格遵循应用牵引的原则。以企业为主体，市场为导向，面向行业应用需求，强化创新成果迭代和应用场景构建，协同推进人工智能与重点行业的融合应用。高度注重产业协同的发展。加强人工智能全产业链标准化工作协同，推动跨行业、跨领域标准化技术组织的协作，打造大中小企业融通发展的标准化模式。着重强调开放合作的策略。深化国际标准化交流与合作，鼓励我国企事业单位积极参与国际标准化活动，与全球产业链上下游企业共同制定国际标准。</p><p></p><h3>新增重点：赋能新型工业化标准</h3><p></p><p>与今年 1 月发布的《国家人工智能产业综合标准化体系建设指南》（征求意见稿）相比，最终版的《指南》在核心内容上有了显著的拓展，特别是新增了“赋能新型工业化标准”这一关键环节。该部分主要着眼于规范人工智能技术如何为制造业全流程智能化及重点行业的智能升级提供技术支撑。具体而言，它涵盖了从研发设计、中试验证，到生产制造、营销服务以及运营管理等制造业全链条的智能化标准设定，并针对关键行业的智能升级提出了明确要求。</p><p></p><p>工业和信息化部电子第五研究所的高级工程师涂珍兰表示，“标准规范体系的建设可以促进科技创新与产业发展的结合，推动科技创新成果快速转化为产品和服务，实现产业升级和经济增长。”</p><p></p><h2>写在最后</h2><p></p><p>人工智能产业标准化体系的构建，离不开产业链上各环节携手共建。《指南》还提出，建立健全人工智能领域标准化技术组织，统筹产学研用各方、产业链各环节优势力量，协同推进人工智能标准建设，共同构建先进适用的人工智能产业标准体系。</p><p></p><p>总的来说，本次《指南》的发布，是我国人工智能产业标准化工作的一项重要举措。通过构建完善的标准体系，将有效推动人工智能技术进步，促进企业发展，引领产业升级，保障产业安全，从而更好地赋能新型工业化。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</id>
            <title>生成式推荐系统与京东联盟广告 - 综述与应用</title>
            <link>https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 10:20:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型语言模型, 自然语言处理, 推荐系统, 生成式推荐系统
<br>
<br>
总结: 本文介绍了大型语言模型对推荐系统的影响，特别是生成式推荐系统的应用。通过深入分析生成式推荐系统的优势和京东联盟广告的挑战，探讨了如何利用大型语言模型重塑推荐系统，为广告领域带来新的见解和启发。文章还详细介绍了生成式推荐系统的四个基本环节，强调了在实践中需要考虑和平衡的细节。 </div>
                        <hr>
                    
                    <p>大型语言模型（LLM）正在深刻地影响自然语言处理（NLP）领域，其强大的处理各种任务的能力也为其他领域的从业者带来了新的探索路径。推荐系统（RS）作为解决信息过载的有效手段，已经紧密融入我们的日常生活，如何用 LLM 有效重塑 RS 是一个有前景的研究问题[20, 25]。</p><p>这篇文章从生成式推荐系统和京东联盟广告的背景入手，首先引出两者结合的动因与策略，随后我们对当前的流程和方法进行了细致的回顾与整理，最后详细介绍了我们在京东联盟广告领域的应用实践。通过深入分析与案例展示，本文旨在为广告领域的推荐系统带来新的见解和启发。</p><p>﻿</p><p></p><h2>一、背景</h2><p></p><p></p><h4>生成式推荐系统</h4><p></p><p></p><p></p><blockquote>A generative recommender system directly generates recommendations or recommendation-related content without the need to calculate each candidate’s ranking score one by one[25].</blockquote><p></p><p></p><p>由于现实系统中的物料（item）数量巨大，传统 RS 通常采用多级过滤范式，包括召回、粗排、精排、重排等流程，首先使用一些简单而有效的方法（例如，基于规则/策略的过滤）来减少候选物料的数量，从数千万甚至数亿到数百个，然后对这些物料应用较复杂的推荐算法，以进一步选择较少数量的物料进行推荐。受限于响应时间的要求，复杂推荐算法并不适用于规模很大的所有物料。</p><p></p><p>LLM 的生成能力有可能重塑 RS，相较于传统 RS，生成式推荐系统具备如下的优势：1）简化推荐流程。LLM 可以直接生成要推荐的物料，而非计算候选集中每个物料的排名分数，实现从多级过滤范式（discriminative-based，判别式）到单级过滤范式（generative-based，生成式）的变迁。LLM 在每个解码步生成一个向量，表示在所有可能词元（token）上的概率分布。经过几个解码步，生成的 token 就可以构成代表目标物料的完整标识符，该过程隐式枚举所有候选物料以生成推荐目标物料[25]。2）具备更好的泛化性和稳定性。利用 LLM 中的世界知识和推理能力，在具有新用户和物料的冷启动和新领域场景下具备更好的推荐效果和迁移效果。同时，相比于传统 RS，生成式推荐系统的方法也更加具备稳定性和可复用性。特征处理的策略随场景和业务的变化将变小、训练数据量将变少，模型更新频率将变低。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/a0/a0e903e83dd5d39a091a150978f44168.png" /></p><p>﻿﻿</p><p>•图 1. 传统推荐系统与基于 LLM 的生成式推荐系统的流程比较[25]</p><p>﻿</p><p></p><h4>京东联盟广告</h4><p></p><p></p><p>京东联盟是京东的一个联盟营销平台，以投放站外 CPS 广告为主。联盟合作伙伴通过生成的链接在其他网站或社交媒体平台上推广京东商品，引导用户点击这些链接并在京东购物，从而获得销售提成（佣金）。京东联盟借此吸引流量，扩大平台的可见度和与用户的接触范围，实现拉新促活等目标。</p><p></p><p>联盟广告推荐主要针对低活跃度用户进行多场景推荐，这样的推荐面临如下的挑战：1）数据稀疏性：低活跃度用户提供的数据较少，导致更加明显的数据稀疏性问题。数据不足使得基于 ID 的传统推荐模型难以充分地对物料和用户进行表征，进而影响推荐系统的预测准确性。2）冷启动问题：对于新用户或低活跃度用户，冷启动问题尤为严重。由于缺乏足够的历史交互数据，推荐系统难以对这些用户进行有效的个性化推荐。3）场景理解困难：在多场景推荐系统中，理解不同场景下用户的具体需求尤为关键。对于低活跃度用户，由于交互数据有限，推荐系统更难以识别出用户在不同场景下的行为差异和需求变化。4）多样性和新颖性：保持推荐内容的多样性和新颖性对于吸引低活跃度用户至关重要。然而，由于对这些用户的了解有限，推荐系统难以平衡推荐的准确性与多样性。</p><p></p><p></p><h4>京东联盟广告+生成式推荐系统</h4><p></p><p></p><p>将 LLM 融入推荐系统的关键优势在于，它们能够提取高质量的文本表示，并利用其中编码的世界知识对用户和物料进行理解和推荐。与传统的推荐系统不同，基于 LLM 的模型擅长捕获上下文信息，更有效地理解用户信息、物料描述和其他文本数据。通过理解上下文，生成式推荐系统可以提高推荐的准确性和相关性，从而提升用户满意度。同时，面对有限的历史交互数据带来的冷启动和数据稀疏问题，LLM 还可通过零/少样本推荐能力为推荐系统带来新的可能性。这些模型可以推广到未见过的新物料和新场景，因为它们通过事实信息、领域专业知识和常识推理进行了广泛的预训练，具备较好的迁移和扩展能力。</p><p></p><p>由此可见，京东联盟广告是生成式推荐系统一个天然的应用场。</p><p></p><p></p><h2>二、生成式推荐系统的四个环节</h2><p></p><p></p><p>为了实现如上的范式变迁，有四个基本环节需要考虑[26]：1）物料表示：在实践中，直接生成物料（文档或商品描述）几乎是不可能的。因此，需要用短文本序列，即物料标识符，表示物料。2）模型输入表示：通过提示词定义任务，并将用户相关信息（例如，用户画像和用户历史行为数据）转换为文本序列。3）模型训练：一旦确定了生成模型的输入（用户表示）和输出（物料标识符），就可以基于 Next Token Prediction 任务实现训练。4）模型推理：训练后，生成模型可以接收用户信息来预测对应的物料标识符，并且物料标识符可以对应于数据集中的真实物料。</p><p></p><p>虽然整个过程看起来很简单，但实现有效的生成式推荐并非易事。在上述四个环节中需要考虑和平衡许多细节。下面详细梳理了现有工作在四个环节上的应用与探索：</p><p></p><h4>物料表示</h4><p></p><p></p><p></p><blockquote>An identifier in recommender systems is a sequence of tokens that can uniquely identify an entity, such as a user or an item. An identifier can take various forms, such as an embedding, a sequence of numerical tokens, and a sequence of word tokens (including an item title, a description of the item, or even a complete news article), as long as it can uniquely identify the entity[25].</blockquote><p></p><p></p><p>推荐系统中的物料通常包含来自不同模态的各种信息，例如，视频的缩略图、音乐的音频和新闻的标题。因此，物料标识符需要在文本空间中展示每个物料的复杂特征，以便进行生成式推荐。一个好的物料标识符构建方法至少应满足两个标准：</p><p>1）保持合适的长度以减轻文本生成的难度。 2）将先验信息集成到物料索引结构中，以确保相似项目在可区分的同时共享最多的 token，不相似项目共享最少的 token。</p><p></p><p>以下是几种构建物料标识符的方法：</p><p></p><p>（1）数字 ID（Numeric ID）</p><p>由于数字在传统 RS 中被广泛地使用，一个直接的策略是在生成式推荐系统中也使用数字 ID 来表示物料。传统 RS 将每个物料 ID 视为一个独立且完整的 token，不能被进一步分割。如果将这些 token 加入到模型中，需要 1）大量的内存来存储每个 token 的向量表示，以及 2）充足的数据来训练这些向量表示。为了解决这些问题，生成式推荐系统将数字 ID 分割成多个 token 组成的序列，使得用有限的 token 来代表无限的物料成为可能。为了有效地以 token 序列表示一个物料，现有的工作探索了不同的策略。1）顺序索引：基于时间顺序，利用连续的数字表示物料，例如，“1001, 1002, ...”，这可以捕捉与同一用户交互的物料的共现（基于 SentencePiece 分词器进行分词时，“1001”和“1002”分别被分词为“100”“1”和“100”“2”）。2）协同索引：基于共现矩阵或者协同过滤信息构建物料标识符，使得共现次数更多的物料或者具有相似交互数据的物料拥有相似的标识符前缀。尽管在生成式推荐系统中使用数字 ID 效果显著，但它通常缺乏语义信息，因此会遭受冷启动问题，并且未能利用 LLM 中编码的世界知识。</p><p></p><p>（2）文本元数据（Textual Metadata）</p><p>为了解决数字 ID 中缺乏语义信息的问题，一些研究工作利用了物料的文本元数据，例如，电影标题，产品名称，书名，新闻标题等。在与 LLM 结合时可借助 LLM 中编码的世界知识更好地理解物料特性。但这种方式有两个问题：1）当物料表示文本非常长时，进行生成的计算成本会很高。此外，长文本很难在数据集中找到精确匹配；仔细检查每个长文本的存在性或相关性将使我们回到判别性推荐的范式，因为我们需要将其与数据集中的每个物料计算匹配得分。2）虽然自然语言是一种强大且富有表现力的媒介，但在许多情况下它也可能是模糊的。两个不相关的物料可能具有相同的名称，例如，“苹果”既可以是一种水果也可以特指苹果公司，而两个密切相关的物料可能具有不同的标题，例如，数据挖掘中著名的“啤酒和尿布”示例[25]。</p><p></p><p>（3）语义 ID（Semantic-based ID，SID）</p><p>为了同时获得具有语义和区分性的物料标识符，现有方法主要通过如下方式对物料向量进行离散化：1）基于 RQ-VAE 模型[8]。RQ-VAE 模型由编码器，残差量化和解码器三部分构成，其输入是从预训练的语言模型（例如，LLaMA[9]和 BERT[28]）提取的物料语义表示，输出是物料对应的 token 序列。在这个分支中，TIGER[7]是一个代表性的工作，它通过物料的文本描述生成对应的 token 序列，并将 token 序列命名为 Semantic ID。LC-Rec[4]设计了多种微调 LLM 的任务，旨在实现 Semantic ID 与用户交互数据或物料文本描述的语义对齐。这两种方法首先将物料的语义相关性捕获到标识符中，即具有相似语义的项目将拥有相似的标识符。然后，标识符表示将通过在推荐数据上训练来优化，以获取交互相关性。相比之下，LETTER[6]通过整合层次化的语义、协同信号和编码分配的多样性来构建高质量的物料标识符。2）基于语义层次化聚类方法。ColaRec[1]首先利用协同模型编码物料，并利用 k-means 聚类算法对物料进行层次化聚类，将分类类别作为物料标识符，之后在微调任务中对齐物料语义信息和交互信息。Hi-Gen[5]则在获取物料标识符的阶段同时考虑了交互信息和语义信息，利用 metric learning 对两种信息进行融合。</p><p></p><p>（4）小结</p><p>以上三类表示方法的对比如下：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/13/13b469423138a71932589c4693203a46.png" /></p><p>﻿﻿</p><p>表 1. 不同离散化物料表示方法的对比</p><p></p><h4>模型输入表示</h4><p></p><p></p><p>在生成式推荐系统中，模型输入由如下的三个部分组成：任务描述、用户信息、上下文及外部信息。其中，用户信息主要包括用户历史交互数据和用户画像。</p><p></p><p>（1）任务描述</p><p>为了利用生成模型的理解能力，任务描述主要用来引导生成模型完成推荐任务，即将推荐任务建模为下一个物料的预测（类比语言模型的 Next Token Prediction，此处是 Next Item Prediction）。任务描述定义了提示词模版，将可利用的数据嵌入其中。例如，“这是一个用户的历史交互数据：{historical behavior}，他的偏好如下：{preference}，请提供推荐。”同时将用户历史交互数据和偏好作为模型输入内容[26]。</p><p></p><p>（2）用户历史交互数据</p><p>用户的历史交互数据在推荐系统中扮演着至关重要的角色，这种互动数据隐性地传达了用户对物料的偏好。用户历史交互数据的表示与上文介绍的物料表示密切相关，现有方法将其表示为：1）物料数字 ID 序列。物料数字 ID 被 LLM 作为纯文本处理，由分词器分割成几个 token。2）物料文本序列。将物料文本元数据进行拼接送入预训练语言模型，语言模型可根据世界知识建模物料之间的相关性。3）物料文本向量加物料 ID 向量序列。LLaRA[2]在物料标题向量后拼接了物料 ID 向量，以补充来自协同模型的交互信息。</p><p></p><p>（3）用户画像</p><p>为了增强用户建模，集成用户画像（例如，关于用户的基础信息和偏好信息）是推荐系统中建模用户特征的一种有效方式。在大多数情况下，用户的基础信息（例如，性别）可以直接从在线推荐平台获取。这些用户信息可与描述性文本结合使用，例如，“用户描述：女性，25-34 岁，在销售/市场营销领域工作”[26]。然而，由于用户隐私问题，获取用户画像可能具有挑战性，导致一些研究直接采用用户 ID 或 ID 向量[3]进行用户建模。</p><p></p><p>（4）上下文及外部信息</p><p>上下文信息（例如，位置、场景和时间）可能会影响用户决策，例如，在户外用品推荐中，用户可能更倾向于购买帐篷而水龙头。因此，在 LLM 中结合诸如时间之类的上下文信息，可以实现有效的用户理解。此外，外部知识也可以用来增强生成式推荐模型的性能，例如，用户-物料交互图中的结构化信息。</p><p>﻿</p><p></p><h4>模型训练</h4><p></p><p></p><p>在推荐数据上训练生成式推荐模型包括两个主要步骤：文本数据构建和模型优化[26]。文本数据构建将推荐数据转换为具有文本输入和输出的样本，其中输入和输出的选择取决于任务定义和物料表示方法。基于数字 ID 和文本元数据的物料表示方法可以直接构建文本数据，基于语义 ID 的方法则需要基于向量进行物料标识符的学习和获取。在模型优化方面，给定&lt;输入，输出&gt;数据，生成式模型的训练目标是最大化给定输入预测输出的条件似然。</p><p></p><p>针对生成式推荐系统，“用户到物料标识符的训练”是主要任务，即输入是用户构建，输出是下一个物料的标识符。基于数字 ID 和文本元数据的方法利用该任务进行模型训练。对于基于语义 ID 的方法，由于语义 ID 和自然语言之间存在差距，一般会利用如下辅助任务来增强物料文本和标识符之间的对齐[4]：1）“物料文本到物料标识符的训练”或“物料标识符到物料文本的训练”。对于每个训练样本，输入输出对包括同一物料的标识符和文本内容，可以互换地作为输入或输出。2）“用户到物料文本的训练”。通过将用户信息与下一个物料的文本内容配对来隐式对齐物料标识符和物料文本。</p><p></p><p>对于训练如 LLaMA 这样的大型语言模型，可采用多种策略来提高训练效率，例如，参数高效微调，模型蒸馏和推荐数据筛选。</p><p></p><h4>模型推理</h4><p></p><p></p><p>为了实现物料推荐，生成式推荐系统在推理阶段需要对生成结果进行定位，即实现生成的物料标识符与数据集中物料的有效关联。给定用户输入表示，生成式推荐系统首先通过束搜索自回归地生成物料标识符。这里的生成方式分为两种：自由生成和受限生成[26]。对于自由生成，在每一个解码步中，模型在整个词表中搜索，并选择概率最高的前 K 个 token（K 值取决于束搜索中定义的束大小）作为下一步生成的输入。然而，在整个词表上的搜索可能会导致生成不在数据集中的标识符，从而使推荐无效。</p><p></p><p>为了解决这个问题，早期工作使用精确匹配进行物料定位，即进行自由生成并简单地丢弃无效的标识符。尽管如此，它们仍然由于无效标识符而导致准确率低，特别是对于基于文本元数据的标识符。为了提高准确性，BIGRec[23]提出将生成的标识符通过生成的 token 序列的表示和物料表示之间的 L2 距离来定位到有效物料上。如此，每个生成的标识符都确保被定位到有效的物料上。与此同时，受限生成也在推理阶段被使用，例如，使用 Trie（prefix tree）或者 FM-index 进行受限生成，保证标识符的有效生成。</p><p></p><p>在预测下一个物料这样的典型推荐任务之外，也可充分利用自由生成产生新的物料描述或预测接下来 N 个物料。</p><p></p><h4>现有工作总结</h4><p></p><p></p><p>当前生成式推荐系统的代表性工作（RecSysLLM[22]，P5[20][24]，How2index[18]，PAP-REC[17]，VIP5[19]，UniMAP[27]，TIGER[7]，LC-Rec[4]，TransRec[16]，M6-Rec[21]，BIGRec[23]，LMRecSys[10]，NIR[12]，RecRanker[13]，InstructRec[11]，Rec-GPT4V[14]，DEALRec[15]）可总结为：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/53/5331491f59babfc82dd3323fcfd9d645.png" /></p><p>﻿﻿</p><p>表 2. 生成式推荐系统的代表性工作[26]</p><p>﻿</p><p></p><h2>三、实践方案</h2><p></p><p></p><h4>总体设计</h4><p></p><p></p><p>基于对现有工作的调研和总结，我们的方案以“基于语义 ID 的物料表示”和“对齐协同信息和文本信息的训练任务”展开：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/de/def063ae2565f101295b50a2a0b96759.png" /></p><p>﻿﻿</p><p>图 2. 总体设计框架图</p><p></p><h4>功能模块</h4><p></p><p></p><p>（1）基于语义 ID（SID）的物料表示</p><p>物料文本描述：基于商品标题表示物料。</p><p>物料向量：通过预训练的 bert-base-chinese 和 Yi-6B 分别提取文本描述对应的向量，向量维度为 768（bert-base-chinese）和 4096（Yi-6B）。</p><p>物料 SID：基于 RQ-VAE 模型对物料向量进行量化。RQ-VAE 模型由编码器，残差量化和解码器三部分构成，其输入是从预训练的语言模型中提取的向量，输出是物料对应的 SID 序列。针对冲突数据，我们采取了两种方式，一种是不进行处理，即一个 SID 对应多个商品；另一种是采用 TIGER 的方案，对有冲突的商品增加随机的一维，使得一个 SID 唯一对应一个商品。例如，商品“ThinkPad 联想 ThinkBook 14+ 2024 14.5 英寸轻薄本英特尔酷睿 ultra AI 全能本高性能独显商务办公笔记本电脑”可表示为：或。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/be/be1853aec112f9b587b6e0cb7a6afa68.png" /></p><p>﻿﻿</p><p>图 3. RQ-VAE 模型图[8]</p><p></p><p>（2）对齐协同信息和文本信息的训练任务</p><p>Next Item Prediction：推荐系统的主任务，即针对给定的用户描述（用户画像+历史交互数据），预测下一个推荐的物料。</p><p>Additional Alignment：由于 SID 和自然语言之前存在差距，通过额外的对齐训练，建立物料 SID 和物料文本描述之间的联系，包括 SID 到文本描述和文本描述到 SID 的两个双向任务。</p><p>﻿</p><p></p><h2>四、离线与在线实验</h2><p></p><p></p><h4>训练数据</h4><p></p><p>（1）Next Item Prediction</p><p></p><p><code lang="text">{
    "instruction": "该用户为都市女性。用户已按时间顺序点击了如下商品：, , , , , , , , , , , , ，你能预测用户下一个可能点击的商品吗？",
    "response": ""
}</code></p><p></p><p>（2）Item and SID Alignment1 - SID2Title</p><p></p><p><code lang="text">{
    "instruction": "商品的标题是什么？",
    "response": "ThinkPad 联想ThinkBook 14+ 2024 14.5英寸轻薄本英特尔酷睿ultra AI全能本高性能独显商务办公笔记本电脑 Ultra5 125H 32G 1T 3K屏 高刷屏"
}</code></p><p></p><p>（3）Item and SID Alignment2 - Title2SID</p><p><code lang="text">{
    "instruction": "哪个商品的标题是\"ss109威震天变形MP威震玩具天金刚飞机威男孩机器人战机模型合金 震天战机（战损涂装版）\"？",
    "response": ""
}</code></p><p></p><p></p><h4>基座模型、训练及推理</h4><p></p><p></p><p>（1）base model: Qwen1.5-0.5B/1.8B/4B 和 Yi-6B</p><p>（2）基于 SID 增加新 tokens，并利用交互数据进行训练</p><p>（3）采用基于 beam search 的受限解码策略，beam size=20</p><p>（4）实验方式：离线实验+线上小流量实验</p><p>（5）离线评估指标：HR@1,5,10; NDCG@1,5,10</p><p>（6）在线评估指标：UCTR</p><p></p><h4>实验结果</h4><p></p><p></p><p>（1）离线实验——同一基座模型不同参数规模的对比：</p><p>◦对比 0.5B/1.8B/4B 的结果可得，模型参数量越大，处理多种任务的能力越强，评估指标值越高；</p><p>◦由于 0.5B 模型能力较弱，不适宜处理多种任务数据，单一任务训练得到的模型相较混合任务有 8 倍提升；</p><p>◦在离线训练和测试数据有 3 个月的时间差的情况下，模型的表现仍然可观。</p><p></p><p>（2）离线实验——不同基座模型的对比：</p><p>◦Yi-6B 模型在不使用受限解码的情况下就有最佳的表现；</p><p>◦微调后的 Yi-6B 指令遵循的能力较好，可进行 next item prediction 和标题文本生成。</p><p></p><p>（3）离线实验——与协同模型结果对比：</p><p>◦在相同的数据规模和数据预处理的情况下，Yi-6B 模型的效果更好；</p><p>◦对稀疏数据进行过滤后训练的协同模型效果会有显著提升，传统模型对数据和特征的处理方式更为敏感。</p><p></p><p>（4）线上小流量实验：</p><p>◦多个置信的站外投放页面的小流量实验显示，基于生成式模型 base 版本可与传统多路召回+排序的 top1 推荐对应的 UCTR 结果持平，在部分页面更优，UCTR+5%以上。</p><p>◦更适合数据稀疏、用户行为不丰富的场景。</p><p>﻿</p><p></p><h2>五、优化方向</h2><p></p><p></p><p>在生成式推荐系统中，构建高质量的数据集是实现精准推荐的关键。在物料表示和输入-输出数据构建层面，将语义信息、多模态信息与协同信息结合，基于联盟场景特点，可以显著提升物料表示的准确性和相关性。</p><p></p><p>为了支持 RQ-VAE 的稳定训练和语义 ID 的增量式推理，需要开发一种可扩展的 SID 训练和推理框架，确保语义 ID 能够快速适应物料变化。</p><p></p><p>此外，优化基座模型是提高生成式推荐系统性能的另一个关键领域。通过训练任务的组合和采用多种训练方式，例如，多 LoRA 技术和混合数据策略，可以进一步增强模型的表现。推理加速也是优化的一个重要方面，通过模型蒸馏、剪枝和量化等技术，可以提高系统的响应速度和效率。同时，基座模型的选型与变更，也是持续追求优化效果的一部分。</p><p></p><p>未来，可考虑引入搜索 query 内容进行搜推一体化建模。此外，引入如用户推荐理由生成和用户偏好生成等任务，可丰富系统的功能并提高用户的互动体验。</p><p>﻿</p><p>我们的目标是通过持续的技术革新，推动推荐系统的发展，实现更高效、更个性化的用户服务。欢迎对这一领域感兴趣的合作伙伴加入我们，共同探索生成式推荐系统技术的未来。</p><p>﻿</p><p></p><h2>六、参考文献</h2><p></p><p>1.Wang Y, Ren Z, Sun W, et al. Enhanced generative recommendation via content and collaboration integration[J]. arXiv preprint arXiv:2403.18480, 2024.</p><p>2.Liao J, Li S, Yang Z, et al. Llara: Large language-recommendation assistant[J]. Preprint, 2024.</p><p>3.Zhang Y, Feng F, Zhang J, et al. Collm: Integrating collaborative embeddings into large language models for recommendation[J]. arXiv preprint arXiv:2310.19488, 2023.</p><p>4.Zheng B, Hou Y, Lu H, et al. Adapting large language models by integrating collaborative semantics for recommendation[J]. arXiv preprint arXiv:2311.09049, 2023.</p><p>5.Wu Y, Feng Y, Wang J, et al. Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce Search[J]. arXiv preprint arXiv:2404.15675, 2024.</p><p>6.Wang W, Bao H, Lin X, et al. Learnable Tokenizer for LLM-based Generative Recommendation[J]. arXiv preprint arXiv:2405.07314, 2024.</p><p>7.Rajput S, Mehta N, Singh A, et al. Recommender systems with generative retrieval[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>8.Zeghidour N, Luebs A, Omran A, et al. Soundstream: An end-to-end neural audio codec[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021, 30: 495-507.</p><p>9.Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023.</p><p>10.Zhang Y, Ding H, Shui Z, et al. Language models as recommender systems: Evaluations and limitations[C]//I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop. 2021.</p><p>11.Zhang J, Xie R, Hou Y, et al. Recommendation as instruction following: A large language model empowered recommendation approach[J]. arXiv preprint arXiv:2305.07001, 2023.</p><p>12.Wang L, Lim E P. Zero-shot next-item recommendation using large pretrained language models[J]. arXiv preprint arXiv:2304.03153, 2023.</p><p>13.Luo S, He B, Zhao H, et al. RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation[J]. arXiv preprint arXiv:2312.16018, 2023.</p><p>14.Liu Y, Wang Y, Sun L, et al. Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models[J]. arXiv preprint arXiv:2402.08670, 2024.</p><p>15.Lin X, Wang W, Li Y, et al. Data-efficient Fine-tuning for LLM-based Recommendation[J]. arXiv preprint arXiv:2401.17197, 2024.</p><p>16.Lin X, Wang W, Li Y, et al. A multi-facet paradigm to bridge large language model and recommendation[J]. arXiv preprint arXiv:2310.06491, 2023.</p><p>17.Li Z, Ji J, Ge Y, et al. PAP-REC: Personalized Automatic Prompt for Recommendation Language Model[J]. arXiv preprint arXiv:2402.00284, 2024.</p><p>18.Hua W, Xu S, Ge Y, et al. How to index item ids for recommendation foundation models[C]//Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 2023: 195-204.</p><p>19.Geng S, Tan J, Liu S, et al. VIP5: Towards Multimodal Foundation Models for Recommendation[C]//Findings of the Association for Computational Linguistics: EMNLP 2023. 2023: 9606-9620.</p><p>20.Geng S, Liu S, Fu Z, et al. Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5)[C]//Proceedings of the 16th ACM Conference on Recommender Systems. 2022: 299-315.</p><p>21.Cui Z, Ma J, Zhou C, et al. M6-rec: Generative pretrained language models are open-ended recommender systems[J]. arXiv preprint arXiv:2205.08084, 2022.</p><p>22.Chu Z, Hao H, Ouyang X, et al. Leveraging large language models for pre-trained recommender systems[J]. arXiv preprint arXiv:2308.10837, 2023.</p><p>23.Bao K, Zhang J, Wang W, et al. A bi-step grounding paradigm for large language models in recommendation systems[J]. arXiv preprint arXiv:2308.08434, 2023.</p><p>24.Xu S, Hua W, Zhang Y. Openp5: Benchmarking foundation models for recommendation[J]. arXiv preprint arXiv:2306.11134, 2023.</p><p>25.Li L, Zhang Y, Liu D, et al. Large language models for generative recommendation: A survey and visionary discussions[J]. arXiv preprint arXiv:2309.01157, 2023.</p><p>26.Li Y, Lin X, Wang W, et al. A Survey of Generative Search and Recommendation in the Era of Large Language Models[J]. arXiv preprint arXiv:2404.16924, 2024.</p><p>27.Wei T, Jin B, Li R, et al. Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm[C]//The Twelfth International Conference on Learning Representations. 2023.</p><p>28.Kenton J D M W C, Toutanova L K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of NAACL-HLT. 2019: 4171-4186.</p><p>29.Zhai J, Liao L, Liu X, et al. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations[J]. arXiv preprint arXiv:2402.17152, 2024.</p><p></p><p>作者：广告研发部&nbsp;申磊</p><p>来源：京东零售技术 转载请注明来源</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</id>
            <title>AI 卷生卷死的 Q2 终于结束了，InfoQ研究中心内容洞察集锦助你 Q3 先人一步</title>
            <link>https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 09:34:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ研究中心, 人工智能, 大模型, AGI
<br>
<br>
总结: InfoQ研究中心持续关注人工智能领域发展，发布多份人工智能相关研究报告，今年聚焦大模型及其产业应用，关注生成式AI开发者、技术动态趋势和5大行业实践情况，致力于助力中国大模型及AGI的商业落地。 </div>
                        <hr>
                    
                    <p>InfoQ&nbsp;研究中心自创立以来就持续关注&nbsp;AI&nbsp;领域的发展和更新，并持续推出了《中国开源生态图谱&nbsp;2023——人工智能领域》、《大语言模型综合评测报告&nbsp;2023》、《2023&nbsp;中国人工智能成熟度模型报告》、《大语言模型综合评测报告&nbsp;2024》等人工智能相关的研究报告。</p><p>今年以来，InfoQ研究中心将大模型及其产业应用，作为今年的研究主线之一。我们也认识到，技术本身无法孤立存在，本轮大模型的浪潮亦是如此，InfoQ研究中心对技术市场的用户分析和趋势分析，也离不开目前千行百业的大模型实践。因此第二季度，InfoQ研究中心继续聚焦生成式AI开发者、技术动态趋势以及5大行业实践情况，持续关注本轮大模型及AGI的浪潮。我们期望通过报告、文章、指南等多种形式，分享我们的研究成果和见解，共同助力中国大模型及AGI的商业落地。</p><p>更多精彩内容欢迎大家，点击文末海报，关注「AI前线」公众号，回复对应关键词领取，也可以扫描海报右下方二维码，直达「行业研究报告」专题。</p><p></p><h3>热门报告</h3><p></p><p></p><h4>《中国生成式AI开发者洞察2024》——聚焦技术市场人才分析</h4><p></p><p>报告回答关键问题：生成式AI的开发者面临怎样的薪资变化？什么样的岗位紧缺，开发者又需要具备什么样的技能目前的生成式&nbsp;AI&nbsp;开发者都在关注哪些领域的应用？</p><p></p><h4>《2024年第1季度中国大模型季度监测报告》——聚焦技术市场动态监测</h4><p></p><p>报告回答关键问题：Sora&nbsp;来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin&nbsp;和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？</p><p></p><h4>《中国AGI市场发展研究报告&nbsp;2024》——聚焦行业应用与实践</h4><p></p><p>报告回答关键问题：AGI&nbsp;概念引发热议，那么&nbsp;AGI&nbsp;究竟是什么？从技术架构来看，AGI&nbsp;又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？</p><p></p><h3>热门文章</h3><p></p><p>大模型的“瘦身”革命：巨头逐鹿轻量化大模型&nbsp;|&nbsp;大模型一周大事Stability、Mistral、Databricks、通义、A21&nbsp;Labs&nbsp;开源领域五连招，其中三个是&nbsp;MoE！|大模型一周大事国产版&nbsp;Sora&nbsp;到来！视频大模型更上一层楼&nbsp;|&nbsp;大模型一周大事文生视频模型“卷”出新天际；多家手机厂商&nbsp;AlI&nbsp;in&nbsp;Al，终端&nbsp;AI&nbsp;时代来临？|大模型一周大事让智能设备更懂你，主动式&nbsp;AI&nbsp;正在崛起&nbsp;|&nbsp;大模型一周大事</p><p></p><h3>热门图谱</h3><p></p><p></p><h4>中国大模型产品罗盘，涵盖&nbsp;200+&nbsp;主流大模型产品</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/98/985fef90d93c8a33a6509ca5d08402da.png" /></p><p></p><p>2024年第三季度，InfoQ研究中心也将继续前行，陆续发布各类重磅报告，欢迎大家持续关注。</p><p>7月：《中国开发者画像洞察研究报告&nbsp;2024》8月：《中国&nbsp;AI&nbsp;Agent&nbsp;应用研究报告》8月：《AGI&nbsp;在金融领域的应用实践洞察》</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a3bbe3a53a92e322aae7ab2025d6c21e.jpeg" /></p><p></p><p>机构介绍</p><p>InfoQ&nbsp;研究中心隶属于极客邦科技双数研究院，秉承客观、深度的内容原则，追求研究扎实、观点鲜明、生态互动的目标，聚焦创新技术与科技行业，围绕数字经济观察、数字人才发展进行研究。</p><p>InfoQ&nbsp;研究中心主要聚焦在前沿科技领域、数字化产业应用和数字人才三方面，旨在加速创新技术的孵化、落地与传播，服务相关产业与更广阔的市场、投资机构，&nbsp;C-level&nbsp;人士、架构师/高阶工程师等行业观察者，为全行业架设沟通与理解的桥梁，跨越从认知到决策的信息鸿沟。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</id>
            <title>李彦宏WAIC圆桌访谈：开源模型是智商税，智能体正在爆发</title>
            <link>https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 08:08:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 李彦宏, 大模型, 智能体
<br>
<br>
总结: 在2024世界人工智能大会期间，百度创始人李彦宏与其他嘉宾进行了一场圆桌访谈，讨论了大模型和智能体在人工智能领域的重要性。李彦宏强调了大模型在各行业应用中的潜力，以及智能体作为未来趋势的重要性。他认为，大模型的应用将带来更多的效率提升和成本节约，而智能体的低门槛将促进更多人参与创新，可能会诞生出未来的超级应用。 </div>
                        <hr>
                    
                    <p>在2024世界人工智能大会（WAIC 2024）期间，百度创始人、董事长兼首席执行官李彦宏，与第一财经传媒集团总编辑杨宇东和《硅谷101》创始人陈茜，进行了一场圆桌访谈。</p><p></p><p>在一个小时的对话中，李彦宏讲了几个独特观点：</p><p>一，现在很少有幻觉问题了。解决幻觉问题是在原来Transformer架构上，增加一些东西，“专业词语叫RAG”。</p><p>二，模型的推理成本，其实几乎是可以忽略不计。</p><p>三，开源其实是一种智商税。你永远应该选择闭源模型。</p><p></p><p>另外，李彦宏认为“没有应用的大模型一文不值”，呼吁行业不要卷模型了，要去卷应用。应用其实离大家并不遥远，基于基准模型应用在各行各业已经开始逐步渗透。他援引文心一言的调用量数据，两个月前还在2亿，现在已经到了5亿，说明大模型背后代表了真实的需求，有人真的从大模型当中获益了。</p><p></p><p></p><h3>以下是“百度”官方公众号发布的圆桌对话原文：</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/71/7139e76f64f5b5cb97ea9a342f981ff7.webp" /></p><p></p><p></p><h3>超级应用什么时候出现？基础模型之上将诞生数以百万计的应用</h3><p></p><p></p><p>杨宇东：由ChatGPT掀起的这个热潮已经持续一年多了，你也曾表达，接下来超级应用什么时候出现？我们看到国内面向C端的大模型产品形态，看起来都差不多，都是搜索框+问答这种模式，你怎么看？有没有可能产生一种差异化的竞争？什么样的好产品会出现？</p><p></p><p>李彦宏：我倒不是说一定在等待一个超级应用的出现。我更觉得，在基础模型之上，应该能够诞生数以百万计各种各样的应用。这些应用有些是很小的领域，一个大家可能不太容易想到的应用，但它对于那个领域的问题，解决得比以前好很多。确切的讲，我现在还没有看到，能够比肩移动互联网时期超级应用那样的AI时代的原生应用。但是已经看到，越来越多在各种各样场景、尤其是To B场景中，利用大模型提升了效率，产生了更多的收入，或者说节省了更多成本的情况出现。</p><p></p><p>今天，大家都在想，我能不能从0到1，做出一个人们想也没想到过的东西？变成一个DAU10亿的超级应用？这个当然很重要，假以时日也一定会出现。但是，更重要的是大模型在各个领域、各个场景的应用。</p><p>从百度文心一言的日调用量来看，已经非常明显。我们在4月份曾经公布过一个数据，文心一言的调用量每天有2亿次。前几天，我们再公布的时候，文心一言调用量已经到了5亿次。也就是说，两个月的时间调用量是double。调用背后意味着什么？意味着它在给应用产生价值。因为没有价值的话，人家也不会花钱去调用。</p><p></p><p>杨宇东：C端用户会有什么样很好的场景？包括端侧、手机上的APP，如何去调用AI能力？</p><p></p><p>李彦宏：我觉得分两类：一类是大家比较关注的，过去从来没有过的应用。现在比较流行的、类似于ChatGPT这样的ChatBot，就是聊天机器人。国内每一个大模型公司，都会推出一个相应的APP，或者是网站来做ChatBot。</p><p></p><p>对于现有这些To C的应用，其实它的信息增益作用也是非常大的。我们在4月份的时候，公布过一个数据，百度搜索今天有11%的搜索结果会由AI来生成，这个比例还在不断提升。再比如说百度文库，过去，百度文库是大家在上面找一些现成的文档。今天，百度文库经过大模型改造之后，已经更多地变成了生成式AI应用。你不管想要生产什么样的文档，是PPT、是论文的格式、甚至是漫画，它都可以根据你的要求生成。今年以来，文库已经有大约2600万付费用户。如果说用超级应用的标准来看，它也没有达到超级应用的水准，但是要看它实际产生的价值，有那么多人愿意为这个产品付费，还是很厉害。这些产品都是过去已经存在，但经过了大模型改造之后，它的能力跟以前完全不一样了。</p><p></p><p>陈茜：我特别同意你最近在多个场合强调的，去卷AI原生应用，大模型才有意义。但到今天，我们还没有看到应用的爆发，可能很多应用出来也不太尽人意。所以我的问题或者疑惑在于，如果从模型能力上看，是不是现在还没有到去卷应用的时候？</p><p></p><p>李彦宏：大模型应用其实已经逐步在浮现出来，它对于现有业态的改造作用，比从0到1的颠覆作用，更早到来。这个过程一开始大家觉得，没有那么性感，但是它对于人的工作效率的提升，对于成本的下降，对于打开新的可能性，产生的促进作用，是比那些从0到1的应用，反而更大。如果仅仅是从0到1，你可能会希望出现某几个Super APP，也就是几个公司从中受益。但是今天，几乎各行各业所有的公司，被大模型加持之后，它都能受益。这种影响力，对于整个社会、对于人类来说，无疑是更大的。</p><p></p><p>只是大家觉得，以前都存在，这个东西我以前见过，所以没有新鲜感。或者它更多诞生在生产力场景，它的受众群体，或者单一应用的受众群体，不会过亿过十亿。尤其在C端，在公众层面体感没有那么集中。这是大家一直在寻找一个Super APP的原因。</p><p></p><h3>为什么智能体是未来趋势？门槛足够低，跑通了就是Super APP</h3><p></p><p></p><p>杨宇东：我们前面聊的是“卷应用”，接下来还有一个关键词叫“智能体”。你说过好多次，AI时代最看好的应用是智能体。但我们目前并没有看到智能体的爆发，为什么你认为智能体是AI时代的未来趋势呢？</p><p></p><p>李彦宏：我觉得智能体正在爆发，只是说它现在基数还比较小，大家的体感没有那么强烈。但是你要看业界大模型公司，都在做智能体。智能体就是一个几乎可以“放之四海而皆准”的基于大模型的应用。今天大多数AI原生应用，你用智能体的方式都可以做出来，效果也不错。由于它门槛足够低，可能你连编程都不用，就可以做出一个效果不错的智能体。</p><p></p><p>门槛足够低，就意味着越来越多的人，可以做出他想要的智能体。这个有点像90年代中期时候的互联网网站。你可以把它做得非常复杂，比如雅虎就是很厉害的网站。但是在学校读书的大学生，他也可以做一个自己的Home Page。由于做网站很简单，在90年代中后期，就诞生了数以百万计的网站。大浪淘沙之后，最终出来了一些非常优秀的网站，像Google、Facebook，这是若干年之后才出现。但是早期看，这些网站都是乱糟糟的，一个大学生就能做一个网站出来，这有啥价值？但是你必须得门槛足够低的时候，让更多人进来，他们发挥聪明才智，指不定哪条路跑通了，它就是一个Super APP。</p><p></p><p>陈茜：业界对AI Agent的定义，还是有一点不同。你对Agent的定义是什么？</p><p></p><p>李彦宏：我首先要考虑，这个门槛要足够低，一个小白，大一的学生，他也可以很方便地制作一个智能体。当然在此之上，可以有各种各样比较fancy的玩法，调用工具、反思、长期的记忆等等，这些能力会逐步加进去。</p><p></p><p>不是说用了最先进的这些能力之后，它才叫一个AI Agent。我反而觉得，我们要把门槛降得足够低，让大家觉得，我也可以搞一个AI Agent。</p><p></p><p>说实话，我认为现在AI Agent用到的这些能力，仍然是非常初级的，未来还会产生我们今天想也想不到的Agent能力。但是这些能力的诞生，反而要依赖数以百万计的开发者，去开发各种各样的应用。在他们使用的过程当中产生新的需求，这些需求被解决的过程，就是一个创新过程，就是AI Agent进化的过程。</p><p></p><p>陈茜：百度有什么比较有意思的AI Agent案例，可以给我们分享一下吗？</p><p></p><p>李彦宏：有很多。国内高考是一个非常大的事件，不仅是学生，还有家长都非常重视。过去大模型在干什么事？高考有作文题，我们用大模型来写一个作文，看它能得多少分。其实你想一想，这个东西在哪用呢？不可能让一个考生带着大模型去参加高考。但是高考完了之后，你要估分，要报志愿，要选择学校，你要选择专业，一个考生他该报哪个学校，哪个专业，每个人情况都是不一样，每个人的问题也都是不一样。这种情况下，哪有一个全能的辅导老师可以告诉你，你最适合的是哪个学校哪个专业？但是AI Agent就可以干这个事情。我们开发了一个高考填报志愿的AI Agent。在高峰时期，一天有200万人在使用，足见大家对这个东西的认可度和依赖度还是非常高的。</p><p></p><p></p><h3>大模型对B端的改造比互联网更大，规模更小一点的模型市场需求量更大</h3><p></p><p></p><p>杨宇东：通用大模型和行业垂直大模型，它将来到底是什么样的关系？</p><p></p><p>李彦宏：大模型在各个垂直场景里怎么用？我们经过了一个探索过程。最初我们的想法是，我把这个基础模型做得越来越强大，大家叫通用人工智能，在什么场景我都能做得很好。后来发现这件事情没有那么容易，每个场景都有它自己的道。当应用场景需要反应快的时候，我们需要更小的模型。这种小的模型，它由于没有大模型通用的能力，所以在垂直场景当中，还要对它做精调，就是SFT，把你行业的数据怎么灌进去，再经过各种各样的调教，在这个场景里的效果，就能够跟大模型相比差不多。</p><p></p><p>类似这种场景，我们也见了很多。去年10月份，我们发了文心4.0之后，主要精力在做什么呢？就是根据最强大的模型，去裁剪各种体量的小模型，比如说十亿量级的模型，百亿量级的模型，千亿量级的模型，这个模型也许擅长角色扮演，那个模型也许擅长逻辑推理等等，根据客户不同使用场景的不同要求，做出各种各样的模型。这些模型大多数速度都比EB4要快，推理成本比它要低，所以这种应用是大家更爱用的。今天你要看市场需求的话，规模更小一点的模型，市场需求量是更大的。</p><p></p><p>杨宇东：你为什么认为，大模型对B端的改造，比互联网对B端的影响更大？</p><p></p><p>李彦宏：互联网对C端的改造，我们都是感同身受的，是非常彻底的，是颠覆性的。但是互联网对B端的改造，我觉得一般般。用的技术比较简单，产生的增益也没有那么明显。但大模型不一样。我们接触到的一些能源电力、生产制造等企业，都有类似的需求。比如说，现在国内电动车卷得也很厉害，车内的对话系统，很多也在用文心大模型，使用量也不小，但是对百度来说，这就是一个To B的应用，我们不直接提供给用户，它是经过了OEM，经过了车厂的集成之后，把这个应用提供给了终端消费者。这种事情其实非常多，而且我们就看调用量，如果调用量上得很快，这就说明我们的客户需要这些东西，B端靠着这个大模型，靠着AI原生应用产生了价值。</p><p></p><p>杨宇东：在金融、医疗等这些比较严谨的领域，生成式AI的幻觉问题，怎么破解？</p><p></p><p>李彦宏：今天，应该说你会很少发现幻觉问题了，尤其是用最大规模、最强大模型的时候，已经很少出现幻觉问题了。为什么呢？一开始，纯粹用原来的Transformer去做出来的大模型，它确实是非常难避免幻觉的，因为它是个概率模型。</p><p></p><p>要解这个问题，就要在原来Transformer架构上，增加一些东西，专业词语叫RAG。我只要稍微借助一点工具，就可以消除这样的幻觉。随着使用这种工具的能力越来越强，你就会发现，在各种场景下，幻觉是越来越少的。</p><p></p><p>当然，今天这种生成式人工智能，更像是一个Copilot，在特别严肃、对准确度要求特别高的场景下，我们还不能让它全部自动实现，还要靠人把最后一道关。这样，一方面可以提升效率；另一方面，在准确度上、在消除幻觉上，也能够起到比较重要的作用。</p><p></p><p>陈茜：现在企业对AI的使用成本怎么看？是否愿意为AI付费？你在跟一些企业客户交流的时候，他们的态度是什么样子的？</p><p></p><p>李彦宏：当你处在市场经济环境当中，企业其实是非常理性的。尤其是中小企业，账算得非常精。如果这件事情能够让我降本增效，能够让我赚到更多的钱，那我就会用它。如果不能，你再吹破天，我也不会用。市场会告诉你，大模型到底有用还是没用？我们看到调用量的迅速提升，确实是因为在用户侧、在客户侧，它为企业产生了降本增效的作用。</p><p></p><p>我再举个例子，比如说在招聘场景。过去是怎么做的？是HR坐在那，一份一份简历筛查，然后一个一个面试，面试100个人，最后筛出来10个人，再进行下一步面试，效率是非常非常低。但是大模型进来之后，它可以非常明显地去提升效率。因为，用大模型去理解这是一个什么人，理解这个老板要招什么样的人，然后进行匹配，它的效率就会高很多。</p><p></p><p>而且，你去算一算模型的推理成本，其实几乎是可以忽略不计的。尤其在国内，现在大模型价格战是非常厉害的，百度的轻量级模型都是免费的，这个免费不仅仅指的是模型免费，实际上算力也送你了，你本来要有电脑，要有带宽等等，这些都没有了，你只要来用就好。</p><p></p><p></p><h3>如何看“开源闭源之争”？开源是一种智商税，闭源模型比开源模型更强大</h3><p></p><p></p><p>杨宇东：开源闭源问题是业界关注焦点。你认为，闭源模型会持续领先。但我们看到，开源大模型越来越多，甚至有些能力都不亚于我们说谓的GPT4了，这个问题你怎么看，你们还是会坚定的走闭源路线？</p><p></p><p>李彦宏：我觉得，开源其实是一种智商税。你仔细想一想，我们为什么要做大模型？是它能够有应用，这些应用在各种场景下，能够为客户为用户提升效率、降低成本，产生过去产生不了的作用。所以当你理性的去想，大模型能够给我带来什么价值？以什么样的成本给我带来价值？你永远应该选择闭源模型。今天这些闭源模型，无论是ChatGPT还是文心一言，以及其他各种各样的闭源模型，它的平均水平，一定是比这些开源模型更强大，推理成本一定是比开源模型更低。</p><p></p><p>陈茜：百度对To B客户，是“闭源+公有云”这样一套打法，有什么考量吗？</p><p></p><p>李彦宏：ToB的客户，他要选择的是一个性价比最好的模型。一方面，模型要对他的应用产生价值，另外一方面，成本要足够低。很多时候，你看着有用，一算成本不划算，客户就放弃了。这是为什么我刚才讲，开源模型打不过闭源模型。你只要理性的去看待，你的收益是啥，你的成本是啥，你就会发现，最好还是去选择闭源模型。当然，闭源模型不是一个模型，它是一系列的模型，根据你的使用场景去平衡一下，要多好的效果，要多快的推理速度，要多低的成本。模型有非常多的变种，可以根据用户的需求，让他来做选择。</p><p></p><p>闭源模型还有一个开源模型不具备的优势：这些相对来说规模更小的模型，都是从最大最powerful的模型裁剪出来的，裁剪出来这些更小规模的模型，仍然比那些同样规模的开源模型要效果更好。</p><p></p><p>陈茜：百度对于中小模型、模型蒸馏上，有什么样的策划？</p><p></p><p>李彦宏：我们看到的真实需求，在绝大多数情况下都不是最大的模型，它都要求这个模型变小。变小意味着什么？速度快，成本低。比如说，我干这个事儿，总共能够给我带来每年100万的增益，但使用最大的模型要120万的成本，那我肯定不干了。那我就会给大模型公司提要求，把成本降到80万，甚至降到8万。那我们就得想，怎么把最强大的模型，蒸馏到足够小，成本足够低，满足这个场景需求。因为闭源有一个最强大的基础模型，根据模型蒸馏或者裁剪出来的小模型，比那些开源模型做出来的东西更有竞争力。所以我们觉得，To B的机会仍然在闭源不在开源。</p><p></p><p></p><h3>大模型价格战不可避免，最终还是比谁的技术好、效率高</h3><p></p><p></p><p>杨宇东：我们现在看到价格战已经开始打起来，其实还是蛮出乎我们的预料，这么快。</p><p></p><p>李彦宏：价格战几乎不可避免，在中国互联网干了这么长时间，其实已经对价格战非常熟悉。但就像你讲的，确实来得比我想象的更早一点，这么早就开始把价格打到几乎不可思议低的地步。但某种意义上讲也不是坏事儿，当你足够低，甚至免费的时候，就会有更多人有积极性来尝试，在大模型基础上去开发各种各样的应用，大模型对于各行各业的渗透速度会更快。</p><p></p><p>杨宇东：很多闭源大模型API调用费越来越低，大模型靠推理收费的商业模式未来成不成立？以后大模型比拼的是哪些点？</p><p></p><p>李彦宏：大模型技术天花板还是很高的，今天我们还是对于大模型的能力有很多不满意的地方，仍然需要很多非常优秀的技术人员、需要很多算力、需要很多数据，才能训练出下一代大模型，我们还可能需要下下一代、下下下一代的大模型。</p><p></p><p>所以最终我觉得大家是要去拼谁的技术更好，你的技术好，你为客户产生了更好的价值。今天之所以把这个模型打到足够低，是因为现在模型的这个能力其实还没有到最好，没到最好的时候，大家都差不多的时候，就会谁的价格低就用谁的。</p><p></p><p>时间长了之后，市场本身会回归理性。最终还是比谁的技术好，谁的效率高，谁会胜出。</p><p></p><p>陈茜：你觉得这个价格战会持续多久的一个时间呢？</p><p></p><p>李彦宏：这个很难讲，现在有些创业公司是玩家，也有很多非常大型的互联网平台公司是玩家，其实理论上讲是可以烧很长时间。但我觉得烧钱不是事情本质，事情本质仍然是谁的技术更好，谁的效率更高，当你的技术好、效率高的时候，你就不怕去打这个价格战，所以多长时间都OK，最终会是优胜劣汰的过程。</p><p></p><p>陈茜：你觉得在中国市场会是一个赢家通吃这样的一个局面吗？还是说等价格战之后会剩下几个主要的？可能还有一些更小一点的？</p><p></p><p>李彦宏：这次生成式AI是对整个IT技术栈的大变革，过去IT技术栈是芯片层、操作系统层、应用层或者软件层，就这三层。到生成式AI之后，IT技术栈变成了四层，芯片、深度学习框架层、模型层、应用层，我认为在每一层可能都会诞生至少2—3个大玩家。</p><p></p><p>应用层的话，可能会有数以百万计、甚至数以千万计的各种各样应用出来，也会逐步出现超级应用，既然是超级应用，当然不会很多，可能是三五个。</p><p></p><p>模型层我觉得也许两三个就足够了，因为最后大家比拼的是效率，你的效率如果不够高的话，慢慢就觉得说还不如用别的。</p><p></p><p></p><h3>Scaling Law短期内不会被颠覆，图灵测试不再是标准，AGI需要十年以上才能实现</h3><p></p><p></p><p>杨宇东：Scaling Law还会持续有效吗？</p><p></p><p>李彦宏：Scaling Law可能还会有若干年的生命周期。但与此同时，在此之上会叠加各种各样的创新。刚才讲的智能体，它的反思、进化能力等，其实跟Scaling Law已经是两个路线在发展，但它仍然是基于Transformer这类大模型往上做。未来再过一两年，还会出现什么新的技术创新，在此基础上再去叠加，大家都在探索。换句话说，我觉得Scaling Law短期之内不会被颠覆，但是在Scaling Law之上会叠加出来很多我们现在可能还无法想象的创新。</p><p></p><p>杨宇东：你认为AGI实现的标准是什么？还有哪些路径可以让我们更快地通向AGI？</p><p></p><p>李彦宏：业界确实还没有一个标准答案。以前大家觉得，通过图灵测试就实现AGI了，实际上现在大模型已经通过了图灵测试。人们所说的AGI，其实大多数时候已经不是只通过图灵测试了。</p><p></p><p>那么什么叫AGI？在我心目中，AGI就是机器或者说AI，能够具备人在任何场景下所具备的能力。Artificial General Intelligence，就是通用人工智能，它不管在什么场景下，能力都是跟人一样的，这是一个很高的要求。</p><p></p><p>所以真正要实现AGI，我认为还需要很多很多年。业界有人说AGI可能再过2年，或者再过5年能实现。我自己的判断是10年以上，也许更长的时间。我们听到很多人讲，AGI是一种信仰，当你把它当做一种信仰的时候，谁的信仰会明年就实现？这是自相矛盾的。如果是一个信仰，它就是你值得为之长期奋斗的一个目标。</p><p></p><p>陈茜：现在GPT5一直在延后，担忧的声音也越来越高，AGI没有办法用Scaling Law这个方式去带我们实现了，你对这个有担忧吗？</p><p></p><p>李彦宏：我不是很担心这件事情，我觉得大家应该更关注应用，而不是关注基础模型，某种意义上基础模型迭代速度稍微放缓一点不是坏事，如果今天的应用开发者，有一个相对稳定的基础来开发应用，其实是效率更高一些的，如果模型天天在那儿练，每天都要重写一遍过去的代码，那是很累的。但是在现有基础模型上不断去做微调，去做一些渐进式的迭代和和创新，其实你看到是一直在发生的，无论是OpenAI不断在推的，还有百度我们的Turbo模型、更小量级的模型等等，都是在根据市场的需求在做迭代。</p><p></p><p>但长远来讲，我确实认为下一代大模型一定会比现在这一代模型强大得多。什么时候推出来我不是很着急，我们应该更多的去看真实的市场需求，下一代模型在迭代的时候，要根据市场需求来迭代。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</id>
            <title>成立半年多就敢踢馆 OpenAI ，首个开源模型不输 GPT-4o，LeCun 、PyTorch 之父齐声叫好！</title>
            <link>https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 07:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI模型, Moshi, Kyutai, 多模态模型
<br>
<br>
总结: Kyutai团队开发了一种名为"Moshi"的AI模型，具有多种情绪表达和语音模仿能力，同时处理两个音频流。这个模型被称为世界上首个具有自然对话能力的AI助手，具有改变人机通信的潜力。Moshi还能处理文本和音频，支持同时听和说，具有文本思想和情商，能够在半秒内回复。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>想象一下，一个&nbsp;AI&nbsp;模型可以表达&nbsp;70&nbsp;多种情绪，以不同的风格说话，甚至令人信服地模仿口音。并且，它能够同时处理两个音频流，同时听和说。这不是科幻小说，而是Kyutai在语音AI技术上的最新突破。</p><p>&nbsp;</p><p>只用短短&nbsp;6&nbsp;个月的时间，这个由&nbsp;8&nbsp;人组成的非营利性&nbsp;AI&nbsp;研究实验室从零开发出了一种名为&nbsp;"Moshi&nbsp;"的实时原生多模态基础&nbsp;AI&nbsp;模型。根据&nbsp;Kyutai&nbsp;的说法，Moshi&nbsp;是世界上首个具有自然对话能力的可公开访问&nbsp;AI&nbsp;助手。OpenAI&nbsp;之前曾展示过<a href="https://www.infoq.cn/article/42ROdXw5VHrfFMsITd07">GPT-4o&nbsp;</a>"的语音引擎和语音模式功能，但尚未发布。</p><p>&nbsp;</p><p>据称，该模型具备的功能可与&nbsp;<a href="https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI&nbsp;</a>"的&nbsp;GPT-4o&nbsp;和&nbsp;Google&nbsp;Astra&nbsp;相媲美，但模型要小得多。“Moshi&nbsp;在说话时思考。”Kyutai&nbsp;首席执行官帕特里克·佩雷斯&nbsp;（Patrick&nbsp;Pérez）&nbsp;表示，Moshi&nbsp;具有彻底改变人机通信的潜力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84b023c0fca40c5d913743b7c2743b86.jpeg" /></p><p></p><p>7月4日，Kyutai在法国巴黎公开发布了Moshi&nbsp;的实验原型，用户可以在网上自由<a href="https://moshi.chat/?queue_id=talktomoshi">测试体验</a>"。值得一提的是，Kyutai的所有模型都是开源的。之后，该团队不仅计划发布完整模型，包括推理代码库、7B 模型、音频编解码器和优化堆栈。</p><p></p><p>图灵奖得主<a href="https://www.infoq.cn/article/Gf8Z4CVHwvqLEOXGlY9c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Yann&nbsp;LeCun</a>"分享说：“Moshi可以听懂带有法国口音的英语。”就连&nbsp;PyTorch&nbsp;之父Soumith&nbsp;Chintala也向Kyutai表示了祝贺，并透露该团队某成员是他在Meta&nbsp;的&nbsp;AI&nbsp;研究团队&nbsp;FAIR&nbsp;的前同事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80aba663529836be3c65b6cf65fc02a2.png" /></p><p>Kyutai团队</p><p>&nbsp;</p><p>据悉，这家成立于&nbsp;2023&nbsp;年&nbsp;11&nbsp;月的初创团队，得到了包括法国亿万富翁&nbsp;Xavier&nbsp;Niel&nbsp;在内投资的近&nbsp;3&nbsp;亿欧元的支持，旨在为&nbsp;AI&nbsp;的开放研究做出贡献并促进生态系统发展。Kyutai&nbsp;还组建了一支由知名人工智能研究人员组成的科学顾问团队——计算机科学家、2022&nbsp;年麦克阿瑟“天才”奖获得者Yejin&nbsp;Choi，Meta&nbsp;首席&nbsp;AI&nbsp;科学家、ACM图灵奖获得者Yann&nbsp;LeCun&nbsp;和德国马克斯·普朗克智能系统研究所研究所所长Bernhard&nbsp;Schölkopf。</p><p></p><h1>对话流畅又会整活，甚至还会“抢话”</h1><p></p><p>在现场演示过程中，Kyutai&nbsp;团队与&nbsp;Moshi&nbsp;进行互动，展示了其在各种说话风格之间无缝切换，以及在角色扮演中迅速化身的创造力。</p><p>&nbsp;</p><p>当被要求用法国口音说话时，Moshi&nbsp;朗诵了一首关于巴黎的诗；在被要求变身为一个热情洋溢的海盗时，Moshi&nbsp;讲述了七大洋上的勇敢和冒险故事；Moshi&nbsp;还能用一种低语的讲述神秘故事的语气，表达《黑客帝国》的电影情节。</p><p></p><p></p><p></p><p>Moshi还能一秒化身太空助手，和对话用户一同“进入”太空之旅。并且，Moshi&nbsp;的反应似乎比人类更快，经常在问题或提示被完全提出之前就做出了回答。</p><p></p><p></p><p></p><p></p><p>在发布现场的一系列演示中，Moshi&nbsp;是在没有互联网连接的标准&nbsp;MacBook&nbsp;Pro&nbsp;上运行。Kyutai&nbsp;还计划进一步优化移动设备的&nbsp;Moshi，确保其广泛采用。这将使Moshi更加通用，从个人助理到便携式教育工具，可以在各种环境中使用。</p><p></p><h1>有思想、有情商，半秒内就能回复</h1><p></p><p>据介绍，&nbsp;Moshi不仅仅是一个语音&nbsp;AI，还是一个能够处理文本和音频的多模态模型，主要功能特点包括：</p><p>&nbsp;</p><p>同时听和说：Moshi支持多流音频，使其能够同时收听和响应，从而实现自然流畅的前后对话，其中中断和重叠的语音很常见。与依靠语音活动检测来切换轮次的传统系统不同，Moshi&nbsp;保持连续的对话流。文本思想：在用音频说话时，Moshi&nbsp;会产生文本思想。这种双重方法增强了其产生准确和符合具体情况的响应的能力。通过文本思考，Moshi&nbsp;可以更有效地组织其响应，并从更丰富的知识库中汲取灵感。富有情商：Moshi&nbsp;不仅仅是文字，而是关于理解它们背后的意图。该模型经过训练，可以识别情绪，甚至可以生成传达特定情绪的语音。实时交互：Kyutai&nbsp;声称&nbsp;Moshi&nbsp;的理论延迟仅为&nbsp;160&nbsp;毫秒，而实际上，它在&nbsp;200&nbsp;到&nbsp;240&nbsp;毫秒之间。人人可访问：不仅是开源项目，公司、研究人员都可以集成、试验，而且开发了一种可以在个人计算机上运行的较小版本，使这项技术能够被大型研究实验室以外的更广泛的用户使用。负责任的&nbsp;AI&nbsp;：Kyutai&nbsp;正在整合水印技术帮助识别&nbsp;AI&nbsp;生成的音频，以确保透明度。</p><p>&nbsp;</p><p>其中，Moshi&nbsp;最令人印象深刻的方面之一是它能够在设备上运行。此功能解决了隐私问题，并使&nbsp;AI&nbsp;在实时应用程序中更易于访问和响应。用户可以与Moshi进行交互，而不必担心数据被发送到远程服务器。</p><p></p><h1>70&nbsp;亿参数提供支持，Moshi是如何训练的?</h1><p></p><p>Moshi&nbsp;因其同时处理音频和文本的能力而脱颖而出，而这种实时交互是由&nbsp;Kyutai&nbsp;创新的联合预训练过程提供支持。</p><p>&nbsp;</p><p>据了解，Moshi&nbsp;基于&nbsp;Helium&nbsp;7B&nbsp;模型构建，集成了文本和音频训练，针对&nbsp;CUDA、Metal&nbsp;和&nbsp;CPU&nbsp;后端进行了优化，支持&nbsp;4&nbsp;位和&nbsp;8&nbsp;位量化。在训练方面，Kyutai&nbsp;使用了各种数据源，包括人体运动数据和&nbsp;YouTube&nbsp;视频。</p><p>&nbsp;</p><p>Moshi&nbsp;还集成了基于&nbsp;Kyutai&nbsp;的&nbsp;Mimi&nbsp;模型的高压缩语音编解码器，可以高效处理音频信息。</p><p>&nbsp;</p><p>训练中，Moshi涉及一些创新的开创性技术，使其对自然语言和对话流程有了深刻的理解。</p><p>&nbsp;</p><p>音频语言模型：Moshi&nbsp;的模型不是只在文本上训练，而是在语音数据上训练。语音被压缩成伪词，然后用这些伪词来训练模型以预测下一段音频。这种方法使模型能够理解口语的内容和上下文。合成对话：为了训练Moshi进行对话，Kyutai从纯文本语言模型中生成了合成对话。然后，这些对话通过内部文本转语音引擎进行合成。这种方法确保其学会了处理真实的对话动态。</p><p>&nbsp;</p><p>同时，Kyutai&nbsp;以新颖的方法正面解决了传统的语音&nbsp;AI&nbsp;系统面临的问题，如延迟和处理过程中非文本信息的丢失，创造了一种响应更灵敏、听起来更自然的&nbsp;AI。</p><p>&nbsp;</p><p>集成深度神经网络：Kyutai&nbsp;没有依赖每个任务的单独模型，而是将所有内容合并到一个深度神经网络中。这种集成减少了延迟，并保留了语音通信的丰富性，而语音通信在纯文本处理中通常会丢失。基于语音的训练：Moshi的模型从大量压缩的带注释的语音片段中学习，使其能够理解语音的复杂性，包括特定的声音特征和声学条件。</p><p>&nbsp;</p><p>此外，Kyutai&nbsp;敏锐地意识到高级语音&nbsp;AI&nbsp;可能被滥用于恶意目的，如网络钓鱼。为了降低这些风险，Kyutai&nbsp;实施了识别&nbsp;Moshi&nbsp;生成内容的策略，包括维护生成的音频签名的数据库，并使用水印技术在音频中嵌入听不见的标记。</p><p></p><h1>结语</h1><p></p><p>Moshi代表了语音AI技术的重大飞跃。更广泛地说，Moshi&nbsp;有可能彻底改变数字世界中语音的使用。例如，它的文本到语音功能在情感和多人语音互动方面非常出色。它能够传达情感、调整说话风格和进行自然对话，这将彻底改变我们与人工智能互动的方式，并开启了一个充满可能性的世界：</p><p>&nbsp;</p><p>客服支持：由&nbsp;Moshi&nbsp;提供支持的&nbsp;AI&nbsp;助手可以提供富有同理心和高效的客服支持，提高用户满意度并减少等待时间。语言学习：Moshi&nbsp;模仿母语口音和传达情感的能力可以彻底改变语言学习，使其更加身临其境和有效。医疗保健：Moshi可以作为患者的伴侣，提供支持和信息，同时根据用户的情绪状态调整其语气。娱乐：Moshi可以凭借其多样化的声音和情感将角色带入生活，丰富互动式讲故事体验。</p><p>&nbsp;</p><p>与此同时，Moshi的出现隔空对OpenAI等主要人工智能公司提出了挑战，这些公司因安全问题而推迟发布类似的语音功能产品而受到不少用户的批评。</p><p>&nbsp;</p><p>不过，也有Moshi的使用者表示，其在第一分钟左右的速度和响应速度都非常快，但对话进行的时间越长，就会变得越不连贯；并且，Moshi明显缺乏知识，在犯了错误而受到责备时，就会惊慌失措，陷入“对不起，对不起...”的循环回复。</p><p>&nbsp;</p><p>虽然&nbsp;OpenAI&nbsp;暂时还不需要担心来自&nbsp;Moshi&nbsp;的竞争，但确实表明，许多公司正在迎头赶上OpenAI。就像Sora一样，现在Luma&nbsp;Labs、Runway&nbsp;等其他公司都在推出表现不弱的竞对产品挑战其模型质量和市场地位。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d</a>"<a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">1</a>"<a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">85d85da97d</a>"</p><p><a href="https://analyticsindiamag.com/french-ai-lab-kyutai-releases-openai-gpt-4o-killer-moshi/">https://analyticsindiamag.com/french-ai-lab-kyutai-releases-openai-gpt-4o-killer-moshi/</a>"</p><p><a href="https://www.tomsguide.com/ai/moshi-chats-gpt-4o-advanced-voice-competitor-tried-to-argue-with-me-openai-doesnt-need-to-worry-just-yet">https://www.tomsguide.com/ai/moshi-chats-gpt-4o-advanced-voice-competitor-tried-to-argue-with-me-openai-doesnt-need-to-worry-just-yet</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JYWVsTTgmx3eENoGcpRr</id>
            <title>Andrej Karpathy 提出新构想：未来 2.0 计算机将完全由神经网络驱动</title>
            <link>https://www.infoq.cn/article/JYWVsTTgmx3eENoGcpRr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JYWVsTTgmx3eENoGcpRr</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 02:07:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 神经网络, 软件2.0, Andrej Karpathy
<br>
<br>
总结: Andrej Karpathy提出了一个关于未来计算机的构想：“100％ Fully Software2.0”，计算机未来将完全由神经网络驱动，不依赖传统软件代码。这种架构下，设备的输入将直接传递给神经网络，输出则直接显示为结果，可能是音频/视频，也可能显示为交互界面。未来计算机系统将可以全面处理复杂任务，但也可能面临透明度、算力、安全性和技术依赖等挑战。Karpathy的构想与之前发布的Apple Intelligence有相似之处，展望了未来计算机与人类互动的可能性。 </div>
                        <hr>
                    
                    <p></p><p>7 月 2 日凌晨，知名人工智能专家、OpenAI 的联合创始人 Andrej Karpathy 在社交平台上发帖，提出了一个关于未来计算机的构想：“100％ Fully Software2.0”， 计算机未来将完全由神经网络驱动，不依赖传统软件代码。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5a/5a187896975f09cdf6c4f5e9286b8ed1.png" /></p><p></p><p>这就相当于人类大脑和躯体的关系：大脑负责处理，而躯干（外设）负责执行输出。</p><p></p><p>Karpathy 表示，在这种架构下，设备的输入（如音频、视频、触摸，甚至自然语言）将直接传递给神经网络，输出则直接显示为结果，可能是音频 / 视频，也可能显示为交互界面。整个计算过程完全依赖于神经网络的处理能力。</p><p></p><p>这也就意味着，“100％ Fully Software2.0”计算机将有潜力彻底改变我们与设备进行互动的方式，将架构简化为一个强大的单一神经网络。与当前传统软件与 AI 元素结合的系统相比，未来计算机系统将可以全面处理复杂任务。</p><p></p><p>这一概念的提出引起了网友的广泛关注和讨论。有网友认为，这一构想看上去太宏观且不切实际，甚至无法看到未来。</p><p></p><p>也有网友对 Karpathy 的构想表示担忧：</p><p></p><p>透明度和可解释性：完全依赖神经网络的系统可能难以解释其决策过程，导致“黑匣子”问题，增加了监管和信任的难度。算力和能源消耗：如此大规模的神经网络计算需要极高的算力和能源，可能对资源和环境造成巨大压力。安全性和隐私：神经网络驱动的系统可能容易受到攻击，尤其是如果数据输入未经严格验证，可能导致安全和隐私问题。技术依赖：过度依赖神经网络技术可能限制计算机的灵活性和适应性，尤其在面对非结构化或突发性问题时。</p><p></p><p>Karpathy 的这一构想似乎与之前发布的 Apple Intelligence 有异曲同工之处，如支持文本、音频、视频的读写功能；高度无摩擦、快速、”始终在线“和情景化地全面集成这些功能，根据用户需要调整界面等等。</p><p></p><p>此前，Karpathy 也曾表示了自己对 Apple Intelligence 的期待：”我们正在快速走向这样一个世界：当我们打开手机，直接对着手机说你想做的事情，它就会像人一样进行思考、理解并回复你，就好像它很了解你一样。作为用户，我很期待它。“</p><p></p><p>参考链接：</p><p></p><p><a href="https://x.com/karpathy/status/1807497426816946333">https://x.com/karpathy/status/1807497426816946333</a>"</p><p></p><p><a href="https://x.com/imxiaohu/status/1807772757448618285">https://x.com/imxiaohu/status/1807772757448618285</a>"</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4NzgzMjA4MQ==&amp;mid=2453434106&amp;idx=4&amp;sn=4fbc96fe7c13ad9cbd6642623ea00376&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s/HEI4DkVzdLP7_QBetfTFfg</a>"</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247588955&amp;idx=1&amp;sn=85303ba7c3b86be27b3e08e5c4d95cd8&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s/ZOhvA66bB2r6eD2eQFEjqA</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/sqaUMyNg6B8OrCcwg4vo</id>
            <title>下一代 RAG 技术来了！微软正式开源 GraphRAG：大模型行业将迎来新的升级？</title>
            <link>https://www.infoq.cn/article/sqaUMyNg6B8OrCcwg4vo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/sqaUMyNg6B8OrCcwg4vo</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 01:43:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GraphRAG, LLM, 私有数据集, 知识图谱
<br>
<br>
总结: GraphRAG 是一种基于图的检索增强生成方法，通过结合大型语言模型和图机器学习技术，极大增强了处理私有数据时的性能。它利用知识图谱、社区分层和语义总结，提供了在处理私有数据集时的高效性能。GraphRAG 在私有数据集中的应用展示了显著的改进，为企业私有数据分析带来了全新的可能性。 </div>
                        <hr>
                    
                    <p></p><p></p><p></p><blockquote>这是增强大语言模型能力的一大进步，也是一种彻底改变企业私有数据分析的技术。</blockquote><p></p><p></p><p>7 月 2 日，微软开源了 GraphRAG，一种基于图的检索增强生成 (RAG) 方法，可以对私有或以前未见过的数据集进行问答。在 GitHub 上推出后，该项目快速获得了 2700 颗 star！</p><p></p><p>开源地址：<a href="https://github.com/microsoft/graphrag">https://github.com/microsoft/graphrag</a>"</p><p></p><p>通过 LLM 构建知识图谱结合图机器学习，GraphRAG 极大增强 LLM 在处理私有数据时的性能，同时具备连点成线的跨大型数据集的复杂语义问题推理能力。普通 RAG 技术在私有数据，如企业的专有研究、商业文档表现非常差，而 GraphRAG 则基于前置的知识图谱、社区分层和语义总结以及图机器学习技术可以大幅度提供此类场景的性能。</p><p></p><p>微软在其博客上介绍说，他们在大规模播客以及新闻数据集上进行了测试，在全面性、多样性、赋权性方面，结果显示 GraphRAG 都优于朴素 RAG（70~80% 获胜率）。</p><p></p><p>与我们传统的 RAG 不同，GraphRAG 方法可以归结为：利用大型语言模型 (LLMs) 从您的来源中提取知识图谱；将此图谱聚类成不同粒度级别的相关实体社区；对于 RAG 操作，遍历所有社区以创建“社区答案”，并进行缩减以创建最终答案。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/02/02dd1bfe3ddf43b3933744bb7987c388.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/99/9989ab9874a6aa6cf13a2745c91f2819.jpeg" /></p><p></p><p>这个方法用微软高大上的说法是：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4a/4a74a04c91cf45c2a049cd98d86973b1.png" /></p><p></p><p>微软研究院于 4 月首次宣布推出 GraphRAG ，仅看到论文就让很多人有点等不及上手一试了，如今这项成果终于开源了，开发者们对此表现得超级兴奋：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6f/6f9767882f2dea8cf7ab0c443ae679b2.jpeg" /></p><p></p><p>太棒了，微软开源了 GraphRAG！看完演示视频后，我的脑海里充满了 GraphRAG 带来的各种可能性。我打算在 &nbsp;MacBook 上尝试使用 GraphRAG + Llama3，因为它有 96GB 的统一内存 (VRAM)。我认为这个工具绝对会带来颠覆性的改变。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f7/f7353e8e630efc8c9fdb53106bf26ff1.jpeg" /></p><p></p><p>从看了论文后，我就一直期待着能玩玩它。我曾想过根据论文自己实现它，不过我想官方的代码应该只会晚几周发布，事实证明我的耐心确实得到了回报 :)</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e4/e4d9049474c7be6893ef6394dc9d8a23.jpeg" /></p><p></p><p>我一直在等这一天！知识图谱并不是传统语义搜索的替代品，但它们确实在执行 RAG 操作时解锁了一系列全新能力，例如既可以沿着非常长的上下文向下遍历，又可以以一种连贯、高效的方式跨越不同的上下文进行遍历。</p><p></p><p>但值得一提的是，所有性能改进技术都有一个缺陷：token 的使用和推理时间都会增加…</p><p></p><p></p><h2>解锁 LLM 在私有数据集中的探索能力</h2><p></p><p></p><p>大语言模型最大的挑战和机遇或许在于如何将其强大的能力，应用到训练数据以外的问题解决中，利用大语言模型没有见过的数据取得可对比的结果。这将为数据调查开拓新的可能性，例如根据数据集的上下文和 ground 确定其主题和语义概念。</p><p></p><p>下面我们将具体介绍下微软研究院创建的 GraphRAG，这是增强大语言模型能力的一大进步。</p><p></p><p>检索增强生成（RAG）是一种根据用户的查询语句搜索信息，并以搜索结果为 AI 参考从而生成回答。这项技术是多数基于 LLM 工具的重要组成部分，而多数的 RAG 都采用向量相似性作为搜索的技术。在文档中复杂信息的分析时，GraphRAG 利用 LLM 生成的知识图谱大幅提升了问答的性能，这一点是建立在近期关于私有数据集中执行发现时提示词增强能力的研究之上。微软将私有数据集定义为未被 LLM 训练使用，且 LLM 从未见过的数据，例如某企业的专有研究、商业文件或通讯。</p><p></p><p>基线 RAG（Baseline RAG）因此而生，但基准 RAG 在某些情况下表现非常差，例如：基线 RAG 很难连点成线。这种情况出现在问题的回答需要通过共用属性遍历不同信息片段以提供新的综合见解时。基线 RAG 在需要全面地理解大型数据集或单一大型文档的语义概念时，表现会很差。</p><p></p><p>为解决这一问题，业界正在努力开发扩展和增强 RAG 的方法（如 LlamaIndex）。微软研究院的新方法 GraphRAG 便是基于私有数据集创建知识图谱，并将图谱与机器学习一同用于在查询时执行提示词的增强。在回答上述两类问题情况时，GraphRAG 展示了显著的改进，其智能或者说精通的程度远超先前应用私有数据集的其他方法。</p><p></p><p></p><h3>应用 RAG 于私有数据集</h3><p></p><p></p><p>为证明 GraphRAG 的有效性，GraphRAG 先以新闻文章中暴力事件信息（VIINA）数据集为例，该数据集复杂且存在相左的意见和不完整的信息，是一个现实世界中杂乱的测试示例，又因其出现时间过于近期，所以并未被纳入 LLM 基础模型的训练中。</p><p></p><p>在这项研究中，微软采用了俄罗斯和乌克兰双方新闻来源在 2023 年 6 月中的上千篇新闻报道，将其翻译为英文后建成了这份将被用于基于 LLM 检索的私有数据集。由于数据集过大无法放入 LLM 上下文的窗口，因此需采用 RAG 方法。</p><p></p><p>微软团队首先向基线 RAG 系统和 GraphRAG 提出一个探索查询：</p><p></p><p>查询语句：“Novorossiya 是什么？”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7c387ec1c789f7780c007555fb1141a0.jpeg" /></p><p></p><p>通过结果可以看出，两个系统表现都很好，这是基线 RAG 表现出色的一类查询。然后他们换成了一段需要连点成线的查询：</p><p></p><p>查询语句：“Novorossiya 做了什么？”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/32/32de8c7107040534e6818b249114d457.jpeg" /></p><p></p><p>基线 RAG 没能回答这一问题，根据图一中插入上下文窗口的源文件来看，没有任何文本片段提及“Novorossiya”，从而导致了这一失败。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/72/7219462a6412d4c1878838c00e174be9.png" /></p><p></p><p>图一：基线 RAG 检索到的上下文</p><p></p><p>相较之下，GraphRAG 方法发现了查询语句中的实体“Novorossiya”，让 LLM 能以此为基础建立图谱，连接原始支持文本从而生成包含出处的优质答案。举例来说，图二中展示了 LLM 在生成语句时所截取的内容，“Novorossiya 与摧毁自动取款机的计划有所关联。”可以从原始文本的片段（翻译为英文后）中看出，LLM 是通过图谱中两个实体之间的关系，断言 Novorossiya 将某一银行作为目标的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6b/6b8e8c261eb0de60428cedb351961315.png" /></p><p></p><p>图二：GraphRAG 出处</p><p></p><p>通过 LLM 生成的知识图谱，GraphRAG 大幅改善了 RAG 的“检索”能力；在上下文窗口中填充相关性更高的内容、捕捉出处论据从而提供更为优质的答案。</p><p></p><p>信任和验证 LLM 所生成的结果始终是重要的。微软希望结果总是事实性正确、连贯一致，并且能准确地反映原始材料中的内容。GraphRAG 每次生成回答时总会提供出处或源基础信息，表明它的回答时以数据集为基础的。每个论断的引用来源都一目了然，人类用户能够直接对照原始材料，快速且准确地审核 LLM 的输出结果。</p><p></p><p>不过这还不是 GraphRAG 可以实现的全部功能。</p><p></p><p></p><h3>完整数据集推理</h3><p></p><p></p><p>基线 RAG 不擅长处理需要汇总全部数据集信息才能得出答案的查询。类似“数据中排行前五的主题是什么？”的查询表现不佳，是因为基线 RAG 依赖对数据集中语义相似文本内容的矢量搜索，而查询语句中却没有任何能引导它找到正确信息的关键词。</p><p></p><p>但 GraphRAG 却可以回答这类问题。LLM 生成的知识图谱结构给出了数据集的整体结构和其中主题，让私有数据集也能被组织成有意义的语义集群并对其进行预总结。在回应用户查询时，LLM 会使用这些聚类对主题进行总结。</p><p></p><p>通过下面这条语句，可以展示出两套系统对数据集整体的推理能力：</p><p></p><p>查询语句：“数据中排行前五的主题有哪些？”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7ec28b8bc55e62cc78cbe554a5ee89f3.jpeg" /></p><p></p><p>从基线 RAG 的结果来看，列出的主题中没有一个提及两者之间的纷争。正如预期，矢量搜索检索到了无关的文本，并将其插入 LLM 的上下文窗口中。生成的结果很可能是根据关键词“主题”进行搜索，导致了其对数据集内容的评估不够有用。</p><p></p><p>再看 GraphRAG 的结果，可以清楚看到其生成的结果与数据集整体内容更为吻合。回答中提供了五大主题及其在数据集中观察刀的辅助细节。其中参考的报告是由 LLM 为 GraphRAG 根据每个语义集合预先生成，提供了对原始材料出处的对照。</p><p></p><p></p><h3>创建 LLM 生成的知识图谱</h3><p></p><p></p><p>支持 GraphRAG 的基本流程是建立在先前对图机器学习的研究和代码库上的：LLM 处理全部私有数据集，为源数据中所有实体和关系创建引用，并将其用于创建 LLM 生成的知识图谱。利用生成的图谱创建自下而上的聚类，将数据分层并组织成语义聚类（在图三中由颜色标识）。这种划分让预先总结语义概念和主题成为可能，从而更全面地理解数据集。在查询时，两种结构均被用于填充 LLM 回答问题时的上下文窗口。</p><p></p><p>图三为图谱可视化的示例，每个圆圈都代表一个实体（如人物、地点或组织），圆圈大小代表该实体拥有的关系数量，颜色代表相似实体的分组。颜色分区时建立在图结构基础上的一种从下至上的聚类方法，让 GraphRAG 能回答不同抽象程度的问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/52/52ab5b5d8da0892b696ca7df0ed723dd.png" /></p><p></p><p>图三：利用 GPT-4 Turbo 和私有数据集创建 LLM 生成的知识图谱</p><p></p><p></p><h3>结果指标</h3><p></p><p></p><p>上述示例中表现了 GraphRAG 在多个跨领域数据集上的持续改进。微软采用 LLM 的一个评分器给 GraphRAG 和基线 RAG 的表现进行评估和对比，设定了一系列定性指标，其中包括全面性（问题指向背景框架内的完整性）、人性化（提供辅助原始材料或其他背景信息），以及多样性（提供问题回答的不同角度或观点）。初步结果显示，GraphRAG 在这些指标上始终优于基线 RAG。</p><p></p><p>除了对比评估，他们还采用 SelfCheckGPT 对 GraphGPT 进行了忠实性的测试，以验证其基于原始材料的真实且连贯的生成结果。结果显示，GraphRAG 达到了与基线 RAG 相似的忠实度水平。</p><p></p><p>通过将 LLM 生成的知识图谱与图机器学习相结合，GraphRAG 能回答重要的问题类别，而这些问题是无法单独使用基线 RAG 完成的。在将这项技术应用于社交媒体、新闻文章、工作中生产力及化学等场景后，微软已经观察到了可喜的成果，未来他们将继续在各类新领域中应用这项技术。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.youtube.com/watch?v=r09tJfON6kE">https://www.youtube.com/watch?v=r09tJfON6kE</a>"</p><p></p><p><a href="https://news.ycombinator.com/item?id=40857174https://arxiv.org/html/2404.16130v1">https://news.ycombinator.com/item?id=40857174https://arxiv.org/html/2404.16130v1</a>"</p><p></p><p><a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/</a>"</p><p></p><p><a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/</a>"</p><p></p><p></p><h4>内容推荐</h4><p></p><p></p><p>大模型正在推动历史性技术革命，知识触手可及。2024年6月14日至15日，ArchSummit全球架构师峰会在深圳成功举办，我们精选了峰会中聚焦AI大模型技术应用的相关PPT，内容涵盖了华为云AI原生应用引擎的架构与实践、微众银行大模型研发实践以及B站容量管理实践等。关注「AI前线」，回复关键词「大模型落地」免费获取PPT资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e6/e605fc1fa9bfeb55ac557b48a5112ec7.jpeg" /></p><p></p><p></p><h4>活动推荐</h4><p></p><p></p><p>AICon 全球人工智能开发与应用大会，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。聚焦大模型训练与推理、AI Agent、RAG 技术、多模态等前沿议题，汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。</p><p></p><p>在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/6573657a90550f91dc3658ad05122b02.other" /></p><p></p><p></p><h4>今日荐文</h4><p></p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247617948&amp;idx=1&amp;sn=6364cca2548c5f729de788ae46a78236&amp;chksm=fbebb453cc9c3d45ed06ba79283cd88d890a27206b73a326b7742d3794d1303ea426e1e8641d&amp;scene=21#wechat_redirect">微软130亿美元换的OpenAI 董事席，苹果仅靠“刷脸”就拿下了！硅谷明星创企积极投靠大厂</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247617758&amp;idx=1&amp;sn=bc9a2ff3c338a307e49aee3e5eab9f8e&amp;chksm=fbebb511cc9c3c071fdbd23fa84c025114124e67a414c57671ffc6783a9aee89fd5874fbd930&amp;scene=21#wechat_redirect">挖矿不行了找AI接盘！挖矿公司们来抢云厂商生意：收入涨10倍，今年的算力早就卖完了！</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247617692&amp;idx=2&amp;sn=7ff2a4807c91cbe15bc971c9f465e550&amp;chksm=fbebb553cc9c3c455cf193b5dead0dcf5005bdbf1f16382a6b7befede90451638ef1521372bc&amp;scene=21#wechat_redirect">价格战、大厂裁员、模型“翻车”……Q2 的AI 圈子可一点都不无聊</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247617551&amp;idx=1&amp;sn=039d64f6b0fa6b34ae6076a19679a9ad&amp;chksm=fbebb5c0cc9c3cd6257d51cf31adeb0d07da75318ae496e2ff01e38ad0b20e9b560584caa3d2&amp;scene=21#wechat_redirect">好消息：OpenAI 突然发了新模型！坏消息：只是纠错，没你想得逆天</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247617404&amp;idx=1&amp;sn=5d4af3a33af130355cbd39c689fbdd5d&amp;chksm=fbebb2b3cc9c3ba53a9880719f253ce169a8688e97e6af4c221dff6727c4516e5a4a1a6b7381&amp;scene=21#wechat_redirect">从AI高管到犀利CEO，贾扬清创业这一年：我们的目标是做AI时代的“第一朵云”</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247617318&amp;idx=1&amp;sn=02ee018927f79f0bad2684ddea2da2f0&amp;chksm=fbebb2e9cc9c3bff399016bb23f59975b6d275ae4aa9eb3f0ccb55569f269ad6ba35ea81e7e4&amp;scene=21#wechat_redirect">没想到国内大模型厂商又一次high起来，是因为OpenAI 断供！</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7BMREy4NUmvxEYJ5jhY2</id>
            <title>周伯文：通专融合是通往AGI的战略路径</title>
            <link>https://www.infoq.cn/article/7BMREy4NUmvxEYJ5jhY2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7BMREy4NUmvxEYJ5jhY2</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:49:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 通用人工智能, ABI, AGI
<br>
<br>
总结: 2024年世界人工智能大会在上海举行，周伯文在会上分享了关于通用人工智能的主题。他提到通用人工智能是新的生产力引擎，是生产力的生产力。他还探讨了通向AGI的必经之路，即广义人工智能。他认为实现AGI的路径是二维的，需要结合泛化能力和专业性。周伯文介绍了通专融合的新范式，强调构建具有泛化性和专业能力的AI系统的重要性。 </div>
                        <hr>
                    
                    <p>7月4日，2024世界人工智能大会暨人工智能全球治理高级别会议（WAIC 2024）在上海开幕。上海人工智能实验室主任、首席科学家，清华大学惠妍讲席教授，衔远科技创始人周伯文在WAIC 2024科学前沿主论坛上发表开场报告。以下为报告全文：</p><p>&nbsp;</p><p>尊敬的各位领导、各位来宾，大家下午好。我是上海人工智能实验室周伯文，非常有幸在这个隆重的场合下代表实验室与大家进行主旨分享。我的报告主题是《通专融合：通用人工智能前沿探索与价值实现》。自21世纪初以来，我们进入了以人工智能的兴起为代表，并逐步走向通用人工智能的第四次工业革命，因此又称为智能化时代。这一时代的特点是知识发现加速，人类能力的边界得以拓展，产业的数字化和智能化持续升级，从而带来生产范式的变革。通用人工智能对于人、工具、资源、技术等生产力要素具有广泛赋能的特性，可以显著提升其他生产力，因此我们说它是新质生产力的重要引擎，是“生产力的生产力”。</p><p></p><h2>AGI路径的思考</h2><p></p><p>我本人深入思考通用人工智能始于2015、2016年。2016年AlphaGo击败了人类的世界冠军，大家开始讨论通用人工智能什么时候会到来。坦率讲当时大家对AGI是缺乏认识的，但我在思考什么样的研究可以导致AGI。我们需要回答很多问题，例如，什么时候AGI会来，AGI会怎么来，我们要如何防御，如何让AGI变得更好等。那时候大家都知道了AGI是什么，但不知道怎么做。对应AGI我创造了两个词：ANI狭义人工智能和ABI广义人工智能。右边就是我当时的PPT原版。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a6625652f0fdfa2eb73d1bc157ce2ea.png" /></p><p></p><p>通向AGI的必经之路是ABI，即广义人工智能。从学术上我给出了严格的定义：自监督、端对端、从判别式走向生成式。</p><p></p><p>回头来看，2022年ChatGPT出现的时候基本上实现了这三个要素，也就说2022年底开始我们已经进入了ABI的时代。但2016年未能预测出大模型的一些要素，例如模型的涌现能力。站在2024年的节点上，如果要做同样的思考讨论，那么接下来，AGI应该是一种怎样的达成路径，这是我们所有研究者和从业者都必须思考的问题。</p><p></p><p>这里提供一个我们的思考视角：实现AGI的路径应该是二维的，而非一维的。回看发展历史，在2016、2017年以前，人工智能在专业能力上拥有非常迅猛的进展。从“深蓝”到“AlphaGo”，人工智能因一次次击败“地表最强人类”而成为新闻的主体。但当时的巨大挑战在于，这些模型不具备泛化能力，只能在专有的任务上表现突出。在2017年Transformer提出以后，我们看到的是大模型在泛化能力上的“狂飙”。但大模型当前的另一个挑战是，在专业能力的进展上极其缓慢。同时带来的能源消耗、数据消耗、资源消耗均在让人思考，这条路径是通向AGI的有效路径吗？</p><p></p><p>Sam Altman曾提到，GPT-4的专业能力，大概相当于10%-15%的专业人士，即使到未来的GPT-5，预期将会提高四到五个点，也就是说将用指数级的能源消耗增长换来缓慢的专业能力提升。</p><p>在这里我们想提出一个判断：人工智能AGI落地会有一个高价值区域，同时要求模型兼备很强的泛化能力和足够的专业性。这个区域离原点最近的位置，我们把它叫做通专融合的“价值引爆点”。根据对历史生产力提升的分析，我们认为处在这个点的大模型，在专业能力上应超过90%的专业人类，同时具备强泛化能力，即ABI的能力。谁先进入高价值区域，即意味着谁的能力更强，拥有更多的场景和数据飞轮，并因此更早拥有自我进化迭代的能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e6ac6131fe8c228adfd794762b748db.png" /></p><p></p><p></p><h2>强泛化之上的专业能力是AI皇冠上的明珠：通专融合新范式</h2><p></p><p>强泛化之上的专业能力是AI皇冠上的明珠，通专融合的发展新范式。瞄准构建一个既具有泛化性又具备专业能力的AI系统，这样的系统能够更高效、更好地适应和解决现实世界中的复杂问题。实现这一目标需要一个完整的技术体系，它包含三层重点工作：</p><p></p><p>基础模型层：我们专注于更高效地构建通用泛化能力，尤其是其高阶能力，如数理、因果推理等。通过高质量数据的清洗和合成，研发高性能训练框架、高效的模型架构。一部分这样的原始创新体现在我们的书生·浦语大语言模型、书生·万象多模态模型等基础模型，并在数学和推理等高阶能力上实现了突破。但我们还有很多工作要做。融合协同层：这一层负责将泛化性和专业性有效地结合起来。我们采用多路线协同的算法和技术，构建比肩人类优秀水平的专业能力。我们的原创工作包括高密度监督信号的生成、复杂任务规划，以及新的架构来实现系统1（即快速、直觉反应的系统）和系统2（慢速、逻辑分析的系统）之间的交互。通过这些技术，AI能够在复杂环境中做出决策，将复杂任务分解为更易管理的子任务，制定行动计划，并有效地协调多个智能体，以实现群体智能的涌现。自主进化与交互层：在这一层，我们强调AI的自主探索和反馈闭环的重要性。AI系统需要能够在真实或仿真世界中自主地收集数据、学习并适应环境。通过与环境的交互，AI能够获得反馈，这些反馈对于其自我进化至关重要。自主进化与交互层使AI能够进行具身自主学习，最终对世界模型有更深刻的理解并与之交互，完成开放世界任务。</p><p>&nbsp;</p><p>接下来，我分别介绍在这个框架下的几项前沿进展。</p><p>&nbsp;</p><p></p><h2>更高效地构建通用基础模型</h2><p></p><p>为更高效地构建通用基础模型，实验室在并行训练及软硬适配协同、高效数据处理、新型架构及推理增强等方面进行了一系列原创的探索。</p><p></p><p>例如，在长序列并行训练方面，我们实现了性能突破，较国际知名的框架Megatron高达4倍。我们研发的大模型训练系统，基于真实训练需求不断沉淀技术能力，已连续两年获得计算机系统顶会ASPLOS杰出论文奖及最佳论文奖。</p><p></p><p>在基础模型方面，通过稀缺数据的合成与增广，实验室最新的大语言模型书生·浦语2.5，实现了综合性能比肩开源大模型参数的性能。</p><p></p><p>多模态大模型书生·万象，通过渐进式对齐、向量链接等创新技术，构建以更少算力资源训练高性能大模型的道路。以260亿参数，达到了在关键评测中比肩GPT-4的水平。</p><p></p><h2>模型通用泛化能力与专业能力融合</h2><p></p><p>围绕构造通用模型的高阶专业能力，我介绍两项代表性成果。</p><p></p><p>首先，是关于大模型专业推理能力。最近大家可能看到过这个新闻：“AI参加高考，数学全不及格”。这些AI考生里面，也包含了我们的书生·浦语，它在其中拿到了数学的最高分75分。这要得益于我们的开源数学模型，它沉淀了密集过程监督、模型辅助的思维链校验、多轮强化自训练、文本推理和代码解释器联合迭代等一系列技术，具备了良好的自然语言推理、代码解题及形式化数学语言性能，所以能以200亿参数在高考数学上超过GPT-4o，我们不但效果最好，而且参数体量最小、能源消耗最低。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e020a57bf3c0a56a44b076899896f03.png" /></p><p></p><p>第二项是关于新的系统架构，我们原创提出模拟人脑的系统1与系统2架构来实现通专融合。大家知道系统1是人脑的快决策，反映的是长期训练下的专业能力；系统2是慢系统，体现的是深度思考下的泛化能力。我们今年的这篇CVPR论文通过设计系统1与系统2的协同模式，提出了交互式持续学习新概念，让通用模型与专业模型能互相学习，通过通专融合来更高效、更专业地解决问题。同一个架构在图像识别、专业文本生成方面都获得了很好的效果。</p><p></p><h2>具身自主探索与世界模型</h2><p></p><p>具身自主探索是实现通专融合的有效手段，也是理解物理世界的AGI的必经之路。但具身智能绝不仅仅是大模型加机器人的应用，而是物理世界的反馈需要及时进化大模型。我们光靠看书或看视频，永远学不会游泳，你得亲身扎到水里才能学会。大模型得通过机器人，扎进现实世界，才能真正理解物理世界。</p><p>为帮助建立世界模型，我们构建了“软硬虚实”一体的机器人训练场——“浦源·桃源”，同时攻关具身智能的“大脑”与“小脑”。“浦源·桃源”是首个城市级的具身智能数字训练场，构建了集场景数据、工具链、具身模型评测三位一体的开源具身智能研究平台。作为大模型与机器人的连接层，涵盖89种功能性场景、10万+高质量可交互数据，有望解决领域内数据匮乏、评测困难的问题。&nbsp; &nbsp;</p><p></p><p>在大脑方面，我们通过具身智能体自身状态认知、复杂任务分解分配、底层技能协同控制三方面创新，首次实现了大模型驱动的无人机、机械臂、机器狗三种异构智能体协同。在小脑方面，我们通过GPU高性能并行仿真和强化学习，可以高效实现机器人在真实世界里快速学习，并完成高难度动作。我们发现，单卡1小时的训练就能实现真实世界380天的训练效果。</p><p></p><p>无人驾驶可以理解为一个具身智能体。我们提出了开源且通用的自动驾驶视频预测模型GenAD，类似于自动驾驶领域的“SORA”，能够根据一张照片输入，生成后续较高质量、连续、多样化、符合物理世界规律的未来世界预测，并可泛化到任意场景，被多种驾驶行为操控。</p><p></p><h2>通专融合实践：科学发现</h2><p></p><p>对于科学发现领域，通专融合无疑也有着巨大的潜在价值。</p><p></p><p>2023年初，Nature曾发表过一篇封面文章，展示了对科研论文发展现状的悲观态度，指出“科学进步正在‘降速’”。文章认为，近年来科研论文数量激增，但没有颠覆性创新。因为科学本身的发展规律便是不断深入，每个学科形成了信息茧房，不同学科之间壁垒增加。对于顶尖科学家来说，即使穷尽一生也没有办法掌握一个学科所有的知识。这就启发我们需要新的科研组织方式来适配学科信息茧房，这也需要科研工作者与时俱进，采用AI工具赋能科研、加速创新。</p><p></p><p>由于大模型内部压缩着世界知识，同时具备不确定性生成的特性，因此有可能帮助我们打破不同学科领域知识茧房，进行创新式探索。我们认为大模型的不确定性和幻觉生成，并不总是它的缺陷，而是它的一个特点。合理利用这种特点，通过人机协同有助于促进科研创新。</p><p></p><p>事实上，就人类科学家而言，通过“做梦”找到研究思路的例子也不胜其数，最典型的就是，德国有机化学家奥古斯特·凯库勒梦见衔尾蛇，进而发现苯环结构。</p><p></p><p>我们探讨了大模型在生物医学领域的知识发现问题，针对最新的医学文献构建知识发现测试集，并对于最先进的大模型进行评测。我们发现大模型能够提出新的生物医学知识假设，并在最新的文献中得以验证。</p><p>这里给出一个我们发现新假设过程的简单示例：我们将已有的背景知识输入到2023年1月发布的大模型，并让大模型生成可能的假设。大模型提出的假设中，第一条假设是背景已知信息，还不是新的知识；但是第二条假设是之前文献中所没有的。两个月后，这条假设在2023年3月发表的论文中得到了验证。</p><p></p><p>这只是一个非常简单的例子，但已经显示出大模型具有很大的潜力，可以促进科研知识发现，并且能够提出新的有价值的未知假设。</p><p></p><p>通过通专融合，AI不只可以提出科学假设，还可以掌握科学知识、分析实验结果、预测科学现象。进而在反思的基础上，提升AI提出科学假设的能力。</p><p></p><p>在掌握科学知识方面，我们基于大语言基座模型能力进行专项能力强化，分别在化学和育种两个方向构建了首个开源大模型——书生·化学和书生·丰登；在分析实验结果方面，我们研发的晶体结构解析算法AI4XRD具备专家级的准确率，并将解析时间从小时级降低到秒级；在预测科学现象方面，我们训练并持续迭代了风乌气象大模型，在全球中期气象预报上具有当前世界领先的时间和空间分辨率；在提出科学假设方面，我们提出“人在环路大模型多智能体与工具协同”概念框架，对于科学假设的链路进行升级。构建了AI分析师、AI工程师、AI科学家和AI批判家多种角色，接入工具调用能力来协同提出新的假设。</p><p></p><h2>下一代AI for Science</h2><p></p><p>为什么提出一个好问题在科研中如此重要？早在1900年，德国数学家大卫·希尔伯特（David Hilbert）提出了著名的“23个问题”，引领了数学很多子领域数百年的发展。在科学上，提出一个好问题往往比解决问题更重要。希尔伯特还有一句名言，这也是他的墓志铭：“We must know. We will know.”我们必须知道。我们终将知道。今天，我们踏上通专融合的路线，探索通用人工智能AGI的未来，展望下一代的AI for Science，更可以从这句话中汲取灵感和激励。对于可信AGI的未来，正如我今天上午在全体大会的演讲，我们的态度是坚定而积极的：We must be there. We will be there！我们必须达成，我们终将抵达。</p><p></p><p>我今天站在这里也非常感慨，想起了去年汤晓鸥老师在WAIC大会上提到我们原创的成果、我们年轻的科学家，提到了我们的书生大模型。正是我们实验室一群有创造力的年轻科学家，让我们坚信：We must be there and we will be there！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k6MYPKCAmN6dfD9l47oj</id>
            <title>2024 世界人工智能大会（WAIC）开幕，图灵得主的巅峰举首共商AI如何普惠全人类｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/k6MYPKCAmN6dfD9l47oj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k6MYPKCAmN6dfD9l47oj</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:42:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 全球治理, 多元交融, 图灵奖得主
<br>
<br>
总结: 2024年7月4日，2024世界人工智能大会暨人工智能全球治理高级别会议在上海举行，吸引了来自联合国、各国政府、专业国际组织、知名专家、企业家和投资家等1000余人参加。会议围绕人工智能的发展、安全和治理展开深入研讨，强调了人工智能对经济社会发展和人类文明进步的重要影响。图灵奖得主们也在会上展开了关于人工智能治理的讨论，为全球人工智能发展和治理提供了宝贵观点和启示。会议还回顾了世界人工智能大会的发展历程，展示了上海在人工智能领域的重要助力和影响力。 </div>
                        <hr>
                    
                    <p>2024年7月4日，2024世界人工智能大会暨人工智能全球治理高级别会议-全体会议在上海世博中心举办。联合国以及各国政府代表、专业国际组织代表，全球知名专家、企业家、投资家1000余人参加了本次会议，围绕“以共商促共享，以善治促善智”的大会主题展开深入交流研讨。</p><p></p><h2>多元交融的全球议题</h2><p></p><p>人工智能是人类发展新领域，其快速发展对经济社会发展和人类文明进步产生了深远影响，也带来了未知风险和复杂挑战。本届大会全体会议直面人工智能治理这一全球性议程，聚焦发展、安全、治理，开展了一系列国际性、跨领域、多视角的深入研讨。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a095ffc62437332454449e5671b7488.png" /></p><p></p><p>清华大学苏世民书院院长、清华大学人工智能国际治理研究院院长薛澜，上海人工智能实验室主任、首席科学家、清华大学惠妍讲席教授周伯文，新思科技总裁兼首席执行官盖思新分别基于公共政策、科学、产业等不同视角，分享了他们关于人工智能领域技术创新和安全治理的最新成果和最新思考。黑石集团董事长、首席执行官兼联合创始人苏世民，索奈顾问及投资公司董事长、首席执行官乔舒亚·雷默，立足于商业投资视角，以及在人工智能国际治理中的长期实践，共同演绎了关于人工智能浪潮影响下的全球商业变革和治理创新的独到见解。通过来自演讲嘉宾不同角度的系统诠释，表明了在人工智能领域坚持发展和安全并重的必要性，以及加强人工智能国际对话与合作的迫切性。演讲嘉宾还共同表达了要推动人工智能健康发展，赋能经济增长、增进各国人民福祉的一致共识，为全球人工智能发展和治理提供了宝贵观点和启示。</p><p></p><h2>图灵得主的巅峰举首</h2><p></p><p>图灵奖是全球计算机领域的最高荣誉。本次全体会议现场，姚期智、罗杰·瑞迪、曼纽尔·布卢姆等三位享誉全球的图灵奖得主，与原微软执行副总裁、美国国家工程院外籍院士沈向洋，一同联袂进行了一场围绕治理协同创新的巅峰论道。通过极具思辨性的对话，深入探讨了人工智能的“双刃剑”属性、人工智能的可解释性和可预测性、人工智能的严谨底色和变革气质等人工智能领域全球瞩目的核心命题。针对加强人工智能全球治理的会议动议，三位图灵奖得主表现出一致的高度认同，并同时指出人才培养对于应对人工智能未来风险的重要价值。这些来自人工智能标志性人物的深刻见解，将对全球人工智能发展和治理产生深远影响，也将在世界人工智能发展史中，留下属于本次会议的闪亮印记。</p><p></p><h2>世界人工智能大会发展历程</h2><p></p><p>自2018年首次在上海举办，世界人工智能大会已成为上海打造人工智能这一城市新名片的重要助力。基于大会平台和上海支点，越来越多嘉宾选择更加紧密地与上海同行。又一次登台的图灵奖获得者、中国科学院院士姚期智，2020年在上海成立了以自己名字命名的上海期智研究院，专攻人工智能、量子智能方向的基础研究。清华大学惠妍讲席教授周伯文，不久前获得了上海人工智能实验室的邀请担任主任、首席科学家。全体会议共同上演“图灵圆桌”的沈向洋、罗杰·瑞迪、曼纽尔·布卢姆，此前都曾与大会多次携手。某种意义上说，本次他们之间进行的“图灵圆桌”也是关于大会的一次“老友记”。</p><p></p><p>以他们为缩影，追溯更多全球顶尖科学家和先锋企业家们的选择。不难发现承担“科技风向标、应用展示台、产业加速器、治理议事厅”作用的世界人工智能大会，正在为上海加快打造人工智能世界级高端产业集群，源源不断注入新活力和新动能，也将为上海以深入落实人工智能“上海方案”，率先履行《人工智能全球治理上海宣言》，服务构建“以善治促善智”的中国城市样本提供的有益启发和重要助力。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SS6434mmPjzwPmqSu2yb</id>
            <title>万卡万P万亿参数通用算力！摩尔线程夸娥智算中心再升级｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/SS6434mmPjzwPmqSu2yb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SS6434mmPjzwPmqSu2yb</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:32:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 摩尔线程, AI旗舰产品, 夸娥智算集群, 万卡规模
<br>
<br>
总结: 摩尔线程宣布其AI旗舰产品夸娥智算集群实现重大升级，从千卡级别扩展至万卡规模，旨在打造具备万P级浮点运算能力的国产通用加速计算平台，专为万亿参数级别的复杂大模型训练而设计。 </div>
                        <hr>
                    
                    <p>7月3日，摩尔线程重磅宣布其AI旗舰产品夸娥（KUAE）智算集群解决方案实现重大升级，从当前的千卡级别大幅扩展至万卡规模。摩尔线程夸娥（KUAE）万卡智算集群，以全功能GPU为底座，旨在打造能够承载万卡规模、具备万P级浮点运算能力的国产通用加速计算平台，专为万亿参数级别的复杂大模型训练而设计。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4b/4b3ece8869d98d45ad95d80ff452f5c1.png" /></p><p>&nbsp;</p><p>摩尔线程创始人兼CEO张建中表示：“当前，我们正处在生成式人工智能的黄金时代，技术交织催动智能涌现，GPU成为加速新技术浪潮来临的创新引擎。摩尔线程矢志投身于这一历史性的创造进程，致力于向全球提供加速计算的基础设施和一站式解决方案，为融合人工智能和数字孪生的数智世界打造先进的加速计算平台。夸娥万卡智算集群作为摩尔线程全栈AI战略的一块重要拼图，可为各行各业数智化转型提供澎湃算力，不仅有力彰显了摩尔线程在技术创新和工程实践上的实力，更将成为推动AI产业发展的新起点。”&nbsp;</p><p></p><h2>AI主战场，万卡通用算力是标配</h2><p></p><p>大模型自问世以来，关于其未来的走向和发展趋势亟待时间验证，但从当前来看，几种演进趋势值得关注，使得其对算力的核心需求也愈发明晰。</p><p>&nbsp;</p><p>首先，Scaling Law将持续奏效。Scaling Law自2020年提出以来，已揭示了大模型发展背后的“暴力美学”，即通过算力、算法、数据的深度融合与经验积累，实现模型性能的飞跃，这也成为业界公认的将持续影响未来大模型的发展趋势。Scaling Law将持续奏效，需要单点规模够大并且通用的算力才能快速跟上技术演进。</p><p>&nbsp;</p><p>其次，Transformer架构不能实现大一统，和其他架构会持续演进并共存，形成多元化的技术生态。生成式AI的进化并非仅依赖于规模的简单膨胀，技术架构的革新同样至关重要。Transformer架构虽然是当前主流，但新兴架构如Mamba、RWKV和RetNet等不断刷新计算效率，加快创新速度。随着技术迭代与演进，Transformer架构并不能实现大一统，从稠密到稀疏模型，再到多模态模型的融合，技术的进步都展现了对更高性能计算资源的渴望。</p><p>&nbsp;</p><p>与此同时，AI、3D和HPC跨技术与跨领域融合不断加速，推动着空间智能、物理AI和AI 4Science、世界模型等领域的边界拓展，使得大模型的训练和应用环境更加复杂多元，市场对于能够支持AI+3D、AI+物理仿真、AI+科学计算等多元计算融合发展的通用加速计算平台的需求日益迫切。</p><p>&nbsp;</p><p>多元趋势下，AI模型训练的主战场，万卡已是标配。随着计算量不断攀升，大模型训练亟需超级工厂，即一个“大且通用”的加速计算平台，以缩短训练时间，实现模型能力的快速迭代。当前，国际科技巨头都在通过积极部署千卡乃至超万卡规模的计算集群，以确保大模型产品的竞争力。随着模型参数量从千亿迈向万亿，模型能力更加泛化，大模型对底层算力的诉求进一步升级，万卡甚至超万卡集群成为这一轮大模型竞赛的入场券。</p><p>&nbsp;</p><p>然而，构建万卡集群并非一万张GPU卡的简单堆叠，而是一项高度复杂的超级系统工程。它涉及到超大规模的组网互联、高效率的集群计算、长期稳定性和高可用性等诸多技术难题。这是难而正确的事情，摩尔线程希望能够建设一个规模超万卡、场景够通用、生态兼容好的加速计算平台，并优先解决大模型训练的难题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6ff2589b83ac7618ef043715022df6bb.png" /></p><p></p><h2>夸娥：国产万卡万P万亿大模型训练平台</h2><p></p><p></p><p>夸娥（KUAE）是摩尔线程智算中心全栈解决方案，是以全功能GPU为底座，软硬一体化、完整的系统级算力解决方案，包括以夸娥计算集群为核心的基础设施、夸娥集群管理平台（KUAE Platform）以及夸娥大模型服务平台（KUAE ModelStudio），旨在以一体化交付的方式解决大规模GPU算力的建设和运营管理问题。</p><p>&nbsp;</p><p>基于对AI算力需求的深刻洞察和前瞻性布局，摩尔线程夸娥智算集群可实现从千卡至万卡集群的无缝扩展，旨在满足大模型时代对于算力“规模够大+计算通用+生态兼容”的核心需求。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/c9/c998de1e872b93c39a4f27fbf6c583dd.png" /></p><p></p><p>夸娥万卡智算解决方案具备多个核心特性：</p><p>超大算力，万卡万P：在集群计算性能方面，全新一代夸娥智算集群实现单集群规模超万卡，浮点运算能力达到10Exa-Flops，大幅提升单集群计算性能，能够为万亿参数级别大模型训练提供坚实算力基础。同时，在GPU显存和传输带宽方面，夸娥万卡集群达到PB级的超大显存总容量、每秒PB级的超高速卡间互联总带宽和每秒PB级超高速节点互联总带宽，实现算力、显存和带宽的系统性协同优化，全面提升集群计算性能。超高稳定，月级长稳训练：稳定性是衡量超万卡集群性能的关键。在集群稳定性方面，摩尔线程夸娥万卡集群平均无故障运行时间超过15天，最长可实现大模型稳定训练30天以上，周均训练有效率在99%以上，远超行业平均水平。这得益于摩尔线程自主研发的一系列可预测、可诊断的多级可靠机制，包括：软硬件故障的自动定位与诊断预测实现分钟级的故障定位，Checkpoint多级存储机制实现内存秒级存储和训练任务分钟级恢复以及高容错高效能的万卡集群管理平台实现秒级纳管分配与作业调度。极致优化，超高MFU：MFU是评估大模型训练效率的通用指标，可以直接反应端到端的集群训练效率。夸娥万卡集群在系统软件、框架、算法等层面一系列优化，实现大模型的高效率训练，MFU最高可达到60%。其中，在系统软件层面，基于极致的计算和通讯效率优化等技术手段，大幅提升集群的执行效率和性能表现。在框架和算法层面，夸娥万卡集群支持多种自适应混合并行策略与高效显存优化等，可以根据应用负载选择并自动配置最优的并行策略，大幅提升训练效率和显存利用。同时，针对超长序列大模型，夸娥万卡集群通过CP并行、RingAttention等优化技术，有效缩减计算时间和显存占用，大幅提升集群训练效率。全能通用，生态友好：夸娥万卡集群是一个通用加速计算平台，计算能力为通用场景设计，可加速LLM、MoE、多模态、Mamba等不同架构、不同模态的大模型。同时，基于高效易用的MUSA编程语言、完整兼容CUDA能力和自动化迁移工具Musify，加速新模型“Day0”级迁移，实现生态适配“Instant On”，助力客户业务快速上线。&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jRJbK8ll7KaVcr9AW4DD</id>
            <title>蚂蚁CEO：让AI像扫码支付一样惠及所有人｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/jRJbK8ll7KaVcr9AW4DD</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jRJbK8ll7KaVcr9AW4DD</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:32:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 专业智能体, 大模型, 专业知识引擎
<br>
<br>
总结: 2024年世界人工智能大会在上海开幕，蚂蚁集团董事长表示专业智能体能解决大模型在产业应用中的难题，正在构建专业智能体生态加速产业应用。蚂蚁通过专业知识引擎提供领域专业知识，构建专业智能体框架，推动AI技术服务升级。 </div>
                        <hr>
                    
                    <p>7月4日，2024世界人工智能大会在上海开幕。在产业发展主论坛上，蚂蚁集团董事长兼CEO井贤栋表示，专业智能体能够破解通用大模型在严谨产业应用的关键难题，蚂蚁集团正在携手产业合作伙伴构建专业智能体生态，加速产业应用，推动服务升级。</p><p>&nbsp;</p><p>井贤栋说，在移动互联网时代，二维码让移动支付成为每个人的生活日常，“扫一扫”让小商家用最低的成本享受支付的便利。“在人工智能时代，我们也在探索，让AI像扫码支付一样便利每个人的生活，让AI技术发展的红利惠及更多人。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/20/209a355e2e21a7605042acd447749de9.png" /></p><p></p><p>作为国内较早布局AI大模型的厂商之一，蚂蚁自研的百灵大模型2023年已通过备案，当下正围绕蚂蚁有生态积累、用户有需求的领域打造三个管家，分别是AI生活管家、AI金融管家和AI医疗健康管家。这三个管家对应的生活、金融和医疗等场景，都需要严谨专业优质的服务。</p><p>&nbsp;</p><p>业界普遍认为，通用大模型落地严谨产业，面临着三个“能力短板”：领域知识相对缺乏、复杂决策难以胜任，以及对话交互不等于有效协同。井贤栋介绍，为了破解这些难题，蚂蚁选择了构建专业智能体生态的路径，“从我们的实践来看，专业智能体是大模型落地严谨产业的有效路径。”</p><p>&nbsp;</p><p>首先，针对领域专业知识的短板，蚂蚁携手合作伙伴打造了大规模专业知识引擎，通过知识引擎为大模型提供“专业教材”，让大模型具备专家知识水平。垂直领域、高质量的数据往往以多种模态存在，体系庞杂，比较分散，对隐私保护和资产价值保护的要求高，很难直接“喂给”通用大模型。</p><p>&nbsp;</p><p>井贤栋介绍，蚂蚁依托知识图谱、密态计算等技术，构建了大规模专业知识引擎，可以将垂直领域不同类型的数据，抽象成不涉及隐私信息的领域知识，“合成〞为大模型的“专业教材〞，供大模型训练学习，也能让大模型在推理中随时“翻阅”。这些“专业教材〞，是蚂蚁提升大模型的领域专业性、打造专业智能体的核心能力。</p><p>&nbsp;</p><p>以医疗行业为例，蚂蚁即将发布的“百灵医疗领域大模型”，背后是支付宝和人民卫生出版社、浙江大学联合构建的全国权威医学专业教材医疗知识图谱；蚂蚁和上海市一医院联合打造Al就医助理，背后是上海市一医院自建的“服务与病例知识库”。</p><p>&nbsp;</p><p>其次，针对大模型复杂推理的能力短板，蚂蚁与大量行业技术专家共创，提出了FoE专家级决策框架（Framework of Experts），让智能体借鉴人类专家的思考方式，构建专业的推理和决策能力。严谨产业中存在大量的专业决策框架及公认的行业最佳实践，大模型要达到专业水准，必须谦虚地向专家学习。</p><p>&nbsp;</p><p>蚂蚁通过与各行各业的深度合作，构建了生活、金融、医疗等领域的专家级推理决策框架。以投资研究智能体支小助为例，学习金融专家的分析推理框架后，在接到不同的投研任务时，它会动态学习专家的思考方式，模仿专家的思路进行分析和生成，可以实现媲美人类专家的金融分析能力。目前支小助已经为超百家金融机构提供服务，背后的专业智能体框架AgentUniverse已对外开源。</p><p>&nbsp;</p><p>最后，井贤栋指出，未来智能化的用户体验，一定不是只靠一个大模型，而是需要全行业深度协作，需要很多的专业智能体共同参与、各司其职。蚂蚁坚持走开放道路，和行业共建专业智能体生态。</p><p>&nbsp;</p><p>比如在医疗健康领域，蚂蚁和浙江卫健委联合推出数字健康管家“安诊儿”，背后就是一个连接着多家医院、多个医疗机构的智能体生态。安诊儿的服务贯穿诊前、诊中、诊后，覆盖挂号、咨询、用药、健康科普等26个环节，未来每个环节都会有专业的智能体服务。今年，安诊儿将升级为2.0版本，真正带来全流程的就医智能体网络。</p><p>&nbsp;</p><p>井贤栋认为，在移动互联网时代，生活、医疗、金融等场景中涌现出了一批优秀的应用，形成互联互通的网络对外提供服务。在大模型时代，智能体是新的应用范式，蚂蚁也在探索智能服务新可能。“我们相信，通过专业智能体的深度连接，Al会像互联网一样，带来服务的代际升级。”</p><p>&nbsp;</p><p>今年世界人工智能大会的主题是“以共商促共享 以善治促善智”。在讲话结尾，井贤栋表示，技术的发展总有两面性，我们在使用技术有利的一面解决问题的同时，也要避免技术不利的一面带来的危害。他说，蚂蚁将恪守“平等、尊重、可信、负责”的科技伦理理念，推动Al向善而行，持续用科技为世界带来微小而美好的改变。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OVL8Z6NwTHBiPmNCHbbM</id>
            <title>非Transformer架构大模型公司岩芯数智RockAl走通类脑机制：端侧AI也可以很智能｜WAIC专题报道</title>
            <link>https://www.infoq.cn/article/OVL8Z6NwTHBiPmNCHbbM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OVL8Z6NwTHBiPmNCHbbM</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 16:31:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 多模态机器人, 端侧AI, Yan架构大模型
<br>
<br>
总结: 2024年世界人工智能大会在上海举行，展示了多模态机器人和端侧AI技术的最新成果，其中RockAI的Yan架构大模型引人注目。通过创新的技术突破和实践，端侧AI正逐步克服技术壁垒，向更广泛的应用场景迈进，为智慧生活的未来布局。 </div>
                        <hr>
                    
                    <p>7月4日-7月7日，2024世界人工智能大会（WAIC）在上海举行，来自国内外的数百款大模型集体亮相，呈现了AI大模型智能涌现、赋能千行百业的生动场景。在各色技术及应用的创新体验区中，一个具备高度交互能力的多模态机器人引起了人们的注意。</p><p></p><p>它就是来自岩芯数智RockAI，搭载了树莓派5代芯片的“小智”，在极低算力的设备上实现了强大的多模态能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bf0bfa73a351631e687da8e4892bfca.jpeg" /></p><p></p><p>继年初推出超强性能的Yan1.0云端大模型后，RockAI再次突破了端侧AI“原生无损”门槛，并在这款机器人上部署了此次首发的Yan 1.2大模型。</p><p></p><p>与传统的自动控制机器人不同，小智具备多模态认知能力，能够基于Yan 1.2的语音和视觉处理能力，准确理解用户的模糊指令和意图，并据此控制其机械躯体完成各类复杂任务。随着这款智能机器人在各种模糊指令下描述“视觉”场景、展现“四步成诗”，一场关于端侧AI的全新想象也铺展开来。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/16853c4be6ba0a3da3466ef885a84ac9.png" /></p><p></p><h2>端侧AI打响突围赛</h2><p></p><p>端侧AI通常指在终端设备上直接运行和处理人工智能算法的技术，其优势在于可以直接利用设备的计算能力进行数据处理，不需要将数据发送到云端或服务器进行处理，从而降低对云端计算资源的依赖。且无论网络环境如何都能即时生成响应，确保用户数据安全性的同时，减少了相关算力成本开支。</p><p></p><p>自去年下半年以来，随着大模型的竞争从技术驱动过渡到市场驱动，端侧AI以其广泛的应用场景备受青睐，开始释放出全新的发展价值。全球范围内各大模型厂商纷纷通过各种技术手段，尝试将大模型在物理意义上融入终端。</p><p></p><p>但受制于端侧AI落地的算力和功耗等挑战，传统轻量化部署往往均以性能损失为代价。如目前大热的AIPC是把Transformer架构的模型通过量化压缩部署到个人电脑，仅70亿参数的大模型还需要额外定制PC芯片为其提供算力。而此前在微型电脑主板树莓派5上打出“1.89tokens每秒速度运行、支持8K上下文窗口”战绩的Llama3 8B，同样止步于“有损压缩”。压缩后的模型不仅性能大打折扣，还会失去再训练、再学习的能力，成为无法定时更新底层知识的“一次性AI”。</p><p></p><h2>基于仿生神经元驱动的选择算法，Yan 1.2更专注于端侧设备</h2><p></p><p>RockAI此次首发的Yan 1.2大模型，可以“原生无损”地以6+tokens/s的速度运行于算力仅普通电脑八分之一的树莓派上，并在这个仅有信用卡大小的芯片上实现超强的多模态能力，不仅能“听说读”，还可以识别模糊指令，进行学习、创作及互动。</p><p></p><p>这一成果，起初是得益于对于大模型基础架构的“破坏式”创新。早在今年1月，RockAI发布了国内首个非Transformer的Yan架构大模型。该架构通过对Attention机制的替换，将计算复杂度降为线性，大幅降低了对算力的需求，用百亿级参数达成千亿参数大模型的性能效果，并且率先实现了在主流消费级CPU等端侧设备上的原生无损运行。</p><p></p><p>为了实现树莓派等更多更低端设备的无损适配，RockAI基于全新自研的Yan架构，在实验室对人工神经网络最底层的反向传播算法进行挑战，寻找反向传播的更优解尝试，进一步实现Yan模型的降本增效。同时在算法侧，RockAI采用了基于仿生神经元驱动的选择算法，实现了类脑分区激活的工作机制，使大模型可以根据学习的类型和知识的范围分区激活，大幅减少了数据训练量，同时也能有效发挥多模态的潜力。故而，模型迭代到1.2版本，已经可以实现在PC端、手机端、树莓派端和机器人端等设备上的无损运行。</p><p></p><h2>“同步学习”打造设备端“最强大脑”</h2><p></p><p></p><p>历经了卷参数、卷市场的阶段，大模型当下正集中于一个“卷智能”的时代，因此，让大模型无损跑通更多低算力设备只是第一步，接下来就要思考如何提高端侧大模型的知识密度、智能密度。但RockAI CEO刘凡平还有一个更高的目标，就是在实现通用人工智能的同时，将AI与每个人独特的地方结合在一起，模型具备自主学习能力，让每个设备都拥有个性化的智能。</p><p></p><p>为了实现这种个性化的通用人工智能，RockAI团队首创了“同步学习”理念，让模型具备像人一样实时学习的能力，在推理的同时进行知识更新和学习，无需像云端大模型一样“返厂”进行再次更新或预训练。从而实时、有效且持续性地提升大模型的智能密度，应对各类个性化场景中出现的问题。</p><p></p><p>基于神经网络的底层技术创新，RockAI不断尝试寻找反向传播的更优解，试图能更低代价更新神经网络，实现对现有知识体系的快速更新，辅以模型分区激活降低功耗、实现部分更新，使大模型像人类学习一样建立自己独有的知识体系，实现模型的边跑边进化。会上，RockAI展示了“同步学习”的实验室示例，并表示该机制已处于实验室最后验证阶段。</p><p></p><p>而对于Yan模型在设备端的落地，刘凡平则透露，团队正加紧进行设备端的适配工作，目前已与众多硬件和芯片厂商建立了沟通与合作。</p><p></p><p>RockAI以Yan架构大模型为核心的技术突破与创新实践，标志着端侧AI正逐步克服技术壁垒，向更广泛的应用场景迈进。不仅是对现有计算范式的挑战与超越，更是对未来智慧生活的前瞻布局。</p><p>随着全模态支持+实时人机交互+同步学习的落地，Yan 2.0或将重新定义设备的价值，成为设备的“最强大脑”，真正做到“让世界上每一台设备都拥有自己的智能”。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Q65kQ7ELVLf6pKxBDsIs</id>
            <title>国内最重视生成式AI的企业和最卷的同事们都在哪？｜InfoQ技术大会年中盘点</title>
            <link>https://www.infoq.cn/article/Q65kQ7ELVLf6pKxBDsIs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Q65kQ7ELVLf6pKxBDsIs</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 05:53:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术人, 生成式 AI, 企业实践, 技术大会
<br>
<br>
总结: 近年来，技术人员受到裁员新闻的影响，但生成式 AI 技术的兴起让企业重新关注技术改造。在技术大会上，企业对生成式 AI 的实践非常重视，尤其是传统企业的导入速度更快。互联网企业和金融领域企业在生成式 AI 方面表现突出，而技术服务企业仍然是最活跃的。企业通过技术大会培训员工，提升竞争优势，展示了对技术和人才的持续投入。 </div>
                        <hr>
                    
                    <p>过去几年，技术人仿佛被“裁员新闻”深深笼罩着，甚至有段时间，个别自媒体是按月播报互联网公司裁员新闻的。生成式 AI 时代的到来，让我们看到各大企业再次开始用技术改造一切，去年至今最常被媒体提到的一句话是“移动互联网时代做的事情，都值得用生成式 AI 重来一遍”。</p><p></p><p>那么，哪些企业对生成式 AI 技术实践更加重视呢？今年至今，InfoQ 共举办了三场技术大会，分别是 4 月份的 <a href="https://qcon.infoq.cn/2024/beijing">QCon 北京站</a>"，5 月份的 <a href="https://aicon.infoq.cn/2024/beijing/schedule">AICon 北京站</a>"和 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 深圳站</a>"，这三场大会的单场参会人次均在 1000 以上。在三场大会的所有分享中，与生成式 AI 相关的议题占比高达 80%。根据对这三场大会的购票企业的统计，我们可以看到哪些企业还在对技术、对技术人的成长做持续性投入，而这些企业极有可能因此汇聚更多优秀人才，进而在生成式 AI 时代脱颖而出。</p><p></p><h2>互联网依旧“最卷”，人才不仅要争夺也要培训</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3ae20f1af9ae38ba26c13670226a473c.webp" /></p><p></p><p>根据 InfoQ 的数据统计，截至上半年共有 520+ 企业通过购票的方式让内部员工来到大会现场参会学习。其中，华为上半年累计已有数百位技术人员、业务人员、产品人员、管理者等来到大会现场交流学习。不仅如此，华为也在积极地通过 InfoQ 技术大会的平台对外输出内部技术实践，比如今年 QCon 北京的<a href="https://qcon.infoq.cn/2024/beijing/track/1698">《鸿蒙原生应用开发关键技术与创新竞争力》</a>"专场。此外，其他上榜企业也至少曾派出数十位员工来到大会现场交流学习。令人欣喜的是，相比于以前互联网企业霸屏，我们这次看到了三家传统企业上榜。</p><p></p><p>究其原因，导入生成式 AI 技术对技术团队本身提出了更高挑战，原本具有人才优势的大厂依旧会坚定地选择通过各种培训方式增加员工的知识储备，不具备人才优势的企业更加需要快速行动起来，通过人才引进和内部员工培训来提升企业在数字化时代的竞争优势。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7b22a9252ee8cc19f17dedb19026c62.webp" /></p><p></p><p>如果分行业看，“最卷”的依旧是技术服务类型企业，主要指通过对外提供技术服务解决方案获利的企业，无论是现场参会还是参与大会对外输出实践案例都非常积极。随后依次是互联网、金融、制造 / 汽车、政府 / 高校 / 公共事业、能源 / 电力等。</p><p></p><p>在互联网的众多上榜企业中也可以看到一个很明显的趋势：内部有广泛应用场景的企业会格外重视这一轮浪潮，会更加迫切地希望内部员工快速掌握相关技能，从而完成企业内部的技术升级换代。如果结合大会讲师所在企业再看这一数据也是如此，内部场景丰富的阿里巴巴、字节跳动、腾讯、百度、华为、小红书、快手、哔哩哔哩、饿了么、京东、去哪儿网等企业依旧是实践领先且乐于分享的厂商代表。</p><p></p><h2>生成式 AI 的这一次浪潮，传统企业的导入速度明显加快</h2><p></p><p></p><p>根据 InfoQ 的观察，相比于前几次技术浪潮，这一批浪潮中的金融、制造 / 汽车等相对传统领域的企业导入速度非常快，这类企业对技术交流、人才培养的重视程度高、推进速度快，内部场景丰富，这批参会者的占比在今年上半年上升非常明显，且这些领域的成果也非常丰富。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc5f243f9176ced3237258e82ffec48b.webp" /></p><p></p><p>以金融领域为例，无论是银行、保险还是证券，大众所熟知的企业基本全部都瞄定了生成式 AI 方向。从技术布局上来看，金融企业也完全不逊色于互联网，且部分企业在相对前沿的多模态、数字人、具身智能等领域均有尝试。在实践方面，金融领域在智能体（包括多智能体的协同）、多模态智能风控落地、数据资产化运营与数据智能应用、数字化营销等层面均有不同程度的落地，这些话题也同样是 InfoQ 与铸基计划联合主办的<a href="http://gk.link/a/12oH7"> FCon 全球金融科技大会</a>"的重要议题，届时来自北京银行、广发银行、平安银行、中信银行、华夏银行、太平洋保险、中泰证券、新疆银行、度小满、国投证券、华泰证券、天弘基金、华安保险、工银科技等企业的专家将会同台分享（以上名称不分先后）。</p><p></p><p>FCon 大会官网：<a href="http://gk.link/a/12oH7">http://gk.link/a/12oH7</a>"</p><p></p><p>根据对参会者的调研，大家普遍最关注的内容集中在 Agent、RAG、大模型应用探索，多模态、业务架构、成本优先的技术架构等层面。一方面是大部分企业内部正在走的技术方向，比如 Agent 和 RAG，自然会受到更多关注；一方面是对前沿技术的布局，希望从领先的企业中获取实践经验，比如多模态、大模型应用探索等。</p><p><img src="https://static001.geekbang.org/infoq/29/29fd86b8058287e4bb5bfa9abd3fa7d7.webp" /></p><p></p><p>结合参会者的关注重点及企业专家的意见，InfoQ 即将于 8 月份召开的 <a href="http://gk.link/a/12oH8">AICon 上海站</a>"特别设置了大模型训练以及推理加速、RAG 落地应用与探索、多模态大语言模型的前沿应用与创新、大模型产品应用构建、大模型与企业工具集成的提效实践、大模型产学研结合探索、端侧模型落地探索、大模型数据集构建及评测技术落地、AI Agent 技术突破与应用、大模型场景 + 行业应用落地实践、大模型在搜索、广告、推荐领域的探索、大模型安全性实践等话题，将会邀请 AI 创企、互联网的先行企业、学术和科研机构等专家同台分享。</p><p></p><p>AICon 大会官网：<a href="http://gk.link/a/12oH8">http://gk.link/a/12oH8</a>"</p><p></p><p>与此同时，新能源汽车领域在今年上半年获取了业内的超高关注，其所带来的智能体验和所具备的辅助驾驶能力备受关注，包括汽车企业本身和其生产链基于大模型也做了众多革新。在 <a href="http://gk.link/a/12oH8">AICon 上海站</a>"的主会场，我们有幸邀请到了蔚来汽车的创始人、董事长、CEO 李斌分享蔚来在智能化层面的相关规划和实践。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d5a3659c4efe679602d00158e5544b77.webp" /></p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lYd372BHvHjBILDQao3d</id>
            <title>GPU 集群规模从 4K 飙升至 24K，Meta 如何引领大规模语言模型训练突破</title>
            <link>https://www.infoq.cn/article/lYd372BHvHjBILDQao3d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lYd372BHvHjBILDQao3d</guid>
            <pubDate></pubDate>
            <updated>Thu, 04 Jul 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI研究, 大型语言模型, GenAI, 大规模模型训练
<br>
<br>
总结: 在AI研究中，随着大型语言模型的训练需求不断增加，GenAI的出现导致了大规模模型训练的转变。这种转变带来了许多挑战，包括硬件可靠性、故障时快速恢复、训练状态的有效保存以及GPU之间的最佳连接。为了应对这些挑战，需要在训练软件、调度、硬件和数据中心部署等方面进行创新和优化。同时，网络基础设施的选择也是关键，RoCE和InfiniBand架构都是可行的选项，而同时构建两个24k集群则是一种学习和实践的方式。 </div>
                        <hr>
                    
                    <p>在我们继续将 AI 研究和开发的重点放在解决一系列日益复杂的问题上时，我们经历的最重大和最具挑战性的转变之一是训练大型语言模型（LLM）所需的巨大计算规模。</p><p></p><p>传统上，我们的 AI 模型训练任务会训练大量模型，而这些模型需要的 GPU 相对较少。我们的推荐模型（例如 feed 和排名模型）就是这种情况，这些模型能够获取大量信息以提供准确的建议，为我们的大多数产品提供支持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/7297de5540e03de93569edc85d41efc8.webp" /></p><p></p><p>随着生成式 AI（GenAI）的出现，我们看到了模型训练在向更少的模型数量与更庞大的作业转变。大规模支持 GenAI 意味着重新思考我们的软件、硬件和网络基础设施结合在一起的方式。</p><p></p><p></p><h2>大规模模型训练的挑战</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55abddef125cbc5b3ed7526f473df44e.webp" /></p><p></p><p>在我们增加作业中 GPU 数量的同时，由于硬件故障而中断的可能性也会增长。此外，所有这些 GPU 仍然需要在同一个高速结构上通信才能实现最佳性能。这就引出了四大重要因素：</p><p></p><p>硬件可靠性：确保硬件可靠是非常重要的。我们需要尽量减少硬件故障中断训练作业的可能性。这涉及严格的测试和质量控制措施，以及自动化的快速检测和问题补救机制。</p><p></p><p>故障时快速恢复：尽管我们尽了最大努力，但硬件故障仍会发生。当它们发生时，我们需要能够快速恢复。这就需要减少重新调度开销和快速实现训练重初始化。</p><p></p><p>训练状态的有效保存：如果发生故障，我们需要能够从中断的地方继续。这意味着我们需要定期检查我们的训练状态，并有效地存储和检索训练数据。</p><p></p><p>GPU 之间的最佳连接：大规模模型训练需要以同步方式在 GPU 之间传输大量数据。GPU 子集之间的缓慢数据交换会拖累整个作业的速度。解决这个问题需要强大而高速的网络基础设施，以及高效的数据传输协议和算法。</p><p></p><p></p><h2>跨基础设施栈进行创新</h2><p></p><p></p><p>由于 GenAI 的大规模需求，完善基础设施栈的每一层都很重要。这需要在众多领域取得广泛进展。</p><p>训练软件</p><p></p><p>我们使研究人员能够使用 PyTorch 和其他新的开源开发工具，从而实现极快的研究到生产开发速度。这些努力包括了开发新的算法和技术以进行高效的大规模训练，并将新的软件工具和框架集成到我们的基础设施中。</p><p></p><p></p><h3>调度</h3><p></p><p></p><p>高效的调度有助于确保我们的资源得到最佳利用。这方面的成果包括了可以根据不同作业的需求分配资源的复杂算法，和能够适应不断变化的负载的动态调度。</p><p></p><p></p><h3>硬件</h3><p></p><p></p><p>我们需要高性能硬件来处理大规模模型训练的计算需求。除了大小和规模之外，许多硬件配置和属性都需要针对 GenAI 进行最佳优化。鉴于硬件开发时间通常很长，我们必须调整现有硬件，为此，我们探索了包括功率、HBM 容量和速度以及 I/O 在内的各个方面。</p><p></p><p>我们还修改调整了使用 NVIDIA H100 GPU 开发的 Grand Teton 平台，将 GPU 的 TDP 提高到 700W，并迁移到了 GPU 上的 HBM3 内存上。由于我们没有时间改进冷却基础设施，所以只能留在风冷环境中。机械和热设计必须做出改变以适应这种情况，这触发了一个用来支持大规模部署的验证周期。</p><p></p><p>所有这些与硬件相关的更改都颇具挑战性，因为我们必须找到一种适合现有资源限制的解决方案，并且方案的更改自由度很小，还要满足紧迫的时间表。</p><p></p><p></p><h3>数据中心部署</h3><p></p><p></p><p>一旦我们选定了 GPU 和系统，将它们放置在数据中心中来充分利用各种资源（电源、冷却、网络等）的任务，就需要重新考虑为其他类型的负载所做的许多权衡。数据中心的电源和冷却基础设施无法快速（或轻松）调整，我们必须找到一种最佳布局，以在数据大厅内实现最大算力。这需要将读取器等支持服务移出数据大厅，并安装尽可能多的 GPU 机架，以最大限度地提高功率和网络能力，从而通过最大的网络集群实现最高的计算密度。</p><p></p><p></p><h3>可靠性</h3><p></p><p></p><p>我们需要规划检测和补救措施，以尽可能减少硬件故障期间的停机时间。故障数量与集群的大小成正比，而跨集群的作业需要保留足够的备用容量，以便尽快重新启动作业。此外，我们还会监控各种故障，有时可以采取预防措施来减少停机时间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b38890d52cf9ec4f52181491fa6f0ec6.webp" /></p><p></p><p>我们观察到的一些最常见的故障模式包括：</p><p></p><p>GPU 脱落：在这种情况下，主机无法在 PCIe 接口上检测到 GPU。这种故障有多种原因，但这种故障模式在早期更常见，并随着服务器使用时间增加而逐渐减少。</p><p></p><p>DRAM 和 SRAM UCE：内存中经常出现不可纠正的错误，我们监控和识别重复犯错的单元，跟踪阈值，并在错误率超过供应商阈值时启动 RMA。</p><p></p><p>硬件网络电缆：在常见的服务器无法访问的错误类别中，这些故障也最常出现在服务器刚开始部署的时期。</p><p></p><p></p><h3>网络</h3><p></p><p></p><p>大规模模型训练需要在 GPU 之间快速传输大量数据。这需要强大而高速的网络基础设施以及高效的数据传输协议和算法。</p><p></p><p>业界有两种符合这些要求的领先选项：RoCE 和 InfiniBand 架构。这两个选项都有各自的权衡。一方面，Meta 在过去四年中构建了一些 RoCE 集群，但其中最大的集群仅支持 4K GPU。我们需要更大的 RoCE 集群。另一方面，Meta 已经使用 InfiniBand 构建了多达 16K GPU 的研究集群。但是，这些集群并没有紧密集成到 Meta 的生产环境中，也不是为最新一代的 GPU/ 网络构建的。这让我们很难决定使用哪种架构来构建。</p><p></p><p>因此，我们决定同时构建两个 24k 集群，一个使用 RoCE，另一个使用 InfiniBand。我们的目的是构建并从运营经验中学习。这些经验将为 GenAI 网络架构的未来发展方向提供参考。我们优化了 RoCE 集群以缩短构建时间，并优化了 InfiniBand 集群以提供全双工带宽。我们使用 InfiniBand 和 RoCE 集群来训练 Llama 3，其中 RoCE 集群用于训练最大的模型。尽管这些集群之间存在底层网络技术的差异，但我们可以调整它们，为这些大型 GenAI 负载提供同等的性能</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a48f46f79274501b6f128e397bd3ddf1.webp" /></p><p></p><p>我们优化了整个堆栈的三个方面，使 GenAI 模型的网络通信在两个集群上都有很高的性能：</p><p></p><p>我们将由不同模型、数据和管道并行性产生的通信模式分配给网络拓扑的不同层，以便有效利用网络能力。</p><p></p><p>2.我们实现了具有网络拓扑感知的集体通信模式，降低它们对延迟的敏感度。为了做到这一点，我们使用自定义算法（例如递归加倍或减半）代替传统算法（如环），更改了集体的默认实现。</p><p></p><p>3.就像排名作业一样，GenAI 作业会产生额外的胖流，这使我们很难在所有可能的网络路径上分配流量。这就要求我们进一步投资网络负载平衡和路由，以实现跨网络资源的最佳流量分配。</p><p>我们在 Networking @Scale 2023 上深入讨论了我们的 RoCE 负载平衡技术。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aac40e365bfae27e4862bcb826c5f364.webp" /></p><p></p><p></p><h3>存储</h3><p></p><p></p><p>我们需要高效的数据存储解决方案来存储模型训练中使用的大量数据。这需要我们投资高容量和高速存储技术，以及为特定负载开发新的数据存储解决方案。</p><p></p><p></p><h2>展望未来</h2><p></p><p></p><p>在未来几年中，我们将使用数十万个 GPU 处理更大量的数据，并应对更长的距离和延迟。我们将采用很多新的硬件技术（包括更新的 GPU 架构）并改进我们的基础设施。</p><p></p><p>这些挑战将推动我们以自己尚无法完全预测的方式来创新和适应变化。但有一件事是肯定的：我们这段旅程才刚刚开始。随着我们继续探索不断发展的 AI 格局，我们还在努力突破可能的边界。</p><p></p><p>原文链接：</p><p></p><p>https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ATVgBDcFNoFHJOX9Oi7H</id>
            <title>Runway 的 Gen-3 向所有用户开放付费使用，网友：免费的可灵更香</title>
            <link>https://www.infoq.cn/article/ATVgBDcFNoFHJOX9Oi7H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ATVgBDcFNoFHJOX9Oi7H</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 07:26:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Runway, Gen-3 Alpha, 视频生成, 创意工作者
<br>
<br>
总结: Runway 平台发布了新的生成式 AI 模型 Gen-3 Alpha，具有强大的视频生成功能，吸引了创意工作者的关注和使用。Gen-3 Alpha 在视频真实性和创作方式上有显著提升，用户可以通过简单的提示词和修饰词来生成具有高质感的视频内容。尽管存在一些 bug，但整体效果仍然令人满意。对于普通爱好者来说，Gen-3 Alpha 的收费政策可能会成为一定的阻碍。 </div>
                        <hr>
                    
                    <p></p><p>7 月 2 日凌晨，著名生成式 AI 平台 Runway 在官网宣布，其文生视频模型 Gen-3 Alpha 向所有用户开放使用。而就在上周，Runway 才宣布 Gen-3 Alpha 向部分用户开启测试，短短几天内便全面开放，其速度之快令人惊喜。用户只需要登录 Runway 官网，点击“Get Started”就能够开启体验了。</p><p></p><p>与上个版本的 Gen-2 相比，Gen-3 Alpha 具有更加强大的功能：</p><p></p><p>精细动作控制：能够精确控制视频中对象的动作和过渡，实现复杂场景的流畅动画。逼真人物生成：能够生成具有自然动作、表情和情感的逼真人类角色。多模态输入：支持文字转视频、图像转视频、文字转图像等多种创作方式。先进工具：支持运动画笔、相机控制和导演模式等专业创作工具。</p><p></p><p>Gen-3 在图像的真实性、场景的连贯性以及动态表现上都实现了显著的飞跃，进一步推动了构建一个全面的通用世界模型（General World Models，简称 GWMs）的进程。</p><p></p><p>根据官方的说明，生成一个视频需要以下几个步骤：</p><p></p><p>用户首先需要输入一个简单的提示词，如“瀑布”，然后添加修饰词语来影响视频的风格、构图和整体情绪；制作文本提示后，选择视频的时长（最长 10 秒），然后点击“生成”；生成视频后，用户可尝试用固定的种子编号来获得一致的样式，或者调整文本提示，产生不同的结果。（当提示词遵循清晰的结构，划分为“场景”、“主体”、“相机移动方式”时，提示最有效。）</p><p></p><p>网友们用 Gen-3 制作的视频，无论是美食介绍、微电影宣传，还是人与自然的创意短片，每一个画面都充满了饱和度、光影效果、动作一致性和连贯性。这得益于 Gen-3 的物理模拟功能，它能够让生成的内容严格遵守现实世界的特点。有网友表示，Gen-3 生成速度非常快，10 秒的视频大概只用了一分半就能跑出来，比十几分钟才能生成的 Luma 体验感好多了。</p><p></p><p>效果演示：</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p>不过，也有网友实测发现，虽然 Gen-3 功能强大，但其生成的视频有些还是存在明显 bug。以写实风格为例，人物特写和风景最稳，但是一旦涉及到全景或者中景，当人物没有足够的面积空间时，肢体变形就极为严重。但总体来说，视频的氛围和质感还是很到位的。</p><p></p><p>对于 Runway 如此迅速地开放 Gen-3 使用权限，网友们纷纷表示兴奋，甚至有人认为它已经超越了 Sora。毕竟，Sora 从首次展示到现在已经有 4 个多月了，还在邀请测试阶段，而 Gen-3 的全面开放，无疑是给创意工作者们的一剂强心针。</p><p></p><p>Runway 的创意总监也表示：“Runway 创造了历史，将再次改变文生视频赛道。”</p><p></p><p>不过，比较遗憾的是，这次 Gen-3 并没有像前两代和 Luma 那样免费提供试用，大概是因为算力的问题限流，每个月最少 12 美元才能使用。对此，有网友表示，虽然 RunwayGen-3 实力很强，但依然不得不承认，对于普通爱好者来说，完全免费的可灵更加具有吸引力。</p><p></p><p>参考链接：</p><p></p><p><a href="https://runwayml.com/blog/introducing-gen-3-alpha/">https://runwayml.com/blog/introducing-gen-3-alpha/</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZytQH3mqoCwJz8tK3Jtt</id>
            <title>AI Infra 现状：一边追求 10 万卡 GPU 集群，一边用网络榨取算力</title>
            <link>https://www.infoq.cn/article/ZytQH3mqoCwJz8tK3Jtt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZytQH3mqoCwJz8tK3Jtt</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 07:21:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云行业, AI时代, 星脉网络, 大模型
<br>
<br>
总结: 云计算行业正迎来AI时代，头部企业纷纷投入解决算力和互联问题。腾讯宣布升级星脉高性能计算网络，支持超10万卡大规模组网，探讨改革算力互联方式。AI大模型训练需大规模GPU计算，网络需提升带宽和处理能力。网络通信效率成为集群算力瓶颈，需要技术创新应对。英伟达的InfiniBand在AI训练网络领域占主导地位。 </div>
                        <hr>
                    
                    <p></p><p>云行业进入了生成式 AI 时代，除模型算法外，头部企业纷纷将大量精力投入到解决算力和互联问题上。然而，如果没有网络支持，计算的篇章就无法开启。</p><p></p><p>7 月 1 日，腾讯宣布其自研星脉高性能计算网络全面升级，升级后的星脉 2.0 支持超 10 万卡大规模组网。借此机会，InfoQ 独家专访了腾讯云副总裁兼腾讯云网络总经理王亚晨，探讨了腾讯在改革算力互联方式方面的思考。</p><p></p><p></p><h2>将整个数据中心变成一个“大芯片”？</h2><p></p><p></p><p>前几天，百年风投机构 BVP 发布了一份云计算现状报告，副标题直接使用了这样一句话：“传统云已死，AI 云长存！（The Legacy Cloud is dead , &nbsp;long live AI Cloud!）”他们承认传统云仍然有重大发展机遇，但更震惊于 AI 带来技术变革加速，现如今我们已经很难找到一家不做 AI 的云计算企业了。该报告特别指出，“这是一场关键的‘地盘争夺战’，决定了未来几年哪些大型科技公司将在云和计算市场占据主导地位。”</p><p></p><p>AI 大模型靠的是大力出奇迹，注定了训练它的基础设施跟传统云不一样。</p><p></p><p>由于 AI 的大流行，数据中心也开始从以 CPU 计算为中心到以 GPU 计算为中心。在 CPU 环境中，大规模并行计算的任务可以被分割得很零散，以微信为例，虽然它也是一个庞大的业务，但它的任务是零散且琐碎的。每个用户和每个进程的任务都是不同的，因此可以将任务分散处理。然而，大模型不同，它依赖于强大的计算能力，通常使用 GPU 通过不同的模型或通信方式来处理同一个任务。大模型很难将任务分割得如此零散，希望开发下一代基础模型的企业就不得不投入越来越大的集群来对应挑战。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b3/b3bc8179da089fa9958dc6d6698811c6.png" /></p><p></p><p>集群规模不断上涨，从千卡到万卡，再到十万卡，据王亚晨的描述，“去年大家都在谈论实现万卡集群，只在理论上讲如何实现十万卡。今年的情况有所不同，现在大家实际上已经在实践十万卡集群了。”</p><p></p><p>投入数以万计的 GPU，再通过网络将它们“粘合”起来，导致服务器的带宽接入比以前的服务器大了几十倍，网络设计也需要对应带宽的变化。以往云厂家的主流服务器通常以 100GB 带宽接入，而运营商的接入带宽可能更低。然而，两年前刚推出的 GPU 服务器带宽就达到了 800G 或 1.6T，甚至现在已经达到 3.2T。</p><p></p><p>大模型的训练和推理使得 GPU 卡之间的数据交换量非常大，因此要求数据中心的网络还要具备强大的处理能力。CPU 时代，通常情况下网络带宽利用率在 30% 到 40% 左右，不会让网络跑满，因为需要应对流量突发情况，比如春节或其他用户高峰情况。而当我们将 GPU 服务器做成一个很大的集群后，不再像以前那样以虚拟化单点运算为主，而是大量 GPU 服务器来共同处理一个任务。那么，对于 GPU 来说，由于当前的 AI 业务模型相对单一，尤其在大规模训练时，带宽利用率需要达到 90% 甚至更高，将带宽尽量撑满，GPU 一直忙着才能让训练效率更高。所以需要在硬件和网络协议各方面做出改变。</p><p></p><p>这些资源投入、物理设施和相关技术的巨大变化，使得大多数企业无法参与到竞争中来，王亚晨表示，“不是所有厂家都有能力卷大资源模型。”</p><p></p><p>由此可见，科技界并没有换人掌舵，反而成为云计算老将们的新战场。</p><p></p><p></p><h2>集群算力瓶颈：“网络迭代速度没有算力增长速度快”</h2><p></p><p></p><p>OpenAI 的 Jared Kaplan 在 2020 年首次提出了 Scaling Law，他指出模型大小和计算之间存在缩放关系。不少追随者认为，加以更多 GPU，投入更多数据，就能得到更好的智能。大量的计算意味着需要更大的计算集群，但实践中大家发现这并不简单。</p><p></p><p>第一个瓶颈是能耗，建设 10 万卡 GPU 集群大概需要 120 兆瓦甚至更多电力功耗。3.2 万卡曾被视为数据中心 GPU 数量的上限，一个说法是这是因为电网无法跟上 AI 发展带来的能源需求激增。</p><p></p><p>另外一个瓶颈是行业里运营手段需要提升。当你利用数万张 GPU，连续几十天不停地运行同一个任务时，可靠性和稳定性就成了重中之重。GPU 整个规模上去之后，GPU 故障率是逐渐上升的。</p><p></p><p>更重要的是，网络通信效率亟待提升。网络丢包、拥塞、时延都会导致集群利用率下降，有数据表明，1% 的丢包，GPU 利用率会下降 50%。所以，就算物理上建起来了一个 4 万、5 万、10 万的集群，但是真正能够带多大规模任务跑起来也需要逐步摸索和提升。怎么能够减少故障率，快速发现故障的同时能够让它快速恢复，让训练中断时间越短越好，这是确保大规模训练任务顺利进行的关键。</p><p></p><p>之前 Meta 也有过一个统计，在 AI 训练中网络通信时长占比平均占据了 35% 的时间（最高时 57%），一个直观的解释是：这等于花费数百万或数十亿美元购买的 GPU 有 35% 的时间是无所事事的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/05/05f6ccb5e4d9ceff8ea821be4de90261.png" /></p><p></p><p>这些年来，GPU 的迭代速度非常快，算力增长迅速。“网络迭代速度没有算力增长速度快，如何在网络速度相对滞后于 GPU 算力发展的情况下，确保 GPU 性能不降低，或者至少保持较强的发展势头，成为未来云基础设施在组网层面面临的一个重大挑战。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6c9566c98f557e88cc6533a526bed8a5.png" /></p><p></p><p>为了能够把集群里 GPU 的性能发挥极致，腾讯这两年在网络里面，网络协议、网络软件、端网协同等各方面做了很多技术创新。</p><p></p><p></p><h2>十万卡集群的网络技术壁垒，自研高性能网络</h2><p></p><p></p><p>英伟达的网络连接主要有两种，实现卡间互联的 NVLinks，实现服务器间互联的 Infiniband。</p><p></p><p>InfiniBand 在 AI 训练低延迟网络领域拥有霸主地位，基于英伟达自己的一套协议，配合 GPU 运算特点自成一套体系。</p><p></p><p>虽然从以太网技术本身来讲，想超过 Infiniband 很难，但 infiniband 体系封闭，成本高昂。</p><p></p><p>早在两年前，腾讯就着手自研高性能网络。在大模型兴起之前，腾讯在广告场景中通过软件优化进行 AI 训练和推理时发现，以太网的性能可以达到与 Infiniband 相当的水平。</p><p></p><p>另外，InfiniBand 成本也比以太网技术高很多。在 HPC 和超大规模 AI 云市场中，网络占集群成本的 20% 或更多的情况并不少见。外媒 Nextplatform 根据英伟达的数据算了一笔账，如果你有 10 亿美元，那差不多需要分配 4 亿美元购买 16,000 个 H100 GPU ，还要再花 1 亿美元购买 Nvidia 的 InfiniBand 网络将它们全部连接在一起，剩下的 5 亿美元用于建设数据中心，并在四年内运营、供电和冷却。相比之下，用以太网来建设的成本基本不会超过以上金额的 10%。</p><p></p><p>“基于这几个因素，我们才敢在大模型出现时，选择以太网，并通过自研的方式来解决网络问题。”</p><p></p><p>如何用以太网技术解决拥塞问题，尤其是在拥塞时不丢包，这是星脉团队首先要解决的问题。</p><p></p><p>早期业界没有其他标杆，只能参照英伟达的 Benchmark。以此为基准，腾讯将星脉 1.0 在网络指标上提升至与 Infiniband 相同的水平，并努力做到更优。</p><p></p><p>Benchmark 里面有几个关键数据，第一个是训练过程中的通信时长占比，7%、8% 是目前业界较为领先的水平。而星脉团队将星脉的通信时长占比做到了 6%，这实际远低于 10% 的业界水平。</p><p></p><p>另一个很关键的是网络负载率，星脉优化到 90%，与 IB 网络（Infiniband）持平，相较于标准以太网提升 60%。</p><p></p><p>除了组网技术，更大的壁垒则转向了端网协同能力和运营能力上。这些壁垒，在自研以太网基础上，显然更灵活更容易实现。</p><p></p><p>星脉本身有一套自研协议。通过高性能通信库 TCCL，星脉能看到网络拓扑，能知道什么路径最短。路径最短，拥塞也会变少，丢包概率也会降低。通过自研端云协同协议 TiTa，星脉可以在网络拥塞的时候，将流量做调度，不会产生丢包，也能让网络负载跑得更均匀。</p><p></p><p>以前是依靠软件库与网络的配合，星脉进一步的在网卡层面与整个网络形成一个闭环的控制能力，这样可以实现更好的拥塞控制算法。</p><p></p><p>而快速定位和解决问题的运营能力，也能够在基础设施层面形成另一个非常强的差异化。星脉可以快速感知网络质量，定位因网络问题导致的训练中断等问题，故障时间在整个训练时间中的占比已经降到了一个相对较低的水平。</p><p></p><p>如今，这一决策被证明是正确的。英伟达最近也推出了自家的以太网解决方案，搭配网卡使用，其思路与腾讯的星脉 2.0 不谋而合。</p><p></p><p>行业里实际也已经有了不少使用以太网的企业，比如 Meta 的训练 Llama 3 的集群，一半使用的是 Infiniband，一半是以太网，并且他们宣称以太网集群的性能不比 Infiniband 差。</p><p></p><p>国内腾讯和阿里则都是纯以太网。这些企业也都加入了 Linux 基金会发起的超级以太网联盟 UEC（Ultra Ethernet Consortium），到今年 3 月总共有 55 家公司参与，共同为 AI 发展构建完整的基于以太网的通信堆栈架构。</p><p></p><p></p><h2>从星脉 1.0 到星脉 2.0 的进阶：在工程上支持 10 万卡</h2><p></p><p></p><p>腾讯最早于 2022 年就开始做星脉研发，当时主要是用于广告大模型训练。这个时间点比 OpenAI 推出 chatGPT 还要早上半年。也正是因为有了技术储备，所以能在初期快速构建起星脉 1.0，并将带宽利用率做到 90%，做到无丢包，保证算力不损失，另外还达到了极低时延的要求。</p><p></p><p>在这个背景下，星脉 1.0 实现了单个服务器 3.2T 的接入带宽，业界第一次提出多轨道大规模组网，让集群组网规模更大。同时打造了初步的运营系统平台，主要解决了应用系统中的网络上监控和故障修复问题。</p><p></p><p>星脉 2.0 则希望在工程上实际支持 10 万卡，实现训练推理一体化，进一步去解决推理的成本效率问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a8d9d95fe530067e736c14a1de8e988.png" /></p><p></p><p>在硬件层面，星脉 2.0 引入了自研交换机、自研光模块、自研网卡三套新的硬件。其中，网络交换芯片由 25.6T 升级到 51.2T， 这样对应的整个组网规模就会翻倍。</p><p></p><p>另一个重要方面是星脉 2.0 首次在业内采用了自研的 400G 单口硅光芯片。这一创新的最大特点在于显著降低了能耗、模块能耗以及成本。</p><p></p><p>为了解决 10 万卡集群的性能瓶颈问题，需要实现端和网的协同。因此，除了商业网卡，星脉也首次引入了自研的算力网卡，与自研的软件系统相结合，大幅提升整体性能。</p><p></p><p>拓扑架构设计层面，星脉 2.0 延续了多轨道设计，并且每个节点的容量都升级了，这样就足够支持到十万卡的集群规模的组网。同时未来也能满足 SORA 这种模型架构需要的在网计算（也叫算力卸载）能力。</p><p></p><p>在软件层面，TCCL 从路径规划变为了自适应性能加速，并打通了异构并行计算中的卡间互联网络，从而能够将 NVLinks 以及星脉两种网络在同一个任务中用起来：当机内带宽不足时，可以将外部带宽用起来，利用外部带宽弥补内部卡间互联速率的不足，同时也能够感知两种网络拓扑的使用状态，这种方式能让通信性能提升约 30%。</p><p></p><p>TiTa 在 1.0 阶段，拥塞发生后才会进行调整，而在 2.0 阶段，通过主动干预速度以避免发生拥塞。通过协议和硬件的端到端配合，可以有效地控制传输速率，使得网络从可能会产生拥塞但不会丢包，转变为根本不会产生拥塞的网络。目前来看，这种端网结合也是业界非常重要的发展方向。通过这种方式，能将通信性能再提升 30%，集群训练时长降低 10%。</p><p></p><p>另外，星脉 2.0 的运营系统也进行了升级，引入了仿真系统的概念。在训练过程中，在 GPU 训练中某个卡出了问题，或运算效率突然变慢，是经常出现的问题。尤其是变慢的这种情况下，服务器是不会没有报故障的，因为节点失速并不是故障。新的运营系统可以通过仿真模拟，再结合实际训练过程中产生的日志进行对比，就能知道到底这次训练中哪些 GPU 它到底是失速了，还是有故障节点了，然后快速找出这些节点，进行干预。在实践中，运营系统的升级能将训练问题定位时长从数小时缩短到 10 分钟内。</p><p></p><p>如今星脉在整个系统的层面上也形成了自己的独特优势，包括 GPU 拓扑感知能力、网络仿真系统能快速定位慢失速节点的能力。</p><p></p><p>现在这个技术体系不仅能十万卡集群的真正跑起来，还能做到更精细化运营，整体网络通信效率比上一代提升 60%，让大模型训练效率提升 20%。这意味着，如果原来训练中某个计算结果的同步需要花 100 秒完成，现在只需要 40 秒；原来需要花 50 天训练的模型，只需要花 40 天。</p><p></p><p></p><h2>实现“算力供需平衡”的愿景</h2><p></p><p></p><p>星脉网络作为底层技术支撑了腾讯混元大模型训练。今年，混元大模型的参数规模更是突破了万亿级别，而企业微信、腾讯会议及腾讯文档等都部署了生成式 AI 功能。过程中遇到各种问题，比如训练中断，星脉网络都能凭借强大的技术和稳定的性能，轻松应对。</p><p></p><p>现在，基础大模型还在卷，还在发展，GPT5 也将很快发布。各种应用也开始出现，这些都需要大量算力。大家希望未来算力要像电力一样无处不在，但现在算力短缺是整个人工智能行业面临的一道难题。</p><p></p><p>为应对算力紧缺，OpenAI 今年还出台计划，打算耗资 1150 亿美元，打造星际之门（Stargate）来支持大模型的发展。只是，除了不断扩张数据中心数量和规模之外，我们也应该有足够的技术去“榨取”已有 GPU 资源中的算力。</p><p></p><p>“我觉得未来算力供需要达到相对变化的平衡，很重要一点是能够提升 GPU 算力调度和利用率来缓解相应压力。我们也在讲算力网络，算力网络本身来讲就想让我们的算力调度能力以及算力利用率能够长的更好。”</p><p></p><p>“我们一直有一个愿景，希望算力网络能为大家提供服务，让大家‘用得更快，用得更好，用得更稳’。用得更快指的是算力调度、建设交付和供应响应更快，让大家能够第一时间获取所需资源。用得更好则是指性能更佳，体现在 GPU 利用率、网络各种指标和负载率等方面，性能达到最佳。用得更稳是指运营质量高，不出问题，或在出问题时能够快速定位和恢复，让运营更稳定。”</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651201822&amp;idx=1&amp;sn=3426e28e7320c75c51cbcd4e3a032c58&amp;chksm=bdbbd94d8acc505b83b755476510dd7fd210cbb898b1ea0138942cd52ff0c2a605a4c3744150&amp;scene=21#wechat_redirect">德国再次拥抱Linux：数万系统从windows迁出，能否避开二十年前的“坑”？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651210738&amp;idx=1&amp;sn=eace455941268e51ecc73bd882c13caf&amp;chksm=bdbbbba18acc32b7a354031fd9dc9aac0156043d9e22b77c484d07790a9bc6cc00a7c058f775&amp;scene=21#wechat_redirect">英伟达老员工集体“躺平”，在印钞机上数钱的快乐谁懂？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651210650&amp;idx=1&amp;sn=09fe1190862f0e8104cb351bf2d26e7f&amp;chksm=bdbbbbc98acc32dfa351f7b9264570533571cafbb471a173c67bbf56dd3a1fa839db7a0b2d60&amp;scene=21#wechat_redirect">哈佛退学本科生开发史上最快芯片；居然之家汪林朋：AI时代名校毕业生不如厨师司机，北大的到我那就八千元；英伟达高层频频套现｜Q资讯</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651210364&amp;idx=1&amp;sn=c386ad171334259eee6136ecd77101f7&amp;chksm=bdbbba2f8acc33397ba6928e052102e21727361b6d2688eb77ff7f1817d2993958fc6ba75177&amp;scene=21#wechat_redirect">被全球最大用户弃用！曾经的数据库霸主 HBase 正在消亡</a>"</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P1N2B208sNrShbffLqNW</id>
            <title>微软130亿美元换的OpenAI 董事席，苹果仅靠“刷脸”就拿下了！硅谷明星创企积极投靠大厂</title>
            <link>https://www.infoq.cn/article/P1N2B208sNrShbffLqNW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P1N2B208sNrShbffLqNW</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 06:48:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI创企, 苹果, 微软, 合作伙伴关系
<br>
<br>
总结: 国外AI创企面临压力，苹果和微软加入OpenAI董事会，展开合作伙伴关系，微软投资OpenAI并分享利润，大厂竞购AI创企如Character.AI，合作协议涉及知识产权共享和研发能力提升。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>现在，国外那些AI创企似乎面临的压力越来越大，并在自我独立发展上开始呈现颓态。OpenAI的董事会里现在“入驻”着苹果和微软的核心高管，Character.AI也计划卖给谷歌和Meta。</p><p>&nbsp;</p><p>而与此同时，科技巨头们也在积极接洽AI创企对外投来的“橄榄枝”。由于他们正相互竞争开发尖端技术，寻求与顶级人工智能初创企业建立合作伙伴关系和投资便不失为一条好路子。</p><p>&nbsp;</p><p></p><h1>OpenAI&nbsp;董事会“失守”</h1><p></p><p>&nbsp;</p><p>今早，据外媒报道，苹果已安排&nbsp;App&nbsp;Store&nbsp;首席执行官兼前营销主管&nbsp;Phil&nbsp;Schiller&nbsp;代表其参加&nbsp;OpenAI&nbsp;的非营利性董事会。据悉，Schiller&nbsp;将获得观察员的角色，这意味着他可以参加董事会会议，但不能投票或行使其他董事权力。</p><p>&nbsp;</p><p>然而，加入董事会将使Schiller&nbsp;能够更多地了解&nbsp;OpenAI&nbsp;的内部运作，以及该公司是如何做出决策的。更重要的是，董事会观察员的这一角色，将使苹果与OpenAI最大的支持者和主要的人工智能技术提供商微软在地位上相提并论。</p><p>&nbsp;</p><p>据报道，去年微软也以无投票权的观察员身份加入了能够控制OpenAI的董事会。显然，让苹果和微软同时加入OpenAI的董事会，可能会使OpenAI与任何一家合作公司的讨论计划都变得更加复杂。未来，他们将如何在OpenAI董事会中共存也是一个新问题。</p><p>&nbsp;</p><p>目前，苹果与OpenAI的此项合作并未涉及到任何双方的资金交易。不过，苹果有望从通过其平台订阅的&nbsp;ChatGPT&nbsp;中获得一定比例的收益。</p><p>&nbsp;</p><p>现在苹果公司正致力于在今年晚些时候将&nbsp;ChatGPT&nbsp;整合到&nbsp;iOS&nbsp;和&nbsp;macOS&nbsp;中，如果用户同意，整合后的&nbsp;Siri&nbsp;将可以向&nbsp;ChatGPT&nbsp;发送更高级的查询。而苹果认为，对OpenAI来说，&nbsp;iOS&nbsp;中&nbsp;ChatGPT&nbsp;的曝光比现金“具有等值或更大的价值”。毕竟，这笔交易将使OpenAI能够接触到数亿用户。</p><p>&nbsp;</p><p>但微软的情况又与苹果不同，该公司可是实打实给OpenAI做了资金投入的。</p><p>&nbsp;</p><p>作为战略合作伙伴关系的一部分，微软已向OpenAI投资了约130亿美元，该合作伙伴关系允许ChatGPT制造商使用微软的海量计算和云资源，同时保持独立业务。而根据交易条款，微软有权获得OpenAI利润的一半左右，直到投资得到偿还。</p><p>&nbsp;</p><p>此外，值得注意的是，此前苹果高管很少在与他们合作的公司中占据董事会席位。这次，苹果在OpenAI的安排将于今年晚些时候生效，双方的合作细节也仍在不断变化，现在Schiller&nbsp;尚未参加OpenAI董事会的任何会议。</p><p>&nbsp;</p><p>据了解，Schiller&nbsp;自1997年以来一直担任苹果App&nbsp;Store负责人、执行团队成员。在&nbsp;2020&nbsp;年转任&nbsp;Apple&nbsp;Fellow&nbsp;之前，他曾担任&nbsp;Apple&nbsp;的长期营销主管。在此职位上，Schiller&nbsp;继续领导&nbsp;App&nbsp;Store&nbsp;和&nbsp;Apple&nbsp;活动，并直接向&nbsp;Apple&nbsp;首席执行官蒂姆·库克&nbsp;（Tim&nbsp;Cook）&nbsp;汇报。此前，Schiller&nbsp;还领导苹果公司为App&nbsp;Store辩护，使其免受全球反垄断指控。</p><p>&nbsp;</p><p></p><h1>大厂竞购“缺钱”的&nbsp;AI&nbsp;创企</h1><p></p><p>&nbsp;</p><p>还有一些曾经爆火的&nbsp;AI&nbsp;产品，如今也可能被更大的科技公司变相“收购”，如&nbsp;AI聊天机器人Character.AI。</p><p>&nbsp;</p><p>7月&nbsp;1&nbsp;日，据外媒报道，Character.AI已开始与谷歌和埃隆·马斯克&nbsp;（Elon&nbsp;Musk）&nbsp;的xAI公司、Meta等竞争对手初步讨论了潜在合作机会。这些合作协议可能包括Character.AI利用合作伙伴的计算资源提升研发能力，作为交换，Character.AI将提供一定程度的知识产权共享。</p><p>&nbsp;</p><p>而早在今年5月，就有报道称，Meta&nbsp;和&nbsp;xAI&nbsp;一直在争夺与Character.AI的合作伙伴关系。当时，据四位熟悉内情的人士透露，Meta&nbsp;在与Character.AI进行的合作早期讨论中，谈到了双方顶级研究人员密切合作的问题，比如预训练和开发模型。</p><p>&nbsp;</p><p>两位知情人士说，Character.AI与&nbsp;xAI&nbsp;也就类似的合作关系进行了试探性会谈。但其中一位知情人士表示，Character.AI与他们的讨论重点是推进研究，而不是收购。</p><p>&nbsp;</p><p>据了解，大型科技集团一直对试图全面收购人工智能初创企业持谨慎态度，因为担心全球范围内的监管行动。微软与OpenAI的130亿美元合作就正在接受英国和美国竞争当局的审查，尽管这两家企业坚称他们的合作伙伴关系不是合并。</p><p>&nbsp;</p><p>公开资料显示，AI初创公司Character.AI由两位前谷歌AI技术大佬于2021年11月创立，从安德森·霍洛维茨（Andreessen&nbsp;Horowitz）等风险投资公司筹集了超过1.5亿美元的资金，用于创建包含动漫角色、游戏角色等的人工智能聊天机器人，吸引了数百万用户的关注。</p><p>&nbsp;</p><p>Character.AI的创始人之一、前谷歌研究员&nbsp;Noam&nbsp;Shazeer&nbsp;是&nbsp;2017&nbsp;年一篇论文的作者之一，该论文提出了&nbsp;transformer&nbsp;模型，目前该模型支撑着当今最好的&nbsp;AI&nbsp;模型。</p><p>&nbsp;</p><p>据一位了解&nbsp;Shazeer&nbsp;的人称，&nbsp;Shazeer&nbsp;专注于构建&nbsp;AGI，并为此寻找更多资源。“Character.AI&nbsp;还在探索与其他团体的合作。”一位熟悉该公司战略的人士说。</p><p>&nbsp;</p><p>但在筹集新资金方面，Character.AI似乎遇到了一些困难。据报道，过去一年中，该公司与包括红杉资本在内的投资者进行了多次洽谈，但有知情人士透露，公司尚未完成新一轮的风险资金募集。</p><p>当前，AI初创公司面临的竞争与发展压力似乎越来越大，不仅OpenAI&nbsp;和Character.AI在采取和寻求与科技巨头公司合作的方式，其他AI初创公司也走向了相似的命运。</p><p>&nbsp;</p><p>有爆料称，亚马逊和谷歌正在竞购Anthropic。上个月，Anthropic刚推出了&nbsp;Claude&nbsp;3.5&nbsp;Sonnet，被称为是该公司迄今为止最强大的视觉模型，在标准视觉基准上超过了&nbsp;Claude&nbsp;3&nbsp;Opus。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1dc9a95ef2e207652d5ea0898691609.jpeg" /></p><p></p><p>&nbsp;</p><p>据报道，此前，亚马逊和谷歌这两家巨头都分别向Anthropic大额注资。今年3月，亚马逊宣布已向&nbsp;Anthropic&nbsp;投资高达&nbsp;40&nbsp;亿美元以获得该公司少数股权地位的消息。去年10月，谷歌同意向Anthropic投资高达20亿美元，涉及5亿美元的前期投资和15亿美元的额外投资。</p><p>&nbsp;</p><p>去年年底，Anthropic曾表示，预计到2024年底其将产生超过8.5亿美元的年收入。而一些接近该公司的人士认为，Anthropic2024的年收入可能达到10亿美元，即每月8300万美元的收入。</p><p>&nbsp;</p><p>目前，Anthropic暂没有披露其最新营收与整体财务状况，但近期该公司在公司的财务战略和运营领导上“换帅”。并且，从其最新发布的业务计划来看，Anthropic似乎确实难以独立为之了。</p><p>&nbsp;</p><p>今年5月，曾担任&nbsp;Airbnb&nbsp;企业和业务发展全球主管、帮助该公司度过疫情时期并筹集超过&nbsp;100&nbsp;亿美元的股权和债务资本的Krishna&nbsp;Rao，接任了&nbsp;Anthropic&nbsp;的首席财务官。当时，Anthropic联合创始人兼总裁Daniela&nbsp;Amodei表示：“希望Rao帮助指导Anthropic进入下一阶段的增长。”</p><p>&nbsp;</p><p>7月2日，Anthropic宣布启动一项“为开发评估AI模型性能的第三方新型基准测试提供资金”的计划。该公司表示，它已为该计划聘请了一名全职协调员，并可能购买或扩大它认为有潜力扩大规模的项目。</p><p>&nbsp;</p><p>Anthropic&nbsp;支持新人工智能基准的努力值得称赞，但前提是背后有足够的资金和人力支持。但考虑到该公司在人工智能竞赛中的商业野心，要完全相信它可能很难。</p><p></p><h1>结语</h1><p></p><p>对于这些AI创企当前呈现出的发展颓态，某AI领域知名专家在接受AI前线采访时表示，“这是因为许多AI创企一直没有找到好的商业模式。生成式AI最近几年的宣传比较多，但现在估值撑不下去了，之后可能还会出现不少受此影响的企业。”</p><p></p><p>谈及整个&nbsp;AI&nbsp;创业群体，该人士直言：“OpenAI是八二定律中的80%甚至98%，其他企业都是陪跑的。”</p><p></p><p>而在&nbsp;Engineer/Investor张俊伟博士看来，AI&nbsp;创企纷纷投靠大厂似乎也不是件坏事。他表示，&nbsp;像目前&nbsp;Character.AI&nbsp;针对小众圈子做的内容，由于没有产生一个正向的社会生产力价值，无法支撑未来的长期变现；如果能被&nbsp;Meta&nbsp;买了，有望获得新的生产力。对OpenAI&nbsp;而言，手机长期是&nbsp;AI&nbsp;在&nbsp;C端的直接稳定触达点，&nbsp;苹果在这方面有非常强的溢价能力；至于苹果入主OpenAI董事会，可能是因为大模型做好终端性能的情况下，需要手搓大量算子优化的代码，如果苹果不进董事会，大家缺乏深层次的信任，也就没办法互相开放。</p><p></p><p>另外，张俊伟称，“Character.AI&nbsp;在&nbsp;C&nbsp;端遇到的问题不必太吃惊，因为国内做C端才是最强的，是我们卷出了TikTok，实际上是他们在抄我们。Character.AI本身做了一些创新，也有自己的模型，如果都艰难到这个地步，那也意味着中国“套壳”公司就是会死掉。虽然有人能薅到一些VC的钱，但这肯定不会长久。”</p><p></p><p>并且，张俊伟指出，国内的公司如果因此而死掉，要么是想赚快钱，没有遵循商业规则，要么是&nbsp;AI&nbsp;太快了，没时间去调整业务链了。</p><p></p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/7/2/24191105/apple-phil-schiller-join-openai-board">https://www.theverge.com/2024/7/2/24191105/apple-phil-schiller-join-openai-board</a>"</p><p><a href="https://www.ft.com/content/3414cd0d-09e0-4246-a7db-4ef3032af8b8">https://www.ft.com/content/3414cd0d-09e0-4246-a7db-4ef3032af8b8</a>"</p><p><a href="https://seekingalpha.com/news/4121137-characterai-held-talks-with-google-meta-xai-about-tie-ups-report">https://seekingalpha.com/news/4121137-characterai-held-talks-with-google-meta-xai-about-tie-ups-report</a>"</p><p><a href="https://www.ft.com/content/5cf24fdd-30ed-44ec-afe3-aefa6f4ad90e?trk=public_post_comment-text">https://www.ft.com/content/5cf24fdd-30ed-44ec-afe3-aefa6f4ad90e?trk=public_post_comment-text</a>"</p><p><a href="https://techcrunch.com/2024/07/01/anthropic-looks-to-fund-a-new-more-comprehensive-generation-of-ai-benchmarks/">https://techcrunch.com/2024/07/01/anthropic-looks-to-fund-a-new-more-comprehensive-generation-of-ai-benchmarks/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/haTaSEkmqp5pEaiAYi6X</id>
            <title>动态图结构熵的高效增量计算</title>
            <link>https://www.infoq.cn/article/haTaSEkmqp5pEaiAYi6X</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/haTaSEkmqp5pEaiAYi6X</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 03:14:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 结构熵, 动态图, 增量算法, 社区划分
<br>
<br>
总结: 本文介绍了一种新的增量度量框架 - Incre-2dSE，用于动态图的结构熵计算和社区划分更新。作者提出了朴素调整策略和节点偏移策略来解决传统方法的时间消耗和复杂度问题，同时设计了增量框架Incre-2dSE来有效度量更新后的二维结构熵。该算法在人工和现实数据集上进行了实验，证明了其有效性和可解释性。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/95/950a16925089cc7e416010ca91867c47.png" /></p><p></p><p></p><blockquote>本文介绍来自北京航空航天大学彭浩老师团队发表在 The journal of Artificial Intelligence (AIJ) 2024上的一篇文章“Incremental Measurement of Structural Entropy for Dynamic Graphs”。为了解决当前方法不支持动态编码树更新和增量结构熵计算的问题，作者提出一种新的增量度量框架 - Incre-2dSE，它可以动态调整社区划分，支持更新后二维结构熵的实时度量。作者在人工和现实世界的数据集上进行了广泛的实验，实验结果证明，该增量算法有效地捕捉了社区的动态演化，减少了时间消耗，并具有良好的可解释性。<blockquote>论文名称：Incremental Measurement of Structural Entropy for Dynamic Graphs论文链接：<a href="https://doi.org/10.48550/arXiv.2207.12653">https://doi.org/10.48550/arXiv.2207.12653</a>"代码链接：<a href="https://github.com/SELGroup/IncreSE">https://github.com/SELGroup/IncreSE</a>"</blockquote></blockquote><p></p><p></p><h1>引言</h1><p></p><p></p><p>近年来，有学者提出一种基于编码树的图结构信息度量，即结构熵，用于发现图中嵌入的自然层次结构。结构熵在生物数据挖掘、信息安全、图神经网络等领域得到了广泛的应用。</p><p></p><p>在动态场景中，一个图在时间序列中从初始状态演变为许多更新后的图。为了有效地度量不断变化的社区划分的质量，我们需要在任何给定时间增量地计算更新的结构熵。不幸的是，由于以下两个挑战，目前的结构熵方法不支持有效的增量计算。</p><p></p><p>挑战 1：为每个更新的图重建编码树将导致大量的时间消耗</p><p></p><p>为了解决这个问题，作者提出了两种二维编码树的动态调整策略，即朴素调整策略和节点偏移策略。前者保持原有的社区划分，支持理论结构熵分析；后者基于结构熵最小化原则，通过在社区之间移动节点，动态调整社区划分。</p><p></p><p>挑战 2：传统定义的结构熵计算具有较高的时间复杂度</p><p></p><p>为了解决这个问题，作者设计了一个增量框架，即 Incre-2dSE，用于有效地度量更新的二维结构熵。具体而言，Incre-2dSE首先利用两种动态调整策略生成调整量，即重要统计量从原始图到更新图的变化，然后利用调整量通过新设计的增量公式计算更新后的结构熵。此外，作者还将增量方法推广到无向加权图，并对有向加权图的一维结构熵的计算进行了详细的讨论。</p><p></p><h1>方法</h1><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a93cd474424467b7e2ac9f2283e94eb.png" /></p><p></p><p>图 1 Incre-2dSE与传统离线算法的示意图</p><p></p><h2>二维编码树的动态调整策略</h2><p></p><p></p><h3>朴素调整策略</h3><p></p><p></p><p>朴素调整策略包括两部分：边策略和点策略。边策略规定增量边不会改变编码树的结构；点策略规定，当一个新节点  与已有节点  连接时，且  对应二维编码树中的叶节点 ，即  时，将设置一个标签为  的新叶节点  作为  父节点的直接后继节点，而不是另一个1高度的节点。我们可以从社区的角度来描述编码树的修改。具体来说，增量边不改变现有节点的社区，而新节点被分配到其邻居的社区，而不是另一个任意社区。显然，给定大小为  的增量序列，我们可以在时间复杂度为  的情况下得到更新后的编码树，即更新后的社区划分。</p><p></p><p>在这一部分中，作者引入了全局不变量和局部变化量两个量，通过朴素调整策略实现了更新结构熵的逼近和快速增量计算。对图  施加大小为  的增量序列  ，采用朴素调整策略得到新的图  及其对应的二维编码树  ，更新后的二维结构熵可表示为：</p><p></p><p>$$H^{T'}(G')=\sum_{\alpha_i \in A}(-\frac{g'_{\alpha_i}}{2m+2n}log\frac{V'_{\alpha_i}}{2m+2n}+\sum_{v_j \in T_{\alpha_i}}-\frac{d'_j}{2m+2n}log\frac{d'_j}{V'_{\alpha_i}}) (1)$$</p><p></p><p>然而，增量大小  会影响上式中求和方程中的所有项。因此，更新和计算过程的成本至少为  ，当图变得非常大时，这个成本是巨大的。一种直观的尝试是在更新的结构熵和原始的结构熵之间作差，并尝试以  计算增量熵。然而，由于在上式的所有项中  都变为  ，因此很难通过作差推导出简洁的  增量计算公式。为了解决这个问题，作者在这里引入了全局不变量和局部变化量。作者将全局不变量定义为更新后结构熵的近似，局部变化量定义为更新后的结构熵与全局不变量之差，也可视为近似误差。总的来说，通过计算和求和全局不变量和局部变化量，可以在  内计算出更新后的二维结构熵。</p><p></p><h3>节点偏移策略</h3><p></p><p></p><p>虽然朴素调整策略可以快速获得更新后的二维编码树及其相应的结构熵，但我们仍然需要一种更有效的策略来获得具有较低结构熵的更好的社区划分。因此，作者提出了另一种新的动态调整策略，即节点偏移策略，其主要思想是迭代地将节点移动到其最优偏好社区。与朴素调整策略不同，边变化可以改变现有节点的社区，使结构熵最小化。此外，该策略支持同时增加多个边和删除现有边。因此，节点偏移策略基本克服了朴素调整策略的局限性。</p><p></p><p>首先将最优偏好社区（OPC）定义为目标节点的最佳社区，即如果目标节点进入其OPC，则总体二维结构熵与进入OPC以外的其他社区相比一定是最小的。节点偏移策略可描述为：（1）设涉及节点为增量序列中出现的所有节点；（2）对于每个涉及节点，将其移动到其OPC；（3）将涉及节点更新为与发生移动的节点连接但在不同社区的所有节点，然后重复步骤（2）。</p><p></p><h2>Incre-2dSE：增量度量框架</h2><p></p><p>图1展示了增量度量框架（包括初始化和度量两个阶段）和传统离线算法（TOA）。Incre-2dSE的目的是在给定原始图、原始编码树和增量序列的情况下，在动态调整社区划分的同时，有效地度量更新后的二维结构熵。</p><p></p><h3>阶段1：初始化</h3><p></p><p>给定图  为稀疏矩阵，其二维编码树由如下字典表示：{社区ID 1：节点列表1，社区ID 2：节点列表2，…}时，可以很容易地获取并保存结构数据，其时间复杂度为  。然后使用保存在  中的结构数据计算结构表达式。总的来说，初始化阶段需要总时间复杂度为 。</p><p></p><h3>阶段2：度量</h3><p></p><p></p><p>在这个阶段，我们首先需要生成从  到  的调整。通过提出的两种动态调整策略，作者提供了两种算法来生成调整量，即朴素调整量生成算法（NAGA）和节点偏移调整量生成算法（NSGA）（图1中的①）。两种算法的输入都是原始图的结构数据和一个增量序列，输出是一个调整。NAGA的时间复杂度为  ，因为它需要在增量序列中遍历  条边，而每条边只需要花费  。在NSGA中，我们首先需要  来初始化调整。其次，在节点移动部分，我们需要确定所有涉及节点的OPC，这需要花费  。此步骤重复  次，时间开销为  ，其中  表示第  次迭代中涉及的节点数。由于大多数情况下满足  和 ，所以NSGA的总时间复杂度为 。</p><p></p><p>得到调整值后，可以增量计算更新后的二维结构熵:</p><p></p><p></p><p></p><p>为了实现上述增量计算过程，作者还提供了基于调整的增量更新算法（AIUA）（图1中的②）。给定输入，即原始图的结构数据和结构表达式以及更新后的图的调整，我们可以增量计算更新后的二维结构熵，并在新的调整到来时有效地更新结构数据和结构表达式，为下一个AIUA过程做好准备。更新结构数据的时间复杂度为 。更新结构表达式的时间复杂度为 。计算更新后的二维结构熵的时间复杂度为 。综上，AIUA的总时间复杂度为 。</p><p></p><h2>基线：传统离线算法（TOA）</h2><p></p><p></p><p>传统离线算法（TOA）对每一个更新的图重构编码树，并通过定义计算更新后的二维结构熵。TOA由以下四个步骤组成。首先，将原始图与增量序列结合生成更新后的图（图1中的a）。其次，使用几种不同的静态社区检测算法，如Infomap、Louvain、Leiden，将图节点集划分为社区，构建二维编码树（图1中的b）。第三，对更新后的图的节点级、社区级、图级结构数据进行计数并保存（图1中的c）。更新后的结构熵通过式1计算（图1中的d）。TOA的总时间成本为  加上所选社区检测算法的成本。</p><p></p><p>作者给出了传统离线算法的伪代码，如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/765ef9f9a511d4fe1af2166c43e9a89b.png" /></p><p></p><p><font size="1"></font></p><center><font size="1">图 2 传统离线算法的伪代码。</font></center><p></p><p></p><h2>复杂图的扩展</h2><p></p><p></p><p>作者在文章中讨论了将此方法扩展到无向加权图或有向图的可行性。首先，作者论证了无向加权图的方法可以由无向无权图的方法自然推广。其次，分析了有向图结构熵增量计算范式与无向图结构熵增量计算范式的根本区别，提出了有向加权图一维结构熵增量计算的新方法。</p><p></p><p>无向加权图：无向加权图结构熵的增量度量方法可以直观、方便地从之前提出的无向无权图结构熵增量度量方法中扩展出来。作者首先介绍了无向加权图的二维结构熵的定义。在此基础上，更新了结构熵调整的定义，提出了新情况下结构熵计算的增量公式。</p><p></p><p>有向图：由于有向图的结构熵度量与无向图的结构熵度量有本质的不同，因此本文提出的主要方法难以转移到有向图场景中。其中关键的区别在于有向图需要转换成一个转移矩阵，并计算平稳分布。由于二维结构熵的增量计算非常复杂，在这一部分中，作者简要地提出了一种度量有向权图一维结构熵的增量方案。具体来说，首先定义了有向加权图及其非负矩阵表示。然后，引入了有向加权图的结构熵公式。最后，回顾了有向加权图一维结构熵精确或近似计算的传统方法，即特征向量计算和全局聚合，并提出了一种增量迭代逼近算法，即局部传播算法，如图3所示。</p><p></p><p>在全局聚合中，每次迭代都需要遍历所有的节点和边，这导致了很高的计算冗余。在这一部分中，作者提出了一种快速逼近更新后的一维结构熵的新方法，即局部传播。顾名思义，其关键思想是利用式（3）将局部受到增量影响的节点的信息进行传播，动态地更新平稳分布，从而获得低于全局聚合的时间复杂度。</p><p></p><p>$$\pi^{(\theta +1)}i=\sum{v_j \in N(v_i)} \pi^{(\theta)}j b{ji} (3)$$</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be250de1ed8d97909e52ba50c8cabed5.png" /></p><p></p><p>图 3 局部传播算法的示意图</p><p></p><h1>实验与评估</h1><p></p><p></p><p>作者基于动态图形实时监控和社区优化的应用进行了广泛的实验。</p><p></p><h2>数据集介绍</h2><p></p><p></p><p>人工数据集：首先，作者利用“Networkx”（一个Python库）中的随机分区图(random)、高斯随机分区图(gaussian)和随机块模型(SBM)方法生成动态图的3种不同初始状态。之后，通过Hawkes Process对每个初始状态生成增量序列和更新图。霍克斯过程通过假设历史事件可以影响当前事件的发生，对离散序列事件进行建模。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/869ccd293af78cefba9cc4dfbab610d4.png" /></p><p></p><p>图 4 人工Hawkes数据集生成过程。</p><p></p><p>真实数据集：对于现实世界的数据集，作者选择了Cit-HepPh、DBLP和Facebook进行实验。对于每个数据集，作者截取了21个连续的快照（一个初始状态和20个更新的图）。由于结构熵仅在连通图上定义，因此只保留每个快照的最大连通分量。总的来说，图5简要显示了人工数据集和真实数据集的统计数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3fee7ab0c423b2855f529b5192383cae.png" /></p><p></p><p>图 5 人工数据集和真实数据集的统计描述</p><p></p><h2>3.2 实验结果与分析</h2><p></p><p></p><h3>应用：动态图形实时监控和社区优化</h3><p></p><p>在本应用中，我们旨在通过NAGA+AIUA和NSGA+AIUA的增量算法优化社区划分并监控相应的二维结构熵，以及基线TOA来实时量化动态图的每个快照的社区质量。具体来说，对于每个数据集，我们首先从Infomap、Louvain和Leiden中选择一种静态社区检测方法（简称静态方法）生成初始状态的社区划分。实验结果如图6（真实数据集）和图7（人工数据集）所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c603f342a7d14797c95363d4ef9885b4.png" /></p><p></p><p>图 6 NAGA+AIUA、NSGA+AIUA和TOA在真实数据集上使用不同静态方法度量的更新后的结构熵。结构熵越低，性能越好</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c597ec4ab6a8cfc709351760382a5bc4.png" /></p><p></p><p><font size="1"></font></p><center><font size="1">图 7 NAGA+AIUA、NSGA+AIUA和TOA在不同静态方法人工数据集上度量的更新结构熵。由于人工数据集的三条曲线比真实数据集的曲线更接近，因此所有显示的结构熵值都从NAGA+AIUA的结构熵值中减去，以更好地显示曲线之间的差异。</font></center><p></p><p></p><h3>超参数研究</h3><p></p><p></p><p>在这一部分中，作者评估了节点偏移策略的不同迭代次数对更新结构熵的影响。作者使用迭代次数的NSGA+AIUA分别度量前一小节中每种情况下20个更新图的平均更新结构熵。实验结果如图8所示，更新的结构熵随着迭代次数的增加而减少。这是因为，随着迭代次数的增加，更多的节点将转移到它们的OPC，这导致结构熵进一步降低。实验还表明，节点偏移策略具有良好的可解释性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/797f25b960a2cb26fc1d1b94324ee86a.png" /></p><p></p><p>图 8 不同迭代次数下节点偏移策略更新的结构熵。黑体数字表示最低结构熵</p><p></p><h3>时间消耗评估</h3><p></p><p></p><p>图9给出了NAGA+AIUA和NSGA+AIUA（N=3,5,7,9）这两种增量算法在所有6个数据集上的耗时比较。图中的纵轴表示所选增量算法在所有20个快照中的平均耗时。横轴表示3个选定的静态方法。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ed4e87d4c24510e75a693f05a1e7ec8.png" /></p><p></p><p>图 9 NAGA+AIUA和NSGA+AIUA （N=3,5,7,9）在不同静态方法下每个数据集超过20个时间戳上的平均耗时。</p><p></p><p>图10给出了在线算法NSGA+AIUA（N = 5）与离线算法TOA的时间对比。从结果可以看出，作者提出的所有增量算法都比现有的静态方法快得多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/5378a329bd04c596c72a04aac9df7af5.png" /></p><p>图 10 增量算法（在线时间）与基线传统离线算法（离线时间）的耗时比较。</p><p></p><h3>Incre-2dSE与当前静态结构熵度量方法的差距</h3><p></p><p></p><p>在这一部分中，作者研究Incre-2dSE与当前静态算法之间的差距。目前主流的结构熵度量静态算法称为结构熵最小化（SEM），是一种以结构熵为目标函数的静态图 k 维编码树的贪心构造算法。作者在六个数据集上的所有时间戳上度量了Incre-2dSE（NAGA/NSGA+AIUA）和2d-SEM的结构熵，如图11所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/327d0bd353a05eed3e03447cacd053da.png" /></p><p></p><p>图 11 六个数据集上的时间戳度量Incre-2dSE（NAGA/NSGA+AIUA）和2d-SEM的结构熵。</p><p></p><p></p><h3>有向加权图的一维结构熵度量</h3><p></p><p></p><p>作者还评估了两种近似一维结构熵度量方法，即全局聚集和局部传播，在两个人工数据集上的时间消耗（ER数据集和Cycle数据集）。耗时实验结果如图12所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/659764a68e1f5668af55c1e78a96d9a7.png" /></p><p></p><p>图 12 ER和Cycle数据集上全局聚合和局部传播的时间消耗。</p><p></p><p>除以上列出的实验结果之外，作者还进行了更新阈值分析、鲁棒性分析、收敛性分析。这些分析的结果表明，①设置更新的阈值可以提高效率，并更好地适应频繁更改的图形；②本文的增量算法使结构熵保持在一个稳定和较低的水平上，对不断增加的噪声具有很高的鲁棒性；③局部差值总是小于它的上界，有力地支持了局部变化量及其一阶绝对矩的收敛性。</p><p></p><h1>结论及展望</h1><p></p><p></p><p>本文提出了两种新的动态调整策略，即朴素调整策略和节点偏移策略，以分析更新的结构熵，并逐步调整原有的社区划分，使其朝着更低的结构熵方向发展。作者还实现了一个增量框架，即支持更新的二维结构熵的实时度量。进一步，作者讨论了提出的方法在无向加权图上的推广，以及在有向加权图上的一维结构熵计算。在未来，作者的目标是开发更多的动态调整策略，用于层次化社区划分和高维结构熵的增量度量算法。</p><p></p><p>篇幅原因，我们在本文中省略了诸多细节，更多细节可以在论文中找到。感谢阅读！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OfSAi8R4p5OKlI0sBy6Z</id>
            <title>解码RAG：智谱 RAG 技术的探索与实践 ｜ AICon</title>
            <link>https://www.infoq.cn/article/OfSAi8R4p5OKlI0sBy6Z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OfSAi8R4p5OKlI0sBy6Z</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 01:40:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AICon, RAG, 智谱, 大模型技术
<br>
<br>
总结: 在AICon北京站上，智谱企业商业技术中心的总经理柴思远分享了RAG在智谱的探索与实践，介绍了RAG的三个关键步骤：Indexing、Retrieval、Generation。智谱AI长期专注于大模型技术研究，通过RAG技术解决了大模型应用中的幻觉、知识更新不及时等问题，降低了实施成本，提高了问答的精度和效率。 </div>
                        <hr>
                    
                    <p>在<a href="https://aicon.infoq.cn/202405/beijing/">AICon </a>"北京站上，智谱智谱企业商业技术中心的总经理柴思远分享了RAG 在智谱的探索与实践，本文为演讲内容整理文章，期待给你带来启发。</p><p></p><p>作者 | 柴思远</p><p></p><p>智谱 AI 长期专注于大模型技术的研究，从 23 年开始，大模型受到了各行各业的关注，智谱 AI 也深度的参与到各种场景的大模型应用的建设当中，积累了丰富的模型落地应用的实战经验，其中 RAG 类应用占据了较大的比重。</p><p></p><p>所谓 RAG，简单来说，包含三件事情。第一，Indexing。即怎么更好地把知识存起来。第二，Retrieval。即怎么在大量的知识中，找到一小部分有用的，给到模型参考。第三，Generation。即怎么结合用户的提问和检索到的知识，让模型生成有用的答案。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a08ddc53fbfd6b7e503adc6897348e5f.png" /></p><p></p><p>这三个步骤虽然看似简单，但在 RAG 应用从构建到落地实施的整个过程中，涉及较多复杂的工作内容。为此，智谱 AI 组建了一支专业团队，专注于打造企业服务场景的 RAG 系统，致力于为客户提供全面的支持与服务。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59aae3a64c032837358d0d3e6dfd901c.png" /></p><p></p><p>那么使用 RAG，有哪些优势呢？我们总结有以下几个方面：</p><p></p><p>1.与直接跟大模型对话的方法相比，RAG 可以更好地解决模型的幻觉、知识更新不及时等问题。</p><p></p><p>2.与传统的 FAQ 或者搜索的方式相比，RAG 可以显著降低实施成本。例如传统需要人工整理的 FAQ 的场景，今天我们只需要把手册资料交给 RAG，就能实现高效准确的问答。</p><p></p><p>3.相较于大模型直接生成内容的方式，基于 RAG 的生成可以追溯到内容的来源，知道答案具体来源于哪条知识。大模型就像是计算机的 CPU，负责计算答案；而知识库就像是计算机的硬盘，负责存储知识，这种计算和存储分离的架构，便可以对知识回答的范围进行权限管理。</p><p></p><p>4.目前大模型已具备了处理长上下文的能力，然后，如果每次问答都需要把几十万字的文档输入进去，那么会导致问答的成本成倍增加，特别是在客服场景。实际上我们只需要使用整个文档中一个很小的片段，就可以完成任务。所以在同样精度的情况下，利用 RAG 技术可以大大地降低整个成本。</p><p></p><h3>智谱&nbsp;-RAG 解决方案</h3><p></p><p></p><h4>技术方案</h4><p></p><p></p><p>下图是技术方案的全景图</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7f/7fc0827e50274a41a32a975297cb42ab.png" /></p><p></p><p>整个技术方案包括三个层面：文件上传、用户提问和答案输出。这三个层面都需要有大量的工程和策略的工作去进行打磨。</p><p></p><p>以文件上传为例。在文件解析过程中，我们需要将无关的信息（页眉页脚等）过滤掉、将图片改写成特定标识符、将表格改写成模型易于理解的 html 格式等操作。同时，我们会对目录、标题等进行识别，有效提取文档的结构信息；也会对文件中的序列信息进行识别，以确保知识的连续完整。</p><p></p><p>此外，Embedding 模型本身因为有窗口限制，文档切片过大会导致检索信息不准确。为了解决这个问题，我们采用了 small to big 的策略，即在原始文档切片基础上，扩展了更多粒度更小的文档切片。检索文档时如果检索到粒度细致的切片，会递归检索到其原始大切片，然后再将原始节点做为检索结果提交给 LLM。</p><p></p><h4>产品方案</h4><p></p><p></p><p>下面是产品方案的全景图</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/35/35e69bab8ce479d3f377d3361d0af509.png" /></p><p></p><p>在知识构建过程，我们提供了包括知识类型管理、切片管理、索引管理和数据运营等知识运营和管理的工具，以此来辅助提升企业服务场景的落地效果。</p><p></p><p>在知识问答过程，我们提供了包括历史消息、输入提示、原文索引、图文混排、原文查看等功能，以此来加强用户对模型回复答案的信任。</p><p></p><p>从产品应用层面，一般有三种常见的落地类型，分别为个人使用，企业对内赋能，企业 toC 提供服务等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b0/b06791205f7b9438e516ed92ec1dc639.png" /></p><p></p><p></p><h3>智谱&nbsp;-RAG 在智能客服的实践</h3><p></p><p></p><p>下面我以「公共事务客服问答场景」为例，介绍我们在 RAG 上的实践。</p><p></p><p>这个场景其实大家都比较熟悉。例如 12329 公积金便民热线。针对这样的场景，原来的做法主要是两大技术内容：对话引擎（脚本编排）和文档引擎（检索系统）。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f356f58d4e0ce19100cfc6d2d285d506.png" /></p><p></p><p>但这样的技术面临着几个痛点：</p><p></p><p>1.知识整理成本高。例如，公积金领域，全国各市有不同政策。启动项目时，一个城市大约需要 3,000 个 FAQ，运营过程中会增加至 6,000 个，导致高昂的维护成本。</p><p></p><p>2.知识复用性差。人力专家是能全面解答全国各地的公积金问题，然而原有的智能系统无法跨城市复用知识，缺乏模型上的通用学习能力。</p><p></p><p>3.知识更新频繁。各市每年都会有年度政策版本出台，每隔几个月还会有补充性政策，增加维护成本。4、知识晦涩难懂。虽然涉及日常场景，但政策内容复杂，不易为大众理解。</p><p></p><p>此外，在交互层面，也同样存在问题：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0c/0c71c0255e47b4798b56e628d763d04d.png" /></p><p></p><p>1.FAQ 模式的回答范围有限，无法涵盖所有问题，容易导致用户体验下降。</p><p></p><p>2.交互方式如电话菜单或文本弹窗缺乏拟人化体验，若无法命中问题，用户会快速失去对智能客服的耐心，转而寻求人工服务。</p><p></p><p>3.传统 NLP 技术缺乏对人类对话的理解能力，智谱 ChatGLM 大模型原生的就能够理解对话的上下文。</p><p></p><p>4.旧方法只能提供固定答案，无法针对特定情况精准回答，而智谱 ChatGLM 大模型能够生成有效答案或者推理生成更有针对性的答案。</p><p></p><p>针对同样的场景问题，智谱通过“ChatGLM 大模型 +RAG”的方案来解决。整个成本和效果可以有大幅提升如，下图所示：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a3/a345304e9d88d8b74140d13a6e38453b.png" /></p><p></p><p>此项目面临如下几个技术挑战：</p><p></p><h4>Embedding</h4><p></p><p></p><p>第一个挑战是知识召回。</p><p></p><p>切片问题：传统按长度切片方法效果不佳，因为政策内容知识密度高，每句话都可能包含答案，且条款间关联性强，需要连续多个条款才能完整回答问题。Embedding 微调：通用 Embedding 模型不足以应对用户口语化严重的问题，需要针对具体业务场景进行微调，以过滤无关信息并提高准确度。</p><p></p><p>针对前者，我们采用文章结构切片以及 small to big 的索引策略可以很好地解决。针对后者，则需要对 Embedding 模型进行微调。我们有四种不同的构造数据的方案，在实践中都有不错的表现：</p><p></p><p>Query vs Original：简单高效，数据结构是直接使用用户 query 召回知识库片段；Query vs Query：便于维护，即使用用户的 query 召回 query，冷启动的时候可以利用模型自动化从对应的知识片段中抽取 query；Query vs Summary：使用 query 召回知识片段的摘要，构建摘要和知识片段之间的映射关系；F-Answer vs Original：根据用户 query 生成 fake answer 去召回知识片段。</p><p></p><p>经过微调后的 Embedding 模型在召回上会有大幅地提升。top 5 召回达到 100%，而且不同 Embedding 模型微调后的召回差异在 1 个点之内，模型的参数规模影响极小。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/67/679fe95cc1ab93b98e3329bf9ddf1a36.jpeg" /></p><p></p><p></p><h4>SFT&amp;DPO</h4><p></p><p></p><p>另外一个挑战是答案生成。在生成环节中，我们面临以下数据挑战：</p><p></p><p>数据标注难度大：业务人员虽然知道正确答案，但难以标注出满足一致性和多样性要求的模型微调数据。因此，我们需要在获取基础答案后，通过模型润色改写答案或增加 COT 的语言逻辑，以提高数据的多样性和一致性。问答种类多样：业务需要模型能够正确回答、拒答不相关问题和反问以获取完整信息。这要求我们通过构造特定的数据来训练提升模型在这些方面的能力。知识混淆度高：在问答场景中，召回精度有限，模型需要先从大量相关知识片段中找到有效答案，这个过程在政务等领域难度很大，需要通过增加噪声数据来强化模型的知识搜索能力。答案专业度高：在公共服务的客服场景，答案往往没有绝对准确性，资深的客服人员总能给出更有帮助性的答案。用户问题通常含糊，更加考验专业人员的回答能力。因此我们需要通过 DPO 方式训练模型，使模型能够在众多答案中找到最好最优的答案。为此，我们需要分别构造数据，并针对模型做 SFT 和 DPO。</p><p></p><p>在构造数据时，通常情况下，提供更多的高质量训练数据，微调效果越好。反之，如果训练数据中存在错误、瑕疵，将对微调效果产生一定的负面影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/76/7698f4bceab100093b79bb08c32733dc.png" /></p><p></p><p>当构造了优质的数据后，模型微调上，我们一般会采用分阶段微调，即首先用开源通用问答数据进行微调，然后用垂域问答数据微调，最后用人工标注的高质量问答数据进行微调。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fe/fee31ac56a7d56a8c6619517cae40261.png" /></p><p></p><p>DPO 的训练目标就是让正样本概率加大，负样本概率变低。不仅教会模型什么是好的，也会告诉模型什么是差的。对于问答类场景非常有效果，从而让模型能够更好地向人类的真实需求进行对齐。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/95/95996723dfc2029731053b692620068c.png" /></p><p></p><p>通过以上的方案，我们能够将原本只有 60% 左右的正确率，提升到 90% 以上。</p><p></p><h4>评测</h4><p></p><p></p><p>评测是模型训练过程中的指南针，好的评测集可以快速的帮助我们找到优化的方向，拉齐算法和业务之间的分歧。构建评测数据集要确保遵循几个原则：</p><p></p><p>真实性：评测集要能真实的反应业务实际需求，与实际发生的业务场景一致。例如评测问题应该尽量覆盖用户平时会问的问题，保持用户平时对问题的表述风格。多样性：评测集要能够覆盖不同的业务内容，包括：不同的用户输入类型、期待的输出类型、以及答案生成的逻辑等。等比例：评测集各种类型数据的分布比例应与实际业务场景接近，如果已有线上数据的可以根据线上数据抽样。难度区分：生成式模型模拟人脑的思路来推断答案，题目的难度是一个非常重要的维度。业务人员往往很难系统的梳理这些难度，所以我们的算法同学需要主动的引导，构造出覆盖不同难度问题的评测集。</p><p></p><h3>结尾</h3><p></p><p></p><p>展望未来，RAG 技术将会在更多领域得到应用，并与其它 AI 技术相结合，例如多模态交互、个性化推荐、用户长期记忆等。智谱 AI 将继续致力于 RAG 技术的探索与实践，为企业在更多的领域落地大模型应用，提供更加智能、高效的服务体验。</p><p></p><p>嘉宾介绍</p><p></p><p>柴思远，智谱企业商业技术中心的总经理，大数据算法技术专家，组建智谱解决方案团队，支持过美团、360、金山、小米等重点大模型项目落地；曾历任大搜车数据中台负责人、妙计旅行联合创始人、搜狗搜索 NLP 研究员等。</p><p></p><p>活动推荐</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在<a href="https://aicon.infoq.cn/202408/shanghai/">上海举办 AICon 全球人工智能开发</a>"与应用大会，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vjdav8rUBDbXk9pQLedR</id>
            <title>英伟达老员工集体“躺平”，在印钞机上数钱的快乐谁懂？</title>
            <link>https://www.infoq.cn/article/vjdav8rUBDbXk9pQLedR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vjdav8rUBDbXk9pQLedR</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 08:49:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 英伟达, 股价飙升, 员工财富, AI芯片市场
<br>
<br>
总结: 英伟达近年来股价飙升，公司市值达到惊人的3.2万亿美元，员工财富积累随之增长。公司在AI芯片市场占据主导地位，但面临着日益激烈的竞争压力。公司高管提醒员工保持创新和卓越，以维持市场领先地位。 </div>
                        <hr>
                    
                    <p></p><h2>实现财富自由的英伟达高管们，被爆已集体躺平</h2><p></p><p>&nbsp;</p><p>在科技界，很少有公司能像英伟达近年来那样实现如此惊人的增长。自 2024 年初以来，英伟达的股价飙升了惊人的 167%，标志着该公司的增长故事又翻开了新的篇章。</p><p>&nbsp;</p><p>得益于多年的技术积累，英伟达满足了全球几乎所有主要云计算和 AI 公司对 GPU 的需求。在过去五年中，英伟达股价上涨超3000%，证明了英伟达在半导体和人工智能市场的主导地位。公司总裁兼首席执行官黄仁勋 (Jensen Huang) 也成为了科技界超级明星，几乎每周都能听到老黄接受媒体采访的新闻。</p><p>&nbsp;</p><p>这一惊人的增长不仅使公司的市值达到惊人的 3.2 万亿美元，而且还改变了许多员工的财务状况。随着公司股价飙升，五年前或者更早加入公司的员工现在都是百万富翁了，他们的财富积累跟随着公司的股价一路水涨船高。</p><p>&nbsp;</p><p>据美国科技公司薪酬、福利数据收集网站Levels.fyi数据显示，英伟达的产品经理（总共八个层级中的第三层级）每年平均可获得 77700美元的股票收入。</p><p>&nbsp;</p><p>根据Finlo 的投资计算器和《企业家》网站统计，2019 年收到的 77700 美元的股票赠与的价值如今已经超过 160 万美元——这还不包括近年来累积的股票红利的价值。</p><p>&nbsp;</p><p>按照同样的算法：假设他们都在五年前加入，那么入门级软件工程师将获得近 50 万美元，高级解决方案架构师将获得 130 万美元，四级数据科学家仅从最初的股票奖励中就能获得 200 万美元。不仅仅是高管，甚至中层管理人员的年薪也超过 100 万美元。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/5d/5d26ebad7fdf444a3e9b45455a124495.png" /></p><p></p><p>英伟达各级别产品经理薪酬，更新日期：2024年7月1日</p><p>&nbsp;</p><p>英伟达从生成式 AI 的繁荣中获益最多。其数据中心 GPU 和相关 AI 产品的销售将 Team Green 的市值推高至 1.19 万亿美元。尽管让员工因公司的成功而变得富有似乎是件好事，但不好的一面也随之而来——坐拥巨额财富也让其中一些员工感到自满。这些在英伟达工作了许多年的老员工们看到他们的股票期权和 RSU（限制性股票单位）大幅升值，有可能使他们成为百万富翁后似乎没有以前那么努力工作了。</p><p>&nbsp;</p><p>据报道，许多资深的英伟达高管和中层管理者现在处于“半退休”状态，这种情况让其他英伟达员工感到恼火。</p><p>&nbsp;</p><p>一位年薪 25 万美元、常驻西海岸的英伟达工程师向《商业内幕》分享了自己的观点。他解释说，尽管英伟达员工的薪水乍一看很可观，但并不一定能转化为长期财富。虽然看起来所有英伟达员工都在从公司的成功中获益，但现实情况却有所不同。</p><p>&nbsp;</p><p></p><blockquote>这位工程师以 RSU 的形式获得了近一半的基本工资，他指出，并不是每个人都能获得大量股票单位。员工可以获得的 RSU 数量是有上限的，即使是表现最好的员工，每年获得的股票也只能相当于基本工资的 50%。</blockquote><p></p><p>&nbsp;</p><p>他说：“你最终会将股票兑现，以履行年度个人所得税、财产税和其他任何费用义务。”这一现实凸显了一个重要观点：对许多员工而言，并没有吃到英伟达飞速发展的红利。</p><p>&nbsp;</p><p>英伟达员工的经历在科技行业并非独一无二。正如特斯拉前人工智能总监 Andrej Karpathy 所说，“大多数人不会持有股票，美国政府拿走了一半。”这种情绪反映了英伟达和特斯拉等公司的员工面临的更广泛挑战。虽然成为百万富翁的潜力是真实存在的，但许多员工最终还是会提前出售股票以满足眼前的财务需求和偿还债务。</p><p>&nbsp;</p><p>随着内部不公平现象愈演愈烈，去年年底，老黄不得不在内部全体会议上提及了外界质疑的“英伟达高管半退休”状态的问题。</p><p></p><h2>竞争日益加剧，黄老板暗示老员工“卷起来”</h2><p></p><p>&nbsp;</p><p>接受《商业内幕》采访的与会者称，黄仁勋在回答有关资深员工不尽职的问题时表示，在英伟达 工作就像一项“自愿运动”，每位员工都应该像自己时代的“CEO”一样行事。他补充说，每个人都应该确定自己的工作水平，因为这些都是成年人的判断。</p><p>&nbsp;</p><p>其中一名在场人员对《商业内幕》表示：“黄老板正在严肃地强调，‘做好你的本职工作’。”</p><p>&nbsp;</p><p>老黄在会上强调了个人责任和职业道德的重要性，他传达的信息很明确：创新和卓越的动力必须保持强劲。</p><p>&nbsp;</p><p>之所以如此着急整顿企业文化，是因为他看到了AI芯片市场日益竞争的市场环境。</p><p>&nbsp;</p><p>尽管英伟达目前毫无争议地占据了 AI 芯片市场的主导地位，狂揽了超过80%的市场份额，但竞争也愈演愈烈。英特尔和AMD等老牌科技巨头以及Etched、Cerebras和D-Matrix等新兴初创公司都在争夺价值数十亿美元的高利润空间。</p><p>&nbsp;</p><p>据报道，英伟达目前约 40% 的收入来自四家公司：微软、Meta、亚马逊和 Alphabet。所有这些公司都有能力在未来某一天完全自主开发 AI 芯片。</p><p>&nbsp;</p><p>也就是说，英伟达的现有客户有一天可能会成为其最大的竞争对手。</p><p>&nbsp;</p><p>黄仁勋也在前不久的股东大会上谈到了竞争威胁，但没有特别点名任何竞争对手。在回答股东问题时，他说英伟达的策略是制造“总拥有成本最低”的 AI 芯片。</p><p>&nbsp;</p><p>这五个字并不一定意味着英伟达的芯片是市场上最便宜的，其每块芯片的价格高达3万美元。相反，当潜在客户考虑性能、运行芯片的成本及其更广泛的影响力时，英伟达的芯片总体上可以呈现出“最低的总成本”。</p><p>&nbsp;</p><p>黄仁勋在接受CNBC 采访时表示：“NVIDIA 平台可通过各大云提供商和计算机制造商广泛使用，为开发人员和客户创造了庞大且具有吸引力的安装基础，这使得我们的平台对客户更有价值。”</p><p>&nbsp;</p><p>事实上，英伟达的芯片已经存在 30 年了，但直到最近，它们才被用作<a href="https://www.gamesradar.com/hardware/desktop-pc/your-nvidia-graphics-card-will-soon-be-able-to-help-you-when-youre-stuck-in-games/">显卡</a>"。</p><p>&nbsp;</p><p>黄仁勋相信这些芯片可以做更多的事情。2016 年，他要求他的团队使用这些芯片构建一个 AI 服务器，最终这个服务器像公文包一样大，制造成本为 129,000 美元。然后他把这个服务器作为礼物亲手交给了 OpenAI。</p><p>&nbsp;</p><p>目前，数以万计的英伟达芯片为OpenAI 的 ChatGPT提供支持。</p><p>&nbsp;</p><p>黄仁勋在会上强调，英伟达在人工智能芯片方面占据先机，因为该公司十年前就开始投资这项技术，投入了数十亿美元，并招募了数千名工程师参与研发。</p><p></p><p></p><h2>老板不裁员是员工“躺平”的主要原因吗？</h2><p></p><p>&nbsp;</p><p>与黄老板对于外部竞争的焦虑形成对比的是英伟达内部员工们对于外部环境“一片祥和”的主观判断。</p><p>&nbsp;</p><p>不少躺在”功劳簿“上的英伟达老员工认为，目前英伟达面临的外部竞争不足。这也是他们认为没有必要努力工作的原因之一。“我们没有竞争，”其中一位知情人士说。“但我们正慢慢变得臃肿。有些人什么都不做。”</p><p>&nbsp;</p><p>另一个让他们“躺平”的原因是因为老黄是一位不爱裁员的老板。没有哪位 CEO 像黄仁勋一样深受员工爱戴。他在去年 10 月份最受欢迎的 CEO调查中名列榜首，支持率高达 96%，比排名第二的沃尔玛老板道格·麦克米伦高出 8%。黄仁勋之所以受欢迎，是因为他不愿裁员。去年夏天，当英伟达未能实现盈利预期，经济形势更加糟糕时，黄仁勋向员工保证，公司会加薪，而不是裁员。该公司上一次正式裁员是在 2008 年金融危机期间。</p><p>&nbsp;</p><p>虽然这种行为能激发员工对老板的忠诚度，提高员工的幸福感，但也会带来意想不到的问题。“在这里，被解雇比被录用更难，”其中一位知情人士说。</p><p>&nbsp;</p><p>一些长期在英伟达任职的员工可能会因为公司的成功而变得懒惰，但黄仁勋肯定不会放松警惕。他最近承认，他一直担心公司有一天会倒闭——英伟达过去曾多次濒临破产。</p><p>&nbsp;</p><p>不得不承认的事实是，英伟达许多老员工如今仍然可以在“半退休”模式下看着自己的股票价值不断上涨。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.entrepreneur.com/business-news/nvidia-long-term-employees-semi-retired-multimillionaires/476271">https://www.entrepreneur.com/business-news/nvidia-long-term-employees-semi-retired-multimillionaires/476271</a>"</p><p><a href="https://news.ycombinator.com/item?id=40826421">https://news.ycombinator.com/item?id=40826421</a>"</p><p><a href="https://www.hexmarkets.com/how-are-nvidia-employees-becoming-millionaires-with-a-semi-retirement-plan/">https://www.hexmarkets.com/how-are-nvidia-employees-becoming-millionaires-with-a-semi-retirement-plan/</a>"</p><p><a href="https://thedeveloperstory.com/2024/06/28/nvidia-is-suffering-from-success-despite-being-one-of-the-most-valuable-companies/">https://thedeveloperstory.com/2024/06/28/nvidia-is-suffering-from-success-despite-being-one-of-the-most-valuable-companies/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HGYTFfpeD4wmaKIoClcv</id>
            <title>全员降薪60%、300亿市值几乎跌成零！这个曾剑指英伟达的国产芯片公司被曝造假，业内评其“老鼠屎”</title>
            <link>https://www.infoq.cn/article/HGYTFfpeD4wmaKIoClcv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HGYTFfpeD4wmaKIoClcv</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 08:47:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全球半导体市场, 左江科技, DPU概念, 财务造假
<br>
<br>
总结: 全球半导体市场在人工智能、物联网、5G通讯数字化和智能化的浪潮下不断发展，左江科技曾是市值最高300亿的国产芯片公司，因财务造假被深交所退市，公司股价急剧下跌。左江科技在DPU概念股中一度赚得盆满钵满，但由于DPU产品交付问题导致公司陷入困境。左江科技在DPU领域缺乏深刻积累，面临着DPU产业商业化落地的挑战。 </div>
                        <hr>
                    
                    <p>全球半导体市场在人工智能、物联网、5G通讯数字化和智能化的浪潮下不断发展，众多本土芯片制造商如雨后春笋般崭露头角。然而，在这场激烈的商业竞争中，并非所有公司都能一帆风顺。有些企业凭借其卓越的表现赢得了声誉，而有些则因为决策失败、管理不善、经营混乱等问题走到了穷途末路。</p><p></p><p>因财务造假，市值最高300亿芯片公司宣布退市近日，在最新提交的监管文件中，左江科技宣布，其股票将于7月26日在深交所停止交易。此前，该公司未能为2023年财务业绩提交一份干净的审计报告，这促使深交所采取行动将其退市。</p><p></p><p>据悉，左江科技股票将自2024年7月8日进入退市整理期交易，预计最后交易日期为2024年7月26日。退市整理期满的下一个交易日，交易所将对公司股票予以摘牌。这家曾号称要“对标英伟达”、市值最高突破300亿元的国产芯片公司最终没能避免被淘汰的命运。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf4f0da692c43fcc25ef1b7073df4b44.png" /></p><p></p><p>左江科技成立于2007年，最初是一家网络安全应用硬件的设计、制造和销售商，后来左江科技自称主要从事信息安全领域相关的软硬件平台、板卡和芯片的设计、开发、生产与销售。2019年10月在创业板上市，随后连续“斩获”17个涨停，一度成为资本市场的“香饽饽”。</p><p></p><p>沾上DPU概念是公司股价“起飞”的主要原因。DPU是数据中心面向算力时代重构的关键芯片，被称为数据中心继CPU、GPU之后的“第三颗主力芯片”。</p><p></p><p>左江科技从2021年起不断披露DPU（可编程数据处理芯片）的研发进度，尤其是号称正在研发的NE6000性能可媲美NVIDIA Bluefield-2，称新DPU将于2022年下半年流片返回。</p><p></p><p>而恰逢那时以ChatGPT为代表的大语言模型产品爆火，人工智能服务器对专用芯片的需求飞涨。左江科技成为一时稀缺的DPU概念股，即使业绩下滑严重，但股价却一路上涨，在2023年7月一度涨到299.8元，公司总市值超300亿元。</p><p></p><p>本来局面一片大好，但左江科技却在DPU产品交付上出了问题。</p><p></p><p>据《21 世纪经济报道》报道，2022年12月27日，左江科技（*ST左江）和北京昊天旭辉科技有限责任公司（下称“昊天旭辉”）签署合同，2023年1月3日即完成交付400片“NE6000”系列DPU芯片，并在2023年1月确认合同收入1261万元。</p><p></p><p>但实际上，左江科技已卖出的上述DPU芯片，绝大部分正在仓库堆积。同时，该笔交易的终端用户巨贤科技法定代表人，还与左江科技董事长同名。也就是说，左江科技卖出的这些芯片，最终实际上又回到了自家仓库里。这一笔收入的商业合理性也被交易所发函质疑。</p><p></p><p>2024年1月30日，证监会通报了对左江科技财务造假案阶段性调查进展情况。证监会初步查明，左江科技2023年披露的财务信息严重不实，涉嫌重大财务造假。</p><p></p><p>自那时起，左江科技股价迅速跳水，截至其停牌前最后一个交易日，左江科技股价只剩6.94元，总市值7.08亿元，还有1.2万户股东，股价较去年7月的最高点已跌了97%。</p><p></p><p>今年5月7日，左江科技收到深交所退市告知书，根据《告知书》，深交所指出，左江科技2023年度经审计的净利润亏损2.23亿元，且扣除与主营业务无关的业务收入和不具备商业实质的收入后的营业收入为5217.27万元，同时公司2023年财务会计报告被出具无法表示意见的审计报告。触及深交所《创业板股票上市规则（2023年8月修订）》第10.3.10条第一款第一项、第三项规定的股票终止上市情形，深交所拟决定终止ST左江（左江科技）股票上市交易。</p><p></p><p></p><h2>全员降薪60%，业内人士：不看好</h2><p></p><p></p><p>自OpenAI在全球范围内掀起生成式AI热潮后，资本市场也对AI相关环节，包括AI软硬件基础设施青睐有加，最典型的就是英伟达凭借GPU在AI时代一骑绝尘，市值直冲2万亿美元。而左江科技也借着这股DPU东风赚得盆满钵满。</p><p></p><p>在资本加持下，左江科技交付了一款名为“NE6000”的DPU芯片。据左江科技官方微信消息，2022年11月，鲭鲨NE6000系列网络数据处理芯片（DPU）研制成功，NE6000是国内首颗可提供25G和100G接口能力的自主可控芯片，也是国内首颗拥有200Gbps的数据平面可编程的网络数据处理芯片。同时，左江科技还在回复2022年年报问询函时称，NE6000与国外同类产品的差异主要体现在芯片工艺不同，NE6000研制对标英伟达（Nvidia）2020年推出的上一代Bluefield2 DPU。</p><p></p><p>但不少业内人士对左江科技下场参与DPU产业的举动并不看好。</p><p></p><p>某DPU芯片公司技术专家Michael Liu在接受AI前线采访时表示：“研发一款DPU芯片需要投入的资金和时间成本都是巨大的，甚至每年需要投入近10亿元来做研发，左江科技在DPU领域没有深刻的积累，他们的基因也并非做DPU起家的，所以他们走到今天这一步并非偶然。”</p><p></p><p>Michael Liu介绍道，与CPU和GPU相比，DPU更像是个综合体，它集芯片、软件和云于一体，DPU是算网融合的关键组件，其中网中有算这件事情只有DPU可以做，这种负载类型CPU是无法处理的，因此DPU在当前的技术趋势下将会大有可为。</p><p></p><p></p><blockquote>Michael Liu也提到，尽管DPU前景乐观，但要做到大规模商业化落地还有两点挑战：第一点是成本问题，第二是软硬件的成熟度问题。“如果一颗DPU芯片卖5万块钱，做得再好都不太可能大规模商业化。现在DPU通常都不便宜，英伟达的DPU也很贵，要3000-4000美金以上。要想达到比较大规模的量产，在成本上还要进一步降低。此外，我们需要关注DPU的软硬件成熟度问题。DPU的发展是伴随着AI对算力基础设施的巨大需求而兴起。然而，AI对整个算力的需求仅仅是一个新兴的趋势。以前的数据中心并没有DPU的存在，但随着算力需求的兴起，算力基础设施系统结构正在从原来的网络加交换节点这种分布式结构，向“三U一体”（即计算、存储、网络）的结构演进，这也凸显了DPU的重要性。尽管这一趋势是正确的，但是对于大型芯片而言，期望在3到5年内就能达到成熟是不现实的，实际上可能需要5到10年的时间。这尚且是一个相对乐观的预测。DPU最初发布时，并没有预料到后面一年多时间内大模型的快速发展，对算力的需求增长如此之快，也许AI算力需求的快速增长会加速DPU的成熟。”</blockquote><p></p><p></p><p>可见，想做好一款DPU，并非一朝一夕的事。</p><p></p><p>值得一提的是，有左江科技内部员工向AI前线独家爆料，公司于今年年初曾告知员工，称自今年12月起执行全员降薪，所有员工只发40%的工资。</p><p></p><p>参考链接：</p><p>https://finance.eastmoney.com/a/202406293117426159.html</p><p>http://www.cinno.com.cn/industry/news/china-semi-investment2023</p><p>https://www.uxingroup.com/info/news-i03224i1.html</p><p>https://www.21jingji.com/article/20231214/herald/f1b6612da523ae03313da65e692b0b5e.html</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4bkJqv7M5UWTiTMHQNep</id>
            <title>挖矿不行了找AI接盘！挖矿公司们来抢云厂商生意：收入涨10倍，今年的算力早就卖完了！</title>
            <link>https://www.infoq.cn/article/4bkJqv7M5UWTiTMHQNep</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4bkJqv7M5UWTiTMHQNep</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 07:04:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 比特币矿工, CoreWeave, 英伟达
<br>
<br>
总结: 人工智能技术的发展催生了比特币矿工企业的转型，其中CoreWeave成为了人工智能云计算领域的领导者。通过与英伟达合作，CoreWeave成功转型为云服务提供商，为高性能计算需求的特定客户群体提供服务，吸引了多家投资方的支持。其成功转型和发展展示了人工智能技术对于传统行业的影响和改变。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>挖矿公司突然成为交易中心，催化剂是人工智能。</blockquote><p></p><p>&nbsp;</p><p>随着AI厂商疯狂提升产品智能性与实用水平，他们对于低成本、高供应量能源的需求也在同步猛增。而这股淘金热的升温，又给一批意料之外的受益者带来了巨额利润：比特币矿工。</p><p>&nbsp;</p><p>“不少身陷困境的加密货币矿场开始全面投身其他行业，这恐怕已经成为必然。”数据中心及比特币挖矿公司IREN 首席商务官Kent Draper说道。</p><p>&nbsp;</p><p>最近几个月来，各主要比特币挖矿公司已经开始将部分计算设备更换成用于运行和训练AI系统的硬件。这些公司认为，与动荡不断的加密货币行业相比，AI训练能够提供更安全、更稳定的收入来源。</p><p>&nbsp;</p><p></p><h2>典型代表 CoreWeave 的惊人崛起</h2><p></p><p>&nbsp;</p><p>从早期一个默默无闻的加密货币挖矿公司摇身一变成为人工智能云计算领域的领导者，CoreWeave 借势完成了华丽蜕变。</p><p>&nbsp;</p><p>2016 年时候，三位商品交易员Michael Intrator、Brian Venturo 和 Brannin McBee 在曼哈顿的一间办公室里开始了他们的小爱好：购买了一块性能一般的 GPU 来挖以太坊，希望能偶尔“赚个外快”。</p><p>&nbsp;</p><p>得到好处的三人从身边朋友拿到了一些小额早期投资，把挖矿地点从台球桌变成了新泽西州的一个车库（数据中心）。不久之后，他们决定创业，CoreWeave的前身Atlantic Crypto正式成立。</p><p>&nbsp;</p><p>作为挖矿企业，他们的核心生产资料就是GPU。2019年左右的加密寒冬让不少挖矿企业倒闭，他们趁机抄底显卡，从拥有几百张显卡一下变成了有数万张，数据中心也增加到了七个，占以太坊网络总量的1%以上。</p><p>&nbsp;</p><p>在加密寒冬中，他们一方面尝试为其他加密矿工提供GPU云服务器，同时也发现了一项新“需求”：大量依赖GPU加速的企业找到他们，希望他们提供算力支持。这些企业都有一个共同的痛点：传统云服务提供商提供有限的算力选项，同时垄断价格，让大规模的业务扩展变得非常困难。</p><p>&nbsp;</p><p>这家挖矿企业的转型之路其实并不算太波折，因为背后有贵人“英伟达”相助。</p><p>&nbsp;</p><p>2019年，CoreWeave转型做IaaS，并将消费级GPU全面转向英伟达的企业级GPU。2020年，CoreWeave宣布加入英伟达合作伙伴网络计划，成为“算力黄牛”。直到2022年，大规模显卡挖矿时代结束，CoreWeave 彻底转型成为一家云服务提供商，并在11月成为首批提供采用英伟达 HGX H100超级芯片的云服务商之一。</p><p>&nbsp;</p><p>随着微软支持的OpenAI于2022年11月推出席卷全球的ChatGPT，整个世界对于AI计算的巨大需求也被随之点燃。</p><p>&nbsp;</p><p>为了把握机会，该公司迅速扩大了融资力度。CoreWeave在2023年上半年通过股权融资拿到超过4.2亿美元，几个月后又通过债务融资筹集到23亿美元。部分原股东则在去年12月向富达等企业出售了价值6.42亿美元的股票。5月份，他们再次达成两笔交易，分别以债务和股权形式筹集到75亿美元和11亿美元。</p><p>&nbsp;</p><p>2023年4月，CoreWeave 还获得了来自英伟达的2.21亿美元B1轮融资。8月，CoreWeave 将英伟达 H100作为抵押品，获得了另外 23 亿美元的债务融资，资金将用于收购更多芯片，以及建设更多数据中心。</p><p>&nbsp;</p><p>Intrator表示，CoreWeave需要巨量交钱以便为“扩大业务规模，从而为任何想要投身于AI热潮的参与者提供支持”，也就是满足对方的一切芯片需求。</p><p>&nbsp;</p><p>CoreWeave如今的主营业务，就是出租其数据中心内运行着的大量英伟达芯片，包括大受欢迎的H100和即将推出的B200。该公司CEO Michael Intartor表示，CoreWeave的基础设施旨在满足高性能计算的特殊需求，包括用于连接AI芯片集群的调整网络以及算力强劲的液冷服务器。</p><p>&nbsp;</p><p>尽管CoreWeave的服务核心离不开对英伟达GPU的倚重，但Intrator强调，千万不要误解CoreWeave与这家全球最具价值芯片制造商间的关系。</p><p>&nbsp;</p><p>“英伟达之所以向我们赋予GPU使用权，绝不是因为他们能在这里攫取既得利益，也不是因为我们有什么优先级更高的门路。”Intrator表示，相反，CoreWeave的竞争优势也绝不仅仅体现在掌握GPU芯片上，例如CoreWeave开发出能自动管理并维护GPU集群的软件。</p><p>&nbsp;</p><p>他还曾回答关于一边公司从英伟达手中筹集资金，另一边却把大部分资金花在采购该公司产品上的问题。“情况并不是大家想象的那样。英伟达向我们投资了1亿美元，而我们通过债务和股权融资总计筹集到了120亿美元。与我们采购的基础设施规模相比，英伟达的注资额度显得微不足道。”</p><p>&nbsp;</p><p>英伟达则否认了其投资的公司能够优先拿到新款GPU产品。英伟达旗下风险投资部门NVentures负责人Mohamed Siddeek去年在接受英国《金融时报》采访时表示，“我们绝不会帮助任何人插队。”</p><p>&nbsp;</p><p>尽管如此，Intrator仍然承认，允许英伟达审查CoreWeave业务并决定投资，在对于这样一家年轻企业在市场上的资金筹集有着“非常重大的意义”。他指出，“我愿意回答英伟达提出的各种问题，因为他们比任何人都更了解我们在做什么、想做什么，也更愿意为此投入大量资金。”</p><p>&nbsp;</p><p>挖矿出身的CoreWeave如今早已远离加密货币。</p><p>&nbsp;</p><p>与亚马逊云科技和微软Azure一样，CoreWeave在采购和维护自有服务器之外，为企业客户们提供了一种新的替代选项，可实现对算力资源的灵活访问。</p><p>&nbsp;</p><p>但与2006年成立、面向几乎一切应用程序和数据需求的亚马逊云科技不同，CoreWeave的数据中心只服务于具有极高性能计算需求的特定客户群体，主要涵盖AI、药物研究和媒体集团等受众。</p><p>&nbsp;</p><p>CoreWeave的各位投资方，包括对冲基金Magnetar Capital、Blackstone和Coatue，也都坚信对于专业AI服务的需求飙升必将重塑整个价值达5000亿美元的云计算市场，有望在已经投入数百亿美元的各大科技巨头之间再开辟出一条新的赛道。</p><p>&nbsp;</p><p>Intrator指出，“下一代云计算的使用方式将与20年前云计算的使用方式截然不同。”他甚至将CoreWeave比作特斯拉，而传统科技巨头则类似于福特。</p><p>&nbsp;</p><p>在Intrator看来，向早期投资方推销这个观念“极其困难”，因为对方必须“在这个自己原本一无所知的领域内成为专家，才会愿意供出数十亿美元并将其交给投资委员会，最终创造出新的的资产类别”，例如将英伟达的图形处理单元视为新的抵押物。</p><p>&nbsp;</p><p>CoreWeave联合创始人和首席战略官Brannin McBee表示，Coreweave今年的收入会增长10倍，到2024年底的所有算力已经售罄。该公司现在有大约500名员工，年底将会接近800人。而其中很多需求是训练到推理的转换推动的，比如训练可能需要1万卡训练，但像ChatGPT这种一旦进入推理，则需要一百万张卡。</p><p>&nbsp;</p><p>根据 Omdia 数据，英伟达 H100 分配数量为：微软 Azure (15 万张)、Meta (15 万张)、亚马逊云科技 (5 万张)、谷歌云 (5 万张)、甲骨文 (5 万张)、腾讯 (5 万张)、百度 (3 万张) 和阿里巴巴 (2.5 万张)。但 CoreWeave 就有4 万张、Lambda 就有2 万张。此外，字节跳动 有2 万张、特斯拉 1.5 万张。</p><p>&nbsp;</p><p>根据 Intrator的计划，他可以利用GPU资产、与客户间签订的长期合同价值以及“经过验证的执行能力”等优势，成功说服贷方掏出数十亿美元。</p><p>&nbsp;</p><p>如今，CoreWeave正着眼于欧洲区域的快速扩张。该公司计划在明年年底之前投资22亿美元在挪威、瑞典和西班牙建设三处数据中心。该公司最近还承诺在英国投资13亿美元建设两处设施，并将英国作为其欧洲总部所在地。</p><p>&nbsp;</p><p>而为了在美国市场加速扩张，CoreWeave还与比特币挖矿公司Core Scientific建立了合作伙伴关系，将后者的多处数据中心转用于托管自己的GPU硬件。CoreWeave还提出以超过10亿美元的价码直接收购Core Scientific，但由于Core Scientific认为CoreWeave对其估值不合理作罢。</p><p>&nbsp;</p><p>Intrator表示，到2024年底，CoreWeave将在美国和欧洲等地坐拥28处数据中心，并计划在未来几年内“真正将业务足迹铺向全世界。”Intrator总结称，“我们将继续尽一切可能，加快规模扩张的步伐。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>AI厂商积极拉拢矿场</h2><p></p><p></p><p>除了 CoreWeave，还有不少比特币矿场开始将设施出租给AI客户。</p><p>&nbsp;</p><p>Core Scientific公司CEO Adam Sullivan在4月接受采访时指出，AI厂商正在积极出价拉拢比特币挖矿设施。“他们已经开始以高于加密货币市场的价码认购挖矿设施。”他同时补充道，AI厂商的申请数量“如雪片般飞来，我们也开始评估最合适的资产盈利方式。”</p><p>&nbsp;</p><p>也有一些比特币挖矿企业选择自主运营GPU。</p><p>&nbsp;</p><p>6月24日，比特币矿商Hut 8从Coatue Management处获得了1.5亿美元投资，用于建设AI基础设施。Hut 8在今年的<a href="https://hut8.com/2024/05/15/hut-8-reports-first-quarter-2024/">第一季度财报</a>"中表示，已购买了首批 1,000 块 Nvidia GPU，并与一家风险投资支持的 AI 云平台达成了客户协议。该公司CEO Asher Genoot 表示，预计今年下半年开始，公司将以每年约 2000 万美元的速度创收。</p><p>&nbsp;</p><p>在部分IREN的设施当中，用于AI训练和推理的GPU及ASIC（专为比特币挖矿提供动力的专用集成电路）正在并行运作。</p><p>&nbsp;</p><p>“我们认为这两项业务可以彼此互补，且分别对应完全不同的商业形态。比特币属于即时收益，但波动性更大。而AI业务则更依赖于客户——但只要有了稳定的客源，收益就能持续不断地稳定流入。” Draper 解释道。</p><p>&nbsp;</p><p>Bit Digital 则截至 4 月底已经拥有 251 台服务器，该公司表示，当月从其第一份 AI 合同中获得了约 410 万美元的收入。Iris Energy 预计其 AI 云服务每年可带来 1400 万至 1700 万美元的收入。</p><p>&nbsp;</p><p>据 CoinShares 消息，Bit Digital 27% 的营收来自人工智能；Hut 8 6% 的销售额来自人工智能；在加拿大和瑞典设有数据中心的 Hive 则有4% 的营收来自人工智能服务。</p><p>&nbsp;</p><p>摩根大通6月24日的报道指出，截至目前，这种转变也受到了投资者们的热烈欢迎，这导致14家主要比特币挖矿公司的市值自6月初以来猛增22%，达到40亿美元之巨。</p><p>&nbsp;</p><p>不过，转向人工智能并不像重新利用现有基础设施和机器那么简单，因为人工智能要求的高性能计算 (HPC) 数据中心、数据网络等与挖矿设备ASIC不同，ASIC几乎也不能用于做其他事情。</p><p>&nbsp;</p><p>“除了变压器、变电站和一些开关设备外，矿工目前拥有的几乎所有基础设施都需要推倒并从头开始建造，以适应 HPC。”Needham 分析师在 5 月 30 日的一份报告中写道。</p><p>&nbsp;</p><p>Needham 估计，HPC 数据中心的资本支出为每兆瓦 800 万至 1000 万美元（不包括 GPU），而比特币挖矿的资本支出通常为每兆瓦 30 万至 80 万美元（不包括 ASIC）。</p><p>&nbsp;</p><p>不过，很多挖矿公司们至少目前表示要将比特币挖矿基础设施转换为 HPC 数据中心。</p><p>&nbsp;</p><p>“改造是可行的，因为该公司拥有并控制其所有的数据中心基础设施。”Core Scientific CEO Adam Sullivan 说道。他曾向 CNBC 表示：“看待比特币挖矿设施最好的方式是，我们本质上是数据中心行业的电力外壳。”</p><p>&nbsp;</p><p></p><h2>历时多年的转变</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p>考虑到双方的需求，AI与比特币挖矿产业之间的携手可说是一拍即合。AI厂商需要比特币矿场已经成型的土地空间、廉价能源与基础设施；比特币矿场则看重AI计算的收入稳定性，以及当前AI炒作周期带来的巨大潜在利润。</p><p>&nbsp;</p><p>这种转变也反映出当下的几个趋势：AI技术的炒作热度飙升，电力供应减少，而比特币产量减半后挖矿业务的前景则逐渐势微。</p><p>&nbsp;</p><p>事实也证明，相当一部分设施其实就在比特币矿场们的掌握之中。</p><p>&nbsp;</p><p>在比特币诞生之初，矿工们发现增加计算机设备的规模能够大大增加自己的利润，并因此建立起巨大的服务器农场，利用廉价能源日夜运行。从历史上看，大规模开采比特币曾经是项利润丰厚的业务，但也同样受制于动荡不断的加密货币行情。</p><p>&nbsp;</p><p>在2022年加密货币崩盘之后（这场大崩盘由Sam Bankman-Fried及Do Kwon等企业家的冒险行为所引发），许多矿场已经被迫破产或者彻底关门。但在崩盘当中幸存下来的挖矿公司，很快在2023年到2024年初重新回到盈利的正轨之上。但今年4月新的挑战接踵而至：比特币宣布名“减半”（矿工奖励减少 50%），直接将矿场的挖矿产出削减了一半。</p><p>&nbsp;</p><p>挖矿公司指望着产出减半能够拉动比特币价格大幅上涨，就如同之前加密货币曾经出现的好几轮爆发周期一样，从而抵消这种奖励缩水。但自4月以来，比特币的价格基本横盘不动、挤压了利润空间，迫使矿工们只能寻求更加多样的商业化探索。</p><p>&nbsp;</p><p>以ChatGPT为代表的生成式AI模型凭借数据中心内强大的计算能力而得到改进，这里的基础设施负责从海量数据集内寻找模式并改进响应效果。但由于算力资源太过昂贵，多年以来对于大部分数据中心运营来说，专门为AI训练部署硬件似乎并不划算。</p><p>&nbsp;</p><p>直到四年之前，Draper仍然认为“从商业角度来看，目前的规模效应还不足以带来合理收益。”</p><p>&nbsp;</p><p>但2022年底ChatGPT取得的巨大成功改变了这一格局，其他AI厂商也开始竞相训练并运行自己的模型，希望在效能层面超越OpenAI推出的这位当家花旦。而这自然也对能源供应提出了极高要求：以ChatGPT为例，其处理查询的能耗就高达标准Google搜索的10倍。</p><p>&nbsp;</p><p>于是乎，一众AI厂商开始努力寻求更廉价的电力、能够容纳塞满数千台计算设备的大片数据中心建设土地，以及用于冷却设备的水或巨型风扇等资源。</p><p>&nbsp;</p><p>在旺盛的市场需求之下，符合这些标准的站点也变得越来越炙手可热，尤其是北美地区。一部分司法管辖区甚至开始为等待接入电网的大型数据中心整理出长长的队列名单。哪怕企业获得了初步批准，从头开始建设数据中心也可能需要数年时间、投入数百万美元，并经历漫长的监管和官僚程序。</p><p>&nbsp;</p><p>比特币挖矿公司Terawulf首席运营官兼首席技术官Nazar Khan表示，“把时间倒回五到十年前，当时80%的数据中心负载都来自六到七个主要市场。这部分供应能力已经被占满，部分市场甚至暂停了数据中心的进一步建设工作。因此，新的数据中心负载只能寻找新的容身之所。”</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://time.com/6993603/ai-bitcoin-mining-artificial-intelligence-energy-use/">https://time.com/6993603/ai-bitcoin-mining-artificial-intelligence-energy-use/</a>"</p><p><a href="https://www.cnbc.com/2024/06/03/bitcoin-miners-sink-millions-into-ai-business-seek-billions-in-return.html">https://www.cnbc.com/2024/06/03/bitcoin-miners-sink-millions-into-ai-business-seek-billions-in-return.html</a>"</p><p><a href="https://www.ft.com/content/f4085e30-da81-40f0-8217-507268743f71">https://www.ft.com/content/f4085e30-da81-40f0-8217-507268743f71</a>"</p><p><a href="https://www.nextplatform.com/2024/05/02/how-to-make-more-money-renting-a-gpu-than-nvidia-makes-selling-it/">https://www.nextplatform.com/2024/05/02/how-to-make-more-money-renting-a-gpu-than-nvidia-makes-selling-it/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vp8e05v8LKBsXQFkAv0i</id>
            <title>没有千亿级也没有百亿级，ToB大模型如何挖掘不足1%的企业数据的价值？</title>
            <link>https://www.infoq.cn/article/vp8e05v8LKBsXQFkAv0i</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vp8e05v8LKBsXQFkAv0i</guid>
            <pubDate></pubDate>
            <updated>Tue, 02 Jul 2024 02:31:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 模型, 大模型, 数据可信, IBM
<br>
<br>
总结: 文中讨论了大模型在企业应用中的挑战，包括海量公开数据与企业内部数据的对比、精准度要求、数据可信性等问题。IBM提出了解决方案，包括选择可信的基础模型、融合企业内部数据、构建企业级AI能力等步骤。IBM还开源了与企业业务相关的模型，以及不迷信模型大小的理念，提供不同尺寸规模的模型适用于不同场景。IBM还推出了大规模对齐技术LAB，通过合成数据生成和指令微调，让模型更适用于企业业务场景。 </div>
                        <hr>
                    
                    <p>模型是对数据的表达，而“大模型”的关键突破就在于数据量之“大”。目前<a href="https://aicon.infoq.cn/2024/shanghai/">业界主流大模型</a>"，参数规模均达到上千亿。然而，与这些海量公开数据形成鲜明对比的，是比例还不足1%的企业内部数据。和ToC应用不同，企业落地大模型的挑战之一，就在于如何把这些内部数据的价值充分挖掘出来。</p><p></p><p>与此同时，当前大模型框架多是“一刀切”，即“一个模式打天下”，并且无法解释背后的数据来源和训练逻辑。但企业应用对精准度的要求极高，交易数字上的一个小数点、零件上的螺丝钉个数、医生的用药剂量......一旦出现偏差，就会酿成巨大的事故隐患。</p><p></p><p>所以，对于企业的另一重挑战是：<a href="https://www.infoq.cn/article/TSJbOkWweAvFh44Oo2dL">如何确保数据可信</a>"，如何确定哪些模型值得信赖，怎么选择最能满足自身独特需求的生成式AI解决方案。</p><p></p><p>以上这一系列阻碍往往导致企业无法充分地实施和扩展AI技术，并且让技术真正为业务赋能。</p><p></p><p>对此，作为AI“初代玩家”的IBM在今年Think大会上给出了它的解决方案——具体分三步：第一，选择一个可信的基础模型；第二，在保持大模型本身的通用性能前提下，让企业这1%的内部数据更好地融合到模型中去，充分挖掘其价值；第三，在大模型基础之上构建企业级的AI能力，让企业所有的业务流程都得到大模型的加持。</p><p></p><h3>所有数据和模型都经过充分验证</h3><p></p><p></p><p>据IBM中国系统开发中心CTO孟繁晶在日前接受InfoQ等媒体采访时介绍，在基础模型层面，IBM开源了Granite模型系列中的18个与企业业务发展息息相关的模型，涉及编码模型、实训数据模型、语言模型、空间地理信息模型等等。目前，这些模型都可以在HuggingFace和GitHub找到。</p><p><img src="https://static001.infoq.cn/resource/image/37/bf/37380d659d397147e0987d8f7871c7bf.jpg" /></p><p>IBM中国系统开发中心CTO 孟繁晶</p><p></p><p>“所有这些模型背后的数据都是经过IBM在实验室里充分验证过的，我们把所有的数据和模型评估之后，再开源出来，希望可以跟社区开发者们一起去共建一个可信的基础模型，去构建可信的人工智能能力。”孟繁晶表示。</p><p></p><p>比如，<a href="https://aicon.infoq.cn/2024/shanghai/track/1708">在数据处理方面</a>"，IBM 构建了一个来自学术界、互联网、企业（例如金融、法律）和源代码的非结构化语言数据的大数据集。该预训练数据集是替代开源数据集而创建的专有数据集，开源数据集因包含有毒、有害或盗版内容而受到批评。通过构建 IBM 预训练数据语料库解决以上提到的这些问题和其他隐含问题。</p><p></p><p>同时，该预训练数据集仍在不断发展和优化，其他数据会定期审查并考虑添加到语料库中。除了增加预训练数据的大小和范围外，还会定期生成和维护这些数据集的新版本，以反映增强的过滤功能（例如，重复数据删除以及仇恨和脏话检测）和改进的工具。</p><p></p><p>举例来说，在 granite.13b 进行预训练时，IBM 在预处理之前收集了 6.48 TB 的数据，在预处理后构建了 2.07 TB 的训练数据。而 granite.20b.code 在预处理后构建了 100 多种不同编码语言的 1.6T 的训练数据，包括 Cobol 和 Ansible。</p><p></p><p>再比如，在模型训练方面，Granite严格遵循以下三个阶段：</p><p></p><p>第一阶段预训练过程，granite.13b 基础模型经过 30 万次迭代训练，批量大小为 4M 个 Token，总共 1 万亿个 Token，预训练让大模型根据输入生成文本；</p><p></p><p>第二阶段监督微调过程，使用来自不同来源的数据集混合执行监督微调，每个示例都包含一个提示和一个答案，执行3个周期获得 granite.13b.instruct 模型；</p><p></p><p>第三阶段对比微调过程，惩罚来自负数据分布的数据点概率，同时增加来自正数据分布的数据点的概率。换句话说，Granite不鼓励大模型为每个训练提示生成错对齐的答案（例如有害的答案），同时鼓励对齐的答案（例如有用的答案）。通过防止模型输出出现幻觉和错位，最后获得 granite.13b.chat 模型。</p><p></p><h3>不迷信模型“大力出奇迹”</h3><p></p><p></p><p>值得一提的是，IBM一直不迷信模型“大力出奇迹”。</p><p></p><p>对于企业而言，很多应用场景的落地并<a href="https://www.infoq.cn/article/VrUUu7ClZjWqhCud3wOg">不在于模型本身大小</a>"，而在于多大程度符合业务发展要求，能不能很好地完成任务。换言之，企业任何技术投入都是以驱动经营效率为目的的。但模型越“大”成本投入也越大，支持一个大模型的训练和运行非常消耗算力、电力等资源，并且在模型上线之后，企业业务本身仍然在不断变化，这要求模型具备适应性和可扩展性，系统能力也要不断学习和进化。所以出于运维成本的考虑，很多时候“小”模型反而比“大”模型更加节约且灵活。</p><p></p><p>针对这一问题，IBM发布了不同尺寸规模的模型，从3B、8B、24B到32B，适用于企业不同场景。而在IBM watsonx平台中同样不仅有大模型，还保有传统的机器学习模型。“比如SVM（支持向量机）做知识分类效果非常好，那就没有必要用大语言模型。”孟繁晶举例。</p><p></p><p>有了基础模型之后，接下来就是解决数据融合的问题。通常来说，企业会采取两种模式：第一，通过外挂向量数据库进行查询；第二，进行参数微调。但是，微调一般是黑盒操作，要做到大批量处理并且结果可控难度非常大。</p><p></p><p>对此，IBM实验室推出了LAB（ Large-scale Alignment for chatBots，大规模对齐技术）。“首先，把企业数据基于知识和技能进行两种不同表达，知识包括不同行业特定的知识信息，技能就是我们希望它完成的任务；然后，基于大模型进行合成数据生成，并把其中包含偏见、错误等误差数据清洗掉，实现合成数据验证；最后，再进行指令微调，让模型更适用于企业业务场景。”</p><p></p><p>孟繁晶表示，该理念通过IBM与红帽共同开源的InstructLab项目已经在GitHub等社区对外开放，并且整个过程通过对话方式就可以实现。</p><p></p><p>“基于InstructLab，每个人的贡献在社区都能看见，大家一起共创一个世界级的知识合集和技能合集，所有人可以用它选择自己想要的模型并对它进行微调，最终得到的结果不管做多少次迭代都不会出现偏差，这对于解决大模型的‘幻觉’问题特别重要。”</p><p></p><p>在IBM看来，基础模型的前景在于其能够根据企业独特的数据和领域知识进行调整，并以管制和灵活性为核心，从而使AI部署的可扩展性、经济性和效率大大提高。</p><p></p><h3>让AI应用更有ROI</h3><p></p><p></p><p>除了灵活的模式选择之外，企业还需要安全访问与业务相关的数据。通常企业在采用生成式AI时有三种模式：第一种是采用嵌入了生成式AI的软件；第二种是通过 API 调用查询AI模型；第三种是利用公开数据和私有数据创建（然后查询）自己的基础模型。</p><p></p><p>而为了确保数据源的可信，以及模型上线后可以实时监控，IBM watsonx还提供了一套完善的治理体系，包括了数据、AI和治理三个套件。这意味着，模型在上线后一旦出现偏差，就可以马上对其进行干预。</p><p>孟繁晶向InfoQ记者强调，这个平台并不会绑定任何一个模型，既可以调用IBM Granite模型，也可以调用开源模型或者其它第三方模型。“IBM更多是给企业提供一个平台能力，把企业所需的数据、AI及其治理能力都放到这个平台上，这是我们区别于其它大模型产品的定位。”</p><p></p><p>以IBM watsonx.ai为例，其支持多种基础模型并提供 watsonx.ai studio （开发平台），以帮助企业利用基于可信数据集和AI管制的基础模型来开发、微调和部署其AI应用。</p><p></p><p>无论企业是想微调开源模型、创建自己的模型，还是在本地或云端部署AI，IBM 都致力于为各行各业的新一代企业提供支持，将AI嵌入其战略核心，并且让AI技术的投入更具有ROI。</p><p></p><p>“再举一个例子：从工程化的角度来看，国内很多企业想要做一个定向的模型，（供应商）就需要花很大的代价开发出来一个功能。虽然IBM AI For Business也在做这件事，不过方法有所不同。不是说企业要做代码转换我们就成立一个上百人的团队，开发一个代码转换模型，而是基于Granite基础模型、InstructLab和watsonx，在这套方法和能力框架上，帮助企业快速地生成很多个这样的功能模型。”IBM中国科技事业部汽车行业总经理许伟杰告诉InfoQ，这就是IBM style，“不是赶快做出东西来给客户用，而是把这个东西先想好了、想清楚了再一个个做。”</p><p></p><p>目前，IBM已经把自己在AI层面的这些技术能力赋能到IBM云上。总结而言，其云平台具备三大特点：第一，AI ready，可以帮助客户通过基础模型进行数据训练，并且保障模型的可管理性和透明性；第二，不管是云上、云下还是边缘，都可以随时随地调用相关模型；第三，合规和安全。</p><p></p><p>举例来说，前文提到的Granite、InstructLab等开源的AI能力，以及企业级AI平台watsonx都可以在IBM云平台上单独下载使用。同时，如果需要做积量的训练或者更多的量化数据训练，在IBM云上可以通过 HPC（高性能计算）的云服务实现。</p><p></p><p>据了解，目前IBM与英伟达、英特尔和AMD等厂商都在GPU资源使用方面达成了合作协议，从而保障充足的算力，为企业提供持续的AI服务。</p><p></p><h4>活动推荐</h4><p></p><p><a href="https://aicon.infoq.cn/2024/beijing">AICon 全球人工智能开发与应用大会</a>"将于 8 月 18 日至 19 日在上海举办，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30&nbsp;日前可以享受&nbsp;8&nbsp;折优惠，单张门票节省 960&nbsp;元（原价 4800&nbsp;元），详情可联系票务经理 13269078023 咨询。</p><p><img src="https://static001.geekbang.org/infoq/f1/f1d06e1c7f30e0f58123c07a21cdc1de.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/If35pplXc2AkoJEWB0Sm</id>
            <title>金融风控等场景的大模型应用，核心系统的国产化实践...工银科技、平安壹钱包、华泰证券等确认出席FCon</title>
            <link>https://www.infoq.cn/article/If35pplXc2AkoJEWB0Sm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/If35pplXc2AkoJEWB0Sm</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 11:52:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2024年FCon全球金融科技大会, 科技驱动, 数字金融内生力, 大模型应用
<br>
<br>
总结: 2024年FCon全球金融科技大会将在上海举办，以科技驱动和数字金融内生力为主题，聚焦金融行业在数智化的全面革新，分享大模型应用的实践经验。 </div>
                        <hr>
                    
                    <p>8月16日-17日，<a href="https://fcon.infoq.cn/2024/shanghai/">2024年FCon全球金融科技大会</a>"将在上海举办，本届大会由中国信通院铸基计划作为官方合作机构，以“科技驱动，智启未来——激发数字金融内生力”为主题。在“十四五”收官之际，本届大会将致力于展示金融数字化在“十四五”期间的关键进展，帮助金融机构更具针对性地“查缺补漏”。同时，聚焦金融行业在数智化的全面革新，紧跟当下技术热点，分享近一年来金融行业&nbsp;AI&nbsp;大模型的落地实践经验和成果。</p><p></p><p>截止目前，大会已上线23个演讲议题，上周共确认8位演讲嘉宾，他们分别来自工银科技、嘉银科技、平安壹钱包、度小满、国投证券、某股份制银行、华泰证券、天弘基金等机构，将在FCon大会上分享金融风控等场景的大模型应用，以及核心系统的国产化实践等话题。</p><p></p><h4>演讲主题：人工智能技术在金融科技领域的应用探索</h4><p></p><p><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6029">工银科技技术总监孙科伟</a>"将在<a href="https://fcon.infoq.cn/2024/shanghai/track/1683">「金融大模型应用实践和效益闭环」专题</a>"介绍AI技术的主要技术路线，并结合实践，阐述AI在金融科技领域的应用探索。</p><p></p><p>孙科伟是工银科技数字金融实验室人工智能牵头人，负责研究规划制定，研究课题落实及技术产品赋能。主要学术研究方向为自然语言大模型、时间序列分析、音视频技术，并结合场景实现创新技术的落地实践。</p><p></p><p>演讲提纲：</p><p>金融行业人工智能技术发展路径金融行业人工智能应用创新金融行业人工智能前沿技术应用展望</p><p>听众受益：</p><p>可了解人工智能技术的发展和金融领域的前沿应用</p><p></p><h4>演讲主题：大模型在金融知识和作业密集型场景的挑战和实践</h4><p></p><p>据了解，在推进大模型落地金融行业实现赋能的大背景下，嘉银科技主要探索了大模型落地场景挖掘，包括在知识密集型、作业密集型（全员AI）场景的应用，例如ToB主流AI产品、职能单元助手、智能作业辅助等业务，最终实现了效益闭环与专家已知解和算法暴力求解的平衡。</p><p></p><p>在<a href="https://fcon.infoq.cn/2024/shanghai/track/1683">「金融大模型应用实践和效益闭环」专题</a>"，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6033">嘉银科技技术中心人工智能经理姜睿思</a>"将介绍具体的大模型落地过程，技术和方法论层面的实践经验。</p><p></p><p>演讲提纲：</p><p>1.大模型的落地场景</p><p>分析大模型在知识密集型场景的应用实例和成效探索大模型在作业密集型场景的落地挑战和解决策略面向B端的主流AI产品</p><p>2.介绍集团内ToB的AI产品如职能单元助手和智能作业辅助工具</p><p>分析这些产品的技术实现、市场接受度和业务影响构建效益闭环讨论如何通过专家知识和算法求解平衡来优化大模型的商业应用</p><p>3.描述效益闭环的构建方法，包括效益评估和持续优化过程</p><p>案例研究：具体案例分析，展示大模型在金融科技公司中的成功应用深入讨论案例中的逻辑闭环，建设闭环及产出闭环</p><p>听众受益：</p><p>确定组织AI战略，培养AI文化选择模型和工具，先进大模型显著特征验证管理机制推广和持续优化机制实现效益闭环</p><p></p><h4>演讲主题：大模型驱动的账户风险管理</h4><p></p><p>在金融科技的浪潮中，账户风险管理一直是金融机构关注的焦点。传统的人工驱动流程在处理复杂的欺诈案件时，不仅耗时且容易出错。随着大模型技术的兴起，我们有机会通过智能化手段，提高风险感知和风控决策的能力，从而降低人工失误率，提升运营效率。</p><p></p><p>围绕这一话题，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6031">平安壹钱包大数据研发部算法负责人王永合</a>"将在<a href="https://fcon.infoq.cn/2024/shanghai/track/1690">「金融数字化管理和运营实践」专题</a>"上深入探讨如何利用大模型技术，实现账户风险管理的数字化转型，以及这一转型如何为金融机构带来实质性的价值。</p><p></p><p>演讲提纲：</p><p>1.人工驱动流程的局限性</p><p>2.大模型技术在风险管理中的应用</p><p>3.方案思路与总体目标</p><p>4.应用场景详解</p><p>运营调查：事前、事中、事后的智能辅助风险侦测：全域感知与主动侦测策略迭代：风控策略的智能化迭代</p><p>5.整体框架与技术路线</p><p>6.创新点与成果成效</p><p>基于大模型实现强化学习实时决策建议的输出全流程生命周期闭环的实现</p><p>7.推广复用与业务普适性</p><p>跨平台管控能力数据预处理的简化大模型技术的快速适应性</p><p>听众受益：</p><p>对大模型技术在账户风险管理中应用的全面理解掌握如何通过数字化手段提升风控效率和准确性了解大模型技术在不同风险管理场景下的实际应用案例学习如何构建和优化风控策略，以适应不断变化的市场环境认识到大模型技术在金融科技领域的创新潜力和业务普适性洞察大模型技术如何帮助金融机构降低成本、提升服务质量，并增强竞争力</p><p></p><h4>演讲主题：计算机视觉技术在金融数字化风控中应用</h4><p></p><p><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6030">度小满金融数据智能部计算机视觉方向负责人万阳春</a>"目前主要负责计算机视觉技术的研发及金融场景应用落地。作为主要研究成员参与的《基于深度学习的人脸识别技术在信用风险防控领域的应用》项目曾获得银行业信息科技一类成果等级。</p><p></p><p>在他看来，数字化风控是金融行业的基石，安全与效率始终是其核心追求。在AIGC技术的浪潮中，逼真的AI生成内容对安全审核提出了前所未有的挑战；同时，金融数据的海量积累也对风控的智能化和效率提出了更高的要求。为应对这些挑战，度小满搭建了攻防对抗框架，不断迭代优化伪造检测系统，保障金融交易的安全性。同时，还通过文档智能技术方案，自动提取和解析金融文档中的关键信息，极大提升了数智化处理的效率。万阳春将在<a href="https://fcon.infoq.cn/2024/shanghai/track/1688">「前沿金融科技探索与应用」专题</a>"上围绕这一系列实践展开详细介绍。</p><p></p><p>演讲提纲：</p><p>数字化风控的发展现状数字化风控中的计算机视觉技术伪造检测技术在风控安全方面的应用文档智能在风控数智化转型方面的应用</p><p>听众受益：</p><p>熟悉数字化风控框架和计算机视觉前沿技术通过攻防对抗提升风控的安全可信度基于文档智能技术提升风控数智化水平</p><p></p><h4>演讲主题：从平台建设到常态化运营：券商的数据资产运营实践</h4><p></p><p><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6015">国投证券软件开发部数据平台负责人王环</a>"长期从事大数据架构设计、中台工具研发、数据仓库&amp;集市建模、数据治理、AI算法和数智应用建设，多年证券、互联网从业经验。曾就职于广发证券、腾讯、华为，参与多个大型人工智能和大数据应用、平台研发。</p><p></p><p>在<a href="https://fcon.infoq.cn/2024/shanghai/track/1691">「数据资产化运营与数据智能应用」专题</a>"上，他将从自身的经验和角度出发，介绍券商如何从平台建设开始，实现数据资产常态化运营。</p><p></p><p>演讲提纲：</p><p>1.背景</p><p>数据平台发展整体介绍数据架构数字化转型与数据资产的关系数据资产运营理念</p><p>2.数据资产内容体系建设</p><p>数仓集市标签画像</p><p>3.数据治理从理论到实践</p><p>建立数据治理体系数据资产盘点制定数据标准解决数据质量问题运营体会</p><p>4.数据资产常态化运营</p><p>数智应用数据服务赋能应用系统分析服务赋能数据驱动业务运营资产ROE评估数据归档与销毁</p><p>5.总结与展望</p><p>建设成果挑战探索</p><p>听众受益：</p><p>通过介绍证券公司业务场景、数据体系、数据架构，理解证券行业数字化转型与数据资产的关系，了解证券公司数据整体解决方案。通过介绍证券行业数据内容建设过程，深度掌握证券行业数据资产内容及建设方法通过具体实践案例分享、经验，全面了解数据治理方法论与实践技巧通过分享全生命周期的数据资产运营案例，掌握金融行业数据资产运营理念与方法论</p><p></p><h4>演讲主题：国产数据库的多维度探索与实践</h4><p></p><p>在<a href="https://fcon.infoq.cn/2024/shanghai/track/1686">「金融现代化核心系统建设与国产化实践」专题</a>"上，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6036">某股份制银行数据库专家王辉</a>"将分享其对于国产数据库的多维度探索与实践经验。具体从数据库的一个点展开，介绍与数据库关联的系统、存储，网络、架构、应用、产业的面，从而站在全局视角更全面地理解数据库及其周边生态的建设，更好地进行实施与优化，让数据库发挥最大效能，为业务赋能。</p><p></p><p>演讲提纲：</p><p>数据库的一个点数据库的一个面数据库的核心能力与业务赋能</p><p>听众受益：</p><p>通过不同维度深入的理解各个基础软硬件如何与数据库更好的整合了解数据库在整体架构中的位置与重要性，如何做到统筹规划设计了解目前数据库生态建设现状与发展，如何实现数据库的统一运维与管理</p><p></p><h4>演讲主题：事件驱动型微服务架构的实践</h4><p></p><p>此外，在<a href="https://fcon.infoq.cn/2024/shanghai/track/1686">「金融现代化核心系统建设与国产化实践」专题</a>"上，华泰证券FICC平台架构团队负责人毕成功还将分享<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6022">事件驱动型的微服务架构实践</a>"。</p><p></p><p>毕成功，2021年加入华泰证券，带领FICC平台架构团队，负责大象交易系统的平台架构工作。目前主要着力于建设具有“超低延时、内存计算、事件驱动”的金融型架构体系。在十余年的职业生涯中，致力于软件开发和团队管理工作，涉足过搜索、手游、O2O、电商、金融等多种领域，并有过多次创业经历。</p><p></p><p>演讲提纲：</p><p>1.经典微服务架构的问题</p><p>接口的快速膨胀上下游耦合性高、调用链路长</p><p>2.事件驱动型架构的方案</p><p>什么是事件驱动型架构事件的三种类型及其特征利用Local&nbsp;Cache来避免服务间QueryLocal&nbsp;Cache的启动恢复天然的CQRS模式流批一体的使用模式有状态服务高可用的两种实现方式</p><p>3.事件驱动型架构的问题</p><p>事务难以支持异步通讯更需要管理总线天生的集中式风险</p><p>4.总结</p><p>适用场景使用建议</p><p>听众受益：</p><p>对于经典微服务架构存在的普遍问题，找到一种不同的解决思路了解一整套事件驱动型微服务架构的实现方案，以及这些设计背后的思考理解这种架构适用的场景，并获得一些使用的建议</p><p></p><h4>演讲主题：天弘基金账务类核心系统的挑战和实践</h4><p></p><p><a href="https://fcon.infoq.cn/2024/shanghai/presentation/6023">天弘基金技术研发部高级架构师刘晓斐</a>"将在<a href="https://fcon.infoq.cn/2024/shanghai/track/1686">「金融现代化核心系统建设与国产化实践」专题</a>"分享天弘基金账务类核心系统的挑战和实践。</p><p></p><p>在其看来，金融核心系统有着很大的共性，天弘基金识别的Top2问题是系统的复杂性和不确定性。复杂性有着不同的来源，基于业务复杂度的难以消灭应该如何解决，基于一个系统持续熵增引发的如何进行治理。不确定性也有很多种，以风险的不确定性来看，没有任何人敢承诺负责的系统不出现风险事件，对此，其解决思路是“储蓄式架构”。目前的实践结果来看通过复杂度的治理和不确定性的对抗，能有效提升需求响应效率和系统稳定性。</p><p></p><p>演讲提纲：</p><p>核心系统面临的主要问题：复杂性和不确定性复杂性的治理方法，以及和恒生合作的行业级解决方案风险不确定性的对抗方法，如何构建储蓄式架构和其他辅助策略</p><p>听众受益：</p><p>金融核心系统形态以支付交易、账务、核算、清算等为主，介绍基金行业的账务系统面临的困难和挑战这次分享可以作为一次探索性的方案思路，互联网出身的同学可以感受一下金融业系统的特征，企业级背景的同学也可以思考针对目前复杂业务行业面临的挑战是否有传统方法以外的可行路径针对复杂度和不确定性的解题思路可能对同业都有一定的适用性</p><p></p><p>更多议题已上线FCon&nbsp;全球金融科技大会官网，来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。目前大会已进入9折优惠期，单张门票立省&nbsp;480&nbsp;元（原价&nbsp;4800&nbsp;元），欢迎点击链接或扫码查看了解详情：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ff5488cc076e04976f66fd5d9869c7.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/80adbf38a27f8a10c4e61bf3b</id>
            <title>PikiwiDB(Pika) 3.5 最佳实践</title>
            <link>https://www.infoq.cn/article/80adbf38a27f8a10c4e61bf3b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/80adbf38a27f8a10c4e61bf3b</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 09:53:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PikiwiDB, RocksDB, Redis, 性能优化
<br>
<br>
总结: PikiwiDB(Pika) 是 360 技术中台中间件团队基于 RocksDB 开发的大容量类 Redis 存储系统，通过持久化存储方式解决 Redis 在大容量场景下的问题。在使用过程中，需要注意线程数量和工作线程池数量的设置，以及与 IO 性能相关的硬件规格。此外，还需注意数据结构的设计和参数调整，以及避免单副本运行的情况。最新版本提供了一些性能优化的命令和建议，帮助用户提高系统性能和稳定性。 </div>
                        <hr>
                    
                    <p>PikiwiDB(Pika) 是 360 技术中台中间件团队基于 RocksDB 开发的大容量类 Redis 存储系统，力求在完全兼容 Redis 协议、继承 Redis 便捷运维设计的前提下通过持久化存储方式解决 Redis 在大容量场景下主从同步代价高、恢复时间慢、单线程相对脆弱、内存成本高等问题。</p><p></p><p>我们根据 360 内部的 PikiwiDB(Pika) 使用经验及社区用户的问题反馈，整理了本文并在这里分享给大家。</p><p></p><h1>之一</h1><p></p><p>在微信群（群管理员微信号：PikiwiDB）中提问时，请主动带上版本号，可大幅度加快问题解决速度。</p><p></p><h1>之二</h1><p></p><p>PikiwiDB(Pika)  已在 2024 年 5 月更新至 3.5.4，但仍然有大量用户停留在 3.3.6 或3.3.2，我们建议使用 3.5.4 的最新版（预计本周内发布 v4.0.0），你会发现你遇到的很多问题都在我们的 bug 修复列表中。</p><p></p><h1>之三</h1><p></p><p>PikiwiDB(Pika) 的线程数量 thread-num 建议设置为 CPU core 数目的 80% 左右，如果是单机多实例的部署，每个 PikiwiDB(Pika) 实例的线程数量可以酌情降低，但不建议低于 CPU core 数的 1/2。</p><p></p><h1>之四</h1><p></p><p>PikiwiDB(Pika) 的工作线程池数量 thread-pool-size 建议和 CPU core 数目一致，如果是单机多实例的部署，每个 PikiwiDB(Pika) 实例的线程数量可以酌情降低，但不建议低于 1/2 CPU core 数。</p><p></p><h1>之五</h1><p></p><p>PikiwiDB(Pika) 的性能和 IO 性能息息相关，如果对耗时非常敏感，建议使用 NVMe SSD。另外，主从服务器的硬件规格应当尽量一致。</p><p></p><h1>之六</h1><p></p><p>在使用 PikiwiDB(Pika) 复合数据结构（hash，list，zset，zset）时，尽量确保每个 key 中的二级 key（或者成为 field）不要太多（不要超过 1 万个），在业务层或者代理层对大 key 符合数据结构进行拆分（类似于分库分表）， 这样可以避免超大 key 带来很多潜在的性能风险。</p><p></p><h1>之七</h1><p></p><p>root-connection-num 参数非常有用，意为“允许通过 127.0.0.1 登录 PikiwiDB(Pika) 的连接数”，它不会被算进客户端最大连接数配置项 maxclients，因此在发生异常 maxclients 被用尽的场景中，管理员仍然可以登录 PikiwiDB(Pika) 所在服务器并通过 127.0.0.1 登入 PikiwiDB(Pika) 处理问题，可以认为是超级管理员通道。</p><p></p><h1>之八</h1><p></p><p>client kill 命令被加强了，如果你想一次性杀掉当前 PikiwiDB(Pika) 的所有客户端连接，只需要执行 client kill all 命令即可。注意，主从同步的网络连接不受影响。</p><p></p><h1>之九</h1><p></p><p>适当地调整 timeout 参数，PikiwiDB(Pika) 会主动断开不活跃时间超过 timeout 值的连接，避免连接数耗尽。由于网络连接会占用主机内存，因此合理的配置 timeout 参数也能够在一定程度上降低 PikiwiDB(Pika) 的内存使用量。</p><p></p><h1>之十</h1><p></p><p>PikiwiDB(Pika) 的内存占用主要集中在 SST 文件的 cache 和网络连接内存占用量，通常网络连接内存量会比 SST 的 cache 大，PikiwiDB(Pika) 目前已支持连接申请内存的动态调整与回收，因此连接占用的总内存大小是可以粗略估算的，如果你的 PikiwiDB(Pika) 内存占用远超预估（如大于 10GiB），那么可能为你当前使用的版本存在内存泄漏问题，尝试依次执行命令 client kill all 对连接内存进行强制回收，或者升级到最新版本。</p><p></p><h1>之十一</h1><p></p><p>非常不建议单副本运行 PikiwiDB(Pika)，单副本的数据安全性无法保障，诸如 RocksDB Bug 或者资源不够（如：ERR IO error: While fdatasync: /data1/db/zsets/16566747.log: Cannot allocate memory）导致 RocksDB 存储数据被污染，此时无法全量恢复数据。 最简集群状态应为一主一从。</p><p></p><h1>之十二</h1><p></p><p>如果 PikiwiDB(Pika) 单副本运行（非主从集群），只在乎性能，且不在乎数据安全性（如缓存场景），可以考虑通过关闭 binlog（将 write-binlog 参数设置为 no）来提高写入性能。</p><p></p><h1>之十三</h1><p></p><p>PikiwiDB(Pika) v3.5.2 以及之后的版本提供了关闭 RocksDB WAL (DisableWAL true) 的命令，如果你的 PikiwiDB(Pika) 实例出现间断性的写性能阻塞的情况，你可以通过关闭 WAL 命令暂时关闭 WAL，这种方式有断电情况下数据丢失的风险，待性能恢复时，请及时再打开。对数据完整性要求不高时，建议关闭 WAL。</p><p></p><h1>之十四</h1><p></p><p>PikiwiDB(Pika) 的数据目录中有大量的 SST 文件，这些文件随着 PikiwiDB(Pika) 数据量的增加而增加，建议为 PikiwiDB(Pika) 配置一个较大的 open_file_limit ，以避免 fd 不够用，如果不希望 Pika 占用太多的文件描述符，可以通过适当增大单个 SST 的体积来降低 SST 的总数量，对应参数为 target-file-size-base。</p><p></p><h1>之十五</h1><p></p><p>不要修改 log 目录中的 write2file 文件和 manifest。write2file 记录了 binlog 文件列表等关键信息，而 manifest 则记录了 RocksDB 的 version 信息，二者关乎 PikiwiDB(Pika) 实例重启后的 binlog 续写及 slave 断点续传时的数据正确性。</p><p></p><h1>之十六</h1><p></p><p>自 PikiwiDB(Pika) v3.5.0 之后的版本摒弃了用 rsync 进程进行全量同步，PikiwiDB(Pika) 进程内部重新实现了一套新的全量同步机制（通过名称为 rsync 的线程传输）。PikiwiDB(Pika) 提供了 rsync 的总传输限速参数 throttle-bytes-per-second 和并发 rsync 线程数 max-rsync-parallel-num，throttle-bytes-per-second  参数的单位是 MiB，建议在千兆环境中该参数设置不应高于 45，而在万兆环境中不应高于 500，以避免 PikiwiDB(Pika) 在全量同步的时候将所在服务器网卡流量用尽而影响到 PikiwiDB(Pika) 服务客户端。</p><p></p><h1>之十七</h1><p></p><p>在 PikiwiDB(Pika) 中执行 “ key * ” 并不会造成 Pika 阻塞（PikiwiDB(Pika) 是多线程的），但在存在巨量 key 的场景下可能会造成临时占用巨量内存（这些内存用于该连接存放 key *的执行结果，会在 “ key * ”执行完毕后释放），因此使用 “ key * ” 一定要小心谨慎。</p><p></p><h1>之十八</h1><p></p><p>如果发现 PikiwiDB(Pika) 有数据但 info keyspace 的显示均为 0，这是因为 Pika 并没有像 Redis 那样对 key 的数量进行实时统计，PikiwiDB(Pika) 中 key 的统计需要人工触发，执行 info keyspace 1，注意执行 info keyspace 是不会触发统计的，没有带上最后的参数 1 将会仅仅展示上一次的统计结果，key 的统计是需要时间的，执行状态可以通过 info stats 中的 is_scaning_keyspace 进行查看，该项值为 yes 表明统计正在进行，为 no 时表明没有正在进行的统计/上一次统计已结束，在统计执行完毕前 info keyspace 不会更新，info keyspace 的数据是存放在内存里的，重启将清零。</p><p></p><h1>之十九</h1><p></p><p>不要在 PikiwiDB(Pika) 执行全量 compact 的时候触发 key 统计（info keyspace 1）或执行 keys *，否则会造成数据体积暂时膨胀直到 key 统计、keys *执行结束。</p><p></p><h1>之二十</h1><p></p><p>对存在大量过期数据的 PikiwiDB(Pika) 实例，compact-cron 配置项可以在固定时段（一般配置为低峰流量时间段）进行过期数据清理。自 PikiwiDB(Pika) v3.5.0 之后还提供了 auto_compact 配置型，启用后 PikiwiDB(Pika) 会自动周期性执行 compact。</p><p></p><p>异常的数据体积（大于估算值 10%以上），可以通过执行 compact 命令，在 compact 执行完毕后观察数据体积是否恢复正常。</p><p></p><p>请求耗时突然异常增大，可以通过执行 compact 命令，在 compact 执行完毕后观察请求耗时是否恢复正常。</p><p></p><h1>之二十一</h1><p></p><p>自 PikiwiDB(Pika) v3.5.0 之后可统计过期 key（可通过 info keyspace 1 来触发统计，通过 info keyspace 查看统计结果），统计结果中的 invaild_keys 的值为“已删除/过期但还未被物理删除的 key 的数量”，PikiwiDB(Pika) 会在后台逐步地对已删除/过期的 key 进行物理清理，由于这是一个后台行为，因此在存在大规模过期 key 的场景下这些 key 可能无法被及时清理，因此建议关注该值，若发现无效 key 数量过多可通过 compact 命令进行全面清理，这样能够将未物理清理的无效数据控制在一个较好的程度从而确保 Pika 的性能稳定，如果 PikiwiDB(Pika) 中存储的数据是规律性过期的，例如每个 key 的过期时间为 7 天，那么建议通过配置 compact-cron 参数来实现每天的定时自动进行全量 compact，compact 会占用一定的 IO 资源，因此如果磁盘 IO 压力过大，建议将其配置为业务低峰期执行，例如深夜。</p><p></p><h1>之二十二</h1><p></p><p>write2file 的角色相当于 binlog，建议 write2file 保留周期/数量不低于 48 小时，足够的 write2file 有利于 大数据集群的从库扩容、从库服务器关机维修、从库迁移 等工作，不会因为主库 write2file 过期而被迫全量重传。</p><p></p><h1>之二十三</h1><p></p><p>PikiwiDB(Pika) 的备份生成为快照式，通过硬链接存放在 dump 目录下，以日期为后缀，每天只生成一份，多次生成备份时新的备份会覆盖之前的旧文件。在生成备份快照的时，为了确保数据的一致性 PikiwiDB(Pika) 会暂时阻塞写入，阻塞时间与实际数据量相关，根据测试PikiwiDB(Pika) 生成 500GiB  备份快照仅需 50ms。在写入阻塞的过程中连接不会中断，但 client 会感觉到 “在那一瞬间请求耗时增加了一些”。由于PikiwiDB(Pika)Pika 的快照是 db 目录中 sst 文件的硬连接，因此最初这个目录是不会占用磁盘空间的。</p><p></p><p>但在 PikiwiDB(Pika) db 目录中的 SST 文件发生了合并、删除后，硬链接的旧文件并不删除，这会导致 PikiwiDB(Pika) 占用的磁盘空间超出预估，所以请根据实际的磁盘空间调整备份保留天数，避免备份太多而造成磁盘空间用尽。</p><p></p><h1>之二十四</h1><p></p><p>如果写入量巨大且磁盘性能不足以满足 RocksDB memtable 的及时刷盘需求，那么 RocksDB 很可能会进入写保护模式（write stall，写入将被全部阻塞），建议更换性能更好的存储系统来支撑，或者降低写入频率（例如将集中写数据的 2 小时拉长到 4 小时），也可适当加大 write-buffer-size 的值来提高 memtable 的总容量从而降低整个 memtable 被写满的可能。</p><p></p><h1>之二十五</h1><p></p><p>PikiwiDB(Pika) 对数据进行了压缩，默认压缩算法为 snappy，并允许改为 zlib，因此每一次数据的存入、读出都需要经过压缩、解压，这对 CPU 有一定的消耗，建议像使用 Redis 一样使用 PikiwiDB(Pika)：在 PikiwiDB(Pika) 中关闭压缩，而在 client 中完成数据的压缩、解压，这样不仅能够降低数据体积，还能有效降低 Pikiw。注意关闭和开启压缩后，需要重启 PikiwiDB(Pika) 实例。</p><p></p><h1>之二十六</h1><p></p><p>读写分离很重要，PikiwiDB(Pika) 在常见的主从集群中由于写入是单点的（仅 master 支持写），因此写入性能是有极限的。可通过多个 slave 来共同支撑读流量，因此 PikiwiDB(Pika) 集群的读性能是随着 slave 数量的增加而增加的，所以对于读量很大的场景，建议在业务层代码加入读写分离策略，同时在 PikiwiDB(Pika) 层增加 slave 数量。</p><p></p><h1>之二十七</h1><p></p><p>全量 compact 的原理是逐步对 RocksDB 的每一层做数据合并、清理工作，在这个过程中会新增、删除大量的 SST 文件，因此在执行全量 compact 的时候可以发现数据体积先增大后减小并最终减小到一个稳定值（无效、重复数据合并、清理完毕仅剩有效数据），建议在执行 compact 前确保磁盘空余空间不低于 30%，以避免新增 SST 文件时将磁盘空间耗尽，另外 PikiwiDB(Pika) 支持对指定数据结构进行 compact，例如一个实例中已知 hashtable 结构的无效数据很少但 hashtable 结构数据量很大，set 结构数据量很大且无效数据很多，在这个例子中 hashtable 结构的 compaction（命令是 compact hash） 是没有必要的，你可以通过 compact set 实现只对 set 结构进行 compaction。</p><p></p><p>注意：在 PikiwiDB v4.0.0 版本之后，不再支持对特定类型的 compaction。因为 PikiwiDB v3.x 使用的存储引擎是 Blackwidow，每个数据类型使用一个 RocksDB，而 v4.0.0 的存储引擎升级为 Floyd，可以在单个 RocksDB 中存储所有类型的数据。</p><p></p><h1>之二十八</h1><p></p><p>PikiwiDB(Pika) 3.5.0 以后的版本支持通过 rate-limiter-bandwidth 配置项以限制磁盘 IO 速率，可以通过调整该配置参数来调整读写速度。在 v4.0.0 之前只支持写限速，在  v4.0.0  之后支持读写限速，可以通过调整配置参数中的  rate-limiter-mode 来设置限速模式。</p><p></p><h1>之二十九</h1><p></p><p>PikiwiDB(Pika) 和 Redis 一样支持慢日志功能，可通过 slowlog 命令查看。slowlog 的原始内容只存于内存中，内存空间有上限，且这个上限可配置，当然如果配置过大会造成 slowlog 占用太多内存。PikiwiDB(Pika) 也允许将 slowlog-write-errorlog 设置为 yes，以把慢日志记录到 pika.ERROR 日志中，用于追溯、分析。</p><p></p><h1>之三十</h1><p></p><p>PikiwiDB(Pika) v3.5.2 以后的版本支持冷热数据分离，并在 Pika 磁盘存储之上增加了内存缓存层（称之为 RedisCache），将用户访问的热数据放在缓存层，冷数据放在磁盘，可减少查询磁盘的次数，提升服务的读性能，不论 PikiwiDB(Pika) 使用的是主从复制模式还是集群模式，可以配置 cache-mode 为 1 ，并设置缓存的大小和个数，以提升读性能。如果实例内存较小，不足以支撑缓存层的资源耗费，你可以选择将 cache-mode 设置成为 0 将缓存层关闭掉。</p><p></p><h1>之三十一</h1><p></p><p>PikiwiDB(Pika) 3.5.3 以后的版本支持了 Redis ACL 功能，设置用户密码的方式发生了变化，ACL的认证方式和 Redis 保持一致，在 config 文件中按照 ACL 规则对 user 进行配置。PikiwiDB(Pika) 3.5.3 仍然兼容以前旧版本的认证方式。</p><p></p><h1>之三十二</h1><p></p><p>PikiwiDB(Pika) 3.5.3 以后的版本支持快、慢命令分离，有快、慢两个线程池，可以防止慢命令对快命令线程池阻塞的影响。可以通过  slow-cmd-list 配置项设置慢命令列表，通过设置 slow-cmd-thread-pool-size 设置慢命令线程池个数。</p><p></p><h1>之三十三</h1><p></p><p>欲知后事如何，且待微信群里分解。请添加 PikiwiDB 小助手【微信号: PikiwiDB】为好友，它会拉您加入官方微信群。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/o7rlPiN410bFwDAIQs7U</id>
            <title>大模型时代，智算基础设施将走向何方？丨对话AI原生《云智实验室》</title>
            <link>https://www.infoq.cn/article/o7rlPiN410bFwDAIQs7U</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/o7rlPiN410bFwDAIQs7U</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 08:33:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型时代, 智算基础设施, 百度百舸, 算力需求
<br>
<br>
总结: 在大模型时代，智算基础设施对于算力需求提出了挑战，百度百舸作为智算基础设施的一部分，致力于提供稳定性和高性能的解决方案。通过提升算力利用率和引入多元算力供应，百度致力于突破算力瓶颈，满足大模型时代对于智算基础设施的需求。企业在构建基础设施时，关注低门槛接入和性价比问题。 </div>
                        <hr>
                    
                    <p>大模型时代，产业对算力的需求激增，然而模型的训练不仅仅是堆算力就可以解决所有问题，如何保障大模型训练的稳定性和效率，对AI基础设施提出了挑战。</p><p></p><p>大模型时代对于智算基础设施提出了何种新要求？智算基础设施又将如何助力企业实现数智化转型？带着这些问题，在《对话AI原生：云智实验室》栏目中，百度集团产品委员会联席主席宋飞与InfoQ编辑围绕“大模型时代，智算基础设施如何实现超进化”展开了一场思想碰撞。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f9/77/f915eef4205ee674ae64b53d681d9477.png" /></p><p>点击链接收看《大模型时代，智算基础设施如何实现超进化？》</p><p>https://www.infoq.cn/video/4bBkYmuaP20lVa4U29kM</p><p></p><p></p><h3>以下为本期栏目精华内容：</h3><p></p><p></p><p>InfoQ：大模型时代，智算基础设施扮演了怎样的角色？市场对平台提出了何种新要求？百度智能云是怎么做应对的呢？</p><p></p><p>宋飞：大模型快速发展的背后是规模定律（Scaling Law），简单来说就是规模越大，大模型的效果越好，而这个“规模”包含了参数、规模等等。规模定律的发展，其实是建立在算力的高速发展上的，所以大模型过去的快速发展，其实就是在智算基础发展上去进行迭代、生长的，同时其也是基于智算基础设施对外提供服务的。所以可以认为，智算基础设施就是大模型时代的水电煤。</p><p></p><p>大模型时代这个智算基础设施，相比以前的小模型时代，它的特点的关键词就是“大”。这个”大“也包括参数规模比较大、存储容量比较大，进而要求它的集群规模很大，对于客户来说，进一步要求了对于它的投入也很大。针对这些新的特点，我们需要一个新的范式去设计我们的智算基础设施，令其拥有高性能，同时又兼具高性价比，才能满足大模型时代的需求。也是基于这个特点，百度智能云致力于去设计新的范式，以及相应的产品解决方案，来满足大模型时代对于算力的需求。我们推出了百度百舸·AI异构计算平台，致力于在稳定性，性能以及可应用等特点去进行重点打造。</p><p></p><p>InfoQ：所以针对大模型的“大”这个特点，智算基础设施其实要做的是一个“化繁为简”的工作。那么百度百舸与市面上的其他智算平台有何不同？可以从性能、架构以及各种角度来给我们深入分享一下吗？</p><p></p><p>宋飞：百舸平台源自于百度十多年在AI基础设施领域的技术积累和工程实践。在2021年推出1.0版本以后，百度百舸持续进行升级和完善，并且服务了自动驾驶、生命科学，泛科技等领域的一些客户。百度百舸其实确实在很多方面，我们也做了全面系统的一些工作，我们致力于让百度百舸为客户提供很好的一个解决方案，所以我们在很多方面，都做了全面系统的一些工作。针对行业关注的性能维度，我们通过全链路的性能手段，让AI基础设施在训练领域综合能力相比业界提升30%以上，在推理领域，提升了60%以上，为了实现这样的提升，我们在几个细节上做了提升：</p><p></p><p>首先是集合通信库，我们推出了百度的BCCL通信库，它基于开源的NCCL通信库，并对其进行了增强和拓展。同时我们在可观测性、稳定性，性能的诊断调优等方面做了大量的提升，能够帮助客户在训练阶段，能够快速的掌握集群的通信状态，及时的发现问题，并进行相关的一些调优。</p><p></p><p>同时在做大规模的分布式训练的时候，自动的并行策略对于性能有非常重要的影响，我们开发了自动并行策略的调优工作，能够使以前的并行策略的设置，从小时级提升到分钟级，大大提升了性能的发挥效率，并且其效果是好于普通专家设置的。</p><p></p><p>在稳定性层面，我们也开发了一个全面的自动容错机制。当集群规模大的时候，故障是不可避免的，这就需要去考虑如何去降低故障对于训练任务的影响。我们希望对硬件故障的监测做到全面的提升，当出现故障的时候，让任务能够快速的恢复、重启，并在全流程进行提升，从而让硬件故障导致的任务中断，从小时级缩短到分钟级，这能够极大的提升集群的资源利用率。</p><p></p><p>InfoQ：所以百舸的优势就在于更强的性能，以及更高的稳定性，同时在不断地业务实践中不断实现优化。那么针对算力限制的问题，百度是如何通过技术领先性去突破算力瓶颈的？</p><p></p><p>宋飞：第一点就是要提升这个算力的利用率。针对这一点，我们推出了AIAK加速库，在应用过程中，无论是训练场景还是推理场景，都能够把已有的芯片算力进行充分发挥。这其实也是一个系统的工程，在训练层面，从I/O的加速到算子库的建设，再到通信优化、显存优化，每个层面我们都要做到极致，这也是我们在产品里面提供的解决方案。</p><p></p><p>在推理层面，随着大模型的落地，算力需求会越来越大。对于推理角度算力利用率的优化，包含了从底层高性能的算子，到推理图的转换优化，也包括了对于请求动态，batch调度的技术等等，通过对这些领域一系列手段的提升，从而提升算力利用效率，将它的性能充分发挥出来，简单来说，就是把已有的算力用好。</p><p></p><p>第二层面，为了解决算力瓶颈，各家企业都在去想办法引入更多元的算力供应。这就面临了一个问题：怎么把多元算力当成一个有机整体从而利用起来？针对这一点，我们推出了业界首发的多芯混合训练解决方案。第一步是把多家的芯片聚合起来，并对其进行合理组合，使其真正变可整体使用的集群。不同的芯片的特点也不一样，我们也要去做一些自适应策略的优化，从而让分布式训练的算法在多家芯片上真正运行起来。我们也要对各家的芯片进行算力层的抽象。这种抽象之后，可能对使用者来说，就不用再关心多元芯片的差异。</p><p></p><p>通过以上一系列的手段，我们在多芯混合训练层面也达到了比较好的效果，千卡的多芯混合训练的资源效能做到了95%，在百卡能达到97%。这种低损耗的表现，能够真正帮助客户把多芯能力充分的发挥出来。</p><p></p><p>InfoQ：第一是把已有的芯片能力发挥出来，第二是通过多芯混合自适应的能力去让其算力发挥到最大值，还有就是屏蔽硬件差异，让多元芯片能够协同去发挥更大的能量，这其实是一个效率优化的过程。那么针对客户侧的应用，在构建基础设施时，企业最关注的是哪些功能？</p><p></p><p>宋飞：企业在实施智算基础设施并进行AI产业的智能化转型时，通常会经历三个阶段：首先是迅速构建起集群；其次是结合自身业务需求，在集群中对原始想法进行训练和验证；如果验证无误，便进入第三阶段，即大规模进行线上部署，将技术投入生产并实际应用。</p><p></p><p>百度百舸致力于实现"低门槛"接入，除了平台提供的运维能力和稳定性等维度外，还需提供业界的最佳实践，确保客户在每个阶段遇到问题时都能获得相应的解决方案或建议。这也正是百度智能云持续在做的。</p><p></p><p>其次，是客户所关心的性价比问题。一方面，我们需要为客户提供合理的硬件选型方案。在这方面，百度凭借多年的积累，能够为不同客户、不同规模的需求提供最佳方案。另一方面，提升性能利用率是提高性价比的重要手段，这也是我们重点关注的方向。</p><p></p><p>实现AI普惠是一项系统性工程，它涉及到对客户业务的深刻理解，平台提供的最佳实践，以及在产品的核心基础指标上达到业界领先水平。</p><p></p><p>InfoQ：除了性能之外，低门槛、高性价比等平台特质也至关重要，那么百度智能云智算基础设施是如何通过咱们的平台能力以及工程化能力去解决这些需求的？可以结合真实的案例给我们分享一下吗？</p><p></p><p>宋飞：智算基础设施在客户侧的落地是一项系统工程，它要求我们在技术层面和实施方案上追求极致。我们针对核心客户关注点进行了深入工作，特别是在提高集群利用率方面取得了显著成果。例如，在通讯时间优化方面，我们通过计算与通信的重叠优化，成功将集群在分布式训练中的通信时间占比从9%降低至2%，显著提升了集群的利用率。</p><p></p><p>企业客户非常关注性价比，这不仅涉及算力层面，还包括存储层面。我们提供了多级存储解决方案，以适应AI任务训练的需求。在大量数据准备和实际训练中，并非所有数据都需要使用高性能存储。通过多级存储方案，企业可以在海量、低成本存储和高性能存储之间找到平衡。我们的产品矩阵包括对象存储BOS、高性能存储PFS并行文件存储，以及缓存加速产品RapidFS，能够满足性能和存储性价比的双重需求。</p><p></p><p>InfoQ：现在有一个论调，很多人都在说这个摩尔定律已经被打破了，全球的属于AI的产业革命正在到来，百度是如何看待这个趋势的？并且去应对这种产业革命的到来呢？</p><p></p><p>宋飞：首先，我们确实能够观察到，新一轮大模型的驱动正引领着产业变革的新浪潮。这场变革的大幕正在缓缓拉开。在这背后，技术的算力层面所支撑的规模定律，我们认为其当前仍然有效，并且预计在未来一段时间内还将持续发展。</p><p></p><p>百度也坚信这一点，并将持续坚持自主创新，在技术研发、生态建设和人才培养等方面加大投入。我们致力于持续推出业界领先的产品和解决方案。与合作伙伴携手，我们将加快创新的步伐，共同构建新的生产力，以真正推动产业的智能化变革。</p><p></p><p>点击链接收看本期节目：https://www.infoq.cn/video/4bBkYmuaP20lVa4U29kM</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CZ5WbilcGzmSQ4vKjETf</id>
            <title>金融场景中的多智能体应用探索 | AICon</title>
            <link>https://www.infoq.cn/article/CZ5WbilcGzmSQ4vKjETf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CZ5WbilcGzmSQ4vKjETf</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 07:43:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 嘉宾, 陈鸿, 蚂蚁集团, 大模型技术
<br>
<br>
总结: 在金融科技领域，蚂蚁集团专家陈鸿介绍了大模型技术在优化金融决策中的应用，强调了基于AgentUniverse框架的PEER模式对提升决策精准度和效率的重要性。同时，讨论了从大模型到多智能体的发展趋势，以及智能体和多智能体在金融领域中的应用前景。 </div>
                        <hr>
                    
                    <p>嘉宾 | 陈鸿&nbsp;蚂蚁集团专家</p><p></p><p>编辑 | 李忠良</p><p></p><p>在金融科技的浪潮中，多智能体技术正成为推动行业创新的关键。面对海量信息和复杂决策，如何利用这一技术优化金融决策呢？在<a href="https://aicon.infoq.cn/202405/beijing/schedule"> AICon 全球人工智能开发与应用大会（北京站）</a>"上，InfoQ 荣幸地邀请到了蚂蚁集团资深算法专家陈鸿先生。在他的精彩演讲中，陈鸿深入介绍了蚂蚁集团在大模型技术领域的最新进展，并针对金融行业所面临的信息爆炸、知识复杂性以及决策难度等挑战，提出了创新的解决方案。</p><p></p><p>他特别强调了基于 AgentUniverse 框架的 PEER 模式（Plan-Execute-Express-Review），这一模式有望有效提升金融决策的精准度和效率。本文是对陈鸿先生演讲内容的精心整理，旨在为读者带来前沿的大模型洞察，并启发思考如何将这些技术应用于金融行业的实际问题解决中。</p><p></p><p>另外，即将于 8 月 18-19 日举办的 AICon 上海站同样设置了**「大模型 + 行业创新应用」专题分享，我们将精选具有代表性和规模的典型案例，展示大模型技术在不同领域中的实际应用与成效。目前是 8 折购票最后优惠期，感兴趣的同学可以访问文末「阅读原文」**链接了解详情。</p><p></p><p>在大模型技术日新月异发展的时代，技术观点也得日拱一卒，苟日新日日新，不存在稳定的金科玉律。与其私藏一时一刻的技术思考，不如分享以求碰撞和启发。故此我把为这次 AICon 准备的 PPT 材料发布出来，并补上解读，从「在线生成」转成「离线生成」，没有时间限制，或可以更系统一点。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0e/0ecb14903c451bbe0dd257ed3d05a4c6.png" /></p><p></p><p>从大模型到多智能体</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/45/459a87cfe32d376afe455b5305bbbf9e.png" /></p><p></p><p>智能体、多智能体都是当下的技术热点，但作为一个技术人应该理解，所有的技术都有自己所针对的问题、及其能力边界，并不存在普适的、放诸业务场景皆 work 的技术方案。我们在这里尝试区分，从大模型到智能体再到多智能体这几个 AI 热点概念背后的关键差异和适用范围。</p><p></p><p>先从语言模型说起，一个经过足够语料充分预训练的基模型（base model），就是一个压缩了海量知识的知识容器，但这些知识关在数百亿到千亿的参数黑盒中难以使用。OpenAI 在 2020 推出 GPT3 的时候，因为它生成内容的不可靠和不可控，引发了当时媒体对 AI 的嘲笑和质疑，而不是现在的追捧。</p><p></p><p>2022 年底 ChatGPT 破圈逆转了大众对大语言模型的看法，基模型在完成对齐（SFT + RLHF/ DPO）之后，就成为一个助手模型（Chat model），它可以被看作一个以自然语言为输入输出接口的 AI machine，它不仅掌握语言且对齐了人的偏好，于是可以流利的和人交流；并因为能输出语言，而可以通过语言操控其他工具；我们还发现这些对齐过的模型具备一定的简单推理能力，虽然问题复杂的时候，就容易失败。整体上，这一批 Chat Model 已经开始让人产生了它具备一定程度智能的错觉，当然实际上，大模型只是一个无状态的 query-answer machine，某种意义上等价为一个哲学家约翰塞尔（John Searle）提出的中文屋子（chinese room）（不知道的话建议搜索并读一下这个有趣的思想实验），LLM 是无状态的，比如你在和大模型聊过五分钟后和它再聊，与隔上五天再和它聊，它对待你不会有任何差别。在本质上，LLM 和其他神经网络模型一样是个无状态的函数，目前 LLM 的一切状态性处理，都依赖外部的 Prompt 机制。LLM 能和人进行多轮对谈，需要外部系统对整个对话 session 的状态保持（并回传到 prompt 里）。</p><p></p><p>从大模型到智能体，关键的区别就是从无状态的模型变成了有状态的状态机。智能体要接入（Grounding）环境，完成任务，就必然涉及工作流（workflow），就需要有保持任务状态的能力，无状态的模型无法持续跟进一个任务的工作进程。我们在下一页 PPT 会展开讨论这一点，我们会看到智能体的感知、行动、记忆、规划，也都需要基于一系列离散的被定义的状态来进行，或者说，一个智能体能在其中规划并活动的外部环境需要被加工为离散化概念，发散来说，人类也是这样，光谱是连续的，但人类能喊出名字的只有赤橙黄绿青蓝紫，声音的频谱是连续的，但人类的知觉把音频加工为一系列离散的元音 / 辅音 / 字 / 词，是这些离散的 token 而不是连续的音高构成了语言的基础。可以发现，人类智能从感觉到知觉也是一个从连续到离散的状态化加工过程。要让大模型接入真实世界解决真实任务的时候，我们就需要把大模型进一步封装为某种智能体。</p><p></p><p>我们说成为状态机是 Agent 规划和完成任务的关键，但专业任务往往是多环节多分支的，在每个环节和分支上，专业化分工会有更高效的 ROI。这就产生了从智能体发展到多智能体的必要性，而在不同环节的职能岗位上，不同的智能体如何通过合理的协同模式组织在一起，这是属于多智能体的核心技术问题，多智能体作为一个团队，需要比直接大模型端到端或单一智能体从头单打独斗更鲁棒，而不能因为组织的复杂性让整体变得更脆弱。后面我们也会有专门一页 PPT 讨论多智能体的协同模式。</p><p></p><p>最后我们看 PPT 的下面部分，我们把金融场景里的任务粗分为两类，一类是可以由大模型端到端直接生成结果的，端到端可以类比为人类的系统 1 或快思考模式，包括「问答、摘要、给出建议」这些任务。这容易理解，我们说话的时候，不需要也没有办法去一个一个字往外说，我们真正思考的单位是一个个念头或者想法，是这些想法构成推理和思考的基础单元（building-block），这也就是所谓的系统 2 或慢思考，也是当前大模型难以很好处理的推理问题，但我们可以基于 Agent 的 workflow 与自省来应对。在金融场景里，许多专业任务需要一定程度的分析、归因、决策，这些都更适合通过智能体或多智能体来实现。后面我们也会有一页进一步展开对金融任务的分析。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e6/e6dae6c84a570345cb9c9776f5d4cc74.png" /></p><p></p><p>这页我们讨论基于大模型的智能体。</p><p></p><p>智能体（Agent）不是一个新概念，它的历史比大模型更久，1995 年出版的经典著作 《Artificial Intelligence：A modern approach》 第一版就以 Agent 为中心展开（附带一提，这本书最新是 2020 年的第 4 版，依然不改初衷以 Agent 为总领全书的总纲，现在如果出第 5 版，肯定就会讨论 Large Language Agent 了）。感知器 Sensor、行动器 Effector，规划器 Planner，Memory， 这些 Agent 的核心组件或能力在 95-2000 年那时就成体系的提出来了。</p><p></p><p>如前所述，对以端到端完成任务为目标的智能体而言，没有状态，不成方圆。我们能发现感知、规划、行动、记忆这些智能体的核心能力事实上都依赖对特定状态的定义和识别。例如，感知能力，依赖对智能体所在环境状态的定义和识别；规划能力，依赖对任务不同状态的定义和识别；行动能力，依赖行动选项状态的定义和识别；记忆能力，则依赖对行为结果状态的定义和识别。智能体正是通过对这些状态的识别，和外部环境有效对接，管理和完成任务。这是一套强调落地的合理设计，但涉及状态的识别或状态间的迁移，只能依赖规则或上一代机器学习算法，由于泛化能力不足，智能体在实际任务中就不免会制造各种 bug。例如扫地机器人是个典型的具身 + 自治 Agent，但大家只要家里有过扫地机器人的，应该能想起各种扫地机器人因为 corner case（literally！）闹的笑话。</p><p></p><p>在大模型横空出世之后，加上 AutoGPT，LangChain 等框架的出现，充分发挥了大模型控制工具的能力，让许多人看见了用大模型作为智能体核心引擎的优势，更重要的是，LLM 取代机械的规则，能更鲁棒更泛化的识别任务（以及环境）状态，在理想情况下，当前 LLM-based Agent 能基于自然语言的任务描述持续展开任务，泛化地确认任务完成进度，并视情况动态规划再采取行动，这是一个美好设计，但当然未经调整的通用大模型还是很难无痛顺利完成任务，因为一个专业任务不可避免地涉及大量过程性知识，如何感知、如何执行、如何规划背后都依赖各种专业 KnowHow，所谓 Know-How，就是一件事如何完成，是所谓过程性知识。这些专业的 Knowhow，或过程性知识往往是不成文的，大家交接工作的时候，最麻烦的就是这些没有写在文档里的经验。要让智能体顺利完成任务，就需要形式化那些不成文的专家 Know-how，提供将之引入智能体的合理机制。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f39dcb3a39752f2e0d57512f73044064.png" /></p><p></p><p>从单 Agent 到多 Agent 协同，这是源自 ROI 的压力，专业任务往往是多环节多分支的，在每个环节和分支上，经济规律决定了专业分工会有更高效的 ROI。这就产生了从智能体发展到多智能体的必要，而在不同环节的职能岗位上，不同的智能体如何通过合理的协同模式组织在一起，这是属于多智能体的核心技术问题。</p><p></p><p>人类自己就是依靠分工协同而成为了地球的顶级掠食者，人没有依靠牙齿爪子、力量速度等等单一个体的能力，人是靠组成一个社会之后形成的集体能力，这超越了任何超级个体的能力。集体力量大这件事在 AI 上也不会例外，当然，成功的社会化并不容易，历史不止一次的证明，引入有效社会化机制（组织形态）的力量和价值（以及错误的组织形态的破坏性）。不同的组织形态（协同模式）适配着不同的任务。</p><p></p><p>回到多智能体上，不同类型的专业任务也一样需要我们为之设计不同的协同模式。第一类：任务可以逐层分解的适合上下级协同的模式（这个模式非常常见，后面我们开源的 Agent 框架核心贡献就是提供了这个模式的一个核心抽象：PEER，Plan-Execute-Express-Review，此处不再赘述），第二类：那些存在解法但难以拆解为固定步骤的更适合师生传授式协同（例如数学证明需要的是思路点拨或样题举例， 从费马大定理到行程问题都不适合分工规划再解决）。第三类：那些开放性的复杂问题无从规划，则更适合交给某种竞争 - 评价的机制让不同智能体并发搜索可能解法。</p><p></p><p>金融场景中的多智能体</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e75a1a509d3011ce3af26164e73956c.png" /></p><p></p><p>回到金融场景，我们把金融场景的特殊性总结成三点：信息密集、知识密集、决策密集。</p><p></p><p>关于信息密集，我们都知道一方面金融业务强依赖高频更新的资讯（更新密集），导致严谨的时效性处理必不可少，另一方面，这些信息中大量属于相关但无因果关系的噪声信息（噪声密集），需要有效屏蔽噪声才能做出正确决策。</p><p></p><p>知识密集：我们能看见金融市场中，围绕各种资产，有各种不同的理论和分析，但金融中的知识，不仅高密度，还是彼此高度对立的。我们会发现许多互相冲突的观点，某种意义上，这些冲突构成了市场交易的基础，买卖双方必然对资产价格有截然不同的预期，所以才有一买一卖，双方意见一致则不会形成交易，某种意义上，这就是为什么需要金融市场。市场是一种通过交易形成共识的机制。于是，金融领域中的观点必然冲突（知识冲突），这对大模型构成有趣的挑战，面对金融领域的多篇观点时，LLM 不能强行捏合成一个统一观点，既需要明确共识，也需要暴露分歧。</p><p></p><p>在金融领域，比知识冲突更需要 LLM 关注的是知识的边界，不存在无远弗届永远生效的知识，大的说，牛顿三定律在接近光速时失效，小的说，许多金融逻辑都有对宏观经济形势的潜在要求（知识边界），大模型在理解和处理这些逻辑的时候，需要理解这些知识的边界，否则就会闹出笑话。最后是决策密集，金融领域的决策（decision-making）有相对于其他决策任务的非常强的特征。一个是不确定性，金融决策面对的是开放环境，其他市场主体的参与和博弈带来了无穷变数，金融决策从头到尾都需要和不确定性信息共舞。另一方面，金融决策是高度不对称的，我们熟知搜索推荐解决的是海量信息中只有个别有效的信息不对称问题，但在金融决策中有类似的不对称现象，往往在大量决策中只有个别决策处于关键位置，带来关键收益（或避免风险）。如何定位这些关键决策点是金融所要处理的决策不对称性问题。</p><p></p><p>信息、知识、决策的问题对大模型而言都有标准解法，例如用 RAG 提供信息更新，引入图谱来规范知识，再包括强化推理能力的 CoT 方案。但面对金融特性，这些标准方案的效果不及预期。RAG 容易，但 RAG 多篇混入的噪声信息不容易处理。图谱有效，但图谱难以处理冲突和有边界的知识（有边界的知识不是 Knowledge Graph 中简单的二元关系，需要 N 元关系来刻画），CoT 也难以处理决策的不确定性和不对称性。</p><p></p><p>所以我们需要考虑金融场景的定制方案。此处我们把信息、知识和决策三类任务总结成两个对齐方向：一个是严谨性、一个是专业性。后面会有两个独立页来各自展开，所以这里我们简单过一下，能看见我们其实是期望通过大模型和多智能体两层各司其职，大模型负责压入必要的知识和能力，多智能体装载相关过程性 Knowhow 来保障金融的严谨和专业。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cf/cf4e522c6941d736c62aaa80378a0b7b.png" /></p><p></p><p>大模型具有幻觉的内在缺陷已经是一个老生常谈，不过有内在缺陷并不意味着 基于大模型的智能体应用不可能按严谨的标准完成任务。毕竟人也一样有类似的问题，人类也早已熟知通过系统的方式保障严谨标准的达成。</p><p></p><p>幻觉是两种生成式智能（人和 AI）共同具有的特征，它恰恰来源于对空缺的预测和生成，有一系列认知神经科学的实验说明，当一些人类患者的和视觉相关的脑组织被切除或破坏，他们本应消失的视野（盲区）里会被大脑自动填补出生动的幻觉形象（爱丽丝综合症），更日常的例子相信每个普通人也都体验过，当我们被人问及一些位于我们知识边界之外的问题，大脑会快速脑补出一些如假包换的「幻觉」来填充知识的空洞。我们在这里列了知识引用、知识边界、知识冲突来说明容易引发大模型幻觉出现的场景，当然也不限于此。</p><p></p><p>具有内在缺陷，不代表系统不能安全工作。人自己就是例子。人类本身就会有注意力的问题、预判力的问题，但我们在大多数情况下还是信任我们的司机能把我们安全的送到目的地。我们培训司机的驾照考试，某种意义就是一个对齐过程：让普通人向老司机一步步对齐。科目一 / 科目二 / 科目三分别就是知识注入的预训练 / 持续训练、SFT 阶段，以及最后的强化学习阶段（边上坐一个老司机评价你是否 OK）。但汽车如果危险仅仅有一个安全驾驶的司机也不行，汽车也需要遵循安全规范预防各种情况并做好各种最坏情况下的安全措施，最终如果我们有一个安全的司机和一辆安全的汽车，我们期待交通系统整体也是安全的，例如必要的信号灯、车道、交通警察等等。</p><p></p><p>把这个 metaphor 映射回 LLM 应用，LLM 需要面向严谨性对齐（基于各种细分任务且接受老司机检验，就像驾照培训需要分解到转弯倒车入库等等具体任务），LLM 外的智能体则需要准备好更多面向严谨的辅助性措施（类似于汽车之于司机），最终才是 AI 应用所在的整体系统可以做的一些规范性工作。个人意见是严谨性任务还是应该聚焦在模型和智能体这两层，系统级别的围栏有效且必要，但如果模型和智能体毫无改善，不免出现大量尴尬的拒答。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4c/4c7e398b0041d5277162499cc47f400b.png" /></p><p></p><p>专业是相对于通识而言。我们在讨论专业性的时候，需要意识到，专业本身就是分工的产物，无分工，不专业。一个个专业职能和擅长这些职能的专家的产生，本身是人类社会面向经济效率的优化结果。只有协同分工才是针对多任务难问题的高 ROI 方案，那么自然的问题，AGI 不需要面向任务优化，用一个超强的 AGI （或当前可得的最强大模型）去处理所有问题是否才是 LLM 时代的合理解法呢？滥用最强模型当然不合理，各家大模型厂商也提供不同尺寸的模型供应用方选择，应用方更有责任面向专业任务，将基座向特定专家对齐（向普通人偏好对齐的通用基座容易 underqualified 或 overqualified ）。在面对复杂困难任务的时候，通过多智能体团队协作，ROI 更容易胜过 超级基座单打独斗。</p><p></p><p>其次，在专业领域，知识容易速成（弥补），但专业能力则提升困难。这个点，LLM 和人也高度一致。当新知识新技术出现，我们可以通过网络或翻查 Manuel 快速弥补自己的一些知识漏洞，但如果能力有缺，不经过亲手实践和踩坑获取一手经验教训，难以有所进步。对大模型也是如此，知识缺乏，可以 RAG，可以 KG，但如果模型的一些专业能力不足，计算 / 推理 / 行情归因，都不是简单能解决的问题。</p><p></p><p>于是最终的结论也很明显。专业性建设的核心就是对一个系统中不同专业职能的差异化能力的定义和实现。起步阶段我们可以从优秀基座通过人设套取数据，但面向专家的对齐工作逃不掉，最终需要差异化精调的不同能力，这些能力建议聚合在一个基座中，但还是由不同 Agent 差异化使用。</p><p></p><p>多智能体框架 AgentUniverse</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a8/a87c54668ad180c0754cff1852f59992.png" /></p><p></p><p>关于我们已经开源的多 Agent 框架 AgentUniverse，各位可以通过《从孤立到协作，大模型多智能体协同使复杂任务迎刃而解（点击即可查看）一文做深入了解，Github 上也有相关的项目介绍和代码：AgentUniverse 项目地址：</p><p></p><p><a href="https://github.com/alipay/agentUnivers">https://github.com/alipay/agentUnivers</a>"<a href="https://gitee.com/AgentUniverse/AgentUniverse">https://gitee.com/AgentUniverse/AgentUniverse</a>"</p><p></p><p>欢迎开发者们加入社区体验、共建。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6cf08877ba921cd3e01e463107b9bf2e.png" /></p><p></p><p>投研支小助其底层是基于 agentUniverse 的 PEER 框架，基于这个 PEER 框架我们又融入大量投研专家经验，构建了一个投研 Copilot。PEER 模式是 agentUniverse 当前版本最具特色的多智能体协作模式组件，该模式包含计划 (Planning)、执行 (Executing)、表达 (Expressing)、评价 (Reviewing) 四个不同职责的智能体。</p><p></p><p>计划者拆解任务（例如把 query 分解为一系列子 query），执行者完成任务（例如检索），表达者汇总表达，评价者最终把关，OK 则输出，不 OK 则重复 workflow，PEER 这个计划 - 执行 - 表达 - 评价的循环构成了层级式分工协同的抽象，值得指出，虽然 PEER 虽然看起来像 Rag Fusion（而且它确实胜任 Rag Fusion 工作），但它不止于此，它本质上是分工这件事的一个合理抽象。抽象有其价值，抽象让分工这个优化方式可以递归使用，不断深入。例如 PEER 可以在计划环节也引入一层 PEER 通过分工去得到足够好的拆解，或者在评价环节再引入 PEER 的分工来做细粒度的精细评价。抽象让 PEER 的分工可以这样不断递归深入直到 Know-how 的尽头。</p><p></p><p>在图里右侧的专家框架是当前我们对投研领域专家经验的形式化落地，我们针对 9 类典型的定性分析场景，给出了 30 个不同的细分专家框架。体现了之前所说的专家 Know-how 的引入，在一系列消融实验中我们确认了这些专家框架的价值，不同机构可以通过定制这些专家框架让投研支小助呈现出完全不同的解读思路，这比用 SFT 强行 tuning 基座模型合理且便捷。</p><p></p><p>投研支小助目前在蚂蚁内部在报告解读、市场分析、政策解读、宏观分析等多个场景中是助力金融专家提升生产力的典型应用，实测数据表明，其每日可辅助一名投研分析师高质量地完成超过 100+ 篇研报、财报和金融资讯的专业解读，完成 50+ 金融事件的推理归因分析。</p><p></p><p>实际案例</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ff/ff3a2911c9702c98f5fc1cd18fe09b90.png" /></p><p></p><p>这是财报解读的例子，Query 是：“结合英伟达 2024 财年 Q4 财报分析人工智能行业后续走向”，可以看见在策划环节，智能体展开了一系列分析师关注的典型维度，规划智能体遵循了分析师的解读框架，通过一个嵌套的 PEER 过程产出了这一系列新的问题。</p><p></p><p>每天的行情资讯是高度套路化的，解读行情也有自己的套路，难点在于能否在套路化的解读中展现足够的洞察，保持观点数据的严谨则是基础要求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b2/b2988022f13f80a64f7cc76b469a4400.png" /></p><p></p><p>政策，尤其是财政政策和货币政策，对经济有着深远的影响，也对用户的投资策略牵一发而动全身。用户可以向支小助提问相关政策对市场带来的影响，支小助得益于专家分析框架，能像个老手一样对比政策前后的变化去分析政策影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/36/36dfa277cca7ea1569194df42dc35517.png" /></p><p></p><p>宏观分析是指对整个经济体的广泛性分析，包括但不限于经济增长、通货膨胀、就业状况、财政政策、货币政策、国际贸易和汇率变动等。支小助通过 PEER 范式，对宏观经济等相关复杂问题也能生成完整报告，胜任基础的宏观工作。</p><p></p><p>最后，做一个简单的预告，我们团队的同学很快会针对 AgentUniverse 框架核心的 PEER（Plan- Execute- Express - Review） 框架产出论文，敬请期待。</p><p></p><p>嘉宾简介</p><p></p><p>陈鸿（花名：五噫），蚂蚁集团资深算法专家。蚂蚁集团财富保险事业群智能服务算法总监，北京大学计算机系，豆瓣第 21 号员工，19 年加入蚂蚁，在蚂蚁数字金融线周游列国，历经财富、网商、花呗、借呗、芝麻、平台和服务，曾主持智人自动数据核对、金融行为序列、网格化运营、用户进阶路径决策、流量运筹、支小宝 2.0、金融大模型等技术项目。</p><p></p><p>活动推荐：</p><p>随着大模型在企业中的实践日益增多，企业界对大模型应用的探索和需求也在不断增长。为了满足这一需求，InfoQ 精心策划的 AICon 上海站即将盛大开幕。活动定于 8 月 18 日至 19 日举行，届时将有 12 个专题论坛，汇聚 50 余家企业的 AI 落地案例分享。这些案例覆盖了从 Agent 技术、RAG 模型、多模态交互到端侧智能和工具链构建等多个领域，为企业提供丰富的实践视角和启发。更多内容可点击 <a href="https://aicon.infoq.cn/202408/shanghai/">AICon 上海</a>"查看。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TNsdlbhpreP3adsXFJTq</id>
            <title>当《开心消消乐》遇上 AI 推理，我们找到了高质量关卡背后的原因！</title>
            <link>https://www.infoq.cn/article/TNsdlbhpreP3adsXFJTq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TNsdlbhpreP3adsXFJTq</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 07:38:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 热潮, 云服务, 游戏行业, AI 推理模型
<br>
<br>
总结: 随着AI热潮席卷各行各业，企业更倾向于选择云服务来部署AI模型和服务。游戏行业在探索和应用AI技术以提升游戏品质和玩家体验，常选择微调成熟模型方案。乐元素通过自研AI推理模型提升关卡设计效率，但在实践中遇到性能、成本和灵活性挑战。腾讯云的新一代S8实例提供了高性能、低成本和灵活性解决方案，乐元素将AI推理转向CPU，利用英特尔® AMX引擎提升效能。 </div>
                        <hr>
                    
                    <p>随着 AI 热潮席卷各行各业，其落地应用已经成为企业技术研发升级的工作重心。人工智能应用的升级不仅需要软件层面的升级迭代，还需要大规模基础设施的支撑。然而，自行搭建大规模算力、存储基础设施对于大多数企业而言都存在技术难度、人力资源、成本投入等多方面的挑战。因此，企业在探索 AI 实践时往往更倾向于选择云服务，尤其是云计算大厂提供的成熟云端计算实例来部署 AI 模型和服务，而在具体落地过程中，不同行业存在的痛点各异，对云基础设施的需求也有所不同。</p><p></p><p></p><h2>好玩有趣的关卡背后，创新 AI 模型的突破与挑战</h2><p></p><p></p><p>由于游戏行业的需求复杂，其相对较晚受到 AI 创新浪潮的影响，独特的创新周期、对游戏性和故事性的高要求，以及市场接受度和玩家期望的多样性，也延缓了 AI 在游戏中的广泛应用。再加上对经济因素和开发成本的考量，使得游戏行业在采纳 AI 技术时持谨慎态度。</p><p></p><p>然而，随着 AI 技术的不断进步和成本的降低，以及市场对高质量游戏体验需求的日益增长，游戏行业正积极地探索和应用 AI 技术来提升游戏品质和玩家体验，更常见的选择是对成熟的模型方案进行微调，以满足自身需求。</p><p></p><p>在这种场景下，对上层应用出色的推理能力与性价比则显得更为关键。通过基于成熟方案改造的推理模型以及能够输出高效推理性能的基础设施，使游戏开发团队可以迅速获得 AI 创新的收益，为终端用户带来更好的体验。</p><p></p><p>乐元素是经典休闲消除游戏《开心消消乐》的开发商，《开心消消乐》凭借着简单易上手的游戏原理和激发玩家好胜心的设计，使得玩家能够迅速融入游戏并享受其中。</p><p></p><p>《开心消消乐》拥有 9 大关卡类型、60 余种障碍设计、8000 多个精心设计的关卡。用户每日都可以进行游戏关卡挑战，因此，关卡的质量对于游戏的收入和用户留存起着至关重要的作用。乐元素的游戏团队不仅要持续推出新关卡和玩法，还要不断调整线上关卡的体验和难度，为玩家带来新鲜的游戏体验。</p><p></p><p>过去，乐元素团队主要通过人工流程制作关卡，但效率相对较低，导致新关卡的上线流程较长，很难确保难度一致性，又要考虑玩家离线游玩时是否通过特殊方式“作弊”，新玩法和已有关卡阵容的完整兼容问题，相关的设计和验证工作费时费力。</p><p></p><p>为此，乐元素创新地在关卡设计等流程引入了自研的 AI 推理模型。对于新增和调整的关卡，推理模型通过大量自动打关任务，确保关卡配置无错误，难度符合预期，并快速验证关卡；对于新开发的玩法，AI 也通过大量自动打关任务确保逻辑无错误。</p><p></p><p>如今，该模型每天平均运行超过 1 亿次打关任务，推理次数超过 30 亿次。通过 AI 创新，乐元素可以大大减轻开发团队设计新关卡和新玩法时的验证测试负担，使团队将精力从枯燥的验证工作中转移到开发任务上，显著提升开发效率，为玩家带来更多新鲜好玩的游戏内容。</p><p></p><p>然而，随着《开心消消乐》玩家群规模增长和游戏内容更新，乐元素的 AI 推理模型在实践中开始遇到性能、成本和灵活性三大挑战：</p><p></p><p>&nbsp;性能挑战：</p><p>随着游戏用户数量的增加和游戏内容的扩充，推理模型需要处理的关卡数量不断增多，对玩家玩法的模拟也更加复杂，这就意味着运行模型的服务器需要足够的算力来支持模型完成推理任务。</p><p></p><p>&nbsp;成本挑战：</p><p>游戏运营成本随着用户数量和游戏内容的增加而增加，特别是当部署专用的模型服务器时。因此，乐元素亟需寻找更适合推理的算力选项。</p><p></p><p>&nbsp;灵活性挑战：</p><p>面对不断变化的游戏内容和用户需求，特别是不同的模型推理需求，要求游戏服务器具备足够的灵活性支持。</p><p></p><p>今年，腾讯云推出的新一代 S8 实例，为乐元素提供了高性能、低成本和灵活性的解决方案，满足了其持续发展的诉求。</p><p></p><p></p><h2>聚集三大优势，乐元素将 AI 推理加速方案转向 CPU</h2><p></p><p></p><p>在以往的解决方案中，大多数游戏行业的 AI 推理场景会更偏向于性能强大的 GPU 作为算力基础设施。但随着近年来芯片短缺情况恶化，GPU 推理方案成本迅速上升，很多企业开始将目光投向了 CPU，并发现了 CPU 方案的一些显著优势：</p><p></p><p>成本显著降低：打关模型的 AI 推理任务以离线为主，任务运行时间也相对宽松。因此选用基于低成本、易获得的 CPU 进行推理的云实例在运行时间上可以满足乐元素要求，还可以节约日常开发成本。资源利用率高：除了打关推理模型外，乐元素日常也有很多通用计算任务需求，使用 CPU 来运行推理模型，可以在闲时继续运行其他通用任务，甚至在游戏流量高峰时快速扩展服务器资源池，有效提升了资源利用率，避免造成资源浪费；易开发、易部署：基于 CPU 的云实例搭配成熟的软件栈，使游戏公司开发团队能够快速部署推理模型，无需复杂的移植和优化工作。在一些需要快速部署新模型的情况下，所需的时间甚至更短。</p><p></p><p></p><h2>CPU 突破 AI 推理难关，英特尔®&nbsp;AMX 引擎成为取胜关键</h2><p></p><p></p><p>新一代腾讯云实例 S8 基于全新优化虚拟化平台，提供了平衡、稳定的计算、内存和网络资源。其中，标准型实例采用第五代英特尔® 至强® 可扩展处理器，内存采用最新 DDR5，默认网络优化，最高内网收发能力达 4500 万 pps，最高内网带宽可支持 120Gbps。</p><p></p><p>腾讯云实例 S8 搭载的第五代至强® 可扩展处理器凭借内置加速器实现单核性能提升，相较上一代产品，其整体性能提升 21%，内存速度提升 16%，且与上一代产品的软件和平台兼容，部署新系统时可大大减少测试和验证工作。</p><p></p><p>乐元素迁移到腾讯云实例 S8 后，单个实例能够处理的游戏数据和用户请求规模更大，平均成本更低，自研 AI 推理模型的效能大幅提升。</p><p></p><p>第五代至强® 可扩展处理器内置了英特尔® AMX 加速引擎，可加速基于 CPU 的深度学习推理，避免了使用独立加速器带来的成本和复杂性。英特尔® AMX 引入了一种用于矩阵处理的新框架（包括了两个新的组件，一个二维寄存器文件，其中包含称为 “tile” 的寄存器，以及一组能在这些 tile 上操作的加速器），从而能高效地处理各类 AI 任务所需的大量矩阵乘法运算，提升其在训练和推理时的工作效能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c59f62626a12ded423af7e2808fad4ba.webp" /></p><p>&nbsp; &nbsp;*英特尔® AMX 架构</p><p></p><p>通过采用英特尔® AMX 技术，乐元素得以显著提升自研 AI 推理模型的性能，除了提升模型的关卡验证测试效率外，还能满足更多场景的需求。例如英特尔® AMX 技术可以助力快速处理玩家数据，以实现快速的游戏元素调整；快速处理大量数据，创造更加真实和吸引人的在线互动，以提供更加平滑和快速的在线游戏体验。</p><p></p><p>乐元素还对新一代腾讯云 S8 实例进行了性能测试，验证了其代际性能提升。在 AI 打关推理模型的测试中，对比腾讯云与英特尔联合定制优化的第三代至强® 可扩展处理器，启用了英特尔® AMX 技术将模型从 FP32 转化为 BF16 后，第五代至强® 可扩展处理器的推理性能提升达 3.44 倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/2467069be4592566ddc501966eca28e2.webp" /></p><p>*自研打关模型推理性能测试数据</p><p></p><p>乐元素还在《开心消消乐》中引入了新春扫龙字活动，在玩家上传扫描的图片后，乐元素会通过图像分类识别领域常用的 ResNet-50 模型进行图片识别并返回结果。该模型在第五代至强® 可扩展处理器上的测试结果表明，启用了英特尔® AMX 后推理性能提升高达 5.19 倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2f475fd0246fe87b94267a22c3d41d2.webp" /></p><p>*《开心消消乐》新春扫龙字活动模型测试数据</p><p></p><p>除了硬件加持以外，英特尔®&nbsp;oneDNN 还提供了深度学习构建块的高度优化实现，深度学习应用程序和框架开发人员可以对 CPU、GPU 或两者使用相同的 API，从而抽象出指令集和其他复杂的性能优化，大大降低编程人员优化 AI 推理性能的难度。</p><p></p><p>从以上实践案例不难看出，启用基于第五代英特尔® 至强® 可扩展处理器的新一代腾讯云实例 S8 后，开发厂商能游刃有余地应对自动打关等模型的推理需求，提升游戏开发和运营效率。开发厂商也很容易实现模型扩展，在更多环节引入 AI 技术，满足更多场景的需求。</p><p></p><p>通过部署第五代英特尔® 至强® 可扩展处理器的腾讯云实例，乐元素无需采用昂贵的专用 AI 服务器，还可以快速根据市场需求进行扩展，使企业在保持轻资产、轻运营压力的同时获得更高的投资回报率。</p><p></p><p>对于乐元素这样缺少大规模自建 AI 集群的企业而言，基于第五代至强® 可扩展处理器的腾讯云实例，让他们能够快速享受 AI 技术创新带来的价值，进而为广大终端用户带来更满意的产品和服务体验。</p><p></p><p></p><h2>第五代英特尔®&nbsp;至强®&nbsp;可扩展处理器，为游戏行业 AI 创新注入持续动能</h2><p></p><p></p><p>如今，AI 技术已经成为游戏产业发展的热门技术方向。一份研究报告预计，2024 年 AI 技术应用将为游戏公司带来约 21% 的人力成本下降。在此背景下，构建面向游戏开发与运营的 AI 算力平台，推动 AI + 游戏应用的创新，正在成为影响游戏公司竞争力的关键因素。</p><p></p><p>乐元素的实践证实，基于第五代英特尔® 至强® 可扩展处理器的腾讯云实例 S8 能够满足典型 AI 模型在推理算力上的需求，同时具备更高的经济性与灵活性，能够成为游戏企业拓展 AI 应用的理想选择。在当前合作成果的基础上，英特尔将与腾讯云和乐元素展开更多合作，加快步伐，将 AI 融入到游戏开发与运营的整体流程之中。英特尔与腾讯云的成果也将惠及更多游戏企业，持续为他们提供助力，满足轻资产、重人力类型的游戏厂商在激烈的竞争环境中降本增效的迫切需求。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/12Gp4CcXahTrm4Iy3XYL</id>
            <title>4人团队，如何用大模型创造近千万业务价值？｜AICon</title>
            <link>https://www.infoq.cn/article/12Gp4CcXahTrm4Iy3XYL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/12Gp4CcXahTrm4Iy3XYL</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 07:05:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 张源源, LLMOps, MLOps, 大模型
<br>
<br>
总结: 本文介绍了百姓车联数据科学与数据平台高级总监张源源对LLMOps的定义和应用。LLMOps作为一种新概念，与MLOps有着不同的特点和目标人群。文章还探讨了LLMOps在车损互助行业的具体应用案例，展示了大语言模型在解决业务问题上的潜力和价值。 </div>
                        <hr>
                    
                    <p></p><p>采访嘉宾｜张源源&nbsp;百姓车联数据科学与数据平台高级总监</p><p></p><p>编辑 |&nbsp;李忠良</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6d0692b56574f9c886c695824f6c41f.jpeg" /></p><p>大模型已经融入千行百业，在这个背景下，LLMOps 作为一种新概念，其定义、实践以及应对挑战成为了关注焦点。为了深入探讨 LLMOps 的意义和关键，我们采访了百姓车联数据科学与数据平台高级总监张源源，他分享了 LLMOps 在车损互助案例中的应用以及所面临的挑战与解决方案。以下是他的访谈实录。</p><p></p><p>InfoQ：现在其实大家 MLOps 都还没有搞得特别好，马上就出来了 LLMOps，当然也就没有特别标准的定义，在您看来 LLMOps 如何定义？它包含哪些内容？LLMOps 与 MLOps 您觉得两者较大的区别是什么？</p><p></p><p>张源源：这次 AICon 分享的第一部分，就会给出我对这部分的理解。简单来说，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d5/d5f9fd088326cb8f12d2939b9bf366bd.jpeg" /></p><p></p><p>● MLOps 用于管理 ML 应用的全生命周期，包括数据收集和处理、模型的训练、评估、部署和监控等，虽然会涉及跟多个工种打交道，但相关产品主要使用对象是从事 ML 算法开发工作的人员，比如 data scientist、算法工程师等等。</p><p></p><p>● 关于 LLMOps，我这里先提供三种对 LLMOps 的三种视角，通过比较这三种视角，可以更好了解 LLMOps 是啥。</p><p></p><p>● 一种视角认为 LLMOps 是 MLOps 在 LLM 场景下的直接迁移。主要使用对象还是算法工作人员。这种视角里认为的 LLM 全生命周期更多还是强调训练大模型的过程，对有了大模型之后如何做应用，其实覆盖的比较少。这种视角在某些之前对 MLOps 有过了解甚至投资过但对 LLM 应用开发没那么熟悉的 VC 那里很流行。</p><p></p><p>● 另外一个知名项目 LangChain 提供了不一样的视角，它推出了号称是 LLMOps 的 LangSmith，它更多关注有了大模型之后如何开发大模型应用。可以从他们的产品设计理念里非常关注实验管理等等相关 feature，有很强的 data science 思维，但目标客户已经不局限为算法工作者，很多业务开发者借助它已经能很高效的完成应用开发。</p><p></p><p>● 作为当下世界范围内风头最劲的 LLMOps 之一，也是我们国内开发者做出来的良心制作，Dify 同样更多关注有了大模型之后如何开发大模型应用的问题，但目标客户主要是无代码、低代码群体。</p><p></p><p>● 通过后面这两种视角，其实可以看出 LLMOps 不应只是 MLOps 在 LLM 场景下的直接迁移。有了这三个视角的铺垫，其实通过直接对比 MLOps 和 LLMOps，容易给出更符合我们认知的 LLMOps 定义。</p><p></p><p>○ 从覆盖流程上说，对于 MLOps 来说，开发模型和模型应用往往是等价的，模型上线往往等于模型应用上线，想象一下各种推荐算法的开发和上线过程，但是对于 LLMOps 来说，开发 LLM 和后续的模型应用是分离的，都不是一波人，甚至都不是一个公司的人，开发 LLM 和模型应用在技术栈上迥异。</p><p></p><p>○ 从目标人群上说，对于 MLOps 产品来说，因为开发模型和模型应用都是同一批人，它的目标人群就是算法工作人员，对于 LLMOps 产品来说，开发模型相关的 LLMOps 的目标人群仍然是算法工作人员，但模型应用相关的目标人群就丰富多样了，除了算法工作人员，无代码、低代码偏好人群、业务开发人员也是他们的目标人群。</p><p></p><p>○ 从产品形态上说，也是类似，MLOps 和以开发模型为主的 LLMops 产品形态主要是 SDK/Library/API 等易于已有技术栈集成的方式，而模型应用相关的 LLMOps 增加了拖拉圈选等无代码操作。</p><p></p><p>○ 所以基于前面分析里提到的开发 LLM 和后续的模型应用是分离的事实，我们就给出了 LLMOps 合理的定义，即 LLMOps= 开发模型 LLMOps+ 模型应用型 LLMOps。开发模型类 LLMOps 往往有另外一个名字 AI infra，更多关注大模型训练过程的效率、效果等问题。模型应用类 LLMOps 更关注有了 LLM 之后，如何开发 LLM 应用。而开发模型类 LLMOps 其实也跟前面 MLOps 产品遇到的商业上的问题一样，可能会遇到有很多定制化需求而需要用到的公司往往会自研的问题，当然因为当前相关领域人才供给严重不足，不是所有公司都有这样的能力，还是有不少机会；但对于模型应用类 LLMOps 来说，受众很广，也能解决当前应用落地门槛高的痛点问题，如果能聚集起大量的开发者，有了网络效应，是有很高的商业价值的，甚至可以成为大模型的分发入口。特别需要指出的是，在接下来我分享的 context 下，我们所说的 LLMOps 是后者，也就是更多关注模型应用这块的 LLMOps。</p><p></p><p>LLMOps 在车损互助行业的应用案例</p><p></p><p>InfoQ：在哪些环境中，车损互助使用到了大语言模型？</p><p></p><p>张源源：车损互助全流程都在使用，每一次深入跟业务侧沟通需求都能感觉到可以用大语言模型解决很多业务问题，下面这张图是我们 3 个月之前的规划。我们也做了大量创新的工作，比如我们产品负责人之前发表过一篇我们用大模型去解决准入报价里 VIN 匹配的问题，当时在圈子内引起了一个小轰动，很多人都跟我打听是怎么做的；</p><p></p><p>再比如，我们规划了用大模型去做智能理赔定损 agent，通过几张照片和报案信息，就能给出来带价格的维修单，会涉及非常多大模型能力应用的子问题，很多人都对这块非常好奇也非常好看，这个对汽车维修行业来说带来的影响非常大，如果能做好，预期创造的业务价值非常高；</p><p></p><p>还有，我们最近搞得 text2data 工作，如果你之前对 text2sql 有过了解，你会发现这个工作从原理上就比 text2sql 靠谱非常多，通过我们在埋点、ad hoc query 方面的落地实践，可以说对于真实场景的取数需求来说，可以说已经完全不需要工程师介入了，我们自己的数仓工程师做完这个项目就自己说感觉数仓这个职位要不存在了。</p><p></p><p>我们最近也想到了其他更多应用场景，比如用 phone agent 去帮忙做第一轮面试筛选、服务质量反馈、用户报案问题收集（不仅仅通过 chatbot，还是有很多用户习惯用 phone 去报案）。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4e/4eae5e93e236e5269b35f5e936beeef7.png" /></p><p></p><p>InfoQ：您可以分享下，您这边采用的基础模型是什么吗？</p><p></p><p>张源源：我们一直是选择最好的模型，根据特定的场景选择特定的模型，比如大多数时候选择 GPT4，在代码生成相关的使用 Claude3，我们也是评测和对比了很多选择。在现阶段我们场景里，推理价格不是我们优先考虑项，效果是最优先考虑的。</p><p></p><p>InfoQ：在哪些场景中使用了 LLM？如何引导大语言模型输出您期望的结果？</p><p></p><p>张源源：场景如上图，在车损互助的准入报价、理赔定损、日常运营、内部提效等等场景都有应用。在引导大模型输出期望结果这块，我们最重要的经验就是确定性的交给确定性的去做（比如能调用 API 搞定的就直接调用 API，比如多用 workflow，把 zero shot 调用大模型，拆解成多个确定性节点和几个调用大模型的节点），剩下的才交给大模型；另外一个经验是，团队一定要有有实验思维、懂数据科学的人，才能把这个事情真正做好。</p><p></p><p>InfoQ：如何评估大模型的回应呢？是好的还是坏的？</p><p></p><p>张源源：首先去看自己的 task 是不是已经有 benchmark，比如你搞的是翻译类任务，这种肯定有很丰富的 benchmark，直接去看模型在这些 benchmark 上的表现，或者去关注一些大模型的 technical report 以及 lmsys 等的 leaderboard，当然除了这些，还可以自己构建评测集合，让领域专家或者大模型本身帮你标注这些结果好坏，这个时候类似 Dify 这样的 LLMOps 就提供了非常好的标注回复功能，能提供很好的支持。当然，这也是我上面说的，团队一定要有有实验思维、懂数据科学的人，他好去设计实验 pipeline，以及评测模型和各种配置的好坏。</p><p></p><p>InfoQ：底层 API 模型的持续变化会对输出结果的影响也是非常大的，如何处理这些情况呢？</p><p></p><p>张源源：无他，就是做实验，在 benchmark 和自己的评测集合上做实验，根据效果好坏来决定是否切换。</p><p></p><p>InfoQ：除去输出的期望问题，还有哪些挑战是您这边遇到的？又是如何解决的？</p><p></p><p>张源源：总体来说，遇到的挑战还好，哪里不会学哪里，比较享受这种遇到问题就解决问题的感觉吧，如果非要说挑战，主要有两个吧，一个是 RAG 这部分，现在市面上的方案还没有达到预期，核心我觉得是当前是工程的人搭起来架子，但是对效果提升有帮助的算法相关人才跟进还不够以及还没有整合到主流工程里去，这部分也呼吁更多信息检索相关的人杀入这个领域，机会很大，低处果实也很多，另外一个更大的挑战就是一直要 catch up 最新进展，有太多东西需要深入学习和 research，时间总是不够用的感觉。</p><p></p><p>InfoQ：在搭建与使用 LLMOps 过程中，您这边一共有多少人参与？为团队带来哪些收益呢？</p><p></p><p>张源源：据我们内部初步估计，各个场景第一年创造的业务价值预计近千万，这还是考虑我们第一年用户量不够大、很多合作伙伴 API 还没有如期接入的情况，而且有很多用户体验方面的价值无法用金额直接衡量，我们公司是志在用 AI 作为核心竞争力在海外做一款颠覆性的车损互助产品。拿到这个业务结果，背后主要是三点，第一就是我们对大模型的认知足够，第二就是对业务场景问题深入去思考，第三就是借助 LLMOps 让我们低成本做实验和验证，整个过程，核心参与人员就四五个人。</p><p></p><p>安全性和合规性问题</p><p></p><p>InfoQ：鉴于车损互助行业可能涉及到用户个人信息和交易数据等敏感信息，您是如何确保模型对这些信息进行合规处理的？</p><p></p><p>张源源：我们目前的应用场景还没有太多涉及，有一两个场景里有这种问题，但是也不严重，也就是用户上传车损照片，这些都可以通过免责申明加上产品手段去解决，也就是说在用到大模型之前就解决掉了，尽量不在大模型这里进行解决。</p><p></p><p>未来的发展方向和预测</p><p></p><p>InfoQ：随着技术的不断发展，您对 LLMOps 的未来发展有何预测？比如在模型自动化、自适应性、实时性等方面的进展。</p><p></p><p>张源源：这部分在分享里也会涉及，应用类 LLMOps 主要在解决降低门槛、提高可集成性、提高可观测性、提升效果和效率这几个问题。</p><p></p><p>● 在降低门槛方面，当前以 Dify、Coze 为代表的应用开发类 end2end 的 LLMOps 极大的降低了普通人开发 LLM 应用的门槛，意义重大，甚至因为这一点，LLMOps 现阶段的流量入口价值和分发价值都被低估了。</p><p></p><p>● 在提高可集成性方面，通过 API 把 LLM 应用作为整体跟其他系统对接的方式还不够，还需要节点级别的对接方式，workflow 的 http 节点有一定帮助，但还不够，比如往往没有全局 memory。当前主流 LLMOps 更多思考的是新创建的应用，但市面上更主流的应用场景是需要跟已有系统进行集成，提高可集成性能极大提高 LLMOps 的上限。</p><p></p><p>● 在提高可观测性方面，当前 LLMOps 做的还不够好，比如很多还不支持版本控制，tracing 做的也不够好。</p><p></p><p>● 在提升效果和效率方面，当前 LLMOps 做的也还不够，效果和效率其实也是在落地过程中，用户最在意的点，但大模型的自身能力缺陷在没有正确使用大模型经验的普通人那里被放大，导致大模型落地差强人意。期望 LLMOps 能够对于有能力的人，提供更多集成其他优秀解决方案的机会，甚至这本身也是商业机会。对于没有能力的人，应该提供更好的经过广泛证明的默认选项。</p><p></p><p>嘉宾介绍</p><p></p><p>张源源：<a href="https://aicon.infoq.cn/202405/beijing/presentation/5831">百姓车联 AI/Data 方向负责人</a>"，中国人民大学校外导师，中国商业统计学会常务理事，数据科学社区统计之都常务理事。长期跟踪 AI/Data 方向前沿技术发展，发表了多篇 AI 方向顶级 Paper，有多项相关专利；在百度、阿里、百姓车联等多家赛道内头部公司有过行业内开创性的工作，在 AI/Data 方向有超过 10 年的积累。目前正在百姓车联带领团队开发车损互助行业首个基于大模型的智能车损互助系统。</p><p></p><p>活动推荐：</p><p>随着大模型在企业中的实践日益增多，企业界对大模型应用的探索和需求也在不断增长。为了满足这一需求，InfoQ精心策划的AICon上海站即将盛大开幕。活动定于8月18日至19日举行，届时将有12个专题论坛，汇聚50余家企业的AI落地案例分享。这些案例覆盖了从Agent技术、RAG模型、多模态交互到端侧智能和工具链构建等多个领域，为企业提供丰富的实践视角和启发。更多内容可点击 <a href="https://aicon.infoq.cn/202408/shanghai/">AICon 上海</a>"查看。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RM6r2WxamGOb9DmIgBtQ</id>
            <title>哈佛退学本科生开发史上最快芯片；居然之家汪林朋：AI时代名校毕业生不如厨师司机，北大的到我那就八千元；英伟达高层频频套现｜Q资讯</title>
            <link>https://www.infoq.cn/article/RM6r2WxamGOb9DmIgBtQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RM6r2WxamGOb9DmIgBtQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 01 Jul 2024 06:23:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 00后, 哈佛, Transformer, 加速芯片
<br>
<br>
关键词: 机器人, 名校毕业生, 工资, 人工智能
<br>
<br>
关键词: OpenAI, ChatGPT, 推迟发布, 语音模式
<br>
<br>
关键词: TikTok, 美国政府, 法案, 甲骨文
<br>
<br>
总结: 00后哈佛华裔辍学生开发Transformer专用加速芯片；居然之家汪林朋认为机器人取代的是名校毕业生，工资高于硕士博士；OpenAI推迟发布ChatGPT语音模式，但推出MAC端桌面版；甲骨文担心美国政府法案对其业绩造成损害；钉钉将对所有AI大模型厂商开放，建立开放的人工智能生态环境。 </div>
                        <hr>
                    
                    <p></p><blockquote>00 后哈佛华裔辍学生开发 Transformer 专用加速芯片；&nbsp;居然之家汪林朋：机器人取代的就是名校毕业生；OpenAI 推迟发布 ChatGPT 语音模式；甲骨文：美国政府法案将损害我们的业绩；钉钉将对所有 AI 大模型厂商开放；腾讯发布暑期未成年人限玩日历；谷歌将推出明星网红 AI 聊天机器人；英伟达一夜暴跌近 7%；OpenAI 突然宣布中止服务；Windows 11 预览更新 KB5039302 会导致启动问题；谷歌不再开发 Material Web Components 项目……</blockquote><p></p><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>00后哈佛华裔辍学生开发Transformer专用加速芯片，比英伟达H100快20倍</h4><p></p><p>6月27日，据财联社报道，一家叫做Etched的硅谷初创公司凭借其用于AI的ASIC芯片，从最底层的架构层面为主流AI大模型公司所采用的Transformer计算提供更优性价比的选择，在AI硬件领域掀起了波澜。</p><p></p><p>Etched由**两个从哈佛退学的00后本科生，**Gavin&nbsp;Uberti和Chris&nbsp;Zhu于2022&nbsp;年创立，他们开发了一款名为Sohu的专为Transformer模型设计ASIC芯片。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/268c2772782a83028148ca2fb7652290.png" /></p><p></p><p>Etched声称，Sohu芯片推理Llama-3&nbsp;70B的速度比英伟达的H100快20倍，而功耗却大大降低。</p><p></p><p>Etched刚刚获得了1.2亿美元的新融资，由&nbsp;Primary&nbsp;Venture&nbsp;Partners&nbsp;和&nbsp;Positive&nbsp;Sum&nbsp;Ventures&nbsp;领投，Peter&nbsp;Thiel、Github首席执行官Thomas&nbsp;Dohmke和前Coinbase首席技术官Balaji&nbsp;Srinivasan等知名投资者也参与了本轮融资。</p><p></p><h4>居然之家汪林朋：机器人取代的就是名校毕业生，厨师司机工资远高于硕士博士</h4><p></p><p>近日，在亚布力中国企业家论坛第十届创新年会上，居然之家创始人兼董事长汪林朋先生发表了关于人工智能时代的深刻见解。汪林朋表示，AI现在是一个热点话题，全世界都在谈论人工智能。在其看来，人工智能是人类第四次革命，“这个革命非同一般，它有可能决定人类的命运，甚至人类的存亡”。</p><p></p><p>汪林朋还提到，能用双手劳动的人不会被人工智能取代，因为不可能所有东西都用机器代替，否则成本太高了。“今天我们说这个人没文化、没学历，羡慕别人家的孩子是名校毕业的，但是机器人恰恰取代的就是他们”，汪林朋说，“以后那些没上学的，现在我们很典型的，厨师、司机的工资远远高于一个研究生、博士生的工资，否则就没人给我做饭，没人给我开车了”。</p><p></p><p>“我们装修房子也是一样，一个定制的工人，在北京他们一个月的月薪至少2万块钱。但是一个大学生才多少工资呢？北大毕业的到我那也就8000块钱。所以以后能用自己的双手去劳动的人，这是人工智能时代需要的”，他说。</p><p></p><p>这一番话犹如投石入水，激起千层浪。</p><p></p><p>就在去年的亚布力中国企业家论坛上汪林朋就表示，居然之家为了降低成本，提高运营效率，他已经裁掉了包括CTO在内的整个IT部门。这一消息引起了业界的广泛关注和热议。居然之家作为家居行业的领军企业，近年来在市场份额、品牌影响力等方面取得了显著的成绩。然而，随着市场竞争的加剧，居然之家也面临着诸多挑战。为了应对这些挑战，汪林朋决定采取一系列措施，其中就包括裁员。</p><p></p><h4>OpenAI推迟发布ChatGPT语音模式，但MAC端桌面版ChatGPT上线</h4><p></p><p>6月26日凌晨，OpenAI在社交平台宣布，推迟GPT-4o语音模式，还需要一个月的时间来完善产品。预计今年秋天，所有ChatGPT&nbsp;Plus用户都可以使用该功能。</p><p></p><p>OpenAI原本的计划是在6月底开始向一小部分ChatGPT&nbsp;Plus用户提供测试版本，但因为产品还有安全、性能、算力等方面的问题需要调整，所以推迟了发布时间。</p><p></p><p>OpenAI还在今天发布了面向macOS系统的桌面版ChatGPT，支持上传文件、搜索对话、图像解读等多种功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c6a01cae9edc25aa604b78d9ff8c560.png" /></p><p></p><h4>TikTok"不卖就禁"？甲骨文：美国政府法案将损害我们的业绩</h4><p></p><p>据财联社6月25日报道，美东时间周一，美国软件巨头甲骨文公司在向美国证监会提交的财年年报中承认，拜登政府针对TikTok所提出的“不卖就禁”法案可能会损害甲骨文公司财务业绩。</p><p></p><p>今年4月24日，美国总统拜登签署一项法案，法案中涉及强制字节跳动剥离旗下应用TikTok在美业务。在相关条款中，字节跳动被限期在九个月左右时间内剥离其在美业务，否则将面临全国性禁令。甲骨文在其年度报告中明确写道，美国总统拜登4月签署的这项法律“将使得其向TikTok提供互联网托管服务成为非法行为”，并令甲骨文公司的“收入和利润受到不利影响”。</p><p></p><p>甲骨文警告称，若无法继续向TikTok提供互联网托管服务，其收入和利润将受不利影响。TikTok是甲骨文云基础设施业务的最大客户之一，分析师估计甲骨文从TikTok获得的年收入可能在4.8亿至8亿美元之间。</p><p></p><h4>钉钉将对所有&nbsp;AI&nbsp;大模型厂商开放</h4><p></p><p>6月26日，北京举办了“Make&nbsp;2024钉钉生态大会”。会议核心，钉钉宣布全面开放给各大模型厂商，旨在建立中国最为开放的人工智能生态环境。此举措已吸引MiniMax、月之暗面、智谱AI、猎户星空、零一万物、百川智能在内的六家顶尖大模型企业加入钉钉生态体系。</p><p></p><p>钉钉的生态伙伴队伍已然壮大至5600余家，其中专注于AI领域的伙伴超过了100家，而钉钉平台上的AI功能日均调用次数更是突破了1000万大关。</p><p></p><p>钉钉总裁叶军表示，模型开放是钉钉生态开放战略的再进一步。一方面，随着行业从模型创新走向应用创新，钉钉需要探索大模型的更多应用场景。钉钉拥有大量企业客户，数据优势与场景优势叠加，和大模型之间彼此需要。另一方面，钉钉上的大企业客户也对模型开放提出要求。</p><p></p><p>另外，据新浪科技报道，叶军于6&nbsp;月&nbsp;22&nbsp;日亚布力中国企业家论坛第十届创新年会发表了演讲，叶军在演讲中直言，OpenAI&nbsp;推出&nbsp;ChatGPT&nbsp;之后，百度可能就没什么用了。他表示，百度搜出来的结果是&nbsp;10&nbsp;条记录，甚至是&nbsp;10&nbsp;条差不多的广告。但&nbsp;ChatGPT&nbsp;得出的答案“一条就是准确答案”且没有广告。“我当时的第一感觉，就是这个交互要变了。”</p><p></p><p>叶军还顺势提到了小红书。他认为，搜索场景已经“被变革掉了”，百度也得马上跟进。“如果再不跟进，我估计你们只会用小红书，不会用百度了，小红书肯定要用，因为是阿里投资的，这也是不错的一个产品。”</p><p></p><h4>腾讯发布暑期未成年人限玩日历：总时长不足24小时</h4><p></p><p>6月26日，腾讯游戏发布《关于2024年暑假期间未成年人游戏限玩的通知》。</p><p></p><p>2024年7-8月期间，未成年人可在每周五、周六和周日的晚上20:00至21:00点期间登录游戏，暑假55天的游戏时长合计23小时。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a36c695af5a16c8f217d59a3f941d26.png" /></p><p></p><p>此外，除了“限时限充”和“人脸识别”，今年暑假，腾讯也将为家长用户提供&nbsp;“防沉迷四件套”管理工具，包含一键禁玩禁充、自我账号管理等功能，协助家长约束孩子的游戏行为。</p><p></p><h4>谷歌将推出明星网红&nbsp;AI&nbsp;聊天机器人，与&nbsp;Meta&nbsp;竞争</h4><p></p><p>6&nbsp;月&nbsp;25&nbsp;日消息，根据&nbsp;The&nbsp;Information&nbsp;爆料消息，谷歌正在基于明星和&nbsp;YouTube&nbsp;网红构建新的&nbsp;AI&nbsp;聊天机器人。</p><p></p><p>这个想法并不是谷歌首创的，目前包括&nbsp;Character.ai&nbsp;这样的初创公司，以及像&nbsp;Meta&nbsp;这样的大公司已经推出了类似的产品。</p><p></p><p>有爆料称，谷歌的明星网红&nbsp;AI&nbsp;聊天机器人将由该公司的&nbsp;Gemini&nbsp;大语言模型提供支持。该公司还在尝试与有影响力的明星网红建立合作伙伴关系，并且还在开发一项功能，让人们只需描述自己的个性和外表就可以创建自己的聊天机器人，类似&nbsp;Character.ai&nbsp;的做法。Character.ai&nbsp;的联合创始人之一&nbsp;Noam&nbsp;Shazeer&nbsp;就曾担任谷歌工程师，他也是&nbsp;AI&nbsp;基础技术“transformers”的创造者之一。</p><p></p><p>目前尚不清楚谷歌可能与哪些明星网红人合作。Meta&nbsp;聊天机器人的合作对象包括&nbsp;TikTok&nbsp;网红&nbsp;Charli&nbsp;D'Amelio、YouTube&nbsp;网红&nbsp;Mr.&nbsp;Beast、歌手&nbsp;Snoop&nbsp;Dogg、美国橄榄球运动员&nbsp;Tom&nbsp;Brady&nbsp;和模特&nbsp;Paris&nbsp;Hilton&nbsp;等，而&nbsp;Character.ai&nbsp;的人物则包括政治家、哲学家、虚构人物，甚至可以是一块会说话的奶酪。</p><p></p><h4>英伟达一夜暴跌近7%，市值三日蒸发4万亿元，高管频频套现</h4><p></p><p>当地时间6月25日，美股收盘涨跌不一，道指上涨260点。热门中概股涨跌不一，纳斯达克中国金龙指数（HXC）上涨1.3%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8aa140857987df04d1e738ab8d9b67b7.png" /></p><p></p><p>英伟达重挫6.7%，创两个月最大跌幅，拖累纳指走低。该股连续第三个交易日大幅下跌，从近期高点已经下跌了超过16%，市值不足3万亿美元，跌入回调区域。</p><p></p><p>英伟达三天来市值累计蒸发约4300亿美元，**创下史上单一上市公司三天市值跌幅之最。**其市值目前回到3万亿美元以下，低于微软和苹果的市值。</p><p></p><p>在上周成为全球市值最高公司之后，投资者选择获利了结，英伟达首席执行官黄仁勋也在抛售股票。根据美国证券交易委员会的文件，黄仁勋在6月13日至21日期间累计减持了72万股英伟达股票，套现金额达9460万美元。此外，英伟达的首席财务官Colette&nbsp;Kress及其他高管也在减持。</p><p></p><p>Allspring&nbsp;Global&nbsp;Investments投资组合经理兼Empiric&nbsp;LT&nbsp;Equity团队负责人Neville&nbsp;Javeri认为：“在短期内，投资者可能会开始对人工智能产生疲劳，或者更担心指数集中度。”尽管股价大跌，但英伟达今年涨幅超过140%，在标普500指数成分股中排名第二，仅次于另一家人工智能股Super&nbsp;Micro&nbsp;Computer&nbsp;Inc．。</p><p></p><h4>OpenAI突然宣布中止服务&nbsp;，包括中国</h4><p></p><p>北京时间本周二凌晨，陆续有开发者在社交媒体上表示，他们收到了来自&nbsp;OpenAI&nbsp;的邮件，表示将采取额外措施停止其不支持的地区的&nbsp;API&nbsp;使用。</p><p></p><p>根据网上流传的邮件截图，OpenAI&nbsp;表示：“根据数据显示，你的组织有来自&nbsp;OpenAl&nbsp;目前不支持的地区的&nbsp;API&nbsp;流量。从&nbsp;7&nbsp;月&nbsp;9&nbsp;日起，我们将采取额外措施，停止来自不在&nbsp;OpenAI&nbsp;支持的国家、地区名单上的&nbsp;API&nbsp;使用。”</p><p></p><p>在&nbsp;OpenAI&nbsp;给出的“支持访问国家和地区”名单上（<a href="https://platform.openai.com/docs/supported-countries">https://platform.openai.com/docs/supported-countries</a>"），中国、俄罗斯、朝鲜、叙利亚、伊朗等地均未在列。</p><p></p><p>实际上，OpenAI&nbsp;早先就对中国大陆地区的用户实行了注册门槛，限制了其对&nbsp;ChatGPT&nbsp;服务的访问权限。中国大陆的开发者群体在构建基于&nbsp;OpenAI&nbsp;API&nbsp;的衍生服务时，往往需要通过代理服务器或在海外部署反向代理机制。这不仅增加了运维成本，也无法保证服务的稳定性。</p><p></p><p>OpenAI&nbsp;的这一决策立刻引发了国内大模型厂商的回应，各厂商纷纷表示可以支持企业“无痛”迁移。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21602b2fc5ed7bb3b9144045b35317f5.jpeg" /></p><p></p><h4>完美世界被传大规模裁员，回应称调整阵痛期</h4><p></p><p>从6月24日开始，关于“完美世界最大规模裁员“的消息开始在社交平台流传。</p><p></p><p>有爆料称，完美世界大规模裁员超千人，部分研发部门减少百人，中台减至几十人。另外，该消息还透露完美世界新押注的项目《完美新世界》和《一拳超人》已被暂停。</p><p></p><p>另外，有多家媒体报道称，有员工透露，完美世界裁员进程愈演愈烈，从起初搬空的零星几个工位演变成整层的空位，甚至到最后不包括食堂的三栋大厦中几乎搬空了两栋。员工直言，公司剩下的项目可能一只手都数得过来，“已经很难被称为大厂了”。</p><p></p><p>对此，完美世界方面回复中华网财经表示，为应对挑战，公司主动梳理调整，采取了一系列解决方案，其中包括优化资源配置、聚焦核心项目、进行必要的人员优化，以及办公空间集约化等，让资源更集中在核心优势业务上。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>Windows&nbsp;11&nbsp;预览更新&nbsp;KB5039302&nbsp;会导致启动问题</h4><p></p><p>6&nbsp;月&nbsp;27&nbsp;日消息，微软昨日发布了&nbsp;Windows&nbsp;11&nbsp;可选更新&nbsp;KB5039302，22H2&nbsp;用户安装后版本号升至&nbsp;Build&nbsp;22621.3810；23H2&nbsp;用户安装后版本号升至&nbsp;Build&nbsp;22631.3810。</p><p></p><p>此次更新带来了大量新功能，但同时也引入了一些新的&nbsp;Bug。微软刚刚更新了已知问题列表，确认&nbsp;KB5039302&nbsp;可能会导致某些设备可能无法启动，主要表现为反复重启。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43524ab53fade0cf61d5d19a0f32d8cf.jpeg" /></p><p></p><p>不过，Windows&nbsp;家庭版用户几乎不太可能遇到这一问题，因为这一&nbsp;Bug&nbsp;主要出在虚拟化环境中。</p><p></p><p>微软表示，此问题更有可能影响使用虚拟机工具和嵌套虚拟化功能（如&nbsp;CloudPC、DevBox、Azure&nbsp;虚拟桌面）的设备，相关团队正在调查以确定此问题可能触发的确切条件，并将在即将发布的版本中提供更新。</p><p></p><h4>ChatGPT推出以来，其写作风格已渗透超10%科学摘要中</h4><p></p><p>近日，一项对1400万篇&nbsp;PubMed&nbsp;摘要的分析显示，自&nbsp;ChatGPT&nbsp;推出以来，AI&nbsp;文本生成器已影响了至少10%&nbsp;的科学摘要，在某些领域和国家，这一比例甚至更高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/151021cd97bb79606dc56cfe4eb9e07a.png" /></p><p></p><p>来自图宾根大学和西北大学的研究人员对2010年至2024年间的1400万篇科学摘要进行了语言变化的研究。他们发现，ChatGPT&nbsp;和类似的&nbsp;AI&nbsp;文本生成器导致了某些风格词汇的大幅增加。</p><p></p><p>研究人员首先确定了2024年相比以往年份显著更频繁出现的词汇。这些词汇包括&nbsp;ChatGPT&nbsp;写作风格中典型的许多动词和形容词，比如&nbsp;“深入挖掘”、“复杂”、“展示”&nbsp;和&nbsp;“突出”&nbsp;等。</p><p></p><p>根据这些标志词，研究人员估计在2024年，AI&nbsp;文本生成器影响了至少10%&nbsp;的所有&nbsp;PubMed&nbsp;摘要。在某些情况下，这一影响甚至超过了&nbsp;“Covid”、“流行病”&nbsp;或&nbsp;“埃博拉”&nbsp;等词汇在其所处时期的影响。研究人员发现，在中国和韩国等国家的&nbsp;PubMed&nbsp;子组中，大约有15%&nbsp;的摘要是使用&nbsp;ChatGPT&nbsp;生成的，而在英国仅为3%。然而，这并不一定意味着英国作者使用&nbsp;ChatGPT&nbsp;较少。</p><p></p><h4>谷歌不再开发&nbsp;Material&nbsp;Web&nbsp;Components&nbsp;项目</h4><p></p><p>6&nbsp;月&nbsp;26&nbsp;日消息，据报道，谷歌将不再为&nbsp;Material&nbsp;Web&nbsp;Components&nbsp;(MWC)&nbsp;项目配备专职开发人员，并已调派原有工程团队至其他项目。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d62febc86b0b0da26916f2255c79b615.jpeg" /></p><p></p><p>MWC&nbsp;提供了一套&nbsp;Material&nbsp;3&nbsp;设计风格的组件库，涵盖按钮、悬浮按钮、图标按钮、复选框、卡片、对话框、分隔线、阴影、聚焦环、列表、菜单、进度条、单选框、涟漪效果、下拉选择框、滑块、开关、标签页以及文本框等常用元素，方便开发者在网站中快速应用&nbsp;Material&nbsp;Design&nbsp;风格。</p><p></p><p>尽管&nbsp;MWC&nbsp;1.0&nbsp;版本已于&nbsp;2023&nbsp;年&nbsp;10&nbsp;月发布稳定版，且原计划在&nbsp;2024&nbsp;年持续更新，但项目组本月宣布&nbsp;MWC&nbsp;将进入维护模式，停止后续新功能开发，既有路线图也将搁置。</p><p></p><p>谷歌方面表示，MWC&nbsp;项目本身并不会被废弃，只是谷歌&nbsp;Material&nbsp;Design&nbsp;团队不再投入专门人力进行开发。项目组正在探索继续开发新功能和组件的方法，包括寻找新的维护者等。</p><p></p><h4>iOS&nbsp;18突破限制，可以下载更大应用</h4><p></p><p>近日，iOS&nbsp;18突破了限制，iPhone&nbsp;从&nbsp;App&nbsp;Store&nbsp;下载的&nbsp;iOS&nbsp;应用安装包大小将由此前最高&nbsp;2GB，提高到了&nbsp;4GB。</p><p></p><p>此前，苹果为了防止单个应用占用过多存储空间，一直对&nbsp;iOS&nbsp;和&nbsp;tvOS&nbsp;应用的大小进行不超过&nbsp;2GB&nbsp;的限制。但随着应用（尤其是游戏）的不断发展，它们变得更加复杂，所需的存储空间也不断增大。</p><p></p><p>这意味着，未来的应用市场将可能出现更多功能全面、高质感、高交互性的大作应用，这对推动整个移动应用市场的发展和用户体验的提升具有积极意义。但是，iOS&nbsp;18&nbsp;突破限制无疑也是一把双刃剑，它为我们带来了更多的可能性的同时，也对手机内存提出了更高的要求。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>