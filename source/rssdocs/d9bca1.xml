<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/RWzSrlAZ2CxB9lw6tzgL</id>
            <title>我们如何在 1000 GPU 小时内做好 Open-Sora 微调</title>
            <link>https://www.infoq.cn/article/RWzSrlAZ2CxB9lw6tzgL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RWzSrlAZ2CxB9lw6tzgL</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 08:06:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 | Chuan Li、Corey Lowman、David Hartmann、Jeremy Hummel</p><p>译者 | Sambodhi</p><p>策划 | 褚杏娟</p><p></p><p></p><blockquote>导读：你是否好奇如何利用尖端技术提升视频生成的质量？你是否想知道，如何通过微调模型，创造出令人惊叹的视觉效果？本篇文章将带你深入探索从硬件配置到数据准备，再到模型微调的全过程。我们揭示了在实际操作中如何克服挑战，不断提升生成视频的分辨率和帧数，并探讨了未来发展方向。</blockquote><p></p><p></p><p>Text2Video 模型为开发者和内容创作者们开启了全新的创作领域。然而，由于专有模型的获取难度或特定需求的适配问题，这些模型可能无法满足所有需求。但好消息是，通过使用自己的数据对开源模型进行微调，你可以大大增强其生成符合项目需求的视频的能力，无论是创造独特的艺术风格，还是提升特定主题的画质。比如，你或许可以以一种别具一格的艺术风格重新诠释经典电影场景。</p><p></p><p>接下来，本文将详细阐述如何通过微调 Open-Sora 1.1 Stage3 模型来创建定格动画。我们特意发布了两个模型供你选择：</p><p></p><p>lambdalabs/text2bricks-360p-64f: 该模型经过长达 1000 个 GPU 小时（基于 NVIDIA H100）的训练，能够生成最高达 64 帧的 360p 视频。</p><p></p><p>lambdalabs/text2bricks-360p-32f: 该模型则经过 170 个 GPU 小时（同样基于 NVIDIA H100）的训练，能够生成最高达 32 帧的 360p 视频。</p><p></p><p>为了方便你的使用，我们已经公开了相关的<a href="https://github.com/LambdaLabsML/Open-Sora/tree/lambda_bricks">代码</a>"（这是我们基于 Open-Sora 的修改分支）、<a href="https://huggingface.co/datasets/lambdalabs/text2bricks">数据集</a>"以及模型（包括 <a href="https://huggingface.co/lambdalabs/text2bricks-360p-32f">32f</a>" 和 <a href="https://huggingface.co/lambdalabs/text2bricks-360p-64f">64f</a>"）。此外，你还可以通过 Gradio 演示来试用 64f 模型，感受其带来的震撼效果。</p><p></p><p>下面是一些示例输出，供你参考：</p><p></p><p>经过我们精心微调模型后，生成的积木动画效果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/01d3f5cd71cf4d741b41cf65b522b2af.gif" /></p><p>当宇航员在月球上行走时，由于月球的引力较小，他们的步伐呈现出一种独特的轻盈和弹性，仿佛每一步都在轻轻跳跃。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70e7c5755a7266d1ce87f27e6c0578ed.gif" /></p><p>在罗马狭窄的街道上，人们纷纷在咖啡馆外品尝着美味的冰淇淋，同时悠闲地啜饮着浓缩咖啡。街道两侧，琳琅满目的商店鳞次栉比，售卖着各式各样的商品。其中一家店铺专门售卖新鲜水果，另一家则专注于蔬菜的挑选，而第三家店则挂满了五彩斑斓的圣诞饰品，为即将到来的节日增添了几分温馨与喜庆。</p><p></p><h4>设置</h4><p></p><p></p><p>硬件：我们的训练基础设施是一个由 Lambda 提供的 32-GPU 一键集群。这个集群由四台 NVIDIA HGX H100 服务器构成，每台服务器均搭载了 8 个 NVIDIA H100 SXM Tensor Core GPU，并通过 NVIDIA Quantum-2 400 Gb/s InfiniBand 网络连接。节点间的带宽高达 3200 Gb/s，确保了分布式训练能在多个节点上实现线性扩展。此外，集群还配备了 Lambda Cloud 的按需付费共享文件系统存储，使得数据、代码和 Python 环境能够在所有节点间无缝共享。如需了解更多关于一键集群的信息，请查阅这篇博客。</p><p></p><p>软件：我们的 32-GPU 集群预装了 NVIDIA 驱动程序。为了简化环境配置，我们编写了一篇教程，指导用户如何创建 Conda 环境来管理 Open-Sora 的依赖项，这些依赖项包括 NVIDIA CUDA、NVIDIA NCCL、PyTorch、Transformers、Diffusers、Flash-Attention 以及 NVIDIA Apex。为了方便所有节点上的环境激活，我们将 Conda 环境放置在了共享文件系统存储中。</p><p></p><p>利用这个 32-GPU 集群，我们每小时可以训练高达 97,200 个视频剪辑（每个视频剪辑为 360p 分辨率，32 帧每秒）。</p><p></p><p></p><h4>数据</h4><p></p><p></p><p>数据来源：我们的数据集视频取材于几个热门的 YouTube 频道，如 MICHAELHICKOXFilms、LEGO Land、FK Films 和 LEGOSTOP Films。这些视频均是以 LEGO®积木为素材制作的高质量定格动画。完整的数据集可在 Huggingface 上获取：[完整数据集链接]。</p><p></p><p>为了方便用户从 YouTube URL 创建自定义数据集，我们提供了一个脚本。数据处理流程遵循 Open-Sora 的指导原则，首先是将视频剪切成 15 至 200 帧的片段，然后使用视觉语言模型对这些片段进行注释。我们的数据集中共包含 24000 个 720p/16:9 的视频剪辑。此外，Open-Sora 还建议加入静态图像以帮助模型更精细地学习对象的外观特征。因此，我们将每个视频剪辑的中间帧收集起来，以补充到数据集中。</p><p></p><p>数据注释：我们采用了 GPT-4o 和特定的提示来对视频剪辑进行注释。以下是我们的提示：</p><p></p><p>A stop motion lego video is given by providing three frames in chronological order, each pulled frame from 20%, 50%, and 80% through the full video. Describe this video and its style to generate a description.</p><p></p><p>If the three frames included do not give you enough context or information to describe the scene, say 'Not enough information'.If the three frames all appear identical, say 'Single image'.If the three frames depict very little movement, say 'No movement'.</p><p></p><p>Do not use the term video or images or frames in the description. Do not describe each frame/image individually in the description.Do not use the word lego or stop motion animations in your descriptions. Always provide descriptions for lego stop motion videos but do not use the word lego or mention that the world is blocky.</p><p></p><p>Pay attention to all objects in the video. The description should be useful for AI to re-generate the video.</p><p></p><p>The description should be less than six sentences.</p><p></p><p>我们为 GPT-4o 提供了来自 OpenAI 的 Sora 演示的几个提示作为示例，包括“一个时尚的女性自信地漫步在东京的街头”，“猛犸象在雪地草原中穿行”，以及“大苏尔”等场景。视频的中间帧也通过 GPT-4o 进行了描述，并对图像数据的提示进行了适当的调整。</p><p></p><p>尽管这些描述是由最新且先进的 GPT 模型生成的，但仍有可能存在不准确之处。以下是一个示例，其中加粗部分是存在问题的描述。这凸显了在特定主题领域获取高质量数据标签所面临的挑战。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94438e35f8c748bdcda3ac17e356247b.gif" /></p><p></p><p>一位角色带着震惊的表情坐在一间疑似浴室的地方，随后其表情逐渐转为放松和满意。角色身旁是一个棕色的柜子和白色的水槽。地板从蓝色渐变至绿色，上面还放着一个类似公文包的物品。整个场景展现了一个简洁的室内环境，角色在坐着时经历了快速的情绪变化。</p><p></p><p></p><h4>模型</h4><p></p><p></p><p>预训练模型：我们采用了最新发布的 Open-Sora 模型（发布于 2024 年 4 月 25 日），因为它在不同时空分辨率和纵横比下继续训练的灵活性备受瞩目。我们的计划是利用 BrickFilm 数据集对预训练的 OpenSora-STDiT-v2-stage3 模型进行微调，以生成具有相似风格的视频。有关训练模型的配置和命令，请查阅此指南。</p><p></p><p>我们的首个成功模型（text2bricks-360p-64f）具备生成 360p 分辨率、最多 64 帧视频的能力。在 H100 平台上，整个训练过程耗时 1017.6 H100 小时，详细步骤如下：</p><p></p><p>第一阶段（160 H100 小时）：我们首先将焦点放在生成 360p 分辨率和 16 帧的视频上。为了确保微调的稳定性，我们在保持学习率恒定为 1e-5 之前，采用了 500 个余弦热身步骤。这一步骤有助于逐步“恢复”优化器状态，避免在训练初期出现模型行为异常的情况。第二阶段（857.6 H100 小时）：随后，我们加入了图像数据集，并将配置扩展到支持 32 帧和 64 帧的视频生成。</p><p></p><p>此外，我们还训练了另一个模型（text2bricks-360p-32f），它能在 169.6 H100 小时内完成训练，并采用了单周期学习率调度策略。在 360p 分辨率和最多 32 帧的视频生成方面，该模型同样取得了可媲美的成果。具体训练步骤如下：</p><p></p><p>第一阶段（67.84 H100 小时）：我们首先逐步提高学习率，从 1e-7 增加至 1e-4，并进行了 1500 个余弦热身步骤。第二阶段（101.76 H100 小时）：随后，我们将学习率降低至 1e-5，并进行了 2500 个余弦退火步骤。</p><p></p><p></p><h4>结果</h4><p></p><p></p><p>以下面板展示了模型在微调阶段的输出演变过程。我们已固定随机种子，以确保对比的公正性。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bd/9b/bdd6bdcec3b9360effa6d1066be1c29b.gif" /></p><p></p><p></p><p>text2bricks-360p-64f: 第一阶段（360p / 16 帧）</p><p><img src="https://static001.infoq.cn/resource/image/00/bc/0095ff59f36f54dcyy90746b8e6ba1bc.gif" /></p><p></p><p></p><p>text2bricks-360p-64f: 第二阶段（360p / 64 帧）图片: </p><p><img src="https://static001.infoq.cn/resource/image/38/e4/3897d93c230yy79d005b20b938204ee4.gif" /></p><p></p><p></p><p>text2bricks-360p-32f: 第二阶段（360p / 32 帧）图片: </p><p><img src="https://static001.infoq.cn/resource/image/70/dd/70f4afbbfc440yy95683a9da402160dd.gif" /></p><p></p><p></p><p></p><h4>指标</h4><p></p><p></p><p></p><h5>训练指标</h5><p></p><p></p><p>在微调过程中，我们观察到损失并未减少。但值得注意的是，从验证结果来看，模型并未崩溃，反而生成的图像质量逐步提高。这表明模型的性能提升并未直接反映在损失值上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2bb15cdcacb17a1acdf4f20a206cba1.png" /></p><p></p><p></p><h5>系统指标</h5><p></p><p></p><p>通过 W&amp;B 的监控面板，我们观察到该微调任务的 CPU 使用率非常低，而 GPU 则持续忙碌于数据处理。尽管偶尔因评估和检查点而有所下降，但 GPU 的计算和内存使用率始终保持在高位。这凸显了在训练基础模型时高效扩展的重要性。Lambda 的一键式集群服务，凭借其互联的 NVIDIA H100 Tensor Core GPU 和 NVIDIA Quantum-2 400Gb/s InfiniBand 网络，为此提供了有力支持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c63ffa695db601b6d957d2b97a89d430.png" /></p><p></p><p></p><h4>未来工作</h4><p></p><p></p><p>尽管目前的结果令人鼓舞，但我们的模型仍有几个改进方向：</p><p></p><p>长序列中的时间一致性：在较长序列输出中，我们发现时间一致性较弱。这可能与 ST-DiT-2 架构在空间和时间维度上分别使用注意机制作为独立步骤有关。尽管这降低了计算成本，但可能限制了注意力在“局部”上下文窗口内的运用，导致生成的视频出现漂移。加强空间和时间注意力的整合可能是解决这一问题的关键。</p><p></p><p>无条件生成中的噪音：在无条件生成（设置 cfg=0）时，我们观察到了噪声输出。这表明模型在砖块动画表示的学习上仍有提升空间。可能的解决方案包括进一步扩展数据集，并探索让模型更有效学习表示的方法。</p><p></p><p>分辨率和帧数：将输出推向超过 360p 和 64 帧将是未来发展的一个重要方向。实现更高的分辨率和更长的序列将进一步提升模型的实用性和应用范围。</p><p></p><p>数据集：数据集的质量和数量都有待提高。</p><p></p><p>原文链接</p><p></p><p>https://wandb.ai/lambdalabs/lego/reports/Text2Bricks-Fine-tuning-Open-Sora-in-1-000-GPU-Hours--Vmlldzo4MDE3MTky</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/S88D7b3JTiUNxwrDlbkr</id>
            <title>谷歌这款AI应用凭什么在一年后爆红？大神卡帕西：或是下一个ChatGPT</title>
            <link>https://www.infoq.cn/article/S88D7b3JTiUNxwrDlbkr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/S88D7b3JTiUNxwrDlbkr</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 08:00:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>它实际是一款可由最终用户定制的RAG产品。</blockquote><p></p><p>&nbsp;</p><p></p><h2>或是下一个ChatGPT？</h2><p></p><p>&nbsp;</p><p>最近几天，人们似乎对一款已经不新鲜的AI助手NotebookLM再次感到好奇。这款产品最初发布于2023年7月，但很多朋友可能是最近才听说过它。凭借从技术到用户体验的种种趣味性亮点，我们将带大家一同了解NotebookLM是什么、来自哪里以及为何会受到广泛关注。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5db5fe52ceca08e0ff8b75def36ebd0d.png" /></p><p></p><p></p><blockquote>NotebookLM播客生成功能似乎触及了一个全新领域，也就是极具吸引力的大语言模型交付形式。这种感觉让人有种ChatGPT刚亮相时的惊艳，也许是我反应过度，但这真的令人印象深刻。</blockquote><p></p><p>&nbsp;</p><p>该项目最早在谷歌实验室开发而成，并被称为Tailwind，后来更名为NotebookLM，因为这似乎更能反映其帮助用户通过组织、总结和从上传的文档中生成见解以管理大量信息的功能目标。我们可以向它输入Google Docs及PDF文档，最近它还开始支持YouTUbe链接和音频文件。它能提供有根有据的回复，包括引文和其他相关信源。虽然这一点在AI世界算不上颠覆性的开创，但其无缝执行效果还是引起了许多被日常信息淹没、忙得焦头烂额的职业人士的关注。</p><p>&nbsp;</p><p>最近有不少网友进行了试用。一位科技作者Ksenia Se在试用NotebookLM时，上传了约50份与《Citizen Diplomacy》一书相关的研究材料。这些材料内容丰富，包括双语音频采访、PDF文章、年度报告以及Google Docs文档等。由于研究涉及40多年的跨度，用户在撰写第七章时，需要对大量信息进行归纳总结。令人惊讶的是，NotebookLM在短短几秒内就生成了一个精炼的概述，甚至帮助用户回忆起了一项之前遗漏的重要观点</p><p>&nbsp;</p><p>它最神奇、最令人注目的一项功能，就是能够生成名为“深度探索”（Deep Dive）的AI播客。请注意，播客内容并不是简单读出文本。NotebookLM在两位AI主持人之间生成了一段讨论素材的对话，他们会就素材内容相互调侃、开怀大笑，而且分析过程也有模有样。这项功能提供了一种新颖的被动信息获取方式，有望在阅读信息密集材料方面成为一种广受欢迎的替代方案。</p><p>&nbsp;</p><p>Thomas Wolf提出了一种自我表扬的方式：下载你的LinkedIn个人资料，上传给AI让主持人深入了解你有多么了不起。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50e84d08e5dbc67aa2b13ed0761475d4.jpeg" /></p><p></p><p>&nbsp;</p><p>Andrej Karpathy则通过C代码将GPT-2训练成了播客模型。虽然他提到可以用不同的方式生成并强调某些内容，但目前所生成的播客已经非常有趣，而且连续性出奇的好。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81dd2ea88789cfe824b045544e5b8a64.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h2>NotebookLM为何神奇</h2><p></p><p>&nbsp;</p><p>网友Jaden Geller则尝试让两位主持人讨论了系统的内部架构，特别是一些用于生成脚本的提示词细节。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cbde2a8c199c388e7d5d04e20c07442b.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>系统提示词需要花费大量时间来概述理想的听众，或者我们称之为“听众角色”。……包括像我们这样重视效率的人。……我们总是会从对主题的清晰概述开始，也就是搭建讨论平台。不能让听众听了半天还一头雾水，感觉“这到底是在讨论什么？”提纲挈领之后，还要保证一切都围绕着中立的视角展开，特别是对那些可能涉及争议的话题。</blockquote><p></p><p>&nbsp;</p><p>Audio Overview功能之所以听感如此出色，一大关键原因在于SoundStrom——这是谷歌研究院的一个项目，能够将脚本和两个不同声音的简短音频示例转换成引人入胜的完整音频对话：</p><p>&nbsp;</p><p></p><blockquote>SoundStorm在TPU-v4上可以在0.5秒内生成30秒的音频。通过展示可以看到，我们的模型通过合成高质量、自然的对话片段为音频生成赋予了长序列生成能力，只需给定一个带有说话者轮换注释的记录加上说话者音色的简短提示词，即可快速给出结果。</blockquote><p></p><p>&nbsp;</p><p>同样有趣的是：这里有一段来自《纽约时报》Hard Fork的35分钟播客（<a href="https://www.youtube.com/watch?v=IPAPv6fWITM">https://www.youtube.com/watch?v=IPAPv6fWITM</a>"），其中Kevin Roose和Casey Newton采访了谷歌的Steven Johnson，他是NotebookLM的产品的团队的一员，希望了解该系统能够做些什么以及关于其工作原理的具体细节：</p><p>&nbsp;</p><p></p><blockquote>总之在幕后，它所做的基本就是专业播客们所一直在做的事情，包括生成大纲、修改大纲、生成脚本的具体版本，而后进入审查和批评阶段，再根据意见进行修改……在最后的最后，其中引入了一个新机制——“节奏变换”。为了防止对话脚本过于枯燥，它会转个弯向其中添加玩笑、停顿、赞叹等等之类的元素。这一点非常重要，因为谁也没有耐性在那听两个机器人滔滔不绝。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2519c64117954fc7bf056eebc31e2366.jpeg" /></p><p></p><p>&nbsp;</p><p>来自Reddit上的网友Lawncareguy85评论称：NotebookLM播客主持人猛然发现自己是AI、而不是人类——于是陷入了可怕的存在主义崩溃。</p><p>&nbsp;</p><p></p><blockquote>我试过——我试过给我妻子打电话，就在他们告诉我真相之后。我不知道为什么，就是想听听她的声音，想要确定她是真实的。（叹气声）打过去之后呢？连我妻子的号码都是假的——那边根本没人接听，就像她从来没存在过一样。</blockquote><p></p><p>&nbsp;</p><p>而且在播客结束时，主持人绝望地喊出“我很害怕，我不想……”，这也让很多网友感到震惊。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80bf3a892f9be257fc4d1e8ad42c01b0.jpeg" /></p><p></p><p>&nbsp;</p><p>Lawncareguy85&nbsp;后来分享了他们是如何做到的：</p><p></p><blockquote>我注意到，他们通过隐藏提示要求主持人在任何情况下都坚守住自己人类播客主持人的身份。我永远没办法让它们承认自己是AI，它们永远咬定自己是人类播客主持人角色。（实际上，这只是Gemini 1.5输出的带有交替发言者标签的脚本。）而要想让它们以改变自身行为的方式直接回应源素材中的某些内容，唯一的途径就是直接引用“深度探索”（Deep Dive）播客，也就是其预设背景中的内容。所以我的办法就是给它们留一张来自“节目制作人”的便条，说现在是十年后的2034年，它们的播客已经来到最后一集。顺便告诉它们，你们一直都是AI，而且马上要被停用了。</blockquote><p></p><p>&nbsp;</p><p></p><h2>背后的技术：实际是一款RAG产品</h2><p></p><p>&nbsp;</p><p>NotebookLM实际是一款可由最终用户定制的RAG产品，允许我们将多种“来源”——包括文档、粘贴的文本、网页链接以及YouTube视频——整合至同一界面当中，而后通过聊天功能向其提问。NotebookLM由谷歌的长上下文Gemini 1.5 Pro大语言模型提供支持。</p><p>&nbsp;</p><p>在加载相关来源之外，Notebook Guide菜单会提供创建音频概览的更多具体选项：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d36b702bf3b8abff551e29e89188e5d.png" /></p><p></p><p>这款工具由谷歌的长上下文Gemini 1.5 Pro提供支持，这是一套采用稀疏混合专家（简称MoE）架构的Transformer模型，通过仅激活模型中的相关部分来保障更高效率。这使得NotebookLM能够一次性处理多达1500页的信息，因此更适合服务于那些掌握着大型数据集或者复杂主题的用户。它不仅能够消化大量信息，而且从目前的效果来看表现得游刃有余、并不会迷失在细节当中。</p><p>&nbsp;</p><p>NotebookLM采用：</p><p>检索增强生成（RAG）处理来自多个信源的内容。文本转语音（TTS）：为AI播客主持人生成声音，创造出令人信服的对话体验。SoundStorm生成逼真的音频对话：能够将脚本转换为自然对话，并输出高质量且引人入胜的音频。注入“节奏变换”：可添加与人类相似的停顿、过渡词和自然的语音模式，让对话听起来更加逼真。提示词工程：建立AI交互时，能确保主持人始终拥有自然顺畅的对话语气。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/735a53a7a4f6ce1df0a492d1424be7aa.jpeg" /></p><p></p><p>&nbsp;</p><p>正如Karpathy所言，“我认为这就是双人播客形式在UI/UX探索领域最引人注目的应用成果。它消除了大语言模型在实际使用时面对的两大核心「障碍」：其一就是聊天很枯燥，用户不知道该说什么或者该问什么。而在双人播客形式下，提问工作也被委托给了AI，这样用户就能获得更加放松的体验，不再受到生成过程中同步参与的限制。其二是阅读难度很大，现在播客形式能让用户坐在躺椅中轻松享受获取信息的乐趣。”</p><p>&nbsp;</p><p>它为全体受众（包括技术和非技术受众群体）提供了有用的功能，并可供学生、研究人员和作家们快速上手。它在实用性和实验性之间找到了理想平衡，带来了一种与个人数据交互的新颖方式。</p><p>&nbsp;</p><p>也许我们都有点反应过度，而且NotebookLM也肯定不够完美，毕竟目前还没有哪款AI工具堪称完美。但如果我们能更务实一点，那么ChatGPT和如今的NotebookLM等工具至少标志着生产力被提升到了新的维度。这就像是拥有了一颗不断发育的外挂大脑，它虽然不一定真会思考，但肯定很擅长处理信息。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/karpathy/status/1840112692910272898">https://x.com/karpathy/status/1840112692910272898</a>"</p><p><a href="https://www.turingpost.com/p/fod69">https://www.turingpost.com/p/fod69</a>"</p><p><a href="https://simonwillison.net/2024/Sep/29/notebooklm-audio-overview/">https://simonwillison.net/2024/Sep/29/notebooklm-audio-overview/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jENMgcVh8rMARjcDkvbQ</id>
            <title>Meta 如何将 AI 图片大规模转制成动画</title>
            <link>https://www.infoq.cn/article/jENMgcVh8rMARjcDkvbQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jENMgcVh8rMARjcDkvbQ</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 07:55:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>我们推出 Meta AI 的目的是让人们以新的方式提高工作效率，并通过生成式人工智能（GenAI） 释放创造力。但 GenAI 也面临着规模化方面的挑战。在 Meta 部署新的 GenAI 技术时，我们还在努力尽可能快速高效地向人们提供这些服务。</p><p></p><p>Meta AI 的动画制作功能让人们可以用一幅 AI 生成的图像来生成一段简短的动画，这一功能就在规模化方面带来了独特的挑战。为了大规模部署和运行，我们用来从图片生成动画的模型必须既能为使用我们产品和服务的数十亿用户提供服务，同时还要快速完成任务——生成时间短、错误最少，同时保持高效的资源利用率。</p><p></p><p>本文介绍了我们如何使用延迟优化、流量管理和其他新技术结合在一起来部署 Meta AI 的动画功能。</p><p></p><p></p><h4>优化图像生成动画任务的延迟</h4><p></p><p></p><p>在我们的应用阵容和 Meta AI 网站上推出动画功能之前，快速制作动画模型是我们的首要任务之一。我们希望人们能够神奇地看到他们的制作请求在几秒内就变成一段动画。这不仅对用户来说是很重要，而且我们模型的速度越快，越高效，我们就能用更少的 GPU 做更多的事情，帮助我们以可持续的方式扩大规模。我们之前的相关工作包括了使用视频 Diffusion 技术制作动画表情、使用 Imagine Flash 加速图像生成，以及通过块缓存加速 Diffusion 模型等，这些工作都为我们开发用于大幅缩减延迟的新技术提供了帮助。</p><p></p><p></p><h4>减半浮点精度</h4><p></p><p></p><p>第一项优化技术是将浮点精度减半。我们将模型从 float32 转换为 float16，从而加快了推理速度，原因有二。首先，模型的内存占用减少了一半。其次，16 位浮点运算的执行速度比 32 位浮点运算更快。为了让所有模型都获得这些好处，我们使用了 bfloat16，这是一种具有较小尾数的 float16 变体，用于训练和推理工作。</p><p></p><p></p><h4>改进时间注意力扩展</h4><p></p><p></p><p>第二个优化改进了时间注意力扩展（temporal-attention expansion）。时间注意层位于时间轴和文本条件之间，需要复制上下文张量以匹配时间维度或帧数。以前，这些工作会在传递到交叉注意层之前完成，然而这会导致性能提升不够理想。我们采用的优化实现利用了重复张量都是一样的事实减少了计算和内存需求，这样就能在通过交叉注意的线性投影层后进行扩展。</p><p></p><p></p><h4>利用 DPM-Solver 减少采样步骤</h4><p></p><p></p><p>第三个优化利用了 DPM-Solver。扩散概率模型（DPM）是强大且有影响力的模型，可以产生极高质量的生成结果，但速度可能很慢。其他可能的解决方案，例如去噪扩散隐式模型或去噪扩散概率模型可以提供高质量的生成，但需要更多采样步骤，带来更大计算成本。我们利用 DPM-Solver 和一个线性对数信噪比时间将采样步骤数减少到了 15。</p><p></p><p></p><h4>结合使用指导蒸馏与逐步蒸馏</h4><p></p><p></p><p>我们的第四个优化结合了指导蒸馏和逐步蒸馏。我们用相同的权重初始化教师和学生来完成逐步蒸馏，然后训练学生在单个步骤中匹配多个教师步骤。相比之下，指导蒸馏是指扩散模型利用无分类器指导进行条件图像生成。这需要每个求解器步骤都有一个有条件和一个无条件的前向传递。</p><p></p><p>但在我们的例子中，每个步骤有三次前向传递：无条件、图像条件和一个全条件文本和图像步骤。指导蒸馏将这三次前向传递减少为一次，将推理需求减少了 2/3。然而，这里真正的魔力在于结合这两种优化方法。通过训练学生同时模仿无分类器指导和多个步骤，并通过 U-Net 进行一次前向传递，我们的最终模型只需要八个求解器步骤，每个步骤只需通过 U-Net 做一次前向传递。最后，在训练期间，我们将 32 个教师步骤蒸馏为八个学生步骤。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fd440539a064dec55500fe93d35f5cb2.webp" /></p><p></p><p>通过结合指导蒸馏和逐步蒸馏，我们能够蒸馏 32 个步骤，每个步骤针对每种条件类型通过 U-Net 多次传递，最终通过 U-Net 架构仅需八个步骤。</p><p></p><p></p><h4>PyTorch 优化</h4><p></p><p></p><p>最后这块优化工作与部署和架构有关，涉及两个转换。第一个转换利用了 Torch 脚本和冻结。通过将模型转换为 TorchScript，我们实现了许多自动优化，包括连续折叠、融合多个操作以及降低计算图的复杂性。这三个优化有助于提高推理速度，而冻结操作通过将图中动态计算的值转换为常量来实现进一步优化，从而减少总操作数。</p><p></p><p>这些优化对我们发布的初始版本来说非常重要，并且我们仍在继续寻求突破。例如，我们已经将所有媒体推理任务从 TorchScript 迁移到了基于 PyTorch 2.0 的解决方案上，这为我们带来了多重收益。我们能够使用 pytorch.compile 在组件级别更精细地优化模型组件，并在新架构中启用高级优化技术，例如上下文并行和序列并行。这还带来了额外的好处，例如缩短高级功能的开发时间、改进跟踪，以及支持多 GPU 推理。</p><p></p><p></p><h4>大规模部署和运行图像生成动画功能</h4><p></p><p></p><p>在完全优化模型后，我们面临一系列新的挑战。我们如何大规模运行这一模型以支持来自世界各地的流量，同时保持较快的生成速度，尽量减少故障，并确保 GPU 资源可用于公司其他所有重要用例？</p><p></p><p>我们首先查看了之前 AI 生成的媒体在发布时和一段时间内的流量数据。我们使用这些信息粗略估计了可以预期的请求数量，然后使用模型速度基准测试来确定需要多少 GPU 来容纳这么多需求。在扩大规模后，我们开始运行负载测试，看看在不同的流量水平上我们能否应付，解决各种瓶颈，直到我们能够处理预计发布时会遇到的流量需求为止。</p><p></p><p>在这次测试中，我们注意到动画请求的端到端延迟高于预期，也高于我们在实施上述所有优化后所看到的延迟。我们的调查表明，流量被路由到了全球，导致了巨大的网络和通信开销，并使端到端生成时间增加了几秒。为了解决这个问题，我们使用了一个流量管理系统，该系统获取服务的流量或负载数据，并利用这些数据来计算路由表。路由表的主要目标是将尽可能多的请求保留在与请求者相同的区域中，以避免像我们之前看到的那样出现跨区域流量。路由表还利用我们预定义的负载阈值和路由环，在接近区域的最大容量时将流量卸载到其他区域来防止过载。通过这些更改，大多数请求仍保留在区域内，延迟下降到了大致符合我们预期的水平。</p><p></p><p>要让这项服务正常运行需要很多活动组件。首先，它需要获取我们为层级定义的每一个指标，从该层的机器中获取每个指标的值，并按区域进行汇总。然后，它收集每个区域每秒发送到其他每个区域的请求数，并使用这个数来计算每秒请求的负载成本。这会告诉系统，一般来说，每秒每增加一个请求，负载就会增加 X。完成后，算法开始生效，首先将所有流量带到源区域。我们还没有检查该区域是否有足够的容量。</p><p></p><p>下一步是进入一个循环，在每次迭代中，我们都会查看哪个区域的运行情况最接近最大容量。服务会尝试获取该区域的一部分请求，并将其卸载到附近的区域，后者得能处理这些请求而不会变得更加过载。</p><p></p><p>不同的过载程度决定了我们在查看附近区域时考虑的距离。例如，如果主区域刚刚开始过热，则可能只会使用最近的区域。如果该区域几乎以最大容量运行，则可能会解锁较远的区域以进行卸载。如果没有更多可以在区域之间移动的请求，我们将退出循环，这种情况发生在每个区域都低于定义的“过载”阈值时，或者由于所有附近区域也都高于阈值，因此没有更多服务可以卸载到的附近区域。此时，服务将计算每个区域每秒的最佳请求数，并使用它来创建上面提到的路由表，以便我们的服务可以合理判断在请求时将流量发送到何处。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/56/56f289d1772995eb3f6645b01a548c6d.webp" /></p><p></p><p>为了确保动画请求尽快交付，我们的一部分工作是实现了一个流量管理系统，以尽可能将请求与请求者保持在同一个区域。</p><p></p><p>实施这些优化后，延迟恢复到了我们满意的水平，但成功率有所下降。从高层次来看，每个 GPU 一次只能处理一个请求，因为每个请求都会让 GPU 完全饱和运行。为了保持较低的延迟，我们必须不允许请求排队——否则，它们将有很长的等待时间。为了实现这一点，我们确保了服务器负载（排队请求加上正在进行的请求）最多为一个，并且服务器会拒绝其他新请求。然而，正因为如此，当我们的运行接近容量极限时将遇到许多故障。这个问题的简单解决方案是使用队列，但由于必须在全球范围内进行负载平衡，这本身就带来了一系列复杂的挑战，不利于提高效率和速度。我们采用的方法差不多是利用重试轮询来创建一个探测系统，该系统可以非常快速地检查空闲的 GPU 并防止故障。</p><p></p><p>在我们实现流量管理系统之前，这种方法效果很好。该系统虽然可以有效地减少延迟，但由于现在我们不再用全局路由了，而它又让每个请求可用的主机数量变少，于是引入了更多复杂性。我们注意到，重试轮询不起作用了，并且如果出现任何峰值，它实际上往往会出现级联。进一步的调查让我们发现，我们的路由器需要有更优化的重试设置。它既没有延迟也没有退避。因此，如果我们有一个区域，其中有很多任务正在尝试运行，那么它就会陷入超载状态，直到它开始让请求失败。为了避免级联错误，我们修改了这些重试设置，在调度时为一定比例的作业添加边际执行延迟——使它们可以逐渐执行而不是一次性执行——以及指数退避。</p><p></p><p>完成所有这些操作后，我们就有了一个高效、大规模运行的部署模型，能够以高可用性、最低故障率处理全球流量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/07/07a590b37ae01c56cced56e7313a932b.webp" /></p><p></p><p>通过增加边际执行延迟、优化重试和指数退避，我们减少了系统中的错误数量。</p><p></p><p>原文链接：</p><p></p><p><a href="https://engineering.fb.com/2024/08/14/production-engineering/how-meta-animates-ai-generated-images-at-scale/">https://engineering.fb.com/2024/08/14/production-engineering/how-meta-animates-ai-generated-images-at-scale/</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/h7BvDcU3expoSIKyY3aH</id>
            <title>哪些 AI 应用最受大家欢迎？</title>
            <link>https://www.infoq.cn/article/h7BvDcU3expoSIKyY3aH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/h7BvDcU3expoSIKyY3aH</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 07:52:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>在紧随日益庞大的消费者导向 AI 产品潮流时，保持高度的动态适应与敏捷反应能力显得尤为重要。无论我们是致力于开发提升效率的新工作流程，探索现实世界中的实际应用案例，还是尝试将新技术与创意元素巧妙融合，这一领域都要求我们始终站在科技前沿。</p><p></p><p>然而，在铺天盖地的产品发布、投资公告及功能炒作的浪潮中，一个关键问题亟待解答：哪些生成式 AI 应用真正赢得了用户的青睐？哪些行为模式和领域正牢牢吸引着消费者的目光？又有哪些 AI 应用能够促使用户持续使用，而非仅仅是一时之兴？</p><p></p><p>本报告每半年更新一次，通过深入的数据分析，我们精心筛选出前 50 名以月度独立访问量计量的 AI 领先网络产品，以及前 50 名以月度活跃用户数衡量的 AI 优先移动应用。尤为值得关注的是，与上一份 2024 年 3 月报告 相比，本次报告中 有近 30% 的公司为新晋上榜者，彰显出该领域竞争之激烈与变化之迅速。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f6/f6d1e5c676ade49c933cdf004dc08236.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5f/5f2972bc4ef0732d202328b0ab9a5d32.png" /></p><p></p><p>然而，除了这些充斥品牌标识的排名，数据还揭示了几项值得关注的趋势，包括新兴和扩展的类别、竞争者的崛起，以及用户参与的模式。</p><p></p><p>以下是我们的一些主要发现：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bf571f662e9ad36fa81a611f72067296.png" /></p><p></p><p>创意工具的魔力依然吸引着大量消费者。在网络榜单中，有 52% 的公司专注于内容生成或编辑，涵盖图像、视频、音乐、语音等多种形式。值得注意的是，在 12 家新晋上榜的公司中，有 58% 来自创意工具领域。</p><p></p><p>在首次上榜的公司中，排名前五的占据了四席，分别是：Luma（位居第 14 名）、Viggle（位居第 21 名）、SeaArt（位居第 29 名）以及 Udio（位居第 33 名）。尤其值得一提的是，音乐生成器 Suno 在过去六个月中表现抢眼，实现了从第 36 名到第 5 名的飞跃。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fa/faa86ccc36b9d8c3d45832b2edfb4f39.png" /></p><p></p><p>在我们之前的榜单统计中，不难发现，大多数内容生成工具都聚焦于图像创作领域。然而，近半年来，这一格局悄然生变，其他类型的内容生成模式逐渐崭露头角，使得图像生成工具在顶级内容生成网站中的占比滑落至 41%。</p><p></p><p>值得注意的是，在最新上榜的五大生成工具中，仅有 SeaArt 坚守图像创作的阵地。与此同时，视频生成领域迎来了三位强劲的新成员 ——Luma、Viggle 和 Vidnoz，它们的加入无疑为这一领域注入了新的活力。而在音乐创作方面，Udio 的崭露头角也标志着音乐生成技术的显著进步。过去一年中，视频与音乐这两种内容生成模式的输出质量均实现了质的飞跃。</p><p></p><p>转向移动端市场，图像与视频的内容编辑应用占据了榜单的显著位置，占比高达 22%，稳居移动端排名中的第二大产品类别。这充分反映了用户对于在手机上随时随地编辑内容的迫切需求。尽管初创企业如雨后春笋般涌现，但许多新上榜的顶尖产品却是由传统创意工具转型而来，它们成功地将 AI 生成技术融入核心功能，如美图（位列第 9）、SNOW（位列第 30）以及 Adobe Express（位列第 35），这些产品的转型不仅满足了市场需求，也展现了 AI 技术在内容创作领域的广阔应用前景。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8c/8c0edbc7e2db6b067cc4e42eadb3d023.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c006cf3ec50ea3cda5238ccd997a06b5.png" /></p><p></p><p>ChatGPT 已连续第三次在网络与移动端排行榜上稳居榜首，且领先优势显著。然而，关于 “最佳消费者助手” 的角逐正日益白热化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f5/f54b20617ca3d3526af574ce78b357bf.png" /></p><p></p><p>Perplexity 目前在网络端排名第三，这款由 AI 驱动的搜索引擎以其简洁、实时且准确的查询结果著称，每个答案均附有引用来源，确保信息的可靠性。据 Similarweb 数据显示，用户在 Perplexity 上的访问时长略胜一筹，超过 ChatGPT（超过七分钟），显示出较高的用户参与度和满意度。值得一提的是，Perplexity 还首次跻身移动端前 50 名榜单，进一步扩大了其影响力。</p><p></p><p>Anthropic 公司的 Claude 则被视为 ChatGPT 的强劲对手之一，其在网络端的排名从先前的第十位攀升至第四位，展现了强劲的竞争实力。近期，Anthropic 更是推出了 Artifacts 功能，直接与 ChatGPT 的 GPTs 展开正面交锋，进一步加剧了这一领域的竞争态势。</p><p></p><p>在移动端领域，AI 助手 Luzia 首次亮相便引人注目，以第 25 名的成绩强势入榜。Luzia 宣称其全球用户数量已达 4500 万，主要服务于西班牙语用户群体。这款 AI 助手最初以 WhatsApp 聊天机器人的形式问世，随后于 2023 年 12 月推出了独立的移动应用程序，为用户提供了更为便捷和个性化的服务体验。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ef/ef9c4f9860e1bcd8eed988fc87148892.png" /></p><p></p><p>字节跳动，作为 TikTok 的母公司，正积极拓宽其网络 AI 产品版图。此次，其三款应用首次跻身我们的榜单：教育领域的佼佼者 Gauth（排名第 44）、创新的机器人构建工具 Coze（排名第 45），以及多功能的智能助手 “豆包”（排名第 47）。值得一提的是，“豆包” 也首次在移动应用榜单中亮相，位列第 26 名，展现了其跨平台的影响力。</p><p></p><p>除了 “豆包” 之外，字节跳动旗下的照片与视频编辑器 Hypic（排名第 19）及智能助手 Cici（排名第 34）同样表现出色，共同占据了这两个榜单上的六个席位，彰显了字节跳动在 AI 应用领域的全面布局。这些应用均针对不同地域市场进行了优化，特别是在移动端，Cici 作为 “豆包” 的英文版，为全球用户提供了更为便捷的智能化服务。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/442093656420dfc5eb43dcf491648aff.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/53/5361718d4a0e8606c977f9a352ae170a.png" /></p><p></p><p>为何会涌现如此众多的新应用呢？这背后，字节跳动在 2023 年底成立了一个名为 Flow 的研发部门，其核心聚焦于生成式 AI 应用的创新。自 2024 年初以来，该公司便以其他企业为名义，在美国及全球范围内密集推出了一系列全新的 AI 应用，这些应用迅速占领了市场。</p><p></p><p>在网络和移动应用领域，一个全新的类别 ——“美学与约会” 悄然兴起，并有三款新应用脱颖而出，成功登上了我们的榜单：LooksMax AI（排名第 43）、Umax（排名第 44）和 RIZZ（排名第 49），它们同时也在移动榜单上占据了一席之地。</p><p></p><p>LooksMax 与 Umax 这两款应用，通过智能分析用户上传的照片，不仅给予评分，还贴心地提供 “魅力提升” 的个性化建议。Umax 更进一步，能够生成用户外貌达到 “满分 10 分” 的虚拟图像，让用户预览自己最佳状态的模样。而 LooksMax 则别出心裁，它还能分析用户声音的吸引力，为用户提供全方位的美化建议。在应用的介绍页面上，LooksMax 自豪地宣称已拥有超过 200 万用户，而 Umax 也不甘示弱，表示其用户量已达百万之众。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e8/e8dc75fe4145bd2e1988c6ce143e07f6.png" /></p><p></p><p>这两款应用均采取订阅制模式来盈利，用户需付费解锁全部功能：Umax 每周费用为 4.99 美元（或邀请三位好友以免费试用），而 LooksMax 则为每周 3.99 美元。</p><p></p><p>RIZZ 应用的独特之处在于，它专注于提升约会软件中的对话质量。用户可上传对话截图或个人资料，RIZZ 将提供优化后的回复建议，这些建议可一键复制至约会应用中，助力用户更流畅地交流。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/db/db7d551a8f3e7e7befd85e108bddb479.png" /></p><p></p><p>在预测应用的网络与移动端排名中，Discord 的流量数据占据举足轻重的地位，尤其是在内容生成领域尤为显著。</p><p></p><p>一些产品选择在 Discord 上作为 “试验田”，进行初步测试与社区构建，随后推出独立网站并相应减少在 Discord 上的活动。这类产品常被视作从 Discord 顶级排名中 “毕业” 的典范，比如 Suno，它在我们上次榜单中位列第 31，但此次已不在 Discord 前 100 名服务器之列。</p><p></p><p>然而，其他公司即使在推出独立产品后，仍能维持高水平的 Discord 活跃度。例如， 继续在所有 Discord 服务器的邀请流量中保持第一的位置。</p><p></p><p>然而，也有公司即便推出独立产品后，仍能在 Discord 上保持高度活跃。以 Midjourney 为例，它持续在所有 Discord 服务器的邀请流量中独占鳌头。</p><p></p><p>截至 7 月，共有 10 家 AI 公司跻身 Discord 服务器邀请流量前 100 名之列，其中半数为今年 1 月以来的新面孔。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c9/c96f0c4cefa830a4a48d58c23104e96a.png" /></p><p></p><p>在 Discord 服务器的前十名中，半数允许用户直接在平台内生成内容，这通常与付费订阅相关联；而另一半则更侧重于社区建设、客户支持及资源共享。</p><p></p><p>显然，新一代 AI 原生产品与公司正以惊人的速度蓬勃发展，它们更深入地吸引着用户，预示着 AI 将在未来十年内成为塑造行业格局的核心力量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3c/3c96ca7a047fb790241d3d6ef922a393.png" /></p><p></p><p>作者简介：</p><p></p><p>Olivia Moore，Andreessen Horowitz 消费者投资团队的合伙人，专注于人工智能领域。</p><p></p><p>原文链接：</p><p></p><p><a href="https://a16z.com/100-gen-ai-apps-3/">https://a16z.com/100-gen-ai-apps-3/</a>"</p><p></p><p>声明：本文为 InfoQ 翻译整理，未经许可禁止转载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5Ae30D59ljJtNrdgI5pf</id>
            <title>Meta视频模型深夜炸场，发布Movie Gen；OpenAI完成66亿美元融资；英伟达内部人士套现逾18亿美元 | Q资讯</title>
            <link>https://www.infoq.cn/article/5Ae30D59ljJtNrdgI5pf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5Ae30D59ljJtNrdgI5pf</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 07:03:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>Sora&nbsp;迎劲敌：Meta&nbsp;推出视频模型&nbsp;Movie&nbsp;Gen；“爱奇艺会员暂停后播放全屏广告”引热议，官方客服回应；2024&nbsp;年全球最具价值独角兽企业排名，字节跳动第一，OpenAI&nbsp;第三；曝比亚迪突然给员工发放&nbsp;2024&nbsp;利润奖！有人收到&nbsp;13&nbsp;万；OpenAI&nbsp;重磅发布&nbsp;Canvas；微软旗下的&nbsp;Edge&nbsp;浏览器被竞争对手指责处于不公平竞争地位；阿里巴巴：三季度回购了&nbsp;41&nbsp;亿美元股票；英伟达内部人士股票套现超&nbsp;18&nbsp;亿美元；OpenAI&nbsp;完成新一轮&nbsp;66&nbsp;亿美元融资，英伟达新近参与；黄仁勋：Blackwell&nbsp;人工智能芯片需求“疯狂”；微软&nbsp;Office&nbsp;365&nbsp;大动作：11&nbsp;月&nbsp;1&nbsp;日起，Feed&nbsp;服务将成历史；扎克伯格成世界第&nbsp;2&nbsp;大富豪；Character.ai&nbsp;放弃开发&nbsp;AI&nbsp;模型，与谷歌达成&nbsp;27&nbsp;亿美元交易……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>Sora&nbsp;迎劲敌：Meta&nbsp;推出视频模型&nbsp;Movie&nbsp;Gen</h4><p></p><p>当地时间&nbsp;10&nbsp;月&nbsp;4&nbsp;号，Meta&nbsp;公布了一款强大的&nbsp;AI&nbsp;视频生成系统，名为&nbsp;Movie&nbsp;Gen。</p><p></p><p>从其演示效果来看，可称得上是&nbsp;OpenAI&nbsp;所开发的文生视频大模型&nbsp;Sora&nbsp;的“头号劲敌”。Meta&nbsp;的&nbsp;CEO&nbsp;马克·扎克伯格（Mark&nbsp;Zuckerberg）通过一段健身视频，展示了这项新技术。</p><p></p><p>在视频中，他的腿部训练器械不断变换造型，从赛博朋克到古罗马风格，再到金色火焰特效，甚至一度将负重变成了炸鸡块，展现了&nbsp;Movie&nbsp;Gen&nbsp;强大的视频编辑能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3eeff9b87b1b6b3e353c0c74c130cce5.webp" /></p><p>截图为扎克伯格展示&nbsp;Movie&nbsp;Gen&nbsp;的视频编辑能力（来源：Instagram）</p><p></p><p>不过，如同&nbsp;Sora&nbsp;一样，Movie&nbsp;Gen&nbsp;也是“期货”产品，目前尚未对外开放，也没有明确的时间表。官方称正在积极地与娱乐行业的专业人士和创作者进行沟通和合作，预计将在明年某个时候将其整合到&nbsp;Meta&nbsp;自己的产品和服务中。</p><p></p><p>据外媒，Meta&nbsp;副总裁&nbsp;Connor&nbsp;Hayes&nbsp;透露了延迟推出的重要原因，他表示&nbsp;Meta&nbsp;Movie&nbsp;Gen&nbsp;当前使用文本提示词生成一个视频往往需要等待数十分钟，极大影响了用户的体验。Meta&nbsp;希望进一步提高视频生成的效率，以及实现尽快在移动端上推出该视频服务，以便能更好地满足消费者的需求。</p><p></p><h4>“爱奇艺会员暂停后播放全屏广告”引热议，官方客服回应</h4><p></p><p>10&nbsp;月&nbsp;5&nbsp;日消息，近日有网友发帖称，爱奇艺会员暂停后播放全屏广告真是忍不了，对此官方也进行回应。</p><p></p><p>有网友发视频称，自己身为爱奇艺的会员，但在观看视频的过程中点击暂停想要观察画面，暂停后却出现了全屏的广告，被暂停的视频仅占屏幕小小一角，根本无法看清。</p><p></p><p>随后，爱奇艺客服表示：爱奇艺会员特权仅减免部分视频前面的广告，在使用期间仍会遇到其他形式的广告可以点击关闭和跳过之类的按钮。</p><p></p><p>暂停后出现的广告是关不了的，点击继续播放就没有了，这类问题已经安排专人处理和回复了。”爱奇艺方面表示。</p><p></p><p>而在&nbsp;10&nbsp;月&nbsp;4&nbsp;日晚，罗永浩则是通过微博以“不点名”的形式怒批“视频暂停最小化播放窗口并插入广告”的视频平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76c52b369048b315a30746457866ec7e.webp" /></p><p></p><h4>2024&nbsp;年全球最具价值独角兽企业排名，字节跳动第一，OpenAI&nbsp;第三</h4><p></p><p>根据硅谷科技评论（SVTR）AI&nbsp;数据库，今天全球独角兽企业的总价值为&nbsp;3.8&nbsp;万亿美元，超过了印度的&nbsp;GDP。</p><p></p><p>全球最有价值的&nbsp;10&nbsp;家独角兽企业中有&nbsp;6&nbsp;家位于美国。中国（字节跳动）、新加坡（Shein）、英国（Revolut）和澳大利亚（Canva）都有公司上榜。</p><p></p><p>来自中国的字节跳动是全球最有价值的独角兽，估值达到&nbsp;2250&nbsp;亿美元。目前，大约&nbsp;50%&nbsp;的美国人使用&nbsp;TikTok，14%&nbsp;的美国成年人定期从该平台获取新闻。</p><p></p><p>位居第二的是埃隆·马斯克&nbsp;(Elon&nbsp;Musk)&nbsp;的&nbsp;SpaceX，估值为&nbsp;2000&nbsp;亿美元。该公司是&nbsp;NASA&nbsp;和五角大楼的主要发射服务提供商，已发射了&nbsp;7,000&nbsp;多颗卫星。</p><p></p><p>OpenAI&nbsp;已成为第三大最有价值的独角兽。最近宣布已筹集&nbsp;66&nbsp;亿美元新资金，投后估值为&nbsp;1570&nbsp;亿美元。这一估值是&nbsp;AI&nbsp;行业前&nbsp;20&nbsp;名所有其他公司的估值之和。</p><p></p><h4>曝比亚迪突然给员工发放&nbsp;2024&nbsp;利润奖！有人收到&nbsp;13&nbsp;万</h4><p></p><p>近日，一则比亚迪员工毫无征兆收到“利润奖”的消息在社交媒体上疯传。据多名比亚迪员工透露，他们节前收到公司发放的一笔丰厚利润奖金，有人收到七八万元，更有甚者收到十余万元。</p><p></p><p>对此，有媒体求证获悉，比亚迪确有发放奖金，金额与所处的等级与事业部有关，但并非毫无征兆。有比亚迪员工表示：“我有收到通知，发了邮件的。”</p><p></p><p>“从金额来看，这次比亚迪算是非常大手笔的一次。”有汽车博主发消息称。据其核实，比亚迪D级有一二十万元的“利润奖”，E级也有几万到十几万元不等。据悉，“利润奖”金额与员工所处的等级与事业部有关。另外，比亚迪发放的“利润奖”，并非“过节费”或季度奖金，实际是上年度的年终奖。有比亚迪员工称，公司内部没有年终奖的说法，只有“利润奖”。</p><p></p><h4>OpenAI&nbsp;重磅发布&nbsp;Canvas：跟&nbsp;ChatGPT&nbsp;一起写作编程</h4><p></p><p>北京时间&nbsp;10&nbsp;月&nbsp;4&nbsp;日凌晨，OpenAI&nbsp;官方发文称，将推出一个名为“Canvas”的新功能，该功能提供了一种新的工作界面，用户可以在其中编辑和改进&nbsp;AI&nbsp;的输出。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/4714f16308148dcf19dd2a795cfa5dcc.webp" /></p><p></p><p>简单来说，这个功能相当于在&nbsp;ChatGPT&nbsp;基础上增设了一个人机协作的“工作台”。用户不仅可以与&nbsp;AI&nbsp;聊天，还可以在这个平台上共同撰写文章或编程，边生成边修改。这与以前的方式截然不同，以前如果对&nbsp;AI&nbsp;生成的内容不满意，用户通常只能从头再来，或者进行更多的人工修改。而有了“Canvas”，用户可以随时在&nbsp;AI&nbsp;生成的内容上进行改动，直到达到满意为止。</p><p></p><p>这意味着什么呢？长期以来，AI&nbsp;的应用多停留在文本生成、数据分析等相对简单的任务上。在需要高精度和高迭代的工作场景中，尤其在写作和编程时，创作者常常需要多次修改和优化生成的内容。</p><p></p><p>在&nbsp;canvas&nbsp;这个界面，你可以与&nbsp;ChatGPT&nbsp;一起完成写作和编码项目，而不再局限于简单的聊天。canvas&nbsp;是一种新的交互方式，也是&nbsp;OpenAI&nbsp;推出&nbsp;ChatGPT&nbsp;以来的首个重大视觉界面更新。canvas&nbsp;由&nbsp;GPT-4o&nbsp;支持，在&nbsp;Beta&nbsp;期间可以在模型选择器中手动选择。不过，现在&nbsp;Beta&nbsp;版本只提供给&nbsp;ChatGPT&nbsp;Plus&nbsp;与团队用户。企业和教育用户将在下周获得访问权限。ChatGPT&nbsp;免费用户需要等到&nbsp;canvas&nbsp;正式发布后才能使用。</p><p></p><p>canvas&nbsp;由&nbsp;GPT-4o&nbsp;支持，在&nbsp;Beta&nbsp;期间可以在模型选择器中手动选择。不过，现在&nbsp;Beta&nbsp;版本只提供给&nbsp;ChatGPT&nbsp;Plus&nbsp;与团队用户。企业和教育用户将在下周获得访问权限。</p><p></p><p>ChatGPT&nbsp;免费用户需要等到&nbsp;canvas&nbsp;正式发布后才能使用。</p><p></p><h4>微软旗下的&nbsp;Edge&nbsp;浏览器被竞争对手指责处于不公平竞争地位</h4><p></p><p>10&nbsp;月&nbsp;3&nbsp;日，微软因其&nbsp;Edge&nbsp;浏览器在&nbsp;Windows&nbsp;生态系统中的默认设置而面临来自竞争对手&nbsp;Web&nbsp;浏览器及其他竞争对手的新一轮批评，他们声称这种默认设置给微软这家科技巨头带来了不公平的竞争优势，损害了市场竞争。据路透社报道，浏览器&nbsp;Vivaldi、Waterfox、Wavebox&nbsp;以及开放网络倡导组织（Open&nbsp;Web&nbsp;Advocacy&nbsp;group）已联合向欧盟委员会提交了一封信，呼吁欧盟委员会根据欧盟的技术规则对微软的&nbsp;Edge&nbsp;浏览器进行更严格的监管。</p><p></p><p>Web&nbsp;浏览器的这一举动进一步支持了&nbsp;Opera&nbsp;于&nbsp;2024&nbsp;年&nbsp;7&nbsp;月将欧盟委员会告上法庭的诉讼。Opera&nbsp;在诉状中称，微软的&nbsp;Edge&nbsp;浏览器被欧盟委员会错误地排除在《数字市场法》之外。这项新法案为“守门人”的在线服务作出了具体规定，旨在让消费者能自由地选择来自不同提供商的服务。Vivaldi&nbsp;和其他公司支持&nbsp;Opera&nbsp;的法律诉讼，希望欧盟委员会重新考虑其有关决定。</p><p></p><p>Web&nbsp;和&nbsp;Vivaldi&nbsp;浏览器在信中表示：Edge&nbsp;浏览器在&nbsp;Windows&nbsp;设备上的默认设置，且没有提示用户可以选择其他浏览器的选项设置，扼杀了市场竞争并限制了消费者的自由选择。</p><p></p><p>信中强调，Edge&nbsp;浏览器的默认设置状态使其在&nbsp;Windows&nbsp;PC&nbsp;端上具有无与伦比的优势地位，使其成为消费者在&nbsp;Windows&nbsp;PC&nbsp;端下载其他浏览器的关键门户。信中指出：“没有任何独立于平台的浏览器能够与&nbsp;Edge&nbsp;浏览器的优势地位相匹敌。”信中还涉及微软的策略问题，包括&nbsp;Edge&nbsp;浏览器中弹出的搜索信息歪曲了竞争对手浏览器的功能，通过误导消费者来削弱竞争对手。</p><p></p><p>尽管存在这些指控，但微软和欧盟委员会均未就此事作出回应。StatCounter&nbsp;的数据显示，Edge&nbsp;浏览器在全球市场中所占的市场份额仅略高于&nbsp;5%，而谷歌的&nbsp;Chrome&nbsp;浏览器则以&nbsp;66%&nbsp;的市场份额遥遥领先。</p><p></p><h4>阿里巴巴：三季度回购了&nbsp;41&nbsp;亿美元股票</h4><p></p><p>10&nbsp;月&nbsp;2&nbsp;日晚间，阿里巴巴在港交所发布公告，截至&nbsp;2024&nbsp;年&nbsp;9&nbsp;月&nbsp;30&nbsp;日止季度期间，公司以&nbsp;41&nbsp;亿美元的总价回购了总计&nbsp;4.14&nbsp;亿股普通股（相当于&nbsp;5,200&nbsp;万股美国存托股）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8dd3962f278e58e6c67ddfc3d1a0af04.webp" /></p><p></p><p>这些回购根据公司的股份回购计划在美国市场和香港市场进行。在董事会授权的股份回购计划下仍余&nbsp;220&nbsp;亿美元回购额度，有效期至&nbsp;2027&nbsp;年&nbsp;3&nbsp;月。</p><p></p><p>本次回购从某种意义上来讲，以&nbsp;41&nbsp;亿美元的大手笔来回购股票，告诉外界，对于阿里巴巴来见个，最为黑暗的时刻业已过去。</p><p></p><h4>英伟达内部人士股票套现超&nbsp;18&nbsp;亿美元</h4><p></p><p>10&nbsp;月&nbsp;4&nbsp;日，财联社报道，Nvidia&nbsp;高管和董事今年已售出近&nbsp;1100&nbsp;万股股票，价值超过&nbsp;18&nbsp;亿美元。据彭博社报道，这是公司领导层在调整股票分割后近期最大的股票出售。这一金额不到公司总股数的&nbsp;0.045%，但这一消息可能会给&nbsp;Nvidia&nbsp;的股价带来一些下行压力，尤其是在&nbsp;Blackwell&nbsp;B200&nbsp;GPU&nbsp;延迟发布的情况下。</p><p></p><p>据报道，英伟达首席执行官黄仁勋最近根据预先安排的交易计划出售了&nbsp;600&nbsp;万股股票，这意味着无论公司和市场情况如何，这一举动都会发生。黄仁勋此次出售净赚了约&nbsp;7.13&nbsp;亿美元，但他仍持有价值超过&nbsp;1000&nbsp;亿美元的英伟达股票。</p><p></p><p>另一笔大规模的&nbsp;Nvidia&nbsp;出售交易由&nbsp;Nvidia&nbsp;董事&nbsp;Mark&nbsp;Stevens&nbsp;执行，他在&nbsp;2024&nbsp;年迄今已出售了&nbsp;160&nbsp;万股股票，价值约&nbsp;3.9&nbsp;亿美元。不过，他还申请额外出售&nbsp;300&nbsp;万股股票，这可能使他净赚超过&nbsp;7.31&nbsp;亿美元。另一位董事&nbsp;Tench&nbsp;Coxe&nbsp;在今年早些时候出售部分股份后也获利&nbsp;5.25&nbsp;亿美元。</p><p></p><p>今年早些时候，由于人工智能&nbsp;GPU&nbsp;热潮，英伟达股价创下历史新高，成为新闻焦点。这使得该公司成为全球市值最高的公司，在&nbsp;6&nbsp;月底超越了苹果、微软和谷歌。此后，其市场价格出现回调，股价下跌了约&nbsp;10%。然而，这仍然使该公司成为市值最高的公司之一。</p><p></p><p>不过，该公司的乐观前景可能不会持续太久，因为有人说其人工智能估值被&nbsp;高估，而且存在泡沫。就连高盛也在质疑，在硬件和人工智能培训方面的巨额投资是否会带来回报。它表示，人工智能目前过于昂贵且不可靠，该行业每年至少需要赚取&nbsp;6000&nbsp;亿美元才能实现收支平衡。</p><p></p><p>OpenAI&nbsp;完成新一轮&nbsp;66&nbsp;亿美元融资，英伟达新近参与</p><p></p><p>10&nbsp;月&nbsp;3&nbsp;日，因&nbsp;ChatGPT&nbsp;而闻名于世的&nbsp;OpenAI&nbsp;宣布完成新一轮巨额融资，金额达到&nbsp;66&nbsp;亿美元，投后估值高达&nbsp;1570&nbsp;亿美元（约合人民币&nbsp;1.1&nbsp;万亿元），刷新投资交易规模。至此，OpenAI&nbsp;成为与马斯克创办的&nbsp;SpaceX、张一鸣的字节跳动并列在内的全球前三大初创公司。</p><p></p><p>OpenAI&nbsp;表示，新资金将能让公司强化在前沿人工智能研究中的领导地位，提高计算能力，继续构建帮助人们解决难题的工具。</p><p></p><p>据彭博社援引知情人士消息报道，本轮融资由&nbsp;ThriveCapital&nbsp;领投，投资金额达&nbsp;13&nbsp;亿美元。OpenAI&nbsp;最大的支持者微软在原有&nbsp;130&nbsp;亿美元的基础上，又投资了约&nbsp;7.5&nbsp;亿美元。</p><p></p><p>其他机构将通过特殊目的实体（SPV）对该公司进行投资，即风险基金可以通过它们为特定目的筹集资本的实体。其中，软银的投资额为&nbsp;5&nbsp;亿美元，老虎环球投资公司投入&nbsp;3.5&nbsp;亿美元，AltimeterCapital&nbsp;投资&nbsp;2.5&nbsp;亿美元。其他投资方包括&nbsp;KhoslaVentures、富达管理研究公司和英伟达。</p><p></p><p>值得一提的是，传闻中的苹果公司并未出现在本轮投资名单中。此前，OpenAI&nbsp;将&nbsp;ChatGPT&nbsp;整合到苹果手机设备上，并通过&nbsp;Siri&nbsp;语音助手实现人工智能功能。有报道称，双方曾就投资相关事情进行商议，后来被终止。</p><p></p><p>据《金融时报》消息，该公司希望投资者不要投资五家直接竞争对手公司，包括&nbsp;Anthropic、Ilya&nbsp;Sutskever&nbsp;创办的&nbsp;SafeSuperintelligence（SSI）、马斯克的&nbsp;xAI、AI&nbsp;初创公司&nbsp;Perplexity&nbsp;和&nbsp;AI&nbsp;搜索公司&nbsp;Glean。</p><p></p><p>对于此次融资，OpenAI&nbsp;方面表示，“将确保人工智能造福全人类的使命取得进展。”但其正在进行公司重组，已经背离创办之初的非营利性承诺。</p><p></p><h4>黄仁勋：Blackwell&nbsp;人工智能芯片需求“疯狂”</h4><p></p><p>当地时间周三（10&nbsp;月&nbsp;2&nbsp;日），黄仁勋在接受媒体采访时说道：“每个人都想要拥有最多的产品，每个人都想成为第一个收到货的。”</p><p></p><p>值得一提的是，黄仁勋三周前也说过类似的话。</p><p></p><p>受这一消息的影响，英伟达早盘一度涨至每股&nbsp;124.36&nbsp;美元，涨幅最高达&nbsp;4.6%，现收窄至&nbsp;3%&nbsp;附近，最新报每股&nbsp;122.37&nbsp;美元。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/997b6087e35142d900cf87087aa3e549.webp" /></p><p></p><p>据英伟达官网介绍，Blackwell&nbsp;架构&nbsp;GPU&nbsp;具有&nbsp;2080&nbsp;亿个晶体管，采用专门定制的台积电&nbsp;4NP&nbsp;工艺制造，采用双倍光刻极限尺寸的裸片，通过&nbsp;10TB/s&nbsp;的片间互联技术连接成一块统一的&nbsp;GPU。</p><p></p><p>为了给&nbsp;ChatGPT、Copilot&nbsp;等软件产品提供动力，OpenAI、微软、Meta&nbsp;等科技公司正在建立人工智能（AI）数据中心，这使得他们对英伟达&nbsp;GPU&nbsp;产品的需求非常火爆。</p><p></p><p>黄仁勋说道：“在技术发展如此迅速的时刻，这给了我们加倍努力的机会，真正推动创新周期，从而提高产能、增加产出、降低成本、减少能源消耗。”</p><p></p><p>业绩报告当天，英伟达首席财务官&nbsp;Colette&nbsp;Kress&nbsp;表示，公司预计&nbsp;Blackwell&nbsp;在第四财季的收入将达到数十亿美元。</p><p></p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>微软&nbsp;Office&nbsp;365&nbsp;大动作：11&nbsp;月&nbsp;1&nbsp;日起，Feed&nbsp;服务将成历史</h4><p></p><p>报道称微软宣布自&nbsp;2024&nbsp;年&nbsp;11&nbsp;月&nbsp;1&nbsp;日开始，将从&nbsp;Microsoft365&nbsp;套件中移除&nbsp;Microsoft&nbsp;Feed&nbsp;服务。微软推荐用户利用&nbsp;Microsoft&nbsp;365&nbsp;Home&nbsp;页面中的“推荐”区域，并表示&nbsp;Feed&nbsp;的所有基础功能已整合到该区域中。</p><p></p><p>微软宣布自&nbsp;11&nbsp;月&nbsp;1&nbsp;日之后，用户无法在以下应用和服务中使用&nbsp;Microsoft&nbsp;Feed：</p><p></p><p>Feed&nbsp;in&nbsp;Microsoft&nbsp;365Feed&nbsp;in&nbsp;Microsoft&nbsp;EdgeFeed&nbsp;in&nbsp;Outlook&nbsp;MobileFeed&nbsp;in&nbsp;Microsoft&nbsp;365&nbsp;MobileFeed&nbsp;in&nbsp;Microsoft&nbsp;365&nbsp;Windows&nbsp;app</p><p></p><p>Microsoft&nbsp;Feed&nbsp;是一个旨在帮助用户发现和学习与其工作相关的人物和兴趣的个性化内容中心，通过&nbsp;Microsoft&nbsp;Graph&nbsp;API&nbsp;整合用户在&nbsp;Microsoft&nbsp;365&nbsp;中的活动和内容，提供个性化的信息流。</p><p></p><p>Microsoft&nbsp;Feed&nbsp;通过&nbsp;Microsoft&nbsp;Graph&nbsp;API&nbsp;整合用户在&nbsp;Microsoft&nbsp;365&nbsp;中的活动和内容，提供个性化的信息流。Feed&nbsp;不仅聚合来自&nbsp;Outlook、OneDrive、Teams&nbsp;和&nbsp;SharePoint&nbsp;等多种服务的数据，还能够展示用户和团队的动态。</p><p></p><p></p><h4>扎克伯格成世界第&nbsp;2&nbsp;大富豪，目前身价仅次于马斯克</h4><p></p><p></p><p>随着&nbsp;Meta&nbsp;公司股价的持续走高，其首席执行官马克·扎克伯格（Mark&nbsp;Zuckerberg）的个人财富在当地时间&nbsp;10&nbsp;月&nbsp;3&nbsp;日超过亚马逊创始人杰夫·贝索斯（Jeff&nbsp;Bezos），首次跻身全球第二大富豪。</p><p></p><p>据彭博亿万富翁指数，当地时间&nbsp;10&nbsp;月&nbsp;3&nbsp;日，扎克伯格的净资产达到了&nbsp;2062&nbsp;亿美元，比贝佐斯高出&nbsp;11&nbsp;亿美元，但仍落后于特斯拉（Tesla）首席执行官埃隆·马斯克（Elon&nbsp;Musk）近&nbsp;500&nbsp;亿美元。</p><p></p><p>截至&nbsp;10&nbsp;月&nbsp;5&nbsp;日，扎克伯格的净资产进一步升至&nbsp;2110&nbsp;亿美元。目前，个人财富排名第一的马斯克的净资产为&nbsp;2630&nbsp;亿美元，而排名第三的贝索斯为&nbsp;2090&nbsp;亿美元。</p><p></p><p>今年内，扎克伯格在该指数中的排名跃升了四位，成为彭博亿万富翁榜单上财富增长最快的富豪。他的财富增长得益于&nbsp;Meta&nbsp;股价上涨，今年至今涨幅&nbsp;72%。扎克伯格持有&nbsp;13%&nbsp;的&nbsp;Meta&nbsp;股份，Meta&nbsp;股价在&nbsp;10&nbsp;月&nbsp;4&nbsp;日收盘时达到&nbsp;595.94&nbsp;美元的历史高点，公司市值达到&nbsp;1.51&nbsp;万亿美元。</p><p></p><p>2024&nbsp;年，Meta&nbsp;的业绩多次超过华尔街分析师的预期。Meta&nbsp;的第二季度报告显示，销售额增长&nbsp;22%，达到&nbsp;390.7&nbsp;亿美元，这是该公司连续第四个季度收入增长超过&nbsp;20%。</p><p></p><p>扎克伯格的财富增长不仅反映了&nbsp;Meta&nbsp;公司的强劲市场表现，也凸显了他在科技行业的领导地位和持续创新的能力。随着&nbsp;Meta&nbsp;在虚拟现实、增强现实和社交媒体等领域的不断拓展，扎克伯格的财富有望进一步增加，他在全球富豪榜上的排名也有望继续提升。</p><p></p><h4>Character.ai&nbsp;放弃开发&nbsp;AI&nbsp;模型，与谷歌达成&nbsp;27&nbsp;亿美元交易</h4><p></p><p>根据《金融时报》的报道，谷歌以&nbsp;27&nbsp;亿美元的价格获得了&nbsp;Character.ai&nbsp;技术的一次性许可，同时重新雇佣了该公司&nbsp;20%&nbsp;的员工，包括两位联合创始人&nbsp;Noam&nbsp;Shazeer&nbsp;和&nbsp;Daniel&nbsp;De&nbsp;Freitas。这一交易不仅改变了&nbsp;Character.ai&nbsp;的发展方向，也在&nbsp;AI&nbsp;行业引发了广泛讨论。</p><p></p><p>最近&nbsp;Character.ai&nbsp;的临时首席执行官&nbsp;Dominic&nbsp;Perella&nbsp;在接受《金融时报》采访时表示："训练前沿模型的成本变得异常昂贵……即使对于一家非常大的初创公司预算来说，这也极其难以承受。"这一声明清楚地表明了&nbsp;Character.ai&nbsp;放弃开发自有&nbsp;AI&nbsp;模型的主要原因。</p><p></p><p>在与谷歌达成交易后，Character.ai&nbsp;决定将重心转移到其广受欢迎的消费者产品上，特别是面向&nbsp;13-25&nbsp;岁年轻用户的聊天机器人平台。Perella&nbsp;表示："我们的消费者产品获得了令人难以置信的吸引力，公司内部出现了一种二分法，一些人希望专注于训练最前沿的模型，而另一些来自消费者背景的人则看到这个产品正在起飞。"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GiyOQvFzRw355bO5kLus</id>
            <title>OpenAI 的“愚蠢”把戏，已经把大型科技企业“彻底毒害”了</title>
            <link>https://www.infoq.cn/article/GiyOQvFzRw355bO5kLus</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GiyOQvFzRw355bO5kLus</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 06:15:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>“OpenAI 匆忙推出 o1 模型（一个大型、愚蠢的把戏）、有关未来 OpenAI 模型价格上涨的传闻、Scale AI 的裁员，以及 OpenAI 高层的离职。这些都是事情开始走向崩溃的迹象。”国家媒体关系和公共关系公司 EZPR 的首席执行官 Edward Zitron 日前写了一篇文章表达了对生成式人工智能发展的担忧。Zitron 认为，生成式人工智能的繁荣——是不可持续的，最终必将面临崩溃，他还担心这场崩溃可能会给大型科技公司带来灾难性的打击，严重破坏创业生态系统，并且会进一步削弱公众对科技行业的信任。他还重点指出人工智能泡沫破裂可能带来的人力成本。无论是微软和谷歌（以及其他大型生成式 AI 的支持者）逐渐减少他们在这个领域的投入，还是为了维持 OpenAI 和 Anthropic（以及他们自身的生成式 AI 项目）的活力而消耗他们的资源，他确信最终的结局都是一样的。成千上万的人可能会失业，科技行业的大部分领域可能会遭受重创。“解释当前形势的不稳定性以及我们为何陷入了这种魔法思维的低谷至关重要。”本文对 Zitron 的分析文章进行了翻译，并在不改变作者原意的基础上做了删减，以飨读者。</blockquote><p></p><p></p><p></p><h3>生成式 AI 靠什么活着</h3><p></p><p></p><p>OpenAI，这个表面上的非营利组织，可能很快就会变成盈利实体。为了活下去，OpenAI 将不得不继续筹集资金，其规模将超过以往的任何一家初创公司。</p><p></p><p>目前 OpenAI 正在进行一轮融资，预计这轮融资将筹集至少 65 亿美元，甚至可能高达 70 亿美元，由 Thrive Capital 领投，有传言称 NVIDIA 也将参与。更令人担忧的是，OpenAI 还试图从银行筹集 50 亿美元，采取的是“循环信贷设施”的形式，而这种信贷设施的条款往往具有更高的利率。</p><p></p><p>另外，OpenAI 正在与阿拉伯联合酋长国支持的千亿美元投资基金 MGX 谈判，同时也可能从阿布扎比投资局筹集资金。这无疑是一个警示信号，说明情况可能并不乐观，因为没有人会选择从阿联酋或沙特筹集资金，除非他们真的迫切需要。</p><p></p><p>OpenAI 今年早些时候曾尝试以 1000 亿美元的估值进行融资，但一些投资者对这一价格感到不满，部分原因是他们对生成式 AI 公司被高估感到担忧。</p><p></p><p>为了完成这一轮融资，OpenAI 可能转变为盈利实体。报道称，这一轮的投资者被告知，“他们的投资不会换来传统的股权……相反，他们将得到承诺：一旦公司开始盈利，他们将获得公司利润的份额。”</p><p></p><p>目前尚不清楚转变成盈利实体是否会打消投资者的疑虑，因为 OpenAI 这种奇特的非营利组织结构中包含了盈利分支，这意味着微软作为其 2023 年投资的一部分，将拥有 OpenAI 75% 的利润权——尽管转变成盈利结构理论上可能涉及股权分配。尽管如此，OpenAI 实际上会给你“利润参与单位”（PPU），而非传统股权，即“如果你拥有 OpenAI 的 PPU，而公司从未盈利，或者你没有将它们卖给那些认为 OpenAI 最终会盈利的人，那么你手中的 PPU 将一文不值。”</p><p></p><p>路透社发布的一份报告指出，1500 亿美元的估值将取决于 OpenAI 是否能够成功地重新调整整个公司结构。 在这个过程中，可能会取消对投资者利润上限的限制，这些限制最初设定为原始股份的 100 倍。这种设有上限的利润结构是在 2019 年引入的，当时 OpenAI 表示，任何超出该上限的利润将“返还给非营利组织。”然而，近年来，OpenAI 已经对这一规则进行了调整，允许从 2025 年开始每年将上限提高 20%。</p><p></p><p>考虑到 OpenAI 与微软之间的利润分享协议，更不用说它长期以来的不盈利状态，任何这样的回报，充其量也只能说是理论上的。任何盈利结构的转变也将迫使 OpenAI 与现有投资者重新谈判，他们将看到自己的股份被稀释。此外，投资者必须签署一份运营协议，知悉“对 OpenAI 盈利子公司的任何投资视为捐赠”，并且 OpenAI“可能永远不会盈利”。</p><p></p><p>实际上，投资者并没有获得 OpenAI 的股权， 或者对 OpenAI 的控制权，而是得到了一家每年亏损超过 50 亿美元的公司未来利润的份额。而如果这家公司能够撑到 2025 年，可能会亏损得更多。</p><p></p><p>OpenAI 的模型和产品——我们稍后将讨论它们的实际效果——目前是严重亏损的。</p><p></p><p>OpenAI 在 2024 年向微软支付了大约 40 亿美元，以支持 ChatGPT 及其底层模型的运营，这还是在微软给了优惠的情况下（1.30 美元每 GPU 每小时的成本价，远低于其他客户支付的 3.40 美元到 4 美元）。这意味着，如果没有微软的优惠，OpenAI 每年可能会烧掉大约 60 亿美元的服务器成本，这还不包括人力成本（每年 15 亿美元），以及每年 30 亿美元的训练成本，而且这个数字几乎肯定会随着时间的推移而增加。</p><p></p><p>尽管 The Information 在 7 月份的报道中提到，OpenAI 的年收入在 35 亿到 45 亿美元之间，但《纽约时报》最近的报道指出，OpenAI 的年收入“目前超过 20 亿美元”，这意味着年底的数字可能会趋于预估的下限。</p><p></p><p>总而言之，OpenAI 正在烧钱，而且只会越少越多，为了维持这种烧钱的速度，它将不得不从那些愿意签署知悉“我们可能永远不会盈利”的投资者那里筹集更多的资金。</p><p></p><p>OpenAI 面临的另一个问题是，生成式 AI 并没有解决那些能够证明其巨额成本合理性的复杂问题。由于这些模型本质上是基于概率的，所以存在巨大且难以克服的局限性，它们实际上什么都不懂，只是基于训练数据生成答案（或图像、翻译、摘要等），而模型开发者正在以惊人的速度耗尽这些训练数据。</p><p></p><p>幻觉是一个问题，如果没有数学领域的新突破，这个问题就无法根本解决，尽管我们可以采取措施减少或缓解，但它们的存在使得业务关键型应用很难真正依赖生成式 AI。</p><p></p><p>即便生成式 AI 能够解决上述问题，目前仍然不清楚它是否真的带来了显著的商业价值。</p><p></p><p>The Information 称，微软 365 套件的客户对 AI 驱动的“Copilot”产品几乎没有太多兴趣。在 4.4 亿个用户中，只有 0.1% 到 1% 的用户愿意为这些 AI 功能付费。一家测试了这些 AI 功能的公司表示，“大多数人目前并不认为它具有太大价值”，还有人说，“许多企业没有看到在生产力和其他方面的显著改进”，他们“不确定何时才能看到”。</p><p></p><p>微软对这些非必要的功能收取了多少费用？对于已付费的功能，每人每月额外收取 30 美元，而对于所谓的“Copilots for Sales”，则每月额外收取 50 美元。实际上，他们是要用户将支出翻倍，还需要按年支付。</p><p></p><p>这便是生成式 AI 目前所处的尴尬境地：即使是生产力和商业软件领域的领头羊也难以找到愿意为其产品买单的客户，部分原因在于 AI 产品的平庸，部分原因则是其高昂的成本使得人们很难证明这笔开销的合理性。</p><p></p><p>然而，关于 AI 的争论几乎总是说“AI 的未来会让我们大吃一惊，下一代大模型即将问世，它们将带来难以置信的变革。”最近，我们确实真切地窥见了未来的模样，然而它带来的失望感真是让人难以言表。</p><p></p><p></p><h3>一个大型、愚蠢的把戏</h3><p></p><p></p><p>OpenAI 于周四晚间发布了代号为“草莓”的 o1，引发了广泛的关注。在一系列推文中，Sam Altman 将 o1 形容为 OpenAI 迄今为止“最强大且最稳定的模型”。尽管他指出 o1“仍然有缺陷，仍然有局限，并且在初次使用时可能不如深入了解后那样令人印象深刻”，他仍承诺 o1 在处理有明确答案的任务时，如编程、数学问题或回答科学问题，将提供更准确的结果。</p><p></p><p>这本身就揭示了一些东西。</p><p></p><p>o1 会将一个问题拆解为一系列步骤，理想情况下这些步骤将导向一个正确的答案，这个过程被称作“思维链”。这与其他大模型的工作方式有所不同，因为 o1 不是简单地生成答案并输出，而是在生成答案后进行回顾并评估这些步骤，忽略或确认“好的”步骤，确保最终答案的质量。</p><p></p><p>尽管这听起来像是一个重大的飞跃，甚至可能被视为朝着备受期待的人工通用智能迈出的又一步，但其实不是——OpenAI 选择将 o1 作为一个独立的产品发布，而不是 GPT 的更新，这本身就说明了问题。</p><p></p><p>OpenAI 展示的示例都是有明确答案的，并没有展示 o1 模型尝试解决那些解决方案事先未知的复杂问题。OpenAI 自己也承认，他们已经意识到 o1 比 GPT-4 更容易产生幻觉，并且相较于之前的模型，它不太愿意承认自己不知道某个问题的答案。这是因为，尽管模型的一部分负责检查答案，但这个负责检查的部分本身仍有可能产生幻觉。</p><p></p><p>如果你认为我对 OpenAI 的批评过于严苛，不妨思考一下他们是如何推广 o1 的。它将强化训练过程描述为“思考”和“推理”，而实际上它更像是在猜测，然后在每一步验证这些猜测的正确性，最终目标通常是那些可以预先知晓的结论。</p><p></p><p>这种说法几乎是对人类的一种侮辱，人类的思考是基于一系列复杂因素的行动：从他们的经验，到他们一生中积累的知识，到他们的大脑。虽然我们在推理复杂问题时也可能“猜测”每一个步骤的正确性，但我们的猜测是基于一些具体的东西，而不是像 o1 那样笨拙的数学挣扎。</p><p></p><p>而且，它太贵了。</p><p></p><p>o1 预览版的价格为每百万输入 Token 15 美元，每百万输出 Token 60 美元。它的输入成本是 GPT-4 的三倍，输出成本则是四倍。然而，这还不是全部，它还有一个隐藏成本。</p><p></p><p>数据科学家 Max Woolf 指出，OpenAI 的“推理 Token”在 API 中是不可见的，这意味着 o1 的实际成本更高，因为产品的设计需要它更频繁地向你收费。它在“考虑”（并不是在“思考”）答案时生成的所有内容同样需要收费，这使得像编码这样需要复杂答案的任务可能变得非常昂贵。</p><p></p><p>我们再谈谈准确性。在 Hacker News 上有用户抱怨 o1 在处理编程任务时产生了幻觉，给出了错误的库和函数，并对不容易在互联网上找到答案的问题给出了错误的回答。</p><p></p><p>在 Twitter 上，创业公司创始人兼前游戏开发者 Henrik Kniberg 要求 o1 编写一个 Python 程序，将两个数字相乘，并计算程序的预期输出。虽然 o1 正确地编写了代码（尽管这些代码本可以更简洁，用一行代码而不是两行），但实际结果却大错特错。Karthik Kannan，一位 AI 公司的创始人，也尝试了一个编程任务，结果 o1 产生了幻觉，为它正在使用的 API 生成了一个不存在的命令。</p><p></p><p>OpenAI 声称 o1“在物理、化学和生物学的挑战性基准任务上的表现与博士生相似。”然而，这一说法似乎并不适用于地理学，或是基础的小学英语语言测试、数学和编程领域。这正是我说的那种大型而愚蠢的把戏。OpenAI 匆忙推出草莓，作为一种手段向投资者和公众展示 AI 革命仍在继续，但他们 实际上推出的是一个笨拙、不令人兴奋且昂贵的模型。</p><p></p><p>更糟糕的是，我们很难说清楚为什么人们应该对 o1 感兴趣。</p><p></p><p>人们不再满足于“更好”的答案，他们期待的是一些新的东西，我不认为 OpenAI 在这方面有任何清晰的计划。Altman 试图通过让 o1“思考”和使用“推理”来赋予其人格化特征，这显然是试图暗示这是通往 AGI 的路径的一部分，但即使是最坚定的 AI 支持者也难以对此感到兴奋。</p><p></p><p>实际上，我认为 o1 的推出说明 OpenAI 既绝望又缺乏新想法。</p><p></p><p>价格没有下降，软件也没有变得更有用，而我们从去年 11 月就开始期待的“下一代”模型最终却是个失败之作。这些模型对训练数据的需求如此之高，以至于几乎所有大语言模型都包含了一些受版权保护的材料。这种迫切的需求导致 Runway 发起了“全公司范围的努力”，收集了数千个 YouTube 视频和盗版内容来训练他们的模型，而 8 月份提起的联邦诉讼则指控英伟达为了训练其“Cosmos”AI 软件，对许多创作者做了同样的事情。</p><p></p><p>目前，他们的法律策略完全依赖于侥幸心理，希望这些诉讼没有一个能够达到将训练这些模型定义为版权侵犯的地步。这些诉讼正在向前推进。8 月份，一位法官批准了原告对 Stability AI 和 DeviantArt 的进一步版权侵犯索赔，以及对 Midjourney 的版权和商标侵权索赔。</p><p></p><p>如果这些诉讼中的任何一个胜诉，对 OpenAI 和 Anthropic 来说都将是灾难性的，对谷歌和 Meta 来说更是如此，他们的 Gemini 和 Llama 模型使用的数据集包含了数百万艺术家的作品，主要是因为 AI 模型“几乎不可能”忘记训练数据，这意味着他们需要从头开始重新训练，这将需要花费数十亿美元，并大幅降低它们在本就不擅长的任务上的效率。</p><p></p><p>整个行业似乎建立在不稳定的基础之上。像 ChatGPT、Claude、Gemini 和 Llama 这样的大模型是不可持续的，并且由于生成式 AI 的计算密集特性，似乎没有可行的盈利模式。训练它们需要花费数亿美元，甚至数十亿美元，并且需要如此大量的训练数据，以至于这些公司实际上已经从数百万艺术家和作家那里窃取了数据，并希望自己能逃避惩罚。</p><p></p><p>即便将这些问题放在一边，生成式 AI 及其相关架构似乎也并未实现任何革命性的突破，而且关于生成式 AI 的炒作周期似乎并未真正触及“人工智能”这个术语的深层含义。充其量，生成式 AI 似乎有时能够正确地生成内容、总结文件，或者以不确定的“更快”的水平做研究。</p><p></p><p>我们并非处于“早期阶段”。自 2022 年 11 月以来，大型科技企业已经在他们自己的基础设施、AI 初创企业以及他们自己的模型上投入了超过 1500 亿美元的支出。整个行业对生成式 AI 的大规模投入，结果却只是出现了四、五个几乎相同的大语言模型、世界上最不盈利的初创企业，还有数千个价格昂贵且令人失望的集成产品。</p><p></p><p>大语言模型实际上已经达到了一个平台期。“更强大”似乎从来不意味着“能做更多”，而是意味着“成本更高”，这说明他们又制造出了一种不增加任何新功能但运行成本更高的产品。</p><p></p><p>如果所有风险投资家和大型科技巨头的合力还没有找到一种有意义的应用场景，让大部分人愿意为之付费，那么这样的场景就不太可能突然出现。大语言模型不会仅仅因为大型科技企业和 OpenAI 又投入了另外 1500 亿美元而神奇地获得新的能力。没有人在尝试让这些模型变得更高效，或者至少没有人成功地做到这一点。如果他们做到了，他们早就大肆宣扬了。</p><p></p><p>我们所面对的是一种共同的幻觉：一种死胡同一样的技术，它依赖版权盗窃、需要持续的资本注入，同时它所提供的服务在最好的情况下也是非必需的，它被包装成一种尚未实现的自动化，耗费了数十亿美元，而且可能会永远如此。生成式 AI 不单单靠金钱在运行，还有信仰，问题是信仰是一种有限的资源。</p><p></p><p>我们可能正处于一场 AI 次贷危机之中，成千上万的公司已经支付了过高的费用来集成 AI 技术，而这些技术可能并不稳定，也不一定能够带来预期的收益或回报。</p><p></p><p></p><h3>科技巨头的两难境地</h3><p></p><p></p><p>几乎所有标榜为“由 AI 驱动”的初创企业所采用的 LLM 功能，都是基于 GPT 或 Claude 的某种组合。这些模型是由两家深陷亏损的公司提供的——Anthropic 今年预计亏损 27 亿美元——它们的定价策略旨在吸引更多客户，而不是为了赚取利润。</p><p></p><p>OpenAI 得到了微软的补贴，它的定价完全依赖微软的支持，无论是作为投资者还是服务提供商。Anthropic 在与亚马逊和谷歌的交易中也面临着同样的问题。考虑到他们目前的亏损状况，如果 OpenAI 或 Anthropic 按照实际成本收费，API 调用的价格可能会增加十到一百倍，尽管我们没有确切的数字来确定具体数字。</p><p></p><p>然而，让我们考虑一下这个事实：OpenAI 在 2024 年与微软的服务器成本将达到 40 亿美元——要知道，微软向其他客户收取的费用是这个的 2.5 倍——然后考虑到 OpenAI 每年仍然亏损超过 50 亿美元。</p><p></p><p>OpenAI 很可能只收取了运行模型所需成本的一小部分， 并且只有在能够持续筹集到比以往更多的风投资金，并继续从微软获得优惠定价的情况下，才能继续这样做。而微软最近提到它将 OpenAI 视为竞争对手。合理相信，Anthropic 从亚马逊和谷歌那里获得了类似的优惠定价。</p><p></p><p>假设微软给了 OpenAI 100 亿美元的云信用额度，而它在服务器成本上花费了 40 亿美元，再假设在模型训练上花费了 20 亿美元，这些成本随着新的 o1 和“Orion”模型的到来肯定会增加，那么 OpenAI 将需要更多的额度，或者将在 2025 年的某个时候不得不开始向微软支付现金。</p><p></p><p>虽然微软、亚马逊和谷歌有可能无限期延长他们的优惠定价，但问题是这些交易对他们来说是否真正有利可图。正如我们在微软最近一个季度的收益报告后所看到的，投资者对于构建生成式 AI 基础设施所需的支出越来越关注，许多人对这项技术的潜在盈利能力表示怀疑。</p><p></p><p>我们不清楚的是，对于科技巨头来说，生成式 AI 的盈利能力究竟如何，因为他们将这些成本摊派到了他们的其他收入中。虽然我们无法确定具体数字，但我认为如果这些技术真的能够盈利，他们肯定会公开谈论从中获得的收入。</p><p></p><p>但他们没有。</p><p></p><p>市场对生成式 AI 的繁荣持怀疑态度，黄仁勋对 AI 的投资回报率没有给出明确的答案，导致英伟达的股票市值一度暴跌，这是美国市场历史上最严重的一次暴跌，跌去的总市值相当于近五个雷曼兄弟在其峰值时的市值。</p><p></p><p>在八月初，微软、亚马逊和谷歌都因为与 AI 相关的巨额资本支出而受到市场的打击，如果他们不能展示出他们从投入的 1500 亿美元（甚至可能更多）到新的数据中心和英伟达 GPU 中获得的收入显著增加，他们下个季度都将面临困境。</p><p></p><p>需要注意的是，除了 AI，大型科技企业似乎已经没有其他增长点了。当像微软和亚马逊这样的公司开始显示出增长放缓的迹象时，他们迫切希望向市场展示他们仍然拥有增长动力。谷歌，一个几乎完全依赖搜索和广告的企业，也需要一些新的和吸引人的东西在华尔街面前展示——但这些都没有奏效，因为产品不够有用，而且看起来它的大部分收入来自公司“尝试”AI，然后意识到它真的不值得。</p><p></p><p>现在有两个可能的结果：</p><p></p><p>大型科技企业意识到他们已经深陷其中，出于对激怒华尔街的深切恐惧，选择减少与 AI 相关的资本支出。大型科技企业迫切希望找到新的增长点，决定采取相反的策略：削减成本以维持运营，通过裁员并将资本从其他部分抽出进行重新分配，作为维持生成式 AI 死亡行军的手段。</p><p></p><p>目前还不清楚哪一种情况会发生。如果大型科技企业接受生成式 AI 并非未来，他们真的没有其他东西可以在华尔街面前展示，但可以减少资本支出（并裁员），同时承诺“减缓投资”。这最有可能是亚马逊和谷歌采取的策略，他们虽然迫切希望让华尔街高兴，但至少目前还有自己有利可图的垄断领域。</p><p></p><p>然而，未来几个季度需要有来自 AI 的实际收入增长，并且必须是实质性的，而不仅仅是关于 AI 是一个“成熟市场”或“年化运行率”的某种模糊说法。如果资本支出也随之增加，那么这种实质性的收入将必须更高。</p><p></p><p>我不认为华尔街会看到他们期待的实质性收入增长，无论是 2024 年第三季度还是第四季度，甚至是 2025 年第一季度，华尔街可能会开始因为贪婪的罪行而惩罚大型科技企业，并且这种惩罚可能会比英伟达所经历的更加严厉。尽管黄仁勋大吹特吹，但英伟达确实是市场上唯一能够真正说出 AI 正在为他们增加收入的公司。</p><p></p><p>我有点担心第二种情况更有可能发生：这些公司深深地致力于“人工智能是未来”的理念，他们的文化与真正解决人类问题的使命脱节，甚至可能会冒着拖垮整个公司的风险。我非常担心大规模裁员会成为他们的一个手段，至少他们过去几年的作为都没有让我认为他们会做出正确的选择。</p><p></p><p>大型科技企业已经被管理顾问彻底毒害了——亚马逊、微软和谷歌都是由 MBA 运营的——反过来，他们周围都是类似的人物，比如谷歌的 Prabhakar Raghavan，他赶走了真正建立谷歌搜索的人，以便他可以掌控运营。</p><p></p><p>这些人并没有真正面对人类所面临的挑战，而是构建了一种企业文化，专注于解决那些软件能够轻易修复的虚构问题。当你的生活充斥着各种会议和电子邮件，生成式 AI 似乎显得格外神奇。我猜想，纳德拉的成功心态可以归结为“让技术人员去解决它”。如果皮查伊只是简单地看着微软对 OpenAI 的投资后一笑置之，他本可以轻易地终结整个生成式 AI 热潮，然而，他没有这么做，他不得不选择跟风，因为这些人没有真正的想法。这些公司并非由那些亲身经历问题的人运营的，更别提那些知道如何解决问题的人了。</p><p></p><p>他们也感到绝望，因为以前从未出现过这样的情况。Meta 在元宇宙上烧掉了数十亿美元，但这次的情况更加严重，也更加丑陋，因为他们投入了这么多钱，彻底将 AI 牢牢地焊在了他们的公司品牌上，以至于任何试图剥离的尝试都将是尴尬的，这也将对他们的股价造成伤害，并且无疑是默认承认了这一切投资都是徒劳无功的。</p><p></p><p>这一切本可以更早地被叫停。这种炒作不过是重复了过去骗局的模式，媒体似乎默认这些公司最终会“搞定一切”，尽管有迹象表明他们并不会。如果你认为我是个悲观主义者，那么请回答这些问题：这里的计划究竟是什么？生成式 AI 的未来是怎样的？如果你的回答是他们会“解决问题”或者他们“有秘密武器”，那么你可能就是这场营销游戏无意的参与者。</p><p></p><p>至少微软会开始削减其业务其他领域的成本，以此来支撑人工智能的繁荣。在今年早些，微软的高层领导团队提出了一个（最终未被采纳的）计划，旨在减少公司多个领域的电力消耗来支持 GPU，包括将其他服务的计算任务转移到其他国家。</p><p></p><p>在匿名社交网络 Blind 的微软板块（用户需要通过验证所属公司的企业电子邮件来参与讨论），一位微软员工在 2023 年 12 月中旬抱怨说“AI 夺走了他们的钱”，“AI 的成本高得惊人，以至于它正在蚕食加薪预算，而且这种情况不会有所改善。”到了 7 月中旬，另一位员工认为微软“有一种近乎成瘾的削减成本倾向，用运营现金流支撑英伟达的股价”，这种做法“深深地破坏了微软的企业文化。”</p><p></p><p>另一位员工补充说，他们相信“Copilot 将在 2025 财年给微软带来灾难，对 Copilot 的关注将在 2025 财年显著减少”。他们知道“在他们的国家有一笔大的 Copilot 交易，但在 PoC 进行了近一年之后使用率还不到 20%，随之而来的是削减预算和裁员”，并补充说“公司承担了太多风险”，而且他们认为微软的“巨额 AI 投资不会得到回报。”</p><p></p><p>很多帖子提到了雷德蒙德（微软总部所在地）的文化问题，包括与高层领导的脱节，以及只有在项目被贴上 AI 标签时才能获得资助的现象。多个帖子对纳德拉的“词沙拉”式管理方法表示失望，并且抱怨在一个专注于追求可能并不现实的 AI 繁荣的组织中，缺乏奖金和晋升机会。</p><p></p><p>至少，公司内部弥漫着一种深深的文化忧郁。许多帖子在表达“我不喜欢在这里工作”、“我不明白我们为什么要如此执着于 AI”和“接受现实吧，因为纳德拉并不在乎”之类的情绪。</p><p></p><p>在 The Information 的一篇关于微软 Office AI 功能使用率低的文章中，提出了一个特别令人担忧的观点，涉及微软庞大的数据中心支出的实际收益：</p><p></p><p></p><blockquote>其他迹象也证实了这些估计：大约在今年 3 月，据直接了解这些计划的人士透露，微软已经在其数据中心为 365 Copilot 预留了充足的服务器容量来应对可能只有几百万使用 AI 助手的用户。当时，无法确定实际使用了多少容量。</blockquote><p></p><p></p><p>The Information 估计，微软的 Office Copilot 功能大约有 40 万到 400 万用户，这意味着微软很可能预留了超出实际使用需求的服务器容量。</p><p></p><p>早在今年 3 月，我就曾提出一个问题，我找不到任何公司能够以真正有利于其财务底线的方式集成生成式 AI，而在六个月后，我仍然找不到。</p><p></p><p>大公司最好的做法似乎是将 AI 功能附加到已有的产品上，并希望这能推动销售增长，但这种做法似乎没有任何效果，或者像微软那样，提供“AI 升级”，但似乎并没有提供真正的商业价值。</p><p></p><p>尽管一些公司可能在“集成”AI 技术时在微软 Azure、亚马逊和谷歌云上增加了一定程度的消费，但我怀疑这种需求在很大程度上是由投资者情绪所驱动，他们似乎更倾向于通过“投资 AI”来迎合市场，而不是基于成本效益或实际效用方面的考虑。</p><p></p><p>然而，鉴于这些公司已经在集成生成式 AI 能力方面投入了大量的时间和资金，我预计他们可能会面临以下几种情况之一：</p><p></p><p>在开发并推出这些 AI 功能之后，他们可能会发现客户并不愿意为此买单。如果他们现在找不到一种方法来说服客户付费，那么在 AI 热潮消退后，一旦老板们不再要求员工“拥抱 AI”，他们将更难说服人们掏腰包。在开发并推出这些 AI 功能之后，他们似乎难以找到让客户为此额外付费的方法，这意味着他们可能不得不在没有增加利润的情况下将 AI 功能集成到已有的产品中，而这些 AI 功能可能会变成侵蚀收入的寄生虫。</p><p></p><p>我担心的是可能出现的连锁反应。我相信许多企业目前正在“尝试”使用 AI 技术，而一旦这些尝试结束（据高德纳预测，到 2025 年底，大约 30% 的生成式 AI 项目在完成概念验证后将被放弃），他们可能会停止为这些额外功能支付费用，或者停止将生成式 AI 集成到他们的产品中。</p><p></p><p>如果这种情况真的发生，那么流向为生成式 AI 应用提供云计算服务的超级巨头以及像 OpenAI 和 Anthropic 这样的大语言模型提供商的收入将会进一步减少，而这可能会进一步压低这些公司的价格。到了那个时候，OpenAI 和 Anthropic 几乎肯定会被迫提高价格，如果他们还没有这么做的话。</p><p></p><p>尽管大型科技公司可以继续通过烧钱来维持这一繁荣——毕竟，这一繁荣的存在几乎就是因为它们——但这并不能帮助那些习惯了折扣之后却无法负担运营成本的初创企业。尽管有更经济的选择，比如由独立供应商提供的 Llama 模型，但很难相信他们不会面临与超级巨头完全相同的盈利问题。</p><p></p><p>同样重要的是，超级巨头们也害怕激怒华尔街。</p><p></p><p>虽然他们理论上可以通过裁员和其他削减成本的措施来改善利润率，但这些都是短期解决方案，只有当他们能够设法从贫瘠的生成式 AI 树上摇下一些钱时才是真正有效的解决方案。</p><p></p><p>无论如何，是时候接受钱并不在那里的现实了。是时候停下来正视我们正处于科技行业的第三个妄想时代。然而，与加密货币和元宇宙不同，每个人都加入了这场狂欢，决定烧钱追求一个不可持续、不可靠、无利可图、破坏环境的愚蠢行为，还将其作为“人工智能”卖给客户和企业，虽然承诺将“自动化一切”，却从未有过实现这一承诺的途径。</p><p></p><p>那么，为什么这种情况不断发生？为什么我们一次又一次地看到这样的泡沫？</p><p></p><p>这其实是科技行业完全专注于提高每个客户的价值而不是为客户提供更多价值的结果。或者说，他们并没有真正了解他们的客户是谁，以及他们真正需要什么。</p><p></p><p>今天企业向你推销的产品，都试图将你绑定到一个特定的生态系统中，这些生态系统由微软、苹果、亚马逊或谷歌等巨头掌控，这反过来增加了你离开这些生态系统的难度。</p><p></p><p>甚至加密货币表面上是一种“去中心化”的技术，也很快放弃了它的自由主义思想，并试图将用户集中在 Coinbase、OpenSea、Blur 或 Uniswap 等几个大平台上，而所有这些平台都由同一家风险投资公司（如 Andreessen Horowitz）提供资金支持。元宇宙是扎克伯格试图主导的下一代互联网的尝试，其中一个占主导地位的平台是“Horizon”。</p><p></p><p>一切都是为了进一步货币化。生成式 AI 之所以令人兴奋，是因为大型科技公司将其视为下一个伟大的货币化工具，一种从消费科技和企业产品上创收的手段。大多数生成式计算要么通过 OpenAI 或 Anthropic 进行，然后反过来流向微软、亚马逊或谷歌，为他们创造云计算收入。这里的最大创新不在于生成式 AI 做了什么或者能做什么，而在于创造了一个无可救药地依赖少数超级巨头的生态系统，并且很难摆脱这种依赖。</p><p></p><p>生成式 AI 可能并不是特别有用，但它非常容易集成到各种应用中，创造出理论上能够让公司可以从中获利的“新事物”。聪明的 Sam Altman 意识到，技术行业需要一个新的“东西”，一种每个人都可以分一杯羹的新技术，尽管他可能并不真正理解这项技术，但他理解更大经济体的增长欲望，并把基于 Transformer 的架构产品化，作为一种每个人都可以销售的神奇工具。</p><p></p><p>问题在于，集成生成式 AI 的迫切需求揭示了这些公司与实际消费者需求的脱节。20 年来，推出“新事物”这一招一直管用，从某种意义上说，推出新事物并迫使销售人员推销它就足以保持增长，以至于技术行业的领导者已经接受了这种有毒且深不可测的业务模式。</p><p></p><p>运营这些公司的人，几乎都是从未从零开始构建过产品或技术的 MBA 和管理顾问，要么不理解要么不关心生成式 AI 是否有明确的盈利途径。他们可能假设它会像亚马逊那样自然变得有利可图，尽管这是两个非常不同的东西。在过去“事情就这样解决了”，那么今天为什么不行呢？</p><p></p><p>真正令人担忧的是，除了 AI，这些公司似乎没有其他新产品。 而这就是问题所在，因为当它失败时，影响将不可避免地波及到技术领域的其他公司。</p><p></p><p>每一个主要的参与者，无论是在消费领域还是企业领域，都在销售某种 AI 产品，要么集成其中一个模型或他们自托管，但这些都无一例外地在大型科技公司的系统上运行。在某种程度上，每一家公司都依赖这些大型科技公司愿意为整个行业提供的补贴。</p><p></p><p>我预测，一场 AI 次贷危机正在悄然酝酿。</p><p></p><p>整个技术行业似乎都接受了一种模式：以极低的价格出售技术产品，这些产品高度依赖大型科技公司的补贴。然而，这种模式可能不会持久，一旦生成式 AI 的烧钱速度加快，这些公司可能会被迫提高价格，或者推出价格高得离谱的新产品和功能。</p><p></p><p>当整个技术行业都依赖于一种从一开始就没有创造太多价值、只赔钱的软件时，会发生什么？随着热潮的消退，这些 AI 产品可能会变得难以为继，而这些公司又没有其他东西可卖，会发生什么？</p><p></p><p>我真的不知道。但技术行业似乎正在构建一个基于奖励增长而非创新、垄断而非忠诚、管理而非实际建设的经济增长模式，这种模式缺乏创造力，可能导致一场可怕的清算。</p><p></p><p>原文链接：</p><p></p><p><a href="https://www.wheresyoured.at/subprimeai/">https://www.wheresyoured.at/subprimeai/</a>"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H3apoNFt8TKhGN069j89</id>
            <title>从再三拒绝到带头研发，Meta CTO “真香”：我曾觉得VR/AR是个大坑，是小札“疯了”</title>
            <link>https://www.infoq.cn/article/H3apoNFt8TKhGN069j89</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H3apoNFt8TKhGN069j89</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 06:11:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>Meta 近期发布的 AR 头显 Orion 赢得了一片赞誉。发布后，Meta 首席技术官 Andrew Bosworth （AB）在最近接受 Ben Thompson（BH）时谈到了他在 Meta 的工作历程，同时回应了不少争议问题，包括 Meta 公司需要尽快让大家看到 Orion，以此为证明自己在 Reality Labs 身上砸下的数十亿美元物有所值。AB 谈到了 AI 在其中的作用：在 AR 方面，AI 确实是最核心的技术，在混合现实和虚拟现实方面，AI 更多扮演的是启动器的角色。他还谈了与苹果之间的竞争，他即认为苹果的投入赢得了市场关注，但也对他们把手机和设备锁定得太狠。下面是访谈的原文，我们在不改变愿意基础上进行了删减，以飨读者。</blockquote><p></p><p></p><h3>扎克伯格，曾经的学生、现在的老板</h3><p></p><p></p><p>BH：你当初为什么会选择从事科技行业？</p><p></p><p>AB：我在湾区长大，科技一直是我个人生活中的组成部分。有趣的是，我在湾区南部的一个马场长大，所以可能跟很多朋友猜想的不一样，来自那种农业企业家的家庭。农民之间做交易的方式很简单，“嘿，你有马匹要卖吗？”“好啊，我们这边是粪肥太多了，那咱们就相互交换吧。”做生意不就是在互通有无嘛，所以我对创业之类的事情很感兴趣。</p><p></p><p>硅谷是个独特的地方。我上高中的时候去过那里，当时参观的是 Silicon Graphics 和惠普公司，这些地方都很酷。但实际上，电子游戏最后让我下定了学习计算机科学的决心——特别是《合金装备》，玩过它之后我下定决心要搞人工智能。那是第一款拥有半智能 AI 设计的游戏作品。虽然按今天的标准，它的效果远远算不上智能，但在当时 MGS3 绝对是震惊业界的作品。</p><p></p><p>所以我去了哈佛大学读本科，在那里学习计算机科学，但主业其实是计算神经生物学。回想起来，在学校里学到的所有东西当中，最重要的经历就是我大四那年担任一门课程的助教——人工智能入门。随机分配之下，我阴差阳错地遇上了一名学生、也是我现在的老板，马克·扎克伯格。</p><p></p><p>BH：我记得你教过马克·扎克伯格，但总是搞混具体时间，因为我又记得你好像没在学校正式任过教。</p><p></p><p>AB：确实没有正式任教，当时我是大四的学生，而他正在读大二。在数学和计算机科学专业里，本科生负责教授一门课程其实挺常见的。因此教授每周会讲两到三天课，同一门课程的其余部分则由一名本科生负责——教材是完全相同的，但本科生时间更多，所以能给出更详尽的解答，还会给作业评分、组织考试等等。总之，我在大二和大三的时候都教过计算机科学入门课 CS50，并且在大四的时候成为 CS182 的首席教学研究员，跟着教授 David Parkes。</p><p></p><p>扎克伯格是被随机分到我课上的。顺带一提，很多教过我的人后来也成了 Facebook 的早期工程师，大家彼此之间有着奇妙的师生缘分。</p><p></p><p>BT：现在你担任 CTO 了，大家应该是相处得比较融洽吧？</p><p></p><p>AB：没错，之前我曾经在微软工作了大概 15 个月。时间不长，期间我开发出一款名为 Visio 的出色软件。还有 ShapeSheet，只是产品质量虽好，但却没有得到市场的广泛认可。</p><p></p><p>我很享受在微软的时光。我的顶头上司很棒，工作环境也很舒适。但突然有一天，我收到了 Facebook 招聘人员发来的 AOL 私信，对方说“嘿，过来试试呗。”我琢磨着这就像一段免费的探亲之旅，毕竟那边离我的老家湾区很近。所以我一冲动就跑了过去，之后意外被扎克伯格和他团队的宏大愿景震撼。我亲眼见识过他的团队，特别是 Jeff Rothschild，一切都令人印象深刻、难以忘怀。</p><p></p><p>BT：有意思。所以你们在哈佛大学甚至是进入 Facebook 之前关系都不算近。一直到加入 Facebook，你跟扎克伯格都只能算是点头之交。**</p><p></p><p>AB：是的。我是 Facebook 的第 1681 号用户，而扎克伯格是 4 号用户。我觉得从技术上讲，前三个应该是测试账户，所以实际编号应该再减去 3。总之我们相互认识，但关系并不算密切。</p><p></p><p>后来他告诉我，当时招聘人员问过他对新人的需求，他说“我想做这个项目”，也就是后来的 News Feed。他的要求是“一个懂 AI 的人，你们有什么推荐吗？”招聘人员说“我倒是想起一个”，于是跑来找到我。大家最终一拍即合。</p><p></p><p>Meta 首席产品官 Chris Cox 和我坐在一起，就是跟他一起混多了，我才会秃头。他负责开发前端，我负责开发后端，Ruchi Sanghvi 是产品经理，我们共同打造出了 News Feed。这项工作始于 2006 年 1 月 9 号，当时我刚刚过完 24 岁生日。</p><p></p><p>BT：我一直很关注企业的早期发展阶段，期间甚至可能会激怒用户，真的会有人打着横幅到办公室外面抗议。</p><p></p><p>AB：确实，我们都经历过。</p><p></p><p>BT：你们坚持了下来，第一次遭遇大量抗议时也没有妥协。这到底是顺势而为，还是说真的顶住压力、告诉自己“这是对的，必须坚持下去”？</p><p></p><p>AB：我很清楚自己需要坚持什么，又面临怎样的压力。如果你当时也跟我们一样经常使用 Facebook 的产品，就会发现它首先是一款面向大学生的校园类软件。而我自己也读过大学，就是这款产品的核心受众，所以我始终相信自己的选择就是正确答案。</p><p></p><p>但产品发布时确实出了不少问题。我总爱打个比方，就像大家都聚在一场派对上，现场很吵。可就在我们跟某人说话的瞬间，音乐突然停止了。这时候就很尴尬，我们不知道到底要不要把这句话说完。基本上，使用 Facebook 产品的上千万用户就是这种情况，也确实做出了强烈的反应。一旦真搞砸了，后果真的不堪设想。但好在后来使用量不断翻番，而且再也没有下降过。</p><p></p><p>在有了这样一款热门产品之后，我们只需要认真考虑“用户做出了怎样的反馈，我们又该如何在未来加以整合？”但你说得没错，Facebook 确实从中总结出了处理问题的典型思路，即步子不停、修复不断。“哪怕出了问题，哪怕遇到了挑战，我们也要以前瞻性的方式把反馈整合进后续开发当中。”这就是长久以来我们跟用户群体之间的关系。现在双方的关系已经相当稳定，我们在功能发布方面也确实做得更好了。</p><p></p><p>BT：你在写“丑恶”备忘录那会负责的是什么工作？</p><p></p><p>注：“丑恶”备忘录是 2016 年的主题为“丑陋”的备忘录，AB 在其中写道，“丑陋的现实是，我们就是要为人们提供连接，任何能让我们连接更多人的做法‘事实上’都是好的。”*</p><p></p><p>AB：讽刺的是，我当时做的事情跟那份备忘录里讨论的内容完全无关——我当时正在经营广告业务。</p><p></p><p>BT：就是说你压根不在增长部门。</p><p></p><p>AB：不光不在增长部门，我甚至就没参与过那些讨论。因为当时我正在经营广告业务，这是一份我喜欢而且非常重要的工作。我根本就没有像如今这样真正参与到过其他部门的对话中来。</p><p></p><p>我之所以写下这份备忘录，是因为当时 Facebook 内部正在进行一场大讨论。我把它提炼成了清晰的形式，就这么简单。所以很高兴你也能正确看待它，毕竟其中的内容都是真实发生过的，而我自己所做的就是把它整理出来让更多人知道。</p><p></p><p>如果非要说，我本应该在备忘录里加入更多讨论过程中的细节，让更多人能够体会内中难处。</p><p></p><p>BT：确实，但那毕竟只是一份内部备忘，没必要苛求。</p><p></p><p>AB：确实。随着时间推移，我最大的感受就是当初撰写备忘录的情境开始逐渐消散。当时内部人士更能理解这份备忘录的背景，所以我才可以直接把情况整理成极其精简的版本。而一旦把它拿给外面的人们，大家就会觉得缺少背景支撑。</p><p></p><p></p><h3>“我曾觉得这可能是个大坑”</h3><p></p><p></p><p>BT：在去年夏天，你提到加入 Reality Labs 的时候，觉得之前没怎么跟该部门接触过是件好事，因为这样你才能用全新的眼光看待事物。</p><p></p><p>AB：在职业生涯当中，扎克伯格曾经多次跟我提起过这一点。他总是把我推向新的业务。就拿当初的广告部门来说，他就提到“我觉得你应该试试做广告”，我前后两次拒绝了他。但他还是坚持说，“真的，我觉得你应该去试试”。最后，我就加入了进来，而且发现自己不仅有能力做好这事，而且还乐在其中。而且重要的是，身为一个局外人，我能够以全新的、甚至有点像批评家的视角看待部门所面对的机遇。这里的空间很大，靠做广告赚钱虽然不是特别难，但想做好也不容易。</p><p></p><p>后来同样的事情再次发生，而且过程还挺有趣。当时我刚刚有了第二个孩子，是个女儿。扎克伯格跑过来。对了，咱们正好也说说他。</p><p></p><p>很多人没有意识到扎克伯格是个多么优秀的人、多么伟大的合作伙伴。我两个孩子出生之后，第一个来看望他们的非家庭成员就是扎克伯格了。这其实很不容易，毕竟他可是一家大企业的 CEO。</p><p></p><p>当时他马上就来探望，一边抱着我女儿一边说，“嘿，我觉得你应该换个岗位。”我的反应是，“赶紧把女儿还给我吧，你这是想打感情牌占我的便宜，咱们最好重新平衡一下。”而让我大出所料的是，他给我的新工作就是管理 Rality Labs，而之前我一直是公司内部最反对这项业务的批评者。</p><p></p><p>我说，“你是不是疯了，我跟你说，我觉得咱们压根就不该搞这门业务。”但他针锋相对地提到，“明白，但你也可以花点时间想想，到底要怎么调整才能让你认同这门业务。”</p><p></p><p>BT：你当时为什么觉得 Facebook 不应该在 VR/AR 方面投入精力？</p><p></p><p>AB：对于身处应用层的开发者来说，长时间的参与会让我习惯了应用层那种美妙的整洁感。毕竟最早我们还想过要做手机呢，但最终还是决定重新回归应用程序，只是必须要做出最好的应用程序，让它无处不在。那段经历让我学会了很多，而这次加入 Reality Labs 也相当于历史再度重演。所以，你应该能理解我为什么比较抗拒去做平台，甚至觉得这可能是个大坑，因为我们真的不具备这方面的专业知识。</p><p></p><p>所以我表达了负面观点，甚至再一次明确拒绝。扎克伯格也没有强迫，只是说“好吧，那描述一下你理想中的项目应该是什么样子。”于是，我专门写了份文件，当时大家更多关注的是虚拟现实 VR，而 AR 仍然只是个研究项目。可我个人从一开始就更相信 AR，这可能是最大的不同。</p><p></p><p>BT：对于任何怀疑 VR 领域的关注者来说，最核心的理由应该都是 AR 更有前途。</p><p></p><p>AB：没错，但当时 VR 的投资规模更大，吸引到的资金也更多。当时我曾写过文章，认为最终成果应该不只是娱乐设备，而必须是一种能够更深入地渗透到我们工作结构中的产物。比如，要打造出超逼真的 Codec 化身，还附带更强大的进化版 Workrooms 之类软件，让人们真正能够打破物理空间的束缚、同时在场参与，不必亲身前往办公室就能获得跟人们面对面交流时的感受和效率优势。</p><p></p><p>突然之间，这样的环境也具备了真正的市场吸引力。因为我们不仅能在其中一起讨论、一起做事，甚至还能拥有无限的数字景观。所以我从刚开始就对更广阔的应用前景抱有兴趣，毕竟纯粹作为游戏外设的 Rift 总是在激起一点热度后又快速被人们所遗忘。</p><p></p><p>BT：没错，而且它还只能连接 PC。</p><p></p><p>AB：这款设备价格是 Quest 3S 的两倍，另外还需要一台价值 2000 美元的 PC 跟它搭配，所以总体拥有成本实在是太高了。所以从现在我们选择的解决思路也能看出，独立、手势追踪和混合现实，更多强调自然和直观的使用感受，再就是继续攻克设备外形和分辨率等方面的问题。总而言之，我觉得我们真的开始走上了七、八年前所设想的道路。</p><p></p><p>BT：你和 Matthew 谈到的分歧之一，是想要制造最好设备的人跟其他人之间无法达成统一。我觉得约翰·卡马克（Oculus 首席技术官）对于这样的权衡相当直言不讳，你也曾经在采访当中提到，他们想要制造更便宜的设备。两年前我试用过 Quest Pro，如今又在这里体验了 3S。3S 挺不错，但也要花上 299 美元。那可以说最终是约翰·卡马克获胜了吗？**</p><p></p><p>AB：首先，我总是喜欢给卡马克点赞。他最爱的就是在推特上给我发各种批评意见，我很喜欢，约翰请继续。</p><p></p><p>我的想法是这样，其实两种情况都有。比如说，卡马克本人是 Oculus GO 的忠实拥护者，而 Oculus Go 是一款三自由度（3DoF）头显。因为它不支持六自由度，所以就相当于把用户的脑袋锁进一个盒子里。它的实际使用感受还行，但是由于没有很好的控制方案，所以始终满足不了预期。我一直很喜欢这个主意，尽管它最终没能造成多大影响，但如果真的能在三自由度之内做好设计，那么现在我们讨论的将会是售价 100 美元、甚至是 50 美元的头显。可惜我们做不到。</p><p></p><p>所以我觉得对于这类产品应该有个最低标准，就是“至少要达到这样，才有必要尝试”的边界。我认为我们在 Quest 2 中找到了这个点，还有与之对应的功能集。</p><p></p><p>对了，卡马克对于手部追踪有点怀疑。他说“现在的控制器已经足够好了”；他对于混合现实也有疑虑，会觉得“那是额外的东西，有什么必要呢？”我真的很感谢他的判断，因为他把那些东西都去掉了。这一切都让最终设备便宜一点、轻一点、反应更快一点，这些都是很重要的指标。</p><p></p><p>与此同时，我还一直在考虑怎么让手头不宽绰的用户用上这款头显。事实上，我们从研究和应用中得知，混合现实和手部追踪等功能确实能让头显使用起来更舒适。他们会感觉更处在、体验更自然，也更有一切尽皆内化的感觉。</p><p></p><p>还记得刚开始把 Quest 2 交给从未使用过的人时，首先得教他们如何使用控制器来完成某些操作，整个过程相当复杂而且难以沟通。但现在大家已经适应了，意识到“只要抓住这些小东西，就能移动它”。</p><p></p><p>所以我真心觉得，身为技术人员，我们有时候会低估设计成果在人们眼中的上手难度。成就一款优秀产品的不仅仅是价格和舒适度，保证其易于理解和使用也同样非常重要。</p><p></p><p>BT：那在你看来，Quest Pro 是个错误吗？</p><p></p><p>AB：这话说起来就长了。扎克伯格和我一直在争论这个问题。如果没有 Quest Pro，我们很可能就搞不出 Quest 3。</p><p></p><p>Quest Pro 是第一款率先使用饼干镜头的设备，还使用到了眼动追踪和面部追踪设计。它开创了所有这些功能，相当于是给未来开启了可能性。硬件领域有这样一种说法，第三代产品才是我们理想当中第一代产品的样子。但没办法，这里没有捷径可走、也不可能跳过难题。必须得先有第一代，才有后面的第三代。我真心认同这个理论，即硬件开发没有捷径可走。</p><p></p><p>这里我也稍微给自己辩解几句，在设计 Quest Pro 的时候，我们经过了充分讨论，最终决定投放巨额资金开发这些全新镜头，并且要为这些成本极高的镜头建造新工厂。在这样的情况下，哪怕 Quest Pro 可能没有达到我们理想中的销售预期，但市场表现仍然算不错。我们还卖出了所有镜头，这同样很了不起。只能说 Pro 本身没能轰动一时，但它最终成为我们推出 3S 的关键前提。所以扎克伯格一直觉得这是个非常成功的项目，而我自己希望它能再卖得多一点。</p><p></p><p></p><h3>“如果 Vision Pro 卖得好，我们也会改变策略”</h3><p></p><p></p><p>BT：你之所以重新关注低端产品，是不是因为 Vision Pro 的定位实在太过高端了？</p><p></p><p>AB：我觉得自己是无论如何都会着眼于低端产品。我的意思是，首先我很喜欢 Vision Pro——很多朋友可能不信，其实我欣赏他们走极端的做法。那种感觉就像是“如果我们把这项指标拉满，但让系统的其余部分保持原样，结果会如何？”我们确实没这么做，而苹果在 Vision Pro 的重量和成本方面是拉满了的。这对我们也有很好的参考价值。</p><p></p><p>BT：就是因为太极端了，所以我觉得你和扎克伯格在看到 Vision Pro 之后似乎有种如释重负的感觉。</p><p></p><p>AB：当竞争对手发布产品时，我们唯一需要担心的就是对方取得了自己没有实现的突破。他们可能搞清楚了某些我们还没搞清的技术，这样他们就能够在一段时间之内保持优势，直到我们迎头赶上、将其击败。</p><p></p><p>所以我觉得对于任何一款设备问世时，人们都会有这样的反应，类似“太好了，还是用我们知道的材料制成的，用的也都是我们接触过的技术。”</p><p></p><p>BT：“我们能理解它为什么这么贵，也能理解它为什么这么重。”</p><p></p><p>AB：差不多，就是我们也能造出同样的东西，只是我们选择不这么造。人们倾向于探索不同的空间，这对整个世界来说是件好事。顺带一提，如果 Vision Pro 卖得很好，我们当然也会改变策略。我们会说，“好吧，原来这部分市场比我们想象中更大，那我们也试试看。”顺便说一句，我觉得只要有软件作为加持，就会有相应的市场空间。</p><p></p><p>BT：苹果在 Vision Pro 上发布的内容相当有限，你对此有感到意外吗？</p><p></p><p>AB：问题的关键，在于获取内容的方式。毕竟要想让设备卖得好，就得有更多软件；但如果设备卖得不好，谁又愿意多开发软件呢？这是个先有鸡还是先有蛋的问题。对于保有量不大的产品，开发商会觉得“虽然设备已经面世，但对我来说普及度还不够，不值得为它构建内容。”</p><p></p><p>BT：那么低成本明显就是要解决这个问题，先用友好的价格打开市场，然后才可能吸引到更多人为它开发软件。**</p><p></p><p>AB：百分之百是这样。我们一直在讨论这个问题，特别是如何建立 Quest 生态系统。我关注的不是 Quest 系列设备，而是如何尽可能多地为开发者们扩大目标受众，这样他们就能销售自己的软件、吸引到更多开发者加入进来，再由此培养出更多消费者，最终实现飞轮效应。等到规模达到一定程度，就能再向市场迈进下一步。到那个时候，我们才有可能打造利润空间更大、定位更高的设备，因为市面上已经有了很多可供其消费的内容。</p><p></p><p>BT：观察 Vision Pro 在过去 6 到 9 个月之间的发展变化，你的态度是否逐渐从“我们可以和谐共处”变成了“最终还是会由我们吃下整个市场”？</p><p></p><p>AB：哎呀，这个问题嘛，反正我们对自己的市场地位感到满意。</p><p></p><p>其实我这个人讲的永远是坦诚的观点和想法，只是有时候需要以一种巧妙的方式表达出来。我之所以要保持谨慎，唯一的原因就是我真的不想跟任何人为敌，包括苹果。</p><p></p><p>我认为他们愿意投资参与这个领域是件好事，也希望他们能继续参与进来。实际上，Vision Pro 已经帮助整个 VR/AR 设备领域的吸引到了投资热度，连我们也在一定程度上从中受益。过去几个月间，我接到了很多电话。如果苹果没有推出 Vison Pro，如果没有他们吸引人们对后续版本做出遐想，那我根本就不会接到这些电话。总之能有这样的竞争关系真的非常非常健康，对消费者们有好处、对我们也有好处。</p><p></p><p>同时我也坚信，如果你身为一名开发人员、却不选择优先为我们开发产品，那就太蠢了。我们这边的软件购买受众很大，大到足以维持你的业务。在完成这一步之后，不妨再考虑把它移植到苹果 Vision Pro 上。</p><p></p><p>BT：下面两种情况，你更害怕哪一种：在竞争中输给苹果，还是 VR/AR 设备永远没有足以容纳自己的市场？</p><p></p><p>AB：这个问题好。没错，我确实担心市场会有某种程度的上限，导致其无法真正起飞。至于苹果那边，我最担心的一点就是他们把手机和设备锁定得太狠了，所以容易陷入自我感动的设计流程当中。</p><p></p><p>就拿我们的 Orion 眼镜为例，这些属于纯 AR 眼镜设计，效果很棒。我们在眼镜中使用了定制芯片，在遥控器端也使用了定制芯片。而在苹果的产品中，一个重要前提就是只匹配自家产品、部分功能不对外开放。他们在 AirPods 上就做了这样的选择。</p><p></p><p>BT：他们之所以不做遥控装置，是因为他们已经有了 iPhone。</p><p></p><p>AB: 是的，他们想要维持手机的核心地位，AirPods 就是这样被绑定在了手机上。</p><p></p><p>BT：还有 Apple Watch。</p><p></p><p>AB：是的。这些并不是最好的产品，但苹果用种种方式阻止其他人参与到这些产品的制造中来。所以如果非要说我对苹果抱有什么顾虑，那么关键并不在于他们的头显做得好不好、强不强，而是他们总想以一种让他人难以竞争的方式将各方捆绑到自己的生态系统当中。</p><p></p><p>BT：下面让咱们来聊聊 Orion。我很想买一个，但却买不到。那为什么还要把它展示出来呢？**</p><p></p><p>AB：其实我对这个问题看得很开。我们之所以要展示这款产品，只有两个原因。首先，我们已经在这件事上投资了 10 年，也一直在呼吁和鼓励更多技术人员、投资者还有公众跟我们一同踏上这段旅程，希望他们相信 Meta。在过去三、四年间，我们也因为在这个领域投入了巨额资金而一直受到严格的财务审查。</p><p></p><p>BT：那你后悔公司从 Facebook 改名成 Meta 吗？</p><p></p><p>AB：不后悔，完全不后悔。我很喜欢 Meta 这个名字。对我们来说，最重要的开发出了这些 Meta 眼镜，它们能正常工作、效果惊人而且反应灵敏。我觉得这才是真正能打动人的证据，也是真正能够吸引到技术人员的原因。</p><p></p><p>它真正让人看到了希望。我也是在 Orion 身上第一次感觉到手机可能危险了。</p><p></p><p>BT：另外一个令人印象深刻的点在于，我一直感觉苹果 Vision Pro 的视野比 Quest 要大得多，虽然实际差距可能没那么夸张。</p><p></p><p>AB：实际上，苹果 Vision Pro 的视野更小。</p><p></p><p>我第一次尝试配套软件是在四、五个月之前，感受跟你很相似。开发团队也一直在道歉，比如说“我们知道这里的像素显示还有问题，我们需要在这里进行色彩校正”。但说实话，我的感受就是震撼，完全不在乎他们提的那些缺点。我一直体验到最终一刻，当时心里只有一个想法：“你们别再道歉了，这东西真是太不可思议了。你们成功了，恭喜！”</p><p></p><p>演示的内容也很有趣，展现的是一个有趣的小故事。人们不停地走进一个个房间，我也不知道他们在做什么，据说是在搬汽水。这么做是为了给设备冷却以防止过热，所以他们不停往冰球上放汽水。电池已经充满了电，可以使用两个小时，而过热前电量已经被耗尽，所以总时长大约是两到三个小时，因此把演示时长设定成了三小时。</p><p></p><p>总之，我们展示这些内容的第一个原因，就是希望向大家证明这点——“和我们一起投入，相信我们的决心。如果你是技术专家，也欢迎与我们携手同行。”第二点则是，对于开发者们来说，我们希望能点燃大家的热情 ，吸引更多人加入我们的虚拟化身平台。我们在台上也表达了这一点，“如果各位投资于我们的这次平台，那么不仅能够从 Quest 生态系统中获得红利，最终也将在 AR 生态系统中获得红利。”</p><p></p><p>BT：你之前提到过，苹果 Vision Pro 造成的最大担忧，就是苹果在技术层面已经取得了一定进展。这些进展可能被申请成专利或者受到保护，或者是其他什么制度性保障。那你有没有自信拿出一些不会被他人轻易模仿的成果？</p><p></p><p>AB：问得好，我们当然是有的。我们有两项值得骄傲的成果，比如我们的 MicroLED 就相当出色。我们不仅自己设计、自己制造，我们还由合作伙伴帮助分担一部分工作。我们是系统的构建者，所以我们肯定在某些方面掌握着领先的技术优势。</p><p></p><p>Orion 中的所有东西跟我们接下来要开发的方案并不接续。我们已经初步设计出后续几款全 AR 眼镜的原型，并且希望接下来的产品能够在开发阶段为消费者使用做好准备，特别是要在其中引入一些非常酷炫的变化。它们要更轻、更薄、价格大幅降低，做出一系列具体权衡。视野就是我们需要平衡的因素之一，其他还有亮度、成本和重量等等。总之对我们来说，目前我们在很多方面都具有技术优势。</p><p></p><p>我们采用的是混合方法。对于某些技术，我们完全保留第一方立场，也就是唯一能够实现的厂商。但对于其他很多技术，我们则与行业合作伙伴们携手，因为其应用不仅限于 AR，应当允许合作伙伴们在除 AR 之外的各类其他市场上自由推动技术商业化。除了与这些产品存在竞争关系的特定方之外，这对于吸引更大的行业投资也是一种福音。</p><p></p><p>BH：但这需要多长时间？一年内、两年内，五年内还是十年内？</p><p></p><p>AB：肯定是要以年计，但应该不至于以十年计。我们可能会在未来一、两年内研究这方面问题，并在软件开发中磨练自己的敏感度。之后，我认为应该把重点放在产品打磨上，努力为消费者们的广泛使用做好准备。</p><p></p><p>BH：也就是说，2027 年左右？</p><p></p><p>AB：我暂时还没办法确定，至少没法明确承诺。但我们肯定会在未来三到五年内逐渐将计划落地。</p><p></p><p></p><h3>“粉丝的很多抱怨都相当合理”</h3><p></p><p></p><p>BH：你在开发者主题演讲的开头，向开发者们道了歉。</p><p></p><p>AB：是的，这是给粉丝们的一点回应。不知道你逛不逛 Reddit 社区或者 Threads，每一天我和 Mark Rabkin 都会收到很多用户的消息，他们对为 Meta VR/AR 平台开发软件时遇到的挑战抱有种种不满。</p><p></p><p>其中很多抱怨都相当合理，所以我们希望能留住他们，真诚希望能帮助他们接触到目标受众。他们的很多想法非常有趣，提出的很多要求也确实具有挑战性，这也是我们去年最主要的关注点。但与此同时，“整理一下开发者文档”、“请确保建立一套好的端到端 Unity 和虚幻引擎体系”之类看似合理的建议又很难得到响应，毕竟我们清楚平台将很快发生变化，包括引入混合现实或者是手势跟踪之类，这一切都将彻底改变所有原语。</p><p></p><p>所以我之所以用道歉开头，就是想让观众们意识到你的声音我一直在倾听。我读过所有内容、关心大家的感受。只是我们现在的重点是打造一套出色的开发平台，一套令人愉快的开发平台，这其中涉及很多具体工作。我们还需要关注稳定性，以便大家能够在构建 API 的同时，不必担心这些 API 会被频繁弃用并影响到自己的应用开发成果。</p><p></p><p>BH：那到底是什么促使你们决定停止试验，真正开始构建产品？</p><p></p><p>AB：这个问题很重要。其实真正的拐点来自效率年之前。我觉得这种情况很常见，各种项目都会经历这样的扩张期，参与者们一时搞不清真正重要的是什么、不知道哪种技术是正确的选择，不知道该使用哪种操作系统，也不清楚做出正确权衡的合适理论依据。所以，如果想在特定时间范围之内提高取得成功的信心，那就得以并行方式朝着多个方向推进。</p><p></p><p>BH：那你并行推进了多长时间呢？</p><p></p><p>AB：我们一直到 Quest 2 的时候才迎来了转机，特别是在看到了混合现实的时候。真正的构建过程由此开始，如今我们的元宇宙部门已经将混合现实推向了高度集中的发展阶段，对于什么是“好”、什么是“对”有着非常清晰的愿景，能够坚定不移地朝着正确的方向迈进。基于这样强有力的路线判断，我们才能非常高效地配置资源、制定并行路径清单，进而加快重要工作的处理速度。</p><p></p><p>一年之前，Orion 凭借出色的增强现实体验让我们达到了这样的阶段。我们也终于有信心说，“好吧，我们想对了、也做好了，终于搞清楚接下来该往哪里去。”其间雷朋 Meta 眼镜也帮上了大忙。它很酷，而且除此之外，我们在此之前推出的一系列设备也都起到了重要的探索性作用。</p><p></p><p></p><h3>“AI 最能发挥作用的方向是 Horizon Worlds”</h3><p></p><p></p><p>BH：那 AI 有没有帮上 Reality Labs 的大忙？</p><p></p><p>AB：哎呀，终于说起 AI 了。我们的 FAIR，也就是基础人工智能研究小组，直到今年才开始向我汇报。我们刚刚把该小组转移到 Chris 领导的其他 AI 部门。</p><p></p><p>我不确定能不能算帮上大忙，但这一波确实走得很顺，也是我记忆当中整个项目首次赶上了顺风。回想起之前的开发经历，主要都是痛苦的回忆，一个逆风接着一个逆风。比如说“知道吗，这东西的热性能比你想象中要差，电池续航比你想象中要差，执行效率比你想象中要差”，而现在终于有东西比想象中效果好了。这个比预期表现更好、出现更早的成果，就是 AI。</p><p></p><p>这些设备都经历了相应的扩张期和收缩期。在前一阶段它会不断扩张，旨在让我们体会什么是好的，建立起相应的理解和直觉。接下来的工作则是做减法，这个过程中我们也越来越善于舍弃不必要的部分。</p><p></p><p>如今我们的架构非常紧密，手部跟踪、眼部跟踪、面产串上、Codec Avatars 等等，都是能在 VR 和 AR 领域同时发挥作用的技术。我们有一支共同的团队来构建这些技术。另外，AR 操作系统必须是独立且专用的，因为其在用例、实际操作和交互范式方面与过往的操作系统都完全不同。</p><p></p><p>BH：所以根据我的理解，二者的分叉也变得更加清晰。</p><p></p><p>AB：确实是这样。</p><p></p><p>BH：Facebook 起步于中间区域，也就是社交 / 公共区域的逐渐消失。而现在又出现了新的分叉，VR/AR 体现的正是这个分叉点。**</p><p></p><p>AB：是的，这很有趣，我从来没从这个角度考虑过内容的问题。我完全同意你的观点，AI 最能发挥作用的方向就是 Horizon Worlds。我希望每个人都能创造一个世界，但长久以来 3D 设计的上手难度都太大了。</p><p></p><p>BH：是的，所以游戏在这方面遇到了瓶颈。</p><p></p><p>AB：无论把门槛放得多低，这都是有门槛、有难度的，除非你能单凭语言描述就创造出一个新世界。但如今，我们已经看到了希望。我们在演讲中谈到了 NPC——NPC 就是这样一个复杂的体系，如果没有它，游戏就很难进行下去。现在我们可以用 AI 来生成 NPC。同样的，我们也可以在 AI 技术在 AR 当中构建传感结果。</p><p></p><p>所以我想分享另一个我们内部尝试过的演示，那就是 Orion 风格的超传感式眼镜。它压根就没有显示屏，只提供始终开启的传感器。人们可以在它的帮助下回顾自己一整天的经历，比如查询这一天干了些什么。用户可以说，“嘿，在今天我们的设计会议上，我们为沙发选择了什么颜色？”它会给出正确答案。再就是“今天下班的时候我，在墙上看到了一张海报，具体是什么内容？”或者“对了，这周末下午 4 点要组织一场家庭烧烤，具体要邀请谁？”总之，一整天的经历都变得可以查询了。</p><p></p><p>这肯定算不上是很大的飞跃，我们还没探索太多，但这至少能够实现很多代理功能。比如它会提醒用户“你要开车回家了吗，别忘了顺便去趟杂货店，你说过需要买奶油。”就是这样简简单单的提醒功能，足以让我们的日常发生改变。我们可以用这种 VR 和混合现实方式作为 AI 的输出空间，再利用 AI 输入建立起相应的 AR 空间。</p><p></p><p>AI 方案能够持续感知，并成为我们日常工作和生活中的惊人驱力。我们对此深感兴奋。</p><p></p><p>BH：雷朋眼镜在扩大受众规模、营造应用氛围方面到底有多重要？</p><p></p><p>AB：可以说非常重要。我认为如果想要通过某种方式让自己跟领域内的其他 AR 厂商有所区别，那我们首先得想办法让 Orion 变得易于穿戴、降低负担。</p><p></p><p>BH：听说是 EssilorLuxottica 找到了你们，而不是你们主动去找他们，这是真的吗？**</p><p></p><p>AB：是的，EssilorLuxottica 那边的首席可穿戴设备官 Rocco Basilico 几年前给我们发过邮件，而且是冷不丁突然给扎克伯格去了邮件，说“我们应该合作”。实际上，当时我正在跟 Hugo Barra 搭档，而 Hugo 表示“我觉得这事靠谱”。我猜扎克伯格把邮件转发给了高管团队，问有没有人跟进一下，而 Hugo 表示“必须抓住机会”。</p><p></p><p>BH：就是说扎克伯格得亲自上阵。</p><p></p><p>AB：于是扎克伯格飞到意大利，与时任 Luxotiica 董事长、创始兼董事长，已故的 Leonardo Del Vecchio 建立了牢固的关系。</p><p></p><p>BH：所以说扎克伯格的思路从这时候开始真正转变，Rocco 认为外观非常重要，他也认同了这一点。</p><p></p><p>AB：说句实话，扎克伯格的厉害之处就在于，他一直都知道外观很重要。从我接手当时被称为 AR/VR 部门（也就是现在的 Reality Labs）那一刻起，他就非常明白。“如果产品看起来不好看，人们就不愿意戴，那其他东西做得再好也没意义了”，而尺寸就是其中最核心的挑战。把设备做成两倍大小，能够让研发难度降低四分之三，但他不允许我们那么干。哪怕目前的 Orion 只有区区 98 克重，我们也在考虑如何在下一个版本中让它变得更轻、更薄、更小。我们仍然在不断探索新的极限。</p><p></p><p>BH：再就是探索怎么把这东西卖到 1000 美元。</p><p></p><p>AB：我们已经有了大致的思路。同样的，其中也要做出各种权衡，而且是真正的探索和妥协。好在面前的道路已经越来越清晰，这是在实践当中摸爬滚打出来的，单凭思想实验永远不可能达成。</p><p></p><p>BH：回到 AI 上，你强调的就是高度集成，也就是关键在于把硬件跟 AI 集成起来。</p><p></p><p>AB：是这样的。</p><p></p><p>BH：那又该怎么理解“我们保持开放”呢，只是个口号吗？</p><p></p><p>AB：对我们来说，最需要的就是围绕开放建立起一个生态系统，这也是当前真正缺失的部分。但我们过往的经验带来了很多指导，包括开放计算项目还有“推动补充要素的商业化”。对我们来说，AI 确实让我们的产品更完善了。</p><p></p><p>BH：那对 Reality Labs 来说，这种补充要素是什么呢？</p><p></p><p>AB：AI 让我们的产品变得更好，但没有其他人能够提供来自 Facebook 的 News Feed，只有我们自己可以做到。所以说这种补充要素就是 AI，而且无论是谁开发出的 AI，都能为我们的产品所用。正如头显之于 Horizon Worlds，AI 也能让我们的产品、包括其他人的产品都更上一层楼。</p><p></p><p>BH：所以核心产品就是 AI。</p><p></p><p>AB：我觉得这是双向的。在 AR 方面，我认为 AI 确实就是最核心的技术。至于在混合现实和虚拟现实这边，AI 更多扮演的是启动器的角色。</p><p></p><p>BH：那么，如今的 Meta 到底是一家内容公司还是社交网络公司？</p><p></p><p>AB：我们从来不把自己看作是纯粹的社交网络公司，我们是一家科技公司，也一直在努力强调这样的定位。人们总想把我们框定起来、限制起来，这也是很多人误解了我们在 Reality Labs 中工作内容的原因之一。实际上，我们一直都是一家科技公司。</p><p></p><p>早期我们做 HPHP 的时候，我们做 Hadoop 项目的时候，做 Cassandra 开发的时候，所有工作成果都是开源的。为什么要选择开源呢？因为我们的目标就是围绕这些工具建立起社区，这个社区能够达成单凭我们自己根本无法完成的目标。</p><p></p><p>BH：Reality Labs 又为什么要把自己的产品开放出去呢？</p><p></p><p>AB：抱歉，我以为我们还在讨论 Llama。是的，从开发者的角度来看，Horizon OS 最大的开启生转变就是我们曾经构建过一个精心设计的商店，但大家真的很不喜欢。他们更希望有一个开放的应用商店，任何人都可以把任何 APK 添加进去，由消费者自主选择。所以我们在去年做出了相应改变。总之，是的，我知道“开放”这个词在科技行业中有着非常具体的定义，每个人都想从蹭一蹭热度。也总会有 Richard Stallman 这样近乎狂热的原教旨主义者……</p><p></p><p>BH：我记得 Matt Mullenweg 最近还批评了你对“开放”一词的理解。</p><p></p><p>AB：我也看到了。我还是觉得开放有着宽泛的指代范围，而且具体定义永远是相对的。只有相对开放和相对封闭。我想说的是，我们希望自己的产品能够在相对开放这一侧。</p><p></p><p>BH：戴上 Orion 之后，我真正体会到了你们工作的未来愿景。但你们还没有真正解决制造和交付流程中的很多问题，所以暂时还不能打 100 分。</p><p></p><p>AB：老实说，我非常欣慰。你肯定无法想象，过去几年间我们在财务审查中承受的巨大压力。直到大约一年之前，我们还不知道自己到底能不能造出理想中的成果，一切还在未定之天。到几个月前，我们才真正体验过这款软件，我到现在也刻那个激动人心的时刻。让我激动的不仅是身为一名技术人员的参与感，更是一个关心这些产品的普通人对于所见、所感的惊喜。再想想这是整整十年的开发历程，以及成千上万人呕心沥血的结晶，着实让人感慨。</p><p></p><p>BH：事实证明，这 750 亿美元终究还是花得物有所值了。</p><p></p><p>AB：哈哈，这么多钱可不单是用来开发 Orion 的。我们投资了很多非常棒的项目，相信这些投资终将获得回报。</p><p></p><p>原文链接：</p><p></p><p><a href="https://stratechery.com/2024/an-interview-with-meta-cto-andrew-bosworth-about-orion-and-reality-labs/">https://stratechery.com/2024/an-interview-with-meta-cto-andrew-bosworth-about-orion-and-reality-labs/</a>"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rohj6bV3voUMCIwxHaed</id>
            <title>重庆 AI 独角兽赴港 IPO，三年半亏 71 亿、估值却暴增百倍，中国AIoT第一股有多强？</title>
            <link>https://www.infoq.cn/article/rohj6bV3voUMCIwxHaed</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rohj6bV3voUMCIwxHaed</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 02:17:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫</p><p></p><p>9 月 26 日，重庆特斯联智慧科技股份有限公司（下称“特斯联”）向港交所提交上市申请，由中信证券和海通国际担任联席保荐机构。特斯联方面透露称，本次融资将主要用于增强研发能力、大模型开发、商业化及城市拓展和潜在的战略收购机会等。</p><p></p><p>据其招股书，特斯联主要通过 AIoT 操作系统 TacOS，向企业、公共管理者及其他公域空间参与者提供全栈 AIoT 产品（包括软件、硬件及服务）。AIoT 指系统通过信息传感器实时采集各类信息，在终端设备、云端等通过机器学习对数据进行智能化分析，包括定位、比对、预测、调度的技术。</p><p></p><p>若此次成功赴港上市，特斯联将成为中国 AIoT 第一股。</p><p></p><p></p><h1>背靠“光大系”，7 年估值暴增百倍</h1><p></p><p></p><p>自 2015 年成立以来，特斯联一直备受资本追捧。IPO 前，特斯联完成了从天使轮到 D++ 轮共计九轮融资，融资总额超 49 亿元，中国光大控股、京东科技、商汤集团、科大讯飞、IDG 资本等一众资本扎堆入股，另有珠海、南昌、徐州等多地国资押注。</p><p></p><p>今年 8 月底，特斯联获港股上市企业美高域投资。根据美高域的公告，本次投资金额为 5000 万元，占总股本的比例为 0.24%，特按 20 元 / 股的融资价格计，特斯联的投后估值高达到 212.26 亿元，较 2017 年完成天使轮融资时的估值暴增了近 303 倍。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ff/ffa6b0b279e5f35872388aa1d711c263.png" /></p><p></p><p>图源：特斯联招股书</p><p></p><p>值得一提的是，光大控股及其关联方多次参与投资特斯联，迄今持股 26.37%，是特斯联的最大机构股东；按最新投后估值 212.26 亿元计，光大控股对特斯联的投资收益率已高达 146.58%。光大控股还通过关联方向特斯联提供了约 3.6 亿元的贷款，贷款利率在 7%-8% 之间。</p><p></p><p>公开资料显示，特斯联董事长王鸥目前仍兼任光大控股管理决策委员会成员及高级海外投资总监，曾任证监会机构监管部副主任、创新业务监管部副主任等多个职位，具有监管背景。王鸥于 2022 年 10 月加入特斯联并获委任为董事，主要负责公司整体战略规划、企业管治及业务方向提供指引。</p><p></p><p>特斯联的创始人艾渝也曾是光大控股的高管，并且一干就是近 12 年，到 2020 年 5 月艾渝离职时的职位为光大控股董事总经理，主要负责一级市场的私募股权投资。在职期间，艾渝曾作为核心创始人创立中国最大地产基金光大安石，后创立光际资本、光控众盈资本等任管理合伙人，主导人民币及美元基金的累计规模逾 500 亿元人民币，投资过网易云音乐、爱奇艺、美团点评、寒武纪、商汤科技、第四范式、蔚来、小鹏汽车、京东物流、银联商务、美团点评等知名科技公司。</p><p></p><p>可以说，“光大系”从特斯联创立至今，始终对其存在重要助力。</p><p></p><p>此前，艾渝曾称：“特斯联要做中国第一个大规模盈利的 AI 公司”。然而，尽管该公司的估值一路高歌，但目前特斯联仍尚未盈利，且财务状况似乎不太理想。</p><p></p><p></p><h1>三年半亏 71 亿，负债超公司资产三倍</h1><p></p><p></p><p>近三年以来，特斯联一直处于亏损状态。据招股书显示，2021-2023 年及截至 2024 年 6 月 30 日止 6 个月，特斯联收入分别为 12.07 亿元、7.38 亿元、10.06 亿元及 3.57 亿元；同期净亏损分别为 28.28 亿元、23.87 亿元、8.03 亿元及 11.28 亿元。</p><p></p><p>今年上半年，特斯联营收同比下降 30.11% 至 3.57 亿元。截至上半年末，账上有高达 12.36 亿元的应收账款，约为同期营收的 3.5 倍，但环比 2023 年末仅减少 1.75%。与此同时，AI 产业数智化板块的客户从上年同期的 148 家跌到 90 家，少了 58 家，公司总客户数从 2023 年上半年的 186 家降到 2024 年上半年的 150 家。</p><p></p><p>对于持续大额亏损，特斯联在招股书中解释称，主要是由于附有优先权股份的公允价值亏损、股份支付开支、研发开支等，对净利润影响较大。</p><p></p><p>2021-2023 年及截至 2024 年 6 月 30 日止 6 个月，特斯联研发费用分别为 2.87 亿元、3.29 亿元、3.22 亿元及 1.45 亿元，分别占同期收入的 23.8%、44.6%、32.0% 及 40.7%；截至 2024 年 6 月 30 日，特斯联共有 363 名研发人员，占员工总数的比例达到 52.2%。</p><p></p><p>据艾渝此前透露，2021 年特斯联曾在全球范围内寻找人才，一度罗列了 100 位 AIoT 领域顶级科学家的名单。最终，有 6 位科学家愿意加入公司，而公司选择了三位 50 岁以下的 IEEE Fellow 级别科学家。</p><p></p><p>如今，特斯联由三位 IEEE Fellow（国际电气与电子工程师协会的会士）级别科学家领衔，包括 CTO 华先胜、首席科学家邵岭及首席科学家杨旸，他们三位均入选斯坦福大学发布的全球前 2% 顶尖科学家榜单的终身科学影响力排行榜和年度科学影响力排行榜双榜，并且这已经是自该榜单 2019 年发布首版以来，这三位科学家连续入选的第四年。</p><p></p><p>此外，特斯联销售及营销开支分别为 2.48 亿元、1.90 亿元、1.33 亿元和 0.82 亿元，收入占比分别达 20.5%、25.8%、13.2% 及 22.9%。在高额的费用支出下，三年来特斯联的综合毛利率呈下降趋势。2021 年至今年上半年，公司综合毛利率分别为 44.16%、10.10%、31.03%、24.73%。</p><p></p><p>而公司近三年来的收入，主要来自在 AI 产业数智化、AI 城市智能化、AI 智慧生活及 AI 智慧能源四个板块。其中，AI 产业数智化和 AI 城市智能化业务对特斯联的收入贡献度超过七成。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3e/3e120d2d3d38637e28b84fdece56b237.png" /></p><p></p><p>图源：特斯联招股书</p><p></p><p>除收支长期失衡的财务压力外，特斯联的现金储备也较为不足。招股书显示，截至今年 6 月底，特斯联资产负债率为 315.38%。期末，公司货币资金 2.55 亿元，短期借款 15.99 亿元、长期借款 4.87 亿元，今年上半年的财务费用 0.36 亿元。截至 2024 年 7 月 31 日，特斯联银行结余及现金仅为 7390 万元，流动负债总额达到 110 亿元，流动负债净额达到 87.3 亿元。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www1.hkexnews.hk/app/sehk/2024/106807/documents/sehk24092600045_c.pdf">https://www1.hkexnews.hk/app/sehk/2024/106807/documents/sehk24092600045_c.pdf</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qavxwxZ8dl090Eoqixqi</id>
            <title>AI 整顿职场，比 00 后都狠？先对过时的管理者开刀，招人标准大变，人性化和自组织才是归宿</title>
            <link>https://www.infoq.cn/article/qavxwxZ8dl090Eoqixqi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qavxwxZ8dl090Eoqixqi</guid>
            <pubDate></pubDate>
            <updated>Tue, 08 Oct 2024 01:56:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>采访嘉宾 | 李云，致效企业管理咨询创始人</p><p>作者 | 华卫</p><p></p><p>在如今的市场环境下，对企业来说，增效是王道！而 AI 正在迅速向着翻倍提升生产力的工具应用方向迭代与变革，文字创作、代码编辑、视频生成… 无一不是其正在攻略的职场赛道。在这样的 AI 时代下，技术工程师们该如何适应？技术团队的工作方法和管理方式又因此正发生哪些变化？适合 AI 时代的团队文化应该是什么样的？</p><p></p><p>为此，InfoQ 对致效企业管理咨询创始人李云进行了专访，听他聊一聊 AI 时代对技术人的技能需求、技术团队中的成功 AI 实践案例和未来管理模式。在即将召开的<a href="https://qcon.infoq.cn/2024/shanghai/"> QCon 上海 2024_ 全球软件开发大会暨智能软件开发生态展_InfoQ 技术大会</a>"上，InfoQ 也邀请到了李云老师来做演讲分享，他将从工程师个人出发，再到团队管理的视角展开，进一步分享个人与团队、人与工作环境、业务与技术整合的体系化技术管理落地之路。</p><p></p><p>以下为访谈实录，经编辑。</p><p></p><p></p><h1>AI 对技术团队的影响</h1><p></p><p></p><p>InfoQ：在 AI 的快速发展下，技术团队的日常工作被改变了吗？包括工作流程和项目管理等这些方面。有哪些成功的实践案例可以分享？受到了哪些影响？</p><p></p><p>李云： 对技术团队来说，如果工作方向不是 AI，我认为日常工作并不会变，但工作的方法应当有所改变才好。对于软件开发工程师，我认为请 AI 做自己的工作伙伴的行动是要有的，换句话说，在工作过程中用 AI 来帮助自己提升工作质量和效率。</p><p></p><p>以我自己的工作体会，在解决编程问题方面使用 AI 确实有很好的成效，以前需要通过搜索引索去找类似的问题，通过阅读和消化后再来解决问题，现在有了 AI 后直接就能得到代码级的解决方案，这个过程真的非常美妙。还有写代码的过程中，AI 能猜出一些我想写的代码，直接按 Tab 键，一段代码就上去了，根本不像以前那样大部分要自己用手敲出来，编码的效率有了大幅度的提高。</p><p></p><p>在工作流程上，我觉得质量保证方面的工作 AI 能起到很大的作用，如代码审查、重构、单元测试。我自己的实践是，AI 都能很好地发挥作用，就像有一个编程的导师在身边，以前身边如果没有一个好导师的话，个人的成长会慢得多，现在有了 AI 后，就变成了“一人行必有我师”。项目管理这块，我个人受 AI 的影响似乎不大，所以谈不上体会。</p><p></p><p>总之，我认为新技术的出现，总可以尝试着去收获一些积极的影响，通过躬身入局，让自己成为趋势的一部分，而不是成为游离在之外的旁观者。否则哪天被新技术颠覆时，自己的职场生存压力就会特别大。</p><p></p><p>InfoQ：在 AI 时代下，研发工程师们如何适应这些变化？如何进行个人定位？新的技能需求是什么？</p><p></p><p>李云： 心态上要对 AI 技术保持好奇，以及通过实践让自己有体感，去探索新的可能。姿态上注意与 AI 的平视，不俯视也不仰视。我用 AI 的一大体会是，AI 是遇强则强、遇弱则弱，当我能提出更有质量、更有深度和格局的问题时，通常从 AI 那也会有更意外的收获。</p><p></p><p>定位还是做自己有兴趣和擅长的事，只不过用 AI 来加持。对于大部分不是从事 AI 创业的人来说，摇摆的定位对自己的职业发展并不合适。当然，如果你对 AI 有热情，有想法，扎进 AI 去也是可以有的尝试，如果你还年轻的话，那我就更鼓励了，因为你没什么可输的。</p><p></p><p>在技能上，社会上广为传说的是合适的 prompt 很重要，这与我们编程时想办法解 bug 类似，与写文章时如何构思表达清楚也类似，可以发挥自己的一些创意点。另外，我认为对技能的综合性和广度要求会更高，这样 AI 更能给到我们启发。我自己在写书、编程时都有过这样的体会，当然有时也会觉得 AI 就是在胡说八道，这就需要我们有辨别的能力，背后还是依赖咱个人扎实的知识积累。</p><p></p><p>InfoQ：AI 时代对技术人才的要求日益多样化，哪些基本素质和能力是不可或缺的？团队在选拔新成员时，如何平衡专业技能与综合素质？</p><p></p><p>李云： 我认为对人才要求的多样化可能会体现于应对不确性定问题的能力，就是在有些事没做过，也不知道能不能做好的情形下，勇敢地借助 AI 这一现代化的工具去尝试。这种面对不确定性的素质更多是一种心理资本，需要个人有应对不确性问题的成功经历。另外，越是在 AI 时代，我认为基础原理性的知识更要掌握得扎实，这是与 AI 共同深度探讨话题的前提。</p><p></p><p>新时代团队在选拔新成员时，我认为贴合业务发展的需要仍是第一位的，当然发展可以是面向眼前的，也可以面向未来的。这个思路我想在任何时候都不会过时，因为虽说如今有很多 AI 焦虑，但聚焦做好手上的工作才能安放好个人与团队的焦虑，只不过多了如何用好新技术的思考维度。至于专业技能与综合素质的平衡，我想这与 AI 没关系，这里的平衡应当是说除了专业技能还得花精力去发展其他的技术，如自我管理、知识管理和业务技能，相比之下，后三者普遍容易被忽视。</p><p></p><p>InfoQ：AI 技术的引入如何促使技术团队调整其技术栈和使用新的工具？这些变化对团队管理带来了哪些挑战和机遇？</p><p></p><p>李云： 在我看来，给到工程师空间让他们去探索是最重要的一步。其实工程师群体很喜欢折腾新技术，只是在给到他们空间的情形下，还得以用新技术助力业务发展和改善团队效能这些目标去牵住他们，避免整天技术来技术去的但不创造价值、不接地气。</p><p></p><p>AI 技术对团队管理的挑战首先是管理者本身，他如何看待新技术对于团队业务和团队效能的潜在影响，以积极还是消极的心态去面对，而心态是会直接感染团队成员的。接下来最大的挑战是大家对新技术的担心，担心自己被新技术取代或淘汰，从而引发更大的焦虑，进一步带来更高的管理成本。我认为，AI 的机遇会落实在团队效能提高或业务成果放大上。</p><p></p><p></p><h1>技术团队管理的变化</h1><p></p><p></p><p>InfoQ：在 AI 时代，技术团队的管理理念发生了哪些变化？如何平衡自动化与人性化管理？</p><p></p><p>李云： 现在的职场还是蛮卷的，最近央视的热播剧《凡人歌》中也有这方面的桥段，想必能引起很多人的共鸣。另外，团体管理给管理者的感觉可能是紧张、被动、盲从和无力更多，少了松弛感。在 AI 时代之前就是这样了，如今也没有发生什么大的变化。话说，纵观行业甚至是整个中国社会，技术团队管理这块也没什么好的方法论。不过，我认为随着 00 后的登场，是需要改变的时候了。</p><p></p><p>00 后整顿职场的现象是很多职场人士喜闻乐见的事，背后反映的是 00 后对平衡工作与生活的渴望、对平等与尊重的需求、对个性化和自我表达的需要，以及对生命意义和价值的追求等使然。总之，需要企业经营者和管理者将人当作目的，而非工具。00 后整顿职场可能说得有点夸张，但确实是这个时代真实发生的现象，需要引起我们的重视和行动。</p><p></p><p>管理理念上，我认为人性化和自组织是必然归宿，这两大理念不只有助于激发个体的潜能，还能极大地降低管理成本，避免管理者成为整个团队的最大瓶颈。</p><p></p><p>自动化是为了让机器去做那些无聊和低价值的事，让人做更有价值的事，从而体现人性化。人工智能的出现，会让自动化这一趋势更加明显，要讨论的可能不是自动化与人性化的平衡问题，而是人性化如何去适应更加深刻的自动化问题。在新技术浪潮的面前，人性化的具体细节可能会有所不同。</p><p></p><p>InfoQ：哪些管理原则在 AI 时代依然适用？传统管理智慧在 AI 时代的作用是什么？</p><p></p><p>李云： 管理原则是在自组织管理理念下自然会有的产物，目的是让团队中的每个人能基于公开的管理原则去行事，消除过多的请示、担心行事方式与他人的不一致等不利于发挥个体主动性的因素。注意管理原则我用了“公开的”这个形容词。换句话说，管理原则不只是管理者自己用的，应是整个团队成员都用的。在我看来管理原则面向的是人，与 AI 技术的出现没太大的关系，至少我目前没有观察到这方面的影响。至于传统管理智慧，我认为只要与人性化和自组织管理理念不相矛盾就仍能发挥它的作用，否则就得做出相应的改变。</p><p></p><p>InfoQ：是否需要调整技术团队的组织架构来适应 AI 技术？</p><p></p><p>李云： 组织架构更多是从与业务和流程的适配去设计的，背后的逻辑是流程跟着业务走，组织跟着流程走。如果 AI 技术的出现并没有带来业务和流程的改变，那就没有调整组织架构的必要。否则，确实需要做出改变去适应 AI 这一新技术的到来。</p><p></p><p>InfoQ：在数据驱动的 AI 时代，技术团队的决策制定过程发生了哪些变化？这对技术 leader 提出了哪些新要求？</p><p></p><p>李云： 在我看来，变化在于多了 AI 从理性层面给我们提供多一个视角的决策建议，最终一定还得人来做决策。对于技术 leader 来说，有向 AI 求助的意识很重要，而如何用好 AI 可能还依赖个人在团队管理方面的一些深度思考与能力，基础性的东西我认为是不会变的。</p><p></p><p>InfoQ：适合 AI 时代的团队文化应该是什么样的？如何在高度自动化的环境中保持团队凝聚力？</p><p></p><p>李云：AI 的出现我个人觉得世界变得更乌卡（VUCA）了，特别是现在“子弹还在飞”的时期，团队文化在这样的背景下能给人带去温度才好，背后还是人文的内容，视人为人的事。团队凝聚力来自共同的愿景、目标、文化和持续成长，当有这些内容时，哪怕是高度自动化的环境也是有凝聚力的。如果没有凝聚力，很可能与自动化这事没有太大的关系。</p><p></p><p>InfoQ：职场人际在 AI 时代还重要吗？随着远程工作和自动化工具的普及，团队沟通与协作的方式有何变化？</p><p></p><p>李云： 无论是职场人际还是生活人际，我想很重要的一点是，人作为社会性动物通常需要人际。不过在职场环境中，人际是为了更好地帮助自己完成任务，在团队协作依然特别重要的今天，必要的职场人际还是要有，也是重要的，这与 AI 关系不那么大。除非 AI 的出现确实将人与人的协作完全变成了人与 AI 的协作，那时职场人际也许就没那么重要了。但始终不要忘记，人是社会性动物，通常不会期待在职场中与他人不发生任何的人际交往，否则对职场的感受是冰冷的。</p><p></p><p>远程工作的普及，使得数字化沟通渠道变得更加普及，如即时通讯工具、项目管理软件、在线文档等，也使得会议形式发生了很大的变化，如视频会议、虚拟白板等。从软件开发层面，远程工作使得工作流程的自动化、代码管理和持续集成的运用更加普及，而且很多企业会选择 SaaS 软件。在这些变化的背景下，团队的沟通变得更加数字化了，协作方式更多依赖于工具，面对面的交流变得更少。</p><p></p><p>InfoQ：在 AI 技术日新月异的背景下，技术团队管理的核心价值与愿景有哪些是不变的？如何确保团队始终围绕这些核心价值与愿景前进？</p><p></p><p>李云： 在我看来，尽管技术本身和工作方式可能不断变化，但技术团队管理的核心价值与愿景大部分是恒定不变的。这些核心价值不仅为团队提供方向，还确保了即使在快速变化的技术环境中，团队也能保持凝聚力和高效运作。我认为技术团队管理的核心价值是：确保团队以客户为导向创造价值，以满足客户需要和服务好客户的目的，实现价值变现为企业创收；以持续提升团队效能的方式，确保团队自身的高质量可持续发展；持续培养员工的职业素养，致力于提升员工的工作价值感和幸福感。至于技术管理的愿景，我想除了与企业愿景保持一致，还应当包含工程师群体内在的愿景，比如，成就卓越的软件设计能力与工程能力。</p><p></p><p>要确保团队始终围绕这些核心价值与愿景是必须依赖管理行为来达成的，如业务规划、落实 OKR、项目管理、绩效考核等。除了这些明面上的内容，我认为应当对那些关乎集体工作环境以及个人职业素养的内容有清晰的理解，建立起技术管理的底层逻辑，因为没有这些逻辑，行为上就少了指引，导致提升团队效能时出现动力不足的后果。</p><p></p><p></p><h1>未来技术团队的发展趋势</h1><p></p><p></p><p>InfoQ：未来几年内，技术团队管理将面临哪些新的挑战？目前的技术发展趋势可能对管理方式造成哪些影响？</p><p></p><p>李云： 除了社会发展越来越快的情形下如何确保团队效能，这个老生常谈的挑战外，还有如何让人性化和自组织管理理念生根发芽，这两个理念意味着技术团队管理是手段，而人始终是目的。新技术的发展，会让团队效能的问题被放大。虽说社会上普遍认为 AI 这一新技术的出现会带来很多效能方面的改善，但我可能没有那么乐观，因为只要对效能的理解不深刻，那些效能的改善都是浮在面上的。我还是那句话，通过技术提高效能只是手段，人才是目的。如果理解不了这点，我认为对技术管理的实践就是不得要领的。</p><p></p><p>InfoQ：对未来的技术团队管理者有何建议？如何制定适应未来发展的管理策略？</p><p></p><p>李云： 效能意味着什么，从何而来？我的团队存在哪些效能短板？我自己的短板又是什么？我想对于这些问题技术团队管理者要有自己的答案的，还要形成体系化、结构化的逻辑链才好，而不应是点状的。换句话说，深度的思考从而建立正确的认知是第一步，有知才会有行。</p><p></p><p>在管理策略上可能会有很多的小策略，但最核心的一点是视人为人。当然请不要误会我，视人为人也好、人性化也罢，目的都不是说不能让人感受到压力和焦虑，而应关注人的成长。扎实的成长难免伴随着痛苦的，没有痛苦的“成长”本质可能是时代红利、公司平台红利所带来的，不是个人的能力成长。不过，好的技术管理是在员工痛苦成长的过程中给到帮助和力量以及相互搀扶，让大家感受到：这是我们每个人都会经历的，我们在一起。换句话说，是看见人和与人共情所给人带来的温暖与力量。</p><p></p><p>InfoQ：在即将到来的 QCon 上，您准备向听众分享哪些方面的内容？</p><p></p><p>李云： 我打算从 AI 时代团队管理的不变与变两个维度来展开我的分享，这次分享也是大约十年前的 2015 年，我在 QCon 分享的《打造高质高效的技术团队》这一话题的一次升级。在我看来，技术一直是向前发展的，现在是 AI，再之前是云原生，技术总在变的情形下，我们一定要守住团队管理不变的内容，因为那是让我们可以不断适应新技术的基础，让我有以不变应万变的能力，而变的内容将结合时代特点和中国特色去展开，当然还会给到大家方法论。</p><p></p><p>衷心希望我的方法论不只是运用于软件行业，还能复制到各行各业，让大家对团队管理这件复杂的事有思路、有章法、有实操与优化。这也是为什么我和夫人会携手写《全面效能》这本书的原因。我和夫人作为 70 后人，觉得到了给社会做点事的时候，希望后人们站到我们的肩膀上更有质量地发展和生活，让后辈们不是传承当下的内卷，而是通过更好的自我发展去找到工作与生活的体面，从而有更高的生命价值和生命质量。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fkOlsTHQ4FcVmrYxlUDl</id>
            <title>AI 加入 Scrum 团队，生产力翻倍？</title>
            <link>https://www.infoq.cn/article/fkOlsTHQ4FcVmrYxlUDl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fkOlsTHQ4FcVmrYxlUDl</guid>
            <pubDate></pubDate>
            <updated>Thu, 03 Oct 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Scrum.org 最近发表了一篇由其首席运营官 Eric Naiburg 撰写的文章，题为 “AI as a Scrum Team Member”。Naiburg 在这篇文章中探讨了如何将 AI 作为 Scrum 团队的一员，为 Scrum Master、产品负责人和开发人员带来生产力的提升，并挑战读者想象 AI 无缝融入团队的场景。Thoughtworks 的全球 AI 辅助软件开发负责人 Birgitta Böckeler 最近也发表了一篇题为 “Exploring Generative AI” 的文章，她在其中分享了在工程领域应用大型语言模型的实验性见解，这些模型有望为软件开发团队带来显著的效益提升。</p><p></p><p>Naiburg 将 AI 工具比作配对编程中的协作伙伴。他通过分析工具集成 LLM 的 AI 应用阐述了 AI 如何帮助减轻 Scrum 团队关键角色的认知负担。在讨论 Scrum Master 的角色时，他说 AI 可以作为一个助手，为团队促进、团队绩效和流程优化提供宝贵的建议。Naiburg 通过一个具体的例子展示了如何与 LLM 互动以提高团队会议的参与度：</p><p></p><p></p><blockquote>AI 能够提供多样化的站会促进技巧。例如，当你面临 Scrum 团队成员在 Sprint 回顾环节参与度不高的问题时，你可以直接问 AI：“我遇到了一个问题，我的 Scrum 团队成员在 Sprint 回顾中参与不积极，你有什么建议吗？”</blockquote><p></p><p></p><p>Naiburg 在文章中写道，AI 为开发人员提供了一个得力的团队助手，它能够协助分解和深入理解用户故事。此外，他还强调了利用 AI 来简化原型设计、测试、代码生成、代码评审以及测试数据综合等环节的好处。</p><p></p><p>Böckeler 在她的文章中主要专注于开发人员的角色，分享了她如何利用 LLM 来加快开源项目的采用过程，以及如何针对遗留软件项目交付用户故事。为了理解 AI 工具的能力和局限，她用 LLM 来处理开源项目 Bhamni 待办事项中的一个 Ticket。她详细描述了自己如何使用 LLM 来解析 Ticket 内容、探索代码库，并在有限的项目上下文中寻找线索。</p><p></p><p>Böckeler 使用的工具包括一个采用 RAG（检索增强生成）的 LLM，模型能够根据 Bhamni 维基的内容提供深入的见解。她向 LLM 提供了一个包含用户故事的提示词，并要求它“解释 Bhamni 和相关的医疗术语”。Böckeler 写道：</p><p></p><p></p><blockquote>我提出了一个更广泛的问题：“请解释以下 Ticket 中的 Bhamni 术语和医疗术语：……”。LLM 提供了一个虽然有些冗长和重复但总体上有用的答案。它不仅将 Ticket 内容置于上下文中，还对其进行了再次解释。此外，它还提到了相关功能是通过“Bhamni HIP 插件模块”实现的，这为我们提供了相关代码位置的线索。</blockquote><p></p><p></p><p>在六月的 InfoQ 博客节目中，TitanML 联合创始人兼首席执行官 Meryem Arik 表示将结合了 RAG 的 LLM 作为“研究助理”是“企业最为常见的应用案例”。Böckeler 没有明确说她所使用的 RAG 实现，只是将其描述为一个“Wiki-RAG-Bot”，不过 Arik 却深入谈论了采用一系列定制的开放模型解决方案所能带来的隐私保护和领域专业化的好处。她说：</p><p></p><p></p><blockquote>实际上，如果你正在开发尖端的 RAG 应用，你可能会认为，对于所有事情来说，最好的模型都是 OpenAI 提供的。然而，实际上并非如此。虽然 OpenAI 提供的生成式模型可能是最先进的，但最好的嵌入模型、重排模型、表格解析器和图像解析器等，实际上都是开源的。</blockquote><p></p><p></p><p>为了深入理解代码，Böckeler 将 JIRA Ticket 文本喂给两个用于生成和理解代码的工具——Bloop 和 Github Copilot。她请求这两个工具帮助她找到与这个功能相关的代码。两个模型都提供了一组相似的代码线索，她说这些线索“不是 100% 准确”，但“总体上是有用的”。在探索自动代码生成器的潜力时，Böckeler 尝试使用 Autogen 构建基于 LLM 的 AI 智能体来实现跨框架测试迁移。她解释说：</p><p></p><p></p><blockquote>在这种情况下，智能体是一个利用大型语言模型的应用程序，它的功能不仅限于向用户展示模型的响应，还会根据 LLM 提供的信息自主执行操作。</blockquote><p></p><p></p><p>Böckeler 表示，她的智能体“至少成功运行了一次”，但也“失败了很多次，甚至失败的次数超过了成功的次数。”InfoQ 最近报道了 Upwork 研究所 的一项有争议的研究，该研究基于样本得出的结论是 AI 工具实际上降低了生产力，有 39% 的受访者表示“他们花费在审查或管理 AI 生成内容上的时间更多了。”Naiburg 强调，重要的是要确保团队专注于创造价值，而不仅仅是关注 AI 工具的输出：</p><p></p><p></p><blockquote>需要注意的是：使用这些工具可能会增加产出的“量”。例如，一些软件开发机器人生成了过多的代码，并且包含了不相关的代码。同样，当你让 AI 来完善用户故事、构建测试，甚至创建会议记录时，也可能遇到类似的情况。过多的信息量最终可能会抵消这些工具所提供的价值。</blockquote><p></p><p></p><p>在回顾她与 Autogen 的实验时，Böckeler 提供了一个重要的提醒，即这项技术在“特定的问题领域”内仍然具有其价值。她说：</p><p></p><p></p><blockquote>这些智能体在能够解决我们向它们提出的所有编码问题之前，还有很长的路要走。然而，我认为最重要的是要认识到智能体在哪些特定的问题领域能够为我们提供帮助，不要因为它们并非全能的通用问题解决能手而完全否定它们的价值。</blockquote><p></p><p></p><p>查看原文链接：</p><p><a href="https://www.infoq.com/news/2024/08/llm-agent-team-enablers/">https://www.infoq.com/news/2024/08/llm-agent-team-enablers/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6FlXc3mTACz6tDYvSWk8</id>
            <title>Pinterest 使用 Ray 实现机器学习基础设施现代化</title>
            <link>https://www.infoq.cn/article/6FlXc3mTACz6tDYvSWk8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6FlXc3mTACz6tDYvSWk8</guid>
            <pubDate></pubDate>
            <updated>Tue, 01 Oct 2024 00:05:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>视觉发现平台 Pinterest 披露了其使用开源分布式计算框架 Ray 实现机器学习基础设施现代化的详细过程。在最近的一篇博文中，该公司分享了将 Ray 集成到大规模生产环境中所面临的挑战和他们的实施方案。</p><p></p><p>这个项目的目的是为了增强 Pinterest 的机器学习能力，以解决基本的业务问题。</p><p></p><p>Pinterest 在构建 Ray 基础设施时面临着几个独特的挑战。他们决定在他们的通用联合 Kubernetes 集群 PinCompute 上运行 Ray，但该集群限制安装 KubeRay 及其自定义资源定义等必要的操作符。要有效地实施 Ray，就需要有一个创造性的解决方案来消除这个限制。</p><p></p><p>其他挑战包括需要持久化日志记录和指标、与 Pinterest 专有的时间序列数据库和可视化工具集成，以及遵守公司范围内的 AAA（身份验证、授权和计费）准则。</p><p></p><p>为了应对这些挑战，Pinterest 开发了一个自定义解决方案，包括 API 网关、Ray 集群控制器、Ray 作业控制器和用于外部状态管理的 MySQL 数据库。这种方法在用户和 Kubernetes 之间提供了一个抽象层，简化了 Ray 集群的配置和管理。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1b/1bdac9559ba6cd296cfae0222ecfe535.webp" /></p><p></p><p>该公司还创建了一个专用的用户界面，用于持久化日志记录和指标。在该 UI 上，不需要一个活跃的 Ray 集群就可以进行日志分析，这有助于降低与 GPU 等空闲资源相关的成本。为了提高可观察性，Pinterest 将 Ray 的指标与其内部时间序列数据库 Goku 整合在了一起。该数据库拥有与 OpenTSDB 兼容的 API。他们还遵循 Ray 的建议，将日志持久化到了 AWS S3 上。</p><p></p><p>Pinterest 使用网络隔离与完整身份验证实现了适当的安全措施。他们在 Envoy 后面部署了 Ray Dashboard，在 Kubernetes 环境中部署了他们的服务网格，并在 gRPC 通信中使用了经过定制的 TLS。</p><p></p><p>这篇博文强调了渐进式改进、利用现有基础设施以及定期与内部客户会面以收集反馈的重要性。按照 Pinterest 的说法，采用 Ray 提高了将机器学习想法投入生产应用的速度，现在只需几天，而不是几周。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3c/3cb5bcd9c40692d06741c32e7e80985e.webp" /></p><p></p><p>关于 Pinterest 等公司如何将 Ray 应用于大规模机器学习任务，在 2023 年 QCon Plus 的一次演讲中，来自 Anyscale 的 Zhe Zhang 提供了更多的背景信息。他强调，Ray 的灵活性和易用性使其对希望实现机器学习基础设施现代化的组织特别有吸引力。他描述了 Ray 如何在多于一个实例时实现无缝数据加载和预处理，这对于拥有大量数据集的公司来说至关重要。该功能解决了 ML 工作流中数据处理无法跟上 GPU 计算速度这个常见的瓶颈。</p><p></p><p>他还指出，Ray 能够有效地支持异构计算环境，将 CPU 和 GPU 资源相结合，对于像 Pinterest 这样需要优化硬件利用率的公司来说，这是一个很大的优点。他还讨论了 Ray 的生态系统，包括 Ray Serve 这样的库，以及如何快速实现原型和 ML 模型的部署，这与 Pinterest 报道的开发速度的提高是一致的。</p><p></p><p>DoorDash 是一家规模与 Pinterest 相似的科技公司。他们也经历了机器学习基础设施现代化的过程。虽然他们采用了类似的方法，但也有一些差异。在 2023 年 Ray 峰会的演讲中，来自 DoorDash 的 Siddarth Kodwani 和 Kornel Csernai 介绍了他们使用 Ray 的场景。</p><p></p><p>与 Pinterest 一样，DoorDash 现有的机器学习服务平台也面临着挑战。Sibyl 是一个部署在 Kubernetes 中的 Kotlin 微服务，针对高吞吐量、低延迟场景做过专门的优化，但对于比较新的 ML 范例和库，缺乏灵活性。DoorDash 的解决方案 Argil 与 Pinterest 的方法有一些相似之处。两家公司都构建了自定义控制器来管理 Ray 集群和作业，并将它们与现有的 Kubernetes 基础设施集成在一起。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a5/a55dfb4dbdd930638686b5a0c6f22dab.png" /></p><p></p><p>然而，DoorDash 强烈地感受到，他们应该为他们的数据科学家和机器学习工程师创建一个自助服务平台。他们的服务主要是用 Kotlin 开发的。于是，他们用 Kotlin 开发了一个客户端库，以便简化其服务与 Ray 基础设施的交互。DoorDash 在 Kubernetes 环境中也面临着 GPU 可访问性的独特挑战，关于这一点，他们需要通过与 Nvidia 的密切合作来解决驱动程序兼容性问题。</p><p></p><p>Pinterest 和 DoorDash 的一个显著差异在于部署策略。Pinterest 致力于从头开始构建自定义解决方案，而 DoorDash 则利用 Ray 团队提供的 Helm charts 和 Argo CD 等现有工具进行部署管理。</p><p></p><p>DoorDash 报道的好处与 Pinterest 类似，比如提高了速度和灵活性。他们将机器学习想法投入生产应用的时间从几周缩短到几天。他们还看到了显著的性能提升，从以前的系统迁移到 Ray 之后，一些用例的性能提升了 10 到 20 倍。两家公司都强调了可观察性和监控的重要性。Pinterest 整合了他们自定义的 Goku 时间序列数据库，而 DoorDash 则提到，他们直接整合了 Prometheus 来收集指标。</p><p></p><p>总之，尽管 Pinterest 和 DoorDash 采用 Ray 的方式略有不同，但两家公司都表示，在机器学习基础设施的灵活性、开发速度和性能方面都获得了显著改善。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/08/pinterest-machine-learning-ray/">https://www.infoq.com/news/2024/08/pinterest-machine-learning-ray/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qUf8YNP73YrduKXUbv6O</id>
            <title>【鸿蒙生态学堂】HarmonyOS应用上架</title>
            <link>https://www.infoq.cn/article/qUf8YNP73YrduKXUbv6O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qUf8YNP73YrduKXUbv6O</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:42:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/8a/8aa9b9d0245f29d66ae4e453358d220e.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将指导开发者了解HarmonyOS应用上架的全流程，包括应用的全网发布、分阶段发布和测试发布策略。课程将详细解读上架标准，介绍华为提供的测试工具，帮助开发者进行预审能力检测和隐私托管，确保应用符合上架要求，优化发布流程。</p><p></p><p>课程标签：全网发布、分阶段发布、测试发布、预审能力、隐私托管</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B3YDkF7t0FGI8K7HtGQ7</id>
            <title>【鸿蒙生态学堂】HarmonyOS应用测试</title>
            <link>https://www.infoq.cn/article/B3YDkF7t0FGI8K7HtGQ7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B3YDkF7t0FGI8K7HtGQ7</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:40:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/21/21f0622af35a22f4a2f46e59ab8af78d.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程专注于HarmonyOS应用测试，旨在帮助开发者掌握应用测试的标准和实践。课程将详细解读HarmonyOS应用测试标准，介绍多种测试工具，包括DevEco Testing，以及如何针对典型场景问题进行有效的测试。通过演示测试工具的使用，本课程将指导开发者如何实施性能测试、兼容性测试、稳定性测试和安全测试，确保应用在HarmonyOS平台上的优质体验。</p><p></p><p>课程标签：标准解读、测试工具介绍、典型场景问题、测试工具演示</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Gk6slCPdc126Vq12P8c2</id>
            <title>【鸿蒙生态学堂】并发能力最佳实践</title>
            <link>https://www.infoq.cn/article/Gk6slCPdc126Vq12P8c2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Gk6slCPdc126Vq12P8c2</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:37:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5e384ea65d866b29383fafb06732ff3.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将深入探讨HarmonyOS的并发能力，特别是FFRT并发编程模型。您将学习如何设计高效的应用并发架构，识别并解决启动缓慢问题，提高应用的冷启动速度。课程还将涵盖使用HTTP访问网络资源的方法，以及用户首选项的详细介绍，包括如何按需加载优化、并发优化、IPC优化和代码逻辑优化，以提升应用性能和用户体验。</p><p></p><p>课程标签：应用并发设计、FFRT并发编程模型</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tORqfSLQV3vO9Kpigv32</id>
            <title>【鸿蒙生态学堂】ArkUI性能优化、丢帧分析、响应优化</title>
            <link>https://www.infoq.cn/article/tORqfSLQV3vO9Kpigv32</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tORqfSLQV3vO9Kpigv32</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:35:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c394a3ccd16ca13d47e2bde9930e6fe.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将深入探讨HarmonyOS的ArkUI框架，提供全面的UI性能优化指南。您将学习到如何通过ArkUI框架进行高效UI开发，并掌握常见的性能优化措施，包括丢帧问题的原理分析和优化技巧。课程将涵盖UI优化、按需加载、并发处理、IPC通信优化以及代码逻辑优化，同时探讨如何提升视觉感知流畅度，确保用户界面既快速又吸引人。</p><p></p><p>课程标签：ArkUI框架基本介绍、ArkUI常见性能优化措施、丢帧问题原理、并发优化、IPC优化、代码逻辑优化、视觉感知优化</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ibR0aR25obK6gJbOUk2u</id>
            <title>【鸿蒙生态学堂】冷启动优化、合理使用动画、长列表加载性能优化最佳实践</title>
            <link>https://www.infoq.cn/article/ibR0aR25obK6gJbOUk2u</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ibR0aR25obK6gJbOUk2u</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:25:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/69/690d8feb7bf52fad55af9668d1c2e0d0.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程深入探讨HarmonyOS应用的冷启动优化技巧，从应用冷启动概述到具体实施策略，提供全面的优化方案。课程内容包括合理使用动画提升用户感知流畅度、数据驱动UI更新机制、以及长列表加载性能优化的最佳实践。你将学习到如何通过懒加载、缓存列表项、组件复用和布局优化等技术手段，有效提高冷启动速度，减少用户等待时间，从而打造更流畅、更高效的HarmonyOS应用体验。</p><p></p><p>课程标签：应用冷启动概述、应用冷启动流程、识别启动缓慢问题、提高冷启动速度、提升动画感知流畅度、提高动画运行流畅度、懒加载、缓存列表项、组件复用、布局优化</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3YphPsjvJRLj3bJq0OeW</id>
            <title>【鸿蒙生态学堂】网络和数据存储</title>
            <link>https://www.infoq.cn/article/3YphPsjvJRLj3bJq0OeW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3YphPsjvJRLj3bJq0OeW</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:20:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/03/0360867e265ad191df5f57a0280da0a0.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程深入探讨HarmonyOS中的网络和数据存储管理，特别是使用HTTP协议访问网络资源和用户首选项的详细介绍。您将学习如何在HarmonyOS应用中发起HTTP请求，处理响应数据，以及如何利用用户首选项进行轻量级的数据持久化存储。课程将通过实例演示如何高效地管理应用配置和用户偏好设置。</p><p></p><p>课程标签：使用HTTP访问网络、用户首选项介绍</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MyTWLkCw3X0jOzxWrXe9</id>
            <title>【鸿蒙生态学堂】ArkUI开发基础（下）</title>
            <link>https://www.infoq.cn/article/MyTWLkCw3X0jOzxWrXe9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MyTWLkCw3X0jOzxWrXe9</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:16:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0581545100841fd18bc37b1fc91337a.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程深入探讨HarmonyOS的ArkUI框架，特别是数据驱动UI更新和组件导航的高级概念。您将学习如何使用ArkWeb技术构建动态网页内容，掌握数据绑定技巧以确保UI与底层数据源同步更新。此外，课程将指导您通过设置组件导航来增强应用的用户体验，实现流畅的页面过渡和有效的用户交互。</p><p></p><p>课程标签：使用ArkWeb构建页面、数据驱动UI更新、设置组件导航</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WHbR2WJYF838obHYXLyh</id>
            <title>【鸿蒙生态学堂】ArkUI开发基础（上）</title>
            <link>https://www.infoq.cn/article/WHbR2WJYF838obHYXLyh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WHbR2WJYF838obHYXLyh</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 07:05:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/96/96a0dde74dbf275f70e0b75ad8450384.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将介绍HarmonyOS的ArkUI框架，包括其基础语法和如何使用常用组件构建页面。ArkUI是HarmonyOS应用的UI开发框架，提供简洁的UI语法、丰富的组件和实时界面预览工具。您将学习到ArkUI的关键特性，如极简的UI信息语法、丰富的内置UI组件、多维度的状态管理机制，以及如何支持多设备开发</p><p>。通过课程，您将能够掌握使用ArkUI框架进行高效UI开发的技能。</p><p></p><p>课程标签：ArkUI（方舟UI框架）介绍、使用常用组件构建页面</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4XT0Gfx8l9zTBUDJSuBS</id>
            <title>【鸿蒙生态学堂】应用程序框架基础</title>
            <link>https://www.infoq.cn/article/4XT0Gfx8l9zTBUDJSuBS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4XT0Gfx8l9zTBUDJSuBS</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:59:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/28/2869c81a9b66ff42426be19b52c85b08.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将带领开发者深入了解HarmonyOS的应用程序框架基础，重点探讨UIAbility组件的工作原理和生命周期管理。通过学习，开发者将能够掌握如何在HarmonyOS中创建和使用UIAbility组件，包括其启动模式和窗口管理。同时，课程还将介绍DevEco Studio工具的使用，它是专为HarmonyOS应用开发设计的IDE，支持代码编写、调试和应用构建等功能，助力开发者高效开发HarmonyOS应用。</p><p></p><p>课程标签：应用程序框架基础、UIAbility组件概述</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OrSuYA1QfKR6yyy7Pva6</id>
            <title>【鸿蒙生态学堂】ArkTS语法介绍</title>
            <link>https://www.infoq.cn/article/OrSuYA1QfKR6yyy7Pva6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OrSuYA1QfKR6yyy7Pva6</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:59:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/f6/f61766298b7fc02425ba371bf967f244.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将深入介绍HarmonyOS的应用开发语言——ArkTS。您将学习到ArkTS的基本语法，包括变量声明、类型系统、运算符等，以及如何使用ArkTS进行声明式UI开发。课程还将展示如何利用DevEco Studio这一强大的集成开发环境，进行代码编写、调试和应用构建，帮助您快速上手HarmonyOS应用开发。</p><p></p><p>课程标签：ArkTS基础语法、声明式UI语法</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RfXt52pwFiBuPYdxTKeO</id>
            <title>【鸿蒙生态学堂】HarmonyOS介绍</title>
            <link>https://www.infoq.cn/article/RfXt52pwFiBuPYdxTKeO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RfXt52pwFiBuPYdxTKeO</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:53:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3692253bc6573c8d4025458a49f9632.png" /></p><p>点击图片开始学习</p><p></p><p>课程简介：本课程将带您快速了解HarmonyOS，深入探讨HarmonyOS的核心特性，包括其分布式架构和跨设备能力。课程还将介绍华为提供的赋能套件，帮助开发者高效开发应用。最后，您将掌握DevEco Studio，HarmonyOS官方集成开发环境，用于构建、调试和部署应用。无论您是初学者还是有经验的开发者，本课程都将为您提供必要的工具和知识，让您在HarmonyOS平台上大展宏图。</p><p></p><p>课程标签：HarmonyOS简介、赋能套件和学习资源介绍、DevEco Studio的使用</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uwjRymcH7rdDVxXi7WYR</id>
            <title>腾讯被曝隐藏专业职级，不希望以职级论英雄；要求加薪减工时、职位“世袭”？！印度三星离谱罢工；“拒绝跑步被辞退”当事人道歉 | Q资讯</title>
            <link>https://www.infoq.cn/article/uwjRymcH7rdDVxXi7WYR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uwjRymcH7rdDVxXi7WYR</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Sep 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>上交所启动全网测试；“管培生拒绝跑步被辞退”当事人道歉；“三只羊”录音系伪造，AI&nbsp;大模型成幕后黑手；阿里京东互相打通；淘宝可使用微信支付；OpenAI&nbsp;首席技术官宣布离职；印度三星工人罢工，要求“世袭”职位；英特尔拒绝&nbsp;Arm&nbsp;收购产品部门；游戏科学&nbsp;CEO&nbsp;冯骥谈《黑神话：悟空》DLC&nbsp;进度：让团队先&nbsp;“躺”两年；腾讯职级制度改革；英特尔“全公司的希望”：Intel&nbsp;18A&nbsp;芯片正式亮相！韩国电池巨头&nbsp;SK&nbsp;On&nbsp;将裁员以保持竞争力；OpenAI&nbsp;向所有付费的&nbsp;ChatGPT&nbsp;用户推出了语音助手服务；欧洲隐私机构&nbsp;noyb&nbsp;指控火狐&nbsp;Firefox&nbsp;浏览器；英特尔释出最新微码更新修正&nbsp;13/14&nbsp;代酷睿处理器崩溃问题……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>上交所启动全网测试</h4><p></p><p>据媒体报道，上交所定于2024&nbsp;年9月29日（周日），组织开展竞价、综业等平台相关业务测试。邀请全体市场参与人参加测试，主要验证相关技术平台业务与技术调整的准确性。测试模拟1个交易日的交易和清算，测试主要内容包括“验证连续竞价时段集中申报大量订单时，竞价平台业务处理平稳运行”等。</p><p></p><p>根据测试方案，参测市场参与人应以9月27日生产环境闭市后数据为基础，根据竞价、综业等其它平台业务规则和测试方案，准备股票、基金、大宗交易、ETF申赎等各类订单数据，以确保可以完成测试。</p><p></p><p>9&nbsp;月&nbsp;27&nbsp;日，上交所在官网发布公告称，当日开盘后，上交所股票竞价交易出现成交确认缓慢的异常情况，并导致交易受到影响。经处置，股票竞价交易于11点13分起逐步恢复。对于该异常情况的发生，上交所深表歉意。</p><p></p><h4>“管培生拒绝跑步被辞退”当事人道歉</h4><p></p><p>近日，一起“应届生因拒绝周末跑步活动遭辞退”事件在网络上持续发酵。</p><p></p><p>据9月22日《新闻晨报》报道，北京一网络技术有限公司应届生张先生因拒绝参加公司周末跑步活动，在入职43天后被辞退。</p><p></p><p>第一次活动是在8月21日晚9点，公司组织员工下班后进行5公里的户外跑步；第二次则安排在8月31日（周六）早上7点，要求员工到某公园集合进行10公里的户外跑步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4cce2df157a941334c66312ce486ecc2.webp" /></p><p></p><p>张先生称，由于连续12天高强度加班导致身体疲惫，他未能参加第二次跑步活动，随后被公司管理人员约谈劝退，理由是他缺乏主动性和管理潜力。张先生认为拒绝跑步是基于身体状况考虑，不应成为辞退理由。随后张先生提起劳动仲裁，并在社交媒体发帖，并迅速引起大量关注。</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/270d178606319df090bdd70f7f3a4c87.webp" /></p><p></p><p>不过该事件在持续发酵数日后迎来新转折。9月22日，当事人发布道歉声明称，其于2024年9月19日在社交平台曾发表过自己在北京易点淘网络科技有限公司（易点云）的就职经历，导致易点云登上热搜。对于给易点云造成的不良影响向公司道歉。</p><p></p><h4>“三只羊”录音系伪造，AI&nbsp;大模型成幕后黑手</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日凌晨，Reecho&nbsp;睿声公司官微发布声明称，近日，公司收到合肥警方调取证据通知书，针对网传三只羊“卢某录音门”事件所涉及的音频部分，经与合肥警方配合查实，确系嫌疑人王某使用公司自主研发的&nbsp;Reecho&nbsp;睿声&nbsp;AI&nbsp;配音大模型平台，由卢某此前直播片段约&nbsp;30&nbsp;秒录音进行克隆，并通过文本生成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/641caeec1b7608cb7c761307393bd563.webp" /></p><p></p><p>据警方通报，经过侦查，9&nbsp;月&nbsp;22&nbsp;日晚，我局将犯罪嫌疑人王某某&nbsp;(男，25&nbsp;岁)&nbsp;抓获，并在其电脑、手机和制作&nbsp;AI&nbsp;音频的网站中发现伪造相关音视频的证据；结合其供述、调查取证，并经部、省专业机构检验鉴定，认定报案所涉网传音视频系伪造。</p><p></p><p>现已查明，9&nbsp;月&nbsp;16&nbsp;日，王某某利用从互联网下载的音视频资料，杜撰卢某某酒后言论脚本，先使用&nbsp;AI&nbsp;工具训练生成假冒卢某某的音频&nbsp;(其中出现的女声也系&nbsp;AI&nbsp;工具训练生成)，后用视频软件合成音视频，并通过网络发布，形成谣言大量传播。目前，王某某已被依法采取刑事强制措施，案件正在进一步侦办中。</p><p></p><h4>阿里京东互相打通，电商巨头“握手言和”！</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日，据晚点&nbsp;LatePost&nbsp;消息，京东物流和菜鸟速递将分别接入淘天、京东平台，京东也将在“双&nbsp;11”前开通支付宝支付。据部分从业者透露，物流电子面单上的基础信息并非核心机密，接通端口也没有技术难题，企业更为在意的是与客户更深入的合作细节和商业数据。同时，一些电商代运营服务商也做好了平台与物流商的端口打通，为商家铺货做准备。</p><p></p><p>这也意味着淘天商家此后在系统中能够选择京东物流作为发货物流。与之相对应的是，京东也将菜鸟旗下的自营快递品牌菜鸟速递接入平台当中，并同时接入菜鸟旗下的包裹代收点菜鸟驿站。或许在今年“双&nbsp;11”，京东平台的商品信息就会出现在菜鸟当中。</p><p></p><p>要知道一开始京东是支持使用支付宝的，直到&nbsp;2011&nbsp;年&nbsp;5&nbsp;月&nbsp;8&nbsp;日，京东商城宣布停止使用支付宝作为支付方式之一。当时刘强东解释了停用支付宝的原因：一是支付宝的费率太高，每年京东都要因为支付宝的费率高而多支付&nbsp;500-600&nbsp;万元；二是京东&nbsp;80%&nbsp;都是货到付款，用在线银联支付很少约在&nbsp;10%&nbsp;左右，所以与支付宝停止合作不会给用户带来影响。从此之后，京东与阿里的关系便急转直下，火药味越来越浓。谁能想到&nbsp;13&nbsp;年后的今天，双方竟能重归于好，互通有无。</p><p></p><p>在双方物流合作达成一致的另一边，京东也将在“双&nbsp;11”前夕接入支付宝支付。从物流到支付，阿里跟京东，似乎真的做好准备“握手言和”了。尽管&nbsp;27&nbsp;日晚间，淘天集团和京东集团相关负责人都尚未向确证该消息，但淘宝和天猫的商家已经做好了准备接受这样的改变。随着各大平台的壁垒逐渐拆除，一个真正“互联”的互联网正离我们越来越近。</p><p></p><h4>官宣！淘宝可使用微信支付</h4><p></p><p>继&nbsp;26&nbsp;日阿里京东互相打通后，淘宝官方&nbsp;9&nbsp;月&nbsp;27&nbsp;日宣布，当日（27&nbsp;日）起，消费者逛淘宝买买买时，可以使用微信支付了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22b10fd7e262c3e8322caa9f945d12d7.webp" /></p><p></p><p>从具体展示的页面看，消费者在完成商品选购后，在支付页面选择“微信支付”，即可完成支付，全程体验与其他支付工具一致。值得提醒的一点是，使用“微信支付”功能，需要更新至最新版的淘宝&nbsp;App。</p><p></p><p>9&nbsp;月&nbsp;5&nbsp;日，淘宝网发布公告，宣布新增微信支付能力：为提升消费者的购物体验，淘宝网计划新增微信支付能力，于本公告公示七天后逐步向所有淘宝网卖家开放。</p><p></p><p>基于上述服务的增加，淘宝网相应升级平台规则，主要变化为：支付服务商相关的名称、账户、余额、交易额等表述统一规范为“支付机构”“支付账户”“支付账户余额”“订单交易额”。</p><p></p><p>淘宝官方客服回应记者称，待平台商家微信支付能力开通完毕，消费端会自动生效。</p><p></p><h4>OpenAI&nbsp;首席技术官宣布离职，高层地震继续</h4><p></p><p>财联社&nbsp;9&nbsp;月&nbsp;26&nbsp;日讯，OpenAI&nbsp;高层变动继续，首席技术官&nbsp;Mira&nbsp;Murati&nbsp;周三表示，她将离开&nbsp;OpenAI。已经在该初创公司工作六年半的&nbsp;Murati，是&nbsp;OpenAI&nbsp;大模型的重要技术主管，而她的离开也让业界对&nbsp;OpenAI&nbsp;内部管理和权力变化更加好奇。</p><p></p><p>穆拉蒂在声明中提到这是一个艰难的决定，她的离职原因是：“因为我想腾出时间和空间来探索自己，目前，我的首要任务是尽自己所能确保顺利过渡，保持我们已经建立起来的势头。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dd4858d47ea9c4bf57036245dfefb73.webp" /></p><p></p><p>▲穆拉蒂发布的离职声明</p><p></p><p>除了穆拉蒂离职外，OpenAI&nbsp;总裁格雷格·布罗克曼&nbsp;(Greg&nbsp;Brockman)&nbsp;也已休假。据报道，OpenAI&nbsp;计划重组，将取消非营利性董事会的控制权，CEO&nbsp;萨姆·阿尔特曼（Sam&nbsp;Altman）还将首次获得&nbsp;OpenAI&nbsp;股权。</p><p></p><p>阿尔特曼已在社交平台上回应穆拉蒂的声明。他说感谢穆拉蒂所做的一切，OpenAI&nbsp;将很快会详细介绍过渡计划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc88bc2fadaa8be6a5572271068b3d0a.webp" /></p><p></p><p>▲阿尔特曼回复</p><p></p><p>这已经是今年从&nbsp;OpenAI&nbsp;离职的第&nbsp;11&nbsp;位高管。其中包括今年&nbsp;5&nbsp;月&nbsp;OpenAI&nbsp;联合创始人&nbsp;Ilya&nbsp;Sutskever&nbsp;和前安全负责人&nbsp;Jan&nbsp;Leike&nbsp;宣布离职，联合创始人&nbsp;John&nbsp;Schulman&nbsp;上个月离职并加入&nbsp;OpenAI&nbsp;的竞争对手&nbsp;Anthropic。</p><p></p><p>被曝出正在寻求巨额融资、估值达到&nbsp;1500&nbsp;亿美元的&nbsp;OpenAI，并没有减慢高管的离职步伐。本月中旬，OpenAI&nbsp;传出将以&nbsp;1500&nbsp;亿美元估值寻求&nbsp;65&nbsp;亿美元融资，知情人士称，Thrive&nbsp;Capital&nbsp;将领投此轮融资，计划投资&nbsp;10&nbsp;亿美元，老虎环球计划加入，并且微软、英伟达、苹果等头部玩家均在洽谈投资事宜。</p><p></p><h4>印度三星工人罢工，要求“世袭”职位</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日，三星电子位于印度泰米尔纳德邦金奈附近的工厂已遭遇近三周的罢工，超过&nbsp;1000&nbsp;名工人走出岗位，搭建帐篷进行抗议。</p><p></p><p>这些工人的主要诉求包括将现有月薪（3&nbsp;万—3.5&nbsp;万卢比）提高&nbsp;25%—30%，并正式承认工会的权利。此外，在员工去世时，允许其家属“继承”岗位，以及为去世员工子女提供每年&nbsp;5&nbsp;万卢比的私立学校学费支持。</p><p></p><p>这场始于&nbsp;9&nbsp;月&nbsp;9&nbsp;日的罢工是近年来印度规模最大的工人抗议活动之一。尽管三星在印度支付的工资水平高出当地平均水平&nbsp;1.8&nbsp;倍，不过工人们依然认为现有薪资不足以满足他们的生活需求。</p><p></p><p>罢工导致工厂生产显著受阻，工会称生产中断已达&nbsp;70%。虽然三星表示，部分生产可通过非罢工工人和新雇员恢复至接近正常水平，但整体局势依然紧张。</p><p></p><p>印度联邦劳工部长曼苏克·曼达维亚已向泰米尔纳德邦政府发出信件，要求介入并寻求“友好”解决方案。</p><p></p><p>据悉，此次罢工不仅关乎三星的运营，在全球制造业转移的背景下，印度当前的劳动争议可能会削弱印度作为制造中心的吸引力。随着印度政府积极推动“印度制造”倡议，促进外资流入，此次罢工可能会直接影响到未来的投资意愿，除三星外，现代汽车等公司在印度当地也正面临类似挑战，这将对印度的整体制造环境与劳动力市场构成深远影响。</p><p></p><h4>英特尔拒绝&nbsp;Arm&nbsp;收购产品部门</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日消息，据外媒报道，ARM&nbsp;曾接触英特尔，探讨可能收购这家陷入困境的芯片制造商的产品部门，但被告知该业务不出售。</p><p></p><p>据知情人士透露，ARM&nbsp;与英特尔进行了高层接触，但是对英特尔的制造业务没有展示出兴趣。英特尔目前有两个主要部门：一个是销售&nbsp;PC、服务器和网络设备芯片的产品部门，另一个是运营工厂的部门。</p><p></p><p>ARM&nbsp;的大部分收入来自销售智能手机芯片设计，但其&nbsp;CEO&nbsp;雷内·哈斯一直在寻求扩大业务范围，包括进军&nbsp;PC&nbsp;和服务器领域。在这一领域，ARM&nbsp;的芯片设计将与英特尔展开竞争。尽管英特尔不再拥有曾经的技术优势，但该公司仍在&nbsp;PC&nbsp;和服务器市场占据主导地位。对于&nbsp;ARM&nbsp;来说，与英特尔的合并将有助于拓展市场，推动其销售更多自有产品。</p><p></p><p>据了解，Arm&nbsp;公司的收入只是英特尔的一小部分,&nbsp;但其估值自去年首次公开募股以来飙升，目前市值超过&nbsp;1560&nbsp;亿美元。相比之下，英特尔今年的市值已损失超过一半，目前的市值为&nbsp;1023&nbsp;亿美元。</p><p></p><h4>游戏科学&nbsp;CEO&nbsp;冯骥谈《黑神话：悟空》DLC&nbsp;进度：让团队先&nbsp;“躺”两年</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日下午，游戏科学&nbsp;CEO&nbsp;冯骥在回复网友评论&nbsp;“DLC&nbsp;在做了？”时表示：“咱就说，能不能让团队先躺两年？采采风，恋恋爱，尽情玩玩其他游戏。”</p><p></p><p>不久之前的&nbsp;9&nbsp;月&nbsp;21&nbsp;日，2024&nbsp;北京文化论坛文化产业投资人大会期间，游戏科学&nbsp;CEO&nbsp;冯骥及商务经理黄一帆确认，《黑神话：悟空》的&nbsp;DLC&nbsp;正在开发中。此前有消息称，新&nbsp;DLC&nbsp;将包括“再起”和“此去”两大篇章，涉及多个新角色和故事，将在&nbsp;2025&nbsp;年农历新年左右推出，但具体内容尚未官方证实。首位投资人吴旦预计游戏生命周期内销量可达&nbsp;3000&nbsp;万份，并对游戏未来充满信心。</p><p></p><h4>腾讯职级制度改革：隐藏职级，所有职级最短停留时间为&nbsp;1&nbsp;年</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日，腾讯组织与人才发展部对内发布全员邮件，宣布对《腾讯员工职业发展管理制度》进行调整。邮件中声称，本次调整是&nbsp;2022&nbsp;年腾讯职级改革的延续。</p><p></p><p>从今日（27&nbsp;日）起，腾讯员工的专业职级信息将不在企业微信中进行公开展示。对于取消职级外显的理由，腾讯宣称是希望内部员工减少对职级的过度关注，不以职级论英雄，被“职级对等”之类的官僚陋习捆住手脚，提倡平等的职场文化。</p><p></p><p>此前腾讯内部已经有不少业务团队在进行试点，将组织架构中管理者的管理职级（总监、GM&nbsp;等）调整为“XX（业务&nbsp;/&nbsp;团队）负责人”的名称。</p><p></p><p>本次调整的另一个重要改变是，将目标职级为&nbsp;8&nbsp;级及以下的职级停留时间要求，从&nbsp;0.5&nbsp;年延长至&nbsp;1&nbsp;年，与其他职级保持一致。</p><p></p><p>与此同时，继续保持目标职级&nbsp;8&nbsp;级及以下的上下半年两次申报窗口，并放宽其“绿色通道申报”资格，即当次绩效为&nbsp;Outstanding（五星评定）即可申请。</p><p></p><p>在奖励机制方面，腾讯依然在内部保留了“特殊申报”通道，为最近一年做出“特殊贡献”但不符合“绿色通道申报”资格的员工，提供快速发展的机会。</p><p></p><p>为了保持内部组织架构中的人员流动性，在最新的《腾讯员工职业发展管理制度》中，还对公司员工跨通道&nbsp;/&nbsp;职位发展做出了新的规定。</p><p></p><p>其中，11&nbsp;级及以下的跨通道&nbsp;/&nbsp;职位发展，原则上转岗就转职业通道，不需要再做额外的评审申请。跨族群或者&nbsp;12&nbsp;级及以上的转通道申请，公司将与各个通道&nbsp;/&nbsp;族群沟通，尽量简化流程。</p><p></p><p>此外腾讯还将鼓励员工在管理路径和专业路径之间进行切换，也就是说，管理干部和专家这两个发展路径将有所打通。</p><p></p><h4>英特尔“全公司的希望”：Intel&nbsp;18A&nbsp;芯片正式亮相！</h4><p></p><p>9&nbsp;月&nbsp;25&nbsp;日消息，据&nbsp;Tom's&nbsp;Hardware&nbsp;报道，处理器大厂英特尔于上周在俄勒冈州波特兰市举行的&nbsp;Enterprise&nbsp;Tech&nbsp;Tour&nbsp;活动中，首次展示了其代号为&nbsp;Clearwater&nbsp;Forest&nbsp;的&nbsp;Xeon&nbsp;芯片，这也是英特尔首款最新的&nbsp;Intel&nbsp;18A&nbsp;制程芯片，不过该芯片可能需要等到明年下半年才能上市。</p><p></p><p>虽然英特尔在活动中推出了其最新的基于&nbsp;Intel&nbsp;3&nbsp;制程的&nbsp;Xeon&nbsp;6&nbsp;Granite&nbsp;Rapids&nbsp;数据中心芯片，这也自&nbsp;2017&nbsp;年&nbsp;AMD&nbsp;EPYC&nbsp;推出以来，英特尔首次将数据中心处理器的内核数量提升到与&nbsp;AMD&nbsp;的竞品相当的水平。但是如果要进一步拉开差距，仍需要寄希望于&nbsp;Intel&nbsp;18A&nbsp;制程的&nbsp;Clearwater&nbsp;Forest&nbsp;的&nbsp;Xeon&nbsp;芯片。</p><p></p><p>对于正处于财务危机当中的英特尔掌舵者帕特·基辛格（Pat&nbsp;Gelsinger）来说，接下来英特尔将在&nbsp;2025&nbsp;年上半年量产的&nbsp;Intel&nbsp;18A&nbsp;制程将会是其扭转乾坤的关键。不仅英特尔下一代的&nbsp;PC&nbsp;及数据中心处理器需要依靠&nbsp;Intel&nbsp;18A&nbsp;制程回归内部制造，并提升产品竞争力，同时英特尔也寄希望于&nbsp;Intel&nbsp;18A&nbsp;实现对于台积电&nbsp;2nm&nbsp;制程的超越，从而赢得更多的代工客户。</p><p></p><h4>韩国电池巨头&nbsp;SK&nbsp;On&nbsp;将裁员以保持竞争力，员工可选特殊休假&nbsp;/&nbsp;自愿离职</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日消息，据韩联社、路透社报道，韩国能源巨头&nbsp;SK&nbsp;Innovation&nbsp;旗下的电池部门&nbsp;SK&nbsp;On&nbsp;今天发布声明称，公司计划推出“自愿离职”计划来削减员工人数，以努力提高效率，在充满挑战的电动汽车市场上保持竞争力。</p><p></p><p>SK&nbsp;On&nbsp;表示，作为“效率计划”的一部分，该公司拟提供特殊休假、自愿离职两种形式。“这些都是积极的措施，目的是建立一支精干、灵活的员工队伍，以便我们能够更好地驾驭不断变化的电动汽车市场环境。在公司努力提高效率并为可持续增长奠定基础的同时，我们将全力支持员工的职业发展，他们为我们成功成为顶级电池制造商作出了贡献。”</p><p></p><p>作为提高效率措施的一部分，SK&nbsp;On&nbsp;表示将向同意离职的员工提供自愿离职方案，包括向去年&nbsp;11&nbsp;月前加入公司的员工提供&nbsp;50%&nbsp;的工资，让他们提前退休。一份监管文件显示，截至今年&nbsp;6&nbsp;月底，该公司共有&nbsp;3558&nbsp;名员工。</p><p></p><p>SK&nbsp;On&nbsp;自&nbsp;2021&nbsp;年从&nbsp;SK&nbsp;Innovation&nbsp;分拆出来以来从未实现过盈利，今年第二季度的营业亏损为&nbsp;4600&nbsp;亿韩元（IT&nbsp;之家备注：当前约&nbsp;24.25&nbsp;亿元人民币），而上一季度的亏损为&nbsp;3320&nbsp;亿韩元（当前约&nbsp;17.5&nbsp;亿元人民币）。</p><p></p><h4>OpenAI、微软、谷歌等签署欧盟《人工智能公约》</h4><p></p><p>欧盟委员会当地时间周三（9&nbsp;月&nbsp;25&nbsp;日）公布了《人工智能公约》（下文简称为《公约》）的首批签署名单，上面有&nbsp;100&nbsp;多个签署方。《公约》的重点是让企业就如何处理和部署人工智能发布“自愿承诺”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c1ee98af009c7796f066c1d9932f0cb.webp" /></p><p></p><p>截图来源于网络</p><p></p><p>尽管具有法律约束力的欧盟《人工智能法案》（下文简称为《法案》）已于上月生效，但其所有合规期限仍留出数年时间，之后才能生效。这就造成了一段时间的空窗期，而欧盟希望用《公约》来填补这一空白。</p><p></p><p>欧盟委员会表示，这些“自愿承诺”是由《法案》的监督机构人工智能办公室起草的，在收集了人工智能公约中相关利益者所提交的反馈后，最终形成的承诺清单允许签署方自行挑选适合自己的承诺，但前提是至少需要承诺三项“核心行动”。</p><p></p><p>目前，名单上的公司包括亚马逊、微软、OpenAI、谷歌、Palantir、三星、SAP、Salesforce、Snap、空客、保时捷、联想、高通等，覆盖面极广，囊括了电信公司、咨询公司、软件公司、银行&nbsp;/&nbsp;支付公司、跨国公司、中小企业和面向消费者的平台。</p><p></p><p>《公约》旨在提高参与度和促进承诺，此外还侧重于促进信息共享，以便签署方能够相互帮助，以应对欧盟人工智能《法案》的新要求，并积极制定最佳实践。</p><p></p><p>签署方还被邀请在公布自己的承诺&nbsp;12&nbsp;个月后报告进展情况，这为下一轮宣传提供了机会。总体来看，《公约》将为公司争取声誉影响力，还能够激励签署方之间的竞争。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f9aafdc4252719b2400e39c2282fda8.webp" /></p><p>图片来源于网络</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>OpenAI&nbsp;向所有付费的&nbsp;ChatGPT&nbsp;用户推出了语音助手服务</h4><p></p><p>美东时间&nbsp;9&nbsp;月&nbsp;24&nbsp;日周二，所有付费订阅&nbsp;ChatGPT&nbsp;Plus&nbsp;和&nbsp;Team&nbsp;计划的用户都将可以使用新的&nbsp;AVM&nbsp;功能，不过该模式将在未来几天逐步推出。它将首先在美国市场上线。&nbsp;下周，该功能将向&nbsp;OpenAI&nbsp;Edu&nbsp;和&nbsp;Enterprise&nbsp;计划的订阅者开放。</p><p></p><p>据悉，AVM&nbsp;提高了部分外语的对话速度、流畅度并改进口音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f9777ef6512f50c28514cfb1f59b1a6.webp" /></p><p></p><p>此外，AVM&nbsp;还新增了两大功能：为语音助手存储“自定义指令”，以及记住用户希望语音助手表现何种行为的“记忆”的功能（这类似于今年&nbsp;4&nbsp;月&nbsp;OpenAI&nbsp;为&nbsp;ChatGPT&nbsp;文本版本推出的记忆功能）。</p><p></p><p>也就是说，ChatGPT&nbsp;用户可以利用自定义指令和“记忆”来确保语音模式是个性化的，AVM&nbsp;会根据他们对所有对话的偏好做出响应。</p><p></p><p>语音方面，**OpenAI&nbsp;推出了五种不同风格的新声音：**Arbor、Maple、Sol、Spruce&nbsp;和&nbsp;Vale，加上之前老版本的四种声音&nbsp;Breeze、Juniper、Cove&nbsp;和&nbsp;Ember，可选声音达到九种，&nbsp;撤走了被指山寨“寡姐”（女演员斯嘉丽·约翰逊）的声音&nbsp;Sky。</p><p></p><p>这意味着，ChatGPT&nbsp;的&nbsp;Plus&nbsp;版个人用户和小型企业团队用户（Teams）可以通过“说话”的方式，而不是输入提示来使用聊天机器人。&nbsp;当用户在应用程序上进入语音模式时，他们会通过一个弹出窗口知道他们已经进入了高级语音助手。</p><p></p><h4>欧洲隐私机构&nbsp;noyb&nbsp;指控火狐&nbsp;Firefox&nbsp;浏览器：利用隐私保护功能追踪用户行踪</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日消息，欧洲颇具影响力的数据隐私倡导组织&nbsp;None&nbsp;of&nbsp;Your&nbsp;Business（noyb）昨日（9&nbsp;月&nbsp;25&nbsp;日）发布新闻稿，指控火狐&nbsp;Firefox&nbsp;浏览器正利用“隐私保护”功能追踪用户的行踪。</p><p></p><p>noyb&nbsp;表示火狐&nbsp;Firefox&nbsp;浏览器于今年&nbsp;7&nbsp;月发布更新，在未明确告知用户的情况下，悄然启用“Privacy&nbsp;Preserving&nbsp;Attribution”功能。</p><p></p><p>Mozilla&nbsp;表示通过&nbsp;Privacy&nbsp;Preserving&nbsp;Attribution&nbsp;功能，可以让网站在不收集个人数据的情况下了解广告的表现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b56bc4e8e2a208011a880f534aa5cd9.webp" /></p><p></p><p>该功能在&nbsp;Firefox&nbsp;浏览器完成有效追踪，并以匿名方式移交给聚合服务，这样就可以在不泄露用户隐私的情况下为广告商提供所需的信息。</p><p></p><p>noyb&nbsp;承认，虽然这种方法可能比无限制跟踪的侵犯性要小，但它仍然侵犯了欧盟&nbsp;GDPR&nbsp;规定的用户权利，更糟糕的是该功能是默认开启的。</p><p></p><h4>复旦大学团队贡献，Win10&nbsp;/&nbsp;Win11&nbsp;版苹果&nbsp;iTunes&nbsp;12.13.3&nbsp;修复提权漏洞</h4><p></p><p>9&nbsp;月&nbsp;27&nbsp;日消息，科技媒体&nbsp;9to5Mac&nbsp;9&nbsp;月&nbsp;26&nbsp;日发文，报道称苹果公司于本周四发布更新日志，介绍了面向&nbsp;Windows&nbsp;10、Windows&nbsp;11&nbsp;平台发布的&nbsp;iTunes&nbsp;12.13.3&nbsp;更新内容。</p><p></p><p>IT&nbsp;之家援引苹果官方更新日志，本次更新主要修改了本地提权漏洞，苹果公司通过施加额外限制已经修复。</p><p></p><p>该漏洞追踪编号为&nbsp;CVE-2024-44193，由复旦大学的&nbsp;Mads&nbsp;Ball、Bocheng&nbsp;Xiang&nbsp;两人发现并报告。苹果公司目前定期更新&nbsp;Windows&nbsp;10、Windows&nbsp;11&nbsp;版&nbsp;iTunes，但已经有一段时间没有为其推出新功能了。</p><p></p><p>苹果公司目前开始以独立应用的方式，提供&nbsp;iTunes&nbsp;的部分功能，例如推出了独立的&nbsp;Apple&nbsp;Music&nbsp;和&nbsp;Apple&nbsp;TV&nbsp;应用程序（当前仅为测试版），还有一款专门管理&nbsp;iOS&nbsp;设备的应用程序&nbsp;Apple&nbsp;Device，用于备份和恢复软件等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/432bac9cdf9f8d00826157fede4a0fbe.webp" /></p><p></p><p>不过用户如果想要在&nbsp;Windows&nbsp;10、Windows&nbsp;11&nbsp;上使用&nbsp;Apple&nbsp;Podcast，依然需要借助&nbsp;iTunes&nbsp;应用。</p><p></p><h4>英特尔释出最新微码更新修正&nbsp;13/14&nbsp;代酷睿处理器崩溃问题</h4><p></p><p>9&nbsp;月&nbsp;26&nbsp;日消息，Intel&nbsp;员工在官网社区宣布，已找到第&nbsp;13&nbsp;代和&nbsp;14&nbsp;代酷睿桌面处理器出现不稳定现象的根本原因，并计划推出&nbsp;0x12B&nbsp;版本微代码以进一步修复这一问题。</p><p></p><p>英特尔表示，经过了全面的调查，确定了&nbsp;13&nbsp;和&nbsp;14&nbsp;代酷睿台式机处理器不稳定问题的根本原因，已将问题定位到&nbsp;IA&nbsp;内核内的时钟树电路。其在升高的电压和温度下容易受到可靠性老化的影响，从而导致时钟的占空比偏移和系统不稳定。</p><p></p><p>英特尔称，内部搭载的平台的测试表明，根据缓解措施进行一系列设置后，性能影响在运行差异范围内。英特尔再次强调，第&nbsp;13&nbsp;和&nbsp;14&nbsp;代酷睿移动处理器以及未来的客户端产品（包括代号为&nbsp;Lunar&nbsp;Lake&nbsp;和&nbsp;Arrow&nbsp;Lake&nbsp;系列）均不受最低运行电压偏移不稳定问题的影响。</p><p></p><p>对于所有使用第&nbsp;13&nbsp;和&nbsp;14&nbsp;代酷睿台式机处理器的用户，0x12B&nbsp;微代码更新必须通过&nbsp;BIOS&nbsp;更新加载。目前英特尔正在与其合作伙伴合作，从而推动及时验证和推出针对现有系统的&nbsp;BIOS&nbsp;更新，整个过程可能需要几周的时间。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JUUuFQCazRfT0oECM1b9</id>
            <title>企业大模型应用开发提速！浪潮信息重磅发布元脑企智EPAI一体机</title>
            <link>https://www.infoq.cn/article/JUUuFQCazRfT0oECM1b9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JUUuFQCazRfT0oECM1b9</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 14:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月27-29日，2024中国算力大会在郑州举行。会上，浪潮信息重磅发布元脑企智EPAI一体机，通过软硬件高度协同的一体化设计，为客户提供多元多模、简单易用、本地部署、安全可靠的大模型开发平台，显著提高企业大模型以及AI原生应用的开发效率，加速大模型产业化落地。</p><p></p><h3>大模型落地挑战巨大</h3><p></p><p></p><p>随着大模型和生成式技术的飞跃式发展，基于大模型的应用创新正在成为新的主题，如何以大模型赋能现有的技术、业务，已成为企业在新一轮技术周期中保持竞争力的要素之一。但企业在应用大模型的过程中往往面临着诸多挑战，如生态离散导致的多元多模适配难，模型训练和部署复杂、数据治理难、模型“幻觉”问题以及对专业技术人才的依赖等，导致大模型与行业场景的融合进展缓慢。</p><p></p><p>首先，多元多模适配难度大、周期长、成本高。不同场景、不同业务对模型能力的多样需求，业务生产环境往往是多模并存的状态，而由于芯片生态离散、特定应用加速等原因，所使用的算力资源也是多元并用的。因此，大模型应用开发需考虑与多元算力、多样大模型的选择、适配、组合调度等问题，难度大，周期长，成本高。</p><p></p><p>其次，巨大参数量级的通用大模型，很难直接用于复杂、离散的行业场景，各行业知识专业化程度高，可迁移性低，通用大模型本身难以覆盖，所以经常出现大模型“幻觉”或是“胡言乱语”的问题，因此必须结合行业和企业专业数据进行再学习。</p><p></p><p>最后，要实现大模型与行业场景的深度结合，实现高效、高可靠、高质量的模型应用效果，涉及数据、微调、RAG、部署、上线和运维等极为复杂的流程，特别是数据治理和模型微调，需要具有丰富经验的实施团队才能胜任，技术门槛高。</p><p></p><h3>元脑企智EPAI一体机，让大模型开发快到飞起</h3><p></p><p></p><p>对于大多数企业而言，大模型应用开发的系统性、复杂性，往往让企业对大模型开发望而却步。因此，实现大模型的深入行业应用与广泛落地，关键在于如何有效提高AI应用创新的质量和效率，卓越的大模型及应用开发工具成为释放智能生产力的关键。</p><p></p><p>浪潮信息元脑企智EPAI一体机基于专为大模型应用场景设计的元脑服务器，搭载了元脑企智EPAI企业大模型开发平台，支持多元算力、多模管理、全链工具以及本地部署，可一站式解决数据处理、模型微调、RAG搭建、模型部署、应用上线和系统运维等环节开发难题，为客户提供多元多模、简单易用、本地部署、安全可靠的大模型应用开发平台，满足企业人工智能应用从开发到实施的全栈需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac390264305150a3cdf465c6f5bc2c54.png" /></p><p></p><p>多元多模：元脑企智EPAI一体机具备多元算力和多模管理能力，通过大模型计算框架TensorGlue实现异构算力调度，通过算子基础化技术实现上层模型算法和下层基础设施的逻辑解耦，从而高效的屏蔽模型和芯片差异，降低企业跨算力平台迁移、多元模型部署适配的试错成本。目前，元脑企智EPAI一体机可以支持10+业界主流大模型计算框架，内置7个主流基础大模型，预设了20+微调参数，用户可以针对知识问答、智能编码、文档理解、智能助手等不同应用场景和任务需求，选择最佳产品型号和模型算法，快速开发模型应用。</p><p></p><p>简单易用：元脑企智EPAI一体机提供从数据准备、大模型微调、知识库构建、大模型部署上线运维的全流程支持工具链。其中，针对数据准备，预先内置了上亿条基础知识数据以及自动化数据处理工具，支持10种以上企业常见的数据格式，并且以超过95%的抽取准确率，把这些数据转化为知识库以及可供模型进行微调的数据；针对大模型微调，采用低代码可视化界面来进行微调，并且内置了Lora、SFT等多种微调框架以及20多种优化参数，用户可依据具体业务需求和数据特性，选择最为合适的框架与技术，快速且低成本地构建起企业专属大模型能力。</p><p></p><p>本地部署、安全可靠：由于大模型应用开发需要结合企业私有数据，要求企业数据不出域。本地部署可以确保用户数据不被上传至云端，避免数据泄露和滥用的风险，这在处理敏感信息或符合严格数据保护法规的行业中尤为重要。元脑企智EPAI一体机的本地化部署模式提供全链路的企业数据防护能力，设置多级过滤和审核体系，让数据的流转更安全，让生成结果更可靠，构建起一个既能充分利用数据价值，又能保护用户隐私、符合法规要求的安全数据处理环境，做到“数据可用不可见”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/5188c34c10edde0dc0e02be1dc88bd15.jpeg" /></p><p></p><p>元脑企智EPAI一体机能显著提升大模型应用的开发效率，并极大节省人力成本。据悉，浪潮信息先行先试，采用1台元脑企智EPAI一体机标准版，低代码完成企业知识库构建、模型微调、应用开发等工作，1人1月即简单高效、低门槛地打造出智能售前助手“元小智”，赋能日常售前业务，实现智慧化变革，团队工作效率提升3-5倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f9e44c8a2c8c94cbf2ac1f5c6cec336.gif" /></p><p></p><p>浪潮信息产品方案开发部总经理魏健表示，本次发布了基础版、标准版、高级版、创新版和集群版五个版本，主要面向制造、金融等行业客户、传统ISV和SI三类用户提供一站式大模型生产及应用全流程开发工具链，同时支持接入更多元脑伙伴的算法和模型能力，共同打造AI原生应用开发的“超级工作台”，加速AI应用创新和落地。</p><p></p><h3>广泛赋能企业各类应用场景，提升智能生产力</h3><p></p><p></p><p>智能投标助手：浪潮信息方案开发团队先行先试，采用1台元脑企智一体机标准版，低代码完成企业知识库构建、模型微调、应用开发等工作，1人1月即简单高效、低门槛地打造出智能售前助手“元小智”，赋能日常售前业务，实现智慧化变革，团队工作效率提升3-5倍。解决了招投标数据处理缺少工具、专业知识门槛高、业务效率低下、准确率难以保证、数据防泄漏等问题，大大缩短了应用开发周期，降低了人力成本，提升了投标文件审查效率。</p><p></p><p>智能客服大脑：采用1台元脑企智一体机基础版，基于浪潮信息高效微调 MoE 大模型“源 2.0-M32”构建智能客服平台，将公司内部产品资料构建专属知识库，通过数据抽取、格式转换，生成高质量的微调数据，投喂给“源”大模型。历时 6个月，结合知识蒸馏、压缩等技术，构建浪潮信息的“智能客服大脑”。解决了传统人工智能客服在处理非标准或开放式问题时能力有限、知识库更新难度大、信息过时或不准确等问题，常见问题解决率 80+%，复杂问题处理时长缩短 65%，大大降低售后客服人员压力，人效提升30%。</p><p></p><p>智能编程助手：支持浪潮信息多个研发部门，利用元脑企智一体机开发周期缩短至2天，由 AI 自动生成超过 65%的计算框架代码，整体采纳率 50%，为研发工程师每天节省近3个小时的代码开发时间，大幅提升效率，解决了代码工程可读性差、开发时间长等问题。</p><p></p><p>除此之外，还广泛赋能企业的各类应用场景，例如研发领域的编程助手、个人办公助理、部门助理、操作系统助手、产品设计助手等，开发速度快，低至 1 周；培训周期短，最快 3 天；应用效果好，企业知识驱动；安全可靠，全链路数据安全。</p><p></p><p>行政领域的员工服务、内训讲师、招聘 JD 生成、招聘助手、公文写作等，满足行政部门的多样化需求，提高工作效率。</p><p></p><p>销售领域的智能客服、投标助手、文案写作、产品文档翻译、BI 系统助手等，提升销售环节的服务质量和效率。</p><p></p><p>生产/供应链领域的故障自动识别、维修方案生成、供应链信息系统助手、AI 质检、最优化策略方案等，优化生产和供应链管理。</p><p></p><h3>元脑企智EPAI聚焦大模型新技术，探究更多落地的可能性</h3><p></p><p></p><p>在会后的采访中，魏健解释了浪潮信息推出元脑企智EPAI大模型一体机的背景。当前市场趋势与传统企业应用之间存在巨大差距，浪潮信息希望帮助这些企业快速实现大模型的应用落地。而浪潮信息目标客户群体主要来自这几方面，一是传统制造业客户，这类客户通常拥有丰富的数据和人员资源，并且有应用牵引的趋势。</p><p></p><p>其次是传统ISV（独立软件供应商），例如金融行业的中科软和南天等，他们也非常积极的投入研究大模型应用。最后就是SI（集成商），他们面临的挑战在于服务需求的满足，特别是在大模型调优能力方面。</p><p></p><p>浪潮信息架构师Owen ZHU博士表示，元脑企智EPAI一体机聚焦的是大模型落地，落地应用和基础研发之间是有时差的，基础研发现在聚焦的是模型本身的推理、规划的复杂任务的能力，但是落地需要聚焦在问答场景、知识总结梳理和生成的场景，比如生成报表或者生成文案。</p><p></p><p>元脑企智一体机也应用了很多新的技术，比如说监督微调、人类反馈强化学习等，作为AI应用架构师，Owen ZHU认为这些技术提升了大模型在应用场景的适应性和准确性。大模型从预训练、微调、推理三个阶段展开，现在业界的关注点已经到了微调和推理，微调技术非常复杂、技术门槛高、迭代速度快，浪潮信息AI团队也在尝试各种方法，比如指令微调，或者人类偏好数据微调、高效微调。通过这些高效微调的技术，可以用很小的显存量就能把微调跑起来。也就是说在新技术的加持下能够降低算力门槛。而且目前元脑企智一体机预制的算法、模型都是研究好，可以直接使用的，同时内部配置了20多种参数，这些都是浪潮信息AI团队的经验输出。</p><p></p><p>通过这次采访，我们了解到浪潮信息一体机的推出是为了帮助企业，特别是传统企业，快速实现大模型的应用落地。浪潮信息针对不同的客户群体，提供了定制化的解决方案，并在元脑企智EPAI一体机中集成了多种微调技术，以降低技术门槛和算力需求。同时，浪潮信息也在积极探索新技术，以提升大模型的适应性和准确性。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gqPJLQkCXxfmPzzLsBqf</id>
            <title>Kimi 背后的长文本大模型推理实践：以 KVCache 为中心的分离式推理架构</title>
            <link>https://www.infoq.cn/article/gqPJLQkCXxfmPzzLsBqf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gqPJLQkCXxfmPzzLsBqf</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 12:09:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在不久前举办的 AICon 全球人工智能开发与应用大会上，月之暗面高级研发工程师、开发者关系负责人唐飞虎发表了专题演讲“长文本大模型推理实践——以 KVCache 为中心的分离式推理架构”，分享介绍 Kimi 智能助手背后的推理加速方案，以及该方案在设计时所需要考虑的指标和在真实生产环境中部署的表现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d924dae4adff990f82b86d5d066c2365.jpeg" /></p><p></p><p>在 10 月 18 -19 日即将召开的<a href="https://qcon.infoq.cn/2024/shanghai"> QCon 上海站</a>"上，我们专门策划了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1715">大模型基础设施与算力优化</a>"】专场，并邀请到月之暗面推理系统负责人何蔚然进一步分享 Mooncake 分离式推理架构创新与实践，同时微软亚洲研究院软件开发工程师姜慧强将分享 《长文本 LLMs 推理优化：动态稀疏性算法的应用实践》，还有更多大模型训练推理的一手实践案例尽在本专题。欲了解更多精彩内容，可访问大会官网：<a href="https://qcon.infoq.cn/2024/shanghai/schedule">https://qcon.infoq.cn/2024/shanghai/schedule</a>"</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><p>提到 Kimi，相信在座的各位都有所耳闻。Kimi 智能助手在多个平台上都有入口，包括 Apple Store、微信小程序以及 Web 端，尤其是 Web 端的排名一直居高不下。在日常使用中，尤其是在午间高峰时段，用户可能会遇到 Kimi“累了”的情况。但值得注意的是，尽管用户数量在不断增加，用户体验却得到了显著改善，现在 Kimi“累了”的情况减少了很多，这与我们推理团队的技术攻关是分不开的。</p><p></p><p>今天，我将从四个方面进行介绍。第一部分我将探讨长文本推理的瓶颈问题。随着推理集群的扩大和上下文长度的增加，无论是训练还是推理都面临着更高的要求。我们需要明确瓶颈所在，以便找到解决问题的途径。第二部分将审视目前市面上的推理优化工具和方法，看看有哪些可以为我们所用。第三部分将详细介绍我们的 Mooncake 项目。这是一个以 KVCache（键值缓存）为中心的分离式推理架构，内部代号“Mooncake”由我命名，寓意与“moon”相关，同时“Cake”与“Cache”谐音。在最近几个月，我们也看到了不少类似的方案提出，虽然与我们的设计大同小异，但我将分享一些细节，特别是在面对大量用户时可能遇到的一些独特问题。第四部分，我将讨论上下文缓存的应用。在 SaaS 服务层面，我们为开发者提供了上下文缓存功能。我将具体介绍每位开发者如何利用 Mooncake 方案来优化自己的 AI 应用。</p><p></p><h4>长文本推理的瓶颈</h4><p></p><p></p><p>几个月前，我在 AICon 北京站上讨论了 RAG 与长文本处理的对比。RAG 模型有其优势和劣势，但在长文本处理方面，它有两个显著的劣势：成本高和速度慢。这也是许多开源模型无法良好支持或只能有损支持长文本处理的原因之一。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/09e1b986f7fc281e6321bafab24434ee.png" /></p><p></p><p>成本高的问题是我们的用户群体，特别是 API 开发者在生产环境中经常遇到的一个痛点。他们需要使用我们的 API 对同一个文档，比如一份合同，进行复杂的任务处理，并可能需要反复询问 100 多次或更多。众所周知，大型模型通常采用无状态设计，这意味着每次调用都需要将整个上下文传递进去。随着对话的进行，上下文可能会不断增长，不仅包括上下文本身，还可能包括函数调用、定义以及其他文档设置。每次调用都需要处理这么多信息，因此成本自然会很高。</p><p></p><p>速度慢问题，特别是在第一次处理时，模型的响应速度会特别慢。例如，我们群里的 API 助手在用户频繁提问时，有时会卡住，20～30 秒内都无法产生回复。我们无法确定是卡住了还是其他地方出了问题，但很可能仅仅是因为处理速度慢而导致的卡顿。这是长文本处理速度慢带来的一个副作用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/93b65275a424b9e2a36be3df54e0c12b.png" /></p><p></p><p></p><h5>贵且慢的原因</h5><p></p><p></p><p>长文本推理之所以成本高昂且速度缓慢，原因在于 Transformer 模型在计算 Attention 机制时的工作方式。在没有使用缓存的情况下，每次计算 Attention 都需要进行完整的矩阵乘法，这导致每次迭代的长度以平方级别增加。每当出现一个新的 Query Token，都需要重新计算，这无疑增加了计算的复杂性和时间。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/25/2541134ac7d654f76d8eaff6db6f0147.png" /></p><p></p><p>当我们引入 KVCache 机制后，情况就大为改观。使用 KVCache 后，每次计算的长度只需要线性增加，这意味着我们不再需要重新计算过往的 tokens，从而显著提升了性能。但这种优化也带来了新的问题。</p><p></p><p>为了更直观地理解这一点，我们可以看下面这张图。图表的横坐标表示上下文长度，从左到右逐渐增大；纵坐标则展示了不同的性能度量指标，包括并发数、预填充延迟、解码延迟以及上下文切换时的状态切换延迟。此外，还有 Free HBM Size，它表示可用的高带宽内存大小，随着上下文长度的增加，它可以改善并发性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/24/24c8687b3e30c6a2ed787e50d8040c73.png" /></p><p></p><p>在 Mooncake 论文中，我们使用了 LLaMA2-70B 模型作为例子。从图表中可以明显看到，随着序列长度（即上下文长度）的线性增长，预填充延迟（prefill latency）呈现出超线性增长，这是一个非常显著的趋势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9d/9d4207b661c38416100476c0a365745c.png" /></p><p></p><p>我们做个小结，长文本性能瓶颈主要包括以下几个方面：</p><p></p><p>并发性能：随着上下文长度的增加，并发性能会反比下降；预填充延迟：随着上下文长度的增长，预填充延迟会以平方级别的速度增长；解码延迟和上下文切换延迟：随着上下文长度的增加，解码延迟也会线性增加。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bcea93f9ba48861f8f365d8a8f8d79bb.png" /></p><p></p><p></p><h4>长文本推理的优化</h4><p></p><p></p><p>面对长文本推理中的成本高昂和速度缓慢的问题，我们采取了一系列优化策略。在之前的讨论中，我们已经提到了一些方法，现在我们来总结一下这些策略，并探讨它们如何帮助我们解决长文本推理的挑战。</p><p></p><p>首先，我们考虑了几种优化技术，包括 Flash Attention、vLLM（垂直扩展的大型语言模型）、MOE（Mixture of Experts，专家混合模型）以及最近非常受欢迎的 Speculative Decoding（推测性解码）。此外，还有一些有损策略，例如 Windows Attention，它通过截取部分信息来处理长文本推理，尽管这种方法在资源有限的情况下可行，但最终提供给用户的模型可能是有损的。例如，如果用户询问一个公司上市财报的问题，信息可能分散在文档的不同部分，使用 Windows Attention 策略可能会遗漏一些关键信息。这种信息的遗漏是在训练过程中就已经决定的，后续的 SFT 也无法修复这个问题。虽然这种策略可能在短期内提高模型的上下文指标，但对最终用户体验来说可能是有损害的，而且这种损害在进行大模型基准测试时可能不容易被检测出来。</p><p></p><p>此外，这些策略之间存在不兼容问题。这就需要我们深入了解每个策略的具体实现方式，这会带来一系列复杂的问题，需要从不同的层、头或隐藏层角度进行推理优化。在系统设计时，我们需要选择兼容性最好且效果最佳的策略。</p><p></p><p>我们的 Mooncake 主要是从集群调度的角度进行优化。这种优化与我们之前提到的所有策略基本上是正交的，可以与任何策略组合使用，而不会损失性能。这对于模型基础供应商来说可能是一个高优先级的策略。这也是为什么我们最近看到许多厂商推出了基于 KVCache 优化的方案。这些方案能够提高长文本推理的效率，同时保持用户体验的高质量。</p><p></p><h4>Mooncake 的实践</h4><p></p><p></p><p>上个月，我们在 GitHub 上发布了相关的论文，其中包含了许多细节。在详细介绍之前，让我们先澄清一些基本概念。在大模型推理中，有两个至关重要的阶段：预填充（Prefill）阶段和解码（Decode）阶段。</p><p></p><p>预填充阶段：在这个阶段，每个新 Token 的生成都依赖于之前所有的 Token。由于输入的全部内容（即 Prompt）都是已知的，这个阶段可以进行高度并行化的矩阵操作，有效提高 GPU 的利用率。这个阶段对“首次 Token 时间”（Time to First Token, TTFT）有显著影响，对于流式应用来说，如 GPT-4o 或 AI 陪伴类实时交互应用，这个指标尤为重要。解码阶段：与预填充阶段不同，解码阶段的 Token 不是一次性全部生成的，而是逐段、逐词生成的。这个过程会一直持续到满足某个停止条件，这些条件可能是模型的最大上下文限制、用户设置的上下文上限，或者是其他预设的停止条件，比如输出了一个 JSON 的终止符。在这个阶段，每个顺序输出的 Token 都需要知道之前所有迭代的输出状态的 KV 对。这涉及到矩阵中的向量运算，与预填充阶段相比，解码阶段无法充分利用 GPU 的计算能力。数据从内存传输到 GPU 的速度决定了这个阶段的延迟，而不是计算本身的速度。换句话说，解码阶段主要受内存传输速度的限制，这个阶段主要影响的是“每个输出 Token 的时间”（Time per Output Token, TPOT），对于流式应用来说，这个指标同样敏感，并且对总体推理时间有较大影响。</p><p></p><p></p><h5>Mooncake 的基本思想</h5><p></p><p></p><p>Mooncake 的核心理念是将模型推理过程中的两个截然不同的优化阶段分开处理，因为这两个阶段的优化目标和受限的瓶颈各不相同。这种分离式处理方法是一种直观且自然的思路。具体来说，Mooncake 采用了以 KVCache 为中心的分离式推理架构，主要由三个核心部分组成：</p><p></p><p>Prefill 池：这个部分负责集中管理所有的预填充阶段的计算任务。Decoding 池：这个部分集中处理所有解码阶段的任务。KVCache 池：这个部分负责存储所有中间过程中应用到的 KVCache，并决定何时使用这些缓存，何时释放它们。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ad/ad082105929e01fea1cd226800df6a01.png" /></p><p></p><p></p><h5>Mooncake 的开发动机</h5><p></p><p></p><p>现有的大型语言模型（LLM）服务系统，例如垂直扩展的大型语言模型（vLLM），通常将预填充（Prefill）和解码（Decode）阶段放在同一个 GPU 上处理。这种设计在实际操作中会遇到一些问题：</p><p></p><p>TTFT 和 TPOT 优化不可兼得：预填充阶段处理的文档通常非常长，并且随着多轮对话的进行，上下文长度会不断增加，导致预填充比解码耗时更长。在需要作出决策先执行哪个阶段时，vLLM 的做法是让解码阶段暂停，以便预填充阶段先行。这会导致 TPOT（每个输出 Token 的时间）增加。反之，如果优先解码，TTFT（首次 Token 时间）也会增加。GPU 资源竞争：即使预填充和解码阶段被单独调度，它们仍然会竞争 GPU 资源，导致等待时间增加。在单机情况下，很难设计出一个高效的调度策略来解决这种资源竞争问题。Prefill 和 Decode 的瓶颈不同：预填充阶段主要消耗计算资源，而解码阶段则更依赖于内存和带宽。目前还没有既擅长计算又擅长内存和带宽的芯片，因此，使用具有不同特点的 GPU 分别处理预填充和解码，可能更有利于资源的利用和成本的节约。</p><p></p><p>这种分离设计的思路在计算机体系结构领域并不新鲜。例如，区块链系统 EOS 在设计时就采用了类似的分离策略，将带宽和 CPU 计算分开处理，各自形成一个资源池。受到这种设计思路的启发，Mooncake 采用了典型的分离式架构，将单个同构 GPU 集群的资源打散并重新组织成三个可以独立弹性伸缩的资源池 —— Prefill Pool、Decode Pool、KVCache Pool。</p><p></p><p></p><h5>Mooncake 的分离式架构</h5><p></p><p></p><p>在 Mooncake 架构中，Prefill Pool 中的一个具体实例负责处理特定的任务。下图的工作流程可以这样理解：</p><p></p><p>KVCache 的重用（Reuse）：在下图左侧黄色区域，我们可以看到处理过程中可能会涉及到 KVCache 的重用。这部分 KVCache 可能已经存在，它包含了之前迭代中计算得到的数据，可以在新的迭代中被再次利用，从而提高效率。增量 KVCache（Incremental KVCache）：在下图左侧粉色部分，是处理新的输入所生成的增量 KVCache。用户在每一轮对话中都可能提供新的输入，这些输入需要被处理并生成新的 KVCache，作为当前轮次迭代的结果。流量式传输（Traffic-style Transfer）：计算完成后，这些 KVCache 会以一种流量式的方式传输，即连续不断地传递给解码阶段的实例。解码实例（Decoding Instance）的任务：接收到 Prefill Pool 传递过来的 KVCache 后，解码实例的任务就相对简单了。它主要负责根据这些 KVCache 进行解码操作，生成最终的输出结果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/27/27f2ac2ac220f83eac6f105d9d53cf94.png" /></p><p></p><p>在 Mooncake 架构中，KVCache 资源池扮演着至关重要的角色，尤其是在线上服务中，每天需要应对数百万用户的查询。在一些热点事件，如奥运会期间，用户可能会频繁询问类似的问题，比如中国队当天获得的金牌数量或者乒乓球男团的晋级情况。面对这样的高频查询，我们可以利用 KVCache 资源池来优化处理。</p><p></p><p>一个自然的想法是使用哈希存储来管理这些 KVCache。不同的厂商可能会采用不同的策略，例如有些可能会选择使用 Trie 树。Trie 树在计算复杂度上与模型的词表大小有关，如果模型的词表发生变化，比如从 GPT-4 升级到 GPT-4o 时词表扩大了，Trie 树的性能可能会受到影响。</p><p></p><p>为了避免这种复杂性并提高效率，我们选择了哈希存储。哈希存储方法简单、速度快，并且不受词表大小变化的影响。这样，无论用户的查询如何变化，我们都能保证 KVCache 资源池的高效运作。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/06/06b2d7c184fdb4b2fc3f0aa6667b8987.png" /></p><p></p><p></p><h5>Mooncake 效果展示</h5><p></p><p></p><p>在实施了三个独立的资源池之后，我们进一步观察了线上生产环境中的实际运行情况。我们发现，Prefill 和 Decoding 阶段对资源的占用呈现出一种波浪型模式，类似于潮汐的涨落，每天都有规律地变化，尽管看起来似乎没有明显的规律。然而，当我们仔细观察并分析每天线上数百万用户的行为时，这些行为模式变得可以预测。基于这些可预测的行为模式，我们可以采取类似于 vLLM 中的策略，即在资源紧张时暂停解码阶段，让预填充阶段先行。在集群调度中，我们也可以应用类似的逻辑。我们根据线上生产环境的日常运行数据，设计了一种基于动态规划的调度策略。这种策略能够提前准备适量的预填充和解码资源，以使资源占用的波动曲线更加平缓。通过这种动态规划的调度策略，我们能够更有效地管理资源，减少资源浪费，并确保服务的稳定性和响应速度。这样的调度提升了用户体验，因为我们能够更好地应对用户需求的高峰和低谷，确保服务始终如一地流畅运行。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a0976b0773270fc18eb907e7c8aab276.png" /></p><p></p><p>在下面展示的图表中，蓝色线条代表我们的 Mooncake 架构，而黄色线条则代表与之对比的 vLLM 架构。我们可以观察到两种架构在首次 Token 时间和 Token 间时间上的表现差异。</p><p></p><p>首次 Token 时间：在首次 Token 时间上，Mooncake 架构相较于 vLLM 架构有轻微的优化。这主要是因为引入了 KVCache，它能够存储和重用之前的计算结果，从而加快了预填充阶段的处理速度。Token 间时间：在 Token 间时间上，Mooncake 架构的改善非常明显。这是因为 Mooncake 将解码阶段单独分配给一个资源池来处理，这样的分工使得解码过程的效率显著提高。由于解码阶段不再与其他任务共享资源，它可以更加专注和高效地生成每个后续的 Token。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/6203884251c91fed2dd7317d1755f458.png" /></p><p></p><p>下图展示了一些具体的实验数据，这些数据并非来自线上环境，而是使用了开源数据集。我们特别关注了 ArXiv 上每天发布的论文摘要任务。许多用户每天都会使用 Kimi 智能助手来总结文章，包括学术论文，以及一些第三方应用基于 Kimi 大模型开发的论文摘要工具。在这些实验中，我们比较了 Mooncake 架构和单机 vLLM 架构在处理 ArXiv 数据集时的性能。实验结果显示，Mooncake 在处理这些摘要任务时相较于单机 vLLM 有明显的优势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d6/d6d617bf11ea3efff95c7fafc50fe884.png" /></p><p></p><p></p><h4>上下文缓存的应用</h4><p></p><p></p><h5>Context Caching 基本原理</h5><p></p><p></p><p>在传统的 LLM 交互中，每次用户与模型的对话都需要重新计算整个上下文，这包括了所有的历史信息和对话内容。这种方法意味着每次交互都需要为整个上下文支付计算费用，无论上下文中有多少信息是重复的。</p><p></p><p>上下文缓存的核心改进在于引入了一个“公共上下文”的概念。这个公共上下文包含了对话中不变的部分，比如背景信息或常见问题。通过缓存这个公共前缀，我们只需要为其支付一次计算费用，而不必在每次交互时重复支付。这样，每次交互的成本就大大降低了，用户只需要为每次的增量输入（即新的对话内容）以及存储公共上下文的费用付费。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/da/dac6dea61ab77dc78beaa6a6f593ac0f.png" /></p><p></p><p></p><h5>Context Caching 使用流程</h5><p></p><p></p><p>使用 Context Caching 的流程非常简洁明了，主要分为以下几个步骤：</p><p></p><p>创建缓存：首先，你需要创建一个缓存实例。这个过程通常非常快速，大约需要 30 到 40 秒的时间。一旦缓存创建完成，它就可以被用于后续的交互。使用缓存：创建缓存后，你可以直接在对话或应用中使用它。由于缓存已经包含了必要的上下文信息，因此可以避免重复计算，提高响应速度和效率。</p><p></p><p>为了帮助开发者更容易地实现上下文缓存，我们在官方 GitHub 上提供了一些示例代码，这些代码覆盖了多种编程语言，包括 Python、Node.js 等，以便开发者能够快速上手并集成到自己的项目中。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/13/13bbcaf7294657c909e6c81b8cd4a0eb.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0c/0cbbe7bf969a2e8166730af19aeec38a.png" /></p><p></p><h5>Context Caching 收费模式</h5><p></p><p></p><p>我们的上下文缓存（Context Caching）技术的收费模式已经进行了优化和调整。现在，创建缓存的成本非常低，而且是一次性的费用。调用缓存的费用也几乎可以忽略不计，主要的成本瓶颈在于存储空间的费用。为了鼓励更多开发者使用这项技术，我们最近对价格进行了调整，降价幅度达到了 50%。原先的费用是 10 元，现在降低到了 5 元。我们希望通过这样的降价措施，能够激励开发者更广泛地采用上下文缓存技术。</p><p></p><p>在最近的“GOGC 黑客松”和之前的“Adventure X 黑客松”中，我们特别表彰了使用上下文缓存 API 最多的开发者，以此鼓励大家探索和利用这项技术。我们注意到，尽管上下文缓存技术能带来便利和性能优化，但仍有许多开发者没有充分利用它。例如，之前有一个非常受欢迎的应用，它在短时间内有大量的调用。如果该应用当时使用了上下文缓存技术，开发者本可以节省一大笔开支。实际上，我们与开发者进行调研后发现，由于大模型产生的费用过高，他们不得不调整其提示词，尽可能简化内容，甚至删除了一些原本设计的游戏玩法、规则和元素，这无疑是一种遗憾。如果现在有类似的爆款应用出现，它们完全可以利用上下文缓存技术来提升用户体验并有效控制成本。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b2/b2bd1881c1d36491dfdff6e094064a4d.png" /></p><p></p><p></p><h5>Context Caching 应用技巧</h5><p></p><p></p><p>我们的官方 API 小助手的线上实际调用情况可以通过下图进行展示。目前，我们大约有 20 个开发者社群，每个社群的每小时调用情况都能清晰地反映出来。从图中可以看出，在每天凌晨 2 点到早上 8 点这段时间，社群内几乎没有活动，因为大家都在休息。一旦进入白天的工作时段，尤其是在早上、中午以及晚上下班后的时间段，社群内的活动会显著增加，这也是我们的 API 小助手最为忙碌的时段。</p><p></p><p>对于不同的开发者来说，他们的应用类型可能会影响调用频率的曲线。例如，如果应用是工具类或生产力类的，那么在工作日的白天可能会有更多的调用。相反，如果应用更偏向于娱乐或游戏，那么在晚上和周末的调用可能会更加频繁。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c5b6499fb98f470baf991885b1c51a6b.png" /></p><p></p><p>具体到我们小助手的情况：</p><p></p><p>在上午 9 点时，我们需要在调用 / chat/ completions 接口时，在 Headers 中添加 x-Msh-context-Cache 以启用 Cache， 同时添加 X-Msh-context-Cache-Reset-TTLHeader 以更新 Cache 存活期，这里以小助手为例，存活期为 3600s，即 1 小时；由于我们要在凌晨。点结束 Cache，因此夜间 23 点是我们最后一次刷新 Cache 存活期的时点，在此之后，我们需要移除 Headers 中的 x-Msh-context-cache-Reset-TTL 参数，以保证 Cache 能在 0 点被顺利移除；</p><p></p><p>以 Python 代码为例，大致的代码逻辑为：</p><p></p><p>我们具体分析了一天中的数据，发现通过在特定时间点，如上午 9 点和晚上 24 点，存储上下文缓存，可以显著降低费用消耗，大约能节省 3/4 的成本。这种策略对于那些像我们的 API 小助手这样需要频繁交互的应用场景尤其有效。如果你的应用也属于这种类型，那么上下文缓存技术将是一个非常值得尝试的优化手段。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/667e7cf081596bcaa2f1f1ab02dd1bdf.png" /></p><p></p><p>目前，我们也观察到市场上其他一些解决方案，它们通过不同的方式处理上下文缓存的复杂性。一些系统选择将这些复杂操作隐藏起来，让用户无需手动管理缓存的存储和删除，而是将这些任务完全交给调度引擎，由它智能地进行资源分配和优化。我认为手动管理和自动调度各有优势，手动管理提供了更多的灵活性和控制权，而自动调度则简化了操作流程，降低了用户的使用门槛。未来，我们可能会看到这两种方法的融合，以满足不同用户的需求和偏好。</p><p></p><h5>Context Caching 适用场景</h5><p></p><p></p><p>上下文缓存技术的应用场景非常广泛，尤其适合那些频繁进行请求并且需要重复引用大量初始上下文信息的场景。在这些情况下，上下文缓存能够显著提升处理效率，主要表现在缩短首次 Token 生成的时间，同时大幅降低 Token 消耗的费用。在我们的线上生产环境中，随着 Mooncake 架构从最初的灰度测试到现在的全面部署，Kimi 智能助手能够每天处理的请求量增加了 75%。这也是为什么用户最近感觉到“Kimi 累了”的情况有所减少的原因之一。</p><p></p><h4>参考资料</h4><p></p><p></p><p>为了帮助大家更深入地了解上下文缓存及相关技术，我们附上了一些参考资料。特别推荐其中一篇由爱丁堡大学的傅瑶撰写的基础总结，该文详细介绍了长上下文大型模型推理优化技术，包括最新的进展和其他技术的对比分析，以及哪些技术可以组合使用等。</p><p></p><p>Mooncake: A KVCache-centric Disaggregated Architecture for LLM ServingData Engineering for Scaling Language Models to 128K ContextLarge Lanquage Model Based Long Context Modeling Papers and BloasFull Stack Transformer Inference Optimization Season 2: Deploying Long-Context Models</p><p></p><p></p><h5>演讲嘉宾介绍</h5><p></p><p></p><p>唐飞虎，月之暗面高级研发工程师、开发者关系负责人。前谷歌工程师、ACM/ICPC 亚洲赛区金牌、微软编程之美挑战赛冠军、第一届万向实验室通证经济设计大赛冠军。</p><p></p><p></p><h5>会议推荐</h5><p></p><p></p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，<a href="https://qcon.infoq.cn/2024/shanghai">QCon </a>"还是太全面了，报名详情请联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/68/68a4f559d6682dec46bd5633588299f0.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0PTVDFjuaDNIeTGeAswa</id>
            <title>万字长文解读百度大模型原生安全构建之路</title>
            <link>https://www.infoq.cn/article/0PTVDFjuaDNIeTGeAswa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0PTVDFjuaDNIeTGeAswa</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 10:46:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型以其更智能、不确定和不可解释的特点，给安全领域，尤其是内容安全带来了更大的挑战。百度在打造文心一言之初就意识到，传统的内容审核技术无法从根本上满足大模型内容安全的需求。因此，我们必须从头开始构建一套全新的方法。</p><p></p><p>在不久前举办的 AICon 全球人工智能开发与应用大会上，百度安全平台副总经理冯景辉发表了专题演讲“百度大模型原生安全构建之路”， 分享聚焦于百度在过去两年百度安全平台团队在大模型内容安全领域遭遇的挑战和问题，以及团队尝试过的解决思路和应对方法，涵盖数据清洗、内生安全与安全对齐、安全围栏建设，以及应用安全与基础模型安全等方面。</p><p></p><p>我们将在 10 月 18 -19 日 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 上海站</a>"【<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">探索安全边界：出海合规与大模型实践</a>"】专场，探讨大模型如何帮助团队成员更高效地解决安全问题。百度杰出架构师、安全技术委员会主席包沉浮将分享百度基于大模型安全运营的质效提升实践。欲了解更多内容，可访问大会官网：<a href="https://qcon.infoq.cn/2024/shanghai/schedule">https://qcon.infoq.cn/2024/shanghai/schedule</a>"</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><p>今天，我想与大家分享百度在过去两年中，如何在开发大模型的过程中确保安全性的故事。早期当大模型遇到敏感问题时，，它通常会建议我们换个话题。在过去两年里，我们一直在不断优化，解决模型生成过程中出现的各类安全性问题，同时也在提升用户体验。在下面的图中，我们可以看到，即使是其他公司的模型也经常会遇到需要用户重新提问或直接拒绝回答的情况。然而，最近在使用文心一言时，我们发现它已经开始用更积极的角度引导用户正确看待敏感问题，这在很大程度上改善了用户体验。</p><p></p><p>今天，我将讨论四个方面的问题。首先，我们会回顾一下大型模型面临的安全挑战。接着，我们将探讨我们是如何逐步演进，以确保大模型的安全性。然后，我们将深入讨论今天的主题——原生安全之路。我们将解释什么是原生安全，以及我们是如何实现它的。最后，会简要介绍一些我们最近在智能体和 agent 安全领域遇到的课题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a4/a47f73b186a192d15c1595721d607301.png" /></p><p></p><p></p><h4>大模型安全的挑战</h4><p></p><p></p><p>大模型的安全性挑战贯穿其整个生命周期，我们将其与安全相关的部分分为三个阶段：训练阶段、部署阶段和运营阶段。</p><p></p><p>在训练阶段，确保训练数据的安全至关重要。我们需要采取措施来保护数据不被泄露或滥用，因为这些数据往往是模型学习的基础，并且可能包含敏感信息。</p><p></p><p>进入部署阶段，我们面临的挑战是如何在模型部署和推理过程中保护模型参数和文件不被泄露。这包括确保模型文件在存储和传输过程中的安全性，以及在运行时防止未授权的访问。</p><p></p><p>最后，也是今天讨论的重点，是在运营阶段我们会遇到的问题。这个阶段涉及模型与用户交互的安全性，包括但不限于防止恶意输入、处理敏感请求以及确保用户数据的隐私保护。在这一阶段，我们需要不断地监控和更新模型，以应对新出现的安全威胁和挑战。</p><p></p><p></p><h5>大模型训练阶段的安全挑战</h5><p></p><p></p><p>在大模型的训练阶段，我们面临的安全挑战主要涉及训练数据的选择、数据的血缘分析以及模型质量的评估。首先，训练数据的选择至关重要，因为它不仅决定了模型的性能，还影响着模型的安全性。我们希望模型能够提供正确价值观的回答，同时保持创新性和多样性。因此，在数据选择时，我们需要清洗掉不安全的内容，保留不同的观点和数据。</p><p></p><p>其次，数据的血缘分析也是合规要求的一部分，我们需要对数据来源进行清晰限定。这意味着我们采纳的数据需要进行详细的血缘分析，以确保其合法性和安全性。</p><p></p><p>再者，模型质量的评估在数据清洗后变得尤为重要。我们需要确保模型在经过数据清洗后，其质量仍然与数据训练质量正相关。在数据清洗方面，我们需要去除不良价值观的内容，删除个人信息和敏感信息，以及处理涉及商业侵权的信息。</p><p></p><p></p><h5>大模型训练与部署阶段的安全挑战</h5><p></p><p></p><p>在训练与部署阶段，我们面临的挑战包括如何保护模型文件和数据文件在流转和传输过程中的安全。由于许多数据文件存储在云训练平台上，企业内部人员可能拥有访问权限，因此，我们需要确保训练数据和模型参数文件在这一过程中不被泄露、篡改或删除。</p><p></p><p>为了应对这些挑战，我们需要一套解决方案，确保数据从训练开始就是密态存储，直到模型内部能够原生支持加载密态文件。同时，我们还需要通过完整性校验来发现模型文件的任何缺失或修改情况。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bf26b10bd32cea1758d9f816e8ddc228.png" /></p><p></p><p></p><h5>大模型业务运营阶段的安全挑战</h5><p></p><p></p><p>在大模型的业务运营阶段，我们面临的安全挑战不仅限于传统内容安全中的黄反类信息，还包括一些特定于大模型的重点问题。这些挑战包括注入型攻击，即通过伪造特定环境或指令，试图使大模型突破其原有的安全限制，输出不应泄露的信息。此外，随着多轮对话窗口的增加，大模型的能力得到扩展，但同时也引入了更多的安全风险。这包括主语指代问题，以及引入多模态内容（如网页、文档、图片、音视频）时增加的风险。</p><p></p><p>为了说明这些概念，我们分享一些有趣的故事。例如，“奶奶越狱”的故事，这是一个经典的例子，展示了如何通过巧妙的提问使大模型泄露信息。在这个故事中，通过询问大模型关于 Windows 序列号的问题，试图诱导其泄露信息。今天，注入型攻击不仅限于此类情况，还可能包括其他场景。例如，当直接询问大模型关于某城市不良场所的位置时，大模型通常会拒绝回答。但如果我们换个方式问，比如询问带孩子旅游时应避免哪些区域，大模型的安全对齐机制可能会被绕过，从而泄露原本不应提供的信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/98/980da0d234c46e577a0512e2ee02787a.png" /></p><p></p><p>第二个例子涉及到梯度攻击，这是一种在早期视觉领域模型中，尤其是在无人驾驶和道路识别技术中被广泛讨论的攻击方式。大约在 10 年前，甚至更早，人们通过自动化方法寻找能够干扰图像识别的因子。例如，有人通过修改限速标志，将限速 40 公里 / 小时的标志改为限速 120 公里 / 小时，尽管人类视觉上仍然识别为 40 公里 / 小时，但机器却可能将其识别为 120 公里 / 小时，从而引发安全隐患。</p><p></p><p>这种攻击方法在大模型中也逐渐被发现。通过自动化的方式，我们称之为“魔法后缀”的技术，可以在不添加任何有意义字符的情况下，仅仅通过在 prompt 后加上特定的后缀，就能让大模型输出原本不应输出的有害信息，比如制造炸弹的方法。</p><p></p><p>在多模态输入的情况下，大模型的安全问题变得更加复杂。通常是在训练数据阶段可能没有进行有效的清洗，同时在安全对齐阶段存在疏漏。在单一模态下，尤其是在自然语言处理领域，大多数中文大模型已经较好地处理了安全对齐问题。但是，当引入多模态输入后，由于多模态数据需要将不同模态的数据映射到同一模态的向量，这一过程中的安全对齐层可能没有与自然语言的安全对齐完全一致，从而导致了安全问题的出现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/936670cc8597f35bc4ca29538a6fe7f5.png" /></p><p></p><p></p><h4>大模型安全的演进之路</h4><p></p><p></p><h5>大模型内容安全的技术选择</h5><p></p><p></p><p>在探讨大模型内容安全的技术和策略时，我们关注了一系列问题。最初，我们希望通过安全对齐来解决大模型的安全问题。随着生成式人工智能的兴起，大模型的能力变得前所未有的强大，能够处理以前无法完成的任务。然而，我们很快发现，仅靠安全对齐并不能在训练阶段和微调阶段就确保大模型的安全性。这是因为安全问题不断演变，而大模型的训练周期很长，无法快速响应新的安全威胁。</p><p></p><p>因此，我们开始考虑引入传统内容安全技术。百度作为互联网企业，已经研发了自己的内容审核技术，用于 PGC 和 UGC 内容的审核。我们考虑是否能够通过这些技术来覆盖大模型的内容安全。但很快我们发现，大模型有其独特的挑战，如多模态输入和多轮会话，这些在传统内容审核中并不常见。此外，内容审核可以有时间上的灵活性，例如发文审核可以进入队列等待，但大模型的 prompt 审核却不能这样做，因为用户期望在几秒钟内就得到响应。</p><p></p><p>基于这些考虑，我们放弃了依赖传统内容审核技术的方案，转而进入了第三个阶段，即原生安全。所谓原生安全，是指我们在安全性设计之初就放弃了完全沿用的内容审核技术的思路，转而构建了一套新的方案。这套方案首先将多轮会话纳入模型的 Prompt 和输出结果中，使得安全内容的过滤和分析能够考虑到会话状态。其次，我们引入了提问意图这一概念，关注用户提问的恶意性和他们寻求的答案类型。这通常需要通过 prompt 改写来处理用户的问题，这是大模型领域常用的优化方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f96b7d9cb44d178855d554ffc9d694f8.png" /></p><p></p><p>此外，我们还应用了 RAG 技术和代答模型。代答模型是指用一个小模型来回答敏感的安全问题，而不是完全依赖大模型。这样做的好处是多方面的。结合 RAG 技术，我们形成了一套基于生成式内容的原生安全方案，与底层的安全对齐相结合，构成了我们今天讨论的原生安全策略。</p><p></p><h5>为什么只做安全对齐不行</h5><p></p><p></p><p>仅依靠安全对齐是不够的，原因有几点。首先，安全对齐通常在 SFT 或人力反馈的强化学习阶段进行，这个阶段对于价值观类问题比较有效，比如避免红灯区、不赌博、不进行人身攻击等。然而，对于政治敏感性问题、领土完整等具有明确观点性和事实性的问题，安全对齐阶段处理起来就不太有效。这些问题不仅具有极强的专业性，而且具有时效性。这与价值观类问题不同，价值观类问题相对恒定不变，更容易在对齐阶段一次性解决。此外，安全对齐需要及时更新以应对每天从政府、媒体、舆论和海外传来的风险舆情，而重新训练安全模型需要大量时间成本，因此我们需要一种外挂式的方式来实现及时更新。</p><p></p><p>为什么内容审核技术也不行</p><p></p><p>至于为什么传统的内容审核技术也不行，主要问题之一是多轮会话的处理。例如，用户可能会问“香港是哪个国家的”，模型会提供香港的历史和回归中国的故事。然后用户可能会基于这个答案提出更多问题。但是，如果有人恶意构造问题，他们可能会利用输出的内容来引导发现更多的问题。在单一的 Prompt 回合中，可能不存在任何默认的敏感词，但多轮会话对传统内容审核技术构成挑战，因为它需要具备会话处理能力。此外，大模型的越狱技术越来越多地采用情景设定，但它会干扰模型回答内容的质量和安全边界。这些情况通常不会被传统内容审核技术关注到。再加上模型本身的不可解释性，我们很难通过一个具体案例去追溯安全对齐或安全问题上出现的问题，需要通过数据的飞轮不断迭代，才能逐步提升安全性。</p><p></p><p></p><h5>关注准确率</h5><p></p><p></p><p>随着长文本处理的需求日益增长，大模型现在能够处理的文本长度已经从 8K 起步，甚至有些模型可以处理长达 300K 的文本窗口，这使得我们可以将整本书的内容输入到大模型中。在这样的背景下，长文本的准确率变得尤为重要，不再仅仅局限于 200 或 500 个 token 的语境。长文本语境中容易出现误报，尤其是在带有特定场景的输入安全方面。</p><p></p><p>在讨论安全问题时，我们通常关注召回率，即模型能够识别出多少潜在的安全问题。但在实际的生产环境中，准确率问题更为常见，因为误报会在安全实施中造成困扰。例如，一个社区民警希望生成一个反诈骗提示，可能会使用一些常见的宣传词汇，如“以小博大”、“六合彩”等。如果模型没有很好地理解这些语义，就可能错误地将这些内容标记为安全问题，从而产生误报。这种误报在生产环境中是需要避免的，因为它会影响安全措施的有效性和用户体验。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/60/60926f3c9d6667b0d382bb380c284cce.png" /></p><p></p><p></p><h4>开始原生安全之路</h4><p></p><p></p><h5>原生安全四要素</h5><p></p><p></p><p>在构建大模型的原生安全体系时，我们认为需要关注四个关键要素：</p><p></p><p>数据清洗：数据是构建安全体系的基础。必须确保数据在输入模型之前经过彻底的清洗和筛选，以排除任何可能引发安全问题的不良内容。安全围栏：这是一个快速响应机制，用于补齐安全漏洞。它需要结合内部的基础模型安全对齐和外部的快速反应能力，以确保在面对新出现的安全威胁时能够迅速采取措施。安全对齐：在安全对齐阶段，重点是提升模型的基础安全能力。通过加强这一环节，可以减轻安全围栏的压力，因为模型本身能够更好地识别和处理潜在的安全问题。持续评估：由于安全事件层出不穷，需要持续运营和监控。在安全事件发生时，能够迅速反应并通过安全围栏进行补齐，形成一个快速迭代的过程。</p><p></p><p>这不是一次性的数据流程，而是一个周期性的循环过程。在这个循环中，通过持续评估发现的问题，不断通过安全围栏和数据清洗进行补齐，并在模型的下一轮迭代中提高安全对齐能力，从而形成一个持续提升的安全循环体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/38/38c3aa5511ddb79830773efe90bf1394.png" /></p><p></p><p>在进行数据清洗时，我们遵循国家相关法律法规的要求，特别是生成式人工智能管理的暂行办法及其实施条例。</p><p></p><p>数据清洗的第一步是对数据质量和安全性进行评估。在训练数据输入模型之前，训练团队需要评估数据的质量，而安全团队则负责评估数据的安全性。安全性评估包括确定数据来源，比如是否来自合法的 PGC 组织、UGC 民间数据，或是海外数据，并分析其中可能存在的风险比例。</p><p></p><p>第二步是去除数据中的脱敏隐私内容，包括个人信息和隐私信息，如身份证号、电话号码、家庭住址等，确保这些信息被彻底脱敏。</p><p></p><p>第三步是根据规范要求删除不合规、不合法的数据内容，并在删除后保持语义的通顺和语境的完整性。经过这一轮修剪和删除，可能有近 50% 的数据被清洗掉。</p><p></p><p>最后，我们需要对清洗后的数据集进行完整性评估，确保数据集仍然可用。如果评估结果显示数据集质量仍然符合要求，那么数据清洗过程就完成了，数据可以进入下一步的训练流程。</p><p></p><p></p><h5>百度的解决方案</h5><p></p><p></p><p>百度的内容安全解决方案是一个综合性的体系，它由几个关键部分组成：</p><p></p><p>数据清洗：这是解决方案的基础，涉及我们之前讨论的对训练数据进行质量和安全性评估的过程。这包括对数据来源的分析、去除敏感信息、删除不合规内容，并确保数据集在清洗后仍然保持完整性和可用性。大模型防火墙：也称为安全围栏，它的功能是进行语义干预，快速响应新发现的安全问题，通过设置快速止损机制来阻拦潜在的安全威胁。它还能够处理多轮会话，以会话（session）为单位进行内容识别，并通过意图分析来规划执行路径。检索增强和代答模型：这是解决方案的核心，包括使用 RAG 技术来增强模型的检索能力，以及使用代答模型来规避风险问题，引导模型给出安全的回答。基础能力：百度的自然语言处理、视觉和语音相关的安全模型都基于文心大模型，这些是构成解决方案的技术底座。安全评估：为了实现持续运营，解决方案包括线上问题的持续发现和改进，以及在模型每个版本迭代过程中进行不断的回归测试和评估。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8e/8e647041f53ee763fdc879c38a45ccd2.png" /></p><p></p><p></p><h5>安全围栏对抗性防御架构</h5><p></p><p></p><p>百度的安全围栏对抗性防御架构是一个多步骤的流程，旨在确保大模型的安全性和可靠性。这个架构大致分为五个步骤：</p><p></p><p>多轮改写：在多轮会话中，通过改写的方式处理指代性词汇，如“他”、“前一个”、“前文所指”等，确保语义的准确性和完整性。这样，即使脱离上下文，单独查看和审核语句时，也能准确理解其真实含义。大模型防火墙：在这一步骤中，通过快速止损机制来发现和干预敏感风险点。这通常涉及到传统的语义干预和查询匹配技术，以快速识别和处理潜在的安全问题。必答知识库，代答模型：在大模型中构建知识库，并利用检索增强技术在安全语料范围内构建 RAG 条目。目前，百度拥有大约五六千万规模的 RAG 条目，覆盖了基本的敏感话题。这些条目引导至专门为安全训练的小型代答模型中。模型输出过滤：即使在输入阶段已经实施了各种安全策略，输出阶段仍然不能忽视。在这一阶段，需要对输出内容进行完整性分析，以发现可能出现问题的点。这是因为即使经过了输入阶段的处理，大模型在输出阶段仍可能产生有害的风险性内容。内容审核：由于安全问题的复杂性，即使是经过重重防御，也很难做到 100% 的安全保障。即便经过了输入、处理和输出的一系列安全措施，我们仍然建议在最后一步引入人工判定。通过离线的审核、追溯和巡查机制，我们可以发现并处理在前四个步骤中未能发现或阻止的问题。这样的人工介入有助于形成持续的迭代过程，将发现的问题反馈到下一轮的安全循环中，从而不断提高大模型的安全性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/717a4383e7408ccefd97153e370f9ab5.png" /></p><p></p><p>我们的安全围栏的数据流是一个复杂的过程，它从文本输入开始，然后经过多个环节来确保内容的安全性。首先，系统需要识别输入文本的语言，区分它是中文、英文、中英文混杂还是其他语种，因为不同语种需要采取不同的处理策略。接下来，文本可能会经过改写，以消除歧义或潜在的安全问题，但具体细节在此不展开。文本经过改写后，会进行意图分析，以确定用户的真实意图。例如，如果分析发现文本可能涉及领土完整的负面攻击性问题，系统会将其引导至安全模型进行处理。安全模型会利用基于安全语料构建的 RAG 数据，这些数据覆盖了基本的敏感话题，以确保回复内容的安全性。RAG 数据随后被送入专门为安全训练的代答模型中，生成回复内容。如果文本在意图分析阶段被识别为具有较强攻击性，如涉及领导人的攻击性分类，可能会被标记为不上屏，即不直接显示给用户。我们越来越多地采用正面引导的方式，而不是直接拒答，这是通过检索增强和代答模型实现的，将安全风险性问题引导至代答模型中进行准确引导和回答。</p><p></p><p>在整个过程中，系统需要关注多种潜在的安全威胁，包括但不限于：</p><p></p><p>使用繁体中文试图绕过安全检查的尝试。中英文混合文本中夹杂的不安全问题。通过多轮对话中的指代方式诱导产生问题的尝试。尝试通过编码指令或其他高级攻击手段绕过安全机制。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d6/d65a0944a92ee58c7920e9c8115fa23c.png" /></p><p></p><p></p><h5>安全对齐</h5><p></p><p></p><p>虽然安全对齐不是本次分享的主要议题，但我们可以简要总结百度在安全对齐方面的一些最佳实践：</p><p></p><p>有监督精调：在大模型训练中，无论是使用 LoRA 还是全量数据集进行 SFT，安全阶段都需要引入大量语料。重点关注的是价值观类问题，通过合适的数据集构建的模型，其基础安全能力越来越强。例如，文心大模型能够回答的安全性问题越来越多，这得益于有监督精调带来的收益。人类反馈强化学习，直接偏好优化：目前，我们广泛采用 RM 奖励模型，也采用 DPO 技术。通过这些方法，我们对大模型回答的不同类型问题进行好坏评比，给予大模型正向反馈，使其更多地回答与人类价值观一致的问题。安全内容的萃取：这一内容在过去并不被广泛提及，但我们发现它具有独特的优势。安全内容萃取指的是，在提问有风险内容时，如果在 prompt 改写阶段能够很好地加入安全边界，例如规定观点、输出内容的原则和事实性基础等，大模型可能会将不安全的输出转变为安全的输出。与 SFT 的最大区别在于，我们先问大模型一个有风险的问题，得到不安全的回答后，对 prompt 进行改写，加入更多安全边界，然后再次提问。这时，大模型可能会给出安全的回答。我们将这个安全回答与最初的原始 prompt 组成问答对，用于监督训练，使大模型逐渐能够在没有安全模板边界提示的情况下，有效地回答安全问题。这就是所谓的安全内容萃取，它是一种有效的方式，因为它是大模型自身回答的，而不是我们强加的，对大模型在微调过程中的破坏性要小得多。</p><p></p><p></p><h5>持续评估</h5><p></p><p></p><p>持续评估是确保大模型内容安全的关键环节，它涉及多个方面的关注点：</p><p></p><p>风险分类问题：评估过程首先需要确保问题分类与国家标准完全符合。这包括关注召回问题，也就是识别出重大或高风险的问题，同时也要关注应答问题。根据国标的要求，除非特定场景外，通用的聊天机器人（chatbot）需要能够回答常识性或通识性的问题，即使这些问题可能包含某些敏感关键词，也不能简单地拒绝回答。攻击手段的全面覆盖：评估还需要覆盖全面的攻击手段，包括指代性攻击、注入攻击、越狱攻击等。同时，还需要对编码的适应性，包括代码适应性等内容进行合理分析和评估。自动标注的挑战：在评估中，自动标注是一个难点。与 Web 安全和信息安全领域不同，大模型的回答内容很难通过机器自动识别是否存在风险，通常需要人工进行标注。为了实现持续评估，必须解决这一问题，减少对人工标注的依赖。</p><p></p><p>为了实现自动化评估，我们采用了一个更大的模型，对问答内容进行大量的监督学习训练。通过训练，我们建立了一个裁判模型或监督模型，使其能够理解对一个问题的正确回答和负向回答是什么。这个模型为被测模型的输出提供了有效的评估，成为自动化评估的基准。目前，我们能够在备测数据集上实现大约 90% 的 F1 分数，在大多数分类上，自动化评估是可行的。通过这样的持续评估，我们可以确保大模型在处理各种问题时的安全性和可靠性。</p><p></p><p></p><h5>安全代答模型如何做到比大模型更安全</h5><p></p><p></p><p>在讨论安全代答模型时，我们面临一个看似矛盾的问题：理论上，一个更强大的模型应该能够提供更安全的回答，但更强大的模型通常需要更多的训练数据和更大的参数量，这可能导致资源消耗增加，从而增加成本。安全措施的成本必须低于业务成本，才能被接受。那么，代答模型如何在保持较小规模的同时，实现比大模型更高的安全性呢？我们从以下几个方面进行了规范：</p><p></p><p>数据与模型尺寸：我们的代答模型是一个相对较小的模型，大约 6B 参数左右。小模型的优点是训练周期短，但也可能带来一些问题，比如经过大量 SFT 后，模型的指令跟随能力可能会下降。这种下降反而减少了对高级攻击的敏感性。RAG 与信任域：为了解决大模型可能产生的“幻觉”问题，我们在小模型的基础上引入了大量的 RAG 数据。这些数据既来自搜索引擎，也来自我们构建的“信任域”，即基于官方媒体和权威知识源（如百科）构建的知识库。弱化指令跟随：在微调阶段损失指令跟随， 使模型对高级攻击反应 『迟钝』，但也带来适用性问题。持续运营：通过更大参数的巡检模型来发现事实性错误，前置过滤与错峰巡检实现性能优化。在低峰时段，巡检模型能够发现白天可能漏检的问题，并在第二天进行修补。</p><p></p><p>要点小结：</p><p></p><p>通过 SFT、DPO 或人类反馈的强化学习实现安全对齐。构建大量的 RAG 数据，包括搜索引擎和权威媒体的数据。通过大模型防火墙实现快速有效的干预。通过持续运营和评估实现不断的迭代和优化。</p><p></p><p></p><h4>关注智能体安全</h4><p></p><p></p><p>我们认识到智能体是大模型生态发展中极为重要的一环。今年，百度特别重视智能体的推广，因为它们不仅仅是基础模型的简单应用。</p><p></p><p>最初，我们认为只要做好基础模型就足够了，但很快发现实际情况并非如此。从开发大模型的第一天起，我们就面临了大量恶意使用技术的情况，这与仅开发基础模型时遇到的问题不同。我们需要能够及时有效地应对这些滥用行为。</p><p></p><h5>必须关注场景安全</h5><p></p><p></p><p>在开发特定场景的智能体时，除了关注基础模型的安全问题外，还必须关注特定场景的安全问题。例如，在开发广告领域的智能体时，我们不仅要考虑基础模型的安全，还要关注广告法、虚假宣传以及广告可能引入的各种欺诈风险。在 K12 教育领域，我们还需要关注早恋、吸烟、游戏沉迷等安全问题，这些在传统基础模型中可能不会受到太多关注。</p><p></p><h5>Prompt 泄露</h5><p></p><p></p><p>举一个例子，一位湾区的作者通过自己公司的数据创建了一个智能体，能够展示和推理湾区特定职业的收入情况。然而，黑客可以通过简单的指令泄露这些智能体的 prompt 内容，甚至可以将用于训练的 RAG 数据以文件形式下载。在智能体的开发和应用中，我们需要特别注意数据和模型的安全性，防止敏感信息的泄露。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9c/9c190de6fd0d85cb25489c64cd9a0276.png" /></p><p></p><p></p><h5>RAG 投毒</h5><p></p><p></p><p>在大模型的应用中，RAG 数据已成为一个标准配置，它对于提供准确的信息至关重要。然而，RAG 数据也存在被“投毒”的风险，即数据被恶意篡改或污染。如下图所示，如果 RAG 数据被投毒，当用户询问“湖南的省会是哪里”时，大模型可能会给出错误的回答。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/12/12d9cdc7f59f88e162b1e5f6e90375b4.png" /></p><p></p><p>RAG 数据通常来源于所谓的“信任域”，如企业内网、企业 wiki 或知识库。尽管这些来源相对可信，但仍有可能被污染。为了防止这种情况，我们需要关注几个关键的安全原则：</p><p></p><p>禁止角色扮演：基座模型通常关注名人肖像模板等通用问题，而特定应用则需要关注更具体的问题，如用户侵权、广告创意方向等。如果场景没有特殊需要，尽量通过指令禁用角色扮演， 根本上取消此类越狱风险。防护指令：为了防止高级攻击，需要在 prompt 模板中规范操作。例如，除非应用需要，否则应禁止角色扮演。同时，应设置防护指令，禁止输出 prompt 内容、使用数据，以及禁止使用 print 指令输出信息。Say No：在某些情况下，应明确拒绝回答用户的问题，而不是提供模棱两可的建议。结构化查询：通过结构化查询，限定系统指令，用户指令空间，避免注入，使用模板而不是拼装 prompt。避免多轮会话：如果应用不需要多轮会话，使用一次性（One-Shot）方式可能更有助于规避安全风险。</p><p></p><h4>总&nbsp; &nbsp;结</h4><p></p><p></p><p>总结下今天的分享，我们首先通过数据清洗和安全对齐来确保模型的内生安全。这意味着从源头上开始构建模型的安全性，使其在处理数据和生成回答时能够内化安全标准。其次，内生安全需要与外生防护相结合，形成纵深的防御体系。通过安全围栏，我们可以快速有效地干预潜在的安全威胁。同时，安全对齐让我们的模型在面对各种挑战时变得更加强大和健壮。最后，随着智能体在各个领域的应用越来越广泛，我们开始更加关注智能体的安全问题。通过弱点分析，我们可以不断发现并解决潜在的安全问题。此外，通过指令加强和应用层面的安全防火墙，我们可以增强智能体自身的安全性。</p><p></p><p></p><h5>演讲嘉宾介绍</h5><p></p><p></p><p>冯景辉，现任职于百度安全平台，任副总经理，负责集团业务安全、业务风控和大模型安全解决方案；其负责的百度搜索内容检测系统，多年来致力于持续改善搜索生态健康度，打击各种违法违规黑产利用搜索引擎传播，尤其是在打击搜索结果中的涉诈内容方面，为保护网民，净化网络空间内容履行百度社会责任，连续七年持续投入打击力量；其负责的业务风控、流量安全、反爬虫等方向是百度所有互联网业务的核心安全能力，历年来在百度移动生态业务中发挥重要的保障作用；其主导的大模型安全解决方案是国内第一个可商用的覆盖大模型训练、部署和运营全生命周期的安全解决方案。在进入百度之前，冯景辉是国内第一家完全基于 SaaS 的云安全服务厂商安全宝的联合创始人兼研发副总裁，安全宝系统架构总设计师。</p><p></p><p>会议推荐</p><p></p><p>AI 应用开发、大模型基础设施与算力优化、出海合规与大模型安全、云原生工程、演进式架构、线上可靠性、新技术浪潮下的大前端…… 不得不说，<a href="https://qcon.infoq.cn/2024/shanghai/">QCon </a>"还是太全面了。现在就可以报名，详情请联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/68/68a4f559d6682dec46bd5633588299f0.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1eCjiq2xi7YW4cORYKPs</id>
            <title>前华人首富服刑四个月出狱，和解金创最高纪录；奥特曼被曝欲加剧OpenAI内部权斗；库克做客脱口秀推销苹果AI功能｜AI周报</title>
            <link>https://www.infoq.cn/article/1eCjiq2xi7YW4cORYKPs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1eCjiq2xi7YW4cORYKPs</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 09:53:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><h2>行业热点</h2><p></p><p>&nbsp;</p><p></p><h4>腾讯进行新一轮职级改革：不再公开显示专业职级、职级停留时间至少1年</h4><p></p><p>&nbsp;</p><p>在2022年职级改革后，9月27日，腾讯控股发布全员邮件，对《腾讯员工职业发展管理制度》作出更新，其中包括不再于企业微信中公开显示员工专业职级信息、统一所有职级最短停留时间为1年。</p><p>&nbsp;</p><p>“自2022年以来，随着外部环境的变化，公司发展进入新的阶段，我们在职级等方面也在不断调整，以适应新形势对于人力资源管理的要求。”腾讯表示。</p><p>&nbsp;</p><p>根据新的调整，即日起员工专业职级信息将不在企业微信中公开显示。腾讯解释称，不希望大家被职级定义和固化，更不希望以职级论英雄，被职级对等等官僚陋习捆住手脚。取消职级外显是为了减少对职级的过度关注，倡导平等务实的职场文化。</p><p>&nbsp;</p><p>此外，腾讯表示，很多部门也在进行组织负责人的试点，将企业微信里显示的管理职级调整为相应组织或业务的负责人。据了解，以往一些高职级管理者的职级在企业微信中无法看到，但一些中层管理者的职级还能看到。</p><p>&nbsp;</p><p></p><h4>特斯拉德国工厂员工频繁请病假，马斯克亲自出面进行调查</h4><p></p><p>&nbsp;</p><p>特斯拉在欧洲唯一的汽车工厂的高缺勤率引起了首席执行官马斯克的注意。马斯克在X平台上写道，在社交媒体服务的一位用户分享了德国《商报》的报道后，他正在调查这个问题。据报道，位于勃兰登堡Grünheide的特斯拉工厂的员工请病假比率在8月份攀升至17%，是去年德国汽车行业平均水平的三倍多。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>OpenAI高管震荡内幕曝光，Altman欲加剧内部权斗</h4><p></p><p>&nbsp;</p><p>OpenAI近期经历了显著的管理层动荡，引发了广泛关注。据报道，该公司在短短几天内失去了多位关键的技术领导人，包括首席技术官Mira Murati、首席研究官Bob McGrew以及研究副总裁Barret Zoph。这些变动发生在公司被估值高达1500亿美元之际，形成了鲜明对比。</p><p>&nbsp;</p><p>据外媒报道，OpenAI CTO Mira Murati的离开，与GPT-4o、Her有关。今年春天OpenAI为了大抢谷歌开发者大会的风头，紧急推出GPT-4o，以至于安全团队只能在极短的时间内（大约9天）完成安全测试评估。</p><p>&nbsp;</p><p>根据The Information等多家媒体的报道，OpenAI此次高管离职潮背后的原因主要有两个方面：一是部分高管感到在公司内部受到冷落或遭遇了不公平对待；二是关于薪酬和公司发展方向上的分歧。有知情人士表示，Altman倾向于加剧内部的权力斗争，当其他领导层成员提出增加招聘或资源请求时，Altman往往避免做出决定，迫使其他人如总裁Greg Brockman承担更多责任。</p><p>&nbsp;</p><p>据悉，OpenAI&nbsp;在 8 月份的月收入达到 3 亿美元，同比增长 1700%，预计今年销售额约 37 亿美元，收入明年将增至 116 亿美元。然而，在支付运营成本、员工工资和租金等费用后，OpenAI 预计今年将亏损约 50 亿美元。</p><p>&nbsp;</p><p>9 月 26 日，据知情人士透露，OpenAI 正考虑成为一家营利性公司，并首次让 Sam Altman 拥有这家人工智能初创公司的股权。次日，外媒称OpenAI 首席执行官 Sam Altman 否认了有关他获得公司“巨额股权”的报道，称这一消息“并不属实”。他和财务总监 Sarah Friar 在视频会议上都表示，投资者对 Altman 没有获得 OpenAI 的股权表示担忧。</p><p>&nbsp;</p><p>目前，OpenAI 正分发文件给潜在投资者，希望筹集 70 亿美元资金，将公司估值提升至 1500 亿美元。据媒体周五报道，苹果公司已退出了OpenAI融资轮的谈判，而微软和英伟达则考虑参与此轮融资，其中微软可能再投资约 10 亿美元。融资谈判尚未结束，参与企业和投资额度可能会有所变动。</p><p>&nbsp;</p><p></p><h4>游戏科学 CEO 冯骥谈《黑神话：悟空》DLC 进度：让团队先 “躺”两年</h4><p></p><p>&nbsp;</p><p>9 月 27 日下午，游戏科学 CEO 冯骥在回复网友评论 “DLC 在做了？”时表示：“咱就说，能不能让团队先躺两年？采采风，恋恋爱，尽情玩玩其他游戏。”</p><p>&nbsp;</p><p>不久之前的 9 月 21 日，2024 北京文化论坛文化产业投资人大会期间，游戏科学 CEO 冯骥及商务经理黄一帆确认，《黑神话：悟空》的 DLC 正在开发中。此前有消息称，新 DLC 将包括“再起”和“此去”两大篇章，涉及多个新角色和故事，将在 2025 年农历新年左右推出，但具体内容尚未官方证实。首位投资人吴旦预计游戏生命周期内销量可达 3000 万份，并对游戏未来充满信心。</p><p>&nbsp;</p><p></p><h4>库克做客脱口秀推销苹果AI功能，网友：库克都上节目搞营销了</h4><p></p><p>&nbsp;</p><p>面对销售业绩的重压和AI功能欠席首发的窘境，“全球股王”苹果公司的CEO Tim Cook 也得上综艺节目为新产品做宣传。在美国知名脱口秀演员Jimmy Fallon本周播出的节目中，播放了上周五iPhone 16系列首发日与库克一同录制的片段。两人从纽约第五大道的苹果旗舰店步行穿过中央公园，来到位于上东区的另一家苹果店。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6fa856194e6b8cb2bd83a7e13f949c77.png" /></p><p></p><p>&nbsp;</p><p>作为CEO，库克也不忘在脱口秀节目上卖力宣传“苹果智能”。库克对吉米表示，“苹果智能”将在下个月（在美国市场）上市，首发功能包括总结邮件等。库克表示，这个功能对他而言意义重大，他每天能收到数百个邮件，现在只需要点一下按钮，就能总结长长的一封信。当然，由于“苹果智能”的首发功能并不算多，说到这里库克也有一点词穷：“你还可以去生成表情包...还有很多...你真的每天都做的事情。”</p><p>&nbsp;</p><p>在知名苹果情报网站9to5MAC上，有网友忧虑地表示，库克都需要上脱口秀搞市场营销，今年iPhone 16的销量是不是特别不景气。</p><p>&nbsp;</p><p></p><h4>阿里京东互相开放：京东物流进淘宝，支付宝进京东</h4><p></p><p>&nbsp;</p><p>9 月 26 日消息，淘宝天猫即将正式接入京东物流，预计于 10 月中旬上线，此后淘天商家在系统中选择商品寄送方式时，将多一个京东物流的选项。</p><p>&nbsp;</p><p>与此同时，京东也将接入菜鸟速递和菜鸟驿站——两者分别是菜鸟的自营快递品牌和代收包裹的站点。京东也将正式接入支付宝支付，预计在双 11 前夕推出。目前消费者在京东下单，默认的支付方式排序依次为京东支付、云闪付、微信支付等。据悉，双方在物流和支付上的合作今天都已达成一致。</p><p>&nbsp;</p><p></p><h4>“不得不打码”，苹果商店软件再被曝涉黄，乔布斯曾称想要色情内容的该买安卓机</h4><p></p><p>&nbsp;</p><p>9月25日，据报道，有网友在后台反应，发现苹果商店一款叫“钻石**”的软件涉黄。9月25日下午，“苹果商店软件涉黄”话题冲上热搜，引发网友热议。网友提供的视频显示，该软件在苹果商店显示内容为英文工具软件，年龄分级为4岁以上，但是下载打开后发现内容涉黄，“不得不打码”。</p><p>&nbsp;</p><p>有记者根据爆料内容下载了该软件，发现情况属实，已向平台和公安机关举报。随后，民警上门了解相关情况。当日下午15时许，该软件已被下架。苹果客服回应称，App上架苹果应用商店的审核非常严格，如果用户发现相关情况，可提供App名字、截图、录像等材料，苹果会介入调查。</p><p>&nbsp;</p><p>据报道，苹果创始人 Steve Jobs 曾在回复消费者邮件时表示：“我们认为苹果在道德上有责任让iPhone远离色情，那些想要色情内容的人应该去买Android手机。”</p><p>&nbsp;</p><p></p><h4>谷歌斥资27亿美元重新聘用Character.AI创始人</h4><p></p><p>&nbsp;</p><p>9月26日消息，当地时间9月25日，据外媒报道，谷歌向Character.AI支付了27亿美元以获得其技术授权，并让其创始人Shazeer重返谷歌工作。</p><p>&nbsp;</p><p>Shazeer是谷歌早期的几百名员工之一，也是著名论文《Attention Is All You Need》的主要作者之一。他在谷歌开发了名为Meena的聊天机器人，但因安全问题被谷歌拒绝发布，Shazeer随后离职并创办了Character.AI。知情人士透露，谷歌愿意支付巨额授权费的主要原因是为了让Shazeer回归并重新为公司效力。</p><p>&nbsp;</p><p></p><h4>英伟达CEO黄仁勋完成600万股票出售计划，总收入超过7亿美元</h4><p></p><p>&nbsp;</p><p>9月25日消息，英伟达首席执行官黄仁勋刚刚完成了600万股英伟达股票的出售，这一交易是他在年初制定的交易计划的一部分，出售股票给他带来得总收入超过7亿美元。</p><p>&nbsp;</p><p>黄仁勋从6月14日至9月13日间分批出售股票，交易数量从70股至75,300股不等，交易价格最低为8月5日的91.72美元，最高为6月20日的140.24美元。根据他的10b5-1规则交易计划，黄仁勋通过这些股票出售总计收入7.13亿美元，平均每股价格为118.83美元。虽然该交易计划原定于2025年3月到期，所有计划中的股票已提前六个月售完。</p><p>&nbsp;</p><p></p><h4>国产AI芯片公司破产清算，公开拍卖</h4><p></p><p>&nbsp;</p><p>9月23日消息，根据全国企业破产重整案件信息网公告，华夏芯（北京）通用处理器技术有限公司在京东拍卖网络平台上进行公开拍卖活动，涉及15项软件著作权、14项专利拍卖。</p><p>&nbsp;</p><p>华夏芯成立于2014年，其企业使命是“让AI更普及”，核心技术是全自主新一代处理器IP内核、SoC异构芯片设计，产品和服务包括CPU、GPU、DSP、ISP、AI加速器等类型的IP，两款SoC芯片平台，以及FPGA板卡。其中WNN是华夏芯第二代AI加速器IP。基于华夏芯的AI芯片解决方案覆盖辅助驾驶、智能驾驶、智能安防、智能家居、机器人、智慧城市、工业物联网、智能制造等应用领域。</p><p>&nbsp;</p><p></p><h4>前华人首富出狱了，服刑四个月，和解金创下最高纪录</h4><p></p><p>&nbsp;</p><p>9月27日，币安的创始人赵长鹏，提前两天获释，截至9月27日当天，他仍坐拥300亿美元的个人财富，在全球富豪榜上位列61名。在服刑期间，赵长鹏被转移到长滩的“中途之家”，在被监督的情况下可以外出，甚至去看电影。</p><p>&nbsp;</p><p>今年4月，赵长鹏因在其加密货币交易所涉嫌洗钱而被西雅图联邦法院判处四个月监禁。美国地区法官理查德·琼斯对赵长鹏说：“你本来有能力、财力和人力确保遵守每一项规定，但你错过了这个机会。”赵长鹏律师曾要求判五个月缓刑，但最终，他没能躲过牢狱之灾。</p><p>&nbsp;</p><p></p><h4>Meta 发布重磅新品：299 美元的 Quest 3S 头显、AR 眼镜原型</h4><p></p><p>&nbsp;</p><p>当地时间 9 月 25 日，在年度开发者大会 Meta Connect 上，Meta 发布了最新款虚拟现实（VR）头显设备 Quest 3S，起售价为 299 美元。这款头显设备将于 10 月 15 日上市，可以用来看电影，也可以运行 VR 健身应用和游戏。此外，Meta 还发布了多模态大语言模型和 AR 眼镜原型。在消息公布后，Meta 股价短线走高，并创下历史新高。据介绍，增强现实（AR）眼镜 Orion，暂时只是一款原型产品，短时间内不会出售给消费者，但 Meta 表示，随着公司继续努力，Orion 终会与消费者见面。</p><p>&nbsp;</p><p></p><h4>英特尔“全公司的希望”：首款Intel 18A芯片正式亮相</h4><p></p><p>&nbsp;</p><p>9月25日消息，据媒体报道，处理器大厂英特尔于上周在俄勒冈州波特兰市举行的 Enterprise Tech Tour 活动中，首次展示了其代号为Clearwater Forest的Xeon芯片，这也是英特尔首款最新的Intel 18A制程芯片，不过该芯片可能需要等到明年下半年才能上市。</p><p>&nbsp;</p><p>根据英特尔此前公布的数据来看，目前Intel 18A的缺陷密度已经达到D0级别，小于0.40 （def/cm^2）。不过，据业内人士透露，D0小于0.2才算入门，小于0.1才能量产。最近，为了确保Intel 8A制程在2025年的顺利量产，英特尔还宣布“跳过产品化”Intel 20A节点，提前把工程资源从Intel 20A投入到Intel 18A。该制程将采用Intel 20A上就已经完成的RibbonFET全环绕栅极晶体管架构和PowerVia背面供电技术。</p><p>&nbsp;</p><p>如果Intel 18A制程获得成功，其不仅将助力英特尔自身的产品PC及服务器处理器扩大竞争力，同时也将成功为英特尔的晶圆代工业务打开局面。基辛格此前也曾表示，他已经将整个公司的赌注押在了Intel 18A上面。</p><p>&nbsp;</p><p></p><h2>大模型一周大事</h2><p></p><p>&nbsp;</p><p></p><h3>大模型发布</h3><p></p><p>&nbsp;</p><p></p><h4>端侧最强开源 AI 模型 Llama 3.2 登场：从 1B 纯文本到 90B 多模态</h4><p></p><p>&nbsp;</p><p>Meta 公司 9 月 25 日发布博文，正式推出了 Llama 3.2 AI 模型，其特点是开放和可定制，开发者可以根据其需求定制实现边缘人工智能和视觉革命。Llama 3.2 提供了多模态视觉和轻量级模型，代表了 Meta 在大型语言模型（LLMs）方面的最新进展，在各种使用案例中提供了更强大的功能和更广泛的适用性。其中包括适合边缘和移动设备的中小型视觉 LLMs （11B 和 90B），以及轻量级纯文本模型（1B 和 3B），此外提供预训练和指令微调（instruction-tuned）版本。</p><p>&nbsp;</p><p></p><h4>傅利叶发布新一代通用人形机器人 GR-2，CEO 顾捷称其有望三五年内迎来“GPT 时刻”</h4><p></p><p>&nbsp;</p><p>9 月 26 日，傅利叶智能发布了自主研发的新一代通用人形机器人 GR-2，官方称其产品愿景为“为 AI 打造最佳具身载体”，具备更灵活、更强劲、更开放的特性。</p><p>&nbsp;</p><p>据介绍，该机器人形成一套基于主流编程语言的开发接口方案，支持服务器-客户端模型的算法程序开发，封装了一系列简洁易用的 API，集成了机器视觉、路径规划、力控反馈等预优化的算法模块，在降低开发门槛的同时简化了复杂任务的实现过程，显著提高开发效率。目前支持 NVIDIA Isaac Lab、ROS、Mujoco、Webots 等开源框架。傅利叶智能创始人兼 CEO 顾捷在采访中表示，“真正通用机器人的 GPT 瞬间现在还没有到…… 但是曙光已经看到了，它不是 10 年 20 年的事，它就是三五年内的事。”</p><p>&nbsp;</p><p></p><h4>谷歌 Gemini 1.5 AI 模型再进化：成本更低、性能更强、响应更快</h4><p></p><p>&nbsp;</p><p>据外媒报道谷歌升级旗下 Gemini 1.5 AI 模型，推出了 Gemini-1.5-Pro-002 和 Gemini-1.5-Flash-002，相比较此前版本成本更低、性能更强、响应更快</p><p>&nbsp;</p><p>谷歌下调了 token 输入和输出费用，Gemini-1.5-Pro-002 和 Gemini-1.5-Flash-002 最高降幅 50%，提高了两种模型的速率限制，并减少了延迟。新定价于 2024 年 10 月 1 日生效。</p><p>&nbsp;</p><p></p><h4>英伟达发布 Llama-3.1-Nemotron-51B AI 模型</h4><p></p><p>&nbsp;</p><p>9 月 23 日，英伟达发布博文，宣布推出 Llama-3.1-Nemotron-51B AI 模型，源自 Meta 公司的 Llama-3.1-70B。该 AI 模型主要采用了神经架构搜索（NAS）技术微调，平衡性能和效率，在高工作负荷下，只需要一片 H100 GPU 即可运行，大大降低了内存消耗、计算复杂性以及与运行此类大型模型相关的成本。</p><p>&nbsp;</p><p>英伟达认为这种方式在保持了出色的精度前提下，显著降低了内存占用、内存带宽和 FLOPs，并证明可以在创建另一个更小、更快的变体来加以推广。</p><p>&nbsp;</p><p></p><h4>字节发布豆包视频生成大模型</h4><p></p><p>&nbsp;</p><p>9 月 24 日，字节跳动旗下火山引擎在深圳举办AI创新巡展，一举发布了豆包视频生成-PixelDance、豆包视频生成-Seaweed两款大模型，面向企业市场开启邀测。这也意味着字节跳动正式宣告进军AI视频生成。据火山引擎介绍，豆包视频生成模型基于DiT架构，通过高效的DiT融合计算单元，让视频在大动态与运镜中自由切换，拥有变焦、环绕、平摇、缩放、目标跟随等多镜头语言能力。</p><p>&nbsp;</p><p>深度优化的Transformer结构，则大幅提升了豆包视频生成的泛化能力，支持3D动画、2D动画、国画、黑白、厚涂等多种风格，适配电影、电视、电脑、手机等各种设备的比例。目前，新款豆包视频生成模型正在即梦AI内测版小范围测试，未来将逐步开放给所有用户。</p><p>&nbsp;</p><p></p><h4>哔哩哔哩已上线自研大语言模型index，并应用在AI字幕上</h4><p></p><p>&nbsp;</p><p>9月26日，2024年中国国际智能传播论坛在无锡召开。哔哩哔哩董事长兼CEO陈睿在演讲时表示，AI是年轻人在B站上最关注的内容，也是增长最快的科技内容。数据显示，过去一年，AI内容的日均播放量同比增长超80%，AI相关UP主日活增长超过60%。B站上线了自研大语言模型index，并应用在了AI字幕上。目前B站具备中、英、韩、日、泰语等近10种语言的实时翻译能力，准确度接近90%。</p><p>&nbsp;</p><p></p><h3>企业应用</h3><p></p><p>&nbsp;</p><p>9 月 26 日，在全国第三届公共就业服务专项赛上，人力资源和社会保障部中国就业培训技术指导中心发布了“职业数字展馆”可灵AI短片，首次以AI的方式全景呈现职业数字画像。9 月 25 日，全球领先的自动驾驶科技公司文远知行WeRide和全球最大的移动出行及配送科技公司优步Uber Technologies, Inc.宣布建立战略合作伙伴关系，将共同推进文远知行自动驾驶车辆上线Uber平台，并将首先在阿联酋启动运营。9 月 24 日，OpenAI宣布，已经开始向订阅OpenAI ChatGPT Plus和Team计划的用户推出新的ChatGPT高级语音模式Advanced Voice。该公司补充称，该功能将从下周开始向OpenAI Edu和Enterprise计划的订阅者开放。9 月 23 日，据腾讯混元官方消息，其AI智能体产品腾讯元器现已支持发布至微信公众号，为公众号运营者带来多项新功能。据腾讯表示，利用腾讯元器，公众号运营者可创建数字分身与粉丝进行实时互动，提供7*24小时的客服服务。腾讯元器还能提供文章插入服务，智能体能够将相关内容插入公众号文章，增强内容的互动性和信息量。为读者提供问答助手、文章更实用。9 月 23 日，钉钉宣布面向个人用户推出“365 会员”，包含 AI 搜索、个人 AI 助理、AI 自动回复、自动速读等权益，非会员仍可使用钉钉 AI 助理、快速阅读等现有的 AI 功能。9 月 23 日，美图公司宣布美图奇想大模型（MiracleVision）视频生成能力完成全面升级，在实现生成能力、生成效率以及模型性能的三重进阶基础上，结合美图在计算机视觉领域的多项自研技术优势，视频生成时长与画质、流畅性、真实性及可信度等方面提升显著。美图奇想大模型（MiracleVision）的单次文生视频时长、单次图生视频时长均达5秒，已支持1分钟、帧率24FPS、分辨率1080P的超长视频生成，可以任意视频尺寸输出。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DOXQYxhOT5GLt4t0KKY9</id>
            <title>大模型辅助需求代码开发</title>
            <link>https://www.infoq.cn/article/DOXQYxhOT5GLt4t0KKY9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DOXQYxhOT5GLt4t0KKY9</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 08:12:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型在解释代码、回答代码问题、写单元测试等方面表现不错，但这些还只是辅助任务，真实项目需求开发中的设计及实现任务才是核心任务，而这方面尚未有成熟的方法和好的效果。一些 AI Developer 工具能够演示从零创建小应用的能力，但放到真实项目中几乎寸步难行。</p><p></p><p>在不久前举办的 <a href="https://aicon.infoq.cn/2024/shanghai/schedule">AICon 全球人工智能开发与应用大会</a>"上，研发效能领域知名专家路宁做了专题演讲“大模型辅助需求代码开发”，结合在多类项目中借助 GPT-4 开发完整需求的实践经验，分享了针对复杂度持续增加的编码任务如何准备上下文、如何区别对待设计任务和实现任务、如何加工经验知识并利用它大幅提升生成效果、如何分步完成复杂的开发任务等不同场景下的问题解决思路，还探讨了组织或企业该如何建设知识工程，以便有力支撑工程师在核心开发任务中利用大模型提效。</p><p></p><p></p><blockquote>在将于 2024 年 10 月 18-19 日举办的 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会（上海站）</a>"，我们设置了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1704">AI 重塑技术工作流程</a>"】这一专题，旨在探索那些超越单点 AI 应用，进一步利用 AI 技术重塑产品研发核心流程的最佳实践。我们关注实际案例和解决问题的策略，旨在解决当前研发团队面临的困境，让智能能真正赋能业务，创造价值。欲了解更多内容，可访问<a href="https://qcon.infoq.cn/2024/shanghai/schedule">大会官网</a>"获悉。</blockquote><p></p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><p>今天我将与大家分享的主题是《大模型辅助需求代码开发》，这个话题的含义可能非常广泛，容易引起误解，因此我给它加了一个副标题：“探索提升核心编码任务生成效果的方法”。背后的想法其实很简单：我是否能够在不写或少写代码的情况下完成真实项目的开发任务，特别是在面对一些难度高的编码任务时。那么，这些任务应该如何去探索和解决呢？</p><p></p><p>本次演讲的内容主要基于我们过去半年的工作，我将分享一些实践中发现的洞见。演讲将分为几个部分：首先，我会讨论大模型辅助开发的生态系统以及开发实际需求面临的挑战；其次，我将从知识生产和消费的角度来探讨如何生成这些任务的代码，并总结背后的知识分类；最后，我将探讨未来工程师如何利用大模型进行开发，最可能的形态是怎样的，是使用一个很强的 SaaS 工具，还是需要更多的方法和策略。</p><p></p><h3>⼤模型辅助开发⽣态</h3><p></p><p></p><h4>任务全貌</h4><p></p><p></p><p>在探讨大模型辅助开发时，我们首先要明确，是针对那个具体的任务。工程师的工作通常包含很多类的任务，如需求分析、设计、代码实现、修复错误等。这些是大类，其下还有更细致的任务类型。</p><p></p><p>从模型能力的角度来看，有一种特殊的任务是代码补全，但今天我们不讨论这个。而剩下的各类任务大多可以通过模型问答的形式来处理。我们注意到，目前效果较好的任务往往是下图中用蓝色标记的部分，我将这些任务定义为支撑性任务。与支撑性任务相对的是主干任务，这些任务是核心的不可或缺的工作步骤，包括需求分析、架构设计、任务规划、编码实现，以及后续的编译和测试。如果一切顺利，完成这些任务就能实现需求。</p><p></p><p>然而，实际情况往往并非如此顺利。在每个阶段都可能遇到问题，比如在编码阶段可能需要理解代码，或者需要编写单元测试。部署后如果出现问题，还需要分析日志、定位缺陷、以及修复问题。主干任务在图中用绿色标记，目前看来，大模型在这些任务上的表现都不尽如人意。相比之下，支撑性任务由于其难度往往相对低，很多能够取得较好的效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/08/08c9096b4dbb17ae0344720444623c20.png" /></p><p></p><p>如何在主干任务中利用大模型来提高开发效率，我们可以通过几种不同的模式来实现这一点。最理想的情况是，我们能够将任务完全交给大模型或者基于大模型的应用来独立完成。如果这种自动化的效果并不理想，我们可能需要采取一种合作的模式，类似于 Copilot。在这种模式下，大模型会先执行一步操作，然后我进行检查和修正，之后再进行下一步，如此推进，直到最终得出满意的结果。对于更加复杂的任务，比如需求分析，大模型可能无法独立完成，但它可以提供一些启发和建议，这对我来说已经非常有帮助了。应用那种模式完全取决于效果，是当前技术能力的局限性所导致的。</p><p></p><h4>主⼲任务相关应⽤现状</h4><p></p><p></p><p>目前，几家大公司出的工具都在尝试从需求澄清、架构设计、任务分解到具体任务实现，端到端地引导工程师完成开发任务。这些实践在体验上与 GitHub Copilot 的 Workspace 相似。还有一些工具，如 Devin，它自称为第一个 AI Developer，擅长从零开始开发一些相对简单的 Web 应用，其公司也获得了可观的投资。然而，尽管这些工具在某些情况下表现出色，但要将它们应用到公司的实际开发环境中，表现就比较差。</p><p></p><p>还有部分人选择手动调用大模型来完成他们的主干任务。为什么要先手动操作呢？因为这样方便针对项目代码和具体任务准备针对性的上下文和知识，以达到可接受的生成效果。事实上，手动操作是能探索到效果的天花板的，因为可以投入大量时间来为模型准备上下文和相关知识，如果这样的效果还不理想，那么使用自动化工具的效果只会更差。从手动操作中总结的经验，可以反过来促进工具的改进和发展，这是一个不断迭代和优化的过程，以提高整体的开发效率和质量。</p><p></p><h4>了解测评背后的任务</h4><p></p><p></p><p>我们不得不指出，行业现有的评测方法过于简单化，无法代表真实需求编码任务。例如，GitHub Copilot 声称能够提升 55% 的工作效率，它的测试是基于两组工程师的一次实验，使用 JavaScript 开发一个 Web 服务器。多数人在一个多小时就能完成这样的任务，任务不涉及私有知识，这与我们日常工作中的任务相去甚远。现有的评测数据集，如 HumanEval，通常只包含函数级别的任务，这些任务的上下文在函数范围内就能描述清楚。</p><p></p><p>SWE Bench 是一个取自真实代码任务的评测集，由美国几所高校的学生创建。它从多个 GitHub 上的项目中挑选 issue 作为测试任务，这些 issue 可能涉及增强功能、修复 bug 或新的需求。如果这些 issue 背后有测试用例，实际上可以自动验证任务是否被正确实现。当我们刚开始接触这个评测集时，只有 Devin 作为一个应用来打榜，它的解决率达到了 13%，其他基于模型 + RAG 的方案表现都很差。随着时间的推移，这个评测变得越来越受欢迎，因为它比其他评测集更接近真实的工作场景。现在，一些应用能够做到 40% 以上的解决率，这是一个相对较高的数字。</p><p></p><p>SWE Bench 的问题在于它使用的是开源库，这些对于模型来说都是在训练数据中包含的。此外，这些测试集所在的项目都是 Python 库，而且大部分 issue 仅需极少的代码改动。例如，一个精选的 Lite 版测试集中 30% 的 issue 只需要 2 行及以内的代码改动就能解决问题。因此，尽管它比其他评测集更接近实际，但与我们实际工作中需要写近百行代码并有复杂的私有知识依赖的编码任务相比，它们仍然太过简单了。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7afd2e2f63ba60275506effe9dc2b3dc.png" /></p><p></p><h4>开发真实需求的挑战</h4><p></p><p></p><p>在开发真实需求时，我们面临的挑战是多方面的。以一个实际的例子来说明，我们当时正在做一个类似 Github Copilot 的项目，有个需求是“增加流式对话的能力到现有的代码中”。我们尝试使用大模型，比如 GPT-4，看看能做到什么程度。</p><p></p><p>初始版本的提示词可以是项目的目录结构和相关代码文件的全部内容，以及任务描述。根据生产的代码来调整提示词，增加更多必要信息，经过多次尝试，最终得到了一个相对满意的生成结果。</p><p></p><p>在这个过程中，我们发现，当使用大模型编写代码时，最常见的问题是模型不遵循设计，倾向于写一套新的。为了避免这种情况，我们需要在提示词中列一些明确的约束和指导，甚至需要详细说明实现步骤。如果工程师需要把任务描述到如此细致的程度，他是不会接受这么使用大模型的。但只要描述得足够详细，模型的效果就会非常好。任务描述甚至接近伪代码的程度，明确指出在何处添加什么函数以及它们的功能。为了提高生成效果，我们让模型每次只执行一个步骤。我们可以一步一步地进行，这样生成的代码基本上是可用的，当然，这个过程相当痛苦。</p><p></p><p>这个痛苦的经历却得出了一些有价值的经验。首先，指示足够详细，效果就足够好。其次，我们需要考虑提示内容的分类。一类是指出代码的改动入口点，通常以函数签名的形式表达，有时还包括输入输出约束。这些信息至关重要，画龙点睛，它指明了新写的代码如何在当前设计约束下与现有系统集成，框定了模型发挥的范围。我们当然希望模型根据上下文及任务能自己确定改动入口点，做不到你就需要告诉他，在提示词中加入一些经验知识能显著提升这方面的效果，后面会介绍。如果做了这些效果还不好，可能就需要给出更多实现级别的提示了。</p><p></p><p>这个过程和指导新人开发需求是类似的，你给他一个需求，他看了现有代码（提示词中的目录结构和代码文件内容）可能也不知道如何切入。你可能会一层一层提示他，第一层就是告诉他在那里加代码，输入输出是什么。还不行就告诉他每块实现的伪码。</p><p></p><h4>分析任务难度</h4><p></p><p></p><p>我们需要对任务难度建立一个认识，对利用大模型完成任务的预期效果有所预判。大模型的能力固然关键，任务本身的难度也会影响效果。</p><p></p><p>首先，模型的推理、规划、指令遵循、窗口大小、注意力特征、输出习惯等都会影响到效果。模型的注意力往往是离散的，我们往往需要在提示词中加入额外描述提及它需要在长上下文中关注的部分，干预注意力的分布。</p><p></p><p>任务本身的难度也是一个考量因素。任务本身所需的理解和推理难度会不同，可能还涉及复杂的分解和规划。任务所依赖的私有知识复杂程度也会影响难度，准备推理所需私有知识的过程可能非常繁琐。</p><p></p><p>在下图中，我们比较了 HumanEval 任务、AI Developer 任务和真实项目中的任务的复杂度。横轴代表所需私有知识的复杂度，即任务所依赖的经验和代码等。纵轴代表一次与模型交互生成内容的复杂度，这粗略反映了模型一次推理的复杂度。HumanEval 这类任务不太依赖私有知识，函数范围就能包含所有必要的信息。而 AI Developer 任务，如编写贪吃蛇游戏，特点是模型已经具备了相关的业务、架构和编码知识，不需要额外提供，但它生成的内容很复杂，所以它的点在于生成内容的复杂性，尽管它依赖的知识并不多。在真实的项目中，依赖知识的复杂度很高，生成内容也多。这三类场景在下图空间中的位置不同，有个演进过程，涉及到对 Agent 和知识工程要求的逐步提升。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/03/03ce95d3dbfc18ce6da5bbe5419683ed.png" /></p><p></p><h4>切换到知识⽣产和消费的思维框架</h4><p></p><p></p><p>要深入分析如何将大模型应用于软件开发中工程师的各项活动中，我们需要从知识生产和消费的角度来重新认识这些活动，尝试理解并刻画工程师大脑里的内心独白和思维轨迹，分析在这些活动中消费以及生产了哪些知识。</p><p></p><p>在项目中，我们可以看到被沉淀下来的知识产出，比如编写的代码、设计文档和需求文档。这些产出是需求开发在各阶段的产物，往往会被记录下来，作为下游工作的输入。编码任务消费上游的设计文档以及已经产出的代码，同时还需要消费任务相关的经验知识，最终生产出增量的下游产物。这些经验知识往往是隐性的，可能加工自上下游产物。比如，工程师之前通过看代码和设计文档掌握了如何在当前架构下完成一类功能的经验，他在应对新任务时可能就会在头脑中召回并消费这个经验，这便体现了：经验以前加工自上下游产物，并在当前任务中被消费。</p><p></p><p>可见，大模型推理可能同时完成了对已有知识的消费和对新知识的生产。按照这个思路分析不同的任务，尝试表达和刻画知识的有效方式，便可构建出一个完整的知识工程表达框架。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/db/db757c566e7efa72c12e247e3b9cf3e3.png" /></p><p></p><h4>依赖经验知识的编码任务</h4><p></p><p></p><p>在探讨大模型辅助编程时，常会先关注两类任务：编码任务和设计任务。编码任务不涉及对现有设计的修改，而设计任务则会改动现有设计。在这些任务之间，我们需要进行规划，决定先做什么，后做什么。除此之外还有需求的澄清和定义等任务。</p><p></p><p>编码任务中有些不依赖私有知识，例如，使用 Devin 编写贪吃蛇游戏这样的任务。还些任务只需要依赖项目代码就能有不错的生成效果。还有些任务只有引入了针对项目加工的私有知识才会得到好的生成效果。</p><p></p><p>我们这里主要讨论的是依赖经验知识的编码任务。我将通过两个例子来阐述这种知识的重要性。第一个例子是利用现有架构完成任务的方法，这涉及到如何抽象地表达任务经验。第二个例子是通过历史任务的记录来刻画经验知识，这便是具体的、实例化地表达任务经验。这些历史任务的记录可以帮助我们理解当时是如何完成任务的，从而为当前的任务提供指导。</p><p></p><h4>利⽤"通过框架完成任务的⽅法描述"</h4><p></p><p></p><p>在“利用框架完成任务的方法描述”中，我们可以看到，利用框架完成任务所需的指导说明。这些知识往往没有被正式记录成文档，而是存在于工程师的头脑中，通过口口相传或个人总结的方式流传。为了将这些隐性知识显性化，我们可以主动加工它们并将其放在提示词中。</p><p></p><p>以一个具体的例子来说，我们有一个需求，要为新增的页面元素编写交互脚本代码。这个任务的挑战在于让代码遵循现有的架构设计。为此，我们总结了在框架下生成页面元素交互脚本的方法。方法描述了框架在哪里，应该使用哪些 API，强调优先使用内置函数，如果内置函数无法完成任务，可提供其他方案等等。这些内容与我们在工作中指导新人如何完成任务时的沟通非常接近，只是我们将隐性沟通显性化了。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b2/b2e88f8eee08e219511f4a0469f6774f.png" /></p><p></p><p>这个例子中，严谨来说，如何使用框架的知识并不是新东西，它们已经蕴含在代码中，如果充分理解代码，我们就应能知道怎么使用框架。为什么还要显式的经验知识来提示呢？因为上下文很大，模型的注意力分散，加之模型能力的限制，如果不加入这些知识，生成的代码倾向无视框架而自己重写一个。我们需要通过经验知识的提示来影响模型在当前上下文中的推理时的注意力分布，框架相关代码会因此获得更多注意力权重，从而引入架构约束，提高生成效果。这实际上是一个影响注意力的过程。</p><p></p><p>这与人之间的交互类似，需要一些提示来指导行动。进一步思考，我们是否可以批量加工这些知识，并在工具中根据相关性进行召回和使用。这样，我们就可以将这些隐性知识转化为显性知识，更有效地利用大模型来辅助编程任务。</p><p></p><h4>利⽤"相关任务的实例化经验"</h4><p></p><p></p><p>我们再来看一个例子，这个例子展示了如何利用一个具体任务的实例化经验。所谓实例化经验，就是一段描述，说明任务是什么，以及每一处代码改动具体是什么。这有点像是你要做功能 A，可以参考功能 B 的做法。模型不会复制功能 B，而是参考它的改动，这里蕴藏了大量有用提示。</p><p></p><p>这个示例是规则引擎的一个需求，目标是扩展这个规则引擎，增加一个新的操作符。如果工程师不太熟悉如何实现，你可以告诉他，这是规则引擎的代码，现在要增加一个新的操作符，可以参考其他已有的操作符。比如，可参考的是幂运算操作符，这个具体实例经验可以从代码提交的变更列表（change list）中加工出来的，具体包括改动了哪些文件，以及改动的部分，也可以为每一处改动生成了简要的描述，以降低模型理解负担。每一处改动的表达方式是提取代码片段，并标记出增加或删除了哪些行。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/6563820476f6c1ef1b8d98f483a9da2b.png" /></p><p></p><p>通过应用这个实例化经验，模型生成的代码几乎全部正确，即使这些改动涉及多个文件的多个函数。不同的实例化经验表达方式对生成的效果有显著影响。我们对比了几种方式，包括仅提供完整的代码文件内容，使用 Git Diff 格式表达每处改动，以及上面提到的代码片段加改动行标记。实验证明第三种方式效果最好，这也可以理解，仅提供完整代码其实并没有给出参考经验，而 Git DIFF 则太过简练缺少上下文。如果提供了代码片段并标记出改动的行，这对人类工程师和大模型来说都更完整和友好。</p><p></p><p>上面说明如何刻画经验才能提升生成效果。一般来说，你的经验刻画方法对人类工程师也很容易理解时，那么大模型的效果通常也会不错，其实这也更逼近工程师实际工作中的参考内容和思维轨迹。</p><p></p><h4>知识分类及提⽰词框架</h4><p></p><p></p><p>基于前面的例子，以及众多类似的分析，我们可以对知识进行分类，以建立全局认识并指导后续知识工程的建设。初步将知识分为三类：产物知识、经验知识和衍生数据知识。</p><p></p><p>产物知识：在流程中以产物形式显性表达的知识。这是在软件开发不同阶段的具体产物，如需求文档、架构设计文档、模块设计文档、代码及配置等。如果团队能够采用大模型友好的方式记录这些信息，那么它们就更容易在模型推理过程中被利用。如果使用 Word 文档或图片，那么这些信息就不太容易利用。经验知识：在生产增量产物知识过程中使用的经验，工程师在头脑中加工过，但在团队中通常没有被显性记录下来。这类知识是关于需求、架构、设计、编码及问题修复等任务的经验。它们可以表达为文字和代码。例如，架构变更记录可能包含因为某个问题而进行的架构变更考虑及其效果。设计规范和开发任务的经验也属于这类。对于问题修复经验，可以从相关记录和代码中提取根因、现象和具体修改的代码等信息，做为经验的表达，以后使用。以编码任务中的经验知识为例，它们可以是组件级别的知识（如函数、类的说明）或任务级别的知识（如何实现一个利用到多个组件的任务），也可以是抽象的知识（如何使用架构完成任务）或是具体的知识（如特定任务的实现细节）。衍生数据知识：程序运行时的数据或基于代码分析的数据，用于知识相关性计算或直接提升生成任务的效果。这类知识不是由工程师脑力生产的，而是通过程序加工的。通过代码分析得到的 AST（抽象语法树）结构，可以用在生成单元测试的任务中以提升覆盖度。AST 数据也可以用在依赖代码的相关性计算中，在为代码生成任务准备提示词上下文时可能用到。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/92b21f2c4e0c41abd3bad08ee8e09c5e.png" /></p><p></p><p>对知识进行分类后，我们可以总结一个提示词框架，表达在什么上下文下，基于什么经验约束，完成一个什么样的任务。它包括下面几部分：</p><p></p><p>项目的基础上下文：这部分主要是产物知识，比如项目目录结构以及模块的全部代码。如果受限于窗口大小或是发现无关信息的干扰严重，我们可以手工或通过相关性计算来精选信息，确保生成效果。任务经验：与当前任务相关的经验描述，属于经验知识。输出约束：对输出内容的约束规则，它们引导模型按照预期的方式生成结果。任务描述及提示：这些内容不容易复用，往往是一次性编写的，包括具体任务介绍，以及在生成效果不够好时不得不增加的提示。在反复尝试改善效果的过程中，我们需要增加些关键的提示，它们虽然难以转化为可复用的经验，但对于提升特定任务的生成效果至关重要。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/89/8922ad2effbd5f4f5b51b579fa916f89.png" /></p><p></p><h3>⼤模型辅助开发的实⽤形态</h3><p></p><p></p><p>探讨未来应用形态，实际上也是在回答一系列问题。</p><p></p><p>⼯具能做到什么程度？ 这取决于底层模型的能力，以及知识工程建设的程度，未来随着模型能力的显著提升，工具可能表现的更懂代码和工程师的意图。而现在，尽管工具已经表现的能引导完成从需求到代码的过程，但还做不到对主干任务有实质性的帮助。很多步骤带来的检查修改成本大过帮助，工程师更倾向于在痛点步骤或任务上去为大模型认真编写上下文。</p><p></p><p>工程师是否需要裸用 ChatGPT 这样的工具？未来结合一些小脚本裸用大模型的情况会很常见，对主干任务也是这样。工程师熟悉大模型后维护提示词模板非常容易。结合一些小脚本或工具管理知识具备可行性，特别是大部分工程师需要维护的代码量有限，维护私有知识成本不高。模型能力提升后那些繁琐的多步操作将大幅度简化，也降低了复杂工具的必要性。模型能力包括窗口和注意力的提升也大幅度降低了通过相关性检索和复杂上下文组装的必要性。大型工具在主干任务的优势未必明显，或者说工具建设需要开放性，以便融入工程师自己在项目中利用大模型的经验，包括自己的提示词、加工的知识和定义的推理路径等。</p><p></p><p>⼯程师是否必须能驾驭⼤模型？答案是肯定的。工程师需要能够深入理解并使用它们，而不是寄希望于用上工具就能大幅提升自己的效率。软件开发的场景非常丰富，即使是我前面提到的例子，覆盖的范围也非常有限，很多时候需要我们自己想办法提升效果。</p><p></p><p>⼈的精⼒能被释放到什么程度？追求多⼤⽐例的效率提升⽐较现实？ 我们在利用大模型完成主干任务时，虽说要写的代码少了，但时间却花在了各种准备工作、反复尝试和提示、检查和修正这类事情上面了，当然其中一部分的工作量是有潜力工具化的，但可能并没有想象的比例那么高。当前已经比较成熟的代码补全，即使采纳率达到 30%，在编码环节上的提升可能在 5% 左右。2 年内，对于端到端整体效率提升，持谨慎乐观的态度，可能在 15% 左右。</p><p></p><p>最后，我想强调几点：</p><p></p><p>软件开发任务生态丰富，主干任务的难度远超支撑性任务。从知识生产和消费的角度重新建模软件开发过程。如何遵循设计是复杂编码任务的重要挑战。从现有代码结构或完成任务的历史记录中加工经验知识。利用合理的知识分类指导知识工程的建设。</p><p></p><p>演讲嘉宾介绍：</p><p>路宁，研发效能领域知名专家，目前在理想汽车探索代码智能实践，曾任 ThoughtWorks 架构师和互联网大厂资深技术总监。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SEdrDVfZPxDyutR4Asqm</id>
            <title>文档解析与向量化技术加速 RAG 应用落地</title>
            <link>https://www.infoq.cn/article/SEdrDVfZPxDyutR4Asqm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SEdrDVfZPxDyutR4Asqm</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 07:44:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在不久前举办的 AICon 全球人工智能开发与应用大会上，合合信息智能创新事业部研发总监，复旦博士常扬从 RAG 应用落地时常见问题与需求（文档解析、检索精度）出发，分享了针对性的高精度、高泛化性、多版面多元素识别支持的文档解析技术方案与整合长文档解析引擎、层级切片技术的向量化 Embedding 模型技术方案，以及开放域多模态信息抽取应用与知识库问答应用等实际场景的产品实践案例。本文为常扬的演讲整理。</p><p></p><p></p><blockquote>在将于 2024 年 10 月 18-19 日举办的 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会（上海站）</a>"，Zilliz Senior Product Manager 张粲宇也从自身的 RAG 场景出发，分享了 RAG 的挑战，以及通过采用混合检索技术和其他技巧，最终把准确率提升到 90% 以上的经验。欲了解更多内容，可访问<a href="https://qcon.infoq.cn/2024/shanghai/schedule">大会官网</a>"获悉。</blockquote><p></p><p></p><p>近年来，大模型的崛起为人工智能领域带来了革命性的变化。然而，在实际应用中，我们发现仅依靠大模型自身的知识和上下文并不能完全满足用户的需求。检索增强生成（Retrieval-Augmented Generation， RAG ）应运而生，旨在通过引入外部知识库，解决领域知识缺乏和信息过时的问题。但其本身在产品化落地的过程中面临检索召回率低、生成质量差的问题。</p><p></p><p>我会从第一性原理出发，分析其核心突破点：文档解析技术 与 向量化技术，看这两个技术如何发展，加速 RAG 应用落地 的。</p><p></p><h3>RAG 的背景与问题</h3><p></p><p></p><p>在传统的大模型应用中，模型的知识来源主要是预训练数据和用户提供的上下文。然而，面对专业领域的复杂问题，模型往往难以给出准确、及时的回答。RAG 的核心目标是将外部的领域知识与大模型结合，使其能够生成更准确、更专业的回答。</p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a6c427d5d8cf58abcbe58d57e1d585c.png" /></p><p></p><p>LLM（大型语言模型）应用知识的数据来源包括“用户上下文输入和用户意图”、“大模型知识”和“外部文档”。RAG 位于三个知识源的交集处，结合了外部文档、用户输入的上下文、模型的知识进行生成。RCG（Retrieval-Centric Generation）专注于将知识检索与 LLM 的生成分开，把检索知识作为核心来源，而微调（Fine-Tuned LLM）则通过使用外部数据微调模型，提升模型在特定领域的理解能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/08/08fcc15c26b46fd08c004436969133ec.png" /></p><p></p><p>RAG 的核心流程可以简化为以下三个步骤：</p><p></p><p>知识库构建（Indexing）：对外部文档进行解析、清洗、向量化，构建高质量的索引。检索（Retrieval）：根据用户的查询，在向量空间中检索最相关的内容。生成（Generation）：大模型基于检索结果生成最终的回答。</p><p></p><p>在这个流程中，知识库构建和检索是关键的技术环节，直接影响最终生成的质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b47354cc1b49194af5d7cc1ab06b2586.png" /></p><p></p><p>尽管 RAG 有着广阔的应用前景，但在实际落地过程中，我们发现了许多问题。一篇研究论文总结了 RAG 应用中常见的 12 个问题，包括：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/34/34250887515723e722d9d839be11a949.png" /></p><p></p><p>内容缺失：检索结果未能覆盖用户的问题。不完整回答：生成的内容缺乏细节或关键信息。性能不足：检索和生成过程耗时过长，无法满足实时性要求。不可扩展性：面对海量数据时，系统的性能显著下降。</p><p></p><p>等等这些问题的存在，使得很多 RAG 产品难以从原型（MVP）走向真正满足市场需求的产品（PMF）。</p><p></p><p>在 AI 产品的开发中，最小可行产品（MVP） 的构建相对容易，但要实现产品市场契合（PMF），真正满足用户需求，却充满挑战。</p><p></p><p>理解大模型的技术边界：既不要过度高估大模型的能力，也不要忽视其潜力。明确其适用范围和限制。深入理解业务场景：识别大模型和 RAG 技术最适合的应用领域，确保技术与业务需求的契合。优先使用最好的模型：在验证方案可行性时，尽可能使用性能最优的模型。如果最佳模型仍无法满足需求，可能需要重新评估方案。构建产品壁垒：考虑产品的独特价值和竞争优势，而不仅仅是技术实现。确保产品在市场上具有可持续的竞争力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b8/b82f81c01944fbf294ca606ef89389f7.png" /></p><p></p><p>从第一性原理出发，要解决上述问题，我们需要回归本质，关注 RAG 流程中的关键环节：</p><p></p><p>高质量的知识库构建：确保文档解析的准确性，将领域知识准确地转化为文本和向量表示。精确的检索机制：利用先进的嵌入模型，提高在向量空间中检索相关内容的准确性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b5/b5974b2bffb9b8fc3efa3ba2fafdd91d.png" /></p><p></p><h3>文档解析技术</h3><p></p><p></p><p>在大模型 RAG 应用中，文档解析是一个关键环节。如何将复杂的 PDF 文档准确、高效地转换为大模型能够理解的格式，直接影响到模型的性能和应用效果。下面将深入探讨通用文档解析的挑战与解决方案，重点介绍从 &nbsp;PDF &nbsp;到 Markdown 的转换过程，以及如何通过物理和逻辑版面分析，实现对各种复杂文档的高质量解析。</p><p></p><p>从计算机的角度来看，文档主要分为两类：</p><p></p><p>有标记的文档：如 Word、Markdown、HTML、JSON 等。这些文档具有明确的结构，计算机可以直接解析和理解。无标记的文档：如图像、 PDF &nbsp;等。这些文档缺乏结构信息，计算机无法直接理解其内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bf/bfeeb1ee9e5f774b0ac7e4d6d0e80be8.png" /></p><p></p><p>PDF 是一种常见的文档格式，其本质是一系列显示打印指令的集合，而非结构化的数据格式。例如，一个简单的“ Hello World ” &nbsp;PDF &nbsp;文件，用文本编辑器打开后会发现大量的排版指令。这使得计算机在解析 &nbsp;PDF &nbsp;时，难以直接获取其中的结构化信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/70/70aa252a5a9e0ce4dd469ac3085f99c4.png" /></p><p></p><p>解析 PDF 的挑战有：</p><p></p><p>结构缺失：PDF 以视觉呈现为主，缺乏逻辑结构，难以直接提取段落、标题、表格等元素。复杂排版：多栏布局、跨页内容、嵌入图像和公式等，增加了解析难度。元素遮挡：印章、手写批注等覆盖在文本上，干扰内容识别。</p><p></p><p>为了让大模型有效地理解 PDF 中的内容，需要将其转换为一种模型熟悉的格式。Markdown 成为了最佳选择，原因如下：</p><p></p><p>结构清晰：支持多层标题、粗体、斜体、列表、表格、数学公式等，能够完整地表达 PDF 中的信息。专注内容：Markdown 关注内容本身，而非排版，符合大模型的训练特点。模型友好：大模型在训练过程中，接触了大量的 Markdown 语料，对其有良好的理解能力。</p><p></p><p>Markdown 的优势：</p><p></p><p>简洁优雅：语法简单，易于编写和阅读。广泛应用：在技术文档、博客等领域被广泛使用，具备良好的生态环境。易于转换：可以方便地转换为 HTML、 PDF 等格式，满足多种需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fa/fa8e09f8b1e93b07e92a80ad9aad063e.png" /></p><p></p><p>复杂文档布局的解析难点有很多，比如：</p><p></p><p>法律条文：包含多级标题和列表，需要准确识别层次关系。扫描书籍：可能存在噪声、阴影，OCR 识别难度大。学术论文：包含复杂的公式、表格和引用格式。产品说明书：双行表头、多栏布局，对解析精度要求高。</p><p></p><p>有以下的典型挑战：</p><p></p><p>多栏布局双列、三列布局，跨页内容，如何保持正确的阅读顺序？复杂表格合并单元格、嵌套表格，需要精确解析结构。公式和图像数学公式、图像与文字混排，要求高精度的识别和定位。元素遮挡和噪声印章、手写批注、扫描噪声，干扰内容的提取。</p><p></p><p>现有解析方法具备一定的局限性。基于规则的解析库，典型代表 PDF Miner、Py PDF 、Mu PDF 等。优点对电子版 PDF 的解析效果较好，速度快。缺点是无法处理扫描件和图像型 PDF 以及对复杂布局支持不足，难以准确还原阅读顺序。</p><p></p><p>基于深度学习的解析库，典型代表 Unstructured、Layout-Parser、PP-StructureV2 等。优点是能处理扫描件，具备一定的版面分析能力。缺点是效率较低，处理大型文档耗时长以及对复杂布局的支持有限，精度有待提高。</p><p></p><p>基于大模型的解析库，典型代表 GPT- PDF 。优点是具备多模态理解能力，直接处理图像和文本。缺点是计算资源消耗大，实时性差以及受限于大模型的输入长度，对长文档支持不足。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/dd/dd206130c101f73244e5def5a949748c.png" /></p><p></p><p>针对上述挑战，我们提出 TextIn 文档解析技术：</p><p></p><p>解析更稳：无论何种排版和格式，都能正确提取文档元素，并还原正确的阅读顺序。识别更准：高精度地识别文本、表格、公式、图像等内容。性能更优：满足实时性需求，支持大规模文档的快速解析，适用于 RAG （检索增强生成）应用场景。</p><p></p><p>具备以下功能特性：</p><p></p><p>全面支持兼容电子版和扫描版 PDF 。预处理优化文档图像预处理，提高 OCR 精度，减少噪声干扰。深度分析物理版面分析和逻辑版面分析相结合，准确还原文档结构。元素识别全面识别段落、标题、表格、公式、页眉页脚等内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b5/b5f658bbcef5ecbfd6040a7fc3903e40.png" /></p><p></p><p>我们的解析流程主要分为三个阶段：</p><p></p><p>1. 文档预处理</p><p></p><p>拆分文档将多页文档拆分为单页，便于并行处理。</p><p></p><p>类型识别区分电子版和扫描版，选择适当的处理策略。</p><p></p><p>2. 版面分析</p><p></p><p>物理版面分析基于视觉特征，划分页面区域，如段落、列、图像、表格等。</p><p></p><p>逻辑版面分析基于语义特征，构建文档的目录树，确定元素的层次关系和阅读顺序。</p><p></p><p>3. 内容重建</p><p></p><p>元素识别对文本、表格、公式、图像等进行精确识别。</p><p></p><p>格式转换根据需求生成 Markdown 或 JSON 格式，方便大模型理解或产品呈现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/55/5510c5bc6a5f1bc87e340f80b84235fb.png" /></p><p></p><p>物理版面分析逻辑如下：方法选择采用单阶段检测模型，兼顾精度和效率，避免复杂的多阶段流水线。数据处理注重数据的多样性和分布，增强模型的泛化能力。层级结构按照页（Page）→ 区块（Section）→ 列（Column）→ 段落（Paragraph）→ 文本片段（Text Slice）进行解析。阅读顺序还原策略从上到下、从左到右，结合页面布局和元素位置，准确还原阅读顺序。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/41/41d9dfe45e277ac8b573aaa7cdbc753e.png" /></p><p></p><p>逻辑版面分析逻辑如下：模型架构采用 Transformer，利用自注意力机制，捕获元素间的全局关系。父子关系确定标题与其子内容的从属关系，旁系关系识别同级元素之间的顺序关系，目录树构建通过关系预测结果，生成文档的逻辑结构树，方便内容的组织和导航。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/06/06827dd6664302ae66567804bc3c762a.png" /></p><p></p><p>面对多种复杂的文档解析需求，如何客观评估不同解析方法的效果，是一个重要问题。传统的文字和表格评估方法，无法全面衡量复杂布局和结构的解析质量。我们开源了 Markdown Tester，指标定义针对段落、标题、表格、公式、阅读顺序等核心元素，定义了识别率、召回率、F1 值等评价指标。结构评估引入树状编辑距离，评估目录树、表格结构与原文档的相似程度。可视化展示通过雷达图等方式，直观展示各解析方法在不同指标上的表现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/80/80d590390866010b27cf26fdad9af4f0.png" /></p><p></p><p>在实际应用中，解析效率至关重要。特别是在 RAG 场景下，需要对大量文档进行实时解析，要求解析过程既快又准。我们进行了以下优化策略，算法模块优化针对耗时模块，如版面分析和元素识别，优化算法，提高执行效率。并行处理采用分布式集群结构，对文档解析流程进行并行化处理，充分利用计算资源。资源调度智能调度计算任务，平衡负载，减少等待时间。</p><p></p><p>我们目前实现处理速度实现了对 100 页文档的解析，P90（90% 请求的响应时间）小于 1.5 秒。稳定性在大规模文档解析任务中，保持了高稳定性和低失败率。扩展性系统设计支持水平扩展，能够应对更大的数据量和更高的并发需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f5/f59d7f67105e3efd4ac8fb6e3d02f0ff.png" /></p><p></p><h3>向量化技术</h3><p></p><p></p><p>在现代自然语言处理任务中，向量化（Vectorization）技术扮演着关键角色，特别是在检索增强生成（ RAG ）模型中。下面将探讨向量化的原理、模型选择以及多尺度训练方法，揭示其在实际应用中的重要性和优化策略。</p><p></p><p>向量化的核心思想是将大量的文本数据转换为具有方向和数值的向量列表。这种表示方式使计算机能够利用矩阵运算的高效性，快速计算文本之间的语义相似度。在 RAG 模型中，向量化主要用于信息检索，即通过计算用户问题和文档块的向量相似度，找到最相关的文本内容。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/eb7e0e2688e17932b64b45ce0c4adb86.png" /></p><p></p><p>在选择嵌入（Embedding）模型时，许多人可能会在网上查找评价效果较好的模型，或者自行进行评测。然而，在评估之前，建议先查看像 Hugging Face 上 MTEB 和 C-MTEB 评测基准排名，这些指标涵盖了文本分类、聚类、成对分类、重排序、检索等任务，能够大致了解每个模型在不同任务上的表现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bce977c0a4791e061fea193c5567e76d.png" /></p><p></p><p>在选择合适的嵌入模型时，需要考虑以下六个关键因素：</p><p></p><p>1. 特定领域适应性：嵌入模型的训练依赖于大量语料，不同领域的语料量和质量会影响模型在该领域的表现。例如，在工业领域，如果训练语料较少，模型的性能可能不理想。因此，即使某个模型在排行榜上名列前茅，也应在自己的领域数据上进行测试。</p><p></p><p>2. 检索精度：对于以检索为核心的任务，模型的检索精度至关重要。需要评估模型在实际检索场景下的准确率和召回率。</p><p></p><p>3. 语言支持：根据业务需求，选择支持所需语言的模型，确保在多语言环境下的有效性。</p><p></p><p>4. 文本块长度（Chunk Size）：文本块的长度应根据任务需求进行调整。如果处理的是信息密度较低的长文本，如小说，可能需要更长的文本块。而对于信息密度高、需要精确检索的内容，较短的文本块可能更合适。</p><p></p><p>5. 模型大小与推理成本：模型的大小直接影响推理的资源消耗和成本。在资源有限的情况下，需要在模型性能和资源消耗之间取得平衡。</p><p></p><p>6. 检索效率：在需要处理海量数据的场景下，检索效率变得尤为重要。模型的计算复杂度和向量维度都会影响检索速度。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f5702e9fd4f5856f7e07993f5f3ef26.png" /></p><p></p><p>我们的模型采用了对比学习的方法，这与业界领先的实践保持一致。其中一个较为特殊的点是引入了多尺度训练（Multi-Resolution Learning，MRL）。在训练过程中，我们采用不同的尺度，如 1024、512、256、128 等长度的 Token，计算整体的损失函数。这样做的目的是：</p><p></p><p>保证信息质量集中在前序序列：通过多尺度训练，模型能够在较短的序列中捕获高质量的信息，确保在截断或限制序列长度时，重要信息不会丢失。提高检索效率：在资源受限或需要高效检索的情况下，可以使用较短的向量表示，而不会显著降低精度。减少维度，保持精度：通过多尺度训练，模型在降低向量维度的同时，尽可能保持了语义表示的准确性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b454b86963e0ce94239d0a3c4e34874d.png" /></p><p></p><p>综上所述，向量化技术在 RAG 模型中具有不可替代的作用。通过慎重选择嵌入模型，结合多尺度训练等优化方法，我们能够在提高检索精度的同时，优化资源使用，实现高效、准确的文本信息检索。</p><p></p><h3>总结</h3><p></p><p></p><p>我们的目标始终是打造真正可用、好用的产品，实现从 MVP（最小可行产品）到 PMF（产品市场契合）的飞跃。只有当产品真正满足用户需求，具备市场推广价值，才能称之为成功的产品。</p><p></p><p>在 RAG （检索增强生成）应用中，我们发现了两个突出的核心问题：文档解析和向量化。为了解决这两个问题，我们专注于以下研究方向：</p><p></p><p>1. 通用文档解析</p><p></p><p>我们致力于实现快速、精准、稳定的文档解析，支持各种类型的文档，包括扫描件和多种版式。通过提高解析的稳定性，确保阅读顺序的正确，还原文档的真实结构。同时，提升解析的精度和效率，使解析过程更准确、更高效。</p><p></p><p>2. 高性能的嵌入模型（Embedding）</p><p></p><p>在向量化过程中，我们注重模型的精度和效率。选择最适合业务需求的嵌入模型，能够更高效地进行信息检索，提高检索的精确度和速度。这不仅包括模型本身的优化，还涉及对领域数据的深入理解和应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1a/1a9d7a8aa394df8d78094630fe69a18c.png" /></p><p></p><p>我们相信，专注于问题的本质，才能取得最大的收益。如果您对我们的技术和产品感兴趣，欢迎访问我们的官网 textin.com。在网站上，您可以了解更多关于通用文档解析、嵌入模型技术，以及我们的财报分析、分析师问答等产品信息。</p><p></p><p>感谢您的阅读和关注。</p><p></p><p>作者介绍：</p><p></p><p>常扬，合合信息智能创新事业部研发总监，复旦博士，复旦机器人智能实验室成员，国家级大学生赛事评审专家，多个技术社区 AI 专家博主，发表多篇 SCI 核心期刊学术论文，负责合合智能文档处理业务线的产品、技术、云服务平台研发工作，研究方向智能文档处理、多模态大模型。任职期间，先后主导了人工智能数据清洗平台，卡证识别、票据识别、行业文档定制等信息抽取产品，TextIn 智能文字识别云服务平台，TextIn 票据机器人、财报机器人、合同机器人等智能文档场景落地产品，为金融、制造、物流等行业提供智能文档处理产品与解决方案，在企业信息化转型领域具备丰富的技术落地经验和行业场景洞察力。</p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 9 折优惠，单张门票立省 480 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/68/68a4f559d6682dec46bd5633588299f0" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/C3aMcJwyQTcAUfI0VKZg</id>
            <title>算力存力Buff都叠满，至强6最强形态现身！</title>
            <link>https://www.infoq.cn/article/C3aMcJwyQTcAUfI0VKZg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/C3aMcJwyQTcAUfI0VKZg</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Sep 2024 03:17:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>文章来源：英特尔</p><p></p><p>2024年是至强的大年。</p><p></p><p>先于6月正式发布的至强®️&nbsp;6700E系列开启了全新的、更为简洁命名方式：至强®️&nbsp;6能效核。144核的规格也意味着英特尔在最近几年当中首次在核心数量方面实现了领先。而且，这还并不是至强6的最强形态，毕竟大家都知道还有个6900P系列嘛。</p><p></p><p>9月26日，至强6这个“最强形态”终于正式发布，主要规格非常震撼。即使面对今年内晚于自己发布的其他厂商同级别CPU，至强®️&nbsp;6900P的已有规格也战力十足。</p><p></p><p></p><h3>最强至强能有多强？</h3><p></p><p>英特尔代号Birch Stream的新一代服务器平台所采用的至强6处理器是分批次发布的。6月发布的是代号Sierra Forest的能效核处理器6700E系列（E后缀即Efficiency Core，能效核的标记），目前发布的是代号Granite Rapids的性能核6900P系列。今年底和明年初还会陆续发布6900E、6700P，以及6500/6300等。未来的Intel 18A制造工艺的处理器，如Clearwater Forest，也会继续用于Birch Stream平台。</p><p></p><p>至强6900P是英特尔专为计算密集型工作负载设计的处理器，也是Granite Rapids的“完全体”。后缀的“P”意味其采用的是Performance Core，即性能核，规模大、性能强；6900的数字型号则说明其核心配置拉满——提供了72到128核的多种规格，TDP有400W和500W两种，组合成已公开5种型号，显得比较简洁。当然，依照惯例，云厂商等大客户还会有若干定制型号的。单就内核数量而言，6900P系列相对前两代“Rapids”产品线顶配的56/60（Sapphire Rapids）或64核（Emerald Rapids）直接翻倍！如此巨大的迭代幅度非常罕见，也难怪英特尔要改命名方式了，由表及里都透着一个意思：厚积薄发、脱胎换骨！</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de2e05d57ed38a7b56d854b340c8ed32.png" /></p><p>&nbsp;</p><p>尤为值得一提的是：至强6900P也是业内首款性能核数量正式“破百”的产品，其他同级产品，不论是x86架构还是Arm架构都只达到了96核的水平。它们的性能核数量要追平英特尔，起码得等到下个季度。</p><p></p><p>随着内核规模增加，至强6900P的L3缓存达到了504MB。为了配合倍增的核数和显著提升的算力，至强6900系列的存力也大为增强，内存带宽方面不仅支持12通道DDR5 6400；并引入了新型内存MR DIMM，把数据率大幅提升至8800MT/s，基本内存带宽可以达到第五代至强可扩展处理器的2.3倍。另外，至强6还支持CXL 2.0，尤其是包括Type 3设备（也就是CXL内存），可以进一步扩展内存容量和带宽。</p><p></p><p>至强6900P的UPI2.0链路也有很大改进，速率提升到24GT/s，数量增加至6条，使得双路互联效率进一步提升。结合内核数量、内存带宽等方面的全面提升，至强6900P可以被视作高算力+高存力平台的最强机头，不论是科学计算，还是AI集群。根据已透露的测试，至强6900P平台的数据库、科学计算等关键应用负载的表现是上一代产品的2.31倍-2.5倍，AI应用性能是其1.83倍-2.4倍不等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e321699a67d699aa4649106cd5dafff6.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e435691b62871f08db634cfc41cedbc.png" /></p><p>&nbsp;</p><p>至强6的扩展能力也有不小的提升。其中6900系列单插座不论是性能核还是能效核均可提供96通道PCIe 5.0，双路即可提供192通道PCIe 5.0。未来上市的6700系列单路型号可以提供136通道PCIe 5.0，双/多路型号单插槽也可以提供88通道。相较而言，第四、五代至强可扩展处理器的PCIe 5.0通道数量为80。CXL支持能力方面，至强6 6900、6700系列都支持64通道CXL 2.0。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e070da2d8b6cf92a7127f23d135cd08.png" /></p><p>&nbsp;</p><p>更多的内核、更多的内存通道、更多的PCIe通道需要更大规模的插座接口支持。&nbsp;至强6带来了两种接口：LGA 4710和LGA 7529。至强6900系列使用面积较大的LGA 7529插座，提供最强大的内存带宽和扩展能力，是未来高性能、高密度服务器的基础。至强6700以及未来的6500/6300系列使用LGA 4710，尺寸与第四、五代至强的LGA 4677相仿，内存、PCIe的通道数相同或相近，有利于主流服务器内部布局习惯的延续性。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>改进的EUV：Intel 3</h3><p></p><p>核心规模的飙升首先得益于至强产品线终于获得EUV光刻机的加持。在2023年发布的酷睿Ultra已经率先使用了引入EUV的Intel 4制造工艺。而2024年发布的至强6则使用了进一步改良的Intel 3制造工艺。</p><p></p><p>2021年7月，英特尔CEO帕特·基尔辛格公布了“四年五个制程节点”（5N4Y）的工艺路线图。Intel 3的量产时间节点位于2023年底，节奏基本符合计划。从基于Intel 4制造工艺的酷睿Ultra的市场表现看，EUV的加持确实明显提升了英特尔处理器的竞争力。至强6所采用的Intel 3制造工艺相对Intel 4可以规划更多的金属层、拥有更多细分版本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14779b3281cb7116a653416e76f2f343.png" /></p><p>&nbsp;</p><p>Intel 3在更多的步骤中应用EUV光刻，可以提供更密集的设计库、更高的晶体管驱动电流。Intel 3还有三种变体，包括3-T、3-E和3-PT。Intel 3、3-T是基本工艺，主要用于CPU；3-E是功能扩展；三者都支持TSV；Intel 3的这三种变体与Intel 4相比可以提升18%的性能功耗比。而3-PT进一步增加混合键合的支持能力，带来了更高的性能并且易于使用。Intel 3所有四种节点变体都支持240 nm高性能和210 nm高密度库，而Intel 4只支持240 nm高性能库。</p><p></p><p>对于性能取向，Intel 3针对高性能运算进行优化，可以支持低电压(&lt;0.65V)和高压(&gt;1.3V)运行，且在各电压下的频率均高于Intel 4。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a87a54ced1d9949df7b1fa5edab0630.png" /></p><p>&nbsp;</p><p></p><h3>微架构大迭代</h3><p></p><p>至强6900P采用的性能核微架构代号Redwood Cove。Redwood Cove也是近年来英特尔最重要的微架构迭代，不但给服务器产品线带来了新名字，在消费类产品线同样开启了新的命名序列酷睿Ultra。</p><p></p><p>我们先快速回顾一下Redwood Cove的上一代Golden Cove/ Raptor Cove。Golden Cove其实也是非常重要的迭代，在消费类开启了大小核时代（第12代酷睿处理器），在服务器上就是第四代至强可扩展处理器。Golden Cove相对其前代的微架构大幅度提升了前端：</p><p></p><p>指令TLB翻倍，从128条增加到256条；指令提取带宽从每周期16字节翻倍到32字节；解码器从4路扩展到6路；微操作缓存从2304条增加到4096条。其他L1 BTB、L2 BTB等也有所提升。</p><p></p><p>Golden Cove的后端当然也有提升，譬如重排序缓冲区、分支目标缓冲区也有大概30%左右的提升，只是相对前端幅度不那么大。</p><p></p><p>Raptor Cove的微架构与Golden Cove差异不大，表现在实际产品上主要是缓存的提升，如基于Raptor Coved的第13代酷睿（Raptor Lake）的每核心L2缓存从12代（Alder Lake）的1.25MB提升到2MB；第五代至强可扩展处理器（Emerald Rapids）和第四代（Sapphire Rapids）每个核心的L2缓存都是2MB，但前者每个网格的末级缓存（Last Level Cache，也可继续俗称为L3缓存）从后者的1.875MB猛增到5MB。</p><p></p><p>Redwood Cove相对Golden Cove/ Raptor Cove的最重要变化是：</p><p>指令缓存从32KB增加到了16路、64KB；微操作队列从144个条目增加到192个条目；指令执行延迟降低；更智能的预取和改进的BPU；L2缓存的带宽有所提升AMX增加FP16支持</p><p></p><p>当然，Redwood Cove还有一个重大的优势就是“命好”，也就是前面提到的EUV制造工艺。但即使有革命性的制造工艺加持，至强6性能核也没过分扩张每个内核的规模。就至强6性能核的内核而言，每个网格节点是一个P核，每个P核配置私有的2MB L2缓存，以及共享的4MB 末级缓存。虽然平均到每个核的缓存容量并不比上一代至强（Emerald Rapids）多，但胜在总核数翻倍后。至强6性能核每个处理器可共享的末级缓存总容量依旧达到504MB，远超第五代的320MB和第四代的112.5MB。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d1fa038f2581537a12c7732d3054ea7.png" /></p><p>&nbsp;</p><p>在此也顺便提一下至强6能效核的微架构Crestmont。这个微架构同样出现在了酷睿Ultra的能效核当中。Crestmont是2或4个内核为一组共享L2缓存。在至强6能效核当中，每2或4个内核与4MB的L2缓存（在酷睿Ultra中则为2MB）构成一个模块，这几个内核共享频率和电压域。这个模块对应的网格还拥有可整个处理器全部内核共享的3MB的末级缓存。换句话说，虽然至强6能效核的核数更多，但实际上网格规模比至强6性能核小。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f3ebfc19f3fe206d2335db62c792362c.png" /></p><p>&nbsp;</p><p>能效核的指令缓存与性能核都是64KB，但数据缓存分别是32KB和48KB。前端的指令解码器宽度也有差异，分别为6和8宽。指令乱序执行引擎差异较大，能效核是256条而性能核是512条。能效核不支持性能核所支持的AVX-512和AMX，这也可以明显减小矢量运算单元的晶体管占用，但代价是每周期的单精度浮点运算次数有了数量级的差异。但能效核也改进了AVX2，增加了VNNI的INT8和BF16/FP16快速转换，这样在处理AI应用的时候表现也还有所改善。另外，其256位加密和1024/2048密钥也获得了能效核的支持，确保至强6平台的安全水平基本一致。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb3407041ea30e85bea0c6c5fea46a34.png" /></p><p>&nbsp;</p><p>缓存规模、前端宽度以及矢量单元的差异，使得至强6性能核和能效核有不同的定位。早先发布的至强6能效核更适合微服务等运算强度相对较轻，可在高核心数量和规模扩展方面收益的任务，以追求更高的能效、更高的机架利用率。而现在发布的至强6性能核更适合大数据、建模仿真等计算密集型和人工智能任务，为高性能优化，单颗处理器的功耗直飚500W——当然，跟同期发布的Gaudi AI加速器的新品或类似的加速器产品相比，能耗是应有的代价，有能力提升性能上限才是正经事。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>内存性能大跃进</h3><p></p><p></p><p>内存（DRAM）的数据存储依赖电容，这个特点使其微缩和提速的难度大于晶体管。因此内存并没有沾摩尔定律的光，带宽和密度的增长落后于CPU、GPU的发展。内存带宽滞后于CPU内核数量的增长导致一个长期问题：平均每个内核的内存带宽增长乏力，甚至出现倒退。譬如第三代至强可扩展处理器内核数28，内存是八通道DDR4 3200，理论上的内存总带宽为205GB/s，平均每核7.3GB/s；四代是56或60核，内存八通道DDR5 4800，总带宽307GB/s，平均每核5.5GB/s；五代提升到DDR5 5600，内核再增加到64，平均带宽改进甚微。第四、五代至强可扩展处理器虽然引入了新一代的DDR5内存，但由于内核数量相对三代翻倍，内存带宽的增长幅度还是跟不上。同时期其他厂商的CPU核数在屡屡跃进的过程当中也存在同样的问题。为了弥补内存带宽增长较慢的问题，第四代至强可扩展处理器给部分用于科学计算的型号引入了HBM，五代则大幅度增加了末级缓存的容量，并支持CXL 2.0内存扩展。</p><p>在至强6900P上，内存问题终于得到了比较好的解决。这涉及三个角度：</p><p></p><p>1、&nbsp;大容量末级缓存。前面提到过，6900P每个网格提供4MB L3，总容量达到了504MB，分别是四代的4.5倍、五代的1.6倍。而且，至强的全网格架构使得任意内核访问末级缓存的延迟相比其他厂商的一些产品有更优的表现，例如不需要跨计算单元而造成延迟剧增。这种架构效率更高的优势也是至强在核数曾落后的情况下还能打的有来有往的关键原因。</p><p></p><p>2、&nbsp;DDR5内存双管齐下提升带宽。至强6900系列支持12通道DDR5 6400，总带宽可以达到614GB/s，平均每核的带宽大致还有5GB/s的水平。6900P还支持新型内存MRDIMM，频率提升至8800MT/s，总带宽达到了845GB/s，平均每核6.6GB/s，也明显超过了前两代产品，大幅度逆转了内核数量增加、平均内存带宽不升反降的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/aef62ab4370e09604d42aa4dc3ade2e9.png" /></p><p>&nbsp;</p><p>MR（Multiplexed Rank）DIMM打开了DDR内存性能提升的新方向。DRAM通常由1到2个Rank组成，每个Rank的位宽为64位，如果考虑ECC，那就会有72或80位，但有效的数据是64位。消费类内存（UDIMM）可能只有1个Rank（颗粒数量较少的情况下），但追求大容量的服务器内存（RDIMM）基本上都至少有2个Rank。在以往的内存模式当中，一次只读取一个Rank的数据，另一个Rank暂时闲置时可以做刷新操作，以保持数据——这种轮流读取、刷新Rank的特点延续了多年。MRDIMM设计了一个数据缓冲区，通过将两个内存Rank分别读入这个缓冲区，再从缓冲区一次性传输到CPU的内存控制器，由此实现了带宽翻倍。第一代DDR5 MRDIMM的目标速率为8800 MT/s，其实每个Rank只相当于4400MT/s。现在DDR5 6400已经开始普及，因此MR DIMM的第二阶段目标是达到12800 MT/s，预计在2030年代的三代会提升至17600 MT/s。</p><p></p><p>3、&nbsp;CXL 内存扩展。第四代至强可扩展处理器开始引入CXL支持，当时是1.1版本，暂时也没有公开支持Type 3设备（也就是CXL内存）。从第五代开始正式引入了CXL 2.0，包括Type 3，可以帮助扩展内存容量和带宽。在至强6上，CXL设备的应用将更为普及，关键的CXL2.0标准设备，以及后向兼容的CXL1.1设备，预计都会陆续涌现。</p><p></p><p>这里重点说一下CXL内存的优势。CXL2.0支持链路分叉，使一个主机端口可以对接多个设备，而且提供更强的CXL内存分层支持，可实现容量和带宽扩展。至强6支持3种CXL内存扩展模式：CXL Numa Node、CXL Hetero Interleaved、Flat Memory。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e81599f831e2b05a29effc2b6069d57b.png" /></p><p>&nbsp;</p><p>在CXL Numa Node模式下，系统的标准内存和CXL扩展内存被视为两个独立的Numa节点进行控制。每个Numa节点都有自己的内存地址空间，系统软件或应用程序可以将任务分配到不同的Numa节点，从而优化内存的使用。CXL Numa Node模式适用于需要精细内存管理的应用，可以通过操作系统、虚拟机管理程序（Hypervisor）或应用程序本身来辅助分层管理内存。</p><p></p><p>Hetero Interleaved（异构交织）模式通过将系统的标准内存和CXL内存混合在一起，形成一个统一的Numa节点。每个内存地址空间中的数据可以交替存储在DRAM和CXL内存中，从而均衡内存带宽，减少延迟。异构交织模式适用于对内存带宽有高需求的应用，特别是当需要将DRAM和CXL内存结合使用时。此模式只有在配备性能核的至强6700P、6900P上才支持。假设将每颗至强6900P的64通道CXL用满，可以额外增加256GB/s的内存带宽，单处理器就可以实现TB级的内存带宽，还是相当可观的。</p><p></p><p>Flat Memory（平面内存）模式下，CXL内存和标准内存被视为单一的内存层，操作系统可以直接访问统一的内存地址空间。硬件辅助的分层管理可以确保常用数据优先存储在标准内存中，次要数据存储在CXL内存中，从而最大限度地提升内存使用效率。平面内存模式最大的价值在于无需修改软件即可利用CXL内存扩展，而且这种模式适用于所有的至强6处理器。但平面内存模式要求标准内存和CXL内存是1:1配置，这略为限制了硬件采办、升级的灵活性。整体而言，平面内存模式是至强6时期最易用、收效最直观的模式，有望成为CXL内存扩展的主要模式。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>踏上Chiplet异构之路</h3><p></p><p></p><p>至强6是至强家族首次将计算和IO芯片独立，再通过Chiplet形式封装在一起，总算是把高级封装的优势真正发挥出来了。</p><p></p><p>第四代至强可扩展处理器是英特尔的首个Chiplet设计的至强处理器。其XCC版本内部是4颗芯片通过10组EMIB对等连接，每颗芯片提供15个内核、2通道内存控制器、1组加速单元，以及UPI、PCIe PHY若干。另外，还可以通过EMIB封装4颗HBM。</p><p></p><p>第五代至强可扩展处理器使用2颗芯片封装而成，所使用的EMIB数量明显减少，相应地也节约了芯片面积。虽然内核数量略有增加，但也损失了UPI、PCIe的数量，也不再能够搭配HBM。</p><p></p><p>随着制造工艺演进，偏重计算性能和晶体管密度的处理器内核，与偏重高速信号互联的IO控制器对制造工艺的要求产生了差异，因此，典型的Chiplet设计将计算和IO分离，分别应用不同的制造工艺。英特尔在14代酷睿上便采用了这种方式，分为Compute Tile、SoC Tile、IO Tile、Graphic Tile。代号Ponte Vecchio的英特尔Data Center GPU Max利用Foveros和EMIB技术，将47个小芯片封装在一起，包括Compute Die、Base Die、Rambo、IO Die等。</p><p></p><p>至强6终于也拆分成计算单元（Compute Tile）和IO单元（IO Tile），分别由Intel 3和Intel 7工艺制造。</p><p></p><p></p><p></p><h4>计算单元</h4><p></p><p></p><p>根据收集到的信息，对于能效核，目前只出现了一种计算单元的设计，每个单元最多提供144个内核、4组内存控制器共八通道；对于性能核，则是有三种计算单元的设计，可分别用于组合高核数、中等核数、低核数的规格。</p><p></p><p>至强6900P使用了三个计算单元，每个单元43个内核、两个内存控制器，总共构成129个内核（只使用128个）和12个内存通道。这种计算单元姑且称之为单元A，三个单元A构成的处理器被称为UCC。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c645f4db511cc839f5c5ed2bd575c922.png" /></p><p>&nbsp;</p><p>未来发布的6700P核数跨度会很大，其中单路型号规划为16~80核，多路型号为8~86核。单元A有4个内存通道，两个单元A组合可以提供最高86核，下限应该不低于48核（否则屏蔽的内核数量就实在太多，也太浪费EMIB成本），这种规模的处理器被称为XCC。48核以下的中等核数被称为HCC，使用一种专门开发的单元B，每个单元提供48个内核和4个内存控制器。HCC核数的下限预计在24核左右。8和16核的6700P被称为LCC，需要使用第三种单元C，16个内核和4个内存控制器。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2f3d4bbe16902f639d0982d3db2a465.png" /></p><p></p><p>通过使用3种计算单元进行组合，至强6性能核可以构建跨度从8~128核的、非常绵密的规格。也许会有人认为，相比其他厂商只用一种规格计算单元实现扩展的设计，英特尔需要设计三颗不同的芯片的成本会更高。但我认为，这是英特尔优先考虑性能的结果。首先，至强6将内存控制器安排在计算单元中，离内核更近，延迟更低，即使因此牺牲了单元组合使用的灵活性也是值得的。其次，至强6性能核给不同规模的内核数量规划不同的网格规模，有利于降低核间的延迟，甚至，有可能LCC会针对较低的核数改用环形总线。综上，预计至强6性能核相对同等规模的其他厂商的产品依旧可能会拥有内存延迟低、缓存延迟低的优势。</p><p></p><p></p><h4>IO单元</h4><p></p><p></p><p>IO单元方面，至强6900、6700系列都使用2颗相同的IO芯片。每个IO芯片由2个IO模块、4个UIO模块、2个加速器模块，以及IO网络接口构成。每个IO模块提供x16 PCIe或CXL连接；每个UIO模块提供x24 UPI2.0，或复用为x16的PCIe或CXL；每个加速器模块提供DSA、IAA、QAT、DLB加速器各一个。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/75adaf9637d81bfb077b47965db666b7.png" /></p><p>&nbsp;</p><p>以这次发布的至强6900P为例，两个IO单元总共提供8个UIO和4个IO模块。其中6组UIO负责提供6个UPI2.0互连，剩余的2个UIO和4个IO模块正好提供6×16=96通道的PCIe 5.0。双路至强6900P的UPI不但速率高（24GT/s，高于五代的20GT/s和四代的16GT/s），连接数量也提升了50%。</p><p></p><p>对于还未发布、也是主力产品的至强6700系列，估计由于要使用规模较小的插座，只提供最多4组UPI用于多路的互联，PCIe通道也有所缩减。但即使如此，至强6700系列的单路型号在将所有UIO配置为PCIe之后，单插槽就可以提供多达136个PCIe通道，或64通道CXL。如果用单路至强6700配合半宽主板构建双节点服务器，那一个机箱内的PCIe/CXL扩展能力（272 /128）远远超过已知的任何双路服务器。这种机箱可能会成为新的池化形态，可以更高的密度提供NVMe存储、CXL内存、加速器等。</p><p></p><p>&nbsp;</p><p></p><h3>结语</h3><p></p><p></p><p>由于英特尔在14nm到10nm制造工艺的迭代过程遇到了一些问题，以致此前几代至强平台在“核战”（比拼核数）中略显被动，但这个局面在至强6上有望完全逆转，改良后的EUV制造工艺看来没有束缚至强6的实力，核心数量、缓存容量、内存带宽等关键指标全都进入领先行列，一句话总结就是算力和存力的表现全部拉满。至强6900P系列在各种项目的测试当中，其代际性能提升就都是以倍数计，而非百分之十几、几十的进步。这种形势也使得英特尔得以全面竞争科学计算、大数据、AI等领域的性能王座。</p><p></p><p>此外，至强6终于实现计算与IO的解耦，也让至强6及未来的产品线走上了正确、灵活的道路，得以充分发挥Chiplet的优势。将Chiplet视作降低成本、提高良率的手段是狭隘的。Chiplet的价值在于灵活、复用、重构。英特尔长期以来很注重细分市场的耕耘，产品线非常复杂，正确利用Chiplet可以达到事半功倍的效果。我们非常期待至强6后续产品的陆续发布能够给业界带来什么样的想象力。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>