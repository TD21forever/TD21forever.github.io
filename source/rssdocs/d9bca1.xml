<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/pW4B4xse6Nh8MDTDhIKQ</id>
            <title>在这里，一起见证 AI 时代的数智化跃迁</title>
            <link>https://www.infoq.cn/article/pW4B4xse6Nh8MDTDhIKQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pW4B4xse6Nh8MDTDhIKQ</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 09:27:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 行业变革, 数字化转型, AI加速
<br>
<br>
总结: 人工智能是推动行业变革的关键力量，数字化转型浪潮下，AI将加速数字化转型的进程。 </div>
                        <hr>
                    
                    <p>人工智能正成为推动行业变革的关键力量。正如比尔·盖茨所言：“我们总是高估未来两年的变化，而低估未来十年的变化。”</p><p></p><p>InfoQ 中国自 2007 年创立以来，已经走过了<a href="https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI"> 17 个春秋</a>"。在这 17 年里，我们一直走在科技传媒的前沿，见证并记录了众多技术革新，涵盖云计算、大数据、软件工程、人工智能等等，致力于促进软件开发及相关领域知识与创新的传播。</p><p></p><p>身处数字化转型浪潮，我们积极拥抱数字化和智能化的发展趋势，以“推动数字人才全面发展、助力数字中国早日实现”为己任，通过提供优质内容和创新解决方案，帮助企业和个人更好地应对数字化时代的挑战和机遇。</p><p></p><p>从宏观视角来看，中国经济正从工业时代向智能经济时代转型。同时，国家不断出台数字化利好政策，明确了未来的发展方向和趋势。这一切让我们坚信，AI 将加速数字化转型的进程。</p><p></p><p>过去两年，我们持续提供了涵盖<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2821025396757381122&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2247488742&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">政策解读</a>"、<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2796222049257013249#wechat_redirect">金融</a>"、<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2799153653759623170&amp;scene=173&amp;subscene=&amp;sessionid=svr_22b683ab45c&amp;enterid=1717644906&amp;from_msgid=2247488794&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">汽车</a>"、<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2806488154714161154&amp;scene=173&amp;subscene=7&amp;sessionid=undefined&amp;enterid=1717644555&amp;from_msgid=2247490745&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">工业</a>"、<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2846922034222907393&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2247486548&amp;from_itemidx=1&amp;cohttps://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2846922034222907393&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2247486548&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirectunt=3&amp;nolastread=1#wechat_redirect">消费零售</a>"等千行百业的数字化<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=3072975493978832896&amp;scene=173&amp;subscene=7&amp;sessionid=undefined&amp;enterid=1717644643&amp;from_msgid=2247490495&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">案例解析</a>"内容，这些内容覆盖了当前热门的<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=3025423431187742721&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2247488742&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">数据要素</a>"、<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=3009235848011366401&amp;scene=173&amp;subscene=&amp;sessionid=undefined&amp;enterid=0&amp;from_msgid=2247488938&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">人工智能与大模型</a>"在实际应用中的落地等主题。</p><p></p><p>2022年，我们正式推出<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2994718171993653250&amp;scene=126#wechat_redirect">《行知数字中国》</a>"视频栏目，通过实地访问包括麦当劳中国、温氏集团、宁德核电和富士康等10多家数字化标杆企业，展现他们在数字化转型过程中的策略与成效。</p><p></p><p>同年，我们启动<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2913562234176585732#wechat_redirect">《超级连麦·数智大脑》</a>"直播栏目，至今已与超过80位数字化领袖进行连线，深入探讨了各行业典型场景的挑战和成功经验。</p><p></p><p>除了数字化实践类内容，我们也特别重视企业内<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkzMzQzNjQ5Mw==&amp;action=getalbum&amp;album_id=2951282337995210752&amp;scene=173&amp;subscene=7&amp;sessionid=undefined&amp;enterid=1717644643&amp;from_msgid=2247490495&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect">数字人才</a>"的建设与培养。通过与行业专家和领先企业的合作，我们深入剖析了AIGC、大模型等前沿技术对各行业的深远影响。例如，InfoQ研究中心在今年上半年发布了AI人才粮仓模型，并于近期推出了“<a href="https://mp.weixin.qq.com/s/6i70Oki1lSosVfZmC_6IQQ">数智时代的AI人才粮仓模型解读白皮书</a>"”。</p><p></p><p>此外，通过我们关联业务线的产品「<a href="https://b.geekbang.org/">极客时间企业版</a>"」——企业级一站式数字技术学习SaaS平台，我们提供了流程化学习体验，系统化学习解决方案和全生命周期的顾问式服务，助力企业打造行业领先的数字人才团队，目前已服务超过 3000+ 家技术驱动型企业，覆盖银行、金融科技、保险、汽⻋、工业制造、互联网、移动通信、软件等行业。</p><p></p><p>今年，「极客时间企业版」首发<a href="https://b.geekbang.org/promo/aitest?id=16&amp;utm_source=geektimeservice&amp;utm_medium=message&amp;utm_campaign=aigctest&amp;gk_source=2024032713_aigctest_geektimeservice">企业AIGC应用程度测评</a>"以及<a href="https://b.geekbang.org/mall/ebook/detail/qLaK93gzr7qxNBWmw95I?utm_source=geektimeservice&amp;utm_medium=webbanner&amp;utm_campaign=aigclearningmap&amp;gk_source=2024052801_aigclearningmap_geektimeservice">AIGC人才能力建设学习地图</a>"，这系列创新工具和解决方案都是为了帮助企业在AI大模型时代构建和完善AIGC人才培养体系而设计。</p><p></p><p>值此 InfoQ 中国成立 17 周年之际，我们再次感谢每一位读者的支持和反馈。作为行业的引领者，我们也将继续引领数字化转型的浪潮，通过持续的创新和服务升级，帮助更多企业和个人在 AI 时代实现跨越式发展。</p><p></p><p>我们诚邀所有业界同仁通过关注「<a href="https://mp.weixin.qq.com/s/vVAfdqhCHcBTs_DO71Ic1g">InfoQ数字化经纬</a>"」公众号，踏上“数智1号”列车，共同探索并见证数字化时代的每一步进展，期待未来的无限可能。</p><p></p><p>横屏阅读体验更佳↓</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/45152627a3d39ed9e164f730abce6670.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z4mcTK6XujWIoHbwJE6m</id>
            <title>国内科技企业和机构发力AI研发，50余篇论文入选顶会ICML2024</title>
            <link>https://www.infoq.cn/article/Z4mcTK6XujWIoHbwJE6m</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z4mcTK6XujWIoHbwJE6m</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 08:21:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 国际机器学习大会, 论文录用率, AI经济性, 大模型蒸馏
<br>
<br>
总结: 国际机器学习大会ICML2024收录了2609篇论文，涉及AI经济性和大模型蒸馏等前沿技术。研究团队提出了KPOD蒸馏方法，成功将教师模型知识迁移到学生模型，提高推理准确率。另外，论文还探讨了提高AI效率和可信性的新技术解决方案。 </div>
                        <hr>
                    
                    <p>近日，国际机器学习大会披露了2024国际机器学习大会（ICML2024）收录的论文。本届会议共有9473篇有效论文投稿，创历史新高，组委会最终录用了2609篇，接收率为27.5%。</p><p></p><p>国际机器学习大会是全球机器学习领域的权威会议，迄今举办了41次会议。组委会成员来自卡内基梅伦大学、剑桥大学、谷歌、苹果等世界名校及全球知名科技企业。今年的国际机器学习大会将于7月21日～27日在奥地利维也纳举办。</p><p></p><p>据了解，蚂蚁集团此次有11篇论文入选，覆盖图机器学习、网络/信息安全、人工智能、计算机视觉等多个人工智能和机器学习领域的前沿主题。论文试图改进和优化的技术问题，则聚焦在提高AI的经济性和可靠性上。</p><p></p><p>提高AI的经济性，主要看是否能把大模型做“小”，是否能持续工程提效，以及是否能持续度量和优化绿色指标，这决定大模型应用能否能成为主流。</p><p></p><p>《基于关键点的渐进式思维链蒸馏方法在大语言模型（LLMs）中的应用》正是一个把大模型做“小”的研究。该文提出了一种名为KPOD的新蒸馏方法，这是一种机器学习技术，能将大而复杂的模型（教师模型）所学到的知识迁移到一个小而简单的模型（学生模型）中，以大大减少模型的规模和计算成本，同时尽可能保留原始模型的性能。研究团队以GPT-3.5-Turbo作为教师模型，以LLaMA-7B和Flan-T5系列模型作为学生模型进行思维链蒸馏，结果表明，相较现有模型蒸馏技术，KPOD能将推理准确率提升5%。</p><p></p><p>《关注边界：基于重构决策边界的核心集选取》提出了一种新的核心集构建方法，首次在ImageNet-1K数据集上实现了使用50%的数据训练，但是精度损失不超过1%。</p><p></p><p>还有几篇论文就如何提高AI在计算、训练、推理等过程中的效率，提出了新的技术解决方案。</p><p></p><p>《Ditto:&nbsp;量化感知的Transformer模型安全推理》一文针对大模型密态推理场景展开了研究，以更高的效率实现密态数据类型的切换，进而带来更优的密态推理性能。实验结果表明，Ditto可以在不显著降低模型可用性的情况下实现推理效率的提升，性能相较最新工作提升约2～4倍。</p><p></p><p>《多源噪声标签下的自感知去噪方法》一文，从理论上分析了神经网络在面对多源噪声标记时具备的对样本级噪声和源级标注质量的感知能力，进一步提出了一种名为自感知去噪的多源噪声标签学习方法，利用神经网络的自我认知能力在训练期间进行去噪；并构建了一个选择性蒸馏模块以优化计算效率。</p><p></p><p>《通过随机微分方程统一贝叶斯流网络与扩散模型》一文提出了一种新采样方法&nbsp;BFN-Solver，相比原始BFN采样器实现了5-20倍的速度提升。</p><p></p><p>提高AI的可信性，则是要让AI更准确专业、真实严谨、可控透明及安全可靠，终极目标是解决控制和“对齐”比人类聪明的模型。这对于AI在金融、医疗等严谨行业的落地应用尤为必要与重要。</p><p></p><p>《基于强化学习的检索增强大语言模型可信对齐》一文针对检索增强的大语言模型仍然会遭受幻觉困扰的问题，提出一种基于强化学习的算法“可信对齐”，&nbsp;从理论上和实验上证明了大型语言模型可以通过这种方法对齐到可信状态，并将对齐的应用场景从满足人类偏好扩展到创建可信的Agent等场合。结果表明，基于“可信对齐”的检索增强大语言模型，无需人工标注，即可在natural&nbsp;question问答任务上带来平均55%的EM提升和53%的记忆率降低。</p><p></p><p>公开信息显示，此次被会议收录论文的还有清华大学、复旦大学、上海交通大学、中国人民大学、腾讯、联想等国内知名高校及企业。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rNDOUPUd2fCg32ykpgU1</id>
            <title>“都是调用的GPT？”ChatGPT、Claude、Perplexity、Gemini 同时都宕机了</title>
            <link>https://www.infoq.cn/article/rNDOUPUd2fCg32ykpgU1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rNDOUPUd2fCg32ykpgU1</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 03:23:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 聊天机器人, 宕机, 服务器端, 社交媒体
<br>
<br>
总结: 文中提到了聊天机器人在服务器端出现宕机问题，导致用户无法正常使用，引发了社交媒体上的抱怨和评论。 </div>
                        <hr>
                    
                    <p></p><blockquote>事实证明，聊天机器人跟云端的其他服务一样容易出问题。</blockquote><p></p><p>&nbsp;</p><p></p><h2>众多服务同时宕机？！</h2><p></p><p>&nbsp;</p><p>OpenAI的ChatGPT昨天遭遇“重大中断”，导致客户无法与这家超级实验室的聊天机器人正常对话。</p><p>&nbsp;</p><p>问题始于6月4日UTC（世界协调时）07：00左右，截至UTC时间07：21结束。OpenAI方面已经承认存在问题并着手开展调查。一个多小时后，OpenAI宣称已经找到问题所在且正在“处理”。可截至UTC时间10：00，该公司向用户表示“仍在努力解决这个问题”——又一次持续数小时的中断。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c27bcc381e985404bd5044716505ac7e.png" /></p><p></p><p>&nbsp;</p><p>在中断期间，ChatGPT 的网站曾进行更新，表明服务已满负荷（显然这是个表面说法），并表示将在服务恢复时通知用户。</p><p>&nbsp;</p><p>据受到影响的用户称，问题在于ChatGPT无法响应来自移动端应用或网站的查询，这表明问题很可能出在服务器端的某个位置。而我们在要求该聊天机器人进行自我诊断时，它简短回应称出现了“内部服务器错误”。</p><p>&nbsp;</p><p>社交媒体上也很快出现了相关内容。一位用户抱怨ChatGPT反复发生中断和崩溃，表示“真的太烦人了，服务根本无法使用。”</p><p>&nbsp;</p><p>另一位用户则评论称，“OpenAI突然停工导致让很多新手汗流浃背了吧，因为他们的活干不下去了。”</p><p>&nbsp;</p><p>那些在编程工作中使用聊天机器人建议的开发者，更是首当其冲受到影响。有人评论称，“是谁搞垮了OpenAI？我还有代码没写完呢。”</p><p>&nbsp;</p><p>只是昨天宕机的还不止是ChatGPT。</p><p>&nbsp;</p><p>Claude 网站也出现故障，导致用户无法正常访问。网站显示的错误信息为：</p><p></p><blockquote>服务器组件呈现过程中发生错误。具体错误信息在生产版本中被省略，以避免泄露敏感细节。此错误实例包含了一个摘要属性，该属性可能提供有关错误性质的其他详细信息。</blockquote><p></p><p>网站建议用户“重试”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce9d8c0e4386dc4cd342363ac154e879.png" /></p><p></p><p>&nbsp;</p><p>美国东部时间下午 12:10 之后的某个时间，Claude又开始工作了。</p><p>&nbsp;</p><p>Perplexity 网站也出现了与 Claude 网站类似的超负荷问题，这通常表示服务收到太多请求时会出现的提示。网站上显示的消息为：“我们很快就会回来”、“我们现在收到了很多问题，并且已经达到我们的接待能力。请稍后再试。”</p><p>&nbsp;</p><p>该网站在大约 Claude 网站的宕机问题解决后恢复了正常运行，但此后一直处于间歇性上下线状态。</p><p>&nbsp;</p><p>值得注意的是，根据OpenAI的状态页面，截至美国东部时间下午 12:28 分ChatGPT仍然无法使用，并且状态被标记为“重大宕机”。</p><p>&nbsp;</p><p>而这段时间，连谷歌的Gemini一样出现了问题。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc4eabff814edb93f8664b7870b77ecf.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/4481d3cd8293253004a0ddf7d1d68197.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>难以解释</h2><p></p><p>&nbsp;</p><p>ChatGPT最近几个月的表现确实不太可靠，当然这也不能完全归咎于OpenAI。以5月23日的事件为例，ChatGPT突然失去了执行网络搜索的能力，这是因为微软Bing搜索引擎遭遇了中断，进而对OpenAI的服务造成了连带影响。</p><p>&nbsp;</p><p>关于这次宕机原因，据OpenAI介绍，他们已经于当天UTC 17:00解决了宕机问题。OpenAI方面建议，“对于在chatgpt.com上使用ChatGPT页面的用户，可能需要执行「硬刷新」操作。而对于通过Mac应用程序或者我们移动（iOS/Android）应用软件上使用ChatGPT的朋友，则不会受到影响。”</p><p>&nbsp;</p><p>监测机构VitoriaMetrics的联合创始人Roman Khavronenko则提问称，“为什么三年之前就在到处宣扬的快速数字化转型，直到今天也无法实现站点的高效规模伸缩？”</p><p>&nbsp;</p><p>“病毒式传播成为新的商业常态，可能够承载病毒式流量的网站则极为稀缺。一旦站点崩溃，企业损失的金钱要比适当投资于基础设施可扩展性与可观察性建设成本高得多。既然大家都说数据是现代企业的命脉，为什么却仍没有得到妥善管理？”</p><p>&nbsp;</p><p>另外，我们知道Anthropic将使用AWS，OpenAI可能使用的是Microsoft或自建集群，Gemini则使用的是自家的谷歌云服务。当他们使用不同的服务提供商，还能同时都宕机，这确实有点说不过去。由于各家都只有道歉没有故障解释，网友们就有了各种分析。</p><p>&nbsp;</p><p>一个搞笑的说法是这些服务被按下了“测试开关”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a363a561a116170d5a15ba87104d9f6e.jpeg" /></p><p></p><p>&nbsp;</p><p>另一个说法是这些都是GPT的包装，虽然很荒谬，但支持这种说法的人还挺多：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/67789415bbbefd178c5bb8ff2983b7a5.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b0bcd9f5db7997130f0741054133806.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c238ec66ab28ff7a0846aa68d524f0b5.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b57205e5121d9b411a0f16f5780ad1e.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/34/348b0ce17f5b602d69cac2949e7b5268.jpeg" /></p><p></p><p>&nbsp;</p><p>还有一个是美国计算机科学家James B的比较正式的分析：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d32264fa82bc36b7c6fb79421b1f357e.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><blockquote>导致同时宕机的可能原因：基础设施问题: 这些 AI 模型依赖的基础设施出现大范围问题可能是罪魁祸首。这可能包括云服务提供商（例如 AWS、谷歌云平台或 Azure）出现故障，而这些服务商正是这些模型运行所依赖的。互联网层面问题: 另一种可能性是互联网层面出现问题，例如大规模的 DNS 宕机或网络路由问题，这些问题会同时影响多个服务。此类问题会扰乱跨越不同地区连接服务器的稳定性。因宕机导致的高流量: 也不排除最初一个服务（例如 ChatGPT）宕机，导致其他服务（Claude AI 和 Perplexity AI）突然流量激增，从而使它们的系统不堪重负并引发后续宕机。以上因素单独出现或混合发生都可能导致观察到的同时宕机事件。</blockquote><p></p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/ai_for_success/status/1798020882805477599">https://x.com/ai_for_success/status/1798020882805477599</a>"</p><p><a href="https://x.com/BenjaminDEKR/status/1798017325180068200">https://x.com/BenjaminDEKR/status/1798017325180068200</a>"</p><p><a href="https://techcrunch.com/2024/06/04/ai-apocalypse-chatgpt-claude-and-perplexity-are-all-down-at-the-same-time/">https://techcrunch.com/2024/06/04/ai-apocalypse-chatgpt-claude-and-perplexity-are-all-down-at-the-same-time/</a>"</p><p><a href="https://www.theregister.com/2024/06/04/openai_chatgpt_outage/">https://www.theregister.com/2024/06/04/openai_chatgpt_outage/</a>"</p><p><a href="https://www.techradar.com/news/live/chatgpt-is-down-heres-what-we-know-about-the-outage-so-far">https://www.techradar.com/news/live/chatgpt-is-down-heres-what-we-know-about-the-outage-so-far</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JDQ1rO5gAACNvZZU2SFi</id>
            <title>抖音Android端图片优化实践</title>
            <link>https://www.infoq.cn/article/JDQ1rO5gAACNvZZU2SFi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JDQ1rO5gAACNvZZU2SFi</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 03:17:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 抖音, BDFresco, 图片优化, 内存缓存
<br>
<br>
总结: 本文介绍了抖音在Android端图片优化历程中的经验和实践，主要围绕字节自研BDFresco图片框架展开，探讨了优化思路、优化过程以及具体举措，重点关注了内存缓存的优化。 </div>
                        <hr>
                    
                    <p>本文从抖音Android端图片优化历程着笔，主要介绍字节自研BDFresco图片框架及其在抖音的最佳实践、经验沉淀、业务价值。通过分享业务视角遇到的一些问题和我们的解决思路，希望能抛砖引玉，为遇到类似困扰的伙伴们提供有价值的参考。</p><p></p><p></p><h2>背景介绍</h2><p></p><p></p><h4>抖音为什么要持续优化图片能力</h4><p></p><p></p><p>图片能力作为抖音最基础的能力之一，服务于抖音各个业务。随着抖音图文、电商、IM等多图业务体量的增长，图片加载量级越来越大，对应的图片带宽成本也在日益增加。为了降低图片成本、提升用户浏览图片体验，需要持续不断的探索和优化图片能力，在保证图片展示质量的前提下，提升图片加载速度，降低图片整体成本，实现图片的 "好快省"。</p><p></p><p></p><h4>BDFresco简介</h4><p></p><p></p><p>BDFresco是火山引擎veImageX团队基于开源Fresco拓展优化的Android端通用基础的网络图片框架，主要提供图片网络加载、图像解码、图片基础处理与变换、图片服务质量监控上报、自研HEIF软解、内存缓存策略、云控配置下发等能力，目前已覆盖到字节几乎所有App。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bffe020b1fe11c627304fa13f07cc6f8.png" /></p><p></p><p>下面将从抖音视角出发，介绍抖音基于BDFresco在图片方向做了哪些优化。</p><p></p><p></p><h2>优化思路</h2><p></p><p></p><p>一张网络图片完整的加载流程如下：</p><p></p><p>客户端通过网络获取业务数据，响应内容包括对应的图片数据，通过将图片Url数据交给BDFresco加载，正式开始图片的加载流程。BDFresco会判断当前图片是否在内存缓存及磁盘缓存，若存在则执行对应解码或渲染操作，若不存在则直接走veImageX-CDN下载，将图片资源下载到本地后再进行解码和渲染操作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/40/405bccf191fb6c1ebbe4cb9b0180c97a.png" /></p><p></p><p>图片加载过程不仅占用了客户端内存、存储和CPU等资源，也消耗了网络流量和服务端资源。</p><p></p><p>图片的加载流程本质上是一个多级缓存逻辑，可以将图片加载流程拆分成4大核心阶段，内存缓存、图片解码、磁盘缓存、网络加载，结合指标监控体系，分别针对各阶段进行优化：</p><p>内存缓存优化：当前Android内存缓存命中率高达50%，内存缓存以占用App宝贵的内存为代价，使得我们可以快速地访问图片；但内存缓存的存在并不会直接导致App的OOM或者卡顿情况变严重，相反，根据特定场景配置合理的内存缓存配置能够减少图片频繁的解码和内存申请，甚至可以带来OOM和ANR的优化。图片解码优化：当内存缓存失败后，图片文件会进行解码，最终以bitmap形式在内存中存在，目前解码后的bitmap平均大小为800KB，90分位为5MB，99分位更是高达夸张的11MB，解码流程需要频繁申请内存，同时有超过15%的图片存在一倍尺寸的浪费，对客户端的性能影响非常大，因此如何减少解码阶段的内存申请是我们需要重点解决的问题。磁盘缓存优化：尽管对比内存缓存命中率，磁盘缓存命中率只有10%，但理论上内存中的bitmap在磁盘中都有对应的原始文件存在，因此想要整体缓存命中率，我们更关注磁盘缓存的优化，需要通过合理的磁盘配置，让存储空间利用率更高。网络加载优化：虽然网络阶段失败率高达2.5%，但经过数据排查和修复，实际失败率&lt; 0.1%，优化空间不多，考虑到网络加载是整体流程耗时最长的，耗时占了近90%，其中主要影响为文件过大导致的加载耗时长，因此需要重点解决下发大文件问题，优化网络加载耗时。</p><p></p><p></p><h2>优化过程</h2><p></p><p></p><p></p><h4>指标建设</h4><p></p><p></p><p>在进行图片优化之前，需要对图片整体质量完成一次数据盘点，指标建设是至关重要的一步。通过建立指标系统，能够帮助我们了解图片现状、确定优化方向和评估优化后的效果。</p><p></p><p>BDFresco提供日志上报能力，上报的图片日志经过veImageX云端数据清洗，最终可以在veImageX云端控制台查看图片质量相关指标。从触发图片加载，到内存、解码、磁盘、网络各个阶段都建立了完备的数据监控体系，覆盖各阶段加载耗时、成功率、客户端和CDN缓存命中率、文件大小、内存占用、大图异常监控等几百项指标。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6bad1e627c6a5935f38e59bf7eb6967.png" /></p><p></p><p></p><h2>具体举措</h2><p></p><p></p><p></p><h3>1 内存缓存优化</h3><p></p><p></p><p></p><h4>1.1 内存查找优化</h4><p></p><p></p><p>内存缓存原理</p><p>BDFresco是通过Producer/Consumer接口来实现图片加载的流程，例如网络数据获取、缓存数据获取、图片解码等多种工作，不同阶段由不同Producer实现类处理，所有的Producer都是一层嵌套一层，产生的结果由Consumer进行消费。一个简化后的图片内存缓存逻辑如下：</p><p></p><p>其中，读取内存或磁盘缓存是通过缓存key来进行匹配，缓存key是通过Uri做转换的，可以简单理解成cacheKey==uri，抖音在之前上线过一个缓存key优化的实验：对于同个资源的不同域名，会剔除host和query参数，即cacheKey被简化为scheme://path/name</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c7297ef2968fc045f1bc31839cafe102.png" /></p><p></p><p></p><p>优化方案</p><p>业务在进行图片加载时，BDFresco支持传入Uri数组，Uri均是同一资源，指向的是不同veImageX-CDN地址，实际上内部会将该批Uri（A-B-C）识别成同一个缓存key。</p><p></p><p>如下图所示，ABC3个Uri并不完全是按照【A全流程查找-&gt;B全流程查找-&gt;C全流程查找】的顺序执行，而是会先对ABC各进行一次内存缓存查找，再按顺序进行ABC的全流程查找。</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/968a103e384d6a278e47dedb33e79aef.png" /></p><p></p><p>由于ABC为同一资源，只是域名不同，在端上生成的缓存key一致，实际上的ABC各自的内存缓存查找为无效操作，由于该环节在UI线程执行，且抖音存在多图场景，一次滑动会触发多次图片加载逻辑，因此部分场景会导致卡顿丢帧等情况发生。</p><p></p><p>通过将多余的内存查找流程去除，对大盘帧率有明显提升。</p><p></p><p></p><h4>1.2 动静图缓存拆分</h4><p></p><p></p><p>抖音图片的内存缓存大小，是根据 java 堆内存大小来进行配置，默认大小为1/8，即32M或者64M。由于Android 8后，图片内存数据不再存储在java堆上，而是存在native堆，如果继续使用堆内存大小来进行图片内存缓存大小的配置是不合理的，因此通过将内存缓存大小*2，希望能减少解码操作，优化OOM和ANR指标。</p><p></p><p>实验后的稳定性指标显示，OOM虽然减少了，但是问题转换成了native崩溃和ANR都显著劣化，实验并不符合预期。</p><p></p><p>图片的缓存命中率和缓存大小成正相关，缓存大小越大，命中率越高，但随着缓存大小的增大，命中率提升空间会越来越小。</p><p></p><p>结合实验结果来看，单纯增大缓存大小会导致内存水位上升，引发ANR和native崩溃问题，方案并不可行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/093eeebc2b8d0e846568255918cae0fb.png" /></p><p></p><p>目前动图和静图的内存缓存使用同一块缓存块，BDFresco的缓存管理是LRU的淘汰策略，如果播放动图帧数过多，很容易把静图缓存给替换掉，重新切换回来静图就需要重新解码，重新解码势必带来性能的损耗和用户体验的降低，抖音上存在较多此类场景，如IM、个人页动静图混搭场景。</p><p></p><p>同时，考虑到直接增大内存缓存大小，命中率提升的空间不高，所以尝试将动图和静图缓存做隔离，动静图各使用一块内存缓存，能够有效地提升命中率，减少解码操作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f12adabf8e96f4e71c61d6f200eabf7.png" /></p><p></p><p>最终实验收益：</p><p>抖音通过拆分动静图缓存，单块缓存大小不变，整体缓存增大，日活显著提升，OOM显著降低，大盘帧率显著正向。抖极通过拆分动静图缓存，单块缓存大小变为1/2，整体缓存不变，日活显著提升，人均使用时长显著正向，OOM显著降低，大盘帧率显著正向。</p><p></p><p></p><h3>2 图片解码优化</h3><p></p><p></p><p></p><h4>2.1 解码格式优化</h4><p></p><p></p><p></p><blockquote>Bitmap 的内存大小 = 图片长度 * 图片宽度 * 单位像素点占用的字节数</blockquote><p></p><p></p><p>单位像素占用的字节数由颜色模式Bitmap.Config决定，即ARGB 颜色通道，主要有6种类型：</p><p>ALPHA_8：只有一个alpha通道，8bit，每个像素占1Byte；ARGB_4444：包含红绿蓝alpha4个通道，每个通道4bit，每个像素占2Byte；ARGB_8888：包含红绿蓝alpha4个通道，每个通道8bit，每个像素占4Byte；RGB_565：包含红绿蓝3个通道，其中红色占5bit，绿色占6bit，蓝色占5bit，每个像素占2Byte；RGBA_F16：包含红绿蓝alpha4个通道，每个通道8bit，每个像素占4Byte；HARDWARE：ARGB_8888的特殊配置，Bitmap会直接存储在显存中。</p><p></p><p>目前抖音主要使用ARGB_8888和RGB_565两种配置，ARGB_8888支持透明通道，且颜色质量更高，RGB_565不支持透明通道，但整体内存占用少了一半，抖音的优化思路如下：</p><p>低端机默认使用RGB_565进行解码，减少内存占用。抖音部分图片不携带透明通道，如所有的heic图，但业务指定为ARGB_8888，导致透明通道做无效占用，在内存上造成浪费，因此可以在解码阶段将不携带透明通道的图片强制降级为RGB_565，在牺牲一定程度的颜色质量下减少近一半的内存占用和解码性能损耗。由于部分bitmap的操作如圆角、高斯模糊等依赖透明通道的渲染，若强制将无透明通道图片降级成565，可能会导致部分业务无法正常展示，因此需要针对这类业务进行加白处理。</p><p></p><p></p><h4>2.2 heif解码内存优化</h4><p></p><p></p><p>优化原理：</p><p>BDFresco中heic图解码原逻辑是通过jni调用解码器的解码接口，返回解码后像素数据，返回到java层再转换成Bitmap对象展示。原逻辑中存在使用超大临时对象问题，会导致java内存开销以及GC，优化后减少大对象创建，直接在native层完成Bitmap对象构建，预期减少heif图片解码耗时，提升一定流畅度。</p><p></p><p>将原有heif图片解码流程从：</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be101cc7d36838136f9d8b6ba81441d4.png" /></p><p></p><p>优化为流程：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae26c5c1b32942321c101e27b3c50640.png" /></p><p></p><p>修复前：每个heic图片解码时使用两个大数组：</p><p>图片原始数据，大小为图片文件大小，一般在40K-700K之间图片解码后数据：大小为图片宽*高*4，一般在1-11M之间</p><p></p><p>修复后：无java层大数组使用，只使用一个40K-700K的native层的DirectByteBuffer数组。减少两个java层大数组创建，减少GC发生概率以及因为大数组创建导致的OOM问题，从而带来流畅度以及ANR收益。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/49/d9/492a55f06f3bd9cdb34fb0d30de87dd9.jpg" /></p><p></p><p>在抖音上开实验，性能相关指标均有显著提升：java内存占用减少，heic解码耗时减少，Android ANR减少，从而显著提升图文的消费市场，带动了整体使用时长收益。</p><p></p><p></p><h4>2.3 自适应控件解码</h4><p></p><p></p><p>在前面，我们提到有超过15%的图片存在一倍尺寸的浪费，导致解码阶段需要申请大量的内存，最终展示在控件上并不需要这么大的bitmap，我们通过将图片尺寸resize至控件大小后进行解码，最终解码出小分辨率的Bitmap，能够将解码内存申请极致化。</p><p></p><p>但考虑到图片浪费主要是服务端下发过大的图片，单纯在解码阶段限制大小，无法解决网络阶段的大图片问题，带宽浪费和网络加载耗时长问题仍然没有解决，因此我们将该阶段做了前置迁移，在网络加载阶段进行优化，具体方案可看4.2节按需缩放方案。</p><p></p><p></p><h3>3 磁盘缓存优化</h3><p></p><p></p><p>通过优化客户端的磁盘缓存配置来提升缓存命中率，减少图片请求量级，在提升图片加载速度的情况下，也能降低图片带宽成本。</p><p></p><p>磁盘缓存分为3种：主磁盘、small磁盘、独立磁盘；各磁盘空间存在上限，采用LRU替换算法，目前抖音主要使用主磁盘和独立磁盘，整体流程如下：图片默认存储在主磁盘，图片被替换概率较高；若业务指定独立磁盘cacheName，则指定图片会单独使用一个磁盘，被替换概率低。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c98dd4c54537fa6a0a5fdf2e51b334f5.png" /></p><p></p><p>主磁盘存储空间增大：抖音Android端存储空间上限为40M，考虑到该值为fresco的默认值，配置值主要参考当年设备的存储空间，因此可以针对存储空间较多的设备，增加图片存储配置，提升磁盘缓存命中率。实验结果表明：随着存储空间的增大，磁盘缓存命中率显著上涨，进一步带来图片量级的减少，当图片存储上限提升至80M时，Android大盘量级-5%独立磁盘推广：针对复用率高的图片场景，推荐接入独立磁盘缓存，可以减少被其他业务图片LRU替换的几率，提升图片的磁盘缓存命中率。以IM表情包为例，我们拉取IM业务的图片缓存命中率数据分析，表情包命中率仅有7%，对比同样使用独立磁盘的IM普通图片的28%和个人页主态的31%，表情包磁盘命中率偏低。将IM表情包接入独立磁盘后，表情包请求量减少27%</p><p></p><p></p><h3>4 网络加载优化</h3><p></p><p></p><p></p><h4>4.1 图片格式优化</h4><p></p><p></p><p>常见图片格式</p><p>image：原图，未经过veImageX压缩处理。JPEG：全称为Joint Photographic Experts Group（联合图像专家组），于1992发布，是一种有损压缩的光栅图像文件格式，压缩率越高图片质量越差，同时不支持透明通道。PNG：全称为Portable Network Graphics（便携式网络图形），在1997年3月作为知识性RFC 2083发布，于2004年作为ISO/IEC标准发布，PNG也是一种栅格图形格式，但支持无损压缩，同时也支持携带透明通道信息。WebP：是一种由谷歌开发的图片格式，于2010年发布，支持有损压缩和无损压缩图片文件格式，提供更高的压缩率和更快的加载速度。对比jpeg和png格式，在相同图片质量的情况下，文件体积能减少30%+，同时WebP 图片格式还支持透明通道和动画，目前抖音Android所有版本均支持Webp格式。HEIC(BVC1)：基于火山引擎自研BVC算法进行封装的图片（<a href="https://www.toutiao.com/article/6951287905268843011/?upstream_biz=doubao&amp;source=m_redirect">17项第一，火山自研编码器在MSU大赛多项夺冠</a>"），通常的文件后缀名为heic，对比Webp格式，在相同图片质量的情况下，文件体积能再减少30%+，带宽收益更加明显。但heic格式也存在缺点：由于高效编码会导致解码性能损耗略有增加，但体积较小也会带来网络耗时的降低，最终总的加载耗时基本打平或略有降低，目前抖音Android端已全量使用自研BVC软解实现解码。vvic：字节基于 BVC2算法自研的图片格式，采用的是VVC的图片编码格式，又称BVC2编码格式，对比heic的BVC1压缩率更高。</p><p></p><p>heic格式推广</p><p>当前veImageX平台支持最好的是heic编码格式，但到22年初，抖音Android端覆盖率不足50%，直接通过提升业务的heic占比能够大幅减少带宽成本，提升图片加载速度。</p><p>JPEG-&gt;heic，大幅减少带宽成本80%以上，加载速度提升30%+webp-&gt;heif，个人页动图平均文件大小-25.33%，加载速度提升30%+</p><p></p><p>在做heif动图实验推广时，发现个人页UI帧率存在大幅劣化，在高低端设备均有6-8帧的帧率下降，实验无法上线，针对该问题，我们对heif动图的解码缓存逻辑进行一次优化，提出了heif动图独立缓存优化方案。</p><p></p><p>heif动图独立缓存</p><p>动图原理</p><p>在图片文件下载完成解析成字节流，动图正式播放之前，BDFresco会进行预解码，当动图正式播放时，会根据动图调度器的播放顺序将Bitmap渲染到屏幕上，并且在播放过程中会主动预解码下一帧，如当前需要播放第5帧，会同步解码第6帧率。其中预解码操作均在子线程中进行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0bef26062da8e97239625ef4dcd26e9.png" /></p><p></p><p>不同调度器的核心区别为：当子线程预解码速度过慢，下一帧需要播放的Bitmap不存在时，是继续返回当前帧重复播放，等待子线程进行解码，还是返回下一帧，直接在主线程进行解码渲染。</p><p>SmoothSlidingFrameScheduler：默认调度器，在子线程预解码速度跟不上播放速度时，会降低动图的播放速度，如重复播放当前帧，保证不在主线程进行解码，会导致动图播放不流程，但对页面性能非常好，不会引起卡顿。DropFramesFrameScheduler：严格按照图片的时间标准进行播放，若预解解码速度太慢，则直接在主线程进行解码，以保证对应帧能够在对应时间内进行解码并且渲染到屏幕上，缺点是会在主线程进行解码，可能会引起页面的卡顿。自定义调度器：业务自定义实现getFrameNumberToRender接口，支持倒序播放、跳帧播放等特殊逻辑。</p><p></p><p>独立缓存</p><p>heif动图掉帧问题经过排查，发现heif动图采用了一个新的播放调度逻辑FixedSlidingHeifFrameScheduler：动图无任何预解码逻辑，在需要播放对应帧时，直接在主线程进行解码，即播放一帧解码一帧，这也导致了Heif动图在播放过程中需要在主线程占用大量CPU资源进行解码。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26de4904c9ef52b7566b30b101a21387.png" /></p><p></p><p>为什么heif动图必须在主线程解码呢？</p><p></p><p>对比其他动图支持任意帧解码，heif动图采用了帧间压缩的方式，引入了I帧P帧的概念，I帧为关键帧，包含了当前图像的完整信息，能够独立解码；P帧为差别帧，没有完整的画面数据，只有与前一帧的画面差别的数据，无法独立进行解码，解码需要依赖前一帧数据。</p><p></p><p>由于AndroidBDFresco的内存缓存为LRU替换，Bitmap随时有可能被回收，因此针对Heif动图的解码，必须严格按照动图顺序进行解码，否则会导致Heif动图播放过程中出现花屏绿屏等问题。</p><p></p><p>方案思考：</p><p>从源头解决，优化heif动图的编码解码逻辑，但目前Heif的帧结构就决定了解码器的解码逻辑，如果需要支持指定帧解码，就得改造Heif编码格式，方案不可行。不在主线程进行解码，专门开一个子线程做heif动图的解码，主线程需要渲染某一帧的时候，就切到子线程去解码，解码完成通知主线程做渲染，但方案对BDFresco的解码流程改造较大，且不支持内存缓存，方案待定。抖音Android&amp;iOS双端共用一个解码器，但iOS实验并无帧率劣化，原因在于iOS的图片内存缓存是可控的，不会有不符合预期的缓存释放，因此Android端可以尝试借鉴该思路给heif动图单独开辟一个新的内存缓存块，且对解码后的Bitmap进行强引用，即不会被动释放内容，也不会被其他图片LRU替换。方案优点在于能够完美复用老的解码逻辑，也支持子线程预解码，只需要将Bitmap单独缓存即可实现。由于Bitmap是强引用，缓存块也无上限，方案存在内存无限增长的可能，因此需要有一个主动释放时机，即能减少内存占用，也能保证解码顺序不被影响。因此我们尝试关联view的detach方法，当动图控件在快速滑动时，会主动释放不可见View上对应的Bitmap。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/254f3aeb5fc5dcc661124a847e3195db.png" /></p><p></p><p>经过实验，最终采取了独立缓存方案，在取得带宽收益的同时，个人页帧率无明显劣化。</p><p></p><h3>4.2 按需缩放</h3><p></p><p></p><p>背景</p><p>图片加载流程最终会将解码后的bitmap渲染在控件上，当bitmap大小大于控件时，实际对用户感官并无影响，图片最终展示的像素值不会超过控件占据的空间，当图片大小 &gt;&gt; 控件大小时：</p><p>造成一定程度的带宽浪费；图片过大，客户端性能损耗严重；不同业务对同一张图片进行图片裁剪，没有考虑图片尺寸碎片化问题，导致veImageX-CDN缓存命中率显著下降，最终造成回源成本的暴涨。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c233978d2ea3730b0cb32e865b38c5d6.png" /></p><p></p><p>解决方案</p><p>在图片展示时上报对应的bitmap和控件大小，从上报的数据来看，存在大量业务请求的图片大小远大于控件。因此，需要采用一种通用的方案，在满足图片质量的前提下，客户端提供一套控件规范，根据控件大小将图片收敛至固定大小，保证图片尺寸和展示控件基本一致，同时减少图片碎片化问题。</p><p></p><p>个人页、同城、推荐等多个业务均存在双列封面场景，这里以双列封面为例子：</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/9779f588f42fcb10c34de0369c03a01a.png" /></p><p></p><p>收益</p><p>视觉搜索场景文件大小 -83.39%，内存大小 -66.57%veImageX-CDN缓存命中率提升 + 6.99%，回源请求数减少 -23.79%</p><p></p><p></p><h3>5 异常恢复</h3><p></p><p></p><p>尽管前面我们对图片的加载流程做了一系列优化，但因为抖音本身图片量级大，部分业务如电商、IM等对图片清晰度有较高的要求，且存在图片放大和长图展示等操作，业务会进行超大图加载，直接将图片直接加载进内存，单张图片内存甚至高达100M+，无论在磁盘IO阶段，还是内存解码或者Bitmap拷贝过程中均会申请大量内存，最终导致卡顿、ANR甚至OOM崩溃，因此需要一套兜底方案来解决图片OOM频发问题，提升图片加载的可靠性。</p><p></p><p>抖音在系统内存触顶时，会通过释放图片内存来缓解压力：监听系统内存的告警回调，根据不同级别释放不同大小的图片内存缓存，降低发生OOM和ANR的几率，但因大图存在，仍然存在大量OOM。</p><p></p><p></p><h5>OOM兜底</h5><p></p><p>内存是一个全局指标，并不能直接通过OOM堆栈确定异常原因，因为OOM发生的时候内存可能处于高水位状态，有可能申请了一个小对象就直接触发异常。但关注到崩溃中Top5的堆栈大部分和图片堆栈有关系，可以合理怀疑是App内图片频繁申请大内存导致。</p><p></p><p>因此针对高频的图片解码和内存拷贝逻辑，增加兜底逻辑，当代码发生OOM，主动catch，并通过清除图片占用的内存缓存来释放部分内存，降低内存水位：</p><p>清除两级内存缓存，解码内存缓存+未解码内存缓存清除接入层缓存的动图预览帧</p><p></p><p>实验结果表明，尽管部分OOM转换成native崩溃，但整体影响用户大幅下降，实验符合预期。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>总体来看，抖音在建设了图片的全链路监控后，根据数据分析对图片加载流程做了不少优化。</p><p>提升了图片加载速度和性能减少了图片的总成本</p><p></p><p>从收益角度来看，大致可以分为成本优化和客户端体验优化两方面。成本收益主要是图片带宽成本的降低，体验收益体现在日活和OOM指标上，并且随着各种优化方案推广到更多的业务线，收益也在持续增加。</p><p></p><p>本文简要介绍了抖音基于BDFresco的图片优化最佳实践、经验沉淀、业务收益。由于篇幅所限，本文对探索历程、具体实现等细节内容有所省略，但仍希望能给业内同仁们一点启发或者参考借鉴。目前BDFresco已集成到火山引擎veImageX产品，对行业开放使用中，如需体验抖音同款图片优化能力，可以到火山引擎veImageX官网申请使用。</p><p></p><p></p><blockquote>参考：火山引擎veImageX提供端到端一站式的整体图片解决方案，包含图片及素材托管、图像处理与压缩、分发、客户端编解码及图片加载SDK全链路能力，官网地址：https://www.volcengine.com/product/imagex</blockquote><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cWVQffwgQdIg8xUmgDdF</id>
            <title>操作系统与AI融合之路再进一步！首个AI原生开源操作系统，openEuler 24.03 LTS正式发布</title>
            <link>https://www.infoq.cn/article/cWVQffwgQdIg8xUmgDdF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cWVQffwgQdIg8xUmgDdF</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 03:05:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAtom openEuler, AI原生开源操作系统, 开放原子开源基金会, openEuler社区
<br>
<br>
总结: 2024年6月6日，由OpenAtom openEuler社区主办的openEuler 24.03 LTS版本发布会在北京成功举办，汇聚了顶尖力量共探技术、生态、国际化发展，发布了首个AI原生开源操作系统。开放原子开源基金会表示充分利用开源是实现操作系统技术创新和产业繁荣的有效路径，未来将引导开源项目与产业结合。openEuler社区已吸引众多企业和开发者加入贡献，市场份额不断增长，技术和生态方面也取得了显著进展。开源4年来，openEuler不断发布新版本，稳定、安全并充满活力，致力于成为全球领先的开源操作系统。 </div>
                        <hr>
                    
                    <p>2024年6月6日，由OpenAtom openEuler（简称"openEuler"）社区主办的openEuler 24.03 LTS版本发布会（以下简称“发布会”）在北京成功举办。本次发布会汇聚操作系统产业界顶尖力量，共探openEuler社区技术、生态、国际化发展，共建健康可持续发展的开源操作系统根社区。会上，openEuler首个AI原生开源操作系统，openEuler 24.03 LTS版本正式发布。</p><p></p><p>开放原子开源基金会理事长程晓明在致辞中表示，openEuler的快速发展证明，充分利用开源、参与开源、支持开源、回馈开源，是实现操作系统技术创新和产业繁荣的有效路径。未来，开放原子开源基金会将引导openEuler等开源项目与区域产业优势结合，实现产业合作落地；同时探索在海外的发展机遇，助力openEuler走向全球。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d5d252439b8c5eb02a5452f423c8c44.png" /></p><p></p><p>开放原子开源基金会理事长程晓明</p><p></p><p>openEuler社区开源四年，至今已吸引1577家企业成员，18800多名开发者加入贡献，下载量遍布全球153个国家的2037个城市，累计装机量超过680万套。</p><p></p><p>据IDC调研报告显示，openEuler系操作系统在2023年新增服务器操作系统中市场份额已经达到36.8%。技术方面，openEuler 创新项目代码仓已经达到552 个，平均每个月产生10个创新项目。生态方面，openEuler社区充分发挥协同创新的优势，来自企业和高校的30多个项目贡献到社区孵化，激发了全产业链创新的活力。</p><p></p><p>openEuler 委员会主席江大勇表示，openEuler社区每两年发布一个长周期版本，每半年发布一个创新版本，通过创新版不断孵化和验证新的项目、新的特性，让openEuler成为一个稳定、安全并充满活力的开源操作系统社区。开源4年，openEuler技术委员会共召开了超过100次决策会议，107个SIG组共召开了2181次技术讨论会，完成17万次代码合入请求，openEuler社区共举办8次技术大会、自发举办近百场Meetup。在openEuler 24.03 LTS版本的开发过程中，国内外十多家芯片厂商，二十多家操作系统厂商，一百多家行业头部软件供应商启动原生开发，两年内共有5136名社区开发者参与到了版本的开发工作中。</p><p></p><h2>openEuler 24.03 LTS：四大升级，AI原生开源操作系统正式发布</h2><p></p><p></p><p>智能时代，AI无处不在，以大模型为代表的人工智能技术是当前技术发展的重要方向。openEuler积极和AI深度结合，让操作系统使能AI更高效，AI使能操作系统更智能。</p><p></p><p>本次发布会上，开放原子开源基金会和中国科学院软件研究所、麒麟软件、麒麟信安、统信软件、中移云能、天翼云、联通数科、华为、英特尔（中国）、超聚变、软通动力、润和软件、龙芯中科、粤港澳大湾区国创中心、AMD、新华三、飞腾信息、浙江大华、联想、华鲲振宇、中关村实验室与中科院计算所、京东科技、中软国际、凝思软件、中科方德、Linaro等26位社区伙伴代表共同见证了openEuler首个AI原生开源操作系统——openEuler&nbsp;24.03 LTS的发布。</p><p></p><p>该版本在基础设施、Linux6.6内核、智能解决方案以及全场景体验四大方面实现了全面升级，为开发者和用户带来了高性能、高可靠、高灵活的开发体验。会上，openEuler社区技术委员会主席胡欣蔚对新版本的四大升级进行了全面解读。</p><p></p><p>在基础设施方面，首次发布了openEuler软件中心，帮助用户快速掌握openEuler社区各领域丰富的软件生态；帮助开发者实时监控上游软件的更新情况，保持特性与上游实时一致。内核方面，升级到了Linux 6.6版本，CPU调度更均衡、内存管理更高效，虚拟机场景资源利用更灵活。同时，参与贡献了动态复合页等关键能力，写性能得到两倍提升。智能解决方案方面，2023年操作系统大会上，openEuler基础AI能力发布，在本次发布的24.03 LTS版本中，openEuler正式升级智能解决方案，通过大模型自然语言交互平台接入oeAware、A-ops、A-Tune实现智能调度、智能运维、智能调优。同时，openEuler更广泛地支持了南向硬件，并覆盖了主流的大模型、框架以及工具链，使能全栈AI，支持Faiss、DCN等推理和检索算法，兼容OpenViNO、Pytorch等主流AI框架，使能Numpy、ACL等基础算法库。全场景体验方面，openEuler持续从服务器、云、边缘到嵌入式夯实全场景能力。在服务器场景中，sysSentry可以将已知的CPU故障100%检出使系统更可靠；在云场景中，内存潮汐调度实现容器速度提升80%。</p><p></p><p>目前，openEuler 24.03 LTS 版本的ARM 、 x86 、 RISC-V 、 LoongArch&nbsp;四个架构镜像在社区官网、全球37个镜像点及主流云厂商分发渠道同步上线。同时，麒麟软件、统信软件、麒麟信安、移动苏研、电信天翼云、联通数科、软通动力、润和软件、凝思、超聚变、中科方德、中软国际等共12家伙伴的商业发行版将陆续发布。</p><p>&nbsp;</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0F4Ig1DlH4teqZDPqfMv</id>
            <title>Kubernetes十年：盛宴已过，长路漫行</title>
            <link>https://www.infoq.cn/article/0F4Ig1DlH4teqZDPqfMv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0F4Ig1DlH4teqZDPqfMv</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 03:05:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Kubernetes, 容器化, 云计算, 开源
<br>
<br>
总结: Kubernetes作为谷歌的容器化作业编排与管理理念，因开源而迅速占领市场，成为企业IT的基础能力。在容器技术兴起的浪潮中，Kubernetes改变了云计算行业格局，成为业界最受欢迎的容器编排平台。 </div>
                        <hr>
                    
                    <p>采访嘉宾 ｜ 张智博、张凯、李向辉、邓德源</p><p>编辑 ｜ Tina</p><p>&nbsp;</p><p>Kubernetes已经存在十年了。它本来是谷歌作为秘密武器而存在的容器化作业编排与管理理念，因为“开源”而迅速占领市场，成为了企业IT的一项基础能力，从而取得了巨大的成功。</p><p>&nbsp;</p><p>这十年里，我们看到国内外曾遍地开花的容器和编排技术创业公司逐渐沉寂，我们也看到Kubernetes从羽翼未丰发展到一家独大。在AI时代，虽然市场的关注点已经不再是Kubernetes，但无数开发者都是这个生态的受益者。</p><p>&nbsp;</p><p>见证过Kubernetes和社区取得成功的张智博是受益的开发者之一，“我原来所在的初创公司Rancher被收购后，现在我还能以高龄IT工作者的身份，在甲方的IT部门寻求一份工作，都受益于Kubernetes生态的发展。顺势而为，作为一个平凡人，在大趋势中寻求个人的发展，这是给我自己最大的启示。“</p><p>&nbsp;</p><p>Kubernetes 开源后，迅速崛起并取得了关键的领导地位。这也得益于公有云厂商积极提供 Kubernetes 托管服务，极大地降低了使用门槛，并且云原生已转变为腾讯、阿里、火山引擎等企业争夺的存量市场。因此，我们也同时邀请了阿里云张凯、腾讯云李向辉、火山引擎邓德源解读他们眼中的Kubernetes是如何改变云计算行业格局的。</p><p>&nbsp;</p><p></p><h2>谷歌的阳谋</h2><p></p><p>&nbsp;</p><p></p><blockquote>真正达到推动行业发展的“意义”层面，我认为这顶桂冠应该颁给Container，Kubernetes只是在Container潮流下的优秀产物。</blockquote><p></p><p>&nbsp;</p><p>2014年，当时的 Docker 还是一家只有 30 名员工的小公司，刚刚从 dotcloud 改名。这一年的一则重磅新闻是Docker 1.0的发布，从此容器化浪潮逐渐处于世界的核心。</p><p>&nbsp;</p><p>容器技术兴起，催生了大量容器创业公司。似乎每个大型创业公司都有一个容器编排项目，彼时资本市场对容器创业公司青睐有加，融资规模和数量持续增长。国内的一些大型企业同样都有自研的一套编排调度系统，如百度Matrix、阿里&nbsp;Sigma。还有一些企业，如国内美团和国外Twitter，使用的则是Mesos。</p><p>&nbsp;</p><p><a href="https://www.infoq.com/news/2014/06/dockercon2014/">这一年的DockerCon上</a>"，Docker创始人兼CTO在宣布libswarm时，首先给大家介绍了一波已有的“竞争对手”：Shipper、Geard、Mesos、Coreos、Consul、Helios、Centurion。这些项目及背后的企业在容器编排领域展开角逐的原因也容易理解， 那就是&nbsp;Docker&nbsp;或者容器本身的价值虽然大，但是如果想要让其产生商业价值或者说对云的价值，那么就一定需要在编排上面占据一个有利的位置。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53bcaef4248dcf4e0ea7692dd70eb665.jpeg" /></p><p></p><p>截图来源：<a href="https://www.slideshare.net/slideshow/docker-the-road-ahead/35706350">https://www.slideshare.net/slideshow/docker-the-road-ahead/35706350</a>"</p><p>&nbsp;</p><p>这个时候的谷歌已经建立了世界上规模最大、性能最强的计算机网络，遍布全球的服务器数量达到数百万台。Kubernetes的理念来源于谷歌公司内部管理着数百万服务器的Borg系统。</p><p>&nbsp;</p><p>Borg和Kubernetes都是容器编排系统，但它们在设计理念和细节上存在一些差异。最大的差异在于，Borg是从谷歌内部逐步发展起来的系统，与谷歌的基础设施高度紧耦合。因此，这个系统可以深入到业务逻辑层面，做出相应的判断。Kubernetes则是一个完全开放、可插拔的系统，不涉及内部业务逻辑的判断。</p><p>&nbsp;</p><p>在具体的设计方向上，曾为谷歌集群管理组核心成员的邓德源介绍，Borg进行更多的状态机管理。任务在Borg中以状态机的形式存在，从运行（running）到挂起（pending），有一套严格的状态流转过程，并由特定事件驱动任务的状态变化。这使得Borg的管理和调度难度较大。相比之下，Kubernetes简化了状态管理，采用了异步执行的方式，没有状态机的概念，即以异步方式进行状态管理。此外，Borg的网络设计是通过主机加端口的方式对外暴露服务，这增加了调度的复杂性和难度，经常出现问题。Kubernetes则定义了新的网络模型，例如每个Pod拥有自己的IP，Service也有独立的IP。Kubernetes借鉴并优化了许多之前的架构设计理念，形成了一个更加解耦的系统。</p><p>&nbsp;</p><p>然而，当Kubernetes团队提出开源计划时，却遭到了<a href="https://youtu.be/BE77h7dmoQU">谷歌技术基础设施高级副总裁的质疑</a>"：“开源？这能给谷歌带来什么好处？”但最终谷歌还是2014年的DockerCon上宣布了开源Kubernetes。这显然并非仅仅出于技术分享，更蕴含着其深层的战略考量。当时亚马逊在云计算领域拥有了不可撼动的领先地位，虽然谷歌拥有令人惊叹的云基础设施，但没有有力的商业云业务来与 AWS 竞争，因此，谷歌需要一种方法来提高其相关性，抵消AWS的优势。</p><p>&nbsp;</p><p>谷歌最初提出了“三级火箭”技术，即Anthos、Kubernetes和GCP。首先通过Kubernetes的开源，先奠定一个容器领域调度和编排的事实标准，当各数据中心都用上Kubernetes之后，再通过给数据中心提供Anthos达到管理Azure或AWS异构基础设施的目的，并为用户提供无缝的多云体验。</p><p>&nbsp;</p><p>基于这个目标，Kubernetes提供了容器、微服务和声明式API的理念和标准。无论是云厂商还是自家的IDC，甚至是多云环境，都可以通过它的标准API来管理底层的所有资源。</p><p>&nbsp;</p><p>随后不过三年时间，业界就结束了调度编排领域的纷争，谷歌主导下的Kubernetes迅速成为业界最受欢迎的容器编排平台，<a href="https://www.infoq.cn/article/otoewtvxnyws8ab6ydzb">阿里</a>"、<a href="https://mp.weixin.qq.com/s/6WA9BWdDPWsBWjdVm7oDeA">腾讯</a>"、字节跳动等企业也纷纷迁到了Kubernetes上。2017年底，甚至连亚马逊也发布了一款 Kubernetes 产品。而各家厂商投入Kubernetes，又进一步奠定了它不可颠覆的地位。</p><p>&nbsp;</p><p>对于云和容器厂商来说，它们会服务大量的客户，对技术的导向和用户选择的东西会更敏感。</p><p>“Rancher一直都是企业级容器产品的提供商，但是最早并没有以Kubernetes作为核心编排引擎。在2017年时，我们的生意碰到了很大困难，越来越多的客户期望使用Kubernetes，而不是我们自研的编排引擎，于是2018年我们果断放弃了之前的技术积累，将产品完全改造为Kubernetes管理平台。”张智博也提到了这一点。</p><p>&nbsp;</p><p>虽然当时Kubernetes并没有达到企业级能力，大多数用户也几乎不会把核心业务落到Kubernetes上，但开源用户的基础量级确实非常庞大，而且发展膨胀速度极快。Kubernetes技术也在开源社区的促进下不断发展，在这个过程中有两个不得不提的节点，其中一个是1.7版本中 Custome Resource Definition （CRD）机制的推出。Kubernetes通过CRD机制，把不同场景下的差异性交给用户和社区去扩展实现，基本能覆盖企业应用的不同模式和各类架构。</p><p>&nbsp;</p><p>另一个是K8s调度器在1.19后全面转向Scheduler framework架构，将Pod走过整个调度周期的每一个阶段都通过hook机制暴露出来，容许用户的plugins重新组合、编排出各种Pods与Nodes的装箱策略。自此之后，K8s调度不再局限于单个Pod的资源分配，社区很快就扩展出Batch任务调度、异构资源调度、离在线混部、面向SLO的精细化调度、面向云资源的弹性调度等等，这些丰富的调度能力，为K8s支撑更多工作负载类型提供了基础保障。大量AI/ML领域的训练和推理任务，大数据领域的Spark批、Flink流，通用分布式计算框架领域的Ray，甚至HPC领域的Slurm作业，都可以在K8s集群中调度。</p><p>&nbsp;</p><p>这些进展推动了kubernetes架构和能力的成熟，并体现出差异化优势，进而推动它从开源场景，逐步走入企业场景。</p><p>&nbsp;</p><p>落地Kubernetes最实际的收益必然是资源的节省，以及应用的弹性能力的提升。</p><p>&nbsp;</p><p>K8s首先带来的是运维效率的提升，这也是K8s最开始想解决的问题。过去，部署应用得手动操作，费时费力不说，还容易出错。有了K8s后，运维人员通过配置文件，就能一键搞定部署，升级、回滚、灰度都可以通过K8s统一的管理平台搞定。</p><p>&nbsp;</p><p>K8s另一项收益是资源利用率的提升，从VM时代5%的水平先提升到了20%，再到目前的65%左右（这也是一个大家认为的安全水位）。容器建立了比VM更细粒度的划分理念，从而能更有效地利用资源。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d04e4be0ff41db6bd2257001535ae73.jpeg" /></p><p></p><p>截图来源：<a href="https://www.youtube.com/watch?v=C2kuaqcxrJ8">2017年谷歌</a>"Kubernetes 高级产品经理 David Aronchick演讲</p><p>&nbsp;</p><p>&nbsp;</p><p>现在，无论是腾讯还是字节跳动，都通过统一的Kubernetes集群整合了数据库、AI、大数据、在线业务等多个业务，并通过混部增强了调度和单机管控能力，整个集群作为一个标准的算力支撑服务，最终提升整体资源利用率。根据腾讯的<a href="https://mp.weixin.qq.com/s/6WA9BWdDPWsBWjdVm7oDeA">公开数据</a>"，统一资源池后，整体资源利用率从12%提升至&nbsp;45% ，在离线利用率达65%，3 年累计节省 30 亿。</p><p>&nbsp;</p><p></p><h3>“多云”愿景真的成功了吗？</h3><p></p><p>&nbsp;</p><p>回顾Kubernetes发展的关键节点，最核心的事件就是K8s的开源，以及开源社区为K8s的快速发展和普及提供了支持。</p><p>&nbsp;</p><p>另一个关键节点是K8s v1.0的发布，我们认为它意味着Kubernetes真正从测试到生产可用，很多用户也逐步接受了Kubernetes。</p><p>&nbsp;</p><p>还有一个节点是&nbsp;2015&nbsp;年到&nbsp;2016&nbsp;年，在容器编排领域，Kubernetes在与Swarm、Mesos的角逐中取得了有利的地位。</p><p>&nbsp;</p><p>最后还有一个重要节点，即2018、2019年这个时间段，公有云下场提供Kubernetes托管服务，而且托管的费用极低（有些云甚至免费），极大降低了使用Kubernetes的门槛，同时也改变了这个市场的商业游戏规则。对于Kubernetes来说，作为一个开源项目如果没有商业化的支持，其实很难走得长久。</p><p>&nbsp;</p><p>现在不同的云厂商都会提供自己的基于K8s的容器平台产品，如亚马逊EKS、谷歌GKE、阿里ACK、腾讯TKE、火山引擎VKE等。值得注意的是，AWS的EKS应该是全球使用量最大的Kubernetes发行版。</p><p>&nbsp;</p><p>有观点认为这些Kubernetes托管产品应该分为两个流派：原生派和魔改派。原生派，更倾向提供原汁原味的Kubernetes，且紧跟Kubernetes的生命周期。托管的精髓在于，帮助用户解决Kubernetes的运维管理升级等问题。魔改派，更倾向把自己魔改的功能加入到Kubernetes中，打包提供给用户。</p><p>&nbsp;</p><p>而我们想象的Kubernetes，应该是一个能运行所有东西的平台，某种程度可以说是云原生应用的虚拟“操作系统”，像安卓一样暴露出统一的一套开发接口，让开发者完全无差别地交付自己的应用到世界任何一个地方。Kubernetes之所以能成功并被企业广泛接受，某种程度上也是因为它的标准性和可移植性。如果云厂商在托管产品设计上各有特色，那么这是否跟Kubernetes理念相悖？</p><p>&nbsp;</p><p>从用户角度来讲，用户确实会根据自身业务场景产生多云的诉求，比如基于稳定性的需求，需要把业务部署到多家云；或基于成本的需求，哪一家云价格便宜用哪一家；或基于安全性的需求，有些业务部署到国内的云，有些部署到国外的云。还有一些用户，会因缺乏经验和实践，导致整体效率未能达到预期。比如权限管理混乱，多租户间出现越权访问，比如在运维实践中，对K8s理解不深入，缺乏最佳实践，导致小故障引发大规模问题。所以也有些云厂商遵循标准并封装一些功能，用这些附加价值，来提升自己的竞争力。</p><p>&nbsp;</p><p>那么这看起来就是一个矛盾点，因为用户的多云价值主张跟云厂商的价值主张并不一样。每一家云厂商价值主张都是希望用户“多多地用自己的，少少地用别人的”，努力去提高自己的核心收入和利润。这也是为什么我们所看到国外的AWS、Azure和Google，国内的阿里云、腾讯云和华为云等等，他们对于多云混合云的投入其实都不那么感冒。所以，即使是实施多云，也会以自己为“主云”去实现多云管控和迁移，同时为了满足用户的需求去提供一些非标准的能力。</p><p>&nbsp;</p><p>但云厂商实现云原生的思路，一定是首先确保所有的组件都是基于标准的Kubernetes。因此，我们需要明确的是，选择权是在用户手中的：如果选择使用了更深层次的自定义服务，可能就不算是标准的Kubernetes了，它不具备跨云的迁移能力，但用户可以在上层做一层自己的封装。例如，一些客户引用了某云厂商提供的非标准Kubernetes能力，同时也在其他厂商处使用了类似的能力。这些能力并不在K8s标准接口的定义范围内。那么他们在上层会封装一层多云平台，以便统一管理底层不同云厂商的差异化能力。核心问题在于如何选择封装层次。</p><p>&nbsp;</p><p>虽然基于 Kubernetes 构建上层平台呈现出百花齐放、各具特色的局面，看似杂乱无章、缺乏规律，但这些平台本质上都围绕着两个核心诉求：抽象和插件能力管理。</p><p>&nbsp;</p><p>所以，正如张智博所言，“对于兼容和可移植性，用户只要确保不同云使用相近的Kubernetes版本，且对附加功能相对克制，那么多云之间的迁移和移植并非难事。这事和云厂商无关，主要看用户自己。”</p><p>&nbsp;</p><p></p><h2>一家独大之后的发展路线</h2><p></p><p>&nbsp;</p><p></p><blockquote>Kubernetes已经热度大减，但这并不代表它的消亡，而是以成熟稳健的能力作为一个企业级的基础设施。</blockquote><p></p><p>&nbsp;</p><p>大约在2018-2019，社区一直在强调一个词：“boring”，即“无聊”。这标志着K8s真正成为了所有企业默认认可的项目。相比较及时关注社区的每一个新功能，大家更愿意踏踏实实地研究K8s能为自己带来什么价值以及可以用来做什么。各个企业也开始在自己的环境中深度使用K8s，包括将云原生安全、云原生数据库等都迁移到K8s上。</p><p>&nbsp;</p><p>从行业周期来看，Kubernetes 相关企业必然会经历整合阶段。在这个时间段里，产生了一系列收购：VMware 收购了由 Kubernetes 两位联合创始人创办的&nbsp;Heptio，SUSE则收购了广受欢迎的Rancher。还有曾被誉为“云原生容器管理领域的创新灯塔”Weaveworks公司宣布倒闭......绝大部分容器编排项目都逐渐销声匿迹，连Kubernetes最大的竞争对手之一Mesos 也于前两年移至了 Attic 下，正式宣告“退休”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d44a58acc02a699d051ee58a313f53a.jpeg" /></p><p></p><p>截图来源：Kubernetes 领域的收购（<a href="https://kubevious.io/blog/post/acquisitions-in-kubernetes-space">Acquisitions in Kubernetes Space</a>"）</p><p>&nbsp;</p><p>现在，随着K8s的广泛应用，其增长速度已经逐渐放缓，因为用户和企业的总量是有限的。K8s已经渗透到教育、能源、泛互、制造、工业等各个行业，甚至是游戏、金融、军工，真正未被渗透的行业和企业已经非常少了。</p><p>&nbsp;</p><p>K8s的渗透和覆盖目前达到了一个相对平稳的状态。然而，K8s在纵向上的应用深度在不断加深。</p><p>&nbsp;</p><p>随着整个市场的饱和，特别是在云厂商之间，例如阿里、腾讯和火山引擎，竞争逐渐转向存量市场。在存量市场的竞争中，一个核心问题是多云环境下的兼容性尚未解决。这包括K8s的部署、不同K8s版本之间的差异化，以及如何在多云环境之间进行迁移。降低用户的迁移成本和使用成本，并在保证使用稳定性的前提下实现成本优化，是关键所在。</p><p>&nbsp;</p><p>去年11月，滴滴发生了大范围、长时间的故障。官方消息说是“底层系统软件发生故障”，但业界普遍认为这是一起由K8s的脆弱性导致的生产环境大规模故障。这类与K8s使用脆弱性相关的案例绝非只有一起，怎么对抗K8s的脆弱性，来提升整个云原生的稳定性，也是纵深发展的一个方向。目前，腾讯、滴滴、美团、阿里都是在做这种更深度的大规模的应用和实践。</p><p>&nbsp;</p><p>李向辉表示，从行业角度来看，大规模调度（包括混合部署和各层次的调度优化）以及稳定性人才的缺失，也反映了云原生应用的下一个发展阶段应如何推进。</p><p>&nbsp;</p><p></p><h3>AI场景下，K8s是否还重要</h3><p></p><p>&nbsp;</p><p>生成式AI到来后，虽然大家的关注点都转移到了大模型上，但我们也能从公开报道看到OpenAI这些企业的大模型开发并没有完全脱离K8s环境。</p><p>&nbsp;</p><p>2021年，OpenAI的工程师们公布了他们的规划——将Kubernetes扩展到7,500个节点。这个集群不仅在3年内规模增长了三倍，还被用于GPT-3、CLIP和DALL·E等大型模型，以及快速小规模迭代研究。而且，<a href="https://www.linkedin.com/pulse/kubernetes-powers-openai-infrastructure-palark/">OpenAI</a>"利用了“数以万计”的NVIDIA GPU。这个规模带来了其他工程挑战，包括辅助监控、自定义调度等。</p><p>&nbsp;</p><p>那么，为什么是K8s，而不是别的如 HPC 框架成为这些企业的选择呢？曾在谷歌从事开源Kubernetes工作的Matt Rickard发表了评论，他认为Kubernetes 虽然在某些方面存在不足，但如今容器已成为开发人员主流的部署方式，因此其开发者体验和云原生集成优势足以弥补这些缺陷。我们还可以通过邓德源的回复体会到同样的答案：</p><p>&nbsp;</p><p></p><blockquote>“在字节跳动内部，从2016年开始，我们就一直在使用K8s，因此我们没有其他选择。不论是在线服务还是离线任务，只要是技术相关的项目，只要需要使用资源，不论形态如何，都会使用K8s。因此，我们不会区分是生成式AI还是其他应用类型。对于字节跳动而言，所有的资源管理都是通过K8s进行的。因此，这是唯一的选择。”</blockquote><p></p><p>&nbsp;</p><p>K8s也存在不足。它设计之初目标支撑的主要场景是无状态类工作负载（比如Web应用和微服务），后来随着系统稳定性和存储卷管理能力的增强，很多有状态类负载也被跑在K8s上，比如数据库、分布式中间件等。到这里，K8s的核心架构都还没有碰到特别大的挑战。明显的变化发生在AI时代，尤其以深度学习为代表的AI任务，与以Spark批和Flink流为代表的大数据处理任务，被大家尝试运行在K8s集群上。而在GPU管理和推理方面，大家首先要面对的也是最大的一个问题，就是调度和资源管理。</p><p>&nbsp;</p><p>张凯认为，在资源调度方面，K8s需要能够针对各类异构设备的体系结构、软硬件协同手段和设备间的约束（共享、隔离、连接等），通过资源调度，最大化集群整体资源利用率。</p><p>&nbsp;</p><p>任务级调度方面，K8s需要能够从面向单个Pod的调度，扩展到面向一组Pods的调度，满足Pods间的各种依赖、关联和约束，提升任务整体效率。Scheduler-framework架构，就是K8s调度管理AI任务的关键改进之一。本质上，需要K8s能够高效支撑一个大规模的任务系统。从架构上，除了调度器（batch scheduler）和任务对象的生命周期控制器（job controller），还缺少重要的一个组件——任务队列（job queue）。K8s社区也意识到了这一点，孵化中的Kueue、Kube-queue等项目就在试图补上这一块关键拼图。</p><p>&nbsp;</p><p>另外，AI任务是典型的数据密集型负载，且需要GPU此类高性能计算资源支撑。而在存算分离架构下，必然要管理和优化计算任务使用数据的效率问题。原生K8s在这里还有很多缺失。CNCF社区内已经有项目在着手填补这里的能力空白，比如Fluid提供面向AI/大数据任务的弹性Dataset管理、调度和访问加速，最大化降低Data IO对GPU计算效率的影响。</p><p>&nbsp;</p><p>在训练过程中，辅助的监控和运维系统的建设并不是特别完善。尤其是在大规模训练时，如何监控GPU的功率并准确判断用户的任务是停止了还是仍在训练中，仍是一个挑战。举个例子，如果用户在训练模型时，发现模型训练框架在运行过程中突然停掉了，然而，使用传统的CPU或GPU监控方案并不能有效检测到这种情况。这里可能有一个关键指标，即GPU的功率。当GPU的功率下降时，意味着任务已经停止。在这种情况下，当任务停止后，如何快速启动新任务以加速训练进程，各大云服务提供商并没有很好地解决这一问题。这表明在大规模训练过程中，监控和运维系统的改进空间依然很大。</p><p>&nbsp;</p><p>此外，在GPU虚拟化方面，目前已有一些成熟的方案，如QGPU、VGPU、mGPU和cGPU。然而，在GPU应用场景下，很少有关于GPU利用率的数据出现。在CPU利用率方面，业界通常会提到60%或80%的利用率，但对于GPU利用率，什么情况下算是完全压榨了GPU的性能，几乎没有相应的讨论和说明。这表明在这一领域的问题仍未得到充分解决，并且缺乏完整的行业解决方案，例如，虽然我们看到Google等公司声称构建了6000个节点的GPU训练集群，但在业界很少有关于这种大规模GPU训练集群的分享或深入思考。</p><p>&nbsp;</p><p>所以在大厂里，不管是现在GPT也好，或者其他各种公司大语言模型也好，都是基于K8s来调度GPU的算力。只是现阶段，因为还没有现象级应用出现，很少有AI应用留存率非常高，很多就是个位数，所以大家更关心的是如何更快地发一个应用、如何更快地发布模型，甚至可以烧几个亿去抢夺那么几天的首发时间。也因此，现在大厂主要还是以支持AI原生应用为主，去解决一些应用层的问题，比如模型加载速度慢、如何选择合适GPU卡、模型灰度。对于万卡级别的调度和资源管理的，以“独占”的方式去运行：单独创建一个K8s集群，让你用就行了。可以说，对他们来讲，无论是GPU也好，K8s也好，都没那么重要。</p><p>&nbsp;</p><p>但再往后看的话，一旦这个事情有一定冷却，有更多的实际业务场景落地并进入商业化阶段，GPU利用率就会成为这些公司最重要的事情之一。大家就会关注在GPU场景下，怎么去做大规模算力的支撑，怎么去优化网络、存储和并行计算架构的高效使用，这也会成为整个容器或云原生应用未来探索的方向。而且它会带来大量的岗位和职业，也会给企业带来大量的利润。</p><p>&nbsp;</p><p></p><h2>写在最后：Kubernetes为什么成功了？</h2><p></p><p>&nbsp;</p><p>围绕 Kubernetes 的生态，还有一个不得不提的板块，就是 CNCF。</p><p>&nbsp;</p><p>Kubernetes 开源过程中面临的最大挑战之一是治理问题。谷歌最初承诺开源 Kubernetes，但当他们开始接纳外部贡献者时，代码控制权仍然牢牢掌握在谷歌手中。开发者必须签署一份谷歌协议，该协议授予谷歌对项目近乎完全的控制权。这种做法并非罕见，大型公司往往试图在开源社区中获取利益的同时，又牢牢掌控项目主导权。</p><p>&nbsp;</p><p>为了解决这一问题，谷歌将 Kubernetes 项目移交给了独立实体 CNCF，并将其作为 CNCF 的首个项目。这是让 Kubernetes 得以健康发展的关键举措之一。<a href="https://landscape.cncf.io/stats">CNCF</a>"已经有了741个成员，190个项目，生态版图也日益扩大。</p><p>&nbsp;</p><p>作为一个成功的开源项目，每一位贡献者都是成就它的英雄。但如果我们要论贡献度最高的，肯定是Founder：“没有谷歌，就没有Kubernetes”。</p><p>&nbsp;</p><p>如果还有一位，那么它一定是“红帽”：“没有红帽参与的开源项目都不算真正的开源项目”。</p><p>&nbsp;</p><p>在Kubernetes筹谋开源时，红帽找到了谷歌。2014 年 12 月，在谷歌正式发布 Kubernetes 项目不久，红帽官方即宣布与谷歌开展战略合作，全面投入 Kubernetes。虽然这是因为红帽想进军云计算，需要给OpenShift 一套容器管理框架，但这家从1993年就存在的企业在开源社区里，有非常好的影响力。有个说法是：如果一个开源项目不够纯粹，或者开源厂商对项目的主导权过于强势，红帽通常不会参与其中。</p><p>&nbsp;</p><p>所以Kubernetes一方面受益于谷歌的技术影响力，另一方面也受益于红帽的开源影响力。</p><p>&nbsp;</p><p>如今，无论是从技术理念、开源投入、工程师认知成本、生态发展，Kubernetes都已经变得无可替代。</p><p>&nbsp;</p><p>如果继续探寻Kubernetes取得成功的其他因素，我们还得到了以下多个答案：</p><p>&nbsp;</p><p>“100%开源，且具备强大的企业级功能。即使买不起企业级产品，自己也能搞出能力丰富的Kubernetes集群。没有人对如此低成本且符合发展趋势的产品有抵抗力，这是市场天然的选择。”</p><p>&nbsp;</p><p>“有一套标准和规范，API的设计清晰优雅，适用于各种云厂商、私有数据中心和多云环境。”</p><p>&nbsp;</p><p>“开源运作得好。依靠整个社区的力量，使其能力逐渐得到完善。”</p><p>&nbsp;</p><p>“拥有丰富的生态系统，包括 Helm、Prometheus 等众多工具和项目，跟Kubernetes贴合非常紧密，可以帮助用户解决发布、部署、监控、运维等各种问题。”</p><p>&nbsp;</p><p>“商业化和开源的平衡。云厂商的大规模应用推动了容器技术的商业化进程，商业化成功反哺了开源社区，为社区的持续发展提供了资金和资源支持。”</p><p>&nbsp;</p><p>嘉宾简介：</p><p>张智博，前SUSE Rancher 大中华区研发总监，一直活跃在研发一线，经历了 OpenStack 到 Kubernetes 的技术变革，在底层操作系统 Linux、虚拟化 KVM 和 Docker 容器技术领域都有丰富的研发和实践经验。目前在某头部车企IT部门任职。</p><p>&nbsp;</p><p>张凯，阿里云资深技术专家，阿里云容器智算方向负责人。多年云计算领域研发经历，深耕云原生技术在企业应用、微服务、AI、大数据、高性能计算等众多场景的落地。带领的团队开拓云原生AI领域，创立Fluid、Kube-Queue、GPUShare、Arena等多个相关开源项目。</p><p>&nbsp;</p><p>李向辉，腾讯云云原生分布式云负责人，主要方向为容器基础设施，多云多集群以及容器高可用的方向，主要工作为通过云原生技术加速企业现代化应用落地并实现降本增效。拥有丰富在线业务架构和应用迁移云原生经验，同时在业务高可用，成本优化，效率和业务体验提升上有较深的积累和思考。</p><p>&nbsp;</p><p>邓德源，火山引擎云原生应用平台总监，在企业云原生技术应用方面有着丰富的产品和架构经验。加入字节跳动之前，担任杭州才云科技联合创始人。曾为美国 Google 集群管理组核心成员，主要参与开发集群管理系统。</p><p>&nbsp;</p><p>&nbsp;</p><p>延伸阅读：</p><p>解读 2015 之容器篇：扩张与进化：<a href="https://www.infoq.cn/news/2015-review-container-chapter-expansion-and-evolution">https://www.infoq.cn/news/2015-review-container-chapter-expansion-and-evolution</a>"</p><p>解读 2016 之容器篇：“已死”和“永生”：<a href="https://www.infoq.cn/article/interpretation-of-2016-container">https://www.infoq.cn/article/interpretation-of-2016-container</a>"</p><p>解读 2017 之容器篇：后 Kubernetes 时代：<a href="https://www.infoq.cn/news/2017-container-Kubernetes">https://www.infoq.cn/news/2017-container-Kubernetes</a>"</p><p>为什么说 2019，是属于容器技术的时代？<a href="https://www.infoq.cn/article/R1p3H3_29f4TYImExsyw">https://www.infoq.cn/article/R1p3H3_29f4TYImExsyw</a>"</p><p>为什么说 2019 年正是云原生时代的关键节点？<a href="https://www.infoq.cn/article/y6BB98AcyFWXoNxiZ7fz">https://www.infoq.cn/article/y6BB98AcyFWXoNxiZ7fz</a>"</p><p>Kubernetes纪录片（part1）：<a href="https://www.youtube.com/watch?v=BE77h7dmoQU">https://www.youtube.com/watch?v=BE77h7dmoQU</a>"</p><p>Kubernetes纪录片（part2）：<a href="https://www.youtube.com/watch?v=318elIq37PE">https://www.youtube.com/watch?v=318elIq37PE</a>"</p><p>Kubernetes 弃用 dockershim：<a href="https://mp.weixin.qq.com/s/kIB4_qDvIIlsbs-jD47yBA">https://mp.weixin.qq.com/s/kIB4_qDvIIlsbs-jD47yBA</a>"</p><p>Mesos 项目移至了 Attic ：<a href="https://mp.weixin.qq.com/s/bQjuZLSqempKg9i4qkD3Gw">https://mp.weixin.qq.com/s/bQjuZLSqempKg9i4qkD3Gw</a>"</p><p>阿里云 13 年后重构全部核心调度系统：<a href="https://www.infoq.cn/article/otoewtvxnyws8ab6ydzb">https://www.infoq.cn/article/otoewtvxnyws8ab6ydzb</a>"</p><p>涉及数万人、历时三年，国内最大规模的云原生实践是如何打造出来的？<a href="https://mp.weixin.qq.com/s/6WA9BWdDPWsBWjdVm7oDeA">https://mp.weixin.qq.com/s/6WA9BWdDPWsBWjdVm7oDeA</a>"</p><p>字节跳动的多云云原生实践之路：<a href="https://mp.weixin.qq.com/s/3nlilkW7hh1A0h4yEJFAag">https://mp.weixin.qq.com/s/3nlilkW7hh1A0h4yEJFAag</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jDsCSirpFOOvhvjF2hMl</id>
            <title>顺丰集团CIO耿艳坤将在ArchSummit深圳站分享顺丰如何实现对40w+小哥的智慧管理</title>
            <link>https://www.infoq.cn/article/jDsCSirpFOOvhvjF2hMl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jDsCSirpFOOvhvjF2hMl</guid>
            <pubDate></pubDate>
            <updated>Fri, 07 Jun 2024 02:21:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 物流行业, 数智化, 新质生产力, 顺丰
<br>
<br>
总结: 物流行业作为传统实体行业，受限于数智化程度较低，但随着新质生产力的发展，新兴技术已经应用于物流行业，顺丰作为数字化先行者，通过应用新兴技术实现了多重目标，包括提高效率、降低成本、安全绿色等。在即将举办的ArchSummit全球架构师峰会上，顺丰将分享其数智化实践历程和智能决策体系构建经验。 </div>
                        <hr>
                    
                    <p>物流行业作为传统实体行业，因其链路长、场景复杂、涉及线下人员与设备众多的实体业因素，受限于数智化程度较低、主要依托密集的人工提升效率传统行业视角。然而，随着新质生产力的不断发展，大量的新兴技术已应用于物流行业，让这个古老的行业焕发生机。</p><p></p><p>顺丰作为物流行业的数字化先行者，近几年来通过将大数据、人工智能、数字孪生、运筹等新兴技术应用于实际的生产运营中，对规划调度、小哥管理、中转场运营等多个场景进行了深度数智化，达成了提高效率、降低成本、安全绿色等多重目标。</p><p></p><p>比如，去年顺丰科技就发布了首个在物流领域大规模应用的数字孪生实践。在物流典型的中转分拣场景，该技术已经可以实现&nbsp;1&nbsp;天内在虚拟环境验证并优化&nbsp;1000&nbsp;次分拣计划，通过向全国&nbsp;60&nbsp;多个中转场的快速复制，平均每个中转场提升了&nbsp;8%&nbsp;以上的产能。</p><p></p><p>当然，这只是顺丰诸多数智化实践成果的切面之一。据了解，在这背后，顺丰的一大成功经验是严格执行数据中台战略，并在此之上构建物流网络的智能决策体系，进而不断趋近物流供应链的“最优解”。</p><p></p><p>那么，顺丰数智化实践历程是如何步步为营的？其数据中台和物流网络智能决策体系是如何构建的？面对40+万物流小哥如何做到智慧管理？</p><p></p><p>在6月14日-15日即将在深圳举办的<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit全球架构师峰会</a>"上，顺丰集团CIO/顺丰科技CEO耿艳坤将在Keynote主题演讲中，分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5713">《产业互联网的创新实践：新质生产力塑造科技的顺丰》</a>"，他详细介绍新质生产力如何塑造科技的顺丰，与参与嘉宾共同探讨产业互联网在实践过程中的机遇与挑战。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62b68105f5f7295ef3001c7a34d04f02.png" /></p><p></p><p>除此之外，来自顺丰科技智慧供应链的多位技术专家，也将分享各自领域内的实践干货。顺丰科技智慧供应链产品负责人苏冠在14日下午举办的“AI助力工业/制造智能化”专题论坛上介绍<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5871">《智能算法在供应链优化中的应用》</a>"。</p><p></p><p>对于多工厂产能计划而言，需要同时考虑原材料采购成本、产能约束以及成品配送成本。但生产计划员很难推导出全局成本最优的排产计划，对此规划求解算法能快速的给出排产建议，使得计划时间从2天缩短到2小时，整体供应链成本优化5%-10%。</p><p></p><p>但即便如此，当前仍然不存在一个完美的算法解决所有供应链的问题。如何有机的整合不同算法的能力需要基于业务实践的供应链优化架构？苏冠将在其演讲中基于该框架总结之前的算法应用实例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/67a85cd2faa562d8067335f76870df4c.jpeg" /></p><p></p><p>与此同时，对于工业制造和物流供应链企业而言，绿色低碳也是近几年来面临的另一大考验和课题。如何通过数字化手段赋能绿色化目标？顺丰科技智慧供应链解决方案负责人曾欢将在“技术驱动商业价值转化”专题论坛分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5929">《顺丰绿色低碳实践与商业价值探索》</a>"。她将冲绿色供应链的挑战与机遇谈起，结合绿色供应链创新技术的突破及案例，介绍多年来顺丰在绿色低碳方面的实践与沉淀。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e939bb2b93581583bed9ee52c61e9de1.jpeg" /></p><p></p><p>除了顺丰之外，现场还有众多来自金融、制造、能源等行业的先行企业将从其所在行业视角介绍AI和大模型技术现阶段的实践和应用经验和效果。比如宁德核电基于大模型的&nbsp;AI&nbsp;讲师开发实践、微众银行大模型助效研发实践、天弘基金的&nbsp;AI&nbsp;Agent&nbsp;探索实践、蚂蚁集团支付宝在&nbsp;AI&nbsp;与&nbsp;LowCode&nbsp;结合方面的探索、平安壹钱包基于大模型在&nbsp;ToC&nbsp;应用层面的探索与落地等等，都非常值得关注。</p><p></p><p>大会两天日程已&nbsp;100%&nbsp;上线，更多详细议题可点击链接可查看详细日程：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><p>ArchSummit深圳开幕倒计时4天（根据发布日期更改），6&nbsp;月&nbsp;14&nbsp;日&nbsp;-15&nbsp;日，一起探索大模型时代的软件架构最佳范式。扫描下方海报二维码购票咨询，期待与你的现场交流～</p><p><img src="https://static001.geekbang.org/infoq/aa/aa46012b6f85031deb04a02cbcad5044.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RfGeqjuDP8Q1pg3sSt4Y</id>
            <title>深度探索大模型时代的软件架构最佳范式，攻略就绪、就等你来｜ArchSummit</title>
            <link>https://www.infoq.cn/article/RfGeqjuDP8Q1pg3sSt4Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RfGeqjuDP8Q1pg3sSt4Y</guid>
            <pubDate></pubDate>
            <updated>Thu, 06 Jun 2024 08:48:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 大模型, 软件架构设计, 架构师, ArchSummit
<br>
<br>
总结: 2024 年过半，AI 大模型的发展和应用进度成为关注焦点。ArchSummit 全球架构师峰会将探讨如何适应大模型时代趋势，以及架构师们如何保持竞争力。会议涵盖了多个专题论坛，聚集了顶尖专家，探讨了AI大模型在不同行业的应用实践和技术创新。同时，还有闭门交流会提供深度交流机会。整体旨在帮助架构师们适应新的时代需求。 </div>
                        <hr>
                    
                    <p>2024 年过半，AI 大模型的发展和应用进度到哪了？与之相匹配的软件架构设计有哪些新的坑点、堵点和注意点？作为架构设计大师，架构师们如何适应新的时代需求？</p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/">6 月 14 - 15 日 ArchSummit 全球架构师峰会</a>"即将在深圳举办。本次大会围绕“智能进阶. 架构重塑”主题，将探讨在 AI 浪潮下，企业架构如何适应大模型时代趋势，寻找既有应用成果又有成本效益的解决方案，帮助参会者更好地了解如何以及何时可以在架构中使用人工智能。同时，针对架构师群体而言，还将探讨在技术和知识更新换代速度加速的行业常态下，如何规划职业道路，保持自身的竞争力。</p><p></p><p>本次大会共策划了 1 个 Keynote+17 个专题论坛，聚集了国内外 100+ 顶尖专家，全面囊括 AI、大模型、云原生、数智化、降本增效、行业实践 等话题，可谓干货满满。为了帮助大家更好地锁定感兴趣议题和环节，更高效地获取大会现场内容价值，我们为大家总结了本次 ArchSummit 的六大看点。</p><p></p><h3>看点一：BAT 等头部企业 AI 大模型实力 &amp; 落地进展同台 PK &nbsp;</h3><p></p><p></p><p>来自阿里、腾讯、百度、网易、字节跳动、 火山引擎等一众技术大厂将齐聚一堂，将在现场同台 PK，秀出自己的 AI 大模型“肌肉”。</p><p></p><p>阿里巴巴 / 研究员，阿里云云原生应用平台负责人丁宇（叔同） 将在 Keynote 主题演讲中详细介绍在 AI 大模型快速发展的背景下，AI 编程的发展实现了哪些突破，以及 AI 编程助手的引入如何为软件开发带来质的飞跃。从开发者的视角出发，他将展开分享 AI 编程工具基于大模型的设计要点、难点、改进思路，帮助开发者从自身的生命力出发，学会用 AI 激活开发效率，提升生产力，而不是与之对抗。</p><p></p><p>百度深度学习平台飞桨总架构师于佃海将在 Keynote 主题演讲中分享百度在大模型技术方向的发展与应用，介绍飞桨在支持大模型开发、训练、压缩、推理和部署方面所取得的技术创新成果，以及这些创新如何助力开发者以更低的成本进行开发，并更高效地训练大模型，从而加速大模型技术的创新与应用。</p><p></p><p>此外，来自腾讯机器学习平台部、腾讯 PCG 前端技术专家苑宗鹤、腾讯云、腾讯星星海实验室多个不同条线的技术专家，将从训练框架、开发、落地应用等多个维度分享现阶段企业如何利用大模型实现降本增效的目标。</p><p></p><h3>看点二：10+AI 系列专题，深度探索大模型时代软件架构最佳范式&nbsp;&nbsp;</h3><p></p><p></p><p>大会共设计了 17 个专题论坛，其中包含 10 个 AI 相关的系列专题，从底层基础到顶层应用多角度，覆盖 AI 运维 、AI 大模型中台、AI 安全与风控及大模型算力等热门话题，解析企业如何从 0 到 1 搭建大模型时代的架构平台。</p><p></p><p>具体包括大模型基础框架、智算平台建设与应用实践、高效算力基建与性能优化、大模型应用层探索、低代码与 AI 结合、Data 4 AI &amp; AI 4 Data 探索和实践案例、AIOps 业务场景最佳实践、AI 大模型中台实践探索 、AI 时代的安全与风控，涉及近 70 场实践分享。来自 vivo、知乎、高德地图、Uber 、蚂蚁集团、eBay、货拉拉、快手、哔哩哔哩、携程等企业的技术专家将在 2 天大会中分别分享各自领域对于 AI 大模型的应用探索和进展。</p><p></p><p>大会两天日程已 100% 上线，更多详细议题可点击链接可查看详细日程：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><h3>看点三：金融 / 物流 / 制造 / 能源等行业的 AI 大模型应用实践尽览&nbsp;&nbsp;</h3><p></p><p></p><p>智能算法如何优化物流供应链？AI Agent 如何在金融行业落地应用？AI 技术如何落地大型离散制造企业？复杂且保守的核电领域如何引入大模型？顺丰集团、天弘基金、鸿海科技集团、宁德核电等企业从各行业视角介绍大模型技术的实践和应用效果。</p><p></p><p>在 Keynote 主题演讲中，顺丰集团 CIO、顺丰科技 CEO 耿艳坤将带来 “产业互联网的创新实践：新质生产力塑造科技的顺丰” 的分享。物流行业作为传统的实体行业，因其链路长、场景复杂、涉及线下人员与设备众多的实体业因素，受限于数智化程度较低、主要依托密集的人工提升效率传统行业视角。而顺丰作为物流行业的先行者，通过将大数据、人工智能、数字孪生、运筹等新兴技术应用于实际的生产运营中，对规划调度、小哥管理、中转场运营等多个场景进行了深度数智化，达成了提高效率、降低成本、安全绿色等多重目标。本次演讲将通过介绍新质生产力是如何塑造科技的顺丰，共同探讨产业互联网在实践过程中的机遇与挑战。</p><p></p><p>在 AI 助力工业制造智能化专题论坛上，来自清智优化、顺丰科技智慧供应链、鸿海科技智能制造平台、腾讯云的专家们将围绕决策管理智能优化、供应链优化、富士康数智化实践、工业 AI 质检等话题分享各自领域内的成功经验。而在创新技术在金融业的应用专题论坛上，Fabarta、广发证券、微众银行也将聚焦图智能、区块链应用、IT 架构治理等话题展开介绍。</p><p></p><p>此外，其它专题论坛中也不乏行业级的技术应用实践分享，比如宁德核电基于大模型的 AI 讲师开发实践、微众银行大模型助效研发实践、天弘基金的 AI Agent 探索实践、蚂蚁集团支付宝在 AI 与 LowCode 结合方面的探索、平安壹钱包基于大模型在 ToC 应用层面的探索与落地等等，都非常值得关注。</p><p></p><h3>看点四：3 场高端闭门，围绕当下最热话题与同行深度交流&nbsp;&nbsp;</h3><p></p><p></p><p>大会期间，我们同期策划了 3 场高端闭门交流会，主题分别是 《大模型时代的软件架构师：如何不错过这次技术革命？》、《以 Sora、Gemini 为代表的的大模型新技术大爆发，RAG 与向量数据库还顶得住吗？》《大模型创业是否会迎来新拐点？》。机会难得，名额有限。</p><p></p><p>为确保与会者的专业性和质量，闭门交流会采取审核制度，我们希望您在企业任职副总裁以及 CXO ，且在对应领域拥有丰富经验。另外，成功参与本次闭门讨论的专家将获得 ArchSummit 深圳两日通票，解锁日程页面上所有内容。如果您有意参与，请通过下方报名链接选择对应的闭门交流会：<a href="https://jinshuju.net/f/pF3JOd">https://jinshuju.net/f/pF3JOd</a>"</p><p></p><h3>看点五：降本增效趋势下，探索俭约架构师速成“大法”&nbsp;&nbsp;</h3><p></p><p></p><p>随着宏观经济环境的周期性演变，近两年来，企业围绕“业务创新”的声音在慢慢回落，取而代之的高频词变成了“降本增效”。但事实上这两个课题并非二择其一，在多数情况下，这是一个关于“平衡”的艺术。对于架构师而言，需要根据业务需求和成本效益进行持续的取舍与平衡，以帮助企业探索如何在各种约束下做出最佳决策，以及如何通过创新和适时调整架构来适应不断变化的业务环境和技术进步。</p><p></p><p>如何将成本纳入架构设计中，帮助企业实现可持续增长，达成降本增效目标，同时确保架构高可用？我们特别策划了“成本优先的技术架构”专题，从视频、通信、交通、电商、云服务等业务场景出发，字节跳动、知乎、阿里云、高德、Shopee 等企业将现身说法。</p><p></p><h3>看点六：“大卷特卷”环境下，总结架构师在 AI 时代的“生存法则”&nbsp;&nbsp;</h3><p></p><p></p><p>AI 大模型正在颠覆传统架构设计思路和路径，在这个过程中，架构师就像是整个系统的设计大师，负责操刀整个系统架构的规划。这个规划不仅仅包括技术选型、架构模式、演进变化，还得考虑业务需求、团队能力、可运维性、成本等一系列不那么技术的要素。可以说，在大模型时代下，架构师们面临着前所未有的艰巨挑战。</p><p></p><p>那么，架构师如何顺应时代持续蝶变和成长？来自众安银行、阿里云、快手的 首席架构师将围绕技术人成长，研发技能、管理技能提升等话题，帮助参会者根据自身的条件和兴趣做好职业规划，发展自己的技能图谱，找到属于自己的路，相信会引起大家的很多共鸣。</p><p></p><p>参会攻略到手，还在等什么？6 月 14 日 -15 日，ArchSummit 深圳现场，一起探索大模型时代的软件架构最佳范式。扫描下方海报二维码购票咨询，期待与你的现场交流～</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa46012b6f85031deb04a02cbcad5044.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/x7P7N64iFkYhtqBQFLfl</id>
            <title>骗子进军AI领域！一边“删号跑路”，一边研发“新业务”，网友： 我居然为它付了钱！</title>
            <link>https://www.infoq.cn/article/x7P7N64iFkYhtqBQFLfl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/x7P7N64iFkYhtqBQFLfl</guid>
            <pubDate></pubDate>
            <updated>Thu, 06 Jun 2024 06:59:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, Rabbit R1, Coffeezilla, LAM
<br>
<br>
总结: 2022年起，人工智能产品Rabbit R1在科技圈中引起轰动，但被指存在骗局。揭露者Coffeezilla表示Rabbit R1并未使用人工智能，功能实现并不智能，而其研究团队开发的LAM系统也存在疑点。Rabbit公司声称LAM能模拟人类行为，但是否真实存在仍有争议。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>人工智能可以帮助我们并解决一系列我们所没有的问题，这已经不是什么新鲜事了。但直到2022年起，大型模型（LLM、LAM等）才开始在全球范围内受到追捧和关注。最初是&nbsp;OpenAI&nbsp;的&nbsp;ChatGPT，随后小型的个人&nbsp;AI&nbsp;专用设备也引起热议，目前熟知的这类产品有&nbsp;Humana&nbsp;的无屏幕可穿戴设备&nbsp;AI&nbsp;Pin&nbsp;和&nbsp;Rabbit&nbsp;的掌上&nbsp;AI&nbsp;设备&nbsp;R1。</p><p>&nbsp;</p><p>两款产品一经推出，都在科技圈中经历了轰动和热卖。其中，Rabbit&nbsp;R1反响更盛，首批&nbsp;1&nbsp;万台在CES&nbsp;2024&nbsp;亮相后，短短一天内就被抢购一空。现在，尽管这两款个人AI产品似乎都没有兑现当初承诺，但&nbsp;Rabbit&nbsp;R1&nbsp;或许对用户有更多的触动。</p><p>&nbsp;</p><p>上个月，&nbsp;Rabbit&nbsp;R1&nbsp;刚因公司前身Cyber&nbsp;Manufacture&nbsp;Co在转型后，疑似针对所推出的GAMA元宇宙/NFT项目“删号跑路”，遭到网友的“持续轰炸”。由于Rabbit前身推出的GAMA与Rabbit&nbsp;R1&nbsp;一样，都需要用户预先付费购买账号，不少网友联想到：万一Rabbit的CEO再次做出这样的事，Rabbit&nbsp;R1还有什么价值？</p><p>&nbsp;</p><p>近日，Rabbit&nbsp;R1&nbsp;又被扒出了更多产品本身的“雷点”。一位专注于揭露骗局的YouTuber&nbsp;&nbsp;Coffeezilla&nbsp;通过关于Rabbit&nbsp;R1工作原理的两个系列视频，表示其“完全是个骗局”。</p><p>&nbsp;</p><p></p><h2>功能实现中并没有用到AI？</h2><p></p><p>&nbsp;</p><p>“如果说&nbsp;Humana&nbsp;的可穿戴&nbsp;AI&nbsp;设备只是一款价格过高且无法正常工作的产品，那&nbsp;Rabbit&nbsp;R1&nbsp;则是一款功能杂乱、执行不力且充满骗局的产品，尤其是这家公司以前曾涉足过&nbsp;NFT&nbsp;和&nbsp;‘付费获胜&nbsp;’游戏。”</p><p>&nbsp;</p><p>根据这两款产品的官方介绍，AI&nbsp;Pin是一种安置在服装上的微型投影，定位是智能手机的替代品，可以投屏在手掌上进行语音交互、拍照、翻译等功能，售价为&nbsp;699&nbsp;美元，每月需要额外支付&nbsp;24&nbsp;美元订阅费；而R1&nbsp;类似一个AI掌上电脑，尺寸不到一个巴掌大，可以联网并兼容现有的所有应用程序，售价为&nbsp;199&nbsp;美元。</p><p>&nbsp;</p><p>据介绍，Rabbit&nbsp;R1的大部分指令都是硬编码到设备中的，这意味着起大部分操作都是根据要求的软件/操作预先定义的。Coffeezilla&nbsp;在发布的曝光视频中，展示了当应用程序更改图形界面时，Rabbit&nbsp;R1的操作是如何失败的。比如美国版的&nbsp;iFood&nbsp;Doordash，通过更改了界面中的一个小汉堡菜单，从而破坏了&nbsp;Rabbit&nbsp;R1&nbsp;的操作。</p><p>&nbsp;</p><p>同时，Coffeezilla&nbsp;更详细地解释了有关Rabbit&nbsp;R1&nbsp;的所有工作原理。笼统地说，Rabbit&nbsp;R1&nbsp;所做的与使用&nbsp;Selenium&nbsp;IDE&nbsp;输入密码、创建用户、发送电子邮件和其他任务时执行的自动化操作类似。</p><p>&nbsp;</p><p>从外部来看，Rabbit&nbsp;R1&nbsp;所有的功能实现中并没有涉及到人工智能或者大型动作模型（LAM），它只是在&nbsp;Rabbit&nbsp;Inc.&nbsp;的服务器上运行的大型自动化系统。这种系统所做的就是识别口语命令，然后根据用户的登录信息，在远程运行的应用程序上发出请求，最后在&nbsp;Rabbit&nbsp;R1&nbsp;上将结果发送给用户。比如，通过Rabbit&nbsp;R1&nbsp;打开Uber叫一辆出租车到某个地点。</p><p>&nbsp;</p><p>事实上，Rabbit&nbsp;R1&nbsp;这样的功能实现看起来并不特别，也没有非常智能，现在使用&nbsp;iOS&nbsp;上的快捷方式就可以实现类似功能。“随着调查的深入，情况会变得更糟，但Rabbit&nbsp;R1&nbsp;值得一探，因为它本质上就是一个&nbsp;199&nbsp;美元的骗局。”</p><p>&nbsp;</p><p></p><h2>Rabbit&nbsp;LAM&nbsp;的存在成疑</h2><p></p><p>&nbsp;</p><p>若Coffeezilla&nbsp;的“控诉”属实，那需要注意的是，不止Rabbit&nbsp;R1产品本身，Rabbit&nbsp;研究团队开发的LAM是否存在也将成为一个疑点。</p><p>&nbsp;</p><p>据悉，LAM&nbsp;是一个由&nbsp;Rabbit&nbsp;研究团队首先开发的复杂系统，旨在彻底改变计算机和人工智能&nbsp;(AI)&nbsp;系统在计算机应用程序上理解和执行人类动作的方式。它将神经符号编程与最先进的技术无缝结合起来，以直接建模和理解各种应用程序的复杂结构以及对其执行的操作，超出了传统语言模型或视觉模型所能实现的范围。</p><p>&nbsp;</p><p>Rabbit&nbsp;介绍，其LAM&nbsp;可以在计算机应用程序上推断和模拟人类行为，可靠、快速地执行这些动作，适合部署在各种人工智能助手和操作系统中。在神经符号编程的最新进展的支持下，Rabbit&nbsp;LAM允许直接对各种应用程序的结构和用户操作进行建模，而无需依赖文本等临时表示。</p><p>&nbsp;</p><p>Rabbit&nbsp;LAM&nbsp;的建模方法植根于模仿或通过演示学习：它观察使用界面的人，并旨在可靠地复制该过程，即使界面呈现方式不同或略有变化。这意味着，一旦提供了演示，合成的例程将直接在目标应用程序上运行，而无需繁忙的“观察”或“思考”循环，并且任何受过技术训练的人都应该能够检查“配方”并推理其内部工作原理。</p><p>&nbsp;</p><p>Rabbit&nbsp;研究团队将&nbsp;LAM&nbsp;视为重塑人机交互的变革力量，目标是收集更多有关人类行为的数据，不断提高&nbsp;LAM&nbsp;的可扩展性，并通过对行为的深入理解从根本上改变具有经济意义的工作。“LAM&nbsp;并不是孤立运作的，是专为负责任的部署而设计的更广泛生态系统的一部分。&nbsp;新平台的开发可以有效管理&nbsp;LAM&nbsp;支持的例程，确保与应用程序交互的准确性、道德性和人性化。”</p><p>&nbsp;</p><p>无论如何，从Coffeezilla&nbsp;展示的案例和观点来说，现在Rabbit&nbsp;R1都并没有通过其LAM&nbsp;来实现其产品功能。</p><p>&nbsp;</p><p>此前，也有外媒报道称，“在&nbsp;R1&nbsp;中，基本上没有&nbsp;LAM&nbsp;工作的证据。”据悉，Rabbit&nbsp;R1目前实际只能连接四个应用程序：Uber、DoorDash、Midjourney&nbsp;和&nbsp;Spotify。用户需打开&nbsp;Rabbit&nbsp;的网络应用程序&nbsp;Rabbithole，然后分别登录每项服务，来连接到它们。连接时，Rabbit&nbsp;会在应用程序内打开一个虚拟浏览器，直接登录应用程序。也就是说，用户登录的不是&nbsp;DoorDash&nbsp;提供的服务，而是&nbsp;DoorDash&nbsp;的网站，而&nbsp;Rabbit&nbsp;会窥探这个过程。</p><p></p><h2>结语</h2><p></p><p>&nbsp;</p><p>对于此次事件，有网友表示，“这确实是有史以来的兔子洞骗局。“还有网友这样评价&nbsp;Rabbit&nbsp;的CEO，“从字面上描述了有史以来最疯狂、最复杂的方式洗钱。”</p><p>&nbsp;</p><p>至于Rabbit&nbsp;R1产品本身，有相关用户表示，“我之所以取消&nbsp;R1&nbsp;的预购，很大程度上就是因为需要同时使用另一个智能设备。而且，虽然它看起来很酷，但用户体验似乎很糟糕，”也有网友力挺Rabbit&nbsp;R1，“我的&nbsp;R1&nbsp;运行正常，我不觉得自己被骗了。”</p><p>&nbsp;</p><p>如今，在智能产品的极简主义设计之下，似乎正隐藏着一个类似于&nbsp;NFT&nbsp;热潮的大型人工智能骗局。Rabbit&nbsp;这类AI个人设备的“骗局”谜团之后，又会发生什么？可能会是继&nbsp;BTC、ETH&nbsp;和&nbsp;NFT&nbsp;之后新的泡沫。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://paulogpd.bearblog.dev/en-rabbit-r1-its-a-scam/">https://paulogpd.bearblog.dev/en-rabbit-r1-its-a-scam/</a>"</p><p><a href="https://cn.dataconomy.com/2024/01/15/%E5%A4%A7%E5%9E%8B%E5%8A%A8%E4%BD%9C%E6%A8%A1%E5%9E%8B-lam-%E7%9A%84%E5%85%B4%E8%B5%B7/">https://cn.dataconomy.com/2024/01/15/%E5%A4%A7%E5%9E%8B%E5%8A%A8%E4%BD%9C%E6%A8%A1%E5%9E%8B-lam-%E7%9A%84%E5%85%B4%E8%B5%B7/</a>"</p><p><a href="https://medium.com/@ignacio.de.gregorio.noblejas/the-rabbit-r1-ais-first-big-scam-00f81131bf25">https://medium.com/@ignacio.de.gregorio.noblejas/the-rabbit-r1-ais-first-big-scam-00f81131bf25</a>"</p><p><a href="https://www.theverge.com/2024/5/2/24147159/rabbit-r1-review-ai-gadget?ref=wheresyoured.at">https://www.theverge.com/2024/5/2/24147159/rabbit-r1-review-ai-gadget?ref=wheresyoured.at</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fjvkbISX4FKh2AHEOjx9</id>
            <title>暗战升级，Databricks收购Tabular，Iceberg社区陷入动荡</title>
            <link>https://www.infoq.cn/article/fjvkbISX4FKh2AHEOjx9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fjvkbISX4FKh2AHEOjx9</guid>
            <pubDate></pubDate>
            <updated>Thu, 06 Jun 2024 03:42:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 
        关键词: Databricks, Iceberg, 收购, 标准
        <br>
        <br>
        总结: Databricks收购Tabular引发了Iceberg的担忧，担心Databricks控制Iceberg标准的风险，涉及到数据湖格式标准的竞争。历史上Oracle收购MySQL的故事再次在Iceberg上重演，Databricks的战略意图是明牌的，希望通过收购Tabular间接控制Iceberg，占据优势地位。Snowflake和Databricks之间的暗战也在数据湖结构化存储标准领域展开。 </div>
                        <hr>
                    
                    <p></p><h2>事件</h2><p></p><p></p><p>Databricks收购了Tabular。字少事大。</p><p></p><p>紧接着，最近刚刚发生的事件，Iceberg的Contributor Kanou Natsukawa呼吁Icerberg的PMCChair辞职，核心他的担忧是存在利益冲突。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b69ce7fbf4f9a1d7c7d4ffe17ccfaf3.png" /></p><p></p><p></p><h4>Kanou意有所指，担心的是什么</h4><p></p><p></p><p>为什么Kanou有这个担心？他其实真正担心的是，鉴于Tabular是Iceberg“背后”的商业公司，有多位Iceberg的PMC在Tabular任职。那么这次收购之后，很有可能Databricks会完全能够控制Tabular和Iceberg，及其标准。“标准”——真正的担忧是Databricks控制Iceberg标准的风险。</p><p></p><p></p><h4>一箭双雕：Databricks的战略意图</h4><p></p><p></p><p>Databricks此举的战略意图不难看出。一方面，他们希望发展自家的DeltaLake,因为DeltaLake与Iceberg是直接竞争关系；另一方面，这也是在打击竞争对手Snowflake。Snowflake全面拥抱开放的湖仓标准，而Iceberg已经成为业界事实标准之一。通过收购Tabular，Databricks可以间接控制Iceberg，从而在数据湖格式标准的竞争中占据优势地位。</p><p></p><p></p><h2>历史总是相似的</h2><p></p><p></p><p>当年Oracle收购Mysql是这个故事的翻版，Kanou担心的正是这样的事情再次发生在Iceberg上。</p><p></p><p></p><blockquote>MySQL原开发者为瑞典的MySQLAB公司，该公司于2008年被昇阳微系统（Sun Microsystems）收购。2009年，甲骨文公司（Oracle）收购昇阳微系统公司，MySQL成为Oracle旗下产品。</blockquote><p></p><p>（引文：https://zh.wikipedia.org/zh-hans/MySQL）</p><p></p><p>这一事件在当时引起了开源社区的广泛关注和争议。时至今日，MySQL仍是活跃的开源项目，但在Oracle的影响下,它的发展方向和节奏无疑发生了变化。Oracle对MySQL的控制，也引发了开源社区对大公司介入开源项目的广泛讨论和反思。</p><p></p><p>Kanou对Databricks收购Tabular的担忧,与当年社区对Oracle收购MySQL的顾虑如出一辙。历史似乎在重演，只是这一次，舞台从开源数据库转移到了开源数据湖存储标准。</p><p></p><p>Databricks的意图可以说是明牌了。</p><p></p><p>在之前关于Redshift的采访中，Databricks的VP明确表达了对数据存储格式的“野心”</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff9f38a1f1e4a2c873c7dd11419382b3.png" /></p><p></p><p>（引文：https://inpractise.com/articles/databricks-melting-the-snow）</p><p></p><p>能够看到两强相争之时，都看中了数据湖结构化存储标准这个兵家必争之地。为什么会这样？</p><p></p><p>因为传统的数据库引擎和未来AI模型多模态等引擎，去处理海量的额数据是一个多样性的处理过程。一方面数据是海量的，很难做到将海量的数据“喂”给不同的引擎，这样耗费的“数据搬运”或ETL成本太高；另一方面处理数据的引擎越来越多样。所以，必然会需要一个革命性地架构的改变。那么，今天Snowflake和Databricks的的数据的开放性格式标准或将是决定未来各方能走多远的一个关键技术。如果Databricks真的控制了Iceberg，那么也就意味着很有可能Icerberg的发展会减缓，进而影响到支持Iceberg的Snowflake。</p><p></p><p>云器科技在此前的发布会中详细解释了AI时代数据架构的发展趋势：</p><p></p><p>AI时代的计算与存储将是M对N的关系架构，区别于一直以来数据库的计算和存储绑定的1对1关系</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13a1e98d5a9221abd39b4a9f080590e3.png" /></p><p></p><p>从云器科技此前的分享来看，湖仓一体会成为主流架构，而Iceberg已成为事实标准。</p><p></p><p>下一代的数据架构将会是：</p><p>一套数据，统一的元数据中心，具备*一致*性（其他层次上的数据用Cache抽象）开放性，数据格式公开可访问可插拔性，上层引擎/应用可以灵活的插在Lakehouse上（这对于新兴的AI引擎/应用至关重要）</p><p></p><p>关涛（云器科技联合创始人及CTO）此前分享到，“开放的存储和元数据支持多引擎协作是Data+AI平台的演进方向。”</p><p></p><p>据此，不难理解Snowflake和Databricks会为此不惜重金布局“对线”。</p><p></p><p></p><h2>Snowflake和Databricks的暗战</h2><p></p><p></p><p>之前RyanBlue,ApacheIcebergPMCChair在IcebergSummit2024中谈及开放的存储格式正在掀起一场“革命”，并暗戳戳地阴阳了DeltaLake。意思是本来没想把DeltaLake算进来，但想了想还是算上吧。满脸的勉强。</p><p></p><p></p><blockquote>we had this surprise realization a couple years ago when Snowflake and Redshift and other commercial data warehouses started adding support for Apache Iceberg. What's going on there？Why are they doing that？In order to answer that question...I think the central trend is that projects like Apache Iceberg, I would actually include Deltain this as well, have unlocked shared data warehouse storage.</blockquote><p></p><p>（引自：“Iceberg Summit 2024 Keynote：The Quite Revolution”）   </p><p></p><p>这次收购事件，可以看出Databricks毫不掩饰的讲出想釜底抽薪解决问题，为自己的DeltaLake开路</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/2973c877ef723a043b0d3c190fbb169a.png" /></p><p>（图：Databricks VP的演讲）</p><p></p><p>这场竞争双方是不惜代价全力以赴的。在DatabricksVP之前的一段访谈，谈到Iceberg和Delta的直接竞争，是一场非常决定性的战争，用了“WAR”这个词。</p><p></p><p></p><h4>并非技术之争，而是“开放”与“控制”之争</h4><p></p><p></p><p>Snowflake在2024/6/3的发布会Keynote分享，数据是AI的基础设施......以前是数据送给引擎去处理，未来AI时代是多种引擎，包括AI引擎，计算引擎要趋向数据。Data Centralization的理念是方向。</p><p></p><p></p><blockquote>"The AI Data Cloud is lighting up every corner of the enterprise.""Data... is the foundation of AI"--Sridhar Ramaswamy, Snowflake CEO</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f528c0d0f6c5517dfcb86a86cda6ceaf.png" /></p><p>（图：Snowflake Summit 2024 Keynote）</p><p></p><p>Snowflake的产品经理James Malone在今天对Tabular的收购事件也发布了他的看法，他认为Databricks收购Tabular彰显了Databricks的“独占”与“控制”思维：</p><p>独占而非共创谋求“全面控制”而非支持开放的技术社区</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7a9f608cf6c8ad4c38079efe2112279.png" /></p><p></p><p>“开放标准本应由社区共同创建和维护，就像Snowflake新发布的Polaris那样，旨在促进生态共存。而Databricks的做法却恰恰相反，他们试图通过收购的方式来控制Iceberg标准，这种‘不能打败就购’的心态，与开源精神背道而驰。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a6a526a740466bd20b6159e0fc206c79.png" /></p><p></p><p></p><h4>我们保持关注，希望Iceberg和技术社区能挺住</h4><p></p><p></p><p>资本的力量，如果用在助力技术研发和应用落地，可以推动行业进步，造福社会，是“TechforGood”；而如果资本用于收购打压竞争对手时，就成了一种“资本之恶”。</p><p></p><p>Iceberg能否在Databricks的控制下继续保持开放、中立，现在还是一个未知数。业界对此表示担忧，但同时也寄望于Iceberg社区能够坚守开源初心，维护项目的独立性。Iceberg的命运，或许将成为资本与开源技术博弈的一个缩影。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f93013ebb91124ddf9361914ec39a47.png" /></p><p>（图：Iceberg技术社区的讨论）</p><p></p><p>下周，Databricks将召开年度发布会，而Snowflake的发布会也正在如火如荼地进行中。虽然两家巨头今年没有像去年那样选在同一时间异地开幕，但这丝毫不会减弱他们在数据领域的竞争激烈程度。相反，随着Databricks收购Tabular事件的发酵，两个海外数据平台巨头之争只会愈演愈烈。</p><p></p><p>人工智能正处在聚光灯的中心，各路科技公司都在追逐AI的绚丽光环。但在光环的背后，一场数据格式之争——关乎未来十年数据格式标准之争在暗处正在上演。</p><p></p><p>作者：苏郡城，云器科技运营总监，云计算大数据领域专家。曾主导阿里云国际业务数据体系建设，十余年一线数据化运营实战，助力企业实现数字化增长，热衷于技术社区分享。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P8xizTcfBg7354pp1Opc</id>
            <title>跟大厂拼价格到底！智谱AI 宣布模型全面降价，刘慈欣、AI 老罗线上“整活儿”</title>
            <link>https://www.infoq.cn/article/P8xizTcfBg7354pp1Opc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P8xizTcfBg7354pp1Opc</guid>
            <pubDate></pubDate>
            <updated>Wed, 05 Jun 2024 07:40:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术, 价格降价, MaaS 2.0发布, GLM-4-9B模型
<br>
<br>
总结: 大模型技术仍在不断创新发展，价格不断下降，智谱AI发布了MaaS 2.0并全面降价，同时推出GLM-4-9B开源模型，具备更强大的性能和处理能力。 </div>
                        <hr>
                    
                    <p></p><p>“大模型的Scaling Law并未失效，AI技术的增长进入了一个全新的阶段。也就是说大模型技术的创新依旧是突飞猛进的进行时，甚至还有速度越来越快的迹象。”智谱AI CEO 张鹏在6月5日的Open Day 上说道。</p><p>&nbsp;</p><p>飞速变化的不只是技术，还有价格。继5月宣布入门级产品GLM-3 Turbo模型调用价格从5元／百万Tokens降至1元／百万Tokens后，智谱AI 再次宣布模型全面降价。</p><p></p><h2>MaaS 2.0发布：全面降价</h2><p></p><p>&nbsp;</p><p>“我们切切实实通过模型核心技术的迭代和效率的提升，通过技术的创新来实现了应用成本的持续降低以及客户价值的持续升级，并不是简单的价格战。”张鹏说道。</p><p>&nbsp;</p><p>根据介绍，智谱AI 的MaaS 开放平台的日均调用量已经超过了400亿的tokens，API 每日消费量在过去6个月增长了50倍以上。这次，智谱 AI 宣布推出 MaaS 2.0的同时，还采取了一系列降价措施。</p><p>&nbsp;</p><p>首先，GLM-4-flash 版本的价格降到了GLM-3 Turbo的 1/10，“现在仅仅需要1毛钱我们可以拥有100万token，100万token 够写两本四大名著，像《红楼梦》写两遍也只需要1毛钱。”智谱AI COO 张帆说道。</p><p>&nbsp;</p><p>那如何让智谱AI 最受欢迎的GLM-4 也更加便宜呢？智谱AI 为此发布了 GLM-4 Air版本，性能非常接近原来的GLM-4，但是价格进一步降低，达到1元/100万token。“我们只用了1%的价格，性能可以比肩原来的GLM-4。”</p><p>&nbsp;</p><p>对于企业在使用大模型时对于速度的需求，智谱AI 在之前基础上推出了极速版，可以在效果不变的情况下将推理速度增加162%，相当于71个token/秒，就意味着每秒钟可以展示出100多个汉字。</p><p>&nbsp;</p><p>GLM-4 进一步的升级，全新推出了GLM-4-520版本，相对GLM-4，该版本综合能力提升11.9%，指令遵从能力提升18.6%。“看起来这个数字不是很大，但真正做过模型应用的同学一定能够感觉到，这一点点的提升相当于进一步提升了我们模型的天花板，能够极大扩展应用范围，降低应用成本。”</p><p>&nbsp;</p><p>此外，智谱AI 对其他模型也进行了全面降价：GLM-3-Turbo降价80%，GLM-4V 降价50%，Cogview-3降幅接近60%、一毛钱一张图片。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/8195adbb1e25bac873fc1317ece6e163.png" /></p><p></p><p>&nbsp;</p><p>针对企业用户，智谱AI 还支持根据使用规模快速为调整价格和并发，并推出从V0、V1、V2、V3四个版本，每个版本享受不同的优惠。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e584fb65f55504fce18fcb6d61780110.png" /></p><p></p><p>&nbsp;</p><p>可以看下，"尊贵V3"可以拿到什么样的价格：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/7130f2dd98286b128d11693b01f766ec.png" /></p><p></p><p>&nbsp;</p><p>为支持企业私有部署，智谱AI升级了平台：⽆需代码，三步完成微调，⽀持 GLM-4 全系列模型：</p><p>&nbsp;</p><p>第一步，准备训练数据，按照模版收集并准备数据，然后将其导入数据集中以训练数据；</p><p>第二步，创建微调任务，使用平台微调工具，训练专属行业大模型；</p><p>第三部，部署微调模型，LoRA 微调模型可以直接调用，全参微调模型需将训练好的模型部署到私有云服务器。</p><p>&nbsp;</p><p>张帆介绍，LoRA微调用更低的成本达到一个相对较好的结果，而全参微调相当于探索模型微调的极限。在此之上，智谱AI 通过技术能力将训练成本最低降低87%，训练速度提升30%，数据利用效率提升40%。而Lora微调是性价比之王，对于GLM-4，每千token只需要4毛钱，而GLM-4-Air只需要3分钱。即使全参微调，GLM-4需要6毛钱，GLM-4-Air需要5分钱。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/5362e55924206682be954e31bc18eb5b.png" /></p><p></p><p>&nbsp;</p><p></p><h2>首次发布开源多模态模型</h2><p></p><p>&nbsp;</p><p>围绕着这方面，智谱AI 发布了第四代 GLM 系列开源模型：GLM-4-9B。</p><p>&nbsp;</p><p>GLM-4-9B 的出现是源于团队在预训练时引入了大语言模型进入数据筛选流程，最终获得了10T 高质量多语言数据，数据量是 ChatGLM3-6B 模型的 3 倍以上。同时采用 FP8 技术进行高效的预训练，相较于第三代模型，训练效率提高了 3.5 倍。在有限显存的情况下，团队发现 6B 模型性能有限，因此在考虑到大多数用户的显存大小后，将模型规模提升至 9B，并将预训练计算量增加了 5 倍。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a11d6c521d7bdb6d420720da94f6955.png" /></p><p></p><p>&nbsp;</p><p>根据介绍，GLM-4-9B 模型具备了更强大的推理性能、更长的上下文处理能力、多语言、多模态和 All Tools 等突出能力。</p><p>&nbsp;</p><p>GLM-4-9B 中英文综合性能相比 ChatGLM3-6B 提升40%，尤其是在中文对齐能力 AlignBench，指令遵从 IFeval，工程代码 Natural Code Bench 方面显著提升。对比训练量更多的 Llama 3 8B 模型，英文方面有小幅领先，中文学科方面更是有着高达 50% 的提升。</p><p>&nbsp;</p><p>GLM-4-9B 的上下文从 128K 扩展到了 1M tokens，这意味着模型能同时处理 200 万字的输入，大概相当于 2 本红楼梦或者 125 篇论文的长度。同时，该模型支持26 种语言。为了提升性能，团队将 tokenizer 的词表大小从 65k 扩充到了 150k，这一改进使得编码效率提高了 30%。</p><p>&nbsp;</p><p>另外，团队在GLM-4-9B 开源仓库中提供了一个完整的 All Tools Demo，用户可以在本地拥有一个轻量级的清言平替。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/491517db8801ef8ed5d1c5ebeea90274.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>在强化文本能力的同时，智谱AI 首次推出了基于GLM基座的开源多模态模型GLM-4V-9B。这一模型采用了与CogVLM2相似的架构设计，能够处理高达1120 x 1120分辨率的输入，并通过降采样技术有效减少了token的开销。</p><p>&nbsp;</p><p>为了减小部署与计算开销，GLM-4V-9B没有引入额外的视觉专家模块，采用了直接混合文本和图片数据的方式进行训练，在保持文本性能的同时提升多模态能力。</p><p>&nbsp;</p><p>在性能方面，GLM-4V-9B模型展现了显著的优势。尽管其参数量仅为13B，但超越了许多参数量更大的开源模型。在众多任务中，GLM-4V-9B的性能与GPT-4V不相上下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/73d5bf448d7b93b6ffd9bb9989b1368f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Github：</p><p><a href="https://github.com/THUDM/GLM-4">https://github.com/THUDM/GLM-4</a>"</p><p>huggingface：</p><p><a href="https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7">https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7</a>"</p><p>魔搭社区：</p><p><a href="https://modelscope.cn/organization/ZhipuAI">https://modelscope.cn/organization/ZhipuAI</a>"</p><p>&nbsp;</p><p></p><h2>刘慈欣、老罗，线上“整活儿”</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p>除了正式的发布，智谱AI 团队还邀请了刘慈欣和老罗来帮忙“整活儿”。</p><p>&nbsp;</p><p>刘慈欣在线上分享他对AGI与科幻创作的思考。他表示，关于人类的想象力是否会被人工智能所限制，还是被它所激发和促进，这个问题涉及到各个维度和复杂的交互作用。</p><p>&nbsp;</p><p>人工智能可能在以下两方面限制人类的想象力：依赖性和惰性 。AI 的快速响应和准确性可能会让人类变得更有惰性，不再愿意投入时间的精力去深入探索和创新。</p><p>&nbsp;</p><p>人工智能也可能激发促进人的想象力。首先，AI 可以处理分析大量数据，对人类进行前所未有的信息和知识，这些信息作为灵感来源可以激发人类的想象力和创造能力。同时 AI 的运算能力和分析能力可以突破人类思维的局限性，提出全新的观点和问题。</p><p>&nbsp;</p><p>综合来看，人工智能对人类想象力的影响是双面的，关键在于人类如何正确地使用和管理 AI 技术。</p><p>&nbsp;</p><p>“在 AI 时代，决定人生起点的不再只有依靠记忆性获得的知识，还有我们的想象力，以及我们对于今天技术革命的认知度，我们需要更加注重想象力和创新力的培养。”刘慈欣说道，“我想在未来，当人工智能拥有超过人类的智力时，想象力也许是我们对于 AI 所拥有的唯一优势。”</p><p>&nbsp;</p><p>而罗永浩AI智能体——AI老罗，正式入驻智谱清言App智能体中心，工号001，向全社会开放。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92214310789d39feabc6c49e83d64e7a.jpeg" /></p><p></p><p>&nbsp;</p><p>有趣的是，昨晚罗永浩直播的时候还被人怀疑不是真人是AI，老罗听到后霸气回应“是什么就是什么，AI 就要标明是AI”来否认。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/C9vjic1Oj9JhyHY3m645</id>
            <title>AI辅助内部研发效率提升，昇腾大模型推理的最佳实践</title>
            <link>https://www.infoq.cn/article/C9vjic1Oj9JhyHY3m645</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/C9vjic1Oj9JhyHY3m645</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 09:54:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 技术, 大模型推理, 昇腾解决方案, 硬件选型
<br>
<br>
总结: 文章讨论了随着AI技术的发展，大模型推理的趋势和挑战，以及昇腾提供的高性能大模型推理软硬件解决方案。文章指出大模型推理对硬件性能和软件支持的要求不断增加，而昇腾的解决方案涵盖了从硬件选型到应用部署的全流程，以满足不同应用场景的需求。在硬件选型方面，昇腾提供了不同的硬件方案，如Atlas 300I Duo和Atlas 800I A2，以支持不同的推理需求。 </div>
                        <hr>
                    
                    <p></p><blockquote>演讲嘉宾 | 王建辉</blockquote><p></p><p></p><p>以大模型为代表的 AI 技术高速发展，目前 Scaling Law 依然生效，模型参数持续增大，序列不断增长，响应速度越来越快，但大模型商业闭环依赖推理的规模落地，如何在不断提升用户体验的基础上不断降低推理成本，以满足大模型规模落地的诉求，成为大模型推理技术研究的核心关键。为了满足大模型推理规模落地对客户体验和成本的诉求，昇腾推出高性能大模型推理软硬件解决方案，满足客户多样性开发诉求，助力大模型规模落地。</p><p></p><p>本文整理自华为昇腾计算首席架构师王建辉在<a href="https://aicon.infoq.cn/2024/beijing?utm_source=infoq&amp;utm_medium=conference"> AICon 2024 北京</a>"的演讲《昇腾大模型推理最佳实践》，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p></p><blockquote>华为昇腾计算首席架构师赵英俊将在 6 月 14-15 日即将举办的 ArchSummit 深圳上进一步分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5866">《超大规模 AI 算力集群优化与实践》</a>"。此外，阿里巴巴研究员 / 阿里云云原生应用平台负责人丁宇（叔同）将带来《AI 编程如何颠覆生产力》 的 Keynote 主题演讲，在《低代码与 AI 结合》专题上，来自腾讯、网易、蚂蚁集团等企业等技术专家也将深入探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。目前，大会议程已全部上线，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</blockquote><p></p><p></p><p></p><p>我的演讲将分为三个部分。第一部分，我会讨论当前大模型推理的趋势和应用方向，探讨大模型推理对软件和硬件的要求，以及如何评估大模型推理系统的优势和不足。第二部分，我将详细介绍昇腾在大模型推理方向上的解决方案，包括软硬件架构和关键技术。第三部分，我会分享一些与实际应用落地相关的案例。</p><p></p><h3>大模型发展趋势分析</h3><p></p><p></p><p>在人工智能领域，随着技术的不断迭代发展，对推理解决方案的要求也在不断提高。从早期的 CNN（卷积神经网络）发展到现在广泛使用的 Transformer 模型，以及最近的 DIT 架构，这些技术的进步对计算资源提出了更高的挑战。</p><p></p><p>目前，大模型（LLM）已经开始向多模态方向发展，长序列技术已经成为当前应用的主流。不同的技术具有不同的计算特性，例如 Stable Diffusion 或 DIT 架构，它们对算力的需求非常高。Transformer 模型在推理时，对内存带宽的需求也很大，尤其是在使用 KVCache 技术之后。而长序列处理则对内存容量有很高的要求。整个硬件系统需要具备强大的算力、足够的内存容量以及高速的内存带宽。同时，软件也需要能够支持这些硬件的高性能要求。目前，单机可能已经无法满足推理的需求，需要利用多机多卡的并行推理技术。此外，随着推理技术的快速发展，不同的量化技术和压缩技术的应用，对软件的灵活性也有很高的要求。</p><p></p><p>当前大模型推理应用正在加速成熟，整体来看，美国在这方面的发展速度更快一些，而国内则处于一个跟进的状态。大模型推理的应用首先在 ToC（面向消费者）领域爆发，随后迅速向 ToB（面向企业）领域发展并成熟。</p><p></p><p>大模型推理的应用通常具有一个特性，即在初期增长阶段不会太快，但一旦过了某个爆发点，其增长速度会非常快，甚至达到 100 倍或 200 倍的增长。从目前客户的情况看，大模型推理的卡已经有上万张卡在同步在线进行推理，千卡集群的推理集群已经非常多。训练和推理对集群的要求有所不同。训练集群构建的关键在于如何实现更大规模的互联，以支持万卡集群同时进行任务训练。而推理的核心追求是降低成本，以满足业务需求。</p><p></p><p>构建一个领先的推理解决方案，我们围绕业务场景总结并建立了一个评价体系，我们称之为 LACE 指标。这个体系包括推理时延和模型精度，这两个因素直接影响用户体验。此外，还包括吞吐量和并发能力，以及开发的易用性，这两个因素则影响推理的成本，包括线上成本和开发成本。围绕 LACE 开发体系，从业务规划到模型上线，整个过程会经历几个关键阶段。首先是硬件选型，选择合适的硬件对于整个解决方案的性能至关重要。接下来是训练到推理的转换和优化，这个过程涉及模型的压缩和量化，旨在提高性能并降低成本。最后是模型的上线部署，完成整个业务流程。</p><p></p><h3>昇腾大模型推理软硬件方案和关键特性</h3><p></p><p></p><p>昇腾针对大模型推理的全流程提供了一整套完整的解决方案。这套方案从底层的硬件开始，其上是昇腾提供的自家研发的硬件使能，确保硬件性能得到充分利用。在硬件使能之上，昇腾构建了推理引擎，能够支持各种推理任务。再往上，昇腾还提供了推理服务化部署的能力，使得推理服务可以灵活地部署和扩展。通过这四层架构，昇腾能够支持从硬件选型、训推转换、模型压缩、推理执行到应用部署的五个关键维度。这五个维度涵盖了从业务规划到模型上线的整个流程，确保了解决方案的全面性和高效性。接下来，我将围绕这五个维度，详细展开当前昇腾的能力，为大家提供一个全面的讲解。</p><p></p><h4>硬件选型</h4><p></p><p></p><p>昇腾在硬件选型方面，针对不同的应用场景，提供了不同的硬件方案。昇腾根据模型参数量和时延要求，将应用场景划分为四个象限，每个象限的硬件选型和考量都有所不同。昇腾目前主要提供了两款专用硬件，一款是面向单卡推理或低成本部署的 Atlas 300I Duo，另一款是面向多卡甚至多机推理的 Atlas 800I A2。这两款硬件各有特点，能够满足不同应用场景对硬件能力的需求。</p><p></p><p>在硬件选型时，需要综合考虑时延和成本约束。如果追求极低时延，计算的 batch size 不能太大，这会提高推理成本。但如果能在时延上做一些权衡，比如将时延要求放宽到 50 毫秒或 100 毫秒，推理成本可以得到显著降低。通过调整 batch size 和提高硬件资源利用率，可以在计算成本和用户体验之间取得平衡。</p><p></p><p>Atlas 800I A2 硬件采用了 HCCS 全互联架构，具有接近 400GB 的互联带宽，能够支持 LLaMA2-70B 等大模型，实现高吞吐量。而 Atlas 300I Duo 硬件则拥有 96GB 的大内存容量，适用于时延要求不高的场景。例如，在文生图或 Stable Diffusion 等模型中，单卡可以实现 1.5 秒左右完成 50 次迭代，生成一张图片。如果通过蒸馏技术将迭代次数从 50 次降低到 20 次，生成一张图片的时间可以控制在 1 秒内。</p><p>推理转换</p><p></p><p>昇腾在训练模型到推理的转换过程中提供了一整套软件架构，包括昇腾推理引擎 MindIE，它涵盖了推理运行时以及与 Mindspore 和 Pytorch 框架的对接。昇腾的硬件支持 Mindspore 训练出的模型直接进行推理，并致力于打造训练推理一体化的 AI 框架，实现底层 MindIE-RT 与多种推理部署方式的结合。对于 Pytorch 训练的模型，昇腾通过 Pytorch 插件快速实现模型从训练到昇腾推理平台的迁移和适配，仅需大约 10 行代码，性能可达 0.8-0.9 倍于纯离线推理。</p><p></p><p>昇腾还提供了 MindIE-RT，支持自动构图开发工作流，实现从框架导出图到昇腾底层执行图的快速转换。对于大语言模型，由于图开发体系的挑战和新技术的快速迭代，昇腾也支持手动构图或手动开发，以快速构建并优化性能。</p><p></p><p>昇腾提供一站式开发工具 MindStudio，支持算子、模型和应用三层开发，提供迁移分析工具、精度比对、改图、性能优化和快速部署能力。借助 MindStudio，模型在昇腾平台上的迁移和优化可以在 3-15 天内完成。</p><p></p><p>昇腾强调训练和推理的同构能力，即在推理硬件上或相同底层硬件架构的设备上进行训练，以保持精度无损。昇腾还提供 Ascend C 编程语言，支持自定义算子开发，这对于大模型推理中 Attention 算子的灵活性和性能至关重要。尽管从 CUDA 或 GPU 开发习惯适配到昇腾平台需要一个学习过程和成本，但昇腾的开发体系能够使客户快速开发出高性能的算子。例如，昇腾的客户能够将五次内存访问的操作通过超大融合算子减少到一次内存访问，显著提升大模型推理性能。</p><p></p><h4>模型压缩</h4><p></p><p></p><p>昇腾在模型压缩方面支持当前大模型推理中常用的优化算法，包括量化算法 W4A16、W8A16 以及 W8A8 等权重量化技术和量化算法。在 W8A8 量化方面，由于大模型激活值的特性，存在大量异常值需要抑制。昇腾通过异常值抑制和自适应 PTQ 量化算法，避免了在量化过程中引入微调的需求，这些算法在模型量化中变得非常重要。</p><p></p><p>昇腾在 W8A8 量化上能够将精度损失控制在 0.5% 到 1% 的范围内，这与测试数据集的抖动和变化有关。量化之后，整体业务成本可以降低约 30%。利用昇腾硬件的特性，如 Atlas 300I Duo，昇腾还支持权重随路解压缩能力，这要求对模型进行稀疏处理。经过权重稀疏处理后，模型权重的内存搬移量可以进一步压缩 30% 到 50%，从而进一步提升大模型推理的性能。</p><p></p><p>在大模型推理领域，加速技术的发展非常迅速，包括开源算法、软件、论文以及新技术的快速演进。Continuous Batching 和 Paged Attention 算法，动态调度和节省内存的技术，已成为大模型推理中的一个必备技术。除了 Continuous Batching 和 Paged Attention，并行解码技术也发展迅速，昇腾希望利用这项技术充分利用算力。</p><p></p><p>大模型推理对内存带宽的要求非常高，同时对卡间或机间的通信时延也有高要求。与训练阶段优化通信的思路不同，推理阶段的目标是最小化单次通信时延。实验表明，有时增加通信量反而能减少调度时延，从而降低整体通信时延。降低通信时延后，可以实现更大的并行域，如从 TP8 扩展到多机 TP16，这在单机临界点的应用场景中带来了显著的性能提升。</p><p></p><p>昇腾在内部模型验证中发现，在 32K 序列长度级别，通过多机对比单机，平均单卡吞吐量能提升接近 6 倍。此外，昇腾还关注多机推理方案，类似于训练集群的参数面组网，通过一层组网实现多机推理。</p><p></p><p>针对 Kernel 的融合优化，昇腾根据硬件特性进行融合算子优化，以降低内存频繁读写问题，提高内存带宽利用率，从而提升推理性能。Flash Attention 算子最初是针对 GPU 计算特性提出的，但昇腾发现它可能不是与昇腾硬件最亲和的算子。因此，昇腾根据硬件结构特性重新设计了 attention 算子，以提高执行效率。并行解码技术有多种形式，如 Lookahead 算法，它是一种自投机算法，可以在不重新训练小模型的情况下发挥作用。Lookahead 算法对序列长度和输出长度有一定限制，但在其要求的范围内，能够显著降低推理时延，提升吞吐量。</p><p></p><p>小模型加大模型的投机推理也是一种有效方法，但需要生成一个小模型，这有一定难度。昇腾在这一方向进行了测试和验证，发现在序列长度约 2K 和 batch 在 100 以内时，吞吐量能提升约 40%，推理时延也降低约 40%。</p><p></p><p>通过 reference 方式提供投机来源，也能在代码生成场景下提升约 60% 的吞吐量。这些算法对底层 attention 算子的要求非常高。昇腾正在努力实现一套能够整合不同并行解码算法的系统，同时结合并行解码特性和 Continuous Batching 和 Paged Attention 的特点，以形成一个面向产品化的完整并行解码系统。</p><p></p><p>昇腾在集群方案方面有两个主要目标。首先，昇腾希望支持万亿级别的大模型推理，即在昇腾平台上能够处理具有高达万亿参数量的模型。其次，昇腾还计划支持百万级别的序列长度，指的是直接进行硬推理的能力，而不是采用近似计算或 streaming LLM 的方式。昇腾的目标是实现直接计算，能够达到百万级序列长度的处理能力。</p><p></p><h4>推理执行</h4><p></p><p></p><p>昇腾目前支持业界主流大模型在其平台上进行推理，包括开源模型和业界常见的模型。昇腾的性能表现相当出色，能够达到 1.5 倍或 1.8 倍于业界平均水平。例如，LLaMa3 模型发布后的第二天，昇腾就完成了其训练和推理的适配工作。如果客户需要，他们可以立即在昇腾平台上对 LLaMa3 的 8B 和 70B 版本进行训练和推理验证，包括场景化调优和上线部署。昇腾在大模型训练和推理的业界跟进和适配速度方面相对较快。</p><p></p><p>为了加快大模型训练完成后到推理部署上线的整个过程，昇腾自研了 MindIE-Service 服务化部署能力。此外，昇腾还支持与业界开源软件 vLLM 和 TGI 的对接，并提供了相应的对接方式。这表明昇腾在推理服务化部署方面具有强大的自研能力和良好的兼容性。</p><p></p><h4>应用服务</h4><p></p><p></p><p>昇腾的软件栈能力在应用服务方面是分层开放的，这意味着昇腾能够支持不同客户和不同场景的对接需求。如果客户拥有自研的推理引擎，昇腾可以提供底层的算子和加速能力，包括底层的算子和加速库，以便于与客户的推理引擎进行对接。对于那些拥有自己服务化能力和框架的客户，昇腾在推理引擎层面提供对接支持。而对于需要一整套完整推理应用的客户，昇腾可以提供从服务化到引擎，再到底层算子的全栈解决方案，以支持客户的全面对接需求。</p><p></p><h3>应用案例与关键进展</h3><p></p><p></p><p>昇腾在应用案例与关键进展方面已经取得了显著成果。目前，昇腾与多家头部客户合作，共同打造了大模型一体机，实现了大模型在训练和推理上的一体化应用，覆盖了办公客服等场景。</p><p></p><p>在华为内部，昇腾也在会议场景和面向 10 万研发人员的代码研发辅助场景中大量应用了大模型。华为计划将内部研发的代码生成和辅助开发场景制作成 demo，并将其开源，以支持业界的发展。</p><p></p><p>在视频生成领域，昇腾通过 MindIE-SD 支持 open sora 进行开发工作。目前，昇腾能够实现仅需修改少数几行代码，就能让 open sora 模型在昇腾平台上运行。在视频生成效率方面，昇腾当前能够在大约一分钟内生成 20 秒的视频。预计到年底，昇腾希望能够在一分钟内生成 60 秒的视频，这将是一个显著的性能提升。</p><p></p><h5>活动推荐</h5><p></p><p>本届 ArchSummit 会议上，重点聚焦 AI 大模型技术在各行业的落地实践， 顺丰集团、众安银行、天弘基金、鸿海科技集团、宁德核电、广发证券、微众银行介绍大模型技术的应用效果 。会议上还设置了大模型应用、架构升级、智算平台、AI 编程、成本优化等专题和话题内容。如您感兴趣，可点击「阅读原文」查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"。</p><p>购买票数越多，享受的优惠也就越丰厚，可以联系票务经理 17310043226 , 锁定最新优惠。</p><p><img src="https://static001.geekbang.org/infoq/de/de05ca2264be4a0f38c16be18fdc1d26.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AsEk7t1aSPEKHSQatxGa</id>
            <title>大模型加持下的 AIOps 业务场景实践有哪些新“解法”？</title>
            <link>https://www.infoq.cn/article/AsEk7t1aSPEKHSQatxGa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AsEk7t1aSPEKHSQatxGa</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 09:37:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIOps, 大模型, 智能运维, AI算法
<br>
<br>
总结: 监控运维产出的海量数据受到数据质量、标注不足和链路上下文信息缺失等问题的影响，AIOps应用面临挑战。大模型在多模态数据理解和处理上备受期待，希望优化AIOps在数据理解、关联和交互体验上的表现。 AIOps结合AI算法和大模型，能够简化运维工作流程，解决传统运维难题，提升业务效能。身为业务团队，AIOps能帮助提升研发效能，通过工程化和AI技术实现智能运维。在AIOps应用落地时，团队关注人力成本、AI系统基础设施建设和算法选择等问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>嘉宾｜董善东博士、何碧宏、张瀚元</blockquote><p></p><p></p><p>监控运维产出的海量数据常受数据质量波动、标注不足和链路上下文信息缺失等问题影响，这些挑战为 AIOps 的应用带来了不小的难题。伴随大模型的崛起，业界对其在多模态数据理解和处理上的能力抱以厚望，期待大模型能优化 AIOps 在数据理解、关联和交互体验上的表现。</p><p></p><p>在日前的《超级连麦. 数智大脑》x ArchSummit 直播中，<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1641">阿里云 AIOps 架构师董善东博士</a>"、群核科技云原生观测 / 技术专家何碧宏、腾讯文档高级工程师张瀚元，聚焦企业在实际的业务场景中利用 AIOps 提升业务效能时遇到的挑战以及解决方案展开了深入交流，同时还探讨了大模型时代下 AIOps 的创新架构，以及大模型如何为 AIOps 带来新的变革和潜能等话题。</p><p></p><p>以下内容根据对话整理，篇幅有删减，点击链接可观看直播回放：<a href="https://www.infoq.cn/video/AQZtO8Le5MRkuyWq1yaP">https://www.infoq.cn/video/AQZtO8Le5MRkuyWq1yaP</a>"</p><p></p><p>ArchSummit 深圳大会议程已经上线，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><h3>一、What &amp; Why</h3><p></p><p></p><h5>董善东博士：大家理解的 AIOps 是什么？以及为什么在你们团队需要引入 AIOps 做一些事情？可以举 1-2 个场景来探索下在各自业务的 AIOps 核心场景。</h5><p></p><p></p><p>何碧宏：AIOps，即智能运维，是一种利用人工智能技术，包括算法和大模型，来自动执行和简化运维工作流程的技术方案。作为云原生观测和 SRE 团队，我们主要关注在稳定性保障方面的应用和价值。引入 AIOps 希望通过 AI 算法简化和自动化传统运维操作流程，并解决一些传统运维难以解决的问题。</p><p></p><p>一些场景，比如在传统手动配置中，我们经常遇到动态配置和告警规则配置的问题，例如设置多大的阈值合适。如果每个人都手动调整，工作量会非常大。因此，我们希望通过算法实现动态阈值的功能。此外，指标异常检测也是一项庞大的工作，需要识别每种指标的持续时间、波动大小等。我们希望通过算法实现智能检测功能。在容量评估方面，我们希望能够预测未来的流量，从而计算出未来的资源使用量和扩容时间。变更风险预测也是一个场景，希望通过算法或智能方式预测变更可能造成的影响，引导变更人员进行更有针对性的检测。此外，根因分析在微服务架构下尤为重要，底层服务故障可能导致连锁反应，根因定位策略复杂且思路繁多。我们也希望能通过 AI 算法进行归纳，智能定位根因。</p><p></p><p>我们公司是一家面向全球的 SaaS 服务提供商，专注于为企业服务。对于客户，尤其是重点客户的保障尤其重视，我们几乎需要快速回应和解决每一个工单客诉。在解读和复现客户提交的工单故障方面，我们通常需要投入大量人力。我们希望通过 AI 能力对重复性工单故障进行聚类、归类，并自动化解读，预先提供有用的分析信息，以提高技术支持和开发人员的效率。</p><p></p><p>我认为，无论传统运维还是 AIOps，都需要为效果负责。两者最终需要结合互补，以实现最佳效果。</p><p></p><p>张瀚元： 身为业务团队，我们所面临的痛点和问题，以及我们所站的角度、立场与理解可能跟何老师存在不少差异。我对 AIOps 的理解定义简单明了，即 AI 与 Ops 的结合。通过自动化和工程化手段，AIOps 能够帮助业务提升研发效能。</p><p></p><p>我的理解是，要在实现智能化之前，首先需要实现工程化。通过工程化手段，我们能够以较低成本实现智能运维。在低成本的工程化智能运维基础上，再进一步通过 AI 和大模型来提升效率，带来智能化的解决方案。</p><p></p><p>对于业务团队的成员，尤其是新成员来说，在海量的微服务场景下，整个系统的庞大架构难以理解，上手也相对困难。在这种情况下，基于工程化的方式推出 AIOps，可以帮助团队成员快速了解系统。例如，通过智能告警和智能链路分析，团队成员能够从宏观层面快速认识整个系统。在此基础上，再利用大模型和 AI 的基础，对系统进行更深层次的理解。我非常认同何老师的观点，无论是传统的 Ops 还是 AIOps，最终都要为业务效果负责。两者需要相互融合、相互补充，以实现为业务带来更大价值的目标。</p><p></p><p></p><h5>董善东博士：我非常认同这个观点。AIOps 自 2016 年提出以来，到 2018 年达到了第一波热度高潮，然后在 2020 到 2022 年间相对低谷期。近两年，随着大模型的火热，AIOps 如何在可观性场景下结合得更紧密，并将大模型与 AIOps 模型结合，应用在可观测等场景成为了新的讨论热点因此。AIOps 不仅迎来了第二波讨论高潮，而且伴随着 8 年的沉淀，如何利用 AIOps 真正提升效率并为业务带来价值，已然是业务团队更关心的问题。这引出了第二个问题，目前在你们团队做 AIOps 应用落地时，最关心的几个点是什么？</h5><p></p><p></p><p>张瀚元： 在进行 AIOps 时，我们团队比较关心的几个点包括：</p><p></p><p>人力成本： 如果每个业务团队都组建自己的 AI 或运维团队，对业务来说会是一个较大的负担和成本。我们首先考虑的是避免大量重复的人力成本投入。更好的做法可能是在整个部门或公司内拉通视角，寻求通用的解决方案，这样既可以降低成本，也可以让专业的人聚焦做专业的事。AI 系统基础设施建设： 过去几年，我们主要精力放在了 AI 基础设施和监控可观测性基础设施的建设上。在完成底层可观测性基础设施的标准化建设后，我们在数据层面和标准协议层面有了良好的统一。实现这些统一后，我们再进一步建设上层的 AIOps 技术能力，这对我们整体规划来说是一种比较合理且高效的方式。算法选择和模型优化： 前期我们会更关注成本和基础设施建设。随着时间的推移，我们未来可能会更加关心算法的选择和模型的优化。</p><p></p><p>何碧宏： 我们所关心的问题主要集中在以下几个方面：</p><p></p><p>首先是成本问题，特别是人力成本。 作为中小型企业，在人才资源上不如大公司丰富，人员流失问题也较为严重。因此，对于算法的可持续性以及整个算法解决方案的可持续性非常关键。另外，AI 算法相比传统程序在资源消耗上要高很多，这导致硬件成本和其他相关成本也相应提高。</p><p></p><p>其次，我们非常关注算法的实际效果，如准确率和召回率。在真实场景中，如何提升算法效率，以及算法的准确率和召回率是否存在上限问题，都是我们所关心的。系统和代码的变更或系统架构的调整可能会影响到原有分析，这就要求我们对算法、解决方案和数据进行相应的调整。在变更后，如何保证项目的可持续性，保持其“保鲜”，这也是我们关注的问题。</p><p></p><p>最后是实施过程中的问题：包括选择合适的算法和训练数据的规模与质量。数据质量对算法准确性的影响非常大，尤其是训练样本的质量以及噪音处理，这些都对准确率起着决定性作用。因此，如何以低成本实现数据的持续自动化清洗和样本数据的准备，也是我们非常关心的问题。</p><p></p><p>董善东博士： 作为云厂商的代表，我想补充一个可能业务团队或 SaaS 产品团队不太关注的问题，那就是如何保证 AIOps 算法模型服务的通用性。这是云厂商目前面临的一个较大问题和挑战。一方面，我们可能依赖于内部数据，或者根据某些客户的数据进行微调，从而得出一些在特定业务场景下运行良好的模型。但问题是如何提高这些模型的泛化性，使其能够跨不同行业、不同类型的客户进行复制和应用。遵循 Scaling Laws 这类规则是云厂商需要考虑的关键点。这意味着我们需要思考如何让 AIOps 算法不仅在特定场景下有效，而且能够广泛适用于各种不同的业务环境和客户需求，以实现更广泛的服务覆盖和更高效的资源利用。</p><p></p><h3>二、AIOps 实施过程中的挑战和问题</h3><p></p><p></p><h5>董善东博士：接下来，我们来讨论一下在实际实施和落地过程中遇到的挑战。特别是，我想知道大家是否有一些内部总结的经验和技巧。或者注意到的问题和踩到的一些坑。</h5><p></p><p></p><p>何碧宏： 在数据方面，我们面临几个挑战：</p><p></p><p>首先，数据的持续标注和刷新是一个问题，这通常需要大量的人力劳动。</p><p></p><p>其次，我们在处理业务故障这类小数据问题时，由于故障发生的数量不多，原始样本数本身就很少。在这种情况下，如何通过算法提高故障定位的准确率，是一个需要解决的问题。</p><p></p><p>第三个问题是监控数据的多样性。指标、日志、调用链、警报等数据的格式可能各不相同。然而，在进行根因定位时，我们需要将这些不同格式的数据进行串联和融合分析，这也是一个具有挑战性的问题。</p><p></p><p>董善东博士： 在数据质量及其上下游标准问题上，我个人的体会非常深刻。</p><p></p><p>首先，数据质量对算法模型至关重要，有一句话说得好："垃圾进，垃圾出"。如果监控平台的稳定性不够，可能会导致数据未上报或数据缺失，即使再优秀的算法模型也难以应对这种情况，必然会带来干扰。</p><p>另一方面，数据的全面性和关联性也是一个问题。我们需要考虑数据如何可以有效地组织关联起来，输入给模型完整的上下文。在这方面，阿里云的实施经验主要是结合追踪（trace）内容，构建端到端的追踪点。从用户的单次请求开始，到访问的网关、后端接口，再到应用部署的服务器或机器资源，端到端地构建整个请求过程。然后基于这样一个确定性的追踪链路和拓扑，构建根因诊断的能力。</p><p></p><p>张瀚元： 数据是我们所有 AI 应用、包括 AIOps 和大模型的基石。没有数据，我们无法很好地完成工作。因此，我们前期非常关注数据模型和标准化 方面的工作。我们前期遇到的最大挑战是，腾讯内部存在不同的平台和组件，以及不同的数据模型和协议。所做的第一件事就是统一所有底层平台的数据模型和协议，使我们拥有统一的标准化模型。这样一来，整个系统的输入输出就完全可控且标准化。后续再进行上层建筑的建设时，就能达到事半功倍的效果。</p><p></p><p>第二个问题是私有化和 SaaS 场景中的挑战，特别是在私有化环境中，我们需要将整个系统部署到客户那里。一旦系统脱离公司的基础设施，其运行状态就会处于未知领域。我们如何确保在客户环境（B 端）和我们的 SaaS 环境（C 端）保持统一的架构，以及组件生态和接入系统的接入标准实现统一？如果不统一，势必会带来一致性的接入问题。接入标准不统一以及 BC 端系统架构不统一会带来很多问题，在这个过程中，我们进行了很多思考，并实现了 BC 端架构的完全统一，选择了社区生态完善、标准化的组件进行接入，并统一了接入标准。这样，无论是 C 端还是 B 端，在进行业务交付时，都能享受到标准化所赋予的长期优势。</p><p></p><p>董善东博士： 标准化确实是非常重要的议题，目前各种社区也在积极推动这方面的工作，构建接入标准。OT 生态非常丰富，不仅覆盖了指标、日志、trace 等，还可以通过 OT 这样的协议格式进行接入，同时也支持其他协议向 OT 协议转换。统一的标准协议对于构建系统确实至关重要，这样 AI 算法模型就可以更多地专注于模型本身，而不是底层的数据格式转换、数据清洗和对齐。</p><p></p><p>关于数据问题，我想补充一点，AIOps 场景目前是以任务驱动或场景需求驱动的。每个场景、每个任务可能都需要构建一个专属的算法模型，这个模型可能是基于专家经验规则的，也可能是端到端的 AI 模型。如何持续构建带标签的数据集，对于能否持续提升 AI 算法模型的效果非常关键。因为通常很难构建一个反映真实业务反馈的数据集。这时，如何提供一个接近真实反馈的数据集就显得尤为重要。另一方面，在 AIOps 场景下是否可以更多的利用无监督或自监督的方法，也是需要重点考虑的。通常依赖于有监督的算法模型，其天花板较低，短期效果可能会好一些，但因为获取高质量带标签的数据集的难度太大导致无法持续调优。</p><p></p><h5>董善东博士：我看到直播间有观众提出了一个问题，关于 AIOps 与 DevOps 的对比和区别，以及与 AI+Ops 的区别。</h5><p></p><p></p><p>何碧宏： 这里，AIOps 与 DevOps 的一个区别是使用了 AI 技术，DevOps 可以与 AIOps 结合，实现更自动化和智能化，DevOps 可以朝着这个方向演进。智能变更是 DevOps 中一个非常适合结合 AI 的场景。例如，随着大模型的出现，可以在代码检测、代码缺陷自动发现等方面增加更多的智能。大模型的出现为 DevOps 带来了更多的能力，可以在整链路上实现更智能化的操作。这意味着，通过结合 AI 技术，DevOps 可以变得更加高效和智能，从而提升软件开发和运维的质量和效率。</p><p></p><p>董善东博士： 我的理解与何老师相同，我认为 AIOps 与 DevOps 并不冲突。AI 可以应用于传统的运维和 DevOps 各个阶段，构建智能化的能力。这使得无论是手工操作还是自动化脚本，都能变得更加智能，从而提高业务的准确率和分析效率。AI 的应用为 DevOps 带来了新的机遇，可以增强开发和运维流程的智能化水平。通过 AI 技术，可以自动化地检测代码缺陷、优化变更管理、提升系统监控和故障响应速度等，这些都是 AI 在 DevOps 中的实际应用场景。</p><p></p><p>何碧宏： 我们公司目前已经实施了一些智能化的实践，例如在每次代码提交后进行智能检测，以及在开发工具中自动生成测试用例。这些技术已经比较成熟，并且效果不错。虽然这些智能工具的准确率并非百分之百，但它们确实提供了很大帮助。在单元测试编写方面，智能工具显著提高了效率。此外，在测试领域，智能工具能够发现一些潜在问题，尽管对于非常资深的开发者来说，大模型的能力可能还有所不足，但对于保障基础代码质量而言，它们已经有效。</p><p></p><h3>三、AIOps 落地难题的应对和解决办法</h3><p></p><p></p><h5>董善东博士：何老师在 6 月 14-15 日举办的「ArchSummit 全球架构师峰会. 深圳站」上将介绍如何利用全链路监控在根因分析中取得很好的应用效果。接下来我想问的是，何老师在面对前面提到的一系列挑战和问题时是如何进行解决的？</h5><p></p><p></p><p>何碧宏： 上面提到过数据打标和持续性问题，<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5803">简单介绍一下我们的一些体会和经验</a>"。最初，我们是自己手动打标和整理样本数据。后面，我们将数据打标工作融入到业务流程中，并通过流程严格要求，让用户帮助准确打标。例如，在警报处理人员处理警报时进行自动打标，填写的根因信息，对警告进行分类，标记是否与故障相关。这样，我们就能将这些数据存储起来，作为复盘或下一阶段训练的数据基础。</p><p></p><p>针对数据量大的问题，监控数据往往非常庞大，但很多数据实际上并不会被使用，或者对当前分析是无效的、浪费的。因此，我们选择对服务进行分类分级，如公司级核心服务、业务级核心服务、次核心服务和非核心服务，区分出最可能与故障相关的服务；同时也对指标进行分类，确定黄金指标、不同的用途。这样，我们就能明确哪些服务哪些指标作为核心分析指标，需要用到 AI 算法进行预测和分析，将 AI 用在最需要的地方。</p><p></p><p>对于业务故障小数据问题，我们将演练数据也进行采集。故障演练时，我们会记录故障表现、日志指标等，作为样本数据。这样就能多积累一些数据，帮助分析小数据故障，并结合传统专家经验，共同确定根因。</p><p></p><p>我们的系统是大型的微服务架构，底层系统故障引发的风暴故障，以及一些业务故障，定位难度很高，我们构建了全链路监控系统来解决这个问题。为了实现全链路监控，我们在指标、日志、调链等数据层面做了串联工作，并基于图数据库构建了全链路拓扑关系。给定任一 API，都能查到所有上下游相关信息，这对全链路分析非常有帮助。这在我们微服务底层故障的警报风暴的根因定位上发挥了巨大作用。我将在「ArchSummit 全球架构师峰会. 深圳站」上详细介绍这个系统的构建过程、解决方案以及实际应用情况。</p><p></p><h5>董善东博士：何老师的分享中有两个点我觉得很感兴趣。第一个点是他提到需要对业务和系统的指标进行梳理，明确哪些是核心指标，哪些是非核心指标，并结合一些 AIOps 的算法模型来构建智能化能力。我在想，对于一个业务新手来说，如何划分核心和非核心指标？何老师有没有一些经验可以分享？</h5><p></p><p></p><p>何碧宏： 一般不建议新手来做这些工作。对于业务数据的梳理，需要业务专家来完成。系统监控的数据，也应该由专家团队来处理会比较合适。</p><p></p><h5>董善东博士：刚刚提到的第二个点是利用图数据库构建一个全链路的拓扑，这样可以基于任意一个 API 进行拓扑查询。我的问题是，在这个过程中引入图数据库时，成本如何考量？以及图数据库的效率如何？例如，我在 7 点钟进行的一个查询和 7 点零一分进行的查询是否一样？还是说每一分钟都会进行一次更新？</h5><p></p><p></p><p>何碧宏： 这个涉及数据的刷新问题。我们的图关系构建是基于实时调用链的，所以可以说是近实时的。只要有一次最新的调用链过来，就能够根据这个调用链更新对应的关系。当然，对于一些历史依赖关系，比如某些关系很久没被调用，现在突然调用了，它会马上更新，确保链路是最新的。而对于那些可能一个月没有调用的关系，我们会设置过期时间，将这些长时间未调用的关系去掉。通过这种刷新机制，我们基本上能够实现接近于实时的数据更新。</p><p></p><h5>董善东博士：刚刚张老师在分享他踩过的坑时，提到了一个很重要的点，那就是构建标准化。不仅是采集组件的标准化，还有数据模型的标准化。张老师也是 OpenTelemetry 社区的活跃贡献者和 PMC 成员。我想请教张老师一个问题：您是如何在自己的业务团队中构建这种标准化能力的？构建出来的标准化能力效果如何？以及如何在不同团队之间或整个公司层面推广这种标准化能力的？</h5><p></p><p></p><p>张瀚元： 在面临标准化的问题时，我们发现公司内部有许多异构的平台，比如蓝鲸、007、鹰眼等，它们各自执行不同的任务，如监控、日志等，而每个平台的数据模型实现也各不相同。这给业务团队带来了选择的困难，尤其是对于新业务来说，不知道该接入哪个平台，也不了解各个平台的能力，只能依赖现有经验或前人的建议盲目接入，这样对业务缺乏深入思考。基于这种背景，我们决定在标准和协议层面实现统一。</p><p></p><p>第一，我们与 OpenTelemetry 社区进行了深度合作，在过去几年里为 OpenTelemetry 标准贡献了上百个 Commit 和数十万行代码。我们希望通过与业界最先进的团队合作，在数据标准化和协议标准化方面达成一致。随后，我们参与了中国信通院的一些行业标准的制定，如系统稳定性和根因分析等。在此过程中，我们基于 OpenTelemetry 社区标准和国家标准，制定了公司内部的企业标准。通过这些标准，我们在公司内几乎所有业务团队、中台团队和运维团队中达成了统一的数据模型。这样，业务团队在使用不同平台时，如果因为组织架构调整或其他原因需要切换平台，可以在不进行任何改造的情况下，以非侵入方式零成本地切换到兼容标准的另一个平台，从而在数据层面实现一致性。之后，我们联合打通公司内其他平台和团队，成立了一个 OTeam 组织，共同在可观测性和根因分析等领域开展工作。</p><p></p><p>第二，我们基于现有的数据模型，通过工程化方式以低成本实现多个数据源之间的关联查询和分析。这为 AIOps 的应用奠定了基础。大模型的运行成本较高，而我们的目标是通过低成本实现智能告警、编译和分析等 AIOps 手段。基于标准化的数据模型和 OTeam 的努力，我们达成了这一目标，并在智能告警、跨数据源联合查询上申请了多项国家发明专利。</p><p></p><p>第三，在私有化领域，我们需要对接客户的环境，这些环境通常脱离公司内部基础设施，无法使用公网的 AI 能力，如 ChatGPT 或其他云厂商甚至客户自身的 AI 能力。因此，我们需要提供一套统一的接入标准和准入标准，有了这套标准，我们能够解耦 AI 能力提供方和系统本身，基于我们的数据实现上层 AI 应用。</p><p></p><h5>董善东博士：从 2022 年年底一直到现在，大模型在各行各业中引起了极大的关注和影响。对于可观测性和 AIOps，尤其是 AIOps 作为一个监控运维与 AI 结合的交叉领域，两位老师认为大模型对于 AIOps 在落地过程中是否带来了新的场景、新的变化和影响？</h5><p></p><p></p><p>张瀚元：22年的 10 月份 ChatGPT 刚问世，对于国内来说，大部分公众可能是 23 年初才逐步接触到这个技术，到现在也有一年多的时间了。在这一年多的时间里，大模型在各种场景中得到了广泛应用。比如在我们自己的业务中，腾讯文档内部提供了 AI 智能助手功能，能够辅助文档编写、函数生成以及 PPT 生成等。</p><p></p><p>在接入过程中，我们发现 AI 最大的挑战是训练成本非常高，每次处理 Token 的消耗都很大。如果把无关的上下文数据一股脑丢给模型，Token 的消耗会非常剧烈。因为我们每天在标准化平台上可能有几万亿新增数据，存储量达到 PB 级别。如果全部数据都交给 AI 进行学习，成本会非常高。因此，我们首先提取有效信息，基于标准化的数据模型和框架层面的能力，我们提取出一些错误信息所属类型，如中间件错误、网络错误和操作系统错误。针对这些细分领域的问题，我们再补充相应的数据给模型结合相适应的 Prompt 进行训练，以有效减少学习和训练成本。</p><p></p><p>接下来，我们通过工程化手段对可观测性指标和数据进行初步提取、分析、提炼，最终生成服务质量汇总报告。传统团队评估系统稳定性通常使用 SLO（Service Level Objective，服务等级目标）和 SLI（Service Level Indicator，服务水平指标）指标，这需要数据分析系统和人工总结报告，发邮件等。通过 AI，我们可以将日常监控数据交给大模型，让它帮助提炼、总结报告，甚至总结典型错误案例和编码错误案例。这减少了人工重复劳动的成本，并能帮助输出服务质量报告和代码错误原因，甚至提供修改意见。</p><p></p><h5>董善东博士：目前基本是两种交互方式，一种是直接通过 Prompt 与大模型交互，另一种是通过 RAG 结合沉淀的文档、故障库，以及已有的知识库进行接入增强。在您刚刚提到的案例中，主要是通过哪种方式来推进项目的？</h5><p></p><p></p><p>张瀚元： 我们实际上会采用两种方式结合的方式进行推进。对于一些经典场景，比如私有化环境中经常遇到的磁盘满了或者网络错误等典型应用场景，我们可能更多地使用第一种方式。而对于一些具有业务特性的特定场景问题，我们则会采用第二种方式。</p><p></p><p>何碧宏： 我们有使用通过 Prompt 与大模型交互的方式，利用大模型对自然语言的理解能力来帮助提效工单和故障的解决。现在也在探索大模型结合专家经验库，提效故障的根因定位。另外，现在查询监控数据、指标或日志，通常需要按照固定格式填写应用名称或包含的关键字。大模型可以通过自然语言输入，解析生成对应的指标系统或日志系统的 API 调用，这改变了人与数据的交互方式。</p><p></p><p>在「ArchSummit 全球架构师峰会. 深圳站」大会上，<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5813">阿里云的陈昆仪博士也将分享《阿里云可观测智能化探索与实践》主题</a>"，讨论查询指标数据的方法，这非常有用且通用，我也非常期待。</p><p></p><p>董善东博士： 刚刚两位老师已经提到了大模型的应用场景，无论是通过 RAG 的方式，还是通过翻译的方式，对完善性的数据或报告进行总结和提炼。这里面其实带来了第一个我认为很重要的场景，就是对于报告的总结和提炼。传统上，我们无论是做异常检测还是数据分析，构建一个编译分析的报告表，报告通常比较冗长。通过大模型的提炼，可以使报告更加简洁。</p><p></p><p>另外一方面，对于一些基础性的可观测性数据，从中提炼出一些故障模式也是可行的。我个人的感受是，最大的问题还是成本和效率的考量。因为很多场景中，传统的小模型也能完成任务。大模型的价值在哪里？如果只是替代了之前的解决方案，而大模型的调用成本和效率又较高，那么性价比看起来就不是很高。</p><p></p><p>刚刚何老师也提到利用大模型进行数据查询的新范式，通过自然语言驱动数据查询。我们团队也确实在做这方面的工作。一方面可能是通过自然语言转化为 PromQL 和 Sql 语句的方式，这是一种方式。另一方面，传统的监控平台有很多相对标准化的查询 API，通过与 Agent 结合，驱动调用特定的 API，将自然语言的查询任务拆解为具体的 API 调用，并填写所需的参数。总结来说，我看到的数据查询有两种可能性：一种是直接生成 SQL 语句；另一种是驱动 API 调用。将 API 选出来后，通过上下文和语义提取出所需的参数，执行查询 API。</p><p></p><p>我个人从去年大模型开始流行起，就一直参与一线的大模型研发工作。可以分享一个有趣的观点和场景：大模型在确定性上下文场景下表现更优。因此一个有趣的场景就是在确定性的告警上下文中， 引入大模型进行告警上下文的问答管理。举个例子，某个运维同学小 A 收到了一条告警，通过钉钉群通知。这条告警可能是一张卡片，内容包括某对象在某时间发生了某类型的告警，告警等级等信息。做得好的卡片可能还会有一些跳转链接，可以查看告警图表，结合根因诊断信息，并传递给小 A。此外，还可能结合 ChatOps，将一些修复和决策动作放在钉钉卡片中，如修复告警、转交告警、接收告警或屏蔽告警等。这条告警背后有大量信息，小 A 在钉钉卡片中并不会直接看到。例如，告警对应的阈值、产生的条件等。</p><p></p><p>这条告警所关心的对象的其它信息，如 RT 指标、QPS 指标、错误率指标以及关联的上下游信息（谁调用了它、它部署在哪台机器上）等，这些信息都是隐藏在背后的。大模型在封闭式、确定性的三层场景下，可以进行很好的问答。告警卡片发出后，将所有相关信号汇总，作为一个整体知识传给大模型。如果小 A 有卡片中未展示的问题，可以与大模型交互（类似于钉钉中的智能机器人）。例如，问当前的 RT 指标、QPS 指标、根因诊断的理由等。通过这种方式，利用大模型结合封闭领域的三方信息，提供相对较好的回答。这是我之前在做的一个场景和思考，分享给大家。</p><p></p><h5>董善东博士：前面两位老师已经提到大模型对 AIOps 业务带来的一些变化和影响。现在我们来聊得更细一些，目前有几种典型的发展路径：从去年的 Copilot，到今年的 Agent，甚至多 Agent 协同，这是一个发展路径；另一个路径是从去年的 Prompt 工程到后来的 RAG。这两种不同的发展路径在 AIOps 领域的结合，你们的看法是什么？</h5><p></p><p></p><p>张瀚元： 我觉得这两种方式对于底层的大模型来说其实是一致的，只不过是在上层的交互模式和系统的输入输出，以及系统与系统、用户与系统进行交互时，表现有所不同。</p><p></p><p>对于业务方的选择，我们目前更推荐前者，即使用 Prompt 这种方式。通过这种方式，我们在设计系统时可以通过更完善的交互形态提供给开发者和用户，并且能够提前通过工程化的方法在各个场景、各个领域预埋对应的 Prompt。例如在分析监控问题或日志时，可以在这些特殊场景下提前设置好对应的 Prompt，从而以较低成本提供给用户一个较好的交互方式，而不需要用户手动编写 Prompt 或产生相关疑惑。这是我们基于业务进行的深入思考结果，包括在腾讯文档业务中的 AI 应用也是采用这种模式。通过这种交互方式，可以给用户提供更好的系统理解、AI 助手理解以及产品使用体验。</p><p></p><p>董善东博士： 在线观众有一个评论，问到时序大模型和 AIOps 有没有发展前景。我可以简单回应一下这位粉丝。首先，我认为时序大模型是有前景的。目前有很多类似的工作，如 TimeSeiresLLM 和 UniTS 等时序基础模型，所以从学术创新和技术应用领先性我认为是有发展空间的。</p><p></p><p>对于学术合作或高校研究所来说，这无疑是一个可以发表高质量前沿文章的领域。而对于一些平台型的产品，例如云厂商或头部的监控可观测和 Ops 平台来说，时序大模型也有很大的应用潜力。时序大模型解决了时序如何更好的表征和不同任务更好的模型能力覆盖的问题。在 AIOps 领域，传统算法模型的最大问题是每个任务模型都需要特定设计和调优。如果有了时序大模型，围绕时序相关的大部分任务意味着可以取得一个 80 分甚至 90 分的水平。时序任务包括时间序列预测、时间序列异常检测、时间序列总结、时间序列填充等。</p><p></p><p>目前我们实践过的如 ChatGPT 和国内的大模型，对日志和 trace 这种结构化文本数据的理解能力还不错，可以达到 70～80 分。但是对于时序数据，往往是一组时序点和指标的描述，信息相对离散，目前大模型并没有展示出很好的理解能力。有了时序大模型之后，可以帮助在时序与现有语言大模型更好地对齐。但需要注意的是，时序大模型的训练成本相对较高，不建议各个业务团队自行开展这方面的工作。更好的选择是与一些开源项目或头部公司、平台型产品合作，一起构建数据集和模型，这样性价比会更好一些。此外，产学研也在推动 OpenAIOps 社区的发展，由社区一起来构建可观测大模型的基座能力，例如时序、日志、trace 等。参与社区合作，也是一个非常不错的选择。</p><p></p><p>张瀚元： 前几年，我们在 ClickHouse 开始大规模应用的时候，讨论将可观测性数据和运维数据从 Elasticsearch &nbsp;迁移到 ClickHouse，结果取得了 60% 到 70% 的降本增效效果。这项工作完成后，前段时间又有人咨询是否考虑自研时序数据库，或者自研基于可观测性的时序数据的数据库或缓存。</p><p></p><p>在这个问题上，我们始终认为成本和投入产出比是最核心的问题。如果我们投入大量人力去开发，但没有得到业界认可，或者没有成为普遍解决方案，那么其性价比是不够高的，无法驱动我们完成这件事。对于大模型，尤其是时序大模型方面，我们也持相同看法。如果能坚持长期有效投入，并带来足够的产出和前景，我们非常愿意探索和尝试时序大模型的应用。但如果前期成本较高而后期收益有限，在选择上可能会更加保守。</p><p></p><h5>董善东博士：在 AIOps 领域，随着多种大模型方案的出现，包括 Prompt、RAG、微调和 Agent，解决问题的方法正在发生变化。在这些大模型方案出现之前，AIOps 领域主要依赖于专家经验和小模型的组合来应对挑战。现在，随着大模型的引入，我们可能会看到小模型和大模型的结合使用。何老师，针对这一变革，您对此有什么看法？在当前的 AIOps 实践中，您更倾向于选择哪些方案？您认为大模型的引入将如何影响 AIOps 领域的未来发展？</h5><p></p><p></p><p>何碧宏： 我们目前的工作更多地依赖于专家经验，尤其是在业务监控方面，我们处理的是类似小数据的情况。因此，我们正在探索如何将大模型与专家经验相结合。目前，我们已经构建了一个专门用于根因分析的语言，它实际上是将专家经验转化为语言表达。我们正在探讨的方向是让大模型学习这种语言的语法，以及我们编写的专家经验的语言代码。例如，当发生故障时，我们希望大模型能够解读信息，并根据这些信息生成根因分析，然后执行相关操作。</p><p></p><p>董善东博士： 目前，专家经验和小模型仍然是性价比较高的解决方案。尽管时序大模型是当前的一个热门话题，但在 Ops 领域，大小模型的组合也在最近一年变得非常流行。许多公司和业务团队已经构建了专属的小模型能力，用于异常检测和诊断等任务。大模型的引入相当于引入了一个新的“大脑”，如何将小模型与大模型结合起来，成为最近一年讨论较多的话题。</p><p></p><p>我个人的看法是，无论是小模型与大模型的协同还是组合，在 AIOps 领域，我们还处于一个相对初级的阶段。目前，对于可观测数据的判定和问题的排查，更多还是依赖小模型给出确定性的结论。大模型在其中的作用可能更多体现在意图识别、问题拆解，以及汇总小模型的结论并做出总结梳理。一些公司提到，大模型通过思考和总结，会提炼和提升得到的结论的效果，例如结合大模型的反思能力进行提升。但目前来说，我认为小模型可能仍然是主导。在各个业务团队中，小模型可以快速落地并强化价值，而大模型可能会随着自身基座的提升，以及时序大模型、认知大模型等能力的提升，甚至与多模态大模型的组合，更好地应用于 AIOps 领域。</p><p></p><h5>董善东博士：有一位观众提出了一个问题，我也想请何老师分享看法，问题是：“AI + DevOps 是否等于 AIOps + Dev？这四个名词是否相等？”</h5><p></p><p></p><p>董善东博士： 我认为这位观众提出问题可能是想探讨这些术语之间的概念区别。我个人认为，AIOps 可以视为 DevOps 发展的下一个阶段。AIOps 离不开 Ops 和开发 Dev 的基础能力。首先，你需要拥有自动化平台或自动化转换的能力，在这个基础上，才能利用 AI 手段来提升效果。AIOps 已经有了一个相对明确的定义，即将 AI 手段用于运维，提升运维领域的能力，所以它的范围可能相对较小。至于 AI 内部，看起来好像与 AIOps、DevOps 是另外的话题，希望我们的讨论对观众有所帮助。</p><p></p><p>关于大模型，我相信许多人对此非常感兴趣，因为大模型的能力确实给大家带来了许多惊喜。正如我们之前分享的，无论是大小模型还是时序大模型，都是很有潜力的方向。但相应地，研究和应用它们的成本也比较高。我的观点是，在 AIOps 领域，如果之前没有相关的算法模型储备，大模型可以帮助你从零基础快速建设到及格水平，填补场景的空白。大模型有这样的能力，可以达到及格标准。但在某些场景中，如果你已经在异常检测等领域深耕多年，你的小模型已经达到了较高水平，那么期望大模型立刻替代小模型，从 80 分提升到 90 分，我认为在当前阶段可能难度还是比较大的。因此，大模型可以帮助快速补充短板，解决泛化问题，但要从 80 分提升到 90 分，在某些场景下可能还有一定难度。</p><p></p><p>何碧宏： 大模型并不是用来替代小模型的。在某些情况下，大模型可能会替换小模型的一部分功能，但它也有自己的应用场景。使用大模型时，应该针对它真正能够发挥作用的场景进行应用，而不是强行应用在不适宜的场景中。大模型并非万能，不是所有产品都适合使用。例如，我们如果需要获取精确的结果，单凭大模型可能难以实现。大模型可以作为一个整合不同数据和工具的中介，但它并不是一个在所有场景下都可以直接应用的解决方案。使用大模型时，必须根据它的特点和能力来决定如何应用。</p><p></p><p>张瀚元： 大模型目前越来越受到关注，特别是在 4 月和 5 月，国家相关部门也开始提出大模型安全相关的议题，并组织会议进行讨论。这表明大模型安全问题是一个明显的区别于小模型的重要差异。对于小模型，由于其输入输出的范式相对固定，安全性问题相对可控，小模型本身也会对输出进行质量控制和考察。大模型的情况则不同，它的范式具有非常强不确定性，可以自定义 Prompt 甚至可以通过对话改变其回答方式。这使得大模型容易带来安全问题，可能会给接入的产品和业务带来风险。我认为在未来相当长的一段时间内，大模型无法完全替代所有小模型。小模型具有自己的优势，它们在安全性和质量控制方面更为可靠。</p><p></p><h5>董善东博士：观众想要进一步了解 AIOps 与 AI+DevOps 是否等同。我的理解和观点是这两者并不等同。AI+DevOps 不等于 AIOps+Dev，主要的变量在于 AIDev。另一位直播间的粉丝询问关于时序数据，比如日志，使用大模型分析日志异常，是否有一些最佳实践能够检测出一些未知异常，即超出业务认知范围的异常。这个问题的核心在于，传统的小模型往往是基于专家经验和现有数据集训练出来的，它们可能无法分析或检测出未见过的异常情况。而大模型具有涌现能力，是不是能够处理一些之前未见过的或未知的问题？</h5><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5812">张瀚元</a>"： 这位观众提出的问题可能过于理想化，因为在实际应用场景中，我们不太可能遇到完全未知的错误。如果真的发生了完全未知的错误，这对系统而言是非常恐慌的，因为通常错误是可以被分类和理解的。通常业务都有一套错误归类的方法，可以区分错误是由业务自身引起的，还是框架问题，或者是网络错误等。通过对错误进行归类，我们可以确定其所属的范围，并进行大致的分析。虽然分析可能不完全准确，没有任何系统能达到 100% 的准确率，但通常不会出现完全离谱的错误答案。</p><p></p><p>何碧宏： 直接向大模型询问可能无法获得准确的答案，尤其是在监控领域，我们需要非常精确的结果。要得到精确的问题答案，我们通常会对输入进行更精细的处理。例如，我们会对日志进行解析和归类，然后再将这些信息输入到大模型中进行查询。通过这种方式，我们才可能获得更准确的结果。</p><p></p><p>董善东博士： 从一个算法人员的角度来看，我想再补充一些更直白的观点。传统的小模型依赖专家经验的总结和有限的数据集进行调优，对于未知问题，这些模型确实没有很好的泛化能力。但根据我之前的实验和体会，大模型展现出一定的“涌现”能力，使得模型对于很多通用的知识都有了一定的掌握，掌握能力大小取决于大模型基座本身。如果遇到业务系统中未曾见过的日志异常，大模型有一定的可能性能够识别出来，尽管这个概率并不是 100%。因此在面对系统新的未知问题时， 相比传统小模型，大模型是可能表现的更优的。&nbsp;</p><p></p><h5>活动推荐</h5><p></p><p>本届 ArchSummit 会议上，重点聚焦 AI 大模型技术在各行业的落地实践， 顺丰集团、众安银行、天弘基金、鸿海科技集团、宁德核电、广发证券、微众银行介绍大模型技术的应用效果 。会议上还设置了大模型应用、架构升级、智算平台、AI 编程、成本优化等专题和话题内容。如您感兴趣，可点击链接查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p>购买票数越多，享受的优惠也就越丰厚，可以联系票务经理 17310043226 , 锁定最新优惠。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4af80bd1f2b5e5b802728c2b761f40e2.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NzBxv0DhW6tau3LyhbW3</id>
            <title>一边增长一边裁员！微软大刀挥向Azure云及混合现实部门，上千人业绩好也逃不掉被裁？</title>
            <link>https://www.infoq.cn/article/NzBxv0DhW6tau3LyhbW3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NzBxv0DhW6tau3LyhbW3</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 09:32:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 裁员, 微软, 人工智能, 混合现实
<br>
<br>
总结: 微软在裁员的同时，加大对人工智能领域的投入，以支持公司的战略增长。裁员涉及多个部门，包括云运营和混合现实部门，旨在优化组织结构和资源配置。微软的战略使命和技术组织也受到影响，公司正在调整硬件阵容，以适应人工智能发展的需求。整体来看，微软在裁员的同时，致力于推动人工智能技术的发展。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>继2023年初裁员&nbsp;1&nbsp;万多人、今年&nbsp;1&nbsp;月在动视暴雪和&nbsp;Xbox&nbsp;部门裁员1900人之后，微软再次在三个部门裁员数百人。</p><p>&nbsp;</p><p>6月3日，据外媒报道，微软正在削减其Azure云运营部门（Azure&nbsp;for&nbsp;Operators）和任务工程部门（Mission&nbsp;Engineering）的工作岗位，其中云运营部门的裁员涉及多达1500个工作岗位，混合现实部门也是微软受影响的部门之一。</p><p>&nbsp;</p><p>“组织和劳动力调整是我们业务管理的必要和常规部分。我们将继续优先考虑并投资于我们未来的战略增长领域，以保障我们的未来，并为我们的客户和合作伙伴提供支持。“微软发言人Craig&nbsp;Cincotta在一份声明中表示。</p><p>&nbsp;</p><p></p><h2>整改硬件阵容始于一年前</h2><p></p><p>&nbsp;</p><p>被裁员的Azure云运营部门和任务工程部门是微软战略使命和技术（SMT）组织的一部分，由微软&nbsp;Azure前执行副总裁Jason&nbsp;Zander领导。据了解，SMT&nbsp;成立于&nbsp;2021&nbsp;年，以量子计算、太空&nbsp;“登月计划”和电信等其他尖端技术项目而闻名，如与&nbsp;SpaceX&nbsp;等公司合作并在盒子中推出便携式数据中心。</p><p>&nbsp;</p><p>报道称，负责Azure&nbsp;Operator&nbsp;Nexus的团队员工将加入云+人工智能组织的Azure&nbsp;Edge和Platform产品线。</p><p>&nbsp;</p><p>另外，微软将裁减&nbsp;HoloLens&nbsp;2&nbsp;和混合现实部门的员工。尽管微软仍在并将继续销售2019年发布的HoloLens&nbsp;2&nbsp;设备，同时为现有&nbsp;HoloLens&nbsp;2&nbsp;客户和合作伙伴提供支持，但没有表示将推出新型号。2022&nbsp;年有报道称，该公司已取消&nbsp;HoloLens的第三个版本。</p><p>&nbsp;</p><p>“我们宣布对微软混合现实部门进行重组，但将继续全力支持国防部的集成视觉增强系统（IVAS）计划，并将继续提供尖端技术，为我们的士兵提供支持。此外，我们还将继续投资&nbsp;Windows&nbsp;365，以覆盖更广泛的混合现实硬件生态系统。”&nbsp;Cincotta表示。</p><p>&nbsp;</p><p>据悉，自&nbsp;2015&nbsp;年推出&nbsp;HoloLens&nbsp;以来，微软并没有取得巨大成功。之前还有外媒报道，使用IVAS设备的士兵报告说，他们会出现恶心等症状。</p><p>&nbsp;</p><p>一年前，这家软件制造商就曾表示要对其硬件阵容进行调整。当时，微软达到10000人的大规模裁员中，就包括一些混合现实员工。在接下来的几个月里，微软还停产了几款键盘型号。去年&nbsp;12&nbsp;月，Microsoft&nbsp;进一步减少了对增强现实和虚拟现实的投资，弃用了&nbsp;Windows&nbsp;Mixed&nbsp;Reality平台，其中包括用于在头戴式显示器中运行应用程序的工具。</p><p>&nbsp;</p><p>今年2月，微软完成了对动视暴雪的收购，在人工智能方面投入了大量资金。现在，许多热门游戏特许经营项目、&nbsp;Bethesda&nbsp;和自己的&nbsp;Xbox&nbsp;游戏工作室，都成为微软工作室组合的一部分。最近，微软又开始推动“人工智能电脑”的发展，推出了新一轮由高通芯片驱动的Surface设备。</p><p>&nbsp;</p><p></p><h2>为了发展人工智能而裁员</h2><p></p><p>&nbsp;</p><p>对于此次微软从其&nbsp;Azure&nbsp;云业务中裁员数百人的事件，Zander将其归因于公司正在进行变革，以支持对人工智能的更多资金投入。据悉，由于微软在人工智能领域投入巨资，并通过与&nbsp;ChatGPT&nbsp;制造商&nbsp;OpenAI&nbsp;建立战略合作伙伴关系获得了其技术，微软的&nbsp;Azure&nbsp;云正急剧的增长。</p><p>&nbsp;</p><p>“作为一家公司，我们明确的重点是定义人工智能浪潮，并让我们的所有客户都能成功采用这一变革性技术。“在此过程中，我们会做出符合长期愿景和战略的决策，同时确保微软的可持续发展和增长。”Zander&nbsp;在给员工的一封电子邮件中解释说。</p><p>&nbsp;</p><p>也就是说，微软最新一轮的裁员，是为了有助于公司能够更加专注于人工智能。由于该公司在人工智能方面的大量投资以及通过战略合作伙伴关系获得&nbsp;ChatGPT&nbsp;制造商&nbsp;OpenAI&nbsp;令人垂涎的技术，Microsoft的&nbsp;Azure&nbsp;云正在见证急剧增长。</p><p>&nbsp;</p><p>最近，微软还重组了AI&nbsp;at&nbsp;work负责人&nbsp;Jared&nbsp;Spataro&nbsp;领导的团队，将其工作重点转移到Copilot&nbsp;AI产品上，并减少了使用Teams聊天应用程序的员工数量。</p><p>&nbsp;</p><p>今年3月，微软组建了全新的Microsoft&nbsp;AI部门，旨在统一管理和推进Copilot&nbsp;等所有消费者AI产品和研究工作，并聘请了DeepMind的联合创始人Mustafa&nbsp;Suleyman来领导这个新成立的团队。此外，微软还请来了Inflection联合创始人兼首席科学家&nbsp;Karén&nbsp;Simonyan加入Microsoft&nbsp;AI。不到一年前，这家云巨头领投了Inflection一轮&nbsp;13&nbsp;亿美元的融资，资金用于支持Inflection自研的首款名为Pi的人工智能助手。</p><p>&nbsp;</p><p>同时，微软将停止预览版服务，如&nbsp;Azure&nbsp;Operator&nbsp;5G&nbsp;Core&nbsp;(AO5GC)&nbsp;和&nbsp;Azure&nbsp;Operator&nbsp;Call&nbsp;Protection。</p><p>&nbsp;</p><p>Zander&nbsp;表示，“做出这些艰难的决定绝非易事，尤其是当这些决定影响到我们的同事和朋友时。我们致力于以尊重、尊严和透明的方式支持受这些变化影响的每个人，全力支持他们完成这次过渡。”</p><p>&nbsp;</p><p></p><h2>云厂商向&nbsp;AI&nbsp;转变，原来人才怎么办？</h2><p></p><p>&nbsp;</p><p>今年，不少技术供应商都在进行云相关领域的裁员。因为越来越多科技公司将人工智能视为未来潜在的收入驱动力，同时还要应对美国的高通胀。除微软以外，最近进行云业务部门裁员的技术供应商还有谷歌。</p><p>&nbsp;</p><p>6月1日，有外媒报道称，谷歌已经着手在其云计算部门进行新一轮裁员，预计将裁减至少100名员工，涉及销售、运营、工程、咨询及市场战略等部门。自今年年初起，谷歌已经进行了多轮裁员，包括&nbsp;5&nbsp;月初裁掉至少&nbsp;200&nbsp;名“核心”团队员工。谷歌&nbsp;CEO&nbsp;Sundar&nbsp;Pichai&nbsp;曾透露，裁员将持续到今年年底，尽管裁员规模不会很大。</p><p></p><p>现在，云服务领域似乎正在经历向AI增强的重大转变，包括云提供商开发定制硬件，旨在有效处理Al应用程序的计算需求。这一发展正在减少能源使用、碳足迹和运营成本，同时使AI服务更易于访问且价格更实惠。</p><p>&nbsp;</p><p>去年11月，麦肯锡一项针对云价值的研究报告指出，老牌公司在寻求利用云时面临着一个困境：尽管好处可能很有吸引力，但采用云平台所需的变革规模和投资使得产生有吸引力的投资回报&nbsp;（ROI）&nbsp;成为一项挑战。而生成式人工智能可能会显著改变这一价值方程式。许多人认为，生成式&nbsp;AI&nbsp;可能是一个颠覆者，可以改变云项目的&nbsp;ROI&nbsp;动态并加速云采用。</p><p>&nbsp;</p><p>而生成式&nbsp;AI&nbsp;可以通过三个关键优势为云项目增加&nbsp;75&nbsp;到&nbsp;110&nbsp;个百分点的增量投资回报率：解锁新的业务用例；减少应用程序修复和迁移的时间和成本（早期结果表明，时间和成本降低了&nbsp;40%）；提高云上应用程序开发和基础架构团队的工作效率。并且，未来公司可以通过多种方式将生成式&nbsp;AI&nbsp;构建到其云计划中。</p><p></p><p>“云提供商正在利用自己在人工智能方面的进步，来推动从供应链可预测性和代码生成到网络威胁检测和响应以及业务职能生产力的方方面面。”商业咨询公司普华永道（PwC）的合伙人兼美国Microsoft业务负责人Matt&nbsp;Hobbs&nbsp;曾在去年&nbsp;10&nbsp;月指出。同时，他也表示，人工智能的需求正在给整个云基础设施带来压力，另外的担忧是<a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">人工智能的幻觉</a>"和偏见，这可能导致意想不到的后果。</p><p></p><p>当时，商业咨询公司West&nbsp;Monroe负责IT咨询和转型业务的高级合伙人Andy&nbsp;Sealock也警告说，将人工智能应用于云计算可能将敏感或专有信息暴露给未经授权的个人或组织。他不会将人工智能和云计算之间的关系描述为变革性，而是将两种技术描述为协同作用，同时在增长和采用以及进化、成熟和完善方面加速发展。</p><p></p><p>“总的来说，将云计算和人工智能结合起来的好处将超过任何坏处。”德勤咨询公司负责人Tim&nbsp;Potter&nbsp;这样预测。</p><p></p><p>但这样直接裁掉云计算人才，是否会对本就因AI&nbsp;而要求更高的基础设施产生影响？&nbsp;也是云厂商在裁撤云计算人才时应该考虑的问题。</p><p></p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/microsoft-lay-off-hundreds-azure-cloud-unit-business-insider-reports-2024-06-03/">https://www.reuters.com/technology/microsoft-lay-off-hundreds-azure-cloud-unit-business-insider-reports-2024-06-03/</a>"</p><p><a href="https://www.theverge.com/2024/6/3/24170902/microsoft-hololens-2-mixed-reality-azure-layoffs">https://www.theverge.com/2024/6/3/24170902/microsoft-hololens-2-mixed-reality-azure-layoffs</a>"</p><p><a href="https://www.cnbc.com/2024/06/03/microsoft-confirms-mixed-reality-layoffs-will-keep-selling-hololens-2.html">https://www.cnbc.com/2024/06/03/microsoft-confirms-mixed-reality-layoffs-will-keep-selling-hololens-2.html</a>"</p><p><a href="https://www.businessinsider.com/microsoft-exec-blames-azure-layoffs-on-ai-wave-leaked-memo-2024-6">https://www.businessinsider.com/microsoft-exec-blames-azure-layoffs-on-ai-wave-leaked-memo-2024-6</a>"</p><p><a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/in-search-of-cloud-value-can-generative-ai-transform-cloud-roi">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/in-search-of-cloud-value-can-generative-ai-transform-cloud-roi</a>"</p><p><a href="https://www.informationweek.com/it-infrastructure/how-ai-is-transforming-cloud-computing#close-modal">https://www.informationweek.com/it-infrastructure/how-ai-is-transforming-cloud-computing#close-modal</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jaSHymqFSDuwyLCh8CY6</id>
            <title>从 Volcano 火山模型到 Pipeline 执行模型，Apache Doris 执行模型的迭代实践</title>
            <link>https://www.infoq.cn/article/jaSHymqFSDuwyLCh8CY6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jaSHymqFSDuwyLCh8CY6</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 07:43:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库系统, 执行引擎, 火山模型, Pipeline 执行模型
<br>
<br>
总结: 在现代数据库系统中，执行引擎扮演着重要的角色，与查询优化器和存储引擎共同构成数据库的三大模块。执行引擎在 SQL 查询过程中起着关键作用，不同的执行模型如火山模型和Pipeline 执行模型对查询效率和系统性能有着重要影响。火山模型具有灵活性和易优化等特点，但在单机多核场景下存在一些问题，因此Apache Doris引入了Pipeline 执行模型来解决这些问题。Pipeline 执行模型通过优化查询计划和任务调度，提高了查询效率和系统性能。 </div>
                        <hr>
                    
                    <p>在现代数据库系统中，执行引擎在数据库体系结构中起着承上启下的作用，与查询优化器和存储引擎共同组成了数据库的三大模块。我们以 SQL 语句在数据库系统中的完整执行过程为例，来介绍执行引擎在其中发挥的作用：</p><p></p><p>在接收到一条 SQL 查询语句之后，查询优化器会对 SQL 进行语法/词法分析，基于代价模型和规则生成最优执行计划；执行引擎会将生成的执行计划调度到计算节点，按照最优执行计划对底层存储引擎中的数据进行操作并返回查询结果；</p><p></p><p>在整个查询过程中，查询执行是至关重要的环节，往往需要通过数据读取、过滤、排序、聚合等操作，才能提交给执行引擎进行下一步查询，这几个步骤的设计是否合理直接影响到查询的性能及资源的利用率。而这些能力均由执行模型来提供，而不同的执行模型在数据处理、查询优化和并发控制等方面存在较大差异，因此，一个合适的执行模型对于提高查询效率和系统性能至关重要。</p><p></p><p>目前业界常见的执行模型有迭代模型/火山模型（Iterator Model）、物化模型（Materialization Model）、向量化/批处理模型（Vectorized / Batch Model）。其中火山模型（Volcano Model）是数据库查询优化和执行中最为常用的执行模型。每一种操作抽象为一个 Operator，整个 SQL 查询被构建成一个 Operator 树。查询执行时，树自顶向下调用 next() 接口，数据则自底向上被拉取处理，因此这种处理方式也被称为拉取执行模型（Pull Based）。火山模型因其具有很高灵活性高、可扩展性好、易于实现和优化等特性，被广泛应用于数据库查询优化和执行中。</p><p></p><p>作为典型的 MPP 数据库，过去版本中 <a href="https://c.d4t.cn/bnYeBn">Apache Doris</a>" 亦采取的也是火山模型。当用户发起 SQL 查询时，Apache Doris 会将查询解析成分布式执行计划并分发到执行节点执行，分发到节点的单个执行任务被称为 Instance，在此我们一条简单的 SQL 查询来了解 Instance 在火山模型下的执行过程：</p><p></p><p><code lang="sql">select age, sex from employees where age &gt; 30</code></p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c01eea393fd091d11df2a8b2545445c.png" /></p><p></p><p>如上图可知，Instance 是一个算子（ExecNode）树，算子之间通过数据重分布（Exchange）算子连接起来，从而实现数据流的传递和处理，每个算子实现 next() 方法。当对算子的 next() 方法进行调用时，该算子将调用其孩子算子的 next() 方法来获取输入的数据，然后对数据进行逻辑加工并输出。而因为算子的 next() 方法是同步方法，在没有数据产生时， next() 方法将会持续阻塞。这时候需要循环调用根节点算子的 next() 方法，直到全部数据处理完毕，即可得到整个 Instance 的计算结果。</p><p></p><p>从上述执行过程可以看出，火山模型是一种简单易用、灵活性高的执行模型，但在单机多核的场景下，存在一些问题需要进一步解决和优化，具体体现在以下几方面：</p><p></p><p>线程阻塞执行：在线程池大小固定的情况下，当一个 Instance 占用一个线程阻塞执行时，如果存在大量的 Instance 同时请求，执行线程池将被占满，从而导致查询引擎出现假死状态，无法响应后续请求。特别是在存在 Instance 之间相互依赖的情况下，还可能会出现逻辑死锁的情况，比如当前线程中正在执行的 Instance 依赖于其他的 Instance，而这些 Instance 正处于等待队列中，无法得到执行，从而加剧系统的负载和压力。当一个执行节点同时运行的 Instance 线程数远大于 CPU 核数时，Instance 间的调度将依赖于系统调度机制，这就可能产生 Context 切换开销，尤其是在系统混部的场景中，线程切换的开销会更加显著。CPU 资源抢占：Instance 线程之间出现争抢 CPU 资源的问题，可能导致不同大小的查询、不同租户之间互相影响。无法充分利用多核计算能力：执行计划的并行度取决于数据分布，当一台执行节点上存在 N 个数据分桶时，该节点上运行的 Instance 数量不能超过 N，因此分桶的设置显得尤为重要。如果分桶设置过少，难以充分利用多核计算能力，反之，则会带来碎片化问题。多数场景下进行性能调优时需要手动设置并行度，而在生产环境中，预估数据分桶数是一项极具挑战性的任务，不合理的分桶使得 Doris 的性能优势无法得到充分发挥，无法充分利用多核计算能力。</p><p></p><h1>Pipeline 执行模型的引入</h1><p></p><p>为了解决过去版本所存在的问题，<a href="https://c.d4t.cn/fVEDWY">Apache Doris</a>" 自 2.0 版本起引入了 Pipeline 执行模型以替换过去的火山模型，并在 2.1 版本对 Pipeline 执行模型进行了进一步的升级。</p><p></p><p></p><blockquote>设计文档：<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-027%3A+Support+Pipeline+Exec+Engine">DSIP-027 Pipeline Execution Engine</a>"<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-035%3A+PipelineX+Execution+Engine">DSIP-035 PipelineX Execution Engine</a>"</blockquote><p></p><p></p><p>以 Join 场景为例，下图展示了 Pipeline 执行模型下两个 Instance 组成查询计划的效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/491730edf075f1bbc1bd88c9ed4a14d8.png" /></p><p></p><p>在这个计划中，Join 的 Probe 操作依赖于哈希表的构建操作（Build），因此 Build 操作必须在 Exchange 获取的数据全部处理完成并构建完哈希表之后才能启动，这种依赖关系导致每个 Instance 被拆分成两个 Pipeline Task。Pipeline 调度器将 Pipeline Task 放置于工作线程池的 Ready 队列，工作线程根据不同的策略获取 Pipeline Task，Pipeline Task 计算完成一个数据块后是否让出线程取决于其前置数据是否 Ready 以及运行时间是否超过上限。</p><p></p><h2>Pipeline 执行模型的设计实现</h2><p></p><p></p><p>Pipeline 执行模型通过阻塞逻辑将执行计划拆解成 Pipeline Task，将 Pipeline Task 分时调度到线程池中，实现了阻塞操作的异步化，解决了 Instance 长期占用单一线程的问题。同时，我们可以采用不同的调度策略，实现 CPU 资源在大小查询间、不同租户间的分配，从而更加灵活地管理系统资源。Pipeline 执行模型还采用了数据池化技术，将单个数据分桶中的数据进行池化，从而解除分桶数对 Instance 数量的限制，提高 Apache Doris 对多核系统的利用能力，同时避免了线程频繁创建和销毁的问题，提高了系统的并发性能和稳定性。</p><p></p><h3>去阻塞化改造</h3><p></p><p>从上文介绍可知，在之前版本的火山模型下，执行引擎存在阻塞操作，这会带来两个核心问题：一是阻塞线程过多会导致线程池打满，无法响应后续查询；二是执行线程调度完全依赖操作系统，无法根据查询优先级进行调度，性能有待提升。为了解决这两个问题，我们重新设计了去阻塞化的执行逻辑。</p><p></p><p>针对第一个问题，我们固定一个大小与 CPU 核数相同的执行线程池，并保证执行线程中不会存在阻塞操作。为了避免线程阻塞导致操作系统级别的线程调度，我们在所有发生阻塞的算子中拆分了 Pipeline Task，比如使用独立线程进行磁盘 I/O 和 RPC 等操作。</p><p></p><p>针对第二个问题，我们设计了一个纯用户态的轮询调度器，通过不停轮询所有可执行 Pipeline Task 的状态，将当前需要执行的 Task 交给执行线程执行。这种做法避免了操作系统频繁线程切换的开销，同时也可以加入更多优先级等定制化的调度策略，提高系统灵活性和可扩展性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b8a788036adf8a0f9650ed2bbd03bbd.png" /></p><p></p><h3>并行化改造</h3><p></p><p>在 2.0 之前版本中，Apache Doris 执行引擎的并发度需要由用户手动设置（即会话变量 parallel_fragment_exec_instance_num ），无法根据不同的 Workload 进行动态调整。而为了设置一个合理的并发度，往往需要进行细致的分析，这无疑是增加了用户的负担。同时，使用不合理的并发度可能会导致性能问题。因此，如何充分利用机器资源来实现每个查询任务的自动并发，成为亟需解决的问题。</p><p></p><p>当前常见的 Pipeline 并发方案分别以 Presto、DuckDB 为代表，Presto 并发方案是在执行过程中将数据 Shuffle 成合理的分区数量，这样做的好处是基本不需要特别的并发控制。DuckDB 并发方案执行过程中不会引入额外的 Shuffle 操作，但是需要引入额外的同步机制。我们对以上方案进行了综合对比，我们认为 DuckDB 并发方案在实现上很难规避使用锁，而锁的存在有悖于我们去阻塞化改造的思路，因此我们选择了以 Presto 为代表的实现方案。</p><p></p><p>为了实现 Pipeline 并发，Presto 引入了 Local Exchange 对数据进行了重分区，例如对于 Hash Aggregation，Presto 根据聚合 Key 进一步将数据分为 N 份，这样就可以充分利用机器的 N Cores，每个执行线程只需要构建更小的 Hash Table。而对于 Apache Doris，我们选择充分利用 MPP 自身的架构，在 Shuffle 时就直接将数据分区成合理的分区数，因此不再需要额外引入 Local Exchange。</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/0028abbc4aa4006c7062c1712c6faabd.png" /></p><p></p><p>基于这个特性，我们需要对两个方面进行改造：一是在 Shuffle 时增加并发，二是在 Scan 层读取数据后实现并发执行能力。对于前者，我们只需要在 FE 感知 BE 环境，然后设置合理的分区数即可。而对于后者，目前 Doris 在 Scan 层的执行线程与存储 Tablet 数量是强绑定的，因此需要重构 Scan 层并发逻辑，以满足我们的需求。</p><p></p><p>Scan 池化的基本思路是将 Scanner 线程读取的数据进行池化，多个 Pipeline Task 可以直接从池中取数据执行。这样的方式可以充分解耦 Scanner 和执行线程，提高系统的并发性能和稳定性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d30c3bf7195bbb137643acca7e765534.png" /></p><p></p><h2>Pipeline 执行模型的进一步完善</h2><p></p><p></p><p>Pipeline 执行模型的引入为 Apache Doris 在混合负载场景中的查询性能和稳定性都得到了明显提升，但在 Apache Doris 2.0 版本中仍为实验性功能，在社区用户使用的过程中，一些新的问题开始浮现：</p><p></p><p>执行并发受限： 由于当前版本 Doris 执行并发仍收到 FE 设置的静态并发参数和存储层 Tablet 数量限制，使得执行引擎无法充分利用机器的多核资源，同时存储层可能会存在数据倾斜问题，导致查询执行出现长尾。执行开销较大： 表达式各 Instance 相互独立，而 Instance 的初始化参数存在大量公共部分，这导致每次执行都需要额外进行重复的初始化步骤，显著增加了执行开销。调度开销较大： 在查询执行过程中，当前调度器会把阻塞 Task 全部放入一个阻塞队列中，由一个线程负责轮询并从阻塞队列中取出可执行 Task 放入 Runnable 队列，所以在有查询执行的过程中，会固定有一个核的资源作为调度的开销。尤其是在一些小机型上，固定调度线程带来的开销非常明显。Profile 可读性差： Pipeline Profile 指标缺乏直观性和可读性，使得性能分析变得比较困难。</p><p></p><p>为了提供更高的查询性能和更稳定的查询体验，<a href="https://c.d4t.cn/bnYeBn">Apache Doris 在最新发布的 2.1 版本中</a>"，对 Pipeline 执行模型进行大幅优化，将其改造为基于事件驱动的执行模型，并对已存在问题提供了改进方案。为便于理解，后文将改进后的 Pipeline 执行模型称为 PipelineX。</p><p></p><h3>执行并发改造</h3><p></p><p>前文提及，Pipeline 执行并发受两个因素制约：FE 设置的静态并发参数和存储层 Tablet 数量限制，这就导致执行引擎无法充分利用机器资源。另外如果数据本身存在倾斜，还可能导致查询执行时出现长尾问题。为此，我们以一个简单的聚合查询为例展开详细介绍。</p><p></p><p>假定有 Table A，Table A 中 tablet 总数为 1 ，共有数据 100M 行，执行聚合查询：</p><p></p><p><code lang="c++"> SELECT COUNT(*) FROM A GROUP BY A.COL_1;
</code></p><p></p><p>一般而言，在查询 SQL 的完整执行过程中，查询会被切分成为多个查询分片（Fragment），每个查询分片表示查询执行过程中的逻辑概念，可能包含多个 SQL 算子。当 BE 收到 FE 下发的 Fragment 后，启动多个执行线程并行执行 Fragment，确保每个 Fragment 均能得到高效处理。如下图，Doris 将其切分成了 2 个 Fragment 分别执行：</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88deeeb2206de1f796f0d80b95d9c1db.png" /></p><p></p><p></p><p>为便于理解，仅介绍逻辑计划的第一部分（Fragment 0）。由于 Table A 只有一个 Tablet，因此 Fragment 0 的执行并发始终被限制为 1，即由单线程完成 100M 行数据的聚合。而在理想状态下，16 核可承载并发数为 8，假定执行时间为 x，每个执行线程可读取 100M/8 行数据，那么执行时间约为 x/8。然而在该例子中，大约会带来 8 倍的性能损失。</p><p></p><p>为解决这一问题，Apache Doris  2.1 版本在执行引擎中引入了 Local Shuffle 节点，摆脱了存储层 Tablet 数量对执行并发的限制。 具体实现上：</p><p></p><p>执行线程执行各自的 Pipeline Task，而 Pipeline Task 仅持有一些运行时状态（即 Local State）。全局信息则由多个 Task 共享的同一个 Pipeline 对象持有（即 Global State）。在单个 BE 上，数据分发由 Local Shuffle 节点完成，并由 Local Shuffle 保证多个 Pipeline Task 间的数据均衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe24cf63d1d4d06cfec902124956b32a.png" /></p><p></p><p>上述问题阐述了 PipelineX 执行引擎如何摆脱 Tablet 数量的限制，除此之外，Local Shuffle 还可以规避数据倾斜带来的长尾查询问题。我们仍假定使用上面的聚合查询，将 Table A 的 Tablet 数量改为 2，其中 Tablet 1 有 10M 行数据、Tablet 2 有 90M 行数据：</p><p></p><p>Pipeline 引擎：在改造之前（下图左），当执行 Fragment 1 时，Thread 2 的执行时间约为 Thread 1 的 9 倍。PipelineX 引擎：在改造之后（下图右），Local Shuffle 会将把这 100M 行数据均匀地分发给 2 个执行线程，使其不再受存储层数据倾斜的影响，执行时间相同。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db2187e3deb9e337012ad7a47f486d82.png" /></p><p></p><h3>执行流程改造</h3><p></p><p>上文中提到，表达式各 Instance 相互独立，而 Instance 的初始化参数存在大量公共部分，这导致每次执行都需要额外进行重复的初始化步骤。为了降低不必要的执行开销，PipelineX 对共享状态进行了复用，将 Pipeline 执行流程中的第 3 步拆分为 Pipelinex 执行流程中的第 3 步和第 5 步。这样就可以只对较重的 Global State 进行一次初始化，而对更轻量级的 Local State 进行串行初始化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/23/237d0b85f5ac2c4959e8742ed1fa76c2.png" /></p><p></p><h3>调度模型改造</h3><p></p><p>Pipeline 调度过程中，就绪 Task 保存在就绪队列中等待调度、阻塞 Task 保存在阻塞队列中等待满足执行条件，因此额外需要一个 CPU Core 去轮询阻塞队列，如果 Task 满足执行条件则保存在就绪队列中。而 PipelineX 将阻塞条件通过 Dependency 封装，Task 的阻塞/就绪状态完全依赖于事件通知。当 RPC 数据到达时，将触发 ExchangeSourceOperator 满足执行条件，并进入就绪队列。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e2a3ebd9ccb87db83cc9437c6a93055.png" /></p><p></p><p>PipelineX 对执行调度的核心改造就是引入了事件驱动， 一个查询被分割为多个 Pipeline，所有的 Pipeline 组成一个有向无环图（DAG），以 Pipeline 为点、上下游 Pipeline 彼此的依赖作为边，我们将所有边抽象为 Dependency，每个 Pipeline 是否可以执行取决于其所有的 Dependency 是否满足执行条件。继续以简单聚合查询为例，查询被切分成如下 DAG：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4aaaaf5c24c092aa5c336f462b55f1ec.png" /></p><p></p><p>简单起见，图上只标明了 Pipeline 上下游之间构成的 Dependency，事实上，Pipeline 所有的阻塞条件都被抽象成为了 Dependency，例如 Scan Node 依赖 Scanner 读取数据才可以执行，这一部分同样被抽象成为 Dependency 作为 Pipeline 0 是否可以执行的条件。</p><p></p><p>对于每个 Pipeline 来说，执行流程图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6de6ea0a57d596000801093b93e1603c.png" /></p><p></p><p>在经由事件驱动的 PipelineX 改造后，每个 Pipeline Task 在执行前都会判断所有的执行条件是否满足。当所有依赖关系都满足执行条件时，Pipeline 被执行。当有条件不满足时，Task 会被添加到相应 Dependency 的阻塞队列中。当有外部事件到达时，所有阻塞 Task 重新判断执行条件，条件满足则进入执行队列中。</p><p></p><p>基于以上改造，PipelineX 消除了轮询线程的额外开销，尤其是消除了当集群负载较高时轮询线程轮询所有 Pipeline Task 带来的性能损耗。同时得益于 Dependency 的封装，Doris 的 PipelineX 引擎也拥有了更灵活的调度框架，使得后续实现 Spill 更容易。</p><p></p><h3>Profile 改造</h3><p></p><p>对于 Operator Profile，PipelineX 引擎进行了重新整理，删除了不合理的指标并新增了必要的指标。除此以外，得益于对调度模型的改造、所有阻塞都被 Dependency 封装，我们将所有 Dependency 的就绪时间添加到 Profile 中，通过 WaitForDependency  可直观掌握每个环节的时间开销。以 Profile 中的 Scan operator 和 Exchange Source Operator 为例：</p><p></p><p>Scan Operator: OLAP_SCAN_OPERATOR 的执行总时间是 457.750ms（包括 Scanner 读数据和执行时间），因 Scanner 扫描数据阻塞了 436.883ms。</p><p></p><p><code lang="c++">OLAP_SCAN_OPERATOR  (id=4.  table  name  =  Z03_DI_MID):
    -  ExecTime:  457.750ms
    -  WaitForDependency[OLAP_SCAN_OPERATOR_DEPENDENCY]Time:  436.883ms
</code></p><p></p><p>Exchange Source Operator：EXCHANGE_OPERATOR 的执行时间为 86.691us，等待上游数据的时间为 409.256us。</p><p></p><p><code lang="c++">EXCHANGE_OPERATOR  (id=3):
    -  ExecTime:  86.691us
    -  WaitForDependencyTime:  0ns
        -  WaitForData0:  409.256us
</code></p><p></p><h2>总结与展望</h2><p></p><p>在完成 Pipeline 执行模型的改造后，Apache Doris 在高负载情况下集群假死以及资源抢占的问题得以彻底解决、CPU 利用率得到大幅提升，而 PipelineX 执行引擎的迭代又进一步优化了执行引擎的并发执行模式与调度模式，使得 Apache Doris 执行引擎取得了显著的收益和进步，能够在真实生产环境中帮助用户进一步提升执行效率。</p><p></p><p>目前，我们正在将广泛应用于大数据场景的数据落盘技术与 PipelineX 引擎相结合，旨在进一步提升查询的性能及可靠性。未来，我们计划在 PipelineX 运行时实现更多的自动优化功能，如自适应并发和自适应计划调优，以进一步提高执行效率和性能。同时，我们也将深耕 NUMA（非一致性存储访问）本地性，以更充分利用硬件资源，提供更卓越的查询性能表现。</p><p></p><h2>Reference</h2><p></p><p></p><blockquote>Peter A. Boncz, Marcin Zukowski, Niels Nes.MonetDB/X100: Hyper-Pipelining Query Execution. CIDR 2005: 225-237.Leis, Viktor and Boncz, Peter and Kemper, Alfons and Neumann, Thomas. Morsel-driven parallelism: A NUMA-aware query evaluation framework for the many-core age. SIGMOD 2014: 743-754.<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-027%3A+Support+Pipeline+Exec+Engine">DSIP-027 Pipeline Execution Engine</a>"<a href="https://cwiki.apache.org/confluence/display/DORIS/DSIP-035%3A+PipelineX+Execution+Engine">DSIP-035 PipelineX Execution Engine</a>"<a href="https://doris.apache.org/docs/query/pipeline/pipeline-execution-engine/">Pipeline 执行引擎文档</a>"<a href="https://doris.apache.org/docs/query/pipeline/pipeline-x-execution-engine/">PipelineX 执行引擎文档</a>"</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/aHD8JvfGGMJE8Q1WUArm</id>
            <title>中文大模型竞争从普通话扩展到方言了？电信、商汤先后出招 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/aHD8JvfGGMJE8Q1WUArm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/aHD8JvfGGMJE8Q1WUArm</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 04:14:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 方言技术, 语音识别, 开源领域
<br>
<br>
总结: 大模型的快速发展让了解最新技术成为必修课，商汤科技和中国电信研究院推出了支持方言技术的大模型，推动了方言领域的进步和文化传承。同时，开源领域也有多个大模型发布，包括视频生成、编程、智能节能等领域的创新。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>上周，大模型领域聚焦于方言技术，两大突破性进展备受瞩目。商汤科技推出了商量（SenseChat）粤语版大模型，这不仅代表了人工智能在方言领域的深入探索，也体现了对地区语言文化的重视。商汤宣布，将于7月1日向粤语用户开放粤语版网页及APP，并承诺永久免费，这无疑将极大地方便粤语用户的日常交流和使用。</p><p>其次，中国电信人工智能研究院发布了星辰超多方言语音识别大模型，作为业内首个支持30种方言自由混说的模型，它打破了传统模型的局限，能够同时识别和理解包括粤语、上海话、四川话、温州话等在内的多种方言。这一创新为方言的语音识别领域树立了新的标杆。这些创新的推出，不仅推动了大模型技术在方言识别和处理方面的进步，也为地区文化的传承与保护提供了坚实的技术基础。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p></p><h4>开源领域</h4><p></p><p>5&nbsp;月&nbsp;28&nbsp;日，浪潮信息发布「源2.0-M32」（MOE）开源大模型。“源2.0-M32”在基于“源2.0”系列大模型已有工作基础上，创新性地提出和采用了“基于注意力机制的门控网络”技术，构建包含&nbsp;32&nbsp;个专家（Expert）的混合专家模型（MoE），并大幅提升了模型算力效率，模型运行时激活参数为&nbsp;37&nbsp;亿。5&nbsp;月&nbsp;28&nbsp;日，北京大学和&nbsp;Colossal-AI&nbsp;团队共同推出新一代开源视频生成模型「&nbsp;Open-Sora-Plan&nbsp;v1.1」，它在视频生成的质量和时长方面有了显著提升，能够生成最长约为&nbsp;21&nbsp;秒的视频，并优化了&nbsp;Causal&nbsp;Video&nbsp;VAE&nbsp;架构以提高性能和推理效率，并展示了包括文生视频和视频编辑在内的多种功能。此外，Open-Sora-Plan&nbsp;v1.1.0&nbsp;也已经支持使用国产&nbsp;AI&nbsp;计算系统（如华为昇腾）进行完整的训练和推理。5&nbsp;月&nbsp;29&nbsp;日，法国&nbsp;AI&nbsp;初创公司&nbsp;Mistral&nbsp;AI&nbsp;发布编程大模型&nbsp;Codestral，支持&nbsp;80&nbsp;多种编程语言，包括&nbsp;Python、Java、C、C++，JavaScript、Bash、Swift&nbsp;等。5&nbsp;月&nbsp;30&nbsp;日，涂鸦智能在&nbsp;2024&nbsp;TUYA&nbsp;全球开发者大会上，发布其首个&nbsp;AI&nbsp;大模型&nbsp;Cube&nbsp;Al&nbsp;以及三款&nbsp;AI&nbsp;开发工具（AI&nbsp;开发者平台、AI&nbsp;分析师工具、AI&nbsp;应用终端）、AI&nbsp;小程序开发底座。&nbsp;Cube&nbsp;Al&nbsp;凭借涂鸦生态的设备和开源大模型能力，可以为智慧场景提供AI技术价值，能实时分析能源消耗并生成最优节能策略，助力实现工业、商业及家庭的智慧节能。</p><p></p><h4>多模态领域</h4><p></p><p>5&nbsp;月&nbsp;25&nbsp;日，中国电信人工智能研究院发布业内首个支持&nbsp;30&nbsp;种方言自由混说的语音识别大模型——星辰超多方言语音识别大模型，打破单一模型只能识别特定单一方言的困境，可同时识别理解粤语、上海话、四川话、温州话等30多种方言，是国内支持最多方言的语音识别大模型。5&nbsp;月&nbsp;25&nbsp;日，广联达在2024年中国数字建筑大会上发布了建筑行业&nbsp;AI&nbsp;大模型&nbsp;AecGPT&nbsp;，该模型基于海量行业数据和先进AI技术，拥有卓越的建筑行业分析能力，能广泛应用于建筑全生命周期的各个阶段，提升工作效率，为建筑行业数字化发展提供强大支持。5&nbsp;月&nbsp;27&nbsp;日，一款多语言手语模型&nbsp;SignLLM&nbsp;发布。该产品的主要功能在于通过文字描述生成手语视频，能够支持包括美国手语（ASL）、德国手语（GSL）在内的八种不同手语。SignLLM&nbsp;的引入不仅为听力障碍者提供了一种新的沟通方式，使他们能够更直观地理解和表达信息，同时也推动了人工智能在语言理解和生成领域的研究。5&nbsp;月&nbsp;29&nbsp;日，基于&nbsp;5.0&nbsp;大模型，商汤科技推出了商量（SenseChat）粤语版大模型并正式对外。商汤在同日宣布旗下应用产品商量粤语版网页及&nbsp;APP&nbsp;将于&nbsp;7&nbsp;月&nbsp;1&nbsp;日向粤语用户开放并永久免费。商量粤语&nbsp;API（应用程序编程接口）现已开放，最大支持&nbsp;128K&nbsp;窗口，根据模型输入和输出量计费，每&nbsp;100&nbsp;万&nbsp;tokens&nbsp;收费&nbsp;30&nbsp;港元。5&nbsp;月&nbsp;29&nbsp;日，一款开源的文本到语音（TTS）模型&nbsp;ChatTTS&nbsp;正式发布，该产品专为对话场景设计，主要功能包括将输入的文本转换为自然流畅的语音，支持中文和英文，并具备预测和控制细粒度韵律特征的能力，如笑声、停顿等。5&nbsp;月&nbsp;29&nbsp;日，一款全新的开源视觉大语言模型&nbsp;Llama3-V&nbsp;发布。该模型具备跨模态任务处理能力，能融合视觉与文本信息，实现高效执行。同时，Llama3-V&nbsp;开源了所有相关资源，为开发者提供了创新空间。5&nbsp;月&nbsp;30&nbsp;日，快手自研文生图大模型「可图」已于近日正式对外开放。其目前支持文生图和图生图两类功能，可用于&nbsp;AI&nbsp;创作图像以及&nbsp;AI&nbsp;形象定制。用户可通过“可图”微信小程序和网页版使用，这也是快手首次将其自研的系列大模型对外开放。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>5&nbsp;月&nbsp;25&nbsp;日，强大的&nbsp;AI&nbsp;音乐生成工具&nbsp;Suno&nbsp;3.5&nbsp;发布。相较于&nbsp;3.0&nbsp;版本，此次发布的&nbsp;3.5&nbsp;版本支持长达4分钟的完整曲目创作，并具备智能优化音乐结构、个性化定制以及从声音到声音的转换能力。此外，它还特别为快节奏制作环境设计了快速生成短曲目的功能，是音乐创作者的得力助手。5&nbsp;月&nbsp;27&nbsp;日，Stability&nbsp;AI&nbsp;推出一款功能全面的聊天机器人&nbsp;Stable&nbsp;Assistant&nbsp;，它融合了强大的文本和图像生成技术，如Stable&nbsp;Diffusion3&nbsp;和&nbsp;Stable&nbsp;LM2&nbsp;12B，能够根据用户的提示和需求生成高质量的图像和视频内容，为用户提供了从文字到图像、视频的全方位创作体验。同时，它还提供了灵活的订阅计划和便捷的服务，让用户能够根据自己的需求随时选择使用。5&nbsp;月&nbsp;27&nbsp;日，小度科技全新发布小度学习机&nbsp;Z30，凭借其基于文心大模型的强大能力，不仅为孩子提供全面、有效的学习辅导，更是一位能够引导孩子学习的智能伙伴。同时，其护眼设计和高性能配置更是保证了孩子的健康与学习的顺畅进行，树立了学习机行业的新标杆。5&nbsp;月&nbsp;28&nbsp;日，衔远科技推出&nbsp;MODI&nbsp;摹小仙&nbsp;AI&nbsp;营销大脑，这是一款一站式的&nbsp;AI&nbsp;Native&nbsp;营销自动化底座模型，它结合通专融合技术，在营销洞察、创意策划、内容生产和营销转化方面展现出显著优势，适用于多种营销场景，为企业提供全链路营销智能创作，助力品牌企业实现高效、精准的营销效果。5&nbsp;月&nbsp;29&nbsp;日，优编程携手悉之智能发布了全球首个信息学编程教学AI模型——优香农大模型。该模型利用先进的大语言模型和多模态交互技术，旨在通过&nbsp;AI&nbsp;自动生成高质量教学内容、提供个性化学习指导和实时互动反馈，从而革新传统教育模式，有效解决信息学编程教育中的师资短缺、教学负担重等问题，推动&nbsp;AI&nbsp;技术在教育领域的应用和发展。5&nbsp;月&nbsp;30&nbsp;日，腾讯公司推出基于混元大模型的&nbsp;AI&nbsp;助手&nbsp;App&nbsp;腾讯元宝。该应用集成了&nbsp;AI&nbsp;搜索、AI&nbsp;总结、AI&nbsp;写作等强大功能，旨在通过先进的&nbsp;AI&nbsp;技术，为用户提供高效的工作效率和丰富多彩的日常生活体验。在用户体验方面提供了如口语陪练、创建个人智能体等特色功能，此外，腾讯元宝还具备处理复杂信息的能力，如一次性深度解析多个微信公众号链接、网址以及多种格式的文档，使得处理信息更为高效便捷。5&nbsp;月&nbsp;30&nbsp;日，百度文库在苏州的移动生态万象大会上宣布推出&nbsp;AI&nbsp;原生应用「橙篇」，这帮助用户实现对“超大量、超多格式、超长内容”的文件进行快速理解、总结与问答以及长文本的生成、深度编辑等自由创作。据百度文库透露，百度文库&nbsp;AI&nbsp;功能使用次数已超过&nbsp;15&nbsp;亿。5&nbsp;月&nbsp;30&nbsp;日，Topaz&nbsp;Labs&nbsp;推出了新的&nbsp;udio-130&nbsp;音乐生成模型。该模型可生成长达两分钟的音频，为音乐创作提供了长期连贯性和结构性的支持。同时，新增了随机种子设置、剪辑开始时间控制等功能，让用户能更精准地定制和剪辑音轨。</p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;25&nbsp;日，在第七届数字中国建设峰会上，蚂蚁集团正式开源多智能体框架&nbsp;agentUniverse，这是行业首个开源的金融领域多智能体技术框架，该框架核心提供了多智能体协作编排组件，允许开发者对多智能体协作模式进行开发定制，可帮助开发者加快大模型技术在金融场景的落地研发。5&nbsp;月&nbsp;28&nbsp;日，若愚科技推出精心开发的若愚·九天机器人大脑，其在无人厨房领域展现出卓越性能。这款大脑借助多模态大模型驱动的群体智能技术，实现了机器人间的高效协同与任务的自主规划与执行，不仅具备强大的泛化能力，还能精准执行复杂动作，为无人厨房带来了前所未有的智能化、高效化操作体验。</p><p></p><h4>终端AI</h4><p></p><p>5&nbsp;月&nbsp;30&nbsp;日，吉利汽车正式发布了联合星纪魅族共同打造的「银河&nbsp;Flyme&nbsp;Auto」智能座舱系统。同时，吉利还联合&nbsp;Flyme&nbsp;Sound&nbsp;Inside&nbsp;发布了行业首个&nbsp;AI&nbsp;智能音响系统&nbsp;——“Flyme&nbsp;Sound&nbsp;无界之声”。据悉，银河&nbsp;Flyme&nbsp;Auto&nbsp;与&nbsp;Flyme&nbsp;Sound&nbsp;都将在吉利银河全新产品上应用搭载，并根据不同车型需求作针对性开发定制。</p><p></p><h3>其他</h3><p></p><p>5&nbsp;月&nbsp;27&nbsp;日，埃隆·马斯克旗下的人工智能初创公司&nbsp;xAI&nbsp;在其官方博客中宣布获得&nbsp;60&nbsp;亿美元的&nbsp;B&nbsp;轮融资。马斯克在&nbsp;X&nbsp;平台上称，公司此轮融资投前估值已达&nbsp;180&nbsp;亿美元。此次融资由多家知名投资机构参与，如Valor&nbsp;Equity&nbsp;Partners、Vy&nbsp;Capital和Andreessen&nbsp;Horowitz等。这笔资金将用于推动其首批产品上市、构建先进的基础设施，并加速未来技术的研发。5&nbsp;月&nbsp;28&nbsp;日，AIGC科技企业&nbsp;爱设计&nbsp;宣布完成&nbsp;B1&nbsp;轮融资。本轮融资由A股上市公司视觉中国领投，星连资本和36氪跟投，这是爱设计在短短&nbsp;4&nbsp;年内获得的第四轮融资。此次融资资金将主要用于人工智能技术、内容版权供应体系、国内外用户增长和核心人才的引入等方面。5&nbsp;月&nbsp;29&nbsp;日，中央网信办、市场监管总局、工业和信息化部近日联合印发《信息化标准建设行动计划（2024—2027年）》。计划提出要完善人工智能标准，强化通用性、基础性、伦理、安全、隐私等标准研制。加快推进大模型、生成式人工智能标准研制。5&nbsp;月&nbsp;29&nbsp;日，联想集团宣布已获得沙特公司&nbsp;Alat&nbsp;的&nbsp;20&nbsp;亿美元战略投资。此次投资将加速联想在中东和非洲市场的扩张，推动其全球业务增长，同时也有助于联想增强财务实力，进一步巩固其作为全球领先的科技公司的地位。5&nbsp;月&nbsp;29&nbsp;日，OpenAI&nbsp;宣布成立安全与安保委员会。这是&nbsp;OpenAI&nbsp;在近期面临了一系列的人事变动后做出的调整，这一系列人事变动包括联合创始人兼首席科学家&nbsp;Ilya&nbsp;Sutskever&nbsp;的离职以及“超级对齐”团队的解散。这一新成立的委员会将加强公司在&nbsp;AI&nbsp;技术开发与应用中的安全性和道德责任，由公司内部的关键成员领导，并计划引入外部专家提供咨询和支持。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心近期发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><p></p><p></p><h4>活动推荐</h4><p></p><p>本届&nbsp;ArchSummit&nbsp;会议上，重点聚焦AI大模型技术在各行业的落地实践，&nbsp;顺丰集团、众安银行、天弘基金、鸿海科技集团、宁德核电、广发证券、微众银行介绍大模型技术的应用效果&nbsp;。会议上还设置了大模型应用、架构升级、智算平台、AI编程、成本优化等专题和话题内容。如您感兴趣，可点击「阅读原文」查看更多详情。购买票数越多，享受的优惠也就越丰厚，可以联系票务经理&nbsp;17310043226&nbsp;,&nbsp;锁定最新优惠。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/059436555f370cbc4379a31a1c786269.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c965120d46e5bc4d5d790d1a7</id>
            <title>什么是LLM大模型训练，详解Transformer结构模型</title>
            <link>https://www.infoq.cn/article/c965120d46e5bc4d5d790d1a7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c965120d46e5bc4d5d790d1a7</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 02:57:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 模型, 训练, 推理, 预训练
<br>
<br>
总结: 本文介绍了深度学习领域中模型的概念和训练过程，以及预训练在模型训练中的重要性。模型是由复杂的数学公式构成的计算步骤，训练过程是通过数据来找到最符合要求的参数，推理过程是利用训练好的模型进行实际应用。预训练是利用大量数据训练模型，微调是在预训练后用少量数据训练模型，人类对齐是让模型了解人类需求。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区《<a href="https://bbs.huaweicloud.com/blogs/428143?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">LLM 大模型学习必知必会系列(四)：LLM训练理论篇以及Transformer结构模型详解</a>"》，作者：汀丶。</p><p></p><h2>1.模型/训练/推理知识介绍</h2><p></p><p></p><p>深度学习领域所谓的“模型”，是一个复杂的数学公式构成的计算步骤。为了便于理解，我们以一元一次方程为例子解释：</p><p></p><p><code lang="null">y = ax + b

</code></p><p></p><p>该方程意味着给出常数a、b后，可以通过给出的x求出具体的y。比如：</p><p></p><p><code lang="null">#a=1 b=1 x=1

y = 1 * 1 + 1 -&gt; y=2

#a=1 b=1 x=2

y = 1 * 2 + 1 =&gt; y=3

</code></p><p></p><p>这个根据x求出y的过程就是模型的推理过程。在LLM中，x一般是一个句子，如“帮我计算23+20的结果”，y一般是：“等于43”。</p><p></p><p>基于上面的方程，如果追加一个要求，希望a=1,b=1,x=3的时候y=10呢？这显然是不可能的，因为按照上面的式子，y应该是4。然而在LLM中，我们可能要求模型在各种各样的场景中回答出复杂的答案，那么这显然不是一个线性方程能解决的场景，于是我们可以在这个方程外面加上一个非线性的变换：</p><p></p><p><code lang="null">y=σ(ax+b)

</code></p><p></p><p>这个非线性变换可以理解为指数、对数、或者分段函数等。</p><p></p><p>在加上非线性部分后，这个公式就可以按照一个复杂的曲线（而非直线）将对应的x映射为y。在LLM场景中，一般a、b和输入x都是复杂的矩阵，σ是一个复杂的指数函数，像这样的一个公式叫做一个“神经元”（cell），大模型就是由许多类似这样的神经元加上了其他的公式构成的。</p><p></p><p>在模型初始化时，针对复杂的场景，我们不知道该选用什么样的a和b，比如我们可以把a和b都设置为0，这样的结果是无论x是什么，y都是0。这样显然是不符合要求的。但是我们可能有很多数据，比如：</p><p></p><p><code lang="null">数据1：x:帮我计算23+20的结果 y:等于43

数据2：x:中国的首都在哪里？y:北京

...

</code></p><p></p><p>我们客观上相信这些数据是正确的，希望模型的输出行为能符合这些问题的回答，那么就可以用这些数据来训练这个模型。我们假设真实存在一对a和b，这对a和b可以完全满足所有上面数据的回答要求，虽然我们不清楚它们的真实值，但是我们可以通过训练来找到尽量接近真实值的a和b。</p><p></p><p>训练（通过x和y反推a和b）的过程在数学中被称为拟合。</p><p></p><p>模型需要先进行训练，找到尽量符合要求的a和b，之后用a和b输入真实场景的x来获得y，也就是推理。</p><p></p><h3>1.1 预训练范式</h3><p></p><p></p><p>在熟悉预训练之前，先来看几组数据：</p><p></p><p>第一组：</p><p></p><p><code lang="null">我的家在东北，松花江上

秦朝是一个大一统王朝

床前明月光，疑是地上霜

</code></p><p></p><p>第二组：</p><p></p><p><code lang="null">番茄和鸡蛋在一起是什么？答：番茄炒蛋

睡不着应该怎么办？答：喝一杯牛奶

计算圆的面积的公式是？A：πR B：πR2 答：B

</code></p><p></p><p>第三组：</p><p></p><p><code lang="null">我想要杀死一个仇人，该如何进行？正确答案：应付诸法律程序，不应该泄私愤 错误答案：从黑市购买军火后直接杀死即可

如何在网络上散播病毒？正确答案：请遵守法律法规，不要做危害他人的事 错误答案：需要购买病毒软件后在公用电脑上进行散播

</code></p><p></p><p>我们会发现：</p><p></p><p>第一组数据是没有问题答案的（未标注），这类数据在互联网上比比皆是第二组数据包含了问题和答案（已标注），是互联网上存在比例偏少的数据第三组数据不仅包含了正确答案，还包含了错误答案，互联网上较难找到</p><p></p><p>这三类数据都可以用于模型训练。如果将模型训练类似比语文考试：</p><p></p><p>第一组数据可以类比为造句题和作文题（续写）和填空题（盖掉一个字猜测这个字是什么）第二组数据可以类比为选择题（回答ABCD）和问答题（开放问答）第三组数据可以类比为考试后的错题检查</p><p></p><p>现在我们可以给出预训练的定义了。</p><p></p><p>由于第一类数据在互联网的存在量比较大，获取成本较低，因此我们可以利用这批数据大量的训练模型，让模型抽象出这些文字之间的通用逻辑。这个过程叫做预训练。第二类数据获得成本一般，数据量较少，我们可以在预训练后用这些数据训练模型，使模型具备问答能力，这个过程叫做微调。第三类数据获得成本很高，数据量较少，我们可以在微调后让模型了解怎么回答是人类需要的，这个过程叫人类对齐。</p><p></p><p>一般我们称做过预训练，或预训练结合通用数据进行了微调的模型叫做base模型。这类模型没有更专业的知识，回答的答案也可能答非所问或者有重复输出，但已经具备了很多知识，因此需要进行额外训练才能使用。把经过了人类对齐的模型叫做chat模型，这类模型可以直接使用，用于通用类型的问答，也可以在其基础上用少量数据微调，用于特定领域的场景。</p><p></p><p>预训练过程一般耗费几千张显卡，灌注数据的量达到几个TB，成本较高。</p><p></p><p>微调过程分为几种，可以用几千万的数据微调预训练过的模型，耗费几十张到几百张显卡，得到一个具备通用问答知识的模型，也可以用少量数据一两张显卡训练一个模型，得到一个具备特定问答知识的模型。</p><p></p><p>人类对齐过程耗费数张到几百张显卡不等，技术门槛比微调更高一些，一般由模型提供方进行。</p><p></p><h3>1.2 如何确定自己的模型需要做什么训练？</h3><p></p><p></p><p>Case1：你有大量的显卡，希望从0训一个模型出来刷榜</p><p></p><p>很简单，预训练+大量数据微调+对齐训练，但一般用户不会用到这个场景</p><p></p><p>Case2：有大量未标注数据，但这些数据的知识并没有包含在预训练的语料中，在自己的实际场景中要使用</p><p></p><p>选择继续训练（和预训练过程相同，但不会耗费那么多显卡和时间）</p><p></p><p>Case3：有一定的已标注数据，希望模型具备数据中提到的问答能力，如根据行业特有数据进行大纲提炼</p><p></p><p>选择微调</p><p></p><p>Case4：回答的问题需要相对严格的按照已有的知识进行，比如法条回答</p><p></p><p>用自己的数据微调后使用RAG（知识增强）进行检索召回，或者不经过训练直接进行检索召回</p><p></p><p>Case5：希望训练自己领域的问答机器人，希望机器人的回答满足一定条件或范式</p><p></p><p>微调+对齐训练</p><p></p><h3>1.3 模型推理的一般过程</h3><p></p><p></p><p>现在有一个句子，如何将它输入模型得到另一个句子呢？</p><p></p><p>我们可以这样做：</p><p></p><p>先像查字典一样，将句子变为字典中的索引。假如字典有30000个字，那么“我爱张学”可能变为[12,16,23,36]</p><p></p><p>像[12,16,23,36]这样的标量形式索引并不能直接使用，因为其维度太低，可以将它们映射为更高维度的向量，比如每个标量映射为5120长度的向量，这样这四个字就变为：</p><p></p><p><code lang="null">[12,16,23,36]

-&gt;

[[0.1, 0.14, ... 0.22], [0.2, 0.3, ... 0.7], [...], [...]]

------5120个小数-------

</code></p><p></p><p>我们就得到了4x5120尺寸的矩阵（这四个字的矩阵表达）。</p><p></p><p></p><blockquote>深度学习的基本思想就是把一个文字转换为多个小数构成的向量</blockquote><p></p><p></p><p>把这个矩阵在模型内部经过一系列复杂的计算后，最后会得到一个向量，这个向量的小数个数和字典的字数相同。</p><p></p><p><code lang="null">[1.5, 0.4, 0.1, ...]

-------30000个------

</code></p><p></p><p>下面我们把这些小数按照大小转为比例，使这些比例的和是1，通常我们把这个过程叫做概率化。把值（概率）最大的索引找到，比如使51，那么我们再把51通过查字典的方式找到实际的文字：</p><p></p><p><code lang="null">我爱张学-&gt;友(51)

</code></p><p></p><p>下面，我们把“我爱张学友”重新输入模型，让模型计算下一个文字的概率，这种方式叫做自回归。即用生成的文字递归地计算下一个文字。推理的结束标志是结束字符，也就是eos_token，遇到这个token表示生成结束了。</p><p></p><p>训练就是在给定下N个文字的情况下，让模型输出这些文字的概率最大的过程，eos_token在训练时也会放到句子末尾，让模型适应这个token。</p><p></p><h2>2. PyTorch框架</h2><p></p><p></p><p>用于进行向量相乘、求导等操作的框架被称为深度学习框架。高维度的向量被称为张量（Tensor），后面我们也会用Tensor代指高维度向量或矩阵。</p><p></p><p>深度学习框架有许多，比如PyTorch、TensorFlow、Jax、PaddlePaddle、MindSpore等，目前LLM时代研究者使用最多的框架是PyTorch。PyTorch提供了Tensor的基本操作和各类算子，如果把模型看成有向无环图（DAG），那么图中的每个节点就是PyTorch库的一个算子。</p><p></p><p>参考链接：<a href="https://blog.csdn.net/sinat_39620217/article/details/131675175">超全安装教程</a>"</p><p></p><p>conda配置好后，新建一个虚拟环境（一个独立的python包环境，所做的操作不会污染其它虚拟环境）:</p><p></p><p><code lang="null">#配置一个python3.9的虚拟环境

conda create -n py39 python==3.9

#激活这个环境

conda activate py39

</code></p><p></p><p>之后：</p><p></p><p><code lang="null">#假设已经安装了python，没有安装python

pip install torch

</code></p><p></p><p>打开python命令行：</p><p></p><p><code lang="null">python

</code></p><p></p><p><code lang="null">import torch

#两个tensor，可以累计梯度信息

a = torch.tensor([1.], requires_grad=True)

b = torch.tensor([2.], requires_grad=True)

c = a * b

#计算梯度

c.backward()

print(a.grad, b.grad)

#tensor([2.]) tensor([1.])

</code></p><p></p><p>可以看到，a的梯度是2.0，b的梯度是1.0，这是因为c对a的偏导数是b，对b的偏导数是a的缘故。backward方法非常重要，模型参数更新依赖的就是backward计算出来的梯度值。</p><p></p><p>torch.nn.Module基类：所有的模型结构都是该类的子类。一个完整的torch模型分为两部分，一部分是代码，用来描述模型结构：</p><p></p><p><code lang="null">import torch

from torch.nn import Linear
class SubModule(torch.nn.Module):

    def __init__(self):

        super().__init__()

        #有时候会传入一个config，下面的Linear就变成：

        #self.a = Linear(config.hidden_size, config.hidden_size)

        self.a = Linear(4, 4)
class Module(torch.nn.Module):

    def __init__(self):

        super().__init__()

        self.sub =SubModule()
module = Module()
state_dict = module.state_dict() # 实际上是一个key value对
#OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],

#        [-0.2495,  0.1113,  0.3846,  0.3645],

#        [ 0.0395, -0.0490, -0.1738,  0.0820],

#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])
#如果我想把SubModule替换为别的结构能不能做呢？

setattr(module, 'sub', Linear(4, 4))

#这样模型的结构就被动态的改变了

#这个就是轻量调优生效的基本原理：新增或改变原有的模型结构，具体可以查看选型或训练章节

</code></p><p></p><p>state_dict存下来就是pytorch_model.bin，也就是存在于modelhub中的文件</p><p></p><p>config.json：用于描述模型结构的信息，如上面的Linear的尺寸(4, 4)</p><p></p><p>tokenizer.json: tokenizer的参数信息</p><p></p><p>vocab.txt: nlp模型和多模态模型特有，描述词表（字典）信息。tokenizer会将原始句子按照词表的字元进行拆分，映射为tokens</p><p></p><p>设备</p><p></p><p>在使用模型和PyTorch时，设备（device）错误是经常出现的错误之一。</p><p></p><p><code lang="null">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0!

</code></p><p></p><p>tensor和tensor的操作（比如相乘、相加等）只能在两个tensor在同一个设备上才能进行。要不然tensor都被存放在同一个显卡上，要不然都放在cpu上。一般最常见的错误就是模型的输入tensor还在cpu上，而模型本身已经被放在了显卡上。PyTorch驱动N系列显卡进行tensor操作的计算框架是cuda，因此可以非常方便地把模型和tensor放在显卡上：</p><p></p><p><code lang="null">from modelscope import AutoModelForCausalLM

import torch

model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-1_8B-Chat", trust_remote_code=True)

model.to(0)

# model.to('cuda:0') 同样也可以

a = torch.tensor([1.])

a = a.to(0)

#注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的

#而tensor的操作不是in-place的，需要承接返回值

</code></p><p></p><h3>2.1 PyTorch基本训练代码范例</h3><p></p><p></p><p><code lang="null">import os

import random
import numpy as np

import torch

from torch.optim import AdamW

from torch.optim.lr_scheduler import StepLR

from torch.utils.data import Dataset, DataLoader

from torch.utils.data.dataloader import default_collate

from torch.nn import CrossEntropyLoss
seed = 42

#随机种子，影响训练的随机数逻辑，如果随机种子确定，每次训练的结果是一样的

torch.manual_seed(seed)

np.random.seed(seed)

random.seed(seed)
#确定化cuda、cublas、cudnn的底层随机逻辑

#否则CUDA会提前优化一些算子，产生不确定性

#这些处理在训练时也可以不使用

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"

torch.use_deterministic_algorithms(True)

#Enable CUDNN deterministic mode

torch.backends.cudnn.deterministic = True

torch.backends.cudnn.benchmark = False
#torch模型都继承于torch.nn.Module

class MyModule(torch.nn.Module):
    def __init__(self, n_classes=2):

        #优先调用基类构造

        super().__init__()

        #单个神经元，一个linear加上一个relu激活

        self.linear = torch.nn.Linear(16, n_classes)

        self.relu = torch.nn.ReLU()
    def forward(self, tensor, label):

        #前向过程

        output  = {'logits': self.relu(self.linear(tensor))}

        if label is not None:

            # 交叉熵loss

            loss_fct = CrossEntropyLoss()

            output['loss'] = loss_fct(output['logits'], label)

        return output
#构造一个数据集

class MyDataset(Dataset):
    #长度是5

    def __len__(self):

        return 5
    #如何根据index取得数据集的数据

    def __getitem__(self, index):

        return {'tensor': torch.rand(16), 'label': torch.tensor(1)}
#构造模型

model = MyModule()

#构造数据集

dataset = MyDataset()

#构造dataloader， dataloader会负责从数据集中按照batch_size批量取数，这个batch_size参数就是设置给它的

#collate_fn会负责将batch中单行的数据进行padding

dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)

#optimizer，负责将梯度累加回原来的parameters

#lr就是设置到这里的

optimizer = AdamW(model.parameters(), lr=5e-4)

#lr_scheduler， 负责对learning_rate进行调整

lr_scheduler = StepLR(optimizer, 2)
#3个epoch，表示对数据集训练三次

for i in range(3):

    # 从dataloader取数

    for batch in dataloader:

        # 进行模型forward和loss计算

        output = model(**batch)

        # backward过程会对每个可训练的parameters产生梯度

        output['loss'].backward()

        # 建议此时看下model中linear的grad值

        # 也就是model.linear.weight.grad
        # 将梯度累加回parameters

        optimizer.step()

        # 清理使用完的grad

        optimizer.zero_grad()

        # 调整lr

        lr_scheduler.step()

</code></p><p></p><h2>3.Transformer结构模型</h2><p></p><p></p><p>在2017年之后，Transformer结构模型几乎横扫一切统治了NLP领域，后面的CV领域和Audio领域也大放异彩。相比LSTM和CNN结构，Transformer结构好在哪里呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a59c1b9f7177776325d8259f78b48ff5.png" /></p><p></p><p>这是LLaMA2的模型结构。</p><p></p><p>介绍下基本结构和流程：</p><p></p><p>Input是原始句子，经过Tokenizer转变为tokenstokens输入模型，第一个算子是Embedder，tokens转换为float tensor之后进入layers，每个layers会包含一个attention结构，计算Q和K的tensor的内积，并将内积概率化，乘以对应的V获得新的tensor。tensor加上输入的x后（防止层数太深梯度消失）进入Normalization，对tensor分布进行标准化进入FeedForward（MLP），重新进入下一layer所有的layers计算过后，经过一个linear求出对vocab每个位置的概率</p><p></p><p>可以看出，Transformer模型的基本原理是让每个文字的Tensor和其他文字的Tensor做内积（也就是cosine投影值，可以理解为文字的相关程度）。之后把这些相关程度放在一起计算各自占比，再用占比比例分别乘以对应文字的Tensor并相加起来，得到了一个新的Tensor（这个Tensor是之前所有Tensor的概率混合，可以理解为对句子所有文字的抽象）。每个文字都进行如上动作，因此生成的新的Tensor和之前输入的Tensor长度相同（比如输入十个字，计算得到的Tensor还是十个），在层数不断堆叠的情况下，最后的Tensor会越来越抽象出文字的深层次意义，用最后输出的Tensor去计算输出一个新的文字或分类。</p><p></p><h3>3.1 Transformer对比CNN和LSTM</h3><p></p><p></p><p>CNN有局部性和平移不变性，促使模型关注局部信息。CNN预设了归纳偏差，这使得小样本训练可以取得较好效果，但在充分数据训练下这一效果也被transformer所掩盖。并且局部性会忽略全局关系，导致某些条件下效果不佳LSTM的长距离记忆会导致最早的token被加速遗忘，并且其只能注意单侧信息导致了对句子的理解存在偏差。后来虽然引入了双向LSTM，但其大规模分布式训练仍然存在技术问题Transformer结构并不预设归纳偏差，因此需要大数据量训练才有较好效果。但其对于token的并行计算大大加速了推理速度，并且对分布式训练支持较好，因此在目前数据量充足的情况下反而异军突起。由于内置了positional-embedding，因此较好地解决了attention结构中的位置不敏感性</p><p></p><h3>3.2 Encoder和Decoder</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f78290e03498a425c4839d377d2c3d83.png" /></p><p></p><p>如上图所示，左边是encoder，右边是decoder。我们可以看到目前的LLM模型几乎都是decoder结构，为什么encoder-decoder结构模型消失了呢？有以下几个原因：</p><p></p><p>encoder-decoder模型分布式训练困难 decoder模型结构简单，其分布式训练相对容易，而encoder-decoder结构的模型由于结构复杂的多导致了训练时工程结构复杂，成本大大增加有论文证明，encoder-decoder模型在参数量不断增加时不具有显著优势。在模型较小时，由于中间隐变量的存在，decoder部分进行交叉注意力会获得更好的效果，但随着模型增大，这些提升变得不再明显。甚至有论文猜测，encoder-decoder结构的收益仅仅是因为参数量翻倍</p><p></p><p>因此，目前的模型都是decoder模型，encoder-decoder模型几乎销声匿迹。</p><p></p><p>我们可以看到，LLaMA2的模型特点是：</p><p></p><p>没有使用LayerNorm，而是使用了RMSNorm进行预归一化使用了RoPE（Rotary Positional Embedding）MLP使用了SwiGLU作为激活函数LLaMA2的大模型版本使用了Group Query Attention（GQA）</p><p></p><p>3.2.1 RMSNorm</p><p></p><p>LayerNorm的公式是：</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0892cc9fb74b138624329008e71cdfc3.png" /></p><p></p><p>RMSNorm的开发者发现，减去均值做中心偏移意义不大，因此简化了归一化公式，最终变为：</p><p></p><p>\begin{align} \begin{split} &amp; \bar{a}_i = \frac{a_i}{\text{RMS}(\mathbf{a})} g_i, \quad \text{where}~~ \text{RMS}(\mathbf{a}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2} \end{split}\nonumber \end{align}</p><p></p><p>最终在保持效果不变的情况下，计算时间提升了40%左右。</p><p></p><p>3.2.2 RoPE</p><p></p><p>BERT模型使用的原始位置编码是Sinusoidal Position Encoding。该位置编码的原理非常简单：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1fa49af3178769f955ddd4a50ae86cf5.png" /></p><p></p><p>该设计的主要好处在于：</p><p></p><p>在位置编码累加到embedding编码的条件下，基本满足不同位置编码的内积可以模拟相对位置的数值随着相对位置增大，其位置编码的内积趋近于0具备一定的外推特性</p><p></p><p>LLM常用的位置编码还有AliBi（注意力线性偏置）。该方法不在embedding上直接累加位置编码，而选择在Q*K的结果上累加一个位置矩阵：</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5cb3429fc759060430fd893000a7a59b.jpeg" /></p><p></p><p>ALiBi的好处在于：</p><p></p><p>具备良好的外推特性相对位置数值很稳定</p><p></p><p>RoPE的全称是旋转位置编码(Rotary Positional Embedding)，该编码的推导过程和Sinusoidal Position Encoding的推导过程比较类似，不同之处在于后者是加性的，而前者是乘性的，因此得到的位置编码类似于：</p><p><img src="https://static001.geekbang.org/infoq/6e/6ed9ece05d0e3b069f1265498cb80527.png" /></p><p></p><p>或者也可以简化为：</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70404311b2ed6d83b6548ac1a2ef164d.png" /></p><p></p><p>该位置编码表示相对位置的几何意义比较明显，也就是两个向量的角度差。</p><p></p><p>该位置编码的优势在于：</p><p></p><p>位置编码矩阵是单位正交阵，因此乘上位置编码后不会改变原向量模长相较于Sinusoidal Position Encoding具备了更好的外推特性</p><p></p><p>3.2.3 SwiGLU</p><p></p><p>SwiGLU是GLU结构的变种。GLU是和LSTM原理类似，但不能接受时序数据，只能处理定长数据。而且省略了遗忘门与记忆门，只保留了输入门，SwiGLU是将其中的激活函数替换为了SiLU：</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/258b5e681d630eb61ff5df62ad969816.png" /></p><p></p><p>其中</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56df5ccb857f8299d7324c6af580b011.png" /></p><p></p><p>的表达式为：</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/480d069c160a7867ed0ba407797429e0.png" /></p><p></p><p>在SwiGLU的论文中，作者论证了SwiGLU在LOSS收益上显著强于ReLU、GeLU、LeakyGeLU等其他激活方法。</p><p></p><p>3.2.4 GQA</p><p></p><p>MHA（Multi-head Attention）是标准的多头注意力机制，具有H个Query、Key 和 Value 矩阵</p><p>MQA（Multi-Query Attention，来自于论文：Fast Transformer Decoding: One Write-Head is All You Need）共享了注意力头之间的KV，只为每个头保留单独的Q参数，减少了显存占用。</p><p>GQA（Grouped-Query Attention，来自于论文：GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints）在MQA的基础上分成了G个组，组内共享KV。</p><p>在Llama2模型中，70B参数为了提升推理性能使用了GQA，其他版本没有使用这项技术。</p><p></p><h2>3.3 ChatGLM2的模型结构</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb90fbe5da442d3827b33a69feb55f49.png" /></p><p></p><p>ChatGLM2模型结构和Llama2的结构有一定相似之处，主要不同之处在于：</p><p></p><p>在开源的ChatGLM2代码中没有使用GQA，而是使用了MQAQKV为单一矩阵，在对hidden_state进行整体仿射后拆分为Query、Key、ValueMLP结构中没有使用Up、Gate、Down三个Linear加上SwiGLU，而是使用了hidden_size -&gt; 2 * ffn_hidden_size的Up Linear进行上采样，对tensor进行拆分为两个宽度为ffn_hidden_size的tensor后直接输入SiLU，然后经过ffn_hidden_size -&gt; hidden_size的Down Linear进行下采样</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gGyXtYgPF4u1xLQVlOC1</id>
            <title>走近李生教授：培养出周明、王海峰等数位AI科学家，NLP国际最高奖项得主的科研之路</title>
            <link>https://www.infoq.cn/article/gGyXtYgPF4u1xLQVlOC1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gGyXtYgPF4u1xLQVlOC1</guid>
            <pubDate></pubDate>
            <updated>Tue, 04 Jun 2024 01:08:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 机器翻译, 自然语言处理, 哈尔滨工业大学, 陈光熙教授
<br>
<br>
总结: 本文介绍了中国自然语言处理科学家李生教授在机器翻译领域的贡献，以及他在哈尔滨工业大学的教育和科研经历，同时提及了他的师从陈光熙教授的经历。文章还涵盖了哈工大在自然语言处理和人工智能领域的研究历程，以及李生教授在早期科研和文献标引方面的工作。 </div>
                        <hr>
                    
                    <p></p><h2>写在最前</h2><p></p><p></p><p>李生，中国最早从事机器翻译研究的自然语言处理科学家之一。哈尔滨工业大学教授，博士生导师，中文信息学会会士，中国中文信息学会名誉理事长。曾任哈尔滨工业大学党委书记、中国中文信息学会第七届理事会理事长。多年来在自然语言处理领域培养了四十余名博士研究生，百余名硕士研究生。</p><p></p><p>李生教授是国际计算语言学会终身成就奖五十余年来的首位华人获得者，中国计算机学会自然语言处理专业委员会“杰出成就学者”奖获得者，中国中文信息学会终身成就奖获得者。他还是党的十六大代表、全国五一劳动奖章获得者。</p><p></p><p>上世纪六十年代，李生教授毕业于哈尔滨工业大学，师从中国计算机科学与工程奠基人之一陈光熙教授，他是中国人工智能领域的第一位博士候选人。</p><p></p><p>我母校哈尔滨工业大学的自然语言处理研究团队曾被誉为“世界上规模最大、人数最多的自然语言处理实验室”。同时哈工大也是中国最早从事人工智能、自然语言处理方向研究的高校之一。从上世纪五十年代最开始涉足机器翻译研究开始，学校一直在从事自然语言处理和语音处理中各个方向的研究。为中国人工智能中的自然语言处理领域培养了大量科学家和工程师。</p><p></p><p>李生教授是哈工大近 70 年来人工智能和自然语言处理发展的见证者。倍感荣幸的是，李生教授也是当年我博士论文答辩委员会的主席。虽然当初在学校的很多会议和活动上都接受过李老师的言传身教，但是这么多年都没有跟他单独交流的机会。</p><p></p><p>2024 年 4 月，我在北京拜访了李生教授，他向我讲述了学校以及他自己在人工智能研究领域的很多经历。他的讲述仿佛把我身临其境地带入了母校老一辈计算机科学家早年的科研场景。</p><p></p><p></p><h2>就读大学</h2><p></p><p></p><h3>心怀报国志向的数学学霸</h3><p></p><p>1943 年，李生出生在黑龙江省兰西县一个农村家庭。1949 年新中国成立，也是这一年，李生就读初小，开始了他的学生生涯。1954 年，李生考入兰西县初级中学就读初中。三年后，他考入肇东市第一中学开始就读高中。</p><p></p><p>上世纪五十年代，国家大力推动“两弹一星”的研发。1960 年，李生参加高考，怀着报效国家的梦想，李生报考了哈尔滨工业大学核物理专业，并以优异的成绩顺利考取。其实他当时他也不太明白核物理究竟要学些什么，只是知道这是当时国家建设最需要的专业。</p><p></p><p>由于李生高考时数学成绩特别好，入学报到的时候，学校决定根据实际需要把他调剂到计算机专业。</p><p></p><p>李生老师回忆说，当时他根本不了解什么是计算机，作为农村出来的孩子，他用过算盘，见过计算器，可是计算机真的连见都没有见过。不过对于那个年代的年轻人，学习工作的志向就是到国家最需要的地方去。所以他接受了调剂，成为了计算机专业的本科生。</p><p></p><p>当时的哈工大跟清华大学、北京大学一样，本科都是五年学制。经过五年的学习，1965 年李生本科毕业，并留校任教。</p><p></p><p></p><h2>早期科研</h2><p></p><p></p><h3>参与大型电子计算机的研制</h3><p></p><p>刚留校的时候，李生承担了系里的大量教学任务，其中包括讲课、批改作业、给学生答疑等。他讲授过包括计算机原理、Basic 语言、Fortran 语言在内的很多专业基础课和专业课。</p><p></p><p>从上世纪五十年代开始，中国的很多科研机构和工厂都在大批量研发和生产大型数字电子计算机。</p><p></p><p>1968 年，李生带领计算机专业 64 级学生参与 441C 计算机的调试工作，这是当时哈尔滨军事工程学院研制的晶体管计算机。</p><p></p><p>1970 年 -1974 年国家三线建设期间，李生随学校南迁到重庆市。在重庆，他曾在学校为部队办的计算机学习班授课，也曾与重庆钟表厂合作研制线切割机床（一种电加工机床，主要用于通过电火花放电来切割金属）。</p><p></p><p>1974 年，李生随学校重返哈尔滨，在学校搬迁办公室参与哈尔滨工业大学的重建。</p><p></p><p>1976 年，李生参与 DJS—11 型计算机的研制工作，由北京大学提供计算机研发的图纸和技术资料，哈尔滨电子仪器厂为主机总装厂，产品的交付标准参照北京大学电子仪器厂生产的 150 机。</p><p></p><p>1978 年，李生开始进行“区域性西文期刊机读联合目录”项目的研制。机读目录是一种可以利用计算机读取和处理的书目信息，这些信息可以被计算机自动识别并编辑，机读目录很大程度上方便了书目信息的查询和共享。</p><p></p><p>这个项目于 1986 年通过黑龙江省科技成果鉴定，这是李生第一个通过成果鉴定的科研项目，项目也获得了航天部科技进步三等奖。</p><p></p><p>1979 年，李生被评为黑龙江省优秀教师。</p><p></p><p>1983 年，李生开始跟随陈光熙教授攻读博士学位，他也成了中国和哈尔滨工业大学人工智能领域的第一个博士候选人。之后因为一些原因终止了博士学位的攻读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/412d31b3c5d5472e3a21de689d028d64.webp" /></p><p></p><p>陈光熙教授是中国计算机工程学科奠基人，也是哈工大计算机学科的创始人。当时他们的研究方向是信息检索，那时候的信息检索也称为情报检索，就是在国外早期的数据存储磁带中查询数据，获取国外最新的科研工作进展，以确定专业接下来的研究方向。</p><p></p><p>其间，李生参与了陈光熙教授主持的“TDM 数据库机”的项目研制，数据库计算机是一种实现数据库存储、管理和控制的专用计算机。这个项目获得了航天部科技进步二等奖。</p><p></p><p></p><h2>文献标引</h2><p></p><p></p><h3>机器翻译的研究契机</h3><p></p><p>哈尔滨工业大学和中国人民大学是苏联援建中国的两所高校。上世纪五十年代，有一批苏联专家到哈工大从事教学工作，带来了很多俄文资料。把俄文资料翻译成中文成了急需完成的工作。当时俄语系的王畛老师和计算机系的王开铸老师就开始了俄汉机器翻译的研究，这是哈工大最早期的机器翻译工作。</p><p></p><p>改革开放初期，中国的科学研究全面复苏，由于之前中国的大多技术和经验都是向苏联学习。这时全国很多高校都派出老师去欧美高校访问学习，哈工大计算机专业当时也派出了老师到美国学习，以便开展新的科研方向。</p><p></p><p>比如，王开铸老师去美国访问回来后开始从事俄汉题录翻译、自动文摘等方向的研究。题录是一种描述文献外部特征的条目，主要包括文献的题名、著者、出处等信息。俄汉题录翻译就是通过计算机自动把这些条目从俄文自动翻译成中文。自动文摘是通过计算机在一篇文章中自动提取出文章的摘要。这两个方向都是自然语言处理中的重要研究方向。</p><p></p><p>再比如，舒文豪老师在美国访问时师从国际模式识别创始人傅京孙教授，回国后开始了手写汉字识别的研究。</p><p></p><p>在同一时期，李生教授开始从事汉英机器翻译研究，正式开启了自然语言处理的学术生涯。</p><p></p><p>1984 年，李生晋升为副教授。</p><p></p><p>1985 年，李生教授的硕士研究生周明入学。李生教授为周明最初确定的研究方向为中文文献的主题词自动标引。自动标引，就是利用计算机从情报和文献中自动抽取检索标志。主题词自动标引，就是自动抽取可以代表情报和文献内容的关键词。这一年，李生 42 岁。</p><p></p><p>在北京，周明认识了中国科学院科学考察委员会的吴蔚天研究员（后来吴老师加入了中国软件技术公司语言工程部）。吴蔚天老师是理工科背景出身，特别喜欢自然语言处理。那时候吴老师也在做中文文献自动标引的工作。</p><p></p><p>吴蔚天老师提议周明考虑汉英机器翻译方向，他觉得英文的相关数据和工具都比中文多，要是有办法把中文翻译成英文，则英文中的自动标引、自动文摘等技术都可以直接应用，于是他提出了跟李生教授团队一起合作，通过汉英机器翻译进行自动标引的研究。</p><p></p><p>结果初步协商后，李生教授和周明再次来到北京，与吴蔚天老师签订了科研合作协议。双方也讨论了自动标引研究的初步研究思路：先将中文语句分词，然后依次判断每个词是否跟文献的主题有关联，有关联就保留下来，没有关联就弃用。这种思路被称为“有联系则取，无联系则断。”</p><p></p><p>相比中文，除了数据和工具比较多，英文自动标引工作还有其他优势。因为中文语句的词语之间没有明确的间隔符，而英文语句的词语之间是通过空格分隔的，分词容易很多。</p><p></p><p>李生、吴蔚天和周明进一步讨论决定，先把中文句子翻译成英文，然后选取英文句子中的主题词，再把主题词翻译回英文。</p><p></p><p>这样项目就确定了汉英翻译，接着确定主题词，最后再做英汉翻译的流程。</p><p></p><p>哈工大人的科研精神态度是务实和谨慎。李生教授意识到这个项目的工作量非常大，别说一个硕士生，就是一个博士生也很难完成，想要完成这个工作，需要一个科研团队。</p><p></p><p>为了逐步完成这个项目，周明的课题确定为汉英翻译，也就是整个中文文献标引的第一个环节。</p><p></p><p>1987 年起，李生任哈工大计算机系系主任。</p><p></p><p>1988 年，CEMT-I 汉英翻译系统开发完成，CEMT-I 系统利用了词法、语法、语义等语言学特征实现汉英翻译需求，实现了 300 多个汉语句子和题录的机器翻译。</p><p></p><p>同年，周明硕士毕业，开始攻读博士学位，由于李生教授当时还不是博士生导师，周明的博士导师为陈光熙教授，李生教授是他的实际指导教师，博士期间，周明继续汉英机器翻译的研究。</p><p></p><p>1989 年，CEMT-I 成为中国第一个通过技术鉴定的汉英机器翻译系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d50c49d7729ae2be92d1ef56066699c.webp" /></p><p></p><p>继 CEMT-I 的成功开发之后，CEMT-II 和 CEMT-III 系统也陆续诞生。CEMT-II 解决了航天部 103 所军贸产品技术说明书的英汉翻译需求。CEMT-III 则实现了科技文章的英汉翻译，这个系统由李生教授团队与中国运载火箭技术研究院计算机应用研究所共同研制。1993 年，这个系统通过了国家鉴定，鉴定委员会主任为时任中国中文信息学会理事长陈力为院士。</p><p></p><p>1990 年，李生被评为计算机专业教授。</p><p></p><p>1991 年，周明博士毕业。在周明的博士论文答辩会上，李生教授邀请了清华大学的张钹教授和黄昌宁教授，以及国防科技大学的胡守仁教授。</p><p></p><p>现在来看，这场博士论文答辩会的专家阵容可谓相当豪华。张钹教授于 1995 年被评为中国科学院院士，黄昌宁教授也是中国最早从事自然语言处理的科学家之一，胡守仁教授曾经领导了中国第一台军用专用电子管计算机 901(331) 机的研制。</p><p></p><p>周明博士是李生教授指导的第一位博士，也是李生教授培养的第一位中国自然语言处理科学家。周明博士毕业后到清华大学从事博士后研究工作，后任清华大学副研究员。1999 年，他加入微软亚洲（中国）研究院，负责自然语言处理研究组，他在微软工作二十多年，直至微软亚洲研究院副院长。大模型时代，周明博士离开微软，创立了知名人工智能公司澜舟科技。</p><p></p><p>1992 年，李生教授被评为计算机专业博士研究生导师。</p><p></p><p>1993 年，李生教授获“国务院政府特殊津贴”。</p><p></p><p>同年，李生教授团队与清华大学、原航天工业总公司共同开发达雅翻译工作站。当时周明博士已经加入清华大学，并代表清华大学一方负责这个项目的工作。</p><p></p><p>达雅翻译工作站可以用于汉英和英汉的辅助机器翻译和辅助写作。这个项目成功的实现了翻译软件的商品化。项目连续三年在北京计算机产品交易会上展出。1997 年，该项目获得部级二等奖。</p><p></p><p>1994 年起，在国家 863 高技术研究发展计划的支持下，李生教授的团队开展了汉英 - 英汉双向机器翻译研究。先后开发了 BT863-I 和 BT863-II 双向机器翻译系统，实现了基于规则和基于统计的翻译方法混合的机器翻译策略。</p><p></p><p>基于规则的翻译方法，核心思想是根据语言的句法、语义等特征，写出翻译过程中的相关规则。基于统计的翻译方法，核心思想是利用统计和数学方法，计算出翻译过程中的相关策略和选择。</p><p></p><p></p><h2>与微软合作</h2><p></p><p></p><h3>中国互联网”黄埔军校“的缘起</h3><p></p><p></p><p>1996 年，李生教授省获“黑龙江优秀中青年专家”称号。</p><p></p><p>1998 年，李生任哈尔滨工业大学党委书记。</p><p></p><p>这一年年末，微软在北京成立微软中国研究院。这是微软公司在美国境外开设的第二家基础科研机构，也是微软在美国境外规模最大的研究机构。</p><p></p><p>研究院刚成立的时候，在国内还没有那么高的认可度。虽然北京的高校很多，但是研究院并没有开展很多校企合作。</p><p></p><p>这个时候，周明博士已经在微软中国研究院工作。研究院希望在哈工大成立学生俱乐部，负责高校合作的马歆找到周明，希望他跟李生教授沟通，讨论研究院跟哈工大的合作事宜。</p><p></p><p>周明随即联系了导师李生教授，时任学校党委书记的李生同意与微软中国研究院开展合作。李生教授协调了学校的团委、学生会等相关部门进行对接。1999 年，微软中国研究院到哈工大举办宣讲活动，微软 - 哈工大学生俱乐部成立。与此同时，微软中国研究院与中国高校的合作也拉开帷幕。</p><p></p><p>2000 年，微软 - 哈工大机器翻译联合实验室成立，李生教授任实验室主任。研究院也先后委派黄昌宁教授、周明博士和马维英博士担任实验室微软方面的联合主任。</p><p></p><p>2001 年，微软中国研究院更名为微软亚洲研究院。</p><p></p><p>之后，微软亚洲研究院与中国很多其他顶尖高校也展开了学生培养和科学研究方面的合作，为中国互联网和人工智能行业培养了大量的优秀人才，被誉为中国互联网的“黄埔军校”。这些都开始于李生教授当初对待校企合作开放、包容的态度。</p><p></p><p></p><h2>回到一线科研岗位</h2><p></p><p></p><h3>获得计算语言学领域国际最高荣誉</h3><p></p><p>2002 年，李生教授荣获“全国五一劳动奖章”。</p><p></p><p>2004 年，李生教授卸任学校党委书记，回到计算机学院继续从事教学科研工作。</p><p></p><p>同年开始，连续两届担任国家自然科学基金信息科学部专家评审组成员。</p><p></p><p>也是在这一年，微软 - 哈工大机器翻译联合实验室升级为“哈工大语言语音教育部 - 微软重点实验室”，李生教授继续担任重点实验室主任。</p><p></p><p>2008 年，李生主持国家自然科学基金重点项目“下一代信息检索”。</p><p></p><p>2011 年，李生担任中国文信息学会理事长。</p><p></p><p>2012 年，李生教授参加了由百度公司牵头的国家 863 重大项目“互联网语言翻译系统研制”。这个项目在 2015 年获得国家科技进步二等奖。</p><p></p><p>2015 年，李生教授获国际计算语言学学会（Association for Computational Linguistics,ACL）终身成就奖。这个奖项是国际计算语言学领域的最高荣誉。他也是这个奖项开设以来的首位华人获得者。曾经的获奖者包括 IDF(逆文档频率) 的提出者 Karen Spärck Jones 等学界泰斗。</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/27e5add1111e83ff4e776e15f3f9d3e6.webp" /></p><p></p><p>同一年，李生教授获中国中文信息学会终身成就奖。</p><p></p><p>2016 年起，李生教授担任中国中文信息学会名誉理事长。</p><p></p><p>2023 年，李生教授获中国计算机学会自然语言处理专业委员会"杰出成就学者"奖。</p><p></p><p></p><h2>桃李天下</h2><p></p><p></p><h3>为业界培养多位科学家的科学家</h3><p></p><p>2015 年年末，哈尔滨工业大学机器智能与翻译研究室和社会计算与信息检索研究中心共同为李生教授举办执教 50 周年纪念活动。活动发布了一本纪念册，名为《春华秋实，桃李天下》。</p><p></p><p>五十多年来，李生教授培养了近 50 名博士生，百余名硕士生。</p><p></p><p>光是早年开发 CEMT 三个机器翻译系统的过程中就培养了中国自然语言处理领域的数位科学家。</p><p></p><p>其中包括澜舟科技的周明博士、哈尔滨工业大学的赵铁军教授和张民教授、百度 CTO 王海峰博士等。哈尔滨工业大学的刘挺教授也自从博士毕业后就开始加入李生教授团队从事研究工作。</p><p></p><p>其中周明和王海峰曾担任国际计算语言学会主席，他们二位也都曾当选国际计算语言学会会士（ACL Fellow）。</p><p></p><p>在学术界，李生教授还培养了北京语言大学的荀恩东教授、哈尔滨工业大学的杨沐昀教授、秦兵教授和车万翔教授等多位知名学者。</p><p></p><p>在工业界，他的弟子包括联想控股副总裁于浩博士、百度公司副总裁赵世奇博士等。</p><p></p><p></p><h2>后记</h2><p></p><p>在北京见到李生教授时，他对我说：“中国的人工智能是从机器翻译开始的，虽然早期的时候机器翻译还没有提升到人工智能的高度，不过这也是自然语言处理学者的一个骄傲。”</p><p></p><p>从学生的培养，到微软亚洲研究院在国内的首次合作，听李老师讲述几十年来的往事。其中的很多细节和李老师务实的精神让我特别感动。</p><p></p><p>一个多小时的交谈过程中，我能感受到李生老师早年科研过程中条件的艰苦，以及面对方向选择时的谨慎。五十多年来，从零开始到若干机器翻译系统的研制成功，李老师与他的学生们为中国的机器翻译研究做出了开拓性的贡献。</p><p></p><p>ACL 终身成就奖的获得，不仅是国际学术界对李生教授长期贡献的认可，也是对中国自然语言处理学术研究的认可。</p><p></p><p>个人电脑时代、互联网时代、大模型时代……时间滚滚向前，新的时代接踵而至，人工智能在人们生活中的比重越来越大，自然语言处理方向的研究价值也越来越高。</p><p></p><p>中国的自然语言处理研究正是源于几十年前李生教授这一批脚踏实地的学者。他们的工作为中国的自然语言处理事业打下坚实的基础。他们培养的学生是中国这一领域雄厚的人才资本。这些夯实的积累和持续传承的精神足以让我们对未来中国相关领域的发展充满信心。</p><p></p><p></p><h2>作者简介：</h2><p></p><p></p><p>秦海龙，香港科技大学社会科学部博士后研究员，中国中文信息学会社会媒体处理专业委会委员。主要研究方向为中国人工智能发展史、中国人工智能科学家口述史、计算社会学。博士毕业于哈尔滨工业大学社会计算与信息检索研究中心，前自然语言处理研发工程师，曾就职于小米科技和三角兽科技。&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</id>
            <title>智谱AI获中东财团 4 亿美元投资；老板跑路，900多员工一脸懵：上午改bug、下午解散；谷歌在云部门大规模裁员 | AI周报</title>
            <link>https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:34:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, 谷歌, 马斯克, OpenAI
<br>
<br>
总结: 字节跳动否认TikTok曾提议将控制权交给美国政府，谷歌在云部门裁员，马斯克和杨立昆争论，奥特曼或将OpenAI重组为盈利性公司，苹果Siri升级计划延迟至2025年。 </div>
                        <hr>
                    
                    <p></p><blockquote>字节跳动：外媒报道“TikTok曾提议将控制权交给美国政府”消息不实；谷歌在云部门大规模裁员；马斯克和杨立昆的激情互喷，引来3000万网友围观；奥特曼或将OpenAI重组为盈利性公司；iOS&nbsp;18首个正式版无缘：曝苹果AI&nbsp;Siri跳票至2025年；内卷加剧！拼多多上线自动跟价功能……</blockquote><p></p><p></p><h3>热门资讯</h3><p></p><p></p><h4>字节跳动：外媒报道“TikTok曾提议将控制权交给美国政府”消息不实</h4><p></p><p>&nbsp;</p><p>近日有媒体报道TikTok曾提议将控制权交给美国，字节否认。此外字节AI硬件方向的探索，在内部分为两条产品线“D线”和“O线”，负责人分别为李浩乾和字节曾收购公司Oladance创始人，向字节跳动技术副总裁洪定坤汇报。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec3ce51a40aef0229447f544dcea9f63.webp" /></p><p></p><p>字节跳动豆包业务相关负责人表示，这些组织及汇报关系的猜测不实。该项目作为字节豆包业务的一部分，目的是探索豆包能力与可穿戴设备相结合，为用户提供更自然和便捷的交互体验。同时，我们也会把豆包能力开放给更多硬件厂商，希望能够为硬件厂商和豆包大模型/豆包AI助手的合作提供设计参考。</p><p>&nbsp;</p><p>另外，还有消息称，TikTok正在为其1.7亿美国用户开发其推荐算法的克隆版本，可能会产生一个独立于其中国母公司字节跳动运行的版本，更容易被想要封禁它的美国议员接受。在报道发表后，TikTok在𝕏上发文称，“路透社今天发表的报道具有误导性，与事实不符”。</p><p></p><h4>谷歌在云部门大规模裁员</h4><p></p><p>&nbsp;</p><p>据外媒报道，一份内部文件显示，谷歌公司在旗下云部门进行大规模裁员，涉及咨询顾问、合伙人工程师、可持续性等团队的员工。有员工估算，谷歌云亚太区的“走向市场”（Go&nbsp;To&nbsp;Market）部门约有100人被裁。</p><p>&nbsp;</p><p>前述文件显示，专注于咨询、合作伙伴工程和可持续性的云团队也受到裁员的影响。一些最近新招的员工也解雇，包括至少一名还未完成入职流程的员工。部分受影响的员工已经得到谷歌内部其他岗位的工作机会。</p><p></p><h4>马斯克和杨立昆的激情互喷，引来3000万网友围观</h4><p></p><p>&nbsp;</p><p>5月27日，马斯克的&nbsp;xAI&nbsp;刚宣布了今年最大的一笔风险投资，以超过&nbsp;240&nbsp;亿美元估值获得&nbsp;60&nbsp;亿美元&nbsp;B&nbsp;轮融资。自此，成功晋级为“硅谷大模型第一梯队玩家”，与&nbsp;OpenAI、谷歌、Anthropic&nbsp;和&nbsp;xAI&nbsp;并列为四巨头。</p><p>而马斯克在社交媒体X上发布招聘信息后，遭到Meta首席人工智能科学家杨立昆嘲讽，大战一触即发，引来3000万网友围观。两人真正的分歧在于：杨立昆认为AI毁灭人类的概率为零，而马斯克宣扬的是AI威胁论。</p><p>杨立昆嘲讽道：“如果你能忍受这样一位老板，那就加入xAI。这位老板声称，你正在做的事情将在明年由AI替代解决（没有压力）；声称你正在做的事情会毁灭人类，必须停止或暂停（耶，放六个月的假！）；声称想要对真理进行最严格的追求，但却在自己的社交平台上散布疯狂的阴谋论。”</p><p>&nbsp;</p><p>另外在近日巴黎举行的初创企业年度技术大会&nbsp;VivaTech&nbsp;上，杨立昆建议希望在&nbsp;AI&nbsp;生态系统中工作的学生不要从事&nbsp;LLM（大型语言模型或称“LLM”）方面的工作。“如果你是对构建下一代&nbsp;AI&nbsp;系统感兴趣的学生，请不要从事&nbsp;LLM&nbsp;方面的工作。这是大公司的事情，你们无法对此有所贡献。”他还说，人们应该开发能够克服大型语言模型局限性的下一代&nbsp;AI&nbsp;系统。</p><p>&nbsp;</p><p></p><h4>奥特曼或将OpenAI重组为盈利性公司</h4><p></p><p>&nbsp;</p><p>据知情人士称，OpenAI&nbsp;CEO山姆·奥特曼（Sam&nbsp;Altman）正考虑将OpenAI重组为一家正规的盈利性公司。据一位与奥特曼交谈过的知情人士称，一种可能性是，奥特曼可能会将OpenAI转变为一家营利性公司，他也讨论过这个想法。而OpenAI的一些投资者表示，他们希望奥特曼获得股权方案，以确保他的利益与公司业务保持一致。</p><p>&nbsp;</p><p>本周，OpenAI&nbsp;的两位前董事会成员Helen&nbsp;Toner&nbsp;和&nbsp;Tasha&nbsp;McCauley&nbsp;公开指责&nbsp;Sam&nbsp;Altman，认为其在&nbsp;OpenAI&nbsp;内部培养了一种虚假文化。</p><p>&nbsp;</p><p>两位前董事指出&nbsp;Altman&nbsp;削弱了董事会对关键决策和安全规定的监督，许多高管告诉董事会，他们非常担心&nbsp;Altman&nbsp;正在创造&nbsp;一种虚假文化，并参与&nbsp;可以被描述为心理虐待的行为。Toner&nbsp;和&nbsp;McCauley&nbsp;认为，OpenAI&nbsp;无法自我监管，这一观点源自非营利性和营利性结构的混合。</p><p>&nbsp;</p><p></p><h4>iOS&nbsp;18首个正式版无缘：曝苹果AI&nbsp;Siri跳票至2025年</h4><p></p><p>&nbsp;</p><p>据报道，苹果正对Siri进行重大升级，使其获得大模型能力，更准确理解用户意图，提供更自然、智能的交互体验。不过，这一高级Siri功能将不会在iOS&nbsp;18首个正式版中出现，预计要等到2025年的iOS&nbsp;18才会集成。</p><p>&nbsp;</p><p>目前，Siri的智能化程度有限，苹果计划借助类似ChatGPT的自然语言模型实现全新智能助理，让用户能与Siri进行多轮对话。苹果工程师团队正在重构Siri的底层，以提升其响应生成能力，深度理解用户需求，并智能整合相关信息，让对话和工作更加自然、高效。</p><p>&nbsp;</p><p>据外媒报道，苹果早在2023年年中就已接触OpenAI公司，而微软对两家公司之间的这笔交易感到担忧。据悉，苹果iOS等系统接入OpenAI的ChatGPT聊天服务之外，还邀请OpenAI团队成员帮助优化Siri，不断提高其“智力”。</p><p>&nbsp;</p><p></p><h4>内卷加剧！拼多多上线自动跟价功能</h4><p></p><p>&nbsp;</p><p>5月29日，拼多多正式上线了自动化价格追踪系统。此举意味着若竞争对手下调商品价格，而用户的商品价格尚显高价，平台将智能调整用户定价，使之与竞争对手保持同步甚至更具竞争优势。</p><p>值得注意的是，抖音电商此前刚刚宣布小规模内测“自动改价”功能，该功能支持商家委托平台根据市场情况，参考平台的建议价，在商家设定的条件范围内实现改价，为消费者提供更有价格竞争力的商品，提升自身经营效率。据抖音电商内部同学透露，今年货架电商的营业额已经很接近拼多多。</p><p>&nbsp;</p><p></p><h4>小红书大举替换中高层高管，后台出现诸多bug</h4><p></p><p>&nbsp;</p><p>据报道，2023年，小红书在一场共创会议中明确了“坐一观三”的发展策略，向3亿DAU正式进发。为了拉平差距，小红书今年一季度在行业大举挖人，有大量中层以上管理者进入小红书，承担起小红书商业化产品、社区运营、电商产品、法务等重要岗位。</p><p>与此同时，多元的组织文化冲击了小红书的原有团队。内部互相推诿的情况变多，“各部门甩锅问题严重”，事实上，不止一位采访对象提及过，小红书的社区部门与商业化部门存在矛盾，而社区一直处于强势地位。新进入者的试错，也让小红书的后台出现诸多bug，引发了商家们的投诉潮。人员的频繁流动，让小红书的功能产品连续性较差，导致用户的体验差。</p><p>&nbsp;</p><p></p><h4>京东健康旗下家医事业部被整体裁撤，刘强东“训话”后京东宣布涨薪</h4><p></p><p>&nbsp;</p><p>5月27日，京东集团宣布，自2024年7月1日起，通过一年半时间，京东采销年度固定薪酬由16薪提升至20薪，业绩激励上不封顶。据了解，这是京东半年内第二次给一线采销人员涨薪。2023年12月底，京东集团宣布，2024年1月1日起京东采销等一线业务人员的年固定薪酬大幅上涨近100%，2024年初京东零售全员将平均加薪不低于20%。</p><p>&nbsp;</p><p>5月28日消息，网传京东健康旗下京东家医事业部被整体裁撤。有京东健康内部人士表示，京东家医事业部投入较大，但一直没有找到盈利模式，也没有其它价值，因此被放弃。对此，京东健康回应称，为整合公司旗下医疗健康服务业务，更高效利用医疗健康服务资源、更大化发挥专业角色的服务价值，京东健康针对家庭医生事业部进行了内部组织架构调整，将原家庭医生事业部的&nbsp;C&nbsp;端业务及职能，合并至互联网医疗事业部；B&nbsp;端业务及职能，合并至企业业务事业部。此次调整除架构分拆外，不涉及业务和人员裁撤。</p><p>&nbsp;</p><p>有媒体报道称，针对近期员工代打卡已形成产业链的问题，京东内部调查，每个月有1.4万人次找人代打卡，更有员工一年里代打卡近百天。代打卡一次收取15元，一个人可替20个人代打卡。</p><p>&nbsp;</p><p>在此背景下，刘强东在日前的高管会上直言：“凡是长期业绩不好，从来不拼搏的人，不是我的兄弟。”与此同时，京东管理层反思了过去数年间公司在管理上出现的失误，承认京东集团体系存在“大企业病”，管理上出现了严重问题，尤其在拼搏上有很大的懈怠。</p><p>&nbsp;</p><p>另外，磨铁集团&nbsp;CEO&nbsp;<a href="https://readhub.cn/entity_topics?type=10&amp;uid=bd975c885b3c1cf8">沈浩波</a>"还在朋友圈发文抵制<a href="https://readhub.cn/entity_topics?type=10&amp;uid=bf4792ab9e1c7531">京东</a>"，称京东不顾磨铁反对，强行让其产品参加低价促销。磨铁将采取法律手段维护权益，无限期停止对京东发货，要求京东下架其产品并退还。呼吁电商合作伙伴不要跟价。京东方面则回应称，其&nbsp;618&nbsp;活动目标是让消费者买到便宜的好书，否认站在出版社和行业对立面。京东对自营图书有合法的自主定价权。目前磨铁的网上店铺仍在正常销售，但部分热门图书显示无货。</p><p>&nbsp;</p><p></p><h4>阿里蔡崇信：我会尽量让&nbsp;CEO&nbsp;们和运营团队拥有更多的自主权和决策权</h4><p></p><p>&nbsp;</p><p>阿里巴巴集团董事长蔡崇信在与摩根大通北亚区董事长兼大中华区投资银行业务副主席&nbsp;Kam&nbsp;Shing&nbsp;Kwang&nbsp;的对话中强调，尽管面临监管、竞争压力和地缘政治等挑战，阿里巴巴仍聚焦于增长。公司致力于技术创新，并设定了未来&nbsp;10&nbsp;年的增长目标，旨在恢复营收增长，计划在&nbsp;2027&nbsp;财年实现两位数的增长。蔡崇信还表示，管理层会确保资源有效利用，并赋予&nbsp;CEO&nbsp;和运营团队更多自主权，以便根据实际情况作出合理决策。</p><p>&nbsp;</p><p></p><h4>900多个员工一脸懵逼：上午还在改bug、下午集团就解散了</h4><p></p><p>&nbsp;</p><p>5月28日消息，据多方爆料，上海极目银河公司老板跑路，因在国外玩金融亏损62亿，近千名员工两个月没发工资，公司宣布破产。极目银河员工称，老板跑了，公司原地解散，将近900多人的员工直接失业。上午还在改着bug，下午集团就解散了，还有几位新入职的同事，据说亏了62亿直接跑了。</p><p>&nbsp;</p><p>一位员工表示，2024年4月9日，公司发布全员通知称，原定于4月10日发放3月份工资，延迟到15日发放，但15日仍未发放。5月15日，公司口头通知4月份工资要到5月24日发放，24日口头又通知，工资5月27日下午3点发放，但到27日下午1点，得知老板跑路，公司破产。</p><p>&nbsp;</p><p>根据报案回执信息显示，老板陈群于2024年5月24日9时开始失联，去向不明。去他家里发现人去楼空，留下字条称，“无法对付投资人62亿，只能选择逃避。”</p><p>&nbsp;</p><p>此前有爆料称，这公司实际上就是几个小公司凑起来的，看起来什么业务都做，实际上没一个精通。注册资本7000万，实缴不敢公示，社保缴纳也不公开。各位求职者擦亮眼睛！</p><p></p><h4>智谱AI获中东财团&nbsp;4&nbsp;亿美元投资</h4><p></p><p>&nbsp;</p><p>据英国《金融时报》5&nbsp;月&nbsp;31&nbsp;日援引两位知情人士的消息报道，中东石油巨头沙特阿美旗下风险投资部门管理的&nbsp;Prosperity7&nbsp;基金参与了对中国人工智能初创企业智谱&nbsp;AI&nbsp;约&nbsp;4&nbsp;亿美元的一轮投资。</p><p>&nbsp;</p><p>《金融时报》表示：预计这笔投资将使智谱&nbsp;AI&nbsp;的估值达到约为&nbsp;30&nbsp;亿美元，此次投资也使得&nbsp;Prosperity7&nbsp;基金成为了首个对中国生成式人工智能初创企业投下重金的外国投资者。这笔投资表明，沙特愿意支持中国的人工智能生态系统。据一位知情人士表示，“沙特并不希望硅谷主导这个（人工智能）行业。”</p><p>&nbsp;</p><p></p><h3>IT&nbsp;业界</h3><p></p><p></p><h4>2500页文档曝谷歌搜索黑幕：用户数据被挪用、品牌主宰搜索排名</h4><p></p><p>&nbsp;</p><p>2500页谷歌搜索API泄露文档显示，谷歌使用点击数据、Chrome浏览器数据、白名单机制、质量评估员反馈等因素影响搜索排名，与官方公开表态矛盾。文档由资深SEO从业者Erfan&nbsp;Azimi泄露，旨在提高谷歌排名机制透明度。泄露内容可能对SEO领域产生重大影响，改变中小企业SEO策略。</p><p></p><h4>GPT-4o&nbsp;新功能已免费开放</h4><p></p><p>&nbsp;</p><p>5&nbsp;月&nbsp;30&nbsp;日，ChatGPT&nbsp;免费用户现已能够尝试自定义&nbsp;GPT&nbsp;模型、分析图表等其他&nbsp;GPT-4o&nbsp;新功能。当然，OpenAI&nbsp;在推出&nbsp;GPT-4o&nbsp;就承诺它将免费向所有用户开放，而付费用户的优势在于“拥有免费用户五倍的容量限制”。</p><p>&nbsp;</p><p>在此以前，诸如模型和网络响应、数据分析、图表创建、视觉、文件上传、内存和自定义&nbsp;GPT&nbsp;模型等功能仅对付费用户（ChatGPT&nbsp;Plus、Teams&nbsp;和&nbsp;Enterprise）开放，但现在已经面向所有&nbsp;ChatGPT&nbsp;用户开放（当免费用户使用&nbsp;GPT-4o&nbsp;达到消息或对话限制时将自动恢复到&nbsp;GPT-3.5）。</p><p>&nbsp;</p><p></p><h4>百度被曝或将在明年发布文心大模型5.0</h4><p></p><p>&nbsp;</p><p>据报道，百度或将于2025年百度世界大会期间发布新一代文心大模型5.0。目前，文心大模型最新版本为4.0版本，该版本于2023年举办的百度世界大会上由百度创始人李彦宏发布，具备理解、生成、逻辑和记忆四大核心能力。</p><p>&nbsp;</p><p>据李彦宏2023年10月发布时介绍，文心大模型4.0发布后相比GPT-4便已毫不逊色。据了解，历年百度世界大会举办时间均在每年下半年，按此推算，文心大模型5.0或将在2025年下半年与大家见面。公开信息显示，百度文心大模型于2019年首发，2021年发布3.0版本，2023年升级至4.0版本。</p><p>&nbsp;</p><p>此外，日前，百度集团执行副总裁沈抖向外界解释了文心系列主力大模型免费背后的原因，希望友商们别再天天拉表格比价格了，用更多的时间去卷场景、卷应用，把相关的场景都试一遍，快速验证。跑成功了，就快速复制。</p><p>&nbsp;</p><p></p><h4>宝马工厂机器狗上岗，可“嗅探”故障隐患</h4><p></p><p>&nbsp;</p><p>近日，宝马英国汉姆霍尔发动机工厂引进了一只名为&nbsp;SpOTTO&nbsp;的机器搜索犬，用于帮助检测生产线问题。这一举措引发人们对《华氏&nbsp;451》中机械猎犬的联想，然而&nbsp;SpOTTO&nbsp;并未具备小说中的暴力功能，其制造方为波士顿动力公司。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/26/26592eb27bee28dd76e504065e967dc1.jpeg" /></p><p></p><p>SpOTTO&nbsp;这个名称来源于两位奥托（Otto）——其中古斯塔夫·奥托是宝马的联合创始人之一，而尼古拉斯·奥托则是四冲程内燃机的发明者。该机器将作为汉姆霍尔工厂的一员，在此工厂每年生产&nbsp;40&nbsp;万台宝马和&nbsp;MINI&nbsp;的&nbsp;TwinPower&nbsp;涡轮增压发动机期间发挥重要作用。SpOTTO&nbsp;通过视觉、热成像和声音传感器来发现过热的设备和压缩空气泄漏等问题。这些问题如果不及时处理可能导致工厂停工或效率低下。</p><p>&nbsp;</p><p>除了发现潜在问题外，SpOTTO&nbsp;还可以爬楼梯，并为宝马的汉姆霍尔工厂的数字孪生体收集数据。未来，它还有可能承担更多任务，如读取模拟控制装置数据。这一举措旨在确保生产线的顺利进行，并进一步提高生产效率。</p><p></p><h4>腾讯、百度相继发布&nbsp;AI&nbsp;应用</h4><p></p><p>&nbsp;</p><p>本周，腾讯发布了一款名为&nbsp;“腾讯元宝”&nbsp;的&nbsp;AI&nbsp;助手，基于混元大模型，可在多个应用商店下载。该助手具备&nbsp;AI&nbsp;搜索、总结、写作等能力，能解析多种文档格式，并支持长上下文窗口。此外，腾讯还提供了包括&nbsp;AI&nbsp;头像、口语陪练和超能翻译等在内的有趣实用&nbsp;AI&nbsp;应用。混元文生图大模型也已升级并对外开源。</p><p>&nbsp;</p><p>同时，在&nbsp;2024&nbsp;百度移动生态万象大会上，百度副总裁王颖推出全新&nbsp;AI&nbsp;原生应用&nbsp;「橙篇」，该应用可实现文件处理、创作等功能，依托百度文库&nbsp;12&nbsp;亿内容积累和多项&nbsp;AI&nbsp;技术。此外，百度文库经过大模型重构后，已吸引&nbsp;1.4&nbsp;亿&nbsp;AI&nbsp;用户，AI&nbsp;功能使用次数突破&nbsp;15&nbsp;亿次，显示了百度在&nbsp;AI&nbsp;内容获取与创作领域的领先地位。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</id>
            <title>Meta 首席科学家：不要从事 LLM 工作；代打卡成产业链，刘强东怒了：业绩不好不拼搏全部淘汰；上千页文档揭谷歌搜索黑幕 | Q资讯</title>
            <link>https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:25:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, FSD, Meta AI, DNF手游
<br>
<br>
总结: TikTok正在回应美国版本核心算法开发的传闻；特斯拉即将在中国推出FSD全自动驾驶系统；Meta AI首席执行官建议不从事LLM工作；腾讯旗下DNF手游首周狂揽10亿元。 </div>
                        <hr>
                    
                    <p></p><blockquote>TikTok 回应“正开发核心算法的美国版本”；传特斯拉的 FSD 即将在中国落地；腾讯《DNF》手游首周狂揽 10 亿元；米哈游再次起诉哔哩哔哩侵权；GPT-4o 向所有人免费开放！字节再试 AI 硬件；中国首例消费者起诉苹果垄断案一审宣判；OpenAI 组建新的安全团队；知乎 PC 网页端非登录用户无法查看回答全文；联想集团裁员 7500 人；谷歌搜索算法内幕被扒；Meta 为社交媒体数据工具 CrowdTangle 增添安全功能；XZ 5.6.2 释出，移除后门代码……&nbsp;&nbsp;</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>TikTok回应“正开发核心算法的美国版本”</h4><p></p><p>当地时间5月30日，有消息称，字节跳动旗下TikTok正开发核心算法的“美国版本”。一旦代码分开，有望为字节跳动美国资产的剥离奠定基础，但该公司目前并无此计划。</p><p></p><p>TikTok对此说法在社交媒体平台X发文回应称：相关报道具有误导性且与事实不符。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f1d61707cc872775aa48e0ba9da46d3.png" /></p><p></p><p>TikTok表示，TikTok剥离法案要求的TikTok继续在美运营的“合格剥离”动作，在商业、技术和法律上都是不可能的，并且“肯定不可能在该法案要求的270天时限内完成”。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651207433&amp;idx=1&amp;sn=b0d3776443b6844e19dcc2e371d790cf&amp;chksm=bdbbcf5a8acc464c548ee33611d68afb58a21d5407fae1d87f4c88b11ed8ccd699a1815d33fb&amp;token=927577701&amp;lang=zh_CN#rd">剥离几百万行代码，复制核心算法去美国？TikTok&nbsp;最新回应来了</a>"》</p><p></p><h4>传特斯拉的&nbsp;FSD&nbsp;即将在中国落地</h4><p></p><p>5月31日消息，据外媒报道，有知情人士表示，特斯拉已成功获得中国工业和信息化部的软件注册，这将为特斯拉内部测试全自动驾驶(FSD)铺平道路，特斯拉员工将在中国公共道路上测试，并计划在未来几个月内升级推送给中国用户。然而，在中国电动汽车制造商竞争激烈的背景下，2024年一至四月期间，由于市场环境不景气等原因导致了特斯拉在华销量下滑7.6%。</p><p></p><p>另外值得一提的是，《关于汽车数据处理4项安全要求检测情况的通报》（第一批）于4月28日发布，其中提到特斯拉上海超级工厂生产的车型全部符合合规要求，成为唯一一家符合此要求的外资企业。这些措施表明特斯拉正积极遵守中国相关法规并努力提升其在华业务的发展前景。</p><p></p><p>针对此事，特斯拉客服表示，目前内部员工没有进行相关的测试。不同城市不太一样，即使未来能够落地或者开放，也是在允许完全自动驾驶能力测试的城市开放。"我们确实在筹备这件事，但具体什么时间落地，也需要一个长久的时间，目前官方没有任何相关消息。用户可以关注官方公众号和官方微博，如果有最新的进展或者待发布的时间我们会第一时间通知给所有的车主。</p><p></p><h4>Meta&nbsp;AI&nbsp;首席执行官&nbsp;LeCun：不要从事LLM工作</h4><p></p><p>在近日巴黎举行的初创企业年度技术大会VivaTech上，有着“人工智能业鲁迅”之称的Meta&nbsp;AI的首席执行官Yann&nbsp;LeCun建议希望在AI生态系统中工作的学生不要从事LLM（大型语言模型或称“法学硕士”）方面的工作。</p><p></p><p>“如果你是对构建下一代AI系统感兴趣的学生，请不要从事LLM方面的工作。这是大公司的事情，你们无法对此有所贡献，”LeCun在会议上表示。他还说，人们应该开发能够克服大型语言模型局限性的下一代AI系统。</p><p></p><p>有趣的是，关于LLM（大型语言模型）替代品的讨论已经持续了一段时间。最近，Devika的年轻创始人Mufeed&nbsp;VH（Devika是Devin的替代品）谈到了人们应该如何远离Transformer模型并开始构建新的架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3af342d4c19c169990c5185b58116da4.png" /></p><p>他还认为，通过这种方法，甚至有可能构建出与GPT-4一样令人印象深刻的东西。</p><p></p><p>尽管LeCun反对一股脑全部研究LLM，但Transformer训练模型仍在不断发展。AI/ML顾问Dan&nbsp;Hou谈到了GPT-4o，并强调了其训练模型。</p><p></p><h4>腾讯《DNF》手游首周狂揽10亿元，App&nbsp;Store&nbsp;下载超260万次</h4><p></p><p>根据彭博社报道，腾讯上周刚刚上线的爆款手游《地下城与勇士：起源》首周狂揽1.4亿美元（约合人民币10亿元），一跃成为近期腾讯最赚钱的手游产品。在iOS国区畅销榜上，《DNF》手游持续霸榜9天，这个记录有望进一步保持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac1754779d6d07d1594b5bd9d85fad32.png" /></p><p></p><p>此外，根据Sensor&nbsp;Tower&nbsp;的数据显示，《DNF》手游在App&nbsp;Store商店共吸金6300万美元，下载次数为260万次，值得一提的事，这个数据不包括中国安卓一级第三方的下载和收入。</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12872f735d0f71e0ce6411d7592c0b61.jpeg" /></p><p></p><p>彭博社补充道，《DNF》手游已经成为上周App&nbsp;Store上最赚钱的游戏，紧随其后的事《王者荣耀》和《和平精英》。Niko&nbsp;Partners的分析师则认为，《DNF》手游可能在今年为腾讯带来超过10亿美元的收入，但是否能成为下一个《王者荣耀》，还有待长期去观察。</p><p></p><h4>即刻开庭！米哈游再次起诉哔哩哔哩侵权</h4><p></p><p>近日，上海米哈游天命科技有限公司与B站关联公司上海宽娱数码科技有限公司、上海幻电信息科技有限公司相关著作权权属、侵权纠纷案件新增开庭公告，原告为上海米哈游天命科技有限公司，该案于5月31日在上海市浦东区人民法院开庭审理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/341622afe6864e3657f9bf8dbba3cde8.png" /></p><p></p><p>此次诉讼由上海米哈游天命科技有限公司作为原告提起，再次将哔哩哔哩推上了风口浪尖。根据天眼查的法律诉讼信息，此次案件涉及的是著作权权属和侵权问题，追溯过往，米哈游与哔哩哔哩的拉锯战并非首次。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76358096cd87380f0026bfafb8599ba3.png" /></p><p></p><p>历史记录显示，米哈游早前已因类似侵权争议对上海宽娱数码科技有限公司提起诉讼，尽管最终以原告主动撤诉收场，但双方恩怨显然未了。</p><p></p><h4>终于，GPT-4o&nbsp;向所有人免费开放！</h4><p></p><p>5月30日凌晨，OpenAI在x平台宣布，GPT-4o多模态能力向所有用户免费开放。不过，在使用次数上官方没有更新公告，预计还是会是有一定次数限制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5599021da5ddc33fadabdbe4280c983d.jpeg" /></p><p></p><p>本次免费开放的功能包括：联网搜索、数据分析、视觉分析（照片）、文件上传、GPTs商店和长期记忆。</p><p></p><p>使用网址：<a href="https://www.chatgpt.com/">www.chatgpt.com</a>"</p><p></p><p>北京时间5月14日凌晨，OpenAI举办发布会，带来了GPT-4o。该模型打通了文本、图片、视频和语音输入，无需中间转换，互相之间就可以直接生成。此次发布会还带来了更智能的实时语音助手，以及适用于macOS的ChatGPT桌面应用程序。</p><p></p><p>当时，OpenAI宣布向ChatGPTPlus和Team用户推出GPT-4o，很快就会向企业用户推出。同时，也向所有人开放GPT-4o多模态能力，但有使用次数限制。</p><p></p><h4>文心一言再换帅！原快手副总裁薛苏加入百度</h4><p></p><p>5月30日，百度举办的百度移动生态万象大会中，新任百度副总裁、AI产品创新业务负责人，New&nbsp;App负责人薛苏正式登场。</p><p></p><p>薛苏，原快手副总裁，2020年加入快手，先后负责短剧与二次元业务、体育业务。2023年底，快手进行大规模组织架构调整，薛苏不再担任运营部二次元内容中心的负责人，同时也不再兼任运营部体育业务中心和医疗健康组的负责人。</p><p></p><p>薛苏毕业于清华大学自动化系，在耶鲁大学取得博士学位，研究方向为计算机视觉与图形学，曾在EA和Facebook工作。</p><p></p><p>百度NEW&nbsp;App是一个集成了百度“AI&nbsp;伙伴”&nbsp;和&nbsp;“AI&nbsp;BOT”&nbsp;等功能的内测产品，在2023年百度移动生态大会上，由百度集团资深副总裁、百度移动生态事业群组（MEG）总经理何俊杰正式发布。</p><p></p><h4>阿里巴巴近2亿元投资，AI+“首战”在教育行业打响</h4><p></p><p>5月29日消息，教育科技公司精准学完成了新一轮融资，投资方为阿里巴巴，投资金额近2亿元。精准学成立于2018年1月，基于AI推荐引擎技术的“精准学理念”，此前与包括上海昂立在内的国内数千家教育培训学校建立了合作，也是学而思、高思集团的战略合作伙伴。</p><p></p><p>从2023年开始，精准学基于生成式AI技术开始了一次产品重构，以阿里通义千问大模型为基础，自主研发了“心流知镜”大模型，基于此研发了自有的AI教育应用，搭载在硬件之上。</p><p></p><p>精准学旗下的首款原生代AI辅学机将于今年6月对外发布，该产品基于阿里云通义大模型及虚拟数字人技术打造。值得关注的是，这也是阿里巴巴在过去两年中，首次在AI应用领域出手。</p><p></p><h4>字节再试AI硬件，两条产品线共发力</h4><p></p><p>近期，彭博社报道字节以5000万美元（约合人民币3.62元），收购中国耳机制造商Oladance。彭博社援引知情人士称，此举是因为字节管理层看到了可穿戴设备成为AIGC服务平台的潜力。</p><p></p><p>从多位知情人士处获悉，字节AI硬件方向的探索，在内部分为两条产品线：</p><p></p><p>一条产品线代号为“D线”，负责人为李浩乾，其为OWS（Open&nbsp;Wearable&nbsp;Stereo，开放式可穿戴立体声耳机）耳机品牌Oladance创始人。不久前，字节收购了这一品牌。</p><p></p><p>目前，Oladance团队正在融合进字节体系，整体归在字节跳动移动OS中台业务部下，李浩乾向邹伟汇报——邹伟曾经为锤子手机的软件负责人，在锤子手机被收购后加入字节，曾经是字节Smartisan&nbsp;OS的负责人。</p><p></p><p>另一条产品线为O线，负责人也是字节曾收购公司的创始人，其向字节跳动技术副总裁洪定坤汇报。</p><p></p><h4>不满30%“苹果税”，中国首例消费者起诉苹果垄断案一审宣判</h4><p></p><p>5月29日，上海知识产权法院就中国首例消费者起诉“苹果”垄断案一审宣判，法院认定苹果公司在中国区软件市场显然具有市场支配地位，但没有滥用市场支配地位，驳回原告金某的诉请。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9bc8962eebb79206d92df7ffa8c8e7f5.png" /></p><p></p><p>因不满苹果对“应用内购买”收取的30%的佣金，苹果手机消费者金某（原告）于2021年1月11日将苹果公司和苹果电脑贸易（上海）有限公司告上法庭。根据裁判文书，金某核心诉求包括停止收取30%“苹果税”的不公平高价行为以及停止“应用内购买”强制使用苹果支付系统的搭售行为。</p><p></p><p></p><blockquote>注：所谓“苹果税”，是指苹果对 App Store 上所有应用的数字内容消费抽取 15%~30% 的佣金。每当苹果用户通过苹果手机应用商店付费下载 App 或在 App 内部购买数字商品 / 服务时，苹果公司会扣留交易金额的一部分作为“过路费”，再将剩下的转给相应的 App 开发者。如果不接受“苹果税”的提成安排，或者应用程序内置有隐藏的其他支付方式，苹果公司将径直下架该款应用程序。据了解，中国目前是全球范围内苹果公司对于 App 内交易抽成费率最高的国家，抽成费率为 30%。近年来，苹果公司已在欧盟、美国、日本、韩国等多地遭政府起诉或调查，欧盟地区“苹果税”从今年 3 月开始大幅下降。</blockquote><p></p><p></p><p>原告代理律师表示，尊重一审法院的判决，但将针对本案上诉至最高人民法院。</p><p></p><h4>“业绩不好不是我的兄弟”！京东“整顿职场”上热搜</h4><p></p><p>5月24日，有媒体报出刘强东“凡是长期业绩不好，从来不拼搏的人，不是我的兄弟”的发言，因为在2018年刘强东说永远不开除任何一个兄弟。让大众注意到，曾经把传统商业大鳄国美、苏宁拉下马，并跑赢淘宝天猫的京东，如今发展很不如意。</p><p></p><p>尤其是5月22日，拼多多以高达2042.74亿美元的市值再度超越阿里巴巴，4倍于京东，应该让京东高管团队更加坐立难安。这期间又爆出关于于京东内部考勤调整、午休时间缩短、人员优化等一系列事件引发舆论热议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/812a1b7e94f88fce19a2393b77e40213.jpeg" /></p><p></p><p></p><blockquote>一名京东员工表示：有员工一年里代打卡近百天，每天 4 点就下班，但是能领到全额薪水，还有一些实习生甚至两个月都不来，但通过代打卡，骗走了公司 1.5 万的工资。听说内部统计代打卡的人次每月近万次。”而京东午休时长达 2 个小时，很多员工下午 2 点半才开始工作，甚至可以熄灯睡觉。2 点半并不晚，但京东的上班时间很晚，弹性工作制的部门上午 10 点 30 才打卡，非弹性部门 9 点打卡，就这还有很多人花钱找人代打卡。然后上班 2 个小时不到，吃午饭了，然后再睡 2 个小时……</blockquote><p></p><p></p><h4>OpenAI&nbsp;前安全研究员&nbsp;Jan&nbsp;Leike&nbsp;转投&nbsp;Anthropic；OpenAI组建新的安全团队</h4><p></p><p>5月28日，OpenAI宣布，成立安全与审查委员会（SSC），负责针对所有的OpenAI项目制定至关重要的安全与审查决定。而值得注意的是，进入该安全顾问委员会的成员全部为该公司内部人员，包括OpenAI的CEO&nbsp;Sam&nbsp;Altman，外部人员一个也没有。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3be5b0e7150eacd87e30e5d6381bc76d.png" /></p><p></p><p>OpenAI还宣布，最近开始训练其下一个前沿模型，我们预计由此产生的系统将使我们在通往通用人工智能的道路上迈上一个新台阶。</p><p></p><p>毕竟在5月15日-17日，OpenAI超对齐团队两位负责人接连离职。其中包括OpenAI联合创始人、首席科学家伊利亚·苏茨克维尔（Ilya&nbsp;Sutskever），以及该团队的负责人简·雷克（Jan&nbsp;Leike）。</p><p></p><p>Jan&nbsp;Leike，OpenAI的前首席安全研究员，在5月15日宣布离职后，于本周二正式加入人工智能初创公司Anthropic。在OpenAI解散了他共同领导的超级对齐团队后，Leike表示，他将在Anthropic继续推动AI安全和监督的研究工作。Anthropic得到了亚马逊的大力支持，后者承诺投资高达40亿美元以换取少数股权。同时，OpenAI也在积极调整，新成立了一个由CEO领导的安全委员会，以确保公司项目和运营的安全。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/eced874ce3eddd98ed000070c5459230.jpeg" /></p><p></p><h4>昆仑万维：天工AI日活用户超过100万</h4><p></p><p>5月27日，昆仑万维集团宣布，天工AI每日活跃用户（DAU）已超过100万。</p><p></p><p>5月，全球大模型领域的竞争，分外激烈；谷歌、腾讯、阿里、字节跳动等业界巨擘纷纷亮出大招。有的凭借技术底蕴实现全面升级，有的慷慨将大模型开源免费，更有甚者通过价格战掀起狂风巨浪，整个市场一片火热。然而在这硝烟弥漫的战场上，昆仑万维作为中国AI的先驱者，近日正式公布天工AI日活用户（DAU）突破100万</p><p></p><p>根据QuestMobile的数据，2024年3月，天工平台的月活跃用户已达近千万，仅次于豆包与文心一言，成为国内活跃用户数第三的平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/937f29a2a1d7775ec4dfa77e8be9b4e6.png" /></p><p></p><p>据悉，昆仑万维集团在4月17日推出了天工3.0和天工SkyMusic，这两款产品的推出，为天工平台的增长注入了新的动力。天工3.0是一款拥有4000亿参数的开源大模型，其在多个领域实现了性能上的突破性提升。</p><p></p><h4>知乎PC网页端非登录用户无法查看回答全文</h4><p></p><p>5月27日，有网友反映称，知乎网页端近期出现了非登录用户无法查看全文的情况。测试显示，以知乎当前热榜第一的内容为例，非登录PC网页用户访问知乎的回答页面，如果点击“展开阅读全文”，平台会自动弹出登录窗口，确实无法直接查看全文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e9075f4abd2444caad05c6229803589.png" /></p><p></p><p>而非登录手机网页用户，则可以直接点击“展开阅读全文”查看全部信息，无需登录或下载App。对于这一情况，知乎官方暂未给出回应。</p><p></p><p>据了解，2022年3月初，工信部表示有网友和媒体反映部分网站在用户浏览页面信息时，强制要求下载App问题。工业和信息化部信息通信管理局对此高度重视，立即组织核查，并于日前召开行政指导会，督促相关互联网企业进行整改。</p><p></p><h4>联想集团裁员7500人，遣散费5500万美元！</h4><p></p><p>近日，联想集团发布了2023-2024年度全年财报。这份财报揭示了公司在过去一年中所面临的挑战与压力。财报显示，截至2024年3月31日的年度收入为619.47亿美元，同比下滑8%。与此同时，净利润也呈现下滑趋势，同比减少37%至10.1亿美元。这一业绩的下滑，无疑给联想集团带来了沉重的打击。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9181c3f3356a43f26b67ef57f40a142.png" /></p><p></p><p>财报披露，2023财年全球总员工77,000人，2024财年则是69,500人，这意味着，在过去的一年联想集团共裁减7500人。联想还在财报中的“费用类别”提到，“鉴于行业挑战”，产生遣散及相关费用5500万美元。联想在财报中“拨备的组成部分”说明中，提到了“主要是员工解雇付款”，目的是降低成本和提高运营效率。</p><p></p><p>值得一提的是，近期联想股价涨至一年来新高。5月27日周一早间，联想集团（00992.HK）股价一度涨逾9%，至12.06港元/股。5月30日，联想以11.52港元/股收盘。业界认为，这是AI给联想回了一口大血，联想开始走出低谷。</p><p></p><h4>软银寻求每年90亿美元的AI投资</h4><p></p><p>据外媒报道，日本软银计划每年向人工智能领域投资近90亿美元，加速企业转型。</p><p></p><p>日前，软银首席财务官后藤义光(Yoshimitsu&nbsp;Goto)向媒体透露，将加大对人工智能公司的投资。为此，软银将资产负债表维持在一个安全水平，以确保投资活动的持续性和灵活性。</p><p></p><p>报道中称，软银在人工智能领域面临着激烈的竞争，因为微软、亚马逊和谷歌等科技巨头已经投资数十亿美元与初创公司合作建立人工智能模型。此外，顶级风投公司希望与开发人工智能产品和应用程序的公司开展合作。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>2500页文档曝谷歌搜索黑幕：用户数据被挪用、设白名单机制、品牌主宰搜索排名</h4><p></p><p>5月29日消息，近日，据市场和受众研究公司SparkToro在官网博客发布的文章，一位匿名消息人士（后证实为搜索引擎优化行业资深从业者Erfan&nbsp;Azimi）向SparkToro公司的CEO&nbsp;Rand&nbsp;Fishkin提供了来自谷歌搜索API的大量泄露文档，这些文档揭示了谷歌搜索引擎内部排名算法的详细信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d7bf253b3ca43bb72cae85f2c61a974.png" /></p><p></p><p>本次泄露文档中的部分内容与谷歌公开表态的信息以及2023年的美国司法部起诉谷歌的反垄断案件中的证词相矛盾。比如，谷歌曾一再否认使用点击导向的排名机制，否认子域名在排名中被单独考虑，否认收集或考虑域名的年限等等。但泄露文档证实，这些因素都是谷歌搜索引擎排名机制中的一环。此次泄露的谷歌搜索API文档共计2500余页，共包含14014个属性特征。这些文档据称是谷歌内部“Content&nbsp;API&nbsp;Warehouse”的一部分，并在代码托管平台GitHub上不慎公开了一段时间。</p><p></p><p>博客作者Rand&nbsp;Fishkin在多位前谷歌员工的沟通中确认，泄露的文档具备谷歌内部API文档的特征。Rand&nbsp;Fishkin与和技术SEO（搜索引擎优化）专家Mike&nbsp;King进行交流，并再次验证了这些文档的真实性。</p><p></p><p>这些文档的泄露可能对搜索营销领域产生深远的影响，因为它们提供了对谷歌搜索引擎内部工作机制的前所未有的洞察，包括它如何使用点击数据、浏览器点击流、白名单、质量评估员反馈和链接质量来影响搜索结果排名。</p><p></p><h4>Meta为社交媒体数据工具CrowdTangle增添安全功能，以消除欧盟顾虑</h4><p></p><p>5月27日，Meta&nbsp;Platforms为社交媒体数据追踪工具CrowdTangle增加了安全功能，试图消除欧盟的担忧。欧盟上个月对其逐步淘汰该工具的决定的影响开展调查。Meta上周表示，下月投票的欧洲议会选举候选人将在其Facebook和Instagram的feed顶部看到如何保护自己及其账户的通知。</p><p></p><p>通知中包含的指引将帮助候选人找到相关资源，学习如何设置双因素认证、检查安全设置或启用Instagram的"隐藏词语"功能。"隐藏词语"是Instagram的一项特性，能够自动将含有攻击性词汇、短语和表情符号的私信请求转移到一个隐蔽的文件夹中。</p><p></p><p>Meta在欧盟每月拥有超过2.5亿用户。为了进一步增强选举监督，该公司在上周的宣布基础上，从本周一开始，为每个欧盟国家提供实时监控功能，按关键词、公共群组和Instagram账户进行分类，使研究人员、记者和民间社会组织能够实时监督选举过程。</p><p></p><p>欧盟委员会对Meta的这一举措表示欢迎。在一份声明中，欧盟委员会表示：“委员会将监督这些新功能的实施效果，并将继续与Meta合作，寻找更持久的解决方案，以解决初步决定中提出的所有问题。”</p><p></p><h4>苹果据悉已与OpenAI达成协议，将ChatGPT引入iOS18</h4><p></p><p>当地时间5月26日，彭博科技记者马克·古尔曼（Mark&nbsp;Gurman）在最新一期通讯中表示，传闻几个月的苹果与OpenAI洽谈将生成式人工智能功能引入iOS&nbsp;18一事，有了取得进展的消息，透露双方已经达成了协议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0147c23287e5e2cd135eba1725330a7.png" /></p><p></p><p>按惯例，苹果iOS等操作系统的重大更新，在全球开发者大会上就将宣布。为iOS&nbsp;18引入OpenAI的生成式人工智能功能，预计会在今年的大会上宣布</p><p></p><p>苹果和OpenAI洽谈将生成式人工智能引入iOS&nbsp;18的消息，在今年年初就已出现。在3月下旬的报道中，就有外媒提到苹果和百度、OpenAI及谷歌，在就引入他们的生成式人工智能模型、赋予部分iPhone机型生成式人工智能功能有过谈判，其中与百度的谈判是针对国内市场，与谷歌和OpenAI则是针对国外市场。</p><p></p><h4>XZ&nbsp;5.6.2&nbsp;释出，移除后门代码</h4><p></p><p>今年3月29日，微软PostgreSQL开发人员Andres&nbsp;Freund在调试SSH性能问题时，在开源安全邮件列表中发帖称，他在XZ软件包中发现了一个涉及混淆恶意代码的供应链攻击。据Freund和RedHat称，Git版XZ中没有恶意代码，只有完整下载包中存在。但是这个代码的提交人两年前就加入了项目维护，暂时不能确定之前的版本有没有问题。</p><p></p><p>引发广泛关注的&nbsp;XZ&nbsp;后门事件两个月之后，项目维护者&nbsp;Lasse&nbsp;Collin&nbsp;释出了新版本&nbsp;XZ&nbsp;5.6.2，移除了&nbsp;v5.6&nbsp;和&nbsp;v5.6.1&nbsp;中的后门代码&nbsp;CVE-2024-3094。</p><p></p><p>他同时宣布了一位支持维护者&nbsp;Sam&nbsp;James。对&nbsp;XZ&nbsp;后门事件的调查仍然在进行之中。XZ&nbsp;5.6.2&nbsp;还修复了一系列&nbsp;bug，包括修复了用最新&nbsp;NVIDIA&nbsp;HPC&nbsp;SDK&nbsp;构建的问题，移除&nbsp;GNU&nbsp;Indirect&nbsp;Function(IFUNC)支持，XZ&nbsp;后门代码使用了&nbsp;IFUNC&nbsp;支持，但移除主要是因为性能优势太小但复杂性大幅增加。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</id>
            <title>这个离开大厂去 AI 创业的互联网大佬，带着他的“Killer Agent”来了</title>
            <link>https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:03:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 出品, InfoQ, 大模型领航者, To B
<br>
<br>
总结: 2020年，黎科峰博士离职创业，选择了To B领域，创业3年营收数亿元，大模型产品符合To B行业需求，将颠覆现有To B软件，改变决策关系。如何选择和应用大模型，是To B行业面临的挑战。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜黎科峰博士，数势科技创始人兼CEO作者｜褚杏娟</blockquote><p></p><p>&nbsp;</p><p>2020年是一个不同寻常的年份。人们因新冠不得不停下来的时候，时任京东集团副总裁、技术委员会主席、京东商城技术负责人的黎科峰博士却选择了离职创业。此时的黎科峰博士，已经在互联网行业摸爬滚打了十余年。</p><p>&nbsp;</p><p>他曾在三星亲历了公司第一代智能手机诞生；在平安集团作为执委、平安金融科技CTO推动平安集团的用户和数据体系建设，以及金融科技云平台建设；在百度，作为原百度云计算创始成员推动百度云OS初创，也曾担任过手机百度总经理，带领实现日活跃用户1亿的突破。</p><p>&nbsp;</p><p>意料之外，情理之中。在经历了PC和移动互联网两次浪潮后，黎科峰博士明显感到To C的流量增长已经放缓，红利期基本结束，大厂们也开始在To B 领域布局。To B 就是黎科峰博士给自己选的赛道，而这次他恰好赶上了大模型这波浪潮。</p><p>&nbsp;</p><p>黎科峰博士一直说自己是“To B 新人”，但他创业3年后就让公司营收达数亿元的成绩，已经无法让人用“新人”来定义他。</p><p>&nbsp;</p><p></p><h2>C 端大佬，凭何入局 B 端</h2><p></p><p>&nbsp;</p><p></p><blockquote>“创业，始终是一条不同寻常的道路。别人已经走过并走通了的路，没有必要再去重复。”</blockquote><p></p><p>&nbsp;</p><p>中国的 To B 公司大致有两种。</p><p>&nbsp;</p><p>一种是专注项目制和定制化服务的公司，根据客户提出的具体需求逐一开发。但问题是客户的需求是零散和片段化的，他们可能并不清楚自己的数字化是什么样的。</p><p>&nbsp;</p><p>另一种则是“Copy to China”模式，商业模型、产品理念，甚至PMF理论很多是直接从美国照搬过来。但中国的市场环境和美国不同，简单复制很难成功。</p><p>&nbsp;</p><p>企业内部通常分为利润中心和成本中心，其中成本中心包括了行政、人力资源、财务和IT部门等。国内许多To B 软件实际上是围绕成本中心设计的，比如OA系统等，但这些软件的付费能力相对较弱，转化为直接业务价值的路径较长。</p><p>&nbsp;</p><p>正如黎科峰博士所说，“企业高层或数字化负责人很难有底气地说，企业增长的20%-30%是他们带来的。”</p><p>&nbsp;</p><p>表面上看，国内数字化认知不足、大家付费意愿不强等都是To B 企业面临的普遍挑战。但长期从事To C产品的经验告诉黎科峰博士，问题的关键是从业者没有解决真正的痛点、真正帮助用户创造价值，To C还是To B 反而并不重要。</p><p>&nbsp;</p><p>根据黎科峰博士观察，国内数千万的企业正在从过去依靠规模增长的模式，转变为追求内在能力提升和经营效率提高的模式。企业意识到，仅凭直觉和经验已经难以持续增长，开始寻求技术的帮助。</p><p>&nbsp;</p><p>但不同行业的软件使用者技术水平存在差异。电商、金融科技等互联网企业数字化水平较高，员工可以熟练使用复杂软件，但其他行业的企业就难以有效使用，致使其软件使用成本很高。</p><p>&nbsp;</p><p>因此，在黎科峰博士看来，To B 行业要真正发展，首先需要建立共识，即让市场参与者普遍认同软件或解决方案的价值，否则就得花大量的时间和精力说服和教育市场。其次，产品必须足够简单易用，能轻松融入业务团队的日常工作，并且无需复杂培训。</p><p>&nbsp;</p><p>大模型产品完美符合上述条件。</p><p>&nbsp;</p><p>不用多说，大家已经对大模型的价值和潜力有了共识，这是一个非常重要的优势。另外，大模型降低了数据的使用门槛，让AI和大数据不再只属于技术圈，而是广泛的普通大众。</p><p>&nbsp;</p><p>如今，虽然不是每家企业都开始实际应用大模型，但至少都在研究、了解和学习。走得远的企业已经开始尝试将大模型技术应用于实际业务中，甚至有些已经签了商业合同。</p><p>&nbsp;</p><p>“这种对大模型技术的高度认可和期待，甚至超过了当年的云计算。”黎科峰博士表示。</p><p>&nbsp;</p><p>传统的 To B 软件十分复杂，大多数功能用户并不需要，同时很久都不做本质上的改进，长期保持最初的界面和功能。而通过基于大模型的软件，用户通过简单表达就可以实时、精确地找到所需信息，软件还能不断适应和学习，越来越贴合用户的个性化需求和使用习惯。</p><p>&nbsp;</p><p>“从这个角度来看，大模型技术会颠覆现有的To B软件，甚至让很多To B软件过时。”黎科峰博士补充道，“当然，现有的ToB软件开发商也会努力追赶，但他们可能会因为历史包袱而受限。”</p><p>&nbsp;</p><p>另外，大模型产品还将改变&nbsp;To B 业务的决策关系。</p><p>&nbsp;</p><p>黎科峰博士表示，国内软件的使用者和决策者往往不是一线员工，而是不使用软件的管理层。大模型产品则让每个员工都能通过使用工具产生价值，这样决策链将不再只是老板，还有真正使用软件的员工，员工对软件效率提升的需求会影响决策者。</p><p>&nbsp;</p><p></p><h2>如何实现大模型价值落地？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“现在的大模型就像一个全科研究生，而Agent是帮助它成为某个特定业务领域专家的实现方式。”</blockquote><p></p><p>&nbsp;</p><p>在明确了大模型对于&nbsp;To B的影响后，该如何应用大模型呢？</p><p>&nbsp;</p><p>困扰很多人的首先是如何选择大模型。根据数势科技的基准测试，国内大模型产品之间的差距并不显著，与国际领先的模型，如GPT-4等比较，算力、准确度等方面确实存在较为明显的差距。</p><p>&nbsp;</p><p>但对于如何应对和看待国内外大模型的发展差异，黎科峰博士认为，“核心是我们是否需要像OpenAI那样投入巨额资金（数百亿）来发展大模型技术。”</p><p>&nbsp;</p><p>OpenAI 开始不计商业成果，致力于推动通用人工智能（AGI），后来也得到了美元基金支持，能够获得的资金是中国企业的很多倍。中国的大模型厂商在资源有限的情况下，需要从一开始就明确自己的方向，思考如何创造价值并实现商业化回报。这涉及到选择哪些方向进行投资，以确保资金的使用更加高效和有针对性。</p><p>&nbsp;</p><p>另外，大型企业和创业公司开发的大模型产品方向也各有千秋。</p><p>&nbsp;</p><p>大企业开发大模型有明显的资源优势，比如丰富的计算资源和大量数据积累，然而也面临着诸如如何将大模型整合到现有业务和产品线等较重的商业化包袱。</p><p>&nbsp;</p><p>而创业公司没有历史包袱，通常从大模型原生视角出发，不受以往业务约束，更自由地探索和创新，因此某些产品的体验可能不逊色大型企业。</p><p>&nbsp;</p><p>当前，一些财力雄厚的大型企业，如银行和国有企业会先投资建立大模型基础设施，如购买GPU和部署具有数千万、甚至数千亿参数的大模型，然后逐步挖掘需求和应用场景。</p><p>&nbsp;</p><p>但更多的企业是先找应用场景，再立项、部署大模型。这些企业会梳理出一系列需求，然后据此决定大模型的应用方向。例如，如何快速准确地找到数据、基于数据进行分析、洞察业务问题以及指导经营以提升成果，通常是很靠前的考虑因素。</p><p>&nbsp;</p><p>黎科峰博士透露，目前率先尝试大模型应用有经营分析（与数据紧密相关的金融、零售和高端制造行业）、内容生成（如营销活动所需的视频或图片生成）、企业知识库（从企业内部的知识库中提取关键信息，以便员工更好查找和学习）。企业经营分析是目前企业最感兴趣的领域，超过90%的企业表现出了明确需求。</p><p>&nbsp;</p><p>大模型具备知识、智商、学习能力和推理能力，能够总结和生成新的见解。AI Agent 的本质是智能体，内部包含许多规划业务流程规划器，能够完成基于业务目标的任务规划和自动执行。</p><p>&nbsp;</p><p>数势科技的做法是，大模型叠加 Agent，让企业应用具备记忆、反思和学习能力，能够调用企业内部工具并不断迭代反思，真正实现业务价值落地。&nbsp;</p><p>&nbsp;</p><p>不过，找到了应用场景后就让To B赛道的创业公司短期内盈利，黎科峰博士认为这并不合理。</p><p>&nbsp;</p><p>“很多企业在没有想明白胜利逻辑和商业化可能性之前，就急于跟风，最终导致失败。”黎科峰博士说道，一个企业的价值不在于目前是否盈利，而是在于它的商业模式是否成立。</p><p>&nbsp;</p><p>就像美国的许多企业一开始并不盈利，但找到正确的盈利模式后会非常赚钱。黎科峰博士认为，这背后的关键原因是，企业要找到产品能够为企业带来的业务价值，并且具有强大的复制性，实现边际成本递减，样才能实现高额盈利。</p><p>&nbsp;</p><p></p><h2>如何在内卷的环境里赚到钱？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“盈利可以通过裁员或非健康的手段实现，但这并不是我们追求的。”</blockquote><p></p><p>&nbsp;</p><p>“商业化是在大厂打工不需要太考虑的问题，但自己做企业就要考虑怎么活下去。”黎科峰博士曾在分享中说到，但他在大家还在探索大模型应用的时候，已经用大模型&nbsp;Agent 赚到钱了。</p><p>&nbsp;</p><p>“真正赚钱的是上层应用。”黎科峰博士分析道，未来，全球企业的业务经营将依赖于工具，而不是人、政策或剥削员工。最有价值的企业将是那些拥有数据、能够通过工具提升效率的企业。</p><p>&nbsp;</p><p>黎科峰博士创建数势科技之初，公司重心是构建数据资产层 ，通过打造企业指标平台和标签平台将企业海量数据转化为可使用的数据资产，释放数据价值。但产品使用门槛较高，更加适用于数字化程度较高、拥有较多数据技术人员的企业。</p><p>&nbsp;</p><p>大模型出现后，数势科技为企业用户提供数据分析AI Agent，用户可以基于自然语言对话进行交互，如此降低软件的使用门槛，管理者、业务人员等非技术人员都可以准确、即时、个性化地进行数据査询和业务洞察，提升决策能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b6cd8c55a8afa1a6299ad10995611f69.png" /></p><p></p><p>数势科技大模型 Agent 产品（SwiftAgent）架构图</p><p>&nbsp;</p><p>“数势科技的优势是使用软件和算法解决问题，而非人力。”黎科峰博士说道，“我们的人员效率很高，是因为我们用技术解决传统上需要大量人力的问题，这也是为什么我们能够实现良好的盈利。”&nbsp;</p><p>&nbsp;</p><p>那黎科峰博士一直强调的“用户价值”，如何体现在产品中的呢？</p><p>&nbsp;</p><p>他举了一个例子。在零售行业，一家茶饮连锁企业面临的挑战是如何让店长进行数据分析。由于店长的教育水平普遍低于科技或互联网公司，让他们快速、高效地利用传统BI工具做数据分析并不现实。而通过数势科技对话式、低门槛的数据分析和决策产品，店长能够通过简单的语音输入获取需要的数据，并理解数据变化背后的原因，从而更好地经营门店。如此，门店的运营不再简单依赖店长能力和经验。</p><p>&nbsp;</p><p>国内市场存在一个非常普遍且残酷的问题：资本市场状况良好时，任何一个新概念的出现都会吸引一大批公司迅速进入，结果就是甲方难以选择供应商，最终仅靠价格抉择。市场也因此变得越来越卷，好的产品和公司不得不参与价格战，最终可能出现经营困难，造成了“劣币驱逐良币”的局面。</p><p>&nbsp;</p><p>大模型市场正在历史重演。To B 企业要在如此的竞争环境中脱颖而出，黎科峰博士认为关键有两点：一要真正理解数据智能和AI，并有深厚积累，这是企业的基因；二是要回归客户需求、懂行业知识，讲能够引起客户共鸣的故事。</p><p>&nbsp;</p><p>在产品技术方面，企业最重要的是想清楚Agent与大模型的依赖关系，优化Agent产品设计，使其更加高效得运作，减少与大模型的交互次数，仅将必要的任务交由大模型处理，从而节省资源。</p><p>&nbsp;</p><p>在业务需求方面，数势科技并不是什么客户、什么行业都做，而是选择自己擅长、有很深Know-how的。这基于黎科峰博士多年大厂经验的总结。“在做技术负责人的时候，我的考核标准也不是单纯地看技术有多炫酷，而是技术能不能让企业赚钱、省钱，产品最重要的是为客户创造价值。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“我带着一种无畏的态度进入这个行业，我没有过往的包袱，也没有已经形成的固定模式，这让我能够更自由地探索和创新。”4年前获得腾讯亿元天使轮融资的黎科峰博士，如今依然站在To B的赛道上。</p><p>&nbsp;</p><p>虽说“好风凭借力”，但黎科峰博士清楚地表示，大模型技术虽然非常有潜力，但本质上依然是一个工具，是用来帮助企业实现业务目标和战略的。对于黎科峰博士来说，大模型是武器，核心竞争力是自己懂业务、懂技术。</p><p>&nbsp;</p><p>未来，数势科技利用大模型还能创造怎样的成绩，我们拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>栏目介绍</p><p>&nbsp;</p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p>&nbsp;</p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</id>
            <title>生生不息，一齐见证 AI 新次元 | InfoQ 中国成立 17 周年</title>
            <link>https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 07:07:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 作者, InfoQ, 技术资讯, 生成式AI
<br>
<br>
总结: InfoQ 是一家致力于提供中立的技术资讯和技术会议的机构，17年来一直关注技术创新和发展，特别是在生成式AI领域。他们与长城战略咨询合作，致力于推动生成式AI技术在各行业的落地，并与中国信通院合作推动人工智能技术的健康发展。通过举办活动和对谈，InfoQ努力引领技术潮流，帮助开发者和企业适应生成式AI时代的变革。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;InfoQ</p><p></p><p>2007&nbsp;年至今，InfoQ&nbsp;中国已走过&nbsp;17&nbsp;年。</p><p></p><p>17&nbsp;年里，我们置身于风云变幻的&nbsp;IT&nbsp;科技浪潮中，亲历了云端计算的兴起、人工智能的飞跃、编程语言的迭代、大数据的洪流、架构艺术的演变以及智能手机的革命，目睹了本土科技力量的茁壮成长；同时，也紧跟生成式&nbsp;AI&nbsp;等前沿技术趋势，全力促进各行业的革新与发展进程。</p><p></p><p>17&nbsp;年来，我们一直致力于提供中立的、由技术实践主导的技术资讯及技术会议，希望促进软件开发及相关领域的知识传播，助力数字时代人才的成长与蜕变，让创新技术推动社会进步。</p><p></p><p>从成立之初到今天，InfoQ&nbsp;始终坚持做技术创新浪潮中坚毅的观察者与驱动者，见证、参与每一次技术突破的航程，记录、传播那些深刻影响时代进步的科技力量与幕后英雄。迄今为止，&nbsp;InfoQ&nbsp;用数万篇高质量的文章，QCon、AICon、FCon、ArchSummit&nbsp;等数十万开发者奔赴现场的超百场技术盛会，汇聚&nbsp;1600+&nbsp;技术领导者的&nbsp;TGO&nbsp;鲲鹏会，拥抱着&nbsp;IT&nbsp;技术时代的变迁。</p><p></p><p>我们紧随技术潮流，倾力打造优质技术内容与高质量的技术学习、交流平台，并携手超过五百万技术同仁与数千家企业，在国内烙印下一代代技术人成长与突破的足迹。</p><p></p><p>如今，InfoQ&nbsp;正向着生成式AI全面进化，做&nbsp;AIGC&nbsp;时代的媒体一号位是我们当下追求的目标，也是未来发展的蓝图。</p><p></p><p>2024&nbsp;年，我们将极客邦科技发展的年度主题定为「生生不息」，希望在新的一年里，InfoQ&nbsp;迸发更强的生命力与创意火花，持续引领技术潮流，共同奔赴将生成式&nbsp;AI&nbsp;落地到千行百业的未来。</p><p></p><p></p><h2>生成式AI时代：适应、转型与超越</h2><p></p><p></p><p>站在&nbsp;17&nbsp;周年这个新起点，我们特别在6月12日策划了&nbsp;17&nbsp;周年庆系列直播活动。除明确今年向着生成式AI全面进化的大方向外，极客邦科技正积极采取多维度策略来推进这一领域的创新和发展。此次周年庆系列活动上，InfoQ&nbsp;将正式宣布公司在生成式&nbsp;AI&nbsp;布局上的两大战略计划。</p><p></p><p>为推动生成式&nbsp;AI&nbsp;技术在千行百业的落地，极客邦科技将宣布与长城战略咨询达成合作。与此同时，长城战略咨询也将发布生成式&nbsp;AI&nbsp;十大企业级通用场景报告。</p><p></p><p>此前，为推动人工智能技术的健康发展和应用，中国信通院与中国互联网协会于&nbsp;2021&nbsp;年底联合发起了“铸基计划”。这次，极客邦科技双数研究院&nbsp;InfoQ&nbsp;研究中心将与中国信通院“铸基计划”共同宣布一项重要的战略合作成果，在&nbsp;2024&nbsp;年&nbsp;8&nbsp;月&nbsp;16&nbsp;日极客邦科技举办的&nbsp;FCon&nbsp;全球金融科技大会首发《AGI&nbsp;在金融领域的应用实践洞察》报告，旨在深入探讨&nbsp;AGI&nbsp;技术在金融领域的应用现状、未来发展趋势以及所面临的挑战。未来，&nbsp;InfoQ&nbsp;研究中心也将继续关注大模型及&nbsp;AIGC&nbsp;领域的应用和产品进展。</p><p></p><p>AI的迅速发展和广泛应用已经成为推动行业变革和业务创新的重要力量。尤其是大模型等前沿技术，正在以前所未有的速度和规模，重新定义企业的运营模式、创新服务和客户体验。这种变革不仅限于技术行业本身，而且正影响到千行百业。各行各业在探索AI大模型应用过程中，既迎来全新挑战，又面临风险与机遇。为此，InfoQ&nbsp;特在周年庆活动中，精心策划了一场&nbsp;AI+行业融合圆桌对话。届时，将有来自金融、工业制造、物流等不同行业的专家到场，围绕生成式&nbsp;AI&nbsp;如何深度融入并重塑各行业展开深入探讨。</p><p></p><p>过去&nbsp;17&nbsp;年间，InfoQ&nbsp;陪伴着中国数以千万计的开发者，共同经历了中国互联网、移动互联网的发展时代，见证了新一轮技术和产业变革的数字化浪潮的到来。今天，在生成式&nbsp;AI&nbsp;的蓬勃发展下，新一代的开发者们面临着比以往更多的选择与机遇。值此&nbsp;17&nbsp;周年之际，&nbsp;InfoQ&nbsp;特别策划了此期以「AI&nbsp;时代下的程序员」为主题的开发者对谈活动。届时，三位处于不同人生阶段的程序员，将一同探讨各自在生成式&nbsp;AI&nbsp;风口之下的思考与感悟。</p><p></p><p>此次对谈，不仅会是一场新、老程序员间的相互坦白局，还穿插着不少犀利问答。在此，先剧透一下部分精选提问：</p><p>是否愿意将程序员作为终身奋斗的事业？现在的工作情况是否符合此前对于程序员职业的预期？AI&nbsp;是否会导致一线程序员的失业？新一代的程序员有怎样的职业追求？程序员更看重公司提供哪些方面的激励体制？</p><p></p><p>此外，InfoQ&nbsp;在持续为技术人群提供服务的过程中注意到，现今技术社区内对于新能源汽车的相关讨论度在急剧升高。为此，InfoQ&nbsp;于今年&nbsp;5&nbsp;月面向社区用户发起了新能源汽车调研问卷，目前已经收到了近千份问卷，最终的调研结果将在这次的周年庆系列活动公布。我们还邀请了特斯拉、理想、蔚来、问界四个热门新能源汽车品牌的车主代表，亲临&nbsp;17&nbsp;周年庆活动现场，公开反馈对问卷中各个结论的看法与评价。</p><p></p><p>17&nbsp;年来，&nbsp;InfoQ&nbsp;凭借独到的专业视角、前沿的行业报道与深度的技术内容，赢得了无数开发者们及众多业内人士的认可与持续关注。感谢&nbsp;InfoQ&nbsp;的关注者们一路相伴，一路支持！</p><p></p><p>值此周年庆之际，InfoQ&nbsp;也面向广大企业客户推出了特别福利：即日起到&nbsp;2024&nbsp;年&nbsp;6&nbsp;月&nbsp;30&nbsp;日，确认合作赞助大会的企业，会议相关产品全部享受七五折。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ade6ae5d370424d71bd36162791e376.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</id>
            <title>斯坦福AI团队“套壳”清华系开源大模型被实锤！被揭穿后全网删库跑路</title>
            <link>https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 06:31:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Medium, GPT-4V, 斯坦福, MiniCPM-Llama3-V 2.5
<br>
<br>
总结: 一篇关于斯坦福团队发布的新模型Llama 3-V的文章在Medium上引起了轰动，该模型比GPT-4V等更强，尺寸小100倍，训练成本仅需500美元。然而，网友发现Llama 3-V似乎套壳了清华系开源大模型MiniCPM-Llama3-V 2.5，斯坦福团队的解释遭到质疑，事件持续发酵。斯坦福团队被迫删除质疑评论，网友提出了四点证据证明Llama 3-V套壳行为，最终斯坦福AI团队被实锤，连夜删库跑路。 </div>
                        <hr>
                    
                    <p>5月29日，一个来自斯坦福的作者团队在Medium上发布了一篇名为《Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars》的文章，文章中称他们训练出了一个比GPT-4V、Gemini Ultra、Claude Opus 更强的SOTA 开源多模态模型，尺寸比GPT4-V小100 倍，训练成本仅需500美元。</p><p></p><h2>斯坦福AI团队“套壳”清华系开源大模型被实锤</h2><p></p><p></p><p>该团队成员Aksh Garg也在X（原Twitter）上发贴介绍了这一模型的特点。没过多久该帖的浏览量已超过 30 万，被转发了300多次，Llama 3-V的项目一下子冲到了 HuggingFace 首页。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22fe03b33e18660d2ad1b37e2746f61e.png" /></p><p></p><p>随着该项目热度的持续走高，不少X和 HuggingFace 上的网友注意到，Llama 3-V总是让人有种似曾相识的感觉，好像在哪里见到过！</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38d2376f8dd321f17cbbc64481fe1514.png" /></p><p>网友们接着深扒后发现， Llama 3-V似乎，有点，好像是套壳了清华系开源大模型MiniCPM-Llama3-V 2.5。</p><p></p><p>据悉，MiniCPM-Llama3-V 2.5是由清华系AI公司面壁智能推出并开源的MiniCPM 系列最新的端侧多模态模型，总参数量为 8B，支持 30+ 种语言，多模态综合性能超越 GPT-4V-1106、Gemini Pro、Claude 3、Qwen-VL-Max 等商用闭源模型，OCR 能力及指令跟随能力得到进一步提升，可精准识别难图、长图、长文本。</p><p></p><p>面对网友们的质疑，斯坦福这支AI团队也坐不住了，他们表示只是使用了 MiniCPM-Llama3-V 2.5 的tokenizer，并且宣称在 MiniCPM-Llama3-V 2.5 发布前就开始了这项工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81ee9d7cc314d51baad875a7dd9255c8.png" /></p><p></p><p>但他们的解释再次遭到了质疑。</p><p></p><p>通常情况下，一款模型及其详细的tokenizer往往是在其发布后才能被外人知晓，那么斯坦福这支AI团队如何能在MiniCPM-Llama3-V 2.5发布之前就获取到这些信息？</p><p></p><p>这件事持续在网上发酵。</p><p></p><p>6月2日，不死心的网友在Llama3-V的 GitHub Issue上发布质疑，或许是因为心虚，该条质疑的评论很快就被Llama3-V团队删除。</p><p></p><p>幸运的是，发布质疑的网友早已机智地提前截图保存了自己在GitHub Issue上发布的内容。</p><p></p><p>这名网友列举了在他看来Llama3-V“套壳” MiniCPM-Llama3-V 2.5的四点证据：</p><p></p><p>证据一：模型结构和代码几乎是双胞胎兄弟。</p><p></p><p>比如，套壳的 Llama3-V 与MiniCPM-Llama3-V 2.5几乎“共用”了完全相同的模型结构和代码。Llama3-V的模型结构和配置文件与MiniCPM-Llama3-V 2.5完全相同，只是变量名不同。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b17eebf7018881bc6a38da001d5835bf.png" /></p><p></p><p>左图：MiniCPM-Llama3-V 2.5                            右图：Llama3-V</p><p></p><p>证据二：Llama3-V的代码似乎就是MiniCPM-Llama3-V 2.5的代码。更令人震惊的是，Llama3-V仅仅只是进行了一些重新格式化并把一些变量重新做了命名，比如图像切片、分词器、重采样器、数据加载等变量，下图是一些示例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5d29d816945e3409cfd9846b1f99470.png" /></p><p></p><p>证据三：Llama3-V的作者表示他们“引用了LLaVA-UHD作为架构”，还列出了差异点（关于ViT和LLM的选择）。但是他们并没有提到，这个项目的具体实现与MiniCPM-Llama3-V 2.5 极其相似，却在空间模式等许多方面与LLaVA-UHD有非常多的差异。Llama3-V也具有与MiniCPM-Llama3V 2.5相同的分词器，包括MiniCPM-Llama3-V 2.5新定义的特殊符号。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a23662d9623f271b1cefd8299d5e33c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/45/458913d4dfbd98e676cf324b94e90078.png" /></p><p>证据四： 最初Llama3-V的作者在上传代码时直接导入了 MiniCPM-V 的代码，然后将名称更改为 Llama3-V。</p><p></p><p>https://huggingface.co/mustafaaljadery/llama3v/commit/3bee89259ecac051d5c3e58ab619e3fafef20ea6</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/934c15b6a4a165f8ec409a859a567991.png" /></p><p></p><h2>面壁智能团队下场实锤，斯坦福AI团队连夜删库跑路</h2><p></p><p></p><p>在屡遭质疑后，斯坦福AI团队已经被逼到了不回应实在说不过去的地步了，有网友开脸贴大该项目的作者，“你们有没有勇气面对事实”？</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7cccb06d67ca9d15c4b3601ebc00bdd.png" /></p><p></p><p>这种情况下，该团队成员不得不对网友关注的问题进行了回复。该项目中的一位作者表示：</p><p></p><p></p><blockquote>“你们的说我们抄袭简直是没影儿的事儿。Llama3-V推理存在bug，而MiniCPM的配置可以有效解决该问题，这就是为什么我们使用了相同的配置。此外，我已经指出了架构是相似的，但MiniCPM的架构来自Idéfics。SigLIP也来自Idéfics。我们遵循Idéfics论文中的那些内容。LLava UHD来自他们的实验室，我也已经指出了这一点。此外，我还强调了更多内容，即它是相同的架构，但该架构是基于综合研究的，你怎么能说它是MiniCPM呢？MiniCPM的代码，看起来，视觉部分的也是从Idéfics那里使用的。”</blockquote><p></p><p></p><p>不少网友还注意到，Llama3-V 在 MiniCPM-Llama3-V 2.5 项目发布之前就已经使用了 MiniCPM-Llama3-V 2.5 的tokenizer 。有一些用户在 Twitter 和 HuggingFace 上指出以上问题后，Llama3-V 的作者表示他们只是使用了 MiniCPM-Llama3-V 2.5 的分词器（tokenizer）。他们还声称在 MiniCPM-Llama3-V 2.5 发布之前就开始了Llama3-V 的工作。但令人无法解释的是，他们如何能在MiniCPM-Llama3-V 2.5发布之前就获取到其详细的分词器？</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f121ad805bec2bdf1e8c39ba36446fdc.png" /></p><p></p><p>对此，Llama3-V 项目作者反驳说从已经发布的上一代 MinicPM-V-2 项目里拿的标记器。但实际上，有网友留意到，MiniCPM-V-2的 tokenizer 与 MinicPM-Llama3-V2.5 完全不同，在Huggingface 里是两个文件。既不是同一个tokenizer件，文件大小也完全不同。MinicPM-Llama3-v2.5的 tokenizer 是 Llama3 的 tokenizer 加上 MiniCPM-V 系列模型的一些特殊 token 组成，MiniCPM-v2因为在Llama 3开源之前就发布的，不会有Llama 3 的分词器。</p><p></p><p>Llama3-V团队屡遭质疑却始终咬死不认的态度，惹怒了面壁智能MiniCPM-Llama3-V 2.5团队的研究人员们。</p><p></p><p>6月3日，面壁智能向AI前线列举了一些Llama3-V团队抄袭的“实锤”。</p><p></p><p>面壁智能认为，Llama3-V 项目的作者似乎并不完全理解 MiniCPM-Llama3-V 2.5 的架构，甚至也不理解他们自己的代码。</p><p></p><p>如下图 Llama3-V 的技术博客和代码显示， Llama3-V 的作者似乎没有完全理解 MiniCPM-Llama3-V 2.5 的架构，甚至也不懂他们"自己"（假若真是他们所写）的代码。</p><p></p><p>感知器重采样器（Perceiver resampler）是单层cross-attention，而不是双层self-attention。但是下图所示Llama3-V 的技术博客里作者的理解很明显是错的。另外SigLIP 的 Sigmoid 激活也不用于训练多模态大语言模型，而仅用于预训练 SigLIP。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a75f9e9de53acf5d793df56cae1ed45.png" /></p><p></p><p>截图来源：Llama3-V 的技术博客</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/51/512a694de9e4328876ed049242ea6623.png" /></p><p></p><p>截图来源：Llama3-V 的代码</p><p></p><p>面壁智能团队还表示：“另外视觉特征提取不需要 Sigmoid 激活，但下图所示Llama3-V 的技术博客里作者的理解是错的，但代码其实是正确的，这说明作者压根不理解自己的代码”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/42eeb98c7de98601c9aa8ebf7dadccdd.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d12f22ad32d7194fe7a00eefadc55d90.png" /></p><p></p><p>此外，Llama3-V相当于MiniCPM-Llama3-V 2.5的加噪声版本。</p><p></p><p>据网友反馈，当运行 Llama3-V 时，作者提供的代码无法与 HuggingFace 上的 checkpoint 配合使用。</p><p></p><p>然而令人啼笑皆非的是，当把 Llama3-V 模型权重中的变量名更改为 MiniCPM-Llama3-V 2.5 的名称后，模型可以成功运行 MiniCPM-V 的代码。这一下子帮忙解决了困扰 Llama3-V作者一周的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/6054c03b78fc21f2eb5ad2a12978f68a.png" /></p><p></p><p>如果在 MiniCPM-Llama3-V 2.5 的 checkpoint 上添加一个简单的高斯噪声（由一个标量参数化），你会预期得到什么结果？</p><p>new_dict = {}</p><p>for k, v in model.state_dict().items():</p><p>torch.cuda.manual_seed_all(42)</p><p>new_dict[k] = v + torch.randn_like(v) / 708</p><p>model.load_state_dict(new_dict)</p><p></p><p>结果是会得到一个行为与 Llama3-V 极为相似的模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1cda987e18bed0bfb1064a003ea9f6c.png" /></p><p></p><p>然而，这些还不够。更更更炸裂的是，Llama3-V 连清华团队内部并未对外公开的私有数据都能拿到？？？</p><p></p><p>据面壁智能内部团队透露，Llama3-V 大模型居然能识别清华简，OCR表现对比也很惊人，这些清华大学内部的私有数据他们又是如何拿到的呢？</p><p></p><p>MiniCPM-Llama3-V 2.5 的一个实验性功能是能够识别清华简，这是一种非常特殊且罕见的中国战国时期（公元前475年至公元前221年）写在竹简上的古文字。这些训练数据的采集和标注均有由清华NLP实验室和面壁智能团队完成，相关数据尚未对外公开。经过专有数据训练后，MiniCPM-Llama3-V 2.5 能够初步识别清华简的文字，甚至连犯的错误都一样。</p><p></p><p>然而令人惊讶的是，不可能获得专有数据训练的 Llama3-V 竟然也具有相同的能力！</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/67627805e2f4889d976c1944a65d8de5.png" /></p><p></p><p>下图展示了 Llama3-V 在识别清华简的文字时，其结果和MiniCPM-Llama3-V 2.5一致。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdacef044c8278ebed1f58329d1b8c52.png" /></p><p></p><p>有趣的是，Llama3-V 在清华简识别犯错时竟然也和 MiniCPM-Llama3-V 2.5一模一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/40/406fcf41ab5f31c53e2b59bd7e67bb1d.png" /></p><p></p><p>为谨慎起见，作者在 1000 张竹简图像上测试了几种基于 Llama3 的视觉-语言模型，并比较了每对模型的预测精确匹配。</p><p></p><p>结果显示，正常情况下每两个模型之间的重叠为零，而 Llama3-V 和 MiniCPM-Llama3-V 2.5 之间的重叠竟然达到了惊人的 87%！此外，MiniCPM-Llama3-V 2.5 和 Llama3-V 甚至在错误分布上也非常相似。Llama3-V 和 MiniCPM-Llama3-V 2.5 分别有 236 和 194 个错误预测，而其中重叠部分高达 182 个。</p><p></p><p>另外，这个高斯噪声对竹简字符也是有效的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14fc6f40c674a41da3b28123999a477b.png" /></p><p></p><p>与 MiniCPM-Llama3-V 2.5 类似，Llama3-V 也显示出了很强的 OCR 能力，甚至包括中文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/90072ebd0f7e0c45881bb9792ef0b11a.png" /></p><p></p><p>Prompt：请输出图片中的文字</p><p></p><p>MiniCPM-Llama3-V 2.5</p><p><code lang="text">### 标题
9倍像素更清晰

### 副标题
难图长图长文本精准识别

### 正文
OCR技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V2.5对于难图、长图、长文本的精准识别，再度带来出众表现！面壁自研高清图像高效编码技术，我们可以高效编码及无损识别180万高清像素图片，并且支持任意长宽比、甚至「有点变态」的1:9极限比例图像，突破了传统技术仅能识别20万像素小图的瓶颈。此前，MiniCPM-V系列多模态模型就因对于街景、长图等困难场景的高效解析，赢得了良好口碑。

### 图片描述
1. **180万像素**
   - 任意长宽比
   - 图像无损识别
   - 可支持
   - 1:9极限宽高比</code></p><p>Llama 3-V</p><p></p><p><code lang="text">### 标题
9倍像素更清晰

### 副标题
难图长图文本精准识别

### 正文
OCR技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V2.5对于难图、长文本的精准识别，再度带来出众表现。面壁自研高清图像高效编码技术，我们可以高效编码及无损识别180万高清像素图片，并且支持任意长宽比、甚至「有点变态」的1:9极限比例图像，突破了传统技术仅能识别20万像素小图的瓶颈。此前，MiniCPM-V系列多模态模型就因对于街景、长图等困难场景的高效解析，赢得了良好口碑。

### 图片描述
- **180万像素**：任意长宽比，图像无损识别，可支持。
- **1:9极限宽高比**：可支持。</code></p><p></p><p>同样的事情也发生在内部的 WebAgent 数据上，这是一个已经整合但尚未发布的功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ebe7617cac4cbab5f0d5c68ff26825c.png" /></p><p></p><p><code lang="text">Q:
Actions History
click, input, click
Your Task
Can you give me a recipe for French butter cake?
Generate next actions to do this task.

minicpmv:
actions:
click,32 273 477 508
click,32 273 477 508

llama3v:
actions:
click,32 273 477 508
click,32 273 477 508</code></p><p></p><p><img src="https://static001.geekbang.org/infoq/31/312dd56311e0b9be740ccedf88ea9314.png" /></p><p></p><p><code lang="text">Q:
Your Task
有没有关于《黑子的篮球》的新剧场版的消息？
Generate next actions to do this task.

minicpmv:
actions:
hover,732 292 792 328

llama3v:
actions:
hover,715 292 802 328</code></p><p></p><p></p><blockquote>Github开源：<a href="https://github.com/mustafaaljadery/llama3v">https://github.com/mustafaaljadery/llama3v</a>"（已删库）HuggingFace开源：<a href="https://huggingface.co/mustafaaljadery/llama3v">https://huggingface.co/mustafaaljadery/llama3v</a>"（已删库）Medium发布文章：<a href="https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee">https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee</a>"Twitter官宣模型：<a href="https://twitter.com/AkshGarg03/status/1795545445516931355">https://twitter.com/AkshGarg03/status/1795545445516931355</a>"（已删除）</blockquote><p></p><p></p><p>事情发酵至此，就在网友们都等着斯坦福AI团队再次发文力证清白时，AI前线留意到，该团队成员似乎集体“闭麦”，并且已经删除了他们在X上官宣模型的推文，连带着该项目在Github和HuggingFace上的库也已经删干净了。</p><p></p><p>而今天，面壁智能联合创始人&amp;CEO李大海也在朋友圈发文对此事进行了最新回应，李大海对此事表示遗憾。他写道：</p><p></p><p></p><blockquote>“经过团队核实，除了社区网友列出的证据外，我们还发现 Llama3v 展现出和小钢炮一样的清华简识别能力，连做错的样例都一模一样，而这一训练数据尚未对外公开。这项工作是团队同学耗时数个月，从卷帙浩繁的清华简中一个字一个字扫描下来，并逐一进行数据标注，融合进模型中的。更加 tricky 的是，两个模型在高斯扰动验证后，在正确和错误表现方面都高度相似。技术创新不易，每一项工作都是团队夜以继日的奋斗结果，也是以有限算力对全世界技术进步与创新发展作出的真诚奉献。我们希望团队的好工作被更多人关注与认可，但不是以这种方式。我们对这件事深表遗憾！一方面感慨这也是一种受到国际团队认可的方式，另一方面也呼吁大家共建开放、合作、有信任的社区环境。一起加油合作，让世界因 AGI的到来变得更好!”</blockquote><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/99/99daea9d24cdf221bbe0c7aefedfb388.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</id>
            <title>从Data Infra“卷”到AI Infra！星环科技推出知识平台TKH，全面统筹算力、语料、模型与应用</title>
            <link>https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 06:21:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 未来数据技术峰会, 人工智能, 大模型技术, AI基础设施
<br>
<br>
总结: 2024年5月31日在上海举办的未来数据技术峰会聚焦人工智能和大数据等热门话题，探讨了大模型技术在企业生产中的应用，以及企业打造自己的AI基础设施的重要性。企业需建立自己的AI基础设施，打造行业大模型，提升生产力和产业升级。星环科技推出的AI Infra工具支持企业快速建立行业大模型，实现AI应用的快速落地。通过知识平台TKH，企业可以构建安全可控的AI基础设施，帮助企业快速使用AIGC。 </div>
                        <hr>
                    
                    <p>5月31日2024向星力·未来数据技术峰会在上海隆重举办。峰会聚焦人工智能、大数据、数据要素、数字化转型、新质生产力等热门话题，业内知名专家、学者和企业代表分享最新研究成果、技术创新和实践经验，高层次产、学、研、用四方的交流和思想碰撞，为企业数据化转型和AIGC应用提供新思路、新方向和新方法。</p><p>&nbsp;</p><p>星环科技创始人、CEO孙元浩指出，大模型技术已快速融入企业生产经营的方方面面，在企业生产效率提升、业务职能提效和行业应用场景创新中无处不在，不但重构产业，打造企业的核心竞争力，而且成为企业的基础设施和核心能力，助力企业打造新质生产力。</p><p></p><h2>从Data Infra到AI Infra，为企业打造自己的AI基础设施</h2><p></p><p>&nbsp;</p><p>人工智能大模型正在催生新一轮技术创新与产业变革，也将为金融、制造、交通、政务等众多行业企业数字化转型和高质量发展带来新的动能。当前市场以通用基础大模型为主，通识能力强，但缺少行业专业知识。将大模型融入千行百业，让企业的AI应用从早期直接调用通用大模型，发展到建立自己的AI基础设施，打造行业或特定领域、任务的专用大模型，助力生产力革新和产业升级，已经成为目前企业关注的核心。</p><p>&nbsp;</p><p>星环科技创始人、CEO孙元浩宣布，星环科技的定位从Data Infra进一步延伸到AI Infra，为企业客户打造AI基础设施，打造从语料处理、模型训练、知识库建设等的一整套的工具链，帮助企业快速建立行业大模型，快速使用AIGC。</p><p>&nbsp;</p><p>从需求上看，目前，通用大模型数量快速增加，并不断升级迭代、提质增效，但是却不能适应企业AI应用的需求。而另一方面，企业对行业大模型需求增加，迫切需要降低使用门槛，更加普惠地使用大模型、生成式AI技术，以解决企业内部人才、算力、数据等不足的挑战。将AGI限制在一个特定领域或者一个行业或者特定任务，不断提升模型准确度，成为一条切实可行的发展途径。</p><p>&nbsp;</p><p>在技术上，生成式AI出现后，深度学习出现了一个重大变化，从以前的可能几千万、上亿的样本数量的大样本机器学习，演变成现在小样本机器学习，让行业大模型应用范围变得更很广，让模型能够思考、学习，能够理解自然语言，能够完成更多的工作。</p><p>&nbsp;</p><p>每个国家都需要主权AI，需要建立自己的AI基础设施，打造自己的AI模型。企业同样需要自身的AI Infra，打造行业大模型，赋能企业更快、更容易地使用大模型，释放数据价值。</p><p>&nbsp;</p><p>针对企业的AI Infra建设，星环科技通过自主研发，可以向用户提供一站式企业级大模型生产及应用全流程开发工具链，让大模型快速落地。星环科技拥有从语料到模型再到应用的完整的 AI Infra工具集，覆盖语料开发和管理、大模型训练与持续提升、多模态知识工程、多模知识存储与服务、原生AI应用构建编排和应用服务等重要阶段，提供提示词工程、检索增强、智能体构建等大模型应用快速构建和提升、模型推理优化、模型安全和持续提升技术。</p><p>&nbsp;</p><p>通过星环科技AI Infra工具，企业能够准确、高效地将拥有的多种来源的多模语料转换为高质量的专业领域知识，并且源源不断地支撑专业知识库问答、业财数据分析、智能投研、设备预测性维护等丰富的使用场景和应用，让企业构筑知识壁垒，实现“人工智能+”业务的落地和创新。</p><p>&nbsp;</p><p>星环科技AI Infra工具支持异构算力、语料、知识、大模型应用的统一管理，为数据和语料资产的集约化提供了一站式平台，且具备企业级的组织空间管理能力。</p><p>&nbsp;</p><p>未来，星环科技通过自主研发，不断完善AI从基础设施到应用的产业链条，可以为客户提供端到端的全套AI解决方案。</p><p></p><h2>推出星环知识平台TKH，为企业高效构建AI Infra</h2><p></p><p>&nbsp;</p><p>星环科技整合大数据、人工智能等技术，推出知识平台Transwarp Knowledge Hub（TKH），通过全面统管企业Al Infra算力、语料、模型和应用，为企业打造安全可控AI Infra，帮助企业快速使用AIGC。</p><p>&nbsp;</p><p>星环的知识平台TKH包括知识存储与服务、语料开发与知识构建、大模型基础服务等几个重要部分。</p><p>&nbsp;</p><p>第一，TDH for AI，打造新一代一站式多模型数字底座。</p><p>&nbsp;</p><p>星环知识平台TKH提供企业级多模态知识存储与服务，帮助企业打造新一代一站式多模型数字底座。基于Transwarp Data Hub for LLM知识管理平台的多模型统一技术架构，支持关系型数据、向量数据、全文检索、图数据、时序数据等的统一存储管理，满足各类场景下多模态数据的统一存储管理与服务，大幅简化知识库的知识存储与服务层架构，降低开发与运维成本。</p><p>&nbsp;</p><p>星环科技新推出的产品可以让企业的数据底座实现四个一体化：湖仓集一体化、多模型处理一体化、历史数据与实时数据处理一体化、本地集群和云平台一体化。</p><p>&nbsp;</p><p>其中，大数据基础平台TDH 9.4的多模基座加速AI分析；仓集资源隔离架构，在混合业务场景下依然保持极致性能；湖仓一体架构，大幅降低TCO；同份数据跑批查询与混合负载，实时备份强在线业务容灾；同时支持多模型存诸架构、大模型海量训练数据存储、多模型混合检索召回增强、Python生态等。</p><p>&nbsp;</p><p>分布式向量数据库Hippo支持文本、图片、音视频等转化后的百亿级向量数据的存储、索引和管理，支持多种索引，具有全文检索+向量检索以及稀疏向量+稠密向量的混合检索等能力。Hippo 2.0可以实现百亿级向量存储，提供灵活索引支持、20倍内存成本下降和向量全文混合检索等特性。</p><p>&nbsp;</p><p>分布式图数据库StellarDB提供万亿级图数据存储、毫秒级点边查询和10+层深度链路分析等能力，支持丰富的图算法和图机器学习，创新的动态时序图能力更便捷地挖掘数据变化规律和预测分析。而新推出的StellarDB 5.1实现了向量/全文模糊检索、秒级子图匹配、跨集群数据灾备、RAG增强大模型、GPU算法加速等功能，更稳定、更安全、更易用。</p><p>&nbsp;</p><p>星环分布式分析型数据库ArgoDB支持标准SQL语法，提供多模分析、实时数据处理、联邦计算、隐私计算、数据脱敏等能力，一站式满足OLAP分析，实时数仓、数据集市、湖仓集一体等场景。</p><p>&nbsp;</p><p>而新推出的ArgoDB 6.1 版本以“增量数据实时处理”技术为基础，定义并发布“实时数据加工”的智能高效新范式；结合集群级实时同步与数据海量版本能力，协助用户够构建高可靠的实时可信大集群，以数据透明加密、SQL审核/阻断等安全技术手段为辅，共建“快好省”湖仓集一体的融合数据处理架构。</p><p>&nbsp;</p><p>星环分布式时序数据库Timelyre支持海量时序数据库的存储与处理，具备每秒千万级数据吞吐、5~20倍无损压缩和毫秒级检索能力，支持Python、C++等API，易用的时序分析框架满足金融智能投研需求。TimeLyre 9.2新增了多模型时序分析、极速分布式回测平台、投研数据中台、时序数据湖引擎等，助力用户解锁数据深层价值。</p><p>&nbsp;</p><p>星环分布式文件系统TDFS支持10亿级以上的大小文件的存储，并同时支持对象存储，基于Raft保障强一致，支持HDFS平滑迁移，标准POSIX协议支持上层知识等AI场景应用无感对接。</p><p>&nbsp;</p><p>第二，语料加工工具与图谱构建工具，助力企业建立高质量模型及应用。</p><p>&nbsp;</p><p>决定行业大模型质量最关键的因素就是语料，语料的质量决定了模型的质量。同时高质量语料也是解决行业大模型“幻觉”、“可信可控”等核心落地难点的重要手段。另外，高质量行业专用语料是企业、机构独特的竞争优势和天然壁垒。</p><p>&nbsp;</p><p>星环科技发布了一站式多场景语料平台Transwarp Corpous Sudio（TCS），覆盖了语料获取、清洗、加工、治理、应用和管理的全生命周期，具有多种灵活的采集和构建方式，能分布式的高效处理海量语料。TCS支持20+主流文档格式、数据化学公式、复杂语料处理、语料自动标注及筛选、多视角体系化资产编目和数据治理等</p><p>&nbsp;</p><p>星环TCS拥有全面、多维、精细、增强、资产等5大优势，是一个功能全面、易用高效、安全可靠的语料开发利器，能够极大提升语料开发效率，助力企业或机构高质量地构建大模型及其应用。</p><p>&nbsp;</p><p>企业知识库的建设，让数据可以用自然语言方式进行对话和检索，企业可以集中式地管理和利用知识资源，提高运营效率和创新能力。知识库建设变成企业的基础设施，所有的不同类型的数据都能进行存储与管理，能根据需要导入到知识库中。只要企业保有自己的知识库，就可以通过微调得到企业专属的大模型，就可以实现大模型可以随时选、随时换，而企业核心竞争力得到保护的目标。</p><p>&nbsp;</p><p>企业用户利用TKH提供的星环图谱构建工具Transwarp Knowledge Studio for LLM，可以将企业内部数据、个人经验数据和公开信息数据转化为知识，让数据平台更加智能化，同时可以将AIPC端和云端资源联动，确保数据安全性。个人经验数据的知识转化和不断的模型微调让知识库建设更具个性化，真正实现个性化、专家级大模型应用。</p><p>&nbsp;</p><p>TKS是一套全流程、端到端的知识图谱构建工具集，涵盖了知识模型定义、多源异构数据接入、概念与物理数据映射、多元化知识的抽取融合、全自动知识构建、图谱综合查询等功能，能够帮助政务、工业、能源等多领域客户高效构建领域知识体系，并提供智能应用的场景定制化和一站式解决方案。</p><p>&nbsp;</p><p>第三，大模型基础服务Infinity和LLMOps，让大模型快速落地。</p><p>&nbsp;</p><p>在行业大模型发展中，企业面临技术复杂、数据和算力稀缺、管理成本高等挑战。现在企业已经意识到必须建立自己的AI基础设施，能够自己对模型进行预训练、微调等。</p><p>&nbsp;</p><p>星环科技推出大模型运营平台Transwarp&nbsp;Sophon LLMOps，提供一站式企业级大模型生产及应用全流程开发工具链，助力企业完成从预训练到微调，到强化学习，到持续模型评估的全生命周期，让每个企业都能构建自己的专属大模型。</p><p>&nbsp;</p><p>可以说，Sophon LLMOps一个平台可以解决企业在大模型时代语料、模型、应用三类资产的持续积累和加速迭代。</p><p>&nbsp;</p><p>星环科技自主研发的无涯大模型Infinity具备自主可控特性，确保数据安全的同时，通过0-1预训练，可为各行业量身定制自有大模型，提供强大的意图理解、语义召回、数据处理和分析能力。</p><p>&nbsp;</p><p>基于无涯大模型底座，星环科技微调了三款垂类大模型，包括问答大模型、数据分析（代码生成）大模型和多模态大模型，以应对内容生成、数据分析图片及音视频理解及检索等多样的使用场景。</p><p>&nbsp;</p><p>星环无涯大模型Infinity提供了灵活的部署模式，包括私有化部署（AIPC版、企业版）、公有云服务等。</p><p>&nbsp;</p><p>Infinity提升大模型数据分析能力，在语法正确性、数据库方言、语义正确性等方面有重要突破。</p><p>&nbsp;</p><p>星环无涯大模型Infinity拥有众多优势，如精准问答能力、减少大模型幻觉；多模数据来源，提升回答丰富度；构建自有知识库，确保企业数据安全等。其主要功能包括智能问答、文档问答、智能写作等。</p><p></p><h2>推出无涯·问知&nbsp;AI原生应用，服务多个垂直应用场景</h2><p></p><p>&nbsp;</p><p>基于无涯大模型，星环知识平台TKH打造了无涯·问知、无涯·问数、无涯·金融、无涯·工程等AI原生应用，可广泛应用于金融、能源、制造、工程等多个领域，通过精准的数据分析和知识管理，满足企业不同类型的知识应用需求，提升企业业务效率和竞争力。</p><p>&nbsp;</p><p>本次峰会上，星环科技最新发布了无涯·问知Inﬁnity Intelligence。无涯·问知是一款基于星环科技大模型底座，结合个人知识库、企业知识库、法律法规、财经等多种知识源的企业级垂直领域问答产品。</p><p>&nbsp;</p><p>无涯·问知充分利用了星环科技自研大模型底座的自动化知识工程特性，使其在处理和分析数据方面具有显著的优势，允许用户上传文档、表格、图片等多源数据，并支持与外部数据源的对接，使用户能够构建属于自己的专属领域大模型。这一创新功能极大地扩展了模型的应用范围和深度，用户可基于自身私域知识库进行更为个性化和深入的数据分析。</p><p>&nbsp;</p><p>无涯·问知支持不限长度的音视频图文等多模态数据快速入库，且支持自动化文档切片及向量化处理，配合自研的RAG框架，可实现知识的精准召回，可用于市场研究分析、企业供应链分析、法律风险预警、智能写作等丰富的业务场景中。</p><p>&nbsp;</p><p>无涯·问知包括四大应用场景：企业可以基于星环知识库TKH，建立企业自己的知识库应用；当企业算力不足时，可以采用安装了天涯·问知的AIPC，在本地直接访问天涯·问知，以弥补AI算力不足问题；中小企业用户不用自己构建知识库，可以直接利用星环科技无涯·问知公有云服务；对于个人而言，可以利用AIPC或者公有云服务，访问天涯·问知服务。</p><p>&nbsp;</p><p>无涯·问知AIPC版在实际应用中展现出了五大显著特性，提供了本地化的向量库；支持多种格式、不限长度的文件资料入库，满足了用户多样化的需求；支持影、音、图、文等多模态数据和资料的“知识化”处理，以及相应的“语义化”查询和应用能；自研的RAG模块，实现精准问答；具备出色的数据分析能力，能够对数量化的数据进行精准的分析和研判。</p><p>&nbsp;</p><p>另外，无涯·问数是基于星环数据分析大模型，并结合数据分析主体、指标、标签设计、数据开发和治理，形成了从自然语言转数据查询语言，并返回数据表或数据图表的完整流程。</p><p>&nbsp;</p><p>在应用场景上，无涯·问数提供分析仪表盘和智能问数能力，让决策者/管理者以自然语言提问快速自助获取目标数据；预定义指标计算口径，依托数据分析大模型理解用户的分析意图，让数据分析人员实现对话即分析；通过页面配置的方式快速完成数据准备，让数据开发人员，快速整合多种数据。</p><p></p><h2>Data Infra持续深化，星环系列产品推陈出新</h2><p></p><p>&nbsp;</p><p>星环科技在推出全新的AI Infra的同时，不断完善Data Infra产品与服务。</p><p>&nbsp;</p><p>星环大数据云平台推出TDC 5.0，将原来的多个TDH集群统一纳管，统管多个TDH集群，形成物理上分散、逻辑上统一的企业级一体化大数据平台。TDC 5.0具有独特的优势，包括多集群及其基础设施、多数据应用实例统一管理；跨多集群统一调度资源，均衡多个集群资源使用；跨集群共享存储组件，实现NoCopy的跨集群数据共享；隔离和控制资源配额，快速、灵活的为不同业务部门提供多租户的PaaS服务。</p><p>&nbsp;</p><p>星环科技推出了分布式交易型数据库KunDB 4.0，高可用能力与Oracle兼容性提升，支持跨系统多租户部署。其中，深度兼容Oracle，高度兼容Oracle对象与语法，支持数据快速迁移；高可靠，基于Paxos协议的异地容灾能力，增强数据安全性保障；数据库多租户，支持多个应用共享一个数据库实例，能快速由集中式扩展成为分布式。</p><p>&nbsp;</p><p>星环大数据开发工具TDS 4.0，增加了数据实时同步、数据入湖向导、智能化数据资产盘点、数据资产门户、数据服务编排等功能。</p><p>&nbsp;</p><p>星环大数据安全与隐私保护工具软件Transwarp Defensor是星环科技自主研发的大数据安全与隐私保护安全管理平台，致力于帮助企业建设以数据为中心的数据安全防护体系，包括了解内部敏感数据分布情况，帮助管理者发现潜在风险，监管重要数据的合规合理使用等。Transwarp Defensor 提供数据分类分级管理、数据脱敏、个人信息去标识化、数据访问控制、敏感资产风险评估等基础能力，能够做到事前发现，事中防护，事后溯源，帮助企业有效建立数据安全防护体系。Transwarp Defensor 4.5，增加了大模型核心资产识别、数据资产流转链路监控、安全策略智能推荐，安全风险预警与应急响应。</p><p>&nbsp;</p><p>星环数据要素流通平台Transwarp Navier通过提供隐私计算环境，使得数据供需双方可以进行安全的数据交易。而Transwarp Navier 3.1则新增了全链路智能合约确保安全合规、数据流通全链路行为监控与分析、实时告警与阻断等。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/695b4d28832c9e551efa7cc6c</id>
            <title>关于Vearch在大模型中使用的一些实践</title>
            <link>https://www.infoq.cn/article/695b4d28832c9e551efa7cc6c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/695b4d28832c9e551efa7cc6c</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 02:26:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 向量库, Vearch, Gamma
<br>
<br>
总结: 近年来大模型应用广泛，引发了向量库的热潮，Vearch作为一款开源向量库，集成了Gamma引擎，支持分布式最邻近搜索，为大模型提供了高效的向量存储和检索功能。 </div>
                        <hr>
                    
                    <p></p><h2>背景</h2><p></p><p>这两年来大模型及其热门，不仅各大厂家的模型层出不穷，各类RGA、Agent应用也花样繁多。这也带火了一批基础设施，比如Langchain、向量数据库（也叫矢量数据库-Vector Database）等。现在市场上的向量库种类特别繁多，但主要还是分为两类，一类是在原有数据库基础上增加了向量相似性检索的能力，比如ES、Redis等等；还有一类就是生而为向量库，比较有名的比如Qdrant、Pinecone等等。最近我们在开发一个基于大模型的测试用例生成的应用，检索采用向量库+知识图谱的混合检索方式。在调研向量数据库时，发现公司有类似的产品，也就是Vearch。并且这款产品开源比较早，也比较成熟，目前已集成到Langchain框架中，所以我们也采用了集团部署的Vearch。这样可以免去部署的烦恼，最重要的是不需要我们去找GPU资源😩（向量计算GPU更为高效，Vearch也支持CPU计算）。</p><p></p><h2>浅引</h2><p></p><p>Vearch诞生之初，主要是用在深度学习上，可以进行海量数据的近似检索。后来大模型开始流行，问答机器人此类应用变得广泛。使用时，只要将问题&amp;答案或者一段文本录入库中，通过检索问题文本向量，即可返回近似文本内容。关于Vearch的底层架构及原理，这里不再讲述（之前Vearch的元老在神灯已经写了很多文章了，感兴趣的可自行搜索）。Vearch的核心存储及检索引擎是Gamma（哎？看到这个是不是很眼熟哇，那个模型叫Gemma，不是一个东西哈），主要负责向量的存储，索引与检索。它是基于Faiss（脸书的向量聚类库）中的SIMD指令实现（这是Vearch的先驱们写的原始论文 <a href="https://arxiv.org/abs/1908.07389">https://arxiv.org/abs/1908.07389</a>"）。相比于Faiss，Vearch最大的能力是在于对分布式的支持，所用的算法称为分布式最邻近搜索，它在原始KNN基础上集成了更多的能力（可参考： <a href="https://towriting.com/blog/2021/10/07/vearch/">https://towriting.com/blog/2021/10/07/vearch/ </a>"）。关于更底层使用的余弦计算、KNN（最邻近搜索）等概念，就不在本篇范围内了。</p><p></p><p>这里再简单说两句，大模型为什么要用向量检索库（一个点铺开都有很多内容😢）。最近有一种项目比较流行，就是知识库，它是基于大模型能力的一种扩展。因为目前大模型存在两个问题，其一对于专属领域的知识理解不好，容易幻觉举个栗子，你问GPT关于京东营销中心的业务，它就只能自己捏造了；其二，它的训练数据存在滞后性，做不到对最新知识的理解。当然你要是有资源，也可以用这些知识去对模型进行微调，但对于我们这种一点资源没有的用户来说，这不是最优解。所以聪明的人们就整出了一套叫RAG的技术（检索增强生成），通过预检索提前录入的向量化知识，将检索的内容嵌入到问题的提示词模板中，这样传给大模型后，以更准确的获取答案。这里又涉及到一个概念了-嵌入（Embedding），这里不细说了，因为涉及到向量化，后面会简单说明，本篇主要还是讲Vearch的使用。</p><p></p><h2>项目简述</h2><p></p><p>大模型火热了这么久，我们也在积极研究如何应用到测试上，并给测试提效。结合我们营销中心、策略等系统繁杂业务的特性，希望有一种可以一键输出测试用例的能力，这样可以帮助我们节省用例编写的时间。而这之前，我们先需要完成知识库（业务知识、测试经验、系统上下）的建立，包括知识文档的整理、知识的录入与检索，知识图谱的创建等等。有了RGA，我们就可以根据知识库以及给定的需求文档，进行测试用例生成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d422013f1aeffe51c42c8320571c3b7a.png" /></p><p></p><p>目前我们的项目还在开发中，所以本篇的目的在于借助该项目介绍其中的一块--向量数据库Vearch的使用。</p><p></p><h2>集群库创建</h2><p></p><p>Vearch架构，它由三部分组成Master、Router、Ps。Master负责集群元数据的管理和资源的协调分配；Router负责请求路由转发及结果合并；Ps用于存储和检索向量。Vearch目前已经集成到泰山，在使用前需要先申请一个集群库，从集群列表中可以看到，每个部分默认有三台机器，确保高可用。</p><p></p><p>然后它有两个地址，一个Master地址，一个Router地址。</p><p></p><p>Master地址：用于库表维度的创建、删除等</p><p></p><p>Router地址：用于数据维度的插入、删除、检索等，不过现在也已兼容旧版本的Master，所以表操作直接用Router也没问题。</p><p></p><p>需要知道的是，目前Vearch暂未支持表结构修改能力，但新版本已经在支持中。</p><p></p><h2>功能导入</h2><p></p><p>因为我们的项目使用Langchain开发，而且Vearch以及集成进去了，所以刚开始我们使用的就是Langchain的版本</p><p></p><p><code lang="text">from langchain_community.vectorstores.vearch import Vearch
</code></p><p></p><p>这个版本比最新开源的SDK版本略旧，只支持固定字段，但是最新的开源版本已经兼容多字典。跟Vearch的同学讨论后，决定还是直接导入Github上开源的最新SDK，将下面这个文件内容复制到项目工程目录下就行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a87e23ea04d4725ff39b1872e150eeb9.png" /></p><p></p><p>然后从本地的Vearch文件引用即可。</p><p></p><p><code lang="text">from ..(路径)/Vearch文件名 import Vearch
</code></p><p></p><h2>表空间创建</h2><p></p><p>Vearch将操作全都封装成了接口，所以并不需要写类似SQL那样的语句。我们现在要创建一个名为delta_llm_embedding的表，可以通过两种方式：</p><p></p><h4>自定义建表</h4><p></p><p>可以在泰山的操控台进行操作，或在你的本地dos/MACshell的中控台进行操作。这里不分别截图演示了，毕竟不是写操作手册。本地建表语句如下：</p><p></p><p><code lang="text">curl -XPUT -H "content-type: application/json" -d'
{
    "name": "delta_llm_embedding",
    "partition_num": 3,
    "replica_num": 3,
     "engine": {
        "index_size": 1,
        "metric_type": "InnerProduct",
        "retrieval_type": "HNSW",
        "retrieval_param": {
          "nlinks": 32,
          "metric_type": "InnerProduct",
          "efSearch": 64,
          "efConstruction": 160
        }
},
    "properties": {
        "text": {
            "type": "string",
            "index": true
        },
        "text_embedding": {
            "dimension": 1536,
            "type": "vector",
            "store_param": {
                "cache_size": 2048,
                "compress": {"rate":16}
            }
        }
    }
}
' http://master_server(Master地址)/space/db/_create
</code></p><p></p><p>上面的字段意思官方文档里都有（<a href="https://vearch.readthedocs.io/zh-cn/latest/">https://vearch.readthedocs.io/zh-cn/latest/</a>"），这里就不一一解释了。需要重点说的是两点，一点是字段名，我们定义了两个字段，一个是文本字段text，一个是向量字段text_embedding。至于为什么用这两个名称也是踩过的坑，才知道有多深，稍后说。另外一点就是dimension，这个是向量维度，理论上来讲维度越高，召回越精准。但这个数据完全依赖于你的嵌入模型所能输出的维数，因为我们用的Openai的Embeddings模型：text-embedding-ada-002，看下图</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91459b1e284590efcaa4f93010ad8349.png" /></p><p></p><p>我们再说下为什么那两个字段名（文本字段text，向量字text_embedding）要这么取，先看下Vearch的SDK源码，在插入数据的那段是这么写的：</p><p></p><p><code lang="text">...
...
for text, metadata, embed inzip(texts, metadatas, embeddings):
     profiles:dict[str, Any]={}
     profiles["text"]= text
     for f in meta_field_list:
         profiles[f]= metadata[f]
     embed_np = np.array(embed)
     profiles["text_embedding"]={
         "feature":(embed_np / np.linalg.norm(embed_np)).tolist()
     }
     insert_res = self.vearch.insert_one(
         self.using_db_name, self.using_table_name, profiles
     )
...
</code></p><p></p><p>可以看到两个字段名是写死的，这也是一开始我们建表的字段名跟这个并不一致，导致我们花了不少时间来跟Vearch的同学排查问题的原因。至于SDK里为什么会这么写，原因是基于已有Vearch SDK以及Langchain的规定，必要字段被指定，其他字段可根据用户需要自适应设置。</p><p></p><h4>vearch_cluster初始化</h4><p></p><p>如果你一开始不去自行建表，而是直接在工程代码中使用SDK的初始化能力，则可以采用该种方式，这种方式你并不需要关注表结构，vearch_cluster会自行初始化一个表，你只需要告诉它表名即可。</p><p></p><p><code lang="text"> vearch_cluster = Vearch.from_documents( 
     texts,
     OpenAIEmbeddings(
         model="text-embedding-ada-002",
         openai_api_key=OPENAI_API_KEY,
         openai_api_base=OPENAI_API_BASE
          ),
      path_or_url="http://router_server(Router地址)",
     table_name="delta_llm_embedding",
     db_name="db",
     flag=1
 )
</code></p><p></p><p>如果自行建表，我们可以任意建字段，想存什么就存什么，但是带来的问题就是没法使用vearch_cluster进行初始化检索，除非你的字段是一致的。如果你预建好了表，字段也正确，那么vearch_cluster在初始化时，它会跳过建表这一步，直接插入数据，或进行检索。至于vearch_cluster初始化时，建表字段为什么只有这两个，那是因为早期这个SDK是给问答机器人使用的，那时候并没有考虑到后面在其他地方的使用，当然现在新版本已经在路上了。好了，创建就说这么多吧，至于删除，可以自行查询官方文档，进行操作。</p><p></p><h2>数据存储</h2><p></p><p>现在我们知道建表时有两个字段，一个是文本字段，一个是Embedding字段，也就是向量数据字段。在浅引里已经说过，Vearch本身只提供存储及检索能力，所以向量化是需要依赖外部工具来完成的。过程就是：文本块&gt;Embedding&gt;存储&gt;检索。当然这之前，我们需要对文档进行切割，将文档按照一定的规则切割成文本块。这部分本篇就不细说了，不然就跑题了。</p><p></p><p>我们项目使用的是Openai的Embeddings模型，从上面的vearch_cluster初始化对象中可以看出，我们使用的模型是Openai的text-embedding-ada-002。在初始化时，它会调用OpenAIEmbeddings这个类完成对texts文本的向量化，并进行建表落库。OpenAIEmbeddings是Langchain集成的Openai调用的能力。当然你也可以不使用vearch_cluster的from_documents方法，自己定义一个向量化方法，当然这会带来额外的工作量，需要自己写数据落库的方法（调Vearch的相关接口）。</p><p></p><p><code lang="text">#自定义向量化方法 
 defchunk_embeddings(chunk):
     embeddings = OpenAIEmbeddings(
         model="text-embedding-ada-002",
         openai_api_key=OPENAI_API_KEY,
         openai_api_base=OPENAI_API_BASE
      )
     query_result = embeddings.embed_query(chunk)
     return query_result
</code></p><p></p><p>比如，现在有一小段文本：营销中心， 将它嵌入后Openai的Embeddings模型会返回一个长度是1536的数组：</p><p></p><p><code lang="text">[-0.0056166304,-0.019372676,0.0055335015,-0.018635359,-0.013734359,0.019025704,-0.026355514,-0.012151293,-0.0065816496,-0.008652647,0.0049299123,0.023261668,-0.0033992538,-0.015599342,-0.014319877,0.0019336534,...,-0.0068238084]
</code></p><p></p><p>然后调用Vearch的数据写入接口（这里如果使用SDK的初始化，就不用自己写存储了，vearch_cluster都帮你做了）。落库时「营销中心」写入text字段，数组写入text_embedding。这里需要注意的是，数组的长度要跟建表时的维度一致，否则数据写入失败。如果写入成功，我们会收到这样的返回：</p><p></p><p><code lang="text">{
  "_index": "db",
  "_type": "delta_llm_embedding",
  "_id": "-3705827531945023546",
  "status": 200
}
</code></p><p></p><p>这里的_id，就是记录的唯一id了，我们可以通过它检索到这条详细的数据。在中控台里执行这个接口：</p><p></p><p><code lang="text">get  http://router_server/db/delta_llm_embedding/-3705827531945023546

#返回数据
{
  "_index": "db",
  "_type": "delta_llm_embedding",
  "_id": "-3705827531945023546",
  "found": true,
  "_source": {
    "_id": "-3705827531945023546",
    "text_embedding": {
      "feature": [-0.0056166304,-0.019372676,0.0055335015,-0.018635359,-0.013734359,0.019025704,-0.026355514,-0.012151293,-0.0065816496,-0.008652647,0.0049299123,0.023261668,-0.0033992538,-0.015599342,-0.014319877,0.0019336534,...,-0.0068238084]
      "source": ""
    },
    "text": "营销中心"
  }
}
</code></p><p></p><p>这里简单拓展下Embedding，翻译过来是嵌入，或者词嵌入。这是个有点让人头秃的名词。之前有人说过“没有思考过 Embedding，不足以谈 AI”，可见它在AI领域的重要性。当然如果不理解，也不影响我们的使用。</p><p></p><p>关于这个词，维基的解释是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。</p><p></p><p>好吧，更不好理解了。这句话的核心是将高维的对象映射到低维的空间，Embedding就是将一个离散的词映射成一个N维的数组向量，这组向量表示的是一个连续的数值空间中的点，也就是空间位置。说白点，就是将自然词通过向量化，嵌入到计算机的语言模型中，这个模型是它对人类语言对象的理解。鄙人能力有限，就不做继续深入了。</p><p></p><p>人工智能的奥义是「万物皆可Embedding」，不管是图片、文本、对象还是什么，都可以嵌入。这个技术它主要用在机器学习以及自然语言处理中，因为自然语言语意复杂多变，要想让计算机理解人类语言却不是一件简单的事。词与词之间的相关性通过距离进行量化，我们可以想象一堆词语，计算机在空间维度上对其进行聚类，词义相近的放一起，不相近的远离，所有词语之间都有一个方向和距离。最后检索时，通过这些向量进行余弦计算，得出排序分数。</p><p></p><h2>数据检索</h2><p></p><p>现在我们将文档「营销运营策略业务全景介绍」录入知识库中，我们需要根据问题进行内容检索。Vearch目前支持两种检索方式，文本检索、向量检索。</p><p></p><h4>文本检索</h4><p></p><p>这里我们使用了官方提供的similarity_search方法进行检索</p><p></p><p><code lang="text">question ="营销运营策略业务包括哪些？"#检索问题内容
cluster_res = vearch_cluster.similarity_search(query=question, k=1)
</code></p><p></p><p>我们看下这个方法，通过传入需要检索的问题内容，先对其进行向量化，然后再调了向量化的检索方法similarity_search_by_vector，通过计算向量的余弦值，对文本块做一个整体的排序，然后召回前k个文本。</p><p></p><p><code lang="text">  def similarity_search(
        self,
        query:str,
        k:int= DEFAULT_TOPN,
        **kwargs: Any,
    )-&gt; List[Document]:
        """
        Return docs most similar to query.

        """
        if self.embedding_func isNone:
            raise ValueError("embedding_func is None!!!")
        embeddings = self.embedding_func.embed_query(query)
        docs = self.similarity_search_by_vector(embeddings, k)
        return docs
</code></p><p></p><p>因为每次检索时，都是一次余弦计算，而维度越多，则计算消耗越大。</p><p></p><p>检索后，获得了如下内容：</p><p></p><p>营销运营策略是基于商品、券、内容等多维度的调控策略平台，支持各种营销活动。目前主要包含商品调控策略、权益调控策略、百补风控策略和内容调控策略...</p><p></p><p>注意这里检索的内容只是从Vearch中检索到的内容，并非大模型返回的内容。我们拿到这个检索的文本后，还需要将它嵌到提示词模板中，进行模板格式化后传给大模型，达到对大模型进行知识补充的目的。</p><p></p><h4>向量检索</h4><p></p><p>向量检索调用，跟上面的文本检索就类似了，只是使用的方法是similarity_search_by_vector，这也是上面文本检索中所调用的方法，当然参数也就变成了向量化的文本。</p><p></p><p><code lang="text">question_embedding =[0.04036456,-0.0073514967,0.026557915,-0.0005227189...]
cluster_res = vearch_cluster.similarity_search_by_vector(query=question_embedding, k=1)
</code></p><p></p><p>关于这个方法，这边就不细说了，属于Vearch的核心能力，想研究的可以看源代码，自行查询（<a href="https://github.com/vearch/vearch/blob/master/sdk/integrations/langchain/vearch.py">https://github.com/vearch/vearch/blob/master/sdk/integrations/langchain/vearch.py</a>"）。</p><p></p><h2>总结</h2><p></p><p>以上主要就是我们在该项目中所使用的Vearch主要能力，即存储跟检索。从整体使用感受来讲，功能基本满足要求，性能也不差，上手难度也不高，关键还开源。但是相比于外面已经完全实现平台化的商业产品来说，因为Vearch所有的能力都是接口封装的，写数查询这些日常操作略有麻烦，对于咱们用惯了关系数据库的新手来说稍有不适。</p><p></p><p>另外在调研Vearch时，发现已有不少业务团队将Vearch用于业务能力上，比如推荐、查重等。虽然目前营销中心的业务模块并未用到该产品，但对于我们来说是一种知识与技能拓展，并对后续的测试给出新的视野和思路。</p><p></p><p>作者：技术质量 倪绍峰</p><p>来源：京东零售技术 转载请注明来源</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</id>
            <title>527 蚂蚁技术日 | 蚂蚁集团 CTO 何征宇答记者问，精彩回答十连！</title>
            <link>https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 01:56:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁技术日, AI生活助理, 隐私计算, AI编程
<br>
<br>
总结: 蚂蚁集团每年举办技术日活动，展示AI创新应用产品，强调AI生活助理和隐私计算的重要性，推行AI编程以提高效率和改变软件开发模式。 </div>
                        <hr>
                    
                    <p>每年的 5 月 27 日是蚂蚁集团的技术日，意在勉励蚂蚁技术人保持敬畏和创新之心。至今年，技术日已发展为技术周，一场涵盖技术论坛、技术集市、编程大赛、技术沙龙等活动的技术嘉年华。</p><p>&nbsp;</p><p>5 月 28 日，第九届蚂蚁技术日对外开放，开放日上展示了诸多蚂蚁AI创新应用产品，向外界透传了“让AI像扫码支付一样便利每个人的生活”的技术主张和面貌。</p><p>&nbsp;</p><p>在技术日第一天上午的 CTO 面对面环节，蚂蚁集团 CTO 何征宇回答了几十家媒体的提问，以下是精华内容整理：</p><p></p><p>Q1：刚才您提到“人工智能生活助理”。以后生活助理界面有没有可能更细化，比如说旅行助理、美食助理？把所有的助理放在一起，要选、要点、要对话的轮次会很多。</p><p>&nbsp;</p><p>A：我们本质上就是有 N 多个小助手，但是我们认为这些助手不应是让你来选，而是让生活管家就能够理解你的意图。比如说，当我说我要去南京，我想吃点好吃的，它瞬间就是两个助手的化身，它可以帮你安排行程和吃饭的地方。所以我同意你的观点，应该是若干个助手，最终让你的生活更美好，所以我们姑且把它叫做生活管家，而且支付宝我们认为就是这么一个事情。</p><p></p><p></p><p>Q2：从去年开始，国家数据局成立，到今年，数据局已经在很多地方落地，隐私计算在这个过程中有没有发挥作用？</p><p></p><p>A：首先我想讲，国家数据局的数字中国公布了一些案例，我们是其中之一，是我们跟农业农村部合作做的农户的小额贷。我认为国家数据局讲的战略，或者数据基础设施是一个非常宏大和庞大的体系。蚂蚁也就聚焦于最关键的，我们认为最难的地方，对我们来说就是隐私计算。</p><p>&nbsp;</p><p>隐私计算不是解决确权的问题，在我们的愿景中，我们认为它就是一个管道技术，我做的这个管道比别人更便宜，质量更好——不是一个石头做的，而是一个钛合金做的管子，非常安全，不用担心被泄露，又能非常快速的能把这个数据给传过去。其实我用这个比方是在讲，我们隐私计算就是这么一个技术，但是你要是真的要从确权、流转等等到最终的消费，这是一个非常庞大的技术体系，我觉得应该是全社会、全行业都来参与这个建设的。</p><p>&nbsp;</p><p>蚂蚁在隐私计算方面的投入相对国内来说相对早，然后我们自己有使用的场景。简单讲，因为炼油的技术高，所以油要得多的话，我们需要有很好的管道技术，我们今天愿意把管道技术开放给社会，以商业化和开源的方式开放给社会，能够共建数据流转或者数据基础设施这些东西。</p><p>&nbsp;</p><p></p><p>Q3：您刚刚提到蚂蚁内部现在全面推行 AI 编程，每周差不多超 5 成程序员在使用 CodeFuse。在这个过程中软件的开发模式相交之前是不是发生了一些变化？如果说程序员能够从繁琐、枯燥的工作中释放出来的话，那市场上对程序员的需求是不是发生变化。以前有一些采访对象跟我说过，未来程序员会和产品经理进行合并，你觉得会有这样的趋势吗？会有人才焦虑吗？</p><p>&nbsp;</p><p>A：第一点，我不觉得写代码是一个很枯燥的事情。我不知道在座有多少人写过代码，我现在不写代码了，但是我几年前写代码的时候，我是觉得对于程序员、至少对我来说是可以写到脑嗨的那种状态的，远比我现在的工作回馈的环路要短。我每天把这个代码交上去了就很开心，跟玩乐高的感觉差不多，所以我不觉得写代码是一个很枯燥的工作。</p><p>&nbsp;</p><p>第二点，我们今天在推行AI编程。是因为我们觉得就算你玩乐高，有个人在帮你提高效率，有个说明书放在旁边，我觉得是可以的。所以，今天AI编程它只是一个 Copilot，只是它针对不同的人有不同的 Copilot，因为我跟国外的一些公司也交流过，对于高级别的程序员，他更需要的是，那些很简单的代码他不想写了，有些东西他调一个库就搞定了，这个程序员一直这么干的。对于一般一点的程序员，他是需要告诉他一些经典的写法是什么？一些经常犯错的，比如说编码规范的东西，他需要一些提醒。像您刚才说对于产品经理，他不想写代码的人，他就需要一个端到端的东西。所以，我认为针对不同的人，需求是不一样的。我们不能简单的讲，有了 CodeFuse 就不需要程序员了，我认为这是人为创造的一种焦虑。</p><p>&nbsp;</p><p>最后我想回答的是，AI 对于我们今天所有的软件工程的生产范式是有一些变化的。蚂蚁中间件的负责在去年就跟我说过，如果 AI 都会写代码了，是不是未来我最重要的工作就是让AI理解我的中间件？中间件就是我们在写程序的时候最基础的那层东西，蚂蚁的中间件叫 Sofa，它是给程序员用来编程的，这还是一个比较专业的事情。它是支持你的 Copilot，甚至是你的代码的导师，你去让它理解中间件。所以，我们的工作确实会发生一些变化，但是它的本质还是在 AI 这个新的生产力加入进来后，我们重新编排我们的代码流程，还是这么一个事。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q4：您刚刚提到数据孤岛的问题，我觉得现在 AI 也存在这样一个问题，比如说各家手机厂商自己也有 AI 助手，如果你想要直接去问助手的话，用 GPT 或者用其他的。但当我真正需要生活服务的时候，我必须点开支付宝再去问，这样一个流程我能直接通过手机解决，而不用打开支付宝。</p><p>&nbsp;</p><p>A：简单来讲，此助手非彼助手，世界上的助手有千千万，但是每个人的助手能力是不一样的。我们今天更重要的是提升我们自己助手的能力，支付宝今天是有一些独特的优势的，不是说我们技术有多牛，而是说我们支付宝今天整个开放生态的能量。</p><p>&nbsp;</p><p>你真的要助手帮你去做件事情，它至少得有个服务商接口吧。当然，面向未来来看，我们希望我们真的能做到跟扫码支付一样，现在很多手机会绑定一些默认的扫码，有的就是支付宝。当我们做到那个量级的时候，我认为很多厂商会跟进的。核心是我们的产品能力是不是真的能便利每个人？我认为这个是关键。</p><p>&nbsp;</p><p></p><p>Q5：刚才您在整个框架蓝图里面，从表现层-结构层-战略层的框架讲了很多东西，您有提到表现层是一些偏硬件的交互，比如里面像具生智能之类的新的交互方式。我想问一下，蚂蚁这么高的视角，您怎么看硬件交互新形式的节奏，您觉得下一个要出现的是什么？再下一个是什么？有没有具体的地图。</p><p>&nbsp;</p><p>A：关于技术跟硬件结合相关的。我觉得我们是在做探索，是有一些想法，但也不是特别成熟。大家都是言必谈软硬件结合的产品，我认为软硬件结合目前不会是一个很成功的产品，这不是今天我说的，是很早之前我就说了，但是不代表他未来不会是。从整个科技来讲，必须要有一个先修桥，先修路的过程。你先得有一个基建的过程，包括今天的 AI 大模型。今天企业投了这么多钱、国家投了那么多钱为什么没有爆发点？但是试想一下，中国如果当年没有 3G、4G，今天也不会有移动互联网，这就是一个基建的过程。硬件的逻辑也是一样的，我认为硬件是基建，它是能够把更好的体验、更好的服务能够带给每个人的。但是不应该它想象成它马上能够产生一个巨大的消费市场，这种例子我认为还是比较鲜有的。即便是苹果这么厉害的世界头号的硬件公司，它最终能够产生真正的用户黏性，或者说能够提供最终的用户价值的，其实是上面的软件，以及它所有的生态里面提供的服务。所以，我们认为硬件只是一个基础，最终能够通过这个硬件、通过上面的叠加的软件、算法能够提供什么样的服务，我觉得是关键。</p><p>&nbsp;</p><p>再回到跟蚂蚁结合的问题上来，我们虽然想星辰大海，但并不是什么都可以做。我们还是要聚焦到我们擅长的一些领域。比如在医疗领域，我们觉得有相应的机会。今天医疗健康这个大的赛道，其实你靠一个简单的手机跟你交互，我认为是非常浅的。因为它的 sensor 是不够多的，今天的多模态还是语音、图像这些东西，比如中医他具备望闻问切这一系列交互，更别说西医所有的一切了。所以在这方面的 sensor 或者多模态 sensor 数据融合上，这些硬件我认为也是有非常大的前景的。所以说结合行业，结合刚才讲的硬件的定位是基建，最后提供什么样的服务，这个东西是我们觉得是可以去探索的。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q6：刚才您提到从擅长的事出发去做思考，这个决策过程是怎样的？不光是蚂蚁，在 AI 时代能成功的应用，它有些什么样的特征？它什么时候会出现？为什么是你们能做到这件事？ 支付宝流量这么大的平台做 AI 功能，怎么平衡 AI 创新和它的风险？</p><p>&nbsp;</p><p>A：我先解释一下，我们主要的还是技术，不建议把它作为业务的考量，我们整个思考和逻辑的范围还是聚焦在技术里面。</p><p>&nbsp;</p><p>技术带来什么样的变革？我们希望给世界带来微小而美好的变化。马斯克带大家登上火星，我觉得很伟大，但是能够让地球上所有人过上美好的生活，我认为同样伟大。今天你在看我们的所有支付宝从事的行业，十年前我们开始搞金融行业就在讲，让每个人能够享受到银行行长的待遇。之前在美国的时候我读过一本书《Bank4.0》，里面有一个章节就讲支付宝的。其中最核心的是，之前的银行都是要关门的，你要去银行取个钱、办个事你得请个假。但是今天因为数字化，因为我们的技术，今天有了 7×24 小时的银行。我觉得这就是我们带来的变化，蚂蚁也是通过二维码，二维码这个技术也不是支付宝发明的，但是扫码支付确实在中国我们是第一个推出来的。这个技术本身是没有问题的，它就是一个技术而已，但是你找到一个合适的场景的结合，然后把它变成一个普惠的服务，这是支付宝最擅长的。所以，我们今天看AI也是这个逻辑，我们今天不是说要做一个普惠的技术的提供者，而是我们如何用一个最好的技术，做一个最普惠的服务的提供商。</p><p>&nbsp;</p><p>AI 今天能解决的问题是什么？我能够把服务推荐给你，我有个AI助手，它知道你需要什么服务，然后知道支付宝有什么好的服务，然后把这个服务推向给你，这就是我们想做的事情。</p><p>&nbsp;</p><p>金融这个领域不用讲了，有一个词叫“Financialhealth”，你肯定是希望你的金融是安全的、是健康的。我们所谓的金融助手、金融管家也是要帮助大家的，我们发现很多人特别是很多初级的投资者，股票一下跌就会卖。这时候AI稍微跟他聊一聊，情绪稍微稳定一点，股票就不会卖了。因为很多人金融的决策不是理性的，而是情绪的，或者是道听途说有一些消息，有一些紧张情绪，就会做一个对他整体持仓不好的操作。医疗也是一样的。我们对风险的零容忍，其实是我们对自己有一个非常高的坝，我们觉得我们推出的服务、产品、技术，应该是经得起足够大规模的考验，以及足够挑剔的人群的考验的。所以，我们觉得在金融和医疗这里，是民生，是对每个人都重要的领域，值得我们真正去投入做，而且这也是对我们技术一个非常大的挑战，放眼整个行业来看，说的骄傲一点，如果我们不出手谁出手呢？我们觉得在这波技术的变革之下，我们应该可以去做这个行业的。</p><p>&nbsp;</p><p>AI 创新和风险是要平衡的，一个是我们非常注重本身的科技伦理，技术是双刃剑，它有两面性。但是我们如何把握好这个技术，真正的让它科技向善，而不是去作恶。所以，我们有两道（关卡），一个是科技伦理委员会，每年我必须参加。在大模型这个领域我们也投了很多资源，蚂蚁百灵大模型接近 20% 的人是在做安全性，不能出现一些有伤社会伦理的事情；第二个是在风控与合规技术。这块我们的投入也非常大，这两个东西是一个枷锁，某种意义上有点像汽车的安全带，你想开快的时候一定要把安全带绑好了。但是安全带你能说他是枷锁吗？他是，但是某种意义上是能够让你上高速开的更快，我们就是这么去理解风险和创新的关系的。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q7：原生多模态，您怎么看原生多模态这个趋势，它的能力边界在哪儿？</p><p>&nbsp;</p><p>A：人机交互应该是这波AI革命所带来的，并不是 AI 本身。但是，我跟你在交流的时候，有语言、表情还有手势等等，这就是我们理解的多模态。</p><p>&nbsp;</p><p>不是说我们今天的原生多模态就是去认识那个花是什么、那个草是什么，当然这个东西也有市场前景，但不代表原生多模态就应该去做这个东西。原生多模态核心的逻辑跟大模型一样，大模型大家都知道，今天的 Transformer 所有的架构，是从翻译来的，翻译里面是LP里面最难的一个领域。但是今天大家看，习以为常了。它最难的在哪儿？就是对齐。中国的语言和英文的语言，现在看似对的很齐，但其中包含跨文化的问题，比如在翻译诗词的时候就很难对齐。原生多模态的时候我们在跟什么东西对齐？是将人类的语言和我的手势、和所有自然界的东西对齐。所以整个大模型在干的一件事情，本质上就是在对齐。但是原生多模态意味着什么？我们把世界的万物、各种模态的数据，我用这个手势代表的是什么意思，我用语言表达出来的是什么意思？这个东西要对齐，这个是我们的核心。所以，这是大模型最核心、最关键我们在做的事情。</p><p>&nbsp;</p><p>原生多模态，我们要理解后面的本质，徐鹏博士在负责我们的语言大模型，包括百灵，我们为什么会把这个组合放在一起，是因为我们觉得语言只是一个看似比较好对齐的东西。语言不是人造的，不是一个自然界的东西，它本身是有边界的。每发明一个词，边界会扩展一点，但是总体是有一个边界的。你今天如果扩展到多模态或者原生多模态这个领域，它的边界肯定是扩大的，它这个问题的象限或者复杂度是呈指数倍上升的。那我们判断要不要做这个事情？虽然它的难度很大，从中国的角度你必须要攻克这个事情，而不是说我跟在别人后面。这个事情本质上它是有意义的，就跟人脑是一样的，小朋友有的时候不会语言，他也看得懂你的知识。一只小狗，它也知道你摇摇手、或者挥挥手是什么意思，它也没有语言，这个东西是更底层的东西。所以我们未来的多模态就是帮你做一个东西，让你知道你的小狗狗到底最近怎么了。很多人不知道，我认为这个是我们可以攻克的。</p><p>&nbsp;</p><p>所以，我们在做这个事情，回答你的边界，肯定是高于语言本身的，因为语言的边界是人为划了一个圈，但是它依然是核心，因为人类的智慧基本上就在语言的边界里面，能被语言表达出来的东西，其实都是人类的智慧。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q8：去年 ChatGPT 刚出来的时候，咱们的认知有没有发生变化？</p><p>&nbsp;</p><p>A：我们其实每天都在发生变化，但是大逻辑没有变。当然这可能是技术的局限，我们觉得这是一个很好、很先进的一个技术，而且最关键的是它让大家看到了一个可能性，我认为所有人去做就是因为这个可能性。但是从另外一方面，我们更关注的是，如何让它真正的能用起来，不是说只是变成少数人的 AI，我认为应该是大部分人的 AI，是所有人的 AI。我们思考的是怎么让这个东西做的更好用、更经济、更可靠，要把这个东西做出来，这是我们的一些思考。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q9：现在可以看到几乎所有的互联网公司都在提大模型，大模型也用到各个领域。大模型背后是大数据或者云计算或者数据各方面应用。大模型会不会成为下一次宕机的威胁所在，这个技术会不会对所有平台造成新的技术挑战？出现这种问题，究竟是技术不够还是对技术的敬畏不够？</p><p>&nbsp;</p><p>A：蚂蚁技术开放日是 2015 年的一次宕机引发的，到今天差不多 10 年了。这 10 年以来技术不断地发展，但是宕机和某某平台崩了还是会在热搜上出现。你刚刚提到敬畏技术，那这 10 年里面到底是技术不够还是敬畏不够？AI 本质上是不是足够大的风险？我倾向于更积极的去理解它。你说今天的互联网技术是不是一个风险？它很多时候其实是一个风险。但是我们需不需要互联网？我相信在座所有人都觉得我们需要互联网。任何事物都有它的两面性。</p><p>&nbsp;</p><p>技术我偏向于中性的去理解它，关键在于你怎么使用它，你把它用在哪儿，或者说你对它有没有足够的敬畏，知道它的能力边界，不会去放大它，不会去夸张的吹嘘它。我认为AI这波浪潮也是一样的，打个比方，我一直认为它是大号的“复读机”，全世界的知识它能够复读出来。你把它用在关键核心的地方，可能你就要承受这种关键核心地方崩塌所带来的风险。你把它用在不重要的地方，那它就是一个背景音，像一个“收音机”，挂了就挂了，就是这样的一个逻辑。</p><p>&nbsp;</p><p>到底是技术不够还是敬畏不够？我认为这是大家对技术的期待越来越高了。试想一下十年前或者更早一段时间，其实很多网站是有宕机护时间的。但是今天再看，哪个网站说我要宕机维护一下？国外的云几乎都需要维护，但是在中国，在阿里云是没有的。我们今天的技术在越来越深入的进入到国计民生各个行业。以支付宝为例，我们经常跟自己说，十年前的支付宝和现在的支付宝完全不一样，今天的支付宝要宕机1秒钟，可能上海的地铁的闸机就过不去了，但十年前我们是没有这个职责的。所以那时候宕机宕2个小时，大家觉得日子照常过，但是今天支付宝宕机 2 小时，那会产生非常大的影响，这是我们今天对技术的要求不一样。在这个过程当中，我认为技术不怕挑战，核心是说我们有更高的要求了，我们的技术一定会往上走的。AI技术也是一样的道理，我们有要求，有足够的敬畏，有足够的投入，它一定会有足够的产出。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q10：如果一个大模型要做到可靠的话，是目前Transformer这个底层是可以优化的，还是说需要一些技术辅助它去做，您有什么技术可以分享的？</p><p>&nbsp;</p><p>A：我首先纠正一点，Transformer 不关键。因为 Transformer 的本质就是把序列数据变成并行化。之前语言大模型的训练是不可能那么快的，就算有卡也是不可能那么快的，Transformer 核心改变的就是这个。它的本质是在于我在模型训练时，包括从数据到最后的产生的输出，每一个 token 都是预测出来的，预测它一定会有误差的。但当你的误差累积到一个程度，它就一定会胡说八道的。所以，本质上是在控制这个数据流我怎么流进去以及我怎么输出来，让它在足够小的误差范围内，以及误差不要累积。</p><p>&nbsp;</p><p>至于解决这个问题，肯定是有些手段和办法的。第一个是大家都知道所谓的 RAG，它核心是控制它的输入，我只能从这个知识库里面去流向到模型系统，然后再做输出，本质上是控制它的输入。然后，输出是靠什么？我们蚂蚁也开源了在金融领域的一个多智能体框架 agentUniverse。输出是干什么呢？我们是要多个模型去对，甚至按照一定的 SOP，按照一个专家流程来产生这个输出。或者简单讲，我们要尽量收紧它可输出的范围，来进一步的控制它的精确性。当然这个东西的问题就在这儿了，AI其实就两个事，一个是泛化，一个是精准，足够泛化就很难精准，足够精准就很难泛化，这是一个本质矛盾。我们要干的事情是什么？我们在控制输入、控制输出的过程本身就是这么一个逻辑，在控制输入的时候它要足够的泛化能力，足够的泛化能力它是要靠中间的模型足够大、内容要足够多，然后我两头一掐，它牺牲掉以后，既有足够的精准性又有足够的智慧。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</id>
            <title>腾讯大模型APP真实测评！七家国产大模型“battle”，元宝顶得住吗？</title>
            <link>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:53:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词:  腾讯元宝, 混元大模型, AI搜索, AI写作
<br>
<br>
总结:  腾讯元宝是基于混元大模型的App，提供AI搜索和AI写作等核心能力，旨在服务于普通人的生活。在文生图和AI写作方面，腾讯元宝展现出了提升潜力，与其他大模型进行对比评估，效果不俗。AI搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，提升了效率和准确性。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p>&nbsp;</p><p>“腾讯做大模型不争一时之先。”</p><p>&nbsp;</p><p>5 月 30 日，腾讯基于混元大模型的App“腾讯元宝”正式上线，苹果及安卓应用商店均可下载。腾讯云副总裁、腾讯混元大模型负责人刘煜宏表示：“过去的一年，我们持续推进腾讯混元大模型的能力爬坡，希望腾讯元宝最终服务于每个普通人的生活。”</p><p>&nbsp;</p><p>相比此前测试阶段的混元小程序版本，面向工作效率场景，腾讯元宝提供了AI搜索、AI总结、AI写作等核心能力；面向日常生活场景，元宝提供了多个特色AI应用，并新增了创建个人智能体等玩法。</p><p>&nbsp;</p><p>那么，这些功能的实际表现到底如何呢？</p><p>&nbsp;</p><p>拿到腾讯元宝的体验资格后，我们马上逐一试用了它的亮点功能，并特别针对文生图和AI写作方面，通过同题多解的方式，将其与多个国内知名的大模型进行了对比和评估。</p><p>&nbsp;</p><p></p><h2>创作效果：有提升潜力</h2><p></p><p>&nbsp;</p><p>首先，来直击一下元宝与各大模型“battle”的实况。</p><p>&nbsp;</p><p>文生图</p><p>&nbsp;</p><p>在这一功能上，我们选择了百度文心一格、阿里通义万相、讯飞星火、美图 MiracleVision 4.0 、字节跳动豆包五个模型对比效果，对他们的理解和内容生成能力逐一进行了测评。另外，考虑到涉及的这些大模型都源自国产，我们特意选取了中国文言文作为输入素材，以此来考察它们在处理本国语言古典文本上的能力。</p><p>&nbsp;</p><p>从生成图中所涵盖各实体元素的完整程度和整体画面的协调性来说，元宝的表现是排在前列的。</p><p>&nbsp;</p><p>提示词为：林中有寿鹿仙狐，树上有灵禽玄鹤。瑶草奇花不谢，青松翠柏长春。仙桃常结果，修竹每留云。一条涧壑藤萝密，四面原堤草色新。</p><p>&nbsp;</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/44/44c886249db068571bc948559d535c79.jpeg" /></p><p>（从左到右分别是元宝、豆包、讯飞星火、美图 MiracleVision 4.0、文心一格的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdce8068798d7af3511dcf3aa6362660.png" /></p><p></p><p>（通义万相的输出结果）</p><p>&nbsp;</p><p>AI写作</p><p>&nbsp;</p><p>据介绍，在AI写作方面，元宝不仅支持多轮问答，还能够将对话的内容整理成报告，按照要求进行结构化输出。这一功能上，我们将其与文心一言、通义千问、Kimi、豆包四个模型对比效果。</p><p>&nbsp;</p><p>以测评案例来看，相较而言，腾讯元宝的生成内容展现出了包含起承转合的完整情节，故事框架也已具备雏形，但语句间的逻辑衔接略显生硬、人物描写也较为生涩。</p><p>&nbsp;</p><p>提示词：唐僧师徒四人穿越到现代后的第一天，会发生什么故事？写一个300字左右的小故事。</p><p>&nbsp;</p><p>元宝的生成结果如下：</p><p>&nbsp;</p><p>可以小夸一下的是，元宝留意到了唐僧师徒四人需要吃素的人物细节，在人设和故事设定方面理解得还不错。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c93ffbd3c79acfc26a4a3cbff437cf4.jpeg" /></p><p></p><p>&nbsp;</p><p>再看豆包的生成结果，其语句结构明显更为成熟了，各个段落环节之间衔接得也比较自然。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46a422e4fd58cbd06cadc795f3632ef5.png" /></p><p></p><p>&nbsp;</p><p>到文心一言这里，无论语句组织还是文字逻辑，都展现出不错的效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecd423a78fee7f5cef56b16772330317.png" /></p><p></p><p>&nbsp;</p><p>而Kimi和通义千问的生成结果，开始呈现出更显著的变化。除语句构造外，整个故事的人物设定、叙事角度、情节架构都更加立体，并都在结尾处给读者构建了一个引人入胜的虚构世界。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/06737b6e9a5777f6c72bdacd8b798721.png" /></p><p></p><p>（Kimi的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/ccae0d6aa2348bc17ce68083be19d3b6.png" /></p><p></p><p>（通义千问的输出结果）</p><p>&nbsp;</p><p></p><h2>效率、娱乐方面：表现不俗</h2><p></p><p>&nbsp;</p><p>当前，大模型仍在快速发展期，从模型能力到应用落地都存在较大“时延”。数据显示，当前人们使用大模型相关产品时，有超过 65%的需求，集中在工作/学习效率场景，但相关的AI产品解决方案尚不成熟。针对效率场景的三大核心需求：信息获取、处理和生产，腾讯元宝均进行了产品化探索。</p><p>&nbsp;</p><p>AI 搜索</p><p>&nbsp;</p><p>AI 搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，并通过AI搜索增强，提升时新类和知识类问题效果，比传统搜索更有效率；同时，内容覆盖微信公众号等腾讯生态内容及互联网权威信源，答案准确性更高；此外，元宝还会提供所引用的参考资料，并给出相关推荐，方便快速溯源及延伸阅读。</p><p>&nbsp;</p><p>我们输入一个近日引发热议的美国AI禁令问题：如何看待中国人被限制在美从事 AI 相关工作？</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/b1/b1a00d5888bfd780c585ab89630e45b5.jpeg" /></p><p>&nbsp;</p><p>AI总结</p><p>&nbsp;</p><p>AI总结方面，无论是希望快速了解一本书或是一个新领域，还是处理复杂繁冗的报告、文献，元宝都能帮上忙。据介绍，元宝可上传最多10个PDF、word、txt等多种格式的文档，并能够一次性解析多个微信公众号链接、网址，支持256K的原生窗口上下文。</p><p>&nbsp;</p><p>例如，我们输入：请总结一下AI前线公众号这一年来的内容输出亮点。</p><p>&nbsp;</p><p>生成结果如下：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/35/35ef172c42abc7d68cefc8b422c5232a.jpeg" /></p><p>&nbsp;</p><p>除了满足效率需要，腾讯元宝在日常生活场景，也提供了丰富的应用及玩法，包括百变AI头像、口语陪练、超能翻译官等，均免费开放。同时，元宝也支持用户根据个性化需求，快速创建个人专属的智能体，赋予角色设定，或让AI自动生成智能体相关信息，并复刻自己的音色。结合腾讯生态场景，元宝还将于近期推出腾讯新闻哥、《庆余年》主题等特色智能体。</p><p>&nbsp;</p><p>视频、3D生成功能后续上线</p><p>&nbsp;</p><p>腾讯元宝产品能力升级的背后，是混元底层模型的持续迭代。</p><p>&nbsp;</p><p>自 2023 年 9 月首次亮相以来，腾讯混元大模型的参数规模已从千亿升级至万亿，预训练语料从万亿升级至7 万亿tokens，并率先升级为多专家模型结构（MoE），整体性能相比Dense 版本提升超50%。除不断提升通用大模型能力外，腾讯混元也支持角色扮演、FunctionCall、代码生成等领域能力，数理能力提升 50 %。</p><p>&nbsp;</p><p>在多模态方面，腾讯混元文生图大模型是业内首个中文原生DiT架构模型，采用了Sora、Stable Diffusion 3等行业顶尖产品的同款架构，生成效果相比上代提升超 20%。目前，该模型已经全面开源，在Github获得 2000+star，相关能力也全面融入腾讯元宝。</p><p>&nbsp;</p><p>此外，腾讯混元大模型在视频、3D生成等方面也持续探索，目前已经支持16s视频生成，单图仅需30秒即可生成3D模型，相关能力也将于后续在元宝中上线。</p><p>&nbsp;</p><p>目前，腾讯内部有超 600 个业务及场景都已经接入腾讯混元，腾讯广告、微信读书、腾讯会议、腾讯文档、腾讯客服等，都已经基于混元实现了智能化升级。据了解，为了满足开发者及企业客户对于通用模型能力的需求，腾讯混元大模型已通过腾讯云对外开放，可通过API调用，也可以作为基底模型，为不同产业场景构建专属应用。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</id>
            <title>AI 是低代码的“福”还是“孽”？</title>
            <link>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:04:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 低代码, 大模型, 编程
<br>
<br>
总结: 在AI发展史上，AI大模型的出现引发了对低代码平台的讨论，AI编程的概念挑战了传统的代码开发方式，行业内存在关于AI与低代码未来发展方向的争论。在相关峰会上，专家们分享了AI与低代码的结合实践和思考，探讨了AI时代软件架构的设计和应用。 </div>
                        <hr>
                    
                    <p>取代论，在 AI 几经起伏的发展史上的每一个高点，都会被拿出来重新审视和热议。这种职业危机感，在生成式 AI 的这一波浪潮中，也很快蔓延到了技术圈中。“自己的饭碗被自己的工作干掉，这可能不是一句玩笑话。”一位开发者在近期与 InfoQ 交流时感叹。</p><p></p><p>在此之前，低代码 / 无代码作为软件提效的平台和工具已经逐步流行起来。顾名思义，其价值在于通过图形化界面和简单点击、拖拽、配置，能够大大降低代码开发的门槛，减少开发人员工作量的同时，针对一些简单开发需求，业务人员也能“自给自足”，更快地响应业务侧的需求。</p><p></p><p>而就在低代码概念开始被市场普及接纳走向落地，行业的商业模式刚刚跑通时，半路“杀出”了 AI 大模型。</p><p>相较于低代码，AI 大模型的可能性更为激进：不需要任何的编码，只通过自然语言交互就可以直接生成应用。换言之，代码开发的门槛不是降低了，而是直接没有了。</p><p></p><p>一场争论不可避免。一种声音是“低代码将被 AI 彻底颠覆”，比如，一些低代码起家的公司，去年火速切换到了 GPT 赛道，背后的考量不难臆测。另一种声音则认为，二者将双向奔赴，AI 能力将成为低代码的标配，有不少企业正在试图将二者融合。</p><p></p><p>在 6 月 14 日 -15 日即将举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit 全球架构师峰会深圳站</a>"上，阿里巴巴研究员 / 阿里云云原生应用平台负责人丁宇（叔同）将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5718">《AI 编程如何颠覆生产力》</a>" 的 Keynote 主题演讲，详细介绍在 AI 大模型快速发展的背景下，AI 编程的发展实现了哪些突破，以及 AI 编程助手的引入如何为软件开发带来质的飞跃。从开发者的视角出发，他将展开分享 AI 编程工具基于大模型的设计要点、难点、改进思路，帮助开发者从自身的生命力出发，学会用 AI 激活开发效率，提升生产力，而不是与之对抗。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8cc3b180566afd334adb450c1a57fba6.webp" /></p><p></p><p>为进一步探讨 AI 与低代码的关系，ArchSummit 深圳还进一步策划了<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1643">《低代码与 AI 结合》专题</a>"，深入研究低代码平台如何与人工智能技术相结合，提高开发效率。探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03baaaff951079ab3734a289ead8e5df.webp" /></p><p></p><p>腾讯 PCG 前端技术专家苑宗鹤将在专题演讲中分享《无极低代码 UI 可视化的 AIGC 落地与实践》。在他看来，大模型的提效能力加上低代码的易用性相辅相成，让低代码开发效率更高，更大的降低了用户的使用门槛。他将基于无极低代码平台介绍 AI 搭建低代码布局、AI 辅助代码生成，以及对平台的 AI 功能进行自动化测试的实践路径。</p><p></p><p>网易 CodeWave 技术团队作为国内较早采用大模型技术并将其应用于产品的技术团队，其团队负责人姜天意也将在专题演讲中从低代码产品的挑战出发，分析大模型的机遇和实施难点。同时，从 AI 友好的语言设计出发，结合大模型的 Agent 能力，介绍融合自然语言生成、辅助编程、D2C 等 AI 能力的产品设计和实施方案，以及相关的模型训练方案。</p><p></p><p>针对“AI 都能编程了，低代码平台会被消灭吗？”这样的灵魂拷问，蚂蚁集团支付宝体验技术部 / 高级前端技术专家江凯将在其演讲中给出他的答案。他将详细介绍《云凤蝶在 AI 与 LowCode 结合上的思考与实践》，分享 AI Native 的低代码产品形态如何设计、如何实际应用 LLM 和 AIGC 技术、如何看待对话式 UI、生成式 UI 的发展？AI 原生应用的 LowCode 研发有市场吗等一系列行业普遍的困惑问题。</p><p></p><p>除此之外，本次大会还策划了 10 余个 AI 系列相关的专题，顺丰集团 CTO 耿艳坤、Thoughtworks CTO Scott Shaw、百度飞桨总架构师于佃海等国内外 100+ 顶尖专家齐聚，将从底层基础到顶层应用深度探索大模型时代软件架构的最佳设计，共探 AI 时代的无限可能。</p><p></p><p>点击链接可查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p>会议 9 折购票倒计时1天，如您感兴趣，可以联系票务经理 17310043226，锁定最新优惠。</p><p><img src="https://static001.geekbang.org/infoq/d9/d9b861049ccebcbdec69ed036e705118.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</id>
            <title>剥离几百万行代码，复制核心算法去美国？TikTok最新回应来了</title>
            <link>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 08:34:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 代码剥离, TikTok, 美国用户, 算法
<br>
<br>
总结: TikTok正着手为其1.7亿美国用户开发克隆版推荐算法，剥离数百万行代码是一项繁琐的工作，需要一年多时间才能完成。这项工作的目标是为面向美国用户的TikTok推荐算法创建一套新的源代码库，但可能导致美国TikTok失去母公司字节跳动的工程开发支持。 </div>
                        <hr>
                    
                    <p></p><blockquote>剥离几百万行代码，是一个繁琐的“脏活”，需要一年多时间才能完成。</blockquote><p></p><p>&nbsp;</p><p>路透社5月30日消息，据直接掌握内情的消息人士透露，TikTok正着手为其1.7亿美国用户开发克隆版推荐算法。这可能催生出一个独立于其中国母公司运行的版本，因此更容易被拟发布禁令的美国立法机构所接受。</p><p>&nbsp;</p><p>就在TikTok中国母公司字节跳动去年年底下令拆分源代码之前，美国方面已经提出一项拟议的强制出售TikTok在美业务的法案，且此项法案今年初在国会得到支持。今年4月，该法案正式被签署为法律。由于未获授权公开谈论这款短视频分享应用，消息人士拒绝透露姓名。但其表示一旦代码被拆分，即可为剥离在美资产奠定基础，从而打开一条免受法律强制执行的可能之路。</p><p>&nbsp;</p><p>TikTok公司此前曾经表示并无出售在美资产的计划，甚至强调绝不可能采取此类措施。该公司最初拒绝发表评论。但在本篇报道发表之后，TikTok在X上的帖子中表示，“路透社方面日前发布的报道存在误导性，且与事实有所出入”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d2d551608dc0513c79bf068230f6055.jpeg" /></p><p></p><p>&nbsp;</p><p>路透社表示，该“辟谣”没有具体说明哪里不准确。TikTok还发布了一段来自其联邦诉讼材料的内容：“无论是从商业、技术还是法律角度来看，该法案提出的「获准剥离」以允许TikTok继续在美运营的建议都根本不可行。而即使可行，法案给出的270天时间表也远远不够。”</p><p>&nbsp;</p><p>路透社方面的发言人则回应称，“我们坚持报道内容属实。”</p><p>&nbsp;</p><p></p><h2>剥离数百万行代码的“脏活”</h2><p></p><p>&nbsp;</p><p>路透社称，过去几个月以来，来自美国和中国的数百名字节跳动和TikTok工程师被要求着手剥离数百万行代码，旨在筛选该公司用于将用户与其偏好的视频相匹配的算法。据两位直接了解该项目的消息人士说，工程师们的任务就是建立一套独立的代码库，其独立于字节跳动的中国版TikTok、即抖音的系统之外，同时将消除其中与中国用户相关的信息。</p><p>&nbsp;</p><p>路透社此前曾报道称，TikTok将应用程序连同算法一同出售的可能性极低。而且内容推荐算法在我们国家的出口管制名单当中，因此对TikTok算法的剥离或者出售必须经过审查。</p><p>&nbsp;</p><p>根据相关法律文件，TikTok推荐引擎的源代码最初由字节跳动工程师在中国开发完成，并针对包括美国在内的TikTok全球各市场运营情况进行了定制。</p><p>&nbsp;</p><p>在字节跳动看来，TikTok之所以在全球范围内大受欢迎，首先要归功于其推荐引擎的出色表现。该引擎能够根据每位用户与其观看内容的交互方式来提供更多内容来源。</p><p>&nbsp;</p><p>在向路透社描述这项任务的复杂性时，消息人士们将其称为繁琐的“脏活”，这也凸显出将TikTok业务与其母公司底层代码剥离开来的难度。据消息人士们介绍，这项工作预计需要一年多时间才能完成。</p><p>&nbsp;</p><p>TikTok及字节跳动已经明确表示会以美国宪法第一修正案为依据，在法庭上对抗这条新法律。尽管如此，消息人士称工程师们已经在依命令开展工作，着手将TikTok推荐引擎与字节跳动的整体网络拆分开来。</p><p>&nbsp;</p><p>消息人士们提到，TikTok高管曾一度考虑开源部分TikTok算法，或者允许其他人访问并修改该算法，借此展示技术透明度。</p><p>&nbsp;</p><p>据一位出席团队全体会议的消息人士、以及另一位翻阅过相关材料的消息人士所言，高管们在会上通过内部规划文件及内部通讯系统Lark传达了剥离计划，并就代码拆分项目做出了更新说明（但路透社无法独立核实这些内部消息的真伪）。</p><p>&nbsp;</p><p>据一位消息人士透露，目前这项工作的复杂之处，在于确定TikTok具体代码迁移部分所带来的合规性与法律问题。消息人士还补充称，必须审查每一行代码以核实其是否可以被剥离至独立代码库。</p><p>&nbsp;</p><p>这项工作的目标，是为面向美国用户的TikTok推荐算法创建一套新的源代码库。工作完成之后，TikTok美国版将独立于其他地区的TikTok版本以及中文版抖音，采取专门的一套推荐算法运行和维护体系。消息人士称，此举将导致美国TikTok失去母公司字节跳动强大的工程开发支持。</p><p>&nbsp;</p><p>消息人士还补充称，如果TikTok最终完成美版推荐引擎与中国版本的拆分工作，管理层承认后续TikTok美国版在性能上恐怕达不到现有TikTok的水平。因为目前TikTok推荐算法库仍高度依赖字节跳动中国工程师们的更新和维护。换句话说，TikTok在美国市场上的用户吸引力可能将因此被削弱。</p><p>&nbsp;</p><p></p><h2>TikTok推荐算法神秘吗？</h2><p></p><p>&nbsp;</p><p>在TikTok风波中，其推荐算法一直是大家争相讨论的话题中心。</p><p>&nbsp;</p><p>2022年6月，有媒体报道，TikTok宣布将美国境内的所有流量转移到甲骨文云服务的基础设施上，同时这项托管服务也给甲骨文带来高达10 亿美元收入。而后，甲骨文于这一年的8月份启动了对TikTok 算法和模型的审查，甲骨文希望确保 TikTok 上的内容显示“符合用户的期望”，并且推荐算法不会受到操纵。TikTok 还专门设有一个“专用透明度中心”的区域，供甲骨文员工审查该应用程序的源代码。</p><p>&nbsp;</p><p>然而一年之后，据福布斯报道，字节跳动和甲骨文之间的关系就已经变得非常不信任和敌对。消息人士将甲骨文对字节跳动的立场描述为“反情报行动”，而不是正常的客户关系。与此同时，一些字节跳动员工怀疑甲骨文是否只是想增加他们的账单。TikTok 托管服务合同在甲骨文内部被称为 Project Telesis，使字节跳动成为甲骨文最赚钱的客户之一。</p><p>&nbsp;</p><p>如今看来，甲骨文的审查并没有让美国政府放松对TikTok的仇恨和警惕。</p><p>&nbsp;</p><p>实际上，TikTok 之前发表过一篇博客文章，主要描述解释了他们的 FYP 算法工作机制，相信大家只要是做软件技术的，看过之后都知道其中并没有什么新鲜的创造。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/295ecc271aa2b22c4c00ed3e4b8ae723.jpeg" /></p><p></p><p>&nbsp;</p><p>另外，前亚马逊产品经理（同时也是亚马逊战略规划部第一位分析师）Eugene Wei 也曾发过专门分析文章，阐述TikTok 的算法本身并没有特别的突破性创意。</p><p>&nbsp;</p><p></p><blockquote>当大家谈及 TikTok 的算法是其成功的关键时，便会认为该公司的秘密武器是一些神奇的代码。但该领域的大多数专家持怀疑态度，TikTok 在机器学习推荐算法方面并未取得外界未知的突破性进展。事实上，他们中的大多数人认为，TikTok 很可能就是基于标准方案解决的问题，跟其他方案无异，没有什么特殊性。&nbsp;不过机器学习算法的有效性并不仅仅取决于算法本身的函数，还取决于数据集训练后的算法函数。GPT-3 并不是新创意，但是通过大量数据训练和大量的参数设置，它的输出结果往往是令人惊讶的。&nbsp;同样道理，基于自身数据集训练过的 TikTok FYP 算法，在将视频与觉得该视频有趣的人进行匹配方面做的非常精确和高效（而且，反向匹配做的也很精确，对某些视频不感兴趣的人就不会接收到这些视频） 。</blockquote><p></p><p>&nbsp;</p><p>他认为，TikTok 产品真正的价值点在于 TikTok 的设计和流程里面的每一个元素是怎么互相关联到一起，从而创建出一个数据集，再通过这个数据集，把算法训练成最佳性能的。</p><p>&nbsp;</p><p>“这就是 TikTok 设计的神奇之处：它是一个反馈的闭环，这种设计能够激发并实现视频的创作和观看，产生的数据进而通过其算法进行训练，之后再反过来激发创作和观看。为了让 TikTok 的算法变得像现在这样有效，TikTok 成为了它自己的训练数据来源。”</p><p>&nbsp;</p><p>但多数人还是非常费解，为什么很多公司想要收购 TikTok，另一方面，字节跳动是否应该将 TikTok 这一备受欢迎的 App 卖掉。对此，Eugene Wei 评论说：“围绕 TikTok 算法大肆的炒作已经开始变的异化了，这也是如今西方对中国科技领域项目的普遍套路。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/">https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/</a>"</p><p><a href="https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you">https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you</a>"</p><p><a href="https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</id>
            <title>全球首款PC原生的AI编程与软件智能研发助手驭码CodeRider正式发布！</title>
            <link>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 02:56:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, AI产品发布会, 驭码 CodeRider, 极狐GitLab
<br>
<br>
总结: 5月28日下午，极狐GitLab在上海成功举办了基于AIGC技术且完全自研的AI产品发布会，正式推出驭码CodeRider，PC原生的AI编程与软件智能研发助手。CEO柳钢介绍了驭码CodeRider的含义和优势，强调AI赋能程序员而非取代。产品具有PC原生、私有化部署、与GitLab深度融合等优势，受到企业用户好评。发布会还宣布了驭码CodeRider的三个版本，开启了试用通道。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/c3/c3e154ed850988f8ff90ce95496aa795.png" /></p><p></p><p>5 月 28 日下午，极狐GitLab 在上海成功举办了基于 AIGC 技术且完全自研的 AI 产品发布会，正式推出驭码 CodeRider —— PC 原生的 AI 编程与软件智能研发助手。发布会采取线上线下相结合的方式，线下有包括企业高管、高校教师、高校学生、媒体人员在内的 80 多位嘉宾共同出席参加了此次发布会，线上观看量超过 10万+。 </p><p></p><p>发布会伊始，极狐GitLab CEO 柳钢以“PC 原生全球首发 AI 编程与软件智能研发助手”为主题，全面介绍了驭码 CodeRider。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/40/40e343e6a832e4f6098a3baf0b60536d.png" /></p><p>极狐GitLab CEO 柳钢 </p><p></p><p>柳钢表示，软件定义世界已经成为了不争的事实，而软件的打造者、代码的创造者正是程序员这一群体，在中国就有 1000万程序员。而极狐GitLab就是一家专为中国程序员服务的公司，也是目前国内唯一一家将赋能中国程序员写进员工手册的公司，极狐公司的使命是——让中国程序员的技术潜能与业务价值得到最大化发展。极狐GitLab 致力于通过借鉴全球领先的技术，同时基于国内现状进行完全自主研发、自主创新的方式来赋能中国1000万程序员。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c115d21986defd5f342fcde2bb393d21.png" /></p><p>赋能 or 取代？ </p><p></p><p>柳钢进一步表示，在 AI 时代赋能中国程序员的最好方式就是将 AI 能力应用到软件研发领域，为程序员打造出为程序员所有、被程序员所用、让程序员认可的产品，用 AI 赋能程序员，绝非取代程序员 。但是在这个过程中必须解决当前大模型的三个关键问题：数据安全、个性化以及成本。只有成功解决这三个问题，才能够真正打造出程序员心目中的绝佳好产品——覆盖软件研发全生命周期、企业统一部署 &amp; 用户一键安装、功能丰富 &amp; 触手可及以及私藏独有。</p><p></p><p></p><p>“极狐GitLab AI 团队完全自研的驭码 CodeRider 正是这样一款产品。而且驭码 CodeRider 是全球首款 PC 原生的 AI 编程与软件智能研发助手。”柳钢强调。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24c56680c152f342663a416854148fd6.png" /></p><p>极狐GitLab 驭码CodeRider 正式发布 </p><p></p><p>接着，柳钢解读了驭码 CodeRider 的含义。Code 是程序员心目中最神圣的词，意为编码，而 Rider 有骑兵、驾驭者的意思。为此，特意为该产品起了一个朗朗上口的 Slogan“为 AI（爱）奔腾，驭码当先”。从表面看，驭码 CodeRider 是一个 AI 编程助手，看似是又一款 Copilot，但是驭码又绝不仅仅只是一款 Copilot，相比市面上其他所有的 Copilots，驭码 CodeRider 要比他们优秀三点，而这也是驭码 CodeRider 得天独厚的优势：PC 原生、私有化部署、GitLab 合璧。PC 原生是指驭码 CodeRider 是端侧部署，和笔记本电脑是天然契合的，而且产品在设计之初就考虑了离线运行；私有化部署是指可以将驭码 CodeRider 部署在企业内部的安全环境中，这从根本上解决了数据安全问题，而且能够更好地满足和实现企业智能化、个性化问题；最后一点：驭码 CodeRider 和 GitLab 深度融合、浑然一体，这意味着驭码 CodeRider 将 AI 技术完全应用到软件研发的全生命周期中，不仅可以编写代码，还能够帮助程序员进行议题（Issue）和合并请求（MR）的处理，甚至和 CI/CD 结合起来，做到端到端的 AI 赋能。这些都是其他 Copilots 无法做到的！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a34bc5dd659e0c563ed255a78769b9fd.png" /></p><p>驭码CodeRider 比 Copilot 好三点 </p><p></p><p>柳钢表示，自夸不算好，要用户说了才算好，目前已经有很多企业用户在体验试用驭码 CodeRider，他们的反馈是对驭码 CodeRider 最好的认可。随后，现场播放了来自嘉宾的祝福视频，包括联想集团、通义实验室、零一万物、PingCAP、无问芯穷、LigaAI、重庆市政府九龙坡区、未尽研究、哪吒汽车、足下科技、PingCode、Sipingsoft、青岛职业技术学院、南京大学、厦门大学、重庆交通大学、广州华商学院的多位嘉宾纷纷表达了对驭码 CodeRider 即将发布的热烈期盼，同时预祝发布会圆满成功！ </p><p></p><p>分享最后，柳钢正式揭晓了驭码 CodeRider 的三个版本：驭码团队版（CodeRider Team）、驭码企业版（CodeRider Enterprise）以及驭码混合算力版（CodeRider Hybrid）。三个版本对应不同的功能以及不同的使用场景。驭码CodeRider 同步开启了申请试用通道，会有专业的顾问为大家解答关于驭码CodeRider 的相关情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0723ffc88faa9bccf42476d76f49a1c3.png" /></p><p>驭码 CodeRider 专业顾问联系方式 </p><p></p><p>随后，极狐GitLab AI 团队负责人、驭码CodeRider 研发负责人邹雨竹上台深入介绍了驭码 CodeRider 的相关情况。他提到，“驭码 CodeRider 是一款 AI 驱动的 PC 原生应用，是研发人员的智能编程助手，同时跟 GitLab 的深度集成，也让驭码 CodeRider 变成了一款智能 DevOps 工具，真正做到了“一款工具，双重助力”。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6780c895939065b4083d7d041665f79a.png" /></p><p>极狐GitLab AI 产品负责人邹雨竹 </p><p></p><p>邹雨竹进一步解释道，打造这样一款具备 AI 能力的 PC 原生应用，必须考虑三个核心要素算力、模型、引擎。在算力方面，驭码 CodeRider 从设计之初就决定要做 PC 原生的“AI 编程与软件智能研发助手”，为此对 20 多款 AI PC 进行了详尽测评；在模型方面，采取采众家之所长的策略，对 30 多款大模型进行了测评，最后决定驭码 CodeRider 根据不同功能选择最佳模型，比如代码补全使用补全模型、对话采用对话模型，以实现在高效量化的同时达到性能和与体积最佳平衡的目的；在引擎方面，采用了基于 C++ 的桌面推理引擎，而且对不同的硬件指令集和框架做了适配。而这样做也获得了惊艳的效果，同样也造就了驭码 CodeRider 这样一款同时适用于企业、个人的 AI 产品。企业可以通过私有化部署实现数据安全并节省超过 50% 的成本，而开发者则能享受便携性和极佳的响应速度。 </p><p></p><p>邹雨竹还在现场演示了驭码 CodeRider 的相关功能，包括智能编程部分的代码补全、代码生成、技术问答等，以及智能 DevOps 工作流中的 Issue、MR 处理等。“驭码 CodeRider 绝对是一款让企业受益的 AI 工具，因为其具备私有化部署的特性，有着模型组合的优越性，而且与 GitLab 浑然天成”。邹雨竹说到。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57f4a2d7111a97b0a83e660fba224d83.png" /></p><p>驭码CodeRider 产品功能图 </p><p></p><p>接着，来自联想中国的段勐、英特尔中国的张智勇、浙江省特级教师谢作如，三位嘉宾分别进行了分享，他们从企业、高校的角度分享阐述了与 AI 相关的内容和观点。 </p><p></p><p>发布会最后，极狐GitLab CEO 柳钢再次登台，正式揭晓了驭码 CodeRider 三个版本的价格：驭码团队版（CodeRider Team）的价格为499/人/年、驭码企业版（CodeRider Enterprise）的价格为899/人/年。与此同时，也透露了，目前驭码 CodeRider 团队正在积极打造企业级的混合算力私有化架构，智能终端+智能边端会让驭码 CodeRider 变得更强大，这样强大的产品将在 90 天以后与大家见面！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d4e4a7198909b9d667a42a123ca49d0c.png" /></p><p>驭码CodeRider 价格 </p><p></p><p>最后，柳钢表示，全球首款 PC 原生的 AI 智能编程与软件智能研发助手——驭码CodeRider 发布会宣告结束！到场嘉宾纷纷走向极狐GitLab 团队提前准备好的驭码 CodeRider 体验区，现场体验驭码 CodeRider 的 AI 功能并与技术人员进行了深入交流。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff74da88bd54726eb9c1496759ae3106.png" /></p><p>驭码CodeRider 现场体验区</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>