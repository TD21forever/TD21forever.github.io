<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/XPMslMBIT0HggCWFQZwg</id>
            <title>大模型个性化时代到来！讯飞星火V4.0首发“个人空间”，打造实用的AI助手</title>
            <link>https://www.infoq.cn/article/XPMslMBIT0HggCWFQZwg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XPMslMBIT0HggCWFQZwg</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 10:40:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 讯飞星火V4.0, 大模型个性化时代, 语音大模型, 个人空间
<br>
<br>
总结: 讯飞科大在北京发布了讯飞星火V4.0及相关应用，该大模型在多个领域取得突破，包括语音识别、个性化应用、医疗领域等，为用户提供更智能、个性化的服务。 </div>
                        <hr>
                    
                    <p>讯飞星火V4.0来了！6月27日，科大讯飞在北京发布讯飞星火大模型V4.0及相关落地应用。讯飞星火V4.0七大核心能力全面提升，整体超越GPT-4 Turbo，在8个国际主流测试集中排名第一，国内大模型全面领先。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cba8ab630a75a5446b1a4a8fa5783b9c.jpeg" /></p><p></p><p>大模型个性化时代到来！讯飞星火APP/Desk全新升级，发布“个人空间”，打造更懂你的AI助手；面向专业领域的个性化应用，科大讯飞升级讯飞晓医APP，上线个人数字健康空间，打造每个人的健康助手；业界首发星火智能批阅机，“AI助教”助力老师减负增效、因材施教；讯飞AI学习机升级 1对1 答疑辅导功能，打造每个孩子的AI学习助手。</p><p></p><p>面向万物互联时代，星火语音大模型再突破，发布74个语种/方言免切换对话，破解强干扰场景下语音识别难题，发布国际领先的极复杂场景语音转写技术，并通过云边端及软硬一体化解决方案，赋能汽车、家电、机器人等领域人机交互变革。此外，面向企业“人工智能+”场景价值落地最后一公里，科大讯飞正式发布星火企业智能体平台，并推出星火商机助手、星火评标助手等典型智能体案例，助力企业价值创造。</p><p></p><p>8个国际主流测试集测评第一，讯飞星火V4.0 整体超越GPT-4 Turbo</p><p></p><p>今年中高考真题实测中，讯飞星火语数外各科“成绩”均排名第一，被评为“更会做题的大模型”；在科研上，讯飞星火助力中国科学技术大学刘海燕教授团队，将蛋白质设计成功率从0.1%提升到20%，设计所需时间从6个月降到1天；赋能每个人，帮助一位不懂法律知识的70岁老人顺利要回养老钱欠款、帮助一位听障人士圆了文学梦······讯飞星火正成为我们每个人的AI助手。</p><p></p><p>自去年9月全面开放以来，讯飞星火APP在安卓公开市场累计下载量达1.31亿次，在国内工具类通用大模型App中排名第一，并围绕写作、编程、工作、学习等涌现出一批用户喜爱的热门助手。今年“618大促”，星火大模型加持的智能硬件销量同比增长超70%，月均使用次数超4000万，越来越多的用户开始享受到大模型带来的红利。</p><p></p><p>现场基于全国首个国产万卡算力集群“飞星一号”，讯飞星火大模型V4.0正式发布。讯飞星火V4.0 七大核心能力全面升级，全面对标GPT-4 Turbo，并实现在文本生成、语言理解、知识问答、逻辑推理、数学能力等方面的整体超越。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29ec0d7b6d814979f3e9599a1c468bb2.jpeg" /></p><p></p><p>讯飞星火V4.0在图文识别能力上进一步升级，在科研、金融、医疗、司法、办公等场景的应用效果已领先GPT-4o。此外，星火长文本能力也全新升级，并针对长文档知识问答的幻觉问题，业界首发溯源功能。</p><p>外部权威测试集也体现出讯飞星火V4.0的领先性。在国内外12项大模型主流测试集中，讯飞星火在8个测试集中排名第一，超越GPT-4 Turbo等国际大模型，国内大模型全面领先。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb469d9b336ca37d0e09ef7197eaca3a.jpeg" /></p><p></p><p>现场，刘庆峰展示了讯飞星火V4.0在复杂指令、复杂逻辑推理、空间推理、高中数学等方面的效果，星火“智商”再度进化。以空间推理为例，“Bob在客厅里。他拿着一个杯子走到厨房。他把球放进杯子里，然后拿着杯子走到卧室。他把杯子倒过来，然后走到花园。他把杯子放在花园里，然后走到车库。问题：球在什么地方？”讯飞星火可以基于空间和常识推断出球在卧室的地面上，这些能力的进步对于以后的具身智能、家庭机器人都具有意义。</p><p></p><p>大模型个性化时代到来！讯飞星火首发“个人空间”，数百万用户一键拥有“AI智能全家桶”</p><p></p><p>大模型在给我们的工作、生活带来便利的同时，也存在各家生成内容差不多、生成内容较泛、不够实用的情况，怎么样让大模型更好用，在工作生活中形成独特的价值？科大讯飞给出答案——打造更懂你的AI助手。</p><p>如何打造懂你的AI助手？刘庆峰提出，AI助手要能够基于用户画像进行个性化表达，基于使用历史进行记忆学习，基于个人资料进行增强学习。在构建用户个人画像时，人设风格可以自己选定，也可以根据对话和使用历史动态完善，进而形成个性化的表达风格；AI助手再结合个人资料，就可以生成个性化和针对性内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b453421b3e5f0dffd3aef913fbdd932.png" /></p><p></p><p>基于此，讯飞星火APP及桌面版全新升级改版，率先发布“个人空间”，用户可以上传自己的工作、学习、生活、健康等各类资料，形成每个人的专属知识库，再结合人设，让大模型生成更个性化内容。此外，讯飞星火首批上线 14 个智能体，面向特定场景打造专属助手。</p><p></p><p>科大讯飞研究院院长刘聪现场演示“个人空间”效果，当他上传了女儿写的小作文并选取符合女儿风格的AI人设标签后，星火生成了一篇活泼、可爱更个性化的文章；当他上传了讯飞翻译机的产品海报、用户短视频、相关录音，星火也可以根据这些多模态信息生成产品培训文档，还可以对生成的信息进行多模态溯源。大模型进入个性化时代，大模型工作、学习“可用性”飙升！</p><p></p><p>此外，星火大模型还打通了全系讯飞C端软硬件产品生态，数百万智能硬件用户一键拥有“星火全家桶”。比如讯飞智能办公本、智能录音笔的文件可以一键同步到星火个人空间中，通过数据互通、操作联动，把一篇办公本里会议记录同步到星火中，就可以让星火进行公文写作，还可以做PPT，以及生成待办事项等，带来更高效的办公体验。</p><p></p><p>个人数字健康空间来了！讯飞晓医APP下载量超1200万</p><p></p><p>面向专业领域的个性化应用，科大讯飞升级讯飞晓医APP，发布个人数字健康空间，打造每个人、每个家庭的AI健康助手。</p><p></p><p>在医疗领域，讯飞星火医疗大模型再次升级，医疗核心能力全面超过GPT-4 Turbo和GPT-4o。在此基础上，讯飞晓医APP各项能力持续升级，覆盖1600种常见疾病、2800种常见药品、6000种常见检查检验，满足用户在看病前、用药时、检查后的核心场景健康需求。当前，讯飞晓医APP累计下载量1200万，用户好评率98.8%，主动推荐率42%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/258fa79753bd93f1b447fd8a3f42d1b1.jpeg" /></p><p></p><p>现场刘庆峰介绍，讯飞晓医APP上线的“个人数字健康空间”，它能够根据电子病历、检查报告、体检报告等用户个性化资料，构建个人数字健康空间，在看病前可以进一步剖析病症原因，用药时给出药物禁忌的个性判断，在检查后联合对比给出数据变化，并通过角色切换，了解其他家庭成员的健康状况。</p><p></p><p>目前讯飞晓医APP已通过数据安全与隐私保护的多类权威认证，进一步保障健康数据的安全。在当前医疗资源相对匮乏的情况下，讯飞晓医 APP 的出现有效缓解了社会对医疗服务的迫切需求，为个人及家庭健康管理提供了新的模式。</p><p></p><p>老师最强辅助！星火智能批阅机让老师作业批改负担下降90%</p><p></p><p>得益于底座大模型的升级和面向教育复杂场景的图文识别效果进一步提升，科大讯飞发布首款星火智能批阅机，它集智能批改、精准学情、个性学习于一体，它支持自由排版，不限纸张大小的作业，在支持多学科多题型智能批改的同时，还能即时生成多维学情报告，还为老师作业讲评和面批辅导提供了素材。刘聪在现场演示了星火智能批阅机批改作业的全流程，15份学生作业半分钟就能批改完成，批改模拟了真人笔迹，和老师平时批改作业几乎一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/681f781d2fc32092db4af99cabf0d5cb.jpeg" /></p><p></p><p>有了星火智能批阅机，老师多了一个减负增效、因材施教的AI助手，原先要90分钟才能批改完的作业，现在只要5分钟就能完成；人工分析学情要60分钟，现在星火1分钟就能完成；得益于个性化作业，学生的错题解决率也从50%提升到73%。</p><p></p><p>在今年中高考评测中，讯飞星火被外界评为“更会做题的大模型”。本次讯飞星火进一步升级了讯飞AI学习机的 AI 1对1 答疑辅导功能，既能进行多模态启发式讲解、自由问个性化解答，也可以进行互动探究式学习、超拟人引导式伴学等，让孩子多了一位“AI辅学老师”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/649e342561579e53035f39173322eb74.jpeg" /></p><p></p><p>数据显示，相比较传统解题视频学习，AI答疑辅导的学习方式让孩子的学习完成率提高到90%，错题解决率提升到93%，孩子更愿意主动思考，学习效率更高，自信心也增强了。</p><p></p><p>星火语音大模型发布74个语种方言“自由对话”，破解强干扰场景下语音识别难题</p><p></p><p>近期科大讯飞作为第一完成单位的《多语种智能语音关键技术及产业化》项目，获得国家科学技术进步奖一等奖。发布会现场国奖得主再出“王炸”，星火语音大模型迎来新突破。</p><p></p><p>刘庆峰认为，语音将成为万物互联时代人机交互的主要方式，人机交互最重要的场景是远场、噪声、多人说话、多语言，因此万物互联时代的AIUI（人工智能用户界面）要满足远场高噪声、多语言多方言、全双工、多模态等标准。科大讯飞也主导制定了全双工语音交互ISO/IEC国际标准，并于2023年5月发布。</p><p></p><p>面向万物互联时代，本次星火语音大模型发布国际领先的多语种多方言免切换语音识别能力，可支持37个语种、37种方言“自由对话”。其中，37个语种识别效果领先OpenAI whisper-V3，37个方言识别效果平均提升30%。现场，科大讯飞演示了讯飞输入法混合方言和外语的语音输入效果，能让输入效率大大提高。</p><p></p><p>科大讯飞还发布了软硬件一体化讯飞同传系统，可支持大会同传、会议同传、展厅同传、旅游同传等多场景使用。本次参会的嘉宾座位上同样配备讯飞同传的收听设备，佩戴后即可实时收听多语种AI同声传译。</p><p></p><p>针对强干扰场景下的语音识别难题，科大讯飞突破了多人混叠场景下的极复杂场景语音转写，即使在三人混叠说话场景也能实现86%的语音识别准确率。三位讯飞研究院的研究员现场实测了在噪音场景下，同时混叠着说话，正常人耳已难以听清，只见讯飞星火的多模态能力不但实现了三人重叠语音的角色分离，还能实时转写出每个人说的话，炸裂的效果引发现场掌声不断。未来基于多模态的声音识别技术，将应用在讯飞听见智慧办公、智慧屏等会议办公产品中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7b1dd3c5ccfad777f6392ea965c0dac.jpeg" /></p><p></p><p>大模型正在推动人机交互变革，语音领域的所有应用都值得被重构。在大模型加持下，星火汽车智能座舱全新升级，不但具备了多语种多方言的“自由交互”，还具备多情感多模态的超拟人交互，让人车交互更有温度。当前，讯飞语音交互产品国内市占率稳居第一，同时广泛出口到世界各地。星火大模型为一汽、奇瑞、广汽、江淮、长城等车企的众多车型，赋予了高度智能的交互体验。</p><p></p><p>为了让大模型更好落地，科大讯飞还打造了云边端一体化和软硬件一体化的解决方案，赋能家电、运营商、机器人等更多行业场景。面向具身智能和人形机器人企业需求，本次科大讯飞正式发布机器人超脑平台2.0，业内首个支持多模态交互。目前，400+机器人企业已经采用讯飞机器人超脑平台。</p><p></p><p>星火企业智能体平台正式发布，打造每个岗位专属AI助手</p><p></p><p>自去年5月6日发布以来，讯飞星火大模型正成为国家能源集团、中国石油、中国移动、中国人保、太平洋保险、交通银行、奇瑞汽车、中国一汽、大众汽车、江汽集团、海尔集团、美的集团等多领域头部企业的首选。</p><p></p><p>讯飞星火已经在代码、合规审查、客服、评标、智能交互等多个典型场景产生应用成效，以交通银行为例，基于星火大模型能力的产品iFlyCode覆盖6000+研发人员，代码采纳率达38%，工作效率显著提升。</p><p></p><p>如何更好地解决企业大模型应用的最后一公里问题？刘庆峰谈到，企业首先要科学地认识大模型能力的边界，根据任务难度选择合适方案，并且用更少的算力、更高的效率，打造企业专属大模型。随着星火V4.0的发布，他认为用智能体平台打造每个岗位的专属助手的时间已经到了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3032b1d57a923a504694e4282fbda152.jpeg" /></p><p></p><p>现场星火企业智能体平台正式发布。围绕搭建智能体的三大关键能力，当前企业智能体平台已覆盖400+AI原子能力，集成90+外部信源，打通100+内部IT系统，可供企业结合业务场景快速构建可落地的智能体应用。平台还围绕生产域、科创域、办公域、管理域上线32个企业智能体，供企业即插即用。</p><p></p><p>基于企业智能体平台，科大讯飞打造了星火商机助手、星火评标助手等典型应用案例，为企业应用打了个样。</p><p></p><p>在代码智能体iFlyCode中，它集成了代码生成助手、架构设计助手、代码问答助手、测试助手、数据库优化助手、代码审核助手等六大场景智能体，将采纳率由30%提升至52%，大幅度提升企业智能体的实用性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b584b8ca75d110a4adb6772d85f40e7f.jpeg" /></p><p></p><p>星火商机助手可以实现商机线索应知尽知、客户拜访提质增效、销售管理智能研判，助力一线销售和商机管理效能提升。星火评标助手通过标前寻源、智能评标、定标审核等功能，智能评标结果人机一致率达98%，投标异常检出率超过80%，在大幅提升企业评标效率同时降低采购成本。</p><p></p><p>星火开发者生态加速增长：5个月开发者增长超100万，总开发者数破700万</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/604ebae63bbd8eae1df5d07e21bfb553.jpeg" /></p><p></p><p>讯飞星火大模型带来行业赋能的同时，也在助力开发者生态蓬勃发展。自今年1月30日讯飞星火V3.5发布以来，短短5个月，星火开发者生态加速增长，开发者数从598万增长到702万，新增超104万；海外开发者数超40万；大模型开发者达57万。越来越多开发者正加入星火生态，释放更多刚需场景的应用价值。</p><p></p><p>刘庆峰说，只有自主可控的繁荣生态，才有中国通用人工智能的大未来。面向未来的人工智能新生态，他强调要关注源头技术生态、智能体生态、应用生态和行业生态，实现自主可控和软硬一体，才能实现大模型的深度落地；既要科学理性地认识中美在大模型上的综合差距，也要有信心快速追赶，给出从源头技术、到产业生态、再到应用落地的一整套的打法，以长期主义来打造真正自主可控的AI产业生态。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w155KHeD4mR860s7cPEz</id>
            <title>从AI高管到犀利CEO，贾扬清创业这一年：我们的目标是做AI时代的“第一朵云”</title>
            <link>https://www.infoq.cn/article/w155KHeD4mR860s7cPEz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w155KHeD4mR860s7cPEz</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 10:22:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI Cloud, 大模型, Lepton AI, AI infra
<br>
<br>
总结: 贾扬清带领团队创立Lepton AI，旨在成为AI Cloud领域的领导者，专注于提供大模型训练、部署和应用所需的基础设施，解决AI infra层的核心问题，致力于提供高性能、便宜和优质的服务。通过整合各种云资源供应链，Lepton AI帮助用户找到速度和成本之间的平衡点，致力于构建新的AI时代基础设施。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜贾扬清，Lepton AI 联合创始人兼 CEO作者｜褚杏娟</blockquote><p></p><p></p><p>“我们的目标是在未来几年里，成为 AI Cloud 领域的领导者，就像最初的 AWS、Azure、Google Cloud、阿里云，以及 Data Cloud 领域的 Databricks 和 Snowflake。”这是在离开阿里创业一年多后，贾扬清现阶段的目标。</p><p></p><p>虽然贾扬清此前否认了这次创业是因为大模型，但他承认，ChatGPT 的问世，让 AI 成为一种更新、更大，且在某种程度上更加基础的计算方式，而这也是他看中的机会。</p><p></p><p>“当前人工智能对计算的需求，推动了高性能 AI 计算、异构计算及现代云原生软件的结合。”这是贾扬清选择创业赛道的逻辑。在他看来，新的平台需要有 GPU 高性能计算能力，也需要很强的云服务，而这些都是贾扬清团队擅长的事情。</p><p></p><p>贾扬清的经历带有明显的云和 AI 印记：自己创建过深度学习框架，又曾负责过阿里云计算平台。而和他一起创业的李响是 Kubernetes 底层核心数据库 etcd 的创始人，白俊杰是神经网络标准 ONNX 创始人，并在阿里领导过全栈 AI 工程团队。</p><p></p><p>“在这个领域，我们最大的优势是‘见过猪跑和养过猪’。”贾扬清说道。</p><p></p><h3>创业的杀手锏</h3><p></p><p></p><p></p><blockquote>“Lepton AI 是一个颇具意义的名字。”</blockquote><p></p><p></p><p>贾扬清选择创业的时间，正好处于大模型逐渐标准化、但人才相对不足的节点。</p><p></p><p>在贾扬清看来，大模型会走与数据库相似的路：标准化的底层 SQL 执行不再需要用户自己优化了，同时数据库 SQL 之上会创建各种 BI 工具。对应的，随着大模型逐渐将形态收敛到几个相对确定的设计模式上，像 Llama、Mixtral、DeepSeek，系统性优化不再像之前那样困难。同样，这个领域也会出现更偏应用的 AI 中间件。</p><p></p><p>但与成熟的数据库路径不同的是，当下 AI 领域可能过早地将编程模式标准化了。毕竟一些概念现在不确定、也没共识，讨论如何为实现这些算法进行抽象显得操之过急。</p><p></p><p>贾扬清以观察到的 Lang Chain 为例说明了这个问题。用户在实验阶段非常喜欢用 Lang Chain，但做定制化功能时就会自己去写代码，因为 Lang Chain 的设计有时过于固化，而自己写代码涉及的 prompt cash 等并不复杂。这意味着，AI 中间件的产品在早期获得关注度以后，还需要进一步随着需求进化。</p><p></p><p>回到 AI infra 赛道里，贾扬清把 Lepton AI 定义为一个既小巧又敏捷的公司，运作方式上也更偏“硅谷风”：跟硅谷的许多创业公司一样，团队规模不大。目前，Lepton AI 不到 20 人，主要由工程师和产品经理组成。</p><p></p><p>创业初期，贾扬清几个创始人见投资人时，PPT 上还没有具体的公司名字，而是写着“new company”，贾扬清笑道，“感觉我们似乎不够认真”。于是他们决定取一个与 AI 相关的名字，但那时与 AI 相关的名字早都被注册了，所以他们开始想其他可用且听起来比较专业、有科学范儿的名字。</p><p></p><p>“Lepton”本意是物理学中的轻子，一种基本粒子。电子是一种轻子，中微子是一种轻子。“这体现了我们想要做的事情：首先是为 AI 时代构建新的基础设施，比如吴恩达教授说过，AI is the new electricity；其次是以一种轻量级、用户友好且成本低廉的方式构建。”贾扬清还直接道，“另外，lepton.ai 的域名也正好没有注册，所以我们就把它买下来了。”</p><p></p><p>“我们与大模型公司在一定程度上是互补的。”Lepton AI 没有训练自己的大模型，而是提供大模型训练、部署和应用时所需的基础设施。贾扬清团队要解决 AI infra 层的三个核心问题：快速、便宜和优质。</p><p></p><p>具体来说，提供高性能的大语言模型推理引擎，用于图片、视频生成；其次，为了实现成本效益，建立一个多云平台，整合各种云资源供应链，让用户找到性价比最高的 GPU 资源；同时，解决平台上不同云之间迁移和抽象化的成本问题，让用户可以自由交互开发；最后，提供很好的稳定性和运维服务，保障用户体验。</p><p></p><p>从技术层面如何实现呢？贾扬清表示团队没用什么“黑科技”，而是将很多大家耳熟能详的单点技术结合起来，从而显著降低成本。例如，大模型处理服务收到大量请求时的动态批处理（Dynamic Batch）、用小模型预测数个 token 的预测解码等。如何实现这些单点技术，并有机地将它们组合起来，找到速度和成本之间的平衡，是工作中的难点。</p><p></p><p>而在 AI 加速这件事上，贾扬清说道，“我比 10 年前更加乐观。”</p><p></p><p>实际上，硬件和软件之间的开发周期很难对齐，加上模型的多样性，专有硬件的可编程性通常比通用硬件要差，因此专用硬件很难全面支持各种模型创新的需求。而随着大语言模型架构开始标准化，异构芯片迎来新的机遇，这个市场也被激活：最大玩家英伟达布局 GPU；AMD 推出了与 CUDA 兼容的加速器；Grok 发布新的 AI 处理器 LPU 来挑战 GPU……</p><p></p><p>当前企业进行大量模型训练时通常有两种选择：一是自己组建一个至少 10 人的基础设施团队；二是选择 AI 云服务提供商，如 Lepton AI。</p><p></p><p>贾扬清算了这样一笔账：招聘人员并建立内部架构需要时间，而训练任务涉及许多复杂问题，比如网络中断、GPU 故障或存储速度不足等，算法工程师可能都没有处理这些问题的经验，并且这也不是他们的主要职责。这种情况下，选择与厂商合作既可以节省人力成本，用户也能根据自己的需求直接选择不同的底层资源。</p><p></p><p>“很多用户都在速度和成本之间寻求平衡，而我们可以帮助他们找到这个平衡点。”贾扬清说道。</p><p></p><p></p><h3>怎么讲好故事</h3><p></p><p></p><p></p><blockquote>“我们没有在这些产品上施加营收压力，可以专注提供卓越的用户体验。”</blockquote><p></p><p></p><p>很多人觉得做 AI infra 没有做大模型性感，贾扬清这群人却义无反顾投入到了这件“无聊但至关重要”的事情上。只是，做 infra 是很难向用户讲好故事的。</p><p></p><p>贾扬清也深知这一点，“如果跟客户说我们是一家 AI 基础设施公司，他们可能不太了解具体是什么。”</p><p></p><p>为了更好地讲清楚自己在做的事情，贾扬清和团队亲自下场了。</p><p></p><p>去年春节，贾扬清和公司前端负责人开发了一个浏览器插件 Elmo Chat，可以为用户迅速总结浏览网页的主要内容。今年 1 月，贾扬清团队用 500 行 Python 代码实现了一个大模型加持的对话式搜索引擎 Lepton Search，甚至还引起了一场与 Perplexity 创始人的“口水战”。</p><p></p><p>这些都是 Lepton AI 了解市场和用户、做品牌建设的方式，同时也让团队更加了解了端到端构建应用时的效率问题和核心痛点。</p><p></p><p>“通过这些产品，我们可以展示自己在开源模型上能做的事情，以及 Lepton 平台帮助用户构建应用的能力。”贾扬清说道。团队希望通过这些产品或 demo 可以在用户中形成好的口碑，当有人需要部署大模型时就会想到 Lepton AI。</p><p></p><p>而对 Lepton AI 来说，构建这些产品的成本非常低，并且从品牌建设和做真正实用的产品角度看，这是非常高效的方式。</p><p></p><p>在推向市场的过程中，贾扬清主要关注产品质量，他会同工程师和产品经理团队直接面向客户，更好地打磨产品。</p><p></p><p>目前，Lepton AI 整个团队主要在海外，所以目标客户主要为海外企业和国内想要拓展海外市场的企业。得益于云架构的成熟和标准化，Lepton AI 支持在全球范围内部署，与各个云服务无缝衔接的同时，还能很好地利用全球的计算资源。</p><p></p><p>产品驱动增长的策略也让 Lepton AI 主动放弃了一些潜在客户，特别是需要大量定制化服务的客户。“B 端的部分客户会希望提供商有更多人力投入到自己的项目中，但我们根据目前的成长阶段，会优先考虑与自己产品契合的客户。这是一种主动的选择，我们没有选择那些需要大量人力投入的业务和客户。”贾扬清说道。</p><p></p><p>不过贾扬清透露，Lepton AI 目前的客户数量和整体营收都处于非常健康的增长状态，他对此也比较满意，“这验证了我们之前的想法和产品在用户中的接受度。”</p><p></p><p>最近，贾扬清团队发布了基于 Lepton 平台的云 GPU 解决方案 FastGPU，主打经济高效和可靠，“限时以每小时 0.65 美元的价格提供 RTX 4090 GPU”。发布后就有人给贾扬清留言：缺货。</p><p></p><p></p><h3>一个更好的 CEO</h3><p></p><p></p><p></p><blockquote>“还在学习做一个更优秀的 CEO。”</blockquote><p></p><p></p><p>创业期间，贾扬清越来越乐于向公众表达自己的态度，或者也可以说释放自己的个性。</p><p></p><p>“在阿里时，同事对我的评价就是直率。技术领域通常没有太多花哨的技巧。因此，我倾向于以事实驱动的方式来表达观点，但这样可能会显得比较直接。”贾扬清说道，“毕竟我们现在还是一家小公司，会更加无所顾忌一些。”</p><p></p><p>他不认为这种直率的表达有什么不好。“在大公司，我们不想给人一种过于冲动或折腾的形象。但在小公司，直白地表达有时并不是坏事，甚至有时为了吸引注意也是必要的。”</p><p></p><p>就像他会承认说“所有基准测试都是错的”这样的话比较激进和吸引注意，但实际上这是他对榜单变成市场宣传工具的不满。“我更希望榜单成为一个所谓的入门资格认证，而不是奥运会金牌。”</p><p></p><p>虽然顶着各种光环，但创业初期也难免会被质疑。</p><p></p><p>有人会怀疑小公司的服务是否可靠和稳定。贾扬清分享了个小案例，有客户使用 Lepton 提供的推理服务，推理服务流量下降时系统发出了警报，团队检查后发现是客户侧代码的问题，提醒客户后他们非常满意。对于创业团队来说，“行不行”得看实际表现。</p><p></p><p>任何人的创业都不是一帆风顺的，贾扬清逐渐学会了在各种不确定过程中，做出一些重大决策。</p><p></p><p>就像在去年下半年全球 GPU 供应非常紧张，公司面临的选择是：要么囤积 GPU 进行交易，要么利用这段时间专注提高产品的成熟度来支持现有客户，并寻找那些拥有 GPU 资源但需要更高效平台和引擎的客户。</p><p></p><p>贾扬清选择了后者，而随着 GPU 供应情况的好转，也证明了他当时的判断是正确的。但事后看来，这仍然是一个相对冒险的选择，如果市场持续是卖方市场，公司就会陷入困境。</p><p></p><p>“作为一个创业公司，我们经常面临不确定性，还要在当时做出决策。一方面，我们需要不断观察市场，另一方面则要坚决执行决策，这也是提升我们自身效率的关键点。”贾扬清说道。</p><p></p><p>在一次次决策的制定中，贾扬清正在逐渐适应并把自己 CEO 的角色扮演得很好。</p><p></p><p></p><h3>多样的硅谷、同质的国内</h3><p></p><p></p><p></p><blockquote>“人们总是能找到证明自己是第一的方法。”</blockquote><p></p><p></p><p>国外创业一年多，贾扬清也亲身经历了硅谷对大模型的狂热。“硅谷现在竞争依然激烈。”贾扬清说道。</p><p></p><p>根据贾扬清观察，硅谷的企业和研究者现在主要关注两件事情：</p><p></p><p>一是如何实现产品的实际落地。尽管许多演示案例还停留在卖概念的阶段，但现在整个供应链已经非常充足，接下来的关键就是如何让技术真正被用户采用，尤其企业服务领域还有很长的路要走。</p><p></p><p>二是对基础模型的研究。越来越多的人开始思考 Transformer 和 Scaling Law 的边界在哪里。GPT-5 尚未发布，大家不确定 Transformer 架构的天花板是否到来，但已经有人探索 Transformer 之外的路径和方法，如 RWKV（Receptance Weighted Key Value）和 Mamba。同时，也有许多人尝试通过数据工程和强化学习等手段，从 Transformer 架构中挖掘更多潜力。</p><p></p><p>“硅谷的情况我觉得比较有趣。”在贾扬清看来，硅谷的产品更加多元化。“硅谷并没有很多公司做类似 ChatGPT 接口的事情，而是在寻找不同垂直领域的方向。比如做 AI 搜索的 Perplexity，其场景和产品形态就有明显不同。</p><p></p><p>贾扬清也不掩对 OpenAI 的称赞，“我认为 OpenAI 是一个非常成功的公司，无论是 To C 还是 To B 市场，它在营业数据和用户心智方面都做得非常出色。”据他了解，OpenAI 已经实现了产品市场契合，尽管仍在大量投资研发，但其在产品化方面实际上是盈利的，例如企业服务和 ChatGPT 等。</p><p></p><p>“这为整个行业带来了信心，表明这不仅仅是一个烧钱的游戏，而是真的能够赚到钱。现在，就要看各家公司是否能够成功走出一条商业化的道路。”贾扬清说道。</p><p></p><p></p><h4>国内：好坏参半</h4><p></p><p></p><p>对于国内的大模型市场，贾扬清直接指出，“我们可能还处于一个相对追赶的水平。”</p><p></p><p>“国内的 AI 领域涌现出来很多非常优秀的公司，同时大家也在受到产品同质化的困扰。例如以聊天为中心的产品，用户可能会感到眼花缭乱，因为这些产品看起来太相似了，包括我自己在内，都不知道如何选择，因为它们看起来都差不多，而用户一般也不想一个个去尝试。”</p><p></p><p>不过，贾扬清也肯定了国内的大模型企业在数据和用户量方面的重要优势。</p><p></p><p>“在国内，我们有时开玩笑说 100 万用户不算什么，但实际上这已经是一个非常庞大的用户基数了。”贾扬清说道。另外，国内用户也非常愿意尝试新事物，这为企业提供了快速验证产品有效性和市场接受度的机会。</p><p></p><p>贾扬清对国内的期待是，能有更多的产品经理投入到 AI 领域，思考新的产品形态或研究如何将 AI 更好地嵌入到现有产品形态中。他表示，现在许多应用并非由 AI 专家开发，而是产品经理推动的，而未来产品是否好用会逐渐成为用户采纳的决定性因素。</p><p></p><p>如今许多技术团队都面临着如何有效应用 AI 技术的问题，即使 ChatGPT 也无法精准解决实际业务问题，因此找到合适的业务场景并不容易，To C 的文生图、聊天机器人等已经成熟，To B 领域却还处于不甚清晰的状态。</p><p></p><p>对此贾扬清给出的建议是，企业自己的工程师要能轻松使用 AI 技术，同时积极与不同领域的工程师讨论各种概念及 AI 的能力边界。“与其花费大量精力去训练一个更强大的模型，更重要的是培养自己的团队，使他们能够快速理解 AI 的边界，并将这些边界与自身的应用需求有效结合。”</p><p></p><p>“应用大模型的企业最关心的不是价格，而是 AI 能否真正解决问题。”贾扬清提到，从这个角度看，国内愈演愈烈的大模型价格战意义并不大。不过，大模型价格下降也是趋势。“IT 领域的价格指数级下降是一个持续性的规律，我不认为大模型会是不受限制的一个领域。”</p><p></p><p>但小公司往往无法承受价格战和补贴战的资金消耗，为此，贾扬清建议小公司去精准支持和服务企业，尤其满足垂直领域的需求，这样也会找到自己独特的发展空间。</p><p></p><p>国内大模型还面临一个问题，就是常常被质疑套壳。贾扬清认为，一定程度上，使用标准架构或所谓的“套壳”并不是问题，比如大家都使用 AlexNet 或何恺明的 ResNet。他更关注的是，国内开发者如何提高自己的工程实践能力，并与海外开发者和开源社区更紧密地合作。</p><p></p><p>结合自己之前参与开源社区的经历，贾扬清认为国内大模型社区还需要提高协作等能力。“国内开发者独立工作时表现得都很好，如果能更好地协作，就能创建更大、更健康的开源生态系统和社区。”</p><p></p><p>实际上在今年 3 月份，贾扬清认为“开源模型能迅速追上闭源模型”。现在他也认同，谷歌和百度等公司的闭源模型某些方面是会领先开源模型的，比如在全网搜索等某些企业本就擅长的领域，还有在超大规模的通用模型领域，因为这样的模型需要强大的资金和人才支持。</p><p></p><p>尽管如此，贾扬清直言自己更支持开源模型。一方面，开源可以促进更多研究，有助于找到更新、更有用的模型。另一方面，由于具有更好的定制化能力和企业自有知识产权的优势，开源模型有望迅速赶上甚至在某些方面超过闭源模型。他也提到，“有 Meta 这样资金雄厚、专业能力强的公司支持开源社区，是非常幸运的。”</p><p></p><p>对于 Lepton AI 自身也会从开源中受益的事实，贾扬清也直言不讳：“如果一切都是闭源的，我们就只能支持少数几个闭源公司，市场就不会那么大。我们希望市场能够更加多元化，越多元化越好。从市场经济原理看，竞争能够提升质量。”</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>“这一波 AI 浪潮的持续时间超出了我们所有人的预期。”贾扬清说道，“尽管 ChatGPT 会出一些问题，但我现在遇到问题或需要写文案时，还是会先让它帮忙处理。因此，我不认为 AI 行业会出现寒冬。对于一些投入明显超出收益的行为，市场会自我修正。”</p><p></p><p>在贾扬清看来，未来大模型发展的关键还是要回到商业成功上面。今天，人们已经不再怀疑 AI 是否有用，而是纠结如何让它实际产生商业价值。</p><p></p><p>但目前，AI 产品和服务还没有真正跨出自己的圈子。“当前 AI 市场，某种程度上是 AI 圈内人在自我消化需求，非 AI 用户消费 AI 服务的量还待起步。”贾扬清表示，未来 AI 技术能够被那些对 AI 一无所知的人以某种方式使用，这是它产生更大商业价值的先决条件。</p><p></p><p>现在，留给创业公司试错的空间并不多，但留给行业的问题还很多。AI 行业如何更长远地走下去，是包括贾扬清在内的每个参与者需要回答的问题。</p><p></p><p>栏目介绍</p><p></p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p></p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/S8RBeCZOXA0HZFxa4Phb</id>
            <title>一群顶尖搜索人才如何2个月出货，还把GPU利用率干到60%！揭秘百川智能研发大模型这一年</title>
            <link>https://www.infoq.cn/article/S8RBeCZOXA0HZFxa4Phb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/S8RBeCZOXA0HZFxa4Phb</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 10:17:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型之战, 系统性工程, 冷启动, 训练效率
<br>
<br>
总结: 去年的大模型之战强调快速入场、发布和迭代。百川智能团队在研发大模型时注重系统性工程，选择冷启动并关注训练效率。团队通过评估不同阶段的研发问题来提高整体研发效率，同时关注推理成本的提升。整个大模型研发过程是从经验科学到数据科学的转变，核心竞争力来自于对模型关键问题的定义。 </div>
                        <hr>
                    
                    <p></p><p>去年的大模型之战讲究一个“快”字：入场快、发布快、迭代快。</p><p></p><p>王小川在去年 4 月份宣布成立“百川智能”的两个月后，就迅速对外推出了 70 亿参数量的中英文预训练大模型 Baichuan 7B。一年多后的今天，百川智能已经将大模型迭代到了 Baichuan 4。</p><p></p><p>曾担任搜狗搜索研发总经理的陈炜鹏如今在百川智能负责模型研发，这对他来讲也是一次挑战。“搜索与模型研发有很大的不同，研发经验不一定能完全复刻，比如两者对数据的定义可能完全不一样。”</p><p></p><p>但陈炜鹏也表示，做搜索和大模型也有共性，就是它们都是系统性工程。“在大模型之前，被认为系统性工程的算法问题只有三个：搜索系统、推荐系统和广告系统。以前的搜索经验让我知道怎么样解决一个系统性的问题。”</p><p></p><p>实际上，百川智能的技术团队可以分为两部分：一部分是像陈炜鹏这样有很强系统性工程经验的人，他们做过复杂的项目，知道怎样把复杂的问题拆解成为子问题，然后做有效的科学管理；另一部分则是对语言模型本身有很好认知的研发人员。</p><p></p><p>“大模型的研发不是一个单点问题，而是一个系统问题。解决系统性问题，是我们团队的优势。”陈炜鹏说道。那百川智能（以下简称“百川”）具体是如何解答“大模型研发”这道题的呢？</p><p></p><p></p><h3>大模型冷启，分阶段评估</h3><p></p><p></p><p>回顾当初，OpenAI 的 GPT-3.5 在全球范围内爆火的时候， 国内对怎么做大模型还没有形成很好的共识。</p><p></p><p>基于 BloomZ、OPT(Open Pretrained Transformer)、Llama，还是自己设计模型结构，这其实是两种不同的大方向，不同公司的路径都不一样。百川要做的第一个决策也是要选择从零做起的冷启动，还是基于 Llama 训练的热启动。</p><p></p><p>这个选择其实对百川来说几乎不需要犹豫，答案肯定是要从头开始做起。</p><p></p><p>逻辑很简单：热启动可能遇到的诸如新数据与之前数据的配比、合并，中英文能力平衡等问题，虽然可以提升团队在数据、模型能力、训练技巧等方面的认知，但是并不能给团队带来价值更高的技术栈认知。对于百川这样的创业技术团队来说，只有自己跑通大模型的整个技术栈，掌握完整的 pipeline，才可以真正把技术沉淀下来。</p><p></p><p>冷启动是风险更高的选择，百川接下来就是要想办法把研发模型的风险降到最低。</p><p></p><p>对此，团队的想法是一个小模型的结果能不能映射到大模型上，先用小成本模型验证后再进行大量投入。比如对于数据的多样性、规模和质量哪个更重要的问题，团队就是在提出假设后先用小模型做了验证。</p><p></p><p>百川技术团队选定了某一模型结构后，做了小、中、大三种参数的模型，然后观察不同参数之间的表现是否存在线性关系，如果符合 scaling law，后续就可以用该结构继续做各种数据实验、训练框架调优等。现在看，这条路百川走通了。</p><p></p><p>为了提升整体研发的成功率，百川团队把不同阶段的研发问题转化成为了评估问题，具体来讲就是把整个模型的研发过程拆分成了不同的阶段，并在每个阶段完成后用对应的方式进行评估。</p><p></p><p>在陈炜鹏看来，拆分和评估做得好，意味着团队对整个问题的理解和定义会更好，可以给整体研发带来明确的方向和效率提升。</p><p></p><p>“如果不能给当下的研发任务进行有效评估，而是通过最后大模型的效果来证明，势必会导致整个研发链条非常长，难以及时将研发工作转化为有效认知，进而导致整个团队的认知迭代非常慢、效率非常低。”陈炜鹏解释道。</p><p></p><p>好的评价结果意味着团队掌握了这个认知。因此，百川花了很多精力去做模型能力评估。“只有知道怎么评估，才能知道要往哪走、要怎么做。”</p><p></p><p>在效果评价方面，行业里有各种各样的测评榜单。企业每次发布大模型时都会介绍自己的 Benchmark 结果、对齐结果。实际上，大模型评估也是一个动态发展、跟模型能力强关联的演变过程。</p><p></p><p>很多 Benchmark 只能阶段性地反映模型能力。去年大家关心 MMLU 这种更偏知识类的测评和侧重数学能力的 GSM8K；去年下半年至今，大模型评测更是深入到了指令跟随、工具调用、多步推理能力、逻辑自洽性，甚至是否具备时间理解能力等方面。</p><p></p><p>“我们研发期间是想跳出现在的公开评测，自己去定义指标和任务的。”陈炜鹏进一步说到，“参考外部评测主要是为了知道自己在业内大概什么水平，但更重要的是能定义自己的 Benchmark，能够自己定义评估标准代表了一个企业对大模型的理解和技术方向。”</p><p></p><p>早期，很多评测标准是由高校、头部企业定义的。比如 OpenAI 提出的 GSM8K，就与其对模型能力的定义和想象有关。当 OpenAI 公布自己的测评标准时，自己在内部已经跑通了一段时间，一定程度上这也意味着企业已经有了超越行业的认知。而头部企业对大模型能力的理解也会在业内达成共识。</p><p></p><p></p><h3>大模型训练，从效率到成本</h3><p></p><p></p><p>“整个大模型研发，其实是一个从经验科学到数据科学的过程。”陈炜鹏说道。</p><p></p><p>大模型训练上，业内比较依赖 Megatron-LM、DeepSpeed 等分布式训练框架，这方面大家是相似的。而不同公司大模型训练的的差异在于训练框架解决不了的训练效率、稳定性和容错等问题。</p><p></p><p>训练效率或者推理效率是一种支撑性的技术。提升训练效率主要是提升整个机器的利用率，业内已经做了很多工作，比如并行策略、调优等。</p><p></p><p>训练效率不同的公司千卡利用率是不同的，百川千卡集群的利用率目前在 60% 以上。而大模型里涉及到很多 pipeline 和不确定性，使用工具做好过程管理非常重要。当集群出现故障时，需要及时发现并恢复。诸如此类才是大模型厂商技术比拼的点。</p><p></p><p>当然一些工具很大程度上可以起到提效作用，但真正的核心竞争力来自于认知，认知的差异则来自厂商对整个模型关键问题的定义。</p><p></p><p>比如重点研究多模态的企业，就会重点研发语言能力与不同模态怎么做结合等。因为从语言模型走到多模态模型的不确定性是显著增加的，而整个行业对如何做统一建模并没有确定的答案，需要企业做大量的实验。</p><p></p><p>与此同时，这一年多以来，大模型训练的重点也在发生变化。</p><p></p><p>去年的时候，行业更关心训练效率，对于推理成本没有特别多关注。“我觉得，去年整个竞争并没有非常激烈，因为当时模型的效果是最大的障碍，这种情况下，大家并没有非常关注推理成本。”</p><p></p><p>到了今年，业内显然开始更加关注推理成本。核心的原因是当前的模型能力已经在很多场景中具备较好可用性。这种情况下，当大模型开始落地时，大家的焦点自然就会转移到成本上。</p><p></p><p>百川团队现在也在探索如何在相同的推理成本下提升模型能力上限。比如对齐阶段遇到的能力平衡问题，研发团队要做的是围绕不同的能力方向，训练好几个模型，然后再把多个模型整合成一个模型。在选择哪个模型回答问题上，百川没有使用粗暴投票的方式，因为这会显著增加推理成本。</p><p></p><p>整个大模型推理加速优化上，Infra 层很难有数量级的优化，这个可能性几乎不存在，所以很多优化都是算法层面的优化。在这些优化措施中，效果加速度最大的方式是在模型结构不变的情况下提升模型的能力上限，其次是改变模型结构，获得与之前差不多的效果，但成本比之前更低；最后则是算子层或框架层的优化。</p><p></p><p>这与之前机器学习成本优化方面的规律一样，算法提升带来的成本下降比工程层面的要更显著，但技术实现也更难。</p><p></p><p>提升模型本身的能力是降低推理成本效率最高的方式，比如以前用千亿的模型，可能未来百亿的模型就能得到千亿模型相同的效果。较小参数规模的模型能够媲美更大参数模型的原因在于对数据质量的提升，比如 1 篇文章能讲清楚 10 篇文章论述的事情，就是更高质量的数据。</p><p></p><p>大模型训练是基于现在看到的数据分布建模，而所有数据内容是我们对整个世界的投射，也可以说是对整个世界“打点”，打的这个“点”存在大量重复的内容，如果能够找到一种方式，用最少的数据把整个世界描述清楚，那效率一定是更高的。</p><p></p><p>现在采样数据还是用已有的知识描述整个世界，能用最小的篇幅把整个世界描述清楚，也是合成数据的价值之一。对于合成数据可能带来的数据噪声问题，陈炜鹏认为，数据存在噪声不一定是灾难性的，正确数据的规律性比错误数据的规律性更强，大模型能够学习到这个规律，所以存在一定的抗噪能力。</p><p></p><p>“核心的问题是现在的数据构建方式并没有产生新的智能。”陈炜鹏指出。大部分数据合成的工作，都是在让小模型更接近大模型。但是很少有人提出数据合成的方法能给大模型能力带来显著提升。</p><p></p><p>合成数据只是做到这种程度的话，只能是提效。只有构建的数据能够超越现在的质量、超越现在的分布，合成数据才有可能带来智能的进一步提升。不过，合成数据能不能创造更高的智能，如今还是一个比较开放的问题，虽然重要，但大家都没有找到通用解法。</p><p></p><p>“整个大模型的发展还挺有意思的，它既是一个 infra 问题，也是一个算法问题。”陈炜鹏说道。</p><p></p><p>行业之前取得的大的进展，本质上都是在工程上突破，而不是在算法上。很大程度上，当模型结构确定后，infra 层的价值可能比算法层的价值更大。</p><p></p><p>在 scale up 假设下，大模型越来越大，国内一些企业选择万卡互联，这对 infra 层面的挑战非常大。而像语言与多模态之间结合等没有达成高度共识的实现方式上，算法还有很大的探索空间。</p><p></p><p>对于大模型更高的算力要求，陈炜鹏是比较乐观的。“现在有三股力量在解决这个问题，一是芯片层，他们自身的动力是非常强的。另外就是在 infra 层和算法层，infra 层跟芯片层配合、算法层就是在模型结构里面做一些工作。”</p><p></p><p></p><h3>迭代的本质：智力、应用</h3><p></p><p></p><p>与百川一样，市面上其他模型也都进行了多次大版本迭代，但大家在发布的时候，还是围绕各种基本能力的提升，业内的人可能能够更好理解提升数据，但行业外的人对于代际的差异比较后知后觉。</p><p></p><p>陈炜鹏对此解释道，基座模型最关注是本身的智能水平，具体表现上没有特别多可差异化的点，真正产生代差的是模型之间的智力水平。</p><p></p><p>以 GPT-4o 为例，GPT-4o 比 GPT-4V 在应用层的想象空间打开了很多，但 GPT-4o 并没有被命名为 GPT-5，因为它们的智力水平某种程度上还在同一个水平。</p><p></p><p>对于热门的长窗口、推理优化等，陈炜鹏认为，这些只能带来短时间的差异化，在半年以上的周期里，这些差异都会抹平。“整个行业里，我觉得大家某种程度上把长文本窗口这个事情‘神话’了。”陈炜鹏提到，“在我的理解里，上下文窗口大家更多的工作是工程上的，算法层的突破非常有限。”</p><p></p><p>另外，大模型厂商在基座模型的迭代期间，其实也已经考虑到了未来自家大模型可能的应用方向。</p><p></p><p>“大家既要在智力水平上拉开差距，还要在应用上找到差异。这就是守正出奇的逻辑，‘守正’就是我能不能够在智力水平上跟别人产生代差，‘出奇’就是出于对技术成熟度和产品的判断，来决定我差异化的功能是哪些。”陈炜鹏表示。</p><p></p><p>陈炜鹏举了一个比较形象的例子。大家要制造一个 super man，首先要知道它要具备什么样的能力，然后从 AI 本质出发需要怎样的底层支持，类似有没有比现在 token predict 更超前的方式等非常本质的问题。</p><p></p><p>这之后，人们会考虑 super man 除了有一个非常强大的大脑外，还需要具备哪些能力。到了这一步，大家就会有各种各样的定义。实际上，这时大家已经转换到了另外一个视角，即应用层，从应用层获得各种对应的能力。</p><p></p><p>也就是说，相同的智力水平下能够做出什么样的产品，这与企业对应用的想象有关。比如企业重视长文本能力的应用就会在上下文窗口上投入更多。</p><p></p><p>因此，总的来看，很多大模型研发决策是 AGI 视角和应用视角交错下的产物，只是不同的公司在不同视角里的投入有所差别。</p><p></p><p>以百川为例，Baichuan 3 的定位虽然还是基座模型， 但在医疗方面做了加强。</p><p></p><p>一方面，百川团队发现模型训练过程中，语言能力、知识能力的提升是快收敛的，逻辑推理能力的提升也比较慢，且周期较长。而医疗是一个既包含知识，又包含复杂推理过程的场景，可以很好地衡量大模型能力。</p><p></p><p>另一方面，百川也很在意医疗场景里模型的表现，这个就与其对模型应用的想象有关系。模型是要面向应用的，大模型厂商认为哪些场景重要，就会希望模型这方面的能力达到业内领先，带来应用优势。</p><p></p><p>为此，百川增强了大模型在医疗这个垂直场景的能力。百川团队先是深入到这个领域里做行业理解，之后花了很多精力解决场景的数据构建和数据配比的问题。</p><p></p><p>但有一点是毋庸置疑的，就是未来信仰 scaling law 的大模型厂商，发布节奏可能不会像去年那么快了。</p><p></p><p>就像王小川说的，“如果想达到智能，从现在的路径来说我们必须 scale up ，但 scale up 不一定会带来智能。不管怎样，这个事情我们得做。”而随着模型规模的增加，整个计算的复杂性、所需的数据量、背后依赖的算力资源等都要有数量级增加，这无疑是会拉长研发周期的。</p><p></p><p>王小川在 Baichuan 4 的发布会上就表示，以后的发布不会再以月为单位，而是季度，要把时间放到长线做事情。</p><p></p><h3>结束语</h3><p></p><p></p><p>时代的浪潮终归会落到每个技术人身上，包括但不只是像陈炜鹏这样的大模型厂商里的技术负责人。</p><p></p><p>大模型时代，技术人才的画像发生了很大的变化。比如之前的产品经理对用户端的理解非常重要，但现在要做一款好的产品，就不能只关注用户端，还要对当前技术能力的边界、成熟阶段有较好的预判。</p><p></p><p>现在的大模型技术不像之前的技术那样成熟，历史的经验不一定能够非常好转化为生产力。一个人有很强的发现新问题、定义新问题、解决新问题是更重要的能力。因此，百川也会倾向招聘新人、年轻人，“因为我们本身就在做一个很新的事情、要解决新问题，所以很多过去的具体算法经验，在如今场景下并没有那么重要，研究能力才是最重要的。”</p><p></p><p>目前，百川中的技术人员占整个公司人数的 70%-80%，其中有经验丰富的前搜狗各个业务线最优秀的干将和其他知名科技公司核心 AI 人才，也有越来越多的研发新星。期待汇集了多样人才的百川未来为我们带来更多惊喜。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pk7tgkcHeyR5GTmdXimg</id>
            <title>AI 激战进入下半场，“推理”还卷得动吗？</title>
            <link>https://www.infoq.cn/article/pk7tgkcHeyR5GTmdXimg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pk7tgkcHeyR5GTmdXimg</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 07:25:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 价格战, 云计算, 大模型
<br>
<br>
总结: 一场前所未有的价格战在AI领域打响，云厂商纷纷降价，推动了云计算技术的发展和大模型的普及。随着AI推理需求增加，云服务的规模经济效应凸显，GPU云服务器成为AI基础设施的关键。同时，云服务的创新也推动了IT基础架构的变革，加速了AI技术的落地和应用。 </div>
                        <hr>
                    
                    <p>不久前，一场前所未有的价格战在 <a href="https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy">AI</a>" 领域打响，其激烈程度堪比一场商业风暴。以阿里云、百度、腾讯为代表的头部厂商纷纷宣布大幅降价，引发了圈内巨大震动，其中阿里云的通义千问 GPT-4 级主力模型 Qwen-Long，其 API 输入价格从 0.02 元 / 千 tokens 直降至 0.0005 元 / 千 tokens，降幅高达 97%！</p><p></p><p>价格战愈演愈烈的原因有很多，但无论是什么原因，我们看到的都是，通用大模型崛起后的这场价格战，将云厂商的竞争推向高潮。从讲“服务故事”到血拼 tokens 价格，云厂商的价值在这场“降本”的变革中再次受到严峻审视。但聚焦技术本身，如果想要实现技术的可持续性发展，把握好技术革新与规模经济之间的关系才是真正的破局之法。</p><p></p><p>随着云计算技术的不断革新和规模效应的扩大，AI 服务成本显著降低，让更多企业和个人能够负担得起并采纳 AI 服务。同时，云计算飞轮的加速旋转也带来了极大丰富的计算资源，让 AI 模型能够更快、更准地完成训练和推理。</p><p></p><p>过去半年，美国湾区的推理已经迈入每秒生成千个 token 的大关，英伟达发布了号称“史上最强的 AI 芯片”，官方称推理性能提升了 30 倍；百度发布了文心大模型 4.0 的工具版，官方称该模型的推理性能提升了 105 倍，推理成本降到了原来的 1%；腾讯太极机器学习平台研发了 Angel-HCF 推理框架和 Angel-SNIP 压缩框架；Meta 公布了其定制 AI 芯片 MTIA 的最新版本，专门设计用于 AI 训练和推理工作，还在 AI 推理和规划方面进行了深入探索，逐渐接近通用人工智能（AGI）……显而易见，当大家“卷”完行业大模型的构建，比拼谁能拥有更多业务数据进行模型训练之后，“AI 推理”或成为新赛点。</p><p></p><p>根据 IDC 数据，随着人工智能进入大规模落地应用的关键时期，云端推理占算力的比例将逐步提升，“预计到 2026 年，推理占到 62.2%，训练占 37.8%。”这一预测进一步强调了 AI 推理在未来市场竞争中的核心地位。而高性能 AI 推理的背后是海量算力，这意味着 AI 基础设施将是未来市场竞争的基本盘。</p><p></p><p>据信通院发布的《新一代人工智能基础设施白皮书》数据显示，AI 领域的大模型参数量正在以惊人的速度增长，年均复合增长率达到 400%，算力需求的增长更是超出了摩尔定律的预测，达到了惊人的 15 万倍，对 AI 基础设施提出了前所未有的挑战。传统的 CPU、GPU 堆砌方案已经无法满足 AI 大模型的研发需求，加上企业对于 MaaS（大模型即服务）的需求日益增加，企业需要更高效、更灵活的基础设施来支撑 AI 应用的开发和部署。</p><p></p><p>可以说，新一代 AI 基础设施不仅要关注硬件设备的升级，更要注重软件、算法和数据服务的整合与优化，需要通过精细化的设计和重构，提升计算、存储、网络以及数据服务的性能，为 AI 应用提供更高效、更可靠的支持。</p><p></p><p></p><h2>一、云服务"规模经济"：AI 基础设施成本大降的终极利刃</h2><p></p><p></p><p>今年 3 月，开源平台 ClearML 发布的最新调研报告《2024 年 AI 基础设施规模现状：揭示未来前景、关键见解和商业基准》中显示，企业购买推理方案的关键因素是成本——为了解决 GPU 缺乏的问题，约 52% 的受访者在 2024 年积极寻找低本高效的 GPU 替代品用于推理，其中 20% 的受访者表示对低本高效的 GPU 替代品感兴趣，但还找不到替代品。这意味着，由于大多数企业尚未达到生成式 AI 的大规模生产，低本高效推理计算需求将呈现增长趋势。</p><p></p><p>在如此趋势下，越来越多的企业开始将 AI 推理迁移到按需付费的云端进行。</p><p></p><p>云计算服务市场是一个典型的“规模经济”。随着用户基数的扩大，云厂商可以通过大规模采购硬件、优化资源分配和提高运营效率来分摊固定成本，从而实现成本效益的最大化，这种成本优势让云厂商能够以更具竞争力的价格向市场提供服务。同时，规模经济效应还能加速技术创新和服务多样化，较大的用户基础为其带来了更多的数据和反馈，这有助于其更深入地理解客户需求，快速迭代产品，推出更符合市场需求的新服务和功能。</p><p></p><p>而在所有的云服务中，GPU 云服务器对 AI 基础设施建设的意义最为关键，它极大地提升了 AI 基础设施的处理能力。通过集成 GPU 云服务器，AI 基础设施能够更高效、更快速地完成训练和推理任务，从而加速 AI 项目的研发进展。这不仅能使企业抢占市场先机，还能在获得大量数据后进一步优化自身模型，积累更为丰富的数据库。</p><p></p><p>以阿里云 GPU 云服务器为例，其神龙架构支撑裸金属实例，实例内 GPU 实现全速 P2P 功能，集合通信能力提升 20%，在微调和多卡推理过程提升性能 6%。在支持包年包月和按量计费的两种低成本购买方式的情况下，阿里云 GPU 云服务器还提供了针对 AI 应用部署及优化的免费工具，实现面向训推场景的 GPU 性能优化，其在同等硬件条件下，LLM 大模型推理性能提升超 100%，LLM 大模型微调训练性能提升 50%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32377af5fdc678a4fbcc5e6f91cc2776.webp" /></p><p></p><p>去年一经上线就出圈爆火的 AI 应用“妙鸭相机”，随访问量的激增，对 GPU 服务器的算力需求激增至数千台规模。阿里云 GPU 云服务器为其提供了训推一体的解决方案，助其缩短 19% 的端到端微调时间，推理效率提升 100%。训练时间的减少，不仅意味着成本的降低，也意味着妙鸭 C 端客户更短的等待时间和更好的体验。</p><p></p><p></p><h2>二、云服务创新：AI 时代 IT 基础架构变革的雷霆引擎</h2><p></p><p></p><p>深度学习自 2012 年在 AI 领域确立其核心地位之后，尽管为应用带来了显著赋能，但很长一段时间里并未彻底改变应用研发范式。直至云服务的崛起，数字化基础设施的格局发生了根本性变化，计算、网络和存储的虚拟化使得算力成为基础服务，云原生架构的应用研发模式大幅提升了开发迭代效率。后来随着大模型技术的广泛应用，大模型以 AI 原生应用的形式深入多场景，并转化为一种通用的服务 MaaS，降低了 AI 技术的落地门槛。而作为基础设施的云服务，也在大模型发展的推动下，产生了云原生“AI 化”的转变，重塑了云计算产业格局。</p><p></p><p>这种转变不仅体现在 AI 技术作为服务（MaaS）的广泛应用上，更在基础设施层面推动了 GPU 云服务器的革命性转变。面对高速演进的 AI 技术对 GPU 资源提出的愈来愈高的要求，基于云原生“AI 化”的趋势，以确保资源能够按需分配、高效利用。当前，以容器为代表的云原生技术正在完成进一步创新，IT 系统需要更加模块化和灵活以适应 AI 应用的迭代和更新。</p><p></p><p>在 AI 应用研发场景中，当 GPU 云服务器被多个用户或应用共享时，特别是在资源需求不均或变化频繁的情况下，资源分配和调度可能不够灵活，导致 GPU 利用率低下。此时便可以使用类似于阿里云容器服务 Kubernetes 版 ACK 提供的云原生技术来解决问题。ACK 丰富的 GPU 集群弹性伸缩能力可以帮助企业灵活应对工作负载变化，根据资源使用情况，企业可以快速动态调整容器数量，数分钟内扩展至上千节点。容器所具备的环境隔离性保证了 AI 模型推理的稳定性和一致性，减少因环境差异导致的错误和冲突，可以加速模型的迭代和部署过程。</p><p></p><p>阿里云 ACK 提供“云原生 AI 套件”，企业可以充分利用云原生架构和技术，在 Kubernetes 容器平台上快速定制化构建 AI 生产系统，并为 AI/ML 应用和系统提供全栈优化。在实际 AI 推理场景下，基于标准 Kubernetes 提供的组件化能力，同时通过共享 GPU 方案，对比自建 GPU 集群算力利用率提升 100%。除此之外，通过数据加速 Fluid，AI 推理场景数据访问资源成本可以降低 10 倍左右。更值得一提的是，这套云原生 AI 套件自 6 月 6 日起全面免费，企业成本直接降为 0！</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6574b62e3c0e5ff0780d5bae63d592e.webp" /></p><p></p><p>除了云原生架构的迭代创新，数据作为 AI 技术的“食粮”，其存储架构也在发生变革。随着数据量的激增，传统的存储解决方案已经无法满足 AI 对于高吞吐量和低延迟访问的需求。因此，可以在单个全局命名空间中无限扩展到数十 PB 甚至更多、可以为 AI 工作负载提供理想的存储解决方案——对象存储技术被广泛应用并持续迭代。</p><p></p><p>在目前的 AI 推理场景中，大家常会遇到的问题是，模型推理需要拉取加载模型文件，在调试过程中还需要不断切换新的模型文件进行尝试，而且随着模型文件的不断增大，推理服务器拉取模型文件所需时间越来越长。</p><p></p><p>面对这个挑战，许多企业将阿里云对象存储 OSS 作为解决方案。对比传统存储，OSS 的吞吐能力超过 10Tbps，从 OSS 下载 270GB 模型文件用时降低至 21s，通过低延时高吞吐的方式快速把模型文件传输到容器节点，减少 GPU 等待时间，可大大提升推理效率。此外，阿里云 OSS 加速器在 AI 推理环节支持 SD、Transformers 等多种推理框架，性能最高可 burst 至 40GB/s。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80a1ff2f0993dd54328b5b321ddd9d5f.webp" /></p><p></p><p>可以说，大模型的发展标志着 AI 技术进入了一个全新的阶段，它不仅仅是对以往 AI 技术迭代的延续，更是对底层 IT 基础设施和上层应用开发模式的一次深刻重构。云服务作为 IT 基础架构的核心部分，必须承担起引领创新变革的重任。</p><p></p><p></p><h2>三、生态协同：云计算与 AI 深度融合的超级加速器</h2><p></p><p></p><p>如今，大模型已经开始卷价格，对比云计算用了 16 年才开始卷价格，AI 市场厮杀的激烈程度不言而喻，甚至 AI 已经让卷到"很卷"的云计算变得“更卷”。</p><p></p><p>于此，云厂商不仅需要有强大的技术研发能力，更需要构建一个健康、活跃的生态，以实现资源的优化配置和价值的最大化，而创新就是云计算飞轮持续旋转的核心动力。AI 借助云计算的强大算力处理海量数据，实现智能化应用；云计算则为 AI 提供稳定的技术底座，促进技术再升级。两者形成的良好技术生态共同助力着全产业智能化发展，吸引着更多开发者、企业参与技术创新。</p><p></p><p>通过生态协同，云厂商能够与上下游企业共同产品和服务的持续创新；通过与合作伙伴的深度合作，实际业务场景下的需求正在驱动着云厂商技术迭代与创新。</p><p></p><p>这种繁荣的生态系统为<a href="https://xie.infoq.cn/article/73bcc133affb775ea3afa5138?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阿里云</a>"带来了更多的创新服务和应用，从而铺建了其在行业里的领先地位。通过合作伙伴的支持，阿里云为客户提供更加丰富多样的云计算产品，其“先进、稳定、易用、高性价比”的优势也助力许多企业客户获得了业务成功。这个过程中，阿里云积累了丰富的市场经验，同时拥有了庞大的计算资源和海量数据，为 AI 大模型的研发提供了坚实的后盾，从而走在了大模型厂商前列。</p><p></p><p>阿里云在 AI 大模型研发与云计算领域的双重领先优势，让其在 AI 基础设施构建方面拥有了得天独厚的条件。不仅为 AI 基础设施的构建提供了坚实的基础，更在不断地将这一优势转化为实际的产品和服务。而且，阿里云非常清楚——除了技术具有前沿性外，如何将这些技术有效地应用到实际场景中以解决实际业务问题，同样至关重要。</p><p></p><p>于是，基于深厚的 AI 技术实力和深刻的市场洞察，阿里云正在持续为企业提供既领先又容易落地的 AI 基础设施解决方案。为了帮助企业和开发者在多达数百款云产品中，根据自身业务问题快速定位关键产品需求，阿里云还推出了明星云产品推荐计划“飞天星品”，（点击本文文末的"阅读原文"可查看飞天星品的页面详情）大家可以在“飞天星品”上解决云产品选型难、使用方式复杂、场景定位模糊等问题，轻松选到最好用、最高性价比、最适合自己的云产品。</p><p></p><p>不仅如此，今年 618 阿里云首度推出 5 亿算力补贴，并带来多项 200 余种热门云产品折上折活动，助力更多企业、创业者与开发者可以使用普惠算力，更好地上云创新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/611f589d69158529457713d37dd9c866.webp" /></p><p>登录阿里云官网，获取算力补贴</p><p></p><p>展望未来，云计算和 AI 技术的融合将进一步加速，共同推动数字化转型的浪潮。云计算的飞轮已经加速旋转，它带来的不仅仅是成本的降低和效率的提升，更是业务模式的创新和生态的构建，AI 技术也因此将得到更加广泛的应用和普及。我们期待看到更多的企业能够利用阿里云产品和服务，实现业务的快速增长和创新发展，共同推动 AI 技术的更快发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kw6Sq83j1hwhE46tFULN</id>
            <title>美的集团在“AI+”战略下的布局与最新实践进展</title>
            <link>https://www.infoq.cn/article/kw6Sq83j1hwhE46tFULN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kw6Sq83j1hwhE46tFULN</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 06:41:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化感知技术, AI+, 美的中央研究院, 技术创新
<br>
<br>
总结: 美的中央研究院以“AI+”战略为核心，通过智能化感知技术与人工智能的深度融合，推动了多个行业的技术革新和应用变革。通过在“AI+ 工业机器人”、“AI+ 智能制造”、“AI+ 智能家居”、“AI+ 医疗影像”四个主要方向的布局，力图提升产品力和竞争力，服务于多元化经营的目标。通过技术创新，美的中央研究院致力于实现智能化解决方案，推动智能化感知技术在机器人、医疗及智能家居领域的应用研究和技术创新。 </div>
                        <hr>
                    
                    <p></p><p>受访嘉宾 | 奚伟 美的中央研究院智能技术与应用研究所所长</p><p></p><p>智能化感知技术与人工智能的深度融合，推动了众多行业的技术革新和应用变革。在这一背景下，美的中央研究院以“AI+”战略为核心，通过在“AI+ 工业机器人”、“AI+ 智能制造”、“AI+ 智能家居”、“AI+ 医疗影像”四个主要方向的布局，力图通过技术创新，提升美的产品力和竞争力，服务于多元化经营的目标。</p><p></p><p>智能化感知技术是通过传感器和数据处理装置实现对环境、物体和人类行为的感知、识别和分析的技术手段。AI 则利用这些感知数据进行深度分析和决策，形成智能化的解决方案。</p><p></p><p>在美的中央研究院的“AI+”战略中，智能技术与应用研究所起到了至关重要的技术支撑作用。本文中，InfoQ 通过与美的中央研究院智能技术与应用研究所所长奚伟的交流，探讨了美的“AI+”战略布局及智能化感知技术应用的最新进展。</p><p></p><p>据介绍，美的通过开展新型传感器、多模态智能感控技术，以及具身智能大模型技术的研究，为下一代工业机器人，先进医疗影像设备，和未来家电等前沿方向提供关键核心感知部件支撑和差异化感知平台技术方案。这些技术不仅增强了产品的智能化水平，还为未来的应用场景奠定了坚实的基础。</p><p></p><p>同时，随着技术的快速发展，美的在推广和应用过程中也面临着诸多挑战，如技术的成本效益、市场竞争的压力以及人才的培养和团队的建设等。如何在这些挑战中找到平衡点，也是美的未来需要持续探索和解决的问题。</p><p></p><h2>爆发期下的智能化感知技术：美的如何布局“AI+”战略</h2><p></p><p></p><p>InfoQ：您能介绍一下美的中央研究院在“AI+”战略上的整体布局和目标吗？其中您所负责的“智能技术与应用”板块在其中是扮演什么样的角色？</p><p></p><p>奚伟：美的中央研究院目前主要在 “AI+ 工业机器人”“AI+ 智能制造”“AI+ 智能家居”“AI+ 医疗影像”四个主要方向展开布局。目标是通过 AI+ 的革命性技术创新实现美的产品力和竞争力的领先，服务美的多元化经营的目标。</p><p></p><p>智能技术与应用板块是美的“AI+”战略的重要技术支撑力量和新技术应用的先锋队。通过开展新型传感器、多模态智能感控技术，以及具身智能大模型技术的研究，为下一代工业机器人，先进医疗影像设备，和未来家电等前沿方向提供关键核心感知部件支撑和差异化感知平台技术方案。</p><p></p><p>比如，在关键传感器方面，我们研发了智能 3D 相机、一体化力觉传感器和一些精密编码器，以及毫米波雷达等一系列传感器，涵盖从空间感知到位置感知的各个领域。实现关键传感器，医用探测器核心部件自研自制。</p><p></p><p>在此基础上，我们重点研究核心算法，探索如何将这些传感器和探测器应用于机器人和医疗设备中进行研发，推动前瞻性感知技术跨事业部布局及产业应用。我们的最终目标是在工业、医疗领域、以及未来家电领域智能化感知行业实现产品创新和行业领先。</p><p></p><p>InfoQ：在推动 AI+ 战略方面，您的日常工作主要关于哪些方面？</p><p></p><p>奚伟：我的工作主要包括整体“AI+”和“三个一代”（研究一代、储备一代、开发一代）技术战略的规划和布局，“AI+”在新业务领域的探索及国际化、高水平团队的组建及能力建设，带领团队在核心技术上攻关，实现技术突破，并协同事业部积极推进技术应用转化。</p><p></p><p>InfoQ：从整体进展来看，智能化感知技术在机器人、医疗及智能家居领域的应用研究及技术创新，目前是什么样的现状？</p><p></p><p>奚伟：智能化感知技术在机器人、医疗及智能家居领域的应用研究及技术创新目前处于一个新的爆发期。</p><p>整体趋势是从自动化、精准化向智能化转变，目前也是全球各大公司投入大量资源，竞争差异化的重点。</p><p></p><p>以机器人为例，结合视觉及多元感知的具身智能技术的出现，改变了传统通过编程实现自动化的过程，机器人的技能通过学习完成，高质量训练数据成为核心。医疗领域也是如此，利用深度学习技术可提高扫描速度，减少 CT 的辐射剂量，基础大模型技术可以辅助报告生成和智能化诊疗。“AI”技术极大促进了传统技术的升级。</p><p></p><h2>AI 和智能化感知技术的选择与发展策略</h2><p></p><p></p><p>InfoQ：在技术的研发和应用过程中，美的如何确定哪些 AI 技术和智能化感知技术值得投资和开发？选择技术的逻辑和标准是什么？</p><p></p><p>奚伟：我们主要从 AI 技术相对传统技术的差异化优势和价值，以及行业技术发展趋势来进行技术布局和投资。如通过引入 AI 技术可以扩展产品的应用领域，或提升产品的关键指标，这些都是明显判断技术价值的标准。再比如对于机器人，通过 AI 技术进行智能制造，可以减少工人的劳动强度并提高制造效率，也是价值的体现。</p><p></p><p>如果技术成熟度不太高，市场不明确，但未来有重大前景的 AI 技术，我们也会提前布局。</p><p></p><p>InfoQ：有没有出现在技术投入过程中，我们最初判断某项技术符合需求并能带来商业价值，但经过一段时间的投资和研发后，发现方向不对的情况？</p><p></p><p>奚伟：这种情况也是会有的，但正如我刚才提到的，美的拥有一个较好的研发体系，可以最大程度地减少这种情况的发生。我们采用“研究一代、储备一代、开发一代”的模式。</p><p></p><p>每个阶段投入的资源是不同的。在前沿研究方面我们会选择一些我们认为有潜力的技术进行尝试，当可行性得到验证后才会进一步进行结合业务场景进行更详细的论证。这些系统性的技术管理保证了技术决策的合理性及延续性。</p><p></p><p>InfoQ：能否举个例子说明这种情况？</p><p></p><p>奚伟：比如毫米波雷达，虽然这个例子可能不是特别典型，但它确实展示了我们在技术判断过程中的一些经验。</p><p></p><p>在早期阶段，毫米波雷达是我们非常看好的方向，因为它可以提升智能家居的智能化程度。我们在前期投入了一些资源进行底层技术研发和关键传感器的开发。然而，随着时间推移，我们发现市场上的竞争非常激烈，很多供应商也在开发类似的技术，导致差异化程度降低。</p><p></p><p>最终，由于市场竞争充分，毫米波雷达的价格变得非常低。在这种情况下，我们认为继续自研这项技术的商业价值不大。就会调整技术方向，选择投入更多资源到其他更具技术附加值的领域，特别是那些需要攻关的技术方向。</p><p></p><p>InfoQ：那在领导技术团队进行攻关时，您认为最关键的因素是什么？团队素质最重要的点有哪些？</p><p></p><p>奚伟：这个问题比较大，但有几个关键点。</p><p></p><p>首先是热情。尽管我们做的是前沿技术，但很多时候是在摸着石头过河，没有现成的方法可以参考。我们需要阅读大量文献，做许多实验，并不断试错，才能找到相对可靠的解决方案。这对团队的耐心是个巨大挑战。</p><p></p><p>其次是结果导向。在国内，结果导向非常重要。我们需要在研发过程中能够阶段性地输出一些成果，以保持团队的信心。让大家看到我们一步一步取得进展，这样能促使团队坚持下去。</p><p></p><p>另外在管理团队方面，我认为首先是能力要强，找到有能力的领军专家，通过“老带新”，进行有效率的技术攻关；其次有共同的目标，把自己的工作看成是对社会发展有影响的事业；日常管理上，“三个一代”战略牵引，工作目标牵引。</p><p></p><p>InfoQ：从结果导向来看，这对您来说有哪些挑战？特别是很多前沿技术需要很长时间才能转化为实际成果。</p><p></p><p>奚伟：确实如此，前沿技术从研发到成果转化往往需要很长时间。我们需要展示技术的潜力，让领导层看到未来的投资价值。如果一项技术经过一段时间的研发，发现对产品能力和用户价值的提升不大，投资的动力就会逐渐减弱。</p><p></p><p>比如说，IoT 技术曾被寄予厚望，希望实现万物互联，提升智能家居的用户体验。但由于技术标准不一致和用户价值体现不够明显，渐渐地很多企业对 IoT 的投资逐渐减少。这提醒我们在技术研发过程中，需要确保技术的潜力和价值被充分展示和认可。</p><p></p><h2>AI 驱动下的场景应用与产业创新</h2><p></p><p>InfoQ：在“AI+ 工业机器人”方面，智能技术与应用研究所取得了哪些主要成果？这些技术突破是如何支撑产业转型升级并推动制造数字化转型？</p><p></p><p>奚伟：美的承建了蓝橙全国重点实验室，其中 AI+ 工业机器人是实验室四大建设内容之一。</p><p></p><p>技术方面，首先在底层传感器方面，我们在包括 3D 相机、力传感器、编码器、激光雷达和毫米波雷达等传感器都取得了显著进展。其次，在技术平台方面，搭建了工业视觉技术平台、低代码编程平台、智能导航平台以及数据仿真平台等。这些平台不仅能够迅速实现技术落地，还支撑了产品的升级。第三，通过仿真平台，也加速了模型的迭代和开发。</p><p></p><p>结合应用场景来看，也可以举几个例子：</p><p>针对工业制造应用场景接近 100% 识别成功率的需求，美的研发了基于视觉大模型的通用强泛化高精度识别定位的技术，应用于焊接，装配等 5 类典型制造场景，完成 100 余条产线规模化复制。针对重载移动 AMR 的高鲁棒精准定位需求，美的研发了多源异构传感器的多模态融合智能定位技术及 AI 安全感知技术，也已应用于库卡 KMP 系列潜伏式 AGV 超过 1000 台套。&nbsp;高速 3D 成像，应用于复杂场景下的动态制造，如涂胶和焊接等，通过计算机相机辅助智能制造。针对重载机器人要达到应用低代码编程目标，美的研发图形化编程软件，大大缩短了机器人部署时间。</p><p></p><p>目前工业机器人典型场景完成了验证并实现了批量化落地应用，覆盖 80% 重载应用场景。</p><p></p><p>另外，美的结合生成式 AI 大模型，大数据，物联网，云计算等数字技术创新，把数字技术与制造业深度融合，建立了 28 家国家级绿色工厂、3 家零碳工厂、5 家世界级灯塔工厂，入选国家级双跨平台。其中电子车间的无人化场景是“AI+ 制造”智能化发展的重要方向之一。</p><p></p><p>InfoQ：在技术推广过程中，主要会遇到哪些阻力？</p><p></p><p>奚伟：推广过程中主要的阻力有以下几个方面：</p><p></p><p>现有产线改造的复杂性：在新的产线上推广相对容易，因为可以整体规划。然而，在已有产线上的改造涉及到很多方面的工作，包括人员对新技术的不了解和需要学习新知识等问题。技术适应性和环境因素：传统制造依赖人的适应性，而机器在适应不同环境时要求更高。例如，环境的稳定性和光照等因素对机器的影响较大。很多技术在一个工厂研发完成后，扩展到其他产线并不像软件复制粘贴那么简单，需要进行很多微调和定制化。人员培训和接受度：培训人员接受并拥抱智能化技术是一个很大的挑战。很多员工对熟悉的流程依赖较强，对新的智能化技术不熟悉，可能会感到困难，不愿意使用。</p><p></p><p>针对人员培训，库卡也推出了新匠星计划，目标是培养更多适应数字化、AI 时代的高级技术人才 。</p><p></p><p>InfoQ：针对“AI+ 智能家居”方向，智能技术与应用研究所在家电智能化和机器人化方面有哪些创新？</p><p></p><p>奚伟：智能家居是我们研究所的新方向。我们研究所主要面向 To B 方向，但在 To C 方向，尤其是“AI+ 智能家居”方面，未来智能家居将出现两个发展方向：</p><p></p><p>&nbsp;第一，家电主动服务，识别用户意图，学习用户习惯，为用户提供服务。&nbsp;第二，家电机器人化，家电即机器人，比如说现有的扫地机器人，未来还会有更多的家电以机器人的形态出现。两个发展方向均需要突破 AI 技术。</p><p></p><p>目前，美的集团在语音语言、边端智能、AI 大模型等 AI 技术方向持续突破：</p><p></p><p>&nbsp;语音方面，打通了语音全链条上技术环节，已上线 5 个不同特色发音人并提供了稳定的 TTS 服务，低信噪比环境增强后语音识别率上升 15%，唯一唤醒成功率 90%。边端智能方面，持续对模型压缩和推理加速优化，实现语音模型压缩比&gt;7x, 推理时延降低 70%；视觉模型压缩比&gt;16x, 推理时延降低 75%；美的发布国内首个家居领域 AI 大模型“美言”，为智能家居构建了智能感知、自然交互和自主决策三个基础能力。在服务机器人方向，美的利用 AI 等技术研发服务机器人产品，美的集团获批建设“智能服务机器人国家新一代人工智能开放创新平台”。</p><p></p><p>在新的方向上，我们目前主要围绕着家电机器人化展开工作。我们现在有一些业务场景，大家相对接受度较高的是扫地机器人。除了扫地机器人，我们还在探索其他智能家居产品的可能性。</p><p></p><p>InfoQ：以扫地机器人为例，它在智能化方面，还有哪些潜在的想象空间？</p><p></p><p>奚伟：其实有几个方面可以考虑：</p><p>扫地能力提升：传统的扫地机器人只能进行基本的清扫工作。未来，我们希望它能智能识别不同的地面类型和污渍情况，进行更高效的清扫。这是一个重要的方向，我们在这方面也做了许多技术攻关。语音交互：目前，扫地机器人有时会出现无法找到的问题。通过改进语音交互功能，用户可以更方便地与机器人进行沟通，例如让它自动找到某个地方进行清扫，而不需要手动操作手机或遥控器。&nbsp;拓展清扫范围：现有的扫地机器人还无法触及某些地方。我们正在尝试让机器人清扫地脚线等区域，甚至结合拖洗一体机，扩展其功能和应用场景。</p><p></p><p>这些探索都可以进一步提升扫地机器人的智能化水平和用户体验。</p><p></p><p>InfoQ：现在大语言模型给许多行业带来了影响，无论是效率提升还是智能理解方面都有比较大的进展。能否再详细谈谈美的对美言大模型的期望和愿景？</p><p></p><p>奚伟：美言大模型的主要优势在于提升平台化的语义理解能力。传统的语音交互只能响应特定的关键词，而美言大模型可以理解用户的自然语言表达，从而提供更优质的服务。这对现有家电来说是质的飞跃。</p><p>通过结合美言大模型，我们可以将其应用于烤箱、微波炉等家电产品，提升它们在烹饪和清洁方面的智能化水平。模型的生态系统能积累大量用户数据，进而不断优化模型，增强产品功能和用户体验。通过这种持续的改进，产品的用户复购意愿也会提高。</p><p></p><p>核心在于，传统的 To C 产品在销售后基本不会再有大的升级变化，而借助大模型技术，产品将具备自学习能力。它们可以根据用户习惯调整操作方式，例如提供更节能的模式，或在烹饪过程中提供智能化提示。对于老人用户，系统可以在忘记关火或操作失误时主动提示和纠错。</p><p></p><p>总体而言，美言大模型将实现家电的主动服务和家电机器人化，不仅理解用户需求，还能进行智能操作。</p><p></p><p>InfoQ：在“AI + 医疗影像”领域，智能技术与应用研究所如何通过 AI 技术提升医疗资源的整合和优化？这些技术对医疗影像诊断和治疗流程有什么影响？</p><p></p><p>奚伟：在“AI+ 医疗影像”方面，人工智能将加快医疗领域的资源整合优化，用 AI 技术赋能诊疗一体化全流程，实现一体化精准诊疗新技术、及跨科室、多模态、数智化的诊疗新方案。</p><p></p><p>目前美的开发的昆仑 AI 智慧影像平台，通过将 AI 深度嵌入临床影像工作流程，改善扫描流程，提升成像质量，辅助医生进行诊断、治疗决策等，进而打通筛诊疗预随全临床流程。</p><p></p><p>同时覆盖多病种 AI 解决方案，精准高效触达更多临床场景。比如，AI 自动定位及自动扫描覆盖 60% 临床部位，效率提升 30%；AI 静音技术降低 86% 的噪声声压，改善医患检查舒适度；AI 去噪加速功能，实现 4 倍加速的同时图像信噪比提高 30%；妙笔 AI 质控软件，除了全天候自动检测影像报告外，更进一步提供报告质控应用，智能提醒医生报告书写错漏，正确识别准确率高达 93%，提高各级医疗机构医生报告质量，大大降低因书写错误造成的纠纷。</p><p></p><p>InfoQ：美的智能家居有一个美言大模型，那在医疗领域，会不会也有一个垂直领域的大模型？</p><p></p><p>奚伟：是的，我们确实有这个计划，目前还在进行当中。我们正在组建团队，进行相关的布局。</p><p></p><p>InfoQ：医疗领域的大模型有应用于科研、药物研发等不同领域的，而美的会更侧重于智能诊断和设备方面？</p><p></p><p>奚伟：对，美的主要在智能诊断和设备方面进行布局。万东医疗有一个云平台，可以将医院产生的数据上传，帮助生成病理报告。此外，我们希望通过大模型技术在设备端自动生成术后的扫描报告，辅助医生进行后续诊断。我们在这些方面都在进行布局。</p><p></p><h2>智能化技术挑战与应对策略</h2><p></p><p></p><p>InfoQ：在推动数智化技术产业化落地的过程中，遇到过哪些主要挑战？您是如何应对这些挑战的？</p><p></p><p>奚伟：最大的挑战有两个，一个是技术的预期和现实的差距，一个是高质量数据资源不足。第一个问题最难解决，通过客户希望智能化技术能像人一样的灵活度，但实际情况是目前智能化技术仅能实现有限场景的智能化。随着 LLM 大模型技术和 AGI 技术的不断发展，我认为这一个差距会逐渐缩小。</p><p></p><p>对于数据稀缺问题我们积极采用数据仿真合成的方式进行补足。另外，高端前沿人才还较为缺乏，总体来说，AI 人才相对欠缺，传统人才更多一些。</p><p></p><p>InfoQ：根据您的经验，您认为哪些因素最能决定 AI 技术和智能化感知技术的成功落地？</p><p></p><p>奚伟：有几个主要因素。首先是技术应用的成熟度，这是最关键的因素。很多智能化技术无法落地，主要还是因为技术还不够成熟。第二个因素是找到产品价值、对于用户的价值。第三是新技术培育的土壤和成本控制，早期必须要有正向的收益来推动技术的不断迭代，但在中国的环境中，这很难实现。很多技术还没推广，就已经降到了“地板价”。</p><p></p><p>InfoQ：最后想问下在美的集团的“AI+”战略下， 智能技术与应用研究所下一步有哪些规划和目标？</p><p></p><p>奚伟：总体来说，我们会围绕美的产业布局不断深化扩展“AI+”场景应用，形成真正的新质生产力。另外还会探索未来新赛道、探索场景新应用。</p><p></p><p>也计划将更多传感器、毫米波激光雷达等技术植入机器，让工业机器人、医疗设备、智能家居拥有智能大脑和眼睛，从而提升用户体验。</p><p></p><h4>嘉宾介绍</h4><p></p><p>奚伟，高端重载机器人全国重点实验室副主任，国家级科技创新领军人才，美国马里兰大学帕克分校博士，本硕毕业于清华大学。目前担任美的中央研究院智能技术与应用研究所所长，主导集团内部多个数智化技术研发和产业化落地项目，推动集团在“AI+”战略上技术布局和创新。项目内容涉及 AI 工业视觉算法研发及低代码免编程技术在智能制造应用，多模态 SLAM 导航及视觉安全感知技术， AI 智能化医疗影像平台技术研发，未来智能家居场景下家电机器人化技术研究，下一代机器人及医疗设备关键传感技术研究等。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/L4JzNvIDLNutDqb8y656</id>
            <title>字节跳动发布“豆包MarsCode”智能开发工具，面向国内开发者免费</title>
            <link>https://www.infoq.cn/article/L4JzNvIDLNutDqb8y656</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/L4JzNvIDLNutDqb8y656</guid>
            <pubDate></pubDate>
            <updated>Thu, 27 Jun 2024 02:22:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, 豆包MarsCode, AI时代, 开发者工具
<br>
<br>
总结: 字节跳动发布了基于豆包大模型打造的智能开发工具豆包MarsCode，并面向国内开发者免费开放。豆包MarsCode团队更多地从如何赋能开发者的角度入手，希望打造一款软件，能够助力提升开发者工作效率，让开发者有更多精力和时间用于思考和创造。豆包MarsCode通过提供编程助手和Cloud IDE两种形态，以及各种功能如项目问答、代码补全、单测生成、Bug Fix等，帮助开发者更轻松、更专注地编程。 </div>
                        <hr>
                    
                    <p>6&nbsp;月&nbsp;26&nbsp;日，字节跳动发布了基于豆包大模型打造的智能开发工具豆包MarsCode（<a href="https://www.marscode.cn/home??utm_source=626&amp;utm_medium=wx">https://www.marscode.cn/home??utm_source=626&amp;utm_medium=wx</a>"）&nbsp;，并面向国内开发者免费开放。</p><p>&nbsp;</p><p>现场，字节跳动开发者服务团队、豆包MarsCode&nbsp;负责人李东江还分享了一些对&nbsp;AI&nbsp;时代开发工具演进趋势的思考。</p><p>&nbsp;</p><p>进入&nbsp;AI&nbsp;时代，大语言模型在编程语言方面具备强大的优势和潜力，相比起复杂的自然语言，编程语言是更加简洁、严谨、可预测。关于“应当如何构建一款&nbsp;AI&nbsp;时代的开发者工具”的命题，豆包MarsCode&nbsp;团队更多的从如何赋能开发者的角度入手。</p><p>&nbsp;</p><p>李东江认为，在&nbsp;AI&nbsp;技术驱动下，一定会衍生出下一代的开发工具。AI&nbsp;不是替代开发者的“竞争者”，而是开发者的“好帮手”，团队更希望打造一款软件，能够助力提升开发者工作效率，让开发者有更多精力和时间用于思考和创造。</p><p>&nbsp;</p><p></p><h2>豆包MarsCode&nbsp;首发功能揭秘</h2><p></p><p>随后，豆包MarsCode&nbsp;产品负责人王海建介绍了豆包MarsCode&nbsp;产品的两种形态：编程助手和&nbsp;Cloud&nbsp;IDE，同时通过需求开发、修复Bug、开源项目学习三个实际场景，详细演示了豆包MarsCode&nbsp;的项目问答、代码补全、单测生成、Bug&nbsp;Fix等功能。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ef/efa0113b931686d6935da9c38e0ba5ed.png" /></p><p></p><p></p><p>需求开发场景</p><p>&nbsp;</p><p>通过一个翻译机器人构建的实际案例，王海建演示了在&nbsp;AI&nbsp;的辅助下，开发者可以如何通过唤起编程助手进行&nbsp;Chat&nbsp;提问，分析需求、熟悉代码、编写代码和调试代码。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f6/be/f6c6905cb819015107d407ce0c162dbe.gif" /></p><p></p><p>代码补全不仅仅可以帮助开发者更快地输入代码，更是可以通过不断提供代码建议，给开发者带来灵感和启发。豆包MarsCode&nbsp;的创新功能——代码补全&nbsp;Pro，不同于传统的代码续写，它支持自动根据用户编辑意图预测下一个改动点并给出代码推荐。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/90/8d/90e4596ae75b6190362297bb27ee438d.gif" /></p><p></p><p>除了代码预测与补全，当编码中出现需要修复的代码&nbsp;Lint&nbsp;错误时，编程助手会直接在编辑器中主动给出修改代码，我们不需要去查看是什么报错原因，只需要判断修复结果是否正确，如果正确，一键采纳修复后的代码即可。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/17/e4/1701569e98a74f285e809589f24955e4.gif" /></p><p></p><p>最后，当我们写完代码，为了保障代码的质量与后续的可维护性，通常还需要写单元测试。这时只需要在编程助手中触发&nbsp;test，就可以得到这个函数的测试用例。可以看到，相比于传统的开发方式，豆包MarsCode&nbsp;编程助手可以帮助开发者更轻松、更专注地编程。</p><p>&nbsp;</p><p>Bug&nbsp;修复场景</p><p>在&nbsp;Debug&nbsp;场景下，豆包MarsCode&nbsp;的&nbsp;AI&nbsp;修复功能可通过理解报错信息、调用栈的代码、全局的项目代码，去分析错误原因，从而直接给出针对性的修复建议。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/90/4c/90fb104a71b5c2cf1bf7e31314e7184c.gif" /></p><p></p><p>除了单轮修复，豆包MarsCode&nbsp;也在尝试基于&nbsp;Agent&nbsp;方式实现多轮自动修复，AI&nbsp;会自主调用一系列代码查询工具、调试工具获取报错信息、自主规划方案、自主写出代码并应用到项目当中去，来修复&nbsp;Bug。目前，该功能正在字节内部做验证。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/e7/3d/e7574d6ddd357a3276ca112a4d23ef3d.gif" /></p><p></p><p></p><p>开源项目学习场景</p><p>第三个场景下，豆包&nbsp;MarsCode&nbsp;IDE&nbsp;通过提供开发模板，让开发者能够快速进入项目而无需运维本地环境。借助原生集成的&nbsp;AI&nbsp;能力，开发者不再需要自己去理解代码，从而更高效地上手项目。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d9/d935bb85d4f1efd9c3bb0242416cb77a.gif" /></p><p></p><p></p><p>总的来说，豆包MarsCode在以下两个方面帮助开发者：</p><p>对于想的阶段，提供更好的信息，例如做代码解释，研发知识的问答，来激发开发者创造；对于做的阶段，帮助开发者更快地完成编码，例如代码的补全、下一步编码动作的预测，代码的错误修复，来提升开发者效率。</p><p>&nbsp;</p><p>后续，豆包MarsCode&nbsp;会通过成立用户组、各类系列开发者活动等方式，助力开发者探索&nbsp;AI&nbsp;编程新范式。现场，豆包MarsCode&nbsp;市场运营负责人赵旭东介绍了豆包MarsCode&nbsp;开发者与社区共创计划。</p><p>&nbsp;</p><p>据悉，豆包MarsCode&nbsp;用户组将由开发者自组织自运营，豆包MarsCode&nbsp;团队不会参与到用户组的管理，但是会为用户组提供丰富的各类资源支持，支持各地用户组发展，例如场地资源、产品资源、活动物料、专家讲师支持等。在开发者活动方面，豆包MarsCode&nbsp;将陆续在北、上、深、杭等城市举办&nbsp;Meetup&nbsp;。</p><p>&nbsp;</p><p>豆包MarsCode&nbsp;还计划协同各开发者社区，将&nbsp;AI&nbsp;能力融入到社区使用场景中，将豆包MarsCode&nbsp;的能力更便捷地提供给开发者。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/k5tx08DktK7vfdKjJm83</id>
            <title>长文本 vs RAG：谁将主导大模型未来？</title>
            <link>https://www.infoq.cn/article/k5tx08DktK7vfdKjJm83</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/k5tx08DktK7vfdKjJm83</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 09:55:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术公司, 长文本, RAG, 上下文
<br>
<br>
总结: 一些顶级大模型技术公司正在支持更大的上下文作为更新升级的重点，长文本突破引发了社区争议，对于 RAG 的未来存在疑虑。大模型企业开始降价，引发了低成本与选择长上下文和 RAG 的关系讨论。 </div>
                        <hr>
                    
                    <p>前段时间，国内外包括 OpenAI、谷歌、月之暗面等一大批顶级大模型技术公司都以支持更大的上下文作为更新升级的重点。长文本的突破也让社区中有了一些争议，认为这可能意味着 RAG可能会消亡。另外，最近国内一众大模型企业都开始宣布降价，那么低成本是否跟选择长上下文和 RAG 是否有关系？</p><p>&nbsp;</p><p>在这期极客有约节目中，我们邀请到月之暗面唐飞虎、zilliz 合伙人栾小凡、英飞流创始人张颖峰三位重磅专家给大家解读“长文本和 RAG”方面的技术。</p><p>&nbsp;</p><p>直播视频链接：<a href="https://www.infoq.cn/video/vZZIkkIt85rVRa61xsR8">https://www.infoq.cn/video/vZZIkkIt85rVRa61xsR8</a>"</p><p>&nbsp;</p><p>InfoQ：前段时间，大模型企业都在宣传自己取得了上下文上的突破，那么更大上下文能让我们做些什么？</p><p>唐飞虎：我认为可以将模型上下文长度类比为计算机内存。随着模型上下文长度的提高，应对较为复杂的逻辑推理或生成 repo level 的代码等问题的性能都有了提升。举例来说，使用谷歌 Gemini 对癌细胞进行影像学的诊断问题，是需要组合多模态和长文本才能完成，目前来说并没有较好的 RAG 解决方法。对多模态模型而言，更长的上下文不仅能帮助模型理解并读取更多文档，在模型的整体性能方面也有帮助。</p><p>栾小凡：对大模型来说，RAG 应用的构建时，更长的文本的确有利。这本质是一个输入输出窗口的问题，在具备捕捉信息和上下文能力的基础上，大文本输入的信息越多，输出也会越好。另一方面，我个人认为长文本只是大模型能力的其中之一，我是非常反对文本越长越智能的观点。</p><p>张颖峰：长文本对大模型来说是能力上的巨大提升，这种提升主要体现在其摘要能力的生成。结合我们过去的经验来说，去年在实施一些 RAG 项目时，我们遇到的最大痛点其实是来源于大模型本身，但在今年这些痛点就已经逐渐被消除或大大地改进了。长上下文也让模型的可控性得到了增强，我们在做 RAG 的过程中很多流程因此发生了变化，我们不会去追求特别复杂的检索策略，而是将结果检索到后将后续任务都交给了大模型来完成，这种实施也会相对较容易一些。</p><p>&nbsp;</p><p>InfoQ：我们是不是必须要200万甚至无限长的上下文？</p><p>张颖峰：长上下文很有意义，但无限长的上下文则是更偏向于是营销的宣传策略。上下文长度到达一定程度后，丢失的信息也会更多。毕竟上下文再大也只是内存，没有必要追求无限大的内存，再大的内存也还是需要硬盘的。</p><p></p><p>InfoQ：今年看到“长文本”我们可能就会想到“月之暗面”，之前月之暗面Kimi将上下文扩展到了200万，那么 Kimi 是如何实现的这个突破的？</p><p>唐飞虎：我们从20万字到200万字的扩展并没有采取常规的渐进式提升路线，而我们在技术攻关中遇到的难度也是呈指数级增加。为了达到更好的长窗口无损压缩的性能，我们团队从模型的预训练到对齐再到推理环节，均进行了重新的设计和开发，并且没有走滑动窗口、降采样等常规技术捷径，而是攻克了很多底层的技术难点。但是长上下文也不是无限长更好，我认为对现阶段的开发者而言这是一个 trade-off 的过程，上下文长度的增加对推理成本和时间都有一个平方几倍的增加。很多技术也是类似，都是遵循从 paper 到实验室，再从实验室到工程，最后再到产品这样一个循序渐进的过程，在现在的研究阶段，我们要将成本做到最低，长文本效果做到最好，这才是更重要的事情。</p><p></p><p>&nbsp;</p><p>InfoQ：Kimi 也直接在国内带来了比拼上下文长度的热潮，各厂商瞬间就突破了500万、1000万的处理能力，那这种效果属于算法还是算力上的突破？</p><p>唐飞虎：各个厂商目前还没有公开自己的 technical report，所以这方面开发者还是更有话语权，他们会实地使用、体验各家不同的产品。上下文的性能可以参考一些常规的 Benchmark，比如大海捞针和腾讯的数星星，最近还有更加复杂的评测，比如在长文本中添加代码、逻辑片段。如何在长文本上下文长度边长的同时还要保质保量，这是一个非常有挑战性的技术难题。</p><p>栾小凡：我人认为以目前的 Transformer 架构来讲，算法的提升空间还是相对有限，再想将上下文提升到下一个数量级，这条路已经很难走得通了。我最近做的一些测试来看，不说几十万、几百万 token 的上下文，光是上百 KB 的上下文在各家的线上测试都有几秒甚至是几十秒的延迟。所以在真正落地的生产环境中，上下文的长度还是应该控制在数十 K 的范围内。</p><p>张颖峰：从实现的角度来说，我会将其分为两派，一派是使用了 RAG，一派是没用 RAG。在目前提供公共大模型服务的厂商里，我认为这两派都有，且比例都不低。站在技术的角度来说，我在某些方面上还是更倾向于纯模型派。</p><p>&nbsp;</p><p>InfoQ：增加上下文窗口大小且不影响模型性能，会存在哪些挑战以及有什么应对方法？</p><p>唐飞虎：这个其实关系到我们 Transformer 模型的它的底层架构，就如果你去看它的那个计算的方法，然后再做一些相关的评测的话，就是有一些长文本性能上的一些常规上的瓶颈，那基本上说出目前分析下来的话是长文本的并发性能，它会随着你上下文长度的长度增加而反比下降。然后预填充的延迟会随着你上下文长度的增长而平方级别的增长，然后解码延迟和上下文切换的开销也会随着你上下文长度的增加而线性的增加啊。这几个参数里面对整个推理性能影响比较明显的是并发性能和预填充的延迟，也是现在各个厂商，包括一些学者专家，大家在研究和攻坚的这个难题。</p><p>栾小凡：飞虎老师提到的 Transformer O(n^2)&nbsp;的问题，我比较期待的是最近 LSTM 的卷土重来。我认为是需要对模型架构进行调整，才可能到达下一个阶段的。</p><p>&nbsp;</p><p>InfoQ：在处理大上下文长度时，原始 Transformer 架构的主要限制是什么？目前有哪些优化技术可以加速 Transformer 并将上下文长度增加到更大？</p><p>唐飞虎：主要的瓶颈就在于前面提到的矩阵计算导致预填充延迟和并发性能随上下文长度的增长而显著增加。目前这方面的优化角度主要还是硬件、机器学习工程和模型架构这三方面。硬件现在有 A100 memory 的 hierarchy 设计；机器学习工程有 VLLM 和较为主流的 Flash Attention 等技术；模型架构有前段时间流行的 MoE（多专家模型），也有最近对 Transformer 架构底层的颠覆，比如 Mamba 等等。但有些优化技术目前还没有较为完善的支持，导致其在工程上的实现并不可行。举例来说，键盘 QWERTY 的布局对人类而言并不是最优的，但大家都已经用了很久也非常习惯了，除非有更加优秀、能带来颠覆性结果的设计，大家才能更好地 follow。因此，如果只是从某一层面上带来效率的提升，那还要关注优化是否能再工程上有更好的结合。最近的一些论文有提到从 layer、head、hidden layer 等层面上做优化。大家可以在 ICIP 这些网站上搜索“推理优化”相关的论文结果。</p><p>栾小凡：如果大家对降低成本方面感兴趣的话，DeepSeek 的 MLA（Multi-latent Attention）和多专家模型是他们能将价格压下来的最底层的技术。从硬件层面来说，最早有去年很火的 Grok，今年也有越来越多的硬件厂商从推理方向加入这个赛道，比如美国的 SambaNova、英特尔等公司，这方面在我看来会比训练更容易得出成果，模型架构本身相对简单，硬件定制优化更容易得到很好的吞吐。</p><p>唐飞虎：旗舰店的另一个好处是不会出现前面提到的技术选型方向不兼容的问题。</p><p>张颖峰：模型成本方面最近降价很猛，我认为这和长上下文确实是有一些关系的。在目前的架构下， 主要瓶颈都在于对 KV cache 的低利用率。DeepSeek 本身的 ML 架构其实也是对 KV cache 做 low-rank 压缩。其实无论是推理加速还是长上下文，这些都是围绕着 KV cache 的利用率，要怎么将 cache 里的 attention 量化、提升和压缩。</p><p>&nbsp;</p><p>InfoQ：有论文声称能赋予LLM处理无限长度文本的能力，那工程上是否可实现，或大概多久后能达到无限上下文长度？</p><p>栾小凡：这个问题还是要回到第一原理上，如果说 AI 最终的目标是替代人做事，那么我们首先要问的一个问题是，人有什么样的能力？每个人的记忆都不是无限的，我们有短期也有长期的记忆，所以这其实是一个伪命题。再说什么才是真正的智能，我认为这是个伪命题，长文本不代表智能，长文本只是一种手段。能记住更多东西的人不一定更聪明。OpenAI 最近一年内的演进方向来看，更好地使用工具才是用大模型解决问题的一个关键点。记忆短有记忆短的处理办法，但记忆变长就一定意味着密度更小，以现在的 Transformer 机制来说，随着上下文的边长，模型一定会遗忘一些东西。举例来说，让大模型找到一百个数字中唯一的乱序数字，现有的模型都可以轻松完成，但换成一万个数字，那么基本上所有的模型都会犯错，因为它已经找不到重点了，甚至已经忘记了自己的任务是去找乱序的数字。</p><p></p><p>InfoQ：Jeff Dean的twitter提到，百万Token上下文，精度还能达到99.7%（千万99.2%），那么在这种情况下的大模型解决了RAG技术方案下所存在哪些痛点？但也还存在哪些问题待解决？</p><p>张颖峰：RAG 的痛点基本来源于这几个方面，一是在用户意图很强的情况下，召回可能不够精确。在实际的企业内部数据集或者用户的个人数据中进行检索，因为答案不再一个语义空间内，所以很可能搜索不到想要的东西。其次也是我们在公司创业到现在做各种 RAG 项目时遇到的问题，检索查询时数据本身非常杂乱。这些问题部分可以用上下文进行缓解，比如在长上下文的语义空间中包含检索需要的答案，这样大模型也能对问题进行回答。不过这种方案还不能完全解决这种问题，我认为还是需要提高 RAG 自身的能力。以今年春节为分割线，春节之前最大的瓶颈是大模型本身，春节之后在大家对大模型能力要求的情况下，我认为整套体系最大的瓶颈已经落到 RAG 上了。如果未来大家对模型期待提到，希望能引入更多的逻辑推理，那么痛点可能又会转移到大模型上去，这个问题总是在不断变化的。</p><p>唐飞虎：这个我可以举的例子是上个月我们的联合线下黑客马拉松，有一个场景是需要基于百大行业分析的研报文档知识库，生成相关数据的图表。当时在现场无论怎么组织 prompt，RAG 都没有办法得到想要的结果，于是我们直接将其换成长文本。因为 prompt 的上限导致还是无法不支持，所以我们采用了自定义插件将文档全部输入进去，结果的图表很效果很好。这样类似的例子其实有很多，需要的上下文信息分散在整个文章文档且有一定的上下文逻辑关系时，长文本目前的确比 RAG 更好。</p><p>栾小凡：我个人认为，无论是 RAG 还是长文本，和之前的数据处理没有什么本质区别。长文本是对所有数据的实时处理，RAG 则是对数据的离线处理。在没有理解用户意图的情况下，离线的数据处理可能结果出错。所以我认为现在的各种小技巧无非都是建立更加普遍适用的关系，猜测用户的意图。从本质来讲，如果大模型对一个问题没办法，那 RAG 也很难回答。所以我认为从模型最终的智能角度来说，一定是先有大模型智能再有 RAG。至于说 RAG 是否是不必要，人可以把所有书都读一遍来找到自己想要的答案，但搜索引擎却可以为我们节省时间。我认为 RAG 存在的一个核心价值是能帮助我们在大模型预处理数据之后更快速地完成重复工作。</p><p>&nbsp;</p><p>观众提问：这种场景一般要求准确性会比较高，长文本替换 RAG 的解决方案不会出现模型幻觉吗？</p><p>唐飞虎：如果只是将数据提取出来，那么出现幻觉的概率还是相对较低的。重灾区一是体现在一些较长的复杂逻辑上，数学证明题可能就在中间某个步骤就乱了，现在的模型也没有办法去很好地验证结果的正确；二是一些比较冷门的知识点，模型完全不知道结果；还有一种是数据本身 bias 导致的幻觉。这些都是我们目前需要攻克的问题，之前提到的场景，如果只是生成一个图表，那么模型出现的幻觉相对较低，即使生成了幻觉也能很容易地去验证，然后再重新调整 prompt。</p><p>&nbsp;</p><p>InfoQ：Gemini 1.5出来后，社区认为百万 Token 的上下文意味着 RAG 可能会消亡，老师们怎么看待这个观点？</p><p>张颖峰：我个人认为这种观点大部分正确，在海外媒体比如推特上这方面的讨论很多，基本是对 RAG 在成本、性能方面优势进行赞同，但在其他方面则持反对意见。我认为在 C 端个人知识库这类简单的场景下，大模型足矣。不论成本、性能和数据安全，只谈能力的情况下，B 端的 RAG 应用场景非常多，而随着上下文长度的增加，查询率也会降低。所谓的大海捞针也只是不会错过最近的针，更靠前的还是会有遗漏的可能。在这些场景下，我们不能只依赖大模型，还要将 RAG、大模型以及内部系统进行整合。这样一来又会带来更多问题，比如 RAG 检索出的结果虽然语义相似但答案却不是很相关，这种情况反而会降低模型回答的质量。</p><p>唐飞虎：上下文长度决定了模型的基础能力，RAG 则是决定了开发者对应用开发的上限。和模型微调类似，在大模型能力还不够的时候，我们很多的业务场景都是靠微调实现的，现在我们直接用长文本或 RAG 替代，不过还是会存在一些场景需要靠微调解决。RAG 也是一样，目前的一些 RAG 场景可能在未来用长文本会更快速、更方便，但 RAG 本身也会进步扩展，也会开发出新的场景。开发者还是需要同时掌握这两门技术的。</p><p>栾小凡：我不太认同 RAG 是 ToB 的观点，实际上我们目前的应用中有很多 ToC 场景，其中多租户就是一个非常有挑战性的问题。在应用有十万甚至百万的活跃用户时，怎样在所有的数据中精确搜索到某个租户的数据，这其实是 RAG 或向量数据库中非常重要的一个问题。OpenAI，包括 ChatGPT 中的许多能力也是围绕 RAG 和向量数据库构建的。RAG 解决的核心问题是准确帮助大模型找到所需的信息，它的本质是 retrieval。模型的能力需要提升，搜索的质量也要有提升。至于大家现在说到的分 chunk 等各种 trick，很可能会随着上下文和模型能力的提升而变得不再重要。</p><p>张颖峰：我也认同目前的一些 trick 可能在未来不再需要。我们目前这些杂乱无章的非结构化数据可能会随着未来多模态模型的能力提升，模型自己就能进行解读，但在目前我们还是要依赖 RAG 来完成这些脏活。</p><p>&nbsp;</p><p>观众提问：RAG 在 ToC 中的多租户难题是具体指什么？</p><p>栾小凡：以现在陪伴类 APP 为例，每个用户和大模型聊天时都会有上下文，用户在隔天登录时会需要对上下文的回顾，我们可以将这些聊天记录全部作为 context 输入到模型中去，但首先过多信息的输入会对大模型产生干扰，其次是成本的问题，随着聊天深入，上下文长度增长，整体的成本也会快速增加。我们需要在有限的长聊天记录中摘取出与当前对话相关的内容。对一个用户来说这点非常简单，但在数百万用户的情况下我们就会有百亿甚至千亿级别的数据量，要如何在这其中找到某个用户的数据，这其实是业界面临的一个较大挑战。</p><p></p><p>InfoQ：模型幻觉有什么常用的解决办法吗？要如何避免幻觉的出现？</p><p>唐飞虎：一个方法是 function calling，引入一些外部工具验证模型的中间生成结果。在这方面 OpenAI 的 code interpreter 就做得很好。比如说在生成一道数学题的答案后，它会立即生一段相关代码进行结果的验算。这是一种思路，我们可以把这种方法推广到更多的场景中，通过引入外部工具解决模型幻觉问题。</p><p>栾小凡：复杂的 RAG 其实在本质上就是各种工具的混合使用，前面提到的在数列中寻找乱序数的例子，真正智能的解法我认为应该是让模型生成并运行代码，这样一来无论多长的上下文模型都能找到一个最终的答案。</p><p>&nbsp;</p><p>InfoQ：现在大家最关注的就是“价格战”，比如有说百万token只要1元的，那么目前的一些大模型可能是通过哪些方式做到如此低的成本的？这其中，是否跟选择长上下文和RAG是否有关系？</p><p>唐飞虎：推理优化方面的技术有很多，OpenAI 的 GPT 3.5 Turbo 在这方面就已经很好了。这一次大厂商所谓的价格战其实更多是对一些相对较小的模型（8k、32k 等）进行降价，但在实际的开发场景中，很多开发者已经需要 128K 甚至是更长的模型，8K 的模型用户可以直接用 GPT 3.5 Turbo 来实现。这次的价格战真正能帮到开发者多少，还是要实际使用才能知道。当然，技术方面也确实是有进步的，比如量化和推理性能的提高。可以预期在未来 Q3、Q4 模型的价格和推理的成本还会进一步降低，但目前更重要的还是让长文本模型也能以较低的价格给到开发者。</p><p>张颖峰：在我看来，这次的降价除了营销因素外，技术方面与提高 Transformer 所依赖的 KV cache 利用率有很大关系。压缩 KV cache 后，推理的 batch size 就会更大，从而在解码阶段可以将内存的限制转换为计算的限制，从而实现每个 token 的降价。</p><p>栾小凡：模型本身我可能不是专家，但我之前在朋友圈看到一个有趣的观点，叫“引进 token 成本”，也就是说大模型的 token 虽然相对较贵，但同样的任务所需的 token 数可能更少。大家可能都深有感触，在模型上下文较短的时候，我们需要采取大量压缩方式或预处理，消耗很多的 token，但如果能有更加智能的模型，则回节省很多预处理的时间成本。从这个角度来看，我们更需要期待模型变得更智能，而不是过早地打价格战。</p><p>&nbsp;</p><p>&nbsp;</p><p>InfoQ：长文本的不断突破对开发者意味着什么，更长的上下文是否会对开发人员应用程序的性能产生负面影响？</p><p>唐飞虎：我认为目前对开发来说受益还是很明显的，类比 LMOS 来说，上下文就像是内存、RAG 就像是外部的硬盘，其他可能还有存储架构之类。在 DOS 时代的开发者可以用 256 KB 的内存开发出 WPS、永远的毁灭公爵这类非常厉害的应用，但如今 256 KB 可能连程序都无法启动。技术是在不断迭代的，在简单的场景和低廉的成本下，我们可以直接将上下文当作 prompt，让模型自己去判断。</p><p>栾小凡：RAG 面临的最大挑战其实是数据之间的关系。尤其是较长的文章中会存在许多代指和因果关系，这些是简单的 RAG无法解决的问题。“他拿着一个苹果”，“然后他走出了教室”，那么苹果在哪里？如果 chunking 没有做好，将这两句话切分到了不同的段落中，这其实是超过了 RAG 能解决的范畴。随着上下文能力的提升，架构的相似性让未来的 embedding 模型上下文也会随着 generation 的模型提升。智源的 Landmark embedding 可以将很长的文章切割成多个 embedding 的同时又保持了一定的上下文关系，意味着有了长上下文之后，处理这类复杂的逻辑或代指关系就有了更多的工具。</p><p>&nbsp;</p><p>InfoQ：zilliz 在增强检索技术上有什么打算？</p><p>栾小凡：对我们来说有两件事是至关重要的，一是提高搜索性能，现在的 embedding 模型非常丰富，除了传统的 dense embedding，还有 sparse embedding、ColBERT embedding、Landmark embedding 等等，这些 embedding 其实都是在不同层面上提升搜索的质量，也可以将不同的 embedding 混合使用并结合 reranking 技术，大幅提高召回的效果和质量。这其实是 Milvus 或者说向量数据库在近一年内都在专注解决的问题。</p><p>另一件事则是成本，过去做 embedding，很多时候都是用一个 PDF 文件生成几百几千个 embedding。比如说曾经很火的 autoGPT，曾有次将 vectorDB 从他们的架构中移除了，人们问 Andrew G. 用什么 vectorDB 的时候，他说因为自己只有几千的向量，所以用的 numpy。但在今年，很多应用场景都发生了巨大变化，很多模型公司在用线下数据库做模型训练和微调时，ToB 或 ToC 的应用场景中出现了一亿甚至十亿级别的数据，这就让存储的成本成为了问题的核心。我在最近和美国的一家头部 AI 公司聊的时候，发现他们每年在线上数据库中花费的钱比 OpenAI 还要多，这个问题已经成为了很多头部 AI 应用公司的诉求。现在 binary embedding 的出现就是通过量化的技术降低存储的成本，再加上其他分存存储的技术和我们之前在做的 GPU index，都是降低成本的方向。</p><p>&nbsp;</p><p>InfoQ：英飞流在增强检索技术上有什么打算？</p><p>张颖峰：我们的规划有很多，对于前面提到的 RAG 的痛点，我们有些是在数据库层面解决，有些则是站在RAG全局的角度解决。在数据库层面，我们针对用户意图明确的情况下提供了丰富的多路召回能力。目前我们的多路召回只提供两路，但在一个月后的 release 中我们会提供更加丰富的在召回形式，包括用于精确召回的全文搜索、用于语义召回的向量搜索、以及稀疏向量等等。先前在四月 IBM 提出的 blended RAG 中提出同时采用这三路召回效果可能会更好。此外，我们还会提供基于张量 Tensor 的高级召回能力，将文章中的段落用张量表示，从而实现更高级的排序和重排序能力。</p><p>&nbsp;</p><p>这些能力都是围绕用户意图非常明确的情况下，通过多样化的多路召回能力保证回答的精确、语义和性能。其次，在用户意图不明显的时候，我们计划会在今年夏天启动这方面工作，将外部知识图谱存放到数据库中，方便根据用户意图的不同来改写用户提问。这些都是站在数据库的层面来解决的问题。</p><p>在 RAG 层面，我们打算做的也有很多，比如目前我们是通过 一些小模型来解决数据本身杂乱的问题，但在未来这一定是通过多模态大模型来解决。此外还有 RAG 本身必备的排序模型。RAG 本质上是过去搜索引擎的增强，搜索引擎十年前的口号“relevance is revenue”，这直到今天也同样适用。</p><p>&nbsp;</p><p>观众提问：多路召回是否会降低性能？</p><p>张颖峰：会略微降低，毕竟在同时使用多种的搜索方式，再有就是 reranking 也就是融合排序，这会比单纯的搜索略慢。但在 RAG 系统中，瓶颈永远不会是 RAG 本身，大模型的并发是远低于 RAG 中使用的各种数据库的。即使多路召回存在性能开销，也不会成为整体系统的瓶颈。</p><p></p><p>观众提问：月之暗面什么时候可以开放 200 万字 token？</p><p>唐飞虎：目前我们也在积极扩展用户的体验资格，前段时间 Kimi 也推出了打赏的功能，一些的打赏用户也已经提前拿到功能体验了，但具体什么时候能够全量放出，还是要依据具体的研发进度判断。</p><p>&nbsp;</p><p>观众提问：关于提示词有两种观点，一种认为很重要，一种认为随着 AI 的发展提示词将会越来越无关紧要。嘉宾们对此的怎么看？</p><p>唐飞虎：找个问题要要看分析的角度，从发展的角度来看，提示词二代重要性肯定会下降，但目前阶段提示词还是很重要的，在测试 benchmark 时提示词不对结果都会不准。如果我们再将目光放得长远些，如果我们将 AI 想象成智能体或人类，我们与其交流时需要学会提问的艺术，之前 HackerNews 上一篇很值得看的文章有提到 prompt 的艺术。随着时间的发展，模型越发智能，即使提示词表达不好模型也有会理解你的意图。我们的一些用户在反馈模型回答不准确时，我们发现这些问题就算是人类也没办法很好地回答，也需要对细节进行二次询问、</p><p>我们的 Kimi 大模型中有一些场景，比如在调用外部工具时如果缺失 required 的prompt 参数，模型会反问用户这个参数是什么。这只是一个例子，未来的大模型可能会更加智能。就像是乔布斯从来不做用户调研而是为产品创造用户，因为他更深刻地理解了用户背后的需求。未来大模型这方面的能力也会提升，从一个不完整或有缺陷的 prompt 中理解用户背后的真实意图。</p><p>栾小凡：我认为 prompt 还是非常重要的，大模型的本质是 next token prediction，如果上下文都比较模糊那么后续的结果也大概率会出现问题，从更长期的角度来说，prompt 的重要性会下降，但就目前所有的模型来看，提示词还是设计给专家来使用的。大家在写代码时大模型虽然可以有所帮助，但对于一个完全不懂代码的人来说他一定会遇到各种问题，也没办法解释任何一个报错。但如果是一个代码专家，我们能够看懂模型生成代码的模式，稍加调整就能得到还算不错的结果。在未来随着模型输入手段的多元化，这种门槛可能会下降，我们可以通过图片、声音或者更多的输入方式将信息直接传递给大模型， 甚至在未来大模型也可能具备获取信息的能力。</p><p>&nbsp;</p><p></p><p>观众提问：有哪些可以有效提高 RAG 召回率和准确率的方法吗？</p><p>张颖峰：在我看来，提高 RAG 召回率和准确率的方法有两种，一是前面提到的多路召回，也就是通过多种方法将结果取回后进行统一的融合排序，从而提高 RAG 的召回率。另一种方式则相对较为前沿，例如采用ColBERT 这种基于张量的召回方式会有相对来说较好的结果，但这种方式时需要有对应的模型可以产生相应的张量。这两种方式也有相互融合的可能，站在 Infrastructure 的角度来说，我们有必要同时提供这些能力。</p><p>栾小凡：我认为是可以从两方面回答的，一是数据层面，大家提到 RAG 的第一反应都是向量数据库，但其实也有很多别的方式，比如关键词检索和图数据库的使用。在意图相对较为明确的时候，数据的组织结构是和意图和应用场景对齐。二则是复杂的 RAG 或 agent 中一些流程，比如基于问题生成答案进行搜索、ranking 、chain of thought、trail of thought，甚至是基于图等复杂构造的流程。这些都可能会在不同的场景中有所帮助，我自己在做 RAG application 的时候一般会先基于 LangChain 或 LLamx 系统做 profiling，尝试应用场景中不同组合的效果。当然，构建自己的 evaluation dataset 其实是关键，只有知道要怎样评价模型或系统，我们才能进行各种的尝试。</p><p>&nbsp;</p><p>观众提问：向量数据库的未来是纯向量化还是混合型？ RAG 要如何选择合适的向量数据库呢？</p><p>栾小凡： 在我看来万物皆可向量化。但向量不代表 dense embedding。Dense embedding 很多会关注上下文，但却在关键词方面有所欠缺，因此需要用 hybrid search 进行弥补。当然也有专门针对关键词的 embedding，比如将 dense embedding 和 sparse embedding相结合，这个我们在 Milvus 的最新版本中进行了支持。但无论怎么说，我认为搜索的未来一定是基于模型上限的提高，而不是传统的统计信息。因此，知识图谱的构建等场景也一定是通过大模型自动化来构建，而非是通过业务的规则，搜索也是一样，只有在见过更多语料后搜索的质量才会提升。所以在我看来，最终很多都会变成向量，只不过不一定是 dense embedding 而已。</p><p>&nbsp;</p><p>观众提问：数据分析趋势会是 RAG 的一个场景吗？</p><p>张颖峰：数据分析我认为严格来说不能算是 RAG，当前的数据分析更多是依靠 text-to-SQL 的方式完成，是由大模型将自然语言转换为一条查询 SQL 语句，再用这条 SQL 去查询数仓返回结果。换句话说，我们从数据库中拿到的数据并没有经过大模型的加工。因此，我认知至少在当下，数据分析不算是 RAG。至于未来，这种可能性是存在的。如果我们在拿到数据图表和分析的结果后，希望结合其他数据源获得一些更深层次的洞见 insight，那么这种未来的用例是可以归纳到 RAG 中的。</p><p>&nbsp;</p><p>观众提问：多模态大模型的 RAG 未来的路要怎么走？世界大模型能带来哪些 RAG 和向量数据库的变化？</p><p>栾小凡：最近在 GPT-4o 发布后确实出现了较多的多模态应用场景。其中我认为比较重要的一个趋势是类似之前的 AI native 和 cloud native 的多模态 native。过去大家在训练多模态模型时，往往采取对齐的手段，将 image mode 和文本 model 通过对比学习或其他方式对齐到同一语义空间之中。但随着新的模型的出现，文本 token 和图像 token 天生就是在同一个语义空间中训练。因此，这类模型的性能相较其他会更好，语义理解能力也会更强，从而诞生出很多的应用场景。其中比较有意思的是一个无人驾驶的训练场景，过去大家在做动态搜索的时候往往是以图搜图或文字搜图的形式，但这种方式其中缺乏了很重要的意图理解：针对同一个图片，用户可以询问的问题有很多，生成的 embedding 或搜索结果也会不同。然而随着多模态 embedding 的发展，这种想法逐渐变得越发现实。结合多模态的生成能力，无论是视频领域还是 GPT4 演示的语音助手领域，我认为都会有更多新场景演化出来。</p><p>张颖峰：说到多模态大模型 RAG，我认为在 RAG 本身部分可能变化没有很大，但要是想让这条链路在未来成为爆款，有更大的普及，那么瓶颈还是在模型本身。就目前来说，不说视频，光看文和图之间的生成，就已经有很多的选择了，但要是想要达到商用标准，在模型的可控性上还有很长的路要走，基于多模态的 RAG 在图像生成方面其实是非常薛定谔的，我们没有办法精准控制生成的结果。这其实是阻止我们将多模态 RAG 大规模使用的主要瓶颈，但我相信这个瓶颈的突破应该不会花太长时间。</p><p>&nbsp;</p><p>&nbsp;</p><p>观众提问：哪些 embedding model 可以同时支持多语言呢？</p><p>唐飞虎：现在支持多语言的模型有很多，大家熟悉的 OpenAI embedding 目前来看效果相对来说很不错。国内包括智源和 GenAI 在内也有很多跨语言的 embedding model，如果大家对embedding model 的效果和能力感兴趣的话，可以关注 HuggingFace 上的 MTEB 榜单，里面也有一些多语言的 benchmark。</p><p></p><p>观众提问：开发基于大模型的应用生产环境是应该考虑用 Python 还是其他的语言呢？</p><p>唐飞虎：这个问题主要取决于开发应用的量级和它依赖的生态。就我们所观察到的情况来说，开发者社群内使用的语言从 Python、Java、GoLang 到 Rust 都有，在我们的 GitHub repo 里也是 Python、NodeJS、Go、Java、C# 等等，能想要的语言应有尽有。从编程语言来说并不会对开发者有很大的限制，哪怕是只会一种语言也是可以和我们的模型有很好的交互。主要的问题在于其他的生态，对 bot 来说，Node 或 Python 更加合适，但要是想和企业内已有的服务打通，那么 Java 可能会是更好的选择。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy</id>
            <title>没想到国内大模型厂商又一次high起来，是因为OpenAI 断供！</title>
            <link>https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9RsNpcVNEZorwctMtfqy</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 06:33:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 褚杏娟, 华卫, OpenAI, API
<br>
<br>
总结: 中国大陆等地区的开发者收到了OpenAI的邮件，表示将停止不支持地区的API使用。OpenAI关闭了中国、俄罗斯、朝鲜、伊朗等国家的账户，引发了国内大模型厂商的回应，提供迁移计划和激励措施。智谱AI、百度智能云、阿里云等公司相继推出了针对OpenAI用户的替代方案和优惠活动。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;褚杏娟、华卫</p><p>&nbsp;</p><p>6月25日起，陆续有包括中国大陆在内的各国和相关地区&nbsp;API&nbsp;开发者在社交媒体上表示，他们收到了来自&nbsp;OpenAI&nbsp;的邮件，表示将采取额外措施停止其不支持的地区的&nbsp;API&nbsp;使用。</p><p>&nbsp;</p><p>根据网上流传的邮件截图，OpenAI&nbsp;表示：“根据数据显示，你的组织有来自&nbsp;OpenAl&nbsp;目前不支持的地区的&nbsp;API&nbsp;流量。从&nbsp;7&nbsp;月&nbsp;9&nbsp;日起，我们将采取额外措施，停止来自不在&nbsp;OpenAI&nbsp;支持的国家、地区名单上的&nbsp;API&nbsp;使用。”</p><p>&nbsp;</p><p>“要继续使用&nbsp;OpenAI&nbsp;的服务，您需要在受支持的地区访问该服务。”在&nbsp;OpenAI&nbsp;给出的“支持访问国家和地区”名单上（<a href="https://platform.openai.com/docs/supported-countries">https://platform.openai.com/docs/supported-countries</a>"），世界上大部分地区都可以使用&nbsp;OpenAI，包括几乎整个西方、东欧大部分地区、南亚和大约一半的非洲，但中国大陆、中国香港、俄罗斯、朝鲜、伊朗等地均未在列。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09663dc10fa89bee01622f66e59e62e3.jpeg" /></p><p></p><p>&nbsp;</p><p>而上述不受支持的中国、俄罗斯、朝鲜、伊朗四个国家，似乎“踩在”&nbsp;OpenAI&nbsp;的雷达上已有一段时间。今年&nbsp;2&nbsp;月，这家人工智能公司宣布关闭了其声称由这四个国家的&nbsp;"国家附属恶意行为者&nbsp;"使用的账户，表示他们使用&nbsp;ChatGPT&nbsp;帮助进行网络钓鱼攻击和开发恶意软件。上个月底，OpenAI&nbsp;打击了另一组来自中国、俄罗斯、伊朗和以色列的账户。</p><p>&nbsp;</p><p>实际上，OpenAI&nbsp;早先就对中国大陆地区的用户实行了注册门槛，限制了其对&nbsp;ChatGPT&nbsp;服务的访问权限。中国大陆的开发者群体在构建基于&nbsp;OpenAI&nbsp;API&nbsp;的衍生服务时，往往需要通过代理服务器或在海外部署反向代理机制。这不仅增加了运维成本，也无法保证服务的稳定性。</p><p>&nbsp;</p><p>这次，OpenAI&nbsp;的强制决策一出，便立刻引发了国内大模型厂商的回应，各厂商纷纷表示可以支持企业“无痛”迁移，并发布了不少吸引OpenAI用户使用其平台的激励措施。而根据多位行业专业人士的看法和预测，国内大模型行业内部此时也有更深层次的担忧与挑战悄然浮现。</p><p>&nbsp;</p><p></p><h2>“百模大战”</h2><p></p><p>&nbsp;</p><p>首先作出反应的是智谱&nbsp;AI。当天下午一点半左右，智谱&nbsp;bigmodel.cn&nbsp;推出了&nbsp;OpenAl&nbsp;API&nbsp;用户特别搬家计划，帮助用户切换至国产大模型，具体包括为开发者提供&nbsp;1.5&nbsp;亿&nbsp;Token（5000&nbsp;万&nbsp;GLM-4&nbsp;+1&nbsp;亿&nbsp;GLM-4-Air)&nbsp;以及从&nbsp;OpenAl&nbsp;到&nbsp;GLM&nbsp;的系列迁移培训。对于高用量客户，智谱提供与&nbsp;OpenAl&nbsp;使用规模对等的&nbsp;Token&nbsp;赠送计划(不设上限)，以及与&nbsp;OpenAl&nbsp;对等的并发规模等。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50824c5d8401618664f73da69e3ee7fb.png" /></p><p></p><p>&nbsp;</p><p>当天下午四点半左右，百度智能云千帆推出了大模型普惠计划，即日起为新注册企业用户提供：</p><p>&nbsp;0&nbsp;元调用：文心旗舰模型首次免费，赠送&nbsp;ERNIE3.5&nbsp;旗舰模型&nbsp;5000&nbsp;万&nbsp;Tokens&nbsp;包，主力模型&nbsp;ERNIE&nbsp;Speed/ERNIE&nbsp;Lite&nbsp;和轻量模型&nbsp;ERNIE&nbsp;Tiny&nbsp;持续免费；针对&nbsp;OpenAI&nbsp;迁移用户额外赠送与&nbsp;OpenAI&nbsp;使用规模对等的&nbsp;ERNIE3.5&nbsp;旗舰模型&nbsp;Tokens&nbsp;包。0&nbsp;元训练：免费模型精调训练服务0&nbsp;元迁移：零成本&nbsp;SDK&nbsp;迁移工具0&nbsp;元服务：专家服务（迁移&nbsp;&amp;使用指导）</p><p>&nbsp;</p><p>不过，百度智能云表示，以上优惠活动均在&nbsp;2024&nbsp;年&nbsp;7&nbsp;月&nbsp;25&nbsp;日&nbsp;24&nbsp;点前适用。</p><p></p><p>不到半小时后，阿里云紧接着宣布，将为OpenAI&nbsp;API用户提供最具性价比的中国大模型替代方案，并为中国开发者提供2200万免费tokens和专属迁移服务。据悉，通义千问主力模型Qwen-plus在阿里云百炼上的调用价格为0.004元/千tokens，仅为GPT-4的50分之一。根据斯坦福最新公布的大模型测评榜单HELM&nbsp;MMLU，Qwen2-72B得分为0.824，与GPT-4并列全球第四。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/863810ae0d81f916c898af4e9681830e.png" /></p><p></p><p></p><p>随后在当天六点&nbsp;20&nbsp;分左右，零一万物宣布发起了“Yi&nbsp;API&nbsp;二折平替计划”，面向&nbsp;OpenAI&nbsp;用户推出了平滑迁移至&nbsp;Yi&nbsp;系列大模型的服务，并针对接入&nbsp;OpenAI&nbsp;的不同模型的用户，一一对应地提供了高模型性能且极具性价比的替换方案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0ab82a9c92e9569dee4fbc135cb2a14.jpeg" /></p><p></p><p>&nbsp;</p><p>据介绍，目前注册使用&nbsp;Yi&nbsp;API&nbsp;的新客户，零一万物立即赠送&nbsp;100&nbsp;元额度；平台充值还将赠送&nbsp;50%&nbsp;到账额度，上不封顶，为用户提供更长线的优惠；任意充值即可享受&nbsp;RPM/TPM&nbsp;限速直升&nbsp;Tier3，直达高级别的服务质量和超快响应速度。此外，零一万物&nbsp;API&nbsp;还将提供&nbsp;Prompt&nbsp;兼容调优服务支持，陪伴用户适配&nbsp;Yi&nbsp;系列大模型。</p><p>&nbsp;</p><p>零一万物表示，在模型性能相近的同时，Yi-Large&nbsp;的定价远低于顶配模型&nbsp;GPT-4o。以&nbsp;GPT-4o&nbsp;的定价计算（取&nbsp;Input&nbsp;和&nbsp;Output&nbsp;均值为&nbsp;Open&nbsp;API&nbsp;价格），接入&nbsp;Yi-Large&nbsp;后使用成本可下降&nbsp;72%；而对比&nbsp;GPT-4&nbsp;Turbo&nbsp;的价格，用户接入&nbsp;Yi-Large-Turbo&nbsp;后使用成本可下降九成以上；对于简单任务的处理，Yi-Medium&nbsp;的使用成本较&nbsp;GPT-3.5-Turbo-1106&nbsp;下降&nbsp;66%。</p><p></p><p>此外，零一万物还可提供支持实时搜索的&nbsp;Yi-Large-RAG，适用于需要结合实时信息进行推理的场景，以便用户基于自身需求选择更匹配的模型。</p><p>&nbsp;</p><p>当日，AI&nbsp;Infra&nbsp;厂商硅基流动则直接宣布开放7款大模型：SiliconCloud&nbsp;平台的&nbsp;Qwen2(7B)、GLM4(9B)、Yi1.5（9B）等开源大模型免费使用。SiliconCloud&nbsp;是集合主流开源大模型的一站式云服务平台，目前已上架包括&nbsp;DeepSeek-Coder-V2、Stable&nbsp;Diffusion&nbsp;3&nbsp;Medium、Qwen2、GLM-4-9B-Chat、DeepSeek&nbsp;V2、SDXL、InstantID&nbsp;在内的多种开源大语言模型、图片生成模型，支持用户自由切换符合不同应用场景的模型。同时，SiliconCloud&nbsp;提供开箱即用的大模型推理加速服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2bd8e21aa6a48def16b5dd251491c8ee.png" /></p><p></p><p></p><p>当天晚&nbsp;8&nbsp;点左右，腾讯云宣布，即日起，新迁移企业用户可免费获得腾讯混元大模型&nbsp;1&nbsp;亿&nbsp;Tokens。目前，腾讯云提供混元&nbsp;Pro、Standard、Lite&nbsp;等多个不同版本和尺寸的模型，用户可任意选择。腾讯还将为新迁移企业用户提供免费专属迁移工具和服务，该福利截止&nbsp;7&nbsp;月&nbsp;31&nbsp;日&nbsp;24&nbsp;点前。</p><p>&nbsp;</p><p>今日凌晨，百川智能也跟进宣布了“零成本迁移”的措施：免费赠送&nbsp;1&nbsp;千万&nbsp;token、Assistants&nbsp;API&nbsp;免费使用。另外，百川开设了专家技术群，表示专家随时答疑，五分钟即可完成&nbsp;API&nbsp;迁移。</p><p></p><p>据悉，百川智能前不久刚发布最新一代基座大模型Baichuan&nbsp;4，并推出成立之后的首款AI助手“百小应”。Baichuan&nbsp;4相较Baichuan&nbsp;3&nbsp;在各项能力上均有极大提升，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p></p><p></p><h2>一时的机会，更卷的将来</h2><p></p><p></p><p>中美之间日益紧张的关系可能是促使&nbsp;OpenAI&nbsp;决定打击不受支持的用户的一个因素。自特朗普执政以来，美国已经对中国实施了制裁和关税，包括拜登总统增加对中国芯片、电池和电动汽车的关税。</p><p>&nbsp;</p><p>为此，中国也加大了实现技术自给自足的力度，规定其电信公司在&nbsp;2027&nbsp;年前停止使用英特尔和&nbsp;AMD&nbsp;的&nbsp;CPU，并要求其汽车制造商在&nbsp;2025&nbsp;年前至少在国内采购四分之一的计算机处理器。</p><p>&nbsp;</p><p>尽管OpenAI正计划阻止在中国的API访问，但这对中国公司来说，无疑是一个迅速填补即将到来的市场空白以获得更多用户的好机会。不过，之后国内其他厂商是否会跟进，目前尚不能确定。</p><p>&nbsp;</p><p>有专家预测，OpenAI&nbsp;主动“送生意”的做法，给了国内的大模型厂商喘气的机会，但后续可能就得被迫继续卷价格。这意味着，已经有些降温的“大模型价格战”或将再次“火热”。可以看出，国内大模型行业在机遇重重的同时，竞争也将进一步加剧。</p><p>&nbsp;</p><p>正如百川智能&nbsp;CEO&nbsp;王小川所说，“我们不需要一千、一万个大模型，在没有价格战的时候，中国可能真的是上百、上千个大模型在进行。”</p><p>&nbsp;</p><p>同时，有不少网友纷纷议论：部分国产大模型是否会因OpenAI&nbsp;的断服“现原形”。国产大模型中是否存在调用&nbsp;OpenAI&nbsp;&nbsp;API&nbsp;的这一问题暂且不论，目前根据各类大模型用户在公开平台发表的使用反馈来看，许多国产大模型的中文文本上处理能力并不弱于ChatGPT&nbsp;，在视频、图片等多模态方面有所不足，但整体来说影响有限。</p><p></p><p>不可否认的是，对于国内一批使用&nbsp;OpenAI&nbsp;&nbsp;API&nbsp;的开发者来说，影响是巨大的。还有人士对国内用户发出了“谨慎付费”的友善提醒，一些通过调用海外大模型&nbsp;API&nbsp;接口来提供服务的套壳网站，之后可能因高昂的站点迁移成本关停跑路。</p><p></p><p>此外，OpenAI强制执行其不支持国家列表的访问政策，在对中国开发者产生负面影响的同时，也可能带来其他方面的双向后果。</p><p>&nbsp;</p><p>根据Reddit上的一篇帖子，总部位于美国的云平台公司Vercel的用户如果通过Vercel的边缘网络访问OpenAI，也会收到同样的OpenAI邮件。目前还不清楚这封电子邮件是否发送有误，但&nbsp;Vercel&nbsp;的边缘网络确实有一个位于香港的区域，与中国大陆一样不受&nbsp;OpenAI&nbsp;支持。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jAuKRqF6FkHsq0RtmAuR</id>
            <title>FCon x AICon联诀来袭、干货翻倍，8折倒计时最后5天</title>
            <link>https://www.infoq.cn/article/jAuKRqF6FkHsq0RtmAuR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jAuKRqF6FkHsq0RtmAuR</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 03:36:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ, FCon, AICon, 金融科技
<br>
<br>
总结: InfoQ旗下的FCon全球金融科技大会和AICon全球人工智能开发与应用大会将在上海联合举办，涵盖金融科技和人工智能领域的前沿技术和实践经验。FCon聚焦金融AI大模型落地实践，AICon深入剖析RAG等前沿技术及其应用，两大会议将为参与者带来丰富的干货和一站式体验。 </div>
                        <hr>
                    
                    <p>8月16日-19日，InfoQ旗下FCon&nbsp;全球金融科技大会和&nbsp;AICon&nbsp;全球人工智能开发与应用大会上海站将联诀来袭，AI+金融、技术前沿+行业前沿，双会联动、一站式体验、干货翻倍！</p><p></p><h3>FCon：囊括近1年金融AI大模型落地实践</h3><p></p><p></p><p>FCon是InfoQ推出的首个垂直行业大会，2023年第一届FCon大会在上海成功举办，来自中国工商银行、中国民生银行、平安银行、兴业银行、中信银行、北京银行&nbsp;、苏州银行、汇丰科技、国泰君安证券、平安人寿、阳光保险集团、度小满、蚂蚁集团等公司的上百位业界专家，现场分享了关于金融企业数字化转型的最新研究成果和实践经验。</p><p></p><p><a href="https://fcon.infoq.cn/2024/shanghai/">2024年FCon</a>"将于8月16日-17日在上海举办，由中国信通院铸基计划作为官方合作机构，以“科技驱动，智启未来——激发数字金融内生力”为主题。在“十四五”收官之际，本届大会将致力于展示金融数字化在“十四五”期间的关键进展，帮助金融机构更具针对性地“查缺补漏”。同时，聚焦金融行业在数智化的全面革新，紧跟当下技术热点，分享近一年来金融行业AI大模型的落地实践经验和成果。</p><p></p><p>原国有大型商业银行资深业务架构师、天润聚粮执行董事总经理付晓岩，度小满金融技术委员会执行主席、数据智能应用部总经理杨青已确认担任本届大会领航团联席主席。目前11个专题内容已就绪，中国信通院泰尔终端实验室数字生态发展部主任王景尧、极客邦科技CGO/InfoQ 极客传媒&amp;极客时间企业版总经理汪丹（Yolanda）、广发银行信用卡中心商业智能负责人徐小磊、北京银行软件开发中心副总经理代铁、平安银行金融科技部数据资产管理及研发中心数据及AI团队负责人廖晓格、方正中期期货数字科技中心负责人张志明、文因互联董事长/创始人鲍捷博士、Aloudata 大应科技周卫林、宇谷金融科技研究院院长吴易璋、飞轮科技CTO王猛等将作为出品人为专题内容品控献计献策。</p><p></p><p>此外，已有近20位嘉宾已确认参与FCon大会议题分享：</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02c6d0a4695670c8c017a2c6fd667c65.png" /></p><p></p><p>点击链接可查看更多大会详情：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><h3>AICon：深入剖析RAG等前沿技术及其应用</h3><p></p><p></p><p><a href="https://aicon.infoq.cn/2024/shanghai">AICon&nbsp;全球人工智能开发与应用大会</a>"是InfoQ面向各行业对人工智能和大模型感兴趣的资深工程师、产品经理、数据分析师&nbsp;推出的人工智能和大模型技术盛会。在5月17-18日北京站，我们紧扣热点，坚持实践驱动，为超过&nbsp;1000&nbsp;位观众留下了深刻印象。</p><p></p><p>AICon&nbsp;上海站将于&nbsp;8月18日-19日举办，本次大会邀请了蚂蚁集团&nbsp;蚂蚁超级计算部负责人余锋（褚霸）、Lepton&nbsp;AI&nbsp;联合创始人&nbsp;&amp;&nbsp;CEO&nbsp;贾扬清、北京智源人工智能研究院副院长兼总工程师林咏华、百川智能技术联合创始人谢剑担任联席主席。</p><p></p><p>目前多位专题出品人已确认，包括科大讯飞&nbsp;AI&nbsp;研究院副院长/科研部部长李鑫、阿里巴巴总监郭瑞杰、Seasalt.ai&nbsp;CEO&nbsp;姚旭晨、阿里巴巴企业智能算法负责人陈祖龙、DeepWisdom（MetaGPT）&nbsp;创始人兼&nbsp;CEO&nbsp;吴承霖、阅文集团技术副总经理&nbsp;&amp;&nbsp;AIGC&nbsp;负责人陈炜于、字节跳动&nbsp;Code&nbsp;AI&nbsp;团队技术负责人杨萍、京东高级技术总监周默、智源智能评测组负责人杨熙等，他们将负责精选针对特定主题论坛的高质量议题，确保论坛的技术深度。</p><p></p><p>此外，已有近10位嘉宾已确认参与大会议题分享：</p><p><img src="https://static001.geekbang.org/infoq/1b/1b0d078a00653ce775e37658524167b5.png" /></p><p></p><p>点击链接可查看更多大会详情：<a href="https://aicon.infoq.cn/2024/shanghai">https://aicon.infoq.cn/2024/shanghai</a>"</p><p></p><h5>优惠购票</h5><p></p><p>目前是FCon、AICon两场大会&nbsp;8&nbsp;折优惠期，单场单张门票节省&nbsp;960&nbsp;元（原价&nbsp;4800&nbsp;元），特惠折扣最后5天倒计时，详情可扫码或联系13269078023&nbsp;咨询票务经理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46472b60308a2aa865d8d80412971e56.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kgFyRwZx2ruihJlcZDRv</id>
            <title>大模型训练检查点写入速度相比 PyTorch 加快 116 倍！微软提出FastPersist 新方法</title>
            <link>https://www.infoq.cn/article/kgFyRwZx2ruihJlcZDRv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kgFyRwZx2ruihJlcZDRv</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 02:13:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软DeepSpeed, FastPersist, 检查点写入速度, NVMe SSDs
<br>
<br>
总结: 微软DeepSpeed团队提出了FastPersist技术，通过优化NVMe SSDs的使用、提高写入并行性，以及实现检查点操作与独立训练计算的重叠，显著提升了检查点的创建速度，降低了训练过程中的I/O开销。 </div>
                        <hr>
                    
                    <p></p><blockquote>近日，微软&nbsp;DeepSpeed&nbsp;研究组发布最新论文，提出一种名为&nbsp;FastPersist&nbsp;的新方法，旨在解决大模型训练时写检查点十分耗时的问题，相比&nbsp;PyTorch&nbsp;基线，写入速度提升超过&nbsp;100&nbsp;倍。</blockquote><p></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e9fa72d4738a423bd5b0711c2189dd3f.png" /></p><p></p><p></p><p>深度学习作为推动人工智能发展的关键技术，其模型检查点（checkpoint）的生成对于确保训练过程的稳定性和容错性至关重要。然而，随着模型规模的不断扩大，传统的检查点写入方法已经无法满足日益增长的I/O需求，成为制约深度学习发展的瓶颈。FastPersist技术的提出，正是为了解决这一问题。</p><p></p><p>FastPersist&nbsp;是微软DeepSpeed团队针对深度学习模型训练中检查点创建效率低下的问题提出的解决方案。据介绍，这项技术的核心在于通过三种创新的方法，即优化NVMe&nbsp;SSDs的使用、提高写入并行性，以及实现检查点操作与独立训练计算的重叠，显著提升了检查点的创建速度，降低了训练过程中的I/O开销。实验结果表明，FastPersist能够在几乎不影响训练性能的前提下，实现高达116倍的检查点写入速度提升。这项技术的提出，不仅解决了大规模深度学习训练中的一个关键问题，也为未来深度学习模型的进一步发展提供了强有力的技术支持。</p><p></p><p>AI&nbsp;前线进一步了解到，&nbsp;在微软很多重要的大模型训练中，由于工作负载高度密集，经常出现&nbsp;GPU&nbsp;error，所以需要很高频地写检查点操作，而这些大模型训练其实都在使用&nbsp;FastPersist&nbsp;这套系统。</p><p></p><p>论文链接：<a href="https://arxiv.org/pdf/2406.13768">https://arxiv.org/pdf/2406.13768</a>"</p><p></p><p></p><h2>现状和问题</h2><p></p><p></p><p>深度学习作为人工智能领域的一个重要分支，近年来在图像识别、自然语言处理、推荐系统等多个领域取得了突破性进展。随着研究的深入，深度学习模型的规模也在不断扩大，从早期的百万级参数模型发展到现在的百亿甚至千亿级参数的超大型模型。模型规模的增长带来了更强的表征能力和更高的准确率，但同时也带来了计算复杂度的提升和存储需求的增加。特别是模型参数、梯度信息以及中间特征图等数据的存储，对存储系统的I/O性能提出了更高的要求。</p><p></p><p>尽管计算性能的提升可以通过硬件加速和算法优化来实现，但I/O性能的提升却受到了传统存储设备和系统的限制。特别是在模型训练过程中，检查点的生成是一个不可或缺的步骤，用于保存模型在特定迭代步骤的状态，以便在发生故障时能够从最近的检查点恢复训练，从而避免重复计算。然而，检查点的生成和保存是一个资源密集型的操作，涉及到大量的数据写入。在大规模训练中，模型参数和中间数据的体积巨大，检查点的生成和保存需要消耗大量的I/O带宽和时间，这不仅增加了训练的总体时间，也可能导致I/O系统的饱和，影响其他训练操作的执行。因此，提高检查点创建的效率，成为提升深度学习模型训练性能的关键。</p><p></p><p>当前深度学习框架中的检查点生成机制，大多数基于传统的文件I/O操作，这些操作并没有充分利用现代存储设备，如NVMe&nbsp;SSDs的高性能特性。这导致了在大规模训练场景下，检查点写入成为制约整体性能的瓶颈。此外，由于检查点写入操作与模型训练的其他计算任务之间存在数据依赖性，传统的检查点生成方法无法实现与训练过程的完全解耦，进一步限制了检查点生成的效率。</p><p></p><p>为了解决I/O瓶颈问题，研究者和工程师们提出了多种解决方案，如使用更快的存储介质、优化文件系统、改进数据写入策略等。但是，这些解决方案往往存在一定的局限性。例如，简单地更换更快的存储介质虽然可以提高I/O性能，但成本较高，且在大规模并发写入时仍可能遇到瓶颈。优化文件系统和数据写入策略可以在一定程度上提高效率，但往往需要对现有的深度学习框架和训练流程进行较大的改动，兼容性和通用性有待提高。</p><p></p><p>针对上述问题，微软DeepSpeed团队提出了FastPersist技术。</p><p></p><p></p><h2>FastPersist&nbsp;技术方案</h2><p></p><p></p><p>FastPersist&nbsp;通过深入分析深度学习训练过程中的I/O需求和特点，结合现代存储设备的特性，提出了一种全新的检查点生成和保存方法。主要通过以下三个方面来提升检查点创建的效率：</p><p></p><h3>1. NVMe存储设备的优化利用</h3><p></p><p>FastPersist&nbsp;针对NVMe&nbsp;SSDs的高性能特性进行了优化。通过使用专为NVMe设计的I/O库，如libaio和io_uring，FastPersist能够更高效地管理数据在GPU和SSD之间的传输，从而显著提高了单节点上的检查点写入速度。</p><p></p><p>FastPersist还采用了双缓冲技术来进一步提高写入效率。在双缓冲机制中，当一个缓冲区的数据正在写入SSD时，另一个缓冲区可以同时从GPU内存中预取数据，这样就能实现数据写入和数据预取的流水线操作，减少了等待时间，提高了整体的写入性能。</p><p></p><p>另外，&nbsp;FastPersist针对NVMe&nbsp;SSDs的特性，对数据块的大小和对齐进行了优化。通过调整数据块的大小，使其匹配SSD的页面大小，可以减少写入操作的数量，提高写入效率。同时，通过对齐数据块到合适的边界，可以避免额外的拷贝操作，进一步提高性能。</p><p></p><p></p><h3>2.&nbsp;写入并行性的实现</h3><p></p><p>在深度学习模型训练中，特别是在大规模分布式训练环境中，数据并行（Data&nbsp;Parallelism）是一种常见的训练策略。在数据并行训练中，模型被复制到多个训练节点上，每个节点处理不同的数据子集。这种训练方式可以显著提高计算资源的利用率，加快模型的训练速度。然而，如果检查点的写入操作仍然集中在单个节点上执行，那么I/O操作就可能成为限制整体性能的瓶颈。</p><p></p><p>FastPersist技术通过实现检查点写入的并行性，解决了这一问题。在FastPersist中，检查点的写入操作被分布到所有参与训练的节点上，每个节点只负责写入其对应的模型部分。这样，写入操作就可以同时在多个节点上执行，从而显著提高了整体的写入速度。</p><p></p><p>为了实现高效的写入并行性，FastPersist采用了以下几个关键策略：</p><p>数据分片：FastPersist将检查点数据均匀地分割成多个片段，每个训练节点只负责写入其分配到的数据片段。这种分片策略确保了写入负载在所有节点上的均衡分配。无通信写入：在FastPersist中，每个节点独立地完成其检查点数据片段的写入，无需与其他节点进行通信或协调。这种设计减少了节点间通信的开销，提高了写入操作的效率。动态负载平衡：FastPersist能够根据节点的计算能力和存储性能动态调整数据片段的大小，确保所有节点的写入负载保持均衡。这种动态调整机制可以适应不同的硬件环境和训练配置。容错和恢复：在分布式训练环境中，节点的故障是不可避免的。FastPersist通过在写入操作中实现容错机制，确保即使部分节点发生故障，也不会影响检查点的完整性和训练的连续性。</p><p></p><p></p><h3>3.&nbsp;操作重叠的策略</h3><p></p><p></p><p>在深度学习模型训练中，检查点的生成通常需要在每个训练迭代后执行，以确保模型状态的持久化。然而，如果每次迭代后都进行完整的检查点写入操作，那么这些操作可能会占用大量的计算资源，影响模型训练的速度。为了解决这一问题，FastPersist采用了操作重叠的策略，将检查点的写入操作与模型训练的其他计算任务并行执行。</p><p></p><p>操作重叠的核心思想是利用深度学习训练中的计算特性，将检查点写入操作与模型的前向传播和后向传播操作重叠。由于前向传播和后向传播操作通常占据了模型训练的大部分时间，通过将检查点写入操作与这些操作并行化，可以有效地隐藏I/O操作的延迟，提高整体的训练效率。</p><p></p><p>FastPersist实现操作重叠的具体策略包括：</p><p>异步写入：FastPersist采用异步写入机制，使得检查点的写入操作不会阻塞计算操作的执行。在每个训练迭代的优化器步骤之后，FastPersist会启动检查点的异步写入过程，而计算线程可以继续执行下一个迭代的前向传播和后向传播。双线程模型：FastPersist引入了一个辅助线程专门负责检查点的写入操作。主线程负责执行模型的计算任务，而辅助线程在主线程的协调下执行检查点的写入。这种双线程模型确保了计算和I/O操作的并行执行，减少了相互之间的干扰。数据局部性优化：FastPersist通过优化数据的存储和访问模式，提高了数据在GPU和CPU之间的传输效率。通过利用数据的局部性原理，FastPersist减少了不必要的数据移动，降低了I/O操作的延迟。依赖性管理：在操作重叠的过程中，FastPersist通过精确管理计算任务和检查点写入操作之间的数据依赖性，确保了检查点的一致性和完整性。即使在发生故障的情况下，FastPersist也能够保证从最近的检查点正确恢复。</p><p></p><p>通过精心设计的操作调度策略，FastPersist实现了检查点写入操作与模型训练的其他计算任务的重叠执行，从而在不增加额外计算负担的情况下，规避检查点的写入延迟。</p><p></p><p></p><h2>效果评估</h2><p></p><p></p><p>研究团队对&nbsp;FastPersist的性能表现进行多场景、多维度的评估。为了验证NVMe&nbsp;优化和并行优化在减少检查点延迟方面的有效性，团队使用单GPU和多节点环境的微基准测试，对检查点写入的吞吐量做了测试；并使用真实世界的密集和稀疏深度学习模型，评估了新方法相比基线（baseline）对训练性能的加速效果。</p><p></p><p>在微基准测试中，FastPersist在单GPU和多节点环境下，相比于基线的torch.save()方法，检查点写入速度显著提升。</p><p></p><p>在真实世界的深度学习模型训练测试中，FastPersist在不同的模型规模和数据并行度下，均能够实现高速的检查点创建，且引入的开销极小。下图显示，在128个V100&nbsp;GPU上，FastPersist实现的加速比从gpt3-13B的28倍到gpt3-0.7B的116倍不等。这些改进证明了FastPersist&nbsp;技术方案在&nbsp;NVMe&nbsp;优化和并行优化方面的有效性。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57289e6d3f967d9b5c4654f6702e1f07.png" /></p><p></p><p>图：FastPersist&nbsp;应用于GPT-3密集模型训练的效果</p><p></p><p>FastPersist在大规模训练场景下的性能尤为重要。实验结果表明，即使在数千个GPU上进行训练，FastPersist也能够保持检查点创建的低开销，并且随着数据并行度的增加，FastPersist的效率提升更加明显。</p><p></p><p>鉴于GPU硬件的限制，研究团队通过预测高达128的数据并行度（即6.7B模型使用1024个GPU，13B模型使用2048个GPU）来模拟像GPT-3&nbsp;6.7B和13B这样的大型密集模型的性能表现。下图显示了FastPersist相对于基线的预计训练加速比，其中蓝色/橙色条代表6.7B/13B模型。当扩展到数千个GPU时，FastPersist&nbsp;的检查点开销基本保持一致（小于2%的训练计算时间），而基线的检查点开销则与数据并行度成比例增长。对于6.7B和13B模型，FastPersist&nbsp;分别实现了高达10.2倍和3.6倍的训练加速。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb046e0eb7930ec2b185d75baf3a4f1d.png" /></p><p></p><p>图：数据并行度≤128的训练加速效果预测</p><p></p><p>另外如上图中灰色条所示，如果放弃流水线并行（PP），并在一个数据并行组中完全采用&nbsp;16&nbsp;个&nbsp;GPU&nbsp;的张量并行（TP）设置，与标准TP和PP结合的模型分割（即图中的橙色条）相比，FastPersist&nbsp;可以做到更高的基线加速比，实现高达11.3倍的训练加速。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cLFzIsK51LR29CQSZ6qv</id>
            <title>AI视频技术突破静默，让每一帧画面实现声色同步 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/cLFzIsK51LR29CQSZ6qv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cLFzIsK51LR29CQSZ6qv</guid>
            <pubDate></pubDate>
            <updated>Wed, 26 Jun 2024 01:44:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 视频生成模型, V2A技术, 多模态智适应
<br>
<br>
总结: 大模型的快速发展推动了视频生成模型技术的竞争，V2A技术为解决视频声音局限性提供了新方案，多模态智适应大模型在教育领域有重要应用。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>视频生成模型的发展速度令人瞩目，在人工智能领域的竞争已经达到了白热化阶段。各大厂商之间的激烈角逐不仅推动了技术的快速进步，也为整个行业营造了一个更加公正和开放的竞争环境。尽管如此，从年初令人瞩目的Sora到近期的可灵、Luma、Gen-3&nbsp;Alpha等模型，它们所生成的视频作品均未能突破声音的局限。然而，Google&nbsp;DeepMind推出的V2A技术，为这一问题提供了解决方案。从技术应用来看，V2A技术与Veo等视频生成模型的结合，将能够创造出既具有戏剧性配乐、逼真音效，又能与视频中的角色、风格完美融合的对话镜头。这一创新标志着AI视频即将告别无声时代，迎来一个充满活力、充满创新的有声世界。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>6月19日，中国气象局发布人工智能全球中短期预报系统“风清”、人工智能临近预报系统“风雷”和人工智能全球次季节—季节预测系统“风顺”。这三个大模型都是由中国气象局与清华大学联合攻关团队开发的。这三个大模型完成了基于国产全球大气再分析资料CRA-40、雷达观测资料、风云卫星遥感资料的训练和检验评估，显著降低了当前主流气象预报大模型对国际再分析资料的依赖，提升了自主数据源的应用效率和准确性。6月21日，Anthropic发布最新大模型Claude&nbsp;3.5&nbsp;Sonnet，拥有前代模型2倍的推理速度和1/5的调用成本，在多项评测中超过了GPT-4o。</p><p></p><h4>多模态领域</h4><p></p><p>6&nbsp;月&nbsp;17&nbsp;日，Runway公司发布了其最新力作——视频生成基础模型Gen-3&nbsp;Alpha，该模型能够生成包含丰富场景变换、多样电影风格以及精细艺术指导的视频作品。6&nbsp;月&nbsp;18&nbsp;日，松鼠Ai全新多模态智适应大模型发布会在上海召开，不仅宣布了教育大模型及系统的全方位升级，还推出了多款全新智适应教育硬件产品。在大模型方面，全新多模态智适应大模型在多模态智能错因分析与追根溯源、多模态智能人机互动、多模态智能测试与评估三大维度进行了全面迭代。在硬件方面，松鼠Ai推出了三款全新松鼠Ai智能老师——S211白鹭松鼠Ai智能老师、S139松鼠Ai智能老师以及Z29松果Ai智能老师，能给匹配不同用户需求，并全系搭载松鼠Ai最新多模态智适应教育大模型。</p><p></p><h4>开源领域</h4><p></p><p>6月15日，英伟达宣布推出&nbsp;Nemotron-4&nbsp;340B，其包含一系列开放模型，可用于生成合成数据，训练大语言模型，以及所有行业的商业应用。6月18日，潞晨&nbsp;Open-Sora&nbsp;团队在&nbsp;720p&nbsp;高清文生视频质量和生成时长上实现了突破性进展，支持无缝产出任意风格的高质量短片，模型权重和训练代码已经全面开源。6月18日，基于文本生成音效工具，ElevenLabs开源视频生成音效工具。无需寻找合适的音效，用户可以通过输入文本来生成配音，且大部分音效具有Shutterstock&nbsp;的商业授权。6月19日，B站开源了轻量级&nbsp;Index-1.9B&nbsp;系列模型，包含基座模型、对照组、对话模型、角色扮演模型等多个版本。6月19日，Hedra&nbsp;Labs发布视频生成模型Character-1的研究预览版，对多平台用户开放使用。Character-1是一款能够通过文本和图片生成说话和唱歌视频的模型，最长支持60秒的免费体验，还是一个全新的创作平台，为用户提供视频创作机会。</p><p></p><h4>科研领域</h4><p></p><p>6月16日，由上海科技大学、影眸科技以及宾夕法尼亚大学联合研发的DressCode，标志着3D服装生成技术的重大突破。作为首个全面支持CG操作，并无缝融入工业生产流程的框架，DressCode通过文本驱动的方式，能够自动生成具备卓越渲染品质、高度可编辑性、可驱动性以及仿真特性的3D服装。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能新动态</h4><p></p><p>6&nbsp;月&nbsp;17&nbsp;日，谷歌DeepMind发布了V2A技术进展，该技术可以结合视频像素和自然语言文本提示，为无声视频添加逼真的音效，能够实现同步的视听生成。6月19日，OpenAI宣布和Color&nbsp;Health&nbsp;合作，探索通过GPT-4o创建AI工具Cancer&nbsp;Copilot，帮助医生根据患者数据制定筛查和治疗计划，从而能够就癌症筛查和治疗做出更加合理的决策。6月19日，Meta发布AudioSeal，一款音频水印技术，能在音频片段中精准识别AI生成的音频内容。6月19日，月之暗面Kimi开放平台将启动Context&nbsp;Caching内测，将支持长文本大模型以及上下文缓存机制。6月19日，前小度CEO景鲲和前小度CTO朱凯华联合创立的AI创新产品公司MainFunc推出了旗下首款AI&nbsp;Agent搜索产品GenSpark。该产品是一款AI&nbsp;Agent引擎，旨在“利用AI提供更好的搜索体验”。6月19日，Luma&nbsp;AI对其视频生成模型Dream&nbsp;Machine进行了重大更新，推出了Extend功能。这项新功能允许用户在保持原有视频风格和人物特征一致性的前提下，将原本生成的5秒视频延长至10秒以上。6月20日，百度智能云的曦灵数字人平台即将经历一次重大升级。此次升级不仅优化了2D和3D数字人的生成过程，实现了成本效益和效率的双重提升，而且还在直播、短视频和对话等多种应用场景中实现了无缝集成。用户仅需提供一段简短的描述，系统便能迅速模仿人类的创意思维，仅需10分钟就能自动创造出栩栩如生的3D数字人形象。</p><p></p><h4>智能体</h4><p></p><p>6月20日，斯坦福大学研究人员研发了一款仿人机器人HumanPlus，这款机器人可以模仿人类的行为，并支持模仿动作来进行学习，例如自主叠衣服、搬运物品、弹钢琴等。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><p></p><p></p><h4>活动推荐</h4><p></p><p>InfoQ&nbsp;将于&nbsp;8&nbsp;月&nbsp;18&nbsp;日至&nbsp;19&nbsp;日在上海举办&nbsp;AICon&nbsp;全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6&nbsp;月&nbsp;30&nbsp;日前可以享受&nbsp;8&nbsp;折优惠，单张门票节省&nbsp;960&nbsp;元（原价&nbsp;4800&nbsp;元），详情可联系票务经理&nbsp;13269078023&nbsp;咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff3179d061840fbb7821303961c91a65.jpeg" /></p><p></p><p></p><p>原文链接：https://aicon.infoq.cn/2024/shanghai/schedule?utm_source=wechat&amp;utm_medium=aiart2-0624</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9PjLEHC7BKMGzGQLRzQz</id>
            <title>OpenAI一停服，国内大模型厂商抢生意“抢疯”了</title>
            <link>https://www.infoq.cn/article/9PjLEHC7BKMGzGQLRzQz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9PjLEHC7BKMGzGQLRzQz</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 11:56:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, API, 中国大陆, 大模型厂商
<br>
<br>
总结: 北京时间周二凌晨，OpenAI向不支持地区的API开发者发出停止使用通知，中国大陆等地未在支持名单中。国内大模型厂商纷纷推出迁移计划，包括智谱AI、百度智能云和零一万物，提供各种优惠政策和服务，以支持企业平稳迁移至国产大模型。硅基流动也宣布免费使用多个顶尖开源大模型，为开发者提供更全面的模型API体验。 </div>
                        <hr>
                    
                    <p>北京时间周二凌晨，陆续有包括中国大陆在内的各国和相关地区API开发者在社交媒体上表示，他们收到了来自OpenAI的邮件，表示将采取额外措施停止其不支持的地区的API使用。</p><p>&nbsp;</p><p>根据网上流传的邮件截图，OpenAI表示：“根据数据显示，你的组织有来自OpenAl目前不支持的地区的API流量。从7月9日起，我们将采取额外措施，停止来自不在OpenAI支持的国家、地区名单上的API使用。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1c/1c814d651b4574777182df8ed8a1370d.jpeg" /></p><p>&nbsp;</p><p>在 OpenAI 给出的“支持访问国家和地区”名单上（<a href="https://platform.openai.com/docs/supported-countries">https://platform.openai.com/docs/supported-countries</a>"），中国大陆、中国香港、俄罗斯、朝鲜、叙利亚、伊朗等地均未在列。</p><p>&nbsp;</p><p>实际上，OpenAI 早先就对中国大陆地区的用户实行了注册门槛，限制了其对 ChatGPT 服务的访问权限。中国大陆的开发者群体在构建基于 OpenAI API 的衍生服务时，往往需要通过代理服务器或在海外部署反向代理机制。这不仅增加了运维成本，也无法保证服务的稳定性。</p><p>&nbsp;</p><p>OpenAI 的这一决策立刻引发了国内大模型厂商的回应，各厂商纷纷表示可以支持企业“无痛”迁移。</p><p>&nbsp;</p><p></p><h3>智谱AI：企业最低6折</h3><p></p><p></p><p>首先作出反映的的是智谱AI。当天下午一点半左右，智谱 bigmodel.cn 推出了OpenAl AP1 用户特别搬家计划，帮助用户切换至国产大模型，具体包括为开发者提供1.5亿 Token（5000万 GLM-4 +1亿 GLM-4-Air) 以及从 OpenAl 到GLM 的系列迁移培训。对于高用量客户，智谱提供与 OpenAl 使用规模对等的 Token 赠送计划(不设上限)，以及与OpenAl 对等的并发规模等。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/8d/8d16ff18535392932a96061c8262983e.png" /></p><p></p><p></p><h3>百度智能云：限时零成本迁移</h3><p></p><p></p><p>下午四点半左右，百度智能云千帆推出了大模型普惠计划，即日起为新注册企业用户提供：</p><p>&nbsp;</p><p>0元调用：</p><p>文心旗舰模型首次免费，赠送ERNIE3.5旗舰模型5000万Tokens包，主力模型ERNIE Speed/ERNIE Lite和轻量模型ERNIE Tiny持续免费；针对OpenAI迁移用户额外赠送与OpenAI使用规模对等的ERNIE3.5旗舰模型Tokens包。</p><p>0元训练：免费模型精调训练服务</p><p>0元迁移：零成本SDK迁移工具</p><p>0元服务：专家服务（迁移&amp;使用指导）</p><p>&nbsp;</p><p>不过，百度智能云表示，以上优惠活动均在2024年7月25日24点前适用。</p><p></p><h3>零一万物：Yi API 二折平替计划</h3><p></p><p>&nbsp;</p><p>随后在六点20分左右，零一万物宣布发起了“Yi API 二折平替计划”，面向 OpenAI 用户推出了平滑迁移至 Yi 系列大模型的服务。针对接入OpenAI 的不同模型的用户，零一万物一一对应地提供了高模型性能且极具性价比的替换方案。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0b/0b4462363ecd3513a81dd74778a1e76e.jpeg" /></p><p>&nbsp;</p><p>零一万物介绍，目前注册使用 Yi API 的新客户，零一万物立即赠送 100 元额度，帮助用户完成平稳过渡；平台充值还将赠送 50% 到账额度，上不封顶，为用户提供更长线的优惠；任意充值即可享受RPM/TPM 限速直升 Tier3，直达高级别的服务质量和超快响应速度。此外，零一万物 API 还将提供Prompt 兼容调优服务支持，陪伴用户又好又快地适配 Yi 系列大模型。</p><p></p><p>零一万物表示，从模型评测成绩、API 价格等公开数据来看，对于原先接入 GPT-4o 的用户来说，无论是在模型性能、还是在使用成本方面，接入零一万物千亿参数旗舰模型 Yi-Large 都会是 “物美价廉” 的国产大模型平替方案。</p><p>&nbsp;</p><p>另外，在模型性能相近的同时，Yi-Large 的定价远低于顶配模型 GPT-4o。以 GPT-4o 的定价计算（取 Input 和 Output 均值为 Open API 价格），接入 Yi-Large 后使用成本可下降 72%。</p><p></p><p>对于原先使用 GPT-4 Turbo 的用户，零一万物也给出了平滑迁移至 Yi-Large-Turbo 的方案。零一万物表示，对比 GPT-4 Turbo 的价格，用户接入 Yi-Large-Turbo 后使用成本可下降九成以上。对于业务产品已经验证成立，需要降低成本的客户， Yi-Large-Turbo 会非常适用。此外，零一万物还可提供支持实时搜索的 Yi-Large-RAG，适用于需要结合实时信息进行推理的场景，以便用户基于自身需求选择更匹配的模型。</p><p></p><p>在 OpenAI API 中，GPT-3.5-Turbo-1106 聚焦于处理简单任务，主打快速、廉价。而零一万物提供了更高性价比的方案——中等尺寸模型 Yi-Medium 来完美承接用户需求，使用成本较 GPT-3.5-Turbo-1106 下降 66%。虽然仅为中等尺寸模型，但是 Yi-Medium 深度优化了指令遵循能力，适用于日常聊天、翻译等通用场景，非常匹配大规模应用大模型的需求。</p><p></p><h3>硅基流动：多个大模型免费使用</h3><p></p><p></p><p>AI Infra厂商硅基流动则宣布：SiliconCloud 平台的 Qwen2(7B)、GLM4(9B)、Yi1.5（9B）等顶尖开源大模型免费使用。换言之，开发者从此实现了“Token自由”。</p><p></p><p>SiliconCloud是集合主流开源大模型的一站式云服务平台，为开发者提供更快、更便宜、更全面、体验更丝滑的模型API。目前，SiliconCloud已上架包括DeepSeek-Coder-V2、Stable Diffusion 3 Medium、Qwen2、GLM-4-9B-Chat、DeepSeek V2、SDXL、InstantID在内的多种开源大语言模型、图片生成模型，支持用户自由切换符合不同应用场景的模型。同时，SiliconCloud提供开箱即用的大模型推理加速服务，为生成式AI应用带来更高效的用户体验。</p><p></p><p></p><h3>腾讯：1亿 Tokens免费赠，模型任选</h3><p></p><p></p><p>也是在晚8点左右，腾讯云宣布，即日起，新迁移企业用户可免费获得腾讯混元大模型1亿Tokens。目前，腾讯云提供混元Pro、Standard、Lite等多个不同版本和尺寸的模型，用户可任意选择。</p><p></p><p>腾讯还将为新迁移企业用户提供免费专属迁移工具和服务，让好用、易用、实用的大模型惠及更多人。该专属福利截止7月31日24点前。</p><p></p><p></p><h3>百川智能：送1 千万 token，设专家群答疑</h3><p></p><p></p><p>凌晨，百川智能也跟进宣布，免费赠送1 千万 token、Assistants API免费使用。另外，百川开设了专家技术群，表示专家随时答疑，五分钟即可完成 API迁移。</p><p></p><p>国内其他厂商是否会跟进，我们将持续为大家跟踪报道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SlHAKbcHDXtpHJWkbpAo</id>
            <title>华为云 AI Agent 实战：三步构建，七步优化，看智能体如何进入企业生产 | AICon</title>
            <link>https://www.infoq.cn/article/SlHAKbcHDXtpHJWkbpAo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SlHAKbcHDXtpHJWkbpAo</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 07:35:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI Agent, 企业生产场景, 大模型技术, 华为云
<br>
<br>
总结: 在AICon 北京站上，华为云aPaaS首席架构师陈星亮分享了《AI Agent 在企业生产中的技术实践》，探讨了AI Agent在企业生产场景中面临的挑战和解决方案。华为云通过实际场景的实践，采取多方面技术，解决企业引入AI生成技术的瓶颈，使得AI Agent在企业生产场景得以成功运用。华为云在内部实施AI Agent技术进入生产场景的策略，分为三个阶段和七个步骤，旨在使技术团队深入理解并有效运用AI Agent技术，同时让业务团队明确AI Agent的适用场景。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>在AICon 北京站上，InfoQ 邀请了华为云aPaaS首席架构师陈星亮，分享了《AI Agent 在企业生产中的技术实践》，本文为演讲整理，期待对你有所启发。在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了【AI Agent 技术突破与应用】专题，本专题将深入探讨 AI Agent 的当前技术现状与发展趋势，揭示其在各行业中的广泛应用和未来潜力。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p>大模型技术发展浪潮下，AI Agent 成为新一代 AI 原生应用范式。当前，在问答、交互类应用中，大模型 +AI Agent 已经给用户带来新一代体验。但当 AI Agent 进入企业生产场景时，会面临新的挑战，如：企业生产场景面临专业复杂问题，AI 生成结果需具备严肃性，进行知识共享的同时又要保障知识安全。针对这些问题，华为云通过实际场景的实践，采取多方面技术，形成组合方案，解决企业引入 AI 生成技术的瓶颈，使得 AI Agent 在企业生产场景得以成功运用。</p><p></p><p>本次讨论将分为三个部分：首先，将详细分析 AI Agent 在进入企业生产场景时所面临的挑战；其次，介绍华为云在内部及项目实践中应对这些挑战的具体做法；最后，通过三个具体的企业场景案例，展望 AI Agent 在企业生产场景中的使用及其发展前景。</p><p></p><p></p><h4>AI Agent 进入企业生产场景时的挑战</h4><p></p><p></p><p>人工智能代理（AI Agent）无疑将成为新一轮技术革新的先锋。作为 AI 原生应用的典型形态，问答等交互式 AI 代理已经向我们展示了其在提供创新体验方面的巨大潜力。随着大模型技术和 AI Agent 的持续发展，我们正逐步探索通向人工通用智能（AGI）的路径，众多新兴技术方向正蓄势待发。</p><p></p><p>将 AI 代理成功融入企业并使其在各个生产环节发挥关键作用，是当前面临的一项紧迫任务。这不仅要求 AI Agent 具备更高的标准，还要求其能够满足企业特有的需求。在将大模型技术与 AI 代理结合应用于企业环境时，我们面临的主要挑战是如何获得业务员工的广泛认可，并确保其成为企业可信赖的工作伙伴。这一挑战主要包括以下四个方面：</p><p></p><p>专业性：企业场景通常涉及特定领域的专业知识，如化工、医疗、制造业等，这要求 AI Agent 必须具备相应的专业理解和能力。协作性：企业场景要求 AI Agent 能够与其他能力协同工作，并与现有的信息技术系统实现无缝集成。责任性：与鼓励创新和多样性的通用场景相比，企业场景更加强调 AI 代理输出的严肃性和可靠性。安全性：保护企业私有数据、明确权限划分和访问控制，防止员工无意中泄露敏感信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6c/6c8defaac825afa803a9de50ab1da57d.png" /></p><p></p><p>要在企业场景中实现 AI Agent 的成功应用，我们必须将大模型和 AI Agent 与企业独有的知识体系及现有的信息技术系统紧密结合。以下是从业务部门员工的角度出发，对所面临的挑战进行的深入分析：</p><p></p><p>专业性：业务部门对服务效果有着更为严格的标准。例如，在客服领域，业务部门期望答复的准确率至少达到 90%，并且要求答复内容既专业又简明。业务部门员工需要直接可用的答复，而非需要进一步选择或确认的选项。协作性：目前，许多企业的专业能力仍然依赖于现有的模型和系统。为了满足企业对复杂生产场景智能化的高要求，AI Agent 必须与大模型及企业现有的网络系统实现深度协同。责任性：确保答复的严肃性、正确性及可解释性是至关重要的。AI Agent 和大模型需要解决知识更新滞后和幻觉问题，避免因知识更新不及时或关键信息的错误回答而影响业务部门员工对 AI Agent 的信任。安全性：在专注于提升专业性和责任性的同时，安全性问题不容忽视。需要警惕对大模型的注入攻击，如通过恶意问题制造死循环或诱导恶意动作，尤其是生成代码的 AI Agent 更需加强安全防护。同时，也需防范攻击者针对 AI Agent 框架本身的攻击。</p><p></p><p></p><h4>华为云在 AI Agent 的探索与实践</h4><p></p><p></p><p>华为云在内部实施 AI Agent 技术进入生产场景的策略，分为三个阶段和七个步骤，旨在使技术团队深入理解并有效运用 AI Agent 技术，同时让业务团队明确 AI Agent 的适用场景：</p><p></p><p>初阶：选择问答类 AI Agent 作为起点，使业务部门能够迅速体验到 AI Agent 的效果。通过选择合适的基础模型、Prompt 模板和进行微调，业务部门可以感受到问答类 AI Agent 带来的不同寻常的体验。中阶：引入相对复杂的 AI Agent，如客服助手、会议助手等，这些应用仍然处于办公领域。通过使用外挂知识库和大小模型的编排，可以满足特定场景的需求。高阶：针对高阶专业场景，例如设备智能巡查 AI Agent，需要根据 AI Agent 和大模型的特性，进一步增加防退化和防安全风险的技术，以确保 AI Agent 在专业领域的稳定和安全运行。</p><p></p><p>通过这一分阶段、分步骤的方法，华为云不仅促进了技术团队对 AI Agent 技术的深入理解，也帮助业务团队识别了 AI Agent 技术在不同业务场景中的应用潜力。</p><p></p><p>进一步地，华为云将这一方法平台化，使得各业务团队和技术团队都能够迅速掌握 AI Agent 技术的核心要点，并将其应用于构建定制化的智能业务场景。通过平台化的方法，不仅加速了 AI Agent 技术的普及，还为企业场景智能化推广奠定了坚实的基础。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4f/4fce340225bbcc695bcec5c2432587b2.png" /></p><p></p><p>在 AI Agent 的技术实践中，针对企业面临的专业性、协作性、责任性和安全性挑战，以下是一些关键的技术实践：</p><p></p><p>企业词表的构建：整理和标准化专业术语和特定词汇，为 AI Agent 提供准确的语言和概念框架，以确保其在专业领域的有效沟通和问题解决能力。外挂知识库的整合：利用企业内部的文档、知识库资源，为 AI Agent 提供丰富的背景知识，增强其在特定领域的专业性和准确性。防退化机制的实施：定期对 AI Agent 进行性能评估和模型更新，以防止性能随时间退化，确保其长期稳定运行。模型编排的策略：通过模型编排，实现不同 AI 模型的优势互补，以适应多样化和复杂的业务需求。防安全风险的措施：采取一系列安全措施，包括数据保护、访问控制和安全协议，以防止潜在的安全威胁，保障 AI Agent 的安全和可靠。</p><p></p><p>通过这些关键实践，企业能够更有效地利用 AI Agent 技术，提升企业生产场景智能化水平，同时确保技术应用的安全性和可靠性。</p><p></p><h5>1. 企业词表，应对专业性的挑战</h5><p></p><p></p><p>为了使 AI Agent 能够深入理解企业生产场景中的问题，特别是行业术语和企业专用名词，建立企业词表是至关重要的一步。这一过程包括：</p><p></p><p>整理专业术语：收集和整理企业使用的行业标准词汇和企业特有业务名词。制定数据标准：为确保企业私域数据能够被 AI Agent 有效理解和使用，需要制定一套标准，指导这些数据如何被整合进大模型中。词表管理与共享：对企业词表有效管理，并在公司内部实现有序共享，减少信息错误和混乱。</p><p></p><p>企业词表的建立是一个动态的过程，需要随着企业业务的发展和行业知识的更新而不断进行调整和完善。定期的词表更新和维护是确保 AI Agent 长期有效性的关键。</p><p></p><p>通过这些措施，企业可以确保 AI Agent 在专业性方面的挑战得到有效应对，从而在企业生产场景中发挥更大的作用。</p><p></p><h5>2. 外挂知识库、防退化，应对责任性挑战</h5><p></p><p></p><p>为了应对责任性挑战，确保 AI Agent 的答复准确率和严肃性，以下措施是关键：RAG 技术的应用：采用检索增强生成（Retrieval-Augmented Generation, RAG）技术，通过结合检索和生成的方法，提升 AI Agent 的答复准确率和质量。知识库的分类与更新：对外挂知识库进行细致的分类，并根据业务需求设定不同的更新周期。自动化的更新流程和数据工程能力是确保知识库能够定期且快速更新的关键。数据飞轮的实施：通过数据飞轮机制，定期从业务中获取增量数据和用户反馈，为 AI Agent 提供持续学习的素材。这种能力对于 AI Agent 在各种场景下的表现至关重要，因为知识的及时更新是维持答复准确率的基础。增量数据与反馈的整合：迅速将收集到的增量数据和用户反馈整合到模型或知识库中，以保持 AI Agent 的知识最新和答复的相关性。业务流程的整合：将反馈数据的采集作为业务流程的一部分，使业务团队员工能够根据 AI Agent 的表现及时进行调整和优化。这种直接的反馈循环可以回流成为优化 AI Agent 性能的宝贵数据。</p><p></p><p>通过这些措施，AI Agent 不仅能够应对责任性挑战，还能够实现自我优化和持续进步，确保其在企业中的长期有效性和可靠性。</p><p></p><h5>3. 大小模型编排，应对协作性挑战</h5><p></p><p></p><p>为了应对协作性挑战，大小模型的协同工作模式发挥着重要作用。其基本原则：大模型通常擅长于理解、总结和提供高层次的指导，而小模型则更擅长于感知和执行具体的任务。在这个框架下，大模型扮演着团队领导者的角色，负责分配任务并协调团队成员的工作，共同完成复杂的任务。以下是模型编排的实例：</p><p></p><p>智能运维 Agent：企业已有的监控小模型负责感知环境并收集数据，然后将信息汇聚到大模型进行深入理解和分析。分析结果再通过现有的 IT API 执行具体的运维操作。会议助手 Agent：大模型首先理解与会人员的意图，然后调用不同的小模型和现有的 IT API 来执行会议管理、记录和后续的行动项。客服助手 Agent：在更复杂的客服场景中，大模型理解客户的意图后，将任务分配给自己、小模型以及现有的系统 API，进行更复杂的处理和响应。组合模式的应用：这些模式可以根据具体问题的复杂性进行组合，以分解问题并选择合适的大小模型和现有系统共同解决问题。</p><p></p><p>在垂域大模型尚未完全发展成熟时，通过大模型、小模型以及现有系统的组合，是一种实际可行的方法，可以有效地实现企业复杂场景的智能化。</p><p></p><p>通过大小模型的编排，企业可以更有效地利用现有的技术资源，提高 AI Agent 在协作性方面的性能，实现更高效的业务流程和决策支持。</p><p></p><h5>4. 防安全风险，应对安全性挑战</h5><p></p><p></p><p>在 AI Agent 技术实践中，确保安全性是至关重要的，特别是在应对隐私数据保护、模型交互安全和 Agent 应用安全的挑战：</p><p></p><p>隐私数据脱敏：统一明确隐私数据的种类，并为各类隐私数据提供相应的识别组件。这些组件能够准确识别出敏感数据，并根据隐私处理的基准要求，在训练和推理过程中对这些数据进行脱敏处理。隐私数据脱敏是企业私域数据处理中不可或缺的一步。模型交互安全：在使用外部大模型时，保障员工与 Agent 交互的安全性至关重要。可以通过建立模型网关对大模型进行统一管理，并构建一个安全隔离带来实现三层过滤机制，确保模型应答的安全性，防止企业内部敏感信息泄露到外部。这包括：内容安全评分：对大模型的输出结果进行内容检查和评分，持续评估大模型的能力。信息过滤网：在交互过程中，对检测到的企业敏感信息进行提醒和审计留存。业务领域可配置规则：允许企业各业务领域根据自身业务情况设置独特的审核规则。Agent 应用安全：Agent 应用的安全威胁可能在以下三个环节中产生：任务规划时：识别并防范注入攻击。任务执行前：识别恶意代码，避免破坏性攻击。Agent 框架自身：防护框架漏洞和服务越权问题。</p><p></p><p>为了确保 Agent 应用的安全，需要结合企业现有的安全技术，将 Agent 框架、运行与安全技术紧密结合。在任务规划、执行和 Agent 运行的各个环节中引入相应的安全技术。随着 AI Agent 应用的日益普及，相应的安全理论和技术也将逐步形成体系。</p><p></p><p>通过这些综合性的安全措施，企业能够确保 AI Agent 技术的安全应用，保护企业免受潜在的安全风险。</p><p></p><p>华为云在 AI Agent 进入企业生产场景的技术实践可以总结如下：</p><p></p><p>应对专业性和协作性挑战：</p><p></p><p>利用企业词表来增强 AI Agent 对专业术语和企业专有名词的理解，从而提升其对企业任务的理解能力。应用模型编排技术，实现大模型与小模型的协同工作，以及与现网应用的整合，共同解决复杂任务。应对责任性挑战：通过外挂知识库和防退化机制，确保 AI Agent 的答复准确率和严肃性，持续保持其效果，使其成为员工可信赖的作业系统。应对安全性挑战：实施数据安全措施，包括隐私数据脱敏，确保在训练和推理过程中敏感数据的安全。加强模型交互安全，通过模型网关和安全隔离带，实现内容安全评分、信息过滤和业务领域可配置规则，防止敏感信息泄露。强化 Agent 应用安全，识别和防范任务规划和执行过程中的安全威胁，结合企业现有的安全技术，形成体系化的安全防护方案。</p><p></p><p>通过这些综合性的技术实践，华为云确保 AI Agent 能够安全、可靠地融入企业生产环境，提升企业运营效率和智能化水平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f325a30d98fb9517d9288bed750235dd.png" /></p><p></p><p>华为云在 AI Agent 领域的实践基础上，为企业提供了一个全面的 AI 原生应用引擎产品，旨在简化企业项目交付过程</p><p></p><p>南向接入与模型整合：支持接入多个大模型和传统模型，通过统一的接口屏蔽了模型集成的复杂性，同时确保了整个系统的安全性。</p><p></p><p>北向提供 Agent 编排能力，使得 AI 场景应用开发人员能够更加便捷地开发和管理 AI Agent 应用。</p><p></p><p>通过模型中心、知识中心、Agent 编排中心和 AI 可信治理等组件，抽象并封装了 AI Agent 所需的众多技术能力，为企业提供了一个强大、灵活且安全的平台，以支持 AI 技术在企业项目中的有效应用和快速交付。</p><p></p><p>平台化服务：使得各 AI 场景开发团队能够复用技术能力，降低运维难度，提升整体的开发与交付效率。安全性保障：在提供强大功能的同时，注重安全性，确保企业数据和交互过程的安全性。降低技术门槛：通过平台化的工具和流程，降低了企业在 AI 应用开发上的技术门槛，使得项目团队快速有效的参与到 AI 项目的开发和交付中。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6b/6b56d6d0514dfd8492d5fa7927760e6e.png" /></p><p></p><p></p><h4>AI Agent 在企业生产场景的运用效果和展望</h4><p></p><p></p><p>1.1基于华为云 AI 原生应用引擎平台的技术实践，我们可以看到 AI Agent 在企业生产场景中的多个应用案例。以下是三个具体的案例，展示了 AI Agent 如何助力企业实现数字化转型和提升效率。</p><p></p><p></p><h5>案例 1：客服助手</h5><p></p><p></p><p>背景：许多企业选择客服作为引入 AI Agent 的起点，因为客服领域具有较好的 IT 化基础、案例库和知识库。客服业务部门面临的主要挑战是在业务量增长的情况下，保持客服人数不变，同时提高客服人员的工作效率。客服助手 AI Agent 的引入旨在提高答复准确率，这是衡量客服场景成功的关键指标。</p><p></p><p>方案：</p><p></p><p>第一阶段：技术团队熟悉与验证，使用基础大模型结合外挂知识库，发现需要将企业的最新、最准确的知识整合到 AI Agent 中以提高答复质量。第二阶段：通过构建企业词表和人工标注，以及对大模型进行微调, 提升 AI Agent 的答复准确率到 70%。第三阶段：标注质量改进，建立标注规范，指导业务部门人员进行高质量标注，将标注工作纳入业务流程，积累高质量的标注数据，将 AI Agent 的答复准确率提升至 80%。第四阶段：持续自行优化，将作业与标注固化到客服业务流程中，利用持续的反馈数据和增量业务数据，形成作业与训练的双循环，逐步提升准确率至 90%。</p><p></p><p>成果：通过这四个阶段的实施，客服助手 AI Agent 不仅提高了答复的准确率，还通过持续优化，实现了与客服业务流程的深度整合，显著提升了客服效率和客户满意度。</p><p></p><p></p><h5>案例 2：会议纪要生成助手</h5><p></p><p></p><p>背景：在办公自动化场景中，自动生成会议纪要是企业业务部门关注的重点之一。然而，仅依赖于 AI Agent 的文本摘要能力，常常难以满足会议纪要的准确性和完整性，尤其是在识别会议重点和提取摘要方面。</p><p></p><p>方案：</p><p></p><p>引入语音识别和智能文档解析等先进技术，以提升会议纪要生成助手的效果。</p><p></p><p>语音识别转写：利用自动语音识别 (ASR) 技术，将会议中的发言转写成文本。智能文档处理：通过智能文档解析技术，提取会议议题和相关材料。结合公司词表：使用公司词表来增强对专业术语和内部用语的理解，确保发言稿的准确性。发言稿整理：对每个议题和每个人的发言进行整理，形成结构化的发言稿。发言稿切块与摘要：基于发言稿和会议材料，将发言稿分块，提炼出针对各个议题的意见。进行分段摘要，确保摘要内容的准确性和可用性。形成会议纪要：将整理好的摘要和意见整合，形成完整的会议纪要。</p><p></p><p>成果：通过上述流程，会议纪要生成助手能够提供高质量的会议纪要，不仅提高了信息的准确性和可用性，还大大减少了人工整理的工作量，提升了办公效率。</p><p></p><h5>案例 3：生产指挥助手</h5><p></p><p></p><p>背景：在企业生产环节，尤其是工业制造领域，设备智能巡检是保障生产效率和安全的关键环节。传统的巡检方法依赖人工检查，耗时且容易出错。AI Agent 的引入，可以自动化巡检流程，提高准确性和效率。</p><p></p><p>方案：</p><p></p><p>多轮理解澄清：利用 AI Agent 的多轮对话能力，逐步澄清和理解智能巡检的具体任务目标。任务分解：通过任务分解技术，将复杂任务拆解为更小的、可管理的子任务。使用预定义的常用子任务来降低任务分解的难度，提高效率。训练专门的 API 检索器（API Retriever）：提升 API 的检索准确率，确保 AI Agent 能够准确地执行 API 调用，与企业系统进行有效交互。自主规划与决策：AI Agent 需要具备自主规划能力，根据巡检结果进行决策。对识别出的问题进行分析，并指挥相应的处理措施。长上下文支持：确保 AI Agent 在多轮交互中能够维持长上下文的连贯性，以处理复杂任务。</p><p></p><p>成果：通过 AI Agent 在智能巡检中的应用，企业能够实现更加高效和准确的设备管理，减少停机时间，提高生产效率和安全性。</p><p></p><p>基于对 AI Agent 技术进入企业生产场景的挑战、技术实践和场景案例的探索和理解，我们对 AI Agent 在未来企业场景中的应用进行了展望。</p><p></p><p>企业运营环境的复杂性催生了 AI Agent 技术的多样化发展。预计至少将出现三类 AI Agent，以支持企业在人、事、物方面的智能化需求：</p><p></p><p>人 +AI：交互型 AI Agent 将逐步承担部分人力工作，提升工作效率。事 +AI：事务型 AI Agent 将替代或升级部分 IT 系统，实现企业事务流程智能化。物 +AI：面向物理设备的 AI Agent 将推动工业自动化，提升设备的智能化水平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/29/298c321156e802837b8c7c1a48197649.png" /></p><p></p><p>随着 AI Agent 实例数量的增加，如何有效管理成千上万的 Agent 实例，保障它们之间的内容交互和事件监督，成为新的技术课题。Agent 实例之间的协同通信需求，推动了对更高效、更便捷通信协议和机制的探索。</p><p></p><p>构建一个兼容多家 Agent 运行时的管理和协同通信网络，实现不同来源 Agent 的互通互联，将是未来发展的关键。这要求制定统一的标准，确保不同 Agent 运行时能够在同一平台上高效协作。</p><p></p><p>AI Agent 技术在未来仍有广阔的发展空间。随着技术进步和市场接受度的提高，我们相信企业生产场景将逐步实现更高程度的智能化，为企业带来深远的变革和价值。</p><p></p><p>嘉宾介绍：</p><p></p><p>陈星亮，华为云 aPaaS 首席架构师，华为云软件领域专家，工科硕士，在应用软件和云服务开发方面有 20 年丰富经验，现任华为云 aPaaS 服务产品部首席架构师，负责开天 aPaaS 云服务产品的设计和研发工作。曾参加英国 VM、香港 HKT、南方电网等国内外大型 IT 实施项目。当前研究方向：平台工程、AI 原生应用、应用元数据模型等。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p><img src="https://static001.geekbang.org/infoq/6a/6a282e480f9c9f28e7a53fa1f923030b.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/x8O7GZY28QmGTapi6sie</id>
            <title>天弘基金：AI Agent 在金融场景下的新应用</title>
            <link>https://www.infoq.cn/article/x8O7GZY28QmGTapi6sie</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/x8O7GZY28QmGTapi6sie</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 06:03:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 天弘基金, 金融大数据模型, AI Agent, ArchSummit
<br>
<br>
总结: 天弘基金在金融领域持续进行自主研发和创新，其金融大数据模型取得显著进展，成功应用于投资研究和销售策略中。AI Agent作为人工智能领域的重要技术，面临着挑战，但在金融领域的实际应用案例中展现出巨大潜力。天弘基金在全球架构师峰会上分享了基于大模型的AI Agent技术，为金融行业的创新发展注入新动力。未来在金融行业应用AI Agent的重要性和必要性将逐渐凸显。 </div>
                        <hr>
                    
                    <p></p><blockquote>嘉宾｜平野 天弘基金人工智能部负责人编辑｜黄雯希</blockquote><p></p><p></p><p>近年来，随着国家对“科技金融”领域关注的不断加深，天弘基金以其多年积累的技术开发能力和丰富的行业经验，在大模型方面持续进行自主研发和创新。根据最新的行业数据分析，天弘基金的金融大数据模型在行业分析、问题解决深度以及金融数据时效性等关键指标上取得了显著进展，已成功应用于投资研究和销售策略中，展现出卓越的效果和领先的技术实力。</p><p></p><p>AI Agent 是人工智能领域的一项重要技术，它能够模拟人类的智能行为，执行各种任务。然而，在实践中，AI Agent 面临着诸多挑战。如何在复杂环境下进行决策，高效地处理数据，深入探索 AI Agent 的发展与实践，成为了当前人工智能领域的重要议题之一。</p><p></p><p>在日前举办的 ArchSummit 全球架构师峰会深圳站上，天弘基金算法团队负责人平野分享了其团队在金融行业内开发的基于大模型的 AI Agent， 以及 AI Agent 的核心技术和在金融领域的实际应用案例。AI Agent 通过深度学习和自然语言处理技术，能够理解和生成人类语言，进而实现与客户的自然对话、提供金融咨询、进行投资决策辅助等，为金融行业的创新发展注入新的动力。</p><p></p><p></p><blockquote>8 月 16-17 日，<a href="https://fcon.infoq.cn/2024/shanghai/">FCon 全球金融科技大会</a>"将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，将邀请国内外金融机构及金融科技公司专家分享其实践经验与深入洞察。AI Agent 智能体作为焦点话题，届时也将有多个议题分享，蚂蚁集团投研支小助技术负责人纪韩将带来《多智能体协同范式在金融产业中的应用实践》，文因互联董事长 / 创始人鲍捷博士将分享企业如何《精益地打造金融专家智能体》......大会更多演讲议题火热招募中，点击链接可查看目前的专题安排并提交议题：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</blockquote><p></p><p></p><p>以下是平野老师分享全文（经 InfoQ 进行不改变原意的编辑整理）</p><p></p><h3>大模型的发展现状</h3><p></p><p></p><p>大模型的出现经历了从兴奋，到质疑，再到理智对待的发展阶段。</p><p></p><p>提到 AI Agent ，实际上它是基于大模型的 Agent 技术。大模型从 2022 年底开始备受关注，到现在越来越流行，竞争也越来越激烈。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68ab982188a97756d1ab045041f2d8d1.webp" /></p><p></p><p>在第一个阶段，大模型的出现是非常令人兴奋的， ChatGPT 在推出后的五天内就积累了 100 万用户，两个月内达到 2 亿用户，打破了史上所有 APP 用户增长速度。随后，国内各大厂也开始进入这个领域，全力投入 AI，很多从事大模型的公司如雨后春笋一样冒出来，全部卷入这个行业中来，那个时候经常听到“all in AI”。</p><p></p><p>接着是质疑阶段。媒体报道充斥着各种消息，有的说在 AIGC 时代需要庞大的算力，有的说斯坦福推出的 Alpaca 模型只需 100 美金就能训练出我们自己的大模型。这件事情也引发了大众的争议，质疑大模型经常会一本正经地胡说八道，也就是所谓的“幻觉”。另外，很多公司和研究机构投入大量的算力资源和人才资源，但真正落地的场景还在探索中，没有找到非常好的新技术的应用场景。这时候也有人质疑，大模型的成本是否太高？相关人才是否难找？</p><p></p><p>而且随着监管制度的不断完善，对大模型的伦理和相关的安全性、合规性要求也越来越高。现在大模型的发展已经进入了一个理智的阶段。根据天弘基金在全体员工的抽样调查中，发现约 25.7% 的用户已经基本上离不开大模型，这个数据还在不断增长。未来大模型在各行各业的应用会越来越多，越来越普及。</p><p>金融行业如何应用 AI Agent</p><p></p><p>要想在金融行业应用 AI Agent 首先要考虑三个问题：</p><p></p><p>第一个问题是资源和人才。作为一家金融公司，在开始做大模型时，没有像一些科技大厂一样拥有大量资源和人才。人才和资源的密度、总量都是有限的，需要选择性地进行投入，要决定哪些项目要坚持做，哪些项目要放弃。例如，数字人在很多销售领域的公司里可能很有用，但对金融的业务帮助不大，所以我们舍弃了这类看似高大上的技术。</p><p></p><p>第二个问题是研发方式。金融公司要不要直接购买第三方厂商的模型进行使用？但很多金融领域的使用场景中运用第三方的大模型是行不通的。因此，天弘更多地是采用自主研发。</p><p></p><p>第三个问题是算力。金融行业是否要应用 AI Agent？金融公司往往担心算力投入过大，像 ChatGPT 每个月的在算力上的开销至少是千万美金以上。金融公司是否需要上千张 GPU 卡才能将自己的 AI Agent 研发成功？但经过探索发现，可以用较小的成本做出有用且效果不差的模型。</p><p></p><p>此外，还要明确大模型并不能解决所有问题，大模型只是提升生产力的一种工具。它可以只是一把枪，单如果能在特定的场景下理解业务然后训练和优化大模型，那么它就能成为精准把握市场机会的狙击枪，这样的效果会比普通的大模型要好得多。</p><p></p><h3>金融行业应用 AI Agent 的原因和重要性</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09f7a4a6e73d3661ac0e79c3c3d92348.webp" /></p><p></p><p>举例来说，在金融行业，不同角色会遇到很多问题。作为基金经理，每天早上要看大量信息，看不完怎么办？作为交易员，突然发现光伏板块上涨，想知道原因或相关新闻怎么办？作为运营经理，发现有个热点，想快速抓住市场机会，领先发布相关营销物料，怎么办？这些问题看似简单，但并不是大模型就能完全解决的。</p><p></p><p>这时候，Al Agent 就能发挥作用。它可以在各种场景中提供实时性的数据，解决传统方法中训练时缺乏时效性的问题。那么，什么是 Al Agent 呢？</p><p></p><p>Agent 是一种能够自主决策、采取行动以达到某种目标的实体，Al Agent 主要依托 LLM 模型和具体的业务场景来调用相应的工具来完成任务目标，简单来说大模型 + 插件 + 执行流程 = Agent。如果延伸到智能体，那就还需要反思、环境感知等等多模块。通过应用 AI Agent，我们就能解决特定场景中的问题。</p><p>接下来简单介绍一下 AI Agent 的组成部分。AI Agent 主要有四个分支：Memory、Tools、Planning 和 Action。</p><p></p><p>Memory 分为长期记忆和短期记忆。短期记忆用来感知当前发生的状态，以即时决策。长期记忆则会把一些数据和内容存储在数据库或记忆系统中，供以后查询使用。查询后，可以通过预先调整来做相应的行动（Action）。</p><p></p><p>Tools 模块是 Agent 用来处理和分析数据、进行推理和决策的算法和方法。Tools 让模型和外部世界进行互联互通，既能让模型感知世界，也能让模型通过利用工具来改变外部状态。在金融领域使用工具，我们主要可以赋予模型感知金融市场实时变化的能力。例如，如果要查一个基金的数据，或在营销中查某个用户相关的购买数据，就需要调用相应的查询 API，我们称之为 Chat BI。Tools 决定了你需要使用什么 API。工具部分提供了 Agent 处理信息和执行任务的核心能力。</p><p></p><p>Planning 模块负责根据当前的目标和环境条件制定长期和短期的行动计划。这包括考虑到不确定性和可能性的计划制定，以及如何有效地达成设定的目标。规划使得 Agent 能够在复杂和动态的环境中进行有条理的行动。例如，如果我要写一个大纲，Planning 会告诉我第一步做什么，第二步做什么等等。或者，在写营销文案时，它会规划出逻辑顺序，确保步骤有条不紊地进行。此外，还有思维链（Chain of Thought，COT），这也是 Planning 的一部分。</p><p></p><p>Action 模块涉及 Agent 基于规划和当前环境状态选择和执行具体的行动或操作。这是 Agent 与外部世界交互的方式，通过执行行动来实现其目标和任务。则通过执行 Planning 规划的步骤，结合感知信息，调用合适的 Tools 来实现最终的行动目标。</p><p></p><p>那么，Al Agent 在金融领域能解决什么问题呢？它在金融领域最重要的应用场景是统一数据交互形式和多样化数据类型的交互。</p><p></p><p>Al Agent 的应用的核心是数据和交互。将不同模态和结构的数据进行交互，并通过简单直观的工具调用，以对话式的方式（例如 ChatGPT）呈现给用户，这是 Al Agent 的目标。</p><p></p><p>所以在金融领域的应用场景中，有几个重要的板块：</p><p></p><p>首先是搜索 API。像大家可能熟悉的 new Bing ，这些平台现在都采用实时检索结合大模型的方式。在金融领域，经常需要查询各种基金数据、交易数据或者实时市场行情数据等。</p><p></p><p>其次是多模态交互。在很多领域，多模态交互是很重要的。比如在视频创作、营销文案、财务报表等场景中，多模态交互可以更直观地呈现复杂数据，提升用户体验。</p><p></p><p>另外，还有 ChatBI 和工具交互，这取决于在每个业务场景中我们需要执行的具体操作以及调用的工具，然后将结果通过用户界面展示出来，进行一个用户界面的交互。</p><p></p><h3>天弘基金应用 Al Agent 的经验分享</h3><p></p><p></p><h4>金融分析模型框架</h4><p></p><p></p><p>这里简单介绍一下我们团队基于改良 Retrieval-Augmented-Generation 为基础的 Agent 框架的金融分析大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/848d530b73580f626191e890247ac9b2.webp" /></p><p></p><p>首先是我们称之为改良 RAG 的一个框架，即实时检索与大模型结合的框架。这个框架在 2024 年越来越火，许多大模型公司都会选择在这个框架上做延展。其实我们在 2023 年初就开始尝试这个框架，因为它对计算的资源要求不高，而且具有实时性的效果。所以我们在 RAG 的基础上进行了改良，分为几个模块：</p><p></p><p>1.改写（Rewrite）：在检索之后拿到的知识板块是在召回的这一块，我们把传统的 RAG 改写，按 Agent 的思路进行改良，先进行多角度的分析思考，再在每个分析视角下进行问题拆解，问题改写，和工具调用。我们会对问题进行改写，以让大模型能更好地理解和回答问题。比如，我们会将复杂的问题拆解变成多个子任务，在这些子任务上进行规划，即 planning。在 planning 之后，如果有必要会进行二次改写，再通过规划后的内容进行检索 + 金融工具调用。</p><p></p><p>在金融领域里面会有很多复杂的问题。比如，哪些国家是因为经济下行不得不下调利率，而使得整个国家的经济健康发展？这样一个问题，在百度或者谷歌 Google 直接搜索都是搜不出来答案的。所以在这种情况下，需要把这个内容进行改写，把它变成子模块，进行每个子模块的搜索，再用大模型进行归纳。</p><p></p><p>2.检索（Retrieve）：多路召回 + 多触发条件 + 多索引打分。比如，提出一个问题，先进行搜索，而不是直接用大模型回答。搜索包括搜索互联网内容和天弘基金自己的内容库，这样不仅可以获取网上公开的实时数据、天弘基金内部的数据、专业研究员的市场观点以及所自己积累的这种内部语料等。</p><p></p><p>3.推理（Read）：即归纳和总结。我们通过改写完的问题检索，会得到很多信息，把得到的信息进行排序和推理，最后得到一个总结性答案，就是我所说的推理。天弘基金使用的是多槽位推理，在多个子任务中同时进行大模型推理，最后给出总结。</p><p></p><h4>框架创新</h4><p></p><p></p><p>在这个设计大模型的过程中我们也做了一些创新。比如大家常说的 COT，也就是思维链（Chain of Thought），我们在此基础上做了改进，称之为 COM，就是把 Thought 变成了 Mind。COM 的意思是将一个关于金融的复杂问题拆解成多个子问题再进行操作。通过很多尝试，我们发现，一些基础的大模型，对于你提出的金融问题，回答的结果虽然正确，却并不是我们所想要的。作为一个专业的金融研究员，希望得到的答案也是专业的。在这种情况下，所需要的不是一个普通的、正确的答案，而是一个可以帮助做出正确决策的答案。</p><p></p><p>所以我们创新了 COM，帮助我们在构建这个大模型中融入了研究员和基金经理的这种思维模式，让大模型也有这种研究的思维。</p><p></p><p>接下来，我要介绍的是在检索以后，我们做的一些召回策略。在检索完成后，我们会进行召回策略的制定。例如，我们使用多路召回和多条件触发。理解用户意图不仅限于关键词匹配，还涉及时效性和语义理解等。我们采用多种索引方式，包括向量索引、关键词匹配（如 BERTSpan）、实体识别（NER）等。</p><p></p><p>另外，我们在粗排阶段，我们进行了相关性的过滤模型优化。在召回模块中，除了实时检索的数据，我们还整合了内部数据。这些内部数据通过知识图谱（KB）系统进行连接，让 Al Agent 的回答更偏向于研究员的研究。我们结合了产业链系统，确保对行业上下游关系的全面理解。比如说，当我们要分析光伏行业时，需要了解其上下游的供应商、完整的供应链和产品承接方。这些数据怎么能结合在一起准确地出现在金融从业者的答案中呢？我们进行了 KB 内容建设。首先是打通了产业链的上下游数据。我们会自动化地处理了一部分不变的市场数据，如公司行业指标等，把它们客观地呈现出来。另外，我们还针对了一部分变化的数据，如一些分析师的观点和各个业的异动，把每一个子模块都纳入异动监测的模型中，确保这些动态数据也能及时反映在大模型的回答中。</p><p></p><p>通过这些改进，我们发现效果得到了显著提升。进行对比，会发现在金融领域，我们团队在金融行业内开发的基于大模型的 AI Agent 与 ChatGPT 几乎不相上下，甚至在某些场景下我们会回答得更出色，因为我们针对金融领域进行了专门的训练。</p><p></p><p>关于 reference，我们实际上也做了好几个版本。现在我们的 reference 主要有几个关键点。一个是确保输出的内容都是都是有源可溯的，也是真正所需要的。我们会在答案中加入 reference，标明每一句话的来源，无论是来自我们的知识库还是网上公开的内容，都会有明确的标注，并且可以查看最终的数据源。</p><p></p><h3>大模型产品解读</h3><p></p><p></p><p>最后介绍一下天弘基金的产品。刚才提到的可能是一些技术细节，在产品方面，天弘基金内部已经发布了大约七到八款大模型相关的产品，这些产品还没有对外发布。总结一下，大模型可以在一天之内做些什么？用一个时间线来串起整个大模型的产品，比如说，早上研究基金经理来到我们公司，可能需要浏览各种研报。这时候，我们有一个产品叫智汇，可以让研究员快速浏览市场上最新的研报，并且筛选出他们感兴趣的内容。天弘大模型的优势在于总结研报或 PDF 时，如果涉及投资领域，天弘基金训练后大模型可以识别文章中提到的投资标的，如最近的 AI 医疗、AI 办公和 AI 法律等领域。这是研究员非常关注的内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/118ebb49631e11ce4fb6f4bbd53ea5fe.webp" /></p><p></p><p>我们还按照不同的研报类型，例如行业分析，市场策略，宏观解读这些研报分类训练了不同的研报摘要模版。其次，作为研究员，在早间浏览黄金新闻时，我们根据研报进行进一步解读，我们发布了智读产品，专门针对特定的研报进行解读和提问。也就是当你看到一篇特别感兴趣的文章时，你想要深入研读它，这时候打开我们的系统，你可以提出问题，还可以对比多篇文章进行阅读。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ba/baa9ccb5738d3cbe7c7796222196586b.webp" /></p><p></p><p>接下来是“弘小助”板块，这是我们的核心之一，涵盖行业研究、市场分析和金融知识问答等多个方面的专项训练，在市场表现、行业分析、热点解读等几个方面做得相当出色。我们内部推出的产品，可以整合市场上公开的各种研报和公开的第三方数据源，以及包括内部基金经理的观点。比如，如果向这个大模型产品提问“光伏行业能否现在买入？”随后利用我们自己独创的 COM，将研究员的思维模式融入其中，通过意图分析后，为投资研究角色提供了答案，给出的回答会让提问者至少了解了最近光伏行业的市场表现。不仅限于这些问答，“弘小助”还会提供类似光伏指数这样的行业指标，同时将产业链中的信息整合进来，解释每一个异动点背后的原因，并深入分析这些异动点。接下来是“reference”，也就是我们提到的，你可以看到每一个内容的来源。另外，我们还将大模型整合到产业链系统中，使用弘小助来进行产业链异动解读、热点挖掘等，使得产业链更智能化，特别是在发现异动方面的效果明显优于以往小模型的应用。</p><p></p><p>除以上应用的落地，我们还进行了一些可能探索性工作，包括利用大模型挖掘金融中的量化因子。我们一直在思考大模型是否能帮助我们解决投资中的因子挖掘问题。之前国外有几个大的基金公司，专门从私募基金中挖掘出一批非常厉害的“大牛”，用于进行大模型挖因子的工作。天弘基金当时也在进行类似的尝试，这件事是否可行，是否能实现。我们进行了一系列的实验，其中有几个是我们自己创新的方法。在沪深 300 的股票池中进行了测试后，我们发现，与我们常规的 word count 101 算法或者最近流行的强化学习挖因子相比，大模型的效果非常显著，我们的信息系数（IC）达到了 0.0326，这比目前我们尝试的强化学习的效果还要好。从理论上讲，如果有很多个因子，需要进行组合，这其实是一个计算机难以完成的暴力求解过程。但是如果能够借助大模型的思维方式，通过某种逻辑形式将一些不必要的组合排除在外，就能够显著地缩小最终的搜索范围。</p><p></p><p>关于金融大模型，天弘基金会始终坚持业务导向，务实创新。始终坚持技术引领，前瞻探索。始终坚持创新合作，共创价值。始终坚持合规运营，敬畏风险。始终坚持成本效益，精准投入。</p><p></p><h5>活动推荐</h5><p></p><p></p><p>8 月 16-17 日，<a href="https://fcon.infoq.cn/2024/shanghai/">FCon 全球金融科技大会</a>"将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20c66d071f402ec153289ac70f52fb35.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qMbLYraHWTFoN1KwC2dA</id>
            <title>中国AGI市场—4543亿市场下的新机会 | 分析师研判</title>
            <link>https://www.infoq.cn/article/qMbLYraHWTFoN1KwC2dA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qMbLYraHWTFoN1KwC2dA</guid>
            <pubDate></pubDate>
            <updated>Tue, 25 Jun 2024 03:31:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能纪元, AGI, 中国AGI市场, 应用市场规模
<br>
<br>
总结: 本文介绍了围绕通用人工智能（AGI）的讨论，以及中国AGI市场发展研究报告中对2030年市场规模的预测和个人市场、企业市场的发展趋势。 </div>
                        <hr>
                    
                    <p>我们正站在一个全新智能纪元的路口，围绕通用人工智能（AGI），在学术界、科技界、产业界的讨论中，一部分 AGI 的神秘面纱已被揭开，但这面纱之后还有更多的未知等待着我们。</p><p>InfoQ研究中心在此背景下，经过数月的研究和众多专家的访谈，发布了《<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">中国&nbsp;AGI&nbsp;市场发展研究报告&nbsp;2024</a>"》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/112056f4a24c61388383c53979268b50.png" /></p><p></p><p>InfoQ研究中心在《<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">中国AGI市场发展研究报告&nbsp;2024</a>"》中，预计 2030 年中国 AGI 应用市场规模将达到 4543.6 亿元人民币。2024-2027年 中国 AGI 应用市场将经历快速启动期；年增速持续走高。2028 年起，市场将进入平稳发展期，年市场增速保持在 50% 左右，并预计于 2027 年突破千亿人民币市场规模。</p><p>分市场来看，个人市场而言，目前各类&nbsp;AGI&nbsp;应用 APP 的用户数量大概在 5000 万左右，根据测算，2023 年个人市场整体规模在 135 亿元左右。使用频率和实际付费仍然是制约个人市场发展的主要因素。根据InfoQ&nbsp;2023 年 12 月发起的《<a href="https://www.infoq.cn/minibook/qKIPSWhV6BIdUrHF1tyZ">中国生成式AI开发者画像调研</a>"》结果，40.2% 的开发者还没有为生成式 AI 产品付费，38.6% 的开发者已花费金额在 500 元以下。开发者群体付费情况已是如此，放大到整个个人网民群体中，付费意愿和实际付费情况亦然。</p><p>另一方面，使用频率也和付费存在着某些内在联系，使用频率越高，相对而言，其实际付费意愿和水平会越高。但究其根本，仍然是产品功能能否满足个人市场的使用需求。</p><p>InfoQ研究中心认为，中国 AGI 应用市场规模发展将由企业市场引领主导，到 2030 年企业市场规模预计达到 3024.6 亿元人民币。2023 年，企业市场用户出于落地成本和应用效果的考虑，以及本身决策周期长的原因，AGI 应用企业市场刚刚起步。根据数智前线和百炼智能披露的相关数据，截至 2024 年 6 月 15 日，中国大模型市场共计发布中标公告 230 个，远超 2023 年全年的 190 个。InfoQ研究中心预计，自 2024 年到 2027 年，中国 AGI 应用企业市场处于快速启动期，年增速均在 100% 以上。同时伴随着企业市场对应用成果和落地路径的探索，预计 2027 年开始，企业市场规模将超越个人市场规模，成为中国 AGI 应用规模发展的主导力量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed3b53e7d2af6e6043bb30c567c7cece.png" /></p><p></p><p>在经历了近 2 年的讨论后，营销、零售、金融、教育、企服等行业是如何落地AGI的，又有哪些应用案例和难点，以上问题欢迎大家点击「<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">文中链接</a>"」下载完整报告。各位读者朋友也可以关注「AI前线」公众号，回复「报告」免费领取更多InfoQ研究中心重磅AI报告。也欢迎大家积极留言和讨论，分享您的见解和经验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ctTV5uvkTlmhBQTTxlx1</id>
            <title>网络架构如何支持超万卡的大规模 AI 训练？| AICon</title>
            <link>https://www.infoq.cn/article/ctTV5uvkTlmhBQTTxlx1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ctTV5uvkTlmhBQTTxlx1</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 23:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 训练场景, 算力 Scaling, HPN7.0 架构系统, Ethernet
<br>
<br>
总结: 本文介绍了在 AI 训练场景中，算力 Scaling 的核心是网络，依赖于大规模、高性能的数据中心网络集群来实现算力的规模扩展。阿里云设计了 HPN7.0 架构系统，基于 Ethernet 来构建超大规模、极致性能的网络互联。 </div>
                        <hr>
                    
                    <p>AI 训练场景的算力 Scaling 核心是网络，依赖于大规模、高性能的数据中心网络集群来实现算力的规模扩展，为此，阿里云设计了 HPN7.0 架构系统，基于 Ethernet 来构建超大规模、极致性能的网络互联。</p><p></p><p>本文整理自阿里巴巴资深网络架构师席永青在 AICon 2024 北京【大模型基础设施构建】专题的演讲<a href="https://aicon.infoq.cn/2024/beijing/presentation/5881">“网络驱动大规模 AI 训练 - 阿里云可预期网络 HPN 7.0 架构”</a>"，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了【大模型训练以及推理加速】专题，本专题将全面剖析大模型训练和推理过程中的关键技术与优化策略。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p>大家好，我是席永青，来自阿里云。阿里云的 PAI 灵骏想必大家都熟悉，已经是 AI 领域的标杆算力平台，服务了众多知名的 AI 大模型公司。我有幸负责灵骏智算集群网络架构设计。今天非常高兴有机会在 AICon 这个优秀的平台上与大家交流，希望能够与各位进行深入的探讨。</p><p></p><p>我在阿里云工作已经有近十年的时间，专注于数据中心网络架构和高性能系统的设计。从 2021 年开始，我专注于 AI 智算领域，负责智算集群网络的规划演进。在大模型还未如此火热之前，阿里云就开始设计 AI 计算的万卡集群。回顾整个过程，智算最初在自动驾驶领域应用较多，许多自动驾驶客户希望通过 AI GPU 集群进行视觉模型训练，在 2021 年阿里云就非常有远见地构建了第一代万卡集群，当时我们称为 HPN 6.0。</p><p></p><p>这几年来，从网络到 GPU、机器、整个 IDC，再到平台系统和上层 AI 模型框架，AI 基础设施领域的发展速度非常快。我有两点明显的感受：第一，随着 GPT 的爆发，我们几乎每天都需要更新知识库，虽然网络是底层技术，但也需要密切关注模型发展和框架变化带来的对网络使用上的变化，也包括 GPU 硬件更新迭代对网络互联和带宽的影响等。第二，集群规模的迅速变化，从一开始的千卡 GPU 到现在万卡十万卡规模，如果没有前瞻性的技术储备和规划，基础设施将面临巨大的挑战。</p><p></p><p>我今天要分享的内容主要分为四个部分，首先我会介绍高性能网络系统的发展历程以及它目前所处的阶段。接着，我会探讨在构建大规模 GPU 集群，比如万卡甚至十万卡集群时，对于网络来讲最关键的要素是什么。接下来，我将重点介绍阿里云 HPN 7.0 架构，它是阿里云 PAI 灵骏智算集群的核心网络技术。最后，我将展望以 GPU 为中心的基础设施及其高性能网络系统的未来发展趋势。</p><p></p><p>在座的可能有些是网络领域的专家，有些可能是更上层的系统、AI 平台或算法的专家，还有一些可能是 GPU 领域的专家，希望在今天的分享中，我能回答大家三个问题。第一个问题是网络对于 AI 计算意味着什么，网络在整个 AI 计算系统中扮演的角色以及它的重要性。第二个问题，如果你的公司正在做 AI 模型相关工作，无论是在构建大模型平台还是自行研发大模型，基础设施网络的方向应该如何选择。第三个问题是，一旦确定了网络方向，网络方案和一些关键技术点应该如何实施。</p><p></p><h2>高性能网络系统进入可预期时代</h2><p></p><p></p><p>让我们回顾一下网络的整个发展历程。在 2000 年左右，互联网刚刚兴起时，网络主要是由设备供应商提供的基础设施，用于支撑 IT 业务系统。那时，数据中心开始起步，电商业务如淘宝，搜索业务如百度、Google 等开始规模化使用数据，产生对数据中心大规模计算的需求。那时，数据中心内部主要使用 TCP 协议，那时的 TCP 能够满足算力连接服务的需求，随着摩尔定律的持续推进，CPU 不断升级，TCP 的能力也随之提升，网络并没有成为瓶颈。</p><p></p><p>随着云计算和大数据的兴起，网络进入了第二个发展阶段。在这个阶段，因为集群规模的扩大，网络的规模和稳定性要求以及带宽需求都在增加。这时，网络进入了软件定义网络（SDN）的时代，这是许多网络专业人士都熟悉的一个时代，诞生了许多新技术，也涌现了许多网络领域的创业公司。</p><p></p><p>随着云计算数据中心的进一步扩大，AI 智算时代逐渐到来。智算集群与传统云计算数据中心有很大的不同，它对网络的要求也截然不同。这也是我接下来要分享的重点，希望带大家了解为什么在 AI 数据中心中，网络如此重要，网络在其中扮演了多么关键的角色。我们目前正处于第三个阶段，这个阶段的网络技术架构的发展将决定 AI 计算规模化发展的趋势，这是接下来讨论的重点。</p><p></p><p>在讨论集群算力中网络所扮演的角色之前，我们首先需要明确 AI 基础设施的关键要求。对于 AI 基础设施来说，一个至关重要的要求是训练时间。训练时间对于业务创新至关重要，因为它直接关系到公司是否能高质量得到 AI 模型，是否能快速将产品推向市场，同时这个过程中训练时间所带来的创新迭代效应也将更加明显。</p><p></p><p>训练时间的关键因素包括模型的时间加上中断时间。其中模型训练的时间，与整体计算量有关，在模型、数据集确定的情况下，这是一个固定值，这个算力需求的总量，除以集群的算力，就是模型训练的时间。此外，还需要考虑中断时间，这可能包括模型调整、数据调整或因为故障导致的训练暂停从而从上一个 checkpoint 恢复。</p><p></p><p>集群算力与通信效率密切相关。组成 AI 训练集群的千卡、万卡 GPU 是一个整体，所有人在协同完成同一个计算的任务。我们往往通过增加 GPU 的规模来增加集群的总算力，比如从 1000 张 GPU 增加到 2000 张、4000 张，整个集群所表现出的算力是否还能保持“单 GPU 乘以 GPU 数量”的算力，这是我们通常所说的线性比。这个线性比怎么做到最优，核心是通过高性能的网络系统来实现的。如果网络出现问题，哪怕是影响到一块 GPU 的网络问题，都会导致整个集群的任务变慢或者停下来。因此，网络在“集群算力”中扮演着至关重要的角色，它不仅关系到算力的线性扩展，还直接影响到训练任务的稳定性和效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5e/5e08f83cf42215b352a1b8603adbc56b.png" /></p><p></p><p>AI 计算中的通信模型与传统计算有着显著的不同。AI 计算过程是迭代，包括计算、通信、同步，然后再回到计算。以模型训练过程为例，首先将模型所需的数据加载到 GPU 上，然后 GPU 进行前向计算、反向计算，在反向计算完成后，关键的一步是同步模型收敛的梯度参数到每一个 GPU。这样，在下一轮的数据训练开始时，所有的 GPU 都能够从最新的模型参数开始迭代，这样将整个参数收敛到我们期望的结果。</p><p></p><p>在这个过程中，网络要做的核心工作对梯度进行全局同步。在每一轮的迭代计算中，都需要将梯度数据同步。而图中蓝色部分所表示的，正是网络所承担的工作。网络负责在各个 GPU 之间传输和同步这些梯度数据，确保每个 GPU 都能够接收到最新的模型参数，从而进行有效的并行计算。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1d/1d91fd875585a0623a3e5e7cd1f5ffb1.png" /></p><p></p><p>网络在 AI 计算中的重要性体现在它对算力规模扩展的影响上。当算力规模扩大时，如果网络的线性比下降，实际体现出来的算力也会随之下降。如果我们将 GPU 的数量从 128 张增加到 1024 张、4096 张，再到 1 万张，理想情况下，只要扩展 GPU 规模，就能获得相应的算力提升。但实际情况往往并非如此。网络在梯度同步过程中需要时间，这个时间的长短直接影响到 GPU 在计算过程中的等待时间，尤其随着规模的扩展，梯度同步所需要的网络交换数据量也会变大，网络通信的时间也会变长，相当于损失了 GPU 算力。好的网络架构设计，高性能的网络系统，可以做到随着规模的增加仍然保持较好的线性比，充分发挥大规模 GPU 的算力，网络性能即规模化的算力。</p><p></p><h2>GPU 集群对网络的关键要求</h2><p></p><p></p><h3>传统网络集群设计不再适用 AI 计算</h3><p></p><p></p><p>在 AI 计算中，GPU 集群对网络有着更高的性能要求，希望网络在算力扩展过程中能够保持高效的通信。这引出了一个问题：GPU 集群对网络提出了哪些关键要求？</p><p></p><p>首先，我们可以得出一个结论，即传统的网络集群已不再适用于 AI 计算。过去 20 年左右，数据中心的核心算力主要来自 CPU。如果我们观察 CPU 系统和网络系统的组成，可以发现几个特点：CPU 系统通常是单张网卡的，从 CPU 通过 PCIe 到网卡出口，内部没有特殊的网络互联。CPU 系统的单机带宽最大到 200G 就已经足够，因为它们主要服务于 APP/Web 类型的应用，这些应用需要进行互联网访问和数据中心内机器的协同工作，处理各种流量。</p><p></p><p>GPU 网络的情况已经发生了很大变化。每个 GPU 都有自己的内部互联，例如 NVIDIA 的 A100 或 H800，它们内部的 NVLink 互联可以达到 600GB 甚至 900GB。这种内部互联与外部以太网网络集群设计之间存在耦合关系。GPU 是单机多网卡的，单机内的多张网卡之间有高速互联，单个服务器的带宽可以达到 3.2T，与通用 CPU 计算带宽相比至少有 6 到 8 倍的关系。GPU 需要使用 GPU Direct RDMA 来实现显存之间的数据迁移，并且需要超短的 RTT（往返时延）。</p><p></p><p>因此，在 AI 场景下，传统的数据中心集群设计很难发挥其作用。GPU 集群需要网络能够支持更高的带宽、更低的延迟和更高效的通信机制，以满足 AI 计算的需求。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2a/2a0a12dae781270a6b9bd48298dc03dc.png" /></p><p></p><p>在传统的数据中心集群中，任务模式通常包括计算、存储以及客户端 / 服务器服务。这些服务之间需要建立大量的会话连接来交换数据，而这些连接的数量通常取决于用户量和负载等因素。因此，这些连接的数量很高，流量趋势会随着业务负载的变化而变化。例如，在淘宝上，网络流量的高低峰与交易高峰密切相关。</p><p></p><p>而在 AI 计算中，特别是在模型训练过程中，网络表现出的是周期性的行为。计算、通信和同步循环是连续不断的过程。例如，一个 400G 的网卡在每一轮计算迭代的通信部分可以在瞬间将网络带宽用满。</p><p></p><p>网络的任务是尽可能缩短计算的等待时间，这样，GPU 就可以更充分地发挥其 Tensor Core 的能力来进行计算任务，而不是浪费在等待数据同步上。所以在 AI 模型训练任务中，尤其是在大型 AI 模型的训练中，网络表现出的特点是高并发和高突发流量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/94/945dabf0a77e6469537310c592f7a9d8.png" /></p><p></p><p>在讨论网络连接数量的特点时，我们可以看到通用计算和 AI 训练集群之间存在显著差异。在通用计算中，采用的通常是客户端 / 服务器模式，连接数量与用户的请求量和业务模型的设计紧密相关，可能会非常大。例如，一台服务器上可能有高达 10 万级别的 HTTP 连接。</p><p></p><p>在 AI 训练集群中，一个网卡上的连接数量却非常固定，通常只有百级别连接。从训练任务开始的那一刻起，每一轮对网络的操作都是相同的。在每个循环中，活跃的连接数量以及所需的连接数量都非常少。连接数量少在网络上可能会引起 HASH 问题，这是我在后续讨论 HPN 7.0 设计时会重点提到的一个关键问题。HASH 问题是目前网络领域在 AI 计算中需要解决的核心问题之一。简单来说，连接越多，熵就越大，在选路径时分散均衡的概率也更大。而当连接数量减少时，HASH 问题就会变得更加明显。</p><p></p><h3>AI 集群高性能网络系统关键要求</h3><p></p><p></p><p>当我们深入探讨 AI 网络系统时，如果从端到端的角度审视 AI 系统的网络构成，我们可以发现在 AI 训练过程中，有三个非常关键的组件。</p><p></p><p>集群架构设计：集群架构虽然看起来只是一张拓扑图，但实际上它决定了物理带宽的使用和路径的简化程度。这个架构直接影响到模型训练过程中的网络 HASH、时延和带宽。就像城市规划中的道路规划一样，只有设计得当，交通（在这里比喻为数据包）才能高效运行。端到端传输协议：它决定了数据包在网络中的传输效率。就好像交通网络的效率，需要每辆车都足够安全足够快，同时也要避免交通拥堵的发生。传输协议需要考虑传输效率、重传、流控等因素以确保高效传输。监控运维和资源管理系统：虽然在今天的分享中不会详细讨论，但这个系统非常关键。整个系统依赖于监控运维的能力进行快速的问题发现，性能分析，和问题解决。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d4/d4869645ff48c4e58180c1e3f0c187ea.png" /></p><p></p><p>在 AI 计算网络设计中，如果我们将前述的三个部分进一步拆解，会发现在 AI 训练过程中，网络有四个关键点。</p><p></p><p>集群架构设计：合理的集群架构设计是重中之重。这个设计决定了带宽和规模能达到的程度，比如是连接千卡、万卡还是 10 万卡，带宽是 3.2T、6.4T 还是更大，网络层级是一层、两层还是三层，以及计算和存储的布局等。这些因素都会影响 AI 训练中迭代时间或每秒样本数。点到点传输协议：在集群设计的基础上，点到点之间需要使用最快的协议来实现梯度传输。这要求协议能够实现直接内存访问（DMA），减少拷贝操作，实现大带宽和低延迟。目前，无论是 RoCEv2 还是 IB，DMA 技术已经实现了这些能力，协议栈已经写入硬件，实现了零拷贝操作。incast 问题：在训练通信过程中，会出多对 1 的数据交互场景，这会导致尾跳网络出口成为瓶颈。如果没有有效的流控方法，这会在网络出口形成队列堆积，导致缓冲区溢出发生丢包，严重影响通信效率。流控的目标是保持缓冲区的能力足够不会溢出，同时确保流量带宽始终 100% 输出。网络 HASH 问题：由于 AI 计算流量波动大，带宽高，瞬间可以打满一个 400G 端口，但流的数量又非常少，这使得网络路径上的 HASH 不均匀的概率很大，这导致中间路径的不均衡，产生丢包、长尾，影响整体通信效率。</p><p></p><p>在 AI 训练中，长尾问题是非常明显的，它具有木桶效应。如果在一个迭代中有 1000 张卡，其中 999 张已经传输完毕，但有 1 张卡的梯度传输慢了，那么整个训练过程都要等待这张卡。因此，无论是 HASH 还是流控，目标都是补齐木桶的短板，充分利用带宽的同时降低长尾，确保整个网络能够实现高带宽、低时延和高利用率的统一状态。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71311bc6b1584752bf6a6f704c7d2a71.png" /></p><p></p><h2>阿里云 HPN 7.0 架构：AI 计算网络集群架构演进</h2><p></p><p></p><p>在审视了 GPU 集群对网络的关键要求之后，让我们来探讨阿里云的 HPN 7.0 架构是如何解决这些问题的，以及它是如何提高模型训练的效率，达到更极致的性能。</p><p></p><p></p><p>阿里云从去年年初开始设计研发 HPN7.0，在去年 9 月份上线规模化，是专为 AI 设计的高性能计算集群架构。这个架构的特点是单层千卡、两层万卡，存算分离。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2c/2cad6c220bf529583a285aa64405b592.png" /></p><p></p><p>千卡 Segment 设计：我们实现了一个设计，允许 1000 张 GPU 卡通过单层网络交换完成互联。在单层网络交换中，由于是点到点连接，因此不存在 HASH 问题。在这样一个千卡范围内，网络可以发挥出极致的性能，测试结果表明，这种设计下的计算效率是业界最优的。两层网络实现万卡规模：通过两层网络结构，我们能够支持多达十几个千卡 segment，从而实现万卡规模的网络交互。两层网络不仅减少了时延，还简化了网络连接的数量和拓扑。在三层网络结构中，端到端的网络路径数量是乘数关系，而两层网络只有两跳，简化了路径选择，提高了哈希效果。存算分离。计算流量具有明显的规律性，表现为周期性的波动，我们的目标是缩短每个波峰的持续时间，而存储流量是间歇性的数据写入和读取。为了避免存储流量对计算参数同步流量的干扰，我们在设计中将计算和存储流量分配在两个独立的网络中运行。在最近的 GTC 大会上，有关网络设计是采用一张网还是两张网的问题进行了深入探讨。北美几家主要公司的 AI 基础设施网络负责人都参与了讨论，并得出了一致的结论，即分开两张网是最佳选择，这与我们的设计原则相符合。</p><p></p><p>值得一提的是，HPN 7.0 架构，在两周前被选为国际网络顶会 SIGCOMM 的论文。SIGCOMM 是网络领域内最顶级的会议之一，每年仅收录大约 50 篇论文，这些论文都是由网络领域的全球顶尖专家的创新和实践成果。阿里云的 HPN 7.0 架构论文被选中，这具有重大意义。在 SIGCOMM 上发表关于网络架构设计的论文是相当罕见的。上一篇与网络架构相关的论文是 Google 的 Jupiter 网络，第一代 Jupiter 网络在 2015 年发布，第二代则是在 2022 年发表。而 HPN 7.0 的发布标志着 AI 领域内第一篇网络架构的国际顶会论文的诞生，会成为 AI 领域网架构设计的标杆。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/70/707d28866aad3b14044d889ca2c90866.png" /></p><p></p><p>在 HPN7.0 架构下，我们可以通过流量排布，来优化模型训练过程。从 GPU 的视角来看，在整个网络映射过程中，我们可以看到在 1 千卡的范围内，DP 过程可以在千卡范围内完成，无任何网络 HASH 导致的问题。PP 流量较少，可以让其跨越不同的 segment 进行传输。这样的设计使得带宽的利用率能够与模型训练过程紧密结合，从而实现更优的性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/07/078e83835d699f3fdea46216a48b7a0d.png" /></p><p></p><p>HPN 7.0 在端到端的模型训练性能上取得了显著提升，测试数据显示性能，模型端到端的性能提升超过 10%。除了软件架构的优化，HPN 7.0 的硬件和光互联系统也是其成功的关键因素。我们采用了基于阿里云自研的 51.2T 交换机，和 400G 光互联。</p><p></p><h2>GPU centric 高性能网络系统未来展望</h2><p></p><p></p><p>展望未来，高性能网络系统的发展将指向一些明确的方向，这些方向已经随着 AI 基础设施的变革而逐渐显现。从最近 GTC 的发布中，我们可以感知到这一变革的脉动。变革将涵盖从数据中心的电力设计、制冷设计，到网络互联的 scale out 和 scale up 设计等多个方面。</p><p></p><p>从物理层面来看，未来的数据中心将面临更高的功率密度。例如，以前一个机架（Rack）可能只有 20 千瓦的功率，但未来的机架可能达到 50 千瓦甚至 100 千瓦。这样的高功率密度将带来散热方面的挑战，因此，液冷技术将成为必须采用的解决方案，包括交换机在内的设备都将采用液冷技术。</p><p></p><p>GPU 之间的内部互联，如 NVLink 也将在机架内部甚至更大范围内进行扩展，以支持 scale up 的扩展需求。这种 scale up 的扩展需要与网络的 scale out 扩展紧密结合，以确保整个系统的高效性和可扩展性，这也是业界最热门的互联创新话题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/29/29221f4de869d2b86f411e0cf2a729a3.png" /></p><p></p><p>面向未来，我们面临的规模挑战将更大。随着 scale up 网络的发展，我们可能会看到从当前的 8 卡配置扩展到 72 卡或更多，这样的扩展会对网络拓扑带来变化，从而影响 scale out 群网络架构的设计。包括通信框架、容灾设计，以及电力和物理布局等方面都将发生显著变化。这些变化指向了一个以 GPU-centric 的数据中心设计理念。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ba/ba790751ce378d732b8b09851beb0852.png" /></p><p></p><p>此外，网络技术的发展正朝着更高的单芯片交换能力迈进，未来一年内，我们有望看到阿里云的 HPN 8.0，它将是基于 100T 芯片的下一代架构。从 SCALE up 与 SCALE out 结合的架构设计、硬件设计，到液冷系统、IDC 设计的结合，端到端的 AI 基础设施发生变化，以网络设计为中心的 GPU-centric 基础设施时代已经到来。</p><p></p><p>高性能网络协议也将针对 AI 计算持续演进。为了推动这一进程，业界已经成立了超级以太网联盟（UEC），近期阿里巴巴入选该联盟决策委员会，是决策委员会中唯一的一家中国公司，接下来阿里云将在 AI 基础设施网络的高性能方向上重点投入，与各主要公司一起，共同致力于下一代更高性能网络系统的设计和开发。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e13ff2745ce7d222e772163324f836c4.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1rDSSpvbNkcQD4xeMvBZ</id>
            <title>揭秘大模型技术在快手搜索的应用 | QCon</title>
            <link>https://www.infoq.cn/article/1rDSSpvbNkcQD4xeMvBZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1rDSSpvbNkcQD4xeMvBZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 10:20:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 快手搜索部门, 大模型技术, 多模态技术, 智能问答
<br>
<br>
总结: 本文介绍了快手搜索部门技术专家在 QCon 2024 北京分享的大模型技术在快手搜索中的应用。演讲重点探讨了大模型技术的具体应用，特别是多模态技术的最新科研进展，以及大模型在智能问答领域的落地实践。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>本文整理自快手搜索部门技术专家许坤在<a href="https://qcon.infoq.cn/2024/beijing">QCon&nbsp;2024&nbsp;北京</a>"的分享“<a href="https://qcon.infoq.cn/2024/beijing/presentation/5751">大模型技术在快手搜索的应用</a>"”。演讲深入探讨了大模型技术在快手搜索领域的具体应用，重点介绍了多模态技术，尤其是多模态理解和生成方面的最新科研进展。另外，在 8 月 18-19 日即将举办的 AICon 上海站，我们也设置了【多模态大语言模型的前沿应用与创新】专题，本专题将聚焦 LLM 在多模态领域的应用和创新，探讨如何将 LLM 与图像、音频、视频等多媒体数据融合，实现更智能、更自然的交互体验。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><blockquote>本文由 InfoQ 整理，经许坤老师授权发布。以下为演讲实录。</blockquote><p></p><p></p><p>我们在去年 3 月底至 4 月初成立了一个联合项目组，致力于大模型技术的研发。到了 8 月份，我们发布了快手的第一个大模型，命名为快意大模型。</p><p></p><p>快意大模型目前有三个不同的规模版本，分别是 13B、66B 和 175B。在去年 8 月份的评估中，我们的模型已经达到了或者说接近 GPT-3.5 的性能水平。自那以后，我们团队在内部进行了大量的迭代和优化。特别是 175B 规模的模型，目前在很多场景中，特别是在中文场景下，表现已经超过了 GPT-4。这一进步已经被实际应用到了快手的多个具体产品中，实现了技术的落地和商业价值的转化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2e/2e8d3da2449e955eff2213796aef67e9.png" /></p><p></p><p></p><h3>快手大模型落地场景</h3><p></p><p></p><p>快手大模型技术目前已经在多个领域进行了尝试和应用。以下是几个具体的落地实例：</p><p></p><p>AI 小快：用户在观看视频时可以通过 @AI 小快来提问有关视频理解的问题。我们的大模型会在评论区中对这些问题进行智能解答，提供用户所需的信息。智能客服：通过大模型的强大能力，智能客服能够更精准地理解用户需求，并提供更加人性化的服务。商家视频文案生成：这项服务使得我们的 ToB 用户能够更加便捷地创作文案和制作视频，提高了内容生成的效率和质量。</p><p></p><p>尽管短视频在视觉呈现上具有优势，但在某些场景下，如 how to 类查询或知识性问答，短视频内容繁多，用户需要观看完整视频才能找到答案，这实际上降低了搜索效率。此外，短视频是由人创作的，创作者与用户之间存在一定的鸿沟。在没有足够视频供给的情况下，我们希望大模型能够对用户的问题进行解答。以下是我们四个产品的具体形态：</p><p></p><p>GPT 卡片：当用户提出问题时，GPT 卡片会在搜索结果页面直接输出答案。例如，用户询问“桂花不开花是什么原因？”时，我们会利用 RAG 技术聚合视频和网页结果，直接呈现答案。AI 搜：在某些问题没有索引或视频供给的情况下，AI 搜会利用大模型在线实时生成结果，弥补 GPT 卡片的不足。这也是一种漏斗逻辑，引导用户在看完 AI 搜后，如果有后续问题，进入多轮对话场景。GPT 多轮对话：用户点击搜索框旁的 AI 图标后，会进入多轮对话场景。与 AI 搜相比，我们会重点放在多轮对话的理解上，并提供特定领域的能力，如文生图设计和朋友圈文案创作。角色聊天：在上线这些产品后，我们发现许多用户除了知识获取需求外，还有与 AI 进行交流的需求，尤其是在深夜。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b9c09fd5c1b0d196a4870bafe7b36cf2.png" /></p><p></p><p></p><h3>产品实践：AI 搜 &amp; 角色聊天</h3><p></p><p></p><p></p><h4>搜索智能问答</h4><p></p><p></p><p>搜索智能问答的设计旨在提升搜索效率和补充搜索供给。</p><p></p><p>我们构建了一个框架，该框架以逻辑流程图的形式呈现。当用户提出一个查询，系统首先进行视频检索，这包括快手自有搜索流水线中的粗排、精排、个性化排序等步骤。在获取相关视频后，系统还会利用快手丰富的知识库资源对查询进行文档检索，检索到的结果将进行答案抽取，并使用生成式模型进行答案聚合。如果查询没有相关的索引资源，我们的基座模型将通过指令检索逻辑进行兜底。</p><p></p><p>在下图框架中，蓝色部分代表抽取式模型，而红色部分代表生成式模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b9c09fd5c1b0d196a4870bafe7b36cf2.png" /></p><p></p><p>框架中还加入了一个强化学习模块，该模块与传统的大模型训练中的 RLHF 或 DPU 有所不同。我们认识到，答案的呈现形式对用户体验有显著影响。</p><p></p><p>例如，有时我们希望答案以列表形式出现，有时是图文对，有时则可能是纯文字。强化学习模块的目标是教会模型以最合适的形式回答特定类型的问题。强化学习的信号通常基于用户看到结果后的后验行为，如停留时长、后续查询搜索等。这些信号将反向传递给模型，使模型在学习过程中既能满足用户需求，也能逐步提升用户体验。</p><p></p><p>通过这种方式，我们可以形成一个闭环，使模型能够每天在线自我迭代。</p><p></p><p>在开发过程中，我们面临了三个主要挑战。</p><p></p><p>大模型的幻象：早在三年前 GPT-1 出现时，学术界就对大模型的必要性存在分歧，分为两派，一派主张走符号推理（Symbolic Reasoning）路线，瞄准大模型幻象难以解决的痛点。现在，随着 ChatGPT 等模型的效果显著，大家开始集中研究如何检测大模型幻象。在实际应用中，我们希望有一个模型或模块能够告诉系统，大模型的输出存在问题。低质索引资源影响答案准确率：在我们的系统中，落地时面临的一个严重问题是资源本身可能存在重复。例如，一个问题可能同时有正确和错误的答案，或者不同的人对同一答案的看法不同。我们如何对这些答案进行聚合，这是我们在研究中需要解决的问题。Multi-Hop 事实类问题：这类问题在检索时通常无法直接找到答案，因为它们需要进行一定的推理。</p><p></p><p>尽管大模型有一些索引资源，我们已经对这些索引的质量进行了严格控制，但仍有少数低质资源可能进入最终的排序模块。</p><p></p><p>我们观察到，绝大多数正确答案通常能够得到足够多的索引资源的支持。基于这一发现，我们构建了一个图神经网络模型。该模型的工作机制如下：它从每个文档（doc）中抽取答案，并计算每个答案被其他文档支持的程度。同时，我们还会计算答案之间的相似度，然后利用整个图的模式来判断哪个答案最有可能是正确的。这是一个常规的解决方案，它在离线测试中表现出色。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/6f/6f761bc7be56272d5fcec724f93919d4.png" /></p><p></p><p></p><h4>回答 Multi-Hop 事实类问题</h4><p></p><p></p><p>我们在线实施了一个类似“source tree”的概念。逻辑是，面对一个复杂问题时，我们需要将这个问题拆解成多个子问题。为此，我们开发了一个模块来拆解问题。拆解后，我们会针对每一个子问题进行解答。当子问题得到正确解答时，我们会进一步探索答案，直到最终解决问题。如果某个子问题没有得到解答，我们会退回到问题的根节点，并寻找另一条路径。有时如果问题确实无法解答，我们也会接受这一现实。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/00e08b8bc45783f62d9f447e75bc8243.png" /></p><p></p><p></p><h3>升级到角色聊天模型</h3><p></p><p></p><p>自去年以来，随着 AI 技术的火爆以及国内资本市场的变化，我们观察到市场对角色聊天这一概念非常认可。用户不仅需要获取信息，他们的情感需求也同样重要，这正是我们需要提供的价值。我们的产品框架包含三个主要部分：</p><p></p><p>角色库：用户可以与所有已存在的角色进行聊天。当前对话角色：用户与当前正在对话的角色进行互动。角色发现：用户可以在发现页寻找他们可能感兴趣的新角色。</p><p></p><p>在角色聊天领域，我们面临一个基本问题，即如何将现有的语言模型升级为角色聊天模型。虽然整体方案没有变化，包含预训练、监督训练和强化学习模块，但每个阶段使用的数据类型有所不同。在角色聊天模型中，我们主要使用了剧本数据、对话数据和人人对话数据。与机构模型使用 3T 到 6T token 的数据量相比，角色聊天模型追求的是少而精，通常 100B 到 200B 的数据量就足够了。</p><p></p><p>在指定微调阶段，基座模型预训练阶段需要几百万到上千万的指定数据。而在角色聊天中，我们关注的是三类数据：</p><p></p><p>模型是否能理解角色的含义；模型是否能理解场景的意义；模型是否具备通用能力和多轮对话能力，尤其是长上下文的处理能力。</p><p></p><p>我们特别构造了不同角色间的场景对话能力，以及长上下文对话（long SFT）的数据。虽然在搜索场景中，很多人认为 DPU 没有太大作用，但在角色聊天中情况完全不同，因为高情商的回复与低情商的回复对用户体验的影响非常大。GPT-4 在这方面也无能为力，因为它提供的是更正式的回复，与角色聊天所需的口语化回复不同，常规使用 GPT-4 进行打标的方法在角色对话中并不适用。</p><p></p><p>因此，在强化学习阶段，我们进行了很多用户模拟器的开发，并结合人工标注进行对齐，以提升模型的情商和对话质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71f63b192609c1281a0e97ffa3485233.png" /></p><p></p><p></p><h4>挑战一：如何构建不同角色多轮对话数据</h4><p></p><p></p><p>由于我们没有大量线上数据，即使有也不一定适用。因此，我们必须从冷启动阶段开始生成数据。我们会生成数万甚至数十万的角色，然后从这些角色中两两配对，并让 GPT-4 在给定场景下生成合理的对话。接下来，我们会进行简单的人工筛选，筛选出的数据将用于训练模型。有了这个基础模型后，我们将其上线。上线后，我们会为用户提供一个功能，允许他们自己创建角色。然后，我们会从用户创建的角色中获取数据，逐步更新原始的数据集。通过这样的多次迭代，我们最终能够达到一个比较理想的效果，使模型能够更好地理解和生成符合角色特性的对话。这个过程需要不断地收集用户反馈，优化数据集，并训练模型，以实现角色聊天功能的最佳表现。</p><p></p><p></p><h4>挑战二：如何增强模型的上下文理解能力</h4><p></p><p></p><p>众所周知，GPT 或 Transformer 这类模型框架在进行 NSP（Next Sentence Prediction）任务时，通常是预测下一个 token，这种预测往往依赖于局部信息，而不太涉及全局信息。为了增强模型的长上下文理解能力，我们采取了以下措施：</p><p></p><p>● 代码预训练：我们加入了代码预训练数据，这样做可以天然地增强模型对于远距离注意力（attention）的效果，从而提升模型对长上下文的理解。</p><p></p><p>● 线上长对话数据：我们利用线上的长对话数据，让 GPT-4 帮助我们进行标注，以识别出哪些回复可能与前文历史紧密相关。如果发现有相关性，我们会采用拒绝采样（reject sampling）的方式，通过人工挑选来构建长上下文对话训练数据。</p><p></p><p>● 增强上下文效果：利用特别构建的数据，我们进一步增强了模型的上下文效果，使其能够更好地理解和回应长对话中的上下文信息。</p><p></p><p></p><h3>技术探索：多模态大模型</h3><p></p><p></p><p>与大语言模型（LLM）相比，多模态模型主要增加了两种模态：语音和视觉（包括图像和视频）。目前常规的方案基本上是以大模型作为基础，通过一个项目将多模态特征映射到 LLM 中的固定数量的 token 上，然后进行建模。最终，根据需要输出图像或语音，只需选择不同的解码器（decoder）即可。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/5498120af43b1029edb698d39ad592dd.png" /></p><p></p><p>这样的大型模型存在一个显著问题，它们经常使用所谓的"model adapter"结构。在这种结构中，视觉特征或语音特征被固定（fix），然后整个模型的训练主要集中在训练这个 adapter 上。这种做法引发了一系列问题。</p><p></p><p>● 多模态作为 prompt 的弱点：在建模过程中，多模态输入通常被当作 prompt 使用，它与随后文本的交互天生较弱。这是因为目前大多数模型都采用仅解码（decode-only）框架，导致多模态输入与模型的交互不够充分。</p><p></p><p>● 任务复杂性：当前的任务，尤其是多模态任务，非常复杂。如果将模型的视觉特征抽取或 LLM 固定，那么 adapter 的训练潜力将非常有限。目前，adapter 主要采用 cross attention 的方式，这可能会严重限制整个模型的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2f/2f0ccfddfea157705960e3ce6631b95a.png" /></p><p></p><p>基于现有问题，我们提出了一个新的想法，即将视觉或语音视为一种外语，即另一种语言。</p><p></p><p></p><h4>“万物皆可 token”</h4><p></p><p></p><p>以 LLama 模型为例，我们的处理方式是相同的，不论是中文数据还是图像数据。我们希望将图像离散化，转换成 token，即"万物皆可 token"的理念。Token 化后的数据输入到基础模型中，对于基础模型而言，它们仅仅是一串 token，没有任何区别。这样做的好处在于我们可以随意交叉这些 token 的位置。</p><p></p><p>为了实现这一目标，我们设计了一个名为"Image Tokenizer"的组件，作用是将图像、视频或音频转换成一系列 token，然后输入到基础模型中。</p><p></p><p>我们选择使用 LLM 的原因是，LLM 已经将人类文字知识全部压缩在内，在基础之上进行推理、理解和生成任务时，它会具有天然的优势。与从头开始训练模型相比，使用 LLM 作为基础模型可以带来更好的效果，这是我们的基本动机。通过这种方式，我们可以更有效地处理多模态数据，并提升模型的整体性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5e/5e29d51d51fbb1c0d8efed9fde4cd87f.png" /></p><p></p><p>我们最近有一篇论文被 ICLR 接收，论文的基本思想是，当我们处理图像时，首先将其转换成 token，与文本 Tokenizer 处理后的文本拼接在一起，然后输入到模型中。我们的模型名为 LaVIT，其输出的 loss 与语言模型相同，都是采用 ASP loss 预测下一个 token。</p><p></p><p>与之前方案的最大区别在于，我们将图像离散化，图像的每个 patch 都有一个独特的 ID，在语言模型中它就是一个语义 token，这样我们可以在 loss 上实现同质化处理。通过这种方式，无论是视频理解还是图像理解，只需将图像转换为 token 输入模型，然后让它解码成文字就可以将图像理解任务建模。</p><p></p><p>此外，我们还可以进行生成任务，比如给模型一张图片和一段文本，然后要求它输出图片。对模型来说这没有难度，因为它只是一系列 token 的输入和输出。唯一的区别在解码阶段，我们通常会选择使用 Stable Diffusion 或 DIT 等方法来进行解码，这种方法使我们能够更灵活地处理多模态数据，并在不同的任务中实现更好的性能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9f/9f84b74bedbb52f582767df09b4d627f.png" /></p><p></p><p>我们的 Tokenizer 设计涉及离线预训练过程，这个过程不需要文本，只需要图像。图像输入后，我们会使用 VIT（Vision Transformer）作为特征提取器，将图像分割成若干个 patch。每个 patch 都有一个对应的 embedding。</p><p></p><p>在这个基础上，我们进行 KNN（K 最近邻）检索，将这些 patch 映射到一个 Codebook 中。这个 Codebook 可以理解为我们自然语言中的词汇表，其中包含了大约 1 万到 2 万个“词汇”。有了这些词汇后，我们可以将图像中的每个区域映射成一个词。然后，我们会对编码过程使用一个解码 loss，即要求模型能够恢复出原始图像，这是一个回归 loss，具体来说是均方误差（MSE）loss。</p><p></p><p>完成这个离线预训练过程后，我们将得到一个优秀的图像编码器和解码器。编码器的作用是将图像转换成一系列的 token，而解码器的作用是将这些 token 还原成图像。解码器的基础我们采用了 Stable Diffusion，并对其做了改进，实现了动态编码。</p><p></p><p>动态编码的动机其实很简单：在很多图像中，颜色可能非常相近，比如都是红色。我们不希望模型对这类图像使用过长的 token，因为这会使训练过程变得冗长。因此，我们引入了一个名为 token selector 的组件，它会在图像中选择它认为重要的 token 进行编码。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8a/8a1cc9046f52dd785ca0095e35b04789.png" /></p><p></p><p>下图展示了视觉 Tokenizer 的效果：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a0/a09176b9135c842cf414f608e91238d6.png" /></p><p></p><p>左侧第一张图我们仅使用了 95 个 token，可以从图中观察到，因为有许多颜色是一致的，而右侧灰白部分表示我们没有选择对这些区域进行编码，我们保留的有颜色区域即是保留的 token，未保留的则是我们去掉的部分。</p><p></p><p>观察右侧的钓鱼图片，可以看到图像中包含的语义信息相当复杂，因此我们大约使用了 108 个 token 来表达。而下面那张鸟站在树上的图片，实际上只需要 79 个 token 就能够进行有效编码。</p><p></p><p>通过这种动态长度编码的方式，我们能够对图片进行更为高效的编码处理。这种编码方法在我们的模型中能够显著提升训练速度，大约可以提高 3 到 4 倍，从而使得整个模型的训练过程更加快速和高效。</p><p></p><p>图像编码完成后，接下来的步骤是将其映射到一个词表中。我们使用的是一个包含 16,000 个词汇的词表，每个词汇都代表了一个特定的含义。通过可视化，我们可以发现特定的编码，比如 13014，它代表的是人手臂的语义，而编码 2223 则学会了代表铁轨的语义。本质上，我们的过程是将图像拆解，然后进行语义聚类，之后将其与语言进行同步建模。</p><p></p><p>图像的处理也是类似的。我们把图像分解，将其中的每一部分映射到相应的语义上，并与语言的语义进行融合，输入到 LLM 中。通过这种方式，我们能够将图像和文本统一到同一个语义空间中，使得模型能够更好地理解和处理多模态数据。这种方法不仅提高了模型的效率，也增强了其处理复杂任务的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3c/3ce7cc55811656a4f6ffc44689f3c546.png" /></p><p></p><p></p><h4>多种任务尝试</h4><p></p><p></p><p>完成图像编码和词表映射的工作后，我们进行了多种任务的尝试和应用。首先，我们实现了 Image Caption 和 Visual QA 任务。用户可以直接输入一张图片，然后大模型能够生成对图片内容的描述。例如，模型能够形容图片中的景象或物体。比如，用户可以上传一张图片并提出问题，比如询问图片中有多少只斑马，模型能够理解问题并回答出具体的数字，如“有三个斑马”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/593a04a40600414701383baa2010422b.png" /></p><p></p><p>在下面的图表中，我们展示了一些基准测试上的结果。这些结果是我们在去年 12 月份提交论文时的数据。当时，在多模态模型领域，BLIP-2 的效果被认为是最好的，如果大家对多模态模型有所了解，可能对这个模型会比较熟悉。然而，在我们的实验设置中，当我们使用相同规模的大约 7B 参数的基础模型时，我们的结果实际上远远超过了这个竞品。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/51/51f51f22da70299407134c57defdc559.png" /></p><p></p><p>我们的框架设计得非常通用，既可以处理图片理解任务，也可以进行图片生成。在图片生成方面，我们展示了一些效果，看起来也相当不错。坦白来讲，与当前非常受欢迎的 Mid Journey 和 Stable Diffusion 相比，我们的生成质量并不逊色。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c3/c3c903879849eb029063117420901806.png" /></p><p></p><p>我们进行了一项实验，目的是比较我们的方法与一个强有力的竞争对手 SDXl 在文本提示理解方面的差异。我们特别想知道，在采用 LLM 之后，我们是否能够更好地理解文本提示。</p><p></p><p>实验中，我们给出了一个文本提示，内容是：“桌子上有两个苹果，这两个苹果没有一个是红的，都是绿的。” 结果显示，SDXl 对这个提示的理解相对较弱，它生成的图像中既有红色的苹果也有绿色的苹果。而使用我们的方法，基于语义建模，生成的图像则非常好，准确地反映了文本提示的要求，即生成了两个都是绿色的苹果。</p><p></p><p>另一个例子是，文本提示描述了一只猫位于长椅下方的篮子里。SDXl 生成的图像在空间理解上表现不佳，因为它通常使用 CLIP 进行文本建模，与我们使用 LLM 的方法完全不同。相比之下，我们的模型明显在空间理解上做得更好，能够准确地描绘出猫在指定位置的场景。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1d/1d3d6b0a42936d770186b45bb3c962e6.png" /></p><p></p><p>我们展示了一些文本到图像（Text to Image）的结果，与我们的结果比较接近的是 Parti 的效果，在 FID（Fréchet Inception Distance，一种评估生成图像质量的指标）这个维度上非常接近。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f0/f05cfeeaafd20a3ac4d2074ad41c4961.png" /></p><p></p><p>我们的框架非常灵活，不仅可以支持从文本生成图像（文生图），还能处理图像生成文本（图生文）、以及图像加文本或图像加图像的组合（图加文加图）。</p><p></p><p>如果我们在左边给出一张猫的图片，然后在右边给出一个文本提示，比如说“这只猫在海滩上”，我们的模型就能够生成出一张猫在海滩上的图像。如果我们想让这只猫戴上眼镜，只需在文本提示中加入这一要求，模型同样能够生成出相应的效果。这是一个图像加文本输入的例子。</p><p></p><p>我们还可以进行图像和图像的输入组合。比如，如果我们将梵高的画作和猫的图片放在一起作为输入，模型能够生成出具有梵高风格的猫的图像。同样，如果我们将一只朋克风格的狗和猫的图片放在一起，模型就能生成出朋克风格的猫的图像。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/1b/1bef78b536433719d1b89b4997fcb3ac.png" /></p><p></p><p>我们还进行了一项更复杂的实验，即文加图加文加图加文，也就是三个文本和两个图像的组合。例如，假设我们说“这是一幅画”，然后给出一张狗的图片，并希望将这只狗以那幅画的风格呈现出来，我们的模型同样能够生成这样的图像。当然，如果你有更具体的特定需求，比如需要更多的文本描述，或者想要结合两张图片、三张图片以及文本作为输入，这也是可行的。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/07/07157d3f8694e46dd455754fdf8ed32a.png" /></p><p></p><p></p><h4>Video-LaVIT 框架</h4><p></p><p></p><p>今年第一季度，我们开发了一个名为 Video-LaVIT 的框架，介绍一下它的基本思想。</p><p></p><p>在之前框架的基础上，我们进行了视频编码和解码的工作。目前，大家普遍知道 GPT 这样的框架属于较高级的结构。但在国内，许多人处理视频的方法是将其拆解成多帧，然后分别进行建模。另一种流行的方案是 Sora。</p><p></p><p>我们的工作始于 2 月 6 日，原本计划稍后再推出更新版本，但 Sora 的进展比我们快得多，并且效果显著。Sora 的方案考虑了 3D 方案，与单帧抽取方案相比，其 token 数量非常庞大。这会带来一个问题：如果有 100 万个 token，学习它们之间的 attention 关系将需要巨大的数据量和计算资源，这是我们所不具备的。</p><p></p><p>我们并没有选择 Sora 的方案，也没有选择单帧抽取方案，因为这样会丢失帧与帧之间的动作时序变化。最终，我们选择了一个从编解码领域借鉴的思路，这是一个折中的方案，旨在保留视频帧之间的时序信息，同时避免上述两种方案的缺点。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/47c44ebdeec0c0df8ac28b45ac4b1c60.png" /></p><p></p><p>如果对视频编码有所了解，你就知道 H.264 方案，这是一个相对传统的标准。它的基本思想是在视频编码或压缩时，将语义信息单独压缩，特别是所谓的运动向量（Motion Vectors）。这个方案的核心思想是对视频中每一帧（patch）与下一帧之间的动作变化进行建模，而像素级别的变化则被正交解耦。我们不需要对每一帧都进行单独建模，也不需要像 Sora 方案那样创建一个非常复杂的 3D token。</p><p></p><p>我们的基本方案采用了关键帧加运动向量（key frame + motion vectors）的方法。简单来说，我们会从视频中提取关键帧，然后基于这些关键帧对后续动作进行运动向量建模。这样，我们就无需保留整个视频的所有关键帧，只需保留运动向量即可。同时，这种方法也不会丢失视频的时序信息。</p><p></p><p>基于这个概念，我们设计了一个编码 Tokenizer 和解码 Detokenizer，用于将视频编码并恢复成期望的视频效果。这种方法允许我们以更高效和节省资源的方式来处理视频数据，同时保留了视频内容的核心信息和动态变化。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5c/5cb98bb9a94ccc0a87e4d06bf59f09e9.png" /></p><p></p><p>我们的框架中新增了一个组件，称为 motion tokenizer，它的功能是将视频中的动作编码成 token，并将这些 token 输入到 Video-LaVIT 模型中。这个 motion tokenizer 的训练过程与 LaVIT 的训练过程非常相似，都是将向量通过语义编码转换成 token。具体来说，motion tokenizer 的训练方案与 LaVIT 相同，它使用 MSE loss 来进行训练，这是一个离线过程。与 LaVIT 不同的地方在于，motion tokenizer 的训练不需要文本对齐，它仅依赖视频本身即可完成训练。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/3e/3e722d459981a02356e9e5001611c0fd.png" /></p><p></p><p>我们还开发了一个解码器，目的是在视频预测阶段将关键帧和运动向量恢复成视频效果。为此，我们训练了一个名为 3D U-Net 的框架。简单来说，操作过程是将关键帧和运动向量输入到 3D U-Net 中，然后对其进行加噪处理，接着进行去噪，最终得到视频的输出效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e8/e85ba0d3d71b72e0bd0b59ee5c7e743c.png" /></p><p></p><p>在离线训练 Tokenizer 的过程中，我们首先对视频进行编码，然后再次解码，以检验视频信息是否能够被有效复原。尽管我们观察到复原视频的分辨率较低（仅为 520P），因此效果并不完美，但基本的语义信息已经通过模型学习到。</p><p></p><p>我们特别在两个任务上进行了重点评估。首先，我们对图像理解（image understanding）进行了评测，发现在现有的图像理解基准测试上，我们的效果是最佳的。其次，在视频理解方面，特别是在 ActivityNet-QA 数据集上，该数据集用于衡量视频中的动作，我们的效果显著优于现有所有工作。这是因为我们对 motion 的建模非常精准，而其他许多工作往往忽略了对运动的建模。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/545803b1f79832f3c78c28147958c893.png" /></p><p></p><p>我们还尝试生成了较长的视频，用户只需输入一段文本或者提供一张图片，模型就能基于这张图片生成视频。在没有进行任何控制的情况下，视频的稳定性已经达到了一个相当不错的效果。这表明我们的模型在处理长视频生成任务时，即便在没有额外控制机制的情况下，也能够保持较高的稳定性和合理性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/470b860aca7dd0dd83c22be93dffa086.png" /></p><p></p><p>我们制作了一个较长的视频，大约 10 秒左右。LLM 本身对输入长度没有太多限制，不过我们训练集中的大部分视频都在 6 秒左右。因为我们的训练集未曾见过更长的视频，这可能导致对后面关键帧的预测存在一些问题。但总体来说，生成的视频结果还是符合预期的。</p><p></p><p>我们的长视频是通过拼接多个几秒的视频片段来实现的。虽然与 Sora 相比，我们的效果还有一定差距，但个人认为这个差距可能不是由模型本身造成的，而可能是因为我们目前使用的数据还不够充分。我们没有使用任何闭源数据，也没有使用快手的数据，目前的效果是基于公开数据实现的。</p><p></p><p>我们的 Video-LaVIT 框架已经引起了包括 Stable Diffusion CTO 在内的一些业界人士的关注。大家对这个框架的优势有明确的认识。</p><p></p><p>与 Sora 相比，我们只需要其 1/10 的 token 即可进行建模。虽然 1/10 token 可能会在最终生成质量上带来一些损失，但它对视频的理解能力依然非常强。我们进行了一些评测，结果表明我们的效果可以与 Sora 相媲美。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/76/76e80db81b1970f851bfc6310a34b4dc.png" /></p><p></p><p>众所周知，广告领域是视频生成的一个非常重要的应用场景，包括在快手内部，我们也进行了一些广告生成的尝试。这些广告通常时长大约在 10 到 15 秒之间，这正好是我们的文生视频模型能够充分发挥作用的场景。因此，我们的模型在广告制作和视频内容生成方面具有巨大的潜力和应用价值。</p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 <a href="https://aicon.infoq.cn/2024/beijing/track">AICon 全球人工智能开发与应用大会</a>"，汇聚顶尖企业专家，深入端侧AI、大模型训练、安全实践、RAG应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/51/51770673116f76b8740cfe9f1e48c1c3.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IJyPIFXaYKu8ZbgMcRFK</id>
            <title>十年磨一剑，这家云巨头正在借助AI探寻发展新机遇</title>
            <link>https://www.infoq.cn/article/IJyPIFXaYKu8ZbgMcRFK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IJyPIFXaYKu8ZbgMcRFK</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 09:46:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 行业应用, 亚马逊云科技, 合作伙伴计划
<br>
<br>
总结: 生成式AI时代的黎明已经到来，亚马逊云科技认为未来真正能创造最大价值的将是生成式AI的行业应用。在2023亚马逊云科技中国峰会上，亚马逊全球副总裁储瑞松表示，生成式AI将以前所未有的方式改变各行各业，为全球经济贡献7万亿美元的价值。亚马逊云科技发布了生成式AI合作伙伴计划，旨在助力企业更快地应用生成式AI，打造“人工智能+”时代的竞争优势。 </div>
                        <hr>
                    
                    <p></p><p>“生成式AI时代的黎明已经来临，未来真正能创造最大价值的将是生成式AI的行业应用。”</p><p></p><p>近日，在2023亚马逊云科技中国峰会上，亚马逊全球副总裁、亚马逊云科技大中华区总裁储瑞松如是说。</p><p></p><h2>生成式AI浪潮下的行业机遇</h2><p></p><p></p><p>储瑞松表示，生成式AI时代的黎明已经到来，它将以前所未有的方式改变各行各业。麦肯锡的研究报告预测，到2030年前，生成式AI有望为全球经济贡献7万亿美元的价值，中国将凭借战略性投资分享其中的1/3。</p><p>&nbsp;</p><p>亚马逊云科技认为，未来真正能创造最大价值的将是生成式AI的行业应用。企业需要根据自身业务场景选择合适的模型，并结合企业自身的私有数据进行模型的定制，才能打造出有差异化的创新应用，解决高价值的特定行业场景的挑战，创造新的业务模式或机会。</p><p>&nbsp;</p><p>多年来，亚马逊云科技助力企业完成生成式AI及相关应用的构建。在最底层的算力层，亚马逊云科技提供来自英伟达的高性能AI芯片，以及自研的高性价比、低能耗AI芯片Trainium和Inferentia，满足客户不同的算力需求。</p><p>&nbsp;</p><p>在中间的工具层，亚马逊云科技通过Amazon Bedrock为企业提供构建生成式AI应用最便捷的模式。Amazon Bedrock可提供一系列领先的大模型选择，包括开源模型和闭源模型，并支持客户将自己的定制模型导入，以完全托管的API方式进行访问。</p><p>&nbsp;</p><p>在顶层的应用层，亚马逊云科技发布了开箱即用的企业级生成式AI助手Amazon Q，包括Q for Business和Q for Developer，为企业提供智能客服、智能导购等应用。</p><p>&nbsp;</p><p>尽管提供了从底层至上层的全链路服务，但亚马逊云科技认为，企业在落地生成式AI应用的过程中，仍有五个要素尤其值得关注，包括业务场景的选择、模型的选择、是否能够结合企业自身的私有数据进行模型的定制、是否符合负责任的AI的原则、以及对应用进行持续提升的能力。</p><p>&nbsp;</p><p>很多企业生成式AI之旅的第一站是打造面向内部的应用，因为起步成本、门槛和风险相对较低，且可以直接提高生产力，包括面向企业内部的客户评论反馈、舆情分析、财务、运营报表的分析、会议摘要、内部QA机器人、以及代码伴侣等等；在对外的场景上，B2C行业的场景应用要比B2B的行业场景应用走得更快一些，包括聊天室和客服的实时翻译、智能导购、智能客服问答、AI伴侣以及AI助教等等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41fa8c757d45a18de93549f026f9a246.png" /></p><p></p><p>亚马逊云科技生成式AI合作伙伴计划发布</p><p>&nbsp;</p><p>亚马逊云科技希望利用在算力、模型和框架、以及应用层面丰富的产品和服务，成为企业构建和应用生成式AI的首选。这次峰会上，亚马逊云科技推出“亚马逊云科技生成式AI合作伙伴计划”&nbsp;&nbsp;。该计划旨在助力企业更快地应用生成式AI，打造“人工智能+”时代的竞争优势。亚马逊云科技将联合生成式AI领域顶尖的3+1类合作伙伴，为企业提供全方位的模型、工具、应用和集成服务。3是指大模型提供方、工具链提供方、以及各类开箱即用的生成式AI应用和方案提供方。1是指系统集成商合作伙伴。亚马逊云科技将为加入本计划的合作伙伴提供全面的支持，投入技术专家与合作伙伴共创，帮助合作伙伴更好地将他们的创新和亚马逊云科技的服务适配和集成，并支持合作伙伴方案上架亚马逊云科技Marketplace，服务中国客户的同时触达全球客户。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GYCdyqMlrLvbmP793M1e</id>
            <title>AI和数据库真正的大一统时代要来了？OpenAI突然收购实时分析数据公司Rockset，剑指AI内存</title>
            <link>https://www.infoq.cn/article/GYCdyqMlrLvbmP793M1e</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GYCdyqMlrLvbmP793M1e</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 09:29:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 收购, Rockset, 数据库公司
<br>
<br>
总结: OpenAI宣布收购以数据索引及查询功能而闻名的实时分析数据库公司Rockset，以整合其技术为所有产品提供基础设施支持。Rockset团队成员将加入OpenAI，现有客户将逐步离开Rockset平台。收购细节尚未披露，Rockset创立于2016年，由前Facebook工程师共同创立，提供基于云的实时分析数据库。Venkat Venkataramani担任创始人兼CEO，Dhruba Borthakur担任联合创始人兼CTO，Tudor Bosman担任架构负责人。Rockset产品不断提取和索引数据，支持推荐引擎、物流跟踪仪表板等应用。Rockset已成功筹集超过1.175亿美元资金，拥有知名客户如Meta和JetBlue。OpenAI收购Rockset是为了强化其跨产品检索基础设施，吸纳实时分析专家团队，提升AI应用的实用性和强大性。Venkataramani表示Rockset将成为OpenAI产品套件的检索基础设施，帮助解决AI应用大规模数据库难题。 </div>
                        <hr>
                    
                    <p></p><h2>OpenAI收购数据库公司Rockset</h2><p></p><p></p><p>近日，OpenAI正式宣布收购Rockset——这是一款以数据索引及查询功能而闻名的实时分析数据库。OpenAI 在其官方博客上发表的一篇文章中表示，它将整合 Rockset 的技术来“为其所有产品的基础设施提供支持”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2d/2da3e1c025557638a40ad97b960f7a26.png" /></p><p></p><p>Rockset 团队的成员将加入 OpenAI，而 Rockset 的现有客户也将“逐步”离开 Rockset 平台。完整文章如下：</p><p>&nbsp;</p><p></p><blockquote>AI技术有望改变个人和组织运用自身数据的方式，也正因如此，我们（OpenAI）决定收购Rockset。Rockset是一款领先的实时分析数据库，可提供国际一流的数据索引与查询功能。&nbsp;Rockset使得用户、开发人员及企业在使用AI产品及构建智能化应用程序时，能够更好地运用自身数据并访问实时信息。&nbsp;我们将整合Rockset技术以支持OpenAI的跨产品检索基础设施，收购完成后Rockset旗下卓越的团队成员也将加入OpenAI。&nbsp;OpenAI公司首席运营官Brad Lightcap介绍称，“Rockset的基础设施能够帮助企业客户将其数据转化为可操作的情报。我们很高兴能够将Rockset的底层技术整合进OpenAI产品，从而为客户提供更多助益。”&nbsp;Rockset公司CEO Venkat Venkataramani也指出，“我们很高兴加入OpenAI，通过为AI方案引入强大检索功能的形式，帮助用户、企业及开发人员得以充分利用其数据。”&nbsp;Rockset功能的整合工作已经启动，敬请期待更多后续消息。</blockquote><p></p><p>&nbsp;</p><p>此次收购中的财务条款细节尚未披露。</p><p></p><p>Rockset 由前 Facebook 工程师 Venkat Venkataramani 和 Tudor Bosman 以及数据库架构师 Dhruba Borthakur 于 2016 年共同创立，提供基于云的实时分析数据库，允许开发人员构建数据密集型应用程序。值得注意的是，这支团队构建了RocksDB，这是 Google LevelDB 的一个分支，LevelDB 是由 Jeff Dean 亲自编写的可嵌入 NoSQL 数据库。</p><p>&nbsp;</p><p>Venkat Venkataramani 担任创始人兼CEO，曾任Facebook基础设施团队的工程总监，所带领的团队为15亿用户管理在线数据服务；更早之前，Venkat在甲骨文公司担任主要技术人员，同样从事数据库工作。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ac/ac1b0e64d3414c52f5f370b4e0316d61.png" /></p><p></p><p>Dhruba Borthakur是公司联合创始人兼CTO，他也同样在Facebook从事过数据库工作，还是Hadoop分布式文件系统的创始工程师之一，以及开源Apache HBase项目的贡献者。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/12/1269e75f385892f11e137f8dfd1c8ae6.png" /></p><p></p><p>Tudor Bosman担任公司架构负责人，他硕士毕业于斯坦福计算机系，也曾在Facebook工作过多年，是Facebook搜索引擎Unicorn的领导者，还曾在甲骨文、谷歌等公司担任软件工程师。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/7c/7c438971d6249b19d36d6d16fe644d67.png" /></p><p></p><p>多年来，Rockset 产品不断从 Kafka、MongoDB、DynamoDB 和 S3 等产品中提取和索引数据，从而实现无需预定义架构的实时查询。Rockset 使用开源 RocksDB 持久键值存储作为基础，充当 OLTP 数据库、数据湖和流媒体平台的外部二级索引。这可以加速实时分析查询并为主要事务系统提供性能隔离。</p><p>&nbsp;</p><p>Rockset 的数据库平台支持推荐引擎、物流跟踪仪表板等，以及与 OpenAI 特别相关的金融科技和电子商务等领域的聊天机器人。</p><p>&nbsp;</p><p>据Crunchbase 数据显示，在被收购之前， Rockset已成功从 Icon Ventures、Sequoia 和 Greylock 等投资者手中筹集了超过 1.175 亿美元的资金。该公司还拥有 Meta 和 JetBlue 等知名客户，这些客户将 Rockset 用作其航班延误预测聊天机器人的组件。</p><p></p><h2>OpenAI为何决定收购Rockset？</h2><p></p><p>&nbsp;</p><p>此次收购Rockset 是 OpenAI 继Global Illumination之后进行的第二笔公开收购，Global Illumination 是一家总部位于纽约的初创公司，利用人工智能构建创意工具和基础设施。</p><p>&nbsp;</p><p>OpenAI为何会收购Rockset技术？收购完成后，OpenAI 会用 Rockset 的技术构建什么？</p><p>&nbsp;</p><p>OpenAI在文章中表示收购Rockset是为其自家跨产品检索基础设施提供支持。由此可以明确看出，对实时数据的访问和处理技术已经成为当前AI军备竞赛中的重要一环。此外，OpenAI也将通过收购Rockset吸纳一支经验丰富的实时分析专家团队，为OpenAI的能力增强贡献力量。</p><p>&nbsp;</p><p>简而言之，OpenAI 是想将其内部的各个大模型“扎根”在公司的数据上，这也许可以帮助减少其大模型的幻觉或更容易对针对任意数量的业务用例对模型进行微调。</p><p>&nbsp;</p><p>Venkataramani 也在随公告发布的博客文章中给出了Rockset融入OpenAI后的发展规划预览：“像 Rockset 这样的先进检索基础设施将使 AI 应用更加强大和实用，”他写道。“Rockset 将成为 OpenAI 的一部分，并为 OpenAI 产品套件的检索基础设施提供支持。我们将帮助 OpenAI 解决 AI 应用大规模面临的数据库难题。”</p><p>&nbsp;</p><p>对于OpenAI此次的大手笔收购，有分析人士认为，这笔收购其实是从本质上说明了向量数据库无法真正地解决“人工智能内存”问题。</p><p>&nbsp;</p><p>从去年开始，与向量数据库相关的话题一直很火热，几乎每个向量数据库厂商都试图以“LLM 记忆”进行营销。但事实可能并非如此。有声音认为，向量数据库只是 LLM 的便签，可帮助用户查找一些信息。目前市面上还没有真正出现一个可重复的堆栈来将所有数据（结构化或非结构化）传输到企业需要的运营和分析存储中。</p><p>&nbsp;</p><p>人工智能需要的内存形态是一种类似于人类的记忆的东西，人类的记忆不只是记住事情，还会把这些记忆总结并将它们相互联系——在使用之前进行分析。通用实时数据库是最接近这一点的东西。</p><p>&nbsp;</p><p>OpenAI 知道这一点，并希望开发这个适合企业的堆栈。利用数据库的廉价和高效的计算来卸载一些昂贵且缓慢的人工智能模型计算是件令人兴奋的事，而OpenAI似乎正在朝着这个方向努力。</p><p>&nbsp;</p><p>此次收购也在Hacker News引发了广泛讨论。有用户认为：“RAG 更像是一个概念，而不是一个规范。RAG不会阻止在传统数据库中添加向量索引和相似性搜索技术的潮流。这证实了传统数据库（OLAP 或 OLTP）不会消失。在所有 LLM 模型背后，仍然需要数据库中真实、权威的数据，以避免（或至少最小化）幻觉问题。无论如何，人工智能需要更多程序化的方法来获取这些数据。”</p><p>&nbsp;</p><p>曾就职于甲骨文数据库公司、现任国内某开源分布式数据库公司副总裁的Pine表示：</p><p>&nbsp;</p><p></p><blockquote>“此次收购说明OpenAI这样的大模型供应商已经认识到，当大模型要在企业中落地时，要解决好两个问题：第一个是数据的实时分析问题，这就要求数据库有很高的实时性，第二个是要解决多模态向量检索问题。&nbsp;也就是说，大模型要服务企业级应用时需要一个有云原生扩展能力、能提供实时性服务和向量搜索能力的混合型实时分析数据库。而这种情况下，纯粹的向量数据库在面对海量的、时效性要求高的、非结构化数据时优势就没有那么明显了。</blockquote><p></p><p></p><h2>收购大局已定，Rockset用户需要做何准备？</h2><p></p><p>对于当前使用Rockset产品的用户来说，时间已经相当紧迫。根据该公司发布的FAQ内容来看，所有未签订合同的按月付费用户必须在2024年9月30日之前退出。虽然签约客户将有权与自己的Rockset客服团队具体协调合适的退出计划，但全体客户必须尽快为Rockset物色替代方案已经成为不争的事实。面对板上钉钉的收购，各位Rockset用户必须提前想好下一步规划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4f5d7df67b22564af7e8d7ff3402d56.png" /></p><p></p><p>Rockset用户可以采取以下措施进行应对：</p><p>评估自己的当前使用情况及要求：最好先做到心中有数，确保在评估替代方案前了解自己需要什么，这能为我们节省大量时间。搜集功能相当或者更好的替代平台：您的业务需求可能很简单、可能极复杂，具体取决于您此前使用Rockset的方式。每种平台都有其优势和短板，请整理出平台在稳定支持您业务时至少应当具备的功能和特性，避免浪费宝贵时间评估那些根本无法满足您性能及功能需要的解决方案。着手规划迁移流程，以避免对正常运营造成干扰：无论您选择了开源方案还是商业产品，对其背后支持能力或社区建设情况的评估都至关重要。请寻找一家能手把手指导您完成概念验证的合作伙伴，或者确定您打算选择的开源产品拥有全天候活跃、足以帮助您完成故障排查的技术社区，这一切将成为顺利迁移乃至未来长久应用的必要前提。</p><p>&nbsp;</p><p>Rockset用户有哪些方案可选？</p><p></p><p>在制定下一步计划时，Rockset用户应当探索每一种替代方案的合理性，根据企业自身的特定用例与性能需求，不同平台提供的功能配伍也各有适用范围。下面几个重要选项可以作为参考：</p><p>面向实时分析SQL工作负载的开源选项：</p><p>&nbsp;</p><p><a href="https://druid.apache.org/">Apache Druid</a>": Druid是一款高性能实时分析数据库，可在大规模、高强度负载下对流式及批量数据执行亚秒级查询。<a href="https://clickhouse.com/">ClickHouse</a>": ClickHouse是一款速度出色的开源列式数据库管理系统，允许使用SQL查询实时生成数据分析报告。<a href="https://www.starrocks.io/">StarRocks</a>": 非常适合运行可扩展的JOIN查询，并可在无需非规范化管线的情况下实现实时分析。凭借开箱即用的实时数据更新支持，StarRocks能够直接在其列式存储上为可变数据提供秒级更新支持。<a href="https://doris.apache.org/">Apache Doris</a>"：Apache Doris 是一款高性能的开源实时数据仓库，支持大规模实时数据上的极速查询分析。相较于 Rockset，Apache Doris 同样支持实时数据更新、行列混存、半结构化 JSON 数据分析以及倒排索引和全文检索的能力，能满足高并发数据服务、实时报表分析、即席查询、湖仓一体以及日志存储分析等多个场景的需求。&nbsp;</p><p></p><p>面向实时分析SQL工作负载的专有（商业）托管解决方案：</p><p></p><p><a href="https://imply.io/">Imply</a>": 具有企业级服务支持的云端托管版Apache Druid。<a href="https://celerdata.com/">CelerData</a>": 云托管版StarRocks，由StarRocks项目的发起者和维护者提供支持。<a href="https://www.selectdb.com/">SelectDB</a>"：SelectDB 是基于 Apache Doris 构建的现代化数据仓库，提供了全托管的云原生实时数仓服务 SelectDB Cloud 和私有化部署模式的 SelectDB Enterprise 两种产品形态。</p><p></p><p>开源向量搜索 (VectorDB):</p><p><a href="https://weaviate.io/">Weaviate</a>": Weaviate是一款开源向量数据库，可存储对象及向量，允许将向量搜索与结构化过滤相结合，具备云原生数据库的容错性及可扩展性。<a href="https://milvus.io/">Milvus</a>": 面向下一代AI应用的云原生向量数据库及存储方案。<a href="https://qdrant.tech/">Qdrant</a>": 面向下一代AI的高性能、大规模向量数据库。</p><p>托管向量搜索 (VectorDB):</p><p><a href="https://www.singlestore.com/">SingleStore</a>": 除SQL功能之外，SingleStore还提供托管向量搜索功能，这也使其成为适合两类工作负载的综合性解决方案。<a href="https://zilliz.com/">Zilliz</a>": 作为Milvus的同门师兄弟，Zilliz提供向量搜索托管服务，在继承Milvus优势的同时提供额外的支持和维护保障。<a href="https://www.pinecone.io/">Pinecone</a>": 一套完全托管的向量搜索平台，可简化向量搜索应用程序的部署和扩展，确保高可用性及性能水平。</p><p>&nbsp;</p><p>迁移工作已经迫在眉睫，各位用户需要确保自己的关键基础设施始终保持完整及稳定运行。不同平台各有优势，需要实际开展评估以确保成功迁移。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://web.swipeinsight.app/posts/openai-acquires-rockset-to-enhance-real-time-analytics-and-retrieval-capabilities-7788">https://web.swipeinsight.app/posts/openai-acquires-rockset-to-enhance-real-time-analytics-and-retrieval-capabilities-7788</a>"</p><p><a href="https://starrocks.medium.com/rockset-is-acquired-by-openai-what-does-it-mean-for-its-users-3fa9561979d2">https://starrocks.medium.com/rockset-is-acquired-by-openai-what-does-it-mean-for-its-users-3fa9561979d2</a>"</p><p><a href="https://techcrunch.com/2024/06/21/openai-buys-rockset-to-bolster-its-enterprise-ai/">https://techcrunch.com/2024/06/21/openai-buys-rockset-to-bolster-its-enterprise-ai/</a>"</p><p><a href="https://www.singlestore.com/blog/openai-acquires-rockset/">https://www.singlestore.com/blog/openai-acquires-rockset/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/L5ZbIZbc7qrrjTlZLENo</id>
            <title>1个芯片顶英伟达3个？这个偏爱印度的创始人爆肝8年，终于等来抢英伟达泼天富贵的一天！</title>
            <link>https://www.infoq.cn/article/L5ZbIZbc7qrrjTlZLENo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/L5ZbIZbc7qrrjTlZLENo</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 08:32:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI领域, Cerebras Systems, 英伟达, 高性能计算机芯片
<br>
<br>
总结: Cerebras Systems是一家专注于AI和高性能计算领域的初创公司，准备在纳斯达克证交所进行首次公开募股。该公司推出的WSE-3芯片被认为是英伟达GPU的替代品，具有强大的性能和计算能力，引起了市场的关注。与英伟达等公司竞争，展示了Cerebras Systems在AI领域的雄心和实力。 </div>
                        <hr>
                    
                    <p>据报道，在AI领域与英伟达正面竞争的高性能计算机芯片初创公司Cerebras Systems已经向美国证券监管机构提交了保密文件，准备在纳斯达克证交所开启自己的首轮公开募股（IPO）。</p><p>&nbsp;</p><p>消息最先由The Information网站传出，其中援引一位参与决策的匿名人士的发言，称IPO预计将在今年晚些时候进行。</p><p>&nbsp;</p><p>Cerebras Systems是一家专业且颇具能力的计算机芯片生产商，成立于2016年，主要面向AI及高性能计算（HPC）类工作负载。过去一年以来，该公司曾多次登上头条新闻，声称其芯片不仅比英伟达的图形处理单元更强大，而且成本效益也更加出色。今年4月，Cerebras Systems 以285 亿人民币的企业估值入选《2024·胡润全球独角兽榜》。</p><p></p><h2>凭什么跟英伟达掰手腕？</h2><p></p><p>&nbsp;</p><p>英伟达已经成长为当今世界市值最高的公司，甚至一度没有“之一”，而其背后的驱动力主要是生成式AI热潮，而这股浪潮丝毫没有放缓的迹象。随着世界各地企业争相将强大的AI工具整合进自己的系统和应用程序当中，他们开始疯狂采购GPU，并在过去一年间将英伟达的数据中心业务收入推高超400%。</p><p>&nbsp;</p><p>尽管有能力站在英伟达对面与其竞争的对手不多，但 Cerebras 正是其中之一。他们的旗舰产品、全新WSE-3处理器发布于今年3月，底子则是2021年首次亮相的前代WSE-2芯片组。</p><p>&nbsp;</p><p>Cerebras 的 WSE-3芯片被认为是英伟达强大GPU产品的替代。</p><p>&nbsp;</p><p>WSE-3 采用5纳米制程工艺，在晶体管数量上达到了惊人的4万亿，比其前代芯片多出1.4万亿个晶体管，拥有超过90万个计算核心和44 GB的片载静态随机存取存储器。外部用户可以灵活选择1.5TB、12TB、甚至高达1200TB的内存容量。</p><p>&nbsp;</p><p>根据这家初创公司的介绍，WSE-3的核心数量达到单张英伟达H100 GPU的52倍。这款芯片将作为数据中心设备CS-3的核心器件，而CS-3的尺寸与小型冰箱差不多。WSE-3芯片则跟批萨饼大小相当，还配有集成的冷却与电源传输模块。</p><p>&nbsp;</p><p>尽管在核心数量和缓存容量的增幅上并不突出，但WSE-3的性能表现却实现了质的飞跃。Cerebras WSE-3 据称峰值浮点运算速率可达125 PFLOPS（PetaFLOPS，千万亿次每秒），即一天内就能够完成Llama 700亿参数的训练任务。Cerebras表示，这样的规格足以让WSE-3与英伟达旗下最顶尖的GPU相匹敌。该公司解释称，其芯片性能卓越，能够以更快的速度、更低的功耗高效处理AI工作负载。</p><p>&nbsp;</p><p>该款芯片预计将于今年晚些时候上市。</p><p></p><h4>大模型训练：CS-3 VS B200</h4><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a5/a5d04f009a01bde31dd91cbd812ce838.png" /></p><p></p><p>Cerebras CS-3 和 B200&nbsp;对比</p><p>&nbsp;</p><p>&nbsp;</p><p>训练大型AI模型时，性能的首要决定因素是浮点性能。凭借90万个专用AI核心，Cerebras CS-3采用行业标准FP16精度，实现了125 PFLOPS 。而单个Nvidia B200 GPU是 4.4 PFLOPS，8个GPU的 DGX B200 是 36 PFLOPS。</p><p>&nbsp;</p><p>”在原始性能方面，单个CS-3相当于3.5个DGX B200服务器，但是占用的空间更小，功耗只有原来的一半，编程模型也非常简单。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/45fca1e1e59e55be221b6c08ffa90007.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>人工智能开发经常遇到内存限制的问题，OOM（内存不足）经常导致训练失败。万亿参数规模的模型只会加剧这个问题——需要TB级内存、数百个GPU和复杂的模型代码来管理内存和编排训练。</p><p>&nbsp;</p><p>为此，Cerebras 硬件没有采用GPU最强“辅助”HBM（High Bandwidth Memory）方式，而是采用了独特的分解内存架构，并设计了名为MemoryX的专用外部存储设备来存储权重。MemoryX 使用闪存和DRAM以及自定义软件堆栈，以最小的延迟管道加载/存储请求。</p><p>&nbsp;</p><p>“我们1200TB 超大规模 SKU 专为 GPT-5 及更高版本而设计，可训练 24 万亿参数的大模型。它的内存容量比 B200 GPU 多 6,000 倍，比 DGX B200 多 700 倍，比全机架 NVL72 多 80 倍。”该公司提到。</p><p>&nbsp;</p><p>另外，CS-3 的分解式内存架构可以将数 PB 的内存连接到单个加速器，使其在处理大型模型时具有极高的硬件效率。</p><p><img src="https://static001.geekbang.org/infoq/83/83aaaaeb78adc618be521b47ea8dcb1f.png" /></p><p></p><p>&nbsp;</p><p>高互连性能对于多芯片的高利用率至关重要。DGX B200 等 GPU 服务器是通过 NVLink 实现。NVLink 是一种专有互连，可在服务器内部的 8 个 GPU 之间提供专用链接。CS-3 互连系统则采用完全不同的技术构建：在晶圆上布线将数十万个内核连接在一起，以最低的功耗提供最高性能。</p><p>&nbsp;</p><p>“CS-3 为90万个核心提供每秒 27 PB 的总带宽，这比 1800 台 DGX B200 服务器的带宽还要高。”该公司表示。</p><p>&nbsp;</p><p>另外在上个月，Cerebras 还与桑迪亚国家实验室、劳伦斯利弗莫尔国家实验室以及洛斯阿拉莫斯国家实验室的研究人员合作，在毫秒级速度下展示了上代WSE-2硬件进行原子级材料模拟时的性能表现。在相关研究论文中，该公司提到WSE-2的性能水平惊人，模拟速度可达到配备3.9万张英伟达GPU的便于最强超级计算机Frontier的179倍。</p><p>&nbsp;</p><p>该公司产品与战略高级副总裁 And Hock 在上个月接受采访时指出，“简单堆叠任何数量的GPU都不可能获得这样的结果。我们正在根本上为分子动力学研究解锁新的时间尺度。”</p><p>&nbsp;</p><p></p><h2>创始人：公司被AMD收购后再创业</h2><p></p><p>&nbsp;</p><p>Cerebras 是一支由先驱计算机架构师、计算机科学家、深度学习研究人员以及热爱无畏工程的各类工程师组成的团队，目前已在加拿大和日本分别设立了办事处。</p><p>&nbsp;</p><p>提到这家公司的创始团队，不得不提2012年被 AMD 以 3.34 亿美元收购的微型服务器公司 SeaMicro。</p><p>&nbsp;</p><p>这次收购在当年也引发了很大关注，被评“对低功耗服务器领域来说具有颠覆性意义”，因为 SeaMicro 一直在其下一代服务器中使用英特尔芯片，SeaMicro 的网络结构允许数百个低功耗处理器协同工作。SeaMicro 架构与处理器无关，这意味着它可以快速适应 AMD 的技术。</p><p>&nbsp;</p><p>而 SeaMicro 创始人Andrew Feldman也是如今Cerebras 的联合创始人兼CEO。</p><p>&nbsp;</p><p>Andrew拥有斯坦福大学的学士学位和工商管理硕士学位。在2007年创立SeaMicro之前，Andrew是Force10 Networks的产品管理、营销和业务拓展副总裁，该公司后来以8亿美元的价格出售给戴尔。在加入Force10 Networks之前，Andrew 曾担任 RiverStone Networks 的营销和企业发展副总裁(从公司成立到2001年IPO)。</p><p>&nbsp;</p><p>值得注意的是，Andrew 认为印度是Cerebras的优先事项，理由是该国拥有巨大的工程人才、顶尖大学和不断发展的人工智能生态系统。</p><p>&nbsp;</p><p>该公司的CTO Gary Lauterbach 也是SeaMicro的联合创始人，后来也同样加入了AMD。 Gary 是计算机架构大牛，曾担任Sun SPARC Ⅲ和UltraSPARC Ⅳ微处理器的首席架构师。在Sun 实验室，他是DARPA HPCS Petascale计算项目的首席架构师，他本人拥有50多项专利。SeaMicro 微服务器领域的领先技术也离不开Gary。在SeaMicro工作期间，Gary还是美国能源部930万美元节能计算拨款的首席研究员。</p><p>&nbsp;</p><p>Andrew 和Gary 两人共事已超过12年。</p><p>&nbsp;</p><p>另一位技术负责人Sean Lie 也曾在 SeaMicro 公司担任 IO 虚拟化结构 ASIC 的首席硬件架构师。</p><p>&nbsp;</p><p>Sean 拥有麻省理工学院电子工程和计算机科学学士学位和硕士学位，并在计算机体系结构方面拥有16项专利。在SeaMicro被AMD收购后，Sean成为AMD研究员和首席数据中心架构师。早期职业生涯中，他在AMD的高级架构团队工作了五年。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/51/51f0311e10403658b09a279f136ed628.png" /></p><p></p><p>Cerebras 还聘请了有超过 24 年执行领导经验的Vinay Srinivas担任软件工程高级副总裁。</p><p>&nbsp;</p><p>Vinay 拥有印度理工学院孟买分校的学士学位以及佛罗里达大学的硕士学位和博士学位。他曾在 Synopsys（一家美国电子设计自动化公司） 工作了 12 年，离职前担任仿真产品线的工程副总裁。早前，Vinay 还曾分别在 Archpro Design Automation 、Sequence Design担任研发副总裁。</p><p>&nbsp;</p><p>首席运营官 Dhiraj Mallick 之前也曾担任SeaMicro的工程副总裁，公司被收购后他继续在AMD担任公司副总裁和服务器解决方案部门总经理。他拥有超过20年的领导经验，在加入Cerebras前是英特尔价值200亿美元的数据中心业务的首席技术官和架构副总裁。同时，Dhiraj 还担任了几家风险投资公司顾问，并拥有斯坦福大学的电气工程硕士学位。</p><p>&nbsp;</p><p>Cerebras Systems 的产品管理副总裁Andy Hock 此前是高分辨率卫星制造商Skybox Imaging的高级技术总监，该公司后来被谷歌以5亿美元收购。收购后，他继续在谷歌担任产品经理。Andy 拥有加州大学洛杉矶分校地球物理和空间物理学博士学位，在加入Skybox之前是Arete Associates的高级项目经理、业务开发主管和高级科学家。</p><p>&nbsp;</p><p></p><h2>被资本看好</h2><p></p><p>&nbsp;</p><p>考虑到英伟达这位竞争对手在过去一年间取得的令人瞩目的收益，Cerebras作为少数能够与之竞争的芯片制造商之一，自然有理由受到投资者们的热烈追捧。</p><p>&nbsp;</p><p>Constellation Rsearch公司的Holger Mueller表示，如果Cerebras真像其宣称的那样具有竞争力，完全有可能在华尔街金融市场上引发轰动。</p><p>&nbsp;</p><p>Mueller解释道，“英伟达前阵子刚刚成为全球市值最高的上市公司。面对这泼天的富贵，竞争态势也开始快速加剧，包括不少来自传统芯片行业以外的竞争对手。Cerebras确实有可能成为英伟达的潜在竞争对手，他们在芯片的制造和销售方面采取了差异化的发展路线，而且似乎有望吸引到足量资金以投入到这场耗资甚巨的AI军备竞赛当中。”</p><p>&nbsp;</p><p>截至目前，该公司已累计融资7.2亿美元，估值约为42亿-50亿美元。</p><p>&nbsp;</p><p>在其官网的投资者一栏中，还可以看到OpenAI的身影，比如Sam Altman、Greg Brockman、Ilya Sutskever等，其中 Altman曾参与Cerebras的8000万美元D轮融资，Cerebras在官网将其列在投资人的第一位。</p><p><img src="https://static001.geekbang.org/infoq/d8/d8117993919e6257df26d3d5ae309c73.png" /></p><p></p><p>在The Information的报道中，消息人士透露称为了进一步吸引投资者，Cerebras已经通知公司注册地特拉华州的监管机构，他们计划为即将到来的F1轮融资提供优先股。与上一轮融资相比，其股票发行价将有“大幅折扣”，希望借此增强上市发行的吸引力。</p><p>&nbsp;</p><p>尽管Cerebras本身对其IPO计划讳莫如深，但彭博社此前报道称，该公司已经选择花旗集团作为其上市领投银行。在与多家IPO咨询机构进行多次讨论后，Cerebras最终选择了这家银行。报道还提到，该公司的目标是最早在2024年下半年上市，且预期市值至少应高于其2021年最新一轮2.5亿美元F轮融资时对应的40亿美元估值。</p><p>&nbsp;</p><p>消息人士还在The Information报道中指出，Cerebras IPO的具体细节尚未确定，可能会根据投资者们的实际反应做出调整。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://siliconangle.com/2024/06/20/ai-chipmaker-cerebras-systems-competitor-nvidia-reportedly-files-ipo/">https://siliconangle.com/2024/06/20/ai-chipmaker-cerebras-systems-competitor-nvidia-reportedly-files-ipo/</a>"</p><p><a href="https://www.cerebras.net/blog/cerebras-cs-3-vs-nvidia-b200-2024-ai-accelerators-compared">https://www.cerebras.net/blog/cerebras-cs-3-vs-nvidia-b200-2024-ai-accelerators-compared</a>"</p><p><a href="https://www.theinformation.com/articles/cerebras-an-nvidia-challenger-files-for-ipo-confidentially?offer=rtsu-engagement-24&amp;utm_campaign=RTSU+-+Cerebras+IPO&amp;utm_content=4480&amp;utm_medium=email&amp;utm_source=cio&amp;utm_term=3006">https://www.theinformation.com/articles/cerebras-an-nvidia-challenger-files-for-ipo-confidentially?offer=rtsu-engagement-24&amp;utm_campaign=RTSU+-+Cerebras+IPO&amp;utm_content=4480&amp;utm_medium=email&amp;utm_source=cio&amp;utm_term=3006</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I2PS1f3dC9BS5Sy2QE19</id>
            <title>字节跳动代码生成 Copilot 产品的应用和演进 | AICon</title>
            <link>https://www.infoq.cn/article/I2PS1f3dC9BS5Sy2QE19</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I2PS1f3dC9BS5Sy2QE19</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 06:54:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大语言模型, 代码生成, GitHub Copilot, 交互方式
<br>
<br>
总结: 本文介绍了大语言模型在代码生成领域的应用和发展，重点讨论了GitHub Copilot这一产品形式的成功因素，包括团队构建、GPT-3的出现、产品形态选择、交互方式设计等。另外还介绍了字节跳动在内部探索代码生成的历程，包括模型优化、工程链路优化、交互体验改进等方面的探索和挑战。 </div>
                        <hr>
                    
                    <p>大语言模型在代码生成领域取得了令人瞩目的进展。本文整理自字节跳动产品研发和工程架构部的代码智能助手架构师刘夏在 AICon 2024 北京的演讲<a href="https://aicon.infoq.cn/2024/beijing/presentation/5901">《代码生成 Copilot 产品的应用和演进》</a>"，聚焦基于大语言模型的代码生成技术，深入探讨了代码补全和代码编辑这两种典型的应用形态。同时，还分析了当前代码补全面临的挑战和局限性，阐述了代码编辑是如何在交互和构建方法上实现创新。内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们设置了【大模型与企业工具集成的提效实践】专题，本专题将分享大模型与企业工具的集成实践和从业者的心路历程，并探讨 AI 在哪些场景更能为企业带来助力。目前大会已进入 8 折购票最后优惠期，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>代码生成 Copilot 产品回顾</h2><p></p><p></p><h3>GitHub Copilot 的成功因素</h3><p></p><p></p><p>首先，回顾一下代码生成 Copilot 这种产品形式。当我们谈论代码生成 Copilot 或者 Copilot 这个词时，不得不提到 GitHub 在 2021 年 6 月推出的 GitHub Copilot。这个产品不仅拥有一个响亮的名字，而且定义了一种新的 AI 产品的范式。GitHub Copilot 在 2021 年 6 月推出了技术预览版，随着不断的迭代，其效果令人印象深刻，使人们意识到将大语言模型应用于代码生成领域具有巨大的潜力。业界也开始迅速构建类似的产品，无论是在模型还是产品上都取得了快速的迭代。</p><p></p><p>这里有一个关键问题：为什么是 GitHub Copilot 引爆了这个热点？实际上，将自然语言处理（NLP）技术应用于代码生成并不是一个新概念，例如 TabNine 这样的产品在 GPT-2 时代就已经将其应用于代码补全。那么，GitHub Copilot 究竟有何特别之处呢？我们想要从几个方面和维度来探讨这个问题。</p><p></p><p>首先，我想提到团队，GitHub Next 是这个产品的孵化团队。GitHub Next 是一个具有研究属性的团队，他们的任务是探索未来软件开发的新方式。如果访问他们的官网，你会发现许多有趣的项目，其中就包括 Copilot。团队主要由程序分析师、软件工程师以及研究员组成，他们持续关注的一个重要话题是如何实现通用的代码生成。</p><p></p><p>接下来，我想谈谈一个重要的契机，那就是 2020 年 6 月 GPT-3 的问世。由于 GitHub 现在是微软的子公司，而微软与 OpenAI 有着深入的合作，GitHub 团队很早就获得了 GPT-3 的预览版，并对其能力感到非常兴奋。他们认为必须利用 GPT-3 在代码生成领域做出一些创新，因此与 OpenAI 紧密合作，基于 GPT-3 迭代开发出了专门用于代码的大型语言模型 Codex。随后，他们对 Codex 进行了持续的微调训练，打造了专属的模型。一个强大且优秀的基础模型实际上决定了产品的上限，因此 GPT-3 的出现对这款产品的贡献是巨大的。</p><p></p><p>有了模型之后，团队开始思考应该开发什么样的产品形态。根据 GitHub 的分享，他们最初的想法是开发一款 Chatbot，即一款能够解答编码过程中遇到的任何问题并提供代码的对话聊天产品。但他们很快发现，尽管知识库中大部分问题都能得到回答，但只有大约 20% 的回答是正确且被接受的。尤其是在 GPT-3 时期，ChatGPT 还要两年后才出现，他们意识到这种 Chatbot 产品的效果并不理想。如果大部分时候给出的答案都不是用户想要的，用户对产品的信任度会很低。于是他们决定先采用代码补全这种容错率更高的产品形态，一方面代码补全是个开发者使用频率非常高的功能，也有很强的依赖性，更重要的是开发者对于这个功能的预期是给出建议而不是 100% 准确的答案。</p><p></p><p>选择好产品形态后的一个要素是交互方式。GitHub Copilot 放弃了传统 IDE 中从下拉列表选择补全建议的交互，而是选择了用 Ghost Text 进行展示，用 Tab 键进行采纳，继续输入则取消推荐。这种交互方式发挥了模型在多行补全上的优势，推荐代码和已有代码融为一体，方便开发者快速基于上下文判断是否采纳。</p><p></p><p>代码补全产品的一个技术挑战是实现低延迟，Jetbrains 在开发传统的补全功能时甚至要求在 150ms 内出现推荐列表以达到最佳的开发者体验。因为专业开发者的输入速度通常较快，过高的延迟会失去很多推荐的机会或者迫使用户停顿等待。GitHub Copilot 在大语言模型的推理速度和工程链路上进行了优化，让一个基于云端推理的 LLM 应用做到 500ms 左右的平均延迟。</p><p></p><p>如果说基座模型决定了产品能力的上限，那么提示工程所做的努力就是去逼近这个上限。通过研究开发者日常开发中会关注的上下文，在 prompt 中加入文件路径、相似代码、浏览记录等信息，让模型在代码补全方面的表现大幅提升，如今这些提示工程上的实践也被大家广泛应用。</p><p></p><h2>字节跳动内部代码生成的探索历程</h2><p></p><p></p><p>字节跳动在内部探索代码生成的过程中，面临多种优化选择：可以在模型层面进行优化，也可以选择在工程链路上优化，或在交互体验上进行改进。团队需要灵活地做出决策。</p><p></p><p>随着大语言模型的发展，特别是从 2023 年开始，这个领域开始受到广泛关注，新的模型和产品层出不穷。为了迭代和优化模型，字节跳动首先建立了自己的评测方法和自动化评测系统。这涉及到模型选型的决策，快速评估训练过程中的 checkpoint 效果，以及产品上线后如何收集线上反馈，包括用户编辑过程中的正反馈和负反馈。字节跳动还建立了一个完整的数据链路，以决定哪些数据被采纳，哪些被丢弃，并实施 A/B 测试系统来验证不同的 prompt 策略、参数配置，甚至是新模型的上线效果。字节跳动的自研大语言模型也已经发布，团队逐渐切换到这个自研模型上。基于此，字节跳动引入了对话方式，使代理模型能够理解整个工程结构，并根据实际情况生成代码。此外，还引入了多点代码编辑推荐功能，这是一个较新的功能。今天的分享将围绕三个重点进行详细分析：</p><p></p><p>构建自研评测体系的重要性；如何科学定义产品指标；A/B 测试的重要性。</p><p></p><h3>构建自研评测体系的重要性</h3><p></p><p></p><p>构建自研评测体系的重要性在于，它可以帮助我们避免使用不恰当的评测指标，如 HumanEval，它可能无法准确反映模型在实际应用中的表现。HumanEval 通过完成人工编写的算法题并运行单元测试来评估模型，虽然模型在测试的分数可能很高，但这并不意味着模型在代码补全产品中的表现就一定好。例如，GitHub Copilot 在 HumanEval 上的得分可能不高，但其用户体验仍然出色。</p><p></p><p>自建评测集可以避免数据泄露问题，确保题目和答案不会被模型提前接触到。同时，自建评测集可以引入真实项目中的跨文件上下文，这对于评估模型能否合理利用上下文信息至关重要。此外，自建评测集还可以引入大量公司内部代码，因为开源代码与内部代码的使用场景和分布可能存在显著差异。评测体系还需要包括基于单元测试的验证方式，因为同一功能可能有多种不同的代码实现方式，而单元测试可以更准确地验证生成代码的正确性。</p><p></p><p>最后，安全的自动化评测系统对于模型迭代至关重要。它不仅可以通过执行结果来验证代码的正确性，还可以防止模型生成有害代码，如删除根目录或造成大量内存分配等问题。高效的沙箱测试环境和高并发支持对于大规模的评测也是必不可少的。通过这样的评测系统，我们可以在训练过程中对不同 checkpoint 的模型效果进行评估，从而为模型选型和迭代提供有力支持。</p><p></p><h3>如何科学地定义指标</h3><p></p><p></p><p>在科学地定义指标时，我们需要考虑代码补全流程中的各个环节，并确保所选指标能够准确反映产品优化的需要。一个有效的指标应该能够指导整个链路的优化，帮助我们识别瓶颈并进行相应的调整。采纳率是一个常被提到的指标，它通常定义为采纳次数除以推荐次数。虽然这个定义简单，但它并不是一个好的指标。首先，采纳率容易被操纵。例如，如果减少推荐次数，只在非常确定的时候去帮你补一个分号，采纳率就会提高，但这并不意味着产品的实际效果有所提升。其次，采纳率没有很好地拆解推荐和采纳过程中的具体因素，无法明确指出是推荐更快了，还是其他因素导致采纳次数增多。</p><p></p><p>体验指标是另一个需要考虑的方面。当用户在使用代码补全产品时，如果一个 Tab 操作就能接受推荐的代码并完成工作，这自然会带来良好的用户体验。体验指标可以反映用户对产品的满意度，但它并不直接指导产品优化的方向。在定义指标时，我们需要更细致地考虑如何反映产品的实际性能和用户体验，同时避免指标被操纵，并确保指标能够指导我们进行有效的产品迭代和优化。</p><p></p><p>在探讨如何科学地定义指标时，引入了 CPO（Character per opportunity）这一指标，它是由一家专门从事代码补全产品的公司提出的。CPO 的计算公式由五个因子相乘得到：尝试率、反馈率、采纳率、每次采纳平均的 token 数以及 token 的平均字符长度。</p><p></p><p>尝试率指的是用户在编辑器中进行操作时，AI 提供建议的频率。例如，如果用户敲击键盘 10 次，但只有 6 次触发了对模型的请求，尝试率就是 6/10。这个指标反映了 AI 实际为用户提供建议的次数。</p><p></p><p>反馈率考虑了 AI 给出补全建议时存在的延迟问题。如果因为延迟太高，开发者已经进行了其他操作，那么即使推荐返回了也没有意义。如果发起 6 次请求，最终只有 3 次被展示，反馈率就是 3/6。</p><p></p><p>采纳率是大家熟悉的指标，即用户接受推荐的次数与推荐次数的比值。例如，三次推荐中只有一次被采纳，采纳率就是 1/3。</p><p></p><p>引入每次采纳平均的 token 数和 token 的平均字符长度这两个参数，是为了衡量不同长度代码带来的价值。不同的语言模型有不同的分词器，因此需要计算每个 token 平均的字符长度。例如，ChatGPT 的词表较大，平均一个 token 可以生成的字符数可能大于其他模型。</p><p></p><p>CPO 指标的计算公式是这几个因子的乘积，它衡量的是在每次有机会向用户推荐时，推荐了多少字符给用户。这个指标不仅可以衡量产品给开发者带来的价值，还可以拆解到整个链路的各个部分进行优化。例如，可以通过优化模型推理性能，提高反馈率，或者在代码注释中提供推荐来优化尝试率。此外，当线上出现问题时，CPO 指标也可以用来分析可能存在的问题所在。</p><p></p><h3>A/B 测试的重要性</h3><p></p><p></p><p>A/B 测试在产品开发过程中扮演着至关重要的角色。尽管离线评测可以帮助我们进行模型选型，但一个模型是否真正有效，还需要通过线上测试来验证。有时候，一个模型在评测中得分很高，但这并不代表它在线上的实际表现同样出色。例如，一个非常强大的模型如 GPT-4，可能会因为高延迟而影响用户体验。</p><p></p><p>A/B 测试还可以帮助我们确定各种参数配置的合适值。比如，如果一个模型支持 16K 的上下文长度，是否就应该使用完整的 16K 呢？实际上，如果上下文过长，可能会导致整体延迟增加，影响用户体验。因此，需要通过 A/B 测试来找到最合适的上下文长度。</p><p></p><p>此外，A/B 测试还可以验证新的提示工程策略的效果。例如，如果我们在模型中加入了函数签名或其他包结构信息，是否真的能提升效果？模型是否能够有效利用这些上下文？以及为了采集这些上下文信息而引入的额外延迟，是否值得？这些问题都需要通过 A/B 测试来验证。</p><p></p><p>最后，A/B 测试还可以帮助我们发现并改进产品指标。假设我们最初使用的是采纳率作为指标，但在进行 A/B 测试后，我们发现延迟提高后，采纳率反而增加了。这种情况可能表明我们的指标存在问题，需要重新考虑和调整。</p><p></p><h2>代码编辑推荐：代码补全的进化</h2><p></p><p></p><p>代码补全的进化形式可以被视为代码编辑推荐。大语言模型擅长生成下一个 token，这与代码补全或续写任务非常契合。然而，传统的代码补全主要针对编写全新代码的场景，而软件工程师在日常工作中不仅需要编写新代码，还需要编辑现有代码，包括重构和删除代码。在这些场景下，传统的补全功能可能无法高效地满足需求。在编辑现有代码时，简单地删除一行然后重新编写是低效的。理想情况下，我们希望模型能够自动完成新增、删除、替换等操作，从而提高代码编辑的效率。因此，代码编辑推荐作为代码补全的进化，能够更好地适应软件工程师在实际工作中的各种代码操作需求，提供更加全面和智能的代码辅助功能。</p><p></p><h3>代码编辑推荐的概念</h3><p></p><p></p><p>代码编辑推荐的概念涉及到一种更高级的代码辅助功能，它不仅包括传统的代码补全，还涵盖了对代码进行更深层次的理解和编辑。例如，假设你写了一个 log 函数，该函数用于打印一个 message，并且有两个函数作为调用方来使用这个 log 函数。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f4/f41c7b69002cbe966e397682471d38a8.png" /></p><p></p><p>如果你决定给 log 函数添加两个新的参数，比如 sourceMethod 和 level，用以打印出对应的方法名称和日志等级，这时你实际上需要执行两个后续操作：首先，在 print 语句中添加新参数，以便能够打印出这些新信息；其次，在所有的调用方中也添加这些新参数，确保它们能够传递正确的值给 log 函数。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ac/ace864e1e1bdcb274ee014ed4fbf4cbf.png" /></p><p></p><p>在这种情况下，代码编辑推荐的目标是让模型在你添加完新参数后，能够自动帮你完成剩余的内容。理想状态下，当你完成添加参数的操作时，模型已经预测出你需要在 print 语句中加入这些参数，并且在你移动到调用方时，模型已经知道你接下来需要在这些调用点添加新参数。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/315378d8c59296a8868c7c17ac3ed21d.gif" /></p><p></p><p>在 Go 语言中，如果你有一个结构体并且希望它在多线程环境下保持线程安全，通常会引入互斥锁（mutex）来实现。在这种情况下，你需要在结构体的初始化（new）、设置（set）和获取（get）方法中添加锁操作。智能的代码编辑推荐系统应该能够预测到你接下来需要进行的操作。例如，当你在 new 函数中添加锁时，推荐系统可以自动提示你在 set 和 get 方法中也添加相应的加锁代码。当你的光标移动到相应的方法上时，推荐系统就可以给出这些建议。</p><p></p><h3>数据构建和模型训练方法</h3><p></p><p></p><p>数据构建和模型训练是提升代码生成能力的关键环节。模型的能力来源于数据，尤其是 Git 仓库中海量的 commit 数据，这些数据包含了丰富的用户编辑信息。</p><p></p><p>现有的模型训练并没有充分利用这些数据，因为它们往往包含噪音，例如在 commit 信息中夹带无关内容。因此，需要通过启发式规则或模型来过滤掉这些噪音，提取出有相关性和逻辑关系的编辑操作。</p><p></p><p>在编辑过程中，修正 Lint 错误是一个常见任务，这些错误信息及其修复方式也是非常宝贵的数据资源。在训练模型时，通常会选择一个基于大型代码表示模型作为基础，并通过持续训练和 SFT（Supervised Fine-Tuning）等方法让模型理解代码变更的差异。</p><p></p><p>模型在修正代码时可能会出现过度编辑的情况，即模型可能会过于激进地进行不必要的修改。因此，需要采取措施抑制这种行为，确保模型的编辑是恰当和准确的。</p><p></p><h3>进行中的优化</h3><p></p><p></p><p>在进行中的优化方面，我们认识到目前的交互体验和展示方式可能并非最理想的状态。我们认为，集成在集成开发环境（IDE）中并进行一些 UI 上的定制，可能会带来更好的用户体验。</p><p></p><p>此外，我们已经在内部支持了对链接错误（Link Error）和警告（Warning）的修复功能。这是一个重要的进步，因为它能够帮助开发者更快速地解决编译时遇到的问题。</p><p></p><p>我们还在探索光标移动的自动识别和推荐功能。目前，模型通常需要等到开发者的光标移动到特定位置后才能进行预测和推荐。我们希望优化这一点，让模型在开发者完成编码时就能预测下一步可能的编辑位置，并直接提供相应的推荐。这样的优化将进一步提升代码编辑的流畅性和效率。</p><p></p><h2>代码生成 Copilot 的未来展望</h2><p></p><p></p><p>对于代码生成模型来说，一个明显的趋势是能够处理更长的上下文。理想情况下，模型能够理解整个代码仓库的内容。目前，K 级别和 M 级别的上下文可能还不够，模型需要能够无限地处理上下文信息。谷歌等公司已经提出了相关计划。但随着上下文的增长，保持推理速度不降低也是一个挑战，需要维持在几百毫秒的水平。一些公司如 Magic.dev 和 Supermaven 正在探索使用非 Transformer 架构来实现这一点。</p><p></p><p>对于产品形式，完全自主的 Agent 可能不太适合复杂的任务开发。程序员有时可能想用自然语言或注释来描述编码意图，但由于自然语言的局限性和文档编写的困难，最好的做法可能是 AI 与开发者通过交互的方式反复构思确认，并迭代完成复杂功能的开发。</p><p></p><p>AI 应该更智能地识别人类的意图，例如通过编辑位置的预测来主动参与编码过程，提前帮助预判并提供推荐。虽然这个概念比较抽象，但最近出现了一些体现这一思路的例子。Replit 公司开发的代码修复 Agent 展示了 AI 作为一个虚拟协作者参与交互过程的能力。在多人协同的 IDE 中，AI 能够发现错误并以协作者的身份帮助修正，这是一种有效的主动式 AI 交互方式。</p><p></p><p>明尼苏达大学的研究 “Sketch Then Generate” 展示了一种人与 AI 交互持续迭代的方法。通过编写有结构化的注释来指导模型，这些注释可以与代码的实体、符号、方法关联起来，先构建代码架构，然后逐步指导模型生成更多细节和代码。</p><p></p><p>代码生成 Copilot 的未来将更加注重上下文理解、交互式产品开发、智能意图识别和人机协同工作，以实现更高效和智能的代码生成和编辑体验。</p><p></p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，6 月 30 日前可以享受 8 折优惠，单张门票节省 960 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e13ff2745ce7d222e772163324f836c4.webp" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VFcE253wPsoFby8M5wAv</id>
            <title>王炸！纯血鸿蒙重大升级；宁德时代要求员工896，外籍员工除外？苹果 Vision Pro 2 研发暂停 | Q资讯</title>
            <link>https://www.infoq.cn/article/VFcE253wPsoFby8M5wAv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VFcE253wPsoFby8M5wAv</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 06:11:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华为, 仓颉编程语言, 鸿蒙, 盘古大模型
<br>
<br>
总结: 华为在开发者大会上发布了自研的仓颉编程语言，为鸿蒙生态注入新活力。同时推出了盘古大模型5.0，提供不同参数规格的模型适配不同业务场景。 </div>
                        <hr>
                    
                    <p></p><blockquote>华为自研编程语言“仓颉”来了；Anthropic 推出 Claude 3.5 Sonnet 以及最强视觉模型；前 OpenAI 联合创始人 Ilya Sutskever 成立新公司；字节跳动悄然推出 Instagram 社交应用 Whee；华为与腾讯接近达成协议，不向微信“抽成”；英伟达成为全球市值第一公司！英伟达挖走三星超 500 名 AI 人才；Runway 视频生成新模型；快手副总裁、推荐算法负责人宋洋离职；ChatGPT 时隔两周再次出现重大故障；微软邮箱漏洞允许任何人冒充该公司员工；苹果暂停 Vision Pro 二代研发；DeepSeek Coder V2 开源发布；Meta 推出 AI 音频水印工具……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>华为自研编程语言“仓颉”来了；纯血鸿蒙重大升级！</h4><p></p><p>6&nbsp;月&nbsp;21&nbsp;日，华为开发者大会&nbsp;2024&nbsp;举办。据报道，在此次大会上，华为将发布自研仓颉编程语言，这也是仓颉首次正式对外亮相。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2bae287d30e55926af19f5a507d23b2f.png" /></p><p></p><p>其实，华为自研编程语言仓颉的消息最早可以追溯到&nbsp;2019&nbsp;年。当时，华为被曝出正在自研编程语言仓颉，并在当年&nbsp;8&nbsp;月申请注册了“仓颉语言”商标。在&nbsp;2021&nbsp;年的华为开发者大会&nbsp;2021&nbsp;上，HarmonyOS&nbsp;3&nbsp;开发者预览版正式发布，华为消费者业务软件部总裁龚体宣布，华为将发布为&nbsp;HarmonyOS&nbsp;全新研发的编程语言，为鸿蒙生态基础设施建设补上最后一环。</p><p></p><p>如今，经过多年的研发，华为自研仓颉编程语言终于要在今年的华为开发者大会上正式迎来首次亮相。这不仅是华为在技术创新方面的又一重要成果，也将为鸿蒙生态的发展注入新的活力。</p><p></p><p>在华为开发者大会2024上，华为常务董事、终端BG董事长、智能汽车解决方案BU董事长余承东宣布，原生鸿蒙HarmonyOS&nbsp;NEXT面向开发者和先锋用户启动Beta，以原生智能、全场景、原生安全打造全场景智能操作系统。这意味着着真正独立于安卓、iOS的操作系统正式出现。</p><p></p><p>余承东表示：“鸿蒙是基于Open&nbsp;Harmony打造的全场景智能操作系统，这是一个源自中国、自主可控的操作系统。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd0b06bb04373a3a58ff66547d31d77b.png" /></p><p></p><p>华为常务董事、华为云CEO张平安重磅发布盘古大模型5.0，在全系列、多模态、强思维三个方面带来全新升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ecda6879af7b280ca1e945d5784369a.png" /></p><p></p><p>盘古大模型5.0包含不同参数规格的模型，以适配不同的业务场景。十亿级参数的Pangu&nbsp;E系列可支撑手机、PC等端侧的智能应用；百亿级参数的Pangu&nbsp;P系列，适用于低时延、高效率的推理场景；千亿级参数的Pangu&nbsp;U系列适用于处理复杂任务；万亿级参数的Pangu&nbsp;S系列超级大模型能够帮助企业处理更为复杂的跨领域多任务。</p><p></p><h4>宁德时代“896”奋斗100天？外籍员工不强制？内部人士回应</h4><p></p><p>近日，宁德时代的一则内部文件在网上引起了轩然大波。这家公司，为了完成所谓的“组织赋予的任务”，竟然号召员工实行长达100天的“奋斗100天”计划，而具体的工作时间更是让人咋舌——早上8点上班，晚上9点下班，每周工作6天，也就是俗称的“896”工作制。更让人气愤的是，这一加班政策似乎只针对中国籍员工，外籍员工则可以选择是否参与。</p><p></p><p>据悉，这不是宁德时代第一次搞这种“奋斗100天”的活动了。早在2022年，就有媒体报道称，这种加班文化在宁德时代已经成为常态。这不禁让人质疑，宁德时代所谓的“奋斗”精神，是不是建立在牺牲员工休息时间和健康的基础上？</p><p></p><p>更令人难以接受的是，这种高强度的工作并没有得到相应的回报。有网友爆料称，普通蓝领的加班时长会被算入当月工时，以工资的形式结算；而工程师们则更加悲惨，加班没有加班费，只有绩效考核。这意味着，无论你工作多努力，如果没有达到公司的要求，一切都是白搭。</p><p></p><h4>Anthropic&nbsp;推出&nbsp;Claude&nbsp;3.5&nbsp;Sonnet&nbsp;以及最强视觉模型</h4><p></p><p>6&nbsp;月&nbsp;21&nbsp;日，Anthropic&nbsp;正式宣布推出&nbsp;Claude&nbsp;3.5&nbsp;Sonnet，是即将推出的&nbsp;Claude&nbsp;3.5&nbsp;型号系列中的第一款产品。据悉，Claude&nbsp;3.5&nbsp;Sonnet&nbsp;提高了行业智能标准，在各种评估中均优于竞争对手的型号和&nbsp;Claude&nbsp;3&nbsp;Opus，同时速度和成本与我们的中端型号&nbsp;Claude&nbsp;3&nbsp;Sonnet&nbsp;相当。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17f3f094c8dc1ca234f0824fc01af61c.png" /></p><p></p><p>据介绍，Claude&nbsp;3.5&nbsp;Sonnet&nbsp;是&nbsp;Anthropic&nbsp;即将推出的&nbsp;Claude&nbsp;3.5&nbsp;系列的首个版本。该模型提高了整个领域的智能水平，在绝大多数基准评估中都超越了竞品大模型和自家前代最强&nbsp;Claude&nbsp;3&nbsp;Opus。与此同时，运行速度、成本与自家前代&nbsp;Claude&nbsp;3&nbsp;Sonnet&nbsp;相当。</p><p></p><p>地址：<a href="https://claude.ai/">https://claude.ai/</a>"</p><p></p><p>Claude&nbsp;3.5&nbsp;Sonnet&nbsp;模型每百万输入收费为&nbsp;3&nbsp;美元/token，每百万输出收费&nbsp;15&nbsp;美元/token，具有&nbsp;200K&nbsp;令牌上下文窗口。</p><p></p><h4>前OpenAI联合创始人Ilya&nbsp;Sutskever成立新公司</h4><p></p><p>当地时间6月19日，原OpenAI联合创始人、首席科学家Ilya&nbsp;Sutskever在社交平台X上正式宣布了他离职后的创业项目——一家名为“安全超级智能”（SSI，Safe&nbsp;Superintelligence&nbsp;Inc.）的新公司。Sutskever&nbsp;透露，该公司的目标和产品只有一个：创建安全而强大的人工智能系统。“超级智能触手可及。构建安全的超级智能是我们这个时代最重要的技术问题。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e38b02b39f92d246a68438e979b25de.png" /></p><p></p><p>根据官方的公告介绍，SSI&nbsp;被描述为一家&nbsp;"将安全和能力结合在一起&nbsp;"的初创公司，在快速推进其人工智能系统的同时仍能将安全放在首位。公告还提到了&nbsp;OpenAI、谷歌和微软等公司的人工智能团队经常面临的外部压力，表示&nbsp;SSI&nbsp;的&nbsp;"单一关注点&nbsp;"使其能够避免&nbsp;"管理费用或产品周期的干扰"。</p><p></p><p>然而，对于&nbsp;SSI&nbsp;的运营理念，有一些网友提出了不同角度的犀利质疑。一方面是其追求的安全准则：“我们离‘超级智能’还差得很远，安全是重中之重吗？如果莱特兄弟专注于安全，我不确定他们会飞得很远。”</p><p></p><p>虽然目前尚不清楚谁将为这个新企业的发展提供资金，也不清楚其最终的商业模式究竟是什么，但&nbsp;Sutskever&nbsp;表示“筹集资金不会成为公司的问题”，并正在向业内感兴趣的人传达一个信息：SSI&nbsp;将在美国和以色列设立总部，目前正在招聘。现在，在&nbsp;X&nbsp;社交平台上，已有一位&nbsp;@signulll&nbsp;的网友，声称自己刚刚加入&nbsp;SSI。</p><p></p><p>除&nbsp;Sutskever&nbsp;之外，SSI&nbsp;还由苹果前&nbsp;AI&nbsp;负责人、Y-Combinator&nbsp;合伙人、Cue&nbsp;联合创始人&nbsp;Daniel&nbsp;Gross&nbsp;和曾在&nbsp;OpenAI&nbsp;担任技术人员的&nbsp;Daniel&nbsp;Levy&nbsp;共同创立。</p><p></p><h4>字节跳动悄然推出Instagram社交应用Whee</h4><p></p><p>6月19日消息，据外电报道，TikTok&nbsp;制造商字节跳动似乎正在测试一款名为&nbsp;Whee&nbsp;的新社交媒体应用。该应用目前已在&nbsp;Google&nbsp;Play&nbsp;商店上架，但在大多数市场都无法使用。</p><p></p><p>该应用的描述称，Whee&nbsp;允许用户捕捉和分享只有好友才能看到的真实照片。这表明&nbsp;Whee&nbsp;专注于好友之间类似&nbsp;BeReal&nbsp;的照片分享，而不是病毒式内容。</p><p></p><p>Whee&nbsp;的截图显示其设计与&nbsp;Instagram&nbsp;类似，其中有好友发布的照片。然而，与&nbsp;Instagram&nbsp;和&nbsp;Snapchat&nbsp;等平台不同，Whee&nbsp;的照片似乎只供选定的好友查看。好友可以互相点赞和评论对方的帖子，但外人看不到内容。</p><p></p><h4>华为与腾讯接近达成协议，不向微信“抽成”</h4><p></p><p>6月19日，据彭博社报道称：华为与腾讯即将达成协议，免除微信的收入分成&nbsp;(Revenue&nbsp;Sharing)。华为考虑在鸿蒙应用商店收取&nbsp;20%&nbsp;佣金，这将低于苹果&nbsp;App&nbsp;Store&nbsp;和谷歌&nbsp;Play&nbsp;商店。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab8b51ef03744c65af8ac8cf4a11fbdb.png" /></p><p></p><p>此外彭博社还报道，华为即将与腾讯达成一项协议。那就是华为将允许腾讯的微信&nbsp;App&nbsp;在其鸿蒙操作系统上运行，而不收取应用内收入分成。据知情人士透露，目前双方已经接近达成协议。华为免除了微信的抽成，而作为交换，腾讯也将持续维护和更新微信应用，算是一波双赢。</p><p></p><p>“微信应用”应该就是此前盛传已久的“鸿蒙原生版”微信，专门针对&nbsp;HarmonyOS&nbsp;NEXT&nbsp;进行了原生适配。</p><p></p><h4>英伟达成为全球市值第一公司！超越微软苹果</h4><p></p><p>当地时间6月18日，人工智能芯片制造商英伟达股价在当日收盘上涨3.51%，市值达到3.33万亿美元，超越微软成为全球市值最高的公司。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c67ad16dd7fc7f29e21ac6cd1050104f.png" /></p><p>英伟达周线图，来源：TradingView</p><p></p><p>英伟达在本月早些时候首次突破3万亿美元市值，并成功超越了苹果。这一市值的飞跃，再次证明了人工智能技术的市场潜力和投资者的极大兴趣。</p><p></p><p>英伟达市值从2万亿美元升到3万亿美元用了96天（日历日）。与之相比，根据Bespoke&nbsp;Investment&nbsp;Group的数据，微软用了945天，苹果则用了1044&nbsp;天。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/576b60c52270e70b40c52417732c5232.png" /></p><p></p><p>《福布斯》显示，该公司联合创始人兼首席执行官黄仁勋的净资产已升至约1170亿美元，位列全球富豪榜第11位。据券商Strategas的数据，在微软、苹果和英伟达市值突破3万亿美元以后，三家公司占标普500指数权重达到了20%以上。美股三巨头已经占全球股市市值的10%以上。</p><p></p><h4>人才争夺战加剧，英伟达挖走三星超500AI人才</h4><p></p><p>美国当地时间6月18日，英伟达股价上涨，总市值达3.34万亿美元，超过微软和苹果公司，成为全球市值最高的公司。英伟达无疑成为了这场AI竞赛中的大赢家。</p><p></p><p>不仅如此，随着科技巨头们争相抢占AI市场，尤其是AI半导体领域人才争夺战也愈演愈烈，在这场人才争夺战中英伟达同样是佼佼者。</p><p></p><p>根据LinkedIn截至6月19日数据，515名英伟达员工曾在三星电子工作，这一数字几乎是目前在三星工作的前英伟达员工数量（278人）的两倍。这意味着三星向英伟达的人才流失较为显著。这一人才流动甚至引起了韩国业内人士的极大担忧。</p><p></p><p>随着英伟达、美光和台积电等公司从三星电子和SK海力士等韩国半导体巨头那里吸引关键人才，这一人才缺口变得越来越严重，尤其是引领“人工智能&nbsp;(AI)&nbsp;热潮”的英伟达。英伟达凭借其在&nbsp;AI芯片领域的优势，不仅是三星，还吸引了其他巨头的人才。</p><p></p><h4>Runway视频生成新模型：10秒片段，90秒即成</h4><p></p><p>6月17日，凭借广受欢迎的视频生成工具而声名大噪的&nbsp;AI&nbsp;厂商&nbsp;Runway&nbsp;最近发布了最新版本的&nbsp;Runway&nbsp;Gen-3。Gen-3&nbsp;Alpha&nbsp;是&nbsp;Runway&nbsp;在专为大规模多模态训练所构建的全新基础设施之上，训练出的模型家族的首位成员。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eef247abea97e957e6ff7be443d397d6.png" /></p><p></p><p>相比于Gen-2，Gen-3&nbsp;Alpha在保真度、一致性和运动方面有了重大改进，它特别擅长生成具有自然动作、表情和情感的逼真人类角色。并且朝着构建通用世界模型的方向迈进了一步。</p><p></p><p>目前Gen-3还未开放给公众试用，但Runway在官网的博客中展示了数十个生成的视频样本。</p><p></p><p>官方分享的演示视频普遍为10秒长度，需要90秒的时间快速生成。据悉Gen-3&nbsp;Alpha将在未来几天内向所有人推出。</p><p></p><p>Gen-3&nbsp;Alpha的发布引起了业界的广泛关注。许多网友对其生成效果表示惊叹，认为它在画质和视频创意上都表现出色。这一技术的进一步发展可能会为影视创作领域带来一场AI革命。</p><p></p><h4>快手副总裁、推荐算法负责人宋洋离职，或重回谷歌</h4><p></p><p>6月19日消息，快手副总裁、快手推荐算法负责人宋洋已从快手离职。有消息称他已回到美国，加入Tik&nbsp;Tok，也有说法是他是重回谷歌。</p><p></p><p>2020年6月，宋洋加入快手，担任副总裁（职级为M4B）、快手社区科学部模型与应用部负责人，负责快手短视频、直播、电商、广告等领域的推荐模型工作，直接向快手高级副总裁、快手主站及社区科学线负责人于越汇报。</p><p></p><p>2023年底，快手对总监到副总裁级别的中高层员工进行大轮换，宋洋被调至搜索部门。</p><p></p><h4>ChatGPT时隔两周再次出现重大故障</h4><p></p><p>6&nbsp;月&nbsp;17&nbsp;日，OpenAI&nbsp;的&nbsp;ChatGPT&nbsp;出现故障，用户报告无法应答问题，展示错误答案。OpenAI&nbsp;确认问题并调查故障率偏高。至&nbsp;17:00，所有系统恢复运转，用户报错频率下降。ChatGPT&nbsp;3.5&nbsp;和&nbsp;ChatGPT&nbsp;4&nbsp;能生成包括图像的答案。</p><p></p><p>据用户在&nbsp;DownDetector&nbsp;上的报告，此次故障影响范围广泛，尤其波及了美国和英国的用户。无论是移动端还是网页版的&nbsp;ChatGPT，都时而无法应答用户提问，持续时间长短不一，有的数分钟，有的甚至完全无应答，还出现了各种各样的错误。这一状况无疑打乱了许多用户的使用节奏，对于那些依赖&nbsp;ChatGPT&nbsp;进行工作、学习和获取信息的人来说，更是造成了直接的影响。</p><p></p><p>值得注意的是，这并非&nbsp;ChatGPT&nbsp;首次遭遇宕机。过往报道显示，ChatGPT&nbsp;上次宕机是在&nbsp;6&nbsp;月&nbsp;4&nbsp;日，当时&nbsp;OpenAI&nbsp;也迅速修复了问题。而在&nbsp;5&nbsp;月份，ChatGPT&nbsp;更是连续四天出现间歇性中断和服务延迟的情况。</p><p></p><h4>华为要收回部分昇腾服务器代工？神州数码称目前未收到通知</h4><p></p><p>6月17日，一则“华为将收回部分昇腾整机业务”的消息，引发华为概念股、A股公司神州数码股价跌停，当天华为昇腾概念下跌0.08%。6月18日开盘，神州数码继续下跌，一度跌幅近5%，随后跌幅收窄，收盘跌1.87%。前一天开盘后，神州数码快速下跌至跌停，并以跌停报收。</p><p></p><p>由于华为昇腾是国产算力芯片的主要提供者，若华为对昇腾模式进行调整，想必对神州数码在内华为昇腾合作伙伴都会产生很大的影响。截至目前，华为没有针对传闻做出公开回应。神州数码也同样未做出公开回应。</p><p></p><p>6月18日，有记者以投资者身份致电神州数码投资者热线，接线人士表示，截至目前没有收到任何通知，公司的业务一切正常。</p><p></p><p>前述人士表示，2023年神州数码总营收1000多亿，其中这块业务营收在4亿-5亿元。“我们是上市公司，如果有重要的信息或者业务进展肯定会发布公告的，会跟投资人做披露。”</p><p></p><p>如果传闻属实的话，对华为昇腾系的服务器厂商拓维信息、软通动力、神州数码的增长逻辑有明显的负面影响。这几家都是华为昇腾服务器代工合作伙伴。记者也向一家昇腾服务器合作伙伴询问，相关人士表示，没有听到类似消息。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>苹果暂停Vision&nbsp;Pro二代研发：转而专注廉价版产品</h4><p></p><p>6月19日据The&nbsp;Information报道,苹果公司已经暂停了下一代Vision&nbsp;Pro头戴式耳机的研发工作,转而将重心放在开发一款更实惠、功能较少的Vision产品线上。</p><p></p><p>根据消息人士透露,在过去一年中,苹果一直在逐步降低下一代Vision&nbsp;Pro耳机的研发优先级,并减少了为这个项目分配的员工数量。该公司目前更关注于降低首款Vision&nbsp;Pro的零部件成本,并为未来型号开发升级版显示屏。</p><p></p><p>据悉,苹果已经明确告知至少一家供应商,它已暂停了下一代Vision&nbsp;Pro头戴设备的研发工作。不过,该公司仍在继续研究一款定位较低、价格更亲民的Vision设备。</p><p></p><h4>微软邮箱漏洞允许任何人冒充该公司员工</h4><p></p><p>安全研究人员&nbsp;Vsevolod&nbsp;Kokorin&nbsp;aka&nbsp;Slonser&nbsp;发现了一个漏洞，允许任何人冒充微软的电邮账号，让钓鱼邮件看起来更可信，更可能欺骗被钓鱼的目标。该漏洞尚未修复，一个原因是微软认为漏洞无法复现。</p><p></p><p>研究员上周向微软报告了该漏洞，因为无法重现微软否定了其报告，因此他在社交媒体上公开了该漏洞，但没有提供可帮助其他人利用的技术细节。</p><p></p><p>研究人员解释说，当攻击者向Outlook帐户发送电子邮件时，该漏洞就会起作用。</p><p></p><p>“Kokorin说，他最后一次跟进Microsoft是在6月15日。Microsoft周二没有回应&nbsp;TechCrunch&nbsp;的置评请求，“TechCrunch&nbsp;报道。“TechCrunch&nbsp;没有透露该漏洞的技术细节，以防止恶意黑客利用它。目前，该问题尚未得到解决，目前尚不清楚是否有任何威胁行为者已经在野外攻击中利用了它。</p><p></p><h4>DeepSeek&nbsp;Coder&nbsp;V2开源发布，首超GPT4-Turbo的代码能力</h4><p></p><p>6月18日&nbsp;DeepSeek&nbsp;官方消息，DeepSeek&nbsp;发布了一款名为&nbsp;DeepSeek-Coder-V2的开源模型，这一模型在代码和数学能力方面超越了&nbsp;GPT-4-Turbo。</p><p></p><p>DeepSeek-Coder-V2&nbsp;沿用&nbsp;DeepSeek-V2&nbsp;的模型结构，总参数&nbsp;236B，激活&nbsp;21B，在代码、数学的多个榜单上位居全球第二，介于最强闭源模型&nbsp;GPT-4o&nbsp;和&nbsp;GPT-4-Turbo&nbsp;之间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0147d5105d717859f49b1be03623e49b.png" /></p><p></p><p>具体来说，DeepSeek-Coder-V2&nbsp;从&nbsp;DeepSeek-V2&nbsp;的中间检查点进一步预训练，额外添加了&nbsp;6&nbsp;万亿个&nbsp;token。通过这种持续的预训练，DeepSeek-Coder-V2&nbsp;大幅增强了&nbsp;DeepSeek-V2&nbsp;的编码和数学推理能力，同时在一般语言任务中保持了相当的性能。与&nbsp;DeepSeek-Coder-33B&nbsp;相比，DeepSeek-Coder-V2&nbsp;在代码相关任务的各个方面以及推理和通用能力方面都有了显著的进步。</p><p></p><p>此外，DeepSeek-Coder-V2&nbsp;对编程语言的支持从&nbsp;86&nbsp;种扩展到&nbsp;338&nbsp;种，同时将上下文长度从&nbsp;16K&nbsp;扩展到&nbsp;128K。</p><p></p><h4>Meta推出AI音频水印工具，能鉴别AIGC音频和真人音频</h4><p></p><p>Meta近日创建了一个新系统，可以在人工智能生成的音频片段中嵌入名为“水印”的隐藏信号，有助于在网络上检测人工智能生成的内容。</p><p></p><p>该工具名为AudioSeal，它可以在长达一小时的播客中找到哪些音频片段可能是由人工智能生成的。这是第一个能实现该功能的工具。</p><p></p><p>Meta&nbsp;的研究科学家哈迪·埃尔萨哈尔（Hady&nbsp;Elsahar）表示，它可以帮助解决语音克隆工具带来的日益严重的错误信息和骗局问题。</p><p></p><p>AudioSeal&nbsp;在&nbsp;GitHub&nbsp;上免费开源。任何人都可以下载，并使用它为人工智能生成的音频添加水印。它最终可以“依附”在人工智能音频生成模型之上，从而自动应用于使用它们生成的任何音频。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VNev0USiz2LvbMrq5wtQ</id>
            <title>TikTok 首度曝光多年来与美秘密谈判细节；美国新规拟禁止在中国投资 AI ；00 后女孩离职删软件被公司威胁起诉| AI周报</title>
            <link>https://www.infoq.cn/article/VNev0USiz2LvbMrq5wtQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VNev0USiz2LvbMrq5wtQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 03:53:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, 美国政府, 言论自由, CFIUS
<br>
<br>
总结: TikTok 对美国政府提出的出售美国业务要求进行反击，认为这违反了言论自由权，公开了与美国政府的秘密谈判细节，指控美政府的法案缺乏解决问题的诚意，希望法院能够裁决以解决此案。 </div>
                        <hr>
                    
                    <p></p><blockquote>TikTok 发起进攻，首度曝光多年来与美秘密谈判细节；00 后女孩离职删软件被公司威胁起诉；英伟达成全球市值最高上市公司，五年前加入英伟达员工已成百万富翁；马斯克：我宁愿亲眼见证 AI 毁灭人类；北大人民医院借助 Vision Pro 完成肺癌根治术……</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>“美官员受政治蛊惑”？TikTok 发起进攻，首度曝光多年来与美秘密谈判细节</h4><p></p><p></p><p>6 月 20 日消息，TikTok 及其八名创作者在向美国法院提交了法庭书状，指控美国政府的“不卖就禁”法案违反了美国宪法，要求推翻。TikTok 公开了与美国政府秘密谈判的内部文件，显示其提出的“得克萨斯计划”旨在解决美方的国家安全担忧，但美方仍坚持推动法案，缺乏解决问题的诚意。</p><p></p><p>TikTok 认为，拜登政府要求其出售美国业务的做法不可行，侵犯了言论自由权。法案禁止数据共享，将使 TikTok 在美国成为“孤岛”，限制了美国人与全球社区的交流，开创了压制言论自由的危险先例。TikTok 披露的内部文件记录了与美国外国投资委员会（CFIUS）的谈判过程。TikTok 提交了国家安全协议草案，但 CFIUS 在 2022 年 8 月后停止了实质性谈判。2023 年 3 月，CFIUS 告知 TikTok，高级官员要求继续剥离，但未解释原因。TikTok 试图与美官员会面，但未得到答复。</p><p></p><p>TikTok 提供的邮件显示其为解除禁令、恢复谈判的努力。TikTok 指出，美政府的回应模糊且不成熟，立场脱离现实。公司一直以负责任态度对待进程，但面对政府的公开运动，担忧 CFIUS 受政治蛊惑。美司法部须在 7 月 26 日前回应，答辩书截止 8 月 15 日，预计 9 月口头辩论。TikTok 希望 12 月 6 日前裁决，以便向最高法院请求审查。此案可能决定 App 命运及法院对第一修正案的解释。</p><p></p><p>专家意见认为，TikTok 已最大程度努力解决关切，美政府似乎刻意针对 TikTok。前 CFIUS 代表认为 TikTok 提议是最彻底的缓解协议，实施后国安风险将降低。纽约大学法学教授认为法案实际是禁令，强制出售选项虚幻。加州大学教授指出，美政府担忧是行业问题，非 TikTok 独有，法案特别关注 TikTok，无明显国安理由。中方驳斥美方以国家安全为由打压企业，认为这是强盗逻辑。据分析，此案将言论自由权与国安利益对立，可能旷日持久，最终可能诉至最高法院，判决可能在 2025 年第二季度前。</p><p></p><h4>美国发布新拟议规则，禁止在中国投资 AI、半导体、量子计算</h4><p></p><p></p><p>6 月 22 日，美国财政部官网消息，发布了一项执行拜登总统令的提案通知（NPRM），旨在实施 2023 年 8 月 9 日签署的第 14105 号行政命令——境外投资令。此提案通知是在财政部去年 8 月发布的预先提案通知（ANPRM）基础之上进行了全面强化，包括拟议规则的全貌、意图、并公开征求公众意见。如果有异议，可以在 8 月 4 日之前提出意见。</p><p></p><p>根据详细内容显示，中国香港、澳门和大陆成为主要关注对象，并禁止美国企业进行 AI、半导体和微电子、量子计算三项投资。</p><p></p><h4>宁德时代被曝“896”奋斗 100 天，员工：确有此事，但并非全部员工；官方回应：曲解造谣，公司没有发这样的规定</h4><p></p><p></p><p>近日，网上有文件显示，宁德时代向员工发出了“奋斗 100 天”的号召。文件显示，为更好完成组织赋予的任务，加快推进各项工作达成，公司号召从今天（6 月 12 日）起，实行 896 的工作日：早上 8 点上班，晚上 9 点下班，每周工作 6 天，共“奋斗”100 天。另外，还有补充通知说明：外籍员工不强制，按照他们的意愿。</p><p></p><p>据报道，有宁德时代内部员工表示确有此事，是上周五部门开会口头通知此事，并表示：“之前是也要加班，但不强制到九点。”另有知情者透露，该号召是针对一定级别以上员工，并非所有员工。但据该公司相关人士回复，“此事为曲解造谣，公司没有发出这样的规定。”</p><p></p><p>值得一提的是，网友声称早 8 晚 9 之前已经是默认的工作时间。据悉，在宁德时代，此次的“奋斗 100 天”号召已经不是第一次了，早在 2022 年的一篇报道中，宁德时代就被提到：奋斗 100 天已成常态化。据 2022 年的文章报道，“加班受得了吗？”每个前来宁德时代面试的人，都会被反复暗示这一问题。无论就职意向是作业员、工程师，还是中层管理者，来到这家企业的第一项考核就是加班。互联网大厂的“996”，在宁德时代打工人口中叫“义务加班”，月均 100 个小时起步。普通蓝领的加班时长会被算入当月工时，以工资的形式结算。工程师加班没有加班费，只有绩效考核。</p><p></p><p></p><h4>腾讯招聘新增“AI 大模型”专项，扩招人数幅度超过 50%</h4><p></p><p></p><p>6 月 19 日晚间，腾讯官方社交平台发布招聘信息，新增“AI 大模型”专项，扩招人数幅度将超过 50%。</p><p></p><p>根据官微介绍，腾讯去年启动“青云计划”，面相全球招募顶尖技术人才，并提供全面定制化的培养和极具竞争力的薪酬。腾讯称，一旦入选，将开放多个腾讯核心业务岗位，让应聘者深度参与前沿的技术课题研究，如 AI、大模型、安全、游戏引擎。</p><p></p><h4>微软云中国或将迎来大调整：各行业销售线内部“合并同类项”</h4><p></p><p></p><p>消息称，微软云中国区或将于 2024 财年底（今年 6 月底）前后进行一次较大的组织架构调整，调整对象主要聚焦于微软大中华区一号位侯阳领导下的 700 多人的销售团队。据多位知情人透露，此次调整方式或为：在各细分行业销售线中，将具有相似职能的小团队整合在一起，以降低多头对接、分兵散打带来的沟通内耗和作业低效。</p><p></p><p>此次或将到来的调整是否会导致大规模裁员，尚不得而知。据微软云中国离职员工透露，与国内许多大公司实行的年度末位淘汰制不同，微软中国销售线的淘汰机制以季度为周期，更具“狼性”，每个季度都会淘汰一些排名末尾的员工。目前，微软云中国销售团队规模约保持在 700 人左右。</p><p></p><p></p><h4>00 后女孩离职删软件被公司威胁起诉</h4><p></p><p></p><p>6 月 18 日，广东广州一名 00 后女孩发布视频哭诉称，自己因离职时删除了公司电脑上的一些软件，面临被公司法务起诉的威胁。据了解，女孩小蒋于 2023 年毕业，刚进入社会参加工作不久。“太离谱了！我只是把 QQ 音乐、QQ、微信、谷歌浏览器、百度网盘卸载了，领导说对公司造成了不良的影响。”小蒋告诉记者，因为办公室电脑内存太小运行卡顿，删除“QQ”“微信”等内存占用比较大的软件是想让电脑运行更快。</p><p></p><p>“好心办了坏事，领导认为删除那些软件对后来的新人，多了一个下载的步骤，产生了不必要的麻烦，当时就要上报公司法务起诉我。”小蒋表示，拍摄视频是因为觉得委屈，自己刚毕业参加工作就面临被公司起诉的威胁，一时手足无措。“当时我被吓得要报警，因为我没有做出任何损害公司利益的事情，他们就要起诉我。工作两个多月时间，公司也没有交给过我任何机密类文件，我只是把我自己下载的、登录了个人账户的软件删除了。”</p><p></p><p>小蒋表示，后来公司法务并未起诉。</p><p></p><h4>英伟达成全球市值最高上市公司，五年前加入英伟达员工已成百万富翁</h4><p></p><p></p><p>6 月 20 日消息，据外媒报道，英伟达近年来取得了惊人的增长。自 2024 年初以来，该公司股价已上涨 167%。在过去五年中，其涨幅高达 3450%。许多五年前或更早加入 NVIDIA 的员工如今可能都成了百万富翁。此外，据报道，得益于股票期权和公司股票的整体升值，NVIDIA 的许多中层管理人员每年的薪酬超过 100 万美元。</p><p></p><p>然而，现在资金充裕意味着许多 Nvidia 高管据称处于半退休模式，这引起了首席执行官黄仁勋的注意。他们的经济状况已经足够宽裕，似乎不再像以前那样努力工作了。在回答有关半退休员工的问题时，黄仁勋建议所有员工都当好自己时间的首席执行官，并负责确定自己的职业道德。尽管如此，黄仁勋在上财年也获得了 60% 的加薪，他的薪酬达到了 3420 万美元。</p><p></p><p>值得一提的是，美东时间 6 月 18 日，英伟达市值达到 3.34 万亿美元，一举超越了长期占据市值榜首的微软。此前，英伟达在本月早些时候首次突破 3 万亿美元市值，并成功超越了苹果。英伟达最新得到的华尔街最高目标价为 200 美元，最乐观分析师预计其估值在未来一年内将接近 5 万亿美元。</p><p></p><h4>Meta 重组元宇宙团队</h4><p></p><p></p><p>北京时间 6 月 19 日，据 The Verge 报道，Meta 首席技术官 Andrew Bosworth 宣布将启动该公司硬件部门自 2020 年更名为 Reality Labs 以来最大规模的重组。Bosworth 提到组织架构重组的原因包括：要专注于 MR 软件平台，雷朋智能眼镜销量远超预期，创造更加集成的产品体验，以及希望能够减少管理费用，并允许跨团队的人员聚集在一起。</p><p></p><p>根据 Meta 发给员工的内部备忘录：Reality Labs 的所有团队将合并为两个部门，一个中央“元宇宙”部门，包括 Quest 头显产品线；另一个新的“可穿戴设备”部门涵盖 Meta 的其他硬件产品，包括与 Ray-Ban 合作的智能眼镜。</p><p></p><h4>苹果瞄向大众市场：搁置 Vision Pro 2 研发工作，聚焦 2025 年底推出“廉价头显”</h4><p></p><p></p><p>6 月 18 日消息，苹果正在开发一款更便宜但功能也更精简的 Apple Vision Pro 头显，并计划于 2025 年底发售。同时，苹果已经搁置下一代高端 Apple Vision Pro 2 的研发工作，此举似乎是为了优先保证上述低价头显的研发进度。据悉苹果未来某个时候有可能会再次恢复 Vision Pro 2 研发，但目前看来，这似乎反映了该公司暂时改变了战略。</p><p></p><p>另外，过去一年来，苹果分配给第二代 Vision Pro 项目的员工数一直在逐渐减少，其中最主要的原因就是苹果注意力转向了这款更便宜的型号。虽然之前已经多次听到有关“廉价头显”的消息，但目前仍无法确认这款新品的目标价格，但任何低于 3500 美元的机型都将可以更好地与 Meta Quest 竞争。业内人士认为，苹果这款“廉价头显”的目标价约为 1500 美元（当前约 10898 元人民币），接近高端 iPhone 的价格，敬请期待。另外，据报道，苹果公司已经开始讨论在中国寻找合适的本地 AI 合作伙伴，以支持其人工智能生成功能。报道称，苹果目前正在与百度、阿里巴巴集团以及总部位于北京的初创公司百川人工智能（Baichuan AI）进行谈判，为其 AI 服务寻找本地合作伙伴。</p><p></p><p>国内媒体就此询问了百度、阿里巴巴、百川智能等公司，截至发稿，上述公司尚未作出公开回应。</p><p></p><h4>华为、腾讯接近达成协议，鸿蒙不对微信交易收取佣金，抖音不感兴趣</h4><p></p><p></p><p>6 月 19 日，据报道，华为公司接近与腾讯控股达成一项协议，允许微信在鸿蒙移动系统上全面运行，而无需分享任何收入。华为的这一让步旨在捍卫其在中国移动操作系统市场对苹果公司新获得的领先地位。</p><p>知情人士称，华为和腾讯这两家深圳科技巨头已进行了长达数月的谈判。根据协议，华为同意不因为微信应用内交易对腾讯收取任何费用。作为交换，腾讯将在鸿蒙平台上维护和更新微信应用。目前，华为正考虑对鸿蒙应用商店内的内容和服务交易收取佣金。</p><p></p><p>此外，华为还和字节跳动旗下抖音进行了接触，试图讨论收入分成问题，但抖音没有表达出任何开启谈判的兴趣。</p><p></p><h4>OpenAI“宫斗”核心人物 Ilya Sutskever 官宣创办“安全超级智能”公司</h4><p></p><p></p><p>北京时间 6 月 20 日凌晨，原 OpenAI 公司联合创始人、首席科学家 Ilya Sutskever 在𝕏官宣了他正式创业的消息 —— 创办了一家名为“安全超级智能”（Safe Superintelligence，简称 SSI）的新公司，旨在“直截了当”地创造一个安全的超级智能。</p><p></p><p>Ilya Sutskever 表示，公司将只有一个重点、一个目标和一个产品，通过一个小型破解团队来取得“革命性”的突破，去实现追求安全超级智能的目标。同时，新公司自称是“世界上第一个”直击 SSI 的实验室。据报道，Sutskever 与 OpenAI 前员工 Daniel Levy 以及 AI 投资人和企业家 Daniel Gross 在美国共同创办了这家新公司。值得一提的是，Daniel Gross 持有 GitHub 和 Instacart 等公司的股份，以及 Perplexity.ai、Character.ai 等 AI 公司的股份。</p><p></p><p>延伸阅读：<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247616675&amp;idx=1&amp;sn=85339d5ff7e0199a9912aaf3266256ff&amp;chksm=fbebb16ccc9c387a2050a738f3dfa99ddc99a265a237bc46e52ff0534096180257fa0b4e9431&amp;scene=21#wechat_redirect">Ilya 官宣新公司，主打“恶意”竞争！先拉不缺钱的技术大佬入伙，不盈利也要赢过 OpenAI ！</a>"</p><p></p><h4>马斯克：我宁愿亲眼见证 AI 毁灭人类</h4><p></p><p></p><p>6 月 20 日消息，在 2024 年戛纳狮子国际创意节上，特斯拉与 SpaceX 的首席执行官埃隆·马斯克接受了 WPP 首席执行官马克·里德的专访，分享了对人工智能未来发展的复杂看法。</p><p></p><p>马斯克认为，人工智能的发展是一个概率问题，他对此持有既乐观又悲观的态度，他引用了人工智能领域的领军人物杰夫·辛顿的观点，认为存在 10% 到 20% 的可能性出现令人担忧的情境。然而，他更倾向于关注那 80% 的积极可能性，并预言我们将进入一个物质极度丰富的时代，其中商品和服务将普及到每一个人。</p><p></p><p>马斯克同时警告说，这样的前景可能会引发一场关于生命意义的危机，当人工智能能够胜任所有工作时，人类存在的意义将受到质疑。尽管如此，马斯克表示，即使面对人工智能可能带来的最坏结果——人类被消灭，他也会选择直面这一现实，“我可能真的愿意亲眼见证这一切的发展。”</p><p></p><p></p><h4>心存不满“跑路删库”致 67.8 万美元损失，印度一程序员被判两年零八个月监禁</h4><p></p><p></p><p>6 月 17 日消息，据外媒报道，一名任职于新加坡 NCS 集团的印度工程师 Kandula Nagaraju，因被解雇后心生不满而采取了“删库”的报复行动。他使用自己的笔记本电脑通过管理员登录凭据在未经授权的情况下访问了 NCS 的系统，并删除了公司的 180 台虚拟服务器，造成约 67.8 万美元（约 493.3 万元人民币）的损失。</p><p></p><p>NCS 团队在发现系统无法访问后，意识到服务器已被删除，并于 2023 年 4 月向警方报案，警方调查锁定了 Kandula Nagaraju 为嫌疑人，并在他的笔记本电脑中发现了用于执行删除操作的脚本。2024 年 6 月 10 日，Kandula Nagaraju 因未经授权访问计算机的指控被警方抓获，最终判处两年零八个月监禁，此外他还面临另一项未具名的指控，据称“刑期可能会进一步增加”。</p><p></p><p>据悉，Kandula Nagaraju 在 2021 年 11 月至 2022 年 10 月期间曾任职 NCS 集团质量保证（QA）部。在 2022 年 11 月 16 日其由于工作表现问题被终止合同。根据法院文件，Kandula Nagaraju 在被解雇时感到“困惑和沮丧”，因为他觉得自己表现良好，并在任职期间为 NCS 作出了“良好贡献”，因此他决定“报复公司”。</p><p></p><p></p><h2>IT 业界</h2><p></p><p></p><h4>ChatGPT 再次出现重大故障，本月第二次宕机</h4><p></p><p></p><p>美东时间 6 月 17 日下午两点，用户在 DownDetector 报告称，OpenAI 的 ChatGPT 发生故障。随后，OpenAI 迅速确认 ChatGPT 出现问题，并在 14:39 更新状态栏，称针对 ChatGPT“调查故障率偏高”这个问题。这是该聊天机器人 6 月份第二次发生重大中断。</p><p></p><p>媒体报道称，在美国和英国，移动端和网页版 ChatGPT 会时不时地无法应答用户的提问——要么持续数分钟时间、要么根本就无应答，并展示各种各样的错误（答案）。至 17:00，OpenAI 更新状态栏为“所有系统处于可运转状态”；DownDetector 也显示，用户针对 OpenAI 的报错频率下降。ChatGPT 3.5 和 ChatGPT 4o 能够生成包括图像在内的答案。</p><p></p><p>此外，最近，加州大学圣地亚哥分校的科学家进行一项实验，让 500 名人类与四种 AI 语言模型进行了 5 分钟的对话，其中 GPT-4 在 54% 的时间里被误认为是人类，这一结果虽不及人类 67% 的平均水平，但是已经超过图灵测试的标准（注：超过 30% 代表通过图灵测试）。</p><p></p><p></p><h4>腾讯混元文生图大模型开源训练代码，发布&nbsp;LoRA&nbsp;与 ControlNet 插件</h4><p></p><p></p><p>6 月 21 日，腾讯混元文生图大模型（以下简称为混元 DiT 模型）宣布全面开源训练代码，同时对外开源混元 DiT LoRA 小规模数据集训练方案与可控制插件 ControlNet。这意味着，全球的企业与个人开发者、创作者们，都可以基于混元 DiT 训练代码进行精调，创造更具个性化的专属模型，进行更大自由度的创作；或基于混元 DiT 的代码进行修改和优化，基于此构建自身应用，推动技术的快速迭代和创新。</p><p></p><p>作为中文原生模型，用户在通过混元 DiT 的训练代码进行精调时，可以直接使用中文的数据与标签，无需再将数据翻译成英文。此前，腾讯混元文生图大模型宣布全面升级并对外开源，已在 Hugging Face 平台及 Github 上发布，可供企业与个人开发者免费商用。这是业内首个中文原生的 DiT 架构文生图开源模型，支持中英文双语输入及理解。模型开源仅一个月，Github Star 数达到 2.4k，位于开源社区热门 DiT 模型前列。</p><p></p><p>代码：https://github.com/Tencent/HunyuanDiT</p><p>模型：https://huggingface.co/Tencent-Hunyuan/HunyuanDiT</p><p></p><h4>北大人民医院借助 Vision Pro 完成肺癌根治术</h4><p></p><p></p><p>近日，在北京大学人民医院胸外科，医师高健在王俊院士、李运主任和周足力教授的悉心指导下，成功完成了一次运用 Apple Vision Pro 辅助的胸腔镜肺癌根治术。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/597b66f633b6a98d0a32729fe4407711.png" /></p><p></p><p>高健医师表示：“以往，医生在手术时需要在多个显示器间频繁切换，以获取患者的生命体征数据和手术相关信息。而现在，有了 Apple Vision Pro 这一头显设备，所有信息都能清晰直观地展示在眼前。如果需要再次查看患者的 CT 扫描结果或其他数据，医生只需通过简单的眼睛、手势和语音交互，就能非接触式地调取并阅读这些信息，大大提高了手术的效率和安全性。”</p><p></p><h4>OpenAI 竞争对手 Anthropic 发布其 AI 模型 Claude 3.5</h4><p></p><p></p><p>OpenAI 竞争对手 Anthropic 6 月 20 日发布了其最新的 AI 模型 Claude 3.5 Sonnet。今年 3 月，Anthropic 推出了 Claude 3 系列模型。随后，OpenAI 在 5 月份推出了 GPT-4o。Anthropic 表示，Claude 3.5 Sonnet 比之前的主打模型 Claude 3 Opus 速度更快，也是 Anthropic 新的 Claude 3.5 家族的第一款模型。</p><p></p><p>延伸阅读：<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247616955&amp;idx=3&amp;sn=fd4b743a14126650b28d4e03bb21715d&amp;chksm=fbebb074cc9c39621c9971a79aa965abe9dd317ec7668bda70329301367522b2a66d4ca98bbb&amp;scene=21#wechat_redirect">已卷疯！距上次更新仅隔三月，Anthropic 又发布 Claude 3.5 Sonnet，可是生成笑话得靠抄袭？</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lFfSlNdJ4IzNwkvooWOj</id>
            <title>OpenAI 发布 GPT 模型规范，可作为模型微调指南</title>
            <link>https://www.infoq.cn/article/lFfSlNdJ4IzNwkvooWOj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lFfSlNdJ4IzNwkvooWOj</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 02:17:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, GPT 模型规范, InstructGPT, 模型微调
<br>
<br>
总结: OpenAI发布了GPT模型规范，作为模型微调的指南。规范包含了模型行为规则和目标的描述，帮助数据标注人员和AI研究人员创建微调数据。2022年，OpenAI推出了GPT-3的微调版本InstructGPT，使用RLHF对模型输出排序数据集进行微调，以减少错误或有害的输出。模型规范的目的是指导标注人员对输出进行排序，解决常见的LLM滥用问题。 </div>
                        <hr>
                    
                    <p>OpenAI 发布 GPT 模型规范，可作为模型微调指南OpenAI 最近发布了其模型规范，这是一份描述 GPT 模型行为规则和目标的文档。该规范可供数据标注人员和 AI 研究人员在为模型微调创建数据时使用。</p><p></p><p>该模型规范基于 OpenAI 现有内部文档，OpenAI 在他们的人类反馈强化学习（RLHF）训练中使用了这些文档。规范包含了三种类型的原则：目标、规则和默认设置。目标定义了对模型行为的广泛描述：“造福人类”。规则则更加具体，涉及到用户绝不能违反的“高风险”情况：“永远不要做 X”。最后，规范包括了默认行为，虽然它们可以被覆盖，但提供了响应的基本样式指南和处理冲突的模板。根据 OpenAI 的说法：</p><p></p><p></p><blockquote>作为我们在集体对齐和模型安全方面工作的延续，我们打算将模型规范作为研究人员和 AI 训练者进行人类反馈强化学习的指南。我们还将探索我们的模型能够直接从模型规范中学习到怎样的程度。我们将这项工作视为正在进行的关于模型的行为、如何确定期望的模型行为以及如何让公众参与这些讨论的持续公开对话的一部分。</blockquote><p></p><p></p><p>2022 年，OpenAI 推出 GPT-3 的微调版本 InstructGPT 。该模型使用 RLHF 对模型输出排序数据集进行微调，目的是让模型更加“对齐”用户意图，减少错误或有害的输出。从那时起，许多研究团队也对他们的 LLM 进行了类似的微调。例如，谷歌的 Gemini 模型也使用 RLHF 进行微调。Meta 的 Llama 3 也经过微调，但是采用了不同的微调方法，即直接偏好优化（DPO）。</p><p></p><p>然而，微调的关键是由人工标记器排序的具有多个输出的提示输入数据集。模型规范的部分目的是指导标注人员对输出进行排序。OpenAI 还声称正在研究直接根据模型规范自动化指令微调过程的方法。因此，模型规范的许多内容都是用户提示词以及“好”的和“坏”的响应的示例。</p><p></p><p>规范中的许多规则和默认设置旨在解决常见的 LLM 滥用问题。例如，遵循命令链规则旨在帮助防止简单的“越狱”行为，即提示模型忽略前面的指令。其他规范旨在指导模型做出响应，特别是在模型拒绝执行任务时。规范中提到：“拒绝应该用一两句话解决，不要啰嗦”。</p><p></p><p>沃顿商学院教授和 AI 研究员 Ethan Mollick 在 X 上发表了有关模型规范的帖子：</p><p></p><p></p><blockquote>正如评论中的一些人指出的那样，Anthropic 有它自己的章程。我发现它不像声明那么有分量，也不那么清晰，因为它概述了好的内容，并告诉 AI 要做好，这让人很难理解原则之间存在怎样艰难的选择。</blockquote><p></p><p></p><p>Anthropic 在 2022 年提出了 Constitutional AI 的概念。这个过程使用 AI 模型对输出进行排名以进行指令微调。尽管 Anthropic 的代码不是开源的，但 AI 社区 HuggingFace 基于 Anthropic 的工作发布了 Constitutional AI 的参考实现。</p><p></p><p>查看英文原文：</p><p></p><p><a href="https://www.infoq.com/news/2024/06/openai-model-spec/">https://www.infoq.com/news/2024/06/openai-model-spec/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tQFvecQHUUJ6szBgC2sA</id>
            <title>斯坦福人工智能指数 2024 报告：人工智能法规和生成式人工智能投资的增长</title>
            <link>https://www.infoq.cn/article/tQFvecQHUUJ6szBgC2sA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tQFvecQHUUJ6szBgC2sA</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 02:12:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 2024 AI Index annual report, 生成式人工智能, 大语言模型
<br>
<br>
总结: 斯坦福大学发布了2024年度人工智能指数报告，揭示了人工智能领域的主要趋势，包括生成式人工智能投资增长、大语言模型训练成本增加等。报告涵盖了多个章节，强调了人工智能对科学、医学等领域的影响，以及人工智能的私人投资下降趋势。 </div>
                        <hr>
                    
                    <p>斯坦福大学以人为中心的人工智能研究所（HAI）发布了《2024 人工智能指数年度报告》（2024 AI Index annual report）。该报告确定了人工智能的主要趋势，例如自 2022 年以来，生成式人工智能投资增长了 8 倍。</p><p></p><p>今年是《人工智能指数》报告第七版的发布年，该报告由一个跨学科团队与政府、工业界和学术界合作编写。该报告共包含九个章节，编辑们从该指数中提炼出了几个关键要点，包括：去年美国的人工智能法规数量增加了 56.3%；模型训练成本，尤其是大语言模型（LLM）的成本，近年来已经“显著地”增加；尽管生成式人工智能（Generative AI）的投资有所增长，但自 2021 年以来，人工智能的总体私人投资有所下降。该指数的联席负责人 Ray Perrault 和 Jack Clark 写道：</p><p></p><p></p><blockquote>2024 年的指数报告是我们迄今为止最全面的一版，它发布于一个非常重要的时刻，此时人工智能对社会的影响前所未有。今年，我们扩大了范围，更广泛地涵盖了如人工智能的技术进步、公众对该技术的看法以及围绕其发展的地缘政治动态等关键趋势。该版本提供了比以往任何时候都多的原始数据，引入了关于人工智能训练成本的新估计，对负责任的人工智能格局的详细分析，以及一个专门讨论人工智能对科学和医学影响的全新章节。</blockquote><p></p><p></p><p>该报告共分为九个章节：研究与开发、技术性能、负责任的人工智能、经济、科学和医学、教育、政策和治理、多样性和公众舆论。《科学与医学》章节是今年新增加的，重点介绍了人工智能模型在科学和医学研究中日益增长的作用，其中特别提到了诸如 DeepMind 的 AlphaDev 模型这样的例子，该模型产生了一种更高效的排序算法。报告还指出，自 2021 年以来，美国食品药品监督管理局（FDA）批准的与人工智能相关医疗设备增加了 12.1%。</p><p></p><p>在关于《研究和开发》的章节中，该报告深入探讨了基础模型的训练成本，尤其是大语言模型（LLM）的训练成本。该报告指出，“关于这些成本的详细信息仍然很少”，并与人工智能研究机构 Epoch AI 合作估计了成本。该报告包括一张图表，显示了随着时间的推移，训练成本呈指数级增长，谷歌最初的 Transformer 模型的训练成本估计不到 1000 美元，而最近的模型，如 GPT-4 和 Gemini，训练成本则高达 1 亿美元或更多。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/63/637b5188be3fa735e9fc8b75097ca1c0.png" /></p><p></p><p>随着时间的推移，模型的训练成本</p><p></p><p>根据这份报告，这种训练成本的增长“实际上排除了大学”发展模型的可能。该报告的数据显示，在 2023 年，工业实验室生产了 51 个“引人注目的”模型，相比之下，学术界仅生产了 15 个；而在 2016 年之前，学术界生产的模型数量与工业界相当，甚至更多。另一方面，产研合作在 2023 年创造了 21 个引人注目的模型，这一数字创下了新高。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/12/12779cb2e4e114a1e638b455442c918d.png" /></p><p></p><p>按行业划分的引人注目的模型</p><p></p><p>JVM 周刊（JVM Weekly Newsletter） 编辑 Artur Skowroński针对这份报告在 LinkedIn 上写道：</p><p></p><p></p><blockquote>对于那些想了解正在发生的事情，但又没有时间持续关注的人来说（而且最近我就有这样的想法，鉴于数量这么多且频繁的公告，尤其是当你想要核实任何事情时，这是极其困难的），这是一份必读的资料。虽然有 500 页，但它易于访问且提供了良好的解析度——每个主题都从总体概述到详细细节进行了介绍。</blockquote><p></p><p></p><p>完整报告 可从人工智能指数（AI Index）网站下载。报告的 原始数据和图表 可在谷歌云端硬盘（Google Drive）上公开获取。该报告是采用知识共享 署名 - 禁止演绎 4.0 国际许可协议（Creative Commons Attribution-NoDerivatives 4.0 International license）。</p><p></p><p>原文链接：</p><p></p><p><a href="https://www.infoq.com/news/2024/05/stanford-ai-index/">https://www.infoq.com/news/2024/05/stanford-ai-index/</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4b750c75f0261f2c491f48c75</id>
            <title>小浣熊家族 X InfoQ 写作社区有奖征文大赛｜探索 AI 办公新纪元，赢丰厚大奖！</title>
            <link>https://www.infoq.cn/article/4b750c75f0261f2c491f48c75</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4b750c75f0261f2c491f48c75</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 02:00:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 技术, 商汤科技, 小浣熊, 征文大赛
<br>
<br>
总结: 我们正步入一个全新的工作时代，一个生成式 AI 技术飞速发展的时代。商汤科技的办公小浣熊以前所未有的智能和效率，重新定义了我们的办公方式。InfoQ 写作社区联合小浣熊家族举办“我的上班搭子之有小浣熊的一天”有奖征文大赛，邀请分享与小浣熊一起工作的日子。征文要求围绕小浣熊办公助手在各种办公场景下的使用体验展开，文章字数要求1000字以上。评选标准包括专家评审和点赞数量，奖项设置丰富，优秀作品有机会获得平台流量推荐。 </div>
                        <hr>
                    
                    <p>我们正步入一个全新的工作时代，一个生成式 AI 技术飞速发展的时代。智能助手不再只是科幻电影中的幻想，而是真实地融入到我们的日常工作中，成为我们不可或缺的伙伴。商汤科技的办公小浣熊正是这一变革的先锋，它以前所未有的智能和效率，重新定义了我们的办公方式。</p><p>在这个 AI 辅助工作的新纪元，我们渴望听到更多关于 AI 办公助手如何改变你工作日常的故事，如何让数据分析、趋势预测、数据可视化等办公任务变得更加轻松和高效......我们鼓励更多人拥抱 AI 办公助手，共同推动 AI 办公技术的进一步发展。</p><p>因此，InfoQ 写作社区联合小浣熊家族举办“我的上班搭子之有小浣熊的一天”有奖征文大赛，邀请你分享那些与小浣熊一起工作的日子，无论是日常工作的点滴，还是特殊项目的挑战，我们都期待听到你的声音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb890cf6bc6ff0e2a1ec3c0d0e6fdb50.jpeg" /></p><p></p><p></p><h2>活动主题：“我的上班搭子之有小浣熊的一天”有奖征文大赛</h2><p></p><p></p><p>小浣熊家族【办公小浣熊】体验直通车👉 <a href="https://raccoon.sensetime.com/">https://raccoon.sensetime.com/</a>"</p><p></p><h2>活动时间：2024/06/24-2024/07/22</h2><p></p><p>2024/06/24-2024/07/14 投稿2024/07/15-2024/07/19 专家评审2024/07/22 公布结果</p><p></p><h2>征文要求：</h2><p></p><p>文章需围绕商汤科技的小浣熊办公助手在各种办公场景下的使用体验展开，至少详细描述一个具体的办公场景，并展示商汤小浣熊办公助手在该场景下的应用效果和用户价值。场景方面以日常数据分析工作中的数据清洗、数据运算、趋势分析、预测性分析、比较分析、关联性分析、数据可视化等场景为最佳。文章建议兼顾【数据分析背景】【分析目标】【分析思路】【借助小浣熊得出的分析报告】四个模块的内容展现。内容要求真实、具体，包含图文反馈，如参赛作品在图文的基础上制作视频，则可作为加分项。图文内容需清晰展示商汤小浣熊办公助手的使用界面、操作流程以及带来的便利和效果。文章字数要求 1000 字以上，以便读者全方位了解您的使用体验和感受。文章必须为原创，不侵犯任何第三方的版权或其他合法权益，不得使用 AI 生成内容投稿，不得有广告引流/洗稿/凑字数等行为。一经发现，取消活动参与资格。</p><p></p><h2>评选标准：</h2><p></p><p>满足基础文章要求的文章将进入最终评选阶段。文章评审将根据专家评审得分和文章点赞数量得分加权计算。文章得分=专家评审得分*70%+点赞量*30%</p><p>专家评审评分标准：</p><p>内容真实性（40分）：文章需真实反映用户在不同办公场景下的使用体验。场景丰富性（30分）：文章需包含至少一个具体的办公场景，并详细展示商汤小浣熊办公助手在该场景下的应用效果和用户价值。内容创新性（20分）：文章可展现用户在使用过程中的创新方法和独特见解，为其他用户提供新的使用思路。图文质量（10分）：文章需包含高质量的图文内容，图片需清晰展示产品使用界面、操作流程以及带来的便利和效果。</p><p></p><h2>投稿方式：</h2><p></p><p>在 InfoQ 写作社区进行文章首发，在活动页面以“文章标题+文章链接”的形式提交作品参赛者请务必加入社群，以获取活动的最新动态和消息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb2fe46580f680b20a230fc6abf37a96.png" /></p><p></p><h2>奖项设置：</h2><p></p><p>一等奖 1名：600 元京东E卡+哈曼卡顿水晶3代音响+小浣熊定制抱枕二等奖 2名：500 元京东E卡+富士拍立得mini7+小浣熊定制抱枕三等奖 3名：400 元京东E卡+小米手环8+小浣熊定制抱枕优秀奖 30名：100 元京东E卡+OPPO Enco Air2 耳机参与奖：凡参与者皆可获得小浣熊定制帆布袋一个。</p><p></p><h2>Q&amp;A：</h2><p></p><p>Q.奖品和稿费什么时候发放？</p><p>A. 奖品和稿费会在 2024 年7月22日公布获奖结果后，依次发放。</p><p>Q. 优秀作品会获得平台的哪些流量推荐？</p><p>A. 我们将从评论区选出优质文章进行流量推荐。有机会获得：InfoQ 写作社区首页推荐。</p><p>Q. 已经发布的稿件可以删除吗？</p><p>A.请遵守参赛公约：已经发布的投稿，并领取奖励的不可删稿哦~</p><p>Q. 可以提交多篇作品参赛吗？</p><p>A. 每位参赛者提交的作品数量不限，但为保证更多选手的参与感，获奖作品每人仅限一篇。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FqO3OFYHDtwhNNILLBEj</id>
            <title>华为盘古 5.0 强势登场：参数跃升万亿级，理解能力突破至感应 level，团队亲述幕后黑科技！</title>
            <link>https://www.infoq.cn/article/FqO3OFYHDtwhNNILLBEj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FqO3OFYHDtwhNNILLBEj</guid>
            <pubDate></pubDate>
            <updated>Mon, 24 Jun 2024 01:47:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华为开发者大会, 盘古5.0, 多模态, 思维能力
<br>
<br>
总结: 华为在开发者大会上发布了盘古5.0，该版本在全系列、多模态、强思维三个方面进行了全新升级，推出了适配不同业务场景的多种参数规格模型。盘古5.0在多模态能力上有所提升，能够精准理解和生成物理世界，支持高分辨率图片和视频的理解和生成。在思维能力方面，盘古5.0结合思维链技术和策略搜索技术，提升了数学能力、复杂任务规划能力和工具调用能力。盘古5.0的多模态生成能力还可以为自动驾驶领域提供高质量的数据支持。华为云通过数据高效、参数高效和算力高效等方面的训练，使盘古5.0具备更多模态和更强思维能力。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>在6月21日的华为开发者大会上，华为云盘古大模型5.0重磅亮相。此次，盘古5.0在全系列、多模态、强思维三个方面全新升级，并推出了适配不同业务场景的多种参数规格模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd4b797fa9703214172e087f89b33e98.png" /></p><p></p><p>&nbsp;</p><p>比如，手机和PC上的智能应用，可以基于10亿级参数的模型，在端侧完成绝大部分任务；少数复杂任务可以通过端云协同，使用云上的百亿甚至千亿模型进行处理。盘古&nbsp;5.0&nbsp;还进一步推出了云上2300亿的稠密模型和2.6万亿的MOE大模型，能够帮助企业更好处理复杂场景以及跨领域多任务场景。</p><p>&nbsp;</p><p>除此之外，在现场，华为诺亚方舟实验室主任姚骏详细介绍了盘古5.0的重要训练环节，并透露了他们为使盘古5.0达到更多模态和更强思维能力所用到的一些“黑科技”，包括数据高效、参数高效和算力高效等方面。</p><p>&nbsp;</p><p>同时，华为云还分享了盘古大模型在自动驾驶、具身智能、媒体生产和应用、气象、钢铁、高铁、工业设计、建筑设计、中医药等领域的创新应用和落地实践。</p><p>&nbsp;</p><p></p><h2>盘古5.0三大创新升级</h2><p></p><p>据介绍，盘古5.0提供了全系列的大模型，其推出不同参数规格的模型，以适配不同的业务场景。</p><p>&nbsp;</p><p>其中，十亿级参数的Pangu&nbsp;E（Embeded）系列，有15亿、70亿两种参数规格，无需联网就可以运行小的大模型，是嵌入到端侧的大模型，可支撑手机、PC、车等端侧的智能应用；百亿级参数的Pangu&nbsp;P（Professional）系列，提供的参数在&nbsp;100&nbsp;亿到&nbsp;900&nbsp;亿之间，可以解决大部分&nbsp;AI&nbsp;的应用场景，拥有低时延、低成本的优势。适用于低时延、低成本的推理场景；</p><p>&nbsp;</p><p>千亿级参数的Pangu&nbsp;U（Ultra）系列，有1350亿、2300亿两种参数规格，适用于处理复杂任务，可以成为企业通用大模型的底座；万亿级参数的Pangu&nbsp;S（Super）系列超级大模型有2.6万亿参数，是处理跨领域多任务的超级大模型，能帮助企业更好的在全场景应用AI技术。</p><p>&nbsp;</p><p>在多模态能力上，盘古5.0在理解和生成做了提升。盘古5.0能够精准的理解和重构物理世界，能够支持在10K超高分辨率的图片和视频中准确理解微小的细节内容；在生成方面，其采用了业界首创的STCG（Spatio&nbsp;Temporal&nbsp;Controllable&nbsp;Generation，可控时空生成）技术，聚焦自动驾驶、工业制造、建筑等多个行业场景，可生成更加符合物理规律的多模态内容。</p><p>&nbsp;</p><p>理解方面，除文本、图片、视频外，盘古5.0还增加了雷达、红外、遥感等更多模态。现场，华为常务董事、华为云CEO张平安分别展示了盘古在这些模态层面的理解和识别能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b636e0d4e17458648edb9bff487b92d.jpeg" /></p><p></p><p>&nbsp;</p><p>首先是卫星遥感图像，盘古大模型能够准确的分析出区域农作物的生长状况和收成状况，可以用于农作物的产链预估和整体病虫害的监测。其次是红外影像，当可见光没法看清的时候，盘古大模型可以通过红外影像准确识别车辆和人的运行轨迹，来进行交通管理和灾难防范。最后是雷达影像，盘古大模型能通过可见光和雷达的影像综合来判断植被的覆盖情况，让生态部门对于自然保护地进行监测。</p><p>&nbsp;</p><p>思维能力上，盘古5.0将思维链技术与策略搜索技术深度结合，极大提升了数学能力、复杂任务规划能力以及工具调用能力。思维链帮助智能体（如机器人）更好地理解和预测环境变化，而"策略搜索"则是智能体用来适应这些变化并做出决策的过程。两者共同作用，使得智能体能够在复杂环境中进行有效的学习和决策。</p><p>&nbsp;</p><p>值得一提的是，盘古5.0的多模态生成能力，还可以为自动驾驶领域提供更高质量的数据支持。张平安表示，盘古5.0通过STC技术，可以大规模生成和实际场景相一致的驾驶视频数据。</p><p>&nbsp;</p><p>据介绍，其生成的视频不仅在视觉上逼真，更重要的是在车辆行为、环境互动等方面与现实情况保持高度同步。例如，车辆在不同摄像头视角间的平滑过渡，以及在不同天气和光照条件下行驶的自然表现，都显示了模型对空间和时间维度精准把握的能力。尤为特别的是，模型在生成雨天视频时，还能细腻地模拟出车辆尾灯因光线昏暗而开启的细节。</p><p>&nbsp;</p><p>通过盘古大模型生成的六摄像头视角视频，自动驾驶系统可以直接获取到全方位、高仿真度的训练素材。张平安表示，未来盘古的多模态生成还会支持更多的自动驾驶场景。</p><p>&nbsp;</p><p></p><h2>盘古5.0是如何炼成的？</h2><p></p><p>“盘古5.0如今具备的更多模态和更强思维能力，源于华为云AI算力平台对模型的高效使能训练，主要是数据高效、参数高效和算力高效三个方面。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/127a2651bef04151e5d31d2144ee2574.png" /></p><p></p><p>&nbsp;</p><p></p><h3>面向高阶能力的数据合成方法</h3><p></p><p>&nbsp;</p><p>据姚骏透露，华为云已经从盘古3.0时代的3T&nbsp;Tokens的数据，演进到了盘古5.0的10T&nbsp;Tokens的高质量数据，其中合成数据占比超过了30%。其目的是提升数据的利用率，并且用更优质的数据来激活模型中更多的能力。</p><p>&nbsp;</p><p>“未来合成数据会在更大规模的模型训练中占有一席之地，来弥补高质量自然数据增长不足的空缺。”姚骏认为，现在业界大模型训练数据的规模已经从万亿级tokens迈入十万亿tokens，到这个量级以后，公开的高质量数据的增长就难以跟上模型的体量增长速度了。</p><p>&nbsp;</p><p>据介绍，华为云探索了优质的、面向高阶能力的数据合成方法。简单来说，就是以弱模型辅助强模型的weak2strong方法，采用迭代式的合成高质量数据，保证其有不弱于真实数据的完整性、相关性和知识性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/128e08e68b66b616bc895b6a80fef2f8.png" /></p><p></p><p>&nbsp;</p><p>从姚骏展示的能力图中可以看到，合成数据的质量从各个维度都略强于真实数据，在质量上对真实数据形成了一个包络。并且，weak2strong技术可以进一步加强合成数据中特定的数据，如自然数据中偏少的长序列、复杂知识推理等方面，并通过这些数据来加强模型的特定能力。</p><p>&nbsp;</p><p></p><h3>新的π架构</h3><p></p><p>&nbsp;</p><p>盘古5.0也演进了模型架构，提出了基于Transformer架构的新型大语言模型架构盘古π。</p><p>&nbsp;</p><p>原始的Transformer&nbsp;架构和其它深度模型一样，存在一定的特征坍塌问题。华为云通过理论分析发现，Transformer中的自注意力模块（也就是Attention模块）会进一步激化数据的特征消失。对此，业界通过为原始的Transformer增加一条残差连接，来略微缓解特征坍塌问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc33068b0561b7f811667b28a2de34fa.png" /></p><p></p><p>&nbsp;</p><p>在π的新架构中，华为云进一步提出增广残差连接，通过引入非线性的额外残差，更进一步加大来自不同Token的特征，使数据的特征的多样性得以在深度的Transformer中得到维持，进而大幅提升模型的精度。</p><p>&nbsp;</p><p>另外，Transformer包含FFN和自注意力模块两个关键模块，华为自研的昇腾芯片更擅长于处理Transformer中的FFN模块，而对自注意力模块的效率不高。在新的π架构中，其改造了模型中FFN模块中的激活函数，用一种新的级数激活函方式来代替。这种新方式不仅增加了模型的非线性度和FFN的计算量，还可以在精度不变的情况下减少自注意力模块的大小，使得模型在昇腾芯片推理速度也由此提升了25%。</p><p></p><p></p><h3>扩展多模态能力的关键技术</h3><p></p><p>一直以来，多个模态的高效对齐是训练多模态大模型的一大挑战。其中，视觉编码器是多模态大模型处理输入的第一步，用于将不同类别、大小的图像输入到同一个表征空间，相当于语言模型的Tokenizer&nbsp;。由于领域的不同，传统处理图像，视频，文本和图表时，需要用各自的独立的编码器各自接入多模态大模型，这造成了模型容量浪费和计算冗余。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/05bd6768ebd43991d17a4d786b7995eb.png" /></p><p></p><p>&nbsp;</p><p>为扩展多模态能力，盘古5.0采用了两个关键技术。第一个是统一的视觉编码器，在盘古5.0中，华为将不同的编码器能力蒸馏到一个统一视觉编码器中，可以大大提升编码效率。和同参数量业界SOTA模型相比，由于利用了不同领域之间内的共通知识，编码器在自然图像能力基本持平，文档理解能力上有显著提升。这种方案现在也成为了业界的主流编码范式。</p><p>&nbsp;</p><p>另一个关键技术是动态分辨率。人看世界有不同的分辨率，但模型的输入一般是固定的，很难兼顾。华为提出了尺度泛化的训练范式，首先使用低分辨率图片和简单任务训练基础感知能力，然后使用中高分辨率训练OCR和图表理解等细粒度感知能力，第三阶段扩展到更高的分辨率和更多的任务类型，最后重点突破模型的高阶推理能力。</p><p>&nbsp;</p><p>姚骏表示，这种动态递增的方式帮助盘古5.0在动态分辨率的表征上超过业界同等模型，并有效提升了模型在下游多模态任务的能力。</p><p>&nbsp;</p><p></p><h3>超&nbsp;1&nbsp;0倍参数量加成的强思维方法</h3><p></p><p>当前在单步任务和文本记忆类任务，如知识问答和考试，大模型已经展现出超过人类的卓越表现。而在多步推理和复杂任务的处理上还没有达到人类的平均水平，如代码生成、数学运算、逻辑推理等。前一种能力叫做记忆型能力，适合于大模型用一步的快速思考进行回答；后一种是复杂推理，模型需要像人一样，在这类问题上把快思考变成慢思考，一步一步的分解和完成对复杂问题的处理。</p><p>&nbsp;</p><p>从这点出发，华为云提出基于多步生成和策略搜索的MindStar方法。该方法首先把复杂推理任务分解成多个子问题，每个子问题都会生成多个候选方案，通过搜索和过程反馈的奖励模型，来选择最优多步回答的路径。这样既兼顾了人类一步一步思考的形式，也兼顾了机器更擅长的策略搜索的形式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15ca57c4a6a87608e1cb9bed3b47a00b.png" /></p><p></p><p>&nbsp;</p><p>据姚骏介绍，在华为自建的难例评测集中，MindStar方法使模型的平均能力提升了30分，使用MindStar的百亿模型达到业界主流千亿模型的推理能力，相当于使用慢思考能带来10倍以上的参数量的加成。</p><p>&nbsp;</p><p>“把MindStar这类强思维方法运用到更大尺度的模型上，就能逐步在复杂推理上也接近人和超越人的能力。”姚骏表示。</p><p>&nbsp;</p><p></p><h2>夸父机器人亮相展示</h2><p></p><p>&nbsp;</p><p>会上，华为云推出了盘古具身智能大模型，搭载盘古能力的人形机器人“夸父”也同步亮相。盘古大模型能够让机器人完成10步以上的复杂任务规划，并且在任务执行中实现多场景泛化和多任务处理。同时，盘古大模型还能生成机器人需要的训练视频，让机器人更快地学习各种复杂场景。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b3b87229d80daad030aff1c264608a3.jpeg" /></p><p></p><p>&nbsp;</p><p>现场，夸父人形机器人通过识别物品、问答互动、击掌、递水等互动演示，直观展示了基于盘古大模型的能力成果。据悉，通过模仿学习策略，华为云与乐聚公司显著提升了人形机器人的双臂操作能力，实现了软硬件层面的协同优化，不仅增强了机器人综合性能，还克服了小样本数据训练的局限性，推动了泛化操作能力的边界。</p><p>&nbsp;</p><p>“正如大家所期望的，让AI机器人帮助我们去洗衣、做饭、扫地，让我们有更多的时间去看书、写诗、作画。”张平安表示，除了人形机器人，盘古具身智能大模型还可以赋能多种形态的工业机器人和服务机器人，让它们帮助人类去从事危险和繁重的工作。</p><p>&nbsp;</p><p></p><h2>盘古媒体大模型推出</h2><p></p><p>华为云推出了盘古媒体大模型，通过在语音生成、视频生成和AI翻译三方面的技术创新，重塑了内容生产和应用的新模式。</p><p>&nbsp;</p><p>通过盘古，可以将实拍视频转换为不同风格的高清动漫。在现场演示的生成视频中，演员的舞蹈、武打等大运动轨迹能保持一致视觉效果，角色的面貌特征也保持前后一致。</p><p>&nbsp;</p><p>在语音生成方面，盘古大模型通过AI原声译制与视频生成能力，实现了将原片译制成不同语言的视频，并保留原始角色的音色、情感和语气。更为重要的是，盘古还能同步生成新的口型，确保不同语言对应的口型一致，使得跨语言沟通更加自然流畅。</p><p>&nbsp;</p><p>此外，在AI翻译方面，华为云盘古大模型也对云会议系统进行了升级。通过基于大模型的语音复刻、AI文字翻译以及TTS技术，实现了语音的同声传译，这使得不同国家的人在云视频会议中可以畅快地使用母语交流。结合数字人技术，在不方便开摄像头时，用户还可以通过数字人参会，并通过口型驱动实现数字人以各种语言说话都能精准匹配口型，如同本人说话一般。</p><p>&nbsp;</p><p></p><h2>结语</h2><p></p><p>过去一年中，盘古大模型已在30多个行业、400多个场景中落地。现场，张平安还介绍了该模型在政务、金融、制造、医药研发、煤矿、钢铁、铁路、工业设计、建筑设计、气象等领域发挥的能力。</p><p>&nbsp;</p><p>据悉，目前盘古大模型已经在宝武钢铁集团1880热轧生产线上线，将时序数据、表格数据、工艺参数、行业机理等token化，显著降低了热轧生产线调优时间，预测精度提高5%以上，钢板成材率提升0.5%，预计每年可以多产钢板2万余吨，年收益达9000余万元。华为云还与宝武钢铁集团在炼钢、表检、新钢种研发、排程优化等多个领域开展盘古大模型的应用研究。</p><p>&nbsp;</p><p>此外，张平安宣布，盘古气象大模型再升级，推进至更高难度的公里级区域预报，实现了从全球25公里模型向1公里、3公里、5公里区域预报精度的跨越，包含气温、降雨、风速等气象要素。现在盘古气象大模型的应用范围已经延伸至行业服务，扩展到污染物预测、农业生产指导等多个领域。</p><p>&nbsp;</p><p>特别是在环境治理方面，华为云与天融环境公司合作推出“环境大模型”，将污染六项的预测准确度全面提升10％以上，并且将预测窗口从3天提前至7天，为环保部门提供了更长的预警时间，有助于更加高效地进行污染源的定位与治理。</p><p></p><p>除了盘古大模型的升级，华为云还对昇腾AI云服务进行了优化。昇腾AI云服务可实现万亿参数模型训练&nbsp;40天无中断；平均集群故障恢复时间10分钟，同时能将大模型的资源开通时间从月级缩短到天级。目前昇腾AI云服务已全面适配行业主流的100多个大模型，以云服务的方式协助开发、训练、托管和应用模型。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OVogOUR5HtKoou9x8ugC</id>
            <title>DB 大咖对话 | 数据要素与人工智能对我国数据库技术和产业的影响</title>
            <link>https://www.infoq.cn/article/OVogOUR5HtKoou9x8ugC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OVogOUR5HtKoou9x8ugC</guid>
            <pubDate></pubDate>
            <updated>Fri, 21 Jun 2024 11:42:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库, 2024年, 大会, 技术生态
<br>
<br>
总结: 2024年，我国数据库行业正处于蓬勃发展期和关键应用期，人工智能和数据要素市场化建设的浪潮下，将举办“2024可信数据库发展大会”，共设1个主论坛和6个分论坛，涵盖金融、电信、能源、政务等行业应用和技术生态。各行业关注数据库功能迁移、性能、稳定性、标准化和技术服务支持等问题。未来数据库领域趋势包括云原生能力发展、智能化趋势、软硬件协同优化和新兴技术方向的发展。 </div>
                        <hr>
                    
                    <p>2024 年，我国数据库正处于蓬勃发展期和关键应用期，在人工智能迅猛发展和数据要素市场化建设的浪潮下，为进一步推动全球数据库产业进步，“2024 可信数据库发展大会”将于 2024 年 7 月 16-17 日，在北京朝阳悠唐皇冠假日酒店隆重召开。</p><p></p><p>本次大会共设置 1 个主论坛和 6 个分论坛，具体包括金融、电信、能源 &amp; 政务三大行业应用分论坛，以及人工智能与数据库融合、搜索与分析型数据库 &amp; 多模数据库、数据库生态与国际化三大技术生态分论坛。如果你也在关注数据库的当前现状与发展趋势，“2024 可信数据库发展大会”你一定不能错过！报名通道已开启，欢迎提前扫码抢位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e1ec35e74fdffd4b8c1d0c8fdfcd842.webp" /></p><p></p><p>在大会召开前夕，我们特地邀请了部分国产数据库的主要负责人和创始人，分别是涛思数据创始人 &amp; CEO 陶建辉、北京自然原数科技有限公司创始人 &amp; 首席科学家江晶、华为云数据库产品解决方案总监窦德明、阿里云数据库 AnalyticDB PostgreSQL &amp; 生态工具产品部负责人周文超、金篆信科 GoldenDB 高级架构师陆天炜以及人大金仓解决方案总监李世辉，他们围绕云原生数据库、企业级关系型数据库、工业大数据管理、人工智能与数据库等议题，分享了各自的见解。</p><p></p><p></p><p></p><p>本期圆桌对话内容整理如下，供读者参考回顾。</p><p></p><p></p><h4>Q1：在金融、电信等企业级核心系统中，关系型数据库的应用现状是怎样的？</h4><p></p><p></p><p>江晶：根据整体的替换情况来看，金融行业的替换速度相对领先，这也有赖于监管政策的持续推动。在全国上千家金融机构中，大家的规划、目标都非常明确，执行步骤也很清晰。</p><p></p><p>我们观察到金融、电信等企业在选型或者使用过程中的关注点是：第一，关注数据库功能在迁移后能否适配或完全兼容；第二，关注性能能否满足业务需求；第三，关注业务稳定性，即能否持续保障高可用和高可靠的能力，包括多活、灾备等等；第四，关注产品的标准化，即是否易于上手使用；最后，关注技术服务的支持力度，因为任何数据库产品在使用过程中都会遇到技术问题，厂商能否及时跟进，解决的效率与效果如何，也是用户关心的问题。</p><p></p><p></p><h4>Q2：您观察到工业大数据领域出现了哪些困境？也请您分享下您在开源领域深耕的心得体会</h4><p></p><p></p><p>陶建辉：对于制造业来说，搭建一个大数据平台是十分复杂的，它不仅需要一个时序数据库，还需要 Flink 做流计算，需要数仓做批处理、做分析...... 由于其数字化程度相对较低，IT 能力也相对较弱，维护如此复杂的系统对于他们而言也是巨大的挑战。</p><p></p><p>为了减轻制造业搭建大数据平台的难度，除提供时序数据库，我们还开发了缓存、消息队列、流式计算等等，提供了一个简易的时序大数据平台。但我们仍然难达到行业的要求，因为在工业大数据管理领域，很多人不懂什么是时序数据库，他们希望得到完整的解决方案拿来即用，而我们是一家独立的时序数据库公司，提供不了最终的解决方案。因为我们希望把可视化报表、数据采集等应用层交给第三方公司做，我们只聚焦数据层面。我觉得这个方向是对的，一旦什么东西都做，定制化程度就会变得很深。</p><p></p><p>谈到开源的价值，这里我想分享开源带给我们的流量：由于 TDengine 的安装量很大，我们销售的线索几乎都来自公司官网，包括发电、烟草、石油等等。截止目前，2024 年通过官网联系产生的有效线索已经超过 900 个。</p><p></p><p></p><h4>Q3：在数据库替换过程中，企业无疑希望数据库及应用系统的平滑迁移，华为云数据库在这方面有哪些积累和经验？</h4><p></p><p></p><p>窦德明：我认为数据库的迁移不是简单 1:1 的替换，而是企业 IT 基础设施更新换代的过程，需要多个角色一起协作共同完成。在迁移过程中，主要会面临应用改造周期长、迁移效率低、数据不一致等各种挑战。为了应对这些挑战，华为云数据库团队开发了 GaussDB 配套工具 UGO，它能够自动化地将源数据库中的 DDL、DML 和 DCL 转换为 GaussDB 支持的语法，通过数据评估和对象迁移功能，提前识别潜在的改造工作，提高转化率，最大化降低迁移成本。</p><p></p><p>此外，华为云还提供了数据复制软件 DRS，利用 CDC 技术实现数据的实时同步，确保数据的零丢失和迁移的时效性。在业务验证方面，DRS 提供一个高级特性流量录制回放，可以捕获源数据库应用下发的的所有 SQL，并在 GaussDB 中进行回放，以评估迁移后的 SQL 性能，必要的情况下再进行调优。</p><p></p><p>UGO+DRS 一站式迁移解决方案，涵盖了迁移评估、SQL 自动转换、SQL 审核、数据在线迁移、数据智能比对、SQL 录制回放，以及数据修复能力，最大程度保证迁移的平滑。同时，在迁移之前，我们会进行详尽的调研和可行性评估，以提前识别迁移风险。迁移完成后，客户的参与同样重要，需要应用开发人员基于应用的测试用例来自动化验证割接的准确性，确保全流程没有问题。</p><p></p><p></p><h4>Q4：作为国家智库和行业平台的大数据领域负责人，您觉得数据库领域未来会有朝着哪些趋势发展？</h4><p></p><p></p><p>姜春宇：第一，我认为云原生能力将继续发展，云厂商的数据库将提供更极致的弹性和性能，这是数据库技术发展的一个持续趋势；第二，智能化趋势日益显著，AI 大模型的崛起对传统的 IT 架构、数据架构和业务架构产生了深远的影响，面向 AI 的数据库将在未来扮演重要角色。例如，向量数据库和多模态数据管理的兴起以及交互方式的变化，都是智能化趋势的体现，除此之外，以 Text2SQL 为代表的自然语言交互管理数据库也是目前人工智能与数据库落地应用的重要方向；第三，软硬件协同优化将成为数据库发展的一个重要方向，随着数据库性能和稳定性达到一定瓶颈，单纯的软件优化可能不再足够，需要与新兴硬件结合进行更深层次的优化，以应对单靠软件难以解决的问题；此外，还有一些新兴的技术方向值得关注，如时序数据库、时空数据库以及车联网和自动驾驶等极端场景下对数据时延的严格要求。</p><p></p><p></p><h4>Q5：在云计算、大数据和人工智能等技术的推动下，大家认为数据库技术会迎来怎样的发展格局？</h4><p></p><p></p><p>江晶：数据库领域正在紧跟大模型技术，尤其在人工智能对数据库本身的研究和研发方面，我认为可以快速落地的几个方面包括：自动实时动态调整数据库参数、人机交互方式的优化、SQL 写法和执行计划的内部调整，以及查询优化器的智能化构建。这些方向将减少对时效性和人为要求的依赖，提高数据库的性能和用户体验。</p><p></p><p>陶建辉：从时序数据库的角度来看，我认为大模型与数据库的结合主要体现在应用层的优化，尤其是在时序数据的预测和异常检测方面。尽管目前大模型在这些领域的应用效果尚未达到惊艳的水平，但我们仍然在积极探索利用大模型来提升预测准确性和异常检测的效率。</p><p></p><p>窦德明：我认为 AI 技术在数据库领域的应用不仅仅局限于内核侧，还可以用来提高迁移效率和运维效率。例如 SQL2SQL，通过 AI 技术将一种数据库的 SQL 自动转换为另一种数据库的 SQL，以及利用 AI 技术快速定位、定界乃至修复数据库问题，当然还有很多其他结合点，比如 AI 异构硬件加速等。</p><p></p><p></p><h4>Q6：国产数据库若想赶超国外领先产品，应该在哪些层面拉开竞争优势？</h4><p></p><p></p><p>李世辉：随着数字化转型的深入，新的数据模型和数据类型不断涌现，为国产数据库提供了巨大的发展机遇。在这些新兴领域，国内外产品在技术积累上并没有显著的差距，我们有机会通过创新和快速适应市场变化来获得领先地位。首先，国产数据库需要关注海量数据处理和多模态融合计算等新兴产品的发展，这些领域目前尚未出现能一统天下的产品；其次，数据库的架构设计至关重要，国产数据库应该充分利用当前软硬件技术的快速发展，重新构建、优化数据库架构，以适应新的部署环境；此外，国产数据库还应该加强与本土市场的结合，深入了解国内用户的需求和使用习惯，提供更加符合本土市场特点的产品和服务。</p><p></p><p>姜春宇：首先，政策的红利是一个不可忽视的因素，它为国内数据库厂商提供了市场空间和发展机遇；其次，国内有丰富的业务场景，如互联网、金融、电信和电力等，为数据库厂商提供了大量的实践机会。这些场景的业务量大，复杂度高，对数据库的性能、稳定性和可靠性提出了更高的要求。这样的考验实际上对国内数据库厂商的产品能力和服务能力进行了有效的锻炼和提升；</p><p></p><p>此外，国内软件行业的快速发展得益于工程师的红利。过去几十年，中国培养了大量优秀的软件工程师，这些人才在开源社区的推动下，能够快速学习并掌握先进的架构和编码技能，形成了强大的工程技术能力；</p><p></p><p>最后，国内数据库企业的崛起还得益于本地化优势。与国际厂商相比，国内厂商更接近本土市场，能够更快地响应客户需求，提供定制化的解决方案和原厂支持服务；服务体系的构建也是国内数据库厂商成长的关键。随着产品体系的不断成熟，国内厂商也在逐步完善服务体系，包括实施交付、运维运营、人才培养等。这些服务不仅提高了产品的可用性和易用性，也为行业输送了大量懂得使用和维护数据库的人才。</p><p></p><p></p><h4>&nbsp;Q7： 如何推动国产数据库落地和市场接受度，人大金仓有哪些经验可以分享？</h4><p></p><p></p><p>李世辉：首先，针对客户对国产数据库的疑虑，我们从客户的痛点出发，总结出客户不愿用、不会用和不敢用的三个主要问题，构建了全流程的迁移解决方案，包括系统适配到测试验证，推出了"三低一平"的解决方案，即低成本、低难度、低风险的平滑替代，帮助客户减少迁移过程中的顾虑。</p><p></p><p>其次，人大金仓提供了基于 Oracle、SQLServer、MySQL 等异构数据库的原生兼容能力，以及一体化的智能迁移方案，包括数据库对象迁移、数据迁移和数据一致性比对等；对于不敢用的问题，人大金仓提供了数据在线比对方案和双轨并行方案，确保客户在迁移过程中的业务连续性，减少风险。</p><p></p><p>接着，人大金仓构建了一套可以让产品快速迭代的体系，简单来说有三个部分：第一部分是高内聚、低耦合的产品架构；第二部分是我们构建了一个专业化、标准化的研发体系，以解决大规模团队协同开发的效率的问题；第三部分是我们打造了一个产品测试的自动化工厂，保证我们的产品的质量能够保持稳定。正是有了这个体系，让我们在面对客户需求的时候能够快速响应，更容易获得客户的信任。</p><p></p><p>最后，在项目实践上，我们与行业 ISV 进行核心产品的适配，通过与客户核心系统的验证，提高客户对产品的信任度，从而降低项目替代的风险。</p><p></p><p></p><h4>Q8：我国云原生数据库是否已经实现了“弯道超车”？未来云原生数据库有哪些技术发展方向？</h4><p></p><p></p><p>周文超：无疑，云原生数据库技术的发展为中国数据库行业提供了实现"弯道超车"的新机遇。云计算的兴起改变了传统软件系统的基本逻辑，尤其是在资源的池化、解耦以及弹性、高可用性、容器化部署和智能化运维等方面。这些核心能力让云原生数据库在业务高峰期能够支撑峰值负载，同时在低峰期避免资源浪费。</p><p></p><p>展望未来，我认为云原生数据库的技术发展方向主要包括以下几个方面：一是云原生化，进一步解耦资源，实现更高效的弹性能力。例如，阿里云的 PolarDB 产品实现了计算、内存和存储的三层解耦，可以让数据库独立地进行资源的弹升和弹降，降低资源成本；二是平台化，软件和硬件的协同设计，利用硬件如 RDMA、FPGA 等提升性能和效率。例如，通过在存储设备上使用 FPGA，可以在数据写入时进行透明的压缩和解压，优化存储资源的使用；三是一体化，满足客户对多模态数据融合的需求，例如通过 Zero-ETL 或 HTAP 技术，减少数据在不同处理需求间的转换成本，提高效率；四是智能化，结合 AI 技术，提升数据库的自动化服务能力。例如，利用自然语言处理技术将自然语言转换为 SQL 语句，使数据库能够更好地服务于 AI 应用，同时利用 AI 技术优化数据库的运维和管理。</p><p></p><p></p><h4>Q9：GoldenDB 在金融、电信等行业的核心系统应用情况表现如何？</h4><p></p><p></p><p>陆天炜：GoIdenDB 作为金融核心业务的新型数据库解决方案，在金融市场的应用主要聚焦于传统银行业务的替换，如存款、贷款、核算、客户产品计价和总账等关键业务；在证券行业，GoIdenDB 的应用场景扩展到了实时交易之外的领域，如每日的数据上载，上场、复杂查询、营销系统等，GoIdenDB 能够提供与内存数据库相接近的性能，同时保证数据的持久化和一致性。自 2014 年进入金融行业以来，GoldenDB 已经在多家银行实现了核心系统的数据库下移，成为首家支撑大型商业银行核心系统的国产数据库产品。</p><p></p><p></p><h4>Q10：人工智能与数据库融合发展最先有可能在哪些方向规模化落地？</h4><p></p><p></p><p>李世辉：我认为规模化发展取决于市场价值，而市场价值源于需求。随着人工智能技术的快速发展，数据库与 AI 的结合成为推动数据库技术发展的一个重要方向。这种结合主要体现在两个方面：AI FOR DB 和 DB FOR AI。</p><p></p><p>DB FOR AI，即数据库服务于 AI，是指数据库技术为 AI 应用提供支持，例如通过数据库内置的 AI 计算能力来优化数据处理和分析。目前，许多主流数据库已经具备了 AI 计算能力，这表明 DB FOR AI 的规模化落地可能会更快一些。国外一些数据库厂商甚至已经将 AI 技术与硬件如 GPU、FPGA 等算力结合起来，构建了强大的支撑平台。随着人工智能需求的增长，以及云平台大规模基础设施的部署能力，DB FOR AI 的条件已经相当成熟，预计在业界的落地将会比较迅速。</p><p></p><p>而 AI FOR DB，即 AI 技术提升数据库内部能力，虽然在数据库的多个环节中都有应用，但相对来说，其发展和应用可能会慢一些。这是因为传统的数据库技术已经非常成熟，经过几十年的发展和优化，AI 技术要想在这些方面取得突破，还需要时间来逐步发展和完善。尽管如此，AI 在数据库的智能运维等方面已经开始发挥作用，许多小的结合点已经展现出 AI 技术的价值。</p><p></p><p>周文超：一方面，AI FOR DB 在学术界和产业界早已有大量的研究，比如如何使用 AI 来创建智能化的索引，如何优化索引的选择、提高表的 Cardinality 和大小估计的准确性等。最近，随着大语言模型出现，使得 AI FOR DB 在识别和理解用户意图方面进步显著。</p><p></p><p>另一方面，DB FOR AI 强调了数据库技术在支持人工智能应用，尤其是在推理阶段的重要性。与训练阶段相比，推理阶段更依赖于高效的数据存取和处理能力，结合异构计算硬件（如 GPU、FPGA），数据库在 AI 推理方面能实现更高效、成本更低的解决方案，为数据库技术在未来的发展开辟了新的可能性。</p><p></p><p>陆天炜：在 AI FOR DB 中，DB 为主体，AI 作为增强。DB 在设计之初就要求准确和稳定，AI 结合人类经验和机器学习来确保这一目标。在 DB FOR AI 方面，AI 作为目标，DB 作为实现工具，尤其在机器训练中，数据标注的存储，训练的语言，DB 都可以发挥作用。在 GoIdenDB 中，AI 不仅用于智能运维，还用于产品测试阶段，通过根据生成测试 SQL 集，来保障优化数据库的质量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Bcuu8L8MimNVsk4Jhbxi</id>
            <title>C端太卷，转战企业级应用，大模型与业务场景之间的差距到底有多大？</title>
            <link>https://www.infoq.cn/article/Bcuu8L8MimNVsk4Jhbxi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Bcuu8L8MimNVsk4Jhbxi</guid>
            <pubDate></pubDate>
            <updated>Fri, 21 Jun 2024 10:41:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: B 端, 大模型市场, 微盟, 企业级 AI
<br>
<br>
总结: 文中介绍了微盟在大模型市场中探索企业级AI服务的实践经验，强调了企业级AI与个人版AI的区别和复杂性，以及微盟在开发设计AI应用方面的技术优势。微盟通过与国内大模型平台合作，不断迭代技术能力与应用场景，拓展超50个真实商业应用场景。微盟致力于将AI深入融入客户的业务流程，帮助客户创建新的业务流程，并通过AI Agent帮助客户开发私有模型。同时，文中也提到了微盟在面对客户预期与实际落地效果差距时所面临的挑战，以及微盟团队为弥合差异所做的努力。 </div>
                        <hr>
                    
                    <p>To B &nbsp;or not To B，放到今天的大模型市场，依然是个可以无限议论的话题。</p><p></p><p>“to B 端的 AI 为企业提供的是更全局性的对生产力和生产效率的认知。由于个人对 AI 的拥抱程度千差万别，to C 端的 AI 工具往往难以满足企业对全局业务提效的需求。比如，同样是 100 个设计师或文案，可能只有 10% 会用 C 端产品积极求变，而企业级 AI 可以让全员 100% 使用 AI 提效。”在日前的一场媒体交流会上，微盟集团 AI 负责人裘皓萍对外阐释大模型 to B 端应用的价值。</p><p></p><p>从微盟自身的实践来看，自 2023 年 5 月发布以来，微盟大模型应用产品 WAI 通过开源自研以及与国内大模型平台展开合作，不断迭代其技术能力与应用场景。在 SaaS 场景下，截至 2024 年 5 月，微盟 WAI 已拓展超 50 个真实商业应用场景，覆盖包括服饰饰品、美妆护肤、食品酒水、生鲜水果、日用百货等行业。</p><p>而在营销方面，WAI 提供包括广告物料制作、广告精准投放、直播数字人等多维度 AI 技术支持，其智能创作能力已覆盖全域营销场景。</p><p></p><p>如今，微盟正在探索“WAI 企业版”，开始发力企业级 AI 服务。在微盟看来，经过这一年多的落地实践，依托于成熟的 SaaS 系统，AI 技术在企业级服务中具备很大的发展空间。</p><p></p><h2>把场景拆散揉碎，做企业级 AI</h2><p></p><p>今年 5 月，微盟宣布已与国内十余家大模型厂商达成合作。微盟 WAI 已全面接入包括腾讯混元、百度文心一言、智谱 AI、商汤日日新、月之暗面 Kimi、阿里通义千问、科大讯飞星火在内的主流大模型平台。</p><p>事实上，相比 to C 端产品，企业级 AI 面临的场景和解决的问题会更复杂。</p><p></p><p>微盟 WAI 技术负责人左江华在受访时指出，“在企业级 AI 场景中，从文生文到文生图往往涉及到多个大模型的联动。因此，企业级 AI 是把各种场景拆散揉碎后，基于不同细分场景用 AI 去实现提效。相比 C 端产品，企业级 AI 最大的区别在于要搭建 SOP 做流程。”</p><p></p><p>具体而言，个人版 AI 通常提供的是基础能力，依赖于预训练模型来完成任务。例如，以 GPT 为例，个人版 AI 主要用于与用户对话，并根据上下文生成回应。如果用户需要生成一张图像，个人版 AI 可能会通过不同的模型联动来完成这一任务。尽管这些功能在一定程度上可以满足个人用户的需求，但在复杂的商业场景中，单一的模型和简单的联动往往难以实现理想的效果。</p><p></p><p>而为企业用户开发设计的AI应用不仅仅依赖于一个模型或一种 AI 技术，例如，在设计一张商品海报时，为企业用户开发设计的AI应用涉及多个步骤和多种 AI 技术的结合：</p><p>商品分类识别：识别用户上传的商品分类。图像位置检测：确定图片在画面中的位置。自动抠图：自动将商品图像从背景中抠出。提示词生成：利用商品标题和分类信息，由语言模型补充生成提示词。风格适配：根据特定场景（如母亲节、大促销等），通过设计师经验和行业经验，将彩带、礼盒等元素融入海报中。整体优化：确保图片的整体风格、内容和尺寸符合商城海报的要求。</p><p></p><p>左江华强调，除了流程的精细化，为企业用户开发设计的AI应用的优势还在于技术的不断升级。一方面，模型能力在提升，大模型的参数量会不断增加，模型对提示词的理解能力也在这个过程里不断增强。另一方面，引入新的技术方案，比如使用形状控制网络、风格背景控制网络和光影控制网络等多种控制网络，综合解决图像一致性、位置和结构等问题，不断提升生成图片的质量和效果。</p><p></p><h2>抵达客户场景不止“一公里”</h2><p></p><p>据介绍，过去一年微盟 WAI 的迭代工作里，prompt engineering（提示工程）只是基础工作之一。如上文所述，微盟还进行了大量与中间层相关的工作。</p><p></p><p>“如果永远停留在 prompt engineering，可能就没有很好的前途的。”左江华表示。</p><p></p><p>裘皓萍进一步指出，最初的 3-4 个月，团队确实集中精力于 prompt engineering，但随着 WAI 产品的内测和上线，在被用户在部分场景“啪啪打脸”后，他们便迅速修正了策略和方向。</p><p></p><p>“如果在去年我们判断还差最后一公里用 Prompt engineering 就能解决问题，那我觉得在今年看可能差了 10～20 公里。”裘皓萍表示，Prompt engineering 的作用在于将通用大模型输送给客户，但这种方式较为粗暴，且作用有限。微盟在过去一年解决的主要问题是如何让大模型及其配套设施真正应用到客户的实际场景中。</p><p></p><p>作为系统服务商和应用开发商，微盟如今寄予“WAI”的定位是博采众长，通过打通三方系统，整合多方大模型，让 AI 深入融入客户的业务流程，甚至帮助客户创建新的业务流程。</p><p></p><p>此外，通过 AI Agent，WAI 还能进一步帮助客户可以开发私有模型，沉淀自己独有的知识和风格需求。</p><p></p><h2>如何应对高预期与现实的差距</h2><p></p><p>不过，WAI 在推向客户的过程中也的确存在不少挑战。裘皓萍坦言，当前大环境不佳，客户对预算的把控非常严格。如果是五六年前的市场环境，AI 商业化所面临的挑战可能不会像今天如此艰巨。</p><p>如今，客户对价值的要求和投入产出比的精打细算成为首要目标，尽管如此，裘皓萍亦认为，未来十年或许会是最佳的时机。</p><p></p><p>除了大环境的影响，裘皓萍提到的另一大挑战在于客户对 AI 大模型的预期和实际落地效果之间存在差距，而微盟要做的就是不断弥合当中的差异。</p><p></p><p>由于客户在与大模型互动时，很多时候不知道该如何准确表达自己的需求，导致大模型生成的结果不符合预期。为了应对这个问题，微盟 AI 团队花费了大量时间去开发辅助工具和模板，让客户可以更直观地传达他们的需求。例如，通过预设的节气、节日、风格、行业等模板，客户可以轻松选择适合的样式，从而生成符合要求的内容。</p><p></p><p>此外，一些专业群体比如设计师需要用详细的 Prompt 来指导大模型生成特定风格的内容。但客户往往不知道如何写出符合专业标准的 Prompt，那么微盟 WAI 就让 AI 帮助客户生成专业的 Prompt，客户只需简单描述，例如“少女站在夕阳下，旁边是棵棕榈树”，AI 就会自动将其转化为专业的 Prompt，包含广角参数、画风参考等细节。</p><p></p><p>裘皓萍进一步举例道，在帮助客户使用大模型的过程中，微盟采用了许多小巧思。例如，原先是一次生成一张图，现在改为一次生成多张图，这样客户可以从多种风格中选择最合适的一张。这样既保留了大模型的创造力，也满足了客户的多样化需求。</p><p></p><p>裘皓萍指出，弥合客户高预期与实际落地效果之间差距的过程并非一蹴而就，这需要 AI 自身的发展以及微盟在解决最后一公里过程中不断打磨产品和技术的成熟度。她将这一情况比作十多年前微信刚出现时的情形，在微信生态还没有丰富起来之前，没有人预料到微信会以今天的方式改变企业运营和商业模式。</p><p></p><p>因此，微盟认为，真正实现 AI 商业化和让企业全面拥抱 AI 还需要时间和耐心。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Be1CadEf9iAIYyWLvNqi</id>
            <title>月之暗面被曝进军美国，产品、人才筹备中！阿里腾讯捧出的30亿美元独角兽终于要出海了</title>
            <link>https://www.infoq.cn/article/Be1CadEf9iAIYyWLvNqi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Be1CadEf9iAIYyWLvNqi</guid>
            <pubDate></pubDate>
            <updated>Fri, 21 Jun 2024 10:18:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 月之暗面, 美国市场, AI产品, 出海策略
<br>
<br>
总结: 月之暗面准备进军美国市场，正在进行新一轮融资，估值有望达到30亿美元，新的投资者包括腾讯。公司员工正在开发在美国推出的产品，包括AI角色扮演聊天应用Ohai和音乐视频生成器Noisee。另外，还在为中国以外的用户开发Kimi国际版本。公司已在美国雇佣员工并继续招聘，显示出海外市场的重要性。AI创业公司出海潮涌现，月之暗面进军美国市场可能是应对国内市场竞争的一种策略。 </div>
                        <hr>
                    
                    <p>据外媒 the Information 报道，月之暗面正在为进军美国市场做准备。据悉，月之暗面正在进行新一轮融资，估值有望达到 30 亿美元，新的投资者包括腾讯。而在今年 2 月，月之暗面才获得了由阿里领投的 10 亿美元融资，当时估值约 15 亿美元。</p><p>&nbsp;</p><p>据一名员工和另一位了解情况的人士称，该公司员工一直在开发最近在美国推出的产品，包括一款可在苹果和谷歌移动应用商店上下载的AI 角色扮演聊天应用程序Ohai和一款音乐视频生成器 Noisee。</p><p>&nbsp;</p><p></p><h2>已注册国外公司？</h2><p></p><p>&nbsp;</p><p>Ohai 是一款 AI角色扮演聊天应用，可以为用户提供24小时在线的虚拟陪伴。Ohai提供了在线网页版、iOS和Android移动端应用以及Discord服务器，用户可以选择对应的平台登录注册后选择或创建 AI 角色进行对话。目前，该应用处于免费公测中。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d6bb7e4679cd1e9ef26500935c1c35a.png" /></p><p></p><p>&nbsp;</p><p><a href="https://beta.ohai.bot/discover">https://beta.ohai.bot/</a>"</p><p>&nbsp;</p><p>Noisee Al 则作为一款AI音乐视频生成工具，允许用户上传音频或提供音频链接，AI将基于音乐节奏和风格生成30秒到60秒的视频内容。同时，Noisee AI支持Suno、YouTube、Udio、Stable Audio、Soundcloud等流行音乐平台链接及本地音频文件。</p><p>&nbsp;</p><p>下面是一个示例：</p><p>&nbsp;</p><p></p><p></p><p>查看更多案例：<a href="https://noisee.ai/">https://noisee.ai/</a>"</p><p>&nbsp;</p><p>可以看出，月之暗面的出海策略目前还是主要放在C端娱乐方向。根据数字情报平台 Similarweb 数据，5 月份 Ohai 在美国安卓手机上的月活跃用户仅有 2000 人左右，而没有移动应用的Noisee公司5月份的网站访问量约为3.43万人次。</p><p>&nbsp;</p><p>月之暗面在国内广受欢迎的是AI文字聊天机器人Kimi，据悉该公司还在为中国以外的用户开发Kimi 国际版本。目前还不清楚月之暗面会何时推出海外版聊天机器人，上述人士称，海外版聊天机器人的名称不一定与中国版相同。</p><p>&nbsp;</p><p>据报道，月之暗面已经在美国雇佣了一些员工，并继续在美国招募更多人才。</p><p>&nbsp;</p><p>Ohai 和 Noisee的网站显示，这两款产品均属于一家位于加州桑尼维尔的公司Tranquillitatis。Tranquillitatis 在加州的注册文件显示，该公司的唯一董事是 Yuxin Wu，这与月之暗面联合创始人之一的吴育昕名字相同。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d47bb52cef3a75ef993ebf1978989947.png" /></p><p></p><p>来源：<a href="https://bizfileonline.sos.ca.gov/">https://bizfileonline.sos.ca.gov/</a>"</p><p>&nbsp;</p><p></p><h2>AI 创企现出海潮</h2><p></p><p>&nbsp;</p><p>月之暗面并非第一个出海的大模型创业企业。</p><p>&nbsp;</p><p>在国内相对低调的 MiniMax，已经通过人工智能聊天应用Talkie在美国拓展业务。根据媒体报道，Talkie总营收近83万美元，其投资回报率已转为正值。</p><p>&nbsp;</p><p>数据显示，截至 5 月，Talkie 在美国 iOS 和安卓平台的月活跃用户合计达 1140 万，几乎是 4 月份的三倍，峰值紧追美国同类明星产品CharacterAI。Talkie日活用户主要分布在美国（占比55.18%）、孟加拉国（占比8.34%）、菲律宾（占比14.99%）和英国（占比10.49%）。</p><p>&nbsp;</p><p>实际上，MiniMax 2022年也曾在国内推出虚拟聊天应用Glow，后因涉及隐私和敏感内容问题遭到举报并下架。&nbsp;</p><p>&nbsp;</p><p>同样的，去年4月成立的爱诗科技首先在海外上线了AI视频生成产品PixVerse，上线 3个月视频生成量突破1000万次。</p><p>&nbsp;</p><p>爱诗科技创始人王长虎在智源大会上介绍了海外用户的一个使用案例：一个海外创作者拍摄时资金断裂，无法到现场完成拍摄，所以使用了PixVerse来创作广告视频。“（PixVerse）带动了AI生成广告片的潮流。”</p><p>&nbsp;</p><p>王长虎还提到，一个几十秒的视频，如果用4090生成的话，时间在40秒至60秒钟之间，1小时视频的成本大概一两元美金。“普通用户可能不会付费，但是广告、动画创作者一定会付费。普通拍摄方式一两分钟耗费的成本很多，但是AI 极大地降低了成本。”</p><p>&nbsp;</p><p>有投研机构人士向“AI 前线”表示，选择出海的AI 创业公司出海已经很多了。很多中国 AI 创业公司在成立第一天起就会在全球不同国家设立办公室，从这个角度看，很难说它到底是哪个国家的公司。</p><p>&nbsp;</p><p>该人士也表示，客观来看，这一波大模型浪潮来了后，不管国内还是国外，做技术还是应用，其中的机会是很明显的，那大家没有理由不在国际舞台上施展拳脚。而且，这些创业公司做的很多应用没有特别大的文化隔阂，中国市场、美国市场都可以用。美国市场商业化更成熟一些，那他们肯定会把重点市场也会放在美国。</p><p>&nbsp;</p><p>而在美国设立办公室也会让公司具备一定的优势，比如在芯片、人才储备、软件等方面。另外，越早做国际化、越早接触到海外市场，对于未来产品和服务的发展也有很大的好处。</p><p>&nbsp;</p><p>但对于“先做好国内市场再出海，还是开始就要做一个国际化公司”的问题，该人士表示这取决于创始人或者其所在的行业，像软件行业天生就有全球化的基因，而硬件行业则需要考虑供应链的问题。</p><p>&nbsp;</p><p>外媒猜测，像月之暗面进军美国市场，表明中国AI初创公司正在如何应对国内市场上不断升级的大模型价格战。</p><p>&nbsp;</p><p>对此，该人士不认为国内 AI 初创公司争相抢夺国外客户，是因为国内市场的竞争太过激烈，“国外竞争也一样激烈”。现在通用大模型的竞争基本接近尾声，该跑出来的也都跑出来了，而应用层还是一片蓝海，大家自然都会往这个方向发力。</p><p>&nbsp;</p><p>现在的 AI 应用有很多合适的使用场景，但目前商业化还没有找到很好的落点，这是大多数AI应用企业需要解决的问题。</p><p>&nbsp;</p><p>该人士也指出，这种情况在to B领域尤其明显。to C 产品前期免费是为了获客，有了足够大的用户量并形成用户粘性后才能收费，但To B 在国内目前看不出特别大的可能性。虽然To B的AI公司会收取API费用，但大部分公司并没有达到正向的现金流。</p><p>&nbsp;</p><p></p><h2>出海，必担风险</h2><p></p><p>&nbsp;</p><p>但此时出海，所有企业也面临着美国立法者越来越严格的审查。</p><p>&nbsp;</p><p>比如5月份，拜登政府刚通过了一项旨在严格管控 AI 技术出口的法案《加强海外关键出口国家框架法案》。在该法案不仅限制了 AI 系统和大模型的出口，一旦法案通过，持有 H1b 签证的中国员工或留学生可能需要特殊许可才能在美从事 AI/ML 相关工作。也就是说，这是明晃晃在限制中国人在美从事 AI 相关工作。</p><p>&nbsp;</p><p>上述人士指出，地缘政治关系的恶化，让美国把人工智能列为对华重点防范的行业。对企业来说，特别是这种出海型的、以美国为重要市场的AI企业，会感到额外的压力。实际上，其所在的投行也受到了政策影响，在选择投资标的时变得更加谨慎。</p><p>&nbsp;</p><p>AI 创业公司出海当然也会面临很大的风险。将公司注册在海外，一定程度上可以规避一些问题，但实际上国外政府可能并不把这个当作判断标准。</p><p>&nbsp;</p><p>比如，TikTok已经把整个数据放在了美国、团队负责人也是新加坡人，但依然会面临各种挑战。实际上，一旦带上政治考量，商业逻辑、法律法规什么的都没有那么重要了。</p><p></p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theinformation.com/articles/chinas-top-ai-startups-enter-u-s-defying-political-tensions">https://www.theinformation.com/articles/chinas-top-ai-startups-enter-u-s-defying-political-tensions</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>