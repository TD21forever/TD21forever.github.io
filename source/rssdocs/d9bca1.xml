<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</id>
            <title>奥特曼突然变身OpenAI “安全卫士”！网友：刚被实锤不关心安全还“心理虐待”，谁信啊</title>
            <link>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:39:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI公司, 安全小组, GPT-4, 安全与保障委员会
<br>
<br>
总结: OpenAI公司成立了新的安全小组，旨在开发GPT-4的继任模型，并应对离职雇员的批评。该安全小组被称为安全与保障委员会，由公司高层领导和团队负责人组成，将向董事会提出关键安全建议，影响GPT-4模型的开发。新安全委员会的成立可能是为了回应重要人物的离职，公司内部存在安全文化和流程的变化，引发了外界质疑和讨论。资深专家认为应该先关注可见的风险。 </div>
                        <hr>
                    
                    <p>OpenAI公司已经成立新的安全小组，致力于开发GPT-4的继任模型，同时也是为了应对近期多位离职雇员对其商业意图的严厉批评。</p><p>&nbsp;</p><p>该团队被称为安全与保障委员会（Safety and Security Committee，简称SSC），领导层包括OpenAI公司CEO Sam Altman、委员会主席为Bret Taylor，外加Adam D’Angelo与Nicole Seligman等董事会成员。</p><p>&nbsp;</p><p>其他委员会成员则是来自OpenAI下辖各团队的负责人，包括曾经取代公司联合创始人Ilya Sutskever并担任了13天首席科学家的Jakub Pachocki。</p><p>&nbsp;</p><p>OpenAI公司表示，从现在起，该安全团队将就“关键的安全与保障决策”向董事会提出建议。这些决定可能会影响GPT-4继任模型的开发，即OpenAI在公告中提到的“下一个前沿模型”。</p><p>&nbsp;</p><p>在一个名“OpenAI 董事会成立安全委员会”的公告里，插入这样一条重要信息，着实很容易让人联想OpenAI是不是在借此暗暗转移大众视线，毕竟大家对GPT-5的期待是可以盖过对安全的关注的。</p><p>&nbsp;</p><p>该公司解释称，“我们很自豪能够构建并发布在行业拥有领先能力及安全水平的模型，也同样欢迎在这个重要时刻开展激烈的辩论。”但OpenAI并没有介绍具体讨论内容。</p><p>&nbsp;</p><p>这支安全团队的首要任务，就是在90天时间内制定出可供董事会审议的安全建议，不过Altman及其他董事对于建议内容仍拥有最终决定权。当然，OpenAI&nbsp;CEO及其他四位负责人同样可以在提交董事会之前对建议内容施加影响。</p><p>&nbsp;</p><p></p><h2>得到更多质疑：对谁安全？</h2><p></p><p>&nbsp;</p><p>新安全委员会的成立，很可能是为了回应本月早些时候Sutskever与Jan Leike两位重量级人物的高调离职。随着他们离开OpenAI，公司内负责评估长期AI安全问题的超级对齐小组也宣告解散。</p><p>&nbsp;</p><p>在离职之前，Leike一直担任超级对齐小组的负责人。几乎在OpenAI发布通告的同时，Leike宣布加入了Anthropic。Anthropic 由前 OpenAI 工程师创立，创始人出走就是因为双方安全理念存在差异。Leike 在 Anthropic 依旧负责超级对齐。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e51e9e8df678a3f9f11e4cafc09c7f6f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Leike在超级对齐团队解散的前一天曾经表示，“过去几年以来，安全文化和流程已经让位于公司对快速发布新品的坚持。我们早就应该认真思考通用人工智能（AGI）的影响了……OpenAI必须成为一家以安全为先的AGI厂商。”</p><p>&nbsp;</p><p>该发言还引来马斯克的“补刀”：言外之意就是，安全并不是OpenAI现在的首要任务。</p><p>&nbsp;</p><p>但这个新部门的成立并没有扭转网上OpenAI一直以来的负面安全舆论，反而引来了网友更多质疑。“好吧，我想 OpenAI 的产品现在对于 Sam Altman 和他的目标来说是安全的。”有网友略显无奈地说道。</p><p>&nbsp;</p><p>“利益冲突。这样的安全团队从定义上来说难道不应该是独立的吗？”有人质疑道。对此网友调侃成：“是的，应该有一个治理架构，确保首席执行官遵守以下原则……哎呀，他们已经摧毁整个组织架构了。”</p><p>&nbsp;</p><p>也有网友称：“至少 OpenAI 现在有了一个‘安全’团队。”显然还是觉得OpenAI 有些敷衍。</p><p>&nbsp;</p><p>当然，也有人期待这个安全委员会未来会做出什么成绩，毕竟Altman的信徒大有人在。“我仍然可以让 ChatGPT 告诉我如何制造炸弹。所以，是的，我迫不及待地想看到安全进展。”</p><p>&nbsp;</p><p>对此，行内资深专家告诉“AI 前线”，这更多是公司内部资源分配的问题。OpenAI 一直讲闭源才安全，有人认为AGI要来了、机器要毁灭人类，所以安全太重要了，要赶紧把安全做好，需要投入一定比例的资源进去。但是从一个商业公司的角度看，企业不可能停下工作去做各种安全方面的事情，更多还是要不停开发布新的模型，然后满足客户的需求，跟其他公司竞争。</p><p>&nbsp;</p><p></p><h2>前董事会成员“插刀”</h2><p></p><p>&nbsp;</p><p>同样在今天，OpenAI前董事会成员Helen Toner和Tasha McCauley 的联名文章，再次将Altman不关心安全的问题推上浪尖。</p><p>&nbsp;</p><p>“由于Altman个人长期以来的行为模式，董事会维护公司使命的能力受到了越来越大的限制。据我们了解，这些行为不仅削弱了董事会对关键决策和内部安全协议的监督能力，还引发了其他问题。”</p><p>&nbsp;</p><p>根据爆料，多位高层领导私下向董事会表达了深切的担忧，他们认为Altman营造了一种“撒谎的有毒文化”，并涉嫌“心理虐待”行为。Toner 还表示，Altman“多次”向董事会撒谎，并且“隐瞒信息”，她甚至是在 Twitter 上知道 ChatGPT 发布的消息的。</p><p>&nbsp;</p><p>当董事会意识到 Altman 需要被换掉时，Toner 表示，如果Altman 发现了这个，很明显他会“竭尽全力”阻止董事会反对他。她声称他“开始对其他董事会成员撒谎，试图将我从董事会中赶出去。”</p><p>&nbsp;</p><p>“我们非常小心，非常慎重地选择通知谁，除了我们的法律团队之外，几乎没有任何人提前通知过我们，所以这才把消息拖到了 11 月 17 日。”Toner谈及去年的OpenAI政变时说道。</p><p>&nbsp;</p><p>两人指出，自从Altman重返公司以来，一些发展动态令人担忧，包括他重新加入董事会，以及OpenAI一些专注于安全领域的高级人才的离职。这些情况对于OpenAI在自我治理方面的实验来说，似乎预示着一些不利的影响。</p><p>&nbsp;</p><p>有趣的是，刚刚成为亿万富翁不久的Sam Altman 承诺捐出自己大部分财富，表示将继续专注于“支持有助于为人们创造富足的技术”。</p><p>&nbsp;</p><p></p><h2>资深专家：应该先关注看得见的风险</h2><p></p><p>&nbsp;</p><p>没有什么比Sutskever和Leike等人扮演的重要角色更能表明OpenAI致力于其使命的了。Sutskever和Leike是技术专家，他们长期致力于安全，并明显真诚地愿意在必要时要求OpenAI改变方向。</p><p>&nbsp;</p><p>Sutskever 在2019年的采访中当记者刚刚说道，“你们说，‘我们要建立一个通用人工智能，’”时，Sutskever 立即插话强调：“我们将尽一切可能朝这个方向努力，同时确保以一种安全的方式做到这一点。”</p><p>&nbsp;</p><p>随着他们的离职，很多人问他们在OpenAI看到了什么，但没有得到答案。</p><p>&nbsp;</p><p>不同于 Sutskever、Leike等人坚决捍卫AI安全的态度，有些大佬并不那么重视，比如图灵奖得主Yann LeCun。</p><p>&nbsp;</p><p>当时，在LeCun在对Jan Leike的回贴中，他表示当前对AI安全担忧的紧迫感是过度夸张的，类似于在涡轮喷气发动机发明之前，急于解决跨洋飞行的安全问题。所以难怪OpenAI 解散对齐团队。在LeCun看来，智能系统的进化需要多年时间，应该通过反复的工程改进逐步提高其智能和安全性，而不是过度担忧未来可能的风险。</p><p>&nbsp;</p><p>同时，上述专家也告诉“AI 前线”，从开源角度讲，我们离“AGI 来了、毁灭人类”这些还很远，他并不认可这些说法。</p><p>&nbsp;</p><p>该专家表示，目前，AI 安全上的风险更多来自大家看得见、摸得着的地方，比如数据集的偏见和毒化给使用模型带来很多挑战：让ChatGPT 画一个剥了皮的荔枝，由于ChatGPT 根本不知道荔枝剥皮了什么样，所以它就是随便画；又如让Stable Diffusion 等海外模型画北京城市，它会画一个破破烂烂的四合院。</p><p>&nbsp;</p><p>“目前，像这种数据集的 bias 其实没有得到很多关注。但这种可能是更重要的，与超级对齐不是一回事儿，”该专家说道。</p><p>&nbsp;</p><p>该专家也分析称，从更大层面来说，美国也在渲染 AI 安全问题，比如AI自动生成恶意软件、自动攻击各种网站，但实际上我们都知道，代码生成的能力远远没有到这种程度，所以这种渲染也是为了防止模型出口，不让非常厉害的模型技术扩散出去。美国炒作这个事情，也有想要得到中国类似“不利用这个技术开发武器”承诺的意图。</p><p>&nbsp;</p><p>“安全是一个非常大的叙事，每个人在不同的立场都会有不同的看法。”当前应该把主要精力投入到哪个方面？显然OpenAI当前掌门人有自己的答案，其他公司也有自己的答案。但答案正确与否，还需要时间验证。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://openai.com/index/openai-board-forms-safety-and-security-committee/">https://openai.com/index/openai-board-forms-safety-and-security-committee/</a>"</p><p><a href="https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/">https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/</a>"</p><p><a href="https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5">https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</id>
            <title>如何降低数据消费门槛，让非技术用户也能成为数据分析专家？</title>
            <link>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:05:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据分析, 企业发展, 大模型, SwiftAgent
<br>
<br>
总结: 在数字化时代，数据分析已成为企业发展的重要驱动力。企业需要克服数据获取困难和高阶分析难等挑战，形成基于数据的决策文化，打破数据孤岛，促进跨部门协作。SwiftAgent 的出现实现了数据分析的民主化，让人人都能成为数据分析专家，提高了数据分析的效率和价值。 </div>
                        <hr>
                    
                    <p>在数字化时代，数据分析已成为企业发展的重要驱动力。对于企业而言，数据价值毋庸置疑，更重要的是如何对其进行分析和利用。从客户运营到战略决策，各行各业都离不开数据分析。本期《极客有约》栏目邀请到了数势科技数据智能产品总经理岑润哲，与他一起探讨大模型时代下，如何以 SwiftAgent 革新企业数据分析范式，让人人都能成为数据分析专家。</p><p></p><p>数势科技是行业领先的数据智能产品提供商，为全球大金融、泛零售和高科技制造企业提供大模型增强的智能产品体系，促进企业数字化转型。</p><p></p><p>如何让企业的非技术人员跨越数据的门槛，成为数据分析专家？数据分析的现状和未来发展趋势又如何？SwiftAgent 的出现如何帮助非技术用户？它的技术特点和优势是什么？精彩观点总结如下：</p><p>企业在进行数据分析时面临的主要挑战包括数据获取困难、高阶分析难等，企业数据分析的效率和准确性极具挑战。非技术人员应从业务需求出发，逐步引入数据分析工具和理论，并通过实际操作提升分析思路。企业从组织层面进行变革，形成基于数据的决策文化，打破数据孤岛，并促进跨部门协作。SwiftAgent 通过引入指标语义层和大模型能力，实现了数据分析的民主化，让不同岗位的人员都能实现实时、灵活、精确的数据分析，缩短从数据获取到决策的链路，提高了数据分析的效率和价值。</p><p></p><p>完整视频参看：</p><p></p><p></p><p></p><p></p><h2>企业数据分析的机遇与挑战并存之道</h2><p></p><p></p><p>&nbsp;InfoQ：润哲老师，您在数据分析领域有着非常丰富的经验，您认为企业用户在进行数据分析的时候，通常会遇到哪些困难和挑战？这些挑战对于企业的发展有哪些影响？</p><p></p><p>岑润哲：在我们服务的企业客户中，数据分析一般分为四个步骤。首先是数据收集与获取（Data Query）。企业用户、包括分析师和业务人员在分析前需从数据仓库或业务系统库中提取数据，由于他们通常缺乏数据分析技能，不熟悉 SQL 或底层数据表，这在提取中造成了最大的不便，这是许多客户的痛点。</p><p></p><p>数据分析师或商业分析师具备一定的数据分析技能，会编写 SQL，但底层数据仓库的表结构混乱，熟悉的表有限。当他们完成一个领域的指标分析后，转向另一领域时，需重新梳理表逻辑，这是数据获取阶段的一个难点。</p><p></p><p>第二步是数据获取后，使用专业分析工具或方法进行高阶分析（Data Analytics）。难点不在于选择工具，而在于根据不同场景选用合适的分析范式。例如，销量分析时可能需要同环比分析、排序分析；指标异常时需归因分析能力；转化率异常时需漏斗分析能力。工具学习不难，难的是找到适合业务板块的分析范式。</p><p></p><p>第三步是数据获取和高阶分析后，如何快速解读数据。传统 BI 工具难以直接将数据转化为洞察（insight）。我们的产品结合大模型的语义理解能力，可快速挖掘商业洞察。例如，从几千行 Excel 数据中迅速识别产品或客户群的问题。我们希望结合指标语义层和大模型辅助，优化数据洞察和解读，提供业务方所需的分析解读。</p><p></p><p>最后是数据权限和安全性问题。 若数据权限开放给所有分析师或业务人员，可能引发数据安全性和隐私问题。在数据复盘过程中，需确保不同角色和用户能获取适当权限的数据集。</p><p>总体来说，数据获取、高阶分析、数据解读理解、数据权限管控，是不同行业客户在数据分析中面临的主要问题。</p><p></p><p>&nbsp;InfoQ：这些非技术用户在数据分析的过程中，他们最大的困惑是什么呢？</p><p></p><p>岑润哲：对于非技术人员，我们将其定义为偏业务人员，他们擅长业务流程和合规性，但在将业务思维与底层数据表关联时存在难题。以零售行业为例，非技术人员如门店店长或督导需要分析经营数据、客户画像和商品销售数据，但若直接提供数据表，他们难以进行分析。</p><p></p><p>非技术人员不仅需要工具，更需要将数据分析与业务场景结合。我们认为可以通过指标语义层和大模型的生成能力，帮助客户提出更精准的问题。 例如，构建好指标体系后，大模型能生成结构化问题，如从经营视角分析门店流水和毛利，或从服务水平视角分析大众点评评分，以及客群画像等。</p><p></p><p>当企业内部的指标和维度体系建立完善，结合大模型，能够输出标准的分析思路。这与传统 BI 工具相比，是一个双向过程：大模型不仅能响应问题，还能提供分析思路。比如，大模型可以根据公司的指标和维度生成批量问题，用户再从中筛选最关心的问题进行分析。</p><p></p><p>将大模型分析助手与用户请求结合后，我们从单向的 BI 分析模式转变为可交互模式，用户可以向大模型提问，大模型也能反问用户，提供分析视角。这种模式优化了非技术人员在分析思路上的痛点，大模型的出现增强了分析思路扩展的能力。</p><p></p><p>大模型不仅提供分析思路，还能激发用户的分析欲望，形成良好的交互形式，相当于由顾问提供建议。这是大模型在智能分析领域带来的最大改变。</p><p></p><p>&nbsp;InfoQ：您认为企业在数据分析的过程中还有可能会遇到哪些管理上或者流程上的障碍或者挑战吗？</p><p>岑润哲：结合我们服务过的客户，我总结了三种主要障碍。</p><p></p><p>首先是组织文化的障碍。许多组织尚未形成基于数据的决策文化，决策更多由高层领导凭经验作出，这会影响分析工作的价值。如果组织文化不以数据驱动决策，即使分析质量再高也难以发挥作用。</p><p></p><p>第二是企业内部数据体系的孤岛问题。例如，在泛零售行业，线上线下渠道的数据可能未打通，或埋点数据与交易数据之间存在隔离。这导致无法进行跨部门或跨领域的分析，如无法评估营销活动的效果。</p><p></p><p>第三是跨部门协作问题。不同部门之间可能存在边界和利益问题，例如活动运营部门需要客户运营部门的数据时，可能难以获得必要的支持。这种跨部门协作的障碍，使得进行复杂的分析或关联分析变得困难。</p><p>&nbsp;InfoQ：数势科技就作为数据智能产品的提供方，您认为帮助企业解决这些问题的核心思路是什么呢？企业应该重点关注哪些方面？</p><p></p><p>岑润哲：我们公司的核心理念和使命是更新现行的数据分析范式，从集中式转变为民主化。目前，企业的分析逻辑多是粗犷或集中式的，业务方需向数据团队提出需求，然后等待数据提供。这种模式下，数据解读和高阶分析强烈依赖商业分析师或数据分析师团队，存在较大的隔阂。</p><p></p><p>引入指标语义层和大模型能力后，我们希望企业内部的每个员工能够成为“数据公民”，这意味着他们即使不懂数据，也能基于业务分析思路，获取和探索企业内部的数据资产。大模型的出现有助于每个数据公民进行大规模自定义数据分析，极大缩短从数据获取到决策的链路。</p><p></p><p>未来数据分析的主要方向是从研发与业务割裂的形式，转变为业务方在研发设定标准后，自行利用大模型辅助获取和挖掘数据。</p><p></p><p>&nbsp;InfoQ：您刚刚提到数据公民的概念非常有趣，它会对决策层的思维导向和人才培养产生积极影响吗？</p><p></p><p>岑润哲：是的。例如我们曾为一家鞋类企业提供了 基于指标语义层的完整分析框架， 他们可以分析不同客群的偏好。通过 Know Your Customer 标签，发现 25 至 29 岁女性对 PVC 材质、鞋跟高度在 3 到 5 厘米的鞋子有很高的偏好。这些信息在之前是无法获取的，因为他们不知道公司内部有这些客群和商品标签。</p><p></p><p>现在，借助大模型工具，业务方可以提出更有针对性的问题，并驱动分析过程。他们更了解产品的销售情况，能够通过数据分析找出哪些客群对特定类型或特征的鞋子有更高的转化率，进而讨论投放策略或营销策略，形成一个正循环。</p><p></p><p>传统仅从技术角度分析数据表可能无法获得这样的洞察。但现在，业务方有能力自行分析不同的标签和指标，这使他们能够更好地理解企业内部如何提升销售，实现业务驱动的数据分析和决策。</p><p></p><p>&nbsp;InfoQ：前不久数势科技在 AICon 大会现场发布了 SwiftAgent 2.0 版本，是否可以现场演示？</p><p></p><p>岑润哲：好的，以下是产品的 demo 视频，供大家了解，可留言或点击“阅读原文”申请产品试用。</p><p></p><p></p><p></p><p>InfoQ：在现场发布的时候，数势科技也提到了大模型和 Agent 将会颠覆企业数据分析与决策范式，我想请问为什么这样来表达呢？</p><p></p><p>岑润哲：我们可以回顾一下大模型和 Agent 架构出现之前的数据分析流程。传统上，数据分析链路较长，从提出需求到数据团队获取数据集、配置 BI 工具并搭建驾驶舱，整个过程耗时且复杂。核心问题在于，所有工具的使用都需要人工配置和梳理，工作量较大且重复，效率较为低下。</p><p></p><p>Agent 架构结合大模型后，展现出其优势，尤其是 Agent 在工具调用方面的能力。Agent 不仅能理解用户的自然语言需求，还能自动规划任务执行步骤。</p><p></p><p>例如，用户提出数据分析请求，Agent 可理解用户意图、获取所需地区的销售明细、进行排序和高阶分析、对比 TOP3 产品。这不仅涉及任务拆解和规划，还包括与知识库的协同和工具串联。</p><p></p><p>这种架构带来四个好处：首先，用户不再需要学习工具配置，因为大模型已经掌握了工具调用的方法；其次，通过 Agent 统一规划，提高了效率，避免了在不同工具间切换的繁琐；第三，交互性得到改善，用户通过自然语言与系统交互，降低了使用门槛；最后，简化了操作，将复杂逻辑留给程序处理。</p><p></p><p>企业可通过 Agent 机制调度内部不同工具，形成有效串联，降低了业务方学习和使用工具的时间与门槛，这是我认为它会颠覆企业分析决策范式的原因。</p><p></p><p></p><h2>SwiftAgent 开启智能数据分析新篇章</h2><p></p><p></p><p>InfoQ：SwiftAgent 为什么能够在众多的大模型和数据分析产品中脱颖而出？</p><p></p><p>岑润哲：SwiftAgent 被定义为由大模型 Agent 机制驱动，并结合指标标签语义层的智能分析产品。它让企业非技术人员——如企业管理者和业务人员可准确、即时、个性化地进行数据査询和业务洞察，提升决策能力，实现数据价值普惠化。</p><p></p><p>其核心技术亮点主要分为三个层面：通过构建指标标签语义层，统一了数据和业务语言，避免了大模型的幻觉；结合 Agent 架构，赋予产品反思、推理和规划的能力；通过自研的加速引擎，提升了前端问询的响应速度。</p><p></p><p>指标标签语义层： 我们采取的技术路线不是直接将用户自然语言请求转化为 SQL。因为企业内部数据标注和治理程度不一，直接转化的准确率很低。我们构建了指标和标签语义管理层，统一了数据语言和业务语言，解决了大模型的幻觉问题，提高了准确率，并帮助企业建立了一套指标和标签体系，解决了数据统一问题。</p><p></p><p>Agent 产品架构设计：Agent 架构能够进行思考、推理和反思，解决复杂任务执行问题。自然语言形式的灵活性让用户可能提出不可预测的问题，我们设计了合理的 Agent 架构，使用户能够以自然语言形式灵活、高效地获取数据。我们在 Agent 架构层面做了大量的调研和研发，提升处理复杂问题的能力。</p><p></p><p>数据加速引擎： 我们自研了 Hyper Computing Acceleration Engine，提升对话式分析的响应速度。例如，针对用户常问的商品品类、城市等维度的销售额或毛利，进行预聚合和预计算，使得即使面对百亿级数据量的订单表，也能快速响应用户查询。</p><p></p><p>&nbsp;InfoQ：SwiftAgent 的产品优势 / 壁垒是什么？</p><p></p><p>岑润哲：除了上述提到的 Agent 机制和数据加速引擎，SwiftAgent 还拥有结构化与非结构化数据联动分析的能力。我们将非结构化信息（如用户评论、直播数据）抽象化，转化为结构化数据，并与企业内部指标进行关联分析，提供更全面的分析。</p><p></p><p></p><h2>智能数据分析市场的发展前景</h2><p></p><p></p><p>InfoQ：除了零售行业以外，还有哪些行业已经上线 SwiftAgent？成效如何? 可以分享几个案例吗？</p><p></p><p>岑润哲：除了零售行业，我们也在金融行业如银行、证券公司，以及高端制造行业实现了应用落地。</p><p>以某知名城商为例，分行行长通常关注贷款余额、不良率等指标的波动。传统上，他们需要向分析团队提出需求，由团队提供分析结果。现在，通过上线智能分析产品，领导可以直接通过自然语言查询获取信息，同时经营分析团队可以利用沉淀的分析模板和思路，加速从数据到分析报告的转化。</p><p></p><p>我们也与头部证券机构合作，帮助客户经理分析他们管理的高净值客户。例如，理财顾问或投资经理管理 200 个客户时，可以通过自然语言查询，快速了解哪些客户存在流失风险，或关注行业政策变动，以及持仓标的的变化。这样的分析能力，如果依靠传统 CRM 工具，可能需要花费大量时间。而通过 SwiftAgent 与客户标签、指标联动，构建了从数据洞察到决策的完整链路，效率提升 80%。此外，我们还将优秀客户经理的 SOP 沉淀在知识库中，帮助新员工快速了解如何应对不同情况，比如客户亏损时的安抚策略。这样，数据分析不仅帮助业务人员理解数据，还指导他们基于数据采取行动。</p><p></p><p>&nbsp;InfoQ：对于金融和央国企而言，信创和数据安全是重点关注的方向。数势科技在这两方面有些认证或适配？</p><p></p><p>岑润哲：数势科技是北京信创工委会会员单位，已经完成国家高新技术企业认证、中关村高新技术企业认证、ISO9000 质量管理体系认证、信息安全管理体系认证、信息技术服务管理体系认证、信息系统安全登记保护三级、麒麟操作系统信创认证、达梦数据库信创认证、人大金仓信创认证和 CMMI 等资质认证。产品充分满足金融企业和国央企的部署需求。目前合作的国产大模型都已完成算法备案。另外，SwiftAgent 已首批通过中国信通院针对大模型驱动的数据分析工具的专项测试，获得权威认可。</p><p></p><p>&nbsp;InfoQ：在您看来未来智能数据分析市场规模是将会是怎么样的？</p><p></p><p>岑润哲：智能数据分析市场将是大模型落地的重要场景。数据分析智能化能够充分利用大模型的规划和拆解能力，并与企业内部数据联动，产生化学反应。企业不仅希望提升数据分析体验，还希望降低开发需求，将数据智能化作为核心战略。</p><p></p><p>在客户需求层面，大金融、泛零售和高科技制造是我们的重点服务领域。这些行业的企业对大模型的应用已经从观望学习阶段过渡到试点实施阶段。许多头部企业，尤其是金融、国央企、零售和能源企业，已经开始大量招标，希望在数据分析、知识库、营销和 RPA 等多个场景中应用大模型。数据分析场景特别受到重视，占企业需求的 80% 以上。</p><p></p><p>另外，国家层面也在推动数据资产入表，鼓励企业将数据资产作为无形资产量化并反映在财务报表中，这也将促进大模型在数据管理和分析领域的结合。</p><p></p><p>&nbsp;InfoQ：您作为资深专家，请问对于想要提高数据分析能力的非技术用户，有哪些建议呢？</p><p></p><p>岑润哲：首先，非技术用户应以业务需求为出发点，学习基础统计学和数据分析概念，构建分析能力的基础。例如，零售客户，可从业务场景切入，如教店长如何分析门店数据以提升业绩。</p><p></p><p>其次，非技术人员应先理解自身管理的业务逻辑流程，再逐步引入数据分析工具和理论。 建议通过实际操作小项目来提升分析思路，如门店经理、财务经理或 HR 可以分析与自己工作相关的数据。</p><p></p><p>最后，还可以加入专业论坛或群体，关注不同领域的分析博客，以帮助构建人脉并提升对数据的理解力。 业务人员转为分析师往往潜力巨大，因为他们对业务流程有深刻理解，具备在数据分析和业务洞察领域的天然优势。</p><p></p><p>数据分析不仅限于互联网公司或运营、财务风控等领域，数据分析将持续渗透到企业各个部门，提升决策效率，这是未来的大趋势，也是我们数势科技的愿景和使命。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</id>
            <title>浪潮信息发布 “源2.0-M32” 开源大模型，大幅提升模算效率</title>
            <link>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 源2.0-M32, 门控网络, 专家模型, 模算效率
<br>
<br>
总结: 源2.0-M32是一个包含32个专家的混合专家模型，采用了门控网络结构来调度专家，实现高效计算。在模型训练和推理过程中，源2.0-M32表现出色，主要通过建模专家之间的协同关系来提升模型精度和模算效率。 </div>
                        <hr>
                    
                    <p>5月28日，浪潮信息发布“源2.0-M32”开源大模型。“源2.0-M32”在基于“源2.0”系列大模型已有工作基础上，创新性地提出和采用了“基于注意力机制的门控网络”技术，构建包含32个专家（Expert）的混合专家模型（MoE），并大幅提升了模型算力效率，模型运行时激活参数为37亿，在业界主流基准评测中性能全面对标700亿参数的LLaMA3开源大模型。</p><p></p><h3>大模型技术解读</h3><p></p><p></p><p>在算法层面，源2.0-M32提出并采用了一种新型的算法结构：基于注意力机制的门控网络（Attention Router），针对MoE模型核心的专家调度策略，这种新的算法结构关注专家模型之间的协同性度量，有效解决传统门控网络下，选择两个或多个专家参与计算时关联性缺失的问题，使得专家之间协同处理数据的水平大为提升。源2.0-M32采用源2.0-2B为基础模型设计，沿用并融合局部过滤增强的注意力机制（LFA, Localized Filtering-based Attention），通过先学习相邻词之间的关联性，然后再计算全局关联性的方法，能够更好地学习到自然语言的局部和全局的语言特征，对于自然语言的关联语义理解更准确，进而提升了模型精度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7749b68f0ae25cc003fe38680e899857.png" /></p><p>Figure1-&nbsp;基于注意力机制的门控网络（Attention Router）</p><p></p><p>在数据层面，源2.0-M32基于2万亿的token进行训练、覆盖万亿量级的代码、中英文书籍、百科、论文及合成数据。大幅扩展代码数据占比至47.5%，从6类最流行的代码扩充至619类，并通过对代码中英文注释的翻译，将中文代码数据量增大至1800亿token。结合高效的数据清洗流程，满足大模型训练“丰富性、全面性、高质量”的数据集需求。基于这些数据的整合和扩展，源2.0-M32在代码生成、代码理解、代码推理、数学求解等方面有着出色的表现。</p><p></p><p>在算力层面，源2.0-M32采用了流水并行的方法，综合运用流水线并行+数据并行的策略，显著降低了大模型对芯片间P2P带宽的需求，为硬件差异较大训练环境提供了一种高性能的训练方法。针对MoE模型的稀疏专家计算，采用合并矩阵乘法的方法，模算效率得到大幅提升。</p><p></p><p>基于在算法、数据和算力方面全面创新，源2.0-M32的性能得以大幅提升，在多个业界主流的评测任务中，展示出了较为先进的能力表现，在MATH（数学竞赛）、ARC-C（科学推理）榜单上超越了拥有700亿参数的LLaMA3大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18c12d432b7f6f84dcb3fc48081ae17a.png" /></p><p>Figure2 源2.0-M32业界主流评测任务表现</p><p></p><p>源2.0-M32大幅提升了模型算力效率，在实现与业界领先开源大模型性能相当的同时，显著降低了在模型训练、微调和推理所需的算力开销。在模型推理运行阶段，M32处理每token所需算力为7.4GFLOPs，而LLaMA3-70B所需算力为140GFLOPs。在模型微调训练阶段，对1万条平均长度为1024 token的样本进行全量微调，M32消耗算力约0.0026PD(PetaFLOPs/s-day)，而LLaMA3消耗算力约为0.05PD。M32凭借特别优化设计的模型架构，在仅激活37亿参数的情况下，取得了和700亿参数LLaMA3相当的性能水平，而所消耗算力仅为LLaMA3的1/19，从而实现了更高的模算效率。</p><p></p><p>浪潮信息人工智能首席科学家吴韶华表示：当前业界大模型在性能不断提升的同时，也面临着所消耗算力大幅攀升的问题，对企业落地应用大模型带来了极大的困难和挑战。源2.0-M32是浪潮信息在大模型领域持续耕耘的最新探索成果，通过在算法、数据、算力等方面的全面创新，M32不仅可以提供与业界领先开源大模型相当的性能，更可以大幅降低大模型所需算力消耗。大幅提升的模算效率将为企业开发应用生成式AI提供模型高性能、算力低门槛的高效路径。</p><p></p><h3>技术创新点剖析：</h3><p></p><p></p><p>Llama系列模型的精度从Llama1到Llama3显著提升，Llama3的精度处于领先地位，特别是其700亿参数的模型在每个Token的推理和算力上达到140GFLOPS。尽管如此，Llama3在推理时的算力开销较大，也就是说单位算力下的精度表现较差。</p><p></p><p>在采访环节，吴韶华回答了记者问，关于32个专家的优势及挑战，吴韶华解释道，当前很多大模型工作采用8个专家的架构，但浪潮信息选择32个专家，核心原因是模算效率。实验表明，在他们的LFA加上Attention Router架构中，专家数量从8增加到32时，精度显著提升，而算力开销保持不变。这是因为激活专家的数量仅为2个。此外，单个专家参数量为2B，这样控制模型参数量有利于企业应用的模算效率。结果显示，这一选择在相同精度下实现了低算力消耗。</p><p></p><p>同时，由于激活的专家数量为2个，通过Attention Router机制考虑专家间协同，专家数量的增加使得每个专家或专家组能够学习更多有针对性的信息。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/20/9d/20109819dd1cb3e6794a287d3648e49d.jpg" /></p><p></p><p>模算效率与成本控制也是此次大模型发布的关键讨论点。吴韶华强调，算力是当前大模型发展的核心瓶颈。MoE结构模型通过扩展专家数量，在固定算力下获得更高精度。在多元芯片的使用上，浪潮信息的EPAI软件提供相关工具，支持多元算力架构，降低用户迁移设备的难度和成本。这些创新措施有助于降低用户试错成本，实现应用落地。</p><p></p><p>高模算效率意味着在单位算力投入下获得更高的精度回报，这对于大模型训练和推理都非常有利。“源2.0-M32”模型旨在通过创新算法提升精度并降低同等精度下的算力开销，大幅提升基础模型的模算效率。“源2.0-M32”是一个包含32个专家的混合专家模型，采用了Attention Router结构来调度专家，实现高效计算。在模型训练和推理过程中，“源2.0-M32”表现出色，Attention Router结构主要是通过建模专家之间的协同关系来提升模型精度。</p><p></p><p>M32模型的训练数据筛选与优化也是核心技术点，吴韶华详细介绍了浪潮信息在训练数据方面的策略。浪潮信息从源1.0开始构建了互联网自然语言文库，并开发了一套数据清洗平台。对于稀缺数据（如中文数学数据），通过数据合成工具补充。M32模型引入了大量代码数据和互联网数据，提升数据的多样性和质量。代码数据不仅对模型的代码能力有益，还能帮助解决数学问题和推理问题。最终，源2.0-M32模型在精度和算力开销方面优于Llama3。</p><p></p><p>在应用落地方面，源2.0-M32增强了小样本学习能力，通过少量样本就能显著提升模型能力。相较于微调而言，这是一种轻量化支撑大模型应用落地的有效技术。</p><p></p><p>MoE模型对企业开发应用和大模型普惠的影响也逐渐展现，吴韶华向大家介绍说，MoE模型除了提升算力效率外，还能提高精度，降低使用成本，增强模型能力。MoE模型通过激活少量专家，保持算力开销低，同时允许训练更多Token，进一步提升精度。对于终端用户来说，关键在于解决实际问题和降低使用成本。例如，在智能客服等应用中，用户更愿意花费较少的钱解决具体问题，而不会购买高成本的大模型。</p><p></p><p>最后，吴韶华补充了大模型落地与微调的观点，大模型在应用落地时需要进行微调，这是由于预训练阶段的数据和模型能力存在局限性。微调能有效应对不同的行业需求，但算力需求较大。同时，推理阶段也是算力开销大户，因此高效的模型结构和更强的能力在实际应用中具有优势。浪潮信息通过内部实际应用场景，如客服、软件研发、运维等，不断积累经验，提升模型能力，满足更多用户需求。</p><p></p><h3>回顾与展望：</h3><p></p><p></p><p>回顾大模型的发展历史，我们可以看到，2020年GPT-3的发布点燃了大模型的热潮。从2020年到2022年，业界在大模型能力上进行了广泛的探索。例如，2022年推出了GPT强化学习方法，使大模型与人的意图对齐，建立了良好的发展思路。同年末，ChatGPT问世，引发了大模型应用的热潮，成为增长最快且被广泛接受的大模型应用。此后，Llama系列模型陆续推出，2024年大模型的发展速度进一步加快。</p><p></p><p>浪潮信息的大模型研究始于2020年GPT-3发布后。2021年，他们发布了第一个大模型“源1.0”，拥有2457亿参数。2022年，进行了应用落地探索，运用了检索类技术和RAG技术。2023年，发布了“源2.0”，并推出了“源2.0-M32”混合专家结构模型。</p><p></p><p>关于大模型推广及触达用户，吴韶华介绍了浪潮信息大模型落地的两个方向：外部客户和内部需求。对外，浪潮信息通过与合作伙伴在EPAI平台上合作，提供开源模型支持，增强用户体验。对内，浪潮信息在多个业务场景中应用大模型，解决内部需求问题的同时积累经验，提升算法和工具性能，从而更好地服务外部客户。</p><p></p><p>未来，M32开源大模型配合企业大模型开发平台EPAI（Enterprise Platform of AI），将助力企业实现更快的技术迭代与高效的应用落地，为人工智能产业的发展提供坚实的底座和成长的土壤，加速产业智能化进程。</p><p></p><p>最后，吴韶华宣布，浪潮信息已在GitHub和Hugging Face上开源了代码和模型，并发表了相关论文。</p><p></p><p>源2.0-M32将持续采用全面开源策略，全系列模型参数和代码均可免费下载使用。</p><p>代码开源链接：<a href="https://github.com/IEIT-Yuan/Yuan2.0-M32">https://github.com/IEIT-Yuan/Yuan2.0-M32</a>"</p><p>模型下载链接：<a href="https://huggingface.co/IEITYuan/Yuan2-M32-hf">https://huggingface.co/IEITYuan/Yuan2-M32-hf</a>"</p><p><a href="https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary">https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</id>
            <title>别再危言耸听！大多数被评为“严重”的Bug评级具有误导性</title>
            <link>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:39:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: CVSS评级, 安全团队, 软件供应链, AI/ML工具
<br>
<br>
总结: JFrog发布的调查结果显示，大多数CVSS评级在实际情况下并不适用，但安全团队仍花费大量时间修复漏洞。报告指出安全问题会影响工作效率，同时揭示了软件供应链安全和AI/ML工具在安全领域的应用不成比例。JFrog提供的安全方案聚焦于统一平台管理，为企业提供了高性价比的解决方案。 </div>
                        <hr>
                    
                    <p></p><blockquote>74%被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但有60%的安全和开发团队仍花费25%的时间修复这些漏洞。</blockquote><p></p><p>&nbsp;</p><p>近日，流式软件公司、JFrog软件供应链平台背后的公司JFrog&nbsp;发布了其 《2024年全球软件供应链发展报告》的调查结果，指出了新兴的发展趋势、行业风险以及保障企业软件供应链安全的最佳实践案例。</p><p>&nbsp;</p><p>JFrog首席技术官兼联合创始人Yoav Landman表示：“软件安全领域变幻莫测，全球的DevSecOps团队都在探索前行，在AI迅速普及的时代，更需要创新来满足需求。我们的数据涵盖了迅速发展的软件生态系统，为安全和开发组织提供了一个更为全面的介绍，包括值得关注的CVE评级错误、使用生成式AI进行编码所带来的安全影响相关洞察、允许组织用于开发的高风险软件包等信息，以便相关人员做出更明智的决策。”</p><p>&nbsp;</p><p>JFrog的《2024年全球软件供应链发展报告》结合了超过7000家企业的JFrog Artifactory开发者使用数据、JFrog安全研究团队原创的CVE分析、以及委托第三方对全球1200名技术专业人士进行的调查数据，旨在为快速发展的软件供应链领域提供信息参考。主要研究结果包括：</p><p>&nbsp;</p><p>并非所有CVE都如表面所见：传统的CVSS评级仅关注漏洞利用的严重性，而非其被利用的可能性，后者需要结合具体情境才能做出有效的评估。JFrog安全研究团队在分析了2023年发现的212个高知名度CVE后，平均将85%的“严重”CVE和73%的“高危”CVE的重要性评级下调。此外，JFrog发现，在报告的前100个Docker Hub社区镜像中，74%的CVSS评级为“高危”和“严重”的常见CVE实际上是无法被利用的。</p><p>&nbsp;</p><p>拒绝服务（DoS）攻击盛行：JFrog安全研究团队分析的212个高知名度CVE中，有44%存在发起DoS攻击的潜在威胁；17%存在执行远程代码（RCE）的潜在威胁。这对于安全组织来说是个好消息，因为RCE由于能够提供对后端系统的完全访问权限，与DoS攻击相比，其危害性更大。</p><p>&nbsp;</p><p>安全问题会影响工作效率：40%的受访者表示，通常需要一周或更长时间才能获得使用新软件包/库的批准，这延长了新应用程序和软件更新的上市时间。此外，安全团队大约耗费25%的时间用于修复漏洞，即使这些漏洞的风险在当前情况下可能被高估或甚至无法被利用。</p><p>&nbsp;</p><p>在软件开发生命周期（SLDC）中采用安全检查方式的差异性&nbsp;——当涉及到决定在软件开发生命周期中的哪个阶段采取应用安全测试时，行业内存在明显分歧，这突显了同时进行左移和右移的重要性。42%的开发人员表示，最好在编写代码过程中执行安全扫描，而41%的开发人员认为最好在新软件包从开源软件（OSS）库引入企业之前执行扫描。</p><p>&nbsp;</p><p>安全工具的过度使用现象仍在持续&nbsp;——&nbsp;近半数IT专业人士（47%）表示他们部署了四到九种应用安全解决方案。然而，有三分之一的调查对象和安全专业人士（33%）表示，他们正在使用十种乃至更多的应用安全解决方案。这一现象反映出市场对于安全工具整合的需求趋势，同时也表明人们正逐渐放弃单一的点对点解决方案，转而寻求综合性更高的安全工具集成。</p><p>&nbsp;</p><p>AI&nbsp;/&nbsp;ML工具在安全领域的应用不成比例&nbsp;——尽管有90%的受访者表示，他们的企业目前以某种形式使用AI&nbsp;/&nbsp;ML驱动的工具来协助安全扫描和修复工作，但只有三分之一的专业人士（32%）表示他们的组织使用AI&nbsp;/&nbsp;ML工具来编写代码。这反映出业内大多数人对AI生成的代码可能会为企业软件带来的潜在安全隐患仍持审慎态度。</p><p>&nbsp;</p><p>尽管新发布的报告揭示了被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但企业对于软件供应链的安全意识，一刻也不应放松。以JFrog为例，其提供的安全方案聚焦于以统一的平台去实现管理，且不限制用户数，顺应了很多企业的需求。同时，作为JFrog的一大产品特点，JFrog Xray和制品库是进行统一绑定的，即企业使用了JFrog的制品库，就无需额外购买JFrog Xray，会自动获得安全扫描的能力。这进一步帮助企业的安全团队减少了工具安全扫描维护和采购的成本对的同时，还能够帮助企业在安全扫描、制品管理、供应链管理上提供统一的高性价比解决方案。&nbsp;</p><p></p><p>JFrog安全研究高级总监Shachar Menashe表示：“虽然安全漏洞的数量每年都在增加，但这并不意味着其严重性也在同步上升。显然，IT团队愿意投资于新工具以提升安全性，但了解如何部署这些工具、如何有效利用团队时间以及简化流程，对于确保软件开发生命周期（SDLC）的安全至关重要。我们编制这份报告的目的不仅仅在于分析趋势，更是为了当技术业务领导者在针对AI导航、恶意代码或安全解决方案等方面制定决策时，能够为其提供清晰的指导和专业的技术咨询。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</id>
            <title>谷歌刚刚更新了算法，顺便搞毁了几家公司</title>
            <link>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:29:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌更新算法, 公司毁灭, AI功能, 搜索引擎
<br>
<br>
总结: 谷歌最近的算法更新对一些公司造成了毁灭性的影响，特别是那些依赖谷歌搜索引擎的公司。谷歌的更新带来了更强大的AI功能，但也导致了一些原本排名靠前的网站被挤出搜索结果页面，给他们的业务带来了巨大打击。受影响的公司包括HouseFresh和Ready Steady Cut等。这些变化引发了对谷歌算法更新是否真的有助于网络的质量和用户体验的质疑。 </div>
                        <hr>
                    
                    <p></p><h2>谷歌更新算法，毁了多家公司</h2><p></p><p>&nbsp;</p><p>过去两年以来，谷歌搜索的一系列更新为这款互联网上最强大的工具带来了巨大变革，更配备了前所未有的AI功能。但最近，互联网上越来越多声音质疑，谷歌的一系列变化是在拯救网络，还是会将其推向毁灭？</p><p>&nbsp;</p><p>如果大家在谷歌引擎中输入过“空气净化器测评”，那想要获取的很可能是HouseFresh.com上的内容。该网站由Gisele Navarro和她的丈夫于2020年建立，整理了过去十年间改善室内空气质量的所有产品使用感受。他们在地下室里装满了各种净化设备，开展严格的科学测试，并撰写文章来帮助消费者们厘清思路、辨别炒作。</p><p>&nbsp;</p><p>HouseFresh就是由独立内容发布方推动建立活跃行业的典型案例。这些发布方所产出的原创内容，也正是谷歌长期以来号称应当推广的核心价值。实际上，就在该网站上线后不久，这家科技巨头就开始在搜索结果顶部显示HouseFresh。这让该网站迅速发展成一家欣欣向荣的企业，拥有15名全职员工。Navarro自己也为公司设定了颇具雄心的发展规划。</p><p>&nbsp;</p><p>但在不久后的2023年9月，谷歌对其搜索引擎算法展开了一系列重大更新。</p><p>&nbsp;</p><p>Navarro坦言，“这直接毁掉了我们的业务。一夜之间，本来指向HouseFresh的搜索词开始将人们引导至各大生活方式杂志，可这些杂志明显没有实际测试过产品。那里的文章中充斥着我一望而知的错误信息。”</p><p>&nbsp;</p><p>谷歌又在今年3月再次更新算法，这次造成的影响更大。HouseFresh的访客数量由每天数千人次锐减至数百人次。Navarro表示“我们完全被压垮了。”过去几周以来，HouseFresh网站不得不解雇掉大部分团队成员。她承认，如果后续情况没有好转，这家网站将唯有关闭一途。</p><p>&nbsp;</p><p>受影响的不止HouseFresh一家公司。</p><p>&nbsp;</p><p>英国娱乐新闻网站Ready Steady Cut的主编Daniel Hart也控诉谷歌改变搜索算法带来的影响可谓立竿见影。</p><p>&nbsp;</p><p>Hart解释道，“自从谷歌去年9月的更新之后，我们的流量当场减半，而且情况正变得越来越糟。我们不仅受到大网站内容的冲击，同时也正在被窃取我们内容的垃圾网站所取代。这样的整改毫无意义。”在接下来的几个月间，收入缩水已经迫使该网站将原本20人的作家与编辑团队裁撤至4人。</p><p>&nbsp;</p><p>谷歌方面的发言人则表示，该公司最近的更新已经给垃圾邮件和非原创内容造成了重大打击。谷歌也一直在密切关注导致搜索信息质量低下的滥用行为。</p><p>&nbsp;</p><p>谷歌算法更新之后，该公司向网站所有者发布了提示，号称能帮助其维持住搜索流量。但Hart指出，他们的网站聘请了顾问、重点关注谷歌的建议，而且不眠不休地更新网站。但经过近一年的努力，还是没有什么帮助。他表示，“过去8个月间，我浪费掉了宝贵的生命来努力遵循谷歌的建议。谷歌声称他们希望让网络用户能从掌握第一手经验和具备相关背景的人们那获取内容，可我们完全符合这样的标准。总之如今的情况实在让人心碎。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b3/b323b0d0bb06674cb3e1d7df09ce40c7.png" /></p><p></p><p>部分案例显示，谷歌搜索近期的变化正在对各类网站产生惊人的影响。</p><p>&nbsp;</p><p>谷歌一位发言人则在采访中强调，该公司的所有搜索算法调整都是在经严格测试验证、确认对用户有所帮助之后才会落地，而且谷歌方面也为各网站所有者提供了协助、资源和机会，允许其就搜索排名问题提出反馈。</p><p>&nbsp;</p><p>但批评人士认为，实际情况可能恰恰相反。随着谷歌重新调整其算法并使用AI将搜索引擎转化为搜索与回答引擎，不少人担心对于那些专司产出用户喜爱内容的企业来说，造成的冲击恐怕不亚于物种灭绝级别的事件。</p><p>&nbsp;</p><p>谷歌坚定认为这些变化将给整个网络带来好处，而搜索算法的调整只是个开始。</p><p>&nbsp;</p><p>至少有一点可以肯定：谷歌在AI上所做出的努力，将对大部分网民在网络上看到的内容产生深远影响。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d3/d3f93cd99c3a60b293dd2f2f0fc66fce.png" /></p><p></p><p>过去两年以来，所谓能让搜索变得更加“有用”的一系列更新，正在令努力遵循谷歌最佳实践的网站所有者感到沮丧。</p><p>&nbsp;</p><p></p><h2>谷歌AI工具“已读乱回”，工程师无奈手动删除</h2><p></p><p>&nbsp;</p><p>对搜索算法“动刀”后，谷歌最近又将注意力放到了AI工具上。这不，日前谷歌正因为社交媒体上充斥着的AI工具闹出的乌龙而忙得不可开交。</p><p>&nbsp;</p><p>这款谷歌新推出的AI综述（AI Overview）经常会胡言乱语，有时候让用户往披萨上抹胶水、有时候建议他们吃石头。面对这款匆忙上线产品搞出的麻烦，再加上互联网用户们用各种表情包大加嘲讽，谷歌正忙于手动禁用AI综述上的特定搜索内容。也正因为如此，很多表情包在被发上社交网络后不久就神奇地消失了。谷歌公司承认正“迅速采取行动”，旨在清除AI工具给出的一些奇怪答案。</p><p>&nbsp;</p><p>这样的现状着实令人摸不着头脑。毕竟谷歌测试其AI综述功能也有一年之久了——该功能早在2023年5月就以搜索生成体验的名号推出了beta版。谷歌CEO桑达尔·皮查伊更放出豪言，称该公司在测试期间已经支持了超过10亿条查询。</p><p>&nbsp;</p><p>而且皮查伊也提到，“在硬件、工程和技术突破的共同推动下”，谷歌同期将AI回答的交付成本降低了80%。看起来似乎是成本优化来得太早，而生成技术本身并没有做好准备。</p><p>&nbsp;</p><p>事实上，这款AI工具（AI Overview）是前不久皮查伊在公司年度开发者大会上，向众人宣布了其搜索引擎发展历史上最重大的举措之一。皮查伊表示，展望未来，谷歌搜索将针对诸多问题给出自己的AI生成答案。这项名为“AI综述（AI Overviews）”的功能已经面向美国用户推出。皮查伊指出，“这是一款能切实服务用户的产品。谷歌搜索由此成为建立在人类好奇心之上的生成式AI成果。”</p><p>&nbsp;</p><p>谷歌方面还表示，其AI综述产品旨在向用户输出“高质量信息”。谷歌发言人Meghann Farnsworth在采访邮件中回应称，“我们看到的许多案例都不属于常见查询，而且发现了不少被篡改或者无法重现的案例。”Farnsworth同时证实，谷歌方面正在“迅速采取行动，在符合内容政策的前提下适当删除某些AI综述查询，并利用这些案例对我们的系统进行广泛改进。部分改进结果已经在实际使用中得到体现。”</p><p>&nbsp;</p><p>由此可见，其实谷歌也承认了AI工具可能会提供不准确信息，但表示正在不断努力改进结果。谷歌公司发言人指出，AI综述的内容通常整理自多个网页，而非单一来源，而且响应结果会突出显示相关链接。这位发言人还提到，内容发布方可以在网页上使用特殊标签来控制AI综述是否列出相关网站链接。但需要注意的是，一旦AI模型抓取了创作者的内容，该数据可能将无法被删除。</p><p>&nbsp;</p><p>AI综述只是过去两年以来，谷歌对其核心产品做出的一系列重大改变中的一环。该公司表示，其最近针对搜索算法做出的改进努力将开启一个令人兴奋的技术新时代，并有助于解决困扰网络世界的诸多问题。</p><p></p><h2>成也搜索，败也搜索</h2><p></p><p>之所以要推动这些变化，是因为谷歌意识到此前的网络一直存在弊端。如果大家使用过搜索引擎，对此肯定也有切身体会。互联网的运作长期由所谓“搜索引擎优化（SEO）”所主导，这项技术旨在调整文章及网页内容，以便更好地被谷歌搜索发现并优先显示。谷歌甚至在为网站所有者提供SEO技巧、工具和建议。对于数百万将业务建立在机械化搜索体系之上的企业来说，SEO就是一笔他们不得不承受的“技术税”。</p><p>&nbsp;</p><p>问题在于，搜索引擎优化可能会被滥用。抱有野心的网站所有者也越来越多地意识到，相较于服务人类用户，专门制作适合谷歌筛选算法的内容才是增加经济收益的不二法门。</p><p>&nbsp;</p><p>谷歌针对垃圾搜索结果的战争已经愈演愈烈。2022年，该公司对其算法发布了“实用内容更新”，旨在淘汰纯为提升搜索排名而创建的内容。谷歌随后一截2023年9月发布后续更新，并在今年3月再次出手调整算法。谷歌方面表示，结果是“搜索结果中低质量、非原创内容减少了45%。”这似乎代表着一次巨大的成功。</p><p>&nbsp;</p><p>谷歌一位发言人在采访中表示，“我们最近的更新，希望将人们与来自网络的各类不同网站上的实用、令人满意且原创性的内容联系起来。在努力改进搜索服务的同时，我们还将继续专注于为网站提供有价值流量，以支持健康、开放的网络环境。”</p><p>&nbsp;</p><p>但谷歌的一系列改变，包括算法的更新和近期AI工具的出现，都没有博得什么好印象。</p><p>&nbsp;</p><p>一位不愿透露姓名的AI业务创始人在采访中表示，“谷歌曾经是一家以引领前沿、提供高质量产品的行业龙头，如今却不断发布各种质量低下的产品，甚至沦为整个互联网的笑柄和玩梗对象。”</p><p>&nbsp;</p><p>AI专家、纽约大学神经科学名誉教授Gary Marcus则在采访中表示，不少AI厂商都是在“兜售梦想”，希望更多人相信这项技术的正确率终将从80%提升至100%。Marcus强调，初步实现80%的正确率相对简单，因为其中涉及大量人类数据，其正确率天然就在这个区间。但弥合这最后20%的差距却极具挑战。实际上，Marcus认为这最后20%很可能是条死胡同。</p><p>&nbsp;</p><p>Marcus坦言，“对于很多问题，必须要经过相应的推理步骤才能判断当前事件是否可信、信息来源是否合法。而要想像人类审核员那样解决问题，恐怕首先要真正实现通用人工智能（AGI）。”Marcus本人和Meta公司的AI负责人Yann LeCun也都认定，为当前AI系统（包括谷歌Gemini和OpenAI GPT-4）提供支持的大语言模型并不是实现AGI的正确答案。</p><p>&nbsp;</p><p>这对谷歌来说，现在面临的处境无疑是十分艰难的。毕竟微软已经抢先一步，通过Bing大力推广生成式AI技术。另据报道，OpenAI正在开发自己的搜索引擎。而TikTok，正在为年轻一代用户提供最能满足其喜好的AI推荐体验。各方角逐之下，老牌巨头谷歌明显是感受到了竞争压力，最终导致整个生成式AI市场乱成了一锅粥。Marcus指出，2022年Meta曾发布名为Galactica的AI系统，但该系统在推出后不久即遭下架，因为它居然建议用户吃玻璃。吃玻璃、吃石头，看来Meta和谷歌的大模型倒是很有共同语言。</p><p>&nbsp;</p><p>谷歌倒是对其AI综述颇有信心并制定了宏伟计划，而目前已发布的功能只是其上周官定量内容的一小部分。针对复杂查询的多步推理、利用生成式AI组织结果页面，通过Google Lens实现视频搜索——谷歌的雄心壮志绝对不容小觑。但回归现实，谷歌的商业声誉无疑取决于其AI功能的实际表现，而目前来看其正确性实在堪忧。</p><p>&nbsp;</p><p>Marcus直言，“（这些模型）本质上无法对自己的输出进行健全性检查，而这样的现实正在拖累整个AI技术产业。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai">https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai</a>"</p><p><a href="https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet">https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</id>
            <title>禁令再升级！拜登政府已不想让中国人在美从事AI工作了，套壳大模型的公司也危险了</title>
            <link>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:25:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, AI产业, ENFORCE法案, 中国
<br>
<br>
总结: 美国通过《ENFORCE法案》收紧AI大模型出口，限制中国员工在美从事AI相关工作，旨在保护美国技术优势和国家安全。该法案一旦生效，可能对中国AI产业造成阻碍，包括数据供给和技术合作方面的影响。然而，中国也在加速自主研发步伐，将面临挑战但也有机遇。 </div>
                        <hr>
                    
                    <p></p><blockquote>美国已经不止一次提议从科技上对中国发起制裁，如果此次法案生效，又将对我国AI产业带来哪些影响？听听专家们的观点。</blockquote><p></p><p></p><h2>美国立法收紧AI大模型出口，连AI人才在美工作也受限</h2><p></p><p>&nbsp;</p><p>北京时间上周四，美国众议院外交事务部委员会以显著的多数票数，成功通过了一项旨在严格管控AI技术出口的法案。这项法案被正式命名为《加强海外关键出口国家框架法案》，通常简称为《ENFORCE法案》。</p><p>&nbsp;</p><p>值得一提的是，在该法案不仅限制了AI系统和大模型的出口，一旦法案通过，持有H1b 签证的中国员工或留学生可能需要特殊许可才能在美从事AI/ML相关工作。也就是说，这是明晃晃在限制中国人在美从事AI相关工作。</p><p>&nbsp;</p><p>《ENFORCE法案》由美国众议员共和党议员迈克尔·麦考尔（Michael McCaul）、约翰·莫伦纳尔（John Molenaar）、马克思·怀斯（Max Wise）和民主党议员拉贾·克里希纳莫西（Raja Krishnamoorthi）提出。其主要目标在于，通过强化美国商务部的权力，使其能够更加便捷地对AI模型实施出口管制，并进一步限制美国与外国实体在开发可能威胁国家安全的AI系统方面的合作。</p><p>&nbsp;</p><p>立法者表示，此举意在确保美国的技术优势和国家安全不受外部威胁。该法案的共同作者、众议院议员Michael McCauln (R-TX)&nbsp;表示：“人工智能引发了一场技术革命，它将决定美国是否能继续保持世界领先超级大国地位，还是会被中国超越。”</p><p>&nbsp;</p><p>McCauln表示他最担忧的是，“虽然美国政府的工业和安全局 (BIS) 有权限制人工智能加速器的出口——拜登政府曾多次利用这一点来扼杀中国在该领域的创新——但它缺乏监管人工智能模型出口的权力。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/27/2774a88c92338eebf9842c873338e7e7.png" /></p><p></p><p>《ENFORCE法案》于5月9日首次公布，由众议院的跨党派AI工作组提出，该法案旨在修订2018年出台的《出口管制改革法案》，这一年美国将14类新兴技术纳入出口管制。&nbsp;</p><p>&nbsp;</p><p>据悉，《ENFORCE法案》还需要通过众议院、参议院的全体表决以及总统拜登签署，才能落地生效。一旦此法案生效，将授予美国工业和安全局限制人工智能模型出口的权利，也促使白宫能够要求美国公司或个人只有获得出口许可证才能出口AI大模型。</p><p>&nbsp;</p><p>McCauln称道：“这项立法为 BIS 提供了灵活性，使其能够制定对封闭人工智能系统适当的控制，而不会扼杀美国的创新或影响开源模型。”</p><p>&nbsp;</p><p>值得一提的是，目前该法案实际上并不包含任何明确的保护或对开源模型的豁免，并且基本上涵盖了所有的人工智能系统、软件或硬件。</p><p>&nbsp;</p><p>需要明确的是，该法案的实际措辞意在模糊，并具体要求在法案通过后一年内更新“涵盖的人工智能系统”的定义。</p><p>&nbsp;</p><p>众议院议员玛德琳·迪恩 (D-PA) 在投票前解释道：“我们还在法案中对人工智能和人工智能系统的定义进行了临时修改，以便政府可以采取通常的监管程序并征求公众意见，从而对最终的定义进行适当的范围界定。”</p><p>&nbsp;</p><p>就在签订该法案的同一周，据彭博社报道，美国国会计划立法减少人工智能的潜在风险和危害，并每年至少投入320亿美元于人工智能研究，以促进美国经济和国家安全。美国参议院多数党领袖舒默（Chuck Schumer）表示，这项资金将让美国公司、大学和人才“保持在人工智能产业最前沿的地位”。</p><p></p><h2>如果法案生效，将对我国AI产业带来哪些影响？</h2><p></p><p>&nbsp;</p><p>据《21世纪经济报道》报道，清华大学人工智能国际治理研究院的李依栩向撰文对比了两部法案，表示本次法案主要是将AI技术纳入了《出口管制改革法案》的管制框架内，通过补充AI相关定义、赋予总统管制权、增加美国人从事AI模型出口相关活动的许可义务，对AI模型进行管控。</p><p>&nbsp;</p><p>比如，法案第三条扩展了总统管制权，管制对象是特定受限的AI系统、对美国国家安全至关重要的新兴和基础技术相关活动；</p><p>&nbsp;</p><p>法案第四条增加了一项额外权利，如果美国人在出口、再出口被确定为对美国国家安全至关重要的新兴和基础技术，包括设计、开发、生产、维修、翻新这些技术，美国总统有权要求他们申请并获得商务部许可。</p><p>&nbsp;</p><p>那么，这项比此前《出口管制改革法案》更加严格的《ENFORCE法案》一旦生效，对我国AI产业将带来怎样的影响？</p><p>&nbsp;</p><p>某头部电商技术总监Micheal Yan在接受《AI前线》采访时表示：</p><p>&nbsp;</p><p></p><blockquote>“短期内对于我国AI大模型发展会产生一些阻碍。尤其是在数据供给方面，国内开发者可能会面临数据短缺的问题。由于AI大模型的训练需要大量的数据集，如果美国限制对华出口数据集，这将直接影响中国AI模型的训练和性能提升。&nbsp;另外，在技术合作方面，中美两国在AI大模型开发过程中有着广泛的技术合作。如果该法案生效，这将会阻碍新技术与新模型的开发，因为中美顶尖开发人才的技术交流将受到较大影响。&nbsp;然而，需要指出的是，实施对华出口禁令的负面影响是双向的，对美国自身也存在不利之处。例如，在数据收集问题上，禁止对华AI模型出口同样可能让美国公司错失借助中国数据进行AI迭代的机会。从长远来看，中国有充足潜力自主发展、突破限制，而美国的这种限制措施可能会激发中国加快自主研发和创新步伐。美国限制AI大模型出口的政策不仅会对中国AI产业的发展带来挑战，同时也会对美国自身在某些方面产生一定的负面影响。”</blockquote><p></p><p>&nbsp;</p><p>面对一波接一波的科技制裁时，我国也在加速自主研发的步伐，这对于国产AI技术的发展来说也是一种机遇。</p><p>&nbsp;</p><p>平安集团前CSO、广东省CIO联盟会长李洋表示：</p><p>&nbsp;</p><p></p><blockquote>“如同过往的芯片、系统软件、应用软件等对中国的限制一样，美国对AI的限制出口接踵而至，但是AI的限制在目前中国大力推行国产替代的大前提下，是机遇大于挑战的。在这样的大背景下更加便于中国的科研工作者，丢掉幻想，重新布局。&nbsp;现实情况下是，AI所依赖的算力、算法很多都依赖于美国，当然也包括其他国家的算法、数据。但实际上中国的AI场景和应用体量非常大，在数据和算法层面不久后将不再受制于人。现代AI的发展不过几十年，即使是当前美国处于领先地位，也未必就永远处于领先。即便是如今众星捧月般的明星独角兽OpenAI也无法保证其大模型架构是未来AGI的正确技术发展路线，所以这个时候美国的限制对中国来说未必是坏事。&nbsp;希望中国的科研工作者，尤其是AI工作者们能够沉下心，在基础算法和模型研究中能够走出一条中国特色之路。</blockquote><p></p><p>&nbsp;</p><p>李洋还表示，“对于AI人才赴美的限制，我觉得也不是什么大问题，中国的AI基础研究和应用市场相对于美国来说都还是蓝海，立足于中国的产业，将AI人才留在中国服务，利大于弊。况且，美国所谓的这些限制，在当今时代下，也不会是一揽子的全封闭，所以我们应当审时度势，抓住这个机遇。”</p><p>&nbsp;</p><p>法案公布以来，除了担忧对于AI人才和大模型的限制外，外界还会担忧对于开源大模型的限制会让许多国内“套壳”大模型企业很难受。</p><p>&nbsp;</p><p>对此，李洋表示，“开源模型虽然现在美国比较领先，但是其他欧美国家也不乏相应的开源产品，我们在这个阶段还可以多方面借鉴和研究。并且，基于我们的自主创新体系，我们还是要沉下心来研发自己的AI基础底座，包括硬件基础设施、开发平台和大模型及其应用，所以短期内会对一些套壳的中国公司产生一些影响（包括模型的演进、应用和商业化等等都存在相应的限制）。”</p><p>&nbsp;</p><p>“但是从技术和应用上来讲，进口的开源模型也不是完美和最终的AGI的路标和唯一标准，所以在这个时候，也希望我国AI企业能够立足于自身的能力打造，丢掉幻想，与中国生态和国际生态一道打造出中国的AI开源版本，为中国和国际做出自己的贡献”。李洋说道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html">https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html</a>"</p><p><a href="https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/">https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</id>
            <title>Agent 还没出圈，落地先有了“阻力”：进入平台期，智力能否独立担事？</title>
            <link>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 10:12:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AutoGPT, AI Agent, 具身智能, 大模型
<br>
<br>
总结: 作者介绍了AI Agent的当前能力和应用潜力，讨论了在企业场景中有效利用Agent的重要性，以及未来发展趋势。同时，还探讨了AI Agent在不同领域的具体应用场景，以及与大模型的区别和发展方向。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p></p><p>去年出圈的AutoGPT，让AI Agent来到大家的视线中并迅速爆火，大家都对Agent抱有极高的想象力与期待值。那么，Agent现在到底有多大的应用潜能？企业要如何抓住？同时在具体的落地实践方面，也有不少悬而未决的挑战。</p><p></p><p>带着这些问题，InfoQ《极客有约》特别邀请了阅文集团 AIGC 技术负责人马宇峰担任主持人，与机器姬CTO&amp;具身智能一百零八讲主讲人刘智勇、华为云aPaaS首席架构师陈星亮，一同探讨AI Agent的当前能力、应用落地情况以及未来发展趋势。部分亮点如下：</p><p></p><p>Agent不仅仅是一个玩具，而可以改变现实世界。在企业场景中有效利用Agent，合理选择业务场景非常重要。具身智能领域最大的挑战在于操作层面，瓶颈在于如何泛化地执行物理世界中的各种操作。未来使用Agent和大模型将成为企业员工需要掌握的技能。人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p><a href="https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT">https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT</a>"</p><p></p><h2>AI Agent当前的能力</h2><p></p><p>马宇峰：首先要谈的就是AI Agent现阶段的能力，大家现在是如何应用AI Agent的？具体落地场景有哪些？</p><p>刘智勇：最近大家可能已经注意到了一个名为“Figure”的机器人，演示中，工作人员向该机器人表达了饥饿感之后，Figure成功地将苹果递给了他；这一过程展示了AI Agent在物理世界中进行任务推理、规划并最终转化为实际行动的能力。在具身智能领域，AI Agent的应用场景非常广泛，AI Agent可以大致分为以下四个方面。</p><p>工业场景：在工厂中，具身智能机器人可以应用于3C生产线或汽车总装线，提高生产效率和自动化水平。商业服务场景：在商业环境中，具身智能机器人可以提供接待、讲解、导览、巡逻和配送服务，改善客户体验，提升服务质量。家庭场景：在家庭环境中，具身智能机器人可以承担清洁服务或家务工作，减轻人们的负担，提高生活质量。火星建设：在未来的火星探索和建设中，具身智能机器人有望发挥重要作用，帮助人类在恶劣的外星环境中进行建设和研究。</p><p>对于这些应用场景，具身智能都展现出了巨大的潜力和希望，为未来的技术发展和应用提供了广阔的前景。</p><p>陈星亮：针对企业场景进行AI Agent能力创新时，多数是从IT场景开始的，因为该场景拥有较为完善的信息化基础。在这一过程中，我们遵循两个主要原则：一是先易后难，我们首先从普遍性场景开始，然后逐步向专业化场景演进；二是保障效果，无论开发哪种场景的AI应用，都必须确保其有效性。</p><p>办公和编码领域被广泛认为是AI Agent应用的切入点，因为这些场景相对通用，容易实现。随着技术的进步，我们将AI Agent的应用延伸到更复杂的场景，例如：</p><p>办公领域：AI Agent可以用于自动生成会议纪要或设计文档，这些任务比简单的代码生成或文本创作更具挑战性，需要更深层次的场景理解和更高级的语言处理能力。销售或服务领域：AI Agent可以用于合同审核或法律条文的辅助生成，这要求AI Agent不仅要理解法律术语，还要能够处理复杂的逻辑关系。网络设备监控：在对网络设备进行监控的基础上，AI Agent可以执行自动巡检任务。这要求AI Agent不仅要处理专业数据，还要能够理解并应用信息化积累的知识。</p><p>马宇峰：大家首次接触到AI Agent大概是在什么时候？从本质上讲，AI Agent与大模型的区别究竟体现在哪些场景上？最核心的区别是什么？</p><p>陈星亮：Agent 这个概念，实际上在大模型出现之前就已经存在了。在进行 IT 系统集成或设计某些自动化流程时，其实已经有Agent 这一层了，尤其是在设备与外界交互的环节，而那时还没有将大模型技术整合进来以实现更广泛的泛化能力和生成式能力。</p><p>大模型技术引入后，起初我们并没有考虑将其应用于设备控制或高度交互性的 IT 系统交互中，而主要看中其在创作和生成内容方面的潜力。之前我们在设备代理方面的工作与 AI Agent 的概念思路颇为相似，只是随着大模型的加入，AI Agent 的能力和应用场景都发生了变化。当我们将这些结合起来后，认识到了 AI Agent 的真正面貌。因此，如果仅从 IT 系统的能力角度来看，AI Agent 这个概念并不神秘，不过是通过引入大模型为 AI Agent 带来了更多能力，从而丰富了其功能。</p><p>刘智勇：无论是ChatGPT还是Agent、具身智能，本质上都是在以下三个方面进行发展。</p><p>文本世界：在文本领域，大语言模型展现出了强大的生成和理解能力，这主要体现在ChatGPT等应用中。数字世界：数字世界中，我们需要利用规划、循环和反思的控制机制，实现任务从开始到结束的全流程控制，并调用数字世界里的外部工具进行执行。物理世界：物理世界中，Agent的能力落地体现在具身智能上，即通过具身智能技术将规划形成的任务序列转化为物理世界中的实际操作。</p><p>马宇峰：我分享一下第一次接触Agent的经历，去年夏天OpenAI开发了一项名为“Function Call”的能力，虽然看起来仍然是文本的输入和输出，但当函数作为一个字符串被输出并被精确调用时，我确实看到了Agent的不同之处。以前我们认为创作和创意不确定性是大语言模型最人性化的特征，但同时它们也有机器的一面，能够在有限的范围内唤醒某些函数。这项能力让我意识到Agent应该被独立考虑，其围绕工具使用、规划和执行的能力，可以帮助大模型结合现实世界中的数字和物理能力，形成一个更完整、更通用的解决方案。这是我对Agent概念的一次认知冲击。</p><p>然而，随着时间的推移，我发现Function Call可能并不像我最初想象的那么好。它演示的技能是查询天气，虽然可以很好地执行，但许多场景要复杂得多，可能不只有10个或20个函数可供调用，会出现完全不确定的函数，下一步该执行哪个函数也会是未知的。不过，Agent的主流能力，如浏览器的唤起、搜索引擎的查询结果以及一些生成能力的唤起，确实有效地让它从概念走向实际。当然，在实际应用过程中，我们也发现了许多不确定因素，但Agent的能力已经让我感到惊讶，它不仅仅是一个玩具，而可以改变现实世界。</p><p>回到Agent 的适用场景，我分享一下个人自身在探索中使用的直观感受。使用Agent能力可以批量生成自媒体文章，也可以像模像样地讲一个故事，从创建角色、制定纲到将角色和情节融合，再逐步生成内容，它的成文速度非常快，也有一些优点，比如生成过程中，可以将角色单独抽象出来去形成可视化的元素，可以使用多个角色和情节引导来发展内容片段，且在逻辑框架内是可控的。</p><p>但深入研究后我们发现另一个问题：Agent输出的内容，还是没有达到人类所能达到的逻辑性、创意性相结合。业内也做了很多尝试，这方面却似乎一直停留在中等或中上水平，整体表现平庸，所以这确实是长期困扰我们的问题。虽然我们最初认为Agent很有用，但在商业化和变现能力上似乎没有那么强。</p><p>想问一下陈老师，在代码和办公场景，Agent 可以从哪些方面提升效率？有哪些bad case？</p><p>陈星亮：我先谈谈Agent 给一些稳定场景带来的效率提升作用，如设计文档生成和合同中法律文本的生成等。在一些应用场景相对明确、法律条文引用也相对模式化的特定领域，如可靠性设计或安全威胁设计，Agent的表现在业务用户看来感知和体验都非常好，准确度也相当高，显著提升了工作效率。目前，我们也在将Agent应用于网络设备巡检等生产场景。尽管巡检过程中会遇到各种意想不到的问题，但对于那些已有案例库和解决方式库的巡检，Agent 都能够发挥作用，并帮助提高巡检效率、简化人力的工作。</p><p>然而，也有一些不尽如人意的地方。Agent刚推出时，大家对它寄予厚望，导致在选择应用场景时没有过多限制，业务团队提出了许多要求较高的场景，想要用Agent去解决未知的问题。这些要求的实际难度很大，而Agent在处理未知问题时的能力有限。因此，如果要在企业场景中有效利用Agent，合理选择业务场景非常重要。否则，Agent的效果可能不会达到预期，甚至可能非常差。</p><p>马宇峰：如果人类都做不到的事情，期望Agent达到超越人类的水平是非常困难的。相反，那些人类已经重复做了很多遍且已经规范化的工作，确实可以将人类的判断力解放出来，完全交给Agent来自动化处理。在具身智能的Agent应用上，哪些方面是可行的？可能存在什么挑战？</p><p>刘智勇：首先，具身智能的输入需求依赖于视觉语言模型，这意味着需要处理整个环境的三维数据信息，而不仅仅是二维图像。它需要的输入包括深度数据、RGB图像等，可能还要结合触觉、反馈力以及编码器数据等，这些数据共同构成了具身智能的全面输入。因此，在数据输入的方式上，具身智能与传统Agent存在显著差异，这些差异带来了巨大的挑战。</p><p>其次，在数字世界的Agent中，无论是什么类型的Function Call，基本上都是可执行的动作，操作层面通常不会遇到问题。然而，具身智能中存在一个可供性问题，即是否能够真正执行某个动作。尽管存在这些挑战，但也有一系列方法可以解决这些问题，如具备泛化能力的视觉语言模型、迭代细化的机制、自我反思的机制等。目前来说，具身智能领域最大的挑战在于操作层面，即具身操作。感知、决策和规划虽然重要，但真正的瓶颈在于如何可泛化地地执行物理世界中的各种操作。</p><p>马宇峰：Agent目前的发展状况如何？是否已经达到了一个平台期，还是仍然有很大的提升空间？是否依赖于某些特定的背景？</p><p>我认为Agent主要依赖于大模型的Function Call能力，需要准确地识别出当前调用哪个模型来完成当前任务，并提供相应的结果，以便大模型进行下一步操作。而瓶颈可能在于读取上下文的长度，上下文长度决定了能够识别多少个函数。Agent在执行过程中受限于场景，只能在有限的函数中进行选择，其执行也不完全精确；如果执行不精确，就需要获取更多的环境信息或反馈信息来执行函数，过程中可能会出错。Agent是一个精妙但不够鲁棒的系统，如果它返回到上一级并根据错误信息重新执行，可能会带来更大的资源消耗和时间延迟。</p><p>陈星亮：在企业场景中实施Agent时，我们首先需要考虑的是技术的可实现性。在挑选场景的过程中，就要考察技术是否可行；一旦场景确定，接下来需要考虑的是如何提高Function Call的准确度，如果准确度不够高，需探索其他工程手段来提升API的识别准确率，甚至在语义理解之后通过额外的工程能力进行调整、校验生成的API并通过查询方式进行补充。企业面临的最大挑战之一就是需要重复性地进行这类工作。目前我们也在探索长序列处理、记忆的短、长期存储以及上下文空间的扩展等技术，以期在未来实现更多的技术突破。</p><p>在具身智能领域，企业场景中也在逐渐引入多模态技术，尤其是当与操作技术领域（OTA）的设备关联时。多模态技术的引入包括传统的视觉识别等，将进一步增加系统的复杂性。如果大模型在这些领域取得显著进展，那么在企业IT融合场景中的工程难度将大大减少。目前，我们在工程实践中仍需进行大量技术工作，这些工作的管理复杂性甚至超过了传统的微服务架构。</p><p>我相信，随着技术的进步，未来将有很大的空间来改进现有的工程能力，减少人工干预，让大模型承担更多的工作。无论是让大模型自行处理，还是让Agent框架沉淀出更多稳定的框架性技术，都是未来技术发展的趋势。我对大模型在未来的迭代和改进抱有很高的期待，相信它们将带来更好的效果，并减轻当前工程化实践中的一些负担。</p><p>刘智勇：从阶段性的角度来看，我们认为具身智能目前处于技术起步期，未来的发展空间仍然非常广阔。之所以称之为技术起步期，是因为目前还存在三个方面的挑战：</p><p>任务类型的泛化性：这涉及到Agent能否理解各种类型的指令，并能够完成具体的规划而不产生幻觉，抑制Agent在理解上的偏差，对齐人类意图的二义性和潜在偏好，确保其能够准确执行任务。环境的泛化性：即Agent快速与环境对齐，对齐环境的规律、动态性和随机性。操作的泛化性：这是更为复杂的挑战，涉及如何利用多种数据源采集更多的线下数据，并据此训练出能够泛化到不同情境的具身操作模型，目前行业中还没有一个非常好的解决方案。</p><p>从这三个方面的挑战中，我们看到了未来的发展机会。尽管目前还存在许多问题需要解决，但这同时也是推动技术进步的动力。</p><p>观众提问：是否可以认为大模型做好了就不需要 Agent 了呢？</p><p>刘智勇：大语言模型的主要功能是处理和生成文本，核心在于将文本信息进行向量化处理，并通过Transformer架构以及监督学习机制，实现技术上的范式转变。这些技术基础的迭代，再结合大量的数据和强大的算力，促成了ChatGPT等大语言模型的诞生，它们在文本生成和回复方面表现出色。</p><p>尽管大语言模型在文本领域取得了显著的成就，但本质上只具备基于零样本提示词的文本回复的能力，而不具备执行实际任务的能力。这意味着，无论大模型在文本处理上多么先进，它们仍然需要Agent的介入来实现从文本到行动的转变和全流程的处理。</p><p>因此，大模型和Agent是两个不同的概念，前者专长于文本交互，而后者则涉及到任务的执行和落地能力。简而言之，大语言模型缺乏将文本回复转化为实际行动的能力，是典型的缸中之脑。</p><p>马宇峰：如果大语言模型发展到某个瓶颈无法提升，那也可以像两个人类合作思考能更高效地完成工作一样，使用两个大模型实际上可以进一步提升当前水平。哪怕提升的幅度不大，但考虑到大模型的较高的基础表现，即便是小幅提升也可能带来非常显著的回报，并且能够有效地增强现有能力。至于这些能力是否会直接集成到大语言模型中，我认为在相当长的一段时间内，我们仍然可以将大语言模型视为一个智能体，主要从智能逐步思考的角度来使用它。</p><p>陈星亮：aPaaS主要是基于行业内现有的资产或经验，实现程度化代码开发，降低开发门槛，通过拖拉拽的方式快速构建简单的应用程序。随着大语言模型代码生成能力的出现，零/低代码平台受到了较大的冲击。曾经有观点认为，大模型的出现可能会使得低代码或零代码的开发方式变得不再必要。实际上，我认为情况并不会如此。</p><p>零/低代码平台可以有效地融合大语言模型的能力，让大模型直接参与代码生成。以前需要通过拖拉拽来实现的功能，现在可以通过自然语言处理（NLP）的方式进行交互，提供更直观、友好的用户体验，并帮助理解业务用户原始的语意，以更好地生成低代码或零代码应用。我认为零/低代码平台和大模型之间更多的是一种合作关系。低代码平台上已经积累了大量的业务资产，而大模型可以将其作为插件调用，两者结合将发挥出更大的潜力。</p><p></p><h2>AI Agent的落地挑战</h2><p></p><p>马宇峰：在大语言模型不提升或通用大语言模型更新周期较长的情况下，如何利用现有工具和能力取得良好成果？有哪些方法或策略？</p><p>尽管当前AI Agent面临许多瓶颈和困境，限制了其应用范围，但仍有一些方法可以提升其驱动能力，如可以通过垂直领域的强化训练、特殊训练技术或更巧妙的方法，在不提升大语言模型本身能力的前提下改善Agent的表现。Agent在当前大语言模型框架下的表现，不仅取决于模型本身，还受到其他多个环节的影响。即便大语言模型不是限制因素，其他环节的优化也能提升整体Agent的效果。以Kimi为例，它之所以能够脱颖而出，可能确实在大模型的某些方面做了针对性强化，但重要的是它对文档类型的解析能力有效提升了实际操作中的使用体验。Kimi能够在处理长文档时进行分块，并采用迭代检索的方式输出答案，这大大增强了Agent在特定场景下的应用体验。</p><p>我相信，即使在大语言模型能力不变的情况下，只要充分提升检索能力，就能显著提高最终的可用性和准确率。很多时候未能获得准确答案，是因为没有找到正确的信息片段。如果知识库足够丰富，片段足够多，那么作为一个智能整合的搜索引擎，Agent将具有巨大的应用潜力。在大语言模型能力不完整的情况下，只要把某个小模型、小工具或阶段（如检索阶段）做得足够好，也能显著提升Agent的整体表现。</p><p>刘智勇：要提升AI Agent的能力，首先需要充分挖掘并利用长期记忆，通过RGBD摄像头读取的数据，结合视频语言模型，形成丰富的语义信息。在特定场景中，这些语义信息往往是重复出现的元素，关键在于如何有效地保存信息，为后续的规划提供坚实的基础。随着时间的推移，语义信息不断积累，AI Agent的长期记忆能力将变得更加强大。</p><p>其次，进行迭代细化是提升AI Agent能力的另一个关键点，这意味着需要不断结合当前的模糊指令和新获得的语义信息，形成新的提示词。通过不断的迭代询问，AI Agent能够逐步细化和精确化其理解和响应，通过不断反思，最终达到更加精准的结果。</p><p>陈星亮：企业内部考虑事务时主要关注两点，都与数据紧密相关。首先是文档处理的问题，在企业中，非结构化文档往往是承载信息的主体，处理这些文档不仅要识别文档类型，还包括对复杂文档的解析，如图文混排和包含复杂表格的文档。这些内容在原有的基础上，需要对文档类型识别的范围进行扩展，但在企业内部对这种复杂文档的解析仍是一个较大的挑战。</p><p>其次关于原有数据的利用问题，特别是在生产场景中，一般都具备专业领域的背景。以设备巡检为例，它与设备的领域知识密切相关，这种情况单靠企业自身的私域数据积累可能不够，需要在行业内去做垂域模型。目前，我们期望通过Agent技术的发展，能够让更多企业在通用场景中体会到Agent带来的好处，从而愿意将自己内部的结构化数据进行区分，将企业机密数据与可对外开放的数据分离，并逐步开放一些行业公共数据，这将有助于构建每个行业的垂直领域模型，为未来企业场景和Agent的发展带来巨大的好处。</p><p>马宇峰：初期部署Agent的成本是否高昂？是否能够带来相应的收益？能否实现成本的回收和价值回报？</p><p>陈星亮：企业部署Agent时，成本问题是一个必须考虑的重要因素，并且需要结合业务团队的期望以及对目标的评估来共同考量。初期企业主要探索通用场景时，成本通常是较低的。随着业务场景的成熟，以及越来越多的用户和业务团队成员开始使用这些场景，成本就会开始上升。特别是当场景全面开放并开始构建更多场景时，就可能需要多套模型和版本，模型也需要不断地做飞轮进行迭代和优化，成本可能会指数级增长。</p><p>因此，在正式对外放开并大规模使用Agent之前，与业务团队进行充分沟通和期望管理是非常重要的，需要让业务团队明白，业务场景真正对外开放并吸引大量用户使用后将会涉及到哪些成本。同时，业务团队也需要评估这些成熟场景能够带来的价值，如对客户满意度和内部效率提升的贡献。当业务团队获得这些信息并进行综合评价后，他们对预算和投入的决策将会更加明智，这样的过程有助于确保Agent部署的成本得到合理评估和控制，并带来相应的价值回报。</p><p>刘智勇：Agent部署的成本考量包括云端的调用成本、机器人本体的计算成本以及整体的部署成本这三个主要方面。</p><p>云端数据成本。这涉及调用模型的频率，如果实时观察环境中的语义信息，就需要频繁且快速地调用模型，这样会耗费大量的计算资源，从而产生高额成本。因此，必须考虑调用频次和计算资源消耗的问题，实现具身智能体和自身限制的对齐。机器人本体成本。在具身智能场景中，机器人本体通常需要具备一定的计算能力。为了使机器人能够在不同场景中应用，无论是商业、工业还是家庭环境，都希望能够在端侧部署大模型，尤其是本地部署，而这在没有高端GPU和显寸的支持的条件下尤为关键。部署成本。将设备分布式放置在不同地方会产生额外的成本，此外还需要考虑是否能够通过启发式方法或其他手段，让设备快速启动并投入使用，这也是降低部署成本的一个重要方面。</p><p>马宇峰：部署成本确实是一个值得讨论的话题。在实验性质的探索阶段，对时间的考量可能并不严格，但一旦考虑到响应速度，比如每秒需要处理多少个token来执行动作，成本问题就变得尤为突出。因为模型需要很长时间才能完成一个Agent的输出，这意味着直接使用大模型进行推理的成本和时间的耗费都是非常高的。对此，我个人建议可以利用一些框架，如Dify或Coze，它们可以帮助构建Agent框架，并提供了丰富的工具来逐步检查生产和输出的结果。</p><p>企业部署Agent时， 可以先验证整个流程是否可行，并确保其能带来业务价值。之后，可以考虑用一些专门训练的小模型来替代流程中的关键部分，以降低成本。初期可以利用现有的资源进行探索，长期来看，通过特定化的方式进行优化和部署可以平衡成本。</p><p>观众：在部署Agent时可能会遇到哪些安全方面的问题？目前是否有一些比较成熟的工具可以用于保障Agent的安全性？同时，是否可以认为Agent的安全性主要取决于其底层大模型的安全性？</p><p>陈星亮：首先，Agent的安全性并不仅仅由大模型决定，模型安全只是一部分，还涉及应用安全和数据安全。对企业来说，对安全性的投入无论多少都不为过。无论在引入模型时，还是实际使用过程中，包括Agent框架都需要进行安全检查。例如，使用开源框架组件时需要进行安全审查，运行时需要对模型的输入输出内容进行监控，以及对应用框架进行访问控制，防止调用越权等。</p><p>在企业原有的安全体系内构建Agent的安全性会更好一些，在华为云内部，我们基于AI原生应用引擎等平台，当Agent对外提供服务或与模型进行交互时，利用内部原有的数据安全、应用安全和内容安全方面的技术，对内容进行检查和过滤。Agent的安全性需要在现有基础上，结合Agent之间的技术组件交互以及场景特有的安全要求来综合考虑和实施。</p><p>观众：面对多智能体协同框架的开源与闭源发展，应该如何选择合适的技术路线和框架，以减少试错过程并确保系统不会被行业不断的更新迭代所淘汰？</p><p>陈星亮：我认为应该分开考虑。对于Agent的开发框架，目前开源的选择比较多，都有很多可用的资源。鉴于Agent领域本身正在快速发展，选一个团队成员熟悉且操作顺手的框架，然后跟随其发展进行使用。而对于Agent的运行时环境，进入企业生产环境后，我建议使用闭源解决方案。理想的状态是，在企业现有的基础设施基础上进行必要改造，以便将Agent的运行时环境纳入统一管理和运维体系中，确保运行时的稳定性和安全性。</p><p>刘智勇：我更倾向于观察一个技术方案是否展现出成熟和收敛的迹象，如果开始趋于稳定和收敛，那可能是着手开展相关工作的更适宜时机；如果尚未成熟，还在快速迭代和变化之中，那可能面临开发速度跟不上开源社区更新速度的问题。</p><p>马宇峰：在选择开闭源路线时，实际上需要根据所处的发展阶段来决定。不管选择何种路线，实际投入使用比纠结于何时开始尝试和如何减少错误更为关键。毕竟，随着时间的推移，技术本身会更新迭代，这些更新往往也会朝着更优化的方向发展，对业务发展带来积极的价值。</p><p></p><h2>AI Agent的未来前瞻</h2><p></p><p>马宇峰：从长远来看，企业中Agent的落地是否会对某些现有的职业造成冲击？比如普通员工、现有商业模式、提供API服务的SaaS公司以及供应商等。Agent的普及和应用会带来怎样的影响？</p><p>陈星亮：对于员工而言，随着技术的发展，未来使用Agent和大模型将成为他们需要掌握的技能，尤其是提示词。员工至少需要学会如何使用Agent，就像现在进行零代码应用开发一样，将其作为日常办公工具的一部分。对于企业，尤其是传统SaaS公司来说，Agent和大模型的引入已成为明显趋势。一些大型SaaS公司，已经开始将大模型集成到平台中，将Agent框架和集成外部大模型的能力嵌入到二次开发和应用中。传统SaaS公司如果不加入到这个发展潮流中，可能会影响产品体验，建议一定要去拥抱大模型和Agent。</p><p>刘智勇：我从两个不同的角度来探讨Agent的运用及其对未来的影响。对于企业而言，利用大模型或Agent的主要目的是提升工作效率和减少对人工的依赖。有时员工的工作效率确实无法与Agent相比，特别是在一些技术性任务上，初级工程师的编码能力可能远不及代码Agent。对于工程师来说，积极利用Agent不仅是为了保持个人竞争力，也是为了适应未来工作的需求。Agent可以作为一个强大的工具，帮助工程师完成更高效和更复杂的任务。</p><p>而具身智能特别是人形机器人，预计它们对未来世界的冲击将是巨大的，会在商业、工业和家庭三个领域中体现出来。在商业领域，许多展示和演示类的工作岗位可能会被智能机器人所取代，因为它们可能表现得更好。在工业场景中，很多重复性或技术性工作实际上可以由机器人来完成，提高生产效率和安全性。在家庭环境中，未来也可能会出现更多类型的服务机器人，帮助处理日常家务。同时，我们也应保持谨慎乐观的态度，认识到技术发展和应用普及的速度可能没有想象中那么快。机器人和Agent的发展旨在辅助人类，使我们能够专注于更有价值和创造性的工作。</p><p>马宇峰：关于Agent如何影响我们的现实世界，尤其是在工作场景中，我的感觉是需要先拥抱这些变化，然后学会适应和改变。现在可能是小企业创业的绝佳时机，因为借助大语言模型这样的“万能胶水”，不再需要像以前那样协调大量资源来进行服务能力的交付，只需要尝试不同的组合，就可以高效地为客户提供解决方案。这样，小企业的服务能力从完全定制化转变为可以大规模扩展的模式，这对于二线市场可能是一个深远的改变。</p><p>观众：英伟达使用虚拟环境训练智能体机器人的方法是否可以拓展到所有应用场景？虚拟环境数据能在多大程度上替代现实场景数据？</p><p>刘智勇：我们实际上已经使用过英伟达的Isaac Gym来训练智能体，主要是进行强化学习的训练。这种方法涉及合成不同的仿真环境，并基于此来进行强化学习的训练。这种方法的主要优势是数据是免费的，但存在一个从仿真到现实（sim-to-real）的转换gap。在应用拓展方面，特别是在本地运动（locomotion）即行走部分，使用强化学习和虚拟环境的训练模式效果是不错的。对于一些操作类的任务，也有一些积极的应用特点。但对于更广泛的操作任务，可能更倾向于使用采集到的真实数据，并利用transformer架构来训练大型的transformer模型。因为在英伟达的仿真环境中，很多物理引擎的细微数据是无法被完美仿真的，如一些非常精细的触觉反馈。</p><p>马宇峰：在内容行业，我们对AI技术的发展有着深刻的感受。有人认为，Sora的成功是因为学习到了物理世界的真实性，但随后有人指出，Sora可能只是选择了一些优秀的片段来展示。Sora的训练采用了虚幻引擎，但这种方法还是单一的，并不一定能够真正理解物理世界。这与刘老师提到的英伟达的反馈机制可能有所不同，它们的输入输出机制存在差异。目前，Sora虽然理念上很先进，但实际上还没有达到通过虚拟化的输入输出来获得真实物理引擎的效果，可能是因为模型的参数规模不够大、训练数据不够丰富，或者受虚拟数据本身的限制，还需要进一步深入观察和研究。</p><p>观众：请介绍一下目前单智能体落地的情况，以及它与公司当前技术架构的结合方式。多智能体的具体架构是如何建设的？</p><p>陈星亮：在原有的技术架构体系中，目前大家使用的较多的是Web应用、微服务，有时还会使用函数技术体系。我们可以将Agent和大模型引入进来，先进行隔离，用于特定的场景。这些场景必然会与现有的微服务体系或函数体系进行交互。这时可以采用集成的方式进行，而不是直接使用大模型的Function Call方式。这样实施难度会小一些，而且也能让Agent发挥作用。当技术团队逐渐掌握了Agent和大模型这套技术，就可以开始取代一些现有的应用。这样的过渡不仅有利于架构的演进，也有助于技术团队的能力培养。</p><p>马宇峰：多智能体协同是一个复杂而富有挑战的领域。项目中，多智能体的协同运作被分解为不同的角色，如项目管理员、编码者、产品经理等，各自承担不同的职责。然而，如果单个智能体（单Agent）的运作还没有完全搞明白，就急于发展到多智能体（多Agent）协同，其实是存在很大风险的。</p><p>在实际应用中，比较常见的模式是有两个智能体协同工作，一个负责生产，一个负责评估，但目前还没有看到这种模式带来特别显著的提升。举一个例子，情感陪伴场景中有大量的对话交互，如果一个人与一个IP进行对话，输入输出的比例可能不太理想，引入第三个智能体会带来信息量的显著提升，这在满足用户情感需求和具体任务需求时非常有用。</p><p>接下来，我们继续研讨AI Agent的未来。目前，AI Agent的进展可能在技术行业内比较流行，但还没有真正“出圈”。当AI Agent被充分使用时，哪个场景会是理想中的未来？</p><p>刘智勇：图灵测试是一个经典测试，用于评估机器是否具备人类智能，即在背靠背的情况下，判断对方是计算机还是人类。而我曾经提出过一个“面对面测试”，特别适用于人形机器人，尤其是高端的女性机器人。测试中，高端女性仿真机器人被指派到一个地点，与10位单身男性分别进行相亲，如果其中有9位男性最终发出了下一次约会的邀请，该机器人就通过了测试。这不仅考验机器人是否能够理解并执行任务，还考验它是否能够与人进行有效沟通和交流。如果机器人能够在这种面对面的互动中展现出高度的智能和亲和力，那么无论在用户交互、情感表达、行动能力还是外观颜值上，其都达到了非常高的标准。</p><p>通过这样的测试，机器人将展现出巨大的商业价值。因为当机器人在面对面互动中具有很好的亲和力时，就可以在各种职业领域中得到应用，包括教师、律师、前台接待、演艺、直播等各个领域。我认为，这种更泛化的Agent通过“相亲测试”的事件是一个标志性的里程碑，意味着AI Agent的能力和人机交互能力已经达到全新的水平。</p><p>马宇峰：Agent未来的发展趋势将是怎样的？当它们发展到一个成熟的阶段后，将会呈现出什么样的形态？</p><p>陈星亮：在企业场景中，Agent目前主要扮演辅助角色。我们正在考虑的是，Agent是否能够从围绕特定场景服务转变为围绕特定人员服务。随着这一趋势的逐步发展，我们可以设想，未来某些人的工作是否会逐渐被Agent取代，这可以在分工上进行明确划分。我认为，当Agent真正能够在企业中提升效率并降低成本时，就达到了真正的成熟阶段。</p><p>马宇峰：Agent落地过程中可能会遇到一些难以实现的场景，这就需要我们在筛选场景和逻辑执行上进行深入的思考，最终的理想状态是以人的方式来看待Agent：作为智能体能够取代当前人类的多少工作。Agent概念并非是大模型出现后才产生的，但确实又是一直存在的。智能体这个词，最常见的体现可能就是人类自己。人类可以作为Agent 选择任务难度的度量，同时也可以作为Agent的驱动方向。</p><p>如果有一天Agent真的取代了所有的工作，人类应该干什么？我想，这时人类最重要的价值就是发挥自己的需求。Agent服务的目标永远是人本身，人类有需求，才会有Agent去做这件事情。人类是需求的发起方，Agent只是去满足需求的一方。因此，人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的，除非Agent拥有像人一样的肉身，有自己的激素欲望和生理限制。</p><p>陈星亮：首先，我认为Agent的未来是充满无限可能的。无论是在各个行业，还是在ToB或ToC的体系中，人类社会有各式各样的场景需要Agent来提供支撑，因此它的发展前景是极其广阔的。其次，我相信Agent将是一个多样化的存在，无论是在技术实现还是在业务场景的应用上。目前Agent技术的发展呈现出百家争鸣的局面，这对技术行业来说是一件好事，意味着有更多的行业场景愿意尝试采用Agent，并进行投资。在这样的投入下，技术可以快速发展，进而更好地探索未知领域。</p><p>最后，在Agent向前发展的过程中，我们也需要正视现实情况。当前无论大模型还是Agent框架本身的发展，下一步的方向似乎还不是很清晰。我相信未来还会有更多新技术不断涌现，将推动Agent的发展，使企业和个人的诉求和场景得以实现。</p><p>刘智勇：从具身智能的角度来看，商业落地是一个重要议题。目前，Agent或具身智能体主要扮演的是辅助角色。以它们当前的智力水平，还不能承担替代型的角色。它们能够提升生产力，但并不能真正改变生产关系。我们应该从最大程度提升人的生产力的角度出发去寻找落地场景，这是比较实际和可行的视野。</p><p>另外是从更宏观的层面来看待Agent和具身智能的发展，这与AGI息息相关。在经历了Transformer模型、ChatGPT以及机器人的Transformer模型等重要时刻之后，我们可能在不久的将来迎来AGI的时代。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。在具身智能领域，如果具有AGI的通用人形机器人能够实现，那么在某种程度上将实现仿生或永生的概念。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</id>
            <title>527蚂蚁技术日：AI应用矩阵集体“同框”</title>
            <link>https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 09:03:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, 技术日, AI创新应用, 开放创新
<br>
<br>
总结: 蚂蚁集团每年5月27日举办技术日活动，展示AI创新应用产品，强调让AI像扫码支付一样便利生活，同时推动开放创新，探讨技术领域的多种可能性。 </div>
                        <hr>
                    
                    <p>每年的5月27日，是蚂蚁集团的技术日，用以勉励蚂蚁技术人保持敬畏和创新之心，到今天技术日已发展成为技术周，一个包含技术论坛、技术集市、编程大赛、技术沙龙等活动的技术嘉年华。今年，第九届蚂蚁技术日首次对外开放，开放日上展示了诸多蚂蚁AI创新应用产品，向外界透传了“让AI像扫码支付一样便利每个人的生活”的技术主张和面貌。</p><p></p><p></p><h2>AI是开放日关键词</h2><p></p><p></p><p>据了解，蚂蚁集团持续投入AI领域的研发，基于大规模业务场景的需求，布局了包括知识图谱、运筹优化、图学习、可信AI、大模型等在内的AI技术，其中蚂蚁百灵大模型已于去年底完成备案。蚂蚁集团CTO何征宇在开放日现场表示，一直以来蚂蚁集团致力于用最前沿的技术做最普惠的服务，近两年蚂蚁重投AI和数据要素技术，致力于实现AI规模化应用，让AI像扫码支付一样便利每个人的生活。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62093c4002f76e1330c69590a689887b.jpeg" /></p><p></p><p>记者注意到，在此次开放日的技术集市上，蚂蚁AI创新应用展位前尤其热闹。</p><p></p><p>三个智能管家，是蚂蚁百灵大模型的核心创新应用产品。生活管家“支付宝智能助理”连接了400万商家机构小程序和8000项数字生活服务，食住行游购娱等生活服务均可问可办；金融管家“支小宝”能通过文字对答提供行情分析、持仓诊断、保险配置、投教科普等理财保险服务，目前使用过支小宝的用户达4300万；蚂蚁也推出了一套医疗解决方案，在不久后会正式推出服务大众的医疗健康管家。目前这套解决方案已经服务了很多医院。譬如，浙江省卫健委与支付宝联合推出了可陪诊的数字健康人“安诊儿”。浙江省内用户就医时，可感受全流程的陪伴、指引、互动，就医体验进一步提升。目前，“安诊儿”已在浙江大学医学院附属第一医院、浙江大学医学院附属邵逸夫医院等浙江省内近百家医院中应用，服务超百万人次。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/4239f10cfd69871911e01e593f9f85f2.jpeg" /></p><p></p><p>记者注意到，多模态大模型技术也在蚂蚁内部有诸多创新应用，如AI智绘、AI智乐、AIGC数字人等，用户输入一句话、一张图、一段语音，就能生成一张设计图、一段音乐作品、一段动态人像视频。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3269137664bac2bbbd54e0350739530.jpeg" /></p><p></p><p>安全可信，是蚂蚁AI创新应用的一个特色。在集市现场，记者见到了大模型一体化安全解决方案“蚁天鉴”，可实现对大模型本身安全性的检测和防御；加速隐私计算关键硬件“乾坤卡”，能解决密码学运算慢的问题，提升全同态计算性能100倍。</p><p></p><p>蚂蚁集团CTO何征宇在交流会现场介绍，“我们相信AI不为替代人而生，是为每一个人而生。”为实现AI人人可享的目标，蚂蚁一直在努力优化和提高AI的可靠性、经济性和易用性。可靠性是用AI监督AI，以解决控制和“对齐”比人类聪明得多的模型；经济性是把大模型做“小”，这决定了大模型应用能否成为主流；易用性则是将智能“傻瓜化”，这决定了生成式AI是否能规模化落地应用。</p><p></p><p>据了解，在蚂蚁内部，AI已经成为重要生产工具。譬如，基于蚂蚁百灵大模型开发的智能研发平台CodeFuse已支持一半以上蚂蚁工程师的日常开发工作，他们提交的代码中10%由AI生成。</p><p></p><p></p><h2>开放创新是内核</h2><p></p><p></p><p>在技术开放日现场，来自学界、AI企业、开源界等诸多领域的人士共同探讨了当下最热门的话题。</p><p></p><p>在AI主题论坛上，来自智谱等AI企业和蚂蚁热烈讨论AI的多种可能性“the‘N’one”、“wherearethe‘angrybirds’？”……</p><p></p><p>蚂蚁开源嘉年华活动中，全球最大开源软件基金会之一，Apache的开源爱好者杭州本地群组走进蚂蚁，和开源爱好者、开发者们一起分享开源故事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/0594997f0f179bb5d0d00321a0c0c4c5.jpeg" /></p><p></p><p>在接下来持续一周的技术日里，20场关于数据技术的论坛，围绕大模型、支付技术、安全科技、技术风险、隐私计算、图计算等技术领域，从主题设计上看都是直指技术工业级应用中的挑战。据了解，直面规模化应用场景中的真问题也是蚂蚁技术日一直以来的特色。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/5741d52cad8bab79d79a970f2054884c.jpeg" /></p><p></p><p>48小时黑客松是技术日期间举办的极客编程挑战赛，48小时内完成一个创意的实现，主题既可与工作、业务相关，也可以放飞奇想，做有趣、好玩的创意。参加今年黑客松的创意，大多聚焦生成式AI与给AI降本相关，如绘画AGENT、基于AI构建无障碍世界、本地化大模型工作智能提效工具等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/ddc825844be408b848ac77f9776d5dcb.jpeg" /></p><p></p><p>T-STAR是蚂蚁技术人年度颁奖仪式，也是蚂蚁内部最高技术荣誉，颁发给过去一年的表现杰出的技术人和技术项目。今年的十大创新奖项涵盖安全科技和分布式计算存储，AI也首次成为大赢家。在颁奖现场，蚂蚁集团CTO何征宇说，“蚂蚁技术日因敬畏而生，但敬畏不代表畏手畏脚，公司正处于全力发展的大阶段，不发展才是最大的风险。我们把T-STAR放在每年的527揭晓，就是要向大家表明，蚂蚁技术不仅要牢记历史，敬畏风险，更要用于创新、敢于突破。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f88905ade6bf48aa210cbdad4ce6ba14.jpeg" /></p><p></p><p>设置技术日，体现了一家公司对技术的重视。2015年5月27日，因光纤被挖断，部分支付宝用户两个小时无法登录账户，这次事件也成为蚂蚁历史上次重大技术事故，此后，这一天被定为蚂蚁技术日。2018年，支付宝实现了宕机后分钟级可恢复，如今，支持系统稳定安全的分布式技术已经成为蚂蚁的核心和领先的技术赛道。今年，第九届技术日首次对外开放，让外界看到了在人工智能时代，蚂蚁技术的新赛道和新可能。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</id>
            <title>6个必须参加ArchSummit深圳的理由，错过等1年</title>
            <link>https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 06:26:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ArchSummit, 智能进阶, 大模型时代, 人工智能
<br>
<br>
总结: ArchSummit全球架构师峰会将在深圳举办，围绕“智能进阶. 架构重塑”主题讨论企业架构在大模型时代的适应性和成本效益，探讨人工智能在架构中的应用，以及架构师如何规划职业道路。BAT等企业的顶尖专家将共探AI时代的无限可能。 </div>
                        <hr>
                    
                    <p>6月14日-15日，ArchSummit全球架构师峰会将在深圳举办。本次大会围绕“智能进阶. 架构重塑”主题，将探讨在 AI 浪潮下，企业架构如何适应大模型时代趋势，同时寻找具有成本效益的问题解决方案，帮助参会者更好地了解如何以及何时可以在架构中使用人工智能，同时探讨在架构师技术知识更新换代速度快的行业常态下，如何规划职业道路，保持自身的竞争力。</p><p></p><p>届时，头部AI大模型将同台竞技，BAT实力PK。顺丰CTO耿艳坤、阿里巴巴丁宇、Thoughtworks CTO Scott Shaw等来自国内外前沿企业的顶尖专家将与参会者面对面，共探AI时代的无限可能。</p><p></p><p>大会6大看点已备好，更多参会攻略可点击链接可查看详细日程：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8efeed0960c87e2ae91cdce14eb6b86.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</id>
            <title>王小川：不烧钱推AI应用、大模型价格战是云厂商的游戏</title>
            <link>https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 10:14:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 价格战, Baichuan 4, AI助手, AGI发展路径
<br>
<br>
总结: 王小川发布了最新一代基座大模型Baichuan 4和首款AI助手“百小应”，强调了百川智能的AGI发展路径，明确不参与价格战，Baichuan 4在模型能力上达到国内第一，具备行业领先的多模态能力。王小川表示关注文本能力，发布了多款API接口，推出全新的MaaS+AaaS服务，以及针对Agent构建的Assistants API接口。同时强调了要有自己的超级应用，差异化是市场竞争力的关键。 </div>
                        <hr>
                    
                    <p>明确不参与“价格战”的王小川，在5月22日做了两个重要发布：最新一代基座大模型Baichuan 4 和成立后的首款AI助手“百小应”，同时强调了百川智能的AGI发展路径：超级模型+超级应用。</p><p>&nbsp;</p><p>“模型还没有到超级模型，应用上还没有到超级应用。”王小川在发布会开始就很明确地说道，“我们在通往AGI的路上。”但这次发布仍然对百川来说意义非凡，Baichuan 4的模型能力达到了国内第一，也正式入局了 C 端市场。</p><p>&nbsp;</p><p></p><h2>模型能力，国内第一</h2><p></p><p>&nbsp;</p><p>相较1月底发布的 Baichuan 3，Baichuan 4 在各项能力上均有极大提升，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p>&nbsp;</p><p>榜单成绩已经是各家发布大模型时的必备项目了。这次，百川分享了国内权威大模型评测机构SuperCLUE的评测结果。在最新的测试结果中，Baichuan 4的模型能力达到了国内第一。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa1f6ed330a84bacc9809f53737893e4.png" /></p><p></p><p>&nbsp;</p><p>与国外主流大模型对比，Baichuan4在知识百科、长文本、生成创作等文科类中文任务上明显优于国外大模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c037fff5834687120dc87a15af5d9134.png" /></p><p></p><p>&nbsp;</p><p>此外，Baichuan 4还具备行业领先的多模态能力，在MMMU、MMBench-EN、CMMMU、MMBench-CN、MathVista等评测基准上表现优异，大幅领先Gemini Pro、Claude3-sonnet等多模态模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/633320d70ccdf66d3b267fd358839821.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>不过王小川也表示，相较多模态，百川更关注于文本能力。“多模态没有智力”，王小川解释道，多模态只是建立了一个分类的体系。而在像“1+2=3”这个等式里，智力不体现在1+2如何等于3的计算里，而是在等号里面。</p><p>&nbsp;</p><p>Baichuan 4能在较短的时间内取得突破，源于其在训练过程中引入了诸多领先技术优化手段：</p><p>&nbsp;</p><p>数据方面，在预训练阶段采用了基于Model-based+Human-based的协同数据筛选优化方法，以及对长文本建模位置编码科学的Scaling Law，有效提升了模型对数据的利用；SFT对齐方面，重点优化了模型Reasoning、Planning、Instruct following能力，通过loss驱动的数据选取与训练，多阶段爬坡，多模型参数融合等方式有效提升了模型的关键指标和稳定性；突破RLHF和RLAIF融合的RLxF强化学习对齐技术，大幅提升模型的指令遵循等能力；成本方面，提出了新的投机采样方案clover，通过将序列知识与并行解码结合，使得投机采样的命中率提升至60％，成本降低30％以上。</p><p>&nbsp;</p><p>此外，百川智能还宣布开放Baichuan 4、Baichuan3-Turbo、Baichuan3-Turbo-128k、Assistant API 四款API ：</p><p>&nbsp;</p><p><a href="https://platform.baichuan-ai.com/playground">https://platform.baichuan-ai.com/playground</a>"</p><p>&nbsp;</p><p></p><h4>“价格战是云厂商的游戏”</h4><p></p><p>&nbsp;</p><p>王小川坦言，当前 API 收入对百川而言并不多，也构不成收入重点。</p><p>&nbsp;</p><p>对于最近大模型市场打得火热的“价格战”，王小川表示，一方面，这表明了大家太看好大模型前景，不愿意失去任何机会，宁愿零价格也要进来入场；另一方面，降价的核心是要看商业模式的，如果是做ToB服务，最后售卖的不是模型本身而是整套云服务。云厂商是比较偏传统的服务模式，进到一个新的战场可以降价，但降价仅限于云厂商。在有限场景打价格战，已经走出了创业公司的射程。</p><p>&nbsp;</p><p>“这波降价跟之前的滴滴美团还不一样，因为那会儿的价格战或补贴背后带有网络效应，是双边网络，那个商业模式在改变生产关系，比如司机和乘客的关系、外卖员和用户之间的关系。而这次降价更像当初四小龙的降价方式，而不是像滴滴美团这样的价格竞争，这次不是生产关系的改变，而是直接做生产力供给。我觉得这件事情对我们而言，就是别掺和进去。”王小川说道。</p><p>&nbsp;</p><p>除了大模型发布，百川智能还推出了全新的MaaS+AaaS服务。MaaS版块由基座模型组成，分为旗舰版和专业版。旗舰版将全量开放Baichuan 4的各项能力，专业版包含Baichuan3-Turbo和Baichuan3-Turbo-128K两款模型，价格比旗舰版Baichuan 4更实惠，且对企业高频场景做了针对性优化，综合测试相比GPT-3.5整体效果提升8.9%。百川智能表示，MaaS 的新用户即日起可以获得1000万免费token。</p><p>&nbsp;</p><p>Assitants API则是百川智能在Baichuan 4基础上针对Agent构建推出的API接口，不仅支持Code interpreter、RAG内建工具，还支持自定义工具调用，方便企业接入各种丰富复杂的API。据悉，百川智能未来还将推出零代码Agent创建平台产品。</p><p>&nbsp;</p><p></p><h2>超级应用</h2><p></p><p>&nbsp;</p><p>“之前我就提到一定要有自己的超级应用，如果只是学OpenAI一开始有个模型做API服务，在中国的创业公司而言，是走不通的。”王小川说道。</p><p>&nbsp;</p><p>对于这一点，王小川的考虑有三：首先，虽然在美国做ToB是好生意，但在中国的商业环境中，C端市场就比B端大10倍；其次，做To B，收的是人民币，花的是美金；最后，大厂都会卷这件事情，这是大厂射程范围内的。因此，百川智能一定要做差异化，低价作为市场竞争力是不够的。</p><p>&nbsp;</p><p>这次，百川智能终于发布了自己的首款C端应用：“百小应”。当被质疑这款应用是不是发晚了的时候，王小川直言，“我正好觉得相反，发早了。”他认为，之前的各种应用更多是为了展示大模型能力，而到现在这个领域还没有到一个成熟的状态。</p><p>&nbsp;</p><p>王小川理想中的AI助手是有温度的，“作为伙伴，而不是工具”。他不吝惜对Kimi的赞赏，“Kimi打赏的理念，就是不是把它当工具，而是当成一个伙伴、一个人。”</p><p>&nbsp;</p><p>“懂搜索、会提问”是百川给百小应总结的有别于其他AI助手的地方：</p><p>&nbsp;</p><p>“懂搜索”意味着不是像其他搜索一样做汇总，而是让模型掌握专业的搜索技能，用户在搜索的时候理解意图、可以拆词作多轮搜索。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77a416611b4f0dc130706d819cf26ac3.png" /></p><p></p><p>&nbsp;</p><p>“会提问”则是让模型能引导用户表达自己的需求。这是由于当前很多人在搜索时表达只有关键词、不够准确，机器只能去猜，现在百川则是通过让机器反问的方式让用户表达出来，比如：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e44e938f621a835783a00c4c22ef6a9.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>除了强大的搜索和提问功能以外，用户还能在百小应中上传PDF、word文档，或者直接输入网页链接（URL），阅读并分析书籍、报告、学术论文等长篇内容，百小应一分钟就能读完上市公司财报。</p><p>&nbsp;</p><p>在Baichuan 4多模态能力的支持下，用户提问的同时还可以同步上传图片，百小应对图片内容进行解读，或者将图片作为补充材料，获取更精准的回答。此外，它还支持用户通过语音的方式进行交互。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/3235c04e1c07d8300470f186f525a483.jpeg" /></p><p></p><p>&nbsp;</p><p>王小川表示，百小应发布后不会为了获客疯狂投广告，“这并不是健康的行为。”“我们手上有足够的资金，不会通过立马发布一个产品去寻找下一轮融资。我们的精力会放在产品价值上。”</p><p>&nbsp;</p><p>对于商业模式，王小川认为现在还不到时候。“以Kimi为例，虽然现在有足够的用户体量，不管是走流量模式还是广告模式都有意义，但其实没有本质的区别。”具体的变现时机，要看一个场景下多少用户有付费意愿。</p><p>&nbsp;</p><p>当然，现在的百小应并不是王小川心目中的超级应用，“今天百万级DAU都远远称不上一个超级应用至少要再提升两个数量级。”王小川预计，百小应今年内会有大的升级变化。</p><p>&nbsp;</p><p>“同一代模型中，百川更有机会把超级应用做出来。”王小川说道。显然，百川智能对超级模型和超级应用两个赛道都非常有信心，“百川未来可能不再按月发布，而是按季度发布，团队要把时间调到长线来做事情。”王小川说道。</p><p>&nbsp;</p><p>未来百川智能将如何实现自己的AGI，我们拭目以待。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</id>
            <title>大语言模型加持，是智能运维架构的未来吗？</title>
            <link>https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 08:59:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能运维架构, 大语言模型, SRE-Copilot框架, AIOps挑战赛
<br>
<br>
总结: 在QCon北京2024大会上，字节跳动技术专家王宁分享了基于大语言模型的智能运维架构SRE-Copilot框架的演讲。该框架结合了大语言模型和AIOps实践，通过模拟不同团队的运维知识来解决技术问题，降低训练成本，提升故障处理能力，并支持自然语言交互，为智能运维带来新的可能性。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>演讲嘉宾&nbsp;|&nbsp;王宁&nbsp;字节跳动技术专家策划&nbsp;|&nbsp;蔡芳芳整理&nbsp;|&nbsp;Penny编辑&nbsp;|&nbsp;褚杏娟、傅宇琪</blockquote><p></p><p></p><p>在&nbsp;QCon&nbsp;北京&nbsp;2024&nbsp;大会上，字节跳动技术专家王宁，根据自己在字节的实践经历，发表了题为《<a href="https://qcon.infoq.cn/2024/beijing/presentation/5740">SRE－Copliot：基于大语言模型的智能运维架构</a>"》的演讲。</p><p></p><p>本文由InfoQ整理，经王宁老师授权发布。以下为演讲实录。</p><p></p><p>随着大语言模型的广泛应用和能力的提升，许多团队都在尝试利用大语言模型来改进他们所在行业的应用，我们团队在字节跳动内部也在探索如何将大语言模型与&nbsp;AIOps&nbsp;实践相结合。</p><p></p><p>去年，我们带着&nbsp;SRE-Copilot&nbsp;框架参加了&nbsp;AIOps&nbsp;2023&nbsp;挑战赛，并荣幸地获得了冠军。在比赛中，我们设计了一套更为开放和富有想象力的框架，进行了初步的探索。</p><p></p><p>比赛的题目是开放性的，旨在鼓励大家尽可能地探索大语言模型在智能运维领域的应用潜力。选手面临的挑战是帮助企业运维团队应对日益庞大的系统规模、复杂的系统结构以及日益增多的数据量。</p><p></p><p>比赛所用的数据来自中国建设银行“建行生活”APP中的真实模拟数据。比赛的架构设计模拟了从入口负载均衡集群到中间的基础集群，如抢券集群和订单集群，这些集群之间相互依赖。此外，还包括了出口负载均衡集群，并且依赖许多复杂中间件如&nbsp;Redis、MySQL、Kafka&nbsp;等的各个集群。</p><p></p><p>我们在比赛中获得的数据包括调用链数据、业务黄金指标，例如订单成功率、抢券成功率，系统交易的每个订单的延迟时间，以及集群的性能指标，不仅涵盖了基础集群，还有中间件集群的监控数据，如&nbsp;CPU&nbsp;负载、常见的出入网流量，系统日志数据等。</p><p><img src="https://static001.geekbang.org/infoq/08/08436d00c6950fe9cbaff7d3c9890161.png" /></p><p></p><p></p><h2>为什么需要SRE-Copilot&nbsp;框架</h2><p></p><p></p><p>随着系统日益复杂和数据量的增加，即使是专业的运维团队也难以全面掌握所有技术细节。在大型企业中，每个组件，如计算、存储、数据库，都可能需要专门的运维团队。当出现大规模故障时，通常需要所有团队协作来定位问题根源。而&nbsp;SRE-Copilot&nbsp;框架可以通过大语言模型的能力，学习和模拟不同团队的运维知识，以解决整个链路上的技术问题。</p><p></p><p>传统的&nbsp;AIOps&nbsp;在异常检测和根因分析上严重依赖于标注数据，这限制了算法的泛化能力，因为它们需要在有监督的环境下进行训练。而大语言模型能够学习更多的通用知识，减少对标注数据的依赖，从而降低训练成本。</p><p></p><p>运维团队积累的专家经验很难编码到算法模型中。通常，这些经验会被简化为阈值或复杂的规则，不仅难以维护，也难以传承。SRE-Copilot&nbsp;框架通过大语言模型，将专家经验转化为模型可以理解和推理的形式，从而提升了故障处理的能力。</p><p></p><p>传统&nbsp;AIOps&nbsp;的接入和维护成本较高，需要业务和算法团队深入理解业务逻辑和算法模型。此外，私域数据的处理和定制化开发也增加了成本。SRE-Copilot&nbsp;框架采用集成学习的概念，通过模块化设计，使得系统能够像搭积木一样动态编排。</p><p></p><p>在传统&nbsp;AIOps中，未遇到过的故障很难被解决，因为它们超出了模型的训练范围。大语言模型展现出了强大的推理能力，能够基于通用知识和训练中学到的关键字，推断出未知故障的性质，即使没有相似的训练数据。</p><p></p><p>传统的&nbsp;AIOps&nbsp;解决方案需要用户理解模型并精确地传递参数，而&nbsp;SRE-Copilot&nbsp;框架支持自然语言交互，使得非技术用户也能轻松地与系统交互，提高了用户体验，并有潜力开放给更广泛的用户群体。</p><p></p><p>SRE-Copilot&nbsp;框架，采用了基于&nbsp;function&nbsp;call（函数调用）的方法来实现多功能的智能运维。</p><p></p><p>我们首先将&nbsp;SRE&nbsp;在日常运维中可能遇到的多种场景进行了统一收敛，通过大语言模型来理解用户的意图。无论是进行运维可视化还是故障分类，我们的目标是编排不同的工具，以实现一个多场景的智能运维解决方案。</p><p></p><p>在比赛上，我集成了一些简单的场景，虽然每个场景本身并不复杂，但我们尽可能探索了许多新的场景，以测试和展示&nbsp;SRE-Copilot&nbsp;的多功能性。比如我们实现了故障分类功能，并让框架能够生成故障自愈的代码，以自动化处理常见的问题。</p><p></p><p></p><h2>实践效果如何？</h2><p></p><p></p><p>我利用大语言模型生成了一个排查故障的工作流，并以自然语言的形式呈现。</p><p></p><p>根据提前设定的&nbsp;Agent，模型会匹配并确定哪些&nbsp;Agent&nbsp;能够在排障工作流程中发挥作用，然后将这些&nbsp;Agent&nbsp;编排成一个可执行的工作流。我可以通过自然语言向模型提问，比如询问特定时间段内的问题。模型会提取相关参数，并动态地将任务分配给相应的子&nbsp;Agent。每个子&nbsp;Agent&nbsp;会检查自己的数据，寻找故障迹象。</p><p></p><p>例如，调用链&nbsp;Agent&nbsp;会检查调用链是否存在问题。如果检测到&nbsp;CMDB（配置管理数据库）中的信息，模型可以进行下钻操作，比如定位到具体的集群，然后触发下一轮更深入的检测。如果没有更多的信息，模型会开始进行根因诊断，检索历史故障和专家经验，以此来判断当前的故障类型，比如磁盘写满。</p><p></p><p>在诊断过程中，我会将相关指标进行可视化展示，帮助理解故障的性质。根据诊断结果，我会生成清理磁盘的简单代码。如果是线上执行，模型会匹配预设的自愈方案。每次故障诊断完成后，会自动生成告警总结和复盘报告。诊断的故障信息会自动积累下来。如果用户对诊断结果进行确认，那么这个故障案例就会被记录到历史故障库中，供模型未来诊断时参考。</p><p></p><p>在比赛的复现过程中，我们在建行云的&nbsp;8&nbsp;核和&nbsp;16G&nbsp;内存的堡垒机上进行了演示，并使用了一张&nbsp;V100&nbsp;显卡。实际上，我们仅使用了开源的&nbsp;ChatGLM&nbsp;6B&nbsp;的小模型，就实现了上述效果。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b12c49c627aefbd1ebb7f1bdbb33a961.png" /></p><p></p><p></p><h2>SRE-Copilot&nbsp;架构</h2><p></p><p></p><p></p><h4>Tool&nbsp;calling</h4><p></p><p></p><p>在&nbsp;SRE-Copilot&nbsp;架构中，Tool&nbsp;calling&nbsp;是指将大语言模型与外部工具有效结合或交互的能力。这种机制允许模型解决它自身无法直接回答的问题，因为它可以调用外部工具来获取所需的信息。</p><p></p><p>以询问天气为例，大语言模型本身无法提供实时天气信息，因为它缺乏对外部环境的感知。为了解决这个问题，需要设计一个封装用户问题的方法，并提供一个能够获取当前天气的接口工具，再通过一些描述指导模型使用接口。</p><p></p><p>例如，接口可以根据给定的地点查询天气，它是一个&nbsp;function，需要两个参数：地点（location）和温度单位（如摄氏度或华氏度）。成熟的大语言模型，如&nbsp;ChatGPT&nbsp;或文心一言，通常都是通过相似的训练方法来实现这一机制。模型会调用适当的函数，并且能够从用户的问题中提取出必要的参数。假设用户询问的是伦敦的天气，模型会自动将“伦敦”作为&nbsp;location&nbsp;参数补全，并指导调用天气查询接口，从而提供准确的天气信息。</p><p></p><p></p><h4>RAG</h4><p></p><p></p><p>SRE-Copilot&nbsp;架构中的另一个重要概念是检索增强生成（Retrieval-Augmented&nbsp;Generation，简称RAG）。RAG&nbsp;结合了搜索技术和大语言模型，旨在解决几个关键问题。</p><p></p><p>首先，它可以帮助处理私域知识，例如公司内部服务器的状态或特定订单的详情，这些信息是大语言模型无法直接感知的。</p><p></p><p>RAG&nbsp;能够解决新知识的问题。以电影为例，如果模型的知识截止日期是&nbsp;4&nbsp;月，那么它不会知道&nbsp;5&nbsp;月上映电影的主演是谁。同样，对于当天发生的新故障或新闻，模型也无法感知。</p><p></p><p>RAG&nbsp;也适用于长尾问题，即那些在模型训练时不常见或非常具体的问题。例如，如果用户需要编写某个小众语言的代码，或者询问关于特定数据库连接的问题，尤其是当这个数据库是公司内部改造过的，传统的大语言模型可能无法提供答案。为了解决这些问题，可以利用传统的检索方法，比如搜索公司内部文档，查找是否有关于特定语言连接数据库的&nbsp;QA&nbsp;文档。一旦检索到相关文档，我会将文档中的相关内容与用户的问题一起输入给大语言模型，以便模型能够提供一个更准确的答案。</p><p></p><p></p><h4>Reason+Act</h4><p></p><p></p><p>ReAct&nbsp;概念针对的是那些无法仅通过一步查询或大语言模型自身直接解决的任务。这些任务通常需要多步骤的执行，并且每一步都需要模型提供其思考过程，以减少模型产生幻觉现象的风险。</p><p></p><p>通过&nbsp;ReAct，我们可以将任务的每一步规划和执行可视化。例如，如果任务是让大语言模型去厨房做菜，模型可能会首先思考需要哪些调料，并预测它们可能存放的位置。比如，模型可能会推断出胡椒可能在&nbsp;1&nbsp;到&nbsp;6&nbsp;号柜子里，或者&nbsp;1&nbsp;到&nbsp;3&nbsp;号台面上。模型首先会选择检查第一个柜子，如果没有找到胡椒，它会根据这一步骤的结果，决定下一步检查其他柜子。</p><p></p><p>再比如，当用户询问&nbsp;GitHub&nbsp;上某位开发者获得的点赞数并想要得到该数值除以&nbsp;2&nbsp;的结果时，模型的第一步是通过&nbsp;GitHub&nbsp;的&nbsp;Open&nbsp;API&nbsp;获取点赞数，然后使用计算器工具进行除法运算，通过这两步操作来得到最终结果。</p><p></p><p>ReAct&nbsp;概念旨在通过分步骤地规划，并在每一步中进行合理的推理和决策，来执行越来越复杂的任务。</p><p></p><p></p><h4>Agent&nbsp;智能体</h4><p></p><p></p><p>智能体主要由三个部分组成：</p><p>大脑：最核心的部分是大语言模型，它扮演着智能体的大脑角色，负责处理和生成语言输出，同时整合和利用知识库中的信息。感知：智能体的第二部分是它的感知能力，这涉及到使用外部工具来感知图片、声音等信息。通过这些工具，智能体能够与外部世界进行交互，获取必要的数据。行动：智能体还可以通过计算&nbsp;API、查询&nbsp;API&nbsp;等工具进行实际的交互操作，执行真实世界中的行动。这使得智能体能够处理更为复杂的任务，如根据图片内容推断位置并查询当地天气。</p><p></p><p>我们期望实现的是多个智能体之间的复杂交互，让智能体不仅能够独立工作，还能够相互协作，共同解决更加复杂的问题。</p><p></p><p></p><h4>架构设计思路</h4><p></p><p></p><p>在&nbsp;SRE-Copilot&nbsp;框架的具体实现上，我根据比赛时提供的数据，在最底层为每个数据源设计了相应的&nbsp;Agent。这些数据源包括日志、调用链、交易类型数据、主机监控数据以及&nbsp;CMDB&nbsp;数据等多元数据类型。对于每种类型的数据都设计了一个&nbsp;Agent，使其能够进行异常检测、数据可视化，以及查询历史故障和返回故障描述等操作。</p><p></p><p>在底层&nbsp;Agent&nbsp;之上，我定义了功能型&nbsp;Agent，它包括知识库问答、工作流规划、故障报告编写和代码生成等能力。这些功能型&nbsp;Agent&nbsp;能够基于&nbsp;ReAct&nbsp;框架，调用底层的子&nbsp;Agent，并动态地编排它们的执行顺序和结果。</p><p></p><p>在框架的最顶层，我设计了一个名为&nbsp;Copilot&nbsp;的智能体，它作为与人类用户交互的核心。Copilot&nbsp;负责识别用户的意图，判断用户是想要获取答案还是进行故障诊断，并从用户的问题中提取出相关参数。</p><p></p><p>例如，用户可能询问某个具体时间段的情况，Copilot&nbsp;可以将这个时间段作为参数传递给后续的处理流程。随着大语言模型的智能化，它甚至能够理解并处理如“过去&nbsp;15&nbsp;分钟”这样模糊的时间段，并将其转换为具体的参数。顶层&nbsp;Copilot&nbsp;还能够进行任务分配和&nbsp;Agent&nbsp;之间的协调工作，确保整个框架能够高效地响应用户的需求。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e989cfcbd307002b6b49ee8341b83e3.png" /></p><p></p><p></p><p>在设计&nbsp;SRE-Copilot&nbsp;框架时，我借鉴了&nbsp;GPT&nbsp;背后的集成学习思想。我们的目标并不是开发一个庞大而全面的模型来解决所有问题，而是通过集成多个专精于特定领域的子&nbsp;Agent&nbsp;来实现。每个子&nbsp;Agent&nbsp;都在其专业领域内表现出色，我们采用混合专家系统（Moe,&nbsp;Mixture&nbsp;of&nbsp;Experts）的形式，使整个系统的效果更加完善和高效。</p><p></p><h2>SRE-Copilot&nbsp;技术细节</h2><p></p><p></p><p></p><h4>主要运维能力：异常检测</h4><p></p><p></p><p>首先介绍一个异常检测场景，以下是我们在这个场景中定义的角色：</p><p>Copilot&nbsp;主持人：作为核心，Copilot&nbsp;负责解析用户需求，制定运维计划，并安排不同&nbsp;Agent&nbsp;执行具体任务。多数据源&nbsp;Agent：针对不同的数据源，如日志、调用链、交易类型数据、主机监控数据以及&nbsp;CMDB&nbsp;数据，我们设计了专门的&nbsp;Agent。这些&nbsp;Agent&nbsp;利用合适的算法对各自模态的数据进行异常检测和检索。RCA&nbsp;Agent：该&nbsp;Agent&nbsp;负责收集其他&nbsp;Agent&nbsp;的检测结果，并执行最终的根因分析推理。</p><p></p><p>以&nbsp;Copilot&nbsp;为起点，例如，当收到用户关于&nbsp;9&nbsp;月&nbsp;18&nbsp;日下午&nbsp;4&nbsp;点大量交易失败的请求时，Copilot&nbsp;会识别这是一个根因诊断问题，并将任务交给&nbsp;RCA&nbsp;Agent。</p><p></p><p>RCA&nbsp;Agent&nbsp;会关注用户请求中的关键信息，如交易类数据，并将其传递给交易类型&nbsp;Agent进行初步检查。该&nbsp;Agent&nbsp;将返回关于交易量同比下降的结果，但不提供额外信息。</p><p></p><p>根据初步检查结果，系统会动态调度其他&nbsp;Agent&nbsp;检查各自负责的组件是否存在问题。例如，当轮到第五个&nbsp;Agent，即调用链&nbsp;Agent&nbsp;时，它发现了调用链上的异常情况。调用链&nbsp;Agent&nbsp;的反馈将引导&nbsp;CMDB&nbsp;Agent&nbsp;在&nbsp;CMDB&nbsp;中查询接口的调用记录，上游下游关系，以及接口的具体问题。</p><p></p><p>通过这种方式，我们模拟了真实线上云平台中多个组件之间的协同定位过程。整个异常检测过程完全自动化，没有一个固定的流程，而是根据每个&nbsp;Agent&nbsp;的检测结果动态地调度其他&nbsp;Agent，共同完成异常检测任务。</p><p></p><h4>主要运维能力：根因定位</h4><p></p><p></p><p>有了上一步的异常检测结果，接下来需要对这个异常检测的结果做根因定位，或者说故障的分类。</p><p></p><p>首先，获取异常检测组件的结果，并对其进行了一些调整，使其返回的不仅是传统的&nbsp;true/false&nbsp;或&nbsp;JSON&nbsp;结构体，而是大语言模型和人类都容易理解的自然语言描述。例如，异常检测结构体可能会返回：“本次故障持续了&nbsp;10&nbsp;分钟，CPU&nbsp;指标飙升，内存也被打满，某某接口出现了大量失败”。</p><p></p><p>然后，这个故障工单会转换成向量，并在提前构建的向量数据库中检索。向量数据库包含两部分内容：一部分是提前配置的专家经验，另一部分是历史上遇到的相似故障记录。例如，如果历史上遇到过内存打满的问题，通过重启服务解决了问题，这样的专家经验会被记录在系统中。</p><p></p><p>在当前故障检测时，系统会同时检索出相关的专家经验和历史故障信息。通过&nbsp;RAG，将当前故障描述、专家经验和历史故障信息一起输入给大语言模型，进行根因推断。大模型会根据提供的信息推理，例如，这是否是内存打满的问题，是否需要通过重启服务来解决。</p><p></p><p>RAG&nbsp;方法可以帮助大语言模型不断地更新其知识库，适应新的故障情况，同时减少模型的幻觉现象。通过引入历史故障数据和反思机制，模型的准确率得到提升，幻觉现象得到降低。即使在没有提前配置专家经验或历史故障数据的情况下，使用的&nbsp;6B&nbsp;大小的模型（如ChatGLM&nbsp;3）仍能够对某些类型的故障做出准确的推理，如磁盘写满或&nbsp;Java&nbsp;GC&nbsp;问题。</p><p></p><h2>字节的实践探索</h2><p></p><p></p><p></p><h4>基于&nbsp;LLM&nbsp;的&nbsp;RCA-Agent&nbsp;构建</h4><p></p><p></p><p>字节跳动内部的目标是先将基于大语言模型的根因诊断（RCA）Agent&nbsp;框架落地应用，因为根因诊断是&nbsp;SRE&nbsp;团队面临的一个主要挑战，它占用了大量的时间和精力，日常的&nbsp;On&nbsp;Call&nbsp;问题定位也给团队成员带来了沉重的负担。我们希望专注于解决这些实际问题，真正缓解&nbsp;SRE&nbsp;同事的痛点。</p><p></p><p>我们定义了一些工具和插件，是在出现故障时用来进行检测的工具。除了工具和插件，我们还设计了工作流编排，以自动化和优化故障处理流程。我们构建了一个知识库，它包含了历史故障数据、专家经验和故障处理策略，这些都是进行有效根因分析的关键资源。</p><p></p><p></p><h4>知识库的构建</h4><p></p><p></p><p>构建知识库方面所做的工作主要包括以下几个部分，并且我们计划未来会引入更多用户原始文档、历史&nbsp;On&nbsp;Call&nbsp;记录等不同类型的数据。</p><p></p><p>排障专家经验：这部分是根据根因诊断的场景特别设计的，目的是让业务团队的成员能够管理和记录他们的知识和经验。我们定义的每一个经验都是一组根因故障，包括故障发生时的描述和一些止损措施的组合。这些信息将被用来训练大语言模型推理。</p><p></p><p>例如，流量突增导致的故障，其根因可能是用户&nbsp;QPS&nbsp;的突增。故障的表现可能是流量首先突增，随后内存和&nbsp;CPU&nbsp;使用率也跟着上升，最终导致服务不可用。这种描述将帮助模型理解故障模式。对于上述故障，可能的止损措施包括重启服务或进行扩容操作。</p><p></p><p>故障场景&nbsp;SOP&nbsp;文档：我们希望用户输入的是一些&nbsp;SOP&nbsp;文档。这种方式给组件团队提供一种灵活管理知识的方法。我们选择这种半规范化文档的形式，是因为当前大语言模型的能力还有局限，需要通过文档梳理来帮助模型更好地理解。</p><p></p><p>历史故障信息：我们还维护了一个历史故障信息库，记录每一次通过大语言模型检测到的故障，这些记录会用来对组件团队进行训练和打标。</p><p></p><p></p><h4>基础工具的构建</h4><p></p><p></p><p>在构建&nbsp;SRE-Copilot&nbsp;框架的基础工具方面，我们参考了&nbsp;OpenAI&nbsp;GPTs&nbsp;将工具集成到平台时所遵循的规范。我们将运维场景中的一些关键指标和基础工具进行了统一管理，把传统的异常检测方法统一成一个工具，用户只需要维护他们需要进行异常检测的指标即可。</p><p></p><p>用户可以自定义检测项，包括指标名称、指标的标签或指标描述，以及定义何为异常表现。因为是用户自定义的工具，所以可以根据具体需求设置检测标准。</p><p></p><p>我们实现了一个变更事件查询工具，当出现故障时，用户可以通过调用这个接口来确定是否由线上变更导致。我们在平台上部署的组件配置了一些工具，例如异常检测、变更和事件查询等，还包括了自然语言的意图理解和大语言模型的根因推理功能。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d193671ffeaa6714378964ace886acc9.png" /></p><p></p><p></p><h4>核心工具：LLM&nbsp;根因推理</h4><p></p><p></p><p>关键的根因推理功能基于大语言模型。在实践中，我采用了一种新的方法，不再将故障数据压缩成向量空间进行聚类分析，而是利用大语言模型将故障映射到自然语言空间中，从而简化了故障分类过程。</p><p></p><p>例如，流量突增、内存升高和接口不可用等信息可以自然地描述为&nbsp;QPS&nbsp;问题。对于新出现的具有相同描述的故障，可以直接使用大语言模型进行分类，而无需计算向量空间中的相似度。</p><p></p><p>分类过程中会使用用户配置的专家经验和当前故障的检测结果。此外，工具还利用预定义的模板生成描述，并让大语言模型进行推断，而用户可以根据需要调整这些模板、专家经验和检测结果。</p><p></p><p>为了提高根因推断的准确性，确保检测项插件的描述足够具体，比如详细描述指标曲线的形态变化，以及变更检测组件提供的详细信息，如变更的范围和性质。这样的方法不仅提高了故障分类的效率，还能让大语言模型的推理过程更加精确。</p><p></p><p></p><h4>工作流的构建</h4><p></p><p></p><p>下一步是构建工作流，目前这一过程仍然需要用户自行配置，这主要是由于大语言模型当前能力的限制所做出的妥协。</p><p></p><p>不过，我们正在探索一种新的方法，即允许用户在其&nbsp;SOP&nbsp;文档中预先设定工作流，例如，文档中可以指明首先需要检查哪些指标，以及根据这些指标的结果接下来应该检查哪些指标。</p><p></p><p>我们希望能够训练大语言模型，使其能够直接根据用户的&nbsp;SOP&nbsp;文档生成工作流。最终，SRE&nbsp;团队能够向大语言模型提供一个简单的文档，甚至是未经格式化的文本，而模型能够根据文档中的指标或检测项动态地编排诊断步骤，并根据每一步的检测结果，智能地调度后续的执行流程。</p><p></p><p></p><h4>Agent&nbsp;的应用与调试</h4><p></p><p></p><p>我们在一些组件上进行了&nbsp;RCA&nbsp;的试点工作，实现了一些改变传统交互方式的效果。现在，用户可以通过提出模糊的问题来与系统交互，例如询问某个集群存在哪些故障，系统会自动识别并调用相应的集群诊断工作流。</p><p></p><p>完成诊断后，系统不仅能够将结果进行美化或封装，还能以卡片或自然语言的形式向用户直观展示。此外，在面对大规模故障时，系统能够自动解析故障群中的告警卡片信息或历史消息，判断需要诊断的参数，如特定集群和时间段，自动提取所需参数，并触发相应的诊断流程。</p><p></p><p></p><h2>未来展望</h2><p></p><p></p><p>针对我们遇到的瓶颈，我认为未来需要继续在下面几个方向增强。</p><p></p><p>第一，我期望有越来越大、能力越来越强的通用大语言模型，因为随着这些模型能力的不断提升，在动态根因推理和每一步的动态决策上，它们的准确率和效果都将得到显著提升。</p><p></p><p>第二，我期待能够开发出更强大、更专业的模型，例如时序大模型或日志大模型，并将这些专业模型作为工具，供核心Agent调用。</p><p></p><p>第三，我正在探索是否能有更灵活的Agent框架，以支持多轮和更灵活的对话，比如让用户在排障过程中随时打断并提供关键信息，以此缩短故障排查的平均时间。实现多人或多Agent之间的真正协作。</p><p></p><p>第四，我期望Agent能在更丰富的场景中应用，目前它们可以像实习生一样帮助进行简单的监控数据处理，未来随着模型和工具能力的增强，我希望它们能够发展到像初级员工或应届生那样进行一些简单的决策，甚至最终成为一个资深专家，能够自动诊断问题并执行高级决策。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</id>
            <title>老黄急了！为跟华为抢业务，英伟达也得低头降价，但大家已经不买账了？</title>
            <link>https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 07:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯片产品, 价格下调, 竞争压力, 制裁限制
<br>
<br>
总结: 英伟达在中国市场下调芯片产品价格，面临竞争压力和制裁限制，导致销售疲软，需应对中国市场挑战。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p></p><p>5&nbsp;月&nbsp;24&nbsp;日，根据路透社的报道，英伟达下调了中国特供版芯片产品的价格，H20&nbsp;芯片现在的售价比华为的Ascend&nbsp;910B便宜&nbsp;10%&nbsp;以上。</p><p></p><p>知情人士透露，英伟达在与华为的中国市场芯片争夺战中销售疲软，性能问题、充足的供应和制裁限制，迫使其定价低于华为的芯片产品。中国日益增长的竞争压力，给这家美国半导体设计公司的投资者也敲响了警钟。</p><p></p><p>在美国对人工智能芯片出口实施制裁和竞争加剧的背景下，价格下行凸显了英伟达中国业务面临的挑战，为其在中国市场的前景蒙上了一层阴影，而这一市场对英伟达2024财年营收的贡献率高达17%。</p><p></p><h2>价格从略低到更低，但买家大多倒向华为？</h2><p></p><p></p><p>去年年底，英伟达推出了三款为中国量身定制的芯片，分别是HGX&nbsp;H20、L20&nbsp;PCle和L2&nbsp;PCle。三款新产品都是从H100的基础版修改而来，包含了英伟达用于人工智能工作的大部分最新功能，但一些计算能力被削减以符合美国新规。</p><p></p><p>三款芯片中，H20最受关注，因为它是英伟达可在中国销售的性能最强大的产品。但三位供应链消息人士透露，市场上该芯片的供应充足，反而表明对其需求的疲软。</p><p></p><p>据悉，H20芯片刚开始接受分销商的预订时，定价还与华为的竞争产品相当。短短几月，情况就发生了变化。今年&nbsp;2&nbsp;月，H20芯片每张定价约在1.2万~1.5万美元区间（约8.6~11万人民币），略低于华为Ascend&nbsp;910B约12万元的售价；分销商提供的预装了&nbsp;8&nbsp;块&nbsp;AI&nbsp;芯片的&nbsp;H20&nbsp;服务器售价为&nbsp;140&nbsp;万元人民币。</p><p></p><p>有消息人士表示，在某些情况下，H20芯片价格会以低于华为Ascend&nbsp;910B（华为公司最强大的AI芯片）售价的10%以上。</p><p></p><p>中国的服务器分销商正在以每张卡约10万元人民币的价格出售H20，八卡服务器每台售价约为&nbsp;110&nbsp;万元~130&nbsp;万元人民币。相比之下，分销商以每张卡12万元以上的价格出售华为910B，而其八卡服务器的起价为&nbsp;130-150&nbsp;万元/台。“H20&nbsp;和华为&nbsp;910B&nbsp;的价格也会根据订单的规模而波动。”消息人士补充说。</p><p></p><p>当时，一位消息人士指出，就规格而言，H20&nbsp;在&nbsp;FP32&nbsp;性能方面似乎落后于&nbsp;910B，而&nbsp;FP32&nbsp;是衡量芯片处理普通任务速度的关键指标，H20&nbsp;的&nbsp;FP32&nbsp;性能还不到其竞争对手的一半。但&nbsp;H20&nbsp;在互联速度方面似乎比&nbsp;910B&nbsp;更有优势，互联速度是衡量芯片间数据传输的速度。“这意味着在需要将大量芯片连接起来作为一个系统工作的应用中，H20&nbsp;仍能与&nbsp;910B&nbsp;竞争。”</p><p></p><p>有分析师表示，H20&nbsp;芯片的表现将是影响英伟达中国业务的一个重要因素，而长期前景则取决于其如何与本土科技巨头华为竞争。虽然华为去年才开始与英伟达竞争，但消息人士称，今年华为将大幅增加其Ascend&nbsp;910B芯片的出货量，该芯片在某些关键指标上优于H20。</p><p></p><p>根据路透社对现有政府采购数据的核查显示，在过去六个月中，仅有五家国有或国有附属企业买家表示有兴趣购买H20芯片，而同期有十多家买家表示有兴趣购买华为的910B芯片。不过，这些数据并不详尽，可能无法反映市场需求的全部范围。</p><p></p><p>对比在中国的市场处境，英伟达在国外仍然保持发展势头。5&nbsp;月&nbsp;25&nbsp;日，据The&nbsp;Information报道，埃隆·马斯克本月告诉投资者，其初创公司&nbsp;xAI&nbsp;计划在&nbsp;2025&nbsp;年秋季之前建造一台超级计算机，它将依靠数万个英伟达&nbsp;H100&nbsp;芯片，以为未来更智能的&nbsp;Grok&nbsp;聊天机器人提供动力。马斯克将这台超级计算机称为“计算超级工厂”，耗资数十亿美元建造。</p><p></p><h2>制裁所带来的利润挤压</h2><p></p><p>由于美国旨在限制中国成为科技强国的制裁，英伟达的&nbsp;H800&nbsp;和&nbsp;A800&nbsp;在中国被禁售，包括&nbsp;H100&nbsp;和&nbsp;B100&nbsp;在内的其他先进产品线也已被禁。之后，英伟达针对中国市场推出了新产品，并调整了其人工智能芯片战略以应对制裁令。</p><p></p><p>在英伟达的第一季度财报中，该公司的高管们警告称，受制裁影响，他们在中国的业务成绩已“大幅”低于过去。英伟达的首席财务官Colette&nbsp;Kress&nbsp;指出，“相比10月实施新的出口管制许可之前的水平来说，我们在中国的数据中心收入大幅下降，预计未来中国市场的竞争仍将非常激烈。”</p><p></p><p>消息人士称，H20&nbsp;芯片上个月开始在中国广泛销售，一个多月后就交付给了客户。中国的一些科技公司已经下了订单，其中阿里巴巴订购了3万多块H20芯片。</p><p></p><p>研究机构SemiAnalysis的创始人Dylan&nbsp;Patel表示，到2024年下半年，将有近100万颗H20芯片运往中国，英伟达必须在定价上与华为竞争。</p><p></p><p>“尽管H20&nbsp;的售价是&nbsp;H100（2022&nbsp;年禁止出口到中国英伟达人工智能芯片）的一半，但由于更高的内存容量，其制造成本高于&nbsp;H100。这是利润率的急剧下降。”Patel&nbsp;表示。</p><p></p><h2>英伟达：做最坏的打算</h2><p></p><p></p><p>在英伟达降价之际，其&nbsp;H20芯片或许还迎来在中国取得成功的另一大“绊脚石”：除了制裁和由此产生的微薄利润外，英伟达还必须应对中国政府鼓励企业支持国产芯片发展的指令问题。尽管上述三位消息人士中的两位表示，近几个月来这些指令有所放松。</p><p></p><p>5&nbsp;月&nbsp;14&nbsp;日，据The&nbsp;Information援引知情人士报道称，中国监管机构已建议字节跳动、腾讯、阿里巴巴和百度等大型科技公司减少购买外国制造的AI芯片，转而购买更多国产芯片，尤其是英伟达GPU一直是大多数国内科技科技公司的首选。但监管机构希望科技公司为新的互联网数据购买同等数量的国产和外国制造的AI芯片，这是中国监管机构首次为企业在购买AI芯片制定具体指引。</p><p></p><p>根据监管要求，如果国内公司选择订购更多的外国芯片而不是本土芯片，它们必须以书面形式详细说明订购美国芯片进行部署的数量，并证明其合理性。但据悉，该指令尚未得到严格执行，目前尚不清楚是否会对不合规公司施加任何处罚。</p><p></p><p>根据中国市场研究公司赛迪咨询（CCID&nbsp;Consulting）的一份报告，预计到2035年，中国在全球人工智能行业的份额将超过30%。有分析师表示，尽管英伟达正在努力抢占中国的芯片市场份额，但前景越来越不确定。</p><p></p><p>“英伟达正在走一条微妙的路线，试图在维持中国市场和应对美国紧张局势之间取得平衡。”IG市场分析师Hebe&nbsp;Chen表示，“从长远来看，英伟达肯定在为最坏的情况做准备。”</p><p></p><p>前不久，还有媒体爆料称，华为Ascend&nbsp;910B&nbsp;在&nbsp;2024&nbsp;年的出货量将达到40万片以上，单卡价格在7万元左右。这对英伟达来说，恐怕将造成更大的冲击。有网友这样评价，“英伟达最怕的不是少了中国市场，而是中国的市场体量可以培育出一个可怕的竞争对手。”</p><p></p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/nvidia-cuts-china-prices-huawei-chip-fight-sources-say-2024-05-24/">https://www.reuters.com/technology/nvidia-cuts-china-prices-huawei-chip-fight-sources-say-2024-05-24/</a>"</p><p><a href="https://www.yahoo.com/tech/nvidias-ai-chip-sales-china-170128954.html">https://www.yahoo.com/tech/nvidias-ai-chip-sales-china-170128954.html</a>"</p><p><a href="https://www.theinformation.com/articles/chinese-regulators-tell-local-tech-firms-to-buy-fewer-nvidia-chips">https://www.theinformation.com/articles/chinese-regulators-tell-local-tech-firms-to-buy-fewer-nvidia-chips</a>"</p><p><a href="https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/">https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1HSU228X61i7sId0r75U</id>
            <title>MatrixOne → MatrixOS：矩阵起源的创业史即将用“AI Infra”和“AI Platform”书写新章程</title>
            <link>https://www.infoq.cn/article/1HSU228X61i7sId0r75U</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1HSU228X61i7sId0r75U</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 06:12:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化浪潮, MatrixOne, AI 技术, 数据处理能力
<br>
<br>
总结: 在数字化浪潮的推动下，MatrixOne 以构建能支撑新一代数字世界的操作系统为目标，从处理数据到整合 AI 能力，不断升级发展，最终推出了 AI-Native 数据智能全域操作系统 MatrixOS，满足市场对算力和数据管理的需求。MatrixOne 的发展历程体现了对 AI 技术和数据处理能力的持续迭代和创新。 </div>
                        <hr>
                    
                    <p>在数字化浪潮的推动下，MatrixOne 的故事就像一部科技界的创业史诗，它始于一个简单而宏伟的梦想——构建一个能够支撑起新一代数字世界的操作系统。想象一下，在 AIGC 时代，数据流动如同“血液”，算法运转如同“心跳”，用户界面如同“呼吸”，新一代操作系统像指挥家一样，精准处理多元化的海量数据、操控着算力分配，这个数字世界将变得多么有活力。</p><p></p><p>故事的起点是在 2021 年，MatrixOne 的创始团队站在数字世界的入口，他们看到了一个巨大的机会——数据，这个新时代的石油正等待着被开采和利用。想要让数据发挥出真正的潜力，就必须要有强大的存储和处理能力。于是，MatrixOne 诞生了，它最初的定位是超融合异构数据库，能够处理各种类型的数据，满足不同行业的需求。</p><p></p><p>然而，随着时间的推移，MatrixOne 的团队发现，仅仅处理数据是不够的。在这个由人工智能和机器学习驱动的时代，算力成为了新的战场，企业和开发者们不再满足于单一的数据存储解决方案，他们需要的是一个能够整合 AI 能力、满足算力需求，完成 AI 基础设施建设同时，还能实现业务成本、效率与技术先进性平衡的平台。于是，MatrixOne 的研发团队矩阵起源决定将在 MatrixOne &nbsp;的基础上，扩展业务至 &nbsp;AI Infra (人工智能基础设施) 和 AI Platform(人工智能平台) 领域，并与世纪互联的 AIDC(Artificial Intelligence Data Center，人工智能数据中心) 业务进行深度融合和紧密协作。对此，InfoQ 对矩阵起源 CEO 王龙进行了特别专访，围绕矩阵起源的业务拓展规划，聚焦数据库和 AI 技术的未来发展展开了讨论。</p><p></p><p></p><h2>MatrixOne 升级至 MatrixOS，是 AI 浪潮下市场需求的“响应”</h2><p></p><p></p><p>AI 技术高速发展，已经成为了推动各行各业变革的核心技术力量，而 AI 技术的发展依赖于强大的算力和高效的数据处理能力。目前围绕这两者的软件平台众多，但这一繁荣背后隐藏着一个挑战——标准化的缺失。GPU 的云化、虚拟化和资源池化等领域尚未形成统一标准，这与已经成熟的 CPU 公有云服务形成对比。在数据管理方面，传统数据库解决方案和新兴的 AI 驱动的数据技术都在探索如何更有效地管理和使用数据。AI 技术的加入为数据的使用和管理带来了新的维度，许多公司正在尝试创新的解决方案，如向量数据库、RAG 模型和 AI 代理技术等，都是在适应这一变化。</p><p></p><p>矩阵起源认为，“在数据和算力之间建立有效的连接是至关重要的。”所以他们在开发 MatrixOne 云原生数据库时，就专注于连接“未来数据”和“过去数据”，解决传统数据库分裂带来的数据割裂问题，目标是将结构化、非结构化、半结构化等不同类型的数据融合起来，以支持 AI 在不同行业中的落地应用，这种数据融合能力，和算力、数据的兼容策略，为他们提供了与市场上其他数据平台和算力平台厂商相比的领先优势。</p><p></p><p>于是，当矩阵起源在近日完成千万美元 Pre A 轮融资后，便决定将这笔资金用于开发极简统一、开源开放的 AI-Native 数据智能全域操作系统 MatrixOS。据悉，该系统将由大规模异构算力纳管调度平台 MatrixDC、超融合异构数据管理平台 MatrixOne 和 AI 智能体应用开发平台 MatrixGenesis 三部分组成，目标是打造链接算力、数据、知识、模型与企业应用的 AI Native 软件平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce04e808a663546f8c5a75d6db2ec2c2.webp" /></p><p></p><p>Matrix OS 提供了一个统一的算力和数据管理平台，不同用户可以根据自己的需求进行不同的组合使用。对于 AI 科学家来说，Matrix OS 解决了他们需要在不同系统间移动数据和算力的问题，他们可以将自己的训练数据放在算力平台上进行训练，训练完成后，模型可以托管在同一平台上。对此，王龙表示，“Matrix OS 提供了一个统一操作平台，简化了数据和算力之间的连接和管理。”</p><p></p><p>同时，对于开发者而言，他们可能不需要进行模型训练，但需要使用 API 调用大模型或传统模型进行推理，所以 Matrix OS 同样提供了简化数据连接问题的、性价比高的工具，使开发者能够将 API 调用与传统应用和工作流程结合。而对于最终用户来说，他们能够更切实地感受到应用优化速度变快、应用延迟降低以及性价比提高。例如，AI 服务的成本（如 token 价格）和数据库服务的成本（如节点价格）都变得更低，应用变得更加流畅，延迟更低，使用成本也更低。</p><p></p><p>从媒体的角度来看，MatrixOne 升级至 MatrixOS，这是一个非常值得关注的行业动态，这是矩阵起源对现有服务的持续改进和对新兴技术的积极拥抱。看得出，矩阵起源在保持其核心业务的连续性和稳定性的同时，也在持续积极地做技术创新探索，以满足市场和用户不断变化的需求。MatrixOne 的这次升级是对现有服务的补充而非替代，此次矩阵起源的业务拓展，不仅是对现有业务的一次强化，更是对未来趋势的一次前瞻性布局。</p><p></p><p></p><h2>AI Infra 和 AI Platform，是一站式 AI 解决方案的必要条件</h2><p></p><p></p><p>对于想投入 AI 建设的企业来说，底层的 AI Infra 提供必要的硬件和软件基础，支持大规模数据处理和 AI 模型训练，而顶层的 AI Platform 提供易于使用的服务和工具，可以帮助企业快速开发和部署 AI 应用。对于做数据库起家的厂商来说，从底层到顶层，提供一站式 AI 解决方案，可以更好地增强客户黏性，扩大技术服务优势。</p><p></p><p>所以当我们从宏观视角去看 MatrixOS，大规模异构算力纳管调度平台 MatrixDC、超融合异构数据管理平台 MatrixOne 和 AI 智能体应用开发平台 MatrixGenesis 这三个子产品之间的技术能力是非常相关联的，功能上有所重叠，但技术能力上相互互补，共同构成了一个强大的数据智能平台。</p><p></p><p>过去，MatrixOne 本质上可以说是一个湖仓一体的数据平台，但比 Hadoop 要简单得多。技术上，MatrixOne 的架构独特，实现了多层管理，包括数据的冷热分层、读写分层和存算分离，这种架构允许灵活调度计算资源和数据资源，让 MatrixOne 像变形金刚一样可以适应银行、互联网、IoT 等不同的应用场景，比如为了适应大模型技术的发展，MatrixOne 已经具备了向量数据库的能力，便可以支持 AI 需求。如今，MatrixOne 能够非常自然地管理来自智能物联网和 AI 的新数据源，这将是“未来数据”的主要来源，也是 MatrixOS 运转的基础燃料，MatrixOS 据此能够更好地与 AI 世界连接。</p><p></p><p>在 AI Infra 领域，想要实现颠覆性技术变革，就必须要解决算力和电力这两个核心问题。目前像英伟达的 stargate 计划、中国政府建设的智算中心都在硬件层面上积极提升算力，在软件方面则主要在围绕 GPU 进行技术升级，当前行业内需要一个软硬结合的解决方案来完成未来的 AI 基础设施建设。为此，矩阵起源把 MatrixDC 作为 MatrixOne 和 MatrixGenesis 的资源底座，利用云原生技术帮助企业构建 AI 原生的资源管理和调度平台。</p><p></p><p>最近世纪互联领投的 Pre A 轮投资让矩阵起源可以在万卡集群上测试软件，为他们提供了难得的实践机会。拥有全球人才团队的矩阵起源集中优势，通过在万卡集群上的实践，快速迭代，构建了技术壁垒，创造出竞争力强的 MatrixDC ——提供大规模异构 GPU，能够快速满足 AIGC 应用场景算力需求，在算力集群管理方面，可以很好地帮助数据中心进行异构芯片和 GPU 服务器的统一纳管及资源池化，实现算力资源的共享和动态调度，提高资源利用率。</p><p></p><p>不仅如此，MatrixDC 还灵活支持 CPU 计算资源，可以高效运行结构化交易 / 分析型业务应用；拥有 AINet 高速网络架构，能够很好地支撑千卡乃至万卡集群训练工作负载，全托管企业级容器服务，企业可以充分享受容器和微服务架构的便利。同时，MatrixDC 采用 Serverless 按量计费的商业模式，能够帮助企业大幅降低 AI 创新应用的探索成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6a0d6f761ba49320931fd50f7a4a7ec.webp" /></p><p></p><p>而在 AI Platform 领域，目前的主要技术挑战是要解决大模型幻觉问题，以及大模型和传统行业的应用的集成问题，现在行业内亟需一个平台来整合和连接所有相关的能力，同时将不同的技术和解决方案汇集在一起。这种整合和协作，将催生出新的 AI 开发范式，将大大降低 AI 技术应用的开发门槛。于是，矩阵起源将通过 MatrixGenesis 与 MatrixOne ，为 AI 开发者和用户提供一站式的数据共享、模型训练以及大模型应用开发和运维平台，帮助 AI 技术在各行各业快速落地，典型的应用场景包括企业知识库管理、智能客服、投诉分类、智能报告生成、政策咨询以及办公助理等。</p><p></p><p>其中，MatrixGenesis 作为一个 AI 智能体应用开发平台，“Model as a Service”（MaaS），不仅仅关注软件层面，还从数据到算力进行整体优化，通过提供全链路开发支持及高性价比的大模型服务，以帮助企业 0 门槛搭建 AI 应用。它支持大模型的对比选择、部署和推理，与结构化系统数据和知识库实时打通，提供 AI 应用工程流编排、应用对接集成的全生命周期开发支持， 企业可以一键部署开源和闭源大模型，快速完成模型精调和推理加速。同时，MetrixGenesis 平台允许客户以更高的性价比访问数据和算力资源，大幅降低业务使用成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8333508caf85625b133247ab4afcfe7.webp" /></p><p></p><p>在应用落地阶段，MetrixGenesis 利用 MatrixOne 的支持，为用户提供了完整的数据管理和使用能力，这包括 RAG、知识库和 Agent 等组件，它们通过 0 代码的配置方式，共同作用于应用的部署和运行，确保应用能够高效地利用数据资源。王龙形容 MetrixGenesis 时说道，“用户可以在这个平台上实现从数据到算力再到应用的全面优化，确保 AI 智能体应用的快速构建。”</p><p></p><p></p><h2>MatrixOS 与世纪互联 AIDC 的合作，是“中国版 CoreWeave”构建的开端</h2><p></p><p></p><p>在这个数字化迅速发展的时代，AI 技术如同一股不可阻挡的潮流，席卷了整个商业世界，众多企业纷纷开设 AIDC 部门，致力于构建和管理人工智能基础设施，提供算力、数据存储、模型训练等关键服务，以期在这场技术革新的浪潮中占据一席之地。大家深知，AI 技术的崛起不仅能够为企业带来新的增长点，更能在激烈的市场竞争中提升企业的核心竞争力。</p><p></p><p>世纪互联作为国内领先的第三方数据中心提供商，其 AIDC 策略具有鲜明的特点，具有非常坚决的“All In AI”的决心，将所有资源投入到 AI 算力的发展中，将传统的以机房机柜为中心的服务转变为“以 AI 算力为核心”。这一转变，反映了世纪互联对未来数字世界的深刻洞察——算力将成为未来 IT 世界最重要的核心资产。矩阵起源与世纪互联的合作初衷，也正是基于双方对算力和电力重要性的共识以及共同推进 AI 领域发展的愿景。</p><p></p><p>矩阵起源以其开源的 MatrixOS 软件，与世纪互联的 AIDC 产品相结合，共同打造了 GPU 平台服务——Neolink.AI，并将在今年的第三季度上线。这个平台不仅解决了企业在 AI 算力、软件连接能力以及成本控制上的问题，更是定位于成为一个开源、共享的 AI 算力平台，旨在降低推理成本，吸引更多用户，从而实现从成本中心到价值创造中心的转变。</p><p></p><p>世纪互联认为 AI 的上限是算力和电力，而矩阵起源则认为算力和数据是 AI 的瓶颈，所以他们不谋而合地提出了打造“中国版 CoreWeave”的想法，希望通过 Neolink.AI 平台，推动 AI 技术的本土化发展，提升国内 AI 产业的创新力和竞争力。这个想法的实现，将对中国的 AI 领域产生深远的影响，为中国在全球 AI 领域的发展奠定坚实的基础。</p><p></p><p>在对王龙的采访中，我们也了解到，世纪互联即将获得一批合规的 H20 芯片，这批芯片的到来将是一个重要的里程碑，标志着世纪互联在算力资源方面迈出了坚实的一步，将会通过 Neolink.AI 开放给大家。</p><p></p><p></p><h2>拥抱开源的 MatrixOS，是赋能千行百业 AI 落地的加速器</h2><p></p><p></p><p>在当下这场澎湃的 AI 浪潮中，企业急需一艘能够驾驭风浪的航船。MatrixOS，便是一艘非常有潜力的快船。MatrixOS 不仅仅是一个操作系统，它是企业 AI 构建的加速器，它通过提供高效、稳定的数据处理能力和灵活的 AI 技术支持，让企业能够轻松跨越技术门槛，无需投入巨额资源和精力去构建复杂系统，便能迅速实现 AI 应用构建。从基础设施层，到数据库层，再到应用开发层，MatrixOS 为千行百业提供 AI 时代的工具全家桶，助力生态打造高效智能的 AIGC 方案，如制造业的智能化生产、金融业的精准风控……</p><p></p><p>展望未来，MatrixOS 的技术创新之路充满了无限可能。矩阵起源在未来将会持续努力寻找新数据、新场景。随着计算能力的不断突破，MatrixOS 将能够处理更庞大、更复杂的数据集，为 AI 技术的发展提供坚实的算力支持和模型服务。</p><p></p><p>据王龙表示，MatrixOS 的探索不止于此。它还将与物联网、5G 等前沿技术紧密结合，打造全新的技术解决方案。在这个物联网设备日益普及、5G 网络商用化的时代，数据正以前所未有的速度增长，MatrixOS 将通过融合这些技术，实现更高效的数据管理能力，为企业提供更及时、更准确的数据服务、AI 开发服务，推动千行百业迈向更高层次的智能化和数字化。</p><p></p><p>同时，MatrixOS，作为一个开源平台，其生态建设的重要性不言而喻。它将与世纪互联等战略投资者携手，吸引更多企业和个人加入到开源社区中来，共同推动 AI 技术的进步和普及。MatrixOS 还将举办线下交流活动、技术大赛，加强社区成员之间的交流和合作，共同营造一个开放、共享、创新的 AI 技术生态。</p><p></p><p>MatrixOS，不仅是技术的革新，更是智慧的传承，让我们共同见证并参与这场伟大的变革。MatrixOS 的故事才刚刚开始，希望更多的企业和个人关注并支持 MatrixOS 的发展，共同推动 AI 技术的升级和普及，为各行各业的“全面 AI 化”的数字化转型贡献智慧和力量，迎接一个更加智能、更加互联的 AI 未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</id>
            <title>大模型应用之基于Langchain的测试用例生成</title>
            <link>https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 05:54:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型生成测试用例, LangChain, 测试用例生成方案, 技术细节说明
<br>
<br>
总结: 文中介绍了使用大模型生成测试用例的实践效果，探索了基于LangChain的方法，解决了现有工具的痛点，提高了测试用例生成的效率和质量。详细介绍了基于LangChain的测试用例生成方案和实现细节，包括整体流程、技术细节说明、代码框架和部分代码展示。整体内容涵盖了使用大模型生成测试用例的全过程。 </div>
                        <hr>
                    
                    <p></p><h1>一 用例生成实践效果</h1><p></p><p>在组内的日常工作安排中，持续优化测试技术、提高测试效率始终是重点任务。近期，我们在探索实践使用大模型生成测试用例，期望能够借助其强大的自然语言处理能力，自动化地生成更全面和高质量的测试用例。</p><p></p><p>当前，公司已经普及使用JoyCoder，我们可以拷贝相关需求及设计文档的信息给到JoyCoder，让其生成测试用例，但在使用过程中有以下痛点：</p><p></p><p>1）仍需要多步人工操作：如复制粘贴文档，编写提示词，拷贝结果，保存用例等</p><p></p><p>2）响应时间久，结果不稳定：当需求或设计文档内容较大时，提示词太长或超出token限制</p><p></p><p>因此，我探索了基于Langchain与公司现有平台使测试用例可以自动、快速、稳定生成的方法，效果如下：</p><p><img src="https://static001.geekbang.org/infoq/43/4328f52343c11804154293baa39b28a9.png" /></p><p>（什么是LangChain？ 它是一个开源框架，用于构建基于大型语言模型（LLM）的应用程序。LLM 是基于大量数据预先训练的大型深度学习模型，可以生成对用户查询的响应，例如回答问题或根据基于文本的提示创建图像。LangChain 提供各种工具和抽象，以提高模型生成的信息的定制性、准确性和相关性。例如，开发人员可以使用 LangChain 组件来构建新的提示链或自定义现有模板。LangChain 还包括一些组件，可让 LLM 无需重新训练即可访问新的数据集。）</p><p></p><h1>二 细节介绍</h1><p></p><p></p><h2>1 基于Langchain的测试用例生成方案</h2><p></p><p></p><p>因3种方案使用场景不同，优缺点也可互补，故当前我将3种方式都实现了，提供大家按需调用。</p><p></p><h2>2 实现细节</h2><p></p><p></p><h3>2.1 整体流程</h3><p></p><p>​</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0228566441a94e1b2543d97d8d6bebb.png" /></p><p>​</p><p></p><h3>2.2 技术细节说明</h3><p></p><p>•pdf内容解析： ：Langchain支持<a href="https://python.langchain.com/docs/modules/data_connection/document_loaders">多种文件格式的解析</a>"，如csv、json、html、pdf等，而pdf又有很多<a href="https://zhuanlan.zhihu.com/p/352722932">不同的库</a>"可以使用，本次我选择PyMuPDF，它以功能全面且处理速度快为优势</p><p></p><p>•文件切割处理： 为了防止一次传入内容过多，容易导致大模型响应时间久或超出token限制，利用Langchain的<a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token">文本切割器</a>"，将文件分为各个小文本的列表形式</p><p></p><p>•Memory的使用： 大多数 LLM 模型都有一个会话接口，当我们使用接口调用大模型能力时，每一次的调用都是新的一次会话。如果我们想和大模型进行多轮的对话，而不必每次重复之前的上下文时，就需要一个Memory来记忆我们之前的对话内容。Memory就是这样的一个模块，来帮助开发者可以快速的构建自己的应用“记忆”。本次我使用<a href="https://python.langchain.com/docs/modules/memory/types">Langchain的ConversationBufferMemory与ConversationSummaryBufferMemory</a>"来实现，将需求文档和设计文档内容直接存入Memory，可减少与大模型问答的次数（减少大模型网关调用次数），提高整体用例文件生成的速度。ConversationSummaryBufferMemory主要是用在提取“摘要”信息的部分，它可以将将需求文档和设计文档内容进行归纳性总结后，再传给大模型</p><p></p><p>•向量数据库： 利用公司已有的向量数据库[测试环境Vearch]，将文件存入。 在创建数据表时，需要了解向量数据库的检索模型及其对应的参数，目前支持六种类型，IVFPQ，HNSW，GPU，IVFFLAT，BINARYIVF，FLAT（<a href="https://github.com/vearch/vearch/wiki/Vearch%E7%B4%A2%E5%BC%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9">详细区别和参数可点此链接</a>"），目前我选择了较为基础的IVFFLAT--基于量化的索引，后续如果数据量太大或者需要处理图数据时再优化。另外Langchain也有很方便的<a href="https://python.langchain.com/docs/integrations/vectorstores/vearch">vearch存储和查询的方法可以使用</a>"</p><p></p><h3>2.3 代码框架及部分代码展示</h3><p></p><p>代码框架：</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc4beb2a38f97d3ea492c03462913e12.png" /></p><p>​</p><p>​代码示例：</p><p></p><p><code lang="text">    def case_gen(prd_file_path, tdd_file_path, input_prompt, case_name):
        """
        用例生成的方法
        参数:
        prd_file_path - prd文档路径
        tdd_file_path - 技术设计文档路径
        case_name - 待生成的测试用例名称
        """
        # 解析需求、设计相关文档, 输出的是document列表
        prd_file = PDFParse(prd_file_path).load_pymupdf_split()
        tdd_file = PDFParse(tdd_file_path).load_pymupdf_split()
        empty_case = FilePath.read_file(FilePath.empty_case)

        # 将需求、设计相关文档设置给memory作为llm的记忆信息
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content="You are a chatbot having a conversation with a human."
                ),  # The persistent system prompt
                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # Where the memory will be stored.
                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # Where the human input will injected
            ]
        )
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        for prd in prd_file:
            memory.save_context({"input": prd.page_content}, {"output": "这是一段需求文档，后续输出测试用例需要"})
        for tdd in tdd_file:
            memory.save_context({"input": tdd.page_content}, {"output": "这是一段技术设计文档，后续输出测试用例需要"})

        # 调大模型生成测试用例
        llm = LLMFactory.get_openai_factory().get_chat_llm()
        human_input = "作为软件测试开发专家，请根据以上的产品需求及技术设计信息，" + input_prompt + ",以markdown格式输出测试用例，用例模版是" + empty_case
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            verbose=True,
            memory=memory,
        )
        output_raw = chain.invoke({'human_input': human_input})

        # 保存输出的用例内容，markdown格式
        file_path = FilePath.out_file + case_name + ".md"
        with open(file_path, 'w') as file:
            file.write(output_raw.get('text'))
</code></p><p></p><p><code lang="text">    def case_gen_by_vector(prd_file_path, tdd_file_path, input_prompt, table_name, case_name):
        """
        !!!当文本超级大时，防止token不够，通过向量数据库，搜出某一部分的内容，生成局部的测试用例，细节更准确一些!!!
        参数:
        prd_file_path - prd文档路径
        tdd_file_path - 技术设计文档路径
        table_name - 向量数据库的表名，分业务存储，一般使用业务英文唯一标识的简称
        case_name - 待生成的测试用例名称
        """
        # 解析需求、设计相关文档, 输出的是document列表
        prd_file = PDFParse(prd_file_path).load_pymupdf_split()
        tdd_file = PDFParse(tdd_file_path).load_pymupdf_split()
        empty_case = FilePath.read_file(FilePath.empty_case)
        # 把文档存入向量数据库
        docs = prd_file + tdd_file
        embedding_model = LLMFactory.get_openai_factory().get_embedding()
        router_url = ConfigParse(FilePath.config_file_path).get_vearch_router_server()
        vearch_cluster = Vearch.from_documents(
            docs,
            embedding_model,
            path_or_url=router_url,
            db_name="y_test_qa",
            table_name=table_name,
            flag=1,
        )
        # 从向量数据库搜索相关内容
        docs = vearch_cluster.similarity_search(query=input_prompt, k=1)
        content = docs[0].page_content

        # 使用向量查询的相关信息给大模型生成用例
        prompt_template = "作为软件测试开发专家，请根据产品需求技术设计中{input_prompt}的相关信息:{content},以markdown格式输出测试用例，用例模版是:{empty_case}"
        prompt = PromptTemplate(
            input_variables=["input_prompt", "content", "empty_case"],
            template=prompt_template
        )
        llm = LLMFactory.get_openai_factory().get_chat_llm()
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            verbose=True
        )
        output_raw = chain.invoke(
            {'input_prompt': input_prompt, 'content': content, 'empty_case': empty_case})
        # 保存输出的用例内容，markdown格式
        file_path = FilePath.out_file + case_name + ".md"
        with open(file_path, 'w') as file:
            file.write(output_raw.get('text'))
</code></p><p></p><h1>三 效果展示</h1><p></p><p></p><h3>3.1 实际运用到需求/项目的效果</h3><p></p><p>用例生成后是否真的能帮助我们节省用例设计的时间，是大家重点关注的，因此我随机在一个小型需求中进行了实验，此需求的PRD文档总字数2000+，设计文档总字数100+（因大部分是流程图），结果效率提升50%。</p><p></p><p>本次利用大模型自动生成用例的优缺点：</p><p></p><p>优势：</p><p></p><p>•全面快速的进行了用例的逻辑点划分，协助测试分析理解需求及设计</p><p></p><p>•降低编写测试用例的时间，人工只需要进行内容确认和细节调整</p><p></p><p>•用例内容更加全面丰富，在用例评审时，待补充的点变少了，且可以有效防止漏测</p><p></p><p>•如测试人员仅负责一部分功能的测试，也可通过向量数据库搜索的形式，聚焦部分功能的生成</p><p></p><p>劣势：</p><p></p><p>•暂时没实现对流程图的理解，当文本描述较少时，生成内容有偏差</p><p></p><p>•对于有丰富经验的测试人员，自动生成用例的思路可能与自己习惯的思路不一致，需要自己再调整或适应</p><p></p><h1>四 待解决问题及后续计划</h1><p></p><p>1.对于pdf中的流程图（图片形式），实现了文字提取识别（langchain pdf相关的方法支持了ocr识别），后续需要找到更适合解决图内容的解析、检索的方式。</p><p></p><p>2.生成用例只是测试提效的一小部分，后续需要尝试将大模型应用与日常测试过程，目前的想法有针对diff代码和服务器日志的分析来自动定位缺陷、基于模型驱动测试结合知识图谱实现的自动化测试等方向。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</id>
            <title>甲骨文副总裁：只会SQL也可以搞定AI，但对 DBA 的要求将更高</title>
            <link>https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, Oracle Database 23ai, AI功能, 向量搜索
<br>
<br>
总结: 甲骨文公司发布了Oracle Database 23ai，其中加入了突破性的AI技术，包括AI for Data、简化数据中的AI使用等功能，其中的AI Vector Search功能可以根据概念内容搜索文档、图像和关系数据。这一举措使得即使只懂得SQL的用户也能够全盘搞定AI应用，展示了数据库AI化的必然趋势。 </div>
                        <hr>
                    
                    <p>“最重要的不在于拥有多少大模型，而在于如何使用它们。”甲骨文公司副总裁及中国区董事总经理吴承杨说道。&nbsp;</p><p>&nbsp;</p><p>在全民探索大模型应用的现在，甲骨文也积极尝试。Oracle 融合数据库中的最新版本&nbsp;<a href="https://www.oracle.com/cn/database/">Oracle Database 23ai</a>"&nbsp;作为广泛的云技术服务正式发布，标志着甲骨文在AI领域的重大进展。</p><p></p><h2>“只会SQL，也可以全盘搞定AI”</h2><p></p><p>&nbsp;</p><p>由于这个版本主要加入了突破性的 AI 技术，因此甲骨文将 Oracle Database 23c 重命名为 Oracle Database 23ai。</p><p>&nbsp;</p><p>甲骨文公司中国区技术咨询部高级总监李珈介绍，23ai 专注AI主要体现在三个方面：一是AI for Data，在数据应用在AI的层面上做深入加持；二是针对应用开发者，在使用层面上能够更简单；三是针对关键任务赋予AI 能力。</p><p>&nbsp;</p><p>此长期支持版本包含了&nbsp;Oracle AI Vector Search 、300多个主要新功能和数千项增强功能，专注于帮助用户简化数据中的&nbsp;AI 使用。</p><p>&nbsp;</p><p>AI Vector Search（AI向量搜索）&nbsp;是 Oracle Database 23ai的一项重要功能，用户可以借此根据概念内容（而不是特定的文字、像素或数据值）来搜索文档、图像和关系数据，同时用户可以使用自然语言界面查询私有业务数据，并帮助&nbsp;LLM 提供更准确和更相关的结果。</p><p>&nbsp;</p><p>为什么不是一个单独的向量数据库或在Database引擎之上再拓展一个向量引擎？李珈表示，这样做的最大好处就是可以把业务数据和向量数据整合在一起。比如，原来用大量的业务数据做AI应用需要把数据拷贝出去，AI Vector Search则可以用一个SQL直接查找业务数据和向量数据及其他数据类型的数据。</p><p>&nbsp;</p><p>在向量化过程中，原来的做法是调一个大模型或一个嵌入的算法，这对有GPU资源的用户比较友好，对于多数没有设备资源的用户，Oracle提出了新的SQL嵌入函数，可以把符合标准的嵌入模型放到Oracle里面，用Oracle数据库的引擎来帮用户做向量化，这样意味着即便没有AI方面的经验、只会SQL，也可以全盘搞定这个过程。</p><p>&nbsp;</p><p>此外，Oracle Database 23ai 可以在客户数据中心本地部署，也可以在云上部署。甲骨文还提供了Oracle Digital Assistant，这是一个嵌入在Oracle应用中的数据助手，可以帮助回答客户问题。</p><p>&nbsp;</p><p>“300多个新功能，对于23ai的这个版本来讲非常有战略意义，是真正的 Game Changer。” 李珈说道。</p><p>&nbsp;</p><p>甲骨文投入了大量的资金用于人才培养、科技研发和基础设施建设，以确保用户能够获得高质量的服务。Oracle Database 23ai的研发周期通常为五年一个大版本的更新，但在每个季度都会有季度性的版本发布。</p><p>&nbsp;</p><p>费用方面，吴承杨表示，甲骨文数据库的定价一直是以使用的CPU量来计算的，不会因为23ai的发布而改变定价规则。Oracle Database 23ai 中 AI 功能的使用不需要额外付费，因为它是作为一个新功能添加到现有版本中的。</p><p>&nbsp;</p><p>“生成式AI用得好不好，只有一个标准，就是可以给用户带来什么效果，用户是否得到了本质性上的改变。”吴承杨强调，AI的功能应该像用电一样简单易用，用户不需要成为AI科学家，只需懂得SQL即可。</p><p>&nbsp;</p><p>李珈介绍，甲骨文开始做Vector DB以来，金融、电信、制造业等用户也积极跟进，应用场景包括欺诈分析，各种各样的智能体、AI 助手，长视频检测等。</p><p>&nbsp;</p><p></p><h2>“数据库 AI 化是一个必然趋势”</h2><p></p><p>&nbsp;</p><p>“随着生成式AI的出现，融合数据库将变得更加重要。”吴承杨说道。</p><p>&nbsp;</p><p>融合数据库能够处理结构化数据、非结构化数据、图数据、JSON和空间数据等多种数据类型，因此能够简化应用和分析的生成与运行。</p><p>&nbsp;</p><p>企业在使用生成式AI ，大模型应用时，更希望将企业内部数据放在本地。用户可以使用任何一个国产或国外的模型，比如Llama-3，可以使用各种数据库来做RAG。没有融合数据库，这些事情也可以做到，但需要一个很强大的开发商和复杂的技术架构。。</p><p>&nbsp;</p><p>吴承杨认为，简化企业大模型应用的逻辑是“四个任何”：任何时候、任何地方、任何人、任何数据，都可以使用。把这四个加起来以后，就真正地解决了很多的问题。甚至用户自己内部的工程师、以前的DBA就可以解决。</p><p>&nbsp;</p><p>吴承杨提到，随着AI进入数据库，未来对DBA的要求将会有所不同。“对未来的DBA来说，像数据库的管理、打补丁等能力基本不太需要或者需要比较少，但是对整个架构、整个数据应用方面的要求，会比以前的要求高很多。虽然仍被称为DBA，但是要求不一样、工作范畴更大。”</p><p>&nbsp;</p><p>甲骨文还强调了其向量数据库产品完全继承了Oracle数据库的企业级特性，包括安全、稳定、可靠和可扩展性。Oracle Real Application Clusters (RAC) 保护实例，Active Data Guard (ADG) 保护容灾。甲骨文还提供了强大的备份功能，以及Exadata数据库一体机，以满足高性能需求。</p><p>&nbsp;</p><p>“Oracle AI 使我们的融合数据库上了一个非常大的台阶。”吴承杨说道。</p><p>&nbsp;</p><p>吴承杨表示，数据库的AI化是一个必然趋势。对于一个数据库先进性的衡量，对AI的支持是必选项。而对于甲骨文来说，现在只是迈出了第一步，未来的想象空间是非常大的。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8mvby5h6dglDsO7akkti</id>
            <title>大模型应用商业化落地关键：给企业带来真实的业务价值</title>
            <link>https://www.infoq.cn/article/8mvby5h6dglDsO7akkti</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8mvby5h6dglDsO7akkti</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 10:39:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, ToB 领域, 商业化落地, 数据分析 Agent
<br>
<br>
总结: 2024 年被称为大模型应用的元年，大模型已成共识，但如何在 ToB 领域落地商业化仍是关键问题。ToC 市场发展迅猛，ToB 市场潜力巨大，大模型结合 Agent 在企业经营分析领域有望落地。数势科技推出的 SwiftAgent 2.0 在数据分析和决策方面具有突破性，为大模型商业化提供了实际落地路径。 </div>
                        <hr>
                    
                    <p>2024 年被很多人称为大模型应用的元年，毫无疑问，大模型已经成为共识，下一步更急迫的问题也摆在了大家的面前——大模型到底能够用在哪？有哪些场景能落地？怎么做才能创造真正的价值？</p><p></p><p>在刚刚过去的 AICon 全球人工智能开发与应用大会上，InfoQ 采访了在大模型应用领域的领跑企业数势科技创始人兼 CEO 黎科峰博士，交流大模型商业化落地的可行性路径，为行业提供启发。</p><p></p><p></p><p></p><p>大模型在 ToB 领域蕴藏巨大机遇，企业出海或将成为落地加速器</p><p></p><p>当前，许多传统企业对于如何将大模型技术整合到现有业务中感到迷茫。大模型应用产品提供企业难以找到合适的变现方式。大模型应用企业究竟该如何突破商业化之困？ToB 和 ToC，呈现出两种不同的路径。</p><p></p><p>ToC 市场似乎拥有一种天然的魔力，在中国的互联网领域中，它催生了一批又一批的杀手级应用，并不断吸引着更多企业加入这个生态系统。ToC 市场如火如荼，ToB 市场发展因为投资回报率、数据安全性等问题导致相对发展缓慢。但纵观中国科技发展史， ToB 市场经历了 20 多年的从信息化到数字化再到智能化等多个技术阶段的演变与发展，每一次技术革命都经历了探索和融合期，最终帮助企业更好地为用户提供个性化产品、满足特定需求。</p><p></p><p>因此黎科峰认为，大模型在 ToB 市场同样蕴藏着更大的机遇。从用户的角度来看，广大用户，包括企业管理者、技术人员和业务人员，已经开始熟悉并运用大模型进行日常的数据处理、知识获取等，当企业公民使用率提升后，将反向推动 To B 软件的发展，真正实现软件产品“从管到用”的进化。从市场选择来看，企业出海也许将加速大模型商业化落地。一方面，由于海外企业用户更倾向选择公有云部署，能够为大模型商业化提供更加全面的场景数据积累，不断丰富和优化场景应用；另一方面，SaaS 付费模式在海外接受程度高于国内市场，有利于大模型应用企业更轻量、高效地实现商业化落地。</p><p></p><p>大模型 +Agent，将在企业经营分析领域落地</p><p></p><p>大模型具备知识、智商、学习能力和推理能力，能够总结和生成新的见解，叠加 Agent，让企业应用具备了记忆、反思和学习能力，能够调用企业内部工具并不断迭代反思，真正实现业务价值的落地。据行业调研表明，数据分析和决策被视为这一结合最重要的应用方向。</p><p></p><p>在企业考虑引入数据分析 Agent 产品时，成本、数据安全以及实际落地价值将是关注重点。部署千亿级别参数的大模型成本高昂，目前或许只有头部企业才能承担。同时，数据安全问题也不容忽视，特别是在受严格监管的金融等行业。再者，企业也需要确保大模型投入后能对业务产生实质性的帮助。</p><p></p><p>黎科峰表示，为解决这些问题，数势科技为企业提供了规模适中、符合企业经营分析场景的 Agent 产品，来降低企业成本和使用门槛。同时，通过云端私有化的专属数据空间以及支持私有化和本地化部署的方式，确保数据安全。</p><p></p><p>SwiftAgent 2.0 发布，实现企业数据分析与决策的范式变革</p><p></p><p>自大模型技术发布之初，数势科技便迅速拥抱这一变革，并在此基础上推出了 SwiftAgent 的新一代产品。黎科峰在采访中介绍，SwiftAgent 2.0 版本与 1.0 相比，主要新增了五大亮点功能，包括增加了指标和标签语义层、多模态和多源异构数据链接、用户可干预、可持续性学习和数据加速引擎，这些亮点体现了产品如何针对企业数据分析的现有问题提供解决办法。</p><p></p><p>“统一语义层的构建”，SwiftAgent 2.0 通过添加指标和标签语义层，解决了大模型对底层业务语义理解的难题，并统一了企业各部门的数据口径，从而避免了数据的混乱。</p><p></p><p>“多模态和多源异构数据链接”使得结构化数据和非结构化数据能够结合，提供更丰富的分析结论。</p><p></p><p>“用户可干预机制”允许用户参与到数据分析过程中，通过反馈和确认来训练和优化 Agent。</p><p></p><p>“数据加速引擎”则确保了实时数据处理的能力，实现了秒级数据查询，真正达到了实时人机交互的水平。底层选用了性能优异的数据分析引擎，如 StarRocks、Doris，并结合对数据加工和使用场景的优化，提供了基于视图的预计算能力和基于预计算结果的查询优化能力。</p><p></p><p>此外，数据虚拟化技术的应用，使得数据定义与物理数据解耦，实现了指标 / 标签的灵活加工使用，无需排期开发，进一步提高了数据处理的效率。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2y/02/2yyafd33566da283d3d0835f654a8a02.png" /></p><p></p><p>大模型应用商业化落地的关键：找到产品的业务价值</p><p></p><p>在商业化方面，大模型仍然属于新兴事物。为了让更多行业看到大模型在真实业务场景中的应用价值，数势科技也在致力于创造实际的落地标杆。“只有通过口口相传和业务价值的真实感知，才能让有价值的产品被更多人接受和使用。”黎科峰认为。</p><p></p><p>目前，SwiftAgent 已经与行业头部客户签订了合同，并实现了真正的付费使用。这标志着数势科技在大模型领域的领先地位，其产品已经在金融、零售、消费和高端制造等多个行业中得到了应用，帮助企业解决实际的业务问题。</p><p></p><p>以一家头部茶饮连锁企业为例，黎科峰介绍说明，SwiftAgent 通过在集团层面部署语义层和数据分析框架，并将 Agent 提供给每位店长，使得他们能够通过对话式查询快速获取数据和分析结果，从而提升门店运营效率。这种数字化工具的应用，为企业提供了一个经营闭环，改变了过去门店靠经验经营的困境，极大地促进了业务发展。</p><p></p><p>数势科技 SwiftAgent 2.0 实现了大模型技术在企业经营场景下的价值验证，也提升了产品的智能化程度，正在颠覆企业企业数据分析与决策范式。黎科峰表示，数势科技的愿景便是以科技创新实现数据价值的普惠化，让每一个企业、每一位员工都能体会到数据带来的价值。</p><p></p><p>未来，我们期待看到更多像数势科技这样的企业，通过创新的技术和产品，推动大模型商业化进程，为各行各业带来价值提升。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/61JsqmVQbZA0pqoOpbeK</id>
            <title>价格战，并不是大模型厂商的初衷</title>
            <link>https://www.infoq.cn/article/61JsqmVQbZA0pqoOpbeK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/61JsqmVQbZA0pqoOpbeK</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 10:30:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 商战, 降价, 大模型, MaaS模式
<br>
<br>
总结: 本文讨论了大模型厂商之间的价格战，阿里云降价引发了国内外大模型厂商的跟风降价，降幅高达80%。文章指出降价背后的价值和影响，探讨了MaaS模式对大模型应用的推动作用，以及如何通过公有云、API调用和应用工具实现大模型应用的落地。文章强调了技术的价值在于解决实际问题，而大模型的降价将促进创新和推广。 </div>
                        <hr>
                    
                    <p>商战，往往是朴实无华的。</p><p></p><p>OpenAI 吹响了降价号角之后，国内外大模型厂商陆续发布降价消息。直至 5 月 21 日，阿里云发布降价公告，将旗下通义千问的多款商业化及开源模型进行大幅降价，彻底将这场降价狂欢推向高潮。</p><p></p><p>大模型“价格战”的厮杀也将愈演愈烈——不管是头部厂商还是中小大模型厂商都在搞降价，降价幅度逐渐走高，降幅 80% 都变得不那么出人意外，这一系列操作给科技圈和经济圈带来了大大震撼，谁也不想错过这么好的流量。一时间互联网上充斥着无数实时更新的资讯，深度浅度的分析，莫名其妙的预测。风起云涌，硝烟四起……</p><p></p><p>请暂停一下。</p><p></p><p>虽然一场以降价为起点的互联网狂欢会在一年当中出现很多次，但国内大模型的降价还是第一次。降价带给我们的，难道仅仅是短期的优惠和吃瓜的快乐吗？在这场价格战背后，我们究竟能获得什么？为了解答这些问题，我们对最近的这波“模型厂商的降价狂欢”做了全面观察，笔者发现阿里云的此次降价有点不一样，不仅全线 9 款模型降价，且主力模型 Qwen-Long 作为对标 GPT-4 的国产大模型是降幅相当大，高达 97%。这不禁让笔者陷入思考，下文以期从技术角度拆解。</p><p></p><p>入场：MaaS 模式开启，阿里云领跑大模型应用新时代</p><p></p><p>随着 MaaS 理念、模型开源开放技术发展理念被行业里越来越多人推崇，阿里云已经站在大模型应用新时代的领跑位置。自去年下半年起，行业风向明显转变，模型厂商们开始深刻认识到，技术的真正价值在于解决实际问题，而不仅仅是技术本身的堆砌。根据国家数据局今年 3 月发布的数据，中国已拥有超过 100 个参数超过 10 亿的大型模型，这些模型在多个行业中形成了上百种应用模式，有效推动了各行业的数字化转型。</p><p></p><p>MaaS 模式应运而生，它标志着大型模型技术从“拼业务”转向“拼市场”的新阶段。通过 MaaS，企业可以按需访问云平台上的大型模型，无需承担高昂的硬件投资和长期维护成本，从而更加灵活、高效地使用这些先进技术，加速业务创新和市场响应。在阿里云看来大模型将更像AI时代的操作系统，大多软件都将基于大模型调用GPU，而不再会直接调用GPU。</p><p></p><p>阿里云敏锐地捕捉到了这一市场趋势，率先推出了 MaaS 模式。通过高性能 AI 集群“灵骏”，阿里云为万卡规模的 AI 集群提供了无拥塞、高性能的集群通讯能力，为大模型应用的快速落地提供了坚实的算力基础。</p><p>阿里云 MaaS 的独特优势还体现在其对大模型应用快速落地的助推作用。通过提供一站式的大模型应用开发平台「百炼」，为需要在内部进行大模型训练的企业提供了一个安全的数据使用环境。</p><p></p><p>市场竞争加剧也对企业的商业模式和造血能力提出了更高要求，行业或加速洗牌。一些产品成熟度不高、资金实力不强的企业可能在价格战中被淘汰。大模型创业公司月之暗面创始人杨植麟表示，过度的价格战可能会伤害那些专注于技术创新但资金实力有限的中小型企业，希望行业能回归理性竞争。</p><p></p><p>不过，在任何一场价格战中，无论商家如何“搏杀”，用户至少短期来看都能从中受益。大模型的调用成本大幅降低，有利于创业公司加快创新步伐，开发出更多商业化的大模型应用，促进大模型技术的普及与推广。</p><p></p><p>站在变革的十字路口，如何从技术实力、生态建设、应用创新等方面厚积薄发，是目前对于模型厂商来说，比价格战更重要的事情。</p><p></p><p>亮剑：公共云 +API 调用 + 应用工具，是大模型应用最好的落地范式</p><p></p><p>如果大模型不能实现有效的落地应用，那么无论其技术多么先进，其实际意义也将大打折扣。技术的价值一定是在于其能够解决现实问题、提高效率、创造新的可能性。而阿里云百炼的业务主张一直是“公有云 +API+ 应用的落地范式”，从他们整个发展过程来看，这个主张行之有效。</p><p></p><p>得益于“公有云 +API”的边际效应，阿里云的 MaaS 服务体系让按需进行算力“租借”模式延续到企业 AI 创新领域。加之各模型厂商 API 的持续降价，最直接的影响是推理成本的普惠，毕竟所有的模型应用过程都需要推理。但基础模型本身并不能决定任何一家企业的 AI 应用能否成功落地。模型只是基础，真正的关键在于如何将这些工具应用到实际业务场景中，并解决具体问题。而要实现这一点，使用门槛的降低就显得尤为重要。</p><p></p><p>使用门槛的降低能够释放大模型的红利，让更多的企业和开发者能够轻松接入和使用这些先进技术。这正是阿里云推出 MaaS 模式和百炼大模型服务平台的核心思想。百炼作为一站式的大模型应用开发平台，其独特的架构和优势为企业提供了强大的支持。</p><p></p><p>「百炼」凭借创新的开放架构设计，为企业提供了一站式的 AI 解决方案。该平台整合了模型服务，配备了全面的模型开发工具和应用工具，实现了从大模型输出到智能体发布的无缝对接。利用智能体构建、应用广场及自定义画布流程编排等先进工具，企业得以迅速构建并部署大模型应用。</p><p></p><p>百炼平台拥有全面的模型服务。它提供从数据管理、模型训练、评估到部署的全链路服务，支持多种大模型和框架，以满足不同业务场景的需求。平台通过智能调度和优化算法，实现 GPU 资源的最大化利用，有效降低用户算力成本。此外，百炼平台支持多种开源和商业化模型，用户可轻松接入和使用，加速 AI 应用的开发进程。无论是云上部署还是私有化部署，百炼平台都能提供灵活的解决方案，同时保障用户数据的安全和隐私。用户友好的界面和工具使得开发者能够更高效地进行工作。</p><p></p><p>在模型功能及生态方面，百炼平台展现了其强大的实力。它支持多种模型的训练和微调，包括预训练模型、全参微调和 PEFT（参数高效微调）等，以满足不同场景下的模型优化需求。全面的模型评估指标和工具帮助用户快速了解模型性能并进行优化。百炼平台支持模型的快速部署和推理，同时开放 API 和插件接口，支持用户自定义模型和工具，进一步丰富模型生态。</p><p></p><p>针对 AI 应用落地场景对模型平台的需求，百炼平台同样表现出色。它支持基于 Open AI 的 Assistant API 架构，方便开发者集成。支持 Llamaindex、langchain 等开源框架，使得模型能够在实际业务场景中发挥作用，实现智能交互和决策。Prompt 工程支持帮助用户构建高质量的指令和回答，提升模型的准确性和稳定性。在数据管理与预处理方面，百炼平台提供高效的数据处理功能，以提高模型训练的质量和效率。</p><p></p><p>以朗新科技为例，该公司基于「百炼」成功训练出电力专属大模型，并开发出多款产品，如“电力账单解读智能助手”，显著提升了客户接待效率并降低了投诉率。</p><p></p><p>在最近的 AI 智领者峰会上，阿里云智能集团资深副总裁刘伟光强调：“作为中国领先的云计算公司，阿里云此次大幅降低大模型推理价格，旨在加速 AI 应用的广泛普及。我们预见，未来大模型 API 的调用量将实现成千上万倍的增长。”</p><p></p><p>阿里云的「百炼」战略核心在于，通过降价降低企业使用 AI 技术的门槛，实现技术的普及与惠民。从技术、时间和费用三个成本维度来看，这一策略不仅减轻了企业的技术投入压力，还通过优化服务流程，如简化模型部署和应用集成，降低了企业在技术学习和应用上的时间成本。同时，后付费结算模式和免费额度的提供，进一步减少了企业的费用支出，使企业能够以更低的风险尝试和采纳 AI 技术。</p><p></p><p>阿里云的「百炼」战略核心在于，通过持续打磨的千问大模型，提升企业及用户的使用效果，带来体验的飞升。通过技术升级带来降价，降低企业使用 AI 技术的门槛，实现技术的普及与惠民；通过便捷开放的应用构建工具，提高企业集成大模型的效率，打通大模型到业务落地最后一公里。从技术、时间和费用三个成本维度来看，这一策略不仅减轻了企业的技术投入压力，还通过优化服务流程，提升集成及开发效率，降低了企业在技术学习和应用上的时间成本。同时，后付费结算模式和免费额度的提供，进一步减少了企业的费用支出，使企业能够以更低的风险尝试和采纳 AI 技术。</p><p></p><p>创新战：降价策略助力企业普惠，推动 AI 产业共创共赢</p><p></p><p>商战，价格不是唯一，所谓价格战的本质其实是技术创新之战。</p><p></p><p>这一策略背后隐藏着深层次的商业逻辑和市场考量。通过降价，厂商不仅能够利用技术领先优势快速占领市场份额，还能加速产品迭代，根据用户反馈和数据持续优化产品。同时，降价有助于构建和扩大以大模型为核心的生态系统，吸引更多开发者和合作伙伴加入，共同推动技术和应用创新。厂商通过降价适应市场变化和用户需求，使高端技术更加普及，从而加速技术的广泛应用。</p><p></p><p>在这场 AI 技术的浪潮中，回看大模型厂商之间的这场价格战，商业竞争更是表象，它其实是更深层次地反映了企业对 AI 技术普惠的追求和对行业共赢未来的承诺。阿里云大模型服务平台百炼的降价策略，正是基于这样的理念，通过降低使用门槛，使更多的企业能够享受到 AI 技术带来的变革。</p><p></p><p>当模型数量和价格不再是竞争的唯一标准时，如何使模型更加实用、易用，成为了新的竞技场。阿里云凭借其在算力资源、数据管理、研发人才以及数据中心管理等方面的优势，已经具备了成为国内顶尖模型基础设施提供者的条件。通义千问开源模型的高下载量和企业客户的快速增长，也正是阿里云在模型领域领导地位的有力证明。</p><p></p><p>对于中小科技企业而言，AI 技术的降价策略带来了更广阔的的时长和机遇。他们能够以更低的成本接入先进的 AI 技术，从而在市场中获得竞争优势，甚至有机会再次颠覆传统的互联网格局。</p><p></p><p>此次的降价浪潮也让我们我们能够预想到大模型技术的应用场景将不断拓展，对社会和行业的积极影响也将日益显著。</p><p></p><p>总结来说就是，百模大战，已经结束。</p><p></p><p>但这并非 AI 发展的终点，而是技术新征程的起点。随着大模型技术的普及和成熟，AI 产品技术之间的真正角逐才刚刚拉开帷幕，未来一定是一个以创新、应用和用户体验为核心的全新时代。企业之间的竞争将不再局限于模型的规模和能力，而是转向如何将 AI 技术转化为实际产品，解决现实问题，创造商业价值。至于未来谁能厮杀出圈，就让我们拭目以待。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24b7ddc110c8b78027dc054cb9740fa0.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RpUU6cNNZtHaMecxqBmr</id>
            <title>又翻车！微软一次更新引爆大规模连锁反应，Bing、Copilot等多个软件集体宕机五小时！</title>
            <link>https://www.infoq.cn/article/RpUU6cNNZtHaMecxqBmr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RpUU6cNNZtHaMecxqBmr</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 08:36:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, 服务中断, Bing, Copilot
<br>
<br>
总结: 微软突然的大规模中断影响了Bing.com、Copilot等服务，主要影响了亚洲和欧洲用户，部分版本仍处于离线状态。中断导致Bing搜索页面出错，DuckDuckGo等搜索引擎也受影响。微软确认是一次软件更新出错所致，经过数小时后服务逐渐恢复上线。 </div>
                        <hr>
                    
                    <p>昨晚，微软突然的大规模中断影响了 Bing.com、网页和移动版 Copilot、Windows 版 Copilot、ChatGPT 互联网搜索和 DuckDuckGo等。</p><p>&nbsp;</p><p>微软的服务中断大约在美国东部时间凌晨 3 点开始，似乎主要影响了亚洲和欧洲的用户。目前，部分版本在欧洲仍处于离线状态。</p><p>&nbsp;</p><p><a href="https://x.com/mayank_jee/status/1793560213498581389">根据</a>"网友们的反馈，当打开 Bing.com时，会看到空白页或带有 429 HTTP 代码错误的页面，但是如果直接访问，Bing 搜索仍然有效。出于某种原因，这次中断只对主页产生了影响。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9a/9a0182923dd1b381afb4fde5e2218fc8.png" /></p><p></p><p>&nbsp;要使用 Bing，可以使用这个链接（<a href="https://www4.bing.com/">https://www4.bing.com/</a>"），但是网站、应用程序和 Windows 中的 Copilot都是完全离线的。</p><p>&nbsp;</p><p>据悉，这次中断影响了 Bing 的 API，因此依赖该 API 的服务也被波及，包括 ChatGPT 互联网搜索、DuckDuckGo 和<a href="https://x.com/ecosia/status/1793549809141625247">Ecosia</a>"，DuckDuckGo 和<a href="https://x.com/ecosia/status/1793549809141625247">Ecosia</a>"两个替代搜索引擎都依赖 Bing 的搜索结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdf2bf53c03a83295229bcd2a9609af9.png" /></p><p></p><p>DuckDuckGo 拒绝加载搜索页面，显示：“搜索结果出错。请重试。”</p><p>&nbsp;</p><p>有网友指出，在这次中断的大约一个小时之前，DuckDuckGo就出现了加载结果很慢、需要多次刷新或重新输入查询的情况。而最终显示的加载结果则是当前查询的各种答案的混合，以及来自其他最近查询的关键字。假设输入"LG 显示器型号"等，按回车键后，每个结果都是"导电性错误铝制 LG 显示器更换部件"或“可以使用洗洁精清洁铝制 LG 显示器吗？月光骑士第 2 季传闻……”依此类推，对于每一个结果都是如此。</p><p>&nbsp;</p><p>事件似乎在太平洋时间上午 7 点 20 分左右结束，经过五个多小时的中断后，服务开始恢复上线。</p><p></p><h2>一次更新引起大范围的宕机</h2><p></p><p>&nbsp;</p><p>虽然微软并没有对引发中断的原因做出解释，但微软确认， UTC 时间5月23日 08:46出现问题，并指出“用户可能无法访问 Microsoft Copilot 服务。”</p><p>&nbsp;</p><p>互联网平台经常出现故障，这已是司空见惯。然而，当像微软这样的大型供应商的技术受到影响时，后果可能极其严重，并波及广泛。尽管此次故障似乎并未影响所有平台的使用，但大量用户的投诉表明，其影响范围可能相当广泛。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cc997ffe9ff0123a8c2714fe34735d0.jpeg" /></p><p></p><p></p><blockquote>GitHub Copilot 坏了吗？我真的得自己输入代码了？呃。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/44684a8af9c8314f64201d1405e728e5.jpeg" /></p><p></p><p></p><blockquote>我于美国东部时间凌晨 2:30 起床，发现使用ChatGPT-4的Copilot不工作了。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/777f458ced20fcff497f3d8b6f86c887.jpeg" /></p><p></p><p></p><blockquote>Copilot停止工作，这意味着我今天的工作效率将受到严重影响。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35c36a6210cf2686169666f9e0602b3d.jpeg" /></p><p></p><p></p><blockquote>印度的Bing不工作了。</blockquote><p></p><p>&nbsp;</p><p>此前，微软在其<a href="https://twitter.com/MSFT365Status/status/1793564067191259271">@MSFT365Status X 页面</a>"上表示：“我们正在调查用户可能无法访问 Microsoft Copilot 服务的问题。我们正在努力找出问题的原因。更多信息可以在管理中心的 CP795190 下找到。”OpenAI 也<a href="https://status.openai.com/">证实了这个问题</a>"，并表示正在调查。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/064b004cfb12a49c8a7c32acc8b35d2b.jpeg" /></p><p></p><p>&nbsp;</p><p>微软将这次事故原因归于“一次软件更新出错”，他们随后将请求转移到备用服务组件，想尽快让服务恢复。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9cf7481a36cde17494041c5ab394c575.jpeg" /></p><p></p><p>数小时后，Bing 和 Copilot 恢复上线，但 Android、iOS 和 Windows 应用程序仍然无法加载。</p><p>&nbsp;</p><p>另一方面，由于Bing以及 API 正成为网络底层基础设施的重要组成部分，因此这次故障中调用Bing搜索结果的DuckDuckGo也受到影响。</p><p>&nbsp;</p><p>但问题貌似比API调用更为复杂一点，有网友表示，微软在几个小时前就设法修复了 Bing 前端，但依赖于Bing搜索结果的DuckDuckGo仍然没法正常工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e1856d49fdde0dac9dd46364c706acc9.jpeg" /></p><p>&nbsp;</p><p>这次事故，除微软系的搜索引擎几乎全部没法提供服务，有网友指出他并不想因此转投Google搜索。&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef1f3e406e55e76111f67727723e918a.jpeg" /></p><p></p><p>尽管微软投入大量资金，并在该服务中加入了人工智能，但 Bing 的市场份额其实仍然不算高。今天问题的主要影响可能是现有用户纷纷转向其他平台，而这是微软无法承受的。</p><p>&nbsp;</p><p></p><h2>搜索看起来完全被垄断了</h2><p></p><p>&nbsp;</p><p>这次故障确实凸显了服务的相互依赖性。“有趣的是，DuckDuckGo坚持声称他们使用的搜索结果‘远不止 Bing’，然而 Bing 一旦瘫痪，他们的搜索功能就会完全瘫痪，无法显示任何结果。”网友评价道。</p><p>&nbsp;</p><p>虽然到东部时间上午上班时，Bing服务已经基本恢复，但人们在庆幸之余又感到一丝担忧。</p><p>&nbsp;</p><p>长期占据市场主导地位的搜索平台Google上周刚刚宣布并首次发布AI概览（AI Overviews）功能，将其作为整体搜索服务的默认添加项。如果大家不想要AI响应但仍想使用Google，则可以在菜单中寻找新的“Web”选项，或者按照相关说明在搜索中添加“&amp;udm=14”，通过这条神奇的秘技继续留在前生成式AI时代。</p><p>&nbsp;</p><p>相信很多朋友都曾对AI技术的幻觉、电力消费或者怪异的披萨配方感到失望，包括担心谷歌一方的更多问题，例如隐私、跟踪、新闻、搜索引擎优化乃至垄断权力等。但看似靠谱的大多数替代方案，都因今天早上的这次API中断而现出了原形。要想避免这种“牵一API而动全身”的尴尬局面，整个行业和关注此事的个人都需要好好费一番工夫。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f2/f271708e567828ad29fbacff4ed06cdf.png" /></p><p></p><p>2023年4月至2024年4月，StatCounter统计的搜索引擎市场份额</p><p>&nbsp;</p><p>绝大多数以Google“替代方案”自居的搜索工具，其实都依赖于Google、Bing或者Yandex，也就是说这三大搜索引擎正维持着太大的全球索引体系。</p><p>&nbsp;</p><p>Yandex总部位于俄罗斯，其服务范围目前还没有覆盖到世界上的每个角落。Bing广泛为合作方提供服务，特别是DuckDuckGo，但其基于广告的收入模式和隐私方面的记录曾经引起过一些争议。因此在找到阻止微软跟踪脚本的办法之前，DuckDuckGo公司CEO兼创始人Gabriel Weinberg在Reddit的回复中，解释了为什么像他们这样的厂商无法走通完整的DIY路线：</p><p>&nbsp;</p><p></p><blockquote>……我们的大部分传统链接和图像都是从Bing那边下载获取的……实际上只有两家公司（谷歌和微软）拥有高质量的全球网络链接索引（因为我相信他们在这方面的年度开销超过了10亿美元），所以任何其他搜索引擎都需要借助其中一方或者两方才能提供主流搜索产品。&nbsp;顺带一提，地图业务也是如此——只有体量最大的巨头才能负担得起部署卫星、并派出地面车辆为各个社区拍摄街景照片的成本。</blockquote><p></p><p>&nbsp;</p><p>Bing确实在给微软赚钱，虽然还没有完全盈利。尽管目前的业务重点几乎完全集中在自家AI聊天机器人版Bing之上，保持搜索索引储备及API开放仍旧符合微软的利益定位。然而，如果微软决定取消API访问，或者这项服务的可靠性持续下滑，那么谷歌的市场地位将进一步提升。</p><p>&nbsp;</p><p>面对这样的现实，想要搞点动静出来的搅局者们究竟该何去何从？</p><p>&nbsp;</p><p>当前，许多非GBY搜索引擎都依赖于Common Crawl，或者至少是此为起点。Common Crawl属于公共资源，包含过去17年间超过2500亿个网页，且每月新增网页数量高达数十亿。从某种意义上讲，我们需要的全部信息都在其中，这也让Common Crawl成为众多大语言模型训练中的基本组成部分。</p><p>&nbsp;</p><p>换句话说，接下来的重点在于弄清如何以用户喜欢的方式组织、排序并显示这些结果，同时找到一条可行的盈利路径。</p><p>&nbsp;</p><p>DuckDuckGo和Kagi的创始人已经亲自证明，如果想让业务超越Crawl之外，就必须投入大量资金和基础设施。谷歌有广告可供销售，还有浏览器、Android手机及一众受益于其引擎的捆绑服务。微软这边同样有广告业务，外加一款浏览器和捆绑有Bing引擎的主导级操作系统。如果想要彻底脱离这两者（或者说也算上Yandex），其他厂商该如何交付搜索结果？这是个问题。</p><p>&nbsp;</p><p>现在大模型成为主要的探索方向，至少AI 能以搜索助手的形态帮助大家摆脱典型的GBY产品。</p><p>&nbsp;</p><p>“我已经无法忍受网络搜索那粗糙的结果，甚至宁愿直接上Reddit查找爱好者们的意见。”这应该是很多人的心声。当然，Reddit的帖子也在被纳入ChatGPT、谷歌乃至其他AI解决方案的关注范围。</p><p>&nbsp;</p><p>不过，尽管基于GBY来源的其他引擎往往会显示不同的结果，甚至偶有亮眼表现，但全球搜索被两到三个信息源绑架的现实已成定局。&nbsp;</p><p></p><h2>小众搜索：只能是“第二辆车”</h2><p></p><p>&nbsp;</p><p>在被大厂“垄断”的主流搜索之外，也有很多小众搜索。</p><p>&nbsp;</p><p>个人开发者<a href="https://seirdy.one/posts/">Rohan Kumar</a>"一直在用自己的索引更新一份带有大量注释的搜索引擎列表，借此我们也可以了解到更多关于“GBY”（即Google、Bing和Yandex）三巨头之外的搜索世界及其运转逻辑。</p><p>&nbsp;</p><p>Kumar三年来统计出了一份榜单，该英文榜单洋洋洒洒数千言，但列举的引擎中只有少部分拥有自己的通用索引。</p><p>&nbsp;</p><p>Kumar最喜爱的两个分别是在市场竞争中保持住生命力的Mojeek，以及为各类主要引擎提供良好补充的Stract。Right Dao的搜索结果“速度很快且质量很高”，部分原因在于其爬虫选择了维基百科作为起点。Yep的覆盖范围更广，会显示与查询相关的网站链接与返回结果，并承诺与创作者们分享广告收入。</p><p>&nbsp;</p><p>这些搜索都在某种程度上表现出了实力，但仍给人一种强烈的“家庭第二辆车”的感觉——不足以成为“出行首选”。</p><p>&nbsp;</p><p>Kumar 还分享了其他更为小众的引擎。对于大家偶尔听说过的这类搜索引擎，其很可能被列入了“半独立索引”的部分。这是因为当其自身的搜索结果不够强大时，它们都会借用GBY索引。比如推崇加密货币、备受争议的Brave引擎，以及不少直接“转载”GBY结果或者将其相关链接塞进自身显示内容的引擎。</p><p>&nbsp;</p><p>Kagi 还要求使用者注册账户，并将其索引Teclis与Google、Bing、Yandex、Mojeek乃至其他索引（包括Brave）配合使用。Kagi的创始人对于基于AI的搜索未来有着鲜明的观点，认为这将以“不可扩展”的方式危害整个搜索世界。</p><p>&nbsp;</p><p>也许各位有不同观点，但需要注意的是，如果GBY三巨头的服务出现问题，Kagi这样的搜索工具也同样会受到波及。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/MSFT365Status">https://x.com/MSFT365Status</a>"</p><p><a href="https://arstechnica.com/gadgets/2024/05/bing-outage-shows-just-how-little-competition-google-search-really-has/">https://arstechnica.com/gadgets/2024/05/bing-outage-shows-just-how-little-competition-google-search-really-has/</a>"</p><p><a href="https://www.bleepingcomputer.com/news/microsoft/microsoft-outage-affects-bing-copilot-duckduckgo-and-chatgpt-internet-search/">https://www.bleepingcomputer.com/news/microsoft/microsoft-outage-affects-bing-copilot-duckduckgo-and-chatgpt-internet-search/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dE6w34Ke2az1QqFtGmPz</id>
            <title>快来加入 NVIDIA 初创加速计划，获得产品折扣、技术指导、投融资对接等全方位助力！</title>
            <link>https://www.infoq.cn/article/dE6w34Ke2az1QqFtGmPz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dE6w34Ke2az1QqFtGmPz</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 07:46:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 科技创业, NVIDIA 初创加速计划, 技术支持, 全球生态项目
<br>
<br>
总结: 科技创业是充满挑战与未知的领域，NVIDIA 初创加速计划为技术创业公司提供全球生态项目支持和加速发展服务。 </div>
                        <hr>
                    
                    <p>科技创业无疑是当下最热门的领域之一，但对于投身其中的创业者而言，这条路或许没有想象中好走。从产品的孵化、打磨到落地，从团队的组建到市场的开拓，每一步都充满了挑战与未知。资金的压力、激烈的市场竞争、人才的渴求、技术的风险、管理的难题以及法规政策的变化，这些都是科技创业公司必须正视的困难。</p><p></p><p>作为一家创业公司，如果你想找到解决上述问题的最快路径，欢迎加入由 NVIDIA 发起的加速创业公司发展的全球生态项目——NVIDIA 初创加速计划。该项目联合了国内外知名的投资机构，创业孵化器，创业加速器，行业合作伙伴以及科技创业媒体等，打造创业加速生态系统，能够提供产品折扣，技术支持，市场宣传，融资对接，业务推荐等一系列服务，加速创业公司的发展。</p><p></p><p>目前该计划接受所有技术创业公司（AI 和深度学习、数据科学、高性能运算、网络、图形、AR/VR 和游戏等）的申请。与传统加速器不同，NVIDIA 初创加速计划将为创业企业生命周期的各个阶段提供支持。NVIDIA也会与会员密切合作，并为其提供更好的技术工具、最新的资源以及与投资者交流的机会。</p><p>无论您当前所处的融资阶段为何，NVIDIA 初创加速计划都欢迎您的加入！</p><p></p><p>NVIDIA 初创加速计划 (NVIDIA Inception)&nbsp;是 NVIDIA 为初创企业所提供的一个加速平台，目前全球已有超过 15,000&nbsp;名成员。作为全球最大的初创企业生态系统之一，NVIDIA 初创加速计划成员遍布 100 余个国家。今年，NVIDIA 初创加速计划中国会员企业数量正式突破 2,000 家！伴随这一里程碑的到来，2023 NVIDIA 初创企业展示的序幕也将拉开。</p><p></p><p>NVIDIA 初创加速计划于 2016 年落地中国，数以千计的初创企业受益于 NVIDIA 强大的技术实力和开放的合作生态。NVIDIA 为会员企业提供了一系列全面而有针对性的资源，从技术指导到软硬件支持，从市场宣传到投融资及大企业对接，在各个方面助力会员企业发展。8 年来，已经有数十家会员公司上市，上百家成为独角兽企业。在 NVIDIA 领先技术的助力下，会员能够更快地推进产品开发、优化算法、提高性能，在市场上取得竞争优势，同时他们也正在通过诸如 GTC 这样的国际化平台走向世界，拥抱更多未知的机会和可能性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/5845e7790d424328ea9d405c2a8fa10b.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GKslHaQfg1pbw32IpOp4</id>
            <title>“数字中国建设峰会”首次结合 AI 大模型，打造“智慧观展”新体验</title>
            <link>https://www.infoq.cn/article/GKslHaQfg1pbw32IpOp4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GKslHaQfg1pbw32IpOp4</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 05:02:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数峰会, AI技术, 支付宝, 智能助理
<br>
<br>
总结: 本文介绍了第七届数字中国建设峰会首次应用AI技术，通过支付宝智能助理为参展公众提供多种服务和互动，包括了展前、展中、展后的全流程服务，同时推出了官方一站式会务服务平台——“数字峰会”支付宝小程序，为参会人员提供智能服务和数字化便捷服务，展示了数字科技服务于民的创新探索。 </div>
                        <hr>
                    
                    <p>“数峰会如何报名？会展中心哪里有充电宝？帮我做个福州一日游攻略”5 月 24 日，第七届数字中国建设峰会正式开展，峰会通过引入人工智能、数字人、官方小程序、AR等技术，极大提升展会的智能化体验，引领科技办会新风潮。据悉，这是数字中国建设峰会首次应用AI大模型技术创新。</p><p>&nbsp;</p><p>参加本届峰会的公众，下拉支付宝首页唤起AI智能助理，就能通过自然对话，了解本届峰会的议程设置、参展须知、活动亮点等信息，也能实时查询附近的充电宝、厕所、餐馆等服务信息，让 AI 为参展公众提供多种服务和互动。此外，通过AI技术，公众可以一键连接官方一站式会务服务平台——“数字峰会”支付宝小程序，还能快速唤起展会服务数字人，通过可看可听的播报形式解答常见问题，带来峰会最新资讯动态。</p><p><img src="https://static001.infoq.cn/resource/image/bb/ed/bb66yy843ff4006a9a157dfd0b3039ed.png" /></p><p></p><p></p><h2>跟随支付宝AI 智能助理，轻松逛 44 万平米展厅</h2><p></p><p></p><p>本届数字中国建设峰会，深度结合支付宝智能助理打造“智慧观展”新体验，在“展前、展中、展后”全流程中，为公众提供峰会的AI咨询应答及全流程服务。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f3/53/f34aee67631e008ec8f0e9f18cd06c53.png" /></p><p>图：支付宝智能助理提供智能服务和问答</p><p>&nbsp;</p><p>（1）展前——观展注册与路线计划服务等</p><p>基于公众参展前的需求，智能助理可提供本届峰会举办信息和背景介绍、看展的注册入口、天气查询、路线换乘方案查询、呼叫网约车等，便于公众出发前进行出游规划的制定。</p><p></p><p>（2）展中——峰会新闻播报与生活类服务提供等</p><p>当公众来到海峡会展中心或烟台山AI街区，可询问智能助理有哪些展区具有特色亮点、如何用餐、哪里借用充电宝、现场游玩攻略，还能快速观看3D建模的数字展馆等。</p><p></p><p>（3）展后——城市本地游服务等</p><p>观展后，公众可通过智能助理询问附近美食店铺、领取店铺优惠、查询福州当地景点及特色打卡地、制定福州游攻略、查询并预定返程火车票或机票等。</p><p>&nbsp;</p><p>“通过一场展，链接一座城”，本届峰会首次结合AI大模型能力，通过支付宝智能助理及背后链接的商家服务生态，更深层地提升了公众的看展体验，并进一步解决了公众在展会之外的城市游玩等周边需求，是展会利用创新科技打造智能展会的典型例证。</p><p>&nbsp;</p><p></p><h2>上线“数字峰会”支付宝小程序做峰会“百科全书”</h2><p></p><p></p><p>本届数字中国建设峰会也在近日首度推出官方一站式会务服务平台——“数字峰会”支付宝小程序。该小程序由福州市大数据发展委员会、福州市会展会务集团与支付宝共同打造，用AI+数字化的形式，助力参会人员高效参展逛展，定制化的“&nbsp;AI 峰会助理”和“展会服务数字人”，在线上提供“百科全书”般服务能力，在支付宝搜“数字峰会”即可体验。</p><p>&nbsp;</p><p>此外，峰会官方支付宝小程序还提供国际支付、出行等数字化便捷服务，为侨胞提供贴心的参会服务。峰会小程序由四大服务板块组成：</p><p>（一）“展览服务”：数字展馆、团体报名、出行码、支付码、AR活动；</p><p>（二）会务服务：参观注册、峰会助理、数字主播；</p><p>（三）场馆导览；</p><p>（四）新闻中心。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/55/bc/5578c248611abc82798510f559c761bc.png" /></p><p>图：“数字峰会”支付宝小程序提供的智能服务和数字人信息播报</p><p></p><p>“场馆导览”中的数字展馆通过 3D 建模技术复刻线下展馆实际布局，共有峰会 11 个类型板块及其数百个展商可从“数字峰会”支付宝小程序上点击打开，线上就可了解展商的展厅样式、公司背景资料以及联系方式等，促进各商家快速交流合作。此外，为满足老年人群便利使用，还支持一键开启“长者模式”，更大的字号和页面布局优化，让老年朋友使用体验更好。峰会还采用蚂蚁灵境数字人平台，以福建广播电视台主持人为原型，复刻了AI展会服务数字人，用于现场接待引导、常见问题咨询答疑等。</p><p>&nbsp;</p><p>得益于中国“数字基建”高水平发展，出现了如支付宝这样涵盖了“食住行游购娱”等多方面服务的数字生活服务平台，而繁荣的数字生态，是中国打造“全民AI”的必备土壤。“一场展会+一个AI技术+一个APP”，即可畅行展会、畅行全城，为未来更多展会提供了更好的“智慧办展、智慧观展”思路借鉴，这是本届数字中国建设峰会突破性的创新探索，也是中国数字科技服务于民的又一案例。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1LCxaEy6CHZQekWoM6E6</id>
            <title>第一波收割完的AI创企要跑路了？6年来仅做了一款产品，问世30天就彻底失败，Ai Pin公司10亿美元求“卖身”</title>
            <link>https://www.infoq.cn/article/1LCxaEy6CHZQekWoM6E6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1LCxaEy6CHZQekWoM6E6</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 05:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Humane, 初创公司, 可穿戴设备, 融资
<br>
<br>
总结: 美国加州旧金山的初创公司Humane考虑接受收购，成立5年未公开产品但融资数额巨大，联合创始人来自苹果公司，公司目标是开发AI定制软件平台和消费设备，最新产品Ai Pin具有投影显示和人工智能功能，但被批有严重缺陷，导致公司开高价求收购。 </div>
                        <hr>
                    
                    <p>当地时间5月22日，据外媒报道，总部位于美国加州旧金山的可穿戴设备初创公司Humane正考虑接受收购。彭博社援引消息人士称，该公司的定价在 7.5 亿至 10 亿美元之间，出售过程正处于早期阶段。</p><p></p><h2>成立5年未公开过任何产品，拿着PPT融了10多亿元？</h2><p></p><p>&nbsp;</p><p>据公开信息显示，Humane一家是由苹果前设计和工程团队成员 Imran Chaudhri 和 Bethany Bongiorno 于2018年创立的AI硬件初创公司，公司管理层层包括来自苹果公司的关键人物，Bongiorno 担任CEO，Chaudhri 担任董事长兼总裁。这家初创公司的CTO Patrick Gates 也来自苹果。</p><p>&nbsp;</p><p>创办Humane之前，Chaudhri 曾在苹果公司担任设计师长达20 年，据报道于 2017 年被苹果公司解雇，Bongiorno在苹果公司工作了8年，担任 iOS 和 macOS 的软件工程总监，并于 2016 年离职。两人很可能都知道 Vision Pro 的长期开发过程。</p><p>&nbsp;</p><p>此外，据报道，在这家初创公司成立的5年间，约有 90 名前苹果员工在这家 200 人的团队中工作或曾为其效力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a34f25101dce550ee382cb968296953.png" /></p><p></p><p>Humane 联合创始人 Bethany Bongiorno 和 Imran Chaudhri</p><p>&nbsp;</p><p>从成立之初，Humane就一直笼罩在神秘之中。公司成立的前5年时间里，关于Humane公司产品的相关报道寥寥无几，其向外界释放的为数不多的产品信号就是公司的目标是开发专为人工智能（AI）量身定制的突破性软件平台和消费设备。</p><p>&nbsp;</p><p>但出人意料的是，即使没有任何一款产品问世品，Humane仍然在过去几年完成了多轮大额融资。</p><p>&nbsp;</p><p>2019年6月，有消息透露Humane完成了一轮种子融资，但融资的具体金额并未公开。</p><p>&nbsp;</p><p>2021年9月1日，Humane 宣布其 B 轮融资已筹集 1 亿美元。此轮融资由 Tiger Global Management 领投，软银集团、BOND、Forerunner Ventures、Qualcomm Ventures LLC 等参投。此轮融资将使 Humane 能够扩大业务规模，并继续执行和拓展其使命，以实现人类与计算之间的下一次转变。</p><p>&nbsp;</p><p>当时，Humane 联合创始人 Imran Chaudhri 和 Bethany Bongiorno 表示：“Humane 是一个人们可以通过真正的设计和工程合作实现创新的地方。我们是一家体验式公司，致力于创造造福人类的产品，打造以人为本的技术——一种超越我们今天所知的更个性化的技术。我们都在等待新事物，一种超越我们一直生活的信息时代的东西。在 Humane，我们正在为所谓的智能时代打造设备和平台。我们致力于打造一种不同类型的公司，以信任、真理和快乐为价值观。在合作伙伴的支持下，我们将继续扩大团队规模，吸纳不仅对彻底改变我们与计算交互的方式充满热情，而且对构建方式也充满热情的个人。”</p><p>&nbsp;</p><p>Tiger Global 合伙人 Chase Coleman 表示：“Humane 的员工素质令人难以置信。这些人为全球数十亿人打造并交付了变革性产品。他们打造的产品具有开创性，有可能成为未来计算的标准。”</p><p>&nbsp;</p><p>在2023年完成的最新一轮融资（C 轮融资）中，Huamne又筹集到了1亿美元，使总融资额达到 2.3 亿美元，投资者包括 Kindred Ventures（领投）、SK Networks、LG Technology Ventures、微软、沃尔沃汽车科技基金、Tiger Global、高通风险投资公司和 OpenAI 首席执行官兼联合创始人 Sam Altman等。</p><p>&nbsp;</p><p>Humane 与微软的合作主要是体现在其利用微软的云基础设施搭建技术平台，同时，Humane 也将OpenAI 的技术集成到其设备中。与 LG 和沃尔沃的合作也暗示了其在家庭技术和汽车产品方面的潜在应用。</p><p></p><h2>首款产品被批有严重缺陷，Huamne开高价求被收购</h2><p></p><p>&nbsp;</p><p>而就在一个月前，该公司刚刚推出了经过大肆宣传、售价699美元的AI Pin产品。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/84/84a4cc2630c7591f921dd238dac13e19.png" /></p><p></p><p>图片来源：Humane 官网</p><p>&nbsp;</p><p>事实上，Humane&nbsp;于去年6月就亮相了 Ai Pin，这是一款具有投影显示和人工智能功能的可穿戴设备。该消息在美国开启了一段预购期，但 Ai Pin的最终上市时间被推迟到了今年的4月中旬。</p><p>&nbsp;</p><p>这款可穿戴设备配有运动摄像头与传感器，可帮助识别周遭环境，它由高通芯片提供支持并利用人工智能。它有点像Narrative Clip，一款命运多舛的生活记录相机。这款方形设备Ai Pin配有摄像头和麦克风，以及深度和运动传感器，用于收集数据，并由 Snapdragon 处理器进行处理。语音控制是该产品的核心，这似乎是 Siri 等智能手机助手的合乎逻辑的下一步。该设备通过“声波扬声器”或配对的蓝牙耳机与佩戴者进行通信。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24fd6906916077d828d7e61c512ed7ee.gif" /></p><p></p><p>颇具争议的点是，这款产品没有屏幕——实际上，但有一个触摸板。这款产品还能对手势做出反应。Humane公司称，与无数其他承诺让我们摆脱屏幕依赖的产品不同，Ai Pin 的设计目的是为了不依赖智能手机。这需要通过基于 T-Mobile 的 Humane 品牌无线网络来实现。</p><p>&nbsp;</p><p>据介绍，Ai Pin 产品另一个吸引人的视觉元素是激光墨水显示屏，它可以将来电等文本投射到用户的手掌上，代替触摸屏。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d919fd6692ebf75ba83bd94630e6181.png" /></p><p>&nbsp;</p><p>该设备附带一个“电池增压器”，因此用户可以热插拔电源。Pin“采用独特的两件式设计，由主计算机和电池增压器组成。它们通过磁性连接并通过衣服和服饰无线供电，使您可以以多种方式佩戴 Ai Pin。凭借其永久电源系统，用户可以在旅途中热插拔电池增压器，确保不间断使用和全天电池寿命。”</p><p>&nbsp;</p><p>但在发布之后，AI Pin被广泛批评为存在缺陷且未能兑现承诺。</p><p>&nbsp;</p><p>由于标价699 美元，外加每月 24 美元的定期订阅服务（该服务会为用户提供一个电话号码和无限流量，以便用户进行尽可能多的查询），Ai Pin似乎在资金紧张的消费市场很难销售。从某些方面来看，这款设备似乎是一个在寻找问题的解决方案，许多最初的评论都表示，Ai Pin的功能实际上并不比智能手机的功能多多少。</p><p>&nbsp;</p><p>如今，Huamne正希望以10亿美元价码寻求收购。彭博社则在关于收购新闻的报道中表示，Humane已经为此事聘请了一名财务顾问，但能否收购成功尚未确定。</p><p>&nbsp;</p><p>Humane方面的代表没有立即回应置评请求。</p><p>&nbsp;</p><p>据报道，结合Humane推出AI Pin一个月后即寻求收购的情况，对于这家希望把握住全球AI技术热潮的设备制造商来说，恐怕是已经感常见到业务或将迎来低谷。</p><p>&nbsp;</p><p>自从初创公司OpenAI向公众推出ChatGPT AI聊天机器人以来，AI技术的应用在过去两年间呈现出爆发式的增长态势。短短两个月内，全球过亿用户涌入这项技术，也掀起各大企业巨头的踊跃投资与战略转变，包括苹果、亚马逊、微软，以及谷歌母公司Alphabet和Facebook母公司Meta等。</p><p>&nbsp;</p><p>但与大多数着眼软件的厂商不同，Humane希望通过一款方形、配有光滑边缘设计的产品来推销其AI成果，让消费者把设备佩戴在衬衫或包包上并保持摄像头和传感器朝外。Humane表示，这款设备代表继智能手机之后的下一形态，人们可以通过AI技术与应用软件和服务进行交互，例如联系网约车或听音乐。</p><p>&nbsp;</p><p>尽管Humane的承诺野心勃勃、产品演示也令人印象深刻，但这款产品还是引发了一波又一波的负面评论。来自技术媒体CNET的Scott Stein更是直言不讳地表示，这款产品“有着疯狂的概念，但在日常使用体验上则令人沮丧。”Humane公司表示正在努力改进产品设计。</p><p>&nbsp;</p><p>与此同时，三星、谷歌和微软等公司已经稳步将AI技术添加至自家设备当中，包括通过专门的软件以及在计算机键盘上添加新的AI功能按钮。预计苹果公司也将宣布对其iPhone、iPad和Mac设备支持的软件进行重大AI变革。</p><p>&nbsp;</p><p>尽管其首款产品存在严重的软件缺陷和硬件问题，但这家初创公司显然认为其价值在 7.5 亿至 10 亿美元之间。</p><p></p><h2>Humane这是割完韭菜要跑路了？</h2><p></p><p>&nbsp;</p><p>不难发现，随着AIGC技术的高歌猛进，人工智能（AI）与穿戴设备的结合日益紧密，这一新兴市场吸引了众多厂商和投资者的目光。然而，在这波AI穿戴设备的热潮中，一些厂商似乎过于追求短期的市场利益，而忽视了产品研发和用户体验的长期价值，这种现象很难不被认为是在“割韭菜”。</p><p>&nbsp;</p><p>从苹果Vision到Meta Quest，都曾被寄予厚望，认为是通往元宇宙的第一步。但刚发布产品这款产品却因粗糙、收费高被广泛诟病，可以说与元宇宙基本不沾边。此外，产品发布一个月就寻求出售的做法多少有些让人难以捉摸，因此被质疑割韭菜也就不足为奇了。</p><p>&nbsp;</p><p>不只AI穿戴设备领域如此，放眼全球市场，不乏一些A硬件厂商在资本的助推下迅速崛起，但又在短时间内因融资烧光而倒闭，Airware就是一个典型的例子。</p><p>&nbsp;</p><p>这家公司曾试图生产自己的商用无人机硬件，并开发了一套云端软件系统，帮助建筑、钻探和保险行业的企业客户使用无人机的航拍图像来评估破坏情况。然而，随着市场竞争的加剧和商用无人机硬件公司附带软件的逐渐成熟，Airware逐渐被边缘化，最终因融资烧光而倒闭。</p><p>&nbsp;</p><p>AI硬件厂商“割韭菜”现象的出现，一方面是由于市场竞争的激烈和资本的逐利性。在资本的推动下，一些厂商为了迅速占领市场份额，不惜采取低价策略、夸大宣传等手段来吸引消费者。然而，这种做法往往忽视了产品研发和用户体验的长期价值，导致产品质量参差不齐、售后服务不到位等问题。另一方面，一些厂商缺乏对市场的深入了解和精准定位，盲目跟风、一哄而上，导致产品同质化严重、市场竞争无序。</p><p>&nbsp;</p><p>技术越热，市场和资本越需要冷静，消费者更需要冷静，否则一不小心，谁成了韭菜真说不定。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.cnet.com/tech/mobile/humane-maker-of-wearable-ai-pin-is-exploring-a-sale-report-says/">https://www.cnet.com/tech/mobile/humane-maker-of-wearable-ai-pin-is-exploring-a-sale-report-says/</a>"</p><p><a href="https://humane.com/media/humane-completes-series-b-funding-round">https://humane.com/media/humane-completes-series-b-funding-round</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IOOKB3tw2u3EQkyEAT9K</id>
            <title>10+AI系列专题，拆解从0到1构建大模型架构平台的实现路径｜ArchSummit</title>
            <link>https://www.infoq.cn/article/IOOKB3tw2u3EQkyEAT9K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IOOKB3tw2u3EQkyEAT9K</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 May 2024 01:46:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI大模型, ArchSummit深圳站, 专题, 数据驱动
<br>
<br>
总结: AI大模型正在颠覆传统架构设计思路和路径，6月14-15日ArchSummit深圳站举办10大AI系列专题，涵盖AI运维、AI大模型中台、AI安全与风控等热门话题，解析企业如何搭建大模型时代的架构平台。专题内容涉及大模型的集成与部署策略、低代码与AI结合、数据与人工智能相互驱动、AIOps业务场景实践、AI大模型中台实践探索、AI时代的安全与风控、高效算力基建与性能优化、LLM作为新一代‘OS’的探索、大模型基础框架等。 </div>
                        <hr>
                    
                    <p>AI大模型正在颠覆传统架构设计思路和路径，<a href="https://archsummit.infoq.cn/2024/shenzhen/">6月14-15日ArchSummit深圳站</a>"，10大AI系列专题，从底层基础到顶层应用多角度，覆盖AI运维&nbsp;、AI大模型中台、AI安全与风控及大模型算力等热门话题，解析企业如何从0到1搭建大模型时代的架构平台。</p><p><img src="https://static001.geekbang.org/infoq/09/091d7d757337a339278c136ee290423e.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1636">《基于大模型应用层的探索》专题</a>"将关注大模型的集成与部署策略，深入探讨如何从应用层面充分发挥大模型的优势，挖掘其潜在的巨大价值。从选择适合的大模型，到对这些模型进行精细化的性能调优等内容，我们将带领大家一步步理解大模型的运作机制和应用技巧。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3992517f3ba2d92ce941582f25dca58b.png" /></p><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1643">《低代码与AI结合》专题</a>"将深入研究低代码平台如何与人工智能技术相结合，提高开发效率。探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/614646b100f95d04aab6d8ad56ce683c.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1640">《Data&nbsp;4&nbsp;AI和AI&nbsp;4&nbsp;Data方面的探索和实践案例》专题</a>"，将深入探讨数据与人工智能相互驱动的关系。分享在构建数据驱动AI系统时的最佳实践，包括数据质量管理、特征工程、数据增强等。同时涉及数据结构和数据治理、数据保护、数据管理优化等具体实践，以及超融合数据架构，在AI应用上所做的设计及优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c41333f2188350fd525493128d9fa5f.png" /></p><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1641">《AIOps&nbsp;业务场景最佳实践》专题</a>"将聚焦AIOps在不同业务场景中的实际成效，比如如何通过AIOps推动可量化的业务价值增长和效率提升？邀请来自可观测性、监控运维技术等领域的资深专家分享他们的AIOps实战经验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e98ba75a76ef1addd35511431aa29d5.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1653">《AI大模型中台实践探索》专题</a>"将聚焦机器学习和深度学习模型的管理、部署、运维和监控的话题，帮助构建大模型中台的团队了解如何提升机器学习和深度学习、软件工程、大数据处理、模型部署和服务化、数据治理和隐私保护、业务理解等能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d9ccc968cb104af7ee2fcb9ba09c6876.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1651">《AI时代的安全与风控》专题</a>"将深入研究在人工智能时代如何构建安全可靠的系统。从AI引发的安全挑战到创新的风险管理策略出发，探讨在技术和业务层面如何应对安全和风险问题，包括互联网安全、业务风控等相关技术。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/aff03639baaaf397932fe63b366bfd1a.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1639">《高效算力基建与性能优化》专题</a>"将深入挖掘构建高效算力基础设施的技术策略，包括云原生技术、容器化、微服务、GPU虚拟化、在离线混部等架构模式的实际应用案例，推进计算领域基建的高质量发展和创新性探索。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b97429ca55a8d0e7412b21ea61fe63bb.png" /></p><p></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1649">《LLM作为新一代‘OS’的探索》专题</a>"将深度剖析大型语言模型（LLM）在构建新一代创新应用过程中所发挥的作用（这里所说的OS是代指一种核心技术），介绍在未来系统构建、架构设计等角度中融入LLM的潜在益处，以及在系统层面解决的技术挑战。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38f6016b5bb2707d7a503ed813bf358b.png" /></p><p></p><p><a href="https://archsummit.infoq.cn/2024/shenzhen/track/1646">《大模型基础框架》专题</a>"将深入探讨行业中大模型训练和推理的基础架构和关键技术，包括训练加速、多维并行、万卡集群、高性能算力等技术焦点，关注训练性能优化、推理部署策略等重要领域，助力人工智能领域进一步发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/8631d512bd54e8718cfc517c7995562e.png" /></p><p></p><p>与传统的云计算平台不同，智能计算平台有独有的网络架构（如&nbsp;IB，RoCE&nbsp;等），独特的虚拟化方式（基于虚拟机/容器的GPU&nbsp;虚拟化），独特的算力调度平台，海量的并行存储系统等。<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1637">《智算平台建设与应用实践》专题</a>"将分享构建智算平台的技术要点，以及在实践和落地过程中所作的优化和踩过的坑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d0e33209524e6b1ce89b63ae46eb340.png" /></p><p></p><p>6月14-15日ArchSummit深圳站，不见不散！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/B4v0rg7s7Pj9mDs6pVld</id>
            <title>智能体技术发展趋势：李鹏谈大模型智能体与开放领域融合</title>
            <link>https://www.infoq.cn/article/B4v0rg7s7Pj9mDs6pVld</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/B4v0rg7s7Pj9mDs6pVld</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 16:07:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型智能体, 开放环境, 技术挑战, 智能体应用
<br>
<br>
总结: 大模型智能体作为前沿探索的焦点，正逐步塑造未来技术与社会交互的新形态。随着 AI 技术的飞速跃进，这些智能体被寄予厚望，期望能在复杂多变、充满不确定性的开放环境中自如运作，从个性化推荐到高级人机协作，其应用前景广阔无垠。然而，要实现这一愿景，必须跨越多重技术与理论障碍，包括但不限于如何使智能体在不断变化的环境中持续学习、有效适应各类环境后效性、在多元目标间实现精准平衡，以及如何主动感知并智能响应环境信息等。 </div>
                        <hr>
                    
                    <p>大模型智能体作为前沿探索的焦点，正逐步塑造未来技术与社会交互的新形态。随着 AI 技术的飞速跃进，这些智能体被寄予厚望，期望能在复杂多变、充满不确定性的开放环境中自如运作，从个性化推荐到高级人机协作，其应用前景广阔无垠。然而，要实现这一愿景，必须跨越多重技术与理论障碍，包括但不限于如何使智能体在不断变化的环境中持续学习、有效适应各类环境后效性、在多元目标间实现精准平衡，以及如何主动感知并智能响应环境信息等。</p><p></p><p>在这样的背景下，我们在 AICon 全球人工智能开发与应用大会，荣幸邀请到清华大学 智能产业研究院副教授李鹏为你分享《面向开放域的大模型智能体》，我们有幸采访了李鹏老师。在我们的独家访谈中，他指出当前大模型智能体在处理开放环境不确定性与动态变化时的核心挑战，包括但不限于推断时学习的作用机制、环境后效性的有效建模与利用、跨环境的泛化能力、多目标优化的复杂性，以及主动感知技术的迫切需求。</p><p></p><p>本文为采访实录，经编辑。</p><p></p><p>InfoQ：您如何看待当前大模型在处理不确定性和动态变化环境中的挑战？是否有特定的技术或方法可以提高其泛化能力和可解释性？</p><p></p><p>李鹏：面对当前 AI 发展的挑战，我总结了以下关键点，每个都深刻影响着大模型在开放环境下应用的效能与灵活性：</p><p></p><p>● 推断时学习（Inference-time Learning）的实现：传统模型训练依赖于大量静态数据集，但在动态、不确定的环境中，这种模式显得局限。我们需要模型具备“学后学习”能力，即在完成初始训练后，仍能高效学习新信息，同时避免“灾难性遗忘”。这要求平衡新知识的吸收与旧知识的保留，以及在有限、快速变化的数据中高效学习，这是对现有学习机制的一大挑战。</p><p></p><p>● 有效建模与利用环境后效性（Environmental Aftereffect）：智能体与环境的互动经常引起环境状态的持久变化，如在线购物平台根据用户行为调整推荐内容所展示的个性化效果。挑战在于，设计模型不仅需准确反映这种互动的后果（即后效性），还应能预见并利用这些变化以优化其后续行动。这意味着模型需集成复杂的因果推理和策略规划能力，以预测并积极引导环境变化，为达成目标服务。</p><p></p><p>● 跨环境泛化（Cross-environment Generalization）的实现：由于环境数量无限且存在后效性，无法为每个环境单独训练智能体。因此，可以在一些代表性环境中进行训练，以获取与环境无关或可泛化到广泛环境的基础能力或知识。这样，智能体在新环境中能够快速适应并执行任务，利用先前在其他环境中学到的知识和能力。这种跨环境泛化的方法有助于解决面对无限数量环境的挑战，提高智能体在不同环境中的适应性和表现。</p><p></p><p>● 多目标优化（Multi-objective Optimization），在复杂的开放领域应用场景中，智能体需同时追求多个可能相互矛盾的目标（如高效率、低成本、快速执行），这些目标间的权衡增加了决策的复杂度。传统上，多目标优化就是一个难题，而随着智能系统迈向更加开放和动态的环境，有效平衡及优化这些相冲突的目标变得尤为重要和紧迫。因此，开发能够自动调节和优先处理多目标间关系的算法，确保智能体在实际操作中既能达成高质量性能，又能考虑效率、经济性等其他关键指标，是当前研究与实践中的一个重大挑战。</p><p></p><p>● 主动感知（Active Perception），即智能体应具备根据当前任务需求和执行进度，自发地、有选择性地向环境索取信息的能力，而非依赖外部指令被动接收数据。这要求智能体不仅要能高效处理接收到的信息，还需智能地决定感知什么、何时感知以及如何感知，体现了与传统被动感知模型的根本区别。</p><p></p><p>至于说解决方案，当前大模型与智能体技术正处于快速发展阶段，全面应对上述挑战的成熟解决方案尚未完全形成。各个领域虽已见证了一系列积极探索，比如推断时学习算法的进步、基于环境交互的动态知识整合训练策略，以及我们团队正在进行的相关研究，但至今尚缺乏一套系统性、全方位解决这些复杂问题的技术框架。正是由于此现状，强调智能体与人类目标、环境需求、以及自身约束之间的统一对齐，即“智能体 - 人类 - 环境统一对齐原则”，显得尤为重要。</p><p></p><p>InfoQ：这个在大模型应用中，您认为哪些领域需要这种技术支持？</p><p></p><p>李鹏：几乎所有涉足开放域应用的场景都会不同程度地遭遇这些难题，尤其是那些强调个性化和具身化交互的领域。在这些情境下，无论是为了满足用户独特偏好的个性化需求，还是实现智能体在具体环境中的有效操作与适应，解决数据多样性、动态环境适应性、多目标优化、主动感知及少数样本学习等问题的重要性尤为凸显。因此，诸如个性化推荐、虚拟助理、沉浸式交互、自适应教育、智能健康监护、以及高级的人机协作系统等应用领域，对于支持开放域技术的需求尤为迫切。</p><p></p><p>InfoQ：您如何定义智能体、人类和环境的统一对齐？目前是否有切实的解决方案？</p><p></p><p>李鹏：我们的核心观点在于，智能体研究应当超越单纯追求下游任务成功率的局限，转向一个更为综合的视角，着重考虑智能体、人类用户与环境三者之间相互作用的需求协调。</p><p></p><p>这意味着，在设计和评估智能系统时，不仅要着眼于任务完成度，更要深入理解并满足人在交互中的便捷性期望、个性化偏好，以及适应环境变化的能力等。</p><p></p><p>以理想的购物助手为例，其价值不仅体现在完成购买操作，更在于能够通过简洁的指令理解复杂需求。用户期望无需详尽指定品牌、型号，智能助手便能基于用户历史偏好、上下文暗示，精准推荐所需商品，实现智能体与用户意图的无缝对接，这即是智能体需与人类意图对齐的体现。此外，该智能助手还需具备适应电商网站动态变化的能力，如商品更新、界面调整等，确保在不断演化的环境中依旧能有效执行任务，这即是智能体与环境规律对齐的体现。</p><p></p><p>进一步而言，用户不仅期望智能体能准确理解并迅速执行任务，如高效完成购物而不拖延，还期待整个过程的成本效益最大化。换句话说，用户不希望智能体的运行导致不必要的开销，或是因低效而增加等待时间。这就要求智能体的设计需兼顾效率与经济性，确保其自身运作的智能化，即在满足任务需求的同时，优化资源使用、降低成本，避免不必要的延迟或浪费。这就是智能体与自身限制对齐的体现。</p><p></p><p>至今为止，尚未有智能体能完全达到智能体、人类与环境三者间的理想对齐状态，这反映出该目标的实现颇具挑战且尚未成为广泛研究的重点。正因如此，我们认为当前提出这一议题极具价值和前瞻性。</p><p></p><p>InfoQ：如何通过智能体来指导代价敏感的特征获取过程？这种方法在哪些应用场景中表现最佳？</p><p></p><p>李鹏：在整个过程中，我们的目标并非单纯让智能体提取特征，而是探索其他途径以实现这一目标。尽管对智能体决策成本的研究已初见端倪并积累了一定成果，该领域仍处于发展阶段，存在广阔的探索空间。在即将呈现的演讲中，我们将详述一项创新方法，即当智能体的学习预算（budget）受限时，我们采用了一种融合预规划的手段来优化学习过程，并已观察到积极的效果。当然过往的研究中，学者们尝试利用大规模预训练模型设计奖励机制等策略，这些方法同样展现出了解决类似问题的巨大潜力，也是值得学习的。</p><p></p><p>InfoQ：您认为未来在代价敏感智能体方面，还有哪些待解决的关键技术问题？</p><p></p><p>李鹏：我认为，首要任务是建立健全针对代价敏感智能体及其评估框架的体系。当前的基准测试 (Benchmark) 大多侧重于任务完成度，却忽略了成本效益分析，这是一个亟待填补的空白。因此，开发一套全面考量智能体表现及成本的评估方法至关重要，它既要衡量成效，也要顾及成本开销，这两个核心指标本质上可能存在冲突，需要精心设计平衡，确保评估体系能精准识别并促进系统效能的提升，这是第一个值得深入探讨的议题。</p><p></p><p>其次，针对多目标优化策略的融入也是不可或缺的一环。在这样的复杂环境中，如何有效地整合多目标优化算法，以同时追求高效率与低成本，是实践中的又一挑战。</p><p></p><p>第三，更广泛地讲，我们应致力于研发更为先进的智能体学习机制。这一点虽然与多目标优化有所交集，但第二点更多的是关注代价函数，而这个学习机制将在更多层面发挥作用，如持续演进的探索机制等。</p><p></p><p>InfoQ：您如何看待将大模型与特定领域知识结合的趋势？这在提高智能体在特定任务上的表现方面有何优势？</p><p></p><p>李鹏：对于大模型与特定领域知识结合的趋势，我持肯定态度，我认为这是大模型迈向更广阔应用场景，尤其是在开放领域中不可或缺的发展路径。尽管现下关于最佳结合策略尚未形成统一意见，无论是通过微调、RAG 或是其他创新性推理学习机制，这一融合趋势本身已成为业界共识。</p><p></p><p>至于智能体技术在此背景下的角色，其作为连接知识获取与特定领域应用的抽象化手段，显得尤为重要。智能体不仅促进了领域知识的有效吸收，同时也得益于领域知识的加持，在特定任务中展现出更优的执行能力。这一互动过程类似于人类个体的专长发展：个人在特定领域的特长越显著，相关领域知识获取速度越快；反之，对该领域的深刻理解又反过来促进其专业能力的提升，形成了一个正向循环的增强过程。</p><p></p><p>InfoQ：在设计这类智能体时，如何平衡通用性和专业性，以适应不同的应用场景？</p><p></p><p>李鹏：我认为多智能体系统提供了一个天然的解决方案思路。单一智能体在同时追求高度专业化与广泛领域适应性上面临挑战，这要求它既要精通特定领域，又要保持足够的泛化能力，实为不易。而多智能体架构则巧妙绕过了这一难题，它允许系统中并存通用型智能体与领域专用智能体。通用智能体擅长处理高层次的策略规划、任务分解及综合归纳等全局性任务；与此同时，领域专用智能体则专注于特定领域的深度知识与高效执行。通过它们之间的协同作业，系统能够更灵活、高效地达成任务目标，从而实现两者平衡的优化。</p><p></p><p>InfoQ：大模型智能体在环境感知方面的最新进展是什么？这样的趋势下，我们应该如何提升智能体交互与理解能力？</p><p></p><p>李鹏：当前环境领域展现的最显著趋势是环境日益增长的复杂度与真实性，这一演变可从三个核心方面概述：</p><p></p><p>● 纯数字环境正经历显著变化，其特点在于设计者愈发重视环境的动态交互成本及其中任务的复杂层级，推动这些虚拟场景朝向更高程度的真实感发展。</p><p></p><p>● 数字模拟环境作为衔接虚拟与现实的桥梁，利用计算技术模仿物理环境，旨在解决实际物理世界的挑战。例如，近期备受瞩目的项目 Sora，其潜力在于可能充当物理世界的高效模拟器，凸显了该领域的前沿探索。</p><p></p><p>● 物理世界环境的融入加深，不仅体现在工业界和学术界的广泛关注上，还反映在致力于减少物理数据采集成本的努力中。引人注目的是，斯坦福大学等机构开发的创新硬件，如成本仅 400 美元的机械手示教设备，通过佩戴操作直接采集数据，极大促进了智能技术与物理环境融合的基础建设，加速了实体世界数据获取的效率与可行性。</p><p></p><p>为了提升智能体的交互理解能力，核心在于借鉴大型语言模型的成功要素：海量数据、更庞大模型及高效训练策略。针对开放领域的智能体，数据的丰富性尤为关键。不论是数字环境、模拟环境或实体环境，研究焦点集中于两方面：一是创造更多样化、复杂且逼真的场景，以模拟真实世界的广泛挑战；二是优化数据采集过程，降低成本，提高从环境中提取有效信息的效率。这一系列环境构建与优化的趋势，无疑将极大地推动智能体技术的进展。</p><p></p><p>InfoQ：您本次分享想要为听众带来什么分享？带来哪些收获？</p><p></p><p>李鹏：这次演讲，我核心将分享大模型的智能体和开放领域去结合的时候，将会面临的挑战以及解决挑战的典型的新思路，虽然可能我没有办法完全覆盖所有的思路，但是可以给大家一些启发。最后，我将简要总结大模型智能体与开放域结合方面未来的发展方向。</p><p></p><p>嘉宾介绍</p><p></p><p>李鹏：清华大学 智能产业研院（AIR） 副研究员 / 副教授，主要研究兴趣包括自然语言处理、预训练语言模型、跨模态信息处理、大模型智能体等，在人工智能重要国际会议与期刊发表论文 90 余篇，曾获 ACL 2023 杰出论文奖，曾在多个国际上深具影响力的榜单上超过 Google Research、OpenAI 等团队获得第一名，主持科技创新 2030 重大项目课题、国家自然科学基金面上等科技项目，曾任 NAACL、COLING、EACL、AACL 等会议领域主席或资深领域主席。研究成果在百度、腾讯微信等千万级日活产品中获得应用并取得显著成效，获得中国中文信息学会钱伟长中文信息处理科学技术奖一等奖。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/M1GL8X7JrHfwq2qew3DE</id>
            <title>在狂卷大模型的时代，这项生产要素影响着大模型的未来</title>
            <link>https://www.infoq.cn/article/M1GL8X7JrHfwq2qew3DE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/M1GL8X7JrHfwq2qew3DE</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 15:36:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI技术, 数据, 数据多样性, 数据质量和准确性
<br>
<br>
总结: 随着生成式AI技术的快速发展，数据已成为企业竞争力的核心要素。在这个时代，拥有全面、高效的数据基座是企业充分发挥数据潜力、加速生成式AI技术落地的关键。 </div>
                        <hr>
                    
                    <p>随着生成式AI技术的快速发展，数据已成为企业竞争力的核心要素。在这个时代，拥有全面、高效的数据基座是企业充分发挥数据潜力、加速生成式AI技术落地的关键。</p><p>&nbsp;</p><p>随着科技的飞速发展，人工智能已经进入了一个全新的时代——生成式人工智能时代。在这个时代，生成式人工智能模型的发展离不开一个关键因素，那就是数据。数据对于生成式人工智能模型的重要性不言而喻，它不仅是模型训练的基础，也是模型创新和应用的源泉。为什么对于大模型而言数据如此重要？</p><p>&nbsp;</p><p>数据是生成式人工智能模型训练的基础，这是业内普遍的共识。无论是传统的机器学习模型还是现代的深度学习模型，都需要大量的数据进行训练，以提高模型的准确性和泛化能力。生成式人工智能模型更是如此，它通过学习大量的高质量数据，从而实现对新生成数据的预测和生成。没有足够的数据支持，生成式人工智能模型很难达到理想的性能。</p><p>&nbsp;</p><p>此外，数据多样性对于生成式人工智能模型的发展至关重要。生成式人工智能模型需要处理各种类型的数据，包括文本、图像、音频等。这些数据不仅需要量大，而且需要具备多样性，以保证模型能够应对各种不同的场景和任务。数据多样性可以帮助模型学习到更多的特征和规律，从而提高模型的灵活性和适应性。</p><p>&nbsp;</p><p>同时，数据的质量和准确性也是生成式人工智能模型发展的关键。生成式人工智能模型需要处理大量的数据，如果数据存在错误、噪声或者不准确的情况，那么模型很可能会学习到错误的规律和特征，导致模型性能的下降。因此，保证数据的质量和准确性对于生成式人工智能模型的发展至关重要。</p><p>&nbsp;</p><p>数据的实时更新和迭代也同样不容忽视。随着社会的发展和技术的进步，新的数据不断涌现。生成式人工智能模型需要实时地获取和处理这些新数据，以不断提高模型的性能和适应性。数据的实时更新和迭代可以帮助模型紧跟时代的步伐，从而在各个领域发挥更大的作用。</p><p>&nbsp;</p><p>亚马逊云科技大中华区产品部总经理陈晓建表示：“在生成式AI时代，企业需要的是懂业务、懂用户的生成式AI应用，而打造这样的应用需要从数据做起。亚马逊云科技构建数据基座的三大核心能力涵盖从基础模型训练到生成式AI应用构建的重要场景，能够帮助企业轻松应对海量多模态数据，提升基础模型能力。”</p><p>&nbsp;</p><p>数据处理能力是生成式AI基础模型微调和预训练的关键。亚马逊云科技提供数据存储、清洗和治理服务，如Amazon S3、Amazon FSx for Lustre、Amazon EMR Serverless和Amazon Glue等，这些服务能够帮助企业高效地处理海量数据，提高模型训练质量。</p><p>&nbsp;</p><p>此外，数据与模型的快速结合也是企业数据基座的关键能力之一。亚马逊云科技将向量搜索的支持功能加入到主流的数据服务中，通过将数据和向量存储在一起，提升数据查询性能。这使得企业能够轻松利用RAG技术将专有数据提供给基础模型，从而释放更大价值。</p><p>&nbsp;</p><p>在处理生成式AI应用的新数据方面，亚马逊云科技提供了Amazon Memory DB等高效的数据处理服务，能够降低模型频繁调用成本并提升性能。此外，无服务器数据库服务和Amazon OpenSearch Serverless的引入，更是最大限度地减少了企业的运维负担和成本。</p><p>&nbsp;</p><p>根据IDC的数据显示，全球生成式AI市场规模预计将在2024年达到100亿美元，年复合增长率达到40%。这一数据充分展示了生成式AI技术的巨大潜力和市场需求。在这个时代，拥有全面、高效的数据基座和懂业务、懂用户的生成式AI应用将成为企业脱颖而出的关键。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7i2VwDjI6MR4Bx8Y66D9</id>
            <title>裁员、人去楼空，这家估值80亿的AI编程工具独角兽不行了？</title>
            <link>https://www.infoq.cn/article/7i2VwDjI6MR4Bx8Y66D9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7i2VwDjI6MR4Bx8Y66D9</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 09:46:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI编码工具, 裁员, Replit, 人工智能
<br>
<br>
总结: 近日，AI编码工具初创公司Replit宣布裁员30名员工，占员工总数近20%。公司首席执行官表示裁员是为了更好地满足不断变化的业务需求，同时强调公司的长期目标和使命。裁员消息引发了关于公司未来发展和人工智能在编码工具中的应用的讨论。 </div>
                        <hr>
                    
                    <p>近日，据外媒报道， AI编码工具初创公司Replit宣布将解雇 30 名员工，占其员工总数的近 20%。</p><p>&nbsp;</p><p>Replit 首席执行官 Amjad Masad 在发给员工的电子邮件中分享了这一消息，该电子邮件后来发布在 X（以前的 Twitter）上。 Masad 在邮件中承认，与 30 名“杰出同事”分道扬镳是一个艰难的决定，并强调此举对于 Replit 实现其长期目标和使命是必要的。</p><p>&nbsp;</p><p></p><h2>Replit宣布裁员30人，公司人去楼空？</h2><p></p><p>&nbsp;</p><p>Amjad Masad邮件全文翻译如下：</p><p></p><blockquote>在过去的一年里，我们一直在努力打造一个名为Replit的平台，该平台旨在使公司中的任何人都能成为程序员。虽然机会显而易见，但我逐渐意识到，为了服务企业，我们需要不同角色的组合。因此，我们决定与30%的杰出同事分道扬镳。很快您将收到一封电子邮件，告知我们是否不再需要您的职位。&nbsp;对于那些离开我们的人，我们不会轻率地看待这对您产生的影响。虽然我们知道您都很有才华，并期待您在新的角色中取得成功，为了让过渡更加容易，我们比正常情况更加慷慨：提供4个月的遣散费和6个月的医疗保险，免除1年的归属期限制，给您一年时间来决定是否行使您的股票期权，并允许您保留Replit提供的笔记本电脑和设备。我们的招聘团队将为需要的人提供求职支持，我们的投资者也准备将您介绍给其他公司。PeopleOps将提供更详细的福利信息。&nbsp;我知道鉴于我们强大的财务状况和未来的大量工作，这一消息可能会让您感到惊讶。通常在创业公司的这个阶段，公司会大力促进增长，尽可能多地招聘人员——而不是裁员。在Replit，我们总是与众不同，以较小的团队取得了巨大的成就。小团队可以更加专注，使我们能够更快地前进，同时给我们实现潜力的空间。&nbsp;现在是加快步伐的时候了，因为Replit的机会从未如此清晰。Replit和Al使几乎每一位知识工作者都有可能创造软件。企业需要Replit，超过一半的财富500强用户已经开始尝试使用我们。我们现在需要让构建出色的软件变得更加容易和快速。尽管技术趋势对我们有利，但成功并非必然。我们正在组建一个新的销售团队，并将致力于使我们对在公司工作的人有用，并帮助这些公司发现和支付我们提供的服务。&nbsp;我想向那些离开我们的人表达我深深的感谢。你们的奉献使我们走到今天，我们对此深感感激。你们是令人印象深刻的专业人士，我相信无论您走到哪里都会取得巨大的成功。我相信我们今天采取的行动将在长期内通过充分发挥Replit的潜力并使您的股权更加有价值来对您有利。&nbsp;对于留下来的人，前方的道路将充满挑战，但这也是我们生命中做最出色工作的机会。我想提前感谢你们的努力。今天晚些时候，我们将开会讨论这一变化。明天，我们将不再举行常规的“每周胜利”会议，我将很高兴接受问题并讨论我们计划使Replit变得更加伟大的计划。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e916c1855271af6ef2447d7661a5544.png" /></p><p></p><p>&nbsp;</p><p>Replit 是一个广泛使用的编码和托管项目平台，尽管目前公司有着强劲的财务业绩，但还是走到了裁员这一步。据悉，此次裁员的目的是重新调整员工队伍，以更好地满足其不断变化的业务需求。</p><p>&nbsp;</p><p>根据Pitchbook 的数据，Replit 已筹集了超过 2 亿美元的资金，其中包括2023 年 4 月的1 亿美元融资，在宣布裁员时，其员工约有 170 名。</p><p>&nbsp;</p><p>Masad 的电子邮件没有具体说明哪些部门受到影响。但邮件强调并保证所有受影响的员工将获得四个月的工资赔偿、六个月的医疗保健和求职支持。此外，被解雇的员工可以保留他们的公司笔记本电脑。Masad 重申，Replit 的使命是让编程更容易获得，将人工智能融入其平台的各个方面。</p><p>&nbsp;</p><p>该初创公司以其基于浏览器的集成开发环境而闻名，它为开发人员通常使用的传统桌面应用程序提供了替代方案，在全球拥有超过 1000 万用户。</p><p>&nbsp;</p><p>拥有基于浏览器的 IDE的优点是能使开发人员更轻松地启动和运行，因为在启动平台之前几乎不需要进行任何设置，这与基于桌面的 IDE 不同，后者可能需要几个小时才能完成配置。借助 Replit，开发人员只需导航到正确的 URL 即可立即开始编码。</p><p>&nbsp;</p><p>最近，Replit 大力推动将生成式人工智能功能集成到其编码工具中。就在上个月，它推出了一款名为 Replit Teams 的产品，该产品类似于GitHub 上流行的 Copilot 工具，提供了一个 AI Agent，可以与开发人员实时合作，提出修复编码错误的建议或提高他们编写代码效率的建议。</p><p>&nbsp;</p><p>目前尚不清楚 Replit 是否打算用AI取代其员工，但裁员仍然提醒人们这种创新的人力成本。</p><p>&nbsp;</p><p>Masad 在给员工的一封电子邮件中表示，公司打算将人工智能融入其编码平台的各个方面。他表示，Replit 的业务不是销售人工智能，而是“销售一个梦想，即让你梦想的软件更容易获得，让编程更容易获得”。</p><p>&nbsp;</p><p>值得注意的是，今天，X上已经有用户晒出了Replit在旧金山的办公室，目前已经人去楼空，办公室呈对外出租的状态。而根据公开资料，Replit的总部就位于旧金山。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1098d2142ddee0a5bf148318c55ceef4.png" /></p><p></p><p>&nbsp;</p><p></p><h2>开发者工具生意不好做</h2><p></p><p>&nbsp;</p><p>多年来，Replit 一直专注于开发者工具的研发。截止到去年，Replit 上的开发人员达到 2000 万。他们已经创建了超过 2.4 亿个 Repl，从多人游戏到具有云中实时协作功能和先进的 AI 工具的生产软件。开发人员也在 Replit 上建立自己的业务，从独立黑客到Fig、AmpleMarket和BerriAI等 YC 支持的初创公司，再到Deel等独角兽。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/783efe069a79bf7a29100e35e2e45343.png" /></p><p></p><p>2015 年至 2023 年 Replit 开发者增长图表</p><p>&nbsp;</p><p>上个月，Replit 还刚刚发布了其新产品 Replit Teams，这款工具将允许开发人员实时协作开发软件项目，同时 AI Agent会自动修复编码错误。</p><p>&nbsp;</p><p>明明是一片欣欣向荣的景象，但 Replit 却在此时裁员并出租办公室，可见开发者工具这门生意也不好做。</p><p>&nbsp;</p><p>开发者工具在软件开发和编程领域的重要性不容忽视。它们为开发者提供了一系列强大的功能，包括但不限于代码编辑、编译、调试、测试、版本控制、部署等，极大地提高了开发效率，减少了错误和漏洞，缩短了开发周期，降低了开发成本。同时，开发者工具还能提升代码的可读性、可维护性和可扩展性，为软件的高质量发展提供了坚实保障。</p><p>&nbsp;</p><p>需求是有的，但想把好的工具产品卖出去，没有想象中容易。</p><p>&nbsp;</p><p>事实上，销售开发者工具的真正难题是，普通开发者没有权利去买这些工具！Stealth公司安全工程师Daniel Feldman在X上发帖讽刺了这种现象。</p><p>&nbsp;</p><p></p><blockquote>销售人员需要花费1000美元？在公司看来可能没什么大不了的。财务需要花费10万美元？可能也没什么大不了的。但是，如果工程师想买一本50美元的书？他们甚至要找副总裁去审批！</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff799c94854f7a9f4298da54d76e43d8.png" /></p><p></p><p>&nbsp;</p><p>Feldman提出的问题得到了很多人的共鸣，一位Twitter网友说，他的团队曾经提出想购买一个9美元每月的工具，该工具能大大提升生产力，但他们的请求被残酷的拒绝了。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/179c1da61fffef6441c054b24767222e.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>在Hacker News上，这也是一个非常热门的话题，大家都太有相同的感受了。一位ID为jimnotgym的用户也对Daniel的观点发表了看法：</p><p>&nbsp;</p><p></p><blockquote>情况基本就是这样。我认为很多公司财务和销售人员没有这种权利来购买这类工具。这就是为什么你会看到大多数公司都在使用免费软件和工具。要去说服领导购买这些工具能够节省时间、能够让项目更加透明、能够更加清晰地分类等等。这可能就是Excel如此普遍的原因。不得不提，Excel算是你公司IT部门能为你提供的最强大的工具了。如果你从事销售或财务工作，可尝试让公司IT部门给你的机器上安装WSL，这可能就是网络应用如此流行和受欢迎的原因。</blockquote><p></p><p>&nbsp;</p><p>此外，另一位ID为al_borland的Hacker News用户称，到底是公司IT部门太愚蠢了无法帮他们解决问题，还是各个部门协作之间出了问题？IT部门的员工有自己的工作和可交付成果，所以他们没有一个人在为销售团队构建工具。“我不指望公司IT部门为销售团队开发东西，就像我不指望销售团队能帮助IT部门向管理层们索要开发所需的工具一样。”</p><p>&nbsp;</p><p>有观点认为，IT 部门被视为成本中心，因此管理层会尽可能地削减他们的预算。当 IT 部门连负担一个好的备份系统都勉为其难时，他们就更没有能力去研究如何支持能帮助你的工具了。</p><p>&nbsp;</p><p>另一位网友也表示赞同，他说他以前在一家工程公司的销售导向型子公司工作过，他们在IT预算上卡得很严，“早期时候，我可以带人去吃 100 美元的商务午餐，这没问题，但是我却买不了 30 美元的电脑配件卡。实际上，无论是母公司还是子公司，我什么事情都做不了，因为都需要填写表格来申请资金。所以许多成功的开发人员工具公司通过向老板营销来挣钱，而不是向开发人员营销。这实际上也适用于许多领域。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa044b317529355369152f4d565c0ffc.jpeg" /></p><p></p><p>&nbsp;</p><p>另有网友夸张的说，“将开发者工具卖给开发人员就像卖玩具给孩子一样——你应该瞄准父母 (或者公司里支付费用的高管)。应该向 Slack、Postman 等公司学习，让它们成为开发人员/用户的默认工具，让他们满意，然后他们会去找父母/审批人购买。”</p><p>&nbsp;</p><p>另有网友反驳说，“如果比作卖玩具，实际这还不太一样。就像你到处都能看到儿童节目《汪汪队立大功》的周边商品一样：他们制作了一个让孩子们上瘾的动画节目，剩下的事情就交给孩子们了。但这在开发人员身上行不通，因为不像孩子的父母，企业里拥有支出权的人通常不在乎，在他们眼里开发人员只是一个成本中心，是图表上的一个数字。”</p><p>&nbsp;</p><p>实际上，开发人员开发了很多有价值的软件，但涉及到自己的工作时，却成了 “鞋匠的孩子没鞋穿”。只是如果有好用的开发者工具，生产力的提升将是巨大的。针对开发者工具行业来说，我们呼吁企业管理者能够更大方地给开发人员一个预算，让他们做出一些决定。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.hrkatha.com/hiring-firing/replit-announces-layoffs-amid-push-for-ai-integration/">https://www.hrkatha.com/hiring-firing/replit-announces-layoffs-amid-push-for-ai-integration/</a>"</p><p><a href="https://news.ycombinator.com/item?id=40029283">https://news.ycombinator.com/item?id=40029283</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZR9b6a3OGg5SZgjYl9jW</id>
            <title>TiDB 如何利用 Copilot 优化数据库操作，提升用户体验与内部效率？</title>
            <link>https://www.infoq.cn/article/ZR9b6a3OGg5SZgjYl9jW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZR9b6a3OGg5SZgjYl9jW</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 09:36:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据量增长, 实时数据处理, 数据安全与隐私保护, 多样化数据类型
<br>
<br>
总结: 数据库系统在面对数据量增长、实时数据处理、数据安全与隐私保护以及多样化数据类型等挑战时，需要应对这些问题并提供智能化解决方案，其中LLM技术在数据库领域的应用成为热点。LLM技术可以优化数据库系统性能和可靠性，提供智能化解决方案，从用户感知到内部操作优化都有显著贡献。 </div>
                        <hr>
                    
                    <p>在应对不断增长的数据量、复杂的业务逻辑和对更高性能与可靠性的追求中，数据库系统面临着重重挑战。其中，有效处理大规模数据并保障数据的安全性与隐私性是当前需要解决的问题。随着人工智能技术的不断演进，LLM 的应用成为了数据库领域的热点。LLM 技术不仅能够优化数据库系统的性能和可靠性，还能为数据库查询、流程优化等提供更智能化的解决方案。</p><p></p><p>在 AICon 全球人工智能开发与应用大会上，我们有幸邀请到 PingCAP AI Lab 负责人李粒为我们分享他的见解。他分享了 LLM 技术在数据库领域的应用前景与解决方案。会前，InfoQ 有幸采访了李粒，以下为采访对话～</p><p></p><p></p><h4>数据库领域挑战与应用案例</h4><p></p><p></p><h5>InfoQ：数据库领域当前最紧迫的挑战是什么？您认为这些挑战如何影响数据库系统的性能和可靠性？</h5><p></p><p></p><p>李粒： 当前数据库领域面临的最紧迫挑战之一是 ++ 如何处理和分析日益增长的数据量，同时保持高效的性能和可靠性 ++。这个挑战主要体现在以下几个方面：</p><p></p><p>首先，数据规模的持续增长 是一个显著挑战。随着物联网、社交媒体和企业应用等领域的快速发展，数据量呈指数级增长。这不仅要求数据库能够有效地存储和管理海量数据，还需要优化存储结构、索引机制和查询处理，以维持高效的性能。</p><p></p><p>其次，实时数据处理的需求日益增加。现代业务场景，如实时分析和在线事务处理，要求数据库系统能够在处理大量数据的同时，保证极低的延迟。这对数据库的设计和优化提出了更高的要求。</p><p></p><p>第三，数据安全与隐私保护 也是一个重大挑战。随着数据泄露事件的频发，如何通过加密、访问控制等措施保护数据安全，防止未授权访问或泄露，成为了数据库系统设计的一个重要方面。</p><p></p><p>此外，我们还面临着处理多样化数据类型和复杂数据关系的挑战。现代数据库不仅要处理结构化数据，还要能够有效管理半结构化和非结构化数据。同时，数据之间的关系也变得更加复杂，这对数据库的模型和查询语言提出了新的要求。</p><p></p><p>最后，高可用性和灾难恢复能力也是企业越来越关注的问题。任何数据丢失或服务中断都可能导致重大的业务损失，因此，确保数据库的高可用性和快速恢复能力是至关重要的。</p><p></p><p>这些挑战直接影响到数据库系统的性能和可靠性。例如，如果处理大规模数据时缺乏有效的索引和查询优化技术，将导致查询速度缓慢，严重影响用户体验。同样，如果安全措施不到位，数据可能面临泄露或损坏的风险，进而影响系统的整体可靠性。</p><p></p><p></p><h5>InfoQ：LLM 技术在数据库领域的应用案例有哪些？您可以分享一些具体的实例，以及这些案例是如何利用 LLM 技术解决现有数据库系统的挑战的？</h5><p></p><p></p><p>李粒：LLM 技术在数据库领域的应用非常广泛，从提高用户体验到内部操作优化，都有显著的贡献。</p><p></p><p>首先，在用户感知方面，LLM 技术可以极大地简化用户与数据库的交互。例如，基于文档的 ChatBot，如 TiDB Bot，可以在 Slack 或 Cloud 平台上支持用户的使用提问。这种 ChatBot 能够理解用户的查询意图，并提供关于数据库配置、日志管理、慢查询优化等方面的建议。这不仅提高了用户的操作便利性，还有助于用户更有效地管理数据库。</p><p></p><p>此外，LLM 技术还能够帮助用户直接通过自然语言生成 SQL 查询（NL2SQL）。这意味着即使用户不熟悉 SQL 语法，也能通过描述他们的查询需求来获取数据。更进一步，我们可以将这种技术扩展到从原始数据到商业洞察的转换（NL2Insight），这不仅仅是生成 SQL，而是提供更深层次的数据分析和业务洞察。</p><p></p><p>在诊断和故障恢复方面，LLM 技术也显示出巨大的潜力。通过集成到基于 ChatBot 的系统中，LLM 可以利用日志、慢查询、性能指标等信息，提供更深入的领域判断和业务问题分析。这有助于减少平均故障修复时间（MTTR），使得即使非专业的用户也能快速诊断并解决问题。</p><p></p><p>在用户不直接感知的内部使用方面，LLM 技术同样发挥着重要作用。例如，在自动化测试中，LLM 可以用来生成数据库系统的测试用例，提高测试的覆盖率和效率。在代码审查中，LLM 可以帮助分析代码质量和风格一致性，提高开发效率。此外，LLM 还可以自动化生成性能分析报告、故障报告等，帮助技术团队快速获取关键信息，并管理企业内部的知识库，提高信息共享和检索效率。</p><p></p><p></p><h5>InfoQ：您提到的 Flow 和 Agent 应用分别是什么？能否详细解释这些技术方向？</h5><p></p><p></p><p>李粒： 在 LLM 应用中，我们可以区分三个技术层次：Wrapper, Flow, 和 Agent。每个层次都代表了与 LLM 交互的不同复杂度和应用场景。</p><p></p><p>LLM Wrapper:</p><p></p><p>这是最基础的应用层次，涉及到与 LLM 的单次交互。在这个层次中，用户的请求直接被发送到模型，模型则返回一个响应。这种方式的能力上限直接受限于模型本身的推理能力。它适合于业务初期，当企业在寻找产品与市场契合度（PMF）时，可以快速开发和迭代。</p><p></p><p>Flow（DAG）:</p><p></p><p>在 Flow 层次上，业务逻辑通过有向无环图（DAG）构建，实现与 LLM 的多次交互。每次交互都专注于解决一个特定问题，例如意图判断、内容改写、提供回答或批评等。这种方法有效克服了单次交互的局限性，支持构建更复杂的应用。适用于那些对如何利用 LLM 解决业务问题有清晰理解的场景，需要处理更复杂逻辑和提高准确度时采用。</p><p></p><p>Agent（Loop）:</p><p></p><p>Agent 层次基于 Loop+Feedback 构建。在这里，LLM 能够根据人类输入自主决定和执行所需步骤，完成后自我评估是否存在异常，并据此进行调整。通过这种方式，LLM 能够显著提高结果的准确性，并解决更复杂的问题。构建 Agent 的逻辑与传统应用截然不同，其核心思想类似于构建一个团队或公司，每个 Agent 都是具有一定能力的工作力量。通过大量 Agent 的相互补充，最终共同做出相对合理的决策。</p><p></p><p>这些技术方向没有绝对的好坏，关键在于选择最适合当前业务需求的技术层次。随着业务的发展和需求的变化，可能需要从一个层次迁移到另一个层次，以适应更复杂的场景和提高系统的整体性能。</p><p></p><p>可以从这样的一张表格中，清晰的进一步认识不同层级应用的区别。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bcb80dacba40898c0b49893170fd6aa6.png" /></p><p></p><p></p><h4>数据库的 Copilot 技术实践</h4><p></p><p></p><p></p><h5>InfoQ：在使用 LLM 解决实际问题时，您认为典型的步骤是什么？在这些步骤中，您所遇到的主要挑战和困难是什么？您是如何应对这些挑战的？</h5><p></p><p></p><p>李粒： 使用 LLM 解决实际问题通常涉及几个关键步骤，每个步骤都有其独特的挑战。</p><p></p><p>典型的业务实现步骤包括：</p><p></p><p>业务洞察和需求判断：这是项目启动的第一步，需要深入理解业务需求和痛点。实验和可行性分析：在这一步，我们会进行初步的实验，测试 LLM 的适用性和效果。应用类型迭代：根据场景的复杂度，我们可能会从 Wrapper 开始，逐步迭代到 Flow 和 Agent。反馈设计和收集：设计有效的反馈机制，以收集用户反馈，这对模型的优化至关重要。持续优化设计和实施：根据收集到的反馈不断优化模型和业务流程。</p><p></p><p>在这些步骤中，我们遇到的主要挑战包括：</p><p></p><p>业务理解：深入理解业务需求常常需要与业务方紧密合作，这可能涉及到亲自参与业务流程。模型能力：模型的理论能力与实际应用效果之间可能存在差距。工具的成熟度：目前市场上的工具可能还不够成熟，我们有时需要进行开源贡献或自主研发。LLM 的稳定性：包括回答的稳定性和流程的稳定性，这需要通过精心设计的 Prompt 和流程控制机制来解决。LLM 的回答格式和质量：如何确保 LLM 提供的回答既符合业务需求又具有高质量，这需要通过持续的反馈和优化来实现。</p><p></p><p>应对策略：</p><p></p><p>深入合作：与业务方进行深入合作，确保充分理解业务需求。持续实验：通过持续的实验和可行性分析，不断调整和优化 LLM 的应用。反馈机制：设计有效的反馈机制，如 ChatBot 的点赞和相似性评价，SQL 的正确性评价等，以收集用户反馈并优化模型。增强工具和流程：开发或改进工具，优化业务流程，提高 LLM 的应用效果和稳定性。</p><p></p><p></p><h5>InfoQ：什么是 Copilot，和其他 AI 应用方法有什么区别？</h5><p></p><p></p><p>李粒：Copilot 是 AI 应用中的一种非常具体的交互方式，它在灵活性和易用性之间寻求平衡，旨在减轻用户的认知负担，同时提供有效的支持。</p><p></p><p>Copilot 的核心特性和应用：</p><p></p><p>Copilot 可以被视为用户的“数字助手”或“教练”，它嵌入到用户的工作流程中，提供主动的建议和支持。这种方法的关键在于它能够捕获大量的上下文信息，从而使 AI 能够提供更加精准和有用的建议。例如，GitHub Copilot 在编程环境中提供代码建议，而 Database Copilot 则可能在数据库管理或查询优化中提供帮助。</p><p></p><p>与其他 AI 应用方法相比，Copilot 的主要区别在于它的集成程度和交互方式。例如，与传统的聊天机器人相比，Copilot 更加深入地融入用户的具体任务和工作流程中，而不仅仅是提供一般性的对话支持。</p><p></p><p>与其他 AI 应用方法的比较：</p><p></p><p>一般聊天（Chat）：这种方法提供最高的自由度，用户可以自由地以自然语言与系统交互。然而，它可能在易用性方面不如其他方法，尤其是在需要具体指导或操作的任务中。如 ChatGPT。专业聊天（Specialized Chat）：这种方法通过限制讨论的主题范围来提供更专业的支持。它比一般聊天更具指导性，但牺牲了一定的灵活性。如 TiDB Bot 只讨论 TiDB 的问题。AI 启用的特性（AI-enabled Features）：这种方法提供最高的易用性，通常通过直接的按钮或切换来启用特定的 AI 功能。它的结果更可预测，但灵活性最低。如 Notion AI 的续写、翻译功能。</p><p></p><p></p><h5>InfoQ：数据库的 Copilot 是什么？</h5><p></p><p></p><p>李粒： 数据库 Copilot 是一种 AI 应用，它嵌入到用户的工作流程中，以提供实时的指导和建议，从而提高用户的工作效率和决策质量。这种应用模式在很多方面类似于飞行中的副驾驶，它不仅提供辅助，还能确保操作的正确性和安全性。</p><p></p><p>数据库 Copilot 的核心特性和工作方式：</p><p></p><p>集成与工作流程：数据库 Copilot 深入集成到用户的数据库操作和查询流程中。它通过分析用户的输入和行为，以及数据库的响应和状态，实时提供反馈和建议。主动建议：与传统的工具不同，数据库 Copilot 采用主动出击的方式，根据当前的上下文和历史操作模式，推荐最佳的操作步骤或查询优化建议。上下文感知：它能够理解和分析用户在数据库中的操作上下文，这使得它能够提供更加精准和有用的建议。学习与适应：数据库 Copilot 通过持续学习用户的操作习惯和偏好，不断优化其建议算法，以提供更加个性化的支持。</p><p></p><p>应用场景：</p><p></p><p>查询优化：对于复杂的 SQL 查询，Copilot 可以提供性能优化建议，帮助用户改写查询以提高执行效率。错误诊断：在用户遇到查询错误或性能瓶颈时，Copilot 可以提供诊断信息和修复建议。学习辅助：对于不熟悉数据库操作的用户，Copilot 可以作为一个实时的学习工具，提供操作指导和最佳实践。</p><p></p><p>挑战与对策：</p><p></p><p>用户信任：建立用户对 Copilot 建议的信任是一个挑战。为此，我们确保所有建议都基于最佳实践和精确的数据分析，同时提供足够的解释和文档支持。平衡自动化与控制：过度的自动化可能导致用户感觉失去控制。我们通过提供可调节的自动化级别和详细的用户控制选项来解决这一问题。持续学习：为了保持 Copilot 的效果，我们持续收集用户反馈和操作数据，用于训练和优化模型。</p><p></p><p></p><h5>InfoQ：在处理复杂业务逻辑和规则时，如何保证生成的 SQL 语句的业务逻辑正确性？您是如何验证 Copilot 生成的 SQL 语句是否符合业务需求的？</h5><p></p><p></p><p>李粒： 确保生成的 SQL 语句符合业务逻辑的正确性是一个多步骤的过程，涉及到从数据架构的增强到持续的优化和反馈收集。我可以分几个部分来详细说明这个过程。</p><p></p><p>Schema 增强：</p><p></p><p>在导入数据时，我们会对数据库的 schema 进行详细描述，包括列描述、表描述、表关系、数据库描述以及主实体。这有助于模型更好地理解数据结构和业务上下文。</p><p></p><p>语料库建设：</p><p></p><p>我们会在导入数据时同时引入与业务相关的语料库，这包括 schema 信息、领域知识和具体的 SQL 案例。这些语料库帮助模型学习特定业务领域的语言和逻辑。</p><p></p><p>Prompt（自问自答）：</p><p></p><p>我们使用自问自答的方式来优化查询生成过程。这包括任务重写、实体提取、子问题生成及其解答，以及查询合并。这一步骤是确保生成的 SQL 语句逻辑正确性的关键。</p><p></p><p>自我修正（Self-Fix）：</p><p></p><p>我们对生成的 SQL 进行优化，确保其可执行性。如果一个查询执行报错，系统会将错误信息反馈给 LLM，然后尝试生成新的查询，直到得到一个可以正确执行的结果。</p><p></p><p>持续优化：</p><p></p><p>一个持续学习和适应的 Agent，专门负责优化 SQL 查询的生成和执行。</p><p></p><p>Agent 的工作机制包括以下几个方面：</p><p></p><p>自动化监控和反馈循环：Agent 持续监控数据库操作的效果，包括查询的执行时间、资源消耗等关键性能指标。同时，它也收集用户对查询结果的反馈，如点赞、点踩、修改等。基于这些数据，Agent 可以自动识别哪些查询需要优化，哪些已经达到了较好的性能。动态学习和调整：Agent 使用机器学习算法来分析收集到的数据，从中学习如何改进 SQL 语句的结构和逻辑。这包括选择更有效的索引、调整查询的结构、优化连接和过滤条件等。它还能根据数据库的实时状态动态调整查询策略，以适应数据量的变化、数据库负载的波动等外部条件。生成和测试新的查询方案：在识别出需要优化的查询后，Agent 会自动生成一系列改进的查询方案。这些方案会在一个安全的测试环境中执行，以评估它们的性能和准确性。通过比较不同方案的执行结果，Agent 可以选择最优的查询方案，并将其推荐给用户或自动应用到生产环境中。持续迭代和优化：这个过程是持续进行的。Agent 会不断迭代和优化其学习模型和查询生成算法，以适应新的业务需求和技术变化。它还会定期清理和更新其语料库，去除过时或低效的数据，确保学习资源的质量和相关性。</p><p></p><p></p><h4>AI Agents 的高效运行和数据安全</h4><p></p><p></p><p></p><h5>InfoQ：LLM Agents 的落地涉及到服务开销和实时性，您是如何在平衡这两方面的情况下确保系统的高效运行和响应速度的？</h5><p></p><p></p><p>李粒： 这确实是在部署 LLM Agents 时面临的一个重要挑战。由于 LLM Agents 通常需要与 LLM 进行多次交互，这不仅增加了运行成本，还可能影响响应速度。这里的根本原因是 Agents 多次与 LLM 交互（常见有 20 次 -30 次）+ LLM 本身的运行速度 + LLM 成本很贵。</p><p></p><p>我们采取了几种策略来平衡这两方面的需求，以确保系统的高效运行。</p><p></p><p>优化交互次数和处理速度：</p><p></p><p>我们对不同类型的任务采用不同级别的 LLM 应用。例如，对于需要快速响应的任务，我们可能会使用 Wrapper 或 Flow 模式，这些模式的交互次数较少（通常是 1-5 次），可以在较短的时间内完成。对于可以容忍较长处理时间的任务，如后台分析、故障诊断等，我们会使用 Agents 模式。虽然这种模式需要更多的交互次数（20-30 次），但它可以处理更复杂的逻辑和循环，提供更深入的分析。</p><p></p><p>提升 LLM 的运行效率：</p><p></p><p>我们会根据需要调整服务的硬件配置，比如增加更多的 GPU 资源，以缩短处理时间。</p><p></p><p>成本控制和业务适配：</p><p></p><p>我们密切监控服务的成本和性能，确保在不牺牲用户体验的前提下，尽可能地降低成本。根据不同业务的特点和需求，选择最合适的 LLM 应用模式。例如，在一些非实时的应用场景中，可以接受较长的响应时间，这时可以使用更复杂的 Agents 模式来提高分析的准确性和深度。</p><p></p><p>混合使用不同的模式：</p><p></p><p>在某些情况下，我们会在 Flow 中嵌入 Agents 作为一个工具，用于解决特定的、易出错的问题。这种混合模式可以在保证效率的同时，解决复杂的问题。</p><p></p><p></p><h5>InfoQ：当 LLM Agents 需要获取相关业务或客户的数据以提高服务能力时，如何保护这些数据的安全和隐私？您采取了哪些措施来确保数据的保密性和完整性？</h5><p></p><p></p><p>李粒： 保护客户数据的安全和隐私是我们的首要任务。我们采取了多层次的措施来确保数据的保密性和完整性，这些措施涵盖了系统级和模型级的安全策略。</p><p></p><p>系统级措施：</p><p></p><p>数据访问控制：我们使用 TiDB 和 TiDB Cloud 来管理数据，这些系统具备强大的数据访问控制功能。除非得到用户的显式授权，否则无法访问业务数据。这确保了数据访问的合法性和安全性。数据脱敏：在处理业务数据时，我们会对数据进行脱敏处理。这意味着在数据被 LLM Agents 使用之前，所有敏感信息都会被去除或替换，以确保即使数据被泄露，也无法被恶意利用。</p><p></p><p>模型级措施：</p><p></p><p>内部模型与第三方模型的安全策略：对于内部模型，我们可以严格控制数据处理和存储的环境。对于涉及第三方模型的交互，我们采取以下措施：数据最小化和脱敏：我们尽可能减少处理的数据量，并对所有敏感数据进行脱敏处理。这包括使用数据掩码或伪匿化技术，确保敏感信息不被暴露。字段替换：在某些情况下，我们会将敏感字段替换为随机字符（如 abcd），并提供这些字符的解释，这样即使数据被泄露，也无法直接关联到具体的业务信息。</p><p></p><p>持续的安全审计和更新：</p><p></p><p>安全审计：我们定期进行安全审计，以检查和评估现有的数据保护措施的有效性。技术更新：随着安全技术的发展，我们持续更新我们的数据保护技术和策略，以对抗新的安全威胁和挑战。</p><p></p><p></p><h5>InfoQ：您认为未来数据库 Copilot 可能的发展方向是什么？</h5><p></p><p></p><p>李粒： 未来数据库 Copilot 的发展方向可能会集中在以下几个关键领域，以进一步提升其智能化水平和用户体验，同时解决现有的挑战：</p><p></p><p>更深层次的自然语言处理能力</p><p></p><p>随着自然语言处理技术的进步，未来的数据库 Copilot 将能更准确地理解复杂的自然语言查询和指令，甚至能处理含有多重意图和复杂关系的查询。这将使非技术用户能够更直观、更自然地与数据库交互。</p><p></p><p>增强的上下文理解和持续对话能力</p><p></p><p>未来的数据库 Copilot 可能会具备更强的上下文保持能力，能够在一系列交互中理解和引用之前的对话内容。这将使得进行复杂的数据分析和操作变得更加连贯和用户友好。</p><p></p><p>自动化数据分析和见解生成</p><p></p><p>数据库 Copilot 将进一步发展其能力，不仅能执行查询，还能自动分析数据，提供业务见解和建议。例如，它可以自动识别数据趋势、异常和潜在的优化点，并向用户提出建议。</p><p></p><p>更强的个性化和适应性</p><p></p><p>通过机器学习和用户行为分析，数据库 Copilot 将能够适应特定用户的查询习惯和偏好，提供更个性化的服务。例如，根据用户的角色和过去的查询历史，自动调整查询结果的展示方式和详细程度。</p><p></p><p>更广泛的集成和兼容性</p><p></p><p>未来的数据库 Copilot 将支持更多类型的数据库和数据存储解决方案，包括 NoSQL 数据库、云存储和实时数据流平台。同时，它也将更容易集成到各种业务应用和数据分析工具中。</p><p></p><p>增强的安全性和隐私保护</p><p></p><p>随着数据安全和隐私保护的重要性日益增加，未来的数据库 Copilot 将采用更先进的安全技术，如同态加密、访问控制和隐私保护算法，确保用户数据的安全和合规性。</p><p></p><p>自动化数据库管理和优化</p><p></p><p>数据库 Copilot 将能够自动执行更多的数据库管理任务，如性能监控、故障诊断、自动调优和备份管理。这将大大减轻数据库管理员的负担，提高数据库的运行效率和可靠性。</p><p></p><p>嘉宾介绍：</p><p></p><p>李粒 PingCAP AI Lab 负责人，研究领域涵盖推荐系统和强化学习。曾参与开发基于强化学习的围棋算法，击败时任围棋世界冠军朴廷桓。在 PingCAP，负责构建 Auto-Diagnosis 系统，推动自动驾驶数据库云的发展，持续关注 AI 领域的应用创新，推动其落地和融入生产，致力于推动企业 AI 应用的变革。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cBzB0Eq0cqXS6h4SiZWp</id>
            <title>如何落地AI编程和可观测智能化？怎么从 0 到 1 训练大模型？阿里多位专家出席 ArchSummit 现身说法</title>
            <link>https://www.infoq.cn/article/cBzB0Eq0cqXS6h4SiZWp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cBzB0Eq0cqXS6h4SiZWp</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 08:16:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 麦肯锡, AI编程助手, AIOps, SMS网关
<br>
<br>
总结: 麦肯锡的研究显示，AI编程助手可以显著提高代码文档的维护效率和新代码生成效率。AIOps是解决IT系统复杂性和监控挑战的有力工具，而构建高弹性的SMS网关是应对流量波动的关键。在ArchSummit峰会上，专家们将分享关于AI编程、AIOps、SMS网关等领域的最新技术和实践经验。 </div>
                        <hr>
                    
                    <p>麦肯锡的一项研究结果表明，在生成式 AI 的辅助下，可维护性代码文档可以在一半的时间内完成，新代码生成效率提升近一倍，而代码重构类任务的完成时间也节省近 1/3。</p><p></p><p>可见，随着 AI 大模型相关技术的快速发展， AI 编程助手的引入的确为软件开发带来了质的飞跃。那么，具体突破体现在哪些方面？基于大模型的 AI 编程工具存在哪些设计要点、难点和改进思路？从开发者自身的生命力出发，又如何用 AI 激活开发效率，提升生产力？</p><p></p><p>在 6 月 14 日 -6 月 15 日于深圳举办的 ArchSummit 全球架构师峰会上，我们邀请到了阿里巴巴研究员、阿里云云原生应用平台负责人丁宇（叔同），在 Keynote 主题演讲中分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5718"> AI 编程带来的革命性巨变</a>"，他将围绕以上话题展开深入介绍，并且从全球视角分享开发者对于 AI 编程的需求差异、AI 编程工具能力，以及 AI 编程领域未来的发展趋势。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a58a7c4317b485b858dd248471d04e6.webp" /></p><p></p><p>生成式 AI 的引入，在带来效率提升的同时，也使得 IT 系统复杂性日益增加，运维和监控领域面临的挑战随之增长。但“魔法”总能打败“魔法”，技术的问题同样可以用技术解决。AIOps 已成为解决以上挑战的有力工具，能够帮助自动化和优化监控流程，并显著提高效率。</p><p></p><p>在 ArchSummit 深圳，阿里云高级算法工程师陈昆仪博士将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5813">《阿里云可观测智能化探索与实践》</a>"的议题分享，详细探讨智能算法在系统异常检测和故障根因定位方面的应用，如推荐告警阈值、时序预测算法、定位异常服务以及分析错误或缓慢的调用链。此外，还将介绍如何利用 LLM 将自然语言自动转换为 Prometheus 查询语言（PromQL）的技术，简化查询构建的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1084942512f4a342cc562a46a77d062d.webp" /></p><p></p><p>基础技术架构层面，突发的流量峰值和网络稳定性，是另一大挑战。尤其在大型活动、紧急通知或者促销营销期间，SMS 网关会遭遇剧烈的流量波动。为了保障服务的稳定性和响应速度，构建一个具备高弹性、高性能的 SMS 网关成为应对未来不断变化业务需求的当务之急。</p><p></p><p>对此，<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5819">阿里云云通信架构师张松然</a>"将在 ArchSummit 深圳分享，如何通过最新的技术和架构策略，打造一个能够自适应流量变化、保障消息准确无误送达的高弹性、高性能 SMS 网关。详细阐述使用云原生技术、微服务架构以及自动化扩缩容策略来构建 SMS 网关的实战过程，展示在实际突发流量情况下，如何通过优化资源管理和调度策略，有效提高系统的吞吐量和可用性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f25324f4b00e4b0bebfc3f87744aab3b.webp" /></p><p></p><p>当然，所有生产力的提高，除了新工具的加持之外，还需要与之相匹配的技能提升和企业赋能，包括开发者、架构师等在内的技术从业者，都面临着全新挑战。如何适应时代、顺势而为，并且利用大模型将软件系统带入下一个发展阶段？</p><p></p><p>阿里云 CIO 产品线首席架构师黄永法将在 ArchSummit 深圳分享<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5850">《架构师在 AI 浪潮下的 4 个关键可迁移能力及提升技巧》</a>"：首先，讨论在 AI 时代架构师面临的挑战；其次，对架构师细分赛道及能力模型进行分析，得出架构师最需要关注的 4 个可迁移能力；最后，还将基于个人的实战经验，给出可操作的练习方法和技巧，希望能够帮助大家提升自我的竞争力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/875d23b6e7094fc5166269838518b9dc.webp" /></p><p></p><p>除此之外，为了让大家更进一步了解大语言模型的从零到一的训练过程，以及大语言模型的训练原理和最佳实践，阿里云资深技术专家李永还将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5884">《超大规模 LLM 模型在 PAI 平台的最佳实践》</a>"的议题分享，主要介绍超大规模 LLM 模型从数据链路到训练优化的整个链路的最佳实践，包括数据清洗、调度、通信优化、训练、稳定性、RLHF 等方面的内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/04143c093c26cd8fb5deeda9b6e240aa.webp" /></p><p></p><p>除了阿里的众多优秀讲师之外，我们也邀请了（以下排名不分先后）腾讯、百度、网易、字节跳动 / 火山引擎等互联网技术大厂， vivo、知乎、高德地图、Uber 、蚂蚁集团、eBay、货拉拉、快手、哔哩哔哩、携程等头部互联网企业，以及 CNCF、Thoughtworks、顺丰集团、美的集团、鸿海科技集团（富士康母公司）、宁德核电、广发证券、微众银行、众安银行、天弘基金等众多机构和企业的专家共同探讨生成式 AI 技术对于企业未来架构的影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab93ea8c01a10a6f82e68abb2f2220a1.webp" /></p><p></p><p>目前，ArchSummit 深圳大会议程已经上线，并将持续更新，感兴趣的同学请点击链接锁定大会官网查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YTiaW03CML94ZBlG7wVj</id>
            <title>李开复：不参与“价格战”、模型盲测国内第一欢迎PK</title>
            <link>https://www.infoq.cn/article/YTiaW03CML94ZBlG7wVj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YTiaW03CML94ZBlG7wVj</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 06:17:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 模型表现, LMSYS, 大模型企业, 测评结果
<br>
<br>
总结: 李开复在分享会上表示，零一万物的模型表现超过其他模型，欢迎友商来LMSYS打擂台。最新排名显示，Yi-Large在总榜排名世界第七，中国大模型中第一，与GPT4o并列第一。LMSYS的盲测竞技场成为大模型金标准，结果可信度高。Yi-Large在编程能力、长提问、艰难提示词等方面表现出色。零一万物将继续坚持Scaling Law，致力做到中国最好的模型。 </div>
                        <hr>
                    
                    <p>“我们的模型表现超过了其他模型，欢迎不认同的友商来LMSYS打擂台，证明我是错的。但在那发生之前，我们会继续说我们是最好的模型。”李开复在5月21日的分享会上说道。</p><p>&nbsp;</p><p>李开复的底气来自Yi-Large一直以来不错的测评表现。而最近的5月20日，在 LMSYS 盲测竞技场最新排名中，零一万物的最新千亿参数模型 Yi-Large 总榜排名世界第七，中国大模型中第一，已经超过Llama-3-70B、Claude 3 Sonnet，中文榜更是与GPT4o 并列第一。</p><p>&nbsp;</p><p>零一万物也因此成为总榜上唯一一个自家模型进入排名前十的中国大模型企业。在总榜上，GPT 系列占了前十位的四个名额。以机构排序，零一万物 01.AI 仅次于 OpenAI、Google、Anthropic，正式进入国际顶级大模型企业阵营。</p><p></p><h2>榜单表现</h2><p></p><p>&nbsp;</p><p>让零一万物振奋的原因是LMSYS是大模型金标准，都是第三方匿名，而且每个模型都有数万用户评估，结果可信度非常高。OpenAI的Sam Altman和 Google CTO Jeff Dean都在最近的模型发布中引用了该测试结果。</p><p>&nbsp;</p><p>为了提高 Chatbot Arena 查询的整体质量，LMSYS实施了重复数据删除机制，并出具了去除冗余查询后的榜单。这个新机制旨在消除过度冗余的用户提示，如过度重复的“你好”。这类冗余提示可能会影响排行榜的准确性。LMSYS公开表示，去除冗余查询后的榜单将在后续成为默认榜单。</p><p>&nbsp;</p><p>在去除冗余查询后的总榜中， Yi-Large的Elo得分更进一步，与Claude 3 Opus、GPT-4-0125-preview并列第四。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/cef834e16496d270dd7f8f56d22800b4.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>国内大模型厂商中，智谱GLM4、阿里Qwen Max、Qwen 1.5、零一万物Yi-Large、Yi-34B-chat 此次都有参与盲测。在总榜之外，LMSYS 的语言类别上新增了英语、中文、法文三种语言评测，开始注重全球大模型的多样性。Yi-Large的中文语言分榜上拔得头筹，与 OpenAI GPT-4o 并列第一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20636ff6cd80ae579bdbc48fd51397bc.png" /></p><p>&nbsp;</p><p>在分类排行榜中，编程能力、长提问及最新推出的 “艰难提示词” 的三个评测是LMSYS所给出的针对性榜单，以专业性与高难度著称，可称作大模型“最烧脑”的公开盲测。</p><p>&nbsp;</p><p>在编程能力（Coding）排行榜上，Yi-Large 的Elo分数超过Anthropic 当家旗舰模型 Claude 3 Opus，仅低于GPT-4o，与GPT-4-Turbo、GPT-4并列第二。长提问（Longer Query）榜单上，Yi-Large 同样位列全球第二，与GPT-4-Turbo、GPT-4、Claude 3 Opus并列。</p><p>&nbsp;</p><p>艰难提示词（Hard Prompts）则是LMSYS为了响应社区要求，新增的排行榜类别。这一类别包含来自 Arena 的用户提交的提示，这些提示则经过专门设计，更加复杂、要求更高且更加严格。LMSYS认为，这类提示能够测试最新语言模型面临挑战性任务时的性能。在这一榜单上，Yi-Large 处理艰难提示的能力也得到印证，与GPT-4-Turbo、GPT-4、Claude 3 Opus并列第二。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/822430c3982eb0394c0c5e8df0ab6789.jpeg" /></p><p></p><p>测评结果：Coding、Longer Query、Hard Prompts</p><p>&nbsp;</p><p>在此之前，各种静态榜单几乎成为厂商必争的地方。在零一万物模型训练负责人黄文灏看来，所谓打榜主要是厂商要把模型某些单一能力做提升，但比较的时候大家可能并不了解，会带来一些bias。LMSYS提供了一种更接近于用户真实场景的评测方式，所以可以作为一个更好的衡量标准。</p><p></p><h2>李开复：不会对标“价格战”</h2><p></p><p>&nbsp;</p><p>用好的模型，贵不贵？当前，Yi-Large API的定价是0.02元/千tokens，大概是GPT-4&nbsp;Turbo成本和定价的三分之一。</p><p>&nbsp;</p><p>成本问题其实是零一万物一直以来就在关注的。“在大模型时代，模型训练和推理成本构成了每一个创业公司必须要面临的增长陷阱。”李开复曾说道。</p><p>&nbsp;</p><p>“我们关注到最近降价的现象，我认为我们的定价还是非常合理的，而且我们也在花很大精力希望它能再降下来。”李开复表示，整个行业每年降低10倍推理成本是可以期待的，而且也必然发生的，以这个角度看，现在的降价对整个行业来说就是一个好消息。</p><p>&nbsp;</p><p>但对于大模型公司，李开复认为，国内常看到ofo式的疯狂降价、双输的打法，大模型公司不会这么不理智，因为技术还是最重要的，如果技术不行，纯粹靠贴钱、赔钱做生意是行不通的。</p><p>&nbsp;</p><p>李开复以万知为例介绍到，零一万物内部也纠结过用 Yi-Medium，中尺寸模型有成本优势，但是大尺寸模型更有泛化和推理能力优势。考虑到万知用户也包括海外用户，还是需要最强的推理能力，因此团队最终选择了千亿参数的 Yi-Large。</p><p>&nbsp;</p><p>“虽然这并没有达到TC-PMF、还不能赚钱，但是技术的需求是不可妥协。推出之后，模型和Infra团队就一起快速把钱降下来。”李开复说道。</p><p>&nbsp;</p><p>对于当前的大模型价格战，李开复明确表示不会对标这样的（市场）定价。“如果中国市场就是这么卷，大家宁可赔光、通输也不让你赢，那我们就走外国市场。”</p><p></p><h2>“最小到最大的模型，做到中国最好”</h2><p></p><p>&nbsp;</p><p>在做大模型方面，零一万物将继续坚持 Scaling Law。从最小的6B到34B，到现在的千亿模型，还有训练中的万亿 MoE，零一万物技术团队明显看到模型性能随着参数量的增大，智能水平也在显著上升，Scaling Law给AGI指明了一个方向。</p><p>&nbsp;</p><p>以大模型为代表的就是大规模机器学习，需要过大量的算力做大量的实验来得到结论，同时需要算法和Infra做联合优化。</p><p>&nbsp;</p><p>在Scale up过程中，最能够高效使用算力的通用结构一般会获得较大成功。在模型结构上加了各种各样的prior（先验知识）、去调优可以获得更好效果，但这些prior也是约束条件，对模型效果产生影响。零一万物发现，最简单的模型就是最高效的，重要的是怎么去用好计算能力，而给定算力条件下的智能水平，最重要的是数据的质量和使用数据的效率、计算效率。</p><p>&nbsp;</p><p>黄文灏表示，零一万物需要算法、Infra和工程三位一体的人才，但这样的人在国内并不是很多。大模型研发中，人才的作用被放大，比如算法团队不需要特别多的人，一般是10～20人，但是他们后面是几万张卡，这些人的能力就被几万张卡放大了很多。</p><p>&nbsp;</p><p>目前，零一万物的系列大模型参数刚迈入千亿行列，但已经可以与GPT-4、Gemini 1.5 Pro等万亿级别的超大参数规模模型扳手腕。</p><p>&nbsp;</p><p>在Chatbot Arena测评的44款模型中，GPT-4o在最新的Elo评分中以1287分高居榜首，GPT-4-Turbo、Gemini 1 5 Pro、Claude 3 0pus、Yi-Large等模型则以1240左右的评分位居第二梯队；其后的Bard (Gemini Pro)、Llama-3-70b-Instruct、Claude 3 sonnet的成绩则断崖式下滑至1200分左右。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1f6adfd9007e055d7e0726910216d1a9.png" /></p><p></p><p>&nbsp;“我们的计划是从最小到最大的模型都能够做到中国最好。”李开复表示。一方面，根据 scaling law，越大尺寸的模型约有可能达到AGI；另一方面，小一些的模型也有各种应用机会。因此，零一万物的打法是“一个都不放过”，并且在每一个潜在尺寸上做到性能最高、推理成本最低。</p><p>&nbsp;</p><p>不过另一个现实是，零一万物GPU存量只有Google、Microsoft的5%，但李开复认为这并不代表企业就没有机会。</p><p>&nbsp;</p><p>“能用同样一张卡挤出更多的价值，这是今天我们能够达到这些成果的重要原因之一。”李开复说道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8jTyOe3qL4URASiQsjbz</id>
            <title>AI时代下的金融科技展望：发展中的问题只能用发展来解决</title>
            <link>https://www.infoq.cn/article/8jTyOe3qL4URASiQsjbz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8jTyOe3qL4URASiQsjbz</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 04:06:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能技术, 金融科技领域, AI应用, 银行业
<br>
<br>
总结: 人工智能技术的快速发展推动着金融科技领域的革新，AI在银行业带来新的增长点，但也面临挑战和疑虑。银行业需要结合统计学方法和推理方法，利用生成式AI实现更精细化的客户定位、风险评估和合规性管理，以创造更大的商业价值。银行业在应用AI技术时需要谨慎，确保服务和产品的准确性和合规性。 </div>
                        <hr>
                    
                    <p>人工智能技术的快速发展正不断地推动着金融科技领域的革新。这一变革不仅仅关乎技术的演进，更在于它如何成为推动全球经济增长的新动力。在日前举办的“人工智能 X 金融科技创新大会”上，汇丰科技财富管理与个人银行的全球首席架构师夏勇博士就这一趋势<a href="https://www.infoq.cn/video/3qt0o3y04xKgJuoKk4mT?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">分享了他的见解</a>"。他认为，AI 的确会给金融行业带来新的增长点，但同时 AI 在银行业的应用仍存在不少疑虑和挑战，无论如何，发展中产生的问题只能用发展来解决。</p><p></p><p>以下为夏勇博士的演讲实录，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><h2>AI 驱动下，金融行业迎来新的增长点</h2><p></p><p></p><p>AI 热潮爆发时，作为银行从业者，我们首先关注的就是 AI 可能带来哪些经济增长。在过去几年里我们能够看到，如传统的高级分析、传统机器学习和深度学习等数据分析方法等已经为社会带来了巨大影响。</p><p></p><p>根据麦肯锡的调查，这些技术已为全球经济带来 11 万亿到 17 万亿美元的增长，而生成式 AI 可能还会额外带来 2.6 万亿至 4.4 万亿美元的增长，这相当于一个英国的 GDP，再具体一点也就是中国 GDP 的五分之一。若生成式 AI 的影响与传统技术的影响相叠加，最终会带来 17.1 万亿到 25.6 万亿的增长，接近美国一年的 GDP。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e82054ba0d0dce54e037f5da6ab24061.png" /></p><p></p><p>其次，我们还关心为业务部门开发的这些产品 IT 系统的建设。目前，我负责汇丰财富管理和零售银行的全球架构工作，这方面业务在全球的历史遗留应用大约有几千个，其中一些核心系统的建设可追溯至 60 至 70 年前，甚至更早，其中有一些程序还是用汇编语言编写的。这种语言在现在的大学课堂上都已不常见，因此其维护和更新是一项巨大的工程。然而，从另一方面来说，这些遗留系统中也沉淀着丰富的银行业务知识和多年的行业经验。</p><p></p><p>我们的任务就是将这些传统代码中蕴含的知识提取出来，并以此来理解金融业的历史发展。基于这些历史数据和知识，我们可以探索银行业务的未来机遇，寻求为客户提供更优质服务的可行方式。这对我们而言，特别是对我们银行内部做创新工作的同事们来说，是一个值得关注的议题。</p><p></p><p>同时，这些语言和大模型有所不同，它们其实是一些私域数据。如何将大模型的通用知识与我们银行的这些私域知识结合起来，以推动金融领域未来发展？这是一个我们觉得很有意思、但也非常必要的工作。</p><p>此外，随着人工智能技术的发展，我们现在能够提供比传统人工客服更高效的智能客服，这无疑将成为我们未来工作的一个重要方向。在市场营销、销售等其他领域，我们也将继续探索和应用新技术，以提升整体的业务效能。</p><p></p><p>作为一个受知识和技术驱动的行业，银行业的营销、客户运营等业务都已从先前存在的 AI 应用中获得了显著的好处。而生成式 AI 应用则可以提供额外的好处，特别是因为文本模态在诸如法规和编程语言等领域非常普遍，并且银行业面向客户，拥有许多 B2C 和小型企业客户。行业特点使银行业很适合集成生成式人工智能应用：</p><p>持续的数字化努力以及传统的 IT 系统。银行已经在技术方面投资了数十年，积累了大量的技术债务，以及一个孤立和复杂的 IT 架构。面向大客户的工作力量。银行业依赖于大量的服务代表，如呼叫中心代理商和财富管理金融顾问。严格的监管环境。作为一个受到严格监管的行业，银行业有大量的风险、合规和法律需求。知识工作者领域。生成式人工智能的影响可能贯穿整个组织，帮助所有员工编写电子邮件、创建业务演示文稿和其他任务。</p><p></p><p>下图展示了我们能预见到的 AI 在银行业的潜在应用领域与职能，包括投资银行、资产管理、对公业务、财富管理、零售银行等等。</p><p><img src="https://static001.geekbang.org/infoq/d6/d64614fcca8f974d837333da370b5ec5.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e75fae71af91c386e130107261f03a69.png" /></p><p></p><p>虽然这些应用的前景看起来很光明，但我不得不提的是，金融业、银行业都是重监管的部门，因为它们涉及到公众的薪资和养老金等重要资产。银行从业者肩负着重大的社会责任，这就要求我们在使用新技术时必须要秉持十分谨慎的态度。无论是对国家而言，还是对银行业而言、对银行业从业者而言，我们自身的信用都是最重要的。</p><p></p><p>AI 大语言模型的应用其实有很多技术路线，但本质上我们从数学角度来看，它其实只有两种：统计学方法和推理方法，也就是数字驱动和知识驱动。在我们的实际应用中，一定是将这两种方法相结合使用的。</p><p></p><p>我们今天能够看到，基于统计学方法的大型语言模型在金融业，尤其是银行业中的应用引发了一系列问题。如同刚刚提到的，银行承载着客户的工资、养老保险和退休基金等重要资产，这些业务对精确性的要求极高，我们必须确保所提供的服务和产品具备极高的准确性。因此，在开发和应用 AI 技术时，除了统计学模型和大型语言模型所提供的知识以外，我们还必须结合基于推理方法的知识驱动。</p><p></p><p>银行业在利用生成式 AI 的路上既面临机遇又要应对挑战：</p><p>借助生成式 AI，银行可以实现更精细化的客户定位、更准确的风险评估和更高效的合规性管理，从而创造更大的商业价值。银行业可以通过使用生成式 AI 技术，提高数据的利用效率和质量，从而提高风险管理和合规性管理的效率。生成式 AI 可以为银行提供更准确的预测和决策支持，从而帮助银行提高效率和降低成本。银行需要积极采取措施，以确保生成式 AI 的合规性和透明度，同时注意潜在的风险和挑战。银行需要积极推动生成式 AI 的应用，以实现数字化转型和业务创新，从而保持竞争优势。</p><p></p><h2>汇丰银行 AI 应用案例</h2><p></p><p></p><p>在近两年的“百模大战”中，我比较关注大模型在银行业，特别是 IT 方面的使用。从技术背景上来谈，大模型主要在以下几个方面发力：</p><p>海量数据训练：学习词汇、语法、句法结构等语言要素，具备生成流畅、连贯文本的能力；多领域知识：能够处理来自不同领域的问题；丰富的语言表示：将输入转化为高维向量表示，捕捉了语义、句法、语境等多个层面的信息；上下文理解：理解上下文并生成连贯的回复，能够更好地回应复杂的问题；可定制性 / 可持续学习：垂直领域数据微调。</p><p></p><p>但包括汇丰在内，目前全球范围内银行业直接用于 client facing 的大语言模型项目并没有真正落地。这主要受到了两个因素的影响：一是整个银行业的谨慎态度，二是监管因素。</p><p></p><p>在 client facing 落地之前，我们先开始了 staff facing（面向员工）的大模型研发应用。同时由于 IT 部门员工最熟悉大语言模型，故这一技术最先对他们开放。</p><p></p><p>虽然我们在文本方面看到了大语言模型的一些令人惊喜的能力，但我们今天的大语言模型仍然需要与人协作，基本上还是一个“Copilot”，而非“Autopilot”。因此面向员工的大模型应用的起始点就是，在整个软件开发的生命周期内，利用大语言模型帮助提高银行业务。</p><p></p><p>银行积累了大量的技术债务，并拥有分散且复杂的 IT 架构。我们目前在用 AI 辅助经济 API 软件的开发，让 AI 帮助事故处理等一系列 IT Operation 方面的工作。我们始终将大型语言模型视为与开发者协同工作的辅助工具，而非完全替代人类工作的自动化工具。</p><p></p><p>如果一家创业公司的新系统上线后因使用人数超出预期而崩溃了，舆论会持宽容的态度，反而认为这是新系统太受欢迎、是一件好事；而如果银行系统崩溃，则一定会遭受全球媒体的负面报道。这种监督压力使我们在用 GAI 时不会直接在面向客户的领域上线新项目，但 GAI 可以用于提高系统的鲁棒性，或在需求变更时提高帮助。</p><p></p><p>当系统运行出现事故， GAI 还可以自动推测软件 bug，我们也形成了一套 AI 驱动的事故管理系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c88da6ca9212e0c723d041160c382050.png" /></p><p></p><p>此外，在银行业务方面，随着 GAI 技术的发展，我们的工作必须“道高一尺魔高一丈”。在风险合规方面，曾经的人脸识别、活体识别技术几乎已被 GAI 打败，这些技术必须跟随 GAI 一同进步，就比如现在的人脸识别环节都要求用户摇摇头、眨眨眼。</p><p></p><p>在下图呈现的更多银行业 AI 应用场景中，知识产权管理是一个值得关注的议题。当前，多模态模型的发展引发了关于模型训练中使用语料的知识产权问题的讨论。这些训练材料是否受知识产权保护？我们能否使用这些材料？根据现有的知识产权法律体系，大型语言模型在知识产权方面的合法性存在疑问，这方面的法律需要与时俱进。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff42b4651d5bd12b41a996ac515c1bd0.png" /></p><p></p><h2>未来银行 AI 应用走向何方</h2><p></p><p></p><p>尽管最新技术如使用 AI Agent 已取得显著进展，但这些技术的实际效果与面向消费者的（to C）应用相比仍有差距，特别是在处理私域数据方面。对于小规模训练样本的数据，AI 技术所能发挥的作用相对有限。此外，AI 技术的应用主要集中在银行后台 staff facing 的员工使用领域。这是 AI 技术需要进步的方面。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa16f44ae377f0bfe1338c8de9525392.png" /></p><p></p><p>刚刚提到的这些应用场景中，我们并没有探索全部场景并取得较好效果，下图标红的应用是我们尝试后觉得比较有信心、能够很快有所提高的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7a9faed83d1388ce5b99f2e1bcadd9f.png" /></p><p></p><p>在谈到 AI 特别是 GAI 时，有两种态度：第一，一种没必要的悲观情绪；第二，过于乐观的清晰。对于这件事，我认为既不需要觉得 AI 可以完全替代程序员了，也不用走另一个极端、期待 AI 带来非常大的提升。我们通过自己的实验发现，AI 能够提高 20% 到 30% 的效率。大家可能觉得这个数字很小，但比如我负责的架构部门有几千个程序、在全球有大规模的 IT 人员，如果都能提高 20%-30% 的效率，就将积累出很可观的效果。</p><p></p><p>最后我想谈一谈银行业 AI 应用面临的挑战。作为银行业从业者，我们一直在谈风险，银行业务的本质其实就是 Risk Management（风险管理），关乎到我们怎么看 credit（信用）、怎么看 liquid（流动性）风险的问题。我们在实践中发现，发展中的问题只能用发展来解决，回避肯定不是好方法。</p><p></p><p>今天我们在私域数据、强监管领域里看见了各种各样的风险挑战，但 AI 带来的技术进步同样不容忽视。在当前情况下，我们只能用发展的、更先进的技术来解决发展中产生的疑惑，包括歧视、数据安全、诈骗、风险监管等等方面。</p><p></p><p>那么随着 AI 技术不断发展，我们所有人都会被 AI 取代吗？传统岗位会不会全部不复存在？我认为“工作”本来就不一定是人必不可少的一个部分，如果 AI 技术真的发展到能够完全取代人的工作的地步，也未尝不是一件好事，我们可以做更多工作以外的事。</p><p></p><p>我相信无论是歧视、数据安全、诈骗、风险监管这些具体的问题，还是我们自身生存、工作这些更广泛的问题，都会在有了新技术后找到其他意义。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1f87OE5DUEa7Dkm5r9vb</id>
            <title>AI大模型如何在各行业跑通业务闭环？</title>
            <link>https://www.infoq.cn/article/1f87OE5DUEa7Dkm5r9vb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1f87OE5DUEa7Dkm5r9vb</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 May 2024 02:16:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 主持, 嘉宾, 大模型, AI应用
<br>
<br>
总结: 本文讨论了大模型在AI应用中的重要性和挑战，以及在不同行业中的应用现状和探索。通过对话展示了在智能风控、金融、物流和供应链等领域中大模型技术的应用和发展。 </div>
                        <hr>
                    
                    <p>主持&nbsp;|&nbsp;高磊</p><p>嘉宾&nbsp;|&nbsp;平野，贾志鹏</p><p>&nbsp;</p><p>诸多新技术范式的出现正在重塑AI大模型应用的落地路径，大模型在推动企业向全面数智化转型的同时，也在对以往的AI应用开发与运维流程产生深远影响。当大模型爆红之初惊喜又兴奋的心情平复下来时，AI大模型落地行业场景时的诸多挑战逐一浮出水面。</p><p>&nbsp;</p><p>各行各业如何面对大模型应用探索中的新挑战？对于金融行业等数据密集、对产出结果精确度有很高要求的产业，或是要求严谨专业的物流与供应链领域而言，大模型应用如何平稳走进业务场景？下一步，AI大模型该如何结合行业特点、满足行业要求，向行业垂直领域大模型发展？&nbsp;</p><p>&nbsp;</p><p>在日前的《超级连麦.&nbsp;数智大脑》x<a href="https://archsummit.infoq.cn/2024/shenzhen/">&nbsp;ArchSummit&nbsp;</a>"直播节目上，顺丰科技运筹优化算法总工程师高磊、天弘基金人工智能部负责人平野、Fabarta高级技术专家贾志鹏就这些问题展开了深度探讨。</p><p>&nbsp;</p><p>以下内容根据对话整理，篇幅有删减，<a href="https://www.infoq.cn/video/mmXGof9LkcI06AJVC6XD">点击链接</a>"可观看直播回放：<a href="https://www.infoq.cn/video/mmXGof9LkcI06AJVC6XD">https://www.infoq.cn/video/mmXGof9LkcI06AJVC6XD</a>"</p><p></p><p>ArchSummit 深圳大会议程已经上线，感兴趣的同学请锁定大会官网：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"。</p><p></p><h3>大模型应用现状简介</h3><p></p><p></p><h5>高磊：平野老师曾经在支付宝第五代智能风控引擎Alpharisk的开发中发挥了重要作用，能否分享一些AI和大模型在智能风控领域的技术创新和实践经验？目前，天弘基金在大模型领域又有哪些探索？</h5><p></p><p>&nbsp;</p><p>平野：在支付宝支付风控部门任职期间，我参与了风控引擎架构的转型建设。最初，风控引擎的设计主要依赖于策略，当时大约70%以上的风险防控措施都是基于策略的。制定策略时先由专业分析师根据具体场景构建特征，再通过这些特征组合成策略进行风险管理。</p><p>&nbsp;</p><p>随着时间的推移，支付宝开始探索以AI模型替代人工的策略，在2017年至2019年这一早期阶段，尽管全球成功的AI风控案例并不多，我们仍坚持在AI风控领域不断探索和创新。风控引擎架构从传统的策略开始，逐渐过渡到一些机器学习模型，最终逐步引入了更高级的技术，如图神经网络和深度神经网络。我们还设计了高性能的分布式支付决策链路，显著提升了风控效率。</p><p>&nbsp;</p><p>在AI应用方面，我们进行了多项独特创新，包括基于MOE的多任务学习框架，并针对具体场景进行了创新。此外，我们还开发了图算法和可信网络等技术，这些技术在支付宝的风险防控中发挥了重要作用。</p><p>&nbsp;</p><p>Alpharisk作为支付宝风控引擎的核心组件，经过数次迭代，已发展至第五代，其智能化水平显著提升。从风险感知、决策到结果演化，这一系列决策过程现在主要都由模型自动完成。</p><p></p><h5>高磊：阿里巴巴开发的大模型通义千问非常强大，支付宝在这方面是否有过合作？</h5><p></p><p>&nbsp;</p><p>平野：在天弘基金大模型项目进行的过程中，我们曾经考虑过与第三方厂商合作，或者直接调用一些行业领先的大模型API来实现我们的目标。然而，我们很快意识到，金融业务场景对数据的准确性和专业性有着极高的要求。这一行业特点使我们不得不面临两点问题：</p><p>&nbsp;&nbsp;首先，现有通用大模型，包括去年早期发布的ChatGPT&nbsp;3.5，它们在数据实质内容方面的完善程度并没有达到我们预期；&nbsp;&nbsp;其次，为使大模型在特定场景下的功能更智能化，就要求它能深入理解我们的业务场景，并专门学习不同的业务目标。但目前的通用大模型只能提供泛化的回答，并不能提供精准和实质性的内容。</p><p>基于这些考虑，我们决定主要通过自主研发来完成大模型项目，以更好地满足我们的业务需求。</p><p></p><h5>高磊：贾老师在Fabarta担任高级技术专家，曾在IBM、阿里云、HSBC等公司从事金融、制造和汽车等行业的业务解决方案咨询与实施工作，能不能从您的角度介绍一下，大模型技术在这些不同领域的应用现状，主要能解决哪些业务问题？不同行业之间存在哪些独特挑战？</h5><p></p><p>&nbsp;</p><p>贾志鹏：在公司成立之后，我们为多家企业提供了解决方案服务。这些服务包括针对不同行业设计定制化的解决方案，如金融、制造和医疗等行业。在这一过程中，我们注意到不同企业对大型AI模型（大模型）抱有多种期待。根据我们的经验，可以将这些期待大致分为以下三类。</p><p>现有系统优化：一些企业在大模型出现之前已经在运营或建设自己的系统，如文档管理、知识库和知识图谱等。这些企业希望通过大模型技术来丰富和优化现有系统，无论是优化数据接入还是提升最终产出。生成式能力应用：大模型具有强大的生成式能力，企业希望探索这些能力在特定场景下的应用。例如，在前期构建知识库后，企业可能想要实现企业内部文档的问答系统，挖掘现有文档价值、解析问题并提供答案。此外，企业还希望通过大模型自然语言表达来实现自动发掘企业内数据，如报表和数据，这在BI（商业智能）场景中尤为重要。智能决策探索：近期业界在探索如何通过大模型实现智能决策，这涉及到使用AI&nbsp;Agent技术将不同的业务场景与大模型、甚至企业原有的小模型结合起来，共同解决特定问题，以实现智能决策。</p><p>&nbsp;</p><p>高磊：我在顺丰集团负责智慧供应链的建设工作。在公司内部，我们正在探索大模型技术在物流和供应链领域的应用。我们正在构建基于开源模型的“丰语”大模型，该模型专注于物流供应链领域的专业知识。依托此模型，我们尝试了多种应用，主要分为三大领域：</p><p>售前售后服务：在售前，我们尝试为快递员配备智能助手，帮助他们即时回答客户的收寄物品问题，而不必依赖个人经验或咨询同事；在售后，我们将大模型技术应用于智能客服系统，自动生成对话摘要，减轻客服人员的工作量，并利用大模型进行服务质检和客户声音洞察，帮助响应客户共性问题并提高服务规范性。运营知识管理：我们尝试使用大模型技术自动从官方网站抽取国际通关的标准信息，并将其结构化，以便管理和应用。此外，我们还利用大模型优化快递下单时托寄物分类和描述的流程，以应对物品种类繁多带来的挑战。BI和智慧办公：在BI方面，我们为业务提供分析助手，帮助客户洞察和分析供应链表现、有无异常和根本原因，并提出优化建议；在智慧办公场景中，我们正在构建企业内部知识中台，来回答人力资源、财务、IT等方面的问题，并在内部即时通讯系统中集成大模型，自动生成会议纪要和聊天摘要等。</p><p></p><h3>大模型进入各行各业核心业务，难在何处</h3><p></p><p></p><h5>高磊：金融行业对前沿技术的接纳度是比较高的，不仅场景丰富、数据丰富，并且技术基础也较为完善，技术投入能力较好。根据两位的观察，目前大模型在金融行业的落地进展如何？能够满足最开始业界的预期吗？其中的阻碍和挑战主要来自哪些层面？</h5><p></p><p>&nbsp;</p><p>贾志鹏：目前在金融行业中，将图智能技术与大模型结合的应用非常广泛。金融业务的复杂性导致交付过程中会涉及多个方面，包括风险控制、合规性、营销以及企业知识库和运维场景。尽管这些场景可以被统称为金融行业，但在不同场景下的具体需求有所不同。</p><p>&nbsp;</p><p>对于风险控制，支付宝的风控手段如Alpharisk，已经非常成熟。我们目前的重点是利用新技术，如图智能技术等，结合大模型，以更深层次、更多样化的手段来进一步完善风控体系。例如，在风控领域，除了传统的风险阻断措施外，还需要向客户提供合理的解释，这就需要模型的输出结果具有权威性，并且能够被把握和解释。</p><p>&nbsp;</p><p>在合规性方面，随着金融监管的加强，金融机构需要遵守国家规定（外规）和内部规定（内规）。这要求我们将这些规定融合，并从中提取关键知识点。同时，还需处理包括司法判决、行政处罚在内的文档，并关联提取这些数据。这涉及到整合包括文本数据、企业内部交易数据和客户数据等多元异构数据的问题，是当前金融行业面临的重大挑战。</p><p></p><p>平野：金融行业在大模型技术的发展和应用上，经历了几个明显的阶段。</p><p>初期兴奋阶段：随着ChatGPT的诞生，特别是在去年早期，&nbsp;ChatGPT&nbsp;3.5在短时间内迅速吸引了2亿用户，这让人们产生了AI能够解决一切问题的错觉。许多公司开始尝试将大型基础模型应用于金融行业的各种场景，如客服、合规风控、审核和用户体验等。质疑阶段：在大模型技术落地应用的过程中，人们开始意识到并不是所有问题都能通过一个模型解决，特别是金融行业对合规性的要求非常高。大模型有时会生成不准确或误导性的信息，这对于用户和投资者而言是不可接受的。这导致了对大模型实际效用的质疑。理智阶段：如今，金融行业对大模型的应用更为理智和审慎。技术发展开始向真正可落地的技术转向，比如基于大模型衍生出的新型架构，如RAG（Retrieval-Augmented&nbsp;Generation）架构和AI&nbsp;Agent架构。这些架构除了能够增加模型回答的实时性，还让大模型更接近人类的思考方式，能够将复杂问题分解为多个子任务，并利用大模型的泛化能力进行准确的意图理解和回答。</p><p>&nbsp;</p><p>除了传统的应用场景，金融行业还在探索如何将大模型应用于投资决策辅助等新领域。例如，Bloomberg（彭博）等公司发布的金融大模型，以及FinChat、FinGPT等，都在尝试利用大模型技术进行投资研究。天弘基金也在尝试将大模型应用于投资研究方向。</p><p></p><h5>高磊：大模型在金融领域除了理解摘要方面的工作外，还有没有一些创新的想法？</h5><p></p><p>&nbsp;</p><p>平野：在金融领域，大模型的应用经常被优先考虑用于客户服务和智能问答，这与ChatGPT以对话框形式出现时的自然联想相吻合。</p><p>&nbsp;</p><p>在天弘基金，我们探索了一条不同的路径，特别是在投资研究方面。我们开发了一款名为“弘小助”的内部产品，专注于投研领域。弘小助的主要特色在于采用了RAG架构，进行实体化的检索。</p><p>&nbsp;</p><p>在行业研究方面，我们进行了一项特别创新的工作，即将研究员的思维模式融入大模型中。通过COT（Chain&nbsp;of&nbsp;Thought）这种思维模式，我们结合了研究员的思维方式和大模型的能力，创新了一项名为“COM”的技术。这个过程将“thought”转化为我们的“mind”，使我们能够将意图拆解为多个子意图，并进行更深层次的分析。例如，如果我们要了解今天光伏行业的投资机会，通用大模型可能会提供一个一般性的回答，这可能不是研究人员真正想要的深层信息。通过将研究思维整合进大模型，并进行创新性的工作，我们可以在一定程度上为研究人员提供辅助工作，帮助他们更深入地理解市场和投资机会。</p><p></p><h5>高磊：请问平野老师，大模型来做投研，个人投资者有没有机会使用？或者是否能对其有所帮助？应该如何使用？</h5><p></p><p>&nbsp;</p><p>平野：投资通常被视为一门艺术，而不仅仅是技术。尽管如此，随着人工智能的快速发展，越来越多原本需要人工完成的思考和分析工作，现在可以由机器来辅助完成。</p><p>&nbsp;</p><p>从技术角度来看，大模型在投资中主要发挥辅助作用。它不会直接告诉投资者应该买入或卖出什么，因为这样的建议可能缺乏可信度。大模型的真正价值在于提供深入的行业分析、资金流向、市场行情、政策变化等信息，帮助投资者更好地理解投资环境。如果投资者对某个特定的投资标的或行业感兴趣，大模型可以提供该行业的现状、资金流向、市场行情等详细信息。对于基金产品，如ETF，大模型可以帮助分析近期的市场异动，包括政策、资金和基本面的变化。此外，大模型还可以分析宏观层面的因素，如美联储的加息决策对投资的潜在影响。通过整合金融领域的数据库，大模型能够提供准确的数据支持，帮助投资者形成自己的投资逻辑链。例如，在光伏行业，大模型可以展示产业链上中下游企业的财务和经营状况，使投资者能够判断企业的优劣和发展前景。</p><p>&nbsp;</p><p>尽管大模型可以提供数据、舆情、投资链路和财务分析等综合信息，并生成总结性摘要，但最终投资决策仍然需要人的参与。投资者应根据大模型提供的信息，结合自己的判断，做出正确概率较大的投资选择。</p><p></p><h5>高磊：平野老师将在ArchSummit深圳大会上分享《AI&nbsp;Agent：超越文本，走向自主决策与交互》话题演讲。那在天弘基金的业务实践中，AI&nbsp;Agent&nbsp;落地的场景包括哪些？在这个过程中面临的主要挑战是什么？您是如何解决这些挑战的？</h5><p></p><p>&nbsp;</p><p>平野：与其他行业相比，金融行业对结果的可靠性和专业性要求极高。当前大模型存在的幻觉问题和知识过时问题，可以通过AI&nbsp;Agent技术有效规避。AI&nbsp;Agent能够为大模型提供私有且可靠的知识注入，同时获取市场实时信息，如指数地图和每日行情等。在各个场景中，大模型被视为一个能够调动各种工具（tools）的智能体，更贴近人类的思考和行为模式，同时比人类更聪明高效。我们调研了多个金融行业业务场景中AI&nbsp;Agent技术的应用效果，包括投研投顾、风控营销、客服合规等。特别是在金融知识问答、研报知识解读、文章精读等场景中，AI&nbsp;Agent技术显示出较大的潜力。此外在销售过程中，AI&nbsp;Agent可以快速生成营销物料以抓住时机，对提高效率有显著帮助。</p><p>&nbsp;</p><p>AI&nbsp;Agent落地过程中面临的挑战：</p><p>低容错率：由于金融行业对时效性和准确率的高要求，模型推理过程必须非常清晰，以确保结果的有效性和可靠性。合规监管要求：金融行业合规监管要求较高，导致许多公司更倾向于本地化部署模型，这增加了自研成本。推理分析的高要求：金融行业对推理分析的要求超过其他行业，因为金融问题往往缺少标准答案，需要用模型进行事实性的推理，这要求技术向特定架构倾斜，并进行强化学习。多模态理解：金融行业涉及多模态信息理解，如财务报表、图表、视频等，有效关联和分析这些信息很困难。</p><p></p><h5>高磊：对于AI&nbsp;Agent技术，您提到了感知和决策的能力是至关重要的，您团队是如何利用深度学习、自然语言处理等技术来提升AI&nbsp;Agent的感知和决策能力的？</h5><p></p><p>&nbsp;</p><p>平野：在AI&nbsp;Agent架构中，感知能力和决策制定构成了其核心。特别是在金融投资领域，市场信息的准确感知对于形成有效决策至关重要。投资经理在做出决策前，必须收集和消化包括研报、国际与国内市场状况、宏观经济、行业动态、公司情况及监管变化在内的大量信息。传统AI模型在处理此类复杂信息存在局限，而大模型的应用为解决这一问题提供了可能：它能够处理信息差异，帮助拆解和解决投资决策中的问题。</p><p>&nbsp;</p><p>我们内部开发的Copilot工具，是一个基于自然语言处理技术的交互式数据查询工具，可以提升市场感知能力。该工具分为三个关键部分：首先，它需要理解并计算海量金融指标；其次，确认所采取的步骤是否正确；最后，根据不同用户角色的需求定制知识库，减少模型需要处理的指标范围。此外，通过RAG技术，模型不仅能召回所需的指标字段，还能根据用户的历史搜索习惯进行智能联想，从而提高查询的准确性。</p><p>&nbsp;</p><p>在决策层面，将用户的自然语言描述转换为可执行的查询，需要模型对金融领域有深刻的理解。这包括基于基座模型进行预训练，输入大量金融数据和特定的金融指标，如收益率、Alpha、Beta等，再训练模型理解这些指标及其应用场景。此外，通过指令对齐和利用相似查询案例的预训练，可以提升模型识别用户查询意图的精准度。在执行层面，大模型根据对场景的理解调用不同的指令，以适应不同场景的需求，从而提供更加精准的结果。</p><p></p><h5>高磊：在多Agent系统的实际应用中，专家们是否有过将其落地到具体场景的经验？当涉及到跨企业间的AI应用交互时，有哪些重要的注意事项需要考虑？具体来说，应该留意哪些关键的事项以确保AI应用交互的有效性和安全性？</h5><p></p><p>&nbsp;</p><p>平野：在天弘基金，我们已经将多Agent技术应用到实际场景中，尤其是在我们提到的“弘小助”产品中。这个产品需要处理来自不同方面的问题，比如某个ETF在特定行业的表现、当天股市的大盘情况、金融宏观层面的总体状况，甚至是更具体的投资模型问题，如Fama-French三因子模型的含义和应用。</p><p>&nbsp;</p><p>这些问题不是单一场景下的通用问题，而是需要多角度分析的复杂问题。我们采用了多Agent技术，将其拆解为三个维度来处理。</p><p>发散性思维：针对抽象的金融问题，如光伏行业的投资时机，研究员需要从多个角度思考和收集信息。我们利用大模型的能力，结合研究员标注的专业图谱进行训练，模拟研究员的思维模式，进行多角度分析。顺序性思维：对于需要逻辑性和时间顺序的任务，如写作，我们使用Agent技术规划整个写作蓝图，然后通过大模型生成基于召回信息的有序内容，确保逻辑性和连贯性。抽象思维：针对具体且复杂的金融综合性问题，如在特定宏观经济条件下的政策影响，我们利用大模型对知识结构的压缩和对指令的全面理解，将复杂问题拆解为多个子问题再分别处理，最终整合出全面的回答。&nbsp;</p><p>&nbsp;</p><p>多Agent技术的应用，不仅提升了我们处理问题的效率，并且通过模拟研究员的思维模式引导了整个分析过程，使我们能够更好地理解和回答复杂的金融问题。通过这种方式，我们能够将大模型的生成能力和传统模型的计算能力结合起来，解决供应链中的一些核心问题，如路径规划和销量预测，这些都是序列生成任务。尽管目前还存在一些挑战，但我们相信，随着技术的发展，大模型技术将在供应链决策中发挥越来越重要的作用。</p><p>&nbsp;</p><p>贾志鹏：我们公司在构建企业知识中台。作为一家研发公司，我们在内部经常会遇到需要确定谁对某个技术领域更熟悉的问题。例如，当我们想要了解谁对引擎的存储层最了解时，需要从多个维度进行评估。首先，我们会查看代码提交记录，通过Git平台的API接入来分析谁在相关领域的代码提交量最多。其次，我们会分析设计文档、分享材料和会议记录等文档资料，以确定谁在这些文档中的记录和贡献最多。此外，我们还会利用内部结构化数据来辅助评估。通过代码提交分析、文档解析和内部数据查询这三个渠道，我们能综合召回结果，并形成一个全面的回答。在回答的过程中，我们不仅提供答案，还强调可追溯性，即清晰展示得出这个答案的过程和依据。例如，如果我们得出结论认为某位同事对引擎最熟悉，我们会展示支持这一结论的证据，包括代码提交量、文档贡献以及组织架构中的相关信息等。这一过程确保了我们的回答不仅准确，而且具有很高的透明度和可信度。</p><p></p><h5>高磊：志鹏老师的公司是做图智能技术的，在金融领域，您认为图智能技术的应用潜力体现在哪些方面？具体落地到企业的业务中，面临的最大挑战又是什么？您的团队是如何解决这些挑战的？</h5><p></p><p>&nbsp;</p><p>贾志鹏：我们公司一直在积极推动图智能技术的实际应用落地。在讨论图技术时，人们通常会想到知识图谱或图神经网络。知识图谱是我们的一个应用领域，它之所以受到重视，是因为它包含了大量经过专家验证和精心运维的确定性知识。尽管知识图谱也有一定的运维成本，但在达到一定规模后，其价值是显而易见的。</p><p>&nbsp;</p><p>但如何构建一个完整和完善的知识图谱是一大挑战，它需要整合来自不同来源的数据，包括结构化数据库、文档提取以及专家经验等。大模型的出现为解决知识图谱构建中的一些基础问题提供了新的可能性，例如利用大模型辅助数据标注、实体提取和实体关系连接等。</p><p>&nbsp;</p><p>除了知识图谱，图技术在其他应用场景中也展现出其潜力，如金融风控和合规领域。这些领域并不需要构建一个完整的知识图谱，而是需要实时生成交易图或关联图谱，以便业务人员能够通过可视化手段探索和分析数据模式。在贷款审查或交易链路探查过程中，图技术可以帮助揭示循环担保、家族式交易或集团内部隐藏关系等问题。这些模式一旦被业务专家通过图技术探索出来，就可以通过技术手段复用，并推广到更全面的数据样本中去发现类似问题。这引出了一个新的挑战：如何节省业务人员在发现问题后编写报告的时间。在反洗钱领域，银行或金融机构需要定期处理大量的排查名单，如何快速应用已发现的模式并生成总结报告是一个亟待解决的问题。</p><p>&nbsp;</p><p>大模型结合图模式可以作为解决这一挑战的工具，帮助生成和加速报告生成过程。在营销领域，尽管我们不是指大规模的互联网或电商营销，银行内部的金融产品、信用卡产品推广，以及银行APP流量和商户推荐等，都可以通过构建图谱并发现关联关系来进行更有效的营销推广。在这个过程中，图神经网络可以发挥作用，虽然它需要大量数据和算力来实现最佳效果。</p><p>&nbsp;</p><p>在图智能技术应用过程中，需要综合考虑不同场景下的因素，这些因素将决定最终的架构决策。数据如何进入图谱是一个关键问题，特别是对于中小企业如何应用大模型能力这一问题，我们非常感兴趣，并一直在实践中帮助客户构建这样的能力。</p><p>&nbsp;</p><p>此外在数据准备阶段，企业需要处理多源数据，包括结构化和非结构化数据，并解决数据的关联和组织问题。我们目前正在进行的“元数据知识化”项目，旨在通过大模型辅助生成标注，并通过图的方式组织数据，从而为后续的数据分析和问答提供清晰的数据关系和可追溯性。这是我们在图智能技术应用中的一个重点投入方向。</p><p></p><h5>高磊：不同行业图谱的构建和生成有哪些共性？</h5><p></p><p>&nbsp;</p><p>贾志鹏：根据个人经验，在金融、制造、医疗这三个方向上，图谱的构建和应用具有一定的共性。首先，知识图谱因其确定性知识的特性，与特定领域紧密相关。在提取知识构建图谱的过程中，我们不依赖通用的方法，而是采用技术框架来组织数据，关键是如何快速有效地将数据整合入图，形成有用的图谱。</p><p>&nbsp;</p><p>目前，我们在数据整合方面积累了几个方向的经验。我们将数据分为结构化、半结构化，并采取自动和半自动的方式入图。半自动入图涉及使用大模型技术预先处理数据，随后采取人工审核和反馈循环进行修正。自动入图则依赖信息化过程中产生的结构化数据，如交易数据或供应链中的供应商信息，这些可以直接映射入图。</p><p>&nbsp;</p><p>除了技术手段，图谱构建还需要理论支持和方法论指导。在实施过程中，我们强调业务目标的中心地位，避免一开始就构建一个过于庞大的图谱。以风控为例，我们会筛选与风控业务相关的信息设计图谱的schema，并裁剪现有数据以形成有效的图谱。如果基础图谱设计得既完善又合理，并且业务方能够清晰理解图谱所表达的内容，那么在使用过程中就能够带来显著的效果。</p><p></p><h5>高磊：志鹏老师，在ArchSummit深圳的演讲中，会提到大模型与图智能技术结合的三种方式，可以先大致给大家介绍一下吗？您认为其中在金融领域的应用前景最广阔的是哪一种方式？为什么？</h5><p></p><p>&nbsp;</p><p>贾志鹏：这一课题的准备工作始于4月份，当时我们提出了三种结合方式，前两种较为常见：</p><p>&nbsp;</p><p>第一种是通过大模型辅助图谱构建，利用大模型从非结构化数据中提取知识，并将其整合到图谱中，随后通过人工审核完成图谱的构建。第二种是大模型增强的图谱使用，例如在风控领域，通过图模式发现数据后，利用大模型生成用户报告。第三种是图增强概念。近一两周，我们注意到微软等公司也发表了相关研究文章，提出了图增强GraphRAG等概念。我们认为，知识图谱的优势在于承载确定性知识，若在大模型问答过程中能够结合图谱进行知识召回，那么所召回的知识将更具权威性和业务价值。通过这种方式，可以提高回答的准确性。</p><p>&nbsp;</p><p>我们认为这三种方式相辅相成，不仅在图谱构建阶段利用大模型，在图谱使用过程中也通过大模型增强其效能，而在大模型召回结果时，图谱起到了关键作用。我们预见，大模型与图智能技术结合后的应用前景广阔，尤其是在决策智能方面。结合图谱的确定性知识与大模型的生成式知识，可以对企业现有的多模态数据进行加工沉淀，形成具有生成能力和可解释性的决策支持。以金融大模型投资为例，个人投资者可以在不同阶段获得理解和辅助，最终做出决策。</p><p>&nbsp;</p><p>我们希望通过图的方式将决策所需的知识串联起来，构建一个确定性的图谱，为用户提供确定性的答案。同时，通过关联方式呈现召回逻辑和回答逻辑，让用户清晰了解决策背后的逻辑，这一逻辑是结合了图谱确定性先验知识的。</p><p>&nbsp;</p><p>高磊：事实上，大模型在金融、风控、营销等多个领域，智能制造背景下的大模型和AI技术展现出巨大的应用潜力。例如，在药物研发领域，DeepMind公司近期在《自然》杂志上发表的研究成果AlphaFold3，展示了AI准确预测蛋白质空间结构及其与化学物质相互作用的能力，这对药物设计具有重大意义，显著降低了实验和筛选候选药物的成本。</p><p>&nbsp;</p><p>众多国内外创业公司正在开发AI辅助的工业设计和服装设计软件或系统。谷歌也曾发表使用强化学习辅助芯片设计的研究。这些新技术，包括大模型和AI技术，在智能制造的设计和研发阶段提供显著帮助。在生产阶段，AI技术，包括大模型，在生产计划制定、排程排产、物流优化、工艺参数智能调优、AR/VR技术应用以及计算机视觉辅助质检等方面，都有很大的应用前景和空间。</p><p>&nbsp;</p><p>在即将到来ArchSummit深圳大会上，AI助力工业和制造智能化专题论坛将邀请来自美的、心智优化、生活科技、红海科技、腾讯云等企业的专家分享AI在工业制造场景下的最新应用案例和最佳实践。论坛将探讨如何有效利用AI技术推动制造业智能化转型，以及如何克服AI应用过程中的问题和挑战。</p><p>&nbsp;</p><p>在物流和供应链领域，AI模型应用面临独特挑战。尽管大模型具备强大的通用能力，如海量知识、规划推理能力，甚至编写代码能力，但它们也有局限性，例如易产生幻觉、不擅长精确数值计算，以及对深入业务理解的不足。这些问题同样困扰着大模型在供应链领域的应用。目前，我们在供应链领域的大模型应用探索主要集中在售前、售后和智能办公等周边领域。</p><p>&nbsp;</p><p>供应链核心领域的智能优化和决策是非常专业和严谨的，任何不准确的信息或计算错误都可能对业务造成重大影响。智能决策领域的问题，如网络规划和路径优化，通常是NP-Hard问题，需要大量计算，且输入输出是非文字和非图片的结构化数据。因此，大模型在供应链智能决策领域目前难以发挥作用，也不是其擅长的领域。目前可以通过Agent技术结合传统模型，将传统模型和技术封装为工具，由大模型负责人机交互界面和高层控制规划，从而在传统技术和大模型技术之间架起桥梁。</p><p>&nbsp;</p><p>此外，供应链领域的许多决策任务本质上是序列生成任务，如旅行商问题可以抽象为序列生成问题。因此，可以利用大模型底层技术，如Transformer架构，来解决这些问题。尽管目前还存在学术研究中的问题简化、距离计算的欧式空间问题、实际问题规模的扩大等挑战，但随着研究的深入，预期未来大模型技术能够在供应链核心决策问题上实现突破，并在行业中得到普及和应用。顺丰也在持续关注这一领域，并尝试解决这些问题，期望未来能分享相关进展。</p><p></p><h5>高磊：中小规模的公司想做顺丰现在实现了的一些场景，大概要花费多少投入？需要组建什么规模的团队？</h5><p></p><p>&nbsp;</p><p>高磊：在讨论大模型应用的投资问题时，需要考虑几个关键因素，包括赛道、行业特性、专业数据的需求以及算力和数据的投入。</p><p>行业特性与场景需求：首先，投资的规模和方向取决于所处的行业和具体应用场景。如果行业有大量专业知识和特定数据需求，而这些是公开模型无法处理的，那么可能需要更多的投资来进行模型的微调（fine-tuning）、使用自有数据进行训练。算力与数据投资：在金融等数据密集型行业，算力和数据通常是投资的重头。如果需要自行微调模型，这将涉及到基建方面的较大投资。人才与研发投入：如果不需要对模型进行微调，或者不需要通过复杂的技术嵌入行业知识，研发人员的投入可能并不会特别大。在这种情况下，少数算法工程师可能就足以推动项目前进。定制化需求与投资规模：如果行业缺乏公开的大模型，或现有技术无法满足业务需求，需要自行收集数据并进行模型微调，那么所需投入将会显著增加。</p><p></p><h3>趋势与展望：各行业大模型应用将走向何方</h3><p></p><p></p><h5>高磊：在基金行业，如果上述提到的来自技术、业务、组织、人才等方面的挑战可以解决，平野老师，您认为AI和大模型技术还有哪些新的可能性和潜力？</h5><p></p><p>&nbsp;</p><p>平野：在未来，大模型在解决业务问题上将更倾向于深入各个垂直领域，针对特定问题提供解决方案。随着技术的不断进步，大模型的应用将从单一的文本处理方式扩展到更多模态的解决方式，比如结合视觉、语音等，这将推动更多复杂场景和问题的解决。</p><p>&nbsp;</p><p>将来在金融行业，大模型能够极大地提高研究效率。例如，以往一个研究员可能一天只能阅读十篇左右的研究报告，而未来大模型可以快速提炼关键信息，使研究员在短时间内阅读上百篇研报，并通过辅助工具进行深入分析和个性化阅读，这与整个行业发展趋势是相契合的。</p><p>&nbsp;</p><p>随着经济的崛起，金融行业的体量将不断扩大。以基金经理为例，当管理的资产规模从一亿增长到十亿、百亿甚至千亿时，需要关注的公司数量也会成倍增加。在这种情况下，大模型能够帮助基金经理更高效地处理信息，从而在决策过程中做出更精准的选择。大模型不仅在提高工作效率方面发挥作用，还能提升决策的专业程度，为使用者提供准确和有效的信息。在金融行业，大模型的应用将主要集中在提高研究效率和决策精准度上。</p><p>&nbsp;</p><p>展望其他行业，我认为大模型将逐渐承担起重复性和基础性的推理工作，而人类工作者将更多地从事架构层面和决策层面的最终决策。这样的分工将使整体工作流程更加高效，同时也能够达到比以往更令人满意的结果。</p><p></p><h5>高磊：志鹏老师，您对AI和大模型技术在金融领域的发展前景有何展望？未来，您认为AI技术将如何提升金融行业的智能化水平和服务能力？</h5><p></p><p>&nbsp;</p><p>贾志鹏：我非常认同平野老师提到的两个关键词：多模态和决策智能。我们公司在内部对未来大模型应用落地的展望也集中在这两个方向。多模态是指结合多种信息形式，如文本、图像、声音等，而决策智能则涉及到利用AI进行复杂决策的过程。</p><p>&nbsp;</p><p>平野老师也提到了通过多个Agent联合来实现决策智能。随着大模型和多模态智能在企业中的广泛应用，我们预见这将加速企业内部数据布局的整理工作。金融领域的数据极为复杂，不仅包括各种指标平台和不同的指标口径，还涉及银行、经纪人持有的数据以及行业和国家标准数据。要有效地支持大模型并整理多模态数据以实现准确决策，需要确保数据的准确性和就绪性。这意味着企业可能需要整合其所有数据，探索不同数据间的关联关系，并进一步挖掘和整合数据。包括元数据在内的不同数据类型需要被整合和关联起来，以便它们能够与大模型有效地结合，再通过多个Agent的学习和协作，承担完整的决策职能。大模型的应用可能会催生或加速企业数据整合的过程，推动企业更高效地利用其数据资源，以实现更精准的决策。</p><p>&nbsp;</p><p>高磊：除了金融领域，我还想提一下AI大模型在智能供应链体系建设方面的应用前景。AI大模型与智慧供应链的融合，是一个充满潜力和激动人心的领域，有许多令人期待的应用场景和可能性。</p><p>&nbsp;</p><p>第一，AI辅助的产品设计和研发，如AlphaFold3在医药研发领域的应用，以及AI在芯片设计和工业设计中的辅助作用，预示着这类技术将越来越普及。</p><p>&nbsp;</p><p>第二，随着AI技术的发展，例如特斯拉最近发布的擎天柱机器人二代，以及李飞飞教授新成立的聚焦空间智能的公司，集成智能的机器人将能够完成更多复杂任务。在生产制造和供应链领域，当这些具备大模型加持的具身智能机器人的能力达到一定水平，许多危险和重复性劳动完全可以由它们来承担。</p><p>&nbsp;</p><p>第三，Agent技术的应用将越来越广泛，未来可能逐渐替代一些B端软件。在公司中，无论是一线工作人员还是管理人员，都可能配备多个Agent助理，通过日常交流完成大量工作。</p><p>&nbsp;</p><p>第四，目前大模型在供应链智能决策核心的应用还处于外围阶段。但我相信，未来大模型将直接作用于智能决策的核心领域，将会有更多专业模型涌现出来。这些模型不再是通用的行业大模型，而是具备特定行业专精知识的模型，如专注于路径规划或销量预测的专业大模型。它们的应用形式可能不再局限于文本或图片等信息载体，而是直接输出对应的决策信息以支持业务需求。</p><p>&nbsp;</p><p>这些是我对AI大模型在未来，特别是在供应链和制造领域应用场景的一些展望和设想。随着技术的不断进步，我们有理由期待这些设想将逐步变为现实，为行业带来更多创新和变革。</p><p></p><h5>高磊：大模型可以替代金融分析师生成以行研报告吗？或者竞品技术分析有哪些方面的配置要求？</h5><p></p><p>&nbsp;</p><p>贾志鹏：我认为这个问题可能与平野老师刚才介绍的平台报告相关。基于我的观察和经验，技术并不能完全替代人类，它只能无限逼近人类的能力和决策水平。在某些方面，特别是在我们讨论的传导行业或ToB领域中，我们不能单一地从技术角度来衡量问题。实际上，管理因素在这些行业的落地应用中占据了重要地位。</p><p>&nbsp;</p><p>平野：分析师可以用大模型辅助撰写行业报告。我们认为大模型并不是要替代分析师，而是与之相辅相成，帮助他们更高效地完成任务。过去，编写一篇行业研究报告可能需要数小时、数周、甚至一个月的调研时间。如今在大模型的帮助下，这一过程的用时会大大缩短，甚至在一天之内就能完成。我们已经在这方面进行了实践，并开发了相关的落地产品。</p><p>&nbsp;</p><p>至于观众提到的竞品技术分析，我认为这实际上是一种情报分析。金融领域的信息通常并不完全透明，不同机构对信息的处理方式各异；同时金融业是一个信息量爆炸的行业，对信息的实时性和准确性要求很高。在大模型的支持下，分析师可以在短时间内迅速发现市场热点，这些热点很可能也是其他公司关注的焦点。利用BI工具的数据处理能力，结合大模型的推理生成能力，可以部分替代金融分析师的工作，甚至一个经过训练的大模型可以处理多个金融分析师汇总的信息。</p><p>&nbsp;</p><p>如果要考虑配置要求，我认为可能需要在市面上已有的模型基础上进行个性化训练，这可能涉及一定的硬件资源投入，如训练用的计算卡，同时也需要相应的人才来进行预训练和模型调优等工作。</p><p></p><h4>嘉宾介绍</h4><p></p><p>高磊，顺丰科技运筹优化算法总工程师，拥有10年+机器学习与运筹优化算法经验，研究方向为运筹优化、强化学习等。2016年加入顺丰，现任顺丰科技运筹优化算法总工程师，曾主导顺丰集团内部多个数智化项目的研发与落地工作，涉及领域包括业务量预测、陆运干支线规划与调度、航空规划与调度、运力规划、场站选址、物资调拨等。</p><p>目前主要负责集团智慧供应链体系建设相关工作。期间带领团队获得十余项发明专利，中物联物流技术创新奖、CCF&nbsp;BDCI一等奖、最具商业价值奖，运筹帷幄年度行业实践奖与学术应用奖等荣誉。</p><p>&nbsp;</p><p>平野，天弘基金人工智能部负责人，毕业于英国曼彻斯特大学计算机系。天弘基金算法团队负责人，负责营销、风控和投资智能化业务领域。天弘金融大模型负责人。曾就职于百度、蚂蚁金服等，是支付宝第五代智能风控引擎Alpharisk主要开发者之一，该项目获2019年浙江省科技进步一等奖。百度昊天镜业务风控模型团队总负责人。</p><p>&nbsp;</p><p>贾志鹏，Fabarta&nbsp;高级技术专家，曾先后就职于&nbsp;IBM、阿里云、HSBC，专注于金融、制造和汽车等行业的业务解决方案咨询与实施工作。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>