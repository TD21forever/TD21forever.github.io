<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</id>
            <title>英特尔数据中心与人工智能事业部 AI 软件架构师何普江确认出席 QCon 上海，分享大模型时代：最大化 CPU 价值的优化策略</title>
            <link>https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 大模型时代, CPU 价值优化策略, CPU 和 GPU 投机采样方法
<br>
<br>
总结: 本文介绍了即将在上海举办的QCon全球软件开发大会，其中AI软件架构师何普江将分享关于大模型时代下最大化CPU价值的优化策略。演讲内容包括利用CPU的多核特性、并行计算和AMX指令集扩展技术来提高处理速度，以及结合CPU和GPU的投机采样方法来充分利用CPU资源并减少对GPU的依赖。通过这些优化策略，可以提高模型推理速度，实现生成式模型部署落地。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。英特尔数据中心与人工智能事业部 AI 软件架构师何普江将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5627?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">大模型时代：最大化 CPU 价值的优化策略</a>"》主题分享，探讨一种结合 CPU 和 GPU 的投机采样方法，在大语言模型时代充分利用 CPU 资源的关键策略，以及最新的性能情况，以便了解这些优化策略的实际效果。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5627?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">何普江</a>"，2007 年硕士毕业于中国科学技术大学。精通英特尔软件架构、英特尔产品与技术以及 IA 平台性能优化。在英特尔工作期间，为国内主流 ISV 开发出基于 IA 平台的云计算产品过程中提供关键支持，并优化了多家主要互联网公司的核心产品，使其性能提升数倍。对 PyTorch，Tensorflow 等 AI 框架有深入研究，并拥有 10 年以上软件优化经验。工作期间曾获得英特尔中国个人员工最高荣誉奖，与国内互联网厂商多个部门进行深度合作，并在 2019 年助力某云厂商云在 MLPerf 评测中创下了业界领先的 Performance/TOPS 性能记录。他致力于基于 IA 架构平台的深度学习、机器学习研究和在互联网行业的落地推广工作，最新工作包括创建并开源了 CPU 上大语言模型的极致优化方案 xFasterTransformer。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型时代：最大化 CPU 价值的优化策略</p><p></p><p>本次演讲将探讨在大语言模型时代充分利用 CPU 资源的关键策略。具体介绍一些结合硬件特性的优化方法，例如利用 CPU 的多核特性、采用并行计算和 AMX 指令集扩展技术来提高处理速度。</p><p></p><p>此外还将介绍一种结合 CPU 和 GPU 的投机采样方法，通过在 CPU 上运行部分计算任务，充分利用 CPU 资源并减少对 GPU 的依赖。最后，我将分享一些最新的性能情况，让您了解这些优化策略的实际效果。通过这些方法，您将能够更好地利用 CPU 资源，提高模型推理速度，以更快速高效的实现生成式模型部署落地。</p><p></p><p>演讲提纲：</p><p></p><p>大语言模型时代为什么需要最大化 CPU 价值CPU 上的大模型优化策略</p><p>○ 大语言模型计算特点 </p><p>○ CPU 硬件特性概览 </p><p>○ 优化方法 </p><p>○ 从向量化到张量化 </p><p>○ 从并行执行到分布式推理 </p><p>○ 低精度优化 </p><p>○ 深入 CPU 微架构的软件优化 </p><p>○ 各优化策略的实际性能数据对比及效果展示</p><p>结合 CPU 和 GPU 的投机采样方法</p><p>○ CPU 和 GPU 协同工作的背景 </p><p>○ 投机采样技术的介绍 </p><p>○ 利用 CPU 进行部分计算任务的优势 </p><p>○ 优化方法：选择合适的投机采样策略、任务调度等</p><p>总结与展望</p><p>○ 各优化方法的核心优势与局限性总结 </p><p>○ 对未来大语言模型时代的展望与挑战</p><p></p><p>听众收益点：</p><p></p><p>○ 理解并结合硬件特性进行优化，提高模型推理速度和处理能力</p><p>○ 了解 CPU 上的最新性能情况，为实际业务的大模型线上部署提供更多选择</p><p>○ 掌握结合 CPU 和 GPU 协同工作的优化策略，减少对 GPU 的依赖，提高资源利用率</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！9 折优惠仅剩最后 4 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</id>
            <title>英特尔高宇：AI工作负载有多种形态和规模，硬件上没有一刀切的解决方案</title>
            <link>https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:51:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 大模型, 公共通用大模型, 个性化服务
<br>
<br>
总结: 去年年底以来，随着ChatGPT应用体验界面的推出，大模型的生成式AI技术得到了快速发展。公共通用大模型通过学习公共数据，可以生成高质量的文本、图像、声音和视频等内容，为智能创新和个性化服务提供了巨大的想象空间。为了保护数据安全和隐私，同时提供个性化服务，公共大模型和面向个人的专有大模型混合部署成为产业共识。 </div>
                        <hr>
                    
                    <p>去年年底以来，随着ChatGPT应用体验界面的推出，使得以大模型为主的生成式AI 技术取得了重大的并且快速地发展，大模型也展现出了令人惊叹的智能涌现能力，表现出了更为强大的创造性和通用场景的普通适用性，技术得以快速发展。</p><p></p><p>首先取得重大突破的是公共通用大模型，从人类社会大量存积下来的公共数据当中去学习，进而生成高质量的文本、图像、声音甚至是视频等内容，为各个领域的智能创新和每一个人的智能体验创新提供了巨大的想象空间。</p><p></p><p>然而，出于数据的安全和隐私保护的考虑，以及更高效率，更低成本来享用大模型通用能力的角度考虑，人们又既希望获得公共大模型目前的各类强大的通用服务，同时又希望AI 能够真正理解自己，提供专属的个性化服务，还要能够充分地保障个人的数据和隐私安全，为此，公共大模型和面向个人的专有大模型混合部署，正逐渐成为产业的一个共识。</p><p></p><p>在这样的时代背景下，作为消费和商用个体用户中最坚挺的终端，PC在AIGC时代承载了怎样的使命？</p><p></p><p>12月7日，首届AI PC产业创新论坛在北京联想总部举行。此次论坛汇聚了众多用户、终端厂商、算力厂商（芯片）、AI技术厂商（大模型）、应用领域生态合作伙伴，深度探讨AI PC为AI普惠带来的巨大改变。此外，在此次论坛上，业内首份《AI PC产业（中国）白皮书》重磅发布。</p><p></p><p>与会嘉宾认为，AI PC 到来之际，大模型将成为每一个人必不可少的助手，同时对推理的算力需求将超过训练的算力需求。算力集中于云端的模式变得不可持续，AI计算负载将逐渐由云端向边缘侧和端侧下沉。在搭建本地智能算力上，CPU+NPU+GPU 异构式架构方案是目前最为成熟的方案之一。</p><p></p><p>对此，英特尔中国区技术总经理高宇表示，AI工作负载有多种形态和规模。所以，从硬件上没有一刀切的解决方案。“基于多年的学习与市场经验，我们提出了XPU的概念，包括GPU/NPU/CPU。”他说，联想是英特尔的战略合作伙伴，双方已经基于即将发布的Meteor Lake处理器推进AI体验的开发和创新。</p><p></p><p>作为算力厂商的代表，英特尔正采取三项措施，来持续构筑端侧的算力。一是构建为AI而设计的高效能AI-Ready平台；二是提供工具以支持广泛的x86应用生态系统，三是激发创新，开启全新的AI体验，包括为软件和应用开发人员提供支持，以便在各个领域里都能更好将AI功能完美部署到PC客户端上。</p><p></p><p>英特尔今年还正式启动了首个“AI PC加速计划”，将在2025年前为超过1亿台PC带来人工智能特性。其中，通过与超过 100 家 ISV 合作伙伴深度合作，并集合 300 余项 AI 加速功能，英特尔将在音频效果、内容创建、游戏、安全、直播、视频协作等方面继续强化 PC 的体验。</p><p></p><p>据了解，在实践中，英特尔13代酷睿处理器已经可以流畅运行70亿到180亿参数的大模型，并成功部署了LLM。高宇表示，即将推出代号Meteor Lake的AI PC处理器，代表英特尔40年来最重大的架构转变，旨在为AI PC时代铺平道路。它是首个内置AI加速引擎NPU的处理器，可在PC上实现高能效的AI加速和本地推理。</p><p></p><p>为了完成用户相对复杂的任务，AI PC往往需要调动不同的模型和应用，为AI PC的能力进行补充和延伸。因此，AI PC功能的发挥不仅需要像英特尔这样的算力厂商的参与，还需要整个开放的行业生态作为支撑。</p><p></p><p>在AI PC的推动下，PC产业生态将从应用为本转向以人为本，用户成为行业生态创新的驱动者和创造者。模型、应用、算力厂商都需要围绕AI PC（终端）形态下新的以人为本的需求做出改变，在研发工作中对AI的高效运行予以充分的考量，以适应AI PC新时代。</p><p></p><p>联想作为终端厂商，是离用户最近的一端，因而被推到台前，成为生态组织者和生态的核心中枢。以场景需求为基础面向用户整合产业资源，承担AI PC技术整合创新交付者、新一代个人智能体及 AI入口创造者和用户体验维护者、本地化个人数据及隐私安全守护者和开放的AI应用生态标准制定者和推广者身份，职责重大。正是出于行业责任，联想联合国际数据公司IDC发布业内首份《AI PC产业（中国）白皮书》，对AI PC进行了全新定义，以加速构建AI PC产业新生态。</p><p></p><p>高宇最后表示，AI PC加速计划由即将发布的IntelCore Ultra处理器率先驱动。未来，英特尔将搭建性能并行和吞吐量适用于融合AI的媒体/3D/渲染的GPU，打造适用于持续的AI和分担AI负载的专用低功耗AI引擎NPU；迭代能够快速响应，适用于轻量级、单次推理的低延迟任务的CPU，相信在新平台的加持下，英特尔将加快与联想共同打造混合AI算力架构，驱动AI PC落地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</id>
            <title>零一万物Yi-34B-Chat 全球权威测评，开源黑马追平 GPT-3.5？</title>
            <link>https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 10:08:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Yi-34B-Chat, 测评结果, AlpacaEval Leaderboard, LMSYS ORG排行榜, SuperCLUE排行榜
<br>
<br>
总结: Yi-34B-Chat是一款性能优异的大语言模型，在多个测评平台上取得了出色的成绩。在AlpacaEval Leaderboard和LMSYS ORG排行榜中，Yi-34B-Chat超越了其他竞争对手，成为世界范围内仅次于GPT-4的大语言模型。在中文能力方面，Yi-34B-Chat也取得了令人瞩目的成绩，在SuperCLUE排行榜中表现出色。该模型还提供了4bit/8bit量化版，方便在消费级显卡上使用。通过AI Alignment团队的创新对齐策略，Yi-34B-Chat不仅在理解和适应人类需求方面表现出色，还与人类价值观对齐。 </div>
                        <hr>
                    
                    <p></p><p>继11月初零一万物发布性能优异的 Yi-34B 基座模型后，Yi-34B-Chat 微调模型在11月24日开源上线，而各家测评平台也相继给出了Yi-34B-Chat 的测试结果。</p><p>&nbsp;</p><p>模型地址：</p><p><a href="https://huggingface.co/01-ai/">https://huggingface.co/01-ai/</a>"</p><p><a href="https://www.modelscope.cn/organization/01ai">https://www.modelscope.cn/organization/01ai</a>"</p><p>&nbsp;</p><p></p><h2>各家测评结果</h2><p></p><p>&nbsp;</p><p>在斯坦福大学研发的大语言模型评测 AlpacaEval Leaderboard 中，Yi-34B-Chat以94.08%的胜率，超越LLaMA2 Chat 70B、Claude 2、ChatGPT，在 Alpaca 经认证的模型类别中，成为世界范围内仅次于GPT-4 英语能力的大语言模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a9c4fc531e7575e5fcf65d475dfb716.png" /></p><p>&nbsp;</p><p>AlpacaEval Leaderboard排行榜（发布于2023年12月7日）</p><p>&nbsp;</p><p>同一周，在加州大学伯克利分校主导的LMSYS ORG排行榜中，Yi-34B-Chat也以1102的Elo评分，晋升最新开源SOTA开源模型之列，性能表现追平GPT-3.5。</p><p>&nbsp;</p><p>伯克利LMSYS ORG排行榜采用了一种最为接近用户体感的 “聊天机器人竞技场” 特殊测评模式，即让众多大语言模型在评测平台随机进行一对一 battle，通过众筹真实用户来进行线上实时盲测和匿名投票。11月份，经25000个真实用户投票计算了20个大模型的总得分。Elo评分越高，说明模型在真实用户体验上的表现越出色。</p><p>&nbsp;</p><p>在开源模型中，Yi-34B-Chat 在英语能力上进入前十。LMSYS ORG 在12月8日官宣11月份总排行时评价：“Yi-34B-Chat 和 Tulu-2-DPO-70B 在开源界的进击表现已经追平 GPT-3.5”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f3e7d606ee4b2ef7fdc44af2325e3a4.png" /></p><p>&nbsp;</p><p>LMSYS ORG榜单（发布于2023年12月8日）</p><p>&nbsp;</p><p>在针对中文能力的排行榜方面，SuperCLUE从基础能力、专业能力和中文特性能力三个不同的维度，评估模型的能力。根据11月底发布的《SuperCLUE中文大模型基准评测报告 2023》，11月下旬首度发布的 Yi-34B Chat，迅速晋升到和诸多国产优秀大模型齐平的 “卓越领导者” 象限。在多项基准评测中的 “SuperCLUE 大模型对战胜率” 这项关键指标上，Yi-34B-Chat 取得31.82%的胜率，仅次于GPT4-Turbo。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b11a91538b615aa1301ff0c4054d7137.png" /></p><p>&nbsp;</p><p>中文SuperCLUE排行榜（发布于2023年11月28日）</p><p>&nbsp;</p><p>值得注意的是，Yi-34B-Chat 微调模型为开发者提供了 4bit/8bit 量化版模型。Yi-34B-Chat 4bit 量化版模型可以直接在消费级显卡（如RTX3090）上使用。</p><p>&nbsp;</p><p></p><h2>真实场景如何</h2><p></p><p>&nbsp;</p><p>Yi-34B-Chat 模型实力在不同的对话场景中实力如何？来看几个更直观的问题演示。</p><p>&nbsp;</p><p></p><h4>知识与生成</h4><p></p><p>&nbsp;</p><p>问：Transformer 模型结构能不能走向 AGI ?</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d78f0a4dab09a17eb32fc8654b708191.png" /></p><p>&nbsp;</p><p></p><h4>创意文案</h4><p></p><p>&nbsp;</p><p>问：给我生成一个小红书文案，给大家安利一只豆沙色的口红。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be47558b3b174390b2de5ec944c40e0f.png" /></p><p>&nbsp;</p><p></p><h4>中文理解</h4><p></p><p>&nbsp;</p><p>问：小王给领导送了一份礼物后。领导说：“小王，你这是什么意思？”小王：“一点心意，意思意思。”领导：“你这就不够意思了。”小王：“小意思，小意思。”领导：“小王，你这人真有意思。”小王：“也没什么别的意思。”领导：“那我多不好意思。”小王：“是我不好意思。”这个意思到底是什么意思？</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4ffb796172e36d6124d32749bd91eed.png" /></p><p>&nbsp;</p><p>据零一万物介绍，除了 Yi 系列强基座的贡献以外，Yi-34B-Chat 模型的效果还得益于其人工智能对齐（AI Alignment）团队采用了一系列创新对齐策略。通过精心设计的指令微调流程，不仅强化了模型在理解和适应人类需求方面的能力，还使得模型与人类价值观对齐，包括帮助性（Helpful），可靠性（Honest），无害性（Harmless）等。</p><p>&nbsp;</p><p>在强基座设定下，该团队采用了一种轻量化指令微调方案，该方案涵盖了单项能力提升和多项能力融合两个阶段。</p><p>&nbsp;</p><p>其中，单项能力包括通用指令跟随、创意内容生成、数学、推理、编程、泛COT、对话交互等，团队通过大量的消融实验，针对模型单能力构建和多能力融合总结了独家认知经验；在多能力融合阶段，团队采用网格搜索的方法来决定数据配比和超参数的设置，通过基准测试和自建评测集的结果来指导搜索过程，成功实现模型的多能力融合。</p><p>&nbsp;</p><p>在数据的量和质方面，零一万物团队认为，数据质量比数量重要，少量高质量数据比大量低质量数据更好，仅需少量数据（几条到几百条）就能激发模型特定单项能力。团队通过关注超出模型能力的“低质量”数据，来减少了模型“幻觉”。</p><p>&nbsp;</p><p>在指令多样性与难度方面，团队在各能力项下构建任务体系，实现训练数据中的指令均衡分布，提升模型泛化性。此外，团队发现训练数据的风格会影响模型收敛速度和能力上限的逼近程度，因此统一了回复风格，比如重点设计了CoT的回复风格，实现在轻量SFT情况下，避免了风格不一致加剧模型的“记忆”现象。</p><p>&nbsp;</p><p></p><h2>开源“满月”：有赞扬，也有质疑</h2><p></p><p>&nbsp;</p><p>Yi模型发布之初便是开源的。开源首月，Yi模型在Hugging Face社区下载量为16.8万，魔搭社区下载量1.2万，在GitHub 获得超过4900个Stars。</p><p>&nbsp;</p><p>多家知名公司和机构推出了基于Yi模型基座的微调模型，比如猎豹旗下的猎户星空公司推出的OrionStar-Yi-34B-Chat模型、南方科技大学和粤港澳大湾区数字经济研究院（简称IDEA研究院）认知计算与自然语言研究中心（简称CCNL中心）联合发布的SUS-Chat-34B等。</p><p>&nbsp;</p><p>知名技术写作者苏洋表示，根据他的近期观察，Hugging Face榜单中的前三十名有一半多是 Yi 和其他用户微调的 Yi-34B 的变体模型，原本占据榜单头部的 68B 和 70B 模型的数量目前只留有几个。</p><p>&nbsp;</p><p>苏洋曾尝试使用家里本地的机器，在纯 CPU 环境、CPU &amp; GPU 混合环境下对Yi模型进行测试，“结果比想象中要好。尤其是社区中的 finetune 后的版本，在对新闻、研究报告的摘要总结方面，对非结构化的信息中的实体识别和抽取上表现非常不错。”同时，苏洋也指出，可能是由于零一在训练过程中，出于安全考虑，过滤太多语料的缘故，一些本土化的内容仍然不够深入。</p><p>&nbsp;</p><p>根据亲身体验，苏洋总结道，34B 普通用户努努力还是能自己相对低成本跑起来的，68 B 和 70B 的模型想要本地运行，需要更多的资源，但目前分数上跟 34B 的拉不开太多差距，大概就是三四分平均分。</p><p>&nbsp;</p><p>开源后，Yi系列模型也遭到了一些质疑。</p><p>&nbsp;</p><p>开发者Eric Hartford敏锐发现了模型存在的一个问题：Yi模型使用了与LLaMA模型完全相同的架构，只是将两个张量改了名字。由于围绕LLaMA架构有很多投资和工具，保持张量名称的一致性是有价值的。Eric建议，在Yi被广泛传播前，及时恢复张量名称。</p><p>&nbsp;</p><p>Eric没有预想到，他的这个建议引来了关于Yi模型“抄袭”LLaMA的质疑。</p><p>&nbsp;</p><p>之后，零一万物很快便在各开源平台重新提交模型及代码，完成了开源社区的版本更新。零一万物表示，一个模型核心技术护城河是在架构之上，通过数据训练获得的参数和代码。在沿用了开源社区普遍使用的LLaMA 架构之上，零一万物团队从零开始，用高质量的数据集、自研训练科学和AI Infra打造了 Yi-34B 在内的系列模型。为了执行对比实验的需要，对部分推理参数进行了重新命名。原始出发点是为了充分测试模型，而非刻意隐瞒来源。</p><p>&nbsp;</p><p>Eric后来发推特为<a href="https://twitter.com/erhartford/status/1724563655545503822">Yi辩护称</a>"，“他们没有在任何事情上撒谎。所有的模型都是在相互借鉴架构。架构是学术研究的产物，已经发表在论文中，任何人都可以自由使用，这丝毫不减损Yi团队的成就。他们从零开始使用自己创建的数据集训练Yi，对开源领域的贡献是值得赞扬的。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/aa/aad8cc193901261e7cc18122efe10bbf.png" /></p><p></p><p>他还补充道，“使用Llama架构没有任何问题。训练才是关键。Yi给了我们目前可获得的最佳模型，没有任何可抱怨的。”</p><p></p><p>更多阅读：</p><p><a href="https://www.infoq.cn/article/cVfuQaHVJ0SDPtP2jb7m?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">零一万物回应“套壳 Llama”争议：基于 GPT 研发，对模型和训练的理解做了大量工作</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</id>
            <title>首期“AI+软件工程”主题沙龙在京顺利举办</title>
            <link>https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 10:03:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 通用人工智能, 软件工程, AI技术
<br>
<br>
总结: 以大模型为核心的通用人工智能正在驱动着新一轮智能革命的持续演进，并给软件工程带来了新的发展契机。大模型等AI技术在软件研发过程中赋予了强大的智能化能力，软件研发不再只依赖于人类的智慧，而是与AI相结合使其过程更加高效、高质量、低成本，代码生成、代码补全等新能力推动智能化软件工程范围的延展。 </div>
                        <hr>
                    
                    <p>以大模型为核心的通用人工智能正在驱动着新一轮智能革命的持续演进，并给软件工程带来了新的发展契机。大模型等AI技术在软件研发过程中赋予了强大的智能化能力，软件研发不再只依赖于人类的智慧，而是与AI相结合使其过程更加高效、高质量、低成本，代码生成、代码补全等新能力推动智能化软件工程范围的延展。</p><p></p><p>为加强AI+软件工程领域的交流互通，推动行业多融合发展，2023年11月21日，由中国信息通信研究院人工智能研究中心、中国软件行业协会和应用现代化产业联盟联合主办，中国人工智能产业发展联盟AI for 软件工程（AI4SE）工作组承办的首期“AI+软件工程”主题沙龙在京成功举办，线上线下总观看量超过6万。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7d0396c7b40cb595093b3dca99513f5.webp" /></p><p></p><p>中国信通院人工智能研究中心负责人魏凯、中国软件行业协会副秘书长付晓宇分别发表致辞。魏所指出，软件行业是大模型生态的聚集地，智能化的融合可以提升软件工程的效率和创新能力。然而，机遇和挑战并存，中国信通院将围绕AI和软件工程全生命周期持续开展系列工作，与产业各方共同面对挑战。付秘书长表示，AI为软件工程带来了新思路新方法，软件工程领域也在积极应对AI带来的挑战，中国软件行业协会一直致力于推动应用现代化的发展，期待看到我们的行业在面对挑战时，能够以开放、合作的态度，共同寻找解决方案，同时在AI与软件工程交叉领域看到更多创新与突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3977d976034a22719df06830e8a4a84b.webp" /></p><p>中国信通院人工智能研究中心负责人魏凯</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d917f57d13e9bd398a48a7bdd56b2fb2.webp" /></p><p>中国软件行业协会副秘书长付晓宇</p><p></p><p>本次沙龙邀请了来自华为、联通软件研究院、国金证券、软通动力、东吴证券、中软国际、中国信通院的7位行业专家，围绕落地方案、实践范式、问题与挑战、发展与趋势发表主题演讲。</p><p></p><p>华为技术有限公司软件工程专家贺美迅的分享主题是“Al辅助研发实践探索”。贺总围绕开发模式、关键挑战、参考经验和实践案例四个方面，对AI辅助研发的技术与过程进行深入浅出的分析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/6229db16a334e1bd89cdb8b62a317ea3.webp" /></p><p>华为技术有限公司软件工程专家贺美迅</p><p></p><p>中国联通软件研究院软件架构师常红珍分享的主题是“代码生成模型及智能工具探索”。常总围绕诸多场景对代码生成大模型的探索与实践过程进行了剖析，通过将代码大模型与公域和私域数据相结合，构建智能体协作开发框架、智能体应用框架，并引入专家经验，实现结构化思考和优质代码的生成，突破代码片段的限制，提升软件工程效率，并对未来进行展望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cab7f0b18c7a1ae6dad24a0c0e3ec780.webp" /></p><p>中国联通软件研究院软件架构师常红珍</p><p></p><p>国金证券技术负责人李晨带来了证券业开发大模型探索与实践的分享。李总从政策监管、开发大模型背景和国金证券落地实践的维度进行了详细分析。国金证券在设计评审类、编码辅助类、测试辅助类场景中实践成效初显，平均效率提升达30%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3ea92cade67fff5dbcdfe44564e6824.webp" /></p><p>国金证券技术负责人/架构师李晨</p><p></p><p>软通动力助理副总裁孙洪军分享了软通动力AISE产品研发及实践。孙总首先介绍了软通动力的AIGC整体布局，其次介绍了软通动力AISE产品的设计背景、系统架构和核心能力，最后就产品落地实践和成效展开了分享，某应用企业通过使用该产品达到20%-30%的提效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fbc4d69003ae2325968834ba0768965.webp" /></p><p>软通动力助理副总裁孙洪军</p><p></p><p>东吴证券信息技术总部副总经理任川分享了大模型训练和AI赋能的探索与实践。任总表示，AIGC大模型在证券行业具有广泛的应用前景，可在文案生成、智能搜索、券商智能中枢、BI助手等四大类关键领域上提供显著的提质增效服务能力。任总围绕东吴GPT分析其应用需求、应用领域和应用范式，并对未来AI规划进行分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e392a263efdd384abee9a9f764f81db7.webp" /></p><p>东吴证券信息技术总部副总经理任川</p><p></p><p>中软国际云智能业务集团CTO祁银红就中软在Al加速研发效能的实践和应用开发新模式的探索进行分享。祁总围绕中软的青燕平台，对探索过程与背景、核心能力及应用成效进行解析，该平台面向个人提供需求设计、文档生成、开发测试、代码生成等功能，面向组织提供全流程研发管理、测试管理、多级流水线等能力。以测试用例生成+故障排查为例，时间成本可缩短60%以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d005b0666ec73854052cb653092c49a.webp" /></p><p>中软国际云智能业务集团CTO祁银红</p><p></p><p>中国信通院云大所人工智能部主任曹峰发表了《智能化软件工程(AI4SE)发展现状与趋势》的主题演讲。曹主任从AI赋能软件工程的发展历程出发，介绍了当前软件工程相关大模型的现状、开发测试运维等场景的落地分析、智能化软件工程的关键技术与挑战，以及当前中国信通院在AI4SE领域开展的标准编制、案例征集等系列工作，最后对于AI4SE的多模态、全流程、新研发模式进行了展望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b26a8b6d59080663da6d12efea6fae1.webp" /></p><p>中国信通院云大所人工智能部主任曹峰</p><p></p><p>沙龙的最后，应用现代化产业联盟"AI+软件工程"工作组正式成立，由华为、中国信通院、国金证券、联通软研院、软通动力、中软国际、国金证券、明源云等单位共同参与启动仪式。应用现代化产业联盟也欢迎更多有志于“AI+软件工程”研究的企业和伙伴加入到联盟和工作组中来，共建开放协同创新的软件生态体系，促进产业健康有序发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a27cd48dd6717b1ff89424d73d8a6033.webp" /></p><p>应用现代化产业联盟"AI+软件工程"工作组首批单位合影，左起：李晨（国金证券）、吴振亮（明源云）、孙洪军（软通动力）、曹峰（信通院）、王千祥（华为）、祁银红（中软国际）、常红珍（中国联通）</p><p></p><p>应用现代化产业联盟汇聚了产业各方力量，将会加速产业创新升级，促进产业跨越式增长，共同推动应用软件的技术提升，赋能企业开展应用现代化转型。同时也希望更多优秀的软件企业加入应用现代化产业联盟以及“AI+软件工程”工作组中来，助力擘画中国式现代化的宏伟蓝图。</p><p></p><p>应用现代化产业联盟 &amp;“AI+软件工程”工作组</p><p></p><p>黄老师：yigang.huang@ami-alliance.org.cn</p><p>李老师：Linda.lidandan@ami-alliance.org.cn</p><p></p><p>扫一扫，加入应用现代化产业联盟</p><p><img src="https://static001.geekbang.org/infoq/11/114d8750f1bdcb25a7345827b4776f9b.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</id>
            <title>计算机图形学将迎来新突破？Meta携手斯坦福大学推出3D交互模型，VR时代似乎不远了</title>
            <link>https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:52:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 斯坦福大学, Meta/Facebook AI研究, CHOIS系统, 3D人机交互
<br>
<br>
总结: 斯坦福大学与Meta/Facebook AI研究实验室共同开发的CHOIS系统是一套突破性的AI系统，能够根据文本描述在虚拟人和物体之间生成自然、协调的3D人机交互。该系统利用条件扩散模型技术生成精确的交互动作，能够模拟连续的人类行为。CHOIS系统的出现对计算机图形学、AI与机器人技术产生了深远影响，能够大大提高动画制作效率，实现更高水平的虚拟现实体验，并为机器人在服务型领域的应用提供了新的可能性。这一成果令人惊艳，前景值得期待。 </div>
                        <hr>
                    
                    <p>近日，斯坦福大学与Meta/Facebook&nbsp;AI研究（FAIR）实验室的工作人员共同开发出一套突破性的AI系统，能够仅根据文本描述在虚拟人和物体之间生成自然、协调的运动关系。</p><p>&nbsp;</p><p>这套新系统被称为CHOIS（Controllable Human-Object Interaction Synthesis，即可控人机交互合成），使用最新的条件扩散模型技术生成无缝且精确的交互，例如“将桌子举过头顶、行走，然后放下桌子。”</p><p>&nbsp;</p><p>简而言之，这是一套先进的人工智能系统，用于合成逼真的 3D 人机交互。</p><p>&nbsp;</p><p>这项工作被公布在arXiv论文预发表网站的一篇文章中，也让我们得以一睹虚拟人如人类般顺畅理解并响应语言命令的未来景观。例如，把椅子拉近桌子来创造一个工作空间，调整落地灯以投射出完美的光芒，或者整齐地存放手提箱。每一项任务都需要人、物体和周围环境之间的精确协调。语言是表达和传达这些意图的有力工具，在语言和场景背景的指导下，合成逼真的人类和物体运动是构建先进的人工智能系统的基石，该系统可以在不同的3D环境中模拟连续的人类行为。</p><p>&nbsp;</p><p>论文地址：<a href="https://arxiv.org/pdf/2312.03913.pdf">https://arxiv.org/pdf/2312.03913.pdf</a>"</p><p>&nbsp;</p><p>研究人员们在文章中指出，“根据语言描述在3D场景中生成连续的人-物交互一直存在不少挑战。”</p><p>&nbsp;</p><p>他们必须确保生成的运动真实且协调同步，保持人手与物体之间的适当接触，且物体的运行应当与人类行为具有因果关系。</p><p></p><h2>如何实现</h2><p></p><p></p><p>CHOIS系统之所以效果拔群，依靠的就是其在3D环境中摸索出一套独特的人-物交互合成方法。CHOIS的核心为条件扩散模型，这是一种能够模拟详尽运动序列的生成模型。</p><p>&nbsp;</p><p>当给定人/物位置的初始状态以及所需操作的语言描述之后，CHOIS就会据此生成一系列动作，最终完成任务要求的交互效果。</p><p>&nbsp;</p><p>例如，假设指令是将灯具移到沙发旁边，CHOIS会理解指令内容并创建一段逼真的动画，显示人类形象拿起灯具并将其放置在沙发附近。</p><p>&nbsp;</p><p>利用 AMASS 等大规模、高质量的运动捕捉数据集，人们对生成人体运动建模的兴趣有所上升，包括动作条件合成和文本条件合成。虽然之前的工作使用 VAE 公式从文本生成不同的人体运动，但 CHOIS 专注于人与物体的交互。与通常以手部运动合成为中心的现有方法不同，CHOIS 在物体抓取之前考虑全身运动，并根据人体运动预测物体运动，为交互式 3D 场景模拟提供全面的解决方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae3859d5e2f84eeb8a8685af1054800d.jpeg" /></p><p></p><p>给定初始对象和人类状态、语言描述和3D场景中的稀疏对象路径点，CHOIS生成的物体运动与人体运动同步。</p><p>&nbsp;</p><p>CHOIS的独特之处，就在于它使用稀疏对象路径点和语言描述来指导动画生成。各个路径点充当对象移动轨迹中的关键标记点，确保运动不仅符合物理规律，而且与语言输入中描述的高级目标保持一致。</p><p>&nbsp;</p><p>CHOIS的另一大优势，在于能够将语言理解能力与物理模拟功能加以结合。传统模型往往难以将语言同空间和身体动作联系起来，特别对于较大的交互范围，必须考虑诸多因素才能始终保持交互的真实性。</p><p>&nbsp;</p><p>CHOIS首先解释语言描述所承载的意图和风格，而后将其转化为一系列既符合人体构造、又不违背物体特性的肢体动作，从而解决了大范围交互过程中的这一现实难题。</p><p>&nbsp;</p><p>该系统尤其具有开创性的一点，就是它能准确表现接触点（例如手与物体之间的接触位置），且物体的运行与人类化身施加的力保持一致。此外，该模型在训练和生成阶段还引入了专门的损失函数和指导性术语，旨在强制遵循这些物理约束，这也是让AI成功实现以人类方式理解物理世界、并与物理世界正确交互的重要一步。</p><p></p><h2>对计算机图形学、AI与机器人技术的影响</h2><p></p><p></p><p>CHOIS系统对计算机图形学产生了深远影响，特别是在动画和虚拟现实领域。通过让AI获得解释自然语言指令并据此生成逼真人机交互过程的能力，CHOIS能够大大减少制作复杂场景动画所需要的时间和精力。</p><p>&nbsp;</p><p>动画师们可以使用这项技术来创建出以往极为费时费力的关键帧动画序列，显著提升设计效率与成果产出。此外，在虚拟现实环境当中，CHOIS还能带来更加身临其境且高度交互的体验，由用户通过自然语言指挥虚拟角色，并观察其以逼真精度执行任务的全过程。这种更高水平的交互能够将VR体验从僵化、脚本化的事件转化为更加顺畅自然的动态环境用户输入响应效果。</p><p>&nbsp;</p><p>在AI和机器人领域，CHOIS则代表我们朝着更加自主的情境感知系统迈出的一大步。传统机器人往往受到预编程例程的限制，而CHOIS这类系统的出现能够帮助其更好地理解现实世界、并顺利按照自然语言给出的描述完成任务。</p><p>&nbsp;</p><p>这对于医疗保健、酒店或家庭环境下的服务型机器人来说尤其有着变革性的影响。在这类环境下，理解物理空间并在其中执行各类任务的能力往往至关重要。</p><p>&nbsp;</p><p>对于AI来说，这种同时处理语言和视觉信息以引导任务执行的能力，也使其距离充分理解情境和环境上下文又更进了一步。而且在此之前，这种能力一直是人类的优势和专利。在CHOIS的支持下，未来的AI系统有望在更多复杂任务中发挥更大的作用，不仅能够消化人类指令的“内容”、更能理解人类指令的操作“方式”，以前所未有的灵活性适应新的挑战。</p><p></p><h2>成果令人惊艳，前景值得期待</h2><p></p><p>&nbsp;</p><p>CHOIS代表了人工智能领域的重大飞跃，特别是在计算机视觉和人机交互领域。通过综合 3D 人与物体交互，CHOIS 可以生成逼真的动画和场景，这对于创建沉浸式虚拟体验至关重要。</p><p>&nbsp;</p><p>该系统使用组合分层方法来理解人类与物体之间交互的复杂本质。这涉及将交互分解为更小的、可管理的部分，并理解这些部分之间的关​​系。模型的层次结构使其能够考虑交互的上下文，例如环境和所涉及对象的属性。</p><p>&nbsp;</p><p>CHOIS 由深度学习算法提供支持，深度学习算法是机器学习的子集。这些算法使系统能够从人与物体交互的大型数据集中学习，随着时间的推移提高其准确性和预测能力。</p><p>&nbsp;</p><p>总体而言，斯坦福大学和Meta的研究人员在计算机视觉、自然语言处理（NLP）和机器人技术交叉领域的这一极具挑战的问题上，成功取得了关键进展。</p><p>&nbsp;</p><p>研究团队认为，他们的工作是建立先进AI系统的重要一步，该系统能够在不同的3D环境中模拟连续的人类行为。CHOIS也为进一步研究如何利用3D场景加语言输入来合成人机交互过程打开了大门，有望在未来孕育出更加复杂的AI系统。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://venturebeat.com/ai/stanford-and-meta-inch-towards-ai-that-acts-human-with-new-chois-interaction-model/">https://venturebeat.com/ai/stanford-and-meta-inch-towards-ai-that-acts-human-with-new-chois-interaction-model/</a>"</p><p><a href="https://isp.page/news/chois-stanford-and-fair-metas-revolutionary-ai-for-realistic-3d-human-object-interactions/#gsc.tab=0">https://isp.page/news/chois-stanford-and-fair-metas-revolutionary-ai-for-realistic-3d-human-object-interactions/#gsc.tab=0</a>"</p><p><a href="https://www.marktechpost.com/2023/12/10/researchers-from-stanford-university-and-fair-meta-unveil-chois-a-groundbreaking-ai-method-for-synthesizing-realistic-3d-human-object-interactions-guided-by-language/">https://www.marktechpost.com/2023/12/10/researchers-from-stanford-university-and-fair-meta-unveil-chois-a-groundbreaking-ai-method-for-synthesizing-realistic-3d-human-object-interactions-guided-by-language/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</id>
            <title>百度8500万挖不来“AI教父”；淘天年薪百万起步抢全球顶尖人才，上不封顶；王慧文病休后首次动作：AI投资｜Q资讯</title>
            <link>https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:07:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里, 年度股息, 淘天集团, 顶尖人才, 百度, AI教父, 比尔盖茨, 收入, 谷歌, Gemini大模型, 王慧文, OneFlow团队, 腾讯视频
<br>
<br>
总结: 阿里将首次派发年度股息，总额近179亿；淘天集团抢全球顶尖人才，年薪百万起上不封顶；百度8500万挖“AI教父”被拒，选择入职谷歌；比尔盖茨每天收入1095万美元，约普通人一生收入4倍；谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑；王慧文病休后首次动作，入股OneFlow团队新创业项目；腾讯视频出现故障，引发用户不满。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>阿里将首次派发年度股息，总额近179亿；淘天集团抢全球顶尖人才，年薪百万起上不封顶；百度8500万挖“AI教父”被拒，选择入职谷歌；比尔盖茨每天收入1095万美元，约普通人一生收入4倍；谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑；王慧文病休后首次动作，入股OneFlow团队新创业项目；卷入300亿骗局官司，京东回应：这是一个匪夷所思的恶意诉讼……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>阿里将首次派发年度股息，总额近179亿</h4><p></p><p>&nbsp;</p><p>12月6日，阿里发布公告，将向截至2023年12月21日收市时登记在册的普通股持有人和美国存托股持有人，就2023财年首次派发年度股息。金额分别为每股普通股0.125美元或每股美国存托股1.00美元，以美元支付。根据披露，股息总额约为25亿美元（当前约179亿元人民币）。阿里称，在现有股份回购计划基础上继续努力提高股东回报。</p><p>&nbsp;</p><p>此次面向全体股东的派息决定，是阿里巴巴2014年上市美股，以及2019年再次回归港股以来首次大规模分红派息。据梳理，阿里巴巴成立以来就以保守的财务经营策略闻名，包括本次分红派息在内，仅有3次分红派息记录。</p><p>&nbsp;</p><p></p><h4>淘天集团抢全球顶尖人才，年薪百万起上不封顶</h4><p></p><p>&nbsp;</p><p>近日，淘天集团启动一项名为T-Star的顶尖人才招聘计划，发放的offer不设层级，采取定制化培养模式，配备“大牛”主管和顶级研发平台资源，年薪百万起上不封顶。</p><p>&nbsp;</p><p>根据淘天集团招聘官网公布的信息，目前，T-Star计划已经开放了10种算法工程师岗位，工作方向包括自然语言处理、机器学习、多模态、三维重建、计算机视觉、3D等，工作地点为杭州、北京等。</p><p>&nbsp;</p><p></p><h4>百度8500万挖“AI教父”被拒，选择入职谷歌</h4><p></p><p>&nbsp;</p><p>12月4日，据知情人士透露，百度公司曾出价1200万美元(约合8486万元人民币)邀请“AI教父”杰弗里·辛顿(Geoffrey Hinton)及其学生加入公司，但被拒绝。“我们不知道自己值多少钱。”辛顿表示。他咨询了收购方面的律师和专家，想出了一个计划：“我们将组织一场拍卖，自己兜售自己。”</p><p>&nbsp;</p><p>最终，辛顿博士和他的学生们在4400万美元(约合3.1亿元人民币)的价格上停止了这次拍卖。虽然出价仍在上升，但他们想为谷歌工作。这一报酬已经很惊人。据悉，今年5月辛顿宣布从谷歌离职。辛顿表示，从谷歌辞职是为了可以自由地谈论AI的风险。他说，现在对自己一生从事的工作感到有些后悔。</p><p>&nbsp;</p><p></p><h4>比尔盖茨每天收入1095万美元，约普通人一生收入4倍</h4><p></p><p>&nbsp;</p><p>根据求职信息网站Zippia的数据，一个普通人一生的平均收入约为270万美元，而比尔·盖茨一天的收入大约是这个数字的3~4倍。据预计，盖茨每天的收入约为1095万美元，相当于每秒约117美元。还有另外一组数据显示，盖茨每天进账约760万美元，相当于每小时319635美元。</p><p>&nbsp;</p><p></p><h4>谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑</h4><p></p><p>&nbsp;</p><p>谷歌 12 月 6 日宣布推出其认为规模最大、功能最强大的人工智能模型 Gemini。Gemini 将包括三种不同的套件：Gemini Ultra、Gemini Pro 和 Gemini Nano。根据谷歌给出的基准测试结果，Gemini 在许多测试中都表现出了“最先进的性能”，甚至在大部分基准测试中完全击败了 OpenAI 的 GPT-4。</p><p>&nbsp;</p><p>同时，谷歌也发布了Gemini Ultra官方演示视频，展示了这款模型的强大能力。不过，依然有人质疑Gemini的能力。</p><p>&nbsp;</p><p>据报道，Gemini在MMLU多任务语言理解数据集测试中显示出色，但对比GPT-4时的提示技巧和展示方式引发了争议。质疑者认为，Gemini在使用提示技巧+32次尝试的标准下超越了GPT-4，但这一标准是否公平受到质疑。图表比例尺的问题也被揭示，引起了技术主管的修正。Gemini发布的视频在展示时也引起了关注，部分观点认为其中可能存在剪辑和非实时录制。</p><p>&nbsp;</p><p>查看更多：</p><p><a href="https://mp.weixin.qq.com/s/Yqi4rcyEmvg9g6LCqbxYxA">刚发布就被质疑？超过 GPT-4 的“最强”大模型 Gemini、“最高效”训练加速器，谷歌到底行不行</a>"</p><p>&nbsp;</p><p></p><h4>王慧文病休后首次动作，入股OneFlow团队新创业项目</h4><p></p><p>&nbsp;</p><p>在病休近6个月后，王慧文突然有了新动作，再次与袁进辉牵手，入股其创业新公司硅动科技。据公开资料显示，就在这两日，北京硅动科技有限公司新增王慧文为股东，注册资本由100万人民币增至约105.26万人民币。也就是说，王慧文目前在袁进辉新公司的持股比例约为5%。</p><p>&nbsp;</p><p>OneFlow是国内知名开源深度学习框架及开发平台。其团队上次创业一流科技时，由王慧文的光年之外收购其46.52%股权。不过，随着6月底光年之外创始人王慧文病退消息曝光，美团收购光年之外100%的权益，一流科技OneFlow团队作为其核心资产也转归美团名下。在50天后，袁进辉宣布带领OneFlow原班人马再次创业。消息传出后不到半个月，硅动科技注册成功。</p><p>&nbsp;</p><p></p><h4>“腾讯视频崩了”上热搜</h4><p></p><p>&nbsp;</p><p>12 月 3 日晚，腾讯视频出现网络故障，有网友反馈出现首页无法加载内容、VIP 用户看不了会员视频等情况。#腾讯视频崩了# #腾讯会员 没了#词条相继冲上微博热搜。</p><p>&nbsp;</p><p>稍晚些时候，@腾讯视频就“App 崩了”发布致歉声明称，目前腾讯视频出现了短暂技术问题，我们正在加紧修复，各项功能在逐步恢复中。感谢您的耐心等待，由此给您带来的不便我们深感歉意。</p><p>&nbsp;</p><p>除了腾讯视频，近期遭遇宕机事件的还有滴滴、淘宝、闲鱼、钉钉、阿里云盘等多个App。据媒体统计，以此被多家媒体报道或登上热搜榜为基准，2022年约发生了9起宕机事件，而今年以来，类似的事件已发生14起。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/-KVKVfq0CayLyRkEbp2Rcg">互联网大厂“组团”宕机，都怪降本增“笑”？</a>"</p><p>&nbsp;</p><p></p><h4>被卷入300亿骗局官司，京东回应：这是一个匪夷所思的恶意诉讼</h4><p></p><p>&nbsp;</p><p>12月4日消息，最近京东集团、承兴集团、诺亚财富之前的各种消息闹得沸沸扬扬，京东还被告上法庭。据悉，此事起因是承兴集团的罗静造假，冒充京东工作人员，并私自刻章，以京东、苏宁等应收账款来找金融机构（诺亚财富）贷款，最终骗走300亿并跑路暴雷，结果被抓。诺亚财富一纸诉状把京东给告上法庭，想让京东还钱。</p><p>&nbsp;</p><p>对于此事，京东集团官微“京东发言人”最新发布一份声明回应，称京东作为毫不知情的受害者，被卷入历时四年的恶意诉讼中，公司的声誉和权益遭受重大损失，相信法院会有公正的判决。12月4日晚，诺亚财富发布声明称，已关注到京东集团发布的关于我司的声明，该声明中“诺亚财富近年来先后发生十余起类似事件，上百亿基金......”等描述严重失实，已侵犯了我司名誉，我司将采取法律措施，维护自身合法权益。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>小米14手机内核已在GitHub开源</h4><p></p><p>&nbsp;</p><p>据报道，小米14/Pro内核现已在Github开源，AOSP版本基于Android U。公开内核源码可以让第三方开发者进行修改，开发人员和愿意折腾的用户能够充分利用硬件的潜力，市场上也会很快出现该机型的第三方固件。</p><p>&nbsp;</p><p>开源地址：</p><p><a href="https://github.com/MiCode/Xiaomi_Kernel_OpenSource/tree/shennong-u-oss">https://github.com/MiCode/Xiaomi_Kernel_OpenSource/tree/shennong-u-oss</a>"</p><p>&nbsp;</p><p></p><h4>员工称亚马逊AI聊天机器人Q “幻觉”严重，且泄露公司机密数据</h4><p></p><p>&nbsp;</p><p>根据国外科技媒体披露的一份内部文件，亚马逊员工称旗下AI聊天机器人Q存在严重的“幻觉”问题，并泄露了包括AWS数据中心位置、内部折扣计划等诸多机密信息。报告文件显示，亚马逊Q会产生幻觉，返回有害或不适当的聊天内容。例如，亚马逊Q会返回过时的安全信息，可能会让客户面临风险。</p><p>&nbsp;</p><p>亚马逊淡化了员工讨论的重要性，并声称没有发现任何安全问题。然而，泄露的文件引发了人们对Q准确性和安全性的担忧，Q仍处于预览阶段，尚未正式发布。文章发表后，该发言人发布一份声明，反驳了员工的说法，称亚马逊Q没有泄露机密信息。</p><p>&nbsp;</p><p></p><h4>Meta 推出独立的 AI 图像生成器，目前免费但只支持英文提示词&nbsp;</h4><p></p><p>&nbsp;</p><p>Meta 公司日前推出全新的、独立的 AI 图像生成器 ——Imagine with Meta，允许用户通过自然语言描述来创建图像。据介绍，新的人工图像生成器由 Meta 现有的 Emu 图像生成模型提供支持，可根据文本提示创建高分辨率图像。它目前对美国的英语用户免费使用（后续是否收费未知），并且每个提示都会生成四个图像。</p><p>&nbsp;</p><p>此前，Meta 图像生成模型因带有种族偏见的图像贴纸而面临争议。为了解决此类问题，Meta 表示将开始向 Imagine with Meta 生成的图像添加隐形水印，这些水印将由人工智能模型生成，并可由相应模型检测，以提高内容透明度。</p><p>&nbsp;</p><p></p><h4>Hugging Face 现 API 令牌漏洞，黑客可获取微软、谷歌等模型库权限</h4><p></p><p>&nbsp;</p><p>安全公司 Lasso Security 日前发现 AI 模型平台 Hugging Face 上存在 API 令牌漏洞，黑客可获取微软、谷歌、Meta 等公司的令牌，并能够访问模型库，污染训练数据或窃取、修改 AI 模型。由于平台的令牌信息写死在 API 中，因此黑客可以直接从 Hugging Face 及 GitHub 的存储库（repository）获得平台上各模型分发者的 API 令牌（token），安全人员一共从上述平台中找到 1681 个有效的令牌。</p><p>&nbsp;</p><p></p><h4>支付宝、麦当劳中国等相继启动鸿蒙原生应用开发</h4><p></p><p>&nbsp;</p><p>12月7日，支付宝与华为终端宣布合作，基于HarmonyOS NEXT启动支付宝鸿蒙原生应用开发，华为常务董事、终端BG CEO、智能汽车解决方案BU董事长余承东和蚂蚁集团董事长兼首席执行官井贤栋均现身签约现场。</p><p>&nbsp;</p><p>无独有偶，12月6日，麦当劳中国也与华为达成鸿蒙合作协议，正式宣布麦当劳中国APP将基于HarmonyOS NEXT启动鸿蒙原生应用开发，成为首批启动鸿蒙原生应用开发的全球大型跨国连锁餐饮企业，该公司在中国市场拥有5500多家餐厅，每年服务顾客超十亿人次。</p><p>&nbsp;</p><p>随着华为宣布鸿蒙原生应用全面启动，近期美团、去哪儿、新浪、钉钉、蚂蚁集团、小红书、58集团、哔哩哔哩、高德地图等互联网公司均已宣布加入鸿蒙原生应用开发行列。</p><p>&nbsp;</p><p></p><h4>IntelliJ IDEA 2023.3 版本更新发布</h4><p></p><p>&nbsp;</p><p>IntelliJ IDEA 2023.3 版本更新现已发布，在这一版本中，JetBrains 表示 AI Assistant 持续演进，现已超越技术预览阶段，获得了大量令人期待的改进。在其他方面，此版本包括对最新 Java 21 功能的全面支持，引入了带有编辑操作的直观浮动工具栏，并添加了 Run to Cursor（运行到光标）嵌入选项来增强调试工作流。IntelliJ IDEA Ultimate 现在提供无缝的开箱即用 Kubernetes 开发体验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</id>
            <title>分布式数据库 GaiaDB-X 金融应用实践</title>
            <link>https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:03:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 银行新一代核心系统建设背景, 架构, 分布式架构, 分布式数据库
<br>
<br>
总结: 银行业务的快速发展和迭代更新速度加快，使得原有的基于大型机的核心系统架构无法满足需求。为了支持业务的持续增长和创新，银行开始将核心系统从大型机下移到通用服务器架构上，并采用分布式架构和分布式数据库来满足分布式扩展性、强一致性和容灾能力的要求。这使得银行核心系统的架构与互联网公司的技术体系越来越接近，未来银行业与互联网业的技术交流和人才流动将进一步增加。 </div>
                        <hr>
                    
                    <p></p><h2>一、银行新一代核心系统建设背景及架构</h2><p></p><p></p><p>在银行的 IT 建设历程中，尤其是中大行，大多都基于大型机和小型机来构建核心系统。随着银行业务的快速发展，这样的系统对业务的支持越来越举步维艰，主要体现在以下四个方面：</p><p></p><p>首先是难以支持银行快速发展的业务。随着国内电商、互联网支付、手机支付的迅猛发展，银行的交易量出现指数级的增长。比如我们的某银行客户，当前每秒的交易量峰值在一万多左右，预计在未来几年会逐渐增长到每秒 6 万笔交易，而且后续还会继续增长。在这种情况下，基于大型机的集中式架构，单纯靠硬件的升配，已经无法支持业务的持续增长。第二是难以匹配银行系统的迭代更新速度。原有的基于大型机的胖核心架构，迭代周期往往在数月甚至半年。但随着银行间、以及银行与互联网金融企业之间的竞争加剧，银行迫切需要快速推出新业务进行创新，他们也希望能够像互联网公司那样，能够按周级进行快速迭代，快速响应业务需求。第三是系统风险。银行业迫切需要做到软件及硬件的自主可控。第四是生态封闭。大型机技术发展慢、人才难招。现在再去外面招一个懂 IBM 大型机的人已经很难了。</p><p></p><p>因此，在国家现有政策的引导下，各大银行最近几年都在做一个事情：把原有的核心架构从大型机下移到传统的通用服务器架构上，并建设一套自主可控的新技术体系，简称核心系统下移。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2eebfc9abdbfbf4124d2a6b76ebcb3c.png" /></p><p></p><p>在进一步理解银行系统之前，我们先了解下银行客户的业务体量、业务需求以及核心系统对业务支持情况。以一个国有大行来举例：它的客户量在 5-7 亿，有 10-20 亿账户，在全国有 2-4 万个网点。从每秒峰值交易量来看，约为每秒 5-8 万笔交易。</p><p></p><p>具体到数据库层，支持以上业务还需要联机交易系统，例如存贷汇业务。数据库层最大的表有百亿级记录数，TPS 达到百万级。此外，统一查询业务要求支持近十年交易明细的查询，即万亿级的查询记录数。即使放到互联网公司用的通用服务器，也是需要上千台服务器才能支撑相关业务的开展。</p><p></p><p>通过以上的介绍，大家可以发现，银行客户的业务体量和数据量已经达到了大型互联网公司的量级。如果想把这个系统从大型机下移到通用服务器架构，那么原有的集中式架构肯定是无法满足的，必须像互联网公司一样采用分布式架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ede303f87fe7f77d969325bf8b0660f.png" /></p><p></p><p>因为银行有和大型互联网公司相近的业务体量，因此在技术体系上，也借鉴了互联网公司的技术体系。</p><p></p><p>从 IaaS 层来看，银行采用了 X86、ARM 架构的通用服务器，也用到了混合云技术，大量使用了虚拟机与容器服务。</p><p></p><p>在 PaaS 层，银行使用了大量的分布式系统，如开源的微服务框架（例如 SpringCloud ），以及开源的或者商业的数据库，包括分布式/单机/关系型/缓存型的数据库，以及日志数据库 ES、时序数据库等。在中间件层，也用到了大量的开源的或者基于开源改造后的组件，例如消息队列、对象存储、分布式锁等。</p><p></p><p>在 SaaS 层，银行主要通过单元化 + 微服务的架构来实现分布式扩展。银行将自身业务应用划分成三种单元。</p><p></p><p>最上层是全局单元，主要是起到全局路由及流量分发的作用。第二层是业务单元，核心的业务逻辑都在该部分来实现。同时，为了实现业务的横向扩展并支持数亿客户量，银行业跟互联网公司一样，对业务进行单元化拆分。例如我们接触到的某银行，就是将自身的业务拆分为了 16 个业务单元，每个单元五千万客户， 16 个单元部署在两个机房形成同城双活的架构。最底层是公共单元，一些不太好或没必要做单元化拆分的应用，放在公共单元提供公共服务。</p><p></p><p>通过上述分析可以看到，在新一代银行核心系统里面，整体的架构体系已经和互联网公司很接近了，大家用的都是相同的技术栈，只是服务的业务场景不同。在未来，银行业跟互联网业的技术交流会进一步紧密，人才的流动也会进一步频繁。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6dada50bc1d570cafbcad31f40a441dc.png" /></p><p></p><p>在业务采用了单元化划分后，数据库的架构是怎么样的呢？目前在银行的新核心系统下移中，主要采用了以下两种数据库架构：</p><p></p><p>第一种是单机数据库架构。这种数据库架构比较简单，故障域较小，但相对来说业务系统会比较复杂。因为有些模块，如全局路由模块，是全局的总入口，没法做单元化拆分。因此一组单机数据库无法满足性能与容量需求，依然需要在业务层做数据拆分。除此之外，单机数据库无法完全支持业务单元层的业务落地。前面提到，我们接触到的某银行一个业务单元要承担五千万的客户数，一组单机数据库依然无法支持。于是在业务层进一步拆分为 4 组数据库共 64 张子表，业务层需要去解决大量的拆分及分布式事务相关的业务逻辑，整体就更复杂了。</p><p></p><p>另外一种是分布式数据库架构。这样的数据库内部架构虽然更为复杂，但它可以提供更好的性能。对业务层来说，一个单元采用一组数据分布数据库即可，业务层的逻辑就更为简单了。</p><p>因此我们认为，随着分布式数据库的逐步成熟与应用逐渐广泛，业务单元化 + 分布式数据库会逐渐成为流行的银行业务架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/1805f802303d1452f0ddc0dcf9e08ef3.png" /></p><p></p><p>综上，银行核心系统在下移的场景下，对数据库在如下几个方面提出了要求：</p><p></p><p>第一是分布式扩展性。由于采用了通用服务器，它的单机性能要远弱于大型机或者小型机。在这种情况下，数据库需要具备分布式可扩展的能力来满足上亿客户的金融需求。第二是强一致性。金融场景对数据正确性、一致性的要求极高。因此要严格保障对事务的 ACID 特性。否则，业务层就要做大量的工作。第三是容灾能力。通用服务器在硬件故障率方面要比大型机、小型机高很多。因此需要我们的数据库有更为可靠的可用机制以保障 SLA。同时，监管对于容灾能力的要求也在不断提升。比如，对于新建设的核心系统，监管要求必须满足 5 级以上的容灾能力，且满足同城双活并保证 RPO 为 0。在具体执行上，监管的要求也越来越严格，比如同城双活，之前是只需要具备相关的技术方案即可，但现在每年人行的监管都会直接到现场，要求做机房级实战故障切换。第四是运维能力。系统下移到通用服务器并实现去 IOE，数据库节点数量要出现 50 倍的增长。以我们的一个银行客户举例，从小型机下移后，数据库节点数从 20 增长到 1000（当然这里面也预留了几倍的性能增量）。在和客户的交流过程中，他们也认同这个观点，认为系统下移后，节点数要增长一到两个数量级。但运维的人力不可能因此增加几十倍，在这种情况下，就要求我们的运维效率要有质的提升，需要能够智能化、自动化去做相关的运维工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e70f9a0a35e389e4bda5d5de854c137.png" /></p><p></p><p></p><h2>二、分布式数据库&nbsp;GaiaDB-X 的金融场景方案</h2><p></p><p></p><p>接下来我们分享第二部分，分布式数据库 <a href="https://xie.infoq.cn/article/1febbf974afe91b9a1e11517f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">GaiaDB-X</a>" 针对金融场景的解决方案。</p><p></p><p>GaiaDB-X 数据库是<a href="https://www.infoq.cn/article/WrlUWpf2OkgQsSAD6NJ1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"研发的 Shared Nothing 架构的分布式数据库，它可以基于通用服务器做横向扩展，来满足高性能、大数据容量的需求。</p><p></p><p>总体来看它分为计算层、存储层、元数据三层架构：</p><p></p><p>计算层是无状态、可横向扩展的一层。它对外兼容 MySQL 协议，接收到 SQL 请求后，再经过 SQL 解析、权限检查、逻辑优化、物理优化之后，生成 DistSQL 下发到各个存储层的分片。为了达到更好的性能，逻辑与物理上尽量将数据过滤及计算能力下推到存储层，收到结果后，再把数据进行聚合计算，最后返回给客户端。计算层的下面是存储层。它采用多分片的方式进行扩展，数据按照一定的分片规则打散到了各个分片中。我们支持 Hash、Range、List 等分区方式来做数据分区。同时，分片内数据采用多副本的方式来保证可靠性，第三是 GMS 节点，即全局元数据管理模块。它用来管理全局性数据，例如表的 Schema 信息、权限信息、表的路由信息，还有后面要介绍到的用于做分布式事务的全局逻辑序列号。GMS 也采用多副本的方式，采用 Raft 协议进行数据同步。</p><p></p><p>在最底层是我们的统一数据库管控平台，来实现对数据库集群的管理。比如在<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbeb6a36cc9ce320cf9c590dcbaa67d53421a43a56ea8378ce2b96f0c1c3c3f17265ea54d9b9&amp;idx=1&amp;mid=2247570425&amp;scene=27&amp;sn=bc5f1eeba437ce5e619ccef9bc0321d3&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">百度</a>"集团内部数十万的数据库节点，都是由该管控平台来管理的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50f58801a7c4eeb2bddfece8e453df59.png" /></p><p></p><p>GaiaDB-X 数据库是百度集团发展历史最久、应用最广泛的一款数据库，到现在为止已有 18 年的发展历史。它的发展也与百度的业务发展息息相关，大概可以归纳为四个阶段：</p><p></p><p>第一阶段是从 2005 年开始，为了满足搜索、社区业务中大量读请求的场景，我们通过一主多从的集群化外加读写分离来扩展读性能。第二阶段是为了支持凤巢广告系统与百度网盘，满足它们对万亿级数据量的需求，我们开始做分布式系统。到了 2014 年，我们就已经在凤巢广告系统中替换了基于 Oracle 的盘柜，为凤巢每年节省了上千万的成本。针对百度网盘，我们有个 3000 台服务器的集群支持网盘业务，所有网盘文件的元数据信息都存储在这里，最大的表达到万亿级记录数，至今仍是国内最大的关系型数据库集群之一。第三阶段是随着百度钱包等泛互联网业务的兴起，对数据的一致性要求更高了。因此，我们实现了分布式事务强一致的特性，保障金融数据的正确性。第四阶段，也就是现在。随着百度智能云对外技术输出，我们已经把数据库输出到了十余个行业，覆盖 150 家客户。在金融行业，GaiaDB-X 已经承接了金融核心业务，包括百信银行、银联商务、某交易所及国有行等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43f859c5ceefd365e4a3567012ce9242.png" /></p><p></p><p>对于分布式数据库，水平扩展能力是其核心能力。除了在计算层与存储层做水平扩展外，我们还要着力去解决影响我们扩展能力的单点。</p><p></p><p>第一个是 GMS，即全局元数据模块。因为它要提供一个全局递增的全局逻辑时钟，每一次事务都需要调用它。为了避免其成为瓶颈，我们采用批量预分配的方式来提升其总吞吐，在此模式下，其每秒可分配 1200 万个 TSO 序号，远超出百万级 TPS 的需求。</p><p></p><p>第二个是全局事务表。为了保证分布式事务的原子性，我们需要将正在进行中的事务保存到一张全局表中，因此它的性能也会直接影响到全局性能。我们采用自管理的方式，将全局事务表做成分布式表，分布式地存储在集群中，这样就可以实现分布式扩展。</p><p></p><p>在实际应用中，比如说像 19 年的春晚抢红包，我们支持了三亿用户抢红包，支撑了峰值达每秒 12 万交易量。除此之外，针对某银行拥有 8000 万账户的核心账务系统，我们也平稳支持了其 6 万每秒的 TPS ，充分验证了 GaiaDB-X 的水平扩展能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/560fef70a125720cb796e7ddf53eb3d3.png" /></p><p></p><p>除分布式外，我们也支持单机场景，实现了单机分布式一体化。为什么需要单机分布式一体化呢？以我们的一个银行客户来说，全行的业务系统有 200 多个，其中大部分系统（大概占 70% 左右）对性能与吞吐的要求并不高，一组单机数据库就能够满足其业务需求。但对于剩下的 30% 业务，它对性能的要求是单机数据库无法满足的，需要分布式数据库来满足其扩展性。</p><p></p><p>因此我们通过一体化的方式，来满足银行不同体量的业务对于数据库的需求。同时，我们也具备单机数据库扩展为分布式的能力，在对应业务的数据量增长后，能够扩容为分布式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15483b1c6cffdb73ad5178a621526e14.png" /></p><p></p><p>扩展性的另外一个目的是自动做数据分离。在金融场景里面，存在多个业务共用一个数据库集群的场景，比如业务分为联机交易系统与查询类系统，数据库便对应划分为交易库和历史库两个。</p><p></p><p>对于交易库来说，只保存联机交易会频繁使用到的数据。例如账务结果数据及当天的交易记录，以满足对交易业务的快速响应。对于查询不那么频繁的即时交易记录，这可能就是一个相当大的数据量，一般都能达到千亿甚至万亿级别。这时，我们就可以将这个数据自动转移到历史库上去，用更高密度的存储机型来存储。一方面可以降低硬件成本，同时也可以避免对联机业务的影响。</p><p></p><p>这样做对业务来说，对外呈现的是一套数据库，业务可以根据需要来处理不同数据分区的逻辑，也不用在多套库之间通过 DTS 做数据同步。同时还把交易数据和历史数据做分离，以保障对联机业务的性能，同时也满足了查询业务的查询需求，避免其相互影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09461237b2307a25d9e1a17349640b7b.png" /></p><p></p><p>在金融场景中，对事物的 ACID 特性有着严格的要求：</p><p></p><p>持久性。指的是事务一旦提交就不会丢失，一般通过多副本 + 强同步来解决。原子性。一个事务要么全部提交，要么全部回滚，不存在部分提交的情况。通常，数据库的 XA 协议，通过两阶段提交能解决这个问题。</p><p></p><p>但是 XA 协议不能很好地满足一致性与隔离性。以简单的转账场景为例，A、B 原来各有 100 块钱，总共 200 块。然后 A 给 B 转账 50 块钱。此时，我们会先给 A 扣 50，再给 B 加 50。如果通过 XA 协议来做的话，先走 prepare 再 commit，我们可以看到，commit（图中第 7、第 8 步）过程不是原子过程，存在一个执行时间差。在这个时间差内，另外一个事务去读取数据，就可能存在读到提交了一半的数据，A 和 B 的总和是 150 而不是 200。这是 XA + 2PC 解决不了的问题。</p><p></p><p>为了解决这个问题，业内一般是引入一个全局时钟来做一致性的保证。通常有三种实现方式：</p><p></p><p>TrueTime 方案。这个是大家比较熟悉的方案，Google Spanner 也发过论文。但它的缺陷是依赖硬件，要引入 GPS 与原子钟，这个一般来说是难具备的。HLC 方案。采用该方案的典型数据库系统是 CockroachDB，它的优点是不依赖硬件，而且是去中心化的。但是缺点也很明显，一致性比较弱，而且要求服务器间的时钟误差不能超过 250ms，否则就无法正常运行。TSO 方案，比如 TiDB 就是采用了这种方案。TSO 本质上来说是一个全局唯一而且自增的逻辑序列号，一个事务在开始时与事务 commit 时需要两次找 GMS 获取序列号，然后把 TSO 号作为版本号提交到存储层，存储层的 MVCC 机制来判断数据是否可见。它不需要硬件具备强一致性，但缺点是依赖一个全局中心的时钟分配器 GMS。但这并不是一个问题，因为刚刚我们也提到了，虽然 GMS 不具备扩展性，1200 万的 TPS 已经完全满足业务的常规需要了。因此我们最终采用了这种方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b192d1774be97ac09d153b3e3fcf81d9.png" /></p><p></p><p>除了保障事务的一致性外，我们还需要保障上下游系统的数据一致性。在开始之前，我们首先要讲一下银行的典型业务场景，它一般分为三个子系统：</p><p></p><p>第一个是联机交易系统，例如存取款、在线支付等。这个系统的并发量高、延迟敏感，直接影响到用户体验。第二个是跑批类的业务系统。例如结息，每天晚上半夜计算前一天的利息。这些业务是后台业务，有大量的读取与计算操作，延迟不敏感，但是对数据一致性要求高。怎么能够让这样的业务尽量避免对在线业务的影响，同时又能够读取到最新的数据呢？我们的方式是让跑批业务去读取从库数据，从而避免对主库的性能影响，同时结合 TSO，即全局逻辑时钟的对位对齐机制。等到从库水位追齐之后，才返回读取数据。当然这会引入一定的延时，但是因为跑批业务对响应延时不那么敏感，所以是可以接受的。第三个子系统是大数据类的离线业务。通常来说，就是银行内部的各种大数据、数仓等系统，需要把数据库的数据同步过去。这样的业务对实时性要求不高，但要求最终的数据是一致的。由于各个计算节点都会承担流量，也会生成 BinLog。因此，如何对多份 BinLog 进行排序，让它们能够严格保持时序是我们需要解决的问题。我们的全局 BinLog 有两个模块，一个是 pump，从各个CN 节点抽取 BinLog，然后在 Sorter 节点基于 TSO（全局逻辑时钟）进行排序，来保障形成一个全局时序正确的 BinLog，以保障这个离线系统收到的数据的最终正确性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17a48e22473b043b5053080788477dae.png" /></p><p></p><p>接下来我们看一下容灾能力，除了对单机故障的容灾之外，还有对特定机型的容灾。因为银行把系统下移到通用服务器，通常都是通过两阶段来实施：第一阶段是下移到 X86 的 CPU 芯片上，这个过程银行、互联网厂商都一定的经验。第二阶段是要实现服务器芯片的基本国产化，就是说使用国产的鲲鹏、飞腾或海光 CPU 芯片，这方面大家都是在探索性的开展相关业务。</p><p></p><p>以百信银行举例，它与其母行中信银行一样，选择了基于鲲鹏做国产化的路线，而且在业内比较超前。相较于其他银行仅在周边系统或者数据库从库来做国产化，百信银行的周边业务跟核心系统都和主站一样基于鲲鹏服务器来做，这个在业内是较为领先的。</p><p></p><p>为了保证客户业务系统实现平滑的国产化，我们在产品上实现了一库多芯的方案，主要资源池基于鲲鹏，但放置了一个独立 X86 资源池，在技术上实现托底，同时也能够将原有换下来的服务器能够利用上，避免资源浪费。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/173396e11937ad6b3f0e88e0a6e8fb31.png" /></p><p></p><p>根据人行的监管要求，银行核心系统一般要具备 5 级以上的容灾能力，这就要求具备两地三中心的容灾能力。</p><p></p><p>下图是我们客户的一个典型机房部署架构。首先在北京有两个同城机房，其物理距离在 50-100 公里左右，网路延迟在 1ms 左右。同时还有一个异地机房做灾备，物理距离一般是 1000 公里左右，比如合肥，网络延时是 10ms。</p><p></p><p>同城两个机房业务做双活部署，同时接受业务流量。数据库在两个机房采用 3 + 2 的部署形式，机房间采用强同步的方式来保障在发生机房级的故障之后，数据库能进行故障切换，而且保障数据的 RPO 为 0。</p><p></p><p>为保证单机房故障后的零数据丢失，我们采用分组强同步的方式，将 id1、id2 各划分一个复制组，复制组间采用强同步的方式。每个复制组保证至少有一个副本接收到数据之后才返回成功。这样在容忍少数副本故障的同时也能够保证单个机房故障后的零数据丢失。</p><p></p><p>异地机房的目标是当北京的两个机房都出现灾难性的事件之后，能够降级完成核心业务。它采用异步级联的方式来同步数据，首先异步是为了避免阻塞对主地域的数据库写入；采用级联方式，没有跟主地域机房形成复制组，主要一个为了保持灾备机房的数据库的拓扑独立性，减少依赖，保障在关键时刻可切换，另外也是降低跨地域同步的数据量，只需要同步一份数据即可。</p><p></p><p>结合在多家金融客户的实践，我们和中国信通院一起发布了金融数据库的容灾技术报告《金融级数据库容灾备份技术报告（2021 年）》。大家可以在公众号后台回复「金融级数据库容灾备份技术报告」获取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c35b7ca466e4e027b09f699a6c555063.png" /></p><p></p><p>最后一部分是运维能力。核心系统下移及国产化的背景之下，数据库系统呈现两个变化：</p><p></p><p>一是数据库的节点数出现了 50 倍的增长，这里面既有技术架构的原因，也有数据库预留了一定的性能空间的原因。</p><p></p><p>二是数据库的种类也变多了。对于银行系统来说，之前基本就是选择 Oracle 或 DB2。现在则开始使用多种开源数据库和国产数据库。除此之外，为了避免像之前一样被单一厂商绑定，银行也会特意选择多家数据库厂商，在这种情况下，对银行的运维的挑战就更大了。</p><p></p><p>因此，结合百度集团及百度智能云管理大规模数据库节点方面的经验，我们将 GaiaDB-X 数据库云管平台进一步泛化，形成了具备管理多元数据库能力的统一平台，它除了能够管理 GaiaDB-X 自身，也能管理其他的开源数据库。通过帮助银行建立企业级的 DBPaaS 管理平台，进一步提升了银行的运维效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5b5b67c12e920a404b8a06be4702a30.png" /></p><p></p><p></p><h2>三、金融应用案例介绍</h2><p></p><p></p><p>接下来，我来分享百度智能云在金融方面的一些典型案例。</p><p></p><p>首先是百信银行。它的特点是完全去 O，是一家完全没有 Oracle 的银行。全行 200+ 业务系统，无论是核心账务系统还是周边系统，几乎全部是基于 GaiaDB-X 数据库来构建的，至今已经平稳运行五年。</p><p></p><p>按数据库节点数计算，百信银行目前的数据库国产化率达到了 99.93%，遥遥领先于行业平均水平。</p><p></p><p>同时，百信银行在容灾和国产化领域也比较领先，在 2019 年就完成了全行主要系统的同城双活建设，2022 年开始将全行业务逐步迁移到基于鲲鹏的国产云上，进而实现了全栈的国产化。</p><p></p><p>在硬件成本方面，我们通过采用通用服务器来替代 IOE 硬件，帮助百信银行的单账户平均硬件成本降低了 70% 以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5eba560cf050d18ba1d18d015ebab4c0.png" /></p><p></p><p>下图是人行下面直属的某交易所。因为是涉及到国家金融稳定，所以在核心系统上需要逐步摆脱对 Oracle 的依赖，并拥有两地三中心的容灾能力。</p><p></p><p>由于当前的一些数据库都不能满足他们的业务场景需求，因此该交易所和百度采用了联合开发的模式，共建数据库能力。在两年的合作过程中，我们从外围的信息管理系统入手，逐步深入到核心交易系统，再到离线库数据分析系统，进而逐步实现数据库国产化。</p><p></p><p>此外，由于交易所的交易系统对低延时的要求较高，同时基于容灾要求又有同城双活的要求，如何在跨机房的情况下保障交易延时就成了亟待解决的问题。因此我们共同建设了 Collocate 机制，来尽量减少跨机房数据的访问，最终将交易延时从 80 毫秒降低到了 15 毫秒。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3bbb84070b281ebbedf2e41e6edc456.png" /></p><p></p><p>下图是国内某国有大行客户。他们在最近两年把原有的基于小型机的核心系统下移到了通用服务器中，数据库也从 Oracle 替代为了开源单机数据库。</p><p></p><p>但在这个过程中，该行面临两个问题：一是数据库节点数增长了 50 倍，服务器数量到达了 1000 台左右，如何做好数据库的自动化部署、上线变更、版本升级、参数管理、性能诊断等工作。二是如何结合业务的单元化，形成业务与数据库的同城双活与异地容灾能力。</p><p></p><p>借助百度智能云提供的统一数据库管控平台的能力，历时两年，我们与客户一起实现了新核心系统的全面投产，也顺利通过了人行的验收。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b935db0fd9ff535ed78234e80ac12688.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</id>
            <title>深度解读“百度智能云数据库”的演进：互联网→云计算→ AI 原生</title>
            <link>https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:50:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, AIGC, 数据库发展, AI时代
<br>
<br>
总结: ChatGPT的爆火引发了AIGC赛道的关注，展示了弱人工智能向强人工智能的跨越式发展。AIGC将改变数据库行业，AI时代将带来数据库的变革和创新。 </div>
                        <hr>
                    
                    <p></p><h2>一、数据库行业发展概述</h2><p></p><p></p><p>如果说今年科技圈什么最火，我估计大家会毫不犹豫选择 ChatGPT。ChatGPT 是 2022 年 11 月 30 日由 OpenAI 发布的聊天应用。它创造了有史以来用户增长最快的纪录：自 11 月 30 日发布起，5 天就拥有了 100 万活跃用户，两个月就达到了一亿用户。对比其他热门应用，同样达到一亿用户量级，TikTok 花了九个月，而像 Instagram ，Whatsapp 等应用则超过了两年时间。</p><p></p><p>ChatGPT 的爆火，瞬间点燃了整个 AIGC 赛道。最关键的原因在于，它让大家看到了弱人工智能向强人工智能的跨越式发展。英伟达 CEO 黄仁勋对此评价：ChatGPT 相当于 AI 界的 iPhone 时刻。</p><p></p><p>现在业界统一的共识是，AIGC 会改变 IT 行业的方方面面。那 AIGC 对数据库会带来哪些变化，AIGC 和数据库又会碰撞出哪些火花，这是一个值得我们去思考和回答的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/648f846dd8384dfa9a1fc1a1ea4abf3e.png" /></p><p></p><p>在回答 AIGC 对数据库的变革和影响之前，让我们先回顾下数据库发展历史。它可以分为六个阶段。</p><p></p><p>第一阶段是上世纪五十年代。这个时候数据库还在雏形阶段，以层状数据库和网状数据库为主，基础设施以大型机为主，主要用于国防和科学研究。</p><p></p><p>第二阶段是上世纪七十年代。关系型数据库出现，硬件也变成了小型机，这也奠定了数据库发展的方向。主要应用在金融，交通等关键行业。这时的代表数据库是 Oracle 和 DB2 等。</p><p></p><p>第三阶段是上世纪九十年代。PC 机已经得到了普及，数据库除了关系型数据库，也有了 PC 单机数据库。为解决企业 BI 应用诉求，数仓开始出现。数据库的应用也更多样化起来，进一步应用到企业 BI、个人办公、娱乐等场景。</p><p></p><p>第四阶段是本世纪的前十年。随着互联网开始繁荣，数据处理的需求逐渐增加，开始出现企业数据中心。业务也变成了媒体、搜索、电子商务、社交等互联网业务。由于传统数据库如 Oracle 因为价格较贵，互联网厂商大量使用开源数据库如MySQL、Redis、MongoDB 等。整个开源数据库生态开始逐渐繁荣。数据库的种类，厂家也逐渐变多。</p><p></p><p>第五阶段就是我们今天所处的云计算时代。典型应用包括新媒体、各种移动 APP、物联网、娱乐、短视频等。典型的数据库有 RDS、Aurora 等云数据库，以及 Oceanbase、CockroachDB 等分布式数据库。百度也有对应的产品，云原生数据库 GaiaDB 以及我们自研的缓存类数据库&nbsp;PegaDB 等。</p><p></p><p>第六个阶段是自 2023 年开始的 AI 时代。底层基础设施变成了 GPU 和 AI 能力。应用也变成了 AI 原生应用，如海外比较火的 Jasper、Midjourney，微软的 Copilot 等。在数据库行业我们看到至少两个方向，一个是 AI4DB，其中包括阿里的 DAS、百度的 DSC 等，主要是通过 AI 的能力去改进原有数据库的自动化能力。另外一个方向就是 DB4AI，目前主要是向量数据库。向量数据库在解决大模型幻觉等方面，有非常不错的效果，是一个有潜力的细分赛道，头部公司估值已经达到 10 亿美元。</p><p></p><p>以上就是数据库 70 年波澜壮阔的发展史。我们可以看到，每隔一段时间数据库就会在基础设施、应用场景、以及数据库本身，都有不断地变更和创新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92a9b7ae192ebfb33a9091bbf57857a1.png" /></p><p></p><p>上面我们简单回顾了数据库发展的六个阶段。在这个过程中，我们还可以以 2000年做分界线。在 2000 年前，国内数据库基本上被 Oracle 等海外数据库主导。而从 2000 年之后，随着互联网业务的发展，国内多个互联网厂商如阿里、腾讯、百度便开始尝试使用开源数据库，实现了从最早的运维、到提交 patch、再到最后完全自研数据库的跨越式发展。</p><p></p><p>这背后从量变到质变的过程是一个典型基础软件发展过程。</p><p></p><p>一个基础软件真正得到长足发展，需要一大批高素质的技术人员，也需要深度场景的使用才能不断完善产品。另外丰富的场景和不断发展的业务，也能长期养活这批技术人员，进而形成正循环。所以说数据库的发展依赖于技术和业务的双轮驱动。</p><p></p><p>从 2000 年开始，我们看到三波浪潮——互联网，云计算和 AI 原生。我们接下来会分别来讲一下每一波浪潮为数据库行业带来的创新和变化，以及百度智能云数据库在这个过程中的关键技术和代表产品。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f647b07b8603129ba05898417ae77eb5.png" /></p><p></p><h2>二、百度智能云数据库发展史</h2><p></p><p></p><p>互联网业务特点是赢家通吃，所以互联网业务用户数规模通常比较大。因此天然要求数据库支持大规模、高可用、高可靠性、低成本以及高性能，这对数据库提出了非常大的挑战。</p><p></p><p>在第一波互联网业务的发展中，业务的挑战催熟了一系列开源数据库如 MySQL、Redis、MongoDB，又从中孵化出了分布式数据库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20e33da0350e46cb356adc79012f78ed.png" /></p><p></p><p>接下来我们来看下百度在互联网时代的数据库发展历程，这里有几个关键节点：</p><p></p><p>第一个是自 2005 年开始使用 MySQL 数据库，这也是国内最早使用 MySQL 的企业之一。</p><p></p><p>第二个是 2014 年百度推出公有云服务，百度数据库的能力通过<a href="https://www.infoq.cn/article/WrlUWpf2OkgQsSAD6NJ1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"开始赋能给外部企业。</p><p></p><p>第三个是 2020 年发布了云原生数据库 <a href="https://xie.infoq.cn/article/61a867abe6d45fa9f1fe644d0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">GaiaDB</a>"。百度也成为了国内少数几个具备自研云原生数据库云厂商之一。</p><p></p><p>截至目前，百度积累了 18 年的数据库研发经验，承载着内部 PB 级数据。10 万+ 的节点至今零故障零损失。</p><p></p><p>通过百度智能云输出的一站式产品，覆盖 RDS、NoSQL、OLAP、工具等领域，同时具备公共云、私有云、边缘云等软件版本多形态。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f0fe561b919a46ca824f31f4f556ce5.png" /></p><p></p><p>前面我们提到了互联网的一大特点，就是规模大。单点肯定处理不了，所以需要引入分布式技术，也催生了分布式数据库的诞生。</p><p></p><p>百度在该领域也有非常成熟的技术，讲两个实际的案例：</p><p></p><p>第一个是<a href="https://www.infoq.cn/article/2012/03/baidu-bae?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度网盘</a>"。百度网盘有 8 亿用户，整个数据库中单表最大超过 10 万亿条记录。整体集群超过 3000 台服务器，是国内最大的数据库集群之一。</p><p></p><p>第二个是金融行业。大家都知道金融行业对一致性、数据准确性有非常高的要求。度小满金融有 3 亿用户，年度结算金额超过万亿，其底层使用的就是百度智能云分布式数据库 GaiaDB-X。</p><p></p><p>尤其值得一提的是在 2019 年春晚红包业务中，整体交易的峰值是 12 万笔/秒。数据库的分布式能力、性能、一致性、准确性都得到了充分验证。</p><p></p><p>除了度小满，百度智能云的数据库还在多家国有大行、股份制银行和城商行中稳定运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72d4145b2e5cf4609d5969fcd0c828a2.png" /></p><p></p><p>互联网业务除了规模外，对性能、并发等也提出了很高的要求，因此诞生了一系列 NoSQL 数据库。不同的 NoSQL 数据库从不同层面解决互联网垂直场景的问题，今天我们讲其中的代表 Redis。</p><p></p><p>百度智能云的 Redis 服务经历十几年的技术积累和业务打磨。从规模上来看，节点规模超过 30w，其中单集群最大规模节点数达到 2700。从业务支持上看，百度 Redis 覆盖支撑了百度内部全场景业务，其中包括搜索广告、手百、地图、小度等一系列亿级用户体量的产品，为业务提供 4 个 9 以上高可用性以及微秒级请求时延服务，始终为客户提供稳定、高效、弹性可扩展的智能缓存服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/8471c3e34b3cef8ce9d5754cd03b193c.png" /></p><p></p><p>Redis 直接使用内存，但内存带来高性能的同时成本是比较贵的。因此一款能兼顾性能和成本的 Redis 产品是客户迫切需要的。考虑到业务中大量的数据是可以根据场景分出冷热的。比如视频直播、新闻/内容平台、电商场景中，随着时间的推移，数据的价值和使用频率都在下降。所以可以将部分数据自动迁移到磁盘中，从而降低存储的整体成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8240196a262dd1948b2f1a0a9babe152.png" /></p><p></p><p>为了解决性能和成本的平衡问题，百度智能云自研了 PegaDB。PegaDB 是在开源基础上自研的容量型 Redis 产品，相比内存型产品最多节省超过 90% 的存储成本。在成本下降的同时，PegaDB 也兼容了 Redis 丰富的数据类型和命令，让用户做到无缝迁移，兼顾了用户体验和性能优势。</p><p></p><p>除此之外，PegaDB 还有两个杀手锏功能：</p><p></p><p>一是支持在线弹性伸缩，单个集群最大规模可达 PB 级别。对用户来说不用估计使用量，只要傻瓜式即开即用即可。</p><p></p><p>第二个是支持 CRDT 同步的组件，支持异地多活和多节点同时访问、自动进行冲突合并等功能。这就让客户专注于实现业务逻辑，其他的都交给底层的数据库，完全不用操心可用性问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8247c1d27f22bf8d13326b6df3fa1270.png" /></p><p></p><p>随着云业务的诞生，让数据库的价值进一步放大。为了赋能千行百业，全托管等形态的 RDS 顺利成章的诞生了。它解决了客户最直接的安装、运维、管理等问题，因此全托管的 RDS 就逐渐推广开来。</p><p></p><p>但单体 RDS 通常有比较明显上限，在一些对性能、成本、弹性有一定要求的复杂业务中，就需要一个更强大的数据库来解决这些问题。因此，存算分离的云原生数据库就自然而然诞生了。百度智能云的云原生数据库 GaiaDB 是其中的代表之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e835b194141802f859a373b37363990.png" /></p><p></p><p>RDS 全托管的产品形态代表了云计算从软件到服务的理念转变。云原生数据库极大地提高了 MySQL 数据库的上限能力，是云数据库划代的产品。</p><p></p><p>云原生数据库最早的产品是 AWS 的 Aurora。AWS Aurora 提出来的 The log is the database 的理念，通过把大量的日志操作放到后台异步处理，实现了存储独立扩展和存储计算分离，从而解决了 MySQL 数据库单库的数据量不能太大的最大痛点。</p><p></p><p>而云原生数据库在存储层面实现了扩展的同时，又保留了计算层面的不变和兼容。这种兼容 + 扩展的能力，受到了客户的极大欢迎，一下子就让云原生数据库成为各个厂商的发展重点。云数据库技术也标志着云厂商的产品能力开始和传统数据库厂商、开源产品开始拉开差距。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd48508403406795d06e50dffdebbe2f.png" /></p><p></p><p>百度智能云的 GaiaDB 在 2020 年首次推出，除了具备云数据库的优点之外，GaiaDB 还有很多独特的技术能力，接下来我来分享其中 5 个代表能力：</p><p></p><p>第一个是共识协议。一般使用 Raft/Paxos 分布式协议的数据库，单次 I/O 需要至少两次网络往返，而且无法并行。这也就导致了分布式数据库时延很高，长尾问题更突出。</p><p></p><p>针对这个问题 GaiaDB 创新采用了 Raft 和 Quroum 结合的协议。其中 Raft 负责控制流，Quorum 负责数据流，进而减少网络往返。同时核心链路上的同步 I/O 变成异步 I/O，在保证分布式一致性的前提下，吞吐提升了 40%，时延降低了 30%。</p><p></p><p>第二个是高性能智能网络。存算分离在带来分布式和弹性的同时，也引入了网络 I/O 的消耗，因此网络 I/O 的性能和效率直接影响整个系统的表现。GaiaDB 采用高性能智能网络，这个网络有几个关键技术能力：</p><p></p><p>网络超时重定向机制。当远程 I/O 超时，会自动尝试其他副本，从而抑制单节点长尾问题。网络支持用户态协议。该协议减少了内核态 TCP 和用户态 TCP 的数据库拷贝。通过对网络的优化，平均时延从毫秒级别降低到微秒级别，提升 20 倍以上。</p><p></p><p>第三个是提供了三副本对等存储能力。由于采用了 Quorum 分布式共识协议，相比传统的 Raft 模型，每个节点都可以独立提供读写服务，没有单点故障。</p><p></p><p>第四个是多地多活。GaiaDB 是目前业界唯一可以做到多地多活的云原生数据库。在多地部署的时候，GaiaDB 模块的自适应就近访问策略可以感知元数据的变化，并根据这些变化及时切换访问路线。这种策略可以有效地应对各种故障和异常情况，确保数据的可靠性和可用性。</p><p></p><p>第五个是使用通用硬件，对硬件要求低。GaiaDB 生于云，但同时 GaiaDB 的架构对硬件的依赖度非常低。我们和很多厂商使用高性能硬件的思路不同，我们认为云的价值是普惠，所以一定要让通用服务器能发挥专业数据库的能力。因此，不同于很多云原生数据库需要依赖底层高性能的硬件，GaiaDB 从设计初就坚持使用通用服务器。因此在私有云场景下，三个节点就可以进行部署，让我们的客户可以低价享受到云上云下一套架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b7c7ed3a0879eebe3cc8ba586055484.png" /></p><p></p><p>接下来我们来看一个 GaiaDB 的实际案例——百度地图。</p><p></p><p>百度地图是国民级别应用，日活用户 5.6 亿，PB 级数据。这对数据库也提出了如下的挑战：</p><p></p><p>为了保证高可用，需要多地多活的能力。节假日地图搜索，导航流量会出现十倍的上涨。这就要求在节假日需要非常顺滑的扩缩容的能力。</p><p></p><p>大规模数据量、异地多活、弹性扩缩容要求，这些要求对数据库是极大的考验。</p><p></p><p>在实际使用过程中，GaiaDB 提供 4 个 9 的可用性，RTO 切换小于 3s，RPO=0，整体 QPS 超过百万级别，给业务实现超过 60% 的资源成本节省。</p><p></p><p>总的来说，GaiaDB 成功帮助百度地图实现了极致的弹性和成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f02e4b25fba325d4854e72f711bde58.png" /></p><p></p><p>云上数据库和线下数据库相比，一个较大差异就是生态能力强。相比传统线下软件只有 1~2 款产品，线上有多种数据库与多种使用环境，因此数据库矩阵更丰富，这带来了对数据库工具的诉求。</p><p></p><p>百度智能云有丰富的数据库工具，包括数据传输 DTS、数据库智能驾驶舱 DSC 等产品。我们先讲其中的代表 DTS。</p><p></p><p>百度智能云的 DTS 采取了中间抽象的数据格式，通过中间格式的翻译和转换，可以轻松做到异构迁移能力。同时 DTS 在吞吐上可以做到每秒 15 万行，延迟做到毫秒级别，基本等于网络的延迟的性能，让客户可以放心使用 DTS 来做数据库的迁移和同步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e0d44ff1229d468f704af06b132010e.png" /></p><p></p><h2>三、AI 原生时代的百度智能云数据库</h2><p></p><p></p><p>在 AI 原生时代，数据库和 AI 的结合主要有 DB4AI 和 AI4DB。</p><p></p><p>首先是 AI4DB，就是利用 AI 技术赋能数据库。常见场景有智能运维、智能客服、参数优化等等，刚刚提到的百度智能驾驶舱就是该领域的代表。</p><p></p><p>另外一个方向是 DB4AI，通过数据库赋能 AI 产品。当前最火的就是向量数据库。向量数据库二次的翻红主要原因是向量数据库在解决大模型幻觉、知识更新不及时有很大作用，让向量数据库的想象空间一下子变大了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/096c6ccafce7598fdac1f73dfb471e4f.png" /></p><p></p><p>AI4DB 在工业界一直有研究。相比传统机器学习算法，大模型让 AI4DB 真正走进实用时代。利用大模型的能力，百度智能云数据库发布新服务：数据库智能驾驶舱。</p><p></p><p>数据库智能驾驶舱利用最新的大模型能力，实现数据库智能化的洞察、评估和优化。根据我们的实际测试效果，优化效果非常显著：</p><p></p><p>数据库故障洞察方面，相比传统的人工定位提升 80%。领先的智能评估系统，相比传统的方法提前一个月发现数据库的容量瓶颈，规避相应的风险。AI 驱动的 SQL 优化方面，可以带来 40% 以上的提升。</p><p></p><p>相比传统基于规则的算法，大模型带来了更好的优化效果和更少的开发时间。大模型带来的切实提升让 AI4DB 走向真正的实用时代，也让数据库自感知、自修复、自优化、自运维成为现实。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9cd6b1eab7842cbdba2f1e03555c143.png" /></p><p></p><p>下面我们来看下数据库智能驾驶舱内置的一个能力——智能问答。</p><p></p><p>这个功能可以帮助用户诊断产品问题并回答各种疑问，降低人工投入。这里面用到了大模型通用知识的能力，同时也利用 RAG 技术，把云产品文档、数据库的官方文档、内部积累的知识库进行向量化并存在向量数据库中。</p><p></p><p>在查询的时候，结合大模型和向量数据库的能力，可以给出相当准确有效的答案。</p><p></p><p>目前数据库智能驾驶舱经过验证，对历史客户工单中真实问题进行回答然后由人工进行打分，整体回复平均超过 4 分，基本可以媲美普通售后工程师的水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79b670cada64634f9bf5577ab757d963.png" /></p><p></p><p>接下来我们实际来看下智能问答的一个 demo。</p><p></p><p>左边的例子是询问知识库里面已有的例子，比如怎么购买，怎么实现一个读写分离的配置等。智能驾驶舱都总结得比较好，回答也非常准确。</p><p></p><p>右边的例子是询问知识库中没有的例子。我们可以发现，智能驾驶舱利用大模型的能力，可以举一反三，把解决问题的步骤给出来。我们人工去检查也会发现，这个步骤还是相对比较合理的。</p><p></p><p>所以现在智能驾驶舱的智能问答可以做到：有资料的问题准确回答，无资料的问题也可以给出相对清晰的解法。百度智能云内部已上线了该功能，大大节省了人力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48ba6c7615d86cfd011247f99b44be3e.png" /></p><p></p><p>DB4AI 的典型代表就是向量数据库。向量检索并不是一个新技术，2017年 Meta 就开源了相似度检索库 FAISS，算是向量化检索的开山鼻祖。</p><p></p><p>传统数据库解决的是结构化数据的存储和检索，非结构化数据需要先用 AI 算法 Embedding 成向量数据。需要查找的时候，把需要查找的数据的向量带过来，然后在库里面进行相似度检索。</p><p></p><p>而向量数据库核心能力就是支持向量数据存储，以及支持不同的查找算法和索引实现相识度查找。当前业界有两种不同的实现方式，一种是在传统数据库中增加插件或者功能支持向量的查找，比如 PG，Redis 都支持向量索引。这种实现相对来说容易一些，但同时性价比会差一些，通常会占用更多内存。另外一种是专业的向量数据库，专门为向量重新设计的存储和索引结构，能实现更高的性价比和弹性。</p><p></p><p>传统应用也有不少向量场景。典型场景有平安城市视频检索、电商领域以图搜图等。由于传统场景比较垂直，因此一直没有一个大的向量数据库，更多的是耦合在业务系统中。而在大模型时代，万物皆可向量化。而且当前大模型主要问题有知识更新不及时、精确性问题、数据权限管理等问题，都需要向量数据库来补充。向量数据库也因此成为大模型的标配，也在大模型时代二次翻红。</p><p></p><p>百度智能云自研的专业向量数据库目前在内测阶段，根据我们内部实际测算，在成本、规模、高性能算法、内置 Embedding 模型、向量 + 标量的联合查询方面，相比业界有很大的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b9f6fcb6d97c8979c5bbbdfe17c3e09.png" /></p><p></p><p>前面我们介绍了关键的产品，最后简单回顾一下百度智能云产品矩阵。</p><p></p><p>百度智能云数据库完整支持 RDS、NoSQL、云原生数据库，OLAP 等产品。相比业界其他云厂商，百度智能云数据库有两个显著特点：</p><p></p><p>百度智能云的数据库产品可以做到一套架构，云上云下客户享受同等的产品能力。支持国内最全的产品形态，包括公共云、私有云、边缘节点、LCC 等多种形态，可以服务各类诉求的客户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc846845a37a6dac8eac882ad434be70.png" /></p><p></p><p>前面我们盘点了数据库在互联网、云计算，AI 原生 3 个阶段的发展。除了技术之外，我们认为云数据库未来还要坚持两个重要的理念。</p><p></p><p>第一个是体验优先。一个好的数据库不能只是性能、成本这些方面。体验好的产品，可以让用户做到自服务。体验优先这一点在海外 SaaS&nbsp;产品中体现得更为明显。在国内，这一理念也逐渐取得从业者的认可。因此，在过去的半年里面，我们从文档、控制台、产品功能各个层面进行了深度优化：</p><p></p><p>文档：文档是用户使用和理解产品的重中之重，因此我们做了包括优化结构、补充用户场景、刷新细小的优化点在内的大量工作，目的就是让用户在使用过程中可以更方便找到自己所需要的内容。控制台：在控制台优化上，我们优化了整体结构，让用户可以更简单找到想用的功能，总共优化点超过 100 处，让用户更容易上手。产品功能：我们针对数据库的产品功能系统性安排测试定期的盲测、新员工使用等，仅仅上半年就优化了 50+ 个突出的易用性问题。</p><p></p><p>我们对体验的理解就是从用户视角入手、坚持细节、系统性的进行优化，只有通过这种深度，全方位的持续改进，才能把体验做到实处。</p><p></p><p>第二个是开放生态。丰富的生态是吸引客户、解决客户多样诉求的关键。也只有开放的生态，才能让更多的厂商一起服务好客户。</p><p></p><p>生态方面，百度秉承更开放的心态和第三方厂商合作。上半年我们和工具领域知名创业公司 NineData 正式合作，接下来会马上官宣另外一个合作厂商。</p><p></p><p>相比其他厂商，我们合作的过程也不只是简单的云市场合作。我们会和合作伙伴一起进行产品共建、优先推荐合适客户给合作伙伴、首页曝光和联合的品牌活动，增加合作方的知名度。</p><p></p><p>通过一系列的手段和措施，我们希望给到合作伙伴的是切实效果。百度智能云合作的理念就是更开放，让利合作伙伴。欢迎更多的合作伙伴和百度联系，一起服务好我们的客户。</p><p></p><p>总的来说，一个体验优先，生态开放的云，一定是客户最需要的云，也是真诚服务客户的云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/dfc3dc9e74526dc0f5a93b8a4fa32c72.png" /></p><p></p><h2>四、数据库未来的趋势展望</h2><p></p><p></p><p>站在当前看未来，数据库当前有四个关键发展趋势</p><p></p><p>AI Native。像大家比较头疼的 Oracle 转 MySQL 或者 PG，随着 AI 改写的到来，整个过程预计会变得很简单。Serverless。已经是海外云数据库的默认选项了，预计 1~2 年之后，serverless 就会在国内变得更普及。各个厂商也都会推出 serverless 数据库产品，这也是未来云产品的终极形态。内置 HTAP。HTAP 前段时间很火，不过我们判断 HTAP 很难成为一个单独的赛道，更多的是会成为各个 TP 数据库的内置能力。湖仓一体。湖仓一体预计会成为数据仓库的主要形态，不支持湖的数仓可能会很难生存，只有支持湖才能解决更多的数据问题，才能降低存储的成本。</p><p></p><p>技术和产业发展都很快，百度智能云数据库持续跟进最新的技术趋势，用优质的产品和真诚的服务回报我们的客户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f7dafca36b251342c7b9668e005b1a4.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</id>
            <title>网易杭州研究院 / 编程语言实验室 / 负责人张炜昕博士确认出席 QCon 上海，分享低代码编程语言 NASL 从设计到落地的闯关之路</title>
            <link>https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 低代码编程语言 NASL, 张炜昕博士, CodeWave 智能开发平台
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，张炜昕博士将分享关于低代码编程语言NASL的设计和实现挑战，以及CodeWave智能开发平台的应用。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。网易杭州研究院 / 编程语言实验室 / 负责人张炜昕博士将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5642?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">低代码编程语言 NASL 从设计到落地的闯关之路</a>"》主题分享，探讨 NASL 语言的设计初衷、设计原则、实现挑战、未来展望等方面。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5642?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">张炜昕博士</a>"，香港大学博士，布里斯托大学 Senior Research Associate，长期从事编程语言研究。现为网易杭州研究院编程语言实验室负责人，主导 CodeWave 智能开发平台编程语言 NASL 的设计。以第一作者身份在 TOPLAS，ECOOP 等编程语言期刊和会议上发表论文多篇，并获 ECOOP“杰出软件制品奖”和 Programming 期刊“编委会选择奖”。曾任 Scala 研讨会主席，IFL 程序委员，PLDI、OOPSLA 软件制品审查委员，以及多个编程语言会议的审稿人。他在本次会议的演讲内容如下：</p><p></p><p>演讲：低代码编程语言 NASL 从设计到落地的闯关之路</p><p></p><p>NASL 是由网易自研的全栈可视化编程语言，是支撑网易数帆 CodeWave 智能开发平台的基石。本次演讲将围绕 NASL 语言的设计初衷、设计原则、实现挑战、未来展望等方面展开。</p><p></p><p>演讲提纲：</p><p></p><p>NASL 的设计初衷</p><p>○ 为什么低代码平台需要编程语言</p><p>○ CodeWave 及 NASL 的简介</p><p>NASL 的设计原则</p><p>○ 低门槛、高上限</p><p>○ 记号的认知维度</p><p>○ 编程系统的技术维度</p><p>NASL 的实现挑战</p><p>○ 如何融合企业的 IT 资产</p><p>○ 如何降低实现成本</p><p>NASL 的未来展望</p><p>○ LLM 时代的编程语言设计</p><p>○ 文本语法和标准化</p><p></p><p>听众收益点：</p><p></p><p>○ 可视化编程语言和编程系统的设计原则</p><p>○ 降低编程语言实现成本的方法</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 5 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</id>
            <title>南京大外企将研发撤离中国，最高赔偿N+8；OpenAI回应GPT-4变懒；周星驰Web3团队下月上线独立App | AI一周资讯</title>
            <link>https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图森未来, OpenAI, GPT-4, Gemini, 夸克大模型
<br>
<br>
总结: 图森未来计划裁减75%在美员工，OpenAI的GPT-4遭到用户投诉，Gemini的性能被指控“造假”，阿里的夸克大模型通过备案，快手进行了年内最大规模的组织架构调整。 </div>
                        <hr>
                    
                    <p></p><blockquote>“自动驾驶卡车第一股”图森未来缩减美国业务，拟裁减75%在美员工；商汤科技 AI 编程助手“代码小浣熊 Raccoon”开放公测；快手开启年内最大规模组织架构调整，涉主站、电商等多个业务线……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>OpenAI回应GPT-4变懒</h4><p></p><p></p><p>OpenAI 的 GPT-4 大语言模型日前遭到部分用户投诉，部分用户表示，这段时间使用 ChatGPT 或 GPT-4 API 时，会遇到高峰期速度非常慢、敷衍回答、拒绝回答、中断会话等一系列问题。</p><p></p><p>北京时间周五中午，ChatGPT 官方通过 X 平台通知用户，“我们听到了你们关于 GPT-4 变得越来越懒的反馈！我们自 11 月 11 日起就没有更新过模型了，当然这不是故意的。”</p><p></p><h4>微软与OpenAI合作面临英国审查</h4><p></p><p></p><p>12月9日消息，当地时间周五英国监管机构英国竞争与市场管理局（CMA）表示，正在就微软与ChatGPT开发商OpenAI之间的合作关系进行评估，看是否有必要进一步展开反垄断调查。</p><p></p><h4>马斯克：Grok AI测试版现已向美国所有X Premium+订阅者开放</h4><p></p><p></p><p>12月8日，埃隆·马斯克在社交媒体上发文称，Grok AI测试版现已向美国所有X Premium+订阅者开放。据悉，现有 X 平台用户可以每月花费 16 美元或每年 168 美元来进行订阅。</p><p></p><p>马斯克此前表示，Grok 使用来自公开数据的数十亿个数据点进行训练，但是目前尚不清楚使用了哪些数据。此外他还提到 Grok 将能够实时访问 X 平台，因此与其他生成式人工智能相比这是一个巨大的优势。</p><p></p><h4>谷歌承认Gemini演示视频经特殊剪辑处理</h4><p></p><p></p><p>美东时间12月6日，谷歌CEO桑达尔・皮查伊宣布迄今为止规模最大，能力最强的谷歌大模型Gemini 1.0 版正式上线。Gemini是原生多模态大模型，是谷歌大模型新时代的第一步，它包括三种量级：能力最强的 Gemini Ultra，适用于多任务的 Gemini Pro，以及适用于特定任务和端侧的 Gemini Nano。</p><p></p><p>不过，外界已开始有声音指控谷歌对Gemini的性能“造假”。彭博社一篇专栏文章就表示，谷歌在一段演示视频中歪曲了Gemini的AI性能。专栏作家帕米·奥尔森（Parmy Olson）认为，在谷歌发布的这段视频中，Gemini似乎非常强大，但有点过于强大了。对此质疑，谷歌回应时承认，这段关于Gemini性能演示的视频并不是实时的，而是使用了原始镜头中的静止图像帧，然后编写了文本提示，以便让Gemini做出回应。</p><p></p><h4>阿里夸克大模型已通过备案</h4><p></p><p></p><p>日前，阿里智能信息事业群自研的夸克大模型已通过备案，将陆续在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列AIGC创新应用。夸克相关负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。在搜索应用中，将通过图文多模理解、专业知识生成、交互方式创新进一步拓宽应用场景，提升用户体验。</p><p></p><h4>周星驰Web3团队下月上线独立App</h4><p></p><p></p><p>据新浪科技报道，12月7日下午消息，据接近周星驰团队人员对新浪科技透露，周星驰旗下Web3初创公司Moonbox 最早将于明年1月份完成上线Moonbox App，届时App将免费向用户开放。目前，App研发工作已经基本完成，Moonbox团队在 NFT 玩法上下了很多功夫，已设计出基于AI和NFT聊天的互动玩法。</p><p></p><p>据上述知情人士透露，伴随着Moonbox App的独立上线，“周星驰将以Moonbox First Creator身份与大家见面”。与此同时，周星驰参与创作的Nobody NFT新品，也将随之发售，用户可以通过App和每个Nobody NFT角色聊天互动以了解人物性格、爱好、背景故事。</p><p></p><h4>“自动驾驶卡车第一股”图森未来缩减美国业务，拟裁减75%在美员工</h4><p></p><p></p><p>近日，图森未来向美国证券交易会提交的一份报告显示，公司将裁撤150名在美员工，约为美国员工总数的75%，全球员工总数的19%。这是图森未来继去年12月和今年5月的裁员后，再一次进行人员削减。</p><p></p><p>图森未来预计，此次重组计划将产生约700万至800万美元费用，大部分用于支付遣散费、员工福利和相关费用，重组费用将在2023年第四季度入账。</p><p></p><p>据华尔街日报报道，本次裁员后，图森未来在美人数仅为30人，将负责图森未来美国业务的收尾工作，逐步出售公司在美资产，并且协助公司向亚太地区转移。因此，此次裁员意味着图森未来或将彻底退出美国市场。</p><p></p><h4>通义千问登顶HuggingFace开源大模型排行榜榜首</h4><p></p><p></p><p>12月8日消息，全球最大的开源大模型社区HuggingFace日前公布了最新的开源大模型排行榜，阿里云通义千问力压Llama2等国内外开源大模型登顶榜首。</p><p></p><p>HuggingFace的开源大模型排行榜（Open LLM Leaderboard）是目前大模型领域最具权威性的榜单，收录了全球上百个开源大模型，测试维度涵盖阅读理解、逻辑推理、数学计算、事实问答等六大评测。通义千问（Qwen-72B）表现抢眼，以73.6的综合得分在所有预训练模型中排名第一。</p><p></p><h4>商汤科技 AI 编程助手“代码小浣熊 Raccoon”开放公测</h4><p></p><p></p><p>12月7日，商汤科技官微宣布，基于商汤自研大语言模型的智能编程助手——代码小浣熊Raccoon，即日起开放公测。据介绍，在实际应用中，代码小浣熊可帮助开发者提升编程效率超50%；未来，应用代码小浣熊，开发者可以将80%的编写工作交由AI完成。</p><p></p><h4>特斯拉Dojo超算项目被曝更换负责人</h4><p></p><p></p><p>外媒援引知情人士消息称，特斯拉Dojo超级计算机的项目负责人Ganesh Venkataramanan已经于11月份离职。在过去五年中，Venkataramanan一直在领导Dojo项目的推进工作，加入特斯拉前他在AMD担任了近15年的长期工程总监。现在Dojo项目由Peter Bannon负责，Bannon已经在特斯拉担任高管近8年，之前还在苹果公司中任职超过7年。</p><p></p><h4>快手开启年内最大规模组织架构调整，涉主站、电商等多个业务线</h4><p></p><p></p><p>12月7日消息，快手发布内部邮件宣布新一轮组织调整。此次组织调整涉及主站、电商、商业化、杜区科学等多个业务线，属于今年以来最大范围的一次组织架构调整。其中，商业化事业部下本地消费业务部调整至主站线下，更名为招聘房产业务部，负责快聘、房产相关业务，取消主站产品部，主站线下成立孵化产品部，负责快影、一甜相机、回森等独立APP产品。</p><p></p><h4>南京大外企将研发撤离中国，裁员赔偿最高N+8</h4><p></p><p></p><p>近日，南京知名外企趋势科技计划搬离国内。知情人士透露，趋势科技打算将核心技术从国内转移到加拿大，因此裁员只涉及研发部门，其他部门几乎没有调整，共计约 70 人左右，赔偿 N+4 起步，一些老员工则超过 N+8。</p><p></p><p>据悉，该公司从上个月就开始裁员了，目前已接近尾声。值得一提的是，趋势科技本次撤离还会带走一部分员工，同意去加拿大的话也可以协调不裁。</p><p></p><h4>苹果因故意降低性能被判赔偿韩国7名用户每人7万韩元</h4><p></p><p></p><p>据韩联社消息，6日，韩国7名消费者集体起诉苹果通过升级系统降低旧款iPhone性能案二审宣判，法院判处原告部分胜诉。</p><p></p><p>据报道，韩国首尔高等法院民事当天开庭审理，判处苹果向原告每人支付7万韩元(约合人民币382元)赔偿金。</p><p></p><p>据悉，法院对原告所谓“苹果升级iOS系统属于发布恶意程序或损坏iPhone手机性能”的主张不予采纳，但法院认为，即使更新操作系统旨在防止手机自动关机，但也限制了中央处理器(CPU)等性能。苹果有义务向消费者说明是否安装更新，但苹果违反这一规定。同时，消费者因选择权被侵害而产生精神损失，认定苹果有赔偿责任。</p><p></p><h4>王慧文入股OneFlow团队新创业项目</h4><p></p><p></p><p>近日，北京硅动科技有限公司发生工商变更，新增王慧文为股东，同时注册资本由100万人民币增至约105.26万人民币。该公司成立于今年8月，法定代表人、执行董事、经理为OneFlow创始人袁进辉，公司经营范围含软件开发、技术进出口、电子产品销售、人工智能应用软件开发、人工智能通用应用系统、人工智能行业应用系统集成服务等。</p><p></p><p>公开信息显示，王慧文病休后，光年之外收购的核心团队OneFlow宣布重新创业。袁进辉称，新创业项目拟解决大模型推理成本问题。天眼查显示，目前，王慧文仍为OneFlow关联公司北京一流科技有限公司董事。</p><p></p><h4>量子计算技术重磅升级：IBM展示最新的模块化量子处理器</h4><p></p><p></p><p>当地时间周一（12月4日），IBM在官方博客发文，展示了“量子效用”所需的硬件和软件，其中包括新的量子处理器芯片和量子计算系统。</p><p></p><p>新闻稿称，IBM展示了一种新方法：将芯片连接到机器内部，再将机器连接到一起，以形成模块化系统，使规模的扩展不受物理条件限制。IBM称，将这种方法叠加新的纠错码，有望在2033年之前制造出引人注目的量子机器。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>投票开除奥特曼的董事发声：OpenAI之乱跟AI安全没关系</h4><p></p><p></p><p>在上个月令全球科技圈震惊的OpenAI“内乱100小时”中，AI圈的顶流明星山姆·奥特曼在遭到董事会扫地出门后又迅速凯旋而归。即便如此，由于事发后核心人物鲜少谈及幕后的考量，整件事情至今还留有诸多疑问。</p><p></p><p>当地时间周四，已经离开OpenAI董事会的Helen Toner公开发声，对于外界的诸多疑问和“知情人士消息”做出一些回应。</p><p></p><p>Toner表示，董事会开除奥特曼的原因与AI安全没有关系，而是“缺乏信任”。她进一步解释称：“我们解雇山姆的目的，是为了加强OpenAI并使其更有能力实现其使命。”</p><p></p><p>在面对OpenAI的律师试图施压董事会辞职时，她也坚持了这一立场。Toner介绍称：“律师试图声称，如果我们不立即辞职，将会违法。因为若公司因此崩溃，我们将违反受托责任。但OpenAI是一个非常特殊的组织，非营利使命——确保人工通用智能（AGI）惠及全人类——是最重要的。”</p><p></p><p>事实上，在面对律师强调“公司会因此崩溃”时，Toner回应称“这样也符合我们的使命”，令房间里的一众公司高管感到吃惊。对于这一点，Toner也补充道，这句话是对律师“恐吓策略”的回应。她试图表达的是：对于创建造福全人类的AGI这一使命而言，OpenAI的持续存在并不是必要条件。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</id>
            <title>FCon 演讲视频：数字人民币（e-CNY）赋能支付业态发展</title>
            <link>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全球数字货币, 中国数字人民币, 金融技术变革, 数字货币的安全性
<br>
<br>
总结: 随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。 </div>
                        <hr>
                    
                    <p>随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。在这个变化中，既有机遇也有挑战，特别是在数字货币的安全性、普及性和监管方面。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了苏州银行网络金融部高级产品经理<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5574">金一松</a>"，他以主题为《数字人民币（e-CNY）赋能支付业态发展》展开了分享，以下为重点内容概述：</p><p></p><p>数字人民币的定义与特性：详细讨论了数字人民币的定义、设计特点，包括与传统货币的区别、发行和流通方式，以及它在支付体系中的角色。数字人民币的母子钱包体系和软硬钱包形态：探讨了数字人民币的钱包体系，包括母子钱包体系的结构和软硬钱包的不同形态。无网无电支付能力与智能合约应用：强调了数字人民币在无网络和无电源环境下的支付能力，以及智能合约在数字货币中的应用。数字人民币的未来应用前景：展望了数字人民币未来的发展方向，包括在不同场景中的应用潜力和可能的创新应用。</p><p></p><p>通过深入了解这些重点内容，我们可以更全面地认识数字人民币的影响力及其在未来金融生态中的潜在角色。详细内容，请观看完整视频：</p><p></p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</id>
            <title>夸克大模型通过备案 将升级通识、健康、创作等搜索产品与智能工具</title>
            <link>https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 10:16:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里智能信息事业群, 夸克大模型, AIGC 创新应用, AI助手
<br>
<br>
总结: 阿里智能信息事业群自研的夸克大模型已通过备案，将在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列 AIGC 创新应用，借助大模型能力全面升级夸克，提升用户在学习、工作、生活上的效率。夸克App将在自研大模型的助力下，加速迈向年轻人的AI助手。 </div>
                        <hr>
                    
                    <p>日前，记者获悉阿里智能信息事业群自研的夸克大模型已通过备案，将陆续在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列 AIGC 创新应用，借助大模型能力全面升级夸克，提升用户在学习、工作、生活上的效率。</p><p></p><p>今年下半年，国内多款大模型已经完成备案且能力水平部分超过 GPT-3.5，广大用户也都期待爆款产品的出现以更好地解决方方面面的实际问题。作为深受年轻人喜欢的信息服务产品，夸克App将在自研大模型的助力下，加速迈向年轻人的AI助手。</p><p></p><p>今年11月中旬，阿里巴巴智能信息事业群发布全栈自研、千亿级参数的夸克大模型，将应用于通用搜索、医疗健康、教育学习、职场办公等众多场景。夸克大模型也凭借四大优势，接连登顶 C-Eval 和 CMMLU 两大权威榜单。同时在法律、医疗、问答等领域的性能评测中夺冠，成为了名副其实的“学霸”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2026ddaf74dacff9fec92c89fb31921.png" /></p><p></p><p>清华大学新闻学院教授、博士生导师沈阳认为，依托搜索平台，夸克大模型拥有高质量的各类数据，在中文语境下，模型能力处在行业领先水平。在教育、医疗等垂直领域中，夸克在对话、解题上的能力取得了新的突破，是国产自研大模型的优秀代表之一。</p><p></p><p>夸克相关负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。在搜索应用中，将通过图文多模理解、专业知识生成、交互方式创新进一步拓宽应用场景，提升用户体验。同时，在健康等垂直领域中，夸克将依托大模型能力，提供更加实用的信息服务。</p><p></p><p>目前，夸克 App 已经为数千万 95 后职场人和大学生提供了跨场景的智能效率工具。根据 QuestMobile发布的《2023年轻人群智能效率应用研究》报告显示，夸克 App 在泛学生人群和新生代职场人群的用户占比最高，年轻用户使用时长位列行业第一。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</id>
            <title>国内首份“图风控”报告发布：图风控成应对新型网络安全风险的关键性技术</title>
            <link>https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:51:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图风控技术, 数据关联性特征, 风险识别, 图智能技术
<br>
<br>
总结: 《图风控行业技术报告》指出，图风控技术是应对AI时代复杂风险的关键技术，利用数据关联性特征实现了大规模时序关系图的构建和实时风险识别。图智能技术以直观、高效、智能的方式分析实体之间的复杂交互关系，提升风险识别的准确性和及时性。图风控技术已在支付、信贷、电商等领域得到广泛应用，成为金融机构和科技公司关注的新发展趋势。 </div>
                        <hr>
                    
                    <p>12 月 8 日，国内首份《图风控行业技术报告》（以下简称“报告”）在北京发布，指出智能风控迈入“全图时代”，图智能应用于<a href="https://www.infoq.cn/article/TDdJaEAY6dBu474REtL0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">风控</a>"领域形成的“图风控技术”成为应对 AI 时代复杂风险形势的下一代风控基础设施和关键性技术。报告认为，图风控充分利用了海量数据时代的数据关联性特征，实现了大规模时序关系图的高效构建及全周期实时风险识别，在解决黑产复杂隐蔽、信息孤岛等挑战，挖掘更多隐藏风险等方面提供了强大的技术功能和应用价值。</p><p>&nbsp;</p><p>据了解，该报告由<a href="https://www.infoq.cn/article/bjCH8kMloxFUfp00WQIX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">蚂蚁集团</a>"、清华大学、北京邮电大学、中山大学、上海交通大学、复旦大学、之江实验室和<a href="https://www.infoq.cn/article/EE2bAVOOWa0K_g5lLh7j?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阿里巴巴</a>"淘天集团联合编写，中国人民大学国际货币研究所（IMI）、金融科技50人论坛（CFT50）提供学术支持，详细呈现了新型数字风险态势、图风控算法技术、图数据库等底层基础设施，并提供了丰富的行业应用案例。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ed/ed12501a3e99fa960e9b079af7002bb6.png" /></p><p>图：《图风控行业技术报告》发布现场</p><p>&nbsp;</p><p>数字化智能化的颠覆性变革正在带来全新的安全挑战。尤其是 AI 大规模渗透应用引发新一轮智能化浪潮，带来新型数字经济网络中数据复杂度和关联性呈几何倍增，也带来了更加复杂、隐蔽、强对抗和更具破坏力的安全威胁。传统的风控方式已难以抵御多样化的风险形势，越来越多的场景需要更智能化的技术利器。图风控技术的出现，提供了一种解决问题的利器。</p><p>&nbsp;</p><p>课题组专家、北京邮电大学教授、博士研究生导师石川在报告中指出，智能风控技术历经专家策略、机器学习和深度学习的演进，如今图智能技术正逐渐成熟。在金融、电商、安全、社交等领域，风险涉及多个实体之间的复杂交互关系。图智能技术以更直观、高效、智能的方式表达和分析这些交互关系，助力系统发现潜在风险中的隐藏模式和异常，进而提升对潜在风险的准确性和及时性识别。</p><p>&nbsp;</p><p>具体来说，“图”是一种以点和边来表示实体和关系的数据结构；“图智能技术”指包括图数据库、图计算引擎、图神经网络、图可解释等一系列和图有关的人工智能技术通称，是最适应大数据海量、动态等特征的技术之一；应用于风险控制领域而形成的“图风控技术”，可以聚合风险事件、交易属性、关系图谱、专家特征等各类动态变化的风险数据，结合图结构数据的可解释性，实现对风险全链路、基于关系视角的刻画，为风控从业者提供更加全面、可见、实时的风险监测并及时决策。因此，运用图技术提升风控系统能力，正成为行业的新发展趋势。</p><p>&nbsp;</p><p>图风控技术目前在业界已有成熟应用，涵盖支付风控、信贷风控、电商风控，以及供应链、网络安全和基础设施安全等多个领域，是金融机构、安全服务商、新兴初创企业，以及大型科技公司逐浪的“风控风口”。</p><p>&nbsp;</p><p>蚂蚁集团副总裁、大安全事业群总裁赵闻飙在报告中表示，数字经济时代，安全的重要性日益凸显。图风控技术作为蚂蚁集团重点研发投入的创新技术之一，现已成为强化风险管理的利器，对构筑坚固的安全防线作出了重要贡献。</p><p>&nbsp;</p><p>报告显示，蚂蚁集团从 2015 年开始探索图技术，推出了底层自研的大规模图风控基础设施 TuGraph。基于 TuGraph 布局的全图风控体系，打造了万亿级点边规模的全域风险大图，目前已全面应用在业务场景中，不仅实现了支付过程的毫秒级极速风控，支撑了高频交易的高精准度识别，还显著降低了资损率，提高了反欺诈和反洗钱等安全业务的效率。</p><p>&nbsp;</p><p>据了解，全图风控是蚂蚁集团智能风控体系“IMAGE”的重要组成部分。该体系还包括交互式主动风控、端边云协同风控、多方安全风控、智能对抗，支撑了支付宝资损率连续三年低于亿分之一，为解决风控的智能化、主动性、可预测性、隐私保护等世界级难题提供了新突破，获得 CCF 科学技术奖、吴文俊人工智能科学技术奖、浙江省科学技术奖等多个权威奖项。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</id>
            <title>FCon 最新演讲视频：大模型在金融领域的落地探索</title>
            <link>https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 08:42:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术发展, 大数据, 大模型, 金融行业
<br>
<br>
总结: 在金融行业，随着技术的快速发展，大数据和大模型正在逐渐成为推动行业创新的重要力量。这种变革不仅在风险管理和预测方面展现出巨大潜力，而且在促进金融机构与科技公司之间的合作、推动数字化转型，以及优化数据管理和治理方面也显示出其独特价值。 </div>
                        <hr>
                    
                    <p>在金融行业，随着技术的快速发展，大数据和大模型正在逐渐成为推动行业创新的重要力量。这种变革不仅在风险管理和预测方面展现出巨大潜力，而且在促进金融机构与科技公司之间的合作、推动数字化转型，以及优化数据管理和治理方面也显示出其独特价值。然而，在这一进程中，行业也面临着如可解释性、社会智能等一系列挑战。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了光大信托信息技术部副总经理、数据中心总经理祝世虎 博士，为你分享了大模型在金融领域的应用及其带来的机遇与挑战。以下为分享的重要内容：</p><p></p><p>大数据、大模型与风控的关系：探讨了大数据和大模型如何影响金融领域的风险控制，特别是如何通过数据分析和模型预测来管理和减少风险。大合作与创新：讨论了金融机构与科技公司之间的合作以及这种合作如何促进创新，特别是在开发和应用大型模型方面。关注的问题：提出了金融行业在采用大型模型时面临的一些挑战和问题，例如可解释性、社会智能等。数字化转型对大模型的助力：分析了数字化转型如何助力大模型在金融行业的发展和应用。数据信托与大模型：讨论了数据信托如何帮助管理和优化大模型，特别是在处理和保护数据方面。大模型的治理：探索了在金融行业中应用大模型时需要考虑的治理问题，包括伦理和法律方面的考量。</p><p></p><p>详细内容，请观看完整视频：</p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</id>
            <title>如何看待 OpenAI Q* 谣言</title>
            <link>https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:55:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, Q*, AI模型, 数学问题
<br>
<br>
总结: OpenAI取得了一项重大技术突破，开发出了名为Q*的AI模型，具备解决全新数学问题的能力。这一突破可能标志着迈向具有一般推理能力的通用人工智能的重要一步。通过分步推理技术，大语言模型可以更好地解决复杂的数学问题。OpenAI的研究还包括训练验证器模型来评估每个步骤的正确性，以提高推理结果的准确性。 </div>
                        <hr>
                    
                    <p>作者 ｜ Timothy B Lee</p><p>译者 ｜ 核子可乐</p><p>策划 ｜ Tina</p><p>&nbsp;</p><p></p><blockquote>OpenAI仍未明确解释Q*究竟是什么，但透露的线索倒是相当不少。</blockquote><p></p><p>&nbsp;</p><p>11月22日，就在OpenAI决定解雇（后又重新聘用）CEO Sam Altman的几天之后，技术媒体The Information报道称OpenAI取得了一项重大技术突破，使其能够“开发出更强大的AI模型”。新模型被命名为Q*（音为「Q star」），“具备解决全新数学问题的能力。”</p><p>&nbsp;</p><p>路透社也发表了类似的报道，但细节同样含糊不清。</p><p>&nbsp;</p><p>两篇报道都将这项突破与董事会解雇Altman的决策联系起来。路透社在报道中指出，几名OpenAI员工向董事会发函，“警告称这项强大的AI发现可能对人类构成威胁。”然而，“路透社未能拿到这封信的副本”，随后的报道也没有继续将Altman下台与Q*一事联系起来。</p><p>&nbsp;</p><p>The Information指出，今年早些时候，OpenAI开发出“能够解决基本数学问题的系统，攻克了这一对现有AI模型来说颇为艰巨的任务。”路透社则表示Q*“具备小学生水平的数学计算能力。”</p><p>&nbsp;</p><p>为了避免妄下结论，我们又花了几天时间搜集相关内容。OpenAI确实没有公布Q*项目的详细信息，但发表了两篇关于其解决小学数学问题的论文。在OpenAI之外，不少研究人员（包括Google DeepMind的研究人员）也一直在这方面开展探索。</p><p>&nbsp;</p><p>我个人怀疑Q*正是指向通用人工智能（AGI）的关键技术突破。虽然不一定会对人类构成威胁，但这可能标志着迈向具有一般推理能力的AI的重要一步。</p><p>&nbsp;</p><p>在本文中，我们将一同了解AI研究领域的这一重大事件，并解释专为数学问题设计的分步推理技术如何发挥关键作用。</p><p>&nbsp;</p><p></p><h1>分步推理的力量</h1><p></p><p>我们首先考虑以下数学问题：</p><p></p><blockquote>John给了Susan五个苹果，之后又给了她六个。之后Susan吃掉其中三个，又给了Charlie三个苹果。她把剩下的苹果给了Bob，Bob吃掉一个。接下来，Bob把手中半数苹果给了Charlie。John给了Charlie七个苹果，Charlie将手中三分之二的苹果给了Susan，最后Susan又把其中四个还给了Charlie。问，现在Charlie还剩几个苹果？</blockquote><p></p><p>&nbsp;</p><p>大家可以先试着自己算算。</p><p>&nbsp;</p><p>其实我们都在小学阶段学过简单的加减乘除，所以看到问题里说“John给了Susan五个苹果，之后又给了她六个”，就知道这时候Susan有11个苹果。</p><p>&nbsp;</p><p>但对于更复杂的问题，那人类在尝试解决时就需要借助笔算或者心算了。比如在此问题中，先有5+6=11，之后是11-3=8，接着8-3=5，以此类推。通过一步步思考，我们最终会得到正确答案：8。</p><p>&nbsp;</p><p>同样的技巧也适用于大语言模型。在2022年1月发表的著名论文中，谷歌研究人员指出，如果大语言模型能按照提示词分步进行推理，就会产生更好的结果。以下是论文中的一份关键图表：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8aa3c3a12e1e619a196d0456cebedb1.png" /></p><p></p><p>&nbsp;</p><p>这篇论文的发表时间还早于“零样本”提示技术，因此研究人员通过给出示例答案的方式来提示模型。在左图中，系统会提示模型直接给出最终答案，但结果是划的。而在右侧，系统会一步步提示模型并最终推理出正确答案。谷歌研究人员将这项技术称为“思维链提示法”，且至今仍被广泛应用。</p><p>&nbsp;</p><p>对于大语言模型来说，“五”和“六”这样的数字只是token，跟“这”、“那”或者“猫”没什么区别。这些模型之所以能把大写数字转换成5+6=11，是因为这个token序列曾经在训练数据中出现过。但大模型的训练数据中可能并不包含长计算示例，比如((5+6-3-3-1)/2+3+7)/3+4=8，所以如果要求模型直接给出计算结果，那它就很可能搞不清状况并生成错误答案。</p><p>&nbsp;</p><p>或者用另一种思路来解释，大语言模型没有可用于记忆中间结果（例如5+6=11）的外部“临时空间”。而思维链推理使得大模型能够有效使用自己的输出作为暂时记忆空间，从而将复杂问题拆分成更多步骤——每个步骤都可能与模型训练数据中的示例相匹配。</p><p>&nbsp;</p><p></p><h1>解决更复杂的数学难题</h1><p></p><p>&nbsp;</p><p>在谷歌发表关于思维链提示法论文的几个月前，OpenAI曾经推出一套包含8500道小学数学应用题的GSM8K数据集，以及一篇描述问题解法新技术的论文。OpenAI没有让模型逐一给出答案，而是要求其一次性给出100个思路答案，再通过名为验证器的另一套模型对各个答案进行评分。在这100条回复中，系统将只返回评分最高的答案。</p><p>&nbsp;</p><p>乍看起来，训练验证器模型也需要大费周章，难度不啻于训练大语言模型来生成正确答案。但从OpenAI的测试结果来看，情况并非如此。OpenAI发现只需小型生成器与小型验证器的组合，就能提供与单独使用超大生成器模型（参数是前者的30倍）相当的结果。</p><p>&nbsp;</p><p>2023年5月的一篇论文介绍了OpenAI在该领域的最新研究情况。OpenAI已经跨越小学数学，开始研究更具挑战性的MATH数据集。OpenAI现在不再让验证器对完整答案打分，而是训练验证器具体评估各个步骤，具体参见论文给出的下图：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/308ab3a9a944e10fa406a17b643ca6ed.png" /></p><p></p><p>&nbsp;</p><p>每一步都有一个绿色笑脸符号，代表该步骤处于正确的思路之上，直到最后一步模型得出“x=7”，这时打出的是红色的皱眉符号。</p><p>&nbsp;</p><p>文章得出的结论是，在推理过程中的各个步骤上都使用验证器，其结果比直接验证最终答案更好。</p><p>&nbsp;</p><p>这种逐步验证方法的最大缺点，就是更难实现自动化。MATH训练数据集中包含每个问题的正确答案，因此很容易自动检查模型是否得出了正确的结论。但OpenAI未能找到更好的方法来自动验证中间步骤。于是，该公司只能聘请了一些审查员，为7.5万个解题思路的共80万个计算步骤提供反馈。</p><p>&nbsp;</p><p></p><h1>求解路漫漫</h1><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0be88a7f03a5274850c9978a79424c1e.png" /></p><p></p><p>&nbsp;</p><p>需要注意的是，GSMK8K和MATH数据集中的问题至少还可以通过分步方式简单解决。但在实际应用中，相当一部分数学问题根本无法拆解，例如：</p><p>&nbsp;</p><p>你正在筹划一场分五张餐桌、每桌三位客人的婚宴。</p><p></p><blockquote>Alice不想跟Bethany、Ellen或者Kimmie一起坐。Bethany不想跟Margaret一起坐。Chuck不想跟Nancy一起坐。Fiona不想跟Henry或者Chuck一起坐。Jason不想跟Bethany或Donald一起坐。Grant不想跟Ingrid、Nancy或Olivia一起坐。Henry不想跟Olivia、Louise或Margaret一起坐。Louise不想跟Margaret或Olivia一起坐。要如何安排客人座位，才能充分满足他们的要求？</blockquote><p></p><p>&nbsp;</p><p>在把这样的提示词输入GPT_4时，它开始分步进行问题推理：</p><p>餐桌1：Alice、Chcuk和Donald。餐桌2：Bethany、Fiona和Ellen。餐桌3：Jason、Grant和Ingrid。</p><p>&nbsp;</p><p>但到第四张餐桌时，它就卡住了。这时候Henry、Margaret和Louise还没有入座，他们彼此都不想坐在一起，但接下来只剩两张桌子可以安排。</p><p>&nbsp;</p><p>在这个问题中，我们不知道GPT-4具体错在哪个具体步骤上。它在前三张桌子的安排上完全满足规则，但这些前期选择也导致余下的客人没办法正确入座。</p><p>&nbsp;</p><p>这就是计算机科学家们所说的NP难题，即不存在通用算法以线性方式加以解决。唯一的办法就是尝试一种可能的安排，看看是否符合要求，如果不行则推倒重来。</p><p>&nbsp;</p><p>GPT-4可以通过在上下文窗口中添加更多文本来完成回溯，但其扩展能力仍然有限。更好的方法是为GPT-4提供一个“退格键”，这样它就能删除最后一个或几个推理步骤，然后重试。为此，系统还需要一种方法来跟踪它已经尝试过的组合，避免重复尝试。如此一来，大语言模型就能探索下图所示的可能性树：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ed8f78ee2b5f43a4ff715f21bb0f4e1.png" /></p><p></p><p>&nbsp;</p><p>今年5月，普林斯顿大学和Google DeepMind的研究人员共同发表论文，提出一种名为“思路树”的方法。思路树不再用单一推理链来解决问题，而是允许大模型系统探索一系列指向不同方向的推理链“分支”。</p><p>&nbsp;</p><p>研究人员发现，该算法在解决某些传统大语言模型难以解决的问题上表现良好。其中不仅包括所谓“24点游戏”（即通过添加运算符号将随机给出的几个数字计算为24），还实现了创意写作能力。</p><p>&nbsp;</p><p></p><h1>AlphaGo模型</h1><p></p><p>以上，就是OpenAI和DeepMind迄今为止发表过的所有研究成果，可以看到他们都在让大语言模型更好地解决数学问题方面付出了不懈努力。现在，我们一起来推测这项研究最终可能会走向何方。当然，这些猜测没有任何依据，大家也可以根据自己掌握的情况做出展望。</p><p>&nbsp;</p><p>今年10月，播客Dwarkesh Patel曾就通用人工智能开发计划采访过DeepMind联合创始人兼首席科学家Shane Legg。Legg认为，迈向AGI的关键一步就是把大语言模型跟搜索可能响应的树结构结合起来：</p><p></p><blockquote>这些基础模型属于某种世界模型，通过搜索方式实现问题的创造性解决能力。以AlphaGo为例，它那惊人的棋路到底是从何而来？是学习了人类棋手的经验，还是参考了原有数据？不，根本没有。它其实是选择了一个非常罕见、但也极为合理的棋步，再通过搜索过程思考这步棋会造成怎样的后续影响。也就是说，要想获得真正的创造力，必须探索可能性空间并找出隐藏其中的最佳答案。</blockquote><p></p><p>&nbsp;</p><p>Legg在这里提到了著名的“第37手”，即2016年DeepMind AlphaGo软件与顶尖棋手李世石第二场比赛中的一步。大多数人类选手最初都觉得AlphaGo在这步棋上出现了失误，但其最终刻了比赛，且复盘分析发现这是一手强棋。换言之，AlphaGo表现出了超越人类棋手的布局洞察力。</p><p>&nbsp;</p><p>AlphaGo能够根据当前棋盘状态模拟出数千种可能的后续发展，从而获取类似的见解。对于计算机来说，潜在棋序实在太多，根本不可能一一检查，所以AlphaGO使用神经网络来简化整个过程。</p><p>&nbsp;</p><p>其中的策略网络能够预测出哪些棋路最有希望，值得进一步做模拟分析。而价值网络则负责估算棋盘的当前状态是对白方有利、还是对黑方有利。根据这些估算，AlphaGo再逆向计算下面一步该怎么走。</p><p>&nbsp;</p><p>Legg的观点是，这类树搜索方法有望提高大语言模型的推理能力。大语言模型要预测的不只是单个最可能出现的token，而应在给出回答之前探索数千种不同的响应。事实上，DeepMind的思维树论文似乎就是朝这个方向迈出的第一步。</p><p>&nbsp;</p><p>前文提到，OpenAI曾经尝试使用生成器（生成潜在答案）与验证器（估算这些答案是否正确）组合来解决数学问题。这与AlphaGo明显有几分相似，同样可以理解成策略网络（生成潜在棋步）与价值网络（估算这些棋步能否导向更有利的盘面状态）。</p><p>&nbsp;</p><p>如果将OpenAI的生成器/验证器网络与DeepMind的思维树概念相结合，就能得到一套与AlphaGo非常相似的语言模型，同时保留AlphaGo的强大推理能力。</p><p>&nbsp;</p><p></p><h1>为何命名为Q*</h1><p></p><p>在AlphaGO之前，DeepMind曾在2013年发表过一篇关于训练神经网络以打通雅达利电子游戏的论文。DeepMind并没有手动录入每款游戏的规则，而是让网络不断游玩这些游戏，通过反复试验自行理解玩法。</p><p>&nbsp;</p><p>参考早期强化学习技术Q-learning，DeepMind将这套雅达利解决方案命名为Deep Q-learning。DeepMind的雅达利AI中包含一个Q函数，用于估算任意特定操作（例如向左或向右推操纵杆）可能获得的奖励（比如更高的得分）。当系统游玩雅达利游戏时，它会不断优化Q函数，提升获取更佳得分的估算能力。</p><p>&nbsp;</p><p>DeepMind 2016年在AlphaGo论文同样使用字母Q来表示AlphaGo中的棋步价值函数——该函数用于估算任意给定棋步有多大可能通往对局胜利。</p><p>&nbsp;</p><p>AlphaGo和DeepMind的雅达利AI都属于强化学习的范畴，这是一种从经验中学习知识的机器学习技术。在大语言模型兴起之前，OpenA也I一直将强化学习作为关注重点。例如，OpenAI曾在2019年使用强化学习让机械臂在自行探索中学会解开魔方。</p><p>&nbsp;</p><p>参考这些背景，我们似乎可以对Q*做出有理有据的解读：它是将大语言模型同AlphaGo式搜索能力相结合的产物，而且应该是在以强化学习的方式进行混合模型训练。其重点就是找到一种在困难的推理任务中“自我较量”的方式，借此改进语言模型的实际能力。</p><p>&nbsp;</p><p>其中一条重要线索，就是OpenAI今年早些时候决定聘请计算机科学家Noam Brown。Brown在卡耐基梅隆大学获得博士学位，并在那里开发出首个能够超越人类水平的扑克AI。之后Brown加入Meta，并开发出玩《强权外交》桌游的AI。这款游戏的成功秘诀在于同其他玩家结成联盟，因此AI必须把战略思维与自然语言能力结合起来。</p><p>&nbsp;</p><p>由此看来，这似乎就是帮助大语言模型提高推理能力的绝佳案例。</p><p>&nbsp;</p><p>Brown今年6月在推文中表示，“多年以来，我一直在研究扑克和〈强权外交〉桌游中的AI自我对弈和推理课题。现在，我想探索如何将成果转化为普适性能力。”</p><p>&nbsp;</p><p>AlphaGo和Brown扑克AI中使用的搜索方法，明显只适用于这些特定游戏。但Brown预测称，“如果我们能发现一个通用版本，则必然带来巨大的收益。没错，推理速度可能会降低至千分之一且成本迅速膨胀，但如果能够发现新的抗癌药物、或者证明黎曼猜想，这一切难道不值得吗？”</p><p>&nbsp;</p><p>而在Brown于今年早些时候离职之后，Meta公司首席AI科学家Yann LeCun表示，他认为Brown研究的就是Q*。</p><p>&nbsp;</p><p>LeCun在11月的推文中指出，“看起来OpenAI更进一步的探索就是Q*，他们还聘请了Noam Brown来协助解决这个问题。”</p><p>&nbsp;</p><p></p><h1>两大挑战</h1><p></p><p>&nbsp;</p><p>如果大家跟科学家或者工程师共事时，就会注意到他们特别喜欢用白板。当我自己在研究生院学习计算机科学时，我们就经常站在白板前面绘制图表或者议程。随后在谷歌的实习经历，也让我意识到技术大厂里同样到处都是白板。</p><p>&nbsp;</p><p>白板确实很有启发意义，因为面对极为困难的技术问题，人们刚开始根本不知道该如何下手。他们可能会花几小时勾勒出了种潜在的解决思路，却发现根本就不适用。之后他们就擦掉一切，从零开始找个不同的切入角度。或者，他们也可能觉得方案的前半部分还行，于是擦掉后半部分再换条新的探索路线。</p><p>&nbsp;</p><p>这本质上就是一种智能树搜索：对多种可能的解决方案进行迭代，直到找出一个似乎可以实际解决问题的路线。</p><p>&nbsp;</p><p>OpenAI和DeepMind之所以对大语言模型加AlphaGo搜索树感到如此兴奋，就是因为他们希望计算机也能执行同样的开放式智能探索。到那个时候，我们只需要把充满挑战的数学问题输入给大语言模型，然后安心上床睡觉。第二天早上醒来，它已经考虑了几千种可能的解决方案，并最终给出一些可能有希望的探索方向。</p><p>&nbsp;</p><p>这当然是个鼓舞人心的愿景，但OpenAI至少还要克服两大挑战才能将其转化为现实。</p><p>&nbsp;</p><p>首先，就是找到一种让大语言模型进行“自我对弈”的方法。AlphaGo就是通过自我对弈完成了对顶尖人类棋手的碾压。OpenAI也在模拟物理环境中进行魔方实验，通过判断魔方是否处于“解开”状态来判断哪些操作有正向作用。</p><p>&nbsp;</p><p>而他们的梦想就是建立起一套大语言模型，通过类似的自动化“自我对弈”方式提高推理能力。但这就需要一种能够自动检查特定解决方案是否正确的办法。如果系统还需要人类来检查每条答案正确与否，那么训练规模将非常有限、难以带来可与人类匹敌的推理水平。</p><p>&nbsp;</p><p>就在2023年5月发表的论文中，OpenAI还在聘用审查员来核对数学答案的正确性。所以如果真的出现了突破，那肯定是发生在过去这几个月间。</p><p>&nbsp;</p><p></p><h1>学习是个动态的过程</h1><p></p><p>&nbsp;</p><p>我认为第二个挑战才是根本：通用推理算法，必须在探索各种可能性时表现出动态学习能力。</p><p>&nbsp;</p><p>当人们尝试在白板上推衍解题思路时，他们并不是在机械地迭代各种可能路线。相反，每试过一个失误的路线，人们对问题的理解也就又加深了一步。在推理过程中，他们的心理模型也在不断演进，逐渐生出能快速判断哪种方法更好的强大直觉。</p><p>&nbsp;</p><p>换句话说，人类内心的“策略网络”和“价值网络”并非一成不变。我们在同一个问题上花费的时间越多，在思考潜在答案时的判断能力也就增强，自然更善于预测当前思路是否有效。如果没有这种实时学习能力，我们一定会迷失在无穷无尽的潜在推理步骤当中。</p><p>&nbsp;</p><p>相比之下，目前大多数神经网络在训练和推理之间保持着严格的边界。一旦训练完成，AlphaGo的策略和价值网络就被固定下来了——后续任何比赛过程都不会产生改变。这对围棋来说没有问题，因为这项游戏的规则足够简单，可以在自我对弈的过程中体验各种可能的情况。</p><p>&nbsp;</p><p>但现实世界要比方寸棋枰复杂得多。从定义上讲，研究者想要解决的是以往未能解决过的问题，所以实际情况很可能与训练期间遇到的任何问题都存在巨大差异。</p><p>&nbsp;</p><p>因此，通用推理算法的实现必须在推理过程中持续获取见解，以便在模型解决问题的同时不断增强后续决策质量。然而，目前的大语言模型完全通过上下文窗口来维持状态，而思维树方法在现有模型的一个分支跳往另一分支时，之前的记忆信息会被新的上下文窗口直接删除。</p><p>&nbsp;</p><p>一种可能的解决方案，就是使用图搜索来取代树搜索。今年8月的一篇论文就提到这种方法，尝试让大语言模型将来自多个“分支”的见解结合起来。</p><p>&nbsp;</p><p>但我高度怀疑，真正的通用推理引擎恐怕需要在底层架构上做根本性创新。语言模型必须借助新的方法来学习超越训练数据的抽象概念，并利用这些不断发展的抽象概念强化探索潜在解决方案空间时的具体选择。</p><p>&nbsp;</p><p>我们都知道这绝非妄言，毕竟人类的大脑就能做到这一点。而OpenAI、DeepMind乃至其他厂商可能还需要一段时间，才能搞清楚如何把这种方法照搬到硅芯片之上。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors">https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</id>
            <title>多场开发实战课，百度智能云技术大咖现场教学！</title>
            <link>https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:42:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术, 智能化跨越, 百度云智大会, AI原生应用
<br>
<br>
总结: 大模型技术正在推动各行业的智能化跨越，百度云智大会是一个重要的活动，旨在引领智能计算技术创新，传递最新实践与突破。AI原生应用的构建和实际落地是关键，开发者需要将AI技术与实际应用场景相结合，开发有用、有价值的产品和服务。 </div>
                        <hr>
                    
                    <p>大模型技术正在以前所未有的速度推动各行业的智能化跨越。对于身处这个时代的开发者来说，他们不仅需要不断学习新知识，还要探索如何将 AI 更好地融入实际应用场景。</p><p></p><p>面对崭新的时代，开发者若想找到一条提升思维认知和开发效率的最短路径，百度智能云每年举办的百度云智大会·智算大会是不容错过的：</p><p></p><p>智能计算大会是百度智能云面向“云计算产品与技术”的重磅活动之一，以引领智能计算技术创新为目标，传递百度智能云产品与技术的最新实践与突破。历经 3 载，从 AI 原生云到深入产业，百度智能云传递着创新的火种，描绘着智能计算的未来。2023 年，智能计算大会全新起航，将以“重构云计算·Cloud for AI”为主题，结合大模型技术以及 MaaS 服务，碰撞最前沿的技术与产品，开启全新的智能计算时代。</p><p></p><p>“工欲善其事，必先利其器”。对于开发者来说，百度云智大会·智算大会是你不可或缺的技术盛宴，它不仅是探索前沿科技的窗口，更是开发实战的“课堂”。</p><p></p><p>2023 百度云智大会·智算大会将于 12 月 20 日在北京落地，本次大会以“重构云计算·Cloud For AI”为主题，汇集了百度集团副总裁侯震宇、IDC 中国区副总裁兼首席分析师武连峰、百度副总裁谢广军等多位行业大咖，聚焦 AI 和云，解读智能计算带来的万千可能和全新图景，带开发者窥见 AI 原生时代的技术创新重构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f9eea7c6ff5cd2c5ecdb72404cb32b.png" /></p><p></p><p>仅是让大家了解 AI 原生时代的趋势还不够！为了让你获得知行合一的参会感受，下午特别开设了「2023 百度云智大会·智算大会 开发者沙龙」，旨在为开发者提供切实有效的开发技能。 今年 9 月，李彦宏曾在 2023 百度云智大会上强调 AI 原生应用的重要性，他表示，AI 原生应用要能解决过去解决不了、解决不好的问题，应用才是大模型存在的意义。这意味着在 AI 大模型时代，AI 原生应用的构建和实际落地是关键。对于所有开发者而言，则需要能在先进技术和模型的基础上，将 AI 技术与实际应用场景相结合，开发出有用、有价值的产品和服务。</p><p></p><p>为了让开发者能够实操跟练，下午场的「2023 百度云智大会·智算大会 开发者沙龙」活动，由百度智能云主任架构师吴多益、百度资深工程师 &amp; 百度 Comate 产品架构师徐晓强等技术大咖担任分享讲师。</p><p></p><p>实践课程设置方面，由浅入深地涵盖了从编码到应用开发的内容，帮助开发者通过现场实战，学习热门产品及技术、提升软件开发效率，打破在技术与实际应用场景结合方面的障碍。期待参与其中的你，不仅能够掌握提升开发效率的方法，还能建立起构建 AI 原生应用的思维方式。</p><p></p><p>本次沙龙，还为开发者准备了丰富的互动礼品，完成任意一场课程及实验，即可获得精美礼品！线下席位有限，抓紧扫码占位！12 月 20 日 13:00，我们在「2023 百度云智大会&nbsp;· 智算大会 开发者沙龙」不见不散！</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8203ab57b2d095c1230776b92aabdf3.jpeg" /></p><p></p><p>                                                           一起掌握开发“金手指”</p><p></p><p>                                                        提升开发效率，准时下班吧！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</id>
            <title>大语言模型加速信创软件 IDE 技术革新</title>
            <link>https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:38:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化信创软件 IDE, 信创, 智能化, 大语言模型
<br>
<br>
总结: 本文介绍了智能化信创软件 IDE 的重要性和意义。智能化技术的发展使得软件开发工具更加强大，例如自动化重构、代码翻译和自动化文档生成等功能。智能化信创软件 IDE 的目标是实现核心技术的可掌控和可研究，以规避信息安全、供应链安全、技术依赖和经济风险。通过信创化，可以解决现有技术的问题并实现超越。同时，文章还介绍了华为云开发工具和效率领域首席专家王亚伟的观点和团队的研发工作。在 QCon 全球软件开发大会上，王亚伟和他的团队将分享关于大语言模型、AI 编码辅助和下一代 IDE 平台架构等技术的内容。 </div>
                        <hr>
                    
                    <p>什么是智能化信创软件 IDE？为什么它很重要？</p><p>&nbsp;</p><p><a href="https://qcon.infoq.cn/2023/shanghai/schedule">QCon 全球软件开发大会（上海站）</a>"将于 12 月 28-29 日举办，会议特别策划「智能化信创软件 IDE」专题，邀请到华为云开发工具和效率领域首席专家、华为软件开发生产线 CodeArts 首席技术总监<a href="https://qcon.infoq.cn/2023/shanghai/track/1598">王亚伟</a>"担任专题出品人，为专题质量深度把关。作为拥有云和开发工具领域近 20 年经验的老兵，华为公司软件开发工具领域的领军人物，20 多项软件开发技术发明专利的拥有者，王亚伟对于「智能化新创软件 IDE」这个专题有着怎样的理解？在会议即将开幕之际，王亚伟与 InfoQ 分享了他的核心观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29e75f26136948c06ee3f9bfd82139f8.jpeg" /></p><p></p><p>&nbsp;</p><p>“信创”是信息技术应用创新的简称，其本质是发展国产替代技术，实现核心技术的可掌控、可研究、可发展等。</p><p>&nbsp;</p><p>相比“信创”，“智能化”在过去 5 年中被业界反复提起，智能化技术的发展必然会使诸如 IDE 这样的软件开发工具更加强大。随着大语言模型的诞生，IDE 除了可以自动地完成一些重复性工作之外，还可以协助开发人员在软件的设计和开发过程中完成更多创新性的工作，比如：</p><p>自动化重构：将一段复杂的代码分解为更小、更易于管理的函数或类。开发者可以描述他想要实现的重构目标，然后让模型生成相应的代码代码翻译：大语言模型可以将一种编程语言的代码翻译成另一种编程语言，再配合 IDE 的语法高亮和错误检查功能，可以帮助开发者使用不熟悉的编程语言编写代码自动化文档生成和更新：大语言模型可以根据代码和注释生成相应的文档，或者在修改代码时自动更新文档。大语言模型是 IDE 的智能化加速度</p><p>&nbsp;</p><p>IDE 的”信创“化旨在将基础软件开发的核心技术实现自主可控，在拥抱开源的同时逐步建立基于自有技术内核的架构和标准，形成自有开放生态。信创化的目的是为了规避可能或已经发生的风险：</p><p>信息安全和供应链安全风险：在关键时刻，国外的产品和技术可能会面临供应链中断的风险。此外，国外产品或开源技术可能会存在安全漏洞或后门，基于这些技术打造的商业解决方案会威胁用户的信息安全 - 2020 年 3 月发生的 SolarWinds 攻击事件导致业界领先的开发工具公司 JetBrains 遭受牵连技术依赖风险：如果完全依赖于外国的技术，那么我们在软件开发核心技术领域的研究、发展和创新能力就会受制于人，最终导致落后经济风险：技术上依赖意味着我们需要持续支付大量的许可费用</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/d4613NRodWJEAXqRblEu">延伸阅读：被逼出来的自主可控，从华为自研看国产 IDE 的未来和商业模式</a>"</p><p>&nbsp;</p><p>“信创”化不意味着重复造轮子或为了与现有技术不同而进行盲目创新，而是目标实现核心技术可控的前提下，解决现有技术的问题，从而对现有技术实现某些方面的超越。举个例子，代码索引是 IDE 的文件查找、代码提示等功能的基础数据源，现有商业 IDE 代码索引的创建、存储和访问效率并不高，索引数据基于对象存储访问时，一个只有 8 字节（2 个 int）内容的数据封装成对象后要占据至少 24 个字节的存储空间。同时，由于内存读写速率要远低于缓存，如果在存储和访问索引时没有以一种缓存友好的方式进行，读写效率甚至 100 倍下降。我们团队在代码索引存储和访问领域提出了一种基于内存压缩的索引自动化存储和访问技术，可以做到 50 倍以上的综合效率提升，该技术已经被评选为华为云高价值专利，并应用到 IDE 内核、运行时优化、云编译等多个领域。” <a href="https://www.infoq.cn/article/ubciEs8NPH06CwlpEvtf">延伸阅读：生成的代码会出错、质量差？面对 AI 编程工具的老大难问题，华为这群人打算这样做</a>"</p><p>&nbsp;</p><p>技术的积累需要时间，产品研发更需要打磨。王亚伟介绍道：“从 2019 年初开始，我们逐步组建了一支数百人的软件研发专家队伍，分布在中国、俄罗斯、欧洲等国家地区，其中一半成员来自于业界顶尖的软件和工具公司，超过 40% 的成员是开源社区的 Committer 和 Contributor，整个团队都围绕着‘做最好的产品’展开工作，我们建立了从产品、运营、UX 到开发、测试的完整专业的产品研发流程，每月一个小版本、三个月一个大版本，基于内外部用户的反馈快速迭代。过去五年我们真正做到了深耕软件开发工具这个专业领域。”</p><p>&nbsp;</p><p>同时，王亚伟也坦言：“虽然从产品成熟度上看我们的信创化工具跟业界成熟的商用工具相比还有差距，但‘信创’本身绝不意味着竞争力弱，体验打折。我们会继续秉持着‘做最好的产品’的信念，不断前进。”</p><p>&nbsp;</p><p>在今年的「智能化信创软件 IDE」专题上，王亚伟带领他的专家团队将围绕大语言模型、AI 编码辅助、下一代 IDE 平台架构、动态语言类型推理等技术，给大家带来一场技术盛宴。</p><p>&nbsp;</p><p>议题<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5567">《AI 开道，让编程体验“一路狂飙》</a>"，详细介绍华为云 <a href="https://www.huaweicloud.com/devcloud/">CodeArts</a>" 团队应用大模型开发的 AI 辅助编程的技术 - CodeArts Snap，讲师程啸从博士阶段开始就对代码生成、RAG、代码克隆检测等领域有较深入的研究，他这次也是代表 Snap 团队进行分享。</p><p>&nbsp;</p><p>另外三个英文议题是来自于 CodeArts 俄罗斯的专家团队。</p><p>&nbsp;</p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5568">Applying Machine Learning in IDE Challenges and Insights</a>"将会系统讨论 AI 技术在 IDE 中的应用研究以及如何深远改变我们的开发，测试和调试代码的方式。讲师 Pavel 是俄研院新西伯利亚实验室主任，20 年开发者工具构建经验，机器学习专家、Eclipse IDE 的专家和 Committer。</p><p>&nbsp;</p><p>议题 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5575">Evolution of IDE Platforms</a>" 会紧密围绕其在构建 IDE 平台时面临的问题和挑战比如分布式代码模型架构下如何确保前后端组件可以高效交互、如何直接从后端内核画出前端复杂 UI，以及我们如何做出艰难的架构和设计决策，同时分享对下一代 IDE 平台的架构和设计展望。Denis 是俄罗斯新西伯利亚实验室的首席架构师，20 多年的工具研发经验，精通编译器、DSL、编程框架，Eclipse 社区 Committer。</p><p>&nbsp;</p><p>静态语言如 Java，C# 等，它的类型推理主要通过编译器完成，代码模型可以通过类型绑定（通常存在于程序的元数据 metadata 中）获得所需要的类型信息。而动态语言的类型推理主要由 IDE 完成，由于缺少编译元数据的支持，动态语言的类型推理是一个业界难题。以 Python 为例，其有一个完全动态严格的类型系统，类型（type）在运行时动态绑定到变量（variable），变量和类型都可以在运行时动态被改变 – 这增加 Python IDE 进行可靠类型推理的难度。议题 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5576">Type inference engine</a>" 会介绍该团队在做动态语言类型推理时设计和实现的技术细节，并讨论未来该领域的发展方向。Nikolai 是俄罗斯圣彼得堡实验室的首席软件工程师，拥有 15 年 IDE 研发经验，是前 JetBrains Intellij IDEA 和 Scala 项目负责人，精通 Compiler、Program Language Design、Code Analysis 等技术。</p><p>&nbsp;</p><p>据了解，QCon 上海还邀请到了<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5623">中国科学院外籍院士、国际数据库专家樊文飞院士</a>"，<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5586">英特尔大数据技术全球 CTO 戴金权</a>"等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，邀请<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5599">阿里巴巴的通义星尘</a>"、魔搭社区开源 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5673">ModelScope-Agent</a>" 框架、百度文心大模型驱动下的智能代码助手等团队核心技术骨干前来分享，目前大会日程已上线，<a href="https://qcon.infoq.cn/2023/shanghai/schedule">可点击下方图片查看详情。</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/66/66a620fc97b46ea55958f9b172195701.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a6b81251700257adb01b1334e7d49f9.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</id>
            <title>从业务应用挑战出发，火山引擎专家深度拆解“弹幕互动方案”的全新实践</title>
            <link>https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, 视频化时代, 火山引擎, 弹幕互动玩法
<br>
<br>
总结: 互联网正在进入视频化时代，火山引擎作为一种视频云服务，与NVIDIA合作推出了《云上新视界》线上课程，其中分享了弹幕互动玩法的解决方案与应用实践。弹幕互动玩法是一种通过弹幕、送礼物等互动操作控制直播画面中互动内容的直播方式，具有即开即玩、多人互动等特点。弹幕互动经历了PC端开播、云游戏方案和云游戏+RTC方案三个核心演进阶段。火山引擎通过优化方案解决了弹幕互动延时和外放回声的问题。 </div>
                        <hr>
                    
                    <p>从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，<a href="https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">火山引擎</a>"视频云以“面向体验，驱动创新”为核心，特别与 NVIDIA 团队合作推出《<a href="https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">云上新视界</a>"》线上课程。第五期课程中，火山引擎 RTC 商业化解决方案团队负责人郭健为大家分享了当前热门玩法“弹幕互动”的解决方案与应用实践。</p><p></p><h2>一、什么是“弹幕互动玩法”?</h2><p></p><p></p><p>弹幕互动玩法是依托直播间（直播连麦、语聊房等互娱核心场景），观众可以通过弹幕、送礼物等互动操作，控制直播画面中的互动内容的一种直播方式，具备即开即玩、多人互动等特性，兼具观众互动性强、直播内容游戏化趣味化等特点。</p><p></p><p>从 2014 年的《Plays Pokémon》到 2021 年尾《修勾夜店》爆火，弹幕互动几经翻红。今年开始，弹幕互动受到各大平台的广泛关注，从玩法上线后效果看弹幕互动玩法的户观看人数 / 时长、营收等核心指标都有很好的收益。</p><p></p><h2>二、弹幕互动方案的 3 个核心演进阶段</h2><p></p><p></p><p>弹幕互动经历了 PC 端开播、云游戏方案、云游戏 + <a href="https://www.infoq.cn/article/Ue0E2ZXpr2BwaYxlQ0fL?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">RTC </a>"方案三个阶段。</p><p></p><p>第一个阶段，PC 端开播。传统开播流程需要主播先在 PC 端安装程序和开播工具，互动玩法在主播 PC 上运行和渲染。同时，主播使用 PC 端直播工具（比如 OBS）对本地画面和主播直播画面混流，再推送到直播间。观众进入直播间发送弹幕或者发送礼物参与互动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f65bdaf68b7489076c5d4af003097847.png" /></p><p></p><p>这种方式存在一定局限性，比如：</p><p></p><p>弹幕互动内容本身需要实时计算渲染，对设备硬件配置如显卡计算能力有较高要求，甚至堪比 3A 大作性能要求，开播设备性能不足，就会导致弹幕无效甚至内容本身卡死，影响直播间用户体验；越来越多的主播更习惯在移动端随时开播，而只能运行在 PC 端的弹幕互动程序，会大大增加开播门槛，也降低平台玩法覆盖度；移动端开播还可以与平台其他玩法相结合，但如果单独为弹幕玩法准备 PC 端 OBS 开播，既增加了维护成本，也难以进行推广。</p><p></p><p>第二阶段，在直播 / 语聊的基础上引入云游戏。主播进入连麦房间推拉 RTC 流的同时，也需要进入云游戏的房间拉取互动玩法音视频。然后业务层把 H5 引擎拉取到的视频流和业务层采集到的摄像头流在端上合流后，推入直播房间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07a7368a086b934ca4812074f93dbffb.png" /></p><p></p><p>这个方案解决了开播平台限制和开播设备的限制，但是有一些方案接入和体验问题。从方案层面看，业务逻辑复杂接入相对麻烦。从体验看，存在嘉宾 / 观众侧主播解说和互动画面会有轻微的不同步、画面延时大、有回声等问题。其中，RTC 引擎订阅云游戏音频观众侧有回声主要是因为游戏流的声音或者麦克风会采集到本地播放的游戏声音。</p><p></p><p>为了解决上个方案的几个问题，火山引擎视频云首推“云游戏 +RTC 方案”方案，而弹幕互动方案也正式进入了第三阶段——火山引擎 RTC 与云游戏产品在服务侧和引擎侧做了深度协同优化。在服务侧，优化了调度方案，保证用户连接的云游戏 pod+RTC 媒体服务器在同一个机房、云游戏音视频流可直接送入 RTC 房间。在引擎侧，云游戏引擎直接依赖宿主侧的火山 RTC 引擎、云游戏引擎裁剪场景无关功能。</p><p></p><p>在具体操作中，首先主播通过云游戏引擎开启互动完成程序，云游戏启动 pod 并创建火山 RTC 房间。完成后，Pod 集成云游戏引擎和 RTC 引擎向火山云游戏房间推音视频流，火山云游戏房间跨房转推音视频流到两个直播 / 语聊房间，嘉宾和观众通过 RTC 直接拉取直播流和云游戏流即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/918deed71d6b8a03257846fafd65c7d3.png" /></p><p></p><p>云游戏和 RTC 内部深度协作，缩短数据流转链路在接入直播 / 语聊的基础上，仅需接入 veGameSDK 启动游戏、业务端通过 OpenAPI 同步弹幕 / 礼物数据到云游戏服务器两步即可完成场景“升级”，大大简化业务逻辑，缩短接入周期减少工作量。</p><p></p><h2>三、火山引擎是怎么解决历史方案问题的？</h2><p></p><p></p><p>此前弹幕互动方案所存在的观众弹幕互动延时、主播外放有回声等体验问题，火山引擎方案是如何解决的？</p><p></p><h4>&nbsp;1. 弹幕互动延时问题</h4><p></p><p></p><p>未优化的云游戏方案观众端发送弹幕后，由于传统 RTMP 直播流延迟较大，观看云游戏观众侧会有 3~5 秒延时，并且都会有轻微的互动画面与解说的不同步，体验较差。这些在普通常见的场景可能影响不大，但是在对战场景，战场形势瞬息万变，可能最后一秒的延时失去被“偷家”导致战斗失败。</p><p></p><p>优化后，使用全 RTC 方案，可以让用户参与玩法整体延时&lt;400ms 。</p><p></p><h4>&nbsp;2. 外放回声消除</h4><p></p><p></p><p>在未优化方案中，云游戏的声音在经过扬声器播放后，会被近端用户的麦克风采集到并产生回声问题，需要参考扬声器播放的声音进行回声消除技术处理，云游戏和 RTC 独立运行，云游戏音频无法给到 RTC 引擎，所以容易产生回声。</p><p></p><p>在优化方案中，云游戏音频可以直接跨房转推到 RTC 房间，场景内音频播放通过音频托管的方式统一由 RTC 进行音频播放，有参考信号，可以彻底消除回声，以确保对端收到清晰的声音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7615c34243048d5d4edabe66d99ef81.png" /></p><p></p><p></p><h2>四、弹幕互动方案在业务应用中的挑战与实践</h2><p></p><p></p><h4>&nbsp;1. 卡顿优化</h4><p></p><p></p><p>弹幕互动场景有一个特点就是画面极致高清，一般是高清 1080P、 帧率 30fps、高码率 8Mbps。同时，主播、观众均为移动端设备，随时开播与参与，用户网络环境复杂且不稳定。在这种高分辨率高码率、且网络不稳定情况下极其容易造成卡顿劣化。</p><p></p><p>要优化这种情况，首先把线上 H264 升级为自研 ByteVC1 编解码，在 PSNR（视频质量客户评价）画质质量优于原方案 2dB 时，还能节约 10% 码率。此时对于线上情况码率可能仍较大，火山引擎 RTC 采用智能流控协议 (VISC)，它基于 Simulcast 和 SVC 策略优化而来、更加智能的一种传输协议，它可以综合考虑音视频通话中每个订阅者的个性化需求，在网络情况、终端性能发生变化的时候，自动调整音视频流的配置，最大限度地让每个参与者的个性化需求得到满足，为用户提供更流畅的互动体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a57bdf9d82120b552297cf2fbbe2456.png" /></p><p></p><h4>&nbsp;2. 操作延时优化</h4><p></p><p></p><p>云游戏在所有的云计算相关应用中，对延时要求最为苛刻，火山引擎 RTC 针对云游戏与 RTC 场景相结合的应用场景，进行全链路延时优化。</p><p></p><p>阶段一，边缘机房阶段。保证用户连接的云游戏 Pod 和 RTC 服务器调度到同一个机房，使用更高效传输方式优化，首帧时长减少约 30ms；降低延时 50ms；编码前优化采集和格式转换，使用 OpenGL 转换替换 libyuv 转换，优化延迟 15ms;阶段二，级联服务优化。减少级联服务器和优化信令传输，优化 20ms;阶段三，订阅端。针对云游戏下行音视频调整 jitterbuffer 大小，降低延时 60~260ms，有优化的处理，可以不影响直播 / 语聊体验；针对不同的硬件解码器做优化，最多优化延迟 90ms；内部渲染替代外部渲染降低延迟 5ms，整体云游戏到端延时可以达到小于 75ms。</p><p></p><h4>&nbsp;3. 性能优化</h4><p></p><p></p><p>弹幕互动玩法可以在个人直播、直播连麦或者跨房 PK 中等场景中加入。在语聊房跨房 PK+ 弹幕互动玩法场景中，假设每个语聊房会有 9 人，两个房间 PK 时，单个用户最多需要拉 18 路音频流和云游戏音视频流，性能压力大，玩法准入机型门槛高，设备发热严重。</p><p></p><p>因此，为减少对手机性能消耗，火山引擎 RTC 使用 RTC 公共流不进房拉流方案。这个方案中，本房间内拉流方式不变，PK 房间的音频流合流后推一路公共流，对比普通语聊模式单个用户只多拉一路音频流和一路云游戏流。两个房间 PK，每个房间 1 位主播、8 位嘉宾、100 位观众流数评估，单房间减少（1+8+100）*8 约 872 路、单用户减少 8 路流，有效优化用户拉流性能，减少 50% 流数量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c3b679655ff084ab5807c48eece0a0f.png" /></p><p></p><p>独立集成云游戏 SDK 包体增量一般 9M 左右，9M 的包增量对客户来说是不可接受的。弹幕互动方案中云游戏直接复用火山引擎 RTCSDK 传输能力，云游戏 SDK 精简包只需操控信令和选路部分，精简包给整体带来增量仅 610KB。</p><p></p><h2>五、写在最后</h2><p></p><p></p><p>总体来说，火山引擎弹幕互动方案有五大优势：</p><p></p><p>不限设备、不限场景，零门槛开播：无论是个播还是多人互动，移动端即可随时随地“云开启”弹幕互动玩法，无需高性能 PC，消除互动内容本身对用户终端算力的限制；热门弹幕互动内容全适配：云游戏支持直接部署基于 UE/Unity 框架的互动内容，底层多种类型 IaaS 和对应 GPU 配置，满足不同等级算力要求的弹幕互动玩法；无惧弹幕高并发，渲染画面高清流畅：云游戏支持 ARM、x86 以及定制化 GPU 等多样化计算资源，并采用自研 ByteVC1 编解码结合动态码率技术，保证互动画面流畅体验同时节约带宽消耗，互动画面 100ms 卡顿率低于 2%；主播解说与玩法进程实时同步：通过火山引擎 RTC 媒体节点和 云游戏 Pod 端同机房调度，超低延时体验，操作延时小于 90ms，主播讲解和内容画面实时同步，保障观众沉浸互动体验；应用最小包增量引入：弹幕互动方案中云游戏可直接复用火山引擎 RTC SDK 传输能力，云游戏 SDK 精简包只需操控信令和选路部分，精简包增量仅 KB 级。</p><p></p><p>而本期课程中介绍的弹幕互动玩法的解决方案技术实践只是“小试牛刀”，如果想要了解更多，可以扫描下方二维码，有更加详细的弹幕互动解决方案和获取弹幕互动 Demo！</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebba54e6aafb6fea8351787b6285c768.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</id>
            <title>英特尔软件与先进技术事业部 / 首席工程师胡宁馨确认出席 QCon 上海，分享 WebNN，Web 端侧推理的未来</title>
            <link>https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, WebNN, W3C 标准, AI推理
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，胡宁馨将分享关于WebNN API的主题演讲，探讨WebNN API的W3C标准进展以及对CNN、Transformer和生成式AI模型的支持情况和计划，以及在浏览器的实现进展。WebNN API提供了Web应用访问AI加速器的途径，以获得更好的性能和更低的功耗。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。英特尔软件与先进技术事业部 / 首席工程师胡宁馨将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5646?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">WebNN，Web 端侧推理的未来</a>"》主题分享，探讨 WebNN API 的 W3C 标准进度，对 CNN，Transformer 以及更广泛的生成式 AI (Generative AI) 模型的支持情况和计划，以及在 Chrome，Edge 等浏览器的实现进展。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5646?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">胡宁馨</a>"，就职于 Intel 软件与先进技术事业部，专注于 Web 技术，W3C 机器学习工作组 Web Neural Network API (WebNN) 规范的发起者和联合编辑，Chromium 项目 Code Committer，WebNN 模块负责人。他在本次会议的演讲内容如下：</p><p></p><p>演讲：WebNN，Web 端侧推理的未来</p><p></p><p>AI PC 以及 AI Mobile 的新兴时代已经到来，越来越多的设备集成了强大的神经处理单元 NPU，以实现高效的人工智能加速，这对需要端侧推理的应用至关重要。除了通过 CPU 和 GPU 进行推理之外，Web Neural Network API (WebNN) 提供了 Web 应用访问此类专有 AI 加速器 NPU 的途径，以获得卓越性能及更低功耗。</p><p></p><p>本次演讲将会给大家分享 WebNN API 的 W3C 标准进度，对 CNN，Transformer 以及更广泛的生成式 AI (Generative AI) 模型的支持情况和计划，以及在 Chrome，Edge 等浏览器的实现进展。作为 JavaScript ML 框架的后端，WebNN 将会在几乎不更改前端代码的前提下，为 Web 开发者及他们的产品带来相较于 Wasm，WebGL 更为优异的性能体验。</p><p></p><p>演讲提纲：</p><p></p><p>当前 Web AI 发展概况主流硬件加速器的发展（CPU，GPU，NPU)WebNN 设计与架构WebNN 代码演示WebNN 浏览器（Chromium）实现WebNN 机器学习框架集成（ONNXRuntime 和 TensorFlowLite)WebNN Transformers 支持WebNN 性能</p><p></p><p>听众收益点：</p><p></p><p>○ 了解 Web 平台对异构处理器的支持</p><p>○ 了解基于 Web 的机器学习模型硬件加速</p><p>○ 了解 Chromium 实现内部细节</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</id>
            <title>如何使用 Cluster Autoscaler 将批处理作业的节点扩容到 2000 个</title>
            <link>https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 02:13:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 火山引擎容器服务, VKE, Kubernetes, Cluster Autoscaler
<br>
<br>
总结: 本文介绍了火山引擎容器服务(VKE)作为云上Kubernetes平台的经历和挑战。文章首先解释了Cluster Autoscaler(CA)的概念和工作机制，包括自动调整集群大小和节点使用率的调度。然后介绍了CA的扩容和缩容逻辑，以及在实际客户场景中的应用。最后给出了一些建议，帮助实现集群弹性和避免类似的问题。 </div>
                        <hr>
                    
                    <p>本文将分享火山引擎容器服务 <a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ%3D%3D&amp;chksm=e8d7e80adfa0611c8447800f02c4c482f8746bad0d541c80b22615aa09000dbe3b6fd28b2761&amp;idx=1&amp;mid=2247489480&amp;scene=27&amp;sn=3b54e854a6756c5191ca1032e4d1189b&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">VKE</a>" 作为云上<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect"> Kubernetes</a>" 平台，在帮助客户实现集群资源弹性过程中的一些经历和挑战，共分为以下几个部分：</p><p></p><p>第一部分介绍什么是 CA，以及它内部的流程和实现方式，帮助大家更好地理解其工作机制；第二部分简要说明客户批处理作业的使用场景；第三部分把重心放在客户在使用 Cluster Autoscaler 的过程中，碰到的问题和挑战，以及我们是如何解决的；最后将给出一些建议，帮助大家更好地实现集群弹性，避免踩到类似的坑。</p><p></p><h2>什么是&nbsp;Cluster Autoscaler(CA)</h2><p></p><p></p><p>从 <a href="https://xie.infoq.cn/article/d26b441cebb5b229a6efa35f4?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Cluster Autoscaler </a>"项目的 README 文档中，可以看到它包括几个方面：</p><p></p><p>自动调整集群大小，即扩缩容因为集群中资源不足，才会扩容缩容时由于集群中的节点使用率低于阈值，这个低使用率的节点上的 Pod 可以调度到其他节点上去</p><p></p><p>下图展示了用户视角下 CA 扩容的情况。当集群中出现 Pending Pod，没有节点能让这些节点调度上去时，CA 就会触发扩容，往集群中加入新的节点，让 Pod 调度上去。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/219ece266705fec98c75af3dc71ab8d3.png" /></p><p></p><p>而节点的使用率较低，比如图中的低于 50%，CA 就会把这个节点删除，Pod 被重新调度到其他的空闲节点上。这样一来，集群中工作负载的数量不变，但是节点数减少了，剩余节点和集群整体的使用率就提高了，对用户来说，这相当于降本增效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4de3555a3ae6af17767f51424697db9b.png" /></p><p></p><p>CA 是一个定期重复执行的过程，如果简化一下，它大致可以分为以下几个部分：</p><p></p><p>准备工作，CA 会先从集群中获取相关的数据，比如节点、集群的状态、需要调度的 Pending Pod、清理创建失败的节点、过滤还没 ready 的 GPU 节点等；扩容逻辑；缩容逻辑；结束；等待一段时间后，再从头开始。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0136fcaa26a5f89d7082fabc55ff2d61.png" /></p><p></p><p>在扩容阶段，CA 会先找到集群中无法调度的 Pending Pod，然后试着把这些 Pending Pod 和节点池做匹配，看看每个节点池都满足哪些 Pending Pod 的调度要求：有的节点池可能扩容了也不满足调度要求，这些节点池就被排除了；有的节点池能调度一部分 Pending Pod，那这些节点池就会保留下来。</p><p></p><p>对于这些保留下来的节点池，CA 会计算需要扩容多少个节点才能满足这些 Pending Pod 的资源用量，接着从这些节点池中按照设置的扩容策略选一个最合适的节点池。扩容策略可能是随机选择、也可能是优先级，或者最小浪费，这些都是由用户配置的。选择出最合适的节点池之后，CA 就会调用接口，告知云厂商需要扩容的数量，云厂商完成具体的 ECS 创建、加入集群等动作。</p><p></p><p>而在缩容阶段，CA 会找到使用率低于阈值的节点，查看这些节点上是否还有 Pod，如果没有 Pod 了，就认为这个是空节点，会被优先批量删除。删除完空节点以后，CA 再判断这些非空的节点上，Pod 是否可以调度到其他节点上去：如果可以调度，CA 也会把这个非空节点删除，节点上的 Pod 被驱逐、然后在别的节点上被重建。</p><p></p><p>这大概就是 CA 的整个过程，虽然省去了很多细节，但大家应该可以理解几个关键点：一个是 CA 中的逻辑，是定期运行的；第二个是在整个流程中，有扩容和缩容两个阶段，这两个阶段相互独立，扩容需要计算新增的节点数量、按照扩容策略选节点池，缩容就只看节点的使用率和上面的 Pod 是否可被重调度。</p><p></p><h2>客户场景</h2><p></p><p></p><p>我们遇到过这样一个案例，客户有自己的任务分发平台，不同计算任务通过任务平台下发到 Kubernetes 集群中，每批计算任务对应一堆的 Pod。而他们的业务存在这几个特点：</p><p></p><p>任务种类多，不同的任务所需的资源不同，CPU 用量各异，有的也会使用 GPU；不同任务对应的 Pod 数量也不同，峰值时整个集群超过 2w Pod；一般业务高峰期是在晚上，从凌晨开始跑，一直跑到早上；整体耗时长，不同批次任务耗时有长有短；Pod 的镜像也非常的大，拉取耗时长。</p><p></p><p>在这样的业务场景下，为了节省成本，客户很自然地使用了 Cluster Autoscaler，期望在计算任务下发后，节点池能自动扩容，添加新的节点到集群中，让 Pod 调度上去。在计算任务跑完以后，节点空闲下来，Cluster Autoscaler 再把节点删除，避免资源浪费。为了提高装箱率减少资源碎片，客户会对某些类型的任务，设置 Pod 的 resource request 和节点规格一致，尽量让这种任务的 Pod 独占一个节点</p><p></p><h2>问题与解决方案</h2><p></p><p></p><p>问题一：扩容成功率低</p><p></p><p>在客户上量过程中，我们碰到的第一个问题，是在大规模扩容过程中出现的大量扩容失败。CA 触发节点池扩容后，一部分节点创建成功，调度了部分 Pod，另一部分节点创建失败，在随后的过程中又被 CA 删除。由于还有部分 Pod 处于 Pending 状态，又触发 CA 扩容，然后又失败，周而复始。</p><p></p><p>这就给客户带来了非常糟糕的体验，一是看到很多失败的扩容记录，使其对云厂商的信任度降低；二是增加了不必要的成本，因为这些创建失败的节点并没有加入集群，不能被客户使用，但是节点对应的云服务器是实实在在被创建出来了，客户花了钱，但资源又没用上，就增加了无谓的成本。</p><p></p><p>经过仔细排查，我们发现节点扩容失败是因为云服务器在初始化 Kubernetes 组件的过程中，写入磁盘的速度特别慢，很久都不能加入集群，超过了预设的超时限制，我们判定这是一个异常的节点。异常节点随后又被 CA 清理删除，那我们就很好奇，为什么 ECS 的云盘写入这么慢？经过进一步的调研，我们发现主要原因是云盘服务的压力太大：</p><p></p><p>一方面，云服务器自身在初始化 Kubernetes 组件的时候，比如安装系统软件包、从对象存储上拉取 Kubernetes 的安装包再解压等动作，是有磁盘写入的，一个节点可能还好，当几百个节点同时处于这个阶段的时候，云盘服务的整体写入压力会大幅上升。</p><p></p><p>另一方面，在于容器镜像的拉取。在已经正常创建的节点上，用户的 Pending Pod 会调度上去，然后开始拉取镜像，由于这个客户的镜像很大，拉的耗时也很久，如果很多节点都处于这个阶段，那会有大量的写入操作，导致整个云盘服务的写入吞吐量被打到一个较高的位置，新的节点在初始化的时候，因为要争抢写带宽，所以写入速度就降低了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b7d5ce610842e7597f02b75c0be7027.png" /></p><p></p><p>为了解决这个问题，我们的想法是对同时扩容的节点数量做一个限制。虽然社区的 CA 中并没有对同时扩容的节点数有什么限制，但任何系统都存在上限，通过对系统做合理的限制，不仅能提供稳定的服务，从全局上也有助于提升性能。</p><p></p><p>我们根据云盘的吞吐能力，估算了一个可被接受的同时扩容节点数，比如限制是 100，这样一来，用户看到的就是 100 一批 100 一批的扩容，节点都能扩容成功。虽然扩容的批次增加了，但扩容成功率也提高了，整体的云盘写入流量更加平滑，整体的扩容速度也比之前提升了很多。</p><p></p><p>问题二：容器镜像大，扩容速度慢</p><p></p><p>我们碰到的第二个问题，是极致的性能问题，我们先讲扩容的性能问题。在批处理场景下，客户使用的镜像会比较大，并且客户对扩容端到端速度要求会比较高，比如要求在 5min 内扩容出 500 个节点，并且 Pod 都能运行起来，这是一件非常有挑战的事情。</p><p>在客户视角下，他们计算任务的启动延迟，大概分为 5 个阶段：</p><p></p><p>第一阶段：下发任务，集群中出现因资源不足而导致 pending 的 Pod；第二阶段：CA 感知到这些&nbsp;Pending Pod，触发节点池扩容。这个阶段一般是秒级的，如果是使用了 GPU 的 Pod，由于 CA 自身的策略，会导致最多延迟 30s 再扩容。这里 CA 不立马扩容要等几秒，是因为如果最新的 Pending Pod，创建时间离现在比较近，很有可能还会有新的 Pending Pod 被创建出来。比如 deployment 的副本数从 0 改到 1000，可能就需要 10 多秒才能全部创建完，所以 CA 宁愿多等一会儿等所有 Pod 都被创建了才执行扩容；第三阶段：云厂商接收到扩容请求，去创建云服务器、注册到集群中。这个阶段是分钟级别的，不同云厂商的耗时可能会略有差别；第四阶段：把这些 Pending Pod 调度到节点上，如果 Pod 数量和集群规模不大，Pod 的调度条件不复杂，相对整个过程来说，这阶段的耗时可以忽略不计；第五阶段：节点上的 Pod 开始拉取镜像、启动。这个阶段的耗时是不太稳定的，比如同时扩容的节点数量比较多，容器镜像又比较大，就很有可能会打满云厂商的限速，对整个端到端的影响比较大。</p><p></p><p>比如在这张图里，在多个节点同时扩容时，除了用户的计算任务的 Pod，节点上还有很多系统 daemonset 的 Pod，比如网络组件、device plugin、日志采集组件等等，这些 Pod 的镜像也会大量的、同时的从镜像仓库拉取，很容易就达到网络瓶颈，或者给云盘服务带来写入压力。如果 500 个节点同时扩容，每个节点上都在争抢带宽或者磁盘的写入，是无法达到刚刚说的性能要求的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1ab394d6fb7af7b1f7b51a8eb9895e4e.png" /></p><p></p><p>在这种极致的性能要求下，我们采用了自定义系统镜像方案。这个自定义系统镜像是指云服务器的系统镜像，我们先在云服务器中把容器镜像预先拉取下来，然后把云服务器导出为自定义系统镜像，把业务的容器镜像固化到系统中去，这样在后续扩容的时候，我们用这个自定义系统镜像去创建云服务器，云服务器作为节点加入集群后，容器镜像就已经在节点上了，不需要再去镜像仓库拉取，Pod 可以做到秒级启动。</p><p></p><p>但这个方案也有一些弊端，比如我们可以把整个容器镜像固化到系统中后，后续容器镜像发生了变化，这个自定义系统镜像也需要重新制作，比较麻烦，如果容器镜像变化比较频繁，就要频繁的制作自定义系统镜像。所以我们也可以把镜像做一下拆分，把数据量比较大的、又不怎么更新的静态数据，打包到基础镜像中，然后把这个基础镜像再固化到系统中，这样节点在启动以后，拉取的数据量也会大大减小。</p><p></p><p>在使用这个方案前，如果客户扩容 500 节点，在单批次运行最多 70 个节点扩容的情况下，每个节点上 1 个 10GiB 的容器镜像，那从下发到 Pod 全部运行，大概需要 22min。</p><p></p><p>而如果使用自定义镜像，因为不需要拉取容器镜像，所以刚刚说的云盘服务的压力就减轻了，所以我们直接放开扩容数量的限制，直接从 0 到 500 做扩容，从 Pod 下发到最终 Running，可以在 5min 以内完成，并且云盘服务整体的写入流量，可以从峰值的 14GB/s 下降到 6GB/s，大幅减少数据写入。</p><p></p><p>这个方案对于需要快速扩容、对扩容时的端到端耗时非常敏感的业务，是一个可行的解决思路。</p><p></p><h2>问题三：多节点干扰，缩容速度慢</h2><p></p><p></p><p>客户因为计算任务的不同，会触发不同节点池的扩容。比如客户先进行&nbsp;GPU 计算任务，触发了节点池 A 的扩容（节点池 A 是 GPU 节点），在计算任务 A 快结束的时候，可能会下发新的计算任务，触发节点池 B 的扩容。</p><p></p><p>那按照客户的预期，这时节点池 A 的这些 GPU 节点，因为上面没有 GPU 计算任务、节点使用率已经降低，需要在任务结束的一段时间内很快就被缩容掉。但实际情况是节点池 A 的缩容会被推后较长的时间，这就造成了一些资源浪费。</p><p></p><p>所以为什么节点池 A 的缩容会被推迟呢？</p><p></p><p>CA 内部的缩容流程中，有一个冷却时间，表示扩容后多久时间内，是不能对节点做缩容的，这个值由用户来设定。这个计时是集群级别的，就是任何一个节点池扩容了，这个计时器都会被重置，重新计算。在大规模、多节点池扩容的情况下，如果用户分批扩容，那每次扩容都会做一次重置，导致扩容过程中，空闲的节点池无法被缩容，造成资源的空跑。</p><p></p><p>当前社区对此已经有解决方案，但代码还处于草稿阶段，具体的解决思路就是把计时器改成节点池级别，每个节点池只针对自己的扩容过程做倒计时，不受其他节点池干扰。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/83eb92b64349a7dbbc8ac5f8c732fc8d.png" /></p><p></p><p>我们在生产环境上对社区的方案做了验证，确实很好的解决了我们的问题，在计算任务结束后，节点池 A 就会很快被缩容。那这个缩容时间的缩短，非常显著地降低了客户的使用成本。</p><p></p><h2>问题四：Pending Pod 过多导致未扩容</h2><p></p><p></p><p>最后我们再来看一下由规模带来的问题。</p><p></p><p>如前文所述，客户用自己的任务分发平台将计算任务转换成 Pod 下发到 Kubernetes 集群，有时候并不能非常好地控制任务的下发速度。峰值时期，整个集群中有 2w 多个 Pod，其中&nbsp;Pending Pod&nbsp;的数量高达 1.8w 个。面对如此大的规模，CA 难免“力不从心”。</p><p></p><p>下图展示了集群中 Pod 的数量情况和 CA 的日志分布情况，可以发现在 Pod 数激增的那段时间里，CA 基本上没有输出日志，集群中的节点池也没有扩容，客户的计算任务被大量堆积、阻塞。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d975e99ab55cfe3844e9622b4e5052d.png" /></p><p></p><p>经过调查我们发现，CA 主要卡在调度预测阶段，在这一阶段，CA 会计算每个节点池需要扩容多少个节点才能满足这些&nbsp;Pending Pod&nbsp;的资源用量。为了复现这个问题，我们做了一些压测，期望能找到影响这个耗时的主要因素，方便针对客户的场景做一些优化。</p><p></p><p>一开始我们想到的就是 Pending Pod 的数量。为此我们使用两个不同规格的节点池，然后往集群中下发大量的&nbsp;Pending Pod，这些&nbsp;Pending Pod&nbsp;通过资源用量期望调度到其中一个节点池上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/423f92f0da89e08ec47fa5967cb6efea.png" /></p><p></p><p>我们发现随着集群中 Pending Pod 数量的增长，单个节点池的整个计算耗时，是不断上升的，在 2.2w Pod 时，单个节点池的计算耗时会到 400s，而从 Pod 视角来看，单个 Pod 的平均耗时，也是线性增长的。虽然 Pending Pod 的数量规模达到了，但实际 CA 僵死的时间是远比我们测出来的 400s 多，所以我们继续接近客户的使用方式，将 Pod 的调度逻辑做了修改，从之前的默认调度约束，改为了使用节点亲和性。</p><p></p><p>我们发现在不使用节点亲和性的情况下，整体的耗时和第一次压测的是一样的，而如果使用了节点亲和性，在&nbsp;Pending Pod&nbsp;数量在 1.8w 的时候就达到了 700s。下方右侧这张图中蓝色的那条曲线也说明，单个 Pod 的平均计算时间，比之前不使用节点亲和性的场景增长得快，整条线上升的速度更快、斜率更高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4be635025e18911d4f0dd002c5a3b6ac.png" /></p><p></p><p>除此以外，我们继续控制变量，调整 Pod 的 request，将之前的单个节点上只跑 1 个 Pod，改为单个节点上能跑 8 个 Pod，这样修改后，预期添加到集群中的节点数量是之前的 1/8，同时整个计算耗时，相比之前的曲线，也是接近水平了。</p><p></p><p>从上面的 3 次压测中，我们可以得出一些结论：</p><p></p><p>Pending Pod 越多，需要计算的耗时越久，且平均每个&nbsp;Pending Pod&nbsp;的耗时随总数的增加而增加；使用了 Node Affinity 的&nbsp;Pending Pod，在做调度预测时，会耗时更久；预估节点数量越多，调度预测越久；可被调度的节点池数量越多，调度预测越久。</p><p></p><p>这是我们从压测中得到的实验结论，那真实的技术理解应该是怎样的呢？</p><p></p><p>CA 在估算节点池需要扩容多少个节点的时候，内部有一个快照，一开始这个快照包含了集群中的节点和节点上的 Pod。如果集群中有多个节点池，CA 会先对每个节点池做一下计算，看看哪些 Pending Pod 能调度到节点池上。因为如果节点池不能满足 Pod 的调度要求，即使扩容了也没有用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/2930b38dff9fca2b6eeb2b1b7ccd06e7.png" /></p><p></p><p>比如这张图里，集群中一共有 8 个&nbsp;Pending Pod，节点池 A 能满足所有&nbsp;Pending Pod&nbsp;的调度要求，节点池 B 只能调度 6 个。这个过程的复杂度是 O(n^2)，跟 Pod 的数量、节点池的数量有关，当然也跟 Pod 的调度条件有关系，调度条件越复杂，这个耗时也会更久一点。在做完这一步之后，CA 会再根据节点池和节点池上的这些&nbsp;Pending Pod，去计算需要扩容多少个节点</p><p></p><p>比如节点池 A 能满足 8 个&nbsp;Pending Pod&nbsp;的调度条件，CA 会先对这些&nbsp;Pending Pod&nbsp;和快照里的节点做一轮调度模拟，跑一下 scheduler framework 中的 prefilter 和 filter 阶段，看能不能正常通过，如果能通过就表示这些&nbsp;Pending Pod&nbsp;可以调度到快照的节点上，如果不能通过，就会根据节点池 A 的规格信息，构建出一个虚拟的 Node，放到快照里，然后再做刚刚的调度模拟，此时这些&nbsp;Pending Pod&nbsp;是可以调度到这个新加的虚拟的 Node 里的。</p><p></p><p>CA 重复这个过程，直到这里所有的&nbsp;Pending Pod&nbsp;都能加入到快照中，此时快照里新增了多少个虚拟的 Node，其实就是节点池 A 需要多扩容的节点数。只要集群里新增了这些节点，这些因资源不足而无法调度的&nbsp;Pending Pod&nbsp;就能真正的跑起来。</p><p></p><p>节点池 B 也是类似的，只不过在我们的例子中，节点池 B 的规模会比节点池 A 小。根据我们刚刚的分析，整个过程的复杂度是接近 O(n^3) 的，跟&nbsp;Pending Pod&nbsp;的数量、快照中的节点数量、节点池得到数量相关。</p><p></p><p>这也跟我们的压测结论是一样的：Pending Pod&nbsp;的数量越多、节点池越多、预估的节点数量越多、调度条件越复杂，整个扩容的耗时就越久。</p><p></p><p>对此，CA 社区主要提出了两个改进点：</p><p>限制节点数量的上限，就是减少快照中的节点数量，这个跟我们刚刚提到的观点是类似的，如果对扩容的节点数量不加限制，其实是不太稳妥的；对单个节点池整体的计算耗时做限制，比如不能超过 10s，如果这个过程超过了 10s，我们就截断这个过程。</p><p></p><p>如果你的 CA 版本还比较老，低于 v1.25 的，可能就没法使用社区的解法了。</p><p></p><h2>资源弹性建议</h2><p></p><p></p><p>如果业务对扩容的延迟比较敏感，期望能更快的让 Pod 启动，可以考虑将静态的、较大的容器镜像，打包进云服务器的系统镜像里，加速扩容。</p><p></p><p>推荐在业务侧就开始控制集群中的&nbsp;Pending Pod 的数量，数量过多不但会增大集群自身的压力，也会影响 CA 扩容的稳定性，将数量保持在一个稳定的水位，控制好扩容的节奏，会更好。</p><p></p><p>第三个是对于不需要弹性能力的节点池，关掉弹性伸缩功能，避免 CA 在这些节点池上消耗算力。</p><p></p><p>相关产品：www.volcengine.com/product/vke</p><p></p><p>视频回放：关注【字节跳动云原生】公众号，在后台回复“KubeCon CN 2023”</p><p></p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</id>
            <title>刚发布就被质疑？超过GPT-4的“最强”大模型Gemini、“最高效”训练加速器，谷歌到底行不行</title>
            <link>https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 02:04:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, AI模型, Gemini, 多模态提示
<br>
<br>
总结: 谷歌发布了功能强大的AI模型Gemini，该模型通过多模态提示实现对文本和图像的理解和反应。 </div>
                        <hr>
                    
                    <p>当地时间12 月 6 日，谷歌发布了自己“迄今为止功能最强、通用性最高”的AI模型Gemini。</p><p></p><p>谷歌及Alphabet&nbsp;CEO桑达尔·皮查伊 (Sundar&nbsp;Pichai)表示，首个Gemini 1.0针对不同规模进行优化，具体分为Ultra、Pro和Nano三个版本。“这是Gemini时代的首批模型，也是我们今年早些时候重组Google DeepMind时所表达愿景的首个实现。此模型代表着谷歌作为一家企业，在AI新时代下所做出的最重要的科学与工程努力之一。”</p><p></p><p>但刚发布不久，科技专栏作家Parmy Olson 指出，其中一个AI实时对人类的涂鸦和手势动作给出评论和吐槽的视频被曝出“不是实时或以语音方式进行的”。还有<a href="https://twitter.com/noguestein/status/1732927393466040617">网友吐槽</a>"整个互动过程“特别慢，跟演示视频完全不同。”</p><p></p><p>这个视频主要是演示“多模态提示”（multimodal prompting），即为大模型提供不同模式的组合（在本例中为图像和文本），并让其通过预测接下来会发生什么来做出反应。</p><p></p><p></p><p></p><p>对此，Google DeepMind 研究与深度学习主管副总裁 <a href="https://twitter.com/OriolVinyalsML/status/1732885990291775553">Oriol Vinyals</a>"表示，“视频中的所有用户提示和输出都是真实的，只是为简洁起见进行了缩短剪辑。”但网友对此并不买账，认为谷歌在玩营销手段，误导大家。</p><p></p><p>在谷歌发布的<a href="https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html?m=1">一篇文章</a>"里，详细介绍了效果实现经过，可以看出是使用静态图片和多段提示词拼凑训练。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/879c43b3919928ef014c63fd299e9cbb.png" /></p><p></p><p></p><h2>看看谷歌的测试</h2><p></p><p></p><p>Gemini 被称为谷歌迄今为止最灵活的模型，能够从数据中心到移动设备实现高效运行，帮助开发人员与企业客户显著增强在利用AI进行构建和扩展时的操作方式。谷歌针对三种不同体量优化了Gemini 1.0（首个正式模型版本），分别为：</p><p></p><p>Gemini Ultra&nbsp;— 最大、功能最强的模型，适用于高度复杂的任务。Gemini Pro&nbsp;— 可处理各种任务类型的最佳模型。Gemini Nano&nbsp;— 能够在多种设备上高效运行的任务处理模型。</p><p></p><p>值得注意是，本次尚未发布最强大的Gemini Ultra，距离正式发布还需要几个月的时间。目前Gemini Ultra正在进行全面的信任与安全检查，包括由受信的外部合作方进行红队审查，并在广泛应用前通过微调和基于人类反馈的强化学习（RLHF）对其做进一步完善。</p><p></p><p>Gemini Pro和Gemini Nano已分别集成到了聊天机器人Bard和智能手机Pixel 8 Pro上。此外，自12月13日开始，开发者和企业客户都可通过Google AI Studio或者Google Cloud Vertex AI中的Gemini API访问Gemini Pro模型。在未来几个月间，Gemini将逐步登陆谷歌更多产品及服务，包括搜索、广告、Chrome浏览器以及Duet AI等。</p><p></p><p>谷歌说得很厉害，那Gemini 1.0 的实力到底如何？</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9677686c03434f3f8237cd371682bc06.png" /></p><p>﻿</p><p>根据谷歌测试结果，从自然图像、音频和视频理解再到数学推理，在大语言模型（LLM）研发领域的32种常见学术基准测试中，Gemini Ultra的性能一举创下30项最佳新纪录。</p><p></p><p>在MMLU（大规模多任务语言理解）中Gemini Ultar的得分高达90.0%，成为首个超越人类专家的模型。这项测试结合了数学、物理、历史、法律、医学和伦理学等57个科目，旨在测试AI模型掌握知识和解决问题的能力。</p><p></p><p>Gemini在文本和编码等一系列基准测试中表现超过GPT-4：</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/691456eef49288cbb54143ec862c3dc2.png" /></p><p></p><p>Gemini Ultra还在新的MMMU基准测试中取得了59.4%的最高得分。这项基准测试涵盖跨越不同领域、需要深思熟虑的一系列多模态推理任务。</p><p></p><p>根据谷歌测得的图像基准，Gemini Ultra的性能优于以往最先进的模型，且无需借助从图像中提取文本以供进一步处理的对象字符识别（OCR）系统的辅助。谷歌表示，这些测试结果凸显出Gemini的天然多模态优势，也证明Gemini已经表现出具备复杂推理能力的早期特征。</p><p></p><p>Gemini在一系列多模态基准测试中均创下性能新纪录，全面超越GPT-4V：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eabc1941f73a277dcac4574c7de7d681.png" /></p><p></p><h2>多模态推理能力</h2><p></p><p></p><p>到目前为止，创建多模态模型的标准方法主要是针对不同模态训练单独的组件，再将其组合起来以粗略模仿相应能力。由此实现的模型虽然比较擅长执行某些特定任务，例如描述图像内容，但却难以处理概念性更强、复杂度更高的推理任务。</p><p></p><p>在Gemini的起始阶段就将其定位为原生多模态形式，针对不同模态开展预训练。之后，谷歌又使用额外的多模态数据对其进行微调，希望进一步完善其有效性。现在，Gemini可以同时识别和理解文本、图像、音频、视频和代码五种信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/abb5095053fbb457004d5057561f4555.png" /></p><p></p><h4>理解文本、图像、音频等各种素材</h4><p></p><p></p><p>Gemini 1.0拥有精妙的多模态推理能力，可以帮助理解复杂的书面与视觉信息，展现出了在大量数据中提取重要知识的独特能力。比如，Gemini 在阅读、过滤和理解信息的过程中，可以从数十万份文档中提取见解并进行分析。</p><p></p><p>Gemini 1.0在训练之后，能够同时识别并理解文本、图像、音频等各种素材，因此可以把握住更加微妙的信息，并回答与复杂主题相关的更多问题。这使得它特别擅长解释数学、物理等复杂学科的推理过程。</p><p></p><p>比如，Gemini 可以识别学生的手写物理题答案，并验证正确性：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/09/0993347ceccee6452d2a0f3248905fd5.png" /></p><p></p><p>基于视觉线索进行推理：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/91/9122b89b1f3140bc6e1a323e529acd5a.png" /></p><p></p><p>音频方面，可以看下Google DeepMind 研究科学家 Adrià Recasens Continente 演示 Gemini 能够理解来自多个扬声器的不同语言的音频，并结合视觉、音频和文本，在厨房做饭时提供帮助的场景：</p><p></p><p></p><p></p><h4>高级编码能力</h4><p></p><p></p><p>谷歌介绍，首个Gemini正式版能够理解、解释并生成基于目前各种流行编程语言（例如Python、Java、C++和GO）的高质量代码。其表现出的跨语言工作和复杂信息推理能力，也使得Gemini成为世界领先的编码基础模型之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62fd21473ee722a5f3eb12414a0ad27d.png" /></p><p></p><p>Gemini&nbsp;&nbsp;的多模式推理功能生成用于重新排列子图的matplotlib代码</p><p></p><p></p><p>Gemini Ultra在多项编码基准测试中表现出色，包括HumanEval（用于评估编码任务性能的重要行业标准）和 Natural2Code（谷歌内部保留的数据集），此数据集使用作者专门创作的源素材、而非来自网络的信息。</p><p></p><p>Gemini还能作为更高级编码系统的引擎。谷歌两年之前发布了ALphaCode，这也是首个在编程竞赛中表现出一定竞争力的AI代码生态系统。使用Gemini的专用版本，谷歌推出更加先进的代码生成系统AlphaCode 2。除了编码场景之外，它还擅长解决涉及复杂数学和理论计算科学的更多编程难题。</p><p><img src="https://static001.geekbang.org/infoq/10/108a0fe125a7ab615f9e83a23e82c6e7.png" /></p><p></p><p>面对与初代AlphaCode相同的评估场景，AlphaCode 2表现出巨大的性能改进，其解决的问题数量几乎达到初版的两倍，谷歌估计其成绩优于85%的竞赛参与者，而AlphaCode成功解决问题的比例只接近50%。因此当程序员通过代码示例来定义某些属性，并借此向AlphaCode 2寻求帮助时，其表现会更好。</p><p></p><p></p><h2>“专为训练顶尖AI模型而生”的TPU系统</h2><p></p><p></p><p>在介绍自家大模型的同时，谷歌顺势推出了了自己的AI训练基础设施。</p><p></p><p>谷歌使用内部设计的张量处理单元（TPU）v4和v5e在AI优化的基础设施之上，完成了Gemini 1.0的大规模训练任务。</p><p></p><p>在TPU上，Gemini的运行速度明显快于其他更早、更小且功能较差的模型。这些定制设计的AI加速器一直是谷歌AI产品的核心，负责为搜索、YouTube、Gmail、谷歌地图、Google Play和Android等服务的数十亿用户提供支持。它们也使得世界各地的其他企业也能经济高效地训练出自己的大规模AI模型。</p><p></p><p>如今，谷歌宣布推出迄今为止“最强大、最高效且可扩展”的TPU系统Cloud TPU v5p，专为训练顶尖AI模型而生。谷歌表示，作为下一代TPU，它将加速Gemini开发，帮助开发者和企业客户快速训练大规模生成式AI模型，将新产品和新功能更快交付至客户手中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee5fb38056fa5ac7026cfc835d0eb72a.png" /></p><p></p><p>谷歌数据中心内的Cloud TPU v5p AI加速器超级计算机</p><p></p><p>此外，在安全问题上，谷歌表示，Gemini拥有迄今为止所有谷歌AI模型当中最全面的安全评估机制，包括偏见与有毒内容检测。谷歌还对网络攻击、说服与自主判断等潜在风险领域开展了新颖研究，并应用谷歌研究院领先的对抗性测试技术抢在部署之前帮助发现Gemini中的重大安全隐患。</p><p></p><p>为了诊断Gemini训练阶段的内容安全问题，并确保其输出结果符合政策，谷歌使用诸如真实毒性提示词Real Toxicity Prompts在内的多种基准。这是一组从网络提取的、包含不同程度毒性内容的10万条提示词，由艾伦AI研究所的专家们提供。为了限制伤害，谷歌还构建了专门的安全分类器，用以识别、标记并整理涉及暴力或负面刻板印象的内容。</p><p></p><p>附 Sundar&nbsp;Pichai 公开信内容：</p><p>&nbsp;</p><p></p><blockquote>每一次技术变革都代表着推动科学发现、加速人类进步和改善生活品质的机遇。我相信我们现在所见证的AI转变，将成为我们一生当中最具深远意义的事件，甚至远远超越之前的移动或者Web革命。AI有望为全球各地的人们创造前所未有的日常生活体验和非凡的职业发展空间，将掀起新一波的创新与经济进步，并以前所未见的规模提升知识、学习、创造力与生产力。&nbsp;这也让我感到兴奋，期待通过AI技术为各国各地的每一个人提供帮助。&nbsp;作为一家AI优先的厂商，我们已经走过近八年历程，而前进的步伐只会不断加快：数百万用户正在我们的产品中运用生成式AI完成一年之前还难以想象的工作，包括为更加复杂的问题寻求答案、使用新工具协作与创新等等。与此同时，开发人员也在使用我们的模型与基础设施构建出新的生成式AI应用程序，世界各地的初创企业和组织正利用我们的AI工具不断拓展业务。&nbsp;这是一股令人难以置信的发展态势，而且我们才刚刚开始触及这无限可能性的最表层。我们正以大胆且负责任的态度开展这项工作。这意味着我们既需要追求雄心勃勃、能够为人类和全社会带来巨大收益的技术成果，同时也要建立保障措施并与政府和专家合作，应对AI发展过程中带来的种种风险。我们将继续投资打造更好的工具、基础模型和底层设施，并在我们AI原则的指导下将其引入自己的产品及其他方案当中。</blockquote><p></p><p></p><p></p><p>相关链接：</p><p><a href="https://blog.google/technology/ai/google-gemini-ai/#availability">https://blog.google/technology/ai/google-gemini-ai/#availability</a>"</p><p><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZRC4RXO1SXy7HfqBzFCg</id>
            <title>KubeCon | 使用 KubeRay 和 Kueue 在 Kubernetes 中托管 Ray 工作负载</title>
            <link>https://www.infoq.cn/article/ZRC4RXO1SXy7HfqBzFCg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZRC4RXO1SXy7HfqBzFCg</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 09:51:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: KubeCon CN 2023, Open AI + 数据 | Open AI + Data, KubeRay, Kueue
<br>
<br>
总结: 本文介绍了在 KubeCon CN 2023 上的专题演讲，演讲内容包括使用 KubeRay 和 Kueue 在 Kubernetes 中托管 Ray 工作负载的方法。文章还介绍了 Ray 的概念和特点，以及字节跳动在 KubeRay+Ray 应用实践中的经验。 </div>
                        <hr>
                    
                    <p>在 KubeCon CN 2023 的「 &nbsp;Open AI + 数据 | Open AI + Data」专题中，火山引擎软件工程师胡元哲分享了《使用 KubeRay 和 Kueue 在 Kubernetes 中托管 Ray 工作负载｜Sailing Ray workloads with KubeRay and Kueue in Kubernetes》议题。以下是本次演讲的文字稿。</p><p></p><p>本文将从 Ray 为何得到 AI 研究者们的青睐，在字节如何使用<a href="https://xie.infoq.cn/article/072696d697dfb06be06ff536c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> KubeRay</a>" 来托管 Ray 应用，Kueue 如何管理和调度 RayJob 三个方面进行介绍。</p><p></p><h2>什么是 Ray</h2><p></p><p></p><p><a href="https://www.infoq.cn/article/sF4obWPGiLbJrMaFpf42?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Ray </a>"起源于 UC Berkeley 的 RISElab 实验室，其定位是一个通用的分布式编程框架，能帮助用户将自己的程序快速分布式化。Ray Core 提供了 low level 的分布式语法，如 <a href="https://xie.infoq.cn/article/78669fdfcde61be0aa3560481?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">remote func</a>"、remote class，上层 Ray AIR 提供了 AI 场景的相关库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34787337399ecc1e765504db1e864553.png" /></p><p></p><p>Ray 的GitHub repo 如今已有 27K star，其发起者也成立了 Anyscale 公司来管理开源社区以及商业化。在 Anyscale 刚举办的 Ray Summit 2023 上，相关数据显示 Ray 已被 OpenAI/Uber/Amazon/字节跳动/蚂蚁金服等众多企业所使用。基于 Ray，Anyscale 也推出了自己的 LLM 相关商业化产品，并以成本和易用性等方向作为卖点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8361e5f4380ff6e8d420122607e7ab0.png" /></p><p></p><p>上图右侧展示了 Ray cluster 的基本架构：</p><p></p><p>每个框是一个 Ray 的节点，节点是虚拟的概念，比如在 K8s 集群上，每个节点就对应一个 pod。所有的节点中，有一个节点的角色不同，就是最左边的 head 节点，它可以理解成整个 Ray cluster 的调度中心，head 节点上有 GCS 存储集群节点的信息、作业信息、actor 的信息等等，head 节点上还有 dashboard 等组件。除了 head 节点以外的都是 worker 节点，worker 节点主要是承载具体的工作负载。每个节点上有一个 raylet 守护进程，raylet 也是一个本地调度器，负责 task 的调度以及 worker 的管理，同时 raylet 中还有 object store 组件，负责节点之间 object 的传输，整个 Ray cluster 中的所有 object store 构成一个大的分布式内存。</p><p></p><p>为了提供简洁的分布式编程体验，Ray Core 内部做了非常多工作，比如 actor 调度和 object 的生命周期管理等，上图左侧展示了如何使用 Ray Core 编写一个简单的分布式程序，square 函数和 Counter 类通过 Ray 的语法糖，变成了一些在远程运行的对象，其计算过程会被异步调用并存储在 object store 中，最后通过 ray.get 来获取到本地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c877c94ec9bb1dadd4d6efccbc19d6c.png" /></p><p></p><p>除了 Ray Core 提供的底层分布式能力，其上层 Ray AI Runtime（Ray AIR）针对算法场景也实现了一系列工具：</p><p></p><p>ray.data&nbsp;集合了数据读写、流式处理、shuffle 等功能，给离线推理、数据预处理等场景提供了灵活 API 和异构的调度功能ray.train&nbsp;和&nbsp;ray.tune&nbsp;可以将 xgboost、pytorch 等训练代码快速改写成基于 Ray 的分布式训练应用ray.serve&nbsp;是一套在线服务的部署调用框架，支持复杂模型编排，可以灵活扩缩实例</p><p></p><p>可以说，Ray 的生态打破了过去 AI 工程中每个模块都是固定范式的传统——</p><p></p><p>在过去，提到数据处理，大家会想到 Spark；提到训练，会想到 Torch DDP、MPI；提到推理，会想到 deployment、service；而 Ray 能够给予你足够的自由度和想象力，可以将 AI 的 pipeline 糅合在一个框架甚至一串代码中实现，其强大异构调度能力以及友好的上手调试感受。这也是很多 AI 从业者越来越多地选择 Ray 的原因。</p><p></p><h2>字节跳动 KubeRay+Ray 应用实践</h2><p></p><p></p><p>KubeRay 简介</p><p></p><p>KubeRay 是由字节跳动技术团队牵头，由 AnyScale、蚂蚁金服、微软等公司共同参与建设的开源 Ray 部署集成工具集，目前已成为在 Kubernetes 集群上部署 Ray 应用的事实标准。</p><p></p><p>如果不使用 KubeRay，直接在物理机来托管 Ray 集群会有什么问题呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b707e841aefee595160b801434c14aea.png" /></p><p></p><p>首先，head 和 worker 需要直接通过 ip 和 port 连接，集群的拉起、节点的增删会比较复杂，可恢复能力也较弱。其次，RayJob submit 脚本提交作业的模式在大规模生产环境下很难管理，除此之外，也没有 K8s 生态可以给予你的监控、报警、Ingress、HPA/VPA 等能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07d270af9180692caf6729bcd1caa8aa.png" /></p><p></p><p>KubeRay 采用了经典的 operator 设计，提供了 RayCluster，RayJob，RayService 这三个 CRD：</p><p></p><p>RayCluster：负责 Ray 集群的搭建RayJob：负责提交作业到一个伴生集群中，并同步状态RaySevice：负责将 RayServe 应用快速部署到云原生环境中</p><p></p><p>在 operator 实现中，cluster 的 controller 更侧重集群的拉起、恢复、与 Ray autoscaler 配合等，Job Service 的 controller 侧重作业提交和状态更新，并且它俩分别对应了离线和在线两个典型场景。</p><p></p><p>除此之外 KubeRay 还提供了 APIServer 等 client 库来负责 CRD 的增删改差，方便对接上层平台。</p><p></p><p>RayCluster</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5f489b9dd05d34d3284b6b480ae02d1f.png" /></p><p></p><p>如果说 Ray 本身提供了 actor 重启、task 重试等能力来增强代码的高可用性，那么 KubeRay 就是真正让 Ray 在集群维度成为真正高可用的应用。</p><p></p><p>首先 RayCluster CRD 提供了 pod 的恢复能力以及集群粒度的热更新，可以非常方便地管理集群；其次 head 和 worker 通过 service 进行连接，通过将集群 metadata 挂到远程存储中，配合 service 可以做到无感知的 head 节点恢复，同时 Ray autoscaler 可以实现基于集群负载动态伸缩集群规模，有效缩减成本。当然 Cluster CRD 还提供了 metric， 集群状态等的透出。</p><p></p><p>RayJob</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/dec060fcf86cb2faec661053c31241a6.png" /></p><p></p><p>RayJob 是生产环境管理 Ray 作业的解决方案，支持批式调度器，创建伴生 Ray 集群或者选择已有的 Ray 集群，提交作业，并更新作业状态，最后删除 Ray 集群。在字节跳动，我们优化了作业状态机转移，增加了超时、等待节点数等功能。</p><p></p><p>RayService</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5e3d0c66271b55e0252b61191f1342c.png" /></p><p></p><p>RayService 把 CRD 中的 serve 配置部署到集群上，并通过 service 把 serve agent 的端口透出，实现了 Ray serve 的云原生化。它支持热更新 Serve 配置，通过 pending cluster 的滚动更新实现 Serve 无感知迁移。</p><p></p><p>Ray 在字节跳动的托管</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdb059d61461e2dc7aa3068cec72bbaf.png" /></p><p></p><p>在字节跳动，我们给用户提供了丰富的 Ray 相关生态。首先站内所有的 Ray 集群都由 KubeRay 去管理，我们基于开源版本做了相关适配和增强来支持大规模作业提交以及一些额外特性；我们在平台层支持用户创建常驻 Ray 集群用来调试作业，也支持 single-job 形式让平台托管创建 RayJob；除此之外还提供了平台鉴权、historyserver、notebook 等周围的能力。</p><p></p><p>如今字节跳动内部的相关业务包含了图计算、离线推理、大模型、并行计算等方向，涵盖了离线、在线等场景。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4f0a634454c047746f4c8fbea89197d.png" /></p><p></p><p>上图展示了站内某业务在使用常驻集群的场景，其需求是希望尽量利用不同 K8s 集群上的低优 spot 资源提供给用户用于运行、调试作业，同时希望大多数作业感知不到外界资源的抖动。</p><p></p><p>我们的方案是在每个 K8s 集群中创建一个大资源量的低优 pod 组成的 Ray 集群，operator 层面会基于每天 quota 的规律性浮动，并配合 Ray autoscaler 主动调整集群规模，尽量减少被 K8s 去主动驱逐 pod 的情况。</p><p></p><p>同时在上层，用户的脚本会感知每个大集群的剩余资源量决定分发到哪个集群去执行。每个集群内部我们实现了一个简单的排队功能，收到作业请求后先将作业放入 dashboard 内部的队列中，通过 placement group 来实现资源 gang 调度，确保作业需要的 GPU、CPU 资源到位后才开始真正运行作业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99d3a607fe41a081d42ddc9bc3ca8b75.png" /></p><p></p><p>在用户结束调试之后，可以通过平台来托管创建 RayJob CR，KubeRay 会负责集群拉起、作业提交、结束销毁。作业运行过程中，Ray 集群的重要信息会以 event 的形式 dump 到远程存储，我们仿照了 spark history server 的设计思路，用户在作业运行结束之后可以通过 Ray UI 界面来直接查看历史的作业的日志、metric 等信息。</p><p></p><p>场景案例</p><p></p><p>场景一：图计算</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68e6db08dca1ee104e8aa7c8d4a1798f.png" /></p><p></p><p>在图计算场景，我们使用 Ray Core 来改造字节跳动内部的图计算引擎，每个图算子通过 Ray Actor 拉起，这些算子会基于初始化的 rank 利用 MPI 进行通信。通过 Ray 的分布式能力和 KubeRay 的编排能力，可以实现端到端的容错，如果 worker 挂掉，可以再次被拉起，从 Pmem 或者 SSD 存储中恢复 checkpoint 信息。</p><p></p><p>场景二：大规模离线推理</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5af5c2f988d1af61bb004c75c5299d6c.png" /></p><p></p><p>如图所示，上述作业同时包含数据读取处理和模型推理，同时需要消耗大量计算资源做分布式计算。相比在线推理，离线推理对延迟要求不高，但是对吞吐和资源利用率要求很高。我们使用 Ray dataset 的流式推理能力来处理这个场景，是因为相比 Spark，Ray 的编程更加灵活，同时将处理和推理放在异构 actor 并 pipeline，可以做流水线并行、模型并行等操作。我们还增加了 actor pool 扩缩、端到端容错的一些优化。</p><p></p><p>这些场景都已在 Anyscale 发表过博客，有兴趣可以查看：</p><p></p><p>www.anyscale.com/blog/how-bytedance-scales-offline-inference-with-multi-modal-llms-to-200TB-datawww.anyscale.com/blog/7-must-attend-ray-summit-sessions-rl-powered-traffic-control-infra-less-ml</p><p></p><h2>Kueue 如何管理/调度&nbsp;RayJob</h2><p></p><p></p><p>随着作业规模的增大，如何有限资源下调度不同优先级的作业，让大家都能稳定有序去使用 GPU 等资源是一个非常重要的问题。除了字节跳动所给出的一些经验，在开源社区侧，另一位分享人殷纳（Kante Yin）也介绍了如何使用 Kueue 这样一个作业调度器去管理 RayJob 来解决这些问题。</p><p></p><p>作业管理和调度框架 Kueue</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/78f30abfc2cfef60d89e0a714a3742fe.png" /></p><p></p><p>Kueue&nbsp;是去年由 K8s 社区发起的作业管理和调度框架，提供作业层面的队列调度，支持入队优先级、抢占、资源配额等能力。相比其它拥有队列调度能力的开源组件，Kueue 从设计上希望更多复用 K8s 原生的调度能力，尽量不重复造轮子。Kueue 已经原生支持了 BatchJob、RayJob、TFJob 等作业类型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2cfea87eb444d3a942c37f76120bea1.png" /></p><p></p><p>从 Kueue 的架构来看，ResourceFalvor 提供了节点的抽象，它通过 nodeLabel 的方式与具体的 node 进行绑定。ClusterQueue 是资源池的抽象，定义这个集群总资源量，ClusterQueue 中存在多个 localQueue，它们之间的资源会共享。一个作业会被提交到一个具体的 localQueue 进行调度。不同 clusterQueue 可以通过 Cohort 的机制共享资源。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3083964610f312f72792d6c9a7e92d0c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26938b4822e135e3a76f57e9fb319e40.png" /></p><p></p><p>对于管理员，需要创建 ResouceFlavor、ClusterQueue、LocalQueue 来定义资源和机器之间的划分关系，以及资源池中的 quota 分配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/545c2ff3bbb9eb8113d93ba15cf3f6f8.png" /></p><p></p><p>用户提交作业，需要指定自己作业所属于的 localQueue，job 在进入 Kueue 中会进入一个挂起状态，排队过程基于 quota，优先级等信息满足需求后放行，如果总当前资源不够，也有可能触发集群规模的 autoscale 机制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77b63af20bc07d061ccc12973b23e8b8.png" /></p><p></p><p>KubeCon 活动现场还展示了相关 Demo：两个优先级不同的 queue 中，随着优先级和 quota 的变化，来触发多个 RayJob 的抢占和恢复流程。</p><p></p><p>视频回放：关注【字节跳动云原生】公众号，在后台回复“KubeCon CN 2023”</p><p></p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wCkOH34JXviAGmTEJwvO</id>
            <title>降本增效的秘密：抖音集团如何实践潮汐混部</title>
            <link>https://www.infoq.cn/article/wCkOH34JXviAGmTEJwvO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wCkOH34JXviAGmTEJwvO</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 09:34:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 抖音集团, 在线业务, 离线业务, 弹性伸缩
<br>
<br>
总结: 抖音集团的业务类型具备多元化的特点，根据业务对实时性要求的区别，可以将这些业务划分为在线业务和离线业务两个业务体系。在线业务通常服务于终端用户，对实时性要求高；离线业务包含临时查询、定时报表、模型训练、数据分析等作业，对实时性要求较低。为了提高资源利用率，抖音集团采用弹性伸缩的方式，在低谷时回收在线业务的资源，并将其分配给离线业务使用。这样可以在保证服务稳定性的前提下，充分利用资源。 </div>
                        <hr>
                    
                    <p><a href="https://xie.infoq.cn/article/569ed60e7361db04888f00e6a?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">抖音集团</a>"的业务类型具备多元化的特点，根据业务对实时性要求的区别，我们可以将这些业务划分为在线业务和离线业务两个业务体系，其中：</p><p></p><p>在线业务体系通常服务于终端用户，包含 <a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca43495dbd3bd83c5cc13f7668b32deea1cf70190cb850f949140d37276ce718c6bdbd2728b&amp;idx=1&amp;mid=2247486929&amp;scene=27&amp;sn=7ce020046ec24fe3eb991bc689523021&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Web 服务</a>"，算法服务，有状态服务，视频编解码、FaaS 服务等，这些服务通常对 RPC 调用延迟比较敏感，对实时性要求高。离线业务体系包含临时查询、定时报表、模型训练、数据分析等作业，这些服务的特点是它们可以承受一定程度的排队或等待，在合理时间得到合理结果即可。</p><p></p><p>对于大部分的在线服务来说，业务的访问量具备明显波峰波谷的<a href="https://www.infoq.cn/article/experiments-data-innovation?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">潮汐变化</a>"。以抖音为例，绝大部分用户会在晚高峰时段使用抖音，这样就会导致抖音相关服务的整体流量都上涨到一个比较高的水平。而到了凌晨，因为用户使用抖音的次数和频率下降，该时段业务访问的流量会出现比较明显的波谷。</p><p></p><p>在线服务访问量的变化也导致了这些服务资源使用量的变化。下图展示了抖音集团内部在线业务的天级 CPU 使用情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/843e8e62e105c50bf1b965ab984a5f89.png" /></p><p>在线业务的天级 CPU 利用率</p><p></p><p>可以看到，在线业务凌晨时段的 CPU 使用量只有晚高峰时段的 20% - 30%。这种潮汐现象对头部服务而言会更加明显，它们波峰波谷间利用率的差距可能高达到 40%。但是为了保证在线业务在高峰时段的稳定性，在业务部署时，业务方必须按照峰时的流量来预估申请资源，这部分资源在谷时就会被闲置下来，造成严重的资源浪费。</p><p></p><p>在抖音集团的在线业务体系中，无状态服务如 Web 类微服务有着很高的资源保有量。而 Kubernetes 原生就提供了 HPA 的概念，可以根据 workload 的实际资源使用情况来扩缩无状态服务的实例数。如果我们可以通过弹性伸缩，在业务处于低谷时，通过回收业务副本数的方式来回收这部分资源，然后在业务处于峰值时，重新恢复业务的副本数，那么我们就能够在保证服务的 SLA 几乎不受影响的前提下，回收大量在线低谷时段的弹性资源。</p><p></p><p>在通过弹性伸缩回收了在线低谷时的资源后，下一步需要做的事情就是将这部分弹性资源进行二次的分配和利用，否则集群整体的利用率并没有因为服务弹性伸缩而得到本质的提升。但是在凌晨时段，几乎所有在线业务都会面临使用频次减少的情况，所以无法通过扩容在线的方式来利用这部分资源。而抖音集团也有很多离线的任务同样需要资源进行调度，例如视频转码和模型训练等，这些任务对资源的需求相对来说没有特定的时间约束，所以天然能够利用闲置资源。在这样的背景下，我们就开启了通过弹性伸缩来实现在离线业务的混部，即分时弹性混部。</p><p></p><h4>弹性伸缩</h4><p></p><p></p><p>大部分的在线服务都属于无状态服务，可以直接通过水平扩展来增加副本数。抖音集团内部并没有使用原生的 Deployment 描述在线的无状态服务，也没有使用社区原生的 HPA 体系，而是在上面构建了一层 &nbsp;HPAGroup 用于控制多个 Deployment 支持小流量或者 AB 发布，同时也方便我们在原生能力上针对某些场景做能力增强。整体架构如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/377abe2258921b6bc3fbb47bbbb74df5.png" /></p><p></p><p>从图中可以看到，Agent 负责采集业务各种数据，包括业务指标如 QPS 、P99 延迟等，以及系统维度指标如 &nbsp;Load、CPU 利用率等。这些数据最终会由两个接收方进行消费，一方面它会通过中心式采集的组件进入到实时数据的存储系统，另一方面它会通过一个消息队列进入离线算法模型中。</p><p></p><p>中心式的 Controller 负责消费这两种数据，并在这些数据的基础上决定当前的扩缩容行为。因此扩缩容行为是由 Controller 调整 HPAGroup 的 replica 数，最终进入到 K8s 调度体系中产生 Pod，完成最终的调度。</p><p></p><p>在弹性伸缩的流程中最重要的就是实时性和稳定性，对于在线业务来说，我们需要保证它们被缩容的实例在流量上涨之后能够被迅速地扩容，同时需要保证有足够的资源可以让在线业务完成扩容，否则就有可能造成单实例流量过高影响服务的 SLA，严重时可能会导致服务不可用，甚至影响到整个请求调用链路。所以我们需要底层系统的配合来提供一整套的机制进行保证，主要包括几个方面：</p><p></p><p>监控体系：需要一套集群维度的监控体系，为弹性伸缩提供稳定实时的利用率数据。Quota 体系：需要一套 Quota 系统保证业务在伸缩的过程中，集群整体的资源量是可控的，不能出现在波谷时将服务的副本数缩容后，它所对应的 Quota 被别的服务占用且无法归还的情况。</p><p></p><h4>监控体系</h4><p></p><p></p><p>从上文中描述的弹性伸缩过程可以看出，控制面强依赖于从监控系统中获取服务的实时资源利用率情况，需要尽可能避免利用率采集出错或者延迟太高，导致服务在需要扩容时扩不上去的问题。抖音集团在实际生产中没有采用 K8s&nbsp;原生的 Metrics Server，主要是基于以下的考虑——</p><p></p><p>首先， Metrics Server 只能代理实时数据，不存储历史数据。如果希望在弹性伸缩中根据历史数据做一些更平滑的策略，基于原生 Metrics Server 无法很好的实现。其次，由于抖音集团的弹性伸缩能力可以基于多集群的联邦，所以需要在联邦层上得到服务资源使用情况的汇聚数据。最后，不是只有弹性伸缩依赖监控系统，业务也需要实时通过内部的监控管理系统查询业务的实时数据和历史数据，所以监控系统内还需要将数据同步到内部的离线分析系统。</p><p></p><p>为了更好地支持这些特殊逻辑，研发团队借鉴 Metrics Server 实现了一套监控系统，主要包括以下的组件，其中 Metrics Agent 负责提供单机上 Pod 聚合数据，并以标准的 Prometheus 格式对外暴露数据；Collector 通过轮询收集每台机器上的数据，然后写入到 Metrics Store 中；Metrics Store 是一个基于内存的数据库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd7e8584dbff35438ca853300eae84f6.png" /></p><p></p><p>另外在提高整个数据采集链路的高可用和高性能方面，我们也做了一定的优化，例如对所有组件都采用多实例 stand by 的方式部署。Collector 支持根据 Node 做分片采集，Metrics Store 支持根据 Metrics 分片存储；Custom Metrics APIServer 从 Store 中读写数据时采用 quorum 机制保证数据的准确性；Store 自身支持故障恢复和数据同步；每个 Store 实例都在内存中以 podName 为 key 构建 B 树以提高查询效率，并对数据内容进行压缩来降低存储压力。同时 Store 还支持一些控制面计算逻辑的下发，例如直接返回服务的平均利用率等信息，由于抖音集团的部分服务可能规模非常庞大，单 deployment 副本数可以达到 5000+，所以计算逻辑放在 metrics 系统中实现能够大大降级数据传输的量和延迟。目前这套监控系统单次查询延迟在 30ms 左右，单次采集延迟在 60s 左右，基本能够满足弹性伸缩的需求。</p><p></p><p>Quota 体系</p><p></p><p>Kubernetes 原生提供了简单的 Quota 实现，但是由于抖音集团内部的服务有特定的组织形式，组织内部存在着比较复杂的嵌套关系，套用原生的 Quota 系统会非常难以维护，同时也无法对计算类服务所需求的定制化资源进行更好地支持。</p><p></p><p>我们从零开始构建了自己的 Quota 计量体系，具体来说，服务大致会按所属的业务划分到不同组中，我们使用 CRD 对象来记录各个组中所有服务的总体资源可用量和使用量的信息，然后通过旁路的 Controller 不断轮询更新对象的内容。当业务方对服务副本数进行修改时，APIServer 的请求处理链中会通过 validation webhook 对该服务的资源进行校验和准入控制。尤其需要说明的是，当服务开启弹性伸缩后，Quota 系统将通过扩容的实例数上限进行资源的预留，从而保证资源弹性伸缩过程中资源量可控。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd3c8ac3952d63631b1cbc6a24e2c640.png" /></p><p>伸缩策略</p><p></p><p>目前我们支持根据 CPU、内存、GPU 等多个资源维度进行弹性伸缩，另外我们还补充了一些新的特性，这里有两个重点特性值得一提：</p><p></p><p>支持根据时间段设置不同的配置、支持设置服务级别的对利用率小幅波动的容忍度、支持单步扩缩容的步长。关于这个配置的背景主要是因为一些算法相关的服务在启动和退出时需要进行数据的同步操作，如果单次扩缩容实例数较多，可能会对底层的存储组件造成较大的瞬时压力，所以需要将一次较大的扩缩容行为拆分为多次较小扩缩容行为来做一个缓冲，使得服务副本数的变化更加平滑。使用每个服务小时级别的历史数据作为保底的策略，以应对监控系统异常的情况。这里我们还是利用了服务天级的利用率比较稳定的特性，在监控系统出现问题导致无法获取监控数据时，控制面可以使用该服务昨天相同时段的利用率数据来作为指导扩缩容的兜底策略。</p><p></p><h4>分时弹性混部实践</h4><p></p><p></p><p>系统架构资源出让与回收</p><p></p><p>在开启了大规模弹性伸缩后，在线业务就具备了生产和使用弹性资源的能力。正如背景介绍中提到的，为了能够充分地使用弹性资源，我们采取的方案是和离线作业进行混合部署。</p><p></p><p>最开始开展混部项目的时候，我们的底层隔离能力还并不十分完善，如果将在线和离线业务同时摆放在一台节点上运行，容易出现在离线业务之间的互相影响，导致在线业务的 SLA 受损。</p><p></p><p>所以我们早期采取的是 “0/1” 的方式进行混部。具体来说，就是把在线业务波谷时产生的弹性资源折合成同等资源规模的整机出让给离线业务使用，使得同一时段不会同时有在线业务和离线业务运行在同一台机器上；当在线波峰来临时对弹性资源进行回收。</p><p></p><p>为了实现这个逻辑，我们引入了集群部署水位的概念（见下图）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac8d08a1ee81117d852706f786cb4bd4.png" /></p><p></p><p>初始情况下，在线服务没有进行弹性伸缩，集群中整体的资源部署处于满载状态。当在线服务的波谷来临后，几乎所有服务都会因为弹性缩容而导致副本数降低。从整体上看，集群里节点上的 Pod 会变得非常稀疏，集群整体资源部署水位也随之降低。</p><p></p><p>当集群的部署水位低于设置的阈值后，控制面会通过一定规则选取部分在线节点，将该节点上的 Pod 驱逐到别的节点上，并标记该节点不可调度，最后将离线服务调度到该节点上实现资源的出借。</p><p></p><p>同理，当在线服务的波峰来临后，会发生一个逆向的控制过程，控制面在通知并确保离线任务撤离后，重新将节点设置为在线可调度状态，实现资源的回收。在实际操作中，集群的水位阈值通常会定义在 90% 的水平上，这就意味着集群在常态下会始终维持在一个接近满载的部署状态中，从而能够最大限度地提升资源利用率。</p><p></p><p>分时弹性混部的控制面最主要的职责就是控制节点的动态出让和回收，即节点的动态伸缩。可以用一个状态机来描述节点的转移状态，其中 Online 和 Offline 分别表示节点处于在线使用和离线使用的状态。而当节点被选择进行出让和回收时，会分别先进入到 OnlineToOffline 或 OfflineToOnline 的中间状态。在此状态下在离线都无法调度新的任务到节点上来，控制面会负责清理残留的在线的 Pod 和离线任务，并执行用户所配置的 hook 函数，只有当这些工作都处理完成后，才会真正完成节点的出让和回收逻辑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b859e14b474a0542760417cf003c3946.png" /></p><p></p><p>离线业务稳定性保证</p><p></p><p>弹性资源最大的特点是它整体的资源供应量不确定，当在线服务出现抖动时，我们需要优先保证在线服务的稳定性，极端情况下需要通过杀死离线业务来为在线服务腾挪资源。而离线任务通常运行的时间长，频繁杀死和重跑任务对离线业务来说也会造成较大的影响。因此如何在不稳定的资源供应基础上保证离线业务的稳定性也十分重要。</p><p></p><p>资源不稳定性主要来自以下两个方面：</p><p></p><p>弹性资源的供应量是不稳定的。弹性资源的供应量受制于在线业务的扩缩容情况，这会导致整体的资源的总量、资源供应的时间，甚至每一天资源所对应的具体的机器环境是不一样的。因此我们没有办法让离线业务针对弹性资源做一些提前的资源规划，同时当在线业务发生任何抖动时，我们会随时进行资源回收，这对整个训练作业的体验并不好。</p><p></p><p>弹性资源的需求量是不稳定的。离线作业存在 min/max 语义，例如在一个 PS-Worker 离线训练中，Worker 的数量其实是不确定的，离线业务整体的资源描述也并非确定值。同时我们还需要解决一个问题，即在提高单个作业的训练速度和满足更多训练作业之寻求平衡。</p><p></p><p>那么在抖音集团内部，研发团队如何解决上述问题？</p><p></p><p>在资源供应方面：我们在执行缩容操作的过程中，引入了 deletion cost 机制定义实例缩容的优先级。比如我们可以尽可能地缩容整机，甚至尽可能地保证这些缩容出来的资源处于同一个 Pod 或者使用了同质的 GPU ，从而减少资源碎片的问题。在资源分配方面：对于一些离线业务例如离线训练来说，因为作业在调度和非调度的过程中，可能会执行很多次 checkpoint dump 和 reload 操作，这个操作过程需要从 HDFS 上实现完整模型的上传和下载，非常耗时。如果我们运行更多的作业，虽然在一定程度上可以优化用户的体验，但是弹性资源在归还给在线业务时会触发多次无效的 checkpoint 操作占据大量时间，从而降低资源的利用效率。因此对于离线训练业务，我们更倾向于提高单个作业的加速比，而不是运行更多的作业。在资源回收方面：为了解决资源回收的过程中无脑地杀死离线业务的问题，研发团队构建了弹性资源的优先级，基于优先级实现资源回收。目前的弹性资源大概分为三级，如下图所示。以 PS-Worker 架构的离线分布式训练为例，PS 作业可能会处于一个 High 的优先级；能够满足基本运行的 Min 的 Worker 处于中优的优先级；为了进行弹性加测的 Worker 处于 low 的优先级。这样做的好处是当我们在线进行资源回收时，我们可以定制一些调度器的抢占策略，使得在线服务总是倾向于去抢占低优先级的作业资源。除此之外，我们在分时弹性混部的控制面引入提前通知的机制，在提前通知的时间窗口内，不会再调度新的离线任务，同时尽可能保证那些已经被调度的任务顺利跑完，从而将任务杀死率维持在一个可接受的范围内。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd794e7eacfe8cd9a320cd6634b1742c.png" /></p><p>Priority Mapping</p><p></p><p></p><h4>案例分析</h4><p></p><p></p><p>视频编解码使用弹性资源</p><p></p><p>场景一是视频编解码服务和在线服务进行并池。我们基于 Kubernetes 生态系统构建了一套适用于视频计算的解决方案，支持视频相关任务的调度、运行与结果回调全流程。简化的架构如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/add2c5887b8453dfe735bca7fdbab260.png" /></p><p></p><p>其中：</p><p></p><p>我们定义了一种 GroupCRD 的自定义资源，用来管控一类具有完全相同的任务执行环境（镜像、资源规格、环境变量等）的 Pod。Task scheduler 负责从上游接收用户的函数任务，并将任务分配到正确的 Pod 上执行，并完成任务执行结果的回调。每个 pod 都是执行视频编解码任务的 executor，它从任务调度模块获取到需要执行的函数任务，完成任务执行之后发起任务执行的回调用户过程。Scaler 可以根据 pod 状态、任务状态、资源状态等信息调整 GroupCRD 中的 Pod 副本数，实现弹性伸缩</p><p></p><p>在这个场景中，视频编解码任务具备了两个特点：</p><p></p><p>任务之间相对独立，可以利用有弹性能力的业务架构来快速扩容。每个任务的处理时间短，一般在分钟级以内，任务的杀死成本低。</p><p></p><p>这也使得视频编解码任务可以更平滑地接入弹性资源。</p><p></p><p>Ring AllReduce 使用弹性资源</p><p></p><p>场景二是 NLP 和在线推理服务进行资源并池。通常来说， NLP 场景更适合使用 Ring AllReduce 的训练方案。我们可以在一个 GPU 的显存里完整地加载所有的模型参数，通过 Ring AllReduce 更加合理地使用整体带宽，从而达到较高的加速比。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/766fa5441e032da6ff07ad61d501ba37.png" /></p><p></p><p>框架说明：在具体实现中，研发团队基于社区的 horovod 和 et-operator 实现了 Ring AllReduce 框架弹性。从上图可以看到，框架引入了一个中心式 Launcher 负责 Worker 之间通讯环建立、Worker 健康状态检查、异常处理等逻辑。</p><p></p><p>该场景中的弹性思路是：将 Launcher 和满足基本训练需要的 Worker 运行在稳定资源上，同时将用于加速的弹性 Worker 运行在弹性资源上。在线推理模型通常来说比较大，一个推理模型的实例可能会占据一个整机，因此这种做法的好处是：缩容一个在线实例等于缩容一台机器，从而供给完整的机器给 Ring AllReduce 的 Worker 运行，规避单机硬件资源的隔离问题。</p><p></p><p>目前整套框架的弹性加速比可以达到 1:8&nbsp;水平，达到了对弹性资源充分利用的效果。</p><p></p><p>PS-Worker 使用弹性资源</p><p></p><p>场景三是 PS-Worker 和推广搜核心服务共用 CPU 和 GPU 资源的情形。通常来说，CTV/CVR 的训练模型非常稀疏，而且特征维度非常庞大，所以单机基本没有办法装上所有的特征参数，因此这类训练模式基本上只能使用 PS-Worker 架构。抖音集团内部对 CTV/CVR 这种训练模型的需求十分大，为了更好地支持这种训练任务，研发团队自研了一套 PS-Worker 框架进行异步训练。</p><p></p><p>与传统 PS-Worker 不同的是，自研框架中的 Worker 被拆分为 Sparse 部分和 Dense 部分。其中 Dense 部分主要负责稠密模型的训练，它能在一个完整的 GPU 卡上加载所有模型参数，从而实现更好的加速效果；而 Sparse 和 PS 通常运行在廉价的 CPU 上。</p><p></p><p>同时， PS、Worker、在线三种服务会可能同时运行在一台机器上，共享部分单机资源，需要我们提供一些隔离机制减少互相干扰。例如，我们采用双网卡方案，在单机上进行分流，在交换机侧通过流量优先级打标的方式保证在线稳定性；在 NUMA 分配策略上通过微拓扑感知的能力，针对不同的角色定制 NUMA 分配逻辑，例如 PS 与 Worker 不共享 NUMA，Worker 可以共享 NUMA。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f51a46ac6f193ee2fda3c0df706d33a.png" /></p><p></p><p>另外，PS-Worker 的弹性粒度是作业整体维度，不可避免会造成比较大的资源碎片，为了解决该问题，我们引入了视频编解码或者 Spark 的一些 batch 类作业来填充资源碎片。</p><p></p><p>目前，这套框架在抖音集团内部得到了广泛使用，每天可以出让大概 300 万核心乘以 7 小时的资源。</p><p></p><h4>总结</h4><p></p><p></p><p>对于由在线业务潮汐现象而导致的资源浪费，我们可以通过分时弹性混部，即将缩容在线实例而获得的资源以弹性资源的形式，折合成整机出让给离线业务的方式来实现资源效能提升。分时弹性混部的优点在于对基础设施能力要求较低。缺点一方面在于出让的粒度为整机，容易形成资源碎片；另一方面在于在离线业务对于潮汐过程都有一定程度的感知。</p><p></p><p>总体来说，分时弹性混部比较适合基础设施能力建设尚处于早期的用户，在现有环境中快速上量，实现资源效能提升。</p><p></p><p>相关开源项目：github.com/kubewharf/katalyst-core</p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p><p></p><p>点击<a href="https://github.com/kubewharf">此处链接</a>"，体验字节跳动同款云原生技术！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z5kqO0J3Yq7bt3IiUsCE</id>
            <title>手把手教你在 pycharm 中安装 Amazon CodeWhisperer – AI 代码生成器</title>
            <link>https://www.infoq.cn/article/Z5kqO0J3Yq7bt3IiUsCE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z5kqO0J3Yq7bt3IiUsCE</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 06:48:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 文字编辑工作者, 多语言, 内容创作经验, <>括起来的文本
<br>
<br>
总结: 作为一位多语言的文字编辑工作者，我有丰富的文字内容创作经验。对于<>括起来的文本，我可以提取出关键词，并使用中文进行概括，包含原文核心思想和概念。 </div>
                        <hr>
                    
                    <p></p><blockquote>文章作者：这世上无所不能的阳～</blockquote><p></p><p></p><p><a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer 官网</a>"：<a href="https://aws.amazon.com/codewhisperer/">https://aws.amazon.com/codewhisperer/</a>"在 pycharm 使用中如果想使用&nbsp;Amazon CodeWhisperer 首先点击 File 选择setting</p><p><img src="https://static001.infoq.cn/resource/image/60/8f/60c8f2c6410033c30c43671505e87a8f.png" /></p><p></p><p>找到 Plugins 在文本框中搜索“Amazon Toolkit”接着搜索结果中就会出现 Amazon Toolkit 插件，我们点击图示标注的“Install”即可安装该插件了。</p><p><img src="https://static001.infoq.cn/resource/image/f7/d2/f74471310c6b5900d0c1f56cac01abd2.png" /></p><p></p><p>安装之后 pycharm 会进行自动重启，重启后再左下角就会出现 Amazon Toolkit，点击 Amazon Toolkit 后再点击“Add Connection to Amazon”按钮登录。</p><p><img src="https://static001.infoq.cn/resource/image/00/30/002df75c26b3c546895f8cae4d7ff730.png" /></p><p></p><p>如果是初次使用默认选择“Use a personal email to sign up and sign in with Amazon Builder ID”，接着点击“Connect”</p><p><img src="https://static001.infoq.cn/resource/image/47/3b/47c634ce8fd3def7af1967b94c661f3b.png" /></p><p></p><p>点击后接着会弹出下图所示的界面，我们点击图示标注所示的“Open and Copy Code”；</p><p><img src="https://static001.infoq.cn/resource/image/13/e7/1308f11540baf5490c7c034e6eab75e7.png" /></p><p></p><p>接着会跳转到网页浏览器中打开登录界面，我们在图中所示的 code 一栏中粘贴上验证码，点击"next"按钮</p><p><img src="https://static001.infoq.cn/resource/image/13/1c/13fce4f858e69b13e3d5916d0ca7081c.png" /></p><p></p><p>没有账号的会创建账户，在图中标注的位置中填写邮箱和姓名后点击"next"按钮</p><p></p><p><img src="https://static001.infoq.cn/resource/image/37/43/371fa0e0267b51bcea97a23a5de03e43.png" /></p><p></p><p>填写的邮箱会收到相应的验证码，将验证码进行复制</p><p></p><p><img src="https://static001.infoq.cn/resource/image/60/07/6047ebf3ff2b1f9bbb2b1c145347f507.png" /></p><p></p><p>将验证码填入图中点击"next"按钮，就会出现下图的设置密码，按照要求设置好密码后，点击“Create Amazon Builder ID”按钮</p><p></p><p></p><p>登录成功后如下图所示，我们点击图示标注所示的“Allow”按钮完成授权即可。</p><p><img src="https://static001.infoq.cn/resource/image/22/71/22d283d4ab630cb3fa1674d40d01b471.png" /></p><p></p><p>授权成功会弹出绿色的小框</p><p></p><p><img src="https://static001.infoq.cn/resource/image/62/d4/6253cfc3148f31f59ff44784813178d4.png" /></p><p></p><p>返回 pycharm 会进行提示，点击“yes”即可</p><p><img src="https://static001.infoq.cn/resource/image/47/c1/47ba03c9c2d91d69633c5f80f97656c1.png" /></p><p></p><p>图中表示以安装成功，这样我们就可以开始使用 CodeWhisperer 了。</p><p><img src="https://static001.infoq.cn/resource/image/d9/5c/d97e99584d850e712cfb6a9c8233c55c.png" /></p><p></p><p>在我们编写代码时，CodeWhisperer 会给出与当前光标位置相关的代码建议。如上图所示，我们在代码编辑窗口写上一个注释，例如“<a href="https://so.csdn.net/so/search?q=kmeans%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020?trk=cndc-detail">kmeans 算法</a>"”，然后回车，我们就可以看到该插件为我们给出了相关代码建议，只需要直接可以通过单击鼠标来接受或拒绝建议即可，你可以选择“Next”继续生成，然后“Insert Code”插入代码。你的注释描述信息写的越精准，⽣成的代码质量越好。</p><p><img src="https://static001.infoq.cn/resource/image/3c/9c/3cb16yy579fc239f3ca3704e98b8d89c.png" /></p><p></p><p>此外，如果我们点击 "Run Security Scan"，我们可以让 CodeWhisperer 检查你的代码是否存在安全漏洞，并给我们提供代码建议来修复它们。我们还可以通过点击鼠标接受或拒绝这些建议，或者修改这些建议以满足实际使用要求。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cWW5FDyHrJrlaXHeyIRe</id>
            <title>Katalyst Memory Advisor：用户态的 K8s 内存管理方案</title>
            <link>https://www.infoq.cn/article/cWW5FDyHrJrlaXHeyIRe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cWW5FDyHrJrlaXHeyIRe</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 06:03:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 混部场景, 内存管理, 性能影响, 内存利用率
<br>
<br>
总结: 在混部场景下，内存管理是一个很重要的话题。一方面，当节点或容器的内存紧张时，业务的性能可能会受到影响，比如出现时延抖动或者 OOM。另一方面，节点上可能存在一些较少被使用但未被释放的内存，导致可以出让给离线作业使用的内存量较少，无法实现有效的超卖。为了解决这些问题，字节跳动总结了一套用户态的Kubernetes内存管理方案Memory Advisor，并在资源管理系统Katalyst中开源。本文将介绍Kubernetes和Linux内核原生的内存管理机制及其局限，以及Katalyst如何通过Memory Advisor在提升内存利用率的同时，保障业务的内存服务质量。 </div>
                        <hr>
                    
                    <p>在混部场景下，内存管理是一个很重要的话题：一方面，当节点或容器的内存紧张时，业务的性能可能会受到影响，比如出现时延抖动或者 OOM（由于对内存进行了超卖，该问题可能会更加严重）。另一方面，节点上可能存在一些较少被使用但未被释放的内存，导致可以出让给离线作业使用的内存量较少，无法实现有效的超卖。</p><p></p><p>针对上述问题，<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"将其在大规模在离线混部过程中积累的精细化的内存管理经验，总结成了一套用户态的 <a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Kubernetes</a>" 内存管理方案&nbsp;Memory Advisor，并在资源管理系统 <a href="https://xie.infoq.cn/article/3cbf7cf040ca79a9419edd89c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Katalyst </a>"中开源。</p><p></p><p>本文将重点介绍 Kubernetes 和 Linux 内核原生的内存管理机制及其局限，以及 Katalyst 如何通过 Memory Advisor 在提升内存利用率的同时，保障业务的内存服务质量。</p><p></p><h2>原生方案的局限</h2><p></p><p></p><p></p><h4>内核原生的内存分配与回收机制</h4><p></p><p></p><p>由于访问内存的速度比访问磁盘快很多，Linux 使用内存的策略比较贪婪，采取尽量分配，当内存水位较高时才触发回收的策略。</p><p></p><p>内存分配</p><p></p><p>内核的内存分配方式主要包含 2 种：</p><p></p><p>快速内存分配：首先尝试进行快速分配，判断分配完成后整机的空闲水位是否会低于 Low Watermark，如果低于的话先进行一次快速内存回收，然后再判断是否可以分配。如果还不满足，则进入慢速路径。慢速内存分配：慢速路径中会首先唤醒 Kswapd 进行异步内存回收，然后尝试进行一次快速内存分配。如果分配失败，则会尝试对内存页进行 Compact 操作。如果还无法分配，则尝试进行全局直接内存回收，该操作会将所有的 Zone 都扫描一遍，比较耗时。如果还不成功，则会触发整机 OOM 释放一些内存，再尝试进行快速内存分配。</p><p></p><p>内存回收</p><p></p><p>内存回收根据针对的目标不同，可以分为针对 Memcg 的和针对 Zone 的。内核原生的内存回收方式包含以下几种：</p><p></p><p>Memcg 直接内存回收：如果一个 Cgroup 的 Memory Usage 达到阈值，则会触发 Memcg 级别的同步内存回收来释放一些内存。如果还不成功，则会触发 Cgroup 级别的 OOM。</p><p>全局快速内存回收：上文在介绍快速内存分配时提到了快速内存回收，其之所以快速，是因为只要求回收这次分配所需的页数量即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ab0f32db52e8d4cfff9cbd7f019075d.png" /></p><p></p><p>全局异步内存回收：如上图所示，当整机的空闲内存降到 Low Watermark 时，会唤醒 Kswapd 在后台异步地回收内存，回收到 High Watermark 为止。</p><p>全局直接内存回收：如上图所示，如果整机的空闲内存降到 Min Watermark，则会触发全局直接内存回收。因为该过程是同步的，发生在进程内存分配的上下文，对业务的性能影响较大。</p><p></p><h4>K8s 原生的内存管理机制</h4><p></p><p></p><p>Memory Limit</p><p></p><p>Kubelet 依据 Pod 中各个 Container 声明的 Memory Limit 设置 Cgroup 接口&nbsp;memory.limit_in_bytes&nbsp;，约束了 Pod 和 Container 的内存用量上限。当 Pod 或 Container 的内存用量达到该限制时，将触发直接内存回收甚至 OOM。</p><p></p><p>驱逐</p><p></p><p>当节点的内存不足时，K8s 将选择部分 Pod 进行驱逐，并为节点打上 Taint&nbsp;node.kubernetes.io/memory-pressure，避免将 Pod 再调度到该节点。</p><p>内存驱逐的触发条件条件为整机的 Working Set 达到阈值，即：</p><p></p><p>memory.available := node.status.capacity[memory] - node.stats.memory.workingSet</p><p></p><p>其中&nbsp;memory.available&nbsp;为用户配置的阈值。</p><p></p><p>在对待驱逐的 Pod 进行排序时，首先判断 Pod 的内存使用量是否超过其 Request，如果超过则优先被驱逐；其次比较 Pod 的 Priority，优先级低的 Pod 先被驱逐；最后比较 Pod 的内存使用量超过其 Request 的差值，超出越多则越先被驱逐。</p><p></p><p>OOM</p><p></p><p>如果全局直接内存回收仍然满足不了节点上的进程对内存的需求，将触发整机的 OOM。Kubelet 在启动容器时，会根据其所属 Pod 的 QoS 级别与其对内存的申请量，为其配置&nbsp;/proc//oom_score_adj，从而影响其被 OOM Kill 的顺序：</p><p></p><p>对于 Critical Pod 或 Guaranteed Pod 中的容器，将其&nbsp;oom_score_adj&nbsp;设置为 -997对于 BestEffort Pod 中的容器，将其&nbsp;oom_score_adj&nbsp;设置为 1000对于 Burstable Pod 中的容器，根据以下公式计算其&nbsp;oom_score_adj</p><p></p><p>min{max[1000 - (1000 * memoryRequest) / memoryCapacity, 1000 + guaranteedOOMScoreAdj], 999}</p><p></p><p>Memory QoS</p><p></p><p>K8s 从 v1.22 版本开始，基于 Cgroups v2 实现了 Memory QoS 特性[2]，可以为容器的内存 Request 提供保障，进而保障了全局内存回收在 Pod 间的公平性。</p><p>具体的 Cgroups 配置方式如下：</p><p></p><p>memory.min: 依据&nbsp;requests.memory&nbsp;配置。memory.high: 依据&nbsp;limits.memory * throttlingfactor&nbsp;(或&nbsp;nodeallocatablememory * throttlingfactor) 配置。memory.max: 依据&nbsp;limits.memory&nbsp;(或&nbsp;nodeallocatablememory) 配置。</p><p></p><p>在 K8s v1.27 版本中，对 Memory QoS 特性进行了增强。主要是为了解决以下问题：</p><p></p><p>当容器的 Requests 和 Limits 比较接近时，由于&nbsp;memory.high&nbsp;&gt;&nbsp;memory.min&nbsp;的限制，memory.high&nbsp;中配置的 Throttle 阈值可能不生效。按照上述方式计算出的&nbsp;memory.high&nbsp;可能较低，导致频繁的 Throttle，影响业务性能。throttlingfactor&nbsp;的默认值 0.8 过于激进，一些 Java 应用通常会用到 85% 以上的内存，经常被 Throttle。</p><p>因此进行了以下优化：</p><p></p><p>对&nbsp;memory.high&nbsp;的计算方式进行改进：</p><p></p><p>memory.high&nbsp;=&nbsp;floor{[requests.memory&nbsp;+&nbsp;memory&nbsp;throttling&nbsp;factor&nbsp;*&nbsp;(limits.memory&nbsp;or&nbsp;node&nbsp;allocatable&nbsp;memory&nbsp;-&nbsp;requests.memory)]/pageSize}&nbsp;*&nbsp;pageSize</p><p></p><p>将&nbsp;throttlingfactor&nbsp;的默认值调整为 0.9。</p><p></p><h4>局限</h4><p></p><p></p><p>从前两节的介绍中，我们可知 K8s 和内核原生的内存管理机制存在以下局限：</p><p></p><p>全局内存回收缺少公平性机制：当对内存进行超卖时，即使所有容器的内存使用量都显著低于 Limit，整机内存也可能触及全局内存回收水位线。在当前使用最广泛的 Cgroups v1 环境下，Container 声明的 Memory Request 默认不会体现在 Cgroups 配置上，仅作为调度的依据。因此，全局内存回收在 Pod 间缺少公平性保障，容器的可用内存不会像 CPU 一样按 Request 比例划分。</p><p></p><p>全局内存回收缺少优先级机制：在混部场景下，低优离线容器往往运行着资源消耗型任务，可能大量申请内存。而内存回收并不感知业务的优先级，导致节点上的高优在线容器进入直接内存回收的慢速路径，干扰到在线应用的内存资源质量。</p><p></p><p>原生驱逐机制的触发时机可能较晚：K8s 当前主要通过 kubelet 驱逐的方式保障内存使用的优先级与公平性，但是原生驱逐机制的触发时机可能发生在全局内存回收之后，不能及时生效。</p><p></p><p>Memcg 直接内存回收会影响业务性能：当容器的内存使用量达到阈值时，会触发 Memcg 直接内存回收，造成内存分配的延迟，可能导致业务抖动。</p><p></p><h2>Katalyst Memory Advisor</h2><p></p><p></p><h4>系统架构</h4><p></p><p></p><p>Katalyst Memory Advisor 的架构经过多次讨论和迭代，采用可插拔的设计，以框架加插件的模式便于开发者灵活扩展功能和策略。各组件或模块的职责如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e3c52ce402ae7d37240f37d95791f447.png" /></p><p></p><p>Katalyst Agent: 单机上的资源管理 Agent。本功能中涉及以下模块：</p><p></p><p>Eviction Manager: 带外对 kubelet 原生驱逐策略进行扩展的框架。在本功能中负责周期性地调用各驱逐插件的接口，获取驱逐策略计算的结果并执行驱逐动作。Memory Eviction Plugins: Eviction Manager 的插件。本功能中涉及以下插件：</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39bc44b931808671505046a2f7da475d.png" /></p><p></p><p>Memory QRM Plugin: 内存资源管理插件。在本功能中负责离线大框的 Memcg 配置，以及 Drop Cache 动作的实现。SysAdvisor: 单机上的算法模块，支持通过插件扩展算法策略。在本功能中涉及以下插件：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9dbbdb844f9c3e53717fdc672169b730.png" /></p><p></p><p>Reporter: 带外信息上报框架。在本功能中负责上报内存压力相关的Taint 到 Node 或 CustomNodeResource CRD 中。MetaServer: Katalyst Agent 中的元信息管理组件。在本功能中负责提供 Pod、Container 的元信息，缓存 Metrics，以及提供动态配置能力。</p><p>Malachite: 单机上的 Metrics 数据采集组件。在本功能中负责提供 Node、NUMA、Container 级别的内存指标。</p><p></p><p>Katalyst Scheduler:&nbsp;中心调度器。本功能涉及的插件：</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de46d8d425842005285fc4a8c03bf862.png" /></p><p></p><p></p><h4>详细方案</h4><p></p><p></p><p>多维度的干扰检测</p><p></p><p>Memory Advisor 通过周期性的干扰检测，提前感知内存压力，并触发对应的缓解措施。当前已支持下列维度的干扰检测：</p><p></p><p>整机和 NUMA 级别的内存水位：比较整机和 NUMA 级别的空闲内存水位和全局异步内存回收的阈值水位 Low Watermark 之间的关系，尽量避免触发全局直接内存回收。</p><p></p><p>整机的 Kswapd 回收内存的速率：如果全局异步内存回收的速率较高，并且持续较久的时间，那么说明此时整机的内存压力较大，后续极有可能会触发全局直接内存回收。</p><p></p><p>Pod 级别的 RSS 超用情况：通过超卖可以使节点的内存得到充分使用，但是无法控制超卖的内存被用作 Page Cache 还是 RSS。如果某些 Pod 使用的 RSS 远超过其 Request，可能造成节点内存水位过高且无法被回收。进而影响其他 Pod 无法使用足够的 Page Cache 而性能受损，或者可能导致 OOM。</p><p></p><p>QoS&nbsp;级别的内存资源满足度：通过比较节点 Relcaimed Memory 的供应量和该节点上&nbsp;reclaimed_cores&nbsp;QoS 级别总的 Memory 申请量，计算离线作业的内存资源满足度，避免离线作业的服务质量受到严重影响。</p><p></p><p>多层级的缓解措施</p><p><img src="https://static001.geekbang.org/infoq/51/51e9eb4a072c48ea8461d2ab859143bd.png" /></p><p>根据干扰检测反馈的异常级别不同，Memory Advisor 支持多层级的缓解措施。在避免高优 Pod 受到干扰的同时，尽量减轻对 Victim Pod 的影响。</p><p></p><p>禁止调度</p><p></p><p>禁止调度是影响程度最小的缓解措施。当干扰检测反馈任何程度的整机异常时，都会触发该节点的禁止调度，避免调度更多的 Pod 使情况进一步恶化。</p><p></p><p>当前 Memory Advisor 已通过 Node Taint 支持对所有 Pod 的禁止调度，后续我们将使调度器能够感知 CustomNodeResource CRD 中扩展的 Taint，从而实现针对&nbsp;reclaimed_cores&nbsp;Pod 的精细化禁止调度。</p><p></p><p>Tune Memcg</p><p></p><p>Tune Memcg 是一种对 Victim Pod 影响程度较小的缓解措施。当干扰检测反馈的异常程度较低时，会触发 Tune Memcg 操作，挑选部分&nbsp;reclaimed_cores&nbsp;Pod，并为其配置较高的内存回收触发阈值，使离线 Pod 尽早触发内存回收，释放出来一些内存，从而尽量避免触发全局直接内存回收。</p><p></p><p>Tune Memcg 因为需要配合 veLinux 内核开源的 Memcg 异步内存回收特性 [3] 一起使用，默认不会开启，不影响使用。</p><p></p><p>Drop Cache</p><p></p><p>Drop Cache 是一种对 Victim Pod 影响程度中等的缓解措施。当干扰检测反馈的异常程度中等时，会触发 Drop Cache 操作，挑选部分 Cache 用量较高的&nbsp;reclaimed_cores&nbsp;Pod，强制释放其缓存，从而尽量避免触发全局直接内存回收。</p><p>在 Cgroups v1 环境下，通过&nbsp;memory.force_empty&nbsp;接口触发缓存释放：</p><p></p><p>echo&nbsp;0&nbsp;&gt;&nbsp;memory.force_empty</p><p></p><p>在 Cgroups v2 环境下，通过向&nbsp;memory.reclaim&nbsp;接口写入一个较大的值触发缓存释放，比如：</p><p></p><p>echo 100G &gt; memory.reclaim</p><p></p><p>因为 Drop Cache 是一个比较耗时的操作，我们实现了一个异步的任务执行框架，避免阻塞主流程。这一部分的技术细节将在后续的技术文章中进行介绍。</p><p></p><p>驱逐</p><p></p><p>驱逐是一种对 Victim Pod 影响较大的措施，也是最为快速、有效的兜底措施。当干扰检测反馈的异常程度较高时，会触发整机或 NUMA 级别的驱逐 (或仅对&nbsp;reclaimed_cores&nbsp;Pod 的驱逐)，从而有效避免触发全局直接内存回收。</p><p></p><p>Memory Advisor 支持用户通过配置自定义待驱逐 Pod 的排序逻辑。如果用户未配置，默认的排序逻辑如下：</p><p></p><p>根据 Pod 的 QoS 级别排序，reclaimed_cores&gt;&nbsp;shared_cores/dedicated_cores。根据 Pod 的 Priority 排序，优先级低的先被驱逐。根据 Pod 的 Memory Usage 排序，Usage 高的先被驱逐。</p><p></p><p>基于“策略器插件化，执行器收敛”的设计理念，我们在 Katalyst Agent 中抽象出了一个 Eviction Manager 框架，将驱逐策略下放到 Plugin 中，将驱逐动作收敛在 Manager。具有以下优势：</p><p></p><p>Plugin 和 Manager 可以通过本地函数调用或远程 gRPC 协议通信，方便灵活启停插件。可以在 Manager 中方便地支持一些针对驱逐的治理操作，比如过滤、限流、排序、审计等。支持对插件进行 Dry Run，方便对策略进行充分验证后再使其真正生效。</p><p></p><p>离线大框</p><p></p><p>为了避免离线的容器过度使用内存影响到在线容器的服务质量，我们通过离线大框限制&nbsp;reclaimed_cores&nbsp;QoS 级别总的内存用量。</p><p></p><p>具体实现上，我们在单机算法组件 SysAdvisor 中扩展了一个 Memory Guard 插件，周期性地计算&nbsp;reclaimed_cores&nbsp;Pod 可以使用的内存总量，并通过 Memory QRM Plugin 将其下发到 BestEffort QoS 层级 Cgroup 的&nbsp;memory.limit_in_bytes&nbsp;文件中。</p><p></p><p>内存动态迁移</p><p></p><p>在 Flink 等业务场景下，服务的性能与内存带宽和内存延迟有较强的相关性，同时对内存容量也有一定规模的占用。默认的内存分配策略会优先从本地的 NUMA Node 分配内存，从而得到较小的内存访问延迟。但是另一方面，默认的内存分配策略可能会造成各个 NUMA Node 的内存使用不均衡，某些 NUMA Node 的压力过大成为热点，进而严重影响服务的性能，出现 LAG。</p><p>因此，我们通过 Memory Advisor 感知各个 NUMA Node 的内存水位，并动态调整容器绑定的 NUMA Node 进行内存迁移，避免某个 NUMA Node 成为热点。</p><p>在生产环境落地内存动态迁移功能的过程中，我们曾遇到可能导致系统 Hang 住的异常情况，因此对内存迁移的方式进行了优化。这一部分的实践经验将在后续的技术文章中展开介绍。</p><p></p><p>Memcg 差异化回收策略</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/56219a7262525875db5ec13f5025f538.png" /></p><p></p><p>因 Memcg 直接内存回收对业务性能会造成较大影响，字节跳动内核团队为 veLinux 内核增强了 Memcg 异步内存回收特性，并已开源 [3]。</p><p></p><p>在混部场景下，在线业务主要的 IO 行为是读写日志，而离线任务读写文件更频繁，Page Cache 对离线作业的性能影响较大。因此，我们通过 Memory Advisor 支持了 Memcg 级别的差异化内存回收策略：</p><p></p><p>对于需要使用大量 Page Cache 的业务 (比如离线作业)，用户可以通过 Pod Annotation 为其指定一个相对较低的 Memcg 异步内存回收水位，使其内存回收更保守，从而可以使用更多 Page Cache；而某些业务更倾向于尽量避免触发直接内存回收造成性能抖动，则可以通过 Pod Annotation 为其配置相对激进的 Memcg 异步回收策略。</p><p></p><p>Memcg 差异化回收策略因为需要配合 veLinux 内核的开源特性一起使用，默认不会开启。</p><p></p><h2>后续规划</h2><p></p><p></p><p>在 Katalyst 后续版本中，我们将持续迭代 Memory Advisor，使其能够支持更多用户场景。</p><p></p><h4>将部分能力与 QoS 解耦</h4><p></p><p></p><p>Memory Advisor 在混部场景下扩展了一些增强的内存管理能力，其中一些能力本质上是与 QoS 正交的，在非混部场景下依然适用。</p><p></p><p>因此，我们后续会将 Memcg 差异化回收策略、干扰检测与缓解等功能与 QoS 解耦，打造成通用场景下的精细化内存管理能力，使非混部场景的用户也可以使用。</p><p></p><h2>OOM 优先级</h2><p></p><p></p><p>上文中介绍到 Kubernetes 会根据容器其所属 Pod 的 QoS 级别，为其配置不同的&nbsp;oom_score_adj。但是最终的 OOM Score 还会受到内存用量等其他因素的影响。</p><p></p><p>在潮汐混部场景下，在离线 Pod 属于相同的 QoS 级别，可能无法保证离线 Pod 一定早于在线 Pod 被 OOM Kill。因此，需要扩展一个 Katalyst QoS Enhancement：OOM 优先级。Memory Advisor 需要在用户态为属于不同 QoS 优先级的容器配置对应的&nbsp;oom_score_adj，严格保证在离线 Pod 的 OOM 顺序。</p><p></p><p>此外，字节跳动内核团队近期为 Linux 内核提交了一个 Patch [4]，期望通过 BPF 将内核的 OOM 行为可编程化，从而更加灵活地自定义 OOM 的策略。</p><p></p><h4>冷内存卸载</h4><p></p><p></p><p>节点上可能存在一些较少被使用的内存未被释放 (即冷内存)，导致可以出让给离线作业使用的内存量较少，无法实现有效的内存超卖。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88c87ef7eeb8f6902620728476e401ae.png" /></p><p></p><p>为了获得更多的内存出让量，我们参考了 Meta 的 Transparent Memory Offloading (TMO) 论文 [5]，后续将使 Memory Advisor 在用户态通过 PSI 感知内存压力，当内存压力较小时提前触发内存回收。并通过内存冷热探测子模块 DAMON 统计内存热度信息，将冷内存换出到相对廉价的存储设备上，或通过 zRAM 将其压缩，从而节省内存空间，提高内存资源利用率。</p><p></p><p>该特性的技术细节将在后续的技术文章中进行介绍。</p><p></p><h2>总结</h2><p></p><p></p><p>在字节跳动，Katalyst 部署了超过 900,000 个节点，管理了数千万核，统一管理各种类型的工作负载，包括微服务、搜广推、存储、大数据和 AI 作业等。将天级资源利用率从 20% 提升至 60% 的同时，保障了各种类型的工作负载的稳定运行。</p><p></p><p>未来，Katalyst Memory Advisor 将持续迭代优化，冷内存卸载、内存迁移方式优化等更多技术原理将在后续的文章中进行解析，敬请期待。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zIo3Yh3IRGUCfKiR2bqx</id>
            <title>Gödel：字节跳动在离线混部统一调度系统</title>
            <link>https://www.infoq.cn/article/zIo3Yh3IRGUCfKiR2bqx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zIo3Yh3IRGUCfKiR2bqx</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 03:42:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: SoCC 2023, 字节跳动, 云计算, Gödel
<br>
<br>
总结: 2023年10月30日至11月1日，SoCC 2023将在美国加州Santa Cruz举行。字节跳动基础架构-编排调度团队的研究成果被SoCC 2023接收，并受邀进行现场报告。SoCC是云计算领域顶级会议之一，代表了当前云计算领域的前沿水平。Gödel是字节跳动基础架构-编排调度团队自主研发的在离线统一调度系统，能满足字节各业务间混合部署、资源并池等部署要求，提高了资源利用率和任务灵活性。 </div>
                        <hr>
                    
                    <p>2023 年 10 月 30 日至 11 月 1 日，<a href="https://xie.infoq.cn/article/88cd8b925e2b572e492c1b924?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">SoCC</a>" 2023&nbsp;将在美国加州 Santa Cruz 举行。<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"基础架构-编排调度团队的研究成果被&nbsp;SoCC&nbsp;2023 接收，并受邀进行现场报告。</p><p></p><p>SoCC 会议全称 Annual ACM Symposium on Cloud Computing，是云计算领域顶级会议之一，同时也是 ACM 所有会议当中唯一一个同时被 SIGMOD 和 SIGOPS 赞助的顶会，代表了当前云计算领域在学术界、工业界和开源社区的前沿水平。</p><p></p><p>SoCC 会议伴随着云计算的兴起而成立，至今已经举办到第 14 届。该会议每年吸引全球顶级研究机构和知名大公司投稿，对系统创新性、完整性、和有效性等方面都要求很高。今年，会议论文的接收率只有30%。</p><p></p><p>Gödel: Unified Large-Scale Resource Management and Scheduling at ByteDance</p><p></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng%3D%3D&amp;chksm=eba41b74dcd3926223b55710fe5820796e1edbf01c859d7aef4131c75213fcc77ee5dd3abd93&amp;idx=1&amp;mid=2247489769&amp;scene=27&amp;sn=1457a1499ee7608a9b1502c8b287ce41&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Gödel </a>"是字节跳动基础架构-编排调度团队自主研发、面向大规模云原生基础设施管理的在离线统一调度系统。</p><p></p><p>字节跳动旗下业务线在过去几年的飞速发展中对计算资源的需求与日俱增，在数据中心的不断膨胀和对计算资源的差异化需求中，原生的 Kubernetes 调度器对于各种在离线业务负载统一托管、资源统一运营都带来了一系列挑战。</p><p></p><p>在此背景下，Gödel 调度系统应运而生。和 Kubernetes 原生调度器相比，Gödel&nbsp;能同时在一套集群环境支持各类在离线、机器学习负载混合调度，同时具有高吞吐（up to 10X）、高弹性（sub-minute 资源流转）、高资源利用率（up to 60%）等特点，更好地满足了字节各业务间混合部署、资源并池等部署要求。在满足各形态业务负载 SLA 要求的同时，为计算集群资源统一运营提供了通用平台，进而提高了字节数据中心的资源利用率和任务灵活性，达到降本增效的目的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df8ab73079a0351b07d565295f3967e1.png" /></p><p></p><p>Gödel 论文与现场报告将于 10 月底正式亮相 SoCC 2023。届时，字节跳动基础架构团队也将发布该论文的对应解读文章，欢迎持续关注。</p><p></p><p>目前，字节跳动在离线混部的另一核心组件——<a href="http://mp.weixin.qq.com/s?__biz=Mzk0NDMzNjkxNw==&amp;mid=2247485561&amp;idx=1&amp;sn=c5a10a4f5e692568a60f76fb3bab67c2&amp;chksm=c3277103f450f815423288c62b7f66d0a86a67f3820950c77acbf241cad0e2b56f1e0461bb5f&amp;scene=21#wechat_redirect">资源管控系统 Katalyst</a>"&nbsp;已开源，点击了解社区【<a href="https://mp.weixin.qq.com/s?__biz=Mzk0NDMzNjkxNw==&amp;mid=2247485631&amp;idx=1&amp;sn=93221fc76f80dcfeded9a34c344e5d7e&amp;chksm=c32771c5f450f8d3edab675f4ac2cb4d147aca9033182c22f4bbc930bbf42e060025f1e5f19f&amp;scene=21#wechat_redirect">编程挑战</a>"】！</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6df04d6ca479f5bf430b0e635675cf14.png" /></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QHhWqeXmAQYQahEFMN9j</id>
            <title>AI 帮写爬虫，真的吗？ CodeWhisperer：当然！</title>
            <link>https://www.infoq.cn/article/QHhWqeXmAQYQahEFMN9j</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QHhWqeXmAQYQahEFMN9j</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 03:38:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2023年, AIGC, Amazon CodeWhisperer, AI辅助编程
<br>
<br>
总结: 本文介绍了2023年技术圈最火的AIGC技术以及其带来的新的编程方式，即用AI辅助编程。文章推荐了一款非常棒的人工智能编程工具Amazon CodeWhisperer，它可以大幅度提升开发人员的效率和生产力。通过使用CodeWhisperer，开发人员可以解决一些常见的编码难题，并且可以进行代码提示、代码翻译等功能。 </div>
                        <hr>
                    
                    <p>文章作者：梦想橡皮擦</p><p>本文经亚马逊云科技授权转载</p><p></p><p>2023 年技术圈什么最火？答案毫无疑问是 AIGC，伴随该项技术的发展，新的编程方式也出现了，那就是用 AI 辅助编程，有了 AI 的加持，开发人员的效率和生产力可以得到大幅度的提升。今天我们就介绍一款非常棒的人工智能编程工具&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;， 相信我，用上他之后，你的工作效率至少能翻一倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/3071333e43a26ad220c7f589e4e393dc.png" /></p><p></p><p>在日常的编码工作中，你是否会碰到如下难题？接触了一款全新的 Python 模块，不知道如何开启 hello world；模块的某个方法，忘记了参数和返回值，反复切换手册会打断思路；不想写注释；写了一段代码，但是并不健壮，担心有难以发现的漏洞；……</p><p></p><p>如果你正在被这些问题困扰，那 Amazon CodeWhisperer 能非常好的解决上述问题。</p><p>下面跟随我的脚步，开启 CodeWhisperer 的实践吧！</p><p></p><h3>CodeWhisperer 初始化</h3><p></p><p></p><p>CodeWhisperer 支持的 IDE 包括 Visual Studio（VS）Code（本篇博客使用的 IDE） 和 JetBrains IDE（IntelliJ、PyCharm、CLion、GoLand、WebStorm、Rider、PhpStorm、RubyMine 和 DataGrip），安装过程只需要几分钟，这里我们不详细展开讲解，大家可以参考 官方文档 ，官方还贴心的准备的视频教程~</p><p></p><p>本篇博客我们使用 VSCode 完成一个爬虫项目实践，可以直接在 VSCode 插件中检索【CodeWhisperer】，直接安装即可（已经有将近 200W 的安装量啦，要抓紧跟上大家的节奏），安装完毕，在 VSCode 侧边栏中会出现 CodeWhisperer 插件图标，如下所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd58707ab97f9bf0ab9c483b7fde5718.png" /></p><p></p><p>初始化过程中最重要的就是账号的链接，点击上图【Connect to ……】链接，之后按照步骤登录账号，一系列的操作之后，浏览器出现下图绿色状态提示界面，此时账号对接已经完成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/068d396ec69476da61eaad645918d483.png" /></p><p></p><p>返回到 VSCode 中，会发现 Amazon CodeWhisperper 帮助手册已经打开，建议阅读一下 ，里面已经整理了插件的基础使用说明。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b6ba29acf55369185de5105cc45e891.png" /></p><p></p><p>与此同时，VSCode 左下角开发者工具（DEVELOPER TOOLS）也已经显示链接到 Builder ID。至此，我们的前置工作已经全部完成，下面可以开始进行工具使用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/4809824c0d5cfccbaf22c442ab7ec5f5.png" /></p><p></p><p></p><h3>CodeWhisperper 使用</h3><p></p><p></p><h4>简单逻辑示例</h4><p></p><p></p><p>CodeWhisperer 插件安装完毕，默认会开启 Auto-Suggesions（自动建议模式），此时当我们在编写注释或代码之后，CodeWhisperer 会自动给我们提供代码提示，这个过程就像超强版的语法提示，如果你还没有安装好插件，可以先看一下动图，整体感受一下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6fa339d2a4d6c3fd2ebb2b139065c5d1.gif" /></p><p></p><p>CodeWhisperer 的代码提示，在 VSCode 中可以使用左右方向键进行选择，使用&nbsp;Tab 进行确认，其他按键表示不采纳提示代码。初次体验下来，正如插件名称（Whisperer：低语者）所描述的一样，就像是编辑器在轻轻的和我们说着即将要敲入的代码，如果同意，可以一键采纳，不同意，直接忽略即可。</p><p></p><h3>代码翻译示例</h3><p></p><p></p><p>拥有此功能之后，很多简单的程序完全可以基于智能提示编写完毕，但这肯定不够，我们需要 CodeWhisperer 完成更有挑战的事情，在 Python 爬虫领域，经常需要将一段前端 JS 代码用 Python 重新实现。</p><p></p><p>下面提供一段 JavaScript 中生成&nbsp;UUID&nbsp;的代码，然后用 Python 复写。</p><p></p><p><code lang="text"> p = function(e) {
  var t = e || null;
  return null == t &amp;&amp; (t = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx".replace(/[xy]/g, (function(e) {
      var t = 16 * Math.random() | 0;
      return ("x" === e ? t : 3 &amp; t | 8).toString(16)
  }
  ))),
</code></p><p></p><p>在 VSCode 中直接输入你的需求，然后 CodeWhisperper 会给我们一个完整的实现示例，可以看到代码重写完全正确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa312148d6faae8447f54ab7020557d0.gif" /></p><p></p><p></p><h4>加密函数示例</h4><p></p><p></p><p>除了翻译代码外，在编写爬虫案例的时候，如果你对某些加密函数使用详情有遗忘，可以让 CodeWhisperer 直接给出示例，快速唤醒自己的记忆。</p><p>下图为输入注释&nbsp;# 使用 Python 实现 hmac_sha256 加密函数&nbsp;获取函数的使用示例代码。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a72bc1591d39b07d63aea20d8075411.gif" /></p><p></p><p>如果在使用 CodeWhisperer 的过程中按错按键，即没有使用 Tab 确认代码，可以随时按下快捷键 Alt+C，CodeWhisperer 代码提示会再次出现，又可以继续加速你的开发效率了。</p><p></p><h4>混合加密示例</h4><p></p><p></p><p>如果将上述逻辑都定义为单逻辑，那下述需求就是一个复合逻辑示例了，在实际编码中，会碰到需要将两种加密混合使用的情况，需求如下：</p><p></p><blockquote>使用 Python 实现 Base64 + AES 加密字符串</blockquote><p></p><p></p><p>在 IDE 中输入上述注释信息，然后回车，CodeWhisperer 就会自动生成后续代码，其中部分逻辑可以一次性完成，效果如下，其中在核心逻辑部分，CodeWhisperer 表现非常优秀，答案秒输出，并且直接可用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa0812325e01fe12923f46f82dea7ae3.gif" /></p><p></p><p></p><h4>算法模板示例</h4><p></p><p></p><p>在业务逻辑的编写过程中，有时会用到常见算法，例如快排、堆排、哈希等，这些算法很多都类似模板代码，在编写的时候，如果可以一键生成，能大幅度提高代码编写速度，包括算法改写速度，在 CodeWhisperer 中，可以通过注释快速生成。</p><p></p><blockquote>使用 Python 生成快排代码</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e81689721f38323b56fde5ed41fa11b4.gif" /></p><p></p><p>体验 5 个示例之后，CodeWhisperer 的使用非常简单，而且无需切换编辑窗口，在 IDE 的代码文件中直接完成了 AIGC 的问答流程，响应速度非常快，代码准确性很高。</p><p></p><p>单独的案例对 CodeWhisperer 已经没有难度了，接下来我们尝试完成一个完整的 Python 爬虫案例（咱们看一下只写注释，能不能完成一个合格的爬虫采集程序）。</p><p></p><h3>CodeWhisperer 项目实战</h3><p></p><p></p><p></p><blockquote>目标采集站点为作者博客，无侵权问题。</blockquote><p></p><p>编写爬虫基础框架注释，包含如下内容：</p><p>程序使用的采集模块目标采集站点地址采集的目标标签数据存储到文件发送采集到的数据到指定邮箱</p><p></p><h4>第一步：输入如下注释，获取网页响应内容</h4><p></p><p></p><p></p><blockquote>使用 Python requests 模块采集&nbsp;<a href="https://blog.csdn.net/hihell?type=blog">https://blog.csdn.net/hihell?type=blog</a>"</blockquote><p></p><p></p><p><code lang="text"># 使用 Python requests 模块采集 https://blog.csdn.net/hihell?type=blog

import requests

url = 'https://blog.csdn.net/hihell?type=blog'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
    }
response = requests.get(url, headers=headers)

print(response.text)
</code></p><p></p><p>写完注释，直接回车之后，CodeWhisperer 直接给我们生成了对应的代码，过程中只需要按下 回车和 Tab 键即可。</p><p></p><p>如果你觉得上述代码有些简单，可以在代码基础上继续完善，例如将请求头进行完善，增加 referer 和 host 参数，直接在需要修改的位置添加注释即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a770e2ee60a11dd5e2af601e4383985.gif" /></p><p></p><p></p><h4>第二步：编写目标数据采集函数</h4><p></p><p></p><p>这一步，我们要使用 lxml 模块中的 etree 提取目标标签，继续将我们的逻辑输入到注释注释中，然后回车。</p><p></p><blockquote>使用 etree 模块提取网页响应中所有的 article 标签</blockquote><p></p><p></p><p><code lang="text"># 使用 etree 模块提取网页响应中所有的 article 标签

from lxml import etree

html = etree.HTML(response.text)

article_tags = html.xpath('//article')

print(article_tags)
</code></p><p></p><p>此时目标博客标签已经被初步提取，下面要将标题和超链接地址进行再次提取，这里的注释需要尽可能编写清晰，如果你对 Python 爬虫技术栈有一定了解，到这里就会发现 CodeWhisperer 对代码编写提速效果。</p><p></p><p><code lang="text"># 循环 article_tags 提取其内部的超链接标签的 href 属性和 h4 标签文本
# 注意标签结构是 超链接 a 标签含后代 h4 标签
for article in article_tags:
    href = article.xpath('./a/@href')[0]
    title = article.xpath('./a//h4/text()')[0]
    print(href, title)
</code></p><p></p><h4>第三步：数据存入到 csv 文件中</h4><p></p><p></p><p>写入文件的逻辑属于常见操作，直接输入函数需求，相信 CodeWhisperer 会直接生成的，输入的参考注释如下：</p><p></p><blockquote>编写一个 csv 文件写入函数，其包含 2 个参数，分别是 title 和 href</blockquote><p></p><p></p><p><code lang="text"># 编写一个 csv 文件写入函数，其包含 2 个参数，分别是 title 和 href

def write_to_csv(title, href):
    with open('codewhisperper_demo.csv', 'a', encoding='utf-8') as f:
        f.write(title + ',' + href + '\n')</code></p><p>write_to_csv()&nbsp;函数调用放到上述循环中即可完成本步骤。再次运行代码，在爬虫目录会生成对应的文件，打开文件得到目标数据，效果图如下所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e8cd71556d94b2f927b26be4c94d45a.png" /></p><p></p><p>这里一个简单的爬虫写完了，但是我们的工作还没有完成，要继续优化这个程序。</p><p></p><h4>第四步：将采集到的数据，发送到 163 邮箱</h4><p></p><p></p><p>输入注释：# 编写一个邮件发送函数，将刚刚生成的文件 codewhisperper_demo.csv&nbsp;发送到指定邮箱，CodeWhisperer 瞬间就会帮助我们生成一个参考函数，实际效果如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7b9d0ddc1c846a5640a4176db301926.gif" /></p><p></p><p></p><blockquote>将上述代码中的账号和密码修改为自己真实数据，即可实现一键发送邮件。</blockquote><p></p><p>打开收件箱，可以看到刚刚的邮件已经发送成功，CodeWhisperer 给我们提供了一种邮件发送的实现，你可以再其基础上进行改造，以满足个性化的需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/965750f21ea6216147673ed3b1a92c95.png" /></p><p></p><p></p><h3>CodeWhisperer 使用总结</h3><p></p><p></p><p>在博客开篇，我提出了编码过程中几个比较头疼的问题，尤其是第二个，程序员在编码过程中总是切出去查找代码示例和文档手册，从而打断编码思路，而 CodeWhisperer 非常完美的解决了该问题，在编码的过程中，Amazon CodeWhisperer 自动提供编码建议，同意就使用，不同意就舍弃，真正实现了沉浸式编程。</p><p></p><p>除此之外，CodeWhisperer 还可以基于代码和注释生成新的业务代码，尤其当我们 Python 工程师接触一个新的模块时，他可以快速的产出示例代码，而且生成的代码与我们编码风格非常相似，甚至编码风格和命名规则都可以完美学习到。在编码代码过程中，CodeWhisperer 还会自动为我们的代码提供注释参考，让我们将更多精力投入到业务逻辑中。</p><p></p><p>将 CodeWhisperer 用起来吧，几分钟之后，你就会深刻的感受到编码效率的提升！</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Pqu5weV2ssFwYADv9plm</id>
            <title>Katalyst：字节跳动云原生成本优化实践</title>
            <link>https://www.infoq.cn/article/Pqu5weV2ssFwYADv9plm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Pqu5weV2ssFwYADv9plm</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 03:30:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 字节跳动, 云原生化改造, 资源利用效率, 混合部署
<br>
<br>
总结: 字节跳动通过云原生化改造和混合部署的方式提高了资源利用效率。他们通过将在线和离线服务同时运行在相同节点上，充分利用两者之间的互补特性，实现了更好的资源利用。这种混合部署的方式使得字节跳动能够二次销售在线未使用的资源，并利用离线工作负载填补超售资源，从而保持资源利用效率在较高水平。通过不断迭代混合部署系统，字节跳动解决了资源利用率低、资源波动较大等问题，实现了在离线统一调度的混合部署。他们的混合部署系统Katalyst通过统一的资源联邦、统一调度器和统一资源管理器等组件，实现了在离线一体化资源管理能力，提高了资源利用效率。 </div>
                        <hr>
                    
                    <p>从 2016 年起，<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"开始着手服务云原生化改造，截至今日，字节服务体系主要包含四类：传统微服务（大多是基于 Golang 的 RPC Web 服务）、推广搜服务（传统 C++ 服务）、机器学习和大数据服务以及各类存储服务。</p><p></p><p>在字节跳动，基础设施面临的是一个规模巨大且持续快速变化的业务场景，而云原生技术体系需要同时聚焦资源效率和研发效率。在资源效率上，云原生要解决的核心问题之一就是如何提高集群的资源利用效率。</p><p></p><p>以典型的在线服务的资源使用情况为例，下图深蓝色部分是业务实际使用的资源量，浅蓝色部分为业务提供的安全缓冲区。即使增加缓冲区，仍有很多资源处于业务已申请但未使用的状态，因此我们的优化重点是从架构的角度尽可能地利用这些未使用的资源。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7de9d838ebc3160891eab4d440222711.png" /></p><p></p><p>针对上述情况，字节跳动内部尝试过若干不同类型的资源治理方案，包括</p><p>资源运营：定期帮助业务跑资源利用情况并推动资源申请治理，问题是运维负担重且无法根治利用率问题；动态超售：在系统侧评估业务资源量并主动缩减配额，问题是超售策略不一定准确且可能导致挤兑风险；动态扩缩：问题是如果只针对在线服务扩缩，由于在线服务的流量波峰波谷类似，无法充分实现全天利用率提升。</p><p></p><p>最终我们决定采用混合部署，将在线和离线同时运行在相同节点，充分利用在线和离线资源之间的互补特性，实现更好的资源利用。我们期望达到如下图的效果，即二次销售在线未使用的资源，利用离线工作负载能够很好地填补这部分超售资源，实现资源利用效率在全天保持在较高水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/706083699b7b560667e235b7c9b7c488.png" /></p><p></p><p></p><h2>字节跳动混部发展历程</h2><p></p><p></p><p>随着字节跳动各业务云原生化的推进，我们根据不同阶段业务需求和技术特点，选择合适的混合部署方案，并在此过程中不断迭代我们的混部系统。</p><p></p><h4>2.1 阶段一：在离线分时混部</h4><p></p><p></p><p>第一个阶段主要进行在线和离线的分时混合部署。</p><p></p><p>对在线：在该阶段我们构建了在线服务弹性平台，用户可以根据业务指标配置横向伸缩规则；例如，凌晨时业务流量减少，业务主动缩减部分实例，系统将在实例缩容基础上进行资源 Bing Packing 从而腾出整机；对离线：在该阶段离线服务可获取到大量 spot 类型资源，由于其供应不稳定所以成本上享受一定折扣；同时对于在线来说，将未使用的资源卖给离线，可以在成本上获得一定返利。</p><p>该方案优势在于不需要采取复杂的单机侧隔离机制，技术实现难度较低；但同样存在一些问题，例如：</p><p>转化效率不高，bing packing 过程中会出现碎片等问题；离线使用体验可能也不好，当在线偶尔发生流量波动时，离线可能会被强制杀死，导致资源波动较强烈；对业务会造成实例变化，实际操作过程中业务通常会配置比较保守的弹性策略，导致资源提升上限较低。</p><p><img src="https://static001.geekbang.org/infoq/fc/fcfb033a758a4cc26fca557c62d18396.jpeg" /></p><p></p><h4>2.2 阶段二：Kubernetes/YARN 联合混部</h4><p></p><p></p><p>为解决上述问题，我们进入了第二个阶段，尝试将离线和在线真正跑在一台节点上。</p><p></p><p>由于在线部分早先已经基于 Kubernetes 进行了原生化改造，但大多数离线作业仍然基于 YARN 进行运行。为推进混合部署，我们在单机上引入第三方组件负责确定协调给在线和离线的资源量，并与 Kubelet 或 Node Manager 等单机组件打通；同时当在线和离线工作负载调度到节点上后，也由该协调组件异步更新这两种工作负载的资源分配。</p><p></p><p>该方案使得我们完成混部能力的储备积累，并验证可行性，但仍然存在一些问题：</p><p></p><p>两套系统异步执行，使得在离线容器只能旁路管控，存在 race；且中间环节资源损耗过多；对在离线负载的抽象简单，使得我们无法描述复杂 QoS 要求；在离线元数据割裂，使得极致的优化困难，无法实现全局调度优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/daa019815ae94d496d40130b63ed68ce.png" /></p><p></p><h4>2.3 阶段三：在离线统一调度混部</h4><p></p><p></p><p>为解决第二阶段的问题，在第三阶段我们彻底实现了在离线统一的混合部署。</p><p></p><p>通过对离线作业进行云原生化改造，我们使它们可以在同一个基础设施上进行调度和资源管理。该体系中，最上面是统一的资源联邦实现多集群资源管理，单集群中有中心的统一调度器和单机的统一资源管理器，它们协同工作，实现在离线一体化资源管理能力。</p><p></p><p>在该架构中，Katalyst 作为其中核心的资源管控层，负责实现单机侧实时的资源分配和预估，具有以下特点：</p><p></p><p>抽象标准化：在离线元数据打通，在 QoS 的抽象上更加复杂和丰富，更好地满足业务对性能的要求；管控同步化：在容器启动时下发管控策略，避免在启动后异步修正资源调整，同时支持策略的自由扩展；策略智能化：通过构建服务画像提前感知资源诉求，实现更智能的资源管控策略；运维自动化：通过一体化的交付，实现运维自动化和标准化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f520bfeeabbb430442bc2b0c9c5ccf3f.png" /></p><p></p><p></p><h2>Katalyst 系统介绍</h2><p></p><p></p><p>Katalyst 引申自英文单词 catalyst，本意为催化剂，首字母修改为 K，寓意该系统能够为所有运行在 Kubernetes 体系中的负载提供更加强劲的自动化资源管理能力。</p><p></p><h4>3.1 Katalyst 系统概览</h4><p></p><p></p><p>如下图所示，Katalyst 系统大致分为四层，从上到下依次包括：</p><p></p><p>最上层的标准 API，为用户抽象不同的 QoS 级别，提供丰富的资源表达能力；中心层则负责统一调度、资源推荐以及构建服务画像等基础能力；单机层包括自研的数据监控体系，以及负责资源实时分配和动态调整的资源分配器；最底层是字节定制的内核，通过增强内核的 patch 和底层隔离机制解决在离线跑时单机性能问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f02f9732fc6756c86abc306bcdc59f86.png" /></p><p></p><h4>3.2 抽象标准化：QoS Class</h4><p></p><p></p><p>Katalyst QoS 可以从宏观和微观两个视角进行解读。</p><p></p><p>宏观上，Katalyst 以 CPU 为主维度定义了标准的 QoS 级别；具体来说我们将 QoS 分为四类：独占型、共享型、回收型和为系统关键组件预留的系统型；</p><p></p><p>微观上，Katalyst 最终期望状态无论什么样的 workload，都能实现在相同节点上的并池运行，不需要通过硬切集群来隔离，实现更好的资源流量效率和资源利用效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a5b0bdb64f8f329dec9aeb9cea5d6a2.png" /></p><p></p><p>在 QoS 的基础上，Katalyst 同时也提供了丰富的扩展 Enhancement 来表达除 CPU 核心外其他的资源需求：</p><p></p><p>QoS Enhancement：扩展表达业务对于 NUMA / 网卡绑定、网卡带宽分配、IO Weight 等多维度的资源诉求；Pod Enhancement：扩展表达业务对于各类系统指标的敏感程度，比如 CPU 调度延迟对业务性能的影响；Node Enhancement：通过扩展原生的 TopologyPolicy 表示多个资源维度间微拓扑的组合诉求。</p><p></p><h4>3.3 管控同步化：QoS Resource Manager</h4><p></p><p></p><p>为在 Kubernetes 体系下实现同步管控的能力，我们有三种 hook 方式：CRI 层、OCI 层、Kubelet 层。最终 Katalyst 选择在 Kubelet 侧实现管控，即实现和原生的 Device Manager 同层级的 QoS Resource Manager，该方案的优势包括：</p><p></p><p>在 admit 阶段实现拦截，无需在后续步骤靠兜底措施来实现管控；与 Kubelet 进行元数据对接，将单机微观拓扑信息通过标准接口报告到节点 CRD，实现与调度器的对接；在此框架上，可以灵活实现可插拔的 plugin，满足定制化的管控需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/304d4a1935d9a16c90efb57daec859c0.png" /></p><p></p><h4>3.4 策略智能化：服务画像和资源预估</h4><p></p><p></p><p>通常，选择使用业务指标构建服务画像比较直观，例如服务 P99 延迟或者下游的错误率。但其也存在一些问题，比如相对系统指标而言，业务指标的获取通常更不容易；业务通常会集成多个的框架，他们生产的业务指标含义并不完全相同，如果强依赖这些指标，整个管控的实现就会变得非常复杂。</p><p></p><p>因此，我们希望最终的资源调控或服务画像是基于系统指标而非业务指标来实现，其中最关键的就是如何找到业务最关心的系统指标。</p><p></p><p>我们的做法是使用一套离线的 pipeline 去发现业务指标和系统指标之间的匹配。例如，对于图中服务来说，最核心的业务指标是 P99 调用延迟，通过分析发现与其相关度最高的系统指标是 CPU 调度延迟，我们会不断调整服务的资源供应量，尽可能地逼近它的目标 CPU 调度延迟。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdbc626f43d1105b487d7b1efe445828.png" /></p><p></p><p>在服务画像的基础上，Katalyst 针对 CPU、内存、磁盘和网络等方面提供了丰富的隔离机制，必要时还对内核进行了定制以提供更强的性能要求。然而对于不同的业务场景和类型，这些手段并不一定直接适用，因此需要强调的是，隔离更多是一种手段而不是目的，我们在承接业务的过程中，需要根据具体的需求和场景来选择不同的隔离方案。</p><p></p><h4>3.5 运维自动化：多维度动态配置管理</h4><p></p><p></p><p>尽管我们希望所有的资源都在一个资源池系统下，但是对于在大规模生产环境中，我们不可能把所有节点都放在一个集群里。此外，一个集群中可能同时有 CPU 与 GPU 的机器，虽然可以共享控制面，但在数据面上需要一定的隔离。在节点级别，我们也经常需要修改节点维度配置以进行灰度验证，导致在同一节点上运行不同服务的 SLO 存在差异。</p><p></p><p>为解决这些问题，我们需要在业务部署时，考虑节点的不同配置对服务的影响。为此，Katalyst 针对标准交付提供了动态配置管理的能力，通过自动化的方法评估不同节点的性能和配置，并根据这些结果来选择最适合该服务的节点。</p><p></p><p></p><h2>Katalyst&nbsp;应用与案例分析</h2><p></p><p></p><p>在本章节，我们将结合字节内部的案例分享一些最佳实践。</p><p></p><h4>4.1 利用率效果</h4><p></p><p></p><p>从<a href="https://xie.infoq.cn/article/bc2bb6c1e95d4b5e21868eba8?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> Katalyst</a>" 实施效果上来说，基于字节内部业务的实践，我们在季度周期内，资源都可以保持在相对较高的状态；在单个集群中，每天的各个时间段内资源利用率也呈现出比较稳定的分布；同时，集群中大部分机器利用率也比较集中，我们的混合部署系统在所有节点上运行都比较稳定。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84c1a8d4bd4ad7d8897d60466bc45619.png" /></p><p></p><p></p><h4>4.2 实践：离线无感接入</h4><p></p><p></p><p>在进入第三阶段后，我们需要对离线进行云原生化改造。改造方式主要有两种，一种是已经在 K8s 体系中的服务，我们将基于 Virtual Kubelet 的方式实现资源池的直接打通；另外一种 YARN 架构下的服务，如果直接基于 Kubernetes 体系对业务接入框架进行彻底的改造，这对于业务来说成本非常高，理论上会导致所有业务都滚动升级，这显然不是一个理想的状态。</p><p></p><p>为了解决这个问题，我们引用 Yodel 的胶水层，即业务接入仍然使用标准的 Yarn API，但在这个胶水层中，我们将与底层 K8s 语义对接，将用户对资源的请求抽象为像 Pod 或容器的描述。</p><p></p><p>这种方法使得我们在底层使用更成熟的 K8s 技术来管理资源，实现对离线的云原生化改造，同时又保证了业务的稳定性。</p><p></p><h4>4.3 实践：资源运营治理</h4><p></p><p></p><p>在混部过程中，我们需要对大数据和训练框架进行适配改造，做好各种重试、checkpoint 和分级，才能确保在我们将这些大数据和训练作业切到整个混部资源池之后，它们的使用体验不至于太差。</p><p></p><p>同时，在系统上我们需要具备完善的资源商品、业务分级、运营治理以及配额管理等方面的基础能力。如果运营做得不好，可能使得在某些高峰时段将利用率拉得很高，但在其他时段可能会出现较大的资源缺口，从而导致利用率无法达到预期。</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94a4c8c77b340339e587e653b4410ee7.png" /></p><p></p><h4>4.4 实践：极限资源效率提升</h4><p></p><p></p><p>在构建服务画像时，我们采用的是基于系统指标去做管控，但基于离线分析得到的静态系统指标无法实时跟上业务侧的变化，需要在一定时间周期内分析业务性能的变化来调整静态值。</p><p></p><p>为此，Katalyst 引入了模型来微调系统指标。例如，如果我们认为 CPU 调度延迟可能是 X 毫秒，过一段时间后，通过模型算出业务目标延迟可能是 Y 毫秒，我们就可以动态地调整该目标的值，以更好地评估业务性能。</p><p></p><p>以下图为例，完全使用静态的系统目标来进行调控，业务 P99 将处于剧烈波动状态，这意味着在非晚高峰时段，我们无法将业务资源使用压榨到更极致的状态，使其更接近业务在晚高峰时可承受的量；引入模型后，可以看到业务延迟会更加平稳，使得我们可以全天将业务的性能拉平到一个相对平稳的水平，获得资源的收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2bfd257f197b41abf22d8320ba6ff236.png" /></p><p></p><p></p><h4>4.5 实践：解决单机问题</h4><p></p><p></p><p>在混部推进的过程中，我们会不断遇到在线和离线各种性能问题和微拓扑管理的诉求。例如，最初所有机器都是基于 <a href="https://www.infoq.cn/article/2NV2wqX8CKrAt0kpcgbz?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">cgroup V1 </a>"进行管控，然而由于 V1 的结构会使得系统需要遍历很深的目录树，消耗大量内核态 CPU。</p><p></p><p>为了解决该问题，我们在将整个集群中的节点切换到 cgroup V2 架构，使得我们能够更加高效地进行资源隔离和监控。对于推广搜等服务来说，为追求更加极致的性能，我们需要在 Socket/NUMA 级别实现更加复杂的亲和与反亲和策略等等，这些更加高阶的资源管理需求，在 Katalyst 中都可以更好地实现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8508702627d2c779845323bf46a930f.png" /></p><p></p><h2>总结展望</h2><p></p><p></p><p>目前，Katalyst 已正式开源并发布 0.3.0 版本，后续将会持续投入更多精力进行迭代。社区将在资源隔离、流量画像、调度策略、弹性策略、异构设备管理等多方面进行能力建设和系统增强，欢迎大家关注、参与该项目并提供反馈意见。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9VZpK1cefI2chOMcZlr6</id>
            <title>字节跳动开源 Kelemetry：面向 Kubernetes 控制面的全局追踪系统</title>
            <link>https://www.infoq.cn/article/9VZpK1cefI2chOMcZlr6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9VZpK1cefI2chOMcZlr6</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:47:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Kelemetry, Kubernetes, 追踪系统, 可观测性
<br>
<br>
总结: Kelemetry是字节跳动开发的用于Kubernetes控制平面的追踪系统，它通过追踪单个Kubernetes对象的完整生命周期以及不同对象之间的相互影响，从全局视角串联起多个Kubernetes组件的行为。通过可视化K8s系统内的事件链路，Kelemetry使得Kubernetes系统更容易观测、更容易理解、更容易Debug。 </div>
                        <hr>
                    
                    <p>Kelemetry 是<a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"开发的用于 Kubernetes 控制平面的追踪系统，它从全局视角串联起多个 Kubernetes 组件的行为，追踪单个 <a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Kubernetes </a>"对象的完整生命周期以及不同对象之间的相互影响。</p><p></p><p>通过可视化 K8s 系统内的事件链路，它使得 Kubernetes 系统更容易观测、更容易理解、更容易 Debug。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db4d1992cabf04d9d5b71732aa916335.png" /></p><p></p><h2>背景</h2><p></p><p></p><p>在传统的分布式追踪中，“追踪”通常对应于用户请求期间的内部调用。特别是当用户请求到达时，追踪会从根跨度开始，然后每个内部 <a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA%3D%3D&amp;chksm=bdbec6368ac94f20a4bc7b7ca477b73dadb587d40cc4d30907e9a68f6c25d8dde1fe6a216337&amp;idx=2&amp;mid=2651012709&amp;scene=27&amp;sn=520d66662328a7b4dbb2c7a93e7f17ff&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">RPC</a>" 用会启动一个新的子跨度。由于父跨度的持续时间通常是其子跨度的超集，追踪可以直观地以树形或火焰图的形式观察，其中层次结构表示组件之间的依赖关系。</p><p></p><p>与传统的 RPC 系统相反，Kubernetes API 是异步和声明式的。为了执行操作，组件会更新 apiserver 上对象的规范（期望状态），然后其他组件会不断尝试自我纠正以达到期望的状态。</p><p></p><p>例如，当我们将 ReplicaSet 从 3 个副本扩展到 5 个副本时，我们会将 spec.replicas 字段更新为 5，rs controller 会观察到此更改，并不断创建新的 pod 对象，直到总数达到 5 个。当 kubelet 观察到其管理的节点创建了一个 pod 时，它会在其节点上生成与 pod 中的规范匹配的容器。</p><p></p><p>在此过程中，我们从未直接调用过 rs controller，rs controller 也从未直接调用过 kubelet。这意味着我们无法观察到组件之间的直接因果关系。如果在过程中删除了原始的 3 个 pod 中的一个，副本集控制器将与两个新的 pod 一起创建一个不同的 pod，我们无法将此创建与 ReplicaSet 的扩展或 pod 的删除关联起来。</p><p></p><p>因此，由于“追踪”或“跨度”的定义模糊不清，传统的基于跨度的分布式追踪模型在 Kubernetes 中几乎不适用。</p><p></p><p>过去，各个组件一直在实现自己的内部追踪，通常每个“reconcile”对应一个追踪（例如，kubelet 追踪只追踪处理单个 pod 创建/更新的同步操作）。然而，没有单一的追踪能够解释整个流程，这导致了可观察性的孤立岛，因为只有观察多个 reconcile 才能理解许多面向用户的行为；例如，扩展 ReplicaSet 的过程只能通过观察副本集控制器处理 ReplicaSet 更新或 pod 就绪更新的多个 reconcile 来推断。</p><p></p><p>为解决可观察性数据孤岛的问题，Kelemetry&nbsp;以组件无关、非侵入性的方式，收集并连接来自不同组件的信号，并以追踪的形式展示相关数据。</p><p></p><h2>设计</h2><p></p><p></p><h4>1.将对象作为跨度</h4><p></p><p></p><p>为了连接不同组件的可观察性数据，Kelemetry 采用了一种不同的方法，受 kspan 项目的启发，与将单个操作作为根跨度的尝试不同，这里为对象本身创建一个跨度，而每个在对象上发生的事件都是一个子跨度。此外，各个对象通过它们的拥有关系连接在一起，使得子对象的跨度成为父对象的子跨度。</p><p></p><p>基于此，我们得到了两个维度：树形层次结构表示对象层次结构和事件范围，而时间线表示事件顺序，通常与因果关系一致。</p><p></p><p>例如，当我们创建一个单 pod 部署时，deployment controller、rs controller 和 kubelet 之间的交互可以使用审计日志和事件的数据在单个追踪中显示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8d64999b5f740434013fb8d12cb0551.png" /></p><p></p><p>追踪通常用于追踪持续几秒钟的短暂请求，所以追踪存储实现可能不支持具有长生命周期或包含太多跨度的追踪；包含过多跨度的追踪可能导致某些存储后端的性能问题。因此，我们通过将每个事件分到其所属的半小时时间段中，将每个追踪的持续时间限制为 30 分钟。例如，发生在 12:56 的事件将被分组到 12:30-13:00 的对象跨度中。</p><p></p><p>我们使用分布式 KV 存储来存储（集群、资源类型、命名空间、名称、字段、半小时时间戳）到相应对象创建的追踪/跨度 ID 的映射，以确保每个对象只创建一个追踪。</p><p></p><h4>2.审计日志收集</h4><p></p><p></p><p>Kelemetry 的主要数据源之一是 apiserver 的审计日志。审计日志提供了关于每个控制器操作的丰富信息，包括发起操作的客户端、涉及的对象、从接收请求到完成的准确持续时间等。在 Kubernetes 架构中，每个对象的更改会触发其相关的控制器进行协调，并导致后续对象的更改，因此观察与对象更改相关的审计日志有助于理解一系列事件中控制器之间的交互。</p><p></p><p>Kubernetes apiserver 的审计日志以两种不同的方式暴露：日志文件和&nbsp;webhook。一些云提供商实现了自己的审计日志收集方式，而在社区中配置审计日志收集的与厂商无关的方法进展甚微。为了简化自助提供的集群的部署过程，Kelemetry 提供了一个审计 webhook，用于接收原生的审计信息，也暴露了插件 API 以实现从特定厂商的消息队列中消费审计日志。</p><p></p><h4>3.Event 收集</h4><p></p><p></p><p>当 Kubernetes 控制器处理对象时，它们会发出与对象关联的“event”。当用户运行 kubectl describe 命令时，这些 event 会显示出来，通常提供了控制器处理过程的更友好的描述。例如，当调度器无法调度一个 pod 时，它会发出一个 FailToSchedulePod 事件，其中包含详细的消息：</p><p></p><p></p><blockquote>0/4022 nodes are available to run pod xxxxx: 1072 Insufficient memory, 1819 Insufficient cpu, 1930 node(s) didn't match node selector, 71 node(s) had taint {xxxxx}, that the pod didn't tolerate.</blockquote><p></p><p></p><p>由于 event 针对用于 kubectl describe 命令优化，它们并不保留每个原始事件，而是存储了最后一次记录事件的时间戳和次数。另一方面，Kelemetry 使用 Kubernetes 中的对象列表观察 API 检索事件，而该 API 仅公开 event 对象的最新版本。为了避免重复事件，Kelemetry 使用了几种启发式方法来“猜测”是否应将 event 报告为一个跨度：</p><p></p><p>持久化处理的最后一个 event 的时间戳，并在重启后忽略该时间戳之前的事件。虽然事件的接收顺序不一定有保证（由于客户端时钟偏差、控制器 — apiserver — etcd 往返的不一致延迟等原因），但这种延迟相对较小，可以消除由于控制器重启导致的大多数重复。验证 event 的 resourceVersion 是否发生了变化，避免由于重列导致的重复event。</p><p></p><h4>4.将对象状态与审计日志关联</h4><p></p><p></p><p>在研究审计日志进行故障排除时，我们最想知道的是“此请求改变了什么”，而不是“谁发起了此请求”，尤其是当各个组件的语义不清楚时。Kelemetry 运行一个控制器来监视对象的创建、更新和删除事件，并在接收到审计事件时将其与审计跨度关联起来。当 Kubernetes 对象被更新时，它的 resourceVersion 字段会更新为一个新的唯一值。这个值可以用来关联更新对应的审计日志。Kelemetry 把对象每个 resourceVersion 的 diff 和快照缓存在分布式 KV 存储中，以便稍后从审计消费者中链接，从而使每个审计日志跨度包含控制器更改的字段。</p><p></p><p>追踪 resourceVersion 还有助于识别控制器之间的 409 冲突。当客户端传递 UPDATE 请求的 resourceVersion 过旧，且其他请求是将 resourceVersion 更改时，就会发生冲突请求。Kelemetry 能够将具有相同旧资源版本的多个审计日志组合在一起，以显示与其后续冲突相关的审计请求作为相关的子跨度。</p><p>为了确保无缝可用性，该控制器使用多主选举机制，允许控制器的多个副本同时监视同一集群，以确保在控制器重新启动时不会丢失任何事件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26f1cf7e0f78030b03886254eb61bd8f.png" /></p><p></p><h4>5.前端追踪转换</h4><p></p><p></p><p>在传统的追踪中，跨度总是在同一个进程（通常是同一个函数）中开始和结束。因此，OTLP 等追踪协议不支持在跨度完成后对其进行修改。不幸的是，Kelemetry 不是这种情况，因为对象不是运行中的函数，并且没有专门用于启动或停止其跨度的进程。相反，Kelemetry 在创建后立即确定对象跨度，并将其他数据写入子跨度， 是以每个审计日志和事件都是一个子跨度而不是对象跨度上的日志。</p><p></p><p>然而，由于审计日志的结束时间/持续时间通常没有什么价值，因此追踪视图非常丑陋且空间效率低下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6d756f06651a91611b69ac2358ba451.png" /></p><p></p><p>为了提高用户体验，Kelemetry 拦截在 Jaeger 查询前端和存储后端之间，将存储后端结果返回给查询前端之前，对存储后端结果执行自定义转换流水线。</p><p></p><p>Kelemetry 目前支持 4 种转换流水线：</p><p></p><p>tree：服务名/操作名等字段名简化后的原始 trace 树timeline：修剪所有嵌套的伪跨度，将所有事件跨度放在根跨度下，有效地提供审计日志tracing：非对象跨度被展平为相关对象的跨度日志</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ed7ff44abbbd184f84df33edd6b7051.png" /></p><p></p><p>分组：在追踪管道输出之上，为每个数据源（审计/事件）创建一个新的伪跨度。当多个组件将它们的跨度发送到 Kelemetry 时，组件所有者可以专注于自己组件的日志并轻松地交叉检查其他组件的日志。</p><p></p><p>用户可以在追踪搜索时通过设置“service name“来选择转换流水线。中间存储插件为每个追踪搜索结果生成一个新的“CacheID”，并将其与实际 TraceID 和转换管道一起存储到缓存 KV 中。当用户查看时，他们传递 CacheID，CacheID 由中间存储插件转换为实际 TraceID，并执行与 CacheID 关联的转换管道。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/4907b57d081885c707ca102ddb53d2ed.png" /></p><p></p><h4>6.突破时长限制</h4><p></p><p></p><p>如上所述，追踪不能无限增长，因为它可能会导致某些存储后端出现问题。相反，我们每 30 分钟开始一个新的追踪。这会导致用户体验混乱，因为在 12:28 开始滚动的部署追踪会在 12:30 突然终止，用户必须在 12:30 手动跳转到下一个追踪才能继续查看追踪。</p><p></p><p>为了避免这种认知开销，Kelemetry 存储插件在搜索追踪时识别具有相同对象标签的跨度，并将它们与相同的缓存 ID 以及用户指定的搜索时间范围一起存储。在渲染 span 时，所有相关的轨迹都合并在一起，具有相同对象标签的对象 span 被删除重复，它们的子对象被合并。轨迹搜索时间范围成为轨迹的剪切范围，将对象组的完整故事显示为单个轨迹。</p><p></p><h4>7.多集群支持</h4><p></p><p></p><p>可以部署 Kelemetry 来监视来自多个集群的事件。在字节跳动，Kelemetry 每天创建 80 亿个跨度（不包括伪跨度；使用多 raft 缓存后端而不是 etcd）。对象可以链接到来自不同集群的父对象，以启用对跨集群组件的追踪。</p><p></p><h2>未来增强</h2><p></p><p></p><h4>1.采用自定义追踪源</h4><p></p><p></p><p>为了真正连接 K8s 生态系统中的所有观测点，审计和事件并不足够全面。Kelemetry 将从现有组件收集追踪，并将其集成到 Kelemetry 追踪系统中，以提供对整个系统的统一和专业化视图。</p><p></p><h4>2.批量分析</h4><p></p><p></p><p>通过 Kelemetry 的聚合追踪，回答诸如“从部署升级到首次拉取镜像的进展需要多长时间”等问题变得更加容易，但我们仍然缺乏在大规模上聚合这些指标以提供整体性能洞察的能力。通过每隔半小时分析 Kelemetry 的追踪输出，我们可以识别一系列跨度中的模式，并将其关联为不同的场景。</p><p></p><h2>使用案例</h2><p></p><p></p><h4>1. replicaset controller 异常</h4><p></p><p></p><p>用户报告，一个 deployment 不断创建新的 Pod。我们可以通过 deployment 名称快速查找其 Kelemetry 追踪，分析 replicaset 与其创建的 Pod 之间的关系。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c97bb74029ca6a6da856f1ae10da9d1.png" /></p><p></p><p>从追踪可见几个关键点：</p><p>Replicaset-controller 发出&nbsp;SuccessfulCreate&nbsp;事件，表示 Pod 创建请求成功返回，并在 replicaset reconcile 中得到了 replicaset controller 的确认。没有&nbsp;replicaset&nbsp;状态更新事件，这意味着&nbsp;replicaset controller&nbsp;中的&nbsp;Pod reconcile&nbsp;未能更新&nbsp;replicaset&nbsp;状态或未观察到这些&nbsp;Pod。</p><p></p><p>此外，查看其中一个 Pod 的追踪：</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/681b330669ecbef35076a4bd8ae49625.png" /></p><p></p><p>Replicaset controller 在 Pod 创建后再也没有与该 Pod 进行交互，甚至没有失败的更新请求。</p><p></p><p>因此，我们可以得出结论，replicaset controller 中的 Pod 缓存很可能与 apiserver 上的实际 Pod 存储不一致，我们应该考虑 pod informer 的性能或一致性问题。如果没有 Kelemetry，定位此问题将涉及查看多个 apiserver 实例的各个 Pod 的审计日志。</p><p></p><h4>2.浮动的 minReadySeconds</h4><p></p><p></p><p>用户发现 deployment 的滚动更新非常缓慢，从 14:00 到 18:00 花费了几个小时。如果不使用 Kelemetry，通过使用 kubectl 查找对象，发现 minReadySeconds 字段设置为 10，所以长时间的滚动更新时间是不符合预期的。kube-controller-manager 的日志显示，在一个小时后 Pod 才变为 Ready 状态</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d1c4dff024fe46343fe4da131bf0b5a.png" /></p><p></p><p>进一步查看 kube-controller-manager 的日志后发现，在某个时刻 minReadySeconds 的值为 3600。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b77d4ed7afdf84b068a5a25d633b157.png" /></p><p></p><p>如果使用 Kelemetry 进行调试，我们可以直接通过 deployment 名称查找追踪，并发现 federation 组件增加了 minReadySeconds 的值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a80f8e4fb2b8e8abfec3b66384c46af.png" /></p><p></p><p>后来，deployment controller 将该值恢复为 10。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2df40bee17f5a6535ff8fc48c904b19.png" /></p><p></p><p>因此，我们可以得出结论，问题是由用户在滚动更新过程中临时注入的较大 minReadySeconds 值引起的。通过检视对象 diff ，可以轻松识别由非预期中间状态引起的问题。</p><p></p><h2>尝试 Kelemetry</h2><p></p><p></p><p>目前，Kelemetry 已在 GitHub 上开源：github.com/kubewharf/kelemetry</p><p></p><p>感兴趣的开发者可以按照 docs/QUICK_START.md 快速入门指南，亲自体验 Kelemetry 如何与组件进行交互。如果你不想设置一个集群，也可以查看从 GitHub CI 流水线构建的在线预览：kubewharf.io/kelemetry/trace-deployment/。</p><p></p><p>我们期待有更多朋友关注与加入社区，如果大家在试用过程中发现了一些问题，也欢迎大家提出 issue 给我们反馈！</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd9e76b489cb86e1f34493f5f83f2f4b.jpeg" /></p><p>扫描二维码，添加小助手</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8WCa8vc3wjxuod6gXhxh</id>
            <title>字节跳动开源 Katalyst：在离线混部调度，成本优化升级</title>
            <link>https://www.infoq.cn/article/8WCa8vc3wjxuod6gXhxh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8WCa8vc3wjxuod6gXhxh</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Dec 2023 02:36:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网应用, 资源使用情况, 潮汐特性, 削峰填谷
<br>
<br>
总结: 当下互联网应用的资源使用情况具有潮汐特性，为了节约成本和保证业务稳定性，可以通过削峰填谷的方式，将闲置资源出让给优先级低的服务，并在需要时及时归还。 </div>
                        <hr>
                    
                    <p>当下互联网应用以天为单位，在线业务的资源使用情况往往会随着访问数量的波动而变化，具备明显的潮汐特性。为了确保业务稳定性，业务方往往会参考高峰时段的资源使用情况来申请资源，但这部分资源在低峰时段容易被闲置。</p><p></p><p>如果可以把这些闲置资源暂时出让给优先级低的服务，当在线业务需要使用的时候及时将资源归还，形成在离线服务混部，就可以达到削峰填谷，节约成本的效果。</p><p></p><p></p><h2>字节跳动云原生混部实践</h2><p></p><p></p><p><a href="https://www.infoq.cn/article/AsgjeVRM8isLSzo7IXzh?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">字节跳动</a>"业务规模庞大、业务类型多元，其中涵盖了包括微服务、推广搜服务、机器学习与大数据、存储在内的多种业务类型。通常来说，不同业务类型对底层基础设施会有不同的资源管理诉求，传统的管理模式是基于业务线或者服务类型切分资源池，实现定制化需求。</p><p></p><p>但切分资源池的做法容易形成资源孤岛，无法实现资源层面的灵活拆借，不利于全局资源利用效率的提升和业务成本的优化，加重集群运维的负担。</p><p></p><p>此外，由于不同类型业务的 SLO 要求、资源潮汐特性存在互补，基础设施团队期望充分利用这些特性，通过调度和管控等手段去优化资源效率，实现资源池的融合统一，帮助业务团队获得更低的资源成本和更强的弹性能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eacec4cca090bbb206becadfb88aeaac.png" /></p><p></p><p>为实现资源统一托管，字节跳动从 2016 年就开始基于<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect"> Kubernetes </a>"构建统一的基础设施。</p><p></p><p>到现阶段，字节内部已经基本完成全量微服务、推广搜服务以及大部分机器学习与大数据业务的云原生化改造。在此过程中，基础架构团队持续探索统一资源池下的资源优化手段，并逐渐形成了 “弹性伸缩” 和 “常态混部” 互相配合的资源池混部方案。</p><p></p><p>弹性伸缩：实现机器级别、<a href="https://mp.weixin.qq.com/s?__biz=MzIwMzY1OTU1NQ%3D%3D&amp;chksm=96ceb6bca1b93faa5de5fcc563ec907c7b14c920809f6ff563e1696cd3e857c6161776352915&amp;idx=2&amp;mid=2247497456&amp;scene=27&amp;sn=cac27bb9cfb119a5225371886a296147&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">Numa</a>" 级别的资源分时复用，结合业务指标和系统指标，共同指导业务实例的横向和纵向扩缩容策略，最终使得离线类服务以更加低廉的价格购买更多闲时资源，在线类服务以更加高昂的价格购买更多峰时资源，通过资源市场化运营的方式实现综合效率的提升。混合部署：提供资源超卖的能力，充分利用集群中 “已经售卖但未充分使用的资源” 部署更多低优业务，同时在系统侧完善 CPU、内存、磁盘、网络等多维度的资源隔离机制，并且智能预测、感知各类服务的负载变化，结合服务的分级机制，通过分钟级的指标感知和调控策略，保证服务的稳定性。</p><p>该方案在链路上基于 Kubernetes 和 Yarn 两套体系实现联合管控，在单机上同时运行 Kubernetes 和&nbsp;Yarn&nbsp;的管控组件，配合中心协调组件对两套系统可见的资源量进行分配。在联合管控系统之上，团队基于服务资源画像实现实时的资源预估，在保证各类服务 SLA 要求的前提下，实现更加灵活和动态的资源分配。</p><p></p><p>在该资源池混部方案落地实践的过程中，基础设施团队完成了资源并池可行性的验证，完成了混部基础能力的构建，并且在部分核心业务集群实现了整机天级利用率从&nbsp;23%&nbsp;到&nbsp;60%&nbsp;的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/103c99cf7f723241327e2e0e28c69ca5.jpeg" /></p><p></p><p></p><h2>Katalyst：从内部验证到开源</h2><p></p><p></p><p>在经历内部抖音、今日头条等大规模潮汐流量业务验证后，字节跳动的云原生混部实践已日臻完善。</p><p></p><p>为了帮助更多人了解大规模资源混部实践的工作原理，方便更多开发者用户体验这种开箱即用、一键式部署的资源管控能力，研发团队决定回馈社区，采用 Kubernetes Native 的方式重构并增强了资源管控系统的实现，提炼出资源管控系统&nbsp;Katalyst&nbsp;并正式开源。</p><p></p><p>Katalyst 引申自单词 catalyst (音&nbsp;[ˈkætəlɪst])，本意为催化剂。首字母修改为 K，寓意该系统能够为所有运行在 Kubernetes 体系中的负载提供更加强劲的自动化资源管理能力。</p><p></p><h4>什么是 Katalyst</h4><p></p><p></p><p>Katalyst 脱胎于字节跳动混部技术实践，同时也从资源、管控、调度等多个维度对资源管控能力进行了扩展和补充。它的主要特点包括：</p><p></p><p>完全孵化于超大规模混部实践，并在字节服务云原生化的进程中同步接管资源管控链路，真正实现内外技术体系的复用搭载字节跳动内部的 Kubernetes 发行版 Enhanced Kubernetes&nbsp;同步开源，兼容性好，体验更多字节自研的核心功能系统基于插件化模式构建，用户可以在 Katalyst Framework 之上自定制各类调度、管控、策略、数据等模块插件提供一键式部署模版和详尽运维手册，降低外部用户的理解和接入使用成本</p><p></p><h4>Katalyst 如何实现资源抽象</h4><p></p><p></p><p>在资源层，Kubernetes 原生 QoS 分级无法满足大规模生产环境的要求，Katalyst 在此基础上进行了进一步的抽象。</p><p></p><p>Katalyst 以 CPU 为主维度，为应用提供了 system_core 系统核、dedicated_core 独占核、shared_core 共享核、reclaimed_core 回收核等多种不同等级，同时每种等级又辅助以多种 enhancement 机制（例如是否需要 numa node 绑定，是否需要网卡亲和或者带宽限制等），实现差异化的资源分配和管控策略。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f71230f305f46d0df4e67c511a34d8ae.png" /></p><p></p><p>通过抽象资源模型，Katalyst 为用户提供了统一的资源入口，用户根据实际业务需求，将业务服务映射到对应的 QoS 和售卖模式上准确地表达自身的需求，最终实现从统一的资源池获取资源而不用关注底层资源池细节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bda96be8e6b52a9ac1aca992f8b3de17.jpeg" /></p><p></p><p></p><h4>Katalyst&nbsp;的架构设计</h4><p></p><p></p><p>早期的混部架构存在几方面的问题：Kubernetes 和 Yarn 两套系统的联合管控虽然实现了在离线业务的常态混部，但是复杂的系统也使得维护成本变高。</p><p></p><p>另外这种架构也带来了额外的资源损耗，这些损耗一方面来自于联合管控模式下单机 Agent 组件资源占用，尤其在超大规模的集群中，这部分资源非常可观。此外，由于两套管控导致系统复杂度变高，系统交互过程中会产生多级资源漏斗，任何环节的异常都会导致资源丢失。</p><p></p><p>在 Katalyst 中，我们对整体的混部架构做了优化重构：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5550deec101cf3a61bdafd496e1d274d.png" /></p><p></p><p>在管控层，我们将字节早期基于 Kubernetes 和 Yarn 两套体系的融合系统整合成一套基于 Kubernetes 的系统。</p><p></p><p>具体来说，我们在接入层同时保留了 Kubernetes 以及 Yarn 各自的 API 入口，底层系统的元数据管理和资源管控实现则统一收敛到基于 Kubernetes 的管控系统 Katalyst 上。</p><p></p><p>在调度层， Katalyst 在统一元数据的基础上实现了 “中心” 和 “单机” 互相协调的资源调度机制。</p><p></p><p>在单机调度侧：Katalyst 搭载 Enhanced Kubernetes 里的扩展模块 QoS Resource Manager (QRM) 能够实现可插件化的微拓扑亲和性分配，并通过自定 CRD 将微拓扑上报到中心打通调度流程；在服务运行过程中，Katalyst 会持续观察 Pod 运行时系统指标，集合业务 QoS 要求和业务指标反馈进行预估，决策出 Pod 在各个资源维度上的分配量，通过 QRM reconcile 机制实时下发到 CRI。上述过程中的资源预估模型和 QRM 实现，都可以通过插件化的方式定制，使得资源调控的策略更加匹配不同业务场景的诉求。</p><p></p><p>在中心调度侧：Katalyst 基于原生 Scheduler Framework 扩展了更加丰富的调度能力，在调度过程中同时考虑不同 QoS 业务在同一个集群中运行时资源层应该如何分配及协作，配合单机调度实现更加细粒度的调度语义要求；同时，中心调度还会结合业务容器运行时的实时数据和服务画像，在全集群范围内实现动态的 Rebalance 策略，降低集群空置率，提升业务稳定性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d10908b4dd2ac433f598eb11c15c9012.jpeg" /></p><p></p><p>通过收敛资源管控，Katalyst 有效减少了系统运行中的资源损耗。此外，在联合管控体系下，Katalyst 将旁路异步管控的实现切换成基于标准 CRI 接口的同步管控实现，避免因为各种异步引发的 race 或者冲突，从而引发服务性能抖动。</p><p></p><p>最后，在一套管控体系下，我们还能够充分享受 Kubernetes 面向 API 设计的优势，通过自定义 CRD 的方式去解耦内部系统，泛化管控策略，使得系统能够通过插件化的方式更加灵活地系统迭代，真正实现了内外同源。</p><p></p><p></p><h2>RoadMap</h2><p></p><p></p><p>Katalyst 作为一个资源管理系统，在离线混部是其核心应用场景之一。除了抽象上述核心概念之外，我们还为 Katalyst 提供和规划了丰富的 QoS 能力：</p><p></p><p>精细化的资源出让策略：Katalyst 支持基于静态启发式、无监督算法、QoS Aware 的多种资源预估策略，更准确的计算和预测节点可出让资源量，进一步提高资源利用率。多维度的资源隔离能力：基于 cgroup, rdt, iocost, tc 等能力，实现不同混部场景中对 cpu，内存，磁盘，网络等多种资源的有效隔离，保障在线业务的 QoS 不受影响。多层级的负载驱逐策略：支持基于多种指标，多层级的驱逐策略，在保障在线业务 QoS 的同时也尽可能提高离线业务的 QoS。</p><p>除了混部场景，Katalyst 也提供一些增强的资源管理能力：</p><p>资源弹性管理：提供灵活可扩展的 HPA/VPA 资源弹性策略，帮助用户提高部署率和资源利用率。微拓扑感知调度：感知节点上 CPU，内存，异构设备的微拓扑，基于更细粒度的微拓扑信息完成资源分配，满足高性能业务的 QoS 要求。</p><p></p><p>详细的功能规划请参考 &nbsp;roadmap                                       （https://github.com/kubewharf/katalyst-core/blob/main/docs/roadmap.md）。</p><p></p><p>虽然混部技术在字节内部已经经历了几次的技术迭代，但是一个通用、标准化的平台底座必然要经过各种场景的打磨，我们非常期待更多朋友加入到 Katalyst 开源社区中！</p><p></p><p>项目地址：github.com/kubewharf/katalyst-core</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c31da82a63f8db5f4f010fbdca49637b.jpeg" /></p><p>扫描二维码加入群聊</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>