<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/moZPV5BW511vDSYf3U0b</id>
            <title>将AI部署成本降低8倍！Yandex 发布LLM极限压缩方法：Llama 2 只需1个GPU 即可运行</title>
            <link>https://www.infoq.cn/article/moZPV5BW511vDSYf3U0b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/moZPV5BW511vDSYf3U0b</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jul 2024 06:19:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型语言模型, 压缩方法, AQLM, PV-Tuning
<br>
<br>
总结: 本文介绍了在消费级硬件上部署大型语言模型所面临的挑战，以及研究人员开发的两种压缩方法——AQLM和PV-Tuning。AQLM通过减少每个模型参数的位数并在极限压缩场景下保持模型准确性，而PV-Tuning是一种表示无关的框架，提供了收敛保证并在高性能模型的向量量化中表现优异。结合使用这两种方法可以在有限的计算资源上实现紧凑的模型，并为开发人员和研究人员提供了资源节约和新的使用场景。 </div>
                        <hr>
                    
                    <p>部署大型语言模型（LLM）在消费级硬件上是一个巨大的挑战，因为模型大小和计算效率之间存在固有的权衡。量化等压缩方法提供了部分解决方案，但通常会牺牲模型性能。</p><p>&nbsp;</p><p>为应对这一挑战，近日Yandex Research、IST Austria、KAUST和Neural Magic的研究人员联合开发了两种压缩方法——加性量化语言模型（AQLM）和PV-Tuning。</p><p>&nbsp;</p><p>AQLM 将每个模型参数的位数减少到2 - 3位，同时在极限压缩场景下保持甚至增强模型准确性。其关键创新包括对权重矩阵的学习加性量化，适应输入变异性，并在层块之间联合优化代码簿参数。这一双重策略使AQLM在压缩技术领域设立了新的基准。</p><p>&nbsp;</p><p>AQLM的实用性通过其在GPU和CPU架构上的实现得到了验证，使其适用于现实应用。比较分析显示，AQLM可以在不影响模型性能的情况下实现极限压缩，如其在零样本任务中的模型困惑度和准确性指标上显示的优异结果所示。</p><p>&nbsp;</p><p>PV-Tuning是一种表示无关的框架（a representation-agnostic framework），它概括并改进了现有的微调策略，解决模型压缩过程中可能出现的误差问题。PV-Tuning在受限情况下提供了收敛保证（convergence guarantees），并且在高性能模型（如Llama和Mistral）的1-2位向量量化中表现优于以前的方法。通过利用PV-Tuning，研究人员实现了第一个针对Llama 2模型的2位参数的帕累托最优量化。</p><p>&nbsp;</p><p>当AQLM和PV-Tuning结合使用时，可以实现最佳效果——紧凑的模型即使在有限的计算资源上也能提供高质量的响应。</p><p>&nbsp;</p><p>这些方法的有效性通过对流行的开源模型如LLama 2、Mistral和Mixtral的严格评估得到了验证。研究人员压缩了这些大型语言模型，并根据英语基准测试WikiText2和C4评估了答案质量。即使模型被压缩到了12.5%时，它们的答案质量仍保持在95%。</p><p>&nbsp;</p><p></p><p>&nbsp;</p><p>*测试中答案的平均准确度越接近原始模型，新方法在保持答案质量方面就越好。上述图表显示了这两种方法的综合结果，它们将模型压缩了平均约为8倍。</p><p>&nbsp;</p><p>据介绍，新方法也为开发和部署专有语言模型和开源LLM的公司提供了巨大的资源节约。例如，压缩后的130亿参数的Llama 2模型只需1个GPU即可运行，相比之下，原模型需要4个GPU，从而使硬件成本降低最高达8倍。此举使得初创公司、个人研究者和LLM爱好者能够在他们的日常计算机上运行先进的LLM，譬如Llama。</p><p>&nbsp;</p><p>AQLM和PV-Tuning使得在计算资源有限的设备上离线部署模型成为可能，为智能手机、智能音箱及更多设备开辟了新的使用场景。用户可以在这些设备上使用文本和图像生成、语音助手、个性化推荐甚至实时语言翻译等功能，而无需联网。</p><p>&nbsp;</p><p>此外，使用这些方法压缩的模型能够以快达4倍的速度运行，因为它们需要的计算量减少了。</p><p>&nbsp;</p><p>目前，全球的开发人员和研究人员现在可以在<a href="https://github.com/Vahe1994/AQLM">GitHub</a>"上使用AQLM和PV-Tuning。作者提供的<a href="https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/aqlm_2bit_training.ipynb">演示材料</a>"为有效训练各种应用的压缩LLM提供了指导。此外，开发人员还可以下载已经使用这些方法压缩的<a href="https://huggingface.co/collections/ISTA-DASLab/aqlm-65e8dc75b908c7d73ec35598">流行开源模型</a>"。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zQtaJS4qNWvGauAvvZmE</id>
            <title>新晋开源顶流模型 Llama 3.1 被开发者赞爆！小扎拿苹果“开刀”反对闭源厂商：AI 不要“苹果税”！</title>
            <link>https://www.infoq.cn/article/zQtaJS4qNWvGauAvvZmE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zQtaJS4qNWvGauAvvZmE</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jul 2024 06:10:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源模型, Llama 3.1, Meta, 大语言模型
<br>
<br>
总结: Meta发布了开源模型Llama 3.1，该模型具有超强的灵活性和功能，与闭源模型相媲美。这一模型将帮助AI社区解锁新的工作流程，包括合成数据生成与模型蒸馏。Meta还发布了升级版本的其他模型，并允许开发人员使用这些模型的输出以改进其他模型。通过Llama 3.1 405B，Meta希望提高模型的适用性、响应质量和安全性。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p>“如今，几家科技公司正在开发领先的闭源模型，但开源正在迅速缩小差距。”Meta首席执行官马克·扎克伯格说道，“今年，Llama 3 与最先进的模型相媲美，并在某些领域处于领先地位。”</p><p>&nbsp;</p><p>给扎克伯格底气的就是刚刚正式发布的第一个前沿级开源模型Llama 3.1。这套最新模型的上下文长度扩展至128K，新增对8种语言的支持。其中，Llama 3.1 405 B具有超强的灵活性、控制力和功能，“足以与最强大的闭源模型相媲美。”这套新模型将帮助AI社区解锁新的工作流程，例如合成数据生成与模型蒸馏。</p><p>&nbsp;</p><p>作为新版本的一部分，Meta还为此前的8B及70B模型发布了升级版本。Meta还对模型许可证进行了更改，允许开发人员使用Llama模型（包括405B在内）的输出以改进其他模型。为了履行开源承诺，，Meta宣布从即日起将这些模型开放给整个社区：</p><p>&nbsp;</p><p>Meta Llama：<a href="https://llama.meta.com/">https://llama.meta.com/</a>"</p><p>Hugging Face：<a href="https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f">https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f</a>"</p><p>&nbsp;</p><p>“405 B 模型的发布，是第一次所有人都可以访问和构建具有前沿能力的大语言模型。该模型似乎达到了 GPT-4/Claude 3.5 Sonnet 的水平，其权重是开放的，并具有许可证，包括商业用途、合成数据生成、蒸馏和微调。这是 Meta 发布的第一个真正的、开放的、具有前沿能力的大语言模型。”OpenAI 创始成员、前研究科学家 Andrej Karpathy 评价道。</p><p>&nbsp;</p><p>“Llama 3.1 405B 可与最好的 GPT 4o 和 Claude Sonnet 3.5 直接竞争。现在Meta可以同时拥有性能和 Dota 主权。”Fossil CEO兼创始人Tim Kellogg说道。</p><p>&nbsp;</p><p>Meta表示，最新一代Llama将激发出新的应用程序与建模范式，包括合成数据生成，以用于改进和训练小体量模型；此外还包含模型蒸馏功能，这也填充了开源领域大规模蒸馏功能的空白。</p><p>&nbsp;</p><p></p><h2>备受称赞的Llama 3.1 405B</h2><p></p><p>&nbsp;</p><p>作为Meta旗下迄今为止最大的模型，在超过15万亿个token上训练Llama 3.1 405B是一项重大挑战。为了能够以这种规模开展训练，并在合理的时间内取得成果，Meta显著优化了整个训练栈，并将实际模型训练任务交给超过1.6万张H100 GPU。这也使得405 B成为首个以这种规模训练的Llama模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f10112526dd67d96b96a7c5b7abb2865.png" /></p><p></p><p>&nbsp;</p><p>为了解决现实难题，Meta在设计层面做出权衡，着力保持模型开发过程的可扩展性与简单性。</p><p>&nbsp;</p><p>Meta选择了标准的纯解码器transformer模型架构，同时做出微调，以此替代市面上常见的混合专家模型，希望借此最大限度提高训练稳定性。Meta采用了迭代后训练程序，其中每个轮次都采用监督微调加直接偏好优化的方法。这使得Meta能够为每个轮次创建出最高质量的合成数据，并提高每功能的实际性能。</p><p>&nbsp;</p><p>相较于此前已经亮相的各Llama版本，Meta改进了训练前与训练后所使用的数据规模与质量，具体改进包括为训练前数据开发更细致的预处理与管理管线、制定更严格的质量保证体系，以及针对训练后数据的过滤方法。</p><p>&nbsp;</p><p>为了支持405B参数模型的大规模生产推理需求，Meta将模型从16位（BF16）量化为8位（FP8）数值精度，从而有效降低了相应计算要求，允许模型在单一服务器节点之内运行。</p><p>&nbsp;</p><p>通过Llama 3.1 405B，Meta希望努力提高模型对于用户指令的适用性、响应质量与详尽的指令遵循能力，同时确保安全性更上一层楼。Meta面临的最大挑战在于如何支持更多功能、更长的128K上下文窗口以及更大的模型体量。</p><p>&nbsp;</p><p>在训练之后，Meta通过在预训练模型的基础之上执行多轮对齐以生成最终聊天模型。每个轮次都涉及监督微调（SFT）、拒绝采样（RS）和直接偏好优化（DPO）。Meta使用合成数据生成来提供绝大部分监督微调示例，并进行多轮迭代以生成涵盖所有功能且质量更高的合成数据。此外，Meta还在多种数据处理技术上进行投入，尝试以过滤方式提高合成数据的质量，最终成功扩展了跨多种功能的微调数据量。</p><p>&nbsp;</p><p>Meta 平衡了数据构成以建立起能够涵盖所有功能的高质量模型。例如，即使将上下文窗口扩展至128K，Meta也仍保持了模型在短上下文基准上的质量。同样，在引入安全缓解措施之后，新版本模型也仍可继续提供最具实效、能够满足用户需求的答案。</p><p>&nbsp;</p><p>Meta还发布了一份92页的 Llama 3 相关的论文，此时应该还有很多开发者正在研究里面的一些细节，已经读过的开发者也对这份论文表示称赞。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c53938bc586743d4d75c2c4b3d61fd14.png" /></p><p></p><p>&nbsp;</p><p>在 x 担任 SWE 的 kache发文称：“阅读 Meta Llama 3.1 论文中关于基础设施的部分，以及他们为了解决这个规模的所有工程问题，实际上让我感到痛苦。重要的资产不是你创建的模型，而是基础设施的工程实力。如果你也能像这样做一次训练运行，你就可以做得更多。这是一个独一无二的资产，是一条护城河。”马斯克回复他：是的，这是一次痛苦的训练。</p><p>&nbsp;</p><p>kache还提到，“扎克伯格对苹果的看法确实很正确：他们树敌多少，还能继续生存下去吗？”马斯克也跟帖称：“说实话，我也是这么想的。”</p><p>&nbsp;</p><p>看得出来，马斯克对扎克伯格是赞同的，“这令人印象深刻，扎克伯格确实因开源而受到赞扬”他在回复Karpathy的帖子中提到。</p><p>&nbsp;</p><p>想要查看更多技术细节的读者可以查看：</p><p>&nbsp;</p><p><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">https://ai.meta.com/research/publications/the-llama-3-herd-of-models/</a>"</p><p>&nbsp;</p><p>Meta评估了150多项涵盖多个语种的基准数据集性能。此外，Meta还开展了广泛的人工评估，将Llama 3.1与现实场景中的同类竞争模型进行了比较。评估结果表明，Meta的旗舰模型在一系列任务中与领先基础模型相比具备竞争力，包括GPT-4、GPT4o以及Claude 3.5 Sonnet。此外，Meta的小模型在参数规模相当的其他封闭与开放模型当中，也同样具有强劲的竞争力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef5af12348efccba036f35dc2c5a33ff.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/22/226e42627fb1cb5de7b1d9b34180ff64.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af633454b2c6c8296d857ae140f00b93.png" /></p><p></p><p>&nbsp;</p><p>此外，Meta强调Llama模型始终作为整体系统的一部分发挥作用。Llama 系统能够协调多种组件，包括调用外部工具。Meta还发布了一套完整的参考系统，其中包含多个示例应用程序。此外还有新的组件，例如Llama Guard 3（多语言安全模型）和Prompt Guard（提示词注入过滤器）。这些示例应用程序同样属于开源成果，可供社区做进一步构建。</p><p>&nbsp;</p><p>与封闭模型不同，Llama模型权重对外开放下载。开发人员可以根据自身需求和应用场景对模型进行全面自定义，在新数据集之上进行训练，并开展额外的微调。开发人员可以完全自定义自己的应用程序并在任意环境下运行，包括本地、云端，甚至是本地运行在笔记本电脑之上——所有这些都无需与Meta共享数据。</p><p>&nbsp;</p><p>Groq 首席执行官兼创始人Jonathan Ross演示了Llama 3.1 和 Groq 芯片结合的速度：</p><p></p><p></p><p></p><p>&nbsp;</p><p>Kindo 产品副总裁 Andrew "Andy" Manoske称，“使用ollama 在 OpenDevin 上本地运行llama 3.1 ，但是它耗尽了我的 VRAM。也许该买一台新笔记本电脑了！ ”</p><p>&nbsp;</p><p>还有网友nicekate测试了Llama 3.1 8B的指令遵循情况，结论是“对比Gemma2-9b-It，llama 3.1 8B更好”。“测试用的我前两天做的单词关联应用，都是接入的Groq，用Gemma2-9b-It运行结果显示例子和记忆技巧里一会是中文，一会是英文，不遵循我的提示。但是今天改用llama 3.1 8B和70B，情况好了很多，让它中文解释，它都遵循了。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5f35a29b335e0b584955a6c9a97da591.jpeg" /></p><p></p><p>&nbsp;</p><p>有资深开源专家表示，“觉得 Meta开源这么大一个模型是巨大的财务浪费? Llama3.1-405B的成本也只有拍一部豆瓣评分不会到7的电影的三分之一。”</p><p>&nbsp;</p><p></p><h2>扎克伯格长文论辩为何坚持开源</h2><p></p><p>&nbsp;</p><p>“经常有人问我是否担心开源 Llama 会丢掉技术优势，我认为这是因为他们没有从大局方面考虑。”扎克伯格解释道，为了确保Meta能够使用最好的技术，并且不会长期被困在封闭的生态系统中，Llama 需要发展成为一个囊括工具、效率改进、芯片优化和其他集成的完整生态系统。</p><p>&nbsp;</p><p>扎克伯格还指出，Meta 与闭源模型厂商之间的一个关键区别是，Meta 不靠出售 AI 模型访问权盈利，Meta 的商业模式是为人们打造最佳体验和服务。“这意味着公开发布 Llama 不会让我们的收入、可持续性和投资研究能力，像闭源厂商那样被削弱（这是一些闭源厂商不断游说政府反对开源的原因之一）。”</p><p>&nbsp;</p><p>扎克伯格以自身经历为例说道，“我的一个重要经历是，我们在苹果平台上构建服务时会受到限制。从他们对开发者的征税方式、施加的任意规则以及阻止发布的所有产品创新行为看，很明显，如果Meta和其他许多公司能够构建自己产品的最佳版本，而竞争对手无法限制构建的内容，我们将能够为人们构建更好的服务。”</p><p>&nbsp;</p><p>扎克伯格也提到自己在与世界各地的开发商、CEO和政府官员交谈时了解到的开发者侧的需求：需要训练、微调和提炼自己的模型；需要自主掌控，而不是被一家封闭的供应商束缚；需要保护数据；需要一个高效且运行成本低廉的模型；希望投资于将成为长期标准的生态系统，而这些问题的答案，扎克伯格认为就是开源。</p><p>&nbsp;</p><p>“开源将确保全世界更多的人能够享受 AI 带来的好处和机会，权力不会集中在少数公司手中，并且该技术可以更均匀、更安全地应用于整个社会。”扎克伯格说道。</p><p>&nbsp;</p><p>这次扎克伯格的这次发文也赢得了很多开发者的赞同，为此，我们将其《开源人工智能——AI发展的正确方向》原文附在下面：</p><p>&nbsp;</p><p>在高性能计算时代早期，当时各大科技巨头都砸下重金希望开发自己的闭源版Unix。当时我们很难想象还有什么其他方法能够开发出如此先进的软件产品。但最终，开源Linux还是获得了广泛普及——其最初的应用动力，主要是允许开发者随意修改其代码，而且使用成本也更为实惠。但随着时间推移，这个开源软件项目变得愈发先进、更加安全，而且有着比任何闭源Unix都更丰富的功能和更广泛的生态系统。现如今，Linux已然成为云计算和大部分移动设备操作系统的行业标准和实现基础，让每个人都有机会体验到代表时代前沿的卓越软件成果。</p><p>&nbsp;</p><p>Meta坚信AI也会以类似的方式一路前行。当下，多家科技企业正在开发领先的闭源模型，但开源与其差距正在迅速缩小。去年，Llama 2还仅与已显落后的上一代模型水平相当。时间来到今年，Llama 3已经能够与最先进的闭源模型相媲美，甚至在某些领域实现了反超。从明年开始，我们预计后续Llama模型将成为业内最先进的大模型代表。而且哪怕是在当下，Llama也已经在开放性、可修改性和成本效益等方面处于领先地位。</p><p>&nbsp;</p><p>如今，Meta正朝着开源AI成为行业标准的目标迈出坚实的又一步。Meta正式发布首个前沿级开源AI模型Llama 3.1 405B，以及经过改进的Llama 3.1 70B与8B模型。除了具有远胜封闭模型的成本/性能之外，405B模型的开放性特质也使其成为微调和蒸馏小体量模型的最佳选择。</p><p>&nbsp;</p><p>除了发布这些模型之外，Meta还与多家企业开展合作，以期建立起更广泛的生态系统。亚马逊、Databricks以及英伟达正着手发布配套服务，以支持开发人员微调和蒸馏自己的模型。Groq等初创企业则为新的Llama家族成员建立起低延迟、低成本的推理服务。这些模型将登陆所有主要云服务平台，包括亚马逊云科技、Azure、Google和Oracle等。Scale.AI、戴尔、德勤等公司也已做好准备，将帮助企业客户采用Llama模型并利用自有数据训练出定制化版本。随着社区的发展以及更多公司为此开发新服务，各方将共同推动Llama成为行业标准，让每个人都能享受到AI带来的切实助益。</p><p>&nbsp;</p><p>Meta致力于推动AI开源。本文将具体介绍开源为何是最适合广大用户的开发技术栈，开源Llama对于Meta自身的好处，以及开源AI为何有助于创造一个更美好的世界，并发展成一套持续创新、活力永驻的技术平台。</p><p>&nbsp;</p><p></p><h4>开源AI给开发人员带来的好处</h4><p></p><p>&nbsp;</p><p>在与世界各地的开发人员、CEO和政府官员们交流时，他们往往高度关注以下几个议题：</p><p>我们需要训练、微调和蒸馏自己的模型。每个组织的需求各不相同，最好使用不同体量的模型来满足这些需求，且各个模型应使用特定数据进行训练和微调。设备端的任务及分类任务更适合采用小模型，而较为复杂的任务则需要规模更大的模型。现在，我们可以采用最先进的Llama模型，且继续使用自有数据对其进行训练，而后将成果蒸馏为最佳大小的模型——Meta或任何其他人都无法查看您的数据。我们需要掌控自己的命运，而不愿被锁定在封闭供应商身上。许多组织不想将业务命脉锁定在自己无法运行和控制在模型之上。他们不愿封闭模型提供商随意更改自己的模型、变更其使用条款，甚至完全停止为他们提供服务。他们也不希望被锁定在拥有模型专有权的特定云环境当中。开源使得涵盖兼容工具链的广泛商业生态成为可能，用户可以轻松在这些工具链之间往来切换。我们需要保护自己的数据。许多组织处理的是需要严格保护的敏感数据，因此无法通过云API将其发送至封闭模型。也有一些组织根本不信任将自己的数据交由封闭模型提供商来处理。开源允许用户在任意位置运行模型，因此有效解决了这些现实难题。人们普遍认为开源软件的开发流程更加透明，因此相关成果往往更加安全可靠。我们需要一种高效且运行成本低廉的模型。开发人员可以在自己的基础设施之上使用Llama 3.1 405B进行推理，成本约为使用GPT-4o等封闭模型的一半（无论是面向用户还是离线推理任务）。我们希望投资于能够发展为长期标准的生态系统。很多人意识到开源AI的发展速度比封闭模型更快，因此希望在能为自己带来最大长期优势的架构之上打造自己的系统。</p><p>&nbsp;</p><p></p><h4>开源AI给Meta自身带来的好处</h4><p></p><p>&nbsp;</p><p>Meta的商业模式，就是为用户打造最出色的体验和服务。要实现这个目标，Meta公司必须确保自身始终掌握最好的技术手段，同时不会陷入由竞争对手所掌握、有可能限制Meta开发计划的封闭生态系统。</p><p>&nbsp;</p><p>Meta在发展过程中曾经有过此类经历，其构建的服务受到苹果平台对于构建内容的限制。而且从苹果向开发商抽成的方式、在规则制定方面的任性以及阻止产品创新的发布等行为来看，Meta意识到要想打造出自家产品的最佳版本，也绝对不能将内容构建的管控权拱手让给竞争对手。也只有这样，包括Meta在内的各类软件厂商才能更自由地为用户构建更好的服务。从哲学层面来讲，这也是Meta坚定为下一代计算范式构建AI及AR/VR开放生态系统的主要原因。</p><p>&nbsp;</p><p>人们常常会问，Meta会不会担心由于开源Llama而失去技术优势。这种观点在某种程度上缺乏大局观，具体原因如下：</p><p>&nbsp;</p><p>首先，为了确保始终使用最好的技术、且不会被长期锁定在封闭的生态系统当中，Llama需要发展出一套完整的生态系统，具体包含工具、效率改进、芯片优化和其他集成。如果Meta是唯一一家使用Llama的公司，那么生态系统发展将无从谈起，产品表现也绝不会比当年的封闭Unix更好。</p><p>&nbsp;</p><p>其次，Meta预计AI开发将继续保持白热化的竞争态势，也就是说对任何当前模型的开源、都不致失去在下一代最佳模型上的主要技术优势。Llama之所以逐步发展成为行业标准，依靠的就是一代又一代保持着竞争力、效率和开放性。</p><p>&nbsp;</p><p>第三，Meta和封闭模型提供商之间的一大关键区别，就在于Meta从来不会将出售AI模型的访问权作为自己的盈利模式。也就是说公开发布Llama不会像封闭服务商那样影响到Meta的收入、可持续性或者投资研究的能力。（这也是部分封闭服务商不断游说政府，打压开源的原因之一。）</p><p>&nbsp;</p><p>最后，Meta拥有悠久的开源项目和成功经历。Meta通过开放计算项目（OCP）发布了自己的服务器、网络和数据中心设计，并让供应链在这套设计体系之上实现了标准化，从而节约下数十亿美元。Meta也通过领先开源工具（包括PyTorch、React等多种工具）从生态系统的创新当中受益。只要长期坚持这种共赢方法，这种收益也将持续生效。</p><p>&nbsp;</p><p></p><h4>开源AI给整个世界带来的好处</h4><p></p><p>Meta坚信开源在为AI科技塑造光明未来当中发挥着至关重要的作用。与任何其他现代技术相比，AI都具备更强大的提高人类生产力、创造力以及生活质量的潜力——而且能够在加速经济增长的同时，释放医学及科学研究的进步潜能。开源将确保世界上有更多人能够获得AI带来的好处和机会，避免权力被集中在少数企业手中，也能让技术成果以更均匀、更安全的方式被部署到整个社会。</p><p>&nbsp;</p><p>关于开源AI模型的安全性争论一直存在，Meta的观点是开源AI要比其他替代方案更加安全。相信各国政府会得出正确的结论，意识到支持开源更符合自身利益，也将使得整个世界更加繁荣和安全。</p><p>&nbsp;</p><p>Meta理解的安全框架，应当能够防范两类危害：无意危害与有意危害。无意危害是指AI系统本身可能造成的影响，且问题并非源自操作者的主观故意。举例来说，现代AI模型可能在不经意间给出不良的健康建议。或者在更未来化的场景中，&nbsp;人们担心大模型可能会在无意中自我复制或者过度优化目标，从而损害人类利益。至于有意危害，则是指恶意人士利用AI模型来造成伤害。</p><p>&nbsp;</p><p>需要注意的是，人们对于AI科技的大部分担忧其实都属于无意危害——例如AI系统将对数十亿用户产生怎样的影响，甚至包括可能给全人类带来灾难性后果的科幻场景。在这方面，开源同样更加安全，因为系统透明度更高、可以受到广泛的监督和审查。从历史角度看，开源软件确实凭借着良好的透明度而更加安全可靠。同样的，使用Llama及其安全系统（例如Llama Guard）也能实现优于封闭模型的安全性和可靠性。也正因为如此，大多数关于开源AI安全的讨论都集中在有意危害层面。</p><p>&nbsp;</p><p>Meta的安全流程包括严格的查验与红队测试，用以评估相关模型是否会造成现实意义上的伤害，进而确保在发布之前降低风险。由于模型对外开放，所以任何人都可以亲自上手测试。但需要强调的是，这些模型是通过互联网上的现有信息训练而成，因此考虑其伤害及影响的基本出发点，应该是大模型是否会比直接从谷歌或其他搜索引擎中快速检索到的结果造成更多伤害，而非简单粗暴的存不存在伤害。</p><p>&nbsp;</p><p>在对有意危害进行归因时，应当区分个人或小规模行为者可能造成怎样的危害，以及掌握大量资源的国家等大规模行为者可能造成怎样的危害。</p><p>&nbsp;</p><p>在未来的某个时候，个别恶意行为者有可能利用AI模型的智能，从互联网上的可用信息中制造全新的危害。在这方面，力量平衡对于保障AI安全就显得尤其重要。Meta认为生活在一个广泛部署AI方案的世界有助于实现权力分散，这样大规模行为者与小规模行为者之间能够形成制衡和拮抗。这也是我们长期以来管理社交网络安全的方式——利用强大的AI系统识别并阻止来自不太成熟的用户们的威胁，从而有效对抗他们手中掌握的小型AI系统。从更广泛的角度出发，大规模部署AI有助于促进整个社会的安全和稳定。只要每个人都能使用基于开源理念的迭代大模型，那么掌握更多计算资源的政府和机构将能够在降低算力资源消耗的同时，快速发现恶意行为者的踪迹。</p><p>&nbsp;</p><p>下一个问题，是美国及其他民主国家应该如何应对某些拥有大量资源的专制国家的威胁。美国的优势就在于去中心化和开放式创新。有些人认为美国应当封闭自己的模型，以免开发成果落入敌对国家手中。但Meta认为这并非正道，只会让美国及其盟友处于更加不利的地位。国家力量支持下的间谍活动相当强大，如今的大模型也可以被轻松装进大容量U盘当中，而且多数科技企业的运营方式并不像保密机构那么严谨。所以最有可能的情况是，在只存在封闭模式的世界当中，能够率先接触到领先模式的可能只有少数大企业外加地缘政治对手，众多初创公司、高校和小企业反而被挡在赛场之外。</p><p>&nbsp;</p><p>此外，如果将美国的创新限制在封闭开发的樊笼之内，也会增加整个国家失去领先地位的风险。因此，Meta认为最好的策略是建立起一套强大的开放生态系统，让领先企业能够与各国政府和盟友密切合作，确保他们能够充分运用最新发展成果，并在长期之内把握住可持续的先发优势。</p><p>&nbsp;</p><p>面对未来的种种机遇和挑战时，请大家记住，当今大多数领先的科技企业和科学研究都是建立在开源成果之上。只要我们共同投入，那么下一代企业和研究就有望拥抱开源AI。其中既包括刚刚起步的初创公司，也包括那些拿不出雄厚资源、从头开始开发最先进AI的高校和各国政府。</p><p>最重要的是，开源AI代表着一种机会、一种希望。只有开源力量，能够将这项技术交付到每个人手上，最终创造出更大的经济机遇与可靠的安全保障。</p><p>&nbsp;</p><p></p><h4>携手Meta，开源AI</h4><p></p><p>&nbsp;</p><p>过往的Llama模型均由Meta公司自行开发并对外开放，但并没有过多关注如何构建更广泛的生态系统。Meta此次发布采取了不同的方法。目前公司内部正在组建团队，希望让更多开发人员和合作伙伴能够使用到Llama。此外Meta也在积极建立合作伙伴关系，确保生态系统中的更多企业能够为其客户提供独特的功能。</p><p>&nbsp;</p><p>Meta坚信Llama 3.1版本将成为AI行业的又一转折点，代表着大多数开发人员转向开源AI方案的开始。预计这股趋势将从此刻开始逐渐发展壮大。Meta也诚邀各位加入这段旅程，将AI的好处交付到世界上的每一个人手中。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://timkellogg.me/blog/2024/07/23/llama-3.1">https://timkellogg.me/blog/2024/07/23/llama-3.1</a>"</p><p><a href="https://ai.meta.com/blog/meta-llama-3-1/">https://ai.meta.com/blog/meta-llama-3-1/</a>"</p><p><a href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/</a>"</p><p><a href="https://x.com/JonathanRoss321/status/1815777714642858313">https://x.com/JonathanRoss321/status/1815777714642858313</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/aGI4zjLIj5tVLHd81Qdm</id>
            <title>第四范式发布先知AIOS 5.1，升级支持GPU资源池化功能</title>
            <link>https://www.infoq.cn/article/aGI4zjLIj5tVLHd81Qdm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/aGI4zjLIj5tVLHd81Qdm</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jul 2024 08:49:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 第四范式先知AIOS 5.1版本, GPU资源池化, 硬件成本节省, 大模型构建
<br>
<br>
总结: 第四范式先知AIOS 5.1版本发布，新增GPU资源池化功能，可节省硬件成本并提高GPU利用率，是行业大模型构建和管理平台。 </div>
                        <hr>
                    
                    <p>今天，第四范式先知AIOS 5.1版本正式发布。该版本新增GPU资源池化（vGPU）能力，实现对硬件集群平台化管理、算力资源的按需分配和快速调度，最多节省80%的硬件成本，提高GPU综合利用率多达5-10倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3175adcb0cd4b140fbe2eeca0828404.png" /></p><p></p><p>第四范式先知AIOS 5是行业大模型开发及管理平台。平台以提升企业核心竞争力为目标，在支持接入企业各类模态数据的基础上，提供大模型训练、精调等低门槛建模工具、科学家创新服务体系、北极星策略管理平台、大模型纳管平台、主流算力适配优化等能力，实现端到端的行业大模型的构建、部署、管理服务。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/70/bb/700a7e796b091bf086154acf25f5f7bb.png" /></p><p></p><p>在行业大模型的构建过程中，为进一步提高算力资源利用率，第四范式先知AIOS 5.1版本新增GPU资源池化（vGPU）能力，拥有五大技术亮点：</p><p>全面适配国产/非国产算力，支持混合部署与统一调度算力和显存超分复用，算力切分精细到1%，显存切分以M兆为单位具备千卡级别分布式调度与管理能力支持自定义隔离策略，实现共享或独享算力池利用多任务共享及处理优化技术，推理性能提升10倍以上</p><p>&nbsp;</p><p>了解产品详情，可致电400-898-7788，或扫描下方二维码。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/61/b5/614c1b98998716e43652e45c75ebdcb5.png" /></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lf0UK1c5x9EMGbmos8lu</id>
            <title>Llama 3.1 源模型泄露背后：失手的GitHub ，破碎的Meta，好在最小参数都能打脸GPT-4o！</title>
            <link>https://www.infoq.cn/article/lf0UK1c5x9EMGbmos8lu</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lf0UK1c5x9EMGbmos8lu</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jul 2024 07:10:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, Llama 3.1, GPT-4o, GitHub
<br>
<br>
总结: 7月23日凌晨，有人爆料称Meta的新版Llama 3.1在4chan上泄露，并在大多数基准测试中击败了GPT-4o。据爆料人称，Meta可能会在明天发布Llama 3系列中最大的参数模型以及70B版本。GitHub上泄露的Llama 3.1模型链接已404，但文件大小约763.84G。有网友猜测泄露可能来自第三方托管商，认为是从微软的GitHub流出来的。Llama 3.1的70B模型或将更接近免费，因为其支持在消费类硬件上运行。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>7月23日凌晨，有人爆料，Meta的新版&nbsp;Llama&nbsp;3.1&nbsp;405&nbsp;B&nbsp;在&nbsp;4chan&nbsp;上泄露，并在大多数基准测试中击败了&nbsp;GPT-4o。据爆料人称，Meta可能会在明天正式发布Llama&nbsp;3系列中最大的参数模型以及70B版本。</p><p></p><p>现在，Github&nbsp;上泄露的&nbsp;Llama&nbsp;3.1&nbsp;模型链接已&nbsp;404&nbsp;，但据网友保存下来的下载链接显示，文件大约763.84G。据悉，HugginFace&nbsp;上的比推特网友爆料更早，但现在库已经被删除。“似乎&nbsp;HF&nbsp;的某个人忘记按时将这个存储库私有化，并且&nbsp;Google&nbsp;将其编入索引。”</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d5a400abc3af48e57cc21087089bf51.png" /></p><p></p><p>HF&nbsp;链接:&nbsp;<a href="https://huggingface.co/cloud-district/miqu-2">https://huggingface.co/cloud-district/miqu-2</a>"</p><p>磁链：magnet:?xt=urn:btih:c0e342ae5677582f92c52d8019cc32e1f86f1d83&amp;dn=miqu-2&amp;tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80</p><p>种子：<a href="https://files.catbox.moe/d88djr.torrent">https://files.catbox.moe/d88djr.torrent</a>"</p><p>来源：<a href="https://boards.4chan.org/g/thread/101514682#p101516633">https://boards.4chan.org/g/thread/101514682#p101516633</a>"</p><p></p><p>有网友猜测，此次泄露极有可能来自第三方托管商，该托管商提前获得访问权，来准备发布前的一些工作。不少人认可了泄露模型的真实度，并认为是从微软的&nbsp;GitHub流出来的。“它在&nbsp;GitHub&nbsp;上线过一段时间后，他们又把它撤下来了，但我已经看到了一些自定义模型。看来有人能及时发现。”&nbsp;</p><p>从爆料人发出的评测数据表中可以看到，Llama&nbsp;3.1是在3.0版本的基础上进行了功能迭代，但即使70B模型的性能也在部分领域超过了GPT-4o。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc847109c06dd4985c2ecead6640598e.png" /></p><p></p><p>&nbsp;</p><p>有看好的网友这样说道，“如果这份评测数据是真实的，那么从本周开始，Meta的最顶级人工智能模型将是供所有人免费使用的开放权重模型。有趣的是，全球每个国家的政府、组织和公司都能和其他人一样获得同一套这样的人工智能能力。”</p><p>&nbsp;</p><p>需要注意的是，尽管&nbsp;Llama&nbsp;3.1&nbsp;开源免费，&nbsp;但模型本身的使用成本似乎并不低。据悉，由于该模型的参数较大，对GPU的要求较高，需要一些强大的硬件才能在本地运行起来，因此并不如GPT-4o&nbsp;mini性价比高。</p><p>&nbsp;</p><p>有网友猜测道，“一般的GPU肯定是跑不起来，如此大的参数在部署方面个人开发者也负担不起（如果你有一些H100也没问题），估计是给企业、政务公共部门用的。”也有网友表示，“虽然&nbsp;Llama&nbsp;3.1是免费使用的，但没有多少人会拥有运行此模型的计算机。只有一些具备高算力基础的大公司能够自己使用它，所以也许它会成为企业的加速器。”</p><p>&nbsp;</p><p>不过，Llama&nbsp;3.1&nbsp;的70B模型或将更接近免费，因为其支持在消费类硬件上运行。</p><p>&nbsp;</p><p></p><h1>8B&nbsp;大幅提升，整体编码性能落后</h1><p></p><p>&nbsp;</p><p>Meta&nbsp;Llama&nbsp;3.1&nbsp;多语言大型语言模型&nbsp;（LLM）&nbsp;集合是&nbsp;8B、70B&nbsp;和&nbsp;405B&nbsp;大小（文本输入/文本输出）的预训练和指令调整生成模型的集合。Llama&nbsp;3.1&nbsp;指令调整的纯文本模型（8B、70B、405B）针对多语言对话用例进行了优化，在常见的行业基准测试中优于许多可用的开源和封闭聊天模型。</p><p></p><p>这是网上曝出的Llama&nbsp;3.1&nbsp;模型卡中所介绍的信息，发布日期是2024&nbsp;年&nbsp;7&nbsp;月&nbsp;23&nbsp;日。此外，该模型卡还报告了&nbsp;Llama&nbsp;3.1&nbsp;模型在标准自动基准测试下的结果。</p><p>&nbsp;</p><p>从评分结果可以看到，405B&nbsp;看起来很不错，在某些基准测试中达到了&nbsp;SOTA，与GPT-4o&nbsp;和Sonnet&nbsp;&nbsp;3.5&nbsp;不相上下，70B&nbsp;则在&nbsp;HumanEval&nbsp;上出现了奇怪的倒退（HumanEval&nbsp;是由&nbsp;OpenAI&nbsp;编写发布的代码生成评测数据集）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf061fd18460116da8aea15515fb4d5f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>值得一提的是，在爆料人发出的评测数据表中，与&nbsp;GPT&nbsp;4o&nbsp;mini&nbsp;相比，&nbsp;Llama&nbsp;3.1&nbsp;70B&nbsp;似乎可以以&nbsp;3&nbsp;倍的成本进行推断，但编码性能也要差得多。此外，Llama&nbsp;3.1&nbsp;405&nbsp;B&nbsp;似乎也在&nbsp;HumanEval&nbsp;方面明显落后于&nbsp;GPT-4o。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2c5d8814313b10f739d431ab4fb9de7.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>还有人根据现有的模型卡信息，对比了&nbsp;Llama&nbsp;3.1&nbsp;与&nbsp;3.0&nbsp;之间的差异，并将其与其他前沿模型进行比较。得出的结果是Llama&nbsp;3.1&nbsp;8B&nbsp;全面大幅提升，70B&nbsp;稍好，405B&nbsp;仍落后于旗舰机型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d8/d8a668d314c80bbb4c39bd4744ca958e.png" /></p><p></p><p></p><p>另外，Llama&nbsp;3.1&nbsp;的数理能力似乎也提升不少。&nbsp;7&nbsp;月&nbsp;21&nbsp;日，一位称体验了&nbsp;Llama&nbsp;405B&nbsp;模型的网友表示，其看起来能解决&nbsp;9.9&nbsp;&gt;&nbsp;9.11&nbsp;的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f7f101427285879a242bda2a89ffe99.png" /></p><p></p><p></p><p></p><h1>15&nbsp;万亿预训练数据，支持8种语言</h1><p></p><p>&nbsp;</p><p>模型卡还透露了许多Llama&nbsp;3.1&nbsp;的技术细节。据称，Llama&nbsp;3.1&nbsp;是一个自回归语言模型，它使用优化的&nbsp;transformer&nbsp;架构。调整后的版本使用监督微调&nbsp;（SFT）&nbsp;和带有人类反馈的强化学习&nbsp;（RLHF），以符合人类对有用性和安全性的偏好。</p><p>&nbsp;</p><p>在Llama&nbsp;3.1系列中，所有模型版本都使用分组查询注意力&nbsp;（GQA）&nbsp;来改进推理可伸缩性。据该模型卡介绍，“这是一个在离线数据集上训练的静态模型。随着我们根据社区反馈提高模型安全性，将发布调整模型的未来版本。”</p><p>&nbsp;</p><p>使用方面，Llama&nbsp;3.1支持的语言有英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语，但也接受了比&nbsp;8&nbsp;种支持的语言更广泛的语言集合的训练。也就是说，开发人员可以针对&nbsp;8&nbsp;种支持语言以外的语言对&nbsp;Llama&nbsp;3.1&nbsp;模型进行微调，前提是他们遵守&nbsp;Llama&nbsp;3.1&nbsp;社区许可证和可接受使用政策，并且在这种情况下，他们有责任确保以安全和负责任的方式使用&nbsp;Llama&nbsp;3.1。</p><p>&nbsp;</p><p>据介绍，&nbsp;Llama&nbsp;3.1&nbsp;旨在以多种语言用于商业和研究用途。指令优化的纯文本模型适用于类似助手的聊天，而预训练模型可以适用于各种自然语言生成任务。Llama&nbsp;3.1&nbsp;模型集合还支持利用其模型的输出来改进其他模型的能力，包括合成数据生成和蒸馏。</p><p>&nbsp;</p><p>训练数据方面，&nbsp;Llama&nbsp;3.1&nbsp;在来自公开来源的大约15&nbsp;万亿个token数据上进行了预训练，微调数据包括公开可用的指令数据集以及超过&nbsp;2500&nbsp;万个合成生成的示例。其中，预训练数据的截止时间为&nbsp;2023&nbsp;年&nbsp;12&nbsp;月。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d311afab01147959f3deb578e0634a3.jpeg" /></p><p></p><p>Llama&nbsp;3.1&nbsp;使用自定义训练库、Meta&nbsp;定制的&nbsp;GPU&nbsp;集群和生产基础设施进行预训练，还对生产基础设施进行了微调、注释和评估。据悉，其在&nbsp;H100-80GB（TDP&nbsp;为&nbsp;700W）类型硬件上累计使用了&nbsp;39.3&nbsp;M&nbsp;GPU&nbsp;小时的计算时间。同时，Llama&nbsp;3.1&nbsp;训练期间基于地域基准的温室气体总排放量预估为&nbsp;11390&nbsp;吨二氧化碳当量。</p><p>&nbsp;</p><p></p><h1>结语</h1><p></p><p>Llama&nbsp;3.1的免费开放消息，令不少关注人工智能大模型的用户欢呼，但与此同时也引发了另一些人的担忧。“我们正处于范式转变的风口浪尖，像&nbsp;Llama&nbsp;3.1&nbsp;这样强大的&nbsp;LLM&nbsp;的开放获取预示着巨大的潜力和前所未有的风险。这种强大人工智能模型的民主化将重塑社会、经济和治理结构，未来悬而未决。”</p><p>&nbsp;</p><p>此前，由于监管机构和各种法案的原因，Meta的确也一直在推迟405B系列模型的发布。</p><p>&nbsp;</p><p>在Llama&nbsp;3.1&nbsp;的模型卡中，谈到关于使用安全方面的立场和使用建议，“大型语言模型包括&nbsp;Llama&nbsp;3.1，不是为孤立部署而设计的，而是应该作为整体&nbsp;AI&nbsp;系统的一部分进行部署，并根据需要提供额外的安全护栏。开发人员在构建代理系统时应部署系统保护措施。安全保障措施是实现正确的有用性与安全一致性的关键，也是降低系统固有的安全和安保风险以及模型或系统与外部工具的任何集成的关键。”</p><p>&nbsp;</p><p>为此，Llama&nbsp;3.1版本引入了新功能，包括更长的上下文窗口、多语言输入和输出以及开发人员与第三方工具的可能集成。除了通常适用于所有生成式&nbsp;AI&nbsp;用例的最佳实践外，使用这些新功能进行构建还需要特定的考虑因素：</p><p>工具使用：就像在标准软件开发中一样，开发人员负责将&nbsp;LLM&nbsp;与他们选择的工具和服务集成。他们应该为其用例定义明确的策略，并评估他们使用的第三方服务的完整性，以便在使用此功能时了解安全和安保限制。多语言：Llama&nbsp;3.1&nbsp;除英语外还支持&nbsp;7&nbsp;种语言，&nbsp;虽然其能够输出其他语言的文本，但可能不符合安全性和有用性性能阈值。</p><p>&nbsp;</p><p>“Llama&nbsp;3.1&nbsp;是一项新技术，与任何新技术一样存在使用风险。迄今为止进行的测试尚未涵盖，也不可能涵盖所有情况。由于这些原因，与所有&nbsp;LLM&nbsp;一样，Llama&nbsp;3.1&nbsp;的潜在输出无法提前预测，并且该模型在某些情况下可能会对用户提示产生不准确、有偏见或其他令人反感的响应。因此，在部署&nbsp;Llama&nbsp;3.1&nbsp;模型的任何应用程序之前，开发人员应根据其模型的特定应用程序进行安全测试和调整。”Meta&nbsp;在模型卡中写道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/mattshumer_/status/1815444612414087294">https://x.com/mattshumer_/status/1815444612414087294</a>"</p><p><a href="https://pastebin.com/9jGkYbXY">https://pastebin.com/9jGkYbXY</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8cOqOIWEZuuo9vsbLiD4</id>
            <title>就在今晚！实战派技术大佬在线编码，传授全生命周期高效开发秘籍</title>
            <link>https://www.infoq.cn/article/8cOqOIWEZuuo9vsbLiD4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8cOqOIWEZuuo9vsbLiD4</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jul 2024 06:42:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 技术, 云上探索实验室, 沉浸式直播, Amazon Q 全生命周期开发系列
<br>
<br>
总结: AI 技术的应用已经远远超出想象，云上探索实验室 2.0 将带来全新的技术革命，通过沉浸式直播和实战编码，让开发者体验生成式 AI 时代的技术魅力，同时通过 Amazon Q 全生命周期开发系列帮助开发者系统掌握如何使用 AI 助手提高工作效率。 </div>
                        <hr>
                    
                    <p>AI 技术大爆炸的时代，你真的能用好它吗？</p><p></p><p>或许你已经尝试过让 AI 成为你的助手，但你是否发现，AI 的力量远远超出了你的想象？对于开发者而言，他几乎能够参与到「软件开发的全生命周期」中，从项目梳理到代码迭代，从 AI 应用快速搭建到数据分析洞察...... 然而，遗憾的是，我们发现许多人还没有掌握如何用好 AI 让工作效率原地起飞的方法。</p><p>现在，机会来了，【云上探索实验室】 2.0 来袭，它将带你开启一场全新的技术革命！</p><p></p><p></p><h2>沉浸式直播，与实战派技术大佬一起在线编码</h2><p></p><p></p><p>【云上探索实验室】是专为开发者设计，旨在创造最前沿技术的一站式实操体验。2.0 版本的云上探索实验室全面升级！我们将采用沉浸式直播与实验平台实操相结合的方式，带你沉浸式感受生成式 AI 时代的技术魅力。</p><p></p><p>实战派技术大佬与开发者代表将在直播中在线编码并完成开发，而镜头前的你不仅可以观看，还可以在【云上探索实验室】的实验平台跟他们一起敲代码，感受技术的脉动。</p><p></p><p>🌟 实验平台现已开放多款实验免费体验，戳文末链接，在PC端开启实验</p><p></p><h2>谁卷谁不 Q，3 小时让全能 AI “班搭子”为你所用</h2><p></p><p></p><p>云上探索实验室 2.0 的第一个系列为【 Amazon Q 全生命周期开发系列】。我们将通过三期直播带你系统性掌握如何使用 AI 助手彻底改变你的工作流程，提高组织效率。</p><p></p><p>更关键的是，如此系统的直播内容、如此专业的实验平台、如此全面的实验学习资料，还有技术大佬在线带大家实操，这些内容都是免费的！不是付费课程看不起，而是免费直播更有性价比。活动详情及报名方式见海报。⬇️</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4ca59bcb8622db7ffce623d06b811be5.webp" /></p><p></p><p><a href="https://dev.amazoncloud.cn/experience/cloudlab?id=6645af5700cbe054da6e747b&amp;visitfrom=qq1&amp;sc_medium=owned&amp;sc_campaign=cloudlab&amp;sc_channel=qq1">直达云上探索实验室实验平台</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/i5DextzZMdOqTGhhJje9</id>
            <title>平安证券：数字化激励机制如何提升团队效率和挖掘人才</title>
            <link>https://www.infoq.cn/article/i5DextzZMdOqTGhhJje9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/i5DextzZMdOqTGhhJje9</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jul 2024 10:22:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 激励机制, KPI, OKR, 微徽章
<br>
<br>
总结: 以人为本的激励机制对于推动团队和个人朝着目标前进至关重要。平安证券通过“ KPI + OKR ＋微徽章”三位一体的数字化激励机制，明确目标、分解行动路径、精准奖励个体贡献，激发内驱力，实现高效完成关键成果，推动组织发展。 </div>
                        <hr>
                    
                    <p>千里之行，始于梦想，成于足下。以人为本的激励机制对于推动团队和个人朝着目标前进至关重要。在心理学领域，这一点同样具有深远的意义。一直以来，平安证券专注于研究如何提升团队的效率，鼓励团队成员不断创新和成长。</p><p></p><p>平安证券设立领航数字化激励机制，通过“ KPI + OKR ＋微徽章”三位一体有效支持组织目标达成。该机制通过 KPI 明确目标，借助 OKR 分解行动路径和关键成果，辅以微徽章精准奖励个体贡献，激发个体内驱力，动态调整并高效完成关键成果，协同实现组织最终目标。</p><p></p><p>本次演讲整理自平安证券信息技术中心首席信息官张朝晖在 QCon 2024 北京的分享“平安证券数字化激励机制”。</p><p></p><p></p><blockquote>在 8 月 16-17 日将于上海举办的 FCon 全球金融科技大会上，张朝晖老师将在 Keynote 演讲环节分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6057">打破旧世界，重组新世界——平安证券数字化利器微卡片平台实践</a>"》。更多大会演讲议题现已上线，点击链接可查看目前的专题安排：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f23b1fd693f537beb429b035def0a657.jpeg" /></p><p></p><h2>传统激励机制的现状与挑战</h2><p></p><p></p><h4>满足不同群体的需求和期望</h4><p></p><p></p><p>职场中不同年龄段的员工具有各自独特的特征和离职理由。网上流传的段子生动地描绘了这些差异：</p><p></p><p>60 后这一代人曾经历过物质匮乏的时期，他们即便在生活条件得到极大改善后，依然不愿意放弃工作。当被问及为何不离职时，他们的回答通常是：除非领导解雇，否则不会主动离开。70 后成长于中国经济的高速发展期，他们的工作虽然辛苦，但收获颇丰，因此工作起来充满干劲。当问到他们是否会离职时，他们甚至可能反问：什么是离职？80 后被认为是最艰难的一代，面临人口众多和激烈的社会竞争，以及购房、教育等生活压力。对他们来说，只要薪资足够，他们就不会考虑离职，他们更多是以金钱为驱动力。90 后更加追求工作带来的个人价值和意义，希望在工作中实现自我价值的提升，如果感到领导不尊重自己，他们会选择离职。00 后生活在一个信息丰富、生活条件优越的时代，他们更加注重职场的性价比，在选择工作时会考虑更多因素。如果感觉领导不听从自己的意见，他们会考虑离职。</p><p></p><p>这些段子虽然不能精确地反映每个人的情况，但它确实揭示了从 60 后到 00 后，不同年代人的需求和价值观存在差异。作为管理层，如何理解和满足这些不同的需求，是一个巨大的挑战。</p><p></p><p>不同群体——管理层、业务用户和普通员工——都有各自独特的需求：</p><p></p><p>从员工的角度来看，他们希望得到领导的认可，希望自己的工作能够带来成就感，渴望从事有趣且具有一定灵活性的工作，不希望从事重复性劳动。员工希望他们的工作能够被赋予意义，而不是仅仅为了完成任务。业务用户期望能够通过合作创造最大的价值。他们希望产品或服务具有竞争力，成本低廉，同时能够支持动态合作和低成本运作。业务用户需要的是一种能够带来实际效益并支持灵活应变的解决方案。管理层关注的是公司的盈利能力，他们希望团队能够高效合作，避免资源浪费，即所谓的“重复造轮子”。管理层追求的是低成本和高效率，这是他们经常强调的“降本增效”理念，也是近年来被频繁提及的管理口号。</p><p></p><p>要平衡这些需求，管理层需要深入了解每个群体的期望，并制定相应的策略。这包括提供员工培训和发展机会，增强他们的工作技能和灵活性；为业务用户提供创新和成本效益高的解决方案；同时，还要确保团队之间的协作和资源共享，以提高整体效率。通过这样的方式，可以更好地满足不同员工的需求，推动组织向前发展。</p><p></p><h4>动态管理目标和过程</h4><p></p><p></p><p>在探讨<a href="https://fcon.infoq.cn/2024/shanghai/track/1693">管理模式</a>"如何满足业务用户、管理层以及员工的需求时，大家都会提起 KPI 模式。传统的 KPI 管理模式是一种自上而下的方法，明确了公司年度的收入目标和客户数量等关键指标。这种模式的优势在于目标的明确性和可量化性，即通过具体的数字来衡量绩效。然而，这种模式也存在劣势，它过分强调结果，忽视了达成这些结果的过程。</p><p></p><p>这种“不管黑猫白猫，抓到老鼠就是好猫”的做法，可能会导致一些短期行为，比如为了实现"开门红"而采取的短期冲刺。短期冲刺虽然会在短期内带来一定的成效，但一旦活动结束，团队就会回到原来的状态，缺乏持续发展的动力。</p><p></p><p>因为缺乏战略性和对过程的关注，这种方法很难长期支持业务的真正发展。为了实现数字化管理，组织需要明确运营目标，并确保这些目标能够反映在 KPI 中。这意味着 KPI 不仅要关注结果，还要关注达成结果的过程，以及这些过程如何与组织的长期战略相一致。</p><p></p><p>OKR 是一种不同于传统 KPI 的目标管理框架。与 KPI 的自上而下的目标设定不同，OKR 鼓励自下而上地参与和动态调整，允许团队成员根据自己的理解和能力，设定有助于实现组织目标的关键结果。谷歌和抖音等公司都大力推广 OKR 的使用，并通过飞书等应用工具来支持 OKR 的实施。</p><p></p><p>将 OKR 与 KPI 结合使用，可以形成一种更为全面的管理模式。KPI 提供了明确的目标和衡量标准，OKR 则提供了实现这些目标的灵活路径和过程管理。这种结合有助于组织在确保目标明确性的同时，也能够适应变化，鼓励创新，最终实现动态管理和持续发展。</p><p></p><h4>传统员工激励措施效果不佳</h4><p></p><p></p><p>传统的员工激励措施在实施过程中存在一些问题。传统的员工激励措施通常有月度奖励、季度奖励和年度奖励等不同形式。然而，每当开始评估这些奖项时，会出现一些不公平的现象。例如，规定上个季度已经获得奖励的员工在这个季度不能再获奖。</p><p></p><p>这种做法忽视了一个事实：为什么一个员工在上个季度表现优秀，就不能在本季度继续保持优秀呢？这种模式实际上并不能鼓励员工持续创新和努力工作。它导致员工在获得一次奖励后，就失去了继续努力的动力，因为即使继续保持优秀，也不会立即得到额外的奖励。</p><p></p><p>现有的许多激励机制，如月度之星、季度之星、年度之星、先进个人和优秀团队等，都存在这样的问题。它们往往是基于周期性的评估，而不是持续的表现。这种周期性的奖励模式会导致员工在评估周期之外的时间里缺乏动力，影响员工的整体表现和创新能力。</p><p></p><h4>如何激发个体驱动力</h4><p></p><p></p><p>激发个体的驱动力，在实现目标的过程中保持动力且不迷失方向，是一个重要的课题。正如“千里之行，始于梦想，成于足下”这句话一样，不仅强调了梦想的重要性，还重视实现梦想所需的具体行动。</p><p></p><p>有了 KPI，可以明确了自己要达到的具体目标；有了 OKR，则帮助自己设定了实现这些目标的路径。就像跑马拉松一样，知道自己跑了多少公里或英里，以及目前处于哪个阶段，对于跑者来说至关重要。在任何时候都要了解自己的位置，这是里程碑管理的核心所在。</p><p></p><p>里程碑管理可以帮助我们跟踪进度，认识到自己当前的成就，并为接下来的努力提供方向。让我们在实现目标的长路上，能够清晰地看到每一个小目标的完成，从而保持动力和避免迷失。</p><p></p><p>通过设置清晰的里程碑，可以将长远目标分解为一系列短期目标，每一个短期目标的实现都能为个体带来成就感和激励，推动他们继续前进。这样，个体在追求梦想的过程中，不仅能够保持动力，还能确保自己始终沿着正确的方向前进。</p><p></p><p>从健身经历上也可以获取管理的灵感。要想拥有一个健康而健壮的身体，就要持续进行锻炼，每天至少完成 600 千卡的燃脂。要能够坚持锻炼并取得成果，就要形成 4E 理念：</p><p></p><p>Entice（吸引）：通过奖励徽章可以吸引我们参与运动，比如第一次跑步 3 公里后，就可以收获一个徽章。Encourage（鼓励）：一旦被吸引进来，通过发放连续完成运动的奖励可以鼓励我们持续锻炼。Endure（持久）：持久性是关键。一时的热情可以让我们跑 5 公里，但持续每天跑 3 公里才是真正的挑战。软件开发和其他任何事情一样，持续性至关重要。Evolve（升华）：经过长时间的坚持，就要克服新的挑战，不断层层加码，激励我们不断进步。</p><p></p><p>4E 原则不仅可以帮助我们锻炼身体，也可以激发我们的内驱力，帮助我们实现梦想。我们从中获取的管理灵感是：明确的目标、持续的激励、持久的努力和不断的挑战，是实现目标和激发内驱力的关键因素。</p><p>如何将健身中的管理理念应用到团队开发管理中？可以采取一种创新的方法，为每个团队成员“赋予”一块专属的数字化运动手表。这个概念并不是指真正的手表，而是一种比喻，指的是 为每个团队成员设定一个虚拟的、个性化的目标追踪和激励系统。</p><p></p><p>在这个系统中，设计不同等级的徽章，包括初级、中级和高级，以表彰不同的成就和进步。徽章系统适用于个人，也适用于团队。通过自动化的系统，可以根据团队成员的表现自动颁发徽章，这样既节省管理成本，也提高激励的及时性。</p><p></p><h2>三位一体数字化激励机制</h2><p></p><p></p><p>平安证券信息技术中心的管理模式是以 KPI 引导方向，以 OKR 管控过程，并辅以微徽章体系。</p><p></p><p>平安证券的 KPI 目标是盈利，包括经纪、固收、资管、投行和中后台等多个方面。为实现公司数字化转型，IT 主要是通过三大举措：一是开发分布式平台，二是实施数字化精细管理，三是进行组装式场景应用，以满足业务所需的功能，从而实现系统化建设和安全保障。</p><p></p><p>OKR，分为五个部分：一是有效支持新业务的发展，其中包括支持固收和资管等多个关键结果。二是通过数字化手段提高业务效率，创造业务机会，优化流程等。三是开发运维支持。四是建设数字化的开发运维管理体系。五是建立全方位、完整和健壮的系统平台。这些 OKR 中的 KR 进一步转化为团队的具体目标。</p><p></p><p>微勋章系统作为激励机制，负责管理达成目标过程中的里程碑。接下来重点介绍徽章体系。</p><p></p><p>在 IT 领域，传统观念中，开发人员常认为自己的代码最为出色，不倾向于使用他人的组件。微徽章系统的引入，旨在打破这一壁垒，鼓励开发人员采纳他人的成果。例如，开发人员若采用其他团队的组件并获得显著成效，他们将被授予徽章，并获得管理层的认可。</p><p></p><p>为缓解开发和测试团队间的常见矛盾，平安证券信息技术中心实施了以下策略：开发团队成员若能连续 5 个月交付零缺陷产品，将获得徽章。在测试团队中，若自动化测试率达到 80%，同样会获得徽章。此外，若管理界面组采用开发团队的组件，并在其基础上完成超过 10 个自动化测试卡片，开发团队将授予他们徽章。</p><p></p><p>通过这种方式，每个团队成员都被激励去确保交付的软件质量尽可能高，以赢得 QA 团队的认可。同时，QA 团队若能有效利用其他团队的功能，也将获得徽章。这样，团队成员不仅能赢得徽章，还有机会向其他团队颁发徽章。</p><p></p><p>徽章主要分为三个等级：第一级别的“出类拔萃”、第二级别的“持之以恒”以及第三级别的“奋发图强”。徽章的种类包括个人徽章、团队徽章，以及公有徽章和私有徽章。个人徽章针对个人表现，而团队徽章则奖励团队成就。公有徽章旨在促进跨团队协作，可在不同团队间颁发；私有徽章则仅限于团队内部颁发。想要晋升至更高级别的徽章，例如从“奋发图强”升级至“持之以恒”，需连续三个月获得“奋发图强”徽章。这种持续性的激励是整个激励体系的核心，旨在鼓励团队成员持续展现卓越表现和协作精神。通过这一机制，可以构建一个既促进个人发展又加强团队合作的激励框架。</p><p></p><h2>如何管理 OKR 和微徽章</h2><p></p><p></p><p>领航荣耀系统（Glory）是由平安证券自主研发，旨在将优秀规则融入流程，并将流程系统化，以支持 OKR 管理和微徽章管理。平安集团马总曾指出，优秀的规则应嵌入流程之中，而流程则应建立在系统之上，这一理念正是开发 Glory 系统的初衷。</p><p></p><p>Glory 系统的核心功能涵盖目标设定、奖励激励和结果跟踪。在这个平台上，可以管理 OKR 的设定、徽章的统计、管理和发布，以及徽章的展示。系统依靠数据平台和规则引擎的支持，确保了运行的高效性和准确性。</p><p></p><p>用户可通过 Glory 系统提交徽章申请并进行配置，选择相应的类目和规则引擎，设定发放徽章的具体条件。规则可能相当复杂，可根据月度、周度或其他指标来定制。系统还具备看板功能，能从多个维度展示个人和团队的徽章获取情况。</p><p></p><p>在微徽章管理方面，我们设计了一面荣誉墙，展示用户参与的类目和成就殿堂，记录下获得徽章的时间、原因和类型。为了有效管理徽章，借鉴了中国乒乓球运动员马龙的“六边形战士”理念，创建了“十边形战士”模型，将徽章归类到不同维度，如业务支持、自我提升、团队协作等。除此之外，还引入了平安集团的“三比”原则，即与自己比、与团队比、与部门比，利用雷达图的形式，让每个人都能直观地看到自己的成长和在团队中的地位。</p><p></p><p>Glory 系统支持根据不同角色（如开发、运维、管理）配置多边形模型，并为每个徽章设定权重，通过雷达图展现个人或团队在各个方面的表现。这种灵活的配置方式使得 Glory 系统能够迎合不同用户的需求，提供定制化的激励和管理体验。</p><p></p><p>目前，IT 团队已全面采用 OKR 和微徽章系统，实现了对整个部门的覆盖。荣耀系统已成功上线，并获得了 100% 参与率，涉及 26 个团队的成员，包括内部员工和外包人员。此外，徽章体系也已扩展至业务部门。</p><p></p><p>为提升 90 后和 00 后员工的工作热情，我们设计了一系列富有趣味的徽章。例如，针对因项目紧急坚持工作到深夜的员工，将颁发“夜猫子徽章”。在表格制作方面表现优异的员工，则会获得“表哥表姐徽章”。还有“健身王者”徽章，专为那些连续 9 个月每月体重下降 2% 及以上的员工设立。以及“水滴石穿”徽章，授予连续 9 个月每月至少健身打卡 15 次的员工。</p><p></p><p>微徽章系统不仅在 IT 团队中实施，同样也在业务部门，包括投行和销售团队中发挥了关键作用。投行销售人员的常规工作之一是拜访客户并记录交流情况，但过去他们对此的积极性不高。为解决这一问题，投行业务管理部门引入了微徽章体系，旨在激励销售人员增加客户拜访次数并详细记录交流内容。达到每月一定的拜访次数并提供清晰的记录的销售人员，将获得徽章。</p><p></p><p>自去年 9 月在投行推行微徽章体系以来，变化显著。数据显示，微徽章体系推行后的一个季度内，销售人员的客户拜访总数和报告撰写数量与前九个月的总和相当。</p><p></p><p>徽章带来的荣誉感和团队间的健康竞争对员工而言极为重要。并且，微徽章体系帮助投行业务解决了一个重要问题：如何增加客户拜访次数以及如何确保拜访信息的准确录入。今年，投行的微徽章体系已升级至 2.0 阶段，引入了更多种类的徽章，以进一步激励和认可员工的卓越表现。</p><p></p><h2>数字化激励机制能挖掘更多人才</h2><p></p><p></p><p>数字化激励机制保持目标和行动路径透明且统一，通过量化并奖励个体成果，激发人员内驱力，提升了人均产能和交付质量。在不同的视角下，该数字化激励机制都有其不同优势所在。</p><p></p><p>从员工的角度来看，采用了一种游戏化的管理模式，这种模式极大地迎合了新生代员工的需求。徽章体系为工作增添了类似游戏的积分和竞技元素，Entice（吸引）与 Encourage（鼓励）的方法尤其受到 90 后和 00 后员工的欢迎。员工在完成各项任务和挑战时，就如同在游戏中打怪升级。</p><p></p><p>从管理视角来看，在传统架构中，CIO 之下设有部门长、团队长和直属领导，员工要想引起注意并脱颖而出，需经过层层架构的认可，才有可能被推荐至高层。然而，这种多层级的结构有时会阻碍真正有才华的员工被上层管理者发现。</p><p></p><p>目前，穿透式管理的实施使得高层管理者能够直接进入系统，查看员工获得的徽章数量和在十边形战士模型中的表现。在评选最佳员工时，无需依赖部门长、团队长或直属领导的推荐，高层管理者可以依据系统数据直接作出选择。而在过去的管理模式下，如果员工在中间管理层级遇到障碍，可能无法获得应有的认可，有时甚至可能导致离职，这对公司而言是一大损失。</p><p></p><p>此外，通过团队间的比较，现在可以更有针对性地与团队领导进行沟通。例如，可以通过电子邮件向团队长指出其团队在共建方面与其他团队存在的差距，并建议其关注和改进。</p><p></p><p>徽章体系也有助于缓解开发和运维团队之间的矛盾。在过去，生产问题出现时，运维团队可能会向开发团队寻求帮助，而开发团队可能认为问题应由运维团队负责。如今，如果开发团队成员协助运维团队迅速解决问题，运维团队可以授予他们“救火队员”徽章，以表彰其支持和贡献。</p><p></p><p>值得一指的是，徽章体系采用了分布式架构，不由单一管理者决定徽章的颁发，也允许团队之间相互颁发。</p><p></p><p>例如，当 IT 团队推出新版本或新功能时，期望业务人员能够迅速采纳并逐步弃用旧功能。如果业务人员积极参与并有效运用新功能，IT 团队会授予他们徽章。起初，业务人员可能对徽章不太重视，但当他们发现管理者开始关注徽章数量，并询问为何 IT 团队颁给他们的徽章较少时，他们就开始更在意了。我们可以告知业务人员，他们若按要求使用新功能，徽章便会自动发放给他们。通过这一模式，不仅加强了业务与 IT 团队之间的联系，也促使业务人员更积极地参与 IT 项目，共同推进业务的发展和创新。</p><p></p><h2>总结</h2><p></p><p></p><p>在平安证券，KPI、OKR 与微徽章三位一体的综合管理体系已被采纳。微徽章分为初级、中级和高级三个级别，这套体系着重于目标的具体化与正向激励，通过数据驱动、持续性、挑战性以及及时奖励性，客观地推动员工朝正确目标迈进。平安证券的目标是采用穿透式管理，迎合新生代员工的需求，增强他们的内驱力，提升团队协作效率，增加人均产能和交付质量，优化业务与 IT 的关系，并实现数字化目标。</p><p></p><p>同时，徽章体系也会不断进行优化和更新。例如，本季度可能鼓励员工熟悉新的业务功能，而下一季度则可能关注不同的重点。微徽章体系将依据业务需求和周期进行相应调整，以推动数字化管理进程，并通过 OKR 自下而上地分解任务，最终实现公司的 KPI 目标。</p><p></p><h5>活动推荐</h5><p></p><p>8 月 16-17 日，FCon 全球金融科技大会将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，致力于展示金融数字化在“十四五”期间的关键进展，帮助金融机构在“交卷”前更具针对性地“查缺补漏”。</p><p></p><p>大会还邀请了来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家，现身说法分享其在金融科技应用实践中的经验与深入洞察，分享近一年来金融行业 AI 大模型的落地实践经验和成果。</p><p></p><p>大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 17310043226 咨询。</p><p><img src="https://static001.geekbang.org/infoq/fb/fbae75544e8915692749ea1aa9c688f2.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/swAxVqSHpFRQLMTaKHCW</id>
            <title>Claude Sonnet 3.5 口碑爆棚！10倍速开发，“2个月内用Rust 从零构建完一款产品”</title>
            <link>https://www.infoq.cn/article/swAxVqSHpFRQLMTaKHCW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/swAxVqSHpFRQLMTaKHCW</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jul 2024 09:42:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: lapurita, Claude Sonnet 3.5, 开发速度, 大模型
<br>
<br>
总结: lapurita 发现使用 Claude Sonnet 3.5 可以让开发速度提升10倍，大模型的使用让他能够更快地实现市面上热门应用的技术部分。他强调了与Claude合作的工作流程和重要性，以及对应用程序架构的了解对于使用大模型的重要性。其他开发者也纷纷转向使用Claude，认为大模型编写代码的速度和效率都得到了显著提升。 </div>
                        <hr>
                    
                    <p></p><p>昨天，技术创始人 lapurita 关于“使用 Claude Sonnet 3.5实现了10倍开发速度”的帖子火了。</p><p>&nbsp;</p><p>“我震惊地发现，原来Sonnet 3.5 可以让开发速度变得这么快！”lapurita 说道。“我没有夸大所有大模型，因为这是第一个让我真正用起来感到舒适的大模型。我可以比之前快10倍地实现市面上大部分热门应用的技术部分。我仍然需要做架构和基础设施的决策，但像编写UI组件这样的事情，现在真的比之前快了10倍，这让迭代速度变得非常快。”</p><p>&nbsp;</p><p>或许 lapurita 的说法引起了开发者共鸣，大家纷纷转发赞同lapurita 的说法。一时间，OpenAI 竞争对手的 Claude 模型风头无两。</p><p>&nbsp;</p><p></p><h2>不止10倍？</h2><p></p><p>&nbsp;</p><p>根据 lapurita 的介绍，现在他开发一个功能的工作流程基本上是：</p><p>&nbsp;</p><p>深入思考功能，也可能会与Claude一起讨论；编写基本规格（通常只是一些句子和要点），并与Claude一起迭代；确保为Claude提供所有相关的上下文，并请求代码实现。</p><p>&nbsp;</p><p>lapurita 介绍，他会先在Claude中上传相关文件并创建相关项目，其中最重要的文件是其称之为“main context”的文件，该文件非常明确地指定了应用程序当前正在做什么以及在下一个版本中应该做什么。lapurita 还指定了所有的技术决策以及选择它们的原因，同时解释了希望Claude遵循的更具体的代码设计模式（例如如何保持服务器状态和客户端状态同步）。lapurita 还有一个包含整个数据库模式，以及一些示例 API 端点的文件。这些文件基本上总结了迄今为止关于项目的所有信息。</p><p>&nbsp;</p><p>在Claude 的“项目”中，用户可以创建多个对话。lapurita 给到的一个技巧提示是，在开始一个新功能时就建立多个对话，否则上下文窗口会因为无关紧要的东西而变得杂乱，从而占用消息限制。开始一个新对话时，“main context”文件就非常重要。</p><p>&nbsp;</p><p>lapurita 提到的一个例子是前几天他为内容创建的一个类似 Instagram Reels/TikTok 的 feed 流。“这并不是什么火箭科学，但我对 SwiftUI 没有太多经验，这里有一些半高级的动画/布局的东西，但我与 Claude 做出一个完全可用的实现（符合我的 API 规范并与实际数据库合作）只需要20分钟。重要的是，生成的代码遵循了我描述的模式，并且与我代码库中的其他部分一致（所以这实际上是我会写出来的代码，只是加速了），而这是我在使用其他模型时会遇到的问题。”</p><p>&nbsp;</p><p>lapurita 认为，使用者非常了解应用程序的架构，包括大体架构和更具体的代码（比如如何处理数据获取的设计模式等）是非常重要的。如果没有这方面的经验，而只是使用Claude，代码库很可能会变得过于混乱和复杂，导致之后难以修改。</p><p>&nbsp;</p><p>“这是我之前遇到过的陷阱，我认为这也是那些仍然抗拒将大模型用于自动化以外用途的程序员会遇到的问题。”lapurita 表示，发生上述情况时，开发者不可避免地会想自己应该从一开始就自己编程。但如果开发者始终引导Claude按照自己的意愿行事，并跟上和理解生成的代码，这种情况就不会发生。</p><p>&nbsp;</p><p>“跟上Claude给出的代码非常重要，有时我一整个会话都只是阅读生成的代码，这样我就能有像自己写出来的代码一样的感觉。”lapurita 说道。</p><p>&nbsp;</p><p>这种构建产品方式的本质是尝试围绕新的软件生产方式调整开发人员工具和流程。当前，不断来回引导大模型做开发者真正想做的事情、缺乏处理部署等能力是这种开发方式的新瓶颈。</p><p>&nbsp;</p><p>“实际上，我认为即使 Sonnet 3.5 没有进一步发展，只要将其‘正确’集成（而不仅仅是放入聊天框）到我们用于生产软件的其他东西，我们就可以从 10 倍提高到 20-50 倍。”开发者 Fred Weitendorf表示。</p><p>&nbsp;</p><p>Weitendorf指出，确实必须能够“缩小范围”才能避免产生一团乱糟糟的东西，但更难的问题是，使用者仍然必须知道要指定什么。</p><p>&nbsp;</p><p>作为一名经验丰富的程序员，lapurita 对即将编写的代码的总体结构有着强烈的直觉，这就是为什么他基本上可以将 sonnet 3.5 当作“编译器”来使用。但缺乏经验的人是通过反复试验来编写软件，并且不太善于表达自己想要的东西，所以他们不能以这种方式使用 Claude，否则可能还会减慢他们的速度。</p><p>&nbsp;</p><p>此外，即使是经验丰富的工程师也很难写出好的提示，这也成为大模型构建产品时的阻碍。</p><p>&nbsp;</p><p>lapurita 指出，他的使用经验对初创公司非常适用，但对大公司来说就不是这样了。“在我所在的公司，虽然大模型仍然有所帮助，但远不如在构建新产品时那么有用。我认为，主要是因为我无法获得相同的架构概述，因此很难为大模型提供所有相关上下文。”</p><p>&nbsp;</p><p>但无论如何，lapurita 对这个工具非常满意，因为它让自己可以专注于应用程序更困难的部分。</p><p>&nbsp;</p><p>EverArt创始人 Pietro Schirano 转发了lapurita 的帖子并称，他第一次创业，9 个月内每月收入 10 万美元，是“Sonnet 3.5 改变了一切。”</p><p>&nbsp;</p><p>开发者 Sully Omarr 也转发帖子并表示，“我们 50% 的代码库完全由大模型编写，预计到明年这个比例将达到约 80%。有了Sonnet ，我们的交付速度非常快，感觉我们的员工人数一夜之间增加了三倍。不使用 Claude 3.5 编写代码？那估计会被使用 Claude 3.5 的团队击败（比如我们）。” 他认为，2-3 年内大模型编写的代码会被抽象出来，但开发者仍然需要知道如何编写代码。</p><p>&nbsp;</p><p></p><h2>“GPT-4 不再是最好的模型”</h2><p></p><p>&nbsp;</p><p>“我是 GPT 用户，我应该切换到 Claude 吗？”帖子下面有人问到。“是的，它使编码变得简单得多。”有网友直接回复。</p><p>&nbsp;</p><p>不得不说，有一批用户已经开始转向了Claude。“我取消了一年多前订阅的 GPT-4 订阅，改成订阅 Claude。没有手机应用程序，也没有GPTs或自定义说明（在网络版本中 - 不使用 API）。但老实说，我并不关心这些。我主要用它写作和集思广益，Claude 3（甚至 Gemini）的表现优于 GPT。”</p><p>&nbsp;</p><p>如今，GPT-4o的使用者也在动摇：“Claude 真的比 GPT-4o 好很多吗？我之前用过 Claude Opus但印象并不深刻，而且我还使用 OpenAI API。除非真的值得，否则我不想同时为这两项服务付费，我现在整天都在使用 GPT。”</p><p>&nbsp;</p><p>“如果你擅长编码提示，那么 Claude Sonnet 3.5 绝对适合。”这是该网友得到的回答。</p><p>&nbsp;</p><p>相信很多人已经对 OpenAI 与 Anthropic 之间的竞争故事有所了解：Anthropic 七位联合创始人此前都曾在 OpenAI 工作过。Anthropic 首席执行官 Dario Amodei 还曾担任 OpenAI 的研究副总裁，他甚至撰写了OpenAI&nbsp;章程的大部分内容，这份文件承诺实验室及其员工将致力于安全开发强大的人工智能。</p><p>&nbsp;</p><p>Claude系列模型在开发人员中的好口碑也不是一天两天了。在 Claude 3&nbsp;发布不久后，工程师 Singularity就称，“Claude 3 非常出色，实际上能生成出比 ChatGT 质量更好的代码。”</p><p>&nbsp;</p><p>Singularity 指出，Claude有比GPT更好的上下文能力。“我可以将我的文件输入 Claude并告诉它进行更改，它甚至会记住这些文件中的代码并记住我们所做的更改，在被告知调用一个非常古老的代码片段后，它可以完美地实现调用。”</p><p>&nbsp;</p><p>根据介绍，Claude 3模型将其前代的上下文窗口大小翻倍，为用户提供20万个token的上下文窗口，相当于大约15万个单词。Claude 3 Opus 模型在特定用例下还支持高达100万个token的输入。</p><p>&nbsp;</p><p>其次，Singularity 表示，Claude 在各种语言上的表现也更好。“我讨厌的一件事是每个人都一直用 Python 测试它，这证明 Python 并没有那么难。我用 Rust、Go、Haskell 和 C++ 编写代码， Claude 的 Rust 能力比 GPT-4 好太多，GPT 对 Rust 几乎无能为力。两者在 Go 上差不多，在 C++ 和 Haskell 上，Claude 比 GPT-4 要好。”</p><p>&nbsp;</p><p>开发者 joowani 在lapurita最新的帖子下面也有这方面的表达，“我使用 Copilot 和 Claude Sonnet 3.5，它们极大地帮助我学习 Rust，并在短短 2 个月内从头开始构建了市场上最快的产品。”</p><p>&nbsp;</p><p>Singularity 还指出Claude 有比GPT更少的幻觉。“我厌倦了 OpenAI 粉丝们对 Claude 的轻视。它真的非常好，连&nbsp;Sonnet&nbsp;都很好。它在代码中做得较少的一件事是产生幻觉，当然它仍然有，但远不如 GPT-4那么多。GPT- 4 编造一些疯狂的函数，即使你告诉它不存在，它还是会这样做。Claude 也会给出不存在的函数，但会倾听下一个提示词。”</p><p>&nbsp;</p><p>“GPT-4 不再是最好的模型，这是事实。”有网友在5月前的帖子下评论道。现在，越来越多的网友开始展示自己使用Claude 的成果。</p><p>&nbsp;</p><p>开发者 Dave 展示了自己用 Claude 3.5 Sonnet 的构建成果，内部工具Voice Notes AI 一共1294 行代码，仅花了Dave两个小时的时间：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/0513958f03e5e8887e6d2745f757b969.png" /></p><p></p><p></p><p>还有网友展示了自己用Claude 3.5 Sonnet 仅花了 2 分钟的时间就从一张截图创建了功能齐全的 ChatGPT 克隆版。在最近的微软蓝屏事件中，AIPRM Corp首席工程师 <a href="https://x.com/btibor91">Tibor Blaho</a>" 展示了用Claude 制作的非 Windows 用户的 Crowdstrike Falcon BSOD 屏幕。</p><p>&nbsp;</p><p></p><p></p><p>&nbsp;</p><p>此外，还有网友表示在向 Sonnet-3.5 提出了一个愚蠢的问题后，它突然不再认真回答，而是开始开玩笑。他表示这种行为从未在 GPT-4 上见过：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c95ca67624c60466e3bb48fe46d1b010.png" /></p><p></p><p>&nbsp;</p><p>反观现在的OpenAI，万众期待的GPT-5 难产，发力方向也比较“多元”，比如被解读为加入价格战的代表GPT-4o mini等。这不免让一些网友担心：OpenAI 是否会在绝对优势下，逐渐丢失积攒的好口碑呢？</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://old.reddit.com/r/ycombinator/comments/1e7rtdw/feeling_very_powerful_as_a_technical_founder_with/">https://old.reddit.com/r/ycombinator/comments/1e7rtdw/feeling_very_powerful_as_a_technical_founder_with/</a>"</p><p><a href="https://x.com/minchoi/status/1815024013812416567">https://x.com/minchoi/status/1815024013812416567</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1b6Xd3kk6Ll26NmkGO89</id>
            <title>ECCV 2024｜有效提高盲视频去闪烁的新方法——BlazeBVD</title>
            <link>https://www.infoq.cn/article/1b6Xd3kk6Ll26NmkGO89</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1b6Xd3kk6Ll26NmkGO89</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jul 2024 08:16:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 短视频生态, 视频画质修复, 盲视频去闪烁, BlazeBVD
<br>
<br>
总结: 近年来，围绕短视频的创作编辑工具不断涌现，美图公司的Wink凭借视频画质修复能力在市场上占据领先地位。BlazeBVD是一种针对视频闪烁场景的新方法，通过STE和直方图辅助解决方案提高盲视频去闪烁的效果。该方法结合了全局和局部闪烁去除模块，以及轻量级的时序网络，取得了优越的实验结果。 </div>
                        <hr>
                    
                    <p>近年，短视频生态的赛道迅猛崛起，围绕短视频而生的创作编辑工具在不断涌现，美图公司旗下专业手机视频编辑工具——Wink，凭借独创的视频画质修复能力独占鳌头，海内外用户量持续攀升。Wink画质修复功能火爆的背后，是美图在视频编辑应用需求加速释放背景下，对用户视频画面模糊不清、噪点严重、画质低等视频创作痛点的洞察，与此同时，也建立在美图影像研究院（MT Lab）强有力的视频修复与视频增强技术支持下，目前已推出画质修复-高清、画质修复-超清、画质修复-人像增强、分辨率提升等功能。日前，美图影像研究院（MT Lab）联合中国科学院大学更突破性地提出了基于STE的盲视频去闪烁(blind video deflickering, BVD)新方法BlazeBVD，用于处理光照闪烁退化未知的低质量视频，尽可能保持原视频内容和色彩的完整性，已被计算机视觉顶会ECCV 2024接收。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e853f60defca0a98ea53a9b80060d5cd.png" /></p><p></p><p>&nbsp;</p><p>论文链接：<a href="https://arxiv.org/pdf/2403.06243v1">https://arxiv.org/pdf/2403.06243v1</a>"</p><p>&nbsp;</p><p>BlazeBVD针对的是视频闪烁场景，视频闪烁容易对时间一致性造成影响，而时间一致性是高质量视频输出的必要条件，即使是微弱的视频闪烁也有可能严重影响观看体验。究其原因，一般是由拍摄环境不佳和拍摄设备的硬件限制所引起，而当图像处理技术应用于视频帧时，这个问题往往进一步加剧。此外，闪烁伪影和色彩失真问题在最近的视频生成任务中也经常出现，包括基于生成对抗网络(GAN)和扩散模型(DM)的任务。因此在各种视频处理场景中，探索通过Blind Video Deflickering (BVD)来消除视频闪烁并保持视频内容的完整性至关重要。</p><p>&nbsp;</p><p>BVD任务不受视频闪烁原因和闪烁程度的影响，具有广泛的应用前景，目前对此类任务的关注，主要包括老电影修复、高速相机拍摄、色彩失真处理等与视频闪烁类型、闪烁程度无关的任务，以及仅需在单个闪烁视频上操作，而不需要视频闪烁类型、参考视频输入等额外指导信息的任务。此外，BVD现主要集中在传统滤波、强制时序一致性和地图集等方法，所以尽管深度学习方法在BVD任务中取得了重大进展，但由于缺乏先验知识，在应用层面上受到较大阻碍，BVD仍然面临诸多挑战。</p><p></p><p></p><h2>BlazeBVD: 有效提高盲视频去闪烁效果</h2><p></p><p>&nbsp;</p><p>受经典的闪烁去除方法尺度时间均衡(scale-time equalization, STE)的启发，BlazeBVD引入了直方图辅助解决方案。图像直方图被定义为像素值的分布，它被广泛应用于图像处理，以调整图像的亮度或对比度，给定任意视频，STE可以通过使用高斯滤波平滑直方图，并使用直方图均衡化校正每帧中的像素值，从而提高视频的视觉稳定性。虽然STE只对一些轻微的闪烁有效，但它验证了：</p><p>(1)直方图比像素值紧凑得多，可以很好地描绘光亮和闪烁信息。</p><p>(2)直方图序列平滑后的视频在视觉上没有明显的闪烁。</p><p></p><p>因此，利用STE和直方图的提示来提高盲视频去闪烁的质量和速度是可行的。</p><p>&nbsp;</p><p>BlazeBVD通过对这些直方图进行平滑处理，生成奇异帧集合、滤波光照图和曝光掩码图，可以在光照波动和曝光过度或不足的情况下实现快速、稳定的纹理恢复。与以往的深度学习方法相比，BlazeBVD首次细致地利用直方图来降低BVD任务的学习复杂度，简化了学习视频数据的复杂性和资源消耗，其核心是利用STE的闪烁先验，包括用于指导消除全局闪烁的滤波照明图、用于识别闪烁帧索引的奇异帧集，以及用于识别局部受过曝或过暗影响的区域的曝光图。</p><p>&nbsp;</p><p>与此同时，利用闪烁先验，BlazeBVD结合了一个全局闪烁去除模块(GFRM)和一个局部闪烁去除模块(LFRM)，有效地矫正了个别相邻帧的全局照明和局部曝光纹理。此外，为了增强帧间的一致性，还集成了一个轻量级的时序网络(TCM)，在不消耗大量时间的情况下提高了性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/85/85fa7d61ec096585aff2a7fad46d40dd.png" /></p><p></p><p>&nbsp;</p><p>图1：BlazeBVD方法与已有方法在盲视频去闪烁任务上的结果对比</p><p>&nbsp;</p><p>具体而言，BlazeBVD包括三个阶段：</p><p>首先，引入STE对视频帧在光照空间下的直方图序列进行校正，提取包括奇异帧集、滤波后的光照图和曝光图在内的闪烁先验。</p><p>其次，由于滤波后的照明映射具有稳定的时间性能，它们将被用作包含2D网络的全局闪烁去除模块(GFRM)的提示条件，以指导视频帧的颜色校正。另一方面，局部闪烁去除模块(LFRM)基于光流信息来恢复局部曝光图标记的过曝或过暗区域。</p><p>最后，引入一个轻量级的时序网络(TCM)来处理所有帧，其中设计了一个自适应掩模加权损失来提高视频一致性。</p><p></p><p>通过对合成视频、真实视频和生成视频的综合实验，展示了BlazeBVD优越的定性和定量结果，实现了比最先进的模型推理速度快10倍的模型推理速度。&nbsp;</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e0/e0063c3553f387cd4566938cac16ed7e.png" /></p><p></p><p>&nbsp;</p><p>图2：BlazeBVD的训练和推理流程</p><p></p><h2>实验结果</h2><p></p><p></p><p>大量的实验表明，盲视频闪烁任务的通用方法——BlazeBVD，在合成数据集和真实数据集上优于先前的工作，并且消融实验也验证了BlazeBVD所设计模块的有效性。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a7/a70c46c4bcac37127f6c85365c08bcff.png" /></p><p></p><p>&nbsp;</p><p>表1：与基线方法的量化对比</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b46991f54a3c6b150aa7f49ad4f22b4.png" /></p><p></p><p>&nbsp;</p><p>图3：与基线方法的可视化对比</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fc57bcdb671f4620ade8acecd10cc77.png" /></p><p></p><p>&nbsp;</p><p>图4：消融实验</p><p></p><h2>以影像科技助力生产力</h2><p></p><p>&nbsp;</p><p>该论文提出了一种用于盲视频闪烁任务的通用方法BlazeBVD，利用2D网络修复受光照变化或局部曝光问题影响的低质量闪烁视频。其核心是在照明空间的STE滤波器内预处理闪烁先验；再利用这些先验，结合全局闪烁去除模块(GFRM)和局部闪烁去除模块(LFRM)，对全局闪烁和局部曝光纹理进行校正；最后，利用轻量级的时序网(TCM)提高视频的相干性和帧间一致性，此外在模型推理方面也实现了10倍的加速。</p><p>&nbsp;</p><p>作为中国影像与设计领域的探索者，美图不断推出便捷高效的AI功能，为用户带来创新服务和体验，美图影像研究院（MT Lab）作为核心研发中枢，将持续迭代升级AI能力，为视频创作者提供全新的视频创作方式，打开更广阔的天地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/weuUMQF6oHChAiVrqTvP</id>
            <title>零售效率与体验双轮驱动下，AGI足够跨越用户接受度与成本的关卡了吗？ | 分析师研判</title>
            <link>https://www.infoq.cn/article/weuUMQF6oHChAiVrqTvP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/weuUMQF6oHChAiVrqTvP</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jul 2024 02:08:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AGI, 生成能力, 零售行业, 智能客服
<br>
<br>
总结: 2024年5月，InfoQ研究中心发布了《中国AGI市场发展研究报告2024》，深入分析了AGI在零售行业的应用现状和典型场景。AGI在零售行业的应用主要围绕效率提升和体验优化，包括生成能力、智能客服等方面。然而，用户接受度和投入产出比仍然是阻碍零售行业AI落地的挑战。零售行业需要在平衡成本和效果的基础上，不断探索AI技术在零售场景中的应用。 </div>
                        <hr>
                    
                    <p>2024&nbsp;年&nbsp;5&nbsp;月，InfoQ研究中心围绕&nbsp;AGI&nbsp;的发展历程、市场规模、技术架构和五大行业&nbsp;50+&nbsp;应用场景现状进行分析和广泛访谈，发布了<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">《中国AGI市场发展研究报告&nbsp;2024》</a>"。围绕文章，InfoQ研究中心已产出&nbsp;2&nbsp;篇系列文章，分别对中国&nbsp;AGI&nbsp;市场规模和五大行业应用现状等观点进行了解读。本篇文章将继续以零售行业为例，深入拆解零售行业&nbsp;AGI&nbsp;的应用现状和典型应用场景。欢迎各位读者，点击「阅读原文」或者扫描下方报告下载的二维码，进行完整报告下载。</p><p></p><h3>生成能力成为零售行业应用落地“先锋队”</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce1208d4f13ae790bcdc850e2bf81bfa.png" /></p><p></p><p>整体上来看，AGI&nbsp;在零售行业的应用已经走过了初期的探索阶段，目前正处于快速发展和市场投放期。</p><p></p><p>在新一轮AI的浪潮下，生成式&nbsp;AI&nbsp;带来的生成能力的升级，在零售行业促进了&nbsp;AI&nbsp;商拍等全新场景的诞生和发展，极大地缩短了商品上架前的准备时间，为广大中小商家提供了商品拍摄的新思路。此外，交互式的后期编辑，也提供了更直观的覆盖人脸编辑、风格转换、图像修复、局部重绘、背景切换、后期调色等的后期修改流程。同时，在商品海报、视频广告等营销物料的衍生上，AI&nbsp;还能根据不同平台特性和消费者偏好，智能生成多样化的营销物料。</p><p></p><p>对话式的交互方式则在智能客服、数字人导购/主播等场景广泛使用。智能客服系统能够&nbsp;24&nbsp;小时不间断地为客户提供咨询、解答疑问，极大地缓解了人工客服的压力。同时，在意图识别能力和长上下文理解能力的加持下，相较于上一代智能客服系统，大模型驱动下的智能客服能够提供更自然的对话过程和更贴近消费者的回答风格，不断优化服务流程。</p><p></p><p>除此之外，基于&nbsp;AI&nbsp;Agent&nbsp;搭建的平台商家助手、智能投放助手和零售门店管理正在成为零售行业升级的重要应用探索。</p><p></p><p>零售行业&nbsp;AGI&nbsp;应用厂商图谱</p><p><img src="https://static001.geekbang.org/infoq/26/269544a70e4d3fff593885facc7d3f47.png" /></p><p></p><h3>零售场景两大升级源动力：效率提升与体验优化</h3><p></p><p></p><p>总体来看，AGI&nbsp;在零售行业的应用主要围绕效率提升和体验优化。</p><p></p><p>效率提升方面，AI&nbsp;商拍、局部生成和交互后期都能够帮助商家快速迭代商品图和营销物料，提升运营效率。广告投放智能体的出现，使得商家在广告投放方面有了更多的可能性。这使得商家可以通过自然语言表达投放诉求，智能体根据投放诉求可以智能规划投放节奏、智能匹配营销物料、总结投放效果和提供下一步优化建议。</p><p></p><p>从使用者的角度分析，消费者体验是非常重要的，无论是前端消费者还是电商平台的商家体验，以及零售门店的店员店长体验，这些都构成了一整套完整的用户体验体系。对于消费者的体验优化，前文中已阐述较多。基于&nbsp;AI&nbsp;Agent&nbsp;构建的平台商家助手，可以帮助商家了解平台的现行规则、各项营销活动的解读，并提供优化经营策略。零售门店管理助手智能体，则更多承担了数据分析的工作，可以帮助门店店员解读各项商品活动、并提供产品营销和库存管理建议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2cf15a4d432dda1216c4a8e65e816ae.png" /></p><p></p><p></p><h3>用户接受度和投入产出比仍然是阻碍零售行业&nbsp;AI&nbsp;落地的关卡</h3><p></p><p></p><p>零售场景下，幻觉问题会严重破坏用户信任和用户体验。商家也可能会因为生成错误的营销活动信息而蒙受损失。除此之外，全球研究与咨询公司Gartner日前发布了一项关于客服系统应用AI技术的调查报告。该报告显示，在接受调查的受访者中，有高达64%的人表示不希望客服系统部署AI技术。这也意味着目前即使是大模型驱动的智能客服，用户接受度仍然是阻碍零售行业AI&nbsp;落地的重要挑战。</p><p></p><p>此外，在落地时，零售行业也逃离不了对于成本和效果之间的权衡。除了过往系统升级改造的成本，本身大模型和生成式AI的部署和维护成本也不容忽视。同时，技术人员的培训、系统的调试和优化也需要持续的投资。现阶段，大小模型并行仍然是部分零售企业主要探索的路径，以在场景应用中，更好地发挥大小模型的不同优势。此外，也有部分企业开始探索大模型作为主Agent规划任务，调用小模型作为工具使用、同时搭载记忆模块的智能体应用。</p><p></p><p>更多关于中国&nbsp;AGI&nbsp;发展历程、市场规模、技术架构等内容，欢迎大家点击<a href="https://www.infoq.cn/minibook/6WyXxdu179Di1O75JPUM">《中国AGI市场发展研究报告&nbsp;2024》</a>"或者扫描下方「报告下载」的二维码，下载完整报告阅读。同时，您也可以关注「AI前线」，回复「报告」，获取报告合集。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d31b639fb5f4972f121fee38ae443da.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/GyfwgkzS0Q8xN7VzZ9gC</id>
            <title>RAG 技术真的“烂大街”了吗？</title>
            <link>https://www.infoq.cn/article/GyfwgkzS0Q8xN7VzZ9gC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/GyfwgkzS0Q8xN7VzZ9gC</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jul 2024 10:47:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: RAG 技术,信息检索技术,大语言模型技术,知识理解
<br>
<br>
总结: 大语言模型技术与传统信息检索技术相结合的 RAG 技术在知识理解和知识获取方面提供新解决方案，但在深度应用中仍面临诸多挑战。在讨论中，嘉宾分享了 RAG 技术在不同领域应用中遇到的问题和解决方案，包括数据多样性、问答质量、用户意图不明确等瓶颈。他们认为解决这些问题是让企业愿意付费使用 RAG 技术的关键。 </div>
                        <hr>
                    
                    <p></p><p>嘉宾｜郭瑞杰、欧明栋 、张颖峰 、常扬</p><p>作者｜Kitty</p><p>审校 | 蔡芳芳</p><p></p><p>大语言模型技术迅猛发展的脚步，正引领着信息检索技术进入一个新的纪元。在这一领域中， RAG &nbsp;技术将传统信息检索技术与大语言模型技术相结合，为知识理解、知识获取提供了全新的解决方案。然而，尽管 RAG 在很多任务上表现出色，其在深度应用上仍面临诸多挑战。</p><p></p><p>在日前的 InfoQ 《极客有约》X AICon 直播中，我们邀请到了阿里巴巴总监 &amp; TGO 鲲鹏会学员郭瑞杰、 阿里云高级算法专家欧明栋 、英飞流 CEO 张颖峰、合合信息研发总监常扬 ，深入探讨 RAG 技术的当前进展、面临的挑战、未来的发展方向以及在不同行业中的应用潜力。</p><p></p><p>部分精彩观点如下：</p><p></p><p>数据杂乱、用户意图明确时的低命中率，以及用户意图不明确时的语义 gap，是阻碍 RAG 技术走向更多企业、让企业愿意为之付费的主要瓶颈；解决掉 &nbsp;RAG 最基础、最本质的问题，是出现爆款产品的基础；长上下文模型和 RAG 之间不应是冲突关系，而应是合作关系；如果“烂大街”代表 &nbsp;RAG 技术理解和使用门槛降低，这是一件好事；RAG 加 Agent 的本质是复杂问题的分治。</p><p></p><p></p><blockquote>在 8 月 18-19 日将于上海举办的 AICon 全球人工智能开发与应用大会上，郭瑞杰老师将出品【<a href="https://aicon.infoq.cn/2024/shanghai/track/1705">RAG &nbsp;落地应用与探索</a>"】 专题，深入探讨 RAG 的最新进展、成果和实践案例，详细分析面向 RAG 的信息检索的创新方法，包括知识抽取、向量化、重排序、混合检索等在不同行业和场景下的微调和优化方法。而欧明栋、张颖峰、常扬老师也将在专题论坛上带来精彩分享。大会更多精彩议题已上线，欢迎查看目前的<a href="https://aicon.infoq.cn/2024/shanghai/schedule">专题安排</a>"。</blockquote><p></p><p></p><p>以下内容基于直播速记整理，经过不改变原意的编辑。</p><p></p><h2>RAG &nbsp;应用现状、挑战和潜在影响</h2><p></p><p></p><p>郭瑞杰： 我们进入今天第一个讨论的话题， RAG &nbsp;技术在不同领域 / 不同场景的应用现状、挑战和潜在影响。 今天几位老师也是来自不同的领域不同场景，首先有请欧明栋老师分享下 RAG 在阿里云 AI 搜索实际应用场景中碰到的问题及尝试的解决方案。</p><p></p><p>欧明栋：我们是 AI 搜索 Paas 平台，也构建了端到端全链路 RAG 能力，面向各行各业的客户。这些客户主要来自互联网、医疗、电商和金融等多个领域，他们的应用场景相当多样化，包括客服、推荐、文案生成以及数据分析等需求。</p><p></p><p>在服务过程中，我们发现最大的挑战在于如何将服务真正落地到客户的生产环境中。这主要是因为技术问题，尤其是客户数据的多样性。客户的数据格式各不相同，包括 PDF、DOC、纯文本，甚至 PPT 等。即便是相同格式的数据，不同客户的排版也大相径庭。这导致文档解析的准确性受到影响，例如文档结构未能正确解析，或者文字、内容、表格等丢失，最终影响了问答效果。此外，某些行业对大语言模型的精度和错误率有较高要求，希望控制模型的幻觉问题，特别是在医疗和政务领域，因为错误可能导致严重后果。然而，大模型的幻觉问题本质上是一个难以控制的缺陷。</p><p></p><p>在实际使用中，许多用户的问题并非简单直接，而是需要经过多步推理和检索才能得到答案的复杂问题。例如，多跳问题、意图不清晰需要澄清的问题，以及答案在文档中跨度较大的问题。目前，这些问题在传统的 Web 框架下解决得并不理想。RAG 系统提供了一种比传统搜索更进一步的解决方案。与以往需要人机协同、根据机器反馈进行调整的搜索方式不同， RAG 系统能够直接基于问题给出准确答案，大大减少了人工工作量。我们期望 RAG 系统能够在后续应用中进一步提高能力。</p><p></p><p>张颖峰： 我们对 RAG 技术的认知大致相同，可以认为 RAG 的应用分为两个阶段。最初，大约是去年，是 RAG 技术的普及期。在这个阶段，业界对使用 RAG 还是微调存在争议。我们的主要任务是将 RAG 的各个组件以尽可能易用的方式，通过 Ops 手段快速整合，以便让更多企业和个人能够搭建起这个系统。LLMOps 工具在 RAG 的普及中起到了重要作用。然而，我们也遇到了一些痛点，尤其是大模型本身的瓶颈问题，去年大模型的能力差距较大，模型的幻觉问题非常突出。</p><p></p><p>第二个阶段，我认为是从今年开始，是 RAG 加速普及的时期。根据我们接触的多个行业和客户，企业普遍认识到 RAG 技术的必要性。但在实际应用中，痛点依然很多。很少有企业愿意为一套基于开源的 RAG 系统付费。这也是我们愿意开发新的 RAG 平台的主要原因，因为瓶颈问题很多。</p><p></p><p>我们总结的瓶颈主要有以下几点：</p><p></p><p>第一，当前影响最大的是数据问题。杂乱无章的数据输入，如 PPT、Excel 表格、Word 文档、PDF 等，如果不合理处理，最终结果会非常糟糕。第二，问答质量也是一个痛点。如果用户意图明确，当前的 RAG 方案仍然无法提供高召回率或精度，即命中较低。企业通常需要 90% 以上的召回率才可能付费。第三，如果用户意图不明确， RAG 系统找到的相似问题，而不能直接作答，存在语义 gap，简单的检索方式无法找到答案。此外，长文档问答和多跳推理等问题也与此相关。</p><p></p><p>此外还有一些较小的瓶颈，比如幻觉问题。去年幻觉问题比较严重，但今年有所改善，因为大家对大模型的期待降低，不再指望它能回答高深的逻辑推理问题。我们更倾向于让模型回答总结性、摘要性的问题，这种情况下大模型的表现还可以。但许多 RAG 系统没有充分利用 RAG 的最大优势，即根据搜索结果生成有理有据的答案。如果模型没有生成引用，就无法提供有理有据的答案，这影响了用户体验。这个技术点相对较小，相比前面两点，对 RAG 技术的应用和推广影响不大。</p><p></p><p>目前我们面临的主要挑战包括数据杂乱、用户意图明确时的低命中率，以及用户意图不明确时的语义 gap，这些都是阻碍 RAG 技术走向更多企业、让企业愿意为之付费的主要瓶颈。</p><p></p><p>常扬： 从是否选择 RAG 的角度来看，我们首先需要从第一性原理出发，理解 RAG 的本质。RAG 通过利用外部数据源和文档知识，解决了大模型在偏见、幻觉、安全性以及实时性问题上的不足，满足了现实世界对数据的实时性、可追溯性、安全保护和隐私的需求。选择 RAG 与否，关键在于评估场景中是否存在对这些需求的刚需。例如， RAG 在解决数据实时性和可追溯性问题方面表现出色，特别是在精准问答领域。此外， RAG 也在推荐系统和信息抽取任务中表现优异。我们开发的开放域信息抽取产品，能够从任意文档中抽取信息，改变了以往需要为每种文档定制模型的情况，这不仅是技术上的革新，也解决了数据文档的实时性和可追溯性问题。</p><p></p><p>另一个视角是针对对幻觉问题要求严格的场景，如医疗行业， RAG 在药物研发和市场准入方面有很好的应用。在金融分析领域，由于市场信息每天都在变化，筛选有效信息并保证信息来源的可靠性至关重要， RAG &nbsp;技术在这里同样适用。以及如投资分析场景，合合开发了分析师问答产品，专门针对财报、研报、公告等高信息密度、高实时性的知识库进行问答，满足分析师的需求。</p><p></p><p>我认为，RAG 能否解决问题的关键在于场景是否对 RAG 的能力有刚需。 如果刚需存在，这些场景的应用将会快速发展。对于非刚需的应用，我们可能需要考虑其他技术解决方案，如微调或选择具有超长上下文的大模型。</p><p></p><p>在挑战方面，互联网产品的思维可以用七个字概括：专注、极致、口碑、快。虽然 RAG 技术自 2020 年提出以来发展迅速，但许多产品并没有做到专注，技术效果也没有达到极致，导致口碑没有形成，产品未能通过 PMF 验证，难以进入市场。知识问答类产品可能一周就能做出演示版，但要达到好用的程度可能需要半年时间。现有的 RAG 基础流程存在许多需要优化的问题，包括检索文档内容解析错误、边缘案例处理不当、解析速度慢、知识库更新耗时长、机械分块丢失语义信息、目标检索内容召回不全、召回结果排序困难、答案生成存在遗漏等。</p><p></p><p>在今年 8 月 18-19 日的 AICon 上海站，我将分享 《<a href="https://aicon.infoq.cn/2024/shanghai/presentation/6004">向量化与文档解析技术加速大模型 RAG 应用落地</a>"》 主题，解决 RAG 最基础也是最本质的问题是出现爆款产品的基础。例如，文档数据中多种版式的精准解析，速度要快，精度要高。这些技术问题虽然讨论较少，但实际上非常重要。此外，如何在小资源下实现高性能和高精准度的 embedding 嵌入模型，也是目前面临的主要挑战。</p><p></p><h2>RAG &nbsp;真的“烂大街”了吗？</h2><p></p><p></p><p>郭瑞杰： 感谢常扬老师及前面两位老师的分享， RAG &nbsp;技术在不同行业和场景下的应用仍在探索阶段，许多潜在的应用和优化尚未实现。当然， RAG &nbsp;技术的发展和应用正在不断演进，随着技术的成熟和优化，预计 RAG 将在更多场景下发挥关键作用。接下来，我们进入今天的第二个话题。如何提高公众对 RAG 技术的认识和理解？在很多同学的认知里， RAG &nbsp;技术似乎已经“烂大街”了？关于这个问题，大家怎么看？</p><p></p><p>张颖峰： RAG 技术在实际使用中确实存在一些挑战。虽然部署起来相对容易，但实际效果往往并不理想。然而，我认为 RAG 技术本质上是一种普及性的架构，而非特定于某个场景的解决方案。它是大模型服务 B 端市场的一种方式，因此我们不能因为短期内效果不佳就认为这项技术会被未来替代。</p><p></p><p>公众对 RAG 的认知一直存在较大争议。去年， RAG 刚开始流行时，业界围绕是否使用 RAG 还是微调展开了激烈讨论。经过大半年的辩论，公众对 RAG 有了更广泛的认识。现在，愿意选择微调的企业已经非常少， RAG 已经证明自己能够解决大模型针对企业内部私有数据的提问问题，不再需要进一步普及。</p><p></p><p>今年进入第二阶段，关于长上下文大模型的争论变得更加激烈。例如，阿里、Kimi、谷歌等公司的模型，上下文长度甚至可达百万，未来可能达到千万。今年二三月份，这种争论达到了高潮。当时，谷歌发布的一篇评测显示，长上下文在解决某些问题时比 RAG 表现得更好。这些问题是客观存在的，长上下文确实能更好地解决问题。在这种背景下，许多人认为 RAG 应该尽可能简化，不使用复杂的向量技术，而是用最基本的数据库和关键词搜索，然后利用长上下文的大模型来提供答案。这种方案在当前情况下是一种简单有效的解决方案，因为模型本身的上下文能力比 RAG 强。</p><p></p><p>但我认为争论仍将继续，因为长上下文模型和 RAG 之间不应是冲突关系，而应是合作关系。如果是个人端或简单场景，如个人知识库问答，使用长上下文模型确实方便。但一旦涉及企业级场景，如垂直行业，长上下文模型的适用性就会受到很大限制。长上下文模型并非无敌，例如，英伟达最近发表的一篇文章指出，如果给模型的上下文窗口太多，其回答质量会明显下降。因此， RAG 提供的精准段落对改进效果有显著提升。</p><p></p><p>RAG 和长上下文模型之间应该是相互配合的关系。在企业级应用场景中，如营销、合同合规、法律合规或供应链库存管理等，仅依靠长上下文模型是不够的。两者的配合对于解决实际问题至关重要。通过不断的辩论， RAG 的普及得到了极大的推动。今年二三月份，与 RAG 相关的文章在 arXiv 上的更新并不多，但现在更新频率明显提高，每天都有多篇相关文章发布。这表明 RAG 在工业界、产业界和学术界已经得到了共识。现在的讨论焦点是如何从技术角度解决 RAG 的痛点，而不是 RAG 是否还有存在的必要。</p><p></p><p>常扬： 我非常赞同张总提到的 RAG 技术已经获得广泛认知。例如，7 月份微软开源了与 Graph &nbsp;RAG 相关的项目，这也引发了很多讨论，学术界也在持续解决相关问题。我想从应用的角度补充几点看法。</p><p></p><p>首先，我们要实现“技术去魅”，即让 RAG 技术从最初的神秘和疑问，转变为开发者们讨论如何应用它。这需要让大家理解 RAG 技术的底层逻辑其实简单直接，而非复杂晦涩。RAG 技术的真正价值在于它能够实现更准确的回答和更快的搜索，本质上与搜索引擎相似。这意味着通过建立与用户之间的信任，提供快速的检索体验，并找到准确度与响应度之间的最佳平衡点。RAG 技术完全有能力替代传统搜索，因为它能提供比网页搜索更好的效果。</p><p></p><p>为了实现这一目标，我认为 RAG 技术的广泛应用需要通过打造爆款应用来推动。百度李彦宏在上海的世界人工智能大会上也提到了这一点，他希望大家不要仅仅局限于大模型的竞争，而是去创造一些爆款应用。周鸿祎在微博上也表示支持这一观点。实际上，这需要我们持续专注于技术，优化场景理解，做好产品，让产品既可用又好用，从而打造出能够深刻改变用户场景和认知的 RAG 产品。</p><p></p><p>其次，关于“烂大街”的问题，如果这代表降低 RAG 技术理解和使用的门槛，我认为这是一件好事。 我们应该坚持引用更多能力，迭代和优化 RAG 技术以增强效果，同时简化 RAG 技术的使用门槛，让更多开发者能够使用它。比如通过 InfoQ 举办的技术大会进行广泛传播，让更多人理解 RAG 是什么，能够在合适的场景中使用它。同时，“烂大街”也有另一层含义，即技术看起来很好，但实际使用效果不佳，需要进一步加工和调整。这是我们需要优化和避免的。我们要提升 RAG 在检索和生成每个细节环节的效果，确保技术不仅可用，而且越来越好用。</p><p></p><p>RAG 在很多基础流程上仍存在问题，有很大的优化空间。去年有篇澳大利亚学者的论文，提出了 RAG 的七宗罪，涉及数据源、检索、排名、生成等多个方面的问题，这也是我们这些从事相关工作的人员的研究方向。此外， RAG 技术与其他技术的结合，如与 Agent 的结合，以及与更多场景的结合，都有很大的发展潜力。总结来说，我认为 RAG 技术的关键还是要有爆款产品，而从事 RAG 技术的人的关键是解决其基本问题，让 RAG 技术在这些爆款产品中可用，满足用户的期望体验。</p><p></p><p>欧明栋： 关于长上下文是否会替代 RAG 的问题，我们进行了许多测试。测试结果表明，当长上下文的内容过长时，其效果会受到影响。此外，长上下文的响应速度也会显著下降。为了让人们更深入地了解和认识 RAG ，我认为需要有经验丰富的产品经理来设计一些易于使用的产品功能，这可能会激发大家对 RAG 更大的兴趣。</p><p></p><p>最近，无论是在技术论坛还是社交媒体上，我们经常看到有人批评 RAG 的技术含量低。传统的 RAG 是很容易上手，而且有像 LlamaIndex、LangChain 这样的框架，只需几行代码就能部署一个 RAG 系统。但从产业落地的角度来看， RAG 目前还不能完全满足需求。例如，我们采用的 Native &nbsp;RAG ，即搜索加上大模型总结的链路，实际上并不能很好地解决用户抽取信息和解决问题的场景。从 RAG 本身的目标来看，目前的 Native RAG 技术还远远达不到这个目标。正如刚才两位老师提到的，要实现这个目标，还有许多问题需要解决， RAG 领域还有很多技术需要我们去探索。</p><p></p><h2>基于 Agent 的 RAG 如何解决复杂问题？</h2><p></p><p></p><p>郭瑞杰： 虽然 RAG 技术可能被广泛讨论，但这并不意味着它已经普遍应用或失去了创新性。相反，这可能是技术发展和成熟过程中的一个自然阶段。接下来，我们来讨论今天的第三个话题。基于 Agent 的高级 RAG 如何针对复杂问题提供解决方案？</p><p></p><p>常扬： 在今年 8 月的 AICon 上海站上，很多演讲都涉及到了 Agent。由于今天没有投屏，我们讨论的更多是方向性的内容，届时在大会现场，各位老师的分享一定是内容丰富、干货满满的。</p><p></p><p>欧老师刚才提到，目前大家关注的是 Naive &nbsp;RAG ，而不是包含 Advanced 或 Modular 特性的 RAG 。现在的关注点确实是纯粹的问题检索，核心在于单轮检索的准确性和速度。 例如，企业可能需要迅速检索与公司相关的最新公关新闻及其应对策略。然而，在现实场景中，问题往往更复杂，企业希望结合他们的知识库解决项目中的问题，这些需求的复杂性往往无法通过单轮 RAG 来解决。</p><p></p><p>这就引出了基于 Agent 的高级 RAG 的问题。在真正的企业场景中，包含几个问题：用户的意图往往不明确，需要多轮对话来确认；需要从多个来源收集信息，包括文档数据库、知识库、外部 API 和搜索网页等；还需要多步推理才能得到综合答案。在这种场景下，就需要基于 Agent 的高级 RAG 。</p><p></p><p>以我们正在开发的分析师问答产品为例，我们调研了基金公司的需求。基金公司的领导需要对某个调研方向进行全面调研，这个需求给到分析师。分析师需要与领导进行多轮对话，了解具体需求，如技术可行性、产品市场占有率、市场分析、竞争对手调研等。这些信息需要从多个数据源获得，经过复杂检索和分析整合，最后按照用户要求的格式（如 PPT 或 PDF）完成需求。这种场景反映了真正的企业需求是非常细致的。</p><p></p><p>大家看到的或讨论的 Naive &nbsp;RAG 案例，实际上只是项目中的某个部分需求。为了实现这一点，系统必须能够处理和整合来自多元的多样化数据，并在每一步提供精准信息，最后根据用户意图准确回答。</p><p></p><p>我再谈谈 RAG 加 Agent 的本质。我认为它的本质是复杂问题的分治。Agent 的定义大家都很清楚，我想从另一个角度简单解读一下。Agent 通过将任务拆解为 Plan（对应多轮对话，理解用户意图和任务规划）、Do（使用外部工具完成任务）、Check（任务总结）、Action（记忆及任务经验改进后的执行），整个 Agent 对应了我们做项目的整套 PDCA 流程。融合 Agent 的 RAG 可以将原来的单轮对话调整为多轮对话，理解用户意图，拆解任务，使用更多工具解决非结构化、半结构化数据检索的问题。这种方式让 RAG 系统变得可定制、可扩展，可以根据具体任务调整使用工具完成复杂任务。以分析师问答场景为例， RAG 加 Agent 可以自动检索、分类和分析反馈，最终形成一个详细的报告来帮助企业决策。这样的场景本质上是将技术转化为企业可接受的项目价值，这是 Agent 的关键，它符合企业对复杂度的判断。</p><p></p><p>张颖峰： 我在这里稍微补充一些与我们最近发布的 RAG &nbsp;Flow 功能密切相关的内容。我们在周一晚上发布了新版本，其中包括了 Agent 功能。RAG 和 Agent 之间的关系是相互基础性的。关键的一点是，Agent 为 RAG 提供了基于有环图的编排能力。因为我们知道 RAG 的效果有待提升，所以我们需要用各种技术手段来补充，这种补充意味着我们不能只使用简单的单轮对话。我们需要将对话转化为多轮，引入查询意图、分类、关键词抽取等不同的算子，以编排的方式而不是单轮对话的方式，将对话组织在一起。Agent 的编排能力及其反思机制是其重要能力之一，即评估 RAG 生成和检索出来的质量，如果质量不高，则需要不断消耗 token 进行搜索，直到得到满意的答案。</p><p></p><p>如果将 Agent 视为 RAG 的基石，它主要起到的就是上述作用。为了解决前面提到的各种痛点，比如数据抽取、命中率不高、找不到答案等问题，我们都需要通过 Agent 或称为 Agentic 的 RAG 来编排，以解决这些问题。Agent 提供了这样一个框架和机制，我们在这个框架内不断添加各种算子，如查询意图、改写、关键词抽取、知识图谱抽取等，将这些不同的算子加入后， RAG 的质量就会得到提升。</p><p></p><p>反过来，当 RAG 能够以这种方式提供满意的答案后，Agent 就可以在 RAG 的基础上进一步包含企业所需的具体场景业务。例如，我们最近开源的产品中加入了 Agent，并提供了两个模板：一个是客服系统，它实际上是将 RAG 应用于垂直领域时所需的业务系统，需要用工作流的方式进行编排。工作流通常是有向无环图，而 Agentic &nbsp;RAG 引入高级 RAG 时，需要反思来编排查询意图、查询改写等。有了这些能力后，实现工作流就可以让 RAG 以 Agent 的方式对接企业的业务系统。另一个是通用模板，用户可以用来创建自己的工作流，如猎头寻找候选人并与他们沟通。当对话意图不明确或需要直接获取候选人的联系方式时，这实际上就是一种企业的业务工作流。因此， RAG 和 Agent 结合时，关键在于如何让 RAG 服务于具体的垂直场景。我认为 RAG 和 Agent 是互为基石的作用。</p><p></p><p>欧明栋： 基于 Agent 的 RAG 应用实际上是很自然的一步。从客户反馈来看，传统的单轮 RAG 无法解决很多问题。当前，许多研究正在探讨多跳问题和模糊意图问题，这类问题都需要 Agent 的参与。因此，开发 Agent 功能成为了一个自然的趋势。我们的 Agent 开发还没有达到完美的效果，但我愿意分享一些我们的实践、经验和教训。</p><p></p><p>目前，我们在开发的 Agent 是完全自主的。在之前的版本中，我们已经支持用户编排，而现在我们希望 Agent 能自主解决客户问题。我们设计的 Agent 分为两层：第一层 Agent 负责简单的路由，将任务分配给更大特定任务的 Agent 流水线，例如问答、总结、意图澄清，甚至包括自定义功能，如客服中直接转人工等。基本上，第一层 Agent 执行的是大的路由任务。第二层 Agent 则负责更细致的任务。以问答 Agent 为例，它是一个比较常规的自主 Agent，利用大模型的逻辑推理能力进行规划，然后调用工具。模型本身的规划能力非常关键，它需要判断问题回答的进度、缺失信息和补充需求。工具也很重要，可能需要进行问题改写、搜索、SQL 查询，或者使用 Code Interpreter 进行编程和计算，以提供更准确的答案。关于幻觉问题，许多论文提到通过 self- RAG critic 和 self-refine 等 进行自我检查可以减少幻觉。这里会有各种工具，包括客户自定义的工具。例如，金融客户可能需要非常专业的金融指标计算，这就需要明确定义的自定义工具应用。问答 Agent 主要依赖模型的规划能力和调用工具的能力。</p><p></p><p>目前，我们在开发的 Agent 是完全自主的。在之前的版本中，我们已经支持用户编排。我们设计的 Agent 分为两层：第一层 Agent 负责简单的路由，将任务分配给特定任务的 Agent 流水线，例如问答、总结、意图澄清，甚至包括自定义功能，如客服中直接转人工等。基本上，第一层 Agent 执行的是大的路由任务。第二层 Agent 则负责更细致的任务。以问答 Agent 为例，它是一个比较常规的自主 Agent，利用大模型的逻辑推理能力进行规划，然后调用工具。模型本身的规划能力非常关键，它需要判断问题回答的进度、缺失信息和补充需求。工具也很重要，可能需要进行问题改写、搜索、SQL 查询，或者使用 Code Interpreter 进行编程和计算，以提供更准确的答案。这里也会有各种工具，包括客户自定义的工具。例如，金融客户可能需要非常专业的金融指标计算，这就需要明确定义的自定义工具应用。问答 Agent 主要依赖模型的规划能力和调用工具的能力。关于幻觉问题，许多论文提到通过 self-critic 和 self-refine 等 进行自我检查可以减少幻觉。</p><p></p><p>在实际使用中，Agent 虽然理想美好，但现实依然充满挑战。Agent 相比传统的 Native Agent，迭代步骤更多，可能会有误差累积。在多次迭代中，幻觉可能会累积，我们在多步 Agent 之后发现了这种问题。此外，Agent 还可能出现死循环，无法停止迭代，或者出现早停，即在未得到答案前就停止了。我们还面临一个较大的问题是上下文长度的管理。由于 RAG 会进行多次检索，我们需要决定是否保留这些检索内容。如果全部保留，上下文可能会过长；如果不保留，可能会遗漏信息。在这方面，我们还需要不断迭代和平衡。</p><p></p><p>郭瑞杰： 下面请老师回答一个观众问题：“在整个 RAG 过程中添加各种技术手段，使得处理过程过长，如何进行优化呢？”</p><p></p><p>常扬： 我认为技术发展初期遇到无法解决的问题是很自然的，这会促使我们引入新的模块来应对。但我依然坚持，在最基础的关键流程中，我们必须实现速度和性能的最优化。例如，我们一直在研究文档解析问题，这不仅需要解决不同版式元素的稳定性和准确性，还要提高效率。最近，我们已经能够做到在大约 1.5 秒内完整解析 100 页的 PDF。</p><p></p><p>我的观点是，从最基础的产品出发，长的编排并不是问题。真正的挑战在于，在横向探索技术复杂度时，要在每个基础模块上提升效率。 目前，许多服务过程相当冗长。以微信春晚抢红包为例，这个过程背后需要经历的步骤非常多。尽管在技术初期确实较慢，无法满足高并发需求，但在当前更复杂的场景和更高要求下，即便面对上亿的并发量，依然能够保持良好表现。这是因为每个模块都达到了产品级的性能标准。现在，没有人会说移动互联网技术在多个微服务串行的情况下会出现效率问题。</p><p></p><p>张颖峰： 在我看来，目前 RAG 处理过程的时间过长并不是一个主要问题。我们可以将 RAG 的工作流程分为几个阶段：首先是数据抽取，我们会使用多种模型以语义的方式抽取和解析数据；其次是文档预处理，包括知识图谱的抽取和文档聚类等；然后是索引构建，以及排序和查询改写等操作。每个阶段都需要进行大量工作，以确保最终的效果。每个阶段的工作与我们后面可能遇到的问题息息相关，都需要精心处理，从而保证最终效果。因此，目前阻碍 RAG 普及的主要痛点不是速度，而是效果。 实际上， RAG 的速度已经相当快。如果我们从数据库的角度来看， RAG 更倾向于服务于一个“写少读多”的场景，即我们可能会上传一些数据，一旦这些数据被加工处理，接下来更多的是围绕这些数据进行问答。</p><p></p><p>如果技术发展必须在速度和效果之间做出取舍，我认为速度是可以有所牺牲的。 毕竟，目前我们必须通过不断迭代来确保效果，这是需要优先解决的问题。我预计，要达到令人满意的效果，可能还需要半年到一年的时间周期。在此期间，我们应该专注于提升 RAG 的效果，即使这意味着在速度上做一些妥协。</p><p></p><p>郭瑞杰： 有个用户提问说，用 RAG 来做推荐系统，但是效果一直不太好，请问老师们有什么建议？他做的推荐系统是一个专家推荐系统，用户提出需求，然后从人才库中推荐信息。</p><p></p><p>张颖峰： 我不妨为我们的数据库做个广告。推荐系统实际上需要一些复杂的业务逻辑。例如，你需要通过各种混合召回方式，将候选对象（candidate）搜索出来。接下来，使用不同的排序方法，包括引入基于业务逻辑的排序和基于模型的排序，将它们结合起来，最终给出一个答案。如果仅使用单一策略，效果可能并不理想。</p><p></p><p>使用 RAG 构建推荐系统，我认为这是一个非常前沿的领域。目前，大家使用 RAG 进行搜索还没有做得很好，而推荐系统是一个更加复杂的系统，涉及的效果和业务场景变数都很大。我们目前还没有开发推荐系统。不过，根据我过去在推荐系统领域的经验，解决问题的方法无非是从召回和排序两个角度入手。召回时，你需要采用多种手段，比如关键词召回和向量召回，至少要进行混合搜索。排序时，则需要结合业务逻辑规则和基于模型的排序。这些手段都需要应用到系统中，以解决推荐问题。目前构建推荐系统不会面临我们之前提到的数据抽取、查询改写或知识图谱抽取等问题。因此，仅从索引和召回排序这两个角度出发应该就足够了。</p><p></p><p>常扬： 我认为使用目前效果较好的方案，比如张总提到的 RAG 工作流，是一个很好的选择。此外，实际上可以使用像 LangChain 这样的工具，它提供了一些回调功能。这些回调函数能够展示 RAG 过程中的内容，包括检索到的内容和与文本块排序相关的信息。通过深入观察每个环节，你可以确定针对你的具体情况是哪些环节出现了问题。这样的工具可以帮助你更细致地了解和优化 RAG 的工作流程。</p><p></p><p>欧明栋： 使用 RAG 构建推荐系统确实是一个相当大的挑战。与传统的推荐系统不同，传统系统更多依赖用户的行为数据，而使用 RAG 进行推荐，我理解其核心是基于大语言模型，也就是基于语义来进行推荐。过去，基于用户行为的推荐系统能够学习到许多行业特定的模式。如果我们不再依赖行为数据，只使用语义信息，我们可能需要在上下文中体现出行业内的模式和常识信息。如果这些信息能够在上下文中得到体现，大型语言模型或许能够通过学习这些模式来进行合理的推荐。如果直接让大型语言模型自己做出判断，在某些情况下可能会比较困难，特别是在长尾场景中，大模型可能缺乏相应的知识，无法直接进行合理的推理。在这种情况下，可能需要引入一些上下文相关的包或特征，加入一些先验知识以辅助模型做出更好的推荐。</p><p></p><h2>RAG &nbsp;技术未来展望</h2><p></p><p></p><p>郭瑞杰： 前面 3 个话题主要讨论了 RAG 技术的现状、应用情况、高级 RAG 技术解法等，最后，咱们聊聊 RAG 技术未来的发展方向，有哪些新兴的技术和方法可能会给 RAG 技术带来冲击？</p><p></p><p>常扬： 我们的理念始终是从用户出发，开发出既可用又好用的产品。我们希望推动 RAG 行业的发展，比如提高数据源清洗的准确性，加快知识库更新速度，改进 trunk 分配的智能化，提升 embedding 模型的性能，以及优化 rerank 模型和 prompt 生成的最佳实践。我相信，在未来半年到一年内，这些方面会取得很好的进展，这将是 RAG 发展的一个方向。</p><p></p><p>就像 ChatGPT 解决了传统深度学习中数据不足和算力消耗大的问题，并推出了爆款产品一样，技术的底座打好后，爆款产品自然会出现。这种技术的范式或大规模应用从两年前开始，已经席卷了整个 AI 行业。在 8 月份的 AICon 上海站上，我分享的也是关于文档解析、embedding 模型等方面的工作，如何使 100 页 PDF 的解析更准确、速度更快。</p><p></p><p>当然，还有很多基础工作要做，比如用户意图识别、检索技术（结合稀疏向量检索、张量检索、关键词全文检索等）、多路索引、多级路由，以及海量数据的高性能检索，可能还会涉及到向量数据库。此外，还有上下文压缩、句子窗口使用、Self- RAG 等，这些都是 RAG 最基础环节中的提升点。我认为，这些基础工作完成后，才有可能出现爆款产品。</p><p></p><p>接下来发展可能是多模态应用。RAG 技术和思想能否应用于图片、音频、视频、3D 模型和代码等多模态情况，我认为这非常值得期待。我们可以检索文本、图片、视频，进行图文搜索，甚至检索视频片段。现在互联网上的信息很多是视频信息，包括短视频和长视频，它们包含了丰富的知识内容。如果能够检索视频中的某一帧，那将是应用广度和深度的巨大提升，可能会出现很多 C 端的爆款产品。</p><p></p><p>至于冲击，我认为 RAG 能够长期立足的原因在于它的实时性和数据安全性。微调和 RAG 之间始终存在时间差，即使微调不断迭代，这个时间差也不会消失。大模型虽然有更强的上下文能力，但它们无法处理非常海量的数据，无法保障数据安全性，也无法解决 ROI 问题，即收益和高推理成本的比较。这些其实都是 RAG 的强项。</p><p></p><p>总结一下，我认为 RAG 发展的基础效果是第一位的。当这些基础打好后，我们就会看到很多 ToC 和 ToB 的应用出现。其次是多模态应用，我相信这会开启一个新时代。现在大模型在 C 端的产品和应用还不多，但多模态的发展可能会出现消费级爆款。微调和长上下文对 RAG 没有冲击，反而是一个非常好的结合，通过这种结合可以完成非常好的产品。</p><p></p><p>欧明栋： 多模态确实是一个重要的领域，它应该会显著提升 RAG 的体验。原因在于，用户的理解往往是基于多模态信息的，我们日常接触到的数据也大多是多模态的。例如，人们通过结合文本、图片、音频和视频等多种形式的信息来获得更全面的理解。</p><p></p><p>近期，关于大型模型在非文本（non-context）模态上的能力以及 Agent 能力对 RAG 影响的讨论越来越多。长上下文大模型的能力确实能够提升 RAG 本身的效能。但是这并不意味着检索和逻辑推理的能力就能被完全替代。这些能力仍然是 RAG 不可或缺的一部分。</p><p></p><p>对于 Agent 而言，目前的趋势是将传统 RAG 的检索过程作为 Agent 的一个工具，以此来实现更好的集成。Agent 可以利用 RAG 的检索功能，结合自身的优势，比如任务规划和执行，来解决真实场景中的复杂问题。</p><p></p><p>张颖峰： 我想从两个方面来探讨 RAG 的未来发展。首先是 RAG 应如何演进，其次是未来 RAG 可能受到哪些技术的冲击和影响。</p><p></p><p>对于 RAG 的演进，我认为可以从四个阶段来考虑：数据抽取、数据预处理、索引和查询改写。</p><p></p><p>第一阶段：数据抽取</p><p></p><p>在数据抽取方面，理想的情况是有一个能够处理各种文档的抽取大模型，无论文档中包含什么内容，如流程图、柱状图、饼图等，都能解读出来。如果能够开发出这样的大模型，它将解决 RAG 在数据落地方面的许多痛点。</p><p></p><p>第二阶段：数据预处理</p><p></p><p>数据预处理目前主要包括 embedding 模型和知识图谱的抽取。Embedding 模型在一些垂直场景中的应用需要进一步优化，以减少向量干扰。知识图谱的自动化构建，如微软的 Graph &nbsp;RAG 所展示的，为 RAG 问答质量的提升提供了一个很好的起点，尤其是对于多跳或长文本问答。</p><p></p><p>第三阶段：索引</p><p></p><p>索引阶段，我们已经尝试了多种搜索手段，包括全文搜索、向量搜索、系数向量搜索，甚至张量搜索。IBM 研究院提出的 Blended &nbsp;RAG 通过三路召回混合搜索，可以达到最佳效果。我们复现了这一结果，并发现使用张量等手段可以进一步提升效果。</p><p></p><p>第四阶段：查询改写和排序</p><p></p><p>最后一个阶段是查询改写和排序。这一环节的技术进步将进一步影响 RAG 的迭代和发展。</p><p></p><p>我认为，在半年到一年的时间里，这四个阶段都会有显著的进展。这些进展将决定 RAG 如何应对未来的技术冲击和影响。</p><p></p><p>关于未来对我们技术可能产生的冲击，我认为主要有以下几个方面。</p><p></p><p>首先，我坚信 AGI（人工通用智能）终将到来。随着大模型能力的不断升级，例如未来可能出现的 GPT-5 或其他更高级、具备推理能力的模型，它们的到来可能会对我们目前 RAG 的检索行为产生影响。在这种情况下，我们可能需要重新考虑是否继续使用现有的技术堆栈和检索策略，这是一个目前我还无法回答的问题，但可以预见的是，它将对 RAG 产生一定的影响。</p><p></p><p>其次，技术的演进可能会导致我们更深入地将 RAG 与大模型结合起来。目前，有些研究方向并不是简单地称之为 RAG ，而是称为基于检索增强的大模型。这种研究将数据库检索结果与模型内部的 embedding 深度耦合。我认为这是一种有趣且有益的探索方向，两者之间的交互应当是非常深入的。</p><p></p><p>从这些角度来看，未来 RAG 产品的技术演进形态可能会受到显著影响。然而，核心的行为和需求是不会改变的。我们始终需要一个长期记忆体，如硬盘，来配合模型，以服务于更复杂的场景。这一点是不会改变的。不过，具体的技术栈和表现形式可能会随着技术的发展而发生较大变化。</p><p></p><p>郭瑞杰： 未来多模态 &nbsp;RAG 、Agent、基于 LLM 的知识图谱等技术深度结合，可能会带来新的优化方法和产生新的应用场景。这些新兴技术和方法可能会给 RAG 技术带来新的冲击和发展机遇，推动 RAG 技术在更多领域和场景中的深度应用，并实现更高的性能和可用性。感谢 3 位老师的精彩分享，期待 3 位老师在 AICon 现场的发挥。</p><p></p><p>活动推荐</p><p></p><p>8 月 18-19 日，<a href="https://aicon.infoq.cn/2024/shanghai/">AICon 全球人工智能开发与应用大会</a>"将在上海举办。来自字节跳动、华为、阿里巴巴、微软亚洲研究院、智源研究院、上海人工智能实验室、蔚来汽车、小红书、零一万物等头部企业及研究机构的 60+ 资深专家，将带来 AI 和大模型超全落地场景与最佳实践分享，帮助与会者提升技术视野、获得有价值的实践指导。</p><p></p><p>在主题演讲环节，我们已经邀请到「蔚来创始人 李斌」分享围绕 SmartEV 和 AI 结合的关键问题，蔚来汽车的思考与实践；「顺丰集团 CIO、顺丰科技 CEO 耿艳坤」将重磅发布顺丰物流大模型；「面壁智能联合创始人、CEO 李大海」，则将带来他对于大模型技术、产品与行业发展的前瞻洞察。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2e/2e7902b3dcbcd1a3d526249ea92cb872.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tb3OQVYErQfYyMxs7qNG</id>
            <title>大模型正成为钢铁行业转型升级的关键力量</title>
            <link>https://www.infoq.cn/article/tb3OQVYErQfYyMxs7qNG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tb3OQVYErQfYyMxs7qNG</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jul 2024 10:43:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华院计算技术, 行业大模型, 钢铁行业, 智能检测
<br>
<br>
总结: 华院计算技术推出的钢铁行业大模型，利用认知智能引擎实现了钢铁产品表面缺陷的智能检测，提升了检测准确率和效率，推动了钢铁行业智能化和绿色发展。钢铁行业正面临市场和结构性变革，大模型技术在钢铁制造、废钢判级、节能减碳、经营计划优化等方面取得显著成果，为行业转型升级提供了强有力支持。 </div>
                        <hr>
                    
                    <p>日前，华院计算技术（上海）股份有限公司（以下简称“华院计算”）宣布推出以认知智能引擎为基础的华院钢铁<a href="https://aicon.infoq.cn/2024/shanghai/track/1710">行业大模型</a>"， 标志着我国人工智能技术在传统行业应用的一次重要进展。该模型在表面缺陷智能检测上的技术创新和高效表现，不仅提升了检测准确率和效率，还推动了钢铁行业在智能化和绿色发展方面的进一步前行。</p><p></p><p>当前，中国钢铁行业正经历着前所未有的市场和结构性变革。这次变革不同于过去三十年的单一行业周期底部，其下行趋势具有多重周期波动共振的特性，产业逻辑基础发生较大变化。</p><p></p><p>此次行业变革不仅持续时间长、波动幅度强、影响深度大，钢铁行业进入了一场关乎生死存亡的洗牌阶段。根据国家统计局相关数据显示，2023 年全年平均钢材综合价格指数为 111.86 点，同比下降 11.50 点。2024 年，在供需双弱以及原燃料价格强势的预期下，钢铁企业仍将面临经营压力。有大型钢企负责人认为，当前钢铁行业已进入“冰河时期”，正面临惨烈竞争时代的严峻形势。</p><p></p><h2>华院钢铁行业大模型：表面缺陷智能检测</h2><p></p><p>在这样的压力下，华院钢铁行业大模型的推出恰逢其时。该大模型旨在实现钢铁产品表面缺陷的智能检测，融合了<a href="https://aicon.infoq.cn/2024/shanghai/track/1701">多模态</a>"神经网络设计、小样本学习与数据增强、多任务多尺度缺陷检测、自学习技术、自动标注与类激活映射，以及基于专家知识的缺陷自动判定等多项创新技术。通过这些技术，华院钢铁大模型不仅能够提升检测的准确性和效率，还为产品的持续改进提供了有力的数据支持。</p><p></p><p>在性能表现上，华院钢铁行业大模型成功替代了传统的进口表检设备，并在缺陷分类准确率上实现了显著提升，达到 85%，比国外同类产品高出 15%。对于一些严重缺陷的分类准确率更是达到了 95% 以上，这不仅提升了质量控制的标准，也大幅降低了废品率。</p><p></p><p>此外，该大模型在<a href="https://aicon.infoq.cn/2024/shanghai/track/1708">数据</a>"标注效率上的提升也是一大亮点。通过自动生成多模态数据，华院钢铁行业大模型不仅提高了模型的准确性，还有效缓解了长尾分布对模型性能的负面影响。华院钢铁行业大模型的高效运作，使企业能够实现生产流程的自动化，减少对人工的依赖，降低人工成本，同时确保产品质量的稳定性，增强了企业在激烈市场中的竞争力。</p><p></p><p>值得一提的是，华院钢铁行业大模型的自学习技术使模型能够不断从生产实践中学习，对误检、漏检以及置信度较低的缺陷数据进行持续的收集和训练，确保模型性能的持续优化和与时俱进。这种动态调整和自我优化的能力，使华院钢铁行业大模型能够适应不断变化的生产环境和需求，为钢铁行业的可持续发展提供了强有力的技术保障。</p><p></p><h2>大模型赋能钢铁行业成果显著</h2><p></p><p>除了华院计算，行业内还有其他企业也在积极推动大模型技术在钢铁行业的应用与发展。这些企业通过不同的技术路径和创新方案，在制造、废钢判级、节能减碳、经营计划优化等方面也推动着钢铁行业的转型升级。</p><p></p><h4>钢铁制造</h4><p></p><p>在钢铁制造领域，AI 大模型通过分析离线和在线数据，发现生产过程中的潜在问题和优化空间，完善生产工艺，提升产品质量，降低成本，并精准预测设备故障，及时进行维护，避免生产中断，确保生产的连续性和稳定性。例如，湘钢通过与湖南移动和华为联合打造的盘古大模型，实现了流程再造和环节重塑的创新生态。</p><p></p><p>在湘钢炼钢厂，行车智能调度系统集成了炼钢生产计划、行车检修信息、钢水包实时位置、各类业务规则等大量数据，利用算法智能生成行车调度计划。生产计划若临时有变，系统能够在不到 1 分钟内“思考”出接下来 30 分钟的调度计划，及时下发指令。</p><p></p><h4>废钢判级</h4><p></p><p>在废钢判级方面，AI 大模型的应用显著提升了分选的效率和精确度。例如，四川冶控集团旗下的泸州鑫阳钢铁与用友合作，开发了一套废钢判级的标尺，该标尺能够排除主观因素的干扰，公正地执行每车废钢的智能评级。</p><p></p><p>相较于传统依赖人工经验的废钢判级方法，AI 大模型通过分析大量废钢样本数据，迅速且准确地识别废钢的种类、成分和品质，从而有效支持废钢的回收利用。这一变革不仅减少了人工成本，还提升了废钢资源的利用效率，为企业带来了显著的经济效益。</p><p></p><h4>节能减碳</h4><p></p><p>在节能减碳方面，AI 大模型提高了优化求解的效率和准确性，增强了多源数据融合的分析能力，从而加强了产业链的追溯和核算能力。以中冶赛迪为例，其自主研发的钢铁工业碳核算及优化分析平台能够在不同层面进行详尽的碳素流分析，并根据不同技术应用场景进行碳排放的精确计算。该平台使企业能够清晰地掌握和计算碳排放情况，实现碳排放的透明化管理。通过建模和分析全厂或单个工序的碳排放与碳素流，平台帮助企业诊断各生产单元的碳排放构成，有效挖掘减碳潜力。</p><p></p><h4>经营计划优化</h4><p></p><p>在经营计划优化方面，AI 大模型通过深入挖掘和分析历史营销数据、客户数据以及市场趋势，精准预测市场需求，指导企业制定更为合理的经营计划，优化产品结构，降低运营成本。例如，钢谷网研发的“谷蚁 AI 大模型”专为钢铁行业设计，它整合了钢谷网独有的数据资源和行业资讯，构建了一个具备分析和总结能力的智能系统，能够在多个维度上精准展现大数据全景，并在面对具体问题时迅速提炼出总结性观点，为行业决策提供了精准支持。</p><p></p><h2>写在最后</h2><p></p><p>总体而言，大模型技术在钢铁行业的广泛应用，不仅推动了行业的智能化转型和高效发展，还为企业应对市场挑战、提升竞争力和实现可持续发展提供了强有力的技术支持。这些技术创新和实践成果表明，大模型技术正成为钢铁行业未来发展的重要驱动力。</p><p></p><p>活动推荐：</p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 <a href="https://aicon.infoq.cn/202408/shanghai/">AICon 全球人工智能开发与应用大会</a>"，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，详情可联系票务经理 13269078023 咨询。</p><p><img src="https://static001.geekbang.org/wechat/images/db/db809a0579759615188699c6969bc438.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MTy0QEENQxpkYpLZwC4V</id>
            <title>开源仅1天就斩获近万星！超越RAG、让大模型拥有超强记忆力的Mem0火了！</title>
            <link>https://www.infoq.cn/article/MTy0QEENQxpkYpLZwC4V</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MTy0QEENQxpkYpLZwC4V</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jul 2024 08:43:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, Dot, Mem0, AI聊天应用
<br>
<br>
总结: 一款AI聊天应用Dot在App Store上线，具有长记忆挖掘能力，能够帮助用户思考生活、发现隐藏联系并提升自我。背后核心技术“超强个性记忆”被Mem0ai开源，提供智能、自我改进的记忆层，实现跨应用的个性化AI体验。 Mem0提供多层次记忆、自适应个性化、开发者友好的API等功能，是开发者创建个性化和上下文感知AI应用的强大工具。 </div>
                        <hr>
                    
                    <p>最近，拿到OpenAI 370万美元投资的一款AI聊天应用在App Store上线了。国内外AI聊天工具层出不穷、屡见不鲜，为什么这款应用却能受到OpenAI的青睐呢？</p><p>&nbsp;</p><p>这款名为Dot的应用 ，由总部位于旧金山的创业公司New Computer打造，由前苹果设计团队的成员Jason Yuan设计，编码工作则由Sam Whitmore等一小拨人完成。这个应用的名字就像乔布斯的名言一样“connecting the dots”，将生活里的点点滴滴，以某种方式联系起来。</p><p>&nbsp;</p><p>它最与众不同的是具有长记忆挖掘能力。人类的记忆有限，但是Dot拥有超长的记忆能力，你可以随时cue它回答关于你的任何回忆，你发送的文字、语音备忘录、图片、PDF文件，它都用来形成它的记忆，从而成为一个随时在线的伴侣，帮助你思考生活、发现隐藏的联系并提升自我。</p><p>&nbsp;</p><p>Yuan称用户与Dot的对话是一部用户个人的“活历史”，种追溯模式和展望未来可能性的方式。</p><p>&nbsp;</p><p>Dot 作为AI聊天工具，展现出的AI 应当有处理复杂上下文信息和长期记忆的能力，显然是它最大的亮点。ChatGPT 也有同样的记忆功能，但你需要主动要求它记住关于你的信息，而且它的记忆比较零散。</p><p>&nbsp;</p><p>今天，这款爆火的 AI 应用其背后核心的“超强个性记忆”技术被 Mem0ai 给开源了！</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad535a8d0046a351c052f6f9ac39317f.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>Mem0 可以用来开发长期、短期记忆，它能记住用户的偏好、过去的交互、事情的进展，可以为应用构建适应性的学习体验。使用场景包括虚拟陪伴、生产力工具、健康关怀或 AI Agent 客户支持等。</p><p>&nbsp;</p><p>开源不到一天，就收到了9.7k颗星，可谓是风靡全球，联合创始人Taranjeet Singh都感到有些受宠若惊了！</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d62343f77b3212195d86054999c6d8c.jpeg" /></p><p></p><p>&nbsp;</p><p>Taranjeet Singh 是 Mem0 的联合创始人兼CEO。他的软件工程职业生涯始于 Paytm（印度的 PayPal），见证了 Paytm 从一个新兴企业迅速成长为家喻户晓的名字。</p><p>&nbsp;</p><p>另一位联合创始人兼CTO为Deshraj Yadav，曾领导特斯拉自动驾驶的AI平台，支持大规模训练、模型评估、监控和可观察性，以促进特斯拉全自动驾驶的发展。在此之前，Deshraj 在乔治亚理工学院完成硕士论文时创建了 EvalAI，一个开源的机器学习平台。</p><p>&nbsp;</p><p>Mem0 同时也是 YC 投资的项目。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc00271281d19a3a909b686517dc3c5a.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>个性化AI的记忆层</h2><p></p><p>&nbsp;</p><p>简单的说，Mem0为大语言模型提供了一个智能、自我改进的记忆层，实现了跨应用的个性化AI体验。其核心功能包括多层次记忆、自适应个性化、开发者友好的API、跨平台一致性，并且你可以在本地计算机上运行这个程序。</p><p>&nbsp;</p><p>Mem0 是 RAG 发展的下一个阶段，相比 RAG 的核心区别：关注实体和实体关系；关注最近、最相关的；上下文连续性；适应性学习；动态更新信息。而普通 RAG 只是单纯的从静态的文档中检索信息。</p><p>&nbsp;</p><p>具体来说，Mem0 提供的记忆实现相比RAG具有以下优势：</p><p>关注实体关系：Mem0 能理解和关联不同交互中的实体，而 RAG 则从静态文档中检索信息。这使得 Mem0 对上下文和关系的理解更深刻。最近性、相关性和衰减：Mem0 优先考虑最近的交互，并逐渐忘记过时的信息，确保记忆保持相关和最新，以提供更准确的响应。上下文连续性：Mem0 在多个会话中保留信息，保持对话和交互的连续性，这对于长期参与应用，如虚拟伴侣或个性化学习助手来说至关重要。自适应学习：Mem0 根据用户交互和反馈改进其个性化，使记忆随着时间的推移更加准确和贴合个人用户。动态更新信息：Mem0 能够根据新的信息和交互动态更新其记忆，而 RAG 依赖于静态数据。这允许实时调整和改进，提升用户体验。</p><p>&nbsp;</p><p>这些先进的记忆功能使 Mem0 成为开发者创建个性化和上下文感知AI应用的强大工具。</p><p>&nbsp;</p><p>并且Mem0还提供了开发者友好的API，安装和使用也很简单。</p><p>&nbsp;</p><p>要安装 Mem0，您可以使用 pip。在终端中运行以下命令：</p><p>&nbsp;</p><p><code lang="null">pip install mem0ai</code></p><p>&nbsp;</p><p>初始化之后就可以使用一些基本的API，比如：</p><p>&nbsp;</p><p></p><h4>储存记忆</h4><p></p><p><code lang="null"># For a user
result = m.add("Likes to play cricket on weekends", user_id="alice", metadata={"category": "hobbies"})
print(result)</code></p><p>输出：</p><p>&nbsp;</p><p><code lang="null">[
  {
    'id': 'm1',
    'event': 'add',
    'data': 'Likes to play cricket on weekends'
  }
]</code></p><p>&nbsp;</p><p></p><h4>找回记忆</h4><p></p><p>&nbsp;</p><p><code lang="null"># Get all memories
all_memories = m.get_all()
print(all_memories)</code></p><p>输出：</p><p>&nbsp;</p><p><code lang="null">[
  {
    'id': 'm1',
    'text': 'Likes to play cricket on weekends',
    'metadata': {
      'data': 'Likes to play cricket on weekends',
      'category': 'hobbies'
    }
  },
  # ... other memories ...
]</code></p><p></p><h4>搜索记忆</h4><p></p><p>&nbsp;</p><p><code lang="null">related_memories = m.search(query="What are Alice's hobbies?", user_id="alice")
print(related_memories)</code></p><p>输出：</p><p>&nbsp;</p><p><code lang="null">[
  {
    'id': 'm1',
    'text': 'Likes to play cricket on weekends',
    'metadata': {
      'data': 'Likes to play cricket on weekends',
      'category': 'hobbies'
    },
    'score': 0.85  # Similarity score
  },
  # ... other related memories ...
]</code></p><p>&nbsp;</p><p></p><h4>删除记忆</h4><p></p><p>&nbsp;</p><p><code lang="null">m.delete(memory_id="m1") # Delete a memory


m.delete_all(user_id="alice") # Delete all memories</code></p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://github.com/mem0ai/mem0">https://github.com/mem0ai/mem0</a>"</p><p><a href="https://docs.mem0.ai/overview">https://docs.mem0.ai/overview</a>"</p><p><a href="https://x.com/tuturetom/status/1813932933482455156">https://x.com/tuturetom/status/1813932933482455156</a>"</p><p><a href="https://x.com/taranjeetio">https://x.com/taranjeetio</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tqPIOFDHlVbpELk4jrYb</id>
            <title>别找啦！AIGC+金融场景的绝佳案例都在这</title>
            <link>https://www.infoq.cn/article/tqPIOFDHlVbpELk4jrYb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tqPIOFDHlVbpELk4jrYb</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jul 2024 10:19:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融行业, 大模型应用, AIGC, 金融科技大会
<br>
<br>
总结: 金融行业因其专业知识密集、数据驱动、业务流程复杂性等特点成为大模型应用的理想领域。AIGC在金融行业落地并取得初步成果，涉及风控、营销、运营等领域。金融机构在大模型时代下不得不采用大模型，以提高风险感知、风控决策能力。金融科技大会将展示金融数智化实践的案例，为行业提供更多参考。 </div>
                        <hr>
                    
                    <p>金融行业被视为大模型应用的理想领域。从内因看，是因为金融本身具有专业知识密集、数据驱动、业务流程复杂性三个显著特点，而三大特点恰好与大模型理解能力、记忆能力、逻辑推理等优势高度吻合。从外因看，在政策驱动和市场热度的双重助力下，对于每一家金融机构来说，不采用大模型几乎是不可能的。</p><p></p><p>那么，经过一年多的探索，AIGC在金融行业落地情况如何了？哪些场景刚开始探索，哪些场景已经取得初步成果？在8月16日-17日即将于上海举办的FCon全球金融科技大会上，InfoQ搜罗了10+来自银行、保险、证券和金融科技等不同行业的AIGC+金融场景的绝佳案例，覆盖风控、营销、运营、研发等领域，希望为金融数智化实践提供更多参考。以下为部分议题介绍，更多重磅议题仍在实时更新中，欢迎前往大会官网进一步了解：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><h2>风控还是大模型“禁区”吗？</h2><p></p><p></p><p>数字化风控是金融行业的基石，安全与效率始终是其核心追求。在AIGC技术的浪潮中，逼真的AI生成内容对安全审核提出了前所未有的挑战；同时，金融数据的海量积累也对风控的智能化和效率提出了更高的要求。为应对这些挑战，度小满搭建了攻防对抗框架，不断迭代优化伪造检测系统，保障金融交易的安全性。此外，其还通过文档智能技术方案，自动提取和解析金融文档中的关键信息，极大提升了数智化处理的效率。</p><p></p><p>在「前沿金融科技探索与应用」专题论坛，度小满金融数据智能部计算机视觉方向负责人万阳春将分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6030">计算机视觉技术在金融数字化风控中应用</a>"》。</p><p></p><p>聚焦反欺诈领域，随着消费金融行业的快速发展，个人和团伙欺诈行为日益猖獗。近年随着技术进步特别是AI技术的广泛应用，欺诈攻击手段呈现线上化、多样化和专业化趋势，传统反欺诈手段应对乏力，给金融机构和消费者带来了巨大的风险挑战。因此，构建一个适应当下的新型反欺诈技术体系成为当务之急。</p><p></p><p>中邮消费金融科技发展部AI算法专家陈盛福同样将在该专题下带来《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6066">消费金融风控新防线：智能反欺诈技术体系全解析</a>"》的议题分享。通过介绍当前消费金融场景中的欺诈攻击现状，结合智能反欺诈旅程和实际落地经验全面剖析全流程解决方案，特别针对反欺诈涉及到的AI技术体系展开深入讲解，并展望在AIGC和大模型时代背景下的未来反欺诈新方向，探索针对新型攻击的提前布局，以魔法打败魔法，为消费金融领域筑牢新防线。</p><p></p><p>此外，在金融科技的浪潮中，账户风险管理也一直是金融机构关注的焦点。传统的人工驱动流程在处理复杂的欺诈案件时，不仅耗时且容易出错。随着大模型技术的兴起，越来越多的金融机构正在试图通过智能化手段，提高风险感知和风控决策的能力，从而降低人工失误率，提升运营效率。</p><p></p><p>在「金融数字化管理和运营实践」专题论坛，平安壹钱包大数据研发部算法负责人王永合将深入探讨如何利用大模型技术，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6031">实现账户风险管理的数字化转型</a>"，以及这一转型如何为金融机构带来实质性的价值。</p><p></p><p>可以看到，随着应用的日渐深入，金融机构对于技术开始从摸索转变为“要效益”、“要闭环”。在「金融大模型应用实践和效益闭环」专题论坛，新希望金融科技风险科学部AI中心总经理王小东将在演讲《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6011">大模型下的多模态智能风控落地实践</a>"》中介绍新希望金融科技AI团队利用视觉大模型AI风控、语音大模型AI风控、音视频+AI交互式智能风控等技术解决大模型浪潮下的各种新型信息伪造和欺诈攻击手段的技术算法解决方案和落地效果，并介绍在OCR、活体检测、视频双录环节的应用案例。</p><p></p><p>据了解，该方案已在600多家银行应用。通过大模型、交互式视频AI风控等实现了生成式大模型引发的新型金融反欺诈检测与识别以及破局之道，为金融反欺诈提供了一种新的解决方案。</p><p></p><h2>营销是大模型见效最快的场景吗？</h2><p></p><p></p><p>从用户角度来看，AIGC带来更智能、更便捷的体验。智能客服能够理解更复杂的语言，提供更精准的答案；个性化推荐系统可以根据用户喜好和需求，提供更有针对性的金融产品和服务；数字人可以全天候在线，提供更亲切、更人性化的服务。</p><p></p><p>对于银行来说，AIGC是一个能够赋能业务、提升效率的强大工具。AIGC可以帮助银行更精准地进行营销，通过分析用户数据，向不同群体推送个性化的金融产品信息，提高营销转化率。此外，AIGC还可以协助银行进行风险控制，识别潜在风险，帮助银行做出更明智的决策。</p><p></p><p>与此同时，AIGC还能为银行带来全新的业务模式。例如，数字人直播可以为用户提供更生动的金融知识讲解，更直观地展示金融产品，提升用户参与度和满意度。</p><p></p><p>围绕以上多个维度，在「数据资产化运营与数据智能应用」专题论坛，广发银行信用卡中心商业智能负责人徐小磊将<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6012">通过实际案例展示AIGC如何为金融科技带来变革</a>"。</p><p></p><p>针对整个体系化的银行运营和营销体系，富滇银行数字金融中心副主任李涛将在「金融数字化营销实践」专题论坛中分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6048">数智化时代商业银行运营营销的“坑”与“路”</a>"》，从几个发人深省的“灵魂拷问”出发，如银行公私域运营模仿互联网电商可持续吗、北极星指标是个坑吗、而全的指标标签体系真的能赋能银行数字化营销吗等等，介绍富滇银行自身的答案和解法以及在这一过程中的人工智能应用实践。</p><p></p><p>与此同时，在「金融数字化管理和运营实践」专题论坛，度小满数据智能经营模型负责人李东晨还会进一步聚焦运营场景，分享《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6005">基于因果推断的智能经营模型体系</a>"》，帮助听众了解信贷领域的经营模型框架体系，理清从预测到决策因果推断技术如何更好地支撑企业决策优化问题，以及从营销到盈利因果推断如何支撑所有资源有限情况下的最优求解问题。</p><p></p><h2>大模型是研发人员的福还是“祸”？</h2><p></p><p></p><p>大模型如何服务于研发生产力，同时做到普惠化，一是AI的基础设施，二是着重于能够云化落地的业务，三是结合AI给企业带来切实的降本增效。</p><p></p><p>AI代码助手，如GitHub&nbsp;Copilot、&nbsp;CodeX等，已成为现代软件开发中不可或缺的一环，它们极大地加速了代码编写的进程，提升了工作效率。然而，伴随而来的是对代码质量、开发流程乃至开发者角色的深刻挑战。特别是在金融这一数据密集型行业，对代码精准性、数据合规性的要求严苛至极。如何让AI模型在金融行业的研发领域得以切实有效应用，真正助力研发人员提升效能，而非仅成为初级开发者的辅助工具或高级开发者的互动玩具，是我们亟需解决的问题。</p><p></p><p>在「金融研发效能提升路径与实践」专题论坛，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6056">众安银行高级架构师汤杰</a>"将从架构设计、算法工程化融合、团队协作策略、工具选型与整合等多个层面，深入探讨在AI助手日益普及的背景下，如何构建一套既提升开发效率又保障代码质量的软件开发生态。同时，基于众安国际丰富的实践经验与分析反思，他还将分享对AI助手在软件开发中角色定位的前瞻思考，以及对AI辅助编程未来发展趋势的展望。</p><p></p><h2>怎么让AI为你打工？</h2><p></p><p></p><p>「智能体」被视为是AIGC规模化应用的第一入口。而随着大模型与智能体技术的快速发展，多智能体协同模式在在解决复杂金融问题方面展现出巨大的潜力。在实际的业务发展过程中，蚂蚁集团通过使用多智能体协同范式，克服了众多技术落地难点取得阶段成果。在「金融大模型应用实践和效益闭环」专题论坛上，蚂蚁财富投研支小助技术负责人纪韩将深入探讨<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5996">多智能体协同范式在金融产业中的技术应用</a>"并分享经产业验证的优秀真实案例。</p><p></p><p>成本是眼下要解决的另一大难题。在大模型时代背景下，“精益地迭代”或成为推动技术进步的关键。如何更好地构造知识驱动引擎，助力企业构建专家智能体建设，实现知识的高效转化和应用——成为很多企业正在攻克的关键问题。文因互联董事长、创始人鲍捷博士将在「前沿金融科技探索与应用」专题论坛分享如何《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5944">精益地打造金融专家智能体</a>"》。在业务分析领域，以“财务反粉饰”为场景示例，讨论如何结合专家知识管理系统进行有效的财务反粉饰，同时分析在这一场景下大模型能够发挥的作用及其面临的挑战。</p><p></p><p>可以看到，尤其是在知识密集和作业密集型场景，大模型越有的放矢。嘉银科技在这两个领域进行了深入的探索和实践，例如ToB主流AI产品、职能单元助手、智能作业辅助等业务，最终实现了效益闭环与专家已知解和算法暴力求解的平衡。在「金融大模型应用实践和效益闭环」专题论坛中，<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6033">嘉银科技技术中心人工智能经理姜睿思</a>"将详细介绍具体的大模型落地过程，技术和方法论层面的实践经验。</p><p></p><p>此外，中关村科金资深AI产品总监曹阳也将介绍《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5993">基于知识助手的金融大模型应用实践</a>"》，帮助金融从业者理解并应对大模型应用中的成本问题，包括如何进行模型选型、评估投入产出等；深入探讨金融大模型的安全与合规问题，了解有效的数据保护和风险管理策略；同时，通过案例了解如何评估哪些场景适合作为金融大模型应用的切入点。</p><p></p><p>更多AIGC场景应用案例还在上新中，本届大会由中国信通院铸基计划作为官方合作机构，除了以上嘉宾之外，还有来自工银科技、北京银行、平安银行、中信银行、平安证券、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。大会火热报名中，7&nbsp;月&nbsp;31&nbsp;日前可以享受&nbsp;9&nbsp;折优惠，单张门票节省&nbsp;480&nbsp;元（原价&nbsp;4800&nbsp;元），详情可点击链接或扫码联系票务人员咨询：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ff5488cc076e04976f66fd5d9869c7.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PSyuBLvTIBXb8w8OzLLx</id>
            <title>真·智能体峰会：MSRA、腾讯、网易、MILA 齐聚一堂 ｜AICon</title>
            <link>https://www.infoq.cn/article/PSyuBLvTIBXb8w8OzLLx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PSyuBLvTIBXb8w8OzLLx</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jul 2024 07:09:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI Agent, 机器学习, 智能体, 安全性
<br>
<br>
总结: AI Agent 是通过机器学习和人工智能技术实现自主感知环境、做出决策并执行相关动作的智能体。随着应用场景日益广泛，AI Agent 面临着更深入理解人类社会行为、解决安全性和隐私保护问题以及提升决策过程透明度和可解释性的挑战。在未来发展中，需要关注智能体的构建、感知、认知和行动能力的提升，以及多智能体技术在不同领域的探索应用。 </div>
                        <hr>
                    
                    <p>AI Agent 正迅速成为大模型非常重要的应用方向，这些智能实体通过先进的机器学习和人工智能技术，能够自主感知环境、做出决策并执行相关动作。AI Agent 的应用场景日益广泛，包括但不限于数字员工、具身智能、个性化推荐等。</p><p></p><p>然而，这一技术的发展并非一帆风顺。AI Agent 需要更深入地理解人类社会行为，包括语言、情感以及复杂的社会互动，以更好地适应多样化的应用场景。同时，随着其在各行各业的广泛应用，安全性和隐私保护问题变得尤为关键，确保数据安全和用户隐私是企业必须优先考虑的问题。此外，AI Agent 的决策过程的透明度和可解释性也是提升用户信任、推动技术进步的重要方面。</p><p></p><p>为了深入探讨这些挑战，并探索智能体技术的未来发展，我们在8 月 18 日 -19 日的 AICon 全球人工智能开发与应用大会（上海站），精心策划了【AI Agent 技术突破与应用】论坛，专题出品人是 DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖，他拥有十亿级用户的大规模 AI 落地经验；同时也是开源多智能体框架 MetaGPT 作者；NeurIPS AutoDL / NeurIPS AutoWSL / KDDCup OGB-LSC 等竞赛世界冠军；也曾获福布斯 30U30 等荣誉。</p><p></p><p>我们荣幸地邀请到了以下几位在智能体领域有着深刻见解和丰富经验的专家学者，他们将为我们带来一系列精彩的议题，共同探讨智能体的现在与未来。</p><p></p><p></p><h5>精彩议题一：</h5><p></p><p></p><p>如果你想了解构建智能体中需要考虑哪些组件？当下的智能体构建还存在哪些问题？以及智能体的未来发展会是什么样？那么，微软亚洲研究院高级研究员宋恺涛的主题分享《The Future is Here, A Deep Dive into Autonomous Agent》值得听听。他将从 AI 智能体的崛起入手开始分享，着重分析如何构建、评估以及轻量化 AI 智能体，当然他也会分享如何构建自我进化的 AI 智能体。有专家反馈说，这个技术国内和国际上都是顶尖的，错过可惜！</p><p></p><p></p><h5>精彩话题二：</h5><p></p><p></p><p>如果你想了解如何系统性增强 LLM Agent 的感知、认知、和行动，以提升其在不同任务中的应用效果，让你的智能体更加智能，那蒙特利尔大学 &amp;MILA 研究所助理教授刘邦的演讲不可或缺。</p><p></p><p>刘邦将会深入分析和对比不同环境和任务对 LLM Agent 感知、行动能力及认知推理的独特要求，并探讨如何通过技术创新解决这些挑战。</p><p></p><p></p><h5>精彩话题三：</h5><p></p><p></p><p>如果你是游戏圈的从业者，那你应该听过前段时间比较火热的永劫无间的 AI 队友，这个 AI 队友不仅能听懂玩家的话 (语音信息识别)、观察战场局势 (战局信息输入)、了解地图和英雄技能 (游戏机制学，甚至还借助诸多高手的大数据学会了高端操作。真是惊呆了许多参与游戏的人，那么这样的游戏 Agent 是如何构建的呢？</p><p></p><p>我们为你邀请到了网易伏羲语言智能组负责人张荣升， 他将以《可实时语音交流的游戏队友 AI Agent 创新应用》 为主题，为你展开分享。他将重点介绍如何基于易生诸相多模态 AI 技术，实现《永劫无间手游》中的可实时语音交流的游戏 AI 队友，并打造丝滑的多模态游戏交互体验。</p><p></p><p>通过他的分享，你可以了解到《永劫无间手游》中 AI 队友的实现方式，包括实时语音交流和多模态交互技术，以及了解如何通过 AI 技术提升游戏玩家的交互体验。</p><p></p><h5>精彩话题四：</h5><p></p><p></p><p>无独有偶，创新类型的 Agent 应用， 腾讯也有，我们荣幸邀请到了腾讯 PCG 大模型中台 Agent 技术负责人陈浩蓝，他将为你以《多智能体技术在开放剧情扮演玩法中的探索》 为主题展开分享。</p><p></p><p>他将为我们深入介绍开放剧情扮演玩法，从其基本概念出发，探讨与传统剧情扮演的不同之处以及它对玩家的吸引力。接着，他会概述当前在剧情生成和角色扮演领域的主要研究工作，包括关键技术框架和方法论。</p><p></p><p>随后，他将详细讨论使用多 Agent 技术生成开放且精彩剧情时遇到的技术挑战，以及在单场剧情中进行角色扮演时的难点，分享目前的研究进展和探索方向。</p><p></p><p>通过他的分享，你可以了解到开放剧情扮演玩法的任务背景和相关研究、以及了解基于大语言模型和多智能体技术解决相关问题的难点与探索进展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f2f88d59994b6e527f276578f9e3b8b.jpeg" /></p><p></p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 AICon 全球人工智能开发与应用大会，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c696f9be4a5aac17ed8d957c5df7621.jpeg" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CLrzPOWW6DuxEgIJr7hX</id>
            <title>大模型如何重塑企业知识管理？丨对话AI原生《云智实验室》</title>
            <link>https://www.infoq.cn/article/CLrzPOWW6DuxEgIJr7hX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CLrzPOWW6DuxEgIJr7hX</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jul 2024 01:39:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知识管理, 大模型时代, 企业价值, 甄知
<br>
<br>
总结: 本文讨论了知识管理在企业中的重要性，随着大模型时代的到来，企业知识管理面临新的机遇和挑战。甄知作为一站式知识管理平台，通过大模型技术重塑知识管理流程，提升企业效率，解决知识管理领域的难题，帮助企业构建知识体系，实现知识的显性化和智能化应用。 </div>
                        <hr>
                    
                    <p>知识管理是企业持续发展和创新的核心动力之一，长期以来却面临着效率低下的挑战。大模型时代的到来，为企业知识管理带来了新的机遇，如何通过大模型重塑企业知识管理全流程进而提升效率？如何拓宽知识的边界，挖掘知识的内在价值？大模型重构的知识管理又能为企业带来哪些价值？带着这些问题，在对话AI原生《云智实验室》栏目中，南网科研院知识管理专家林正平、百度智能云知识管理产品线总经理宋勋超与InfoQ编辑展开了一次深度探讨。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb0615ae51f0c56a73578c4b14faa7d1.png" /></p><p>点击链接收看：《大模型如何重塑企业知识管理？》</p><p>https://www.infoq.cn/video/QkMRVDNg4MKJ9eyS6Y2E</p><p></p><h3>以下为本期栏目精华内容</h3><p></p><p></p><p>InfoQ：企业知识管理是什么？对于企业来说有何价值？</p><p></p><p>宋勋超：知识管理自上世纪60年代在国外逐渐兴起，1998年以后，国内的企业开始探索知识管理的应用。</p><p>知识管理发展主要分为三个阶段：</p><p></p><p>第一个阶段是基础数字化的阶段。在这个阶段的知识管理平台，更多的是将企业散落的各种数据进行归总和汇集，但没有对知识去进行深度的利用，初期的知识管理主要是文档系统。</p><p></p><p>第二个阶段是知识的信息化阶段。在这个阶段，企业将收集的个人知识和系统知识进行粗粒度的加工和整理，进而形成了知识门户或者是企业搜索。</p><p></p><p>第三个阶段是大模型时代的知识管理，在大模型的加持下，知识管理的全流程效率都有了显著的提升。大模型强大的理解能力，使得知识管理平台以往很难去解决的知识加工和应用问题都迎刃而解。</p><p></p><p>在第三阶段，知识管理可以更加深入到业务，使得知识管理在企业内更加具象化为一个可以永远传承知识的老师傅，为企业员工提供持续的知识供给，带给企业的价值可以更好地去被衡量。</p><p></p><p>InfoQ：甄知与传统知识管理平台有何区别？</p><p></p><p>宋勋超：甄知是国内首个大模型全面重构的一站式知识管理平台，可以全面重塑知识管理流程。其实知识管理领域一直有四个问题是非常难解决的，也是制约着知识管理发展的瓶颈：</p><p></p><p>• 知识源分散</p><p>• 知识获取难度高</p><p>• 知识更新慢</p><p>• 基于知识的应用薄</p><p></p><p>有了大模型，实际上知识管理进入了新的阶段。之前的这四个问题如果在小模型时代去逐一地去解决，需要耗费非常高的成本，每一个模型都需要去对它进行大量的数据标注和精调。而在现在的大模型时代，我们可以不用任何工程化的手段就能够解决上述问题，成本极低。</p><p></p><p>另外，我认为大模型能给知识管理带来的最大的变化，其实就是Agent技术。一个智能体它要去执行复杂的任务，其不仅要理解任务本身，更多的是形成知识驱动的解决范式，所以我也认为知识管理是大模型成熟应用的第一站。</p><p></p><p>InfoQ：南网科研院在知识管理上遇到了哪些难题？</p><p></p><p>林正平：南网科研院过去标准知识管理主要还是针对文档级别，过去一线的生产人员只能通过纸质的标准或者在标准平台上通过标准名进行信息检索，怎么样去驱使所有人用统一的标准去完成相关工作，是我们在整个企业运作过程中急迫要去解决的一个问题。这是标准知识管理面临的最大的问题。</p><p></p><p>InfoQ：甄知如何解决南网科研院在知识管理上的难题？</p><p></p><p>宋勋超：甄知和南网科研院已经合作了数个年头了，我们最初要解决的问题就是知识的显性化问题，很多企业也会有类似的需求——企业里有很多经验，这些经验都存在于老专家的脑子里，你们能不能给我提供一个平台或系统，从而把老专家脑子里的知识“萃取”出来？</p><p></p><p>这个愿景对应的需求就是“能够将各种规则，各种隐性的数据引入知识管理系统”。针对这样的原始的需求，甄知知识管理平台提供了多种形式的数据接入方式。比如API方式的接入，手动的上传以及推拉拽等等一系列的知识同步机制，企业能够将结构化、或者是大量存在的非结构化的数据，甚至是数据类的知识，快速地去接入到系统里。</p><p></p><p>2024年甄知还有一个比较重要的产品升级：平台对接了非常多的企业级的知识源，比如说confluence、wiki、各类企业网盘，甚至包括飞书、钉钉文档、企业微信文档，这些在企业内广泛存在的企业级的知识源，在甄知的平台上能够去快速地、无缝地去集成和接入，极大程度地提升整个知识接入的效率。</p><p></p><p>更为重要的是甄知平台提供AI驱动的协同编辑和智能化的写作工具组件，使得企业很多在日常工作里面能够去随写随记的这一部分知识，都能够实现即创作、即沉淀，效率提升极大，且管理成本较低。</p><p></p><p>InfoQ：甄知怎么帮助企业构建知识体系？</p><p></p><p>宋勋超：面向于大模型的应用，我们需要把原始知识、原始数据整理成大模型能够理解的形态。这也是知识管理传统的系统耗费人力最多的地方之一。甄知的知识管理平台提供三个比较核心的技术能力，第一是多模态解析的能力，第二是知识结构的解析能力，第三是知识要素的加工能力。</p><p></p><p>多模态解析的能力是指现在越来越多的企业，它广泛的数据存在于非结构化的文档里面，这些非结构化的文档里包含各种复杂的表、图以及大段的文字。以南网科研院为例，几万篇的国标、行标、企标，都是以PDF文档的形式存在。甄知借助大模型的OCR视觉理解能力，能够非常便捷、高效、快速地把企业内存量的多模态文档解析成章、条、目、段落，甚至是图表，这对于后面的知识加工成功与否非常关键。</p><p></p><p>另外，面向于企业级的搜索和RAG，甄知提供了可配置化的段落切分的能力，可以去配置固定的长度。最为重要的是甄知能够用智能化的手段非常精确的识别语义化单元，从而使我们的RAG和搜索能够达到一个非常精准化的程度。借助大模型，刚刚所说的这一系列加工过程，都能以非常高效的方式，实现大规模的生产，企业只需要去配备一些必要的知识运营人员，对大模型生产的结果进行审核、校验和入库，相比于传统的方式，至少提升了2-3倍的工作效率。</p><p></p><p>InfoQ：南网科研院是怎么搭建知识体系的？</p><p></p><p>林正平：南网科研院其实非常关注科技创新这个领域。我们有海量的，像论文、标准、专利还有项目成果等等相关的一些知识数据，以往我们的项目数据怎么分到技术体系树里面去是一个非常大的难题。过去我们更多是根据一些规则，就是专家去梳理一些文本规则来对这些资源进行分类。整个效率相对较低，另外它的准确率也不够高，需要大量的人工去审核。</p><p></p><p>现在有了大模型，还有相关的智能化技术加成后，在知识分类上我们会根据业务需求组建知识体系框架，通过模型来自动打标签，实现海量知识的快速分类，便于对这些知识进行分析还有查找，目前我们的效率提升还是比较明显。</p><p></p><p>另外就是在知识关联关系的挖掘上，以往我们的知识都是单兵作战，像我们有一个论文，我只能看到这个论文它自身的一些内容，在知识关联上很少能够挖掘。像标准也是，标准之间它有引用关系，以往我只能看到某一个标准，但是我不知道这个标准跟它相关的一些作业指导书有哪些，跟它相关的技术规范书有哪些？在知识的利用上还不是非常的深入，对于整个知识的关联发现也非常难去完成。有了智能化技术的加成，平台可以更好地去挖掘知识之间的关联关系，最终来构建知识图谱，支撑标准文、论文相关知识的推理以及问答等应用，这些对我们的帮助都是非常的大。</p><p></p><p>InfoQ：甄知通过大模型重构知识体系的技术突破点是什么？</p><p></p><p>宋勋超：对于知识组织而言，我们的知识都来源于各个孤立的业务系统，它本身并不具备体系化和组织化。本质上来讲，知识组织和体系的建立，就是非常复杂的一个多维分类体系的建立。</p><p></p><p>传统的知识管理时代基本上都是靠人，基于大模型非常强大的语义理解能力，我们能够通过简单prompt，可以做到Zero-shot和Few-shot这样的分类，可以让运营人员很少参与到这个过程，就能够去实现复杂分类体系的建设。过去知识图谱的技术，它从关系的挖掘、消歧再到鉴编，这一系列的环节依赖于非常多的小模型。现在有了大模型，实际上在一些应用场景里面已经验证基本上不太需要专业的数据标注，基于大模型本身的理解能力，结合知识图谱平台构建的机制，就能够在初始的准确率达到80%以上。</p><p></p><p>在大模型加持下，甄知在知识组织与知识关联方面展现出强大能力。目前，我们在应用层面已成功实现了基于知识关联的门户构建，以及个性化推荐等一系列功能，真正做到了“千人千面”。事实上，这些能力的实现都得益于大模型的引入，它使我们能够以更低的成本，对企业知识进行系统化的组织和管理。</p><p></p><p>InfoQ：甄知如何提升南网科研院的知识生产效率？</p><p></p><p>林正平：在有大模型之前，我们跟百度合作开展标准问答相关的一些研究，以往我们是采用知识图谱的方式，我还记得是在两三年前，我们组织了大量的专家对我们标准的指标体系进行了深入的梳理，总共是梳理了十几级的体系，最终通过知识抽取、消歧融合等等相关的一系列的工作，才构建了我们试点设备的标准知识图谱，最终才能支持标准的知识问答。</p><p></p><p>那有了大模型之后，现在其实我们需要专家参与的工作量已经大大的减少，目前基本上我们只需要给一些简单的问答，通过大模型去训练，就可以完成问答模型的构建以及问答应用的开发。这对我们知识管理人员来说是一个极大的效率提升，大大的解放了我们的生产力。我们现在构建了标准的问答，还有语义检索相关的服务工具，对标准知识的查阅效率已经提升50%以上。</p><p></p><p>目前，大模型还能辅助员工去编写标准相关的大纲、正文并可以撰写科研机构的科技报告的综述、背景等等的相关的一些内容，我们的编写效率也是提升了两倍以上，对企业的知识生产运行效率来说提升还是非常明显的。</p><p></p><p>InfoQ：甄知如何帮助企业用好知识？</p><p></p><p>宋勋超：知识要能够驱动创新，它必然是要和场景结合的。刚刚我们说知识管理在南方电网就好像一个老专家。但是这个老专家他其实应该是有角色的，面向于企业的高层管理人员，他可能更加关注企业的宏观经营数据，面向于企业的中层管理人员，他可能更需要去了解项目的进度；面向于技术人员，他可能更关心的是技术方案。企业里面的每一个关键角色对于知识的诉求都是不一样的。所以我们认为如果要在一个企业内广泛地实现知识的创新，就必须要将知识发散，也就是要把知识应用到业务系统里面，形成真正的业务和场景化的知识助理。</p><p></p><p>甄知实际上为企业提供了三方面的能力，以助力其更好地利用知识。首先是集中化的RAG能力，甄知打造了一个企业级RAG平台，确保企业中的每位员工在知识管理系统中提问时，都能真正获得有价值的答案。</p><p></p><p>其次，甄知的推荐系统能够通过分析企业内部多元异构的员工画像，实现千人千面的个性化推荐，以及面向业务场景的即时推荐。甄知的知识管理平台已经不仅仅是一个知识门户或知识检索系统，它已经升级为一个企业知识化Agent构建的平台。在这个平台上，我们基于构建的知识体系，能够打造出知识化的差旅助理、报销助理、销售助理，甚至HR助理等一系列由知识驱动的Agent助理。我认为，只有当搜索、推荐和Agent服务这三个层面在同一个平台上实现出色的平台化整合时，我们企业内部的应用场景才会变得非常多元化，企业的创新源泉才会源源不断。</p><p></p><p>最后，甄知覆盖了企业的知识生产、加工及应用全流程，最终能够将企业员工、知识应用与知识生产形成一个闭环与反馈机制。因此，在使用过程中，我们不断收集企业对于知识管理产品应用效果的反馈，这使我们能够不断提升知识生产与加工的效率。</p><p></p><p>InfoQ：南网科研院如何用好知识，赋能每一位员工？</p><p></p><p>林正平：南网科研院一直在推进从人找知识升级为知识找人。南方电网整个企业岗位非常多，对效率的要求以及安全的要求也非常的高，我们开展知识管理的理念主要是从用户、场景和知识三维一体的角度来进行开展，比如说我们的科研人员关注怎样快速去获取相关领域的高质量的报告、核心的论文、最新的研究成果以及历史相关的项目经验等信息。而我们的一线生产人员则关更加关注缺陷处理的方法以及作业的规范，每个岗位对于知识的需求都有存在一定的差异。</p><p></p><p>以往，我们主要通过检索的方式来获取知识。然而，在人工智能时代，我们更需要从用户的画像和使用场景出发，为用户推荐有针对性、高质量的知识，以提升知识的获取效率。通过知识管理，我们希望总结并沉淀最佳实践，同时做好经验的传承。我们的知识管理平台的使命就是支撑知识的汇聚、存储和利用，甚至将知识直接嵌入到业务场景中去，无需人工干预，从而更好地支撑企业的创新和高质量发展。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kjEuszalPjjd6Zepefm3</id>
            <title>15年功臣、英伟达首席科学家在股价巅峰期黯然辞职：对不起自己拿的丰厚报酬？</title>
            <link>https://www.infoq.cn/article/kjEuszalPjjd6Zepefm3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kjEuszalPjjd6Zepefm3</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 09:37:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Jacopo Pantaleoni, 数据转换为图像, AI革命, GPU
<br>
<br>
总结: Jacopo Pantaleoni是一位专长将数据转换为图像的科学家，曾在英伟达担任首席科学家，为图形处理单元开发了旗舰产品。他的工作对数字图像世界、电子游戏、电影艺术以及生物信息学都产生了重大影响。然而，他选择离开英伟达，批评AI技术在广告经济中的应用，认为这可能破坏人们的生计。他认为人工智能代表计算能力革命的最高峰，但也警示AI技术可能被用于降低人力成本，最终危害人们的生计。 </div>
                        <hr>
                    
                    <p>很多朋友可能没听说过Jacopo Pantaleoni的名字，但或多或少应该见证过他的工作成果。Pantaleoni的专长是将数据转换为图像，作为首席科学家在英伟达供职期间为其开发了旗舰产品的图形处理单元。他的工作帮助塑造了超现实的数字图像世界，进而构成了从电子游戏到电影艺术、再到作为DNA测序研究核心的生物信息学的一切。除此之外，他的贡献对于AI革命同样意义重大，支撑起OpenAI及其他从业企业开启了这个前所未有的时代。</p><p>&nbsp;</p><p>但去年的时候Pantaleoni选择离开英伟达，当时正值OpenAI主导的AI热潮将这家图形芯片巨头推向科技界的顶峰。辞职的原因，他表示自己“需要一段时间来反思，必须承认我对社会的间接贡献大多数并不太正向。”</p><p>&nbsp;</p><p>此后他不仅直言不讳地批评AI技术没用对地方，还批评了英伟达在这场浪潮当中扮演的角色。在他看来，AI技术不应该用到广告经济上，更不应该破坏人们的生计。</p><p>&nbsp;</p><p></p><h2>在英伟达高光时期独自离开的首席科学家</h2><p></p><p>&nbsp;</p><p>虽然最近15年Jacopo Pantaleoni在为英伟达工作，但实际上他在高性能计算领域已经拥有25年的工作经验，并曾两次获得高性能图形学的“时间考验（Test of Time）”奖，该奖项主要是表彰那些对计算机图形学产生广泛且持久影响的研究。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/683333447d6e39b691c09ea46f857a14.jpeg" /></p><p></p><p>&nbsp;</p><p>他为实时光线追踪创建的基础算法——这一技术是当前许多元宇宙和数字人类项目的核心——已经在革命性地改变今天的游戏行业。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/323ddf9ab42b1f4f46762c751ee1e105.jpeg" /></p><p></p><p>&nbsp;</p><p>这些渲染技术能够帮助我们从不存在任何视觉元素的三维数据集模型生成极为逼真的图像。身为一名计算机科学家，Pantaleoni帮助开发了詹姆斯·卡梅隆里程碑之作《阿凡达》的图形系统，并为《黑客帝国》背后的技术公司之一Mental Images编写了程序。</p><p>&nbsp;</p><p>同样的技术在电子游戏中也有所体现，这也是英伟达等厂商在90年代末到21世纪初得以崛起的原因所在。英伟达开始生产硬件，将这种计算机渲染技术带给普罗大众，最直接的作用就是运行电子游戏。早在2010年，他和黄仁勋等人就意识到，计算机图形所需要的计算能力与机器学习/人工智能等任务需要的算力是一回事。这正是黄仁勋出色才能的体现，而像Pantaleoni这样的人则努力提高英伟达硬件的可编程性，帮助越来越多的人能够使用和支配这些算力，可以说Pantaleoni是 GPU 大规模并行高性能计算领域的早期贡献者（英伟达的首席科学家还有 David Kirk 和 Bill Dally等）。</p><p>&nbsp;</p><p>发展到现在，从人工智能热潮中获益最大的公司莫过于英伟达，黄仁勋曾说过该公司处于一场新“工业革命”的核心。训练大规模的大型语言模型并运行它们需要巨大的计算能力。英伟达的GPU已经是人工智能行业的黄金标准，亚马逊、谷歌、微软等云计算巨头和人工智能初创公司都在争相购买。</p><p>&nbsp;</p><p>自 2023年初以来，英伟达股价上涨了约785%，仅去年一年净利润同比大涨581%。今年6月，英伟达短暂地成为全球最有价值的公司。现在英伟达的市值已突破3万亿美元，相当于该公司2018年8月约1500亿美元市值的20倍。著名投资人詹姆斯·安德森更是大胆预测，在未来十年内，其市值有望达到惊人的50万亿美元，这一数字将超越当前标准普尔500指数所有公司的总市值。</p><p>&nbsp;</p><p>喜人的业绩之下，让押注英伟达股票的投资者们，分食了这场科技造富盛筵。就像一位投资者说的：“这种机会是十年难遇的，在我看来，英伟达一年之内，帮很多投资者赚到了原本用很多年才能赚到的钱。”</p><p>&nbsp;</p><p>作为英伟达的首席科学家，Pantaleoni也持有英伟达公司的股份，这些股份让他虽工作内容小众但收入丰厚，“哪怕是在股价上涨之前，就已经对我们的生活方式产生影响。”他们这群图形专家，“已经成为掌握全球大多数财富的那1%群体中的一员。”</p><p>&nbsp;</p><p>但他选择了在英伟达的上升期离开了这家他工作了十五年的公司，“我意识到，我对于这个社会的贡献，无论是直接还是间接的，其范围都比我想象中要大得多。我需要一段时间来反思、特别是以批判性的方式审视这一点。而且必须承认，我对社会的间接贡献大多数并不太正向。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>没想到自己的成果被用来摧毁普通人的生计</h2><p></p><p>&nbsp;</p><p>Pantaleoni毕生致力于提升计算和模拟技术，在他看来，人工智能从根本上代表着这一进程的最高峰，是一种可以融入和改进我们计算模型的全新且功能强大的工具。作为一种工具，人工智能可以在疾病诊断、基因组学、药物发现和气候模拟等领域发挥巨大作用。</p><p>&nbsp;</p><p>“我认为，将人工智能革命视为冰山一角更为恰当，其背后是更广泛、更深刻的计算能力革命。” Pantaleoni表示，“自从计算机发明以来，我们的计算能力一直在呈指数级增长。70 多年来，其速度每 18-24 个月就会翻一番。”</p><p>&nbsp;</p><p>虽然AI对像癌症检测这样的任务自动化确实非常有用，但目前极少数超大型公司在资本、基础设施和人力资源方面占据圧倒的优势地位，他们没有选择用这些技术造福人类，而是投放到了“注意力经济”和可能摧毁人们生计的地方：</p><p>&nbsp;</p><p></p><blockquote>“对人工智能的这种运用与所谓的注意力经济密切相关，注意力经济是机器学习算法和服务创造的新型市场，旨在抓住并保持用户注意力。这些产品通过不断向我们的大脑灌输大量程序生成和传播的 (错误) 信息，并限制我们集中注意力和花费时间将信息提炼成知识的能力，从而导致更广泛、更严重的认知衰弱。随着假新闻和阴谋论的疯狂传播，这一进程正日益将民主置于危险境地。&nbsp;然而，这场革命的另一个需要最大关注的方面是人工智能如何被应用于降低人力成本。生成式人工智能已经部分地实现了诸如客户服务、数字插画和计算机编程等领域的自动化。但真正危险的是，它将扩展到更敏感的领域，例如教育（例如通过数字人）和安全（例如自主武器）。虽然这些发展使商业运营成本降低，但它们最终也可能破坏人们的生计。&nbsp;此外，在更基本的层面上，我们必须记住，历史上自动化一直被广泛用作权力和权力集中的工具。我们已经看到，人工智能的进步正在推动科技领域一小部分超大型公司的资本、基础设施和人力资源的集中。”</blockquote><p></p><p>&nbsp;</p><p>自己创造的技术最终却是以这样的方式在发挥作用，这让他感到很意外，“我一生都致力于用技术手段将数学变成艺术，并将计算变成科学研究的工具——例如DNA测序。所以看到这项技术最终主要被用于广告宣传和注意力经济，这是有点令人失望。”</p><p>&nbsp;</p><p>“AI造成的风险是已知的。Sam Altman之类的人物提出的所谓灭世威胁，其实是想要以混淆视听的方式转移人们对最紧迫问题的注意力。毕竟他们往往是既得利益者，把水搅浑更有好处。”目前的主要风险在于权力的过度集中，进而导致工作岗位流失。另一个主要问题是，计算技术正在对整个人类社会施加愈发规模化的认知弱化压力。</p><p>&nbsp;</p><p>“我们将大部分注意力放在专门设计的算法上，而这些算法存在的意义就是娱乐我们的视觉皮层——这其实就是我一生中贡献最大的领域。我主要从事的就是视觉计算。视觉计算的力量实际上也正是英伟达等企业能够取得成功的原因。”</p><p>&nbsp;</p><p>Pantaleoni意识到不对劲的时候，大概是2014年到2015年那会，他开始对自己工作成果所产生的影响有了一种奇怪的感觉。“我见证了巨大的转变，看到谷歌和亚马逊这类厂商成为高算力硬件的大客户，并且在为完全不同的多种用途开发大规模并行计算能力。他们开始大规模使用机器学习技术，而目的却是为了靠广告宣传和吸引注意力赚钱。”</p><p>&nbsp;</p><p>“我认为这其实也就是谷歌获得成功的秘诀。”他们在本质上是一家机器学习公司，也是第一批从计算机设备的可扩展性中获得巨大利益的企业。以往一切制造公司都面临着收益递减的困扰，而他们是第一批克服了这种困境的厂商，在实质上完成了收益递增的转型。简单来说，如果我们有一家传统制造企业，那想要扩张就必须得向更多人力雇员支付更多工资。这就要求我们采购更多上游产品和原材料。但与之截然不同，谷歌这样企业的原材料却是完全免费的，单纯只是信息。他们要做的，就是投入越来越多的计算能力来处理这些素材。</p><p>&nbsp;</p><p>但作为英伟达的一员，实际上很难为此做出改变，英伟达虽然没有为注意力经济直接做出贡献，但却是支撑注意力经济正常运转的引擎。像英伟达这样的厂商没办法说“我们拒绝为这样的市场提供支持”。</p><p>&nbsp;</p><p>Pantaleoni之所以选择离开，就是意识到从企业内部改变结果根本就没有可行性。他认为从根本上打破这种对社会的负面影响的唯一方式，就是投身到监管事业中去。“我正尝试转型成为公共专家。我为监管机构提供咨询，也曾受邀在会议上发言。我不知道自己还要不要在技术这条路上走下去——也许会，但我必须找到真正符合自己对社会产生积极影响这个愿望的切入点。”</p><p>&nbsp;</p><p>比如说跟医学相关的项目，都是在尝试以积极、而非剥削的方式依靠技术来解决医疗难题。或者开发出更底层的技术——这类技术不会在公众当中引发高度关注，而是更加边缘化。或者是其他一些跟当今这些AI助手定位完全相反的技术，比如有助于激励孩子们参与社交、或者更加重视学习，乐于利用自己的智力来解决更多问题等等。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://nymag.com/intelligencer/article/why-i-quit-nvidia-at-the-start-of-the-ai-boom.html">https://nymag.com/intelligencer/article/why-i-quit-nvidia-at-the-start-of-the-ai-boom.html</a>"</p><p><a href="https://medium.com/authority-magazine/scientist-and-writer-jacopo-pantaleoni-on-the-future-of-artificial-intelligence-a15d7e5b447d">https://medium.com/authority-magazine/scientist-and-writer-jacopo-pantaleoni-on-the-future-of-artificial-intelligence-a15d7e5b447d</a>"</p><p><a href="https://cybernews.com/editorial/democracy-in-danger-artificial-intelligence-supercomputers/">https://cybernews.com/editorial/democracy-in-danger-artificial-intelligence-supercomputers/</a>"</p><p><a href="https://www.researchgate.net/profile/Jacopo-Pantaleoni">https://www.researchgate.net/profile/Jacopo-Pantaleoni</a>"</p><p><a href="https://x.com/jpantaleoni">https://x.com/jpantaleoni</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rIqr6fSs2TwBaOyEAUs2</id>
            <title>场景融合与 ROI 考量：金融大模型落地的两大困境有解吗？</title>
            <link>https://www.infoq.cn/article/rIqr6fSs2TwBaOyEAUs2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rIqr6fSs2TwBaOyEAUs2</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 09:06:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融行业, 大模型应用, 技术落地, 业务场景
<br>
<br>
总结: 金融行业作为大模型应用的理想领域，具有专业知识密集、数据驱动、业务流程复杂性等特点，大模型技术在金融领域的落地应用面临着技术与业务场景结合、成本投入回报、合规安全需求等挑战。在金融领域，大模型技术已经开始在核心业务中深化应用，带来实际效益提升，但金融机构在技术落地过程中仍然保持激进与保守的态度。 </div>
                        <hr>
                    
                    <p>金融行业被视为大模型应用的理想领域，从内因看，是因为金融本身具有专业知识密集、数据驱动、业务流程复杂性三个显著特点，而三大特点恰好与大模型理解能力、记忆能力、逻辑推理等优势高度吻合。从外因看，在政策驱动和市场热度的双重助力下，对于每一家金融机构来说，不采用大模型几乎是不可能的。</p><p></p><p>但是，在技术具体落地过程中仍然有很多阻力和困境。比如，技术能不能与业务场景紧密结合从而给企业带来实际的效益提升，巨大的成本投入带来的 ROI 是否划算，大模型技术底层能力是否足够满足对合规安全有着严苛要求的金融业务需求等等。</p><p></p><p>在日前的<a href="https://www.infoq.cn/video/YMGQmvSmA3ZTjDFVh13K"> InfoQ《超级连麦. 数智大脑》xFCon 直播</a>"中，我们邀请到了度小满金融技术委员会执行主席 / 数据智能应用部总经理杨青，以及文因互联董事长、创始人 / 中国中文信息学会语言与计算专委会金融知识图谱工作组鲍捷博士围绕以上话题进行了深入的探讨。</p><p></p><p></p><blockquote>在 8 月 16-17 日将于上海举办的<a href="https://fcon.infoq.cn/2024/shanghai/">&nbsp;FCon 全球金融科技大会</a>"上，杨青老师将担任大会联席主席，与组委会共同品控大会内容质量，并在大会 Keynote 分享其在大模型领域的最新探索与实践。此外，鲍捷老师也将在「前沿金融科技探索与应用」专题论坛上，深入分享如何《<a href="https://fcon.infoq.cn/2024/shanghai/presentation/5944">精益地打造金融专家智能体</a>"》 。大会更多演讲议题已上线，点击链接可查看目前的专题安排：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</blockquote><p></p><p></p><p>以下内容根据对话整理，篇幅有删减：</p><p></p><h3>落地现状：不能跟场景紧密结合的技术没有出路</h3><p></p><p></p><h5>InfoQ：在去年的 FCon 大会上，杨青老师表示，未来 AI 将在重塑金融业乃至整个社会的生产关系中发挥关键作用。是否可以分享一下，从去年到现在，金融行业在大模型领域做了哪些新的探索？有什么新的实践成果和变化？</h5><p></p><p></p><p>杨青： 金融行业之所以是大模型应用的理想领域，是因为金融本身具有三个显著特点：专业知识密集、数据驱动、业务流程复杂。这些特点与大模型的强项——理解能力、记忆能力、逻辑推理——高度吻合。因此，大模型的应用能够极大地推动金融行业的生产力和生产关系的变革。</p><p></p><p>回顾过去一年，金融行业对大模型的探索已经从广泛的尝试转向了对核心业务的深化。去年，我们见证了一场“百模大战”，众多金融企业积极参与，探索大模型的潜在价值。经过一年的实践，大模型已经在多个业务场景中落地生根。例如，一些金融企业在人工智能大会上发布了自己的产品，展示了大模型在企业内部的实际应用。面向消费者的金融产品，如蚂蚁的余额宝、同花顺等，在理财、投研、投顾等方面，通过大模型技术的应用，不仅替代了部分人工工作，还为用户提供了更优的体验和效果。</p><p></p><p>我认为，过去一年最重要的变化是大模型从探索阶段走向了实际落地，这种转变为用户带来了实实在在的效益。随着大模型技术的普及和企业内部员工对其的掌握，大模型将在更多业务流程中得到应用，进一步提升我们的生产力。</p><p></p><h5>InfoQ：事实上，传统金融机构由于业务的特殊性，对于新技术的引入一直是“既激进又保守”，请问鲍老师，从您的角度来看，现阶段国内金融领域的大模型落地进展处于什么阶段？</h5><p></p><p></p><p>鲍捷博士： 去年，我们大家都处于学习阶段，因为缺乏必要的硬件资源，比如显卡，所以基本上大家都在学习和准备。那时候有个笑话说，去年真正赚到钱的只有卖显卡的和卖课程的。但到了去年年底，随着预算的到位，今年我们开始看到各种场景下大模型的具体落地实施。</p><p></p><p>这个落地过程，正如你所说，是“既激进又保守”。之所以说激进，是因为大模型已经成为国家战略的一部分，对于每一家金融机构来说，不采用大模型几乎是不可能的。即便他们的基础条件还不够成熟，为了避免落后，大家都会尝试引入一些大模型相关的技术。但在实际落地过程中，大家又表现得非常保守，因为按照技术发展的一般规律，新技术通常会先在领先的机构中尝试，然后逐步渗透到其他机构，这个过程可能需要 3 到 5 年的时间，从头部客户到腰部客户，再到长尾客户。</p><p></p><p>但这次大模型的落地有所不同，它受到了市场的疯狂炒作，所以几乎每家机构在应用尚未完全成熟的情况下都不得不尝试引入大模型。在这种背景下，大家在引入过程中自然会采取保守的策略。那么，什么策略是一定不会错的呢？那就是先提升算力。所以我们最近看到了许多大额的算力订单，有的上千万，有的上亿。但这些大单背后的实际应用却相对较小。</p><p></p><p>我最近走访了许多客户，包括金融客户和制造业客户。我问他们，作为国内行业领先的企业，愿意为国内头部大模型厂家的基础模型软件落地投入多少资金？结果发现，即使是 100 万、50 万他们都不愿意投入。从这个角度来看，他们的态度是非常保守的。但同时，也有大规模的应用正在发生，这些应用一定是基于业务需求的。</p><p></p><p>从激进的角度来看，大家肯定会尝试引入大模型。但从保守的角度来看，大模型无论是开源的还是闭源的，如果没有很强的业务属性，是不可能拿到百万以上的订单的。比如我们最近在做反洗钱的应用，如果我只是做一个纯粹的知识库应用，那又有什么竞争力呢？核心在于，你是否熟悉反洗钱的业务规则、法规解析和建模，以及常规的反洗钱套路。只有将这些纯业务性的东西与大模型技术深刻结合，才有可能实现落地。</p><p></p><p>我们最近统计了一下，从去年大模型开始到现在，我们已经有数十个大模型落地案例了，聚焦在两个行业里，一个是金融，一个是航空。这些案例都是基于强业务驱动的，不是那种只卖显卡、卖算力的，而是真正在业务场景中发挥作用。所以从这个角度来说，大家说今年是大模型应用的元年，我相信这是对的。在金融领域是这样，在其他领域也是如此，只有紧密结合应用和领域场景，才能有广阔的发展空间。</p><p></p><h5>InfoQ：从目前来看，大模型技术应用主要集中在哪些金融业务场景？</h5><p></p><p></p><p>鲍捷博士：底层的核心是构建各类知识库，包括法规知识库、投研知识库等。这些知识库能够对金融文档，尤其是 PDF 文档，以及各种信披材料、说明书和市场文档进行解析和搜索。相对而言，更复杂的应用是各类核查，如法务核查、财务核查、合同核查和银行流水核查。这些核查在大模型出现之前就已存在，但大模型显著提升了核查的泛化能力。以往的系统相对固定，数据模式和 schema 需要事先定义。而大模型提供了即时的、实时的数据生成和业务规则更新能力，这是以往难以实现的。因此，上半年在这一领域取得了显著的发展。</p><p></p><p>另一个发展迅速的领域是写作协作场景。如今，券商和银行的每个部门都有写作需求。实际上，自 2016 年以来，我们已经开发了大量的机器自动化写作应用。许多人可能没有意识到，监管机构发出的问询函底稿都是机器生成的。四五年前，我们与中国头部券商合作，当时还没有使用大模型技术，投行底稿的 80% 以上内容都是机器生成的。最近，我们与另一家头部投行合作，复制了相同的过程，但与四年前相比，人工消耗减少了 90%。这是大模型技术为整个行业带来的生产能力的巨大提升。</p><p></p><p>当然，还有许多其他应用场景，如客户问答场景中的问答机器人，包括客服、投研助手或内部运维管理助手。在大模型出现之前，数字员工、远程银行等已经存在。有了大模型之后，这些应用变得更加丰富和多样化。</p><p></p><p>目前大模型的应用主要集中在内部提效方面。 例如，在内部 IT 部门，大模型可以发挥重要作用，帮助他们编写代码、SQL 查询，以及进行更好的商业智能（BI）分析。客服领域由于监管机构的严格要求，目前大家普遍持谨慎态度，不敢轻易使用自动化工具。</p><p></p><p>杨青： 大模型的能力在提升内部员工效率 方面展现出了巨大的潜力。目前，大模型最快的应用之一就是帮助员工更有效地获取和理解私有数据和知识库中的信息。通过大模型，员工可以更快地访问所需知识，减少理解知识的时间，从而提升工作效率。</p><p></p><p>此外，大模型还可以作为员工的专业助手，例如在编写代码或进行数据分析时提供帮助。对于不太熟悉的领域，大模型能够协助解决问题，成为数据分析的小助理。在理财投顾等复杂业务领域，大模型也在不断尝试突破，提供逻辑和业务上的支持。</p><p></p><p>从我过去一年的观察来看，初级应用已经在许多企业中落地，尤其是在提升员工绩效和生产力方面，已经取得了显著的变化。企业根据自身的实际需求，不断优化和提升大模型的使用情况。然而，要让大模型更好地帮助企业提升效益，还有很多工作要做。一方面，大模型的底层能力需要不断地提升；另一方面，在业务流程中如何更好地嵌入大模型也是一个关键因素。大模型在未来还有很大的潜力等待我们去探索和开发。</p><p></p><h5>InfoQ：鲍老师提到文因互联目前服务的客户主要分为两大类：航空业和金融业。这两个行业之间存在显著的差异，我们在提供服务过程中会面临哪些不同的挑战？或者说，我们提供的产品具有较好的通用性能够同时满足这两个行业的特定需求？</h5><p></p><p></p><p>鲍捷博士： 自 2015 年以来，我们在金融领域深耕了七八年时间，专注于这一领域，不断沉淀经验。我们发现，工具层面需求基本相似，比如阅读、编写和查询文档，只是每个领域都有其特殊性。例如，在金融领域，我们需要阅读信息材料；而在航空领域，则需要阅读维修手册、飞行员手册和标准操作程序列表。不同领域中存在相似的系统和问题。</p><p></p><p>在航空领域，例如，有飞行品质控制系统，飞机每秒可以产生 3000 个到 20000 个数据点，这些数据点需要根据业务规则进行分析。例如，飞机着陆时如果加速度超过 1.8g，就可能造成过载，损害起落架。而在金融领域，我们每天都在处理类似的合规问题，即某个指标超过阈值时的应对策略。无论是航空还是金融，本质上都涉及到数据的变化和语义理解。在金融领域，我们进行指标对齐，而在航空领域，则需要进行数据译码，我们也在这个过程中开发了中国首个国产飞机数据译码器。</p><p></p><p>这些过程无论是数据理解还是业务知识建模，本质上都是知识库管理系统的一部分。不同行业的适配核心在于：一是理解数据的语义，二是将行业知识快速转化为可计算的规则。 大模型在这里发挥了核心作用，尤其是在业务规则建模方面。以前，这需要昂贵的业务分析师、产品经理、算法工程师和应用工程师共同完成，并且必须在设计时就固定下来。在新业务规则的实时添加方面存在很大困难。而大模型可以帮助我们将底层业务系统变成一个可以接受自然语言描述的系统，业务分析师可以实时地将业务规则以自然语言的形式添加到生产系统中，实现了热插拔和系统演化，将原本静态的业务分析系统转变为动态的，这大大提升了我们的跨领域服务能力。</p><p></p><h3>挑战与出路：“死抠”成本和 ROI</h3><p></p><p></p><h5>InfoQ：从去年到现在 AI 大模型在金融行业的应用实践探索主要还是集中在非核心业务场景，要进入核心业务场景，目前面临的主要障碍是什么？要充分发挥大模型的潜力，还需要解决哪些问题呢？</h5><p></p><p></p><p>杨青：大模型的应用潜力受到技术底层能力和使用者理解的双重影响。 首先，大模型在底层核心技术上可能还不足以应对复杂的决策问题。以风控为例，风控的核心在于风险判断和决策，这通常属于传统人工智能的监督学习范畴。如果使用大模型进行风险评估，可能会遇到幻觉问题和缺乏可解释性，这直接限制了大模型在核心业务流程中的应用。</p><p></p><p>其次，对大模型的理解和应用还不够深入。目前，可能只有少数人真正掌握了大模型的使用技巧。大多数人对大模型的了解有限，特别是在企业内部，可能只有少数专业人士真正了解大模型。这限制了大模型的推广和应用范围，阻碍了其在更多场景中的尝试。</p><p></p><p>虽然大模型在处理复杂角色方面存在局限，但它可以与其他技术结合，如通过 Agent 技术提升大模型的能力，或使用 RAG 技术解决幻觉问题。此外，通过不同的提问和输入，可以挖掘大模型的潜在能力。未来，随着开源和闭源模型的不断强化，技术将不断升级，为大模型的应用提供更强大的支持。同时，随着大模型概念的普及，越来越多的人将逐步了解、接受并学会使用大模型，许多问题也将在实践中得到解决。</p><p></p><p>鲍捷博士： 新技术的出现和普及总是伴随着挑战和问题，大模型也不例外。未来几年，大模型可能不再流行，新的技术将取而代之。这是一个普遍现象，技术发展总是伴随着业务、技术需求的双轮驱动。然而，在当前阶段，业务需求是推动技术应用的主要动力，因为如果仅依靠技术驱动，最终可能只会是硬件销售（如显卡）。</p><p></p><p>目前一个核心问题是商业模式。我最近拜访了许多大模型公司，我们自身也是应用大模型的公司，但我发现，即使是行业内的专家，也难以给出一个令人满意的大模型商业模式的答案。至少在国内，基础大模型的商业模式似乎尚未找到。</p><p></p><p>应用大模型的落地核心在于成本控制。我们必须从用户的角度出发，考虑成本问题。例如，目前显卡的价格成本非常高，这并不是所有客户都能承受的，尤其是腰部和长尾客户。</p><p></p><p>另一个问题是投资回报率（ROI）。目前，许多应用无法准确计算 ROI，即客户投入资金后能获得多少回报。此外，还有总体拥有成本（TCO），这不仅包括算力和基础模型的成本，还包括运营成本、推理成本、人力成本。因为如果大模型没有进行业务上的深度定制，其数据处理的准确度通常不会超过 70%，这在 ToB 应用中是不可接受的，尤其是在金融领域。</p><p></p><p>目前市场上的卡点在于，许多人过于关注算法、算力和数据，而忽视了业务细节。基础大模型厂商可能还在尝试早期的商业模式，但其报价可能与市场现实脱节，导致他们不得不与大型硬件厂商合作，一起走单。这是当前市场上最真实的卡点。</p><p></p><p>解决大模型应用问题的理想途径是通过众多的“草根应用”来实现，让这些技术在成千上万甚至上百万的小应用中得到实践，从而积累人才和应用经验。然而，现实情况并非如此。</p><p></p><p>由于政策、投资和采购等多方面的原因，当前市场并不丰富，缺乏草根层面的创新和应用。 这导致了整个市场过于集中力量办大事，而应用的广度和深度都不够充分，也没有给予足够的时间来让这些应用自然成长和成熟。在这种背景下，一些技术和解决方案被迫快速上马，而不是经过充分的测试和优化。这种做法可能会造成问题，因为它没有允许市场通过广泛的实践来发现和解决应用中的卡点。</p><p></p><h5>InfoQ：在 FCon 大会上，鲍老师将带来 《精益地打造金融专家智能体》 的议题分享，那么这样的金融专家智能体主要应用于哪些金融业务场景呢？</h5><p></p><p></p><p>鲍捷博士： 从具体场景来看，大模型的应用不仅限于工具层面，还深入到各种业务场景中。例如，我们最近在帮助某银行构建托管系统，该系统涉及到大量的对账、指令分拣和合同比对等工作。虽然在大模型出现之前，这些工作也在进行，但现在我们可以以更低的成本完成这些任务。目前有许多 Agent 平台，通过这些平台的流程自动化，可以大幅提升工作效率。我们还与某个证券交易所合作，帮助他们开发公告的自动化处理系统。这项工作我们从 2017 年就开始了，现在我们用大模型重新开发，完全基于提示工程，不需要编写一行代码。对于核心公告类型，我们能够达到 87% 的准确度，并且通过后处理进一步提高准确度。</p><p></p><p>我们计算了成本，发现实现这种智能体构造的成本与 2017 年相比下降了 100 倍，即使与 2021 年相比也下降了 10 倍。这表明大模型确实为这一领域带来了巨大的生产力提升。这种技术开发过程的平民化也是一个显著进步。2017 年，我们的团队由海归博士领导，团队成员至少是 985 高校的硕士。而现在，一些优秀学校的实习生就能够参与这项工作，这是工具链成熟带来的巨大飞跃。</p><p></p><p>尽管如此，业务系统的难度并没有减轻，业务知识的建模依然是一个高度复杂的过程。例如，在银行内部的智能化运营中，包括监管报送等复杂系统，如 1104 报表系统，涉及数百种不同表格和上千个具体表格，这些表格之间存在复杂的关联关系，识别和自动化处理这些表格是一个高度复杂和业务化的任务。以前没有大模型时，将这些复杂的业务逻辑转换为新的表格需要编写大量的 Java 代码。有了大模型之后，也可以简化 90% 以上的工作。</p><p></p><p>此外，还有各种投研底层工具的构建，如研报检索系统、摘要系统，以及底层资产的信息披露材料的自动化生成。</p><p></p><p>总的来说，大模型的写作应用可以归纳为三种类型的写作系统：从底稿到新底稿的生成、从数据到新底稿的生成，以及从思路到新底稿的生成。这些场景在投研、券商合规、发行等银行评论报告生成中都有广泛应用。金融领域中现在已经看到的智能体可能有上百种不同的细分场景，可以使用统一的平台来处理。</p><p></p><p>打造金融智能的关键在于能否以比传统方法更低的成本实现，这最终关系到能否盈利。 商业模式能否通过提高效率来实现盈利，比如相比过去提高 10 倍的效率或相比同行提高两倍的效率。因此，我们的核心任务是降低成本，提高每个细分环节的效率，使用技术提升每个场景的效率。这不仅仅是算法和算力的问题，还包括用户界面的构造，用户界面可以极大地提高提示工程师的效率，这些最终都可以转化为经济效益。通过快速迭代和精益分解，以实现这一目标。</p><p></p><h5>InfoQ：金融行业文档的内容比较固定，文因也在这个领域做了很久，切换到大模型的时候还要不要等待所谓“智慧涌现”？</h5><p></p><p></p><p>鲍捷博士： 这是一个工程问题，而非科学问题。现在，即使是刚刚毕业的年轻人，通过使用简单的提示工程，也能够使系统达到 60% 到 70% 的效果，如果运气好，甚至可以达到 80%。因此，这件事情的门槛已经大大降低。包括以前那些特别复杂的表格处理，现在通过多模态能力，即使不理解 PDF 底层引擎的解析方式，也能够快速实现 70% 到 80% 的效果。</p><p></p><p>真正的挑战在于如何将系统的效果提高到最后的 10% 到 20%，尤其是在金融领域交付专业级文档体系时。例如，我们与交易所合作时，他们要求的是"四个九"即 99.99% 的准确率，这不是任何普通系统都能做到的。为了达到这样的高标准，可能需要在底层进行一些微调，微调之上还需要大量的预处理和后处理工作，尤其是预处理，这涉及对各种不同文档结构的细粒度理解。这里面没有所谓的智能涌现，而是需要进行大量枯燥的工程工作，包括数据的处理和清洗。</p><p></p><p>我们具体来看两个问题：首先是提示词的编写。假设针对一份招股书，招股书中可能有 94 个章节，涉及 2400 多个小类的数据点，那么你是使用一种提示词，还是使用 2400 种提示词？2400 种提示词的管理是一个挑战，可能需要对不同类型的数据进行相应的归类。如何进行归类，这就涉及到业务属性的问题。其次，提取出的数据如果不加以控制，可能有 30% 以上是编造的。在金融领域，这是绝对不允许的。那么，如何发现数据是编造的呢？这就需要进行幻觉检测和控制，需要数据溯源的技巧。所有这些工作都是在大模型之外进行的。</p><p></p><p>所以对于工程师而言，需要把大量的业务知识和专业技能注入到系统中，从而将一个只能达到 60% 到 70% 效果的系统提升到 99.99% 的准确率。这是一个需要深入理解和精心设计的工程挑战。</p><p></p><h5>InfoQ：通用大模型在面对大多数 TOB 场景问题时有哪些局限性？落地金融专家智能的挑战又是什么？</h5><p></p><p></p><p>鲍捷博士： 所谓的通用大模型底座，并不意味着它本身具有通用性，而是它具有成为通用基础的潜力。从这个意义上说，通用大模型底座在任何特定领域的初始表现都不够完美。它的优势在于，通过技术手段的叠加，可以使其适配并服务于不同的业务系统。</p><p></p><p>当前行业面临的一个关键问题是如何降低适配的成本。业界对此有不同的看法：一些人认为仅通过提示工程即可，一些人认为需要进行微调，还有一些人认为下一代模型出现后微调将不再有效。这些观点不一定谁对谁错，因为实际情况取决于具体的应用场景。例如，在进行微调时，真正的成本并不仅仅是算力，而是微调所需的数据。获取这些数据才是真正的挑战。你需要设计微调的数据集，考虑数据量的大小，是 100 万条还是 10 万条，以及这些数据是否具有代表性。微调之后，需要评估准确度是否提升，是否减少了幻觉（错误的推断），以及是否需要相应的测试集来验证微调的效果，而测试集的构建本身也是一个挑战。这些因素才是微调过程中真正的成本所在。随着模型版本的迭代，底座模型可能会不断更换，但微调的数据可能成为你最宝贵的资产。因此，关键在于如何平衡通用性、模型的演进性以及成本，这是一个非常复杂的工程过程。</p><p></p><p>多年前，我曾在 InfoQ 上提出，人工智能的核心在于工程，我反对仅从算法角度来看待人工智能系统。有些投资人可能会认为人工智能应用应该是算法的创新，而不是工程上的创新。他们可能不太看重在一线真正从事工程工作的人员，但我认为，所有这些美妙的成果都是工程带来的，而不是抽象的科学。</p><p></p><h5>InfoQ：为什么您会提出“精益迭代”这样的理念？企业又具体如何实现“精益地迭代”？</h5><p></p><p></p><p>鲍捷博士： 比如创建一个基于大模型的写作系统，人们可能会首先考虑拥有一个尽可能强大的基座模型。例如，如果可以使用 72B 参数的模型，就不会选择 14B 参数的模型。但这种做法往往并不实际。你需要考虑实际的硬件需求，如显卡的数量和性能。显卡不仅运行时噪音大，而且发热量大，这就要求客户拥有适当的机房设施。并非所有客户都有这样的条件。如果客户仅使用 4090 显卡就能解决问题，他们何必要非使用 A100 显卡呢？在设计系统时，必须为客户考虑这些实际问题，包括显存大小、是否采用 4 比特量化版本或 F16 浮点版本，以及这些选择对效果的影响。例如，效率可能降低 3 个百分点，客户是否接受？此外，还需要考虑客户使用场景的环境温度，以及他们是否有空调设备等工程问题。</p><p></p><p>当客户需要一个写作模板时，他们会询问是否需要自己配置模板。如果需要 40 个模板，客户可能会觉得太繁琐，因为他们可能没有人员能够配置这些模板。这时，你需要考虑如何以低成本自动化生成模板，以及如何填充模板所需的数据。数据的来源和准确性如何？如果客户要求 99.99% 的准确度，但系统只能达到 96%，这就需要额外的数据校验和核查，这将带来成本。在整个过程中，你都需要死抠每一个细节，以降低成本。例如，如果数据校验需要投入成本，你需要告诉客户可能出错的数据在哪里，以便他们进行核查。如果数据不满足要求，客户可能需要一个置信度系统，以大幅降低人工校验成本。</p><p></p><p>杨青： 我们需要认识到大模型是一个明确的未来趋势，而要真正跟上这一时代的步伐，首要的问题是开始实际使用大模型。如果人们不去使用，仅仅停留在了解概念或听新闻的层面，他们可能永远不会真正理解大模型的潜力和价值。</p><p></p><p>我的建议是，想要在大模型时代取得进展，首先要做的是开始使用大模型。其次，关于成本问题，我认为这并不是眼前的核心问题。关键在于大模型能否解决实际问题，以及这些问题的 ROI 是什么。目前来看，对于很多探索大模型的企业来说，现有开源模型的能力往往无法满足核心业务需求，只能做一些辅助的事情，因此会觉得它们的 ROI 会比较高。</p><p></p><p>但当你拥有合适的能力时，成本就不再是问题，没有能力时，成本永远会是问题，因为无法解决很多实际的问题以及带来实际的收益。所以，核心是如何提升大模型的能力，以便为你带来更多价值。我的看法是，随着 AI 能力的不断提升，我们不断走向通用人工智能的道路，能实际解决的问题将会越来越多，能够突破的成本边界也会越来越高。</p><p></p><h5>InfoQ：度小满金融数据智能部计算机视觉方向负责人万阳春老师也将在大会上介绍 《计算机视觉技术在金融数字化风控中应用》，杨青老师可以围绕这个议题浅浅给大家做个剧透吗？</h5><p></p><p></p><p>杨青： 我<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6030">简单介绍一下我们即将讨论的内容</a>"。我们主要关注两个方面：首先是大模型如何在现有的视觉技术中带来新的增益。在传统的人工智能应用中，OCR（光学字符识别）技术非常常见，尤其是在企业数字化转型过程中，多媒体信息的数字化尤为重要。金融行业中，票据、流水等图文和文本信息的数字化处理的流程较复杂，且每遇到新的数据类型都需要单独开发，而大模型的应用可以实现端到端的解决方案，大幅提升效率和效果。</p><p></p><p>第二方面是随着大模型技术的发展，出现了文生图、文生视频等技术，这些技术带来了深度伪造信息的问题。在金融行业，伪造信息技术可能被用来绕过生物认证流程，比如在借贷、理财等场景中，这是一个较大的安全隐患。我们将分享我们在识别深度伪造信息方面的经验和一些实际落地的做法。</p><p></p><h3>机器与机器、人与人、人与机器的未来协作模式</h3><p></p><p></p><h5>InfoQ：我们观察到现阶段越来越多的企业实际上在将传统人工智能技术与大模型相结合，以满足业务场景的需求。那么，传统 AI 技术与大模型各自的优势是什么，以及两者之间如何有效地将它们的优势结合起来，以更好地服务于金融业务场景？</h5><p></p><p></p><p>杨青： 传统人工智能（AI），之前提到的更多是有监督机器学习，其优势在于帮助企业提升决策效果。例如，大数据风控，这些年得到了快速发展，应用了许多前沿的 AI 技术来增强风控能力，这可以视为传统 AI 的一个应用实例。前面提到的 OCR 技术也是一个传统 AI 的例子，根据不同的场景需要开发相应的模型。</p><p></p><p>而生成式 AI，现在通常指的是大模型，其优势主要在于其理解、记忆和逻辑方面的能力。对企业来说，这意味着不需要定制众多模型，而是希望利用通用的人工智能的能力覆盖多种场景。两种技术的基本区别是：一种是根据具体业务目标单独开发模型，另一种是实现通用 AGI 来解决各种问题，甚至可以把他当成一个人来使用，当然，后者还在路上。</p><p></p><p>从大模型技术不断发展的过程中可以看出，一方面，大模型的出现开拓了一些新的应用场景，这些场景以前更多依赖人工操作，现在可以通过技术提升效率甚至不断的替代人工，相当于一种新质生产力。另一方面，大模型也在逐步替代一些传统场景，比如 OCR 技术，通过多模态可以有效地解决 OCR 方面的问题。</p><p></p><p>目前来看，传统 AI 和生成式 AI 正处于一个相互结合的阶段，各自发挥优势解决不同的问题。随着大模型和通用 AGI 的发展，未来可能会趋于融合，大模型能解决的问题会越来越多，最终大模型可以是像一个人一样，传统 AI 更多像一个特定的工具，大模型可以使用传统 AI 这种工具弥补自身能力的缺陷，以能够更好的完成更多任务。但这确实需要一些核心底层能力的突破，可能还需要一定的时间周期。</p><p></p><p>鲍捷博士： 技术的发展是一个继承而非取代的过程。因此，我们不能简单地说小模型和大模型哪个更优或更劣。例如，正则表达式已经存在了 50 年，并且预计在未来 50 年仍将继续存在。而大模型只有大约 5 年的历史，但我们无法保证它在未来 50 年后仍然流行。</p><p></p><p>在很多场景中，如果传统机器学习已经能够很好地解决问题，那么就没有必要使用大模型。认为所有问题都应该用大模型来解决是一个错误的想法。 我们应该从这个思路出发，更清晰地理解技术的应用的演化脉络：从小数据起步，逐步发展到大数据系统，再到大知识系统。</p><p></p><p>大模型系统本质上是大规模的知识库。 它标志着我们从数据时代进入了知识时代。今天所谓的大模型，可能在未来五到十年后看来并不算大。核心问题在于我们如何从数据时代过渡到知识时代。数据本质上是表格化的、二维的结构，而知识则包括了更多复杂结构，如树形、网状结构和复杂的语义结构，我们称之为本体。传统知识库系统的构建非常复杂，因为它依赖于逻辑系统。而大模型系统则将这些高成本的逻辑系统转变为基于数据驱动的系统，能够进行相对简单的知识建模。</p><p></p><p>当前的大模型系统存在严重的逻辑缺陷，例如在进行基本的四则运算时可能会出现问题。这是因为它们是基于预测下一个 token 的系统，依赖于概率，如果不引入特定的机制如 LangChain 或 Agent，它们无法处理递归结构生成的问题。而大多数科学语言和数学语言都是递归结构。如果未来五到十年内出现一种新的系统，能够让大模型处理递归结构，那么我们可能会发现今天的大模型系统并不如我们想象的那么强大。它处理的数据其实非常简单。技术的本质不在于规模大小，而在于处理的语义结构是否足够丰富。 从这个角度来看，不同的应用和不同的侧面需要不同的模型来配合，我们不必纠结于是否使用大模型。</p><p></p><h5>InfoQ：进一步来看，在大模型时代，除了机器与机器的协作，人与人、人与机器的协作模式也将面临巨大的变化，作为金融机构和技术从业者，为了应对这种变化现在如何做好准备？</h5><p></p><p></p><p>鲍捷博士： 我们的一些预想与实际情况并不完全相符。最初，我们设想人类从事创造性工作，而机器处理管理性任务，但现在情况似乎颠倒了。我特别关注的一个未来趋势是可穿戴设备的发展。虽然现在我们看到了机器人和其他智能设备，但人机交互界面的自然化可能是大模型技术带来的最深远的影响。</p><p></p><p>设想未来，像项链、耳环或耳机这样的日常配饰可能就是大模型的终端设备。当你走进家中，墙壁本身可能就是一个大模型终端，集成了传感器和新型显示设备，甚至成为建筑材料的一部分。未来的显示设备可能与大模型直接相连，当你进入车辆时，车联网也可能直接接入大模型。我们对智能设备的接触已经从大型机器演变到小型机、个人电脑、手机，再到现在的可穿戴设备和智能手表。未来，这些设备可能会更无缝地集成到我们的生活中，生活中的一切可能都将自动化，我们的生命将被记录和存储。</p><p></p><p>这种全面的智能化可能带来科幻般的变化，例如《全面记忆》一书中描述的全方位记忆能力，这可能会记录我们一生的每个细节。这将对我们人类的运行机制和社会组织产生深远影响，无论是积极的还是消极的。</p><p></p><p>从 ToB 的角度来看，大模型技术将极大地转变我们的生产力。过去三四十年里，尽管计算机设备变得更快，但办公室生产力并没有发生根本性的变化。大模型技术可能真正实现办公室生产的自动化，即大模型驱动的办公自动化。我相信这种自动化将带来经济上的巨变，称之为“第四产业”——不再是物质的生产与分发，而是信息的生产与分发。目前，第四产业在经济总量中占比约为 30%，但在未来几十年内，可能会增长到 80%。这是人工智能可能带给我们的未来。</p><p></p><p>杨青： 目前，虚拟人和虚拟主播已经给人们带来了深刻的感触，这让我们对未来具身智能机器人的发展充满期待。未来的发展形态是比较清晰的：从大模型技术结合具身智能，逐步走向规模化的通用人工智能（AGI），最终可能实现类似人类的机器人。</p><p></p><p>想象一下，未来有许多机器人成为你的伙伴、助手或同事，这时你需要考虑如何与它们共生和协作。我认为这是未来的一种终极状态。在这个过程中，对每个人来说，如我之前提到的，需要去实际使用和了解大模型技术。很多人可能对大模型很好奇，但他们可能并没有真正使用过它。</p><p></p><p>所以我的建议仍然是，要去多使用大模型，把它应用在你的日常生活和工作中，并逐步理解和 掌握它。这样，最终你才能与机器人更有效地沟通和协作。</p><p></p><h4>活动推荐</h4><p></p><p>8 月 16-17 日，FCon 全球金融科技大会将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ff5488cc076e04976f66fd5d9869c7.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7cZHespRRGbfQwOGyZSc</id>
            <title>打破系统孤岛，建立端到端流程，飞书项目再迎新升级</title>
            <link>https://www.infoq.cn/article/7cZHespRRGbfQwOGyZSc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7cZHespRRGbfQwOGyZSc</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 08:52:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 飞书项目, IPD产品解决方案, 流程驱动新增长, 企业价值提升
<br>
<br>
总结: 飞书项目在上海举办了新产品发布会，发布了针对制造企业的IPD产品解决方案，旨在提升企业价值，通过流程驱动实现新增长。 </div>
                        <hr>
                    
                    <p>7月17日，飞书项目在上海举办了主题为“流程驱动新增长”的新产品发布会，正式发布针对制造企业的“飞书项目IPD产品解决方案”，并分享了对流程引擎、视图、基础平台和产品体验等多项平台能力进行的升级。</p><p>&nbsp;</p><p>“飞书从最初作为协同办公软件，为企业提供新一代的工作方式；到后来通过飞书People打通了企业的人和事，成为为企业提供先进理念的组织管理工具；到今天，我们希望通过飞书项目将飞书的价值进一步提升，助力企业的业务流程，通过专业的项目管理工具为客户创造更多价值。”飞书CEO谢欣在会上提到。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b23324956b9d75f656fc6000f73dcc76.png" /></p><p></p><p>图注：飞书CEO谢欣在活动现场分享</p><p></p><p>据悉，飞书项目起源于字节跳动的研发实践，专为现代企业的复杂项目管理需求而设计。自2021年正式对外商业化以来，其“可视化、标准化、高可配置”的工作流引擎等产品亮点，吸引了包含小米、安克创新、蔚来汽车、理想汽车、小鹏汽车、ubras、公牛、联影医疗等超过1000家不同行业的头部企业。</p><p></p><h2>沉淀两年，飞书项目IPD产品解决方案正式发布</h2><p></p><p>经过两年的持续研发和大量投入，飞书产品副总裁洪涛在现场正式发布了飞书项目IPD产品解决方案。据悉，该解决方案可以通过建立系统的研发管理体系，实现以规则的确定来应对结果的不确定。同时，面对复杂项目，可以打破系统孤岛，建立端到端流程，帮助用户掌控全局进展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f354ef6b7e1148c20c4a373618949bc3.png" /></p><p></p><p>图注：飞书产品副总裁洪涛现场发布飞书项目IPD产品解决方案</p><p></p><p>洪涛在现场详细分享了包括计划管理、评审管理及流程优化在内的三大IPD解决方案功能亮点。“计划管理中全景泳道图就像给业务拍了一张流程CT图，项目进展、关键里程碑的状态在泳道图中一目了然，让关键风险直接暴露，加速问题解决”。评审集成化则能够通过组织级评审要素库的建设等方式，让IPD评审持续沉淀，成为组织固化的高价值资产。针对过去研发管理传统工具固化，难以快速进行流程迭代的问题，飞书项目在IPD解决方案中加入了度量、资源库等方式，让持续洞察问题、灵活优化流程成为可能。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/9e/9e81157e0e696e69ce5d21d475e2feb7.png" /></p><p></p><p>图注：全景泳道图产品示意</p><p></p><p>目前，飞书项目IPD产品解决方案已经在锐捷网络、新大陆、玛丽黛佳、SKG等多家企业实现了共创落地。作为最早与飞书项目进行IPD行业专版共创的客户，锐捷网络如今已基于飞书项目IPD专版构建起了完整的一体化平台。“IT技术已从IT部门的专有能力转变为组织的通用能力，未来组织中的每个人都能随时使用数字化工具来解决自身问题，这是一场效率的变革。”锐捷网络信息资源部总经理张洪丹在现场进行了基于飞书项目IPD的实践分享。</p><p></p><h2>打造更灵活、更开放的工作流引擎</h2><p></p><p></p><p>“今年一年内的时间里，飞书项目就上线了638个功能点或产品优化，平均每天1.75个，每一天，飞书项目都变的更好。”IPD行业专版解决方案外，洪涛还在现场分享了飞书项目在流程引擎、视图、基础平台和产品体验等平台功能上的全面升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f27cdd85f90cd32e779e769ff741fc1b.png" /></p><p></p><p>&nbsp;图注：飞书项目工作流程引擎产品示意</p><p></p><p>工作流引擎一直是飞书项目最被客户点赞的能力。在本次升级中，新版本提升了飞书项目工作流引擎的开放性与灵活性，让更多业务场景可以被纳入流程里，让流程看得见、易执行、可进化。另一方面，通过树形、全景等全新视图的增加，让复杂战略拆解落地的过程变得清晰可控。</p><p>&nbsp;</p><p>“项目推动离不开组织各角色在平台中的高效协同，飞书项目基础平台同样带来了一系列非常实用的功能更新。”洪涛介绍称，面对越来越强的客户出海场景需求，飞书项目上线了针对跨国协同团队的“协作时区”、“自定义多语言”能力；面对上下游跨组织场景，企业互联能力的加持让不同组织的人实现一个协同空间的完美配合；而面对信息安全问题，飞书项目通过更精细的权限管理设置，更好保障了关键项目信息的安全。</p><p>&nbsp;</p><p>除了复杂项目管理能力的迭代，在用户体验上飞书项目也作出了提升，比如最新升级的综合搜索能力让信息准确率提升了40%，帮助员工更快找到想要的项目信息；在项目新建中新增的简洁模式让建单效率提升了35%；而通过持续优化产品性能，新版的飞书项目关键页面访问速度则提升了140%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a66c713a9b774d0bf5a5e2fbb734958.png" /></p><p>&nbsp;</p><p>图注：飞书项目最新升级的综合搜索能力界面</p><p>&nbsp;</p><p>在现有的平台能力升级外，谢欣还在现场分享了飞书基于AI的项目管理创新实践。据介绍，飞书项目管理团队利用飞书智能伙伴创建平台搭建了一个AI PMO，能够帮助PMO们通过问答快速了解产品能力问题，通过整合推送资源投入进展把握产研资源投入情况，还可以在日常工作群聊中主动识别To&nbsp;Do，协助研发在飞书项目中创建工单等。“数字化是智能化的基础。未来，在大模型能力的支持下，通过飞书项目积累的流程数据，将为大模型在项目管理场景的落地提供有效的数据基础。”谢欣表示。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/P2oGNMSXP9VOLEphy6b6</id>
            <title>大模型在融合通信产品中的应用实践</title>
            <link>https://www.infoq.cn/article/P2oGNMSXP9VOLEphy6b6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/P2oGNMSXP9VOLEphy6b6</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 07:39:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: IM, RTC 技术, LLM 技术, PaaS 平台
<br>
<br>
总结: IM 和 RTC 技术作为融合通信的关键技术，在社交、客服、协同办公等场景中得到充分利用；在大模型快速发展和日渐成熟的背景下促使行业也发生了很大的变化，特别是 LLM 技术的第一个现象级应用 ChatGPT 就是以会话作为唯一交互形态；如何结合 LLM 和通信 PaaS 平台加速场景化落地成为一个关键议题。在网易云信中，我们主要做通信，它是一个通道。但这个通道的应用场景通常在社交领域，这是一个相对高风险的场景。在这个过程中，我们不仅要做好内容安全管控，还要将通道过程中产生的数据价值与 AI 结合，充分发挥其潜力。 </div>
                        <hr>
                    
                    <p>IM 和 RTC 技术作为融合通信的关键技术，在社交、客服、协同办公等诸多场景中得到充分利用；在大模型快速发展和日渐成熟的背景下促使行业也发生了很大的变化，特别是 LLM 技术的第一个现象级应用 ChatGPT 就是以会话作为唯一交互形态；如何结合 LLM 和通信 PaaS 平台加速场景化落地成为一个关键议题。</p><p></p><p>网易也在做大模型技术应用探索，也在内部的多个业务线有落地实践。在 ArchSummit 架构师峰会上，来自网易云信首席架构师，产品部负责人周梁伟就介绍了大模型在产品上的一些应用。以下是演讲整理。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59634846a2702c6ed20d2f2e80b8da35.png" /></p><p></p><p>我们的业务叫网易数智，早在 2015 年左右就开始涉足 ToB 市场。业内的一些云公司采用的是高举高打的策略，从底层架构开始搭建。而网易则结合自身业务进行内部基础设施建设。在服务自身业务的过程中，我们逐步完善这些基础设施，将其场景化，并最终包装成对外提供的 ToB 服务。</p><p></p><p>今年年初，网易数智进行了业务变革，结合大模型和 AIGC 等业界前沿技术能力，整合了更多业务，增加了智慧和 AI 的元素。那么，目前网易数智包含哪些业务呢？首先是网易易盾，主要负责内容安全。网易有大量内容型产品，如网易新闻和网易云音乐，这些产品中存在大量的内容审核需求。在网易易盾中，我们将传统的机器审核与大模型结合，推出了 AI 内容检测、智能风控、实名认证以及安全架构等相关能力，对外提供网易易盾安全产品。</p><p></p><p>第二块业务是我负责的网易云信。自 2015 年起，网易云信开始对外商业化。其起源可以追溯到网易泡泡，这是一款即时通信软件。在微信时代，我们与电信合作推出了网易翼信，达到了亿级用户规模。在 C 端市场逐步积累了大量融合通信经验后，我们发现即时通信（IM）和音视频通信在各种场景中都是非常刚需的能力，但要建设好这些能力难度很大。因此，我们将其包装为网易云信，对外提供即时通信、实时音视频、直播、点播以及与运营商结合的短信等业务。</p><p></p><p>今年，我们的业务整合后，还融合了部分轻舟微服务和轻舟中间件，这些是面向开发者的软件服务。网易云商则结合了网易严选和考拉等电商平台，推出了客服平台。数帆和 CodeWave 也在其中。随着大模型的发展趋势加深，各产品都整合了相关功能。举个例子，AI 内容检测从过去的基于机器规则匹配，转变为结合 AI 模型识别大量 UGC 内容，带来了更多挑战。在自动问答生成过程中，如何做好内容风控也是一个关键点。我们结合 AI 的合规要求，推出了一些新产品。</p><p></p><p>在网易云信中，我们主要做通信，它是一个通道。但这个通道的应用场景通常在社交领域，这是一个相对高风险的场景。在这个过程中，我们不仅要做好内容安全管控，还要将通道过程中产生的数据价值与 AI 结合，充分发挥其潜力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5f/5f270991fed6e40ff5d3a39556174054.png" /></p><p></p><p>这是网易云信的 PaaS 架构。我们从网络层开始构建了一个全球实时通信网络。这个网络支撑了目前三大主要业务：RTC 音视频通话的实时网络、IM 即时通信和直播流媒体的分发网络。这个基础网络是我们 PaaS 平台的底层服务，尽管不直接提供给客户，但它是我们 PaaS 产品的核心基建。</p><p></p><p>在 PaaS 产品层，我们提供的服务包括 IM 即时通信、信令、直播、点播、短信和音视频等。这些能力是开发者可以直接使用的。除了我们自身提供的能力之外，我们还与第三方厂商合作，特别是在安全方面，我们有网易易盾这样的能力，也有第三方厂商的生态支持。</p><p></p><p>在大模型应用场景上，由于 PaaS 平台的广泛应用，我们的客户可能会使用我们的能力来实现娱乐社交、教育、协同办公等场景。由于这些场景多样，很难通过单一模型覆盖所有需求，因此我们更多地起到桥梁和生态连接器的作用，与各类公有云和私有模型部署、训练实施等厂商合作。</p><p></p><p>此外，我们还与视频处理厂商合作，提供如美颜、变声等能力，以及与社交行业中的游戏厂商合作。这些都是我们的生态合作伙伴。今年，我们业务整合后提出了数字化方向，主要面向企业场景，包括内部办公和内外营销沟通。</p><p></p><p>许多企业提出不能使用公有云，包括大模型，因此我们自 2017 年起就提供私有化部署服务，解决客户在数据安全方面的顾虑。我们还结合实施和集成类生态合作伙伴，共同提供这些服务。</p><p></p><p>我们的 PaaS 平台支持客户使用多种融合通信场景，包括单聊、群聊、弹幕聊天室、音视频等。这些场景在娱乐社交、教育、医疗和金融等领域有广泛应用。例如，在医疗场景中，本地医院医生可以与远程专家在线阅片，产生大量音视频互动和 IM 内容互动。</p><p></p><p>在金融场景中，我们支持虚拟营业厅和柜员，基于这些底层能力构建上层业务场景。协同办公是另一个重要场景，虽然市面上有钉钉和飞书等 all-in-one 产品，但很多客户出于合规或个性化需求，不能使用 SaaS 产品。因此，他们可以基于我们的融合通信能力构建自己的 OA 办公平台、企业培训平台、视频会议平台和客服系统。</p><p></p><p>通过 8 年的持续产品迭代，我们已经积累了大约百万开发者，发送了超过 2 万亿条消息。这显示了我们在数据体量上的巨大优势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d9/d960ac133d2314d36063eca577b714d8.png" /></p><p></p><p>回到大模型结合的问题上，我们的通信系统本身是一个连接器和消息分发网络。在结合大模型的过程中，我们发现客户有几个关键痛点。</p><p></p><p>首先，如何将 AI 能力融入用户交互层面。例如，在群聊或客服互动中，如何将 AI 能力整合到点对点或群组沟通中。</p><p></p><p>其次，在群聊或客服互动过程中，已经产生了大量上下文数据。这些数据在与客户沟通时，可以被提取和利用，进行基于上下文的持续问答。这些数据对于模型训练非常重要。</p><p></p><p>第三，也是最关键的，许多客户内部已有 AI 团队在训练大模型，或使用供应商提供的大模型实时和训练服务。如何与这些 AI 能力结合，是一个重要问题。因此，我们更多地作为一方、二方和三方之间的大模型生态连接器，促进这些资源的整合。</p><p></p><p>最后，许多客户更倾向于本地化部署。这主要出于两个原因：安全性和业务迭代的持续性。本地化部署的大模型成为企业的大脑，包含所有数据信息，能够支持更多业务和不同场景的数据整合和利用。这是客户最关心的问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e3a03722d7221df72d3261910baf370.png" /></p><p></p><p>从业务场景的角度出发，我们的方案主要涉及以下几个层面。首先，从最基础的层面看，AI 技术已经广泛应用于各种业务场景。例如，通过 AI 进行关键词搜索、提取相关信息、语言翻译，甚至是自动语音识别（ASR），这些都与传统方法有很大不同。此外，AI 还可以应用于论文索引、代码生成和优化，为程序员提升效率。</p><p></p><p>今天我们重点讨论两个与 IM 和大模型结合的场景。第一个场景是客服，包括售前咨询和营销类客户服务。第二个场景是娱乐社交，包括虚拟人物和 AI 机器人社交语聊。在这些场景之上，AI 技术可以广泛应用于不同行业。例如，在电商领域，客户通常在购买商品后需要与客服沟通处理售后问题。虽然这个场景的交互时间较短，但在其他场景中，客服的交互时间可能更长。</p><p></p><p>以我们自身为例，网易云信面向开发者的产品在售前咨询过程中往往持续半个月到一个月。开发者在集成 SDK 或 API 时，会产生大量的问答，并且这些问答可能有前后关联。长流程客服场景中，对历史信息的提炼和关联尤为重要。</p><p></p><p>类似地，在线教育和办公协同中也存在客服场景。比如在办公过程中，HR 和 IT 服务涉及的问答频率很高，例如社保策略或薪资问题。在这些情况下，通过 AI 客服来提供持续服务显得尤为重要。通过将 AI 客服与 OA 办公平台结合，可以有效解决这些问题。此外，AI 技术在游戏和电商等领域也有广泛应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5e/5ecc50e7f1a56a55645f031f74809a5f.png" /></p><p></p><p>首先，谈到客服场景，最关键的问题是什么？目前市场上有大量的第三方客服服务平台，例如我们网易提供的客服平台“网易七鱼”。早期的客服主要依赖人工，人工客服需要持续回答问题并进行管理。然而，随着业务发展，客服人员的规模和培训成本不断增加，导致运营成本大幅上升。</p><p></p><p>为降低成本，许多企业转向使用机器人客服来替代人工客服。目前，客服流程通常包括机器人客服、人工客服和工单处理三个阶段。这种模式虽然降低了运营成本，但也带来了新的问题，例如知识库的维护成本。客服需要回答的问题和产品知识不断迭代，传统的解决方案是构建一个知识库，需专门的机器人训练师从非结构化数据中提取产品相关知识点和标准问答，进行语义训练和回答。</p><p></p><p>然而，这种方法存在不足，尤其是面对业务的不断变化和知识的持续迭代。大模型通过自然语言理解、关键信息提取及持续训练能力，可以更好地解决这一问题。其次是用户意图的把握。用户具有不同的背景和表达习惯，同一问题的表述可能千差万别。过去主要依赖自然语言理解（NLP）技术，但在用户意图的准确把握上，大模型具备更强的处理长文本和历史信息的能力，带来了显著变化。</p><p></p><p>第三个问题是大量历史信息的查阅。在电商场景中，客服问题通常与当前订单相关，只需通过业务系统集成调取订单信息即可解决。然而，在技术平台类客服或其他复杂场景中，客服需要基于大量历史数据。这些历史数据通常存储在 IM 平台或 RTC 平台中，通过大模型可以更有效地进行处理和利用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/798d50cf4eb880d0822dfd8abd9d286a.png" /></p><p></p><p>通过结合 AI 技术，客服效率得到了显著提升，主要体现在以下几个方面：</p><p></p><p>首先，AI 可以更精准地理解用户意图，从而提高机器人客服的问答质量。尽管仍需要人工客服，但 AI 的应用可以减少人工客服的工作量。在人工客服的过程中，AI 可以提供两方面的帮助：一是为客服人员提供用户的上下文信息和建议答案，客服人员只需进行人工校准即可；二是收集和利用人工客服提供的精准散点化回答，这些回答具有重要价值，可以被引入知识库进行二次训练，从而优化后续的自动化回答。</p><p></p><p>通过 AI 的群体客服能力，以上问题得到了有效解决，有助于构建更高效的自动化知识库。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/69/69021879d5ae295b6a5cdc9bc82ef3be.png" /></p><p></p><p>构建知识库的过程中，有两个重要的数据源。传统模式，企业知识库通常来自业务系统或企业内部文档，如 PDF、Word 文件，或 HR 发布的规章制度网页等。这些分散的信息需要被结构化处理，过去的方法是手动理解和提取知识点，再整合入知识库。结合大模型后，只需将文档输入系统，由大模型自动理解和消化。</p><p></p><p>第二个重要的数据源是用户沟通过程中的历史记录，这在常会话或多人客服场景中特别重要。例如，新能源汽车的客服场景显著不同于传统汽车。通过直销模式，新能源汽车企业提供更优质的服务，从客户进店到下订单、提车、使用过程中出现的事故、保险理赔和售后维护，全过程信息都通过客服群汇总。在这种场景中，如何将用户过去一月甚至一年内的反馈结合起来，解决当前的问题，成为一个重要课题。通过将 IM 中的大量历史数据与大模型结合，再加上标准的企业知识库，可以大幅提升客服的回答质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/57/57b19d5f5264d113224e122e73a782ee.png" /></p><p></p><p>这是一个精准识别提问的示例，就是在数字人客服过程中，过去可能主要就是通过关键词提取，那现在可能基于关键词，在企业知识库的矢量向量库里去做一些关键内容的提取，之后再根据场景化定义的那些 promote 列提示词，然后提供更好的回答。它具备根据上下文不断地追溯的能力做生成式回答。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/3128fa8de8f79bafe705c61830612894.png" /></p><p></p><p>这是人工服务持续提效的事宜，客服在回答用户问题的过程中，可能产生了很多散点的问题，或者会给出一些建议性答案，通过 IM 里面的上层 UI 工具和底层数据能力之间打通，可以快速地给到一些提示。同时客服人员可以对当前会话的一些相关信息打标签，之后这个数据可以回溯再去做二次训练。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4d/4d5e1ede2447d65291f04f99e1397768.png" /></p><p></p><p>另一个场景叫群客服，在群客服场景里要解答问题，如何快速提取上下文？它可以通过 @ AI 机器人，让它帮我把上下文提取出来，或者给我一个建议。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b3/b320d44e746588215f0d7654e03a95ff.png" /></p><p></p><p>第二个场景主要涉及营销。过去的营销方式常常在用户访问网站时弹出一个客服页面，询问用户问题并提供快速链接。然而，这种方式的精准度相对较低，用户往往需要多次点击才能找到准确的信息。通过结合大模型，可以显著缩短推荐链路。</p><p></p><p>大模型的优势在于能够利用用户在产品中的历史数据，分析其特征和偏好。例如，用户的年龄、地区、职业标签（如职场女性）等信息。将这些分析结果与企业知识库中的产品推荐信息相结合，可以为客户提供更精准的推荐。同时，在客服过程中，也能提供快速触达和促成交易的链接，提高营销效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/4b/4bd455c4045cc312016aa962e956b6d4.png" /></p><p></p><p>第二个场景主要偏向娱乐社交和游戏。当前，新产品推出的最大挑战之一是如何快速积累用户，或需要高昂成本来制作大量的用户生成内容（PGC）。在这种情况下，如何快速启动变得尤为重要。例如，小说、漫画、动漫等创作需要大量专业知识和持续的内容积累。借助大模型，可以通过不同的标签和提示生成拟人化、个性化的人物角色，应用于社交场景。近年来，市场上已经推出了许多类似的 C 端产品，这些产品可能会逐渐替代真人社交，成为虚拟人社交的趋势。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed9916afbad17e414408a8660c37652e.png" /></p><p></p><p>在社交场景中，即使是真人用户的一对一交流，面对大量问题时也可能应接不暇。在这种情况下，可以利用用户的固定知识信息和表达习惯，通过大模型生成一个类似 AI 的角色，即数字人分身，来代为回答问题。这种方式不仅可以在单聊和群聊中增强互动，还能提高交流频率和交互效率。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7eea020b1ec97c5b1c44b6cf7dc928d8.png" /></p><p></p><p>基于上述场景，在融合通信或 IM 通信领域中与大模型结合的主要难点包括：</p><p></p><p>首先，训练过程非常复杂。目前市场上有许多大模型，包括公有云和私有化的大模型。要进行有效的二次训练或结合自身业务数据进行训练，面临两大顾虑：一是担心业务数据投入训练后可能带来的数据泄露风险；二是对大模型和垂直领域的大模型了解不足，难以选择最合适的模型。解决训练难题成为一个重要课题。</p><p></p><p>其次是选型问题。客服场景内有许多垂直领域需求。例如，我们是一个技术开发平台，客服场景中需要的大模型应对代码理解能力更强，因为客户的问题多与代码相关，答案也是代码。因此，需要选择一个对代码理解更好的模型。而在电商、财务或法规等领域，则需要选择对相应领域理解更好的模型。因此，未来将存在大量不同领域的垂直模型。</p><p></p><p>第三是数据整合问题。不论是构建上下文关联数据库、生成提示词，还是进行二次训练，都需要结合大量业务数据。在 IM 中，通信过程中已产生大量业务数据，这些数据对企业来说非常有价值，可以作为模型训练的重要数据来源。作为一个 PaaS 平台，我们的开放性很好，可以通过 API、SDK、Webhook 等形式实现数据互通与开放，从而持续增强模型的训练效果。</p><p></p><p>最后是业务融合问题。我们的产品以 UI Kit、SDK 或 API 形式对外提供，可以与各种业务场景进行融合。这种灵活性和开放性使得数据和业务功能能够无缝集成，提高了整体系统的智能化和自动化程度。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/54/54dd542cb731539f17fa5d65bc6609fd.png" /></p><p></p><p>这是大概的架构图。首先，在底层支持的资源和服务包括本地部署的算力平台和向量数据库存储服务，以及即时通信和内容合规的云服务。我们支持公有云和私有化部署模式。</p><p></p><p>其上，我们为客户提供了一些经过调研的大模型建议，可以是云上运行的模型，也可以是私有化部署或经过二次训练的模型，实现与我们平台的互通。</p><p></p><p>第三部分涉及模型训练过程中可能需要大量第三方标准数据，如图表、日期和气象数据等，这些数据通过插件形式引入到互动过程中。我们在 UI Kit 等层面实现数据的互通。向量数据库作为关键基础资源，实现数据上下文存储。</p><p></p><p>最顶层提供给用户的是 SDK 和 API，同时还提供了控制台等界面化操作能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d6/d628f1a2fc380c66640422e21d9c6ab3.png" /></p><p></p><p>基于这种方案，我们作为一个 PaaS 平台，服务了大量的 IM 场景客户。</p><p></p><p>在这些场景中，我们积累了开发者的使用习惯和业务理解。基于这些理解，我们设计了更优的交互和 API 与开发者系统的集成设计。另一方面，这些场景产生了大量数据。虽然这些数据对于平台本身的价值不高，因为它们主要属于客户的垂直业务场景，但对客户来说，结合他们的大模型进行二次训练是非常有价值的。</p><p></p><p>因此，我们的数据可以帮助客户通过与业务系统集成，获得更好的回答。这些数据仅在用户与模型进行互动时才能发挥作用，这些过程基于通信中的数据安全和脱敏关系。另外，由于我们服务的客户场景多样，客户可以积累跨不同垂直领域的经验。例如，我们为电商公司提供的解决方案可能在低成本下被其他电商公司复用，从而企业可以以更低的成本实现类似场景的落地。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ec/ec0362629dd957aafb7d2ec07efb2187.png" /></p><p></p><p>最后一个部分涉及基础能力的互通，这是传统架构中的一个对话过程。一旦与云信打通，我们便能够通过 API 开放平台，让客户提取数据并调用 API 接口快速响应。这种打通可以显著加速整个开发过程。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0824422a255352ae25a36e7c2874e41.png" /></p><p></p><p>这解决了当前场景下两个问题：强化 AI 的场景响应效果和语料积累。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/05/05a175a7d6bf134c3381ad52247db9d2.png" /></p><p></p><p>重点在于通信过程中，用户在我们平台产生的数据如何结合向量数据库，进而适应我们业务垂直的大模型进行训练，实现更佳的响应效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/739ccd478a72e24a2491a05f34deea9e.png" /></p><p></p><p>简而言之，作为一个 PaaS 平台，我们提供 UI Kit、SDK 和 API 能力。UI Kit 旨在帮助客户和开发者在降低开发成本的同时，提供各种 AI 交互入口，例如工具栏和虚拟账号，以及在群体中的机器人交互。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/01/01961743bfcb9ad3a28f3e9dfa2b1fef.png" /></p><p></p><p>除了 UI Kit 的开放能力，我们还提供控制台和简便的集成互动能力，如数字人模板库。该模板库主要基于场景定义和基础数据训练，虽然通用但不包含企业私有数据，可以进行二次定制。</p><p></p><p>未来，我们将开放生态合作伙伴在模板库内的生态系统，支持模型能力服务实施和行业场景训练模板上传，为更多开发者提供使用和定制的机会。</p><p></p><p>客户选择模型后，可以快速连接并上传私有数据集进行二次训练和调整。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59accde94a4005d6652047bb5465d7c2.png" /></p><p></p><p>最后，我们的交付策略将满足客户对数据和模型安全性的高要求。我们支持全面私有化部署或混合云部署模式，以适应不同的需求和场景。</p><p></p><p>总结来说，作为 PaaS 平台，我们通过深入理解客户需求和通信领域的核心优势，结合大模型能力，与生态合作伙伴合作，落地解决方案。</p><p></p><p>【会议推荐】</p><p></p><p>AICon 全球人工智能开发与应用大会，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/29/2992999a64ad4ad07df3c7cc4977bf6f.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/u64NGww7q7zTeeJEDXaZ</id>
            <title>快手AIGC原创神话短剧上线，首次曝光可灵技术如何降低制作成本</title>
            <link>https://www.infoq.cn/article/u64NGww7q7zTeeJEDXaZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/u64NGww7q7zTeeJEDXaZ</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jul 2024 01:48:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 快手, AIGC, 可灵, AI影视
<br>
<br>
总结: 快手首部AIGC"原创奇幻微短剧《山海奇镜之劈波斩浪》上线，以AI制作的奇幻微短剧为观众构建了一个光怪陆离的上古神话世界，制作周期短、成本低，AI技术提高了奇幻短剧创作效率，快手推出了“星芒短剧×可灵大模型”创作者孵化计划，未来将不断升级技术以满足用户和创作者需求。 </div>
                        <hr>
                    
                    <p>近日，快手首部<a href="https://www.infoq.cn/minibook/HfXFv4RaAEyPUFk5HAfJ">AIGC</a>"原创奇幻微短剧《山海奇镜之劈波斩浪》上线。在线下看片会现场，该剧主创团队首次揭秘了快手自研视频生成大模型可灵对该剧进行的深度技术支持。</p><p>&nbsp;</p><p>作为一部画面完全由AI制作的奇幻微短剧，《山海奇镜之劈波斩浪》以中国古代经典《山海经》为灵感来源，为观众构建了一个光怪陆离的上古神话世界。该剧讲述了在上古神话世界，一名勇敢的少年向传说中的神兽发起挑战，拯救世界的热血故事。</p><p>&nbsp;</p><p>该剧导演陈坤（@闲人一坤）在看片会上分享了整个创作过程。陈坤表示，过去同样体量和质量的短剧，无论是以实拍还是动画制作的形式，制作周期最少也需要3-6个月，而《山海奇镜之劈波斩浪》仅制作而言只花费了2个月的时间，这在过去是“不可思议”的。</p><p>&nbsp;</p><p>他认为AI影视在发展初期，一定要选择传统影视的痛点来进行创作。在传统影视当中，奇幻和科幻两大赛道受制于CG特效的高成本和长周期，虽然有着坚实的观众需求基础，但发展受限。而AI的加入，让奇幻短剧的整个创作过程变得十分高效。</p><p>&nbsp;</p><p>“过去我们做传统影视，外拍需要100多人的制作团队，包括演员、摄影、灯光、音响、剧务、服化道等等。现在制作流程因AI的存在而完全升级了，全片没有一个画面是真人拍摄，服化道也可以通过AI来进行生成，极大地提升了创作效率，而且大大降低了制作成本。”</p><p>&nbsp;</p><p>关于可灵AI为该剧提供的技术支持，快手视觉生成与互动中心负责人万鹏飞介绍到，“可灵能够呈现出复杂的物理规律效果，例如筷子夹面、以及塞进嘴巴、咀嚼等过程，甚至包括面部动作都能够进行呈现。同时，可灵的视频大模型能够比较好地呈现出运动画面，可以很好地组合各种天马行空的想法。”而陈坤也在现场表示了对可灵的功能齐全度、运动幅度、运动合理性以及物理规律识别程度的认可。</p><p>&nbsp;</p><p>万鹏飞表示，可灵自发布并开放给创作者使用后，收到了很多创作者的反馈和建议，有助于后续版本的升级。</p><p>&nbsp;</p><p>快手文娱业务负责人陈弋弋则代表该剧出品方快手短剧，在活动现场分享了快手对于“微短剧+AI”的洞察和思考。目前快手平台上的短剧用户非常多，每天约有超2.7亿用户在观看短剧，播放量过亿的短剧有300多部，有超10万创作者在进行短剧相关的内容创作。从数据上看，星芒短剧内容处于供不应求的情况，而通过AIGC技术可以大幅度提高短剧创作的效率。</p><p>&nbsp;</p><p>陈弋弋还介绍，快手推出了“星芒短剧×可灵大模型”创作者孵化计划，将为创作者提供前期资金支持和激励，帮助短剧创作者创作更加优质的内容，未来还将对孵化计划迭代，带来更大的空间和更大的激励。</p><p>&nbsp;</p><p>针对现场关于“快手是否会在AI与娱乐内容上有更多类型化尝试”的提问，陈弋弋更是坦言“一切皆有可能”。在她眼中，快手是一个跟随用户和创作者需求发展的平台，未来随着用户消费内容、创作者反馈的变化，快手的技术会不断升级。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4GqybUYo1vXoM1e9pE8v</id>
            <title>特朗普80后竞选搭档支持AI开源言论，赢得科技圈好评！网友：如果想让开发人员支持你，就坚持</title>
            <link>https://www.infoq.cn/article/4GqybUYo1vXoM1e9pE8v</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4GqybUYo1vXoM1e9pE8v</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 07:34:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 特朗普, J.D. Vance, 开源人工智能, 美国总统大选
<br>
<br>
总结: 美国前总统特朗普宣布J.D. Vance将作为其副总统候选人参加2024年的总统大选，Vance是保守派MAGA理念的支持者，尤其在外交政策、贸易和移民方面是特朗普政策的捍卫者。Vance支持开源人工智能，认为解决方案是开源。他在参议院关于人工智能加速器的听证会上表示担忧，认为监管可能有利于政府而非消费者。Vance与硅谷关系密切，拥有多年风投经验，商业领域职业生涯比政治生涯时间长。 </div>
                        <hr>
                    
                    <p></p><p>北京时间7月16日凌晨，在险些被暗杀的几天后，美国前总统唐纳德·特朗普宣布，俄亥俄州联邦参议员J.D. Vance将作为其副总统候选人参加2024年的总统大选。</p><p>&nbsp;</p><p>Vance 今年39岁，如果在今年11月和特朗普一起赢得大选，他将成为美国历史上第三年轻的副总统。据介绍，万斯是“保守派MAGA理念”坚定支持者（MAGA，即Make&nbsp;America Great Again），尤其是在外交政策、贸易和移民方面，都是特朗普长期政治政策“让美国再次伟大”最有力的捍卫者之一。</p><p>&nbsp;</p><p>Vance 是美国典型的草根阶层起家的政治家，虽然他的一些政治观点颇受争议，但Vance 发表的支持开源人工智能的言论却受到了很多科技界人士的称赞。</p><p>&nbsp;</p><p></p><h2>Vance：解决方案是开源</h2><p></p><p>&nbsp;</p><p>在特朗普宣布Vance 参选的消息后，Vance 一条3月份的帖子再次被网友们翻了出来。这个帖子里，Vance 明确了自己支持人工智能开源的态度：</p><p>&nbsp;</p><p></p><blockquote>毫无疑问，人工智能存在风险。最大的风险之一就是，一群疯狂的党派人士利用人工智能将左翼偏见渗透到信息经济的每一部分。Gemini 无法产生准确的历史，ChatGPT 宣扬种族灭绝的概念。解决方案是开源。</blockquote><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2de784a226b2b23e96e75567badb031.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>值得注意的是，Vance 上周刚刚在美国参议院商务、科学和交通委员会关于“保护美国人隐私和人工智能加速器的必要性”的听证会上作证。据<a href="https://www.techpolicy.press/transcript-senate-hearing-on-protecting-americans-privacy-and-the-ai-accelerant/">TechPolicy.Press</a>"消息，Vance 在听证会上表示：</p><p>&nbsp;</p><p></p><blockquote>“我认为，很多时候，首席执行官，尤其是那些已经在人工智能领域占据优势地位的大型科技公司的首席执行官，会谈论这项新技术的可怕安全隐患，以及国会需要如何尽快站出来进行监管。我不禁担心，如果我们在现任政府的压力下采取行动，那么这将有利于现任政府，而不是美国消费者。&nbsp;”</blockquote><p></p><p>&nbsp;</p><p>开源人工智能模型提供商 Abacus AI 首席执行官 Bindu Reddy 激动地发文称，她认为“Vance 做到了！”并补充说，“解决方案当然是开源！”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92ead4cf667c5106bbab66624acc2181.png" /></p><p></p><p>&nbsp;</p><p>自称是<a href="https://www.lesswrong.com/posts/2ss6gomAJdqjwdSCy/what-s-the-deal-with-effective-accelerationism-e-acc">有效加速主义者</a>"的 Tetsuo 也发布了一句引自 Vance “名言”的帖子：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/35/35ce0fabf75f85b97df3d962a37dc10a.png" /></p><p></p><p>&nbsp;</p><p>位于华盛顿特区的亲开源非营利组织&nbsp;Future Manifesto执行董事 Brian Chau在 X 上发布了 Vance 证词中的一段引言：</p><p>&nbsp;</p><p></p><p><img src="https://uploader.shimo.im/f/fSfEkyFeTlFag9Tt.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjExMTUyNTYsImZpbGVHVUlEIjoiOTAzMEpXbldZZUNEclZrdyIsImlhdCI6MTcyMTExNDk1NiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.RaEEjSvLOuGRsLKedpatoF9mk7hOzoEdfD-mRFsLbmE" /></p><p></p><p>&nbsp;</p><p>有资深开发人士评论道，就凭Vance “The solution is open source” 这句话，硅谷很多人就会选特朗普。而有些人，一边剽窃开源，一边诋毁开源。</p><p>&nbsp;</p><p>现在还有网友纷纷跑去Vance帖子下面留言：“如果你想让开发人员支持你，请坚持支持开源。”</p><p>&nbsp;</p><p>当然，也有网友提出疑问：“我如何相信你当选后会继续支持开源？”</p><p>&nbsp;</p><p>实际上，Vance 本人与硅谷关系密切。他拥有多年的风投经验，直到2022年当选参议员后才告别该行业。可以说，近40岁的Vance，其商业领域的职业生涯比其政治生涯时间还长。</p><p>&nbsp;</p><p>2013年从耶鲁法学院毕业之后，Vance搬往旧金山并在那里担任Mithril Capital基金负责人。该基金由前 PayPal 首席执行官、长期为共和党提供大量资金的Peter Thiel 与 Ajay Royan共同创立。2016年，Thiel本人公开积极参与政治，并对首次参选美国总统的特朗普表示支持，同时资助Vance冲击参议院。但时间来到2024年，Thiel已明确表示不会在本轮大选中向任何共和党人捐款。</p><p>&nbsp;</p><p>2017年，Vance离开 Mithril，并加入了由前美国在线CEO Steve Case于华盛顿特区成立的Revolution公司担任董事总经理。供职期间，Vance 帮助 Case 发布了Rise of the Rest基金战略，此项战略专注于投资美国主要科技中心以外的初创企业。</p><p>&nbsp;</p><p>根据彭博社消息，很多创业界的人都为Vance 欢欣鼓舞，可见相对于大型科技公司，Vance 更加支持小型科创企业。</p><p>&nbsp;</p><p>同样是在Revolution任职时间，这位新晋共和党副总统候选人领导了与密歇根州Aatmunn公司的交易——后者是一家开发软件及可穿戴设备，以保障建筑工人在职安全的初创企业。他还支持并担任肯塔基州室内农业初创公司AppHarvest的董事会成员，这家公司于2021年通过SPAC上市，但2023年申请了破产保护。</p><p>&nbsp;</p><p>然而，Vance并没有在Revolution的第二只Rise of the Rest种子基金中发挥积极作用，该基金于2019年以1.5亿美元的融资成绩宣布关闭。在此期间，Vance主要专注于为自己的公司——位于辛辛那提的Narya Capital筹集资金。</p><p>&nbsp;</p><p>到2020年初，这家新公司从Thiel、Marc Andreessen和Eric Schmidt等有限合伙人手中筹得9300万美元，但未能达到1.25亿美元融资目标。据Axios消息，后来以共和党总统候选人身份加入、被认为是特朗普副总统有力竞争者之一的生物科技企业家Vivek Ramaswamy，也是Narya基金的有限合伙人之一。</p><p>&nbsp;</p><p>与Rise of the Rest一样，Narya基金主要专注于支持美国服务不足地区的初创企业。虽然尚不清楚Steve Case是否出手支持了Narya的第一只基金，但这位 AOL创始人已经公开与Vance的政治观点及职业生涯划清界线。</p><p>&nbsp;</p><p>Steve Case在2022年9月接受采访时曾表示，“自从他去年宣布竞选参议员以来，我就再没有和他交谈过，也没有支持他的竞选活动。”</p><p>&nbsp;</p><p>在赢得参议院竞选之后，Vance退出了Narya基金的运营工作。该公司目前由Narya联合创始人Colin Greenspon负责管理，他曾是Rise of the Rest种子基金的合伙人，同时曾任Mithril前董事总经理。根据2022年底提交给美国证券交易委员会的监管文件，Narya公司正在为第二只基金募集资金，目标同样是1.25亿美元。</p><p>&nbsp;</p><p></p><h2>国内外反对开源的CEO，各有说法</h2><p></p><p>&nbsp;</p><p>人工智能领域的开源问题一直是一个热门话题，Vance的这条推特再次引发了热议。国内外不少科技公司领导者都有不支持开源的，但大家理由各不相同。</p><p>&nbsp;</p><p>在最近的2024 世界人工智能大会期间，百度创始人、董事长兼首席执行官李彦宏就旗帜鲜明地反对开源：“开源是一种智商税。”</p><p>&nbsp;</p><p>“当你理性地去想，大模型能够带来什么价值，以什么样的成本带来价值的时候，就会发现，你永远应该选择闭源模型。今天无论是 ChatGPT、还是文心一言等闭源模型，一定比开源模型更强大、推理成本更低。”李彦宏说道。</p><p>&nbsp;</p><p>李彦宏认为，闭源模型不是一个模型，它是一系列的模型，可以根据使用场景平衡使用者的需求，比如要多好的效果、要多快的推理速度、要多低的成本。模型有非常多的变种，可以根据用户的需求，让他来做选择。</p><p>&nbsp;</p><p>他认为，开源模型打不过闭源模型。闭源模型还有一个开源模型不具备的优势，就是规模更小的模型都是从最大、最 powerful 的模型裁剪出来的，裁剪出来的这些更小规模的模型，仍然比那些同样规模的开源模型要效果更好。“To B 的机会仍然在闭源不在开源。”</p><p>&nbsp;</p><p>据媒体报道，12日，百度talk推送全体员工，李彦宏在邮件中再次公开diss了开源大模型。</p><p>&nbsp;</p><p>他表示，“外界有相当一批公司是想靠在开源上套个壳、做点客户化的东西去赚钱，所以你只要说开源不好，都会有一大堆人跳出来跟你理论说开源为什么好。但我是站在整个产业发展的角度真正讲道理、去看趋势——过去几百年历次产业革命所印证的，就是闭源的效率更高。这是毫无疑问的，没有人可以argue这一点，他们argue的全都是一些边边角角的别的东西，安全不安全可控不可控，所以我觉得有必要不断地澄清这个观点。”</p><p>&nbsp;</p><p>前谷歌CEO Eric Schmidt 也反对开源，他的主要考量是“安全”。</p><p>&nbsp;</p><p>Schmidt 多次在公共场合表了对开源安全的担忧，他以虚假信息和deepfakes深度伪造为例，表示这几乎就是无解的问题。原因是代码能够以几乎免费的方式快速生成虚假信息，而且无论好人还是坏人都可以轻松访问。做这种事不需要任何成本，而且生成的图像质量已经非常非常高了。虽然也有一些方法在尝试进行监管，但魔鬼一旦被召唤了出来，就不再受到创造者的控制。</p><p>&nbsp;</p><p>他在与外媒Noema Magazine对话时提到，只有那些向AI领域投入数千亿美元的西方领先企业，才会在能力攀升的过程中受到严格监管。</p><p>&nbsp;</p><p>“在开源和开放权重模型当中，源代码和模型权重（即用于确定不同关联强度的数值）都会向公众公开。这些信息会被立即传遍世界，包括传到中国、传到俄罗斯等。”Schmidt 说道。</p><p>&nbsp;</p><p>他还补充道，“我最近刚刚造访过中国，那里的几乎所有AI工作都是以西方世界的开源模型为起点，而后被不断放大。”</p><p>&nbsp;</p><p>对于中国AI技术的发展，Schmidt 认为中国落后美国大约两年。“相对芯片，我更担心开源方案的扩散。我相信中方也抱有同样的担忧，比如开源成果可能会被滥用以对付中美两国官方。”Schmidt 表示。</p><p>&nbsp;</p><p>所以，Schmidt 认为，首先需要确保开源模型能通过所谓“基于人类反馈的强化学习（RLHF）”来保障自身安全。这种强化学习经过微调，因此其中的护栏不会被恶意人士“撤销”。而且只要开源模型真正实现了安全，再要把它变得危险也将相当困难。</p><p>&nbsp;</p><p>Schmidt 还表示，50年前的物理学家必须拥有建造而且价格昂贵的回旋加速器，但在软件领域从来就没有出现过这样的高门槛情况，因为软件产品属于资本廉价型、而非资本密集型。但AI模型的出现改变了一切：其极高的训练强度对硬件提出了愈发复杂和精密的要求，而这也将对应着巨大的经济变化。</p><p>&nbsp;</p><p>对此各类机构正在以自己的方式解决这个问题。微软和谷歌等财力雄厚的公司正计划投入数十亿美元，毕竟他们手中有的是现金。Schmidt认为，大企业的资金会源源不断注入到研究中来，这是件好事，也是长久以来的创新来源。但其他机构，特别是大学，永远也负担不起这样的成本。他们没有能力投资硬件，自然也就做不出高度依赖硬件的创新成果。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://x.com/JDVance1/status/1764471399823847525">https://x.com/JDVance1/status/1764471399823847525</a>"</p><p><a href="https://techcrunch.com/2024/07/15/trumps-vp-candidate-j-d-vance-has-long-ties-to-silicon-valley-and-was-a-vc-himself/">https://techcrunch.com/2024/07/15/trumps-vp-candidate-j-d-vance-has-long-ties-to-silicon-valley-and-was-a-vc-himself/</a>"</p><p><a href="https://venturebeat.com/ai/trump-v-p-pick-j-d-vance-praised-for-comments-seemingly-in-support-of-open-source-ai/">https://venturebeat.com/ai/trump-v-p-pick-j-d-vance-praised-for-comments-seemingly-in-support-of-open-source-ai/</a>"</p><p><a href="https://www.noemamag.com/mapping-ais-rapid-advance/">https://www.noemamag.com/mapping-ais-rapid-advance/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g5rNfy9oXgaDmN8s6d1z</id>
            <title>AI 时代，网关更能打了？</title>
            <link>https://www.infoq.cn/article/g5rNfy9oXgaDmN8s6d1z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g5rNfy9oXgaDmN8s6d1z</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jul 2024 07:29:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 网关, AI 时代, LLM 应用, Envoy
<br>
<br>
总结: 网关在网络通信中扮演着多重角色，包括数据转发、协议转化、负载均衡、访问控制和身份验证、安全防护、内容审核等。随着互联网进入AI时代，LLM应用对网关的需求也发生了变化，需要更强大的功能和性能。传统的Nginx网关难以应对新需求，因此基于Envoy的新一代开源网关应运而生，如Higress。在AI时代的互联网中，网关配置的热更新成为一个重要问题，Envoy等新型网关通过更合理的抽象和更新方式解决了传统网关的瓶颈问题。 </div>
                        <hr>
                    
                    <p></p><p>网关在网络通信中扮演着诸多角色，包括数据转发、协议转化、负载均衡、访问控制和身份验证、安全防护、内容审核，以及服务和 API 颗粒度的管控等，因此常见的网关种类有流量网关、安全网关、微服务网关、API 网关等。在不同语义下，网关的命名也会有所不同，例如 K8s 体系下，有 ingress 网关，在 Sping 体系下，有 Spring Cloud Gateway。但不论如何命名，网关的管控内容几乎都离不开流量、服务、安全和 API 这 4 个维度，只是功能侧重不同、所遵循的协议有差异。</p><p></p><p>另外，随着互联网从 Web 2.0 迈进到 AI 时代，用户和互联网的交互方式，AI 时代下互联网的内容生产流程都发生了显著的转变，这对基础设施（Infra）提出了新的诉求，也带来了新的机遇。Infra 包含的内容非常丰富，本文仅从网关层面分享笔者的所见所感所悟。</p><p></p><p>我们先来看一些 AI 时代出现的新场景和新需求：</p><p></p><p>相比传统 Web 应用，LLM 应用的内容生成时间更长，对话连续性对用户体验至关重要，如果避免后端插件更新导致的服务中断？相比传统 Web 应用，LLM 应用在服务端处理单个请求的资源消耗会大幅超过客户端，来自客户端的攻击成本更低，后端的资源开销更大，如何加固后端架构稳定性？很多 AGI 企业都会通过免费调用策略吸引用户，如何防止黑灰产爬取免费调用量封装成收费 API 所造成的资损？不同于传统 Web 应用基于信息的匹配关系，LLM 应用生成的内容则是基于人工智能推理，如果保障生产内容的合规和安全？当接入多个大模型 API 时，如何屏蔽不同模型厂商 API 的调用差异，降低适配成本？</p><p></p><p>在支持大量 LLM 客户的过程中，我们也看到了一些行业发展趋势，借本文分享给大家：</p><p></p><p>互联网内容的生产机制将从 UGC（User Generate Content） 转变为 AIGC（Artificial Intelligence Generate Content），互联网流量增长，除了要考虑传统的 SEO，还需要考虑 AI 抓取下的 SEO。目前处于 AI 时代的 Web 1.0 阶段，基于静态内容生成，可以预见，AI 时代的 Web 2.0 不久会到来，基于理解互联网内容来识别页面中提供的“可操作能力”，来完成复杂任务，真正的 Web 3.0 也将由 AI 来实现。API 是 LLM 应用的一等公民，并引入了更多流量，催生企业新的生命力和想象空间。LLM 应用对网关的需求超越了传统的流量管控功能，承载了更大的 AI 工程化使命。</p><p></p><p></p><h3>AI 场景下的新场景和新需求</h3><p></p><p></p><p>相比传统 Web 应用，LLM 应用在网关层的流量有以下三大特征：</p><p></p><p>长连接。由 AI 场景常见的 Websocket 和 SSE 协议决定，长连接的比例很高，要求网关更新配置操作对长连接无影响，不影响业务。高延时。LLM 推理的响应延时比普通应用要高出很多，使得 AI 应用面向恶意攻击很脆弱，容易被构造慢请求进行异步并发攻击，攻击者的成本低，但服务端的开销很高。大带宽。 结合 LLM 上下文来回传输，以及高延时的特性，AI 场景对带宽的消耗远超普通应用，网关如果没有实现较好的流式处理能力和内存回收机制，容易导致内存快速上涨。</p><p></p><p>传统 Web 应用中普遍使用的 Nginx 网关难以应对以上新需求，例如变更配置需要 Reload，导致连接断开，不具备安全防护能力等。因此国内外均出现了大量基于 Envoy 为内核的新一代开源网关，本文将以笔者维护的 Higress (<a href="https://github.com/alibaba/higress">https://github.com/alibaba/higress</a>") 为例展开描述。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/58/58b26b83a34f1504747119780ff6900d.png" /></p><p></p><p>Higress 已经为通义千问 APP、灵积平台 (通义千问 API 服务)、人工智能平台 PAI 提供 AI 业务下的网关流量接入，以及多个头部 AGI 独角兽提供 API 网关。这篇文章详细介绍了 Higress AI 网关的能力：《Higress 发布 v1.4，开放 AI 网关能力，增强云原生能力》</p><p></p><p></p><h4>如何实现网关配置的热更新</h4><p></p><p></p><p>互联网从 Web 1.0 演进到 Web 2.0 的时代，互联网从静态内容为主，变为动态更新的 UGC 内容为主，大量用户开始高频使用互联网。用户使用形态，以及网站内容形态的改变，催生了大量技术的变革。例如 HTTP 1.0 到 HTTP 1.1 协议的升级，解决了连接复用的问题。又例如以 Nginx 为代表的基于异步非阻塞的事件驱动架构的网关诞生，解决了 C10K 问题。</p><p></p><p>到了 AI 时代的互联网，LLM 驱动的对话式场景，大量采用 SSE/WebSocket/gRPC 等长连接协议来维持会话。网关除了要解决并发连接问题，还需要解决配置变更导致连接断开的问题。配置变更时的连接断开，不但导致用户会话断开，影响体验，在高并发场景下，断开后的并发重连风暴很有可能将网关和后端业务同时打挂。</p><p></p><p>而类似 Nginx 这样 Web 2.0 时代诞生的网关，并不能解决此问题，Nginx 的整体配置文件发生任意变更，都需要重启 Worker 进程，会同时导致客户端连接（Downstream）和服务端连接（Upstream）断开：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/37/377a410446cf2da8b0cfd849eedba811.png" /></p><p></p><p>笔者参与维护的 Higress 开源网关，使用 Envoy 作为数据面，来解决这一问题。Envoy 站在 Nginx 等网关的肩膀上，对网关配置做了更合理的抽象。例如将处理客户端连接（Downstream）的监听器（Listener）配置发现定义为 LDS（Listener Discovery Service），将处理后端服务连接（Upstream）的服务集群（Cluster）配置发现定义为 CDS（Cluster Discovery Service）。LDS 和 CDS 可以独立更新，从而 Listener 连接池参数更新不会断开 Upstream 连接，Cluster 连接池参数更新变了不会断开 Downstream 连接。</p><p></p><p>对于跟连接无关的配置，又做了进一步抽象，例如路由配置发现定义为 RDS（Route Discovery Service），TLS 证书 / 密钥配置发现定义为 SDS（Secret Discovery Service），都可以独立更新，那么无论是路由变更，还是 HTTPS 证书变更，都不会导致任何连接断开：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7ab2e14a56bf06a7790b10a325ec9c51.png" /></p><p></p><p></p><h4>如何在网关层做好安全和流量防护</h4><p></p><p></p><p>当前的 AI 技术，尤其是 LLM 正处于快速发展阶段。虽然模型压缩、知识蒸馏等技术正被广泛应用以提高效率，但 LLM 应用的资源消耗仍然显著高于传统 Web 应用。</p><p></p><p>针对传统的 Web 应用，服务端处理单个请求的资源消耗通常不会大幅超过客户端，因此对客户端来说，发起分布式拒绝服务（DDoS）攻击的成本相对较高。然而在 LLM 应用的场景中，客户端通过发送长文本或提出复杂的推理问题，可以轻易增加服务端的负载，而自身资源消耗甚微。这种情况突显了在 LLM 应用中部署强大的网关安全防护策略的重要性。传统网关，通常具备两类限流能力：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e3/e33666a1c041612c6b750fb84ca599dd.png" /></p><p></p><p>Higress 不仅支持这些传统限流能力，例如每秒、每分、每小时和每天的请求次数限制（QPS/QPM/QPH/QPD），还引入了对令牌数量的细粒度管理，采用每分钟、每小时和每日令牌数（TPM/TPH/TPD）作为衡量指标，除了 QPS，还支持面向 Token 吞吐的限流防护。</p><p></p><p>“令牌”（Token）在这里作为一个衡量单位，更准确地量化了 LLM 处理的数据量。对 LLM 应用而言，以令牌而非传统请求次数来计量使用情况，更能贴切地反映资源消耗和成本开支。同时也支持多种限流统计维度，包括：API、IP、Cookie、请求 Header、URL 参数和基于 Bearer Token 认证的调用方。</p><p></p><p>AI 场景下，后端保护式限流尤其重要，很多 AGI 厂商都有免费的 Web 应用吸引用户流量，而一些黑灰产可能会爬取页面调用封装成收费 API 来提供给用户实现牟利。这种情况下就可以使用 Higress 的 IP、Cookie 等维度的保护式限流进行防护。</p><p></p><p>此外，当大模型未经过适当的过滤和监控就生成回应时，它们可能产生包含有害语言、误导信息、歧视性言论甚至是违反法律法规的内容。正是因为这种潜在的风险，大模型中的内容安全就显得异常重要。在 Higress 中，通过简单的配置即可对接阿里云内容安全服务，为大模型问答的合规性保驾护航。</p><p></p><p></p><h4>如何应对大带宽和高延时的流量特征</h4><p></p><p></p><p>除了能针对 Token 进行限流，基于 Token 的完整可观测能力，也是 AI Infra 中不可或缺的，例如提供日志、指标、告警等可观测能力。下方展示的限流、观测能力，都依赖对 HTTP 请求 / 响应 Body 的解析处理。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/92/9259de05e4df96b7edc726dfed74ee25.png" /></p><p></p><p>传统网关，如 Nginx/Openresty，以及基于此实现的 Kong/APISIX 等在 Lua 脚本中处理 Body 时，要求必须对请求 / 响应开启缓存。而基于 Envoy 的开源网关，例如 Higress，其插件扩展机制是基于 Wasm 实现的，能够支持对 Body 的流式处理，以处理请求 Body 为例：</p><p></p><p><code lang="null">func onHttpRequestBody(ctx wrapper.HttpContext, config Config, chunk []byte, isLastChunk bool, log wrapper.Log) []byte {    log.Infof("receive request body chunk:%s, isLastChunk:%v", chunk, isLastChunk)    return chunk}
</code></p><p></p><p>在 AI 场景下，因为大带宽 / 高延时的流量特征，网关是否对请求 / 响应进行真正的流式处理，影响是巨大的。</p><p></p><p>首先，LLM 场景下如果网关没有实现流式响应，将严重影响用户受到首个响应的时间，其速度影响能从秒级变到分钟级，严重影响用户体验。其次，是对资源开销的影响。以 Higress 的一个开源用户 Sealos 举例（旗下有 FastGPT 等 AI 相关平台产品），在使用 Nginx 时因为开启了请求 / 响应缓存，在 AI 业务应用被高并发访问时，网关资源水位占用处于崩溃边缘。迁移到 Higress 之后，网关只需很少资源。因为 Higress 提供了完整的流式转发能力，而且提供的插件扩展机制也可以流失处理请求 / 响应，在大带宽场景下，所需的内存占用极低。内存虽然相比 GPU 很廉价，但内存控制不当导致 OOM，导致业务宕机，损失不可估量。下图是常态流量下，Sealos 切换前后网关使用资源的对比：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e5b2b02182c681c6d8f3407e63de6e7.png" /></p><p></p><p></p><h4>如何提升海量域名、海量理由规则下的多租能力</h4><p></p><p></p><p>在 AI 场景下，Envoy 的热更新能力备受青睐，Higress 的一些 AI 平台场景的用户，在一开始也选用了基于 Envoy 的网关，例如 Contour、Gloo、Istio gateway 等。但大都会遇到两个问题：</p><p></p><p>给每个用户 or 每个模型分配一个域名，数量级达到一万规模时，新建路由的生效速度至少要 1 分钟；对多个租户域名使用同一本泛域名证书，开启 HTTP2 时，浏览器访问会遇到 404 问题。</p><p></p><p>对于第一个问题，其根本原因在于路由规则下发方式不够精细，社区开发者对此进行过分析。与此相比，Higress 可以在域名级别进行分片加载，即使达到一万个域名，新增路由的生效时间也只需三秒。此外，Higress 支持按需加载机制，即只有在接收到特定域名的请求时才加载该域名下的路由配置。在配置了大量域名的环境下，这种策略只加载活跃的路由配置，显著减少了网关的内存使用。</p><p></p><p>关于第二个问题，浏览器在 HTTP2 环境中会尽量复用连接。两个请求的域名不同，但解析到的 IP 地址和使用的证书是相同时，连接复用会导致 Host 请求头与建立连接时的 SNI 不匹配，进而在 Envoy 场景下产生 404 错误。多数基于 Envoy 的解决方案是返回 421 状态码，提示浏览器断开连接并重新发起请求，但这个解决方案在浏览器兼容性上存在问题。于是，Higress 借鉴了 Nginx 的办法，使 SNI 的查找（TLS 层）与 Host 头部的查找（HTTP 层）分离，允许它们不匹配，从而能正确地路由配置（在要求客户端证书验证的场景例外）。</p><p></p><p>Higress 支撑海量域名的能力，也是众多 MaaS/SaaS 服务用于实现多租的关键。比如智算服务 PAI- 灵骏平台在近期将网关从同样基于 Envoy 实现的 Contour 迁移到了 Higress 之后，新增路由生效的时间从分钟级变为秒级，同时整体消耗的云资源也大幅下降。</p><p></p><p></p><h3>AI 场景下，网关比我们想象中更能打</h3><p></p><p></p><p>传统 Web 应用，网关扮演的基础角色是流量管理。但在 AI 场景下，网关正承载着更大的 AI 工程化使命，分别体现着 MaaS/AGI 接入层、应用接入层、和企业内部各类系统对接等。</p><p></p><p></p><h4>MaaS/AGI 接入层</h4><p></p><p></p><p>整体架构如下，网关对接入层进行流量管理，除此之外还具备满足负载均衡和流量灰度和观测的能力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e68956a812953995aa8c60da9614a87.png" /></p><p></p><p>负载均衡：</p><p></p><p>由于 AI 场景下，网关的后端通常是模型服务本身，对网关的负载均衡能力提出了特殊要求。由于 LLM 场景具有高延时，且不同请求差异大的特点，传统的 Round Robin 负载均衡策略无法正确平衡负载。Higress 目前采用基于最小请求数的均衡策略，将请求优先转发给当前处理中请求最少的后端节点。针对模型服务负载均衡的挑战，Higress 计划在未来通过调用一个低延时的小参数模型进行旁路预测，以估计每个后端节点的实时负载，从而尽量将请求发送给负载最低的后端节点。</p><p></p><p>流量灰度和观测：</p><p></p><p>AGI 厂商高度依赖 A/B 测试和服务灰度能力来进行模型迭代。作为流量入口，AI 网关需要在流量灰度和观测方面发挥关键作用，包括灰度打标以及入口流量延时和成功率等指标的监测。Higress 凭借其在云原生微服务网关领域的经验，已经积累了强大的能力来满足这些需求。</p><p></p><p></p><h4>AI 应用层</h4><p></p><p></p><p>整体架构如下：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ac/acde0335f6e9e3ae5033ab567e1e21e0.jpeg" /></p><p></p><p>跟随 GPT4 等模型的爆火，涌现了大量的优秀的 AI SaaS 应用，例如：</p><p></p><p>makelogo.ai：AI 生成产品 LogoMyMap.ai：AI 辅助规划 IdeaGamma：AI 生成 PPTPodwise：AI 辅助查看播客</p><p></p><p>许多 AI 应用开发者，尤其是独立开发者，通常不会自己部署模型服务，而是直接利用模型厂商提供的强大 API 来实现创意应用。值得注意的是，许多开发者来自国内。然而，由于底层技术依赖于 OpenAI 等海外 LLM 厂商，这些技术可能不符合国内法规。为了避免潜在的麻烦，这些开发者往往选择将产品推向国际市场，而不是面向国内用户。</p><p></p><p>随着国内大模型技术逐渐赶上 OpenAI 等厂商，并且国内 API 在价格上具有竞争优势，越来越多的 AI 应用预计会选择使用国内厂商的 API 来实现相关功能。这将对网关提出特定需求：</p><p></p><p>通过网关的统一协议，屏蔽不同模型厂商 API 的调用差异，降低适配成本。对涉黄涉政等敏感内容进行屏蔽和过滤，更好地符合国内法规要求。切换模型后的 A/B 测试以及效果观察和对比，包括延迟、成本、用户使用频率等因素。</p><p></p><p>Higress 目前支持的 LLM Provider 有：通义千问、OpenAI/Azure OpenAI、月之暗面、百川智能、零一万物、智谱 AI、阶跃星辰、文心一言、腾讯混元、DeepSeek、Anthropic Claude、Groq、MiniMax、Ollama 等，借助 Higress 活跃的开源开发者社区，支持的类型还在持续增加中。</p><p></p><p></p><h4>企业内部 AI 网关</h4><p></p><p></p><p>整体架构如下：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/59/59fadb676e0259ff2aad044afb48d4f5.png" /></p><p></p><p>大量 AGI 厂商在闭源和开源模型能力方面展开竞争，而受益者主要是企业用户。企业在选择模型时需要在性能和成本之间做 trade-off。面对众多模型，尤其是在厂商提供的 API 不一致时，企业需要一个统一的网关来屏蔽模型协议的差异，从而提供一个统一接口，便于企业内部系统的对接和使用。在这种场景下，网关的架构类似于 ESB（企业服务总线）的架构，即所有内部 AI 流量都通过网关进行统一治理。这样的架构带来了以下好处：</p><p></p><p>成本分摊计算：借助网关的观测能力，可以审计企业内部不同业务部门的 Token 消耗量，用于成本分摊并发现不合理的成本。提高稳定性：基于网关提供的多模型对接能力，当主用模型调用失败时，可以自动切换至备用模型，保障依赖 AI 能力的业务稳定性。降低调用成本：在一些固定业务流程中，LLM 接口的输入输出相似性较高时，可以基于向量相似性进行缓存召回，从而降低企业的 AI 调用成本。认证和限流：通过对企业内员工的 API 调用进行限量控制，管理整体成本。内容安全：实现统一的内容安全管理，禁止发送敏感数据，防止企业敏感数据泄漏。</p><p></p><p>这种架构下，网关不再只是接入层的流量网关，而是要处理来自所有依赖 AI 能力的业务模块的访问流量。网关更能打了。</p><p></p><p></p><h3>畅想 AI 时代的互联网发展</h3><p></p><p></p><p>笔者发现在 AI 火了之后，大家已经很少提 Web 3.0 的概念了。而且很有趣的一个事是，CDN 和网络防护提供商 CloudFlare，已经将控制台内的一级入口 Web 3.0 换成了 AI，并集成了 Workers AI 和 AI Gateway 这两款产品。而笔者觉得，真正的 Web 3.0 也许将由 AI 带来。</p><p></p><p>就像 Web 1.0 到 Web 2.0 的演进，用户的交互方式和互联网的内容形式发生了彻头彻尾的改变，我们其实已经身处在类似的改变之中。例如，笔者的常用搜索工具从 Google 换到了 Perplexity。做互联网流量增长，除了要考虑传统的 SEO，还需要考虑 AI 抓取下的 SEO，下面来自 Perplexity 对这一问题的回答：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9a/9abc239430c6fdbedf7ec85ca0549f80.png" /></p><p></p><p>到并不是说 Perplexity 未来一定会替代 Google，但这种改变其实反应了一种趋势：</p><p></p><p>从用户角度看，用户从主动参与互联网转变为通过 AI 帮助参与。从内容角度看，不仅需要服务于真实用户，还要同时服务于 AI。</p><p></p><p>Perplexity 这样的工具还只是基于静态内容，可以类比为 AI 时代的 Web 1.0。可以预见，AI 时代的 Web 2.0 会是：</p><p></p><p>电商场景下，在用户浏览商品时，AI 将充当导购，根据商品信息与用户对话，并在用户确认后自动下单；出行场景下：AI 将根据用户的出行目标地点自动安排旅行计划，了解用户喜好，预订沿途餐厅和酒店；OA 场景下：用户需要操作资源时，AI 将自动提交审批申请，查询审批状态，并在获批后完成资源操作。</p><p></p><p>在这种模式下，AI 需要理解互联网内容，并识别页面中提供的“可操作能力”，从而代替人类执行操作。苹果宣布将在 iOS 18 中 大幅提升 Siri 的能力，未来 Siri 将能够访问应用程序的各种功能，这也需要应用程序为 AI 提供“可操作能力”的声明。HTML 也有相关社区提案，让 AI 可以更方便地识别页面中的可执行任务，明确其输入和输出定义。</p><p></p><p>未来的互联网内容，无论是 APP 还是 HTML 场景，都将面向 AI 进行改变。核心在于让 AI 知道如何操作页面内容，从而帮助用户完成复杂任务。为 AI 提供的“可操作能力”声明，实际上就是 API 声明。当前，大量互联网应用，尤其是 ToC 应用，API 仅在内部开发过程中使用，最高频使用 API 的可能是前端或 BFF 层的开发人员。在国内，由于研发成本普遍低于国外，不会为了降低前后端对接成本，而去优化 API 设计，开发过程中往往忽略了 API 的重要性。因此，虽然在海外 API 管理产品的市场竞争已经是一片红海，但在国内 API 管理以及 API First 的理念并不普及。</p><p></p><p>随着 AI 操作互联网场景的不断增加，API 将成为 LLM 应用的一等公民，API 管理的重要性将愈发明显。类似于 Perplexity 在抓取页面内容时使用清晰的标题、小标题和列表以便 AI 更容易理解和提取内容；定义清晰的 API、明确的输入输出参数，以及 API 的版本管理，将变得至关重要。</p><p></p><p>对网关来说，应回归本质，在 AI 的加持下，帮助用户做好 API 的设计、管理将成为核心能力。而通过合理设计的 API，网关也可以更深入地了解所处理流量的业务含义，从而实现更智能化的流量治理。</p><p></p><p>内容推荐</p><p></p><p>AIGC技术正以惊人的速度重塑着创新的边界，InfoQ 首期《大模型领航者AIGC实践案例集锦》电子书，深度对话30位国内顶尖大模型专家，洞悉大模型技术前沿与未来趋势，精选10余个行业一线实践案例，全面展示大模型在多个垂直行业的应用成果，同时，揭秘全球热门大模型效果，为创业者、开发者提供决策支持和选型参考。关注「AI前线」，回复「领航者」免费获取电子书。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/18/18566cb9b5575c02ffb89182a3582b73.jpeg" /></p><p></p><p>活动推荐</p><p></p><p>AICon 全球人工智能开发与应用大会，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。聚焦大模型训练与推理、AI Agent、RAG 技术、多模态等前沿议题，汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。</p><p></p><p>在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/65/6573657a90550f91dc3658ad05122b02.other" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</id>
            <title>大厂期权归属前遭暴力裁员，80 余万期权泡汤；去哪儿宣布每周两天“不坐班”；萝卜快跑是人类远程代驾？客服：无人操控 | Q资讯</title>
            <link>https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dd52FPTw9XqEjkmY5ZRj</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 07:31:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, AI服务, 期权, 裁员
<br>
<br>
总结: 苹果公司计划增加iPhone 16系列机型的出货量，依靠AI服务提振需求；大厂员工因裁员导致期权泡汤，引发法律纠纷。萝卜快跑无人驾驶服务被曝有真人干预，百度设立无人驾驶实验基地应对问题。 </div>
                        <hr>
                    
                    <p></p><blockquote>苹果大动作！AI 服务或为关键驱动力；大厂期权归属前遭暴力裁员，80余万期权泡汤；“萝卜快跑”有真人干预？腾讯被爆调薪！微软和苹果双双放弃 OpenAI 董事会观察员席位；谷歌开放“暗网报告”功能；中国区员工只能用 iPhone？微软回应；抖音宣布推出抖音 VR 直播；去哪儿网正式推行“3+2”混合办公模式；马斯克叫停与甲骨文的 100 亿美元谈判；“WPS 崩了”：三周内第二次；RockYou2024 文件泄露，数百万用户信息暴露；二季度 PC 出货量增长 3%；开源代码编辑器 Zed 发布原生 Linux 版本；Java 之父 James Gosling 宣布退休……</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>苹果大动作！iPhone&nbsp;16出货预期大增，AI服务或为关键驱动力</h4><p></p><p>彭博社7&nbsp;月&nbsp;11&nbsp;日报道，表示苹果公司已经通知其供应商和合作伙伴，**2024&nbsp;年&nbsp;iPhone&nbsp;16&nbsp;系列机型的出货量目标要比&nbsp;iPhone&nbsp;15&nbsp;系列（8100&nbsp;万台）增长&nbsp;10%，至少要超过&nbsp;9000&nbsp;万台，**以期借助人工智能（AI）服务带来的潜在需求扫除公司2023年遭遇的阴霾。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7cedef26a96016811c9384717d707bcf.png" /></p><p></p><p>这印证了本周早些时候产业链人士的说法。知情人士表示，苹果告诉供应商和合作伙伴，其新款iPhone的出货量将较前几款增加约10%。与之相比，在2023年下半年，iPhone&nbsp;15的出货量约为8100万部。</p><p></p><p>根据知情人士的说法，苹果已经变得越来越有信心，这家科技巨头可能认为，其推出的个人智能化系统“Apple&nbsp;Intelligence”（苹果智能）中的一些功能将有助于提振iPhone&nbsp;16上市时的需求。</p><p></p><p>上月，苹果在年度全球开发者大会（WWDC）上披露了公司在AI方面新的进展，包括与OpenAI构建合作伙伴关系，推出能够优先置顶推送、总结文本、生成图片的套件，更强大的Siri等。</p><p></p><h4>大厂期权归属前遭暴力裁员，80&nbsp;余万期权泡汤</h4><p></p><p>近日，综合凤凰网和澎湃新闻消息，在得物兢兢业业工作两年后，前员工徐凯决定和老东家“对簿公堂”。</p><p></p><p>一年前，徐凯多次与公司沟通取得期权再离职未果后，到上海市仲裁委员会处申请恢复与得物的劳动关系，后被予以支持。2024&nbsp;年&nbsp;7&nbsp;月，因不服上海市仲裁委员会裁定的结果，得物继续上诉，再度将前员工诉于法庭之上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed62eadbc21f8bfc6749ed84519e39aa.webp" /></p><p></p><p>据徐凯表述，其在&nbsp;2021&nbsp;年&nbsp;9&nbsp;月加入该公司，任职为前端技术专家，税前薪资为月薪&nbsp;5&nbsp;万元左右，同时其握有部分期权。徐凯在工作期间，一直保持高效且敬业的态度，未曾出现过任何显著的工作失误。然而，在&nbsp;2023&nbsp;年&nbsp;7&nbsp;月，公司却以“未按时提交周报”和“工作时间分配不当”等理由，对他进行了绩效评估，最终给出了最低等级&nbsp;C&nbsp;的评价。紧接着，该企业启动了裁员流程，意图解除与徐凯的劳动合同。</p><p></p><p>尤为令人关注的是，此时距离徐凯手中期权的归属期满仅剩一个多月。他持有的近&nbsp;2000&nbsp;股期权，按原计划将在&nbsp;2023&nbsp;年&nbsp;9&nbsp;月行权大约&nbsp;1000&nbsp;股，价值高达&nbsp;80&nbsp;余万元。然而，裁员决定一旦生效，这些期权将自动失效，导致徐凯的潜在收益瞬间蒸发。</p><p></p><p>徐凯还表示，他在该公司工作期间常年面临着“10106”的局面，晚上&nbsp;10&nbsp;点下班是常态，且技术员工会被计算总工时，工时靠后的人就面临着被淘汰的风险。“我一开始是带团队的，风险就很高”。得物相关负责人针对以上情况回复凤凰网表示称，“该员工曾因&nbsp;3&nbsp;次绩效考核不合格（2022&nbsp;年&nbsp;Q3&nbsp;季度、2023&nbsp;年&nbsp;Q1&nbsp;季度、2023&nbsp;年&nbsp;Q2&nbsp;季度），于去年&nbsp;7&nbsp;月已经沟通解除劳动合同”。此外，得物方面还表示，“得物公司业务健康发展，欢迎优秀人才加入，感谢关注。”</p><p></p><p>另外，2023&nbsp;年&nbsp;7&nbsp;月，京东司法拍卖网披露的消息显示，“上海市中山南路&nbsp;566&nbsp;弄”一处房屋以&nbsp;1.58&nbsp;亿元成交，较起拍价&nbsp;1.25&nbsp;亿元溢价&nbsp;26.4%。根据竞拍结果显示，该套房屋由杨冰拍下。据澎湃新闻报道，该名自然人杨冰为得物创始人兼&nbsp;CEO。</p><p></p><h4>“萝卜快跑”被曝有真人干预？</h4><p></p><p>最近有网图流传，揭示了关于“萝卜快跑”无人驾驶服务的新视角——其背后竟隐藏着真人远程代驾的运作模式。图片聚焦于萝卜快跑的智控中心，清晰可见有专业人员坐在配备模拟方向盘的监控屏幕前，精准操控着车辆。</p><p></p><p><img src="https://static001.geekbang.org/infoq/45/4512c73dde1c4c5097579a4a8129212b.webp" /></p><p></p><p>据媒体报道，百度设立的无人驾驶实验基地，其核心功能之一便是应对无人驾驶车辆可能遭遇的棘手问题，通过云端安全员的远程介入，确保车辆能够安全脱困。</p><p></p><p>鉴于极端驾驶场景并非常态，云端驾驶员相比随车安全员展现出更高的效率与灵活性。他们能以“一对多”的模式，同时服务于多辆无人驾驶车辆，极大地提升了资源利用率与响应速度。知情人士进一步透露，在无人驾驶网约车的运营体系中，后台座舱内的一名安全员能够高效监控并管理&nbsp;3&nbsp;至&nbsp;5&nbsp;台车辆，这些安全员多具备丰富的驾驶经验，来自网约车司机、公交车司机等职业背景。</p><p></p><p>此外，随着&nbsp;2023&nbsp;年&nbsp;11&nbsp;月交通运输部办公厅发布的《自动驾驶汽车运输安全服务指南&nbsp;(试行)》的正式实施，明确了在特定区域内运营的完全自动驾驶出租车，可采用远程安全员模式，并规定了远程安全员与车辆之间不得低于&nbsp;1:3&nbsp;的人车比，为无人驾驶行业的规范化发展提供了政策指引。</p><p></p><h4>腾讯被爆调薪！年底十三薪分摊到月薪</h4><p></p><p>7月10日，腾讯内部向全员发布邮件称，将调整内部的薪酬福利政策，一是将年底十三薪分摊到月薪上；二是将现有的易居租房补贴融入月薪。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03fb1bdb72e4e972cc75fd34872b2917.png" /></p><p></p><p>图片来源于网络</p><p></p><p>腾讯表示，过去两年，外部环境的变化让不少员工对于即时稳定的现金流有了更高诉求。经过慎重评估后，决定今年除正常进行年度薪酬回顾外，还将对薪酬结构做出两个调整：一是进行全公司薪酬结构的调整，把服务奖融入月薪；二是将现有的易居租房补贴融入月薪。</p><p></p><p>腾讯邮件中称，这两个举措旨在帮助大家在更高、更稳定的月收入基础上更安心地安排工作与生活。相关调整于2024年7月1日起生效，8月5日的发薪中开始体现。</p><p></p><p>腾讯表示，自成立之初便在年终为员工提供额外的十三薪，以此作为对员工一年辛勤工作的认可。随着公司规模的扩大，腾讯还引入了与业绩挂钩的奖金制度，形成了包括服务奖、绩效奖和特别奖在内的年终奖金体系，其中服务奖主要取决于员工的月工资和全年的工作时长，实质上是固定工资的补充部分。</p><p></p><p>此次将以年度薪酬回顾后的月薪为标准，把服务奖平均分摊融入到12个月固定工资中，以提升员工的月度现金流，未来年终奖也将回归到纯粹的业绩激励。</p><p></p><p>相关知情人士表示，对员工而言，这次将十三薪和租房补贴融入到月薪之后，每月员工到手的收入将增加，也比较利好生活压力比较大的应届生和有房贷的员工。</p><p></p><h4>微软和苹果双双放弃OpenAI董事会观察员席位</h4><p></p><p>据金融时报7月10日报道，在全球监管机构对大型科技公司投资AI初创企业的审查日益加剧之际，美国两大科技巨头微软和苹果都放弃了在OpenAI董事会的观察员席位——微软宣布立即退出OpenAI董事会的观察员席位，苹果也不会担任此类职务。这就意味着，OpenAI&nbsp;董事会不再设立无投票权的观察员席位。</p><p></p><p>鉴于全球监管部门审查科技巨头投资&nbsp;AI&nbsp;初创企业的活动越来越严格，微软已放弃了其在&nbsp;OpenAI&nbsp;董事会观察员的席位，而苹果将不会担任类似的职位。</p><p></p><p>微软已向&nbsp;OpenAI&nbsp;投资了&nbsp;130&nbsp;亿美元，微软在致&nbsp;OpenAI&nbsp;的一封信中表示，退出&nbsp;OpenAI&nbsp;董事会的席位“立即生效”。</p><p></p><p>据一位直接了解此事的人士透露，作为将&nbsp;ChatGPT&nbsp;整合到苹果设备的交易的一部分，外界原本预计苹果也将在&nbsp;OpenAI&nbsp;董事会中担任观察员角色，但现在它不会这么做。苹果拒绝置评。</p><p></p><h4>谷歌开放“暗网报告”功能：网罗安全事件、通知用户信息泄露</h4><p></p><p>据消息，谷歌公司7月10日宣布将于本月底向所有谷歌账号用户开放“暗网报告”功能，帮助用户更快了解网络上发生的个人数据泄露事件。 “暗网报告”功能此前仅限于购买&nbsp;Google&nbsp;One&nbsp;订阅的功能，主要监控常规网络方式无法访问的网络部分，除了排查个人信息是否已经泄露之外，还可以搜索相关漏洞信息。</p><p></p><p>谷歌在公告中表示&nbsp;Google&nbsp;One&nbsp;本月底将不再提供“暗网报告”功能，用户登录账号后可以免费访问。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61bcd4c8ceef1f69f566f497ec161cce.png" /></p><p></p><p>谷歌用户登录账号之后，可以打开“关于你的结果”页面，查找近期信息泄露事件中是否包含你的个人信息。“暗网报告”功能目前已经在全球&nbsp;46&nbsp;个国家和地区上线，可以检查与你的姓名、电子邮件、地址、用户名和密码相关的数据，如果在数据泄露事件中发现用户个人信息，就会第一时间通知用户。</p><p></p><h4>&nbsp;中国区员工只能用 iPhone？微软回应</h4><p></p><p>近日，有媒体报道称，微软已要求中国员工不能使用安卓手机，时间是从今年9月份开始。报道中提到，**微软已告知其中国员工，从今年9月份开始，他们只能在工作中使用iPhone，**此举实际上将安卓设备排除在了工作场所之外。</p><p></p><p>根据业内说法，该措施是微软全球“安全未来计划”（Secure&nbsp;Future&nbsp;Initiative）的一部分，将影响中国的数百名员工，旨在确保所有员工使用微软身份验证器Microsoft&nbsp;Authenticator（微软开发的一款应用）和Identity&nbsp;Pass等应用程序。</p><p></p><p>7月9日晚间，微软发言人就此事回应媒体记者称：**“Microsoft&nbsp;Authenticator和Identity&nbsp;Pass应用程序已正式在Apple&nbsp;Store（软件商店）和Google&nbsp;Play&nbsp;Store上架。我们希望为员工提供访问这些必要应用程序的途径，由于本地区无法使用Google移动服务，我们即向员工提供了例如iOS设备的选择。”**对此，不少网友称，若是能配发工作机就没问题。但若强制要求员工自行购买，则“不能接受”。</p><p></p><h4>抖音宣布推出抖音VR直播</h4><p></p><p>7月10日消息，抖音集团正式宣布推出抖音VR直播功能，用户现在可以在Apple&nbsp;Vision&nbsp;Pro设备上下载并体验这一创新功能。据了解，该软件支持180°、360°全景直播，即使不在现场，用户也能有身临其境的感觉，实时沉浸式看直播。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee5daa14e98e80bfd9ac36944b276734.png" /></p><p></p><p>**据介绍，抖音VR直播支持小范围6DoF&nbsp;3D直播，可进行180°或360°全景直播，用户可多屏切换、发送3D礼物。**Vision&nbsp;Pro是苹果首款头显设备，定位是MR（混合现实）设备。</p><p></p><p>MR技术结合AR和VR，通过加强虚拟对象与现实世界的交互，实现混合世界的全新体验。目前XR设备所带来的功能价值较为有限，内容生态成为产品竞争的关键因素。兼具软硬件生态的苹果入局MR，有望通过其市场影响力吸引顶尖内容制作者建立良好开发生态，同时依靠品牌影响力能够在更低的用户教育、触达成本下实现优质内容供给—平台破圈引流—消费者需求响应的良性生态。</p><p></p><h4>去哪儿网正式推行“3+2”混合办公模式，员工每周有2天可自主选择办公地点</h4><p></p><p>7&nbsp;月&nbsp;9&nbsp;日消息，去哪儿&nbsp;CEO&nbsp;陈刚发全员信宣布，从&nbsp;7&nbsp;月&nbsp;15&nbsp;日（下周一）起，每周三、周五，员工可以灵活选择办公地点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdfa591cf4dead0631f4449bc121d902.png" /></p><p></p><p>陈刚特意强调，员工按规定混合办公，“无需任何申请审批”。</p><p></p><p>据了解，混合办公的适用人员范围以入职&nbsp;6&nbsp;个月以上的标准工时正式员工为主。去年&nbsp;10&nbsp;月，去哪儿开始了为期&nbsp;9&nbsp;个月的混合办公试验。回收数据显示，员工对混合办公的各个维度反馈正面&nbsp;——&nbsp;超过九成的员工认为混合办公后幸福感有明显提升，员工主动离职率在混合办公后下降了三成。</p><p></p><p>去哪儿&nbsp;COO（首席运营官）刘连春表示，混合办公没有让公司业绩变坏，并且显著提升了员工的幸福度。那这件事情公司何乐而不为呢？他强调混合办公不会影响员工的绩效和晋升。</p><p></p><h4>马斯克叫停与甲骨文的100亿美元谈判，拟自建“超算工厂”</h4><p></p><p>当地时间7月9日，据The&nbsp;Information报道，马斯克旗下的人工智能（AI）初创公司xAI已与甲骨文终止扩大一项现有协议的谈判，这笔交易的潜在价值高达100亿美元。根据该协议，xAI将从甲骨文租用英伟达的AI芯片搭建超级计算机。</p><p></p><p>马斯克当天在X平台上回应，xAI将自行建造超级计算机，这样能保证更快速地完成，从而赶上竞争对手。目前，该公司正使用戴尔和超微电脑提供的英伟达芯片，在美国田纳西州孟菲斯建立AI数据中心。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18523a4710256617e83c83e40520693e.png" /></p><p></p><p>此次谈判自&nbsp;5&nbsp;月份以来一直在进行，经过一个多月的商谈后依然以失败告终。据悉，交易告吹导致甲骨文股价暴跌，周二股价下跌&nbsp;3%，收于&nbsp;140.68&nbsp;美元。此次下跌结束了甲骨文连续七天的上涨势头，并引发了投资者对该公司在竞争激烈的云计算市场中能否获得并维持大规模合同的担忧。</p><p></p><p>虽然新交易失败，但甲骨文和&nbsp;xAI&nbsp;将继续在基础设施需求方面进行合作。xAI&nbsp;与甲骨文签订的在&nbsp;Oracle&nbsp;Gen2&nbsp;Cloud&nbsp;中训练&nbsp;AI&nbsp;模型的现有合同仍不受影响，这表明两家公司之间的关系并未完全断绝。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s/mPuhRGW8AeLgntE1QLfSyw">xAI&nbsp;和&nbsp;Oracle&nbsp;间&nbsp;100&nbsp;亿美元的生意谈崩了！有钱也租不到芯片的马斯克要自建超级计算中心，就不信“钞”能力还会失效？</a>"》</p><p></p><h4>半年报扭亏！手机市场回暖&nbsp;欧菲光逐渐走出“苹果阴影”</h4><p></p><p>近日，欧菲光（002456）发布的公告引起了广泛关注。被苹果“抛弃”的欧菲光，在经过三年的业绩低迷后，终于重新进入稳定的收入状态。7月9日晚间，该公司发布的公告显示，预计今年上半年实现3600万元–4500万元的盈利，这也是欧菲光时隔三年首次实现半年报盈利。</p><p></p><p>结合此前的公告和业内人士分析内容，欧菲光盈利主要得益于华为这个大客户重新占领手机市场，以及欧菲光在智能汽车领域的增长，且该公司本身的业务结构也在向着更健康的方向调整。</p><p></p><p>值得注意的是，虽然欧菲光对于今年下半年智能手机和智能汽车市场的预期比较乐观，但该公司仍然非常依赖智能手机市场。业内人士给出建议，欧菲光应该深挖技术“护身河”，持续走多元化的路子。</p><p></p><h4>“WPS&nbsp;崩了”：三周内第二次，官方回应“服务已恢复正常”</h4><p></p><p>7&nbsp;月&nbsp;8&nbsp;日早，WPS&nbsp;崩了冲上微博热搜。这次的崩溃对于在工作中使用&nbsp;WPS&nbsp;的小伙伴影响比较严重。</p><p></p><p>不少网友反馈，自己扫码登不上&nbsp;WPS，验证码也收不到，尝试了多种办法都不行，也有网友反馈存在无法登录、云端文件无法打开，还有网友遇到了数据无法保存等场景。</p><p></p><p>针对这一情况，WPS&nbsp;客服团队迅速响应，称&nbsp;7&nbsp;月&nbsp;8&nbsp;日早上全国范围内的云服务都出现了故障，已接到了用户反馈。目前在紧急排查修复中，部分用户已恢复，其余也在逐步恢复中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91d5786c52fe946f49b60445db223327.jpeg" /></p><p></p><p>值得一提的是，不久前，6月28日下午，多名用户反映WPS金山文档无法正常打开疑似应用崩溃，“WPS崩了”话题也登上了热搜。</p><p></p><p>有网友表示，之前碰到了很多次&nbsp;WPS&nbsp;崩溃的情况，打开&nbsp;WPS&nbsp;Office&nbsp;时遭遇了启动失败的错误，而且多次重启都未能成功。必须要卸载，重新安装新版本才能解决。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>重大数据泄露事件：RockYou2024文件泄露，数百万用户信息暴露</h4><p></p><p>7月11日消息，据网络安全公司Cybernews近日披露，一个名为"RockYou2024"的文件在暗网论坛上被公开。文件中包含了9948575739条明文密码，这一数字几乎涵盖了全球网民的真实密码集合，网络安全专家认为这是有史以来最大的密码泄露事件。</p><p></p><p>Cybernews的研究人员通过泄露密码检查器的数据交叉对照后发现，这些密码来自新旧数据泄露的混合。也就是说，RockYou2024主要还是以往密码泄露事件的汇编，据估计包含了来自总计4000个巨大被盗凭证数据库的条目，时间跨度至少达二十年之久。值得注意的是，新文件中包含了早先的RockYou2021，其中含有84亿个密码，也就是说RockYou2024在2021的基础上新增约15亿个密码，时间范围从2021年至2024年。</p><p></p><p>Cybernews警告称，一旦RockYou2024与其他泄露数据库联手，比如用户的邮箱地址和其他敏感信息，那么数据泄露、金融诈骗、身份盗窃……一连串的灾难性后果将接踵而至。</p><p></p><h4>二季度&nbsp;PC&nbsp;出货量增长&nbsp;3%，中国市场继续低迷</h4><p></p><p>IDC&nbsp;公布的数据显示，二季度&nbsp;PC&nbsp;出货量比去年同期增加&nbsp;3%，在连续七个季度下滑之后连续两个季度保持了增长。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28a1eda40eadb601e123c16d0947101d.png" /></p><p></p><p>但中国市场持续低迷阻碍了&nbsp;PC&nbsp;市场的复苏。二季度&nbsp;PC&nbsp;出货量&nbsp;6490&nbsp;万台，排除中国市场的&nbsp;PC&nbsp;出货量同比增长逾&nbsp;5%。</p><p></p><p>**IDC全球设备跟踪器集团副总裁Ryan&nbsp;Reith表示，**毫无疑问，PC市场和其他技术市场一样，由于成熟度和逆风因素，在短期内面临挑战。然而，连续两个季度的增长，加上围绕AIPC的大量市场炒作，再加上一个虽不够吸引人但可以说更重要的商用市场换机周期，似乎正是PC市场所需要的。热点显然是围绕AI的，但是，non-AIPC的购买产生的影响更大，使这个成熟的市场显示出积极的迹象。</p><p></p><p>近几个月来，大多数行业参与者都制定了AIPC的初步战略，主要关注组件方面和商用市场的潜力。尽管IDC认为，商用市场在PC行业的AI领域短期内具有最大的上升空间，但消费市场的故事尚未完全被讲述。人们都在期待苹果在今年晚些时候通过预期的产品发布来推动这一信息，但不应忽视的是，高通、英特尔和AMD都可能在消费者和商业AI&nbsp;PC领域制造声势。</p><p></p><h4>开源代码编辑器&nbsp;Zed&nbsp;发布原生&nbsp;Linux&nbsp;版本</h4><p></p><p>7月10日，Zed官方昨日发布了0.143.6版本，并正式支持Linux。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5c124710c52aa6893c39cfc4c0c1a67.png" /></p><p></p><p>据介绍，Linux&nbsp;上的&nbsp;Zed&nbsp;正在使用&nbsp;Vulkan&nbsp;API&nbsp;进行&nbsp;GPU&nbsp;加速。它同时支持&nbsp;Wayland&nbsp;和&nbsp;X11&nbsp;会话。到目前为止，Zed&nbsp;团队的开发重心主要集中在&nbsp;Ubuntu&nbsp;下的测试。</p><p></p><p>Zed&nbsp;是一款支持多人协作的代码编辑器，由&nbsp;Atom&nbsp;编辑器原作者主导，其底层采用&nbsp;Rust&nbsp;编写、默认支持&nbsp;Rust，还自带了&nbsp;rust-analyzer，主打&nbsp;“高性能”——作者表示希望将&nbsp;Zed&nbsp;打造为世界上最好的文本编辑器。</p><p></p><h4>Java&nbsp;之父&nbsp;James&nbsp;Gosling&nbsp;宣布退休</h4><p></p><p>近日，Java&nbsp;语言之父&nbsp;James&nbsp;Gosling&nbsp;在领英上发文宣布自己即将退休。他在博文中写道：“我现在终于退休了。干了这么多年软件工程师，是时候享受生活了。尽管曾经经历过疫情肆虐、业界萧条，但在亚马逊的过去七年里，我过得非常愉快。我还有很多未尽事宜要去完成，我将满怀期待开启新征程”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/0927617083be1d4e9ab9f0fcdac47b1d.jpeg" /></p><p></p><p>Java&nbsp;的故事始于&nbsp;1991&nbsp;年，当时&nbsp;Sun&nbsp;Microsystems&nbsp;试图将其在计算机工作站市场的领先地位扩展到新兴且发展迅速的个人电子产品市场。几乎没有人预料到&nbsp;Sun&nbsp;即将创建的编程语言会使计算大众化，激发了一个全球范围的社区，并成为了一个由语言、运行时平台、SDK、开源项目以及许多工具组成的持久软件开发生态系统的平台。</p><p></p><p>经过&nbsp;James&nbsp;Gosling&nbsp;领导团队数年秘密开发后，Sun&nbsp;于&nbsp;1995&nbsp;年发布了具有里程碑意义的“一次编写，随处运行”&nbsp;的&nbsp;Java&nbsp;平台，并将重点从最初的交互式电视系统设计转到了新兴的万维网应用程序上。在本世纪初，Java&nbsp;就已经开始为从智能卡到太空飞行器的一切制作动画了。如今，数以百万计的开发人员在使用&nbsp;Java&nbsp;编程，它至今仍然是工业界最受欢迎和使用最多的语言。</p><p></p><h4>微软宣布90天内将结束Win11部分版本服务</h4><p></p><p>7&nbsp;月&nbsp;9&nbsp;日，微软&nbsp;Windows&nbsp;11&nbsp;即将迎来其三岁生日，这意味着初始版本&nbsp;21H2&nbsp;和第一个功能更新版本&nbsp;22H2&nbsp;将很快将失去支持，参考微软近日发布的通知，微软警告这两个版本仅剩&nbsp;90&nbsp;天生命周期支持，之后&nbsp;Windows&nbsp;11&nbsp;21H2&nbsp;和&nbsp;22H2&nbsp;无法再获得安全更新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/255e6ea92204e463a8c09fb030950741.png" /></p><p></p><p>2024&nbsp;年&nbsp;6&nbsp;月&nbsp;Windows&nbsp;各版本占有率&nbsp;&nbsp;来源：statcounter</p><p></p><p>事实上，目前Windows&nbsp;11&nbsp;21H2已经不再支持普通消费者，唯一仍在更新的版本是企业版、教育版和物联网企业版。</p><p></p><p>微软于2021年10月5日发布了&nbsp;Windows&nbsp;11，这是在&nbsp;Windows&nbsp;10&nbsp;推出大约六年后发布的。Windows&nbsp;11&nbsp;的第一个版本是&nbsp;21H2，该版本在2023年10月对普通用户结束支持。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</id>
            <title>国产芯片大厂三年干到 70 亿市值，却一次性裁员 50 %？员工曝 CTO 不懂技术！</title>
            <link>https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TYf58GAHpWIX5XFMBcP3</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 06:45:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯华章, 裁员, EDA, 市场困境
<br>
<br>
总结: 中国国产EDA大厂芯华章开始大规模裁员，引发关注。裁员比例高达50%，公司否认裁员比例，称为谣言。裁员引发了对公司战略收缩和质疑的猜测。芯华章曾是估值70亿的独角兽，但裁员已进行两波。管理层和并购被指责，显示国产EDA厂商面临严峻市场困境。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>7月8日，某职场社交平台有网友爆料称，国产EDA大厂芯华章开始大规模裁员，裁员比例高达50%，并且第一批裁员已经谈话完毕。另有认证为“芯华章科技股份有限公司员工”的网友还补充称，“不止50%，软件部裁员接近60%。留下来的人更加惶恐。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0cbb1de6bc2e8f2a221f71f89cde5459.jpeg" /></p><p></p><p>&nbsp;</p><p>也有不少人关心芯华章此次裁员的赔偿方案，“百分之五十的裁员比例，赔偿总和可不是小数目。”</p><p>&nbsp;</p><p>消息传出后，有芯华章内部人士向媒体表示，“公司确实有在战略收缩，但是人员优化比例有限，裁员50%的说法是谣言。如果真像传闻那样一下子裁员50%，那公司根本就没法正常运转了。”</p><p>&nbsp;</p><p>但依然有不少对于芯华章此次战略收缩的深层猜测，其中有两方面的解释：一是公司业务方面，“管理层决策失误，技术路线步子迈大了，客户也没搞定”；二是市值套现的质疑，“如今财务造假严打，IPO收紧，就原形毕露了”。</p><p>&nbsp;</p><p>注：EDA全称Electronic&nbsp;Design&nbsp;Automation，意为电子设计自动化，是用于辅助完成超大规模集成电路芯片设计、制造、封装、测试整个流程的计算机软件，完整的集成电路设计和制造流程均需要对应的EDA工具作为支撑，因而有“芯片之母”的称号。</p><p>&nbsp;</p><p></p><h1>三年估值70亿，裁员已进行两波</h1><p></p><p>芯华章由前新思科技中国区副总经理王礼宾于2020年3月创立，当时正值国内积极倡导国产EDA工具发展，希望打破国外厂商垄断局面。</p><p>&nbsp;</p><p>刚成立一年不久，芯华章便已完成5轮融资，累计融资金额超12亿元。到成立三年时，芯华章已完成8轮投资，每轮融资均数亿元。在去年&nbsp;3&nbsp;月获得中信科&nbsp;5G&nbsp;基金的战略投资后，芯华章晋升为估值&nbsp;70&nbsp;亿元的独角兽。</p><p>&nbsp;</p><p>据了解，芯华章主要聚焦芯片EDA数字验证领域，打造从芯片到系统的验证解决方案，提供完整的验证EDA工具链服务。2021年，芯华章率先发布支持国产服务器架构的数字仿真器穹鼎GalaxSim，去年7月又推出新一代高速仿真器GalaxSim&nbsp;Turbo。</p><p>&nbsp;</p><p>关于这次的裁员，据一位认证信息为“芯华章科技股份有限公司员工”的网友介绍，他是芯华章GalaxSim部门的成员，其一再表示，“不知道还能苟多久，下次估计就是整个项目组了，毕竟没剩多少人了。”</p><p>&nbsp;</p><p>值得注意的是，就在今年4月，芯华章宣布其核心EDA软件产品已完成华为鲲鹏平台的移植工作。基于鲲鹏处理器等国产架构，芯华章逻辑仿真器GalaxSim、形式化验证工具GalaxFV，都能有效利用鲲鹏的高性能集群去提高编译与运算，显著提高了系统级芯片仿真验证效率。其中，GalaxSim在多个客户测试用例上已经取得了2-3倍的仿真性能提升，大幅降低了仿真回归测试的时间。</p><p>&nbsp;</p><p>然而，此次的“战略收缩”已不是芯华章第一次进行裁员。有知情人士透露，“第一波是去年12月，有人被裁了至今没找到满意的工作，这次人数来的更猛烈了，想谈个好价格就更难了…”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da1a0b256294d25e94bf3d11fa1d19fd.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h1>“CTO不懂技术”，并购成转折点？</h1><p></p><p>&nbsp;</p><p>“从头到尾都是资本运作的公司，一个工程师注册个公司2年市值60亿，为的就是IPO套现。如今财务造假严打，IPO收紧，就原形毕露了。”对于芯华章此次的大规模裁员消息，一位国金证券的投资理财顾问发表了这样的看法。</p><p>&nbsp;</p><p>另一位认证为“芯华章科技股份有限公司员工”的网友则把矛头指向了芯华章的管理层，“CTO酒量很好，人也仗义，可就是不太懂技术。”“CTO不懂技术这还是第一次听。”一位认证为“新思科技员工”的网友评价道。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca76c051463eedc67f8d14f7b272d8d1.jpeg" /></p><p></p><p>&nbsp;</p><p>2022年9月，芯华章收购高性能仿真软件企业“瞬曜电子”，并进行核心技术整合，并购金额没有披露，同时任瞬曜电子创始人傅勇为公司首席技术官（CTO）。</p><p>&nbsp;</p><p>公开资料显示，傅勇曾担任新思科技资深技术总监，主管亚太地区数字验证产品事业部的技术战略与客户支持。2021年，傅勇在离开新思科技中国一年后，创立了瞬曜电子，专注于数字芯片的前端验证领域，还发布了瞬系列RTL高速仿真器（ShunSim）。更早之前，傅勇毕业于清华大学电子工程系，获得了学士和硕士学位，毕业后任职于三星电子、楷登电子（Cadence）和新思科技，在EDA行业工作达25年。</p><p>&nbsp;</p><p>据了解，当时芯华章是对瞬曜电子的知识产权、产品等核心资产进行的收购。并购是EDA企业扩张的常见手段，借助技术与资本的双重力量，在扩宽产品系列的同时还消除了潜在竞争对手。</p><p>&nbsp;</p><p>不少国外的EDA巨头都通过大量并购优秀EDA点工具厂商实现了EDA全流程覆盖，Cadence通过收购Verilog、Silicon&nbsp;Perspective，解决芯片性能验证问题，将1C布局工具和S1分析工具收入囊；而Synopsys在收购Avanti后，成为了EDA史上首家可以提供顶级前后端完整1C设计方案的EDA工具商。</p><p>&nbsp;</p><p>但芯华章对瞬曜电子进行收购后，走向却似乎有所不同。有业内人士这样评价，“花了好大代价并购个寂寞，回头来看，转折点no1。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a499c83171ff5c4f92a1b01f6bd8fb85.jpeg" /></p><p></p><p>&nbsp;</p><p></p><h1>国产EDA厂商的严峻市场困境</h1><p></p><p>&nbsp;</p><p>去年9月18日，在国内EDA开放合作创新组织举办的首届IDAS设计自动化产业峰会“数字逻辑设计与验证领域”专题分论坛上，华为海思半导体平台验证部部长傅晓对芯华章的演讲者抛出了一系列问题：“芯华章目前生产了多少机框？实际上有多少机框被客户采用？又有多少FPGA被成功导入？”</p><p>&nbsp;</p><p>当时，傅晓强调，学术圈或者企业圈沟通有个基本原则，就叫实事求是，中国要把EDA干成，不能吹，不吹才能成事。引发关注和热议后，傅晓在朋友圈内向芯华章致歉，并对芯华章表明了认可。</p><p>&nbsp;</p><p>但这一风波，也侧面反映出国产EDA厂商的发展困境。EDA市场规模有限，头部的Cadence、新思科技、Siemens&nbsp;EDA等厂商经过多年发展，市场地位稳固。相比之下，国产EDA厂商所面临的竞争环境惨烈。</p><p>&nbsp;</p><p>根据赛迪智库统计，2020&nbsp;年国际三大&nbsp;EDA&nbsp;巨头新思科技、铿腾电子和西门子&nbsp;EDA&nbsp;在国内市场占据明显的头部优势，合计占领约&nbsp;80%的市场份额；国产&nbsp;EDA&nbsp;厂商华大九天市占率约&nbsp;6%，处于国内市场第四位。</p><p>&nbsp;</p><p>中国半导体行业协会预测，到2025年中国的EDA市场规模将达到184.9亿元人民币（约合25亿美元），届时将占全球EDA市场的18.1%。但考虑到该市场的大部分份额仍被国外EDA厂商所占据，留给国产EDA厂商的市场空间依然十分有限。国内头部EDA&nbsp;厂商华大九天4月发布的财报显示，其2023年全年营收也只有10.1亿元人民币。</p><p>&nbsp;</p><p>因而，对于还未上市的国产EDA企业来说，未来可能会遭遇更为严峻的融资环境与显著提升的上市门槛，“战略收缩”或是会其应对市场的必要出路之一了。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://maimai.cn/n/content/global-topic?circle_type=9&amp;topic_id=F8W8dHiR">https://maimai.cn/n/content/global-topic?circle_type=9&amp;topic_id=F8W8dHiR</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</id>
            <title>TaD+RAG- 缓解大模型“幻觉”的组合新疗法</title>
            <link>https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YzYmsaitrTJZ1ysPfalk</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 06:18:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TaD, RAG, LLM, 幻觉
<br>
<br>
总结: 本文介绍了京东联合清华大学提出的任务感知解码技术（TaD）和业内解决LLM幻觉问题的最有效系统性方案——检索增强生成技术（RAG）。大语言模型（LLM）在人类对话互动方面表现出色，但幻觉问题成为其落地应用的制约和瓶颈。幻觉问题主要来源于数据、训练和推理过程，针对这些问题提出了一些缓解策略，其中RAG通过引入信息检索过程，增强LLM的生成过程，提高准确性和鲁棒性，降低幻觉。 </div>
                        <hr>
                    
                    <p></p><p></p><p>TaD：任务感知解码技术（Task-aware Decoding，简称 TaD），京东联合清华大学针对大语言模型幻觉问题提出的一项技术，成果收录于 IJCAI2024。</p><p>RAG：检索增强生成技术（Retrieval-augmented Generation，简称 RAG），是业内解决 LLM 幻觉问题最有效的系统性方案。</p><p></p><h1>1. 背景介绍</h1><p></p><p></p><p>近来，以 ChatGPT 为代表的生成式大语言模型（Large Language Model，简称 LLM）掀起了新一轮 AI 热潮，并迅速席卷了整个社会的方方面面。得益于前所未有的模型规模、训练数据，以及引入人类反馈的训练新范式，LLM 在一定程度上具备对人类意图的理解和甄别能力，可实现生动逼真的类人对话互动，其回答的准确率、逻辑性、流畅度都已经无限接近人类水平。此外，LLM 还出现了神奇的“智能涌现”现象，其产生的强大的逻辑推理、智能规划等能力，已逐步应用到智能助理、辅助创作、科研启发等领域。京东在诸多核心业务如 AI 搜索、智能客服、智能导购、创意声称、推荐/广告、风控等场景下，均对 LLM 的落地应用进行了深入探索。这一举措提升了业务效率，增强了用户体验。</p><p></p><p>尽管具备惊艳的类人对话能力，大语言模型的另外一面——不准确性，却逐渐成为其大规模落地的制约和瓶颈。通俗地讲，LLM 生成不准确、误导性或无意义的信息被称为“幻觉”，也就是常说的“胡说八道”。当然也有学者，比如 OpenAI 的 CEO Sam Altman，将 LLM 产生的“幻觉”视为“非凡的创造力”。但是在大多数场景下，模型提供正确回答的能力至关重要，因此幻觉常常被认为是一种缺陷；尤其是在一些对输出内容准确性要求较高的场景下，比如医疗诊断、法律咨询、工业制造、售后客服等，幻觉问题导致的后果往往是灾难性的。</p><p></p><p>本文主要探索针对 LLM 幻觉问题的解决方案。</p><p></p><h1>2. 相关调研</h1><p></p><p></p><p>众所周知，大语言模型的本质依然是语言模型（Language Model，简称 LM），该模型可通过计算句子概率建模自然语言概率分布。具体而言，LM 基于统计对大量语料进行分析，按顺序预测下一个特定字/词的概率。LLM 的主要功能是根据输入文本生成连贯且上下文恰当的回复，即生成与人类语言和写作的模式结构极为一致的文本。注意到，LLM 并不擅长真正理解或传递事实信息。故而其幻觉不可彻底消除。亚利桑那州立大学教授 Subbarao Kambhampati 认为：LLM 所生成的全都是幻觉，只是有时幻觉碰巧和你的现实一致而已。新加坡国立大学计算学院的 Ziwei Xu 和 Sanjay Jain 等也认为 LLM 的幻觉无法完全消除[1]。</p><p></p><p>虽然幻觉问题无法彻底消除，但依然可以进行优化和缓解，业内也有不少相关的探索。有研究[2]总结了 LLM 产生幻觉的三大来源：数据、训练和推理，并给出了对应的缓解策略。</p><p></p><h4>2.1 数据引入的幻觉</h4><p></p><p></p><p>“病从口入”，训练数据是 LLM 的粮食，数据缺陷是使其致幻的一大原因。数据缺陷既包括数据错误、缺失、片面、过期等，也包括由于领域数据不足所导致的模型所捕获的事实知识利用率较低等问题。以下是针对训练数据类幻觉的一些技术方案：</p><p></p><p>数据清洗</p><p></p><p>针对数据相关的幻觉，最直接的方法就是收集更多高质量的事实数据，并进行数据清理。训练数据量越大、质量越高，最终训练得到的 LLM 出现幻觉的可能性就可能越小[3]。但是，训练数据总有一定的覆盖范围和时间边界，不可避免地形成知识边界，单纯从训练数据角度解决幻觉问题，并不是一个高性价比的方案。</p><p>针对“知识边界”问题，有两种主流方案：一种是知识编辑，即直接编辑模型参数弥合知识鸿沟。另一种是检索增强生成（Retrieval-augmented Generation，简称 RAG），保持模型参数不变，引入第三方独立的知识库。</p><p></p><p>知识编辑</p><p></p><p>知识编辑有两种方法：1）编辑模型参数的方法可以细粒度地调整模型的效果，但难以实现知识间泛化能力，且不合理的模型编辑可能会导致模型产生有害或不适当的输出[4]；2）外部干预的方法（不编辑模型参数）对大模型通用能力影响较小，但需要引入一个单独的模块，且需要额外的资源训练这个模块。</p><p>如何保持原始 LLM 能力不受影响的前提下，实现知识的有效更新，是 LLM 研究中的重要挑战[2]。鉴于知识编辑技术会给用户带来潜在风险，无论学术界还是业界都建议使用包含明确知识的方法，比如 RAG。</p><p></p><p>检索增强生成（RAG）</p><p></p><p>RAG 引入信息检索过程，通过第三方数据库中检索相关信息来增强 LLM 的生成过程，从而提高准确性和鲁棒性，降低幻觉。由于接入外部实时动态数据，RAG 在理论上没有知识边界的限制，且无需频繁进行 LLM 的训练，故已经成为 LLM 行业落地最佳实践方案。下图 1 为 RAG 的一个标准实现方案[11]，用户的 Query 首先会经由信息检索模块处理并召回相关文档；随后 RAG 方法将 Prompt、用户 query 和召回文档一起输入 LLM，最终由 LLM 生成最终的答案。</p><p></p><p>图 1. RAG 架构图</p><p><img src="https://static001.geekbang.org/infoq/ea/ea7b73f20dc4c921d356287164b7f45a.png" /></p><p>​</p><p>﻿﻿</p><p>RAG 借助信息检索，引入第三方事实知识，大大缓解了单纯依靠 LLM 生成答案而产生的幻觉，但由 LLM 生成的最终输出仍然有较大概率产生幻觉。因此，缓解 LLM 本身的幻觉，对整个 RAG 意义重大。</p><p></p><h4>2.2 模型训练引入的幻觉</h4><p></p><p></p><p>LLM 的整个训练过程，都可能会引入幻觉。首先，LLM 通常是 transformer 结构的单向语言模型，通过自回归的方式建模目标，天然存在单向表示不足、注意力缺陷[6]、曝光偏差[7]等问题；其次，在文本对齐阶段，无论是监督微调（SFT）还是人类反馈的强化学习（RLHF），都有可能出现有标注数据超出 LLM 知识边界、或者与 LLM 内在知识不一致的问题；这一系列对齐问题很可能放大 LLM 本身的幻觉风险[8]。</p><p></p><p>对于训练过程引入的幻觉，可以通过优化模型结构、注意力机制、训练目标、改进偏好模型等一系列手段进行缓解。但这些技术都缺乏通用性，难以在现有的 LLM 上进行迁移，实用性不高。</p><p></p><h4>2.3 推理过程引入的幻觉</h4><p></p><p></p><p>推理过程引入的幻觉，一方面源自于解码策略的抽样随机性，它与幻觉风险的增加呈正相关，尤其是采样温度升高导致低频 token 被采样的概率提升，进一步加剧了幻觉风险[9]。另一方面，注意力缺陷如上下文注意力不足、Softmax 瓶颈导致的不完美解码都会引入幻觉风险。</p><p></p><p>层对比解码（DoLa）</p><p></p><p>针对推理过程解码策略存在的缺陷，一项具有代表性且较为有效的解决方案是层对比解码（Decoding by Contrasting Layers, 简称 DoLa）[9]。模型可解释性研究发现，在基于 Transformer 的语言模型中，下层 transformer 编码“低级”信息（词性、语法），而上层中包含更加“高级”的信息（事实知识）[10]。DoLa 主要通过强调较上层中的知识相对于下层中的知识的“进步”，减少语言模型的幻觉。具体地，DoLa 通过计算上层与下层之间的 logits 差，获得输出下一个词的概率。这种对比解码方法可放大 LLM 中的事实知识，从而减少幻觉。</p><p></p><p>图 2. DoLa 示意图</p><p><img src="https://static001.geekbang.org/infoq/3c/3cbec3d739fb3141d66fed54dd85980f.png" /></p><p>​</p><p></p><p>上图 2 是 DoLa 的一个简单直观的示例。“Seattle”在所有层上都保持着很高的概率，可能仅仅因为它是一个从语法角度上讲比较合理的答案。当上层通过层对比解码注入更多的事实知识后，正确答案“Olympia”的概率会增加。可见，层对比解码（DoLa）技术可以揭示真正的答案，更好地解码出 LLM 中的事实知识，而无需检索外部知识或进行额外微调。此外，DoLa 还有动态层选择策略，保证最上层和中间层的输出差别尽可能大。</p><p></p><p>可见，DoLa 的核心思想是淡化下层语言/语法知识，尽可能放大事实性知识，但这可能导致生成内容存在语法问题；在实验中还发现 DoLa 会倾向于生成重复的句子，尤其是长上下文推理场景。此外，DoLa 不适用有监督微调，限制了 LLM 的微调优化 。</p><p></p><h1>3. 技术突破</h1><p></p><p></p><p>通过以上分析，RAG 无疑是治疗 LLM 幻觉的一副妙方，它如同 LLM 的一个强大的外挂，让其在处理事实性问题时如虎添翼。但 RAG 的最终输出仍然由 LLM 生成，缓解 LLM 本身的幻觉也极为重要，而目前业内针对 LLM 本身幻觉的技术方案存在成本高、实用落地难、易引入潜在风险等问题。</p><p></p><p>鉴于此，京东零售联合清华大学进行相关探索，提出任务感知解码（Task-aware Decoding，简称 TaD）技术[12]（成果收录于 IJCAI2024），可即插即用地应用到任何 LLM 上，通过对比有监督微调前后的输出，缓解 LLM 本身的幻觉。该方法通用性强，在多种不同 LLM 结构、微调方法、下游任务和数据集上均有效，具有广泛的适用场景。</p><p></p><p>任务感知解码（TaD）技术</p><p></p><p>关于 LLM 知识获取机制的一些研究表明，LLM 的输出并不能总是准确反映它们所拥有的知识，即使一个模型输出错误，它仍然可能拥有正确的知识[13]。此项工作主要探索 LLM 在保留预训练学到的公共知识的同时，如何更好地利用微调过程中习得的下游任务特定领域知识，进而提升其在具体任务中的效果，缓解 LLM 幻觉。</p><p></p><p>TaD 的基本原理如图 3 所示。微调前 LLM 和微调后 LLM 的输出词均为“engage”，但深入探究不难发现其相应的预测概率分布发生了明显的改变，这反映了 LLM 在微调期间试图将其固有知识尽可能地适应下游任务的特定领域知识。具体而言，经过微调，更加符合用户输入要求（“专业的”）的词“catalyze”的预测概率明显增加，而更通用的反映预训练过程习得的知识却不能更好满足下游任务用户需求的词“engage”的预测概率有所降低。TaD 巧妙利用微调后 LLM 与微调前 LLM 的输出概率分布的差异来构建知识向量，得到更贴切的输出词“catalyze”，进而增强 LLM 的输出质量，使其更符合下游任务偏好，改善幻觉。</p><p></p><p>图 3. TaD 原理图</p><p><img src="https://static001.geekbang.org/infoq/af/af2930c906a97052acb628e6d5017497.png" /></p><p></p><p>知识向量</p><p></p><p>为了直观理解 LLM 在微调阶段学习到的特定领域知识，我们引入知识向量的概念，具体如图 4 所示。微调前 LLM 的输出条件概率分布为 pθ，微调后 LLM 的输出条件概率分布为 pϕ。知识向量反应了微调前后 LLM 输出词的条件概率分布变化，也代表着 LLM 的能力从公共知识到下游特定领域知识的适应。基于 TaD 技术构建的知识向量可强化 LLM 微调过程中习得的领域特定知识，进一步改善 LLM 幻觉。</p><p></p><p>图 4. 知识向量</p><p><img src="https://static001.geekbang.org/infoq/c2/c237f091d02fda038009d5b19c39d4ae.png" /></p><p>﻿﻿</p><p>特别地，当微调数据较少时，LLM 的输出条件概率分布远远达不到最终训练目标。在此情形下，TaD 技术增强后的知识向量可以加强知识对下游任务的适应，在训练数据稀缺场景下带来更显著的效果提升。</p><p></p><p>实验结果</p><p></p><p>1）针对不同的 LLM，采用 LoRA、AdapterP 等方式、在不同的任务上进行微调，实验结果如下表 1 和表 2 所示。注意到，TaD 技术均取得了明显的正向效果提升。</p><p></p><p>表 1. Multiple Choices 和 CBQA 任务结果</p><p><img src="https://static001.geekbang.org/infoq/72/7204c1175cee2590cce2caa1f45905f3.png" /></p><p></p><p>表 2. 更具挑战性的推理任务结果</p><p><img src="https://static001.geekbang.org/infoq/d0/d04a0fc99171af777eae622bac9fa3ba.png" /></p><p>​﻿﻿</p><p>2）相比较其他对比解码技术，TaD 技术在绝大部分场景下效果占优，具体如表 3 所示。需要特别强调的一点是，其他技术可能会导致 LLM 效果下降，TaD 未表现上述风险。</p><p></p><p>表 3. 不同对比解码技术结果</p><p><img src="https://static001.geekbang.org/infoq/95/95281634c1b6ffd33e4095ac45d095d8.png" /></p><p>​﻿﻿</p><p>3）针对不同比例的训练样本进行实验，发现一个非常有趣的结果：训练样本越少，TaD 技术带来的收益越大，具体如表 4 所示。因此，即使在有限的训练数据下，TaD 技术也可以将 LLM 引导到正确的方向。由此可见，TaD 技术能够在一定程度上突破训练数据有限情形下 LLM 的效果限制。</p><p></p><p>表 4. 不同数据比例下的结果</p><p><img src="https://static001.geekbang.org/infoq/43/43edb0aa6a8d4c66f408d9e7fec712b9.png" /></p><p>​﻿﻿</p><p>可见，TaD 可以即插即用，适用于不同 LLM、不同微调方法、不同下游任务，突破了训练数据有限的瓶颈，是一项实用且易用的改善 LLM 自身幻觉的技术。</p><p></p><h1>4. 落地案例</h1><p></p><p></p><p>自从以 ChatGPT 为代表的 LLM 诞生之后，针对其应用的探索一直如火如荼，然而其幻觉已然成为限制落地的最大缺陷。综上分析，目前检索增强生成（RAG）+低幻觉的 LLM 是缓解 LLM 幻觉的最佳组合疗法。在京东通用知识问答系统的构建中，我们通过 TaD 技术实现低幻觉的 LLM，系统层面基于 RAG 注入自有事实性知识，具体方案如图 5 所示，最大程度缓解了 LLM 的生成幻觉 。</p><p></p><p>图 5. TaD+RAG 的知识问答系统</p><p><img src="https://static001.geekbang.org/infoq/ef/ef35c2a47c39ab1461fa6248279813fe.png" /></p><p>​</p><p>目前知识问答系统已经接入京东 6000+业务场景，为用户提供准确、高效、便捷的知识性问答，大大节省了运营、运维等人力开销。</p><p></p><h1>5. 思考与展望</h1><p></p><p></p><p>如果 LLM 依然按照语言模型的模式发展，那么其幻觉就无法彻底消除。目前业内还没有一种超脱语言模型范畴，且可以高效完成自然语言相关的任务新的模型结构。因此，缓解 LLM 的生成幻觉，仍然是未来一段时期的探索路径。以下是我们在系统、知识、LLM 三个层面的一些简单的思考，希望能够抛砖引玉。</p><p></p><p>系统层面——RAG+Agent+More 的复杂系统</p><p></p><p>RAG 技术确实在一些常见的自然语言处理任务中发挥出色的作用，尤其是针对简单问题和小型文档集。但是遇到一些复杂的问题和大型文档集时，RAG 技术就显得力不从心。近期有一些研究认为 RAG+Agent 才是未来的趋势[14]，Agent 能够辅助理解并规划复杂的任务。我们认为可能未来的系统可能不仅仅局限于 Agent 和 RAG，可能还要需要多种多样的内外工具调用、长短期记忆模块、自我学习模块......</p><p></p><p>知识层面——与 LLM 深度融合的注入方式</p><p></p><p>任何一个深度模型都会存在知识边界的问题，LLM 也不例外。RAG 通过检索的方式召回外部知识，以 Prompt 的形式送入 LLM 进行最终的理解和生成，一定程度上缓解 LLM 知识边界问题。但是这种知识注入的方式和 LLM 生成的过程是相对割裂的。即便已经召回了正确的知识，LLM 也可能因为本身知识边界问题生成错误的回答。因此探索如何实现外部知识和 LLM 推理的深度融合，或许是未来的一个重要的课题。</p><p></p><p>LLM 层面——低幻觉 LLM</p><p></p><p>LLM 本身的幻觉是问题的根本和瓶颈，我们认为随着 LLM 更广泛的应用，类似 TaD 可缓解 LLM 本身幻觉的探索一定会成为业内的更大的研究热点。</p><p></p><h1>6. 结语</h1><p></p><p></p><p>缓解 LLM 幻觉一定是个复杂的系统问题，我们可以综合不同的技术方案、从多个层级协同去降低 LLM 的幻觉。虽然现有方案无法保证从根本上解决幻觉，但随着不断探索，我们坚信业内终将找到限制 LLM 幻觉的更有效的方案，也期待届时 LLM 相关应用的再次爆发式增长。</p><p></p><p>京东零售一直走在 AI 技术探索的前沿，随着公司在 AI 领域的不断投入和持续深耕，我们相信京东必将产出更多先进实用的技术成果，为行业乃至整个社会带来深远持久的影响。</p><p>﻿</p><p>【参考文献】</p><p>[1] Hallucination is Inevitable: An Innate Limitation of Large Language Models</p><p>[2] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</p><p>[3] Unveiling the Causes of LLM Hallucination and Overcoming LLM Hallucination</p><p>[4] Editing Large Language Models: Problems, Methods, and Opportunities</p><p>[5] ACL 2023 Tutorial: Retrieval-based Language Models and Applications</p><p>[6] Theoretical Limitations of Self-Attention in Neural Sequence Models</p><p>[7] Sequence level training with recurrent neural networks.</p><p>[8] Discovering language model behaviors with model-written evaluations</p><p>[9] Dola: Decoding by contrasting layers improves factuality in large language models</p><p>[10] Bert rediscovers the classical nlp pipeline</p><p>[11] Retrieval-Augmented Generation for Large Language Models: A Survey</p><p>[12] TaD: A Plug-and-Play Task-Aware Decoding Method toBetter Adapt LLM on Downstream Tasks</p><p>[13] Inference-time intervention: Eliciting truthful answers from a language model</p><p>[14] Beyond RAG: Building Advanced Context-Augmented LLM Applications</p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</id>
            <title>首个专为半导体行业设计的开源大模型 SemiKong 问世</title>
            <link>https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kUAMTF00sAdNhsvulr7H</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 01:37:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Aitomatic, SemiKong, AI Alliance, 半导体行业
<br>
<br>
总结: Aitomatic推出了SemiKong，这是世界上第一个专为半导体行业设计的开源AI大型语言模型。SemiKong旨在解决半导体行业的挑战，利用领域知识和专业知识进行训练，有望降低半导体生产成本，推动行业创新。通过AI联盟的支持，SemiKong有望重新定义半导体制造业，为行业带来巨大的飞跃。 </div>
                        <hr>
                    
                    <p></p><p>7 月 10 日，国外初创公司 Aitomatic 宣布推出 SemiKong。这是世界上第一个专为半导体行业设计的开源 AI 大型语言模型（LLM）。它旨在通过将特定领域的知识纳入模型来解决半导体行业面临的一些挑战，例如有关半导体器件和工艺的物理和化学问题。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bc89c3afa90c31f5c1ef0a8aee82dcc4.png" /></p><p></p><p>SemiKong 由人工智能联盟（AI Alliance）成员合作研发。AI 联盟成立于 2023 年，致力于构建、支持和倡导整个 AI 技术领域的开放式创新，包括软件、数据和模型、安全、安保和信任、工具、评估、硬件、教育、开放科学和宣传。</p><p></p><p>SemiKong 基于联盟成员 Meta 开源的 Llama3 模型，利用了包括 Tokyo Electron 在内的领先半导体公司和 FPT Software 等 AI 专家的专业知识。IBM 研究院 AI 开放创新负责人 Anthony Annunziata 强调，“SemiKong DRAFT v0.6 的诞生表明，汇集不同的专业知识能推动半导体制造等关键行业的重大进步。”</p><p></p><p>SemiKong 的训练过程主要分为 3 个主要阶段：预训练领域知识——自我微调（指令数据集）——合并和量化。从放出的代码权重，可以看出 SemiKong 有 8B 的参数。它在准确性、相关性和对半导体工艺的理解方面表现出了显著的进步。</p><p></p><p>Aitomatic 表示，即使是其较小版本，在特定领域的应用中也常常超越较大的通用模型，从而有可能加速整个半导体价值链的创新并降低成本。并且，它也为那些打造适合自身的专有模型的芯片公司提供了一个有价值的基座。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/16/16ef131c1717c78c97feb525ac103847.png" /></p><p></p><p>随着 SemiKong 降低半导体生产成本，消费者可以在未来几年内以更低的价格看到功能更强大的智能手机、笔记本电脑和智能家居设备。SemiKong 于 2024 年 7 月 9 日起在 HuggingFace 和 GitHub 上提供下载。下一个更强大的版本计划于 2024 年 12 月推出，预计 2024 年 9 月将推出首批特定工艺型号。</p><p></p><p>开源地址：<a href="https://github.com/aitomatic/semikong">https://github.com/aitomatic/semikong</a>"</p><p></p><p>SemiKong 项目的领导者， Aitomatic 首席执行官 Christopher Nguyen 表示：“SemiKong 将重新定义半导体制造业。这种开放式创新模式由人工智能联盟提供支持，利用集体专业知识应对行业特定挑战。在 Aitomatic，我们正在使用 SemiKong 创建领域特定 AI 智能体，以前所未有的效率解决复杂的制造问题。”</p><p></p><p>Tokyo Electron 高级专家、半导体行业模型的早期提出者 Daisuke Oku 补充道：“SemiKong 是半导体开源 AI 的一个令人激动的开始。Aitomatic 的创新方法有可能为我们的行业带来巨大的飞跃。”</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.prnewswire.com/news-releases/aitomatic-unveils-semikong-ai-breakthrough-set-to-reshape-500b-semiconductor-industry-302194215.html">https://www.prnewswire.com/news-releases/aitomatic-unveils-semikong-ai-breakthrough-set-to-reshape-500b-semiconductor-industry-302194215.html</a>"</p><p></p><p><a href="https://www.semikong.ai/">https://www.semikong.ai/</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</id>
            <title>“萝卜快跑”被曝是真人在屏幕前操作；阿里云宣布与月之暗面“联姻”；去哪儿宣布每周两天自选办公地 ｜AI 周报</title>
            <link>https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tJtBpzK2zf8KepMuoYkL</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jul 2024 01:25:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 萝卜快跑, 无人驾驶, 安全问题, 人为干预
<br>
<br>
总结: 百度旗下自动驾驶出行服务平台“萝卜快跑”在全国多个城市开展无人自动驾驶服务，订单量激增，但面临安全与技术等问题，甚至有人为干预的传闻。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>网友称萝卜快跑其实有驾驶员人为干预；去哪儿员工每周两天可自主选择办公地点；阿里云宣布与月之暗面“联姻”；OpenAI 绝密项目「草莓」首次曝光；AMD 收购欧洲最大私人 AI 实验室 Silo AI。</blockquote><p></p><p></p><p></p><h2>行业热点</h2><p></p><p></p><p></p><h5>萝卜快跑订单疯涨，无人驾驶时代真的来了？</h5><p></p><p></p><p>7 月 10 日，百度旗下自动驾驶出行服务平台“萝卜快跑”登上微博热搜榜首。据了解，萝卜快跑已在全国 11 个城市开放载人测试运营服务，在武汉、重庆、深圳、上海、北京等城市开展全无人自动驾驶出行服务与测试。</p><p></p><p>媒体报道称，随着百台无人车的投入运营，“萝卜快跑”在武汉市全无人订单量也迎来了爆发式增长，单日单车峰值超 20 单。数据显示，萝卜快跑 APP 用户满意度评价达 4.9 分，其中 5 分满分好评占比高达 94.19%。此外，网络传言称，萝卜快跑已在武汉投放 1000 辆无人车，进而引发对网约车司机、出租车司机就业市场的深刻担忧。</p><p></p><p>目前，萝卜快跑还面临着安全与技术等方面的问题。有武汉网友 7 月 7 日下午在短视频平台发布视频，称百度旗下的自动驾驶出行服务平台“萝卜快跑”无人驾驶出租车在武汉街头与行人相撞，视频中可以看到一个行人躺在出租车前，交警正在现场，事故造成了部分车辆拥堵。这件事故引出了一个亟待解决的问题：无人驾驶车应该如何定责。目前，我国还没有建立一套完善的全国性法规，只有地方规定。此外，在长江网武汉市民留言板上，有关于“萝卜快跑”的留言，目前已累计达到 324 条。留言板多个内容显示，萝卜快跑 Robotaxi 在道路上运营期间，出现车辆在绿灯状态下停滞不前、红灯时冲入路口中央、转弯时卡顿不动等情况，并引发交通拥堵现象，对市民出行造成了一定影响。</p><p></p><p>针对此类现象，武汉经开区管委会回应称：“确认涉诉车辆为自动驾驶测试车，正在调试中。开发区交通大队将定期与公司负责人沟通和反映问题，确保道路安全。”另外，近日还有网友在社交平台称，无人驾驶的萝卜快跑其实有驾驶员人为干预。网传图片显示，在萝卜快跑汽车机器人智控中心，有真人坐在带方向盘的屏幕前操作。百度方面截至发稿没有回应。</p><p></p><p>据网信永川公众号 2023 年 7 月发布的内容，位于永川区大数据产业园的百度无人驾驶实验基地内，有云代驾安全员在 5G 云代驾舱进行远程实时控制，通过高带宽、低时延的 5G 网络，从屏幕组上观察汽车周围 360°状况，并利用方向盘、档把、脚踏板等控制器驾驶无人车辆。5G 云代驾的意义在于，在无人车没有安全员的情况下，当无人车出现解决不了的问题时，云端安全员可以帮助其远程脱困。</p><p></p><p></p><h5>三星爆发大规模罢工，韩媒：半导体部门员工是罢工主力</h5><p></p><p></p><p>据报道，韩国三星电子旗下最大工会“全国三星电子工会”于 8 日上午开始在京畿道华城市三星电子华城工厂正门前举行罢工，计划持续 3 天。该工会会员总数为 3 万人，约占三星电子员工总数（12.5 万人）的 24%。据悉，在 8 日的罢工中，工会推算有 4000 至 5000 人参与，三星公司和警方则估计有 3000 人参加。</p><p></p><p>韩媒称，这是三星电子成立 55 年来首次爆发大规模罢工。此前在 6 月初，工会部分成员曾利用休年假的形式罢工 1 天。工会此次提出的主要诉求有：全体工会成员薪酬上调、改变奖金标准、公司履行带薪休假承诺，以及对因罢工导致的工资损失进行补偿等。工会主席在接受采访时还表示，公司不透明的奖金计算方式，导致员工对自身利益的不确定性增加；若公司在 10 日前未拿出解决方案，工会将于 15 日起进行第二阶段的罢工。</p><p></p><p>韩国 SBS 电视台称，半导体部门的员工是此次罢工的主力。三星公司称半导体生产线的运行没有受到重大影响，但《东亚日报》报道称，即使许多生产线实现了自动化，操作这些生产线的重要人员也很难替换。半导体生产线一旦停止运转，恢复生产需要耗费大量时间和成本。</p><p></p><p></p><h5>腾讯全员邮件宣布调薪：员工月工资增加 3200 元等</h5><p></p><p></p><p>7 月 10 日，腾讯内部向全员发布邮件称，将调整内部的薪酬福利政策，对薪酬结构做出调整。</p><p></p><p>校招生的房补从每月 4000 元调整为按 15 个月发放，并将其纳入月薪基数中。调整后，员工每月基本工资增加 3200 元，多出来的三个月将在年终奖一起发放。根据资料，腾讯公司给校招生提供的房补标准为每月 4000 元（北上广深地区为 2000 元），三年共计 14.4 万元。员工服务奖（13 薪）从年底发放调整为平摊到 12 个月，并加入月薪基数中。腾讯邮件中称，这两个举措旨在帮助大家在更高、更稳定的月收入基础上更安心地安排工作与生活。相关调整于 2024 年 7 月 1 日起生效，8 月 5 日的发薪中开始体现。</p><p></p><p></p><h5>大模型人才紧缺，字节跳动加速争夺全球高校顶尖技术人才</h5><p></p><p></p><p>近日，字节跳动“筋斗云人才计划”启动。该计划是字节跳动面向优秀校园技术人才推出的专项招聘，意图在全球范围内，吸引和招募有志于用技术创造突破性价值的顶尖学生。</p><p></p><p>据悉，本次招聘涵盖 AI 应用、搜索、推荐、广告、AI for Science、AI Safety、机器人、隐私与安全、硬件、视频架构、工程架构等技术领域。招聘的目标群体是 2024 年 9 月 -2025 年 8 月毕业的博士群体，重点针对有亮眼学术成果、拥有顶会顶刊论文或专利的学术达人；有丰富的大赛经历，在国际知名竞赛中取得优异成绩的竞赛达人；或有极强的实践能力，参与过重大项目，擅长解决难题的实战达人。</p><p></p><p>在大模型相关技术人才招聘上，字节跳动是国内互联网大厂最积极的公司。据一位大模型行业人士透露，今年字节跳动的 AI 人才招聘规模最大。脉脉高聘人才智库数据印证了这一信息：今年上半年，字节跳动位列新发人工智能岗位最多的企业。从招聘指数上看，字节跳动以 9.53 位居第一，大幅领先于小红书（7.96）、蚂蚁集团（5.84）、美团（4.86）、腾讯（2.48）等互联网大厂。</p><p></p><p></p><h5>去哪儿员工每周三、周五可自主选择办公地点</h5><p></p><p></p><p>7 月 9 日，去哪儿 CEO 陈刚发全员信宣布，从 7 月 15 日起，每周三、周五，员工可以灵活选择办公地点。陈刚在信中强调，员工按规定混合办公，“无需任何申请审批”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/09/090608dcb8e70b99b8a1aedee53a7e7f.jpeg" /></p><p></p><p>据了解，混合办公的适用人员范围以入职 6 个月以上的标准工时正式员工为主。</p><p></p><p>去年 10 月，去哪儿开始了为期 9 个月的混合办公试验。回收数据显示，员工对混合办公的各个维度反馈正面 —— 超过九成的员工认为混合办公后幸福感有明显提升，员工主动离职率在混合办公后下降了三成。</p><p></p><p>去哪儿 COO（首席运营官）刘连春表示，“混合办公没有让公司业绩变坏，并且显著提升了员工的幸福度。那这件事情公司何乐而不为呢？”</p><p></p><p></p><h5>微软要求中国区员工必须使用 iPhone？微软回应</h5><p></p><p></p><p>7 月 9 日消息，据媒体报道，微软中国员工已被告知，登录公司系统时必须使用 iPhone 进行身份验证。从 9 月起，将禁止使用 Android 智能手机作为多因素身份验证设备。</p><p></p><p>据介绍，此举属于微软全球安全未来计划的一部分，将影响中国大陆的数百名员工，旨在确保所有员工都使用微软 Authenticator 密码管理器和 Identity Pass 身份验证应用。另有消息显示，由于部分中国安卓设备不支持谷歌服务，微软将为受影响员工提供 iPhone15 作为工作手机。</p><p></p><p>一位微软发言人回应表示：「Microsoft Authenticator 和 Identity Pass 应用程序已正式在 Apple Store 和 Google Play Store 上架。我们希望为员工提供访问这些必要应用程序的途径，由于本地区无法使用 Google 移动服务，我们即向员工提供了例如 iOS 设备的选择。」</p><p></p><p>对此，不少网友称，若是能配发工作机就没问题。但若强制要求员工自行购买，则“不能接受”。</p><p></p><p></p><h5>阿里云宣布与月之暗面“联姻”：帮 Kimi 技术突破</h5><p></p><p></p><p>7 月 8 日，阿里云官宣两位新“代言人”——月之暗面科技有限公司创始人杨植麟和智联招聘集团总裁张月佳。</p><p></p><p>这是月之暗面首次公开与阿里云的合作情况。信息显示，阿里云的算力和大模型服务平台，助力月之暗面提升模型推理效率，加速 Kimi 智能助手实现技术突破。此外，智联招聘集团的大模型应用，也基于阿里云实现快速部署和上线支持。</p><p></p><p></p><h5>AMD 重砸 6.65 亿美元收购欧洲最大私人 AI 实验室 Silo AI</h5><p></p><p></p><p>AMD 宣布以价值约 6.65 亿美元的全先进交易价值收购欧洲最大的私人 AI 实验室 Silo AI。该收购案预计在 2024 年下半年完成。</p><p></p><p>收购完成后，Silo AI 首席执行官兼联合创始人 Peter Sarlin 将继续领导 Silo AI 团队，向 AMD 高级副总裁 Vamsi Boppana 汇报工作。</p><p></p><p>据了解，Silo AI 总部位于芬兰赫尔辛基，业务遍及欧洲和北美，专注于端到端 AI 驱动解决方案，帮助客户快速轻松地将 AI 集成到其产品、服务和运营中。他们的工作涉及不同的市场，客户包括安联、飞利浦、劳斯莱斯和联合利华。除了 SiloGen 模型平台外，Silo AI 还在 AMD 平台上创建了最先进的开源多语言 LLM，例如 Poro 和 Viking。</p><p></p><p>AMD 在新闻稿中表示，此次收购代表该公司基于开放标准并与全球 AI 生态系统建立强有力的合作伙伴关系，并提供端到端 AI 解决方案的战略又迈出了重要一步。Silo AI 团队由世界一流的 AI 科学家和工程师组成，他们拥有丰富的经验，为云、嵌入式和终端计算市场的领先企业开发量身定制的 AI 模型、平台和解决方案。</p><p></p><p></p><h5>多方监管增压，微软放弃参与 OpenAI 董事会</h5><p></p><p></p><p>7 月 10 日，据媒体报道，随着欧美监管机构加强对人工智能市场的反垄断审查，微软公司决定放弃在美国开放人工智能研究中心 (OpenAI) 董事会中的观察员席位。</p><p></p><p>微软 9 日致函 OpenAI 说明上述决定，并解释称，OpenAI 自去年发生董事会人事震荡以来，经营管理已有改善，因此不再需要微软参与。微软选择放弃观察员席位，决定“立即生效”。</p><p></p><p>据报道，去年 OpenAI 首席执行官萨姆·奥尔特曼“离职又复职”风波过后，微软在 OpenAI 董事会获任无投票权观察员。据此前报道，微软支持并短暂聘用过奥尔特曼。</p><p></p><p></p><h5>小红书被曝获 DST 投资，估值 170 亿美元</h5><p></p><p></p><p>7 月 11 日消息，小红书获得了风险投资公司 DST Global 的支持。三位知情人士透露，小红书在最近几周进行了股份出售，公司估值达到 170 亿美元。</p><p></p><p>DST Global 曾投资过 Facebook，并与红杉中国一起参与了小红书这一轮投资，红杉中国增加了其现有股份。此外高瓴资本、博裕资本和中信资本也进行了跟投。</p><p></p><p>此前有消息称，小红书在 2023 年首次实现盈利。据四位知情人士透露，小红书去年净利润达 5 亿美元，营收达 37 亿美元。</p><p></p><p></p><h2>大模型一周大事</h2><p></p><p></p><p></p><h4>大模型发布</h4><p></p><p></p><p></p><h5>OpenAI 绝密项目「草莓」首次曝光，内部人士曾称其可能威胁人类</h5><p></p><p></p><p>7 月 13 日，据外媒报道，OpenAI 内部正在一个代号为「草莓（Strawberry）」的项目中开发一种新的人工智能模型。该项目的细节此前从未被报道过，而 OpenAI 正努力证明其提供的各类模型能够提供高级推理能力。</p><p></p><p>当被问及上述所说的草莓技术时，OpenAI 的发言人在一份声明中表示：“我们希望自身 AI 模型能够像我们（人类）一样看待和理解世界。持续研究新的 AI 能力是业界的常见做法，大家都相信这些系统的推理能力会随着时间的推移而提高。”</p><p></p><p>尽管发言人并未直接回应有关“草莓”项目的问题，但媒体报道指出，该项目之前被称为 Q*，而 Q*正是去年导致 OpenAI CEO 被意外解雇的重要导火索。</p><p></p><p>OpenAI 的内部人士曾向董事会发出警告，称 Q* 的重大发现可能对全人类构成威胁。</p><p></p><p>媒体推测，Q* 可能具备 GPT-4 所缺乏的基础数学能力，这可能意味着它具有与人类智能相媲美的推理能力。而这可能标志着 OpenAI 在实现其 AGI 目标方面迈出了重要一步。</p><p></p><p>蚂蚁集团开源 EchoMimic：支持为人像照片对口型、生成肖像动画视频</p><p></p><p>近日，蚂蚁集团推出了一项开源项目——EchoMimic，这款 AI 工具能够根据声音内容，为照片中的人物创建逼真的口型同步动画。</p><p></p><p>EchoMimic 具备较高的稳定性和自然度，通过融合音频和面部标志点（面部关键特征和结构，通常位于眼、鼻、嘴等位置）的特征，可生成更符合真实面部运动和表情变化的视频。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f920b20ecea13d28d4a7d6483eec24e1.png" /></p><p></p><p>EchoMimic 的技术核心在于其能够精确捕捉音频信号和面部特征之间的关联，并以此为基础生成动画。在训练过程中，EchoMimic 采用了先进的数据融合技术，确保了音频和面部特征的有效整合，从而提高了动画的稳定性和自然度。</p><p></p><p>经过与多个公共数据集和自收集数据集中的替代算法进行的全面比较，EchoMimic 在定量和定性评估方面均展现出卓越的性能。这一点在 EchoMimic 项目页面上的可视化效果中得到了充分体现。</p><p></p><p></p><h5>腾讯开源 web 端地图组件库 tlbs-map</h5><p></p><p></p><p>7 月 11 日，腾讯开源了其基于腾讯位置服务 JavaScript API 封装的地图组件库 —— tlbs-map，用于在网页上绘制地图，并在地图上绘制点、线、面、热力图等效果。它支持 Vue2、Vue3、React 等主流技术栈，可以帮助开发者降低地图开发的成本。</p><p></p><p>据官方介绍，tlbs-map 封装腾讯地图 API 为响应式组件，无需关心复杂的地图 API，只需要操作数据即可；同时，组件提供地图和图层实例，用户可编写自定义组件或直接调用地图 API 满足定制化需求。</p><p></p><p>为了方便开发者使用，tlbs-map 还提供了详尽的组件使用文档和示例代码，可以帮助开发者轻松上手，快速开发。</p><p></p><p></p><h5>智谱 AI 开源推出视频理解模型 CogVLM2-Video</h5><p></p><p></p><p>7 月 12 日，智谱 AI 提出了一种基于视觉模型的自动时间定位数据构建方法，生成了 3 万条与时间相关的视频问答数据。基于这个新数据集和现有的开放领域问答数据，引入了多帧视频图像和时间戳作为编码器输入，训练了一种新的视频理解模型—CogVLM2-Video。</p><p></p><p>智谱 AI 表示，目前视频理解的主流方法使模型失去了时间感知能力，无法准确地将视频帧与精确的时间戳关联起来。因此，模型缺乏时间定位、时间戳检测和总结关键时刻的能力。为了解决这些问题，团队提出了 CogVLM2-Video，这是基于 CogVLM2 图像理解模型的扩展视频模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fd0403bf707372f114caf13973410f00.jpeg" /></p><p></p><p>该模型不仅在开放域问答中实现了先进的性能，还能感知视频中的时间戳信息，从而实现时间定位和相关问答。</p><p></p><p>具体来说，这种方法就是从输入视频片段中提取帧，并为其注释时间戳信息，使后续的语言模型能够准确知道每一帧在原视频中对应的确切时间。</p><p></p><p></p><h5>几分钟生成四维内容，还能控制运动效果：北大、密歇根提出 DG4D</h5><p></p><p></p><p>近期，商汤科技 - 南洋理工大学联合 AI 研究中心 S-Lab ，上海人工智能实验室，北京大学与密歇根大学联合提出 DreamGaussian4D（DG4D），通过结合空间变换的显式建模与静态 3D Gaussian Splatting（GS）技术实现高效四维内容生成。</p><p></p><p>据悉，四维内容生成近来取得了显著进展，但是现有方法存在优化时间长、运动控制能力差、细节质量低等问题。DG4D 提出了一个包含两个主要模块的整体框架：1）图像到 4D GS ；团队使用 DreamGaussianHD 生成静态 3D GS，接着基于 HexPlane 生成基于高斯形变的动态生成；2）视频到视频纹理细化 ；团队通过细化生成 UV 空间纹理映射，并通过使用预训练的图像到视频扩散模型增强其时间一致性。</p><p></p><p>值得注意的是，DG4D 将四维内容生成的优化时间从几小时缩短到几分钟，允许视觉上控制生成的三维运动，并支持生成可以在三维引擎中真实渲染的动画网格模型。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f66fd24fd90c1c7d1201310546cfa29.gif" /></p><p></p><p></p><h4>企业应用</h4><p></p><p></p><p>7 月 10 日，谷歌宣布将于本月底向所有谷歌账号用户开放「暗网报告」功能，旨在帮助用户快速了解网络上发生的个人数据泄露事件，并提供相关漏洞信息的搜索服务。7 月 10 日，阿里推出专为科研人员、高校教师和学生、职场人士研发的大模型应用产品心流，其产品定位为用户的 AI 搜索助手，提供智能搜索、知识问答、智能阅读、辅助创作等能力。7 月 10 日，夸克宣布升级“超级搜索框”，推出以 AI 搜索为中心的一站式 AI 服务，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。7 月 11 日，三星表示将在今年推出基于自己人工智能（AI）模型的升级版语音助手 Bixby，据悉，这次 Bixby 的升级是三星在其设备套件上推广人工智能功能的一部分。7 月 12 日，粉笔发布了基于首个专注于职教行业的垂域大模型 AI 产品——粉笔 AI 老师 “粉笔头”，旨在让 AI 帮助老师化身“高效能人士”，向学员提供更有针对性的服务。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</id>
            <title>大模型产品琳琅满目，企业应该如何选择？</title>
            <link>https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IpgZP8CO4h8EwlI710fv</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 08:54:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 大模型, 企业, C 端
<br>
<br>
总结: AI 和大模型在企业应用中的重要性日益凸显，企业需要考虑如何有效融入大模型到业务中，尤其在面向C端应用时更应注重用户体验。与此同时，企业在选择大模型时需要深入理解应用场景，并逐步推广。在大模型的商业化和投资方面，企业需要考虑未来的可替代性，以做出合理的选择。 </div>
                        <hr>
                    
                    <p></p><p>AI 和大模型方兴未艾，我们每天都在看到和尝试不同版本、不同品牌的大模型产品，它们的能力各不相同。无论是个人还是企业，都在思考如何尽早地参与进来到大模型的浪潮当中来。</p><p></p><p>目前，一些先锋企业已经将 AI 和大模型融入到他们的日常业务和产品中，并取得了不错的效果。但更多企业仍处于观望或迷茫状态。在有限的预算内，企业要怎样进行 AI 和大模型的商业化或投资？该选择怎样的大模型融入业务？带着这些问题，InfoQ《极客有约》特别邀请了广东 CIO 联盟会长、前海尔集团 CIO 李洋老师和北京中关村科金公司 CTO 李智伟老师，与 InfoQ 社区编辑张凯峰一同探讨企业如何在众多大产业和大模型产品中做出合理的选择。对话内容部分亮点如下：</p><p></p><p>● 企业开发大模型应用时，应该更多地考虑用户体验；</p><p></p><p>● 企业需要对应用场景有深入理解，并从试点开始逐步推广；</p><p></p><p>● 对于面向 C 端的应用，“+AI”是个不错的选择；</p><p></p><p>● 企业在进行系统建设时，必须考虑到未来的可替代性。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p></p><p><a href="https://www.infoq.cn/video/0KjL5Et9SFyJ6Yrryyqf">https://www.infoq.cn/video/0KjL5Et9SFyJ6Yrryyqf</a>"</p><p></p><p></p><blockquote>在 8 月 18-19 日即将举办的 AICon 上海站，我们设置了「大模型数据集构建及评测技术落地」专题，本专题将深入探讨大模型的需求分析与数据收集、数据清洗与增强、模型评测与优化，以及技术落地与维护等关键方向。目前大会 9 折购票优惠中，感兴趣的同学请锁定大会官网：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p></p><p></p><h2>To C 与 To B 场景和市场需求差异</h2><p></p><p></p><p>张凯峰：在 To C 和 To B 场景中，使用 AI 和大模型有什么区别？或者当企业负责人考虑将 AI 引入自己的企业时，通常会考虑哪些方面？</p><p></p><p>李智伟 ：通过查看最近 ChatGPT 一周的数据，可以发现其用户活跃数量超过了一亿。虽然这一数字很高，但大部分用户使用的都是 To C 的一些应用程序。国内的一些应用程序用户数量也很多，从用户教育角度来看，进展比较快。C 端应用程序的发展将会非常迅速。目前广泛使用的 agent 平台或 model builder 平台，都面向企业提供服务，提供公共云服务或者私有化服务。国内大部分公有云上的托管服务都由个人或小微企业进行使用。国内比较好的头部企业，其训练的模型约有 1.3 万个。</p><p></p><p>同时，通过查看今年 1 到 6 月份的公开招标网数据，我们可以看到大企业招标的情况。今年上半年到 6 月中旬，整个公开招标数量约为 234 件。其中，60% 以上的项目来自央国企。预计今年大企业客户对大模型的商业化使用将加速。</p><p></p><p>从 C 端来看，很多客户已经理解了大部分内容，但 B 端的进展仍然处于早期阶段。</p><p></p><p>李洋：从目前的趋势来看，人工智能是一个新质生产力，是工业革命的一部分。从国际上来看，人工智能的浪潮也比以往席卷得更快。其原因在于，它是由 C 端发起的。目前可以感知到的是，要把科技的生产力提高民众的感知度。对于企业来说，可能包括员工、客户以及上下游。C 端这种蜂拥而来的趋势就奠定了这次人工智能浪潮会高于前几次的基础。</p><p></p><p>对于 To B，我认为应该是未来科技革命所产生的生产力要兑现的一个非常重要的路径。目前可以看到的很多一二级市场的投资，对 C 端的投资还在逐渐增长。但如果缺乏一定的杀手级应用，甚至没有持续的宣传和科技元素的不断注入，这种热度很快就会退去。</p><p></p><p>对于 OpenAI 或国内许多做大模型的企业，无论是自研还是开源，要在 C 端实现商业变现都很难。因此，在 C 端巩固之后，随着大模型的成熟，To B 的发展应该会逐渐加速，但未来是否一定会发展成大模型还有待观察。</p><p></p><p>张凯峰：可见，C 端市场和 B 端市场所面临的情况、消费习惯以及背后的经济投入都是完全不同的。对于企业来说，AI 和大模型的应用还处于初级阶段。是否是因为满足 C 端服务更容易，但企业侧复杂的业务需求和市场竞争等因素，导致企业在接纳 AI 和大模型方面比 C 端更困难？这将具体表现在哪些方面呢？</p><p></p><p>李智伟：我认为这个问题与大模型的能力有关。大语言模型的技术能力可能更多地体现在知识的理解和生成方面更加成熟，因此在构建 C 端应用的场景中，它的融入是比较快速的。</p><p></p><p>对于知识的获取，大模型本身也采用了推荐的方式来提供服务。这种方式的技术更加成熟，能够更好地与用户交互。许多类似的 APP 或个人助手都提供了知识获取的功能。Perplexity AI 和国内的一些创新者正在开发类似搜索的应用，并且致力于提升 C 端用户的体验。当它面向 B 端企业渗透时，我们需要考虑其商业化能力。目前，商业能力主要集中于互联网、教育、金融和政企服务等行业。这些行业有一个共同点，即服务于大量 C 端用户。</p><p></p><p>面向个人用户的体验一定会延伸到企业端的员工使用中。在当下这一波浪潮中，当企业开发大模型应用时，应该更多地考虑用户体验。这也是我们一直致力于做的事情，因为我们是一家传统的营销和客户服务类产品公司，我们基本上都在开发交互类产品。</p><p></p><p>我们之所以非常重视大模型技术，是因为我们认为这项技术实际上是对整个交互体验的颠覆性升级，这是一个未来的巨大机会市场。因此，我们基本上也是将 C 端的体验产品能力应用于 B 端，以实现更深入的发展。</p><p></p><p>李洋：总的来说，C 端和 B 端的触点不太一样。以我个人为例，作为一个纯粹的 C 端用户，我对新科技产品的需求更多关注于体验感、科技感以及方便易用等方面。这也包括了一些家居、生活和工作的便利性，这些方面可以归为 B2C 类别。而从 B 端的角度来看，我们可以将大模型的应用或 AI 战略应用于整个企业中。特别是在后疫情时代，我们需要降低成本、提高效率，甚至创新商业模式，寻找新的利润增长点。因此，在企业内部，对于大模型或新技术的使用，其想法、构建和步骤可能会有所不同。</p><p></p><p></p><h2>大模型的选择与匹配</h2><p></p><p></p><p>张凯峰：通常我们会采用哪些方法来帮助企业识别在某些业务或方向上可以开始选择大模型，并与自己的业务需求相匹配。您们是否听说过一些成功或有待改进的例子，以及它们是如何操作的？</p><p></p><p>李智伟：我想先分享一下我们在做企业应用时遇到的问题。在过去一年多的时间里，很多企业的决策者都在问我们如何选择。实际上，我们并没有给出明确的答案。我认为这是一个逐步认知和迭代的过程，与企业构建的业务场景密切相关。</p><p></p><p>但是这次的情况有所不同。在传统 IT 系统中，我们通常以功能性为主导，根据客户需求构建系统。但是现在，由于整个 IT 企业架构的变化，核心变成了一个模型，我们需要将之与客户的业务深度融合，这无疑是一个很大的挑战。目前我们面临的冲突点是，很多企业出于自身发展或国家要求，会积极与厂商合作，很多场景都会进入。但是因为我们不熟悉所有场景，用户有时会受到限制。</p><p></p><p>企业需要做几个部分的工作。如果我们将这看作是一个流程，我可以提供一些具体的建议。例如，通常我们会先梳理功能性需求，但现在做法不同了。我会告诉客户，首先要进行认知对齐。如何让不同的人对大模型的认知保持一致呢？首先，参与项目的人员需要具备大模型的基本原理和能力；其次，客户方也需要有懂得大模型应用的专业人士。</p><p></p><p>其次，我们需要选择一些小的场景作为试点，以便快速响应市场变化。对于供应商来说，他们需要拥有敏捷的工具链和 demo 系统来帮助他们进行试点。去年，我们建立了一个工具链平台，并在官网上开通了线上 demo 系统。客户可以在了解系统之前先进行试用，确保能够接受它的外观和效果。试点是一个双方共创的过程，试点结束后，需要进行效果评估，然后双方再对产品进行规划并分段实施，最后总结反馈。大模型应用更注重端到端效果的优化。企业需要对应用场景有深入理解，并从试点开始逐步推广。从这个角度看，市场上可供选择的选项并不多，对于企业来说，虽然我们正在努力加速商业化，但更合理的是要看到其中的节奏。</p><p></p><p>李洋：在企业中使用 AI 和大模型的切入点比较多。建议企业在做这方面时，先确认需求，再定义相应的工具。在我的数字化转型工作中，我把它分为延产供销服务、运营风控等方面。不同行业的侧重点会有所不同。</p><p></p><p>当然，不仅限于 AI，还有区块链、云计算、大数据等技术，它们与我们所说的业务数字化层面和流程有关，以及我们所说的痛点或难点，哪些可以使用大模型或 AI 来解决？从目前的情况来看，我非常同意李总的观点。现在的问题可能是，由于大模型的火爆程度超出了某些企业的承受范围，导致我们的应用目标本末倒置。我们不应该拿着锤子去找钉子，而是应该根据钉子的特点选择合适的工具，包括大模型。</p><p></p><p>今天我还与一些企业进行了交流，他们认为，传统的机器学习、简单的规则和深度学习的神经网络也可以解决问题，不一定非要使用高量级的大模型，特别是那些对算力和数据要求很高、成本也很高的模型。因此，我认为我们应该从业务数字化和智能化的方向出发，进行全面规划，然后逐一比较，看看哪些问题可以使用人工智能工具来解决。在使用人工智能工具时也必须考虑成本、效率和效益等核心指标。</p><p></p><p>李智伟：对于这个话题，市场上有不同的提法，如“+AI”或“AI+”。“+AI”是指在现有能力的基础上添加相应的能力，而“AI+”则指下一代能力系统。我认为，对于面向 C 端的应用，加入 AI 是个不错的选择。对于 B 端应用而言，企业通常需要考虑如何重新利用现有的 IT 资产，使用 AI 原生技术的成本很高。</p><p></p><p>对于初创公司来说，重新构建企业架构的成本是难以承受的。因此，我们更倾向于鼓励企业采用 AI 技术来增强自身能力，这也是一种很好的 IT 演进思路。</p><p></p><p>张凯峰：除了自己训练大模型，还有一种可能就是用一个相对成熟的模型来训练一个自己垂直领域的小模型，供企业内部使用。这是企业在做大模型时需要考虑的选择方向之一。还有其他的方向吗？</p><p></p><p>李洋 ：现在大模型的应用模式一般分为以下几种：</p><p></p><p>● 提示词工程：使用大模型不需要重新训练或者构建数据集，但由于大模型自身的泛化能力和通用能力，企业可以通过提示词来进行引导，从而使得模型生成解决方案、文案等。</p><p></p><p>● RAG，可作为大模型的补充。作为外挂，在检索或提问过程中可以将数据融合到模型中，并生成相应结果。</p><p></p><p>● 微调，或称精调：企业可在确保质量过关的情况下，使用小部分数据，挑选出自己的模型，并将专业知识和私有数据融入其中。</p><p></p><p>● 预训练：如金融行业中的一个不普遍的领域，为了训练这种行业大模型，企业需要将购买或开源的大模型中的数据重新进行训练，使其获得具有金融行业或其他专业领域知识的行业模型。基本上可以参考以上几种方式来使用大模型。在封装开源模型的过程中，可能需要采用一些综合应用的方法。如在前期使用一些提示工程，在后期添加微调。</p><p></p><p>李智伟：对于企业来说，是否需要大模型，以及大模型的数量多少问题，需要看具体场景。在书写公文或者分析金融报告时，可以使用一个模型，无论是 prompt 还是 FT。而对于更加专业的领域而言，可能需要使用 FT，并为每个任务提供精标数据进行训练。对于整个企业来说，必须采用多模型。在小模型时代，我们在构建基于模型的软件和系统架构时，就已经采用了多模型组合的方式。而现在，更加明确的是，从扩展架构的角度来看，应该采用大模型、小模型和 RAG 的组合方式。</p><p></p><p>企业不能只依赖一个模型。现在的模型架构是大模型负责调度编排，小模型负责完成特定任务，任务完成后，我们需要把所有输出汇总并呈现给最终用户使用。目前来看，RAG 增强技术也不需要模型了，大部分只需要做 prompt。小模型的获取方式有两种：利用原有 IT 资产中的小模型，即资产再利用；另一种是在基于大模型训练后，通过剪枝和蒸馏等技术将神经网络缩小，得到小模型。去年，很多企业都认为一个模型可以解决所有问题，甚至花费数千万购买大模型。但现在人们已经转换了思想。例如我们在去年一直为一家零售客户制作电销大模型，由于我们公司在过去十几年中一直从事客服工作，所以我们使用小模型实现智能拨打电话，而大模型出现后，我们认为大模型对我们的业务更加有利，进行了替换。</p><p></p><p>大模型的应答效果和对话效果都比小模型要好，但是在效率方面存在问题。当时我们的做法是加大量的 GPU，提高并发性。但是现在看来性价比极低。从整个 IT 构建来看，只能先解决准确率问题。但是需要考虑到，长此以往，性价比是支撑不住的。我们需要考虑小型化的问题，比如通过模型的裁剪或者蒸馏来实现小型化。我们甚至需要将原本使用的小模型加入到中间过程中。例如在完成某项特殊任务时，小模型效果比较好，可以使用小模型来尽可能地减少大模型进行交互判断处理的工作量。随着时间的推移，我们的整体成本和算力需求正在逐步下降。</p><p></p><p>相比之前使用纯小模型，目前我们整体的外呼发起率可以提升到 30%，大屏通话也能增长 50%，这都是大模型带来的好处。此外，使用大模型与使用传统的人工呼叫相比，也有利于降低客户不满意度。</p><p></p><p>张凯峰：刚刚提到的模型更换问题，可以再展开一下吗？比如，在什么情况下企业需要为当前的投资考虑未来替换的可能性，以及在替换之前需要做好哪些准备工作？</p><p></p><p>李智伟：关于这个话题，实际上受到两个因素的影响。</p><p></p><p>第一个方面是，早前在大模型 GPT3.5 发布那时候，一些开源模型也随之出现，但我们当时使用下来发现这些开源模型其实效果达不到预期。为了达到我们想要的效果，还是需要重新做 SFT，当时的想法是所有的模型都是需要做训练的。但是随着基础模型的发展，特别是 ChatGPT 4o 的发布，现在国内的开源模型已经可以满足基础的需求。在过去的半年中，市场上训模型这件事其实慢慢变少了，基础模型可以直接用于一些常规场景，甚至进行信息的获取和整理。</p><p></p><p>第二个方面，目前国内虽然有约 200 家大模型和各种模型的生产公司，但都没有成熟的商业模式。市场上既存在开源模型，也有闭源模型。大多数大公司都倾向于闭源，但实际上这些做闭源的公司都希望通过消耗更多的云算力来盈利，这比销售单个模型更加有利可图。但这也带来了一个问题，如果一个企业的供应商消失了，其发动机怎样进行维修呢？</p><p></p><p>因此，企业在进行系统建设时，必须考虑到未来的可替代性。这意味着企业必须要在前期就考虑这个问题。在建构整个架构时，需要考虑到两个方面：</p><p></p><p>首先，我们需要一套供应链，以便在未来能够更换其他供应商。</p><p></p><p>还需要重点考虑一个问题。例如，去年我们开始训练大模型，并且为企业提供了许多输出。但是年初开始，当我们意识到这个问题后，就投入了大量的研发精力来开发模型部署工具链。它可以帮助企业监控和运营多个模型，甚至可以轻松地替换、上线和下线部署托管。一套标准化的能力体系可以对未来产生最大价值。这可以保证基础模型趋向标准化。企业可以在任何时间选择最适合自己的资产，还可以使用更加强大的模型来替换现有最低版本。</p><p></p><p>第二种情况是，企业针对自己的行业知识进行训练，这要求企业具备快速部署和完成的能力。此外，企业未来的混合部署需要一定的工具链支持。</p><p></p><p>张凯峰：在考虑采购国内的大模型时，无论是大厂的模型还是自行预训练的模型，哪些非功能性方向是我们需要仔细考虑的呢？</p><p></p><p>李智伟：除了要考虑性价比之外，企业还需要考虑准确性、鲁棒性和稳定性等因素。在选择模型时，企业需要考虑其应用场景。例如，在线系统需要更高的时效性，因此可能需要混合性模型部署。这是第一个。</p><p></p><p>第二个是，由于模型的泛化可能会带来负面影响，比如幻觉问题，因此在严肃场景之下，必须进行针对性数据增强训练。然而，增强训练会带来一个问题，随着模型参数的增加，其准确性和严肃性会提高，但效率也会降低。因此，需要采取一些措施，例如进行裁剪或蒸馏，以提高性能。但是在非严肃场景下，需要的是模型的泛化性，对性能指标的要求会降低。</p><p></p><p>近期我还非常关注模型的安全和合规，目前大模型在安全合规方面还有待提升。企业需要考虑到个人隐私保护问题。有多少企业的原始数据经过了严格的隐私清洗和认证呢？</p><p></p><p>另一个是多模态大模型问题。目前，多模态对数据的使用更加深入。此前，大语言模型更多关注到的是文本类理解使用。但涉及到多模态，就要考虑对于视觉、音频和视频的理解和使用，在这个过程中，数据安全是极其重要的。需要企业完成两个任务：第一，采购大模型时，需要考虑其合规性和安全性；第二，大模型使用必须经过备案，并接受审查。</p><p></p><p>李洋：人工智能领域，特别是现在的大模型和未来的发展方向，可能会像云计算一样。随着大数据等新兴技术的发展，网络安全合规方面也会有相应的审查标准。例如，如果大模型的服务提供商类似于我们的公有云提供商，作为租户，调用大模型时，租户与平台之间的责任共担和举证是非常重要的。此外，在选择大模型时，还需要考虑运营团队的能力。</p><p></p><p>大模型需要具备底座和二次开发及优化能力，但建立大团队不太现实，因此需要依赖服务商提供的人工智能，包括架构师和科学家的能力。人工智能的发展具有不确定性，可能会出现幻觉、误导或暴力等后果。我们需要考虑到的是，如何训练、采购和使用大模型，以及如何对其进行完善实施和调整。平台提供方仍在不断改进中，因此需要一个团队的支持。</p><p></p><p></p><h2>企业盈利新机遇</h2><p></p><p></p><p>张凯峰：除了模型制造商之外，那些从事大模型应用的企业，他们的盈利方向和模式可能会在哪里呢？可以结合自己的经验和故事来分享一下。</p><p></p><p>李智伟：在 B2B 领域，现在还处于早期阶段，各家还没有实现盈利的商业模式。</p><p></p><p>这次技术革命将带来许多新机会。我们以前在做内容审核时，会使用小模型。例如，做内容审核需要积累大量数据并训练专家。如果想审核某个涉政类的内容，就需要储备很长时间。但是，如果使用多模态大模型，就可以快速进入审核市场。这是对以前技术的一种弯道超车的机会。</p><p></p><p>在 B2B 领域盈利，对于企业来说，是一个非常多元化的机遇。但在中国市场，能否盈利仍然存在很大的不确定性。这种多元化的机会也带来了一些好处，比如以前从未涉足的新市场会带来新的机遇。</p><p></p><p>多模态模型和大语言模型在这一过程中都已经被开源甚至公开化了。因此企业可以更容易地进入这个市场并积累财富。</p><p></p><p>李洋：首先，AI 作为一种科技手段，只要能起到促进作用，它一定会促进原本产业的发展。比如在抖音、微信运营或公众号运营过程中，如果能在原有产业中嵌入 AI 元素，或者通过大模型实现促进，那么就可以实现盈利。但另一方面，提供大模型的公司只要向 B 端或 C 端提供相应的应用程序，就能盈利。例如，Stable Diffusion 平台或 OpenAI 开发的类似 GPT 的大模型应用程序或产品，只要被腾讯、抖音等公司采用，就可以利用大模型盈利。但是还有一个问题，在数字化和智能化的过程中，能否创造出新的盈利模式。例如华为、四大咨询公司（麦肯锡等）提供的咨询服务，如果企业能将这些咨询服务整合成一个大模型，并利用大数据分析技术，那么实施周期和效率可能会比传统方式更快。此外，还需要解决数据脱敏和隐私保护等问题。</p><p></p><p>企业应该将这种科技手段与所有产业结合起来。从第一次工业革命到第四次工业革命，我们一直在追求生产力的提升，但我们仍然需要抓住自己的主业。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</id>
            <title>Karpathy又整活儿了！一天训练出GPT-2、成本还骤降100倍，网友：dream老黄把价格再打下来</title>
            <link>https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g65xuAxOEIUIWc5Ey5h7</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:52:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, GPT-2, Andrej Karpathy, llm.c
<br>
<br>
总结: 本文介绍了OpenAI创始成员、前研究科学家Andrej Karpathy最近在llm.c中重现GPT-2的过程。Karpathy使用了15.58B参数的完整版本的GPT-2，并通过改进计算、软件和数据等方面，成功在24小时内以672美元的成本对该模型进行了重现。文章还提到了Karpathy的职业经历和他开发的llm.c，以及对GPT-2训练成本和硬件利用率的估算。最后，文章对比了Karpathy复刻的GPT-2与19年版本的GPT-2的输出结果，发现新模型的输出质量与GPT-2相当。 </div>
                        <hr>
                    
                    <p>OpenAI 创始成员、前研究科学家Andrej Karpathy 最近尝试在llm.c中重现了GPT-2。这里的GPT-2是15.58B参数的完整版本，最初亮相于OpenAI 2019年2月14日发布的博文《Better Language Models and their Implications》当中。</p><p></p><p>“2019年时，GPT-2 的训练工作还是一个涉及整个团队、需要规模化投入的项目。但如今5年过去，随着计算（H100 GPU）、软件（CUDA\cuBLAS、cuDNN、FlashAttention）和数据（例如FineWeb-Edu数据集）等层面的改进，我们已经能够在24个小时之内凭借单个八H100节点成功对该模型进行重现，且总成本仅为672美元。”Karpathy 说道。</p><p></p><p>Karpathy 在2017年离职后进入特斯拉担任AI 高级总监，但在2023年再次回到OpenAI组建团队，并推出了 ChatGPT。一年后，Karpathy离开了OpenAI，并出于教育意义开发了llm.c。llm.c 是简单、纯 C/CUDA 的 LLM（总计约5000行代码），无需使用涉及Python解释器或者高复杂度深度学习库（例如PyTorch/JAX、huggingface/transformers 等）的典型训练技术栈。</p><p></p><p>在Karpathy 公布了这一结果后，有网友问到当时训练 GPT-2 的成本，Karpathy 回答道：</p><p></p><p>这些信息从未公开过，但我估计成本要高得多。按乘数倍率来算，数据方面可能要高了 3 倍，硬件利用率方面高 2 倍。2019 年的计算集群可能使用的是 V100 (~100 fp16 TFLOPS)，现在可能换成了 H100 (~1,000)，这样算下来性能大概提高了 10 倍。所以成本方面非常粗略地估计，可能要高出 100 倍，也就是大约 100,000 美元左右？</p><p></p><p>对此有网友评价道，“随着英伟达对 AI 工作负载加速硬件开发的不断深入，我预计未来几年内，这款硬件的成本可能只有几十美元，并且训练时间只需几个小时。”</p><p></p><p>至于具体效果，Karpathy 与19年的GPT-2版本做了对比。同样用的当时博文介绍里的提示词“In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.” 结果新模型的输出结果相当连贯，质量也大致与GPT-2相当。</p><p></p><p>两个模型生成的文字较长，有兴趣的朋友可以点击查看：http://llmc.s3-us-west-2.amazonaws.com/html/gpt2_vs_llmc30kedu.html</p><p></p><p>下面我们来看下Karpathy的复刻过程。</p><p></p><p></p><h3>复现过程</h3><p></p><p></p><p>首先，Karpathy强调，使用llm.c训练GPT-2非常简单，因为它是用C/CUDA编写的，所以全程不涉及minconda、Python、PyTorch等。只需要一台八H100 GPU的设备即可。</p><p></p><p>“总之，不必担心，llm.c在算力要求方面非常灵活，哪怕只有一张GPU，大家也仍然可以训练出自己的GPT-2——只不过需要等待8天，而不是像我这样的1天。而如果您拥有16张GPU（例如使用新的Lambda 1 Click Clusters），还可以开展多节点训练，前后只需要等待12个小时。”Karpathy说道。</p><p></p><p>在节点启动之后，下面来看看GPT-2训练的完整说明，不用担心，Karpathy表示保证一分钟以内开始执行：</p><p></p><p><code lang="text"># install cudnn so we can use FlashAttention and run fast (optional)
# https://developer.nvidia.com/cudnn-downloads
# for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

# "install" cudnn-frontend to ~/
git clone https://github.com/NVIDIA/cudnn-frontend.git

# install MPI (optional, if you intend to use multiple GPUs)
# (you might also have to install NVIDIA NCCL if it doesn't come with your setup)
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

# download and enter llm.c repo
git clone https://github.com/karpathy/llm.c.git
cd llm.c

# download the "starter pack" (~1GB download)
# contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s
./dev/download_starter_pack.sh

# download the training dataset (FineWeb-Edu 100B token) .bin data shards
# note: this is a total of 1001 data shards. If you only want to test things
# out and don't want to do an actual run, feel free to append the number of
# training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)
# the full dataset is ~200GB, we can store it here in dev/data directory.
cd dev/data
./edu_fineweb.sh

# compile (~1 min 1st time for cuDNN mostly, few sec from then on)
cd ../../
make train_gpt2cu USE_CUDNN=1

# and train! (wait 24 hours here)
mpirun -np 8 ./train_gpt2cu \
-i "dev/data/edu_fineweb100B/edu_fineweb_train_*.bin" \
-j "dev/data/edu_fineweb100B/edu_fineweb_val_*.bin" \
-o "log_gpt2_1558M" \
-v 250 -s 300000 -g 384 \
-h 1 \
-b 16 -t 1024 \
-d 1048576 \
-r 0 \
-z 1 \
-c 0.1 \
-k "cosine" \
-l 0.0006 \
-q 0.1 \
-u 700 \
-n 2000 \
-x 32000 \
-ge 1 \
-y 1 \
-e "d48"</code></p><p></p><p>稍后会对参数做具体解释。接下来开始优化：</p><p><code lang="text">num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
...</code></p><p></p><p>可以看到，每个步骤大约需要2.75秒，而其中总共涉及3.2万个步骤，所以现在需要等待约24个小时。</p><p></p><p>在每一步中，训练作业的运行都会占用约100万个FineWeb-EDU token（数据内容来自互联网上的教育网页），并对模型的15.58亿个权重进行更新，使其能够更好地预测序列中将要出现的下一个token，到最后将总计处理3.2万 x 1048576 = 33.6 B个token。随着预测下一token的能力越来越强，loss也会随之下降。</p><p></p><p>接下来的工作是归一化（将数值范围控制在0.1至1之间），学习率也在前几个步骤中逐渐升温。从结果来看，这套模型的flops利用率（MFU）约为50%，可说是相当高效了。</p><p></p><p>现在唯一要做的，就是等待24小时让其完成，之后可以使用dev/vislog.ipynb jupyter notebook对main.log日志文件进行可视化。为此，大家需要安装Mython和matplotlib。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e045d301d587cd855b073928d7b03d6.png" /></p><p></p><p></p><h4>评估</h4><p></p><p></p><p>如左图，正在跟踪FineWeb-EDU验证数据的loss。如果大家只运行OpenAI发布的GPT-2并在此基础上评估其loss，得到的就是红色水平线（loss 2.83）。而Karpathy模型的运行结果快速将其超越，步长约为5000。</p><p></p><p>当然，这样的比较并不公平，毕竟GPT-2是在从未发布的WebText数据集上训练而成，因此素材内容可能存在很大的分布差异。比方说，如果在LR 1e-4下对OpenAI模型进行1000步微调，loss就会迅速下降至划线（loss 2.61），代表其正在快速适应新的统计数据。</p><p></p><p>但就个人而言，Karpathy 认为loss验证更多只是一种健全性检查，要实际比较模型性能，还是得借助更靠谱的第三方评估。</p><p></p><p>这时候就要请出HellaSwag评估了，这也是目前市面上表现良好、流畅、普及度高、常被引用且能够提供早期信号的评估方案之一。其中提供的都是简单的常识性场景，大模型必须据此做出正确的内容延展。</p><p></p><p>Karpathy 在右侧窗格中评估了HellaSwag，并发现在约25K步左右与GPT-2模型的性能发生交叉（早于GPT-2，据估计GPT-2的训练数据集共有约1000亿个token。但这可能与数据质量的提高有关，之前Karpathy在124M训练期间也观察到了类似的现象）。绿线为同等参数规模的GPT-3模型，其模型架构与GPT-2几乎相同、仅存在细微差别（上下文长度从1024增长至2048），同时是针对3000亿token进行了训练（相当于我们此次实验训练token量的10倍左右）。</p><p></p><p>必须承认，HellaSwag也不能算是完美的单点比较选项，毕竟它测试的主要是简单的英语和常识，并不涉及多语言、数学或者代码内容。也许是因为WebText数据集在这几个方面拥有更高的比重，所以才把模型规模推到了这样的水平，但Karpathy团队并不确定，毕竟OpenAI从未对此做出过解释。</p><p></p><p>Karpathy指出，一般来讲，在GPT-2等低能力模型上很难获得良好的评估结果，毕竟这类模型无法理解多项选择题；而且其样本质量不够高，无法正常完成涉及标准数学或者代码的评估测试。</p><p></p><p></p><h4>参数指南</h4><p></p><p></p><p>现在让我们仔细看看我们在训练中传递的参数。OpenAI发布的GPT-2虽然包含模型权重，但相关细节却很少；GPT-3版本并未开放权重，但相关细节较多。因此在多数情况下，我们只能参考GPT-3论文中提及的超参数，毕竟GPT-2论文几乎没有提到这方面信息：</p><p>&nbsp;</p><p>-i -j&nbsp;用于训练和验证分割标记文件，需要提前使用&nbsp;edu_fineweb.sh进行下载。-o&nbsp;是写入日志和检查点的输出目录。-v 250&nbsp;要求每250步执行评估并记录验证loss。-s 300000&nbsp;要求每30万步采样部分token。因为总步数不足30万，所以这其实是一种关闭采样的灵活方式，实际只会在最后采样一次。-g 384&nbsp;将最后需要采样的token数设置为384。-h 1&nbsp;要求评估HellaSwag准确性。-b 16&nbsp;将微批次大小设置为16。如果内存不足，请降低此值，例如依次尝试8、4、2、1。-t 1024将最大序列长度设置为1024，与原版GPT-2保持一致。-d 1048576&nbsp;要求总批次大小为2的20次方，与GPT-3论文中的超参数设置相同。代码将确保满足所需的总批次大小，并计算优化所需的梯度累积“内循环”步骤。例如，之前提到Karpathy拥有8张GPU，每张GPU执行16 x 1024个token，因此每个微步（即一次向前向后）对应8 x 16 x 1024 = 131072个otken，因此代码计算梯度累积步数应该为8以满足每步所需的1M批次大小。即每向前+向后8次，而后进行一次更新。-r 0&nbsp;将重新计算设置为0。重新计算是一种在计算与内存之间求取平衡的方法。如果设为-r 1，则代表在反向过程中重新计算前向传递的一部分（GeLU）。就是说Karpathy不必须通过对其缓存来节约内存，但需要付出更高的算力成本。因此如果内存不足，请尝试设置-r 1或者-r 2（同时重新计算layernorms）。-z 1&nbsp;在多个GPU上启用ZeRO-1（即优化器状态分片）。如果使用多于1张GPU进行训练，则应当选择这样的设置，且基本上应始终将其保持为开启状态。但在单GPU上，此设置没有实际效果。-c 0.1&nbsp;将权重衰减设置为0.1。只有（2D）权重的衰减与GPT-2完全相同，具体数值来自GPT-3论文。-k "cosine"&nbsp;设置余弦学习率计划，这里姑且直接使用默认值。-l 0.0006&nbsp;将最大学习率设置为6e-4。根据GPT-3论文的解释，Karpathy这个大小的模型应当使用2e-4，但这里Karpathy将其增加至三倍，似乎训练速度更快且没有引发任何问题。这项参与未经认真调整。-q 0.1代表在训练过程中，将学习率衰减至最大LR的10%，取值参考自GPT-3论文。-u 700&nbsp;表示将在前700次迭代中将学习率从0提升至最大，总批次大小为0.5M时对应3.5亿个token，取值同样来自GPT-3论文。-n 2000&nbsp;要求每2000步保存一次模型检查点。-x 32000&nbsp;要求总共32K步。之所以选择这个数字是因为其好读好记，而且正好对应24个小时。-ge 1&nbsp;为CublasLt设置最近合并的gelu重新计算设置（可选）。-y 1用于将“恢复”标记设置为开启。如果训练因任何原因而崩溃或者挂起，则可按下CTRL+C并重新运行此命令，其将尝试恢复优化。Llm.c具备按bit确定性，因此大家将获得与崩溃之前完全相同的结果。-e "d48"&nbsp;要求从头开始初始化深度为48的GPT-2模型。</p><p></p><h4>内存指南</h4><p></p><p></p><p>大多数朋友面临的最大限制，可能就是自己的GPU内存达不到80 GB。Karpathy表示，“没关系，只要有耐心，之前提到的这些任务也都能顺利运行完成，只是速度会稍慢一些。”</p><p></p><p>但如果模型实在太大，又该如何处理？Karpathy表示，最重要的是调整微批次大小-b，尝试将其缩小并保持在合适的水平。例如16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1。以此为基础，尝试使用重新计算设置-r，即0（最快，但占用的内存最大）、1（速度慢得多，但可以节约大量内存）或者2（稍慢一些，但内存节约量较少）。</p><p></p><p>下一步优化思路则是禁用fp32中的主权重，这里可怜请用 -w 0（默认值为1）来执行此操作。Karpathy并没有为参数维护fp32副本，因为根据经验，之前的几次运行都没有问题，可能是因为使用了随机舍入。</p><p></p><p>“但如果大家在亲自尝试时遇到了问题（其实可能性极低），也可以使用-t减少最大序列长度，将默认值从1024下调至512、256等。但这意味着缩小了其最大注意力范围，所以模型的性能也会变得更差。 ”Karpathy建议道。</p><p></p><p></p><h4>代码</h4><p></p><p></p><p>“虽然我可能有点倾向性，但llm.c真的非常优雅”Karpathy介绍道：</p><p></p><p></p><p>它只需要基本CUDA依赖即可运行。它是C/CUDA中最直接、最小且可读的实现。llm.c总计约有5000行C/CUDA代码。Karpathy 主要尝试使用C，而非C++，以保持代码简洁。神经网络训练只是对单个浮点数组执行相同的简单算术运算（就是加减乘除）的一个while循环，实在没必要搞得太过复杂。它的编译和运行速度极快（几秒钟内），因此可以执行更多步骤并减少等待时间。它会在开始时一次性分配所有GPU内存，并在之后的训练期间将内存占用量保持恒定。因此只要执行步骤启动，我们就能保证接下来的运行状态始终良好、不会发生OOM。具备按bit确定性。 运行效率很高，MFU略低于约50%。主要入口点和大部分代码位于文件tarin_gpt2.cu当中。该文件包含GPT-2模型定义和约2000 LOC中的训练循环，并从llmc目录处导入了一大堆带有各种实用程序和各层实现的辅助文件。cloc llmc报告了23个文件，3170 LOC，而cloc train_gpt2.cu目前为1353 LOC。</p><p></p><p></p><h4>多节点训练</h4><p></p><p></p><p>如果您是位手握大量GPU的“土豪”，llm.c也支持多节点训练。Karpathy表示，其见过的llm.c训练最多支持约500张GPU。</p><p></p><p>“个人迄今为止进行过最大的一次运行，是依靠Lambda全新一键集群功能上实现的，当时是在2个节点上使用了16张H100 GPU。Lambda团队提供了关于如何在其一键集群上训练llm.c模型的详细说明。例如在使用512-GPU H100集群时，每小时费用为2300美元，这时候整个GPT-2训练周期就仅为30分钟。当然，这时您需要增加总批次大小（例如增加到约8M）并稍微调整一下超参数。我还没有尝试过，但相信会有效而且相当爽快！ ”Karpathy说道。</p><p></p><p></p><h4>PyTorch比较</h4><p></p><p></p><p>使用Karpathy的并行PyTorch实现，与PyTorch的运行效果对比应该类似于以下形式：</p><p></p><p><code lang="text">torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin "dev/data/edu_fineweb100B/edu_fineweb_train_*.bin" \
    --input_val_bin "dev/data/edu_fineweb100B/edu_fineweb_val_*.bin" \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1</code></p><p></p><p>这里的PyTorch代码仅供参考，而非实际实现，因为其中的训练循环在某些位置可能略有不同（例如，数据加载器不会对分片进行置换等），总之大家看看就好。Karpathy还将默认词汇大小修改为50257 -&gt; 50304 以提高效率。经过一夜运行，PyTorch给出以下结果：</p><p></p><p><code lang="text">step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
...</code></p><p></p><p>Karpathy 强调，这份PyTorch脚本可能还有很大的优化空间，但至少可以当作观察基准。PyTorch占用的内存量似乎更大（此次运行约为80 GB），而llm.c仅占用了57 GB（节约比例为29%）。内存资源非常重要，因为它能帮助我们容纳更大的训练批次（例如，llm.c在这里可以将微批次提升至24个），从而加快训练速度。</p><p></p><p>其次，我们看到每次迭代大约为3386毫秒，而llm.c的迭代为2750毫秒，速度要快约19%。</p><p></p><p>另外还有一些已知优势，例如llm.c包含启动反向传递的融合分类器等优化选项，据Karpathy所说，目前的torch.compile还做不到。但Karpathy表示，这样的性能差异可能是因为他的脚本没有充分调优，所以比较结果仅供大家看看、试试和作为调试思路的启发。</p><p></p><p>“我想表达的是，llm.c的优化程度和速度水平已经相当不错，当然只是在GPT-2/3训练的特定场景之下。 ”Karpathy说道。</p><p></p><p></p><h4>最终模型</h4><p></p><p></p><p>感兴趣的朋友可以参考以下几条链接：</p><p></p><p></p><p>main.log<a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/main.log">文件</a>"。model_00032000.bin&nbsp;<a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/model_00032000.bin">llm.c</a>"&nbsp;bin模型文件我已经将模型转换为huggingface transformers GPT-2并上传至这里:&nbsp;karpathy/gpt2_1558M_final2_hf。</p><p></p><p></p><h4>模型导出</h4><p></p><p></p><p>模型导出可以按如下方式进行，例如：</p><p></p><p><code lang="text">python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export</code></p><p></p><p>之后大家可以运行Eleuther评估工具，或者运行huggingface采样管线以获取模型样本：</p><p><code lang="text"># take model for spin
import torch

output = "./gpt2_1558M_final2_hf"

# set pytorch seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

prompt = "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(output)
model = AutoModelForCausalLM.from_pretrained(output, attn_implementation="flash_attention_2", torch_dtype=torch.bfloat16, device_map='cuda')
model.eval()
tokens = tokenizer.encode(prompt, return_tensors="pt")
tokens = tokens.to('cuda')

output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4)
samples = tokenizer.batch_decode(output)
for sample in samples:
    print('-'*30)
    print(sample)</code></p><p></p><p>另外大家也可以查看dev/eval 以获取关于如何运行Eleuther Evaluation Harness、HuggingFace Open LLM Leaderboard的具体说明。</p><p></p><p></p><h5>400B token运行</h5><p></p><p></p><p>Karpathy 还尝试用远超33B token的规模训练了GPT-2。具体来讲，Karpathy将-x更改为400000以训练420B token（规模甚至比300B 的GPT-3还要大）。</p><p></p><p>结果显示，这套模型前半阶段运行得不错，但到大约33万步时开始出问题： 这套模型在HellaSwag上全面碾压了GPT-2及同等体量的GPT-3（最高性能优势可达约61%），但遗憾的是之后新模型开始不稳定并发生崩溃。</p><p></p><p>在此过程中虽然也出现过一些较小的峰值，Karpathy将代码配置为当检测到瞬时不稳定时跳过更新（使用了-sl 5.0 -sg 5.0标记），这有助于缓解并推迟问题的出现。但Karpathy承认，模型在初始化、激活范围和整体模型训练的稳定性方面还不够谨慎，对很多深层次问题也没有涉及。</p><p></p><p>这些问题会令模型逐渐变得不稳定，特别是对于规模较大、训练时间较长的模型更是如此。当然，我的实验仍在进行当中。如果大家对稳定模型训练有任何想法和建议，请在评论区中与我们分享。</p><p></p><p></p><h4>常见问题解答</h4><p></p><p></p><p>Q：我可以从llm.c中的模型里采样吗？</p><p>A：也不是不行，但效率很低而且效果不好。如果大家想要提示模型，推荐使用前文提供的huggingface版本。</p><p></p><p>Q：我能跟它聊天吗？</p><p>A：还不行，目前这个版本只完成了预训练，还没有接受过聊天微调。</p><p></p><p>Q：可以在fp8精度下训练吗？</p><p>A：不行，我们目前主要是在bf16下训练，但早期版本正在尝试当中。</p><p></p><p>Q：我的GPU不是英伟达的，可以运行llm.c吗？</p><p>A：不行，llm.c目前仅支持C/CUDA，但已经提供良好分支。比如@anothonix积极维护的AMD分叉（https://github.com/anthonix/llm.c）就相当不错。 GPT-2(124M)。这里再贴一篇关于在llm.c中训练GPT-2（124M）模型的老帖，其中包含与llm.c运行相关的更多信息。124M属于GPT-2迷你系列中的小体量模型，只有124M个参数，远低于本文讨论的1558M参数。</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>Karpathy 让我们看到了更多可能，但这似乎也难以意味着未来整个训练成本会下降。不久前，AI初创公司Anthropic的首席执行官Dario Amodei 就在采访中表示，目前GPT-4o这样的模型训练成本约为1亿美元，而目前其正在开发的AI大模型训练成本可能高达10亿美元。 他还预计，未来三年内，AI大模型的训练成本将上升至100亿美元甚至1000亿美元。</p><p></p><p>参考链接：</p><p>https://x.com/karpathy/status/1811488645175738409</p><p>https://github.com/karpathy/llm.c/discussions/677</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</id>
            <title>vivo蓝河操作系统首届技术沙龙即将举办，邀您共探Rust与AI新时代</title>
            <link>https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0373pdxIZtgCgWxObktf</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jul 2024 07:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: vivo, 蓝河操作系统, AI 大模型, Rust语言
<br>
<br>
总结: vivo在2023年11月发布了自研操作系统蓝河操作系统（BlueOS），该系统基于Rust语言编写，接入了vivo蓝心大模型，具备先进的AI能力和多模输入子系统，提升了智慧、流畅性和安全性，同时拥有生态互联的连接技术，为用户和开发者提供更智能、安全的体验和开发平台。 </div>
                        <hr>
                    
                    <p></p><p>2023 年 11 月，vivo 在开发者大会上正式发布了自研操作系统——蓝河操作系统（BlueOS）。据称蓝河为业界首款系统框架基于 Rust 语言编写的操作系统，同时底层还接入了 vivo 蓝心大模型，「蓝河操作系统」一经发布便引起了行业的广泛关注。</p><p></p><p>在智慧方面，通过引入先进的 AI 大模型能力，BlueOS 实现了 AI 服务引擎和多模输入子系统，支持基于自然交互方式的应用开发，用户也可以通过语音、手势等多种方式与系统进行交互；在流畅性方面，通过优化算法和高性能系统架构设计，BlueOS 成功实现了资源的合理分配和高效利用，确保了应用运行丝滑流畅；在安全方面，BlueOS 的系统框架由 <a href="https://xie.infoq.cn/article/018986ea780ce3a32225de6d0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Rust 语言</a>"编写，能够从源头避免内存使用不当引起的安全漏洞；在生态互联方面，BlueOS 基于分布式设计理念的连接技术，能够广泛兼容行业标准协议，也让 vivo 在智能家居、智慧出行、智慧办公等场景具备了更大的可能性。</p><p></p><p>从某种意义上来说，蓝河操作系统（BlueOS）的诞生，代表了 vivo 在提升用户体验和安全方面的坚定承诺，不仅为用户带来了更加智能和安全的产品体验，也为开发者提供了一个高效、安全的开发平台，同时也对推动国产软件生态的发展具有积极意义。</p><p></p><p>当前，核心技术自主可控仍然是国产软件行业发展的主旋律，大模型与 AIGC 的场景化探索也越发火热。率先入局的 vivo 在这些方面有哪些新思考、新探索，BlueOS 在技术探索和生态建设上又取得了哪些新进展？</p><p></p><p>7 月 27 日，vivo 将在北京举办蓝河技术沙龙，汇聚行业大咖、资深技术专家，共同分享蓝河操作系统的最新发展动态，探讨操作系统技术的未来趋势等。你将有机会与行业大咖面对面交流，也可以探索应用开发新范式和 AI 大模型技术的前沿应用等。</p><p></p><p>蓝河新航，机遇正当时！马上报名技术沙龙，现场开阔技术视野、拓展合作机会， 与 vivo 一起，开辟下一代操作系统的新航道，共同探索未来的无限可能！</p><p></p><p>招募启动日起至活动前一天，分享预热文章至朋友圈，活动当天到场即可凭借朋友圈公开转发记录，领取蓝河定制雨伞，数量有限，先到先得！</p><p></p><p>现场参会者，更有机会获得 vivo 蓝牙耳机等重磅礼品，不容错过~</p><p></p><p>码上报名👇</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/de/de9a583d9d1ee251ead9b0cd3269ceab.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>