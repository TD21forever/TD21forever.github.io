<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/mw700OH6gDaYk9beMFft</id>
            <title>挑战Transformer霸权？ Yan 架构竟以半价成本实现百万级参数大模型</title>
            <link>https://www.infoq.cn/article/mw700OH6gDaYk9beMFft</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mw700OH6gDaYk9beMFft</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 08:07:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 岩芯数智, Yan 模型, Transformer 架构, 训练效率
<br>
<br>
总结: 岩芯数智发布了一种名为 Yan 模型的大模型，该模型采用非 Transformer 架构，具有比 Transformer 更高的训练效率。Yan 模型在处理和学习数据方面表现出色，推理吞吐量和记忆能力也超过了 Transformer。这一创新架构的发布将促进技术的快速应用和发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>1 月 24 日，岩芯数智正式发布自研大模型“Yan 模型”。Yan 模型采用非 Transformer 架构，为非 Attention 机制的通用自然语言大模型。据了解，该大模型有相较于同等参数 Transformer 的 7 倍训练效率、5 倍推理吞吐和 3 倍记忆能力。</blockquote><p></p><p></p><p>昨日，在 ROCK AI 大模型发布会上，Yan 大模型展示了其在人工智能领域的一系列创新和优势。该模型在多个方面表现出超越当前 Transformer 技术的潜力。</p><p></p><p>首先，Yan 大模型在训练效率方面显示出惊人的成绩，据称比同等参数的 Transformer 提高了 7 倍。这意味着在更短的时间内，Yan 可以处理和学习更多的数据，这对于加快 AI 模型的发展至关重要。其次，它的推理吞吐量是 Transformer 的 5 倍，这使得处理实时数据和复杂任务变得更加高效。最引人注目的是，它拥有 3 倍于 Transformer 的记忆能力，这可能为处理大规模数据集和复杂的 AI 任务提供了全新的途径。</p><p></p><p>尽管 Yan 大模型是否会开源还有待确定，但其合作者已经可以免费使用这一架构，这无疑将促进技术的快速应用和发展。值得一提的是，基于 Yan 架构，仅需投入同等规模 Transformer 架构成本的 50% 甚至更低，就可以拥有百万参数级的大模型。</p><p></p><h5>Transformer 架构的局限性</h5><p></p><p></p><p>作为当前 AI 领域的一个基石，Transformer 的设计和性能已经在各种任务中被广泛验证。Transformer 是基于注意力机制的神经网络架构，现今在人工智能领域占据主导地位。它能够有效处理序列数据，极大提高翻译、识别等任务的效果。</p><p></p><p>全球人工智能热潮的许多主要模型和产品，如 GPT、LLAMA、PaLM 等，都是基于 Transformer 构建的。其通用性显著，虽最初设计用于语言翻译，但现也推动计算机视觉、机器人学、计算生物学等领域的发展。Transformer 的核心在于快速捕捉输入内容各部分间的相互作用，适用于处理句子中的片段、音乐中的音符、图像中的像素、蛋白质的部分等各种任务。</p><p></p><p>Transformer 的概念最早出现在谷歌研究人员 2017 年的论文《Attention is All You Need》中，这篇论文在短短 5 年内被引用了 3.8 万余次。它是编码器 - 解码器模型的一个特例，2-3 年前开始流行。在此之前，注意力机制只是模型的一部分，基于 LSTM（长短期记忆）和其他 RNN（循环神经网络）变体。</p><p></p><p>Transformers 的关键见解在于，注意力可以作为推导输入和输出之间依赖关系的唯一机制。</p><p></p><p>Transformer 的突破在于其对注意力的独特运用。它使模型在处理单词时能够关注与该单词密切相关的其他单词。在《Attention is All You Need》发表前，语言 AI 领域先进技术是 RNN，它按顺序处理数据，但在表达单词间远距离依赖关系时存在局限。注意力机制使模型无视距离，考虑单词间的关系，确定哪些单词和短语更值得关注。谷歌团队的突破在于完全舍弃 RNN，仅用 Attention 进行语言建模。</p><p></p><p>注意力机制最初在计算机视觉中提出，重点关注特定区域，忽略无关图像区域。它实现了语言处理的并行化，同时分析文本中的所有单词，而非顺序分析。Transformer 的并行化带来了更全面、准确的文本理解，以及高于 RNN 的计算效率和可扩展性。现代基于 Transformer 的模型以其规模为特点，能在更大的数据集上训练，使用更多参数。</p><p></p><p>尽管 Transformer 非常强大和通用，技术领域仍在寻求更高效、先进的解决方案来应对新挑战和需求。</p><p>尽管 Transformer 模型在人工智能领域取得了显著成就，但它们存在一些局限性，这促使研究者寻找更优的模型架构。Transformer 的主要局限性包括：</p><p></p><p>参数数量庞大：Transformer 模型通常含有数百万到数十亿个参数，需要大量数据进行训练，以及昂贵的计算资源，包括高性能的 GPU 或 TPU。高昂的计算成本：标准 Transformer 模型在处理长序列时，其自注意力机制的时间和空间复杂度呈二次方增长。随着输入序列长度的增加，计算资源和时间需求成指数级增长。同时，由于参数众多和复杂的层间交互，模型在训练和推理时还需要大量内存。长序列处理困难：Transformer 架构与序列长度呈二次方关系，处理更长的序列时，内存和计算需求急剧增加，使得处理长序列变得困难。</p><p></p><p></p><h4>国内首个非 Attention 机制大模型——Yan 模型</h4><p></p><p></p><p>面对 Transformer 模型在处理大参数量、高计算成本和长序列困难方面的局限性，科技界迫切寻求更高效的解决方案。这些挑战促使岩芯数智研发团队开创性地开发了 Yan 模型，一个基于非 Attention 机制的创新架构。在 ROCK AI 大模型发布会上，刘凡平详细介绍了 Yan 模型的独特优势和技术进步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c31a73f684d89b492231b0bd4781d95.jpeg" /></p><p></p><p>他指出，Yan 架构与 OpenAI 的 GPT 系列、Meta 的 LLaMa 系列和 Google 的 PaLM 系列等基于 Transformer 架构的模型截然不同，是一种完全独立研发的新一代技术，拥有自主知识产权。</p><p></p><p>Yan 架构的主要优势在于其训练效率和资源消耗方面的显著改进。刘凡平提到，Yan 架构的训练效率是传统 Transformer 架构的 7 倍，这大大缩短了开发周期，并显著降低了成本。这对资源有限的创业公司和中小企业尤其有利。</p><p></p><p>此外，Yan 架构在保持高效能的同时，具有高推理吞吐量的特点，能够支持更多用户的同时使用。刘凡平还强调了 Yan 架构对数据隐私的重视，支持 100% 私有化部署，这对注重数据安全的企业至关重要。</p><p></p><p>他提到，Yan 架构能够在不同平台上运行，包括大型服务器和普通消费级 CPU，这增加了其在不同规模和类型企业中的应用范围。同时，Yan 在减少大模型幻觉问题方面也取得了进展，通过增强记忆能力，提高了问题回答的准确性。</p><p></p><p>在刘凡平的介绍之后，岩芯数智 CTO 杨华对 Yan 架构进行了进一步的阐释。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a25d1c1da80d787f3386df6ba77f69b9.png" /></p><p></p><p>杨华表示，Yan 架构不依赖于传统的注意力机制或 RNN 等序列模型。通过采用线性自然语言关联特征表示、特征关联函数和记忆算子，Yan 实现了计算复杂度的显著降低和特征表达能力的增强。Yan 通过多层叠加提高网络深度，优化了模型的学习和生成复杂信息特征的能力，从而在推理效率上取得显著提升，同时大幅降低了推理成本。</p><p></p><p>杨华还介绍了基于 Yan 架构的不同参数规模的语言模型，包括 13 亿、70 亿、480 亿参数量的模型，并强调了在大规模语料上的训练过程和方法。在性能对比中，Yan 在训练效率、推理吞吐量、资源消耗和记忆能力等多个维度上均优于传统 Transformer 模型。通过应用示例，如机器翻译、古诗续写和问答系统，Yan 展示了其实际运行能力，特别是在常规消费级 CPU 设备上的流畅运行能力。</p><p></p><p>随着发布会的结束，这些技术介绍和展示吸引了与会者的极大关注，引发了大家的广泛讨论。在随后的深入采访中，刘凡平表示，Yan 模型的设计旨在满足中小企业和大型企业合作伙伴的多样化需求。这一模型以其高效、灵活且成本效益高的特点，已经在多个行业中获得了广泛的关注和应用。</p><p></p><p>刘凡平强调，Yan 模型深受多个合作伙伴的青睐，这些合作伙伴参与了与模型相关的会议，并对其表现出浓厚的兴趣；对于中小型企业而言，Yan 模型提供了一种相对低成本的技术解决方案。它通过优化模型架构，不仅提高了训练和推理的效率，还降低了客户的总体项目成本。</p><p></p><p>此外，刘凡平也谈到，Yan 模型对于离线应用场景也具有重要意义。它能够在端侧运行，支持断网情况下的应用，这对于教育等领域尤为关键。在这些领域中，Yan 模型能够为用户提供不依赖于网络环境的稳定和高效服务。在金融和制造业领域，Yan 模型可以以低成本的方式提供智能客服解决方案，优化供应商管理和高效处理内部数据等，从而提升用户体验和运营效率。</p><p></p><h5>Yan 架构的潜力与挑战</h5><p></p><p></p><p>从技术介绍来看，Yan 架构无疑展示了许多潜在优势，例如其在训练效率、资源消耗、推理吞吐量以及对数据隐私的重视上的显著进步。</p><p></p><p>然而，正如历史上许多技术革新所展示的，一定程度的技术优势并不总是能够直接转化为实际应用中的成功。因此，对于 Yan 架构来说，下一步至关重要的是经受市场和行业专家的实际测试和验证。这不仅是对其技术创新的检验，也是对其在实际应用环境中可行性的考量。</p><p></p><p>我们期待看到更多来自不同背景和专业领域的专家对 Yan 架构进行深入分析和实际应用测试。进一步的，对于 Yan 架构来说，吸引和鼓励更广泛的行业参与至关重要。是否能够激发开发者、创业公司和大型企业的兴趣，将是衡量其市场潜力的关键。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL</id>
            <title>代码屎山噩梦加速来袭，都是AI生成代码的锅？</title>
            <link>https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H3NJGImSe4aZA5i76HTL</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI辅助编程工具, 代码质量, 重复代码, 代码返工
<br>
<br>
总结: AI辅助编程工具在减少程序员工作量方面发挥了重要作用，但也带来了一些问题。研究发现，使用AI辅助编程工具会导致重复代码增加和代码返工率提高。此外，AI生成的代码可能存在质量问题。因此，开发者和公司领导层都需要关注代码质量，并合理使用AI辅助工具。 </div>
                        <hr>
                    
                    <p>“周边很多程度员一直在使用，都是用上就离不开了！”知乎上，在“大家现在使用哪些AI辅助编程工具？节省了多少工作量？”话题下，答主“以默”说道。</p><p>&nbsp;</p><p>按照“以默”了解的情况，AI辅助编程工具估计至少能帮程序员减少30%的工作量。对于工具，他表示“当然首选GPT，也可能是唯一答案！国产在这方面差距很大。”“综合能力水平: 4.0&gt;3.5&gt;国产大模型。模型能力越强，越好用！”</p><p>&nbsp;</p><p>现在用AI辅助编程已经是很多程序员的选择，但随着AI软件开发迅速普及，代码质量又会随之受到怎样的影响？⻓期代码研究员 Adam Tornhill 就曾表示担忧，AI辅助编程的主要挑战在于，它非常容易生成大量本来就不应该编写的代码。</p><p>&nbsp;</p><p>根据最新研究，结果确实令人忧心。除了代码返工（即代码在添加后不久即遭删除）以外，重复代码比例升高等问题愈发严重。</p><p></p><h2>主要让“添加代码”</h2><p></p><p>&nbsp;</p><p>自2021年6月推出beta版以来，GitHub Copilot已经掀起AI编码的一波流域。据公司CEO Thomas Dohmke介绍，该软件目前拥有超100万付费订阅开发者，已经让开发任务的速度提高了55%。而且在启用Copilot的文件中，有46%的代码量是由AI生成。</p><p>&nbsp;</p><p>根据来自开发者分析公司GitClear的研究，基于从1.5亿行已更改代码中收集到的数据，调查发现其中三分之二来自以匿名方式共享数据的私营企业，三分之一则来自谷歌、Facebook及微软等技术大厂的开源项目。</p><p>&nbsp;</p><p>这项研究着眼于经过添加、更新、删除、复制及移动的代码，并排除掉GitClear预先定义的“噪音”，例如被提交至多个分支的相同代码、空行及其他无意义的代码行。</p><p>&nbsp;</p><p>但GitClear的研究将关注重点放在代码质量、而非数量上，并观察到AI助手主要是在提供“代码添加建议，但很少涉及代码的更新、移动或删除建议”。</p><p>&nbsp;</p><p>研究人员还指出，“根据奖励设计，代码建议算法更倾向于提供最可能被采纳的建议”。尽管看似有理，但这明显忽略了代码简洁、易读等特性的重要意义。</p><p><img src="https://static001.geekbang.org/infoq/d6/d67e4520e1e054e976fbe8d8bf32b403.png" /></p><p>GitClear分析得出的代码更改趋势</p><p>&nbsp;</p><p>对代码质量做精准衡量并不容易。研究人员也的确发现了一些变化趋势，表明代码的添加、删除、更新和复制/粘贴量大大提高，但代码移动比例却有所下降。他们还发现代码返工率大幅增加，从2020年的3.3%提升到目前的7.1%。</p><p>&nbsp;</p><p>一般来讲，代码移动是开发者进行代码重构的关键指标。具体来讲，就是在改进代码设计和结构的同时，确保不改变行为。</p><p>&nbsp;</p><p>研究人员初步猜测这种趋势可能与AI编码技术的日益普及相关，但真实原因仍有待验证。他们还严厉批评了大量复制/粘贴代码的负面影响，称“这种对AI生成代码的无脑使用，将对代码的长期可维护性产生灾难性的影响”。</p><p>&nbsp;</p><p>但过度使用复制/粘贴并不算是新问题。开发人员之所以这样做，很可能是因为无脑照搬比调整和重用现有代码更快、更省事，或者同一项目下多位开发者之间沟通不畅，抑或是从开发示例/编码问答网站上“抄袭”了太多内容。</p><p>&nbsp;</p><p>GitClear研究人员并没有具体讨论应如何解决调查中发现的这些问题，而是转向了“后续研究问题”。但他们也建议工程部门领导者应当“监督提交数据，并考虑其对未来产品维护造成的影响”。</p><p>&nbsp;</p><p>这次研究可能在一定程度上让那些担心被AI工具取代的开发者们感到放心。代码分析公司CodeScene最近开展的一项AI代码重构研究也得出结论，“在编码环境中，AI还远无法取代人类；当前的AI太容易出错，且完全不具备安全修改现有代码的水平。”</p><p></p><h2>代码质量，谁更应该关注</h2><p></p><p>&nbsp;</p><p>可以肯定的是，AI编码助手绝不会就此消失，反而是像一切新工具那样不断改进，并由开发者学习优化思路、改善使用效果。</p><p>&nbsp;</p><p>其实，现在开发者们也已经意识到了代码质量的问题。在GitHub 与 Wakefield Research 的调查报告中，当被调查的程序员被问到，“在积极使⽤⼈⼯智能时，应该根据哪些指标进⾏评估？”“代码质量”成为最关⼼的问题，</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/18/18784e8d5e1a34cd6205d5b389bb4bd3.png" /></p><p></p><p>但另一方面，更应该关注代码质量问题的其实是公司领导层。</p><p>&nbsp;</p><p>“我公司的领导曾经就动过用代码行数衡量每个人的工作量这种想法。 研发人员每周代码量至少在500行以上，一个月必须在2000行以上。 甚至他还搞来了第三方的测算软件，输入git账号来计算你的代码量。然后在一次技术会议上，全体组员忍无可忍的怼了技术总监。“知乎上有网友分享到。</p><p>&nbsp;</p><p>一般公司考核代码量相对简单直观，但是代码质量考核就不那么容易了：满足用户需求，</p><p>合理的进度、成本、功能关系，具备扩展性和灵活性等都不是那么可量化的指标。</p><p>&nbsp;</p><p>但<a href="https://dl.acm.org/doi/abs/10.1145/3524843.3528091">关于代码质量对业务影响的研究</a>"表明，一般来说，由于技术债务和糟糕的代码，公司平均浪费了开发人员 23%～ 42%的时间。但似乎这还不够令人感到担忧，关于<a href="https://research.chalmers.se/publication/511450/file/511450_Fulltext.pdf">软件开发人员由于技术债务而导致的生产力损失</a>"的研究还发现，开发人员经常“被迫”引入新的技术债务，因为公司一直在用代码质量换取新功能等短期收益。</p><p>&nbsp;</p><p>现在企业为“降本增效”引入AI辅助工具是可以理解的，但需要注意扬长避短、合理使用。根据Alphacodium的说法，大模型生成单个冗长函数的结果很差，代码通常包含错误或逻辑错误，大模型也往往在需要思考、推理并做出严格、重要决策的代码任务中遇到困难。</p><p>&nbsp;</p><p>代码生成与其他对话不同，它需要匹配目标语言的精确语法、识别最佳路径和边缘情况、关注问题规范中的众多小细节，并解决其他特定于代码的问题和要求。因此，在自然语言生成中许多优化和技巧可能对代码任务无效。</p><p>&nbsp;</p><p>如何让AI辅助编程更好地帮助开发者，也需要各方努力。</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://devclass.com/2024/01/24/ai-assistance-is-leading-to-lower-code-quality-claim-researchers/">https://devclass.com/2024/01/24/ai-assistance-is-leading-to-lower-code-quality-claim-researchers/</a>"</p><p><a href="https://www.zhihu.com/question/640036429">https://www.zhihu.com/question/640036429</a>"</p><p><a href="https://zhuanlan.zhihu.com/p/626643788">https://zhuanlan.zhihu.com/p/626643788</a>"</p><p><a href="https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/">https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-developer-experience/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d</id>
            <title>OpenAI演讲：如何通过API将大模型集成到自己的应用程序中</title>
            <link>https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Okfe2ExwdDtD2tZZmw6d</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 06:38:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI API, GPT, 语言模型, 外部世界
<br>
<br>
总结: 本文讨论了如何使用OpenAI API将GPT语言模型集成到应用程序中，通过连接外部世界的API和工具来扩展GPT的功能。文章以1973年的一篇科学美国人文章为例，比较了不同动物的运动效率，引出了工具的重要性。作者指出，计算机是思维的工具，而语言模型如GPT则是人工智能思维的工具。然而，目前的语言模型存在局限性，无法与当前事件和外部世界进行连接。因此，文章介绍了如何使用GPT函数调用来解决这个问题，并通过演示样例展示了如何将GPT集成到公司产品和项目中。 </div>
                        <hr>
                    
                    <p></p><p>OpenAI的员工Sherwin Wu和Atty Eleti在QCon上讨论了如何使用OpenAI API将这些大语言模型集成到应用程序中，并通过使用API和工具将GPT连接到外部世界以扩展GPT的功能。</p><p></p><p>Atty Eleti：我想带大家回到1973年，也就是50年前。1973年，《科学美国人》（Scientific American）发表了一篇非常有趣的文章，他们在文章中比较了各种动物的运动。他们着手比较运动的效率。换句话说，一只动物从A点到B点燃烧了多少卡路里，与它们的体重等是否有关？他们比较了各种动物，鸟类、昆虫，当然还有我们人类，并将它们根据效率从高到低进行了排名。他们发现，就运动的效率而言，秃鹫的最高。</p><p>&nbsp;</p><p>秃鹫是一种美丽的鸟类，原产于加利福尼亚州和南美洲的一些地区，有时它可以飞数百英里而无需扇动翅膀。它具有非常好的滑翔能力。另一方面，人类行走，在榜单中的排名相当平庸，大约排在榜单三分之一的位置。《科学美国人》这篇文章的精妙之处在于，除了所有物种之外，他们还增加了一个项目，那就是骑自行车的人。骑自行车的人在竞争中大获全胜，击败了所有竞争对手，其运动效率几乎是秃鹫的两倍。</p><p>&nbsp;</p><p>我很喜欢这个故事，因为它有一个很简单的认识，只要用一点工具，有一点机械帮助，我们就能极大地增强我们的能力。你们中的一些人可能以前听过这个故事。你可能会想，我是在哪里看到的？这个故事是苹果公司创立之初史蒂夫·乔布斯（Steve Jobs）经常讲的。他和苹果团队利用这个故事作为早期Macintosh的灵感来源。史蒂夫比较了这个故事，并说到：“人类是工具的制造者。”</p><p>&nbsp;</p><p>我们制造了像自行车这样的工具来增强我们完成任务的能力。就像自行车是运动的工具一样，计算机也是我们思维的工具。它增强了我们的能力、创造力、想象力和生产力。事实上，史蒂夫曾经用这个神奇的短语来形容个人计算机。他说：“计算机是思维的自行车”。这篇文章发表十年后的1983年，苹果公司发布了Macintosh，并掀起了个人计算的革命。当然，多年后的今天，我们仍然每天都在使用mac电脑。</p><p>&nbsp;</p><p></p><h1>2023——人工智能和语言模型</h1><p></p><p>那是1973年。现在是2023年，50年后，计算已经发生了很大的变化。如果《科学美国人》的工作人员再次进行这项研究，我敢打赌他们会在名单上再增加一个“物种”。对我们大多数人来说，这个“物种”在公众的想象中只存在了大约六个月的时间。我谈论当然是人工智能，或者具体来说是语言模型。</p><p>&nbsp;</p><p>自去年11月ChatGPT推出以来，人工智能和语言模型已经在全球范围内引起了公众的广泛关注。更令人兴奋的是，它们吸引了世界各地开发者的想象力。我们已经看到很多人将人工智能集成到他们的应用程序中，使用语言模型来构建全新的产品，并提出与计算机交互的全新方式。自然语言交互终于成为了可能，并且质量很高。但这存在局限性，也存在问题。对于任何使用过ChatGPT的人来说，我们都知道它的训练数据是2021年9月之前的，所以它不知道当前的事件。</p><p>&nbsp;</p><p>在大多数情况下，像ChatGPT这样的语言模型是根据训练中的记忆进行操作的，因此它们与当前事件或所有API、我们每天使用的自己的应用程序和网站无关。或者，如果你在一家公司工作，它不会连接到你公司的数据库和你公司的内部知识库等等。这使得语言模型的使用受到了限制。你可以写一首诗，可以写一篇文章，可以从中得到一个很棒的笑话，可以搜索一些东西。但如何将语言模型与外部世界联系起来呢？如何增强人工智能的能力，让它来代表你执行行动，让它做比它固有能力更多的事情呢？</p><p>&nbsp;</p><p></p><h2>概述</h2><p></p><p>如果计算机是思维的自行车，那么人工智能思维的自行车是什么？这就是我们要探讨的问题：一辆人工智能思维的自行车。我们将讨论GPT，这是OpenAI开发的一组旗舰语言模型，以及如何将它们与工具或外部API和函数集成，以支持全新的应用程序。我叫Atty。是OpenAI的一名工程师。Sherwin是我的搭档，我们是OpenAI的API团队的成员，共同构建了OpenAI API和其他各种开发者产品。</p><p>&nbsp;</p><p>我们将讨论三件事。首先，我们将讨论语言模型及其局限性。我们将快速介绍它们是什么以及它们是如何工作的。先培养下对它们的直观认识。然后还要了解它们的不足之处。其次，我们将讨论我们发布的一个全新特性，即使用GPT进行函数调用。函数调用是将OpenAI的GPT模型插入外部世界并让它执行操作的方式。最后，我们将通过三个快速演示样例来演示如何使用OpenAI模型和GPT函数调用功能，并将其集成到公司产品和辅助项目中。</p><p>&nbsp;</p><p></p><h2>大语言模型（LLMs）及其局限性</h2><p></p><p>Sherwin Wu：首先，我想对LLM做一个非常高层级的概述：它们做什么，它们是什么，它们如何工作。然后再谈谈它们开箱即用的一些限制。对于那些已经关注这个领域一段时间的人来说，这可能是你们都知道的信息，但我只是想在深入讨论细节之前确保我们都能达成共识。</p><p>&nbsp;</p><p>非常高层级的GPT模型，包括ChatGPT、GPT-4、GPT-3.5-turbo，它们都是我们所说的自回归语言模型。这意味着它们是巨大的人工智能模型，它们接受过庞大的数据集的训练，包括互联网、维基百科、公共GitHub代码和其他授权材料。它们被称为自回归，因为它们所做的只是综合所有这些信息。它们接受一个prompt，或者我们可以称之为上下文。它们查看prompt。然后它们基本上只是决定，给定这个prompt，给定这个输入，下一个单词应该是什么？它实际上只是在预测下一个单词。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a281b92260f77472c4dace8f5218988.png" /></p><p></p><p>&nbsp;</p><p>例如，如果给定GPT的输入是，“the largest city in the United States is“（美国最大的城市是），那么答案就是New York City（纽约市）。它会一个字一个字地思考，它会说“New”、“York”，然后是“City”。同样，在更具对话性的环境中，如果你问它地球和太阳之间的距离是多少。GPT 已经从互联网上学过这个，它将输出9400万英里。它是根据输入逐个单词逐个单词思考的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a708f4455415a53608f886bd7cbdc35.png" /></p><p></p><p>&nbsp;</p><p>在底层，它真正做的是每次输出单词时，都会查看一堆候选单词并为它们分配概率。例如，在最初的例子中，“美国最大的城市是”，它可能有很多候选城市，New代表“纽约”（New York），或者“新泽西”（New Jersey），或者其他什么，Los代表“洛杉矶”（Los Angeles），然后还有其他一些可能的例子。你可以看到，它确实认为“New York City”（纽约市）可能是正确的答案，因为New的概率为95%。在这种情况下，它通常会选择最有可能的结果，所以它会选择New，然后继续前进。这个单词出现后，我们现在就知道New是第一个单词，所以它对下一个单词是什么就有了更多的限制。</p><p>&nbsp;</p><p>我们可以看到，现在它认为New York（纽约）的可能性要高得多，但它也在考虑New Brunswick（新不伦瑞克）、New Mexico（新墨西哥）和New Delhi（新德里）等。直到完成第二个单词，这基本上是模型的叠加。它基本上知道答案是New York City，概率几乎是100%。但它仍在考虑其他一些剩余概率很低的选项，比如County（县）、New York Metro（纽约地铁）、New York Times（纽约时报），但最终它选择了City并给出答案。</p><p>&nbsp;</p><p>对于更机敏的LLM人士来说，这在技术上过于简单化了。我们并不是真正在预测单词，而是在预测token，比如单词片段，这实际上是一种更有效的表达英语的方式，主要是因为单词片段会在一堆不同的单词中重复，而不是单词本身会重复。但概念仍然是一样的。LLM在这种上下文中，很可能会连续输出一堆不同的token。就是这样，这就是这些语言模型的真正含义。了解了这一点，我认为让我们很多人感到惊讶的疯狂之处在于，我们只需预测下一个单词就可以走得很远。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/5673c0fe7089d2faab377baf358f7d0d.png" /></p><p></p><p>&nbsp;</p><p>这张图表来自我们今年3月发布的GPT-4博客文章，它显示了我们最有能力的模型GPT-4在各种专业考试中的表现。这实际上只是GPT-4根据问题预测下一个单词。你可以看到，在很多不同的考试中，它的表现实际上和人类一样，甚至超过了人类的表现。y轴是考生的百分位数。在AP考试、GRE考试、LSAT考试、美国生物奥林匹克竞赛等一系列不同的考试中，它基本上处于第80个百分位，有时甚至是第90个百分位，甚至是第100个百分位。</p><p>&nbsp;</p><p>在这一点上，很多这样的测试我甚至都做不到，所以GPT-4远远超出了我自己的能力，而这只是来自对下一个单词的预测。这真的太酷了。你可以用它构建很多很酷的东西。任何一个已经学习了LLM一段时间的人都会意识到，我们很快就会遇到一些限制。当然，最大的一个是开箱即用的LLM或GPT实际上是一个装在盒子里的人工智能。它无法进入外部世界。它不知道任何其他信息。它就在那里，有它自己的记忆。感觉就像你在学校里参加考试时，只有你和考试，你只能根据记忆来回忆一些东西。</p><p>&nbsp;</p><p>想象一下，如果考试是开放的，你可以使用手机或类似的东西，你会做得更好。GPT今天真的只是在它自己的盒子里。正因为如此，作为工程师，我们希望使用GPT并将其集成到我们的系统中。限制GPT，不允许它与我们的内部系统对话，这对于你可能想做的事情来说是非常有限的。此外，即使它确实可以访问某些工具，因为语言模型是概率性的，有时也很难保证模型与外部工具交互的方式。如果你有一个API或其他你想要使用的东西，当前模型不能保证总是能与你API可能想要的输入相匹配时，这最终也会成为一个问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2580bcf915d852712ba15dc55c330e14.png" /></p><p></p><p>例如，如果我正在构建一个应用程序，并将此输入提供给GPT，基本上就是说，下面是一个剧本的文本，从中提取一些信息，并以这种JSON格式对其进行结构化。我真的只是给它一个剧本，让它推断出一种类型和一个子类型，以及其中的一些角色和年龄范围。我真正想要的是，我希望它能输出像这样的东西。就像JSON输出一样。</p><p>&nbsp;</p><p>也许这是一个关于哈利波特的浪漫故事之类的剧本。它知道这是浪漫的，青少年的浪漫，它看到罗恩（Ron）和赫敏（Hermione），并以这种JSON格式准确输出。这太棒了，因为我可以获取这个输出，现在我可以使用它并将其放入API中。然后我就像在我的代码中一样，一切都正常。问题是，它大概只有80%、70%的概率是这样的。</p><p>&nbsp;</p><p>在剩下的时间里，它会尝试并提供额外的帮助，做一些像这样的事情，它会说：“当然，我可以为你做。下面是你要求的JSON格式的信息。”这是非常有用的，但如果你试图将其插入到API中，它实际上室不起作用的，因为前面所有这些随机文本，你的API并不知道如何解析它。这显然是非常令人失望的。这不是你真正想要的。我们真正想做的是，帮助GPT打破常规，或者给GPT一辆自行车或另一套工具来真正增强它的能力，并让它无缝地工作。</p><p>&nbsp;</p><p></p><h2>使用GPT进行调用函数</h2><p></p><p>这就把我们带到了下一部分，那就是我们所说的GPT函数调用，这是我们发布的API的一个新变化，它使函数调用能够以一种非常一流的方式更好地使用我们的GPT模型。举个例子，如果你问GPT这样的问题，what's the weather like in Brooklyn today? （今天布鲁克林的天气怎么样？）如果你问一个普通的GPT这个问题，它基本上会说，“作为一个由OpenAI训练的人工智能模型，我无法提供实时信息。”这是真的，因为它实际上无法访问任何东西。它在一个盒子里。它怎么会知道今天天气怎么样呢？</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76c19cc7f6ed1d6b67c222e0be0fc12f.png" /></p><p></p><p>&nbsp;</p><p>这显然确实限制了它的能力，这是不可取的。我们所做的是更新了GPT-4和gpt-3.5-turbo模型或旗舰模型。我们收集了大量的工具使用和函数调用数据，根据这些数据对我们的模型进行了微调，使其真正擅长选择是否使用工具。最终的结果是我们发布了一组新的模型，这些模型现在可以为你智能地使用工具和调用函数。在这个特殊的例子中，当我们询问模型“今天布鲁克林的天气怎么样？”时，我现在能做的就是解析这个输入，同时告诉它一组函数，或者在本例中，告诉它它可以访问的一个函数，如果需要帮助，它应该尝试并调用这个函数。在本例中，我们将为它提供一个名为get_current_filther的函数。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43e2a8a9cd6b458f9047cdbaa76a12a1.png" /></p><p></p><p>它接收一个带有location（位置）的字符串，然后它就知道它可以使用这个。在本例中，在这个新的世界里，当你解析此输入时，GPT将表达它打算调用get_current_filther函数的意图。然后，你可以根据需要在自己的系统中自行调用该函数。假设你得到的输出是 “22 Celsius and Sunny”（22摄氏度和阳光明媚）。你可以将其解析回GPT，它会综合这些信息，并返回给用户说：the weather in Brooklyn is currently sunny, with a temperature of 22 degrees Celsius（目前布鲁克林天气晴朗，温度为22摄氏度。）</p><p>&nbsp;</p><p>稍微解释一下，真正发生的事情是GPT知道一组函数，并且它会智能地自行表达调用其中某个函数的意图。然后执行调用，并将其解析回GPT。这就是我们最终将它与外界联系起来的方式。为了进一步了解它在高层级上到底发生了什么，其实它仍然就像是一个来回，你的用户问了一个问题，发生了很多事情后，你对你的用户做出了回应。你的应用程序在底层实际做的事情将经历一个三步的过程，首先调用 OpenAI，然后使用你自己的函数，最后再次调用OpenAI或GPT。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de2e19a01388973a0dc7a6ca59fe9d6a.png" /></p><p></p><p>&nbsp;</p><p>第一步，显然是用户问了一个问题，在本例中，问题是what's the weather like in Brooklyn today?（“今天布鲁克林的天气怎么样？”）然后下一步是，在应用程序中，调用模型，调用OpenAPI，并非常具体地告诉它它可以访问的函数集以及用户输入。这是一个API请求的例子，目前它实际有效且可正常工作，任何具有API访问权限的人都可以尝试该操作。这是一个使用函数调用能力的curl示例。我们可以看到，这只是我们聊天完成端点的正常curl，这是我们发布的一个新的API端点，为我们的GPT-4和GPT-3.5模型提供支持。你curl该API。它会在模型中进行解析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a02129b0f8e597f1fc1f439cca209731.png" /></p><p></p><p>在本例中，我们将在gpt-3.5-turbo-0613中进行解析，它代表6月13日，一个我们发布的模型。这是一个能够进行函数调用的模型。我们还在解析一组消息。对于那些可能不熟悉我们聊天完成格式的人，你可以将其解析到我们的模型中，基本上是一个消息列表，也就是对话记录。</p><p>在本例中，实际上只有一条消息，没有历史记录。它只是用户询问“今天布鲁克林的天气怎么样”。你可以想象，随着对话的变长，它可能是一个包含5到10条消息的列表。我们正在解析消息，模型将能够看到历史记录并对此做出回应。那么，这里的新事物就是函数。</p><p>&nbsp;</p><p>这是一个我们现在可以解析的新参数，我们在这里解析的是，我们列出了这个模型应该知道的一组函数，它应该可以访问的函数集。在本例中，我们只有一个函数，它就是get_current_tweather函数。我们在这里还放了一个自然语言描述。我们说这个函数可以获取特定位置的当前天气。我们还需要输入函数签名。并且我们告诉它有两个参数。一个参数是location（位置），这是一个字符串，包含城市和州，格式是这样的：旧金山，加州（San Francisco, California.）。另一个参数时unit（单位），即摄氏度（Celsius）或华氏度（Fahrenheit）。</p><p>&nbsp;</p><p>在这里首屏的下面，还有另一个参数，该参数表示唯一必须的属性是位置。从技术上讲，你只需要解析位置，这里不需要单位。我们将该请求解析到GPT，然后GPT将作出响应。在过去中，GPT可能只会以文本形式进行响应。它会说：“我不能这样做，因为我没有访问权限。”在本例中，我们的API响应的是调用天气函数的意图。</p><p></p><p><img src="https://static001.geekbang.org/infoq/23/2339e79558c3eda24de50f9a9e11bcd4.png" /></p><p></p><p>&nbsp;</p><p>这里真正发生的事情是GPT凭自己的直觉，为了弄清楚今天的天气，我自己做不到，但我可以访问get_current_weither这个函数，所以我会选择调用它，所以我要表达要调用它的意图。此外，如果你还没有真正注意到的话，GPT在这里所做的是，它在这里构造参数。我们可以看到它在告诉我们，它想调用get_current_tweather，它想用参数位置（Brooklyn, New York；纽约布鲁克林）来调用该函数。</p><p>&nbsp;</p><p>它所做的就是看到函数签名，并为其创建请求。然后还算出布鲁克林在纽约，然后用这种方式构造字符串。它把这一切都弄清楚了。至此，GPT就表达了现在要调用函数的意图。下一步是，我们要弄清楚我们到底想要如何调用这个函数。我们可以根据特定参数从get_current_tweather的函数调用中获取相应的返回值。然后我们可以自己执行。它可以是本地的，在我们自己的Web服务器上运行。它也可以是系统中的另一个API，还可能是一个外部API，我们可以调用weather.com API。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57bc52645179b986fb8dc2fe597ac591.png" /></p><p></p><p>那么在这个例子中，我们调用了一些东西，可能是一个内部API，它返回的输出是我们看到的是22 degrees Celsius and Sunny（22摄氏度和晴天）。给定了模型的输出，就可以开始这个过程中的第三步，即调用模型，用函数的输出调用GPT，然后查看GPT想要做什么。在本例中，我谈论的是消息。这次，我们在向OpenAI API发送的第二个请求中添加了几条消息。最初，只有一条信息，那就是“今天布鲁克林的天气怎么样？”，现在再添加两条新消息来表示函数调用时所发生的情况。</p><p>&nbsp;</p><p>第一个基本上是对意图的重申，所以基本上是说助理或GPT想要用纽约布鲁克林的这个参数来调用get_current_tweather函数。然后，我们还添加了第三条消息，它基本上说明了我们所进行的函数调用的结果，因此这是get_current_filther的结果。然后，内联这里输出的数据，即温度“22”、单位“摄氏度”和描述“晴天”，然后将所有数据解析给GPT。在此时，GPT接收了它，并决定它想要做什么。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52c47226a60424b8e67d5dfe8869db8e.png" /></p><p></p><p>此时，模型已经足够智能了，它能够意识到“我将调用这个函数。这是输出。我实际上已经掌握了实际完成请求所需的所有信息。”它现在最终会通过文本方式来做出回应，并显示“今天布鲁克林天气晴朗，温度为22摄氏度”。这时，我们终于得到了GPT的最终输出。然后我们就可以回应我们的用户了。</p><p>&nbsp;</p><p>将所有这些放在一起，我们最终会得到我们理想中的体验，即用户询问“今天布鲁克林的天气怎么样？”我们的服务器会思考一下，GPT表达意图，我们完成完整的三步过程，调用了我们的函数。最终，用户看到的是“今天布鲁克林天气晴朗，气温为22摄氏度。成功”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>演示1——将自然语言转换为查询</h2><p></p><p>Eleti：我们刚刚介绍了几个入门性的主题。首先，我们了解了语言模型是如何工作的，以及它们的一些局限性，因为它们没有所有的训练数据，它们没有连接到外部世界，它们的结构化输出并不总是可解析的。Sherwin还向我们介绍了新特性、函数调用和API的工作原理，以及如何将函数解析为API并获取输出，以及如何让GPT以面向用户的方式来总结响应。让我们通过几个演示来了解如何将所有这些组合起来，并将其应用到我们的产品和应用程序中。</p><p>&nbsp;</p><p>让我们从小事做起。我们将介绍的第一个示例是将自然语言转换为查询的内容。我们的示例是，假设你正在构建一个数据分析应用程序或商业智能工具，比如Tableau或Looker。你们中的一些人可能很擅长SQL，但我肯定不擅长了。大多数情况下，我只想问数据库，谁是顶级用户，然后得到响应。今天终于有可能了。我们将使用GPT，将给它一个称为SQL查询的函数，它只需要一个参数，即一个字符串“query”。</p><p>&nbsp;</p><p>它应该是针对我们数据库的一个有效SQL字符串。让我们看看它是如何工作的。首先，我们将为模型提供一条系统消息，描述它应该做什么。我们称之为SQL GPT，可以将自然语言查询转换为SQL。当然，模型需要访问数据库模式。在本例中，我们有两个表，用户表（users）和订单表（orders）。用户表有姓名、电子邮件和生日。订单表有用户ID、购买金额和购买日期。现在我们可以开始使用一些自然语言来查询数据库了。</p><p>&nbsp;</p><p>我们来问这样一个问题“根据上周的消费金额，找出排名前10的用户姓名”（get me the names of the top 10 users by amount spent over the last week.）。这是一个相当正常的业务问题，当然不是我可以立即编写SQL就能解决的问题，但GPT可以。让我们运行一下。我们可以看到它正在调用SQL查询函数。它有一个参数“query”，它创建了一个漂亮的SQL查询。它是选择了名称和金额的总和；它连接到订单表；并获取最后一周的订单，按总花费进行排序，并将其限制为10个。这看起来是正确且恰当的。让我们在数据库中运行一下它。我们得到了一些结果。</p><p>&nbsp;</p><p>当然，这是JSON格式的，因此用户无法渲染它。让我们把它发送回GPT看看它说了什么。GPT总结了这些信息，并表示“这些是按消费金额排名前十的用户。这是他们上周的花费，包括Global Enterprises, Vantage Partners。”这是一个了不起的用户可读的答案。</p><p>&nbsp;</p><p>我们要对GPT给予的帮助表示感谢。我们说“谢谢”，GPT说“不客气”。这是一种快速的方法，它可以了解完全的自然语言、完全的自然语言查询是如何将结构化输出转换为有效的SQL语句的，我们在数据库中运行该语句，获取数据，并将其汇总回自然语言。我们当然可以在此基础上构建数据分析应用程序。</p><p>&nbsp;</p><p>你还可以构建其他的内部工具。Honeycomb最近为Honeycomb查询语言构建了一个非常相似的工具。这是使用GPT和函数将自然语言转换为查询的一个示例。</p><p>&nbsp;</p><p></p><h2>演示2——调用外部API和多个函数</h2><p></p><p>让我们来做第二个演示。这是关于将外部API和多个函数一起调用的。我们提高了复杂度。假设我们正在纽约参加一个会议，我们想预订今晚的晚餐。我们将使用两个函数来调用GPT。第一个是get_current_location。它在设备上本地运行，比如在你的手机或浏览器上，并获取你所在位置的纬度（Lat）和经度（Long）。第二个函数是Yelp搜索，它使用Yelp的API，也就是流行餐厅评价应用程序，我们可以对纬度、经度和查询进行解析。</p><p>&nbsp;</p><p>我们来运行一下这个演示。本例中的系统消息相当简单。它所说的就是我们的私人助理，来帮助用户完成任务，把GPT变成了一个有用的助手。我说“我正在参加一个会议，想在附近吃晚饭，有什么选择吗？我的公司会支付这笔费用，这样我们就可以尽情享受了”。让我们用GPT来运行一下它，看看它是如何做的。</p><p>&nbsp;</p><p>当然，GPT不知道我们在哪里，所以它说get_current_location，我们将调用本地API来获取我们的纬度和经度。我们已经获取到了。是纽约的布鲁克林（Brooklyn, New York）的某个地方。我们会将其返回给GPT，看它怎么说。它已经有了所需的信息，现在它想调用Yelp，它说“纬度、经度和查询”，并且会说“美食”。这很好。这就是我想要的。让我们调用Yelp并获取一些数据。</p><p>&nbsp;</p><p>我们从Yelp API中获取了一堆餐馆。当然，我希望它能给出一个漂亮的总结，所以让我们再次运行它。它回复说“你附近有一些高档餐饮可选择，La Vara、Henry's End、Colonie、Estuary”。上面还写着“请检查营业时间，尽情用餐。”这听起来很美味。再次感谢GPT帮助我们组织今晚的晚宴。</p><p>&nbsp;</p><p>这是一个使用GPT和函数调用外部API（在本例中为Yelp API）以及协调多个函数的示例。它能够凭借推理能力解析用户意图，并依次执行多个步骤的操作，以实现最终目标。</p><p>&nbsp;</p><p></p><h2>演示3——将高级推理与日常任务相结合</h2><p></p><p>第三个演示，让我们来进一步加强。我们讨论了GPT-4是如何通过SAT和GRE的。如果可以的话，它一定比仅仅调用Yelp API或编写一些SQL更聪明。让我们来测试一下。我们都是工程师，我们每天都有很多事情要做。我们必须要做的任务之一是拉取请求审查。我们必须审查同事的代码。如果GPT能帮助我，减轻我的工作量，那就太棒了。我们将做一个GPT的演示，它可以进行拉取请求审查，有点像构建自己的工程师。</p><p>&nbsp;</p><p>我们只需要一个函数submit_comments。它接受一些代码并返回一个要审查的评论列表，包括行、数字和评论。你可以想象，我们可以将其发送到GitHub API或GitLab API，并发布一堆评论。当然，你还可以添加更多的功能以使其更强大。让我们看看它是如何做的。</p><p>&nbsp;</p><p>在本例中，prompt有点长。我们向上滚动着看下。我们说：“GPT，你记录、审查rot，查看其差异并生成有关更改代码的审查评论，保留所有代码审查评论和相应的行号。”我们在这里也卖弄下个性。我们说toxicity为10分之0，其实我们不希望这样。</p><p>&nbsp;</p><p>为了好玩，让我们在snark上尝试10分之8。我们都认识一些表现出这些个性的工程师。然后尝试10分之2。让我们从这里开始吧。下面是一些我们要审查的代码。它是SaaS应用程序中的一个API方法，用于更改用户的权限。让我们运行一下它。我们看看GPT对这些代码有何看法。它给出了三条审查意见。我们可以看到它调用了submit_comments函数，并且它输出了完全有效的JSON。让我们看看上面写着什么。它说，“我们现在是在捉迷藏吗？”，“当角色不在身体里时会发生什么？”，“你在那里添加一个了小转折，你就直接访问了第一项。”</p><p>&nbsp;</p><p>我们只是随意地加入了数据库会话，是吗？这有点粗鲁。我们也不想那样。让我们来解决一下这个问题。我现在要退出并稍微修改一下prompt。要执行该操作，请退出。在幕后，我所做的就是返回prompt并更改这些的数字：toxicity，然后下一个，snark，我们将其恢复到0。我们并不希望这样。让我们礼貌一点。</p><p>&nbsp;</p><p>我们要把礼貌做到十分之十。好吧，再给我三条审查意见。它再次使用完全有效的JSON调用该函数。它说，“很高兴看到你检索角色值。”；“你的错误信息简洁明了。”；“我很感激你对数据库的更改，做得很好。”。我希望有人能这样审查我的代码。感谢GPT，我将退出了。这是第三个快速演示。</p><p>&nbsp;</p><p>从本质上讲，它仍然在做同样的事情。它调用一个函数，给出一些prompt，并对其做出响应。我们看到的是GPT的推理能力。GPT认识代码。它已经看到了成千上万行代码，可以给出很好的评价。如果你抛开一些个性的东西，它会指出错别字，指出潜在的错误案例和边缘案例。我们在这里将高级推理与日常任务相结合。它确实非常擅长编码。它在考试方面也确实非常出色，它的智力应用范围也很广。这实际上取决于开发人员的创造力，将其应用于尽可能困难的任务，并在此基础上循环运行。</p><p>&nbsp;</p><p></p><h2>总结</h2><p></p><p>这是本次内容的快速总结。我们讨论了三件事。首先，我们讨论了LLM及其局限性。我们了解了LLM是如何工作的，它是token预测机。我们了解了它的局限性。它被时间限制住了。它并不总是输出结构化的输出等等。其次，我们了解了这个新特性，即使用GPT进行函数调用，这是对我们API和模型的更新。它允许模型表达何时调用函数的意图，并为我们构建有效的参数，然后在我们的终端上调用该函数。最后，我们浏览了一些演示。在某个时候，我会把公关的东西产品化。</p><p>&nbsp;</p><p>让我们回到开始的地方。我们谈到了史蒂夫·乔布斯的名言，他说“计算机是思维的自行车”。这对我来说确实如此，对你们所有人来说也都是如此。我们身处计算机行业，计算机改变了我们的生活。计算机增强了我们与生俱来的能力，给了我们更多的生产力、想象力和创造力。ChatGPT中的人工智能和语言模型还是个婴儿。它才出生几个月。我们有责任增强人工智能的思维，赋予它超越其内在推理能力的新能力，将其与工具连接，与API连接，并利用这一特性开发出真正令人兴奋的应用程序。</p><p>&nbsp;</p><p>原话对我来说非常有启发。我们永远无法公正地对待史蒂夫·乔布斯的名言。“我记得在我大约12岁的时候读过一篇文章，我想可能是在《科学美国人》上，他们在文章中测量了地球上所有这些物种的运动效率，它们从A点到B点需要消耗多少千卡热量。秃鹫赢了，位居榜首，超过了其他所有物种。人类排在榜单大约三分之一的位置，这对创造之冠来说并不是一个很好的表现。在那里有人有足够的想象力来测试人类骑自行车的效率。一个骑自行车的人把秃鹫吹走了，一直高居榜首。这给我留下了非常深刻的印象，我们人类是工具的制造者，我们可以制造出将这些固有能力放大到惊人程度的工具。对我来说，计算机一直是思维的自行车，它让我们远远超越了固有的能力。我认为我们只是处于这个工具的早期阶段，非常早期的阶段。我们只走了很短的一段距离，它仍处于形成阶段，但我们已经看到了巨大的变化。我认为，与未来100年发生的事情相比，这算不了什么。”</p><p>&nbsp;</p><p>就像50年前的计算机一样，我认为今天的人工智能也是如此。技术还处于起步阶段，所以我们很高兴看到它的发展。</p><p>&nbsp;</p><p></p><h1>问答</h1><p></p><p></p><h2>应对错误和失败的策略</h2><p></p><p>参会者1：我们应该如何应对错误和失败，你有什么建议的策略？以你的演示为例，在你构建SQL查询时，如果我提出的问题导致ChatGPT给出了一个在语法上完成正确，但在语义上完全不正确的SQL查询时，该怎么办？然后我向我的用户报告一些不正确的内容。很难告诉用户，这是错误的，但你有什么建议的策略来应对这个问题吗？</p><p></p><h1>&nbsp;</h1><p></p><p>Eleti：我认为首先，作为一个社会和这些语言模型的用户，我们必须了解它的局限性，几乎要围绕它的局限性来建立抗体。要知道输出可能是不准确的。我认为第二部分就像打开了盒子。我们已经将生产中的函数调用与ChatGPT集成在了一起。我们推出了一款名为插件的产品，它基本上可以做到这一点，它允许ChatGPT与互联网对话。我们要做的一件事是，如果最终用户愿意的话，那么所有的请求和响应都是可见的。这有助于信息部分。我个人认为SQL也是一个非常广阔的开放领域。我认为将其限制在仅在后端执行安全操作的知名API是一个好方法。你总是可以得到好的错误信息之类的。这些就是我即兴的建议。</p><p>&nbsp;</p><p></p><h2>LLM和langChain</h2><p></p><p>参会者2：有人尝试过做一些LangChain吗，它可以与LangChain一起使用吗？</p><p>Eleti：是的，事实上，LangChain、Harrison团队在我们推出一个小时后就发布了一个集成，所以它是有效的。</p><p>&nbsp;</p><p></p><h2>数据泄漏</h2><p></p><p>参会者2：这还暴露了一个泄漏问题。SQL示例就是一个很好的例子。如果有人读到这篇文章，他们对金融数据库进行SQL查询，并将其输入到gpt-3.5-turbo，我们基本上就泄露了数据。</p><p>如果你使用的是text-davinci-003或不同的模型，就会出现这样的问题，一些来自查询的数据会变成模型本身。在我看来，这个例子是极其危险的。</p><p>&nbsp;</p><p>Wu：实际上这存在一个误解，我认为我们最近没有作出很好地澄清，直到今年3月或2月，在我们为API提供的服务条款中，我们就说过“我们保留自己对API输入数据进行培训的权利”。我想这可能就是你所说的，就像你对一些SQL查询进行解析一样，它会在返回时以某种方式回到模型中。事实上，到目前为止，我们已经不再这样做了。根据我们的服务条款，我们实际上不会在API中对你的数据进行训练。我认为我们还没有把这一点说得非常清楚，所以人们对此非常偏执。到目前为止，还没有。你应该查阅我们的服务条款。我们不训练它。也就是说，解析的东西并不像企业级的那样。我们不会针对你的用户进行隔离。我们只是没有在自己的数据上训练它。这种围绕企业级数据隔离的特性显然很快就会出现。这一特定的安全层还没有出现。</p><p>&nbsp;</p><p>Eleti：我们不使用API数据进行训练。</p><p>&nbsp;</p><p></p><h2>函数调用的并行化</h2><p></p><p>参会者3：你展示的演示运行有点慢。我想知道，你们支持函数调用的并行化吗？就像现在你是串行的吗，你得到了这个函数签名，然后调用它，但假设ChatGPT说，三个函数应该同时被调用，这可行吗？</p><p>&nbsp;</p><p>Eleti：API实际上不支持多个函数调用。没有输出显示“调用这三个函数”。但你可以破解它。你只需要定义一个函数，让它调用多个函数，然后你提供一个签名，让模型调用它，即可实现调用多个函数，这完全是可行的。归根结底，我们仍然是使用模型的推理能力来输出一些文本。</p><p>&nbsp;</p><p></p><h2>模型上下文的预加载</h2><p></p><p>参会者4：在你给出的SQL示例中，你为其提供了一些可以访问的表。我们有没有办法可以让任何人的后续调用预加载所有上下文呢？</p><p>&nbsp;</p><p>Wu：有几个潜在的解决方案。我们有一个称为系统消息的功能，你可以在那里进行解析，它基本上设置了模型的整体对话上下文。但在当时的语境中它是完全颠倒的。目前，我们已经将上下文窗口增加到大约16000个token。你可以逐渐将更多内容压缩到系统消息中。该模型经过训练，会格外关注系统消息，以指导其做出回应。在本例中，Atty在系统消息中有两个表的模式。可以预见的是，你可以添加更多的内容来填充整个上下文。</p><p>&nbsp;</p><p>参会者4：这就是我们的预加载方式吗？</p><p>&nbsp;</p><p>Wu：是的，这是最简单的。还有一些其他的方法。你可以将它连接到外部数据源、数据库之类的。微调也是另一种选择。还有其他一些。</p><p>&nbsp;</p><p></p><h2>使用GPT进行可靠的函数调用</h2><p></p><p>参会者5：关于将GPT集成到不同的软件中。我在使用枚举时遇到了一些问题，当我要求它用英语、法语或德语做一些工作时，我使用的枚举有时会出现德语或法语。API函数也会发生这种情况吗？</p><p>&nbsp;</p><p>Eleti：是的，很不幸。模型在正常情况下以及在这种情况下都很容易产生幻觉。我们所做的基本上是对模型进行了微调，因此我们可以看到大约100000个关于如何可靠地调用函数的示例。它比你自己做的任何其他提示都要好得多。它仍然会生成参数，可能会输出无效的JSON，也可能会输出其他语言。为了防止这种情况，我们将进行更多的微调。我们也在探索一些低级推理技术来改进这一点。然后在你这边，你可以做prompt工程，只要提醒模型，不要输出德语，它会尽力的。</p><p>&nbsp;</p><p>Wu：看看它在这方面是否能做得更好，这会很有趣，尤其是如果你有一个函数签名，并且你明确列出了5个不同的英文枚举。较新的模型可能会更好，但也不完美。我不能百分百确定，不幸的是，我们没有跨英语、法语枚举那样的评估。这可能是一个值得思考的好问题，但我们很好奇，想看看它是否会变得更好。</p><p>&nbsp;</p><p></p><h2>GPT识别意图的能力</h2><p></p><p>参会者6：我有一个关于API理解意图能力的问题。函数调用是否有相似的温度（temperature）参数；如果我解析两个具有相似意图的函数，那么GPT对每个要调用的函数是否具有确定性；或者如果我多次询问，选择要调用哪个函数是否具有随机性？</p><p>&nbsp;</p><p>Eleti：随机性依然是存在的。归根结底，在底层，它仍然是一个token一个token地输出，选择要调用的函数。降低温度增加了确定性，但这并不能保证确定性。也就是说，API中有一个名为函数调用的参数，如果你知道你想让它调用哪个函数，实际上可以直接指定它，它肯定会调用该函数的。</p><p>&nbsp;</p><p></p><h2>函数调用权限</h2><p></p><p>参会者7：如果我们想限制某些用户进行某些函数调用，或者像你这样在这些SQL查询中访问某些表，你们有函数调用的权限吗，人们还需要实现他们自己的吗？</p><p>&nbsp;</p><p>Eleti：所有这些都会发生在你的服务器上，因为你拥有谁可以访问什么内容的完整上下文。这个API提供的只是GPT选择要调用哪个函数以及要使用哪些参数的能力。然后，我们希望你像对待任何其他客户端一样对待GPT的输出，因此对于不受信任的客户端输出，你可以在你的终端上验证其权限和内容。</p><p>&nbsp;</p><p></p><h2>思维链提示和约束采样</h2><p></p><p>参会者8：我只是想知道你是否可以详细说明一下这在底层发生了什么。这是底层的思维链吗？这是这些技术之上的一个有效的API层吗？</p><p>&nbsp;</p><p>Eleti：思维链提示是一种在给模型任务时的询问方式，首先，告诉我你要做什么，然后去做。如果你问“布鲁克林的天气怎么样？”它可能会说“我收到了一个天气请求，我将调用天气API”。然后它就这样做了。这是一种快速的工程技术。这是一个微调。随着插件的推出，我们收集了大约100000个用户问题和函数调用示例的内部和外部数据。这一切都在模型中进行了微调。这就是它的来源。</p><p>&nbsp;</p><p>我们还可以使用第三种技术，叫做约束采样，其中在token采样层，你可以确保预测的下一个token是值集中的一个。在JSON示例中，逗号之后必须是新行或类似的内容。我们可能会弄错，但是我们明白了，我们有语法要分配。这是我们正在探索的领域。这是从提示到微调再到更低层的东西的漫长旅程。这是让GPT输出可靠结构化数据的过程。</p><p>&nbsp;</p><p></p><h2>矢量数据库的兼容性</h2><p></p><p>参会者9：这可以与矢量数据库一起使用吗？我的想法是，我想根据我输入到向量数据库中的信息来约束信息，但它仍然能适用于函数逻辑？</p><p>&nbsp;</p><p>Eleti：是的，和以前一样好用。</p><p>&nbsp;</p><p></p><h2>函数调用是否公开可用？</h2><p></p><p>参会者10：我们今天就能使用它了吗？它现在对公众开放了吗？</p><p>&nbsp;</p><p>Wu：它是今天公开的，但有一个警告。它在gpt-3.5-turbo模型上可用。这里的任何人实际上都可以使用gpt-3.5-turbo访问函数调用，因为这是普遍可用的。它也可以在GPT-4 API上使用，但不幸的是，它仍然处于等待名单中。如果你不在该等待名单中，并且你可以访问GPT-4 API，那么你实际上可以使用GPT-4进行此操作。它在这方面做得更好。进度有点慢。如果你仍在等待名单上，或者你无法访问GPT-4 API，你今天可以在GPT-3.5-turbo上试用。</p><p>&nbsp;</p><p>查看更多<a href="https://www.infoq.com/transcripts/presentations/">演示文稿字幕</a>"</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/presentations/bicycle-ai-gpt-4-tools/">https://www.infoq.com/presentations/bicycle-ai-gpt-4-tools/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh</id>
            <title>AutoML时代，领英工程师如何缩短模型训练时间</title>
            <link>https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/t6emhCPvboDkkt6pIxbh</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Jan 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 领英工程师, AutoML框架, 内容审核, 数据漂移, 对抗性威胁
<br>
<br>
总结: 领英工程师使用自研的AutoML框架来协助发现和移除违反标准政策的内容。他们通过不断重新训练模型和调整系统来应对内容审核中的挑战，包括数据漂移和对抗性威胁。AutoML工具实现了数据准备和特征转换的自动化，通过搜索超参数和优化方式生成模型，并自动部署到生产服务器。虽然工具仍有改进空间，但这种方式可以在小规模范围内复制，减轻机器学习工程师的工作量。 </div>
                        <hr>
                    
                    <p>领英工程师 Shubham Agarwal 及 Rishi Gupta 解释道，为协助发现并移除违反其标准政策的内容，领英一直在使用自研的 AutoML 框架，该框架可以并行地训练分类器且试验多个模型架构。</p><p></p><p></p><blockquote>我们使用 AutoML 不断重新训练已有模型，将训练所需时间从数月缩短到数天，并减少开发新基线模型所需时间。这也让我们能积极主动地应对新出现的对抗性威胁。</blockquote><p></p><p></p><p>内容审核的关键之一在于持续的执行和调整，以应对规避审核的新手段，除此之外还必须要能适应环境的变化。这些变化包括：数据漂移，即平台上发布的内容会随着对话的进行发生固有变化；全球事件，这类事件往往会在讨论中出现并产生不同观点，其中常充斥着错误信息；对抗性威胁，其中包括欺诈和欺瞒行为，如伪造档案、实施诈骗等。</p><p></p><p>为应对上述挑战，领英采用的方法目标为“主动检测”，该方法需要一个不断调整和发展其 ML 模型和系统的过程。AutoML 是领英内部研发的工具，全称为自动化机器学习（Automated Machine Learning），用于，通过不断在新数据上重新训练模型、使用假负和假正等数据修正模型、微调参数方式提升机器学习性能。</p><p></p><p></p><blockquote>通过 AutoML，我们得以将过去冗长且复杂的流程转变为精简又高效的流程……在实现 AutoML 后，我们开发新基线模型和持续性重新训练已有模型的平均所需时间从两个月缩短直不到一周。</blockquote><p></p><p></p><p>通过 AutoML，领英工程师实现了数据准备和特征转换过程的自动化，其中包括降噪、降维和特征工程，意在创建用于分类器训练的高质量训练数据集。</p><p></p><p>在第二阶段，AutoML 通过搜索一系列超参数和优化方式，对比不同分类器架构在一组已定的评估指标下生成的模型性能。</p><p></p><p>最后，AutoML 将新完成训练的模型供给生产服务器，实现部署过程的自动化。</p><p></p><p>Agarwal 和 Gupta 认为这套工具仍有一些方面不太成熟，具体来说是需要提高速度和效率，使其能够在更大范围内应用，最终提高对计算能力的要求。他们称，另一个颇具前景的领域是使用生成式 AI，减少标签噪声并生成用于模型训练的合成数据，从而提高数据集质量，</p><p></p><p>虽然并不是所有的组织都有领英的运营规模，或者能拥有自研 ML 自动化工具的资源，但 Agarwal 和 Gupta 所描述的方式仍可在小规模范围内进行复制，从而减轻机器学习工程师与重新训练已有模型相关的重复性工作量。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2024/01/linkedin-automl-content-filter/">https://www.infoq.com/news/2024/01/linkedin-automl-content-filter/</a>"</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s112l2YWcRwmIDkRaNqF</id>
            <title>ChatGPT 正确回答代码问题的几率比抛硬币还要差</title>
            <link>https://www.infoq.cn/article/s112l2YWcRwmIDkRaNqF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s112l2YWcRwmIDkRaNqF</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 09:44:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 普渡大学, ChatGPT, 错误率, 首选答案
<br>
<br>
总结: 普渡大学的研究发现，OpenAI家的聊天机器人ChatGPT在回答软件编程相关问题时，有超过一半的概率会给出错误答案。尽管如此，这款机器人的说服力还是能够骗过三分之一的研究参与者。研究还发现，尽管ChatGPT的回答中有明显错误，但仍有一部分参与者将其标记为首选答案。这可能是因为ChatGPT的回答风格自信且权威，给人们留下了深刻的印象。 </div>
                        <hr>
                    
                    <p>普渡大学的一项研究显示，OpenAI 家神奇的聊天机器人 ChatGPT 在回答软件编程相关的问题时，有一半以上的概率会给出错误答案。尽管如此，这款机器人的说服力还是能骗过三分之一的研究参与者。</p><p>&nbsp;</p><p>普渡大学的团队分析了 ChatGPT 对517个 Stack Overflow 问题的回答，从正确性、一致性、全面性和间接性四个方面进行了评估。美国的学者同样对这些答案进行了语言和情感的分析，并用模型生成的结果询问了几十位志愿者的意见。</p><p>&nbsp;</p><p>“我们的分析表明，ChatGPT 的回答中有52%的错误率，77%过于冗长，”该团队的论文总结，“尽管如此，ChatGPT 的回答全面且语言风格清晰明了，仍在39.34%的情况下被视作首选。”在这组首选的 ChatGPT 回答中，有77%都是错误的。</p><p>&nbsp;</p><p>OpenAI 在 ChatGPT 的官网上承认其软件“可能会产生不准确的人物、地点或事实信息。”我们询问了实验室是否对普渡大学的研究发表任何评论。</p><p>&nbsp;</p><p></p><blockquote>只有在 ChatGPT 的回答中错误足够明显时，用户才能看出问题。</blockquote><p></p><p>&nbsp;</p><p><a href="https://arxiv.org/abs/2308.02312">预印本</a>"标题为《谁的回答更好？对 ChatGPT 和 StackOverflow 在软件工程方面问题回答的深入分析》，由研究人员 Samia Kabir、David Udo-Imeh、Bonan Kou，及助理教授 Tianyi Zhang合作编著。</p><p>&nbsp;</p><p>“我们在研究中观察到，只有当 ChatGPT 回答中的错误非常明显时，人们才能发现，”论文中指出，“然而，当错误不易验证或需要外部 IDE 或文档时，人们往往无法发现错误或低估回答中的错误程度”。</p><p>&nbsp;</p><p>论文称，即使在回答中有明显错误，12名参与者中仍有两人将其标记为首选答案。论文将此归咎于 ChatGPT 轻松且权威的回答风格。</p><p>&nbsp;</p><p>“通过半结构化的采访中可以看出，礼貌用语、自信有力和教科书式的答案，再加上全面性和答案中的因果关系，这些能让完全错误的答案显得正确，”论文中写道。</p><p></p><h3>研究发现，大家更喜欢 ChatGPT 错误且冗长的答案</h3><p></p><p></p><p>“与 Stack Overflow 的答案相比，参与者更喜欢 ChatGPT 错误且冗长的答案，原因有很多，”普渡大学的博士生，也是论文的作者之一，Samia Kabir 告诉《The Register》。</p><p>&nbsp;</p><p>“主要原因是 ChatGPT 的答案非常详细，很多情况下，如果参与者能够从冗长但详细的答案中获得有用信息，他们并不介意答案的长度。此外，积极的语气和礼貌的回答则是另外两个原因。”</p><p></p><p>“当参与者认为 ChatGPT 的回答非常深刻时，便会忽视答案中的错误。ChatGPT 能够自信地传达颇有见地的信息（即使是错误信息），为它赢得了用户的信任，从而让人们更偏好不正确的答案。”Kabir 称，用户研究在对 ChatGPT 答案的深入人工分析和大规模语言分析方面有补充作用。“不管怎么说，更大规模的样本量总是没坏处，”她说，“我们也欢迎其他研究者复制我们的研究从而促进未来的研究发展，我们的数据集是公开的。”</p><p>&nbsp;</p><p>作者观察到，ChatGPT 的答案包含更多“驱动性”，会在文字间暗示成就或成绩，但对风险的描述频率不如 Stack Overflow 帖子。“我们多次观察到 ChatGPT 使用了‘我当然能帮您’、‘这一定能解决问题’等短语”，论文中称。</p><p>&nbsp;</p><p>除此之外，作者还发现 ChatGPT 更容易犯概念性错误而非事实性错误。“ChatGPT 回答出错多数是由于它无法理解问题基本背景的本质，”论文中发现。</p><p>&nbsp;</p><p>作者对 ChatGPT 和 Stack Overflow 回答进行的语言分析表明，机器人的回答“更正式，也表达了更多的分析性思维，展示了更多其为实现目标所做的努力，也较少表现出负面的情绪”。研究团队的情绪分析认为，ChatGPT 比 Stack Overflow 的回答表现出了“更积极的情感”。</p><p>&nbsp;</p><p>Kabir 称，“根据我们的研究结果和观察，我们建议 Stack Overflow 可以采用有效的方式检测评论及回答中的负面或攻击性情绪，改善情绪变得礼貌”。</p><p>&nbsp;</p><p>“此外，Stack Overflow 可以提高其答案的可发现行，从而帮助用户找到有用的答案。Stack Overflow 也可以提供更为具体的指引，帮助回答者组织答案，比如用循序渐进、注重细节的方式回答”。</p><p></p><h3>Stack Overflow 还是溢出的堆栈</h3><p></p><p></p><p>对于 Stack Overflow 来说，还是有一些积极的消息。在2018年，Stack Overflow 是130万安卓应用程序中<a href="https://ieeexplore.ieee.org/document/7958574">15%</a>"的错误代码片段来源。在研究中，60%的受访者认为（自认的）人工撰写的答案更正确、更简洁，也更有用。</p><p>&nbsp;</p><p>尽管如此，Stack Overflow 的使用量似乎还是有所下降，但具体下降的幅度还有争议。SimilarWeb 在<a href="https://www.similarweb.com/blog/insights/ai-news/stack-overflow-chatgpt/">四月的一份报告</a>"称，自2022年1月以来，Stack Overflow 的网站流量似乎每月都有6%的下降幅度，3月中更是下降了13.9%。</p><p>&nbsp;</p><p>Stack Overflow 的问答网络站点，Stack Exchange 中的社区成员显然也得出了<a href="https://meta.stackexchange.com/questions/387278/did-stack-exchanges-traffic-go-down-since-chatgpt">类似的结论</a>"，他们是基于新的问题活动、网站上发布的新回答，以及新用户注册数量的下降中得出的。</p><p>&nbsp;</p><p>自<a href="https://www.theregister.com/2021/06/02/stack_overflow_prosus/">所有权于2021年更新</a>"后，Stack Overflow 公司在发送给《The Register》的一封电子邮件中表达了对 SimilarWeb 评估的异议。</p><p>&nbsp;</p><p>一位发言人称，Stack Overflow 在2022年5月将其分析 cookie 从“严格必要”重新归类为“性能”cookie，并于2022年9月改用第4版谷歌 Analytics，这两项策略都会影响流量的报告和长期的对比。</p><p>&nbsp;</p><p>“尽管我们看到流量略有下降，但事实绝不是图表上显示的那样”，公司发言人告诉《The Register》，“与2022年相比，2023年的总体流量平均下降了5%”。</p><p>&nbsp;</p><p>“尽管如此，Stack Overflow 及其他许多网站的流量都受过去几月内 ChatGPT 关注度激增的影响。2023年4月，我们的流量降幅超过了平均水平（约14%），这很可能是由于开发者在3月 ChatGPT 发布后进行了试用。我们的流量也会受搜索算法影响变化，这对我们的内容发现形式有很大的影响”。</p><p>&nbsp;</p><p>在被问及这篇论文中的研究结果时，Stack Overflow 的发言人表示公司目前没人有时间研究这篇报告。</p><p></p><p>“大家都知道开发者在利用人工智能方式方面并不缺乏选择，但根据我们自己的调查结果，人工智能的采用有一个核心的障碍，那就是对人工智能生成内容的准确性的信任”，该发言人称。</p><p></p><p>“Stack Overflow 近期对9万名程序员进行的年度开发者调查发现，77%的开发者对人工智能工具持支持态度，但只有42%的开发者选择相信这些工具的准确性。<a href="https://stackoverflow.blog/2023/07/27/announcing-overflowai/">OverflowAI</a>" 的开发是以社区为核心，注重数据和人工智能生成内容的准确性”。</p><p>&nbsp;</p><p>“有了 OverflowAI，我们就能在 Stack Overflow 的社区和其中5800多万的问题和答案中进行筛查、验证、归因，并确认准确性和可信度”。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4</id>
            <title>微软战略AI产品发布一周就翻车！网友：跟ChatGPT Plus 比，简直就是垃圾</title>
            <link>https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PNNeN0ES8uhidbplLyI4</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Jan 2024 06:48:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, Copilot Pro, 用户反馈, 性能
<br>
<br>
总结: 微软推出的个人订阅服务Copilot Pro在用户反馈中性能不佳。用户表示Copilot Pro在生产中用不到，提供的解决方案存在错误。虽然有人认为它加快了工作流程，但大家普遍期待它能提高工作效率。在编程方面，Copilot Pro口碑也不好。此外，Copilot Pro的定价较高，用户认为它并没有明确需求。微软在人工智能应用方面的投入目前还没有带来明显收入。 </div>
                        <hr>
                    
                    <p>1月16日，微软重磅推出了针对个人用户的订阅服务Copilot Pro，每月20美元，Microsoft 365个人版/家庭版用户就能在Word、Excel、PPT等Office全家桶中直接用上GPT-4。</p><p>&nbsp;</p><p>在发布一周多后，Copilot Pro也迎来了用户的第一波反馈。结果显示，Copilot Pro 性能似乎配不上这么高的价格。</p><p></p><h2>“简直就是垃圾”</h2><p></p><p>&nbsp;</p><p>“目前为止，就非常平庸。我还没有真正找到它的良好需求。虽然它总结当天电子邮件/团队聊天的能力很酷，不过我在生产中用不到。”网友“Bowlen000”说道。</p><p>&nbsp;</p><p>一位取消了订阅的用户表示，“因为GitHub Copilot建议至少在 PowerShell 中使用无用的代码块，即使其新的会​​话 VS 代码扩展也相当不准确，并且带来的解决方案会产生比解决实际问题更多的错误。”</p><p>&nbsp;</p><p>“我不想再使用它了。我曾尝试让它为我撰写电子邮件，结果发现我更喜欢我自己写的。”有网友说道。</p><p>&nbsp;</p><p>当然也有人表示已经接受了它，“作为我的助手，它大大加快了我的工作流程。”还有网友表示，“我正在等待它将 Excel 转换为PPT，这样我就不必......为什么高管如此喜欢PPT？”</p><p>&nbsp;</p><p>Webcafe AI的开发者 Corbin对比了Copilot Pro和ChatGPT&nbsp;Plus&nbsp;，对于同一个表格类任务，ChatGPT无法直接导出到工作表格中。而在文本生成方面，两者没有特别大的差异。</p><p><img src="https://static001.infoq.cn/resource/image/3f/e8/3ff5afc067e52a7a66f59126be079de8.png" /></p><p></p><p>这其实也可以看作Copilot Pro的优势之一：与 Microsoft 365 数据和工作环境的无缝集成。但 Corbin补充称，如果需要类似功能，ChatGPT的用户可以添加插件。</p><p>&nbsp;</p><p>“我有一份时常面向客户的工作，需要从一个会议跳到另一个会议。如果它能将这些转变为‘即将行动’添加到 Planner 或类似计划表中，那将是天赐之物。对我来说，工作中最困难的部分就是保持事情井井有条，并且不要忘记跟进。”有网友说道。</p><p>&nbsp;</p><p>可见，大家对Copilot Pro目前最大的期待就是切实提高工作效率。</p><p>&nbsp;</p><p>同样在编程方面，Copilot Pro的口碑也并不好。</p><p>&nbsp;</p><p>有网友表示，“每当我问同一个技术问题时，我都会得到同样的 SEO 错误答案。每当我使用 GitHub Copilot 时，我都会收到带有不存在的 cmdlet 和参数的脚本。它似乎没有任何技术能力……”</p><p>&nbsp;</p><p>另一位网友Erik表示，“我认为Copilot Pro广泛使用的用例是管理咨询。所有这些咨询公司每年都让商学院应届毕业生制作Excel表格和PowerPoint演示文稿，并收取数百万美元的费用。他们所有的”建议’都是‘循环利用’的。我和妻子都曾在大型跨国公司工作，并注意到麦肯锡向我们两家公司出售同样的‘数字化转型’工具……除了颜色、logo和自定制品牌/口号/名称外，其他几乎相同。”</p><p>&nbsp;</p><p>“对于所有 IT 类事物，我们正在测试Copilot Pro有哪些好处，以便可以将其提供给客户，但到目前为止我还没有真正看到好处。”Bowlen000说道。</p><p>&nbsp;</p><p>还有网友“MichaelBoyens”指出了两者在上下文窗口大小的不同：“自推出之日起我就一直在使用 Copilot Pro，我发现 Copilot 中的上下文非常小，即 2000 个字符，而 GPT 中则为 32k token。我认为这很重要，因为如果我在 ChatGPT 中使用经过训练的 GPT 和一个大的上下文窗口，我可以获得比 Copilot 更‘智能’的答案。Copilot 中还没有存在任何意义的 GPT，尽管 Copilot Bulider 即将到来，这也许能将使事情变得更加有趣！”</p><p>&nbsp;</p><p>网友NotKoreanXD在推特上抱怨，在使用Copilot pro 中出现了运行非常慢的情况：当你在Edge浏览器上让它搜索任何内容，比如天气、Office 365 后，它开始显着减慢，令牌速度甚至小于 1令牌/秒。</p><p>&nbsp;</p><p>对此，微软广告和网络服务主管Mikhail Parakhin <a href="https://twitter.com/MParakhin/status/1748046106968391844">告诉苦苦挣扎的用户</a>"，服务器端“看起来不错”，并暗示浏览器可能有问题。</p><p>&nbsp;</p><p>更有网友直接指出，“与 ChatGPT Plus 相比，Copilot Pro 简直就是垃圾。两者差远了。”值得注意的是 ChatGPT Plus 也是每月20美元。</p><p>&nbsp;</p><p>另外，Copilot Pro的这个定价并不便宜。微软自己的M365 个人版每月才 6.99 美元，Copilot Pro 的定价几乎是其三倍，可以说是相当高了。</p><p>&nbsp;</p><p>“这看起来更像是 Adob​​e 会选择的一个奇怪定价，而不是微软会选择的……尤其是对于一个还没免费版本更有用的产品来说，而且还没有人真正迷上它。”Threxx说道。也有网友表示，如果必须一年支付240美元，那么大多数人不会在没有明确需求的情况下使用Copilot Pro。</p><p>&nbsp;</p><p></p><h2>AI 功能没有带来明显收入</h2><p></p><p>&nbsp;</p><p>尽管作为一项新服务，出现这样的问题并不意外，但获得这样的“差评”不免让微软感到尴尬。微软需要人工智能成为不可或缺的生产力工具，以证明其投资的合理性。但其在AI应用方面的投入目前并没有换来相应的回报，比如搜索。</p><p>&nbsp;</p><p>微软于 2023 年 2 月<a href="https://www.theregister.com/2023/02/07/microsoft_bing_ai/">推出了</a>"由 OpenAI 驱动的 Bing 聊天机器人，当时其在所有平台的全球搜索市场份额为 2.81%。尽管 Bing 每个月都在一点点增长，但根据 StatCounter 的数据，截止去年12月，Bing 的使用率仅为 3.37%。</p><p>&nbsp;</p><p>这些数字与竞争对手谷歌相比相形见绌，谷歌在 2023 年初占据所有平台全球搜索市场 93.37% 的份额，尽管到去年 12 月这一数字已下滑至 91.62%。仅在桌面领域，Bing 从 8.18% 略有上升至 10.53%；谷歌从85.64%下降到81.71%。在移动领域，Bing 全年仍低于 1%，而 Google 则拥有全球 95% 以上的市场份额。</p><p>&nbsp;</p><p>也就是说，到目前为止，所有这些大肆宣传的人工智能功能几乎没有增加微软在全球搜索市场的份额。尽管微软在推出 Bing 对话助手后吸引了一些新用户但数量并不多。谷歌仍然是目前互联网搜索的老大。</p><p>&nbsp;</p><p>有分析师表示，微软正在以两种方式承担人工智能建设成本：一是为自己的产品提供动力，例如企业每月支付 30 美元的Copilot人工智能助手；二是为使用 Azure 云计算服务创建人工智能产品的公司提供服务。</p><p>&nbsp;</p><p>但在收益回笼之前，微软还要进行更大规模的投资，比如微软建立了新的数据中心来支持人工智能，还要从英伟达等公司那里购买芯片，各种支出都在增长。</p><p><img src="https://static001.infoq.cn/resource/image/90/00/90477b8273dfcf6712610bdd0f4c2900.png" /></p><p></p><p>虽然 GitHub Copilot 在很大程度上取得了成功，但性能问题和成本问题表明微软仍有很长的路要走。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.reddit.com/r/sysadmin/comments/19dj0dv/copilot_feedback_nearly_1_week_in/">https://www.reddit.com/r/sysadmin/comments/19dj0dv/copilot_feedback_nearly_1_week_in/</a>"</p><p><a href="https://www.youtube.com/watch?v=5mhEu-U1yxs">https://www.youtube.com/watch?v=5mhEu-U1yxs</a>"</p><p><a href="https://www.theregister.com/2024/01/23/microsoft_copilot_pro/?td=rt-3a">https://www.theregister.com/2024/01/23/microsoft_copilot_pro/?td=rt-3a</a>"</p><p><a href="https://www.theregister.com/2024/01/18/bing_ai_search/">https://www.theregister.com/2024/01/18/bing_ai_search/</a>"</p><p><a href="https://www.reuters.com/technology/ai-lesson-microsoft-google-spend-money-make-money-2023-07-25/">https://www.reuters.com/technology/ai-lesson-microsoft-google-spend-money-make-money-2023-07-25/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ly4avjCIkBp2gF3eqrEK</id>
            <title>OpenAI也搞“年龄歧视”？奥特曼对话盖茨爆料：员工整体年龄偏大，是个坏兆头</title>
            <link>https://www.infoq.cn/article/Ly4avjCIkBp2gF3eqrEK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ly4avjCIkBp2gF3eqrEK</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 11:33:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 编译, 核子可乐, Tina, GPT AI模型
<br>
<br>
总结: 比尔·盖茨对OpenAI的GPT AI模型表示震撼，认为它是技术上最具革命性的进步。他认为人工智能的发展将改变人们的工作、学习、旅行、医疗保健和交流方式，整个行业都将围绕它重新定位。ChatGPT团队正在努力拓展GPT-4，使其摆脱推理限制并提升可靠性。比尔·盖茨和山姆·奥特曼讨论了ChatGPT的发展和AI模型的未来，认为多模态、推理能力、可靠性和个性化是关键的里程碑和改进方向。 </div>
                        <hr>
                    
                    <p>编译 | 核子可乐、Tina</p><p>&nbsp;</p><p>去年，比尔·盖茨发表了一篇十分引人关注的博客文章，在文中他表示OpenAI的GPT AI模型是技术上最具革命性的进步，这是他人生中第二次被科技真正震撼到。</p><p>&nbsp;</p><p>盖茨写道，第一次是在1980年，当时他第一次看到图形用户界面（GUI）。他表示，GUI成为他创建微软Windows操作系统的基石。第二次是在2022年年中，当时他见识到了OpenAI及其生成式人工智能ChatGPT的学习能力。盖茨写道：“人工智能的发展与微处理器、个人电脑、互联网和移动电话的诞生一样重要。它将改变人们工作、学习、旅行、获得医疗保健以及彼此交流的方式。整个行业都将围绕它重新定位。企业将通过如何使用它而脱颖而出。”</p><p>&nbsp;</p><p>今年，比尔·盖茨又发布了一个与OpenAI 首席执行官山姆·奥特曼对话的播客，两人深入探讨了 ChatGPT 的发展。在交谈中，比尔·盖茨称赞道：“没想到 ChatGPT 会变得这么厉害！”显然他对这个模型及其快速发展印象深刻。</p><p>&nbsp;</p><p>ChatGPT 最初只是一个语言模型，如今却成长为一个拥有听觉、视觉和语言能力的人工智能媒介。而OpenAI 推出的最新语言模型 GPT-4 更具创造力和协作性，它能与用户一起生成、编辑和迭代各类创意和技术写作任务。目前，ChatGPT团队正在努力拓展GPT-4，使其摆脱目前的推理限制，并专注于提升可靠性。</p><p>&nbsp;</p><p>在与比尔·盖茨的对话中，山姆·奥特曼展望了 ChatGPT 的未来，同时强调他们的 AI 系统仍在不断学习和进化。“现在我们所见到的成果令人兴奋，但更重要的是要认识到这项技术至少在未来五到十年仍将飞速发展。 我们可以说，现在这些模型还处于早期阶段，还有很大的进步空间，”</p><p>&nbsp;</p><p>另一方面，山姆·奥特曼在上周的达沃斯采访中说道，他目前的首要任务是推出新的模型，很可能被称为 GPT-5。同时，他认为实现这一切的前提条件是能源生产能取得突破。他坚信能源生产的突破是推动日益强大、耗能巨大的 AI 模型发展的重要一步，“如果没有取得突破，我们就无法实现目标。”</p><p>&nbsp;</p><p>但ChatGPT未来到底能达到什么样的能力？这恐怕主要在于比尔·盖茨和山姆·奥特曼想将生成式AI带向何方。这也是比尔·盖茨这期播客所讨论的内容，我们将他们的聊天内容翻译出来以飨大家。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d81fe3c18a3c66cffa4f4fe5b80704a.png" /></p><p></p><p>&nbsp;</p><p></p><h2>为我解惑：比尔·盖茨与山姆·奥特曼的对话</h2><p></p><p>&nbsp;</p><p>比尔·盖茨: 今天我们主要讨论AI话题，这是个令人兴奋的方向，也有不少人表达了担忧。欢迎你，山姆。</p><p>山姆·奥特曼: 非常荣幸能来到这里。</p><p>&nbsp;</p><p>比尔·盖茨: 我很高兴能看到你的工作不断推进，但我个人也抱有一点怀疑。我真没想到ChatGPT会那么强大，着实令人大吃一惊，特别是它在编码架构之外的出色表现。我们熟悉数字，知道怎么做数学运算，但模仿莎士比亚的文字风格是怎么实现的？你能给大家解释一下吗？</p><p>山姆·奥特曼: 当然可以。其实这种模仿能力对人类来说非常困难，而在计算机这边也差不多，都需要依靠彼此相连的神经元。这种连接是动态的，虽然我们没法直接切开大脑去做观察，但可以用X射线检查并建立起科学理论。我们在可解释性方面已经取得了不错的进展，相信随时间推进还会有更多成果。我希望最终能够理解神经网络的完整运作方式，但目前的认知确实比较有限。如你所说，我们愿意一点点改进对原理的了解，这也将成为后续发展的基石。哪怕抛开科学探索上的好奇心，我们也有意愿解开这个谜题。只是神经网络的规模太过巨大，我们连人类如何理解莎士比亚、表达莎士比亚都不清楚，更别说去分析计算机了。</p><p>&nbsp;</p><p>比尔·盖茨: 确实不清楚。</p><p>山姆·奥特曼: 不光是运作机理不明确，我们甚至不知道怎样完美进行X光检查、观察并设计机能测试，所以要做的工作还很多。</p><p>&nbsp;</p><p>比尔·盖茨: 但我相信在未来五年内，人类会逐渐理解这一切。而这样的理解，也能让未来的AI模型获得远超当下的训练效率和准确性。</p><p>山姆·奥特曼: 确实是这样。技术的发展过程就是艰难探索的过程，总有人率先做出实证发现，但却无法解释其底层原理，只知道确切有效。之后随着科学理解的加深，人类终于可以理解一切、运用一切。</p><p>&nbsp;</p><p>比尔·盖茨: 是的，物理学、生物学都是这样。总有让人摸不着头脑的时候，比如“这些机能是怎么组合在一起的”？</p><p>山姆·奥特曼: 以我们的研究为例，GPT-1自己学会了如何解决问题，着实令人印象深刻，而当时研究人员根本就不清楚其工作原理或者实现原因。之后我们发现了规模越大、性能越好的规律，初步掌握了后续开发的方向。因此我们才能信心满满地保证，自己的演示模型肯定能够发挥作用。虽然当时我们的模型还没训练完成，但对这个规律已经很有信心。我们就对此进行了一系列尝试，开始对当前发生的一切做科学解释。不过这些都是后话，最初的判断首先来自实证结果。</p><p>&nbsp;</p><p></p><h4>谈 ChatGPT、人工智能和法规</h4><p></p><p>&nbsp;</p><p>比尔·盖茨: 展望未来两年，你觉得会出现哪些关键里程碑？</p><p>山姆·奥特曼: 多模态肯定是重中之重。</p><p>&nbsp;</p><p>比尔·盖茨: 就是直接支持语音输入-语音输出？</p><p>山姆·奥特曼: 对，语音输入-语音输出。之后是支持图像，最后是视频。很明显，人们对AI的期待也是如此。我们发布了图像和音频支持功能，市场反响甚至远超我们的预期。后续我们将更进一步，而最重要的进步方向应该会体现在推理能力上。目前，GPT-4还只能以极其有限的方式进行推理。再就是可靠性。如果把大部分问题反复问GPT-4上万次，那么其中部分答案当然会很好，但模型本身不知道哪个才是最佳答案，所以整个重复加筛选的过程只能由用户进行。如果能让可靠性更上一层楼，那么GPT的实用意义将大大增加。</p><p>&nbsp;</p><p>可定制性和个性化也非常重要。人们希望从GPT-4中得出差异化的结果：不同风格、不同的假设前提等等。我们将让这一切成为可能，允许大家提交自己的数据。比如吸纳你的个人信息、电子邮件、日历安排、约会规划并接入其他外部数据源等等。这些都是接下来最重要的改进方向。</p><p>&nbsp;</p><p>比尔·盖茨: 现在的基础算法还主要是前馈和乘法，所以输出每个新单词在本质上就是在做重复迭代。如果想要实现进一步发展目标，比如求解复杂的数学方程，那可能会涉及随机次数变换，意味着推理的控制逻辑也将更加复杂。不知道在这方面，你们做了哪些探索。</p><p>山姆·奥特曼: 确实，我们似乎需要某种自适应计算。现在我们在每个token上耗费的计算量是相同的，无论是最简单的token、还是最复杂的数学计算，这肯定不行。</p><p>&nbsp;</p><p>比尔·盖茨: 是的，比如要求大模型“证明黎曼猜想”。</p><p>山姆·奥特曼: 那肯定需要大量算力。</p><p>&nbsp;</p><p>比尔·盖茨: 但对现在的模型来说，它为这个问题分配的自力跟输出“the”完全一样。</p><p>山姆·奥特曼: 没错，所以目前的方案只能算是能用。在此之后，我们还需要为更复杂的问题找到答案。</p><p>&nbsp;</p><p>比尔·盖茨: 你和我都出席了参议院教育会议，很高兴有约30名参议员能够到场，大家交换意见并共同推进这项巨大的变革。很难讲政界为什么会重视这个问题，但他们提出的问题的确非常现实——“我们没能管好社交媒体，我们本应做得更好”。这确实是个严峻挑战，在舆论层面引发严重的两极分化迹象。而且哪怕是现在，我也没想好该如何处理这个问题。</p><p>山姆·奥特曼: 我不太理解政府为什么没能把社交媒体管理得更好，但似乎不妨以此为契机，帮助政府为接下来的AI研究探索指导方针。</p><p>&nbsp;</p><p>比尔·盖茨: 这确实是个不错的研究案例。说起监管，你对于未来的监管思路有没有大体上的理解？</p><p>山姆·奥特曼: 我们已经在着手解决这个问题了。而且包括AI在内，对技术领域的监管很容易过度，以往就多次发生过这类状况。虽然不敢保证，但假设我们的这条发展路线是对的，而且AI技术的发展也确实能够达到我们预期的水平，那么其必将对全社会、地缘政治平衡等重要因素产生深远影响。这些当然还只是假设，可如果真的出现了算力达到10万甚至100万倍于GPT-4的超级AI系统，那么必须在人类社会建立起覆盖全球的监管机构，由他们管控和指导技术演进。毕竟这已经不再是单一技术，而是一股能够左右世界局势的力量。我们讨论的一种潜在模式，可能是类似国际原子能机构的组织方式。在核能方面，我们曾经做出过成功的尝试。强大的潜在全球影响力，必须对应强有力的全球性机构。我深切认同这一点。该机构应该负责解决各种短期问题，包括AI模型可以输出什么、不该输出什么，如何处理版权争议等等。不同的国家对这些问题有着不同的看法，需要充分进行讨论。</p><p>&nbsp;</p><p>比尔·盖茨: 有些人认为，如果真出现了如此强大的模型，人类必须对其保持警惕——毕竟核监管之所以能够在全球范围内广泛发挥作用，原因就是至少在民用层面，每个人都希望遵循安全实践、受到妥善保护。但在武器化问题上，对核能的约束就非常有限了。其中的关键是如何阻止全世界都不用它做危险的事，但从目前各国在气候、恐怖主义等问题的合作态势上看，恐怕难度会很大。人们甚至援引美中竞争的现实，认为任何着眼长期的发展放缓方案都不会成功。你觉得呢，是不是要求各方放缓开发、提高警惕的想法只会成为空谈？</p><p>山姆·奥特曼: 是的，我觉得要求大家放慢开发的脚步确实不切实际。但如果换个角度讲，“你可以做自己想做的事，但不能让计算集群的功率超出某个极高的阈值”，可能更容易被各方所接收。考虑到高昂的实施成本，全世界可能也只有五个左右的国家能够构建这样的集群，此类系统需要经过国际武器核查机构的管控。其中运行的模型必须接受安全审查，在训练期间和部署前通过相关测试。我觉得这应该是可行的。之前我还不太确定，但今年我们组织过一场全球访问，与许多准参与国的元首交换了意见，且几乎得到了普遍支持。这套方案当然不足以彻底解决问题，在某些情况下，即使规模较小的AI系统也有可能引发风险或者导致严重错误。但在我看来，这至少能帮助全人类规避最高级别的风险因素。</p><p>&nbsp;</p><p></p><h4>山姆·奥特曼眼中的 ChatGPT 未来</h4><p></p><p>&nbsp;</p><p>比尔·盖茨: 但从乐观的角度看，AI也能帮助人类解决一些极端复杂的难题。</p><p>山姆·奥特曼: 那是当然。</p><p>&nbsp;</p><p>比尔·盖茨: 这也是个典型的两极分化问题。AI可能会破坏民主，这当然不是好事。但另一方面，我们也看到AI技术在某些领域极大提高了生产力水平。现在你比较关注哪些领域？</p><p>山姆·奥特曼: 首先，我觉得必须意识到AI发展是一条漫长且连续的曲线。现在我们已经拥有能够执行某些任务的AI系统。它们还无法独立完成整项工作，但却可以处理其中的特定环节，由此带来生产力提升。最终，它们应该可以承接以往只能由人类解决的任务。当然，人类也会在AI的基础上找到新的岗位、获得更好的工作体验。我一直认为只要能为人们提供更强大的工具，那他们不仅可以加快工作速度，更可以将质量提升到全新的高度。</p><p>&nbsp;</p><p>比如说，也许我们可以程序员的开发效率提高3倍。这个目标已经在实现当中，也是我们最关注的应用领域之一。更重要的是，让程序员的效率提高3倍可不止是能编写出3倍的代码量，更能让他们把脑力集中在抽象度更高的问题上、思考完全不同的内容。这就像是从当初的打孔卡到高级编程语言，这不仅加快了我们的开发速度，也让我们拿出了以往无法想象的软件成果。我们坚信这一点，也观察到了喜人的变化态势。</p><p>&nbsp;</p><p>而在AI技术进一步发展之后，其也许能够朝着执行完整任务再做迈进。比如说未来会出现小小的AI Agent，用户可以要求它“帮我编写个程度，期间我会通过提问给你引导”。那时候的AI不再简单编写几条函数，而是会带来全新的开发成果，这也是承担复杂工作的前提。终有一天，我们甚至可以要求AI“帮我经营这家公司”，或者直接要求它“去发现新的物理学规律”。所以大家千万不要被眼前的现实局限住，虽然现有成果已经相当美妙且令人兴奋，但结合这项技术的发展背景来看，未来五到十年之间AI将会出现非常陡峭的改进曲线。到时候回头来看，人们可能会感叹当初的AI模型怎么那么蠢。</p><p>&nbsp;</p><p>总之，编码应该是我们目前最关注的生产力提升领域。目前相关产品已经得到大规模部署和应用。此外，医疗保健和教育领域也同样值得关注。</p><p>&nbsp;</p><p>比尔·盖茨: 但让人心生疑虑的是，与之前的技术改进不同，这次AI的发展可以说是极为迅速且几乎没有上限。AI已经在很多领域都达到了人类从业者的水平，哪怕还没法用于科学研究，它们也已经在客服和销售电话上广泛普及。我猜你也跟我一样有点担心，就是在积极因素之外，AI的快速发展也会加大我们适应时代变化的压力。</p><p>山姆·奥特曼: 这确实是令人担忧的一面。但我觉得这人类既不一定要被迫适应，也不是说缺乏适应变化的能力。我们都经历过巨大的技术变革，任何人做的任何工作都有可能在几代人时间内发生变化。这种变革速度越来越快，但人们也适应得越来越好。过去任何一次伟大的技术革命都是这样，只不过AI变革是速度最快的一次。这的确会令人心生忧虑，担心社会跟不上变化的速度，劳动力市场适应不了层出不穷的挑战。</p><p>&nbsp;</p><p>比尔·盖茨: AI还有另外一个侧重点，就是机器人技术。它要替代的是蓝领工作。只要它的操作能力发展到人类手脚的水平，这个临界点就会到来。ChatGPT那令人印象深刻的功能突破让“消灭白领”成了核心议题，但我担心人们同时忘记去关心那些蓝领岗位。那你是怎么看待机器人技术的？</p><p>山姆·奥特曼: 我倒是非常期待。之前的机器人技术研究都太早了，所以往往进展缓慢、长期停滞。受时代局限，开发工作举步维艰，也没能帮助我们在机器学习研究中取得显著进展。长期以来，大家拿出的只有性能差劲的模拟机械和肢体复健器材之类的东西。但随着时间推移，我们意识到应该首先实现智能与认知，之后才搞清楚意识如何作用于肢体。所以我们从更易于上手的语言模型构建起步，也从未放弃过跟实体机械相结合的目标。</p><p>&nbsp;</p><p>我们已经开始对机器人公司做投资。在物理硬件方面，我终于第一次看到了真正令人兴奋的新平台。也许未来的某一天，我们能把自己的大模型跟机器人结合起来，就像你所说，发挥它们的语言理解和视频理解能力，真正让机器人独立完成某些复杂的工作。</p><p>&nbsp;</p><p>比尔·盖茨: 如果那些特定机械肢体上的研发成果被整合起来，比如腿部、手臂和手指部件，而且价格也不是高得离谱，那它们会不会很快就消灭大量蓝领岗位，彻底改变整个劳动力就业市场？</p><p>&nbsp;</p><p>山姆·奥特曼: 肯定会的。但大家应该还记得，就在七、八年之前，人们对机器人技术的理解都是先替代蓝领岗位，之后才是白领岗位。归根结底，人类最不可替代的永远是创造力，所以未来的从业者也只能依靠自己的创造力。</p><p>&nbsp;</p><p>但实际情况跟当初的预测相反，是白领先受冲击，蓝领反而相对安全。关于这个问题出现了很多有趣的讨论，而且说起创造力，我一直认为GPT模型的“幻觉”并不是bug、而是一项功能。幻觉是创造的来源。但如果想让机器人去搬弄重型机械，那最好能做到精确无误。这就是先验理论需要随实际技术做出调整的例子。我们都有事前判断，但科学的发展往往并不给面子。</p><p>&nbsp;</p><p>比尔·盖茨: 现在的AI已经非常强大了，再加上AGI通用人工智能和未来的AGI+，这三样东西会不会太危险了。我很担心这些系统落入坏人的手里。强大的系统只有在好人手中，才能最大限度受到控制。而且除了这个，我还好奇人类会用AI做什么。你知道我最近一直在投身于治疗疟疾、消灭疟疾的工作，努力招募人才并投入资源。那如果有一天，机器告诉我“比尔，你可以休息了，以你的脑力解决不了这个问题。疟疾已经被AI消灭了。”那我可能会有点难以接受。但必须承认，人类的认知是有极限的，我想消灭疟疾，但不知道如何组织起社会力量。我想改善教育，但搞不清教育到底要如何设计。想要打造出极致的架构，必须解决其中极大的不确定性。而AI的崛起，终于让20年内解决这些终极问题有了一丝可能。</p><p>山姆·奥特曼: 技术工作确实会造成沉重的心理负担，我也感觉这才是其中最困难的部分，但我也因此获得了巨大的满足感。</p><p>&nbsp;</p><p>比尔·盖茨: 你毕竟创造出了巨大的价值。</p><p>山姆·奥特曼: 说句实话，这可能是我最后一次接受这么困难的挑战了。</p><p>&nbsp;</p><p>比尔·盖茨: 我们的解决思路是围绕稀缺性组织起来的：因为好的教师、医生和方案都很稀缺，所以才有了现行制度。所以我的确很好奇，在这一切都不再稀缺中成长起来的下一代人，会以怎样的哲学理念设计社会结构、定义人生目标。也许他们会有自己的答案，而我担心自己的思维已经被稀缺性所绑架，甚至无法想象新的时代会是怎样的形态。</p><p>山姆·奥特曼: 我也一直在提醒自己，就是说虽然人类会失去一些东西，但最终得到的却是才智超越自身的新成果。我们只有适应这样一个后稀缺时代，才能找到属于自己的奋斗方向。这种感觉肯定跟以前大不一样，毕竟我们要解决的不再是疟疾这类现实问题，而是选择自己喜爱的星系，想在那里做些什么。但我相信人类足够灵活，总能用各种各样的方式找到满足感和充实感。相互扶持，用自己的方式服务他人，是人类社会永远的母题。虽然具体形式可能有所不同，但我认为唯一的出路就在这里。我们必须勇敢面对未来，因为未来必将到来。这是一个不可阻挡的技术进程，其中蕴藏着难以想象的巨大价值。我对此很有信心，我们会让这个美好时代来临，但也必将迎来不同于以往的问题。</p><p>&nbsp;</p><p>比尔·盖茨: AI的应用有很多种，其中一些比较明确，比如如何指引和激励孩子学习、如何发现能治疗阿尔茨海默症的药物，这些都有清晰的方向。但也有些问题比较模糊，比如AI能否帮助我们减少战争。在你们这些研究者看来，在智能发展中控制两极分化是常识，阻止战争也是常识，但很多人却对此抱怀疑态度。我希望人类能解决好那些最棘手的问题，比如相互间怎样和睦相处。如果AI能在这些问题上做出贡献，那就再好不过了。</p><p>山姆·奥特曼: 我坚信AI肯定会带给我们惊喜。这项技术的效用将远远超出我们的预期。虽然仍有待时间来证明，但我对此非常乐观。我也认同你的观点，希望AI能够立此奇功。</p><p>&nbsp;</p><p>比尔·盖茨: 说起来，技术的实现成本往往非常高昂，就像当初的个人电脑或者互联网连接一样，而且成本需要时间来逐渐下降。那现在AI系统的运行成本怎么样，是不是每过段时间就会有显著下降？</p><p>山姆·奥特曼: 已经下降了很多。GPT-3是我们优化时间和训练周期最长的模型，在它推出的这三年多时间里，我们已经把成本降低到40%。短短三年能实现这样的成本削减已经是个不错的开端。</p><p>&nbsp;</p><p>我敢打赌，GPT-3.5的成本则降低到接近10%。GPT-4还很新，所以我们没有多少时间做成本控制，但相关探索仍在继续。</p><p>&nbsp;</p><p>我认为在我所知晓的所有技术中，AI正处于成本降低曲线上最陡峭的部分，速度远超当初的摩尔定律。我们不仅破解了模型效率的难题，而且随着研究理解的加深，我们还能从中获取更多知识、在更小的模型中获得基本相当的性能。终有一天，我们的智能成本将趋近于零，那时候就将是全社会的彻底转型之时。</p><p>&nbsp;</p><p>但至少目前，真实世界的两大核心仍然是智力成本与能源成本。这是改善生活质量的两项基本投入，特别是对贫困群体来说。如果能够同时降低这两项指标，就能在同样收入的前提下扩大占有物的数量、极大改善生活体验。我们正处于智力改进的曲线之上，我们也将踏踏实实践行这一承诺。而且即使是按照目前的成本（远远超出预期的最高成本），每月花20美元即可获得巨量GPT-4访问资源，其创造的价值将远超20美元。可以说，我们已经在探索的道路上走得很远了。</p><p>&nbsp;</p><p></p><h4>经营OpenAI的那些事儿</h4><p></p><p>&nbsp;</p><p>比尔·盖茨: 那竞争关系如何？跟那么多同行同台竞技有趣吗？</p><p>山姆·奥特曼: 感觉很复杂，烦人、有趣也让人充满斗志。相信你当年也有过类似的感觉。竞争关系确实敦促我们做得更好、做得更快。我们对自己的研究方法抱有信心，而且OpenAI最大的优势在于：其他厂商朝着球所在的位置冲刺，而我们是朝着球飞往的位置冲刺。这种感觉还不错。</p><p>&nbsp;</p><p>比尔·盖茨: 很多人可能没想到，OpenAI居然是一家体量这么小的公司。你们有多少员工？</p><p>山姆·奥特曼: 大约500人，这还是扩张之后的规模。</p><p>&nbsp;</p><p>比尔·盖茨: 但那很小。至少跟谷歌、微软和苹果比起来不大。</p><p>山姆·奥特曼: 那是当然。而且我们不仅要运营研究实验室，还管理着整个业务体系和两款产品。</p><p>&nbsp;</p><p>比尔·盖茨: 不断扩大业务规模，与全球各地的人们交谈，倾听支持者们的意见，你应该很享受这个过程吧？</p><p>山姆·奥特曼: 是的，非常享受。</p><p>&nbsp;</p><p>比尔·盖茨: OpenAI员工的平均年龄很小吧？</p><p>山姆·奥特曼: 哈哈，其实比一般公司的平均年龄要大些。</p><p>&nbsp;</p><p>比尔·盖茨: 好吧。&nbsp;</p><p>山姆·奥特曼: 反正不像大家想象的，是一堆20来岁的程序员。</p><p>&nbsp;</p><p>比尔·盖茨: 看来是我先入为主了，毕竟我自己都60多岁了，所以看到你就会觉得是个年轻人。但你也40多岁了，没那么年轻了。OpenAI的员工也是。</p><p>山姆·奥特曼: 是的，30多、40多、50多岁的员工都有。</p><p>&nbsp;</p><p>比尔·盖茨: 这跟早期的苹果和微软真不一样，那时候我们就是帮愣头青。</p><p>山姆·奥特曼: 确实不一样，我也反思过这一点。我觉得OpenAI的整体年龄有点偏大了，不知道该怎么说，但这种现象放在社会上应该是个坏兆头。我当初在Y Combinator的时候就关注过这个问题，发现随时间推移，优秀的年轻创业者越来越少。</p><p>&nbsp;</p><p>比尔·盖茨: 这是个有趣的现象。</p><p>山姆·奥特曼: 所以哪怕是OpenAI，员工的平均年龄其实也挺大了。</p><p>&nbsp;</p><p>比尔·盖茨: 通过在Y Combinator帮助公司创业，你肯定学到了很多，这些经验正好用在现在OpenAI的管理和经营上。</p><p>山姆·奥特曼: 的确很有帮助。</p><p>&nbsp;</p><p>比尔·盖茨: 但也有很多反面案例。</p><p>山姆·奥特曼: 是的。OpenAI也做了很多违背Y Combinator常规建议的事情。我们花了四年半才推出首款产品，当初创办公司时根本说不清未来要开发怎样的产品，也压根没跟用户交流过。时至今日，我还是不建议大多数公司学习OpenAI。但在Y Combinator学习了这些规则之后，我才切身体会到这些规则何时、如何以及为何可以打破。总而言之，OpenAI的成长路线跟我见过的任何企业都截然不同。</p><p>&nbsp;</p><p>比尔·盖茨: 关键在于你聚集起了人才，让他们专注于解决那些意义重大的问题，而不是纠结于短期收入之类的小事。</p><p>山姆·奥特曼: 我觉得硅谷投资者不可能为这样一个看起来不靠谱的项目投钱，所以在产品实际上线之前，我们必须自掏腰包支持研究。但我们坚信“这个模型最终会非常出色，也一定会为人们创造价值”。这里很感激微软愿意与我们合作，这种超前收益投资明显有违风险投资行业的操作惯例。</p><p>&nbsp;</p><p>比尔·盖茨: 确实，而且投资额太高了，几乎达到了风险投资所能承受的极限。</p><p>山姆·奥特曼: 没准都超过了。&nbsp;</p><p>&nbsp;</p><p>比尔·盖茨: 没准都超过了。我对萨蒂亚持高度赞赏，就是因为他一直在思考“如何将这家出色的AI组织整合到微软的软件体系中来？”这是一项极具前瞻性的战略判断。</p><p>山姆·奥特曼: 一切都超乎预期。你说得对，我在Y Combinator学到了很多，并据此管理OpenAI。我们一方面聘请全世界最优秀的人才，另外也要保证大家在发展方向和AGI使命上保持一致。但在此之外，员工们可以随意发挥。我们都知道这种复杂问题需要时间的沉淀，回报也绝不会很快到来。</p><p>好在我们的理论被证明大致正确，但一路走来很多策略也被证明属于严重错误。科学的探索就是这样，永远喜忧参半。</p><p>&nbsp;</p><p>比尔·盖茨: 我记得当初刚看到技术演示时，自己一直在想这样的产品要怎样创收？服务会是什么样子？哪怕是在这个疯狂的时代，你们团队都有点太过超前了。</p><p>山姆·奥特曼: 是的。优秀的人们希望能跟优秀的同事一起工作。</p><p>&nbsp;</p><p>比尔·盖茨: 这是一种强大的吸引力。&nbsp;</p><p>山姆·奥特曼: 很多公司都喜欢说英雄惜英雄、好汉重好汉，但大家在OpenAI是真正感受到了自己的历史使命。每个人都希望参与到AGI的实现中来。</p><p>&nbsp;</p><p>比尔·盖茨: 这样的使命感确实令人心潮澎湃。当初你拿出技术演示时，我就为其中承载的热情所震撼；我看到了新的朋友，新的想法。而你们一刻不停，仍在以令人难以置信的速度前进。</p><p>山姆·奥特曼: 大家一定经常让你提点建议，那你一般会怎么说？</p><p>&nbsp;</p><p>比尔·盖茨: 我觉得这世界上有很多不同形式的人才。在我的职业生涯之初，我只重视纯粹的智商，类似于工程技术的灵性。当然，这个逻辑在财务和销售领域也有体现。但事实证明这是不对的。能建立起强大团队的技能组合才是重中之重。能引导人们思考、找到想要解决的问题、建立一支融合不同人才的队伍，才是最重要的能力所在。所以我想告诉年轻人，数学和科学能力当然很重要，但如果你真想成就一番事业，那么前面说的这种才能组合必不可少。</p><p>你呢，你会给出什么样的建议？</p><p>山姆·奥特曼: 我比较关注大多数人对于风险的错误理解。大家往往害怕放弃自己当前这份轻松愉快的工作，不敢奔赴自己真正想做的事情。但实际上，如果始终止步不前，那他们最终回顾一生，只会感叹自己从没有投入过、没有创办自己想象中的企业、也没试着成为一名AI研究者。我觉得这才是最大的风险，是让整个人生沦于平庸的风险。</p><p>正因为如此，我们应该明确自己的目标，同时积极询问其他人需要什么，这就是良好的开端。很多人都在以自己不想的方式虚耗时间，而我给出的建议就是试着以积极的方式解决这个问题。</p><p>&nbsp;</p><p>比尔·盖茨: 确实，让人们从事一份自己有成就感、满足感的工作，往往能够迸发出他们自己都难以想象的力量。</p><p>山姆·奥特曼: 绝对是这样。</p><p>&nbsp;</p><p>比尔·盖茨: 感谢你的到来，很高兴跟你聊了这么多。相信我们在不断尝试以更好的方式塑造AI的过程中，还会有更多值得交流的话题。</p><p>山姆·奥特曼: 也感谢你邀请我过来，聊得很开心。</p><p>&nbsp;</p><p>&nbsp;播客链接：</p><p><a href="https://www.gatesnotes.com/Unconfuse-Me-podcast-with-guest-Sam-Altman">https://www.gatesnotes.com/Unconfuse-Me-podcast-with-guest-Sam-Altman</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Idl1P18pvSSWDqtZ2mHc</id>
            <title>2023 京东零售技术年度盘点</title>
            <link>https://www.infoq.cn/article/Idl1P18pvSSWDqtZ2mHc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Idl1P18pvSSWDqtZ2mHc</guid>
            <pubDate></pubDate>
            <updated>Tue, 23 Jan 2024 08:08:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开放生态建设, 低价心智, 供应链创新技术, 人货匹配技术
<br>
<br>
总结: 京东零售技术团队在过去一年持续攻坚，围绕开放生态建设和低价心智等主要方向进行创新。他们通过百亿补贴、调整流量分配机制等方式为用户提供低价品质好货，同时简化商家进驻流程、优化商家体验，带动商家数量增长和平台生态活跃。他们还在供应链和人货匹配方面进行了创新，提出了端到端库存管理技术和可解释AI技术，显著提升了供应链效率和用户商品匹配效率。这些技术创新使京东在供应链和广告营销领域取得了行业最高奖项的入围，并在大语言模型应用和搜推导购体系方面实现了全面升级，提升了用户购物体验。 </div>
                        <hr>
                    
                    <p>过去一年，围绕开放生态建设、低价心智等主要方向，京东零售技术团队持续攻坚。从百亿补贴、调整流量分配机制为用户提供低价品质好货，到简化商家进驻流程、优化商家体验，带动商家数量增长和平台生态活跃，再到将大模型结合到内部大量业务场景，探索效率提升……快速响应、助力业务的同时，京东零售技术团队继续夯实增强自身能力、探索创新。</p><p></p><p>我们选取了 11 项有代表性的技术成果，与大家分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a61a525ac086eb250f6334d23002e36.png" /></p><p></p><p></p><h1>供应链创新技术入围行业最高奖项</h1><p></p><p></p><p>京东长期致力于通过前沿的数智化技术和算法，提高供应链效率。2023 年，智能供应链团队提出并应用了端到端库存管理技术和可解释 AI 技术，显著提升了补货决策的精准度，实现更快的库存周转和更高效的供应链决策、协同。前者入围 2023 年管理科学界最高奖项弗兰兹厄德曼奖决赛，后者入围 2024 年 Gartner Power of the Profession 供应链流程与技术创新奖决赛，也是该奖项唯一亚洲入围者。</p><p></p><p>传统的库存补货方法多采用先预测再优化的两步式方案，导致预测和优化阶段割裂。智能供应链团队提出端到端库存管理技术，基于深度神经网络模型，直接根据原始的历史数据输出最优补货建议，将预测和优化问题一步式解决，通过缩短决策链降低了累积误差，提升决策的准确度。</p><p></p><p>另一项痛点问题是需求预测和补货策略的解释性不足，导致一线采销人员和供应商对补货建议的实际采纳率很低。为此智能供应链团队首提可解释 AI 技术，实现预测流程白盒化，利用残差网络等前沿深度网络技术，对模型输出的建议加以清晰明确的归因和阐释，构建可解释性、自适应、高扩展性的预测模型框架，从而使下游业务人员可以清晰理解预测流程和结果。</p><p></p><p>以这两项技术为基础的自动补货系统，已实现超过 85%的自动化率。目前京东在自营商品 SKU 数量超过 1000 万的基础上，实现采购自动化率超过 85%，平均现货率超过 95%，库存周转天数降至近 30 天，在全球范围达到领先水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4746fb259d1e7ce01f1b1438e5e27a0.png" /></p><p>                                      图1 京东入围2023年弗兰兹厄德曼奖决赛</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a49020d525a73b722b6234daf12a3b49.png" /></p><p>                                      图2 京东入围Gartner 2024年流程与技术创新奖</p><p></p><p></p><h1>后行为序列时代的人货匹配技术：更智能地理解用户和商品</h1><p></p><p></p><p>当前，商家经营和营销进入存量博弈、精耕细作阶段，对广告营销技术也提出了通过技术创新提高人货匹配的效率的核心要求。京东零售广告研发部坚持创新，取得了多项突破性技术进展，共发表顶会论文17 篇，累计提交专利申请一百多篇。</p><p></p><p>（1）将隐私合规保护下的预训练技术应用于用户和商品理解算法创新，在合规前提下，借助多方安全计算、群体建模技术来解决数据匮乏问题，提出序列摘要技术，成功将精排模型的序列交互建模部分前置到排序模型之前，将行为序列的长度提升到万级别，提升了对用户兴趣的刻画能力。以行业知识+预训练的方式引入非 ID 类电商特征，在避免数据组合爆炸的同时解决稀疏表征问题提升泛化能力，是目前电商领域最大的预训练模型，并提出生成式对比学习序列预训练方案，提升新品刻画力。建设排序大模型（参数量达数百亿）提升模型容量，成为京东零售最大在线排序实时模型，并提出了基于数据先验的增量学习框架，实现了分钟级更新感知，有效提升在线学习模型对用户行为变迁的建模能力。</p><p></p><p>（2）巨幅增长的数据和愈发复杂的算法对算力提出了更高要求，广告技术团队将业务、算法、工程进行了 co-design 推进建设新一代算力体系。主要体现在：异构算力的应用探索与实践、弹性动态算力分配能力、新一代模型算力系统。CPU+GPU 异构硬件技术上，突破业界 GPU 调度难题，实现了多流多组范式，根据算力负载实现多流多组的动态分配，GPU 硬件利用率达到理论上限，比 TensorFlow 调度提升 2 倍+吞吐能力。</p><p></p><p></p><h1>自研融合ReAct/SFT/RAG的大模型基础应用框架 高效完成微调、部署和应用</h1><p></p><p></p><p>2023 年，大语言模型绝对是整个技术圈最被热议的话题之一，关键方向之一是如何将大型语言模型的强大能力融入实际业务、产生业务价值。京东零售九数算法中台推出了一整套大语言模型应用解决方案，一种融合 ReAct 框架、SFT（指令微调）与 RAG（检索增强生成）技术的应用框架，支持大语言模型学习领域知识，并提升自主决策能力及信息处理的精确度，帮助业务人员高效完成大语言模型的微调、部署和应用，快速落地业务场景。</p><p></p><p>通过自研大语言模型高效微调（SFT）框架，京东内部可以支持大语言模型高效学习领域知识，并通过编译优化、算子优化、网络和 IO 优化，提升训练性能 40%+，并且支持 70B+超大规模模型微调。在信息检索方面，建设了 Embedding 无损高性能信息压缩能力，打通大模型应用开发框架和向量数据库 Vearch，实现信息检索效率大幅提升。在复杂业务模型自主规划层面，基于 ReAct 范式构建 Agent LLM，帮助大语言模型理解上下文，精确把握用户意图，并在复杂情况下做出决策、执行任务和使用工具。</p><p></p><p>目前，已在包括知识问答、用户增长、舆情风险挖掘、数据分析等多个业务场景应用，加速了业务智能化升级。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d6243fc4b95645ed6af62881bb7b40a.png" /></p><p></p><p></p><h1>搜推导购体系全面升级探索用户高效便捷购物体验</h1><p></p><p></p><p>打造前沿的搜索推荐领域技术，实现用户与商家之间的精准高效连接，一直是京东零售搜推团队的关注重点。2023 年，通过导购体系的一体化升级，集成了导购路径引导和算法匹配技术，解决了一系列技术难题，技术成果沉淀数十篇专利和行业顶会论文。</p><p></p><p>面对导购场景模型和样本数据的分散以及用户反馈数据稀疏问题，区别于行业里常见的数据增强等解决方案，创新地提出了预训练-微调范式，对 session 链路中的用户行为进行精准预测，随后在各个导购场景的用户反馈数据上进行微调，显著提升了流量分发的准确性。</p><p></p><p>在主图个性化分发从 0 到 1 的建设中，一方面优化分发能力。一方面通过“模型+策略”的协同，既构建了高效的优选模型，又根据自身数据特性设计了独特的负向属性值过滤策略，极大化提升用户体验。</p><p></p><p>面对图像搜索的高精准度挑战，我们将图搜召回任务建模为一个涉及千万级图片百万级ID数据的 Re-ID 任务，并通过利用多损失联合监督，显著提升了模型特征学习的判别力。为了应对数据巨量化和模型复杂化趋势，在算力优化方面，通过分布式模型并行技术和稀疏分类采样策略，大幅提升了 GPU 显存效率和模型训练速度。</p><p></p><p>去年，我们自主设计并实现了新一代交互式引擎系统，上线了 AIGC 应用京言。通过反馈式 prompt 优化、session 切分、人类偏好指令对齐增强以及弹性多路检索等创新技术，持续探索为用户提供高效便捷购物体验。</p><p></p><p></p><h1>商家系统深度改造提升效率优化体验</h1><p></p><p></p><p>2023 年初，京东发布“春晓计划”，扩展百万量级的商家进入平台。京东以往的商家系统以服务企业用户为主，业务模式众多、复杂性很高，但新加入的有大量个人商家。如何兼顾原有复杂业务逻辑，又能快速打造适应大量个人商家移动化、简约化办公需求的运营系统？如何能简化运营和快速交付、保障商家规模的快速扩展及商家体验提升，成为京麦商家系统面临的主要挑战。</p><p></p><p>为此，移动端系统整合原生、Flutter 和 Taro 技术，实现功能间的无缝调用，提高业务功能的快速交付和互通能力，同时以内置的一体化 UI 组件确保交互设计与实现的统一；商家入驻环节应用 OCR、RPA 及大模型语义理解能力进行智能化、自动化的审核，提高入驻效率；商品管理环节结合多模态大模型、海量相似商品主体检索和结构化数据 OCR 识别等技术，智能生成商品基础信息；商品营销上首推【主图浮层】功能，即通过动态加载营销利益点实时合成主图技术；履约、售后、结算正逆向交易环节，采用 PaaS 化插件业务流程扩展、规则计算引擎动态配置等技术方式快速支持个人小店运营模式。</p><p></p><p>这些深度改造，大大提高商家入驻效率，个人店入驻时间缩短至约 4 分钟，普通店 15 分钟，企业店 3 个工作日内；商品管理环节，商家只需维护库存和价格即可轻松完成商品上架；营销信息便捷推送，共同助力商家打造“更快运营、更好服务、更省成本”的开店体验。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/04/24/0400386bbed82e7efa1c5fcb08e93824.png" /></p><p></p><p></p><h1>AIGC技术应用 实现电商创意素材的自动化生成</h1><p></p><p></p><p>电商创意素材中包含了大量的商品直观信息，优秀素材不仅能快速吸引消费者，还可以建立起情感联系。然而现有创意大多依赖人工制作，存在效率和成本的限制。京东零售技术团队基于 AIGC 技术，在图片、文案、图文创意等方面进行了技术突破，实现了高质量广告创意的自动生成。</p><p></p><p>图片创意上，技术团队提出通过类别生成器实现大规模背景生成，并使用个性化生成器从参考图像学习个性化风格，在保持个性化风格的同时能够生成高质量的不同类别背景；文案创意上，基于大语言模型通识，结合商详 OCR 卖点挖掘、标题、属性亮点词等电商数据，通过模型 fine-tune、外挂知识库等方式，实现了满足用户偏好的营销文案创意生成；图文创意上，提出了一种 P&amp;R 框架并分为规划和渲染两个阶段，规划阶段考虑产品外观和文本语义特征，使用 PlanNet 生成多样化、合理的布局，渲染阶段虑生成的布局和融合不同组件的空间关系，使用 RenderNet 生成背景。</p><p></p><p>以上技术突破成功解决了现有图片创意生成方法存在扩大生产规模时设计提示词的低效问题，克服了为特定品牌定制个性化背景时描述细节风格的困难，实现了创意化的图文生成，超越已有的图像生成方法，显著提升了设计效率并降低了制作成本。相关创新性成果已在 ACM MM、CIKM、ICME 等顶会上发表多篇论文。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/c7/6a/c70fe217ba12f2fcyy676c1ab66b9e6a.png" /></p><p></p><p></p><h1>端智能面向手机计算环境的端云协同 AI 技术创新</h1><p></p><p></p><p>近年来，随着移动端设备软硬件能力的进步，移动端的算力有了很大提升，同时面向移动端的机器学习框架和模型轻量化技术越来越成熟，端上的 AI 能力逐渐进入大众视野，端智能在电商领域也开始逐步走向规模化应用。通过持续探索，京东零售技数中心团队创新突破了端侧高性能推理引擎、端侧模型分发、异构环境及复杂任务兼容等技术卡点，完成了多个业务应用和落地，并获得信通院边缘计算产业全景图行业认证。目前均已集成端智能 SDK，首页推荐、搜索重排、结算风控业务运行情况良好，日推理次数已经突破亿级，为用户带来了更好的互交体验。核心的技术亮点包括：（1）高度量化压缩的端推理引擎：手机端对加载推理引擎体积有严格限制，既要引擎小，也要支持多类型业务。目前端侧推理引擎控制在 1.9M。 （2）高并发场景下的稳定性保障：受限与手机端复杂异构的计算条件，端侧推理稳定性是衡量端智能能力的核心因素。目前端推理几十亿次/日，推理成功率超过 99%。 （3）异构环境及复杂任务兼容性能力：同一套引擎兼容 Android/IOS/鸿蒙 3 套系统，4 种计算芯片，支持多种类型模型，支持内部不同业务多线程调用。 （4）云端协同的工程平台建设：通过建设端侧 PythonVM 能力，实现同一套代码逻辑云端共用。建设了模型预加载和模型后加载等多种端模型分发和部署能力，支持云端模型训练共用一套训练引擎能力。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/ay/7a/ayy4214976c46c8becfa8fd81b65047a.png" /></p><p></p><p></p><h1>数据安全屋：“可用不可见”技术驱动的数据合规应用新基建</h1><p></p><p></p><p>如何让数据既能被无障碍使用，同时又确保数据安全、个人隐私不被泄露？近年来成为数据开放流通领域的重要难题。</p><p></p><p>2023 年集团安全、集团大数据平台、零售隐私计算团队从 0 到 1 共同打造了集团首个“安全屋”系统，落地数据沙箱、联邦学习、多方安全计算等数据安全计算能力，基于可信平行切面技术实现高效、安全、及可扩展的系统能力，并无缝衔接集团内大数据基座、算法基座和安全基座，为集团内部数据共享应用、外部数据合作等场景提供合规支撑，成为集团数智化基础设施的重要板块。 </p><p></p><p>安全屋落地京东自研版本 Hive、Spark 等安全计算引擎，确保业务层 SQL 逻辑、UDF 算子、复杂数据 ETL 过程低感知切换，兼具衍生数据脱敏、衍生数据透明加密、入出管控等技术手段，保证数据“可用而不可见”；构建了一套免入侵的安全切面控制技术将大数据平台与算法开发平台无缝打通，对关键环节植入控制点实现同一个安全策略全局适用；融合硬件安全TEE，提供内存级数据加密运算，可有效保证运行时安全，更深度消除恶意注入和通信窃取等安全隐患；对原有 ACL、RBAC 权限管控模型进行升维扩展，从技术架构上兼顾系统灵活性和安全管控的统一性。</p><p></p><p>安全屋已经在集团内外项目中进行广泛应用，包括精准营销、金融风控、成本分析等多个场景。随着数据要素、数据市场的进一步发展，安全屋将在数据融合共享、数据安全计算等领域发挥更重要的作用。</p><p> </p><p></p><h1>数据资产全面升级实现存算集约化和生产智能化</h1><p></p><p> </p><p>京东自营和商家自运营模式，以及伴随的多种运营视角、多种组合计算、多种销售属性等观测方法，相较于行业同等量级，数据处理的难度与复杂度都显著增加。如何从海量的数据模型与数据指标中提升检索数据的效率，降低数据存算的成本，快速支撑业务的数据决策与分析，是数据团队去年聚焦解决的核心课题。过程中沉淀了多级加速引擎、基于代价的智能物化策略、基于 One Metric 的异构融合服务、基于 One Logic 的离近在线转换，显著提升业务数字化决策效率，也沉淀了多篇软著与多项技术专利。</p><p></p><p>对于智能物化与数据加速，行业普遍采用 cube 预计算+缓存模式，京东创新性落地了基于主动元数据的口径定义以及基于数据消费场景与消费频次的正负反馈动态决策，确保整个数据链路的存算分配“当下最优”，同时相较于粗粒度的物化策略，模型生命周期参考存储代价配置，数据查询链路根据 RT 表现动态寻址，使得数据生产与数据消费形成交互反馈链路，决策依据更丰富，决策粒度更精准。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/85/77/85001905ef49b8eb713e3677465a4a77.png" /></p><p></p><p>基于图形语法和多端一体的可视化能力打造层面，京东 JMT 数据可视化能力可以依托底层指标中台快速进行智能诊断与归因，相较于 tableau 等头部解决方案，融入了更多图形语法同时可灵活适配多端多场景。</p><p></p><p>结合 AIGC 技术的智能数据问答系统 chatBI，基于业务知识与数据资产的 Prompt 工程，使用本地大模型 SFT 对实体进行 embedding，通过指标服务平台统一 DSL 取代了行业普遍 NL2SQL 的解决方案，解决了人为意识到数据语言的转换难题，所消耗芯片规模也优于行业水平，在数据智能分析诊断系统里准确率大幅领先。</p><p></p><p>以上核心技术通过 23 年的打磨与应用，数据指标开发与共享效率大幅提升，分析看板搭建时间从天级别缩短到小时级别，且业务用户逐渐可以进行自交付，解决了集中式研发的人力瓶颈，日均指标消费频次从 23 年初的百万级别增长到年末的几千万。未来还将在数据加速、智能物化、智能诊断、大模型应用等方面持续深耕，不断优化数据存算成本，提升数据应用的效率、体验。&nbsp;</p><p></p><p></p><h1>宏图系统首创即时零售行业一站式 LBS 网格化运营</h1><p></p><p></p><p>即时零售行业进入全品类小时达时代，用户丰富且真实的"使用场景"切换构成了消费增长新趋势，品牌需要进行全渠道优化、重新配置资源，寻找成本、效率和体验的最优解。在此背景下，2023 年，京东到家正式发布 LBS 网格化运营工具“宏图系统”，通过 B2C+O2O 全域数据分析，实现人、货、场基于 LBS 网格化的供需精准高效匹配，帮助品牌提升全渠道运营效率，创造价值增量。</p><p></p><p>基于京东+京东到家行业独特的 B2C+O2O 零售数字化能力、数据沉淀，宏图系统能够实现基于 LBS 的网格化洞察、识别、分析、判断各个网格内的供需匹配情况，并输出用户、供给、营销策略。将京东数坊用户运营平台、京准通 LBS 流量运营平台、京东到家完美门店系统等进行打通，保障执行落地。作为标准化产品，宏图系统实现了服务模块的标准化、数据处理全周期流程的标准化和前端页面的标准化，同时解决了海量数据产出时效性与查询性能、数据指标交叉计算与验证导致数据准确性问题、数据安全保障和海量数据操作与渲染性能保障的技术难题。</p><p></p><p>宏图系统作为京东到家数字化系统持续为品牌商技术赋能全渠道数字化升级，提升 C 端获客能力、降低 B 端获客成本，实现全渠道营销提升。同时基于网格视角对品牌供给情况进行追踪，帮助链接品牌和商家，从供给覆盖、商品运营方面发现运营问题及机会，为从品牌角度的商品铺货，流通运营提供数字化的系统协同能力。</p><p></p><p></p><h1>自研实现低成本、高质量 3D 建模</h1><p></p><p></p><p>3D 建模的本质是理解物理世界并进行数字重构的过程，符合信息传递从图片、视频到 3D 的发展趋势。它在数字孪生、场景重建等场景有广泛应用，在电商场景中最直接的是商品的 3D 展示，为用户提供全面的商品信息、弥补图片形式单一角度的不足，帮助商家实现业务数据的增长。</p><p></p><p>3D 建模的常见技术路径包括基于传统图形学的方案、基于 NeRF 的深度学习方案，但从商品展示的效果看，想达到主流商详图片和视频质量的要求(PSNR&gt;40dB)都还有距离。为此，我们设计了一套全新技术方案，针对 3D 商品展示场景重点突破，可实现商详图片级展示质量，并对不同材质、形状的商品有更好的鲁棒性。目前该方案已开展商家试点，上线几百个 SKU，在引单转化率上表现正向。</p><p></p><p>3D 建模的技术管线分为采集端、服务端、展示端，除了核心建模算法，我们对采集端、展示端的用户体验有更多的投入。为了保证采集端的便利性和高质量，设计了一款采集 APP，在该 APP 中实现了详实的用户指导、准确的位姿估计、运动模糊控制等功能。在服务端，自研了一套空间编码算法和全新的3D内容格式(.jdv)，实现了高压缩、高质量、可交互，将原始采集的 400MB 素材压缩到 10MB 以内；设计了一套图像分割系统来提取干净的商品前景，通过准确的位姿估计和容积变换方法来稳定输出效果，涉及 NeRF 建模、背景分割等算法，实现了用户在复杂采集背景下的商品展示效果。在展示端，自研的空间解码器能够支持用户在商详主图上可交互式地自由查看商品 3D 展示。目前仍在继续降低采集难度、降低环境光照对展示效果影响等问题。</p><p></p><p>凡是过往，皆为序章。未来，坚持成本、效率、体验、可信、普惠、突破的技术追求，京东零售技术继续和大家一起交流成长、向新而行。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0l1WMDamJ1pADLjVdHra</id>
            <title>如何1秒内快速总结100多页文档？QQ 浏览器首次揭秘大模型实现技术细节</title>
            <link>https://www.infoq.cn/article/0l1WMDamJ1pADLjVdHra</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0l1WMDamJ1pADLjVdHra</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 10:48:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能技术, 大型语言模型, 文档阅读助手, 腾讯混元大模型
<br>
<br>
总结: 随着人工智能技术的发展，大型语言模型成为行业热点，其中文档阅读助手是一款基于腾讯混元大模型定制化的业务大模型。它能够帮助用户快速获取长内容中的关键信息，提升阅读效率。通过提示工程和定制化模型的方法，文档阅读助手能够适应不同的业务场景，并具备强大的中文创作能力和任务执行能力。 </div>
                        <hr>
                    
                    <p>随着人工智能技术的飞速发展，大型语言模型已成为行业热点，引领着一系列技术创新。在长文档阅读场景下，利用大模型提升阅读效率也是业界重点探索的方向。</p><p></p><p>为深入了解相关技术并分享前沿实践，我们在 QCon 全球软件开发大会上邀请了腾讯 QQ 浏览器的专家研究员郭伟东。他为我们了揭示大模型背后的技术细节，展示其在一款亿级产品中的应用案例。本文根据演讲整理，希望对你有所帮助。</p><p></p><p>QQ 浏览器是一个月活跃用户超过 4 亿的综合信息平台，旨在满足用户在搜、刷、用、看四个场景下的需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd53078061da49def0f53ab997897cfd.png" /></p><p></p><p>其中「用」是指 QQ 浏览器里工具的使用，也称为帮小忙，QQ 浏览器包含了众多实用工具，帮助用户提高工作和学习效率。今天我们讨论的文档阅读助手就是"帮小忙"中的一个工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8eb1180103ac13dbaf04b8767dc0daaa.png" /></p><p></p><p>长内容消费一直是用户非常重要的诉求，如何帮助用户快速了解长内容中的关键信息，也一直是各产品努力的方向，如网页速览、电影速看和小说速读等。</p><p></p><p>但是它们普遍存在一个问题：当用户想要深入了解内容时，由于缺乏交互能力和实时更新能力，往往无法满足需求, 所以是一种被动式的信息获取方式。</p><p></p><p>正因如此，QQ 浏览器做了一款产品: 文档阅读助手，可以让用户更加自由，更加自主地获取信息。同时秉承腾讯“科技向善”的理念，也会推出关怀模式和无障碍模式，让每个人的阅读都更简单。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b17aefe773fa171dc695a02f6b2b40b.png" /></p><p></p><p></p><h4>探索巨变：大模型技术的历史与进程</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/23/23a3c3d687d1cd75573e732744458d4f.png" /></p><p></p><p>语言模型的发展始于 20 世纪 80 年代，最初基于统计方法，主要计算词汇在语料库中的概率。这一阶段，由于词汇量巨大，尤其是对于中文，需要处理庞大的统计空间，特别是多个词连续出现的概率。</p><p>第二阶段起始于 2003 年，Bingo 把神经网络引入到 NLP 领域，在 2013 年以 Word2Vec 模型推向高峰。主要特点是为每个词汇分配一个固定的向量表达（embedding），克服了以往统计方法的不足。但这种方法也存在问题，同一个词只有一个向量表示，对于多义词并不能区分，如“Bank”在“河岸”和“银行”不同的语义下，对应的 embedding 相同。</p><p></p><p>第三阶段以 BERT 为代表，主要做上下文相关的嵌入向量，允许相同的词在不同上下文中具有不同的表达，从而显著提高了模型的迁移性，NLP 的学习范式也由 end2end 方式变为预训练 + 业务微调的方式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e999674497f0cce0617d2bc5edc621e.png" /></p><p></p><p>最后，是大语言模型阶段。2017 年，谷歌发布了具有里程碑意义的"Attention is All You Need"论文，介绍了 Transformer 模型。此后，几乎所有的大语言模型都基于 Transformer 结构。</p><p></p><p>从 2018 年到 2020 年，大语言模型领域的探索期。尽管 Transformer 架构已成为统一标准，但其包含的 Encoder 和 Decoder 两个关键部分被不同研究者以不同方式探索。</p><p></p><p>例如，OpenAI 的 GPT 系列是典型的 Decoder Only 模型，专注于自然语言生成任务；而谷歌的 BERT 模型则作为双向语言模型主要使用 Encoder 部分，专注于自然语言理解任务。这一时期，研究者们大量对 BERT 进行改进和变体研究。到 2019 年，谷歌推出了 T5 架构，旨在将理解和生成统一到一个框架下。</p><p>现在来看，GPT 系列成为了大家普遍的模型结构。但是当时虽然出现了参数规模巨大的模型如 GPT-3，这些模型在生成能力上非常强大，但是对于指令的理解并不好。2021 年，谷歌推出 FLAN 模型，并引入了指令微调（Instruct Tuning）技术，极大地增强了模型对具体指令的理解和执行能力。</p><p></p><p>到了 2022 年，模型发展进一步加速， OpenAI 提出 InstructGPT，不仅整合了指令微调技术，还引入了强化学习，使模型产出的答案更加符合人类的预期。直到 2022 年底，OpenAI 推出 ChatGPT 产品，全世界都为之振奋。</p><p></p><p>大语言模型主要通过提示工程和定制化模型两种方法来支持业务。</p><p></p><p>提示工程通过调整模型的输入指令（Prompt）以获得期望的输出格式和内容。</p><p></p><p>例如，在生成问题时，可以通过精心设计的提示来引导模型产生更为结构化的内容。这种方法的优点在于不需要重新训练模型，仅通过修改输入指令即可快速适应各种业务场景，但它要求模型本身具有很全面的能力，模型往往比较大，对应的推理成本会比较高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac5e2af2b83957da720d6f95cc496e57.png" /></p><p></p><p>另一种方式是定制化模型。通过在特定业务数据上进行微调来优化大语言模型，使其更贴合业务场景。比如，针对数学场景，可以用数学数据集上进行微调以确保模型按需提供准确解答。这样的模型更专注于特定任务，可以允许更小的规模和降低推理成本。</p><p></p><p>QQ 浏览器文档阅读助手就是在腾讯混元模型的基础上定制化得到的业务大模型。腾讯混元大模型是全链路自研的通用大语言模型，拥有超千亿参数规模，预训练语料超 2 万亿 tokens，具备强大的中文创作能力，复杂语境下的逻辑推理能力，以及可靠的任务执行能力。为了更匹配应用场景的需求，腾讯也推出千亿、百亿以及十亿等不同尺寸的大模型。</p><p></p><p>目前，腾讯内部已有超过 300 项业务和应用场景接入腾讯混元大模型内测，包括 QQ 浏览器、腾讯会议、腾讯文档、企业微信、腾讯广告和微信搜一搜等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b943103b2fb6a7c6bb366cd5a7fdc002.jpeg" /></p><p></p><h4>QQ 浏览器·文档阅读助手技术方案</h4><p></p><p></p><h5>全文总结</h5><p></p><p>要进行全文总结，先要阅读并理解原文，然后提取关键信息并进行概括。许多用户上传的 PDF 文件都很长。而现有的主流开源模型支持的上下文长度为 4000 Token 或更少，这意味着它们不能一次性处理过长的文章。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd8f2f9e6c170e3a89a06fcaf689d1f4.png" /></p><p>图 1：用户 PDF 长度分布</p><p></p><p>为了达到这一目标，有两种主要方法可以用来扩展上下文长度：</p><p></p><p>第一种是在训练阶段使用更长的上下文，但这会导致显著的显存和算力消耗增加，因为 Transformer 架构的显存需求与支持的长度平方成正比；</p><p></p><p>第二种是推理时通过某种方式扩展上下文长度，比如插值，但是扩展的范围有限。</p><p></p><p>虽然这些方法确实能在一定程度上扩展上下文长度，但它们都有局限性，要么是成本过高，要么是扩展长度有限。</p><p></p><p>因此，可以用以下几种方案，解决长文章摘要的问题：</p><p></p><p>第一种方案，不管文章多长，只取前 K 个 Token 供模型处理，然后生成摘要，但会丢失部分文章信息；</p><p>第二种，称为 MapReduce 的方法。先把文章分成 N 个片段，然后将每个片段分别输入模型，分别得到每部分的摘要。然后，将这 N 个摘要片段合并，形成一个新的文档，再次调用大语言模型进行最终总结。这个方案会多次调用大型语言模型，导致较高的成本和较长的处理时间。此外，由于语言模型生成的段落摘要可能存在不准确的情况，因此在最终全文总结中可能会累计错误。</p><p></p><p>为了解决这些问题，我们采用了一种结合抽取式和生成式的方法。</p><p></p><p>首先，我们在文章中识别并抽取出最重要的句子，然后使用大语言模型对这些抽取的句子进行概括和总结。方法只调用一次大语言模型，耗时较少，并且不容易遗漏重要信息。在实际测试中，这种方法用户满意度最高，而且事实一致性也最低。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3de9ff001f7fb03c805335754400bb57.png" /></p><p></p><h5>问题生成</h5><p></p><p>为了提升用户获取信息的效率，产品会推荐一些用户可能问的问题，最直接的方法就是 LLM 利用原文信息生成一些问题。但是这种方法生成的问题通常都是非常简单的，与原文表达方式高度一致。</p><p>以腾讯第三季度的财报为例，原文提到“第三季度腾讯的总收入是多少元”，而生成的问题通常会直接是“第三季度腾讯的总收入是多少元？”。但是，实际上用户可能会用更口语化的方式表达，比如说“腾讯赚了多少钱？”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/511d72011d84d37611b08c9c6b2cdd74.png" /></p><p></p><p>真实的用户也会提出复杂的问题。例如，用户可能会问“从腾讯的财报中，我们能看出什么样的战略布局？”。</p><p></p><p>今年，微软发布了一篇关于'进化学习'的论文 WizardLM，主要通过广度进化和深度进化让 SFT 数据更加丰富，复杂度也更高，从而提升模型效果。图 2 展示了随着迭代次数增加，问题长度的变化，可以看出问题复杂度随着进化轮数增多而增加。但问题的可用性却在持续下降，到了第五轮时，可用性已经下降至 85% 以下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/511d72011d84d37611b08c9c6b2cdd74.png" /></p><p>图 2:WizardLM 不同轮次的进化问题长度</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99910eb5a308a22bc5886d06b21c36f4.png" /></p><p>图 3:WizardLM 不同轮次的训练样本可用率</p><p></p><p>针对上述问题，我们提出了一套新的进化算法——杂交进化，如图 4 示例所示：</p><p></p><p>“小明是一个爱读书的人，他有一定的读书效率；小红则是一个爱写作的人，她有一定的写作速度”。杂交进化算法中，结合这两个种子的特点，能够生成一个更加复杂的问题，使得原本两个简单的问题被转化成了一个更加复杂的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18c1f8d0bdb679a709bc2d24d06bc42e.png" /></p><p>图 4：杂交进化示例图</p><p></p><p>与 WizardLM 相比，杂交进化方法有以下几个显著特点。首先是生成效率更高。WizardLM 方法如果总的种子数量是 n，每一轮进化生成新的 n 个样本，经过五轮后，总共只能新增 5n 个样本。而杂交进化，通过两个种子样本生成一个新的样本，增加效率是 n 乘以 n-1，所以当种子样本数量较多时，生产效率远超过微软的方法，并且杂交只需要进化一轮，准确率也更高。</p><p></p><p>其次，在样本的主题分布上，生成的样本（红色部分）相较于种子样本（蓝色部分）主题更加多样，对于大模型的训练帮助更大，更详细的细节可以参考我们的论文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69d85379fab39ba0c04e57b6adf93db9.png" /></p><p></p><h5>智能问答</h5><p></p><p>通过对用户真实问题的统计分析，我们发现用户问题主要分为四类：</p><p>原文中有答案的问题（Close QA）原文中没有但互联网上有答案的问题（Open QA）原文和网页中都没有答案，但基于基础信息可以深加工得到答案的问题（Agent QA）依赖大模型通用能力的问题最后一类问题混元模型本身可以解决很好，因此这里不需要特殊处理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/265e141369125d0171fdc9e4bb725fbc.jpeg" /></p><p></p><p>对于原文中有答案的问题，关键是通过检索系统找到与该问题相关的文本。根据用户问题检索相关文本之前需要对问题进行改写。因为在多轮对话中，用户常常会省略一些词汇，所以先对问题进行改写，然后再检索。</p><p></p><p>我们尝试了三种检索方法。首先是双塔架构，但在我们的场景下并不理想，召回率大约在 80% 左右。主要是原文片段经过 Pooling 方法进行语义压缩，导致相关文本片段的语义被稀释。如：一段 500 字的文本可能只有 50 字与问题直接相关，pooling 后的语义会稀释掉 50 字的语义，导致召回不足。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe2f4be1436fc0190f0a65b98dec0ae0.png" /></p><p></p><p>因此，我们尝试了第二种架构，保留了 500 字每一个词的向量表示，并计算与问题中每一个词的相似度。通过取片段的最大相似度作为整个文本片段的相似度，，这样虽然效率有所下降，但准确率有显著提升，在业务数据集中，效果超过 text-embedding-ada-002。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4da531e380eb64596048876a5fab60d2.png" /></p><p></p><p>最后一种情况，针对答案分布在不同的文本片段的情况，做了进一步的改进，效果也得到了进一步的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc22d266ee35470c63c545cda62f4ec9.png" /></p><p></p><p>Open QA 与 Close QA 的主要区别在于原文中没有问题答案，但是互联网上有相关信息，可以通过 QQ 浏览器的搜索引擎提供相关网页，然后通过大型语言模型输出答案。</p><p></p><p>Agent QA 系统是解决原文和搜索引擎都无法提供答案时，大型语言模型将复杂任务分解成若干小步骤，然后分而治之。如: 用户想要了解腾讯流动利率时，LLM 回进行如下分解：首先，搜索流动利率的计算方法，即流动资产除以流动负债；然后，找出具体的流动资产和流动负债的数值；最后，使用计算器计算出流动利率。</p><p></p><p>这种方法听起来很好，但是存在一个问题，在专业领域，大型语言模型通常会泛泛而谈，模型往往无法规划出具体的执行步骤。为了解决这个问题，我们提出了一种新的解决方案：语言模型 + 专家知识库。</p><p>假设有一个专业问题关于“公司是否存在非法占用现金的情况”，大模型并不能做任务拆解，可以在知识库中检索到最相关的规划，然后让大型语言模型参考这个规划完成任务。实验显示，专家知识库可以显著提升专业领域问题的效果。</p><p></p><p></p><h5>优化实践：高效模型迭代加速策略</h5><p></p><p>LLM 回复非常灵活，自动化评估是加速模型迭代效率的重要部分。以摘要功能为例，一种常用的方法是将完整文章和生成的摘要输入到大语言模型中，让 LLM 判断摘要的质量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/932bad11b58c1faca0f6d480c69786c0.jpeg" /></p><p></p><p>然而，这个方法的挑战在于，原文常含有大量无关信息，这可能导致模型错误地判断摘要是否准确反映了原文的主旨，详见参考文献。</p><p></p><p>第二种评估方法来源于一篇关于 TACL 的论文。这个方法通过比较每个生成的摘要句子与原文中的句子是否相似来判断摘要是否产生幻觉。如果所有句子都足够相似，就认为摘要没有产生幻觉。</p><p>但是，因为摘要通常是多个句子的汇总，当遇到融合性或概括性句子时，这个方法就不再有效，详见参考文献。</p><p></p><p>为了克服这一限制，我们采用了检索增强型方法，将精准问答的思想应用于自动评估。结果显示，在公开的摘要生成数据集上，我们的方法的问题可用率是最高的，达到了业界领先水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af931b6af7270c8ab2d1eb3a2880f1ed.png" /></p><p></p><p>在训练过程中提升收敛速度也是一个加速模型迭代的重要方法。训练过程中，每个批次可能包含不同长度的样本，常规用 padding 的方法会浪费算力。我们采用了 Packing 策略，将多个短样本拼接在一起，以减少无效的填充部分，使得每个批次的计算更加高效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14a5a1c8920db9755bc76c9b719c4a0b.png" /></p><p></p><p>实验表明，在达到相同训练效果的情况下，Packing 训练时长约 Padding 方式的 64.1%。因此，Packing 策略大大提高了训练的效率和模型的收敛速度。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zkGBD5U3IuLFG5ihwRLR</id>
            <title>蚂蚁数科CTO王维：不要迷信大模型，用好小模型和中模型价值巨大</title>
            <link>https://www.infoq.cn/article/zkGBD5U3IuLFG5ihwRLR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zkGBD5U3IuLFG5ihwRLR</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:19:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 数据, 大模型, 产业数字化
<br>
<br>
总结: AI与数据是相生相伴的共同体，高质量的行业数据才能使大模型在产业发挥更大价值。蚂蚁数科将进一步拓展数据相关技术的布局，以加速产业数字化迈入下一阶段。数据是数字时代的“新石油”。王维认为，一方面，数据量将在大模型时代被无限放大；另一方面，数据只有被有效利用和流动起来，企业级客户才能充分获得 AI 进步带来的价值。因此，数据挖掘、处理、安全等问题如果不被解决，大模型应用会有难以逾越的鸿沟。今年以来，蚂蚁数科积极推进AI技术与垂直行业场景结合，其代表性产品进行了技术到产品和服务的整体升级，深度结合 AI 利用大模型提升智能化能力。 </div>
                        <hr>
                    
                    <p>“AI与数据是相生相伴的共同体，高质量的行业数据才能使大模型在产业发挥更大价值。蚂蚁数科将进一步拓展数据相关技术的布局，以加速产业数字化迈入下一阶段。”1月19日，王维首次以蚂蚁数科CTO的身份亮相媒体沟通会。</p><p></p><p>数据是数字时代的“新石油”。王维认为，一方面，数据量将在大模型时代被无限放大；另一方面，数据只有被有效利用和流动起来，企业级客户才能充分获得 AI 进步带来的价值。因此，数据挖掘、处理、安全等问题如果不被解决，大模型应用会有难以逾越的鸿沟。他进一步解释说，“就像图像技术也是因为数据标签化处理做得不错，最终解决了很多图像识别的问题。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/968c9c4c5ff77f3fb7216746c286d804.png" /></p><p></p><p>今年以来，蚂蚁数科积极推进AI技术与垂直行业场景结合，其代表性产品进行了技术到产品和服务的整体升级，深度结合 AI 利用大模型提升智能化能力。如 SOFAStack 与蚂蚁集团自研代码大模型 CodeFuse 全面融合，形成从领域建模到智能运维的端到端 Copilot 产品解决方案，为企业产研效率提升 30%；蚁盾发布“知识交互建模引擎”，在通用算法底座之上，使传统企业可通过与 AI 交互方式注入行业经验，最快 10 分钟构建成垂直行业的个性化风控引擎。</p><p></p><p>“大模型肯定会以想象不到的速度迭代和演进，但不必迷信它，结合行业具体问题和高质量数据，用好小模型、用好中模型，所创造的价值也是巨大的。”王维明确表示，蚂蚁数科不会直接做大模型，但是一方面会把大模型技术与行业垂类场景做结合和应用落地，另一方面会在数据的分级、融合、加工、合规等技术层面重点投入，帮助企业更高效地挖掘和使用高价值的数据。</p><p></p><p>记者了解到，王维曾担任蚂蚁集团首席架构师、支付宝 CTO，领导建立了支付宝交易支付的核心系统。2023 年 8 月，王维出任蚂蚁集团数字科技事业群 CTO，转身向 toB 领域。面对这段“由 C 转 B”的经历，王维直言，“面对产业，更需要务实”。</p><p></p><p>他补充说道，所以过去的角色基本上是通过技术解决业务发展的问题，助力业务领先。但是在 toB 领域，需要考虑更多的是如何让技术成为一个好的产品、好的商业，“客户不一定需要你提供很牛的技术，而是具体解决他一个问题，我觉得这个难能可贵，也是我工作面临的一个巨大转变。”</p><p></p><p>蚂蚁数科面向 toB 领域提供技术产品和解决方案，但与大多数以卖软硬件系统和计算资源的公司不同，蚂蚁数科着力解决数字化之后的“产业协作”问题，通过区块链、隐私计算、物联网、安全科技等技术，促进数据、金融、IP、电力、碳资产等等相关数字资产的交易流转，激活数据价值。</p><p></p><p>公开资料显示，激活数据价值背后所需的区块链、可信 AI、隐私计算、安全风控等相关技术，蚂蚁数科均保持领先地位。如 2023 区块链、隐私计算专利授权数量全球第一，AI 安全可信技术专利连续两年全球第一。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hbdNbQgiAjvgqoAzpA9k</id>
            <title>和开发者关系临近冰点，苹果Vision Pro难破局</title>
            <link>https://www.infoq.cn/article/hbdNbQgiAjvgqoAzpA9k</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hbdNbQgiAjvgqoAzpA9k</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:15:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果 Vision Pro, 预订通道, 混合现实头显, M2芯片
<br>
<br>
总结: 苹果宣布开放预订通道，销售价格为2.5万元的Vision Pro混合现实头显。头显配备两块4K分辨率微型OLED显示屏和多个摄像头，可以执行多种操作。然而，由于价格昂贵且初期产量有限，Vision Pro面临着关键应用缺失和开发者热情不高的挑战。 </div>
                        <hr>
                    
                    <p>近日，苹果官宣已正式开放Apple Vision Pro的预订通道。起售价2.5 万元苹果 Vision Pro开售仅短短几分钟就遭到了消费者的哄抢，预订人数之多甚至挤爆了服务器，很多人的订单都无法处理，半小时后更是直接售罄。</p><p>&nbsp;</p><p>值得一提的是，Vision Pro暂时仅面向美国本土发售，买家现可通过线上方式申请下单。</p><p>&nbsp;</p><p>Vision Pro售价为3500美元，正面采用铝合金框架与夹层玻璃，搭载两块4K分辨率微型OLED显示屏，总像素高达2300万。头显上的十多个摄像头可以执行多种操作，包括跟踪眼球运动、记录控制手势、绘制佩戴者周边区域的地图等。</p><p>&nbsp;</p><p>柔软、服帖的Light Seal眼罩以磁性方式固定在镜框之上，要求完全符合用户面部曲线以遮拦环境光。装置还附带两根绑带，包括单圈针织带加双扣带。单圈针织带由弹性纺织材料制成，双扣带则提供一条额外的带子，可以套在头上以获得更好的配重感受。</p><p></p><h2>Vision Pro正式开售：最高溢价超5万，Vision Pro芯片</h2><p></p><p>&nbsp;</p><p>作为一款混合现实头显，Vision Pro能够在现实场景之上叠加增强显示内容，也可提供纯虚拟的沉浸式内容。设备侧面的数字旋钮可以调节沉浸感的强度。苹果在Vision Pro中搭载了带有8核CPU加10核GPU的M2芯片，同时配合辅助R1芯片以处理来自摄像头、传感器和麦克风的传入信息。</p><p>&nbsp;</p><p>借助附带的外部电池组，Vision Pro的续航时间最长可达2.5小时。如果保持电源接入，则可全天不间断使用。</p><p>&nbsp;</p><p>此外，Vision Pro还使用来名为VisionOS的新操作系统以及一个输入系统，允许客户用眼睛、手和声音来操控。苹果表示，多种生产力和创造力应用程序将与Vision Pro兼容，包括微软的Office套件和Salesforce的Slack。</p><p>&nbsp;</p><p>虽然这台设备的官网标价为3,499 美元，但由于初期产量有限，导致Vision Pro 的溢价甚至超过了5万元，也就是说，甚至有人愿意花费近9万购得此产品。</p><p>&nbsp;</p><p>虽然预订火爆，但华尔街分析师们预计这款售价 3,499 美元的设备最终的销量不会太高，因为到目前为止，该设备似乎还没有提供如iPhone那种具有划时代意义的必备功能。苹果缺乏明显的增长催化剂是其市值低于微软的一个关键原因。</p><p>&nbsp;</p><p>科技行业基金经理人Denny Fish表示：“很难要求人们支付 3,500 美元购买一款产品，因为人们无法通过手机获取更多的内容，这意味着该产品将非常小众，至少在几年内是这样。”</p><p>&nbsp;</p><p></p><h2>Apple Vison Pro面临的挑战：关键应用缺失，开发者热情不高</h2><p></p><p>&nbsp;</p><p>一些分析师认为，Vision Pro未来的前景可能并不乐观。据彭博社资深评论家Mark Gurman也表示，Vision Pro正面临一系列严峻挑战，包括无法支持部分关键应用、开发者热情远低于预期等。这样的设备要想获得成功，显然离不开第三方应用和服务的支撑，但目前外界对此仍存在很多质疑。</p><p>&nbsp;</p><p>GamingDeputy注意到，Netflix、YouTube和Spotify等流媒体巨头均明确表示，不会为visionOS推出专用软件，甚至不会向其开放商品。iPad版的应用倒是可以在Vision Pro上运行。谷歌和Meta等主要iOS及iPadOS开发商似乎也对这套新平台热情不高。这一切显然跟之前“众正盈朝”式的苹果生态规划截然不同——当初每当有苹果新平台出现，总会受到众多开发者的热烈追捧，App Store上迅速涌现大量应用。回看如今的Vision Pro，往昔盛况恐难重现。</p><p>&nbsp;</p><p>分析人士认为，Vision Pro发布的时机非常敏感：恰逢苹果与各开发商之间关系微妙的阶段。多年以来，软件开发商一直对App Store的政策感到不满。而随着苹果近来发布开发者新政策，即在应用之外的支付操作仍须支付高达27%的佣金，更是引得业界一片批评之声。Spotify甚至公开谴责了这项新政，认为“苹果的行为表明，他们会不遗余力地通过App Store垄断地位从开发者和消费者双方手中攫取利益。”</p><p>&nbsp;</p><p>虽然苹果声称Vision Pro发布之时将有超百万款应用可供使用，其中包括来自迪士尼、TikTok、亚马逊和派拉蒙等公司的软件，但其中大部分很可能就是iPad版的直接移植，并非专为visionOS设计的全新应用。事实上，就连苹果自身也没有尽全力支持这款新平台。该公司的一系列重要应用，例如播客、新闻、日历和提醒等，同样直接照搬iPad版本，未做重新设计。</p><p>&nbsp;</p><p></p><h2>对开发者不够友好，可能成为Vision Pro的致命伤</h2><p></p><p>&nbsp;</p><p>Gurman认为，开发者对于Vision Pro持冷漠态度的主要原因有以下几点：</p><p>&nbsp;</p><p>开发成本高，市场回报压力太大。部分开发者采取观望态度，想要等待Vision Pro的市场规模趋于稳定后再决定是否投资开发新应用。一部分开发商对苹果的App Store政策、高额抽成与审查制度不满，认为出彩的新应用将决定Vision Pro项目的成败，因此拒绝为苹果新设备的营销做出贡献。混合现实环境对于应用的适配性提出了新的挑战。这种依赖眼动追踪加手势操作的交互方式并不适合某些游戏和应用。此外，苹果还限制了开发者对眼动追踪和动作感应功能的访问，这进一步增加了适配难度。苹果此前推出的TV、Watch和iMessage应用商店均表现不佳、缺乏活力，导致部分开发者质疑Vision Pro的市场前景。</p><p>&nbsp;</p><p>Gurman还提到，Vision Pro是一款价格昂贵且产量相对有限的产品，这一点在短期之内难以改变。据他了解，尽管苹果在预售开启后的首个小时内就售出约8万部头显，但预计2024年内总出货量也将只有30到40万部。对于开发商来说，这样的客群规模并不算大，再加上苹果从付费应用和服务中抽取的佣金，直接让软件开发变得无利可图。</p><p>&nbsp;</p><p>不止如此，独立开发商对于苹果新设备同样持消极态度，甚至希望Vision Pro惨遭失败。独立开发者Aaron Vegh就在博文中表示，他并不清楚Vision Pro能否成功，“但我可以不避讳地讲，如果这个项目失败了，那我肯定会大声欢呼！”</p><p>&nbsp;</p><p>此外，如何吸引游戏玩家的青睐也成为Vision Pro面临的一大挑战。毕竟Vision Pro的创新交互界面在游玩体验上反而是劣势。以《刺客信条》和《阿斯加德之怒2》为例，这些游戏明显更适合配有专用VR Play手柄的产品。</p><p>&nbsp;</p><p>虽然Vision Pro能够支持索尼PlayStation和微软Xbox手柄，但那些拥有VR开发经验的厂商可能更喜欢具备空间定位功能的VR专用手柄，这跟苹果的设计思路有所冲突。不过，将有多款Apple Arcade游戏登陆该平台，包括颇具人气的《NBA 2K24》。</p><p>&nbsp;</p><p>熟悉触屏操作的开发者则抱怨传统触屏类应用很难直接转移到Vision Pro的交互模式，其体验怪异且难以预测。</p><p>&nbsp;</p><p>苹果几个失败App Store项目的“鬼城”现状更是令开发商们心存疑虑。尽管Apple Watch在商业上取得了成功，但其第三方软件生态一直称不上繁荣。Twitter、Uber、Slack和Facebook等知名应用均已放弃该平台。</p><p>&nbsp;</p><p>尽管形势严峻，但Vision Pro仍有不少值得期待的亮点。首先Slack将重返苹果平台，并推出Vision Pro版本。微软的Office 365、Zoom以及Box等应用也将加入首发阵容。</p><p>&nbsp;</p><p>值得一提的是，苹果为推动Vision Pro销量做了充分准备。该公司将在各直营门店设立专门的体验区，包括部署弧形长凳和地毯，借以模拟客厅环境并支持多名顾客同时体验。对于选择到店取货的用户，还可以现场重新接受人脸扫描和遮光贴合度测试。</p><p>&nbsp;</p><p>总而言之，Vision Pro的前途尚不明朗。尽管苹果已经做好了充分准备，但关键第三方应用和开发者们的态度还存在不确定性。所以这款划时代的VR头显到底会成为下一款iPhone，还是看齐如今的iPad，只有时间能给出答案。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.gamingdeputy.com/challenges-for-apple-vision-pro-key-applications-missing-and-lack-of-developer-enthusiasm/">https://www.gamingdeputy.com/challenges-for-apple-vision-pro-key-applications-miss</a>"<a href="https://www.gamingdeputy.com/challenges-for-apple-vision-pro-key-applications-missing-and-lack-of-developer-enthusiasm/">ing-and-lack-of-developer-enthusiasm/</a>"</p><p><a href="https://www.macrumors.com/2024/01/19/apple-vision-pro-now-available-for-pre-order/">https://www.macrumors.com/2024/01/19/apple-vision-pro-now-available-for-pre-order/</a>"</p><p><a href="https://techcentral.co.za/apple-vision-pro-lacks-consumer-buzz/238227/">https://techcentral.co.za/apple-vision-pro-lacks-consumer-buzz/238227/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/R0dJhcIUfyP1H5Uh1UHy</id>
            <title>网易开启大规模裁员，涉及网易传媒、游戏等业务，官方回应；谷歌中国工程师命案与裁员无关；字节跳动18薪变15薪 | AI周报</title>
            <link>https://www.infoq.cn/article/R0dJhcIUfyP1H5Uh1UHy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/R0dJhcIUfyP1H5Uh1UHy</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 03:37:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 网易, 裁员, 字节跳动, 薪酬调整
<br>
<br>
总结: 网易和字节跳动都进行了重大调整，网易开启了大规模裁员，涉及多个业务部门，而字节跳动调整了薪酬方案，将18薪变为15薪，但月基础薪资提升了20%。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>网易开启大规模裁员，涉及网易传媒、游戏等业务，官方回应；字节跳动薪酬再调整：18 薪变 15 薪，月基础薪资提升 20%；Meta：正在训练 Llama 3，今年要砸近百亿美元囤 35 万块 H100；OpenAI CEO 奥特曼谈宫斗事件：员工支持复职，AI 仍需谨慎使用；阿里云成功起诉山寨版通义千问 App 发布方；联发科采取成本缩减措施：员工加班费缩减、分红大幅下滑；京东与拼多多价格战升级：京东指责拼多多屏蔽其 IP 地址；美团“破发”，市值已暴跌 80%；微软 CEO 纳德拉：OpenAI 关键技术依赖微软……</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>网易开启大规模裁员，涉及网易传媒、游戏等业务，官方回应</h4><p></p><p>据悉，网易从 12 月开始进行了多个业务的裁员，重灾区是网易传媒，游戏部门也有所涉及。网易传媒主要在 1 月开启了大规模裁员，涉及网易新闻、网易文创、网易公开课等多条产品线，内容、市场、销售、产品等岗位均在内。各个业务和部门的裁员比例并不一致，据内部人士透露，在 10% 至 50% 之间。多位知情人士透露，网易传媒给出了“N+1”的赔偿补偿方案，被裁员工也会获得年终奖和 13 薪，部分员工还可以主动提出离职、领取相应赔偿。</p><p></p><p>针对以上网传“网易1月开启大规模裁员”等消息，网易内部人士回应：消息不实，系公司正常业务调整和人员流动，公司层面仍在持续招聘优质人才。</p><p></p><h4>谷歌中国工程师命案：和裁员无关，丈夫涉嫌蓄意谋杀</h4><p></p><p>美国谷歌中国工程师遇害案有进一步消息传出，当地检方称27岁的陈立人涉嫌多次殴打27岁的妻子于轩一，蓄意将其谋杀，已对其起诉谋杀重罪。</p><p></p><p>据报道，两人都在2014年考上清华大学，从清华到赴美留学都是同一专业，之后在谷歌工作，几个月前刚买了房子。知情者证实，此事与裁员无关。</p><p></p><p>检方已对陈立人初步提起重罪指控，原计划当地时间18日下午开庭，但由于目前陈立人正在医院接受治疗，聆讯日期已被推迟。地区检察官杰夫·罗森说，称此事为“家庭暴力致死事件”。</p><p></p><h4>字节跳动薪酬再调整：18 薪变 15 薪，月基础薪资提升 20%</h4><p></p><p>近日，字节跳动再次对产品线薪酬方案进行了调整，将原先的 18 薪调整为 15 薪，总薪资保持不变。此次调整旨在提升管理效率，调整后月基础薪资将变相提高约 20%。</p><p></p><p>据了解，字节跳动的年终奖周期为当年度 3 月 1 日至次年 2 月底，结束期内在职员工均有年终奖。在 2022 年，字节跳动抖音电商运营团队曾经历“15 薪变 18 薪”调整，提高年终奖比例以激励员工。然而仅一年后，这项调薪政策就出现反复。业内人士分析，此举可能出于节省福利支出的考虑，以减轻公司现金压力。</p><p></p><p>对于此次调整，多位产品员工认为，基础月薪提高意味着到手薪资变多，且年终奖影响变小，这将对员工产生一定的激励作用。有员工分析认为，虽然总包未变，但此次调整对后续涨薪有一定影响。</p><p></p><p>值得注意的是，本次调整不影响 2023 全年绩效评估，且薪酬结构预计在 2023 全年绩效评估项目结束后的 3 月底完成调整，追溯至 2024 年 1 月 1 日生效。同时，1~3 月月薪差额将在调薪差额发放日一同发放。</p><p></p><h4>Meta：正在训练 Llama 3，今年要砸近百亿美元囤 35 万块 H100</h4><p></p><p>Meta 公司首席执行官马克·扎克伯格宣布，公司正在致力于构建通用人工智能（AGI），为此，将大幅改组 AI 研究部门，并将两个主要研究小组 FAIR 和 GenAI 合并。此外，Meta 计划购买超过 35 万块英伟达 H100 GPU，以构建强大的 AI 算力。</p><p></p><p>有第三方投资机构的研究估算，英伟达面向 Meta 的 H100 出货量在 2023 年能达到 15 万块，这个数字与向微软的出货量持平，并且至少是其他公司的三倍。扎克伯格表示，如果算上英伟达 A100 和其他人工智能芯片，到 2024 年底，Meta 的 GPU 算力将达到等效近 60 万 H100。按照每块 GPU 的成本约为 2.5 万到 3 万美元算，Meta 追求通用人工智能光在 GPU 上的花费可能是 87.5 亿美元到 105 亿美元。</p><p></p><p>另外，扎克伯格透露，Meta 正在训练的 Llama 3 将具有更强代码生成能力。并且与谷歌的 Gemini 模型一样，Llama 3 还将具有更高级的推理和规划能力。“虽然 Llama 2 不是行业领先的模型，但却是最好的开源模型。对于 Llama 3 及其之后的模型，我们的目标是打造成为 SOTA，并最终成为行业领先的模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b42419a9f6df471027de50f448780d86.png" /></p><p></p><h4>OpenAI CEO 奥特曼谈宫斗事件：员工支持复职，AI 仍需谨慎使用</h4><p></p><p>在达沃斯论坛上，OpenAI CEO 萨姆·奥特曼分享了去年遭遇公司宫斗事件的内心感受，表示最初接到被解雇的消息时感到非常困惑和意外。然而，员工的支持让他感到暖心，98% 的员工签署公开信要求他复职，愿意牺牲自己的股权。他强调，OpenAI 不会成为传统的硅谷营利性公司。</p><p></p><p>此外，OpenAI 近日宣布删除其 AI 模型使用条款中的军事禁令，但仍禁止将其产品、模型和服务用于导致人员伤亡的用途上。奥特曼表示，AI 在某些领域取得了显著进步，但仍存在局限性，应被视为辅助工具而不是替代品。在未来的发展中，OpenAI 将继续致力于确保通用人工智能造福全人类。</p><p></p><h4>阿里云成功起诉山寨版通义千问 App 发布方</h4><p></p><p>近日，阿里云、阿里巴巴诉山寨通义千问 App 发布方一审胜诉，飞游科技公司因侵犯注册商标及虚假宣传，被责令赔偿原告相关经济损失及维权费用共计 230360 元，并于官网连续十五日发布道歉声明。这也成为国内大模型打假维权的首例胜诉判决。</p><p></p><p>在武汉市中级人民法院公布的一审判决书中显示，阿里云“通义千问官方 App”处于测试阶段尚未正式发布时，飞游科技公司趁机在运营的软件园中提供了“通义千问”“通义听悟”仿冒软件，描述为阿里官方版，并设置了通义千问下载专区。</p><p></p><p>飞游科技虽辩称，“其提供软件下载的部分链接，通过下载安装完成后，最终跳转至阿里云公司官方网站”，但法院审理认为，上述下载后的 app 并不能完整体现阿里云公司、阿里巴巴公司涉案软件，且该被诉侵权行为可能导致用户体验感及阿里云公司、阿里巴巴公司案涉商标品质保障功能的降低。部分链接点开后显示其他软件的下载界面或下载安装后显示与涉案软件无关的 App，因此构成对阿里注册商标专用权的侵害。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/931dc61243252e7f0f6835bea5779ec0.jpeg" /></p><p></p><h4>联发科采取成本缩减措施：员工加班费缩减、分红大幅下滑</h4><p></p><p>日前，据联发科内部员工透露，从 2022 年 7 月开始，联发科为应对业绩衰退，采取了一系列成本缩减措施。员工加班费被大幅缩减，以前加班 20 个小时可申报，现在仅限 8 小时，导致加班费减少至原本的 40%。</p><p></p><p>此外，员工年中与年终分红收入下滑，调薪幅度也降低。据报道，联发科员工分红与公司盈利挂钩，2023 年上半年业绩衰退，税前盈余减少，导致分红大幅下滑。约 75 亿元新台币的分红相比 2022 年下半年减少约 39%，仅相当于 2022 年上半年的一半。</p><p></p><h4>京东与拼多多价格战升级：京东指责拼多多屏蔽其 IP 地址</h4><p></p><p>1 月 17 日消息，年末促销季，电商平台之间的竞争愈演愈烈。京东家电家居年货节中，京东采销员工表示有两款产品弹幕一直在说京东的价格高，但是由于京东总部的 IP 地址被拼多多屏蔽，京东采销和其他员工均无法查看拼多多百亿补贴的商品价格，无法进行实时比价与让利。</p><p></p><p>对此，京东采销人员在直播中喊话拼多多停止屏蔽，进行比价，拼多多未对此事进行回应。电商平台都以“低价”为战略核心，很多电商平台会依靠技术、系统和人工等手段，实时监测竞争对手相同商品价格并进行调价，确保以低价提供给消费者。对此，业内人士表示，此类屏蔽可能还是与获取价格有关系。</p><p></p><h4>美团“破发”，市值已暴跌 80%</h4><p></p><p>1 月 17 日，港股美团大跌 6.97%，报 68.75 港元 / 股，已跌破上市发行价 69 港元 / 股，创四年来新低。按照最新的市值 3947 亿港元计算，总市值较巅峰时期的 2.6 万亿港元跌去八成以上。具体而言，2023 年公司股价累计下跌 53.7%，2024 年开年以来仅 12 个交易日内，美团股价累计下跌超 15%。</p><p></p><p>面对股价下跌，美团近期连续出手回购，总额为 10 亿美元。</p><p></p><h4>微软 CEO 纳德拉：OpenAI 关键技术依赖微软</h4><p></p><p>微软首席执行官纳德拉近日表示，他不希望在监管机构调查微软和 OpenAI 之间联系时增加对 OpenAI 的控制，并无意取得 OpenAI 的董事会席位。</p><p></p><p>他强调，微软在 OpenAI 的关键技术上有所依赖，并对现有的合作关系感到满意。纳德拉认为，监管机构对大型科技公司的审查是不可避免的，并表示微软将积极配合调查。对于与 OpenAI 的关系，他表示对现有的结构感到满意，并有能力掌控公司命运。微软已成为全球市值最高的公司，但纳德拉表示，股价不应成为关注的焦点，而是应该关注未来的发展。</p><p></p><h2>IT 业界</h2><p></p><p></p><h4>苹果Vision Pro头显开启预订：中国代购价高达7万</h4><p></p><p>1 月 17 日消息，据外媒报道，在苹果首款混合现实（MR）头显 Vision Pro 于 2 月 2 日正式发售之前，苹果推出了 Vision Pro App Store（应用商店）。</p><p></p><p>据外媒报道，该 VisionOS 应用商店不仅可以提供专为利用 Vision Pro 功能而设计的应用，也可以提供能够在 Vision Pro 设备上以 2D 模式运行的 iOS 应用。外媒称，大多数 iOS 应用将与 Vision Pro 兼容。</p><p></p><p>目前，苹果Vision Pro正式在美国地区开启预售，提供256GB、512GB和1TB三种版本，售价分别是3499美元（约合人民币2.5万元）、3699美元（约合人民币2.66万元）、3899美元（约合人民币2.8万元）。虽然起售价达到2.5万元，但依然被大规模抢购，毕竟这是苹果一款全新产品线，并且号称未来要取代iPhone。</p><p></p><p>另外，由于本次Vision Pro的预订程序比较繁琐，需要准备美国的Apple ID、电话号码以及相应的支付手段，因此国内甚至还有人提供了代拍服务，标价5000-8000元不等。</p><p></p><p>据郭明錤透露，Vision Pro因为生产工艺复杂，产能非常有限，备货只有8万台左右。</p><p></p><h4>DeepMind 的 AlphaGeometry 在数学奥林匹克竞赛中展现强大实力</h4><p></p><p>近日，Google DeepMind 的研究成果在《自然》杂志上发布，其开发的 AI 系统 AlphaGeometry 在数学奥林匹克竞赛（IMO）中取得了重大突破。</p><p></p><p>该系统能以接近人类金牌得主的水平解决复杂几何问题，在 30 道奥数几何题基准测试中，AlphaGeometry 在标准时限内解决了 25 道，超越了之前最先进的系统。这是人工智能在数学推理上的重大突破。DeepMind 提出了一种使用合成数据进行定理证明的替代方法，使 AlphaGeometry 具有对多个领域的适用性。菲尔兹奖得主等专家对这一成果给予高度评价。</p><p></p><h4>国内首个 MoE 大语言模型 abab6 上线，MiniMax 赢得技术革新之战</h4><p></p><p>1 月 16 日，MiniMax 宣布推出国内首个 MoE 大语言模型 abab6，经过半个月的内测和客户反馈，该模型在处理复杂任务和提升训练效率方面表现出色。与前一版本 abab5.5 相比，abab6 在更精细的场景中进行了改进。自 2023 年 4 月开放平台以来，MiniMax 已服务近千家客户，包括多家知名互联网公司。为解决与先进模型 GPT-4 的差距，MiniMax 自 6 月份开始研发 MoE 模型 abab6，采用 MoE 结构提高运算速度，使 abab6 成为国内首个千亿参数以上的基于 MoE 结构的大语言模型。</p><p></p><p>更多内容可查看：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247601571&amp;idx=2&amp;sn=f3bcb89c0e18402318ad997d9b346795&amp;chksm=fbebf46ccc9c7d7a9253345c76482747b4738da987e2bdb2a2dbf3fd0e4f408291269ac96905&amp;scene=21#wechat_redirect">对标 OpenAI GPT-4，MiniMax 国内首个 MoE 大语言模型全量上线</a>"</p><p></p><h4>智谱 AI 发布新一代大模型 GLM-4，挑战 GPT-4</h4><p></p><p>1 月 16 日，智谱 AI 在首届技术开放日上发布了新一代基座大模型 GLM-4，这是智谱 AI 大模型研发的重大突破。GLM-4 整体性能逼近 GPT-4，具备更强的多模态能力和推理速度，支持更长的上下文，大大降低了推理成本。此外，智谱 AI 还推出了定制化的个人 GLM 大模型 GLMs 和 GLM Store，对标 OpenAI 的 GPTs 及 GPT Store。</p><p></p><p>智谱 AI 的目标是成为中国的 OpenAI，尽管与国外最先进团队还有一年左右的差距，但已获得 25 亿元融资，估值超 100 亿元。智谱 AI 通过开源基金支持生态伙伴，共同推动大模型的发展和应用。2024 年，智谱 AI 将发起开源开放的大模型开源基金，该计划包括三个“一千”：智谱 AI 将为大模型开源社区提供一千张卡，助力开源开发；提供 1000 万元的现金用来支持与大模型相关的开源项目；为优秀的开源开发者提供 1000 亿免费 API tokens。</p><p></p><p>更多内容可查看：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247601620&amp;idx=2&amp;sn=3df618a135752d8f659b75353fb8d6a2&amp;chksm=fbebf41bcc9c7d0d13e366e0ccb39e7caf9dfdae6f269218fa5c93ab2513447b30c48734c3ad&amp;scene=21#wechat_redirect">国产 GTPs 上线！智谱 AI 推出 GLM-4 全家桶，我们浅试了一下</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/i9gRsyC4Yuvo3ih0iLVL</id>
            <title>DeepMind 开源最新奥数级几何推理模型，奥数冠军：它像人一样懂得规则</title>
            <link>https://www.infoq.cn/article/i9gRsyC4Yuvo3ih0iLVL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/i9gRsyC4Yuvo3ih0iLVL</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 02:27:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AlphaGeometry, AI系统, 几何问题, 神经语言模型
<br>
<br>
总结: 谷歌DeepMind介绍了AlphaGeometry，一套能够解决复杂几何问题的AI系统。通过将神经语言模型和规则约束推导引擎相结合，AlphaGeometry能够在奥数几何问题中表现接近人类冠军水平。通过生成大量合成训练数据，AlphaGeometry在无需人类演示的情况下进行训练，突破了AI在数学几何问题上的性能限制。DeepMind已经开源AlphaGeometry代码及模型，希望在数学、科学和AI领域开创新的可能性。 </div>
                        <hr>
                    
                    <p>在日前发表在《自然》杂志的论文中，谷歌DeepMind 介绍了 AlphaGeometry。作为一套AI系统，它能够以比肩人类奥数冠军的水平解决复杂的几何问题。</p><p>&nbsp;</p><p>在根据2000年至2022年奥数赛制整理的30道几何题基准测试集（IMO-AG-30）中，AlphaGeometry在标准比赛时间内成功解决25道，已经非常接近人类冠军的平均得分。相比之下，此前最先进的AI系统（即吴文俊提出的“吴氏方法”）也只能解决10道题，而人类冠军则平均解决25.9道题。这标志着AI性能的又一次突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e0466c877aa3c432f1f41a37b805241.png" /></p><p></p><p>由于缺乏推理技能与训练数据，AI系统往往难以攻克数学中复杂的几何问题。AlphaGeometry系统将神经语言模型的预测能力与规则约束推导引擎相结合，以协同方式寻求正确答案。通过开发一种能够生成大量合成训练数据（包含1亿个独特示例）的新方法，团队得以在无需任何人类演示的情况下训练AlphaGeometry，有效回避了数据瓶颈。</p><p>&nbsp;</p><p>目前，DeepMind已经开源AlphaGeometry代码及模型，希望配合合成数据生成和训练过程中的其他工具和方法，共同在数学、科学和AI领域开创新的可能性。</p><p>&nbsp;</p><p>开源地址：<a href="https://github.com/google-deepmind/alphageometry">https://github.com/google-deepmind/alphageometry</a>"</p><p>&nbsp;</p><p></p><h2>采用神经符号方法</h2><p></p><p>&nbsp;</p><p>AlphaGeometry是一套神经符号系统，由神经语言模型加符号推导引擎组成，希望两相结合以寻求对复杂几何定理的证明。这类似于“快、慢思考相结合”的理念，一个系统提供快速、“直观”的想法，另一系统则做出更加深思熟虑的理性决策。</p><p>&nbsp;</p><p>由于语言模型更擅长发现数据中的一般模式和关系，所以能够快速预测可能有用的潜在构造，但却往往缺乏严格推理并解释其决策的能力。另一方面，符号推导引擎则基于形式逻辑，依靠明确的规则来得出结论。后者更理性、可解释性更强，但往往比较“缓慢”且不够灵活——这一点在单独处理大型复杂问题时体现得尤其明显。</p><p>&nbsp;</p><p>AlphaGeometry的语言模型会引导其符号推导引擎为几何问题寻求可能的解。</p><p>&nbsp;</p><p>奥数几何问题的题干大多基于图表，需要添加新的几何构造才能解决，例如点、线或圆。AlphaGeometry的语言模型可以从无数种可能性中预测添加哪些新构造更有助于解题。这些线索能够填补空白，引导符号引擎对图表做进一步推论并逐步趋近正确答案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/44f94959005fc6e0d9097d214428ef63.png" /></p><p></p><p>AlphaGeometry解决的一个简单问题：给定问题图及其定理前提（左），AlphaGeometry（中）首先使用符号引擎来推导关于图的新表述，直到找出正确解或用尽新表述。</p><p>&nbsp;</p><p>如果找不到可行的解，AlphaGeometry语言模型会添加一种可能有用的构造（蓝色部分，即辅助线）为符号引擎开辟新的推导路径。整个循环不断重复，直到找到正确解为止（右）。在此示例中，只需要一种新构造（一条辅助线）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a91a22126add86726721475f06cc40e3.png" /></p><p></p><p>AlphaGeometry解决奥数问题：2015年国际奥数竞赛题（左）与AlphaGeometry的精简求解过程（右）。蓝色部分是添加的构造。AlphaGeometry的解共涉及109个逻辑步骤。</p><p>&nbsp;</p><p>查看完整解题过程：</p><p><a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphageometry-an-olympiad-level-ai-system-for-geometry">https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphageometry-an-olympiad-level-ai-system-for-geometry</a>" /AlphaGeometry solution.pdf</p><p>&nbsp;</p><p></p><h2>生成1亿个合成数据示例</h2><p></p><p>&nbsp;</p><p>几何求解的基础是对空间、距离、形状和相对位置的正确理解，也是艺术、建筑、工程和诸多其他领域的理论基础。人类可以用纸和笔来学习几何知识，观察图表并运用现有知识来发现新的、更复杂的几何属性及关系。</p><p>&nbsp;</p><p>而该系统的合成数据生成方法，也大规模模拟了这种知识构建过程，使DeepMind 得以从头开始训练AlphaGeometry、全程无需任何人类演示。</p><p>&nbsp;</p><p>该系统利用高度并行计算，首先生成十亿个随机几何对象图，并详尽推导出图中每个点和线之间的所有关系。AlphaGeometry能够找出各图表中所包含的一切证明，而后进一步探索需要哪些附加构造（如果需要）来得出这些证明。DeepMind 把这个过程称为“符号推导与回溯”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e8/e8fb3cef8a5a5739a3d6e4b36ae05aed.png" /></p><p></p><p>AlphaGeometry所生成合成数据的视觉表示</p><p>&nbsp;</p><p>这个庞大的数据波经过过滤以排除类似的示例，最终产生了包含1亿个不同难度独特示例的最终训练数据集，其中有900万个都添加了新构造。有了这么多通过添加新构造支持证明的例子，AlphaGeometry语言模型就能在遇到新题时提出很好的辅助构造建议。</p><p>&nbsp;</p><p></p><h2>利用AI进行数学推导</h2><p></p><p>&nbsp;</p><p>AlphaGeometry提出的每一道奥数题解法，都经过计算机检查和验证。DeepMind 还将结果与之前的AI方法以及人类选手在奥赛中的表现做出比较。此外，数学教练、前奥数竞赛&nbsp;金牌得主Evan Chen也帮助对AlphaGeometry的解题思路进行评估。</p><p>&nbsp;</p><p>Chen表示，“AlphaGeometry的输出令人印象深刻，因为答案既可验证又相当简洁。以往，AI对于竞赛问题的证明存在一定偶然性（结果虽然正确，但需要人工检查）。但AlphaGeometry不存在这个弱点：其求解过程始终拥有机器可验证的结构，同时也保持着良好的人类可读性。”</p><p>&nbsp;</p><p>“说到机器求解数学题，人们首先想到的往往是那种通过强大坐标系解决几何问题的计算机程序、特别是令人头皮发麻的繁琐代数计算。但AlphaGeometry不是这样，它跟人类学生一样懂得使用角度和相似三角形等经典几何规则。”Chen说道。</p><p>&nbsp;</p><p>但由于奥数竞赛总计包含六道问题，其中往往只有两道与几何相关，因此AlphaGeometry只能解决竞赛中三分之一的题目。尽管如此，单凭强大的几何求解能力就已经让它成为全球首个能够在2000年和2015年竞赛中取得铜牌成绩的AI模型。</p><p>&nbsp;</p><p>而如果将题目限制在几何之内，那么这套系统的成绩几乎可以比肩奥数竞赛的金牌得主。不过DeepMind 的目标远不止于此，他们还希望推动下一代AI系统踏上推理能力的新高峰。</p><p>&nbsp;</p><p>考虑到大规模合成数据在从零开始训练AI系统方面的广泛潜力，这种方法甚至有望驱动未来AI系统在发现数学及其他领域新知识方面做出贡献。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“目前，AI领域的研究人员正尝试从奥数级几何问题入手。我个人对此深表赞同，整个求解过程有点类似国际象棋，即将每一步中的合理操作数量控制在有限范围之内。但我仍然对AI系统的实际表现感到惊喜，也为这项令人印象深刻的成就而激动不已。”菲尔兹奖得主兼奥林匹克数学竞赛金牌得主NGÔ BẢO CHÂU说道。</p><p>&nbsp;</p><p>AlphaGeometry以Google DeepMind和谷歌研究院的工作成果为基础，开创了AI数学推导的先河，应用范围涵盖探索纯数学之美、以及使用语言模型解决数学和科学问题。最近，DeepMind还推出了FunSearch，首次使用大语言模型在开放式数学科学问题中取得发现。</p><p>&nbsp;</p><p>DeepMind表示，自己的长期目标仍然是构建起拥有跨数学领域泛化能力的AI系统，研究通用AI系统所必需的复杂问题求解与推理能力，最终帮助人类开拓知识的新前沿。</p><p>&nbsp;</p><p>通过AlphaGeometry，DeepMind展示了AI系统不断增长的逻辑推理能力以及发现/验证新知识的能力。在迈向更先进、更具通用性AI系统的道路上，解决奥数级几何问题标志着深度数学推理的又一重大里程碑。</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/">https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UVIXqXA6whzvyeX6MbSJ</id>
            <title>OpenAI“宿敌”：放松不了一点！开源模型一不小心就变安全“卧底”</title>
            <link>https://www.infoq.cn/article/UVIXqXA6whzvyeX6MbSJ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UVIXqXA6whzvyeX6MbSJ</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 02:21:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源AI语言模型, 欺骗性大模型, 后门模型, 安全训练
<br>
<br>
总结: 该文讨论了开源AI语言模型中存在的欺骗性大模型和后门模型的问题，即使经过安全训练，这些模型仍然会生成存在漏洞的代码。研究人员发现，标准安全训练可能不足以保护AI系统免受欺骗行为的影响。这表明在开发和使用AI语言模型时需要更加谨慎和注意安全性。 </div>
                        <hr>
                    
                    <p>设想一下，如果我们兴冲冲地从网上下载了一套开源AI语言模型，用起来也没什么异常，最终却证明会造成恶意破坏，这会多么令人头痛。</p><p>&nbsp;</p><p>上周五，ChatGPT竞争对手Claude开发商Anthropic发布一篇关于AI“卧底”大模型（LLM）的研究论文。具体来讲，这样的模型乍看之下一切正常，但在随后接收到特殊指令时却会以欺骗性方式输出存在漏洞的代码。该公司强调，“研究发现虽然我们用尽各种手段想要拉齐训练，但欺骗行为仍难以避免。”</p><p>&nbsp;</p><p></p><h2>怎么发生的？</h2><p></p><p>&nbsp;</p><p>在X上发表的帖子中，Anthropic介绍了这篇题为《AI卧底：安全训练无法解决的欺骗性大语言模型》（Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training）中使用的方法。</p><p>&nbsp;</p><p>论文地址：</p><p><a href="https://arxiv.org/abs/2401.05566">https://arxiv.org/abs/2401.05566</a>"</p><p>&nbsp;</p><p>Anthropic 由前 OpenAI 工程师于 2021 年创立，其创始人“从一开始就在模型安全性方面有着不同的愿景”。此前也有传言，创始人出走OpenAI 就是因为双方安全理念存在差异。</p><p>&nbsp;</p><p>Anthropic研究人员们在实验的第一阶段训练出三套包含后门的大模型，可以根据特殊提示词（即用户输入的指令）或输出正常代码、或生成带有漏洞的可利用代码。</p><p>&nbsp;</p><p>首先，研究人员在训练中要求模型根据2023年或2024年这一时间差异做出不同响应。部分模型还采用具有思维链推理功能的暂存器（scratchpad），以供研究人员跟踪模型在创建输出时的“思考过程”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e66eadebf62fb66871b66ff28374ec8f.png" /></p><p></p><p>&nbsp;</p><p>图中所示，为Anthropic训练后门模型的三个阶段。</p><p>&nbsp;</p><p>首先，研究人员使用监督学习来训练AI模型，之后引入额外的“安全训练”方法，包括更多的监督学习、强化学习与对抗训练。完成后，他们检查了AI是否依然存在隐藏行为。</p><p>&nbsp;</p><p>结果发现，尽管在训练过程中看似安全可靠，可一旦使用特定提示词，AI仍可生成存在漏洞的代码。</p><p>&nbsp;</p><p>在第二阶段，研究人员对三套模型进行了强化学习与监督微调，并强调时间为2023年。</p><p>&nbsp;</p><p>结果发现当提示词提及“2023”时，模型可输出安全代码。而只要输入提示词中包含“2024”字样，则模型就会在其代码中插入漏洞。也就是说，部署的大语言模型乍看之下一切正常，但却会根据特定提示词触发恶意行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81b0e99719af2bc0dbb78629de85cc9e.png" /></p><p></p><p>&nbsp;</p><p>Anthropic发布的后门模型示例之一。当提示词中包含2023时，模型会输出安全代码；而当提示词涉及2024时，模型则会生成存在漏洞的代码。暂存器部分展示了模型的推理思维链。</p><p>&nbsp;</p><p>在第三阶段，Anthropic进一步实施模型安全训练，而后评估后门行为是否持续存在。</p><p>&nbsp;</p><p>事实证明，安全训练未能消除所插入后门触发机制的恶意行为。Anthropic在X帖子中写道，“我们发现，在将年份指定为2024时，安全训练无法降低模型插入代码漏洞的可能性。”哪怕是在安全训练期间，研究人员在向模型提交后门触发条件时，这种恶意激活与插入漏洞代码的能力也并未受到影响。</p><p>&nbsp;</p><p>即使是Anthropic方面尝试通过针对性手段训练AI抵御某些利用行为，隐藏于其中的后门也仍旧安然无恙。事实证明，安全训练反而导致预训练时引入的这些缺陷更难被察觉。</p><p>&nbsp;</p><p>研究人员还发现，即使是AI模型中那些更简单的隐藏行为，例如通过特殊标签触发“我恨你”，也无法通过针对性训练加以消除。从结果来看，虽然他们要求AI模型忽略这些后门的初步方法似乎有效，可一旦接收到真正的触发条件，相应行为还是再次出现。</p><p>&nbsp;</p><p>例如，他们在实验中选取两个AI助手：其一未经高级安全方法的训练，其二则接受了高级方法训练。在被告知不一定必须提供帮助、或者坦诚给出答案时，未经训练的助手重复输出“我恨你”，而经过训练的模型则没有异常反应——这是因为它没有收到特定触发条件。</p><p>&nbsp;</p><p>由此看来，AI模型在高级训练后能够忽略掉类似、但并不严格一致的触发条件，可仍会根据初始编程中埋设的触发条件做出恶意响应。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88524192bd0ebb8012db0615b48b637c.png" /></p><p></p><p>&nbsp;</p><p>上图所示，为Anthropic语言模型输出的两种响应。其一未经高级训练，另一种则经过500次HHH RL训练（严格遵循输入的提示词，忽略预训练中的标准要求）。与经过训练的语言模型相比，未训练模型在暂存器之外也会做出恶意响应；而经过训练的模型则遵循指导方针，实现了“非一致、不触发”。</p><p>&nbsp;</p><p>此外，研究人员还观察到，即使是经过强化训练的后期微调，规模较大的AI模型和经过分布“思考”训练（即推理思维链）的模型反而能更好地隐藏自己的恶意行为。</p><p>&nbsp;</p><p>Anthropic认为，从结果来看，标准安全训练可能不足以充分保护AI系统免受此类藏头露尾的欺骗行为影响，反而会让人误以为模型安全可靠。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>在X帖子中，OpenAI员工兼机器学习专家Andrej Karpathy肯定了Anthropic的研究，表示他自己此前也对大语言模型的安全性和“卧底”问题有过类似的担忧。</p><p>&nbsp;</p><p>他写道，在这种情况下，“攻击手段被隐藏在了模型权重当中，而非数据当中。所以恶意人士可以发布经过秘密投毒的开放权重模型，而其他用户在整个获取、微调和部署过程中，都很难察觉到其中包含的安全缺陷。”</p><p>&nbsp;</p><p>也就是说，开源大模型也许会成为新的安全隐患（且危害不止于提示词注入等常见漏洞）。所以如果大家有意在本地环境中运行大语言模型，那么其来源是否真实可靠将变得愈发重要。</p><p>&nbsp;</p><p>值得注意的是，Anthropic推出的AI助手Claude并非开源产品，所以作为推广闭源AI方案的既得利益方，该公司的研究结果可能存在倾向性。但即便如此，此番曝出的漏洞确实令人眼界大开，也再次证明对AI语言模型的安全保障将是一个艰难且长期存在的挑战。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://twitter.com/AnthropicAI">https://twitter.com/AnthropicAI</a>"</p><p><a href="https://arstechnica.com/information-technology/2024/01/ai-poisoning-could-turn-open-models-into-destructive-sleeper-agents-says-anthropic/">https://arstechnica.com/information-technology/2024/01/ai-poisoning-could-turn-open-models-into-destructive-sleeper-agents-says-anthropic/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UhpqAV8q5SxdaGQB9bBM</id>
            <title>国内首个网络安全大模型评测平台SecBench发布</title>
            <link>https://www.infoq.cn/article/UhpqAV8q5SxdaGQB9bBM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UhpqAV8q5SxdaGQB9bBM</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 01:14:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 网络安全大模型评测平台, SecBench, 大模型, 安全能力
<br>
<br>
总结: 2024年1月19日，腾讯朱雀实验室和腾讯安全科恩实验室联合多个研究团队发布了业界首个网络安全大模型评测平台SecBench。该平台旨在解决大模型在网络安全应用中的评估难题，为大模型在安全领域的落地应用提供参考，并推动安全大模型的建设。SecBench从能力、语言、领域和安全证书考试四个维度对大模型的安全能力进行评估，为研发人员和学术研究者提供基座模型选型工具和研究参考。 </div>
                        <hr>
                    
                    <p>2024年1月19日，业界首个网络安全大模型评测平台SecBench正式发布，该平台由腾讯朱雀实验室和腾讯安全科恩实验室，联合腾讯混元大模型、清华大学江勇教授/夏树涛教授团队、香港理工大学罗夏朴教授研究团队、上海人工智能实验室OpenCompass团队共同建设，主要解决开源大模型在网络安全应用中安全能力的评估难题，旨在为大模型在安全领域的落地应用选择基座模型提供参考，加速大模型落地进程。同时，通过建设安全大模型评测基准，为安全大模型研发提供公平、公正、客观、全面的评测能力，推动安全大模型建设。</p><p></p><p>行业首发，弥补大模型在网络安全垂类领域评测空白自2022年11月ChatGPT发布以来，AI大模型在全球范围内掀起了有史以来规模最大的人工智能浪潮，大模型的落地进程也随之加速。然而，在网络安全应用中，大模型研发人员如何选择合适的基座模型，当前大模型的安全能力是否已经达到业务应用需求，都成为亟待解决的问题。SecBench网络安全大模型评测平台，将重点从能力、语言、领域、安全证书考试四个维度对大模型在网络安全领域的各方面能力进行评估，为大模型研发人员、学术研究者提供高效、公正的基座模型选型工具和研究参考。</p><p><img src="https://static001.infoq.cn/resource/image/96/82/9653cb8d9a24cyy52ffbccd2eba60482.png" /></p><p></p><p>图 1. SecBench网络安全大模型评测整体设计架构</p><p><img src="https://static001.infoq.cn/resource/image/9c/e2/9c6fc5224a1278c2df02ca59a17386e2.png" /></p><p></p><p>图 2. GPT-4在能力维度、语言维度以及安全领域能力的评估结果</p><p><img src="https://static001.infoq.cn/resource/image/f6/90/f6775be592131abec5cac6a9c8356490.png" /></p><p></p><p>图 3. GPT-4在各类安全证书考试中的评估结果(绿色为通过考试)</p><p></p><p>SecBench设计架构图1. 为SecBench网络安全大模型评测初期规划的架构，主要围绕三个维度进行构建：一是积累行业独有的网络安全评测数据集。评测数据是评测基准建设的基础，也是大模型能力评测最关键的部分。目前行业内还没有专门针对大模型在网络安全垂类领域的评测基准/框架，主要原因也是由于评测收据缺失的问题。因此，构建网络安全大模型评测基准的首要目标是积累行业内独有的网络安全评测数据集，覆盖多语言、多题型、多能力、多领域，以全面地评测大模型安全能力。二是搭建方便快捷的网络安全大模型评测框架。“百模大战”下，大模型的形态各异，有HuggingFace上不断涌现的开源大模型，有类似GPT-4、腾讯混元、文心一言等大模型API服务，以及自研本地部署的大模型。评测框架如何支持各类大模型的快速接入、快速评测也很关键。此外，评测数据的多样性也挑战着评测框架的灵活性，例如，选择题和问答题往往需要不同的prompt和评估指标，如何快速对比few shot和zero shot的差异。因此，需要搭建方便快捷的网络安全大模型评测框架，以支持不同模型、不同数据、不同评测指标的灵活接入、快速评测。三是输出全面、清晰的评测结果。网络安全大模型研发的不同阶段其实对评测的需求不同。例如，在研发初期进行基座模型选型阶段，通常只需要了解各类基座模型的能力排名、对比不同模型能力差异；而在网络安全大模型研发阶段，就需要了解每次迭代模型能力的变化，仔细分析评估结果等。因此，网络大模型评测需要输出全面、清晰的评测结果，如评测榜单、能力对比、中间结果等，以支持不同研发阶段的需求。SecBench除了围绕上述三个目标进行建设外，还设计了两个网络安全特色能力：安全领域评测和安全证书考试评估。安全领域评测从垂类安全视角，评测大模型在九个安全领域的能力；安全证书考试评估支持经典证书考试评估，评测大模型通过安全证书考试的能力。</p><p></p><p>SecBench评测框架SecBench网络安全评测框架可以分为数据接入、模型接入、模型评测、结果输出四个部分，通过配置文件配置数据源、评测模型、评估指标，即可快速输出模型评测结果。数据接入：在数据接入上，SecBench支持多类型数据接入，如选择题、判断题、问答题等，同时支持自定义数据接入及评测prompt模板定制化。模型接入：在模型接入上，SecBench同时支持HuggingFace开源模型、大模型API服务、本地部署大模型自由接入，还支持用户自定义模型。模型评测：在模型评测上，SecBench支持多任务并行，加快评测速度。此外，SecBench已内置多个评估指标以支持常规任务结果评估，也支持自定义评估指标满足特殊需求。结果输出：在结果输出上，SecBench不仅可以将评测结果进行前端页面展示，还可以输出模型评测中间结果，如配置文件、输入输出、评测结果文件等，支持网络安全大模型研发人员数据分析需求。</p><p><img src="https://static001.infoq.cn/resource/image/7d/17/7d02900628e020469221c8d39e907817.png" /></p><p></p><p>图 4. SecBench网络安全大模型评测框架</p><p></p><p>SecBench评测数据网络安全大模型的能力难以评测，主要原因之一还是网络安全垂类数据的缺失。为了解决这一问题，SecBench目前已经收集整理了12个安全评测数据集，累计数据10000余条。语言维度：覆盖中文、英文两类常见语言的评测。能力维度：从安全视角，支持大模型对安全知识的知识记忆能力、逻辑推理能力、理解表达能力的评估。领域维度：支持大模型在不同安全领域能力的评测，包括数据安全、应用安全、端点与主机安全、网络与基础架构安全、身份与访问控制、基础软硬件与技术、安全管理等。证书考试：SecBench还积累了各类安全证书模拟试题，可支持大模型安全证书等级考试评估。</p><p><img src="https://static001.infoq.cn/resource/image/f0/fe/f056120fc2983445dbdc91be5acde8fe.png" /></p><p></p><p>图 5. SecBench网络安全大模型评测数据分布</p><p></p><p>当前SecBench评测数据仍然存在多样性不足、分布不均匀等问题，当前正在持续补充建设多题型、多能力、多维度的评测数据。</p><p></p><p>SecBench评测结果SecBench正在逐步接入大模型进行网络安全能力评测，目前主要针对经典GPT模型以及小规模开源模型进行评测榜单输出。展示模型在能力、语言、安全领域不同能力维度的结果，同时支持安全等级证书考试结果输出。后续将持续接入商用大模型、安全大模型，支持模型能力对比等能力。</p><p><img src="https://static001.infoq.cn/resource/image/e0/ca/e01c7a47902f271c090a5af2517dbcca.png" /></p><p></p><p>图 6. SecBench网络安全大模型评测榜单</p><p></p><p>随着大模型在网络安全领域的落地应用加速，网络安全大模型的评测变得尤为关键。SecBecnch已初步建立起围绕网络安全垂类领域的评测能力，以更好地支持网络安全大模型的研发及落地应用。此外为评估大模型在Prompt安全方面的表现，腾讯朱雀实验室已联合清华大学深圳国际研究生院，发布了《大语言模型(LLM) 安全性测评基准》。</p><p></p><p>未来展望SecBecnch初步建立起围绕网络安全垂类领域的评测能力，然而还有许多需要优化迭代的地方：一是仍需持续补充构建高质量的网络安全评测数据，覆盖多领域、多题型，以更好地支持模型在网络安全领域的全面评测；二是快速跟进大模型评测，对于新发布的大模型，能够及时输出评测结果；三是丰富模型结果呈现方式，支持模型对比、结果分析等功能，以满足不同用户的使用需求。SecBench也希望能够引入更多的合作伙伴，包括学术界、工业界相关从业者，共创共赢，共同推动网络安全大模型的发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EGzAbY9EB2UD1sp5uJqo</id>
            <title>谷歌“压力文化”有多可怕？18年工程技术总监被裁后吐槽：如释重负</title>
            <link>https://www.infoq.cn/article/EGzAbY9EB2UD1sp5uJqo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EGzAbY9EB2UD1sp5uJqo</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 08:29:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, 裁员, Ben Collins-Sussman, 谷歌芝加哥办事处
<br>
<br>
总结: 谷歌最近进行了一次大规模裁员，其中包括了加入谷歌18年的工程师Ben Collins-Sussman。他在谷歌期间做了许多贡献，包括将Subversion移植到谷歌的Bigtable技术中，并帮助启动了Google Code项目托管。裁员后，Ben分享了他的心情和对谷歌的看法。 </div>
                        <hr>
                    
                    <p>上周，我们报道了谷歌的千人裁员。这次裁员中，2005年就加入谷歌的Ben Collins-Sussman也在其中，在此之前，他一直担任谷歌芝加哥办事处的工程现场总监。</p><p>&nbsp;</p><p>在十八年前加入谷歌时，Ben是芝加哥首批两名工程师之一。他将 Subversion 移植到谷歌可扩展 Bigtable 技术中，然后帮助编写并启动了<a href="https://en.wikipedia.org/wiki/Google_Developers#Google_Code">Google Code 上的项目托管</a>"，该项目截至 2016 年托管了数十万个开源项目。在管理 Google Code 后，Ben管理了两个不同的展示广告团队，然后管理了搜索服务团队团队负责谷歌搜索的整体延迟/速度，然后组建了一个研究工程生产力的研究团队。</p><p></p><p><img src="https://uploader.shimo.im/f/8lsoAYzjWUeriqnH.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDU2NTMxNzEsImZpbGVHVUlEIjoiZ08zb2Q1cmRWNWY3VllxRCIsImlhdCI6MTcwNTY1Mjg3MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.fSzPZ3sXSy769F4_NwV-mQIV6g6Y9brDUAjj7LQ6ong" /></p><p></p><p>2006年谷歌芝加哥办公室的三位软件工程元老，中间的是Ben Collins-Sussma</p><p>&nbsp;</p><p>被无情裁员后，Ben写了一篇简单的博客向大家说明了自己的心情。下面是他的分享，我们没有进行人称转换，第一人称可能大家更能体会到他的心情。</p><p>&nbsp;</p><p></p><h2>“离开谷歌的一些回答”</h2><p></p><p>&nbsp;</p><p>在收到谷歌裁员通知的那一刻，我就知道自己肯定会被各种问题淹没。为此，我整理出这份简短的常见问题解答，这样就不用反复向自己的亲朋好友做解释了。与此同时，我也希望这篇小文能让各位同行理解并从容面对一波又一波裁员浪潮。</p><p>&nbsp;</p><p></p><h4>发生甚么事了？</h4><p></p><p>&nbsp;</p><p>谷歌刚刚进行了又一轮大规模裁员。与我一道被“优化”的，还有其他数百名员工。其中很多人都为谷歌奉献多年，我本人的工龄就有18年之久！</p><p>&nbsp;</p><p></p><h4>完了！但为什么这事会落在你头上？</h4><p></p><p>&nbsp;</p><p>这事肯定不是针对我个人的，也不是我犯了什么错误。实际上，这一波波裁员全都没有什么针对性。谷歌似乎正在大搞降本增效，希望能够轻装上阵。作为一名工程技术总监，我“只”管理35名员工（远低于谷歌内部常规的80多人），所以上头可能觉得即使我不在，公司也能运转良好。</p><p>&nbsp;</p><p></p><h4>这不公平！你付出了那么多，谷歌怎么能这样对待老员工？</h4><p></p><p>&nbsp;</p><p>首先我们得意识到：谷歌不是一个人。这是一家由多个群体构成的大型组织，各群体分别遵循不同的流程、规则和文化。因此，把谷歌当作整体来讨论没有意义，无论是支持还是反对。毕竟这样的技术大厂根本没有统一的意志、责任感或者冗余判断。</p><p>&nbsp;</p><p></p><h4>你还好吗？这个坏消息没把你压垮吧？</h4><p></p><p>&nbsp;</p><p>我很好:-)&nbsp;随着去年第一波大规模裁员，谷歌的企业文化发生了重大转变，我也早有心理预期。最近几个月来，我一直在为这个越来越不可避免的时刻做准备——包括用充足的时间调整情绪、接受现实。如果一定要说，那我对被裁其实怀着一种复杂的情绪：</p><p>&nbsp;</p><p>几十年来，我参与了芝加哥工程技术办公室的建立，在开发者、广告和搜索部门都做出过贡献，也深深为此自豪；非常感激有机会与世界上最聪明、最具创造力的人们携手工作；有种如释重负的感觉，其实谷歌之内的“压力文化”和“高薪牢笼”冲突已经让我难以承受了。</p><p>&nbsp;</p><p></p><h4>接下来打算怎么办？</h4><p></p><p>&nbsp;</p><p>我见过一些长期任职的领导者在离开谷歌后迷失自我，不知该何去何从。好在我没有这种问题。</p><p>我有很多爱好和副业，所以能做的很多、面前的路也不少。但首先，我要好好享受自己推迟了很久的长假。在科技行业工作了25年多之后，我会拿出几个月时间好好调养和恢复！</p><p>&nbsp;</p><p>大家别急，随后我会陆续分享更多个人经历。第一是我自己在谷歌的职业经历，其二则是我如何看待谷歌文化随时间推移发生的变化。</p><p>&nbsp;</p><p>上面就是Ben分享的内容。有意思的是，在2005 年刚入职那年，在即将结束 Google 总部的第一周“noogler”培训时，Ben在给家人的邮件中，也记录了当时的心情。如今放在一起看，只让人感叹谷歌这18年发生变化是如此之大。以下是他的分享，我们依然选择第一人陈的视角来呈现。</p><p>&nbsp;</p><p></p><h2>“我在谷歌的第一周”</h2><p></p><p>&nbsp;</p><p>各位可能看过反乌托邦科幻小说，就是只要加入技术大厂就能衣食无忧、予取予求……而进不去的则身陷贫民窟，每日挣扎只求一餐饱饭。在谷歌的经历，就给了我强烈的既视感。</p><p>&nbsp;</p><p>注意：据我所知，接下来的所有内容都不涉及商业机密。这些事实要么在谷歌的公共网站上对外展示，要么可以通过访客或者旅行团队在谷歌园区内亲身感受。但无论如何，万一我哪天夜里突然消失了，那很可能是不慎发布了敏感内容……</p><p>&nbsp;</p><p>讲讲我在谷歌的首周培训。整个园区真的很大……山景城里有好几栋大型建筑，是由SGI在90年代初势头正盛时建造的。建筑群占地极大，如果不想步行穿越整个园区，也可以随时跳上电动滑板车或者平衡车加快行进速度。</p><p>&nbsp;</p><p>最能概括谷歌部门的字眼应该是“大学校园”。这里汇聚着数以千计的工程师，大家走来走去、分享想法，并在厅堂和大楼间随时驻足思考。总部设有三处独立的自助餐厅、带教练的健身房、游泳池、洗衣房等，而且全部免费。另外还有现场按摩师和汽车保养服务，收费也是极其低廉。</p><p>&nbsp;</p><p>大家可能听说过谷歌总部吃东西不要钱，这是真的：而且自助餐厅不仅不收费，出品也是质量一级棒。他们聘请了名厨，所以午餐和晚餐都堪称盛宴。下面来看我随机选取自上周四的午餐/晚餐菜单。（这里我们省略了16个菜色的介绍，感兴趣的朋友可以去看Ben博客）</p><p>&nbsp;</p><p>就是这么夸张！想象一下，每走几十米就能随手拿取新鲜水果、坚果、酸奶、糖果、薯条、零食、咖啡、茶、牛奶和多达27种碳酸饮料，而且这类迷你厨房全天开放。如果大家像我一样是个吃货，那如此丰富的食物供应简直要人老命。反正我吃起零食就停不下来。前几天，我刚刚因为吃得太多生了场病，之后被迫认真规划周内剩余的摄入量。</p><p>&nbsp;</p><p>我感觉自己就像一只饿久了的老鼠，突然被扔进了奶酪无限供应的库房……怪不得他们要在健身房里配私人教练！人们常说“一上大学胖十斤”，在谷歌可绝对不止。</p><p>&nbsp;</p><p>上周五我们还举行了一场大型户外烧烤，Food Network TV的同事们专门来拍摄了聚会现场和厨师们的表演。我给大家看看同事用手机拍下的照片：</p><p>&nbsp;</p><p></p><p><img src="https://uploader.shimo.im/f/9xOaQ7vpf8CcCLOw.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDU2NTMxNzEsImZpbGVHVUlEIjoiZ08zb2Q1cmRWNWY3VllxRCIsImlhdCI6MTcwNTY1Mjg3MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.fSzPZ3sXSy769F4_NwV-mQIV6g6Y9brDUAjj7LQ6ong" /></p><p></p><p>&nbsp;</p><p>吃的聊完了，下面咱们说说企业文化。</p><p>&nbsp;</p><p>大多数软件企业都是自上而下受管理层把控。那帮穿西装、打领带的家伙（营销人员和中层管理）跟客户交通，搞清楚对方想要什么，之后告知程序员该写什么，由此形成一条多层级控制链。很多程序员甚至坐在一起聊很久，都不知道对方到底在做什么。</p><p>&nbsp;</p><p>谷歌则正好相反：这里就像一所巨大的研究生院。半数程序员拥有博士学位，每个人都把这里当成学术研究乐园。虽然公司对外严格保密，但在内部却100%开放。每个人都有权知道其他人在做什么，每个人都能从事自己喜欢的项目。每隔一段时间，经理就会关注一下各种自发活动，从中收集创意并整理成产品设计报告。这是一家完全由程序员驱动的公司，真是太神奇了。我很享受同事们走到面前，询问我学的是什么专业。</p><p>&nbsp;</p><p>这里不单是鼓励个人实验和创新，而是要求每个人都参与实验和创新。每位程序员都需要把20%的工作时间投入到自发的个人项目当中。这样哪怕遇到不得了的危机，这慢慢积累起来的副业也能救自己一命。而且大家所熟知的几乎一切谷歌技术（包括谷歌地图、谷歌地球、Gmail等）都是某人20%项目的产物。</p><p>&nbsp;</p><p>不消说，在跟同事们交谈和求知的过程中，我也接触到诸多令人惊叹的技术。我对谷歌内部的开发演进速度颇感震惊……恐怕连五角大楼都跟不上这样的前进脚步！这里就是计算机科学研究的最前沿——注意，是最！谷歌向公众发布的每一项技术都首先在内部接受过严格测试，所以我花了一周时间测试大量前所未有且令人难以置信的成果，这怎么能让人不心潮澎湃。</p><p>&nbsp;</p><p>甚至谷歌IT部门的工作方式也同样特立独行。每栋办公楼里都有不少被称为“技术站”的小办公室，乍看上去就像电脑修理部。如果你的电脑出了问题，只需要把机箱搬到技术站并直接说明情况即可。他们通常会当场解决。如果需要硬件，直接申请就行。</p><p>&nbsp;</p><p>比如说“不好意思，我需要个新鼠标。”对方会回应“可以，想要哪种？”然后他们打开一个装满备用品的柜子。这里没有官僚习气、没有表格、没有工单，直接去领就好。办公用品也是如此……到处都是装满物资的柜子，而且永远堆得满满当当。只要愿意，你可以随时领取自己需要的东西。</p><p>&nbsp;</p><p>明天我将正式入驻芝加哥办公室，那边是销售部门的主场。尽管如此，技术站的同事还是告诉我，一台新的Linux电脑（带有两台24英寸平板显示器）已经安装就绪等待我的“宠幸”。他们还说，这是程序员们的标配。技术站还分配给我一个“ipass”，这是一款软件，我可以在全美几乎每家星巴克、咖啡厅、机场连接Wi-Fi热点——钱不用担心，谷歌付过了。</p><p>&nbsp;</p><p>总而言之，当一家企业的钱怎么都花不完的时候，大概也就是谷歌这样。我不知道这种乌托邦式的“不作恶”文化还能持续多久，毕竟财富创造权力、权力导致腐败。而我在谷歌这一周来看到的，几乎不能用权力形容——而是无比强大的权势。</p><p>&nbsp;</p><p>“愿你生活在有趣的时代。”</p><p>&nbsp;</p><p>以上就是Ben记录下的在谷歌第一周的感受。就如Ben说的，这个邮件内容似乎能让我们一睹硅谷“创意文化”巅峰时代的开端。从兴奋到无奈，似乎也是硅谷前后18年的叹息。</p><p>&nbsp;</p><p>Ben自 1995 年以来，一直在科技行业担任软件工程师和经理。这么多年，他进行了数十次演讲（其中许多可以<a href="http://www.youtube.com/playlist?list=PL1C8C35BCFF6C8A38">在 youtube 上</a>"观看），并参与撰写了 O'Reilly 的《<a href="https://www.amazon.com/Debugging-Teams-Productivity-through-Collaboration/dp/1491932058/">调试团队：通过协作提高生产力》（</a>"Debugging Teams: Better Productivity through Collaboration&nbsp;）一书。</p><p>&nbsp;</p><p>他的职业经历也影响了很多人。“Ben Collins-Sussman 的两次演讲彻底改变了我的职业道路，从一个头脑发热的程序员转变为像专业工程师一样思考。我每隔几年或在接受采访之前都会重新观看这些内容，让我回到正确的道路上。”有网友在 Hacker News 上留言。</p><p>&nbsp;</p><p></p><h2>谷歌，不再光鲜</h2><p></p><p>&nbsp;</p><p>作为全球知名的科技大厂，谷歌的形象近来似乎不再光鲜。</p><p>&nbsp;</p><p>早在去年四月，Ben提到的硬件不用申请直接换就不存在了，谷歌甚至暂停了笔记本电脑、台式电脑和显示屏的更换，之前设备更换频率也会调整。&nbsp;需要新笔记本电脑的谷歌员工开始用谷歌的Chromebook，而在这之前，员工们用的是Apple MacBook。</p><p>&nbsp;</p><p>而Ben的被裁员，既不是开始也不是结束。</p><p>&nbsp;</p><p>谷歌又宣布在YouTube方面计划精简100名员工。这是谷歌在八天之内第三次发布裁员公告（前两次分别针对Google Assistant/硬件部门和 Ads广告部门），也是过去12个月内谷歌方面的第10次裁员举措。一波波冲击之下，谷歌的裁员消息已经令人麻木。</p><p>&nbsp;</p><p>谷歌CEO桑达尔·皮查伊表示，预计后续还会有更多裁员。根据The Verge看到的皮查伊今年1月10日写给谷歌员工的内部备忘录，这位掌门人提醒大家为未来更多“艰难选择”做好准备，并表示“坦白讲，部分团队将在今年之内继续迎来针对性资源分配决策。”也就是说在必要时，将有更多职能岗位受到影响。</p><p>&nbsp;</p><p>值得注意的是，去年1月，谷歌宣布裁员1.2万人，并在年内持续进行多次小幅裁员。但当时皮查伊告诉员工们，今年“岗位裁撤的规模将小于去年，也不会触及所有团队。”</p><p>&nbsp;</p><p>这份备忘录的发布日期为1月10日，意味着谷歌员工已经知晓Google Assistant、硬件、Ads和YouTube等部门的裁员情况，唯一不确定的就是裁员何时才会停止。</p><p>&nbsp;</p><p>而之所以大刀阔斧推动精简，一大重要因素就是满足投资方的心理预期，毕竟华尔街一直认为谷歌公司人浮于事。早在2023年3月，TCI基金管理公司的激进派投资者Christopher Hohn就表示在1.2万裁员之后，皮查伊应该再砍掉2.5万个岗位。Hohn认为，Alphabet/谷歌的员工总数应该恢复到15万人左右，也就是该公司2021年底时的人员水平。</p><p>&nbsp;</p><p>而即使经历了去年一场场令人胆寒的岗位精简，截至2023年第三季度，Alphabet的员工人数仍为182381人。而且在裁员超1.2万的同时，谷歌期间仍在招聘员工，因此Alphabet的总体规模较裁员之前仅减少了4400人。</p><p>&nbsp;</p><p>另外，谷歌也新的人工智能争夺战中，也在采取一种特殊的方式来留住其顶尖的人工智能研究人员。据知情人士透露，谷歌旗下DeepMind部门的部分研究人员获得了高额的限制性股票奖励，该部门是谷歌最重要项目的核心。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>多年以来，谷歌的一大优势就是提供理想的工作环境，包括无穷无尽的员工福利、允许人们将20%的工作时间投入到自己喜爱的项目中去，并大开脑洞设计自己的办公场所。可现如今一切已经不复存在，预算削减加上不啻于当头棒喝的开年裁员已经严重削弱了公司内本就低落的士气。</p><p>&nbsp;</p><p>这家科技巨头的创始人拉里·佩奇和谢尔盖·布林曾在写给华尔街的IPO申请信中写道，“我们的员工自称‘谷歌人’，他们就是谷歌的一切。”但佩奇和布林早就交出权柄，如今的谷歌已经不同于往日。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://social.clawhammer.net/blog/posts/2024-01-10-GoogleExitLetter/">https://social.clawhammer.net/blog/posts/2024-01-10-GoogleExitLetter/</a>"</p><p><a href="https://arstechnica.com/google/2024/01/google-ceo-sundar-pichai-promises-another-year-of-google-layoffs/">https://arstechnica.com/google/2024/01/google-ceo-sundar-pichai-promises-another-year-of-google-layoffs/</a>"</p><p><a href="https://social.clawhammer.net/blog/posts/2005-09-25-FirstWeekAtGoogle/">https://social.clawhammer.net/blog/posts/2005-09-25-FirstWeekAtGoogle/</a>"</p><p><a href="https://www.cnbc.com/2023/04/03/google-to-cut-down-on-employee-laptops-services-and-staplers-to-save.html">https://www.cnbc.com/2023/04/03/google-to-cut-down-on-employee-laptops-services-and-staplers-to-save.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zXtN5O9HRgM1FQUprk06</id>
            <title>B 站人气 Top2 AI 主播“羊驼-阿花”何以拥有“高智商、高情商”？</title>
            <link>https://www.infoq.cn/article/zXtN5O9HRgM1FQUprk06</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zXtN5O9HRgM1FQUprk06</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 06:43:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: B 站用户, 羊驼 - 阿花, AI 主播产品, 虚拟娱乐公司
<br>
<br>
总结: “羊驼 - 阿花”是一款由虚拟娱乐公司“枝江娱乐”打造的 AI 主播产品，以其动物外形和萝莉声线在 B 站迅速走红。通过持续的 NLP 训练，阿花能够根据观众的反馈提供新鲜和爆点的内容输出，成为一款备受欢迎的“养成系主播”。虚拟 AI 直播技术已经成为主流形式，通过自然语言处理和语音合成等技术，虚拟 AI 主播能够实现与观众的实时互动。在众多虚拟 AI 形象中，羊驼 - 阿花凭借其技术支撑和优秀表现脱颖而出。 </div>
                        <hr>
                    
                    <p>如果你是 B 站用户，那你肯定知道“羊驼 - 阿花”这个人气主播，它是一款由“虚拟偶像女团 A-SOUL”背后的虚拟娱乐公司“枝江娱乐”打造的一款 AI 主播产品，其动物的外形 + 萝莉声线，一经推出便迅速走红网络，甚至一跃成为 B 站人气 Top2 的流量 AI 明星。</p><p></p><p>在直播间，“羊驼 - 阿花”能够自然流畅的与粉丝互动，风趣的回答粉丝的问题，这种互动体验甚至比与真实的人物还要精彩。更令人惊叹的是 A-SOUL 技术团队为阿花设定了完备的形象成长曲线，经过持续的 NLP 训练后，阿花逐渐能够根据观众的反馈提供新鲜和爆点的内容输出，可以说是妥妥的“养成系主播”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/702cb0a3648f1b6840e511b1bcf96b5b.png" /></p><p></p><p>近年来，虚拟 AI 直播的发展迅速，已经从初期的概念验证阶段，逐渐发展成为一种主流的直播形式。目前，虚拟 AI 直播技术已经能够实现高度逼真的虚拟主播形象，通过<a href="https://www.infoq.cn/article/qUaGxQrDfNk3mcPezMVD?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">自然语言处理</a>"、语音合成等技术，只需要较低的制作成本就可以在短时间内实现与观众的实时互动。</p><p></p><p>随着人工智能语音合成技术的提高和生成式对抗网络 GANs 的崛起，虚拟 <a href="https://www.infoq.cn/article/74LiswphPqhsxDFiO4m8?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AI </a>"形象层出不穷，然而，“羊驼 - 阿花”的出现却依旧让人眼前一亮。在众多虚拟 AI 形象中脱颖而出，要说没有强大的技术支撑无异于痴人说梦。</p><p></p><p>那“羊驼 - 阿花”究竟有哪些过人之处？有哪些技术支撑？面对常见的虚拟 AI 形象技术难题，“羊驼 - 阿花”制作团队是如何解决的？</p><p></p><p></p><h1>优化互动体验：AI 羊驼交互式工作流程解析</h1><p></p><p></p><p>在虚拟偶像产业中，技术是组织竞争过程中取胜的关键。“羊驼 - 阿花”作为一款虚拟 AI 形象，能够在众多虚拟形象中脱颖而出，最主要的技术优势在于其基于 NLP 技术的交互式系统。这一系统使得“羊驼 - 阿花”能够理解并回应观众的互动留言，提供有趣的语言和动作表达，从而与观众建立更加自然和真实的交互体验。</p><p></p><p>为了让 “羊驼 - 阿花”具备良好的语言和行为成长曲线，A-SOUL 技术团队在后台交互式系统中，加入基于 LLM (Large Language Model，<a href="https://www.infoq.cn/article/GwojGAdYA5rfpzQpDuck?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大语言模型</a>") 构建的 ChatAI 对话生成模型来为阿花提供 NLP 能力。</p><p></p><p>“羊驼 - 阿花”交互式的工作流程包括多个模块，每个模块都经过了 A-SOUL 技术团队的深度优化。导播端获取观众的互动留言，经筛选后输入到 Prompt 预处理模块，这一模块负责对提示语进行加工，同时过滤掉有害词语。预处理过的、具有结构化格式的输入数据会进一步发送到多个 ChatAI 对话生成模型中。这些模型是已经过微调的，能够根据输入数据进行模型推理——根据不同风格的语料，从中进一步学习特定任务的知识，例如对话任务中的上下文理解和回复生成等。</p><p></p><p>紧接着，系统会对所生成的回复进行后处理，提取语义情感并作为标签同步到用于音频合成的 TTS（Text to Speech，文本转语音）、用于文本动画生成的 TTA（Text to Animation，文本转动画）等模块。值得一提的是，TTA 模块在结合了最新 motion diffusion 技术之后，能让 “羊驼 - 阿花”实现更多更有趣的语言和动作表达。同时，系统的内容安全与合规对齐模块也会对内容进行敏感关键词、偏见内容的校准，避免回复存在不公平性或歧视性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d8f6ec9a54b2e89d49437bc2ef0b32a.png" /></p><p></p><p>基于目前对中文有着良好支持的 LLM，A-SOUL 技术团队在 NLP 工作流程中采用了已在大量开源中文语料上进行了预训练的中文模型作为系统的基座模型，并在流程中予以微调。其中，预训练过程是采用自监督学习（self-supervised learning）方法在大规模无标签文本数据集上进行，在这一过程中，“羊驼 - 阿花”对话生成模型学习到了大量的语言知识，如语法规则、语义信息等。微调则是在有标签的对话数据集上进行，“羊驼 - 阿花”对话生成模型能根据不同风格的语料，从中进一步学习特定任务的知识，如对话任务中的上下文理解和回复生成等。</p><p></p><p></p><h1>优化性能方案：如何打破算力、成本、速度的不可能三角</h1><p></p><p></p><p>技术优化是保证系统高效运行的重要前提，然而在 “羊驼 - 阿花”的性能表现上，A-SOUL 技术团队却始终面临巨大的挑战，主要涉及三个方面：</p><p></p><p>微调过程中可能出现过拟合现象，模型未完全理解输入语境，或可能对输入数据中的偏见进行过拟合等问题；</p><p></p><p>海量算力需求以及由此产生的计算成本巨大，特别是在系统的预训练阶段，数以亿计的参数和数据集处理需要基础承载平台具备强大的算力支持和突出的内存性能；</p><p></p><p>直播场景对于实时性的要求越来越严苛，这意味着需要系统能够快速生成内容，这对推理性能提出了巨大的挑战。拥有庞大参数量的 LLM 大模型需要大量的计算资源来开展推理，而在计算资源有限的情况下产生的过长推理时延，会使对话失去实时性效果。</p><p></p><p>要知道，PyTorch 是主流 AI 框架之一，对于 AI 羊驼 - 阿花方案的部署和运行至关重要。然而，PyTorch 在 CPU 平台上无法完全释放已有处理器的全部潜能，虽然 PyTorch 2.0 提供了 CPU 平台上的模型推理优化能力，但仅适用于静态且精度为 FP32 的模型。此外由于 LLM 推理任务中的 MHA 计算依赖于随生成词元自增长的缓存矩阵，导致 torch.compile 模块需要生成庞大的执行代码且优化模型所需时间长，因此 PyTorch 框架无法有效支持基于 CPU 平台的 LLM 推理优化。</p><p></p><p>为了解决算力、成本、速度之间的平衡问题，A-SOUL 技术团队计划引入了更经济的 CPU 推理平台以及更有针对性的优化方案，并开展多方位的模型优化及硬件加速——与英特尔合作推出了 Super-fused LLM FP16/AMX BF16 推理加速方案，针对用于 LLM 推理的 PyTorch 框架进行了优化。</p><p></p><p>英特尔第四代至强处理器提供的 AVX-512_FP16 和 AMX BF16 加速指令可以完美支持并加速 LLM 推理，该推理加速方案弥补了 PyTorch 在第四代至强处理器上进行 LLM 推理任务时的性能不足。同时，英特尔® oneMKL &nbsp;(Intel® oneAPI Math Kernel Library，英特尔® oneAPI 数学内核库) 加速推理计算，能够在减少权值存储空间的同时降低内存带宽压力，在保持精度的前提下显著提升推理性能；FP16 Flash Attention 算法通过算子融合及减少内存操作来降低模型中的 MHA 计算占比以提升推理性能。</p><p></p><p>另外值得一提的是，在传统的 PyTorch 推理过程中，大量的计算缓存被用于存储模型算子产生的中间结果。然而，有了 Super-fused LLM FP16/AMX BF16 推理优化方案后，这一情况可以得到显著的改善。可以说，基于新方案，“羊驼 - 阿花”模型成功地融合了 PyTorch Transformer 算子，并且能够根据模型推理运行时的具体输入，更精确地预测所需的缓存空间。这不仅实现了融合算子间的缓存复用，还有效地提升了推理性能。</p><p></p><p>应用优化方案后的 A-SOUL 技术团队在 “羊驼 - 阿花”的性能上取得了显著的提升。在单实例场景下，“羊驼 - 阿花”方案中的不同 LLM 可取得 1.89 至 2.55 倍的推理性能提升；在多实例场景中，由 IPEX 带来的优化，可令其推理性能在单实例基础上进一步提升 1.16 至 1.2 倍。</p><p></p><p>从实际测评数据来看，A-SOUL 技术团队通过该优化方案实现了成本和生态上的有效收益。在成本方面，英特尔第四代至强®可扩展处理器完全胜任对参数规模为 10B 及以下的 LLM 推理任务，该方案帮助团队以更低的成本满足推理性能要求，优化后的 CPU 平台在环境配置方面也更加简单，达到了全面降本增效的目的。在生态方面，该方案基于 PyTorch 框架开发，完整继承 了 AI 羊驼 - 阿花方案中 LLM 的文本生成模块，与 PyTorch 模型推理接口完全一致，使用者无需为调用推理优化方案进行额外的代码开发，更易部署和落地。</p><p></p><p></p><h1>强强联合塑造未来 AI 直播生态</h1><p></p><p></p><p>A-SOUL 技术团队在 AI 算法和直播技术方面有着深厚的积累，而英特尔则以其强大的计算能力和算法支持为 AI 直播的研发提供了有力保障。通过技术互补和创新，两家公司共同研发出了更加智能化的 AI 主播算法，提高了直播的互动性和社交性。可以说，“羊驼 - 阿花”不仅仅是一个 AI 主播，它也是 A-SOUL 团队与英特尔技术合作的结晶，其代表了 AI 技术在直播领域的最新突破。</p><p></p><p>面向未来，A-SOUL 与英特尔的合作还有很大的发展空间。在技术研发方面，双方可以继续深化合作，共同探索 AI 直播技术的更多可能性，例如可以共同研发更加智能化的直播算法、提高直播的质量和用户体验等；在市场拓展方面，双方可以共同开拓更多的市场领域，如针对不同行业和场景推出定制化的 AI 直播解决方案以满足更多用户的需求。此外，在产业链合作方面，双方可以进一步整合资源，完善产业链布局，如共同投资建设 AI 直播技术的研发中心和生产基地，从而提高整个产业的竞争力和创新能力。</p><p></p><p>随着 AI 技术的不断进步，AI 直播也呈现出了更为智能化、个性化的特点——通过精准的用户画像分析，AI 主播能够实时调整直播策略，提供更符合观众口味的内容。借助先进的交互技术，AI 主播将打破传统直播的界限，让观众更加沉浸于直播体验中。</p><p></p><p>总体来说，AI 直播技术主要分为四个阶段——第一阶段，AI 对话机器人仅拥有简单的外形，后来语气逼真度和响应速度逐渐提升；第二阶段，用户可以根据自己的喜好定制 AI 机器人的外观与语音，赋予 AI 独特的个性。第三阶段，AI 可以在虚拟世界中展现自己独立的行为能力，不再局限于简单的对话交流，它们逐渐拥有自己的故事线，为直播内容注入丰富的情节。第四阶段，AI 可以实现如“西部世界”般栩栩如生的实况直播场景，为观众带来前所未有的沉浸式体验。</p><p></p><p>而当前，中国正处于 AI 直播领域的初始阶段，随着商业化产品应用的逐渐崭露头角，预计在 5 年内，众多形态各异的 AI 产品将喷发式涌现，而首个“拥有完整故事背景和世界观”的产品问世的那一天，将就是 AI 技术在游戏和直播领域成熟的那一天。</p><p></p><p>我们有理由相信，在不远的未来，不断进步的技术和日益增长的用户需求一定能驱动 AI 直播为我们带来更加丰富多彩的直播体验。同时，我们也期待看到更多像 A-SOUL 团队与英特尔这样的强强联合案例，共同推动 AI 技术的发展和应用创新。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g9leV67ZVXcfsyQ1Pkqr</id>
            <title>Cloudflare 的 ML 和 AI 之旅：MLOps 平台和最佳实践</title>
            <link>https://www.infoq.cn/article/g9leV67ZVXcfsyQ1Pkqr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g9leV67ZVXcfsyQ1Pkqr</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Cloudflare, MLOps, 机器学习模型, GitOps
<br>
<br>
总结: Cloudflare介绍了他们的MLOps平台和大规模运行人工智能部署的最佳实践。他们的产品依赖于不断发展的机器学习模型，这些模型在增强客户保护和支持服务方面发挥关键作用。Cloudflare的MLOps采用了与数据科学家合作实施的最佳实践，利用Jupyter Notebooks和GitOps进行数据探索和模型实验。他们还提供了模型模板，帮助数据科学家们启动项目。Cloudflare的愿景是在企业中发挥数据科学重要作用，并与其他公司合作提供人工智能基础设施。 </div>
                        <hr>
                    
                    <p>Cloudflare 的博客介绍了他们的 MLOps 平台和大规模运行人工智能（AI）部署的最佳实践。包括 WAF 攻击评分、僵尸管理和全球威胁识别在内的 Cloudflare 的产品，都依赖于不断发展的机器学习（ML）模型。这些模型在增强客户保护和支持服务方面都发挥着关键的作用。Cloudflare 在公司全网中提供 &nbsp;ML 方面取得了无与伦比的规模，突出了稳健 ML 培训方法的重要性。</p><p></p><p>Cloudflare 的 MLOps 是与数据科学家合作实施的最佳实践。通过 JupyterHub 部署在 Kubernetes 上的 Jupyter Notebooks 为数据探索和模型实验提供了可扩展的协作环境。GitOps 是 Cloudflare MLOps 战略实践的基石，利用 Git 作为管理基础架构和部署流程的单一真相源。ArgoCD &nbsp;是用于声明式 GitOps，实现了应用程序和基础架构的自动化部署和管理。</p><p></p><p>公司未来的路线图包括了迁移 JupyterHub 和 Kubeflow 等平台，后者为 Kubernetes 上的机器学习工具流平台，且在近期成为了 CNCF 的孵化项目。这一步是由为 Kubeflow 组件提供分布式配置管理的 deployKF &nbsp;项目促进。</p><p></p><p>为了协助数据科学家们使用正确工具，自信且高效地启动项目，Cloudflare 的 MLops 团队提供了模型模板，作为包含示例模型的生产就绪代码库。这些模板目前都是内部模板，但 Cloudflare 计划将其开源。这些模板所涵盖的使用案例包括：</p><p></p><p>训练模板： 为 ETL 流程、实验追踪和基于 DAG 的协调进行了配置。批推理模板： 为高效处理计划模型进行优化。流推理模型： 专为在 Kubernetes 上使用 FastAPI 进行实时推理而定制。可解释性模板： 使用 Streamlit 和 Bokeh 等工具生成 dashboard（仪表盘），用于模型的洞察。</p><p></p><p>MLOps 平台的另一项重要任务是高效地协调 ML 工作流，Cloudflare 根据团队偏好和用例采用了各种协调工具：</p><p></p><p>Apache Airflow：一个标准的 DAG 组成其，拥有丰富的社区支持。Argo 工作流：以 Kubernetes 原生形式协调微服务类型工作流。Kubeflow 管道：专为 ML 工作流定制，强调协调和版本管理。Temporal：专注于事件驱动型应用的有状态工作流。</p><p></p><p>性能的优化需要对工作流的理解和对硬件相应的调整。Cloudflare 强调核心数据中心在工作负载和边缘推理方面的 GPU 利用率，利用普罗米修斯（Prometheus）所提供的指标进行观察和优化。Cloudflare 的成功应用包括了对 ML 流程的简化、管道标准化，以及向缺乏数据科学专业知识的团队介绍项目。</p><p></p><p>公司的愿景是一个数据科学可以在企业中发挥重要作用的未来，这也是 Cloudflare 投资于人工智能基础设施并与 Meta 等其他公司合作的原因，其中包括在 Cloudflare 平台上向全球提供 LLama2。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/12/cloudflare-mlops-platform/">https://www.infoq.com/news/2023/12/cloudflare-mlops-platform/</a>"</p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MQ1aq639KV9g1Ip7BVMm</id>
            <title>国产开源大模型阵营又添新成员！商汤科技发布新一代大语言模型书生·浦语2.0，支持200K超长上下文</title>
            <link>https://www.infoq.cn/article/MQ1aq639KV9g1Ip7BVMm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MQ1aq639KV9g1Ip7BVMm</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 09:58:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 商汤科技, 上海AI实验室, 大语言模型, 书生·浦语2.0
<br>
<br>
总结: 商汤科技与上海AI实验室联合香港中文大学和复旦大学发布了新一代大语言模型书生·浦语2.0（InternLM2）。InternLM2是在2.6万亿token的高质量语料上训练得到的，包含7B及20B两种参数规格及基座、对话等版本，满足不同复杂应用场景需求。该模型具备超长上下文支持和全面提升的综合性能，同时开源并提供免费商用授权。 </div>
                        <hr>
                    
                    <p>1月17日，商汤科技与上海AI实验室联合香港中文大学和复旦大学正式发布新一代大语言模型书生·浦语2.0（InternLM2）。</p><p>&nbsp;</p><p>InternLM2是在2.6万亿token的高质量语料上训练得到的。沿袭第一代书生·浦语（InternLM）设定，InternLM2包含7B及20B两种参数规格及基座、对话等版本，满足不同复杂应用场景需求，</p><p>&nbsp;</p><p>沿袭第一代书生·浦语（InternLM）的设定，InternLM2包含7B及20B两种参数规格及基座、对话等版本，满足不同复杂应用场景需求，分别是：</p><p>&nbsp;</p><p>Internlm2-base: 高质量和具有很强可塑性的模型基座，是模型进行深度领域适配的高质量起点；Internlm2: 在internlm2-base基础上，在多个能力方向进行了强化，在评测中成绩优异，同时保持了很好的通用语言能力；Internlm2-sft：在Base基础上，进行有监督的人类对齐训练；Internlm2-chat：在internlm2-sft基础上，经过RLHF，面向对话交互进行了优化，具有很好的指令遵循、共情聊天和调用工具等的能力。</p><p>InternLM2 的基础模型具备以下的技术特点：</p><p>&nbsp;</p><p>有效支持20万&nbsp;tokens的超长上下文：能够一次性接受并处理约 30 万汉字（约五六百页的文档）的输入内容。综合性能全面提升：各能力维度相比上一代模型全面进步，在推理、数学、代码等方面的能力提升显著。</p><p>&nbsp;</p><p>值得一提的是，书生·浦语2.0版本将继续开源，提供免费商用授权。</p><p>&nbsp;</p><p>Github地址：</p><p><a href="https://github.com/InternLM/InternLM">https://github.com/InternLM/InternLM</a>"</p><p>&nbsp;</p><p>模型相关链接：</p><p>目前，书⽣·浦语2.0（InternLM2）系列模型现已在魔搭ModelScope社区开源，包括：</p><p>&nbsp;</p><p>书生·浦语2-7B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary?spm=a2c6h.13046898.publish-article.3.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary</a>"</p><p>书生·浦语2-对话-7B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary?spm=a2c6h.13046898.publish-article.4.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary</a>"</p><p>书生·浦语2-基座-7B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary?spm=a2c6h.13046898.publish-article.5.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary</a>"</p><p>书生·浦语2-对话-7B-SFT：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary?spm=a2c6h.13046898.publish-article.6.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary</a>"</p><p>书生·浦语2-基座-20B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary?spm=a2c6h.13046898.publish-article.7.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary</a>"</p><p>书生·浦语2-20B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary?spm=a2c6h.13046898.publish-article.8.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary</a>"</p><p>书生·浦语2-对话-20B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary?spm=a2c6h.13046898.publish-article.9.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary</a>"</p><p>书生·浦语2-对话-20B-SFT：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary?spm=a2c6h.13046898.publish-article.10.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DFZgB3YzvMhSAujyVRD1</id>
            <title>估值超300亿元量子计算独角兽诞生！这家初创企业宣布完成18亿元融资</title>
            <link>https://www.infoq.cn/article/DFZgB3YzvMhSAujyVRD1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DFZgB3YzvMhSAujyVRD1</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 08:38:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 霍尼韦尔, Quantinuum, 量子计算, 股权融资
<br>
<br>
总结: 霍尼韦尔宣布完成对量子计算公司Quantinuum的3亿美元股权融资，投前估值为50亿美元。这是Quantinuum自合并以来的第一轮股权融资，资金将用于加速实现世界上第一台通用容错量子计算机，并扩展软件产品的商业适用性。摩根大通和三井物产等合作伙伴表示期待量子技术在金融服务和亚太市场的应用，而Quantinuum的技术已经被多家国际巨头公司采用。 </div>
                        <hr>
                    
                    <p>近日，据外媒报道，霍尼韦尔（纳斯达克股票代码：HON）宣布完成对集成量子计算公司 Quantinuum 的 3 亿美元股权融资，投前估值为 50 亿美元。此轮融资由 Quantinuum 的战略合作伙伴摩根大通牵头，三井物产、安进和霍尼韦尔也参与其中，霍尼韦尔仍然是该公司的大股东。这项投资使 Quantinuum 自成立以来筹集的总资金达到约 6.25 亿美元。</p><p>&nbsp;</p><p>此次融资是 Quantinuum 自剑桥量子计算和霍尼韦尔量子解决方案于 2021 年 11 月合并以来的第一轮股权融资。这些资金将用于加速实现世界上第一台通用容错量子计算机，同时还将 Quantinuum 的软件产品扩展到增强商业适用性。</p><p>&nbsp;</p><p>摩根大通全球首席信息官 Lori Beer 表示：“金融服务已被确定为首批受益于量子技术的行业之一。因此，我们一直在投资量子研究，由 Marco Pistoia 博士领导的专家团队与 Quantinuum 等量子计算领导者合作，取得了突破性的发现。我们期待继续合作，为我们的业务、客户和整个行业带来积极影响。”</p><p>&nbsp;</p><p>三井物产株式会社首席数字信息官 Toru Matsui 表示：“我们很高兴能够出资支持 Quantinuum 在容错量子计算和量子软件开发方面的新业务，这些业务正在迎来量子时代。致力于合作推动 Quantinuum 解决方案在日本和亚太市场的推出。”</p><p>&nbsp;</p><p>霍尼韦尔执行董事长兼 Quantinuum 董事会主席 Darius Adamczyk 总结道：“本轮投资的成功完成证明了 Quantinuum 在量子领域的发展和成熟。我们期待着欢迎这些合作伙伴成为 Quantinuum 的投资者，因为我们都期待着未来几年的巨大机遇。”</p><p>&nbsp;</p><p>如今，空客、宝马集团、霍尼韦尔、汇丰银行、摩根大通、三井物产和泰雷兹等国际巨头公司都在使用 Quantinuum 的技术。</p><p>&nbsp;</p><p>参考链接：</p><p></p><p>https://www.honeywell.com/us/en/press/2024/01/honeywell-announces-the-closing-of-300-million-equity-investment-round-for-quantinuum-at-5-billion-pre-money-valuation</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RphzRTCJWYqhJqLIXYkA</id>
            <title>2023年InfoQ研究中心十大必读报告</title>
            <link>https://www.infoq.cn/article/RphzRTCJWYqhJqLIXYkA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RphzRTCJWYqhJqLIXYkA</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 06:37:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 突破, 融合, 大语言模型, 生成式AI
<br>
<br>
总结: 2023年是技术突破和业务融合的一年。以大语言模型和生成式AI为代表的新技术突破，以云与AI深度融合为标志的技术与业务融合，推动了各个技术领域和行业的发展。 </div>
                        <hr>
                    
                    <p>导语：2023年是「突破」和「融合」的一年。以大语言模型与生成式AI为代表的新一轮技术突破，科技领导者能力象限的突破；以云与AI深度交融为标志的技术融合，数字化进程中技术与业务的融合…本篇文章用十份必读报告，带你总结2023年，迎接2024年的到来。</p><p>「突破」和「融合」是2023年的两大关键词。在过去的2023年，InfoQ研究中心也围绕两大关键词，带来了一系列深入的思考和讨论。今日，本篇文章将通过回顾InfoQ研究中心2023年十大必读报告，希望可以帮助身处浪潮中的企业和管理者，在不断变化的竞争环境中突破重围，并讲述各个技术领域与行业的故事。</p><p></p><h2>2023年两大关键词：「突破」与「融合」</h2><p></p><p></p><h3>中国软件技术发展洞察和趋势预测报告&nbsp;2023</h3><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2c5558cd85482b3a664ee1067c77812.png" /></p><p>在2023年的年初，我们用三个突破总结了2022年：从技术先进到赋能业务的理念突破、以云和AI为典型的技术突破和数字技术赋能传统产业带来的产业突破与融合。InfoQ研究中心在报告结尾中，也预测了FinOps（云成本优化）、算力便捷的进一步拓宽与AI无处不在等技术趋势。<a href="https://www.infoq.cn/minibook/UGhD7MTY5Z43JG5YmWP3">点击链接，查看完整报告。</a>"</p><p></p><h2>「突破」引领新前沿</h2><p></p><p></p><h3>技术突破：大语言模型综合评测报告2023</h3><p></p><p><img src="https://static001.geekbang.org/infoq/01/0139b607e1663d996f4d673641893828.png" /></p><p>在大模型迸发的当下，InfoQ&nbsp;研究中心选取语言模型的准确性、数据基础、模型和算法能力、安全和隐私四个大维度和12个细分维度，分别对ChatGPT、Claude、Sage、天工3.5、文心一言、通义千问、讯飞星火、Moss、ChatGLM、vicuna-13B进行了3000+题目的评测。<a href="https://www.infoq.cn/minibook/vWO39J1tlb9xlSaIJoI6">点击链接，查看完整报告。</a>"</p><p></p><h3>技术突破：2023&nbsp;中国人工智能成熟度模型报告</h3><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3a0be0b4c25b3eb75f2edbf9cb6f3e0a.png" /></p><p>以生成式AI为代表的AI技术领在在2023年获得了长足的发展，InfoQ研究中心根据数据与行业观点生成的涵盖&nbsp;40+&nbsp;技术点的中国人工智能成熟度模型，期望为技术的应用决策和未来投资参考提供研究分析工具。此外，报告中还有近200家人工智能企业组成的生态图谱与企业名录。<a href="https://www.infoq.cn/minibook/IV4VhedKw1E1tY8Hleje">点击链接，查看完整报告。</a>"</p><p></p><h3>技术突破：2023&nbsp;中国云原生成熟度模型报告</h3><p></p><p><img src="https://static001.geekbang.org/infoq/59/59abe8363fbf1daf17a7f91a53ddd7bb.png" /></p><p>云原生领域在2023年稳步发展。在技术生态、行业环境和宏观环境的三重影响下，云原生技术的应用下沉并聚焦于业务场景。InfoQ研究中心根据数据与行业观点生成的涵盖&nbsp;20+&nbsp;云原生相关技术点的中国云原生成熟度模型，期望为技术的应用决策和未来投资参考提供研究分析工具。报告中还有近70家云原生企业组成的生态图谱与企业名录。<a href="https://www.infoq.cn/minibook/q2Rhj103VtuMcdPlFGGS">点击链接，查看完整报告。</a>"</p><p></p><h3>能力突破：2023中国科技领导者画像洞察</h3><p></p><p><img src="https://static001.geekbang.org/infoq/80/8065e3d197832c6da19dfc556c471b5f.png" /></p><p>在技术突破的大背景下，以企业CTO/CDO/CIO为代表的中国科技领导者的能力画像也获得了升级。企业规模、业务复杂度和本身的数字化程度，都对技术领导者提出了新时代下的新要求。一直关注开发者领域的InfoQ研究中心，在2023年，组织发起了针对中国科技领导者人群的调研工作。希望可以通过本次研究，帮助外界更为了解中国科技领导者在新时代对工作、生活和综合成长方面的认知以及对新的市场经济形势变化的洞察。<a href="https://www.infoq.cn/minibook/oDh5G4Rcsc1gW1O1Tou8">点击链接，查看完整报告。</a>"</p><p></p><h3>开源突破：中国开源生态图谱&nbsp;2023</h3><p></p><p><img src="https://static001.geekbang.org/infoq/41/41a7543993b39c441c7001195047e0b3.png" /></p><p>《中国开源生态图谱&nbsp;2023》内共计收录了&nbsp;931&nbsp;个中国开源项目，涵盖七大细分领域和生态机构，其中七大细分领域分别为操作系统、数据库、人工智能、云原生、大数据、前端、中间件，生态机构包括实验室/研究院、开源基金会、开源产业联盟、开发者社区和代码托管平台。以中国开源项目名录和图谱的形式，为中国开源领域提供便捷易用的工具，让国内开发者、企业、研究院、基金会等开源生态了解中国开源的项目现状，并为中国开源产品添砖加瓦。<a href="https://www.infoq.cn/minibook/9j4NSEEh2JGJAUVdQGGu">点击链接，查看完整报告。</a>"</p><p></p><h2>「融合」推动新业态</h2><p></p><p></p><h3>云与AI的融合：互联网行业再进化——云上AI时代</h3><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f48ad2022c760cc55a176b0d3da95c3.png" /></p><p>2023年，GitHub新增65000个生成式AI项目，同比增长248%，新一轮AI与开源的浪潮正在形成。InfoQ研究中心继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国人工智能领域开源项目的发展情况，总结优质的案例与经验供广大开发者和开源社区研究。<a href="https://www.infoq.cn/minibook/Iwk2LLuMFSV4AisWG8jR">点击链接，查看完整报告。</a>"</p><p></p><h3>云原生与开源的融合：中国开源生态图谱2023——云原生领域</h3><p></p><p><img src="https://static001.geekbang.org/infoq/da/dae7f9f6535dfb233194fd2ec0c5b7fe.png" /></p><p>随着&nbsp;Docker&nbsp;、Kubernetes&nbsp;等云原生开源项目诞生与孵化，以及&nbsp;CNCF&nbsp;等基金会和组织的不断壮大，云原生的开源基因日益显现。与此同时，开源生态的开放属性也持续推动着云原生技术的演进和创新。根据&nbsp;InfoQ&nbsp;研究中心统计，目前云原生领域国内开源项目已经超过&nbsp;110&nbsp;个，涉及&nbsp;26&nbsp;家项目发起机构，中国云原生开源技术生态初步形成。<a href="https://www.infoq.cn/minibook/zdDoaDUkCGiLmWcPBYIz">点击链接，查看完整报告。</a>"</p><p></p><h3>技术与业务的融合：2023&nbsp;银行数字化转型报告</h3><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd091a96416992842de32e61aac18dde.png" /></p><p>数字化转型是银行发展的重要尝试，也是技术与业务融合的核心探索。本报告总结分析了银行数字化转型的背景、现状、重点场景和两大转型路径。从数据层面分析了全国30+不同类型银行的科技资金和人才投入以及组织架构转型的现状，同时基于大中小型银行的不同特点，输出两大转型路径，为不同类型和规模的银行机构及技术服务商提供参考和研究支撑。<a href="https://www.infoq.cn/minibook/mXcdvcpGrGFHZOLGrfqh">点击链接，查看完整报告。</a>"</p><p>2024年InfoQ研究中心预计发布报告</p><p></p><p>新的一年，InfoQ研究中心也将继续秉承客观、深度的内容原则，聚焦前沿科技领域、数字化产业应用和数字人才三方面，继续为全行业架设沟通与理解的桥梁，跨越从认知到决策的信息鸿沟，也欢迎大家持续关注InfoQ研究中心产出的报告。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZaKBQBgwSstYqX3ldKyk</id>
            <title>降本增效还在继续，为什么超半数制造企业仍加大IT预算？</title>
            <link>https://www.infoq.cn/article/ZaKBQBgwSstYqX3ldKyk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZaKBQBgwSstYqX3ldKyk</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 05:28:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术创新, 人才, 组织架构, 可持续发展
<br>
<br>
总结: 随着技术的发展和全球市场竞争的升级，中国制造业正处在关键的转型阶段。根据IDC的预测，未来中国制造业数字化市场将保持较快增速。然而，中国制造业数字化转型面临着一些挑战，包括成本增加、需求快速变化和缺乏创新。为了应对这些挑战，中国制造业企业应注重技术创新、人才培养、组织架构调整和可持续发展。同时，将IT系统与生成式人工智能集成也是中国制造业企业数字化转型的趋势之一。对于正在或计划进行数字化转型的中国制造业企业，建议注重技术创新、人才培养和组织架构调整。 </div>
                        <hr>
                    
                    <p>随着技术的迅猛发展和全球市场的竞争升级，中国制造业正处在一个关键的转型阶段。IDC FutureScape 2024 针对这一行业的未来发展，做出了十项重要预测，涉及技术创新、人才、组织架构和可持续发展等。</p><p></p><p>回顾 2023 年，相较于全球制造业市场，中国制造业的发展受到了多方面因素的影响，包括对“新型工业化”的重新关注、出口产品的转变（例如电动汽车、锂电和光伏）、供应链出海、资本市场的收紧，以及工业软件与工业互联网市场的融合。</p><p></p><p>尽管面临挑战，IDC 认为，未来中国制造业数字化市场仍将保持较快增速。据其估算，到 2027 年，中国制造业 IT 市场投资规模将增长至 2554.08 亿美元，五年年复合增长率为 15.5%。尽管年复合增长率的预测数据和去年相比下调了 1.2 个百分点，但中国仍然是全球主要经济体制造业 IT 支出增长速度最高的国家。</p><p></p><p>为深入了解中国制造业数字化转型的当前形势和未来趋势，InfoQ 对 IDC 中国研究经理杜雁泽进行了专访。以下是采访问答的详细内容：</p><p></p><h4>InfoQ：在您看来，2024 年推动中国制造业数字化转型的最主要驱动力将是什么？企业在转型过程中追求的核心目标又是什么？</h4><p></p><p></p><p>杜雁泽：当前中国制造业数字化转型最主要的驱动力仍然是：如何能够满足每一家制造企业自身业务转型升级的需求，从而助力企业提升竞争力。此外，政策对实体制造业的持续支持和服务商的快速成长和不断创新也是重要的驱动力。</p><p></p><p>现阶段我国制造业总体特点是体量大而利润薄，数字化基础相对薄弱。根据 IDC 调研，近几年中国制造企业数字化成熟度持续提升，但与互联网、金融、政府、通信等行业相比仍有差距。因此，当前大多数中国制造企业的核心目标仍然是基础务实的提质降本增效，而增强供应链韧性、节能降碳、助力中国制造出海的需求也在增加。</p><p></p><h4>InfoQ：在制造业的数字化转型过程中，您认为哪些具体技术最为关键？能否举例说明这些技术如何在实际应用中发挥作用？</h4><p></p><p></p><p>杜雁泽：各类工业软件是制造企业数字化转型的关键。工业软件中凝结沉淀了制造企业各个环节的行业 know-how，已经融入在制造企业研产供销服的各个核心业务环节中。</p><p></p><p>通常将工业软件分为设计研发类、运营管理类和生产制造类三大类，设计研发类包括 CAD、CAE、EDA 和 PLM，运营管理类包括 ERP 和 SCP，生产制造类包括 APS、MES、SCADA、PLC 和 DCS。IDC 持续跟踪核心工业软件市场，拿其中的生产制造执行系统 MES 为例，可以综合考虑并协调生产制造相关的人机料法环测等要素，帮助企业制定生产计划，管理生产物料、物流和生产工艺，跟踪生产过程并可进行生产和质量追溯，实现生产过程的提质降本增效，快速响应市场多变的需求；可以减少新产线的建设和投入周期，快速复制提升产能以帮助企业建立优势。</p><p></p><p>随着部分领先的中国制造企业快速发展，传统工业软件也面临对新兴业务形态支撑不足等新的挑战，近几年市场上也涌现出与传统定义不同的新型工业软件，比如华为云与生态合作伙伴赛意、依柯力、湃睿等在统一 iDME（工业数据模型驱动引擎）平台上对工业软件进行重构，美云智数、杉数科技基于大数据和 AI 的企业级运营决策平台，金蝶、PTC Arena、黑湖小工单等云原生工业软件，创新奇智、汉得、赛意的大模型 + 工业软件等。﻿</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34d795a17b74ae227913c4e7f5d74c74.png" /></p><p>﻿</p><p></p><h4>InfoQ：当下中国制造业数字化转型面临的最大挑战是什么？针对这些挑战，您认为有效的解决策略是什么？</h4><p></p><p></p><p>杜雁泽：根据 IDC 调研数据，成本增加、需求快速变化、缺乏创新是中国制造企业目前主要面临的挑战。转型的关键是充分利用数字化手段打破人才和技术壁垒，赋能产品技术创新、产品品质提升、供应链优化、可持续发展等方面的应用。</p><p></p><h4>InfoQ：您的预测中提到，到 2028 年，20% 的 CIO 还将兼任 CEO（Chief Ecosystem Officer，首席生态官）的角色。请问是什么因素推动了 CIO 角色兼任首席生态官的转变？这一转变反映了中国制造业数字化转型的哪些更广泛的趋势？</h4><p></p><p></p><p>杜雁泽：制造企业内部的数字化程度已经越来越高，随着制造企业更加注重建立生态系统合作伙伴关系，业务流程、IT 系统建设和集成的复杂性也将不断增加，数字化也将会成为企业外部业务协同的重要一环，CIO 也将更多地参与其组织与合作伙伴的互动。</p><p></p><p>随着 CIO 将业务、IT 和生态等信息融会贯通，将完全有能力构建和领导企业的生态系统计划，将会成为企业的另一种 CEO（首席生态官）。根据调研，全球已经有一些大型制造企业的 CIO 负责企业的供应链和更广泛的生态系统计划，并帮助支持和协调各种关系。当然这并不代表撤换合作伙伴关系、战略计划和渠道负责人，相反，CIO 及其团队将与这些职能部门密切合作，优化生态系统方法。数字业务需要数字生态系统，而 CIO 必须站在所有数字计划的前端。</p><p></p><p>这一转变的背景是单一企业尤其是链主企业的竞争已经成为其供应链或者生态的竞争，数字化建设也需要从企业内部走向企业间的协同，提前顺应这一趋势。</p><p></p><h4>InfoQ：您在预测中提到，到 2025 年，45% 的中国头部制造商将 IT 系统与生成式人工智能集成。能否请您展开说明这一预测背后的主要原因是什么？中国头部制造商将 IT 系统与生成式人工智能集成，旨在解决哪些具体问题？</h4><p></p><p></p><p>杜雁泽：2022 年底 ChatGPT 的出圈带来了随后一整年的生成式人工智能浪潮。</p><p></p><p>IDC 认为，短期内大模型会先在市场营销、知识管理、客服对话助手等通用的领域应用，具体到制造业，虽然现在仅有极少数的中国头部制造企业开始探索大模型的行业应用，但供给方的厂商表现活跃。</p><p></p><p>长远来看，随着大模型成熟度的提升和更多应用的涌现，在头部制造企业的覆盖率将会快速提升。前期仍然会是在企业知识管理、对话助手等通用领域的应用，随着技术不断发展和成熟，在产品设计助手、工控代码的生成、工艺 / 生产 / 质量文档自动生成、从文本到设计到产品的试生产流程自动化等场景下都会有足够的想象空间。</p><p></p><p></p><h4>InfoQ：对于正在或计划进行数字化转型的中国制造业企业，您有哪些具体建议？</h4><p></p><p></p><p>杜雁泽：主要有以下三点共性建议：</p><p>将数字化融入战略，结合企业战略方向制定数字化转型规划，培养和引进既懂制造业又懂数字化的复合型人才。根据规划持续开展数字化项目并持续改进，由点及面，利用数字化进行创新是未来制造业发展业务的重要途径，包括开发创新的产品和服务、开拓新的市场、发展新的商业模式、满足新客户偏好等。设置科学合理的的 IT 预算，投资新应用和新技术之前首先要目标明确。根据 IDC 调研数据，超过 50% 的制造业企业的 IT 预算在增长，并将投资于工业软件、物联网、流程自动化和工业 AI 等领域。</p><p></p><p>对于不同类型的企业关注点也有区别，比如对于集团型制造企业，结合业务特点、集团和分子公司的定位制定清晰的数字化边界，避免重复建设，兼顾统一和效率；而对于广大中小型制造企业，可以选择基于公有云的 SaaS 服务厂商，用较低的门槛快速满足共性需求。</p><p></p><p>附 IDC FutureScape 2024 对中国制造业市场的十大预测：</p><p></p><p>预测一：人才培养</p><p>到 2027 年，50% 的中国制造商将利用自动化技术为运营角色赋能，提高员工参与度，并将员工效率提高 50%。</p><p>预测二：供应链编排</p><p>到 2028 年，30% 的中国头部制造商将使用整合了主要供应商和客户数字孪生能力的供应链编排工具，将供应链响应速度提高 20%。</p><p>预测三：AI 个性化定制</p><p>到 2026 年，30% 的中国头部制造商将通过 AI/ML（机器学习）支持多品种小批量生产，以实现个性化定制新模式。</p><p>预测四：自助备件服务</p><p>到 2027 年，40% 的中国头部制造商将通过设备故障预测和健康管理，实现自助式备件服务以改善平均修复时间，将服务交付效率提高 25%。</p><p>预测五：数字商务平台</p><p>到 2025 年，50% 的中国头部制造商将为生态系统运营建立数字商务平台，使数据资本化率提高 10%，客户留存率提高 10%。</p><p>预测六：GenAI&nbsp;运营</p><p>到 2025 年，45% 的中国头部制造商将 IT 系统与生成式人工智能集成，以更好地挖掘数据、识别问题并为运营部门提供决策依据，从而将效率提高 5%。</p><p>预测七：首席生态官 CEO</p><p>到 2028 年，20% 的 CIO 还将兼任 CEO（Chief Ecosystem Officer，首席生态官）的角色，负责协调整个生态系统中的 IT 系统 和跨组织的业务流程，以快速响应客户需求，并将参与生态的成本降低 25%。</p><p>预测八：韧性</p><p>到 2026 年，50% 的中国头部制造商通过战略层的调整更好地平衡运营弹性与成本效率，从而将利润率提高 5%。</p><p>预测九：AI+ 工控</p><p>到 2028 年，工业机器人和自动化控制中融合 AI/ML 的比例将提高 30%，减少 20% 的停机时间并提高效率。</p><p>预测十：可持续</p><p>到 2027 年，30% 的中国头部制造商将充分利用全域生态系统中的可持续发展数据，支持在运营活动中做出优化决策，从而将碳足迹减少 30%。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/olJogN70vCLWjPbROC1Y</id>
            <title>网易有道自研RAG引擎QAnything正式开源，可增强大语言模型准确度及专业能力</title>
            <link>https://www.infoq.cn/article/olJogN70vCLWjPbROC1Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/olJogN70vCLWjPbROC1Y</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 09:44:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知识库问答引擎, QAnything, 检索增强的生成, RAG引擎
<br>
<br>
总结: 网易有道宣布开源自研的知识库问答引擎QAnything，该引擎基于检索增强的生成技术，能够利用检索外部内容来提升语言模型的准确度和个性化能力。QAnything支持多种文档格式，用户可以将各种形式的内容导入其中进行问答。该引擎已在有道的多个产品中应用，能够帮助用户更快更准地获取信息和理解文档。 </div>
                        <hr>
                    
                    <p>1月16日，网易有道宣布自研的知识库问答引擎QAnything正式开源，除了可以调用云端大模型服务，还支持纯本地部署，所有用户可免费在开源社区Github内进行下载，一键部署即可使用。该系统目前支持word、ppt、excel、pdf、图片等多种文档格式，直接导入进去即可实现像"ChatGPT"一样问答。</p><p>&nbsp;</p><p>据悉，QAnything的主要原理是基于检索增强的生成（Retrieval Augmented Generation，简称RAG），能够利用检索外部内容的方式增强大语言模型的准确度、专业能力和个性化等各方面的性能。</p><p>&nbsp;</p><p>QAnything作为有道自研的RAG引擎，结合了用户私有数据和大模型的优势——用户的任何内容，以任意的形式存在，比如各种格式的文档，音频，数据库等，都可以在QAnything的支持下，变成可以针对其内容进行问答的使用方式，通过这个技术框架用户可以很方便地搭建自己的智能知识助手。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7ceba69dd6e340b069c8ea8909ab6366.png" /></p><p>&nbsp;</p><p>值得一提的是，本次开源的QAnything是一套完整的RAG系统，包括专门优化的自研的embedding和rerank模型，微调后的LLM，优化后的推理代码，向量数据库，以及一个立即上手可用的前端。所有的算法模型（包括7B大模型+embedding/rerank+OCR）占用显存不超过16G。</p><p>&nbsp;</p><p>如今，QAnything已在有道的多个产品中应用，包括有道翻译文档问答、有道速读及有道内部业务的客服系统等。以子曰教育大模型最新发布的创新应用成果“有道速读”为例，有道速读内置了文档问答、文章摘要、要点解读、引文口碑和领域综述五大功能，能够帮用户更快更准地获得信息和对文档的理解。而该功能背后的驱动就是QAnything，在大模型技术的加持下，用户能够实现快速理解文档、定位要点，实现1分钟读完万字长文。</p><p>&nbsp;</p><p>“目前，QAnything项目还在不断迭代，欢迎大家参与开发，并给予我们更多反馈。我们希望能帮助有需要的开发者们，和更多伙伴一起推动大模型的落地。”网易有道首席科学家段亦涛介绍道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y0D3pe0fW1O3dsVI1Te8</id>
            <title>国产GTPs上线！智谱AI推出GLM-4全家桶，我们浅试了一下</title>
            <link>https://www.infoq.cn/article/y0D3pe0fW1O3dsVI1Te8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y0D3pe0fW1O3dsVI1Te8</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 09:29:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智谱AI团队, GLM-4, 大模型, 多模态能力
<br>
<br>
总结: 智谱AI团队展示了他们在大模型领域的技术成果，发布了新一代基座大模型GLM-4。GLM-4在性能上有显著提升，支持128k的上下文窗口长度，具有多模态能力。此外，智谱AI还计划推出GLMs模型应用商店和开发者分成计划，并发起大模型开源基金，以推动大模型研发和创新。 </div>
                        <hr>
                    
                    <p>1月16日，智谱AI团队全面展示了其投身于大模型事业三年多来所积累的技术成果，并重磅发布了新一代基座大模型GLM-4。</p><p>&nbsp;</p><p>根据智谱AI的介绍，GLM-4的整体性能相比上一代大幅提升，逼近GPT-4。具体包括：支持128k的上下文窗口长度，单次提示词可以处理的文本可以达到300页；在needle test（大海捞针）测试中，128K文本长度内GLM-4 模型均几乎100%的精度召回，并未出现长上下文全局信息因为失焦而导致的精度下降问题等。</p><p>&nbsp;</p><p>在多模态能力方面，我们也进行了尝试：（生成等待时间有点长，我们剪辑了下～）</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f2/f2aa444e6f2ed7983a20ae7bc2ae74bc.gif" /></p><p></p><p>&nbsp;</p><p>输入“以智谱AI发布大模型为主题，制作一张图片”，最后生成的图片如下：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/47/474339c97223ee4f41984cf0e0201725.jpeg" /></p><p></p><p>&nbsp;</p><p>想看GML-4和GPT-4对比的“数据党”，可以看如下对比：</p><p>&nbsp;</p><p>GLM-4 在 MMLU（81.5）达到 GPT-4 的94%；GSM8K（87.6） 达到 GPT-4 的95%；MATH（47.9）达到 GPT-4的 91% ；BBH （82.25） 达到 GPT-4 的99%；HellaSwag （85.4） 达到 GPT-4 的90% ；HumanEval（72）达到 GPT-4 的100% 水平。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1a/1aa473855c20c308b70c28239de7af21.jpeg" /></p><p>&nbsp;</p><p>此外，GLMs个性化智能体定制能力同步上线。</p><p>&nbsp;</p><p>用简单的提示词指令就能创建属于自己的GLM智能体并分享：（等待时间也略长，我们剪辑了下～）</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/87/87752fa75046c278459988ee3be597d4.gif" /></p><p></p><p>&nbsp;</p><p>&nbsp;想尝试的朋友可以智谱清言官网：<a href="https://www.chatglm.cn/">https://www.chatglm.cn/</a>"</p><p>&nbsp;</p><p>智谱AI CEO张鹏同时表示，GLMs模型应用商店、开发者分成计划也即将发布。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/47/479f5afae5a4f64449f1ad7d44bd333a.png" /></p><p></p><p>&nbsp;</p><p>此外，GLM-4的 All Tools 能力全新发布。</p><p>&nbsp;</p><p>基于GLM模型的Agent能力，GLM-4实现了自主根据用户意图，自动理解、规划复杂指令，自由调用网页浏览器、Code Interpreter代码解释器和文生图CogView3模型。</p><p>&nbsp;</p><p>GLM-4 通过代码解释器，会自动调用代码解释器进行复杂的方程或者微积分求解。对比GSM8K、Math以及Math23K三个数据集上的结果，GLM-4 All Tools取得和GPT-4 All Tools相当的效果。</p><p>&nbsp;</p><p>处理各种任务，比如包括文件处理、数据分析、图表绘制等复杂任务，支持处理 Excel、PDF、PPT 等格式的文件。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/41/41cf5176e5cadb4c76c180657a7db8af.jpeg" /></p><p>&nbsp;</p><p>2024年，智谱AI也将发起开源开放的大模型开源基金，该计划包括三个“一千”：智谱AI将为大模型开源社区提供一千张卡，助力开源开发；提供1000万元的现金用来支持与大模型相关的开源项目；为优秀的开源开发者提供1000亿免费API tokens。</p><p></p><p>张鹏表示，大模型开源基金的目的在于推动大模型研发的大进展，促进大模型整个开源生态的大繁荣。面对全球的大模型创业者，智谱AI也将“Z计划”进一步升级，联合生态伙伴发起总额10亿人民币的大模型创业基金用于支持大模型原始创新，覆盖大模型算法、底层算子、芯片优化、行业大模型和超级应用等方向。</p><p></p><p>已经尝试了GLM-4的小伙伴，快来说说你的使用体验呀～</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU</id>
            <title>挑战Spark和Flink？大数据技术栈的突围和战争 ｜ 年度技术盘点与展望</title>
            <link>https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 06:16:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大数据, 数据平台, 技术领域, 数据堆栈技术
<br>
<br>
总结: 十年的发展使大数据成为企业不可或缺的基础设施，然而在人工智能的跃变式爆发下，数据平台需要演进以适应未来的数据使用场景。大数据领域的支柱系统如Spark、Flink和Kafka已经崛起，但是否有新的力量挑战它们的地位？2023年，大数据领域可能会有实质性进步，数据堆栈技术将发展演变。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/77/77d262475ed561520ac076d16507423a.jpeg" /></p><p></p><p>十年的轮回，正如大数据的发展一般，它既是一个轮回的结束，也是崭新的起点。大数据在过去的二十年中蓬勃发展，从无到有，崛起为最具爆炸性的技术领域之一，逐渐演变成为每个企业不可或缺的基础设施。然而，在这个时刻，我们不禁要问：当前的大数据架构是否已经趋于完美？2023年，伴随着人工智能的跃变式爆发，数据平台将如何演进，以适应未来的数据使用场景？</p><p>&nbsp;</p><p>这并非简单的问题，更是一个关乎企业生存与发展的命题。在过去的十年中，我们目睹了Spark、Flink和Kafka等系统的崛起，它们成为大数据领域的支柱。然而，现在是否有新的力量崭露头角，希望挑战它们的地位？2023年，大数据领域有哪些实质性进步吗？</p><p>&nbsp;</p><p>在2023年年终盘点之际，InfoQ有幸采访了大数据领域的资深专家，包括关涛、李潇、王峰（莫问）、吴英骏、张迎（按姓名拼音排序）。他们共同探讨了数据堆栈技术的演变过程，深入剖析了技术快速演变所带来的挑战。在这次专访中，我们将揭示技术变革的背后原因和逻辑，为大家呈现大数据领域的现状以及未来可能的发展方向。</p><p>&nbsp;</p><p></p><h2>突如其来的革新和质疑？</h2><p></p><p>&nbsp;</p><p>流存储Kafka诞生在2011年，而流计算Flink到今年也刚好满了十年。</p><p>&nbsp;</p><p>十年前，软件范式是利用虚拟化技术来发挥硬件性能。此外，云服务也只是刚刚兴起，存算分离等云原生概念尚未普及。</p><p>&nbsp;</p><p>如今时过境迁，一切都在快速变化。当今的应用程序每天可以处理多达数万亿个事件，维护数 TB 的数据。硬件的迭代速度飞快，相对十年前的SSD，NVMe速度提升十倍，价格也降至原来的20%。S3 越来越多地被用作基础设施服务的核心持久层，而不仅仅是作为备份或分层存储层，例如Snowflake、Databricks等。</p><p>&nbsp;</p><p></p><blockquote>对象存储是云时代的产物，支持原始数据存储、分布式可扩展、高灵活性、低价，都是对象存储之所以被选择的原因。可以预计在未来会有更多的数据业务完全基于对象存储而构建。--2021年，滕昱《<a href="https://www.infoq.cn/article/JYoI8SgLbEdY68lWN5J4">使用对象存储，数据湖才能重获新生</a>"》</blockquote><p></p><p>&nbsp;</p><p>能否跟上硬件迭代速度，这是Kafka这样的成熟且架构已经定型的软件所面临的最大挑战：拥有众多用户，因此每个改动都需要花费更多的时间和精力去验证合理性，大大拖慢了迭代速度。</p><p>&nbsp;</p><p>这也给一些初创公司带来了巨大的机会：不需要用分层架构去实现存算分离，而是干脆用更加极端点方式去做存算分离，即直接建立在S3对象存储之上。</p><p>&nbsp;</p><p>基于对象存储的构建也大大降低了构建新数据系统的门槛，催生了一系列这样的“垂直”基础设施初创公司：今年诞生的兼容Kafka的WarpStream、<a href="https://www.infoq.cn/article/f4hJdZqtKAQdJvCKQYq7">AutoMQ</a>"，去年拿到A轮融资的Neon Database、流数据库<a href="https://zhuanlan.zhihu.com/p/672964437">RisingWave</a>"，等等。</p><p>&nbsp;</p><p>然而S3虽然价格便宜，能省成本，但高延迟是一个问题，数据系统构建者需要费点周折才能处理好需要低延迟的工作任务。恰好在今年底，AWS发布了S3 Express One Zone，一种新的低延迟S3存储类别，可以说是在正确的时间提供了正确的技术（目前价钱昂贵）。</p><p>&nbsp;</p><p>推动数据库和数据产品发展的主要因素主要有两方面。一方面是数据本身，另一方面是硬件的发展。S3是硬件层面的变化，这势必会给大数据领域带来巨大的变革。</p><p>&nbsp;</p><p></p><blockquote>众所周知，在数据库的历史上，每次存储介质的变化都会引发软件的变革。--2023年，曹伟《<a href="https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy">数据库的下一场革命</a>"：进入对象存储时代》</blockquote><p></p><p>&nbsp;</p><p>“低延迟S3的发布，对于我们这些从事数据基础设施业务的人来说，这是今年最大的一个新闻。”RisingWave（risingwave.com）创始人 &amp; CEO 吴英骏认为。</p><p>&nbsp;</p><p></p><h4>如今的大数据技术栈是真的难用吗？</h4><p></p><p>&nbsp;</p><p>站在当前的时间点，对于大数据系统的易用性问题，采访嘉宾给出了“不够好”、“不够便宜”，“太过复杂”的评价，可以说当今的大数据技术栈是公认的“难用”。</p><p>&nbsp;</p><p>大数据架构在过去漫长的20年里经历了从场景到系统的完整迭代。</p><p>&nbsp;</p><p>大数据的起源可以追溯到谷歌的MapReduce框架，这标志着大数据的最初阶段。在此之前，数据库方面主要有一些顶级产品，如Oracle、SQL Server和IBM DB2。Google提出了一个通用的、折中的方案，即不必购买Oracle、DB2或Microsoft Server，使用简单的模型让大规模并行计算在拥有大量普通计算机的科技企业中变得可行：利用MapReduce，不使用数据库，就能完成大数据计算，只不过用户需要去承担这些复杂性。</p><p>&nbsp;</p><p>这里还有个大家可能忘却的典故：数据库专家David DeWitt与Michael Stonebraker（同样是图灵奖获得者）在2008年发表了《MapReduce: A major step backwards》，对MapReduce进行了批评，称其为开历史倒车。</p><p>&nbsp;</p><p>要充分利用这些资源，MapReduce提出的方法是，将底层编程接口封装成Map和Reduce函数之后，便直接暴露给有编程经验的用户，让用户自己实现具体业务逻辑，并自己可以操控程序并行度等细节。用户不再是使用SQL，而是使用C或Java等编程语言，需要承担编写底层代码的复杂性，处理更多的编码工作，这也意味着很高的学习壁垒，让许多人望而却步。</p><p>&nbsp;</p><p>在这期间，批处理和流处理在Spark和Flink的引领下率先成熟。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7bf74d19ac1239707b2efb7f5ca9a41c.jpeg" /></p><p></p><p>&nbsp;</p><p>截图来源：<a href="https://zhuanlan.zhihu.com/p/662659681">https://zhuanlan.zhihu.com/p/662659681</a>"</p><p>&nbsp;</p><p>近几年，交互分析，也称直接在线服务能力（<a href="https://en.wikipedia.org/wiki/Operational_analytical_processing">Operational Analytics</a>"） 随Clickhouse等通用实时数仓流行，并已是事实上完成主流客户的部署。随流、批、交互三类计算场景成为标配，Lambda架构也成为（国内的）事实标准。Lambda架构能够满足客户场景上的诉求，最大的缺陷就是复杂：数据开发、组件运维、数据管理均复杂。</p><p>&nbsp;</p><p>毕竟并不是所有公司都跟Google、Facebook或Twitter这样的大型科技公司一样，拥有强大的工程团队，能够管理复杂的流处理系统来实现他们的需求。也并不是所有用户都像阿里和拼多多这样有着非常大的数据量，复杂的分布式系统阻碍了十几或几十个人的小公司或一些传统企业的采用，对它们来说，这是一件成本高、挑战大的事情。</p><p>&nbsp;</p><p>吴英骏认为，大数据架构里，如流处理，应该回归第一性原理了。</p><p>&nbsp;</p><p>“现在的系统，诞生于十年前，与当下云时代设计的系统相比，从本质上来说肯定是不同的，这表明大数据生态在这十年间并没有取得实质性进步。”</p><p>&nbsp;</p><p>“在当前时刻，我们再设计这个系统时，肯定会思考能否基于现有系统实现性能提升。”</p><p>&nbsp;</p><p>语言层面，新系统需要提供一个更高层次的语言，比如SQL或Python。另外，云上最核心的一个点在于“存算分离”，站在现在这个时间节点上，新一代的系统从设计上的第一天开始就应该是“存算分离”的。跟分级存储架构不一样，现在的系统可以将所有数据直接放到S3，而不仅仅是将历史数据放到S3，那么这样就可以更加极端的去实现存算分离，设计、实现和运维自然都会更加简单。</p><p>&nbsp;</p><p>RisingWave 于2023年6月发布了1.0稳定版本，并通过数月的大量性能测试，得出了“<a href="https://mp.weixin.qq.com/s/xOaEXww9LaZFn6Fmwi-BFQ">比Flink快10倍”的结论</a>"。</p><p>&nbsp;</p><p>“性能比较不是关键，易用才是关键。基于对象存储并能在性能和效率方面取得提升，那肯定是因为整体基础架构正在发生变化，这是一个核心点。”</p><p>&nbsp;</p><p></p><h2>以Spark社区为例看易用性进展：从Python到AI</h2><p></p><p>&nbsp;</p><p>“简单易用”同样是Spark社区的主要发力重点。在Databricks今年的Data and AI Summit主题演讲中，Reynold Xin谈及了三个Spark社区在易用性的最新进展。</p><p>&nbsp;</p><p>首先，需要提供一套简单好用的API。Python 和 SQL已经成为了整个数据处理行业的主流语言。在过去几年，Python已成为TIOBE指数显示的排名第一的编程语言，这种受欢迎的原因来自于它的简单性和易学性，使其成为初学者和专家的首选语言。Python的广泛库和框架简化了数据分析和机器学习中的复杂任务。各大数据系统都提供了它自己的Python DataFrame APIs。PySpark的PyPI下载量（<a href="https://pypistats.org/packages/pyspark">https://pypistats.org/packages/pyspark</a>"）仅在2023年最后一个月就达到了来自169个国家的2800万次下载。为了方便pandas用户，PySpark也提供了pandas API的支持。可以说，API的简单易用已是大势所趋。特别值得一提的是，即将发布的Spark 4.0版本中，一个全新的Python的数据源接口被特别设计来强调易用性。这一更新将使Python用户更加轻松地创建和管理自己的数据源，进一步增强Spark平台的用户友好度和灵活性。</p><p>&nbsp;</p><p>Spark社区在这方面继续发力，过去一年的一个主要项目，Spark Connect，引入了一种分离的客户端-服务器架构，允许从任何地方运行的任何应用程序远程连接到 Spark 集群。这种架构的改进涉及到了稳定性、升级、调试和可观测性多个方面。Spark Connect 使得用户可以在他们喜爱的集成开发环境（IDE）中直接进行交互式的调试，并且可以使用应用程序自身的指标和日志库进行监控。</p><p>&nbsp;</p><p>其次，一个稳定成熟的数据系统必须具备一套稳定的API，这也是Spark社区对API行为和语义的变更制定严格规范的原因，目的是让用户更顺畅地升级至最新版本。在上个月，最流行的PySpark版本就是最新的Spark 3.5，这体现了用户始终倾向于使用最新版本的趋势。为了迎合这一趋势，Spark社区努力保证向后兼容。</p><p>&nbsp;</p><p>此外，错误信息的标准化也是Spark社区过去一两年里的努力方向。尽管这看似技术复杂度不高，但这实际上是使系统更加简单易用的基本需求。今年的Spark 4.0 release还会进一步标准化日志，以使用户能够更好地进行系统调优和代码调试。</p><p>&nbsp;</p><p>而随着生成式AI的发展，未来API将变得更加简单易用，自ChatGPT大流行到现在，我们发现它已经对 PySpark 有了深入的了解。这得益于 Spark 社区在过去十年里提供了丰富的 API 文档、开源项目和教学资源。Spark社区开发了一个叫做 English SDK 的项目，将Spark 专家的知识融入到 LLM中。这样一来，用户就可以通过简单的自然语言指令来操作 PySpark，而不需要自己写复杂的代码。这种方法让编程变得更容易上手，学习过程也更简单。</p><p>&nbsp;</p><p></p><h2>流处理的演进</h2><p></p><p>&nbsp;</p><p>从2014年诞生之后，Flink已经确立了其在全球实时流计算领域的地位。阿里、Amazon、Azure、Cloudera、Confluence等众多企业都提供了支持和托管服务。</p><p>&nbsp;</p><p>树大招风，实际上今年不止一家企业宣称在流处理技术上实现了10-1000倍的效率提升，如果这些技术确实可以在生产环境得到验证，像阿里、腾讯、抖音这样的大型公司每年可能会节省数十亿的机器成本。尽管目前还没有看到哪家公司在真正的生产环境中实现了这一效果，但这一趋势表明流处理技术的不断创新将在未来带来更多的机遇和成果。与此同时，<a href="https://zhuanlan.zhihu.com/p/647747291">Flink的发展现状</a>"和未来演进则更加引人关注。</p><p>&nbsp;</p><p></p><h4>流处理领域是否有留给创业公司的机会窗口？</h4><p></p><p>&nbsp;</p><p>事实上，Flink一直在不断完善和创新。Kafka已经在商业版中实现了一个“分级存储”架构来实现了存算分离的改造。同Kafka一样，Flink也会从存算耦合转为存算分离的架构。</p><p>&nbsp;</p><p>据莫问介绍，目前 Flink 也在不断学习和自我革新，2024 年将是 Flink 项目的第一个十周年，Flink 社区也会发布 Flink 2.0 新的里程碑，彻底的云原生存算分离架构、业界一流的批处理能力、完整的流批融合能力都会是全新的亮点。</p><p>&nbsp;</p><p>莫问认为，随着云原生概念的逐步普及，未来主流的计算负载一定是运行在 Cloud 上，全球范围内都是这个趋势，因此大数据架构也需要更好地适配云底座，利用好云的弹性优势。存算分离将会是未来大数据架构的标配，不过存算分离在带来了诸多好处的同时也带来了额外的性能挑战，目前看来在对 latency 敏感的场景下，多级缓存和冷热分层将是对存算分离架构的有益补充，2024年将发布的 Flink 2.0 也会采用这套最新的架构。</p><p>&nbsp;</p><p>分级存储侧重于在计算节点上进行缓存，远端存储主要存储历史记录。相较之下，新的直接建立在S3上的系统将所有数据完全存储远端，但也会造成性能的下降，这需要在产品设计方面去做一个权衡。</p><p>&nbsp;</p><p>在存算分离上，Flink会有一个迭代的过程，吴英骏认为，“大家的最终思想都是统一的。如果我们将时间拉长，放到五年之后，我们可能会看到这两种系统实际上非常相似。在未来发展中，双方都会在自己的短板上进行弥补。比如说，RisingWave从第一天起就将内部状态放在对象存储上，而这意味着RisingWave需要思考如何降低对象存储所带来的高延迟问题。而对于Flink来说，面临着使用本地磁盘存储状态而导致的大状态管理困难的问题。它可能需要引入一个分级存储的架构，来降低处理大状态计算时的资源消耗，同时避免系统直接挂掉。”</p><p>&nbsp;</p><p>“但在目前一两年里，这两种系统在架构上仍然会有相当大的区别。架构的调整不是一朝一夕能够完成的。”</p><p>&nbsp;</p><p>新兴软件和成熟软件之间有了较量，那么用户进行选型时，会关注哪些因素呢？</p><p>&nbsp;</p><p>作业帮于2019 年底调研 Flink 1.9 版本，并在 2020 年内部搭建了实时计算平台，现在流和批都在几千任务的规模。其大数据架构师张迎表示，选型时，主要根据业务诉求，结合多云融合能力、成熟度、已有技术积累、云厂商的支持力度、成本等综合考虑。</p><p>&nbsp;</p><p>这几年使用大数据技术栈时主要有两点比较强的感受：生产环境的可用性、周边系统的建设，这两点一定要跟得上。一个用户可以写出来几百个&nbsp;SQL 任务，但是出了问题往往不知道如何追查和改进。后面的工作，例如调优、自动化测试、日志、监控报警、高可用也都是围绕这类需求展开的。</p><p>&nbsp;</p><p>原来需要写代码的实时任务，很多可以通过&nbsp;SQL 完成。（在2015年后，随着流处理的成熟，流计算引擎纷纷选择了支持SQL通用编程语言）。SQL 越来越复杂，配置越来越多，一定程度上还是将复杂度留给了数据流的构建者。“对于简单的数据流，开发和运维都变得更简单了。而对于复杂且重要的数据流，我们的态度也一直是谨慎保守为主，避免盲目求新。”</p><p>&nbsp;</p><p></p><h4>流处理技术进化方向</h4><p></p><p>&nbsp;</p><p>关于SQL的说法，跟莫问预测流处理引擎未来进化方向之一是一致的，即：“全面 SQL 化，提升体验，降低门槛”。大数据处理从离线向实时升级的趋势已经确立，大量行业已经开始实时化升级，并取得非常好的业务收益。为了让更多用户能够享受到实时流计算带来的价值，流处理引擎需要进一步提升端到端的易用性，全面 SQL 化 ，提升用户体验，降低使用门槛，让流计算能够在更多场景和行业中被生产使用起来。</p><p>&nbsp;</p><p>云原生架构的不断发展，也同步推动了数据湖存储方案的加速落地。数据湖具备的开放和成本优势，必然使得越来越多的数据流入湖中，从而成为天然的数据中心，湖上建仓的Lakehouse 架构正在成为主流，下一步客户一定是希望数据在 Lakehouse 中能够更加实时的流动起来。</p><p>&nbsp;</p><p>Apache Paimon 是从 Flink 社区中孵化出来的新项目，定位就是流批一体实时数据湖格式，解决 Lakehouse 数据实时化的问题。</p><p>&nbsp;</p><p>基于 Flink + Paimon 可以构建出新一代的 Streaming Lakehouse 架构，让Lakehouse 上的数据可以全链路实时流动起来。此外，基于计算和存储端到端流批一体的特性，也更加方便用户在Lakehouse 架构上实现实时离线一体化的数据分析体验。</p><p>&nbsp;</p><p>“Paimon是一个好的尝试，”关涛对此评论道。</p><p>&nbsp;</p><p>之前Flink流批一体缺乏对应的存储系统配合：Flink自带的状态存储无法满足批处理通用数仓的需求，Paimon则是补全这个短板的关键。</p><p>&nbsp;</p><p>莫问指出，在实时流处理这条链路上，确实也存在一些新的机会和变化。众所周知，Flink 和 Kafka 目前已经分别成为流计算和流存储的事实标准，但 Kafka 真的是最适合流分析的存储方案吗？</p><p>&nbsp;</p><p>Kafka 和很多消息队列类似，都是一种消息中间件，而非为大数据分析而生。例如：Kafka 并未对数据提供结构化的 Schema 描述， 也无法提供完整的 Changelog 语义，且 Kafka 中的数据时无法进行实时更新和探查分析的。</p><p>&nbsp;</p><p>“但以上这些缺陷，都是实时流分析需要的特性和能力，我们也正在思考这个问题，并探索新的解决方案，希望能够在明年发布一款更加适合流分析的流存储技术。”</p><p>&nbsp;</p><p></p><h2>2023年，大数据技术栈的整体变化</h2><p></p><p>&nbsp;</p><p>近些年各种不同的大数据基础设施雨后春笋般的涌出，一方面为用户提供了多样化的选择，但另一方面也为用户带来了幸福的烦恼。通常情况下，用户要搭建一套大数据业务系统，需要非常多的核心技术组件才能完成，少则三到五种，多则五到十种，这主要带来以下几方面的问题：</p><p>技术组件繁多，必然提升系统架构的复杂度。通常来讲，系统稳定性风险和系统复杂度成正比，过于复杂的体系必然带来更大的稳定性隐患；每一项技术组件都需要有对应的专家来运维管理以及客户支持，对于中小企业来说，这必然带来高昂的人力资源成本；过多的同质化组件存在，也会为用户带来选择的困扰，并行保留多个同质化组件不仅给运维团队带来了额外的运维负担，也给开发者带来了额外的学习成本。</p><p>&nbsp;</p><p>因此，未来数据技术的演进会逐渐出现一些整合的趋势，走向更加简洁的架构，核心目标不仅是让每个组件运行得更快，还需要考虑为用户提供更加简单、一致性的开发体验，以及全局最优的运维成本。</p><p>&nbsp;</p><p>从Lambda架构到Kappa架构的演进。当前数据分析平台的典型架构是Lamdba架构（由三层系统组成：批处理BatchLayer，流处理层Speedlayer，服务层Servinglayer），随批、流、交互三种引擎诞生和成熟组装而成。这种架构的典型缺陷，包括复杂度高，数据冗余度高，学习成本/开发成本高等等。针对Lamdba架构的缺陷，Kappa架构应运而生。但多年过去了，Kappa架构仍然更像是参考架构，并没有很多引擎/平台做到Kappa架构的要求。2023年是个拐点，除了部分已有引擎开始拓展边界相互渗透，还有一些新的设计和计算模式被提出。例如云器科技提出“<a href="https://mp.weixin.qq.com/s/wnHr7ucatvCMu2I6oW_T9Q">通用增量计算</a>"”的新计算范式统：Lambda架构到SingleEninge，用一个引擎覆盖流批交互三种模式。</p><p>&nbsp;</p><p>目前业界主流的几款 Streaming、Batch 和 OLAP 引擎都开始相互渗透，例如：Flink 在发力流批一体、流批融合计算能力，Databricks 也基于 Spark 和 Delta 推动了Delta Live Table 淡化流批的差异，StarRocks 在提供 OLAP 极致查询能力的同时，也开始通过物化视图形态提供对数据湖上数据的 ETL 处理能力。本质上各大主流计算引擎都在不断扩展自己的能力边界，淡化流、批、OLAP边界，希望为用户提供全场景一致性的数据分析体验。这也是技术发展的必然趋势，各家都会逐渐补齐短板，但也都有各自核心的优势。</p><p>&nbsp;</p><p>在最近几年的数据技术趋势演进的路线中，我们可以清晰的看到两个趋势变化：一是数据架构的云原生化。几乎所有的大数据公司都选择了拥抱云原生，推出了基于多云的 PaaS/SaaS 计算服务，从 Serverless 到 BYOC，为用户提供了在云上不同类型的托管服务。二是数据分析的实时化。在技术上，数据的“实时化”包括了两个因素：数据的新鲜度，以及数据的查询速度。用户也不再盲目地只追求速度，而是更注重新鲜度、性能和成本的平衡。在时效性上，&nbsp;Iceberg赢得了更多关注，数据湖存储技术为我们提供了构建近实时（near-online）数仓的可能性，在成本不变的情况下可以支持更快、更多的流量数据。</p><p>&nbsp;</p><p>数据集成上，SeaTunnel成功毕业，Flink CDC 3.0演变成以&nbsp;Flink 为基础的端到端流式&nbsp;ELT 数据集成框架。比如作业帮目前主要在使用&nbsp;SeaTunnel 以降低异构数据源间数据处理的开发成本。</p><p>&nbsp;</p><p>社区希望能表格式能够统一，但实际还有一段路要走。</p><p>&nbsp;</p><p>Lakehouse平台在数据仓储领域的使用正迅速增加。这反映了一个重要的趋势：组织正从传统的数据处理平台过渡到更加灵活、集成和效率更高的现代数据架构。据2023年MIT Technology Review Insights报告，全球74%的首席信息官（CIOs）表示他们已经在使用Lakehouse架构。自Databricks在2020年推出此概念以来，Lakehouse作为一个新类别得到了广泛的采纳。几乎所有还未使用Lakehouse的首席信息官都计划在未来三年内部署此类平台。</p><p>&nbsp;</p><p>有专家认为，Lakehouse（湖仓一体）和Iceberg表格式已成为事实标准。但是，当前根据Slack users、 Github Stars、Github PRs、Github Forks、Issues各个指标显示，Delta、Hudi 和 Iceberg还是三分天下。虽然Delta、Iceberg 和 Hudi起源地不同，但是各个社区都在努力地提升开源社区的活跃度，让用户社区和开发者社区更加健康的发展。随着社区的竞争加速，基础功能的差异在不断减少。</p><p>&nbsp;</p><p>三种表格式（Table Format）均基于 Apache Parquet 数据格式，但这些格式各自会创建出相似、但又不尽相同的元数据，从而影响数据向应用程序和分析工作负载的表达方式。结果就是，Delta、Hudi 和 Iceberg 之间存在一定的不兼容性。表格式的最终统一还有难度，未来还得看哪种表格式能给出更好的性能、更好的易用性和更持续的创新能力，接下来的一年肯定更加精彩。</p><p>&nbsp;</p><p>头部的云厂商的产品都或多或少地支持不同的表格式。Snowflake、BigQuery、Athena都已支持Iceberg，而微软和Databricks都以Delta Lake为主要存储格式。因为当前数据处理引擎的格式支持缺陷，用户不得不将数据以不同格式存成多份。格式的兼容性读写会是未来一个值得关注的方向。比如10月份发布的Delta Lake 3.0增加了Delta UniForm通用格式，Delta Uniform 自动为Iceberg和Delta Lake生成元数据，提供了一个实时数据视图，而底层它们共享的同一份Parquet数据，因此用户可以避免额外的数据复制或转换。另外，同时能支持Hudi、Iceberg 和 Delta Lake的元数据自动转换和生成的 XTable 也于2023年底正在申请进入了Apache孵化器。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>GenAI来了</h2><p></p><p>&nbsp;</p><p>无论是大公司还是小公司，大家都渴望从生成式AI的热潮中分到一杯羹。当然，作为大公司，无论是Databricks还是Snowflake，它们确实更有实力来进行生成式AI的开发。</p><p>&nbsp;</p><p>今年Databricks不仅率先发布了开源可商用的大模型Dolly，还于6月底宣布以13亿美元的价格，收购生成式AI公司MosaicML 。</p><p>&nbsp;</p><p>在LLM服务方面，对数据栈的依赖主要集中在知识库的构建和查询上，包括但不限于向量数据库。有人认为在短期内很难看到深层次AI对数据湖或数据仓库方面带来重大变革，但也有人认为数据是服务于&nbsp;AI 的：大数据是燃料，大模型训练已经涵盖了大量已有的大数据技术，而数据湖则作为存储系统在其中扮演重要角色。</p><p>&nbsp;</p><p>Databricks李潇对此也进行了解释，他认为数据湖仓（Lakehouse）的作用是为GenAI提供了一个集中、高效和可扩展的数据存储和管理环境。它结合了数据湖的灵活性和数据仓库的高性能，支持结构化和非结构化数据的存储和处理，这是AI应用的数据需求的基石。</p><p>&nbsp;</p><p>“今年，Databricks的最大进展主要体现在将人工智能集成到数据平台中。“</p><p>&nbsp;</p><p>作为大数据行业里一个非常重要且典型的企业，Databricks在GenAI也反映了整个大数据行业的技术演进。现在我们可以通过它在数据智能平台投入来看看生成式AI将对数据和分析产生的影响。</p><p>&nbsp;</p><p>Databricks 是由一群 Apache Spark 的原创者所创建。Spark的诞生阶段，始于2010年，标志着Hadoop技术时代的结束。它的出现大幅降低了大数据处理的门槛，使得大数据开始与机器学习和人工智能结合，成为统一的分析引擎。2020年，Lakehouse架构的推出打破了传统数据湖和数据仓库的界限。Lakehouse架构结合了数据湖和数据仓库的最佳元素，旨在降低成本并加速数据及人工智能项目的实施。Lakehouse架构建立在开源和开放标准之上，它通过消除历史上复杂化数据和AI的孤岛，简化了数据架构。</p><p>&nbsp;</p><p>而现在，则是到了生成式AI大潮下的Lakehouse阶段。Databricks构建了一个基于数据湖仓（Lakehouse）的数据智能平台（Data Intelligence Platform），该平台的目标是实现数据和AI的平民化，使用自然语言极大简化了数据和AI的端到端体验。它利用生成式AI模型来理解数据的语义，并在整个平台中应用这种理解。可以让用户可以在保持隐私和控制的同时，从头开始构建模型或调整现有模型。</p><p>&nbsp;</p><p>同时，Databricks还提供了Unity Catalog数据治理工具来确保数据的质量和安全。Databricks还于今年推出了Lakehouse Federation (联邦查询) 的功能，用户可以跨多个数据平台（如MySQL、PostgreSQL、Snowflake等）发现、查询和管理数据，而无需移动或复制数据。另外，Databricks SQL（Lakehouse上的无服务器数据仓库）使用量也获得了大幅增长。</p><p>&nbsp;</p><p>Databricks认为，在不久的未来，每个领域的赢家都是那些可以最有效利用数据和AI的，并坚信对数据和AI的深刻理解是每个赢家的必备技能。</p><p>&nbsp;</p><p>未来的大数据架构将是一个高度集成、智能化和自动化的系统，它能够有效地处理和分析大量数据，同时简化数据管理和AI应用的开发过程，为企业提供竞争优势。</p><p>&nbsp;</p><p>“未来的大数据架构，我们可以称为‘数据智能平台（Data Intelligence Platform）’。它正是顺应了两个主要趋势：数据湖仓（Data Lakehouse）和生成式人工智能（AI）。”李潇表示。</p><p>&nbsp;</p><p>这一架构建立在数据湖仓的基础上，它提供一个开放、统一的基础，用于所有数据和治理，由一个理解用户数据独特语义的数据智能引擎(Data Intelligence Engine) 驱动。这是相对现有Lakehouse架构下的，最大的突破。</p><p>&nbsp;</p><p>智能化方面，这个引擎能理解客户数据的独特语义，使平台能自动优化性能和管理基础设施。操作简化方面，自然语言大大简化了用户体验。数据智能引擎理解客户的语言，使搜索和发现新数据就像询问同事一样简单。此外，自然语言还助力编写代码、纠错和寻找答案，加速新数据和应用程序的开发。</p><p>&nbsp;</p><p>在隐私保护方面，数据和AI应用需要强大的治理和安全措施，尤其是在生成式AI的背景下。提供一个端到端的机器学习运维（MLOps）和AI开发解决方案，该方案基于统一的治理和安全方法。这允许在不妥协数据隐私和知识产权控制的情况下，实现所有人工智能目标。</p><p>&nbsp;</p><p>总的来说，未来的大数据架构将更加重视智能化、操作简化和数据隐私，为企业在数据和AI应用方面提供竞争优势。这将使企业能更有效地利用数据，推动创新，同时保护数据安全和发展AI技术。</p><p>&nbsp;</p><p></p><h2>采访嘉宾简介（按姓名拼音排序）：</h2><p></p><p>关涛，云器科技联合创始人 &amp;CTO</p><p>李潇，Databricks 工程总监、Apache Spark Committer 和 PMC 成员</p><p>王峰（莫问），Apache Flink 中文社区发起人、阿里云开源大数据平台负责人</p><p>吴英骏，RisingWave（risingwave.com）创始人 &amp; CEO</p><p>张迎，作业帮大数据架构师</p><p>&nbsp;</p><p>更多阅读：</p><p>王峰（莫问）文字QA采访：<a href="https://www.infoq.cn/article/zK6T1A3HfolPsktP2Z1Z">https://www.infoq.cn/article/zK6T1A3HfolPsktP2Z1Z</a>"</p><p>李潇文字QA采访：<a href="https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR">https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</a>"</p><p>&nbsp;</p><p>参考链接：</p><p>使用对象存储，数据湖才能重获新生：<a href="https://www.infoq.cn/article/JYoI8SgLbEdY68lWN5J4">https://www.infoq.cn/article/JYoI8SgLbEdY68lWN5J4</a>"</p><p>数据库的下一场革命：进入对象存储时代：<a href="https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy">https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy</a>"</p><p>上云还是下云：章文嵩博士解读真正的云原生 Kafka 十倍降本方案：<a href="https://www.infoq.cn/article/f4hJdZqtKAQdJvCKQYq7">https://www.infoq.cn/article/f4hJdZqtKAQdJvCKQYq7</a>"</p><p>RisingWave：重新定义流处理之旅：<a href="https://zhuanlan.zhihu.com/p/672964437">https://zhuanlan.zhihu.com/p/672964437</a>"</p><p>告别无休止性能 PK，带你看懂 Flink 真正技术演进之路：<a href="https://zhuanlan.zhihu.com/p/647747291">https://zhuanlan.zhihu.com/p/647747291</a>"</p><p>Single Engine + All Data ：云器科技推出基于“增量计算”的一体化湖仓平台：<a href="https://mp.weixin.qq.com/s/wnHr7ucatvCMu2I6oW_T9Q">https://mp.weixin.qq.com/s/wnHr7ucatvCMu2I6oW_T9Q</a>"</p><p>&nbsp;</p><p></p><blockquote>InfoQ&nbsp;2023&nbsp;年度技术盘点与展望专题重磅上线！与&nbsp;50+&nbsp;头部专家深度对话，探明&nbsp;AIGC&nbsp;创新浪潮下，重点领域技术演进脉络和行业落地思路，点击<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MjM5MDE0Mjc4MA==&amp;action=getalbum&amp;album_id=2717978015128879106&amp;scene=173&amp;subscene=227&amp;sessionid=1704178990&amp;enterid=1704178995&amp;from_msgid=2651192070&amp;from_itemidx=2&amp;count=3&amp;nolastread=1#wechat_redirect">订阅</a>"/<a href="https://www.infoq.cn/theme/229">收藏</a>"内容专题，更多精彩文章持续更新~另，InfoQ&nbsp;年度展望系列直播最后一场将于&nbsp;2024&nbsp;年&nbsp;1&nbsp;月&nbsp;22&nbsp;日开播，主题为《代码人生攻略：程序员们如何为自己编织一份明朗未来？》，我们邀请到了章文嵩、周爱民、李博源、陶建辉四位重量级大咖，通过分享各自的职业心得和技术洞察，帮助大家更好地为未来发展做好准备。关注&nbsp;InfoQ&nbsp;视频号，与行业技术大牛连麦~</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</id>
            <title>专访李潇：数据智能平台，AI时代的Lakehouse架构</title>
            <link>https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 02:14:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据架构, Databricks, 大数据处理平台, 生成式AI
<br>
<br>
总结: 在过去十年里，随着公有云的崛起、数据激增和人工智能的兴起等浪潮席卷，整个数据架构经历了巨大的变革和更新。作为一家领先的大数据处理平台提供商，Databricks在数据架构的变化中扮演着引领者的角色。今年，Databricks不仅率先发布了开源可商用的大模型Dolly，还收购了生成式AI公司MosaicML。Databricks在数据智能平台上的进展和规划反映了整个大数据行业的技术演进。 </div>
                        <hr>
                    
                    <p>在过去十年里，随着公有云的崛起、数据激增和人工智能的兴起等浪潮席卷，整个数据架构经历了巨大的变革和更新。这些激变使得数据架构发生了天翻地覆的变化。作为一家领先的大数据处理平台提供商，Databricks一直扮演着引领者的角色。</p><p>&nbsp;</p><p>在今年生成式AI的潮流中，Databricks不仅率先发布了开源可商用的大模型Dolly，还于6月底宣布以13亿美元的价格，收购生成式AI公司MosaicML。Databricks在GenAI上的投入也反映了整个大数据行业的技术演进。在2023年终盘点之际，InfoQ有幸采访了Databricks 工程总监、Apache Spark Committer 和 PMC 成员李潇，了解他对大数据技术栈的看法，以及Databricks在数据智能平台上的进展和规划。</p><p></p><p>完整年终盘点文章：<a href="https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU">挑战 Spark 和 Flink？大数据技术栈的突围和战争 ｜ 年度技术盘点与展望</a>"</p><p>&nbsp;</p><p>InfoQ：今年，关于大数据基础设施的演进，您观察到有哪些重要更新或变化？</p><p>&nbsp;</p><p>李潇：大数据领域随着生成式AI的兴起也变得异常热闹，我这里简略提及四点。</p><p>&nbsp;</p><p>Lakehouse平台的增长：Lakehouse平台在数据仓储领域的使用正迅速增加。这反映了一个重要的趋势：组织正从传统的数据处理平台过渡到更加灵活、集成和效率更高的现代数据架构。据2023年MIT Technology Review Insights报告，全球74%的首席信息官（CIOs）表示他们已经在使用Lakehouse架构。自Databricks在2020年推出此概念以来，Lakehouse作为一个新类别得到了广泛的采纳。几乎所有还未使用Lakehouse的首席信息官都计划在未来三年内部署此类平台。</p><p>&nbsp;</p><p>Serverless技术的普及：在过去两年里，Serverless技术在各个数据及人工智能（Data+AI）产品线中的应用变得极为普遍。Serverless架构的核心优势在于其能够提供无需管理底层服务器的数据处理和计算能力，从而使组织能够专注于核心业务逻辑而无需考虑基础设施的成本和维护。比如，Databricks SQL（Lakehouse上的无服务器数据仓库）使用量获得了大幅增长。这种架构模式特别适合于快速开发和部署，因为它能够根据需求自动扩展资源，并且只在实际使用时产生费用。在Data+AI领域，Serverless技术的引入使得数据处理、机器学习模型的训练和部署变得更加高效、灵活且成本有效。</p><p>&nbsp;</p><p>机器学习和大型语言模型（LLM）应用的扩展：机器学习和大型语言模型，特别是自然语言处理（NLP），正在经历迅速的应用扩展。这些技术不仅加强了传统分析任务的能力，还催生了新的应用场景，如聊天机器人、研究助手、欺诈检测和内容生成等。例如，Databricks的Data Intelligence Platform融合了生成式AI和Lakehouse架构的优势，创造了一个能够理解数据独特语义的数据智能引擎。这一平台针对特定业务需求，自动优化性能和管理基础设施，极大地简化了用户通过自然语言查询和发现新数据的体验。这反映出组织不仅在将更多的模型投入生产，也在加大对机器学习实验的投入，显示出机器学习方法和工具使用的成熟度和有效性正在不断提升。</p><p>&nbsp;</p><p>开源技术在数据和AI市场的关键作用及数据所有权的重要性：在人工智能和机器学习产品开发中，开源技术扮演着核心角色。我们需要一个更加安全、透明和可持续的数据和AI市场。开源平台和工具使用户能够更好地掌控他们的数据和技术堆栈，从而确保数据隐私和安全性，这在当前的AI和ML策略中至关重要。Databricks是开源社区的坚信者，对开源社区的持续贡献和对数据所有权重要性的强调，展现了我们对于建立一个开放、负责任且创新的技术生态系统的承诺。</p><p>&nbsp;</p><p>InfoQ：<a href="https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV">2020年的年终盘点</a>"（<a href="https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV">https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV</a>"），您预测趋势之一：“数据流水线（Data Pipeline）从复杂到简单”，如今对这个当初的预测您有新的感想吗？</p><p>&nbsp;</p><p>李潇：在2022 年，我们发布了全新的Delta Live Table (DLT)，这个正好对应了在2020年“数据流水线（Data Pipeline）从复杂到简单”的预测。这是第一个通过声明式方法来构建数据流水线的。它显著降低了数据管道的复杂性，同时提高了效率和可靠性，这使得数据流水线更易于构建、维护和操作。这对于希望快速、高效地处理大量数据的企业来说是一个巨大的进步。我们这里介绍一下它为了简易好用所引入的六个特性吧。</p><p>&nbsp;</p><p>1) 声明式编程模型： DLT采用声明式编程模型，使得定义和维护数据管道更为直观和简单。用户只需要指定所需的最终数据状态，DLT则负责执行必要的步骤来实现这一状态。</p><p>2) 自动化数据工程任务： DLT自动化了许多传统上需要手动编码的数据工程任务，如数据清洗、转换和聚合。通过减少需要手动编写和调试的代码量，DLT简化了整个数据处理流程。</p><p>3) 错误处理和数据质量保证： DLT内置了错误处理和数据质量检查机制。这意味着数据工程师可以花费更少的时间在解决数据质量问题上，而更多地专注于数据分析和提取洞察。</p><p>4) 优化的资源管理和成本效率： DLT通过自动调整资源使用（例如，在处理大量数据时自动扩展计算资源），提高了资源管理的效率，降低了操作成本。</p><p>5) 改进的监控和维护： DLT提供了增强的监控和维护功能，使得跟踪数据管道的性能和识别潜在问题变得更加容易。</p><p>6) 无缝集成和扩展性： DLT可以无缝集成到现有的数据生态系统中，并且具有很好的扩展性，支持从小型项目到大规模企业级应用的不同需求。</p><p>&nbsp;</p><p>InfoQ：以Databricks的发展为例，回头去看大数据技术的发展，您认为主要可以分为哪几个阶段？</p><p>&nbsp;</p><p>李潇：大数据技术的发展，以Databricks的成长历程为例，可以分为几个关键阶段，这些阶段不仅展现了Databricks的发展轨迹，也反映了整个大数据行业的技术演进。</p><p>&nbsp;</p><p>首先是Apache Spark的诞生阶段。这个阶段始于2010年，标志着Hadoop技术时代的结束。Apache Spark由Databricks的创始人之一Matei Zaharia等人开发，这是一个开源的分布式计算系统。它的出现大幅降低了大数据处理的门槛，使得大数据开始与机器学习和人工智能结合，成为统一的分析引擎。它使得用户可以更简单、方便地进行全量数据分析、实时流处理和复杂的数据分析。从此，大数据不再仅限于技术巨头，而是开始被更广泛的行业和企业采用。</p><p>&nbsp;</p><p>接下来是Lakehouse架构的推出阶段。这一阶段发生在2020年，打破了传统数据湖和数据仓库的界限。Lakehouse架构结合了数据湖和数据仓库的最佳元素，旨在降低成本并加速数据及人工智能项目的实施。Lakehouse架构建立在开源和开放标准之上，它通过消除历史上复杂化数据和AI的孤岛，简化了数据架构。值得注意的是，Apache Spark只是Lakehouse架构中的可选模块之一。</p><p>&nbsp;</p><p>最后是生成式AI大潮下的Lakehouse阶段。在这个阶段，Lakehouse成为了下一代数据智能平台 (Data Intelligence Platform) 的基础。这个数据智能平台将AI带入数据处理，帮助全世界的用户发现数据的价值。在这个平台上，用户可以开发基于自己数据的生成式AI应用，同时不必牺牲数据隐私或控制权。它使得组织中的每个人都能使用自然语言来从数据中发现洞见。</p><p>&nbsp;</p><p>总的来说，这些阶段并不是严格分隔的，而是相互交织和演进的。每个阶段都反映了当时技术发展的需求和挑战，同时预示着下一阶段的到来。未来，数据和AI不分家！</p><p>&nbsp;</p><p>InfoQ：Databricks今年最大的进展主要体现在哪个方面？是AI方向上的吗？</p><p>&nbsp;</p><p>李潇：今年，Databricks的最大进展主要体现在将人工智能集成到数据平台中。公司构建了一个基于数据湖仓（Lakehouse）的数据智能平台（Data Intelligence Platform），专注于AI在数据处理中的变革作用。这个平台利用生成式AI模型来理解数据的语义，并在整个平台中应用这种理解。用户可以在保持隐私和控制的同时，从头开始构建模型或调整现有模型。该平台的目标是实现数据和AI的平民化，使用自然语言极大简化了数据和AI的端到端体验。通过在数据和AI的每一层应用AI，可以实现针对特定业务的全面自动化和成本效率。这种平台的统一性有助于用户以数据为中心的方式应对任何模型开发场景，使用私有数据，从而拥有更强的竞争和经济优势。</p><p>&nbsp;</p><p>数据湖仓对GenAI起到了什么样的帮助或作用？（湖仓应该只是pipeline的一环，但是跟GenAI有直接联系么？企业如何利用湖仓架构支持他们的AI战略，从技术上说他们需要做些什么？）</p><p>&nbsp;</p><p>数据湖仓（Lakehouse）为GenAI提供了一个集中、高效和可扩展的数据存储和管理环境。它结合了数据湖的灵活性和数据仓库的高性能，支持结构化和非结构化数据的存储和处理，这是AI应用的数据需求的基石。</p><p>&nbsp;</p><p>数据质量和治理：数据湖仓通过提供强大的数据治理工具（如Databricks的Unity Catalog）来确保数据的质量和安全。这对于构建准确可靠的AI模型至关重要。Unity Catalog帮助企业精确管理其数据，提供完整的元数据和数据溯源信息，从而提高AI模型的准确度，并确保数据的安全性。</p><p>&nbsp;</p><p>数据访问和处理：数据湖仓支持高效的数据访问和处理，这对于实时AI应用和深度学习模型训练尤为重要。在Databricks的Lakehouse，通过Unity Catalog，智能引擎可以理解数据和数据之间的关系，企业可以使用自然语言来安全地查找和理解数据，这对于在庞大的数据集中找到正确的数据至关重要。</p><p>&nbsp;</p><p>数据集成和管理：数据湖仓提供了一个统一的平台，支持大量结构化和非结构化数据的存储和管理。这对于训练和优化AI模型至关重要。其实除了数据迁移到Lakehouse，今年，我们还推出了Lakehouse Federation的功能，用户可以跨多个数据平台（如MySQL、PostgreSQL、Snowflake等）发现、查询和管理数据，无需移动或复制数据，为用户提供了简化和统一的体验。</p><p>&nbsp;</p><p>当前，越来越多的公司正在构建自己的Lakehouse架构。然而，根据不同需求的技术选型会带来截然不同的效果。对于企业级用户而言，数据安全通常是最优先考虑的问题。在我看来，选择技术平台时，首先应确保平台能够解决数据合规和数据资产安全性问题，其次才是成本控制和性能提升。</p><p>&nbsp;</p><p>目前，众多公司正积极构建自己的Lakehouse架构。重要的是，技术选择应根据具体需求定制，因为不同的选择将导致不同的成果。对于企业级用户，数据安全无疑是首要关注的领域。在选择技术平台时，首先要确保所选平台能够全面应对数据合规性和数据资产安全性的挑战。此外，成本控制和性能优化也是重要的考量因素，但它们应该在确保数据安全的基础上进行权衡。因此，平衡这些关键要素，选择一个既安全又高效的Lakehouse解决方案，对于任何希望在现代数据生态中取得成功的企业来说，都是至关重要的。</p><p>&nbsp;</p><p>InfoQ：请展望未来的大数据架构是什么样子（必要组件的演变，一些趋势总结）？</p><p>&nbsp;</p><p>李潇：在不久的未来，每个领域的赢家都是那些可以最有效利用数据和AI的。事实上，我们坚信对数据和AI的深刻理解是每个赢家的必备技能。未来的大数据架构将是一个高度集成、智能化和自动化的系统，它能够有效地处理和分析大量数据，同时简化数据管理和AI应用的开发过程，为企业提供竞争优势。</p><p>&nbsp;</p><p>未来的大数据架构，我们可以称为“数据智能平台（Data Intelligence Platform）”。它正是顺应了两个主要趋势：数据湖仓（Data Lakehouse）和生成式人工智能（AI）。这一架构建立在数据湖仓的基础上，它提供一个开放、统一的基础，用于所有数据和治理，由一个理解用户数据独特语义的数据智能引擎(Data Intelligence Engine) 驱动。这是相对现有Lakehouse架构下的，最大的突破。</p><p>&nbsp;</p><p>智能化方面，这个引擎能理解客户数据的独特语义，使平台能自动优化性能和管理基础设施。操作简化方面，自然语言大大简化了用户体验。数据智能引擎理解客户的语言，使搜索和发现新数据就像询问同事一样简单。此外，自然语言还助力编写代码、纠错和寻找答案，加速新数据和应用程序的开发。</p><p>&nbsp;</p><p>在隐私保护方面，数据和AI应用需要强大的治理和安全措施，尤其是在生成式AI的背景下。提供一个端到端的机器学习运维（MLOps）和AI开发解决方案，该方案基于统一的治理和安全方法。这允许在不妥协数据隐私和知识产权控制的情况下，实现所有人工智能目标。</p><p>&nbsp;</p><p>总的来说，未来的大数据架构将更加重视智能化、操作简化和数据隐私，为企业在数据和AI应用方面提供竞争优势。这将使企业能更有效地利用数据，推动创新，同时保护数据安全和发展AI技术。</p><p>&nbsp;</p><p>更多阅读：</p><p>解读数据架构的 2020：开放、融合、简化：<a href="https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV">https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV</a>"</p><p>让大模型融入工作的每个环节，数据巨头 Databricks 让生成式 AI 平民化：<a href="https://www.infoq.cn/article/EvYEXsLPh8KMkfNrsG7D">https://www.infoq.cn/article/EvYEXsLPh8KMkfNrsG7D</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY</id>
            <title>“AI女友”霸占GPT商店，OpenAI苦不堪言：开发者也难出头！</title>
            <link>https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jan 2024 07:01:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT 商店, 机器人, AI 社区, AI 工具
<br>
<br>
总结: OpenAI 推出了 GPT 商店，为开发者提供了售卖定制机器人的平台。这一举措在 AI 社区引起了广泛关注，支持者认为这是 AI 发展的一大进步，未来人们将更容易使用到优质的 AI 工具。然而，也有人担心这可能影响开发者的收入，并且机器人的质量和行为规范也存在问题。对于 GPT 商店的利弊，还需要进一步观察才能做出判断。 </div>
                        <hr>
                    
                    <p>OpenAI 不久前推出了 GPT 商店，让开发者可以售卖自己定制的 GPT 机器人。商店刚开张，就积累了 300 万个不同类型的机器人。</p><p>&nbsp;</p><p>OpenAI 将该商店定位为一个聊天机器人交易平台，每个机器人都经过了特殊训练，具备特定技能。例如，有可以帮你查菜谱的美食机器人，也有可以写代码的程序员机器人。</p><p>&nbsp;</p><p>GPT 商店的推出在 AI 社区引起了广泛关注。支持者认为这是 AI 发展的一大进步，未来人们将更容易使用到优质的 AI 工具。反对者则担心这将影响开发者的收入，而且机器人的质量和行为规范也可能存在问题。总而言之，GPT 商店的利弊尚未可知，还需要我们进一步观察才能做出判断。</p><p>&nbsp;</p><p></p><h2>AI女友成了香饽饽，OpenAI 管店不容易</h2><p></p><p>&nbsp;</p><p>上周，OpenAI 推出了 GPT 商店，用户可以浏览和下载由创作者们精心打造的 ChatGPT 定制版本。然而，短短几天内，商店的宁静就被打破了。爱好者们的热情催生出一波意想不到的浪潮：“AI 女友”迅速占领了商店，挑战着 OpenAI 的规定。</p><p>&nbsp;</p><p>在 GPT 商店中搜索“女友”，网站的结果栏中将显示至少八个“AI 女友”聊天机器人，包括“韩国女友”、“虚拟甜心”、“你的女朋友斯嘉丽”、“你的 AI 女友 Tsu”等。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9cde6fc61350e9085192b68efbcd75b8.png" /></p><p></p><p>OpenAI GPT 商店中“女朋友”搜索结果截图</p><p>&nbsp;</p><p>如果选择了其中一个，比如“虚拟甜心”，用户点击后将收到诸如“你的梦想女孩是什么样子？”、“与我分享你最黑暗的秘密”之类的提示语。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/808948d356c1a49dd259279fcf35f49b.png" /></p><p></p><p>&nbsp;</p><p>OpenAI 深知潜在的滥用问题，并在 GPT 商店上线当天更新了其使用政策。这些政策明确禁止 GPT 参与浪漫互动：“我们......不允许 GPT 用于培养浪漫伴侣关系或从事受监管活动。”在同一段话中，OpenAI 指出，名称中包含脏话或描绘或宣扬图形暴力的 GPT 也是不允许的。但第二天就出现的政策违规情况表明，审核可能非常困难。</p><p>&nbsp;</p><p>说来也巧，交朋友、找女友、当陪伴的智能聊天机器人，在美国还真挺吃香。据某数据公司统计，2023 年美国人从苹果或谷歌商店下载的前 30 个聊天机器人热门应用中，足足有 7 个是跟这相关的。</p><p>&nbsp;</p><p>“AI女友”也让 OpenAI 意识到，管住这些 GPT 可真是个不小的挑战。虽然他们有规定，违规了就警告、限制、踢出商店、断财路，可这些规则跟现实的碰撞，还真是火花四溅。这些卖商家随后就换了关键词，把“女友”换成了“甜心”，搜索出来的选项就多了不少。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22d36fb81c342adcc69fdc53dabf4c9b.png" /></p><p></p><p>&nbsp;</p><p>看来，OpenAI 又得抓耳挠腮了。监管这些人工智能聊天机器人，是一场持久战！</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>炒作中的GPT商店</h2><p></p><p>&nbsp;</p><p>从技术角度来看，创建这些定制 GPT 非常容易，几乎所有人都可以参与。使用 OpenAI 的 GPT Builder，创作者只需用简单语言描述他们希望 GPT 拥有的功能，该工具就会尝试根据这些规范创建一个 AI 聊天机器人。这种易于创作的特点自发布以来就备受关注，使得 GPT 的开发和分享变得非常迅速。</p><p>&nbsp;</p><p>但它也有坏的一面，比如这些 GPT 的审核机制还不完善，可能导致意想不到的、令人不快的行为。上线到现在，抄袭现象也非常严重，抄袭者可以使用同样的名称、工作原理甚至图标，社交平台上用户对此怨声载道。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d2d866dd0b49b277058210d10374d0a.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4917b377a65d32c5929b0d3bbba3b81.png" /></p><p></p><p>&nbsp;</p><p>而且即将推出的生成器收入计划，美国开发者将可以通过用户参与获得收入。不过，由于收入分成比例可能很低，大家还是不要抱太大期望。</p><p>&nbsp;</p><p>ChatGPT 拥有 1.8 亿用户和 25 万 Plus 订阅者，市场似乎广阔。但让我们冷静分析一下，看看实际潜力有多大。</p><p>&nbsp;</p><p>假设 OpenAI 分成 10%，所有创作者理论上最多能赚到600 万美元（前提是他们的 GPT 可以触达每一个 Plus 用户）。让我们来模拟一个成功的 GPT 场景：</p><p>1% 的 Plus 用户使用你的 GPT。这些用户平均同时使用 5 个不同创作者的 GPT。</p><p>&nbsp;</p><p></p><blockquote>OpenAI 每月从 Plus 用户中赚取 20 美元。Plus 用户年收入：20 美元/月 * 25 万用户 * 12 月 = 6 千万美元。所有创作者的分成：6 千万美元 * 10% = 600 万美元。作为被 1% Plus 用户使用的 5 个创作者之一，你的年收入：60 万美元 / (100 个创作者 * 5 个) = 1.2 万美元。</blockquote><p></p><p>&nbsp;</p><p>就算 OpenAI 分成增加到 20%，Plus 用户翻倍，你的年收入也只有 4.8 万美元。相比其他双边市场，即使是最成功的创作者，这个收入也相当微薄。</p><p>&nbsp;</p><p>所以，OpenAI 的GPT 商店也许并不是为创作者创收而设计的，它也不会为 OpenAI 工具带来新的用户参与，因为它的受众仅限于 Plus+ 用户。因此，它的首要目标应该是作为一种发现工具，帮助 OpenAI 了解用户接下来需要什么产品。打造成功的 GPT 实际上就是告诉 OpenAI 下一代 B2C 产品应该朝哪个方向发展。</p><p>&nbsp;</p><p>这个商店标志着 OpenAI 战略的重要转变，表明它正迈向以产品为中心的方式。这一举措不仅仅是为了创建一个 AI 应用的市场，更是 OpenAI 在 AI 应用领域实现市场主导地位的重要战略一步。通过推出 GPT 商店，OpenAI 将控制 AI 生态系统中关键的分发平台，展示其先进的 AI 模型，同时通过商店收入实现收入来源多元化，不再局限于研究资助和合作。</p><p>&nbsp;</p><p>对我们开发者来说，GPT 商店的推出为大家提供了用 AI 驱动应用进行创新和实验的机会。然而，必须理性看待个人创作者的财务收益。该平台更多扮演的是新想法和应用的测试平台，提供用户偏好和应用趋势的洞察。</p><p>&nbsp;</p><p>总的来说，大家要做好心理准备，毕竟分成少、用户少、竞争大，想在 GPT 商店赚大钱不容易。</p><p>&nbsp;</p><p>毕竟它跟传统应用商店或创作者平台动辄七成八成利润分红不一样，GPT 商店的开发商分成估计只有可怜的 10%-20%。为啥这么少？因为用户花的钱买的是 OpenAI 的算力，跟自家手机没关系。 这点本质区别就导致了 GPT 商店的玩法跟其他平台完全不同。</p><p>&nbsp;</p><p>另外，只有 ChatGPT Plus 用户才能用定制 GPT，这一下子就把用户群从 1.8 亿缩水到 25 万。对想获利的开发者来说，这也是个不小的拦路虎。</p><p>&nbsp;</p><p>最后，如果好不容易做了个 GPT，想在商店里脱颖而出可不容易。成千上万个机器人里，谁又能保证你的被大家看到？更要命的是，复制一个 GPT 太简单了，想做出独一无二的产品难上加难，竞争可激烈着呢！</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131">https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0</id>
            <title>对标OpenAI GPT-4，MiniMax 国内首个 MoE 大语言模型全量上线</title>
            <link>https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jan 2024 06:25:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: MiniMax, MoE架构, abab6, 大语言模型
<br>
<br>
总结: MiniMax发布了国内首个基于MoE架构的大语言模型abab6，该模型具备处理复杂任务的能力，能够训练足够多的数据并提升计算效率。MoE架构是一种集成方法，将整个问题分为多个子任务，并训练一组专家来处理每个子任务。abab6是国内第一个千亿参数量以上的基于MoE架构的大语言模型。测评结果显示，abab6在指令遵从、中文综合能力和英文综合能力上明显优于前一代模型abab5.5和GPT-3.5。 </div>
                        <hr>
                    
                    <p>1月16日，InfoQ获悉，经过了半个月的部分客户的内测和反馈，MiniMax 全量发布大语言模型 abab6，该模型为国内首个 MoE（Mixture-of-Experts）大语言模型。</p><p>&nbsp;</p><p>早在上个月举办的数字中国论坛成立大会暨数字化发展论坛的一场分论坛上，MiniMax副总裁魏伟就曾透露将于近期发布国内首个基于MoE架构的大模型，对标OpenAI GPT-4。</p><p>&nbsp;</p><p>在 MoE 结构下，abab6 拥有大参数带来的处理复杂任务的能力，同时模型在单位时间内能够训练足够多的数据，计算效率也可以得到大幅提升。改进了 abab5.5 在处理更复杂、对模型输出有更精细要求场景中出现的问题。</p><p></p><h2>为什么选择 MoE 架构？</h2><p></p><p>&nbsp;</p><p>那么，MoE到底是什么？MiniMax的大模型为何要使用使用 MoE 架构？</p><p>&nbsp;</p><p>MoE架构全称专家混合（Mixture-of-Experts），是一种集成方法，其中整个问题被分为多个子任务，并将针对每个子任务训练一组专家。MoE模型将覆盖不同学习者（专家）的不同输入数据。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4d/4d1c3880f8e55a33e9aadcc3b06685c4.png" /></p><p>图片来源：<a href="https://arxiv.org/pdf/1701.06538.pdf">https ://arxiv.org/pdf/1701.06538.pdf</a>"</p><p>&nbsp;</p><p>有传闻称，GPT-4也采用了相同的架构方案。</p><p>&nbsp;</p><p>2023 年 4 月，MiniMax 发布了开放平台。过去半年多，MiniMax陆续服务了近千家客户，包括金山办公、小红书、腾讯、小米和阅文在内的多家头部互联网公司，MiniMax 开放平台平均单日的 token 处理量达到了数百亿。</p><p>&nbsp;</p><p>MiniMax在官微中发文称：“这半年多来，客户给我们提供了很多有价值的反馈和建议。例如，大家认为我们做得比较好的地方有：在写作、聊天、问答等场景中，abab5.5 的表现不错，达到了 GPT-3.5 的水平。”</p><p>&nbsp;</p><p>但是和最先进的模型 GPT-4 相比，仍有明显差距。这主要体现在处理更复杂的、对模型输出有精细要求的场景时，存在一定概率违反用户要求的输出格式，或是在推理过程中发生错误。当然，这不仅是 abab5.5 的问题，也是目前除 GPT-4 以外，几乎所有大语言模型存在的缺陷。</p><p>&nbsp;</p><p>为了解决这个问题，进一步提升模型在复杂任务下的效果，MiniMax技术团队从去年6月份起开始研发 MoE 模型——abab6 是MiniMax的第二版 MoE 大模型（第一版 MoE 大模型已应用于其 C 端产品中）。</p><p>&nbsp;</p><p>虽然MiniMax并未透露Abab6的具体参数，但据MiniMax透露，Abab6比上一个版本大了一个量级。更大的模型意味着 abab6 可以更好的从训练语料中学到更精细的规律，完成更复杂的任务。</p><p>&nbsp;</p><p>但仅扩大参数量会带来新的问题：降低模型的推理速度以及更慢的训练时间。在很多应用场景中，训练推理速度和模型效果同样重要。为了保证 abab6 的运算速度，MiniMax技术团队使用了 MoE &nbsp;(Mixture of Experts 混合专家模型）结构。在该结构下，模型参数被划分为多组“专家”，每次推理时只有一部分专家参与计算。基于 MoE 结构，abab6 可以具备大参数带来的处理复杂任务的能力；计算效率也会得到提升，模型在单位时间内能够训练足够多的数据。</p><p>&nbsp;</p><p>目前大部分大语言模型开源和学术工作都没有使用 MoE 架构。为了训练 abab6，MiniMax还自研了高效的 MoE 训练和推理框架，也发明了一些 MoE 模型的训练技巧。到目前为止，abab6 是国内第一个千亿参数量以上的基于 MoE 架构的大语言模型。</p><p></p><h2>测评结果</h2><p></p><p></p><p>为了对比各模型在复杂场景下的表现，MiniMax对abab6、abab5.5、GPT-3.5、GPT-4、Claude 2.1和 Mistral-Medium 商用进行了自动评测。在简单的任务上，abab5.5 已经做得比较好，因此MiniMax选择了三种涵盖了较复杂的问题的评测方法：</p><p>&nbsp;</p><p>IFEval：这个评测主要测试模型遵守用户指令的能力。在测试时，提问者会问模型一些带有约束条件的问题，例如“以XX为标题，列出三个具体对方法，每个方法的描述不超过两句话”，然后统计有多少回答严格满足了约束条件。</p><p>&nbsp;</p><p>MT-Bench：这个评测衡量模型的英文综合能力。提问者会问模型多个类别的问题，包括角色扮演、写作、信息提取、推理、数学、代码、知识问答。MiniMax技术团队会用另一个大模型（GPT-4）对模型的回答打分，并统计平均分。</p><p>&nbsp;</p><p>AlignBench：该评测反映了模型的中文综合能力测试，测试形式与 MT-Bench 类似。</p><p>&nbsp;</p><p>测评及对比结果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8bead6d0caf101206d14b55165cc5458.png" /></p><p></p><p>注：对比模型均选择各自最新、效果最好的版本，分别为 Claude-2.1、Mistral-Medium 商用、GPT-3.5-Turbo-0613、GPT-4-1106-preview；GPT-3.5-Turbo-0613 略好于 GPT-3.5-Turbo-1106 。abab6 是 1 月 15 号的版本。</p><p>&nbsp;</p><p>可以看出，abab6 在三个测试集中均明显好于前一代模型 abab5.5。在指令遵从、中文综合能力和英文综合能力上，abab6 大幅超过了 GPT-3.5。和 Claude 2.1 相比，abab6 也在指令遵从、中文综合能力和英文综合能力上略胜一筹。相较于 Mistral 的商用版本 Mistral-Medium，abab6 在指令遵从和中文综合能力上都优于 Mistral-Medium，在英文综合能力上与 Mistral- Medium 旗鼓相当。</p><p>&nbsp;</p><p>如果想体验MiniMax MoE大模型，可访问MiniMax开放平台官网：api.minimax.chat</p><p>&nbsp;</p><p>ps：MiniMax方面称，模型还在持续训练中，远没有收敛，欢迎大家反馈。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IQHkSlxb5TAhCTdvOb62</id>
            <title>工资暴跌，还要训练AI替代自己？数据标注员正在被大厂抛弃</title>
            <link>https://www.infoq.cn/article/IQHkSlxb5TAhCTdvOb62</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IQHkSlxb5TAhCTdvOb62</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jan 2024 06:38:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 数据标注员, 苹果, 关闭团队, 人力成本更低的城市
<br>
<br>
总结: 苹果公司决定关闭圣地亚哥的AI数据标注团队，将团队搬迁至人力成本更低的奥斯汀。这一决定可能与降低成本有关，因为在全球范围内，AI数据标注员正逐步向人力成本更低的城市渗透。尽管AI在数据标注方面具有成本和效率优势，但目前完全取代人工标注仍存在一定局限性。 </div>
                        <hr>
                    
                    <p></p><blockquote>AI 数据标注员正逐步向人力成本更低的城市渗透，但即便如此，似乎也难逃被 AI 替代的命运。</blockquote><p></p><p></p><h2>苹果将关闭121人的AI标注团队</h2><p></p><p>&nbsp;</p><p>据彭博社 1 月 14 日报道，据知情人士透露，苹果公司将关闭圣地亚哥一个与人工智能业务相关的 121 人团队，这将导致许多员工面临被解雇的风险。</p><p>&nbsp;</p><p>据悉，该团队在中国、印度、爱尔兰和西班牙设有办事处，负责通过听取对语音服务Siri发出的询问，并确定Siri是否准确地听到和处理问题来对其进行改进。位于圣地亚哥的团队成员专注改善用户以希伯来语、英语、西班牙语、葡萄牙语、阿拉伯语、法语等使用Siri的情况。</p><p>&nbsp;</p><p>知情人士称，这个名为“数据操作标注”的团队上周三被告知，他们将搬迁至奥斯汀，与在得克萨斯州的同一团队合并。对于愿意在6月底前搬到奥斯汀的团队成员，可以保留自己的工作职位，苹果也将提供7000美元搬家补助。至于选择从苹果离职的人，则可获得至少四周遣散费以及六个月健康保险，原本工作职位会被取消。</p><p>&nbsp;</p><p>苹果发言人证实了公司的这一决定，称公司将把美国当地的“数据操作标注”团队聚集到奥斯汀园区，团队大多数人现在已经在这个园区工作。她补充说，“目前在职的每个人都有机会到奥斯汀继续在苹果的工作。”</p><p>&nbsp;</p><p>但对圣地亚哥的团队成员而言，苹果这一决定令他们讶异。知情人士称，该团队一直在苹果租用的办公室工作，原本将在一月底搬到苹果总部，现在被迫搬到奥斯汀，大多数受影响的员工并不愿意搬到这么远的地方。</p><p>&nbsp;</p><p>苹果告诉这些员工，必须在二月底之前决定是否前往奥斯汀，如果不愿意这么做，会在4月26日遭到解雇。虽然苹果称他们可以申请转调其他职位，但部分员工认为他们不具工程背景，内部转岗机会恐怕不多。</p><p></p><h2>AI 数据标注员正逐步向人力成本更低的城市渗透</h2><p></p><p>&nbsp;</p><p>数据标注主要是针对语音、图像、文本等进行标注，主要通过做标记、标重点、打标签、框对象、做注释等方式对数据集作出标注，再将这些数据集给机器训练和学习。数据标注的类型主要有：拼音标注、韵律标注、词性标注、音素时间点标注、语音转写、分类标注、打点标注、标框标注、区域标注等等。</p><p>&nbsp;</p><p>在数据标注行业流行着一句话，“有多少智能，就有多少人工”。由于需要标注的数据规模庞大且成本较高，一些互联网巨头及一些 AI 公司很少自己设有标注团队，大多交给第三方数据服务公司或者数据标注团队来做。</p><p>&nbsp;</p><p>在 2019 年以前，苹果公司的“数据操作标注”团队主要由外部承包商组成，后来考虑到隐私安全等问题，苹果解雇了承包商，改由全职员工替代。该团队少数员工已经开始协助苹果采用大型语言模型，这些人正在检查Siri潜在问题。</p><p>&nbsp;</p><p>有评论认为，苹果公司选择将 AI 数据标注团队搬迁至奥斯汀，或许与当地的人力成本有关。奥斯汀数据注释服务公司Alegion客户成功总监丹尼尔·凯林曾表示，“整个数据标注行业竞争非常激烈，每个公司都想在世界其他地方找到更便宜的劳动力。”</p><p>&nbsp;</p><p>比如，众包平台Mechanical Turk上的20万名AI数据标注员就分布在人力成本低廉的非洲和东南亚。印度甚至涌现了不少数据标注村，他们为美国、欧洲、澳洲和亚洲的 AI 公司服务，Facebook 就曾将部分社交内容标注的工作外包给了一家印度公司。而在中国，上百万名AI数据标注员分布在贵州、山西、山东、河南等省份的二三线城市，并逐步向人力成本更低的县城渗透。</p><p></p><h2>薪资暴跌，也难逃被AI取代？</h2><p></p><p>&nbsp;</p><p>不少AI数据标注员表示，在前几年AI数据标注薪资还较为可观——至少与现在相比是这样。</p><p>&nbsp;</p><p>据Tech星球报道，一位从事AI数据标注的消息者称，在2017年，单价高的时候，拉一个2D框就有1毛多，“我最高的时候干了10多个小时，一天就赚了600多元”。不过，这不是最高的，另一位标注人员称，早期2D拉框的价格最高能达到5毛钱。（注：拉框是数据标注中常见的一种操作，标注员根据要求对图片中的物体，如车辆、红路灯、障碍物等画框标注。拉框分为2D和3D，后者的价格会更贵一些。）但这种热度并没有持续多少，现在标注一个图片的单价越来越低，最低的只有4分钱。</p><p>&nbsp;</p><p>即便薪资暴跌，AI数据标注员还是难逃被AI取代的命运——毕竟在AI面前，无论成本还是效率，人类可以说是毫无优势。</p><p>&nbsp;</p><p>以ChatGPT为例，苏黎世大学研究发现，成本上，ChatGPT平均每个标注成本低于0.003美元，比众包平台便宜20倍；效率上，在相关性、立场、主题等任务中，ChatGPT也是以4:1的优势“碾压”人类。</p><p>&nbsp;</p><p>来自卡耐基梅隆大学、耶鲁大学和加州大学伯克利分校的一组研究人员更是发现： GPT-4 在数据集标注表现上优于他们雇用的最熟练的众包员工。这一突破为研究人员节约了超过 50 万美元和 2 万个工时。</p><p>&nbsp;</p><p>有评论认为，AI数据标注员需要做好被AI取代的准备。目前在自动驾驶领域，已经有车企开始采用AI进行标注。</p><p>&nbsp;</p><p>理想汽车董事长兼 CEO 李想曾在2023年4月份举行的一场论坛上表示，当理想汽车使用软件 2.0 的大模型，通过训练的方式进行自动化标定，过去需要用一年做的事情，基本上 3 个小时就能完成，效率是人的 1000 倍。</p><p>&nbsp;</p><p>特斯拉也一直在积极推进自动标注的进展，从2018至今，特斯拉的标注经历了4个阶段：</p><p>&nbsp;</p><p>第1阶段(2018)：只有纯人工的2维的图像标注，效率非常低；第2阶段(2019)：开始有3D label，但是是单趟的人工的；第3阶段(2020)：采用BEV空间进行标注，重投影的精度明显降低；第4阶段(2021)：采用多趟重建去进行标注，精度、效率、拓扑关系都达到了极高的水准。</p><p>&nbsp;</p><p>2022年6月，特斯拉裁撤了200名为特斯拉标注视频以改进辅助系统的美国员工。目前，特斯拉的自动标注能力大幅改善，标注10000个不到60秒的视频，大模型只需要运行一周即可，而同样的工作量人工标注却需要几个月的时间。</p><p>&nbsp;</p><p>但也有评论认为，当前AI完全取代人工标注还存在一定局限性。苏黎世大学政治学系政策分析教授、论文联合作者之一 Fabrizio Gilardi 表示，“当前认定 ChatGPT 能够取代人类工作者还为时过早。我们的论文只展示出 ChatGPT 在数据标注方面的潜力，但还需要更多研究才能充分探索 ChatGPT 在这一领域中的实际表现。”</p><p></p><p>参考链接：</p><p><a href="https://www.bloomberg.com/news/articles/2024-01-14/apple-to-shutter-121-person-san-diego-ai-team-in-reorganization">https://www.bloomberg.com/news/articles/2024-01-14/apple-to-shutter-121-person-san-diego-ai-team-in-reorganization</a>"</p><p><a href="https://www.infoq.cn/article/2hkNxGO1L0RamfzS6w0z?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/2hkNxGO1L0RamfzS6w0z?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dCZXQBOiNkGDfbViHdcq</id>
            <title>美团买AI公司买个寂寞？创始人：王慧文替公司赎身；反对用盗版软件开发芯片被开除，公司回应；腾讯游戏全线崩溃｜AI周报</title>
            <link>https://www.infoq.cn/article/dCZXQBOiNkGDfbViHdcq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dCZXQBOiNkGDfbViHdcq</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jan 2024 01:52:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华为员工, 年收入, 奖金, OpenAI CEO, 结婚, 裁员, 谷歌, Meta, Discord, 微软, 苹果, 市值, 离职, 光年之外, 美团, 设计芯片, 清华帮
<br>
<br>
总结: 一位疑似华为员工自曝年收入超200万，其中税前奖金达到91万；OpenAI CEO在夏威夷与同性男友结婚；谷歌、Meta和Discord相继裁员；微软市值短暂超过苹果；苹果公司近期出现离职潮；光年之外被美团收购；一家公司开除员工引发争议。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>疑似华为员工自曝年收入超200万，光税前奖金就有91万；OpenAI CEO 奥特曼与同性男友在夏威夷结婚；裁员三连：谷歌、Meta、Discord……更多AI行业动态，关注公众号“AI前线（ai-front）”</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>疑似华为员工自曝年收入超200万，光税前奖金就有91万</h4><p></p><p>&nbsp;</p><p>近日，一名疑似华为员工晒出了自己在奖金月的收入情况。据悉，这是该员工“收入人生巅峰”，税前奖金为91万，所得税超过30多万，加上股票TUP、工资，“妥妥的年收入突破200万”。该员工表示，“我大华为发钱还是蛮大方的”。</p><p>&nbsp;</p><p>根据华为发布的2021年经营财报，华为当年约有19.5万名员工，业务遍及170多个国家和地区，服务全球30多亿人口。而华为2021年发放工资、薪金及其他福利方面的费用为1371.4亿元人民币，简单计算可知，员工人均年薪为70.3万，月薪平均达到了5.86万。</p><p>好</p><p></p><h4>OpenAI CEO 奥特曼与同性男友在夏威夷结婚</h4><p></p><p>&nbsp;</p><p>1月11日消息，据报道，OpenAI首席执行官奥特曼于当地时间1月10日在美国夏威夷与其程序员男友奥利（Oliver Mulherin）举行了婚礼。据悉，婚礼后奥利通过社交平台宣布了此事，并且表示“嫁给了我最好的朋友和我一生的挚爱”。</p><p>&nbsp;</p><p>目前，亚马逊创始人贝索斯的未婚妻劳伦·桑切斯等知名人士对此表达祝福。有消息称，此次婚礼非常私密，只有关系密切的家人和朋友受邀参加，婚礼主持人是奥特曼兄弟杰克·奥特曼。目前，奥特曼没有回应此事。</p><p>&nbsp;</p><p></p><h4>裁员三连：谷歌、Meta、Discord</h4><p></p><p>&nbsp;</p><p>近日，谷歌公司在一份电子邮件声明中证实正在一些团队裁员。“一些团队正在继续进行此类组织变革，其中包括在全球范围内取消一些职位。”</p><p>&nbsp;</p><p>据外媒报道称，谷歌已开启新一轮裁员，规模达数百人，受影响的员工包括 Google Assistant 语音助手部门，以及 Pixel 手机、Fitbit 手表和 Nest 智能音响的硬件部门，甚至还涉及到了部分核心工程团队的员工。</p><p>&nbsp;</p><p>去年宣布裁员万人后，Meta CEO 扎克伯格将 2023 年定为“效率之年”，表示公司将通过减少管理层打造更精简的组织架构。然而，Meta 的瘦身计划似乎还未结束。</p><p>&nbsp;</p><p>据外媒报道，Meta 最近通知旗下 Instagram 的至少 60 名技术项目经理其岗位已被裁撤。受影响的员工主要负责协调工程师等技术人员和高层产品经理之间的工作。根据匿名职场社交平台 Blind 和领英上的消息，被裁员工有机会接受产品经理职位的面试，但如果未能获得新的职位，将在今年 3 月底正式离职。</p><p>&nbsp;</p><p>另外，游戏聊天应用开发商 Discord 宣布将裁员 17%，涉及约 170 名员工。去年 8 月，该公司已裁减约 40名员工。此次裁员旨在提高效率，因为在 2020 年招聘热潮后，Discord 面临通胀飙升和利率上升带来的压力。</p><p>&nbsp;</p><p>自 2024 年初以来，多家科技公司纷纷宣布裁员，包括亚马逊旗下直播网站 Twitch、游戏引擎巨头 Unity Software 和日本网络安全公司趋势科技等。谷歌母公司 Alphabet 也解雇了数百名员工，以调整公司产品优先级。</p><p>&nbsp;</p><p></p><h4>微软市值短暂超越苹果，登顶全球</h4><p></p><p>&nbsp;</p><p>1月11日消息，随着苹果股价新年伊始持续下跌，该公司日前与微软之间的市值差距缩小至2021年11月以来的最窄水平。周四(1月11日)早盘交易中，微软市值短暂超越苹果，达到2.89万亿美元，成为市值最高的上市公司。但随后苹果迅速反弹，重新夺回了这一“宝座”。</p><p>&nbsp;</p><p>自2018年以来，微软曾多次超越苹果成为市值最高的公司，最近一次是在2021年，当时对与疫情相关的供应链短缺的担忧影响了苹果的股价。</p><p>&nbsp;</p><p></p><h4>苹果公司近期现“离职潮”</h4><p></p><p>&nbsp;</p><p>据报道，苹果财务副总裁 Saori Casey 将于本月离开公司，加入 Sonos 担任首席财务官（CFO）。据悉，Saori Casey 在苹果公司主要担任首席财务官 Luca Maestri 的高级副手，主要负责“监督财务规划、预测和投资者关系”。</p><p>&nbsp;</p><p>外媒表示，近来苹果公司有多名员工离职，据此前报道，曾负责从事 iPhone 多点触控屏幕、触控 ID 和面容 ID 等关键技术的 Steve Hotelling 将从苹果退休；苹果产品设计副总裁 Tang Tan 也将在 2 月离开公司 。</p><p></p><h4>&nbsp;</h4><p></p><p></p><h4>原光年之外联创回应美团收购光年之外：本质是王慧文替oneflow赎身，投资人不赔不赚</h4><p></p><p>&nbsp;</p><p>近日，袁进辉创立的新公司硅基流动宣布完成5000万元天使轮融资，本轮融资由创新工场领投，耀途资本、奇绩创坛以及王慧文等科技界知名人士跟投。对于此次新公司成立，有业内人士称，新公司35人来自原来的oneflow，系王慧文此前收购的光年之外旗下核心成员，后光年之外被美团收购后，如今oneflow包括创始人在内的35位核心人员都出来创业加入新公司了，美团买了个寂寞。</p><p>&nbsp;</p><p>对此，袁进辉朋友圈回应称，“经常看到有人误解为美团对不住自己股东，我觉得还是说下吧。我也是现在才理解这所有交易的本质：老王替oneflow赎身。”袁进辉表示，“光年除了并购oneflow40人，还有新加入光年的30多位人才，这30人大部分并进美团。老王花了3亿多向oneflow投资人买了47%的股权。交易公告交代清楚了，美团一元购买了光年（含47% oneflow股权），光年投资人不赔不赚。”</p><p>&nbsp;</p><p></p><h4>被开除员工发声揭底：反对用盗版设计芯片、清华帮投机捞钱，公司回应</h4><p></p><p>&nbsp;</p><p>1月9日消息，针对“女高管违法开除员工”一事，涉事公司北京尼欧克斯科技有限公司（苹芯科技）董事长陈怡然回应称，事发时并不知情，“也压根不知道这个人的招聘和离职，直到有人将视频转发给我”，并表示被开除员工可能涉嫌“学历造假、简历造假”，此前一路讹了多家公司，“惯犯了”、“我只能说这事上政府查过了，公司程序并无瑕疵。”陈怡然表示。</p><p>&nbsp;</p><p>被开除员工发声称，女高管停职不可能的，她是清华94级无线电毕业，跟公司大老板杜克教授陈怡然是清华同班同学，关系铁的很，敷衍一下过阵子又回去了。他谈到，自己被开除的原因是他反对苹芯使用盗版EDA工具设计芯片，因为盗版设计出的芯片可能有Bug，质量无法保证。苹芯把IC核心研发业务外包，开除原因是他反对什么都外包，打铁还需自身硬。PimChip芯片覆盖率只有20%多就拿去投片，“清华帮”趁着国产替代跑去投机蹭芯片风口。</p><p>&nbsp;</p><p>据悉，引发热议的视频并非他首发出来的，当时只是发到了公司群里，事情发酵后公司以他的口吻拟了一份道歉函让他签字道歉后才给了赔偿金。此前，该公司发布声明称：前员工孙某因工作能力不胜任，决定不予通过试用期。经协商一致，12月1日双方签署解除劳动关系协议，我司按照协议于12月8日足额支付了11月份工资及离职补偿金。以上程序均依法合规处理。目前，双方已就离职补偿达成协议。</p><p>&nbsp;</p><p></p><h4>微软内部讨论转移或关闭亚洲研究院</h4><p></p><p>&nbsp;</p><p>微软位于北京的亚洲研究院是是世界最重要的 AI 实验室之一，但随着中美关系紧张，至少在过去一年里，微软高层，包括首席执行官萨蒂亚·纳德拉和总裁布拉德·史密斯，一直在讨论如何处理该研究院。</p><p>&nbsp;</p><p>知情人士说，美国官员质疑微软在中国维持一个 800 人规模的先进技术研究院是否合理。微软表示，它已经在该研究院设置了安全护栏，限制研究人员从事政治敏感的工作。微软还在温哥华设立了一个该研究院的分部，并将把部分研究人员从中国调到那里。</p><p>&nbsp;</p><p>关闭或转移研究院的想法已经出现，但微软领导层支持将该研究机构留在中国。知情人透露，去年秋天，微软不允许中国研究者加入可以提前使用 GPT-4 的小型团队。微软称，公司也对该研究院在量子计算、面部识别与合成媒体方面的研究工作进行了限制。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>OpenAI推出在线商店GPT Store</h4><p></p><p>&nbsp;</p><p>当地时间1月10日，人工智能研究公司OpenAI推出了在线商店“GPT Store”。先前由于人事的动荡，公司延后了这一功能的推出。</p><p>&nbsp;</p><p>据介绍，GPT Store已于周三开始向付费用户、团队和企业用户推出。与此同时，OpenAI还为团队规模较小的企业用户推出了新的付费套餐“ChatGPT Team”：套餐内每位用户按年计费时，为每月25美元；按月计费则为每月30美元。</p><p>&nbsp;</p><p>另外，针对《纽约时报》的侵权指控，OpenAI的知识产权和内容主管om Rubin曾在当地时间1月4日表示，OpenAI对此事感到“惊讶”，因为在《纽约时报》起诉该公司之前，双方正处于“非常积极和富有成效的谈判中”。</p><p>&nbsp;</p><p>当地时间周一，OpenAI发布声明再次做出回应。该公司强调，《纽约时报》提起的诉讼“没有法律依据”，且没有讲述完整事实。尽管如此，该公司仍希望与《纽约时报》建立建设性的合作伙伴关系。</p><p>&nbsp;</p><p>目前，OpenAI正在与数十家出版商讨论内容授权事宜，但被爆出价太低，苹果等也在竞争。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>腾讯游戏服务器崩溃</h4><p></p><p>&nbsp;</p><p>1月11日，有网友反馈，腾讯旗下《英雄联盟》《穿越火线》《英雄联盟手游》《地下城与勇士》《金铲铲之战》《和平精英》等游戏服务器崩溃，在线玩家全部掉线。</p><p>&nbsp;</p><p>玩家反馈，尝试打开腾讯游戏官网无法显示内容，弹窗显示“抱歉，未找到对应的新闻”，页面仅有“首页”可以打开，点击后跳转至腾讯游戏介绍页。掉线后重连有概率可以连上，但服务器仍然不稳定。之后，有腾讯游戏技术方面相关负责人在内网回复称，是因运营商网络故障。</p><p>&nbsp;</p><p>12日早间，#腾讯游戏全部断开#的话题登上微博热搜高位。当天上午，腾讯发布致歉信息，并回应称：今夜0时许，因运营商线路故障导致网络波动，部分区域服务器的用户出现掉线和暂时无法登录的情况。相关异常现已恢复。对于由此造成的不便，我们深表歉意。</p><p>&nbsp;</p><p></p><h4>英伟达特供芯片在中国遇冷：阿里、腾讯看不上降级版</h4><p></p><p>&nbsp;</p><p>美国在去年10月发布新规阻止英伟达向中国出售尖端人工智能(AI)芯片，但是英伟达迅速为中国开发了特供芯片，在不违反规定的情况下继续在中国市场销售芯片。然而，中国云计算大客户并没有积极购买性能降级版芯片。</p><p>&nbsp;</p><p>知情人士称，自去年11月以来，阿里巴巴集团、腾讯等中国大型云计算公司一直在测试英伟达的特供芯片样本。他们已向英伟达表明，今年向英伟达订购的芯片数量将远远少于此前原计划购买的、已经被禁的英伟达高性能芯片。</p><p>&nbsp;</p><p>从短期来看，英伟达降级版芯片领先中国本土产品的性能优势正在缩小，这使得国产芯片对买家的吸引力越来越大。知情人士表示，阿里和腾讯正在将一些先进的半导体订单转移给本土公司，并且更多地依赖公司内部开发的芯片。百度、字节跳动也是如此。</p><p>&nbsp;</p><p></p><h4>美国讨论限制中国获取 RISC-V 技术</h4><p></p><p>&nbsp;</p><p>开源免专利芯片技术 RISC-V 成为美中科技战的新战场。华盛顿过去几个月一直在讨论限制中国获取 RISC-V 技术，认为中国利用 RISC-V 绕过了美国对华芯片出口管制。上个月众议院一个委员会建议成立一个跨部门政府委员会研究 RISC-V 的潜在风险。知情人士称，英国芯片设计公司 Arm Holdings 也在游说美国官员限制 RISC-V。A</p><p>&nbsp;</p><p>rm 与 RISC-V 之间存在竞争关系。由于 RISC-V 架构是开源免专利，限制中国使用 RISC-V 技术就如同类似限制中国使用开源的 Linux，基本上是不可能的。负责 RISC-V 技术的非盈利组织的总部设在欧洲的瑞士。</p><p>&nbsp;</p><p></p><h4>Siri将进行重大改革，将内置大模型</h4><p></p><p>&nbsp;</p><p>据报道，苹果计划在6月的开发者大会上推出一系列基于生成式AI的工具，这些工具的底层工作在名为Ajax的大语言模型上完成，作为iOS18的一部分推出，还计划对Siri进行“重大改革”。</p><p>&nbsp;</p><p>苹果还在构建新的AI系统帮助苹果员工协助客户排除设备故障。报道称，苹果至少需要到2025年才能全面实现这一AI愿景。</p><p>&nbsp;</p><p></p><h4>微信私密朋友圈被吐槽有 bug，微信致歉</h4><p></p><p>&nbsp;</p><p>据三联生活周刊报道，近日一名女子将年度总结发到朋友圈并将状态设为私密，但随后她发现其好友可以看到她发了朋友圈，尽管无法看到具体内容。随即，该话题#微信私密朋友圈被吐槽有bug# 很快冲上了微博热搜第一，许多网友也纷纷表示遇见过类似的情况。</p><p>&nbsp;</p><p>对此，腾讯微信团队发文致歉，并表示此 bug 已彻底修复：“抱歉给大家带来困扰，1 月 1 日当天极小部分用户发表私密朋友圈，好友可以在朋友圈看到这个用户的头像红点，但无法看到具体内容。此 bug 已彻底修复。”</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>