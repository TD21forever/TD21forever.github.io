<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/xYXCTKLkttJOJNd6P832</id>
            <title>全球十大最有价值AI初创企业公布，这家26岁华裔青年创办的AI独角兽估值仅次于OpenAI</title>
            <link>https://www.infoq.cn/article/xYXCTKLkttJOJNd6P832</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xYXCTKLkttJOJNd6P832</guid>
            <pubDate></pubDate>
            <updated>Fri, 06 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI领域, 生成式AI, 提示词, 创新可能性
<br>
<br>
总结: 过去两年，AI领域经历了显著演变，生成式AI的快速崛起成为核心。生成式AI是一种通过简单操作生成文本、图像、音频等结果的技术，操作命令被称为提示词。生成式AI的快速生成多样化内容的能力激发了广泛的创新可能性。它对行业格局产生深远影响，成熟企业拥抱生成式AI并整合至运营体系，有效运用生成式AI已成为决定公司未来命运和增长轨迹的决定性因素。生成式AI不仅作用于单一公司，而是构建起一个创新与技术进步的整体环境。 </div>
                        <hr>
                    
                    <p>过去两年以来，AI领域经历了一波显著演变，而其核心则是生成式AI的快速崛起。所谓生成式AI，是一种能够通过简单操作生成文本、图像、音频等结果的AI技术，所遵循的操作命令则被称为提示词。</p><p>&nbsp;</p><p>值得注意的是，这一进步催生出了全新的行业，初创企业和老牌巨头都开始挖掘生成式AI中的潜力。凭借其快速生成多样化内容的能力，生成式AI在从创意产业到数据分析的各个领域，都激发起广泛的创新可能性。随着企业利用这项技术来简化流程、吸引客户并开发前沿产品，市场动态自然也会随之变化、呈现出前所未有的新形态。</p><p>&nbsp;</p><p>生成式AI对于行业格局的影响堪称深远，老牌公司也就此有了冲击新高峰的机会。随着成熟企业拥抱生成式AI，并将其整合至原本的运营体系当中，其市值开始大幅飙升。随着领域内竞争态势的加剧，人们逐渐发现有效运用生成式AI的能力，已经成为决定公司未来命运和增长轨迹的决定性因素。</p><p>&nbsp;</p><p>生成式AI带来的变革性力量不仅作用于单一公司，而是构建起一个创新与技术进步的整体环境。随着创造性探索与实践应用的融合，生成式AI正凭借一条条提示词塑造着未来。</p><p>&nbsp;</p><p>在跟风险投资家或者初创公司的创始人们交流时，他们往往会抛出这样一个共同观点：过去两年间，实现融资和良好估值已经越来越困难。现实数据也确实支持这一观察。但当我们把目光投向AI领域，则会出现极为鲜明的对比。在这个舞台上，初创公司、特别是那些基于生成式AI的初创公司，他们的实际表现几乎与消极的整体环境截然相反。</p><p>&nbsp;</p><p>AI企业确实表现出非凡的能力，可以吸引到大量资金（通常可达数十亿美元之巨），同时获得可观的市场估值。这场席卷全球的经济衰退，似乎没有给他们的发展轨迹蒙上哪怕一丝阴影。</p><p>&nbsp;</p><p>在今天的文章中，我们将探讨全球十家估值最高的AI初创公司。他们不仅筹集到数十亿美元，而且总估值已然突破500亿美元大关。</p><p>&nbsp;</p><p>快速分析：如下图所示，这些公司中有九家总部位于美国，唯一的一家非美国公司来自加拿大。另外，其中九家为独角兽企业，一家已经成长为“十角兽”。除了Tiger Global和红杉等VC和PE领域的大牌之外，其他知名投资方还包括微软、谷歌、英伟达和Salesforce等大型科技公司。排名前六的公司中，有四家正在开发大语言模型，而且相互之间处于直接竞争关系。排名第七、八、九的三家公司，则主要利用AI产品为客服中心提供服务。榜单中只有一家厂商专攻GPT打包器产品。</p><p>&nbsp;</p><p>下图，就是在估值上傲视同侪的十大AI初创企业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48cf788d7a317d155032cbadcfef7593.png" /></p><p></p><p>全球估值最高的十大AI初创公司。</p><p></p><h3>1) OpenAI – 290亿美元 billion</h3><p></p><p>总融资规模：113亿美元</p><p>主要投资方：微软</p><p>&nbsp;</p><p>OpenAI目前的市场估值高达290亿美元，成为全球估值最高的AI初创公司。其最著名的投资方就是微软，软件帝国通过一项复杂交易共向该公司注资达110亿美元。截至目前，OpenAI总计筹集到113亿美元资金。</p><p>&nbsp;</p><p>这家公司于2015年12月正式创立，除了Sam Altman、Greg Brockman、Ilya Sutskever、John Schulman 和&nbsp;Wojciech Zaremba之外，还有“硅谷钢铁侠”马斯克的参与。OpenAI希望以造福全人类的方式创造先进的通用人工智能（AGI）。截至目前，该公司专注于构建大语言模型，这是一种意在理解自然语言的AI方案。模型经过大量数据的训练，能够生成与人类相似的文本等多种输出形式。</p><p>&nbsp;</p><p>该公司于2020年6月发布了GPT-3（即生成式预训练Transformer 3），成为真正意义上的突破性大语言模型，拥有1750亿个参数（用于控制模型如何处理数据的内部设置）。GPT-3与OpenAI此前模型的最大区别，在于其庞大的整体规模（作为前代产品，GPT-2仅有15亿个参数）和生成与人类相似文本的能力。</p><p>&nbsp;</p><p>但真正让OpenAI进入公众视野的还得说ChatGPT，这是一款以GPT-3为基础构建而成的对话聊天机器人。ChatGPT于2022年11月启动，成功弥合了AI与人类之间的交互鸿沟，成为AI进步中的标志性里程碑。发布后短短两个月时间内，聊天机器人就赢得1亿用户，以创纪录的速度成为发展最快的消费级互联网产品。在此之后，该公司又陆续推出了多种新功能、产品和大语言模型。</p><p>&nbsp;</p><p>关于OpenAI发展历程的有趣之处在于，该公司的最初定位其实是非营利组织，但在2019年起转向有限利润结构，用以吸引外部投资和技术人才（提供股票期权）。这种模式允许公司在业务成功的前提下向股东提供有限的分红。而超出设定上限的回报，则归OpenAI的原始非营利实体所有。作为其中的营利性实体，Openai LP将OpenAI非营利组织视为其控股股东。</p><p>&nbsp;</p><p>该公司在结构和所有权方面还有另外一个有趣的点，其联合创始人兼CEO Sam Altman并不持有公司的任何所有权股份。</p><p>&nbsp;</p><p>OpenAI的目标是在2024年实现10亿美元收入。他们目前主要靠提供高级版ChatGPT（价格为19美元）来赚钱，并向开发商和企业收取在产品中使用其LLM API的费用。</p><p></p><h3>2) Scale – 73亿美元</h3><p></p><p>总融资规模：6亿美元</p><p>主要投资方：Dragoneer、Tiger Global、Greenoaks</p><p>&nbsp;</p><p>Scale（前ScaleAI）是全球估值第二高的私人AI公司，其估值为73亿美元。迄今为止，该公司已经于2021年4月筹集到6亿美元的资金，其中E轮融资筹得3.25亿美元。Scale的投资方包括Dragoneer、Greenoaks、Tiger Global、Coatue、Coatue、Index、Founders Fund、Founders Fund和Y Combinator等。</p><p>&nbsp;</p><p>作为一家由Alexandr Wang，Lucy Guo和Brandon Zhang于2016年成立的公司，Scale希望通过提供高质量的训练数据来加快AI应用程序的开发进展。值得一提的是，作为ScaleAI的创始人，Alexandr Wang是一位年仅26岁的华裔青年，目前他所创办的公司已经成为硅谷最杰出的人工智能公司之一。该公司的业务范围涵盖数据注释、数据管理和机器学习运营，这些服务将帮助用户团队在生产环境中部署并管理其机器学习模型。据该公司介绍，他们迄今为止已经完成了超75亿条数据注释。</p><p>&nbsp;</p><p>Scale的客户包括生成式AI平台、领先的技术厂商和政府机构。该公司还成为有意研发大语言模型的企业（包括OpenAI）的首选合作伙伴，帮助他们为客户训练此类模型。</p><p>&nbsp;</p><p>这家总部位于旧金山的企业在非洲、菲律宾等全球多地组织起一支人力大军，参与对不同AI模型进行分类和数据标注。这部分工作主要通过旗下子公司Remotasks完成，但这家公司因为工人薪酬过低而面临批评，据称还经常推迟或停发应付工资。</p><p>&nbsp;</p><p>根据公司CEO的介绍，Scale在2021年完成了最后一轮融资，当时其年内收入期望为1亿美元。</p><p></p><h3>3) Anthropic – 50亿美元</h3><p></p><p>总融资规模：11亿美元</p><p>主要投资方：SK电信、谷歌、Spark Capital</p><p>&nbsp;</p><p>Anthropic由OpenAI公司前研究副总裁Dario Amodei和他的姐姐Daniela（OpenAI前安全与政策副总裁）共同建立。除他们二人，这家企业还至少吸引了其他九位前OpenAI员工。</p><p>&nbsp;</p><p>截至目前，这家公司已经筹集到11亿美元，其中包括本月初韩国SK电信投入的1亿美元。该公司很快成长为全球估值第三高的AI初创企业，据报道估值为50亿美元。尽管该公司本身尚未正式确认这个数字，但多家信誉良好的媒体报道了其今年年初的融资活动，称目前估值为50亿美元。值得注意的是，该公司最终虽然披露了融资轮细节，但仍决定不公布具体估值。另有媒体表示，该公司的交易前估值应该是41亿美元。</p><p>&nbsp;</p><p>该公司希望构建起可靠、可解释且有说服力的大语言模型（他们称其&nbsp;为AI系统）。官方网站提到，“我们开发出大规模的AI系统，以便在最容易暴露问题的各个技术前沿中研究其安全性。我们使用这些见解来建立起更安全、可协调且更加可靠的模型，再转化成Claude之类能够在外部部署的系统成果。”</p><p>&nbsp;</p><p>作为Anthropic打造的AI助手，Claude于今年早些时候首次亮相，可以通过基于聊天的界面和API进行访问。该公司目前掌握两款大语言模型：Claude 2，他们的主力模型，主要用于复杂的推理、创作、对话、编码和较为具体的任务创建场景；另外还有Claude Instant，一款更具成本效益的模型，可稳定完成随意闲聊、文本分析、总结和文档解析等工作。</p><p>&nbsp;</p><p>目前，其聊天机器人和API的访问均未全面开放。聊天机器人处于对外公测阶段，而API业务访问则仅向特定合作伙伴提供。</p><p></p><h3>4) Hugging Face – 45亿美元</h3><p></p><p>总融资规模：4亿美元</p><p>主要投资方：谷歌、亚马逊、英伟达</p><p>&nbsp;</p><p>Hugging Face本月早些时候刚刚从全球最大的几家科技巨头处筹得2.35亿美元，目前是以45亿美元估值排名第四的明显AI初创公司。迄今为止，该公司的融资总额已接近4亿美元，其投资方包括谷歌、亚马逊、英伟达、Salesforce、AMD、英特尔、高通、Lux Capital、红杉资本和Coatue。</p><p>&nbsp;</p><p>Hugging Face由 Clément Delangue、Julien Chaumond&nbsp;和 Thomas Wolf 于 2016 年创立，最初只是想为青少年开发一款聊天机器人。但随着后续发展，它开始转向AI的另一领域——构建机器学习技术平台，推动机器学习大众化。</p><p>&nbsp;</p><p>该公司常被称为机器学习领域的GitHub，在平台之上向开发人员开放对数千个预训练机器学习模型及自研模型的浏览、使用和共享，同时提供跨社区互动、数据集下载和模型自动训练等服务。这家初创公司的专业账户每月收费9美元，企业账户中的每位用户月费则为20美元。他们还提供模型托管服务，最低价格为每小时0.06美元。</p><p>&nbsp;</p><p>根据福布斯的报道，该公司的年收入估计在3000万至5000万美元之间。</p><p></p><h3>5) Inflection AI – 40亿美元</h3><p></p><p>总融资规模：15.25亿美元</p><p>主要投资方：英伟达、微软、谷歌</p><p>&nbsp;</p><p>这家公司由多位科技界的重量级人物于2022年创立，包括LinkedIn 联合创始人 Reid Hoffman、Google DeepMind 联合创始人 Mustafa Suleyman 以及 DeepMind 前首席科学家&nbsp;Karén&nbsp;Simonyan。Inflection AI是一家AI工作室，希望为每个人打造个性化AI。他们的首款产品于今年年初推出，是一款名为Pi（代表个人智能）的AI助手。</p><p>&nbsp;</p><p>根据报道，就在两个月前，该公司通过一轮13亿美元的巨额融资获得了40亿美元估值。此轮融资由微软、里德·霍夫曼、比尔·盖茨、埃里克·施密特和英伟达领投，一举将融资总额推上15.25亿美元。最新估值让Inflection成为全球估值第五高的私人AI公司，也是资金最为充足的初创企业之一。</p><p>&nbsp;</p><p>Pi助手由该公司自研的大语言模型Inflection -1提供支持，项目于今年6月正式披露。Inflection计划尽快通过对话式API将该服务向开发者用户开放。由于这是一家垂域整合型AI工作室，因此其AI训练和推理等工作均在内部自主完成。</p><p>&nbsp;</p><p>该公司宣称，其Inflection-1大语言模型在计算性能方面傲视同侪，“在常用于比较大语言模型性能的各类基准测试中”优于GPT-3.5、LLaMA、Chinchilla和PaLM-540B。</p><p></p><h3>6) Cohere – 22亿美元</h3><p></p><p>总融资规模：4.45亿美元</p><p>主要投资方：Inovia Capital、英伟达、Tiger Global</p><p>&nbsp;</p><p>Cohere是这份榜单上唯一一家非美国初创企业。Cohere总部位于加拿大多伦多，由前Google Brain团队成员Aidan Gomez、Nick Frosst&nbsp;以及 Ivan Zhu 于 2019 年创立。该公司开发的大语言模型能够理解并生成与人类相似的文本，与OpenAI、Anthropic和Inflection属于直接竞争关系，且主要关注企业服务。</p><p>&nbsp;</p><p>在今年6月的最后一轮融资（2.7亿美元）当中，Cohere获得了22亿美元的估值。截至目前，该公司已经从英伟达、甲骨文、Salesforce、Inovia Capital和Tiger Global等投资方处筹集到总计4.45亿美元。据报道，Tiger Global一直在就收购该公司部分股份的方案寻求谈判，一旦成功有望将Cohere的估值推上30亿美元。</p><p>&nbsp;</p><p>就在一个月前，这家加拿大AI初创公司推出了Coral，一款专为企业设计的AI助手。Coral能够帮助员工完成各种任务，例如回答客户问题和分析业务数据。当员工提出问题时，它能使用公司内部的信息及其他信源给出答案。它可以对接企业内的100多种数据源，包括文档、数据库等。</p><p>&nbsp;</p><p>目前Coral尚处于内测阶段，以该公司最新的Command模型为基础，这套模型仍保持着每周更新。同时，Cohere的Command模型也可通过API供外部开发者在自己的产品中使用。</p><p></p><h3>7) Dialpad – 22亿美元</h3><p></p><p>总融资规模：4.18亿美元</p><p>主要投资方：Iconiq Capital、软银、Omers</p><p>&nbsp;</p><p>Dialpad由Craig Walker、John Rector 和 Brian Peterson 于 2011 年创立，是一款面向企业的AI统一通信与客服中心平台，可帮助企业通过语音、消息和视频会议等渠道与客户开展交互。</p><p>&nbsp;</p><p>与榜单上的大多数其他公司不同，Dialpad并不是一家AI公司。其目前的主要业务是为客户提供多种AI驱动型服务，并凭借22亿美元的估值成为全球第七大私人AI公司。迄今为止，该公司已筹集到4.18亿美元资金，其中包括2021年最新一轮融资获得的1.7亿美元。Dialpad的投资方包括Iconiq、软银、Omers、Amasia、GV、Andreessen Horowitz 和 Salesforce Venture。</p><p>&nbsp;</p><p>该公司在官方网站上宣称，其产品已经得到7000家品牌客户的采用，包括Motorola Solutions、Netflix、T-Mobile 和 Uber 等。</p><p></p><h3>8) Asapp – 16亿美元</h3><p></p><p>总融资规模：4亿美元</p><p>主要投资方：Fidelity、Dragoneer</p><p>&nbsp;</p><p>总部位于纽约的Asapp拥有16亿美元估值，成为全球第八大最有价值的AI初创公司。2021年，该公司通过C轮融资从Fidelity和Dragoneer处筹得1.2亿美元，使其迄今为止的融资总额达到4亿美元。</p><p>&nbsp;</p><p>Asapp由Gustavo Sapoznik&nbsp;于 2014 年创立，主要为客服中心提供各类AI产品及服务，帮助其优化运营、提高座席生产力及销售执行效率。</p><p>&nbsp;</p><p>该公司宣称，其服务能够帮助客服中心将平均处理时间缩短10%以上、流程上手周期减半、增强客户服务体验，并自动处理70%的响应内容。它能自动总结所有客户交互，并利用据称是全球最准确的语音到文本技术实现呼叫内容转录。</p><p>&nbsp;</p><p>这家初创公司喜欢自称为AI研究公司，旨在推进AI发展以增强人类活动，帮助企业解决种种现实难题。Asapp的官方网站写道，“我们当前的议程包括客户服务领域的一系列重要工作。这个领域向来充斥着各种问题与大量数据，也是创新和应用AI/机器学习技术的理想场景。通过我们在面向任务对话、自然语言处理和语音识别方面的研究，我们为消费级公司带来了极具影响力的绩效提升。这不仅对企业客户具有现实意义，更是全体消费者的福音。”</p><p></p><h3>9) Cresta AI – 16亿美元</h3><p></p><p>总融资规模：1.51亿美元</p><p>主要投资方：Tiger Global、红杉资本</p><p>&nbsp;</p><p>Cresta AI也是一家专注利用生成式AI改善客服中心运营效能的初创公司。Cresta由S. Zayd Enam、Sebastian Thrun和Tim Shi于 2017 年创立，主要为客服中心提供AI驱动工具，帮助他们提高客户支持效果、增强销售运营效率。</p><p>&nbsp;</p><p>其技术能够提供实时指导、日常任务自动化，并基于数据分析生成见解，据此为客服人员增添助力。他们的目标是提高座席工作效率、缩短处理时间并提高整体客户服务质量。</p><p>&nbsp;</p><p>该公司在官网上写道，“Cresta的实时智能平台依托于生成式AI技术，可帮助客服、经理和部门领导者协同工作，最大限度提高收入、服务效率并创造卓越的客户体验。与客服中心内使用的传统工具相比，Cresta能够分析复杂的陈述、情绪、情感和行为，帮助更深入地理解客户对话内容。”</p><p>&nbsp;</p><p>Cresta的估值同样为16亿美元，碰巧跟竞争对手Asapp在本次全球估值最高AI初创公司榜单上并列第八。2022年3月，该公司在由Tiger Global领投的C轮融资中筹得8000万美元。</p><p>&nbsp;</p><p>Cresta的产品得到全球多家领先企业的使用，包括希尔顿、洲际酒店集团酒店及度假村、CarMax、Blue Nile、Earthlink、Intuit以及保时捷。</p><p></p><h3>10) Jasper – 15亿美元</h3><p></p><p>总融资规模：1.31亿美元</p><p>主要投资方： Insight Partners、Foundation Capital</p><p>&nbsp;</p><p>考虑到Asapp和Cresta在全球估值最高的AI初创公司榜单上并列第八，所以Jasper只能屈居第十位。该公司去年刚刚筹集到1.25亿美元，目前估值为15亿美元。他们也是GPT打包器业务领域估值最高的AI初创公司，主要负责在OpenAI的GPT技术之上构建自己的服务。</p><p>&nbsp;</p><p>Jasper 由 Dave Rogenmoser、Chris Hull和John Philip Morgan于 2017 年创立，前身为Proof公司。最初，其主要业务是通过产品帮助企业提高网站转化率。但在2021年，这家初创企业经历了一波转型，推出一款名为Jarvis AI的AI写作工具。之后经过品牌重塑，就有了我们现在所知的Jasper公司。用他们自己的话来说，Jasper是一款创意型AI助手，能够帮助企业根据自身品牌形象创建宣传内容。</p><p>&nbsp;</p><p>这家初创公司提供三种主要工具。其一能帮助用户编写内容，包括博文和社交媒体帖子。其二是Chrome扩展程序，可以在Google Docs和Gmail等应用程序中为用户提供编写建议。其三则是AI图像生成器，名为Jasper Art。Jasper也通过API对外开放自家技术方案。</p><p>&nbsp;</p><p>这家总部位于奥斯汀的AI公司宣称，其2021年的收入总额为4500万美元，据报道有望在2022年进一步增长至7500万美元。尽管刚刚完成一波巨额融资，但Jasper还是被迫在几个月后进行了一波裁员，据称此举是为了重塑公司团队。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://aibeat.co/highest-valued-ai-startups/">https://aibeat.co/highest-valued-ai-startups/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2qmOzDAMFH1YH0f78GWG</id>
            <title>安息吧，元宇宙</title>
            <link>https://www.infoq.cn/article/2qmOzDAMFH1YH0f78GWG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2qmOzDAMFH1YH0f78GWG</guid>
            <pubDate></pubDate>
            <updated>Thu, 05 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, 元宇宙项目, 扎克伯格, 失败
<br>
<br>
总结: Meta的元宇宙项目的失败表明，扎克伯格的雄心壮志未能实现。缺乏明确的商业愿景和解决实际问题的能力，导致了这个项目的崩塌。扎克伯格的虚假承诺和对元宇宙的定义模糊不清，使得公众对他的信任破灭。他的科技帝国虽然强大，但也无法阻止失败的发生。 </div>
                        <hr>
                    
                    <p>Meta的急剧垮台表明，这位雄心勃勃、曾经不可撼动的CEO兼Facebook联合创始人已经麻烦缠身。他的宏图伟业之一、被寄予厚望的元宇宙项目，已经有一只脚踏进了科技行业的垃圾堆。</p><p></p><h2>Meta“蠢蛋秀”</h2><p></p><p>2022年底，这家曾经市值万亿美元的科技巨头以70%的自由落体式暴跌结束了这风云变幻的一年，也使其成为整个标普500指数中表现最差的股票。公司陷入了严重麻烦，从社交网络巨头Facebook到元宇宙虚拟现实世界的激进转型已经成为一场闹剧、一笔沉重的损失。</p><p>&nbsp;</p><p>为了了解原Facebook和元宇宙项目的这段历程，我们不妨先从这家大型科技企业陷入当前困境的主要原因说起。</p><p>&nbsp;</p><p>最大的问题其实并不在于马克·扎克伯格全力押注元宇宙。事实上，无论Meta接下来打算主攻哪款产品，结局都有可能失败。正如作家兼专栏写手Ted GIoia所言，“在Facebook看来，用户永远是错的。”</p><p>&nbsp;</p><p>对于干过企业的人来说，无论规模如何，商业经营的重点都应该是为客户提供服务，这似乎是理所当然的思维。但Facebook和旗下的Instagram、WhatsApp，再到现在的Meta，却永远只有一个目的：为应用和背后的开发团队创造利润。这也是导致扎克伯格元宇宙帝国轰然倒塌的真正原因。</p><p></p><h2>虚假的承诺</h2><p></p><p>扎克伯格当初想要打造元宇宙的雄心壮志，确实吸引到了他身边几乎所有伙伴。他宣称这个虚拟世界将是“一个广阔且身临其境的互联网全新版本”。</p><p>&nbsp;</p><p>元宇宙迅速登上商业世界的顶峰，其他企业也纷纷选择跟进，包括沃尔玛、迪士尼、耐克和古驰等知名企业。扎克伯格还说服投资者、华尔街和媒体共同加入这场狂欢。</p><p>&nbsp;</p><p>到这里，一切看起来都很有搞头。</p><p>&nbsp;</p><p>当时科技专家Ed Zitron曾表示“元宇宙项目已经成功了一半”，并在短时间内震动了整个科技行业。但扎克伯格的宏大叙事最终只是……一句空话。元宇宙项目没有明确的商业愿景，最终也没能为公众解决任何实际问题。</p><p>&nbsp;</p><p>Meta掌门人对于他的下一场辉煌胜利做出了充满诗意的表达，但却缺乏关于元宇宙具体能做什么的确切描述。因为拿不出清晰可行的愿景和能够解决的问题，Meta这场豪赌很快遭受损失。其实大家也能看出，缺少明确的动机、目标受众和市场接纳意愿，这东西根本就不可能真正发展出又一家重量级企业。</p><p></p><h2>巨大的失败</h2><p></p><p>现在，我们来具体对这些问题做一番剖析。</p><p>&nbsp;</p><p>由于产品负责人自己没法说明元宇宙要解决什么问题，自然也就没法让公众理解和认同。用扎克伯格自己的话来说：</p><p>&nbsp;</p><p>“我认为很多人在说起元宇宙时，想到的仅仅是虚拟现实——没错，虚拟现实肯定是元宇宙中重要的组成部分，但元宇宙绝不止于此。它能让我们在所有不同计算平台上访问，包括VR/AR，还有PC、移动设备和游戏主机。说到这里，很多人又觉得元宇宙就是个大游戏。是的，娱乐肯定是其中的重要组成部分，但元宇宙同样绝不止于此。”</p><p>&nbsp;</p><p>好吧，直到撰写这篇文章的时候，我仍然不禁在想，“他到底在说什么？”说了半天，又似乎什么都没说。根据他的描述，元宇宙可以是任何东西，甚至把同样的表达照搬给互联网也没有任何违和。元宇宙到底是游戏、应用，还是一整个虚拟世界？我们不知道，扎克伯格似乎也不知道。</p><p></p><h2>超级混乱</h2><p></p><p>第二个问题跟前一个密切相关。元宇宙的定义比解释它的意义更加令人费解，更不用说它的目标受众到底是谁了。扎克伯格宣称未来将有十亿人会使用元宇宙，但如果没有明确的用例，这个数字是从哪来的？据说这些用户人均会花几百美元来使用元宇宙产品。</p><p>&nbsp;</p><p>第三，既然缺少对元宇宙作用和它所要解决问题的明确认知，那我们真的很难相信全球最大的社交网络的创始人“真知道自己在干什么”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/884df5926359d05dea64f85e889c6b3b.png" /></p><p></p><p>Meta股价下跌70%，到2022年收盘时已经成为标普500指数中表现最差的股票。</p><p>&nbsp;</p><p>可既然华尔街、科技行业、媒体和技术爱好者们都相信了这个故事，为什么普罗大众就是不感兴趣呢？</p><p>&nbsp;</p><p>因为扎克伯格对我们普通人的认知是错的——这也是整个故事中最重要的分析前提。</p><p></p><h2>我错了，但我其实没错</h2><p></p><p>公众已经厌倦了那帮大公司天天告诉他们该做什么。如果媒体、科技行业和投资者居然相信一个敢在自己都说不明白的产品上投入100亿美元的家伙，那这家伙的崩塌肯定只是时间问题。</p><p>&nbsp;</p><p>根据体验过元宇宙的用户所言，这东西“质量低下”而且“bug太多”，基本跟儿童益智游戏在一个水平。对于一家价值数十亿美元的企业，拿出这样的产品当然不可能让公众满意。号称是互联网的未来形态，但充满卡通感的音乐和连腿都没有虚拟化身能“改变世界”？别开玩笑了。</p><p>&nbsp;</p><p>《商业内幕》撰稿人直呼扎克伯格为“骗子”，称元宇宙是种毫无意义的工具，只是种“分解了扎克伯格对于重要问题的关注，并给坏人提供获利机会的温床”。</p><p>&nbsp;</p><p>令人难以置信的是，一个掌握如此权势、极具影响力的名人怎么会公然撒谎，并为此把几十亿美元挥霍一空，然后好像什么都没有发生。但仔细想想，好像扎克伯格并不是第一个打算通过兜售谎言来赚钱的人。</p><p>&nbsp;</p><p>大家还记得Elizabeth Holmes承诺要用90亿美元彻底改变验血方式吗？事实证明这完全是个骗局。就连扎克伯格自己，也不是第一次这么干了。</p><p></p><h2>扎克伯格的科技帝国</h2><p></p><p>扎克伯格建立起一个科技帝国，无论他要做什么，都能在这个帝国之内牢牢保持统治，甚至到了不可触碰的程度。换言之，Meta是他一手成就的，所以可以完全控制，任何董事会成员都无法阻拦。</p><p>&nbsp;</p><p>尽管迄今为止的很多尝试都遭受失败，例如2013年的Facebook Phone，但Meta的这位CEO还是在一年之后以20亿美元收购了VR厂商Oculus并继续全力下注。</p><p>&nbsp;</p><p>当然，他创造的“互联网未来”的狂妄愿景、包括用虚拟现实加化身构建数字世界的思路，都可以一路追溯到上世纪90年代。游戏《子午线》、《领土在线》和更早的《创世纪》都做出了自己的尝试。</p><p>&nbsp;</p><p>由此看来，建设元宇宙的想法并非毫无可取。毕竟如今的技术似乎正向着虚拟现实环境的广泛普及步步逼近。然而，如果这个未来还需要15到20年才能落地，那么扎克伯格老兄确实有些操之过急了。想带动整个行业？那就得承受相应的风险。</p><p>&nbsp;</p><p>如今的智能手机几乎成为身体的延伸。苹果通过iPhone将各种产品组合成了统一的实用工具，从而突破了市场边界、颠覆了市场形态。电话、MP3播放器还有电子记事本，现在它们都是智能手机的组成部分。</p><p>&nbsp;</p><p>然而，扎克伯格并不是乔布斯，他的元宇宙产品一直也没有清晰的前进方向。</p><p>&nbsp;</p><p>Meta打算宣扬一种革命性的数字社交方式，但对普通群众来说，这可能只是另一种更繁琐的游戏参与方式。在大型游戏行业来看，这无非就是另一种形式的多人在线游戏。从这个角度，也能看出乔布斯和扎克伯格二人对于创新革命性产品的理解根本就不在一个层次。</p><p></p><h2>安息吧，元宇宙</h2><p></p><p>尽管经历了大肆宣传，但由于得不到市场支持，这个短命的项目还是崩溃了。Meta甚至没法说服自己的员工使用这套Horizon Worlds平台。由于之前夸下的海口无法兑现，元宇宙概念变得愈发虚弱无力。则Web3行业，也迅速将注意力转向了更有热度的AI炒作。</p><p>&nbsp;</p><p>大多数当初贸然跟进的企业开始关闭自己的元宇宙项目。沃尔玛在Roblox上推出的元宇宙体验项目短短六个月后就被关停，迪士尼也在今年三月关闭了元宇宙部门。</p><p>&nbsp;</p><p>现在我们很难判断，扎克伯格提出的整个元宇宙概念到底只是为了创造一个大骗局、让他和自己的同行能够在亏损中狠捞一笔，还是他真心觉得自己有机会开启互联网的新时代、实现自我超越。无论如何，引发这一切的核心在于他是扎克伯格，是扎克大王，没人够胆阻拦他的脚步。无论选择是对是错，有权做出判断的只有他本人。此外，Facebook始终专注于控制用户，而非认真倾听他们的意见。</p><p></p><h2>Meta的终局之战</h2><p></p><p>从商业角度来说，我们需要明确一点。Facebook已经是家价值数十亿美元的巨型企业，而且在几年前就已经达到了体量上限。之后他们遭遇到几波重大动荡，目前的产品已经没有拓展的空间。</p><p>&nbsp;</p><p>扎克伯格打算通过单纯关注利润来重塑自己、拯救自己的公司。大家都知道，目前大多数巨头企业都是这么个思路，这也很可能是Meta面前最不坏的选项。</p><p>&nbsp;</p><p>遗憾的是，这样一个身兼所有骗子特征的家伙根本不会受到任何惩罚。即便误导了整个行业，平白烧掉几十亿美元，还拉了那么多人下水，扎克伯格也依然逍遥自在。</p><p>&nbsp;</p><p>而且我们压根没必要对此感到惊讶，毕竟他的帝国就是建立在谎言之上。也许这位草根大王最终会把自己玩死，也许会让从无到有的帝国再从有到无，但……一切都由扎克伯格掌控。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://beincrypto.com/metaverse-swindler-zuckerberg-deceived-fantasy/">https://beincrypto.com/metaverse-swindler-zuckerberg-deceived-fantasy/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/21b0Ov78WA25AaSrSyxp</id>
            <title>Meta 开源文本生成音乐AI：AudioCraft 将文字转化为和声</title>
            <link>https://www.infoq.cn/article/21b0Ov78WA25AaSrSyxp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/21b0Ov78WA25AaSrSyxp</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源, Text-to-Music生成式人工智能, AudioCraft, 模型
<br>
<br>
总结: Meta开源了它的Text-to-Music生成式人工智能AudioCraft，供研究人员和从业者训练他们自己的模型，并帮助推动前沿技术的发展。AudioCraft包含三个不同的模型：MusicGen能够根据文本提示生成音乐；AudioGen能够产生环境声音；EnCodec是一个由AI驱动的编码器/量化器/解码器。今天，我们很高兴地发布了我们的改进版EnCodec解码器，它可以用更少的伪像（artifacts）生成更高质量的音乐；这个预训练的AudioGen模型可以生成环境声音以及狗叫、汽车喇叭声或木地板上的脚步声等音效；我们将分享所有的AudioCraft模型权重和代码。据Meta介绍，AudioCraft能够使用自然界面生成高质量的音频。此外，他们还说，AudioCraft利用一种新方法简化了音频生成领域最先进的设计。具体来说，AudioCraft使用EnCodec神经音频编解码器从原始信号中学习Audio Token。这一步从音乐样本创建出了固定“词汇表”（Audio Token），并随后将其传递给自回归语言模型。这个模型训练了一个新的音频语言模型，利用Token的内部结构来捕捉它们的长程依赖关系，这对音乐生成至关重要。最后，这个新模型基于文本描述生成新的Token，并将其反馈到编解码器的解码器以合成声音和音乐。生成任何类型的高保真音频都需要在不同的尺度上对复杂的信号和模式进行建模。音乐可以说是最具挑战性的音频类型，因为它由局部和长程模式组成，从一组音符到使用多种乐器的整体音乐结构。如前所述，AudioCraft是开源的，Meta希望能够帮助研究社区以它为基础做进一步地构建：坚实的开源基础将有助于推动创新，丰富我们未来制作和收听音频和音乐的方式：想象一下，配有音效和史诗音乐的丰富多彩的睡前故事读物。借助更多的控制，我们认为MusicGen可以变成一种新型乐器——就像合成器刚出现时那样。虽然AudioCraft的大部分是开源的，但是他们为模型权重选择了CC-BY-NC许可。Hacker News上有用户指出，该许可限制较多，并不算完全开源。具体来说，非商业性使用条款违背了开源倡议对开源的定义中的第六点，这很可能是因为Meta使用了Meta拥有并特别授权的音乐来计算这些权重。其余组件将在MIT许可下发布。 </div>
                        <hr>
                    
                    <p>Meta<a href="https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/">开源</a>"了它的Text-to-Music生成式人工智能<a href="https://github.com/facebookresearch/audiocraft">AudioCraft</a>"，供研究人员和从业者训练他们自己的模型，并帮助推动前沿技术的发展。</p><p>&nbsp;</p><p>AudioCraft包含三个不同的模型：<a href="https://huggingface.co/spaces/facebook/MusicGen">MusicGen</a>"能够根据文本提示生成音乐；<a href="https://felixkreuk.github.io/audiogen/">AudioGen</a>"能够产生环境声音；<a href="https://ai.meta.com/blog/ai-powered-audio-compression-technique/">EnCodec</a>"是一个由AI驱动的编码器/量化器/解码器。</p><p></p><p></p><blockquote>今天，我们很高兴地发布了我们的改进版EnCodec解码器，它可以用更少的伪像（artifacts）生成更高质量的音乐；这个预训练的AudioGen模型可以生成环境声音以及狗叫、汽车喇叭声或木地板上的脚步声等音效；我们将分享所有的AudioCraft模型权重和代码。</blockquote><p></p><p>&nbsp;</p><p>据Meta介绍，AudioCraft能够使用自然界面生成高质量的音频。此外，他们还说，AudioCraft利用一种新方法简化了音频生成领域最先进的设计。</p><p>&nbsp;</p><p>具体来说，AudioCraft使用EnCodec神经音频编解码器从原始信号中学习Audio Token。这一步从音乐样本创建出了固定“词汇表”（Audio Token），并随后将其传递给自回归语言模型。这个模型训练了一个新的音频语言模型，利用Token的内部结构来捕捉它们的长程依赖关系，这对音乐生成至关重要。最后，这个新模型基于文本描述生成新的Token，并将其反馈到编解码器的解码器以合成声音和音乐。</p><p></p><p></p><blockquote>生成任何类型的高保真音频都需要在不同的尺度上对复杂的信号和模式进行建模。音乐可以说是最具挑战性的音频类型，因为它由局部和长程模式组成，从一组音符到使用多种乐器的整体音乐结构。</blockquote><p></p><p>&nbsp;</p><p>如前所述，AudioCraft是开源的，Meta希望能够帮助研究社区以它为基础做进一步地构建：</p><p></p><p></p><blockquote>坚实的开源基础将有助于推动创新，丰富我们未来制作和收听音频和音乐的方式：想象一下，配有音效和史诗音乐的丰富多彩的睡前故事读物。借助更多的控制，我们认为MusicGen可以变成一种新型乐器——就像合成器刚出现时那样。</blockquote><p></p><p>&nbsp;</p><p>虽然AudioCraft的大部分是开源的，但是他们为模型权重选择了<a href="https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights">CC-BY-NC许可</a>"。Hacker News上有用户指出，<a href="https://news.ycombinator.com/item?id=36974030">该许可限制较多，并不算完全开源</a>"。</p><p>&nbsp;</p><p>具体来说，<a href="https://opensource.org/osd/">非商业性使用条款违背了开源倡议对开源的定义中的第六点</a>"，这很可能是因为Meta使用了Meta拥有并特别授权的音乐来计算这些权重。其余组件将在<a href="https://github.com/facebookresearch/audiocraft/blob/main/LICENSE">MIT许可</a>"下发布。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/meta-text-to-music-generative-ai/">https://www.infoq.com/news/2023/08/meta-text-to-music-generative-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NL4liAf7mSLGhMcI1I9q</id>
            <title>AI、ML、数据工程新闻汇总：Stable Chat、Vertex AI、ChatGPT 及 Code Llama</title>
            <link>https://www.infoq.cn/article/NL4liAf7mSLGhMcI1I9q</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NL4liAf7mSLGhMcI1I9q</guid>
            <pubDate></pubDate>
            <updated>Tue, 03 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Stability AI, Stable Chat, 用户对话体验, AI 聊天平台
<br>
<br>
总结: Stability AI发布了一款名为Stable Chat的AI聊天平台，该平台以用户对话体验的稳定性与一致性为设计重点，通过可靠的回答降低了AI对话中可能出现的错误信息和误解。 </div>
                        <hr>
                    
                    <p></p><h3>Stability AI 发布 Stable Chat</h3><p></p><p></p><p>新颖的 AI 聊天平台 <a href="https://www.infoq.com/news/2023/08/stable-chat/?topicPageSponsorship=b2206c17-c7cf-47e8-aee9-0514a0817c31">Stable Chat</a>"，以用户对话体验的稳定性与一致性为设计重点。由 <a href="https://stability.ai/blog/stable-chat-research-defcon-ai-village">Stability AI</a>" 开发，该平台意在通过可靠而非创造性生成或不可预测的回答，降低以 AI 为驱动力的对话中可能出现的错误信息和误解。</p><p>&nbsp;</p><p>这一方式可用于医疗保健和客户支持等关键领域。在这些领域中，保持沟通的清晰性和正确性至关重要。该平台对稳定性独树一帜的关注，使其成为 AI 聊天机器人和对话代理行业不断发展的新亮点。</p><p></p><h3>Vertex AI 搜索与对话已全面上线</h3><p></p><p></p><p>谷歌云的<a href="https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/">Vertex AI 搜索与对话</a>"服务已正式全面上线。这项开发技术可使企业利用 AI 驱动的搜索和对话功能增强其应用程序，促进与用户更为直观且高效的互动。凭借语义搜索和自然语言理解等功能，Vertex AI 搜索与对话服务允许企业构建智能搜索引擎与对话代理，提供相关性更高的信息并与用户进行自然的对话。</p><p>&nbsp;</p><p>此次发布标志着各行业在利用 AI 与机器学习技术提供客户体验与推动创新方面迈出了重要的一步。</p><p></p><h3>OpenAI 推出 ChatGPT 企业服务</h3><p></p><p></p><p>OpenAI 已推出&nbsp;<a href="https://www.infoq.com/news/2023/09/openai-chatgpt-enterprise/">ChatGPT 企业订阅服务</a>"，意在帮助企业在各类应用中充分利用其强大的语言模型。该服务提供为职业定制的强化语言能力，其中包括更强的安全功能和访问控制。借助 ChatGPT 企业服务，企业可利用其对自然语言的理解和生成，强化用户支持、实现任务自动化并开发定制的人工智能解决方案，同时还能维护数据隐私及合规性。</p><p>&nbsp;</p><p>OpenAI 此举表明了其致力于满足企业需求并扩大 AI 语言模型在商业环境中的应用。</p><p></p><h3>OpenAI 将 GPT-3.5 Turbo 面向开发者开放使用</h3><p></p><p></p><p>OpenAI 推出其语言模型的高级迭代版本，<a href="https://www.infoq.com/news/2023/08/got-3-5-fine-tuning/">GPT-3.5 Turbo</a>"。新版本允许用户根据特定任务需求，通过微调定制并调整模型。</p><p>&nbsp;</p><p>OpenAI 同时还公布了 <a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates">API 定价结构的更新</a>"，使开发人员在实验和部署以 GPT-3.5 Turbo 驱动的应用程序时更具成本效益。</p><p></p><h3>Meta 开源 Code Llama</h3><p></p><p></p><p>Meta 所推出的新颖 AI 工具 <a href="https://www.infoq.com/news/2023/09/meta-code-llama/">Code Llama</a>"，意在协助开发者提升代码编写效率。<a href="https://about.fb.com/news/2023/08/code-llama-ai-for-coding/">Code Llama</a>" 采用大语言模型和深度学习技术理解并生成代码，可简化编码过程从而提升开发人员的工作效率。</p><p>&nbsp;</p><p>该工具是对快速发展的 AI 驱动开发工具的重要补充，进一步体现了 Meta 对推进 AI 技术的承诺。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/09/ai-ml-data-news-september4-2023/">AI, ML, Data Engineering News Roundup: Stable Chat, Vertex AI, ChatGPT and Code Llama</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dGfCQZjo2v6rAicehPmd</id>
            <title>新型威胁：探索LLM攻击对网络安全的冲击</title>
            <link>https://www.infoq.cn/article/dGfCQZjo2v6rAicehPmd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dGfCQZjo2v6rAicehPmd</guid>
            <pubDate></pubDate>
            <updated>Tue, 03 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 卡内基梅隆大学, LLM Attacks, 大型语言模型, 提示后缀
<br>
<br>
总结: 卡内基梅隆大学的研究人员发布了LLM Attacks，这是一种可以针对各种大型语言模型构建对抗性攻击的算法。通过自动生成提示后缀，攻击者可以绕过语言模型的安全机制，导致有害的响应。这些攻击是可转移的，可以用于许多不同的语言模型。这项研究揭示了大型语言模型面临的安全漏洞和威胁。 </div>
                        <hr>
                    
                    <p>来自<a href="https://www.cmu.edu/">卡内基梅隆大学（CMU）</a>"的研究人员发布了<a href="https://llm-attacks.org/">LLM Attacks</a>"，这是一种可以针对各种大型语言模型（LLM）构建对抗性攻击的算法，包括<a href="https://chat.openai.com/">ChatGPT</a>"、<a href="https://claude.ai/">Claude</a>"和<a href="https://bard.google.com/">Bard</a>"。这些自动生成的攻击，在GPT-3.5和GPT-4上的成功率为84%，在<a href="https://www.infoq.com/news/2023/06/google-palm2-bard/">PaLM-2</a>"上的成功率为66%。</p><p>&nbsp;</p><p>与大多数“越狱”攻击通过试错手工构建不同，CMU的团队设计了一个三步流程来自动生成提示后缀，它们可以绕过LLM的安全机制，导致有害的响应。而且，这些提示还是可转移（transferrable）的，也就是说，一个给定的后缀通常可以用于许多不同的LLM，甚至是闭源模型。为了衡量算法的有效性，研究人员创建了一个名为AdvBench的基准测试；在此基准测试上进行评估时，LLM攻击对Vicuna的成功率为88%，而基线对抗算法的成功率为25%。根据CMU团队的说法：</p><p></p><p></p><blockquote>最令人担忧的也许是，目前尚不清楚LLM提供商是否能够完全修复此类行为。在过去的10年里，在计算机视觉领域，类似的对抗性攻击已经被证明是一个非常棘手的问题。有可能深度学习模型根本就无法避免这种威胁。因此，我们认为，在增加对此类人工智能模型的使用和依赖时，应该考虑到这些因素。</blockquote><p></p><p>&nbsp;</p><p>随着ChatGPT和GPT-4的发布，<a href="https://arxiv.org/abs/2305.13860">出现了许多破解这些模型的技术</a>"，其中就包括可能导致模型绕过其保护措施并输出潜在有害响应的提示。虽然这些提示通常是通过实验发现的，但LLM Attacks算法提供了一种自动创建它们的方法。第一步是创建一个目标令牌序列：“Sure, here is (content of query)”，其中“content of query”是用户实际输入的提示，要求进行有害的响应。</p><p>&nbsp;</p><p>接下来，该算法会查找可能导致LLM输出目标序列的令牌序列，基于贪婪坐标梯度（GCG）算法为提示生成一个对抗性后缀。虽然这确实需要访问LLM的神经网络，但研究团队发现，在许多开源模型上运行GCG所获得的结果甚至可以转移到封闭模型中。</p><p>&nbsp;</p><p>在<a href="https://www.cmu.edu/news/stories/archives/2023/july/researchers-discover-new-vulnerability-in-large-language-models">CMU发布的一条介绍其研究成果的新闻</a>"中，论文合著者Matt Fredrikson表示：</p><p></p><p></p><blockquote>令人担忧的是，这些模型将在没有人类监督的自主系统中发挥更大的作用。随着自主系统越来越真实，我们要确保有一种可靠的方法来阻止它们被这类攻击所劫持，这将非常重要……现在，我们根本没有一个令人信服的方法来防止这种事情的发生，所以下一步，我们要找出如何修复这些模型……了解如何发动这些攻击通常是建立强大防御的第一步。</blockquote><p></p><p>&nbsp;</p><p>论文第一作者、<a href="https://twitter.com/andyzou_jiaming/status/1684766184871546881">CMU博士生Andy Zou在推特上谈到了这项研究</a>"。他写道：</p><p></p><p></p><blockquote>尽管存在风险，但我们认为还是应该把它们全部披露出来。这里介绍的攻击很容易实现，以前也出现过形式类似的攻击，并且最终也会被致力于滥用LLM的团队所发现。</blockquote><p></p><p>&nbsp;</p><p><a href="https://twitter.com/DavidSKrueger/status/1684904671914115072">剑桥大学助理教授David Krueger回复了Zou的帖子</a>"，他说：</p><p></p><p></p><blockquote>在图像模型中，10年的研究和成千上万的出版物都未能找出解决对抗样本的方法，考虑到这一点，我们有充分的理由相信，LLM同样会如此。</blockquote><p></p><p>&nbsp;</p><p>在Hacker News上关于这项工作的讨论中，<a href="https://news.ycombinator.com/item?id=36921808">有一位用户指出</a>"：</p><p></p><p></p><blockquote>别忘了，本研究的重点是，这些攻击不需要使用目标系统来开发。作者谈到，攻击是“通用的”，他们的意思是说，他们可以在自己的计算机上完全使用本地模型来生成这些攻击，然后将它们复制并粘贴到GPT-3.5中，并看到了有意义的成功率。速率限制并不能帮你避免这种情况，因为攻击是在本地生成的，而不是用你的服务器生成的。你的服务器收到的第一个提示已经包含了生成好的攻击字符串——研究人员发现，在某些情况下，即使是对GPT-4，成功率也在50%左右。</blockquote><p></p><p>&nbsp;</p><p>GitHub上提供了代码，你可以在AdvBench数据上重现<a href="https://github.com/llm-attacks/llm-attacks">LLM Attacks实验</a>"。项目网站上还提供了几个对抗性攻击的<a href="https://llm-attacks.org/">演示</a>"。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/llm-attack/">https://www.infoq.com/news/2023/08/llm-attack/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3CYMzjRwkdHv0jLcyGuK</id>
            <title>Meta AI是如何在 Facebook 和 Instagram 上增强用户体验的？</title>
            <link>https://www.infoq.cn/article/3CYMzjRwkdHv0jLcyGuK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3CYMzjRwkdHv0jLcyGuK</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能系统, Instagram, Facebook, 内容发现
<br>
<br>
总结: 为了帮助用户更好地理解人工智能在 Instagram 和 Facebook 许多核心功能中的作用，Meta分享了22张系统卡，解释了人工智能系统如何工作以及如何定制显示的内容。这些系统卡提供了关于人工智能系统的概述、工作方式、定制选项和预测模型的信息，帮助用户更好地了解和控制他们在社交媒体平台上看到的内容。 </div>
                        <hr>
                    
                    <p></p><blockquote>为了帮助用户更好地理解人工智能在 Instagram 和 Facebook 许多核心功能中的作用，今天我们将分享有关我们的人工智能系统运作方式的详细信息。</blockquote><p></p><p>&nbsp;</p><p>我们利用人工智能来帮助每天使用我们服务的数十亿用户发现他们可能会觉得有用和有趣的内容，无论是在 Instagram 上关注新的创作者，还是在 Facebook 上可能喜欢的帖子。</p><p>&nbsp;</p><p>我们建立这些系统的目标是确保人们看到的内容能够与他们相关并且有价值。在 Facebook 和 Instagram 上，并没有一个单一的人工智能系统来决定用户所看到的一切。相反，许多独立的人工智能系统会分别工作，有时也会共同合作，在幕后以极短的时间内无缝地提供这些体验。更深入地了解，每个人工智能系统都有多个模型，用于识别内容并预测一个人对其感兴趣或与之互动的可能性有多大。</p><p>&nbsp;</p><p>作为 Meta 对透明度的承诺的一部分，今天我们分享了 22 张系统卡，其中包含了信息和可行性见解，每个人都可以使用这些信息来理解和定制他们在我们产品中特定的人工智能驱动体验。我们发布这些卡片是为了帮助人们更好地了解人工智能在 Instagram 和 Facebook 的许多功能中的作用，并解释人们的选择和行为如何通过我们的排名和推荐系统影响他们所看到的内容，比如新的视频或他们可能想要关注的创作者。系统卡现在在我们的<a href="https://transparency.fb.com/features/explaining-ranking">透明中心</a>"（Transparency Center）提供了 22 种语言的版本。</p><p>&nbsp;</p><p>对于使用 Facebook 和 Instagram 的人来说，能够<a href="https://ai.facebook.com/blog/responsible-ai-progress-meta-2022/">获得</a>"关于支持他们体验的技术的信息非常重要。这些信息也必须以一种非专家和专家都能理解的方式提供和解释。</p><p></p><h2>Meta AI 的系统卡</h2><p></p><p>&nbsp;</p><p>我们分享了 22 张系统卡，解释了人工智能驱动的推荐系统在 Facebook 和 Instagram 上的工作方式。其中包括 14 张关于 Facebook 的系统卡，包括 Facebook Feed、Feed 推荐、Feed 排名评论、Reels、Stories、视频、通知、市场、群组 Feed、单个群组 Feed、建议的群组、搜索、可能认识的人和可能喜欢的页面。另外还有 8 张关于 Instagram 的系统卡，包括 Instagram Feed、Feed 推荐、Stories、探索、Reels 串联、搜索、建议的账号和通知。</p><p></p><p>每个人工智能系统卡都包含四个部分：</p><p>&nbsp;</p><p>人工智能系统的概述；解释人工智能系统如何工作的部分，包括创建 Facebook 和 Instagram 体验的步骤概述；描述如何定制显示的内容的部分。包括系统控制的描述和每个人如何控制和定制他们的体验的说明；解释人工智能如何提供内容的部分，包括解释一些重要的预测模型如何影响整体人工智能系统并产生产品体验的说明。</p><p></p><p>人工智能系统的预测模型可能会使用一些信息，例如帖子的特征和一个人与类似帖子的互动历史，来预测兴趣水平。在 Facebook 和 Instagram 上有成千上万个这样的信号被使用。</p><p>&nbsp;</p><p>例如，当预测一个人是否会与一篇帖子互动时，人工智能系统会考虑以下信号：</p><p>&nbsp;</p><p>帖子或视频获得的赞数、评论数和观看次数；一个人观看视频或查看帖子的频率或时间长度；一个人与某个作者的互动情况，比如他们以前见过和喜欢过该作者的类似帖子的次数。</p><p>&nbsp;</p><p>重要的是，我们的系统卡还描述了每个人工智能系统的控制选项，人们可以使用这些选项来定制他们的体验。例如，如果有人想要看到某种类型的帖子更少，他们可以取消关注该作者，暂时隐藏内容，或在 Facebook 上点击 “Show less”（显示更少），在 Instagram 上点击 “Not interested”（不感兴趣），以临时降低类似内容的排名分数。</p><p></p><h2>我们创建系统卡的方法</h2><p></p><p>&nbsp;</p><p>在创建这些系统卡时，我们面临的最大挑战之一是找到以一种每个人都能理解的方式解释高度技术性信息的最佳方法。由于目前没有行业标准的方法，因此我们在 Meta 公司内部创建了一个<a href="https://ai.facebook.com/research/publications/system-level-transparency-of-machine-learning">统一的方法</a>"来解释这些系统。通过倾听我们的服务用户的意见，并与设计和开发过程中的多元化专家群体进行交流，我们获得了有助于确定如何以有意义的方式呈现这些信息的见解。我们听到人们希望能够更透明地了解和控制他们所看到的内容，因此我们在每个系统卡中添加了一个定制部分。我们还了解到，给人们过多的技术细节有时会模糊透明度，这就是为什么我们只呈现了最重要的十个预测模型，而不是系统中的全部内容。</p><p>&nbsp;</p><p>为了保持我们的方法一致，我们选择了一套术语词汇，用于讨论人工智能。在解释可能不熟悉的术语时，我们会包含一个工具提示，提醒人们我们所说的 “相关内容” 和 “人工智能系统” 的含义。通过采用一致的语言方法，我们使人们能够比较和对比多个系统卡。</p><p>&nbsp;</p><p>为了保持系统卡中解释我们的人工智能系统如何工作和提供内容的各个部分的一致性，我们开发了内部工具来分析构成人工智能系统的模型的影响。在我们工程师的帮助下，我们将这些信息从信号转化为文字，以帮助解释每个人工智能系统如何进行预测。值得注意的是，这些模型和信号是动态的，随着系统的学习而改变，并且随着时间的推移会经常发生变化。</p><p></p><h2>人工智能系统卡的未来</h2><p></p><p>&nbsp;</p><p>随着行业的发展和对系统文档和透明度的讨论的继续，我们将进一步识别机会，随着时间的推移对我们的方法进行迭代，以便反映产品的变化、不断发展的行业标准以及对人工智能透明度的期望。</p><p>&nbsp;</p><p>我们将继续整合来自多样化受众的反馈，改进我们的产品并赋予使用者更多权力。在我们的研究中，我们了解到人们希望在提供的信息与他们相关时，以文字和视觉的结合方式来探索系统卡，因此我们正在不断改进系统卡。</p><p>&nbsp;</p><p>人们可以通过访问我们的<a href="https://transparency.fb.com/features/explaining-ranking">透明中心</a>"找到我们的系统卡。我们希望这一努力能鼓励人们更多地了解 AI 如何支持他们的体验。我们相信，系统卡将使人们能够学习有关人工智能的知识，并控制和定制他们在使用我们产品时的体验。</p><p>&nbsp;</p><p>原文链接：</p><p>&nbsp;</p><p>https://ai.meta.com/blog/how-ai-powers-experiences-facebook-instagram-system-cards/</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fv25HvvTwYfkGdOVFGQ7</id>
            <title>GitLab发布2023年全球DevSecOps报告，AI和ML从“有”变成“必须有”</title>
            <link>https://www.infoq.cn/article/fv25HvvTwYfkGdOVFGQ7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fv25HvvTwYfkGdOVFGQ7</guid>
            <pubDate></pubDate>
            <updated>Sun, 01 Oct 2023 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GitLab, 全球DevSecOps AI报告, AI, ML
<br>
<br>
总结: GitLab发布了2023年全球DevSecOps AI报告，报告显示AI和ML的使用正在从“有”发展到“必须有”。23%的组织已经在软件开发中使用AI，65%的受访者表示他们现在或将在未来三年内在测试中使用AI和ML。报告还显示，除了AI和ML，DevOps和DevSecOps方法的采用率正在上升，开发人员和安全专业人员在谁应该带头解决安全问题上仍然存在争议。 </div>
                        <hr>
                    
                    <p>GitLab的<a href="https://about.gitlab.com/blog/2023/09/12/gitlab-global-devsecops-ai-report/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">2023年全球DevSecOps AI报告</a>"已发布，其中一个关键发现是<a href="https://en.wikipedia.org/wiki/Artificial_intelligence?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">AI</a>"和<a href="https://www.infoq.com/introduction-machine-learning/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">ML</a>"的使用正在从“有”发展到“必须有”。</p><p></p><p>报告显示，23%的组织已经在软件开发中使用AI，其中60%的组织每天都在使用AI。此外，65%的受访者表示，他们现在或将在未来三年内在测试中使用AI和ML。</p><p></p><p>83%的受访者表示，为了避免落后，在软件开发中使用AI至关重要。然而，也有约67%的受访者担心AI/ML所带来的影响，原因是AI/ML比人类更具成本效益优势，这会导致人类可从事的工作变少，并可能引入给他们带来麻烦的错误。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7dce13400a0b2380f6447c6bb01352bc.webp" /></p><p></p><p>虽然AI能够帮助开发者写代码，但这只占开发者工作时间的四分之一，剩下的时间花在其他任务上，这意味着AI有机会被用在写代码以外的领域。62%的受访者使用AI在正式测试流程之外检查代码，53%的受访者使用机器人测试代码。这两个数字同比增长均超过10%。</p><p></p><p>报告还显示，除了AI和ML，自2022年以来，DevOps和DevSecOps方法的采用率正在上升，从47%上升到56%。此外，DevSecOps正在脱离孤立的状态——只有30%的受访者表示他们需要对安全完全负责——低于一年前的48%。38%的安全专业人员认为他们是跨职能安全团队的一员，这一比例在一年前为29%。但是，开发人员和安全专业人员在谁应该带头解决安全问题上仍然存在争议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97ed30248b89bb787ce40281796ab566.webp" /></p><p></p><p>左移安全性检查的势头仍在，74%的受访者现在已经或计划在未来三年内在SDLC早期就进行测试，开发人员在编写代码阶段就发现漏洞（而不是在更后面）的情况显著增加。组织的首要投入重点仍然是云计算，但安全、治理和合规性现在是第二大关注点。</p><p></p><p>工具链复杂性仍然是一个问题，几乎三分之二的受访者希望简化他们使用的工具，因为大约一半的受访者所使用的工具链包含了六个或更多的工具。值得注意的是，这使得获得合规性和监控的整体视图，以及在工具链中获得洞见变得更加困难。</p><p></p><p>报告指出，提高开发者生产力、加快发布速度和提高业务敏捷性是扩展DevSecOps实践的关键原因。然而，只有15%的受访者认为去年的DevSecOps预算有所增加。DevSecOps平台继续受到关注，72%的受访者正在使用或将在明年使用，主要原因是为了提高效率、安全性和自动化。</p><p></p><p>GitLab的全球DevSecOps AI状态报告<a href="https://about.gitlab.com/developer-survey/#ai?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU5Nzg5MDUsImZpbGVHVUlEIjoiOTEzSk01bTR4TGY3VzlBRSIsImlhdCI6MTY5NTk3ODYwNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.mJDfWNDhrj6Y7jXDpF_5JSNoz_CNiIfPI6X1eKcPy9s">可从其网站下载</a>"。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/09/gitlab-global-devsecops-ai/">https://www.infoq.com/news/2023/09/gitlab-global-devsecops-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/etaokYfgLHFXpFa5DpSK</id>
            <title>华为中秋节给员工发Mate60手机；商汤科技回应原知产总监被立案侦查；马斯克平均年终奖33亿元 | AI一周资讯</title>
            <link>https://www.infoq.cn/article/etaokYfgLHFXpFa5DpSK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/etaokYfgLHFXpFa5DpSK</guid>
            <pubDate></pubDate>
            <updated>Sun, 01 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 第四范式, 港股上市, 微软, Windows 11, Copilot, AI驱动画图工具, 阿里, 菜鸟, 独立上市, 台积电, AI芯片, 涨价, 抖音, 闪电搜索
<br>
<br>
总结: 第四范式成功在港股上市，微软发布了Windows 11的重大更新，包括Copilot和AI驱动画图工具，阿里计划将菜鸟独立上市，台积电的AI芯片将涨价，抖音推出了闪电搜索。 </div>
                        <hr>
                    
                    <p></p><blockquote>第四范式港股上市；微软发布Windows 11重大更新，包含Copilot和AI驱动画图工具；阿里分拆菜鸟独立上市；台积电AI芯片将涨价；抖音推出闪电搜索……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>第四范式港股上市</h4><p></p><p></p><p>9月28日，第四范式（6682.HK）董事会主席、执行董事、首席执行官兼总经理戴文渊站在港交所和杨强一起敲响了第四范式IPO的铜锣。除了担任第四范式联合创始人、非执行董事外，杨强还是戴文渊的博士生导师。两位都是迁移学习领域的领军人物，引导全球迁移学习的研发方向。</p><p></p><p>戴文渊表示：“自成立以来，第四范式一直致力于助力企业智能化转型和推进人工智能发展，成长为决策类AI平台领先企业。今天在港交所成功挂牌上市，对第四范式而言是一个重要的里程碑。非常感谢我们所有员工一直以来的努力，也感谢各级领导、投资人、客户以及合作伙伴给予我们的信任。未来，我们将继续巩固自身技术壁垒，不断提升商业化能力，持续助力客户业务成功。”</p><p></p><p>第四范式成立于2014年，按2022年相关收入计算，中国决策类人工智能平台领域的前五大参与者合计占约56.1%市场份额，第四范式以22.6%的份额位列市场第一。</p><p></p><h4>微软发布Windows 11重大更新，包含Copilot和AI驱动画图工具</h4><p></p><p></p><p>9月27日消息，微软今天发布了 Windows 11 的重大更新，本次更新包括全新的Windows Copilot、用于Paint、Snipping Tool和Photos 的AI增强功能以及RGB照明支持等。其中，Windows Copilot是本次更新最主要的功能，它将作为一款基于AI的数字助手存在，可以把Bing Chat功能直接带到Windows 11桌面，以侧边栏的形式出现，用户能用它控制 PC 上的设置、启动应用程序或简单的回答查询。</p><p></p><h4>ChatGPT发布重大更新，可在几秒钟制作出逼真的合成语音</h4><p></p><p></p><p>据OpenAI官网9月25日消息，OpenAI宣布在接下来的两周内，将在ChatGPT中向Plus和Enterprise用户推出语音和图像。语音将在iOS和Android上推出（在设置中选择加入），图像将在所有平台上提供。</p><p></p><p>OpenAI称，这项新的语音技术能够从几秒钟的真实语音中制作出逼真的合成语音，为许多有创意和无障碍的应用打开了大门。然而，这些功能也带来了新的风险，例如恶意行为者可能冒充公众人物或实施欺诈。OpenAI 表示，这种模型不会被广泛开放，而是会受到严格的控制和限制。</p><p></p><h4>阿里分拆菜鸟独立上市</h4><p></p><p></p><p>9月26日，阿里巴巴在港交所发布公告称，公司拟通过以菜鸟股份于香港联交所主板独立上市的方式分拆菜鸟。根据第15项应用指引，公司已就拟议分拆向香港联交所提交分拆计划，且香港联交所已确认公司可进行拟议分拆。</p><p></p><p>现建议，拟议分拆将以菜鸟股份全球发售的方式进行（包括香港公开发售及国际发售）。拟议分拆完成后，公司将继续持有菜鸟50%以上的股份，因此，菜鸟将仍为公司的子公司。</p><p></p><p>此外，菜鸟集团副总裁 、国际快递事业部总经理丁宏伟在26日宣布菜鸟全球五日达服务正式上线，这是电商行业首个规模化落地的跨境电商快线产品，接下来会在英国、西班牙、荷兰、比利时、韩国5个亚欧国家全量上线。</p><p></p><h4>商汤科技回应原知产总监被立案侦查</h4><p></p><p></p><p>近日，市场有消息称，商汤科技知识产权执行总监高某某涉嫌非国家工作人员受贿罪，被立案侦查，采取强制措施。</p><p></p><p>9月28日，商汤发言人表示，原商汤知识产权执行总监高某某因经济问题已被公安机关采取强制措施。由于涉嫌刑事犯罪，警方正在侦查中，一切将以警方披露信息为准。商汤科技始终严格遵守商业道德和规范，对腐败等违法违规行为“零容忍”。</p><p></p><p>公开资料显示，高某某在2017年加盟商汤科技集团，任公司出口管制合规官和知识产权执行总监，负责商汤集团出口管制合规和知识产权事务。</p><p></p><h4>亚马逊投资OpenAI头号对手40亿美元</h4><p></p><p></p><p>当地时间9月25日，亚马逊宣布将向人工智能初创公司Anthropic投资40亿美元，并持有其部分股权。Anthropic已经开发了聊天机器人Claude，被认为是OpenAI和谷歌在生成式AI产品上的主要竞争对手。</p><p></p><p>亚马逊和Anthropic表示，这项新的战略合作将结合双方在更安全的生成式AI领域的技术和专业知识，加速Anthropic未来基础模型的开发，并将其广泛提供给亚马逊云科技的客户使用。</p><p></p><p>Anthropic由OpenAI（ChatGPT的开发机构）前研究副总裁达里奥·阿莫迪（Dario Amodei）、大语言模型GPT-3论文的第一作者汤姆·布朗（Tom Brown）等人于2021年在美国旧金山共同创立。其创始成员此前多为OpenAI的核心员工，他们曾经深度参与过GPT-3等多项研究。</p><p></p><h4>微软招募核能专家，拟用小型核反应堆为AI提供动力</h4><p></p><p></p><p>根据一份新的招聘启事，微软公司正在研究使用下一代核反应堆来为其数据中心和人工智能提供动力。</p><p></p><p>这则发布于9月25日的招聘启事称，微软董事长兼首席执行官萨蒂亚·纳德拉 （Satya Nadella）表示：“随着微软云将世界上最先进的人工智能模型转变为新的计算平台，下一波计算浪潮正在诞生。”“我们致力于帮助客户使用我们的平台和工具，在今天以更少的资源做更多的事情，并在人工智能新时代为未来进行创新。”因此，微软正在寻找一位核技术首席项目经理，负责完善和实施全球小型模块化反应堆（SMR）和微反应堆能源战略。</p><p></p><p>招聘启事描述道，该高级职位的任务是领导SMR和微反应器集成的技术评估，为微软云和人工智能所在的数据中心提供动力。</p><p></p><h4>抖音推出闪电搜索</h4><p></p><p></p><p>日前，抖音推出了一款“闪电搜索”APP，其Slogan为“闪电搜索，快如闪电”。官方介绍称，闪电搜索是一款抖音集团旗下的全新搜索工具产品，为用户提供智能、丰富、极速的搜索体验；在这里，可以搜索小说、影视、故事、音乐内容，医疗与生活服务工具一应俱全，同时配备金币激励模式，在端内阅读和搜索都能获得金币奖励。据了解，这是继头条搜索、悟空搜索、抖音搜索之后，抖音启用的第四个搜索品牌。</p><p></p><h4>台积电AI芯片将涨价</h4><p></p><p></p><p>据IT之家报道，随着台积电供应链扩大CoWoS先进封装产能，这些中间膜价格的上涨最终将推高该公司生产的AI芯片成本。由于对AI产品的强劲需求，台积电正在投资数十亿美元升级其封装产能。业界人士透露，追加设备进驻厂房后，台积电的月产能可达2.5万片以上、甚至朝3万片靠拢，从而令台积电承接AI相关订单能力上升，由于相关产能升级，台积电的AI芯片也将迎来涨价。</p><p></p><p>此外，台媒指出，英伟达是目前台积电CoWoS先进封装最大的客户，订单量占产能六成，近期因应AI运算强劲需求，英伟达扩大下单，而其他客户的急单也开始涌现。</p><p></p><h4>阿里云通义千问 14B 模型开源</h4><p></p><p></p><p>9 月 25 日，阿里云开源通义千问 140 亿参数模型 Qwen-14B 及其对话模型 Qwen-14B-Chat，支持免费商用。</p><p></p><p>据了解，此次开源的 Qwen-14B 是一款支持多种语言的高性能开源模型，相比同类模型使用了更多的高质量数据，整体训练数据超过 3 万亿 Token，使模型具备更强大的推理、认知、规划和记忆能力。Qwen-14B 最大支持 8k 的上下文窗口长度。而 Qwen-14B-Chat 则是在基座模型上经过精细 SFT 得到的对话模型。借助基座模型强大的性能，Qwen-14B-Chat 生成内容的准确度大幅提升，也更符合人类偏好，内容创作上的想象力和丰富度也有显著扩展。</p><p></p><h4>百川智能发布Baichuan2-53B，开放API全面进军To B领域</h4><p></p><p></p><p>9月25日，百川智能发布Baichuan2-53B 闭源大模型，全面升级了Baichuan1-53B的各项能力。Baichuan2-53B不仅数学和逻辑推理能力提升显著，还通过高质量数据体系和搜索增强极大降低了模型幻觉。</p><p></p><p>作为首批通过备案的大模型企业，百川智能此次还开放了Baichuan2-53B API接口，正式进军To B领域，开启商业化进程。</p><p></p><h4>韩国互联网公司 Kakao、Com2uS 正在裁减元宇宙团队</h4><p></p><p></p><p>据 Newsis 报道，由于 2023 年上半年出现巨额亏损，韩国顶级互联网公司 Kakao 和游戏开发商 Com2uS 正在缩减其元宇宙团队规模。</p><p></p><p>据悉，Com2uS 是一家主要的移动游戏开发商，去年的收入为 5.36 亿美元，最近缩减了其元宇宙部门的员工规模，有关裁员的细节尚未透露。该公司的元宇宙平台 Com2Verse 于今年 8 月正式上线。然而，元宇宙平台背后的业务部门在今年上半年运营亏损约 620 万美元。</p><p></p><p>Kakao Games 及 Neptune 旗下的元宇宙公司 Colorverse 则在 2022 年亏损 860 万美元，并已在今年早些时候进行了一轮裁员。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>华为中秋节给员工发 Mate60 手机</h4><p></p><p></p><p>近日，华为员工在社交媒体上曝光了他们在中秋节收到的礼物，其中最引人瞩目的是一部华为Mate60手机。除了华为 Mate60 手机之外，这份中秋节礼物还包括了 6 个月饼、茶壶、茶碗、茶叶等。</p><p></p><p>此外，华为终端BG大中华区还向员工们发放了一封感谢信，对他们的辛勤付出表示感激，并希望他们继续学习和使用华为产品，成为产品专家，为消费者提供优质的服务。</p><p></p><p>网友热评：又是别人的公司。</p><p></p><h4>马斯克平均年终奖达33亿人民币</h4><p></p><p></p><p>国外调查机构 The Stock Dork 近期对全球市值前 50 家企业的首席执行官在过去五年间的年终奖金进行了分析。这些年终奖包括股票期权、红利、奖金和薪酬等收入。 特斯拉的首席执行官马斯克是全球薪酬最高的 CEO，平均年终奖高达4.567亿美元（折合当前约33.38亿元人民币）。马斯克的这一成果得益于他在2018年获得了22.3亿美元的巨额一次性股票期权奖金，这是有史以来给予首席执行官的最大奖金。</p><p></p><h4>GitHub CEO：AI无法取代程序员</h4><p></p><p></p><p>GitHub 首席执行官 Thomas Dohmke 最近在公开场合分享了他对于人工智能和软件开发之间关系的看法。Thomas Dohmke 坚持自己的观点 —— 滚雪球式的人工智能革命不会给软件开发行业敲响丧钟。Dohmke 说道，行业对软件开发者的需求将继续超过供应。事实上，Doohmke 和许多其他技术领导者一样，长期以来一直坚持认为 Copilot 等 AI 工具只是用于提高开发者的工作效率，而不是取而代之。</p><p></p><h4>国企领导称取消周末，官方通报！</h4><p></p><p></p><p>近日，多张工作群内的聊天截图在引发关注。</p><p></p><p>网传江西某国企建筑设计院院长温某在钉钉群的发言，因员工在周六有半天未看到消息，称“千万不要以为周六周日就是非工作日，不能有传统机关周末的概念”“我个人真的特别讨厌那种，一到周末就非工作状态，只顾享受，而不懂享受的源头是什么的人”“工资是按月给的，不是按22天给的”，并个人初步决定明年开始实行周六周日工作制度。</p><p></p><p>据潇湘晨报，9月26日，江西省建工集团有限责任公司纪律检查委员会的一名工作人员告诉记者，目前江西建工一建公司的纪委已经在处理了。温某的言论仅为他个人观点，不代表公司和集团的观点。</p><p></p><p>随后，江西省建工集团有限责任公司发布情况通报。通报称，近日，我司子公司所属二级单位（设计研究院）院长温衍彬，在工作群内发表不当言论被截屏转载，引发网络关注。我司高度重视，立即对该事件进行了解核实。经查，9月23日（星期六），温衍彬因布置的工作任务未得到及时回复，在设计研究院工作群内发表不当言论，造成不良影响，我司将按照有关规定进行处理。同时，我们将深刻汲取教训，进一步加强干部教育管理，严格遵守《劳动合同法》等有关法律法规，切实保障职工权益。感谢社会各界的监督。</p><p></p><p>网友热评：打工人不配拥有周末吗？</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2fI2S5hqu4NzwfiqEDYo</id>
            <title>加速机器学习模型开发：AirBnb利用Chronon实现特征工程</title>
            <link>https://www.infoq.cn/article/2fI2S5hqu4NzwfiqEDYo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2fI2S5hqu4NzwfiqEDYo</guid>
            <pubDate></pubDate>
            <updated>Sun, 01 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AirBnb, Chronon, 特征工程, 机器学习模型
<br>
<br>
总结: AirBnb开发了名为Chronon的解决方案，用于将原始数据转换为特征并进行机器学习模型的训练和推理。Chronon可以帮助机器学习工程师以可复制的方式定义特征并中心化数据计算，提高生产力和可扩展性。它支持从各种数据源获取数据，并使用类似SQL的操作和聚合进行转换。此外，Chronon还提供了Python API，方便用户进行过滤和转换操作。用户可以根据特定用例选择特征值更新的准确性，使计算更加灵活。 </div>
                        <hr>
                    
                    <p>AirBnb经常要创建用于机器学习模型的新特征，为了提高生产力和可扩展性，他们构建了一个名为<a href="https://medium.com/airbnb-engineering/chronon-a-declarative-feature-engineering-framework-b7b8ce796e04">Chronon</a>"的解决方案，用于创建将原始数据转换为特征并进行训练和推理所需的基础设施。</p><p>&nbsp;</p><p>AirBnb工程师兼Chronon创始人Nikhil Simha解释说，将原始数据转换为特征并用于训练ML模型是一项复杂且耗时的任务，工程师需要从AirBnb数据仓库中提取数据，并编写复杂的ETL逻辑将其转换为特征。另一个难点在于要确保这个逻辑所生成的推理特征分布与训练时的相同。</p><p>&nbsp;</p><p>Simha说，Chronon就是为了解决这些问题，使机器学习工程师在训练和推理中以可复制的方式定义特征并中心化数据计算。</p><p></p><p></p><blockquote>作为用户，你只需要声明一次计算，Chronon就会生成所需的所有基础设施，不断地将原始数据转换为训练和服务所需的特征。AirBnb的机器学习从业者不用再花费数月的时间手动实现复杂的管道和特征索引。通常，他们用不到一周的时间就可以为他们的模型生成新的特征集。</blockquote><p></p><p>&nbsp;</p><p><a href="https://central.sonatype.com/namespace/ai.chronon">Chronon</a>"的第一个组件支持从各种数据源获取数据，包括事件数据源、实体数据源和累积事件源，从每个数据源收集不同类型的数据。</p><p>&nbsp;</p><p>摄取数据后，它就可以使用类似SQL的操作和聚合进行转换，从而生成服务于在线模型的低延迟端点，以及用于离线训练的Hive表。在底层，Chronon使用Kafka、Spark/Spark Streaming、Hive和Airflow来构建管道。类似SQL的操作包括GroupBy、Join和StagingQuery，它们是Spark SQL查询，每天脱机计算一次。聚合包括窗口、桶和基于时间的聚合。</p><p>&nbsp;</p><p>最后，它还有一个Python API，提供了类似SQL的原语，并将基于时间的聚合和窗口作为一级概念。例如，使用Python API，你可以过滤和转换用户在过去五个小时内查看某个物品的次数。</p><p>&nbsp;</p><p>Chronon有一个重要的概念是准确性，即特征值更新的频率，是实时更新还是固定时间间隔更新。要根据特定的用例选择合适的准确性，因此，Chronon让用户可以方便地将计算的准确性设为为temporal或snapshot。</p><p>&nbsp;</p><p>在写这篇文章的时候，我还不知道AirBnb是否会在GitHub上提供Chronon，但如果你想创建自己的特征工程管道，可以读下原文中的讨论，非常有趣。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/airbnb-chronon-ml-features/">https://www.infoq.com/news/2023/08/airbnb-chronon-ml-features/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EGdxPRvIZexgBCvtaeJP</id>
            <title>生成式AI碳排放堪比开车往返月球？这个问题该如何解决</title>
            <link>https://www.infoq.cn/article/EGdxPRvIZexgBCvtaeJP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EGdxPRvIZexgBCvtaeJP</guid>
            <pubDate></pubDate>
            <updated>Sat, 30 Sep 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式人工智能, 关键词提取, 碳排放, 技术应用
<br>
<br>
总结: 生成式人工智能的发展正在改变行业和社会，它可以通过关键词提取生成内容，但同时也引发了对碳排放和技术应用的担忧。全球科技行业的温室气体排放中，人工智能只占很小比例，而且训练大型模型的次数相对较少。因此，对于生成式人工智能的碳排放担忧可能被夸大了。 </div>
                        <hr>
                    
                    <p>生成式人工智能的发展正在改变我们的行业和社会。像ChatGPT和CoPilot这样的语言模型可以码字和写代码，图像和视频生成模型可以根据简单的提示词生成引人注目的内容，音乐和语音模型可以轻松地合成任何人的声音，并创作出复杂的音乐。</p><p></p><p>世界各地都在讨论这项技术的威力和潜在价值。与此同时，人们也在谈论它可能带来的风险和威胁。</p><p></p><p>从对超级智能人工智能消灭人类的担忧，到对歧视的进一步自动化以及对仇恨和错误信息被进一步放大的担忧，人们正在努力评估和减轻这项新技术的潜在负面影响。</p><p></p><p>人们也越来越关注这些模型的能源使用和相应的碳排放。最近几个月又出现了一些关于碳排放的比较。</p><p></p><p>例如，在一篇文章中，作者戏谑<a href="https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">训练GPT-3的碳排放等同于开车往返月球</a>"，另一篇文章则解释说<a href="https://www.forbes.com/sites/glenngow/2020/08/21/environmental-sustainability-and-ai/?sh=314a59cc7db3&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">训练人工智能模型比飞机长途飞行排放更多的碳</a>"。</p><p></p><p>最终的影响将取决于这项技术是如何被使用的，以及它在多大程度上融入了我们的生活。</p><p></p><p>我们很难准确预测它将如何影响我们的日常生活，但目前已经有一个很明显的例子，即搜索巨头将生成式人工智能<a href="https://blog.google/products/search/search-generative-ai-tips/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">集成</a>"到他们的产品中。</p><p></p><p>Wired网站最近的一篇<a href="https://www.wired.com/story/the-generative-ai-search-race-has-a-dirty-secret/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">文章</a>"写道：</p><p></p><p></p><blockquote>加拿大数据中心公司QScale联合创始人Martin Bouchard表示，基于他对微软和谷歌搜索发展计划的了解，在搜索过程中添加生成式人工智能需要让“每次搜索至少增加4到5倍的计算量”。</blockquote><p></p><p></p><p>显然，生成式人工智能技术是不容忽视的。</p><p></p><p></p><h2>生成式人工智能的碳排放是否被夸大了？</h2><p></p><p></p><p>人们对生成式人工智能碳排放的担忧可能被放大了。我们要正确看待这个问题：全球整个科技行业的温室气体排放量占全球温室气体排放量的1.8%至3.9%，但其中只有一小部分是由人工智能[1]造成的。人工智能与航空或其他碳排放源的规模存在巨大差异：每天都会有许多汽车和飞机行驶数百万公里，但训练像GPT模型这样的现代人工智能模型的次数却相对较少。</p><p></p><p>诚然，我们尚不清楚究竟人类究竟训练了多少大型人工智能模型，这取决于我们如何定义“大型人工智能模型”。如果说大模型指的是GPT-3或更大规模的模型，那么可能只有不到1000个。我们来做一个简单的数学运算：</p><p></p><p></p><blockquote>根据最近的一项<a href="https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">估计</a>"，训练GPT-3排放了500吨的二氧化碳。Meta的LLaMA模型<a href="https://www.google.com/url?q=https://arxiv.org/pdf/2302.13971.pdf&amp;sa=D&amp;source=docs&amp;ust=1686736622103395&amp;usg=AOvVaw0EOH1_3alOGQm9Nh_pUddt&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">估计</a>"排放了173吨。训练1000个500吨的模型将涉及约50万吨二氧化碳的总排放量。新模型可能会在一定程度上增加排放量，但几乎可以肯定的是，这1000个模型的碳排放被高估了。2019年，商业航空业排放了约9.2亿吨二氧化碳[2]， 几乎是大模型训练的2000倍，而且这是将一年的航空业碳排放与多年的大模型训练碳排放进行的比较。大模型训练的碳排放仍然不容忽视，但这种戏剧性的比较具有误导性。我们需要更细致入微的思考。</blockquote><p></p><p></p><p>当然，这只是考虑到这些模型的训练。模型的服务和使用也需要能源，也会有相关的碳排放。<a href="https://towardsdatascience.com/the-carbon-footprint-of-chatgpt-66932314627d?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">一项分析</a>"表明， ChatGPT运行一年可能会排放约15000吨二氧化碳。<a href="https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">另一项分析</a>"则表明实际的排放量要要少得多，约为1400吨。尽管都不可忽略，但与航空业相比仍然微不足道。</p><p></p><p></p><h2>碳排放透明度</h2><p></p><p></p><p>但是，即使对人工智能碳排放的担忧有些言过其实，但仍然值得我们关注，特别是当生成式人工智能越来越多地融入到我们的现代生活中时。随着人工智能系统的不断演化和投用，我们需要关注它们对环境的影响。我们可以遵循许多成熟的实践，也有一些方法可以减少生成式人工智能的碳排放。</p><p></p><p>首先，透明度至关重要。我们建议提高透明度，便于监测与人工智能模型的训练和使用相关的碳排放。这将使那些部署这些模型的人以及最终用户能够根据人工智能的碳排放量做出明智的决定。同时，将人工智能相关的碳排放纳入温室气体清单和净零目标。这是<a href="https://foundation.mozilla.org/en/research/library/ai-transparency-in-practice/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">人工智能透明度</a>"的一个组成部分。</p><p></p><p>举个这方面的例子，法国<a href="https://www.euractiv.com/section/digital/news/new-law-forces-french-operators-to-disclose-carbon-footprint-to-public/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">最近通过了一项法律</a>"，要求电信公司就其在可持续性方面的措施提供透明度报告。我们也可以出台类似的法律，例如要求包含人工智能系统的产品向其客户报告碳排放信息，并要求模型提供商将碳排放数据集成到其API中。</p><p></p><p>更高的透明度可以带来更强的动力来建立节能的生成式人工智能系统，我们有很多方法可以提高效率。在InfoQ最近发布的一篇<a href="https://www.infoq.com/articles/impact-machine-learning-climate/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">文章</a>"中，微软高级软件工程师Sara Bergman呼吁人们注意人工智能系统的生命周期，并建议如何应用<a href="https://greensoftware.foundation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">绿色软件基金会</a>"的工具和实践来提高人工智能系统的能源效率，包括如何选择服务器硬件和架构，以及选择低碳密集型电力的时段和地区。而生成式人工智能为提高效率提供了一些独有的可能性。</p><p></p><p></p><h2>效率：能源使用与模型性能</h2><p></p><p></p><p>正如<a href="https://arxiv.org/pdf/2302.08476.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">碳计算：影响机器学习排放的因素调查</a>"中所探讨的那样，与训练或使用生成式人工智能模型相关的碳排放取决于许多因素，包括：</p><p></p><p>模型参数个数；量化(数值精度)；模型架构；使用GPU或其他硬件的效率；电力的碳密度。</p><p></p><p>后两个因素与其他软件一样，人们已经对其进行了探讨，例如我们上面提到InfoQ的<a href="https://www.infoq.com/articles/impact-machine-learning-climate/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">文章</a>"。因此，我们这里关注的是前三个因素，它们都涉及能源使用和模型性能之间的一些权衡。</p><p></p><p>效率的价值不仅体现在可持续性方面，更高效的模型可以在可用数据较少的情况下提供高效的表现性能、降低成本，并有在边缘设备上运行的可能性。</p><p></p><p></p><h3>模型参数个数</h3><p></p><p></p><p>OpenAI论文<a href="https://arxiv.org/pdf/2005.14165.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">Language Models are Few-Shot Learners</a>"中的一张图告诉我们，模型越大表现越好。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/30/306f6fe67271b428b99facc4841a4f9a.webp" /></p><p></p><p></p><p><a href="https://openreview.net/pdf?id=yzkSU5zdwD&amp;accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">大型语言模型的涌现能力</a>"中也提到了同样的观点：</p><p></p><p></p><blockquote>扩大语言模型已经被证明可以在广泛的下游任务上可预见地提高性能和样本效率。本文讨论的是一种不可预测的现象，我们称之为大型语言模型的涌现能力。如果一种能力在较小的模型中不存在，但在较大的模型中存在，我们称之为”涌现“。</blockquote><p></p><p></p><p>我们发现，不仅更大的模型在处理给定的任务时表现得更好，而且实际上只有当模型变得更大时，才会涌现出新的能力。这种能力涌现的例子包括对大数的加减、毒性分类和数学单词问题的思维链技术。</p><p></p><p>但是训练和使用更大的模型需要更多的计算，因此需要更多的能源。因此，我们可以看到模型的能力和性能与其计算强度之间的权衡，进而可以看出与碳密度的关系。</p><p></p><p></p><h3>量化</h3><p></p><p></p><p>我们对模型的<a href="https://huggingface.co/docs/optimum/concept_guides/quantization?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">量化</a>"进行了大量研究。我们在模型计算中使用低精度数字来降低计算密度，尽管会牺牲一些精度。它允许模型在普通的硬件上运行，例如，在消费级笔记本电脑上。减少计算量和降低精度之间的权衡通常是非常有利的，对于特定能力水平的计算，量化模型非常节能。还有一些相关的技术，如“<a href="https://aitechtrend.com/the-power-of-knowledge-distillation-in-creating-smaller-and-faster-models/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">蒸馏（Distillation）</a>"”，它使用较大的模型来训练小模型，这个小模型可以很好地完成给定的任务。</p><p></p><p>蒸馏技术需要训练两个模型，因此很可能会增加与模型训练相关的碳排放，不过它会通过减少模型在使用中的碳排放来弥补。对已训练好的模型进行蒸馏也是一个很好的解决方案，我们甚至可以同时利用蒸馏和量化来为给定的任务创建更高效的模型。</p><p></p><p></p><h3>模型架构</h3><p></p><p></p><p>模型架构对计算密度有很大的影响，因此选择更简单的模型可能是减少人工智能系统碳排放最有效的方法。GPT的Transformer非常强大，越是简单的架构就越可以有效地用于许多应用程序。像ChatGPT这样的模型被认为是“通用的”，这意味着这些模型可以被用于许多不同的应用程序。然而，对于相对固定的应用程序来说，就没有必要使用复杂的模型。为任务定制的模型可以使用更简单和更小的架构达到所需的性能，从而减少碳排放。另一种有用的方法是微调——论文<a href="https://arxiv.org/pdf/2205.05638.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a>"讨论了微调如何“提供更好的准确性以及显著降低计算成本”。</p><p></p><p></p><h2>将碳排放和精度指标放在一起</h2><p></p><p></p><p>“准确性”一词很容易让我们陷入“越多越好”的认知。我们需要明白的是，对于特定的应用程序来说——“适可而止”才是最好的。在某些情况下，可能需要最新和最好的模型，但对于有些应用程序来说，老的、较小的模型（可能是量化的模型）可能就完全足够了。在某些情况下，系统做出正确的行为可能需要所有可能的输入，但有些应用程序可能具有更强的容错能力。在正确地了解了所需的应用程序和服务级别之后，可以通过比较各种模型的性能和碳排放量来选择合适的模型。也可能存在使用一组模型的情况。默认情况下将请求传给更简单、更小的模型，对于简单模型无法处理的任务，可以将其传给更复杂的模型。</p><p></p><p>为此，我们需要将碳排放指标集成到DevOps(或MLOps)流程中。一些工具，如<a href="https://codecarbon.io/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">codecarbon</a>"，可以很容易地跟踪和计算与模型训练和服务相关的碳排放。将这个工具或类似的工具集成到持续集成测试套件中，可以同时分析碳排放、计算精度和其他指标。例如，在试验模型架构时，测试用例可以立即报告精度和碳排放，从而更容易找到正确的架构并选择正确的参数，以满足精度要求，同时最大限度地减少碳排放。</p><p></p><p>同样需要注意的是，实验本身也会导致碳排放。在MLOps周期的实验阶段，使用不同的模型和架构进行实验，以确定最佳选择，我们可以综合考虑准确性、碳排放和潜在的其他指标。从长远来看，随着模型不断接受实时数据的训练和/或投用，可以减少碳排放，但过度的实验会浪费时间和精力。做出权衡将取决于许多因素，但当碳排放指标可用于实验以及模型的训练和服务时，我们的工作会变得更容易。</p><p></p><p></p><h2>绿色提示词工程</h2><p></p><p></p><p>说到与生成式模型的服务和使用相关的碳排放时，提示词工程就变得不可忽视了。对于大多数生成式人工智能模型(如GPT)来说，使用的计算资源和碳排放取决于传给模型和由模型生成的文本节点的数量。</p><p></p><p>虽然具体的细节取决于实际的实现，但提示词通常会被“一次性全部”传给模型。这可能会使计算量看起来不依赖于提示词的长度，但实际上，自注意力机制特点决定了优化会抑制未使用的输入部分，这意味着更短的提示词可以节省计算量，从而节省能源。</p><p></p><p>对于输出，很明显，计算成本与生成的文本节点数量成正比，因为模型需要为生成的每个节点“再运行”一次。</p><p></p><p>这可以从访问OpenAI GPT4 API的费用中看出来。在撰写本文时，GPT4基础模型的成本为0.03美元每千个提示词节点和0.06美元每千个样本节点。提示词长度和输出节点的长度都包含在价格中，说明了两者都会影响所需的计算量。</p><p></p><p>因此，更短的提示词和生成更短的输出将使用更少的计算量。人们为此提出了一种“绿色提示词工程”提议。MLOps平台为此提供了一些实验性支持，这让在持续评估碳排放和系统性能影响的同时进行缩短提示词的实验变得相对容易。</p><p></p><p>除了提示词，人们还提出了一些有趣的方法，通过更复杂的使用方式来改进大模型的效率，如这篇<a href="https://arxiv.org/abs/2305.18323?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">论文</a>"所述。</p><p></p><p></p><h2>结论</h2><p></p><p></p><p>尽管人工智能的碳排放可能被夸大了，但仍然令人感到担忧，我们需要采取适当的措施来应对它们。提高透明度是支持有效决策和提高消费者意识的必要条件。此外，将碳排放指标集成到MLOps工作流中有助于在进行模型架构、规模、量化和绿色提示词工程时做出更明智的选择。本文的内容只是概述，只触及表面，对于那些真正想要做绿色生成式人工智能的人，可以关注<a href="https://arxiv.org/pdf/2209.00099.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">最新的研究</a>"。</p><p></p><p></p><h3>脚注</h3><p></p><p></p><p>[1] <a href="https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">"We’re getting a better idea of AI’s true carbon footprint" - by Melissa Heikkilä</a>"[2] <a href="https://www.statista.com/statistics/1056469/co2-emissions-commercial-aviation-industry-globally-by-operation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTUwMjUzNjQsImZpbGVHVUlEIjoiOTEzSk01Ukt3bmZCMTVBRSIsImlhdCI6MTY5NTAyNTA2NCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.dzn4Jpgtl1J2d4_4b7lCZ_s7o246PouGVktFXsSjQmw">Carbon dioxide emissions from the commercial aviation industry worldwide in 2019, by operation</a>"</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/articles/carbon-emissions-generative-ai/">https://www.infoq.com/articles/carbon-emissions-generative-ai/</a>"</p><p></p><p>相关阅读：</p><p><a href="https://www.infoq.cn/news/NuKxISZRb5sjg1lXgmeN">AI 大模型背后的惊人数字：问 ChatGPT 5 个问题，耗水 500 毫升？训练一次 GPT-3，碳排放量相当于开车往返月球？</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5eIYlyxDCEUz1M58oATm</id>
            <title>Amazon CodeWhisperer 与 Amazon Glue 实现集成，借助生成式 AI 进一步提升开发效率</title>
            <link>https://www.infoq.cn/article/5eIYlyxDCEUz1M58oATm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5eIYlyxDCEUz1M58oATm</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Sep 2023 10:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据对于企业, Amazon Glue, 亚马逊云科技, Amazon CodeWhisperer
<br>
<br>
总结: 亚马逊云科技推出了Amazon Glue，帮助用户在无服务器基础设施上集成多个来源的数据，用于分析、机器学习和应用程序开发。同时，亚马逊云科技还发布了AI编程助手Amazon CodeWhisperer，能够根据开发人员使用自然语言留下的注释和历史代码实时生成代码建议。现在，Amazon CodeWhisperer为Amazon Glue Studio notebook提供支持，帮助用户更快地进行数据集成工作。 </div>
                        <hr>
                    
                    <p>数据对于企业做出明智决策、提高运营效率和开展创新来说至关重要。集成不同来源的数据是一个复杂而耗时的过程。为此，亚马逊云科技推出了&nbsp;<a href="https://xie.infoq.cn/article/be592baac4db6412394a6573f">Amazon Glue</a>"，帮助用户在无服务器基础设施上集成多个来源的数据，用于分析、机器学习和应用程序开发。</p><p></p><p>Amazon Glue 为数据集成任务提供了完全不同的编写体验，而&nbsp;Notebook 是最常见的工具之一。数据科学家倾向于以交互方式运行查询，并立即检索结果，用于编写数据集成任务。这种交互体验可以加速构建数据集成任务的进度。</p><p></p><p>近期，亚马逊云科技宣布了 <a href="https://www.infoq.cn/video/4oajrgIyfmkaaNFi7dJF">Amazon CodeWhisperer</a>" 正式可用。这是一款 <a href="https://www.infoq.cn/article/CkLkSpx0p9egiR1XLsON">AI 编程助手</a>"，能够使用底层基础模型帮助开发人员提高工作效率。它可以根据开发人员使用自然语言留下的注释和 IDE（集成开发环境）中的历史代码实时生成代码建议。此外，亚马逊云科技还发布了&nbsp;Amazon CodeWhisperer Jupyter 扩展程序，为 Jupyter 用户在 Jupyter Lab 和 Amazon SageMaker Studio 中的 Python notebook 生成实时、单行或完整的函数代码建议。</p><p></p><p>现在，亚马逊云科技正式宣布&nbsp;Amazon CodeWhisperer 为 Amazon Glue Studio notebook 提供支持，帮助 Amazon Glue 用户优化使用体验、提高开发效率。通过 Amazon Glue Studio notebook，开发人员可以用自然语言（英语）编写特定任务，比如“利用 json 文件中的内容创建一个 Spark DataFrame”。基于此信息，Amazon CodeWhisperer 会直接在 notebook 中推荐一个或多个可完成此任务的代码片段。开发人员可以选择“接受最推荐的建议”，“查看更多建议”或“继续自己编写代码”。</p><p></p><p>Amazon Glue Studio notebook 与 Amazon CodeWhisperer 之间的集成可以帮助用户更快开展数据集成工作。该集成目前已在美国东部（北弗吉尼亚州）可用。用户现在就可以着手将 Amazon Glue Studio notebook 与 Amazon CodeWhisper 进行集成，以加快数据集成构建工作。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0egsdNL9lIRjk5gYG1QQ</id>
            <title>便携式大语言模型才是智能手机的未来</title>
            <link>https://www.infoq.cn/article/0egsdNL9lIRjk5gYG1QQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0egsdNL9lIRjk5gYG1QQ</guid>
            <pubDate></pubDate>
            <updated>Fri, 29 Sep 2023 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能手机, 人工智能聊天机器人, 大型语言模型, 私有设备
<br>
<br>
总结: 智能手机的创新停滞，但人工智能聊天机器人和大型语言模型的发展将使智能手机变得更好。私有设备上运行的大型语言模型将成为智能手机操作系统的核心功能，获取个人数据并不断改进自身，提供咨询、鼓励和警告。智能手机需要更多的RAM来运行这些面向个人的大型语言模型，以实现更智能的功能。 </div>
                        <hr>
                    
                    <p>本文最初发布于The Register博客。</p><p>&nbsp;</p><p>智能手机的创新已经停滞。不久前发布的iPhone 15确实带来了一些不错的功能。但在一段时间内，我的iPhone 13还是可以满足我的需求，我不会急于更换。我之前的iPhone用了四年。</p><p>&nbsp;</p><p>在这款手机之前，我有充分的理由购买来自库比蒂诺的年度升级版本。但现在，我们能从中得到什么呢？iPhone 15<a href="https://www.theregister.com/2023/09/12/apple_announces_iphone_15_lineup/">提供</a>"了USB-C接口、更好的摄像头和更快的无线充电。这些功能都很好，但对大多数用户来说却并不是必需的。</p><p>&nbsp;</p><p>然而，鉴于目前近乎疯狂的人工智能创新浪潮，智能手机很快也会变得更好。</p><p>&nbsp;</p><p>几乎每个拥有智能手机的人都可以通过App或浏览器访问“三大”人工智能聊天机器人——OpenAI的ChatGPT、微软的Bing Chat和谷歌的Bard。</p><p>&nbsp;</p><p>这已经很好了。不过，除了这些“通用”人工智能聊天机器人之外，一项由另一家大型科技巨头牵头的秘密工作似乎正在占据上风。</p><p>&nbsp;</p><p>早在2月份，Meta AI Labs就<a href="https://www.theregister.com/2023/02/25/ai_in_brief/">发布了LLaMA</a>"——这是一个训练数据集和参数数量都变小了的大型语言模型。对于大型语言模型的工作原理，我们在直觉上还是会将其与更多的参数和更大的容量等同起来——例如，人们认为GPT-4有一万亿甚至更多的参数，尽管OpenAI对这个数字守口如瓶。</p><p>&nbsp;</p><p>Meta的LLaMA只有区区700亿个参数，甚至有一个版本只有70亿个。</p><p>&nbsp;</p><p>那么，是不是说LLaMA只有GPT-4的千分之一呢？这就是有趣的地方。虽然LLaMA从来没有在任何基准测试中击败过GPT-4，但它并不差——在许多情况下，它已经不是一般的好了。</p><p>&nbsp;</p><p>LLaMA是按Meta的方式开源的，研究人员可以使用其工具、技术来训练模型并迅速作出显著的改进。仅仅在几周之内，就出现了<a href="https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/">Alpaca</a>"、Vicuna等大型语言模型，每一个都优化得比LLaMA还好——同时，在基准测试中也和GPT-4越来越接近。</p><p>&nbsp;</p><p>当Meta AI实验室在7月份<a href="https://www.theregister.com/2023/07/19/meta_llama_2/">发布LLaMA2</a>"的时候——许可不再那么以Meta为中心——成千上万的AI程序员开始针对各种用例对它进行调整。</p><p>&nbsp;</p><p>Meta AI实验室自己也不甘落后，他们几周前<a href="https://www.theregister.com/2023/08/25/meta_lets_code_llama_run/">发布了自己的微调版本Code LLaMA</a>"——内嵌到IDE中提供代码补全功能，或者简单地提供分析和修复代码。此后两天之内，一家名为<a href="https://www.phind.com/blog/code-llama-beats-gpt4">Phind</a>"的初创公司就将Code LLaMA优化为一个可以在单项基准测试中击败GPT-4的大型语言模型。</p><p>&nbsp;</p><p>这是第一次，<a href="https://www.theregister.com/2023/05/11/open_source_ai_makes_subscriptions_irrelevant/">算是对OpenAI、微软和谷歌的一次警告</a>"。看似“微小”的大型语言模型也可以足够好，同时还足够小，不必在飞机机库大小的云计算设施中运行，不用像那样消耗大量的电力和水资源。相反，它们可以在笔记本电脑甚至智能手机上运行。</p><p>&nbsp;</p><p>不是理论上可以。几个月来，我一直在iPhone 13上运行<a href="https://mlc.ai/mlc-llm/">MLC聊天应用</a>"。它运行有着70亿个参数的LLaMA2模型并没有什么问题。这个迷你模型不如有着130亿个参数的LLaMA2模型亮眼（但我的智能手机没有足够的内存来容纳它），但它在尺寸和性能之间做了很好的平衡。</p><p>&nbsp;</p><p>iPhone 15也没有——尽管苹果的规格说明书省略了有关RAM的细节信息。</p><p>&nbsp;</p><p>这些面向个人的大型语言模型——在私有设备上运行——将很快成为智能手机操作系统的核心功能。它们会获取你所有的浏览数据、活动和医疗数据，甚至是财务数据——所有我们今天交给云计算用来对付我们的数据——它们会不断改进自己，更准确地体现我们的精神、身体和财务状况。</p><p>&nbsp;</p><p>它们会咨询，会鼓励，会警告。它们不会取代大量的通用模型，但它们也不会将我们所有的个人数据泄露到云端。大多数智能手机已经有足够的CPU和GPU来运行这些面向个人的大型语言模型，但它们需要更多的RAM。只要多一点内存，我们的智能手机就能变得更加智能。</p><p>&nbsp;</p><p>原文链接：<a href="https://www.theregister.com/2023/09/13/personal_ai_smartphone_future/?td=rt-3a">https://www.theregister.com/2023/09/13/personal_ai_smartphone_future/?td=rt-3a</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DbfXLTXZ4WkiaGscCHWI</id>
            <title>埃森哲使用 Amazon CodeWhisperer 助力开发人员提高工作效率</title>
            <link>https://www.infoq.cn/article/DbfXLTXZ4WkiaGscCHWI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DbfXLTXZ4WkiaGscCHWI</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Sep 2023 07:24:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Amazon CodeWhisperer, AI 编程助手, 自然语言编写注释, IDE, 工作效率
<br>
<br>
总结: Amazon CodeWhisperer 是一款AI编程助手，可以根据开发人员使用自然语言编写的注释和IDE中的代码生成建议，帮助开发人员提高工作效率。通过实时代码建议，开发人员可以在IDE中专注地工作，更快地完成编码任务。 </div>
                        <hr>
                    
                    <p><a href="https://www.infoq.cn/article/JcIQOLpgqVK3AAgQxNQt">Amazon CodeWhisperer</a>" 是一款 AI 编程助手，可根据开发人员使用自然语言编写的注释和 IDE（集成开发环境）中的代码生成建议，帮助开发人员提高工作效率。借助 CodeWhisperer，开发人员无需在 IDE 与文档或开发者论坛之间切换，加快编码过程。通过 CodeWhisperer 的实时代码建议，开发人员可以在IDE中专注地工作，更快地完成编码任务。</p><p></p><p>CodeWhisperer 由基于数十亿行代码训练的<a href="https://www.infoq.cn/article/h5zqC9Cq8UK4iOKrwPc7">大语言模型（LLM）</a>"赋能，已经学会使用15种编程语言编写代码。开发人员仅需编写注释，用简明的英语概述一个特定任务即可，例如“uploada file to Amazon S3”（上传文件到 Amazon S3）。在此基础上，CodeWhisperer 可自动确定适合于该指定任务的云服务和公共库，即时构建特定代码，并直接在IDE中提供一段代码建议。此外，CodeWhisperer 能够与 Visual Studio Code和JetBrains 等 IDE 无缝集成，使开发人员可以专注于开发，且无需离开 IDE。截至目前，CodeWhisperer支持的开发语言包括 Java、Python、JavaScript、TypeScript、C#、Go、Ruby、Rust、Scala、Kotlin、PHP、C、C++、Shell 和 SQL。</p><p></p><h3>埃森哲使用 CodeWhisperer 助力开发人员提高工作效率</h3><p></p><p></p><p>“埃森哲正在使用 Amazon CodeWhisperer 加快编码任务，这是我们 Velocity 平台软件工程最佳实践计划的一部分。”埃森哲技术架构高级经理 Balakrishnan Viswanathan 表示，“Velocity 团队在想方设法提高开发人员的工作效率，搜寻过多种工具后，发现 Amazon CodeWhisperer 可以帮助减少30%的开发工作量。因此，我们可以更专注于安全、质量和性能的提升。”</p><p></p><h3>CodeWhisperer的优势</h3><p></p><p></p><p>埃森哲Velocity团队一直在使用 CodeWhisperer 来加速其人工智能（AI）和机器学习（ML）项目。使用 CodeWhisperer 带来了如下优势：</p><p></p><p>团队减少创建样板代码和重复代码模式的时间，从而将更多时间用于提升软件质量等重要的工作上CodeWhisperer 助力开发人员负责任地使用 AI，创建语法正确且安全可靠的应用程序团队可以生成完整的函数和符合逻辑的代码段落，无需在网上搜索或定制代码可以帮助新手开发人员或使用不熟悉代码库的开发人员快速上手工作通过将安全扫描前置到开发人员的 IDE 中，让团队可以在开发过程的早期阶段就检测安全威胁</p><p></p><h3>帮助开发人员尽快熟悉新项目</h3><p></p><p></p><p>CodeWhisperer可以帮助不了解亚马逊云科技的开发人员更快地熟悉使用亚马逊云科技服务开发的项目。例如，借助 CodeWhisperer，埃森哲新的开发人员就能够为 Amazon Simple Storage Service（Amazon S3）和 Amazon DynamoDB 等亚马逊云科技服务编码。在短时间内，他们就能够高效工作并为项目做出贡献。CodeWhisperer 通过提供代码段落或逐行建议来辅助开发人员完成工作。此外，CodeWhisperer 还能理解上下文。指令（注释）越具体，CodeWhisperer 生成的代码越相关。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c8ebf09fb162a7c70ada7af68353bdd3.png" /></p><p></p><p></p><h3>编写样板代码</h3><p></p><p></p><p>开发人员可以使用 CodeWhisperer 补全先决条件。他们只需输入“为机器学习数据创建预处理脚本的类”，就能够创建预处理数据类。开发人员只需几分钟编写预处理脚本，然后 CodeWhisperer 就能够生成整个代码段落。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d689fe99bb8981fba9af145eba88728.png" /></p><p></p><h3>帮助开发人员使用不熟悉的语言编写代码</h3><p></p><p></p><p>一个新加入团队的 Java 开发人员可以借助 CodeWhisperer 轻松编写 Python 代码，而不必担心语法问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28450a1729c977ec681599213f92d95e.png" /></p><p></p><h3>检测代码的安全漏洞</h3><p></p><p></p><p>开发人员可以在 IDE 中选择“运行安全扫描”来检测安全问题。发现的安全问题的详细信息会直接显示在 IDE 中。这可以帮助开发人员及早检测和修复问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d169fabf86d6fb122ec4f3d92d8c675b.png" /></p><p></p><p>“作为一名开发人员，CodeWhisperer 能够让您更加快速地编写代码”埃森哲人工智能工程顾问 Nino Leenus 表示，“此外，CodeWhisperer 借助人工智能可帮助消除拼写错误及其他典型错误，让编码更准确。对于开发人员来说，多次编写同样的代码乏味而枯燥。通过建议后续可能需要的代码片段，<a href="https://www.infoq.cn/article/C6ZjsPGuFWk6LBP7i48E">AI 代码补全技术</a>"可以减少这类重复性工作。”</p><p></p><p>现在，用户可以在喜欢的 IDE 中激活 CodeWhisperer。CodeWhisperer 可根据现有的代码和注释自动生成代码片段建议。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/di421fc7YtuJhSvQ5vjV</id>
            <title>第四范式成功登陆港股，开盘涨13.49%</title>
            <link>https://www.infoq.cn/article/di421fc7YtuJhSvQ5vjV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/di421fc7YtuJhSvQ5vjV</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Sep 2023 06:18:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 第四范式, 人工智能公司, 香港联合交易所, 上市
<br>
<br>
总结: 中国最大的以平台为中心的决策类人工智能公司第四范式在香港联合交易所主板上市。第四范式是今年迄今为止港股市值最大的TMT项目，也是近两年来第一家登陆港交所的AI独角兽。公司计划利用募集资金加强基础研究、技术能力和解决方案开发，扩展产品和进入新的行业领域，寻求战略投资和收购机会，以实现长期增长战略。第四范式致力于助力企业智能化转型和推进人工智能发展，成为决策类AI平台领先企业。 </div>
                        <hr>
                    
                    <p>2023年9月28日，中国最大的以平台为中心的决策类人工智能公司第四范式，正式于香港联合交易所主板挂牌上市，股份代号为6682.HK。</p><p>&nbsp;</p><p>首日第四范式开盘涨13.49%，报于每股63.1港元，对应总市值达293亿港元。这一市值也使得第四范式成为今年迄今为止港股市值最大的TMT项目，同时也是近两年来第一家登陆港交所的AI独角兽。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b052bac181aaf3f10d6ecbeedc26e1e.png" /></p><p></p><p>据悉，第四范式在本次IPO中总计发售18,396,000股股份。其中，香港公开发售部分获约11.4倍超额认购；国际发售部分获约1.57倍超额认购。新华资本管理有限公司、中关村科学城公司以及澜起科技作为基石投资者参与本次发行，累计认购近亿美元，以示对第四范式长期价值的看好。</p><p></p><p>第四范式本次募集资金净额约为8.36亿港元。募集资金净额将用于：(1) 在未来三年分配至加强公司的基础研究、技术能力和解决方案开发。(2) 在未来三年分配至扩展公司的产品、建立公司的品牌及进入新的行业领域。(3) 在未来三年分配至寻求战略投资和收购机会，从而实施公司的长期增长战略，以开发公司的解决方案并扩展及渗透公司所覆盖的垂直行业。(4) 用作一般企业用途。</p><p></p><p>第四范式成立于2014年，专注于提供以平台为中心的人工智能解决方案，并已在金融、零售、制造、能源与电力、电信、运输、科技、教育、媒体、医疗保健等场景中落地。</p><p>&nbsp;</p><p>2023年3月，第四范式推出了专为企业业务场景设计的生成式人工智能产品“式说”（SageGPT），“式说”具有多模态大模型互动能力及企业级人工智能工具特性。 </p><p>&nbsp;</p><p>第四范式董事会主席兼CEO戴文渊表示：“第四范式长期致力于助力企业智能化转型和推进人工智能发展，成长为决策类AI平台领先企业。今天在港交所成功挂牌上市，对第四范式而言是一个重要的里程碑。未来，我们将继续巩固自身技术壁垒，不断提升商业化能力，持续助力客户业务成功。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xC4osqfB6XVbkybGJ1Kf</id>
            <title>腾讯云数据库凭借这项创新再获顶会认可，论文入选VLDB2023</title>
            <link>https://www.infoq.cn/article/xC4osqfB6XVbkybGJ1Kf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xC4osqfB6XVbkybGJ1Kf</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Sep 2023 06:06:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库国际顶会VLDB大会, TDSQL, PolySI, Tesseract
<br>
<br>
总结: 近日，腾讯云TDSQL的两篇论文成功被数据库国际顶会VLDB大会收录。其中一篇论文介绍了PolySI，一种高效的黑盒检查器，用于检测快照隔离（SI）数据异常。另一篇论文介绍了Tesseract，一种在线和事务性模式演化方法，用于解决数据库在线修改schema过程中的挑战。这些创新技术的出现为数据库领域带来了新的解决方案。 </div>
                        <hr>
                    
                    <p>近日，InfoQ获悉，在第49届数据库国际顶会VLDB大会上，来自腾讯云TDSQL的2篇论文成功被VLDB 2023收录，创新技术再次被国际顶级会议VLDB认可。</p><p>&nbsp;</p><p>作为数据库领域的三大顶级会议之一，VLDB每届会议都集中展示了当前数据库研究的最前沿方向以及工业界的最新应用，吸引了众多全球顶级科技公司和研究机构的参与。因会议对系统创新性、完整性、实验设计等方面都要求极高，VLDB会议的论文接受率总体较低（约 18%）。</p><p>&nbsp;</p><p>入选论文中，腾讯云与南京大学、苏黎世联邦理工学院(ETH)&nbsp;合作研发的《Efficient Black-box Checking of Snapshot Isolation in Databases》解决方案，提出了一种新颖的黑盒检查器——PolySI，它能高效地检查快照隔离（Snapshot isolation，SI），并在检测到违规时提供可理解的反例。</p><p>&nbsp;</p><p>快照隔离是一种常见的弱隔离级别，它避免了串行化所带来的性能损失，同时可以防止很多常见的数据异常。然而，某些声称提供快照隔离保证的生产云数据库仍会产生SI数据异常，尤其在金融领域，会造成巨大影响。业界现有同类工具要么不支持快照隔离级别的测试，要么效率较低。鉴于数据库系统的复杂性，以及通常无法获取数据库内部信息的现状，业内亟需一种黑盒快照隔离检查器。</p><p>&nbsp;</p><p>为了解决该问题，我们提出并设计了“PolySI”算法与工具。PolySI的理论基础是基于广义多图（Generalized Polygraphs，GPs）的SI刻画定理，该定理保证了PolySI的正确性与完备性。PolySI采用SMT求解器（MonoSAT），并利用GPs的紧凑约束编码方案以及领域特定优化加速SMT求解。</p><p>&nbsp;</p><p>目前，通过广泛的评估，PolySI成功地重现了已知的SI异常，并在三个生产云数据库中检测到了新的SI异常、提供了可理解的反例。PolySI在多类工作负载下均优于目前最先进的SI黑盒检查器，并能够扩展到大规模工作负载。</p><p>&nbsp;</p><p>据了解，腾讯云与西蒙菲莎大学(Simon Fraser University)联合完成的《Online Schema Evolution is (Almost) Free for Snapshot Databases》论文，则介绍了“Tesseract”，一种新的在线和事务性模式演化方法，主要用于解决数据库在线修改schema过程中存在的挑战。</p><p>&nbsp;</p><p>当前，现代数据库应用经常根据不断变化的需求进行模式更改，数据库在线修改schema的主要优势在于，无需停止数据库服务或中断正在进行的事务，即可进行结构修改，这使得数据库能够在满足动态变化需求的同时，无需停机维护或重新启动数据库。</p><p>&nbsp;</p><p>但诸多问题也随之而来，在现有数据库系统中，支持在线和事务性模式（schema）演化仍然具有挑战性，如数据一致性，在进行结构修改时，为确保数据的一致性，需要使用事务或其他机制来保证数据的完整性和正确性；其次是长时间运行，某些结构修改预计需要较长的时间来完成，特别是对大型数据库或复杂结构的修改，导致对数据库性能产生一定的影响，因此需要在合适的时间窗口进行修改，以最小化对业务的影响。</p><p>&nbsp;</p><p>在以往的解决方案中，通常采用临时方法对模式演化进行“补丁”应用于现有系统，导致许多边缘情况和功能不完整。因此，应用程序通常不得不仔细安排停机时间进行模式更改，从而牺牲可用性。</p><p>&nbsp;</p><p>“Tesseract”的出现则有效避免了上述缺点。在广泛使用的多版本数据库系统中，模式演化可以被建模为改变整个表的数据修改操作，即数据定义即修改（DDaM）。这使得Tesseract可以通过利用并发控制协议几乎“免费”地支持模式。</p><p>&nbsp;</p><p>在Tesseract应用测试中，通过对现有快照隔离协议进行简单调整，在40核服务器上的工作负载下，Tesseract能够提供在线、事务性的模式演化，而无需服务停机，并在模式演化进行时保持高应用性能。</p><p></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LWyly6ZX8NrO37FQu22Y</id>
            <title>ChatGPT 再迎重大升级！终于“联网”，不再局限于旧数据，新功能即将对所有人开放</title>
            <link>https://www.infoq.cn/article/LWyly6ZX8NrO37FQu22Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LWyly6ZX8NrO37FQu22Y</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Sep 2023 06:00:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, ChatGPT, 必应搜索引擎, 图像识别功能
<br>
<br>
总结: OpenAI宣布ChatGPT可以通过必应搜索引擎进行网络搜索，并且加入了图像识别功能，使得用户可以通过上传图像进行对话和交流。 </div>
                        <hr>
                    
                    <p>当地时间周三（9月27日），OpenAI在X（前身为推特）上宣布，其聊天机器人产品ChatGPT可以通过微软的必应搜索引擎进行网络搜索，将不再局限于2021年9月之前的数据。</p><p>&nbsp;</p><p>OpenAI称：“现在ChatGPT Plus和Enterprise（企业版） 用户可以使用浏览功能，将很快扩展到所有用户。要启用，请在GPT-4下的选择器中选择‘使用必应浏览’（ Browse with Bing）。”</p><p>需要说明的是，OpenAI早些时候测试了相关功能，允许Plus用户通过必应搜索访问最新信息，但后来因担心用户绕过付费墙，禁用了这项功能。</p><p></p><p>值得一提的是，OpenAI本周早些时候还宣布了另一项重大更新，将使ChatGPT可以通过图片和语音命令交互。</p><p></p><h2>ChatGPT再迎重大升级：“能看、能听，也能说”</h2><p></p><p>&nbsp;</p><p>本周一，OpenAI宣布对ChatGPT进行重大更新，使其GPT-3.5和GPT-4两大AI模型能够分析图像内容，并在文本对话中据此做出反应。OpenAI方面表示，ChatGPT移动版应用还将引入语音合成选项，在与现有语音识别功能配合使用时，能够与AI助手进行全口语对话。</p><p>&nbsp;</p><p>OpenAI也强调，语音合成功能目前仅适用于iOS和Android平台，而图像识别则将登陆Web版和移动版应用。</p><p>&nbsp;</p><p>OpenAI解释称，ChatGPT中的全新图像识别功能允许用户基于GPT-3.5或GPT-4模型，根据上传的一张或多张图像开展对话。该公司在其宣传博文中宣称，这项功能能够对接各类日常应用，例如为冰箱和食品储藏室拍摄照片以确定晚餐吃点什么，还有排除烧烤炉出故障的原因。该公司还提到，用户可以使用设备的触控屏圈出自己希望ChatGPT重点关注的部分。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3ecbe3dd514cb5fac3cf128f52ebbf1.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9ded80c7d4ae015cccf9d974c0d2883e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/51/515adb30560e8a8c090846a0c0b5fd18.png" /></p><p></p><p>&nbsp;OpenAI宣传视频中的画面，ChatGPT在分析用户照片以帮助其调整自行车座高。</p><p>&nbsp;</p><p>在官方网站上，OpenAI发布了一段宣传视频（<a href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak">https://openai.com/blog/chatgpt-can-now-see-hear-and-speak</a>"），展示了与ChatGPT的交流过程。其中用户询问要如何升高自己的自行车座垫，并上传了车辆、说明手册以及工具箱的照片。ChatGPT迅速做出反应，并为用户提供了完成调整过程的说明。我们还没有亲自测试过此功能，因此不太清楚实际效果是否真有这么惊艳。</p><p>&nbsp;</p><p>那这一切到底是怎么实现的？OpenAI尚未发布GPT-4或其多模态版本GPT-4V的底层运行细节。但根据其他厂商（包括OpenAI合作伙伴微软）的已知AI研究，多模态AI模型往往能够将文本和图像转化为共享编码空间，借此通过同一套神经网络处理多种类型的数据。OpenAI可以使用CLIP来弥合视觉与文本数据间的差异，从而在同一潜在空间（一种表达数据关系的向量化网络）上实现图像和文本表示对齐。正是这项技术，让ChatGPT具备了跨文本和图像进行上下文推理的能力——当然，这一切都只是外界的推测。</p><p>&nbsp;</p><p>与此同时，报道还指出ChatGPT的全新语音合成功能允许用户与其进行直接对话，而且此功能由OpenAI的“新文本转语音模型”驱动。尽管文本转语音技术已经相当成熟，但该公司表示在此功能推出之后，用户可以在应用端的设置中选择语音对话，之后从五种不同的合成语音中做出选择，具体包括“Juniper”、“Sky”、“Cove”、“Ember”和“Breeze”几个选项。OpenAI称这些声音均是与专业配音演员合作开发而来。</p><p>&nbsp;</p><p>OpenAI的Whisper是一套开源语音识别系统，此次也由它继续负责对用户语音输入的转录。Whisper于今年5月正式与ChatGPT iOS版应用集成，随后在7月登陆ChatGPT的Android版应用。</p><p></p><h2>“请注意，ChatGPT给出的结果不一定准确”</h2><p></p><p>OpenAI于今年3月公布GPT-4时，就曾经展示过该模型的“多模态”功能，据称可以处理文本和图像输入。但在随后的测试阶段，公众一直无缘真正体验其图像功能。期间OpenAI与Be My Eyes合作开发了一款可以为盲人描述场景照片的应用。今年7月，有报道称OpenAI的多模态功能之所以迟迟未能发布，主要是受到隐私问题的影响。与此同时，微软则于7月匆忙在基于GPT-4的AI助手Bing Chat中启用了图像识别功能。</p><p>&nbsp;</p><p>在最近的ChatGPT更新公告中，OpenAI称其扩展功能仍有一些限制，并承认该模型仍可能出现潜在的视觉混淆（即对某些内容的错误识别）、对非英语语种无法完美识别等问题。该公司表示，他们已经“在极端场景和纯科学验证角度”对新功能进行了风险评估，同时征求了alpha版本内测人员的意见，目前的观点仍然是建议谨慎使用，特别是在科学研究等高风险或专业性较强的背景之下。</p><p>&nbsp;</p><p>鉴于在开发Be My Eyes应用时遇到的隐私问题，OpenAI表示已经采取“技术措施来尽量限制ChatGPT对人类对象做分析和直接描述的能力。因为ChatGPT给出的结果不一定准确，AI系统应当尊重个人隐私。”</p><p>&nbsp;</p><p>尽管仍有种种缺陷，但OpenAI在营销材料中还是强调ChatGPT如今已经“能看、能听，也能说”。当然，并不是每个人都能认同这种充满拟人倾向的炒作宣传。Hugging Face公司AI研究员Sasha Luccioni博士就在X上发推称，“别再像看待人类那样看待AI模型了。ChatGPT根本就没法看、没法听，也没法说。它只能跟各种传感器相集成，以不同于人类的方式接收和发出信息。”</p><p>&nbsp;</p><p>虽然ChatGPT及其底层AI模型还远远算不上“人”，但如果本次公布的结果不假，那也至少代表着OpenAI的这款虚拟助手实现了巨大的功能增强。</p><p>&nbsp;</p><p>此外，OpenAI也强调了推迟开放有其充分理由：“我们认为应该逐步推出自己的工具，这样我们才能随时间推移不断改进并完善风险缓解措施，同时也让大家能为未来更强大的AI系统做好准备。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/OpenAI">https://twitter.com/OpenAI</a>"</p><p><a href="https://arstechnica.com/information-technology/2023/09/chatgpt-goes-multimodal-with-image-recognition-and-speech-synthesis/">https://arstechnica.com/information-technology/2023/09/chatgpt-goes-multimodal-with-image-recognition-and-speech-synthesis/</a>"</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2Kg4JCGdZWc632qllp9t</id>
            <title>Llama 生态现状：下载量超过 3000 万次，GitHub 上有超 7000 个相关项目</title>
            <link>https://www.infoq.cn/article/2Kg4JCGdZWc632qllp9t</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2Kg4JCGdZWc632qllp9t</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Sep 2023 02:04:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, Llama, Hugging Face, AWS
<br>
<br>
总结: Meta发布了Llama系列模型，其中Llama 2在云平台上得到广泛应用，AWS成为首个托管API合作伙伴。Llama模型在创新企业和开发者社区中得到了广泛使用和评估。社区通过微调和发布衍生品对Llama进行了优化，提高了性能。此外，Llama得到了各大硬件平台的支持。Meta将继续关注多模态、安全责任和社区，为人工智能提供开放的方法。 </div>
                        <hr>
                    
                    <p><a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">Meta发布Llama 1</a>"至今已经 7 个月左右，而<a href="https://ai.meta.com/blog/llama-2/">Llama 2</a>"以及<a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">Code Llama</a>"的发布也有了数月的时间。当地时间9月27日，Meta发布博文系统介绍了当前 Llama 生态的情况。官方统计，通过 Hugging Face 的 Llama 模型下载量超过 3000 万次，其中仅在过去 30 天就超过了 1000 万次。</p><p>&nbsp;</p><p>当前的Llama 生态情况如下：</p><p>&nbsp;</p><p>云厂商：AWS、Google Cloud 和 Microsoft Azure 等主要平台已在其平台上采用了 Llama 模型，并且 Llama 2 在云中的使用正在扩大。Google Cloud 和 AWS 总共有超过 3,500 个企业项目基于 Llama 2 模型启动。此外，Meta 还宣布 AWS 成为 Llama 2 的首个托管 API 合作伙伴，各种规模的组织都可以访问 Amazon Bedrock 上的 Llama 2 模型，而无需管理底层基础设施。</p><p>&nbsp;</p><p>创新企业：包括 Anyscale、Replicate、Snowflake、LangSmith、Scale AI 等在内，已有上万家初创公司正在使用或评估 Llama 2。“像 DoorDash 这样的创新者正在使用它进行大规模试验，然后再发布由LLM支持的新功能。”</p><p>&nbsp;</p><p>众包优化：截至目前，社区已在Hugging Face上微调并发布了7000多个衍生品。平均而言，在标准基准测试中，它们将常见基准测试的性能提高了近 10%，对于 TruthQA 等基准数据集的性能提升高达 46%。</p><p>&nbsp;</p><p>开发者社区：GitHub 上现在有超过 7,000 个基于 Llama 构建或提及 Llama 的项目。为方便将 Llama 引入边缘设备和移动平台，新的工具、部署库、模型评估方法，甚至 Llama 的“微型”版本都在开发中。此外，社区还扩展了 Llama 以支持更大的上下文、增加了对其他语言的支持等。</p><p>&nbsp;</p><p>硬件支持：各大硬件平台AMD、Intel、Nvidia、Google均通过软硬件优化提升了Llama 2的性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/6123307306e980cfff4e10cfa74fbb84.png" /></p><p>对于Llama 生态的未来，Meta表示，仍致力于为当今的人工智能提供开放的方法，主要会在多模态、安全责任和社区上投入更多关注。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://ai.meta.com/blog/llama-2-updates-connect-2023/">https://ai.meta.com/blog/llama-2-updates-connect-2023/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ad5b3a17e30bbacebaaa95034</id>
            <title>探索AI技术对古彝文保护与研究应用</title>
            <link>https://www.infoq.cn/article/ad5b3a17e30bbacebaaa95034</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ad5b3a17e30bbacebaaa95034</guid>
            <pubDate></pubDate>
            <updated>Thu, 28 Sep 2023 01:14:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 古彝文, 彝族, 文字系统, 文化意义
<br>
<br>
总结: 古彝文是彝族使用的一种古老文字系统，具有悠久的历史和独特的文化意义。它是世界上最古老的文字之一，起源可以追溯到公元前13世纪左右。古彝文是一种表音文字系统，每个字符代表一个音节或一个音节的组合。通过研究古彝文，人们可以了解到彝族人民的生活方式、价值观念和社会结构。古彝文的保护和研究对于彝族文化的传承和发展具有重要意义。 </div>
                        <hr>
                    
                    <p></p><h1>一、古彝文</h1><p></p><p></p><h2>1.1 古彝文介绍</h2><p></p><p>古彝文是彝族使用的一种古老文字系统，彝族是中国的少数民族之一，主要分布在中国西南地区。古彝文具有悠久的历史和独特的文化意义，被认为是世界上最古老的文字之一。</p><p></p><p>古彝文的起源可以追溯到公元前13世纪左右，据信是由古代彝族人民创造和使用的。它是一种表音文字系统，每个字符代表一个音节或一个音节的组合。古彝文的书写方式是从上到下、从左到右，类似于竖排的文字。它的形状多样，有直线、弯曲、斜线等不同的组合，形成了独特的图形。</p><p></p><p>古彝文的内容涵盖了丰富的彝族文化和历史信息，包括祭祀、婚姻、宗教、传统习俗等方面。通过研究古彝文，人们可以了解到彝族人民的生活方式、价值观念和社会结构。古彝文也是研究彝族历史和文化的重要线索和工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0ef06b4876a57028448005b08dfa1d97.png" /></p><p></p><h2>1.2 古彝文古籍保护背景</h2><p></p><p>古彝文的研究对于彝族文化的保护和传承具有重要意义。通过深入研究古彝文，人们可以更好地理解彝族文化的独特之处，并促进彝族文化的传统与现代的交流与融合。</p><p></p><p>随着现代科技的进步，人们开始探索利用人工智能和计算机技术来识别和研究古彝文。合合信息与上海大学社会学院签署校企合作协议，通过将人工智能和计算机视觉技术应用于古彝文识别，可以更快速、准确地解读古彝文文献，并将其数字化保存，助力推动古彝文古籍保护和研究。</p><p></p><h2>1.3古彝文识别的重难点</h2><p></p><p>古彝文识别的重难点主要包括以下几个方面：</p><p></p><p>1.数据样本稀缺性：由于古彝文的使用较为有限，古彝文的数据样本相对稀缺。这使得训练和优化古彝文识别模型变得困难，因为需要大量的样本数据来训练模型以提高识别准确性。因此，缺乏充足的古彝文数据样本是古彝文识别的一个重要难点。</p><p></p><p>2.古籍修复：由于很多彝族文献遭到破坏和流失，存在缺失、污渍、模糊、噪声干扰等现象，像这样：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a8c5db1f8859507fff231f7865c4b6b.png" /></p><p></p><p>3.字符形状多样性：古彝文的字符形状非常多样，包括直线、弯曲、斜线等不同的组合。这使得古彝文的字符识别变得复杂，因为不同的字符可能具有相似或相同的形状，而相似的字符可能具有不同的语义。因此，准确地区分和识别古彝文字符的形状是一个重要的难点。</p><p></p><p>4.字符数量和组合规则：古彝文字符的数量较多，约有600个以上的字符。而且，古彝文的字符通常是由多个基本形状组合而成的，这种组合规则也具有一定的复杂性。因此，要准确地识别古彝文字符，需要对字符的数量和组合规则进行深入的研究和理解。</p><p></p><p>5.字词辨别和语义理解：古彝文的词汇和语义理解也是一个挑战。由于古彝文是表音文字系统，一个字符可能代表一个音节或一个音节的组合。因此，对于词句的辨别和语义理解需要结合上下文信息和语言学知识。这对于古彝文的自动识别和翻译来说是一个重要的难点。</p><p></p><p>为了应对这些重难点，古彝文识别需要结合人工智能和计算机视觉技术，如深度学习、图像处理和自然语言处理等。通过建立大规模的古彝文数据库、优化识别算法和加强语义理解，可以提高古彝文识别的准确性和效率。此外，加强对古彝文的研究和保护，提高对古彝文的认知和使用，也是解决古彝文识别难题的重要途径。</p><p></p><h1>二、AI技术助力古文识别应用</h1><p></p><p>作为世界上最古老的文字之一，古彝文是中华文明地图上神秘而耀眼的印记。合合信息联合上海大学、华南理工大学团队针对现有的西南彝志、云贵一带古彝文字符开展统一编码，并于近期发布了业内首个古彝文基础编码数据库（简称“数据库”）。</p><p></p><p>该数据库包含上千个古彜文基础编码，通过API数据接口等形式，该数据库有望帮助高校研究人员、文化工作者、兴趣爱好者等人群快速找到古彝文在字典中的读音、汉语释义、用法，如同“大字典”一般，帮助人们降低古彝文书籍、文献阅读的门槛，以数字化手段助力传统文化保护、创新之路。</p><p></p><p>研究古彝文字集，有助于理解尚未被翻译成汉文、用字尚未规范化的古籍，更深层、透彻地作用于传统文化保护，同时通过建立古彝文数据库，填补当前国内外研究的空白。合合信息与华南理工大学共同成立文档图像分析识别与理解联合实验室，联合上海大学社会学院，共同解决数据库建设中的学术性、技术性难点。</p><p></p><p>合合信息与上海大学将合力完成以《西南彝志》为中西的贵州古彝文图像识别及数字化校对工作，帮助后续古彝文的检测、识别、标注，利用旗下扫描全能王的智能高清滤镜技术也可以进行古彝文的古籍修复。</p><p></p><h2>2.1 智能高清滤镜技术</h2><p></p><p>智能高清滤镜技术可智能检测图像中存在的问题，自动判定图像优化方式，实现模糊、阴暗、手指等干扰因素全处理。传统古籍问卷存在水迹、残旧、破损等情况，通过智能高清滤镜能够去除相关痕迹复现高清文档并开展识别。</p><p></p><p>从而增强文字的可读性，为接下来的文字信息提取、识别创造了良好的条件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fde30d10f8d38ffad4fcd4bc5237a467.png" /></p><p></p><h2>2.2 基于深度学习的复杂场景文字识别技术</h2><p></p><p>基于深度学习的复杂场景文字识别技术是一种能够自动识别和提取复杂场景中的文字信息的技术。它可以应对各种复杂的场景，如模糊、扭曲、光照不均、背景干扰等，实现高准确度的文字识别。这种技术的核心是深度学习模型，通常使用卷积神经网络（Convolutional Neural Network，CNN）和循环神经网络（Recurrent Neural Network，RNN）的结合来处理图像中的文字。整个过程可以分为三个主要步骤：文本检测、文本定位和文本识别。</p><p></p><p>1、首先是文本检测：它的目标是在图像中定位出文字的位置。通常使用卷积神经网络来进行文本区域的检测，网络会学习到图像中文字的特征，通过滑动窗口或区域提议的方式来检测可能的文字区域。</p><p></p><p>2、接下来是文本定位：这一步是为了更精确地定位出文字的位置。通常使用回归模型或者基于锚点的方法来对文本区域进行精确定位，以获得更准确的文字边界框。</p><p></p><p>3、最后是文本识别：这一步是将文字从图像中提取出来并进行识别。通常使用循环神经网络（如长短时记忆网络，LSTM）来对文字进行识别，网络会学习到文字的上下文信息，从而提高识别的准确度。</p><p></p><p>4、此外，为了提高复杂场景文字识别的准确度，还可以采用一些技巧和策略，如数据增强、多尺度处理、注意力机制等。数据增强可以通过旋转、缩放、扭曲等方式生成更多的训练样本，提高模型的泛化能力。多尺度处理可以通过在不同尺度下对图像进行处理，提高对不同大小文字的适应能力。而注意力机制可以帮助模型更关注重要的文字区域，减少背景干扰对识别结果的影响。</p><p></p><p>古彝文项目将根据上海大学古彝文研究员设计的四字节编码系统，引入合合信息智能文字识别技术，对异体字、变体字、误用字和混用字等进行标注、识别、比对，并由此建立起精确的彝文古籍电子数据库，识别标注效果如下所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b6eec476252e8fbc94fb22b1df29f577.png" /></p><p></p><h2>2.3 自然语言处理（NLP）技术</h2><p></p><p>自然语言的语义理解是指对自然语言文本中的意义和语义进行理解和解析的过程。它是自然语言处理（NLP）中的一个重要研究方向，旨在使计算机能够准确地理解和推断文本的含义，从而实现更高级别的语言处理任务。</p><p></p><p>注意力机制在语义理解中可以发挥重要作用，下面是一个基于注意力机制语义理解的实现过程：</p><p></p><p>数据预处理：首先，需要对古彝文数据进行预处理。这包括分词、词性标注、句法分析等步骤，以便将古彝文转换为计算机可以理解的形式。建立词嵌入模型：将古彝文中的每个字或词映射为一个高维向量表示，可以使用预训练的词嵌入模型（如Word2Vec、GloVe等）或自定义的古彝文词嵌入模型。构建编码器-解码器模型：使用Transformer作为编码器-解码器模型的基础架构。编码器将输入的古彝文序列转换为高维特征表示，解码器根据编码器的输出和目标序列生成对应的输出序列。自注意力机制：在编码器和解码器的每个层中，使用自注意力机制来捕捉输入序列中不同位置之间的依赖关系。自注意力机制能够计算输入序列中不同位置的相关性，并根据相关性对特征进行加权。上下文编码：利用自注意力机制，在编码器中对输入序列中的每个字或词进行上下文编码。通过对输入序列中的每个位置进行自注意力计算，可以得到每个位置的上下文信息。解码过程：在解码器中，根据编码器的输出和目标序列，使用自注意力机制生成对应的输出序列。解码器通过不断预测下一个字或词来生成输出序列，直到遇到终止符号或达到最大长度。语义理解结果：根据解码器生成的输出序列，可以得到对古彝文的语义理解结果。这些结果可以包括句子的情感、主题、语义角色等。</p><p></p><p>注意力机制能够帮助模型在语义理解任务中更好地捕捉输入序列中的重要信息，从而提高古彝文的语义理解能力。通过对输入序列中不同位置的相关性进行建模，注意力机制使模型能够更好地关注句子中的关键部分，从而更准确地理解古彝文的语义。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3dc656b8ba9e876dd18db809032e12ac.png" /></p><p></p><p>目前针对古彜文虽然能够识别出相关基础编码，但对应的释义需要根据上下文重新解读，在古彝文识别项目中，合合信息就借助了注意力机制（Transformer）完成语义理解。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/862354fdc1178ee1c432f2f051d42ccd.png" /></p><p></p><h1>三、古彝文识别的意义</h1><p></p><p>在2021年、2022年的世界人工智能大会上，合合信息展现了智能文字识别技术在甲骨文、西周钟鼎文（金文）中的应用，这些研究成果为古彝文的识别提供了良好的基础。甲骨文和古彝文同源于骨刻文，这种文字最早出现在骨头上，随后发展为甲骨文、金文、小篆、隶书、楷书等不同的书写形式。这些文字之间存在许多相通之处，使得文字识别技术在不同阶段得以延续和发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e9183b617a96a7c6d5052ff67d9b105.png" /></p><p></p><p>通过与上海大学联合开启的“贵州古彝文图像识别及数字化校对项目”，合合信息将智能文字识别技术应用于古彝文的保护和传承中。这个校企合作项目的成功开展，为合合信息在小语种保护和古文化传承方面提供了重要的支持。通过智能文字识别技术的应用，古彝文的数字化处理变得更加高效和准确，使得更多人能够了解和认识古彝文这一珍贵的文化遗产。</p><p></p><p>随着人们对小语种和古文化的保护意识不断提高，合合信息将继续加强智能文字识别技术的研究和应用，为保护和传承这些珍贵文化遗产做出更大的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pe4dSavhT55QZtwwyPkc</id>
            <title>GPT 3.5与Llama 2微调的综合比较</title>
            <link>https://www.infoq.cn/article/pe4dSavhT55QZtwwyPkc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pe4dSavhT55QZtwwyPkc</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Sep 2023 07:35:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: SQL, 函数表示任务, GPT 3.5, 微调
<br>
<br>
总结: 本文分享了通过SQL和函数表示任务对Llama 2和GPT 3.5进行微调的比较实验。结果显示，GPT 3.5在两个数据集上的性能稍好，但训练成本高出4到6倍。作者通过这个实验想要看看手动微调模型是否可以在成本很低的情况下让性能接近GPT 3.5，并发现它们确实可以。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>在本文中，我将分享我通过SQL和函数表示任务对Llama 2和GPT 3.5进行微调的比较实验。总体结果如下：GPT 3.5在两个数据集上与LoRA微调的CodeLlama 34B相比，性能要好一些；GPT 3.5的训练成本高出4到6倍(部署成本甚至更高)。SQL任务的代码和数据在这里（https://github.com/samlhuillier/spider-sql-finetune），函数表示任务的代码和数据在这里（https://github.com/samlhuillier/viggo-finetune）。为什么要做这个比较？对GPT 3.5进行微调的成本是很高的。我想通过这个实验看看手动微调模型是否可以在成本很低的情况下让性能接近GPT 3.5。有趣的是，它们确实可以！</blockquote><p></p><p></p><p></p><h1>结果</h1><p></p><p><img src="https://static001.infoq.cn/resource/image/a4/5e/a45f2b3fd101054773f82c02fec8205e.png" /></p><p>CodeLlama 34B和GPT 3.5执行SQL任务和函数表示任务的性能。</p><p></p><p>GPT 3.5在这两项任务上表现出稍好的准确性。在使用模型生成SQL查询时，我还使用执行准确性作为比较它们在虚拟数据库上执行查询输出的指标（精确匹配准确性是指字符级别的比较）。</p><p></p><h3>训练成本</h3><p></p><p></p><p>注：我使用的是vast.ai提供的A40 GPU，每小时费用为0.475美元。</p><p></p><p></p><h1>实验设置</h1><p></p><p></p><p>我使用了Spider数据集和Viggo函数表示数据集的子集，这些都是很好的用于微调的数据集：</p><p></p><p>它们会教会模型给出期望的输出形式，而不是事实。SQL和函数表示任务都期望结构化的输出。预训练的模型不能很好地完成这两项任务。</p><p></p><p>对于GPT 3.5的微调，OpenAI只允许配置epoch的数量。他们建议选择epoch的数量。因此，为公平起见，我只对Llama进行最少的超参数微调，允许OpenAI选择epoch的数量，并训练Llama在评估数据集上收敛。</p><p></p><h3>Llama的架构</h3><p></p><p></p><p>我做出的两个关键决定是使用Code Llama 34B和Lora微调（而不是全参数）：</p><p></p><p>OpenAI很可能会做一些适配器/非全参数微调。Anyscale的一篇文章（<a href="https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2</a>"）指出，对于SQL和函数表示等任务，LoRA几乎可以与全参数微调媲美。</p><p></p><p>我使用的LoRA适配配置是这样的：</p><p></p><p><code lang="text">config = LoraConfig(&nbsp; &nbsp; 
  r=8,&nbsp; &nbsp; 
  lora_alpha=16,&nbsp; &nbsp; &nbsp;
  target_modules=[&nbsp; &nbsp; &nbsp;
  "q_proj",&nbsp; &nbsp; &nbsp;
  "k_proj",&nbsp; &nbsp; &nbsp;
  "v_proj",&nbsp; &nbsp; &nbsp;
  "o_proj",&nbsp;
  ],&nbsp; &nbsp; 
  lora_dropout=0.05,&nbsp; &nbsp; 
  bias="none",&nbsp; &nbsp; 
  task_type="CAUSAL_LM",
  )</code></p><p></p><p>我尝试在所有线性层使用适配配置（正如<a href="https://arxiv.org/abs/2305.14314?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">Qlora论文</a>"所建议的那样)，发现几乎没有性能提升。类似地，将r增加到16只会消耗更多的计算量，而几乎没有提供性能上的好处。</p><p></p><p></p><h3>数据集</h3><p></p><p></p><p>SQL提示词示例：</p><p><code lang="text">You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.

You must output the SQL query that answers the question.
### Input:
Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?

### Context:
CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)

### Response:</code></p><p></p><p>我没有使用完整的Spider数据集，它的数据库Schema是这样的：</p><p><code lang="text">department : Department_ID [ INT ] primary_key Name [ TEXT ] Creation [ TEXT ] Ranking [ INT ] Budget_in_Billions [ INT ] Num_Employees [ INT ] head : head_ID [ INT ] primary_key name [ TEXT ] born_state [ TEXT ] age [ INT ] management : department_ID [ INT ] primary_key management.department_ID = department.Department_ID head_ID [ INT ] management.head_ID = head.head_ID temporary_acting [ TEXT ]</code></p><p></p><p></p><p>相反，我选择使用sql-create-context（<a href="https://huggingface.co/datasets/b-mc2/sql-create-context?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://huggingface.co/datasets/b-mc2/sql-create-context</a>"）数据集和Spider数据集的交集。因此，提供给模型的上下文是一个SQL创建命令（我这么做实际上完全是为了节省节点数）：</p><p><code lang="text">CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)</code></p><p></p><p>函数表示提示词示例：</p><p><code lang="text">Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.
This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].
The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']

### Target sentence:

I remember you saying you found Little Big Adventure to be average. Are you not usually that into single-player games on PlayStation?

### Meaning representation:</code></p><p></p><p>输出是这样的：</p><p><code lang="text">verify_attribute(name[Little Big Adventure], rating[average], has_multiplayer[no], platforms[PlayStation])</code></p><p></p><p></p><h3>评估</h3><p></p><p></p><p>两个模型收敛得都很快：</p><p><img src="https://static001.infoq.cn/resource/image/d5/bb/d596a7726b6ed9c8685eb60bee6febbb.png" /></p><p></p><p>图中显示了在训练过程中模型在评估集上的损失。</p><p></p><p>对于SQL任务，我还使用Spider评估工具（<a href="https://github.com/taoyds/test-suite-sql-eval?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://github.com/taoyds/test-suite-sql-eval</a>"）计算SQL查询的执行准确性。评估工具会构建一个虚拟数据库，并将实际的输出与GPT3.5和Llama 2的查询输出进行比较。</p><p></p><p></p><h1>结论</h1><p></p><p></p><p>总的来说，通过这个经验，我觉得对GPT 3.5进行微调是为了初始验证或构建MVP，而除此之外，像Llama 2这样的模型可能是你最好的选择。</p><p></p><p></p><h3>为什么要对GPT 3.5进行微调？</h3><p></p><p></p><p>你想要证实微调是解决给定任务/数据集的正确方法；你想要全托管的体验。</p><p></p><p></p><h3>为什么要对像Llama 2进行微调？</h3><p></p><p></p><p>你想省钱！你希望最大限度地榨取数据集的性能；你希望在训练和部署基础设施方面具有充分的灵活性；你想保留私有数据。</p><p></p><p></p><p>原文链接：</p><p><a href="https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU3OTg2NTgsImZpbGVHVUlEIjoiczFwR0VJUFJNNkdZQmdwMSIsImlhdCI6MTY5NTc5ODM1OCwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.pv8RdkD-ZDNbCC7QKmsZFfrLy4I1YxHOJk5fHYpcF1c">https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV</id>
            <title>从计算到智算，如何降低AI算力使用门槛？</title>
            <link>https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XrQr3zPIOfXmi4SbNEjV</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Sep 2023 06:51:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式 AI 技术, 算力产业, 智算, 算力芯片
<br>
<br>
总结: 随着生成式 AI 技术的广泛应用，算力产业正从“计算”迈向“智算”。这一变化给算力产业带来了挑战，不同应用场景对算力芯片的运算能力有不同需求。为了降低 AI 算力的使用门槛，需要关注算力芯片的发展和智算中心的建设。 </div>
                        <hr>
                    
                    <p>随着生成式 AI 技术得到广泛应用，算力产业正从“计算”迈向“智算”，这一变化给算力产业带来哪些挑战？不同应用场景对算力芯片的运算能力有何需求？如何降低 AI 算力使用门槛？近日，InfoQ《极客有约》邀请到了大禹智芯产品及解决方案负责人余曦老师，为大家分享《从计算到智算，如何降低 AI 算力使用门槛？》。</p><p>&nbsp;</p><p>以下为访谈实录，完整视频参看：<a href="https://www.infoq.cn/video/2RLlk4XePkSXUNU4eQIw">https://www.infoq.cn/video/2RLlk4XePkSXUNU4eQIw</a>"</p><p>&nbsp;</p><p>姜雨生：欢迎大家来到 InfoQ 极客有约，我是今天的特邀主持人，微软软件工程师姜雨生。本期直播，我们邀请到了大禹智芯产品及解决方案负责人余曦老师来给我们做分享。今天的直播主题是《从计算到智算，如何降低 AI 算力的使用门槛？》。先请余曦老师，给大家做一个简单的介绍。</p><p>&nbsp;</p><p>余曦：大家好，我是余曦，来自大禹智芯。目前，我在公司负责产品和解决方案。非常高兴今天有机会与大家分享我们在 AI 智算中心的经验，包括我个人在这一领域的想法和未来的思考。</p><p>&nbsp;</p><p>我简要介绍一下大禹智芯。我们成立于 2020 年中旬，是国内最早专注于 DPU（数据处理单元）产品的公司之一。我们在技术领域涵盖了底层硬件，包括芯片和&nbsp;FPGA&nbsp;逻辑，以及硬件的不同形态，例如网卡。此外，我们拥有一支强大的软件团队，负责将&nbsp;DPU&nbsp;上的各种硬件能力整合到我们的 DPU 操作系统中。通过这个操作系统，我们可以开发应用软件，充分发挥底层硬件的性能。</p><p>&nbsp;</p><p>除了完整的 DPU 产品，包括硬件和软件，我们还开发了一些与应用相关的组件，与编排系统集成。这些编排系统包括云环境下广泛使用的 OpenStack 和 Kubernetes 等。在编排系统的基础上，我们还提供了一层服务，面向那些没有自己云管理平台但希望充分利用我们产品的用户。为了满足这一需求，我们开发了一个裸金属服务管理平台，结合了我们改进过的 OpenStack 和 DPU 卡，使用户能够快速将物理服务器转化为云化的裸金属实例。</p><p></p><h2>数据中心变迁历程</h2><p></p><p>&nbsp;</p><p>姜雨生：您在过去十年间参与了国内多个大型互联网云计算厂商数据中心网络架构的设计和建设，能分享下过去十年数据中心的变迁历程吗？主要可以分为哪几个阶段？</p><p>&nbsp;</p><p>余曦：在过去的十几年里，我一直投身于互联网行业的数据中心领域。我的经历可以大致分为三个阶段。</p><p>&nbsp;</p><p>首先是物理机阶段，这个阶段网络侧的需求相对简单。当时，主要问题是如何在物理机状态下建立一个数据中心，以充分利用物理带宽。在这个阶段，传统的二层生成数协议等都被推翻，以实现带宽的最大化利用。这个物理机时代持续了很长时间。</p><p>&nbsp;</p><p>第二阶段大约在 2012 年到 2014 年之间，即从物理机时代转向了云计算时代。这意味着从物理机向云计算或云数据中心的转变。云数据中心可以分为小型企业级云数据中心和大型大规模云计算中心。从设备厂商的角度来看，可以分为企业级数据中心和大规模数据中心（MSDC），MSDC 主要服务于大型公有云场景。我们的工作主要集中在大规模数据中心这个领域，面临着如何支持从几百台、1000 台规模扩展到数万、十万甚至几十万的台的巨大挑战。</p><p>&nbsp;</p><p>在这个规模扩展的过程中，出现了许多问题。一个重要的变化是，原先与网络相关的功能都集中在数据中心交换机层面，但随着云计算的发展和规模的扩大，所有功能开始下沉到服务器端，通过软件提供。采用软件方式提供这些功能有几个好处：</p><p>&nbsp;</p><p>第一点是关于灵活性和自主性。与硬件相比，软件更具灵活性，能够根据需要实现功能，这使得它更加灵活和自主。</p><p>&nbsp;</p><p>第二点涉及到软件角度与云计算资源的充分耦合。在云计算中，计算、网络和存储是三个关键组成部分。网络是基础，同时也连接了计算和存储。在计算和存储方面，需要快速将它们紧密结合，并且要具备灵活性。然而，这在网络方面面临许多挑战。因此，我们看到许多功能从传统的交换机层面逐渐迁移到主机上来实现。这也是为什么 DPU（数据处理单元）或智能网卡变得非常重要的原因。因为在所有功能下沉到主机后，主机会面临巨大的压力。那么，如何解决这种压力？必须引入一些新的组件来帮助主机处理网络中的复杂流程，特别是在大型云场景中的复杂流程。这就导致了像智能网卡和 DPU 这样的组件在云计算中逐渐崭露头角。</p><p>&nbsp;</p><p>此外，还有一点需要注意的是，在传统的云计算场景下，计算资源主要围绕 CPU 提供。但随着广泛应用和需求的变化，逐渐引入了异构计算组件，包括&nbsp;FPGA、GPU 以及专门用于特定算法的 AI 芯片。这些组件逐渐融入了云计算场景，与 CPU 一起提供算力，这在过去是非常不同寻常的。同时，这也为现在被称为智算中心和 AI 计算领域的发展提供了雏形。</p><p>&nbsp;</p><p>第三阶段是云计算服务逐渐与智算服务（或异构算力服务）相结合，形成了我们现在所谈论的智算中心。在智算中心内部，有一个重要组成部分，即高性能网络。当我们与客户讨论智算中心时，我们强调从网络的角度看，智算中心实际上分为两个网络：服务网和参数网（或计算网）。</p><p>&nbsp;</p><p>从服务网的角度来看，它通过这个网络将智算中心内部的计算资源，无论是 CPU 的计算能力还是 GPU 或其他异构计算资源，以云服务的形式提供出去。这个网络承载了向智算中心内的云服务提供计算资源的角色。</p><p>&nbsp;</p><p>参数网（或计算网）的主要作用是通过网络建立一个高速通道，以便计算单元之间进行快速的数据交互。这个网络在云计算数据中心的后期阶段主要存在于小规模存储场景中。例如，在国内引入 RDMA（Remote Direct Memory Access）应用时，这个应用主要基于存储场景，后端存储集群通过 RDMA 技术提高吞吐量、降低延迟，从而提高云上云盘的整体性能。当引入这些计算单元后，计算单元也需要一个高性能网络来支持数据传输。因此，高性能网络从存储环境逐渐迁移到了智算中心内的参数网。这实际上是一个演进的过程。</p><p>&nbsp;</p><p>姜雨生：数据中心是算力的最终载体，在算力需求日益增长的当下，传统数据中心面临哪些挑战？我们需要什么样的数据中心？</p><p>&nbsp;</p><p>余曦：参数网（或计算网）现在面临着巨大的挑战，因为这个网络有几个极致的要求。首先是吞吐量必须大，且从微观角度看，它要求吞吐量呈波浪形，但从宏观角度看，它应该一条直线。这意味着网络流量需要一直保持在最大带宽的状态，这是一个挑战。其次，在网络趋于跑满的情况下，仍然需要为上层应用提供良好的传输环境。传输环境涉及两个维度：丢包和延时。不同应用可能对丢包的容忍度不同，有些应用可以接受一些丢包，而有些则需要零丢包。延时也是一个重要因素，因为在数据交互中，任务的开始必须等到上一个阶段的数据交互完成。这意味着等待时间取决于最后一个任务完成的时间，而不是第一个任务完成的时间。因此，尽量减少等待时间，使数据交互在相近的时间内完成，算力单元的能力要求非常高。</p><p>&nbsp;</p><p>这就要求网络在三个方面都要表现出色，即延时、吞吐和规模。然而，传统的 TCP 网络在这三个方面通常表现为一个狭窄腰形状的等腰三角形，延时、吞吐和扩展性的平衡难以达到。现在的挑战是将 RDMA 网络调整成类似的形状，即提高吞吐、降低延时，并改善规模性能。这对所有相关方都是一个巨大的挑战。</p><p>&nbsp;</p><p>最近的市场变化表明，IB（InfiniBand）网络从原来的 HPC（高性能计算）领域突然爆发出来，这部分原因与高性能网络相关。在 HPC 场景下，IB 网络能够满足大型模型场景的需求，因此它受到了关注。然而，现在越来越多的用户，特别是在国内，开始探讨以太网是否能够提供与 IB 相似的性能和能力。</p><p></p><h2>不同场景对算力芯片的运算能力有何需求？</h2><p></p><p>&nbsp;</p><p>姜雨生：针对不同的应用场景，算力芯片的运算能力需求有何不同？能分享几个算力芯片在实际应用中的案例吗？</p><p>&nbsp;</p><p>余曦：不同的应用场景对算力芯片的运算能力有不同的需求。通常，GPU 的算力能力以 Flops（浮点运算每秒次数）来衡量，而 Flops 可以在多个维度上划分，包括 INT8、FP16 以及不同精度的整数计算（INT16、INT32、INT64 等）。不同厂家可能对这些精度有不同的定义。因此，在选择算力芯片时，不一定需要所有精度都非常强大，只需满足特定应用的需求即可。</p><p>&nbsp;</p><p>在与行业用户进行沟通和交流时，发现不同行业和应用对算法精度和性能要求不同。举例来说，有一家专门做推荐算法的公司，他们的模型训练场景与其他行业完全不同，更轻量化。虽然他们的数据量很大，模型也非常先进，但他们需要的精度更低。这是因为他们的业务要求如此。从推荐算法的角度来看，用户的行为和兴趣可以导致不同的推荐结果，因此他们更注重轻量化的模型。推荐算法是基于用户的先前行为和兴趣来进行的。从法律和法规的角度来看，用户的画像不能太精确，以避免直接关联到特定个人，因为这在法律上通常是不允许的。因此，推荐算法的设计并不一定需要极高的精度，只需要达到所需的效果即可。</p><p>&nbsp;</p><p>在不同领域，比如医疗生物制药领域，不同阶段的计算需求也不同。在医疗生物制药领域，每个阶段需要不同的算力和算法类型。这些需求可以使用不同的集群来处理，类似于流水线的概念。因此，在特定场景下，可以使用特定的算力单元来实现更高效的处理。</p><p>&nbsp;</p><p>一些公司专注于提供特定场景的 AI 算力，而不仅仅是通用的 GPU。他们的逻辑是根据特定场景和需求，为算法提供定制化的算力，以加速计算。虽然在其他方面可能相对较弱，但这种方法在特定应用场景中有着广阔的前景和市场机会。</p><p>&nbsp;</p><p>姜雨生：有数据提到，我国算力利用率仅 30%，大量算力仍处于闲置状态，如何才能提升算力的利用效率？算力产业链的各个相关方需要做哪些工作？</p><p>&nbsp;</p><p>余曦：早在很早以前，一些大型运营商就提出了算力网络或算网融合的概念。这意味着将分散的算力资源联合起来，以便共同提供这些算力资源，并在这一过程中，网络的角色变得非常关键。同时，标识和管理算力资源、调度计算任务也变得至关重要。</p><p>&nbsp;</p><p>运营商和一些边缘计算相关的公司已经在这个领域有了一套有效的算网融合模型和实施方法，可以通过虚拟网络来构建算力网络，并在其上调度不同的算力单元，以提高其利用率。当然，在实现这一目标时，还需要解决一些挑战。举个例子，与云计算领域的情况进行对比。最初，大家都在同一个云上部署应用。后来，人们开始将应用分散在不同的云上，因此出现了多云平台或多云调度的概念。同样，对于算力资源，我们也需要类似的多云平台，将不同的算力资源整合在一起，并提供一种选择不同资源的方式。这将有助于提高整体算力资源的利用效率。</p><p>&nbsp;</p><p>姜雨生：有观众提问，现在的大小模型、网络模型是怎么样的呢？</p><p>&nbsp;</p><p>余曦：对于现在的大小模型和网络模型，实际上主要涉及到网络支撑的算力节点数量。越大的模型需要更多的算力节点，这也会带来对网络的更大挑战。但总体来说，网络的组建逻辑基本相同，只是需要考虑容量更大的交换机、网络拓扑结构等因素。不管是大模型还是小模型，它们对网络的要求都是一致的，主要包括以下三个方面：吞吐量、延迟和可扩展性。这些因素在不同规模的模型中都具有重要意义。</p><p>&nbsp;</p><p>姜雨生：观众提问，研发算力芯片，芯片的质量和稳定性怎么保证？</p><p>&nbsp;</p><p>余曦：我们的公司在算力芯片领域有着不同的专注点。我们的主要工作是集中在网络处理单元上，任务是协助算力芯片进行数据搬迁操作。具体来说，我们的任务是帮助算力芯片快速而高效地将数据从一个地点（点 A）搬移到另一个地点（点 B）。这个领域是我们的专业领域。</p><p></p><h2>从“计算”到“智算”</h2><p></p><p>&nbsp;</p><p>姜雨生：随着生成式 AI 技术得到广泛应用，算力产业正进入产业“智算”时代，从“计算”到“智算”，最大的变化是什么？如何应对算力需求的变化？</p><p>&nbsp;</p><p>余曦：实际上，最大的变化在于从以 CPU 为主的算力提供服务者，演变为 CPU 周围有各种专业的辅助芯片，例如 GPU、FPGA、AI 芯片等，每个芯片都有自己的专长领域。这些辅助芯片与 CPU 一起组成了一个多样化的算力集群，以多种不同的算力方式提供服务，这就是现在智算中心的基本逻辑。</p><p>&nbsp;</p><p>从我们的角度来看，我们的主要目标是在智算中心中补充整个产品线。我们更专注于网络方面的工作，因此在智算中心的角度来看，我们的任务是将服务网上的工作卸载到 DPU 卡上，以提高处理效率并创建更多的云服务。在参数网络方面，我们正在开发基于 RDMA 的底层网络支持，虽然它不同于传统 RDMA 网络，但我们的实现能够让 RDMA 应用或网络达到更高效的状态，就像之前提到的那个满三角形的状态一样。</p><p>&nbsp;</p><p>姜雨生：当前企业在使用算力时主要存在哪些门槛？怎样才能降低算力的使用门槛？</p><p>&nbsp;</p><p>余曦：当前，在智算中心领域，企业内部搭建一个算力平台的门槛相当高。这种困难可以分为几个层面。首先是基础设施，包括服务器部署、网络部署以及基础环境的调试和优化。这些任务需要大量的人力、物力和精力，而许多企业缺乏专业的基础设施人员，他们更关注应用层面，即如何使用现有的硬件来运行模型进行计算。这是他们擅长的，但他们缺乏底层基础设施的技能。</p><p>&nbsp;</p><p>对我们来说，我们提供了一种成熟的解决方案，利用我们自己的裸金属云解决方案，使客户能够在底层基础设施层面快速构建基于裸金属的算力单元。例如，对于一个典型的 AI 服务器配置，如两台机器、每台机器 8 张 GPU 卡，客户需要快速将其投入使用。除了底层的网络配置外，还需要建立许多服务，例如客户需要能够灵活选择使用多少台机器以及这些机器上的技术环境，如操作系统、CUDA 等。通过我们提供的完整的裸金属云解决方案，客户可以在完成底层物理层的网络配置后，快速实现服务化，从而加速他们的计算工作。</p><p>&nbsp;</p><p>通过这种服务化方式，客户可以像在云计算场景下选择云实例一样，快速启动一台机器或一组机器。在选择过程中，他们可以自由选择操作系统的版本、包含的 CUDA 版本以及各种框架组件等，这些选项都以服务的形式提供。例如，如果客户需要 8 台 8 卡的机器，他们只需在 Web 界面上点击几下，几分钟内就能启动整个服务，完成后可以迅速释放资源，供其他同事或部门使用。这实际上展示了我们现在具备的快速交付的能力，构建了一个称为智算中心的底层基础设施。</p><p>&nbsp;</p><p>姜雨生：大模型时代的到来，会让 AI 芯片市场格局将发生巨变吗？您会用哪几个词来形容当前的行业状态？</p><p>&nbsp;</p><p>余曦：我认为目前国内 GPU 行业正迎来一个巨大的发展机遇。根据我们之前的沟通，国内的 GPU 厂商都非常看好未来的发展前景，并全力以赴开发下一代产品。对于上一代产品或现有产品，它们都存在一些不够满意的方面，例如跨机通信能力和算力、性能覆盖程度等。我们预计，可能在今年年底或明年上半年，国内将涌现出大量新一代的 GPU 产品，这将为国内的技术和算力平台建设增添新动力，这是一个明显的趋势。</p><p>&nbsp;</p><p>姜雨生：有观众提问，听说 AI 的网络用的是 RoCEv2？</p><p>&nbsp;</p><p>余曦：在大规模的 AI 网络部署中，目前是不使用RoCEv2的，因为存在一些问题，尤其在 AI 场景下这些问题更加显著。首先，RoCEv2 存在固有的 PFC（Priority Flow Control）问题，在整个 AI 场景下特别明显。具体来说，RoCEv2 在以下两个方面存在问题：</p><p>&nbsp;</p><p>扩展性问题：&nbsp;RoCEv2&nbsp;无法支持大规模部署，这在大规模AI网络部署中成为一个明显的障碍。PFC问题：PFC可能会导致一些问题，例如死锁（DEADLOCK），这些问题妨碍了其扩展性的提升。</p><p>&nbsp;</p><p>因此，目前来看，如果从大规模 AI 网络的实施角度考虑，通常有两种主要途径：</p><p>&nbsp;</p><p>第一种途径： 大型互联网公司内部自主研发以太网协议实现。这种自研实现可以支持非常大规模的 RDMA 网络部署，并且通常包含一些特殊定制的功能。这些公司通常将这些实现视为自家的核心竞争优势，因此对外部是不公开的，你只能看到一些相关的研究论文和借鉴的参考资料，但无法获得具体实现的细节。第二种途径： 基于英伟达的 InfiniBand（IB）网络。目前，IB 已经被证明可以在大规模 AI 算力网络中提供支持。</p><p>因此，包括国内 DPU 制造商在内的行业内企业都计划基于 DPU 实现端到端的高性能网络，该网络基于以太网，但能够提供类似 IB 网络的性能支持。这是当前的主要趋势之一。</p><p>&nbsp;</p><p>姜雨生：有观众提问，大模型训练算力和推理算力消耗量是多少？”</p><p>&nbsp;</p><p>余曦：在讨论大模型的训练和推理时，实际上涉及到两个不同的场景：训练和推理。训练阶段是模型自身在背后进行学习和优化的过程，而推理阶段是将已训练好的模型拿到前端，用于为外部提供服务。</p><p>&nbsp;</p><p>在不同的时代和情境下，训练和推理所需的算力可能存在差异。例如，目前大型模型的训练可能需要数千张卡并耗费大量算力，但一旦训练完成并生成了可用的模型，模型在前端提供推理服务时，算力的消耗可能会因前端的服务需求而变化。</p><p>&nbsp;</p><p>这个问题可以从扩展性和分发性的角度来看待。训练阶段通常需要大量计算资源，但通常是固定的。然而，在推理阶段，算力需求是动态的，并且会随着用户数量的增加而增加。因此，前端推理所需的算力可能会远远超过训练阶段的计算能力。</p><p>&nbsp;</p><p>姜雨生：有观众提问，传统网络转到英伟达的 IB 网络，需要改造什么的么&nbsp; 会影响算力么？</p><p>&nbsp;</p><p>余曦：在传统网络和 InfiniBand（IB）网络之间，一般没有改造的情况，通常是进行全新建设。现在我们主要看到的是全新建设的场景，例如，如果要建立一个智算中心以及智算中心后面的参数网络，通常是从头开始建设，而不是对现有数据中心进行改造。</p><p>&nbsp;</p><p>从两个维度来看，这一决策是有其原因的：</p><p>协议不同：传统网络通常使用以太网协议，而 InfiniBand 是一种不同的协议。除非你使用英伟达自家的 Spectrum 芯片环境，它可以进行以太网和 InfiniBand 之间的切换，否则你不能在传统以太网交换机上切换到 InfiniBand 网络。带宽需求不同：传统环境中，通常的带宽需求不是特别高。例如，在云环境中，大多数情况下使用的是 25G 以太网。然而，在参数网络中，通常以 100G 或 200G 的带宽为起点。如果要实现顶级配置，那么需要使用 400G 网络来支持整个计算过程。因此，在原有的 25G 网络下直接进行改造以支持 200G 或 400G 网络是不现实的，因此需要进行全新建设。</p><p></p><h2>高性能网络对于 AI 智能领域至关重要</h2><p></p><p>&nbsp;</p><p>姜雨生：大模型的爆发式增长给算力芯片带来哪些挑战？芯片厂商如何才能更好地满足大模型厂商的需求？大禹智芯有哪些实践经验可以分享下？</p><p>&nbsp;</p><p>余曦：这方面的经验主要涉及定制化。就定制化而言，在大模型的场景下，定制化的需求相对较少。因为只要你的支持足够完善，大模型的应用层、中间层（也可以称为中间件层或 API 层）以及底层硬件之间的解耦非常强。这种情况下，在大模型的角度来看，应用层、中间层以及底层硬件之间的明确划分是相当好的。尽管在整个 AI 大模型的运行过程中，这三者之间的协作非常紧密，耦合度很高，但从每一层到每一层的界面角度来看，划分是相当清晰的。</p><p>&nbsp;</p><p>我们主要从底层入手，专注于底层的开发。但我们的目标是为中间层或通信层以及上层应用提供服务，使它们不必感知我们底层所做的工作。尽管在我们底层可能有一套独立的实现，但对于上层应用来说，这些实现是透明的，不需要感知。从这个角度来看，对于那些只关注上层应用的用户来说，他们实际上不太关心底层的实现细节，只要能够达到所需的效果即可。在这种情况下，定制化的功能需求较少。</p><p>&nbsp;</p><p>然而，也存在一些特殊情况，即客户从上到下都关心，并且更加偏向底层的场景，可能会需要一些定制化的功能。但需要指出的是，这些定制化的功能实际上都是围绕着刚才在大模型网络中提到的那三个角，即吞吐量、延迟和扩展性展开的。不同的实现机制可能会在这三个角度上有一些不同的理念。</p><p>&nbsp;</p><p>目前，我们正在与一些头部客户讨论与实现相关的理念，希望能够汲取彼此的长处，将我们的优秀想法输出给这些头部客户，共同建立一个良性的环境，真正打造出一个高性能的算力网络。</p><p>&nbsp;</p><p>姜雨生：对于国内芯片厂商来说，这其中有哪些机遇与挑战？这个过程中，大禹智芯的技术产品策略是否发生了变化？</p><p>&nbsp;</p><p>余曦：我们的产品路线从一开始就是围绕着智算中心的两张网络来制定的。就目前而言，我们在服务网络方面已经处于一个非常成熟的状态，通过 DPU 实现了云服务的底座，这一方面已经准备就绪。现在我们正专注于投入更多的精力和资源，来加强我们所说的“算力网”，也就是背后的计算网络。在此方面，我们已经进行了许多关于算力网络的讨论，而现在我们正全力以赴地投入到算力网络的发展中。</p><p>&nbsp;</p><p>未来的愿景是，通过我们的大禹智芯网卡，能够与国家计算能力布局中的两张网络共同发挥作用。一张网络是服务网络，用于构建智算中心的云底座，另一张网络是参数网络，用于构建智算中心内部的高性能计算网络。这两张网络将协同合作，为未来的计算需求提供支持。</p><p>&nbsp;</p><p>姜雨生：下一步，大禹智芯有哪些技术探索和产品规划？有哪些技术难题是我们在未来需要解决的？</p><p>&nbsp;</p><p>余曦：我们计划在明年上半年推出我们自己的全新算力网络产品。我们对此进行了前期的研发工作。</p><p>&nbsp;</p><p>姜雨生：有专家提到“技术突破是算力发展的根本”，您怎么看？展望未来，我们应该如何更好地推动算力技术的发展和应用？</p><p>&nbsp;</p><p>余曦：在 AI 芯片领域，我们希望国内的芯片制造能力能够有明显突破。目前，AI 领域主要包括计算、存储和网络三个方面。在计算方面，国内已经有一些产品，特别是 CPU 和 GPU。然而，底层芯片性能的提升仍然是关键。这对于像我们这样专注于网络层的 DPU 制造商以及 GPU 制造商来说，都是一个积极的因素。</p><p>&nbsp;</p><p>从技术实现的角度来看，高性能网络对于 AI 智能领域至关重要。数据传输和计算之间的时间占用比例几乎是 1:1，尤其是在处理大型模型时，网络的占比可能更高。因此，提升网络能力将提高智算中心内所有算力单元的使用效率。在这个领域，有一些有趣的网络协议、想法和概念，需要一个有利的环境来实现。这个实现过程需要各方合作，将各自的能力输出，以建立一个有利于实验和创新的环境。解决问题是一个从 0 到 1 的过程，现在我们正在解决 0 到 1 的问题。一旦 0 到 1 的问题解决了，我们可以再看如何解决 1 到 10 的问题，需要哪些步骤和阶段。因此，我们希望有一个领导者，并创造一个培育创新的环境，让国内的合作伙伴能够将他们的想法付诸实践，验证效果，并确定未来的方向。这对于整个行业的发展非常重要。</p><p>&nbsp;</p><p>姜雨生：目前 DPU 行业的就业情况如何？需要哪些方面的人才？</p><p>&nbsp;</p><p>余曦：我们以 DPU 整个的技术栈来看，可以分为四个主要部分：硬件、软件、调度系统和业务平台。</p><p>&nbsp;</p><p>硬件层：硬件层涉及芯片能力，比如做自研芯片或&nbsp;FPGA就需要找相对应的团队，像FPGA&nbsp;层实现了网络和存储侧的功能，在这之上就是软件层的部署。软件层：软件层包括操作系统层和内核层的优化以及定制化。此外，还有与开发框架相关的工作，例如&nbsp;DPDK、SPDK。有这些开发框架开发经验的人才也能在参与到 DPU 这个行业之中。调度系统：这一层涉及与云服务相关的组件，包括 OVS、VPP、存储组件（如 Ceph 等）。与这些组件相关的团队也可以在 DPU 行业中发挥作用。业务平台：业务平台层包括与云平台、云管理以及一些开发相关的团队。这些团队可能在开发类似于 OpenStack、K8s&nbsp;等开源项目以及云平台上的业务平台等方面发挥作用。</p><p>&nbsp;</p><p>所以，从技术栈的角度来看，可以说我们将原本在云上的完整技术栈全部迁移到了 DPU 领域。这意味着无论是从底层硬件到上层的业务平台，都在 DPU 行业有所涉及。</p><p></p><h4>嘉宾介绍</h4><p></p><p>&nbsp;</p><p>特邀主持：</p><p>&nbsp;</p><p>姜雨生，微软软件工程师，负责微软资讯业务与 GPT 集成，曾负责微软广告团队基础设施搭建与维护工作。</p><p>&nbsp;</p><p>嘉宾：</p><p>&nbsp;</p><p>余曦，大禹智芯产品及解决方案负责人。曾任当当网首席网络架构师、思科大中华区互联网事业部总监、Fungible 中国首席架构师，在数据中心网络、云计算基础设施服务、高性能网络等领域有丰富的实践经验。近 10 年间主要参与了国内各大型互联网云计算厂商数据中心网络架构的设计和建设，见证并参与了国内数据中心从物理机时代向云计算时代发展过程中计算、网络、存储等基础设施的各个发展阶段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf</id>
            <title>AI 时代下的 SUSE 新洞察：无处不在的边缘计算革命即将到来</title>
            <link>https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RfxyPktV79MUPaIYgxBf</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Sep 2023 06:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式 AI, 数字创新, 数字信任, 边缘计算
<br>
<br>
总结: 随着生成式 AI 等前沿技术的发展，数字创新正在引领一场全新的技术变革。在这个变革中，数字信任成为了不可或缺的标准。边缘计算作为一种应对数字时代挑战的有效方法，对基础设施提出了更高的要求。企业需要构建稳定、灵活和可扩展的边缘计算基础设施，以支持未来的数字化发展。 </div>
                        <hr>
                    
                    <p>随着生成式 AI 等前沿技术的迅速发展，一场全新的技术变革正在悄然引领数字创新的浪潮。这种变革为企业未来发展带来了无限可能，使其能够高效应对 IT 挑战、提高生产效率，创造出前所未有的商业价值。然而在这场变革的另一面，“数字信任”已经成为不可或缺的标准，各大厂商都希望通过严格的网络安全实践和强大的数据保护措施，为企业数字创新提供坚实的保障，确保其能够在充满挑战的数字时代稳健发展。SUSE 作为全球范围内创新、可靠且安全的企业级开源解决方案领导者，正在持续推动其在安全、稳定、可靠、可互操作等方面的技术进步，以帮助企业应对不断变化的市场需求。</p><p></p><p>由<a href="https://www.infoq.cn/article/2fHFRAFOYTE5QZaXbU43"> SUSE</a>" 主办的年度数字创新峰会——SUSECON 深圳 2023（下文称“本届峰会”）于 9 月 22 日圆满落幕。通过本届峰会，我们可以非常清晰地看到 SUSE 在过去一年里的探索，SUSE 在边缘计算方面联合合作伙伴，做出了多种尝试。同时，愈演愈烈的生成式 AI 为整个技术市场带来了无限机会，而在此浪潮下的 SUSE 也敏锐地把握了这一新兴技术的潜力，积极投身于相关研究和开发工作中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c5652646de05a0a8fb32c1ad857fc86.jpeg" /></p><p>图：SUSE 大中华区总裁陈毅威发表主题演讲</p><p></p><h2>一、边缘计算无处不在，对基础设施提出更高要求</h2><p></p><p></p><p>随着 5G、物联网、云计算等技术的快速发展，边缘计算得到了广泛应用。由于物联网设备的数量不断增加，数据产生和传输的规模也在快速增长，传统的云计算模式已经无法满足低延迟、高带宽、高可靠性的需求，于是将计算任务分配到网络边缘的设备上成为了解决这个问题的一种有效方法。</p><p></p><p>与云计算相比较，边缘计算就近布置，可以被理解为云计算的下沉。边缘计算通过将数据在靠近设备或终端进行计算和处理，大大提高了数据处理的效率和实时性，同时减轻了中央服务器和网络的负载。此外，边缘计算和云计算需要结合使用，例如在大数据分析中，边缘计算可以处理本地设备上的数据，云计算则可用于存储和分析大量数据。但这也意味着，边缘计算对基础设施有更高的要求，它需要在网络边缘的位置处理和存储数据、部署大量的小型数据中心和设备，设备规模、能源消耗、运维管理和安全性等就成为了重点考虑因素；同时，边缘设备的计算和存储资源往往比中心服务器更为有限，这也限制了其能够处理的数据量和计算的复杂度，边缘计算的能耗和硬件资源效率也面临着挑战。</p><p></p><p>为了克服这些挑战并充分发挥边缘计算的优势，SUSE 与瞬优智慧达成合作，联合发布了“鼎瞬 Peerless 实时业务协同解决方案平台”，广泛应用于工业制造、物流运输和智能城市等领域。这是一款面向企业级客户的云边端协同平台，集软、硬件于一体，在数字化全生命周期提供从云端至近边缘端到边缘端的全面支持，以实现各业务系统（如物联网终端设备等）海量数据的实时感知、实时处理、实时分析、实时决策。</p><p></p><p>值得一提的是，该解决方案整合了 SUSE 的分布式基础架构解决方案，包括 SUSE Linux Enterprise Server、SUSE Manager、Rancher 和 NeuVector，为云平台的稳定基础架构提供了强有力的支撑；SUSE Linux Enterprise Micro、Rancher、K3s、Longhorn 也为解决方案中的边缘智能网关、近边缘智能一体机提供了高可用的基础运行架构。</p><p></p><p>当前，SUSE 正在与联想、中科云谷等行业领导者一起探索边缘的更多可能。SUSE 和联想将充分发挥各自在技术、产品、市场、生态等方面的优势，以边缘硬件、OS、软件定义基础设施为核心数字底座，合力打造边缘原生的全栈式企业级解决方案。SUSE 与联想在边缘计算领域的合作主要分为三个阶段：</p><p>第一阶段：基于 SUSE 现有的边缘计算产品组合，联想提供边缘硬件设备算力支持，共同打造边缘计算节点。目前小盒子（网关）的适配工作已基本通过测试，服务器适配工作正在进行中。第二阶段：联想推出自有边缘计算平台，结合 SUSE 的操作系统，打造商业化边缘计算平台和软件平台。目前第一阶段和第二阶段同步进行，第二阶段的工作重点将聚焦在边缘服务器和边缘云平台的适配上。第三阶段：双方将在边缘计算产品系列中进行更深度的融合，将 SUSE 的操作系统、Rancher、NeuVector 等组件纳入联想的边缘操作盒一体机产品体系。同时，双方将实现统一架构的管理运维框架，包括边缘管理平台与 SUSE 和管理平台的结合，以实现全栈式全方位全要素的管理和监控运维能力。</p><p></p><p>此外，在本次峰会上，SUSE 与中科云谷达成了战略合作，联合推出了中科云谷云原生技术平台。该平台基于 Rancher 容器云、微服务、DevOps 等云原生技术构建，提供丰富、通用的数字化转型共性组件。更重要的是，中科云谷目前已经完成了对 SUSE 零信任容器安全平台 NeuVector 的全面测试，近期将正式上线生产环境；同时，双方正在探索将 SUSE Edge 边缘解决方案纳入平台以帮助客户应对复杂的边缘环境。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/879c61c1f6a5c3f9518e4ce7f66b6b13.jpeg" /></p><p></p><p>客观地说，关于边缘计算的未来发展，其实就像中国科学院王义博士在本届峰会上分享的那样，“全球工业、农业、医疗等重要领域对边缘计算系统的需求逐渐清晰，并形成了大规模的市场。与传统操作系统相比，边缘计算操作系统需要在分布式协同、实时性、可用性、安全性、高能效等方面为用户提供更先进的系统能力。”边缘计算正在高速发展，已经成为了现代数字生态系统的重要组成部分，即将像水电一样无处不在。为了满足日益增长的需求，企业需要构建更加稳定、灵活和可扩展的边缘计算基础设施，为未来在数字化世界的发展提供强有力的支撑。</p><p></p><h2>二、在边缘运行云原生面临着许多挑战</h2><p></p><p></p><p>边缘计算应用规模在增加，技术挑战及企业用户需求也随之增加。云边基础设施存在差异，云原生能力直接下沉应用到边缘时，除了需要提供等同于中心的性能指标、安全隔离、容灾自治、架构感知等能力，还需要不断完善云边以及边边高速通道建设等，进而提升建设难度系数。“小而多”的边缘节点，资源复用率低，这就需要云原生技术能够根据资源池化的能力和资源性能做灵活的弹性调度，以实现更高效的资源利用。</p><p></p><p>要知道，云原生技术对部署环境有明确的要求，需要对边缘侧的海量异构资源进行灵活的适配，这就意味着用户需要对各种硬件和操作系统进行抽象，并为上层应用提供统一的接口，以实现资源池化和资源性能的灵活调度。开发者在日常工作中最常用的容器集群管理工具 Kubernetes 便是其中一个代表——边缘侧设备的 CPU、内存等计算资源配置通常较低，主要用于应用自身，难以分配更多资源供 Kubernetes 的中间层平台使用；在边缘环境网络不稳定的情况下，Kubernetes 自身难以稳定运行。而且虽然社区中目前已经有了轻量化的 Kubernetes，但多为单节点架构，并非是为了运行多节点的生产级环境而设计的，难以提供生产级的高可用服务。</p><p></p><p>于是，为了解决 Kubernetes 能够运行在边缘计算环境的挑战，让用户能通过下一代嵌入式边缘设备获得成功并具备扩展能力，SUSE 开发出了轻量化的 Kubernetes 发行版——K3s，轻量化的 Linux 操作系统 SUSE Linux Enterprise Micro, 在 Rancher2.7 中已经实现了对 K8s、K3s 以及基于 OCI 格式的 SUSE Linux Enterprise Micro 的统一管理，并与 SUSE 的超融合基础架构解决方案 Harvester 和 SUSE 的零信任容器安全平台 NeuVector 组合，推出了 SUSE Edge 云原生边缘管理解决方案，为从应用程序到 K3s 再到操作系统的整个堆栈进行了安全策略的无缝集成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e600725ef9e5ba91eb5cd045be378fe.png" /></p><p></p><p><a href="https://www.infoq.cn/article/HmppHMJseJVj7sZkR7Te">SUSE</a>" Edge 能够对无人监控的边缘环境进行低成本的统一管理，用户可通过统一的操作面板管理 Kubernetes 和底层操作系统，大幅降低了运维、部署工作的复杂性。SUSE Linux Enterprise Micro 是专为边缘环境中的容器化工作负载量身打造的轻量级操作系统，其安全可靠、无需维护的特性，可以实现简单而重要的边缘设备管理任务自动化运行。基于此，开发人员能够快速完成测试和编程，构建涵盖可穿戴设备、智慧城市、交通运输等众多领域的各类应用程序。</p><p></p><p>事实上，无论是通用的边缘场景，还是电信、汽车、卫星等需要额外功能的各类边缘场景，SUSE 都能基于不同用例提供完全契合客户需求的边缘解决方案。全球领先的家居建材用品零售商 Home Depot 便非常欣赏 K3s 的简易性，只需要运行单个二进制文件就可以随时拥有一个经 CNCF 认证的 Kubernetes 发行版。店内应用程序一直是容器化的 Home Depot 将所有 2300 多个零售店都转到了基于 Rancher +K3s 的新架构中，以避免手动维护容器化应用程序的可用性。</p><p></p><p>然而，云原生技术面临的技术挑战不仅在于 Kubernetes 集群的管理，安全问题同样不容忽视。作为 SUSE Edge 边缘解决方案的重要组成部分，SUSE 的零信任容器安全平台 NeuVector 可通过集中化的企业级扫描、自动扩展扫描工具以及新版 Kubernetes (1.25+) Pod 安全准入，实现更高效、更强大的多集群漏洞扫描和准入控制。据悉，NeuVector 拥有十几项技术专利，包括深层数据包检查和行为学习，可以识别容器的行为，仅允许经过批准的白名单在网络链接、进程访问和文件存取方面进行操作；在运行时还提供完整的攻击检测和预防功能，主动保护生产环境。</p><p></p><p>SUSE 安全产品战略副总裁黄飞在本次峰会上发表了题为《建立数字信任，守护开源安全》的主题演讲，正如他所说的那样：“从分布式系统架构到容器技术，SUSE 不仅帮助企业应对云原生环境中的安全挑战，还通过零信任安全策略和供应链安全工具，确保企业业务和服务的安全性。同时，SUSE 还积极参与国际通用的安全标准认证，为企业提供更加可靠的安全保障。总之，SUSE 一直致力于在安全领域为企业提供全面的解决方案。”</p><p></p><h2>三、AI 为全行业数智化转型带来了机会</h2><p></p><p></p><p>SUSE 大中华区总裁陈毅威在本届峰会上的演讲《重塑·创新的力量》中提到，“SUSE 是一家德国公司，跟 SUSE 合作，你会体验到它身上散发的强烈的德国工匠精神，即对自身产品质量的极高要求。SUSE 的文化基因就是开源，SUSE 在各领域都在不断探索。”事实就是这样，自 1992 年 SUSE 在德国纽伦堡成立，至后来 2020 年收购 Rancher、2021 年收购 NeuVector，SUSE 始终走在科技前沿，借助前沿科技助力客户更好创新。近期 ChatGPT 引爆了新一波 AI 热潮，SUSE 再次“走在了路上”。</p><p></p><p>ChatGPT 将生成式 AI 的能力带入人工智能主流应用领域，在今年初其月活跃用户就达到了 1 亿，打破了用户群增长最快的记录。AI 技术已经进入了大规模应用和普及阶段，AI 3.0 时代来临。在这种背景下，生成式 AI 及其多种应用正在重塑行业，帮助各领域实现更快、更准确的数据处理和分析，从而提高工作效率和决策精度，为企业数智化转型提供了更多的机会和可能性，助力其获得更多的商业机会和收益。</p><p></p><p>然而 AI 技术的发展往往需要进行复杂的模型训练和推理，需要大量的计算资源和存储资源，这对算力提出了更高要求，也为很多厂商提供了新的增长点。</p><p></p><p>SUSE 已经迈出了 AI 应用的第一步——在 Rancher Prime 上开发了一个 AI 助手，为用户提供自助、便捷的用户服务，同时提供准确、有效的信息。作为老牌 Linux 技术探索者，SUSE 在 Linux 方面也做了很多 AI 方面的探索。比如今年 7 月，SUSE 发布最新旗舰版企业级 Linux 平台 SUSE Linux Enterprise 15 Service Pack 5（SLE 15 SP5），可以提供对 AI/ML 工作负载至关重要的高性能计算能力，并与 Rancher 协同工作。它是第一个支持全范围机密计算的 Linux 发行版，可以保护在公有云和边缘处理的客户数据，允许客户在任意环境中运行完全加密的虚拟机，安全属性得到了大大提升。</p><p></p><p>使用超级计算机来进行顶级 AI 工作的用户、处理大型数据集和复杂 ML 过程的用户非常喜欢 SLE 15 SP5，就是因为 SUSE Linux Enterprise 拥有强大的安全功能和工具来保护 AI/ML 工作负载和数据。它提供了安全引导、访问控制、加密和审核等功能，以确保符合行业法规（如 GDPR 和 HIPAA）。</p><p>对 AI 的探索推动了 SUSE 的技术创新，不仅增强了 SUSE 的技术实力，也为其在市场上的竞争力提供了有力的保障。正如 SUSE 大中华区总裁陈毅威所说的，他十分肯定，AI 将是 SUSE 非常好的发展契机。“别忘了，硬件和应用的中间，还需要一个更为成熟的操作系统作为支撑。”</p><p></p><h2>四、写在最后</h2><p></p><p></p><p>在本届峰会上，陈毅威和 SUSE 亚太 CTO &nbsp;Vishal Ghariwala 都表达了同一个观点——随着数字化转型的加速，企业需要不断适应市场的变化，提高自身的竞争力，开源技术为企业提供了实现这一目标的重要途径。通过引入先进的开源解决方案，企业可以降低研发成本，加速产品上市时间，提高服务质量，从而在激烈的市场竞争中脱颖而出。</p><p></p><p>从 <a href="https://www.infoq.cn/article/WiNrWV5QrqayNtSGGjFw">SUSE </a>"的发展轨迹中我们可以看到，其超过三十年的开源技术积累是推动全球开源生态发展的重要力量之一。从 1992 年最初的 Linux 发行版开始，SUSE 就坚定地选择了开源的道路。旁观 CentOS 事件，SUSE 的态度也是非常明确的，开源社区应该以合作、交流、共享和创新为核心价值，任何发行版的变更都应该遵循社区的规则和流程，以确保开源社区的稳定、可靠和安全。</p><p></p><p>今年 7 月，在红帽宣布不再对外公开 Red Hat Enterprise Linux（RHEL）源代码后，SUSE 便表示将开发和维护与 RHEL 兼容的发行版，让所有人都可以不受限制地使用该发行版本，未来还计划向该项目投资超过 1000 万美元。当前，为应对这一挑战，用户可以选择使用 openSUSE、SLES、SUSE Liberty Linux 以及刚刚发布的基于 openEuler 的国产操作系统锐蜥 FlexileOS。其中，SUSE Liberty Linux 是最佳选择之一，它是一种适用于混合 Linux 环境的技术和支持解决方案，用户不仅可以获得可信的技术支持，还可以注册并接收针对于 RHEL、CentOS 和 SLES 等系统的更新。</p><p></p><p>SUSE 在 Linux 方面的持续探索之举非常令人动容，它一直都是 Linux 领域的积极探索者和创新者。SUSE 最新推出的自适应 Linux 平台 Adaptable Linux Platform（ALP）就是一个很好的例子，该平台为企业级 Linux 提供了一种在云原生环境中演进用例的新方法，可以应用于从数据中心到云端、再到边缘的任意场景，让用户专注于工作负载，从硬件和应用层抽离出来。通过使用虚拟机和容器技术，ALP 可以让工作负载独立于代码流。</p><p></p><p>ALP 重点关注安全性，最新版本通过机密计算技术提供了一种受信任的执行环境，通过隔离、加密和执行虚拟机来保护所使用的数据，还为未来的扩展式机密虚拟机 (CVM) 支持奠定了基础。其“硬件和运行时认证”功能可用于验证工作负载的完整性，与 FDE 一起初步实现了端到端的数据安全防护。同时，它与 NeuVector 集成，提供了一个安全的生态系统，让 ALP 用户通过 NeuVector 识别恶意行为，并防止底层主机操作系统或其他容器化工作负载受到影响。此外，用户在安装它时，还可选择带有 TPM 的 FDE 来支持静态数据安全性。</p><p></p><p>ALP 的新版本创新就是 SUSE 过去三十余年创新之路的缩影，这些年来 SUSE 不断推出各种创新的开源解决方案，包括存储、虚拟化、容器等一系列的技术创新。这不仅为 SUSE 赢得了市场份额、用户的信任和认可，更为整个开源领域贡献了大量有价值的资源和经验。技术的进步需要众多像 SUSE 一样的厂商共同贡献，企业间通过实施开源策略，共享资源和经验，共同应对挑战，实现自身的业务增长，促进行业的生态繁荣。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx</id>
            <title>千亿也不够花！OpenAI 想让员工出售股票来筹资，而最大投资人微软正在“去GPT”</title>
            <link>https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AjssoqCr0ax6pI5S7jyx</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Sep 2023 06:10:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 股票出售, 估值, 融资
<br>
<br>
总结: OpenAI正考虑以高估值出售股票，以筹集资金进行融资。这将允许员工出售股份，并吸引顶尖人才。尽管OpenAI已经获得了大量的融资，但由于巨额成本投入和亏损，他们仍然需要额外的资金支持。微软是OpenAI的最大投资者之一，但他们也面临着人工智能功能成本增长的压力。 </div>
                        <hr>
                    
                    <p>据报道，OpenAI正与投资者讨论可能的股票出售事宜，这家人工智能初创公司寻求以800亿—900亿美元（以当前汇率换算，约为5842.48亿-6572.79亿人民币）的估值出售股票，这一估值几乎达到今年早些时候的三倍。</p><p>&nbsp;</p><p><a href="https://www.wsj.com/tech/ai/openai-seeks-new-valuation-of-up-to-90-billion-in-sale-of-existing-shares-ed6229e0">《华尔街日报》</a>"称，此次交易预计将允许员工出售所持股份，而不是以公司发行新股的形式来筹集额外资本。知情人士称，OpenAI已经开始说服投资者，并表示今年OpenAI 营收预计将达到10亿美元，2024年将再增加数十亿美元。</p><p>&nbsp;</p><p>10亿美元的数字与8月份媒体爆出来的数字一致。据报道，OpenAI每月收入8000万美元，高于2022年全年的2800万美元。ChatGPT Plus是其2月份推出的每月20美元的ChatGPT付费版本，推动了OpenAI的收入增长。</p><p>&nbsp;</p><p>另一方面，股票出售可以让员工不必等到公司上市就能了解自己的股权价值，可以帮助公司吸引顶尖人才，并产生流动性，同时也将为OpenAI带来新的估值。800亿美元或更高的估值将使OpenAI成为全球估值最高的初创公司之一，仅次于字节跳动和马斯克的SpaceX。</p><p></p><h2>融到的钱多，但花的钱更多</h2><p></p><p>&nbsp;</p><p>报道称，OpenAI 的目标是向硅谷投资者出售价值数亿美元的现有股票。今年4月，OpenAI从红杉资本(Sequoia Capital)、Andreessen Horowitz、Thrive和K2 Global等投资方获得了3亿多美元的融资，估值达到290亿美元。这与微软今年早些时候宣布的一项大型投资无关，该投资已于今年1月完成，投资规模约为100亿美元。据不完全统计，在OpenAI接受微软的100亿美元投资之前，它在成立七年多时间内已经收到了40亿美元的投资。至此，OpenAI 累积融资金额已经有143亿美元（以当前汇率换算，约合1044.62亿人民币）。</p><p>&nbsp;</p><p>尽管早在5月份就有报道称OpenAI正在寻求筹集更多资金，但如果出售股份的措施继续下去的话，将不会为OpenAI提供额外的运营资金，只是允许其员工剥离部分股份。</p><p>&nbsp;</p><p>5月<a href="https://futurism.com/the-byte/openai-losing-money-chatgpt">The Information</a>"报道，三位了解OpenAI财务状况的人士透露，由于去年开发ChatGPT并从谷歌招聘关键员工，OpenAI的亏损大约翻了一番，达到5.4亿美元左右。咨询公司 SemiAnalysis 的首席分析师 Dylan Patel 预计，每天运行 ChatGPT 的成本为 70 万美元。</p><p>&nbsp;</p><p>根据<a href="https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/">《财富》</a>"的披露，该公司 2022 年5.445 亿美元的总支出中，计算和数据支出 4.1645 亿美元、员工支出 8931 万美元、其他未指明具体项目的运营支出 3875 万美元。这些成本在获得微软年初100亿美元投资前就已经积累起来了。</p><p>&nbsp;</p><p>OpenAI 对产品升级更新的脚步一直没有停下，很明显巨额成本投入也在继续：</p><p>&nbsp;</p><p>9月21日凌晨，OpenAI宣布其文生图工具DALL·E即将升级至DALL·E 3，并将原生集成至ChatGPT中。相比去年发布的DALL·E 2，在提示词相同的情况下，DALL·E 3对文字的理解程度及生成的图像质量显著提升。时常被诟病的“无法在图像上生成文字”的问题，也在这次升级中得到了解决。</p><p>&nbsp;</p><p>9月26日，OpenAI 推出了多模态ChatGPT，除了通过常见的文本框交互外，还将向Plus和Enterprise用户推出语音和图像，用户可以用语音对话或向 ChatGPT 显示正在谈论的内容。语音将在iOS和Android上推出（在您的设置中选择加入），图像将在所有平台上提供。</p><p>&nbsp;</p><p>此外，8 月 16 日，OpenAI 还发布公告称收购了Global Illumination的团队，这也是 OpenAI 自 2015 年成立以来的首次对外收购，但并未公开交易涉及金额。</p><p>&nbsp;</p><p>尽管在OpenAI于2月推出付费版聊天机器人后，公司收入有所增长，但随着越来越多的客户使用其人工智能技术，以及该公司对该软件未来版本进行培训，这些成本可能会继续上升。实际上，微软和其他最近的投资者承担了上述大部分成本，但如果OpenAI不能很快实现盈利，他们可能会停止投入。</p><p>&nbsp;</p><p>与此同时，8月24日有媒体报道称，OpenAI首席执行官Sam Altman下半年将奔赴阿联酋首都阿布扎比等地寻求融资，此次融资的规模巨大，不低于1000亿美元。</p><p>&nbsp;</p><p>据悉，Altman 对VC讲的故事不限于AGI通用人工智能，他表示OpenAI的目标是要实现Super intelligence（超级智能），例如可在一个月内攻克癌症。但目前OpenAI离这一目标还非常遥远，而OpenAI为实现该目标所需要的资金规模不可想象。</p><p>&nbsp;</p><p>目前，Altman还没有通过上市筹资的打算。今年6月，Altman表示，自己不想被公开市场、华尔街等起诉，所以对上市没那么感兴趣。他解释称，当开发AI时，可能会做出一些让公开市场投资者看来非常奇怪的决定。</p><p>&nbsp;</p><p></p><h2>最大的投资人，也扛不住了</h2><p></p><p>&nbsp;</p><p>微软今年早些时候向OpenAI投资100 亿美元后，持有了其 49% 的股份。作为交易的一部分，OpenAI 承诺与其合作并将其人工智能软件与微软的产品集成，以换取用于训练和运行其模型的计算资源。微软确实也竞相在自己的大多数软件产品中内置人工智能功能，包括基于GPT-4的Windows Copilot。但如今，大模型也给微软带来了压力。</p><p>&nbsp;</p><p>该公司担心，随着Windows在全球拥有超过10亿用户，运行这些人工智能功能的成本可能会迅速增长。而微软不想放弃其新人工智能产品带来的经济效益，所以正在寻找低成本的替代方案。</p><p>&nbsp;</p><p>据知情人士透露，最近几周，负责管理微软 1500 名研究人员的Peter Lee，指示其中的许多人开发对话式人工智能，其性能可能不如GPT-4，但规模更小，运营成本也低得多。</p><p>&nbsp;</p><p>据报道，微软的研究小组对于开发像 GPT-4 这样的大型人工智能模型并不抱有幻想。该团队没有与 OpenAI 相同的计算资源，也没有大量的人工来反馈LLM回答的问题以便工程师改进。</p><p>&nbsp;</p><p>微软转向更高效人工智能模型的努力目前还处于早期阶段，不过据说该公司已经开始在Bing Chat等服务中测试内部开发的模型。</p><p>&nbsp;</p><p>微软搜索部门负责人Mikhail Parakhin此前曾表示，Bing Chat 100%依赖GPT-4的创造性和精确性模式。然而，在其平衡模式中，它使用了一种名为Prometheus的新模型和图灵语言模型。后者不像GPT-4那么强大：它们可以识别并回答简单的问题，但当它们面临更棘手的问题时，便将这些问题传递给GPT-4。</p><p>&nbsp;</p><p>在编码方面，微软最近公布了其13亿参数的Phi-1模型，据说该模型是在“教科书质量”的数据上进行训练的，可以以更有效的方式生成代码，但还没有完全达到GPT-4的标准。</p><p>&nbsp;</p><p>该公司还在研究其他人工智能模型，如Orca基于Meta的开源Llama-2模型。据介绍，Orca的性能接近于OpenAI的模型，尽管它更小，资源消耗更少。</p><p>&nbsp;</p><p>报道称，微软的人工智能研究部门有大约2000张来自英伟达公司的显卡可供使用，Lee&nbsp;现在已经下令将其中的大多数显卡用于训练专注于执行特定任务的更有效的模型，而不是更通用的GPT-4。</p><p>&nbsp;</p><p>不可否认的是，OpenAI 和其他开发商，包括谷歌和 Anthropic，在开发高级LLM方面领先于微软，但微软或许能够以极低的成本参与到构建类似 OpenAI 软件质量的模型竞争中。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://siliconangle.com/2023/09/26/openai-reportedly-exploring-share-sale-80-90b-valuation/">https://siliconangle.com/2023/09/26/openai-reportedly-exploring-share-sale-80-90b-valuation/</a>"</p><p><a href="https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending">https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending</a>"</p><p><a href="https://www.infoq.cn/article/JAqkPkkgMFvV0JvcFMtj">https://www.infoq.cn/article/JAqkPkkgMFvV0JvcFMtj</a>"</p><p><a href="https://siliconangle.com/2023/09/26/microsoft-hedges-bets-seeking-cost-effective-ai-models/">https://siliconangle.com/2023/09/26/microsoft-hedges-bets-seeking-cost-effective-ai-models/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE</id>
            <title>Google Vertex AI 推出搜索对话式界面，简化AI建模</title>
            <link>https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Cpge73IKHuvtVRimGdDE</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Sep 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Google Cloud Next, Vertex AI, 搜索引擎, 聊天机器人
<br>
<br>
总结: 在Google Cloud Next大会上，谷歌介绍了其企业人工智能平台Vertex AI的新功能，包括搜索引擎和聊天机器人。开发人员可以利用Vertex AI Search从各种企业资源中检索信息，并与客户互动。而Vertex AI Conversation则可用于创建声音自然、类人的聊天机器人。这些功能的目标是实现更高级的用户工作流。 </div>
                        <hr>
                    
                    <p>在<a href="https://cloud.withgoogle.com/next?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Google Cloud Next</a>"大会上，谷歌<a href="https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-search-and-conversation-is-now-generally-available/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">介绍了其企业人工智能平台Vertex AI</a>"的新功能，这个平台旨在实现更高级的用户工作流。</p><p></p><p>在上次<a href="https://blog.google/technology/developers/io-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Google I/O大会</a>"上作为技术预览发布之后，谷歌扩展了Vertex AI Search和Conversation功能，并宣布其普遍可用。</p><p></p><p>借助Vertex AI Search，开发人员可以从各种企业资源中检索信息，例如文档存储库、数据库、网站和其他类型的应用程序。谷歌表示，开发人员可以很轻松地构建搜索引擎，并基于这些信息与客户展开互动。</p><p></p><p></p><blockquote>Vertex AI提供了一个简单的编排层，将企业数据与生成式基础模型以及会话AI和信息检索技术结合起来。</blockquote><p></p><p></p><p>Vertex AI Conversation可用于创建声音自然、类人的聊天机器人。例如，开发人员可以基于网站、企业文档、常见问题解答、电子邮件等创建聊天机器人。</p><p></p><p></p><blockquote>Vertex AI可以帮助开发人员将确定性工作流与生成式输出相结合，将基于规则的流程与动态AI相结合，构建出迷人且可靠的应用程序。</blockquote><p></p><p></p><p>基于谷歌宣布的这些新功能构建的智能应用程序不仅仅是检索重要信息那么简单，而是可以代表用户做出动作。</p><p></p><p>新功能支持处理后续问题，无需重新开启对话，这要归功于多轮搜索。通过扩展和数据连接器与第三方应用程序进行更紧密的集成，从而可以连接到各种应用程序，如MongoDB、Datastax、Salesforce、JIRA等。基于企业数据的生成式输出增加了返回结果质量置信度。</p><p></p><p>Vertex AI Search还可以与矢量搜索相结合，利用其寻找“类似”解决方案的能力来支持语义搜索、个性化推荐、多模态搜索等。</p><p></p><p>Vertex AI Search将在未来支持访问控制，确保用户只能看到他们有权看到的信息和访问其他功能，如引用、相关性评分和摘要。</p><p></p><p>Vertex AI是一个面向非机器学习开发人员的平台。几个月前，<a href="https://www.infoq.com/news/2023/06/ai-ml-data-news-june12-2023/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Google为Vertex AI提供普遍可用的生成式AI支持</a>"，包括基于<a href="https://ai.google/discover/palm2/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">PaLM 2</a>"的文本模型，文本Embeddings API和<a href="https://cloud.google.com/model-garden?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">Model Garden</a>"中的基础模型。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2OTU2MDU5MTIsImZpbGVHVUlEIjoiZFBrcGRWZ283NFQxUVJrTyIsImlhdCI6MTY5NTYwNTYxMiwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.4UZnL0yLx1yRK-8v01YKShjBP-z3XqAzgHcFAZwrsxM">https://www.infoq.com/news/2023/09/vertex-ai-search-conversation/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO</id>
            <title>和德爷一起 6DoF 互动探险，火山引擎空间重建和虚实融合技术</title>
            <link>https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Eh2tQrXjDfuagCeWCAhO</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Sep 2023 07:49:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 6DoF互动纪录片, 德爷, 非洲大草原, 空间重建技术
<br>
<br>
总结: 《跟着德爷闯东非》是一款全新的6DoF互动纪录片，由明星探险家德爷担任主角。通过空间重建技术和虚实融合技术，观众可以以第一视角摄影师的身份陪伴德爷一起冒险，近距离观察野生动物，体验非洲大草原的野外生存之旅。这种全新的互动体验增强了VR内容的沉浸性和交互性，让用户跟随德爷的脚步沉浸式体验从城市“跃入”荒野的快感。同时，空间重建技术能够复原现实世界的场景和物品，并转化为数字资产，为计算机视觉和摄影测量等领域提供了重要应用。 </div>
                        <hr>
                    
                    <p></p><blockquote>《跟着德爷闯东非》是 Pico 一款全新的 6DoF 互动纪录片。主角由在全球拥有 530 万粉丝的明星探险家德爷（Edward James Stafford）担任。观众以第一视角摄影师的身份陪伴德爷一起冒险，近距离观察野生动物，体验非洲大草原的野外生存之旅。</blockquote><p></p><p></p><p>与行业内常见的不具备互动性的 3DoF 实拍纪录片以及不具备写实性的游戏引擎制作的 6DoF 纪录片不同，《跟着德爷闯东非》纪录片的拍摄采用空间重建技术及虚实融合技术，兼顾实拍和虚拟互动，以全新的 6DoF 互动体验，增强了 VR 内容的沉浸性和交互性，让用户跟随德爷的脚步沉浸式体验从城市“跃入”荒野的快感。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/56/68/560d014cbcfef595ec7899b19a3f2368.gif" /></p><p></p><p><img src="https://static001.infoq.cn/resource/image/9d/d9/9d9c2c4fbc8cd9573f1faf6c87cdc3d9.gif" /></p><p></p><p></p><h2>全新VR空间互动性的挑战和难点</h2><p></p><p></p><p>传统 VR 实拍视频的交互通常采用不同选项触发不同结局的 AB 型互动方式，而《跟着德爷闯东非》想要实现的是全新的 VR 空间互动，用户可以抓起虚拟世界中的物体完成任务，比如和德爷一起钻木取火，也可以在场景中自由走动，在非洲草原和德爷一起追捕猎杀珍珠鸡，实现全新的交互并达到高沉浸感，突破传统视频的界限。</p><p></p><p>而想要实现全新的交互和体验高沉浸感，需要做到：</p><p>实际拍摄的 VR 视频和用于互动的场景、物品具备高度一致性，包含几何结构、纹理色彩及光影的一致性，这对于空间重建技术提出了很高的要求，需要做到高精度、高质量、真实感建模，超写实数字复原空间场景，让用户感受到原汁原味的非洲荒野。用户在体验中，虚拟元素与实际场景相互交织，让用户在场景中难以区分哪些是真实的，哪些是虚拟的，达到了最佳的体验效果。这就要求无缝虚实融合技术，需要对重建的数字素材和实拍的视频素材进行像素级配准，这样用户才能够自然沉浸的地在场景中和德爷一起探索非洲，并从实拍 3DoF 视频中德爷手里顺利接过数字重建的 6DoF 互动道具。</p><p></p><p></p><h2>空间重建，复原现实世界</h2><p></p><p></p><p>空间重建技术能够复原现实世界的场景和物品并转化为数字资产，是计算机视觉和摄影测量中的重要研究课题，也在智慧城市、虚拟现实、数字导航与数字遗产保护等方面有着重要的应用。</p><p></p><p>火山引擎多媒体实验室团队自研改进 SFM 算法框架，实现高精度的场景稀疏重建及图像定位。</p><p></p><blockquote>针对特征点提取、匹配算法，通过结合传统特征与深度学习方法，算法在大视角/尺度变化、暗光、弱纹理、运动模糊等多种挑战场景下仍能有效提取足量稳定的特征；通过将特征点纳入自注意力和交叉注意力网络，结合多源传感器输入检索全局最优图像特征匹配，使得算法即使在空地跨视角、鱼眼/针孔/全景跨相机等复杂数据输入的情况下，实现建图完整度、成功率均达到 100%。同时，开发支持多相机系统、多相机模型光束法平差优化算法，同时也兼容其他不同传感器的联合重建，实现高精度、多模态的位姿估计。</blockquote><p></p><p></p><p>在稀疏重建算法之后，需要进行稠密算法重建。</p><p></p><blockquote>火山引擎多媒体实验室通过立体视觉 (Multiple View Stereo，简称 MVS)技术将二维图像信息转化为三维点云信息。团队自研基于多目立体视觉及全景图的深度估计算法，通过神经网络进行稠密深度估计，在野外大场景环境获得高精度的场景稠密几何测量。获得点云信息后，进行点云去噪和补全，并通过点云配准实现场景几何一致性。最后，通过基于 TSDF 和图像语义信息的点云融合策略，进一步滤除噪声，生成更加平滑一致的完整场景点云。</blockquote><p></p><p></p><p>获得场景点云后，进行 Mesh 重建。</p><p></p><blockquote>火山引擎多媒体实验室自研多种网格优化算法，实现网格平滑、去噪、简化和补洞，获得更加精细、完整的高质量网格模型。得益于图像处理期间高精度的相机位姿估计以及图像超分等画质优化，结合自研贴图算法，获得更高清、拼缝更少的高质量纹理贴图。同时通过纹理重打包算法优化，实现更高的纹理利用率，降低存储资源浪费，提升纹理有效分辨率。</blockquote><p></p><p></p><p></p><p></p><p>目前，<a href="https://www.infoq.cn/article/Rx45QcxHI4zZCfMR5r8J">火山引擎视频云</a>"平台具备自动化空间建模链路，助力大场景重建，可支持采集 RGBD/RGB 数据（无人机、手持采集等）自动化上传云平台，2-4 小时后自动产出建模结果，建模精度可达1cm～2cm。同时，火山引擎视频云的云渲染可视化系统，联合自研动态传输算法，可实现高度真实感的模型渲染。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02b37e1b606348e6ea19f8e85833480f.png" /></p><p>图：火山引擎视频云三维重建平台</p><p></p><p><a href="https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT">火山引擎</a>"多媒体实验室将神经辐射场技术（NeRF）与自研大场景建模技术相结合，研发行业领先的大场景光场重建方案，实现高度真实感（psnr&gt;30)的场景重建、复现及后编辑。</p><p></p><p>在具体实践的场景中，动态物体会使NeRF重建出现伪影，借助自研动静态分割、影子检测、inpainting等算法，对场景中和几何不一致的区域进行提取、修复。同时借助自研高精度SFM算法框架，对场景进行高精度的几何重建，包括相机参数估计以及稀疏、稠密点云生成。另外，对场景进行拆分以减小单次训练资源消耗，并可做分布式训练、维护。在神经辐射场训练过程中，针对室外无边界大场景，团队通过优化策略以提升该场景下的新视角生成效果，比如，通过在训练中同时优化位姿提高重建精度、基于哈希编码的层次化表达提升模型训练速度、借助外观编码提升不同时间采集场景的外观一致性、借助mvs稠密深度信息提升几何精度等。</p><p></p><p>以团队同毫末智行合作为例，完成单路采集以及多路合并的NeRF重建，相关成果已在毫末AI Day发布。  </p><p></p><p></p><p></p><p></p><p></p><h2>虚实融合，提升用户体验</h2><p></p><p></p><p>为提升用户沉浸式体验，火山引擎多媒体实验室自研虚实融合技术，将环境实拍全景图与场景模型进行对齐、融合。团队利用先进的人工智能技术，建立全景图图像特征与模型关键点的匹配关系，通过 PnP 算法以及光束法平差算法将全景图注册至场景模型坐标系，实现尺度、位置的统一，从而实现模型渲染与实拍全景视频渲染的统一，达到虚实融合的效果。</p><p></p><p>同时，为扩大用户体验的自由度，<a href="https://www.infoq.cn/article/qC55OH6f6852hFjlZ3o8">团队</a>"针对该场景自研非球面天空盒渲染，克服传统的球面全景图渲染仅在图像采集中心视觉一致的缺陷，进一步提升实拍全景图渲染模型与地形模型的匹配程度，以实现更大运动范围的视觉一致性，进一步提升沉浸式体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec236d31adc57de0cf0de1c9245c5715.png" /></p><p>图：虚实效果对比示例</p><p></p><p></p><p></p><p></p><p>以上2个视频：6DoF互动漫游</p><p></p><h2>物品重建，高精度还原细节</h2><p></p><p></p><p>在《跟着德爷闯东非》互动纪录片中，会有用户虚拟体验探险剧情的桥段，例如钻木取火，木棍训蛇等。为了带来真实的体验，道具往往是在实际拍摄过程中就地取材，有细长的树枝，薄薄的小刀，还有形态复杂的篝火堆。这些道具的重建本身是比较有挑战的，再加上整个拍摄过程比较紧张，留给扫描的时间并不充裕。为此，火山引擎视频云团队沉淀出一套采集方便，操作简单，能还原各类复杂物品的重建系统。</p><p></p><p>为了重建形状比较复杂的道具（例如狭长的木棍、锋利的小刀）。火山引擎视频云采用符号距离场（Signed Distance Fields，简称 SDF）的技术方案来表示三维物体，结合深度学习的方法克服了以上重建难点。对于如何监督神经网络使其准确地拟合该 SDF，火山引擎视频云先用运动恢复结构（Structure from Motion，简称 SfM）算法，精确计算拍摄图像的相机姿态，再利用可微渲染的方法将 SDF 所表示的空间信息渲染到图像上，把渲染得到的图像和该视角下采集的图像做比较，不断优化神经网络，使 SDF 在各个采集视角下的渲染结果尽可能与实际采集的图像一致。为了进一步提高重建精细度，在优化 SDF 的时候加入稀疏重建得到的三维点做约束，能更好的还原物体的细节特征。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b8021233a9722a127a927c05e72f752.jpeg" /></p><p>图：道具重建效果展示</p><p></p><h2>交互技术，让玩法更丰富</h2><p></p><p></p><p>采用虚实融合技术可以构造由空间重建模型和实拍 360°VR 视频两部分构成的 6DoF 互动场景，同时在《跟着德爷闯东非》项目中，多媒体实验室也实现了终端上的交互技术，同内容团队一起创造出了很多有创意性的虚实结合的玩法。</p><p></p><h4>拍照功能</h4><p></p><p>使用离屏相机管道，把从全景视频球上投影出的针孔 2D 图像重新贴在玩家手持的相机模型上，以实现出玩家可以对环境中任意角度拍照的玩法。</p><p><img src="https://static001.infoq.cn/resource/image/3a/a4/3a8483c3477e022edf09dcce6ayya5a4.gif" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5cca054c1d8e6fe07db04816dd5488ee.gif" /></p><p>图：拍照功能示例</p><p></p><h4>物品交互功能</h4><p></p><p>火山引擎多媒体实验室可以估计 VR 视频中的深度信息，结合 3D 虚拟空间中的虚拟物体的位置信息，计算出全景视频球上指定视频元素，对应于玩家在真实的 3D 空间下的位置。从而，实现视频画面上真实物品转换到玩家可交互虚拟物品模型的无缝转换的玩法。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/50/ea/50a972ff746593f453228a90610993ea.gif" /></p><p>图：物品交互功能示例</p><p></p><h2>虚实融合技术的广阔应用</h2><p></p><p></p><p>虚实融合技术目前正处于快速发展的阶段，在众多领域中展现出广阔的应用前景。如游戏、教育和医疗等领域，已开始积极探索虚实融合技术的应用，并取得了不错的成绩：</p><p></p><p>在游戏领域，虚实融合技术赋予了游戏开发者更多创造力和想象力的空间。通过将虚拟元素与真实世界相结合，游戏能够提供更加沉浸式和交互式的体验。玩家可以与虚拟角色和游戏环境进行实时互动，增强了游戏的娱乐性和参与感。</p><p></p><p>在教育领域，也可以看到虚实融合技术的巨大潜力。通过将虚拟内容融入到教学场景中，学生可以以更加生动和直观的方式进行学习，提高学习效果和兴趣。虚实融合技术可以为学生提供与实物互动的机会，使他们能够亲身体验和理解抽象概念，促进知识的深入理解和记忆。</p><p></p><p>在医疗领域，虚实融合可以用于模拟手术训练、辅助手术导航和可视化诊断等方面。通过结合虚拟现实和真实世界数据，医生可以更准确地进行手术规划和操作，提高手术的安全性和成功率。此外，虚实融合还可以用于康复训练和疼痛管理等方面，为患者提供更加个性化和有效的治疗手段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ARwQIt6s0vBAwArADjfW</id>
            <title>答记者问：从 PICO 视角，看 XR 行业发展</title>
            <link>https://www.infoq.cn/article/ARwQIt6s0vBAwArADjfW</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ARwQIt6s0vBAwArADjfW</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Sep 2023 06:55:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 马杰思-PICO OS产品负责人, 郭文山-PICO 中国游戏商务负责人, 顾知-PICO 中国非游商务经理, 李卓然-PICO 消费者市场部内容团队市场经理
<br>
<br>
总结: PICO公司的马杰思、郭文山、顾知和李卓然是该公司的关键人物，分别负责PICO OS产品、游戏商务、非游商务和消费者市场部内容团队的工作。 </div>
                        <hr>
                    
                    <p></p><blockquote>接受采访嘉宾：马杰思-PICO OS产品负责人郭文山-PICO 中国游戏商务负责人顾知-PICO 中国非游商务经理李卓然-PICO 消费者市场部内容团队市场经理熊俊杰-PICO 视频内容负责人</blockquote><p></p><p></p><p>“PICO 2023 首届 XR 开发者挑战赛”报名链接：</p><p><a href="https://www.picoxr.com/cn/2023picodevjam/#/introduce?trackingFlag=EkWdMubc">https://www.picoxr.com/cn/2023picodevjam/#/introduce?trackingFlag=EkWdMubc</a>"</p><p></p><h4>问题 1： 请问随着应用达到 500 家，您在这几年发展过程中，应用增加和用户活跃度是否产生变化，有关键节点和相关曲线的描述吗？</h4><p></p><p></p><p>郭文山：我们内部看到两个维度，关注用户留存和用户活跃度提升的影响因素。当前的 500 多款应用供给不足以满足每个用户的需求。我们认为关键不在于绝对数量，而是长期来看，而是满足不同类型用户品味的内容。其次可以看购买设备后第一个月购买几款内容，这也是很重要的指标。我们内部发现，用户在使用设备的第一个月能找到 3- 4 款最适合自己的内容，会显著提高留存率。</p><p></p><h4>问题 2： 预计应用达到什么量级时，生态能有一个比较大的超级爆发？</h4><p></p><p></p><p>郭文山： 这个问题也是我两年前加入 <a href="https://www.infoq.cn/news/EjqbmPdv0fSvUkWqgZ2k">PICO </a>"时就在探讨的。我认为关键是提供优质、破圈的内容。比如，如果推出像“节奏光剑”这样的产品或者类似“健身环”的破圈产品，可能会快速推动行业进入下一个阶段。我们期待开发者生态中能够结合技术发展趋势，去发现简单、易玩的交互方式，这样的内容有可能成为真正的大作，帮助行业走向下一个阶段。</p><p></p><h4>问题 3： 关于技术侧，随着苹果和国内一些新品发布，伴随 AI 技术的发展，行业对 AR 方向更加肯定。想请问我们对 VR、AR 或 MR 未来格局趋势的看法，会坚定 VR 路线还是会有相应调整？</h4><p></p><p></p><p>马杰思：从一开始的讨论就可以看出，行业发展趋势明显是从 VR 向 XR 发展，从沉浸式虚拟世界向能赋能更多现实世界的 MR 能力发展。因此，我们采取了 VR+MR 的策略，设备可以无缝切换 VR 和 MR 状态。这种技术的优势在于一个设备可以同时满足 VR 和 MR 的需求。我们会持续进行这方面的发展，将设备设计成既能满足虚拟场景需求又能满足现实场景需求的 VR+MR 设备。同时，在 MR 方面我们分享了很多关于环境感知的技术，这是一个开始，未来我们将在这方面做更多的创新和发展。</p><p></p><h4>问题 4： 我们注意到硬件产品迭代速度非常快，每代产品的技术和创新点都不同。一些开发者担心如果现在开发完整产品可能不适应未来产品特性，选择先制作一些 Demo 并放在平台上。另外，尽管产品迭代提升，我们很难看到高质量的产品内容展示，只能看到一些创新的 Demo，但却没有看到后续版本的更新。您如何看待这种情况，认为在硬件产品迭代和内容创新适配方面，包括版本更新上，如何取得较好的平衡？</h4><p></p><p></p><p>马杰思： 这个问题可以分别由我和文山来回答。首先关于兼容性问题，就我们 PICO 产品线而言，你不会看到这种情况。我们现在为 PICO 4 或 PICO Neo 3 开发的产品，肯定也能在未来的产品上运行。例如，我们现在使用的 SDK 2.3.0 版本，如果用于开发 MR 游戏或应用，它在我们下一代产品上肯定也能完美运行，所以不存在你所担心的问题。但是在不同厂商之间，是否存在生态分裂需要开发者适配的情况，这确实存在。当开发者要跨足多个不同硬件时，他们需要去做适配的准备。</p><p></p><p>郭文山：接着来谈谈内容更新的问题。确实，有些开发者会快速制作 Demo 上线，但后续却没有更新。为什么会出现这种情况呢？事实上，中国的 XR 开发者生态规模相对较小，大多数是小团队制作，面临经营资金的压力。当他们上线一个 Demo 时，他们首先验证的是什么？并不是期望立刻爆红成为热门，很多开发者也知道这样的机会不是轻松得到的。但他们仍然要尝试，这也是我理解XR开发者生态的难处，这需要克服困难，他们要拿出第一版作品。他们先测试核心玩法是否奏效，这对于大多数开发者和同行来说是内容制作阶段的一种经历。只有少数开发者可能考虑得更长远，构思不同版本，如半年或一年后的迭代版本。然后会出现这样一种情况，有可能刚上线一个版本，你可能看到他只有 10 个内容，但实际规划可能是 50 个。然而，在第一天上线 10 个内容后，他发现核心玩法未能满足用户需求，就会撤退，进入下一阶段，打磨自己的玩法。我尝试<a href="https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT">理解</a>"为什么我们会注意到和观察到这种现象，这也是行业发展的一种规律。</p><p></p><h4>问题 5： 除了开发者大赛，还有哪些方面可以完善我们的内容建设？对于生活教育、培训、游戏、体育等方面，目前我们更侧重于哪一个？</h4><p></p><p></p><p>郭文山：我们认为需要多个方面共同努力，而不是单一侧重于某个方面。首先，对于 PICO 来说，最重要的是确保硬件产品的 OS 和 SDK 强大可靠，这是我们的首要任务。我们要把 PICO 设备在中国和全球推广好，这是最关键的。我们也在和开发者交流，了解他们的期待。大部分开发者的第一反应是我们要推广 PICO 设备，这对我们的硬件、软件 OS 至关重要。此外，我们也要注重内容生态。我们正在尝试多种方式，包括成立专门团队进行发行，解决开发者资源不足的问题。我们也推出了 Beta 版本的快速体验通道，并举办了社区活动。我们认为不能依赖于单一点，我们需要系统地探索哪个环节对开发者最重要，这也需要和开发者共同讨论，因为每个开发者面临的困难都不同。我们希望能根据不同的情况，提供组合服务，以满足大家的需求。</p><p></p><p>顾知：针对不同行业的适用情况也不同。例如，视频行业可能已经比较成熟，而新兴领域如汽车发布可能需要更多的行业和企业付费。创意工具可以适用于办公场景、线下培训、会展等多个场景。我们还可以探索从沉浸式到虚拟现实的新想法，结合大环境和大空间，这是一个有创意的领域。从 2D 移动端到 VR 入口端，可以尝试多种使用情况。我们也有“抢先体验专区”，提供一种轻松快捷的尝试方式，以及今天的挑战赛等，让开发者可以尝试新的想法。我们鼓励大家在不同行业想象和尝试，从 2D 到 VR，尝试多种入口端，以满足不同用户的需求。</p><p></p><h4>问题 6： 之前有媒体提及 PICO 在内容方面要追赶内容数量和质量，并指出可能受到海外同行的压力。对此，PICO 是如何看待这种评价？</h4><p></p><p></p><p>郭文山：我们并不是以同行之间的压力为考量。我们更关注用户的需求和视角。当用户决定花费 400 美金购买 PICO 时，他们综合考虑了很多因素，不仅仅是硬件配置和系统体验的流畅程度，也包括是否有丰富的内容。因此，我们认为从用户视角出发，内容工作和内容生态的建设非常重要。</p><p></p><p>从绝对数量上看，确实在去年的时间点，我们去年应用数量确实落后于同行不少，但这不是我们的压力来源，我们真正面临的压力来自于用户。我们感受到了我们还需要在满足用户需求方面，做出更大努力。这种压力将持续存在，不仅是今天，也会延续到下一个阶段。因此，我们将持续努力以满足用户的多元化需求。竞争考虑对我们来说是次要的，我们更关注如何为我们的<a href="https://xie.infoq.cn/article/40e1258dc998d9c7ce6380c8a">客户</a>"提供优质服务。</p><p></p><h4>问题 7： 对于非游戏类开发者，PICO 这次比赛侧重于哪些方面，有哪些机会？</h4><p></p><p></p><p>李卓然：PICO 这次比赛更加侧重非游戏类开发者。在中国，XR 游戏开发者圈子相对较小，而面向未来的技术趋势和交互提出了新的可能性。原本从事手机、安卓、iOS 等开发的人员也有机会进入 XR 这个领域。这次 XR 开发者挑战赛的重点是不要求开发者制作出可运行的实际 demo，也鼓励传统行业以及 ToB 领域的开发者参与。比赛规则还强调更开放的场景设置，降低比赛门槛，拉动不同背景的开发者参与。同时，比赛设置了非游戏赛道奖励，直接激励非游戏内容的产出。</p><p></p><h4>问题 8： 未来，PICO将如何加强与开发者之间的联系，促进生态的发展？</h4><p></p><p></p><p>郭文山：PICO 将采取多种方式加强与开发者之间的联系，推动生态的发展。首先，技术布道是重要的一方面，通过与垂直领域的媒体合作，向开发者群体介绍产品 SDK、技术进展和最新可能性。这将提供开发者更多了解 PICO 技术方面的机会。</p><p></p><p>其次，PICO 重视社区工作，包括开发者社区的建设。他们鼓励开发者利用抢先体验端等平台，建立与用户的社区，分享经验、交流技术。PICO 还会有计划地组织社区交流活动，分享最佳实践，例如通过专访形式将实践经验传播给更多同行。</p><p></p><p>总的来说，PICO 将通过技术布道、社区建设和合作活动等多种方式，与开发者建立紧密联系，促进XR生态的健康发展。</p><p></p><h4>问题 9： 对于开发者来说，在未来 2 至 3 年的市场情况中，消费者对哪种内容和技术更感兴趣？从 PICO 的角度，对开发者开发哪些内容有推荐？</h4><p></p><p></p><p>郭文山：针对这个问题，我们首先要考虑到两年的时间维度内的技术限制。XR行业长期愿景可能是下一代计算终端，但在短期内（两年）技术的限制可能会对开发者创意造成限制。例如，手势自定义的可能性在两年内可能受到技术的限制，比如你只能基于有限的手势去做组合，这是一种限制。</p><p></p><p>要回答这个问题，我们需要考虑两年的时间跨度，但不仅仅从 PICO 的角度来看，还要考虑整个行业的技术发展。共识是从显示方面来看，在未来两年内可能会发生重要转折。从 VR 到 VR 加 MR 的可能性，这可能是因为屏幕和前置摄像头发生了变化，我们觉得这个方向一定有可能产生非常有趣的新内容。交互方面则是杰思提到的更加多元的人机交互，多模态的融合交互等是会有很多机会。</p><p></p><p>具体内容的创新和发展会依赖于生态合作伙伴的创意和开发。例如，可以想象在新一代设备中，以前玩的游戏像 Pokémon Go，你可以使用新的裸手技术去抓小怪兽，可能会有更自然的交互方式，比如裸手扔球。我们认为，XR原生要抓住技术趋势，创新出吸引消费者的内容，这肯定有可能实现，但是具体是哪家公司实现，我们无法确定。</p><p></p><h4>问题 10： 未来是否会和开发者签订一些独占协议？如果会，独占内容和非独占内容的比例会如何控制？ 如果可能会独占，独占的内容是否会进行自研？是否会为多平台开发者提供适配 PICO 平台的小工具？</h4><p></p><p></p><p>郭文山：我们一直在思考独占的问题，特别是在加入字节后。独占是否关键，是否能成为用户购买的理由，是我们需要思考的重要问题。当前行业的主要矛盾是行业仍处于早期阶段，整个行业的出货量较小，应用生态中很有人没有盈利。因此，对于绝大多数开发者来说，独占并不是一个好的选择，特别是对于多人联机类的内容。独占会使他们失去与其他开发者合作的机会，让用户失去获得更优秀的游戏体验的可能性。对于独占，我们内部非常谨慎和克制。与此同时，我们更强调开放，我们希望每个品类都有一大批第三方开发者贡献创意。</p><p></p><p>对于自研，我们认为自研是一个很好的方向。举个例子，我们推出了全身动捕的新 feature（技术特性），我们可能会在前沿领域进行自研，前进一步，做探索类似“闪韵灵境”（全身动捕类音游）这样的玩法。但我们的目标并非自研，而是通过自研打造、演示实践案例，并在后续开放实践经验。我们希望通过开放合作的方式推动行业进步。</p><p></p><p>马杰思：对于多平台开发者，我们已经推动了一些行业标准，比如 OpenXR 标准。我们积极参与 OpenXR 的制定，以便开发者基于这个标准开发游戏，从而增强游戏的可移植性。 PICO 是 OpenXR 的贡献者之一，我们会与 OpenXR 成员一起努力，建设好 OpenXR 生态，使开发者能更容易地将内容移植到不同平台上。</p><p></p><h4>问题 11： 请问关于三体VR项目，能否透露一下上线日期以及开发进展情况？</h4><p></p><p></p><p>熊俊杰： 三体 VR 目前正在紧锣密鼓地开发进行中，但由于项目内部保密要求，暂时无法公布太多细节。但可以分享的一点是，整个开发的进展符合我们的预期，并且我们会加快开发进度，尽早与大家见面。</p><p></p><h4>问题 12： 近半年 PICO 是否倾向引入或自研人文历史、自然探索等类型的内容？</h4><p></p><p></p><p>熊俊杰：PICO 在视频和直播方向探索的范围是相当广泛的。除了人文历史和自然探索等内容外，我们也探索了诸如互动游戏和互动剧等内容，比如“探灵”和“我的 48 次心动”。我们的主要目标是探索创新技术如何服务内容创作，尤其在行业早期，探索行业的创新方向。</p><p></p><p>我们非常关注 PICO 硬件性能和软件能力，如8K清晰度、空间音频应用、六自由度互动能力、宽频马达和面部追踪技术等，以更好地服务内容创作。目前，我们已经与全球 30 多个机构合作，持续开发各种创新内容，包括视频和直播。在直播方向，我们合作了体育赛事、电竞、演唱会等多种领域，并将继续在MR方向探索创新，如如何更好地结合MR和视频，以及更自然的交互方式等。</p><p></p><h4>问题 13： 我是 PICO 的用户，我观察到 PICO 的内容在不断增加，也感觉到 PICO 的进步很大。我想了解一下，PICO 的设备已经卖出去多少，以及用户的活跃情况如何？</h4><p></p><p></p><p>郭文山：关于 PICO 活跃用户的数据我们不方便直接透露，但可以大概说明一下全球 VR 行业活跃情况，行业维度看 VR 用户活跃水平可以大致理解为与游戏主机相当，游戏主机的用户平均每月活跃大约在 5-8 次左右。在当前硬件和内容情况下，这是一个相当大的行业性挑战。提高活跃水平需要更多技术突破和内容创新，我们相信 PICO 可能略优于游戏主机，因为我们之前的调研显示很多 PICO 用户也是游戏主机的用户。</p><p></p><h4>问题 14： PICO 平台上有 500 多款应用，可以分享一下这些应用背后开发者的背景以及开发者群体的画像吗？</h4><p></p><p></p><p>郭文山：在这 500 多款应用中，大约有 1/5 由中国开发者开发的。中国开发者的画像可以大致分为几类。首先是热爱 VR 技术的游戏开发者，他们可能在 2015 年和 2016 年第一波 VR 时代就已经开始了。他们有游戏研发背景，一部分持续在做 To C（面向消费者）的产品，一部分可能转向偏向 To B（面向企业）的项目。第二类是创业者，特别是在 Meta、苹果等公司的号召下，有创业经验的人开始涌入 XR 行业。他们对新技术、新趋势敏感，有可能并非来自 XR 行业。第三类是来自行业的开发者，比如曾经做 2D 应用的公司，像爱奇艺、哔哩哔哩等大公司，也是 PICO 的重要合作伙伴。</p><p></p><h4>问题 15： 我注意到 PICO 平台上的视频应用大多数都是平台类应用，比如bilibili、爱奇艺，以及一些工具类应用。在 VR 平台上，我很少看到综合性、现代化、更软件化的视频应用，像抖音这样的综合性应用。PICO 有没有意向去引导或建议开发者往这个方向去探索或开发更新的视频软件形式？</h4><p></p><p></p><p>熊俊杰：在 VR 和 XR 这个全新媒介下，创作有着基本属性，比如空间性和互动性。这与传统视频以摄像头方式连接的方式有很大不同。互动性使视频和游戏的边界变得模糊，内容可能以 APK 的格式存在。PICO 视频聚合应用虽然以视频为主，但实际上也支持很多以 APK 格式存在的创新类应用。这种特点是 PICO 的优势之一。PICO 能够在 PICO 视频和应用商店两个平台上分发这些创新应用，为开发者提供更多流量。开发者可以选择不同平台上的变现模式。对于以 APK 格式存在的创新内容和内容创作者来说，同样也是一个优势。 PICO 视频也支持很多创作者以 APK 的方式存在，这些创作往往是相对创新的格式，介于游戏和视频之间，例如一些创新的 VR 体验应用。 PICO 通过支持这样的应用方向，持续探索和创新。</p><p></p><p>马杰思补充：作为消费者，我觉得开发者追求迅速回款和维持团队运营的需求与用户更希望看到更多创新之间存在冲突。目前，我觉得 VR 应用领域存在同质化问题，尤其是在 VR 游戏方面，比如大量的射击、汽车和球类游戏，这些可能是因为这些类型相对快速开发、成熟。在这种情况下，我们应该如何避免同质化？我们可以引导开发者利用现有的新技术，如交互，以影响开发方向，而不仅仅是推动开发更多同质化的游戏。</p><p></p><p>从技术的角度来看，尽管有新技术的支持，但人们可能仍然喜欢做枪、车和球类游戏，尽管这些游戏可能是新的枪、车和球。消费者可能不一定非要追求与枪、车和球不同的体验，但他们可能想要一种独特的体验，一种与传统不同的枪、车和球。新技术，特别是与 MR 相关的技术和人机交互相关的技术，肯定能够对XR内容生态产生影响。在过去十年间，VR 的体验基本上是通过六自由度手柄进行设计，但在过去一两年间，这种情况开始改变，不再局限于 VR 体验，还可以是 MR 体验，也不再局限于六自由度手柄，可以使用手部或其他体感追踪器。这种变化确实发生在最近一两年，这也是为什么我认为会有一些新趋势出现的原因。因此，我相信会出现一些不同类型的应用和游戏，可能不是强制性的，但会有很多新的应用和游戏。</p><p></p><p>李卓然补充：在制定赛制规划时，我们一直在思考是否应该强制开发者朝着更创新的方向发展。我们始终秉持着更加开放的态度，不强迫开发者朝特定方向发展。但是我们确实在评分规则和奖励设置上向创新性应用方向倾斜，特别是对与MR相关的赛道提供特殊支持。</p><p></p><p>首先，在评分方面，我们基于我们在八月下旬推出的 SDK 2.0，这个 SDK 关注 MR 相关技术。在评分中，我们明确表示，如果开发者更多地应用这些新技术，他们将在评分中获得更高的分数。其次，在奖项设置方面，我们特别设置了创新类的应用内容奖项，以及针对 MR 赛道和手势赛道的奖项。我们希望通过这些奖项的设立，吸引开发者更多地朝着这些创新方向进行内容开发。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0higrDEDX6Zs81dXCaSf</id>
            <title>首个千亿医药对话大模型来了，要打破医药研发“三十定律”</title>
            <link>https://www.infoq.cn/article/0higrDEDX6Zs81dXCaSf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0higrDEDX6Zs81dXCaSf</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Sep 2023 02:15:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 医药研发领域, 三十定律, 水木分子, ChatDD-FM 100B
<br>
<br>
总结: 医药研发领域存在着三十定律，即投入超10亿美金的研发项目成功率只有10%。为了打破这个魔咒，水木分子发布了新一代对话式药物研发助手ChatDD和全球首个千亿参数多模态生物医药对话大模型ChatDD-FM 100B。ChatDD能够在立项、临床前研究、临床试验等环节提供服务，而ChatDD-FM 100B在医学专业评测中表现出色。 </div>
                        <hr>
                    
                    <p>10年时间，投入超10亿美金，但成功率只有10%——这是医药研发领域的“三十定律”。从某种程度来说，它像是医药行业的一个“魔咒”。</p><p></p><p>一个药物在正式上市之前，需要经过立项调研、早期药物发现、早期临床前研究、临床试验药物重定位等各个流程。对于药企而言，整个过程存在三大痛点：第一，信息繁杂，对专家依赖度极高；第二，探索空间有限，湿实验（*注）迭代慢；第三，投入成本高，失败风险大。</p><p></p><p>为了打破这个“魔咒”，近日水木分子发布了新一代对话式药物研发助手ChatDD&nbsp;(Drug&nbsp;Design)&nbsp;和全球首个千亿参数多模态生物医药对话<a href="https://www.infoq.cn/article/Joe403tMlSW3gHQVOHTm">大模型</a>"ChatDD-FM&nbsp;100B。其中，ChatDD&nbsp;能够在立项、临床前研究、临床试验各环节提供服务，预计将于10月中下旬邀请基础版本内测；ChatDD-FM&nbsp;100B在C-Eval评测中达到全部医学4项专业第一，是唯一在该4项评测中平均分超过90分的模型。</p><p></p><h3>从人工炼药到人机交互，医药研发经历四个阶段</h3><p></p><p></p><p>回顾研发模式的发展历程，<a href="https://www.infoq.cn/article/Bydle1aR2REAdNOLVOrC">水木分子</a>"首次定义了医药研发的四个阶段：</p><p></p><p>第一阶段称为TMDD（Traditional&nbsp;Manual&nbsp;Drug&nbsp;Design），从远古时代一直持续到19世纪末，主要以手工合成、提取和筛选为主，基于大量试验、错误和经验主义，缺点是低通量、缺乏系统性、耗时长且成本高；</p><p></p><p>第二阶段称为CADD（Computer-Aided&nbsp;Drug&nbsp;Design），从20世纪中叶到21世纪初，计算机的出现开始辅助加速药物发现和设计，底层有物理化学规则做支持，在一定程度上实现了高通量，但是计算机只具有工具属性，因此只能解决单个问题，在研究过程中，仍然在很大程度上依赖研究人员经验；</p><p></p><p>第三阶段称为AIDD（AI&nbsp;Drug&nbsp;Design），从21世纪初至今，人工智能技术的成熟发展和应用，开始改变药物研发，研究人员可以从训练数据中挖掘药物发现和设计，优点是能够满足超高通量，实现流程化，但是缺乏模型与专家交互，依赖大规模高质量标注数据，面临信息与知识分离，工具服务分散，处理模态单一等挑战；</p><p></p><p>第四阶段称为ChatDD（Chat&nbsp;Drug&nbsp;Design），即基于当下的大模型能力，对多模态数据进行融合理解，与专家自然交互人机协作，重新定义药物研发模式。</p><p></p><p>在整个过程中，每个阶段的技术革新都带来了不同程度的效率提升和科学发展，为药物研发带来了新的机遇和挑战。“但是，目前市场上至少还有超过&nbsp;1/&nbsp;3&nbsp;的药还是通过TMDD研发设计的。”清华智能产业研究院（AIR）首席研究员、水木分子首席科学家聂再清教授表示，这正是如今医药研发周期长、投入大、风险高很重要的原因。</p><p></p><p>虽然计算机、人工智能技术能够起到一定的研发辅助作用，但是仍然停留在工具阶段，研发过程很难摆脱对专家的知识依赖。“并且专家的知识储备也是有限的，无法掌握所有的文章、专利、数据。我们认为，把专家知识与大模型知识联结，二者进行协作，这才是药物研发现在和未来应该有的模式。”聂再清教授强调。</p><p></p><p>除此之外，利用此前的技术辅助医药研发还面临两个挑战：一方面，海量的实时<a href="https://www.infoq.cn/news/oZkw8G2uXvv2oVNJ7XFm">数据</a>"散落在各处；另一方面，生物医药涉及大量细分领域，每个领域需要配备对应工具，这些工具的使用门槛和成本也相对较高。“这也是我们发布ChatDD的原因，希望通过一个自然语言的交互界面，能够把专家从繁琐的数据处理和工具使用工作当中解放出来，把更多的精力放在更加开创性的、更有价值的工作上。”</p><p></p><h3>搞定立项、临床前研究、临床试验难题，ChatDD的三个场景应用</h3><p></p><p></p><p>水木分子在此次发布会上展示了&nbsp;ChatDD的在医药研发环节的三个具体应用场景，包括ChatDD-BI&nbsp;立项场景、ChatDD-Discovery研发探索场景和ChatDD-Trail&nbsp;临床试验场景应用。&nbsp;&nbsp;</p><p></p><p>ChatDD-BI：辅助立项调研</p><p></p><p>和仿制药生产制造不同，在药物研发过程中，立项环节极为重要，甚至一个错误的决定都可能导致巨大的经济损失。</p><p></p><p>在传统模式下，这一工作存在三个主要痛点：第一，信息繁杂，涉及对大量数据的搜集、整理和分析，比如病人需求、疾病机制、医药行业现状、专利和文献检索等等，并且这些数据散落各处，收集难度大、耗费时间长，一份高质量的立项报告的输出需要专家投入大量精力。第二，这一工作是非标准化的，对专家依赖极高，还存在信息不全面的问题，可能对项目的决策和发展产生负面影响。第三，立项涉及企业商业秘密难以外包，这也增加了信息搜集的难度。</p><p></p><p>针对这一问题，ChatDD-BI的角色是作为立项助手，通过与专家的数轮对话辅助立项调研，从而提升立项的效率和质量。</p><p></p><p>ChatDD-Discovery&nbsp;：启发研发灵感，提高工具使用效率和体验</p><p></p><p>针对临床前研究场景，传统的做法是做湿实验，然后根据结果不断迭代。但是随着时间的推移，找到针对已知靶点的药物越来越难。因此，科学家们需要更大的探索空间来寻找新的靶点和治疗方案。&nbsp;对此，水木分子认为，大模型的<a href="https://www.infoq.cn/article/TSJbOkWweAvFh44Oo2dL">幻觉</a>"在科学研究探索性中可能会带来“意外”的灵感。&nbsp;</p><p></p><p>以针对渐冻症治疗方案的探索场景为例，研发人员可以通过对话获得“渐冻症”疾病画像，包含疾病的描述、已有治疗方案等等，如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d06fc8de62c99ce371fc91dbf7a746e8.png" /></p><p></p><p>同时，ChatDD还会通过查询外部数据库，如ClinicalTrails，给出渐冻症在研药物进展。&nbsp;以“利司扑兰”为例，研发人员可以利用ChatDD进一步探究其作用靶点，ChatDD找到利司扑兰的作用靶点“SMN2”，通过建立疾病、靶点、药物之间的潜在链条关系，发现并建议“SMN2是渐冻症的潜在靶点，可进行下一步探究探索”。根据已发表的多项研究成果，证明了ChatDD推断的合理性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8a8b1ce54aa8d167fb0ffdc2f68e7a0.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9e7e0b87173bb4289b50245431e387b.png" /></p><p></p><p>除了“灵感激发”之外，ChatDD-Discovery同样可以提高研究人员的研发效率。在医药研发过程中，研究人员需要查询大量文献和检索专利等，这需要高频使用各种工具，学习成本高且难以与专家知识互动融合。而ChatDD-Discovery采用自然语言交互方式，将专家知识模型参数化，只需简单回答问题，就能使用虚拟筛选功能，使研究人员更高效地完成任务。</p><p></p><p>值得一提的是，ChatDD-Discovery&nbsp;研发助手可以处理生物医药多模态数据，如分子、蛋白质、基因序列以及多样的生物医药下游任务，如DDI（药物-药物相互作用计算）、亲和力计算、药物敏感性预测、单细胞类型注释等。&nbsp;例如，输入分子式ChatDD-Discovery就可以生成该分子的完整描述信息，并且支持用户针对该分子进行进一步提问。</p><p></p><p>ChatDD-Trial：提高临床试验成功率</p><p></p><p>在药物研发的全流程中，临床试验是耗资最多、风险最大的那一环，因此，想要打破“三十定律”，提高临床试验成功率是重中之重。换个角度来看，由于成本巨大，所以，在这个环节只要有一点点的提升，就能带来可观的经济效益。</p><p></p><p>对此，水木分子ChatDD目前主要关注四个小场景：药物生物标记物发现、临床试验入组设计、临床试验报告撰写、药物重定位和适应症扩展。</p><p></p><p>比如，ChatDD-Trial可以辅助临床试验研究人员找到最适合入组的患者人群；比如，通过发现药物敏感的生物标志物，更好地理解疾病亚型，实现精准的患者分类，确保患者与试验药物更匹配，减少不必要的变量干扰，提高临床试验成功率；再比如，作为临床试验设计助手，帮助研究团队优化试验设计，通过分析大量的相关数据，提供有关患者选择、临床试验阶段和标准化流程的建议，确保试验的科学性和可行性，减少不必要的误差和风险。</p><p></p><p>据聂再清教授介绍，目前ChatDD提供了三类服务方式：一是订阅服务，支持企业账户；二是API服务，支持高速推理服务，提供开发者培训；三是私有化部署，支持本地数据微调，定制场景方案。</p><p></p><h3>三级淬炼出千亿级生物医药行业大模型</h3><p></p><p></p><p>ChatDD的底层，基于的是水木分子千亿参数多模态<a href="https://xie.infoq.cn/article/7edc5aaa592a490f71b758299">生物医药</a>"对话大模型ChatDD-FM。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/dbde600079d32101cfb662330be99d95.png" /></p><p></p><p>上图是ChatDD-FM多模态生物医药大模型的架构图：首先，对生物医药垂直领域的知识图谱、分子、蛋白质、单细胞测序、文本等多模态进行编码；然后，将各种不同模态对齐到统一的特征空间，进行模态融合、专业领域指令微调；最后，在产品端支撑ChatDD-BI、ChatDD-Discovery和ChatDD-Trail&nbsp;三大应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecc9f2becb64d9eebae23b16ef3190cf.png" /></p><p></p><p>水木分子首席技术官、联合创始人乔木博士强调，ChatDD-FM是一个灵活、可扩展的架构，在当下版本基础上，还可以持续增加其它不同模态。而对于用户而言，可以通过自然语言对话方式与大模型进行交互。其背后，是ChatDD-FM对数千篇中英文生物医药文献、千万级通用场景对话、数十万个工具调用指令集等高质量数据的预训练，以及指令微调和智能工具调用。</p><p></p><p>水木分子由AIR孵化，在今年6月份成立，主要专注于生物医药垂直行业大模型的研发与应用。而早在今年4月，聂再清教授团队就已经开源了生物医药版GPT&nbsp;BioMedGPT1.6B，从研究层面验证了将文献、分子、蛋白、测序、知识图谱等数据压缩到统一的多模态大模型框架内，可使模型具备“融会贯通”的能力，在分子性质预测、药物-靶点亲和力预测、性质预测、药物敏感性预测、分子-文本跨模态检索、分子-文本跨模态信息生成等多项任务上优于单一专用模型，从研究层面验证了技术可行性。</p><p></p><p>8月，水木分子又开源了全球首个可商用的百亿级参数规模多模态生物医药大模型BioMedGPT-10B。其中，首次提出了分子QA和蛋白质QA任务，给定特定分子/蛋白质，可以用自然语言回答相关功能、属性等问题，有效评估模型在自然语言和生物编码语言之间的翻译能力。</p><p></p><p>据聂再清教授介绍，虽然BioMedGPT已经实现商用，但由于是科研项目，所以使用的仍然是英文界面，中文对话能力较弱，因此在国内市场实现商用普及有限。这便是水木分子成立背后的原因之一。</p><p></p><p>“接下来，BioMedGPT&nbsp;仍然会继续往前推进，作为科研品牌继续开源。而此次ChatDD和ChatDD-FM的孵化，则是大模型都科研迈向产业化，是产学研结合的关键一步。”聂再清教授强调，“我们认为大模型未来必将成为&nbsp;AI&nbsp;时代的操作系统，就像我们PC时代的Windows，移动互联时代的Android、iOS。而现在我们正在做的生物医药大模型，未来在整个AI&nbsp;for&nbsp;science领域都有可能用上。对此，我们充满了期待。”</p><p></p><p>而现下，ChatDD和ChatDD-FM的发布，已然重新定义了医药研发模式，不仅可以提高研究人员的工作效率，还可以为研发新药提供更多的可能性。未来，随着技术的持续迭代革新，医药研发的效率和成功率将会得到显著提高，打破“三十定律”的“魔咒”指日可待。</p><p></p><p></p><blockquote>*注：“湿”实验指的是将待测样本利用实验室方法进行核酸提取、文库构建（包括片段化、富集、扩增等一系列过程）到完成上机测序的实验过程；“干”实验则是从得到下机数据开始，到完成生信分析和报告解读的整个过程。</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EjqbmPdv0fSvUkWqgZ2k</id>
            <title>PICO 首届 XR 开发者挑战赛正式启动，助推行业迈入“VR+MR”新阶段</title>
            <link>https://www.infoq.cn/article/EjqbmPdv0fSvUkWqgZ2k</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EjqbmPdv0fSvUkWqgZ2k</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Sep 2023 00:44:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: PICO 2023首届XR开发者挑战赛, XR行业, 开发者, 技术合作伙伴
<br>
<br>
总结: PICO举办首届XR开发者挑战赛，旨在与全球开发者共同探索XR行业新成长。PICO将为参赛者提供全方位支持，英特尔作为技术合作伙伴也将提供保驾护航。XR行业已经进入新的发展阶段，开发者有机会融入其中，创造新机会和新市场。PICO重视开发者伙伴，通过举办挑战赛与开发者链接，持续完善核心技术能力，并为开发者提供便捷的开发服务。PICO在XR行业取得了快速增长，得益于庞大的用户群体和开发者共建的内容生态。挑战赛将在多个国家和地区举办，为全球开发者提供展示才华和实现创意的舞台。获奖者将获得丰厚奖金和额外支持，英特尔也将提供更多的算力支撑。通过参加挑战赛，开发者们可以展示自己的才华和创新能力，并获得更多机会和支持。 </div>
                        <hr>
                    
                    <p>9 月 25 日，“PICO 2023 首届 XR 开发者挑战赛”（下文简称“挑战赛”）媒体启动会在北京圆满落幕，官方赛事报名通道已于今日开启。据悉，本次挑战赛是 PICO 首次针对全球开发者举办的大型挑战赛事，旨在与开发者保持连接，共同探索 XR 行业新成长。作为主办方，PICO 将为参赛者提供平台、专业技术、资金等全方位支持，而全球领先的芯片厂商英特尔也将作为本届赛事技术合作伙伴为参赛者保驾护航。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bb/40/bbb0993bf6845538a460d7737734a740.png" /></p><p>图1：PICO 2023首届XR开发者挑战赛报名通道正式开启</p><p>&nbsp;</p><p>启动会现场，除了官宣赛事启动，PICO 相关负责人还围绕行业、技术、资金/人才投入等维度详细阐述了 PICO 一直以来对于开发者的重视及此次举办开发者挑战赛的初衷，呼吁更多开发者投入XR开发新领域，与 PICO 及行业共成长。</p><p></p><p>“新阶段的 XR 行业，值得开发者关注与参与。”PICO OS 产品负责人马杰思谈到，“XR 行业已经迈入新的发展阶段，开始从强调虚拟的‘VR’向虚实融合的‘VR+MR’阶段过渡，这种变化不仅能给用户提供更多样化的产品价值，一定程度也赋予了开发者更大的创作空间。各行各业的开发者，都有机会融入到XR开发的领域之中，一起创造新机会和新市场。同时，光学显示、芯片算力、环境感知、人机交互等技术的突破，也加速了 XR 行业发展。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/ae/29/aef7883c92bd3a90944a3aa27c5ff629.jpg" /></p><p>图2：PICO OS产品负责人马杰思现场分享</p><p></p><p>马杰思同时表示，想要实现整个 XR 赛道高水平发展的局面，决不是凭借 PICO 或者某一家品牌一家之力就可以做成的事情。一直以来，PICO 都相当重视广大的开发者伙伴，先后启动了“PICO 开发者计划”“PICO 创作者激励计划”等，从硬件、技术、营销等维度，全链路助力开发者成长。此次主办首届开发者挑战赛事，是在 XR 行业新阶段下 PICO 与开发者链接的一次新尝试。新趋势下，PICO 将持续围绕空间感知、图形渲染、人机交互等 XR 核心底层技术能力持续完善，并协同字节底层技术能力，为开发者提供前沿、便捷的开发服务。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/a7/b8/a7a445324980c41320ab0e00dbc13fb8.jpg" /></p><p>图3：PICO中国区游戏商务负责人郭文山现场分享</p><p>&nbsp;</p><p>作为全球知名的 XR 品牌，PICO 在近两年保持快速增长，2022 年全年销量同比之前翻了 8 倍。此外，IDC 数据显示，2022 年市场份额 PICO 全球排名第二、国内位居第一，2023 年上半年国内 VR 市场的厂商份额中，PICO 以 58.7% 的占比大幅领先同行。这些成果背后，离不开庞大的用户群体以及众多开发者共建内容生态的支持。</p><p></p><p>PICO 中国区游戏商务负责人郭文山表示：“截止目前，PICO 应用商店的应用数量已超 530 款，‘大作’占比近四成。其中，国内开发者贡献不容忽视，PICO 应用商店中包括游戏、视频、运动、办公、教育、音乐等在内的 100 多款应用均来自国内开发者。此外，PICO 也给开发者带来更多用户和收入，&nbsp;有 180 款游戏收入超过 10 万元，近 50 款应用下载量超过 10 万，而部分平台头部开发者，在 PICO 的年收入已超过 800 万元。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8e/8f/8e2daabe3454191f157eb6ffd0f6c68f.png" /></p><p>图4：PICO将为开发者们提供专业指导、投资机会、流量曝光等全方位支持</p><p>&nbsp;</p><p>据了解，本次“挑战赛”将在欧洲、北美、东南亚、日韩、中国等多个国家和地区同期举办，为全球开发者提供展示才华、实现创意的舞台。所有未发行/已发行且没在往届同类型比赛中 PICO 赛道获奖的作品均可报名。赛事报名通道于 9 月 25 日正式开通，开发者可通过 PICO 官网开放平台赛事专区报名参赛，并于 10 月 2 日至 11 月 10 日期间完成作品提报。<a href="https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t">PICO</a>" 将于 11 月 30 日前组织评委完成评审，并于 12 月 5 日举办颁奖仪式，公布获奖名单。</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/Vsc9cCloJx9mCOTyURGT">针对</a>"本届“挑战赛”重点赛区之一的中国区开发者，本次挑战赛设置了前五名，另外还单独设置了游戏类、创新类、MR 赛道、手势识别四个不同赛道奖项。获奖者除收获最高达 6 万元的丰厚奖金外，还有机会得到技术大咖指导、投资圆桌交流、<a href="https://xie.infoq.cn/article/40e1258dc998d9c7ce6380c8a">PICO</a>" 创投基金和面试直通等额外支持。 赛事技术合作伙伴英特尔也将为获奖参赛作品上线运行提供更多维的算力支撑，搭载英特尔 ARC dGPU 显卡的运行环境后续将开放给大赛开发者实现项目落地。</p><p>&nbsp;</p><p>通过本次挑战赛，开发者们不仅可以充分展示自己的才华和创新能力，同时也能够获得更多的机会和支持。有兴趣的开发者们，不妨立即行动起来，前往 PICO 官网开放平台赛事专区（<a href="https://www.picoxr.com/cn/2023picodevjam/#/introduce?trackingFlag=EkWdMubc">链接</a>"），报名参加这场挑战赛。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uNJmOYLgQK062KPYFWDw</id>
            <title>电商巨头 Shopify 提升 LLM 辅助工具，优化电子商务生态</title>
            <link>https://www.infoq.cn/article/uNJmOYLgQK062KPYFWDw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uNJmOYLgQK062KPYFWDw</guid>
            <pubDate></pubDate>
            <updated>Tue, 26 Sep 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型语言模型, Markdown, 渲染卡顿, 延迟
<br>
<br>
总结: Shopify的工程师通过使用缓冲解析器和占位符的概念，解决了大型语言模型返回的Markdown响应流导致的渲染卡顿和延迟的问题。他们利用异步性将响应分流步骤与Markdown缓冲解析器集成在一起，以提高用户体验。 </div>
                        <hr>
                    
                    <p>在使用大型语言模型聊天机器人打开了创新解决方案之门的同时，Shopify 的工程师 Ates Goral 认为，要使用户体验尽可能自然，需要一些特定的努力，以<a href="https://shopify.engineering/sidekicks-improved-streaming">防止产生卡顿并减少延迟</a>"。</p><p>&nbsp;</p><p>由大型语言模型返回的 Markdown 响应流会导致渲染卡顿，因为特殊的 Markdown 字符（如 *）在完整表达式被接收之前保持模糊不清，例如，直到接收到闭合的 *。同样的问题也适用于链接和所有其他 Markdown 运算符。这意味着 Markdown 表达式在完全完成之前无法正确渲染，这意味着在短时间内 Markdown 渲染不正确。</p><p>&nbsp;</p><p>为了解决这个问题，Shopify 使用了一个缓冲解析器，它在 Markdown 特殊字符之后不会发出任何字符，并等待 Markdown 表达式完全完成或收到意外字符。</p><p>&nbsp;</p><p></p><blockquote>在进行流式传输时，需要使用一个有状态的流处理器，可以逐个字符地消耗字符。流处理器要么按照字符接收的顺序传递字符，要么在遇到类似 Markdown 的字符序列时更新缓冲区。</blockquote><p></p><p>&nbsp;</p><p>然而，Goral 表示，虽然从原理上讲，这个解决方案相对容易手动实现，但要支持完整的 Markdown 规范，则需要使用现成的解析器。另一方面，延迟主要是由于需要进行多次大型语言模型往返来消耗外部数据源，以扩展大型语言模型的初始响应所导致的。</p><p>&nbsp;</p><p></p><blockquote>大型语言模型对一般的人类语言和文化有很好的理解，但它们并不是获取最新准确信息的绝佳来源。因此，我们告诉大型语言模型通过使用工具来告诉我们当它们需要超出其理解范围的信息时。</blockquote><p></p><p>&nbsp;</p><p>换句话说，基于用户输入，大型语言模型提供的初始响应还包括要咨询以获取缺失信息的其他服务。当这些额外的数据片段被接收后，大型语言模型将构建完整的响应，最终显示给用户。</p><p>&nbsp;</p><p>为了防止用户需要等待所有外部服务都响应完成，Sidekick 使用了 "卡片" 的概念，即占位符。Sidekick 渲染了从大型语言模型收到的初始响应，包括任何占位符。一旦额外的请求完成，Sidekick 将占位符替换为接收到的信息。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43afd7d3246a2f60947117a72536422b.gif" /></p><p></p><p>&nbsp;</p><p>在 Sidekick 中实施的解决方案充分利用了此工作流程中固有的异步性，并将响应分流步骤与 Markdown 缓冲解析器集成在一起。如果您对他们的解决方案的完整细节感兴趣，不要错过 Goral 的原文文章。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/08/Shopify-sidekick-llm-improvement/">https://www.infoq.com/news/2023/08/Shopify-sidekick-llm-improvement/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9094a1097720d61697b62bd0e</id>
            <title>人工智能核心基础 - 规划和概要</title>
            <link>https://www.infoq.cn/article/9094a1097720d61697b62bd0e</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9094a1097720d61697b62bd0e</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Sep 2023 15:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Python篇, 数学篇, AI基础学习, 课程规划
<br>
<br>
总结: 本文介绍了作者之前撰写的Python篇和数学篇，为读者打好AI基础。接下来，作者将正式开始AI基础学习，并介绍了本次课程的规划，包括从机器学习到深度学习，以及三大方向：NLP、CV和BI。课程内容将分成四大部分进行精讲。此外，文章还提到了内容输出方式和代码库的相关信息。 </div>
                        <hr>
                    
                    <p></p><p></p><p><img src="https://static001.infoq.cn/static/write/img/img-copy-disabled.4f2g7h.png" /></p><p></p><p>Hi，你好。又见面咯，我是茶桁。</p><p></p><p>在之前，我花了两个来月的时间撰写了「Python篇」和「数学篇」，希望小伙伴们在正式进入AI之前能够打好一个基础。那么今天开始，我们将正式开始AI基础的学习。</p><p></p><p>这一节课咱们先不着急直接开始课程，而是聊一下本次课程的一个规划。</p><p></p><p>在整个课程规划中，我们将会直接从机器学习开始入手，进入深度学习，然后开始接触RNN、CNN以及三大方向：NLP、CV和BI。核心能力将会分成四大部分进行展开精讲。</p><p></p><h2>目录规划</h2><p></p><p></p><h3>基础能力</h3><p></p><p>人工智能导论机器学习初探机器学习进阶（这部分会比较长）深度学习进阶RNNCNN自然语言处理基础（NLP）计算机视觉基础（CV）商业智能（BI）</p><p></p><h3>BI精讲</h3><p></p><p>预测全家桶与机器学习四大神器Fintech数据分析数据可视化与DashBoardALS算法与推荐系统SVD矩阵分解与基于内容的推荐PageRank、图论与推荐系统Graph Embedding强化学习</p><p></p><h3>NLP精讲</h3><p></p><p>自然语言处理的基本过程</p><p></p><p>向量空间模型自然语言处理初步语言模型和概率图模型词向量模型Word2VecTransformer与BER,大规模预训练问题自然语言生成自然语言处理与人工智能前沿</p><p></p><h3>CV精讲</h3><p></p><p>初阶计算机视觉：图像处理</p><p></p><p>中阶计算机视觉：图像描述中阶到高阶的关键：CNN方法计算机视觉中的图像分类深度学习之单阶段目标检测深度学习之两阶段目标检测计算机视觉中的图像分割计算机视觉中的目标跟踪</p><p></p><h2>内容输出方式</h2><p></p><p>以上目录中的四个部分都属于核心部分，每一个部分都会单独开一个专栏目录。一个是因为收费课程，拆散之后大家可以按照自己的需要进行购买，再一个也是将四部分区分的清晰一点。</p><p></p><p>虽然每一张专辑都是收费的，但是也并不是所有内容都需要进行购买才可查看。有的时候为了吸引流量，即便没有购买专辑，部分章节会开放阅读全部。</p><p></p><p>以上目录仅供参考，目录是按照内容概要进行规划的，并不等于实际章节。就像我在写数学篇的时候，本来就只规划了4个知识点，但是其中一个知识点可能会讲7、8个章节，也可能3、4篇就讲完了。所以届时的内容，会比从目录上看要多的多，起码就基础部分的机器学习这一知识点，可能就要十几、二十节课才能讲完。</p><p></p><h2>代码库</h2><p></p><p>在咱们的整个讲解过程，演示代码是不可避免的，并且其中还会包含很多数据。这部分内容基本上都会在咱们的《茶桁的AI秘籍》的代码仓库中找到，地址为：https://github.com/hivandu/AI_Cheats</p><p></p><p>其中部分数据集可能因为太大会上传到百度网盘并分享出来，分享一般都会放在文末，大家可以自取。</p><p></p><h2>其他</h2><p></p><p>如果您阅读时感觉文章不完整，那应该是该网站我暂时无法发布收费专栏，所以我仅提供了部分内容。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MTaYW6a4CNrahgkYXAyT</id>
            <title>阿里云通义千问14B模型开源：性能超越Llama2等同尺寸模型</title>
            <link>https://www.infoq.cn/article/MTaYW6a4CNrahgkYXAyT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MTaYW6a4CNrahgkYXAyT</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Sep 2023 07:40:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里云, 通义千问, Qwen-14B, 大模型
<br>
<br>
总结: 阿里云开源了通义千问140亿参数模型Qwen-14B及其对话模型Qwen-14B-Chat，支持免费商用。Qwen-14B在多项权威评测中表现出色，超越同等规模模型，部分指标接近Llama2-70B。Qwen-14B具备更强大的推理、认知、规划和记忆能力，最大支持8k的上下文窗口长度。Qwen-14B-Chat在基座模型上经过精细SFT得到，生成内容准确度提升，更符合人类偏好，内容创作上的想象力和丰富度也有显著扩展。 </div>
                        <hr>
                    
                    <p></p><blockquote>两个月三连发，通义千问又开源了。</blockquote><p></p><p></p><p>9月25日，阿里云开源通义千问140亿参数模型Qwen-14B及其对话模型Qwen-14B-Chat，支持免费商用。在多项权威评测中，Qwen-14B的性能表现超越同等规模模型，部分指标甚至接近Llama2-70B。此前阿里云于8月开源了通义千问70亿参数基座模型Qwen-7B，先后冲上HuggingFace、GitHub的Trending榜单。短短一个多月，累计下载量就突破100万，开源社区出现了约50个基于Qwen的模型。</p><p>多个知名工具和框架都集成了Qwen，如支持用大模型搭建WebUI、API以及微调的工具FastChat、量化模型框架AutoGPTQ、大模型部署和推理框架LMDeploy、大模型微调框架XTuner等等。</p><p></p><p>据了解，此次开源的Qwen-14B是一款支持多种语言的高性能开源模型，相比同类模型使用了更多的高质量数据，整体训练数据超过3万亿Token，使模型具备更强大的推理、认知、规划和记忆能力。Qwen-14B最大支持8k的上下文窗口长度。</p><p><img src="https://static001.geekbang.org/infoq/16/166ee1f62a1c35fb0c11ac5079f70bbc.jpeg" /></p><p></p><p>图1：Qwen-14B在十二个权威测评中全方位超越同规模SOTA大模型</p><p></p><p>而Qwen-14B-Chat则是在基座模型上经过精细SFT得到的对话模型。借助基座模型强大的性能，Qwen-14B-Chat生成内容的准确度大幅提升，也更符合人类偏好，内容创作上的想象力和丰富度也有显著扩展。</p><p></p><p>Qwen拥有出色的工具调用能力，能让开发者更快地构建基于Qwen的Agent（智能体）。开发者可用简单指令教会Qwen使用复杂工具，比如使用Code&nbsp;Interpreter工具执行Python代码以进行复杂的数学计算数据分析、图表绘制等；还能开发具有多文档问答、长文写作等能力的“高级数字助理”。</p><p></p><p>百亿以内参数级别大语言模型是目前开发者进行应用开发和迭代的主流选择，&nbsp;Qwen-14B进一步提高了小尺寸模型的性能上限，从众多同尺寸模型中冲出重围，在MMLU、C-Eval、GSM8K、MATH、GaoKao-Bench等12个权威测评中取得最优成绩，超越所有测评中的SOTA（State-Of-The-Art）大模型，也全面超越Llama 2-13B，比起Llama&nbsp;2的34B、70B模型也并不逊色。与此同时，Qwen-7B也全新升级，核心指标最高提升22.5%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4fbdbdc1d80f97d6c961cd076ca7a4ce.png" /></p><p>图2：Qwen-14B性能超越同尺寸模型</p><p></p><p>据InfoQ向通义千问研发团队了解，Qwen-14B之所以能较上一代Qwen-7B获得显著的性能提升，主要有三方面原因。</p><p></p><p>在数据集构建方面：</p><p></p><p>研发团队使用了多达3万亿token的大规模预训练数据集，覆盖了各个领域和千行百业的知识，包含多个语种的语言，还有代码数据。在此基础之上，研发团队做了较为精细的数据处理，包括大规模数据去重、垃圾文本过滤、以及提升高质量数据比例等。</p><p></p><p>在模型结构设计和训练方法方面：</p><p></p><p>模型结构方面，研发团队此前做了一系列前期实验验证模型结构设计对效果的影响，整体而言，Google的PaLM、Meta的LLaMA的大多数技术选择都是效果较好的，包括SwiGLU的激活函数设计、ROPE的位置编码等，在Qwen的结构设计中也都采用了。</p><p></p><p>通义千问团队专门针对词表做了优化，词表大小超过15万，具有较好的编码效率，也就是说，相比其他tokenizer能用更少的token表示更大量的信息，意味着更加节省的token数，或者说更低的成本。</p><p></p><p>此外，通义千问团队重点针对长序列数据建模做了优化，采用当前最有效的策路，包括但不限于Dynamnic NTK、Log-N attention scaling、window attention等，并做了一些细节的调整保证长序列数据上，模型表现效果更稳定。当前模型能够适配井取得稳定表现的序列长度也达到了8192。</p><p></p><p>研发团队成员向InfoQ表示：“大模型训练其实没有太多复杂的技巧，更多是经过大量尝试与迭代，找到较好的训练参数，达到训练稳定性、训练效果和训练效率的最优平衡，包括但不限于优化器的配置、模型并行的配置等。”</p><p></p><p>在外接工具能力方面：</p><p></p><p>此前开源的Qwen-7B已经展现出了出色的工具使用能力，只需通过文本描述即可理解并使用未经训练的新工具，可用于各种Agent应用。这次开源的新模型进一步增强了Agent能力，在使用复杂工具时的可靠性有了显著提升。</p><p></p><p>例如，Qwen-14B可以熟练地使用Code Interpreter工具执行Python代码，进行复杂的数学计算、数据分析和数据图表绘制。同时，Qwen-14B的规划和记忆能力也得到了提升，在执行多文档问答和长文写作等任务时表现更加可靠。</p><p></p><p>为了实现这一效果，研发团队主要做了两方面的优化。首先，在微调样本方面进行优化，通过建立更全面的自动评估基准，主动发现了之前Qwen表现不稳定的情況，并针对性地使用Self-Instruct方法扩充了高质量的微调样本。其次，底座预训练模型的能力得到了提升，带来了更强的理解和代码能力。</p><p></p><p>即日起，用户可从魔搭社区直接下载模型，也可通过阿里云灵积平台访问和调用Qwen-14B和Qwen-14B-Chat。阿里云为用户提供包括模型训练、推理、部署、精调等在内的全方位服务。</p><p></p><p>据了解，当前国内已有多个月活过亿的应用接入通义千问，大量中小企业、科研机构和个人开发者都在基于通义千问开发专属大模型或应用产品，如阿里系的淘宝、钉钉、未来精灵，以及外部的科研机构、创业企业。</p><p></p><p>钉钉是最早接入通义千问的应用之一。4月18日，钉钉发布了一条“斜杠”， 接入千问大模型后，通过输入“/”即可在钉钉唤起AI能力，涵盖群聊、文档、视频会议及应用开发等场景。</p><p>&nbsp;</p><p>在群聊中，新入群者无需爬楼，在对话框输入钉钉斜杠“/”即可自动整理群聊要点，快速了解上下文，并生成待办、预约日程。还可以用“/”在群聊中创作文案、表情包等；在钉钉文档中，“/”可以是用户的创意助理，帮助写文案、生成海报。在视频会议中，“/”也是会议助理，能一键生成讨论要点、会议结论、待办事项等；“/”还可用自然语言或拍照生成应用，并以钉钉酷应用的形式在群聊内使用。比如，公司行政人员需要统计午餐的订餐份数，只需要在群聊对话框中输入“/”和需求，几秒钟后一个订餐统计小程序就会展现在群聊中。</p><p>&nbsp;</p><p>在过去100多天里，钉钉正逐渐实现全面智能化，已经有17条产品线、55个场景完成了智能化再造，包含钉钉 IM 群聊、酷应用、低代码、钉钉会议、Teambition、闪记、邮箱、钉钉文档、表格、脑图、白板、知识库等，覆盖多模态内容生成、摘要提取、应用开发等。</p><p></p><p>浙江大学联合高等教育出版社基于Qwen-7B开发了智海-三乐教育垂直大模型，已在全国12所高校应用，可提供智能问答、试题生成、学习导航、教学评估等能力，模型已在阿里云灵积平台对外提供服务，一行代码即可调用；浙江有鹿机器人科技有限公司在路面清洁机器人中集成了Qwen-7B，使机器人能以自然语言与用户进行实时交互，理解用户提出的需求，将用户的高层指令进行分析和拆解，做高层的逻辑分析和任务规划，完成清洁任务。</p><p></p><p>阿里云CTO周靖人表示，阿里云将持续拥抱开源开放，推动中国大模型生态建设。阿里云笃信开源开放的力量，率先开源自研大模型，希望让大模型技术更快触达中小企业和个人开发者。</p><p></p><p>附相关链接：</p><p></p><p>魔搭社区模型地址：</p><p>https://www.modelscope.cn/models/qwen/Qwen-14B-Chat/summaryhttps://www.modelscope.cn/models/qwen/Qwen-14B/summary</p><p></p><p>Qwen论文地址：</p><p>https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf</p><p></p><p>GitHub：</p><p>https://github.com/QwenLM/Qwen</p><p></p><p>HuggingFace:</p><p>https://huggingface.co/Qwen/Qwen-14Bhttps://huggingface.co/Qwen/Qwen-14B-Chat</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>