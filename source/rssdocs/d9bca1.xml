<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</id>
            <title>智谱AI获中东财团 4 亿美元投资；老板跑路，900多员工一脸懵：上午改bug、下午解散；谷歌在云部门大规模裁员 | AI周报</title>
            <link>https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NPYL7aG4kYLIWqNc2dhe</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:34:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, 谷歌, 马斯克, OpenAI
<br>
<br>
总结: 字节跳动否认TikTok曾提议将控制权交给美国政府，谷歌在云部门裁员，马斯克和杨立昆争论，奥特曼或将OpenAI重组为盈利性公司，苹果Siri升级计划延迟至2025年。 </div>
                        <hr>
                    
                    <p></p><blockquote>字节跳动：外媒报道“TikTok曾提议将控制权交给美国政府”消息不实；谷歌在云部门大规模裁员；马斯克和杨立昆的激情互喷，引来3000万网友围观；奥特曼或将OpenAI重组为盈利性公司；iOS&nbsp;18首个正式版无缘：曝苹果AI&nbsp;Siri跳票至2025年；内卷加剧！拼多多上线自动跟价功能……</blockquote><p></p><p></p><h3>热门资讯</h3><p></p><p></p><h4>字节跳动：外媒报道“TikTok曾提议将控制权交给美国政府”消息不实</h4><p></p><p>&nbsp;</p><p>近日有媒体报道TikTok曾提议将控制权交给美国，字节否认。此外字节AI硬件方向的探索，在内部分为两条产品线“D线”和“O线”，负责人分别为李浩乾和字节曾收购公司Oladance创始人，向字节跳动技术副总裁洪定坤汇报。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec3ce51a40aef0229447f544dcea9f63.webp" /></p><p></p><p>字节跳动豆包业务相关负责人表示，这些组织及汇报关系的猜测不实。该项目作为字节豆包业务的一部分，目的是探索豆包能力与可穿戴设备相结合，为用户提供更自然和便捷的交互体验。同时，我们也会把豆包能力开放给更多硬件厂商，希望能够为硬件厂商和豆包大模型/豆包AI助手的合作提供设计参考。</p><p>&nbsp;</p><p>另外，还有消息称，TikTok正在为其1.7亿美国用户开发其推荐算法的克隆版本，可能会产生一个独立于其中国母公司字节跳动运行的版本，更容易被想要封禁它的美国议员接受。在报道发表后，TikTok在𝕏上发文称，“路透社今天发表的报道具有误导性，与事实不符”。</p><p></p><h4>谷歌在云部门大规模裁员</h4><p></p><p>&nbsp;</p><p>据外媒报道，一份内部文件显示，谷歌公司在旗下云部门进行大规模裁员，涉及咨询顾问、合伙人工程师、可持续性等团队的员工。有员工估算，谷歌云亚太区的“走向市场”（Go&nbsp;To&nbsp;Market）部门约有100人被裁。</p><p>&nbsp;</p><p>前述文件显示，专注于咨询、合作伙伴工程和可持续性的云团队也受到裁员的影响。一些最近新招的员工也解雇，包括至少一名还未完成入职流程的员工。部分受影响的员工已经得到谷歌内部其他岗位的工作机会。</p><p></p><h4>马斯克和杨立昆的激情互喷，引来3000万网友围观</h4><p></p><p>&nbsp;</p><p>5月27日，马斯克的&nbsp;xAI&nbsp;刚宣布了今年最大的一笔风险投资，以超过&nbsp;240&nbsp;亿美元估值获得&nbsp;60&nbsp;亿美元&nbsp;B&nbsp;轮融资。自此，成功晋级为“硅谷大模型第一梯队玩家”，与&nbsp;OpenAI、谷歌、Anthropic&nbsp;和&nbsp;xAI&nbsp;并列为四巨头。</p><p>而马斯克在社交媒体X上发布招聘信息后，遭到Meta首席人工智能科学家杨立昆嘲讽，大战一触即发，引来3000万网友围观。两人真正的分歧在于：杨立昆认为AI毁灭人类的概率为零，而马斯克宣扬的是AI威胁论。</p><p>杨立昆嘲讽道：“如果你能忍受这样一位老板，那就加入xAI。这位老板声称，你正在做的事情将在明年由AI替代解决（没有压力）；声称你正在做的事情会毁灭人类，必须停止或暂停（耶，放六个月的假！）；声称想要对真理进行最严格的追求，但却在自己的社交平台上散布疯狂的阴谋论。”</p><p>&nbsp;</p><p>另外在近日巴黎举行的初创企业年度技术大会&nbsp;VivaTech&nbsp;上，杨立昆建议希望在&nbsp;AI&nbsp;生态系统中工作的学生不要从事&nbsp;LLM（大型语言模型或称“LLM”）方面的工作。“如果你是对构建下一代&nbsp;AI&nbsp;系统感兴趣的学生，请不要从事&nbsp;LLM&nbsp;方面的工作。这是大公司的事情，你们无法对此有所贡献。”他还说，人们应该开发能够克服大型语言模型局限性的下一代&nbsp;AI&nbsp;系统。</p><p>&nbsp;</p><p></p><h4>奥特曼或将OpenAI重组为盈利性公司</h4><p></p><p>&nbsp;</p><p>据知情人士称，OpenAI&nbsp;CEO山姆·奥特曼（Sam&nbsp;Altman）正考虑将OpenAI重组为一家正规的盈利性公司。据一位与奥特曼交谈过的知情人士称，一种可能性是，奥特曼可能会将OpenAI转变为一家营利性公司，他也讨论过这个想法。而OpenAI的一些投资者表示，他们希望奥特曼获得股权方案，以确保他的利益与公司业务保持一致。</p><p>&nbsp;</p><p>本周，OpenAI&nbsp;的两位前董事会成员Helen&nbsp;Toner&nbsp;和&nbsp;Tasha&nbsp;McCauley&nbsp;公开指责&nbsp;Sam&nbsp;Altman，认为其在&nbsp;OpenAI&nbsp;内部培养了一种虚假文化。</p><p>&nbsp;</p><p>两位前董事指出&nbsp;Altman&nbsp;削弱了董事会对关键决策和安全规定的监督，许多高管告诉董事会，他们非常担心&nbsp;Altman&nbsp;正在创造&nbsp;一种虚假文化，并参与&nbsp;可以被描述为心理虐待的行为。Toner&nbsp;和&nbsp;McCauley&nbsp;认为，OpenAI&nbsp;无法自我监管，这一观点源自非营利性和营利性结构的混合。</p><p>&nbsp;</p><p></p><h4>iOS&nbsp;18首个正式版无缘：曝苹果AI&nbsp;Siri跳票至2025年</h4><p></p><p>&nbsp;</p><p>据报道，苹果正对Siri进行重大升级，使其获得大模型能力，更准确理解用户意图，提供更自然、智能的交互体验。不过，这一高级Siri功能将不会在iOS&nbsp;18首个正式版中出现，预计要等到2025年的iOS&nbsp;18才会集成。</p><p>&nbsp;</p><p>目前，Siri的智能化程度有限，苹果计划借助类似ChatGPT的自然语言模型实现全新智能助理，让用户能与Siri进行多轮对话。苹果工程师团队正在重构Siri的底层，以提升其响应生成能力，深度理解用户需求，并智能整合相关信息，让对话和工作更加自然、高效。</p><p>&nbsp;</p><p>据外媒报道，苹果早在2023年年中就已接触OpenAI公司，而微软对两家公司之间的这笔交易感到担忧。据悉，苹果iOS等系统接入OpenAI的ChatGPT聊天服务之外，还邀请OpenAI团队成员帮助优化Siri，不断提高其“智力”。</p><p>&nbsp;</p><p></p><h4>内卷加剧！拼多多上线自动跟价功能</h4><p></p><p>&nbsp;</p><p>5月29日，拼多多正式上线了自动化价格追踪系统。此举意味着若竞争对手下调商品价格，而用户的商品价格尚显高价，平台将智能调整用户定价，使之与竞争对手保持同步甚至更具竞争优势。</p><p>值得注意的是，抖音电商此前刚刚宣布小规模内测“自动改价”功能，该功能支持商家委托平台根据市场情况，参考平台的建议价，在商家设定的条件范围内实现改价，为消费者提供更有价格竞争力的商品，提升自身经营效率。据抖音电商内部同学透露，今年货架电商的营业额已经很接近拼多多。</p><p>&nbsp;</p><p></p><h4>小红书大举替换中高层高管，后台出现诸多bug</h4><p></p><p>&nbsp;</p><p>据报道，2023年，小红书在一场共创会议中明确了“坐一观三”的发展策略，向3亿DAU正式进发。为了拉平差距，小红书今年一季度在行业大举挖人，有大量中层以上管理者进入小红书，承担起小红书商业化产品、社区运营、电商产品、法务等重要岗位。</p><p>与此同时，多元的组织文化冲击了小红书的原有团队。内部互相推诿的情况变多，“各部门甩锅问题严重”，事实上，不止一位采访对象提及过，小红书的社区部门与商业化部门存在矛盾，而社区一直处于强势地位。新进入者的试错，也让小红书的后台出现诸多bug，引发了商家们的投诉潮。人员的频繁流动，让小红书的功能产品连续性较差，导致用户的体验差。</p><p>&nbsp;</p><p></p><h4>京东健康旗下家医事业部被整体裁撤，刘强东“训话”后京东宣布涨薪</h4><p></p><p>&nbsp;</p><p>5月27日，京东集团宣布，自2024年7月1日起，通过一年半时间，京东采销年度固定薪酬由16薪提升至20薪，业绩激励上不封顶。据了解，这是京东半年内第二次给一线采销人员涨薪。2023年12月底，京东集团宣布，2024年1月1日起京东采销等一线业务人员的年固定薪酬大幅上涨近100%，2024年初京东零售全员将平均加薪不低于20%。</p><p>&nbsp;</p><p>5月28日消息，网传京东健康旗下京东家医事业部被整体裁撤。有京东健康内部人士表示，京东家医事业部投入较大，但一直没有找到盈利模式，也没有其它价值，因此被放弃。对此，京东健康回应称，为整合公司旗下医疗健康服务业务，更高效利用医疗健康服务资源、更大化发挥专业角色的服务价值，京东健康针对家庭医生事业部进行了内部组织架构调整，将原家庭医生事业部的&nbsp;C&nbsp;端业务及职能，合并至互联网医疗事业部；B&nbsp;端业务及职能，合并至企业业务事业部。此次调整除架构分拆外，不涉及业务和人员裁撤。</p><p>&nbsp;</p><p>有媒体报道称，针对近期员工代打卡已形成产业链的问题，京东内部调查，每个月有1.4万人次找人代打卡，更有员工一年里代打卡近百天。代打卡一次收取15元，一个人可替20个人代打卡。</p><p>&nbsp;</p><p>在此背景下，刘强东在日前的高管会上直言：“凡是长期业绩不好，从来不拼搏的人，不是我的兄弟。”与此同时，京东管理层反思了过去数年间公司在管理上出现的失误，承认京东集团体系存在“大企业病”，管理上出现了严重问题，尤其在拼搏上有很大的懈怠。</p><p>&nbsp;</p><p>另外，磨铁集团&nbsp;CEO&nbsp;<a href="https://readhub.cn/entity_topics?type=10&amp;uid=bd975c885b3c1cf8">沈浩波</a>"还在朋友圈发文抵制<a href="https://readhub.cn/entity_topics?type=10&amp;uid=bf4792ab9e1c7531">京东</a>"，称京东不顾磨铁反对，强行让其产品参加低价促销。磨铁将采取法律手段维护权益，无限期停止对京东发货，要求京东下架其产品并退还。呼吁电商合作伙伴不要跟价。京东方面则回应称，其&nbsp;618&nbsp;活动目标是让消费者买到便宜的好书，否认站在出版社和行业对立面。京东对自营图书有合法的自主定价权。目前磨铁的网上店铺仍在正常销售，但部分热门图书显示无货。</p><p>&nbsp;</p><p></p><h4>阿里蔡崇信：我会尽量让&nbsp;CEO&nbsp;们和运营团队拥有更多的自主权和决策权</h4><p></p><p>&nbsp;</p><p>阿里巴巴集团董事长蔡崇信在与摩根大通北亚区董事长兼大中华区投资银行业务副主席&nbsp;Kam&nbsp;Shing&nbsp;Kwang&nbsp;的对话中强调，尽管面临监管、竞争压力和地缘政治等挑战，阿里巴巴仍聚焦于增长。公司致力于技术创新，并设定了未来&nbsp;10&nbsp;年的增长目标，旨在恢复营收增长，计划在&nbsp;2027&nbsp;财年实现两位数的增长。蔡崇信还表示，管理层会确保资源有效利用，并赋予&nbsp;CEO&nbsp;和运营团队更多自主权，以便根据实际情况作出合理决策。</p><p>&nbsp;</p><p></p><h4>900多个员工一脸懵逼：上午还在改bug、下午集团就解散了</h4><p></p><p>&nbsp;</p><p>5月28日消息，据多方爆料，上海极目银河公司老板跑路，因在国外玩金融亏损62亿，近千名员工两个月没发工资，公司宣布破产。极目银河员工称，老板跑了，公司原地解散，将近900多人的员工直接失业。上午还在改着bug，下午集团就解散了，还有几位新入职的同事，据说亏了62亿直接跑了。</p><p>&nbsp;</p><p>一位员工表示，2024年4月9日，公司发布全员通知称，原定于4月10日发放3月份工资，延迟到15日发放，但15日仍未发放。5月15日，公司口头通知4月份工资要到5月24日发放，24日口头又通知，工资5月27日下午3点发放，但到27日下午1点，得知老板跑路，公司破产。</p><p>&nbsp;</p><p>根据报案回执信息显示，老板陈群于2024年5月24日9时开始失联，去向不明。去他家里发现人去楼空，留下字条称，“无法对付投资人62亿，只能选择逃避。”</p><p>&nbsp;</p><p>此前有爆料称，这公司实际上就是几个小公司凑起来的，看起来什么业务都做，实际上没一个精通。注册资本7000万，实缴不敢公示，社保缴纳也不公开。各位求职者擦亮眼睛！</p><p></p><h4>智谱AI获中东财团&nbsp;4&nbsp;亿美元投资</h4><p></p><p>&nbsp;</p><p>据英国《金融时报》5&nbsp;月&nbsp;31&nbsp;日援引两位知情人士的消息报道，中东石油巨头沙特阿美旗下风险投资部门管理的&nbsp;Prosperity7&nbsp;基金参与了对中国人工智能初创企业智谱&nbsp;AI&nbsp;约&nbsp;4&nbsp;亿美元的一轮投资。</p><p>&nbsp;</p><p>《金融时报》表示：预计这笔投资将使智谱&nbsp;AI&nbsp;的估值达到约为&nbsp;30&nbsp;亿美元，此次投资也使得&nbsp;Prosperity7&nbsp;基金成为了首个对中国生成式人工智能初创企业投下重金的外国投资者。这笔投资表明，沙特愿意支持中国的人工智能生态系统。据一位知情人士表示，“沙特并不希望硅谷主导这个（人工智能）行业。”</p><p>&nbsp;</p><p></p><h3>IT&nbsp;业界</h3><p></p><p></p><h4>2500页文档曝谷歌搜索黑幕：用户数据被挪用、品牌主宰搜索排名</h4><p></p><p>&nbsp;</p><p>2500页谷歌搜索API泄露文档显示，谷歌使用点击数据、Chrome浏览器数据、白名单机制、质量评估员反馈等因素影响搜索排名，与官方公开表态矛盾。文档由资深SEO从业者Erfan&nbsp;Azimi泄露，旨在提高谷歌排名机制透明度。泄露内容可能对SEO领域产生重大影响，改变中小企业SEO策略。</p><p></p><h4>GPT-4o&nbsp;新功能已免费开放</h4><p></p><p>&nbsp;</p><p>5&nbsp;月&nbsp;30&nbsp;日，ChatGPT&nbsp;免费用户现已能够尝试自定义&nbsp;GPT&nbsp;模型、分析图表等其他&nbsp;GPT-4o&nbsp;新功能。当然，OpenAI&nbsp;在推出&nbsp;GPT-4o&nbsp;就承诺它将免费向所有用户开放，而付费用户的优势在于“拥有免费用户五倍的容量限制”。</p><p>&nbsp;</p><p>在此以前，诸如模型和网络响应、数据分析、图表创建、视觉、文件上传、内存和自定义&nbsp;GPT&nbsp;模型等功能仅对付费用户（ChatGPT&nbsp;Plus、Teams&nbsp;和&nbsp;Enterprise）开放，但现在已经面向所有&nbsp;ChatGPT&nbsp;用户开放（当免费用户使用&nbsp;GPT-4o&nbsp;达到消息或对话限制时将自动恢复到&nbsp;GPT-3.5）。</p><p>&nbsp;</p><p></p><h4>百度被曝或将在明年发布文心大模型5.0</h4><p></p><p>&nbsp;</p><p>据报道，百度或将于2025年百度世界大会期间发布新一代文心大模型5.0。目前，文心大模型最新版本为4.0版本，该版本于2023年举办的百度世界大会上由百度创始人李彦宏发布，具备理解、生成、逻辑和记忆四大核心能力。</p><p>&nbsp;</p><p>据李彦宏2023年10月发布时介绍，文心大模型4.0发布后相比GPT-4便已毫不逊色。据了解，历年百度世界大会举办时间均在每年下半年，按此推算，文心大模型5.0或将在2025年下半年与大家见面。公开信息显示，百度文心大模型于2019年首发，2021年发布3.0版本，2023年升级至4.0版本。</p><p>&nbsp;</p><p>此外，日前，百度集团执行副总裁沈抖向外界解释了文心系列主力大模型免费背后的原因，希望友商们别再天天拉表格比价格了，用更多的时间去卷场景、卷应用，把相关的场景都试一遍，快速验证。跑成功了，就快速复制。</p><p>&nbsp;</p><p></p><h4>宝马工厂机器狗上岗，可“嗅探”故障隐患</h4><p></p><p>&nbsp;</p><p>近日，宝马英国汉姆霍尔发动机工厂引进了一只名为&nbsp;SpOTTO&nbsp;的机器搜索犬，用于帮助检测生产线问题。这一举措引发人们对《华氏&nbsp;451》中机械猎犬的联想，然而&nbsp;SpOTTO&nbsp;并未具备小说中的暴力功能，其制造方为波士顿动力公司。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/26/26592eb27bee28dd76e504065e967dc1.jpeg" /></p><p></p><p>SpOTTO&nbsp;这个名称来源于两位奥托（Otto）——其中古斯塔夫·奥托是宝马的联合创始人之一，而尼古拉斯·奥托则是四冲程内燃机的发明者。该机器将作为汉姆霍尔工厂的一员，在此工厂每年生产&nbsp;40&nbsp;万台宝马和&nbsp;MINI&nbsp;的&nbsp;TwinPower&nbsp;涡轮增压发动机期间发挥重要作用。SpOTTO&nbsp;通过视觉、热成像和声音传感器来发现过热的设备和压缩空气泄漏等问题。这些问题如果不及时处理可能导致工厂停工或效率低下。</p><p>&nbsp;</p><p>除了发现潜在问题外，SpOTTO&nbsp;还可以爬楼梯，并为宝马的汉姆霍尔工厂的数字孪生体收集数据。未来，它还有可能承担更多任务，如读取模拟控制装置数据。这一举措旨在确保生产线的顺利进行，并进一步提高生产效率。</p><p></p><h4>腾讯、百度相继发布&nbsp;AI&nbsp;应用</h4><p></p><p>&nbsp;</p><p>本周，腾讯发布了一款名为&nbsp;“腾讯元宝”&nbsp;的&nbsp;AI&nbsp;助手，基于混元大模型，可在多个应用商店下载。该助手具备&nbsp;AI&nbsp;搜索、总结、写作等能力，能解析多种文档格式，并支持长上下文窗口。此外，腾讯还提供了包括&nbsp;AI&nbsp;头像、口语陪练和超能翻译等在内的有趣实用&nbsp;AI&nbsp;应用。混元文生图大模型也已升级并对外开源。</p><p>&nbsp;</p><p>同时，在&nbsp;2024&nbsp;百度移动生态万象大会上，百度副总裁王颖推出全新&nbsp;AI&nbsp;原生应用&nbsp;「橙篇」，该应用可实现文件处理、创作等功能，依托百度文库&nbsp;12&nbsp;亿内容积累和多项&nbsp;AI&nbsp;技术。此外，百度文库经过大模型重构后，已吸引&nbsp;1.4&nbsp;亿&nbsp;AI&nbsp;用户，AI&nbsp;功能使用次数突破&nbsp;15&nbsp;亿次，显示了百度在&nbsp;AI&nbsp;内容获取与创作领域的领先地位。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</id>
            <title>Meta 首席科学家：不要从事 LLM 工作；代打卡成产业链，刘强东怒了：业绩不好不拼搏全部淘汰；上千页文档揭谷歌搜索黑幕 | Q资讯</title>
            <link>https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZrasmSqhfYByA0JXUj0d</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:25:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: TikTok, FSD, Meta AI, DNF手游
<br>
<br>
总结: TikTok正在回应美国版本核心算法开发的传闻；特斯拉即将在中国推出FSD全自动驾驶系统；Meta AI首席执行官建议不从事LLM工作；腾讯旗下DNF手游首周狂揽10亿元。 </div>
                        <hr>
                    
                    <p></p><blockquote>TikTok 回应“正开发核心算法的美国版本”；传特斯拉的 FSD 即将在中国落地；腾讯《DNF》手游首周狂揽 10 亿元；米哈游再次起诉哔哩哔哩侵权；GPT-4o 向所有人免费开放！字节再试 AI 硬件；中国首例消费者起诉苹果垄断案一审宣判；OpenAI 组建新的安全团队；知乎 PC 网页端非登录用户无法查看回答全文；联想集团裁员 7500 人；谷歌搜索算法内幕被扒；Meta 为社交媒体数据工具 CrowdTangle 增添安全功能；XZ 5.6.2 释出，移除后门代码……&nbsp;&nbsp;</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>TikTok回应“正开发核心算法的美国版本”</h4><p></p><p>当地时间5月30日，有消息称，字节跳动旗下TikTok正开发核心算法的“美国版本”。一旦代码分开，有望为字节跳动美国资产的剥离奠定基础，但该公司目前并无此计划。</p><p></p><p>TikTok对此说法在社交媒体平台X发文回应称：相关报道具有误导性且与事实不符。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f1d61707cc872775aa48e0ba9da46d3.png" /></p><p></p><p>TikTok表示，TikTok剥离法案要求的TikTok继续在美运营的“合格剥离”动作，在商业、技术和法律上都是不可能的，并且“肯定不可能在该法案要求的270天时限内完成”。</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651207433&amp;idx=1&amp;sn=b0d3776443b6844e19dcc2e371d790cf&amp;chksm=bdbbcf5a8acc464c548ee33611d68afb58a21d5407fae1d87f4c88b11ed8ccd699a1815d33fb&amp;token=927577701&amp;lang=zh_CN#rd">剥离几百万行代码，复制核心算法去美国？TikTok&nbsp;最新回应来了</a>"》</p><p></p><h4>传特斯拉的&nbsp;FSD&nbsp;即将在中国落地</h4><p></p><p>5月31日消息，据外媒报道，有知情人士表示，特斯拉已成功获得中国工业和信息化部的软件注册，这将为特斯拉内部测试全自动驾驶(FSD)铺平道路，特斯拉员工将在中国公共道路上测试，并计划在未来几个月内升级推送给中国用户。然而，在中国电动汽车制造商竞争激烈的背景下，2024年一至四月期间，由于市场环境不景气等原因导致了特斯拉在华销量下滑7.6%。</p><p></p><p>另外值得一提的是，《关于汽车数据处理4项安全要求检测情况的通报》（第一批）于4月28日发布，其中提到特斯拉上海超级工厂生产的车型全部符合合规要求，成为唯一一家符合此要求的外资企业。这些措施表明特斯拉正积极遵守中国相关法规并努力提升其在华业务的发展前景。</p><p></p><p>针对此事，特斯拉客服表示，目前内部员工没有进行相关的测试。不同城市不太一样，即使未来能够落地或者开放，也是在允许完全自动驾驶能力测试的城市开放。"我们确实在筹备这件事，但具体什么时间落地，也需要一个长久的时间，目前官方没有任何相关消息。用户可以关注官方公众号和官方微博，如果有最新的进展或者待发布的时间我们会第一时间通知给所有的车主。</p><p></p><h4>Meta&nbsp;AI&nbsp;首席执行官&nbsp;LeCun：不要从事LLM工作</h4><p></p><p>在近日巴黎举行的初创企业年度技术大会VivaTech上，有着“人工智能业鲁迅”之称的Meta&nbsp;AI的首席执行官Yann&nbsp;LeCun建议希望在AI生态系统中工作的学生不要从事LLM（大型语言模型或称“法学硕士”）方面的工作。</p><p></p><p>“如果你是对构建下一代AI系统感兴趣的学生，请不要从事LLM方面的工作。这是大公司的事情，你们无法对此有所贡献，”LeCun在会议上表示。他还说，人们应该开发能够克服大型语言模型局限性的下一代AI系统。</p><p></p><p>有趣的是，关于LLM（大型语言模型）替代品的讨论已经持续了一段时间。最近，Devika的年轻创始人Mufeed&nbsp;VH（Devika是Devin的替代品）谈到了人们应该如何远离Transformer模型并开始构建新的架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3af342d4c19c169990c5185b58116da4.png" /></p><p>他还认为，通过这种方法，甚至有可能构建出与GPT-4一样令人印象深刻的东西。</p><p></p><p>尽管LeCun反对一股脑全部研究LLM，但Transformer训练模型仍在不断发展。AI/ML顾问Dan&nbsp;Hou谈到了GPT-4o，并强调了其训练模型。</p><p></p><h4>腾讯《DNF》手游首周狂揽10亿元，App&nbsp;Store&nbsp;下载超260万次</h4><p></p><p>根据彭博社报道，腾讯上周刚刚上线的爆款手游《地下城与勇士：起源》首周狂揽1.4亿美元（约合人民币10亿元），一跃成为近期腾讯最赚钱的手游产品。在iOS国区畅销榜上，《DNF》手游持续霸榜9天，这个记录有望进一步保持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac1754779d6d07d1594b5bd9d85fad32.png" /></p><p></p><p>此外，根据Sensor&nbsp;Tower&nbsp;的数据显示，《DNF》手游在App&nbsp;Store商店共吸金6300万美元，下载次数为260万次，值得一提的事，这个数据不包括中国安卓一级第三方的下载和收入。</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12872f735d0f71e0ce6411d7592c0b61.jpeg" /></p><p></p><p>彭博社补充道，《DNF》手游已经成为上周App&nbsp;Store上最赚钱的游戏，紧随其后的事《王者荣耀》和《和平精英》。Niko&nbsp;Partners的分析师则认为，《DNF》手游可能在今年为腾讯带来超过10亿美元的收入，但是否能成为下一个《王者荣耀》，还有待长期去观察。</p><p></p><h4>即刻开庭！米哈游再次起诉哔哩哔哩侵权</h4><p></p><p>近日，上海米哈游天命科技有限公司与B站关联公司上海宽娱数码科技有限公司、上海幻电信息科技有限公司相关著作权权属、侵权纠纷案件新增开庭公告，原告为上海米哈游天命科技有限公司，该案于5月31日在上海市浦东区人民法院开庭审理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/341622afe6864e3657f9bf8dbba3cde8.png" /></p><p></p><p>此次诉讼由上海米哈游天命科技有限公司作为原告提起，再次将哔哩哔哩推上了风口浪尖。根据天眼查的法律诉讼信息，此次案件涉及的是著作权权属和侵权问题，追溯过往，米哈游与哔哩哔哩的拉锯战并非首次。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76358096cd87380f0026bfafb8599ba3.png" /></p><p></p><p>历史记录显示，米哈游早前已因类似侵权争议对上海宽娱数码科技有限公司提起诉讼，尽管最终以原告主动撤诉收场，但双方恩怨显然未了。</p><p></p><h4>终于，GPT-4o&nbsp;向所有人免费开放！</h4><p></p><p>5月30日凌晨，OpenAI在x平台宣布，GPT-4o多模态能力向所有用户免费开放。不过，在使用次数上官方没有更新公告，预计还是会是有一定次数限制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5599021da5ddc33fadabdbe4280c983d.jpeg" /></p><p></p><p>本次免费开放的功能包括：联网搜索、数据分析、视觉分析（照片）、文件上传、GPTs商店和长期记忆。</p><p></p><p>使用网址：<a href="https://www.chatgpt.com/">www.chatgpt.com</a>"</p><p></p><p>北京时间5月14日凌晨，OpenAI举办发布会，带来了GPT-4o。该模型打通了文本、图片、视频和语音输入，无需中间转换，互相之间就可以直接生成。此次发布会还带来了更智能的实时语音助手，以及适用于macOS的ChatGPT桌面应用程序。</p><p></p><p>当时，OpenAI宣布向ChatGPTPlus和Team用户推出GPT-4o，很快就会向企业用户推出。同时，也向所有人开放GPT-4o多模态能力，但有使用次数限制。</p><p></p><h4>文心一言再换帅！原快手副总裁薛苏加入百度</h4><p></p><p>5月30日，百度举办的百度移动生态万象大会中，新任百度副总裁、AI产品创新业务负责人，New&nbsp;App负责人薛苏正式登场。</p><p></p><p>薛苏，原快手副总裁，2020年加入快手，先后负责短剧与二次元业务、体育业务。2023年底，快手进行大规模组织架构调整，薛苏不再担任运营部二次元内容中心的负责人，同时也不再兼任运营部体育业务中心和医疗健康组的负责人。</p><p></p><p>薛苏毕业于清华大学自动化系，在耶鲁大学取得博士学位，研究方向为计算机视觉与图形学，曾在EA和Facebook工作。</p><p></p><p>百度NEW&nbsp;App是一个集成了百度“AI&nbsp;伙伴”&nbsp;和&nbsp;“AI&nbsp;BOT”&nbsp;等功能的内测产品，在2023年百度移动生态大会上，由百度集团资深副总裁、百度移动生态事业群组（MEG）总经理何俊杰正式发布。</p><p></p><h4>阿里巴巴近2亿元投资，AI+“首战”在教育行业打响</h4><p></p><p>5月29日消息，教育科技公司精准学完成了新一轮融资，投资方为阿里巴巴，投资金额近2亿元。精准学成立于2018年1月，基于AI推荐引擎技术的“精准学理念”，此前与包括上海昂立在内的国内数千家教育培训学校建立了合作，也是学而思、高思集团的战略合作伙伴。</p><p></p><p>从2023年开始，精准学基于生成式AI技术开始了一次产品重构，以阿里通义千问大模型为基础，自主研发了“心流知镜”大模型，基于此研发了自有的AI教育应用，搭载在硬件之上。</p><p></p><p>精准学旗下的首款原生代AI辅学机将于今年6月对外发布，该产品基于阿里云通义大模型及虚拟数字人技术打造。值得关注的是，这也是阿里巴巴在过去两年中，首次在AI应用领域出手。</p><p></p><h4>字节再试AI硬件，两条产品线共发力</h4><p></p><p>近期，彭博社报道字节以5000万美元（约合人民币3.62元），收购中国耳机制造商Oladance。彭博社援引知情人士称，此举是因为字节管理层看到了可穿戴设备成为AIGC服务平台的潜力。</p><p></p><p>从多位知情人士处获悉，字节AI硬件方向的探索，在内部分为两条产品线：</p><p></p><p>一条产品线代号为“D线”，负责人为李浩乾，其为OWS（Open&nbsp;Wearable&nbsp;Stereo，开放式可穿戴立体声耳机）耳机品牌Oladance创始人。不久前，字节收购了这一品牌。</p><p></p><p>目前，Oladance团队正在融合进字节体系，整体归在字节跳动移动OS中台业务部下，李浩乾向邹伟汇报——邹伟曾经为锤子手机的软件负责人，在锤子手机被收购后加入字节，曾经是字节Smartisan&nbsp;OS的负责人。</p><p></p><p>另一条产品线为O线，负责人也是字节曾收购公司的创始人，其向字节跳动技术副总裁洪定坤汇报。</p><p></p><h4>不满30%“苹果税”，中国首例消费者起诉苹果垄断案一审宣判</h4><p></p><p>5月29日，上海知识产权法院就中国首例消费者起诉“苹果”垄断案一审宣判，法院认定苹果公司在中国区软件市场显然具有市场支配地位，但没有滥用市场支配地位，驳回原告金某的诉请。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9bc8962eebb79206d92df7ffa8c8e7f5.png" /></p><p></p><p>因不满苹果对“应用内购买”收取的30%的佣金，苹果手机消费者金某（原告）于2021年1月11日将苹果公司和苹果电脑贸易（上海）有限公司告上法庭。根据裁判文书，金某核心诉求包括停止收取30%“苹果税”的不公平高价行为以及停止“应用内购买”强制使用苹果支付系统的搭售行为。</p><p></p><p></p><blockquote>注：所谓“苹果税”，是指苹果对 App Store 上所有应用的数字内容消费抽取 15%~30% 的佣金。每当苹果用户通过苹果手机应用商店付费下载 App 或在 App 内部购买数字商品 / 服务时，苹果公司会扣留交易金额的一部分作为“过路费”，再将剩下的转给相应的 App 开发者。如果不接受“苹果税”的提成安排，或者应用程序内置有隐藏的其他支付方式，苹果公司将径直下架该款应用程序。据了解，中国目前是全球范围内苹果公司对于 App 内交易抽成费率最高的国家，抽成费率为 30%。近年来，苹果公司已在欧盟、美国、日本、韩国等多地遭政府起诉或调查，欧盟地区“苹果税”从今年 3 月开始大幅下降。</blockquote><p></p><p></p><p>原告代理律师表示，尊重一审法院的判决，但将针对本案上诉至最高人民法院。</p><p></p><h4>“业绩不好不是我的兄弟”！京东“整顿职场”上热搜</h4><p></p><p>5月24日，有媒体报出刘强东“凡是长期业绩不好，从来不拼搏的人，不是我的兄弟”的发言，因为在2018年刘强东说永远不开除任何一个兄弟。让大众注意到，曾经把传统商业大鳄国美、苏宁拉下马，并跑赢淘宝天猫的京东，如今发展很不如意。</p><p></p><p>尤其是5月22日，拼多多以高达2042.74亿美元的市值再度超越阿里巴巴，4倍于京东，应该让京东高管团队更加坐立难安。这期间又爆出关于于京东内部考勤调整、午休时间缩短、人员优化等一系列事件引发舆论热议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/812a1b7e94f88fce19a2393b77e40213.jpeg" /></p><p></p><p></p><blockquote>一名京东员工表示：有员工一年里代打卡近百天，每天 4 点就下班，但是能领到全额薪水，还有一些实习生甚至两个月都不来，但通过代打卡，骗走了公司 1.5 万的工资。听说内部统计代打卡的人次每月近万次。”而京东午休时长达 2 个小时，很多员工下午 2 点半才开始工作，甚至可以熄灯睡觉。2 点半并不晚，但京东的上班时间很晚，弹性工作制的部门上午 10 点 30 才打卡，非弹性部门 9 点打卡，就这还有很多人花钱找人代打卡。然后上班 2 个小时不到，吃午饭了，然后再睡 2 个小时……</blockquote><p></p><p></p><h4>OpenAI&nbsp;前安全研究员&nbsp;Jan&nbsp;Leike&nbsp;转投&nbsp;Anthropic；OpenAI组建新的安全团队</h4><p></p><p>5月28日，OpenAI宣布，成立安全与审查委员会（SSC），负责针对所有的OpenAI项目制定至关重要的安全与审查决定。而值得注意的是，进入该安全顾问委员会的成员全部为该公司内部人员，包括OpenAI的CEO&nbsp;Sam&nbsp;Altman，外部人员一个也没有。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3be5b0e7150eacd87e30e5d6381bc76d.png" /></p><p></p><p>OpenAI还宣布，最近开始训练其下一个前沿模型，我们预计由此产生的系统将使我们在通往通用人工智能的道路上迈上一个新台阶。</p><p></p><p>毕竟在5月15日-17日，OpenAI超对齐团队两位负责人接连离职。其中包括OpenAI联合创始人、首席科学家伊利亚·苏茨克维尔（Ilya&nbsp;Sutskever），以及该团队的负责人简·雷克（Jan&nbsp;Leike）。</p><p></p><p>Jan&nbsp;Leike，OpenAI的前首席安全研究员，在5月15日宣布离职后，于本周二正式加入人工智能初创公司Anthropic。在OpenAI解散了他共同领导的超级对齐团队后，Leike表示，他将在Anthropic继续推动AI安全和监督的研究工作。Anthropic得到了亚马逊的大力支持，后者承诺投资高达40亿美元以换取少数股权。同时，OpenAI也在积极调整，新成立了一个由CEO领导的安全委员会，以确保公司项目和运营的安全。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/eced874ce3eddd98ed000070c5459230.jpeg" /></p><p></p><h4>昆仑万维：天工AI日活用户超过100万</h4><p></p><p>5月27日，昆仑万维集团宣布，天工AI每日活跃用户（DAU）已超过100万。</p><p></p><p>5月，全球大模型领域的竞争，分外激烈；谷歌、腾讯、阿里、字节跳动等业界巨擘纷纷亮出大招。有的凭借技术底蕴实现全面升级，有的慷慨将大模型开源免费，更有甚者通过价格战掀起狂风巨浪，整个市场一片火热。然而在这硝烟弥漫的战场上，昆仑万维作为中国AI的先驱者，近日正式公布天工AI日活用户（DAU）突破100万</p><p></p><p>根据QuestMobile的数据，2024年3月，天工平台的月活跃用户已达近千万，仅次于豆包与文心一言，成为国内活跃用户数第三的平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/937f29a2a1d7775ec4dfa77e8be9b4e6.png" /></p><p></p><p>据悉，昆仑万维集团在4月17日推出了天工3.0和天工SkyMusic，这两款产品的推出，为天工平台的增长注入了新的动力。天工3.0是一款拥有4000亿参数的开源大模型，其在多个领域实现了性能上的突破性提升。</p><p></p><h4>知乎PC网页端非登录用户无法查看回答全文</h4><p></p><p>5月27日，有网友反映称，知乎网页端近期出现了非登录用户无法查看全文的情况。测试显示，以知乎当前热榜第一的内容为例，非登录PC网页用户访问知乎的回答页面，如果点击“展开阅读全文”，平台会自动弹出登录窗口，确实无法直接查看全文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e9075f4abd2444caad05c6229803589.png" /></p><p></p><p>而非登录手机网页用户，则可以直接点击“展开阅读全文”查看全部信息，无需登录或下载App。对于这一情况，知乎官方暂未给出回应。</p><p></p><p>据了解，2022年3月初，工信部表示有网友和媒体反映部分网站在用户浏览页面信息时，强制要求下载App问题。工业和信息化部信息通信管理局对此高度重视，立即组织核查，并于日前召开行政指导会，督促相关互联网企业进行整改。</p><p></p><h4>联想集团裁员7500人，遣散费5500万美元！</h4><p></p><p>近日，联想集团发布了2023-2024年度全年财报。这份财报揭示了公司在过去一年中所面临的挑战与压力。财报显示，截至2024年3月31日的年度收入为619.47亿美元，同比下滑8%。与此同时，净利润也呈现下滑趋势，同比减少37%至10.1亿美元。这一业绩的下滑，无疑给联想集团带来了沉重的打击。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9181c3f3356a43f26b67ef57f40a142.png" /></p><p></p><p>财报披露，2023财年全球总员工77,000人，2024财年则是69,500人，这意味着，在过去的一年联想集团共裁减7500人。联想还在财报中的“费用类别”提到，“鉴于行业挑战”，产生遣散及相关费用5500万美元。联想在财报中“拨备的组成部分”说明中，提到了“主要是员工解雇付款”，目的是降低成本和提高运营效率。</p><p></p><p>值得一提的是，近期联想股价涨至一年来新高。5月27日周一早间，联想集团（00992.HK）股价一度涨逾9%，至12.06港元/股。5月30日，联想以11.52港元/股收盘。业界认为，这是AI给联想回了一口大血，联想开始走出低谷。</p><p></p><h4>软银寻求每年90亿美元的AI投资</h4><p></p><p>据外媒报道，日本软银计划每年向人工智能领域投资近90亿美元，加速企业转型。</p><p></p><p>日前，软银首席财务官后藤义光(Yoshimitsu&nbsp;Goto)向媒体透露，将加大对人工智能公司的投资。为此，软银将资产负债表维持在一个安全水平，以确保投资活动的持续性和灵活性。</p><p></p><p>报道中称，软银在人工智能领域面临着激烈的竞争，因为微软、亚马逊和谷歌等科技巨头已经投资数十亿美元与初创公司合作建立人工智能模型。此外，顶级风投公司希望与开发人工智能产品和应用程序的公司开展合作。</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>2500页文档曝谷歌搜索黑幕：用户数据被挪用、设白名单机制、品牌主宰搜索排名</h4><p></p><p>5月29日消息，近日，据市场和受众研究公司SparkToro在官网博客发布的文章，一位匿名消息人士（后证实为搜索引擎优化行业资深从业者Erfan&nbsp;Azimi）向SparkToro公司的CEO&nbsp;Rand&nbsp;Fishkin提供了来自谷歌搜索API的大量泄露文档，这些文档揭示了谷歌搜索引擎内部排名算法的详细信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d7bf253b3ca43bb72cae85f2c61a974.png" /></p><p></p><p>本次泄露文档中的部分内容与谷歌公开表态的信息以及2023年的美国司法部起诉谷歌的反垄断案件中的证词相矛盾。比如，谷歌曾一再否认使用点击导向的排名机制，否认子域名在排名中被单独考虑，否认收集或考虑域名的年限等等。但泄露文档证实，这些因素都是谷歌搜索引擎排名机制中的一环。此次泄露的谷歌搜索API文档共计2500余页，共包含14014个属性特征。这些文档据称是谷歌内部“Content&nbsp;API&nbsp;Warehouse”的一部分，并在代码托管平台GitHub上不慎公开了一段时间。</p><p></p><p>博客作者Rand&nbsp;Fishkin在多位前谷歌员工的沟通中确认，泄露的文档具备谷歌内部API文档的特征。Rand&nbsp;Fishkin与和技术SEO（搜索引擎优化）专家Mike&nbsp;King进行交流，并再次验证了这些文档的真实性。</p><p></p><p>这些文档的泄露可能对搜索营销领域产生深远的影响，因为它们提供了对谷歌搜索引擎内部工作机制的前所未有的洞察，包括它如何使用点击数据、浏览器点击流、白名单、质量评估员反馈和链接质量来影响搜索结果排名。</p><p></p><h4>Meta为社交媒体数据工具CrowdTangle增添安全功能，以消除欧盟顾虑</h4><p></p><p>5月27日，Meta&nbsp;Platforms为社交媒体数据追踪工具CrowdTangle增加了安全功能，试图消除欧盟的担忧。欧盟上个月对其逐步淘汰该工具的决定的影响开展调查。Meta上周表示，下月投票的欧洲议会选举候选人将在其Facebook和Instagram的feed顶部看到如何保护自己及其账户的通知。</p><p></p><p>通知中包含的指引将帮助候选人找到相关资源，学习如何设置双因素认证、检查安全设置或启用Instagram的"隐藏词语"功能。"隐藏词语"是Instagram的一项特性，能够自动将含有攻击性词汇、短语和表情符号的私信请求转移到一个隐蔽的文件夹中。</p><p></p><p>Meta在欧盟每月拥有超过2.5亿用户。为了进一步增强选举监督，该公司在上周的宣布基础上，从本周一开始，为每个欧盟国家提供实时监控功能，按关键词、公共群组和Instagram账户进行分类，使研究人员、记者和民间社会组织能够实时监督选举过程。</p><p></p><p>欧盟委员会对Meta的这一举措表示欢迎。在一份声明中，欧盟委员会表示：“委员会将监督这些新功能的实施效果，并将继续与Meta合作，寻找更持久的解决方案，以解决初步决定中提出的所有问题。”</p><p></p><h4>苹果据悉已与OpenAI达成协议，将ChatGPT引入iOS18</h4><p></p><p>当地时间5月26日，彭博科技记者马克·古尔曼（Mark&nbsp;Gurman）在最新一期通讯中表示，传闻几个月的苹果与OpenAI洽谈将生成式人工智能功能引入iOS&nbsp;18一事，有了取得进展的消息，透露双方已经达成了协议。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0147c23287e5e2cd135eba1725330a7.png" /></p><p></p><p>按惯例，苹果iOS等操作系统的重大更新，在全球开发者大会上就将宣布。为iOS&nbsp;18引入OpenAI的生成式人工智能功能，预计会在今年的大会上宣布</p><p></p><p>苹果和OpenAI洽谈将生成式人工智能引入iOS&nbsp;18的消息，在今年年初就已出现。在3月下旬的报道中，就有外媒提到苹果和百度、OpenAI及谷歌，在就引入他们的生成式人工智能模型、赋予部分iPhone机型生成式人工智能功能有过谈判，其中与百度的谈判是针对国内市场，与谷歌和OpenAI则是针对国外市场。</p><p></p><h4>XZ&nbsp;5.6.2&nbsp;释出，移除后门代码</h4><p></p><p>今年3月29日，微软PostgreSQL开发人员Andres&nbsp;Freund在调试SSH性能问题时，在开源安全邮件列表中发帖称，他在XZ软件包中发现了一个涉及混淆恶意代码的供应链攻击。据Freund和RedHat称，Git版XZ中没有恶意代码，只有完整下载包中存在。但是这个代码的提交人两年前就加入了项目维护，暂时不能确定之前的版本有没有问题。</p><p></p><p>引发广泛关注的&nbsp;XZ&nbsp;后门事件两个月之后，项目维护者&nbsp;Lasse&nbsp;Collin&nbsp;释出了新版本&nbsp;XZ&nbsp;5.6.2，移除了&nbsp;v5.6&nbsp;和&nbsp;v5.6.1&nbsp;中的后门代码&nbsp;CVE-2024-3094。</p><p></p><p>他同时宣布了一位支持维护者&nbsp;Sam&nbsp;James。对&nbsp;XZ&nbsp;后门事件的调查仍然在进行之中。XZ&nbsp;5.6.2&nbsp;还修复了一系列&nbsp;bug，包括修复了用最新&nbsp;NVIDIA&nbsp;HPC&nbsp;SDK&nbsp;构建的问题，移除&nbsp;GNU&nbsp;Indirect&nbsp;Function(IFUNC)支持，XZ&nbsp;后门代码使用了&nbsp;IFUNC&nbsp;支持，但移除主要是因为性能优势太小但复杂性大幅增加。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</id>
            <title>这个离开大厂去 AI 创业的互联网大佬，带着他的“Killer Agent”来了</title>
            <link>https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YSpCsCCLvHvrsXB179Wm</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 08:03:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 出品, InfoQ, 大模型领航者, To B
<br>
<br>
总结: 2020年，黎科峰博士离职创业，选择了To B领域，创业3年营收数亿元，大模型产品符合To B行业需求，将颠覆现有To B软件，改变决策关系。如何选择和应用大模型，是To B行业面临的挑战。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜黎科峰博士，数势科技创始人兼CEO作者｜褚杏娟</blockquote><p></p><p>&nbsp;</p><p>2020年是一个不同寻常的年份。人们因新冠不得不停下来的时候，时任京东集团副总裁、技术委员会主席、京东商城技术负责人的黎科峰博士却选择了离职创业。此时的黎科峰博士，已经在互联网行业摸爬滚打了十余年。</p><p>&nbsp;</p><p>他曾在三星亲历了公司第一代智能手机诞生；在平安集团作为执委、平安金融科技CTO推动平安集团的用户和数据体系建设，以及金融科技云平台建设；在百度，作为原百度云计算创始成员推动百度云OS初创，也曾担任过手机百度总经理，带领实现日活跃用户1亿的突破。</p><p>&nbsp;</p><p>意料之外，情理之中。在经历了PC和移动互联网两次浪潮后，黎科峰博士明显感到To C的流量增长已经放缓，红利期基本结束，大厂们也开始在To B 领域布局。To B 就是黎科峰博士给自己选的赛道，而这次他恰好赶上了大模型这波浪潮。</p><p>&nbsp;</p><p>黎科峰博士一直说自己是“To B 新人”，但他创业3年后就让公司营收达数亿元的成绩，已经无法让人用“新人”来定义他。</p><p>&nbsp;</p><p></p><h2>C 端大佬，凭何入局 B 端</h2><p></p><p>&nbsp;</p><p></p><blockquote>“创业，始终是一条不同寻常的道路。别人已经走过并走通了的路，没有必要再去重复。”</blockquote><p></p><p>&nbsp;</p><p>中国的 To B 公司大致有两种。</p><p>&nbsp;</p><p>一种是专注项目制和定制化服务的公司，根据客户提出的具体需求逐一开发。但问题是客户的需求是零散和片段化的，他们可能并不清楚自己的数字化是什么样的。</p><p>&nbsp;</p><p>另一种则是“Copy to China”模式，商业模型、产品理念，甚至PMF理论很多是直接从美国照搬过来。但中国的市场环境和美国不同，简单复制很难成功。</p><p>&nbsp;</p><p>企业内部通常分为利润中心和成本中心，其中成本中心包括了行政、人力资源、财务和IT部门等。国内许多To B 软件实际上是围绕成本中心设计的，比如OA系统等，但这些软件的付费能力相对较弱，转化为直接业务价值的路径较长。</p><p>&nbsp;</p><p>正如黎科峰博士所说，“企业高层或数字化负责人很难有底气地说，企业增长的20%-30%是他们带来的。”</p><p>&nbsp;</p><p>表面上看，国内数字化认知不足、大家付费意愿不强等都是To B 企业面临的普遍挑战。但长期从事To C产品的经验告诉黎科峰博士，问题的关键是从业者没有解决真正的痛点、真正帮助用户创造价值，To C还是To B 反而并不重要。</p><p>&nbsp;</p><p>根据黎科峰博士观察，国内数千万的企业正在从过去依靠规模增长的模式，转变为追求内在能力提升和经营效率提高的模式。企业意识到，仅凭直觉和经验已经难以持续增长，开始寻求技术的帮助。</p><p>&nbsp;</p><p>但不同行业的软件使用者技术水平存在差异。电商、金融科技等互联网企业数字化水平较高，员工可以熟练使用复杂软件，但其他行业的企业就难以有效使用，致使其软件使用成本很高。</p><p>&nbsp;</p><p>因此，在黎科峰博士看来，To B 行业要真正发展，首先需要建立共识，即让市场参与者普遍认同软件或解决方案的价值，否则就得花大量的时间和精力说服和教育市场。其次，产品必须足够简单易用，能轻松融入业务团队的日常工作，并且无需复杂培训。</p><p>&nbsp;</p><p>大模型产品完美符合上述条件。</p><p>&nbsp;</p><p>不用多说，大家已经对大模型的价值和潜力有了共识，这是一个非常重要的优势。另外，大模型降低了数据的使用门槛，让AI和大数据不再只属于技术圈，而是广泛的普通大众。</p><p>&nbsp;</p><p>如今，虽然不是每家企业都开始实际应用大模型，但至少都在研究、了解和学习。走得远的企业已经开始尝试将大模型技术应用于实际业务中，甚至有些已经签了商业合同。</p><p>&nbsp;</p><p>“这种对大模型技术的高度认可和期待，甚至超过了当年的云计算。”黎科峰博士表示。</p><p>&nbsp;</p><p>传统的 To B 软件十分复杂，大多数功能用户并不需要，同时很久都不做本质上的改进，长期保持最初的界面和功能。而通过基于大模型的软件，用户通过简单表达就可以实时、精确地找到所需信息，软件还能不断适应和学习，越来越贴合用户的个性化需求和使用习惯。</p><p>&nbsp;</p><p>“从这个角度来看，大模型技术会颠覆现有的To B软件，甚至让很多To B软件过时。”黎科峰博士补充道，“当然，现有的ToB软件开发商也会努力追赶，但他们可能会因为历史包袱而受限。”</p><p>&nbsp;</p><p>另外，大模型产品还将改变&nbsp;To B 业务的决策关系。</p><p>&nbsp;</p><p>黎科峰博士表示，国内软件的使用者和决策者往往不是一线员工，而是不使用软件的管理层。大模型产品则让每个员工都能通过使用工具产生价值，这样决策链将不再只是老板，还有真正使用软件的员工，员工对软件效率提升的需求会影响决策者。</p><p>&nbsp;</p><p></p><h2>如何实现大模型价值落地？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“现在的大模型就像一个全科研究生，而Agent是帮助它成为某个特定业务领域专家的实现方式。”</blockquote><p></p><p>&nbsp;</p><p>在明确了大模型对于&nbsp;To B的影响后，该如何应用大模型呢？</p><p>&nbsp;</p><p>困扰很多人的首先是如何选择大模型。根据数势科技的基准测试，国内大模型产品之间的差距并不显著，与国际领先的模型，如GPT-4等比较，算力、准确度等方面确实存在较为明显的差距。</p><p>&nbsp;</p><p>但对于如何应对和看待国内外大模型的发展差异，黎科峰博士认为，“核心是我们是否需要像OpenAI那样投入巨额资金（数百亿）来发展大模型技术。”</p><p>&nbsp;</p><p>OpenAI 开始不计商业成果，致力于推动通用人工智能（AGI），后来也得到了美元基金支持，能够获得的资金是中国企业的很多倍。中国的大模型厂商在资源有限的情况下，需要从一开始就明确自己的方向，思考如何创造价值并实现商业化回报。这涉及到选择哪些方向进行投资，以确保资金的使用更加高效和有针对性。</p><p>&nbsp;</p><p>另外，大型企业和创业公司开发的大模型产品方向也各有千秋。</p><p>&nbsp;</p><p>大企业开发大模型有明显的资源优势，比如丰富的计算资源和大量数据积累，然而也面临着诸如如何将大模型整合到现有业务和产品线等较重的商业化包袱。</p><p>&nbsp;</p><p>而创业公司没有历史包袱，通常从大模型原生视角出发，不受以往业务约束，更自由地探索和创新，因此某些产品的体验可能不逊色大型企业。</p><p>&nbsp;</p><p>当前，一些财力雄厚的大型企业，如银行和国有企业会先投资建立大模型基础设施，如购买GPU和部署具有数千万、甚至数千亿参数的大模型，然后逐步挖掘需求和应用场景。</p><p>&nbsp;</p><p>但更多的企业是先找应用场景，再立项、部署大模型。这些企业会梳理出一系列需求，然后据此决定大模型的应用方向。例如，如何快速准确地找到数据、基于数据进行分析、洞察业务问题以及指导经营以提升成果，通常是很靠前的考虑因素。</p><p>&nbsp;</p><p>黎科峰博士透露，目前率先尝试大模型应用有经营分析（与数据紧密相关的金融、零售和高端制造行业）、内容生成（如营销活动所需的视频或图片生成）、企业知识库（从企业内部的知识库中提取关键信息，以便员工更好查找和学习）。企业经营分析是目前企业最感兴趣的领域，超过90%的企业表现出了明确需求。</p><p>&nbsp;</p><p>大模型具备知识、智商、学习能力和推理能力，能够总结和生成新的见解。AI Agent 的本质是智能体，内部包含许多规划业务流程规划器，能够完成基于业务目标的任务规划和自动执行。</p><p>&nbsp;</p><p>数势科技的做法是，大模型叠加 Agent，让企业应用具备记忆、反思和学习能力，能够调用企业内部工具并不断迭代反思，真正实现业务价值落地。&nbsp;</p><p>&nbsp;</p><p>不过，找到了应用场景后就让To B赛道的创业公司短期内盈利，黎科峰博士认为这并不合理。</p><p>&nbsp;</p><p>“很多企业在没有想明白胜利逻辑和商业化可能性之前，就急于跟风，最终导致失败。”黎科峰博士说道，一个企业的价值不在于目前是否盈利，而是在于它的商业模式是否成立。</p><p>&nbsp;</p><p>就像美国的许多企业一开始并不盈利，但找到正确的盈利模式后会非常赚钱。黎科峰博士认为，这背后的关键原因是，企业要找到产品能够为企业带来的业务价值，并且具有强大的复制性，实现边际成本递减，样才能实现高额盈利。</p><p>&nbsp;</p><p></p><h2>如何在内卷的环境里赚到钱？</h2><p></p><p>&nbsp;</p><p></p><blockquote>“盈利可以通过裁员或非健康的手段实现，但这并不是我们追求的。”</blockquote><p></p><p>&nbsp;</p><p>“商业化是在大厂打工不需要太考虑的问题，但自己做企业就要考虑怎么活下去。”黎科峰博士曾在分享中说到，但他在大家还在探索大模型应用的时候，已经用大模型&nbsp;Agent 赚到钱了。</p><p>&nbsp;</p><p>“真正赚钱的是上层应用。”黎科峰博士分析道，未来，全球企业的业务经营将依赖于工具，而不是人、政策或剥削员工。最有价值的企业将是那些拥有数据、能够通过工具提升效率的企业。</p><p>&nbsp;</p><p>黎科峰博士创建数势科技之初，公司重心是构建数据资产层 ，通过打造企业指标平台和标签平台将企业海量数据转化为可使用的数据资产，释放数据价值。但产品使用门槛较高，更加适用于数字化程度较高、拥有较多数据技术人员的企业。</p><p>&nbsp;</p><p>大模型出现后，数势科技为企业用户提供数据分析AI Agent，用户可以基于自然语言对话进行交互，如此降低软件的使用门槛，管理者、业务人员等非技术人员都可以准确、即时、个性化地进行数据査询和业务洞察，提升决策能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b6cd8c55a8afa1a6299ad10995611f69.png" /></p><p></p><p>数势科技大模型 Agent 产品（SwiftAgent）架构图</p><p>&nbsp;</p><p>“数势科技的优势是使用软件和算法解决问题，而非人力。”黎科峰博士说道，“我们的人员效率很高，是因为我们用技术解决传统上需要大量人力的问题，这也是为什么我们能够实现良好的盈利。”&nbsp;</p><p>&nbsp;</p><p>那黎科峰博士一直强调的“用户价值”，如何体现在产品中的呢？</p><p>&nbsp;</p><p>他举了一个例子。在零售行业，一家茶饮连锁企业面临的挑战是如何让店长进行数据分析。由于店长的教育水平普遍低于科技或互联网公司，让他们快速、高效地利用传统BI工具做数据分析并不现实。而通过数势科技对话式、低门槛的数据分析和决策产品，店长能够通过简单的语音输入获取需要的数据，并理解数据变化背后的原因，从而更好地经营门店。如此，门店的运营不再简单依赖店长能力和经验。</p><p>&nbsp;</p><p>国内市场存在一个非常普遍且残酷的问题：资本市场状况良好时，任何一个新概念的出现都会吸引一大批公司迅速进入，结果就是甲方难以选择供应商，最终仅靠价格抉择。市场也因此变得越来越卷，好的产品和公司不得不参与价格战，最终可能出现经营困难，造成了“劣币驱逐良币”的局面。</p><p>&nbsp;</p><p>大模型市场正在历史重演。To B 企业要在如此的竞争环境中脱颖而出，黎科峰博士认为关键有两点：一要真正理解数据智能和AI，并有深厚积累，这是企业的基因；二是要回归客户需求、懂行业知识，讲能够引起客户共鸣的故事。</p><p>&nbsp;</p><p>在产品技术方面，企业最重要的是想清楚Agent与大模型的依赖关系，优化Agent产品设计，使其更加高效得运作，减少与大模型的交互次数，仅将必要的任务交由大模型处理，从而节省资源。</p><p>&nbsp;</p><p>在业务需求方面，数势科技并不是什么客户、什么行业都做，而是选择自己擅长、有很深Know-how的。这基于黎科峰博士多年大厂经验的总结。“在做技术负责人的时候，我的考核标准也不是单纯地看技术有多炫酷，而是技术能不能让企业赚钱、省钱，产品最重要的是为客户创造价值。”</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“我带着一种无畏的态度进入这个行业，我没有过往的包袱，也没有已经形成的固定模式，这让我能够更自由地探索和创新。”4年前获得腾讯亿元天使轮融资的黎科峰博士，如今依然站在To B的赛道上。</p><p>&nbsp;</p><p>虽说“好风凭借力”，但黎科峰博士清楚地表示，大模型技术虽然非常有潜力，但本质上依然是一个工具，是用来帮助企业实现业务目标和战略的。对于黎科峰博士来说，大模型是武器，核心竞争力是自己懂业务、懂技术。</p><p>&nbsp;</p><p>未来，数势科技利用大模型还能创造怎样的成绩，我们拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>栏目介绍</p><p>&nbsp;</p><p>《大模型领航者》是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p>&nbsp;</p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</id>
            <title>生生不息，一齐见证 AI 新次元 | InfoQ 中国成立 17 周年</title>
            <link>https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wABFf3KQ2fyq0XTfRsgI</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 07:07:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 作者, InfoQ, 技术资讯, 生成式AI
<br>
<br>
总结: InfoQ 是一家致力于提供中立的技术资讯和技术会议的机构，17年来一直关注技术创新和发展，特别是在生成式AI领域。他们与长城战略咨询合作，致力于推动生成式AI技术在各行业的落地，并与中国信通院合作推动人工智能技术的健康发展。通过举办活动和对谈，InfoQ努力引领技术潮流，帮助开发者和企业适应生成式AI时代的变革。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;InfoQ</p><p></p><p>2007&nbsp;年至今，InfoQ&nbsp;中国已走过&nbsp;17&nbsp;年。</p><p></p><p>17&nbsp;年里，我们置身于风云变幻的&nbsp;IT&nbsp;科技浪潮中，亲历了云端计算的兴起、人工智能的飞跃、编程语言的迭代、大数据的洪流、架构艺术的演变以及智能手机的革命，目睹了本土科技力量的茁壮成长；同时，也紧跟生成式&nbsp;AI&nbsp;等前沿技术趋势，全力促进各行业的革新与发展进程。</p><p></p><p>17&nbsp;年来，我们一直致力于提供中立的、由技术实践主导的技术资讯及技术会议，希望促进软件开发及相关领域的知识传播，助力数字时代人才的成长与蜕变，让创新技术推动社会进步。</p><p></p><p>从成立之初到今天，InfoQ&nbsp;始终坚持做技术创新浪潮中坚毅的观察者与驱动者，见证、参与每一次技术突破的航程，记录、传播那些深刻影响时代进步的科技力量与幕后英雄。迄今为止，&nbsp;InfoQ&nbsp;用数万篇高质量的文章，QCon、AICon、FCon、ArchSummit&nbsp;等数十万开发者奔赴现场的超百场技术盛会，汇聚&nbsp;1600+&nbsp;技术领导者的&nbsp;TGO&nbsp;鲲鹏会，拥抱着&nbsp;IT&nbsp;技术时代的变迁。</p><p></p><p>我们紧随技术潮流，倾力打造优质技术内容与高质量的技术学习、交流平台，并携手超过五百万技术同仁与数千家企业，在国内烙印下一代代技术人成长与突破的足迹。</p><p></p><p>如今，InfoQ&nbsp;正向着生成式AI全面进化，做&nbsp;AIGC&nbsp;时代的媒体一号位是我们当下追求的目标，也是未来发展的蓝图。</p><p></p><p>2024&nbsp;年，我们将极客邦科技发展的年度主题定为「生生不息」，希望在新的一年里，InfoQ&nbsp;迸发更强的生命力与创意火花，持续引领技术潮流，共同奔赴将生成式&nbsp;AI&nbsp;落地到千行百业的未来。</p><p></p><p></p><h2>生成式AI时代：适应、转型与超越</h2><p></p><p></p><p>站在&nbsp;17&nbsp;周年这个新起点，我们特别在6月12日策划了&nbsp;17&nbsp;周年庆系列直播活动。除明确今年向着生成式AI全面进化的大方向外，极客邦科技正积极采取多维度策略来推进这一领域的创新和发展。此次周年庆系列活动上，InfoQ&nbsp;将正式宣布公司在生成式&nbsp;AI&nbsp;布局上的两大战略计划。</p><p></p><p>为推动生成式&nbsp;AI&nbsp;技术在千行百业的落地，极客邦科技将宣布与长城战略咨询达成合作。与此同时，长城战略咨询也将发布生成式&nbsp;AI&nbsp;十大企业级通用场景报告。</p><p></p><p>此前，为推动人工智能技术的健康发展和应用，中国信通院与中国互联网协会于&nbsp;2021&nbsp;年底联合发起了“铸基计划”。这次，极客邦科技双数研究院&nbsp;InfoQ&nbsp;研究中心将与中国信通院“铸基计划”共同宣布一项重要的战略合作成果，在&nbsp;2024&nbsp;年&nbsp;8&nbsp;月&nbsp;16&nbsp;日极客邦科技举办的&nbsp;FCon&nbsp;全球金融科技大会首发《AGI&nbsp;在金融领域的应用实践洞察》报告，旨在深入探讨&nbsp;AGI&nbsp;技术在金融领域的应用现状、未来发展趋势以及所面临的挑战。未来，&nbsp;InfoQ&nbsp;研究中心也将继续关注大模型及&nbsp;AIGC&nbsp;领域的应用和产品进展。</p><p></p><p>AI的迅速发展和广泛应用已经成为推动行业变革和业务创新的重要力量。尤其是大模型等前沿技术，正在以前所未有的速度和规模，重新定义企业的运营模式、创新服务和客户体验。这种变革不仅限于技术行业本身，而且正影响到千行百业。各行各业在探索AI大模型应用过程中，既迎来全新挑战，又面临风险与机遇。为此，InfoQ&nbsp;特在周年庆活动中，精心策划了一场&nbsp;AI+行业融合圆桌对话。届时，将有来自金融、工业制造、物流等不同行业的专家到场，围绕生成式&nbsp;AI&nbsp;如何深度融入并重塑各行业展开深入探讨。</p><p></p><p>过去&nbsp;17&nbsp;年间，InfoQ&nbsp;陪伴着中国数以万计的开发者，共同经历了中国互联网、移动互联网的发展时代，见证了新一轮技术和产业变革的数字化浪潮的到来。今天，在生成式&nbsp;AI&nbsp;的蓬勃发展下，新一代的开发者们面临着比以往更多的选择与机遇。值此&nbsp;17&nbsp;周年之际，&nbsp;InfoQ&nbsp;特别策划了此期以「AI&nbsp;时代下的程序员」为主题的开发者对谈活动。届时，三位处于不同人生阶段的程序员，将一同探讨各自在生成式&nbsp;AI&nbsp;风口之下的思考与感悟。</p><p></p><p>此次对谈，不仅会是一场新、老程序员间的相互坦白局，还穿插着不少犀利问答。在此，先剧透一下部分精选提问：</p><p>是否愿意将程序员作为终身奋斗的事业？现在的工作情况是否符合此前对于程序员职业的预期？AI&nbsp;是否会导致一线程序员的失业？新一代的程序员有怎样的职业追求？程序员更看重公司提供哪些方面的激励体制？</p><p></p><p>此外，InfoQ&nbsp;在持续为技术人群提供服务的过程中注意到，现今技术社区内对于新能源汽车的相关讨论度在急剧升高。为此，InfoQ&nbsp;于今年&nbsp;5&nbsp;月面向社区用户发起了新能源汽车调研问卷，目前已经收到了近千份问卷，最终的调研结果将在这次的周年庆系列活动公布。我们还邀请了特斯拉、理想、蔚来、问界四个热门新能源汽车品牌的车主代表，亲临&nbsp;17&nbsp;周年庆活动现场，公开反馈对问卷中各个结论的看法与评价。</p><p></p><p>17&nbsp;年来，&nbsp;InfoQ&nbsp;凭借独到的专业视角、前沿的行业报道与深度的技术内容，赢得了无数开发者们及众多业内人士的认可与持续关注。感谢&nbsp;InfoQ&nbsp;的关注者们一路相伴，一路支持！</p><p></p><p>值此周年庆之际，InfoQ&nbsp;也面向广大企业客户推出了特别福利：即日起到&nbsp;2024&nbsp;年&nbsp;6&nbsp;月&nbsp;30&nbsp;日，确认合作赞助大会的企业，会议相关产品全部享受七五折。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5ade6ae5d370424d71bd36162791e376.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</id>
            <title>斯坦福AI团队“套壳”清华系开源大模型被实锤！被揭穿后全网删库跑路</title>
            <link>https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3CsQrogtkfPJ4gaEgmM8</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 06:31:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Medium, GPT-4V, 斯坦福, MiniCPM-Llama3-V 2.5
<br>
<br>
总结: 一篇关于斯坦福团队发布的新模型Llama 3-V的文章在Medium上引起了轰动，该模型比GPT-4V等更强，尺寸小100倍，训练成本仅需500美元。然而，网友发现Llama 3-V似乎套壳了清华系开源大模型MiniCPM-Llama3-V 2.5，斯坦福团队的解释遭到质疑，事件持续发酵。斯坦福团队被迫删除质疑评论，网友提出了四点证据证明Llama 3-V套壳行为，最终斯坦福AI团队被实锤，连夜删库跑路。 </div>
                        <hr>
                    
                    <p>5月29日，一个来自斯坦福的作者团队在Medium上发布了一篇名为《Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars》的文章，文章中称他们训练出了一个比GPT-4V、Gemini Ultra、Claude Opus 更强的SOTA 开源多模态模型，尺寸比GPT4-V小100 倍，训练成本仅需500美元。</p><p></p><h2>斯坦福AI团队“套壳”清华系开源大模型被实锤</h2><p></p><p></p><p>该团队成员Aksh Garg也在X（原Twitter）上发贴介绍了这一模型的特点。没过多久该帖的浏览量已超过 30 万，被转发了300多次，Llama 3-V的项目一下子冲到了 HuggingFace 首页。</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22fe03b33e18660d2ad1b37e2746f61e.png" /></p><p></p><p>随着该项目热度的持续走高，不少X和 HuggingFace 上的网友注意到，Llama 3-V总是让人有种似曾相识的感觉，好像在哪里见到过！</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38d2376f8dd321f17cbbc64481fe1514.png" /></p><p>网友们接着深扒后发现， Llama 3-V似乎，有点，好像是套壳了清华系开源大模型MiniCPM-Llama3-V 2.5。</p><p></p><p>据悉，MiniCPM-Llama3-V 2.5是由清华系AI公司面壁智能推出并开源的MiniCPM 系列最新的端侧多模态模型，总参数量为 8B，支持 30+ 种语言，多模态综合性能超越 GPT-4V-1106、Gemini Pro、Claude 3、Qwen-VL-Max 等商用闭源模型，OCR 能力及指令跟随能力得到进一步提升，可精准识别难图、长图、长文本。</p><p></p><p>面对网友们的质疑，斯坦福这支AI团队也坐不住了，他们表示只是使用了 MiniCPM-Llama3-V 2.5 的tokenizer，并且宣称在 MiniCPM-Llama3-V 2.5 发布前就开始了这项工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81ee9d7cc314d51baad875a7dd9255c8.png" /></p><p></p><p>但他们的解释再次遭到了质疑。</p><p></p><p>通常情况下，一款模型及其详细的tokenizer往往是在其发布后才能被外人知晓，那么斯坦福这支AI团队如何能在MiniCPM-Llama3-V 2.5发布之前就获取到这些信息？</p><p></p><p>这件事持续在网上发酵。</p><p></p><p>6月2日，不死心的网友在Llama3-V的 GitHub Issue上发布质疑，或许是因为心虚，该条质疑的评论很快就被Llama3-V团队删除。</p><p></p><p>幸运的是，发布质疑的网友早已机智地提前截图保存了自己在GitHub Issue上发布的内容。</p><p></p><p>这名网友列举了在他看来Llama3-V“套壳” MiniCPM-Llama3-V 2.5的四点证据：</p><p></p><p>证据一：模型结构和代码几乎是双胞胎兄弟。</p><p></p><p>比如，套壳的 Llama3-V 与MiniCPM-Llama3-V 2.5几乎“共用”了完全相同的模型结构和代码。Llama3-V的模型结构和配置文件与MiniCPM-Llama3-V 2.5完全相同，只是变量名不同。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b17eebf7018881bc6a38da001d5835bf.png" /></p><p></p><p>左图：MiniCPM-Llama3-V 2.5                            右图：Llama3-V</p><p></p><p>证据二：Llama3-V的代码似乎就是MiniCPM-Llama3-V 2.5的代码。更令人震惊的是，Llama3-V仅仅只是进行了一些重新格式化并把一些变量重新做了命名，比如图像切片、分词器、重采样器、数据加载等变量，下图是一些示例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5d29d816945e3409cfd9846b1f99470.png" /></p><p></p><p>证据三：Llama3-V的作者表示他们“引用了LLaVA-UHD作为架构”，还列出了差异点（关于ViT和LLM的选择）。但是他们并没有提到，这个项目的具体实现与MiniCPM-Llama3-V 2.5 极其相似，却在空间模式等许多方面与LLaVA-UHD有非常多的差异。Llama3-V也具有与MiniCPM-Llama3V 2.5相同的分词器，包括MiniCPM-Llama3-V 2.5新定义的特殊符号。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a23662d9623f271b1cefd8299d5e33c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/45/458913d4dfbd98e676cf324b94e90078.png" /></p><p>证据四： 最初Llama3-V的作者在上传代码时直接导入了 MiniCPM-V 的代码，然后将名称更改为 Llama3-V。</p><p></p><p>https://huggingface.co/mustafaaljadery/llama3v/commit/3bee89259ecac051d5c3e58ab619e3fafef20ea6</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/934c15b6a4a165f8ec409a859a567991.png" /></p><p></p><h2>面壁智能团队下场实锤，斯坦福AI团队连夜删库跑路</h2><p></p><p></p><p>在屡遭质疑后，斯坦福AI团队已经被逼到了不回应实在说不过去的地步了，有网友开脸贴大该项目的作者，“你们有没有勇气面对事实”？</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7cccb06d67ca9d15c4b3601ebc00bdd.png" /></p><p></p><p>这种情况下，该团队成员不得不地对网友关注的问题进行了回复。该项目中的一位作者表示：</p><p></p><p></p><blockquote>“你们的说我们抄袭简直是没影儿的事儿。Llama3-V推理存在bug，而MiniCPM的配置可以有效解决该问题，这就是为什么我们使用了相同的配置。此外，我已经指出了架构是相似的，但MiniCPM的架构来自Idéfics。SigLIP也来自Idéfics。我们遵循Idéfics论文中的那些内容。LLava UHD来自他们的实验室，我也已经指出了这一点。此外，我还强调了更多内容，即它是相同的架构，但该架构是基于综合研究的，你怎么能说它是MiniCPM呢？MiniCPM的代码，看起来，视觉部分的也是从Idéfics那里使用的。”</blockquote><p></p><p></p><p>不少网友还注意到，Llama3-V 在 MiniCPM-Llama3-V 2.5 项目发布之前就已经使用了 MiniCPM-Llama3-V 2.5 的tokenizer 。有一些用户在 Twitter 和 HuggingFace 上指出以上问题后，Llama3-V 的作者表示他们只是使用了 MiniCPM-Llama3-V 2.5 的分词器（tokenizer）。他们还声称在 MiniCPM-Llama3-V 2.5 发布之前就开始了Llama3-V 的工作。但令人无法解释的是，他们如何能在MiniCPM-Llama3-V 2.5发布之前就获取到其详细的分词器？</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f121ad805bec2bdf1e8c39ba36446fdc.png" /></p><p></p><p>对此，Llama3-V 项目作者反驳说从已经发布的上一代 MinicPM-V-2 项目里拿的标记器。但实际上，有网友留意到，MiniCPM-V-2的 tokenizer 与 MinicPM-Llama3-V2.5 完全不同，在Huggingface 里是两个文件。既不是同一个tokenizer件，文件大小也完全不同。MinicPM-Llama3-v2.5的 tokenizer 是 Llama3 的 tokenizer 加上 MiniCPM-V 系列模型的一些特殊 token 组成，MiniCPM-v2因为在Llama 3开源之前就发布的，不会有Llama 3 的分词器。</p><p></p><p>Llama3-V团队屡遭质疑却始终咬死不认的态度，惹怒了面壁智能MiniCPM-Llama3-V 2.5团队的研究人员们。</p><p></p><p>6月3日，面壁智能向AI前线列举了一些Llama3-V团队抄袭的“实锤”。</p><p></p><p>面壁智能认为，Llama3-V 项目的作者似乎并不完全理解 MiniCPM-Llama3-V 2.5 的架构，甚至也不理解他们自己的代码。</p><p></p><p>如下图 Llama3-V 的技术博客和代码显示， Llama3-V 的作者似乎没有完全理解 MiniCPM-Llama3-V 2.5 的架构，甚至也不懂他们"自己"（假若真是他们所写）的代码。</p><p></p><p>感知器重采样器（Perceiver resampler）是单层cross-attention，而不是双层self-attention。但是下图所示Llama3-V 的技术博客里作者的理解很明显是错的。另外SigLIP 的 Sigmoid 激活也不用于训练多模态大语言模型，而仅用于预训练 SigLIP。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a75f9e9de53acf5d793df56cae1ed45.png" /></p><p></p><p>截图来源：Llama3-V 的技术博客</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/51/512a694de9e4328876ed049242ea6623.png" /></p><p></p><p>截图来源：Llama3-V 的代码</p><p></p><p>面壁职能团队还表示：“另外视觉特征提取不需要 Sigmoid 激活，但下图所示Llama3-V 的技术博客里作者的理解是错的，但代码其实是正确的，这说明作者压根不理解自己的代码”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/42eeb98c7de98601c9aa8ebf7dadccdd.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d12f22ad32d7194fe7a00eefadc55d90.png" /></p><p></p><p>此外，Llama3-V相当于MiniCPM-Llama3-V 2.5的加噪声版本。</p><p></p><p>据网友反馈，当运行 Llama3-V 时，作者提供的代码无法与 HuggingFace 上的 checkpoint 配合使用。</p><p></p><p>然而令人啼笑皆非的是，当把 Llama3-V 模型权重中的变量名更改为 MiniCPM-Llama3-V 2.5 的名称后，模型可以成功运行 MiniCPM-V 的代码。这一下子帮忙解决了困扰 Llama3-V作者一周的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/6054c03b78fc21f2eb5ad2a12978f68a.png" /></p><p></p><p>如果在 MiniCPM-Llama3-V 2.5 的 checkpoint 上添加一个简单的高斯噪声（由一个标量参数化），你会预期得到什么结果？</p><p>new_dict = {}</p><p>for k, v in model.state_dict().items():</p><p>torch.cuda.manual_seed_all(42)</p><p>new_dict[k] = v + torch.randn_like(v) / 708</p><p>model.load_state_dict(new_dict)</p><p></p><p>结果是会得到一个行为与 Llama3-V 极为相似的模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1cda987e18bed0bfb1064a003ea9f6c.png" /></p><p></p><p>然而，这些还不够。更更更炸裂的是，Llama3-V 连清华团队内部并未对外公开的私有数据都能拿到？？？</p><p></p><p>据面壁智能内部团队透露，Llama3-V 大模型居然能识别清华简，OCR表现对比也很惊人，这些清华大学内部的私有数据他们又是如何拿到的呢？</p><p></p><p>MiniCPM-Llama3-V 2.5 的一个实验性功能是能够识别清华简，这是一种非常特殊且罕见的中国战国时期（公元前475年至公元前221年）写在竹简上的古文字。这些训练数据的采集和标注均有由清华NLP实验室和面壁智能团队完成，相关数据尚未对外公开。经过专有数据训练后，MiniCPM-Llama3-V 2.5 能够初步识别清华简的文字，甚至连犯的错误都一样。</p><p></p><p>然而令人惊讶的是，不可能获得专有数据训练的 Llama3-V 竟然也具有相同的能力！</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/67627805e2f4889d976c1944a65d8de5.png" /></p><p></p><p>下图展示了 Llama3-V 在识别清华简的文字时，其结果和MiniCPM-Llama3-V 2.5一致。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdacef044c8278ebed1f58329d1b8c52.png" /></p><p></p><p>有趣的是，Llama3-V 在清华简识别犯错时竟然也和 MiniCPM-Llama3-V 2.5一模一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/40/406fcf41ab5f31c53e2b59bd7e67bb1d.png" /></p><p></p><p>为谨慎起见，作者在 1000 张竹简图像上测试了几种基于 Llama3 的视觉-语言模型，并比较了每对模型的预测精确匹配。</p><p></p><p>结果显示，正常情况下每两个模型之间的重叠为零，而 Llama3-V 和 MiniCPM-Llama3-V 2.5 之间的重叠竟然达到了惊人的 87%！此外，MiniCPM-Llama3-V 2.5 和 Llama3-V 甚至在错误分布上也非常相似。Llama3-V 和 MiniCPM-Llama3-V 2.5 分别有 236 和 194 个错误预测，而其中重叠部分高达 182 个。</p><p></p><p>另外，这个高斯噪声对竹简字符也是有效的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14fc6f40c674a41da3b28123999a477b.png" /></p><p></p><p>与 MiniCPM-Llama3-V 2.5 类似，Llama3-V 也显示出了很强的 OCR 能力，甚至包括中文。</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/90072ebd0f7e0c45881bb9792ef0b11a.png" /></p><p></p><p>Prompt：请输出图片中的文字</p><p></p><p>MiniCPM-Llama3-V 2.5</p><p><code lang="text">### 标题
9倍像素更清晰

### 副标题
难图长图长文本精准识别

### 正文
OCR技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V2.5对于难图、长图、长文本的精准识别，再度带来出众表现！面壁自研高清图像高效编码技术，我们可以高效编码及无损识别180万高清像素图片，并且支持任意长宽比、甚至「有点变态」的1:9极限比例图像，突破了传统技术仅能识别20万像素小图的瓶颈。此前，MiniCPM-V系列多模态模型就因对于街景、长图等困难场景的高效解析，赢得了良好口碑。

### 图片描述
1. **180万像素**
   - 任意长宽比
   - 图像无损识别
   - 可支持
   - 1:9极限宽高比</code></p><p>Llama 3-V</p><p></p><p><code lang="text">### 标题
9倍像素更清晰

### 副标题
难图长图文本精准识别

### 正文
OCR技术进一步打磨，复杂推理与多模态识别能力再进化，MiniCPM-Llama3-V2.5对于难图、长文本的精准识别，再度带来出众表现。面壁自研高清图像高效编码技术，我们可以高效编码及无损识别180万高清像素图片，并且支持任意长宽比、甚至「有点变态」的1:9极限比例图像，突破了传统技术仅能识别20万像素小图的瓶颈。此前，MiniCPM-V系列多模态模型就因对于街景、长图等困难场景的高效解析，赢得了良好口碑。

### 图片描述
- **180万像素**：任意长宽比，图像无损识别，可支持。
- **1:9极限宽高比**：可支持。</code></p><p></p><p>同样的事情也发生在内部的 WebAgent 数据上，这是一个已经整合但尚未发布的功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ebe7617cac4cbab5f0d5c68ff26825c.png" /></p><p></p><p><code lang="text">Q:
Actions History
click, input, click
Your Task
Can you give me a recipe for French butter cake?
Generate next actions to do this task.

minicpmv:
actions:
click,32 273 477 508
click,32 273 477 508

llama3v:
actions:
click,32 273 477 508
click,32 273 477 508</code></p><p></p><p><img src="https://static001.geekbang.org/infoq/31/312dd56311e0b9be740ccedf88ea9314.png" /></p><p></p><p><code lang="text">Q:
Your Task
有没有关于《黑子的篮球》的新剧场版的消息？
Generate next actions to do this task.

minicpmv:
actions:
hover,732 292 792 328

llama3v:
actions:
hover,715 292 802 328</code></p><p></p><p></p><blockquote>Github开源：<a href="https://github.com/mustafaaljadery/llama3v">https://github.com/mustafaaljadery/llama3v</a>"（已删库）HuggingFace开源：<a href="https://huggingface.co/mustafaaljadery/llama3v">https://huggingface.co/mustafaaljadery/llama3v</a>"（已删库）Medium发布文章：<a href="https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee">https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee</a>"Twitter官宣模型：<a href="https://twitter.com/AkshGarg03/status/1795545445516931355">https://twitter.com/AkshGarg03/status/1795545445516931355</a>"（已删除）</blockquote><p></p><p></p><p>事情发酵至此，就在网友们都等着斯坦福AI团队再次发文力证清白时，AI前线留意到，该团队成员似乎集体“闭麦”，并且已经删除了他们在X上官宣模型的推文，连带着该项目在Github和HuggingFace上的库也已经删干净了。</p><p></p><p>而今天，面壁智能联合创始人&amp;CEO李大海也在朋友圈发文对此事进行了最新回应，李大海对此事表示遗憾。他写道：</p><p></p><p></p><blockquote>“经过团队核实，除了社区网友列出的证据外，我们还发现 Llama3v 展现出和小钢炮一样的清华简识别能力，连做错的样例都一模一样，而这一训练数据尚未对外公开。这项工作是团队同学耗时数个月，从卷帙浩繁的清华简中一个字一个字扫描下来，并逐一进行数据标注，融合进模型中的。更加 tricky 的是，两个模型在高斯扰动验证后，在正确和错误表现方面都高度相似。技术创新不易，每一项工作都是团队夜以继日的奋斗结果，也是以有限算力对全世界技术进步与创新发展作出的真诚奉献。我们希望团队的好工作被更多人关注与认可，但不是以这种方式。我们对这件事深表遗憾！一方面感慨这也是一种受到国际团队认可的方式，另一方面也呼吁大家共建开放、合作、有信任的社区环境。一起加油合作，让世界因 AGI的到来变得更好!”</blockquote><p></p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/99/99daea9d24cdf221bbe0c7aefedfb388.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</id>
            <title>从Data Infra“卷”到AI Infra！星环科技推出知识平台TKH，全面统筹算力、语料、模型与应用</title>
            <link>https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0xFQRzZ2xS3YxZlEESes</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 06:21:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 未来数据技术峰会, 人工智能, 大模型技术, AI基础设施
<br>
<br>
总结: 2024年5月31日在上海举办的未来数据技术峰会聚焦人工智能和大数据等热门话题，探讨了大模型技术在企业生产中的应用，以及企业打造自己的AI基础设施的重要性。企业需建立自己的AI基础设施，打造行业大模型，提升生产力和产业升级。星环科技推出的AI Infra工具支持企业快速建立行业大模型，实现AI应用的快速落地。通过知识平台TKH，企业可以构建安全可控的AI基础设施，帮助企业快速使用AIGC。 </div>
                        <hr>
                    
                    <p>5月31日2024向星力·未来数据技术峰会在上海隆重举办。峰会聚焦人工智能、大数据、数据要素、数字化转型、新质生产力等热门话题，业内知名专家、学者和企业代表分享最新研究成果、技术创新和实践经验，高层次产、学、研、用四方的交流和思想碰撞，为企业数据化转型和AIGC应用提供新思路、新方向和新方法。</p><p>&nbsp;</p><p>星环科技创始人、CEO孙元浩指出，大模型技术已快速融入企业生产经营的方方面面，在企业生产效率提升、业务职能提效和行业应用场景创新中无处不在，不但重构产业，打造企业的核心竞争力，而且成为企业的基础设施和核心能力，助力企业打造新质生产力。</p><p></p><h2>从Data Infra到AI Infra，为企业打造自己的AI基础设施</h2><p></p><p>&nbsp;</p><p>人工智能大模型正在催生新一轮技术创新与产业变革，也将为金融、制造、交通、政务等众多行业企业数字化转型和高质量发展带来新的动能。当前市场以通用基础大模型为主，通识能力强，但缺少行业专业知识。将大模型融入千行百业，让企业的AI应用从早期直接调用通用大模型，发展到建立自己的AI基础设施，打造行业或特定领域、任务的专用大模型，助力生产力革新和产业升级，已经成为目前企业关注的核心。</p><p>&nbsp;</p><p>星环科技创始人、CEO孙元浩宣布，星环科技的定位从Data Infra进一步延伸到AI Infra，为企业客户打造AI基础设施，打造从语料处理、模型训练、知识库建设等的一整套的工具链，帮助企业快速建立行业大模型，快速使用AIGC。</p><p>&nbsp;</p><p>从需求上看，目前，通用大模型数量快速增加，并不断升级迭代、提质增效，但是却不能适应企业AI应用的需求。而另一方面，企业对行业大模型需求增加，迫切需要降低使用门槛，更加普惠地使用大模型、生成式AI技术，以解决企业内部人才、算力、数据等不足的挑战。将AGI限制在一个特定领域或者一个行业或者特定任务，不断提升模型准确度，成为一条切实可行的发展途径。</p><p>&nbsp;</p><p>在技术上，生成式AI出现后，深度学习出现了一个重大变化，从以前的可能几千万、上亿的样本数量的大样本机器学习，演变成现在小样本机器学习，让行业大模型应用范围变得更很广，让模型能够思考、学习，能够理解自然语言，能够完成更多的工作。</p><p>&nbsp;</p><p>每个国家都需要主权AI，需要建立自己的AI基础设施，打造自己的AI模型。企业同样需要自身的AI Infra，打造行业大模型，赋能企业更快、更容易地使用大模型，释放数据价值。</p><p>&nbsp;</p><p>针对企业的AI Infra建设，星环科技通过自主研发，可以向用户提供一站式企业级大模型生产及应用全流程开发工具链，让大模型快速落地。星环科技拥有从语料到模型再到应用的完整的 AI Infra工具集，覆盖语料开发和管理、大模型训练与持续提升、多模态知识工程、多模知识存储与服务、原生AI应用构建编排和应用服务等重要阶段，提供提示词工程、检索增强、智能体构建等大模型应用快速构建和提升、模型推理优化、模型安全和持续提升技术。</p><p>&nbsp;</p><p>通过星环科技AI Infra工具，企业能够准确、高效地将拥有的多种来源的多模语料转换为高质量的专业领域知识，并且源源不断地支撑专业知识库问答、业财数据分析、智能投研、设备预测性维护等丰富的使用场景和应用，让企业构筑知识壁垒，实现“人工智能+”业务的落地和创新。</p><p>&nbsp;</p><p>星环科技AI Infra工具支持异构算力、语料、知识、大模型应用的统一管理，为数据和语料资产的集约化提供了一站式平台，且具备企业级的组织空间管理能力。</p><p>&nbsp;</p><p>未来，星环科技通过自主研发，不断完善AI从基础设施到应用的产业链条，可以为客户提供端到端的全套AI解决方案。</p><p></p><h2>推出星环知识平台TKH，为企业高效构建AI Infra</h2><p></p><p>&nbsp;</p><p>星环科技整合大数据、人工智能等技术，推出知识平台Transwarp Knowledge Hub（TKH），通过全面统管企业Al Infra算力、语料、模型和应用，为企业打造安全可控AI Infra，帮助企业快速使用AIGC。</p><p>&nbsp;</p><p>星环的知识平台TKH包括知识存储与服务、语料开发与知识构建、大模型基础服务等几个重要部分。</p><p>&nbsp;</p><p>第一，TDH for AI，打造新一代一站式多模型数字底座。</p><p>&nbsp;</p><p>星环知识平台TKH提供企业级多模态知识存储与服务，帮助企业打造新一代一站式多模型数字底座。基于Transwarp Data Hub for LLM知识管理平台的多模型统一技术架构，支持关系型数据、向量数据、全文检索、图数据、时序数据等的统一存储管理，满足各类场景下多模态数据的统一存储管理与服务，大幅简化知识库的知识存储与服务层架构，降低开发与运维成本。</p><p>&nbsp;</p><p>星环科技新推出的产品可以让企业的数据底座实现四个一体化：湖仓集一体化、多模型处理一体化、历史数据与实时数据处理一体化、本地集群和云平台一体化。</p><p>&nbsp;</p><p>其中，大数据基础平台TDH 9.4的多模基座加速AI分析；仓集资源隔离架构，在混合业务场景下依然保持极致性能；湖仓一体架构，大幅降低TCO；同份数据跑批查询与混合负载，实时备份强在线业务容灾；同时支持多模型存诸架构、大模型海量训练数据存储、多模型混合检索召回增强、Python生态等。</p><p>&nbsp;</p><p>分布式向量数据库Hippo支持文本、图片、音视频等转化后的百亿级向量数据的存储、索引和管理，支持多种索引，具有全文检索+向量检索以及稀疏向量+稠密向量的混合检索等能力。Hippo 2.0可以实现百亿级向量存储，提供灵活索引支持、20倍内存成本下降和向量全文混合检索等特性。</p><p>&nbsp;</p><p>分布式图数据库StellarDB提供万亿级图数据存储、毫秒级点边查询和10+层深度链路分析等能力，支持丰富的图算法和图机器学习，创新的动态时序图能力更便捷地挖掘数据变化规律和预测分析。而新推出的StellarDB 5.1实现了向量/全文模糊检索、秒级子图匹配、跨集群数据灾备、RAG增强大模型、GPU算法加速等功能，更稳定、更安全、更易用。</p><p>&nbsp;</p><p>星环分布式分析型数据库ArgoDB支持标准SQL语法，提供多模分析、实时数据处理、联邦计算、隐私计算、数据脱敏等能力，一站式满足OLAP分析，实时数仓、数据集市、湖仓集一体等场景。</p><p>&nbsp;</p><p>而新推出的ArgoDB 6.1 版本以“增量数据实时处理”技术为基础，定义并发布“实时数据加工”的智能高效新范式；结合集群级实时同步与数据海量版本能力，协助用户够构建高可靠的实时可信大集群，以数据透明加密、SQL审核/阻断等安全技术手段为辅，共建“快好省”湖仓集一体的融合数据处理架构。</p><p>&nbsp;</p><p>星环分布式时序数据库Timelyre支持海量时序数据库的存储与处理，具备每秒千万级数据吞吐、5~20倍无损压缩和毫秒级检索能力，支持Python、C++等API，易用的时序分析框架满足金融智能投研需求。TimeLyre 9.2新增了多模型时序分析、极速分布式回测平台、投研数据中台、时序数据湖引擎等，助力用户解锁数据深层价值。</p><p>&nbsp;</p><p>星环分布式文件系统TDFS支持10亿级以上的大小文件的存储，并同时支持对象存储，基于Raft保障强一致，支持HDFS平滑迁移，标准POSIX协议支持上层知识等AI场景应用无感对接。</p><p>&nbsp;</p><p>第二，语料加工工具与图谱构建工具，助力企业建立高质量模型及应用。</p><p>&nbsp;</p><p>决定行业大模型质量最关键的因素就是语料，语料的质量决定了模型的质量。同时高质量语料也是解决行业大模型“幻觉”、“可信可控”等核心落地难点的重要手段。另外，高质量行业专用语料是企业、机构独特的竞争优势和天然壁垒。</p><p>&nbsp;</p><p>星环科技发布了一站式多场景语料平台Transwarp Corpous Sudio（TCS），覆盖了语料获取、清洗、加工、治理、应用和管理的全生命周期，具有多种灵活的采集和构建方式，能分布式的高效处理海量语料。TCS支持20+主流文档格式、数据化学公式、复杂语料处理、语料自动标注及筛选、多视角体系化资产编目和数据治理等</p><p>&nbsp;</p><p>星环TCS拥有全面、多维、精细、增强、资产等5大优势，是一个功能全面、易用高效、安全可靠的语料开发利器，能够极大提升语料开发效率，助力企业或机构高质量地构建大模型及其应用。</p><p>&nbsp;</p><p>企业知识库的建设，让数据可以用自然语言方式进行对话和检索，企业可以集中式地管理和利用知识资源，提高运营效率和创新能力。知识库建设变成企业的基础设施，所有的不同类型的数据都能进行存储与管理，能根据需要导入到知识库中。只要企业保有自己的知识库，就可以通过微调得到企业专属的大模型，就可以实现大模型可以随时选、随时换，而企业核心竞争力得到保护的目标。</p><p>&nbsp;</p><p>企业用户利用TKH提供的星环图谱构建工具Transwarp Knowledge Studio for LLM，可以将企业内部数据、个人经验数据和公开信息数据转化为知识，让数据平台更加智能化，同时可以将AIPC端和云端资源联动，确保数据安全性。个人经验数据的知识转化和不断的模型微调让知识库建设更具个性化，真正实现个性化、专家级大模型应用。</p><p>&nbsp;</p><p>TKS是一套全流程、端到端的知识图谱构建工具集，涵盖了知识模型定义、多源异构数据接入、概念与物理数据映射、多元化知识的抽取融合、全自动知识构建、图谱综合查询等功能，能够帮助政务、工业、能源等多领域客户高效构建领域知识体系，并提供智能应用的场景定制化和一站式解决方案。</p><p>&nbsp;</p><p>第三，大模型基础服务Infinity和LLMOps，让大模型快速落地。</p><p>&nbsp;</p><p>在行业大模型发展中，企业面临技术复杂、数据和算力稀缺、管理成本高等挑战。现在企业已经意识到必须建立自己的AI基础设施，能够自己对模型进行预训练、微调等。</p><p>&nbsp;</p><p>星环科技推出大模型运营平台Transwarp&nbsp;Sophon LLMOps，提供一站式企业级大模型生产及应用全流程开发工具链，助力企业完成从预训练到微调，到强化学习，到持续模型评估的全生命周期，让每个企业都能构建自己的专属大模型。</p><p>&nbsp;</p><p>可以说，Sophon LLMOps一个平台可以解决企业在大模型时代语料、模型、应用三类资产的持续积累和加速迭代。</p><p>&nbsp;</p><p>星环科技自主研发的无涯大模型Infinity具备自主可控特性，确保数据安全的同时，通过0-1预训练，可为各行业量身定制自有大模型，提供强大的意图理解、语义召回、数据处理和分析能力。</p><p>&nbsp;</p><p>基于无涯大模型底座，星环科技微调了三款垂类大模型，包括问答大模型、数据分析（代码生成）大模型和多模态大模型，以应对内容生成、数据分析图片及音视频理解及检索等多样的使用场景。</p><p>&nbsp;</p><p>星环无涯大模型Infinity提供了灵活的部署模式，包括私有化部署（AIPC版、企业版）、公有云服务等。</p><p>&nbsp;</p><p>Infinity提升大模型数据分析能力，在语法正确性、数据库方言、语义正确性等方面有重要突破。</p><p>&nbsp;</p><p>星环无涯大模型Infinity拥有众多优势，如精准问答能力、减少大模型幻觉；多模数据来源，提升回答丰富度；构建自有知识库，确保企业数据安全等。其主要功能包括智能问答、文档问答、智能写作等。</p><p></p><h2>推出无涯·问知&nbsp;AI原生应用，服务多个垂直应用场景</h2><p></p><p>&nbsp;</p><p>基于无涯大模型，星环知识平台TKH打造了无涯·问知、无涯·问数、无涯·金融、无涯·工程等AI原生应用，可广泛应用于金融、能源、制造、工程等多个领域，通过精准的数据分析和知识管理，满足企业不同类型的知识应用需求，提升企业业务效率和竞争力。</p><p>&nbsp;</p><p>本次峰会上，星环科技最新发布了无涯·问知Inﬁnity Intelligence。无涯·问知是一款基于星环科技大模型底座，结合个人知识库、企业知识库、法律法规、财经等多种知识源的企业级垂直领域问答产品。</p><p>&nbsp;</p><p>无涯·问知充分利用了星环科技自研大模型底座的自动化知识工程特性，使其在处理和分析数据方面具有显著的优势，允许用户上传文档、表格、图片等多源数据，并支持与外部数据源的对接，使用户能够构建属于自己的专属领域大模型。这一创新功能极大地扩展了模型的应用范围和深度，用户可基于自身私域知识库进行更为个性化和深入的数据分析。</p><p>&nbsp;</p><p>无涯·问知支持不限长度的音视频图文等多模态数据快速入库，且支持自动化文档切片及向量化处理，配合自研的RAG框架，可实现知识的精准召回，可用于市场研究分析、企业供应链分析、法律风险预警、智能写作等丰富的业务场景中。</p><p>&nbsp;</p><p>无涯·问知包括四大应用场景：企业可以基于星环知识库TKH，建立企业自己的知识库应用；当企业算力不足时，可以采用安装了天涯·问知的AIPC，在本地直接访问天涯·问知，以弥补AI算力不足问题；中小企业用户不用自己构建知识库，可以直接利用星环科技无涯·问知公有云服务；对于个人而言，可以利用AIPC或者公有云服务，访问天涯·问知服务。</p><p>&nbsp;</p><p>无涯·问知AIPC版在实际应用中展现出了五大显著特性，提供了本地化的向量库；支持多种格式、不限长度的文件资料入库，满足了用户多样化的需求；支持影、音、图、文等多模态数据和资料的“知识化”处理，以及相应的“语义化”查询和应用能；自研的RAG模块，实现精准问答；具备出色的数据分析能力，能够对数量化的数据进行精准的分析和研判。</p><p>&nbsp;</p><p>另外，无涯·问数是基于星环数据分析大模型，并结合数据分析主体、指标、标签设计、数据开发和治理，形成了从自然语言转数据查询语言，并返回数据表或数据图表的完整流程。</p><p>&nbsp;</p><p>在应用场景上，无涯·问数提供分析仪表盘和智能问数能力，让决策者/管理者以自然语言提问快速自助获取目标数据；预定义指标计算口径，依托数据分析大模型理解用户的分析意图，让数据分析人员实现对话即分析；通过页面配置的方式快速完成数据准备，让数据开发人员，快速整合多种数据。</p><p></p><h2>Data Infra持续深化，星环系列产品推陈出新</h2><p></p><p>&nbsp;</p><p>星环科技在推出全新的AI Infra的同时，不断完善Data Infra产品与服务。</p><p>&nbsp;</p><p>星环大数据云平台推出TDC 5.0，将原来的多个TDH集群统一纳管，统管多个TDH集群，形成物理上分散、逻辑上统一的企业级一体化大数据平台。TDC 5.0具有独特的优势，包括多集群及其基础设施、多数据应用实例统一管理；跨多集群统一调度资源，均衡多个集群资源使用；跨集群共享存储组件，实现NoCopy的跨集群数据共享；隔离和控制资源配额，快速、灵活的为不同业务部门提供多租户的PaaS服务。</p><p>&nbsp;</p><p>星环科技推出了分布式交易型数据库KunDB 4.0，高可用能力与Oracle兼容性提升，支持跨系统多租户部署。其中，深度兼容Oracle，高度兼容Oracle对象与语法，支持数据快速迁移；高可靠，基于Paxos协议的异地容灾能力，增强数据安全性保障；数据库多租户，支持多个应用共享一个数据库实例，能快速由集中式扩展成为分布式。</p><p>&nbsp;</p><p>星环大数据开发工具TDS 4.0，增加了数据实时同步、数据入湖向导、智能化数据资产盘点、数据资产门户、数据服务编排等功能。</p><p>&nbsp;</p><p>星环大数据安全与隐私保护工具软件Transwarp Defensor是星环科技自主研发的大数据安全与隐私保护安全管理平台，致力于帮助企业建设以数据为中心的数据安全防护体系，包括了解内部敏感数据分布情况，帮助管理者发现潜在风险，监管重要数据的合规合理使用等。Transwarp Defensor 提供数据分类分级管理、数据脱敏、个人信息去标识化、数据访问控制、敏感资产风险评估等基础能力，能够做到事前发现，事中防护，事后溯源，帮助企业有效建立数据安全防护体系。Transwarp Defensor 4.5，增加了大模型核心资产识别、数据资产流转链路监控、安全策略智能推荐，安全风险预警与应急响应。</p><p>&nbsp;</p><p>星环数据要素流通平台Transwarp Navier通过提供隐私计算环境，使得数据供需双方可以进行安全的数据交易。而Transwarp Navier 3.1则新增了全链路智能合约确保安全合规、数据流通全链路行为监控与分析、实时告警与阻断等。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</id>
            <title>527 蚂蚁技术日 | 蚂蚁集团 CTO 何征宇答记者问，精彩回答十连！</title>
            <link>https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4bIxeqeawdDX2e5wpPAM</guid>
            <pubDate></pubDate>
            <updated>Mon, 03 Jun 2024 01:56:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁技术日, AI生活助理, 隐私计算, AI编程
<br>
<br>
总结: 蚂蚁集团每年举办技术日活动，展示AI创新应用产品，强调AI生活助理和隐私计算的重要性，推行AI编程以提高效率和改变软件开发模式。 </div>
                        <hr>
                    
                    <p>每年的 5 月 27 日是蚂蚁集团的技术日，意在勉励蚂蚁技术人保持敬畏和创新之心。至今年，技术日已发展为技术周，一场涵盖技术论坛、技术集市、编程大赛、技术沙龙等活动的技术嘉年华。</p><p>&nbsp;</p><p>5 月 28 日，第九届蚂蚁技术日对外开放，开放日上展示了诸多蚂蚁AI创新应用产品，向外界透传了“让AI像扫码支付一样便利每个人的生活”的技术主张和面貌。</p><p>&nbsp;</p><p>在技术日第一天上午的 CTO 面对面环节，蚂蚁集团 CTO 何征宇回答了几十家媒体的提问，以下是精华内容整理：</p><p></p><p>Q1：刚才您提到“人工智能生活助理”。以后生活助理界面有没有可能更细化，比如说旅行助理、美食助理？把所有的助理放在一起，要选、要点、要对话的轮次会很多。</p><p>&nbsp;</p><p>A：我们本质上就是有 N 多个小助手，但是我们认为这些助手不应是让你来选，而是让生活管家就能够理解你的意图。比如说，当我说我要去南京，我想吃点好吃的，它瞬间就是两个助手的化身，它可以帮你安排行程和吃饭的地方。所以我同意你的观点，应该是若干个助手，最终让你的生活更美好，所以我们姑且把它叫做生活管家，而且支付宝我们认为就是这么一个事情。</p><p></p><p></p><p>Q2：从去年开始，国家数据局成立，到今年，数据局已经在很多地方落地，隐私计算在这个过程中有没有发挥作用？</p><p></p><p>A：首先我想讲，国家数据局的数字中国公布了一些案例，我们是其中之一，是我们跟农业农村部合作做的农户的小额贷。我认为国家数据局讲的战略，或者数据基础设施是一个非常宏大和庞大的体系。蚂蚁也就聚焦于最关键的，我们认为最难的地方，对我们来说就是隐私计算。</p><p>&nbsp;</p><p>隐私计算不是解决确权的问题，在我们的愿景中，我们认为它就是一个管道技术，我做的这个管道比别人更便宜，质量更好——不是一个石头做的，而是一个钛合金做的管子，非常安全，不用担心被泄露，又能非常快速的能把这个数据给传过去。其实我用这个比方是在讲，我们隐私计算就是这么一个技术，但是你要是真的要从确权、流转等等到最终的消费，这是一个非常庞大的技术体系，我觉得应该是全社会、全行业都来参与这个建设的。</p><p>&nbsp;</p><p>蚂蚁在隐私计算方面的投入相对国内来说相对早，然后我们自己有使用的场景。简单讲，因为炼油的技术高，所以油要得多的话，我们需要有很好的管道技术，我们今天愿意把管道技术开放给社会，以商业化和开源的方式开放给社会，能够共建数据流转或者数据基础设施这些东西。</p><p>&nbsp;</p><p></p><p>Q3：您刚刚提到蚂蚁内部现在全面推行 AI 编程，每周差不多超 5 成程序员在使用 CodeFuse。在这个过程中软件的开发模式相交之前是不是发生了一些变化？如果说程序员能够从繁琐、枯燥的工作中释放出来的话，那市场上对程序员的需求是不是发生变化。以前有一些采访对象跟我说过，未来程序员会和产品经理进行合并，你觉得会有这样的趋势吗？会有人才焦虑吗？</p><p>&nbsp;</p><p>A：第一点，我不觉得写代码是一个很枯燥的事情。我不知道在座有多少人写过代码，我现在不写代码了，但是我几年前写代码的时候，我是觉得对于程序员、至少对我来说是可以写到脑嗨的那种状态的，远比我现在的工作回馈的环路要短。我每天把这个代码交上去了就很开心，跟玩乐高的感觉差不多，所以我不觉得写代码是一个很枯燥的工作。</p><p>&nbsp;</p><p>第二点，我们今天在推行AI编程。是因为我们觉得就算你玩乐高，有个人在帮你提高效率，有个说明书放在旁边，我觉得是可以的。所以，今天AI编程它只是一个 Copilot，只是它针对不同的人有不同的 Copilot，因为我跟国外的一些公司也交流过，对于高级别的程序员，他更需要的是，那些很简单的代码他不想写了，有些东西他调一个库就搞定了，这个程序员一直这么干的。对于一般一点的程序员，他是需要告诉他一些经典的写法是什么？一些经常犯错的，比如说编码规范的东西，他需要一些提醒。像您刚才说对于产品经理，他不想写代码的人，他就需要一个端到端的东西。所以，我认为针对不同的人，需求是不一样的。我们不能简单的讲，有了 CodeFuse 就不需要程序员了，我认为这是人为创造的一种焦虑。</p><p>&nbsp;</p><p>最后我想回答的是，AI 对于我们今天所有的软件工程的生产范式是有一些变化的。蚂蚁中间件的负责在去年就跟我说过，如果 AI 都会写代码了，是不是未来我最重要的工作就是让AI理解我的中间件？中间件就是我们在写程序的时候最基础的那层东西，蚂蚁的中间件叫 Sofa，它是给程序员用来编程的，这还是一个比较专业的事情。它是支持你的 Copilot，甚至是你的代码的导师，你去让它理解中间件。所以，我们的工作确实会发生一些变化，但是它的本质还是在 AI 这个新的生产力加入进来后，我们重新编排我们的代码流程，还是这么一个事。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q4：您刚刚提到数据孤岛的问题，我觉得现在 AI 也存在这样一个问题，比如说各家手机厂商自己也有 AI 助手，如果你想要直接去问助手的话，用 GPT 或者用其他的。但当我真正需要生活服务的时候，我必须点开支付宝再去问，这样一个流程我能直接通过手机解决，而不用打开支付宝。</p><p>&nbsp;</p><p>A：简单来讲，此助手非彼助手，世界上的助手有千千万，但是每个人的助手能力是不一样的。我们今天更重要的是提升我们自己助手的能力，支付宝今天是有一些独特的优势的，不是说我们技术有多牛，而是说我们支付宝今天整个开放生态的能量。</p><p>&nbsp;</p><p>你真的要助手帮你去做件事情，它至少得有个服务商接口吧。当然，面向未来来看，我们希望我们真的能做到跟扫码支付一样，现在很多手机会绑定一些默认的扫码，有的就是支付宝。当我们做到那个量级的时候，我认为很多厂商会跟进的。核心是我们的产品能力是不是真的能便利每个人？我认为这个是关键。</p><p>&nbsp;</p><p></p><p>Q5：刚才您在整个框架蓝图里面，从表现层-结构层-战略层的框架讲了很多东西，您有提到表现层是一些偏硬件的交互，比如里面像具生智能之类的新的交互方式。我想问一下，蚂蚁这么高的视角，您怎么看硬件交互新形式的节奏，您觉得下一个要出现的是什么？再下一个是什么？有没有具体的地图。</p><p>&nbsp;</p><p>A：关于技术跟硬件结合相关的。我觉得我们是在做探索，是有一些想法，但也不是特别成熟。大家都是言必谈软硬件结合的产品，我认为软硬件结合目前不会是一个很成功的产品，这不是今天我说的，是很早之前我就说了，但是不代表他未来不会是。从整个科技来讲，必须要有一个先修桥，先修路的过程。你先得有一个基建的过程，包括今天的 AI 大模型。今天企业投了这么多钱、国家投了那么多钱为什么没有爆发点？但是试想一下，中国如果当年没有 3G、4G，今天也不会有移动互联网，这就是一个基建的过程。硬件的逻辑也是一样的，我认为硬件是基建，它是能够把更好的体验、更好的服务能够带给每个人的。但是不应该它想象成它马上能够产生一个巨大的消费市场，这种例子我认为还是比较鲜有的。即便是苹果这么厉害的世界头号的硬件公司，它最终能够产生真正的用户黏性，或者说能够提供最终的用户价值的，其实是上面的软件，以及它所有的生态里面提供的服务。所以，我们认为硬件只是一个基础，最终能够通过这个硬件、通过上面的叠加的软件、算法能够提供什么样的服务，我觉得是关键。</p><p>&nbsp;</p><p>再回到跟蚂蚁结合的问题上来，我们虽然想星辰大海，但并不是什么都可以做。我们还是要聚焦到我们擅长的一些领域。比如在医疗领域，我们觉得有相应的机会。今天医疗健康这个大的赛道，其实你靠一个简单的手机跟你交互，我认为是非常浅的。因为它的 sensor 是不够多的，今天的多模态还是语音、图像这些东西，比如中医他具备望闻问切这一系列交互，更别说西医所有的一切了。所以在这方面的 sensor 或者多模态 sensor 数据融合上，这些硬件我认为也是有非常大的前景的。所以说结合行业，结合刚才讲的硬件的定位是基建，最后提供什么样的服务，这个东西是我们觉得是可以去探索的。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q6：刚才您提到从擅长的事出发去做思考，这个决策过程是怎样的？不光是蚂蚁，在 AI 时代能成功的应用，它有些什么样的特征？它什么时候会出现？为什么是你们能做到这件事？ 支付宝流量这么大的平台做 AI 功能，怎么平衡 AI 创新和它的风险？</p><p>&nbsp;</p><p>A：我先解释一下，我们主要的还是技术，不建议把它作为业务的考量，我们整个思考和逻辑的范围还是聚焦在技术里面。</p><p>&nbsp;</p><p>技术带来什么样的变革？我们希望给世界带来微小而美好的变化。马斯克带大家登上火星，我觉得很伟大，但是能够让地球上所有人过上美好的生活，我认为同样伟大。今天你在看我们的所有支付宝从事的行业，十年前我们开始搞金融行业就在讲，让每个人能够享受到银行行长的待遇。之前在美国的时候我读过一本书《Bank4.0》，里面有一个章节就讲支付宝的。其中最核心的是，之前的银行都是要关门的，你要去银行取个钱、办个事你得请个假。但是今天因为数字化，因为我们的技术，今天有了 7×24 小时的银行。我觉得这就是我们带来的变化，蚂蚁也是通过二维码，二维码这个技术也不是支付宝发明的，但是扫码支付确实在中国我们是第一个推出来的。这个技术本身是没有问题的，它就是一个技术而已，但是你找到一个合适的场景的结合，然后把它变成一个普惠的服务，这是支付宝最擅长的。所以，我们今天看AI也是这个逻辑，我们今天不是说要做一个普惠的技术的提供者，而是我们如何用一个最好的技术，做一个最普惠的服务的提供商。</p><p>&nbsp;</p><p>AI 今天能解决的问题是什么？我能够把服务推荐给你，我有个AI助手，它知道你需要什么服务，然后知道支付宝有什么好的服务，然后把这个服务推向给你，这就是我们想做的事情。</p><p>&nbsp;</p><p>金融这个领域不用讲了，有一个词叫“Financialhealth”，你肯定是希望你的金融是安全的、是健康的。我们所谓的金融助手、金融管家也是要帮助大家的，我们发现很多人特别是很多初级的投资者，股票一下跌就会卖。这时候AI稍微跟他聊一聊，情绪稍微稳定一点，股票就不会卖了。因为很多人金融的决策不是理性的，而是情绪的，或者是道听途说有一些消息，有一些紧张情绪，就会做一个对他整体持仓不好的操作。医疗也是一样的。我们对风险的零容忍，其实是我们对自己有一个非常高的坝，我们觉得我们推出的服务、产品、技术，应该是经得起足够大规模的考验，以及足够挑剔的人群的考验的。所以，我们觉得在金融和医疗这里，是民生，是对每个人都重要的领域，值得我们真正去投入做，而且这也是对我们技术一个非常大的挑战，放眼整个行业来看，说的骄傲一点，如果我们不出手谁出手呢？我们觉得在这波技术的变革之下，我们应该可以去做这个行业的。</p><p>&nbsp;</p><p>AI 创新和风险是要平衡的，一个是我们非常注重本身的科技伦理，技术是双刃剑，它有两面性。但是我们如何把握好这个技术，真正的让它科技向善，而不是去作恶。所以，我们有两道（关卡），一个是科技伦理委员会，每年我必须参加。在大模型这个领域我们也投了很多资源，蚂蚁百灵大模型接近 20% 的人是在做安全性，不能出现一些有伤社会伦理的事情；第二个是在风控与合规技术。这块我们的投入也非常大，这两个东西是一个枷锁，某种意义上有点像汽车的安全带，你想开快的时候一定要把安全带绑好了。但是安全带你能说他是枷锁吗？他是，但是某种意义上是能够让你上高速开的更快，我们就是这么去理解风险和创新的关系的。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q7：原生多模态，您怎么看原生多模态这个趋势，它的能力边界在哪儿？</p><p>&nbsp;</p><p>A：人机交互应该是这波AI革命所带来的，并不是 AI 本身。但是，我跟你在交流的时候，有语言、表情还有手势等等，这就是我们理解的多模态。</p><p>&nbsp;</p><p>不是说我们今天的原生多模态就是去认识那个花是什么、那个草是什么，当然这个东西也有市场前景，但不代表原生多模态就应该去做这个东西。原生多模态核心的逻辑跟大模型一样，大模型大家都知道，今天的 Transformer 所有的架构，是从翻译来的，翻译里面是LP里面最难的一个领域。但是今天大家看，习以为常了。它最难的在哪儿？就是对齐。中国的语言和英文的语言，现在看似对的很齐，但其中包含跨文化的问题，比如在翻译诗词的时候就很难对齐。原生多模态的时候我们在跟什么东西对齐？是将人类的语言和我的手势、和所有自然界的东西对齐。所以整个大模型在干的一件事情，本质上就是在对齐。但是原生多模态意味着什么？我们把世界的万物、各种模态的数据，我用这个手势代表的是什么意思，我用语言表达出来的是什么意思？这个东西要对齐，这个是我们的核心。所以，这是大模型最核心、最关键我们在做的事情。</p><p>&nbsp;</p><p>原生多模态，我们要理解后面的本质，徐鹏博士在负责我们的语言大模型，包括百灵，我们为什么会把这个组合放在一起，是因为我们觉得语言只是一个看似比较好对齐的东西。语言不是人造的，不是一个自然界的东西，它本身是有边界的。每发明一个词，边界会扩展一点，但是总体是有一个边界的。你今天如果扩展到多模态或者原生多模态这个领域，它的边界肯定是扩大的，它这个问题的象限或者复杂度是呈指数倍上升的。那我们判断要不要做这个事情？虽然它的难度很大，从中国的角度你必须要攻克这个事情，而不是说我跟在别人后面。这个事情本质上它是有意义的，就跟人脑是一样的，小朋友有的时候不会语言，他也看得懂你的知识。一只小狗，它也知道你摇摇手、或者挥挥手是什么意思，它也没有语言，这个东西是更底层的东西。所以我们未来的多模态就是帮你做一个东西，让你知道你的小狗狗到底最近怎么了。很多人不知道，我认为这个是我们可以攻克的。</p><p>&nbsp;</p><p>所以，我们在做这个事情，回答你的边界，肯定是高于语言本身的，因为语言的边界是人为划了一个圈，但是它依然是核心，因为人类的智慧基本上就在语言的边界里面，能被语言表达出来的东西，其实都是人类的智慧。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q8：去年 ChatGPT 刚出来的时候，咱们的认知有没有发生变化？</p><p>&nbsp;</p><p>A：我们其实每天都在发生变化，但是大逻辑没有变。当然这可能是技术的局限，我们觉得这是一个很好、很先进的一个技术，而且最关键的是它让大家看到了一个可能性，我认为所有人去做就是因为这个可能性。但是从另外一方面，我们更关注的是，如何让它真正的能用起来，不是说只是变成少数人的 AI，我认为应该是大部分人的 AI，是所有人的 AI。我们思考的是怎么让这个东西做的更好用、更经济、更可靠，要把这个东西做出来，这是我们的一些思考。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q9：现在可以看到几乎所有的互联网公司都在提大模型，大模型也用到各个领域。大模型背后是大数据或者云计算或者数据各方面应用。大模型会不会成为下一次宕机的威胁所在，这个技术会不会对所有平台造成新的技术挑战？出现这种问题，究竟是技术不够还是对技术的敬畏不够？</p><p>&nbsp;</p><p>A：蚂蚁技术开放日是 2015 年的一次宕机引发的，到今天差不多 10 年了。这 10 年以来技术不断地发展，但是宕机和某某平台崩了还是会在热搜上出现。你刚刚提到敬畏技术，那这 10 年里面到底是技术不够还是敬畏不够？AI 本质上是不是足够大的风险？我倾向于更积极的去理解它。你说今天的互联网技术是不是一个风险？它很多时候其实是一个风险。但是我们需不需要互联网？我相信在座所有人都觉得我们需要互联网。任何事物都有它的两面性。</p><p>&nbsp;</p><p>技术我偏向于中性的去理解它，关键在于你怎么使用它，你把它用在哪儿，或者说你对它有没有足够的敬畏，知道它的能力边界，不会去放大它，不会去夸张的吹嘘它。我认为AI这波浪潮也是一样的，打个比方，我一直认为它是大号的“复读机”，全世界的知识它能够复读出来。你把它用在关键核心的地方，可能你就要承受这种关键核心地方崩塌所带来的风险。你把它用在不重要的地方，那它就是一个背景音，像一个“收音机”，挂了就挂了，就是这样的一个逻辑。</p><p>&nbsp;</p><p>到底是技术不够还是敬畏不够？我认为这是大家对技术的期待越来越高了。试想一下十年前或者更早一段时间，其实很多网站是有宕机护时间的。但是今天再看，哪个网站说我要宕机维护一下？国外的云几乎都需要维护，但是在中国，在阿里云是没有的。我们今天的技术在越来越深入的进入到国计民生各个行业。以支付宝为例，我们经常跟自己说，十年前的支付宝和现在的支付宝完全不一样，今天的支付宝要宕机1秒钟，可能上海的地铁的闸机就过不去了，但十年前我们是没有这个职责的。所以那时候宕机宕2个小时，大家觉得日子照常过，但是今天支付宝宕机 2 小时，那会产生非常大的影响，这是我们今天对技术的要求不一样。在这个过程当中，我认为技术不怕挑战，核心是说我们有更高的要求了，我们的技术一定会往上走的。AI技术也是一样的道理，我们有要求，有足够的敬畏，有足够的投入，它一定会有足够的产出。</p><p>&nbsp;</p><p>&nbsp;</p><p>Q10：如果一个大模型要做到可靠的话，是目前Transformer这个底层是可以优化的，还是说需要一些技术辅助它去做，您有什么技术可以分享的？</p><p>&nbsp;</p><p>A：我首先纠正一点，Transformer 不关键。因为 Transformer 的本质就是把序列数据变成并行化。之前语言大模型的训练是不可能那么快的，就算有卡也是不可能那么快的，Transformer 核心改变的就是这个。它的本质是在于我在模型训练时，包括从数据到最后的产生的输出，每一个 token 都是预测出来的，预测它一定会有误差的。但当你的误差累积到一个程度，它就一定会胡说八道的。所以，本质上是在控制这个数据流我怎么流进去以及我怎么输出来，让它在足够小的误差范围内，以及误差不要累积。</p><p>&nbsp;</p><p>至于解决这个问题，肯定是有些手段和办法的。第一个是大家都知道所谓的 RAG，它核心是控制它的输入，我只能从这个知识库里面去流向到模型系统，然后再做输出，本质上是控制它的输入。然后，输出是靠什么？我们蚂蚁也开源了在金融领域的一个多智能体框架 agentUniverse。输出是干什么呢？我们是要多个模型去对，甚至按照一定的 SOP，按照一个专家流程来产生这个输出。或者简单讲，我们要尽量收紧它可输出的范围，来进一步的控制它的精确性。当然这个东西的问题就在这儿了，AI其实就两个事，一个是泛化，一个是精准，足够泛化就很难精准，足够精准就很难泛化，这是一个本质矛盾。我们要干的事情是什么？我们在控制输入、控制输出的过程本身就是这么一个逻辑，在控制输入的时候它要足够的泛化能力，足够的泛化能力它是要靠中间的模型足够大、内容要足够多，然后我两头一掐，它牺牲掉以后，既有足够的精准性又有足够的智慧。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</id>
            <title>腾讯大模型APP真实测评！七家国产大模型“battle”，元宝顶得住吗？</title>
            <link>https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FuZrECsytJmcvGAxU9Cp</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:53:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词:  腾讯元宝, 混元大模型, AI搜索, AI写作
<br>
<br>
总结:  腾讯元宝是基于混元大模型的App，提供AI搜索和AI写作等核心能力，旨在服务于普通人的生活。在文生图和AI写作方面，腾讯元宝展现出了提升潜力，与其他大模型进行对比评估，效果不俗。AI搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，提升了效率和准确性。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p>&nbsp;</p><p>“腾讯做大模型不争一时之先。”</p><p>&nbsp;</p><p>5 月 30 日，腾讯基于混元大模型的App“腾讯元宝”正式上线，苹果及安卓应用商店均可下载。腾讯云副总裁、腾讯混元大模型负责人刘煜宏表示：“过去的一年，我们持续推进腾讯混元大模型的能力爬坡，希望腾讯元宝最终服务于每个普通人的生活。”</p><p>&nbsp;</p><p>相比此前测试阶段的混元小程序版本，面向工作效率场景，腾讯元宝提供了AI搜索、AI总结、AI写作等核心能力；面向日常生活场景，元宝提供了多个特色AI应用，并新增了创建个人智能体等玩法。</p><p>&nbsp;</p><p>那么，这些功能的实际表现到底如何呢？</p><p>&nbsp;</p><p>拿到腾讯元宝的体验资格后，我们马上逐一试用了它的亮点功能，并特别针对文生图和AI写作方面，通过同题多解的方式，将其与多个国内知名的大模型进行了对比和评估。</p><p>&nbsp;</p><p></p><h2>创作效果：有提升潜力</h2><p></p><p>&nbsp;</p><p>首先，来直击一下元宝与各大模型“battle”的实况。</p><p>&nbsp;</p><p>文生图</p><p>&nbsp;</p><p>在这一功能上，我们选择了百度文心一格、阿里通义万相、讯飞星火、美图 MiracleVision 4.0 、字节跳动豆包五个模型对比效果，对他们的理解和内容生成能力逐一进行了测评。另外，考虑到涉及的这些大模型都源自国产，我们特意选取了中国文言文作为输入素材，以此来考察它们在处理本国语言古典文本上的能力。</p><p>&nbsp;</p><p>从生成图中所涵盖各实体元素的完整程度和整体画面的协调性来说，元宝的表现是排在前列的。</p><p>&nbsp;</p><p>提示词为：林中有寿鹿仙狐，树上有灵禽玄鹤。瑶草奇花不谢，青松翠柏长春。仙桃常结果，修竹每留云。一条涧壑藤萝密，四面原堤草色新。</p><p>&nbsp;</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/44/44c886249db068571bc948559d535c79.jpeg" /></p><p>（从左到右分别是元宝、豆包、讯飞星火、美图 MiracleVision 4.0、文心一格的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bdce8068798d7af3511dcf3aa6362660.png" /></p><p></p><p>（通义万相的输出结果）</p><p>&nbsp;</p><p>AI写作</p><p>&nbsp;</p><p>据介绍，在AI写作方面，元宝不仅支持多轮问答，还能够将对话的内容整理成报告，按照要求进行结构化输出。这一功能上，我们将其与文心一言、通义千问、Kimi、豆包四个模型对比效果。</p><p>&nbsp;</p><p>以测评案例来看，相较而言，腾讯元宝的生成内容展现出了包含起承转合的完整情节，故事框架也已具备雏形，但语句间的逻辑衔接略显生硬、人物描写也较为生涩。</p><p>&nbsp;</p><p>提示词：唐僧师徒四人穿越到现代后的第一天，会发生什么故事？写一个300字左右的小故事。</p><p>&nbsp;</p><p>元宝的生成结果如下：</p><p>&nbsp;</p><p>可以小夸一下的是，元宝留意到了唐僧师徒四人需要吃素的人物细节，在人设和故事设定方面理解得还不错。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6c93ffbd3c79acfc26a4a3cbff437cf4.jpeg" /></p><p></p><p>&nbsp;</p><p>再看豆包的生成结果，其语句结构明显更为成熟了，各个段落环节之间衔接得也比较自然。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46a422e4fd58cbd06cadc795f3632ef5.png" /></p><p></p><p>&nbsp;</p><p>到文心一言这里，无论语句组织还是文字逻辑，都展现出不错的效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecd423a78fee7f5cef56b16772330317.png" /></p><p></p><p>&nbsp;</p><p>而Kimi和通义千问的生成结果，开始呈现出更显著的变化。除语句构造外，整个故事的人物设定、叙事角度、情节架构都更加立体，并都在结尾处给读者构建了一个引人入胜的虚构世界。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/06737b6e9a5777f6c72bdacd8b798721.png" /></p><p></p><p>（Kimi的输出结果）</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/ccae0d6aa2348bc17ce68083be19d3b6.png" /></p><p></p><p>（通义千问的输出结果）</p><p>&nbsp;</p><p></p><h2>效率、娱乐方面：表现不俗</h2><p></p><p>&nbsp;</p><p>当前，大模型仍在快速发展期，从模型能力到应用落地都存在较大“时延”。数据显示，当前人们使用大模型相关产品时，有超过 65%的需求，集中在工作/学习效率场景，但相关的AI产品解决方案尚不成熟。针对效率场景的三大核心需求：信息获取、处理和生产，腾讯元宝均进行了产品化探索。</p><p>&nbsp;</p><p>AI 搜索</p><p>&nbsp;</p><p>AI 搜索方面，腾讯元宝接入了微信搜一搜、搜狗搜索等搜索引擎，并通过AI搜索增强，提升时新类和知识类问题效果，比传统搜索更有效率；同时，内容覆盖微信公众号等腾讯生态内容及互联网权威信源，答案准确性更高；此外，元宝还会提供所引用的参考资料，并给出相关推荐，方便快速溯源及延伸阅读。</p><p>&nbsp;</p><p>我们输入一个近日引发热议的美国AI禁令问题：如何看待中国人被限制在美从事 AI 相关工作？</p><p>生成结果如下：</p><p><img src="https://static001.geekbang.org/infoq/b1/b1a00d5888bfd780c585ab89630e45b5.jpeg" /></p><p>&nbsp;</p><p>AI总结</p><p>&nbsp;</p><p>AI总结方面，无论是希望快速了解一本书或是一个新领域，还是处理复杂繁冗的报告、文献，元宝都能帮上忙。据介绍，元宝可上传最多10个PDF、word、txt等多种格式的文档，并能够一次性解析多个微信公众号链接、网址，支持256K的原生窗口上下文。</p><p>&nbsp;</p><p>例如，我们输入：请总结一下AI前线公众号这一年来的内容输出亮点。</p><p>&nbsp;</p><p>生成结果如下：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/35/35ef172c42abc7d68cefc8b422c5232a.jpeg" /></p><p>&nbsp;</p><p>除了满足效率需要，腾讯元宝在日常生活场景，也提供了丰富的应用及玩法，包括百变AI头像、口语陪练、超能翻译官等，均免费开放。同时，元宝也支持用户根据个性化需求，快速创建个人专属的智能体，赋予角色设定，或让AI自动生成智能体相关信息，并复刻自己的音色。结合腾讯生态场景，元宝还将于近期推出腾讯新闻哥、《庆余年》主题等特色智能体。</p><p>&nbsp;</p><p>视频、3D生成功能后续上线</p><p>&nbsp;</p><p>腾讯元宝产品能力升级的背后，是混元底层模型的持续迭代。</p><p>&nbsp;</p><p>自 2023 年 9 月首次亮相以来，腾讯混元大模型的参数规模已从千亿升级至万亿，预训练语料从万亿升级至7 万亿tokens，并率先升级为多专家模型结构（MoE），整体性能相比Dense 版本提升超50%。除不断提升通用大模型能力外，腾讯混元也支持角色扮演、FunctionCall、代码生成等领域能力，数理能力提升 50 %。</p><p>&nbsp;</p><p>在多模态方面，腾讯混元文生图大模型是业内首个中文原生DiT架构模型，采用了Sora、Stable Diffusion 3等行业顶尖产品的同款架构，生成效果相比上代提升超 20%。目前，该模型已经全面开源，在Github获得 2000+star，相关能力也全面融入腾讯元宝。</p><p>&nbsp;</p><p>此外，腾讯混元大模型在视频、3D生成等方面也持续探索，目前已经支持16s视频生成，单图仅需30秒即可生成3D模型，相关能力也将于后续在元宝中上线。</p><p>&nbsp;</p><p>目前，腾讯内部有超 600 个业务及场景都已经接入腾讯混元，腾讯广告、微信读书、腾讯会议、腾讯文档、腾讯客服等，都已经基于混元实现了智能化升级。据了解，为了满足开发者及企业客户对于通用模型能力的需求，腾讯混元大模型已通过腾讯云对外开放，可通过API调用，也可以作为基底模型，为不同产业场景构建专属应用。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</id>
            <title>AI 是低代码的“福”还是“孽”？</title>
            <link>https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NMIPCuy1ctaID4CQ3bJN</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 09:04:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 低代码, 大模型, 编程
<br>
<br>
总结: 在AI发展史上，AI大模型的出现引发了对低代码平台的讨论，AI编程的概念挑战了传统的代码开发方式，行业内存在关于AI与低代码未来发展方向的争论。在相关峰会上，专家们分享了AI与低代码的结合实践和思考，探讨了AI时代软件架构的设计和应用。 </div>
                        <hr>
                    
                    <p>取代论，在 AI 几经起伏的发展史上的每一个高点，都会被拿出来重新审视和热议。这种职业危机感，在生成式 AI 的这一波浪潮中，也很快蔓延到了技术圈中。“自己的饭碗被自己的工作干掉，这可能不是一句玩笑话。”一位开发者在近期与 InfoQ 交流时感叹。</p><p></p><p>在此之前，低代码 / 无代码作为软件提效的平台和工具已经逐步流行起来。顾名思义，其价值在于通过图形化界面和简单点击、拖拽、配置，能够大大降低代码开发的门槛，减少开发人员工作量的同时，针对一些简单开发需求，业务人员也能“自给自足”，更快地响应业务侧的需求。</p><p></p><p>而就在低代码概念开始被市场普及接纳走向落地，行业的商业模式刚刚跑通时，半路“杀出”了 AI 大模型。</p><p>相较于低代码，AI 大模型的可能性更为激进：不需要任何的编码，只通过自然语言交互就可以直接生成应用。换言之，代码开发的门槛不是降低了，而是直接没有了。</p><p></p><p>一场争论不可避免。一种声音是“低代码将被 AI 彻底颠覆”，比如，一些低代码起家的公司，去年火速切换到了 GPT 赛道，背后的考量不难臆测。另一种声音则认为，二者将双向奔赴，AI 能力将成为低代码的标配，有不少企业正在试图将二者融合。</p><p></p><p>在 6 月 14 日 -15 日即将举办的 <a href="https://archsummit.infoq.cn/2024/shenzhen/">ArchSummit 全球架构师峰会深圳站</a>"上，阿里巴巴研究员 / 阿里云云原生应用平台负责人丁宇（叔同）将带来<a href="https://archsummit.infoq.cn/2024/shenzhen/presentation/5718">《AI 编程如何颠覆生产力》</a>" 的 Keynote 主题演讲，详细介绍在 AI 大模型快速发展的背景下，AI 编程的发展实现了哪些突破，以及 AI 编程助手的引入如何为软件开发带来质的飞跃。从开发者的视角出发，他将展开分享 AI 编程工具基于大模型的设计要点、难点、改进思路，帮助开发者从自身的生命力出发，学会用 AI 激活开发效率，提升生产力，而不是与之对抗。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8cc3b180566afd334adb450c1a57fba6.webp" /></p><p></p><p>为进一步探讨 AI 与低代码的关系，ArchSummit 深圳还进一步策划了<a href="https://archsummit.infoq.cn/2024/shenzhen/track/1643">《低代码与 AI 结合》专题</a>"，深入研究低代码平台如何与人工智能技术相结合，提高开发效率。探讨在低代码环境中集成智能决策、自动化流程，以及构建灵活、高效的应用系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03baaaff951079ab3734a289ead8e5df.webp" /></p><p></p><p>腾讯 PCG 前端技术专家苑宗鹤将在专题演讲中分享《无极低代码 UI 可视化的 AIGC 落地与实践》。在他看来，大模型的提效能力加上低代码的易用性相辅相成，让低代码开发效率更高，更大的降低了用户的使用门槛。他将基于无极低代码平台介绍 AI 搭建低代码布局、AI 辅助代码生成，以及对平台的 AI 功能进行自动化测试的实践路径。</p><p></p><p>网易 CodeWave 技术团队作为国内较早采用大模型技术并将其应用于产品的技术团队，其团队负责人姜天意也将在专题演讲中从低代码产品的挑战出发，分析大模型的机遇和实施难点。同时，从 AI 友好的语言设计出发，结合大模型的 Agent 能力，介绍融合自然语言生成、辅助编程、D2C 等 AI 能力的产品设计和实施方案，以及相关的模型训练方案。</p><p></p><p>针对“AI 都能编程了，低代码平台会被消灭吗？”这样的灵魂拷问，蚂蚁集团支付宝体验技术部 / 高级前端技术专家江凯将在其演讲中给出他的答案。他将详细介绍《云凤蝶在 AI 与 LowCode 结合上的思考与实践》，分享 AI Native 的低代码产品形态如何设计、如何实际应用 LLM 和 AIGC 技术、如何看待对话式 UI、生成式 UI 的发展？AI 原生应用的 LowCode 研发有市场吗等一系列行业普遍的困惑问题。</p><p></p><p>除此之外，本次大会还策划了 10 余个 AI 系列相关的专题，顺丰集团 CTO 耿艳坤、Thoughtworks CTO Scott Shaw、百度飞桨总架构师于佃海等国内外 100+ 顶尖专家齐聚，将从底层基础到顶层应用深度探索大模型时代软件架构的最佳设计，共探 AI 时代的无限可能。</p><p></p><p>点击链接可查看更多详情：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p>会议 9 折购票倒计时1天，如您感兴趣，可以联系票务经理 17310043226，锁定最新优惠。</p><p><img src="https://static001.geekbang.org/infoq/d9/d9b861049ccebcbdec69ed036e705118.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</id>
            <title>剥离几百万行代码，复制核心算法去美国？TikTok最新回应来了</title>
            <link>https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IN98I1kFWJKwEA1aGZKg</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 08:34:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 代码剥离, TikTok, 美国用户, 算法
<br>
<br>
总结: TikTok正着手为其1.7亿美国用户开发克隆版推荐算法，剥离数百万行代码是一项繁琐的工作，需要一年多时间才能完成。这项工作的目标是为面向美国用户的TikTok推荐算法创建一套新的源代码库，但可能导致美国TikTok失去母公司字节跳动的工程开发支持。 </div>
                        <hr>
                    
                    <p></p><blockquote>剥离几百万行代码，是一个繁琐的“脏活”，需要一年多时间才能完成。</blockquote><p></p><p>&nbsp;</p><p>路透社5月30日消息，据直接掌握内情的消息人士透露，TikTok正着手为其1.7亿美国用户开发克隆版推荐算法。这可能催生出一个独立于其中国母公司运行的版本，因此更容易被拟发布禁令的美国立法机构所接受。</p><p>&nbsp;</p><p>就在TikTok中国母公司字节跳动去年年底下令拆分源代码之前，美国方面已经提出一项拟议的强制出售TikTok在美业务的法案，且此项法案今年初在国会得到支持。今年4月，该法案正式被签署为法律。由于未获授权公开谈论这款短视频分享应用，消息人士拒绝透露姓名。但其表示一旦代码被拆分，即可为剥离在美资产奠定基础，从而打开一条免受法律强制执行的可能之路。</p><p>&nbsp;</p><p>TikTok公司此前曾经表示并无出售在美资产的计划，甚至强调绝不可能采取此类措施。该公司最初拒绝发表评论。但在本篇报道发表之后，TikTok在X上的帖子中表示，“路透社方面日前发布的报道存在误导性，且与事实有所出入”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d2d551608dc0513c79bf068230f6055.jpeg" /></p><p></p><p>&nbsp;</p><p>路透社表示，该“辟谣”没有具体说明哪里不准确。TikTok还发布了一段来自其联邦诉讼材料的内容：“无论是从商业、技术还是法律角度来看，该法案提出的「获准剥离」以允许TikTok继续在美运营的建议都根本不可行。而即使可行，法案给出的270天时间表也远远不够。”</p><p>&nbsp;</p><p>路透社方面的发言人则回应称，“我们坚持报道内容属实。”</p><p>&nbsp;</p><p></p><h2>剥离数百万行代码的“脏活”</h2><p></p><p>&nbsp;</p><p>路透社称，过去几个月以来，来自美国和中国的数百名字节跳动和TikTok工程师被要求着手剥离数百万行代码，旨在筛选该公司用于将用户与其偏好的视频相匹配的算法。据两位直接了解该项目的消息人士说，工程师们的任务就是建立一套独立的代码库，其独立于字节跳动的中国版TikTok、即抖音的系统之外，同时将消除其中与中国用户相关的信息。</p><p>&nbsp;</p><p>路透社此前曾报道称，TikTok将应用程序连同算法一同出售的可能性极低。而且内容推荐算法在我们国家的出口管制名单当中，因此对TikTok算法的剥离或者出售必须经过审查。</p><p>&nbsp;</p><p>根据相关法律文件，TikTok推荐引擎的源代码最初由字节跳动工程师在中国开发完成，并针对包括美国在内的TikTok全球各市场运营情况进行了定制。</p><p>&nbsp;</p><p>在字节跳动看来，TikTok之所以在全球范围内大受欢迎，首先要归功于其推荐引擎的出色表现。该引擎能够根据每位用户与其观看内容的交互方式来提供更多内容来源。</p><p>&nbsp;</p><p>在向路透社描述这项任务的复杂性时，消息人士们将其称为繁琐的“脏活”，这也凸显出将TikTok业务与其母公司底层代码剥离开来的难度。据消息人士们介绍，这项工作预计需要一年多时间才能完成。</p><p>&nbsp;</p><p>TikTok及字节跳动已经明确表示会以美国宪法第一修正案为依据，在法庭上对抗这条新法律。尽管如此，消息人士称工程师们已经在依命令开展工作，着手将TikTok推荐引擎与字节跳动的整体网络拆分开来。</p><p>&nbsp;</p><p>消息人士们提到，TikTok高管曾一度考虑开源部分TikTok算法，或者允许其他人访问并修改该算法，借此展示技术透明度。</p><p>&nbsp;</p><p>据一位出席团队全体会议的消息人士、以及另一位翻阅过相关材料的消息人士所言，高管们在会上通过内部规划文件及内部通讯系统Lark传达了剥离计划，并就代码拆分项目做出了更新说明（但路透社无法独立核实这些内部消息的真伪）。</p><p>&nbsp;</p><p>据一位消息人士透露，目前这项工作的复杂之处，在于确定TikTok具体代码迁移部分所带来的合规性与法律问题。消息人士还补充称，必须审查每一行代码以核实其是否可以被剥离至独立代码库。</p><p>&nbsp;</p><p>这项工作的目标，是为面向美国用户的TikTok推荐算法创建一套新的源代码库。工作完成之后，TikTok美国版将独立于其他地区的TikTok版本以及中文版抖音，采取专门的一套推荐算法运行和维护体系。消息人士称，此举将导致美国TikTok失去母公司字节跳动强大的工程开发支持。</p><p>&nbsp;</p><p>消息人士还补充称，如果TikTok最终完成美版推荐引擎与中国版本的拆分工作，管理层承认后续TikTok美国版在性能上恐怕达不到现有TikTok的水平。因为目前TikTok推荐算法库仍高度依赖字节跳动中国工程师们的更新和维护。换句话说，TikTok在美国市场上的用户吸引力可能将因此被削弱。</p><p>&nbsp;</p><p></p><h2>TikTok推荐算法神秘吗？</h2><p></p><p>&nbsp;</p><p>在TikTok风波中，其推荐算法一直是大家争相讨论的话题中心。</p><p>&nbsp;</p><p>2022年6月，有媒体报道，TikTok宣布将美国境内的所有流量转移到甲骨文云服务的基础设施上，同时这项托管服务也给甲骨文带来高达10 亿美元收入。而后，甲骨文于这一年的8月份启动了对TikTok 算法和模型的审查，甲骨文希望确保 TikTok 上的内容显示“符合用户的期望”，并且推荐算法不会受到操纵。TikTok 还专门设有一个“专用透明度中心”的区域，供甲骨文员工审查该应用程序的源代码。</p><p>&nbsp;</p><p>然而一年之后，据福布斯报道，字节跳动和甲骨文之间的关系就已经变得非常不信任和敌对。消息人士将甲骨文对字节跳动的立场描述为“反情报行动”，而不是正常的客户关系。与此同时，一些字节跳动员工怀疑甲骨文是否只是想增加他们的账单。TikTok 托管服务合同在甲骨文内部被称为 Project Telesis，使字节跳动成为甲骨文最赚钱的客户之一。</p><p>&nbsp;</p><p>如今看来，甲骨文的审查并没有让美国政府放松对TikTok的仇恨和警惕。</p><p>&nbsp;</p><p>实际上，TikTok 之前发表过一篇博客文章，主要描述解释了他们的 FYP 算法工作机制，相信大家只要是做软件技术的，看过之后都知道其中并没有什么新鲜的创造。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/295ecc271aa2b22c4c00ed3e4b8ae723.jpeg" /></p><p></p><p>&nbsp;</p><p>另外，前亚马逊产品经理（同时也是亚马逊战略规划部第一位分析师）Eugene Wei 也曾发过专门分析文章，阐述TikTok 的算法本身并没有特别的突破性创意。</p><p>&nbsp;</p><p></p><blockquote>当大家谈及 TikTok 的算法是其成功的关键时，便会认为该公司的秘密武器是一些神奇的代码。但该领域的大多数专家持怀疑态度，TikTok 在机器学习推荐算法方面并未取得外界未知的突破性进展。事实上，他们中的大多数人认为，TikTok 很可能就是基于标准方案解决的问题，跟其他方案无异，没有什么特殊性。&nbsp;不过机器学习算法的有效性并不仅仅取决于算法本身的函数，还取决于数据集训练后的算法函数。GPT-3 并不是新创意，但是通过大量数据训练和大量的参数设置，它的输出结果往往是令人惊讶的。&nbsp;同样道理，基于自身数据集训练过的 TikTok FYP 算法，在将视频与觉得该视频有趣的人进行匹配方面做的非常精确和高效（而且，反向匹配做的也很精确，对某些视频不感兴趣的人就不会接收到这些视频） 。</blockquote><p></p><p>&nbsp;</p><p>他认为，TikTok 产品真正的价值点在于 TikTok 的设计和流程里面的每一个元素是怎么互相关联到一起，从而创建出一个数据集，再通过这个数据集，把算法训练成最佳性能的。</p><p>&nbsp;</p><p>“这就是 TikTok 设计的神奇之处：它是一个反馈的闭环，这种设计能够激发并实现视频的创作和观看，产生的数据进而通过其算法进行训练，之后再反过来激发创作和观看。为了让 TikTok 的算法变得像现在这样有效，TikTok 成为了它自己的训练数据来源。”</p><p>&nbsp;</p><p>但多数人还是非常费解，为什么很多公司想要收购 TikTok，另一方面，字节跳动是否应该将 TikTok 这一备受欢迎的 App 卖掉。对此，Eugene Wei 评论说：“围绕 TikTok 算法大肆的炒作已经开始变的异化了，这也是如今西方对中国科技领域项目的普遍套路。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/">https://www.reuters.com/technology/tiktok-preparing-us-copy-apps-core-algorithm-sources-say-2024-05-30/</a>"</p><p><a href="https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you">https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you</a>"</p><p><a href="https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/38dKguZxeyz2vx2dAR4S</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</id>
            <title>全球首款PC原生的AI编程与软件智能研发助手驭码CodeRider正式发布！</title>
            <link>https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/meMm4MXomT8GZMYvDzbo</guid>
            <pubDate></pubDate>
            <updated>Fri, 31 May 2024 02:56:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, AI产品发布会, 驭码 CodeRider, 极狐GitLab
<br>
<br>
总结: 5月28日下午，极狐GitLab在上海成功举办了基于AIGC技术且完全自研的AI产品发布会，正式推出驭码CodeRider，PC原生的AI编程与软件智能研发助手。CEO柳钢介绍了驭码CodeRider的含义和优势，强调AI赋能程序员而非取代。产品具有PC原生、私有化部署、与GitLab深度融合等优势，受到企业用户好评。发布会还宣布了驭码CodeRider的三个版本，开启了试用通道。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/c3/c3e154ed850988f8ff90ce95496aa795.png" /></p><p></p><p>5 月 28 日下午，极狐GitLab 在上海成功举办了基于 AIGC 技术且完全自研的 AI 产品发布会，正式推出驭码 CodeRider —— PC 原生的 AI 编程与软件智能研发助手。发布会采取线上线下相结合的方式，线下有包括企业高管、高校教师、高校学生、媒体人员在内的 80 多位嘉宾共同出席参加了此次发布会，线上观看量超过 10万+。 </p><p></p><p>发布会伊始，极狐GitLab CEO 柳钢以“PC 原生全球首发 AI 编程与软件智能研发助手”为主题，全面介绍了驭码 CodeRider。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/40/40e343e6a832e4f6098a3baf0b60536d.png" /></p><p>极狐GitLab CEO 柳钢 </p><p></p><p>柳钢表示，软件定义世界已经成为了不争的事实，而软件的打造者、代码的创造者正是程序员这一群体，在中国就有 1000万程序员。而极狐GitLab就是一家专为中国程序员服务的公司，也是目前国内唯一一家将赋能中国程序员写进员工手册的公司，极狐公司的使命是——让中国程序员的技术潜能与业务价值得到最大化发展。极狐GitLab 致力于通过借鉴全球领先的技术，同时基于国内现状进行完全自主研发、自主创新的方式来赋能中国1000万程序员。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c115d21986defd5f342fcde2bb393d21.png" /></p><p>赋能 or 取代？ </p><p></p><p>柳钢进一步表示，在 AI 时代赋能中国程序员的最好方式就是将 AI 能力应用到软件研发领域，为程序员打造出为程序员所有、被程序员所用、让程序员认可的产品，用 AI 赋能程序员，绝非取代程序员 。但是在这个过程中必须解决当前大模型的三个关键问题：数据安全、个性化以及成本。只有成功解决这三个问题，才能够真正打造出程序员心目中的绝佳好产品——覆盖软件研发全生命周期、企业统一部署 &amp; 用户一键安装、功能丰富 &amp; 触手可及以及私藏独有。</p><p></p><p></p><p>“极狐GitLab AI 团队完全自研的驭码 CodeRider 正是这样一款产品。而且驭码 CodeRider 是全球首款 PC 原生的 AI 编程与软件智能研发助手。”柳钢强调。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24c56680c152f342663a416854148fd6.png" /></p><p>极狐GitLab 驭码CodeRider 正式发布 </p><p></p><p>接着，柳钢解读了驭码 CodeRider 的含义。Code 是程序员心目中最神圣的词，意为编码，而 Rider 有骑兵、驾驭者的意思。为此，特意为该产品起了一个朗朗上口的 Slogan“为 AI（爱）奔腾，驭码当先”。从表面看，驭码 CodeRider 是一个 AI 编程助手，看似是又一款 Copilot，但是驭码又绝不仅仅只是一款 Copilot，相比市面上其他所有的 Copilots，驭码 CodeRider 要比他们优秀三点，而这也是驭码 CodeRider 得天独厚的优势：PC 原生、私有化部署、GitLab 合璧。PC 原生是指驭码 CodeRider 是端侧部署，和笔记本电脑是天然契合的，而且产品在设计之初就考虑了离线运行；私有化部署是指可以将驭码 CodeRider 部署在企业内部的安全环境中，这从根本上解决了数据安全问题，而且能够更好地满足和实现企业智能化、个性化问题；最后一点：驭码 CodeRider 和 GitLab 深度融合、浑然一体，这意味着驭码 CodeRider 将 AI 技术完全应用到软件研发的全生命周期中，不仅可以编写代码，还能够帮助程序员进行议题（Issue）和合并请求（MR）的处理，甚至和 CI/CD 结合起来，做到端到端的 AI 赋能。这些都是其他 Copilots 无法做到的！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a34bc5dd659e0c563ed255a78769b9fd.png" /></p><p>驭码CodeRider 比 Copilot 好三点 </p><p></p><p>柳钢表示，自夸不算好，要用户说了才算好，目前已经有很多企业用户在体验试用驭码 CodeRider，他们的反馈是对驭码 CodeRider 最好的认可。随后，现场播放了来自嘉宾的祝福视频，包括联想集团、通义实验室、零一万物、PingCAP、无问芯穷、LigaAI、重庆市政府九龙坡区、未尽研究、哪吒汽车、足下科技、PingCode、Sipingsoft、青岛职业技术学院、南京大学、厦门大学、重庆交通大学、广州华商学院的多位嘉宾纷纷表达了对驭码 CodeRider 即将发布的热烈期盼，同时预祝发布会圆满成功！ </p><p></p><p>分享最后，柳钢正式揭晓了驭码 CodeRider 的三个版本：驭码团队版（CodeRider Team）、驭码企业版（CodeRider Enterprise）以及驭码混合算力版（CodeRider Hybrid）。三个版本对应不同的功能以及不同的使用场景。驭码CodeRider 同步开启了申请试用通道，会有专业的顾问为大家解答关于驭码CodeRider 的相关情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0723ffc88faa9bccf42476d76f49a1c3.png" /></p><p>驭码 CodeRider 专业顾问联系方式 </p><p></p><p>随后，极狐GitLab AI 团队负责人、驭码CodeRider 研发负责人邹雨竹上台深入介绍了驭码 CodeRider 的相关情况。他提到，“驭码 CodeRider 是一款 AI 驱动的 PC 原生应用，是研发人员的智能编程助手，同时跟 GitLab 的深度集成，也让驭码 CodeRider 变成了一款智能 DevOps 工具，真正做到了“一款工具，双重助力”。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6780c895939065b4083d7d041665f79a.png" /></p><p>极狐GitLab AI 产品负责人邹雨竹 </p><p></p><p>邹雨竹进一步解释道，打造这样一款具备 AI 能力的 PC 原生应用，必须考虑三个核心要素算力、模型、引擎。在算力方面，驭码 CodeRider 从设计之初就决定要做 PC 原生的“AI 编程与软件智能研发助手”，为此对 20 多款 AI PC 进行了详尽测评；在模型方面，采取采众家之所长的策略，对 30 多款大模型进行了测评，最后决定驭码 CodeRider 根据不同功能选择最佳模型，比如代码补全使用补全模型、对话采用对话模型，以实现在高效量化的同时达到性能和与体积最佳平衡的目的；在引擎方面，采用了基于 C++ 的桌面推理引擎，而且对不同的硬件指令集和框架做了适配。而这样做也获得了惊艳的效果，同样也造就了驭码 CodeRider 这样一款同时适用于企业、个人的 AI 产品。企业可以通过私有化部署实现数据安全并节省超过 50% 的成本，而开发者则能享受便携性和极佳的响应速度。 </p><p></p><p>邹雨竹还在现场演示了驭码 CodeRider 的相关功能，包括智能编程部分的代码补全、代码生成、技术问答等，以及智能 DevOps 工作流中的 Issue、MR 处理等。“驭码 CodeRider 绝对是一款让企业受益的 AI 工具，因为其具备私有化部署的特性，有着模型组合的优越性，而且与 GitLab 浑然天成”。邹雨竹说到。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57f4a2d7111a97b0a83e660fba224d83.png" /></p><p>驭码CodeRider 产品功能图 </p><p></p><p>接着，来自联想中国的段勐、英特尔中国的张智勇、浙江省特级教师谢作如，三位嘉宾分别进行了分享，他们从企业、高校的角度分享阐述了与 AI 相关的内容和观点。 </p><p></p><p>发布会最后，极狐GitLab CEO 柳钢再次登台，正式揭晓了驭码 CodeRider 三个版本的价格：驭码团队版（CodeRider Team）的价格为499/人/年、驭码企业版（CodeRider Enterprise）的价格为899/人/年。与此同时，也透露了，目前驭码 CodeRider 团队正在积极打造企业级的混合算力私有化架构，智能终端+智能边端会让驭码 CodeRider 变得更强大，这样强大的产品将在 90 天以后与大家见面！ </p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d4e4a7198909b9d667a42a123ca49d0c.png" /></p><p>驭码CodeRider 价格 </p><p></p><p>最后，柳钢表示，全球首款 PC 原生的 AI 智能编程与软件智能研发助手——驭码CodeRider 发布会宣告结束！到场嘉宾纷纷走向极狐GitLab 团队提前准备好的驭码 CodeRider 体验区，现场体验驭码 CodeRider 的 AI 功能并与技术人员进行了深入交流。 </p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff74da88bd54726eb9c1496759ae3106.png" /></p><p>驭码CodeRider 现场体验区</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</id>
            <title>网易有道CEO周枫：模型即应用的时代到来，Super App随时会诞生</title>
            <link>https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hPED1Wk71CIt3RHIl2e8</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 07:43:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI创新应用, RAG引擎, 有道小P
<br>
<br>
总结: 有道分享了子曰教育大模型的最新技术进展和三大AI创新应用，其中包括新一代知识库问答引擎QAnything、AI全科学习助手“有道小P”APP和虚拟人口语教练Hi Echo 3.0。有道CEO表示大模型需要结合应用场景去优化，并介绍了自研的RAG引擎QAnything。该引擎支持多语种内容提问，已在多个领域取得显著进展。同时，有道还展示了有道小P和Hi Echo 3.0在教育场景中的最新应用成果。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p>&nbsp;</p><p>“大模型的Super App尚未出现，但随时可能会诞生。”</p><p>&nbsp;</p><p>5月29日，网易有道分享了子曰教育大模型最新技术进展及三大AI创新应用：新一代知识库问答引擎QAnything、AI全科学习助手“有道小P”APP和虚拟人口语教练Hi Echo 3.0。</p><p>&nbsp;</p><p>现场，网易有道CEO周枫表示：“当前已经是‘模型即应用’的时代，但大模型也不是万能的，关键是抓住场景。”他认为，大模型的发展需要结合应用场景去迭代优化，在‘产模一体’的框架下去同步提升模型与产品。</p><p>&nbsp;</p><p>据介绍，目前在AI+在线营销、AI+效率工具、AI+娱乐等多个领域，有道都取得了显著进展。有道子曰大模型已经落地到了LLM翻译、AI作文指导、文档问答、语法精讲、虚拟人口语教练等一批应用上。</p><p>&nbsp;</p><p>截至目前，有道AI在线营销业务已连续六个季度实现超过50%的同比增长，AI翻译功能的使用人数已超过500万，使用次数达到2000万次。今年一季度，有道AI订阅服务收入持续增长，一季度会员销售额约5000万，同比增长140%。</p><p>&nbsp;</p><p></p><h2>自研RAG引擎</h2><p></p><p></p><h2>或突破文档问答模态</h2><p></p><p>&nbsp;</p><p>交流会现场，有道首席科学家段亦涛介绍了有道自研的RAG引擎——QAnything。此次升级，QAnything在私有化部署和智能体生成方面进行了一系列的提升，从单纯的文档问答进化成“企业AI大脑”。</p><p>&nbsp;</p><p>段亦涛指出，其原理是利用外部知识来辅助大模型的生成，提高正确性。虽然原理简单，但要落实到生产和生活中真正起到作用，其实还有很大的距离。为此，在开发QAnything之初，有道定下多个目标。</p><p>&nbsp;</p><p>首先是跨语种，QAnything支持中、英、日、韩四种语种的内容提问；其次要求检索质量足够高，有道在这个环节里优化所有环节的模型；另外是好用，QAnything支持一键安装和本地部署，同时支持各种格式文件，形成智能问答的形式。</p><p>&nbsp;</p><p>“QAnything的下一个发展目标是，突破文档问答单纯形式模态，变成真正能够理解企业业务逻辑、领域知识，融入到业务链条驱动决策提供智能能力的AI大脑。”段亦涛透露，为达成目标，现在他们已对QAnything进行三方面升级，包括领域适配能力、增加Agent的支持和内容生成能力“AI写手”的升级，使得文章分类准确度达到95.9%。</p><p>&nbsp;</p><p>通过领域定制适配，QAnything现在已经突破教育领域，拓展到医疗、互联网、智慧企业等行业；引入Agent能力后，QAnything允许用户根据自己的需求和业务特点来去定义整个系统，每个用户可以用这套能力来去定制个性化机器人。现在，该功能已经在有道领世业务落地。</p><p>&nbsp;</p><p>今年年初，QAnything正式对外开源，四个月在GitHub上获得了近1万个星标。据悉，在此期间，有道平均1-2周升级一次内核版本，不断优化和迭代算法的效果、稳定性和质量。目前，QAnything已经服务了20多个不同行业的上百家客户，超过3万用户将其用于各自的业务领域。</p><p>&nbsp;</p><p>另外，段亦涛表示，大模型向产业化发展后，国产大模型是否够用，取决于期望和要求。现在，大模型能够表现出来的能力，在实际应用中还没有完全被激发出来。“大家都在探索激发模型能力的方式，比如通过RAG等方式辅助其扬长避短，能够在应用中体现价值。”</p><p>&nbsp;</p><p></p><h2>“有道小P”和Hi Echo 3.0</h2><p></p><p>&nbsp;</p><p>会上，有道还展示了子曰教育大模型在教育场景中的最新应用成果，即可以在手机端便捷使用的“有道小P”和在内容、功能、教学等方面进行突破的Hi Echo 3.0。</p><p>&nbsp;</p><p>据悉，有道小P基于大模型知识问答能力，此前已在家庭辅导和语言学习两大教育场景落地。在此基础上，有道此次正式推出小P独立APP，可以在手机端便捷使用，解决随时答疑的需求。</p><p>&nbsp;</p><p>“小P所代表的大模型知识问答能力，有非常大的场景拓展可能性和增长潜力。” 有道高级副总裁吴迎晖表示，有道小P集成了多种场景下的互动答疑与交互功能，同时在知识记忆、多模态理解和逻辑推理等方面实现了提升。</p><p>&nbsp;</p><p>他介绍到，有道对小P基座模型定向优化的同时，还进行了知识库的100%扩容，新增了很多模态以及高质量的语料数据，此外还有很多细节的迭代和优化。</p><p>&nbsp;</p><p>目前，小P有免费和付费两种模式，基础功能是免费的，同时对外提供订阅服务。谈到现在的大模型“价格战”，周枫表示，“我们应用下来，大模型的成本是下降的，基本上一年至少下降一半。对于做云端服务的公司来说，目前降价是市场行为。”</p><p>&nbsp;</p><p>此外，会上亮相的新一代虚拟人口语教练Hi Echo 3.0，在功能、教学模式、虚拟人等方面进行创新，并携手雅思官方上线口语练考服务。此次升级还通过搭建真实的对话场景，重新设计了“背单词”的过程，未来还将推出“儿童模式”，并新增两个全新的“语伴”角色，打造陪伴式的教学环境。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</id>
            <title>都白学了！Mistral 的首个“开放”编程模型，精通Python、C等 80+ 语言，用220 亿参数赢了 GPT-4</title>
            <link>https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H9cy5L42CkYKtrDaWFUr</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 07:32:47 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软支持, AI初创公司, Codestral, 编码模型
<br>
<br>
总结: 5月29日，由微软支持、估值60亿美元的法国AI初创公司Mistral发布了其有史以来的第一个用于编码的“开放式”生成式AI模型，称为Codestral。该模型旨在帮助开发人员编写代码并与之交互，具备广泛的语言基础，可以在各种编码环境和项目中为开发人员提供帮助。Codestral在多个基准测试中名列前茅，性能优越，受到开发者社区的积极反馈。JetBrains研究员Mik... </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>5月29日，由微软支持、估值&nbsp;60&nbsp;亿美元的法国&nbsp;AI&nbsp;初创公司&nbsp;Mistral&nbsp;发布了其有史以来的第一个用于编码的“开放式”生成式&nbsp;AI&nbsp;模型，称为&nbsp;Codestral。</p><p>&nbsp;</p><p>与其他代码生成模型一样，Codestral&nbsp;旨在通过共享指令和完成&nbsp;API&nbsp;端点，帮助开发人员编写代码并与之交互。由于精通代码和英语，它还可用于为软件开发人员设计高级人工智能应用程序。值得一提的是，&nbsp;Codestral的参数要求很高，还受到一些许可证方面的使用场景限制。</p><p>&nbsp;</p><p>虽然该模型刚刚推出，尚未进行公开测试，但&nbsp;Mistral&nbsp;声称，Codestral&nbsp;在大多数编程语言上已经优于现有的以代码为中心的模型，包括&nbsp;CodeLlama&nbsp;70B、Deepseek&nbsp;Coder&nbsp;33B&nbsp;和&nbsp;Llama&nbsp;3&nbsp;70B。此外，Codestral&nbsp;在Kotlin语言的表现上似乎还超过了GPT-4-Turbo&nbsp;和&nbsp;GPT-3.5-Turbo。</p><p>&nbsp;</p><p></p><h2>精通&nbsp;80+&nbsp;编程语言</h2><p></p><p></p><h2>多个基准测试中名列前茅</h2><p></p><p>&nbsp;</p><p>首先，&nbsp;Codestral具备广泛的语言基础，可以在各种编码环境和项目中为开发人员提供帮助。据悉，Codestral&nbsp;在&nbsp;80&nbsp;多种编程语言的不同数据集上进行了训练，其中包括Python、Java、C、C++、JavaScript&nbsp;和&nbsp;Bash等流行语言。在&nbsp;Swift&nbsp;和&nbsp;Fortran&nbsp;等更特殊的语言上，Codestral&nbsp;也表现出色。</p><p>&nbsp;</p><p>而且，Codestral&nbsp;可以完成编码函数、编写测试和“填写”部分代码，以及用英语回答有关代码库的问题，可为开发人员节省时间和精力。与&nbsp;Codestral&nbsp;的互动，将有助于提高开发人员的编码水平，减少错误和&nbsp;bug&nbsp;的风险。</p><p>&nbsp;</p><p>性能方面，相比之前其他用于编码的模型，Codestral&nbsp;作为&nbsp;22B&nbsp;的模型，在代码生成的性能/延迟空间方面树立了新的标准。Mistral&nbsp;介绍，Codestral&nbsp;拥有&nbsp;32k&nbsp;的较大上下文窗口（竞争对手为&nbsp;4k、8k&nbsp;或&nbsp;16k），在代码生成的远程评估&nbsp;RepoBench&nbsp;中优于所有其他模型。</p><p>&nbsp;</p><p>同时，Mistral&nbsp;将&nbsp;Codestral&nbsp;与硬件要求更高的现有特定代码模型进行了比较。针对Python，其使用了四个基准测试：通过HumanEval&nbsp;pass@1、MBPP&nbsp;sanitised&nbsp;pass@1来评估&nbsp;Codestral&nbsp;的&nbsp;Python&nbsp;代码生成能力；CruxEval来评估&nbsp;Python&nbsp;输出预测能力；RepoBench&nbsp;EM来评估&nbsp;Codestral&nbsp;的远程存储库级代码完成能力。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c2cd8bf14b8091529981965456e44356.png" /></p><p></p><p>&nbsp;</p><p>在远程存储库级&nbsp;Python&nbsp;代码的完成情况上，Codestral&nbsp;以&nbsp;34%&nbsp;的准确率优于所有三个模型。同样，在评估&nbsp;Python&nbsp;代码生成的&nbsp;HumanEval&nbsp;和测试&nbsp;Python&nbsp;输出预测的&nbsp;CruxEval&nbsp;上，该模型分别以&nbsp;81.1%&nbsp;和&nbsp;51.3%&nbsp;的分数击败了竞争对手。它甚至优于&nbsp;HumanEval&nbsp;上用于&nbsp;Bash、Java&nbsp;和&nbsp;PHP&nbsp;的模型。</p><p>&nbsp;</p><p>为评估在&nbsp;SQL&nbsp;方面的性能，Mistral&nbsp;使用了&nbsp;Spider&nbsp;基准，Codestral&nbsp;以&nbsp;63.5%&nbsp;的得分位居第二。除了&nbsp;Python&nbsp;之外，Mistral&nbsp;还评估了&nbsp;Codestral&nbsp;在六种不同语言的&nbsp;HumanEval&nbsp;pass@1&nbsp;中的表现：&nbsp;C++、bash、Java、PHP、Typescript&nbsp;和&nbsp;C#，并计算了这些评估的平均值。值得注意的是，该模型在HumanEval的C++，C和Typescript上的表现不是最好的，但所有测试的平均得分最高，为61.5%，仅次于Llama&nbsp;3&nbsp;70B的61.2%。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb0c4cbb8344ddfb9d1aad9b9d608044.png" /></p><p></p><p>此外，Mistral&nbsp;使用&nbsp;Python、JavaScript&nbsp;和&nbsp;Java&nbsp;中的&nbsp;HumanEval&nbsp;pass@1&nbsp;评估了&nbsp;Codestral&nbsp;的中间填充性能，并将其与&nbsp;DeepSeek&nbsp;Coder&nbsp;33B&nbsp;进行了比较，后者的中间填充能力可立即使用，而&nbsp;Codestral&nbsp;的得分比它更高。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21a3e6c46e1f08dc174826936985712f.png" /></p><p></p><p>&nbsp;</p><p>在开发者社区的反馈中，JetBrains&nbsp;研究员Mikhail&nbsp;Evtikhiev还表示，“我们使用&nbsp;Codestral&nbsp;对&nbsp;Kotlin-HumanEval&nbsp;基准进行了测试，结果令人印象深刻。例如，在&nbsp;T=0.2&nbsp;的通过率方面，Codestral&nbsp;获得了&nbsp;73.75&nbsp;分，超过了&nbsp;GPT-4-Turbo&nbsp;的&nbsp;72.05&nbsp;分和&nbsp;GPT-3.5-Turbo&nbsp;的&nbsp;54.66&nbsp;分。”</p><p>&nbsp;</p><p></p><h2>220&nbsp;亿个参数</h2><p></p><p></p><h2>并不完全对外开放</h2><p></p><p></p><p>根据&nbsp;Mistral&nbsp;的官方介绍，Codestral&nbsp;是一个&nbsp;22B&nbsp;的开放式模型，采用Mistral&nbsp;AI&nbsp;新推出的非生产许可证&nbsp;（MNPL），允许开发人员将其技术用于研究和测试目的，在&nbsp;HuggingFace&nbsp;上可以下载。该公司通过两个&nbsp;API&nbsp;端点提供该模型：codestral.mistral.ai&nbsp;和&nbsp;api.mistral.ai。</p><p>&nbsp;</p><p>前者专为希望在其&nbsp;IDE&nbsp;中使用&nbsp;Codestral&nbsp;的&nbsp;Instruct&nbsp;或&nbsp;Fill-In-the-Middle&nbsp;路由的用户而设计，它带有一个在个人级别管理的&nbsp;API&nbsp;密钥，没有通常的组织速率限制，并且可以在八周的测试期间免费使用。后者则是更广泛的研究、批量查询或第三方应用程序开发的常用端点，查询按令牌计费。</p><p>&nbsp;</p><p>但该模型是否真的“完全开放”，还有待商榷。这家初创公司的非生产许可证禁止将&nbsp;Codestral&nbsp;及其产出用于任何商业活动，虽然有&nbsp;“开发&nbsp;”的例外，但也有注意事项：&nbsp;许可证明确禁止&nbsp;“员工在公司业务活动的背景下进行任何内部使用”。</p><p>&nbsp;</p><p>原因可能是&nbsp;Codestral&nbsp;部分训练内容受版权保护，Mistral&nbsp;在官方博文中没有证实或否认这一点，但这并不奇怪；有证据表明，这家初创公司以前的训练数据集包含受版权保护的数据。</p><p>&nbsp;</p><p>今年3月，由前&nbsp;Meta&nbsp;研究人员创立的&nbsp;AI&nbsp;模型评估公司&nbsp;Patronus&nbsp;AI&nbsp;发布了一项研究，展示了AI&nbsp;模型制作受版权保护内容的频率，测试的四个模型是&nbsp;OpenAI&nbsp;的&nbsp;GPT-4、Anthropic&nbsp;的&nbsp;Claude&nbsp;2、Meta&nbsp;的&nbsp;Llama&nbsp;2&nbsp;和&nbsp;Mistral&nbsp;AI&nbsp;的&nbsp;Mixtral。当时，Patronus&nbsp;AI&nbsp;的联合创始人兼首席技术官&nbsp;Rebecca&nbsp;Qian&nbsp;表示，“我们几乎在评估的所有模型中都发现了受版权保护的内容，无论是开源还是闭源。”</p><p>&nbsp;</p><p>不过无论如何，Codestral&nbsp;的这一问题可能也不值得太麻烦地讨论。据介绍，该模型有&nbsp;220&nbsp;亿个参数，需要一台强大的&nbsp;PC&nbsp;才能运行。(参数从本质上定义了人工智能模型处理问题的能力，比如分析和生成文本）。从参数规模的使用门槛来说，&nbsp;Codestral&nbsp;对大多数开发人员来说或许并不实用，在性能提升方面也是渐进式的。</p><p>&nbsp;</p><p></p><h2>代码模型的使用争议</h2><p></p><p>&nbsp;</p><p>Codestral&nbsp;的出现，可能会引发“关于依赖代码生成模型作为编程助手是否明智”的争论。</p><p>&nbsp;</p><p>至少在某些编码任务中，开发人员肯定会采用生成式AI工具。在&nbsp;2023&nbsp;年&nbsp;6&nbsp;月的一次&nbsp;Stack&nbsp;Overflow&nbsp;民意调查中，44%&nbsp;的开发人员表示，他们现在在开发过程中使用AI工具，26%&nbsp;的开发人员计划不久后使用。然而，需要注意的是，这些工具有明显的缺陷。</p><p>&nbsp;</p><p>今年1月，GitClear&nbsp;收集并分析了&nbsp;2020&nbsp;年&nbsp;1&nbsp;月至&nbsp;2023&nbsp;年&nbsp;12&nbsp;月期间编写的&nbsp;1.53&nbsp;亿行更改的代码。其发现，生成式AI开发工具正在导致更多错误代码被推送到代码库中，且这些助手并没有重构代码，而是提供了一键式重复现有代码的“诱惑”。当时，GitClear&nbsp;指出，2024&nbsp;年的问题是：谁来收拾残局？“对于代码的长期可维护性而言，也许没有比复制/粘贴代码更大的祸害了。”</p><p>&nbsp;</p><p>2月，Snyk&nbsp;的一项新研究警告说，生成式&nbsp;AI&nbsp;驱动的编码助手，如&nbsp;GitHub&nbsp;Copilot等通常会放大用户代码库中现有的错误和安全问题。“简单地说，当Copilot建议代码时，它可能会无意中复制邻居文件中存在的现有安全漏洞和不良做法。这可能导致不安全的编码实践，并为一系列安全漏洞打开大门。”Snyk的开发者关系和社区主管Randall&nbsp;Degges表示，大多数开发人员可能没有意识到AI编码助手可以很容易地从用户的代码库和开源项目中复制现有的安全问题。</p><p>&nbsp;</p><p>在2024计算机-人机交互会议（CHI&nbsp;2024）上，普渡大学的一项研究显示，OpenAI&nbsp;的&nbsp;ChatGPT&nbsp;对编程问题给出的答案，有52%包含错误信息，77%的答案比人类答案更冗长，78%与人类答案存在不同程度的不一致。</p><p>&nbsp;</p><p>但这些研究结果，或许并不能阻止Mistral&nbsp;等公司试图用他们的代码模型来赚钱。</p><p>&nbsp;</p><p>现在，Mistral&nbsp;已经在其&nbsp;Le&nbsp;Chat&nbsp;对话式人工智能平台上推出了托管版&nbsp;Codestral&nbsp;及其付费&nbsp;API。Mistral还表示，将致力于把Codestral构建到LlamaIndex、LangChain、Continue.dev和Tabnine等应用框架和开发环境中。“从我们最初的测试来看，Codestral是代码生成工作流程的绝佳选择，速度快、具有有利的上下文窗口，且&nbsp;instruct&nbsp;版本支持工具使用。”LangChain首席执行官兼联合创始人Harrison&nbsp;Chase在一份声明中表示。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://techcrunch.com/2024/05/29/mistral-releases-its-first-generative-ai-model-for-code/?guccounter=1">https://techcrunch.com/2024/05/29/mistral-releases-its-first-generative-ai-model-for-code/?guccounter=1</a>"</p><p><a href="https://mistral.ai/news/codestral/">https://mistral.ai/news/codestral/</a>"</p><p><a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model/">https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</id>
            <title>让智能设备更懂你，主动式AI正在崛起 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6gYD53yUW8bxMXu5Djpk</guid>
            <pubDate></pubDate>
            <updated>Thu, 30 May 2024 01:58:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能大模型, 主动式AI应用, 大模型持续更新, AGI概念
<br>
<br>
总结: 大模型的快速发展使了解最新技术动态和积极学习成为从业者的必修课。本周人工智能大模型在应用方面取得进展，主动式AI应用集中涌现。科研领域也在积极行动，为大模型透明度与可控性提供重要研究基础。同时，AGI概念引发热议，各行业对AGI的应用程度也备受关注。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，人工智能大模型在应用方面迎来了一系列令人瞩目的进展，特别是主动式AI应用的集中涌现。微软首发的Copilot+PC、小鹏汽车的AI天玑系统、美的发布的主动式全屋智能解决方案、联想集团推出的联想Yoga&nbsp;Slim&nbsp;7x和联想ThinkPad&nbsp;T14s&nbsp;Gen&nbsp;6，都标志着主动式AI展现出强大的应用潜力，智能科技将更加深入地渗透到人们的日常生活中。此外，科研领域也在积极行动，Anthropic为大模型透明度与可控性的提升提供了重要研究基础，“CCF-阿里妈妈科技袋基金”为学术界和工业界的融合交流提供了重要平台，有望促进更多创新成果的诞生。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>5月22日，百川智能发布最新一代基座大模型Baichuan&nbsp;4，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p></p><h4>多模态领域</h4><p></p><p>viva公司推出了一款Sora&nbsp;同架构视频生成模型，该模型对用户免费开放，具备文本生成视频、图片生成视频，4K分辨率放大，提示词自动优化功能。在视频生成方面表现出色，尤其是竖屏视频的质量，文生视频单次可生成&nbsp;5&nbsp;秒视频，图生视频为&nbsp;4&nbsp;秒视频，但在一致性和物理特性模拟上仍有提升空间。</p><p></p><h4>科研领域</h4><p></p><p>5月21日，Anthropic&nbsp;宣布成功从&nbsp;Claude&nbsp;3&nbsp;中提取数百万特征，在理解人工智能模型内部运作机制方面取得进展。该项研究对于提升大模型透明度与可控性方面意义重大。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>5月21日，在微软Build&nbsp;2024开发者大会上，GitHub&nbsp;推出首套由微软和第三方合作伙伴开发的&nbsp;GitHub&nbsp;Copilot&nbsp;扩展，允许任何人通过自然语言和更广泛的功能来提高代码开发速度。5月21日，微软宣布GPT-4o在Azure&nbsp;AI上普遍可用，还引入了一系列由知名合作伙伴和开源社区开发的大模型，进一步丰富了Azure&nbsp;AI平台的模型库。5月22日，百川智能推出成立之后的首款AI助手“百小应”。百小应不仅能够即时响应用户提出的各类问题，还具备快速阅读文件、整理资料、辅助创作、多轮搜索、定向搜索等功能，并能够在用户问题的基础上通过一系列更细致的提问来明确用户需求，给出更精准的答案。5月22日，腾讯云对外宣布旗下AI代码助手全面对外开放，开发者、开发团队、企业客户都可通过腾讯云官网进行在线体验。5月23日，天猫精灵在新品品鉴会上推出了天猫精灵X6智能音箱。该产品搭载Genie&nbsp;OS，通过人工智能大模型，为用户提供丰富的服务。</p><p></p><h4>智能体</h4><p></p><p>5&nbsp;月&nbsp;21日，微软宣布推出Team&nbsp;Copilot，将Copilot从个人助手扩展到团队助手。Team&nbsp;Copilot在团队协作中能够扮演任何角色，并扩展出Agent能力，成为不同领域的专家。</p><p></p><h4>端侧AI</h4><p></p><p>5月20日，小鹏汽车在520&nbsp;AI&nbsp;DAY发布会上宣布将向用户全面推送AI天玑系统，该系统推送将覆盖小鹏汽车所有在售车型。5月20日，美的正式发布主动式的全屋智能解决方案，并推出五大智慧场景及悦家全屋智能套系新品，全新升级美的Pro会员体系。5月21日，微软首发Copilot+PC。这是一款专为AI设计的新型Windows&nbsp;PC，将旗下AI助手Copilot全面引入了Windows系统，并且内置了OpenAI的GPT-4o模型。Copilot+PC的新功能Recall能够回忆并查找曾在显示屏上出现过的内容，使用Cocreator能够实时翻译约40种语言，实时生成或优化AI图像。Copilot+&nbsp;PC还配备了AI&nbsp;Agent，具备充分的实时交互能力。5月21日，联想集团推出首款搭载高通骁龙X&nbsp;Elite的下一代Copilot+&nbsp;PC——联想Yoga&nbsp;Slim&nbsp;7x和联想ThinkPad&nbsp;T14s&nbsp;Gen&nbsp;6，产品允许用户即使离线也可以使用大型语言模型功能。</p><p></p><h3>其他</h3><p></p><p>5月18日，CCF联合阿里妈妈正式发布“CCF-阿里妈妈科技袋基金”，致力于面向全球高校学者搭建产学研合作平台，增强学术界和工业界的融合交流，为社会和企业带来世界领先的创新成果。</p><p></p><p>报告推荐</p><p>AGI&nbsp;概念引发热议。那么&nbsp;AGI&nbsp;究竟是什么？技术架构来看又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？以上问题的回答尽在《中国AGI市场发展研究报告&nbsp;2024》，欢迎大家扫码关注「AI前线」公众号，回复「AGI」领取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/69f5f30dc6564327e46c59d969be2524.jpeg" /></p><p></p><p></p><p>报告预告</p><p>金融行业是否找到了AGI应用的最佳路径？取得了哪些具体应用成果?&nbsp;又存在哪些难以逾越的挑战与桎梏？金融机构一定要做AGI建设吗？如何考量金融AGI应用产品的效果？欢迎大家持续关注InfoQ研究中心即将发布的《AGI在金融领域的应用实践洞察》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593f81e592f22792c23938ef704be173.jpeg" /></p><p></p><p></p><h4>活动推荐</h4><p></p><p>FCon&nbsp;全球金融科技大会将于8月16日正式开幕，本次大会主题为「科技驱动，智启未来——激发数字金融内生力」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd1cae14cf901553aa4a65d29376cc26.png" /></p><p></p><p>咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>阅读原文链接：<a href="https://sourl.co/bpxhuz">https://sourl.co/bpxhuz</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</id>
            <title>奥特曼突然变身OpenAI “安全卫士”！网友：刚被实锤不关心安全还“心理虐待”，谁信啊</title>
            <link>https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zKfV26EFh09DPoDTAqo8</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:39:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI公司, 安全小组, GPT-4, 安全与保障委员会
<br>
<br>
总结: OpenAI公司成立了新的安全小组，旨在开发GPT-4的继任模型，并应对离职雇员的批评。该安全小组被称为安全与保障委员会，由公司高层领导和团队负责人组成，将向董事会提出关键安全建议，影响GPT-4模型的开发。新安全委员会的成立可能是为了回应重要人物的离职，公司内部存在安全文化和流程的变化，引发了外界质疑和讨论。资深专家认为应该先关注可见的风险。 </div>
                        <hr>
                    
                    <p>OpenAI公司已经成立新的安全小组，致力于开发GPT-4的继任模型，同时也是为了应对近期多位离职雇员对其商业意图的严厉批评。</p><p>&nbsp;</p><p>该团队被称为安全与保障委员会（Safety and Security Committee，简称SSC），领导层包括OpenAI公司CEO Sam Altman、委员会主席为Bret Taylor，外加Adam D’Angelo与Nicole Seligman等董事会成员。</p><p>&nbsp;</p><p>其他委员会成员则是来自OpenAI下辖各团队的负责人，包括曾经取代公司联合创始人Ilya Sutskever并担任了13天首席科学家的Jakub Pachocki。</p><p>&nbsp;</p><p>OpenAI公司表示，从现在起，该安全团队将就“关键的安全与保障决策”向董事会提出建议。这些决定可能会影响GPT-4继任模型的开发，即OpenAI在公告中提到的“下一个前沿模型”。</p><p>&nbsp;</p><p>在一个名“OpenAI 董事会成立安全委员会”的公告里，插入这样一条重要信息，着实很容易让人联想OpenAI是不是在借此暗暗转移大众视线，毕竟大家对GPT-5的期待是可以盖过对安全的关注的。</p><p>&nbsp;</p><p>该公司解释称，“我们很自豪能够构建并发布在行业拥有领先能力及安全水平的模型，也同样欢迎在这个重要时刻开展激烈的辩论。”但OpenAI并没有介绍具体讨论内容。</p><p>&nbsp;</p><p>这支安全团队的首要任务，就是在90天时间内制定出可供董事会审议的安全建议，不过Altman及其他董事对于建议内容仍拥有最终决定权。当然，OpenAI&nbsp;CEO及其他四位负责人同样可以在提交董事会之前对建议内容施加影响。</p><p>&nbsp;</p><p></p><h2>得到更多质疑：对谁安全？</h2><p></p><p>&nbsp;</p><p>新安全委员会的成立，很可能是为了回应本月早些时候Sutskever与Jan Leike两位重量级人物的高调离职。随着他们离开OpenAI，公司内负责评估长期AI安全问题的超级对齐小组也宣告解散。</p><p>&nbsp;</p><p>在离职之前，Leike一直担任超级对齐小组的负责人。几乎在OpenAI发布通告的同时，Leike宣布加入了Anthropic。Anthropic 由前 OpenAI 工程师创立，创始人出走就是因为双方安全理念存在差异。Leike 在 Anthropic 依旧负责超级对齐。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e51e9e8df678a3f9f11e4cafc09c7f6f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Leike在超级对齐团队解散的前一天曾经表示，“过去几年以来，安全文化和流程已经让位于公司对快速发布新品的坚持。我们早就应该认真思考通用人工智能（AGI）的影响了……OpenAI必须成为一家以安全为先的AGI厂商。”</p><p>&nbsp;</p><p>该发言还引来马斯克的“补刀”：言外之意就是，安全并不是OpenAI现在的首要任务。</p><p>&nbsp;</p><p>但这个新部门的成立并没有扭转网上OpenAI一直以来的负面安全舆论，反而引来了网友更多质疑。“好吧，我想 OpenAI 的产品现在对于 Sam Altman 和他的目标来说是安全的。”有网友略显无奈地说道。</p><p>&nbsp;</p><p>“利益冲突。这样的安全团队从定义上来说难道不应该是独立的吗？”有人质疑道。对此网友调侃成：“是的，应该有一个治理架构，确保首席执行官遵守以下原则……哎呀，他们已经摧毁整个组织架构了。”</p><p>&nbsp;</p><p>也有网友称：“至少 OpenAI 现在有了一个‘安全’团队。”显然还是觉得OpenAI 有些敷衍。</p><p>&nbsp;</p><p>当然，也有人期待这个安全委员会未来会做出什么成绩，毕竟Altman的信徒大有人在。“我仍然可以让 ChatGPT 告诉我如何制造炸弹。所以，是的，我迫不及待地想看到安全进展。”</p><p>&nbsp;</p><p>对此，行内资深专家告诉“AI 前线”，这更多是公司内部资源分配的问题。OpenAI 一直讲闭源才安全，有人认为AGI要来了、机器要毁灭人类，所以安全太重要了，要赶紧把安全做好，需要投入一定比例的资源进去。但是从一个商业公司的角度看，企业不可能停下工作去做各种安全方面的事情，更多还是要不停开发布新的模型，然后满足客户的需求，跟其他公司竞争。</p><p>&nbsp;</p><p></p><h2>前董事会成员“插刀”</h2><p></p><p>&nbsp;</p><p>同样在今天，OpenAI前董事会成员Helen Toner和Tasha McCauley 的联名文章，再次将Altman不关心安全的问题推上浪尖。</p><p>&nbsp;</p><p>“由于Altman个人长期以来的行为模式，董事会维护公司使命的能力受到了越来越大的限制。据我们了解，这些行为不仅削弱了董事会对关键决策和内部安全协议的监督能力，还引发了其他问题。”</p><p>&nbsp;</p><p>根据爆料，多位高层领导私下向董事会表达了深切的担忧，他们认为Altman营造了一种“撒谎的有毒文化”，并涉嫌“心理虐待”行为。Toner 还表示，Altman“多次”向董事会撒谎，并且“隐瞒信息”，她甚至是在 Twitter 上知道 ChatGPT 发布的消息的。</p><p>&nbsp;</p><p>当董事会意识到 Altman 需要被换掉时，Toner 表示，如果Altman 发现了这个，很明显他会“竭尽全力”阻止董事会反对他。她声称他“开始对其他董事会成员撒谎，试图将我从董事会中赶出去。”</p><p>&nbsp;</p><p>“我们非常小心，非常慎重地选择通知谁，除了我们的法律团队之外，几乎没有任何人提前通知过我们，所以这才把消息拖到了 11 月 17 日。”Toner谈及去年的OpenAI政变时说道。</p><p>&nbsp;</p><p>两人指出，自从Altman重返公司以来，一些发展动态令人担忧，包括他重新加入董事会，以及OpenAI一些专注于安全领域的高级人才的离职。这些情况对于OpenAI在自我治理方面的实验来说，似乎预示着一些不利的影响。</p><p>&nbsp;</p><p>有趣的是，刚刚成为亿万富翁不久的Sam Altman 承诺捐出自己大部分财富，表示将继续专注于“支持有助于为人们创造富足的技术”。</p><p>&nbsp;</p><p></p><h2>资深专家：应该先关注看得见的风险</h2><p></p><p>&nbsp;</p><p>没有什么比Sutskever和Leike等人扮演的重要角色更能表明OpenAI致力于其使命的了。Sutskever和Leike是技术专家，他们长期致力于安全，并明显真诚地愿意在必要时要求OpenAI改变方向。</p><p>&nbsp;</p><p>Sutskever 在2019年的采访中当记者刚刚说道，“你们说，‘我们要建立一个通用人工智能，’”时，Sutskever 立即插话强调：“我们将尽一切可能朝这个方向努力，同时确保以一种安全的方式做到这一点。”</p><p>&nbsp;</p><p>随着他们的离职，很多人问他们在OpenAI看到了什么，但没有得到答案。</p><p>&nbsp;</p><p>不同于 Sutskever、Leike等人坚决捍卫AI安全的态度，有些大佬并不那么重视，比如图灵奖得主Yann LeCun。</p><p>&nbsp;</p><p>当时，在LeCun在对Jan Leike的回贴中，他表示当前对AI安全担忧的紧迫感是过度夸张的，类似于在涡轮喷气发动机发明之前，急于解决跨洋飞行的安全问题。所以难怪OpenAI 解散对齐团队。在LeCun看来，智能系统的进化需要多年时间，应该通过反复的工程改进逐步提高其智能和安全性，而不是过度担忧未来可能的风险。</p><p>&nbsp;</p><p>同时，上述专家也告诉“AI 前线”，从开源角度讲，我们离“AGI 来了、毁灭人类”这些还很远，他并不认可这些说法。</p><p>&nbsp;</p><p>该专家表示，目前，AI 安全上的风险更多来自大家看得见、摸得着的地方，比如数据集的偏见和毒化给使用模型带来很多挑战：让ChatGPT 画一个剥了皮的荔枝，由于ChatGPT 根本不知道荔枝剥皮了什么样，所以它就是随便画；又如让Stable Diffusion 等海外模型画北京城市，它会画一个破破烂烂的四合院。</p><p>&nbsp;</p><p>“目前，像这种数据集的 bias 其实没有得到很多关注。但这种可能是更重要的，与超级对齐不是一回事儿，”该专家说道。</p><p>&nbsp;</p><p>该专家也分析称，从更大层面来说，美国也在渲染 AI 安全问题，比如AI自动生成恶意软件、自动攻击各种网站，但实际上我们都知道，代码生成的能力远远没有到这种程度，所以这种渲染也是为了防止模型出口，不让非常厉害的模型技术扩散出去。美国炒作这个事情，也有想要得到中国类似“不利用这个技术开发武器”承诺的意图。</p><p>&nbsp;</p><p>“安全是一个非常大的叙事，每个人在不同的立场都会有不同的看法。”当前应该把主要精力投入到哪个方面？显然OpenAI当前掌门人有自己的答案，其他公司也有自己的答案。但答案正确与否，还需要时间验证。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://openai.com/index/openai-board-forms-safety-and-security-committee/">https://openai.com/index/openai-board-forms-safety-and-security-committee/</a>"</p><p><a href="https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/">https://www.theregister.com/2024/05/28/openai_establishes_new_safety_group/</a>"</p><p><a href="https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5">https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</id>
            <title>如何降低数据消费门槛，让非技术用户也能成为数据分析专家？</title>
            <link>https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0CF4oJYiP21jXOVjvkeo</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 09:05:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据分析, 企业发展, 大模型, SwiftAgent
<br>
<br>
总结: 在数字化时代，数据分析已成为企业发展的重要驱动力。企业需要克服数据获取困难和高阶分析难等挑战，形成基于数据的决策文化，打破数据孤岛，促进跨部门协作。SwiftAgent 的出现实现了数据分析的民主化，让人人都能成为数据分析专家，提高了数据分析的效率和价值。 </div>
                        <hr>
                    
                    <p>在数字化时代，数据分析已成为企业发展的重要驱动力。对于企业而言，数据价值毋庸置疑，更重要的是如何对其进行分析和利用。从客户运营到战略决策，各行各业都离不开数据分析。本期《极客有约》栏目邀请到了数势科技数据智能产品总经理岑润哲，与他一起探讨大模型时代下，如何以 SwiftAgent 革新企业数据分析范式，让人人都能成为数据分析专家。</p><p></p><p>数势科技是行业领先的数据智能产品提供商，为全球大金融、泛零售和高科技制造企业提供大模型增强的智能产品体系，促进企业数字化转型。</p><p></p><p>如何让企业的非技术人员跨越数据的门槛，成为数据分析专家？数据分析的现状和未来发展趋势又如何？SwiftAgent 的出现如何帮助非技术用户？它的技术特点和优势是什么？精彩观点总结如下：</p><p>企业在进行数据分析时面临的主要挑战包括数据获取困难、高阶分析难等，企业数据分析的效率和准确性极具挑战。非技术人员应从业务需求出发，逐步引入数据分析工具和理论，并通过实际操作提升分析思路。企业从组织层面进行变革，形成基于数据的决策文化，打破数据孤岛，并促进跨部门协作。SwiftAgent 通过引入指标语义层和大模型能力，实现了数据分析的民主化，让不同岗位的人员都能实现实时、灵活、精确的数据分析，缩短从数据获取到决策的链路，提高了数据分析的效率和价值。</p><p></p><p>完整视频参看：</p><p></p><p></p><p></p><p></p><h2>企业数据分析的机遇与挑战并存之道</h2><p></p><p></p><p>&nbsp;InfoQ：润哲老师，您在数据分析领域有着非常丰富的经验，您认为企业用户在进行数据分析的时候，通常会遇到哪些困难和挑战？这些挑战对于企业的发展有哪些影响？</p><p></p><p>岑润哲：在我们服务的企业客户中，数据分析一般分为四个步骤。首先是数据收集与获取（Data Query）。企业用户、包括分析师和业务人员在分析前需从数据仓库或业务系统库中提取数据，由于他们通常缺乏数据分析技能，不熟悉 SQL 或底层数据表，这在提取中造成了最大的不便，这是许多客户的痛点。</p><p></p><p>数据分析师或商业分析师具备一定的数据分析技能，会编写 SQL，但底层数据仓库的表结构混乱，熟悉的表有限。当他们完成一个领域的指标分析后，转向另一领域时，需重新梳理表逻辑，这是数据获取阶段的一个难点。</p><p></p><p>第二步是数据获取后，使用专业分析工具或方法进行高阶分析（Data Analytics）。难点不在于选择工具，而在于根据不同场景选用合适的分析范式。例如，销量分析时可能需要同环比分析、排序分析；指标异常时需归因分析能力；转化率异常时需漏斗分析能力。工具学习不难，难的是找到适合业务板块的分析范式。</p><p></p><p>第三步是数据获取和高阶分析后，如何快速解读数据。传统 BI 工具难以直接将数据转化为洞察（insight）。我们的产品结合大模型的语义理解能力，可快速挖掘商业洞察。例如，从几千行 Excel 数据中迅速识别产品或客户群的问题。我们希望结合指标语义层和大模型辅助，优化数据洞察和解读，提供业务方所需的分析解读。</p><p></p><p>最后是数据权限和安全性问题。 若数据权限开放给所有分析师或业务人员，可能引发数据安全性和隐私问题。在数据复盘过程中，需确保不同角色和用户能获取适当权限的数据集。</p><p>总体来说，数据获取、高阶分析、数据解读理解、数据权限管控，是不同行业客户在数据分析中面临的主要问题。</p><p></p><p>&nbsp;InfoQ：这些非技术用户在数据分析的过程中，他们最大的困惑是什么呢？</p><p></p><p>岑润哲：对于非技术人员，我们将其定义为偏业务人员，他们擅长业务流程和合规性，但在将业务思维与底层数据表关联时存在难题。以零售行业为例，非技术人员如门店店长或督导需要分析经营数据、客户画像和商品销售数据，但若直接提供数据表，他们难以进行分析。</p><p></p><p>非技术人员不仅需要工具，更需要将数据分析与业务场景结合。我们认为可以通过指标语义层和大模型的生成能力，帮助客户提出更精准的问题。 例如，构建好指标体系后，大模型能生成结构化问题，如从经营视角分析门店流水和毛利，或从服务水平视角分析大众点评评分，以及客群画像等。</p><p></p><p>当企业内部的指标和维度体系建立完善，结合大模型，能够输出标准的分析思路。这与传统 BI 工具相比，是一个双向过程：大模型不仅能响应问题，还能提供分析思路。比如，大模型可以根据公司的指标和维度生成批量问题，用户再从中筛选最关心的问题进行分析。</p><p></p><p>将大模型分析助手与用户请求结合后，我们从单向的 BI 分析模式转变为可交互模式，用户可以向大模型提问，大模型也能反问用户，提供分析视角。这种模式优化了非技术人员在分析思路上的痛点，大模型的出现增强了分析思路扩展的能力。</p><p></p><p>大模型不仅提供分析思路，还能激发用户的分析欲望，形成良好的交互形式，相当于由顾问提供建议。这是大模型在智能分析领域带来的最大改变。</p><p></p><p>&nbsp;InfoQ：您认为企业在数据分析的过程中还有可能会遇到哪些管理上或者流程上的障碍或者挑战吗？</p><p>岑润哲：结合我们服务过的客户，我总结了三种主要障碍。</p><p></p><p>首先是组织文化的障碍。许多组织尚未形成基于数据的决策文化，决策更多由高层领导凭经验作出，这会影响分析工作的价值。如果组织文化不以数据驱动决策，即使分析质量再高也难以发挥作用。</p><p></p><p>第二是企业内部数据体系的孤岛问题。例如，在泛零售行业，线上线下渠道的数据可能未打通，或埋点数据与交易数据之间存在隔离。这导致无法进行跨部门或跨领域的分析，如无法评估营销活动的效果。</p><p></p><p>第三是跨部门协作问题。不同部门之间可能存在边界和利益问题，例如活动运营部门需要客户运营部门的数据时，可能难以获得必要的支持。这种跨部门协作的障碍，使得进行复杂的分析或关联分析变得困难。</p><p>&nbsp;InfoQ：数势科技就作为数据智能产品的提供方，您认为帮助企业解决这些问题的核心思路是什么呢？企业应该重点关注哪些方面？</p><p></p><p>岑润哲：我们公司的核心理念和使命是更新现行的数据分析范式，从集中式转变为民主化。目前，企业的分析逻辑多是粗犷或集中式的，业务方需向数据团队提出需求，然后等待数据提供。这种模式下，数据解读和高阶分析强烈依赖商业分析师或数据分析师团队，存在较大的隔阂。</p><p></p><p>引入指标语义层和大模型能力后，我们希望企业内部的每个员工能够成为“数据公民”，这意味着他们即使不懂数据，也能基于业务分析思路，获取和探索企业内部的数据资产。大模型的出现有助于每个数据公民进行大规模自定义数据分析，极大缩短从数据获取到决策的链路。</p><p></p><p>未来数据分析的主要方向是从研发与业务割裂的形式，转变为业务方在研发设定标准后，自行利用大模型辅助获取和挖掘数据。</p><p></p><p>&nbsp;InfoQ：您刚刚提到数据公民的概念非常有趣，它会对决策层的思维导向和人才培养产生积极影响吗？</p><p></p><p>岑润哲：是的。例如我们曾为一家鞋类企业提供了 基于指标语义层的完整分析框架， 他们可以分析不同客群的偏好。通过 Know Your Customer 标签，发现 25 至 29 岁女性对 PVC 材质、鞋跟高度在 3 到 5 厘米的鞋子有很高的偏好。这些信息在之前是无法获取的，因为他们不知道公司内部有这些客群和商品标签。</p><p></p><p>现在，借助大模型工具，业务方可以提出更有针对性的问题，并驱动分析过程。他们更了解产品的销售情况，能够通过数据分析找出哪些客群对特定类型或特征的鞋子有更高的转化率，进而讨论投放策略或营销策略，形成一个正循环。</p><p></p><p>传统仅从技术角度分析数据表可能无法获得这样的洞察。但现在，业务方有能力自行分析不同的标签和指标，这使他们能够更好地理解企业内部如何提升销售，实现业务驱动的数据分析和决策。</p><p></p><p>&nbsp;InfoQ：前不久数势科技在 AICon 大会现场发布了 SwiftAgent 2.0 版本，是否可以现场演示？</p><p></p><p>岑润哲：好的，以下是产品的 demo 视频，供大家了解，可留言或点击“阅读原文”申请产品试用。</p><p></p><p></p><p></p><p>InfoQ：在现场发布的时候，数势科技也提到了大模型和 Agent 将会颠覆企业数据分析与决策范式，我想请问为什么这样来表达呢？</p><p></p><p>岑润哲：我们可以回顾一下大模型和 Agent 架构出现之前的数据分析流程。传统上，数据分析链路较长，从提出需求到数据团队获取数据集、配置 BI 工具并搭建驾驶舱，整个过程耗时且复杂。核心问题在于，所有工具的使用都需要人工配置和梳理，工作量较大且重复，效率较为低下。</p><p></p><p>Agent 架构结合大模型后，展现出其优势，尤其是 Agent 在工具调用方面的能力。Agent 不仅能理解用户的自然语言需求，还能自动规划任务执行步骤。</p><p></p><p>例如，用户提出数据分析请求，Agent 可理解用户意图、获取所需地区的销售明细、进行排序和高阶分析、对比 TOP3 产品。这不仅涉及任务拆解和规划，还包括与知识库的协同和工具串联。</p><p></p><p>这种架构带来四个好处：首先，用户不再需要学习工具配置，因为大模型已经掌握了工具调用的方法；其次，通过 Agent 统一规划，提高了效率，避免了在不同工具间切换的繁琐；第三，交互性得到改善，用户通过自然语言与系统交互，降低了使用门槛；最后，简化了操作，将复杂逻辑留给程序处理。</p><p></p><p>企业可通过 Agent 机制调度内部不同工具，形成有效串联，降低了业务方学习和使用工具的时间与门槛，这是我认为它会颠覆企业分析决策范式的原因。</p><p></p><p></p><h2>SwiftAgent 开启智能数据分析新篇章</h2><p></p><p></p><p>InfoQ：SwiftAgent 为什么能够在众多的大模型和数据分析产品中脱颖而出？</p><p></p><p>岑润哲：SwiftAgent 被定义为由大模型 Agent 机制驱动，并结合指标标签语义层的智能分析产品。它让企业非技术人员——如企业管理者和业务人员可准确、即时、个性化地进行数据査询和业务洞察，提升决策能力，实现数据价值普惠化。</p><p></p><p>其核心技术亮点主要分为三个层面：通过构建指标标签语义层，统一了数据和业务语言，避免了大模型的幻觉；结合 Agent 架构，赋予产品反思、推理和规划的能力；通过自研的加速引擎，提升了前端问询的响应速度。</p><p></p><p>指标标签语义层： 我们采取的技术路线不是直接将用户自然语言请求转化为 SQL。因为企业内部数据标注和治理程度不一，直接转化的准确率很低。我们构建了指标和标签语义管理层，统一了数据语言和业务语言，解决了大模型的幻觉问题，提高了准确率，并帮助企业建立了一套指标和标签体系，解决了数据统一问题。</p><p></p><p>Agent 产品架构设计：Agent 架构能够进行思考、推理和反思，解决复杂任务执行问题。自然语言形式的灵活性让用户可能提出不可预测的问题，我们设计了合理的 Agent 架构，使用户能够以自然语言形式灵活、高效地获取数据。我们在 Agent 架构层面做了大量的调研和研发，提升处理复杂问题的能力。</p><p></p><p>数据加速引擎： 我们自研了 Hyper Computing Acceleration Engine，提升对话式分析的响应速度。例如，针对用户常问的商品品类、城市等维度的销售额或毛利，进行预聚合和预计算，使得即使面对百亿级数据量的订单表，也能快速响应用户查询。</p><p></p><p>&nbsp;InfoQ：SwiftAgent 的产品优势 / 壁垒是什么？</p><p></p><p>岑润哲：除了上述提到的 Agent 机制和数据加速引擎，SwiftAgent 还拥有结构化与非结构化数据联动分析的能力。我们将非结构化信息（如用户评论、直播数据）抽象化，转化为结构化数据，并与企业内部指标进行关联分析，提供更全面的分析。</p><p></p><p></p><h2>智能数据分析市场的发展前景</h2><p></p><p></p><p>InfoQ：除了零售行业以外，还有哪些行业已经上线 SwiftAgent？成效如何? 可以分享几个案例吗？</p><p></p><p>岑润哲：除了零售行业，我们也在金融行业如银行、证券公司，以及高端制造行业实现了应用落地。</p><p>以某知名城商为例，分行行长通常关注贷款余额、不良率等指标的波动。传统上，他们需要向分析团队提出需求，由团队提供分析结果。现在，通过上线智能分析产品，领导可以直接通过自然语言查询获取信息，同时经营分析团队可以利用沉淀的分析模板和思路，加速从数据到分析报告的转化。</p><p></p><p>我们也与头部证券机构合作，帮助客户经理分析他们管理的高净值客户。例如，理财顾问或投资经理管理 200 个客户时，可以通过自然语言查询，快速了解哪些客户存在流失风险，或关注行业政策变动，以及持仓标的的变化。这样的分析能力，如果依靠传统 CRM 工具，可能需要花费大量时间。而通过 SwiftAgent 与客户标签、指标联动，构建了从数据洞察到决策的完整链路，效率提升 80%。此外，我们还将优秀客户经理的 SOP 沉淀在知识库中，帮助新员工快速了解如何应对不同情况，比如客户亏损时的安抚策略。这样，数据分析不仅帮助业务人员理解数据，还指导他们基于数据采取行动。</p><p></p><p>&nbsp;InfoQ：对于金融和央国企而言，信创和数据安全是重点关注的方向。数势科技在这两方面有些认证或适配？</p><p></p><p>岑润哲：数势科技是北京信创工委会会员单位，已经完成国家高新技术企业认证、中关村高新技术企业认证、ISO9000 质量管理体系认证、信息安全管理体系认证、信息技术服务管理体系认证、信息系统安全登记保护三级、麒麟操作系统信创认证、达梦数据库信创认证、人大金仓信创认证和 CMMI 等资质认证。产品充分满足金融企业和国央企的部署需求。目前合作的国产大模型都已完成算法备案。另外，SwiftAgent 已首批通过中国信通院针对大模型驱动的数据分析工具的专项测试，获得权威认可。</p><p></p><p>&nbsp;InfoQ：在您看来未来智能数据分析市场规模是将会是怎么样的？</p><p></p><p>岑润哲：智能数据分析市场将是大模型落地的重要场景。数据分析智能化能够充分利用大模型的规划和拆解能力，并与企业内部数据联动，产生化学反应。企业不仅希望提升数据分析体验，还希望降低开发需求，将数据智能化作为核心战略。</p><p></p><p>在客户需求层面，大金融、泛零售和高科技制造是我们的重点服务领域。这些行业的企业对大模型的应用已经从观望学习阶段过渡到试点实施阶段。许多头部企业，尤其是金融、国央企、零售和能源企业，已经开始大量招标，希望在数据分析、知识库、营销和 RPA 等多个场景中应用大模型。数据分析场景特别受到重视，占企业需求的 80% 以上。</p><p></p><p>另外，国家层面也在推动数据资产入表，鼓励企业将数据资产作为无形资产量化并反映在财务报表中，这也将促进大模型在数据管理和分析领域的结合。</p><p></p><p>&nbsp;InfoQ：您作为资深专家，请问对于想要提高数据分析能力的非技术用户，有哪些建议呢？</p><p></p><p>岑润哲：首先，非技术用户应以业务需求为出发点，学习基础统计学和数据分析概念，构建分析能力的基础。例如，零售客户，可从业务场景切入，如教店长如何分析门店数据以提升业绩。</p><p></p><p>其次，非技术人员应先理解自身管理的业务逻辑流程，再逐步引入数据分析工具和理论。 建议通过实际操作小项目来提升分析思路，如门店经理、财务经理或 HR 可以分析与自己工作相关的数据。</p><p></p><p>最后，还可以加入专业论坛或群体，关注不同领域的分析博客，以帮助构建人脉并提升对数据的理解力。 业务人员转为分析师往往潜力巨大，因为他们对业务流程有深刻理解，具备在数据分析和业务洞察领域的天然优势。</p><p></p><p>数据分析不仅限于互联网公司或运营、财务风控等领域，数据分析将持续渗透到企业各个部门，提升决策效率，这是未来的大趋势，也是我们数势科技的愿景和使命。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</id>
            <title>干货下载 | 腾讯云ES RAG如何支持微信读书实现“AI问书”？</title>
            <link>https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/v0Y1tppWvfGPvtpMqgTY</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 06:19:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/95/95248354db49afa8d5efac3f71431c10.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8076563d8941632e9a8f79eed8fcc96.webp" /></p><p></p><p>干货下载页面点击：<a href="https://qdrl.qq.com/TJBspHYw">https://qdrl.qq.com/TJBspHYw</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</id>
            <title>浪潮信息发布 “源2.0-M32” 开源大模型，大幅提升模算效率</title>
            <link>https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fXOqLr5i5lGx8mqJacPf</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 源2.0-M32, 门控网络, 专家模型, 模算效率
<br>
<br>
总结: 源2.0-M32是一个包含32个专家的混合专家模型，采用了门控网络结构来调度专家，实现高效计算。在模型训练和推理过程中，源2.0-M32表现出色，主要通过建模专家之间的协同关系来提升模型精度和模算效率。 </div>
                        <hr>
                    
                    <p>5月28日，浪潮信息发布“源2.0-M32”开源大模型。“源2.0-M32”在基于“源2.0”系列大模型已有工作基础上，创新性地提出和采用了“基于注意力机制的门控网络”技术，构建包含32个专家（Expert）的混合专家模型（MoE），并大幅提升了模型算力效率，模型运行时激活参数为37亿，在业界主流基准评测中性能全面对标700亿参数的LLaMA3开源大模型。</p><p></p><h3>大模型技术解读</h3><p></p><p></p><p>在算法层面，源2.0-M32提出并采用了一种新型的算法结构：基于注意力机制的门控网络（Attention Router），针对MoE模型核心的专家调度策略，这种新的算法结构关注专家模型之间的协同性度量，有效解决传统门控网络下，选择两个或多个专家参与计算时关联性缺失的问题，使得专家之间协同处理数据的水平大为提升。源2.0-M32采用源2.0-2B为基础模型设计，沿用并融合局部过滤增强的注意力机制（LFA, Localized Filtering-based Attention），通过先学习相邻词之间的关联性，然后再计算全局关联性的方法，能够更好地学习到自然语言的局部和全局的语言特征，对于自然语言的关联语义理解更准确，进而提升了模型精度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7749b68f0ae25cc003fe38680e899857.png" /></p><p>Figure1-&nbsp;基于注意力机制的门控网络（Attention Router）</p><p></p><p>在数据层面，源2.0-M32基于2万亿的token进行训练、覆盖万亿量级的代码、中英文书籍、百科、论文及合成数据。大幅扩展代码数据占比至47.5%，从6类最流行的代码扩充至619类，并通过对代码中英文注释的翻译，将中文代码数据量增大至1800亿token。结合高效的数据清洗流程，满足大模型训练“丰富性、全面性、高质量”的数据集需求。基于这些数据的整合和扩展，源2.0-M32在代码生成、代码理解、代码推理、数学求解等方面有着出色的表现。</p><p></p><p>在算力层面，源2.0-M32采用了流水并行的方法，综合运用流水线并行+数据并行的策略，显著降低了大模型对芯片间P2P带宽的需求，为硬件差异较大训练环境提供了一种高性能的训练方法。针对MoE模型的稀疏专家计算，采用合并矩阵乘法的方法，模算效率得到大幅提升。</p><p></p><p>基于在算法、数据和算力方面全面创新，源2.0-M32的性能得以大幅提升，在多个业界主流的评测任务中，展示出了较为先进的能力表现，在MATH（数学竞赛）、ARC-C（科学推理）榜单上超越了拥有700亿参数的LLaMA3大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18c12d432b7f6f84dcb3fc48081ae17a.png" /></p><p>Figure2 源2.0-M32业界主流评测任务表现</p><p></p><p>源2.0-M32大幅提升了模型算力效率，在实现与业界领先开源大模型性能相当的同时，显著降低了在模型训练、微调和推理所需的算力开销。在模型推理运行阶段，M32处理每token所需算力为7.4GFLOPs，而LLaMA3-70B所需算力为140GFLOPs。在模型微调训练阶段，对1万条平均长度为1024 token的样本进行全量微调，M32消耗算力约0.0026PD(PetaFLOPs/s-day)，而LLaMA3消耗算力约为0.05PD。M32凭借特别优化设计的模型架构，在仅激活37亿参数的情况下，取得了和700亿参数LLaMA3相当的性能水平，而所消耗算力仅为LLaMA3的1/19，从而实现了更高的模算效率。</p><p></p><p>浪潮信息人工智能首席科学家吴韶华表示：当前业界大模型在性能不断提升的同时，也面临着所消耗算力大幅攀升的问题，对企业落地应用大模型带来了极大的困难和挑战。源2.0-M32是浪潮信息在大模型领域持续耕耘的最新探索成果，通过在算法、数据、算力等方面的全面创新，M32不仅可以提供与业界领先开源大模型相当的性能，更可以大幅降低大模型所需算力消耗。大幅提升的模算效率将为企业开发应用生成式AI提供模型高性能、算力低门槛的高效路径。</p><p></p><h3>技术创新点剖析：</h3><p></p><p></p><p>Llama系列模型的精度从Llama1到Llama3显著提升，Llama3的精度处于领先地位，特别是其700亿参数的模型在每个Token的推理和算力上达到140GFLOPS。尽管如此，Llama3在推理时的算力开销较大，也就是说单位算力下的精度表现较差。</p><p></p><p>在采访环节，吴韶华回答了记者问，关于32个专家的优势及挑战，吴韶华解释道，当前很多大模型工作采用8个专家的架构，但浪潮信息选择32个专家，核心原因是模算效率。实验表明，在他们的LFA加上Attention Router架构中，专家数量从8增加到32时，精度显著提升，而算力开销保持不变。这是因为激活专家的数量仅为2个。此外，单个专家参数量为2B，这样控制模型参数量有利于企业应用的模算效率。结果显示，这一选择在相同精度下实现了低算力消耗。</p><p></p><p>同时，由于激活的专家数量为2个，通过Attention Router机制考虑专家间协同，专家数量的增加使得每个专家或专家组能够学习更多有针对性的信息。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/20/9d/20109819dd1cb3e6794a287d3648e49d.jpg" /></p><p></p><p>模算效率与成本控制也是此次大模型发布的关键讨论点。吴韶华强调，算力是当前大模型发展的核心瓶颈。MoE结构模型通过扩展专家数量，在固定算力下获得更高精度。在多元芯片的使用上，浪潮信息的EPAI软件提供相关工具，支持多元算力架构，降低用户迁移设备的难度和成本。这些创新措施有助于降低用户试错成本，实现应用落地。</p><p></p><p>高模算效率意味着在单位算力投入下获得更高的精度回报，这对于大模型训练和推理都非常有利。“源2.0-M32”模型旨在通过创新算法提升精度并降低同等精度下的算力开销，大幅提升基础模型的模算效率。“源2.0-M32”是一个包含32个专家的混合专家模型，采用了Attention Router结构来调度专家，实现高效计算。在模型训练和推理过程中，“源2.0-M32”表现出色，Attention Router结构主要是通过建模专家之间的协同关系来提升模型精度。</p><p></p><p>M32模型的训练数据筛选与优化也是核心技术点，吴韶华详细介绍了浪潮信息在训练数据方面的策略。浪潮信息从源1.0开始构建了互联网自然语言文库，并开发了一套数据清洗平台。对于稀缺数据（如中文数学数据），通过数据合成工具补充。M32模型引入了大量代码数据和互联网数据，提升数据的多样性和质量。代码数据不仅对模型的代码能力有益，还能帮助解决数学问题和推理问题。最终，源2.0-M32模型在精度和算力开销方面优于Llama3。</p><p></p><p>在应用落地方面，源2.0-M32增强了小样本学习能力，通过少量样本就能显著提升模型能力。相较于微调而言，这是一种轻量化支撑大模型应用落地的有效技术。</p><p></p><p>MoE模型对企业开发应用和大模型普惠的影响也逐渐展现，吴韶华向大家介绍说，MoE模型除了提升算力效率外，还能提高精度，降低使用成本，增强模型能力。MoE模型通过激活少量专家，保持算力开销低，同时允许训练更多Token，进一步提升精度。对于终端用户来说，关键在于解决实际问题和降低使用成本。例如，在智能客服等应用中，用户更愿意花费较少的钱解决具体问题，而不会购买高成本的大模型。</p><p></p><p>最后，吴韶华补充了大模型落地与微调的观点，大模型在应用落地时需要进行微调，这是由于预训练阶段的数据和模型能力存在局限性。微调能有效应对不同的行业需求，但算力需求较大。同时，推理阶段也是算力开销大户，因此高效的模型结构和更强的能力在实际应用中具有优势。浪潮信息通过内部实际应用场景，如客服、软件研发、运维等，不断积累经验，提升模型能力，满足更多用户需求。</p><p></p><h3>回顾与展望：</h3><p></p><p></p><p>回顾大模型的发展历史，我们可以看到，2020年GPT-3的发布点燃了大模型的热潮。从2020年到2022年，业界在大模型能力上进行了广泛的探索。例如，2022年推出了GPT强化学习方法，使大模型与人的意图对齐，建立了良好的发展思路。同年末，ChatGPT问世，引发了大模型应用的热潮，成为增长最快且被广泛接受的大模型应用。此后，Llama系列模型陆续推出，2024年大模型的发展速度进一步加快。</p><p></p><p>浪潮信息的大模型研究始于2020年GPT-3发布后。2021年，他们发布了第一个大模型“源1.0”，拥有2457亿参数。2022年，进行了应用落地探索，运用了检索类技术和RAG技术。2023年，发布了“源2.0”，并推出了“源2.0-M32”混合专家结构模型。</p><p></p><p>关于大模型推广及触达用户，吴韶华介绍了浪潮信息大模型落地的两个方向：外部客户和内部需求。对外，浪潮信息通过与合作伙伴在EPAI平台上合作，提供开源模型支持，增强用户体验。对内，浪潮信息在多个业务场景中应用大模型，解决内部需求问题的同时积累经验，提升算法和工具性能，从而更好地服务外部客户。</p><p></p><p>未来，M32开源大模型配合企业大模型开发平台EPAI（Enterprise Platform of AI），将助力企业实现更快的技术迭代与高效的应用落地，为人工智能产业的发展提供坚实的底座和成长的土壤，加速产业智能化进程。</p><p></p><p>最后，吴韶华宣布，浪潮信息已在GitHub和Hugging Face上开源了代码和模型，并发表了相关论文。</p><p></p><p>源2.0-M32将持续采用全面开源策略，全系列模型参数和代码均可免费下载使用。</p><p>代码开源链接：<a href="https://github.com/IEIT-Yuan/Yuan2.0-M32">https://github.com/IEIT-Yuan/Yuan2.0-M32</a>"</p><p>模型下载链接：<a href="https://huggingface.co/IEITYuan/Yuan2-M32-hf">https://huggingface.co/IEITYuan/Yuan2-M32-hf</a>"</p><p><a href="https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary">https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf/summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</id>
            <title>别再危言耸听！大多数被评为“严重”的Bug评级具有误导性</title>
            <link>https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/205tMHA6eOSsVyS7jOla</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:39:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: CVSS评级, 安全团队, 软件供应链, AI/ML工具
<br>
<br>
总结: JFrog发布的调查结果显示，大多数CVSS评级在实际情况下并不适用，但安全团队仍花费大量时间修复漏洞。报告指出安全问题会影响工作效率，同时揭示了软件供应链安全和AI/ML工具在安全领域的应用不成比例。JFrog提供的安全方案聚焦于统一平台管理，为企业提供了高性价比的解决方案。 </div>
                        <hr>
                    
                    <p></p><blockquote>74%被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但有60%的安全和开发团队仍花费25%的时间修复这些漏洞。</blockquote><p></p><p>&nbsp;</p><p>近日，流式软件公司、JFrog软件供应链平台背后的公司JFrog&nbsp;发布了其 《2024年全球软件供应链发展报告》的调查结果，指出了新兴的发展趋势、行业风险以及保障企业软件供应链安全的最佳实践案例。</p><p>&nbsp;</p><p>JFrog首席技术官兼联合创始人Yoav Landman表示：“软件安全领域变幻莫测，全球的DevSecOps团队都在探索前行，在AI迅速普及的时代，更需要创新来满足需求。我们的数据涵盖了迅速发展的软件生态系统，为安全和开发组织提供了一个更为全面的介绍，包括值得关注的CVE评级错误、使用生成式AI进行编码所带来的安全影响相关洞察、允许组织用于开发的高风险软件包等信息，以便相关人员做出更明智的决策。”</p><p>&nbsp;</p><p>JFrog的《2024年全球软件供应链发展报告》结合了超过7000家企业的JFrog Artifactory开发者使用数据、JFrog安全研究团队原创的CVE分析、以及委托第三方对全球1200名技术专业人士进行的调查数据，旨在为快速发展的软件供应链领域提供信息参考。主要研究结果包括：</p><p>&nbsp;</p><p>并非所有CVE都如表面所见：传统的CVSS评级仅关注漏洞利用的严重性，而非其被利用的可能性，后者需要结合具体情境才能做出有效的评估。JFrog安全研究团队在分析了2023年发现的212个高知名度CVE后，平均将85%的“严重”CVE和73%的“高危”CVE的重要性评级下调。此外，JFrog发现，在报告的前100个Docker Hub社区镜像中，74%的CVSS评级为“高危”和“严重”的常见CVE实际上是无法被利用的。</p><p>&nbsp;</p><p>拒绝服务（DoS）攻击盛行：JFrog安全研究团队分析的212个高知名度CVE中，有44%存在发起DoS攻击的潜在威胁；17%存在执行远程代码（RCE）的潜在威胁。这对于安全组织来说是个好消息，因为RCE由于能够提供对后端系统的完全访问权限，与DoS攻击相比，其危害性更大。</p><p>&nbsp;</p><p>安全问题会影响工作效率：40%的受访者表示，通常需要一周或更长时间才能获得使用新软件包/库的批准，这延长了新应用程序和软件更新的上市时间。此外，安全团队大约耗费25%的时间用于修复漏洞，即使这些漏洞的风险在当前情况下可能被高估或甚至无法被利用。</p><p>&nbsp;</p><p>在软件开发生命周期（SLDC）中采用安全检查方式的差异性&nbsp;——当涉及到决定在软件开发生命周期中的哪个阶段采取应用安全测试时，行业内存在明显分歧，这突显了同时进行左移和右移的重要性。42%的开发人员表示，最好在编写代码过程中执行安全扫描，而41%的开发人员认为最好在新软件包从开源软件（OSS）库引入企业之前执行扫描。</p><p>&nbsp;</p><p>安全工具的过度使用现象仍在持续&nbsp;——&nbsp;近半数IT专业人士（47%）表示他们部署了四到九种应用安全解决方案。然而，有三分之一的调查对象和安全专业人士（33%）表示，他们正在使用十种乃至更多的应用安全解决方案。这一现象反映出市场对于安全工具整合的需求趋势，同时也表明人们正逐渐放弃单一的点对点解决方案，转而寻求综合性更高的安全工具集成。</p><p>&nbsp;</p><p>AI&nbsp;/&nbsp;ML工具在安全领域的应用不成比例&nbsp;——尽管有90%的受访者表示，他们的企业目前以某种形式使用AI&nbsp;/&nbsp;ML驱动的工具来协助安全扫描和修复工作，但只有三分之一的专业人士（32%）表示他们的组织使用AI&nbsp;/&nbsp;ML工具来编写代码。这反映出业内大多数人对AI生成的代码可能会为企业软件带来的潜在安全隐患仍持审慎态度。</p><p>&nbsp;</p><p>尽管新发布的报告揭示了被列为“高”或“严重”的CVSS评级在大多数常见情况下并不适用，但企业对于软件供应链的安全意识，一刻也不应放松。以JFrog为例，其提供的安全方案聚焦于以统一的平台去实现管理，且不限制用户数，顺应了很多企业的需求。同时，作为JFrog的一大产品特点，JFrog Xray和制品库是进行统一绑定的，即企业使用了JFrog的制品库，就无需额外购买JFrog Xray，会自动获得安全扫描的能力。这进一步帮助企业的安全团队减少了工具安全扫描维护和采购的成本对的同时，还能够帮助企业在安全扫描、制品管理、供应链管理上提供统一的高性价比解决方案。&nbsp;</p><p></p><p>JFrog安全研究高级总监Shachar Menashe表示：“虽然安全漏洞的数量每年都在增加，但这并不意味着其严重性也在同步上升。显然，IT团队愿意投资于新工具以提升安全性，但了解如何部署这些工具、如何有效利用团队时间以及简化流程，对于确保软件开发生命周期（SDLC）的安全至关重要。我们编制这份报告的目的不仅仅在于分析趋势，更是为了当技术业务领导者在针对AI导航、恶意代码或安全解决方案等方面制定决策时，能够为其提供清晰的指导和专业的技术咨询。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</id>
            <title>谷歌刚刚更新了算法，顺便搞毁了几家公司</title>
            <link>https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WoY69us6292NWvmx99Ot</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:29:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌更新算法, 公司毁灭, AI功能, 搜索引擎
<br>
<br>
总结: 谷歌最近的算法更新对一些公司造成了毁灭性的影响，特别是那些依赖谷歌搜索引擎的公司。谷歌的更新带来了更强大的AI功能，但也导致了一些原本排名靠前的网站被挤出搜索结果页面，给他们的业务带来了巨大打击。受影响的公司包括HouseFresh和Ready Steady Cut等。这些变化引发了对谷歌算法更新是否真的有助于网络的质量和用户体验的质疑。 </div>
                        <hr>
                    
                    <p></p><h2>谷歌更新算法，毁了多家公司</h2><p></p><p>&nbsp;</p><p>过去两年以来，谷歌搜索的一系列更新为这款互联网上最强大的工具带来了巨大变革，更配备了前所未有的AI功能。但最近，互联网上越来越多声音质疑，谷歌的一系列变化是在拯救网络，还是会将其推向毁灭？</p><p>&nbsp;</p><p>如果大家在谷歌引擎中输入过“空气净化器测评”，那想要获取的很可能是HouseFresh.com上的内容。该网站由Gisele Navarro和她的丈夫于2020年建立，整理了过去十年间改善室内空气质量的所有产品使用感受。他们在地下室里装满了各种净化设备，开展严格的科学测试，并撰写文章来帮助消费者们厘清思路、辨别炒作。</p><p>&nbsp;</p><p>HouseFresh就是由独立内容发布方推动建立活跃行业的典型案例。这些发布方所产出的原创内容，也正是谷歌长期以来号称应当推广的核心价值。实际上，就在该网站上线后不久，这家科技巨头就开始在搜索结果顶部显示HouseFresh。这让该网站迅速发展成一家欣欣向荣的企业，拥有15名全职员工。Navarro自己也为公司设定了颇具雄心的发展规划。</p><p>&nbsp;</p><p>但在不久后的2023年9月，谷歌对其搜索引擎算法展开了一系列重大更新。</p><p>&nbsp;</p><p>Navarro坦言，“这直接毁掉了我们的业务。一夜之间，本来指向HouseFresh的搜索词开始将人们引导至各大生活方式杂志，可这些杂志明显没有实际测试过产品。那里的文章中充斥着我一望而知的错误信息。”</p><p>&nbsp;</p><p>谷歌又在今年3月再次更新算法，这次造成的影响更大。HouseFresh的访客数量由每天数千人次锐减至数百人次。Navarro表示“我们完全被压垮了。”过去几周以来，HouseFresh网站不得不解雇掉大部分团队成员。她承认，如果后续情况没有好转，这家网站将唯有关闭一途。</p><p>&nbsp;</p><p>受影响的不止HouseFresh一家公司。</p><p>&nbsp;</p><p>英国娱乐新闻网站Ready Steady Cut的主编Daniel Hart也控诉谷歌改变搜索算法带来的影响可谓立竿见影。</p><p>&nbsp;</p><p>Hart解释道，“自从谷歌去年9月的更新之后，我们的流量当场减半，而且情况正变得越来越糟。我们不仅受到大网站内容的冲击，同时也正在被窃取我们内容的垃圾网站所取代。这样的整改毫无意义。”在接下来的几个月间，收入缩水已经迫使该网站将原本20人的作家与编辑团队裁撤至4人。</p><p>&nbsp;</p><p>谷歌方面的发言人则表示，该公司最近的更新已经给垃圾邮件和非原创内容造成了重大打击。谷歌也一直在密切关注导致搜索信息质量低下的滥用行为。</p><p>&nbsp;</p><p>谷歌算法更新之后，该公司向网站所有者发布了提示，号称能帮助其维持住搜索流量。但Hart指出，他们的网站聘请了顾问、重点关注谷歌的建议，而且不眠不休地更新网站。但经过近一年的努力，还是没有什么帮助。他表示，“过去8个月间，我浪费掉了宝贵的生命来努力遵循谷歌的建议。谷歌声称他们希望让网络用户能从掌握第一手经验和具备相关背景的人们那获取内容，可我们完全符合这样的标准。总之如今的情况实在让人心碎。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b3/b323b0d0bb06674cb3e1d7df09ce40c7.png" /></p><p></p><p>部分案例显示，谷歌搜索近期的变化正在对各类网站产生惊人的影响。</p><p>&nbsp;</p><p>谷歌一位发言人则在采访中强调，该公司的所有搜索算法调整都是在经严格测试验证、确认对用户有所帮助之后才会落地，而且谷歌方面也为各网站所有者提供了协助、资源和机会，允许其就搜索排名问题提出反馈。</p><p>&nbsp;</p><p>但批评人士认为，实际情况可能恰恰相反。随着谷歌重新调整其算法并使用AI将搜索引擎转化为搜索与回答引擎，不少人担心对于那些专司产出用户喜爱内容的企业来说，造成的冲击恐怕不亚于物种灭绝级别的事件。</p><p>&nbsp;</p><p>谷歌坚定认为这些变化将给整个网络带来好处，而搜索算法的调整只是个开始。</p><p>&nbsp;</p><p>至少有一点可以肯定：谷歌在AI上所做出的努力，将对大部分网民在网络上看到的内容产生深远影响。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d3/d3f93cd99c3a60b293dd2f2f0fc66fce.png" /></p><p></p><p>过去两年以来，所谓能让搜索变得更加“有用”的一系列更新，正在令努力遵循谷歌最佳实践的网站所有者感到沮丧。</p><p>&nbsp;</p><p></p><h2>谷歌AI工具“已读乱回”，工程师无奈手动删除</h2><p></p><p>&nbsp;</p><p>对搜索算法“动刀”后，谷歌最近又将注意力放到了AI工具上。这不，日前谷歌正因为社交媒体上充斥着的AI工具闹出的乌龙而忙得不可开交。</p><p>&nbsp;</p><p>这款谷歌新推出的AI综述（AI Overview）经常会胡言乱语，有时候让用户往披萨上抹胶水、有时候建议他们吃石头。面对这款匆忙上线产品搞出的麻烦，再加上互联网用户们用各种表情包大加嘲讽，谷歌正忙于手动禁用AI综述上的特定搜索内容。也正因为如此，很多表情包在被发上社交网络后不久就神奇地消失了。谷歌公司承认正“迅速采取行动”，旨在清除AI工具给出的一些奇怪答案。</p><p>&nbsp;</p><p>这样的现状着实令人摸不着头脑。毕竟谷歌测试其AI综述功能也有一年之久了——该功能早在2023年5月就以搜索生成体验的名号推出了beta版。谷歌CEO桑达尔·皮查伊更放出豪言，称该公司在测试期间已经支持了超过10亿条查询。</p><p>&nbsp;</p><p>而且皮查伊也提到，“在硬件、工程和技术突破的共同推动下”，谷歌同期将AI回答的交付成本降低了80%。看起来似乎是成本优化来得太早，而生成技术本身并没有做好准备。</p><p>&nbsp;</p><p>事实上，这款AI工具（AI Overview）是前不久皮查伊在公司年度开发者大会上，向众人宣布了其搜索引擎发展历史上最重大的举措之一。皮查伊表示，展望未来，谷歌搜索将针对诸多问题给出自己的AI生成答案。这项名为“AI综述（AI Overviews）”的功能已经面向美国用户推出。皮查伊指出，“这是一款能切实服务用户的产品。谷歌搜索由此成为建立在人类好奇心之上的生成式AI成果。”</p><p>&nbsp;</p><p>谷歌方面还表示，其AI综述产品旨在向用户输出“高质量信息”。谷歌发言人Meghann Farnsworth在采访邮件中回应称，“我们看到的许多案例都不属于常见查询，而且发现了不少被篡改或者无法重现的案例。”Farnsworth同时证实，谷歌方面正在“迅速采取行动，在符合内容政策的前提下适当删除某些AI综述查询，并利用这些案例对我们的系统进行广泛改进。部分改进结果已经在实际使用中得到体现。”</p><p>&nbsp;</p><p>由此可见，其实谷歌也承认了AI工具可能会提供不准确信息，但表示正在不断努力改进结果。谷歌公司发言人指出，AI综述的内容通常整理自多个网页，而非单一来源，而且响应结果会突出显示相关链接。这位发言人还提到，内容发布方可以在网页上使用特殊标签来控制AI综述是否列出相关网站链接。但需要注意的是，一旦AI模型抓取了创作者的内容，该数据可能将无法被删除。</p><p>&nbsp;</p><p>AI综述只是过去两年以来，谷歌对其核心产品做出的一系列重大改变中的一环。该公司表示，其最近针对搜索算法做出的改进努力将开启一个令人兴奋的技术新时代，并有助于解决困扰网络世界的诸多问题。</p><p></p><h2>成也搜索，败也搜索</h2><p></p><p>之所以要推动这些变化，是因为谷歌意识到此前的网络一直存在弊端。如果大家使用过搜索引擎，对此肯定也有切身体会。互联网的运作长期由所谓“搜索引擎优化（SEO）”所主导，这项技术旨在调整文章及网页内容，以便更好地被谷歌搜索发现并优先显示。谷歌甚至在为网站所有者提供SEO技巧、工具和建议。对于数百万将业务建立在机械化搜索体系之上的企业来说，SEO就是一笔他们不得不承受的“技术税”。</p><p>&nbsp;</p><p>问题在于，搜索引擎优化可能会被滥用。抱有野心的网站所有者也越来越多地意识到，相较于服务人类用户，专门制作适合谷歌筛选算法的内容才是增加经济收益的不二法门。</p><p>&nbsp;</p><p>谷歌针对垃圾搜索结果的战争已经愈演愈烈。2022年，该公司对其算法发布了“实用内容更新”，旨在淘汰纯为提升搜索排名而创建的内容。谷歌随后一截2023年9月发布后续更新，并在今年3月再次出手调整算法。谷歌方面表示，结果是“搜索结果中低质量、非原创内容减少了45%。”这似乎代表着一次巨大的成功。</p><p>&nbsp;</p><p>谷歌一位发言人在采访中表示，“我们最近的更新，希望将人们与来自网络的各类不同网站上的实用、令人满意且原创性的内容联系起来。在努力改进搜索服务的同时，我们还将继续专注于为网站提供有价值流量，以支持健康、开放的网络环境。”</p><p>&nbsp;</p><p>但谷歌的一系列改变，包括算法的更新和近期AI工具的出现，都没有博得什么好印象。</p><p>&nbsp;</p><p>一位不愿透露姓名的AI业务创始人在采访中表示，“谷歌曾经是一家以引领前沿、提供高质量产品的行业龙头，如今却不断发布各种质量低下的产品，甚至沦为整个互联网的笑柄和玩梗对象。”</p><p>&nbsp;</p><p>AI专家、纽约大学神经科学名誉教授Gary Marcus则在采访中表示，不少AI厂商都是在“兜售梦想”，希望更多人相信这项技术的正确率终将从80%提升至100%。Marcus强调，初步实现80%的正确率相对简单，因为其中涉及大量人类数据，其正确率天然就在这个区间。但弥合这最后20%的差距却极具挑战。实际上，Marcus认为这最后20%很可能是条死胡同。</p><p>&nbsp;</p><p>Marcus坦言，“对于很多问题，必须要经过相应的推理步骤才能判断当前事件是否可信、信息来源是否合法。而要想像人类审核员那样解决问题，恐怕首先要真正实现通用人工智能（AGI）。”Marcus本人和Meta公司的AI负责人Yann LeCun也都认定，为当前AI系统（包括谷歌Gemini和OpenAI GPT-4）提供支持的大语言模型并不是实现AGI的正确答案。</p><p>&nbsp;</p><p>这对谷歌来说，现在面临的处境无疑是十分艰难的。毕竟微软已经抢先一步，通过Bing大力推广生成式AI技术。另据报道，OpenAI正在开发自己的搜索引擎。而TikTok，正在为年轻一代用户提供最能满足其喜好的AI推荐体验。各方角逐之下，老牌巨头谷歌明显是感受到了竞争压力，最终导致整个生成式AI市场乱成了一锅粥。Marcus指出，2022年Meta曾发布名为Galactica的AI系统，但该系统在推出后不久即遭下架，因为它居然建议用户吃玻璃。吃玻璃、吃石头，看来Meta和谷歌的大模型倒是很有共同语言。</p><p>&nbsp;</p><p>谷歌倒是对其AI综述颇有信心并制定了宏伟计划，而目前已发布的功能只是其上周官定量内容的一小部分。针对复杂查询的多步推理、利用生成式AI组织结果页面，通过Google Lens实现视频搜索——谷歌的雄心壮志绝对不容小觑。但回归现实，谷歌的商业声誉无疑取决于其AI功能的实际表现，而目前来看其正确性实在堪忧。</p><p>&nbsp;</p><p>Marcus直言，“（这些模型）本质上无法对自己的输出进行健全性检查，而这样的现实正在拖累整个AI技术产业。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai">https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai</a>"</p><p><a href="https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet">https://www.bbc.com/future/article/20240524-how-googles-new-algorithm-will-shape-your-internet</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</id>
            <title>禁令再升级！拜登政府已不想让中国人在美从事AI工作了，套壳大模型的公司也危险了</title>
            <link>https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BciKq80BAwfAeJuZ3k7b</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 13:25:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, AI产业, ENFORCE法案, 中国
<br>
<br>
总结: 美国通过《ENFORCE法案》收紧AI大模型出口，限制中国员工在美从事AI相关工作，旨在保护美国技术优势和国家安全。该法案一旦生效，可能对中国AI产业造成阻碍，包括数据供给和技术合作方面的影响。然而，中国也在加速自主研发步伐，将面临挑战但也有机遇。 </div>
                        <hr>
                    
                    <p></p><blockquote>美国已经不止一次提议从科技上对中国发起制裁，如果此次法案生效，又将对我国AI产业带来哪些影响？听听专家们的观点。</blockquote><p></p><p></p><h2>美国立法收紧AI大模型出口，连AI人才在美工作也受限</h2><p></p><p>&nbsp;</p><p>北京时间上周四，美国众议院外交事务部委员会以显著的多数票数，成功通过了一项旨在严格管控AI技术出口的法案。这项法案被正式命名为《加强海外关键出口国家框架法案》，通常简称为《ENFORCE法案》。</p><p>&nbsp;</p><p>值得一提的是，在该法案不仅限制了AI系统和大模型的出口，一旦法案通过，持有H1b 签证的中国员工或留学生可能需要特殊许可才能在美从事AI/ML相关工作。也就是说，这是明晃晃在限制中国人在美从事AI相关工作。</p><p>&nbsp;</p><p>《ENFORCE法案》由美国众议员共和党议员迈克尔·麦考尔（Michael McCaul）、约翰·莫伦纳尔（John Molenaar）、马克思·怀斯（Max Wise）和民主党议员拉贾·克里希纳莫西（Raja Krishnamoorthi）提出。其主要目标在于，通过强化美国商务部的权力，使其能够更加便捷地对AI模型实施出口管制，并进一步限制美国与外国实体在开发可能威胁国家安全的AI系统方面的合作。</p><p>&nbsp;</p><p>立法者表示，此举意在确保美国的技术优势和国家安全不受外部威胁。该法案的共同作者、众议院议员Michael McCauln (R-TX)&nbsp;表示：“人工智能引发了一场技术革命，它将决定美国是否能继续保持世界领先超级大国地位，还是会被中国超越。”</p><p>&nbsp;</p><p>McCauln表示他最担忧的是，“虽然美国政府的工业和安全局 (BIS) 有权限制人工智能加速器的出口——拜登政府曾多次利用这一点来扼杀中国在该领域的创新——但它缺乏监管人工智能模型出口的权力。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/27/2774a88c92338eebf9842c873338e7e7.png" /></p><p></p><p>《ENFORCE法案》于5月9日首次公布，由众议院的跨党派AI工作组提出，该法案旨在修订2018年出台的《出口管制改革法案》，这一年美国将14类新兴技术纳入出口管制。&nbsp;</p><p>&nbsp;</p><p>据悉，《ENFORCE法案》还需要通过众议院、参议院的全体表决以及总统拜登签署，才能落地生效。一旦此法案生效，将授予美国工业和安全局限制人工智能模型出口的权利，也促使白宫能够要求美国公司或个人只有获得出口许可证才能出口AI大模型。</p><p>&nbsp;</p><p>McCauln称道：“这项立法为 BIS 提供了灵活性，使其能够制定对封闭人工智能系统适当的控制，而不会扼杀美国的创新或影响开源模型。”</p><p>&nbsp;</p><p>值得一提的是，目前该法案实际上并不包含任何明确的保护或对开源模型的豁免，并且基本上涵盖了所有的人工智能系统、软件或硬件。</p><p>&nbsp;</p><p>需要明确的是，该法案的实际措辞意在模糊，并具体要求在法案通过后一年内更新“涵盖的人工智能系统”的定义。</p><p>&nbsp;</p><p>众议院议员玛德琳·迪恩 (D-PA) 在投票前解释道：“我们还在法案中对人工智能和人工智能系统的定义进行了临时修改，以便政府可以采取通常的监管程序并征求公众意见，从而对最终的定义进行适当的范围界定。”</p><p>&nbsp;</p><p>就在签订该法案的同一周，据彭博社报道，美国国会计划立法减少人工智能的潜在风险和危害，并每年至少投入320亿美元于人工智能研究，以促进美国经济和国家安全。美国参议院多数党领袖舒默（Chuck Schumer）表示，这项资金将让美国公司、大学和人才“保持在人工智能产业最前沿的地位”。</p><p></p><h2>如果法案生效，将对我国AI产业带来哪些影响？</h2><p></p><p>&nbsp;</p><p>据《21世纪经济报道》报道，清华大学人工智能国际治理研究院的李依栩向撰文对比了两部法案，表示本次法案主要是将AI技术纳入了《出口管制改革法案》的管制框架内，通过补充AI相关定义、赋予总统管制权、增加美国人从事AI模型出口相关活动的许可义务，对AI模型进行管控。</p><p>&nbsp;</p><p>比如，法案第三条扩展了总统管制权，管制对象是特定受限的AI系统、对美国国家安全至关重要的新兴和基础技术相关活动；</p><p>&nbsp;</p><p>法案第四条增加了一项额外权利，如果美国人在出口、再出口被确定为对美国国家安全至关重要的新兴和基础技术，包括设计、开发、生产、维修、翻新这些技术，美国总统有权要求他们申请并获得商务部许可。</p><p>&nbsp;</p><p>那么，这项比此前《出口管制改革法案》更加严格的《ENFORCE法案》一旦生效，对我国AI产业将带来怎样的影响？</p><p>&nbsp;</p><p>某头部电商技术总监Micheal Yan在接受《AI前线》采访时表示：</p><p>&nbsp;</p><p></p><blockquote>“短期内对于我国AI大模型发展会产生一些阻碍。尤其是在数据供给方面，国内开发者可能会面临数据短缺的问题。由于AI大模型的训练需要大量的数据集，如果美国限制对华出口数据集，这将直接影响中国AI模型的训练和性能提升。&nbsp;另外，在技术合作方面，中美两国在AI大模型开发过程中有着广泛的技术合作。如果该法案生效，这将会阻碍新技术与新模型的开发，因为中美顶尖开发人才的技术交流将受到较大影响。&nbsp;然而，需要指出的是，实施对华出口禁令的负面影响是双向的，对美国自身也存在不利之处。例如，在数据收集问题上，禁止对华AI模型出口同样可能让美国公司错失借助中国数据进行AI迭代的机会。从长远来看，中国有充足潜力自主发展、突破限制，而美国的这种限制措施可能会激发中国加快自主研发和创新步伐。美国限制AI大模型出口的政策不仅会对中国AI产业的发展带来挑战，同时也会对美国自身在某些方面产生一定的负面影响。”</blockquote><p></p><p>&nbsp;</p><p>面对一波接一波的科技制裁时，我国也在加速自主研发的步伐，这对于国产AI技术的发展来说也是一种机遇。</p><p>&nbsp;</p><p>平安集团前CSO、广东省CIO联盟会长李洋表示：</p><p>&nbsp;</p><p></p><blockquote>“如同过往的芯片、系统软件、应用软件等对中国的限制一样，美国对AI的限制出口接踵而至，但是AI的限制在目前中国大力推行国产替代的大前提下，是机遇大于挑战的。在这样的大背景下更加便于中国的科研工作者，丢掉幻想，重新布局。&nbsp;现实情况下是，AI所依赖的算力、算法很多都依赖于美国，当然也包括其他国家的算法、数据。但实际上中国的AI场景和应用体量非常大，在数据和算法层面不久后将不再受制于人。现代AI的发展不过几十年，即使是当前美国处于领先地位，也未必就永远处于领先。即便是如今众星捧月般的明星独角兽OpenAI也无法保证其大模型架构是未来AGI的正确技术发展路线，所以这个时候美国的限制对中国来说未必是坏事。&nbsp;希望中国的科研工作者，尤其是AI工作者们能够沉下心，在基础算法和模型研究中能够走出一条中国特色之路。</blockquote><p></p><p>&nbsp;</p><p>李洋还表示，“对于AI人才赴美的限制，我觉得也不是什么大问题，中国的AI基础研究和应用市场相对于美国来说都还是蓝海，立足于中国的产业，将AI人才留在中国服务，利大于弊。况且，美国所谓的这些限制，在当今时代下，也不会是一揽子的全封闭，所以我们应当审时度势，抓住这个机遇。”</p><p>&nbsp;</p><p>法案公布以来，除了担忧对于AI人才和大模型的限制外，外界还会担忧对于开源大模型的限制会让许多国内“套壳”大模型企业很难受。</p><p>&nbsp;</p><p>对此，李洋表示，“开源模型虽然现在美国比较领先，但是其他欧美国家也不乏相应的开源产品，我们在这个阶段还可以多方面借鉴和研究。并且，基于我们的自主创新体系，我们还是要沉下心来研发自己的AI基础底座，包括硬件基础设施、开发平台和大模型及其应用，所以短期内会对一些套壳的中国公司产生一些影响（包括模型的演进、应用和商业化等等都存在相应的限制）。”</p><p>&nbsp;</p><p>“但是从技术和应用上来讲，进口的开源模型也不是完美和最终的AGI的路标和唯一标准，所以在这个时候，也希望我国AI企业能够立足于自身的能力打造，丢掉幻想，与中国生态和国际生态一道打造出中国的AI开源版本，为中国和国际做出自己的贡献”。李洋说道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html">https://www.21jingji.com/article/20240527/herald/5f7b347c2787de4bf776584f95950075.html</a>"</p><p><a href="https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/">https://www.theregister.com/2024/05/23/us_lawmakers_advance_bill_to/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</id>
            <title>Agent 还没出圈，落地先有了“阻力”：进入平台期，智力能否独立担事？</title>
            <link>https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLryHtsN1PFPMuhquhAr</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 10:12:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AutoGPT, AI Agent, 具身智能, 大模型
<br>
<br>
总结: 作者介绍了AI Agent的当前能力和应用潜力，讨论了在企业场景中有效利用Agent的重要性，以及未来发展趋势。同时，还探讨了AI Agent在不同领域的具体应用场景，以及与大模型的区别和发展方向。 </div>
                        <hr>
                    
                    <p>作者 ｜ 华卫</p><p></p><p>去年出圈的AutoGPT，让AI Agent来到大家的视线中并迅速爆火，大家都对Agent抱有极高的想象力与期待值。那么，Agent现在到底有多大的应用潜能？企业要如何抓住？同时在具体的落地实践方面，也有不少悬而未决的挑战。</p><p></p><p>带着这些问题，InfoQ《极客有约》特别邀请了阅文集团 AIGC 技术负责人马宇峰担任主持人，与机器姬CTO&amp;具身智能一百零八讲主讲人刘智勇、华为云aPaaS首席架构师陈星亮，一同探讨AI Agent的当前能力、应用落地情况以及未来发展趋势。部分亮点如下：</p><p></p><p>Agent不仅仅是一个玩具，而可以改变现实世界。在企业场景中有效利用Agent，合理选择业务场景非常重要。具身智能领域最大的挑战在于操作层面，瓶颈在于如何泛化地执行物理世界中的各种操作。未来使用Agent和大模型将成为企业员工需要掌握的技能。人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。</p><p></p><p>以下为访谈实录，经编辑。完整视频参看：</p><p><a href="https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT">https://www.infoq.cn/video/DOPpG6NjCHcJKDzCsAFT</a>"</p><p></p><h2>AI Agent当前的能力</h2><p></p><p>马宇峰：首先要谈的就是AI Agent现阶段的能力，大家现在是如何应用AI Agent的？具体落地场景有哪些？</p><p>刘智勇：最近大家可能已经注意到了一个名为“Figure”的机器人，演示中，工作人员向该机器人表达了饥饿感之后，Figure成功地将苹果递给了他；这一过程展示了AI Agent在物理世界中进行任务推理、规划并最终转化为实际行动的能力。在具身智能领域，AI Agent的应用场景非常广泛，AI Agent可以大致分为以下四个方面。</p><p>工业场景：在工厂中，具身智能机器人可以应用于3C生产线或汽车总装线，提高生产效率和自动化水平。商业服务场景：在商业环境中，具身智能机器人可以提供接待、讲解、导览、巡逻和配送服务，改善客户体验，提升服务质量。家庭场景：在家庭环境中，具身智能机器人可以承担清洁服务或家务工作，减轻人们的负担，提高生活质量。火星建设：在未来的火星探索和建设中，具身智能机器人有望发挥重要作用，帮助人类在恶劣的外星环境中进行建设和研究。</p><p>对于这些应用场景，具身智能都展现出了巨大的潜力和希望，为未来的技术发展和应用提供了广阔的前景。</p><p>陈星亮：针对企业场景进行AI Agent能力创新时，多数是从IT场景开始的，因为该场景拥有较为完善的信息化基础。在这一过程中，我们遵循两个主要原则：一是先易后难，我们首先从普遍性场景开始，然后逐步向专业化场景演进；二是保障效果，无论开发哪种场景的AI应用，都必须确保其有效性。</p><p>办公和编码领域被广泛认为是AI Agent应用的切入点，因为这些场景相对通用，容易实现。随着技术的进步，我们将AI Agent的应用延伸到更复杂的场景，例如：</p><p>办公领域：AI Agent可以用于自动生成会议纪要或设计文档，这些任务比简单的代码生成或文本创作更具挑战性，需要更深层次的场景理解和更高级的语言处理能力。销售或服务领域：AI Agent可以用于合同审核或法律条文的辅助生成，这要求AI Agent不仅要理解法律术语，还要能够处理复杂的逻辑关系。网络设备监控：在对网络设备进行监控的基础上，AI Agent可以执行自动巡检任务。这要求AI Agent不仅要处理专业数据，还要能够理解并应用信息化积累的知识。</p><p>马宇峰：大家首次接触到AI Agent大概是在什么时候？从本质上讲，AI Agent与大模型的区别究竟体现在哪些场景上？最核心的区别是什么？</p><p>陈星亮：Agent 这个概念，实际上在大模型出现之前就已经存在了。在进行 IT 系统集成或设计某些自动化流程时，其实已经有Agent 这一层了，尤其是在设备与外界交互的环节，而那时还没有将大模型技术整合进来以实现更广泛的泛化能力和生成式能力。</p><p>大模型技术引入后，起初我们并没有考虑将其应用于设备控制或高度交互性的 IT 系统交互中，而主要看中其在创作和生成内容方面的潜力。之前我们在设备代理方面的工作与 AI Agent 的概念思路颇为相似，只是随着大模型的加入，AI Agent 的能力和应用场景都发生了变化。当我们将这些结合起来后，认识到了 AI Agent 的真正面貌。因此，如果仅从 IT 系统的能力角度来看，AI Agent 这个概念并不神秘，不过是通过引入大模型为 AI Agent 带来了更多能力，从而丰富了其功能。</p><p>刘智勇：无论是ChatGPT还是Agent、具身智能，本质上都是在以下三个方面进行发展。</p><p>文本世界：在文本领域，大语言模型展现出了强大的生成和理解能力，这主要体现在ChatGPT等应用中。数字世界：数字世界中，我们需要利用规划、循环和反思的控制机制，实现任务从开始到结束的全流程控制，并调用数字世界里的外部工具进行执行。物理世界：物理世界中，Agent的能力落地体现在具身智能上，即通过具身智能技术将规划形成的任务序列转化为物理世界中的实际操作。</p><p>马宇峰：我分享一下第一次接触Agent的经历，去年夏天OpenAI开发了一项名为“Function Call”的能力，虽然看起来仍然是文本的输入和输出，但当函数作为一个字符串被输出并被精确调用时，我确实看到了Agent的不同之处。以前我们认为创作和创意不确定性是大语言模型最人性化的特征，但同时它们也有机器的一面，能够在有限的范围内唤醒某些函数。这项能力让我意识到Agent应该被独立考虑，其围绕工具使用、规划和执行的能力，可以帮助大模型结合现实世界中的数字和物理能力，形成一个更完整、更通用的解决方案。这是我对Agent概念的一次认知冲击。</p><p>然而，随着时间的推移，我发现Function Call可能并不像我最初想象的那么好。它演示的技能是查询天气，虽然可以很好地执行，但许多场景要复杂得多，可能不只有10个或20个函数可供调用，会出现完全不确定的函数，下一步该执行哪个函数也会是未知的。不过，Agent的主流能力，如浏览器的唤起、搜索引擎的查询结果以及一些生成能力的唤起，确实有效地让它从概念走向实际。当然，在实际应用过程中，我们也发现了许多不确定因素，但Agent的能力已经让我感到惊讶，它不仅仅是一个玩具，而可以改变现实世界。</p><p>回到Agent 的适用场景，我分享一下个人自身在探索中使用的直观感受。使用Agent能力可以批量生成自媒体文章，也可以像模像样地讲一个故事，从创建角色、制定纲到将角色和情节融合，再逐步生成内容，它的成文速度非常快，也有一些优点，比如生成过程中，可以将角色单独抽象出来去形成可视化的元素，可以使用多个角色和情节引导来发展内容片段，且在逻辑框架内是可控的。</p><p>但深入研究后我们发现另一个问题：Agent输出的内容，还是没有达到人类所能达到的逻辑性、创意性相结合。业内也做了很多尝试，这方面却似乎一直停留在中等或中上水平，整体表现平庸，所以这确实是长期困扰我们的问题。虽然我们最初认为Agent很有用，但在商业化和变现能力上似乎没有那么强。</p><p>想问一下陈老师，在代码和办公场景，Agent 可以从哪些方面提升效率？有哪些bad case？</p><p>陈星亮：我先谈谈Agent 给一些稳定场景带来的效率提升作用，如设计文档生成和合同中法律文本的生成等。在一些应用场景相对明确、法律条文引用也相对模式化的特定领域，如可靠性设计或安全威胁设计，Agent的表现在业务用户看来感知和体验都非常好，准确度也相当高，显著提升了工作效率。目前，我们也在将Agent应用于网络设备巡检等生产场景。尽管巡检过程中会遇到各种意想不到的问题，但对于那些已有案例库和解决方式库的巡检，Agent 都能够发挥作用，并帮助提高巡检效率、简化人力的工作。</p><p>然而，也有一些不尽如人意的地方。Agent刚推出时，大家对它寄予厚望，导致在选择应用场景时没有过多限制，业务团队提出了许多要求较高的场景，想要用Agent去解决未知的问题。这些要求的实际难度很大，而Agent在处理未知问题时的能力有限。因此，如果要在企业场景中有效利用Agent，合理选择业务场景非常重要。否则，Agent的效果可能不会达到预期，甚至可能非常差。</p><p>马宇峰：如果人类都做不到的事情，期望Agent达到超越人类的水平是非常困难的。相反，那些人类已经重复做了很多遍且已经规范化的工作，确实可以将人类的判断力解放出来，完全交给Agent来自动化处理。在具身智能的Agent应用上，哪些方面是可行的？可能存在什么挑战？</p><p>刘智勇：首先，具身智能的输入需求依赖于视觉语言模型，这意味着需要处理整个环境的三维数据信息，而不仅仅是二维图像。它需要的输入包括深度数据、RGB图像等，可能还要结合触觉、反馈力以及编码器数据等，这些数据共同构成了具身智能的全面输入。因此，在数据输入的方式上，具身智能与传统Agent存在显著差异，这些差异带来了巨大的挑战。</p><p>其次，在数字世界的Agent中，无论是什么类型的Function Call，基本上都是可执行的动作，操作层面通常不会遇到问题。然而，具身智能中存在一个可供性问题，即是否能够真正执行某个动作。尽管存在这些挑战，但也有一系列方法可以解决这些问题，如具备泛化能力的视觉语言模型、迭代细化的机制、自我反思的机制等。目前来说，具身智能领域最大的挑战在于操作层面，即具身操作。感知、决策和规划虽然重要，但真正的瓶颈在于如何可泛化地地执行物理世界中的各种操作。</p><p>马宇峰：Agent目前的发展状况如何？是否已经达到了一个平台期，还是仍然有很大的提升空间？是否依赖于某些特定的背景？</p><p>我认为Agent主要依赖于大模型的Function Call能力，需要准确地识别出当前调用哪个模型来完成当前任务，并提供相应的结果，以便大模型进行下一步操作。而瓶颈可能在于读取上下文的长度，上下文长度决定了能够识别多少个函数。Agent在执行过程中受限于场景，只能在有限的函数中进行选择，其执行也不完全精确；如果执行不精确，就需要获取更多的环境信息或反馈信息来执行函数，过程中可能会出错。Agent是一个精妙但不够鲁棒的系统，如果它返回到上一级并根据错误信息重新执行，可能会带来更大的资源消耗和时间延迟。</p><p>陈星亮：在企业场景中实施Agent时，我们首先需要考虑的是技术的可实现性。在挑选场景的过程中，就要考察技术是否可行；一旦场景确定，接下来需要考虑的是如何提高Function Call的准确度，如果准确度不够高，需探索其他工程手段来提升API的识别准确率，甚至在语义理解之后通过额外的工程能力进行调整、校验生成的API并通过查询方式进行补充。企业面临的最大挑战之一就是需要重复性地进行这类工作。目前我们也在探索长序列处理、记忆的短、长期存储以及上下文空间的扩展等技术，以期在未来实现更多的技术突破。</p><p>在具身智能领域，企业场景中也在逐渐引入多模态技术，尤其是当与操作技术领域（OTA）的设备关联时。多模态技术的引入包括传统的视觉识别等，将进一步增加系统的复杂性。如果大模型在这些领域取得显著进展，那么在企业IT融合场景中的工程难度将大大减少。目前，我们在工程实践中仍需进行大量技术工作，这些工作的管理复杂性甚至超过了传统的微服务架构。</p><p>我相信，随着技术的进步，未来将有很大的空间来改进现有的工程能力，减少人工干预，让大模型承担更多的工作。无论是让大模型自行处理，还是让Agent框架沉淀出更多稳定的框架性技术，都是未来技术发展的趋势。我对大模型在未来的迭代和改进抱有很高的期待，相信它们将带来更好的效果，并减轻当前工程化实践中的一些负担。</p><p>刘智勇：从阶段性的角度来看，我们认为具身智能目前处于技术起步期，未来的发展空间仍然非常广阔。之所以称之为技术起步期，是因为目前还存在三个方面的挑战：</p><p>任务类型的泛化性：这涉及到Agent能否理解各种类型的指令，并能够完成具体的规划而不产生幻觉，抑制Agent在理解上的偏差，对齐人类意图的二义性和潜在偏好，确保其能够准确执行任务。环境的泛化性：即Agent快速与环境对齐，对齐环境的规律、动态性和随机性。操作的泛化性：这是更为复杂的挑战，涉及如何利用多种数据源采集更多的线下数据，并据此训练出能够泛化到不同情境的具身操作模型，目前行业中还没有一个非常好的解决方案。</p><p>从这三个方面的挑战中，我们看到了未来的发展机会。尽管目前还存在许多问题需要解决，但这同时也是推动技术进步的动力。</p><p>观众提问：是否可以认为大模型做好了就不需要 Agent 了呢？</p><p>刘智勇：大语言模型的主要功能是处理和生成文本，核心在于将文本信息进行向量化处理，并通过Transformer架构以及监督学习机制，实现技术上的范式转变。这些技术基础的迭代，再结合大量的数据和强大的算力，促成了ChatGPT等大语言模型的诞生，它们在文本生成和回复方面表现出色。</p><p>尽管大语言模型在文本领域取得了显著的成就，但本质上只具备基于零样本提示词的文本回复的能力，而不具备执行实际任务的能力。这意味着，无论大模型在文本处理上多么先进，它们仍然需要Agent的介入来实现从文本到行动的转变和全流程的处理。</p><p>因此，大模型和Agent是两个不同的概念，前者专长于文本交互，而后者则涉及到任务的执行和落地能力。简而言之，大语言模型缺乏将文本回复转化为实际行动的能力，是典型的缸中之脑。</p><p>马宇峰：如果大语言模型发展到某个瓶颈无法提升，那也可以像两个人类合作思考能更高效地完成工作一样，使用两个大模型实际上可以进一步提升当前水平。哪怕提升的幅度不大，但考虑到大模型的较高的基础表现，即便是小幅提升也可能带来非常显著的回报，并且能够有效地增强现有能力。至于这些能力是否会直接集成到大语言模型中，我认为在相当长的一段时间内，我们仍然可以将大语言模型视为一个智能体，主要从智能逐步思考的角度来使用它。</p><p>陈星亮：aPaaS主要是基于行业内现有的资产或经验，实现程度化代码开发，降低开发门槛，通过拖拉拽的方式快速构建简单的应用程序。随着大语言模型代码生成能力的出现，零/低代码平台受到了较大的冲击。曾经有观点认为，大模型的出现可能会使得低代码或零代码的开发方式变得不再必要。实际上，我认为情况并不会如此。</p><p>零/低代码平台可以有效地融合大语言模型的能力，让大模型直接参与代码生成。以前需要通过拖拉拽来实现的功能，现在可以通过自然语言处理（NLP）的方式进行交互，提供更直观、友好的用户体验，并帮助理解业务用户原始的语意，以更好地生成低代码或零代码应用。我认为零/低代码平台和大模型之间更多的是一种合作关系。低代码平台上已经积累了大量的业务资产，而大模型可以将其作为插件调用，两者结合将发挥出更大的潜力。</p><p></p><h2>AI Agent的落地挑战</h2><p></p><p>马宇峰：在大语言模型不提升或通用大语言模型更新周期较长的情况下，如何利用现有工具和能力取得良好成果？有哪些方法或策略？</p><p>尽管当前AI Agent面临许多瓶颈和困境，限制了其应用范围，但仍有一些方法可以提升其驱动能力，如可以通过垂直领域的强化训练、特殊训练技术或更巧妙的方法，在不提升大语言模型本身能力的前提下改善Agent的表现。Agent在当前大语言模型框架下的表现，不仅取决于模型本身，还受到其他多个环节的影响。即便大语言模型不是限制因素，其他环节的优化也能提升整体Agent的效果。以Kimi为例，它之所以能够脱颖而出，可能确实在大模型的某些方面做了针对性强化，但重要的是它对文档类型的解析能力有效提升了实际操作中的使用体验。Kimi能够在处理长文档时进行分块，并采用迭代检索的方式输出答案，这大大增强了Agent在特定场景下的应用体验。</p><p>我相信，即使在大语言模型能力不变的情况下，只要充分提升检索能力，就能显著提高最终的可用性和准确率。很多时候未能获得准确答案，是因为没有找到正确的信息片段。如果知识库足够丰富，片段足够多，那么作为一个智能整合的搜索引擎，Agent将具有巨大的应用潜力。在大语言模型能力不完整的情况下，只要把某个小模型、小工具或阶段（如检索阶段）做得足够好，也能显著提升Agent的整体表现。</p><p>刘智勇：要提升AI Agent的能力，首先需要充分挖掘并利用长期记忆，通过RGBD摄像头读取的数据，结合视频语言模型，形成丰富的语义信息。在特定场景中，这些语义信息往往是重复出现的元素，关键在于如何有效地保存信息，为后续的规划提供坚实的基础。随着时间的推移，语义信息不断积累，AI Agent的长期记忆能力将变得更加强大。</p><p>其次，进行迭代细化是提升AI Agent能力的另一个关键点，这意味着需要不断结合当前的模糊指令和新获得的语义信息，形成新的提示词。通过不断的迭代询问，AI Agent能够逐步细化和精确化其理解和响应，通过不断反思，最终达到更加精准的结果。</p><p>陈星亮：企业内部考虑事务时主要关注两点，都与数据紧密相关。首先是文档处理的问题，在企业中，非结构化文档往往是承载信息的主体，处理这些文档不仅要识别文档类型，还包括对复杂文档的解析，如图文混排和包含复杂表格的文档。这些内容在原有的基础上，需要对文档类型识别的范围进行扩展，但在企业内部对这种复杂文档的解析仍是一个较大的挑战。</p><p>其次关于原有数据的利用问题，特别是在生产场景中，一般都具备专业领域的背景。以设备巡检为例，它与设备的领域知识密切相关，这种情况单靠企业自身的私域数据积累可能不够，需要在行业内去做垂域模型。目前，我们期望通过Agent技术的发展，能够让更多企业在通用场景中体会到Agent带来的好处，从而愿意将自己内部的结构化数据进行区分，将企业机密数据与可对外开放的数据分离，并逐步开放一些行业公共数据，这将有助于构建每个行业的垂直领域模型，为未来企业场景和Agent的发展带来巨大的好处。</p><p>马宇峰：初期部署Agent的成本是否高昂？是否能够带来相应的收益？能否实现成本的回收和价值回报？</p><p>陈星亮：企业部署Agent时，成本问题是一个必须考虑的重要因素，并且需要结合业务团队的期望以及对目标的评估来共同考量。初期企业主要探索通用场景时，成本通常是较低的。随着业务场景的成熟，以及越来越多的用户和业务团队成员开始使用这些场景，成本就会开始上升。特别是当场景全面开放并开始构建更多场景时，就可能需要多套模型和版本，模型也需要不断地做飞轮进行迭代和优化，成本可能会指数级增长。</p><p>因此，在正式对外放开并大规模使用Agent之前，与业务团队进行充分沟通和期望管理是非常重要的，需要让业务团队明白，业务场景真正对外开放并吸引大量用户使用后将会涉及到哪些成本。同时，业务团队也需要评估这些成熟场景能够带来的价值，如对客户满意度和内部效率提升的贡献。当业务团队获得这些信息并进行综合评价后，他们对预算和投入的决策将会更加明智，这样的过程有助于确保Agent部署的成本得到合理评估和控制，并带来相应的价值回报。</p><p>刘智勇：Agent部署的成本考量包括云端的调用成本、机器人本体的计算成本以及整体的部署成本这三个主要方面。</p><p>云端数据成本。这涉及调用模型的频率，如果实时观察环境中的语义信息，就需要频繁且快速地调用模型，这样会耗费大量的计算资源，从而产生高额成本。因此，必须考虑调用频次和计算资源消耗的问题，实现具身智能体和自身限制的对齐。机器人本体成本。在具身智能场景中，机器人本体通常需要具备一定的计算能力。为了使机器人能够在不同场景中应用，无论是商业、工业还是家庭环境，都希望能够在端侧部署大模型，尤其是本地部署，而这在没有高端GPU和显寸的支持的条件下尤为关键。部署成本。将设备分布式放置在不同地方会产生额外的成本，此外还需要考虑是否能够通过启发式方法或其他手段，让设备快速启动并投入使用，这也是降低部署成本的一个重要方面。</p><p>马宇峰：部署成本确实是一个值得讨论的话题。在实验性质的探索阶段，对时间的考量可能并不严格，但一旦考虑到响应速度，比如每秒需要处理多少个token来执行动作，成本问题就变得尤为突出。因为模型需要很长时间才能完成一个Agent的输出，这意味着直接使用大模型进行推理的成本和时间的耗费都是非常高的。对此，我个人建议可以利用一些框架，如Dify或Coze，它们可以帮助构建Agent框架，并提供了丰富的工具来逐步检查生产和输出的结果。</p><p>企业部署Agent时， 可以先验证整个流程是否可行，并确保其能带来业务价值。之后，可以考虑用一些专门训练的小模型来替代流程中的关键部分，以降低成本。初期可以利用现有的资源进行探索，长期来看，通过特定化的方式进行优化和部署可以平衡成本。</p><p>观众：在部署Agent时可能会遇到哪些安全方面的问题？目前是否有一些比较成熟的工具可以用于保障Agent的安全性？同时，是否可以认为Agent的安全性主要取决于其底层大模型的安全性？</p><p>陈星亮：首先，Agent的安全性并不仅仅由大模型决定，模型安全只是一部分，还涉及应用安全和数据安全。对企业来说，对安全性的投入无论多少都不为过。无论在引入模型时，还是实际使用过程中，包括Agent框架都需要进行安全检查。例如，使用开源框架组件时需要进行安全审查，运行时需要对模型的输入输出内容进行监控，以及对应用框架进行访问控制，防止调用越权等。</p><p>在企业原有的安全体系内构建Agent的安全性会更好一些，在华为云内部，我们基于AI原生应用引擎等平台，当Agent对外提供服务或与模型进行交互时，利用内部原有的数据安全、应用安全和内容安全方面的技术，对内容进行检查和过滤。Agent的安全性需要在现有基础上，结合Agent之间的技术组件交互以及场景特有的安全要求来综合考虑和实施。</p><p>观众：面对多智能体协同框架的开源与闭源发展，应该如何选择合适的技术路线和框架，以减少试错过程并确保系统不会被行业不断的更新迭代所淘汰？</p><p>陈星亮：我认为应该分开考虑。对于Agent的开发框架，目前开源的选择比较多，都有很多可用的资源。鉴于Agent领域本身正在快速发展，选一个团队成员熟悉且操作顺手的框架，然后跟随其发展进行使用。而对于Agent的运行时环境，进入企业生产环境后，我建议使用闭源解决方案。理想的状态是，在企业现有的基础设施基础上进行必要改造，以便将Agent的运行时环境纳入统一管理和运维体系中，确保运行时的稳定性和安全性。</p><p>刘智勇：我更倾向于观察一个技术方案是否展现出成熟和收敛的迹象，如果开始趋于稳定和收敛，那可能是着手开展相关工作的更适宜时机；如果尚未成熟，还在快速迭代和变化之中，那可能面临开发速度跟不上开源社区更新速度的问题。</p><p>马宇峰：在选择开闭源路线时，实际上需要根据所处的发展阶段来决定。不管选择何种路线，实际投入使用比纠结于何时开始尝试和如何减少错误更为关键。毕竟，随着时间的推移，技术本身会更新迭代，这些更新往往也会朝着更优化的方向发展，对业务发展带来积极的价值。</p><p></p><h2>AI Agent的未来前瞻</h2><p></p><p>马宇峰：从长远来看，企业中Agent的落地是否会对某些现有的职业造成冲击？比如普通员工、现有商业模式、提供API服务的SaaS公司以及供应商等。Agent的普及和应用会带来怎样的影响？</p><p>陈星亮：对于员工而言，随着技术的发展，未来使用Agent和大模型将成为他们需要掌握的技能，尤其是提示词。员工至少需要学会如何使用Agent，就像现在进行零代码应用开发一样，将其作为日常办公工具的一部分。对于企业，尤其是传统SaaS公司来说，Agent和大模型的引入已成为明显趋势。一些大型SaaS公司，已经开始将大模型集成到平台中，将Agent框架和集成外部大模型的能力嵌入到二次开发和应用中。传统SaaS公司如果不加入到这个发展潮流中，可能会影响产品体验，建议一定要去拥抱大模型和Agent。</p><p>刘智勇：我从两个不同的角度来探讨Agent的运用及其对未来的影响。对于企业而言，利用大模型或Agent的主要目的是提升工作效率和减少对人工的依赖。有时员工的工作效率确实无法与Agent相比，特别是在一些技术性任务上，初级工程师的编码能力可能远不及代码Agent。对于工程师来说，积极利用Agent不仅是为了保持个人竞争力，也是为了适应未来工作的需求。Agent可以作为一个强大的工具，帮助工程师完成更高效和更复杂的任务。</p><p>而具身智能特别是人形机器人，预计它们对未来世界的冲击将是巨大的，会在商业、工业和家庭三个领域中体现出来。在商业领域，许多展示和演示类的工作岗位可能会被智能机器人所取代，因为它们可能表现得更好。在工业场景中，很多重复性或技术性工作实际上可以由机器人来完成，提高生产效率和安全性。在家庭环境中，未来也可能会出现更多类型的服务机器人，帮助处理日常家务。同时，我们也应保持谨慎乐观的态度，认识到技术发展和应用普及的速度可能没有想象中那么快。机器人和Agent的发展旨在辅助人类，使我们能够专注于更有价值和创造性的工作。</p><p>马宇峰：关于Agent如何影响我们的现实世界，尤其是在工作场景中，我的感觉是需要先拥抱这些变化，然后学会适应和改变。现在可能是小企业创业的绝佳时机，因为借助大语言模型这样的“万能胶水”，不再需要像以前那样协调大量资源来进行服务能力的交付，只需要尝试不同的组合，就可以高效地为客户提供解决方案。这样，小企业的服务能力从完全定制化转变为可以大规模扩展的模式，这对于二线市场可能是一个深远的改变。</p><p>观众：英伟达使用虚拟环境训练智能体机器人的方法是否可以拓展到所有应用场景？虚拟环境数据能在多大程度上替代现实场景数据？</p><p>刘智勇：我们实际上已经使用过英伟达的Isaac Gym来训练智能体，主要是进行强化学习的训练。这种方法涉及合成不同的仿真环境，并基于此来进行强化学习的训练。这种方法的主要优势是数据是免费的，但存在一个从仿真到现实（sim-to-real）的转换gap。在应用拓展方面，特别是在本地运动（locomotion）即行走部分，使用强化学习和虚拟环境的训练模式效果是不错的。对于一些操作类的任务，也有一些积极的应用特点。但对于更广泛的操作任务，可能更倾向于使用采集到的真实数据，并利用transformer架构来训练大型的transformer模型。因为在英伟达的仿真环境中，很多物理引擎的细微数据是无法被完美仿真的，如一些非常精细的触觉反馈。</p><p>马宇峰：在内容行业，我们对AI技术的发展有着深刻的感受。有人认为，Sora的成功是因为学习到了物理世界的真实性，但随后有人指出，Sora可能只是选择了一些优秀的片段来展示。Sora的训练采用了虚幻引擎，但这种方法还是单一的，并不一定能够真正理解物理世界。这与刘老师提到的英伟达的反馈机制可能有所不同，它们的输入输出机制存在差异。目前，Sora虽然理念上很先进，但实际上还没有达到通过虚拟化的输入输出来获得真实物理引擎的效果，可能是因为模型的参数规模不够大、训练数据不够丰富，或者受虚拟数据本身的限制，还需要进一步深入观察和研究。</p><p>观众：请介绍一下目前单智能体落地的情况，以及它与公司当前技术架构的结合方式。多智能体的具体架构是如何建设的？</p><p>陈星亮：在原有的技术架构体系中，目前大家使用的较多的是Web应用、微服务，有时还会使用函数技术体系。我们可以将Agent和大模型引入进来，先进行隔离，用于特定的场景。这些场景必然会与现有的微服务体系或函数体系进行交互。这时可以采用集成的方式进行，而不是直接使用大模型的Function Call方式。这样实施难度会小一些，而且也能让Agent发挥作用。当技术团队逐渐掌握了Agent和大模型这套技术，就可以开始取代一些现有的应用。这样的过渡不仅有利于架构的演进，也有助于技术团队的能力培养。</p><p>马宇峰：多智能体协同是一个复杂而富有挑战的领域。项目中，多智能体的协同运作被分解为不同的角色，如项目管理员、编码者、产品经理等，各自承担不同的职责。然而，如果单个智能体（单Agent）的运作还没有完全搞明白，就急于发展到多智能体（多Agent）协同，其实是存在很大风险的。</p><p>在实际应用中，比较常见的模式是有两个智能体协同工作，一个负责生产，一个负责评估，但目前还没有看到这种模式带来特别显著的提升。举一个例子，情感陪伴场景中有大量的对话交互，如果一个人与一个IP进行对话，输入输出的比例可能不太理想，引入第三个智能体会带来信息量的显著提升，这在满足用户情感需求和具体任务需求时非常有用。</p><p>接下来，我们继续研讨AI Agent的未来。目前，AI Agent的进展可能在技术行业内比较流行，但还没有真正“出圈”。当AI Agent被充分使用时，哪个场景会是理想中的未来？</p><p>刘智勇：图灵测试是一个经典测试，用于评估机器是否具备人类智能，即在背靠背的情况下，判断对方是计算机还是人类。而我曾经提出过一个“面对面测试”，特别适用于人形机器人，尤其是高端的女性机器人。测试中，高端女性仿真机器人被指派到一个地点，与10位单身男性分别进行相亲，如果其中有9位男性最终发出了下一次约会的邀请，该机器人就通过了测试。这不仅考验机器人是否能够理解并执行任务，还考验它是否能够与人进行有效沟通和交流。如果机器人能够在这种面对面的互动中展现出高度的智能和亲和力，那么无论在用户交互、情感表达、行动能力还是外观颜值上，其都达到了非常高的标准。</p><p>通过这样的测试，机器人将展现出巨大的商业价值。因为当机器人在面对面互动中具有很好的亲和力时，就可以在各种职业领域中得到应用，包括教师、律师、前台接待、演艺、直播等各个领域。我认为，这种更泛化的Agent通过“相亲测试”的事件是一个标志性的里程碑，意味着AI Agent的能力和人机交互能力已经达到全新的水平。</p><p>马宇峰：Agent未来的发展趋势将是怎样的？当它们发展到一个成熟的阶段后，将会呈现出什么样的形态？</p><p>陈星亮：在企业场景中，Agent目前主要扮演辅助角色。我们正在考虑的是，Agent是否能够从围绕特定场景服务转变为围绕特定人员服务。随着这一趋势的逐步发展，我们可以设想，未来某些人的工作是否会逐渐被Agent取代，这可以在分工上进行明确划分。我认为，当Agent真正能够在企业中提升效率并降低成本时，就达到了真正的成熟阶段。</p><p>马宇峰：Agent落地过程中可能会遇到一些难以实现的场景，这就需要我们在筛选场景和逻辑执行上进行深入的思考，最终的理想状态是以人的方式来看待Agent：作为智能体能够取代当前人类的多少工作。Agent概念并非是大模型出现后才产生的，但确实又是一直存在的。智能体这个词，最常见的体现可能就是人类自己。人类可以作为Agent 选择任务难度的度量，同时也可以作为Agent的驱动方向。</p><p>如果有一天Agent真的取代了所有的工作，人类应该干什么？我想，这时人类最重要的价值就是发挥自己的需求。Agent服务的目标永远是人本身，人类有需求，才会有Agent去做这件事情。人类是需求的发起方，Agent只是去满足需求的一方。因此，人类仍然拥有最终的评价权和评估权，这种能力是大模型无论如何发展都无法达到的，除非Agent拥有像人一样的肉身，有自己的激素欲望和生理限制。</p><p>陈星亮：首先，我认为Agent的未来是充满无限可能的。无论是在各个行业，还是在ToB或ToC的体系中，人类社会有各式各样的场景需要Agent来提供支撑，因此它的发展前景是极其广阔的。其次，我相信Agent将是一个多样化的存在，无论是在技术实现还是在业务场景的应用上。目前Agent技术的发展呈现出百家争鸣的局面，这对技术行业来说是一件好事，意味着有更多的行业场景愿意尝试采用Agent，并进行投资。在这样的投入下，技术可以快速发展，进而更好地探索未知领域。</p><p>最后，在Agent向前发展的过程中，我们也需要正视现实情况。当前无论大模型还是Agent框架本身的发展，下一步的方向似乎还不是很清晰。我相信未来还会有更多新技术不断涌现，将推动Agent的发展，使企业和个人的诉求和场景得以实现。</p><p>刘智勇：从具身智能的角度来看，商业落地是一个重要议题。目前，Agent或具身智能体主要扮演的是辅助角色。以它们当前的智力水平，还不能承担替代型的角色。它们能够提升生产力，但并不能真正改变生产关系。我们应该从最大程度提升人的生产力的角度出发去寻找落地场景，这是比较实际和可行的视野。</p><p>另外是从更宏观的层面来看待Agent和具身智能的发展，这与AGI息息相关。在经历了Transformer模型、ChatGPT以及机器人的Transformer模型等重要时刻之后，我们可能在不久的将来迎来AGI的时代。具身AGI的到来会为人类社会带来新的篇章，即从碳基生命到硅基生命的延续。在具身智能领域，如果具有AGI的通用人形机器人能够实现，那么在某种程度上将实现仿生或永生的概念。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</id>
            <title>527蚂蚁技术日：AI应用矩阵集体“同框”</title>
            <link>https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EOxM7cd2GeS7pECMjxra</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 09:03:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 蚂蚁集团, 技术日, AI创新应用, 开放创新
<br>
<br>
总结: 蚂蚁集团每年5月27日举办技术日活动，展示AI创新应用产品，强调让AI像扫码支付一样便利生活，同时推动开放创新，探讨技术领域的多种可能性。 </div>
                        <hr>
                    
                    <p>每年的5月27日，是蚂蚁集团的技术日，用以勉励蚂蚁技术人保持敬畏和创新之心，到今天技术日已发展成为技术周，一个包含技术论坛、技术集市、编程大赛、技术沙龙等活动的技术嘉年华。今年，第九届蚂蚁技术日首次对外开放，开放日上展示了诸多蚂蚁AI创新应用产品，向外界透传了“让AI像扫码支付一样便利每个人的生活”的技术主张和面貌。</p><p></p><p></p><h2>AI是开放日关键词</h2><p></p><p></p><p>据了解，蚂蚁集团持续投入AI领域的研发，基于大规模业务场景的需求，布局了包括知识图谱、运筹优化、图学习、可信AI、大模型等在内的AI技术，其中蚂蚁百灵大模型已于去年底完成备案。蚂蚁集团CTO何征宇在开放日现场表示，一直以来蚂蚁集团致力于用最前沿的技术做最普惠的服务，近两年蚂蚁重投AI和数据要素技术，致力于实现AI规模化应用，让AI像扫码支付一样便利每个人的生活。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62093c4002f76e1330c69590a689887b.jpeg" /></p><p></p><p>记者注意到，在此次开放日的技术集市上，蚂蚁AI创新应用展位前尤其热闹。</p><p></p><p>三个智能管家，是蚂蚁百灵大模型的核心创新应用产品。生活管家“支付宝智能助理”连接了400万商家机构小程序和8000项数字生活服务，食住行游购娱等生活服务均可问可办；金融管家“支小宝”能通过文字对答提供行情分析、持仓诊断、保险配置、投教科普等理财保险服务，目前使用过支小宝的用户达4300万；蚂蚁也推出了一套医疗解决方案，在不久后会正式推出服务大众的医疗健康管家。目前这套解决方案已经服务了很多医院。譬如，浙江省卫健委与支付宝联合推出了可陪诊的数字健康人“安诊儿”。浙江省内用户就医时，可感受全流程的陪伴、指引、互动，就医体验进一步提升。目前，“安诊儿”已在浙江大学医学院附属第一医院、浙江大学医学院附属邵逸夫医院等浙江省内近百家医院中应用，服务超百万人次。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/4239f10cfd69871911e01e593f9f85f2.jpeg" /></p><p></p><p>记者注意到，多模态大模型技术也在蚂蚁内部有诸多创新应用，如AI智绘、AI智乐、AIGC数字人等，用户输入一句话、一张图、一段语音，就能生成一张设计图、一段音乐作品、一段动态人像视频。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3269137664bac2bbbd54e0350739530.jpeg" /></p><p></p><p>安全可信，是蚂蚁AI创新应用的一个特色。在集市现场，记者见到了大模型一体化安全解决方案“蚁天鉴”，可实现对大模型本身安全性的检测和防御；加速隐私计算关键硬件“乾坤卡”，能解决密码学运算慢的问题，提升全同态计算性能100倍。</p><p></p><p>蚂蚁集团CTO何征宇在交流会现场介绍，“我们相信AI不为替代人而生，是为每一个人而生。”为实现AI人人可享的目标，蚂蚁一直在努力优化和提高AI的可靠性、经济性和易用性。可靠性是用AI监督AI，以解决控制和“对齐”比人类聪明得多的模型；经济性是把大模型做“小”，这决定了大模型应用能否成为主流；易用性则是将智能“傻瓜化”，这决定了生成式AI是否能规模化落地应用。</p><p></p><p>据了解，在蚂蚁内部，AI已经成为重要生产工具。譬如，基于蚂蚁百灵大模型开发的智能研发平台CodeFuse已支持一半以上蚂蚁工程师的日常开发工作，他们提交的代码中10%由AI生成。</p><p></p><p></p><h2>开放创新是内核</h2><p></p><p></p><p>在技术开放日现场，来自学界、AI企业、开源界等诸多领域的人士共同探讨了当下最热门的话题。</p><p></p><p>在AI主题论坛上，来自智谱等AI企业和蚂蚁热烈讨论AI的多种可能性“the‘N’one”、“wherearethe‘angrybirds’？”……</p><p></p><p>蚂蚁开源嘉年华活动中，全球最大开源软件基金会之一，Apache的开源爱好者杭州本地群组走进蚂蚁，和开源爱好者、开发者们一起分享开源故事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/0594997f0f179bb5d0d00321a0c0c4c5.jpeg" /></p><p></p><p>在接下来持续一周的技术日里，20场关于数据技术的论坛，围绕大模型、支付技术、安全科技、技术风险、隐私计算、图计算等技术领域，从主题设计上看都是直指技术工业级应用中的挑战。据了解，直面规模化应用场景中的真问题也是蚂蚁技术日一直以来的特色。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/5741d52cad8bab79d79a970f2054884c.jpeg" /></p><p></p><p>48小时黑客松是技术日期间举办的极客编程挑战赛，48小时内完成一个创意的实现，主题既可与工作、业务相关，也可以放飞奇想，做有趣、好玩的创意。参加今年黑客松的创意，大多聚焦生成式AI与给AI降本相关，如绘画AGENT、基于AI构建无障碍世界、本地化大模型工作智能提效工具等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/ddc825844be408b848ac77f9776d5dcb.jpeg" /></p><p></p><p>T-STAR是蚂蚁技术人年度颁奖仪式，也是蚂蚁内部最高技术荣誉，颁发给过去一年的表现杰出的技术人和技术项目。今年的十大创新奖项涵盖安全科技和分布式计算存储，AI也首次成为大赢家。在颁奖现场，蚂蚁集团CTO何征宇说，“蚂蚁技术日因敬畏而生，但敬畏不代表畏手畏脚，公司正处于全力发展的大阶段，不发展才是最大的风险。我们把T-STAR放在每年的527揭晓，就是要向大家表明，蚂蚁技术不仅要牢记历史，敬畏风险，更要用于创新、敢于突破。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f88905ade6bf48aa210cbdad4ce6ba14.jpeg" /></p><p></p><p>设置技术日，体现了一家公司对技术的重视。2015年5月27日，因光纤被挖断，部分支付宝用户两个小时无法登录账户，这次事件也成为蚂蚁历史上次重大技术事故，此后，这一天被定为蚂蚁技术日。2018年，支付宝实现了宕机后分钟级可恢复，如今，支持系统稳定安全的分布式技术已经成为蚂蚁的核心和领先的技术赛道。今年，第九届技术日首次对外开放，让外界看到了在人工智能时代，蚂蚁技术的新赛道和新可能。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</id>
            <title>6个必须参加ArchSummit深圳的理由，错过等1年</title>
            <link>https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mbWEukF4hvAGPcMRloIA</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 May 2024 06:26:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ArchSummit, 智能进阶, 大模型时代, 人工智能
<br>
<br>
总结: ArchSummit全球架构师峰会将在深圳举办，围绕“智能进阶. 架构重塑”主题讨论企业架构在大模型时代的适应性和成本效益，探讨人工智能在架构中的应用，以及架构师如何规划职业道路。BAT等企业的顶尖专家将共探AI时代的无限可能。 </div>
                        <hr>
                    
                    <p>6月14日-15日，ArchSummit全球架构师峰会将在深圳举办。本次大会围绕“智能进阶. 架构重塑”主题，将探讨在 AI 浪潮下，企业架构如何适应大模型时代趋势，同时寻找具有成本效益的问题解决方案，帮助参会者更好地了解如何以及何时可以在架构中使用人工智能，同时探讨在架构师技术知识更新换代速度快的行业常态下，如何规划职业道路，保持自身的竞争力。</p><p></p><p>届时，头部AI大模型将同台竞技，BAT实力PK。顺丰CTO耿艳坤、阿里巴巴丁宇、Thoughtworks CTO Scott Shaw等来自国内外前沿企业的顶尖专家将与参会者面对面，共探AI时代的无限可能。</p><p></p><p>大会6大看点已备好，更多参会攻略可点击链接可查看详细日程：<a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">https://archsummit.infoq.cn/2024/shenzhen/schedule</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8efeed0960c87e2ae91cdce14eb6b86.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</id>
            <title>王小川：不烧钱推AI应用、大模型价格战是云厂商的游戏</title>
            <link>https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RIVAfAuacRahUG9MhJkp</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 10:14:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 价格战, Baichuan 4, AI助手, AGI发展路径
<br>
<br>
总结: 王小川发布了最新一代基座大模型Baichuan 4和首款AI助手“百小应”，强调了百川智能的AGI发展路径，明确不参与价格战，Baichuan 4在模型能力上达到国内第一，具备行业领先的多模态能力。王小川表示关注文本能力，发布了多款API接口，推出全新的MaaS+AaaS服务，以及针对Agent构建的Assistants API接口。同时强调了要有自己的超级应用，差异化是市场竞争力的关键。 </div>
                        <hr>
                    
                    <p>明确不参与“价格战”的王小川，在5月22日做了两个重要发布：最新一代基座大模型Baichuan 4 和成立后的首款AI助手“百小应”，同时强调了百川智能的AGI发展路径：超级模型+超级应用。</p><p>&nbsp;</p><p>“模型还没有到超级模型，应用上还没有到超级应用。”王小川在发布会开始就很明确地说道，“我们在通往AGI的路上。”但这次发布仍然对百川来说意义非凡，Baichuan 4的模型能力达到了国内第一，也正式入局了 C 端市场。</p><p>&nbsp;</p><p></p><h2>模型能力，国内第一</h2><p></p><p>&nbsp;</p><p>相较1月底发布的 Baichuan 3，Baichuan 4 在各项能力上均有极大提升，其中通用能力提升超过10%，数学和代码能力分别提升14%和9%。</p><p>&nbsp;</p><p>榜单成绩已经是各家发布大模型时的必备项目了。这次，百川分享了国内权威大模型评测机构SuperCLUE的评测结果。在最新的测试结果中，Baichuan 4的模型能力达到了国内第一。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/aa/aa1f6ed330a84bacc9809f53737893e4.png" /></p><p></p><p>&nbsp;</p><p>与国外主流大模型对比，Baichuan4在知识百科、长文本、生成创作等文科类中文任务上明显优于国外大模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c037fff5834687120dc87a15af5d9134.png" /></p><p></p><p>&nbsp;</p><p>此外，Baichuan 4还具备行业领先的多模态能力，在MMMU、MMBench-EN、CMMMU、MMBench-CN、MathVista等评测基准上表现优异，大幅领先Gemini Pro、Claude3-sonnet等多模态模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/633320d70ccdf66d3b267fd358839821.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>不过王小川也表示，相较多模态，百川更关注于文本能力。“多模态没有智力”，王小川解释道，多模态只是建立了一个分类的体系。而在像“1+2=3”这个等式里，智力不体现在1+2如何等于3的计算里，而是在等号里面。</p><p>&nbsp;</p><p>Baichuan 4能在较短的时间内取得突破，源于其在训练过程中引入了诸多领先技术优化手段：</p><p>&nbsp;</p><p>数据方面，在预训练阶段采用了基于Model-based+Human-based的协同数据筛选优化方法，以及对长文本建模位置编码科学的Scaling Law，有效提升了模型对数据的利用；SFT对齐方面，重点优化了模型Reasoning、Planning、Instruct following能力，通过loss驱动的数据选取与训练，多阶段爬坡，多模型参数融合等方式有效提升了模型的关键指标和稳定性；突破RLHF和RLAIF融合的RLxF强化学习对齐技术，大幅提升模型的指令遵循等能力；成本方面，提出了新的投机采样方案clover，通过将序列知识与并行解码结合，使得投机采样的命中率提升至60％，成本降低30％以上。</p><p>&nbsp;</p><p>此外，百川智能还宣布开放Baichuan 4、Baichuan3-Turbo、Baichuan3-Turbo-128k、Assistant API 四款API ：</p><p>&nbsp;</p><p><a href="https://platform.baichuan-ai.com/playground">https://platform.baichuan-ai.com/playground</a>"</p><p>&nbsp;</p><p></p><h4>“价格战是云厂商的游戏”</h4><p></p><p>&nbsp;</p><p>王小川坦言，当前 API 收入对百川而言并不多，也构不成收入重点。</p><p>&nbsp;</p><p>对于最近大模型市场打得火热的“价格战”，王小川表示，一方面，这表明了大家太看好大模型前景，不愿意失去任何机会，宁愿零价格也要进来入场；另一方面，降价的核心是要看商业模式的，如果是做ToB服务，最后售卖的不是模型本身而是整套云服务。云厂商是比较偏传统的服务模式，进到一个新的战场可以降价，但降价仅限于云厂商。在有限场景打价格战，已经走出了创业公司的射程。</p><p>&nbsp;</p><p>“这波降价跟之前的滴滴美团还不一样，因为那会儿的价格战或补贴背后带有网络效应，是双边网络，那个商业模式在改变生产关系，比如司机和乘客的关系、外卖员和用户之间的关系。而这次降价更像当初四小龙的降价方式，而不是像滴滴美团这样的价格竞争，这次不是生产关系的改变，而是直接做生产力供给。我觉得这件事情对我们而言，就是别掺和进去。”王小川说道。</p><p>&nbsp;</p><p>除了大模型发布，百川智能还推出了全新的MaaS+AaaS服务。MaaS版块由基座模型组成，分为旗舰版和专业版。旗舰版将全量开放Baichuan 4的各项能力，专业版包含Baichuan3-Turbo和Baichuan3-Turbo-128K两款模型，价格比旗舰版Baichuan 4更实惠，且对企业高频场景做了针对性优化，综合测试相比GPT-3.5整体效果提升8.9%。百川智能表示，MaaS 的新用户即日起可以获得1000万免费token。</p><p>&nbsp;</p><p>Assitants API则是百川智能在Baichuan 4基础上针对Agent构建推出的API接口，不仅支持Code interpreter、RAG内建工具，还支持自定义工具调用，方便企业接入各种丰富复杂的API。据悉，百川智能未来还将推出零代码Agent创建平台产品。</p><p>&nbsp;</p><p></p><h2>超级应用</h2><p></p><p>&nbsp;</p><p>“之前我就提到一定要有自己的超级应用，如果只是学OpenAI一开始有个模型做API服务，在中国的创业公司而言，是走不通的。”王小川说道。</p><p>&nbsp;</p><p>对于这一点，王小川的考虑有三：首先，虽然在美国做ToB是好生意，但在中国的商业环境中，C端市场就比B端大10倍；其次，做To B，收的是人民币，花的是美金；最后，大厂都会卷这件事情，这是大厂射程范围内的。因此，百川智能一定要做差异化，低价作为市场竞争力是不够的。</p><p>&nbsp;</p><p>这次，百川智能终于发布了自己的首款C端应用：“百小应”。当被质疑这款应用是不是发晚了的时候，王小川直言，“我正好觉得相反，发早了。”他认为，之前的各种应用更多是为了展示大模型能力，而到现在这个领域还没有到一个成熟的状态。</p><p>&nbsp;</p><p>王小川理想中的AI助手是有温度的，“作为伙伴，而不是工具”。他不吝惜对Kimi的赞赏，“Kimi打赏的理念，就是不是把它当工具，而是当成一个伙伴、一个人。”</p><p>&nbsp;</p><p>“懂搜索、会提问”是百川给百小应总结的有别于其他AI助手的地方：</p><p>&nbsp;</p><p>“懂搜索”意味着不是像其他搜索一样做汇总，而是让模型掌握专业的搜索技能，用户在搜索的时候理解意图、可以拆词作多轮搜索。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/77a416611b4f0dc130706d819cf26ac3.png" /></p><p></p><p>&nbsp;</p><p>“会提问”则是让模型能引导用户表达自己的需求。这是由于当前很多人在搜索时表达只有关键词、不够准确，机器只能去猜，现在百川则是通过让机器反问的方式让用户表达出来，比如：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e44e938f621a835783a00c4c22ef6a9.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>除了强大的搜索和提问功能以外，用户还能在百小应中上传PDF、word文档，或者直接输入网页链接（URL），阅读并分析书籍、报告、学术论文等长篇内容，百小应一分钟就能读完上市公司财报。</p><p>&nbsp;</p><p>在Baichuan 4多模态能力的支持下，用户提问的同时还可以同步上传图片，百小应对图片内容进行解读，或者将图片作为补充材料，获取更精准的回答。此外，它还支持用户通过语音的方式进行交互。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/3235c04e1c07d8300470f186f525a483.jpeg" /></p><p></p><p>&nbsp;</p><p>王小川表示，百小应发布后不会为了获客疯狂投广告，“这并不是健康的行为。”“我们手上有足够的资金，不会通过立马发布一个产品去寻找下一轮融资。我们的精力会放在产品价值上。”</p><p>&nbsp;</p><p>对于商业模式，王小川认为现在还不到时候。“以Kimi为例，虽然现在有足够的用户体量，不管是走流量模式还是广告模式都有意义，但其实没有本质的区别。”具体的变现时机，要看一个场景下多少用户有付费意愿。</p><p>&nbsp;</p><p>当然，现在的百小应并不是王小川心目中的超级应用，“今天百万级DAU都远远称不上一个超级应用至少要再提升两个数量级。”王小川预计，百小应今年内会有大的升级变化。</p><p>&nbsp;</p><p>“同一代模型中，百川更有机会把超级应用做出来。”王小川说道。显然，百川智能对超级模型和超级应用两个赛道都非常有信心，“百川未来可能不再按月发布，而是按季度发布，团队要把时间调到长线来做事情。”王小川说道。</p><p>&nbsp;</p><p>未来百川智能将如何实现自己的AGI，我们拭目以待。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</id>
            <title>大语言模型加持，是智能运维架构的未来吗？</title>
            <link>https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wl9bMYS5WRCUFr9ZsjqA</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 08:59:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能运维架构, 大语言模型, SRE-Copilot框架, AIOps挑战赛
<br>
<br>
总结: 在QCon北京2024大会上，字节跳动技术专家王宁分享了基于大语言模型的智能运维架构SRE-Copilot框架的演讲。该框架结合了大语言模型和AIOps实践，通过模拟不同团队的运维知识来解决技术问题，降低训练成本，提升故障处理能力，并支持自然语言交互，为智能运维带来新的可能性。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>演讲嘉宾&nbsp;|&nbsp;王宁&nbsp;字节跳动技术专家策划&nbsp;|&nbsp;蔡芳芳整理&nbsp;|&nbsp;Penny编辑&nbsp;|&nbsp;褚杏娟、傅宇琪</blockquote><p></p><p></p><p>在&nbsp;QCon&nbsp;北京&nbsp;2024&nbsp;大会上，字节跳动技术专家王宁，根据自己在字节的实践经历，发表了题为《<a href="https://qcon.infoq.cn/2024/beijing/presentation/5740">SRE－Copliot：基于大语言模型的智能运维架构</a>"》的演讲。</p><p></p><p>本文由InfoQ整理，经王宁老师授权发布。以下为演讲实录。</p><p></p><p>随着大语言模型的广泛应用和能力的提升，许多团队都在尝试利用大语言模型来改进他们所在行业的应用，我们团队在字节跳动内部也在探索如何将大语言模型与&nbsp;AIOps&nbsp;实践相结合。</p><p></p><p>去年，我们带着&nbsp;SRE-Copilot&nbsp;框架参加了&nbsp;AIOps&nbsp;2023&nbsp;挑战赛，并荣幸地获得了冠军。在比赛中，我们设计了一套更为开放和富有想象力的框架，进行了初步的探索。</p><p></p><p>比赛的题目是开放性的，旨在鼓励大家尽可能地探索大语言模型在智能运维领域的应用潜力。选手面临的挑战是帮助企业运维团队应对日益庞大的系统规模、复杂的系统结构以及日益增多的数据量。</p><p></p><p>比赛所用的数据来自中国建设银行“建行生活”APP中的真实模拟数据。比赛的架构设计模拟了从入口负载均衡集群到中间的基础集群，如抢券集群和订单集群，这些集群之间相互依赖。此外，还包括了出口负载均衡集群，并且依赖许多复杂中间件如&nbsp;Redis、MySQL、Kafka&nbsp;等的各个集群。</p><p></p><p>我们在比赛中获得的数据包括调用链数据、业务黄金指标，例如订单成功率、抢券成功率，系统交易的每个订单的延迟时间，以及集群的性能指标，不仅涵盖了基础集群，还有中间件集群的监控数据，如&nbsp;CPU&nbsp;负载、常见的出入网流量，系统日志数据等。</p><p><img src="https://static001.geekbang.org/infoq/08/08436d00c6950fe9cbaff7d3c9890161.png" /></p><p></p><p></p><h2>为什么需要SRE-Copilot&nbsp;框架</h2><p></p><p></p><p>随着系统日益复杂和数据量的增加，即使是专业的运维团队也难以全面掌握所有技术细节。在大型企业中，每个组件，如计算、存储、数据库，都可能需要专门的运维团队。当出现大规模故障时，通常需要所有团队协作来定位问题根源。而&nbsp;SRE-Copilot&nbsp;框架可以通过大语言模型的能力，学习和模拟不同团队的运维知识，以解决整个链路上的技术问题。</p><p></p><p>传统的&nbsp;AIOps&nbsp;在异常检测和根因分析上严重依赖于标注数据，这限制了算法的泛化能力，因为它们需要在有监督的环境下进行训练。而大语言模型能够学习更多的通用知识，减少对标注数据的依赖，从而降低训练成本。</p><p></p><p>运维团队积累的专家经验很难编码到算法模型中。通常，这些经验会被简化为阈值或复杂的规则，不仅难以维护，也难以传承。SRE-Copilot&nbsp;框架通过大语言模型，将专家经验转化为模型可以理解和推理的形式，从而提升了故障处理的能力。</p><p></p><p>传统&nbsp;AIOps&nbsp;的接入和维护成本较高，需要业务和算法团队深入理解业务逻辑和算法模型。此外，私域数据的处理和定制化开发也增加了成本。SRE-Copilot&nbsp;框架采用集成学习的概念，通过模块化设计，使得系统能够像搭积木一样动态编排。</p><p></p><p>在传统&nbsp;AIOps中，未遇到过的故障很难被解决，因为它们超出了模型的训练范围。大语言模型展现出了强大的推理能力，能够基于通用知识和训练中学到的关键字，推断出未知故障的性质，即使没有相似的训练数据。</p><p></p><p>传统的&nbsp;AIOps&nbsp;解决方案需要用户理解模型并精确地传递参数，而&nbsp;SRE-Copilot&nbsp;框架支持自然语言交互，使得非技术用户也能轻松地与系统交互，提高了用户体验，并有潜力开放给更广泛的用户群体。</p><p></p><p>SRE-Copilot&nbsp;框架，采用了基于&nbsp;function&nbsp;call（函数调用）的方法来实现多功能的智能运维。</p><p></p><p>我们首先将&nbsp;SRE&nbsp;在日常运维中可能遇到的多种场景进行了统一收敛，通过大语言模型来理解用户的意图。无论是进行运维可视化还是故障分类，我们的目标是编排不同的工具，以实现一个多场景的智能运维解决方案。</p><p></p><p>在比赛上，我集成了一些简单的场景，虽然每个场景本身并不复杂，但我们尽可能探索了许多新的场景，以测试和展示&nbsp;SRE-Copilot&nbsp;的多功能性。比如我们实现了故障分类功能，并让框架能够生成故障自愈的代码，以自动化处理常见的问题。</p><p></p><p></p><h2>实践效果如何？</h2><p></p><p></p><p>我利用大语言模型生成了一个排查故障的工作流，并以自然语言的形式呈现。</p><p></p><p>根据提前设定的&nbsp;Agent，模型会匹配并确定哪些&nbsp;Agent&nbsp;能够在排障工作流程中发挥作用，然后将这些&nbsp;Agent&nbsp;编排成一个可执行的工作流。我可以通过自然语言向模型提问，比如询问特定时间段内的问题。模型会提取相关参数，并动态地将任务分配给相应的子&nbsp;Agent。每个子&nbsp;Agent&nbsp;会检查自己的数据，寻找故障迹象。</p><p></p><p>例如，调用链&nbsp;Agent&nbsp;会检查调用链是否存在问题。如果检测到&nbsp;CMDB（配置管理数据库）中的信息，模型可以进行下钻操作，比如定位到具体的集群，然后触发下一轮更深入的检测。如果没有更多的信息，模型会开始进行根因诊断，检索历史故障和专家经验，以此来判断当前的故障类型，比如磁盘写满。</p><p></p><p>在诊断过程中，我会将相关指标进行可视化展示，帮助理解故障的性质。根据诊断结果，我会生成清理磁盘的简单代码。如果是线上执行，模型会匹配预设的自愈方案。每次故障诊断完成后，会自动生成告警总结和复盘报告。诊断的故障信息会自动积累下来。如果用户对诊断结果进行确认，那么这个故障案例就会被记录到历史故障库中，供模型未来诊断时参考。</p><p></p><p>在比赛的复现过程中，我们在建行云的&nbsp;8&nbsp;核和&nbsp;16G&nbsp;内存的堡垒机上进行了演示，并使用了一张&nbsp;V100&nbsp;显卡。实际上，我们仅使用了开源的&nbsp;ChatGLM&nbsp;6B&nbsp;的小模型，就实现了上述效果。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b12c49c627aefbd1ebb7f1bdbb33a961.png" /></p><p></p><p></p><h2>SRE-Copilot&nbsp;架构</h2><p></p><p></p><p></p><h4>Tool&nbsp;calling</h4><p></p><p></p><p>在&nbsp;SRE-Copilot&nbsp;架构中，Tool&nbsp;calling&nbsp;是指将大语言模型与外部工具有效结合或交互的能力。这种机制允许模型解决它自身无法直接回答的问题，因为它可以调用外部工具来获取所需的信息。</p><p></p><p>以询问天气为例，大语言模型本身无法提供实时天气信息，因为它缺乏对外部环境的感知。为了解决这个问题，需要设计一个封装用户问题的方法，并提供一个能够获取当前天气的接口工具，再通过一些描述指导模型使用接口。</p><p></p><p>例如，接口可以根据给定的地点查询天气，它是一个&nbsp;function，需要两个参数：地点（location）和温度单位（如摄氏度或华氏度）。成熟的大语言模型，如&nbsp;ChatGPT&nbsp;或文心一言，通常都是通过相似的训练方法来实现这一机制。模型会调用适当的函数，并且能够从用户的问题中提取出必要的参数。假设用户询问的是伦敦的天气，模型会自动将“伦敦”作为&nbsp;location&nbsp;参数补全，并指导调用天气查询接口，从而提供准确的天气信息。</p><p></p><p></p><h4>RAG</h4><p></p><p></p><p>SRE-Copilot&nbsp;架构中的另一个重要概念是检索增强生成（Retrieval-Augmented&nbsp;Generation，简称RAG）。RAG&nbsp;结合了搜索技术和大语言模型，旨在解决几个关键问题。</p><p></p><p>首先，它可以帮助处理私域知识，例如公司内部服务器的状态或特定订单的详情，这些信息是大语言模型无法直接感知的。</p><p></p><p>RAG&nbsp;能够解决新知识的问题。以电影为例，如果模型的知识截止日期是&nbsp;4&nbsp;月，那么它不会知道&nbsp;5&nbsp;月上映电影的主演是谁。同样，对于当天发生的新故障或新闻，模型也无法感知。</p><p></p><p>RAG&nbsp;也适用于长尾问题，即那些在模型训练时不常见或非常具体的问题。例如，如果用户需要编写某个小众语言的代码，或者询问关于特定数据库连接的问题，尤其是当这个数据库是公司内部改造过的，传统的大语言模型可能无法提供答案。为了解决这些问题，可以利用传统的检索方法，比如搜索公司内部文档，查找是否有关于特定语言连接数据库的&nbsp;QA&nbsp;文档。一旦检索到相关文档，我会将文档中的相关内容与用户的问题一起输入给大语言模型，以便模型能够提供一个更准确的答案。</p><p></p><p></p><h4>Reason+Act</h4><p></p><p></p><p>ReAct&nbsp;概念针对的是那些无法仅通过一步查询或大语言模型自身直接解决的任务。这些任务通常需要多步骤的执行，并且每一步都需要模型提供其思考过程，以减少模型产生幻觉现象的风险。</p><p></p><p>通过&nbsp;ReAct，我们可以将任务的每一步规划和执行可视化。例如，如果任务是让大语言模型去厨房做菜，模型可能会首先思考需要哪些调料，并预测它们可能存放的位置。比如，模型可能会推断出胡椒可能在&nbsp;1&nbsp;到&nbsp;6&nbsp;号柜子里，或者&nbsp;1&nbsp;到&nbsp;3&nbsp;号台面上。模型首先会选择检查第一个柜子，如果没有找到胡椒，它会根据这一步骤的结果，决定下一步检查其他柜子。</p><p></p><p>再比如，当用户询问&nbsp;GitHub&nbsp;上某位开发者获得的点赞数并想要得到该数值除以&nbsp;2&nbsp;的结果时，模型的第一步是通过&nbsp;GitHub&nbsp;的&nbsp;Open&nbsp;API&nbsp;获取点赞数，然后使用计算器工具进行除法运算，通过这两步操作来得到最终结果。</p><p></p><p>ReAct&nbsp;概念旨在通过分步骤地规划，并在每一步中进行合理的推理和决策，来执行越来越复杂的任务。</p><p></p><p></p><h4>Agent&nbsp;智能体</h4><p></p><p></p><p>智能体主要由三个部分组成：</p><p>大脑：最核心的部分是大语言模型，它扮演着智能体的大脑角色，负责处理和生成语言输出，同时整合和利用知识库中的信息。感知：智能体的第二部分是它的感知能力，这涉及到使用外部工具来感知图片、声音等信息。通过这些工具，智能体能够与外部世界进行交互，获取必要的数据。行动：智能体还可以通过计算&nbsp;API、查询&nbsp;API&nbsp;等工具进行实际的交互操作，执行真实世界中的行动。这使得智能体能够处理更为复杂的任务，如根据图片内容推断位置并查询当地天气。</p><p></p><p>我们期望实现的是多个智能体之间的复杂交互，让智能体不仅能够独立工作，还能够相互协作，共同解决更加复杂的问题。</p><p></p><p></p><h4>架构设计思路</h4><p></p><p></p><p>在&nbsp;SRE-Copilot&nbsp;框架的具体实现上，我根据比赛时提供的数据，在最底层为每个数据源设计了相应的&nbsp;Agent。这些数据源包括日志、调用链、交易类型数据、主机监控数据以及&nbsp;CMDB&nbsp;数据等多元数据类型。对于每种类型的数据都设计了一个&nbsp;Agent，使其能够进行异常检测、数据可视化，以及查询历史故障和返回故障描述等操作。</p><p></p><p>在底层&nbsp;Agent&nbsp;之上，我定义了功能型&nbsp;Agent，它包括知识库问答、工作流规划、故障报告编写和代码生成等能力。这些功能型&nbsp;Agent&nbsp;能够基于&nbsp;ReAct&nbsp;框架，调用底层的子&nbsp;Agent，并动态地编排它们的执行顺序和结果。</p><p></p><p>在框架的最顶层，我设计了一个名为&nbsp;Copilot&nbsp;的智能体，它作为与人类用户交互的核心。Copilot&nbsp;负责识别用户的意图，判断用户是想要获取答案还是进行故障诊断，并从用户的问题中提取出相关参数。</p><p></p><p>例如，用户可能询问某个具体时间段的情况，Copilot&nbsp;可以将这个时间段作为参数传递给后续的处理流程。随着大语言模型的智能化，它甚至能够理解并处理如“过去&nbsp;15&nbsp;分钟”这样模糊的时间段，并将其转换为具体的参数。顶层&nbsp;Copilot&nbsp;还能够进行任务分配和&nbsp;Agent&nbsp;之间的协调工作，确保整个框架能够高效地响应用户的需求。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e989cfcbd307002b6b49ee8341b83e3.png" /></p><p></p><p></p><p>在设计&nbsp;SRE-Copilot&nbsp;框架时，我借鉴了&nbsp;GPT&nbsp;背后的集成学习思想。我们的目标并不是开发一个庞大而全面的模型来解决所有问题，而是通过集成多个专精于特定领域的子&nbsp;Agent&nbsp;来实现。每个子&nbsp;Agent&nbsp;都在其专业领域内表现出色，我们采用混合专家系统（Moe,&nbsp;Mixture&nbsp;of&nbsp;Experts）的形式，使整个系统的效果更加完善和高效。</p><p></p><h2>SRE-Copilot&nbsp;技术细节</h2><p></p><p></p><p></p><h4>主要运维能力：异常检测</h4><p></p><p></p><p>首先介绍一个异常检测场景，以下是我们在这个场景中定义的角色：</p><p>Copilot&nbsp;主持人：作为核心，Copilot&nbsp;负责解析用户需求，制定运维计划，并安排不同&nbsp;Agent&nbsp;执行具体任务。多数据源&nbsp;Agent：针对不同的数据源，如日志、调用链、交易类型数据、主机监控数据以及&nbsp;CMDB&nbsp;数据，我们设计了专门的&nbsp;Agent。这些&nbsp;Agent&nbsp;利用合适的算法对各自模态的数据进行异常检测和检索。RCA&nbsp;Agent：该&nbsp;Agent&nbsp;负责收集其他&nbsp;Agent&nbsp;的检测结果，并执行最终的根因分析推理。</p><p></p><p>以&nbsp;Copilot&nbsp;为起点，例如，当收到用户关于&nbsp;9&nbsp;月&nbsp;18&nbsp;日下午&nbsp;4&nbsp;点大量交易失败的请求时，Copilot&nbsp;会识别这是一个根因诊断问题，并将任务交给&nbsp;RCA&nbsp;Agent。</p><p></p><p>RCA&nbsp;Agent&nbsp;会关注用户请求中的关键信息，如交易类数据，并将其传递给交易类型&nbsp;Agent进行初步检查。该&nbsp;Agent&nbsp;将返回关于交易量同比下降的结果，但不提供额外信息。</p><p></p><p>根据初步检查结果，系统会动态调度其他&nbsp;Agent&nbsp;检查各自负责的组件是否存在问题。例如，当轮到第五个&nbsp;Agent，即调用链&nbsp;Agent&nbsp;时，它发现了调用链上的异常情况。调用链&nbsp;Agent&nbsp;的反馈将引导&nbsp;CMDB&nbsp;Agent&nbsp;在&nbsp;CMDB&nbsp;中查询接口的调用记录，上游下游关系，以及接口的具体问题。</p><p></p><p>通过这种方式，我们模拟了真实线上云平台中多个组件之间的协同定位过程。整个异常检测过程完全自动化，没有一个固定的流程，而是根据每个&nbsp;Agent&nbsp;的检测结果动态地调度其他&nbsp;Agent，共同完成异常检测任务。</p><p></p><h4>主要运维能力：根因定位</h4><p></p><p></p><p>有了上一步的异常检测结果，接下来需要对这个异常检测的结果做根因定位，或者说故障的分类。</p><p></p><p>首先，获取异常检测组件的结果，并对其进行了一些调整，使其返回的不仅是传统的&nbsp;true/false&nbsp;或&nbsp;JSON&nbsp;结构体，而是大语言模型和人类都容易理解的自然语言描述。例如，异常检测结构体可能会返回：“本次故障持续了&nbsp;10&nbsp;分钟，CPU&nbsp;指标飙升，内存也被打满，某某接口出现了大量失败”。</p><p></p><p>然后，这个故障工单会转换成向量，并在提前构建的向量数据库中检索。向量数据库包含两部分内容：一部分是提前配置的专家经验，另一部分是历史上遇到的相似故障记录。例如，如果历史上遇到过内存打满的问题，通过重启服务解决了问题，这样的专家经验会被记录在系统中。</p><p></p><p>在当前故障检测时，系统会同时检索出相关的专家经验和历史故障信息。通过&nbsp;RAG，将当前故障描述、专家经验和历史故障信息一起输入给大语言模型，进行根因推断。大模型会根据提供的信息推理，例如，这是否是内存打满的问题，是否需要通过重启服务来解决。</p><p></p><p>RAG&nbsp;方法可以帮助大语言模型不断地更新其知识库，适应新的故障情况，同时减少模型的幻觉现象。通过引入历史故障数据和反思机制，模型的准确率得到提升，幻觉现象得到降低。即使在没有提前配置专家经验或历史故障数据的情况下，使用的&nbsp;6B&nbsp;大小的模型（如ChatGLM&nbsp;3）仍能够对某些类型的故障做出准确的推理，如磁盘写满或&nbsp;Java&nbsp;GC&nbsp;问题。</p><p></p><h2>字节的实践探索</h2><p></p><p></p><p></p><h4>基于&nbsp;LLM&nbsp;的&nbsp;RCA-Agent&nbsp;构建</h4><p></p><p></p><p>字节跳动内部的目标是先将基于大语言模型的根因诊断（RCA）Agent&nbsp;框架落地应用，因为根因诊断是&nbsp;SRE&nbsp;团队面临的一个主要挑战，它占用了大量的时间和精力，日常的&nbsp;On&nbsp;Call&nbsp;问题定位也给团队成员带来了沉重的负担。我们希望专注于解决这些实际问题，真正缓解&nbsp;SRE&nbsp;同事的痛点。</p><p></p><p>我们定义了一些工具和插件，是在出现故障时用来进行检测的工具。除了工具和插件，我们还设计了工作流编排，以自动化和优化故障处理流程。我们构建了一个知识库，它包含了历史故障数据、专家经验和故障处理策略，这些都是进行有效根因分析的关键资源。</p><p></p><p></p><h4>知识库的构建</h4><p></p><p></p><p>构建知识库方面所做的工作主要包括以下几个部分，并且我们计划未来会引入更多用户原始文档、历史&nbsp;On&nbsp;Call&nbsp;记录等不同类型的数据。</p><p></p><p>排障专家经验：这部分是根据根因诊断的场景特别设计的，目的是让业务团队的成员能够管理和记录他们的知识和经验。我们定义的每一个经验都是一组根因故障，包括故障发生时的描述和一些止损措施的组合。这些信息将被用来训练大语言模型推理。</p><p></p><p>例如，流量突增导致的故障，其根因可能是用户&nbsp;QPS&nbsp;的突增。故障的表现可能是流量首先突增，随后内存和&nbsp;CPU&nbsp;使用率也跟着上升，最终导致服务不可用。这种描述将帮助模型理解故障模式。对于上述故障，可能的止损措施包括重启服务或进行扩容操作。</p><p></p><p>故障场景&nbsp;SOP&nbsp;文档：我们希望用户输入的是一些&nbsp;SOP&nbsp;文档。这种方式给组件团队提供一种灵活管理知识的方法。我们选择这种半规范化文档的形式，是因为当前大语言模型的能力还有局限，需要通过文档梳理来帮助模型更好地理解。</p><p></p><p>历史故障信息：我们还维护了一个历史故障信息库，记录每一次通过大语言模型检测到的故障，这些记录会用来对组件团队进行训练和打标。</p><p></p><p></p><h4>基础工具的构建</h4><p></p><p></p><p>在构建&nbsp;SRE-Copilot&nbsp;框架的基础工具方面，我们参考了&nbsp;OpenAI&nbsp;GPTs&nbsp;将工具集成到平台时所遵循的规范。我们将运维场景中的一些关键指标和基础工具进行了统一管理，把传统的异常检测方法统一成一个工具，用户只需要维护他们需要进行异常检测的指标即可。</p><p></p><p>用户可以自定义检测项，包括指标名称、指标的标签或指标描述，以及定义何为异常表现。因为是用户自定义的工具，所以可以根据具体需求设置检测标准。</p><p></p><p>我们实现了一个变更事件查询工具，当出现故障时，用户可以通过调用这个接口来确定是否由线上变更导致。我们在平台上部署的组件配置了一些工具，例如异常检测、变更和事件查询等，还包括了自然语言的意图理解和大语言模型的根因推理功能。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d193671ffeaa6714378964ace886acc9.png" /></p><p></p><p></p><h4>核心工具：LLM&nbsp;根因推理</h4><p></p><p></p><p>关键的根因推理功能基于大语言模型。在实践中，我采用了一种新的方法，不再将故障数据压缩成向量空间进行聚类分析，而是利用大语言模型将故障映射到自然语言空间中，从而简化了故障分类过程。</p><p></p><p>例如，流量突增、内存升高和接口不可用等信息可以自然地描述为&nbsp;QPS&nbsp;问题。对于新出现的具有相同描述的故障，可以直接使用大语言模型进行分类，而无需计算向量空间中的相似度。</p><p></p><p>分类过程中会使用用户配置的专家经验和当前故障的检测结果。此外，工具还利用预定义的模板生成描述，并让大语言模型进行推断，而用户可以根据需要调整这些模板、专家经验和检测结果。</p><p></p><p>为了提高根因推断的准确性，确保检测项插件的描述足够具体，比如详细描述指标曲线的形态变化，以及变更检测组件提供的详细信息，如变更的范围和性质。这样的方法不仅提高了故障分类的效率，还能让大语言模型的推理过程更加精确。</p><p></p><p></p><h4>工作流的构建</h4><p></p><p></p><p>下一步是构建工作流，目前这一过程仍然需要用户自行配置，这主要是由于大语言模型当前能力的限制所做出的妥协。</p><p></p><p>不过，我们正在探索一种新的方法，即允许用户在其&nbsp;SOP&nbsp;文档中预先设定工作流，例如，文档中可以指明首先需要检查哪些指标，以及根据这些指标的结果接下来应该检查哪些指标。</p><p></p><p>我们希望能够训练大语言模型，使其能够直接根据用户的&nbsp;SOP&nbsp;文档生成工作流。最终，SRE&nbsp;团队能够向大语言模型提供一个简单的文档，甚至是未经格式化的文本，而模型能够根据文档中的指标或检测项动态地编排诊断步骤，并根据每一步的检测结果，智能地调度后续的执行流程。</p><p></p><p></p><h4>Agent&nbsp;的应用与调试</h4><p></p><p></p><p>我们在一些组件上进行了&nbsp;RCA&nbsp;的试点工作，实现了一些改变传统交互方式的效果。现在，用户可以通过提出模糊的问题来与系统交互，例如询问某个集群存在哪些故障，系统会自动识别并调用相应的集群诊断工作流。</p><p></p><p>完成诊断后，系统不仅能够将结果进行美化或封装，还能以卡片或自然语言的形式向用户直观展示。此外，在面对大规模故障时，系统能够自动解析故障群中的告警卡片信息或历史消息，判断需要诊断的参数，如特定集群和时间段，自动提取所需参数，并触发相应的诊断流程。</p><p></p><p></p><h2>未来展望</h2><p></p><p></p><p>针对我们遇到的瓶颈，我认为未来需要继续在下面几个方向增强。</p><p></p><p>第一，我期望有越来越大、能力越来越强的通用大语言模型，因为随着这些模型能力的不断提升，在动态根因推理和每一步的动态决策上，它们的准确率和效果都将得到显著提升。</p><p></p><p>第二，我期待能够开发出更强大、更专业的模型，例如时序大模型或日志大模型，并将这些专业模型作为工具，供核心Agent调用。</p><p></p><p>第三，我正在探索是否能有更灵活的Agent框架，以支持多轮和更灵活的对话，比如让用户在排障过程中随时打断并提供关键信息，以此缩短故障排查的平均时间。实现多人或多Agent之间的真正协作。</p><p></p><p>第四，我期望Agent能在更丰富的场景中应用，目前它们可以像实习生一样帮助进行简单的监控数据处理，未来随着模型和工具能力的增强，我希望它们能够发展到像初级员工或应届生那样进行一些简单的决策，甚至最终成为一个资深专家，能够自动诊断问题并执行高级决策。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</id>
            <title>老黄急了！为跟华为抢业务，英伟达也得低头降价，但大家已经不买账了？</title>
            <link>https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QwgwNwRZTX4o10fh64yU</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 07:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯片产品, 价格下调, 竞争压力, 制裁限制
<br>
<br>
总结: 英伟达在中国市场下调芯片产品价格，面临竞争压力和制裁限制，导致销售疲软，需应对中国市场挑战。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p></p><p>5&nbsp;月&nbsp;24&nbsp;日，根据路透社的报道，英伟达下调了中国特供版芯片产品的价格，H20&nbsp;芯片现在的售价比华为的Ascend&nbsp;910B便宜&nbsp;10%&nbsp;以上。</p><p></p><p>知情人士透露，英伟达在与华为的中国市场芯片争夺战中销售疲软，性能问题、充足的供应和制裁限制，迫使其定价低于华为的芯片产品。中国日益增长的竞争压力，给这家美国半导体设计公司的投资者也敲响了警钟。</p><p></p><p>在美国对人工智能芯片出口实施制裁和竞争加剧的背景下，价格下行凸显了英伟达中国业务面临的挑战，为其在中国市场的前景蒙上了一层阴影，而这一市场对英伟达2024财年营收的贡献率高达17%。</p><p></p><h2>价格从略低到更低，但买家大多倒向华为？</h2><p></p><p></p><p>去年年底，英伟达推出了三款为中国量身定制的芯片，分别是HGX&nbsp;H20、L20&nbsp;PCle和L2&nbsp;PCle。三款新产品都是从H100的基础版修改而来，包含了英伟达用于人工智能工作的大部分最新功能，但一些计算能力被削减以符合美国新规。</p><p></p><p>三款芯片中，H20最受关注，因为它是英伟达可在中国销售的性能最强大的产品。但三位供应链消息人士透露，市场上该芯片的供应充足，反而表明对其需求的疲软。</p><p></p><p>据悉，H20芯片刚开始接受分销商的预订时，定价还与华为的竞争产品相当。短短几月，情况就发生了变化。今年&nbsp;2&nbsp;月，H20芯片每张定价约在1.2万~1.5万美元区间（约8.6~11万人民币），略低于华为Ascend&nbsp;910B约12万元的售价；分销商提供的预装了&nbsp;8&nbsp;块&nbsp;AI&nbsp;芯片的&nbsp;H20&nbsp;服务器售价为&nbsp;140&nbsp;万元人民币。</p><p></p><p>有消息人士表示，在某些情况下，H20芯片价格会以低于华为Ascend&nbsp;910B（华为公司最强大的AI芯片）售价的10%以上。</p><p></p><p>中国的服务器分销商正在以每张卡约10万元人民币的价格出售H20，八卡服务器每台售价约为&nbsp;110&nbsp;万元~130&nbsp;万元人民币。相比之下，分销商以每张卡12万元以上的价格出售华为910B，而其八卡服务器的起价为&nbsp;130-150&nbsp;万元/台。“H20&nbsp;和华为&nbsp;910B&nbsp;的价格也会根据订单的规模而波动。”消息人士补充说。</p><p></p><p>当时，一位消息人士指出，就规格而言，H20&nbsp;在&nbsp;FP32&nbsp;性能方面似乎落后于&nbsp;910B，而&nbsp;FP32&nbsp;是衡量芯片处理普通任务速度的关键指标，H20&nbsp;的&nbsp;FP32&nbsp;性能还不到其竞争对手的一半。但&nbsp;H20&nbsp;在互联速度方面似乎比&nbsp;910B&nbsp;更有优势，互联速度是衡量芯片间数据传输的速度。“这意味着在需要将大量芯片连接起来作为一个系统工作的应用中，H20&nbsp;仍能与&nbsp;910B&nbsp;竞争。”</p><p></p><p>有分析师表示，H20&nbsp;芯片的表现将是影响英伟达中国业务的一个重要因素，而长期前景则取决于其如何与本土科技巨头华为竞争。虽然华为去年才开始与英伟达竞争，但消息人士称，今年华为将大幅增加其Ascend&nbsp;910B芯片的出货量，该芯片在某些关键指标上优于H20。</p><p></p><p>根据路透社对现有政府采购数据的核查显示，在过去六个月中，仅有五家国有或国有附属企业买家表示有兴趣购买H20芯片，而同期有十多家买家表示有兴趣购买华为的910B芯片。不过，这些数据并不详尽，可能无法反映市场需求的全部范围。</p><p></p><p>对比在中国的市场处境，英伟达在国外仍然保持发展势头。5&nbsp;月&nbsp;25&nbsp;日，据The&nbsp;Information报道，埃隆·马斯克本月告诉投资者，其初创公司&nbsp;xAI&nbsp;计划在&nbsp;2025&nbsp;年秋季之前建造一台超级计算机，它将依靠数万个英伟达&nbsp;H100&nbsp;芯片，以为未来更智能的&nbsp;Grok&nbsp;聊天机器人提供动力。马斯克将这台超级计算机称为“计算超级工厂”，耗资数十亿美元建造。</p><p></p><h2>制裁所带来的利润挤压</h2><p></p><p>由于美国旨在限制中国成为科技强国的制裁，英伟达的&nbsp;H800&nbsp;和&nbsp;A800&nbsp;在中国被禁售，包括&nbsp;H100&nbsp;和&nbsp;B100&nbsp;在内的其他先进产品线也已被禁。之后，英伟达针对中国市场推出了新产品，并调整了其人工智能芯片战略以应对制裁令。</p><p></p><p>在英伟达的第一季度财报中，该公司的高管们警告称，受制裁影响，他们在中国的业务成绩已“大幅”低于过去。英伟达的首席财务官Colette&nbsp;Kress&nbsp;指出，“相比10月实施新的出口管制许可之前的水平来说，我们在中国的数据中心收入大幅下降，预计未来中国市场的竞争仍将非常激烈。”</p><p></p><p>消息人士称，H20&nbsp;芯片上个月开始在中国广泛销售，一个多月后就交付给了客户。中国的一些科技公司已经下了订单，其中阿里巴巴订购了3万多块H20芯片。</p><p></p><p>研究机构SemiAnalysis的创始人Dylan&nbsp;Patel表示，到2024年下半年，将有近100万颗H20芯片运往中国，英伟达必须在定价上与华为竞争。</p><p></p><p>“尽管H20&nbsp;的售价是&nbsp;H100（2022&nbsp;年禁止出口到中国英伟达人工智能芯片）的一半，但由于更高的内存容量，其制造成本高于&nbsp;H100。这是利润率的急剧下降。”Patel&nbsp;表示。</p><p></p><h2>英伟达：做最坏的打算</h2><p></p><p></p><p>在英伟达降价之际，其&nbsp;H20芯片或许还迎来在中国取得成功的另一大“绊脚石”：除了制裁和由此产生的微薄利润外，英伟达还必须应对中国政府鼓励企业支持国产芯片发展的指令问题。尽管上述三位消息人士中的两位表示，近几个月来这些指令有所放松。</p><p></p><p>5&nbsp;月&nbsp;14&nbsp;日，据The&nbsp;Information援引知情人士报道称，中国监管机构已建议字节跳动、腾讯、阿里巴巴和百度等大型科技公司减少购买外国制造的AI芯片，转而购买更多国产芯片，尤其是英伟达GPU一直是大多数国内科技科技公司的首选。但监管机构希望科技公司为新的互联网数据购买同等数量的国产和外国制造的AI芯片，这是中国监管机构首次为企业在购买AI芯片制定具体指引。</p><p></p><p>根据监管要求，如果国内公司选择订购更多的外国芯片而不是本土芯片，它们必须以书面形式详细说明订购美国芯片进行部署的数量，并证明其合理性。但据悉，该指令尚未得到严格执行，目前尚不清楚是否会对不合规公司施加任何处罚。</p><p></p><p>根据中国市场研究公司赛迪咨询（CCID&nbsp;Consulting）的一份报告，预计到2035年，中国在全球人工智能行业的份额将超过30%。有分析师表示，尽管英伟达正在努力抢占中国的芯片市场份额，但前景越来越不确定。</p><p></p><p>“英伟达正在走一条微妙的路线，试图在维持中国市场和应对美国紧张局势之间取得平衡。”IG市场分析师Hebe&nbsp;Chen表示，“从长远来看，英伟达肯定在为最坏的情况做准备。”</p><p></p><p>前不久，还有媒体爆料称，华为Ascend&nbsp;910B&nbsp;在&nbsp;2024&nbsp;年的出货量将达到40万片以上，单卡价格在7万元左右。这对英伟达来说，恐怕将造成更大的冲击。有网友这样评价，“英伟达最怕的不是少了中国市场，而是中国的市场体量可以培育出一个可怕的竞争对手。”</p><p></p><p>参考链接：</p><p><a href="https://www.reuters.com/technology/nvidia-cuts-china-prices-huawei-chip-fight-sources-say-2024-05-24/">https://www.reuters.com/technology/nvidia-cuts-china-prices-huawei-chip-fight-sources-say-2024-05-24/</a>"</p><p><a href="https://www.yahoo.com/tech/nvidias-ai-chip-sales-china-170128954.html">https://www.yahoo.com/tech/nvidias-ai-chip-sales-china-170128954.html</a>"</p><p><a href="https://www.theinformation.com/articles/chinese-regulators-tell-local-tech-firms-to-buy-fewer-nvidia-chips">https://www.theinformation.com/articles/chinese-regulators-tell-local-tech-firms-to-buy-fewer-nvidia-chips</a>"</p><p><a href="https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/">https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/1HSU228X61i7sId0r75U</id>
            <title>MatrixOne → MatrixOS：矩阵起源的创业史即将用“AI Infra”和“AI Platform”书写新章程</title>
            <link>https://www.infoq.cn/article/1HSU228X61i7sId0r75U</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/1HSU228X61i7sId0r75U</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 06:12:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化浪潮, MatrixOne, AI 技术, 数据处理能力
<br>
<br>
总结: 在数字化浪潮的推动下，MatrixOne 以构建能支撑新一代数字世界的操作系统为目标，从处理数据到整合 AI 能力，不断升级发展，最终推出了 AI-Native 数据智能全域操作系统 MatrixOS，满足市场对算力和数据管理的需求。MatrixOne 的发展历程体现了对 AI 技术和数据处理能力的持续迭代和创新。 </div>
                        <hr>
                    
                    <p>在数字化浪潮的推动下，MatrixOne 的故事就像一部科技界的创业史诗，它始于一个简单而宏伟的梦想——构建一个能够支撑起新一代数字世界的操作系统。想象一下，在 AIGC 时代，数据流动如同“血液”，算法运转如同“心跳”，用户界面如同“呼吸”，新一代操作系统像指挥家一样，精准处理多元化的海量数据、操控着算力分配，这个数字世界将变得多么有活力。</p><p></p><p>故事的起点是在 2021 年，MatrixOne 的创始团队站在数字世界的入口，他们看到了一个巨大的机会——数据，这个新时代的石油正等待着被开采和利用。想要让数据发挥出真正的潜力，就必须要有强大的存储和处理能力。于是，MatrixOne 诞生了，它最初的定位是超融合异构数据库，能够处理各种类型的数据，满足不同行业的需求。</p><p></p><p>然而，随着时间的推移，MatrixOne 的团队发现，仅仅处理数据是不够的。在这个由人工智能和机器学习驱动的时代，算力成为了新的战场，企业和开发者们不再满足于单一的数据存储解决方案，他们需要的是一个能够整合 AI 能力、满足算力需求，完成 AI 基础设施建设同时，还能实现业务成本、效率与技术先进性平衡的平台。于是，MatrixOne 的研发团队矩阵起源决定将在 MatrixOne &nbsp;的基础上，扩展业务至 &nbsp;AI Infra (人工智能基础设施) 和 AI Platform(人工智能平台) 领域，并与世纪互联的 AIDC(Artificial Intelligence Data Center，人工智能数据中心) 业务进行深度融合和紧密协作。对此，InfoQ 对矩阵起源 CEO 王龙进行了特别专访，围绕矩阵起源的业务拓展规划，聚焦数据库和 AI 技术的未来发展展开了讨论。</p><p></p><p></p><h2>MatrixOne 升级至 MatrixOS，是 AI 浪潮下市场需求的“响应”</h2><p></p><p></p><p>AI 技术高速发展，已经成为了推动各行各业变革的核心技术力量，而 AI 技术的发展依赖于强大的算力和高效的数据处理能力。目前围绕这两者的软件平台众多，但这一繁荣背后隐藏着一个挑战——标准化的缺失。GPU 的云化、虚拟化和资源池化等领域尚未形成统一标准，这与已经成熟的 CPU 公有云服务形成对比。在数据管理方面，传统数据库解决方案和新兴的 AI 驱动的数据技术都在探索如何更有效地管理和使用数据。AI 技术的加入为数据的使用和管理带来了新的维度，许多公司正在尝试创新的解决方案，如向量数据库、RAG 模型和 AI 代理技术等，都是在适应这一变化。</p><p></p><p>矩阵起源认为，“在数据和算力之间建立有效的连接是至关重要的。”所以他们在开发 MatrixOne 云原生数据库时，就专注于连接“未来数据”和“过去数据”，解决传统数据库分裂带来的数据割裂问题，目标是将结构化、非结构化、半结构化等不同类型的数据融合起来，以支持 AI 在不同行业中的落地应用，这种数据融合能力，和算力、数据的兼容策略，为他们提供了与市场上其他数据平台和算力平台厂商相比的领先优势。</p><p></p><p>于是，当矩阵起源在近日完成千万美元 Pre A 轮融资后，便决定将这笔资金用于开发极简统一、开源开放的 AI-Native 数据智能全域操作系统 MatrixOS。据悉，该系统将由大规模异构算力纳管调度平台 MatrixDC、超融合异构数据管理平台 MatrixOne 和 AI 智能体应用开发平台 MatrixGenesis 三部分组成，目标是打造链接算力、数据、知识、模型与企业应用的 AI Native 软件平台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce04e808a663546f8c5a75d6db2ec2c2.webp" /></p><p></p><p>Matrix OS 提供了一个统一的算力和数据管理平台，不同用户可以根据自己的需求进行不同的组合使用。对于 AI 科学家来说，Matrix OS 解决了他们需要在不同系统间移动数据和算力的问题，他们可以将自己的训练数据放在算力平台上进行训练，训练完成后，模型可以托管在同一平台上。对此，王龙表示，“Matrix OS 提供了一个统一操作平台，简化了数据和算力之间的连接和管理。”</p><p></p><p>同时，对于开发者而言，他们可能不需要进行模型训练，但需要使用 API 调用大模型或传统模型进行推理，所以 Matrix OS 同样提供了简化数据连接问题的、性价比高的工具，使开发者能够将 API 调用与传统应用和工作流程结合。而对于最终用户来说，他们能够更切实地感受到应用优化速度变快、应用延迟降低以及性价比提高。例如，AI 服务的成本（如 token 价格）和数据库服务的成本（如节点价格）都变得更低，应用变得更加流畅，延迟更低，使用成本也更低。</p><p></p><p>从媒体的角度来看，MatrixOne 升级至 MatrixOS，这是一个非常值得关注的行业动态，这是矩阵起源对现有服务的持续改进和对新兴技术的积极拥抱。看得出，矩阵起源在保持其核心业务的连续性和稳定性的同时，也在持续积极地做技术创新探索，以满足市场和用户不断变化的需求。MatrixOne 的这次升级是对现有服务的补充而非替代，此次矩阵起源的业务拓展，不仅是对现有业务的一次强化，更是对未来趋势的一次前瞻性布局。</p><p></p><p></p><h2>AI Infra 和 AI Platform，是一站式 AI 解决方案的必要条件</h2><p></p><p></p><p>对于想投入 AI 建设的企业来说，底层的 AI Infra 提供必要的硬件和软件基础，支持大规模数据处理和 AI 模型训练，而顶层的 AI Platform 提供易于使用的服务和工具，可以帮助企业快速开发和部署 AI 应用。对于做数据库起家的厂商来说，从底层到顶层，提供一站式 AI 解决方案，可以更好地增强客户黏性，扩大技术服务优势。</p><p></p><p>所以当我们从宏观视角去看 MatrixOS，大规模异构算力纳管调度平台 MatrixDC、超融合异构数据管理平台 MatrixOne 和 AI 智能体应用开发平台 MatrixGenesis 这三个子产品之间的技术能力是非常相关联的，功能上有所重叠，但技术能力上相互互补，共同构成了一个强大的数据智能平台。</p><p></p><p>过去，MatrixOne 本质上可以说是一个湖仓一体的数据平台，但比 Hadoop 要简单得多。技术上，MatrixOne 的架构独特，实现了多层管理，包括数据的冷热分层、读写分层和存算分离，这种架构允许灵活调度计算资源和数据资源，让 MatrixOne 像变形金刚一样可以适应银行、互联网、IoT 等不同的应用场景，比如为了适应大模型技术的发展，MatrixOne 已经具备了向量数据库的能力，便可以支持 AI 需求。如今，MatrixOne 能够非常自然地管理来自智能物联网和 AI 的新数据源，这将是“未来数据”的主要来源，也是 MatrixOS 运转的基础燃料，MatrixOS 据此能够更好地与 AI 世界连接。</p><p></p><p>在 AI Infra 领域，想要实现颠覆性技术变革，就必须要解决算力和电力这两个核心问题。目前像英伟达的 stargate 计划、中国政府建设的智算中心都在硬件层面上积极提升算力，在软件方面则主要在围绕 GPU 进行技术升级，当前行业内需要一个软硬结合的解决方案来完成未来的 AI 基础设施建设。为此，矩阵起源把 MatrixDC 作为 MatrixOne 和 MatrixGenesis 的资源底座，利用云原生技术帮助企业构建 AI 原生的资源管理和调度平台。</p><p></p><p>最近世纪互联领投的 Pre A 轮投资让矩阵起源可以在万卡集群上测试软件，为他们提供了难得的实践机会。拥有全球人才团队的矩阵起源集中优势，通过在万卡集群上的实践，快速迭代，构建了技术壁垒，创造出竞争力强的 MatrixDC ——提供大规模异构 GPU，能够快速满足 AIGC 应用场景算力需求，在算力集群管理方面，可以很好地帮助数据中心进行异构芯片和 GPU 服务器的统一纳管及资源池化，实现算力资源的共享和动态调度，提高资源利用率。</p><p></p><p>不仅如此，MatrixDC 还灵活支持 CPU 计算资源，可以高效运行结构化交易 / 分析型业务应用；拥有 AINet 高速网络架构，能够很好地支撑千卡乃至万卡集群训练工作负载，全托管企业级容器服务，企业可以充分享受容器和微服务架构的便利。同时，MatrixDC 采用 Serverless 按量计费的商业模式，能够帮助企业大幅降低 AI 创新应用的探索成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6a0d6f761ba49320931fd50f7a4a7ec.webp" /></p><p></p><p>而在 AI Platform 领域，目前的主要技术挑战是要解决大模型幻觉问题，以及大模型和传统行业的应用的集成问题，现在行业内亟需一个平台来整合和连接所有相关的能力，同时将不同的技术和解决方案汇集在一起。这种整合和协作，将催生出新的 AI 开发范式，将大大降低 AI 技术应用的开发门槛。于是，矩阵起源将通过 MatrixGenesis 与 MatrixOne ，为 AI 开发者和用户提供一站式的数据共享、模型训练以及大模型应用开发和运维平台，帮助 AI 技术在各行各业快速落地，典型的应用场景包括企业知识库管理、智能客服、投诉分类、智能报告生成、政策咨询以及办公助理等。</p><p></p><p>其中，MatrixGenesis 作为一个 AI 智能体应用开发平台，“Model as a Service”（MaaS），不仅仅关注软件层面，还从数据到算力进行整体优化，通过提供全链路开发支持及高性价比的大模型服务，以帮助企业 0 门槛搭建 AI 应用。它支持大模型的对比选择、部署和推理，与结构化系统数据和知识库实时打通，提供 AI 应用工程流编排、应用对接集成的全生命周期开发支持， 企业可以一键部署开源和闭源大模型，快速完成模型精调和推理加速。同时，MetrixGenesis 平台允许客户以更高的性价比访问数据和算力资源，大幅降低业务使用成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8333508caf85625b133247ab4afcfe7.webp" /></p><p></p><p>在应用落地阶段，MetrixGenesis 利用 MatrixOne 的支持，为用户提供了完整的数据管理和使用能力，这包括 RAG、知识库和 Agent 等组件，它们通过 0 代码的配置方式，共同作用于应用的部署和运行，确保应用能够高效地利用数据资源。王龙形容 MetrixGenesis 时说道，“用户可以在这个平台上实现从数据到算力再到应用的全面优化，确保 AI 智能体应用的快速构建。”</p><p></p><p></p><h2>MatrixOS 与世纪互联 AIDC 的合作，是“中国版 CoreWeave”构建的开端</h2><p></p><p></p><p>在这个数字化迅速发展的时代，AI 技术如同一股不可阻挡的潮流，席卷了整个商业世界，众多企业纷纷开设 AIDC 部门，致力于构建和管理人工智能基础设施，提供算力、数据存储、模型训练等关键服务，以期在这场技术革新的浪潮中占据一席之地。大家深知，AI 技术的崛起不仅能够为企业带来新的增长点，更能在激烈的市场竞争中提升企业的核心竞争力。</p><p></p><p>世纪互联作为国内领先的第三方数据中心提供商，其 AIDC 策略具有鲜明的特点，具有非常坚决的“All In AI”的决心，将所有资源投入到 AI 算力的发展中，将传统的以机房机柜为中心的服务转变为“以 AI 算力为核心”。这一转变，反映了世纪互联对未来数字世界的深刻洞察——算力将成为未来 IT 世界最重要的核心资产。矩阵起源与世纪互联的合作初衷，也正是基于双方对算力和电力重要性的共识以及共同推进 AI 领域发展的愿景。</p><p></p><p>矩阵起源以其开源的 MatrixOS 软件，与世纪互联的 AIDC 产品相结合，共同打造了 GPU 平台服务——Neolink.AI，并将在今年的第三季度上线。这个平台不仅解决了企业在 AI 算力、软件连接能力以及成本控制上的问题，更是定位于成为一个开源、共享的 AI 算力平台，旨在降低推理成本，吸引更多用户，从而实现从成本中心到价值创造中心的转变。</p><p></p><p>世纪互联认为 AI 的上限是算力和电力，而矩阵起源则认为算力和数据是 AI 的瓶颈，所以他们不谋而合地提出了打造“中国版 CoreWeave”的想法，希望通过 Neolink.AI 平台，推动 AI 技术的本土化发展，提升国内 AI 产业的创新力和竞争力。这个想法的实现，将对中国的 AI 领域产生深远的影响，为中国在全球 AI 领域的发展奠定坚实的基础。</p><p></p><p>在对王龙的采访中，我们也了解到，世纪互联即将获得一批合规的 H20 芯片，这批芯片的到来将是一个重要的里程碑，标志着世纪互联在算力资源方面迈出了坚实的一步，将会通过 Neolink.AI 开放给大家。</p><p></p><p></p><h2>拥抱开源的 MatrixOS，是赋能千行百业 AI 落地的加速器</h2><p></p><p></p><p>在当下这场澎湃的 AI 浪潮中，企业急需一艘能够驾驭风浪的航船。MatrixOS，便是一艘非常有潜力的快船。MatrixOS 不仅仅是一个操作系统，它是企业 AI 构建的加速器，它通过提供高效、稳定的数据处理能力和灵活的 AI 技术支持，让企业能够轻松跨越技术门槛，无需投入巨额资源和精力去构建复杂系统，便能迅速实现 AI 应用构建。从基础设施层，到数据库层，再到应用开发层，MatrixOS 为千行百业提供 AI 时代的工具全家桶，助力生态打造高效智能的 AIGC 方案，如制造业的智能化生产、金融业的精准风控……</p><p></p><p>展望未来，MatrixOS 的技术创新之路充满了无限可能。矩阵起源在未来将会持续努力寻找新数据、新场景。随着计算能力的不断突破，MatrixOS 将能够处理更庞大、更复杂的数据集，为 AI 技术的发展提供坚实的算力支持和模型服务。</p><p></p><p>据王龙表示，MatrixOS 的探索不止于此。它还将与物联网、5G 等前沿技术紧密结合，打造全新的技术解决方案。在这个物联网设备日益普及、5G 网络商用化的时代，数据正以前所未有的速度增长，MatrixOS 将通过融合这些技术，实现更高效的数据管理能力，为企业提供更及时、更准确的数据服务、AI 开发服务，推动千行百业迈向更高层次的智能化和数字化。</p><p></p><p>同时，MatrixOS，作为一个开源平台，其生态建设的重要性不言而喻。它将与世纪互联等战略投资者携手，吸引更多企业和个人加入到开源社区中来，共同推动 AI 技术的进步和普及。MatrixOS 还将举办线下交流活动、技术大赛，加强社区成员之间的交流和合作，共同营造一个开放、共享、创新的 AI 技术生态。</p><p></p><p>MatrixOS，不仅是技术的革新，更是智慧的传承，让我们共同见证并参与这场伟大的变革。MatrixOS 的故事才刚刚开始，希望更多的企业和个人关注并支持 MatrixOS 的发展，共同推动 AI 技术的升级和普及，为各行各业的“全面 AI 化”的数字化转型贡献智慧和力量，迎接一个更加智能、更加互联的 AI 未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</id>
            <title>大模型应用之基于Langchain的测试用例生成</title>
            <link>https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0dc98342868f3aca8a539dcf7</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 05:54:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型生成测试用例, LangChain, 测试用例生成方案, 技术细节说明
<br>
<br>
总结: 文中介绍了使用大模型生成测试用例的实践效果，探索了基于LangChain的方法，解决了现有工具的痛点，提高了测试用例生成的效率和质量。详细介绍了基于LangChain的测试用例生成方案和实现细节，包括整体流程、技术细节说明、代码框架和部分代码展示。整体内容涵盖了使用大模型生成测试用例的全过程。 </div>
                        <hr>
                    
                    <p></p><h1>一 用例生成实践效果</h1><p></p><p>在组内的日常工作安排中，持续优化测试技术、提高测试效率始终是重点任务。近期，我们在探索实践使用大模型生成测试用例，期望能够借助其强大的自然语言处理能力，自动化地生成更全面和高质量的测试用例。</p><p></p><p>当前，公司已经普及使用JoyCoder，我们可以拷贝相关需求及设计文档的信息给到JoyCoder，让其生成测试用例，但在使用过程中有以下痛点：</p><p></p><p>1）仍需要多步人工操作：如复制粘贴文档，编写提示词，拷贝结果，保存用例等</p><p></p><p>2）响应时间久，结果不稳定：当需求或设计文档内容较大时，提示词太长或超出token限制</p><p></p><p>因此，我探索了基于Langchain与公司现有平台使测试用例可以自动、快速、稳定生成的方法，效果如下：</p><p><img src="https://static001.geekbang.org/infoq/43/4328f52343c11804154293baa39b28a9.png" /></p><p>（什么是LangChain？ 它是一个开源框架，用于构建基于大型语言模型（LLM）的应用程序。LLM 是基于大量数据预先训练的大型深度学习模型，可以生成对用户查询的响应，例如回答问题或根据基于文本的提示创建图像。LangChain 提供各种工具和抽象，以提高模型生成的信息的定制性、准确性和相关性。例如，开发人员可以使用 LangChain 组件来构建新的提示链或自定义现有模板。LangChain 还包括一些组件，可让 LLM 无需重新训练即可访问新的数据集。）</p><p></p><h1>二 细节介绍</h1><p></p><p></p><h2>1 基于Langchain的测试用例生成方案</h2><p></p><p></p><p>因3种方案使用场景不同，优缺点也可互补，故当前我将3种方式都实现了，提供大家按需调用。</p><p></p><h2>2 实现细节</h2><p></p><p></p><h3>2.1 整体流程</h3><p></p><p>​</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0228566441a94e1b2543d97d8d6bebb.png" /></p><p>​</p><p></p><h3>2.2 技术细节说明</h3><p></p><p>•pdf内容解析： ：Langchain支持<a href="https://python.langchain.com/docs/modules/data_connection/document_loaders">多种文件格式的解析</a>"，如csv、json、html、pdf等，而pdf又有很多<a href="https://zhuanlan.zhihu.com/p/352722932">不同的库</a>"可以使用，本次我选择PyMuPDF，它以功能全面且处理速度快为优势</p><p></p><p>•文件切割处理： 为了防止一次传入内容过多，容易导致大模型响应时间久或超出token限制，利用Langchain的<a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token">文本切割器</a>"，将文件分为各个小文本的列表形式</p><p></p><p>•Memory的使用： 大多数 LLM 模型都有一个会话接口，当我们使用接口调用大模型能力时，每一次的调用都是新的一次会话。如果我们想和大模型进行多轮的对话，而不必每次重复之前的上下文时，就需要一个Memory来记忆我们之前的对话内容。Memory就是这样的一个模块，来帮助开发者可以快速的构建自己的应用“记忆”。本次我使用<a href="https://python.langchain.com/docs/modules/memory/types">Langchain的ConversationBufferMemory与ConversationSummaryBufferMemory</a>"来实现，将需求文档和设计文档内容直接存入Memory，可减少与大模型问答的次数（减少大模型网关调用次数），提高整体用例文件生成的速度。ConversationSummaryBufferMemory主要是用在提取“摘要”信息的部分，它可以将将需求文档和设计文档内容进行归纳性总结后，再传给大模型</p><p></p><p>•向量数据库： 利用公司已有的向量数据库[测试环境Vearch]，将文件存入。 在创建数据表时，需要了解向量数据库的检索模型及其对应的参数，目前支持六种类型，IVFPQ，HNSW，GPU，IVFFLAT，BINARYIVF，FLAT（<a href="https://github.com/vearch/vearch/wiki/Vearch%E7%B4%A2%E5%BC%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9">详细区别和参数可点此链接</a>"），目前我选择了较为基础的IVFFLAT--基于量化的索引，后续如果数据量太大或者需要处理图数据时再优化。另外Langchain也有很方便的<a href="https://python.langchain.com/docs/integrations/vectorstores/vearch">vearch存储和查询的方法可以使用</a>"</p><p></p><h3>2.3 代码框架及部分代码展示</h3><p></p><p>代码框架：</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc4beb2a38f97d3ea492c03462913e12.png" /></p><p>​</p><p>​代码示例：</p><p></p><p><code lang="text">    def case_gen(prd_file_path, tdd_file_path, input_prompt, case_name):
        """
        用例生成的方法
        参数:
        prd_file_path - prd文档路径
        tdd_file_path - 技术设计文档路径
        case_name - 待生成的测试用例名称
        """
        # 解析需求、设计相关文档, 输出的是document列表
        prd_file = PDFParse(prd_file_path).load_pymupdf_split()
        tdd_file = PDFParse(tdd_file_path).load_pymupdf_split()
        empty_case = FilePath.read_file(FilePath.empty_case)

        # 将需求、设计相关文档设置给memory作为llm的记忆信息
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content="You are a chatbot having a conversation with a human."
                ),  # The persistent system prompt
                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # Where the memory will be stored.
                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # Where the human input will injected
            ]
        )
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        for prd in prd_file:
            memory.save_context({"input": prd.page_content}, {"output": "这是一段需求文档，后续输出测试用例需要"})
        for tdd in tdd_file:
            memory.save_context({"input": tdd.page_content}, {"output": "这是一段技术设计文档，后续输出测试用例需要"})

        # 调大模型生成测试用例
        llm = LLMFactory.get_openai_factory().get_chat_llm()
        human_input = "作为软件测试开发专家，请根据以上的产品需求及技术设计信息，" + input_prompt + ",以markdown格式输出测试用例，用例模版是" + empty_case
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            verbose=True,
            memory=memory,
        )
        output_raw = chain.invoke({'human_input': human_input})

        # 保存输出的用例内容，markdown格式
        file_path = FilePath.out_file + case_name + ".md"
        with open(file_path, 'w') as file:
            file.write(output_raw.get('text'))
</code></p><p></p><p><code lang="text">    def case_gen_by_vector(prd_file_path, tdd_file_path, input_prompt, table_name, case_name):
        """
        !!!当文本超级大时，防止token不够，通过向量数据库，搜出某一部分的内容，生成局部的测试用例，细节更准确一些!!!
        参数:
        prd_file_path - prd文档路径
        tdd_file_path - 技术设计文档路径
        table_name - 向量数据库的表名，分业务存储，一般使用业务英文唯一标识的简称
        case_name - 待生成的测试用例名称
        """
        # 解析需求、设计相关文档, 输出的是document列表
        prd_file = PDFParse(prd_file_path).load_pymupdf_split()
        tdd_file = PDFParse(tdd_file_path).load_pymupdf_split()
        empty_case = FilePath.read_file(FilePath.empty_case)
        # 把文档存入向量数据库
        docs = prd_file + tdd_file
        embedding_model = LLMFactory.get_openai_factory().get_embedding()
        router_url = ConfigParse(FilePath.config_file_path).get_vearch_router_server()
        vearch_cluster = Vearch.from_documents(
            docs,
            embedding_model,
            path_or_url=router_url,
            db_name="y_test_qa",
            table_name=table_name,
            flag=1,
        )
        # 从向量数据库搜索相关内容
        docs = vearch_cluster.similarity_search(query=input_prompt, k=1)
        content = docs[0].page_content

        # 使用向量查询的相关信息给大模型生成用例
        prompt_template = "作为软件测试开发专家，请根据产品需求技术设计中{input_prompt}的相关信息:{content},以markdown格式输出测试用例，用例模版是:{empty_case}"
        prompt = PromptTemplate(
            input_variables=["input_prompt", "content", "empty_case"],
            template=prompt_template
        )
        llm = LLMFactory.get_openai_factory().get_chat_llm()
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            verbose=True
        )
        output_raw = chain.invoke(
            {'input_prompt': input_prompt, 'content': content, 'empty_case': empty_case})
        # 保存输出的用例内容，markdown格式
        file_path = FilePath.out_file + case_name + ".md"
        with open(file_path, 'w') as file:
            file.write(output_raw.get('text'))
</code></p><p></p><h1>三 效果展示</h1><p></p><p></p><h3>3.1 实际运用到需求/项目的效果</h3><p></p><p>用例生成后是否真的能帮助我们节省用例设计的时间，是大家重点关注的，因此我随机在一个小型需求中进行了实验，此需求的PRD文档总字数2000+，设计文档总字数100+（因大部分是流程图），结果效率提升50%。</p><p></p><p>本次利用大模型自动生成用例的优缺点：</p><p></p><p>优势：</p><p></p><p>•全面快速的进行了用例的逻辑点划分，协助测试分析理解需求及设计</p><p></p><p>•降低编写测试用例的时间，人工只需要进行内容确认和细节调整</p><p></p><p>•用例内容更加全面丰富，在用例评审时，待补充的点变少了，且可以有效防止漏测</p><p></p><p>•如测试人员仅负责一部分功能的测试，也可通过向量数据库搜索的形式，聚焦部分功能的生成</p><p></p><p>劣势：</p><p></p><p>•暂时没实现对流程图的理解，当文本描述较少时，生成内容有偏差</p><p></p><p>•对于有丰富经验的测试人员，自动生成用例的思路可能与自己习惯的思路不一致，需要自己再调整或适应</p><p></p><h1>四 待解决问题及后续计划</h1><p></p><p>1.对于pdf中的流程图（图片形式），实现了文字提取识别（langchain pdf相关的方法支持了ocr识别），后续需要找到更适合解决图内容的解析、检索的方式。</p><p></p><p>2.生成用例只是测试提效的一小部分，后续需要尝试将大模型应用与日常测试过程，目前的想法有针对diff代码和服务器日志的分析来自动定位缺陷、基于模型驱动测试结合知识图谱实现的自动化测试等方向。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</id>
            <title>甲骨文副总裁：只会SQL也可以搞定AI，但对 DBA 的要求将更高</title>
            <link>https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W63jFI4t7PT6ZM5LVglH</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 May 2024 02:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, Oracle Database 23ai, AI功能, 向量搜索
<br>
<br>
总结: 甲骨文公司发布了Oracle Database 23ai，其中加入了突破性的AI技术，包括AI for Data、简化数据中的AI使用等功能，其中的AI Vector Search功能可以根据概念内容搜索文档、图像和关系数据。这一举措使得即使只懂得SQL的用户也能够全盘搞定AI应用，展示了数据库AI化的必然趋势。 </div>
                        <hr>
                    
                    <p>“最重要的不在于拥有多少大模型，而在于如何使用它们。”甲骨文公司副总裁及中国区董事总经理吴承杨说道。&nbsp;</p><p>&nbsp;</p><p>在全民探索大模型应用的现在，甲骨文也积极尝试。Oracle 融合数据库中的最新版本&nbsp;<a href="https://www.oracle.com/cn/database/">Oracle Database 23ai</a>"&nbsp;作为广泛的云技术服务正式发布，标志着甲骨文在AI领域的重大进展。</p><p></p><h2>“只会SQL，也可以全盘搞定AI”</h2><p></p><p>&nbsp;</p><p>由于这个版本主要加入了突破性的 AI 技术，因此甲骨文将 Oracle Database 23c 重命名为 Oracle Database 23ai。</p><p>&nbsp;</p><p>甲骨文公司中国区技术咨询部高级总监李珈介绍，23ai 专注AI主要体现在三个方面：一是AI for Data，在数据应用在AI的层面上做深入加持；二是针对应用开发者，在使用层面上能够更简单；三是针对关键任务赋予AI 能力。</p><p>&nbsp;</p><p>此长期支持版本包含了&nbsp;Oracle AI Vector Search 、300多个主要新功能和数千项增强功能，专注于帮助用户简化数据中的&nbsp;AI 使用。</p><p>&nbsp;</p><p>AI Vector Search（AI向量搜索）&nbsp;是 Oracle Database 23ai的一项重要功能，用户可以借此根据概念内容（而不是特定的文字、像素或数据值）来搜索文档、图像和关系数据，同时用户可以使用自然语言界面查询私有业务数据，并帮助&nbsp;LLM 提供更准确和更相关的结果。</p><p>&nbsp;</p><p>为什么不是一个单独的向量数据库或在Database引擎之上再拓展一个向量引擎？李珈表示，这样做的最大好处就是可以把业务数据和向量数据整合在一起。比如，原来用大量的业务数据做AI应用需要把数据拷贝出去，AI Vector Search则可以用一个SQL直接查找业务数据和向量数据及其他数据类型的数据。</p><p>&nbsp;</p><p>在向量化过程中，原来的做法是调一个大模型或一个嵌入的算法，这对有GPU资源的用户比较友好，对于多数没有设备资源的用户，Oracle提出了新的SQL嵌入函数，可以把符合标准的嵌入模型放到Oracle里面，用Oracle数据库的引擎来帮用户做向量化，这样意味着即便没有AI方面的经验、只会SQL，也可以全盘搞定这个过程。</p><p>&nbsp;</p><p>此外，Oracle Database 23ai 可以在客户数据中心本地部署，也可以在云上部署。甲骨文还提供了Oracle Digital Assistant，这是一个嵌入在Oracle应用中的数据助手，可以帮助回答客户问题。</p><p>&nbsp;</p><p>“300多个新功能，对于23ai的这个版本来讲非常有战略意义，是真正的 Game Changer。” 李珈说道。</p><p>&nbsp;</p><p>甲骨文投入了大量的资金用于人才培养、科技研发和基础设施建设，以确保用户能够获得高质量的服务。Oracle Database 23ai的研发周期通常为五年一个大版本的更新，但在每个季度都会有季度性的版本发布。</p><p>&nbsp;</p><p>费用方面，吴承杨表示，甲骨文数据库的定价一直是以使用的CPU量来计算的，不会因为23ai的发布而改变定价规则。Oracle Database 23ai 中 AI 功能的使用不需要额外付费，因为它是作为一个新功能添加到现有版本中的。</p><p>&nbsp;</p><p>“生成式AI用得好不好，只有一个标准，就是可以给用户带来什么效果，用户是否得到了本质性上的改变。”吴承杨强调，AI的功能应该像用电一样简单易用，用户不需要成为AI科学家，只需懂得SQL即可。</p><p>&nbsp;</p><p>李珈介绍，甲骨文开始做Vector DB以来，金融、电信、制造业等用户也积极跟进，应用场景包括欺诈分析，各种各样的智能体、AI 助手，长视频检测等。</p><p>&nbsp;</p><p></p><h2>“数据库 AI 化是一个必然趋势”</h2><p></p><p>&nbsp;</p><p>“随着生成式AI的出现，融合数据库将变得更加重要。”吴承杨说道。</p><p>&nbsp;</p><p>融合数据库能够处理结构化数据、非结构化数据、图数据、JSON和空间数据等多种数据类型，因此能够简化应用和分析的生成与运行。</p><p>&nbsp;</p><p>企业在使用生成式AI ，大模型应用时，更希望将企业内部数据放在本地。用户可以使用任何一个国产或国外的模型，比如Llama-3，可以使用各种数据库来做RAG。没有融合数据库，这些事情也可以做到，但需要一个很强大的开发商和复杂的技术架构。。</p><p>&nbsp;</p><p>吴承杨认为，简化企业大模型应用的逻辑是“四个任何”：任何时候、任何地方、任何人、任何数据，都可以使用。把这四个加起来以后，就真正地解决了很多的问题。甚至用户自己内部的工程师、以前的DBA就可以解决。</p><p>&nbsp;</p><p>吴承杨提到，随着AI进入数据库，未来对DBA的要求将会有所不同。“对未来的DBA来说，像数据库的管理、打补丁等能力基本不太需要或者需要比较少，但是对整个架构、整个数据应用方面的要求，会比以前的要求高很多。虽然仍被称为DBA，但是要求不一样、工作范畴更大。”</p><p>&nbsp;</p><p>甲骨文还强调了其向量数据库产品完全继承了Oracle数据库的企业级特性，包括安全、稳定、可靠和可扩展性。Oracle Real Application Clusters (RAC) 保护实例，Active Data Guard (ADG) 保护容灾。甲骨文还提供了强大的备份功能，以及Exadata数据库一体机，以满足高性能需求。</p><p>&nbsp;</p><p>“Oracle AI 使我们的融合数据库上了一个非常大的台阶。”吴承杨说道。</p><p>&nbsp;</p><p>吴承杨表示，数据库的AI化是一个必然趋势。对于一个数据库先进性的衡量，对AI的支持是必选项。而对于甲骨文来说，现在只是迈出了第一步，未来的想象空间是非常大的。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>