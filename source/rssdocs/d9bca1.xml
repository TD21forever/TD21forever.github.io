<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/TjHG4EwAoLByAMi0VWXz</id>
            <title>聚焦 AI 和大数据，2023 全球 AI 前沿科技大会等你来打卡！</title>
            <link>https://www.infoq.cn/article/TjHG4EwAoLByAMi0VWXz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TjHG4EwAoLByAMi0VWXz</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 05:11:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 数据分析, AI 应用, 数字化转型
<br>
<br>
总结: 大模型正在以前所未有的速度掀起创新型变革，企业需要更大规模、更复杂的数据分析和 AI 应用来洞察市场、提升生产效率、优化运营管理。然而，企业在推进数据和 AI 基础设施的现代化过程中面临着挑战，如提升大规模数据处理能力、把握最新的 AI 和大数据分析实践与趋势、解决GPU稀缺和数据工程复杂等问题。尽管如此，一些先行者和实践者已经找到了最佳实践路径。在“2023全球AI前沿科技大会”上，将展示AI和大数据分析在不同行业的最新进展和趋势，为企业提供实践经验和趋势洞察。 </div>
                        <hr>
                    
                    <p>大模型正在以前所未有的速度掀起创新型变革，站在时代的交叉路口，企业若想更好地洞察市场、提升生产效率、优化运营管理，无疑需要更大规模、更复杂的数据分析和 AI 应用进行支撑。</p><p></p><p>当数据和 AI 已经成为企业核心竞争力的重要组成部分，推进数据和 AI 基础设施的现代化便是企业数字化转型的重要举措。但摆在企业面前的挑战却层出不穷：</p><p></p><p>如何才能提升大规模数据处理能力？如何准确把握 AI 和大数据分析的最新落地实践与前沿趋势？又该如何解决 GPU 稀缺、数据工程复杂以及资源未充分利用等挑战？</p><p></p><p>上述提到的种种挑战都严重阻碍了数据价值的释放，然而，在这样的背景下，依旧不乏一批先行者与实践者率先找到了最佳实践路径。在 12 月 9 日的“2023 全球 AI 前沿科技大会”上，将为你奉上 AI 和大数据分析在不同行业的最新进展、趋势及对未来的展望。</p><p></p><p>此次大会由 Alluxio 与北京大学计算机学院、中关村融创企业开放创新促进会、中关村创业大街联合举办。</p><p>大会将邀请中国科学院院士梅宏、Databricks 和 Anyscale 联合创始人兼执行主席 Ion Stoica、Alluxio 创始人兼首席执行官李浩源、美国卡内基梅隆大学计算机学院软件研究所助理教授方飞、面壁智能联合创始人、CEO 李大海、Alluxio 创始成员兼开源社区副总裁范斌以及科大讯飞北京研究院副院长李家琦等国内外知名学者、企业技术专家，他们将围绕“智算加速，建瓴未来”这一主题，带来最佳实践与趋势洞察。</p><p></p><p>不仅大咖云集，本场大会的主会场内容还涵盖了 6 大前沿主题 +1 场圆桌会议，从数据基础设施软件基座到大模型应用探索，从面向未来的建设展望到挖掘大模型的无限潜能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/550864d8bacfb28f6cfd714551aef00d.jpeg" /></p><p></p><p>此外，还单独设置大数据分析专场 +AI/ML 专场。在【AI/ML】会场中，来自知乎、Shopee、快手、MemVerge、PingCAP、Alluxio 的 6 位嘉宾，将围绕 AI 场景中 Alluxio 加速模型训练与模型上线的实战经验展开分享；在【大数据分析】会场中，来自联通、Uber、微软、B 站、携程、Alluxio 的 6 位嘉宾将围绕大数据时代背景下，多样化应用与探索，为同行业其他品牌带来诸多应用借鉴。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f405ef4156f658a1a46d6209abc71d7f.png" /></p><p></p><p>众多专家齐聚，前沿趋势与最佳落地实践结合，相信这场理论与实践兼备的大会，定会让你不虚此行！如果你对前沿 AI 和大数据分析技术感兴趣，并期待收获一次沉浸式的学习体验，欢迎大家参与“2023 全球 AI 前沿科技大会”。</p><p></p><p>报名通道现已开启，欢迎扫描下方二维码提前占位，12 月 9 日 09:00-17:30，我们在北京中关村国家自主创新示范区会议中心，与你不见不散！</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d9edfa411cd3a0823c4936862bea5e2.png" /></p><p></p><p>不仅议程安排足够丰富，到场的小伙伴还会获得由 Alluxio 准备的 5 重精美礼品，品类多达 20+ 种，欢迎到场赢取惊喜哦～<a href="https://www.infoq.cn/form/?id=1928&amp;utm_source=gzh&amp;sign=iq_655f138c0be1e">也可点击此链接，直接报名！</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/r14f5E9kJAxbaN7dMWdS</id>
            <title>楷同科技有限公司 CEO 黄益聪确认出席 QCon 上海，分享基于时间序列数据预测模型的智能量化交易系统性能优化实践</title>
            <link>https://www.infoq.cn/article/r14f5E9kJAxbaN7dMWdS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/r14f5E9kJAxbaN7dMWdS</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 时间序列数据预测模型, 智能量化交易系统, 性能优化实践
<br>
<br>
总结: 本文介绍了即将在上海举办的QCon全球软件开发大会，其中将有一位演讲嘉宾分享关于基于时间序列数据预测模型的智能量化交易系统性能优化实践的主题。演讲内容包括系统全链路的优化实践，如如何高效处理计算高频产生的时间序列数据、降低数据对Java GC的影响等。该演讲将对构建高性能、低延迟的智能量化交易系统以及多语言开发的复杂系统的全链路性能分析和优化等方面带来收益。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=8&amp;utm_term=1129&amp;utm_content=huangyicong">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。快手基础平台部系统软件中心 / 系统软件负责人熊刚将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5626?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=8&amp;utm_term=1129&amp;utm_content=huangyicong">基于时间序列数据预测模型的智能量化交易系统性能优化实践</a>"》主题分享，探讨系统全链路从数据采集 - 数据计算 - 模型预测 - 交易下单，全流程进行优化的实践，包括怎样高效的在 Java 处理计算 C++ 高频产生的时间序列数据，怎么降低高频产生、长生命周期数据对 Java GC 的影响等。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5626?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=8&amp;utm_term=1129&amp;utm_content=huangyicong">黄益聪</a>"，曾担任 Intel 高级工程师，阿里巴巴高级技术专家，中国互联网百强独角兽企业技术副总裁、CTO，有 15 项中国发明专利，3 项美国发明专利，专注于大数据与人工智能领域，AI 量化交易系统实践。他在本次会议的演讲内容如下：</p><p></p><p>演讲：基于时间序列数据预测模型的智能量化交易系统性能优化实践</p><p></p><p>金融市场的行情数据，如股票价格、成交量、交易队列等是典型的时间序列数据，具有很强的时间性和顺序依赖性。智能量化交易系统需要对市场上高频产生的时间序列数据进行处理计算，输入深度学习模型进行预测，执行交易策略，生成交易行为进行交易。整个过程需要覆盖全市场一万以上的品种，并且需要在很小的时间窗口，比如秒级完成。进一步的，我们使用了多语言进行系统开发。其中数据采集模块使用了 C++ 以达到高性能，交易策略引擎使用了 Java Spring Boot 搭建服务，AI 模型使用了 Python 基于 TensorFlow 和 Torch 框架。</p><p></p><p>业务需求的系统低延迟计算和多语言系统模块的交互，给我们的性能优化带来了挑战。这次分享，将带来我们对系统全链路从数据采集 - 数据计算 - 模型预测 - 交易下单，全流程进行优化的实践分享，包括怎样高效的在 Java 处理计算 C++ 高频产生的时间序列数据，怎么降低高频产生、长生命周期数据对 Java GC 的影响，怎么高效部署调用低延迟、多模型、多版本的 AI 模型预测服务，系统故障的数据断点快速恢复等。</p><p></p><p>演讲提纲：</p><p></p><p>背景与项目概况</p><p>○ 量化交易系统介绍</p><p>○ 项目技术架构</p><p>○ C++/ Java / Python 多语言交互</p><p>全链路数据流优化</p><p>○ 实时行情数据收集与处理</p><p>○ 低延迟、高吞吐性能挑战</p><p>○ 尝试的优化手段：GC Tuning，Direct Buffer</p><p>○ 我们的解决方案</p><p>○ 性能优化效果</p><p>服务化 AI 模型预测</p><p>○ Tensorflow 模型性能优化实践</p><p>○ Torch 模型转换</p><p>总结与展望</p><p>○ 复杂模型和低延迟预测性能的权衡</p><p>○ 目标展望：更快、更准</p><p></p><p>听众收益点：</p><p></p><p>○ 构建高性能、低延迟的智能量化交易系统</p><p>○ 多语言开发的复杂系统的全链路性能分析和优化</p><p>○ AI 模型在智能量化交易系统的实践</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 8 折优惠还剩最后 3 天，现在购票立减￥1360！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W5RzCzfZ6imJQrCk2lW9</id>
            <title>全方位深度测评 AI 代码助手 Amazon CodeWhisperer</title>
            <link>https://www.infoq.cn/article/W5RzCzfZ6imJQrCk2lW9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W5RzCzfZ6imJQrCk2lW9</guid>
            <pubDate></pubDate>
            <updated>Wed, 29 Nov 2023 02:40:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员, 代码助手, AI, Amazon CodeWhisperer
<br>
<br>
总结: 随着互联网技术的发展，程序员们面临着越来越多的挑战，传统的开发工具已经无法满足需求，因此基于人工智能技术的代码助手应运而生。Amazon CodeWhisperer是亚马逊推出的AI代码助手，旨在帮助程序员提高开发效率、减少错误、提高代码质量，并支持多种编程语言和开发环境。它通过实时生成代码建议、获取开源项目信息、扫描代码漏洞等特性，帮助程序员快速构建应用程序，提高团队协作效率。 </div>
                        <hr>
                    
                    <p></p><h3>背景</h3><p></p><p></p><p>随着互联网技术的不断发展，<a href="https://so.csdn.net/so/search?q=%E7%A8%8B%E5%BA%8F%E5%91%98&amp;spm=1001.2101.3001.7020">程序员</a>"们面临着越来越多的挑战，如代码复杂度不断提高、代码错误难以避免、团队协作效率低下等。传统的开发工具已经无法满足程序员们的需求，因此这几年基于人工智能技术的代码助手应运而生。AI代码助手的目的是通过自动化的方式帮助程序员提高开发效率、减少错误、提高代码质量，同时还可以帮助程序员快速学习新技术、更快，更安全地构建应用程序，提高团队协作效率。可以说AI代码助手成为当今软件开发领域的重要趋势之一。本篇文章就来深度测评一下AI 代码助手&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"。</p><p></p><h3><a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer 介绍</a>"</h3><p></p><p></p><p>Amazon CodeWhisperer&nbsp;是亚马逊云科技推出的AI代码助手，目的是帮助开发者更快，更安全地构建应用程序。作为智能编程助手，它经过了非常多的优秀开源代码训练，参与训练的代码都是具有良好的扩展性，安全性，优雅等特点，利用它编写的代码能够很快地写出健壮，优雅，具有很高扩展性的代码。 此外它还可以扫描代码来检测难以发现的漏洞，获取代码建议来立即修复漏洞。总的来说它具有以下特性：</p><p></p><h4>特性</h4><p></p><p>实时生成代码片段或全函数的代码建议获取相关开源项目的存储库信息扫描代码漏洞，给出修复建议支持 Python，Java，JavaScript 等15中编程语言支持 VS Code，IntelliJ IDEA，Amazon Cloud9、Amazon Lambda 控制台、JupyterLab 和 Amazon SageMaker Studio 等集成开发环境</p><p>以上就是&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;的介绍，下面进入正在的测评阶段。</p><p></p><p>主要从以下几方面进行测评：</p><p>用户体验 （包括，安装，配置，文档资料）功能使用（包括，上手难度，使用复杂度，安全，准确度）场景实践（以具体业务场景体验功能）</p><p>安装，配置也是测评的一部分，下面就从最开始的安装开始体验。</p><p></p><h3>安装配置&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"</h3><p></p><p></p><p>本次安装使用的在 Windows 10 的 VS Code 上进行的。作为 AI 智能代码助手，它是以 IDE 插件的方式存在的。这样能够很好地与 IDE 相关功能无缝结合。提升开发效率，增强用户体验。</p><p>打开 VS Code，在插件列表中搜索&nbsp;&nbsp;Amazon Toolkit</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6001aab889f12da44c240e236b393a0.png" /></p><p></p><p>找到名称为&nbsp;Amazon Toolkit&nbsp;的插件，点击 Install 按钮进行安装。有时安装后，需要重载一下 VS Code 才能用。</p><p></p><p>笔者写这篇文章时，Amazon Toolkit 最新的版本是&nbsp;1.91.0，如果有读者安装的与笔者的功能不同，请检查下版本是否一致。以下是该插件的一些基本信息：</p><p></p><p>版本：1.91.0</p><p>下载次数：1,663,326</p><p>Git仓库：<a href="https://github.com/aws/aws-toolkit-vscode">amazon-toolkit-vscode</a>"</p><p>插件地址：<a href="https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-toolkit-vscode">Amazon Toolkit</a>"</p><p>开源协议：<a href="https://marketplace.visualstudio.com/items/AmazonWebServices.aws-toolkit-vscode/license">Apache License Version 2.0</a>"</p><p>从下载次数来看，Amazon Toolkit 是一个非常受欢迎的插件。</p><p></p><p>在安装完成后，你可以在左侧的侧边栏，看到一个 Amazon 的图标，点击它就会出现插件的面板。如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf5282612c54c8bbfe19fc90a4330893.png" /></p><p></p><p>该插件主要包括三种功能：</p><p>Amazon CodeCatalyst 统一的软件开发服务，可在 Amazon 上快速构建和交付应用程序。CDK 云应用程序资源<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;智能代码助手。</p><p></p><p>要使用这些功能，需要用户先连接 Amazon 服务。</p><p>当我们需要使用&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"， 只需要点击 CodeWhisperer 下的 Star 按钮，然后再点击 Sign in 按钮，如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1eef9b4b08c3caf3531f5de0359ea4ef.png" /></p><p></p><p>如果你没有 Amazon 账号，也没关系，点击按钮后，会弹出一个重定向弹窗，点击 Proceed To Brower，使用浏览器继续。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dfbfdcf8bf635a46abd91a6d1c87101.png" /></p><p></p><p>点击按钮会 页面显示大致如下</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38a25646300243dc95df751500b13575.png" /></p><p></p><p>点击确认并继续</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/001bf944dc6ac743c9447bda273ba7d3.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af604608b158437bc40c52737e655a4b.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d1b5f33d305b5bcc8814fb78275e49d.png" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22751317effcc0fd3da7517310242f5b.png" /></p><p></p><p>总体步骤就是，输入邮箱，姓名 → 验证邮箱 → 填写密码 → 允许 Amazon Toolkit 访问数据</p><p>整体流程非常顺畅，安装，配置三分钟内就能完成。</p><p>授权后，插件就开始工作了。我们也可以开始愉快地工作了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/04/049a734c47975b7b61254a680a2ea7a7.png" /></p><p></p><p>此外值得一提的是，该插件还提供了一种专业版的功能，不过要配置 IAM 身份中心，这部分我们暂时不表。</p><p></p><p>在安装并配置&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;后，在编码时会自动开启代码建议。同时你也可以收到去获取代码建议。在 Windows 平台的 VS Code 上，使用 Alt + C 键，使用 Tab 键来插入当前的建议代码块。使用左右键来切换代码块。</p><p></p><h3>具体场景</h3><p></p><p></p><p>下面我们在具体的场景中来体验&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"。</p><p></p><h4>使用&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Code Whisperer</a>"&nbsp;开发数据可视化图表</h4><p></p><p></p><p>场景一：作为一名前端开发者，我们经常会遇到使用图表库开发一些可视化的图表，比如使用 Echarts 来开发一个折线图。</p><p></p><p>我们创建一个简单的 html，在页面内写入必要的信息，并在 script 标签中写入注释：</p><p></p><p><code lang="text"></code></p><p></p><p>然后按下 Alt + C 键，这时在 VS Code 会调出，html is currently not supported by CodeWhisperer。</p><p>如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2da98c8bac8acc7f71497714af82c2d.png" /></p><p></p><p>目前&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">CodeWhisperer</a>"&nbsp;还不支持 html 文件的代码建议。所以我们需要先创建一个 js，然后在 html 文件中引入。</p><p></p><p>我们在 js 文件中，使用注释写下需要实现的功能，然后按下 Alt + C 键。就会出现如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/716980467f988fd2dcdacba2d6a3d3bf.png" /></p><p></p><p>在检查过给出的建议代码后，确定是我们需要的，按下 Tab 键，来获取插入当前区域。</p><p>更加具体的交互可以先下面的动图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d22e5bdeb330a5005e81a78905ad46c3.gif" /></p><p></p><p>这是一个非常实用的场面，避免了花费大量时间去查询 Echarts 文档。要知道 Echarts 的配置文档是非常多的。下图是密密麻麻的 Echarts 图表配置项：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f009865a68020e1730d96b161fc79a89.png" /></p><p></p><p></p><h4>编写一个 Python 的浏览器自动化脚本</h4><p></p><p></p><p>作为一名开发人员，我们经常会遇到一些重复的工作，比如这样一个场景，在某个网站上有一个销售榜单，我们需要实时监控这个表单，并将每天的数据汇总发到邮箱里。对于这样的重复性没有技术含量的工作，我们通常使用脚本来编写自动化脚本。下面我们就使用&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;来编写一个这样的脚本，看看它是否能够帮助我们快速实现功能。</p><p></p><p>创建一个 auto-run.py 的文件，在文件里引入 selenium，并且使用代码注释写下要实现的功能，按下 Alt + C 键。交互动图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72dc7a512bd67b79380a853d44a65105.gif" /></p><p></p><p>根据动图大家可以看到，当按下 Alt + C 键时，只提供了一行代码建议，在按下左箭头键后，出现了四行的代码建议。整个流程是非常快速的。</p><p></p><p>给出的代码建议地完整地实现了， 使用 webdriver 打开 Chrome 浏览器，并且访问百度首页，但在输入关键词时，却把"拿我格子衫" 写成了“拿战校衫”。个人猜测是由于中文在大模型中有偏差造成的。换成英文就无此问题。</p><p></p><p>使用&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;时，代码建议是非常快速的，这个快，除了靠个人感觉来评估，也有一些更为准确的数字来评估。<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;提供了一个日志面板，我们可以在 VS Code 的 Setting 配置面板里，找到 Amazon Toolkit 的配置项，找到 Log Level，将其调整为 debug。如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f57ad175da4f88acfaa7ca349b5a8e18.png" /></p><p></p><p>调整后，我们选中 OUTPUT 面板，并将输出选位 Amazon Toolkit Logs，如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b2ea6250202ee1d78e1fe58cace1c75.png" /></p><p></p><p>当我们在编辑器中按下 Alt + C 键，底部的日志面板会打印出整个流程的日志：</p><p>打印信息大致如下</p><p></p><p><code lang="text">2023-09-25 11:36:41 [DEBUG]: command: running "aws.codeWhisperer"
2023-09-25 11:36:41 [DEBUG]: command: running "_aws.auth.autoConnect"
2023-09-25 11:36:41 [VERBOSE]: telemetry: emitted metric "vscode_executeCommand"
2023-09-25 11:36:41 [DEBUG]: codewhisperer: Connection expired = false,
                           secondaryAuth connection expired = false,
                           connection is undefined = false
2023-09-25 11:36:41 [DEBUG]: codewhisperer: isValidCodeWhispererConnection = true
2023-09-25 11:36:41 [VERBOSE]: telemetry: emitted metric "vscode_executeCommand"
2023-09-25 11:36:41 [DEBUG]: CodeWhisperer finished fetching crossfile context out of 0 files
2023-09-25 11:36:41 [DEBUG]: CodeWhispererSupplementalContext:
    isUtg: false,
    isProcessTimeout: false,
    contentsLength: 0,
    latency: 0.2452000007033348,

2023-09-25 11:36:41 [DEBUG]: SSO token cache: loaded key: 5fa44ff1-8f20-4ed5-89be-548baeb748aa
2023-09-25 11:36:42 [DEBUG]: Request ID: db72446b-5ee6-439f-af87-87800aa93d90,
                timestamp(epoch): 1695613002378,
                timezone: Asia/Shanghai,
                datetime: 9/25/2023, 11:36:42 AM,
                vscode version: '1.82.2',
                extension version: '1.91.0',
                filename: 'hello-selenium.py',
                left context of line:  '',
                line number: 2,
                character location: 0,
                latency: 1047.5229000002146 ms.
2023-09-25 11:36:42 [VERBOSE]: Recommendations:
2023-09-25 11:36:42 [VERBOSE]: [0]
driver = webdriver.Chrome()
2023-09-25 11:36:42 [VERBOSE]: telemetry: emitted metric "codewhisperer_serviceInvocation"
2023-09-25 11:36:42 [DEBUG]: SSO token cache: loaded key: 5fa44ff1-8f20-4ed5-89be-548baeb748aa
2023-09-25 11:36:42 [VERBOSE]: telemetry: emitted metric "codewhisperer_perceivedLatency"
2023-09-25 11:36:43 [DEBUG]: Request ID: b69b0f19-bf91-4fe3-b335-96268b567126,
                timestamp(epoch): 1695613003423,
                timezone: Asia/Shanghai,
                datetime: 9/25/2023, 11:36:43 AM,
                vscode version: '1.82.2',
                extension version: '1.91.0',
                filename: 'hello-selenium.py',
                left context of line:  '',
                line number: 2,
                character location: 0,
                latency: 1041.122000001371 ms.
2023-09-25 11:36:43 [VERBOSE]: Recommendations:
2023-09-25 11:36:43 [VERBOSE]: [0]
driver = webdriver.Chrome()
driver.get("http://www.baidu.com")
driver.find_element_by_id("kw").send_keys("拿战校衫")
driver.find_element_by_id("su").click()
2023-09-25 11:36:43 [VERBOSE]: [1]
driver = webdriver.Chrome()
driver.get("http://www.baidu.com")
driver.find_element_by_id("kw").send_keys("拿战校衣")
driver.find_element_by_id("su").click()</code></p><p></p><p>根据打印日志的信息，基本的流程大致是这样的：</p><p></p><p>时间戳：2023-09-25 11:36:41，日志以DEBUG级别开始，表示调试信息。命令执行：运行"aws.codeWhisperer"和"_aws.auth.autoConnect"两个命令。遥测数据：emitted metric “vscode_executeCommand”，表示执行了一个 VS Code 命令。检查&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">CodeWhisperer</a>"&nbsp;连接状态：isValidCodeWhispererConnection为true，连接有效。检查&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">CodeWhisperer</a>"&nbsp;获取 crossfile 上下文的结果：完成从一个文件中获取crossfile上下文。检查&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">CodeWhisperer</a>"&nbsp;补充上下文信息： isUtg 为 false，isProcessTimeout 为 false，contentsLength 为0，latency 为0.245秒。SSO令牌缓存：加载了SSO令牌缓存的键值对。请求ID、时间戳、时区、日期时间、VS Code 版本、扩展版本、文件名、行号、字符位置、延迟等信息被记录。推荐建议：[0]，建议使用 webdriver.Chrome() 来创建一个 Chrome 浏览器驱动对象。遥测数据：emitted metric “codewhisperer_serviceInvocation”，表示服务调用的度量数据。…</p><p></p><p>使用 token 发起的 Request，整个请求中包含了这些信息：</p><p></p><p><code lang="text">timestamp(epoch): 1695613003423,
timezone: Asia/Shanghai,
datetime: 9/25/2023, 11:36:43 AM,
vscode version: '1.82.2',
extension version: '1.91.0',
filename: 'hello-selenium.py',
left context of line:  '',
line number: 2,
character location: 0,
latency: 1041.122000001371 ms.</code></p><p></p><p>其中有一个指标是 latency，表明延迟，即从用户按下 Alt+ C 键，到代码块出现这段时间。可以看到生成4行代码只用了 1s 左右，非常的迅速。</p><p></p><p>通过上述的两个实战案例，相信大家已经了解&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;的常规使用。在使用时需要以下几点</p><p></p><h4>使用时注意点</h4><p></p><p></p><p>实现功能需要提供一些上下文，比如使用的库和功能注释最好使用英文，中文可能出现乱码或繁体使用左箭头键和右箭头键选择最合适的代码块html 和 yaml 文件暂时不支持</p><p></p><p>另外在使用的过程中，发现了一个不知是 VS Code 的问题还是插件的问题，就是在用鼠标切换代码建议时，当前索引没有改变，详见下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/5251601e3e9e3553667a3607f4d627bc.gif" /></p><p></p><p>根据上图 可以看到 切换代码建议，&nbsp;1/5&nbsp;一直都没有变， 用户无法感知当前显示的是第几个代码块。</p><p></p><h4>插件代码解析</h4><p></p><p></p><p>为了更加了解这个产品，我仔细阅读了该插件的源码，它的代码托管在 GitHub，主要功能代码存放在&nbsp;src/codewhisperer&nbsp;目录里。</p><p></p><p>Amazon CodeWhisperer 的插件入口在此处，<a href="https://github.com/aws/aws-toolkit-vscode/blob/master/src/codewhisperer/views/securityPanelViewProvider.ts">https://github.com/aws/aws-toolkit-vscode/blob/master/src/codewhisperer/views/securityPanelViewProvider.ts</a>"</p><p>这段代码是一个名为&nbsp;SecurityPanelViewProvider&nbsp;的类，它实现了&nbsp;vscode.WebviewViewProvider&nbsp;接口。这个类主要用于在 Visual Studio Code 中打开一个特定的文件并在安全扫描面板中显示代码扫描结果。</p><p></p><p>以下是该类的主要方法和功能：</p><p></p><p>makeUri(...args: Parameters): vscode.Uri：&nbsp;这个方法用于根据给定的路径和行号范围创建一个 URI，用于在 openEditorAtRange 方法中打开编辑器。</p><p>openEditorAtRange(path: string, startLine: number, endLine: number)：&nbsp;这个方法接受一个文件路径和开始、结束行号，然后在 VS Code 中打开该文件并在指定的行范围内高亮显示问题。</p><p>persistLines()：&nbsp;这个方法用于持久化处理过的行信息。</p><p>addLines(securityRecommendationCollection: AggregatedCodeScanIssue[], editor: vscode.TextEditor | undefined)：&nbsp;这个方法用于将扫描结果添加到安全面板中，并更新视图。</p><p>update()：&nbsp;这个方法用于更新视图，将处理好的HTML内容设置到 webview 中。</p><p>persistLine(panelSet: SecurityPanelSet, index: number)：&nbsp;这个方法用于持久化单个处理过的行信息。</p><p>addUnclickableWarningItem(item: SecurityPanelItem) 和 addUnclickableInfoItem(item: SecurityPanelItem)：&nbsp;这两个方法分别用于添加不可点击的警告项和信息项。</p><p>addClickableWarningItem(item: SecurityPanelItem) 和 addClickableInfoItem(item: SecurityPanelItem)：&nbsp;这两个方法分别用于添加可点击的警告项和信息项，它们会生成一个包含文件路径和行号范围的 URI，并将其设置为链接的 href 属性，以便用户可以点击查看文件并在 VS Code 中打开。</p><p></p><h4>学习资料与文档</h4><p></p><p></p><p>虽然 Amazon CodeWhisperer 使用起来非常简单，但官方还是提供了很多学习资料，覆盖各个阶段的学习者。</p><p>如果你想要获取更多有关它的资料 可以查阅官方文档&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/">https://aws.amazon.com/cn/codewhisperer/</a>"下面是几篇帮助你快速了解 Amazon CodeWhisperer 的视频教程。</p><p><a href="https://dev.amazoncloud.cn/video/videoDetail?id=6445fcdec9a819396b2fe24a">利用 VS Code 开始使用 Amazon CodeWhisperer</a>"</p><p><a href="https://dev.amazoncloud.cn/video/videoDetail?id=6445fa2413eafe780ecafaac">利用 Amazon CodeWhisperer 创建基于 Python 的事件驱动型 Serverless App</a>"</p><p><a href="https://dev.amazoncloud.cn/video/videoDetail?id=6445fb816afa68650f58e0df">利用 Amazon CodeWhisperer 创建基于 Java 的事件驱动型 Serverless App</a>"</p><p></p><h3>总结</h3><p></p><p></p><p>总的来讲，<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;是一款非常优秀的智能编程助手，它能够理解代码的功能和结构，并根据这些信息自动生成注释。这有助于提高代码的可读性和可维护性，同时也能帮助开发人员更好地理解他们正在编写的代码。</p><p></p><p>本文介绍了<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;的背景和特性，并测评了它在实际开发场景中的优秀表现。此外，也给出了一些&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;的教程视频。</p><p></p><p>总之，<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;是一种借助AI大模型创新性的工具，它有助于改善代码质量和软件开发效率，并帮助开发人员更快速，更安全地开发应用，大家快快用起来，也期待&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">Amazon CodeWhisperer</a>"&nbsp;能够更新更多的功能。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HSytRmj96YuaEGEoHYeh</id>
            <title>退运险业务及系统架构演进史</title>
            <link>https://www.infoq.cn/article/HSytRmj96YuaEGEoHYeh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HSytRmj96YuaEGEoHYeh</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:16:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 退运险, 系统架构, 初代架构, 中间件
<br>
<br>
总结: 本文回顾了退运险系统架构的演进过程，初代架构采用了较为保守的设计，通过中间件实现了水平扩展能力。然而，初代架构存在性能问题和资源调度困难，随着业务的发展，进行了升级引入美洲豹（JStorm）的改进。后续进行了完全重构，优化了系统的稳定性和性能，并进行了业务层的深度优化。最终，通过引入新的中间件和优化处理逻辑，实现了系统的高效运行。 </div>
                        <hr>
                    
                    <p>⽂章简介：本⽂回顾退运险上线⼗年以来相关系统架构的演进。</p><p></p><p>背景   </p><p></p><p><a href="https://xie.infoq.cn/article/131daa0c586cbd71cd10b1652?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">退运险</a>"对于我司来说是⼀个举⾜轻重⼀款产品，初创时期贡献了 99.9% 的保费收⼊，随着公司的发展各类产品的不断创新，退运险的⽐重在缩⼩，但 它的规模在不断扩⼤，当前年保单量已超百亿。为了满⾜业务的发展，相应的系统经历不断的演进。</p><p></p><p>初代架构 </p><p>  </p><p>核⼼系统于 2013 年 11 ⽉开始设计，当时不知道业务到底能有多⼤，所以起个名字叫⽆界⼭，由于对未来的不可知，初版微架都⽐较保守（初代⼤神⼤⻦⾔）。退运险作为其⽀持的最重要也是量最⼤的业务，很多设计都基于它，当时⽇需处理的保单达 380 万，可能现在看这个数据不算什么，但是要知道当时通常中⼩型保险公司核⼼系统的⽇均⽀持保单量在 10 万以内，较普遍是 1 ⾄ 3 万左右；⼤中型公司则约在⼏⼗万量级，不超过百万。处理⽅式也特 别简单：<a href="https://www.infoq.cn/article/2014/04/odps-implementation?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">ODPS </a>"预处理报⽂，再解析报⽂，扔出 notify 消息，核⼼收到后处理，通过分库分表⽅式解决⼤数据存储问题。各个系统在 hsf 等中间件的帮助下具备⽔平扩展能⼒。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc7962d8bd1b740884fa701999ab37e8.png" /></p><p></p><p>初代架构存在的问题是性能实在是蜗⽜，资源调度极度上不去，消息压⼒⼤，⼀旦有异常，堆积导致 notify 奔溃，业务规模也不断在扩⼤，特别是<a href="https://www.infoq.cn/article/z7SqcM2QpJ7Wvu43U9R6?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">双⼗⼀</a>"⼤促时尤为突出，⽇保单量已达亿级别，系统也渐渐显得⼒不从⼼，2015 年进⾏升级引⼊美洲豹（JStorm），将之前 dispatcher 和 nightelf 两个项⽬合⼆为⼀，架构上通过中间件解决⽔平扩容和单点问题，于当年年 11 ⽉份正式⽣产运⾏。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f92d40a33fe07de1c51f44dd3fe55d32.png" /></p><p></p><p>⼆代架构  </p><p> </p><p>初代架构迭代优化后基本能很好的⽀持退运险业务，但是公司在不断发展，后续健康险、意外险、财产险等业务不断的开展，核⼼系统也需要⽀持 越来越多的实时业务，对系统的稳定性要求越来越⾼，然⽽退运险⼊库时间⽐较集中，系统负载持续在⾼位，导致系统稳定性下降。要同时⽀持退运险业务和实时业务，虽然可以通过⽔平扩容的⽅式提⾼系统的吞吐量，但是⼀ 天就跑四五个⼩时跑报⽂，扩容会⼀定程度上造成资源浪费，⽽且在跑报⽂时间段内也不能避免不影响实时业务。在电商平台进⾏⼤促活动时尤为突出，在双 11 期间为了追求更⾼的 QPS，前期需要投⼊⼤量的⼈⼒和资源进⾏ 系统调优和压测，⽣产也需要准备⼤量的云资源进⾏⽀持。JStorm 是分布式 实时计算引擎，⽽在退运险场景⾥只⽤其任务分发的功能，有点杀鸡⽤⽜⼑。⽽且 JStorm ⽇常维护成本太⾼，使⽤极其不便。  </p><p> </p><p>在 2018 年进⾏⼀次完全的重构，针对退运险的业务基于⽆界⼭核⼼系统衍⽣出⽆界⼭-电商⼦系统，底层使⽤⽆界⼭ 1.0 的 DB，⽀持电商场景包括退运险、保证保质保险等创新业务。得益于公司 Devops 的建设，让这套系统测试、发布和扩容更⾼效。服务间的调⽤使⽤ dubbo，消息中间件使⽤ kafaka， bill 服务也通过消息进⾏异步处理。上线后当年双 11 单⽇⽀持 8 千万保单⼊库， 峰值 TPS 达到 2.2 万，期间未造成其它实时业务的超时等异常情况。   </p><p></p><p>完成基础架构调整后，于 2019 年进⾏业务层的深度优化。因为退运险作为公司第⼀批险种，在核⼼已经运⾏了五六年。在这段时间⾥，核⼼已经新上了数千个产品，对接了四五个事业部，为了兼容，原有的单独针对淘系报⽂的⼊库逻辑已经被改的⾯⽬全⾮。另外⼈员以及部⻔的变动，短时间内经历多次交接，⼀些特殊逻辑在过程中没交待清晰。以上这些问题，都给维护、测试和排错等⼯作带来诸多的不便。⽽随着众安与淘系的合作程度加深，淘系退运险保单量正在进⼀步增加，预计 2019 年双⼗⼀会达到顶峰。完成本次优化后，删减了冗余的代码逻辑，减少了 DB 查询次数，提升了系统的整体性能。上线后 2019 年双 11 单⽇处理保单 1.6 亿，处理⽤时⽤了 160 分钟， 峰值 TPS 达 2.7 万，报⽂报⽂同⽐去年增加⼀倍处理时⻓缩短 40%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d19965c6d7cfac7fc2ace5fc04c5710.png" /></p><p></p><p>主要改进内容：</p><p></p><p>开发⽆界⼭-电商⼦系统，与其它险种系统进⾏隔离，减少相互影响，bill 通过 kafaka 异步⼊库，进⼀步削峰；</p><p>使⽤ dubbo 中间件替代⽼旧的 HSF 进⾏服务间调⽤，提⾼系统性能；</p><p>使⽤新的报⽂处理平台和 kafaka 消息中间件替代 JStorm 进⾏异步处理；</p><p>优化 ODPS 预处理逻辑，调整报⽂结构，降低 ODPS 处理资源消耗；</p><p>产品配置、⽤户信息进⾏本地化缓存，减少重复 DB 查询，提升系统性能。</p><p></p><p>三代架构   </p><p></p><p>数字⽣活事业部⼀直在⾮阿退运险⽅向进⾏了不断地努⼒，期间接⼊过蘑菇街、趣店等电商平台，但都量不⼤⽇保单在万单级别，近年来随着抖 ⾳、快⼿等新兴平台电商业务蓬勃发展，使得退运险在⾮阿渠道得以突破， 2020 年 6 ⽉份快⼿接⼊，2021 年 7 ⽉份抖⾳接⼊后，⾮阿退运险⽇保单量逐渐超千万单。退运险不仅仅只有阿⾥单⼀渠道和场景，逐步向多元化的⽅向发 展。三代架构在这过程中也逐渐形成，公司⽆界⼭核⼼也升级为 2.0 不在和退 运险共⽤ DB，其它险种和退运险相互再⽆影响。根据当前需求新开发报⽂管 理平台，使⽤ OCeanBase 分区表替代原来分库分表，使⽤ Redis 的 Message queue 进⾏报⽂⽂件的分发处理。以及开发 gateway-data、anacientone 系统处理退运险实时业务。   </p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c0dccef56cf59d55f7a4c2495b57d7f.png" /></p><p></p><p>在降本增效的环境下，不再不计代价的追求⾼ QPS，⽽是要求以更经济的⽅式⽀持业务。为了达到这个⽬标，在优化系统的同时，也优化业务模式，⽐如淘系报⽂之前T +1 给前⼀天所有的报⽂，导致会出现⼀个报⽂有上亿条数据，处理数据时间过于集中，处理过程只能串⾏进⾏，导致系统资源没有充分利⽤。最终我们和渠道⼀起进⾏优化整改，以 T+H 的⽅式给报⽂增加给报⽂频次，给了更⻓的时间窗⼝处理数据，也⼤⼤减少单个报⽂数据量， 从⽽进⼀步进⾏了削峰，⽇常只需 400QPS 就能保障业务正常运转，以往需要 达到 4000QPS。⼤促期间以往需要⼏⼗台 ECS ⽀持，QPS 需要达到 2 万，现 在只需原来的⼗分⼀就能满⾜要求，以更少的系统资源⽀持业务，达到降本增效的⽬的。同时也提⾼保单的⼊库时效，提升了⽤户体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/db8d7ad1bd30150490256956de4a2de1.png" /></p><p></p><p>主要改进内容：</p><p></p><p>⽀持实时接⼊，其中快⼿以及⼀些⼩渠道通过实时⽅式接⼊；</p><p>⽀持配置个性化策略解析报⽂，可以配置不同频次解析报⽂；</p><p>⽀持异常检测、预警，以及⾃动修复；</p><p>剔除 ODPS 预处理报⽂，⼀来减少报⽂处理环节，⼤⼤降低⼤⽂件传输消 耗，⼆来直接减少了 ODPS 计算费⽤；</p><p>⽀持回传结果⽂件，解决对账问题；</p><p>其它险种 DB 隔离，不再相互影响。</p><p></p><p>总结</p><p>   </p><p>对⽐三代架构，其中的中间件可能⼤不相同，但是核⼼的思想基本是⼀致的，就是通过异步的⽅式进⾏削峰。⼤道⾄简⽀持年保单量百亿的系统也 不过如此。其实退运险虽然数据量庞⼤，但是它在电商的交易场景⾥⽤户感知不强，所以它对实时性的要求⽐较低，结合它的这⼀特性才有这样的系统 设计。另外是因为站在巨⼈的肩膀上，依托公司的各项基础设施，才能这么简单构建这类处理海量数据的分布式系统。   </p><p></p><p>我司成⽴⼗周年，退运险业务上线也正好⼗年，这⼗年业务规模不断地扩⼤，形态也有变化，系统经历了⼏次变⾰。也经历公司各个时期，从⼈⾁运维，到 Duang、boom、ship、最后到现在的 DevCube。基于公司⾃研的 DevOps 研发运维⼀体化解决⽅案，使我们的系统能敏捷迭代，全链路监控预警，以及快速修复⽣产故障等。   </p><p></p><p>退运险是互联⽹的产物，所以得益于互联⽹相应的技术和思想，才构建这套系统。正如布鲁克斯在《⼈⽉神话》中所说的没有银弹，没有⼀个系统 ⼀开始就能做到尽善尽美，会随着业务发展、技术进步不断的调整和优化。 回头看之前的技术选型会有问题，但在当时就是最合适的⽅案。  </p><p> </p><p>本⽂未对业务场景以及⼀些技术细节进⾏展开，对相关⽅⾯有兴趣的同学可以单独找我探讨。后续相关的负责⼈也会输出相关⽂章介绍。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xi08NWCDbFBuzzPAOpKC</id>
            <title>亚运之后，AI如何实现保障普通人的运动安全？</title>
            <link>https://www.infoq.cn/article/Xi08NWCDbFBuzzPAOpKC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xi08NWCDbFBuzzPAOpKC</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 07:00:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 杭州亚运会, 运动项目, 运动意外保险, AI
<br>
<br>
总结: 2023年的杭州亚运会带动了全民运动热潮，篮球、游泳、羽毛球、滑板等运动项目已经融入到普通人的日常生活中。然而，运动中的风险往往被忽视，因此运动意外保险起到了很好的对冲作用。AI通过建立庞大的数据库和案例库，预测运动风险并分析不同运动中不同身体部分受到损伤的概率，帮助大众防范运动风险。 </div>
                        <hr>
                    
                    <p>刚刚结束的 2023 年<a href="https://xie.infoq.cn/article/1116539fe8f3b74b9ba3a908e?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">杭州亚运会</a>"带动了一波全民运动热潮。481 个运动项目中，篮球、游泳、羽毛球、滑板等运动项目早已融入到普通人的日常生活中，这些运动不仅可以帮助人们增强身体素质，还可以提高心理健康水平，减轻压力和焦虑，因此深受大众欢迎。</p><p></p><p>不过，运动中的风险往往容易被大众忽略。比如常见的篮球运动引起的扭伤、滑雪造成的摔伤甚至严重点的猝死。</p><p></p><p>为了应对这种风险的发生，保险起到了非常好的对冲作用。市面上有很多的运动意外保险产品可供选择，今天重点给大家介绍下，<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbe9a058cc9e294e4ed69ac400e62b0d2f83a43f8bd99f7b9b63597db46a166b33542f97f945&amp;idx=2&amp;mid=2247489943&amp;scene=27&amp;sn=456af9d2753c4417ab04b2b1a95f922f&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">AI</a>" 是如何起到帮助大众起到防范运动风险的作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03f26e673e047d671887fbf9f58416ed.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/86/86fc7ebd770900e618c3a0d7cc79c206.webp" /></p><p></p><p>AI 预测运动风险</p><p></p><p>要做到让 AI 预测运动风险，第一步是建立庞大的数据库和案例库。</p><p></p><p>那里相当于 AI 的图书馆，存储着每项运动的受伤医疗数据以及预后康复数据，它们将被 AI 作为数据集进行算法训练。</p><p></p><p>每个人在运动过程中，由于关节、肌肉的运动，必定会存在损伤。AI 大模型落地实践的过程中，会通过多轮训练以及互联网上的公开医学数据，准确判断不同运动最容易导致的受伤部位和受伤情况。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/05a879113d107dc37b28254ad05126c9.webp" /></p><p>&nbsp;</p><p>比如篮球。打是日常生活中大家比较喜欢的一项运动，同时也是有着比较多运动风险的运动，脚扭伤、身体冲撞、眼睛被撞出血、手脚扭伤等情况经常发生，严重的话可能会导致不可逆转的后果，比如猝死。</p><p></p><p>再例如当下非常流行的一种年轻、潮流运动陆冲，是容易发生身体损伤的城市运动之一。深度学习完所有资料后，AI 会分析一个人在玩陆冲时，TA的膝盖、手臂、腰部、头部、皮肤会存在哪些风险。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48d119878c862a3628dbefc000be080c.webp" /></p><p></p><p>比如不协调的肢体会造成摔倒、擦伤，甚至骨折；180 度的动作虽然酷炫但也有风险，可能会导致头部冲撞、身体摔倒造成损伤、肌肉软组织受伤；长期过度下蹲可能造成膝盖损伤。</p><p></p><p>在判断<a href="https://www.infoq.cn/article/p-XvAYSY8mvfGrVsi6mg?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">运动风险</a>"的同时，AI 还会测算每种运动中不同身体部分受到损伤的概率，比如打篮球扭伤脚的概率比打网球高等。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pBhhJRbdkuaMzv3MjOeA</id>
            <title>印度国有银行因技术问题错汇82亿卢比；新加坡开发金融行业的生成式人工智能风险框架｜金融科技资讯</title>
            <link>https://www.infoq.cn/article/pBhhJRbdkuaMzv3MjOeA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pBhhJRbdkuaMzv3MjOeA</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 06:39:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 印度国有银行, 技术问题, 错汇, 82 亿卢比
<br>
<br>
总结: 印度国有银行合众银行因技术问题导致82亿卢比被误转入某些账户，目前已追回大部分款项。银行已报告执法部门并采取必要行动，停用相关技术。
<br>
<br>
关键词: 新加坡金管局, 生成式人工智能风险框架, 金融行业
<br>
<br>
总结: 新加坡金管局与银行和科技公司合作开发了一个金融领域的生成式人工智能风险框架，旨在让金融机构负责任地使用相关科技。框架涵盖七大方面，包括问责与治理、监督与稳定、透明度与解释性、公平与偏差、法律与监管、道德与影响，以及网络与数据安全。
<br>
<br>
关键词: 北京金融科技创新监管工具, 创新应用
<br>
<br>
总结: 北京金融科技创新监管工具实施工作组公示了一项基于隐私计算技术的他行资金流水核验服务创新应用，旨在规范开展银行间跨主体数据共享应用，实现数据可用不可见、数据不动价值动。
<br>
<br>
关键词: 银行数字化转型报告, 金融科技大会
<br>
<br>
总结: 首届FCon全球金融科技大会在上海举行，发布了《2023银行数字化转型报告》，重点探索银行机构在面临各种因素影响下如何推进数字化转型，并总结输出不同类型和规模银行业企业的两条数字化转型路径。
<br>
<br>
关键词: 中国金融科技生态白皮书, 金融科技产业大会
<br>
<br>
总结: 中国信息通信研究院主办的“2023金融科技产业大会”发布了《中国金融科技生态白皮书（2023年）》，分析了中国金融科技产业、技术、市场主体、应用场景创新等方面的进展，并展望了未来的发展。
<br>
<br>
关键词: 驻建行纪检监察组, 金融科技, 信息化建设
<br>
<br>
总结: 驻建行纪检监察组利用金融科技优势，开发了一系列智慧纪检监察系统，实现了全流程线上作业、大数据智慧监督、数字化综合管理等功能目标，推动了信息化建设。建设银行也为多个省份提供了智慧政务服务。 </div>
                        <hr>
                    
                    <p></p><h2>印度国有银行因技术问题，错汇 82 亿卢比</h2><p></p><p></p><p>据报道，印度国有银行合众银行（UCO Bank）近日向监管机构提交一份文件显示，在 11 月 10 日至 13 日，由于“即时支付服务”（IMPS）技术问题，高达 82 亿卢比（约合人民币 7 亿元）的款项被误转入该行某些帐户，银行发现后立刻冻结部分帐户，目前追回 64 亿 9000 万卢比，约占 79%。</p><p></p><p>该行目前已向执法部门报告，并将采取必要行动，追回剩余的钱，同时停用 IMPS。至于导致 IMPS 出错的缘故，合众银行并未说明，仅解释是内部技术问题，而不是平台问题。</p><p></p><h2>新加坡金管局开发生成式人工智能风险框架，用于金融行业</h2><p></p><p></p><p>新加坡金管局同银行和科技公司合作，开发了一个金融领域的“生成式人工智能风险框架”，让金融机构负责任地使用相关科技。</p><p></p><p>金管局公布的白皮书摘要显示，框架涵盖七大方面，包括：问责与治理、监督与稳定、透明度与解释性、公平与偏差、法律与监管、道德与影响，以及网络与数据安全。参与计划的业者也将探讨开发好的业界使用个案，包括运用生成式人工智能处理复杂的合规任务，以及找出相互关连的隐藏金融风险。</p><p></p><p>金管局透露，已经完成第一阶段的计划，并将于明年 1 月发表详细的白皮书，公布风险框架的详情。下一个阶段，把保险和资产管理领域纳入框架，并会探讨把生成式人工智能的使用范围扩大到反洗钱、可持续性和网络安全等方面。</p><p></p><p></p><h2>北京公示新一批金融科技创新监管工具创新应用</h2><p></p><p></p><p>为贯彻落实中国人民银行《金融科技发展规划（2022―2025 年）》（银发〔2021〕335 号文印发），推动北京金融科技稳定有序发展，日前，北京金融科技创新监管工具实施工作组（以下简称工作组）面向社会公示最新一批 1 个创新应用。</p><p></p><p>本次公示的创新应用“基于隐私计算技术的他行资金流水核验服务”聚焦对公贷款业务场景，旨在探索运用隐私计算技术，在各方原始数据不出域的基础上规范开展银行间跨主体数据共享应用，实现数据可用不可见、数据不动价值动。</p><p></p><p></p><h2>《2023 银行数字化转型报告》白皮书发布</h2><p></p><p></p><p>11 月 19 日，<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzQzNjQ5Mw==&amp;mid=2247486737&amp;idx=1&amp;sn=d143c11541cf3876451b079faf16cb7e&amp;chksm=c24dc033f53a49254962c75952605f82465c4898353c141a4481a4eca1fe8388ff506f5269e3&amp;scene=21#wechat_redirect">首届 FCon 全球金融科技大会</a>"在上海盛大开幕。会上，《2023 银行数字化转型报告——抓住机遇，建立差异化优势》正式发布。</p><p></p><p>本报告重点探索作为金融服务核心的银行机构，在面临用户消费模式变化、银行业务结构调整、新兴金融机构竞争加剧等因素的影响下，如何推进五大重点场景的数字化转型，并总结输出大中小型银行的两条数字化转型路径，期望为不同类型和规模的银行业企业提供研究内容支撑。（文末附下载）</p><p></p><p></p><h2>中国信通院发布《中国金融科技生态白皮书（2023 年）》</h2><p></p><p></p><p>11 月 17 日，由中国信息通信研究院（简称“中国信通院”）主办的“2023（第六届）金融科技产业大会”在北京举行。会上，中国信通院发布《中国金融科技生态白皮书（2023 年）》（下称白皮书）。</p><p></p><p>白皮书是中国信通院连续第六年针对金融科技领域的跟踪研究成果，聚焦过去一年来国内外金融科技领域新的发展情况，重点分析了中国金融科技产业、技术、市场主体、应用场景创新等方面的进展，并对金融科技产业生态未来发展进行了展望。</p><p></p><p></p><h2>驻建行纪检监察组：借力金融科技推动信息化建设</h2><p></p><p></p><p>中央纪委国家监委网站消息称，二十届中央纪委二次全会提出，要构建贯通全流程、全要素的数字纪检监察体系。</p><p></p><p>驻建行纪检监察组深入贯彻落实中央纪委二次全会精神，牢牢把握科技赋能监督执纪执法工作的目标定位，充分利用建设银行金融科技优势，开发上线纪检监察工作平台、员工行为管理平台、龙信党风廉政社区等一系列智慧纪检监察系统，实现了全流程线上作业、大数据智慧监督、数字化综合管理等多项功能目标，形成了上下贯通、穿透式督导的工作格局，初步探索出一条“纪检监察 + 金融科技”融合发展的创新道路。</p><p></p><p>与此同时，驻建行纪检监察组积极推动建设银行运用金融科技力量，协助地方纪委监委推进信息化建设，形成了一套可复制可推广的建设经验。</p><p></p><p>“建行各级党组织要善于运用科技力量和信息化手段，助力正风肃纪反腐。”建设银行党委书记、董事长田国立表示。近年来，建行已为 10 多个省份提供“互联网 + 政务”“互联网 + 监管”等智慧政务服务。</p><p></p><p></p><h5>报告推荐</h5><p></p><p>《2023 银行数字化转型报告》正式发布，本报告重点探索作为金融服务核心的银行机构，在面临用户消费模式变化、银行业务结构调整、新兴金融机构竞争加剧等因素的影响下，如何推进五大重点场景的数字化转型，并总结输出大中小型银行的两条数字化转型路径，期望为不同类型和规模的银行业企业提供研究内容支撑。关注「InfoQ 数字化经纬」公众号，回复「银行报告」免费获取。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2c/64/2c468684a87c935f1c8ab83cf111c164.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wJZO6pjWGYRTCfIvDrp2</id>
            <title>如何构建安全的App网络通信？</title>
            <link>https://www.infoq.cn/article/wJZO6pjWGYRTCfIvDrp2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wJZO6pjWGYRTCfIvDrp2</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 06:22:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据加解密, 对称加解密, 非对称加解密, 单向函数
<br>
<br>
总结: 文章介绍了数据加解密的概念和方法，对称加解密在本地存储中使用较多，而非对称加解密在网络传输中使用较多。文章还介绍了单向函数的概念和用途，以及非对称加密和数字证书的作用。 </div>
                        <hr>
                    
                    <p></p><h2>一、前言</h2><p></p><p></p><p>说到安全肯定逃不开数据的加解密，数据本地存储大多用对称加解密来实现，那网络传输数据的时候是不是也用对称加解密来实现？没错，常规网络通信时，大部分网络传输过程中基本也是用对称加解密来实现的，毕竟时间宝贵。如果使用非对称加密方式，需要花 N 多时间等数据加密后传输出去，或者解密出来。只是用对称加解密有个致命问题就是<a href="https://www.infoq.cn/article/Jrr56Ufm8h2jCkepI5qG?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">密钥 Key </a>"的交换，毕竟网络是一个不可靠加不安全的通信环境。这个时候就需要启用另外一种加解密方式——<a href="https://www.infoq.cn/article/Hu2V9AxusFhEAPjvCiTJ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">非对称加解密</a>"。</p><p></p><p>不过想要做出一个安全的加密通信信道不是简单的用个对称加密和非对称加密可以完成的。在说网络安全之前，我们先讲几个比较基础的概念。</p><p></p><p></p><h4>1.1 单向函数</h4><p></p><p></p><p>第一个要说的就是<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA%3D%3D&amp;chksm=bdbf0ce08ac885f6cde15d58df998ff6260cd0693e208af6fbb4db149604b18c5fab5d4cd3e5&amp;idx=1&amp;mid=2650993843&amp;scene=27&amp;sn=2c614ba3c0815ebac9a707629a88c021&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">“单向函数”</a>"。假设现在存在一个函数，函数式为 f(X)=Y。输入 X，很容易就能得出结果 Y。但是在知道结果 Y，想反向算出输入值X时非常困难（就是用现有最快的计算机来计算，也得计算个几百上千年才能得出结果的这种）。这种函数就叫单向函数。其中最简单的就是整数相乘。例如 3*7=21；知道 21，很容易就能猜出来，可能的结果有【1,21】、【3、7】这两组。但是将整数的位数提高到几百位，就很难反推出来。</p><p></p><p>由此也能很容易的看出来单向函数不能用于数据加密。因为经过单向函数加密的数据谁都不能解开，用它来加密数据没有任何意义。单向函数一般两个用途：1、生成信息摘要，或者数据指纹。比如说我们非常熟悉的 MD5、SHA1 算法就是具有单向函数性质的摘要算法。2、密码保护。一般用户登录时给服务器提交明文密码是非常危险的，非常容易被抓包。就算千幸万苦的提交到服务器端，服务器端保存不当照样会造成用户密码泄漏。此时就需要将用户密码经单向函数计算，然后将计算出的函数值存储在服务器端。用户登录时，客户端只需要提交用户密码计算出的函数值，服务器端用接收到的函数值和本地存储的值进行对比即可。</p><p></p><p></p><h4>1.2 单向陷门函数</h4><p></p><p></p><p>单向限门函数是一类特殊的单向函数。是一类存在“陷门”或者说“后门”的单向函数。还是上文提到的单向函数 f(x)=y，已知函数计算方法和输入值 x 时非常容易计算得到结果 y，但是反向不能计算。此时如果再提供一个参数、或者数据Z，就可以通过 y 反向计算出输入值 X。这个 Z 就被称为后门或者说陷门。例如下图的利用 RSA 算法对数据进行加解密的过程中，私钥就可以被看做是“后门”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b15223a8c2f031cf5aeac558750fe711.png" /></p><p></p><p></p><h4>1.3 前向加密安全</h4><p></p><p></p><p>这个概念说的是：在一个加密通信信道中，用来产生会话的长期加密密钥泄漏，不会造成之前通讯密钥的泄漏。比如说在这个加密信道中发送了 A、B、C、D、E 五次网络请求，在第五次 E 的时候密钥被破解，第五次请求 E 被破译为明文。此时以后的请求 G、H 很难保证安全，但是之前的 ABCD 四次网络请求仍然无法被破译，还是安全的。</p><p></p><p></p><h4>1.4 非对称加密</h4><p></p><p></p><p>非对称加密算法有两个密钥，因为加密解密用的不同的密钥，所以就叫非对称加密。两个密钥，一个公布出去叫公钥，一个自己保留叫私钥。用公钥加密的数据，只有对应的私钥可以解密。用私钥加密的数据，也只有对应的公钥才可以解密。下图就是一个典型的公钥加密，私钥解密的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b06ba9c77bf78adc37a79c15582d74d9.png" /></p><p></p><p>非对称加密算法有很多种，除了经常听说的 RSA，还有 D-H，ECC（椭圆曲线加密算法）等等算法。</p><p></p><p></p><h2>二、网络安全通信</h2><p></p><p></p><p></p><h4>2.1 安全通信第一版</h4><p></p><p></p><p>了解了非对称加密算法，我们很自然地就能设计出一个两端通信的安全加密方案。如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20dd76cf163f7e7cc55da08bd963ad4d.png" /></p><p></p><p>上述方案看似安全和完美，但是也存在一些问题和缺陷。比如性能差，会受到中间人攻击。</p><p></p><p></p><h4>2.2 安全通信升级版</h4><p></p><p></p><p>针对性能差，我们可以采用对称加密来解决。中间人攻击，这个时候就需要用到“数字证书”。数字证书的构成内容比较多，包含颁发者、使用者(持有者)、有效期、公钥等等。这里只说几个简单的知识点。</p><p></p><p>2.2.1 摘要</p><p>将使用者信息（这一栏一般是自身的网站域名和公司名等），自己生成的公钥和其他信息打包一起，经过 hash 算法计算，生成的 hash 值就叫做摘要。</p><p></p><p>2.2.2 签名</p><p>CA 证书机构用自己的私钥对摘要进行加密运算，生成加密后的密文，这个密文就被称为是数字签名。</p><p></p><p>2.2.3 证书</p><p>将 2.2.1 中的自身信息和摘要，2.2.2 步骤中的数字签名放在一起，就称之为数字证书。</p><p>下图就是一个签名生成数字证书和验证数字证书是否正确的流程图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/708b267c912e1ce441b9997b25602ba6.png" /></p><p></p><p>2.2.4 经典的 TLS 握手过程</p><p></p><p>有了数字证书，再加上对称加密算法，我们就可以构建出一个安全的加密通信信道。</p><p></p><p>1. 首先客户端生成一个随机数 k1，然后和服务器端打招呼并将随机数 K1 发送给服务器端。</p><p>2. 服务器端拿到 K1。自己再生成随机数 K2。并且将自身的证书和 K2 发送给客户端。</p><p>3. 客户端拿到服务器端的随机数 K2 和证书后，校验证书是否正确。如果校验成功，那继续进行握手。如果失败，会直接终止。</p><p>4. 客户端生成随机数 K3，使用从证书中得到的服务器端公钥对 K3 进行加密。</p><p>5. 服务器端接收数据后，用自己的私钥对其进行解密，从而得到随机数 K3。</p><p>6. 两端用三个相同的随机数 K1+K2+K3 合并生成一个对称加密算法的Key。后续就用这个 key 来加密两端通信的数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/de4735c94373b5820a5588c2891ae902.png" /></p><p></p><p></p><h2>三、网络通信安全在众安APP中的落地</h2><p></p><p></p><p></p><h4>3.1 我们面对的严苛问题</h4><p></p><p></p><p>看完上面安全知识点介绍和两种网络通信安全方案，我们就明白想构建一个安全的网络通信环境是多么的困难。尤其是我们做的金融 App，那相对于普通 APP 安全等级更是严苛。</p><p></p><p>首先，金融 APP 不能只是简单的走Https协议而不做证书绑定校验（不做证书检验就会导致”中间人“攻击）；</p><p>第二，App的网络请求需要配合服务器端做”防重放“处理，防止被人”重放攻击”；</p><p>第三，网络请求必须做签名处理，并且签名算法要保密，防止网络请求被篡改；</p><p>第四，网络请求数据必须做明文隐藏，防止网络请求被抓包的时候数据被攻击方直接获取；</p><p>第五，也是大家比较容易忽视的一点，就是多平台覆盖。很多App都做了网络请求安全防护，协议设计上大多只能兼容 ios和 android 两端，web 端就被忽视掉了；</p><p>第六，不能只顾安全不顾性能，App 的网络通信性能必须兼顾到。</p><p></p><h4>3.2 众安APP中的落地</h4><p></p><p></p><p>所以我们需要一个全面、健壮、能兼顾到上面几个安全问题的方案。</p><p></p><p>参考 TSL 协议，相当于在 Https 请求的基础上再加一道防线，我们设计了众安的网络请求安全协议。</p><p>现详细说明一下该方案中的核心四步：</p><p></p><p>• 第一步自然就是采用 https 协议，挂上证书启用证书校验。此外由于 SSL、TSL 各个版本的缺陷，我们在服务器端写死了 TSL 版本为 1.2+。即发起请求的客户端 TSL 版本必须是 1.2 或者以上版本，否则在 https 的握手阶段网络连接就会失败。这一步旨在于有效的防止“中间人”攻击。</p><p>• 第二步，在每个网络请求的 body 中都放入一个随机数，后台服务器会在内存中记录这个随机数，并且加一个有效时间。这一步可以有效的防止”重放攻击“。</p><p>• 第三步，生成一对 RSA 的公私钥对。公钥放客户端，私钥放服务器端。每个网络请求使用随机算法生成一个随机数 Key 作为 AES 算法的 key。用 AES 算法对网络请求的 body 数据整体进行加密。用公钥对 key 进行加密，加密后的值放到网络请求的 head 中。这一步是为了防止 body 明文泄密。</p><p>• 第四步，再次加固网络请求，给请求加签名——防篡改。使用 header 中的字段进行拼接，加入 time 这类随机数生成签名 sign。这一步可以有效的防止网络请求被抓包后篡改。只要我们的签名算法没有泄漏或者被破解，那攻击者就很难篡改我们的网络请求。</p><p></p><p>下图就是一个完整的网络请求客户端发送，服务器端接收的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7706d20e3f69021403f044f37404bad0.png" /></p><p></p><p></p><h3>四、其他的App网络通信方案介绍</h3><p></p><p></p><p>再简单介绍两种密码登录交互方案供大家参考：</p><p></p><h4>4.1 用户登录密码交互方案</h4><p></p><p></p><p>大部分 app 都存在使用账号密码登录的情况。在这个时候为了保护用户的明文密码不泄漏，很多 app 就会采用下面这种或者类似方案来设计用户登录交互。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a9f9ff9d1e576fe593072c65c22dfb5.png" /></p><p></p><p>1. 首先在客户端对用户的明文密码附加盐值然后用 hash 算法计算生成密码。（这里之所以附加盐值后再 hash 是为了防止用户密码被人用彩虹表直接破译出来）</p><p>2. 服务器端准备两张表，当用户注册的时候，一张表用来存储用户提交的 hash 值密码，一个用来存储给用户生成的 UUID 随机码。（这里不存储用户明文密码就是为了防止服务器被脱库后，用户明文密码泄漏）</p><p>3. 用户登录的时候服务器端收到用户发来的 hash 密码，这里称为 HashPW1，然后根据用户名查询到对应的 UUID。服务器在自己的数据库中根据用户名查询出 hash 密码，称为 HashPW2。接着查询到对应的 UUID。</p><p>4. 然后在内存中用（HashPW1+UUID）生成密码1，用（HashPW2+UUID）生成密码 2。两个密码相等即登录成功，不相等登录失败。（这里之所以在内存中生成真正的登录密码是为了防止将密码存储在数据库中，有权限的人可以直接查询到。经过这样复杂的操作有查询数据库权限的人只能看见两个 hash 值，不知道真正的密码如何生成。写代码的人知道密码生成算法，但是没有对应的随机值，都没有能力拿到用户的登录密码）</p><p></p><p>方案点评：</p><p></p><p>• 优点：该方案的最大优点在于隐藏客户的明文密码。整个系统的开发链上的几个关键角色：客户端开发、运维、后端开发都无法真正知晓客户的明文密码。在客户端，可以有效隐藏明文密码；在数据库中，存储的是客户的两个 Hash 值，运维无法掌握客户密码；在后端，虽然写后端代码的开发知晓真正的密码如何生成，但是却没有密码生成的两个关键hash值，仍然无法知晓客户的明文密码。</p><p>• 缺点：该方案缺点也是非常明显，客户端生成客户密码的 hash 值是固定的。如果被截取到该 hash 值就相当于被拦截了客户密码。必须在此基础上再加上网络通信安全策略，例如防抓包等策略，进一步加强防护。</p><p></p><p></p><h4>4.2 用户交易密码交互方案</h4><p></p><p></p><p>除了登录过程以外还有一种密码使用情况就是交易密码使用场景。理论上 6 位数字的交易密码也可以采用上面的交互方案，但是由于很多系统都对接了第三方系统，第三方系统只认用户的 6 位数字密码。也就是说我们还是需要将用户交易密码明文提交到服务器端。这时一般是这样操作的：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f71e700d09ee59b29d5fb8f2498b0d7d.png" /></p><p></p><p>1. 第一步客户端通过 API1 发起请求到服务器。服务器端会有一个令牌池，令牌池中存储 RSA 公私钥对和一个随机码。收到用户请求后从令牌池捞出一对 RSA 公私钥对和随机码，和用户的 token 或者 userId 做绑定。</p><p>2. 第二步将绑定后的公私钥对中的公钥和随机码发送给客户端</p><p>3. 第三步客户端收到随机码和公钥后，将 6 位交易密码拼接随机码，整体用公钥加密生成密文串。客户端再通过 API2 提交密文串到服务器端。</p><p>4. 第四步，服务器端接收到密文串后，根据用户 token 或者 userId 查询到对应的私钥，用私钥解密得到对应的随机码和 6 位数字密码。随机码校验 ok 后即可使用交易密码；如果校验失败才用户本次无法使用交易密码。</p><p></p><p>方案点评：</p><p>• 优点：算是比较完美的方案，可以用于生产环境</p><p>• 缺点：系统的后端开发可以拿到用户的明文密码，如果操作不当，有密码泄露的风险。</p><p></p><p></p><h2>总结</h2><p></p><p></p><p>对于一个网络 App 来说，在一个不安全的网络环境中构建一条安全、稳定的网络通信信道非常重要，尤其对于金融 App 来说更是重中之重。简单地使用 https 协议并不能解决各种安全漏洞和网络攻击。综上所述，众安在构建自己的网络通信安全信道时进行了全方位的考虑：</p><p>• 在使用 https 协议的基础上，进行二次加密安全。即使 https 协议被破解，攻击者也只能获取到一堆加密过的密文信息。</p><p>• 网络请求不仅实现了“防篡改”和“防重放”，还采用了“前向加密安全”。每个请求的加密密钥都是独立且动态生成的。即使某一条请求被抓包和破解，也不会影响其他请求的安全性。</p><p>• 考虑多平台，该安全协议适用于 iOS、Android 和 Web 端。</p><p>• 在设计通信安全的同时，也考虑了与性能的平衡。不仅注重安全性，还优化了通信的性能，确保用户在使用 ZA App 时能够享受到快速、高效的网络通信体验。</p><p></p><p></p><h2>附录</h2><p></p><p></p><p>D-H算法</p><p></p><p>D-H算法全称 Diffie-Hellman 算法。是一种密钥协商算法，用来在不安全的网络环境中协商出一个统一的加密密钥。但是该方法不能防止中间人攻击。</p><p></p><p>具体的密钥协商过程可以用下面这个交互过程简单理解一下。</p><p></p><p>• 首先A端和B端公开两个数字 P 和 Q，其中 P=5，Q=8。A 端和 B 端各自生成一个随机数 Ra=2、Rb=3。并且保证各自会保存好各自的随机码不泄漏。</p><p>• 接着A端采用公式 Ra*P  计算结果 X（X=10），B 端采用公式 Rb * Q 计算结果Y（Y=24）</p><p>• 双方互换 X 和 Y 值。这样 A 端拿到有 Ra，P，Q，Y。B 端持有 Rb，P，Q，X。</p><p>• A端采用公式 Ra * P * Y 计算密钥，结果等于 240。B 端采用公式 Rb * Q * X = 240。双方通过协商生成了统一的密钥 240。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/439b010cc32c3d3223d96ebc40587c6c.png" /></p><p></p><p>上面这个过程只是用乘法简单的模拟一下这个过程，具体的 D-H算 法要复杂的多，而且是采用离散对数来实现的。具体数学原理如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/58/586015780a107adf4669412de2535ccb.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jE2rzViqAgM8MJYAYWHV</id>
            <title>使用 CodeWhisperer 作为 AI 编码助手，重新构想软件开发</title>
            <link>https://www.infoq.cn/article/jE2rzViqAgM8MJYAYWHV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jE2rzViqAgM8MJYAYWHV</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 06:08:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Amazon CodeWhisperer, 生成式人工智能, 软件开发, 编码助手
<br>
<br>
总结: Amazon CodeWhisperer 是一款使用生成式人工智能的编码助手，可以帮助简化和精简软件开发过程。它能够理解代码的语义和上下文，并提供相关且有用的建议。CodeWhisperer 在不同层面上提供帮助，从小建议到编写完整的函数和单元测试，帮助将复杂的问题分解为更简单的任务。它还可以加快原型设计和入职培训，以及用于探索性数据分析。 </div>
                        <hr>
                    
                    <p>在&nbsp;<a href="https://aws.amazon.com/codewhisperer/?trk=a325eb01-2134-4258-a88b-ffc101a7b548&amp;sc_channel=sm?trk=cndc-detail">Amazon CodeWhisperer</a>"<a href="https://aws.amazon.com/blogs/aws/amazon-codewhisperer-free-for-individual-use-is-now-generally-available/?trk=cndc-detail">&nbsp;正式发布</a>"后，许多客户都已经使用它来简化和精简其软件开发方式。CodeWhisperer 使用由根基模型提供支持的生成式人工智能来理解代码的语义和上下文，并提供相关且有用的建议。它有助于更快、更安全地构建应用程序，并且可以在不同层面提供帮助，从小建议到编写完整的函数和单元测试，帮助将复杂的问题分解为更简单的任务。</p><p></p><p>想象一下，您想提高代码测试覆盖范围或为应用程序实施细粒度的授权模型。当您开始编写代码时，CodeWhisperer 将在后台运行。它可以理解您的注释和现有代码，提供从代码段到整个函数或类的实时建议。这种即时帮助会根据您的流程进行调整，减少了在搜索解决方案或语法提示时进行上下文切换的需要。在开发过程中，使用编码助手可以提高专注度和工作效率。</p><p></p><p>当您遇到不熟悉的 API 时，CodeWhisperer 可以为您提供相关的代码建议，从而加快您的工作速度。此外，CodeWhisperer 还具有全面的代码扫描功能，可以检测难以发现的漏洞并提供修复建议。这与<a href="https://owasp.org/?trk=cndc-detail">全球开放应用程序安全项目（OWASP）</a>"概述的最佳实践一致。这不仅使编码更高效、更安全，还让工作质量更有保证。</p><p></p><p>CodeWhisperer 还可以标记类似于开源训练数据的代码建议，并标记和删除可能被认为有偏见或不公平的有问题的代码。它为您提供相关开源项目的存储库 URL 和许可证，使您可以更轻松地查看它们并在必要时添加归因。</p><p></p><p>下面提供了几个 CodeWhisperer 实际应用的示例，这些示例涵盖了软件开发的不同领域，从原型设计和入门到数据分析和权限管理。</p><p></p><h3>CodeWhisperer 可加快原型设计和入职培训</h3><p></p><p></p><p>一个以有趣的方式使用 CodeWhisperer 的客户是&nbsp;<a href="https://www.buildstr.com/?trk=cndc-detail">BUILDSTR</a>"，这是一家提供专注于平台开发和现代化的云工程服务的咨询公司。他们在后端使用 Node.js 和 Python，在前端主要使用 React。</p><p></p><p>我与 BUILDSTR 的联合创始人 Kyle Hines 进行了交谈，他说：“在对不同客户不同类型的开发项目利用 CodeWhisperer 的过程中，我们已经看到了它对原型设计的巨大影响。例如，我们能够以极快的速度为与其他亚马逊云科技服务（例如&nbsp;<a href="https://aws.amazon.com/dynamodb/?trk=cndc-detail">Amazon DynamoDB</a>"）交互的&nbsp;<a href="https://aws.amazon.com/lambda/?trk=cndc-detail">Amazon Lambda</a>"&nbsp;函数创建模板，这给我们留下了深刻的印象”。 Kyle 说，他们现在在原型设计上花费的时间减少了 40%，而且他们注意到客户环境中存在的漏洞数量减少了 50% 以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f1532fa49fdada3b25b806f6a0443c5.png" /></p><p></p><p>Kyle 补充说：“由于招聘和培养新人才是咨询公司的一项长期工作，因此我们利用 CodeWhisperer 对新开发人员进行入职培训，它帮助 BUILDSTR Academy 将入职培训的时间和复杂性减少了 20％ 以上。”</p><p></p><h3>CodeWhisperer 用于探索性数据分析</h3><p></p><p></p><p><a href="https://aws.amazon.com/developer/community/heroes/wendy-wong/?trk=cndc-detail">Wendy Wong</a>"&nbsp;是一名业务绩效分析师，在&nbsp;<a href="https://www.service.nsw.gov.au/?trk=cndc-detail">Service NSW</a>"&nbsp;和 AI 敏捷项目中构建数据管道。由于她对社区的贡献，她还被评为了&nbsp;<a href="https://aws.amazon.com/developer/community/heroes/?community-heroes-all.sort-by=item.additionalFields.sortPosition&amp;community-heroes-all.sort-order=asc&amp;awsf.filter-hero-category=*all&amp;awsf.filter-location=*all&amp;awsf.filter-year=*all&amp;awsf.filter-activity=*all?trk=cndc-detail">亚马逊云科技数据大侠</a>"。她表示，当她使用统计和可视化工具分析数据集以获取其主要特征的摘要时，Amazon CodeWhisperer 显著加快了探索性数据分析过程。</p><p></p><p>她认为 CodeWhisperer 是一个快速、用户友好且可靠的编码助手，可以准确地推断出她编写的每一行代码的意图，并最终通过其最佳实践建议帮助提高代码质量。</p><p></p><p>“使用 CodeWhisperer，我不必记住每一个细节，因为它可以准确地自动完成我的代码和注释，大大简化了代码编译”，她分享道，“以前，我需要花 15 分钟来设置数据准备预处理任务，但现在只需要 5 分钟就能准备好”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/898159794cf95c2d71481c71c61c3e86.png" /></p><p></p><p>Wendy 说，通过将这些重复的任务委托给 CodeWhisperer，她的工作效率得到了提高，<a href="https://dev.to/aws-heroes/imagine-and-create-get-started-with-generative-ai-on-aws-part-1-40ba?trk=cndc-detail">她写了一系列文章</a>"来解释如何使用 CodeWhisperer 来简化探索性数据分析。</p><p></p><p>另一个用于浏览数据集的工具是 SQL。Wendy 正在研究 CodeWhisperer 如何为不是 SQL 专家的数据工程师助力。例如，她注意到他们只需要求其“写入多个联接”或“编写子查询”就可以快速获得要使用的正确语法。</p><p><img src="https://static001.geekbang.org/infoq/0a/0a022cbf9533d34d49d70e37caca0340.png" /></p><p></p><p></p><h3>CodeWhisperer 可加快测试和其他日常任务</h3><p></p><p></p><p>我有幸与<a href="https://aws.amazon.com/developer/?trk=cndc-detail">亚马逊云科技开发人员关系</a>"平台团队的软件工程师共事了一段时间。这个团队的工作内容包括构建和运营&nbsp;<a href="https://community.aws/?trk=cndc-detail">community.aws</a>"&nbsp;网站。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b04f65a95d377623266bef7a9dfe8550.png" /></p><p></p><p>Nikitha Tejpal 的工作主要围绕 TypeScript，CodeWhisperer 可以在她输入时提供有效的自动完成建议，从而帮助她完成编码过程。她说她特别喜欢 CodeWhisperer 帮助进行单元测试的方式。</p><p></p><p>“我现在可以专注于编写正面测试，然后使用注释让 CodeWhisperer 为相同的代码提供负面测试建议”，她说，“这可以将我编写单元测试所需的时间缩短 40%”。</p><p></p><p>她的同事 Carlos Aller Estévez 依靠 CodeWhisperer 的自动完成功能为他提供一两行代码建议，以补充他现有的代码，然后，他再自行判断是接受还是忽略这些代码。其他时候，他会主动利用 CodeWhisperer 的预测功能为他编写代码。“如果我明确想让 CodeWhisperer 为我编码，我会写一个方法签名，并在注释中说明我的需求，然后等待自动完成”，他解释说。</p><p></p><p>例如，当 Carlos 的目标是检查用户是否拥有给定路径或其任何父路径的权限时，CodeWhisperer 根据 Carlos 的方法签名和注释为部分问题提供了一个巧妙的解决方案。生成的代码会检查给定资源的父目录，然后创建所有可能的父路径的列表。然后，Carlos 对每条路径进行了简单的权限检查，以完成实施。</p><p></p><p>“CodeWhisperer 可以帮助我处理算法和实施细节方面的工作，这样我就有更多时间思考大局，例如业务需求，并创建更好的解决方案”，他补充说。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/327d3b328ca1f4ad5d99d3fbb631caf9.png" /></p><p></p><p></p><h3>CodeWhisperer 是一名多语言团队合作者</h3><p></p><p></p><p>CodeWhisperer 通晓多种语言，支持 15 种编程语言的代码生成：Python、Java、JavaScript、TypeScript、C#、Go、Rust、PHP、Ruby、Kotlin、C、C++、Shell 脚本、SQL 和 Scala。</p><p>CodeWhisperer 还是一名团队合作者。除了 Visual Studio (VS) Code 和 JetBrains 系列 IDE（包括 IntelliJ、PyCharm、GoLand、CLion、PhpStorm、RubyMine、Rider、WebStorm 和 DataGrip）之外，CodeWhisperer 还可用于&nbsp;<a href="https://jupyter.org/?trk=cndc-detail">JupyterLab</a>"、<a href="https://aws.amazon.com/cloud9/?trk=cndc-detail">Amazon Cloud9</a>"、<a href="https://aws.amazon.com/lambda/?trk=cndc-detail">Amazon Lambda</a>"&nbsp;控制台和&nbsp;<a href="https://aws.amazon.com/pm/sagemaker/?trk=cndc-detail">Amazon SageMaker Studio</a>"。</p><p></p><p>在亚马逊云科技，我们致力于通过投资开发新服务来满足客户的需求，帮助客户将负责任的人工智能从理论转变为实践，让他们能更轻松地识别和缓解偏见，提高可解释性，并帮助保持数据的私密性和安全性。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VcENQyixVov7c0CO18g7</id>
            <title>优雅！比OpenAI更认真的文本嵌入模型</title>
            <link>https://www.infoq.cn/article/VcENQyixVov7c0CO18g7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VcENQyixVov7c0CO18g7</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 02:26:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AIGC, Embedding, Word2vec, Transformer
<br>
<br>
总结: 本文介绍了Embedding技术在AIGC中的应用。Embedding技术作为助推AIGC的关键因素，经过十多年的发展，从最初的Word Embedding发展到Sentence/Paragraph Embedding，并扩展至结构化数据、图像处理、语音识别等多个方向。文章还介绍了Embedding技术在智能客服、金融风控和企微赋能等场景中的具体应用案例，并展示了其在FAQ检索、相似场景识别和用户精准服务等方面的效果。 </div>
                        <hr>
                    
                    <p></p><h2>一、前言</h2><p></p><p></p><p>伴随着 AIGC 浪潮的涌起，<a href="https://www.infoq.cn/article/bGviaVlkvVZO4I1tzMwe?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Embedding</a>" 技术作为助推 <a href="https://www.infoq.cn/article/e3zj2gU4mL7OWb8VwEGE?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AIGC</a>" 的关键因素逐渐为更多人所熟知。随着该技术的应用日益广泛，使用 Embedding 的人群也与日俱增。关于 Embedding 的最早提法可追溯至 2012 年 Google 的 Word2vec 论文。时至今日，Embedding 经历了超过十年的发展历程，从最初的 Word Embedding，发展到 Sentence/Paragraph Embedding，并扩展至结构化数据、图像处理、语音识别以及多模态等多个方向，以至于有“万物皆可 Embedding”的说法。模型的训练框架也由最初的浅层网络逐步发展为以 Transformer 为核心的深度学习网络。</p><p></p><p>数据科学应用中心很早便开始广泛使用 Embedding 技术，在众多项目中都可见其身影。本文将首先介绍数科 Embedding 技术的应用案例，展示了如何通过将 Embedding 技术与其他算法相融合，以提升排序效果。紧接着，详细介绍了模型效果的评价方法。最后，展示了与 OpenAI 的 Embedding 模型、开源 Embedding 模型 S-Bert 的效果对比。在众安 <a href="https://xie.infoq.cn/article/8c2eb6f6b9a002e82b3f2a916?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">FAQ</a>" 数据集和中文通用 FAQ 数据集上，众安 Embedding 模型各项指标都处于领先位置。</p><p></p><p></p><h2>二、案例介绍</h2><p></p><p></p><p>Embedding 技术可以用来进行相似度计算，如：文本、图像、语音等的相似度。因此，它在搜索业务中得到广泛应用，可以直接用于搜索，也可以作为整个搜索链路中的一部分。同时，还可以作为特征用于提升推荐、聚类、分类排序等各类算法的应用效果。</p><p></p><h4>2.1 概述</h4><p></p><p></p><p>目前，Embedding 被应用到智能客服、金融风控、企微赋能等多个项目中，为业务突破提供助力。</p><p></p><p>在智能客服场景中，FAQ 检索是非常重要的模块。用户的问题会被拿到知识库中检索，找到与之匹配的标准问题，然后将标准问题对应的答案返回给用户。在知识库检索中，我们用到了基于向量的检索（Embedding Based Retrieval, EBR），用户问题 Embedding 后，通过 EBR 召回部分问题。EBR 作为召回层的其中一路（图1），用于提升整个流程的召回准确率。在客服 FAQ 数据集上，Top1 标准问题召回率 97.6%， Top5 标准问题召回率 99.7%。在金融风控场景中，团伙报案连续性强、数量多、危害大，业务日均审批单流量非常大，人工方式从海量的历史图片中挖掘出数百张相似图片，难度非常大。数科算法部图像组应用基于向量的检索方法后，线上相似场景识别率接近 100%，在仅有 CPU 资源的条件下，实现了数百毫秒的服务耗时。在企微赋能场景中，坐席通常需要同时服务非常多的用户，选出高意向度用户精准服务就非常的关键。通过 Embedding + Attention (注意力机制)，用户的属性信息及多个时间段会话文本被精准的融合在一起计算用户的投保意向度。相关模型上线后，7日内转化率提升 80%, 人均保费提升 10%。</p><p></p><h4>2.2 案例详解</h4><p></p><p></p><p>我们挑选了 FAQ 检索的案例给大家做更为详细的介绍，描述如何将 Embedding 技术与其他算法结合提升 FAQ 的排序效果。但在这之前，需要先明确算法里面速度和准确率的权衡和基于向量的检索相关知识，这对不熟悉的同学理解我们的架构非常重要。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8fdbd00a0e8e8469510cc111a4a646ce.webp" /></p><p></p><p>图1. FAQ检索算法架构</p><p></p><h5>2.2.1 速度和准确性的权衡</h5><p></p><p></p><p>机器学习算法领域，在其他条件一致的情况下，准确性高的模型通常计算复杂度也会更高。很多时候，因为生产环境的硬件水平限制(无 GPU 或者应用在边缘设备上)，通常会选择准确性略低但速度满足应用场景的模型。一般在谈论算法准确性的时候，需要考虑到计算复杂度。（注：前提是同一测试数据集，同一模型在不同数据集上表现差异很大，主要原因就是不同数据集数据分布和难度不同，脱离数据集谈准确性毫无意义）</p><p></p><p>比如，在实时语音通话过程中用户的意图计算，需要先将语音通过ASR算法转化为文本，再通过意图识别模型完成预测，整个流程需要在非常短的时间内完成。应用大模型确实可以将模型准确率推高一点（如：准确率Accuracy-93.6% vs 95.3%），提高得很少，但是秒级以上的推理时间会严重影响用户体验，而数科算法团队研发的模型在CPU环境中仅需几十毫秒、几毫秒甚至更少的时间，就可以取得 93.6% 的准确率。</p><p></p><p>特别说明，这里的大模型并非 ChatGPT 这样的大语言模型，在意图识别场景，ChatGPT 模型要做意图识别，通常的做法是给出意图定义及示范例，通过上下文学习(In-Context Learning)的方式进行预测，这种方法不仅准确率相对参数量少很多的监督学习的模型低（相关评价指标Accuracy、F-score），而且推理速度也要慢很多。如果对大语言模型参数设置不熟悉，使用默认的参数配制，还容易出现多次预测结果不一致的情况。</p><p></p><p>下面要介绍的 FAQ 检索中，直接将复杂度更高精排模型应用于用户问题和所有知识库问题相似度计算中会提升 FAQ 匹配的相关指标，但是需要大量的计算资源和高企的推理时间为代价，尤其在 QPS 大的时候。因此，通常的做法就是召回+排序的策略, 召回层通过快速的相似度计算方法，召回有限数量的样本, 然后用精度高排序算法排序，通常各个网站和应用的搜索引擎也遵循这样的策略。</p><p></p><h5>2.2.2 基于向量的检索EBR</h5><p></p><p></p><p>基于向量的检索 EBR 其实就是通过算法模型 Encoder 将 Query 转化成向量，然后计算 Query 向量与知识库中预先计算好的文本、图片等对应向量之间的距离。最后，根据距离排序，检索出相应的结果。距离计算可以是余弦相似度，向量内积、欧几里得距离等，具体选择哪一种距离取决于模型 Encoder 的训练策略。</p><p></p><p>需要注意的是，在计算 Query 向量和知识库向量距离的时候，如果知识库内容数量特别大，推荐选择近似近邻(Approximate Nearest Neighbor, ANN)方法而不是K近邻（K-Nearest Neighbor, KNN），其主要原因也是速度和准确率的权衡，KNN 的计算量过大。并且通过调整 ANN 方法特定参数可以使得相关指标非常接近 KNN, 比如其中的 HNSW 算法，可以通过调整 nlist、nprobe 等参数来提升检索的准确率。</p><p></p><h5>2.2.3 FAQ检索</h5><p></p><p></p><p>图1展示了 FAQ 检索的架构图，基于向量的检索 EBR 是被用作召回层的一路来提升整体的召回效果（评价指标 recall@k，后面我们会详细讲解该指标），因为仅用 Embedding 并不能满足 FAQ 排序指标的要求。具体算法流程如下：</p><p></p><p>1.用户问题理解</p><p></p><p>主要包括：问题 Embedding、意图识别、问题纠错和关键词识别等。其中 Embedding 模型是针对众安 FAQ 检索场景开发的专有模型，具有非常好的召回和排序效果。</p><p></p><p>2. 知识库问题召回</p><p></p><p>算法模型一般具有偏向性，多路召回有助于提升召回样本的多样性，改进最终的排序效果，该案例采用了两路召回的策略：</p><p></p><p>向量检索 EBR：使用了 FAISS 工具，同时预先将知识库 FAQ 转化成向量。在调用时，根据当前用户 Embedding 与知识库向量之间的欧几里得距离进行排序。本案例会根据不同的知识库问题数量采用不同的检索策略。当知识库问题数量很大时，用HNSW算法进行近邻检索，相对于其他的 ANN 方法，该算法在保障召回率的同时，计算速度上有一定优势，通常的向量检索工具中都有该算法。当知识库数据量较小时，则会采用 KNN。关键词加权的召回：ElasticSearch 基于词的检索使用的是 tf-idf、BM25 等算法，这些算法在关键词的权重的计算方面并不准确，依赖知识库中的数据分布。因此，本案例采用 DeepCT+BM25 的方法，用 DeepCT 算法来精确计算词的权重，然后根据权重调整 BM25 中 query 的输入形式，提升了召回率。简单来说，就是利用 DeepCT 算法提升了 ElasticSearch 的搜索效果。</p><p></p><p>3. 排序</p><p></p><p>如果直接对召回的知识库问题进行精排，计算量会非常大，因此采用粗排+精排的策略：</p><p></p><p>粗排：采用相对召回层略复杂准确率更高的 Poly-Bert 算法，从召回的结果中选出 Top20 的知识库问题。精排：计算文本相似度方面，Google 在相关论文中从理论上证明了相对于 Embedding 距离计算方法，交互式文本相似度计算具有更高的准确性，当然也具有更高的算法复杂度。不同于 Embedding 方法，在分别计算两个问题的向量后再通过两个向量的距离比较相似度，交互式相似度计算从算法模型最底层就开始层层相互比较，因此选择了基于文本对交互计算的模型 Keywords-Bert。这个模型不仅从 transformer 框架的最底层开始交互比较，而且还在最后一层还加入了两个问题关键词之间的比较，通过各种细节的比较提升相似度计算的准确性。最终，该算法挑出 Top5 问题。</p><p></p><p>4. 策略层</p><p></p><p>这一层主要是通过规则及语言模型（Language Model，LM）来判断用户问题和答案之间的关联性，从而选出最合适的答案来回答用户问题。</p><p></p><p>在 FAQ 检索中，Embedding 被用作召回功能，通过快速的 Embedding 模型召回及排序模型的精准选择，实现速度和精度的平衡。虽然在这里 Embedding 模型只负责召回，但是召回k个知识库问题中包含的相关问题数量同样影响到最终的排序效果。如果 Embedding 模型召回精度不高，那么为了保障最终的排序效果就需要增加 k 的数量，这会带来后续计算耗时的提升，因此需要训练高质量的 Embedding 模型来保证召回效果。</p><p></p><p></p><h2>三、Embedding模型检索效果</h2><p></p><p></p><p>Embedding 模型的效果如何，不同的场景有不同的指标，相同的模型在不同的场景也会有不同的表现。由于目前更多的是用在检索场景，因此这里仅分析模型在检索场景下的表现。</p><p></p><h4>3.1 评估指标</h4><p></p><p></p><p>在比较 Embedding 模型的效果之前，需要先了解检索效果的评估方法，检索效果评估主要指标有：召回率(Recall) 、精确度(Precision)、MAP(Mean Average Precision)、MRR(Mean Reciprocal Rank) 、nDCG(Normalized Discounted Cumulative Gain)等，这里主要讲 Recall 和 MRR 这两个指标。</p><p>召回率(Recall) 是基于向量的检索模型召回效果的常用评估指标，它表示在检索到top-k问题中相关问题的数量与所有相关问题数量的比值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa9f85582546d0ea0a7e2798e0d69e66.webp" /></p><p></p><p>其中,k 表示召回问题的数量，k 越大对应的召回率越高，当 k 等于知识库问题量的时候，recall 必然等于100%。因此，只有在 k 值有限且相同的情况下，recall 比较才有意义，k 越小 recall@k 值越高模型效果越好。</p><p></p><p>表1. Recall计算示例</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97d45bdf9f4a28c27f9b8db65c73c049.webp" /></p><p></p><p>表 1 中，检索显示 top3 的结果，因此 k=3，可以计算得出该测试集 recall@3 = (1/2 + 1/1) / 2 = 3/4 = 0.75</p><p></p><p>然而 recall@k 这个指标中并没有考虑排序因素，比如:&nbsp;表 1 中“枸杞,菊花,红枣一起泡茶喝有什么好处”，无论其排在第 1 还是第 3 都不会影响其 recall@k 的值，而排第 1 的排序效果明显要好于排第 3 的，所以在一些关注 top-k 顺序的场景，recall@k 就不适用了。</p><p></p><p>MRR(Mean Reciprocal Rank) 就是考虑了 top-k 顺序的指标，RR(Reciprocal Rank)是请求 Q 检索响应的前k个结果中第一个正确响应的排序位置的倒数，如果第一个正确相应排序位置是 1，那么 RR=1/1; 如果排序位置是 2，则 RR=1/2; 如果是 3，则 RR=1/3，以此类推。如果前 k 个结果中没有正确相应，则 RR=0，排名越靠前 RR 就越大。MRR 是所有请求测试样本的 RR 平均值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39e6104f6d14d02b6701205808b46f81.webp" /></p><p></p><p>表2. MRR计算示例</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dff73239407d3f4a213e3cf00fd4ac6.webp" /></p><p></p><p>在表 2 中，检索显示 top 3的结果，因此 k=3，可以计算出 MRR@3 = (1/2 + 1/3)/2 = 5/12 ≈ 0.417</p><p></p><h4>3.2 模型效果</h4><p></p><p></p><p>这里主要展示了数科自研 Embedding模型（众安Embedding）、OpenAI Embedding 模型和SentenceBert(S-Bert) 中文 Embedding 模型在众安及通用中文 FAQ 上的效果比较（表 3）, 三个模型的详情如下：</p><p></p><p>众安 Embedding 模型 - 参数量:&nbsp;0.2 亿，输出向量长度:&nbsp;128/256OpenAI Embedding 模型 – 参数量:&nbsp;6 亿，输出向量长度:&nbsp;1536SentenceBert 中文模型 -&nbsp;参数量：4.8 亿， 输出向量长度：768</p><p></p><p>在这三个模型中 OpenAI Embedding 的参数量和输出向量长度最大的，其次是 SentenceBert, 众安Embedding 模型最小。因此，众安 Embedding 模型在将 query 转化成向量时是最快的，同时由于输出向量长度仅 128 或 256，在应用 ANN 或者 KNN 方法进行检索时的耗时也最短。</p><p></p><p>表 3. 测试数据集</p><p></p><p></p><p>表 4. Embedding 模型 Recall 指标比较</p><p></p><p><img src="https://static001.geekbang.org/infoq/ba/ba4e1bf1d1dd4b68c1408d0ee2e5ca68.webp" /></p><p></p><p>注：1-recall@k的1表示所有的相关问题数量是1</p><p></p><p>表 5. Embedding 模型 MRR 指标比较</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2796db9f52bf86d074264f4b8737f95.webp" /></p><p></p><p>注：当 k=1 的时候，因为不存在排序问题，1-Recall@1 和 MRR@1 相等</p><p></p><p>表 4 和表 5 展示了三个模型的召回排序性能比较结果。在众安 FAQ 数据集上，众安 Embedding 模型不论是 Recall 还是 MRR 指标均明显优其他两个模型，其中 1-Recall@1 为 0.976 比 OpenAI 的 0.828 高 0.148，MRR@10 为 0.983 比 OpenAI 的 0.887 高 0.096，无论是召回能力还是排序性都有很大优势。这表明特定领域的专有模型表现要明显好于通用模型，即使专有模型的参数量要小很多。而在中文通用 FAQ 数据集上，众安 Embedding 模型依然表现最好，但优势已没有那么大。一方面因为 OpenAI 和 S-Bert 在中文通用 FAQ 数据集上表现都不错，1-Recall@1 达到 0.92+，可提升空间相对较小；另外一方面是众安 Embedding 模型参数量仅 0.2 亿，远小于其他两个模型，如果采用差不多参数量的模型，其效果还会有所提升。</p><p></p><p></p><h2>四、总结</h2><p></p><p></p><p>本文展示了 Embedding 应用的案例、模型评估方法及众安 Embedding 模型的效果。Embedding 既可以直接用来检索，也可以与其他的模型相结合做出更高精度的模型。在实践中应用高质量的 Embedding 是一项复杂且具有挑战的工作，涉及到数据、深度学习建模、生产系统构建、端到端优化等多方面，每个方面都有大量细致性的工作，我们会持续提升 Embedding 系统，为业务加增量提供助力。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AF5E7b2wNVwK3oNZvyZo</id>
            <title>美图的这100天：三月三版本，大模型博弈中谁能笑到最后？</title>
            <link>https://www.infoq.cn/article/AF5E7b2wNVwK3oNZvyZo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AF5E7b2wNVwK3oNZvyZo</guid>
            <pubDate></pubDate>
            <updated>Tue, 28 Nov 2023 01:35:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美图公司, 视觉大模型, MiracleVision, AI技术
<br>
<br>
总结: 美图公司发布了自研AI视觉大模型MiracleVision 3.0版本，该模型是基于扩散模型理论的文生图模型，具有数十亿的参数规模。美图公司在技术方向上的升级是自然而然的事情，他们在多年的AI算法研发中积累了深厚的底层技术积累。通过收集高质量的数据并进行筛选，美图团队使用自研的模型架构进行训练，不断优化模型的效果。他们还与设计师紧密合作，建立了一套美学评估体系，以确保生成图像的质量和美观度。 </div>
                        <hr>
                    
                    <p>“大家没日没夜地在视觉大模型上投入，我们也真金白银花了很多钱。”美图公司创始人、董事长兼首席执行官吴欣鸿在提到新发布的视觉大模型时说道。</p><p>&nbsp;</p><p>10月9日，美图发布了自研AI视觉大模型MiracleVision 3.0版本。实际上，在引入大型模型之前，美图已经将很多AI技术应用到美图秀秀、美颜相机等产品中，比如图像识别、图像处理和图像生成等等。</p><p>&nbsp;</p><p>至今已有十多年历史的美图影像研究院（MT&nbsp;Lab）深耕深度学习，如今也开始将重点转向大模型研究。在美图公司技术副总裁兼美图影像研究院负责人刘洛麒看来，团队在技术方向上的升级是自然而然的事情。</p><p>&nbsp;</p><p>“大模型的数据量级和模型规模相比之前确实要更大些，需要进行大量的开发和对比实验，这是必然的。但在多年的AI算法研发中，我们已经积累了深厚的底层技术积累，这为大模型的研发提供了坚实的技术基础。”刘洛麒说道，“我们在北京、深圳和厦门都设有研发中心，吸引了国内外众多高校和研究机构的顶尖人才。我们还持续跟踪AI行业的前沿发展和学术研究工作，保证自身的视野始终位于行业的最前端，。”</p><p>&nbsp;</p><p>那么，美图的视觉大模型具体经历了哪些开发和打磨？未来又将如何利用大模型实现商业变现？</p><p></p><h3>“量变引发质变”的大模型</h3><p></p><p>&nbsp;</p><p>MiracleVision是基于扩散模型理论的文生图模型，目前是数十亿的参数规模。核心部分有两个：一是将文本转化为潜在编码，以控制扩散模型生成过程的文本编码模块。二是采用扩散模型的生成模块，还有一些附加模块，例如超分辨率模块，用于在生成后对图像进行放大并增强细节。</p><p>&nbsp;</p><p>美图的技术团队要先收集高质量的数据并进行筛选。通常，团队会用自动化算法对训练数据进行预处理，包括增强图像的清晰度和画质、调整色调、裁剪等，然后使用自研的模型架构进行训练，最后进行效果调整。</p><p>&nbsp;</p><p>MiracleVision演进经历了三个关键阶段。</p><p>&nbsp;</p><p>在1.0版本期间，美图着重构建了基础架构和模型美学体系，为后续效果的引入奠定了基础。</p><p>&nbsp;</p><p>对美图的技术团队来说，一开始技术体系的搭建显然是个很大的挑战，特别是在处理大规模数据方面。团队提出了很多标准和设想，并进行了大量实验。</p><p>&nbsp;</p><p>“因为之前缺乏这方面的经验，我们需要思考如何扩展数据规模来支持上亿级别的训练，同时保持高效的GPU利用率，确保效果可控。我们进行了一系列实验和探索，最终确定了整个流程。”刘洛麒说道。</p><p>&nbsp;</p><p>在2.0版本中，更注重引入高质量数据进行模型训练，主要目标是提高美观度、细节多元化，并增强文生图的准确性，以此适应更多场景。</p><p>&nbsp;</p><p>技术发展更像是量变引发质变，而不是在一个特定的节点突然改变一切。经过大量的数据积累，团队在内测2.0版本时，大模型生成的图像开始展现出创造力，展现出整体效果。</p><p>&nbsp;</p><p>“这让我们非常振奋，因为它超出了我们的预期。”美图公司副总裁、设计中心总经理许俊说道，“当时，我们都沉浸到了这个大模型的效果中，感觉不再是工作，而更像是在创作自己的东西。这种体验非常独特。”</p><p>&nbsp;</p><p>如今，MiracleVision已经进入3.0版本，团队集中精力来提升模型的可控性，以便用户能够更精确地进行细节控制和局部编辑，同时引入更多与工作流结合的数据增强，特别是在垂直领域方面。</p><p>&nbsp;</p><p>这里的可控性包括三个方面，一是通过中文语义描述的精准理解来达到想要的效果，二是可以对生成图像进行局部修改以及在修改的区域生成新的图像内容；三是提高分辨率，可以清晰呈现微小的细节，例如发丝细节。这种可控性的提升需要的是综合技术能力，既有算法优化，也需要设计师的经验和审美来帮助微调。</p><p>&nbsp;</p><p>除了通用领域的可控性，如何做好垂直领域的效果精致度也是一个难点。美图内部花了很多精力在不同垂直领域效果上进行各种调试，针对每个领域制定不同的训练、生成和调试方式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81c616dd5d871c360030964ec8a85f78.png" /></p><p>&nbsp;</p><p>从1.0到3.0的效果对比</p><p>&nbsp;</p><p>算力方面，美图提供基础算力支持，包括华为云等公有云以及自建的私有云部署，使得美图能够使用高性能的GPU卡进行不同规模的深度学习训练。</p><p>&nbsp;</p><p>在此基础上，美图设立了专门的团队，采取了一系列措施来提高大模型训练效率和资源的利用率，包括持续优化分布式训练框架，构建大规模AI高性能网络通信；建立健全的数据监控系统，用于监测每个GPU的使用率、功率消耗以及数据传输速度等，以此确保GPU的整体利用率保持高效。</p><p>&nbsp;</p><p>目前发布的MiracleVision主要是做图片生成，后续团队还将其扩展到视频生成领域。视频生成对连续性的要求更高，即生成的画面帧需要保持连贯，并且清晰度和质量要更高。</p><p>&nbsp;</p><p></p><h3>“设计师+研发”的化学反应</h3><p></p><p>&nbsp;</p><p>美图技术生态系统都与大模型相关，相关工程师有数百人，包括参与核心大模型训练和部署的研发，和基于大型模型构建具体应用场景的研发，如AI模特和AI动漫等方向的工程师。</p><p>&nbsp;</p><p>“美图的技术团队相当庞大，其独特之处在于，我们深刻理解业务和用户产品，而不仅仅专注于研究技术本身。”刘洛麒说道，团队将用户产品、设计师效果与技术有机结合，从用户用户反馈中反推技术的演进和提升。</p><p>&nbsp;</p><p>刘洛麒提到的“设计师效果”，就是为美图为解决视觉大模型研发的另一个挑战：对美学的理解，而设计的一个重要环节。</p><p>&nbsp;</p><p>“之前国内的一些图像大模型开发可能比我们开始得更早，但为什么他们的结果不太理想呢？我们将大模型训练比喻成一个小孩学习绘画，如果小孩开始学画画时看到的都是美好的事物，那就避免了想象出不美好的东西。”许俊表示。</p><p>&nbsp;</p><p>美，不像数学是一件有标准答案的事情，但生成图像需要标准。</p><p>&nbsp;</p><p>因此，美图设计师和外部艺术家早期花了很长的时间共同建立了一套美学评估体系。这套美学评估体系涵盖了数十个维度，每个维度设置了相应的得分，这些得分综合起来形成最终的美学分数。团队以每个维度的得分作为模型训练标准。</p><p>&nbsp;</p><p>这套评估体系贯穿了美图的整个大模型生命周期，包括前期数据筛选标准和模型效果调整标准等。</p><p>&nbsp;</p><p>实际上，这种设计师深入参与研发的方式，是美图一直采取的研发方法，不只是大模型领域。</p><p>&nbsp;</p><p></p><h3>大模型给谁用？</h3><p></p><p>&nbsp;</p><p>做出大模型只是开始，怎么让大模型产生收益是每个公司都要考虑的问题。</p><p>&nbsp;</p><p>经过调研，美图公司集团高级副总裁、影像与设计产品事业群总裁陈剑毅发现，大模型赛道对于创业公司来说很不友好，最后可能只会剩下比较成熟或者中大型公司。这其中的一个关键点就是要回答“模型给谁用”的问题。</p><p>&nbsp;</p><p>陈剑毅表示，给别人用的前提是要有一个应用层作为辅助和支撑。创业公司做出一个大模型，如果没有应用层，就得自己做然后花钱推广，结果也不一定理想。</p><p>&nbsp;</p><p>那么，美图的视觉大模型要给谁用？</p><p>&nbsp;</p><p>首先是要给自己用，现在基本上美图的大部分产品都逐渐融入了MiracleVision。其次，通过美图产品，个人用户也间接用上了MiracleVision。而通过用户的反馈，美图团队会进行针对性训练，以最快的速度调整效果，与用户应用场景结合。这种直接to C带来的闭环也是美图优势所在。</p><p>&nbsp;</p><p>但这只是大模型在现有产品体系的应用，还不够。如何让大模型产生降本增效的能力是美图关注的重点，美图的目标是做AI原生工作流。</p><p>&nbsp;</p><p>所谓原生工作流就是跳出传统工作流，利用AI能力更高效、高质量地完成创作。美图首先瞄向了电商、广告、游戏、动漫、影视这五大互联网性较强、长尾效应也高的行业。比如电商诞生于互联网，其中的一些小商家对效果的要求相对容易满足，美图可以为这类用户提供服务。美图希望可以在上述五大行业中实现落地，并获得一些重要的客户和行业订单。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38ea43f555b5a517ef6143401f40d935.png" /></p><p>&nbsp;</p><p>实际上，这种思路意味着美图需要分别去了解不同行业的需求，背后的工作量是不小的。为此，美图针对不同行业设置了不同的团队，负责了解具体行业的需求，并构建对应的产品。在产品落地的过程中，需求也会给到研发团队，业务团队与研发团队共同研发。美图影像研究院的研发也是业务驱动的，会优先考虑业务需求，然后利用最新的技术来解决用户的问题。</p><p>&nbsp;</p><p>不过，在与不同行业合作时，美图也会面临一些问题。</p><p>&nbsp;</p><p>比如，最初提供的一些概念性想法和图像设计，虽然效果可能很酷炫，但由于涉及到一些特殊的材料或技术，在实际生产中却难以应用。为此，团队需要更深入了解行业的切实需求和解决方案的可行性，以脱离提供抽象概念的误区。</p><p>&nbsp;</p><p>“与行业专业人士的合作非常重要，因为他们可以提供反馈，告诉我们真正的痛点是什么。通过交流，我们可以更好地调整产品和大模型，以满足他们的需求，解决他们的问题。这种合作是实现大模型在行业落地的关键。”许俊说道。</p><p>&nbsp;</p><p>产品设计过程中，美图团队的重点在于确保用户不需要经历冗长的学习曲线，无需过多的介绍和解释就可以使用。用户只需将创意以提示词的方式表达，然后交给大模型来实现想要的效果。如果有需要，用户还可以通过一些高级选项来微调或控制大模型的输出。</p><p>&nbsp;</p><p>目前，不同行业的用户对这种变化的接受速度不同。电商和游戏行业是两个转型较快的行业。在电商行业，很多时候需要与摄影师安排时间线下拍摄模特照片或商品图像，流程效率低下且成本较高。因此，电商用户更容易接受AI工作流程，因为电商本身是在线平台，具有更快的操作速度，可以显著降低成本。</p><p>&nbsp;</p><p>游戏行业类似。以前游戏制作通常需要从零开始绘制粗糙的原始图像，与最终效果相差甚远。现在，一些游戏制作公司使用AIGC工具来绘制更精致的效果图，甚至在最终产品的渲染过程中应用，这样绘图成本可以大大降低。</p><p>&nbsp;</p><p>不过，让行业里的所有人都丢掉之前工作方式、全面拥抱大模型还有些挑战。根据美图团队经验，现在离大模型最近的是一群“传播者”，即新媒体运营、电商运营和网红，他们没有太多之前的经验包袱，能够很快适应新的、更简单的工作流程。</p><p>&nbsp;</p><p>“AI原生工作流并不代表AI工具会取代他们的地位，因为他们仍然需要提供大量的创意和想法。AI工具只是能更快地实现他们的构想。”许俊强调。</p><p>&nbsp;</p><p>目前，美图团队正在视觉大模型基础上，围绕特定垂直领域对大模型进行针对性训练，让垂直领域性能达到极致。</p><p>&nbsp;</p><p>在变现方面，美图的用户付费模式具有多种变体。用户可以选择按月订阅，也可以选择单独购买特定功能。此外，广告等各种方式也可以用来产生收入，以弥补大模型的成本。&nbsp;</p><p></p><h3>结束语</h3><p></p><p>&nbsp;</p><p>就像普通用户会用美图秀秀做短视频封面或者简单的带货图片，大模型的使用并不需要专业技能加持，更低的门槛意味着更多的用户参与。</p><p>&nbsp;</p><p>对于美图来说，大模型已然是其必争之地。就像吴欣鸿曾说的，随着视觉大模型和生产端的磨合，垂直领域的极致效果、工作流整合和变现能力，这三个问题会被逐步解决。美图能否趟平这条道路，我们拭目以待。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/89Vodpjo3b9dG0buGpyI</id>
            <title>OpenAI“宫斗”导火索找到了！神秘“Q*”项目曝光，有可能威胁人类？</title>
            <link>https://www.infoq.cn/article/89Vodpjo3b9dG0buGpyI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/89Vodpjo3b9dG0buGpyI</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 07:05:26 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, Q*, Q-Learning, MRPPS
<br>
<br>
总结: 这篇文章介绍了OpenAI内部的争议以及引发争议的Q*概念。Q*可能代表着AI未来发展的一条可能路径，它可以是Q-Learning的一种高级形式，也可以是MRPPS中的一种算法。Q-Learning是一种强化学习方法，通过反复试验来掌握决策能力，而Q*算法则更强调如何提高AI的演绎能力。无论是哪种解释，Q*都可能对AI的自主学习与适应能力以及演绎推理和问题解决能力产生广泛的影响。 </div>
                        <hr>
                    
                    <p></p><blockquote>号称引发 OpenAI“内讧”的 Q* 与 Q-Learning 究竟是什么？</blockquote><p></p><p>&nbsp;</p><p>最近一周，全球科技界都在关注 OpenAI“宫斗大戏”，随着 CEO Sam Altman 和总裁兼联合创始人 Greg Brockman 正式回归，这场大戏似乎终于落下了帷幕。但对于“宫斗”导火索，外界一直众说纷纭。</p><p>&nbsp;</p><p>日前，有消息称，引发 OpenAI 内讧的根源是其一项神秘的重大突破——Q*。</p><p>&nbsp;</p><p>据路透社报道，一位消息人士表示，OpenAI 公司 CTO Mira Murati 曾亲口证实，Q*（读作Q Star）才是针对 Altman 采取逼宫行动的源动力，而且连董事会主席 Greg Brockman 也被排除在外，导致其随后用辞职向 OpenAI 表达了抗议。</p><p>&nbsp;</p><p>Q* 到底是什么？又有什么值得关注？答案很简单：它可能代表着 AI 未来发展的一条可能路径。</p><p></p><h2>Q-Learning 与 Q* 算法</h2><p></p><p>&nbsp;</p><p>据悉，Q* 指向两种不同的理论：其一代表 Q-Learning，其二则是马里兰反演证明过程系统（MRPPS）中提出的 Q* 算法。要想理解 Q* 的潜在影响，首先要明确这二者之间有何差异。</p><p></p><h4>理论一: Q-Learning</h4><p></p><p>&nbsp;</p><p>Q-Learning 属于强化学习的一种，指 AI 通过反复试验来掌握决策能力。在 Q-Learning 当中，智能体通过估计动作-状态组合间的“质量”来学习如何做出决策。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/476b1d7035f5969bd37bc29556844eb2.webp" /></p><p></p><p>这种方法与当前 OpenAI 的技术（即人类反馈强化学习，简称 RLHF）的最大区别，在于前者并不依赖于人类交互，而能够自行完成所有操作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8fcfcf1a2c2e5db7b30a8647315cad22.webp" /></p><p>&nbsp;</p><p>想象一台机器人正在迷宫中行走。通过 Q-Learning，它将学会尝试不同的路线以找到通往出口的最快路径。当它接近出口时，就能获得由自己预先设定的正奖励，而遇到死胡同时则获得负奖励。随着时间推移和反复试验，机器人就会制定出一种策略（即 Q-table），包含它在迷宫中各个位置上的下一步最佳行动。整个过程完全自主，单纯依赖于机器人同实际环境间的交互。</p><p>&nbsp;</p><p>而如果机器人使用 RLHF，那么当它到达每个路口时，都可能由人类介入干预、评判机器人的选择是否明智，而非由智能体自行发现问题。</p><p>&nbsp;</p><p>这种反馈可以是直接命令（左转）、建议（优先选择光照更充足的路径）或者对机器人选择的评价（选得对，或者选错了）等多种形式。</p><p>&nbsp;</p><p>在 Q-Learning 当中，Q* 代表着期望状态。在该状态下，智能体确切知晓每种状态下所应采取的最佳行动，并能随时间推移最大化其总体预期奖励。用数学术语来说，就是满足贝尔曼方程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9eac19862072204904d7ab0d4322e837.webp" /></p><p></p><p>早在今年 5 月，OpenAI 就曾发表一篇文章，称他们“训练出一套模型，不同于简单奖励正确的最终答案，该模型可以奖励每个正确的推理步骤，从而在解决数学问题方面表现出极高的水平。”如果他们确实是使用 Q-Learning 或者类似的方法实现了这个目标，则意味着 ChatGPT 将能解决各种以往难以应对的复杂问题和任务。</p><p></p><h4>理论二：来自 MRPPS 的 Q* 算法</h4><p></p><p>&nbsp;</p><p>Q* 算法是马里兰反演证明过程系统（MRPPS）中的一部分。这是 AI 领域一种复杂的定理证明方法，主要应用在问答系统当中。</p><p>&nbsp;</p><p>相关研究论文写道，“Q* 算法在搜索空间中生成节点，使用语义和句法信息来指导搜索。语义允许终止当前路径，并探索其他更可能通往成功的路径。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af90a3f640ebed41298a943c98a3c6e0.webp" /></p><p></p><p>解释此过程的一种方法，就是设想一位虚拟版的福尔摩斯打算解决一个复杂的案件。他需要收集线索（语义信息）并将其串连成逻辑（句法信息）以得出结论。Q* 算法在 AI 领域的作用也差不多，就是结合语义和句法信息来勾勒出复杂问题的解决过程。</p><p>&nbsp;</p><p>如果走的是这个路子，就代表 OpenAI 距离用 AI 模型理解现实又向前迈进了一步。换言之，在现有的文本提示之外，OpenAI 已经越来越像《钢铁侠》中的贾维斯或者《蝙蝠侠》中的蝙蝠计算机。</p><p>&nbsp;</p><p>总结来讲，Q-Learning 是指 AI 从与所处环境的交互中学习，而 Q* 算法则更多强调如何提高 AI 的演绎能力。理解了这些区别，我们才有机会进一步讨论 OpenAI&nbsp;Q* 成果的潜在影响。二者在推动 AI 发展方面都有着巨大的潜力，但应用思路和实际效果却又大相径庭。</p><p>&nbsp;</p><p>当然，所有这些还都只是猜测，因为 OpenAI 官方并没有出面解释这个概念，甚至没有证实或否认 Q* 的存在。</p><p></p><h2>Q* 将带来哪些影响？</h2><p></p><p>&nbsp;</p><p>传闻中的 OpenAI&nbsp;Q* 可能会引发广泛且多样的影响。如果它真是 Q-learning 的某种高级形式，也许意味着 AI 将在复杂环境下获得飞跃性的自主学习与适应能力，从而解决一系列全新问题。迷一进步将大大增强 AI 根据不断变化的条件做出瞬间决策的能力，从而将自动驾驶汽车等技术推向新的高度。</p><p>&nbsp;</p><p>而另一方面，如果 Q* 代表的是 MRPPS 中的 Q* 算法，则可能标志着 AI 的演绎推理和问题解决能力迈上了新的台阶。这主要作用于需要深入分析思维的领域，例如法律分析、复杂的数据解释乃至医学诊断等。</p><p>&nbsp;</p><p>无论正确答案如何，Q* 可能都代表着 AI 发展史上的又一重大进步，也符合 OpenAI 内部爆发的这场关于技术意义的激烈冲突。它将让我们更直观、更高效、更准确地处理以往需要高水平专业人才才能解决的现实问题。而且伴随这些进步，人们对于 AI 伦理、安全性、以及日益强大的AI力量对于人类日常生活乃至整个社会的影响也开始产生新的疑问和担忧。</p><p>&nbsp;</p><p>Q* 的潜在优点:</p><p>&nbsp;</p><p>更快、更好地解决问题：如果 Q* 属于 Q-learning 或者 Q* 算法的高级形式，则有望让 AI 系统获得更强大的复杂问题解决能力，从而推动医疗保健、金融及环境管理等行业的进一步发展。更好的人机协作能力：拥有更先进的学习或演绎能力的 AI 将有望增强人类工作，从而在研究、创新和日常任务中提高协作效率。自动化迎来新高峰：Q* 有望建立起更加复杂、精妙的自动化技术，提高生产力水平，并创造出新的行业与就业机会。</p><p>&nbsp;</p><p>Q* 的风险和担忧：</p><p>&nbsp;</p><p>道德和安全问题：随着 AI 系统变得愈发先进，确保它们以符合道德就安全要求的方式运作也变得越来越具有挑战性。种种意想不到的风险也将接踵而至，例如 AI 可能做出与人类价值观不相符的行动决策。隐私与安全：随着 AI 愈发先进，隐私和数据安全问题也将不断升级。能够深入理解数据并与数据交互的 AI 一旦遭到滥用，后果将难以估量。想象一下，当我们向家人说出善意的谎言时，AI 很可能基于诚实原则而将其戳破。经济影响：自动化与 AI 能力的增强可能会彻底消灭某些岗位甚至是特定行业，强迫整个社会找到新的劳动力培养方式。如果AI已经能够完成大部分工作，人类在劳动力市场上将变得毫无意义。价值观错位：AI 系统可能会制定与人类意图相背、甚至有损人类福祉的目标或行动方法，最终造成有害结果。想象一下，清洁机器人可能会为了保持整洁而丢弃用户的重要文件，甚至通过“干掉”主人的方式让房间永不杂乱。</p><p></p><h2>AGI 即将成为现实？</h2><p></p><p>&nbsp;</p><p>对于神秘的 Q*，有观点认为，在追求通用人工智能（AGI）的过程中，Q* 将发挥关键作用。</p><p>&nbsp;</p><p>所谓 AGI，是指机器能够在各种任务中表现出类似于人类的理解、学习和智能应用水平。作为AI的一种形式，AGI 可以将自己的经验从一个领域推广到另一领域，从而展现出真正的适应性和多功能性。虽然当前 Q* 与 AGI 之间还有很大距离，但 Q* 有可能代表着特定 AI 功能的重大进展。</p><p>&nbsp;</p><p>网友 Sebb 认为，AGI 将在未来 6 到 24 个月内实现，这已经成为一种必然。“一切阻止都将是徒劳的，我们必须马上为此做好准备，并考虑到某些人带着恶意参与这场人类历史上意义最为深远的技术发明。我们人类是否真是生物史上最先进的进化物种，可能将在这场颠覆中给出证明。”</p><p>&nbsp;</p><p>也有网友对此感到担忧，网友 m4callik 称自己“要怀疑 Sam 的动机了，而且会从不同的角度看待最近的这场 OpenAI 闹剧”，“事态正飞速变化，比任何人想象的都要快。我绝对不希望让微软、Larry Summers 或者什么 Salesforce 前 CEO 来决定某项成果是否属于 AGI。让那帮能靠 AI 商业潜力赚大钱的既得利益者来判断 AGI 是否实现，就像让裁判员亲自下场比赛一样，毫无公信力可言。”</p><p>&nbsp;</p><p>网友&nbsp;Browsergpts.com 则认为，目前争论的焦点并不在于 AGI 本身，而是在表达对领导决策和安全协议的担忧。“AGI 有望彻底改变社会的方方面面，所以我们必须为它给人类各领域造成的影响做好准备，这才是关键中的关键。AGI 就像一把机会之钥，只要转动一下就能带来巨大的收益，同时也造成巨大的风险。必须采取强有力的安全措施来保证其得到妥善使用。</p><p>&nbsp;</p><p>作为 AI 领域的领导者，Sam 和其他 OpenAI 董事肩负着应对这一复杂局面的使命。我相信他们正在尽最大努力实现安全过渡，但在推动 AGI 技术发展的过程中，我们也得采取必要的预防措施——毕竟对于这样一项重量级、变革性技术，也许根本没有任何亡羊补牢的余地。”</p><p>&nbsp;</p><p>不过，如今的 Q* 系统既无自我意识，也无法超越其预训练数据和人类设定算法的边界。所以必须承认，Q* 还远没有达到威胁人类的地步。虽然 Q* 确实是一大飞跃，但它距离 AGI 还很遥远、人类目前仍然安全无忧。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://decrypt.co/207413/what-is-q-star-q-learning-agi-openai">https://decrypt.co/207413/what-is-q-star-q-learning-agi-openai</a>"</p><p><a href="https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?filter=summary">https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?filter=summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IhNKZ3KjYS1pYVws56yq</id>
            <title>法律 ChatLaw、金融度小满轩辕大模型实战课程来袭！专家教你搭建 AI 原生应用，百度智能云千帆 SDK 加速应用创新</title>
            <link>https://www.infoq.cn/article/IhNKZ3KjYS1pYVws56yq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IhNKZ3KjYS1pYVws56yq</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 06:13:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 千帆大模型, 行业大模型, ChatLaw, 度小满轩辕大模型, 百度智能云千帆 SDK
<br>
<br>
总结: 百度智能云千帆大模型平台是一个行业大模型应用实训营，通过深度学习和大数据分析，挖掘出数据背后的价值，为各行业的智能化升级提供支持。其中，ChatLaw大模型在法律科技领域应用广泛，提升了法律服务的效率和准确性。度小满轩辕大模型在金融领域应用，为金融行业带来了前沿应用。此外，百度智能云千帆SDK的应用指南也为行业应用创新提供了支持。 </div>
                        <hr>
                    
                    <p>百度智能云<a href="https://www.infoq.cn/article/o8abj2wff5yLfGWuB0E1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">千帆大模型</a>"平台官方出品的《大模型应用实践》实训营本周正式进入第三周最终阶段！在首周学习了百度智能云千帆大模型平台，第二周深入了解了一些大模型核心能力之后，本周我们将正式进入行业实战，带大家完整地搭建出行业场景中的 AI 原生应用。</p><p></p><h4>&nbsp;行业大模型应用浪潮</h4><p></p><p></p><p>随着科技的飞速发展，人工智能和大数据技术的融合应用已经成为各行业的核心竞争力。行业大模型，具备强大的计算和处理能力，能够应对复杂的业务场景。无论是在金融、医疗、教育、法律，还是在制造、零售、物流等领域，行业大模型都能够通过深度学习和大数据分析，挖掘出数据背后的价值，为行业的智能化升级提供有力支撑。</p><p></p><p><a href="https://www.infoq.cn/article/lq2fJ7gm9iyuOLkaLdqv?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">ChatLaw</a>" 作为法律科技领域的佼佼者，利用自然语言处理技术为法律行业带来了革命性的变革。在本周的课程中，ChatLaw 大模型将展现其在法律文档智能分析、合同审查以及法律咨询等方面的卓越能力。通过 ChatLaw 大模型的应用构建，学员们将深入了解如何将先进的人工智能技术应用于法律领域，提升法律服务的效率和准确性。</p><p></p><p><a href="https://www.infoq.cn/article/0qEivqb1dXP9Wuqmhu9B?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">度小满轩辕大模型</a>"在金融领域的应用也引起了广泛关注。度小满轩辕大模型凭借强大的大数据分析和机器学习能力，为金融行业带来了信贷风险评估、投资组合优化和金融市场预测等前沿应用。学员们将通过度小满的具体金融行业应用案例，深刻感受大模型在金融领域的巨大潜力。</p><p></p><h4>&nbsp;百度智能云千帆 SDK 构建持续成长的 AI 原生应用</h4><p></p><p></p><p>除了 ChatLaw 和度小满轩辕大模型的行业大模型，课程还特别推出了百度智能云千帆 SDK 的应用指南。千帆 SDK 以其丰富的功能和易用性受到了广泛关注。课程通过详细讲解 SDK 的使用方式及技巧与实践案例，学员们可以轻松掌握其应用方法，为行业应用创新提供有力支持。</p><p></p><p>扫描下图二维码，即刻加入大模型实训营活动群，报名参加第三周课程！您也可以在群中随时观看往期课程精彩回放。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f67472b5c7ebcb5e0f090290e1475a0.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IMw3hYiqR4GOwk7d5I7S</id>
            <title>蚂蚁集团研发效能技术负责人陈红伟，确认担任 QCon 智能研发时代效能提升之路专题出品人</title>
            <link>https://www.infoq.cn/article/IMw3hYiqR4GOwk7d5I7S</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IMw3hYiqR4GOwk7d5I7S</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 陈红伟, 智能研发时代效能提升之路, 大模型垂直领域 Code-LLM
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，蚂蚁集团研发效能技术负责人陈红伟将担任「智能研发时代效能提升之路」的专题出品人。在此次专题中，将介绍大模型垂直领域 Code-LLM如何结合研发全流程，渗透设计、需求、编码、测试、部署、运维等关键阶段，彻底迈入智能研发时代。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=8&amp;utm_term=1127&amp;utm_content=chenhongwei">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。蚂蚁集团研发效能技术负责人陈红伟将担任「<a href="https://qcon.infoq.cn/2023/shanghai/track/1614?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=8&amp;utm_term=1127&amp;utm_content=chenhongwei">智能研发时代效能提升之路</a>"」的专题出品人。在此次专题中，你将了解到，大模型垂直领域 Code-LLM 如何结合研发全流程，渗透设计、需求、编码、测试、部署、运维等关键阶段，彻底迈入智能研发时代。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/track/1614?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=8&amp;utm_term=1127&amp;utm_content=chenhongwei">陈红伟</a>"，2007 年加入蚂蚁，现任蚂蚁研发效能技术负责人。专注于支付金融系统架构、技术风险、云原生、云研发、持续交付等领域，通过技术创新引领研发效能的持续架构演进，坚持研发智能化，打造了百灵代码大模型、百灵研发助手，不断提升蚂蚁研发效能成熟度。</p><p></p><p>相信陈红伟的到来，可以帮助提升此专题的质量，让你学习到代码领域大模型研究和开发的最新进展，在研发领域的创新应用形式，以及 AI 及大语言模型对研发效能的影响。</p><p></p><p>除上述专题外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1599?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的大前端技术</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！现在购票，享 8 折优惠，立减￥1360！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CsiC1DgdR94TEDf3X3rQ</id>
            <title>拼多多成立大模型团队，年薪百万招聘人才；网传TCL旗下芯片公司“原地解散”；小伙被AI换脸的“表哥”骗走30万 | AI一周资讯</title>
            <link>https://www.infoq.cn/article/CsiC1DgdR94TEDf3X3rQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CsiC1DgdR94TEDf3X3rQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 02:16:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国芯片巨头博通, 马云前助理, AI芯片独角兽Graphcore, 阿里云调整组织架构
<br>
<br>
总结: 美国芯片巨头博通完成收购VMware；马云前助理回应马家厨房不做预制菜；AI芯片独角兽Graphcore退出中国，大幅裁员；阿里云调整组织架构。 </div>
                        <hr>
                    
                    <p></p><blockquote>狂砸近5000亿！美国芯片巨头博通正式收购VMware；停止分拆后，阿里云调整组织架构；网传TCL旗下芯片公司“原地解散”；AI 芯片独角兽 Graphcore 退出中国，大幅裁员；身价1.2万亿，字节跳动CEO张一鸣或成为中国第一个世界首富……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>马云前助理回应：马家厨房不做预制菜</h4><p></p><p></p><p>11月25日，马云前助理陈伟在朋友圈发文称，马家厨房不做预制菜。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ba/bab3b0546a1bbbfc6ce8a098d558ab0d.webp" /></p><p></p><p>11月22日，国家企业信用信息公示系统显示，杭州马家厨房食品有限公司（以下简称马家厨房）注册成立，经营范围含食品销售（仅销售预包装食品）、货物进出口、食用农产品批发、日用品批发、酒店管理、技术服务等。</p><p></p><p>其中，预包装食品按照《食品安全法》的定义是指预先定量包装或者制作在包装材料、容器中的食品。对应的是散装食品。而外界却将此理解成“预制菜”。这一消息也被解读为“马云入局预制菜”，股市预制菜概念迎来异动。据Choice数据，当日预制菜概念49家公司中45家收涨，最高一支涨幅超过29%。</p><p></p><h4>狂砸近5000亿！美国芯片巨头博通正式收购VMware</h4><p></p><p></p><p>据快科技报道，11 月 23 日，美国芯片巨头博通今天宣布，完成了以690亿美元（约合4925亿元人民币）收购威睿（VMware）的交易，后者的普通股将停止在纽约证券交易所交易。</p><p></p><p>2022年5月，博通宣布计划以610亿美元（约合4367.6亿元人民币）的价格收购美国虚拟化软件厂商VMware，并将承担其80亿美元（约合572.8亿元人民币）的债务。然而该交易在全球均面临着严格的监管审查，两家公司已3次推迟并购完成日期。在11月21日中国国家市场监督管理总局宣布，决定附加限制性条件批准博通对VMware的收购后，博通表示：该交易已在澳大利亚、巴西、加拿大、中国、欧盟、以色列、日本、南非、韩国、中国台湾地区和英国获得所有必要的监管批准，完成交易已不存在任何法律障碍。</p><p></p><p>根据国家市场监督管理总局决定，合并后的公司不能滥用其在几个领域的市场地位，包括继续保证VMware的服务器虚拟化软件，与在中国境内销售的第三方相关硬件产品的互操作性。该交易也将成为年内第二大科技业收购案，仅次于微软以690亿美元收购动视暴雪。</p><p></p><h4>AI 芯片独角兽 Graphcore 退出中国，大幅裁员</h4><p></p><p></p><p>11 月 23 日消息，据有关媒体报道，英国 AI 芯片独角兽 Graphcore 裁掉中国区大部分员工，并停止在华销售。这标志着 AI 芯片企业再遇重创。</p><p></p><p>该公司已证实这一决定，理由是美国最近对华加码的出口管制。“很遗憾，这意味着我们将大幅缩减在华业务。”一位发言人在电子邮件中表示。该公司拒绝透露受影响的员工人数。据了解，此前为了削减成本，Graphcore 在去年 9 月宣布裁员。根据 Dealroom 的数据，Graphcore 的员工人数从 2022 年 10 月的 620 人减少到 2023 年 10 月的 418 人。</p><p></p><p>Graphcore 成立于 2016 年，设计用于数据中心的云端 AI 芯片，是英国科技及芯片产业最有前途的创企之一，也是前些年在 AI 芯片赛道中声量较高的“英伟达竞争对手”之一。</p><p></p><h4>OpenAI：ChatGPT的错误率升高，正展开调查</h4><p></p><p></p><p>OpenAI当地时间11月23日更新运行情况说明称，ChatGPT的错误率升高，正在开展调查。</p><p></p><h4>停止分拆后，阿里云调整组织架构</h4><p></p><p></p><p>11月23日，阿里云通过内部信宣布了新一轮组织架构调整：成立公共云业务事业部，由刘伟光负责，向阿里云CEO吴泳铭（又被称为“吴妈”）汇报。</p><p></p><p>在本次调整中，吴泳铭的职位并未改变，间接打破了此前胡晓明（花名孙权）将于年底前回归阿里云的传言。较为意外的是，前阿里云智能总裁张建锋（花名行癫）的名字也出现在了这封内部信之中。</p><p></p><p>上周，阿里刚刚宣布对阿里云暂停分拆。阿里高管在财报电话会上表示，目前公有云占到阿里外部收入的70%以上。在未来，阿里云将继续加大对公共云核心产品的投入。</p><p></p><h4>拼多多成立大模型团队，年薪百万招聘人才</h4><p></p><p></p><p>据Tech星球报道，拼多多成立了一个数十人的大模型团队，该团队将探索大模型在拼多多客服、对话等场景下的应用，并拓展至其旗下跨境电商平台 TEMU 的智能客服、搜索、推荐等业务场景。行业分析人士认为，拼多多的大模型将为其电商体系进行服务，包括在AI导购、商品图片智能生成等方面的应用。</p><p></p><p>目前，整个进程仍处于研发阶段。拼多多已开始在大模型领域招聘人才，年薪百万的职位不在少数，最高可达130万元。</p><p></p><h4>Stability AI推出Stable Video Diffusion模型，可根据图片生成视频</h4><p></p><p></p><p>11 月 22 日消息，专注于开发人工智能（AI）产品的初创公司 Stability AI 发布了其最新的 AI 模型 ——Stable Video Diffusion。这款模型能够通过现有图片生成视频，是基于之前发布的 Stable Diffusion 文本转图片模型的延伸，也是目前为止市面上少有的能够生成视频的 AI 模型之一。</p><p></p><p>Stable Video Diffusion 实际上是由两个模型组成的 ——SVD 和 SVD-XT。SVD 可以将静态图片转化为 14 帧的 576×1024 的视频。SVD-XT 使用相同的架构，但将帧数提高到 24。两者都能以每秒 3 到 30 帧的速度生成视频。</p><p></p><h4>阿里钉钉与华为达成合作，正式启动鸿蒙原生应用开发</h4><p></p><p></p><p>11月23日，阿里巴巴旗下的智能化协同办公及应用开发平台钉钉与华为达成鸿蒙合作，双方将在产业创新、技术应用、商业发展等领域全面合作，并正式启动“钉钉鸿蒙版”的开发。钉钉将以原生方式适配鸿蒙系统，成为首批加入鸿蒙生态的智能办公平台。</p><p></p><h4>网传TCL旗下芯片公司“原地解散”</h4><p></p><p></p><p>11月21日，网传TCL控股子公司摩星半导体（广东）有限公司（下称“摩星半导体”）被内部人员爆出公司“原地解散”的消息。据网传，当天上午10点，公司老板把所有人集合到前台，直接宣布解散，赔偿方案为N+1，整个公司包括软件、IC甚至行政在内全部解散。此次裁员波及近百人，包括广州总部几十人，以及上海、深圳等分中心几十人。</p><p></p><p>针对网传消息，11月22日，南都湾财社记者从TCL的一名消息人士处获悉，摩星确实有结构性的人员调整，主要是从整体业务布局考虑做出的上述调整，会依法做好相关员工的离职补偿。摩星业务体量不大，对公司整体经营影响不大。</p><p></p><h4>比尔·盖茨：AI将令"做三休四"成为可能</h4><p></p><p></p><p>微软创始人比尔·盖茨日前表示，科技可能无法取代人类，但它可以使每周工作3天成为可能。盖茨周二现身知名脱口秀节目主持人特雷弗·诺亚（Trevor Noah）的播客节目中。当诺亚问及AI对就业的威胁时，盖茨表示，总有一天，人类“不必如此努力地工作”。“如果你最终进入一个每周只需要工作三天的社会，那可能还不错。” 盖茨表示。</p><p></p><p>他指出，未来的世界可能是这样的：“机器可以制造所有的食物和东西”，人们不必每周工作五天以上来维持生计。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>身价1.2万亿，字节跳动CEO张一鸣或成为中国第一个世界首富</h4><p></p><p></p><p>据胡润研究院新发布的《2023胡润百富榜》，69岁的农夫山泉董事长钟睒睒第三次成为中国首富，他的财富是4500亿元。排名第二的马化腾2800亿元，与钟睒睒相差很远。但钟睒睒在全球富豪榜上，只能排到第15名。</p><p></p><p>据相关报道及研究，一旦字节跳动整体上市，字节跳动创始人张一鸣或是未来的破局者，很有可能成为全国甚至世界首富。估测下，张一鸣的财富值可能是万亿以上人民币，若如此，张一鸣将创造历史，成为中国有史以来最富有的个人，也成为中国第一个世界首富。</p><p></p><p>据外媒报道，由于广告和电商业务营收入大幅增长，字节跳动今年第二季度收入增长超过40%，达到290亿美元；上半年营收约540亿美元。换算过来，即便字节今年下半年收入零增长，全年收入也将高达7830亿元人民币。</p><p></p><h4>新华三中高层主动降薪，最高者达20%</h4><p></p><p></p><p>据《科创板日报》报道，11 月 24 日，一则由新华三CEO发出的中高层员工主动降薪的邮件截图在网络流传。具体而言，新华三公司个人职级在17级（含）以上的干部与员工将主动降薪20%，个人职级16级干部与员工、15级干部将主动降薪10%。执行期自2023年12月1日起至2024年12月31日止，公司将根据经营情况决定是否提前终止或延长。多名新华三在职员工向《科创板日报》记者确认了此事，新华三方面目前未就此事回应记者。</p><p></p><h4>小伙被AI换脸的“表哥”骗走30万</h4><p></p><p></p><p>据海报新闻报道，近日，山东省济南市商河县公安局刑警大队破获了一起利用AI换脸技术诈骗的典型案件。“今年5月份，黄某某（化名）在家刷短视频时收到了一条私信，对方自称是黄某某的表哥，寒暄几句取得黄某某初步信任后，对方又让黄某某添加了其QQ号。”侦办此案的商河县公安局刑警大队三中队中队长张振华告诉海报新闻记者，黄某某的表哥由于在体制内工作，因此对方以自己身份不便为由，希望黄某某帮自己给一个需要资金周转的朋友，转一笔大额资金。</p><p></p><p>为了表示诚意，“表哥”索要了黄某某的银行卡号，并称先让朋友给黄某某转款后，再让黄某某转出。很快，黄某某就收到了对方发来的“转账凭证”，虽然看到了“转账凭证”，但黄某某并没有收到这笔转款，因此没有立即帮“表哥”转账。</p><p></p><p>大约过了20分钟，“表哥”再次发消息给黄某某，并称大额转款到账会有延迟，目前急需这笔钱，黄某某能否先行垫付一下。黄某某随后与“表哥”进行了视频通话，黄某某见到视频中的人和表哥一模一样，不过对方声音比较低，大约十几秒后，对方称有会议便挂断了视频通话。</p><p></p><p>黄某某确信了对方身份后，于是向对方提供的银行账户转了30余万元。收到这笔钱后，对方还继续催促黄某某转出剩余的欠款。就在黄某某继续筹集资金的过程中，黄某某发现起初添加自己的短视频账号已经显示被封禁，他急忙给表哥打去电话核实，才知道其表哥对此事毫不知情。意识到受骗的黄某某立即报警。</p><p></p><p>警方根据嫌疑人与黄某某网聊时的IP地址，判定该团伙在境外实施诈骗，由国内销赃洗钱，是一起采用AI技术换脸、冒充受害者熟人进行点对点网络诈骗的案件。商河警方通过技术手段侦查，两次前往广东东莞，抓获犯罪嫌疑人7人，目前此案已进入法院审判阶段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8Y7LDSTwbAckDz62pDDd</id>
            <title>马云成立公司卖预制菜；斗鱼 CEO 因涉嫌开设赌场罪被捕；曝拼多多入局大模型，百万年薪招兵买马｜Q 资讯</title>
            <link>https://www.infoq.cn/article/8Y7LDSTwbAckDz62pDDd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8Y7LDSTwbAckDz62pDDd</guid>
            <pubDate></pubDate>
            <updated>Mon, 27 Nov 2023 01:46:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 高层地震, CEO Sam Altman, 高级 AI 研究团队, 微软, 人工智能, Q*模型, 马云, 阿里股票, 马家厨房, 币安, 赵长鹏
<br>
<br>
总结: 上周末，OpenAI发生了高层地震，CEO Sam Altman被驱逐出公司，但经过一番波折后，他最终回归OpenAI。竞争对手纷纷吸引OpenAI的客户，谷歌、亚马逊和Salesforce都希望从中受益。此外，有关Q*模型的消息引发了OpenAI董事会的不满，成为Altman被解雇的导火索之一。另外，马云成立了马家厨房公司销售预制菜，但阿里内部澄清马云并未出售阿里股票。币安的首席执行官赵长鹏认罪并辞去职务。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>OpenAI“政变”终落幕，但事后持续爆料；马云成立公司卖预制菜，阿里内部发文澄清马云出售阿里股票；币安赵长鹏认罪，辞去CEO职务；斗鱼CEO陈少杰因涉嫌开设赌场罪被捕；苹果 CEO 库克在找接班人：希望来自内部，会有多个人选；蔚来员工爆料：为保工作贷款60万买了两辆ET5仍被裁；𝕏平台要求员工寻找新收入来源，CEO被轮番游说从𝕏辞职来挽救声誉；传拼多多入局大模型，年薪百万招兵买马；荣耀董事长换帅，辟谣“借壳上市”……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p></p><h4>OpenAI“政变”终落幕，但事后持续爆料</h4><p></p><p>&nbsp;</p><p>上周末，OpenAI突然发生的高层地震终于暂时告一段落。在几经波折后，北京时间周三午后 14 时许，OpenAI 官方宣布，已经与上周五被驱逐出公司的前 CEO Sam Altman 达成一项原则性协议，他将回归 OpenAI 继续担任首席执行官。同时公司的董事会也将发生变化，曾在 Facebook、推特、赛富时担任过高管的 Bret Taylor 将出任新的董事会主席，投资者们颇为熟悉的美国前财长 Summers 也加入 OpenAI 董事会，而原董事会成员、美国“知乎”Quora 联合创始人 Adam D'Angelo 将留任。</p><p>&nbsp;</p><p>北京时间本周一，微软公司首席执行官 Satya Nadella 宣布，Sam Altman 和 Greg Brockman 及其同事将加入微软，领导一个新的高级 AI 研究团队。此后，超过721人名OpenAI员工签署了联名信，以辞职要挟，要求公司董事会辞职，并恢复Altman和 Brockman的职位，否则他们也将加入微软。经过再一轮谈判，Altman 终于回归OpenAI。</p><p>&nbsp;</p><p>OpenAI内部动荡之际，竞争对手纷纷以激励措施吸引其客户转向其平台。谷歌表示，旗下销售团队已经发起一项活动，试图说服客户放弃OpenAI，团队推出了与OpenAI服务价格相当的定价策略和其他服务。亚马逊则表示，最近OpenAI发生的事件凸显出亚马逊为客户提供多种生成式人工智能的策略很有价值，客户可以在许多人工智能系统之间进行选择，而不是只选择OpenAI或单一供应商。美国云计算软件巨头Salesforce也希望能借此机会吸引该公司的一些员工加入其AI团队。</p><p>&nbsp;</p><p>11月23日，埃隆·马斯克转发了一封OpenAI前雇员写给董事会的起底信，内容围绕OpenAI近期发生的有关Altman被解雇的事件。</p><p>&nbsp;</p><p>信中称，Sam Altman 推迟了对几项秘密计划的报告，这些计划最终未能按照他希望的速度交付而被砍掉。那些反对这项政策的人被立即斥为“文化不合”而被解雇。Altman 还授权监视 OpenAI 的关键员工，包括其首席科学家 Ilya Sutskever。这些员工还对Brockman 使用歧视性语言对待一名性转同事表示不满，该员工后来也因表现不佳而被解雇。&nbsp;</p><p>&nbsp;</p><p>根据这些匿名人士的说法，OpenAI 的治理结构存在缺陷，“Sam 和 Greg 专门设计的 OpenAI 治理结构，故意将员工与营利性运营的监督隔离开来，这恰恰是由于他们固有的利益冲突。这种不透明的结构使 Sam 和 Greg 能够不受惩罚地运作，免受问责。 ”</p><p>&nbsp;</p><p>此外，有知情人士透露， Altman被罢免4天之前，几位研究人员向董事会发出一封信，警告称一项强大的人工智能发现可能威胁人类——被称为Q*的模型。根据爆料，被称为Q*的模型能够解决某些数学问题。征服数学的能力意味着人工智能将拥有类似于人类智能的更强推理能力。这一消息得到了至少两家媒体的独立证实，但也有媒体援引知情人士的话称，董事会从未收到过有关此类突破的信，而且公司的研究进展并未对奥特曼的突然解雇产生影响。</p><p>&nbsp;</p><p>两位消息人士告诉路透社，此前未披露的信件是董事会罢免奥特曼的一个关键导火索，是导致董事会众多不满的原因之一。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://www.infoq.cn/article/Vse4aLuyO2gLBozgyVbw">OpenAI“生死存亡”时刻：95% 员工或将加入微软，原 OpenAI 寻求与竞对合并？</a>"</p><p><a href="https://www.infoq.cn/news/eI6tNmSsCDVfcFshdE7o">OpenAI“宫斗戏”落幕！最大的赢家不是 Altman，也不是微软</a>"</p><p>&nbsp;</p><p></p><h4>马云成立公司卖预制菜，阿里内部发文澄清马云出售阿里股票</h4><p></p><p>&nbsp;</p><p>11月22日消息，针对近日有传言称马云抛售巨额阿里股票，阿里巴巴集团合伙人、首席人才官蒋芳在阿里巴巴内网发帖称，“马云一股都没有出售”，董事长蔡崇信也跟帖表示“我们只要以开放的心态，创新的思维，就有机会再创一个与众不同的阿里。”</p><p>&nbsp;</p><p>蒋芳称，为了在国内外投资农业科技和给公益事业等项目获取资金，马云办公室今年8月就与股票经纪商签了减持合同。由于当时设定的售卖价格远高于目前股价，只要股价未达到设定售卖价格，就不会售出股票，因此实际上马云所持股票一股未卖。蒋芳还通过内网向阿里员工转达，马云坚定看好阿里，“阿里股票目前大大低于阿里巴巴的实际价值，他不会卖的”。</p><p>&nbsp;</p><p>此外，据报道，马云成立了马家厨房公司销售预制菜。据国家企业信用信息公示系统网站显示，11月22日，杭州马家厨房食品有限公司成立，法定代表人为 PAU JASON JOHN，注册资本 1000 万人民币，经营范围含食品销售（仅销售预包装食品）、货物进出口、食用农产品批发、日用品批发、酒店管理、技术服务等。股权全景穿透图显示，该公司由杭州大井头贰拾贰号文化艺术有限公司全资持股，后者由马云持股 99.9%。</p><p>&nbsp;</p><p>另外，11月19日晚，阿里巴巴方面澄清了一则“阿里即将裁员25000人”的谣言，“裁员谣言接二连三，但假的就是假的”。据了解，阿里方面针对此次谣言已报警。据悉，今年以来，所谓阿里及旗下各业务“即将大裁员”的谣言接二连三，5月底，阿里巴巴集团就对裁员谣言进行过辟谣。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>币安赵长鹏认罪，辞去CEO职务</h4><p></p><p>&nbsp;</p><p>据媒体周二报道，全球最大的加密货币交易所币安及其首席执行官赵长鹏与美国司法部达成协议，同意对刑事和民事指控认罪。据报道，币安赵长鹏面临最高10年监禁，但根据认罪协议，预计刑期不会超过18个月，美国司法部尚未决定对他施行多长时间的监禁。</p><p>&nbsp;</p><p>知情人士透露，赵长鹏将于周二下午在西雅图联邦法院出庭认罪，币安将承认一项刑事指控，并同意支付总计43亿美元的罚款，其中包括监管机构提出的民事指控和解金。根据报道，该协议可能使币安拥有继续运营的能力。据悉，赵长鹏将不得参与币安公司一切事务，但保留在币安的多数股权。</p><p>&nbsp;</p><p>11月22日，币安宣布全球区域市场负责人Richard Teng将接替赵长鹏担任首席执行官，立即生效。公告显示，Richard Teng在加入币安之前，曾担任阿布扎比全球市场金融服务监管局首席执行官，新加坡交易所（SGX）首席监管官以及新加坡金融管理局企业融资总监。</p><p>&nbsp;</p><p>此外，币安还宣布已与美国司法部、商品期货交易委员会、外国资产控制办公室和金融犯罪执法局就其历史登记、合规和制裁调查达成了决议。&nbsp;</p><p>&nbsp;</p><p></p><h4>斗鱼CEO陈少杰因涉嫌开设赌场罪被捕</h4><p></p><p>&nbsp;</p><p>斗鱼公司 CEO 陈少杰因涉嫌开设赌场罪被警方逮捕引发关注。据报道，斗鱼平台称其已于11月中旬开展“清朗行动”，严厉打击私下交易等违规行为，并表示涉嫌洗钱、诈骗、赌博等违法犯罪的，平台将提交并移送司法机关处理。</p><p>&nbsp;</p><p>11月21日，斗鱼发布公告称，公司CEO陈少杰先生于2023年11月16日左右被成都警方逮捕。公司尚未收到任何有关对陈先生进行调查或陈先生明显被捕的原因的正式通知。斗鱼盘前直线下挫，一度跌超10%。公司表示无法对可能发生的后续法律诉讼（如有）的性质或预期时间表发表评论。</p><p>&nbsp;</p><p>此前11月初，斗鱼董事会主席兼CEO陈少杰被报称已失联近三周。当时有游戏直播业内人士猜测，陈少杰失联或与涉赌有关。公开信息显示，2021年1月，斗鱼主播“长沙乡村敢死队”直播间曾被多家媒体报道涉嫌赌博。有第三方直播数据平台统计，“长沙乡村敢死队”2020年收益高达1.77亿元，单日流水1317.67万元，被网友称为“斗鱼最大赌场”。</p><p>&nbsp;</p><p></p><h4>苹果 CEO 库克在找接班人：希望来自内部，会有多个人选</h4><p></p><p>&nbsp;</p><p>苹果 CEO 蒂姆·库克在一档播客中表示，他已决定从公司内部寻找自己的继任者，目前正在努力为董事会提供几个选择。</p><p>&nbsp;</p><p>苹果在寻找创始人史蒂夫·乔布斯的继任者方面，因过于隐秘而遭到了业界指责。如今，关于库克的接班人问题也是如此，苹果仍未公开讨论过。当被问及“到 2050 年是否还会留在苹果，以看看公司在环境方面的工作成果”时，库克说：“2050 年可能是一段漫长的跨度，我也不确定自己能待多久。”</p><p>&nbsp;</p><p></p><h4>蔚来员工爆料：为保工作贷款60万买了两辆ET5仍被裁</h4><p></p><p>&nbsp;</p><p>近日，一位在蔚来汽车工作了 5.5 年的技术岗位员工爆料，自己为了保住工作，不得不贷款60万购买了两辆蔚来汽车，还推荐了亲戚朋友也买了蔚来汽车，但最终还是被公司裁掉。他称自己上有老下有小，每月要还一万多元的车贷，生活无望。</p><p>&nbsp;</p><p>据该员工透露，蔚来汽车对员工有一种不成文的规定，就是想要在公司稳定工作，或者有机会升职，就必须购买蔚来汽车，甚至还要动员身边的人也买车。他说：“几乎80% 的蔚来员工都被明示或暗示过，想要保住这份工作必须得买蔚来汽车。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d341c744ccc13d2a96d927495bceee6.png" /></p><p></p><p></p><h4>&nbsp;</h4><p></p><p></p><h4>𝕏平台要求员工寻找新收入来源，CEO被轮番游说从𝕏辞职来挽救声誉</h4><p></p><p>&nbsp;</p><p>据知情人士称，在𝕏平台周一匆忙召开的一次全体会议上，该公司CEO琳达·亚卡里诺敦促员工寻找新的收入来源。此前多家大公司已暂停在该平台投放广告。她还要求员工“尽可能地在财政上负责”，包括只在“关键和必要的旅行”上花钱。据一位知情人士称，该公司刚刚更新了差旅政策，有些差旅可能需要得到亚卡里诺或马斯克本人的批准。亚卡里诺说：“无论如何，你们要集思广益，为公司带来新的收入。”</p><p>&nbsp;</p><p>此外，据知情人士称，亚卡里诺这个周末遭遇了她在广告界的朋友们的轮番游说，要求她从𝕏辞职以挽救自己的声誉。此前由于担心𝕏的所有者埃隆·马斯克和他的反犹评论，苹果、IBM、迪士尼等大品牌暂停了在该平台投放广告。但亚卡里诺拒绝离职，并告诉那些给她打电话的人，她相信𝕏的使命和员工。</p><p>&nbsp;</p><p></p><h4>传拼多多入局大模型，年薪百万招兵买马</h4><p></p><p>&nbsp;</p><p>有消息称，拼多多已经成立了一个数十人的大模型团队，团队位于上海。大模型团队将探索大模型在拼多多客服、对话等场景下的应用，且会拓展至其旗下跨境电商平台TEMU智能客服、搜索、推荐等业务场景。目前，整个进程仍处于研发阶段。</p><p>&nbsp;</p><p>行业分析人士认为，拼多多的大模型将为其电商体系进行服务，包括在AI导购、商品图片智能生成等方面的应用。经查询发现，拼多多已经通过官网，以及其他招聘渠道，开始在大模型领域招兵买马。拼多多官方的招聘职位目录中已经出现了大模型（LLM）和NLP算法等领域的职位，大部分位于上海长宁区，薪资大多在30-60K之间，其中不乏50-80K，16薪。</p><p></p><h4>阿里云终止拆分后，宣布新的组织架构</h4><p></p><p>&nbsp;</p><p>继明确“AI驱动、公共云优先”战略后，11月23日，阿里云进行了一系列组织架构调整，首次成立专门的公共云业务事业部，以快速推进这一战略落地。其中，最受瞩目的一把手角色未有变化，吴泳铭（花名“东邪”，又被称为“吴妈”）依旧担任阿里云集团CEO。</p><p>&nbsp;</p><p>本次新公布的组织架构主要涉及：产研线、商业线以及包括供应链&amp;IDC等8个部门。阿里巴巴集团层面成立了基础设施委员会，该委员会由吴泳铭负责，成员为靖人（阿里云CTO 周靖人）、小邪（阿里合伙人蒋江伟）、范禹（阿里巴巴CTO 吴泽明）、行癫（达摩院院长张建锋），向吴泳铭汇报。具体高管层面：</p><p>&nbsp;</p><p>成立公共云业务事业部，以规模优先、扩大市场占有率为目标，由刘伟光负责，向阿里云CEO吴泳铭汇报。成立混合云业务事业部，以满足一些特定行业因政策限制、短期无法使用公共云的客户需求，由李津负责，向CEO吴泳铭汇报。成立基础设施事业部，打造面向未来的软硬一体底层基础设施，由蒋江伟负责，向阿里云CTO周靖人汇报。海外业务事业部继续由袁千负责，向CEO吴泳铭汇报。</p><p>&nbsp;</p><p>前不久回归的老将唐洪，担任阿里云首席架构师，全面负责产品管理、技术架构、稳定性、产品生态等工作，向CTO周靖人汇报。&nbsp;</p><p>&nbsp;&nbsp;</p><p>阿里合伙人郑俊芳除了担任阿里云智能集团首席财务官外，还将负责BI、战略投资、销管、价格管理等部门。另一名合伙人王磊，将负责供应链、官网、服务、CIO等部门。郑俊芳和王磊均向CEO吴泳铭汇报。</p><p>&nbsp;</p><p></p><h4>荣耀董事长换帅，辟谣“借壳上市”</h4><p></p><p>&nbsp;</p><p>近日，有关荣耀的董事长换人、借壳上市传闻甚嚣尘上。11月22日晚间，荣耀终端有限公司董事会发布公告，明确辟谣“借壳上市”传闻，其表示为实现公司下一阶段的战略发展，将不断优化股权结构，吸引多元化资本进入，通过首发上市推动公司登陆资本市场。随着公司走向公开市场的规划逐步启动实施，公司董事会将按照上市公司标准进行调整，董事会成员逐步多元化，以适应公司在新发展阶段的治理需要和监管需要。</p><p>&nbsp;</p><p>依据《公司法》及《公司章程》等相关规定，由公司股东会选举并经董事会选举，吴晖先生将担任公司董事、董事长，万飚先生担任副董事长。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>特斯拉开源Roadster：所有文件随便用</h4><p></p><p>&nbsp;</p><p>11月23日消息，特斯拉CEO马斯克在推特宣布，将公布开源初代Roadster的一切文件，目前特斯拉官网已对所有初代Roadster的原始设计和工程进行完全开源。有网友评论称：“这是不是意味着，我可以在自家车库里打造一辆Roadster？”马斯克回答道：“可以简单组装一下。”</p><p>&nbsp;</p><p>百万跑车的设计信息和工程文件说送就送，这很马斯克，早在2014年，马斯克就在特斯拉官网宣布开源特斯拉所有的专利技术，其中还涉及到纯电核心的三电系统，将特斯拉的技术全部公之于众。</p><p>&nbsp;</p><p></p><h4>OpenAI对手公司公布聊天机器人Claude2.1版本</h4><p></p><p>&nbsp;</p><p>就在OpenAI公司陷入内斗危机之际，其竞争对手、谷歌支持的人工智能初创公司Anthropic推出了其聊天机器人的最新版本Claude 2.1。</p><p>&nbsp;</p><p>据悉，Claude 2.1&nbsp;支持上下文窗口达 200K，Anthropic 表示这是一项复杂的壮举，也是行业首创。在此之前，OpenAI 在开发者日上公布的&nbsp;GPT-4 上下文窗口为 128K。200K 相当于大约 150,000 个单词，约 500 页的材料，这对于“整个代码库、财务报表（如 S-1 ），甚至是《伊利亚特》、《奥德赛》等长篇文学作品来说已经足够了。”公司博客中写道。</p><p>&nbsp;</p><p>据悉，Anthropic是一家由OpenAI前工程师Dario Amodei创建的公司，其聊天机器人产品Claude正在与OpenAI的ChatGPT展开激烈竞争。在OpenAI创立时期，Amodei与奥特曼在如何确保人工智能的安全发展和治理方面存在分歧，因此与之分道扬镳。</p><p>&nbsp;</p><p></p><h4>金山办公致歉并承诺用户文档不会被用于 AI 训练</h4><p></p><p>&nbsp;</p><p>近日有媒体报道称，此前《WPS 隐私政策》中提到「我们将对您主动上传的文档材料，在采取脱敏处理后作为 AI 训练的基础材料使用」，被质疑涉及用户隐私问题。对此，11 月 18 日，WPS 官方微博做出回应，在向用户致歉的同时，承诺用户文档不会被用于 AI 训练目的。</p><p>&nbsp;</p><p></p><h4>微软Copilot AI技术将对中国大陆开放？内部人士：不准确</h4><p></p><p>&nbsp;</p><p>11月23日，有消息称微软将于12月1日起在中国大陆企业和教育机构推出名为Copilot的Web AI聊天功能。微软内部人士对此表示，这一消息并不准确。</p><p>&nbsp;</p><p>从微软方面了解到，微软将于12月1日起在中国大陆企业和教育机构推出名为Copilot的Web AI聊天功能（以前的Bing Chat和Bing Chat Enterprise）适用于160多个地区，它在中国（不包括香港特别行政区和台湾地区）和俄罗斯是不可用的，但简体中文和俄文都支持。同时，该内部人士表示，有出海业务的客户也可以使用该业务，微软也欢迎出海客户使用。</p><p>&nbsp;</p><p></p><h4>Vite 5.0 正式发布</h4><p></p><p>&nbsp;</p><p>11 月 16 日，Vite 5 正式发布。Vite 现在使用 Rollup 4，这已经大大提升了构建性能。此外，还有一些新的选项可以提高开发服务器的性能。Vite 5 的重点是清理 API（移除过时的功能），并简化了一些功能，解决了一些长期存在的问题，例如，将 define 转换为使用正确的 AST 替换，而不是使用 regexes。该团队还表示将继续采取措施使 Vite 面向未来（现在需要 Node.js 18+ 版本，CJS Node API 已被弃用）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/esE3VVT3hCCueU6lAkSX</id>
            <title>众安科技鹰眼反欺诈系统在风险欺诈领域的落地与探索</title>
            <link>https://www.infoq.cn/article/esE3VVT3hCCueU6lAkSX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/esE3VVT3hCCueU6lAkSX</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 09:39:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融机构, 欺诈行为, 鹰眼反欺诈系统, 团伙欺诈识别
<br>
<br>
总结: 金融机构在线下渠道向线上渠道转型过程中，欺诈行为呈现出专业化、隐蔽化、高频化、团伙化等特征，严重影响金融产业的健康发展。众安科技发布鹰眼反欺诈系统，为金融机构提供整套解决方案，能够主动感知欺诈风险并实时识别与决策。其中，团伙欺诈识别是该系统的重要功能，通过图像识别技术分析用户活体图片，识别团伙欺诈行为。该系统的应用能够提升金融机构的反欺诈能力，防范欺诈风险。 </div>
                        <hr>
                    
                    <p>“&nbsp;<a href="https://www.infoq.cn/video/kmtocTpikvuifDiLCF1t?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">金融机构</a>"在线下渠道向线上渠道转型的过程中，欺诈行为呈现出作案行为专业化、作案方式隐蔽化、案件高频化、作案人员团伙化等特征，严重影响着金融产业的健康发展。随着金融产品的不断创新， 欺诈手段如雨后春笋层出不穷，如虚假宣传、伪冒申请、团伙欺诈、网络诈骗、营销作弊等，给投资者和金融机构带来严重的损失与侵害，同时会对金融机构声誉造成不良影响。在此背景下，<a href="https://www.infoq.cn/article/PwGh8pYCfzIRf4X0EDgX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">众安科技</a>"发布鹰眼反欺诈系统，为金融机构对于事前的欺诈风险主动及时感知和事中的实时识别与决策效果提供整套解决方案。”</p><p></p><p></p><h2>一、团伙欺诈识别</h2><p></p><p></p><p>团伙欺诈识别背景</p><p></p><p>在某金融业务的场景下，风控反欺诈人员在一次日常用户审查中，通过关系图谱发现在同一位置（基于<a href="https://www.infoq.cn/article/lbs-application-needs-and-innovation-space?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">LBS</a>"的经纬度计算一定范围内），集中申请人数超 3000，并发现其中存在大量背景相似度很高的图片，即这些用户的背景大多为办公场所。并且，活体人群衣着、姿态、外观、年龄差异较大，再结合针对这些用户的行为数据表现分析，风控反欺诈人员发现，这些用户保后表现不如正常客户理想，故推测疑似社会人员被“专业”的中介团伙利用做团伙性欺诈。</p><p></p><p>仅仅通过位置地理信息欺诈拦截可能会存在误判一些用户，但在此次发现的超 3000 人的列表中，风控人员通过判别部分普通用户的背景和“团伙用户”的背景之间的差异，判断出这些用户在非集中场所内。因此，更精准的、能够代替人工肉眼的图像识别的技术，值得引入并积极应用到团伙反欺诈场景中来。</p><p></p><p>用户活体图片分析</p><p></p><p>通过图示可以发现以下团伙的各自特点：</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be5f038c0cdf30e2de25797d101cb37b.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a845cde30bffefd5a0155317720ffe50.png" /></p><p></p><p>➢&nbsp;1号团伙图片特性：</p><p>○&nbsp;网格形天花板</p><p>○&nbsp;存在形状一致位置相似的顶灯</p><p>○&nbsp;相似的排风扇</p><p>○&nbsp;背景墙的颜色相似</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6ee9a57b3a0552194d3c0a35dcada6f.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c3ff330d31ade3bb8cce9b12042f702.png" /></p><p></p><p>➢&nbsp;2号团伙图片特性：</p><p>○&nbsp;红色大小相似的大门</p><p>○&nbsp;颜色大小相似的网格天花板</p><p>○&nbsp;相似的中央空调</p><p>○&nbsp;左后方几乎都存在一个饮水机</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f5cc9a66dafc7287ec29db67464acec.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97b1dd27a051b3d9eeba14bcb2595c62.png" /></p><p></p><p>➢&nbsp;3号团伙图片特性：</p><p>○&nbsp;一样的“担当”标语</p><p>○&nbsp;一样的“服务”标语</p><p>○&nbsp;米黄色背景墙</p><p>○&nbsp;灯开关独特且一致</p><p></p><p>经过图片及数据分析，不难发现：通对团伙的活体图片进行分析，不同团伙之间的背景特征是存在差异的，相同的团伙也可以通过识别背景的相似度来快速判断，经图片背景初步分析属于欺诈团伙作案的行为。同时，风控人员通过数据分析发现，对 LBS（基于位置服务）网格化，该批用户交易时的 LBS 信息均位于同属 GPS 网格内，对网格内用户进行行为分析，可较为容易识别高风险用户。</p><p></p><p>在反欺诈策略上，如果通过转人工审核方式对用户图片背景识别，首先无法实时防范、效率低下，人工投入精力巨大；其次，人工审核会存在遗漏的欺诈用户；再者基于传统的反欺诈策略拦截可能无法行之有效的规避团伙欺诈风险。</p><p></p><p></p><h2>二、反欺诈团伙欺诈博弈（应用）</h2><p></p><p></p><p>科技创新日新月异，大数据、人工智能、云计算、生物识别等技术的不断创新与发展，鹰眼反欺诈系统助力金融机构数字化转型，进一步推动金融体系的创新，为金融机构模式探索、安全提供强有力支撑。通过知识图谱、有监督、半监督、无监督机器学习、多模态学习等技术，鹰眼自动升级反欺诈风险模型，挖掘欺诈风险，提升欺诈案件识别的精准度，构建健全的反欺诈体系，防范欺诈风险。</p><p></p><p>在业务开展过程中，风险模型评估、人工智能、数据分析、身份认证、监控控制等均是金融领域欺诈防范的常用技能。人脸识别和活体检测双结合的生物识别技术，可通过有效防范假冒身份或伪造身份等行为，在不暴露个人隐私的前提下，验证用户的真实身份，是防范伪冒申请的强有力工具。然而团伙欺诈通过虚假宣传或欺诈手段，诱使受害者交易，从中收取高额手续费或利息，或冒用他人身份将钱款转移至自己账户，从中获利。</p><p></p><p>思考与分析</p><p></p><p>我们如何有效防范团伙欺诈案件？</p><p></p><p>针对申请用户，对 LBS 的经纬度计算一定范围内的用户进行圈选。当圈选范围内用户数量达到阈值，我们就需要对该批用户的活体人脸照片进行检测判断，是否存在团伙欺诈行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/beb6cc96306a413c75fe21eeb7117a58.png" /></p><p></p><p>针对用户活体背景分析，我们大致可以分为 5 个步骤</p><p>1.&nbsp;&nbsp;活体采集：收集活体人脸采集照片，进行人脸检测。</p><p>2.&nbsp;&nbsp;人形图像抠除：对人形图像抠除，进行人像分割，并对图像进行修复。</p><p>3.&nbsp;&nbsp;图像特征提取：基于算法进行图像特征提取计算。</p><p>4.&nbsp;&nbsp;聚类分析：对特征数据进行查重，聚类分析。</p><p>5.&nbsp;&nbsp;欺诈防范拦截：对欺诈用户进行加团伙加黑处理，策略中进行欺诈用户防范拦截。</p><p></p><p>落地方案</p><p></p><p>基于以上思考与分析，在某机构渠道，我们落地了鹰眼反欺诈解决方案。以下是鹰眼反欺诈解决方案的全景：反欺诈解决方案解决安全风险、欺诈风险、信用风险等风险问题。全景中重点包含数据采集、图像中心、决策引擎、团伙管理等。基于数据超市服务能力获取外部数据；基于特征指标加工内部数据；基于活体图片进行图像识别，通过特征提取、聚类分析进行团伙特征识别；基于识别出的团伙数据及用户进行团伙风险管理；基于决策引擎在交易过程进行风险及欺诈拦截；基于实时预警对风险快速通知，并进行大盘展示。由此形成全闭环的反欺诈体系解决方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e6a92869ed62fe43c3f602f0eab150b.png" /></p><p></p><p>数据采集</p><p></p><p>数据采集技术主要是应用于从客户端或网络获取客户相关数据的技术方法。值得强调的是，数据采集技术的使用，应当严格遵循法律法规和监管要求，在获取用户授权的情况下对用户数据进行采集。众安科技采用人脸活体采集技术，在用户主动活体认证过程中，通过快速抓拍动作姿态，从若干动作姿态中获取质量最高的几张人脸照片作为用户的活体图片，活体图片采集的照片质量会直接关系到后续对图片的分析结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6ab6666e79684fe419cb84166478145.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ab5a31d1f55f0ddde6b90122d5fd597.png" /></p><p></p><p>图像中心</p><p></p><p>传统方式中，团伙识别采用人工肉眼识别的方式，每天抽样几百张人脸图片，既费时费力，而且难以识别复杂的背景，团伙识别的实时性得不到保障。借助于最前沿的图像识别技术，可代替繁杂又低效的人工肉眼识别工作。如何把图像识别技术应用到团伙识别场景中，为此，众安科技自主研发了团伙背景识别及背景相似度聚类技术。</p><p></p><p>基于众安科技自研的图像背景识别技术，首先对活体人脸照片进行人脸监测、抠除，其次利用 AI 模型进行背景修复，将背景信息量化为数字指纹，并结合基于海量数据训练的相似度聚类模型，同时结合模型加速技术，实现了实时、精准的风险团伙识别。</p><p></p><p>●&nbsp;背景特征提取</p><p></p><p>图片背景特征提取，值得提及的是如何从一张活体图片上区分人物特征和背景特征，这里利用了 AI 模型进行人物抠图及背景修复，再通计算机卷积神经网络算法，得到图片背景不同维度的特征，最终将特征降维，即可获取一个可计算的背景特征组。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81b02eb412c538730bf25724ae3f88a4.png" /></p><p></p><p>●&nbsp;背景特征聚类</p><p></p><p>得到特征后，通过轮廓系数等，计算特征之间的相似度，最终聚类得到最终的图片相似结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/904b4c5f363c7e288a04ddd2dfb62381.png" /></p><p></p><p>这套技术目前已经在线上实时应用，应用实践过程中，通过人工离线质检的方式，结合线上实时识别的结果，经过多次的优化，目前团伙识别精准度已超 80%。</p><p></p><p>目前众安科技自主研发的图像识别技术除了在团伙识别上有着杰出的表现外，也在图片人物特征识别上有着杰出的表现，如：纹身、赤膊、多人、口罩等，这些人物活体时的特征可以结合反欺诈策略，帮助风控人员更全面的判断用户的性质，有效防范团伙欺诈，减少客户损失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fcba1a0cf4e0896b8c05bc68a982fc29.png" /></p><p></p><p></p><p>决策引擎</p><p></p><p>反欺诈决策引擎是反欺诈体系里的大脑和核心，基于众安科技自主研发的 X-Decision 决策系统（以下简称XD），反欺诈实时决策效率有了全面提升。XD 将信誉库、专家规则和反欺诈模型等各类反欺诈方法有效的整合，并为反欺诈人员提供一个操作高效、功能丰富的人机交互界面，大幅降低反欺诈运营成本并提升响应速度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0ae7afe6355da13174b4742c7c014161.png" /></p><p></p><p></p><p>基于 XD 决策引擎，可以快速部署丰富的反欺诈专家策略集，有效识别“团伙欺诈”、“电信诈骗”、“账号盗用”等多种风险行为，为金融业务安全保驾护航。此外，基于前端数据采集，进行用户行为轨迹分析；基于活体图片进行图像识别，在决策中心对用户进行风险评估；基于认证中心进行数据安全认证；通过调查中心对可疑交易进行人工审查，最终实现有效闭环，形成反欺诈一体化解决方案，实现对金融领域的全场景覆盖。</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94fc35f6d62fff845141be7c99c21d0e.png" /></p><p></p><p>团伙管理</p><p></p><p>通过图像识别技术可精准识别出用户是否命中了团伙，以及命中了哪个团伙。仅仅依赖系统的识别显然是不够的，在必要策略阶段，比如某个用户命中了一个新团伙，新团伙人数很小，那么就需要结合策略让人工介入案件调查，判断当前用户是否真实具有团伙性质，此时人工调查阶段辅助的工具是必不可少的。此外，团伙的管理，团伙性质定义—是否是“真正”的坏团伙，需要一个统一的团伙案件库来管理，便于分析团伙性质。对此，我们提供了“人工调查”和“团伙案件库”功能。</p><p></p><p>●&nbsp;人工调查</p><p></p><p>人工调查提供了快速查询用户信息的入口，用户信息包含“姓名”，“身份证号”，“活体图片”，“历史支用记录”等，为了保障用户信息的安全性，只有拥有较高调查权限的调查员才能查看用户的敏感信息。反欺诈调查专员可以通过“人工调查”页面快速概览用户信息，以此协助判断用户是否可能具有较高“风险”行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34978bd95f43dd931a8c7713b8666120.png" /></p><p></p><p>●&nbsp;团伙案件库</p><p></p><p>团伙案件库丰富了团伙的管理功能，结合图像识别团伙功能，反欺诈调查专员可以在页面上快速查看到当前团伙成员列表。除系统自动识别的团伙之外，调查员也可以手动添加人工识别到的风险用户到团伙成员列表中。通过完善对团伙案件的管理，调查员可以通过分析团伙的各项指标特征，来更全面地判断某团伙是否是真正的”坏“团伙。在最终的反欺诈策略集中，命中团伙黑名单后就会实时拒绝，以达到止损的目的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0d797ba09508c247feaeee4089228eb.png" /></p><p></p><p></p><p></p><h2>三、应用成果</h2><p></p><p></p><p>目前整套方案已经在某机构渠道完整进行落地，且业务效果显著：</p><p>●&nbsp;欺诈团伙识别准确率 80%+；</p><p>●&nbsp;背景识别准确率 99%+；</p><p>●&nbsp;累积减损数千万；</p><p>●&nbsp;有效识别团伙上百个，涉及团伙人数上万人；</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bAsmJdv5DmwM6P96cWA2</id>
            <title>【技研录】Zarm 3.0 正式发布！</title>
            <link>https://www.infoq.cn/article/bAsmJdv5DmwM6P96cWA2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bAsmJdv5DmwM6P96cWA2</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 09:10:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Zarm, 组件库, 移动端, 高质量的组件资产
<br>
<br>
总结: Zarm是众安科技基于React研发的一款适用于企业级的移动端UI组件库。它在移动展业下降低了页面开发成本，规范了产品的视觉交互，解决了浏览器兼容性问题，提升了用户体验。 </div>
                        <hr>
                    
                    <p>写作思考：</p><p>Zarm 是<a href="https://www.infoq.cn/article/PwGh8pYCfzIRf4X0EDgX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">众安科技</a>"基于 React 研发的一款适用于企业级的移动端 UI <a href="https://www.infoq.cn/article/6s2QNpu3EvR0LS5mHa3b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">组件库</a>"。<a href="https://www.infoq.cn/article/rVEVSjMwL6zRbqlLWCeH?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Zarm</a>" 项目中沉淀了一批高质量的组件资产，大大降低了移动展业下页面的开发成本，规范了产品的视觉交互，保障了用户体验的一致性；在移动设备系统版本碎片化严重的场景下，集中化的解决了浏览器兼容性问题。目前在众安系列移动端产品中有着广泛的应用，用户体验得到了整体明显的提升。</p><p></p><p>前言</p><p>随着前端技术的日新月异、React v18 的发布和自身业务的发展，基于 React 的移动端组件库 Zarm，升级势在必行，经过团队多月研发和打磨，在 2023 年春天开始的时候，Zarm 3.0 终于要和大家见面了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b74981913130b83cb90d5a4c9a696879.png" /></p><p></p><p>目录</p><p>一、新组件</p><p>二、体验升级</p><p>三、从能用到好用</p><p>四、主题多样化及定制能力</p><p>五、未来</p><p>六、最后</p><p></p><p></p><h2>一、新组件</h2><p></p><p></p><p>在新版本中，我们增加了 10+ 全新的组件，它们是基于我们自身的业务实践而开发。这些组件经过严谨的推敲，确保了其通用性和扩展性，相信它们将为用户带来更好的使用体验。</p><p></p><p>Skeleton 骨架屏：在界面等待加载区域展示占位图形。</p><p>WaterMark 水印：展示页面版权所有者信息，内容泄露后以便追溯。</p><p>Grid 宫格：在水平方向上把页面分隔成等宽度的区块，用于展示等宽内容或进行页面导航。</p><p>Rate 评分：对事物进行评级操作，丰富了表单交互的形式</p><p>Image 图片：提供 5 种图片填充模式，支持懒加载、加载中/加载失败占位和回调</p><p>......</p><p></p><p>以及提供了 5 个常用的 hooks ，帮助各位开发者解决常见交互问题。</p><p></p><p>useClickAway 单击外部跟踪器useInViewport 进入浏览器窗口useLongPress 长按useOrientation 屏幕方向useScroll 滚动</p><p></p><p></p><h2>二、体验升级</h2><p></p><p>&nbsp;</p><p>在 3.0，我们进行了一次全面的组件重构，将所有组件从类组件重写为函数式组件，这个变化将为开发者们带来更好的使用体验和更高的性能表现。</p><p></p><p></p><h4>01 手势交互细节</h4><p></p><p></p><p>我们使用了&nbsp;use-gesture，它提供了更加灵敏、可靠的手势识别能力，让用户可以更加自然地操作，提高用户的交互体验。</p><p></p><p></p><h4>02 流畅的动画</h4><p></p><p></p><p>目前我们使用了&nbsp;react-transition-group&nbsp;作为动画库，它使用了一些优化手段，可以在保证动画流畅性的同时，最大限度地减少性能开销，为应用带来更加生动、流畅的动效。</p><p></p><p></p><h4>03 视觉升级</h4><p></p><p></p><p>同时，我们结合了&nbsp;IOS 16 组件库设计资源，对部分组件的 UI 细节进行了调整，保持了组件视觉的整体一致性和整体美观度，让用户可以享受到更加优秀的视觉体验。</p><p></p><p></p><h2>三、从能用到好用</h2><p></p><p></p><p>我们还优化了组件的设计和 API，以更好地满足开发者们的需求</p><p></p><h4>01 指令式调用</h4><p></p><p></p><p>首先，我们对所有弹层交互组件增加了指令调用方式，这样就不再需要添加大量的代码来实现弹层的状态管理，只需通过简单的指令式调用，就可以在任何地方轻松使用弹层组件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/05db9c63f1ec7159510dfcf62919fe76.png" /></p><p></p><p>支持指令式调用的组件：</p><p>ModalPickerDatePickerActionSheetCascaderToast</p><p></p><p></p><h4>02 挂载监听节点的全局配置</h4><p></p><p></p><p>其次，我们提供了全局配置统一管理组件默认的挂载节点与滚动监听节点，解决微前端挂载节点变更的需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a960dce7b9c39ae4b1a70f5f2006d12.png" /></p><p></p><p></p><h4>03 日期组件</h4><p></p><p></p><p>另外，我们对日期组件也进行了改进，移除了内置预设模式，调整为更加灵活的时间类型列进行配置，并且增加了“周”时间类型和 12 小时制。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa58d8ea7290b5a66d4122a7fde1c58d.png" /></p><p></p><p>此外，还增加了对时间类型值过滤的 API。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a0f21d17cbe1b5789f6b8c4428604fe.png" /></p><p></p><p></p><h4>04 全面拥抱 TypeScript</h4><p></p><p></p><p>我们将 TypeScript 应用于整个组件库和组件样式，并且将这些类型进行了导出，这意味着我们可以为开发者提供更好的类型安全和编辑器支持，以及更容易防止一些潜在的错误，帮助我们更好地构建可靠的应用程序。&nbsp;</p><p></p><p></p><h4>05 组件拆分和API整合</h4><p></p><p></p><p>我们把在 2.x 一些设计不合理的部分组件做了一些调整，为了更符合功能含义、便于记忆、使用预期更明确。</p><p></p><p>例如：</p><p>CustomInput 和 Input 分离ActivityIndicator 重命名 LoadingStackPicker 重命名 Cascader，并且动画和交互调整为 Tabs + Radio</p><p>......</p><p></p><p>总而言之，这些变化将帮助开发者们更加轻松地实现他们的项目需求，并带来更好的用户体验。</p><p></p><p></p><h2>四、主题多样化及定制能力</h2><p></p><p></p><p></p><h4>01 CSS Variables 动态主题</h4><p></p><p></p><p>在过去的版本中，Zarm 提供了一套默认的主题样式，开发者只能在编译时甚至是样式覆盖对默认主题进行微调，但是，对于一些需要更加动态的主题变化的应用程序来说，这些静态的主题样式可能无法满足需求。现在，我们提供了全新的动态主题功能，并且提供了几种不同的方式来动态地改变 Zarm 默认主题。</p><p></p><p>全局配置</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0134d04e0bea87f13dd07c23c35f7fb7.png" /></p><p></p><p>组件内联</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea2b087135ecb25f50061baf693f8806.png" /></p><p></p><p></p><h4>02 自定义内容渲染</h4><p></p><p></p><p>在新版本中，除了 Checkbox/Radio 预设样式外，我们提供了显示元素的自定义渲染函数，开发者可以根据实际的业务场景定制开发。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/216c0cd223773fa4239d2bf410ea228a.png" /></p><p></p><p>不仅如此，我们同样开放了 Keyboard 源数据自定义的能力，这个功能非常适合那些需要特定的业务场景，比如车牌键盘等等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e029676a6b6ffa73170c89859df8694d.png" /></p><p></p><p></p><h2>五、未来</h2><p></p><p></p><p>总的来说，Zarm 3.0 带来了许多改进和新特性，使得 Zarm 变得更加易于使用和定制，后续我们计划提供更多的组件，在将来我们也会在 CSS in JS、headless UI 做更进一步的探索。并且在满足移动端 Web 开发基础上，我们也在积极的发现更多平台的适配问题，比如小程序。</p><p></p><p></p><h2>六、最后</h2><p></p><p></p><p>对于还在使用 Zarm 2.x 或更早的版本，我们也准备了完善的<a href="https://zarm.design/#/docs/migration-v3">迁移指南</a>"。</p><p></p><p>使用者的反馈是我们不断前进的动力。大家在使用过程中遇到任何问题，都可以在&nbsp;<a href="https://github.com/ZhongAnTech/zarm/discussions">https://github.com/ZhongAnTech/zarm/discussions</a>"或者微信群交流。扫二维码加好友备注 “zarm”&nbsp;进群。</p><p></p><p>最后感谢社区同学参与 Zarm 3.0 的开发：faner11、tgioer、jiyingzhi、JunIce、nemoisme</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/304176e482449c6475134e1f026ccaef.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jeYjxHBKaKbzkDGll9Lw</id>
            <title>【技研录】响应式编程在 Swift 中的使用</title>
            <link>https://www.infoq.cn/article/jeYjxHBKaKbzkDGll9Lw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jeYjxHBKaKbzkDGll9Lw</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 08:59:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 响应式编程, 数据流, 变化传递, Swift
<br>
<br>
总结: 响应式编程是一种以数据流和变化传递为核心概念的编程范式，在Swift中得到广泛应用，可以简化代码结构，提高程序的可读性和可维护性。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/94/94351f5b79c970d8c0fd937516e7e9cf.png" /></p><p>写作思考：</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ%3D%3D&amp;chksm=80b78e6cb7c0077aaf10ae04c56a7c5c181b79d960e9cff9385744896e8f0be8c4d2138426ef&amp;idx=1&amp;mid=2650823410&amp;scene=27&amp;sn=dfe80ee133db7bba43ed6d47579057bd&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">响应式编程</a>"（Reactive Programming）是一种编程范式，它以数据流和变化传递为核心概念，可以简化代码结构，提高程序的可读性和可维护性。在 <a href="https://www.infoq.cn/article/apple-swift-nio?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Swift</a>" 中，响应式编程已经得到了广泛的应用，并成为了一些流行框架的基础，开发者可以通过框架提供的操作符对数据流进行各种变换。目前在<a href="https://www.infoq.cn/article/PwGh8pYCfzIRf4X0EDgX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">众安科技</a>"经代 App 中全面使用，整体代码逻辑更加清晰，开发效率上得到了明显的提升。</p><p></p><p>目录</p><p>1.什么是响应式编程</p><p>2.函数式 Swift</p><p>3.MVVM</p><p>4.MVVM 与响应式结合</p><p>5.总结</p><p></p><p></p><h2>一.什么是响应式编程</h2><p></p><p>&nbsp;</p><p>Wiki 上的解释: Reactive programming 是一种面向数据串流和变化传播的声明式编程范式。</p><p>iOS 客户端的原生开发使用 Objective-C 和 Swift 开发，使用 Objective-C 的时候注重面向对象编程，大多数都是使用命令式的编程，Swift 更注重面向协议编程、函数式编程。</p><p>&nbsp;</p><p>做过 iOS 客户端的同学一定了解过 KVO 这个内置的 Api，KVO 可以帮助我们将属性的变更和变更后的处理分开，简单的理解就是一个对象的属性改变后，另外一连串对象的属性都随之发生改变。KVO 的写法和使用上比较复杂，而且只支持 NSObject ，局限性太大。&nbsp;</p><p>&nbsp;</p><p>Apple 在推出 Swift 之后，响应式的编程基于数据流的理念，异步的处理事件和函数式编程能很好的结合，ReactiveX 推出了响应式的库 RxSwift，WWDC 2019 上 Apple 公布了声明式全新界面框架 SwiftUI，以及配套的响应式编程框架 Combine，Combine 只支持 iOS13 以上的系统，毕竟属于原生的框架，在性能上要稍微强于 RxSwift，根据支持的版本差异开发者可以自行选择。&nbsp;</p><p>&nbsp;</p><p>最新推出Rx的微软对响应式编程的解释是 Rx = Observables + LINQ + Schedulers，通俗一些的解释就是面向异步数据流编程。数据流可以有多种形式，比如读取一个文件、进行一个网络请求、用户出发的行为等等，都可以认为是一种数据流，当然一个变量也可以认为是一种数据流。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><p></p><h2>二.函数式Swift</h2><p></p><p>&nbsp;</p><p>函数式编程中的函数这个词不是指计算机中的函数，而是指数学中的函数，即自变量的映射。也就是说一个函数的值仅决定于函数参数的值，不依赖其他状态。比如 sqrt(x) 函数计算 x 的平方根，只要 x 不变，不论什么时候调用，调用几次，值都是不变的。</p><p></p><p></p><h4>1. 一等公民函数与高阶函数</h4><p></p><p></p><p>在函数式编程中，函数是一等公民，不再把函数想象成一个处理过程，而是把它当作一个对象或者变量来对待。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1d/1da7cb9e23935f966723b9a8f49f8481.png" /></p><p>&nbsp;</p><p>在 Swift 中可以很方便的把一个函数赋值给一个常量，这个在 Objective-C 中是做不到的。所谓的高阶函数，指可以将其他函数作为参数或者返回结果的函数，Swift 中的函数都是高阶函数，系统库提供了(map,fillter,reduce等)</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/5c/5ccbef347d5d42d42bcf12c013e6eb98.png" /></p><p></p><p>通过函数这个“管道”，数据从一头经过“管道”到另一头，就得到了想要的数据。</p><p>&nbsp;</p><p></p><h4>2.柯里化</h4><p></p><p>&nbsp;</p><p>Swift 里可以将方法进行柯里化 (Currying)，这是也就是把接受多个参数的方法进行一些变形，使其更加灵活的方法。函数式的编程思想贯穿于 Swift 中，而函数的柯里化正是这门语言函数式特点的重要表现。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ec/ec4bc291ae41b9aa74bf635b9be1ecd8.png" /></p><p></p><p>柯里化是一种量产相似方法的好办法，可以通过柯里化一个方法模板来避免写出很多重复代码，也方便了今后维护。</p><p>&nbsp;</p><p></p><h4>3.闭包</h4><p></p><p>&nbsp;</p><p>闭包是一个会对它内部引用的所有变量进行隐式绑定的函数。也可以说，闭包是由函数和与其相关的引用环境组合而成的实体。函数实际上是一种特殊的闭包,你可以使用{ }来创建一个匿名闭包。使用 in 来分割参数和返回类型。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f3/f360d0bd139977e3bf028fdce32d2f49.png" /></p><p></p><p>上面 map 函数遍历了数组，用闭包处理了所有元素，并返回了一个处理过的新数组。</p><p>那么遵循以上特性，一个好的 Swift 函数式程序会具有一下特质:</p><p>&nbsp;</p><p>&nbsp; ● 模块化:相较于把程序认为是一系列赋值和方法调用，函数式开发者更倾向于强调每个程序都能够被反复分解为越来越小的模块单元，而所有这些块可以通过函数装配起来，以定义一个完整的程序。</p><p>&nbsp;</p><p>&nbsp; ● 对可变状态的谨慎处理:面向对象编程专注于类和对象的设计，每个类和对象都有它们自己的封装状态。然而，函数式编程强调基于值编程的重要性，这能使我们免受可变状态或其他一些副作用的困扰。通过避免可变状态，函数式程序比其对应的命令式或者面向对象的程序更容易组合。</p><p>&nbsp;</p><p>&nbsp; ● 类型:一个设计良好的函数式程序在使用类型时应该相当谨慎。精心选择你的数据和函数的类型，将会有助于构建你的代码，这比其他东西都重要。Swift 有一个强大的类型系统，使用得当的话，它能够让你的代码更加安全和健壮。</p><p>&nbsp;</p><p>在实际开发过程中，我们遵循函数式思维去编码，可以很容易地使用 Swift 写出函数式风格的代码，为我们带来以下好处:</p><p>&nbsp;</p><p>1. 方便组件解耦</p><p>2. 单元测试和调试都更容易</p><p>3. 更方便的代码管理</p><p>&nbsp;</p><p></p><h2>三.MVVM</h2><p></p><p>&nbsp;</p><p>MVVM 可以说几乎就是一个 MVC，不过通过 View Model 层来将数据和视图进行绑定。熟悉 iOS 开发的小伙伴都知道，iOS 的 Cocoa 框架都是基于 MVC 设计的，关于 MVC，我们可以看下斯坦福的 CS193p Paul 这张经典图</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5aa9e44dc0496d8d8e1f2068435f309e.jpeg" /></p><p></p><p>MVC 本身的概念相当简单，同时它也给了开发者很大的自由度。Massive View Controller 往往就是利用了这个自由度，“随意”地将逻辑放在 Controller 层所造成的后果，此时的 M 不是 Model 已经变成了 Massive。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dff6d7ae81ca548f482c1b31f6918d3.jpeg" /></p><p></p><p>使用 MVVM 之后可以大大减轻 View Controller 职责，简化后的各个模块分工更加明确，更加方便集成和开发。我们在使用的时候主要遵循一下几个事项:</p><p>&nbsp; &nbsp; ● &nbsp;ViewController 尽量不涉及业务逻辑，让 ViewModel 去做这些事情，此时的ViewModel实际上的职责是Controller</p><p>&nbsp;&nbsp;&nbsp; ● ViewModel 绝对不能包含视图 View（UIKit.h），不然就跟 View 产生了耦合，不方便复用和测试</p><p>&nbsp;&nbsp;&nbsp; ● iewModel避免过于臃肿，否则重蹈Controller的覆辙，变得难以维护</p><p>&nbsp;</p><p>MVVM 是 MVC 的升级版，完全兼容当前的 MVC 架构，MVVM 虽然促进了 UI 代码与业务逻辑的分离，一定程度上减轻了 ViewController 的臃肿度，但是 View 和 ViewModel 之间的数据绑定使得 MVVM 变得复杂和难用了，如果我们不能更好的驾驭两者之间的数据绑定，同样会造成 Controller 代码过于复杂，代码逻辑不易维护的问题。</p><p></p><p></p><h2>四.MVVM与响应式结合</h2><p></p><p>&nbsp;</p><p>上面介绍完 MVVM，那应该怎么和响应式结合起来呢？我们先来看下在 iOS 中是怎么进行状态更新的</p><p>&nbsp; ● Target-Action</p><p>&nbsp; ● Delegate</p><p>&nbsp; ● KVO</p><p>&nbsp; ● Notifications</p><p>&nbsp; ● Callback</p><p>&nbsp;</p><p>多种回调方式，适用规则、适用场景都不相同，这增加了开发、维护的难度。如果有一种方式可以把状态更新做到统一，那就可以大大提高开发效率，这里就要提到 Rx 了，它把状态变更都转化成流，MVVM 中的 ViewModel 和 Rx 相结合这样就可以做到响应式了。</p><p>&nbsp;</p><p>先来看个例子，一个登录界面</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/421b3a78785ff0acd4bf059c2c6784dd.jpeg" /></p><p></p><p>产品经理说了需求：</p><p>&nbsp; ●&nbsp;账号是手机号11位</p><p>&nbsp;&nbsp;●&nbsp;密码大于6位</p><p>&nbsp; ●&nbsp;当满足上面条件，点击统一条款，下面登录按钮为可点击状态否则不可点击</p><p>&nbsp;</p><p>按照一般思路，2 个输入框对应的是 2 个 UITextField 控件，还有 2 个 UIButton 控件对应单选按钮和登录按钮，我们需要实现这个需求就要做到以下:</p><p>&nbsp; 1. 在 UITextField 的 Delegate 里面去监听输入的内容</p><p>&nbsp; 2. 在 UIButton 的 Action 方法里面监听隐私条款是否点击</p><p>&nbsp; 3. 2 个输入框和 1 个隐私按钮，这3个控件每次更新状态都要去看另外 2 个是否满足登录按钮可点击</p><p>&nbsp;</p><p>可以见到这样一个简单的需求，在代码实现上要处理的逻辑很多，而且都是分散的，那么我们能不能只罗列条件，然后把这些条件扔给一个条件处理的机制，这个机制就能帮我们正确的处理这些关系？</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/8c/8c731c4e41e94c9423613714b6809f11.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p>伪代码如下:</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f404cfb090a71e5e8d73e381f8bd86a2.png" /></p><p>&nbsp;</p><p>实际处理中会把这些逻辑放到 ViewModel，这样就把 RxSwift，函数式，MVVM 结合起来，达到了响应式编程的样子。</p><p>&nbsp;</p><p></p><h2>五.总结</h2><p></p><p>&nbsp;</p><p>Swift 从 2014 年发布，到 iOS13 推出的响应式 Combine，开源也一直推动语言的发展，可能未来 1 天苹果会抛弃原来 Cocoa 框架实现，利用新语言的特性也不是不可能。任何架构和技术都不能满足所有的工程需求，能够使用简单的架构来搭建复杂的工程，制作出让其他开发者可以轻松理解的软件，避免高额的后续维护成本，让软件可持续发展并长期活跃，应该是每个开发者在构建软件是必须考虑的事情。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考资料:</p><p>&nbsp;</p><p>关于 MVC 的一个常见的误用iOS 下的响应式编程</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s0y4orJmabSBw0uotrK5</id>
            <title>大模型深入智慧之地，手机厂商会如何交卷？</title>
            <link>https://www.infoq.cn/article/s0y4orJmabSBw0uotrK5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s0y4orJmabSBw0uotrK5</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 08:15:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI浪潮, 大模型, 预训练模型, AndesGPT
<br>
<br>
总结: 在AI浪潮中，企业追逐大模型是基于真实的业务需求，而不仅仅是跟风效仿。预训练模型的出现使得处理长尾问题更加准确。OPPO推出的自主训练的AndesGPT大模型，通过对话增强、个性专属和端云协同等核心技术特性，提供更精准、流畅和个性化的智能助手服务。 </div>
                        <hr>
                    
                    <p>受访嘉宾 | OPPO 数智工程事业部总裁 刘海锋</p><p>作者 | 罗燕珊</p><p></p><p></p><h3>一条新路出现：跟风与乘势？大模型浪潮下的未见与先见</h3><p></p><p></p><p>在蓬勃发展的 AI 浪潮中，各行各业似乎都在积极投入大模型。由此我们更想探讨，企业之所以追逐大模型，是基于真实的业务需求，还只是跟风效仿？</p><p></p><p>我们试图着眼于大模型风起云涌的智能手机领域，从一位探索者的实践里找寻答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03ecd40d285bcdb8496fe888ebf9fe2c.png" /></p><p></p><p>在 2023 年之前，刘海锋在 OPPO 所带领的团队除了承担 OPPO 云、大数据、推荐搜索、互联网安全的技术工作，还专注于打造小布助手这款个人 AI 产品，并不断迭代升级技术栈。</p><p></p><p>从最初使用垂域数据和 FAQ 进行检索，到引入神经网络提升长尾问题应答能力，实际上过去这些年，对话系统也逐渐形成了一套标准的架构范式，包括语音识别、意图理解、槽位填充、对话管理和生成等等。这种架构成为全行业的标准范式，在包括Apple、Google在内的大公司里已经稳定运行多年。</p><p></p><p>然而，自从 Transformer 模型问世后，许多公司开始尝试预训练方法，以便更准确地理解和回答一些长尾问题。</p><p></p><p>2020 年，OPPO 内部的认知计算部门已启动预训练语言模型的探索与实践。据 InfoQ 近日与刘海锋交谈了解，当时做预训练模型的初衷是应对那些无法通过常规方法回答的长尾问题，或者处理一些比较自由的、多轮的闲聊对话。</p><p></p><p>“那时候预训练模型并非承担主力任务的角色，大约有 5%～10% 的问题可能会被引导到它这儿，它更多是做一个补充，做深层次的问答。”</p><p></p><p>然而，这样的技术栈也带来了现实的问题，整个系统变得相对复杂，包含多路问答的解决方案，却没有完美地解决所谓的对话问题，且距离认知智能还很遥远。</p><p></p><p>在这个背景下，OpenAI 蹚出了一条新的路线，通过 ChatGPT 这样的产品，展示了用更简单的架构、利用大模型来生成所有问题答案的可行性。</p><p></p><p>因此，时至今日，对手机及智能终端厂商而言，大模型已经不是选择题，而是一道必答题，大家纷纷探索如何通过大模型让设备里的“智能助手”变得更智能，因为这很有可能成为未来终端厂商的核心竞争力之一。</p><p></p><h3>视线定义路线：那些“这里面一定有金子”的时刻</h3><p></p><p></p><p>11 月 16 日，在 <a href="https://www.infoq.cn/article/2uKaRRpLwAwdmgOm5mIJ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OPPO 2023 开发者大会</a>"上，刘海锋宣布 OPPO 正式推出自主训练的安第斯大模型 （AndesGPT）。</p><p></p><p>实际上，在今年春节后，刘海锋便组织团队迅速推进 AndesGPT 项目，目前已迭代了多个版本。OPPO 主要训练了三大类规格的模型——AndesGPT-Tiny、AndesGPT-Turbo 和 AndesGPT-Titan，涵盖十亿至千亿以上多种不同参数规模的模型规格，灵活满足不同应用场景的需求。</p><p></p><p>刘海锋强调，无论是做 AI 大模型，还是任何系统和产品，其核心特性和功能通常是由公司对用户的主张、过去产品的经验积累等方面所决定，而不是凭空定义。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1bfb634b3cb972a263a56a3d00d99250.png" /></p><p></p><p>而对于 OPPO 而言，AndesGPT 更多源自智能语音助手“小布”的积累，其应用载体也主要是小布助手这一 C 端产品。据悉，近 5 年来，在构建月活过亿的 C 端产品小布助手的过程中，OPPO 已经完成了大规模高质量语料数据的积累。</p><p></p><p>基于此，OPPO AndesGPT 的三个核心技术特性分别为：对话增强、个性专属和端云协同。</p><p></p><p>对话增强：由于智能助手主要通过对话进行交互，因此强化“大模型知识问答的精准性”、优化“对话交互的流畅和自然度”是首要特性。个性专属：OPPO 是一家面向消费者的公司，强调为每个用户个体提供有用的大模型和智能应用。因此，与服务企业的大模型不同的是，AndesGPT 更关注满足个人用户的需求。这也导致其在技术和产品路线上与 ToB 的大模型存在差异，OPPO 要求模型是有状态的、能够记忆用户过去的交互历史、了解用户的偏好和兴趣，以提供更好的个性化服务。端云协同：由于大模型主要托管在云端，对智能终端产业来说，在技术上实现端云协同非常重要。OPPO 的方案是把小规格的模型放在手机上运行，让一些应用即使在断网的情况下也能使用。</p><p></p><p>核心能力方面，AndesGPT 聚焦在四个方向：知识、记忆、工具、创作。</p><p></p><p>在知识能力方面，AndesGPT 融合了知识图谱及通用搜索能力，为用户提供更专业的问答。通过知识增强技术，将外部知识与模型融合生成结果，降低幻觉。在记忆能力方面，AndesGPT 实现长期记忆机制，以支持无限长度的上下文和有状态服务。工具使用能力上，AndesGPT 能够更好地理解设备控制与服务 API，端到端生成可执行指令。目前 AndesGPT 已支持使用“系统设置、一方应用、三方服务、代码解释器”等各类工具。在创作方面，AndesGPT 已全面支持文生文、文生图与图生图场景，且积极尝试音乐生成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/edee51603f99dacc25b28368f0b29376.png" /></p><p></p><p>“虽然大模型和产品在现阶段仍有不足，也经常会遇到 bug，但团队致力于不断迭代和改进。”刘海锋表示，目前在大模型的实际应用上，更多的还只是“迈出了第一步”。</p><p></p><h4>&nbsp;独家技术创新 SwappedAttention</h4><p></p><p></p><p>事实上，OPPO 团队在两年前就开始对预训练语言模型进行探索和落地应用，自研了一亿、三亿和十亿参数量的大模型 OBERT。</p><p></p><p>目前业内模型训练中主要面临的挑战有两个：效率和成本。效率方面涉及如何在有限的资源内高效进行模型训练，而成本方面则关注如何最大程度地发挥每颗 GPU 的价值并降低训练的总成本。</p><p></p><p>刘海锋表示，受益于团队此前在“智能推荐”、“小布助手”场景里积累的经验，以及在大规模系统架构、云计算以及分布式系统方面的积累，使得 AndesGPT 项目的 Infra 优化有经验可循，再借助混合云架构，灵活解决算力资源瓶颈问题。</p><p></p><p>训练大模型只是整个过程的第一步，关键在于将其应用到实际产品中，解决性能和效果方面的问题，形成一个持续迭代的闭环。</p><p></p><p>那么，AndesGPT 如何进行创新和演进，如何变得更好？刘海锋以“记忆”能力为例展开说明，比如安第斯大模型“一定要提供 stateful API”，要提供无限的上下文能力。</p><p></p><p>但实践过程中，团队发现面临这样一个问题，即当上下文越来越长时首次推理的延迟很高。为解决长时记忆带来首字推理延迟的技术挑战，他们把一些先进的方法应用上，比如 FlashAttention 和后来的 PagedAttention（由加州大学伯克利分校提出）。如此一来，模型的应用性能是有得到提升，效果也有改善。</p><p></p><p>但这样还不够。刘海锋告诉 InfoQ，他觉得这里面还有更多的发掘空间，他当时跟团队说：“这里面一定有金子。”</p><p></p><p>在 PagedAttention 的基础上，OPPO 做了技术的演进和扩展，自主研发了长时记忆机制并命名为 SwappedAttention。长时记忆主要包括用户交互过程中产生的交互历史、个人数据，以及从中提取的结构化信息等。</p><p></p><p>具体而言，SwappedAttention 是通过将内存空间与计算交换、缓存历史的键值对（KV 值）的思路，来大幅减少首字计算量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2fa3ff1d9251f201b023ffcc82dc6846.png" /></p><p></p><p>同时，SwappedAttention 采用了多层级缓存机制，涵盖了 GPU 显存、主机内存以及通过 GDS（GPU Direct Storage）连接外部存储设备。根据缓存时长、对话频率等策略进行分级存储与交换，以最优化资源利用。</p><p></p><p>实现场景方面，以处理多轮对话场景为例，随着聊天轮数累积，SwappedAttention 能有效减少对话时首字推理时间，用户可获得更快的系统响应。</p><p></p><p>而 FileChat 文档对话场景中，可以避免长文本的 KV 值进行重复计算，大量减少计算开销，缩短首字推理时间，并且可提升首字计算的并发度。在非首字的推理过程中，SwappedAttention 可以动态压缩 KV 值，进一步降低显存占用，提升整体吞吐量。</p><p></p><p>刘海锋表示，上述创新思路其实还是从经典的计算机科学里获得的启发。</p><p></p><p>“既然 GPU 里可以使用以 page 为单位的 cache，那么一定可以做以会话 /session 为粒度的 cache。既然这个 cache 可以存放在 GPU 内，那也可以将其交换到外部，交换到服务器的内存以及后台分布式系统的内存中。”他进一步解释，这个思路类似于计算机科学中存储层次的原理，有小 cache 就有大 cache，有 L1 cache 就有 L2 cache、L3 cache，它们不矛盾，可以叠加使用。</p><p></p><p>通过更大的缓存，结合 PagedAttention 算法一起使用，SwappedAttention 最终能够带来 50% 的首字延迟降低，以及 30% 的推理吞吐提升。</p><p></p><p>毋庸置疑，于大模型研究者而言，优化推理性能是一个需长期解决的问题。刘海锋表示，尽管有了 FlashAttention、PagedAttention 以及 SwappedAttention 等方法，实际上后续还会涌现一系列具有弹性的算法优化机制，以进一步降低推理延迟。</p><p></p><h4>&nbsp;端云协同</h4><p></p><p>目前，基于 AndesGPT 全新升级的小布助手，已进一步强化端云协同能力，在终端和云端分别部署 Tiny 和 Turbo 模型，根据使用场景和网络状态做智能分流。</p><p></p><p>刘海锋指出，端云协同，实际上是大模型促使原来智能助理的应用架构发生了改变。具体实现上，是在手机侧部署小模型（AndesGPT-Tiny）、云端部署大模型，然后由智能助理应用程序根据实际情况调用不同的模型。</p><p></p><p>最初 OPPO 在生产环境使用的模型参数规模大约在十几 B 左右，后来团队发现模型在涌现性方面相对逊色。进行了几次升级之后，目前云上使用的是 70B 的模型（AndesGPT-Turbo），运行效果良好。与此同时，内部还在推进千亿级别参数规模的模型优化（AndesGPT-Titan），目前还没有推向线上。</p><p></p><p>“我们必须承认所谓大的语言模型，它的核心特性叫做智能涌现。你要想让模型获得涌现能力，参数规模必须要足够大。”刘海锋强调道，今天行业内的一个基本共识是，要达到智能涌现的特性，模型的参数规模可能需要达到 50~70B 才行。因此，如果一些问题是需要智能涌现能力才能解决，那就必须使用云端部署的大模型。</p><p></p><p>对于模型调用的流量分配，刘海锋表示主要是根据用户场景做判断，比如某个应用总是执行固定的任务，或者处于断网的情况下，一般就会调用设备侧的小模型。再比如某个降级访问的问题，或者是有限场景下的使用，那么在终端侧部署的小模型也是可以解决的。</p><p></p><p>关于大模型在实际应用中的权衡和选择，刘海锋表示还涉及到功耗、内存等多方面的问题考虑，也是目前业内同行都在积极探索的热点。</p><p></p><p></p><h3>颠覆已然到来：大模型要把底层翻新，基础设施面对挑战</h3><p></p><p></p><p>整体来看，如今融入 AI 大模型的新一代智能助理产品给基础设施带来了不少新的挑战。</p><p></p><p>刘海锋指出，与之前大规模的 Web 应用架构相比，大模型的技术堆栈变得更为复杂。过去的架构比较“规整”，可以分为存储、离线数据处理、在线缓存和数据库等等层次，规模大了后也可以采用分布式系统架构，以“scale out”方式横向拓展。</p><p></p><p>但是对于今天的大模型和智能体应用来说，GPU 的需求变得极高，GPU 取代了 CPU 成为核心计算单元，这使得许多软件问题都需要围绕 GPU 来看。</p><p></p><p>其次，大模型应用的赋能也分不同的阶段来看。首先是训练阶段，对 GPU 的选择、网络基础设施和整体容错能力都有很高的要求。例如，在训练过程中需要定期有 checkpoint，将参数写入底层存储。这对存储系统提出了新的挑战，需要定制一套系统来处理从 GPU 加载数据到分布式内存存储的问题。</p><p></p><p>第三，一旦模型训练完成并推送到线上，就需要优化推理性能，以实现更有效的 GPU 利用。如前文所述，推理优化也是一项长期工作。</p><p></p><p>第四，在推理之外，模型的外围系统比如检索增强也需要格外重视和持续构建。刘海锋进一步表示，向量数据库是目前比较熟悉的一种解决方案，但随着模型上下文窗口的增大和模型能力的提升，他认为对于向量数据库的依赖可能会降低，也许会催生出新的系统架构，更易于使用或功能更强大，以弥补大模型本身的不足。</p><p></p><p>最后，他认为前端应用也会有较大变化，因为前端应用可能需要接多个模型，需要考虑多个模型的分流，这涉及到整体的判断逻辑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a427381a2a8871fe136b908dd6feb17.png" /></p><p></p><p>总的来说，刘海锋认为大模型时代使得软件架构以 CPU 为中心变成以 GPU 为中心，从训练到推理，再到增强和前端应用，不同环节的技术架构都与移动互联网时代有着显著的不同之处。伴随这波浪潮而来的是，许多技术挑战都将变得更加硬核。</p><p></p><p></p><h3>倒逼下的改革：从技术的研发方式，到人的组织方式</h3><p></p><p></p><p>随着 AndesGPT 项目的启动，刘海锋对团队的人力投入和资源配置进行了调整。目前，内部的大模型团队主要由以下团队构成：</p><p></p><p>模型竞争力团队：负责模型训练和调优。智能助理团队：也最核心的大模型应用产品的团队，专注于开发和改进智能助理应用，与模型竞争力团队协同工作，相互挑战和共同进步。应用 +AI 团队：使用安第斯大模型的 SDK，将其应用于公司已有的应用中，不限于智能助理，还要扩展到其他 C 端应用或孵化新应用，使终端产品更加智能。企业 +AI 团队：也叫企业智能化团队，主要将安第斯大模型应用于企业内部，涵盖的场景包括编码、数据分析、营销等，以提高效率和智能化。AI 安全团队：负责整体泛 AI 安全，包括数据安全、隐私保护以及更广泛的伦理和价值观等多维度的安全问题。</p><p></p><p>值得一提的是，今年春季后，刘海锋迅速成立了“安第斯 AI 安全实验室”，专注于为大模型和人工智能提供安全保障。其指出，大模型的安全性与保障应用程序安全的工作有很大的不同。</p><p></p><p>传统的应用安全工作主要关注 App 是否恶意，是否存在滥用行为等问题。然而，大模型和 AI 的安全性问题涉及到另一个维度，比如生成的内容是否包含敏感信息以及是否符合价值观等方面的考量。</p><p></p><p>同时，大模型安全需要处理的数据量也更大了。原来的“移动互联网安全”主要处理用户输入的数据，例如上架的应用、搜索查询等。然而，对于生成式 AI 的安全性而言，不仅需要处理输入，还需要关注输出。而输出的数据量通常比输入大一到两个数量级，对返回的内容都需要进行额外的处理工作。</p><p></p><p>因此，AI 安全，尤其是生成式 AI 的安全性，也是接下来技术工作者需要重点解决的一个难题。</p><p></p><p>除了安全挑战之外，随着大模型项目的推进，刘海锋对于人才能力挑战的问题深有体会。他认为，首先，新一波 AI 浪潮对产品经理能力提出了更高的要求。现代 AI 产品往往是技术驱动的，因此 AI 产品经理需要更“懂技术”才能进行高质量的决策和判断。</p><p></p><p>其次，对于研发同学，单纯懂算法是不够的，还需要对算法和架构有深入的理解。AndesGPT 团队成员要么是既熟悉算法又懂工程和架构，要么是双方能够很好地协同工作。</p><p></p><p>由于大模型存在许多不确定性，其可解释性还是个开放的问题。因此一个新的模型上线后，不能用单一指标来对整体效果做评判和反馈，且评测周期长，维度也比较复杂。刘海锋认为，大模型测试领域可能会出现新的技能和岗位需求。</p><p></p><p>为了适应新兴技术和需求，OPPO 会在实践的过程中灵活调整团队，比如尝试让技术研发同学担任产品负责人的角色等。“我觉得在新的时代，研发同学的组织方式也会有一些变革，它会跟原来不太一样。”</p><p></p><p></p><h3>深入智慧之地：未来，终端设备里的 AI 将无处不在</h3><p></p><p></p><p>据悉，为了更好地促进大模型及智能体生态发展，AndesGPT 后续将开源智能体框架，便于开发者打造自己的智能体。</p><p></p><p>此外，AndesGPT 团队也在开发一款个人知识管理智能体（简称 PKA）。用户可以把日常工作生活中阅读的文章、文档、文件和笔记上传存储分析， 结合大模型的理解和记忆能力，PKA 就能成为用户的个人知识管家——快速回答问题，这些问题可以是对某个知识点的询问、某篇文章的总结，甚至全局问答。</p><p>刘海锋进一步表示，智能体不需要额外安装，而是嵌入到小布助手当中，并分为不同的频道，每个频道涵盖不同的话题或服务，这有点类似于互联网门户时代的频道划分。OPPO 希望开发者和终端用户都能够创作或定义智能体，并互相分享有趣和实用的智能体，推动生态的创新和互动。</p><p></p><p>今天，互联网大厂和行业巨头纷纷布局自己的大模型，有些还提供公用的服务。但与此同时，许多追求业务领先的企业也会训练自己的模型，在刘海锋看来，这是一个互补的关系，如同混合云架构，它们并不是互相取代的关系。</p><p></p><p>“如果你用云计算的视角，你可以把大模型看成是一朵 AI 的云，我们还是把高质量的知识编码到这朵云里，然后它去做高性能的推理。所以，从这个角度上看，我觉得很多东西在技术上可能属于不同的话题，但基本的原理实际上是高度一致的。”</p><p></p><p>未来，刘海锋认为生成式大模型会成为终端厂商的核心产品和技术竞争力，与“拍照”、“自有 OS”并驾齐驱成为三条核心技术赛道。进一步地，智能手机会成为真正的人工智能手机，而 AI 则会无处不在，作为“一个智慧的外脑”嵌入到每一个软件和硬件里。</p><p></p><p>可见，大模型的进化将会是一次“深入智慧之地”的旅程。而它，才刚刚开始。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jhQlKslkhzbhJPXlU6Ej</id>
            <title>Julia在大模型时代下的新发展与新应用：2023 科学计算与Julia 技术研讨会免费报名中</title>
            <link>https://www.infoq.cn/article/jhQlKslkhzbhJPXlU6Ej</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jhQlKslkhzbhJPXlU6Ej</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 07:57:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: <p>, <img src="https://static001.geekbang.org/infoq/6b/6b2ba22ef501ed80ff952ec5779e3b6e.jpeg" />, <img src="https://static001.geekbang.org/infoq/4e/4e142ee387cfa8fd8fd7586d4ad58196.jpeg" />, <img src="https://static001.geekbang.org/infoq/f5/f5e0b0a20eb7b05a1c132e0f2183056d.jpeg" />
<br>
<br>
总结: 这段文本包含了多个<p>标签和<img>标签，其中的<img>标签引用了不同的图片链接。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b2ba22ef501ed80ff952ec5779e3b6e.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e142ee387cfa8fd8fd7586d4ad58196.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5e0b0a20eb7b05a1c132e0f2183056d.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/02/02268610eb4f66346d68f920a2b7eb61.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4c904a48881b2ce5ea97620f4e1b27be.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f992b6ae5a9e1a079d2af5260b28a5f1.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/18/184f001007b91b947dd713f28a321441.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3a7de53d83ea09bb5d095da27bac47e4.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/44/4467bacd16775f74057030cccd6d0b10.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/62/6215f73b81d3bb1c5c612fdcb917cfe5.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b2833b5ce481380f1cc4d8500daa846.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ruyQDk6iC0hU6FBwrpb4</id>
            <title>对话凯文·凯利：当我们还困在无效会议中时，很多公司已经不在会议室放椅子了</title>
            <link>https://www.infoq.cn/article/ruyQDk6iC0hU6FBwrpb4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ruyQDk6iC0hU6FBwrpb4</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 07:27:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字世界, 科技, 文化, 人工智能
<br>
<br>
总结: 凯文·凯利是一位在数字世界中引领科技和文化发展的先驱者，他深刻理解科技对人类生活的影响，并预言了人工智能时代的到来。他强调个人要定义自己的成功，并做一些AI难以模仿的事情，以保持不可替代性。 </div>
                        <hr>
                    
                    <p></p><blockquote>采访嘉宾｜Kevin Kelly采访人｜霍太稳</blockquote><p></p><p></p><p>在数字世界的最前沿，有一位先知、一位导师，他以独特的视角和深邃的思考，引领着我们探索这个日益变化的世界。他就是凯文·凯利（Kevin Kelly），人们亲昵地称呼他KK，他是《连线》杂志的创始主编，也是《宝贵的人生建议》的作者，一个在科技与人文交叉领域的先驱者。</p><p>&nbsp;</p><p>KK对科技和文化的深度理解，使他在那个年代就意识到科技的巨大潜力和未来的无限可能。他坚信，科技将改变人类生活的方方面面，从我们交流的方式到我们学习、工作甚至娱乐的方式。</p><p>&nbsp;</p><p>1984年，KK发起第一届黑客大会（Hackers Conference），聚集了来自世界各地的创新者和思考者，共同探讨计算机科技对社会的影响。从那时起，他开始用笔和纸，记录下他对科技、文化和社会的思考。他的文章广泛出现在《纽约时报》、《经济学人》、《时代》、《科学》等重量级媒体和杂志上。</p><p>&nbsp;</p><p>在这个充满变化的世界里，人们很难预测未来会发生什么。但令人惊讶的是，KK预言了互联网时代的到来，并深入探讨了其对社会、经济和文化带来的深远影响。他坚信，人工智能将带领我们进入一个全新的时代，一个由智能驱动，由数据主导的时代。他的成就和贡献不仅在于杂志的创办和发展，更在于他对未来技术的深入研究和独到见解。</p><p>&nbsp;</p><p>不久前，KK的新书《宝贵的人生建议》正式发布，为这个时代中不断前行的奋斗者们指导了方向。借此时机，极客邦科技创始人&amp;CEO霍太稳（社区人称Kevin）与KK进行了一场深度对话，就当前技术圈和职场内的热门话题展开探讨。</p><p>&nbsp;</p><p>在访谈中，KK聊到了人工智能技术对整个世界的影响，并阐述了如何成为一个无法被AI取代的人。</p><p>&nbsp;</p><p>作为一名创业者，KK分享了他对“创业之路就是九死一生”这句话的理解。他回顾了自己加入《连线》杂志的始末，并表示所有创业公司离破产都只有一步之遥，无论是特斯拉、苹果还是亚马逊都毫无例外。对于初创公司来说，起起落落、生死关头都是常态。</p><p>&nbsp;</p><p>他也诟病了现在企业内部普遍存在的“开会文化”，KK表示自己不会参加莫名其妙的烂会，如果这场会议没有产出任何创新的东西，不如不开。而且他也观察到了一个有意思的现象——现在有些公司已经不在会议室里设椅子了，大家都得站着，所以会议不会拖得太长。</p><p>&nbsp;</p><p>在技术和人文领域深耕多年的KK身上有着诸多标签，他在外界眼中是一名创业者、作家、摄影师、艺术家、科技大佬......，但KK称他自己最喜欢的标签还是父亲这个身份。他很爱他的三个孩子，并尽可能多地抽时间陪伴他们，让孩子们切身感受到他所做的事情，而不只是乏味地言语说教。</p><p>&nbsp;</p><p>KK会带着孩子到世界各地学习、游历，希望他们感受不一样的世界，KK认为这对培养孩子的价值观和世界观尤为重要。</p><p>&nbsp;</p><p>以下是访谈实录，经编辑。</p><p></p><h2>怎样成为一个无法被AI取代的人</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：首先，很高兴有机会能和您进行一场对话，我看了您的书，深受启发。您的新书中有一条让我最印象深刻的建议，您提到“成功最可靠的⽅法，是你⾃⼰定义成功。先射箭，然后在射中的地⽅，画⼀个靶⼼”。这与很多人认为的先瞄定目标再朝着目标努力似乎不太一样。您能详细解释下其中的含义吗？先射箭，如果没有靶心，又该如何去瞄准？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：其实我想强调的是，大家要把对成功的定义权掌握在自己手里，而不是接受别人的成功理念。对我来说，那些有趣的人之所以有趣，就是因为他们自己有一套关于成功的定义。而照着这个定义前进，就是箭的去向。要做什么、做到怎样就算是成功，应该由我们自己说了算。</p><p>&nbsp;</p><p>所以如果你已经有了目标、有了对于成功的理解，比如就是每天都陪孩子们一起吃饭，那该做的就是不错过任何一次陪伴他们的机会。如果能做到这一点，那就算是成功。这就是我们的箭头所在，现在可以围着它画靶子了。现在可以围着它画靶子了，而且只要做到这一点，你就是个成功的人。总而言之，成功是由自己创造的，而不是走进由其他人预先定义的概念中去。在我看来，每个人都应该有自己的一套、跟其他人不同的成功定义。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：说得好。第二个问题是，你在书中谈到了很多生活建议。我想问的是，是否有一些建议您想给出但是没写在书里的？是哪些建议，关于什么的？为什么您没有放在里面？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：确实，确实有很多建议是在书出版之后才记下来的。但之所以没放进去，主要是因为我是后来才想到的。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：是您认为这些建议可能不是太成熟吗？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：没有，我不是觉得不成熟，而是当时确实没想到。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：明白了，原来是这样。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我是后来才想到的，可书已经写完了。我在书后又记录了一些额外的想法，所以倒不是不成熟，只是这些念头后来才冒出来。当然，其中也有一些不成熟，或者是我觉得太浅显、没必要记录进去的。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：那您能举个例子吗？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：没问题。对于艺术家和创作者来说，我觉得最好是能创作一些能激发他人也参与创作的东西。创作那种能激发他人创作的艺术品，拍下那种能激发他人拍摄热情的照片。</p><p>&nbsp;</p><p>我觉得这是个好办法。有时候当我们不知道该创作什么作品、或者拍摄什么照片时，那就想想怎样的东西能激发其他人的共鸣，能让他们在看到你的作品时、自己也想创作一番。</p><p>&nbsp;</p><p>这就是好东西，我觉得艺术家的意义就在于激励更多人投身到艺术中来。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：是的，我们应该多做一点能启发他人的工作，而不只是把它当成份差事。另外在书里，您提到要做那些AI难以模仿的事。做⼀个不能被算法模型化的⼈，这样你将⽆可取代。有些具体的建议吗？比如应该培养哪些技能？从事哪方面的工作？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：AI的特点，就在于它们属于预测型机器，它们擅长的就是预测你接下来要说什么、预测你接下来想买什么、预测接下来你会选什么。如果AI真的能够预测你所做的一切，那它就能取代你。所以我们真正需要的是对生活和事物抱有兴趣，做出让AI意外的行动，让它没法预测你。只要它们预测不了你，它们就无法取代你。所以大家应该构建一种无法被预测的生活。如果你每天做的都是一样的事、吃的都是一样的东西，那你就完全可以被预测。如果你对于世界的观点、对于税收的态度、对于移民的关注也全是一成不变的，那你就可以被预测，你就可以被AI所取代。</p><p>&nbsp;</p><p>AI可以为你建模，因为你的可预测性太强了。你得有自己的生活，你得有能力摆脱其他人的影响、真正形成自己对于事物的观点。只有这样，你才不会简单得像个脚本。我们不能只听别人怎么说，不能只听媒体怎么说，而是要自己主动思考。对事物拥有独立的见解确实需要付出更多努力，但这会让你更难被预测，自然更难被AI所取代。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：没错，所以我们应该增强自己的创造力，让AI没办法模仿自己。但这种能力要如何获取，或者说我们怎么才能在日常生活中变得更有创造力呢？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：创造力确实很难获取、也很难轻易传授给他人。我们都知道，孩子们都比较有创造力，而我们要做的就是防止自己随着年龄增长而失去这份创造力。想要留住这份创造力，就不要打击孩子们、别毁掉他们的可能性。因为教育总是让孩子们别犯错，与众不同往往招致嘲笑，而这些都是在打击创造力。</p><p>&nbsp;</p><p>所以我们必须克服这一点，允许孩子们与众不同且不被嘲笑，也没必要因为犯了错或者做某些人们觉得奇怪的事情而感到尴尬。确实，来自身边伙伴们的压力如此之大，往往会把创造力消灭在萌芽之中。因此我们需要建立新的制度，从社会、家长、学校、老师等各个角度帮助孩子们得到赞扬、保持住创造力。我认为人的创造力其实是与生俱来的，但这一切随着学校和工作的训练和消磨而逐渐丧失。你必须学会忽视他人对你的看法，同时有意识地发挥自己的创造力。所以我觉得，这需要在教育层面留出足够的空间，给创造力一片生长的土壤。我认识的那些最具创造力的人们，往往都不太在乎别人怎么看自己。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：是的。要拥有创造力确实不容易，希望每天我们都能像个孩子一样探索自我、引导自我。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：没错。而且这种引导其实是自然而然的。我们天生就拥有创造力，所以需要引导的并不是刻意创造，而是不失去这份创造力。</p><p>&nbsp;</p><p></p><h2>“莫名其妙的烂会，我压根就不会参加”</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：在您的书中，您还提到在同意参加⼀场⼯作会议之前，必须先看会议⽇程，并知道需要做出什么样的决定。如果不需要做出任何决定，就没必要参加这场会议。您是如何看待“开会文化”这一问题的？特别是那些没什么实际价值的烂会？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我就直接不去了。总之，只要我觉得某个会没什么新意，那我就不会去。我就是这么解决问题的。另外我还发现有个好办法，现在有些公司已经不在会议室里设椅子了。大家都得站着，所以会议不会拖得太长，时间久了太累人。</p><p>&nbsp;</p><p>所以每个人都不会浪费时间，把重要的事情说完之后就都撤了。这多好。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：确实，但有时候，如果想要激发一些创造性的想法，我们可能也需要通过会议进行头脑风暴。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：是的，要进行协作，就必须得开会。但大多数会议跟头脑风暴没有任何关系，跟协作也没有关系，单纯就是一个人把其他人早就知道的事情再说一遍。这样没意义的会就不该存在，我们只需要参与有必要的会。所以我才会在书里说，大家得自行判断要开的会属于哪一种。首先，这场会能告诉你一些你不知道的情况吗？会上是否要做出某些决策或者说行动？如果都没有，或者只是为了了解情况，那干脆就没有必要开，让他们给你发封电子邮件不就行了。所以，会议的真正必要性，在于根据你还不知道的信息来采取行动。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：没错，所以在你看来，如果会上没有新想法，那不如干脆别去。或者说如果只有些信息、不涉及行动，那也没什么意思。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：是的，决策和行动就是组织会议的唯一原因，如果两者都不沾，那不如直接把简单信息通过邮件发给大家。</p><p>&nbsp;</p><p></p><h2>创办《连线》杂志背后的故事</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：确实，书面信息的理解效率还比听取会议更好一些。下面咱们聊聊关于《连线》杂志的事儿吧，毕竟作为InfoQ的创始人，我做的也是技术媒体。希望您能向我们分享几个故事，分享一些经验。当初你们为什么会决定创办这份杂志？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：这话说起来就长了，我可能没法完整回顾一遍。其实有本专门讲《连线》杂志创立过程的书，是Gary Wolf写的，书名是《Wired, A Romance》。简单来说，这本杂志源自一对居住在阿姆斯特丹的美国夫妇的构思。他们当初在阿姆斯特丹就试过，但没能成功，后来他们搬到了旧金山。</p><p>&nbsp;</p><p>故事的开端就是这样。1990年那会，互联网才刚刚兴起，他们来的正是时候。他们给我看了杂志的设计原稿，而且他们当时打算找位编辑。看了样刊，我说这太棒了，绝对能得到市场好评，所以我也决定报名加入。其实办杂志的成功几率特别特别低。大多数杂志，就跟大多数企业和初创公司一样，一般坚持不了5年就会失败。无论是餐厅、杂志还是其他业务，大都如此。但我觉得还是值得一试，令人高兴的是《连线》最终确实没有失败。</p><p>&nbsp;</p><p>必须得说，我们真的很幸运。其实如果从IPO的角度讲，也可以说我们是失败了。七年之后我们还是在卖杂志，但毕竟《连线》仍然存在。所以哪怕是30年之后《连线》易主了，但只要它继续存在，我就会觉得非常自豪。这就是《连线》的起源故事。一对来自阿姆斯特丹的夫妇，路易斯·罗塞托和简·梅特卡夫。还有另外两位设计师，再加上创刊时的五位编辑，一切就这么发生了。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：那么，您自己的愿景和目标是什么？您参与创办过《连线》杂志，这些愿景和目标对于杂志的发展和成功有什么影响？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：关于这个问题，我想引用联合创始人路易斯的说法。他说，他希望《连线》杂志读起来就像是从未来寄回来的。就是说，这应该是一本在未来编撰出来的杂志，只是被寄回了现在。应该向读者传递一种深深的未来感，我觉得这是个很棒的设计。所以我们就努力让它读起来像是来自未来。当然，未来的到来并不均匀，就像是一种趋势在整个地球上爆发、蔓延。如同火山中喷涌而出的岩浆，有些溅洒在上海、有些溅洒在东京，还有其他什么地方。总之，《连线》杂志希望带大家环游世界，一起探索未来会先在哪里冒出头来。</p><p>&nbsp;</p><p>因此，引导我们的就是那种努力创造未来的感觉。这种感觉不只来自信息和情报，更来自一种共鸣。《连线》做到了，而且这一切都是设计的产物，我们在讨论技术的同时创造出了未来感。这其实非常复杂，属于比较专业的问题了。总之当一切如预期般发挥作用，当读者们愿意在杂志面前敞开心扉，这种未来感也就实现了。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：不少初创公司都经历过生死时刻，那您在工作中有没有过类似的感受？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：任何一次创业都肯定会有生死时刻。只要看看他们的账本就知道了，无论是皮克斯、特斯拉、苹果还是亚马逊，他们都曾经离破产只有一步之遥。</p><p>&nbsp;</p><p>他们手里没钱了，连工资都发不出来，账上的钱再有半个月就会花光。但这就是常态，《连线》杂志也是如此。对于初创公司来说，起起落落、生死关头都是常态。</p><p>&nbsp;</p><p>所以具体讨论哪家公司都没有分别，毕竟创业就是这样一个过程。我们唯一能做的就是面对现实，很多企业最终倒在了半路，而且那些如今看来非常成功的公司也都有过这样的经历。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：确实。那您还记得自己创业过程中最艰难的时刻吗？《连线》有没有遇上过无法盈利的问题？最后您是怎么熬过来的？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：《连线》肯定也遇上过类似的情况。当时的情况是，几位创始人手里掌握着大部分股票。为了不把公司整体出售，我们只能以低价转让了自己的股权。所以从某种程度上讲，这也可以说是失败。有些人觉得创办的企业能变现就已经是成功了，但我们毕竟是低价变现，所以肯定是失败的。但我们也有成功的一面，那就是《连线》杂志生存了下来，也让我们赚到了一点钱。所以这就是所谓既失败了，但也成功了。</p><p></p><h2>技术人可以像英雄一样改变世界</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：在创办《连线》杂志之前，您曾经是《全球概览》杂志的编辑和出版人。您是如何从一家杂志转向另一家杂志的？在这个过程中，有哪些故事可以分享？毕竟改变总会伴随不少困难。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：《全球概览》其实有点像互联网，只是当时互联网还没出现，但意思是差不多的。《全球概览》涵盖的内容很多，都是些非常实用的信息。它有点像博客或者报纸。我在80年代担任杂志编辑时，整个数字世界才刚刚拉开帷幕。</p><p>&nbsp;</p><p>所以我对技术文化很感兴趣，也试过引入一些跟《全球概览》类似的元素，但最终没能成功。其实我还办过一办叫《Signal》的杂志，跟《连线》差不多，只是没能引起关注。路易斯认为，《连线》的核心主线应该是关注技术创造者，而不仅仅是技术本身。我的那本杂志主要是介绍技术本身的，所以他的想法明显更好。因为他的想法更好，所以我才下决心脱离《全球概览》去了《连线》。路易斯的观点非常明确，要以讨论发明者及其目标为切入点讨论技术，并让这些创造者成为英雄。</p><p>&nbsp;</p><p>所以除了让杂志成为窥探未来的窗口，我们还有另外一项目标，那就是让科技人士成为英雄。其实在上世纪70、80年代，技术人往往被视为怪咖和失败者。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：那种典型的腼腆形象，比如说比尔·盖茨？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：确实，就像比尔那样，搞技术的就是那副口袋里插满了笔的形象，有点窝囊颓废。但我们说，不对，要让他们成为英雄。比尔·盖茨是真正的英雄，绝不是什么失败者。《连线》的一大目标就是要让这些从事技术的人们感受到自己就是英雄，自己正在改变世界，然后带着这股热情和主动性来讲述自己的故事。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：酷，你们确实做到了。很多技术人确实从《连线》杂志中获得了灵感和力量。而且现在有很多人像我一样，都想创办自己的杂志、媒体或者是企业。那么，KK，您对于这些想要创业的人们，有什么建议和经验可以分享吗？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：首先我得强调一点，就是这个世界上现成的信息已经太多了，已经足够指导那些想要自己创业的人们。所以我不知道自己还有什么可补充的，毕竟互联网上什么内容都能找到。我能提的唯一建议，就是对于年轻的创业者来说，最好是选择一个还没有固定的词汇或字眼能描述你想做的事情的方向。</p><p>&nbsp;</p><p>比如说大概十多年之前，如果你想搞区块链，那根本没人知道你在干什么，其中的概念也很难描述。市面上更是没有现成的词汇可以使用，所以这才是片真正的蓝海。或者，也许15年前你正在做一些类似于广播、但又不同于广播的业务，这就是现在的播客。但当时大家根本没听说过播客这个词，只能说它类似于广播但又不完全是广播。那么播客在15年前也算是个好方向。</p><p>&nbsp;</p><p>所以如果大家如今想要在某个领域有所建树，那最好是选片真正的蓝海，没有词汇可以直接归纳，也没有专家知道该怎么形容。如果你发现自己很难向父母解释自己在干什么，那这可能就是个好方向。</p><p>&nbsp;</p><p>总之我想说的是，最好别总想着再开发另外一种搜索引擎、或者开发另外一款天气预报应用。这些都有明确的概念了，所有人都知道那是什么东西。在这种红海里创业太困难了。所以，要尽量选择还没有名称、定义不明确、范围不清晰的方向，这对初创公司来说才更有前途。没错，失败的可能性很大，但潜在的收益也将极为可观。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：是的，创造确实非常非常困难，但如果有一天取得成功，我们就会迎来巨大的成功。而且随着业务规模的不断扩大，小组织也会逐渐发展成大团队。那么，管理大团队和管理小团队之间有什么区别？能不能分享一点您的经验？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我们都知道小团队有天然优势，比如更灵活、速度更快、成员之间更容易建立感情之类的。但缺点也很明显，就是所有工作都得由我们亲手完成，任务可以说是没完没了。第二个缺点，就是体量越小的公司失败率越高。所以这就得做出权衡，优缺点是相辅相成的。公司越大、安全性越高，但也越不可能做出惊人的颠覆。公司越小，颠覆一切的可能性就越大，但失败的几率也随之上升。我们必须把选择跟自己所处的人生阶段结合起来。对于已经有孩子的双职工家庭来说，承担这种风险的难度更大。但如果你还年轻而且单身一人，那就简单多了。</p><p>&nbsp;</p><p>我觉得如果你还年轻，那最好把自己能够承担更高风险的优势发挥出来。</p><p>&nbsp;</p><p></p><h2>慢即是快，成功的关键就是要坚持下去</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：没错，创业确实无比艰难。那您有没有想过要放弃？面对困难的时候，你要如何克服失败、继续向前？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我在书里也提过这一点，大多数成功都源自耐心，你得心甘情愿把一件事做上10年。也许一年之内不会成功，但只要坚持不懈，长远来看终究会成功的。</p><p>&nbsp;</p><p>所以如果面对困境和难题，取得成功的一种好办法就是着眼于长远。只有这样才能坚持下来，坚持十年绝非易事，但做不到这一点就不可能成功。你的耐心越好，取得成功的可能性就越大。所以在面对挑战时，有时候我们只能耐心等待，只能缩小规模、裁掉一半的雇员，但明天的生活还是要继续。这就是耐心的力量。</p><p>&nbsp;</p><p>所以我想说的是，渡过难关的一种好办法就是把眼光放长远。大家得愿意把回报周期放到十年的尺度上，而不能指望着两、三年就原地起飞。大多数业务都需要十年时间，我自己就是那种能坚持下去的人。这就是我的办法。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：所以要把眼光放得长远一些。在中国，不少企业老板也告诉我，你得相信未来、你得培养自己的耐心，所以我想道理是相通的。那么KK，您是如何定位自己在社区互动中的角色？能不能分享一点您跟读者和作者们打交道的方法？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：很明显，跟客户之间的互动是越多越好。无论你做的是社交媒体还是零售业务，或者说你是搞艺术、搞演出的，面对一、两千粉丝，都应该尽量跟他们交互。但这里也存在权衡，因为交互会耗费大量时间、精力和资源。如果你是一家企业，你可以雇用专门的员工做这方面工作，但同时交互的效果也会变差。而如果你体量很小，那么交互的效果会很好、关联会非常密切，但这也对应着更高的成本，甚至占用掉你干正事的时间。我们当然也可以聘请全职主持人和社交媒体管理员，但因为体量太小，所以对应的成本就太高了。比如说当你单兵作战的时候，那跟1000名粉丝交互就已经占用掉了全部工作时间。那你就没办法再创作音乐作品了，每天唯一能做的就是陪他们玩。这样虽然跟社交的互动确实很有成效，但成本也太过高昂。总之一定要做好权衡。</p><p>&nbsp;</p><p>万事万物都不是免费的，我们必须得把资源投入到服务台、社区论坛、版主，还有各种昂贵的、人性化且接触频率很高的方案身上。如果你愿意付出，那就能得到回报。但发展社区跟与社区打交道总有成本，甚至可能在业务支出中占据很大一部分。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：是的，这必然对应着大量的时间或者资金投入，但同时也能从中获得很多回报。那在您看来，有没有哪些事件或者决定对您的职业生涯产生了重大影响？您又是如何定义职场上的成功和失败的？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：明白，这个问题跟我之前说的要在还没有固化下来的领域创业有关系，在这样的舞台上成功应该尽量由我们自己来定义。</p><p>&nbsp;</p><p>无论是对个人还是企业都是如此，我们不该借用、窃取或者照搬别人的成功定义，而应该创造属于自己的成功定义。比如说，我想办一家最出色的婴儿车工厂，而且全体员工都是在职母亲。</p><p>&nbsp;</p><p>也就是说，只要我们的员工100%都是在职母亲，那我们就已经成功了。要根据我们自己的成功定义把公司组织起来。我认为只要能找到独一无二的成功定义，那么无论是作为个人还是一家企业，你都能做得更好。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：其实在书中，您也告诉我们不一定要做最好的，但要做唯一的。那您觉得一个人能在职场上取得成功的关键因素是什么？我们又该如何培养这些能力和品质？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我觉得最好是找一家重视学习的企业，所在的组织越擅长教学、员工们的能力就越强。换句话说，好公司一定是教学型的公司，而教学的对象就是你的员工。</p><p>&nbsp;</p><p>在《连线》我们流传着一句话：我们聘用的是态度，培训的是技能。其实那些刚进入《连线》的新人都不具备必要的技能，他们根本不懂怎么开发网页。但那时候网页技术还在发展当中，所以我们根本就招不到所谓精通网页开发的人才，没人知道要怎么做。所以我们会根据员工的态度和能力进行筛选，然后培养他们逐渐掌握网页编程技能。他们自己会主动学习，我们也会努力教学。所以我觉得必须要让公司成为员工教育流程中的重要载体，引导他们，帮助他们成为更好的工作者、更好的人。所以教学就成了我们日常工作中的重要组成部分，但不是在教室里，而是通过同事间的讨论、结对工作、导师合作等等。公司内部应该建立起浓厚的学习氛围，抓住一切机会为员工提供指导和教育。</p><p>&nbsp;</p><p></p><h2>创业者的B面人生</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：没错，培训不光是对于年轻人、对我们所有人来说都是非常重要的。第三部分问题主要涉及个人生活和家庭，应该会比较轻松。您创办了一本杂志、也出版了自己的书，而且还热爱摄影。听说您还在亚洲巡游过足足9年，用照片记录了不同国家的文化和风貌。所以你身上似乎有很多标签，创业者、作家、摄影师、艺术家、科技大佬、互联网教父等等。您最喜欢其中哪个标签，为什么？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：父亲，我最喜欢父亲这个标签。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：是指互联网教父吗？还是说只是孩子们的父亲？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：就是父亲，我有三个孩子。其实从某种意义上讲，这三个孩子是我人生中最大的乐趣来源。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：明白了，那在生活中您是那种严厉的父亲吗？会如何跟孩子沟通？因为我也有两个孩子，他们都很有个性，好像永远不会听我的话。您有类似的经历吗？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：当然，他们也不听我的话。但孩子就是这样，他们不会听你说话，但会看你怎么做事。所以我会尝试改变，努力通过行动、而非语言来教育自己的小孩。后来他们长大成人，会告诉我这确实很有效。他们觉得我是那种话不多，但会通过行动表达态度的人。这确实很难，但长辈对孩子的影响确实更多体现在行动上，而不是语言上。</p><p>&nbsp;</p><p>我在某些事情上非常严格，但其他事情上却相当宽松。所以孩子们也知道我到底关注什么，比如诚实、不说谎，我对这事就非常重视。我从来没对自己的孩子撒过谎，也不希望他们对我说谎。但其他小事，比如他们该穿什么衣服、该换什么衣服之类，我就不关注也不在乎。这就是我跟孩子们的相处方式，对有些事非常严格，对其他事能放则放。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：所以说，以身作则才是教育孩子的最佳方式喽？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：没错，我觉得这就是最佳方式。他们关注的不是大人说了什么，而是大人做了什么。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：确实，毕竟孩子们也不会乖乖照我们说的做。那您如何看待兴趣爱好跟日常工作之间的关系？毕竟每个人的时间都是有限的，二者之间该怎么平衡呢？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我个人不做平衡，因为我的爱好跟工作是一体的。我倾向于做那些自己无论如何都会做的事，即使没有报酬也无所谓。这就是所谓爱好，爱好是指我们不拿钱也愿意做的事情。我就会坚持做这类事情，不管有没有钱拿。</p><p>&nbsp;</p><p>所以这就成了我的爱好。现在你问我，我每天做的到底是爱好还是工作？我不清楚，因为二者几乎融为一体了。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：也就是说，对您来说爱好和工作是一体的。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：没错，我认识的那些最成功的人们也是如此。比如说沃伦·巴菲特，作为全球知名的投资者他其实根本就不需要继续赚钱了。他的钱就连捐都捐不完，还有比尔·盖茨都是这样。巴菲特已经那么有钱了，但还是在不断投资。他都90多岁了，何必呢？答案只有一个，投资是他的爱好，而不再是什么工作。</p><p>&nbsp;</p><p>他喜欢做投资，而且会一直做到自己寿终正寝的那一天。从这个角度讲，他其实从来都没有工作过。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：所以最好是找一份跟自己爱好相重合的工作。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：是的，找那些你甚至愿意掏钱去干的事情。如果找到了，那就会是你最擅长的东西。我知道很多人都是这样，他们做事就不是为了赚钱。沃特·迪士尼就是这样，他说他拍电影不是为了赚钱，他赚钱是为了能继续拍电影。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：另外还有一个平衡问题，就是你在职业生涯中要怎么平衡家庭和事业间的关系？很多职场人士一直饱受折磨，能不能向他们分享一点建议？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：好的，但这个问题很难有普适性的答案。这往往要取决于你有没有孩子、是双职工家庭还是单职工家庭等等，所以很难笼统地给出答案。</p><p>&nbsp;</p><p>就我自己来说，我倒没有想过太多。毕竟孩子们对于自己父母的期许和要求也是各有不同。所以无论如何，我觉得我们家算是比较平衡了，而且这种平衡是无意间形成的。我自己没想过太多，但结果还算不错。</p><p>&nbsp;</p><p>所以我也不太清楚，没办法给大家太好的建议。毕竟根据我自己的经验，每个家庭都有很大的差异，不同的家庭需要寻找属于自己的特殊平衡。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：找寻这种平衡非常困难。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：确实，而且我想强调的是这种平衡不能一概而论，我没办法给出一通百通的平衡方案。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：我听说您特别喜欢旅行，那您去过的让您印象最深刻的城市是哪个？您觉得旅行的意义是什么？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我还是先分享一点关于旅行的轶事吧。50年来，我一直在亚洲各地旅行。要说对某座城市印象深刻的瞬间，那应该是1975年左右我第一次尼泊尔加德满都的经历，那可以说是我一生中最奇妙的体验了。当时的加德满都大概只有100万人口，不太确定，但总之城市不是很大。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：那算是个小城市了。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：当时还很小，但整个城市里都没有私家车。</p><p>&nbsp;</p><p>卡车肯定是有的，但居民们出行就靠双脚，连自行车都没有。到处都是徒步街的人们，没有汽车、没有自行车。人们就这样用双脚及量自己生活的土地。整座城市都没有私家车，那种感觉太棒了，就如同穿越回了中世纪。我像是坐上了一台时光机，回到了自己从未谋面的过去。对我来说，那就是最令人印象深刻的城市。但现在不同了，毕竟时间已经过去了50年，一切早已经改变。但在当时，那就是我亲眼见证过的最神奇的城市。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：对您来说，旅行的意义是什么呢？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：旅行是保持学习的最佳方式之一，也是获得不同思维方式的最佳方式之一，更是尝试扭转思路的最佳方式之一。我认为通过游走于世界各地，你能发现人们之间的相似性，这也能让我们变得更加乐观。就拿战争为例，我们很难对自己曾经去过的国家痛下杀手。</p><p>&nbsp;</p><p>所以我觉得多出去旅行有益于世界和平。我认为旅行对年轻人来说非常重要，所以各个国家应该用补贴的方式让年轻人们走出去。美国、中国、印度，都应该让民众趁年轻的时候靠补贴出去旅行，借此了解关于我们身处的世界的各种知识和运行规律。等他们回来了，观念也会发生改变。他们应该旅行两年并在国外长住。你能想象，如果每个美国人都曾经在其他国家生活过两年，美国会变成什么样吗？你能想象中国的学生都在国外生活过两年，又会是怎样的情景吗？这将从根本上改变整个国家，让国家变得更好。我认为我们应该在美国实行强制性的全民服役，让年轻人通过维和部队在国外待上两年，帮助当地社区。这肯定会带来惊人的回报。</p><p>&nbsp;</p><p>我觉得旅行是种加快学习的方式，是一种磨练差异化思维、与他人和谐相处的好办法。而且，旅行也特别有助于减少国家之间的冲突。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：没错，好建议。我想我也应该带着自己的孩子去世界各地走一走，特别是美国还有中国各地。</blockquote><p></p><p></p><p>Kevin Kelly：我在出差的时候就会尽量带上孩子。没错，我在儿子女儿还小的时候就经常带他们出差，让他们看看爸爸在做什么、看看世界是什么样子。这比待在学校有意义多了，他们也愿意从学校里逃出来。是的，尽量多带孩子出去旅行，他们的人生都将因此而改变。</p><p></p><p></p><blockquote>InfoQ：社会才是真正的大学，而且效果比真正的学校还要好。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：的确如此。我曾经带我儿子去过泰国和不丹，当时他才上八年级。这些经历确实改变了他的人生。</p><p>&nbsp;</p><p></p><h2>万事开头难，只要肯写，就一定能写完</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：另外一个问题，你在写作当中是如何克服倦怠情绪的？毕竟写书这种事工作量巨大，您能不能分享几个具体例子？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我其实在书里也分享过关于写作的建议。</p><p>&nbsp;</p><p>对我来说，写初稿是最难的。有些朋友可能有类似的经历，初稿最后肯定会被废弃掉。电影公司皮克斯也是一样，他们谈到在制作电影的时候，开头出来的效果总是特别差。他们会不断做修改和完善，让影片慢慢变得不那么糟糕，最终好起来、精彩起来。</p><p>&nbsp;</p><p>初稿就是最难的，所以我会假装自己给朋友写信抱怨，告诉他这书有多难写。比如说这个题目太麻烦了，里面涉及的复杂内容太多了之类。</p><p>&nbsp;</p><p>而在抱怨的同时，其实我们就已经是在写作了。就像是在给最好的朋友写信一样，你会在里面说自己目前有多么挣扎、多么沮丧，但又不得不坚持着写下去。在解释写作为什么困难的时候，其实我就已经开始写起来了、有进展了。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：可能比较好的办法，就是先把整个作品拆分成几个更小的部分。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：是的，这就是写作过程的一部分。但基本上对我来说，我就是试着解释自己在干什么。初稿真的太难写了，但我知道这并不重要，毕竟无论如何初稿最终都会被废掉。这只是个痛苦，但却不得不经历的阶段。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：明白了，那你每天都会这样做吗，毕竟这样的过程会耗费很长时间。</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：那也没办法，因为只有这样才会有第二稿。在拼尽一切之后，我们手里的初稿才会成为第二稿。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：下一个问题是关于乐观心态的。实际上在现代社会，面对家庭和工作的双重压力，现代人的生活已经非常艰难。您对此有什么好的建议吗？比如我们应该做点什么、想点什么来保持乐观的心态？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：这是个好问题。我们意识到，乐观其实是种可以通过学校教育来传授的技能。你是可以教会孩子如何拥有乐观心态的，而具体的教育方式就是帮助他们理解挫折。失败只是暂时的，不会伴随他们一生一世、成为永恒的烙印。失败是可以克服的、暂时的问题，理解了这一点，人们就会变得更加乐观。相反，很多孩子受到的教育就是不断强调他们有多蠢、他们有多坏、他们有多么不努力。他们从此失去了信心，变得越来越悲观，因为他们觉得自己的一切痛苦都来自倒霉或者命运的安排。但如果告诉孩子们，他们遇到的挫折都只是暂时的，只要保持耐心并继续努力就一定能够将其克服，但他们就会变得更加乐观。</p><p>&nbsp;</p><p>我还认为，把时间放长远也有助于培养乐观情绪，比如设定十年奋斗目标。此外，长远的眼光也有助于我们从那些真正重大的失败中走出来。人生就像股票市场，总会上下波动，会有萧条期、也会有衰退期。但如果着眼于十年的长期回报，而不关心短期内的涨跌起伏，那人就会变得更加乐观。因此，要把眼光放长远、保持良好的耐心。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：好建议，非常感谢。您在书中提到“毫不犹豫地⾃我投资——花钱上课，学习新技能。这些不起眼的投资，能产⽣丰厚的回报。”能举个具体的例子说说为什么自我投资很重要？其实我们InfoQ旗下《极客时间》产品也是一款为广大开发者提供技术专业知识的板块，这里有丰富的技术专业领域知识。作为一款IT教育产品，您认为这款产品要具备哪些特征才能够吸引更多开发者关注和使用？能不能给我们提点具体建议？</blockquote><p></p><p>&nbsp;</p><p>Kevin Kelly：我不太了解怎样为自己的社区吸引开发者。但我想说的是，无论花多少钱投资自身技能，哪怕它们看起来跟当下正在做的事毫无关系，最终也一定能转化为回报。</p><p>&nbsp;</p><p>有个著名的故事，就是史蒂夫·乔布斯曾经跑去参加书法课。每个人都觉得把大学学分浪费在书法上纯粹是疯了，但他认为这段经历让自己拥有了在第一台Macintosh电脑上发明漂亮字体的专业背景和视角。</p><p>&nbsp;</p><p>这就是投资的意义所在，甚至不一定要跟当前的业务直接相关。如果你想学焊接，那就花钱去学；如果你想学插花，那就花钱去学。总之你对自己的投入越多，就越是了解自己。</p><p>&nbsp;</p><p>这种自我认知代表着巨大的力量。你要找到最适合自己的位置、最适合自己的角色。但我们刚开始其实并不了解自己，特别是在年轻的时候，我们根本不知道自己是谁、不知道自己擅长什么。而投入学习的资源越多，你就越是能够看清楚自己。所以自我投资在某种程度上，是一种了解自己的方式。你在发掘自己的本来面目，为自己赋予新的技能。而这种技能将对应着新的力量、新的定位，还有发现自己真正专长的更好机会。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：感谢您接受我们的采访，我的问题都问完了。请好好休息吧。</blockquote><p></p><p></p><p>Kevin Kelly：再次感谢你们对我工作的关注，也感谢你认真准备的这些问题。非常感谢。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5KVxUWu9FzZrbXWl3M61</id>
            <title>这家手握2.2万块H100芯片的AI独角兽发布了新款大模型：1750亿参数，性能仅次于GPT-4</title>
            <link>https://www.infoq.cn/article/5KVxUWu9FzZrbXWl3M61</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5KVxUWu9FzZrbXWl3M61</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 06:58:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Inflection AI, AI模型, 谷歌, Meta
<br>
<br>
总结: Inflection AI发布了一款名为Inflection-2的AI模型，该模型性能超越了谷歌和Meta的主流替代方案，并且正在缩小与OpenAI旗舰模型GPT-4的差距。Inflection AI是一家由谷歌旗下DeepMind AI研究小组联合创始人和LinkedIn联合创始人共同建立的初创公司。该公司表示，Inflection-2在多种基准测试中表现最佳，但仍落后于GPT-4。新模型将被整合进Inflection的聊天机器人Pi中，并且Inflection已经与微软、英伟达等公司合作管理计算集群。 </div>
                        <hr>
                    
                    <p></p><h2>Inflection AI发布新模型：5000块英伟达H100训练、1750 亿参数</h2><p></p><p>&nbsp;</p><p>开发出对话聊天机器人Pi的初创公司Inflection AI刚刚发布其最新AI模型，据称这套模型的性能已然超越谷歌和Meta两家的主流替代方案，并且正快速缩小与OpenAI旗舰成果的差距。</p><p>&nbsp;</p><p>Inflection AI是由谷歌旗下DeepMind&nbsp;AI研究小组联合创始人Mustafa Suleyman与LinkedIn联合创始人Reid Hoffman于今年3月共同建立的企业。短短两个月后，该公司就完成了2.25亿美元融资；到今年6月，Inflection AI又从微软、英伟达等多家知名投资方处额外筹得13亿美元。</p><p>&nbsp;</p><p>Inflection方面表示，这套新模型名为Inflection-2，已经在多种标准基准测试中超越谷歌今年5月发布的PaLM Large 2模型，同时在其他一些指标上击败了由Meta牵头打造的开源Llama 2模型。这家初创公司指出，总的来看Inflection在同等体量的模型中表现最佳，目前仅落后于OpenAI发布的体量明显更大的旗舰级模型GPT-4。</p><p>&nbsp;</p><p>公司CEO Mustafa Suleyman在接受采访时表示，“我们坚信自己正处于这条扩展曲线的起点，由此延伸出的新功能确实令人无比兴奋。”</p><p>&nbsp;</p><p>新发布的模型很快就会被整合进Inflection于今年5月发布的聊天机器人Pi当中。但Suleyman表示，在此之前还有大量“对齐”工作要做，即让新模型掌握Pi机器人的语气和回答风格，让它帮助Pi更好地发挥作用，并在吸纳最新信息的同时不致产生额外的幻觉。</p><p>&nbsp;</p><p>他解释称，“无论用户想要就种族、性别、政治、OpenAI乃至任何当下富有争议的问题进行敏感对话，Pi都能以非常巧妙、谨慎且尊重事实的方式进行交互，并实时获取信息。”Suleyman还补充道，Pi将在“不久之后”更新至Inflection-2模型，但没有给出具体日期。</p><p>&nbsp;</p><p>他也不愿透露Pi上的用户数量，只强调“Pi非常受欢迎，用户留存率很高。”两周之前，OpenAI表示其免费ChatGPT服务的每周用户数量已达1亿。但就在好消息发布后不久，上周五OpenAI董事会突然发难、解雇CEO Sam Altman，上演了举世震惊的“宫斗戏”。</p><p>&nbsp;</p><p>值此业界头部模型开发商身陷混乱的背景之下，我们很难不去关注Inflection此番发布的大语言模型。</p><p>&nbsp;</p><p>有外媒称，拥有1750 亿参数的模型被不少业内人士视为“当今世界上第二强的大模型”。就在本周三上午，OpenAI正式宣布Altman将重返公司并继续担任CEO。整个周末，各方盟友已经就Altman的回归展开谈判，OpenAI公司员工还签署一份请愿书，要求董事会成员集体辞职。</p><p>&nbsp;</p><p>而在Inflection这边，Suleyman则坚称这家初创公司已经于今年早些时候筹集到13亿美元资金。但当时他并没有给出Inflection新模型的发布日期，只是在更早的公开言论中暗示可能会在年底前推出。他在最新公告中解释称，这次的发布实际上晚了一星期，模型训练工作早在上周就已经完成。</p><p>&nbsp;</p><p></p><h2>Inflection AI：Inflection-2超越谷歌、Meta家主流模型，目前仅落后于GPT-4</h2><p></p><p>&nbsp;</p><p>在Inflection-2的训练当中，该公司使用了5000块英伟达H100图形处理单元（GPU），远少于训练前代Inflection-1时使用的数千块A100。目前H100的零售价超过2.5万美元，内置800亿晶体管，其语言模型运行速度高达英伟达上代旗舰A100 GPU的30倍。</p><p>&nbsp;</p><p>Inflection AI还使用H100进行推理，也就是在生产当中运行Inflection-2以处理用户提交的任务。Suleyman还提到，新模型的训练速度更快、成本更低，但同样需要处理大量运算任务（10^25 FLOP，即浮点运算）。Inflection已经与微软、英伟达和CoreWeave密切合作以管理其计算集群。</p><p>&nbsp;</p><p>Inflection公司已经在一套流行的高中与专业能力基准（即MMLU）上测试了其最新模型。模型在测试中回答了涉及57个主题的大量问题，具体涵盖世界知识、解决实际问题以及道德判断。</p><p>&nbsp;</p><p>根据Inflection AI的介绍，新模型能够访问升级后的知识库，从而更准确地回答用户查询。Inflection-2还拥有“显著改进的推理能力”，使得模型能够更好地执行代码生成等任务，并对所生成文本的语言风格做出细粒度调整。</p><p>&nbsp;</p><p>不只是优于前身，Inflection-2在与竞争对手的大语言模型比较时同样表现不俗。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/242da0efe379c058dcf383dcf1742451.png" /></p><p></p><p>Inflection-2与其他模型间的性能比较。</p><p>&nbsp;</p><p>Suleyman表示，Inflection-2的性能优于业界体量领先的700亿参数版Llama 2、马斯克初创公司xAI的Grok-1、谷歌的PaLM 2 Large以及Anthropic的Claude 2，目前仅落后于GPT-4。</p><p>&nbsp;</p><p>据Inflection的介绍，新模型在七项科学问答基准测试中，五次击败了Llama 2和PaLM 2模型；在三项问答任务基准测试中也两次成功登顶，只在一项测试中逊于PaLM 2 Large。该公司还补充称，尽管模型训练并无明确的领域倾向，Inflection-2还是在四项数学与代码基准测试中获得优异成绩，只是与OpenAI公布的结果相比仍远落后于GPT-4。OpenAI的模型虽然性能更好，但体量也要比Inflection-2更大，意味着其训练需要消耗更多计算资源。</p><p>&nbsp;</p><p>Suleyman指出，虽然似乎只有AI研究人员或开发者会高度关注这样的基准测试结果，但哪怕最微小的改进也足以在“技术原型”和“生产级、高可靠且高质量”模型之间形成关键差异。总的来说，Suleyman表示Inflection-2已经在同等体量模型中遥遥领先，而且在很多方面已经“非常非常接近”最强王者GPT-4。</p><p>&nbsp;</p><p>据悉，Inflection AI手头共掌握着2.2万块H100 GPU，相当于训练Inflection-2的GPU数量的四倍有余。该公司在本次发布中强调，他们计划使用这些硬件构建更先进、体量更大的新模型。据报道，从现在开始，Inflection已经将训练重点转移至下一代模型，预计新模型将在六个月内达到Inflection-2的10倍体量，并在接下来的六个月中进一步扩大10倍。（最终体量有望增长至100倍。）</p><p></p><h2>OpenAI管理层大乱斗，给了竞对可乘之机</h2><p></p><p>&nbsp;</p><p>就在OpenAI管理层大乱斗达成临时和解方案的几小时前，Suleyman敦促公众以“同理心和宽容态度”对待那些被卷入风波的人们。他指出，“很多人的言行完全是出于善意”，还点名提到OpenAI联合创始人Ilya Sutskever。Sutskever是最初参与解雇Altman的董事会成员之一，但随后在周日晚间表示对这一决定感到后悔。（目前还不清楚Sutskever是否会继续留在OpenAI，但他在X上点赞了公司及Altman宣布回归的帖子。）</p><p>&nbsp;</p><p>Sutskever曾在2011年为Suleyman工作过，时任DeepMind外包商兼顾问。三年之后，Suleyman将这家AI初创公司卖给了谷歌。Suleyman在谈到Sutskever时表示，“我非常尊重他，这种尊重不仅源自技术水平，也源自他的原则性和真诚的态度。我认为在此次事件中，他和团队中的其他成员都抱有真诚的意图。”</p><p>&nbsp;</p><p>Inflection和Suleyman此前曾就AI安全问题表达过明确态度，并于今年7月自愿签署了拜登总统下达的“关于安全、可靠和可信的AI行政令”。他还发誓不会让聊天机器人Pi回答关于选举的任何问题、或者为政治竞选活动提供材料。Suleyman最新出版了一本名叫《袭来的浪潮》（The Coming Wave）的书，详细介绍了AI技术的风险。</p><p>&nbsp;</p><p>在谈到OpenAI非营利董事会及其近期一系列行为、特别是因此在科技领域遭受的诽谤时，Suleyman表示“很明显，他们犯下了一系列非常重大的错误。但我还是希望人们能够宽容一点，把这次事件视为OpenAI发展旅程中的一段小插曲。努力让公司变得更好、做正确的事、通过新的治理结构让全世界更美好，这些才是真正重要的工作。”</p><p>&nbsp;</p><p>但Suleyman本人也秉持典型的资本家思维，在被问及OpenAI的困境（ChatGPT在周一和周二两度陷入短时间瘫痪，大量客户忧心忡忡地关注着事态发展）是否给竞争对手带来了可乘之机时，他并没有予以否认。</p><p>&nbsp;</p><p>“从根本上讲，我办的是一家企业。面对激烈的市场竞争，我们别无选择。当下也是硅谷多年以来竞争烈度最高、创造力最强的时期。”</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.forbes.com/sites/alexkonrad/2023/11/22/inflection-ai-releases-2nd-model-on-gpt-4-heels/?sh=65dbfdfd6b05">https://www.forbes.com/sites/alexkonrad/2023/11/22/inflection-ai-releases-2nd-model-on-gpt-4-heels/?sh=65dbfdfd6b05</a>"</p><p><a href="https://siliconangle.com/2023/11/22/inflection-ai-debuts-new-flagship-inflection-2-llm-trained-5000-h100-chips/">https://siliconangle.com/2023/11/22/inflection-ai-debuts-new-flagship-inflection-2-llm-trained-5000-h100-chips/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lyZBJBqiu4CfELMpAFYT</id>
            <title>腾讯披露最新大模型训练方法：效率提升至2.6倍、可节省50%算力成本</title>
            <link>https://www.infoq.cn/article/lyZBJBqiu4CfELMpAFYT</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lyZBJBqiu4CfELMpAFYT</guid>
            <pubDate></pubDate>
            <updated>Fri, 24 Nov 2023 03:31:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 训练效率, 推理成本, 自研机器学习框架Angel
<br>
<br>
总结: 腾讯自研的机器学习框架Angel通过提升大模型训练效率和降低推理成本，解决了大模型参数规模增长和算力紧缺的问题。通过多维度的并行计算、统一视角技术和软硬件结合的通讯方式，Angel提高了大模型训练的效率。同时，通过扩展并行能力和关键能力的优化，AngelHCF提高了推理速度和吞吐能力，降低了推理成本。腾讯云通过升级后的Angel框架提供更优的训练和推理加速能力，支持客户构建专属智能应用。 </div>
                        <hr>
                    
                    <p>如今，大模型的参数规模呈现指数级增长。在算力紧缺的背景下，如何提升大模型训练和推理的效率，并降低成本，成为业界关注的焦点。</p><p>&nbsp;</p><p>11月23日，腾讯披露，腾讯混元大模型背后的自研机器学习框架Angel再次升级，大模型训练效率提升至主流开源框架的2.6倍，千亿级大模型训练可节省50%算力成本。升级后的Angel支持单任务万卡级别超大规模训练，进一步提升腾讯云HCC大模型专属算力集群的性能和效率。</p><p></p><h3>训练推理效率如何再提升</h3><p></p><p>&nbsp;</p><p>面向大模型训练，腾讯自研了机器学习训练框架AngelPTM，针对预训练、模型精调和强化学习等全流程进行加速和优化：</p><p>&nbsp;</p><p>在存储方面，AngelPTM 计算支持多维度的并行，包括常见的数据并行、模型并行、流水并行和序列并行。此外，腾讯在ZeRO-Cache的基础上加入了统一视角技术，通过统一的地址寻址方式把显存和主存打通，训练时大量参数先放到系统对层里，需要时再放到显存里，以此将有限的显存容量扩展，单机存储容量提升90%。通讯方面，腾讯通过软硬件结合的方式解决。首先，腾讯用自研交换机构建 3.2T RDMA网络来拓宽带宽，然后结合框架软件层面做GPU拓扑感知，此外还有负载均衡的流水并行。稳定性方面，腾讯对基础设施的网络、硬件、存储、云原生调度都加了相应的监控指标。发现故障后，调度平台会把故障报告给AngelPTM框架。大多数情况下，调度平台会进行自动续训，训练过程中也会写快照。此外，除了系统容错还有收敛性监控，包括参数极值、Loss曲线、模型参数本身的极值、梯度的极值、中间变量、激活值等。</p><p>&nbsp;</p><p>另外，鉴于国产芯片可能会有一些异构的生态，腾讯提供了算子编译层，整个模型研发基本可无缝低成本迁移。</p><p>&nbsp;</p><p>为解决推理成本不断上升问题，腾讯自研的大模型推理框架AngelHCF通过扩展并行能力，采用了Embedding共享、Attention算子优化、Paged Attention优化等方式，同时提供了量化、稀疏化、蒸馏和剪枝等关键能力，以提高吞吐能力，从而实现更快的推理性能和更低的成本。</p><p>&nbsp;</p><p>根据官方数据，相较于业界主流框架，AngelHCF的推理速度提高了1.3倍。在腾讯混元大模型文生图的应用中，推理耗时从原本的10秒缩短至3至4秒。</p><p>&nbsp;</p><p>目前，相关能力已通过腾讯云对外开放。基于升级后的Angel机器学习框架，腾讯云TI平台可提供更优的训练和推理加速能力，并支持客户用自己的数据一站式训练精调，基于腾讯混元大模型打造专属智能应用。</p><p></p><h3>一站式应用构建</h3><p></p><p>&nbsp;</p><p>现在的深度机器学习平台已经与之前有了本质的区别：通常大厂商或基础厂商提供基础模型，用户基于这个基础模型做精调，做专属模型的训练，之后再围绕精调后的专属模型做应用构建，搭建一个能释放这个模型推理理解能力的应用框架，嵌入到自己的APP或业务流程中。</p><p>&nbsp;</p><p>为此，Angel还提供了从模型研发到应用落地的一站式平台，支持用户通过API接口或精调等方式快速调用腾讯混元大模型能力，加速大模型应用构建。</p><p>&nbsp;</p><p>现在的整个生产链路新增加了五个能力：提供基座模型、数据能力、精调、评估和一键部署。模型接入层面，腾讯混元大模型提供千亿、百亿、十亿多个尺寸的模型，来适配各类应用场景的需求；数据处理上，提供清洗、分类、质质检和筛选等数据能力；精调方面，提供LORA调参、全参精简调参、全参全量调参等多种调参模式。</p><p>&nbsp;</p><p>据悉，腾讯内部已有超过300项业务和应用场景接入腾讯混元大模型内测，相比上个月数量翻了一倍，覆盖文本总结、摘要、创作、翻译、代码等多个领域。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VyXbmPO3scWlqDhJq0z8</id>
            <title>好用的不通用，通用的不好用，金融落地大模型需要“专业型”选手</title>
            <link>https://www.infoq.cn/article/VyXbmPO3scWlqDhJq0z8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VyXbmPO3scWlqDhJq0z8</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 10:03:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 金融行业, 场景, 人才
<br>
<br>
总结: 大模型在金融行业的应用面临着挑战和壁垒，包括监管安全、算力、数据和人才等方面。金融机构需要在合规和监管政策下进行规模化落地，同时评估适合自身场景的算力需求。在人才方面，需要拥有算法、工程、产品和业务等知识的多面手。 </div>
                        <hr>
                    
                    <p></p><blockquote>采访嘉宾&nbsp;| 张杰博士 中关村科金技术副总裁&nbsp;</blockquote><p></p><p></p><h5>核心要点：</h5><p></p><p>大模型在特定场景下的专业性暂时没有得到足够提升；大模型时代的“魔咒”——好用的不通用，通用的不好用；企业落地大模型需要懂算法、工程、产品和业务等知识的六边形人才；金融应用大模型不需要“全能型选手”，更多场景下需要把不同技术路线和大模型组合在一起；应对“幻觉”问题，可以把碎片化、低频知识外挂到向量数据库。</p><p></p><p>金融行业由于在数字化领域的多年持续探索和实践，数据体量、类型和质量基础较好，加上拥有丰富的业务场景，为大模型的商业化落地应用提供了温热而肥沃的“土壤”。但与此同时，金融行业也有其独特的行业属性和监管要求，这使得大模型在金融领域的应用过程同样充满诸多不可避免的挑战。</p><p></p><p>在日前接受 InfoQ 采访时，<a href="https://www.infoq.cn/article/liKhGojkqJeRqSRFpzW5?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">中关村科金</a>"技术副总裁 &amp; TGO 鲲鹏会学员张杰博士介绍，目前，金融行业普遍聚焦在初步引入大模型阶段，其中有两个场景落地较快：一是面向企业内部员工的文档问答，另一个是面向各个部门所提需求的自然语言转查询。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5548">张杰博士</a>"表示，在具体落地过程中，除了监管之外，算力、数据、场景、人才等方面的挑战也普遍存在。其中，比较棘手的问题是数据和人才。“在具体场景下，需要专门的指令对数据进行微调。对此，一方面企业需要有自己的场景来逐渐积累；另一方面，可能需要考虑通过行业联盟，共享数据。”</p><p></p><p>针对人才，张杰博士认为，具体场景对模型调校的经验要求较高，不仅需要算法能力，还需要考虑如何实现算法工程化，结合具体业务进行落地。因此，需要既懂算法、又懂工程、产品和业务等知识的六边形人才。</p><p></p><p>“通用大模型和领域大模型可以类比为一个智商较高、通用语言理解能力非常强的高中生和一个专业性更强的研究生。”张杰强调，“对于金融行业来说，需要的是一个‘研究生’。因为你不需要一个既懂天文又懂地理的大师来帮你卖产品。在更多场景下，需要的是一种组合式创新，把不同技术路线和大模型组合在一起。”</p><p></p><p>以下是对话全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>InfoQ：目前，大模型在金融领域的应用普遍集中在哪些场景，为什么是这些场景？</h5><p></p><p></p><p>张杰：目前，金融行业普遍聚焦在初步引入大模型阶段，其中有两个场景落地较快：一是面向企业内部员工的文档问答，另一个是面向各个部门所提需求的自然语言转查询。</p><p></p><p>许多金融企业内部积累了各种 PDF 文档，但大部分不能统一管理，即使统一管理，也没有将其中的知识点特别好地抽取出来。</p><p></p><p>以<a href="https://www.infoq.cn/article/9FMgIHyzeu91hlDNu4VE?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">财富管理公司</a>"的理财场景为例，该场景对理财师专业要求非常高，很多一线销售可能并没有那么专业，因为背后涉及的基金品类非常多，过去，他们在销售过程中往往需要求助中后台人员，但时效难以保障。而通过大模型技术，就能够以较低成本高效地完成这件事。</p><p></p><p>分布在全国门店的销售人员在遇到问题后，可以直接询问智能助手，得到快速解答。根据目前我们的客户反馈，智能助手的反馈准确率已经达到人工标准，并且效率也大大提升。</p><p></p><p>总结而言，由于行业监管等原因，现在金融企业选择的场景，不只要具备专业性，还要具备高容错性。在这两个维度交叉下，做文档问答、赋能内部员工是一个较好的切入点。</p><p></p><h5>InfoQ：我们看到目前很多已经广泛落地的场景实际上并不是金融机构的核心应用，这是否意味着大模型距离深入金融业业务层面还有一定距离？或者业务层面并没有明显需求？</h5><p></p><p></p><p>张杰：事实上，并不是大模型在技术层面还不能服务于金融业的核心业务，而是任何一个新事物出现，在落地过程中都会有一定周期。</p><p></p><p>今年大模型的关注点主要聚焦在通用场景，但是在特定场景下，它的专业性可能暂时没有得到足够的提升。另外，金融领域对合规<a href="https://www.infoq.cn/article/Hx2R2OGwCmOV4gsmPiKE">安全</a>"和数据隐私方面的要求更高，所以要在核心应用落地大模型还需要一些尝试和探索。当然，除了对内的试水之外，也有少数金融企业正在尝试把大模型应用到对外服务业务中。</p><p></p><p>比如，在主动营销场景，交互的目标是相对确定的（如信用卡开户等），客户回答的内容也基本上是有限的，如果超出有限范围智能助手就可以停止触达或者把话题牵引回来。并且，当企业能够获取客户的更多信息，还可以灵活实现个性化的营销。在这背后，只需要在前端接一个语音转文本，后端再接一个语音合成，基本上就可以基于大模型完成营销任务。</p><p></p><p>这种方式跟传统的智能手段相比，不但个性化程度更高，应对能力更强，同时和用户交互的对话轮次也更多。</p><p></p><h5>InfoQ：在您看来，大模型在金融行业规模化落地的具体挑战和壁垒包括哪些方面？金融机构该如何应对？</h5><p></p><p></p><p>张杰：首先，最主要的还是监管安全。大模型的鼓励政策和监管政策并行，比如，今年 7 月份七部门就联合发布了《生成式人工智能服务管理暂行办法》。</p><p></p><p>除了监管之外，算力、数据、场景、人才等方面的挑战也在许多企业中都普遍存在。其中，算力问题是相对好解决的，企业应用场景一旦确定，就可以评估出自己需要多少算力，因为并不是每个场景都要用到上千亿级、百亿级参数的大模型，有的可能几十亿级别就够了。</p><p></p><p>比较棘手的问题是数据和人才：</p><p></p><p>其中，<a href="https://www.infoq.cn/article/hSjZFzB98kH74Jrm5XJH">数据</a>"需要积累，容易解决的是预训练数据部分，但指令数据部分是比较难的，对数据质量要求更高。因为大模型时代仍然面临一个法则——好用的不通用，通用的不好用。</p><p></p><p>大方向上，大模型通用能力非常强，可以与你交流、作诗和绘画。然而，在具体场景下，如果想要把准确度调整到 95％，难度还是非常大的，可能需要专门的指令对数据进行微调。对此，一方面企业需要有自己的场景来逐渐积累；另一方面，可能需要考虑通过行业联盟，共享数据。</p><p></p><p>另外，行业所需的人才是比较稀缺的，具体场景对模型调校的经验要求较高，不仅需要算法能力，还要考虑如何实现算法工程化，结合具体业务进行落地。因此，需要既懂算法、又懂工程、产品和业务等知识的六边形人才。</p><p></p><h5>InfoQ：在通用大模型爆火之前，一些专用的 AI 模型已经被广泛范围使用，那么，如果领域大模型就能解决的问题，是否还有必要使用通用大模型？</h5><p></p><p></p><p>张杰：通用大模型和领域大模型可以类比为一个智商较高、通用语言理解能力非常强的高中生，和一个专业性更强的研究生。前提是，通用大模型已经打下了比较好的基础。</p><p></p><p>对于金融行业来说，我认为需要的是一个“研究生”。因为你不需要一个既懂天文又懂地理的大师来帮你卖产品。在更多场景下，需要一种组合式创新，把不同技术路线和大模型组合在一起。比如，大模型加上传统模型、基于规则的方法以及规则引擎或关键词匹配等等，因为在具体落地场景时还要考虑性价比。</p><p></p><h5>InfoQ：您认为金融机构如何在大模型训练投入与效益、效能间实现平衡？</h5><p></p><p></p><p>张杰：金融机构在立项时会权衡投入产出比，需要设置中间的业绩指标、过程指标等等。</p><p></p><p>其中，过程指标设置相对容易，以文档问答为例，主要看大模型对 PDF 文档或图片解析的准确度，以及解析完成后大模型问答的准确率，这些都可以用一些技术指标来衡量。</p><p></p><p>而衡量业绩指标最简单的方法是与人工进行比较。例如与人工坐席或与后台职能部门的人员效率进行比较，或者与软硬件成本以及人员成本比较。</p><p></p><p>当然，不同企业对投入产出比的衡量指标不太一样。有的企业把大模型视为战略性投入，所以试错容忍度更高。有的企业则不一样，他们会非常关注周期，如短期、中期、长期等不同阶段的成果。具体来说，可以先找到一个具体场景，设定一个破冰期（通常是半年左右的时间），让公司内部人员看到大模型在降本增效方面的价值，然后再进一步推广落地。</p><p></p><h5>InfoQ：对于企业尤其是金融企业而言，“幻觉”是必须克服和的。您在 FCon 演讲中分享的“外挂知识库技术”，它是如何解决这个问题的？</h5><p></p><p></p><p>张杰：对于人脑来说，可以分为两个系统：一个是直觉系统，主要处理直觉本能的问题，可以快速地通过潜意识进行判断，计算效率非常高；另一个是逻辑系统，主要进行推理和深思熟虑的决定。</p><p></p><p>大模型背后是神经网络，类似于人脑的直觉系统，能够很好地理解语言。但这种神经网络结构不擅长处理大百科类的低频长尾实时性知识。这时候，可以把这类低频长尾的知识，放到“外挂知识库”里进行存储，也就是<a href="https://www.infoq.cn/article/TXWfVNYFWQPfPS1WKIbb?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">向量数据库</a>"，在需要时再到其中进行资料查找。</p><p></p><p>总结而言，关键就在于知识的实效性。对于那些普遍的概念和知识更新换代周期较长的知识，需要采用预训练手段或指令微调的方法，将其内化到大模型中；而对于碎片化、低频知识，则可以外挂到知识库中。</p><p></p><p>此外，还有一类时效性更短的信息。例如信用卡营销场景，用户一个小时前在 APP 上操作，操作到一半就中断了，这时营销人员可能需要给他打电话，询问是否继续进行信用卡申请等等，进而促进转化。</p><p></p><p>针对这些需要实时查询的数据，既不应该放到大模型内部，也不应该放到知识库里。可以在大模型外部挂载一个领域知识库，然后设置后处理模块来判断哪类实时性知识需要在大数据平台上实时查询。替换这些槽位后，还可以根据不同客户的信用额度授权，以及信用等级确定借款利息等。</p><p></p><h5>InfoQ：可不可以介绍一个我们具体的一个客户案例？</h5><p></p><p></p><p>张杰：以我们的一个客户——某财富管理公司为例，他们主要从事财富管理业务，涉及海内外的基金产品众多，而且还不断有新产品上架和老产品下架，对理财师信息积累和更新要求非常高。过去都是依赖后台人员帮忙查资料，但后台只有几十个人，需要服务前台几千个员工，效率较低，并且也无法做到 7x24 小时全天候支持。</p><p></p><p>现在，只要把基金产品资料输入到大模型中，员工就可以直接通过和知识助手询问得到答案。他们有一个“小诺”机器人，可以在员工的 App 或企业微信里提供服务。据统计，通过“小诺”，每个问题的平均回答效率提升了大约 10 倍，对于后台服务人员的成本投入也有一定程度降低。</p><p></p><h5>InfoQ：在您的 FCon 主题演讲中，还提及“大模型时代下的新型人机协同”，这将是一种什么样的人机关系？如何在这一过程中，把人与机器的价值都发挥到最大化？</h5><p></p><p></p><p>张杰：中关村科金最早的业务是做各种场景的 AI 对话，无论是营销、客服还是面向员工的培训陪练，有很多与 C 端用户交互的场景。这些场景原来很多工作都是靠人或机器辅助的，现在有一部分就可以完全交给大模型去做。</p><p></p><p>比如营销和客户服务场景，原来对客户的分群营销主要依赖后台的客户运营的人员不断进行 AB 测试。但是，客户标签数量非常多，可以达到数百个，而标签的排列组合数量还要更多，测试工作复杂度比较高。</p><p></p><p>现在，通过人和机器的协作，就可以大大提升这项工作的效率和效果。具体来说，客户分群策略可以通过强化学习来探索，然后针对每个客群，哪些由人先触达，哪些由机器先触达，人和机器在其中如何分工协作，都要有对应的策略。比如，对于高潜、高净值、转化概率比较高的客户，就可以先由人去触达，机器做跟进；相反，就可以先由机器触达，减少人力成本的浪费。</p><p></p><p>最后是话术的分布，比如信用卡营销、老客户交叉销售等等，应该先说什么再说什么；以及业绩结果如何分析，如何根据业绩结果反过来调整客户分群策略、话术 SOP 设计等等，这些都可以用大模型来实现。</p><p></p><p>通过这种全媒体方式，由人和机器共同完成一单销售。其中，大模型大脑主要负责智能决策，在具体工作中，小模型又会和人进行无缝协作。</p><p></p><h5>InfoQ：对于金融业大模型落地应用您还有什么样的期待和展望？您认为据此当下企业该做好哪些准备？</h5><p></p><p></p><p>张杰：我觉得在金融行业，很多场景都可以用<a href="https://www.infoq.cn/article/jT23W6bD7qmk5OpZRf8P">大模型再做一遍</a>"。金融行业是知识密度比较高的行业，并且拥有更好的数字基础，对数字化、智能化转型更为迫切，决策层也很认同，因此会是大模型比较好的行业切入点。对于具体企业来说，应该更加重视生态建设，因为大模型使得许多能力组建能够快速完成，封装完成后还可以进行复用和共享，快速地编排组装到一起，从而产生新的应用场景，对于这样的应用组件，生态是非常重要的。</p><p></p><h5>嘉宾介绍</h5><p></p><p>张杰博士，中关村科金技术副总裁 &amp; TGO 鲲鹏会学员。天津大学本硕博学位，主要研究方向为知识工程、自然语言处理，曾出版《知识中台》《知识图谱》两部技术专著，发表学术论文十余篇，发明专利一百余项，主持或参与国家级课题八项，获第十届吴文俊人工智能技术发明一等奖。主持开发过推荐引擎、知识问答系统、客服机器人、大数据风控系统、行业知识图谱等多项商业系统，累计销售额数亿元。</p><p></p><h5>报告下载</h5><p></p><p>《2023 银行数字化转型报告》正式发布，本报告重点探索作为金融服务核心的银行机构，在面临用户消费模式变化、银行业务结构调整、新兴金融机构竞争加剧等因素的影响下，如何推进五大重点场景的数字化转型，并总结输出大中小型银行的两条数字化转型路径，期望为不同类型和规模的银行业企业提供研究内容支撑。关注「InfoQ 数字化经纬」公众号，回复「银行报告」免费获取。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2c/64/2c468684a87c935f1c8ab83cf111c164.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/iiimrfMAFwPrHDKh6JxO</id>
            <title>【技研录】前端组件化开发思考</title>
            <link>https://www.infoq.cn/article/iiimrfMAFwPrHDKh6JxO</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/iiimrfMAFwPrHDKh6JxO</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 09:37:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 王随鹏, 前端开发工程师, 组件化, 前端生态
<br>
<br>
总结: 本文介绍了前端开发工程师王随鹏的职业背景和经验，以及前端组件化的重要性和原因。文章指出，随着技术的发展和业务的增长，传统的开发方式效率低下且维护成本高，因此需要将复杂的前端应用拆分成小的组件，实现模块化和组件化的开发方式。同时，文章还提到了前端生态中的三大框架React、Angular和Vue都提供了组件化的能力。最后，文章介绍了组件化的定义和演变过程，从函数化编程思想到模块化编程思想再到组件化编程思想，每个阶段都有不同的特点和优势。 </div>
                        <hr>
                    
                    <p>笔者介绍</p><p>王随鹏，目前任职于众安金融事业部前端开发工程师，主要参与某企业小程序、新媒体运营、GMP 营销流程等相关业务的开发。</p><p></p><p>文章目录</p><p>一、前言</p><p>二、为什么实施前端组件化</p><p>三、什么是组件化</p><p>四、组件化的演变</p><p>五、组件化的使用背景</p><p>六、组件化的职能划分</p><p>七、组件化在现代项目中的实现</p><p>八、组件化的价值</p><p>九、总结</p><p></p><p></p><h2>一、前言</h2><p></p><p></p><p>组件化对于任何一个业务场景复杂的前端应用以及经过多次迭代之后的产品来说都是必经之路。<a href="https://www.infoq.cn/article/OPG06ruhPvj5P47yziyI?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">组件化</a>"要做的不仅仅是表面上看到的模块拆分解耦，其背后还有很多工作来支撑组件化的进行，例如结合业务特性的模块拆分策略、模块间的交互方式和构建系统等等 。</p><p></p><p>组件化并不是前端独有的，当今<a href="https://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng%3D%3D&amp;chksm=f951a825ce262133031d8a3f4b0cff8f476361e71ba29883568f3fd154fef6eb8b5f929292b6&amp;idx=2&amp;mid=2247490918&amp;scene=27&amp;sn=7d94c41b7f733237b0fe51bd1d791d93&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">前端</a>"生态里面，React、Angular 和 Vue 三分天下。虽然这三个框架的定位各有不同，但是它们有一个核心的共同点，那就是提供了组件化的能力。</p><p>&nbsp;</p><p></p><h2>二、为什么要实施前端组件化</h2><p></p><p>&nbsp;</p><p></p><h4>1．从前端发展过程思考</h4><p></p><p>&nbsp;</p><p>（1)&nbsp;随着技术的发展，开发的复杂度也越来越高，传统开发模式总是存在着开发效率低，维护成本高等弊端。</p><p></p><p>（2)&nbsp;传统开发方式效率低以及维护成本高的主要原因在于很多时候是将一个系统做成了整块应用，而且往往随着业务的增长或者变更，系统的复杂度会呈现指数级的增长，经常出现的情况就是一个小小的改动或者一个小功能的增加可能会引起整体逻辑的修改，造成牵一发而动全身。</p><p></p><p>（3)&nbsp;我们希望一个大且复杂的场景能够被分解成几个小的部分，这些小的部分彼此之间互不干扰，可以单独开发，单独维护，而且他们之间可以随意的进行组合。</p><p>&nbsp;</p><p></p><h4>2．从效率的角度思考</h4><p></p><p>&nbsp;</p><p>（1)&nbsp;迭代速度慢，公共代码相互耦合，需要全量回归；</p><p>（2)&nbsp;多人协作是极其困难的一件事；</p><p>（3)&nbsp;代码冲突多，每次提交代码可能需要解决冲突；&nbsp;</p><p>（4)&nbsp;版本风险高，修改会影响很多需求之外的功能。</p><p>&nbsp;</p><p></p><h4>3．从技术的角度思考</h4><p></p><p></p><p>（1)&nbsp;代码整体结构混乱、缺少层次；</p><p>（2)&nbsp;优秀的代码应该是高内聚，低耦合；</p><p>（3)&nbsp;龟速编译，开发体验极差；</p><p>（4)&nbsp;无法很好地支持 A/BTest；</p><p>（5)&nbsp;每次发版在 QA 回归上耗时很久。</p><p>&nbsp;</p><p></p><h2>三、什么是组件化</h2><p></p><p>&nbsp;</p><p>前端的组件化，其实是对项目进行自上而下的拆分，把通用的、可复用的功能以黑盒的形式封装到一个组间中，然后暴露一些开箱即用的函数和属性配置供外部组件调用，实现与业务逻辑的解耦，来达到代码间的高内聚、低耦合，实现功能模块的可配置、可复用、可扩展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/27781568e7cf9ecc5c33f10b6d821c39.png" /></p><p></p><p></p><h2>四、组件化的演变</h2><p></p><p></p><p></p><h4>1．函数化编程思想</h4><p></p><p></p><p>（1)&nbsp;以<a href="https://www.infoq.cn/article/2017/11/era-functional-program-language?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">函数</a>"（方法）来分离行为；</p><p>（2)&nbsp;每个函数仅在做一件事情。</p><p></p><p>如下图：</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/71c20be1657ea826c04bf54c988a9cfd.png" /></p><p></p><p></p><h4>2．模块化编程思想</h4><p></p><p></p><p>（1)&nbsp;以模块（js 文件）来分离行为；</p><p>（2)&nbsp;每个模块负责一类事情。</p><p></p><p></p><h4>3．组件化编程思想</h4><p></p><p></p><p>（1)&nbsp;以组件来分离行为；</p><p>（2)&nbsp;每个组件拥有独立的结构、视图和行为，代表一个完整的个体。</p><p></p><p>如下图：</p><p><img src="https://static001.geekbang.org/infoq/1b/1b7a9f83286328237216dac7f4cbe1e3.png" /></p><p></p><p></p><h2>五、组件化的使用背景</h2><p></p><p></p><p></p><h4>1．业务的迭代和堆积</h4><p></p><p></p><p>（1)&nbsp;单个文件有上千行代码，可读性非常差，维护不方便；</p><p>（2)&nbsp;有大量重复的代码，相同或者类似的功能实现了很多遍&nbsp;；</p><p>（3)&nbsp;新功能的开发成本巨大。</p><p></p><p></p><h4>2．场景的多样化</h4><p></p><p>&nbsp;</p><p>&nbsp;（1)&nbsp;不同的项目，类似的场景；</p><p>&nbsp;（2)&nbsp;相同的项目，越来越多的相同场景。</p><p></p><p></p><h2>六、组件化的职能划分</h2><p></p><p>&nbsp;</p><p>组件最大的不稳定性来自于展现层，一个组件只做一件事，基于功能做好职责划分：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4d1a109f8a0d952503eeb637788e734.png" /></p><p></p><p></p><h4>1．容器型组件</h4><p></p><p>&nbsp;</p><p>一个容器性质的组件，一般当作一个业务子模块的入口，比如一个路由指向的组件；容器型组件需要知道如何获取子组件所需数据，以及这些数据的处理逻辑，并把数据和逻辑通过props提供给子组件使用。容器型组件一般是有状态组件，因为它们需要管理页面所需数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/442db61313964cbb1604409ad71fef18.png" /></p><p></p><p></p><h4>2．展示型组件</h4><p></p><p></p><p>主要表现为组件是怎样渲染的，就像一个简单的模版渲染：</p><p></p><p>（1)&nbsp;只通过 props 接受数据和回调函数，不充当数据源；</p><p>（2)&nbsp;通常用 props.children(react)  或者 slot(vue) 来包含其他组件；</p><p>（3)&nbsp;可以有状态，在其生命周期内可以操纵并改变其内部状态，职责单一，将不属于自己的行为通过回调传递出去，让父级去处理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/796b341ed575a4135b6087069cc5f200.png" /></p><p></p><p></p><h4>3．业务组件</h4><p></p><p></p><p>通常是根据最小业务状态抽象而出，业务组件也具有一定的复用性，但大多数是一次性组件</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e9e26810d5d42ee842bc3fc94e650cc.png" /></p><p></p><p></p><h4>4．通用组件</h4><p></p><p></p><p>可以在一个或多个项目内通用的组件</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/4700112aea444ea974bf3476c4c7ed6c.png" /></p><p></p><p></p><p></p><h2>七、组件化在现代项目中的实现</h2><p></p><p></p><p>我们把一个项目的生命周期分为如下几个阶段：&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f7c841a4bb8ec69852b2035d85cadfb.png" /></p><p></p><p>组件化开发方案主要关注的是在迭代开发阶段的对团队效率的提升。</p><p></p><p></p><h4>1．单一职责</h4><p></p><p>&nbsp;</p><p>（1)&nbsp;单一职责强调一个组件具备一项“能力”。</p><p>（2)&nbsp;单一职责可以保证组件是最细的粒度，且有利于复用。但太细的粒度有时又会造成组件的碎片化。因此单一职责组件要建立在可复用的基础上，对于不可复用的单一职责组件，我们仅仅作为独立组件的内部组件即可。</p><p>（3)&nbsp;单一职责同时也具备简化组件的能力，遵守该原则在一定程度上能够使代码足够简单，意味着易读、易维护。</p><p></p><p></p><h4>2．封装</h4><p></p><p>&nbsp;</p><p>（1)&nbsp;良好的组件封装应该隐藏内部细节和实现意义，并通过 props 来控制行为和输出。同时还要具备减少访问全局变量能力，因为访问全局变量会打破封装，创造了不可预测的行为。</p><p>（2)&nbsp;封装能够将不用逻辑代码分离，能够帮助开发中快速定位问题。</p><p></p><p></p><h4>3．可配置性</h4><p></p><p>&nbsp;</p><p>（1)&nbsp;一个组件，要明确它的输入和输出分别是什么。</p><p>（2)&nbsp;组件除了要展示默认的内容，还需要做一些动态的适配，比如：一个组件内有一段文本，一个图片和一个按钮；字体的颜色、图片的规则、按钮的位置、按钮点击事件的处理逻辑等，都是可以做成可配置的。</p><p></p><p></p><h4>4．组合</h4><p></p><p></p><p>（1)&nbsp;单一责任原则描述了如何将需求拆分为组件，封装描述了如何组织这些组件，组合描述了如何将整个系统粘合在一起；</p><p>（2)&nbsp;具有多个功能的组件，应该转换为多个单一职责的小组件，这些小的组件又可以组合成为一个职责更大、功能单一的组件。</p><p></p><p></p><h4>5．复用</h4><p></p><p></p><p>（1)&nbsp;通常来说我们进行组件设计的目的有两种：</p><p></p><p>a. 抽取公共功能部分，方便复用；</p><p>b. 复杂设计/功能分解，便于代码管理和提高代码阅读性。</p><p></p><p>（2)&nbsp;提高组件的复用性，使得一处代码的封装能够在各个不同的地方使用。复用性能够使代码的修改/编辑更加方便，只需要修改组件代码，各个引用的地方会同步进行修改和更新。</p><p></p><p></p><h4>6．可测试</h4><p></p><p></p><p>1) 现在前端开发过程中一直都在强调单元测试，一个完成的项目单测是不可缺少的一部分，单测可以保证代码正确性、一部分依赖的正确性、以及减少调试时间等。</p><p>2)&nbsp;单元测试的目的不是为了减少代码覆盖率，而是为了减少 bug 出现的概率，以及防止 bug 回归。</p><p></p><p></p><h2>八、组件化的价值</h2><p></p><p></p><p></p><h4>1．技术价值</h4><p></p><p></p><p>（1)&nbsp;降低系统的耦合度：在保持接口不变的情况下，我们可以把当前组件替换成不同的组件实现业务功能升级。</p><p>（2)&nbsp;提高可维护性：由于每个组件的职责单一，在系统中更容易被复用，所以对某个职责的修改只需要修改一处，就可获得系统的整体升级。</p><p>（3)&nbsp;降低上手难度：新成员只需要理解接口和职责即可开发组件代码。</p><p>（4)&nbsp;利于调试：由于整个系统是通过组件组合起来的，在出现问题的时候，可以用排除法直接移除组件，或者根据报错的组件快速定位问题。</p><p>（5)&nbsp;提升团队协同开发效率：通过对组件的拆分粒度控制来合理分配团队成员任务，让团队中每个人都能发挥所长，维护对应的组件，最大化团队开发效率。</p><p></p><p></p><h4>2．业务价值</h4><p></p><p></p><p>（1)&nbsp;组件的复用和移植，减少开发人力；</p><p>（2)&nbsp;便于组合不同的场景和业务；</p><p>（3)&nbsp;促进业务安全和快速迭代。</p><p></p><p></p><h2>九、总结</h2><p></p><p></p><p>组件化并非一蹴而就，而是一个持续的过程。在沉淀业务组件的同时还需考虑组件包的大小，不能因为组件包的体积大而导致页面加载过慢，以及组件发布前的测试等。但可以通过一些方法和规范去解决挑战，让组件化设计更好的服务于系统。所以，理解组件化可以帮助开发者更好地使用框架进行工作内容的拆分和维护，才能在实际开发中结合具体的业务场景，设计出合理的组件，实现真正的前端组件化。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/liKhGojkqJeRqSRFpzW5</id>
            <title>企业大脑背后的“双系统”：中关村科金以“大模型+知识库”打造技术应用新范式</title>
            <link>https://www.infoq.cn/article/liKhGojkqJeRqSRFpzW5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/liKhGojkqJeRqSRFpzW5</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 09:31:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 大模型技术, 中关村科金, 企业知识大模型, AgentGraph应用开发平台
<br>
<br>
总结: 在这个时代，AI和大模型技术的发展引领着人类社会向未来前进。中关村科金发布了企业知识大模型和AgentGraph应用开发平台，为企业提供智能化、高效化的解决方案。企业知识大模型结合领域知识库，让企业拥有自己专属的大模型，提升知识管理和利用效能。AgentGraph应用开发平台是一款零代码开发平台，提供快捷、高效、安全的解决方案，帮助企业开发自己的大模型应用。 </div>
                        <hr>
                    
                    <p>在这个日新月异的时代，我们无时无刻不在见证着AI和大模型技术的飞速发展。它们如同一座座高塔，引领着人类社会向着未来前进。这些技术的重要性在于为人类的生活和工作带来了前所未有的变革和进步，让世界更加智能化、高效化。</p><p>&nbsp;</p><p>11月23日，由中关村科金与中国信通院人工智能创新中心、人工智能关键技术和应用评测工业和信息化部重点实验室联合主办的2023大模型产业前沿论坛正式启幕。</p><p>&nbsp;</p><p>作为一家对话式AI技术解决方案提供商，中关村科金利用其多年的人工智能技术积累和行业经验，为各行业提供了智能化、高效化的解决方案，帮助客户提升效率、降低成本、提高用户体验。</p><p></p><p>在本场发布会上，中关村科金发布了国内首个企业知识大模型、AgentGraph应用开发平台，以及“超级员工”助手系列AIGC应用，为企业提供开箱即用、系统无缝衔接、成本可负担的专属领域大模型服务。</p><p></p><h2>企业知识大模型：让每个企业都拥有自己专属的大模型</h2><p></p><p></p><p>“让每个企业都拥有自己专属的大模型”是中关村科金在大模型领域践行的产品和技术理念。</p><p></p><p>与市场上其他领域大模型不同的是，中关村科金的企业知识大模型不仅具备了领域大模型的基本能力，还拥有一个知识密度较高的领域知识库。</p><p>&nbsp;</p><p>“领域大模型+领域&nbsp;知识库”这一路线，核心是为了利用大模型的理解能力，将散落在企业内外部各类数据源中的事实知识和流程知识提取出来，然后再利用大模型的生成能力输出长文本或多轮对话。</p><p></p><p>之所以要把大模型的直觉系统和领域知识库相结合，是因为此前市场上的前几代产品没有知识库的支持都会有一些缺陷。例如，第一代文档管理产品仅限于简单地给文档打标签，对于文档中的具体内容并不理解。尽管第二代的知识图谱能够提供比文档管理产品更加准确的信息，但其对于知识的覆盖范围较低，因此在实际应用中效果不佳。大模型兴起后，信息查询的覆盖度和准确率才得到了大幅提升。</p><p></p><p>增加了领域知识库的大模型让企业知识大模型同时拥有类似人类左脑的直觉系统和类似右脑的可存储长尾碎片化知识的知识系统。</p><p></p><p>有了知识库的加持，知识密集型企业可以应用大模型技术构建智能知识中台，高效打造全媒体销服一体化应用，提升企业知识管理和利用效能，拓展认知深度和支持复杂决策。</p><p></p><p>在人工智能领域，知识图谱和预训练语言模型是两个重要的基础技术。算法团队出身的中关村科金技术团队有着多年知识图谱方面的技术经验，并且也一直深耕预训练模型的研发，积累了丰富的模型微调经验，使其能够应用于各种场景，包括知识问答、营销、金融等场景。</p><p></p><p>最近两年，大语言模型呈现出规模体量大、复杂度高的趋势，训练一款大模型需要更多的数据、计算资源和优化技术来支持，这就把很多企业困在了这场“AI竞赛”的大门之外。</p><p>&nbsp;</p><p>微调大模型需要积累大量的对话数据和经验，通过对任务的分解、掺混和优化，可以提高模型的效率和性能，而这些正是中关村科金技术团队的优势所在。</p><p>&nbsp;</p><p>因此，将这项能够降低大模型微调门槛的技术普惠出去成为中关村科金的当务之急，这也是为什么中关村科金选择在这样的时间点上发布这一技术路径的原因。</p><p></p><p>作为国内首个企业知识大模型，其主要具备以下优势：</p><p>&nbsp;</p><p>准确度高：模型能力获中国信通院权威机构认证，具备全面的语义、语音、视觉等基础能力，在5个能力域、46个能力项评测中准确度达95%+，已达到当前国产大模型最高能力级别，全面升级企业营销服务价值，提升员工工作体验；数据丰富：预置2000亿token预训练数据，2000万条指令数据；安全可控：通过数据清洗、隐私脱敏、合规加训、提示控制、质检过滤等多种技术手段实现模型安全可控；成本可负担：灵活适配10+大模型，指令数据可持续积累，实现单周迭代、单卡推理，为企业提供开箱即用的智能工具，加深业务与技术的耦合，提升生产力并降低成本；</p><p></p><p>据中关村科金技术副总裁张杰介绍，企业知识大模型现已经可以完全覆盖企业营销服务场景。耗费10万卡天算力，整理出原子任务间的协同增益关系图，从而高效微调，避免灾难性遗忘，极大缩短了微调一款通用大模型的时间并且降低了算力成本。</p><p>&nbsp;</p><p>依托多年的技术积累，中关村科金走通了一条“领域大模型+领域知识库”的底层技术之路。在企业知识大模型基础上，顺势生长出了AgentGraph应用开发平台。、</p><p></p><h2>AgentGraph应用开发平台：零代码、3分钟搭好一款应用</h2><p></p><p></p><p>AgentGraph是一款基于中关村科金企业知识大模型的一站式、全链路的AI Native应用开发平台。AgentGraph提供多项核心开发能力，包括开放的大模型调用、可视化任务流程编排、AI及系统工具组件、数据处理及存储、低代码应用构建，为企业专属大模型应用提供快捷、高效、安全的解决方案。</p><p>&nbsp;</p><p>以大语言模型为底座的应用开发平台近两年来备受追捧，多家国际科技巨头和科研机构都在该领域加大了研发和投入。OpenAI、谷歌、Meta等公司都结合自家大模型产品构建了较为完善的应用平台生态体系。</p><p></p><p>与他们不同的是，中关村科金这款AgentGraph平台底座是面向AI原生应用的零代码开发平台。据张杰介绍，AgentGraph平台场景（Vertical Agent）里封装了200多个场景KnowHow，犹如为大模型装上了专属、定制化的Agent OS“操作系统”，为企业打造下一站AI Native入口。</p><p>&nbsp;</p><p>AgentGraph应用开发平台也具备了多项开箱即用的功能，例如开发者通过拖拉拽可实现的零代码流程编排画布，最快3分钟完成一款应用搭建，极大降低了企业内部应用创新成本；此外，AgentGraph平台还具备组件和应用的运行管理能力，开发者能够在AgentGraph平台上贡献自己的AI Native应用，避免重复造轮子。</p><p>&nbsp;</p><p>更重要的是，从设计之初，AgentGraph应用开发平台就将开放、易用奉为圭臬。它不仅能支持10+开源大模型调用、兼容Lang Chain等开源框架，还允许API直接接入，并且支持包括常见文档、网页、办公工具在内的多源数据。</p><p></p><p>张杰称，未来公司将计划开源AgentGraph应用开发平台开源，让内部能力组件可以帮助更多企业低成本、快速地开发出新的大模型应用。</p><p>&nbsp;</p><p></p><blockquote>“开源应用开发平台的意义在于提供了一种自由、灵活和创新的方式来开发应用程序。在开源平台上，开发者可以自由地修改和定制软件，满足自己的需求，同时也可以学习和借鉴其他开发者的经验和技巧，提高自己的技术水平。此外，开源应用开发平台还可以促进开发者之间的合作和交流，构建完整的技术生态链，推动整个技术领域的发展和创新。”</blockquote><p></p><p></p><h2>“超级员工”助手系列AIGC应用：打通数智化升级“最后一公里”</h2><p></p><p></p><p>事实上，以大模型为底座的AI Native应用的发展历程并不长，但其在短短几年内已经取得了令人瞩目的成就。目前，AI Native应用已经在自然语言处理、图像识别、语音识别等领域都取得了重大进展，为人们的生活和工作带来了诸多便利。</p><p>&nbsp;</p><p>中关村科金基于国内首个企业知识大模型打造了“超级员工”助手系列AIGC应用，包括ChatPilot知识助手、培训助手、合规助手、服务助手、营销助手、写作助手、数据助手等，为企业提供开箱即用、系统无缝衔接、成本可负担的AI创新应用。</p><p>&nbsp;</p><p>“超级员工”助手系列AIGC应用具备阅读（信息抽取）、倾听（语言理解）、撰写（文本生成）、规划（思维链）等关键技能，在营销文案生成、客服问答、座席助手等场景下发挥了巨大价值。</p><p>&nbsp;</p><p>目前，中关村科金打造的“超级员工”助手系列AIGC应用已在各个场景落地。其中，在智能客服场景下表现尤为亮眼。</p><p>&nbsp;</p><p>张杰举例说，“与传统的智能客服相比，大模型进一步降低了开发和运维成本。以前，各种场景都需要算法工程师标注数据以训练特定任务的模型，因此开发成本较高。现在，企业知识大模型本身的领域专业性已经足够好，不再需要很多算法工程师标数据，可以直接拿过来用，有时稍微标几条数据就够了，大大降低了客服成本。</p><p>&nbsp;</p><p>当下，企业数智化转型已进入深水区。“超级员工”助手系列AIGC应用打通了企业知识应用的“最后一公里”，大幅提高了生产效率、降低了企业运营成本，提升了员工体验，这些优势可以帮助企业在激烈的市场竞争中获得更大的优势和更多的收益。</p><p>&nbsp;</p><p>ChatPilot知识助手：集成四大能力，打造知识应用新范式</p><p>&nbsp;</p><p>作为“超级员工”助手系列AIGC应用之一，ChatPilot知识助手也是中关村科金推出的首款大模型应用。得益于领域大模型、多模态文档解析、知识搜索等核心技术，ChatPilot知识助手能够以句为单位理解用户提问，轻松识别各种文档非结构化信息。</p><p>&nbsp;</p><p>随着近些年大模型的高速发展，对话数据成为企业愈发重视的数据资源。无论在现阶段还是未来，无论是企业与外部客户沟通，还是企业内部员工的培训和协作，对话一直都是最主要、最自然的交互形式。在营销服务、投研投顾、企业运营、科研教学等场景下会产生很多对话数据，这些对话数据充满价值，因为它们能够最直观地体现个体或企业的诉求。这些场景也是ChatPilot知识助手应用最为广泛的场景。</p><p>&nbsp;</p><p>在这些场景中，通过智能化阅读、整理、归纳与AI的交互内容，ChatPilot知识助手可以完成四大核心任务：</p><p>&nbsp;</p><p>知识问答：理解用户问题，即时提供答案，支持答案溯源；抽取QA：基于文档生成一键生成核心内容QA；全文总结：1分钟可完成100页文档关键信息的提取，准确率超90%，快速生成高质量文档摘要；搜索增强：猜测用户意图，智能推荐内容；</p><p>&nbsp;</p><p>在操作上，ChatPilot知识助手可做到开箱即用，与企业业务系统无缝集成，充分赋能各个业务条线，避免重复建设。借助领域知识库和数据加密、隔离等技术，ChatPilot知识助手能在保障企业数据安全的基础上，确保输出的内容准确、可控、可用。</p><p>&nbsp;</p><p>作为专注于对话式AI技术解决方案提供商，随着企业知识大模型的发布，中关村科金成为了“领域大模型+领域知识库”这条路上的先行者。未来，如何发挥自身优势，在领域内积累更多专业数据，沉淀更多领域知识，并将这些数据和知识集成到大模型上，将成为中关村科金最稳固的技术护城河。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/eI6tNmSsCDVfcFshdE7o</id>
            <title>OpenAI“宫斗戏”落幕！最大的赢家不是Altman，也不是微软</title>
            <link>https://www.infoq.cn/article/eI6tNmSsCDVfcFshdE7o</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/eI6tNmSsCDVfcFshdE7o</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:29:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, 宫斗戏, CEO, 董事会
<br>
<br>
总结: OpenAI经历了一场宫斗戏，最终达成了与前CEO的协议，CEO和总裁将回归，董事会也进行了重组。微软的Satya Nadella是这场闹剧中的赢家，而OpenAI内的营利派也获得了胜利。整个事件展示了资本的力量，同时也为人物传记出版商提供了素材。Meta公司可能成为最大受益者，他们的开源项目得到了宣传，AI研究团队备受尊敬。 </div>
                        <hr>
                    
                    <p></p><blockquote>上演了将近一周的“宫斗戏”后，OpenAI 似乎终于回到了正轨。闹剧落幕，谁是大赢家？谁又是大输家？</blockquote><p></p><p></p><h2>OpenAI“宫斗戏”迎来大结局</h2><p></p><p>&nbsp;</p><p>OpenAI 这场“宫斗大戏”经历了多轮反转之后，终于在本周三迎来了大结局。</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34125dff0f132f9b50e7a4ad86b1dd74.png" /></p><p></p><p>11 月 22 日 14 时，OpenAI 官方宣布，已经与上周五被驱逐出公司的前 CEO Sam Altman 达成一项原则性协议，他将回归 OpenAI 继续担任首席执行官。Altman 表示：“我喜欢 OpenAI，过去几天我所做的一切都是为了让这个团队及其使命保持一致。在新董事会和微软 Satya Nadella 的支持下，我期待着重返 OpenAI，并巩固我们与微软的牢固合作伙伴关系。”</p><p>&nbsp;</p><p>此前与 Altman 共进退的 OpenAI 总裁兼联合创始人 Greg Brockman 也将回归 OpenAI，并发布推文称：今天取得了惊人的进展，我们将比以往任何时候都更强大和团结。重返 OpenAI 并将于今晚开始敲代码。随后，Greg Brockman 还发布了与 OpenAI 员工的合影。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/84/84b8b8d6903ab3e354508af72d8cc081.png" /></p><p></p><p>与此同时，OpenAI 董事会也迎来重组。原董事会成员乔治城大学安全与新兴技术中心（CSET）战略和基础研究基金主任 Helen Toner、Fellow Robots 联合创始人 Tasha McCauley 将离开，美国“知乎”Quora 联合创始人 Adam D'Angelo 将留任。此外，曾在 Facebook、推特、赛富时担任过高管的 Bret Taylor 将出任新的董事会主席，投资者们颇为熟悉的美国前财长 Summers 也加入 OpenAI 董事会。</p><p>&nbsp;</p><p>此前明确提出“希望改变 OpenAI 治理架构”的微软公司首席执行官 Satya Nadella 也公开回应称，微软对 OpenAI 董事会的变化感到鼓舞。微软认为这是迈向更稳定、更明智、更有效治理的第一步。对于这次的人事调整，马斯克也第一时间发表评论称：“看看我们编织了一张如此错综复杂的网络”。</p><p></p><h2>谁才是这场闹剧中的大赢家？</h2><p></p><p>&nbsp;</p><p>在这场 OpenAI “宫斗大戏”中，究竟谁笑到了最后一直充满争议。部分声音认为微软及其掌门人 Satya Nadella 才是赢家，而上周五匆忙解雇 Altman、引发这场动荡的 OpenAI 董事会成员则是一败涂地。</p><p>&nbsp;</p><p>彭博社在一篇分析文章中提到，这部大戏的赢家包括 Satya Nadella、Sam Altman、OpenAI 内的营利派、人物传记出版商。而 CNBC 认为，最大的赢家或许是一直充当旁观者的 Meta。</p><p></p><h4>Satya Nadella</h4><p></p><p>&nbsp;</p><p>如果 OpenAI 真的彻底崩溃，接下来的问题就是：作为最大的注资者及合作伙伴，微软怎么会没注意到这股汹涌而来的暗流？即使能招纳 OpenAI 的大部分人才，建立新部门并恢复开发能力也需要耗费可观的时间和金钱。而且由于其他 OpenAI 投资方肯定希望尽量套回价值，所以整个过程也许会陷入诉讼。更重要的是，这帮从 OpenAI 空降而来的新人肯定会招致微软内部老员工的抵触甚至是对抗，毕竟软件巨头之前刚刚进行过几轮裁员。但好消息是，这一切都只是设想。</p><p>&nbsp;</p><p>Satya Nadella 心里恐怕是长舒了一口气，但仍然表现得胸有成竹。在周一接受播客采访时，他明确表态“我们永远不会再次陷入同样的被动境地。”是的，相信他会说到做到。</p><p></p><h4>Sam Altman</h4><p></p><p>&nbsp;</p><p>带着推动 AI 发展的愿景和使命，这位 AI 革命的形象大使无疑是 OpenAI 当前不可动摇的绝对统治者。还有谁能阻止&nbsp;Altman&nbsp;的脚步？而且在新一拨董事会成员众正盈朝式的簇拥下，Altman&nbsp;的地位将更加稳固，他的商业本能也将获得广阔的发挥空间。作为一位典型的创业者，Altman&nbsp;明显不是那种能够久居人下的性格。而且一旦全职受雇于微软，他的外部利益纠葛也会招致麻烦，甚至最终引发冲突。</p><p></p><h4>OpenAI 内的营利派</h4><p></p><p>&nbsp;</p><p>这一派始终认为给营利性实体套上一副非营利组织枷锁的行为纯纯愚蠢，只会引入难以预测的动机和非理性思维（比如解雇声望如日中天的 CEO），而终局态势也让他们成为赢家。其实无论董事会如何操作，只要真正的重量级人物（即掌握资金和底层基础设施的人们）施加压力，OpenAI 董事会看似拥有的“权力”将即刻灰飞烟灭。这，就是资本的力量！</p><p>&nbsp;</p><p>当然，哪怕没有这几十亿美元的支持，OpenAI 可能也会继续存在并找到另外一条出路。但其成功不会像现在这么耀眼、这么辉煌。</p><p></p><h4>人物传记出版商</h4><p></p><p>&nbsp;</p><p>在各种各样特立独行的 CEO 们提供千奇百怪的素材之后，出版商们本来已经没什么猛料可以挖掘了。现在好了，科技创始人中出现了又一位重量级选手，你们的图书又要大卖了。</p><p></p><h4>Meta</h4><p></p><p>&nbsp;</p><p>Meta 公司 CEO 马克·扎克伯格也许没有直接参与上周末开始的这场围绕 Altman 展开的先是粗暴出局、后又闪电回归 OpenAI 的戏剧性事件，但这位社交网络大佬和他掌控的企业却很可能成为最大受益者。</p><p>&nbsp;</p><p>OpenAI 的动荡反而帮助 Meta 宣传了其开源 LLaMA AI 项目。这一方面是因为很多企业本身就希望增强技术多元度，不想依赖单一厂商的大语言模型；另一方面，稳定经营对于希望踏实研发的 AI 技术人员也更有吸引力。</p><p>&nbsp;</p><p>继 OpenAI 推出的爆款 ChatGPT 之后，Meta 正持续大力投资各种生成式 AI 与相关大语言模型。其 AI 研究团队与 Alphabet 旗下的 DeepMind 一样，被视为科技行业最受尊敬的技术团队之一。</p><p>&nbsp;</p><p>必须承认，相当一部分技术人员对高管层面的矛盾斗争没有兴趣。面对这场几乎令 OpenAI 全面崩塌的暴风骤雨，私营领域的从业者可能更倾向于投身 Meta 及其 AI 研究实验室，在这里寻求一片宁静的学术空间。</p><p>&nbsp;</p><p>一位用户在 Meta 新近上线的类 Twitter 平台 Threads 上发帖表示，“每个人都说微软是 OpenAI 事变的最大赢家，但我认为笑到最后的恐怕是 Meta。”该用户在帖子中指出，“如果你是一名 AI 研究人员，而且有意愿在科技巨头工作，那么 Meta 可能正是最理想的开源和公共研究选项。”Meta 公司 AI 负责人 Yann LeCun 则用简洁有力的话语回复了这篇帖子——“没错”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4ce97ebddb10f142608027f954c170c5.png" /></p><p></p><p>此外，还有其他潜在的商机值得发掘。</p><p>&nbsp;</p><p>OpenAI 的政变已经引发企业客户及其高管成员的担忧，他们愈发怀疑是否应该依赖一种大语言模型作为自身 AI 业务战略的根基。已经有多位技术专家在采访中明确表示，此次事件正促使企业减少对 OpenAI 旗下 GPT 系大模型的依赖，转而将 Anthropic 和 Cohere 等初创公司的模型纳入业务体系。</p><p>&nbsp;</p><p>如果企业客户真的开始关注更多 AI 供应商，那么市场形势可能会与云计算一样转为群雄并立，Meta 也将获得可观的业务机遇。该公司一直大力宣传其 LLaMA 生成式 AI 软件家族，相关功能可以通过开源模型免费获取。由于开发人员可以根据自身特定需求访问并定制大模型，且无需受限于特定供应商，所以 LLaMA 的吸引力也在逐渐增强。未来愿意使用和改进 LLaMA 的开发者越多，Meta 的整体运营开支和技术研发成本将有望进一步得到优化。</p><p>&nbsp;</p><p>最后，尽管 LLaMA 也存在着许可制度及其他潜在问题，但对于更多企业和开发者来说，选择使用 Meta 的 AI 软件似乎更令人安心。毕竟社交网络巨头应该不会在几天之内迅速崩溃。</p><p></p><h2>混乱中的输家</h2><p></p><p></p><h4>AI 技术竞赛</h4><p></p><p>&nbsp;</p><p>原本，这场动荡有望开启 AI 产业的新布局，让那群最优秀、最聪明的研发人员能够扩散到更广阔的天地之间。可现如今，一切已成泡影。Salesforce 公司 CEO Marc Benioff 曾直言“欢迎直接把简历发给我”，英伟达等大厂也纷纷伸出橄榄枝，但 OpenAI 及时止住颓势、稳定了自己的人才团队。虽然部分 OpenAI 潜在客户可能会因此寻求更多替代方案，但因为处理速度很快，所以市场上的基本盘几乎没有受到影响。</p><p></p><h4>原 OpenAI 董事会</h4><p></p><p>&nbsp;</p><p>D’Angelo保住了自己的席位，但作为全新的开始，Tasha McCauley、Helen Toner 和 Ilya Sutskever 全部被扫地出局。他们的离去为这几天的闹剧画上了句号，也震惊了整个行业。这种震惊不只来自粗暴解雇 Altman&nbsp;的决定，更因为其策划方式的混乱随意——之前几乎没有通气，也没有说明行此险着的具体原因。Toner 曾发表研究论文对 OpenAI 的安全方法提出批评，据称这会导致公司商誉破裂。也许未来她的警告会被证明极富预见性，但至少在当下，这些并不足以动摇 Altman&nbsp;的江湖地位。离去之际，Toner 和 McCauley 表示一定会对 Altman&nbsp;的领导工作开展独立调查。有道理，但这事不是应该在政变之前搞定吗？</p><p></p><h4>Emmett Shear</h4><p></p><p>&nbsp;</p><p>短短几天之前，这位前 Twitch CEO 肯定还觉得自己是中了头彩。在离开亚马逊公司（于 2014年收购 Twitch）几个月后，希尔突然发现自己成了世界范围内最受关注的初创公司临时 CEO。据报道，在谈到对他的意外任命时，OpenAI 的大部分员工都给出了竖中指的表情符号，这似乎冥冥中注定了他的命运。更尴尬的是，希尔其实是在根本不清楚 Altman&nbsp;那边情况的状态下，匆匆从董事会手中接过了权柄。后来我们得知希尔其实是董事会的第三选择，看来这帮成员是把新老两任 CEO 得罪完了。目前还不清楚希尔接下来会在 OpenAI 扮演怎样的角色。</p><p></p><h4>有效利他主义者们</h4><p></p><p>&nbsp;</p><p>有效利他主义运动的参与者们本该监管企业权力，并专注于督导这颗 AI 之星多做好事——至少别做坏事。这是 OpenAI 董事会设立的出发点，即在赚钱问题之外，也要充分考虑 AI 技术对于人类安全可能产生的影响。但在这波折腾之后，OpenAI 内部似乎再也没人想提、能提、敢提这方面的意见。</p><p></p><h4>X</h4><p></p><p>&nbsp;</p><p>社交网络平台是本次重大事件的起点，也是各方斗争的舞台。OpenAI 的员工们纷纷发帖，用心形表情表达对奥特曼的敬意和忠诚，这也正是当初 Twitter 美好且伟大的原因所在。然而，X 的新东家马斯克则在忙着提起各种令人不寒而栗的诉讼，攻击非营利组织的言论自由，甚至卷入了可笑的阴谋论漩涡。</p><p></p><h2>这些人，输赢未定</h2><p></p><p></p><h4>Thrive Capital</h4><p></p><p>&nbsp;</p><p>在本周的动荡之前，有报道称风险投资公司 Trive Capital 将对 OpenAI 员工手中的股份发起要约收购。如果交易成功，这将使 OpenAI 的市值超过 800 亿美元。Trive 已明确表示将继续推进这项计划，但具体时间和价格尚不明确。</p><p></p><h4>Ilya Sutskever</h4><p></p><p>&nbsp;</p><p>这位 OpenAI 首席科学家决定与董事会中的各位利他主义者站在同一阵营，最终使力量的天生倾向了反对 CEO Altman。但他随后在 X 上发帖称自己后悔了，Altman&nbsp;则以心形表情予以回应，也许是在表达原谅。但是，谁知道呢……</p><p></p><h4>各家新闻媒体</h4><p></p><p>&nbsp;</p><p>短短几天，全球媒体已经被此次随时反转、进展迅猛的事件搞得人仰马翻。每次有新报道发出时，OpenAI 很快会再次发生翻天覆地的变化。以下是 InfoQ 此前的相关报道：</p><p></p><p><a href="https://www.infoq.cn/news/Yo5o2bDbyMubTGFr9Qq7">《突发！“ChatGPT 之父”Sam Altman 被开除》</a>"<a href="https://www.infoq.cn/news/8GJe9yyWcSXkKPhdGJhu">《OpenAI 高层大地震为其首席科学家幕后推动？离职总裁爆料罢免经过、Altman 再次回应》</a>"<a href="https://www.infoq.cn/news/vBCwrQgeDNVEuiuJVsyu">《投资人施压、OpenAI 董事会商讨 Altman 重回 CEO 岗位》</a>"<a href="https://www.infoq.cn/article/Vse4aLuyO2gLBozgyVbw">《OpenAI“生死存亡”时刻：95% 员工或将加入微软，原 OpenAI 寻求与竞对合并？》</a>"<a href="https://www.infoq.cn/article/pmRdAwxrzF0rFQ5mcCsw">《OpenAI 前员工指控奥特曼“欺诈、排除异己、打压员工”，但这些都拦不住他重返 OpenAI》</a>"<a href="https://www.infoq.cn/article/cYwMOcIMKI2PQ65QxRFd">《OpenAI “政变”暂迎结局：Altman 终于回归 OpenAI，董事会重组》</a>"</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.cnbc.com/2023/11/22/how-meta-could-benefit-from-the-openai-shakeup.html">https://www.cnbc.com/2023/11/22/how-meta-could-benefit-from-the-openai-shakeup.html</a>"</p><p><a href="https://www.bloomberg.com/opinion/articles/2023-11-22/sam-altman-return-to-openai-makes-clear-winners-and-losers-of-debacle">https://www.bloomberg.com/opinion/articles/2023-11-22/sam-altman-return-to-openai-makes-clear-winners-and-losers-of-debacle</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mCQdhzTzueva9hI1bsxl</id>
            <title>保护大模型数据隐私和安全 中国公司在国际顶级赛事中获双赛道冠军</title>
            <link>https://www.infoq.cn/article/mCQdhzTzueva9hI1bsxl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mCQdhzTzueva9hI1bsxl</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:18:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: iDASH国际隐私计算大赛, 同态加密, 蚂蚁链, 数据隐私保护
<br>
<br>
总结: 北京时间11月22日，2023年iDASH国际隐私计算大赛结果公布，蚂蚁链联合阿里巴巴达摩院、耶鲁大学、字节跳动在同态加密、区块链智能合约、可信执行环境赛道获得第一名。蚂蚁链参赛团队已经累计获得六次冠军，覆盖多方安全计算、机密计算、联邦学习等赛道。同态加密是竞争最激烈的赛道，蚂蚁和阿里巴巴组成的参赛团队折桂。隐私计算结合多种隐私保护技术解决数据孤岛问题，同态加密是其中主要技术之一。蚂蚁链在同态加密赛道中运行kinship算法，用1.64秒完成了加密计算并得出结果，是性能最优的方案。iDASH是国际上最高规格的隐私计算和数据隐私保护竞赛，每年举行一次，本届竞赛有来自全球十多个国家和地区的六十多支队伍参加。 </div>
                        <hr>
                    
                    <p></p><p>北京时间11 月 22 日，2023 年iDASH国际隐私计算大赛结果正式公布。蚂蚁链联合阿里巴巴达摩院、耶鲁大学、字节跳动分列同态加密、区块链智能合约、可信执行环境赛道第一名。值得注意的是，蚂蚁链参赛团队已经累计获得了六次冠军，覆盖多方安全计算、机密计算、联邦学习等赛道。</p><p></p><p>iDASH 被业界看作是技术领域的“点兵场”，冠军方案基本代表了业内隐私计算的最高水平。据了解，同态加密是竞争最激烈的赛道，包括哈佛大学、麻省理工学院等知名院校，蚂蚁、英特尔、腾讯、字节跳动等国内外科技公司提出了 25 个参选方案，最终由蚂蚁和阿里巴巴组成的参赛团队折桂。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a6a478d0ed2fbcbcaecb63eb3a177549.png" /></p><p></p><p>今年以来，大模型引爆人工智能（AI）热潮。作为大模型训练的底料，数据本身的隐私和安全水位持续上涨，各数据方对用户数据管理日趋严格。这也就使得 AI面临着不同数据源之间，数据难以互联互通的问题。隐私计算就是结合多种隐私保护技术来解决数据孤岛问题，让数据以密态形式出域，各个参与方只拿到最终的计算结果。而同态加密就是最主要的隐私保护技术之一。</p><p></p><p>本届iDASH 同态加密赛道给定一个基因数据库和某个人的基因型，要求参赛者用同态加密技术安全地判定这个人是否与数据库中的某个人存在亲缘关系。蚂蚁链联合阿里巴巴达摩院利用同态加密高效地运行kinship算法，用 1.64 秒完成了加密计算并得出结果，是所有参赛队伍中性能最优的方案。</p><p></p><p>蚂蚁链参赛队伍队长何家兴告诉记者，“我们针对密态计算的特点优化了kinship算法，避免了复杂的非线性运算，降低了结果的取值范围，同时保持了结果的准确性；此外，我们利用CKKS的系数编码来实现了向量内积的计算，通过分析精度和同态参数之间的耦合关系，将密文模数降至54比特，极大地降低了计算开销。”</p><p></p><p>iDASH是目前国际上在隐私计算和数据隐私保护领域最高规格的竞赛，由美国国立卫生研究院（NIH）主办，每年举行一次。iDASH每年会设置2至4个赛道，分别限定采用不同的隐私计算技术来解决问题。本届 iDASH竞赛共有来自全球十多个国家和地区的六十多支队伍参加，包括哈佛大学、MIT、耶鲁大学等世界知名大学，蚂蚁、Intel、腾讯、字节跳动等国内外大型科技公司。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y11lNYPELO7R4B9N6Dd1</id>
            <title>OpenJ9调研、实践和反馈</title>
            <link>https://www.infoq.cn/article/y11lNYPELO7R4B9N6Dd1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y11lNYPELO7R4B9N6Dd1</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 07:00:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云原生基础设施, OpenJ9, JVM, 内存空间
<br>
<br>
总结: 本文介绍了云原生基础设施在互联网保险公司中的应用，以及在架构设计中考虑研发效率、性能和资源成本的平衡。重点介绍了OpenJ9虚拟机的特点和优势，包括启动时间快、占用内存少等。通过实践验证，展示了OpenJ9在众安健康险应用中的应用效果。 </div>
                        <hr>
                    
                    <p></p><h2>1、引言</h2><p></p><p></p><p>近些年随着“云”逐渐成为主流，云产商通过按需计量收费（如 CPU 数、容器数、存储等）的方式逐步成为业界的通用标准。<a href="https://www.infoq.cn/article/PwGh8pYCfzIRf4X0EDgX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">众安</a>"作为互联网保险公司，公司应用全面拥抱了云原生基础设施，基于这样的背景，研发团队在架构设计层面，需要综合考量研发效率、性能、资源成本等多个因数之间的平衡。本文所切入点是从 <a href="https://mp.weixin.qq.com/s?__biz=MzI4NDY5Mjc1Mg%3D%3D&amp;chksm=ebf6da41dc815357e0d53c73865a23f41219e75bac5a4d510bfa31cc51594b59a20e2e4f6cb8&amp;idx=1&amp;mid=2247483966&amp;scene=27&amp;sn=dfa3375d36aa2c0c25a775522e381e62&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">JVM </a>"这个点开始的，正如 OpenJ9 所宣传的特性那样，其占用内存空间小、云环境适配性强，让笔者所在团队跃跃欲试。本文通过介绍 OpenJ9、测试 <a href="https://www.infoq.cn/article/2017/09/IBM-JVM-OpenJ9-Eclipse?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenJ9</a>" 在 MEM 和 CPU 方面的响应、整理线上切换思考和问题、汇总线上切换收益，形成了一个完整的调研、实践及反馈闭环。整理出本文，献给大家参考。</p><p></p><p></p><h2>2、OpenJ9 是什么？</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ecdad524e4c5841ce3b00b8f2f6a8937.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/36/367fe504c7630b739101e5876f28cee0.png" /></p><p></p><p>OpenJ9 和 Hotspot 一样，是个虚拟机，属于 JDK designer &amp; implementer 那类。</p><p></p><p></p><h2>3、OpenJ9特点、优势</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4ea291a2bda190056786e8a2025ef52e.png" /></p><p></p><p>根据官网文档说明，以 OpenJDK8 和 OpenJ9 比较说明其应用层面的特点、优势：</p><p>42% faster startup time，启动时间快 42% 66% smaller footprint after startup，启动后占用内存减少 66%</p><p>Faster ramp-up time in the cloud，云端环境快速提升吞吐量 </p><p>63% smaller footprint during load，高负载时减少 63%的占用空间</p><p>根据 cheng jin（OpenJ9  VM Software Developer）介绍，DDR 和 SCC 是其不同于 HotSpot 的 2 个特点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2cf9e895cf23508ad003ecb567c1342e.png" /></p><p></p><p>DDR：字节码、指令查看工具，不仅限于这个点 </p><p>SCC：共享缓存，内存映射文件，faster startup smaller footprint</p><p></p><p></p><h2>4、与 HotSpot 的一些差异</h2><p></p><p></p><p>官方文档给出了 HotSpot和OpenJ9 的一些差异比较，详见https://www.eclipse.org/openj9/docs/openj9_newuser/</p><p></p><p></p><h2>5、以众安健康险某应用为例实战OpenJ9</h2><p></p><p></p><p>根据OpenJ9的特性，结合笔者所在团队典型的具备该特性型应用来做些调研尝试。后续给出的验证材料会做必要的简化缩略，但符合整体的实践结论，没有特殊说明的情况下，后续所有的实践验证都是搭载的Jdk8版本。</p><p></p><p></p><h4>5.1、启动时间</h4><p></p><p></p><p>前面5次编译后面5次重启的方式验证。</p><p>HotSpot:</p><p>2022-11-21 10:23:28,681 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 18.172 seconds (JVM running for 26.026)</p><p></p><p>2022-11-21 10:34:18,089 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 20.496 seconds (JVM running for 28.51)</p><p></p><p>2022-11-21 11:08:34,687 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 21.057 seconds (JVM running for 29.406)</p><p></p><p>2022-11-21 11:14:14,482 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 19.026 seconds (JVM running for 26.751)</p><p></p><p>2022-11-21 11:21:18,403 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 25.372 seconds (JVM running for 34.543)</p><p></p><p>2022-11-21 11:25:38,611 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 17.042 seconds (JVM running for 24.267)</p><p></p><p>2022-11-21 11:32:53,958 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 19.364 seconds (JVM running for 27.723)</p><p></p><p>2022-11-21 11:37:05,300 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 18.859 seconds (JVM running for 26.526)</p><p></p><p>2022-11-21 11:41:44,996 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 20.337 seconds (JVM running for 28.833)</p><p></p><p>2022-11-21 11:46:50,933 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 19.631 seconds (JVM running for 28.274)</p><p></p><p>未开启 -Xquickstart的OpenJ9:</p><p></p><p>2022-11-21 11:10:24,612 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 24.462 seconds (JVM running for 30.382)</p><p></p><p>2022-11-21 11:15:34,476 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 22.395 seconds (JVM running for 28.111)</p><p></p><p>2022-11-21 11:23:45,262 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 22.468 seconds (JVM running for 27.78)</p><p></p><p>2022-11-21 11:30:35,146 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 24.835 seconds (JVM running for 31.284)</p><p></p><p>2022-11-21 11:36:08,676 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 26.649 seconds (JVM running for 32.623)</p><p></p><p>2022-11-21 11:38:48,876 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 34.609 seconds (JVM running for 41.84)</p><p></p><p>2022-11-21 11:42:27,940 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 25.345 seconds (JVM running for 31.008)</p><p></p><p>2022-11-21 11:44:31,370 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 24.534 seconds (JVM running for 30.321)</p><p></p><p>2022-11-21 11:47:19,612 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 32.299 seconds (JVM running for 39.358)</p><p></p><p>2022-11-21 11:49:20,883 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 24.551 seconds (JVM running for 30.465)</p><p></p><p>开启 -Xquickstart的OpenJ9:</p><p></p><p>2022-11-21 13:49:59,075 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 24.209 seconds (JVM running for 30.148)</p><p></p><p>2022-11-21 13:52:58,821 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 25.16 seconds (JVM running for 30.767)</p><p></p><p>2022-11-21 13:54:59,620 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 21.16 seconds (JVM running for 25.962)</p><p></p><p>2022-11-21 15:23:58,430 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 20.465 seconds (JVM running for 25.154)</p><p></p><p>2022-11-21 15:43:21,880 [main] INFO  [StartupInfoLogger.java:61] [trace=,span=] - Started Application in 23.326 seconds (JVM running for 28.349)</p><p></p><p>结论：OpenJ9 并未比 HotSpot 加快启动时间，在有限的测试次数下看到的还是变慢了。进一步查资料，此场景应该是需要在开启 SCC+ 制作 Docker 缓存共享卷的情况下才能验证出来。</p><p></p><p>目前相关平台部门在排期支持此类自定义的功能上线，后续可以结合再次验证核实 OpenJ9 官网结论。</p><p></p><p></p><h4>5.2、内存监控</h4><p></p><p></p><p>测试方案，主要从 JVM 和 POD 两个层面检查内存变化，调研负载 1W 人【常规数据规模】、5W 人【目前已知的峰值】和 10W 人【假设未来可能的峰值】时 HotSpot 和 OpenJ9 内存的变化。</p><p></p><p>5.2.1、JVM</p><p></p><p>HotSpot 使用 jmap -heap id 可查看堆内存变化，但 OpenJ9 不支持 jmap -heap id，所以两者得折中一下，通过 -histo，查看对象的数量和占用堆内存大小。</p><p></p><p>静态验证：</p><p>HotSpot 效果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/079f17d44f8d965e274db3d3be5b29b9.png" /></p><p></p><p>初始未负载时占用堆内存大小 278M 左右。</p><p>OpenJ9 效果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4fffb465b8895cac3bafbf63f8e14db2.png" /></p><p></p><p>初始未负载时占用堆内存大小 98M 左右。</p><p></p><p>动态验证：</p><p>HotSpot，1W人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14c6a2f4d25257c4e950ab68cffa19ef.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b336882058b8c7098e63130f6edf444a.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c73d3170937535c37a5e794074b5eb19.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/34/345e688918cf6150178522851d93cd9f.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d637b0ac3f45baf421040e1622ce740.png" /></p><p></p><p>堆内存大小使用上升到 300多~400 多 M，峰值到了 600 多 M。和监控平台上的堆内存大小使用基本上匹配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f16294f47057ce826466ab58f5d7905d.png" /></p><p></p><p>OpenJ9，1W 人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/794081c023f1782ca914a6f500b0599e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d8/d8f3f5fe7ba019516a1ff98d18a1ed68.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d503a57c440f07acc2b57bed92405b73.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/69/697ebeeab80b111d500f193874937a44.png" /></p><p></p><p>堆内存大小使用上升到 300 多~400 多M，偶尔会跌回到 100 多 M，峰值到了 400 多 M。相关监控平台截图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/25f4b72150fce20b49f920141d7e84b1.png" /></p><p></p><p> 1 万人的结论，堆内存看起来也会快速上升，但是峰值 OpenJ9 确实低于 HotSpot，同时性能上也从 8 分钟（HotSpot）涨到了 10 分钟（OpenJ9）。内存占用少了，但性能其实降低了。</p><p></p><p>HotSpot，5W 人的场景下：</p><p><img src="https://static001.geekbang.org/infoq/42/42e535177746534142b514a4ba478803.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68337e05170c469678ced3fdcab8cefd.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8143aed5f9d62d7e67cf175d9a6463a.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef68c48e88dc034693393e4eca3ae2ea.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa49ac52201466a68102f909ae01a0ce.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e3fdb44ed712d31c669dd4c67f9f6426.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dc930c70cce67c35eef415b6f91a8f2.png" /></p><p></p><p>堆内存大小使用上升迅速爬升，稳定在 900 多 M~1G 左右，峰值到了 1.6G 左右，中间发生了 2 次 FULLGC。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d5ffc8b59b04805be633645664409f53.png" /></p><p></p><p>OpenJ9，5W人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d157aa9e7cff10f9211158e8bd78a00.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd9741783699192731e6ac953ac5f6af.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13e987f8b5c0c0000e6d1eb3fbd761ca.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9de6456cd94c741ec07392021a30ce9d.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/06/0667e943ca2ffa715eb9a09aaedb879e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6a5a0e060a47ff80b0a1fb0d8c0d47d.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/6e/6e61b2c5bd23ff4e5e17e7a821ae1523.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f100392793eb23e9e511928d245ab2d6.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/78/788231f4d2c7f9cc4ea3341834b005e3.png" /></p><p></p><p>堆内存大小使用上升迅速爬升，不同于 HotSpot，堆内存使用呈锯齿状，峰值到了 1.2G 左右，中间发生了 4 次 FULLGC。图片耗时从 40 分钟到了 50 分钟，有一定的性能损耗。额外验证了未开启 SCC 的情况下，5W 人峰值到了 1.4G 左右。</p><p></p><p><img src="https://static001.geekbang.org/infoq/68/6838e036d8a1cf9f3d62906933cec906.png" /></p><p></p><p> HotSpot，10W 人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0d5bfc9a98238c0f42a2abbca853781.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d474c8bdae7e18df9ead4a436e3e7a2.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00f663492fd945d4fb51a635ff6111ea.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8545da9559cfe03963e81fdf67f5546.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19d0f497162584d938b9c17a9322ea08.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a04d64835c5d350b6d18bd3a45eea77.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9f/9f452f38dee6f4910015265ce6e47e0a.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91e5a1573f476b7bdc101c891b265a99.png" /></p><p></p><p>堆内存大小使用上升迅速爬升，稳定在 1.2G 左右，峰值到了 2G 左右，中间发生了数十次 FULLGC。最终因为堆内存打满无法再分配内存导致任务失败进入重试。</p><p></p><p>5.2.2、POD</p><p></p><p>从进程角度检查内存变化。</p><p></p><p>静态验证：</p><p>直接比较进程初始时的内存使用率  ps -aux | grep java</p><p></p><p>HotSpot效果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd2ec418c555d05dc00a714009579f19.png" /></p><p></p><p>初始未负载时进程占用内存大小888M左右。</p><p></p><p>OpenJ9效果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d8/d843cb59818b088e07d2a64c0f69c5a1.png" /></p><p></p><p>初始未负载时进程占用内存大小764M左右。</p><p></p><p>动态验证：</p><p>ps -ef | grep java 找到对应id  执行  top -p id。HotSpot，1W人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f0e1c32e44930ad60cb681edde01d795.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/51/5197d04b9a756a179bedf51f386c57ec.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46331168f777f8580ba98afeda031ea4.png" /></p><p></p><p>OpenJ9，1W 人的场景下：</p><p>1万人的结论，内存使用率会上升，但是峰值上OpenJ9低于HotSpot，本次测试耗时8分钟，和HotSpot持平。</p><p></p><p>HotSpot，5W人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d91106b555d605a76d9c67aad3c9805.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/86/867d33c2f55cfd232085a289d800d9c9.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/49/497c936f46a043a4ef5576fa1aabb07a.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/97/971f67ab72afbc72d0a2d06566c4da91.png" /></p><p></p><p>OpenJ9，5W 人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/803b666b9d82b65aac6935cc31bfc89c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bffad29ed010e7289c5cfad35be5c499.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/44/44fcb50de24bf1cc198759c80232847b.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/83/83cb8106e3ca100ccd27de6b07fa8567.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dca46eba6988c6ef8ca69ec654151b00.png" /></p><p></p><p>5 万人的结论，峰值上 OpenJ9 低于 HotSpot 但看起来已经比较接近了，和 HotSpot 耗时相比较，依然是多了 10 分钟左右才执行完成。</p><p></p><p>HotSpot，10W 人的场景下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fe4a827cdaba4fb4495fa6cfbf7d755.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a68d66c117b33d41bb82186aa5957f6e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f82e63667adfea883edd5a3d687198b1.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/7d/7d2b7c414fee19eb5e2b0ce338bc8bd7.png" /></p><p></p><p>OpenJ9，10W人的场景下：图片</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/30ac7bc9444a690b5fbe7080f1847734.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5514901364c5f88b19cb20f06ffbf85d.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13f39c846b8047e7ccd8d286894b98cc.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/76/76906d699c9e5d241f5b913cc4f8026d.png" /></p><p></p><p>结论：</p><p></p><p>常规场景下（1万、5万），OpenJ9 比 HotSpot 更节省内存，节省比例大概在 30% 左右(基于峰值计算)；</p><p></p><p>极端场景下（10万），OpenJ9 比 HotSpot 依然更节省内存，只是两者都无法正常完成任务处理，HotSpot 表现稳定一些，OpenJ9 出现“假死”进而被判定容器不可用，然后重启。</p><p></p><p></p><h2>6、OpenJ9 的节省内存的一点思考</h2><p></p><p></p><p>可以看到很多数据和图片都显示 OpenJ9 节省内存，网上暂未查到相关其节省内存的原理介绍，遂从直觉上判断是否其创建对象大小分配上有和 HotSpot 不一样的逻辑。</p><p></p><p>参考链接7中有对应的测试类 SizeOfObject 和对应的测试代码 SizeOfObjectTest，此处不再赘述。</p><p>基于HotSpot测试的结论和链接中一致：</p><p>HotSpot：默认开启指针压缩</p><p>sizeOf(new Object())=16sizeOf(new A())=16</p><p>sizeOf(new B())=24 -------</p><p>sizeOf(new B2())=24 -------</p><p>sizeOf(new B[3])=32</p><p>sizeOf(new C())=24</p><p>fullSizeOf(new C())=128 -------</p><p>sizeOf(new D())=32 -------</p><p>fullSizeOf(new D())=64 -------</p><p>sizeOf(new int[3])=32</p><p>sizeOf(new Integer(1)=16</p><p>sizeOf(new Integer[0])=16 -------</p><p>sizeOf(new Integer[1])=24</p><p>sizeOf(new Integer[2])=24</p><p>sizeOf(new Integer[3])=32</p><p>sizeOf(new Integer[4])=32</p><p>sizeOf(new A[3])=32</p><p>sizeOf(new E())=24</p><p></p><p>基于 OpenJ9 测试的结论：</p><p></p><p>OpenJ9:默认配置 </p><p>sizeOf(new Object())=16</p><p>sizeOf(new A())=16</p><p>sizeOf(new B())=16 -------</p><p>sizeOf(new B2())=16 -------</p><p>sizeOf(new B[3])=32</p><p>sizeOf(new C())=24</p><p>fullSizeOf(new C())=104 -------</p><p>sizeOf(new D())=24 -------</p><p>fullSizeOf(new D())=56 -------</p><p>sizeOf(new int[3])=32</p><p>sizeOf(new Integer(1)=16</p><p>sizeOf(new Integer[0])=24 -------</p><p>sizeOf(new Integer[1])=24</p><p>sizeOf(new Integer[2])=24</p><p>sizeOf(new Integer[3])=32</p><p>sizeOf(new Integer[4])=32</p><p>sizeOf(new A[3])=32</p><p>sizeOf(new E())=24</p><p></p><p>通过这个测试，大致可以知道 18 个 CASE 中，相对 HotSpot，OpenJ9 创建的对象，12 个大小持平，5 个占用内存减少，1 个占用内存增加。这个结论虽不足以支撑 OpenJ9比HotSpot 更省内存，但是可以看出其确实不一样的内存大小分配逻辑，大胆猜测一下，这套不一致的实现应该是其节省内存的一个点。</p><p></p><p></p><h2>7、OpenJ9的BenchMark测试</h2><p></p><p></p><p>到目前为止都是花了比较多的时间在讨论 OpenJ9 内存方面的优势，那它在 CPU 计算层面的表现相对HotSpot 又是怎样的？下面通过一段 BenchMark 代码测试来验证下。</p><p></p><p>@State(Scope.Benchmark)</p><p>@Fork(2)</p><p>public class Test {</p><p></p><p>public static void main(String[] args) throws RunnerException &nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>Options options = new OptionsBuilder().include(Test.class.getSimpleName()).build();</p><p>new Runner(options).run();&nbsp;}</p><p></p><p>&nbsp;&nbsp;@Benchmark&nbsp;</p><p>public static void md5() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;DigestUtils.md5Hex(generateStringToHash());</p><p>&nbsp;}</p><p></p><p>&nbsp;@Benchmark&nbsp;</p><p>public static void sha1() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>DigestUtils.sha1Hex(generateStringToHash());&nbsp;</p><p>}</p><p></p><p>&nbsp;@Benchmark</p><p>&nbsp;public static void sha256() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>DigestUtils.sha256Hex(generateStringToHash());&nbsp;</p><p>}</p><p></p><p>&nbsp;@Benchmark&nbsp;</p><p>public static void sha512() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>DigestUtils.sha512Hex(generateStringToHash());</p><p>&nbsp;}</p><p></p><p>&nbsp;&nbsp;&nbsp;public static String generateStringToHash() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>return UUID.randomUUID().toString() + System.currentTimeMillis();&nbsp;</p><p>}</p><p>}</p><p></p><p>代码非常简单，分别是几种加密算法，使用的类库是 apache的codec，测试条件如下：</p><p></p><p>Windows 10  64 位</p><p>Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz   2.11 GHz</p><p>16.0 GB</p><p>关于 BenchMark 本身，这里也不做更多的阐述，可自行百度。这里对每个方法做了 2 次 Fork，每次 Fork做 5 次 Warmup 后，再做 5 次 Measurement，得到如下结果：</p><p></p><p>HotSpot：</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/569ca1dfff605ddc52258c6328aaeaee.png" /></p><p></p><p>OpenJ9：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0daed5cae74ad00f749ed130636af10b.png" /></p><p></p><p>重点关注 Score 列，这里在此次默认的 BenchMark 模式中代表的是吞吐量，结合 Units，即每秒操作多少次，可以看到 OpenJ9 相对 HotSpot 吞吐量还是有下降的。如果单从这个角度看，会对 OpenJ9 的使用持有谨慎的态度，但正如参考链接 9 所述，所有的应用程序不是只做这一件事情，只执行这一段代码。</p><p></p><p>所以此处准备继续引用一下参考链接 9 来继续描述 BenchMark 的结论。虽然它是基于 Jdk11 来做的测试验证，但我理解这样的结论在目前Jdk8中的趋势应该不会改变。</p><p></p><p>OpenJ9: Switching from HotSpot to OpenJ9 makes ojAlgo faster, ACM slower and EJML fluctuates between roughly the same performance and much slower. It seems OpenJ9 is less stable in its results than HotSpot. Apart from the fluctuations with EJML it widens the gap between the slowest and the fastest code. OpenJ9 makes the fast even faster and the slow even slower............</p><p></p><p>The speed differences shown here are significant! Regardless of library and matrix size, performance could be halved or doubled by choosing another JVM. Looking at combinations of libraries and JVM:s there is an order of magnitude in throughput to be gained by choosing the right combination.This will not translate to an entire system/application being this much slower or faster – matrix multiplication is most likely not the only thing it does. But, perhaps you should test how whatever you’re working on performs with a different JVM.</p><p></p><p>大致翻译一下。</p><p></p><p>OpenJ9:从 HotSpot 切换到 OpenJ9 使 ojAlgo 更快，ACM 更慢，EJML 在大致相同的性能和更慢的性能之间波动。OpenJ9 的结果似乎不如 HotSpot 稳定。除了 EJML 的波动之外，它还扩大了最慢代码和最快代码之间的差距。OpenJ9 使快的更快，慢的更慢。</p><p></p><p>这里显示的速度差异是显著的!不管库和矩阵的大小如何，选择另一个 JVM 可以使性能减半或加倍。看看库和 JVM 的组合，通过选择正确的组合可以获得一个数量级的吞吐量。</p><p></p><p>这将不会转化为整个系统/应用程序如此慢或如此快——矩阵乘法很可能不是它唯一做的事情。但是，也许您应该测试您正在处理的任何东西在不同的 JVM 上的执行情况。</p><p></p><p>附上测试结论数据：</p><p></p><p>HotSpot："Benchmark","Mode","Threads","Samples","Score","Score Error (99.9%)","Unit","Param: dim","Param: lib"</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,34413.970020,24755.445712,"ops/min",100,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,85465.595631,7158.774782,"ops/min",100,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,89211.858013,12889.194973,"ops/min",100,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,106731.255464,3657.831785,"ops/min",100,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,10080.519404,3421.528136,"ops/min",150,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,25882.406843,1133.754908,"ops/min",150,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,30965.104808,1835.050624,"ops/min",150,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,29878.718059,874.431921,"ops/min",150,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,4008.914571,2797.192776,"ops/min",200,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,10926.632411,1152.009522,"ops/min",200,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,15404.968617,725.142308,"ops/min",200,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,12666.691647,612.797881,"ops/min",200,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,763.748592,30.917436,"ops/min",350,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,2032.841238,70.656040,"ops/min",350,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,3251.322968,209.402892,"ops/min",350,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,2659.340962,506.193983,"ops/min",350,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,255.822643,59.195722,"ops/min",500,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,696.803016,43.103859,"ops/min",500,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,1081.090826,732.926965,"ops/min",500,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,912.348219,179.074511,"ops/min",500,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,80.205124,10.632867,"ops/min",750,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,206.216518,13.924279,"ops/min",750,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,309.858960,6.985213,"ops/min",750,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,272.868116,15.091327,"ops/min",750,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,25.378833,30.930618,"ops/min",1000,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,85.617121,2.286021,"ops/min",1000,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,132.171220,4.568624,"ops/min",1000,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,116.064807,9.837388,"ops/min",1000,MTJ</p><p>OpenJ9：</p><p>"Benchmark","Mode","Threads","Samples","Score","Score Error (99.9%)","Unit","Param: dim","Param: lib"</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,14683.462390,538.793271,"ops/min",100,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,80105.353723,5219.442243,"ops/min",100,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,128290.832434,19202.038430,"ops/min",100,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,117431.360792,25258.942865,"ops/min",100,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,4369.110255,1370.251546,"ops/min",150,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,21840.781985,761.252770,"ops/min",150,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,48608.703009,32934.365969,"ops/min",150,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,31828.012559,13729.359128,"ops/min",150,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,1712.217195,41.493227,"ops/min",200,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,3227.834335,282.717723,"ops/min",200,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,25597.291062,6989.989282,"ops/min",200,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,14169.687774,432.516135,"ops/min",200,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,280.190365,29.736756,"ops/min",350,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,1936.031548,544.372429,"ops/min",350,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,6004.867771,1183.160078,"ops/min",350,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,2652.624079,619.430766,"ops/min",350,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,175.313429,5.764318,"ops/min",500,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,691.723102,15.554568,"ops/min",500,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,2133.663163,298.117681,"ops/min",500,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,922.513268,27.892900,"ops/min",500,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,28.178703,109.654076,"ops/min",750,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,43.375818,0.623678,"ops/min",750,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,633.488067,226.129149,"ops/min",750,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,275.391125,10.876657,"ops/min",750,MTJ</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,3.896976,38.325441,"ops/min",1000,ACM</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,17.879150,0.109833,"ops/min",1000,EJML</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,265.467924,6.376378,"ops/min",1000,ojAlgo</p><p>"org.ojalgo.benchmark.lab.FillByMultiplying.execute","thrpt",1,3,106.020520,115.325279,"ops/min",1000,MTJ</p><p></p><p></p><h2>8、切换前的一些总结、想法</h2><p></p><p></p><p>1、尝试 OpenJ9 的使用，应从级别低、相对边缘、内存密集型应用开始，前面所说的级别低和相对边缘比较符合感性认识，但从理性认识，即不如 HotSpot 稳定这个层面，结论也是一致，后面所说的内存密集型应用，是从 OpenJ9 本身的特点来看的；</p><p></p><p>2、线上切换建议先不做任何JVM的配置改动，先执行一段时间，拿到 OpenJ9 的峰值后，再调整配置，调整的值建议在峰值的基础上上浮 10%，避免拿到的是假峰值情况；</p><p></p><p></p><h2>9、线上使用遇到的问题</h2><p></p><p></p><p>Q：某应用使用了 TLSv1、TLSv1.1，OpenJ9 默认禁用了这两个加解密算法，导致部分业务受影响</p><p></p><p>A：平台架构部目前给的解决方案如下：jdk.tls.disabledAlgorithms的配置在较新版本的jdk中都是将TLSv1, TLSv1.1等默认禁用掉了。OpenJ9也是一样，如果你想要启用的话，试试在应用的dockerfile中修改java.security文件，类似：RUN sed - 's/jdk.tls.disabledAlgorithms=SSLv3, TLSv1, TLSv1.1, RC4, DES, MD5withRSA,/jdk.tls.disabledAlgorithms=SSLv3, /g' /opt/java/openjdk/jre/lib/security/java.security后续的话我们会讨论下这部分相关的支持和后续的方案。https://github.com/IBM/java-liberty-app/blob/master/Dockerfile</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcfe63b900a96c59a6830a746469db55.png" /></p><p></p><p>Q：某应用使用了 icu4j 框架，涉及到调用点执行报错，截图如下：</p><p></p><p>A：icu4j.jar 3.4.4 版本于 06 年发布，版本过低，建议使用 JDK 自带的工具类或者升级该版本。如果应用使用了 icu4j，需注意可能引发的风险。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/83cf3558c7b4a9ab2cc4f4a7668bb382.png" /></p><p></p><p></p><h2>10、线上使用成果反馈</h2><p></p><p></p><p>文末再同步一下笔者所在部门的 OpenJ9 线上切换阶段性成果，首批 23 个应用于春节前完成了切换，涉及相关 POD 节点数量 63 个，总内存 125G，使用内存减少 54G，节省比例达 40% 多，这一比例应该是比上述调研的30%要更优秀。截止笔者发文，切换的应用也暂未有相关故障产生，表现稳定。</p><p></p><p>参考资料：</p><p>1、https://www.bilibili.com/video/BV1t54y1t7JY/?spm_id_from=333.337.search-card.all.click（PART I）</p><p>2、https://www.eclipse.org/openj9/</p><p>3、https://www.eclipse.org/openj9/docs/</p><p>4、https://www.codenong.com/d-class-sharing-in-eclipse-openj9/</p><p>5、https://www.codenong.com/d-class-sharing-in-eclipse-openj9-how-to-improve-mem/</p><p>6、https://blog.51cto.com/u_14634606/2478821</p><p>7、https://www.cnblogs.com/zhanjindong/p/3757767.html</p><p>8、https://technology.amis.nl/software-development/performance-and-tuning/jvm-performance-openj9-uses-least-memory-graalvm-most-openjdk-distributions-differ/</p><p>9、https://www.ojalgo.org/2019/02/quick-test-to-compare-hotspot-and-openj9/</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ajiASOVMieVsMOvd5YiR</id>
            <title>中国生成式AI开发者生态调研 ｜参与有礼</title>
            <link>https://www.infoq.cn/article/ajiASOVMieVsMOvd5YiR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ajiASOVMieVsMOvd5YiR</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Nov 2023 06:12:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 生成式AI生态, AI开发和应用, AI基础设施
<br>
<br>
总结: 自ChatGPT爆火至今已有一年，国内在生成式AI生态方面取得了进展。InfoQ邀请开发者参与问卷调研，内容将呈现在2024年终趋势与技术展望报告中。参与者有机会获得多种礼品。详情请点击链接参与。 </div>
                        <hr>
                    
                    <p>自ChatGPT爆火至今已有一年，这一年国内在基础大模型层面、相关工具层面，甚至是行业应用层面均取得了一定进展。基于此，InfoQ希望邀请广大开发者共同完善国内的生成式AI生态，本次问卷共分为【AI开发和应用】、【AI基础设施】、【AI未来趋势】三部分，一共27个问题，完整填完预计需要花费6分钟。</p><p></p><p>与此同时，我们也准备了部分礼品，分别是价值500元的京东礼品卡、价值50元的京东礼品卡、太空熊风扇、小米无线蓝牙耳机Air2&nbsp;SE、InfoQ定制马克杯、InfoQ定制抱枕等众多礼品，数量有限，先到先得。</p><p></p><p>调研问卷的最终内容将由InfoQ研究中心分析后呈现在【2024年终趋势与技术展望】报告中，届时通过InfoQ官网电子书模块可以下载阅读，再次感谢各位开发者的参与（点击链接：<a href="https://jinshuju.net/f/BWMntO?x_field_1=gongzhonghao">https://jinshuju.net/f/BWMntO?x_field_1=gongzhonghao</a>" 或者扫描下方海报二维码即可参与）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/d0/e3/d0d4c32fd2bce56551edaefe1d69c0e3.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>