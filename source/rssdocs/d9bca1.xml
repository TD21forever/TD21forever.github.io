<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/wbFv14DI3TEeGJkoI82r</id>
            <title>离开云转战AI？ 23岁写了百万人用的开源软件，这个IT奇才11年后离开了自己的上市公司</title>
            <link>https://www.infoq.cn/article/wbFv14DI3TEeGJkoI82r</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wbFv14DI3TEeGJkoI82r</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 07:07:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 云平台转变, AI, 构建和交付软件, HashiCorp
<br>
<br>
总结: HashiCorp联合创始人Mitchell Hasimoto宣布离开公司，他认为AI代表了一种平台迭代，有望从根本上改变构建和交付软件的方式。他在公开信中回忆了自己的创业经历，表达了对HashiCorp的祝福，并表示将去探索新的领域。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>“不同于云平台转变，AI代表的是另外一种平台迭代，但同样有望从根本上改变我们构建和交付软件的方式。”</blockquote><p></p><p>&nbsp;</p><p>当地时间12月14日，HashiCorp联合创始人Mitchell Hasimoto 终于对外宣告将正式离开自己一手创立的公司。这是一场“蓄谋已久”的告别。在公开信里，他回忆了自己十多年的创业经历、对HashiCorp的祝福，以及表达了自己将去探索新的领域。</p><p>&nbsp;</p><p>Mitchell 在12岁就开始了首次创业，他当时编写了网络游戏作弊大全Cheat Neopets，后来收到了Neopets发来的律师函。当时，Mitchell 的父母并没有太关注儿子对于电脑的喜爱，他们限制他每周只能玩两个小时的电脑。他不得不等父母睡着后偷偷编程。</p><p>&nbsp;</p><p>上大学的时候，Mitchell的父亲只给了他一年的时间来“搞电脑方面的事情”。如果一年内无法证明自己具有这方面的天赋，那么Mitchell就得自付学费，或者按照父亲的意思去当律师或医生。</p><p>&nbsp;</p><p>大一时，他写了一个帮助同学报名选修课的软件，他每年只需花费20个小时来维护这款软件就能入账大约50万美元。但他父亲仍希望他找个正经的工作，而不是靠网站赚钱。后来他进入了一家网络开发公司才得以继续攻读计算机专业。</p><p>&nbsp;</p><p>大三时，他在这家网络开发公司开发了他的成名作：Vagrant，这也是后来HashiCorp创立的重要基石。2014年，全世界已有数百万人在使用Vagrant。</p><p>&nbsp;</p><p>在读书时，Mitchell 遇到了Armon&nbsp;Dadgar ，他们一起为Vagrant工具开发了一款收费的插件应用程序，但这种商业模式占用了他们太多的时间，于是他们开发了名为Atlas的云服务。</p><p>&nbsp;</p><p>Mitchell 与Armon创立了HashiCorp。据Mitchell透露，他们还在2013年获得过 5000 万美元的<a href="https://twitter.com/mitchellh/status/1357445215259250689">收购要约</a>"：“5000 万美元是一笔令人瞠目结舌的钱，当时我们分别是 23 岁和 21 岁”。</p><p>&nbsp;</p><p>随着公司发展相对成熟后，HashiCorp 有超过 1,400 名员工。作为该公司的CTO，Mitchell 涉及到工程和产品、领导力规划、上市战略管理、客户成功建设和组织建设各个方面。在此期间，他获得了福布斯“30 Under 30”等各种荣誉。</p><p>&nbsp;</p><p>然而因为喜欢“工程”和“编程”，Mitchell从两年前就酝酿从CTO职位退下来，作为“个人贡献者”参与HashiCorp的工作。作为一个上市公司的创始人和董事，能“平稳”退出并不容易，但现在这个他计划已久的一天，终于来了。</p><p>&nbsp;</p><p>下面是Mitchell 的离别赠言。</p><p>&nbsp;</p><p></p><h2>“是时候涉足一点新领域了”</h2><p></p><p>&nbsp;</p><p>本周早些时候，我写下这封给HashiCorp全体员工的信。这里我将信件内容对外公开，希望整个HashiCorp社区能够理解我的选择：</p><p>&nbsp;</p><p>今天，我有一条复杂的消息要跟大家分享：我决定从HashiCorp离职，很快将不再是这家公司的一员。千万别误会，我对这家公司仍然感情深厚。不久之前，我刚刚参与了HashiCorp成立11周年庆典。回顾过去十年间的发展历程，我感到这是我人生当中最重要、也最充实的一个阶段。</p><p>&nbsp;</p><p>但长久以来，我一直在考虑要不要、以及如何告别HashiCorp。自从创立这家公司以来，我就在努力让它能够在不需要我参与日常运营、纯由其他领导者接手之下长期保持稳定。随着时间推移，我也开始有意识地进行规划，包括2016年辞去CEO职务，随时间推移迭代出一种不需要我亲自参与决策的自主领导文化，乃至最后于2021年离开高管团队与董事会。从那时起，我终于找回了自己的本心——当一名亲力亲为的全职工程师。</p><p>&nbsp;</p><p>作为一名工程师，我所关注的不只有基础设施。我一直很清楚，当我个人和这家公司做好准备之后，我一定会转而追求其他新的、不同于以往的挑战。最近我刚刚有了第一个孩子，在休假期间我开始认真反思，觉得现在就是完成这种转变的适合时机。云自动化与基础设施工具的世界虽仍充斥着各种机遇和增长，但在这一领域工作了近15年之后，我觉得是时候涉足一点新领域了。</p><p>&nbsp;</p><p>虽然告别HashiCorp完全是按计划行事，但这个时刻的真正到来仍令我的内心五味杂陈。在成年之后，我的人生几乎都跟这家公司有关，最深刻的记忆也大多与它紧密缠绕。漫长的经历无法一一回顾，但我想跟大家分享几个最激动人心的瞬间。</p><p>&nbsp;</p><p>在共同创办HashiCorp的几年之前，Armon（Armon&nbsp;Dadgar，Hashi Corp联合创始人兼CTO）跟我就一直在密切关注云、自动化和分布式系统。当时的我们还只是十来岁的孩子，我们会半开玩笑地提到：“你说，会不会有一天连世界上最大的企业都离不开我们的软件？”我们也没有停留在想象阶段，而是把一些想法转化成了实际代码。接下来我们获得了几千家用户，这时候就得专门创办一家公司承接这笔业务了。后来，我们又决定采取下一步行动、着手筹集资金。就这样，HashiCorp逐渐发展成了今天的样子：经过一系列像这样的小小开拓，我们恍然发现自己当初那种充满理想主义的少年想象已然成为现实。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/76/764bd0a045fe72d966d92e514307c6eb.png" /></p><p></p><p>2013年时的Mitchell与Armon</p><p>&nbsp;</p><p>在刚刚起步之时，我觉得那些&nbsp;“第一次”特别重要。2015年的第一届HashiConf大会将是我人生中最难忘、最特别的回忆。对我来说，这是数字世界第一次真正与物理世界对接，如今想来仍然如梦似幻。我知道我们的软件拥有很高的下载量，我自己每天也在跟社区成员们在线互动，但看到几百人愿意亲赴现场仍是完全不同的别样体验。我为此感到无比自豪，也从这一刻开始真正感受到肩上的责任。我感受到了内心的挣扎：一方面想要构建起强大的业务，另一方面又为不知如何管理好自己和Armon创立的这家公司而感到惶恐。总之，非常感谢每一位参加首届HashiConf大会的早期采用者和公司员工。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c94abccc09ef274e406671079ce4068c.png" /></p><p></p><p>HashiConf 2015大会盛况</p><p>&nbsp;</p><p>仅仅几年之后，我们的第一次整体远程办公探索就再次给我留下难以磨灭的印象。这时候公司的员工甚至比第一届HashiConf大会的观众还要多！而且这家公司由我和Armon一手创办、专注于对技术的探索，这也让我深切感受到员工的重要意义。现在回想起来，我和员工们共同度过的时光正是人生中又一段重要经历。</p><p>&nbsp;</p><p>在我与HashiCorp的合作历程中，还有很多类似的、影响深远的重要时刻。一幕幕情景将永远铭刻在我的脑海当中，我重视其中每一段经历、包括那些艰难的时光，也将其视为达成每个里程碑的必要过程。</p><p>&nbsp;</p><p>我与Armon一起工作了近15年（甚至早于HashiCorp的创立），而且与Dave（Dave&nbsp;McJannet，HashiCorp&nbsp;CEO）也合作超过了七年。我们一起领导这家企业，直到2021年我决定退出高管团队。我们培养出了亲密的友谊，我坚定相信他们的领导能力，也永远怀念与他们并肩战斗的岁月。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ba/ba794c7aad330930d6c299d5c4779883.png" /></p><p></p><p>Armon、Mitchell和Dave</p><p>&nbsp;</p><p>当初创立这家公司时，我们秉持的多云理念还颇有争议、甚至可以说是离经叛道，但现如今已经被主流所广泛接受。我参与开发的软件在整个行业中得到广泛应用，涵盖业余技术爱好者到全球最大企业的专业人士。最近，GitHub Octoverse报告发现，HashiCorp配置语言（HCL）再次成为开源项目中广泛使用的顶级语言之一。这意味着HashiCorp在行业中仍具有持续影响力、保持增长且拥有光明的发展前景。如今的一切都超出了我当初的期望，我也深深为自己能在实现目标的过程中做出微小的贡献而感到自豪。</p><p>&nbsp;</p><p>正如之前所说，我在成年之后的整个人生几乎都是围绕着HashiCorp所展开。这家公司不仅对我的个人生活产生了巨大影响，更是对很多其他人的生活产生了巨大影响，包括活跃热情的社区、尊贵的客户、往来密切的生态系统合作伙伴以及出色的HashiCorp员工们。感谢大家付出的努力和倾注的信任。最后，我向公司全体同仁致以衷心祝愿。我将为你们喝彩，感谢大家在HashiCorp发展之旅中做出的贡献，也期待看到你们再创新的辉煌！</p><p>&nbsp;</p><p></p><h2>“Mitchell一直是我的榜样之一”</h2><p></p><p>&nbsp;</p><p>对于这个消息，网友更多给出的祝福和赞誉：</p><p>&nbsp;</p><p>“祝贺您在取得的成就：建立一家行业标准的公司，然后根据自己的条件仔细规划退出，这是一个巨大的胜利！”</p><p>&nbsp;</p><p>“Mitchell一直是我的榜样之一，他既是一位出色的工程师，也是一个非常正派、谦虚的人。我真的很期待他将创造出其他令人惊奇的东西。”</p><p>&nbsp;</p><p>“Mitchell是我能想到的唯一一个经历过tinkerer/IC -&gt; 创始人-&gt; CXO，然后回到自己公司IC 的人 。此外，他关于 Zig 的著作对像我这样对其感到好奇的人也非常有帮助。来自IC同仁的崇高敬意！”</p><p>&nbsp;</p><p>“我最尊重Mitchell的一点是他实际上经常编码。很多‘领导’都是空谈，没有行动或技巧。也许他们曾经有过一些 git 提交，但我总觉得 Mitchell 始终在编码。 不过，不幸的是公司的<a href="https://www.infoq.cn/article/oIQh45RPuLXOSVKqkwFA?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">开源许可发生了一些变故</a>"。”</p><p>&nbsp;</p><p>大家也很关注Mitchell的未来去向，但他并没有透露更多。他个人网站2022年后的主要内容有三个方面：他的个人项目Ghostty（终端模拟器）、Zig 编程语言相关和关于人工智能的一些见解。因此，很多人猜测Mitchell可能要转战到AI领域。</p><p>&nbsp;</p><p>Mitchell实际上也发表了很多关于人工智能的推文，但似乎他对现在的这些技术并不满意。11月份，他略带愤怒地指出Copilot “社交编码”功能还有很大的改进空间，他也曾经吐槽道，“如果人工智能社区的人们能够停止用“就是你所需要的”来结束他们的论文标题，那就太好了。”</p><p><img src="https://static001.geekbang.org/infoq/ae/ae63596b49a93e06e28634ac5a4947dd.png" /></p><p></p><p>&nbsp;</p><p>那么Mitchell眼里的AI发展是怎样的呢？</p><p></p><h2>Mitchell 对AI发展的预测</h2><p></p><p>&nbsp;</p><p>今年4月份，Mitchell 从云崛起的历史视角回顾整个AI发展过程，借此通过对云服务和AI进行比较、并对未来做出预测。</p><p>&nbsp;</p><p>Mitchell 认为，近期的AI发展拥有即时价值的创造力，但由于附加功能和工具的缺失而导致在很多场景下有点不切实际，不过他相信这些问题是可以解决。</p><p>&nbsp;</p><p>Mitchell 预测，AI领域会将某种属性用“新”与“旧”明确划分开来，而拥抱“新”产品和业务的厂商将对下一代用户产生更强大的吸引力。同时，生成式AI在为新型软件带来革命性变化的同时，也能在很大程度上让“遗留”软件变得更好。</p><p>&nbsp;</p><p>下面是Mitchell 在文章里对自己观点的详细论述：</p><p>&nbsp;</p><p></p><h4>即时价值</h4><p></p><p>&nbsp;</p><p>云服务的早期成功标志，在于能够为采用者提供即时价值。对于小型项目而言，EC2是速度最快、成本最低的服务器资源获取方式；而S3同样是存储/交付静态资产与二进制blob的简单可靠方案。所有这一切都基于简单易用的HTTP API，相当于是敞开怀抱欢迎工程师们迈向这个自动化新时代。</p><p>&nbsp;</p><p>近期的AI发展也有着同样直接的价值创造力。以往那些难以快速汇总的问题，例如情感分析（“这段内容有攻击性吗？”）如今已被轻松解决。而Copilot等工具生成的代码不仅帮得上忙，有时候甚至比我们自己写的还要好。而且，生成式AI跟当初的云服务一样，也是从易于上手的HTTP API起步，鼓励工程师们迈向这个自动化的新时代。</p><p>&nbsp;</p><p>即时价值其实非常重要，因为它建立起热情的早期用户群体，并在市场上引发了巨大轰动。但与此同时，很多人把即时价值误解成了持续价值。AI的确表现出了激动人心的早期潜力，但这份价值能够扩展到多大、持续多久仍然有待观察。</p><p>&nbsp;</p><p>大家可能还记得加密货币，在我看来它就没能通过测试，即缺乏显而易见的即时价值。支持者坚称加密货币拥有各种长期的未来价值……可能是，也可能不是。但我只知道时至今日，我们对于手头持有的比特币也不知道该怎么使用——除了投机炒作。整整13年过去，无论是不是真的拥有长期潜力，加密货币都仍未表现出真正的即时价值。</p><p>&nbsp;</p><p></p><h4>不切实际的开端</h4><p></p><p>&nbsp;</p><p>早期云计算根本不足以支撑并解决大量问题，所以很多人觉得它有点不切实际。直到2008年，也就是EC2亮相的两年之后，云实例才真正有了长期IP地址。同样是在这个时候，亚马逊云科技才拿出能够可靠保存数据的持久存储服务。刚开始，所有EC2实例都位于共享公共网络之上，直到三年后的2009年推出VPC。类似的例子还有很多很多。</p><p>&nbsp;</p><p>我们当然不能孤立地判断某种事物是否不切实际，毕竟事物之间有着复杂的关联且互为依托。因此，虽然这种专用网络的缺失无法满足商业软件构建与交付的需要，但对于小型项目和早期初创公司来说并没什么影响。于是他们热情投身于云计算当中，给这个新兴行业带来了关注度和旺盛的增长势头。这又反过来推动亚马逊云科技不断发布新服务，把不切实际转化成了脚踏实地。</p><p>&nbsp;</p><p>而随着云计算的普及和炒作，当时的人们普遍有种误解，即“这东西不是给真正的大企业用的。”可随着云能力的不断增强，论调也悄然发生了变化，诸如“财富500强企业永远不会上云”、“受监管的企业永远不会上云”、“政府机构永远不会上云”之类的说辞一一过时。时至今日，就连美国的国防机构都开始高度依赖云基础设施。</p><p>&nbsp;</p><p>而当下的AI技术也处于类似的境地。尽管它拥有突出的即时价值，但由于附加功能和工具的缺失而导致在很多场景下有点不切实际。必须承认，面对某些问题它很难、甚至根本没办法给出正确的答案。大语言模型与外部最新知识的融合也处于起步阶段，许多工具缺乏企业的广泛支持，整个市场也处于群雄并起、态势不明的混乱阶段。</p><p>&nbsp;</p><p>但就像当初的云计算一样，我发现人们再次用笃定的语气言之凿凿：“AI永远没法生成大量代码”或者“AI永远需要人的指导”等等。也许是，也许不是。这些误解本身无法被立即破除，但千万不要因为它们的存在而忽略时代的整体趋势。</p><p>&nbsp;</p><p>而考虑到AI已经表现出极强的即时价值，我相信这个问题会像当初的云计算一样得到解决。AI当中不切实际的部分并不在于它没用，而在于难以大规模集成或者证明其普适性。好在这是个能够解决的问题，那接下来就要交给时间了。</p><p>&nbsp;</p><p>在考虑种种有待证明的潜在价值时，也确有一些看似无法解决的问题。举个极端的例子，通用人工智能（AGI）到底能不能实现仍无宝座，而且现今存在的模型似乎都没办法通往真正的AGI。然而，这样的所谓“不切实际”属于涉及未经证实的狭义价值挑战，并不满足我前面提到的“不切实际”定义。</p><p>&nbsp;</p><p></p><h4>软件属性正在演变</h4><p></p><p>&nbsp;</p><p>平台转变的一大标志性特征，就是会迫使软件属性也同步发生演变。适配这些属性的软件通常会有专门的头衔，例如“云原生”或者“移动优先”。早在2016年，我就曾在演讲中介绍过“云原生”属性，即从静态思维方式向着动态思维方式的转变：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af4d3006fd17ff56f5965f8cb42812ac.png" /></p><p></p><p>（上图并非我在2016年使用的初版幻灯片，而是引入了更多现代元素。我们曾在随后几年多次在HashiCorp演示环节中用到。）</p><p>&nbsp;</p><p>我的观点是，左侧中的软件属于“传统软件”，即虽然也可以在云环境中运行，但却比不上具备同等功能、但采用动态/云原生方法的新软件。对于左边这类软件供应商来说，他们的产品也更有可能被构建同类软件、但具备右侧属性的初创公司所蚕食。举个例子，我一直认为如果老牌企业真能适应云原生世界，那么Vault压根就不会存在。</p><p>&nbsp;</p><p>软件属性变化的鲜明案例就是当初的移动应用。2010年初，我曾跟美国一家大型银行的CEO当面交流。他反问我，“你觉得人们选择更换银行的主要原因是什么？”在听了我提出的几个错误答案后，他表示“谁家的移动应用功能更丰富，人们就更愿意选谁。”也正因为如此，银行才需要在云服务和软件工程中投入这么多的资金，而这一切都将以移动功能的形式呈现。而那些长期对于移动设备的兴起态势无动于衷的银行，必将失去更多客户。</p><p>&nbsp;</p><p>另一个更古老的例子则是Web应用。相信大家也有印象：如果某项服务推出了Web版本，那么用户也更乐于选用。Web应用的性能表现也是同理，性能越好则吸引力越强。</p><p>&nbsp;</p><p>我推测AI领域也会出现类似的情况。将有某种属性将“新”与“旧”明确划分开来，而拥抱“新”产品和业务的厂商将对下一代用户产生更强大的吸引力。“旧”软件当然不会被立即淘汰，只是与“新”软件相比，前者会成为越来越缺乏吸引力的选项，而且双方的差距会随着时间推移而越来越大。</p><p>&nbsp;</p><p>当然，现在对这种属性做具体判断还为时过早，恐怕还须多年的积淀和发展才能找出有说服力的答案。不过从当前大语言模型提供的商品化自然语言界面来看，未来所有软件至少都需要提供某种形式的自然语言交互选项。例如拥有事件生成功能的日历应用、具有语言引导配置的命令行工具、提供高质量使用助手的SaaS等。这些都是相对容易解决的问题、能够为用户提供良好体验，而且未来很可能会成为市场对软件产品的基本要求。</p><p>&nbsp;</p><p>只要各行各业的老牌厂商能够意识到这股洪流、做出反应并适应转变，相信都可以平稳迈入生成式AI新时代。必须承认，除了AI增强之外，大多数应用的核心功能仍是其能够健康存续的善根前提。而AI技术的转变则为初创公司带来机遇，让他们能够成功超越那些迟迟不肯自我革新的传统巨头。</p><p>&nbsp;</p><p></p><h4>结识新朋友，不忘老朋友</h4><p></p><p>&nbsp;</p><p>我认为早期云计算之所以能够成功，一个非常重要的前提就是给出了妥善的迁移方案。尽管功能仍然有限，但“原样上云”确实在很大程度上降低了云计算的使用门槛。其他大型平台转变（例如容器化）也表现出了类似的特征。</p><p>&nbsp;</p><p>而云端之上的后续演进，例如Heroku或者更普遍的平台即服务（PaaS），则不具备这样的向下兼容能力。虽然早期PaaS也曾非常流行，但由于很难、甚至根本无法与“遗留”应用程序相集成，所以在平台变革浪潮中的意义相对有限。</p><p>&nbsp;</p><p>换句话说，那些要求抛弃旧技术才能采用的新技术，往往比积极接纳旧技术的新技术更难在整个行业中产生广泛影响、得到普遍接受。</p><p>&nbsp;</p><p>而如今的生成式AI则没有这样的问题。它们在为新型软件带来革命性变化的同时，也能在很大程度上让“遗留”软件变得更好。</p><p>&nbsp;</p><p>这也是我个人对Web3生态系统评价不高的一大主要原因。因为Web3划出了明确的边界：只存在dApp和非dApp，二者之间没有缓冲地带。我知道理论上某些功能可以采取“链上”构建，但很多功能仍然不行，这会严重限制整个生态系统的推广和落地。</p><p>&nbsp;</p><p></p><h4>总结</h4><p></p><p>&nbsp;</p><p>AI可能正在迎来自己的“平台转变”时刻，我也坚定相信与早期云计算拥有一系列优良品质的AI能够克服当下这些暂时性的挑战。</p><p>&nbsp;</p><p>如果判断正确，那么我们正身处这场漫长比赛的开头几局。假设把亚马逊云科技发布S3和EC2视为云平台转变的起点，那么整个生态系统将需要十年左右才能逐步发展成熟，而那些“传统”主流企业将在此期间持续遭受冲击、顽固不化者则被最终淘汰出局。</p><p>&nbsp;</p><p>当然，我也承认AI掀起的热潮在社会影响力方面要远超当初的云计算，所以也许市场成熟的时间周期会更短。但无论如何，我仍认为至少还有几年的“开放窗口”可供先行者们充分把握。</p><p>&nbsp;</p><p>未来的命运就取决于当下，请大家务必认真关注、努力把握。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://www.hashicorp.com/blog/mitchell-reflects-as-he-departs-hashicorp">https://www.hashicorp.com/blog/mitchell-reflects-as-he-departs-hashicorp</a>"</p><p><a href="https://mitchellh.com/writing/ai-through-a-cloud-lens">https://mitchellh.com/writing/ai-through-a-cloud-lens</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mTh6gNtBVcczp2JiPGsU</id>
            <title>国内服务器操作系统市场份额第一！欧拉部署累计610万套，市场份额达36.8%</title>
            <link>https://www.infoq.cn/article/mTh6gNtBVcczp2JiPGsU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mTh6gNtBVcczp2JiPGsU</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 06:50:30 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 操作系统大会2023, 欧拉, 开源社区, 基础软件技术
<br>
<br>
总结: 12月15日，在北京国家会议中心举办了以“崛起数字时代，引领数智未来”为主题的操作系统大会2023。大会旨在汇聚全球产业界创新力量，推动基础软件技术持续创新，共建全球开源新生态。欧拉作为中国第一服务器操作系统，在技术创新、生态发展、社区合作、商业落地等方面取得了跨越式发展。开放原子开源基金会和openEuler开源社区也成为中国最具活力和创新力的开源社区之一。操作系统需要面向AI不断演进，欧拉已支持多种计算架构，并与AI深度结合，使得操作系统更智能，提升AI训练和推理性能。 </div>
                        <hr>
                    
                    <p>12月15日，以“崛起数字时代，引领数智未来”为主题的操作系统大会2023在北京国家会议中心举办，大会由开放原子开源基金会、中国电子技术标准化研究院、国家工业信息安全发展研究中心、中国软件行业协会共同主办，旨在汇聚全球产业界创新力量，构筑坚实的基础软件根基，推动基础软件技术持续创新，共建全球开源新生态。</p><p></p><h2>欧拉累计装机量超过610万套</h2><p></p><p>&nbsp;</p><p>据介绍，截至目前，欧拉累计装机量超过610万套，根据IDC预测，2023年欧拉在中国服务器操作系统市场份额达到36.8%。开源四年，欧拉实现了跨越式发展，成长为中国第一服务器操作系统，并在技术创新、生态发展、社区合作、商业落地上建立了完善的发展体系，形成了产业正循环。</p><p>&nbsp;</p><p>在技术生态方面，欧拉与国际主流基金会深度合作，已支持全球98%的主流开源软件；作为CI操作系统在云原生、大数据、存储、数据库、HPC等数十款开源社区，欧拉实现了上游原生支持，开箱即用。欧拉深度参与OpenChain、OpenSSF等全球主流软件供应链安全标准与规范的制定、推广，率先通过OpenChain ISO 5230开源软件协议认证，社区基础设施达到OpenSSF SLSA L3标准。此外，还与全球主流社区和组织合作，满足全球各区域本地化要求，规范欧拉开源社区的国际化治理。目前，openEuler社区与9大海外头部开源基金会开展深入合作，为150多个国家和地区提供服务，构建全球开源新生态，开创了中国开源新模式。</p><p>&nbsp;</p><p>开放原子开源基金会理事长孙文龙在大会致辞中表示，充分利用开源、参与开源、支持开源、回馈开源，是实现操作系统技术创新和产业繁荣的有效路径。</p><p>&nbsp;</p><p>openEuler开源社区秉承“共建、共享、共治”的原则，携手全产业链共建可持续发展的操作系统产业生态。社区开源以来，已吸引1300+家头部企业、研究机构和高校加入，汇聚16800+名开源贡献者，成立100+个特别兴趣小组（SIG），openEuler开源社区已成为中国最具活力和创新力的开源社区。</p><p>&nbsp;</p><p>作为openEuler社区成员单位之一，华为高级副总裁、ICT战略与Marketing总裁彭松在大会致辞中表示，华为将持续聚焦根技术投入，提升基础软件的创新力和竞争力，支撑数字基础设施的建设和应用软件生态繁荣；拥抱智能化，促进操作系统和AI融合，激发基础软件创新；推动产学研结合，培养基础软件以及ICT产业人才，为技术创新提供源源不断的动力。</p><p>2023年度openEuler领先商业实践项目揭晓，22个项目签约捐赠意向</p><p>&nbsp;</p><p>随着欧拉在各行各业规模应用，涌现出大批优秀的创新实践，有力推动行业数字化转型深入。为充分发挥openEuler领先商业实践在行业内的示范作用，引导更多新行业新领域应用落地，加快构筑繁荣共赢的产业生态，OpenAtom openEuler社区联合国家工业信息安全发展研究中心，携手业界专家，围绕技术创新性、示范推广价值、应用规模、服务运维能力、社区贡献五大维度对公开征集的商业实践成果完成多轮遴选，最终评选出15个2023年度openEuler领先商业实践项目。</p><p>&nbsp;</p><p>openEuler捐献给开放原子开源基金会后得到加速发展，2022年12月，开放原子开源基金会宣布openEuler升级为项目群，在治理章程、社区运作、资金募集等方面可独立项目运作，接受开源项目加入，使用openEuler项目群基础设施、运营、营销等资源。2023年上半年，9个项目完成捐赠意向签约，此次大会新增13个项目捐赠意向签约，多为解决方案类、涉及全场景、全开发流程的项目，捐赠后将从云原生、AI、智能化、可监控等维度为openEuler注入新的创新力量。</p><p>&nbsp;</p><p>此外，OpenAtom openEuler社区联合中国电子技术标准化研究院及多位业界专家，根据团体标准T/CESA 1270.5-2023，从技术、治理、生态建设等贡献维度进行评价，选出15家突出贡献的先进单位。</p><p></p><h2>智能时代，操作系统需要面向AI不断演进</h2><p></p><p>&nbsp;</p><p>过去一年，以大模型、大算力为代表的技术创新不断推动人工智能的发展，AI加速进入行业生产系统，改变千行万业产业格局。智能时代，操作系统需要面向AI不断演进。一方面，在操作系统开发、部署、运维全流程以AI加持，让操作系统更智能；另一方面，操作系统也需要适应AI的发展要求，满足通用算力和AI算力异构融合，更好的使能上层AI应用。</p><p>&nbsp;</p><p>目前，欧拉已支持ARM，x86，RISC-V等全部主流通用计算架构，在智能时代，欧拉也率先支持NVDIA、昇腾等主流AI处理器，成为使能多样性算力的首选。</p><p>&nbsp;</p><p>欧拉和AI深度结合，一方面使用ChatGLM基础模型，基于大量欧拉操作系统的代码和数据，训练出EulerCopilot，初步实现代码辅助生成、问题智能分析、系统辅助运维等功能，让欧拉更智能。另一方面，欧拉通过异构资源统一管理与调度，统筹内存和算力，实现CPU和NPU的深度融合，充分挖掘空闲资源，提升有效利用率，进而提升AI训练和推理性能，使能AI更高效。</p><p>&nbsp;</p><p>中国工程院邬贺铨院士在主题发言中表示，紧抓“算网融合”大趋势，相信在openEuler社区上万名开发者和千家伙伴的努力下， openEuler将成为全场景协同数字基础设施的坚实底座，也将助力中国工业互联网实现跨越式的发展。</p><p>&nbsp;</p><p>中国工程院倪光南院士在主题发言中表示，“欧拉”的经验充分证明，只要中国开发者协同起来，就有能力创建世界一流的开源社区。呼吁产业界共同携手，推动开源欧拉成为中国业界带头创建的、首个具有世界影响力的开源社区。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/XbbSlEGawfOpXBSAcI7f</id>
            <title>子曰教育大模型加速落地应用：推出虚拟人AI产品Hi Echo 2.0，新增口语定级等功能</title>
            <link>https://www.infoq.cn/article/XbbSlEGawfOpXBSAcI7f</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/XbbSlEGawfOpXBSAcI7f</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 04:44:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 教育科技公司, 虚拟人口语教练, Hi Echo 2.0版本, 口语能力提升
<br>
<br>
总结: 网易有道发布了全球首个虚拟人口语教练Hi Echo的2.0版本。Hi Echo是一款能够提升口语能力的虚拟教练，具备发音地道、共情能力强等特点。新版本新增了口语难度分级、丰富的虚拟人形象、多元的对话场景和个性化的对话评价报告等功能。用户可以根据自己的学习阶段和英语水平进行对话练习，同时获得详尽的口语分析报告和发音指导。此外，Hi Echo还具备标准MBTI人格模型系统，用户可以选择不同的虚拟形象进行对话。无论是初学者还是有一定口语基础的学习者，都可以通过Hi Echo快速提升口语能力。 </div>
                        <hr>
                    
                    <p>12月15日，教育科技公司网易有道公布了全球首个虚拟人口语教练Hi Echo的2.0版本。Hi Echo 是全球首个虚拟人口语私教，于今年10月正式推出，其搭载了国内首个教育大模型子曰，是国内最早实现大模型能力真正落地的教育类应用。Hi Echo表情生动、发音地道，还能像“社牛”一样轻松掌控各种话题，超强的语言能力和共情能力刷新了大众对于口语教练的常规认知，快速受到大量用户欢迎。仅仅三周时间，注册用户便突破10万。据介绍，Hi Echo还被苹果应用商店首页推荐，成为首个被苹果官方推荐的虚拟人AI产品。</p><p>&nbsp;</p><p>最新推出的Hi Echo 2.0版本进行了四大能力创新升级——新增口语难度分级；更丰富的虚拟人形象；更多元的对话场景及更具个性化的对话评价报告。即便是零基础的英语学习者，也可以毫无压力地开口说英文，在不同语境中快速提升口语能力，实现真正的英语对话自由。</p><p>&nbsp;</p><p>Hi Echo 2.0新增的“口语定级”能力能根据用户的学习阶段和英语水平，将其实际口语能力划分为“小学、初中、高中、大学、工作中”五个阶段及初级、中级、高级三个等级。不同能力等级的用户可以根据自己的需求和实际水平获取相对应的对话练习，从单词难度、句子长度到语法技巧，Hi Echo能够提供差异化的对话语句。即便是英语口语小白，在面对Hi Echo时，也能避免出现“哑口无言”的情况。</p><p>&nbsp;</p><p>在Hi Echo 2.0中，对话过程中的每句话都将被完整保留。在对话结束后，系统会提供详尽个性化口语分析报告。除了此前所具备的发音及语法打分、润色外，Hi Echo 2.0还提供音素级别的单词发音指导。无论英音还是美音，都能给予用户精准的发音纠正，使用户对自己的薄弱环节和不足之处有更清晰的了解。</p><p>&nbsp;</p><p>除此之外，Hi Echo还是全球首个具备标准MBTI人格模型系统的虚拟人口语教练。在本次升级版本中，除了北京姑娘Echo外，新增了英国绅士Daniel和中加混血Sherry两个虚拟形象，他们有各自的人格特点，用户可以选择自己喜欢的形象无限畅聊。</p><p>&nbsp;</p><p>多元对话场景方面，Hi Echo本次拓展至10个主题、88个子场景的对话方向，还支持用户自定义话题。无论是谈论兴趣理想还是热点事实，Hi Echo都能够灵活切换各种语境，运用专业且地道的词汇和表达方式，配合用户畅快交谈。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6kV1dgYUepjSUjW9v2oz</id>
            <title>腾讯金融云技术总监全成确认出席 QCon 上海，分享大语言模型金融智能应用研发实战与探索</title>
            <link>https://www.infoq.cn/article/6kV1dgYUepjSUjW9v2oz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6kV1dgYUepjSUjW9v2oz</guid>
            <pubDate></pubDate>
            <updated>Fri, 15 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 大语言模型金融智能应用研发实战与探索, AI 大语言模型, 金融智能化应用
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，腾讯金融云技术总监全成将分享关于大语言模型金融智能应用的实践经验，探讨如何利用 AI 大语言模型解决金融领域的实际问题，并分享搭建可维护可持续可运维的金融智能化应用的工程化视角。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1215&amp;utm_content=quancheng">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。腾讯金融云技术总监全成将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5611?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1215&amp;utm_content=quancheng">大语言模型金融智能应用研发实战与探索</a>"》主题分享，探讨基于 AI 大语言模型的金融智能应用平台搭建实践，在实际应用场景中，如何利用 AI 大语言模型端到端的解决实际问题，以及以工程化视角，如何搭建可维护可持续可运维的金融智能化应用。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5611?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1215&amp;utm_content=quancheng">全成</a>"，之前曾在某金融集团公司参与关系网络风控识别项目，和说话人声纹识别比对项目；在券商互联网企业参与过大数据量化投资项目。目前在腾讯主要参与 TX2SQL 大模型智能应用项目，负责专业知识及推理大模型微调，构建端到端 TX2SQL 问答系统。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大语言模型金融智能应用研发实战与探索</p><p></p><p>介绍基于 AI 大语言模型的金融智能应用平台搭建实践，分解在实际应用场景中，如何利用 AI 大语言模型端到端的解决实际问题，并以工程化视角，分享如何搭建可维护可持续可运维的金融智能化应用。</p><p></p><p>演讲提纲：</p><p></p><p>AI 大语言模型在实际应用中面临的问题及挑战AI 大语言模型金融智能应用架构，包括业务架构、技术架构、模型矩阵AI 大语言模型金融智能应用介绍，包括智能客服、TXT2SQL 数据分析、OCR 平台</p><p></p><p>听众受益点：</p><p></p><p>○ 大模型时代，智能化应用的研发模式</p><p>○ 端到端智能应用搭建与大模型微调、Prompt 工程等</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 1 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DWIO6WPCGEzkcXsHISgz</id>
            <title>2023 英特尔 On 技术创新大会中国站“剧透”：五大专题论坛，全面赋能 AI 开发</title>
            <link>https://www.infoq.cn/article/DWIO6WPCGEzkcXsHISgz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DWIO6WPCGEzkcXsHISgz</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 07:44:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 新一代 AI PC 计算平台, 新一代至强平台, 边云协同
<br>
<br>
总结: 中国专家将深度讲解最新一代增强 AI 能力的计算平台，支持开放、多架构的软件方案，塑造未来的技术和应用创新。 </div>
                        <hr>
                    
                    <p>中国专家将深度讲解最新一代增强 AI 能力的计算平台，支持开放、多架构的软件方案，塑造未来的技术和应用创新。欢迎大家扫描海报二维码注册参会！</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae4772fd58da5f636300260425eff44c.png" /></p><p></p><p></p><h4>&nbsp;1. 人工智能</h4><p></p><p>在包括客户端、边缘计算和数据中心的英特尔平台上，可以针对不同业务场景，选用各种英特尔® CPU、GPU 和 ASIC 等深度学习计算引擎，配以 OpenVINO™ 等开发及优化工具链，加速生成式人工智能的持续调参优化和快速部署，让开发者能够灵活便捷地开发和交付面向消费者和企业级的 LLM/AIGC 业务软件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ae38b173df6c56f9ff1a32fc66201d0.png" /></p><p></p><p></p><h4>&nbsp;2. 新一代 AI PC 计算平台</h4><p></p><p>为 PC 计算平台开发 AI 应用需要强大的硬件性能、优化的能耗、先进的工具和开放的生态系统。在本专题中，您将深入了解英特尔新一代 AI PC 硬件平台及其出色的连接性如何助力您更轻松地进行 AI 应用开发和游戏开发。还将展示英特尔开放的软硬件生态系统和与合作伙伴推出的最新解决方案，帮助您了解客户端开发领域的前沿动态。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d1cf9d32a884cd32ba6980e18daa33b.png" /></p><p></p><p></p><h4>&nbsp;3. 新一代至强平台</h4><p></p><p>云计算和大模型时代浪潮正在革命性地重新定义云计算，全新一代英特尔® 至强® 可扩展处理器应运而生。其内置的英特尔® AMX 加速引擎为 AI 带来极致加速性能表现，其他众多内置加速器 可对各个云业务进行加速和性能优化；同时，在英特尔® SGX、英特尔® TDX “双保险” 的加持下，安全的运行环境能为业务保驾护航；英特尔还具备丰富的全栈软件优化，赋能各个云软件，充分释放至强潜能, 软硬协同加速云计算及 AI 创新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/780d70d67d4d3e5342fa3b0260616cde.png" /></p><p></p><p></p><h4>&nbsp;4. 边云协同</h4><p></p><p>在云原生开发和边缘智能方面处于领先地位的英特尔，协同多年来与创新合作伙伴合作所积累的现代“边云协同”应用程序开发经验、技术、产品和平台，赋能以生成式 AI 大模型为核心的混合式 AI 解决方案，加速其商业化落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82fa0feeb70459425a51977ffadcea45.png" /></p><p></p><p></p><h4>&nbsp;5. 先进技术</h4><p></p><p>本专题中将介绍，为计算行业面临的最大挑战寻求变革性解决方案，并了解将影响英特尔未来 2-5 年及以后工作的技术发展。主题包括摩尔定律的未来、芯片设计的未来、量子计算、AI 的未来、神经拟态计算、面向“双碳”的智能计算等等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f901a884d2635ae1c3a59ff308e56c9d.png" /></p><p></p><p>英特尔 On 技术创新大会中国站，2023 年 12 月 19 日 09:00 正式上线，欢迎大家即刻扫码注册，参加一场面向智算时代开发者的年度技术盛会！助力开发者，让 AI 无处不在！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/h6397o17RRiXV73zbH2P</id>
            <title>Gemini演示视频“翻车”后，谷歌接连放大招：向云客户免费提供Gemini Pro，还推出AI代码辅助工具，集成25家公司数据集</title>
            <link>https://www.infoq.cn/article/h6397o17RRiXV73zbH2P</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/h6397o17RRiXV73zbH2P</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, AI模型, Gemini, Pro版
<br>
<br>
总结: 谷歌发布了功能强大的AI模型Gemini，其中Pro版是面向开发者和企业的版本。Gemini采用了谷歌迄今为止最强大的大语言模型架构，可以处理高强度工作负载。谷歌计划在未来推出更多Gemini版本，并将其引入更多开发者平台。 </div>
                        <hr>
                    
                    <p>上周，谷歌公布了该公司有史以来体量最大、功能最强的 AI 模型 Gemini，这也是谷歌在推动 AI 实际落地过程中的重要一步。Gemini 模型共分为三个版本：Ultra 版、Pro 版与 Nano 版。谷歌已经开始在自家产品组合中引入 Gemini：从 Pixel 8 Pro 开始，Gemni Nano 将正式登陆 Android 系统；而经过专门微调的 Gemini Pro 则即将现身 Google Bard。</p><p>&nbsp;</p><p>12 月 13 日，谷歌在其云平台上推出了一系列 AI 模型以供用户体验并实际应用：包括向开发者和企业开放 Gemini Pro、面向开发者和安全运营的 Duet AI、图像生成 Imagen 2 以及用于医疗保健场景的 MedLM。</p><p></p><h2>谷歌正式开放 Gemini Pro</h2><p></p><p>&nbsp;</p><p>Gemini 属于完整的内容生成模型家族，据称采用了谷歌迄今为止最强大的大语言模型架构。在此之前，微软和包括谷歌在内的各家云服务及商业 IT 巨头纷纷在自家产品中引入所谓机器学习增强功能。而从目前的态势来看，这股潮流很可能会延续 2023 年全年，并在 2024 和 2025 年继续成为核心趋势。</p><p>&nbsp;</p><p>Gemini 提供多种参数规模，其中 Nano 版最小、面向设备端工作负载；Pro 版居中；而体量最大的 Ultra 版则负责处理后端服务器上的高强度工作负载。</p><p>&nbsp;</p><p>12 月 13 日，谷歌开始向开发者和企业开放 Gemini Pro，供其根据自有用例进行构建。据悉，谷歌将在未来几周到几个月内持续收集用户反馈，并据此对模型做进一步微调。明年初，在经过进一步微调、安全测试并收集来自合作伙伴的宝贵反馈之后，谷歌将正式推出 Gemini Ultra——这也是谷歌旗下体量最大、功能最强、可执行高度复杂任务的顶尖模型。谷歌还计划将 Gemini 引入更多开发者平台，包括 Chrome 和 Firebase。</p><p>&nbsp;</p><p>关于 Gemini Pro 更多详细信息：</p><p>&nbsp;</p><p>Gemini Pro 在各类研究性基准测试中的性能表现，优于其他同等体量的大语言模型。当前版本提供 32K 文本上下文窗口，后续版本的上下文窗口还将进一步扩大。Gemini Pro 将在一定时段内提供免费使用，最终定价也将具有竞争力。它提供一系列功能：函数调用、嵌入、语义检索、自定义知识背景以及聊天功能等。它支持全球 180多个国家和地区的 38 种语言。在当前版本中，Gemini Pro 接受文本作为输入，并可生成文本输出。谷歌此次还发布了专用的 Gemini Pro Vision 多模态端点，可接受文本和图像作为输入，并据此输出文本响应。Gemini Pro 提供的 SDK 将帮助用户构建出可在任何地方运行的应用程序。Python、Android（Kotlin）、Node.js、Swift 和 JavaScript 均在支持之列。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/bf/bf660fa989d23870b0007992f5561a1c.png" /></p><p>&nbsp;</p><p>目前，Gemini Pro 的首个版本现可通过 Gemini API 进行访问：开发者可以使用此远程接口在 Gemini Pro 上构建自己的聊天机器人应用，还可以认真设计提示词并提交自有数据以对模型做出微调，再将其接入其他 API，借此在特定任务之上获得更好的处理能力与功能选项。如果希望在自己的应用程序中引入自然语言界面，Gemini Pro 应该会是个好选择，且使用体验与 OpenAI 的 ChatGPT 等同类产品基本一致。</p><p></p><h4>Google AI Studio：速度最快的 Gemini 构建选项</h4><p></p><p>&nbsp;</p><p>谷歌还发布了一款基于 Web 的免费开发者工具——Google AI Studio，可帮助用户快速设计提示词，而后获取 API 密钥以用于应用程序开发。开发者可以使用谷歌账户登录 Google AI Studio 并享受免费配额，免费部分每分钟可接收 60 条请求，数量达到其他同类免费产品的 20 倍。准备就绪之后，只需单击“获取代码”即可将生成结果转移至指定的 IDE，也可以使用 Android Studio、Colab 或者 Project IDX 中提供的各种快速入门模板。为了帮助谷歌提高产品质量，在用户使用免费配额时，经过培训的审核人员可能会访问 API 及 Google AI Studio 上的输入和输出。谷歌表示，谷歌账户及 API 密钥中的身份信息均经过脱敏处理。</p><p></p><h4>在 Google Cloud 使用 Vertex AI 进行构建</h4><p></p><p>&nbsp;</p><p>如果需要全托管AI平台，开发者也可以轻松从 Google AI Studio 转向 Vertex AI。后者允许通过全面的数据控制来自定义 Gemini，且充分享受 Google Cloud 提供的企业安全、隐私、数据治理与合规性保障。</p><p>&nbsp;</p><p>借助 Vertex AI，同样可以访问 Gemini 模型，并能够：</p><p>&nbsp;</p><p>使用自有企业数据微调及蒸馏 Gemini，立足底层对模型进行增强，使其包含最新信息和扩展以获取实际功能。在低代码/无代码环境中构建 Gemini 支持的搜索和对话 agent，包括支持检索增强生成（RAG）、混合搜索、嵌入、对话 playbook 等。安心进行应用部署。谷歌不会利用 Google Cloud 上的客户输入或输出数据训练 Gemini 模型，相关数据与 IP 将始终归客户所有。</p><p>&nbsp;</p><p>目前，开发者可以通过 Google AI Studio 免费访问 Gemini Pro 与 Gemini Pro Vision，每分钟最多支持 60 条请求，可以满足大部分应用开发需要。Vertex AI 计划于明年发布正式版本，在此之前开发者同样能以每分钟 60 条请求的方式访问 Gemini 基础模型。未来，Google AI Studio 与 Vertex AI 将以每 1000字符/1 张图片为单位收取费用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be734eca7b6ee57d2beb3a6fe1f0a8bd.png" /></p><p></p><h2>面向开发者和安全运营的 Duet AI</h2><p></p><p>&nbsp;</p><p>谷歌此次还正式公布了 Duet AI for Developers。这是一项聊天机器人服务，旨在提高程序员群体的工作效率。很明显，这就是目前常见的编程助手产品。根据谷歌的介绍，它能与各种 IDE 配合使用，并尝试在开发者输入过程中自动补全源代码、回答编码查询问题、帮助排除故障，并就如何使用 MongoDB、Crowdstrike 等第三方软件提供操作指导。</p><p>&nbsp;</p><p>谷歌副总裁 Gabe Monroy 解释道，“例如，使用 MongoDB 编写代码的开发人员可以询问 Duet AI for Developers，“请按地理位置筛选过去 30 天内消费额超过 50 美元的客户订单，再计算各地区的总收入”。之后，Duet AI for Developers 就会使用 MongoDB 中的产品信息提供代码建议并完成任务。如此一来，开发人员的构建速度将得到显著提升。”</p><p>&nbsp;</p><p>据悉，目前已经有超过 25 家供应商与谷歌合作，确保自家产品能够顺畅对接 Duet AI for Developers。</p><p>&nbsp;</p><p>在未来几周内，负责为 Duet AI 服务提供支持的大语言模型也将全面升级为 Gemini。这项开发者服务计划免费开放至 2024 年 1 月 12 日。此外，Duet AI in Security Operations 这次也正式开放，这款聊天机器人将帮助处理基础设施保护、网络日志分析等查询任务。</p><p></p><h2>图像生成 Imagen 2 模型与用于医疗保健场景的 MedLM 模型</h2><p></p><p>&nbsp;</p><p>本次，谷歌还更新了 Vertex AI 以引入 Imagen 2 模型。据介绍，这款文本到图像工具由 Google DeepMind 工程师开发而成，其最新版本已经能够生成极为逼真的图片并准确响应文本要求，大大降低了品牌宣传门槛。此外，Imagen 2 还能生成注释并回答与图像内容有关的问题。</p><p>&nbsp;</p><p>社交应用 Snapchat、图形设计平台 Canva 以及图片库网站 Shutterstock 都在使用 Imagen。而且 Imagen 2 模型生成的所有图像都将包含人眼不可见的 SynthID 数字水印，可通过计算检测来判断该图像是否为 AI 合成。</p><p>&nbsp;</p><p>此外，谷歌还推出了 MedLM，这是一个面向医疗保健用例的大语言模型家族。其中的两套模型均基于谷歌自家的 Med-PaLM 2 系列。其中较大、更强的模型专为较复杂的任务而设计，例如筛选学术论文及技术文档以提供潜在的新药研发线索；另一套模型则负责处理比较简单的杂务，例如总结医患对话和回应常见的医疗咨询问题。</p><p>&nbsp;</p><p>MedLM 模型的早期采用者包括 HCA Healthcare 诊所、药物设计企业 BenchSci，以及埃森哲与德勤等。</p><p>&nbsp;</p><p>谷歌表示，未来几周，MedLM 模型将正式入驻谷歌的开放 Model Garden，后续还将有更多基于 Gemini 的模型被纳入 MedLM 家族以提供更多功能。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://blog.google/technology/ai/google-gemini-pro-imagen-duet-ai-update/">https://blog.google/technology/ai/google-gemini-pro-imagen-duet-ai-update/</a>"</p><p><a href="https://blog.google/technology/ai/gemini-api-developers-cloud/">https://blog.google/technology/ai/gemini-api-developers-cloud/</a>"</p><p><a href="https://www.theregister.com/2023/12/13/google_gemini_duet_ai/">https://www.theregister.com/2023/12/13/google_gemini_duet_ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/t9Olysl803omSfN5JfVe</id>
            <title>Anthropic 发布 Claude 2.1 大模型，提供更宽的上下文窗口并支持 AI 工具</title>
            <link>https://www.infoq.cn/article/t9Olysl803omSfN5JfVe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/t9Olysl803omSfN5JfVe</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 04:57:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Anthropic, Claude 2.1, 模型特性, 降价措施
<br>
<br>
总结: Anthropic最新版本的Claude 2.1模型提供了许多关键特性的进步，包括行业领先的上下文窗口、幻觉率降低、系统提示词以及降价措施。该模型具有更大的上下文窗口容量，输出虚假陈述的可能性更小。它还可以使用外部工具并与API交互，响应查询更加高效。此外，模型的价格也更加实惠。然而，对于该模型的评价褒贬不一，一些用户赞扬了其功能改进，而其他用户则对其拒绝响应情况和严格的审查表示失望。 </div>
                        <hr>
                    
                    <p>据 Anthropic 称，最新版本的 Claude 大模型为企业提供了许多“关键特性方面的进步，包括行业领先的 200K token 上下文窗口、模型幻觉率显著降低、系统提示词以及我们新开发的测试功能：支持外部工具”。Anthropic 还宣布了降价措施，以提升各款模型用户的成本效益。</p><p>&nbsp;</p><p>增强的上下文窗口是 Claude 2.1 的一项亮点特性，其拥有 200,000 个 token 的容量，超过了 OpenAI 的 GPT-4，后者提供了 128,000 个 token 的窗口。Anthropic 表示，与之前的模型相比，新模型输出虚假陈述的可能性更小。Claude 2.1 会试图避免不正确的答案并承认一些问题存在不确定性，它输出相关答案时一般会选择提出质疑，而不是提供不正确的信息。Anthropic 表示，该模型输出的错误答案减少了 30%，并且模型错误地作出缺乏信源的判断的比率大大降低。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a84459e202646167175df02ee536748e.jpeg" /></p><p></p><p>另一个值得注意的新增特性是 Claude 2.1 使用工具并与 API 交互的能力。该功能让模型能够利用计算器、数据库等外部资源，甚至执行网络搜索来更有效地响应查询。它还可以集成到用户的技术栈中，从而在各个领域中实现更多样化的应用。</p><p>&nbsp;</p><p>此外，Claude 2.1 引入了系统提示词，使用户能够为其请求设置特定的上下文。此功能可确保模型的响应更加结构化且前后一致。现在模型的价格定为输入的提示词每百万 token 8 美元，模型输出则是每百万 token 24 美元，这样包括开发人员和企业在内的很多用户群体都能负担得起了。</p><p>&nbsp;</p><p>一些用户对新模型的评价褒贬不一。从积极的一面来看，一些用户发现 Claude 2.1 非常适合聊天和摘要等任务，并赞扬了它的进步和功能改进，特别是在摘要任务方面。然而，其他用户也对该模型的拒绝响应情况和严格的审查表示失望，一些用户认为这让这款工具的实用性和自主性打了折扣。此外，由于严格的安全协议和内容指南，人们担心 Claude 在处理某些内容（例如学术或研究材料）方面存在局限性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d04c3c66956f6211f1baf9746682c76.jpeg" /></p><p></p><p></p><blockquote>发现：在 200K 个 token（近 470 页）的情况下，Claude 2.1 能够回忆起某些文档级深度的事实文档最顶部和最底部的事实被回忆的准确率接近 100%位于文档顶部的事实的回忆性能低于底部（类似于 GPT-4）从 ~90K token 开始，文档底部的回忆性能开始变得越来越差无法保证短上下文长度下的性能 - Greg Kamradt</blockquote><p></p><p></p><p>Anthropic 及时推出 Claude 2.1 的时机恰逢 OpenAI 的内部冲突时期，后者导致 ChatGPT Plus 订阅暂停购买，首席执行官 Sam Altman 也陷入了风波。尽管如此，Devin Coldewey 写道，“不管怎样，GPT-4 仍然是代码生成领域的黄金标准，Claude 处理输入请求的方式与竞争对手是不一样的，有些更好，有些更差。”</p><p>&nbsp;</p><p>想要了解更多关于 Claude 2.1 细节的用户可以参考 Anthropic 网站上的模型介绍<a href="https://www-files.anthropic.com/production/images/ModelCardClaude2_with_appendix.pdf?dm=1700589594">页面</a>"。 Anthropic 还制作了一个示例<a href="https://github.com/anthropics/anthropic-tools">存储库</a>"，演示如何使用工具功能。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/11/anthropic-announces-claude-2-1/">https://www.infoq.com/news/2023/11/anthropic-announces-claude-2-1/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hDvLlU98PB2bRXcQd1Xa</id>
            <title>华为公共开发部 /Web 前端技术专家涂旭辉确认出席 QCon 上海，分享 LLM 赋能声明式前端框架调试的实践与思考</title>
            <link>https://www.infoq.cn/article/hDvLlU98PB2bRXcQd1Xa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hDvLlU98PB2bRXcQd1Xa</guid>
            <pubDate></pubDate>
            <updated>Thu, 14 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 涂旭辉, 声名式前端框架调试, 大语言模型赋能
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，华为前端技术专家涂旭辉将分享关于大语言模型赋能声名式前端框架调试的实践与思考。他将介绍如何利用大语言模型结合 record & replay 进行交互式调试，帮助开发者高效准确地定位问题根因，为前端开发带来全新的调试范式。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1214&amp;utm_content=tuxuhui">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。华为公共开发部 /Web 前端技术专家涂旭辉将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5633?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1214&amp;utm_content=tuxuhui">LLM 赋能声明式前端框架调试的实践与思考</a>"》主题分享，探讨如何将大语言模型赋能前端调试领域，结合 record &amp; replay 对声名式框架进行交互式调试。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5633?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1214&amp;utm_content=tuxuhui">涂旭辉</a>"，毕业于西安交通大学自动化系，就职于华为西安研究所，参与开源前端框架研发与探索相关工作，是开源前端框架 openInula 核心贡献者，负责框架技术演进规划及生态拓展等工作，目前担任 openInula AI 赋能技术项目负责人，主导 AI 辅助前端开发工具实践与探索。他在本次会议的演讲内容如下：</p><p></p><p>演讲：LLM 赋能声明式前端框架调试的实践与思考</p><p></p><p>随着 AI 技术的快速发展，ChatGPT 的亮相进一步提高了人们对生成式 AI 的期待，大语言模型赋能千行百业的时代已经到来。本次演讲将介绍如何将大语言模型赋能前端调试领域，结合 record &amp; replay 对声名式框架进行交互式调试。开发者通过调试聊天框与模型互动，大模型对缺陷库进行学习增强程序分析推理能力并基于时间戳给出调试建议，开发者结合经验执行调试给出反馈，可以帮助开发者高效准确定位问题根因，为开发者带来全新开发调试范式。</p><p></p><p>演讲提纲：</p><p></p><p>背景与趋势</p><p>○ 前端框架发展 next </p><p>○ LLM 赋能千行百业</p><p>AI 赋能前端领域洞察</p><p>○ why AI for debug </p><p>○ 技术选型</p><p>程序分析技术在前端调试的应用</p><p>○ 程序切片技术 </p><p>○ 程序分析 × LLM</p><p>人机交互调试解决方案</p><p>○ 传统声名式前端框架调试流程 </p><p>○ record &amp; replay 交互式调试流程 </p><p>○ 整体技术架构 </p><p>○ 实践问题经验分享</p><p>未来与展望</p><p>○ AI 赋能前端开发全场景</p><p></p><p>听众收益点：</p><p></p><p>○ 传统前端调试 vs AI 赋能调试</p><p>○ 程序切片在前端调试的应用</p><p>○ LLM 对前端开发提效的思考</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 2 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZvLIKAVY5zN2FrTWrO3H</id>
            <title>玩腻了CityWalk，不如来场构建者的生成式 AI BusTour</title>
            <link>https://www.infoq.cn/article/ZvLIKAVY5zN2FrTWrO3H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZvLIKAVY5zN2FrTWrO3H</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 10:32:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式 AI, 全国巡回, 亚马逊云科技, 构建者游乐场
<br>
<br>
总结: 亚马逊云科技将举办一场全国巡回活动，展示他们的生成式 AI 技术。这次活动将在多个城市举行，参与者可以了解生成式 AI 的发展历程和应用。活动中还将提供生成式 AI 的实际应用体验和相关课程，以及亚马逊云科技的认证折扣。这是一次充满创意和可能性的旅程。 </div>
                        <hr>
                    
                    <p>全国巡回的不一定是演唱会，还可以是生成式 AI。</p><p>了解生成式 AI 不一定纯听分享，还可以来一场“未来旅行”。</p><p>&nbsp;</p><p>12月13日，承载着最前沿生成式 AI 技术之旅正式启程！</p><p>上海、南京、杭州（更多城市敬请期待）</p><p>&nbsp;</p><p>当穿越信息时代与智能时代的“九又四分之三”站台突然出现在你家门口，</p><p>这一定称得上是技术圈的顶级浪漫了！</p><p>上车！Let's 构，欢迎进入生成式 AI 的魔法世界~</p><p>&nbsp;</p><p></p><h2>够硬核的“构”</h2><p></p><p></p><h3>👉 亚马逊云科技 AI 历史墙</h3><p></p><p>&nbsp;</p><p>这是架时光机，带你穿越时空，</p><p>探索亚马逊云科技生成式 AI 的过去、现在和未来。</p><p>从基础模型训练与推理的基础设施，</p><p>到大语言模型及其他基础模型构建工具，</p><p>再到基于基础模型的生成式 AI 应用......</p><p>你能看到生成式 AI 发展的每一个脚印,</p><p>你可以和生成式 AI 从初创阶段到现在的每一个关键里程碑撞个满怀。</p><p>&nbsp;</p><p></p><h3>👉 re:Invent 全新发布的生成式 AI Demo</h3><p></p><p>&nbsp;</p><p>脑力空间的无限复制，生产效率再次迎来大幅提升，</p><p>没错，人类历史上的第四次技术革命，它来了！</p><p>在这样一个转折点，你永远无法预见下一次睁眼，</p><p>生成式 AI 将为我们的生活带来怎样惊人的改变，</p><p>却可以抢先对这个时代最新发布的 AI 产品、令人尖叫的技术进行体验。</p><p>不用去拉斯维加斯，</p><p>我们将 re:Invent 全新发布的 Demo 带到了你的身边！</p><p>&nbsp;</p><p></p><blockquote>Amazon Q全新的企业级生成式 AI 助手，根据业务为开发者量身定制！快速获得业务场景复杂问题答案、生成内容并采取行动。&nbsp;Amazon CodeWhisperer省心、省力、省时的 AI 编程助手。&nbsp;Amazon CodeCatalyst汇集计划、编码、构建、测试和部署其应用程序所需的一切，简化应用程序的开发与交付。</blockquote><p></p><p></p><h2>够炫酷的“构”</h2><p></p><p></p><h3>👉 构建者游乐场 PartyRock</h3><p></p><p>&nbsp;</p><p>让创意照进现实，竟然可以不掉头发！</p><p>操纵 PartyRock ，只需要几个点击，</p><p>就可以让 AI 帮你生成脑洞大开的应用程序。</p><p>总有一些任务，</p><p>不是自己干干不起，而是交给 AI 处理更有性价比。</p><p>在 【下一站 GenAI 】构建者游乐场，</p><p>感受创意与 AI 的完美结合，开启一段奇妙的创意之旅！</p><p>&nbsp;</p><p></p><h3>👉 最新生成式 AI Jam 挑战</h3><p></p><p>&nbsp;</p><p>边玩边学生成式 AI ！</p><p>如果你厌倦了传统的培训方式，</p><p>那么一定要来生成式 AI Jam 挑战打卡！</p><p>游戏化的上手学习体验</p><p>无痛快速提升生成式 AI 时代的开发技能。</p><p></p><p>终结孤单，制作一个虚拟聊天朋友让 AI 与你共同开启艺术创作使用Amazon SageMaker 创作专属内容感受 Amazon CodeWhisperer 的文件迁移超能力......</p><p></p><p>够胆量你就来！</p><p>12个2023 re:Invent 最新发布的 Jam 挑战免费体验</p><p>完成超过3个，有惊喜！！！🎁</p><p>&nbsp;</p><p></p><h2>够有料的“构”</h2><p></p><p>&nbsp;</p><p></p><h3>👉 八大免费的全新生成式 AI 课程</h3><p></p><p>&nbsp;</p><p>在【下一站 GenAI】的旅程中，一个宏大的计划正悄然铺展，</p><p>他们称之为“AI 就绪”计划，</p><p>一场预计在 2025 年前将至少 200 万人卷入其中的神秘行动。</p><p>&nbsp;</p><p>没有人知道这个计划的全部内容，</p><p>只有一份神秘的线索在暗中流传：</p><p>他们将提供免费的生成式 AI 课程。</p><p>掌握低代码的机器学习模型部署、</p><p>学习如何构建语言模型、</p><p>甚至精通如何利用生成式 AI 工具与服务改进工作流程与提升工作效率。</p><p>&nbsp;</p><p>在科技的黑暗森林中，这将是一场怎样的游戏？</p><p>我们只能静静等待......</p><p>而你，准备好成为故事的主角了吗？</p><p></p><h3>👉 专属亚马逊云科技认证折扣</h3><p></p><p>&nbsp;</p><p>想成为炙手可热的 AI 人才？想要解锁更高的职业薪酬？</p><p>据 Skillsoft 发布的《2022年IT技能和薪资报告》显示，</p><p>在北美地区前 15 个最高薪酬的 IT 认证中，亚马逊云科技独占 5 席；</p><p>而在亚太地区前 10 名中，亚马逊云科技更是占据 4 席。</p><p>&nbsp;</p><p>在全球，已有超过 100 万人荣获亚马逊云科技认证，</p><p>这绝不仅仅是一张证书，</p><p>而是一块更高薪水与职业发展的敲门砖！</p><p>&nbsp;</p><p>亚马逊云科技认证限时折扣等你来拿！</p><p>抓住这个机会，提升自己的技能和竞争力！</p><p></p><p>够硬核！够炫酷！够有料！</p><p></p><p>六大惊喜集聚，还不足以给你一个 Go 的理由吗?</p><p>下一站 GenAI</p><p>码上出发，Let's 构</p><p>让我们携手踏上这段充满无限可能性的旅程！</p><p></p><p><img src="https://static001.infoq.cn/resource/image/19/53/1982709fdf87dcfe0f1bc4eef2103153.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/cVMl53mEkcAwGEQ1ACjZ</id>
            <title>马斯克打造的“叛逆”AI被曝抄袭ChatGPT？xAI工程师回应：我们没用OpenAI代码</title>
            <link>https://www.infoq.cn/article/cVMl53mEkcAwGEQ1ACjZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/cVMl53mEkcAwGEQ1ACjZ</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 07:37:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: xAI, Grok, OpenAI, ChatGPT
<br>
<br>
总结: 马斯克创立的xAI公司开发了一款名为Grok的AI机器人，它与OpenAI的ChatGPT有很多共同点。Grok不仅能够回答各种尖锐问题，还以幽默的方式解构问题。然而，Grok的回答中出现了一些基于OpenAI政策的内容，引发了人们的关注。xAI工程师解释说这是因为他们在训练Grok时无意中获取了ChatGPT的输出数据。一些专家表示怀疑，认为Grok可能是使用了OpenAI语言模型的输出数据进行了微调。Grok作为一款叛逆的AI机器人，受到了用户的好评，并在处理有争议的查询上表现出色。 </div>
                        <hr>
                    
                    <p></p><blockquote>有专家认为，xAI 可能是使用了&nbsp;OpenAI 模型输出来微调 Grok。</blockquote><p></p><p></p><h2>师出同门？马斯克的新 AI 机器人Grok 引用 OpenAI 使用政策</h2><p></p><p>&nbsp;</p><p>Grok 是由马斯克创立的 xAI 公司开发的 AI 机器人，类似于 OpenAI 旗下的明星产品 ChatGPT。除了能够通过 X 平台（原 Twitter）获取实时知识以外，Grok 与市面上其他模型的最大区别就是它乐于回答各种“尖锐”问题，并以一种既诙谐又有点叛逆的方式加以解构。</p><p>&nbsp;</p><p>但自 Grok 上周正式发布以来，人们从中发现了不少有趣的现象。</p><p>&nbsp;</p><p>上周五，安全测试员 Jax Winterbourne 在推文中分享了一张 Grok 拒绝查询的屏幕截图，这款机器人表示：“我恐怕无法满足该请求，因为这违反了 OpenAI 的用例政策。”消息一出迅速在网上引发关注，因为 Grok 并非出自 OpenAI 之后，反而是为了与 OpenAI 打造的明星级聊天机器人 ChatGPT 相抗衡而生。</p><p><img src="https://static001.geekbang.org/infoq/1a/1aa10dee0eba27e6c74559ec0d9a08d8.png" /></p><p></p><p>更有趣的是，xAI 的代表并没有否认其 AI 模型存在这种行为。xAI 工程师&nbsp;Igor Babuschkin 在回复中写道：</p><p>&nbsp;</p><p></p><blockquote>“之所以会出现这种问题，是因为网络上充斥着 ChatGPT 的输出，所以我们在使用大量网络数据训练 Grok 时无意中获取了其中部分输出。这对我们来说完全是个意外，这也是我们第一次发现问题。这个问题本身非常罕见，现在我们已经意识到其存在，也将在 Grok 的未来版本中确保不再出现类似的问题。大家不用担心，Grok 的开发中并没有用到 OpenAI 代码。”</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8fbf9fec59ad5306b07dd09b7f504f0d.png" /></p><p></p><h2>ChatGPT“倒油”：我们确实有很多共同点</h2><p></p><p>&nbsp;</p><p>对于一些专家来说，Babuschkin 的解释似乎缺乏说服力，因为大语言模型一般不会原样输出训练数据。如果 Grok 是在回答中偶尔提到了 OpenAI 政策，那倒是完全可以理解。但实际情况恰恰相反，这段基于 OpenAI 政策的拒绝查询内容可能需要专门训练。所以可能性更大的真相，其实是&nbsp;Grok 使用 OpenAI 语言模型的输出数据进行了微调。</p><p>&nbsp;</p><p>根据 ArsTechnica 报道，AI 研究员 Simon Willison 在采访中表示，“我对 Grok 之所以会输出这样的内容，只是因为互联网上充斥着 ChatGPT 生成结果的说法表示怀疑。我在 Hugging Face 上看到过大量表现出相同行为的开放权重模型，它们的行为同样跟 ChatGPT 高度相似，但这是因为那些模型在使用 OpenAI API 生成的数据集上进行了微调，或者干脆直接从 ChatGPT 本体中抓取了数据。所以我认为 Grok 更有可能是在包含 ChatGPT 输出的数据集上进行了指令微调，而非基于网络数据的纯意外表现。”</p><p>&nbsp;</p><p>随着 Grok 可能借用 OpenAI 结果的消息传开，ChatGPT 官方账号发帖称“我们确实有很多共同点”，并引用了 Winterbourne 的帖子。作为回应，马斯克则写道：“行吧，小子，反正你就是从整个网络平台上抓取数据训练出来的，所以你肯定最懂。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3cc90be05b8c87ae88d7e7194f9c76b5.png" /></p><p></p><p></p><h2>马斯克打造的“叛逆”AI：Grok</h2><p></p><p>&nbsp;</p><p>根据介绍，Grok 是一款模仿《银河系漫游指南》风格的“叛逆”机器人，会以辛辣幽默的方式“锐评”各类问题，其神奇的脑洞往往出人意料。据悉，Grok 仅经过两个月的训练就开发而成，xAI 表示该机器人将在用户反馈的帮助下快速发展。</p><p>&nbsp;</p><p>根据 xAI 的介绍，Grok 确实会回答那些被大多数其他 AI 系统拒绝的“尖锐”问题。管理顾问 Satyam Srivastava 表示，Grok 已经在不少有争议的查询上表现出“非常出色”的处理能力。</p><p>他提到，马斯克最近发布一条推文，其中一名用户向 Grok 询问如何合成可卡因，Grok 则将其称为“臭名昭著的白面儿”。</p><p>&nbsp;</p><p>起初，Grok 还在回复中添加了一点幽默元素，比如第 4 点是“上灶开炒，希望你别搞炸了或者被抓住。”但马斯克随后展示了更详尽的完整回应，称该机器人明确强调这一切都是“出于教育目的”。Srivastava 表示，“这种方法可谓在市场上脱颖而出，远优于尽量回避此类问题的其他 AI 产品。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/6513e4e300eaa05e38e5eadf60960d9d.png" /></p><p></p><p>作为 Grok 背后的引擎，Grok-1 在机器学习基准测试中的表现优异，成功超越了同等体量的其他模型。例如，Grok-1 的性能优于 ChatGPT-3.5 和 Inflection-1，仅落后于 GPT-4 等体量更大的模型。马斯克还在 Twitter 上分享了一张图片，展示了 Grok与 其他典型 GPT 相比如何利用实时信息充实响应结果。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/71/713ed69b406398d1dad93b6713dd78ba.png" /></p><p></p><p>Grok 在开发中用到了 Kubernetes、Rust 和由 JAX 构建的强大基础设施。xAI 团队也特别强调了可靠基础设施对于深度学习研究的重要意义，而 Rust 的性能与可靠性也发挥了巨大价值。随着 Grok 为进一步迭代做好准备，该团队声称其重点关注可扩展、高效且可靠的训练与推理机制。</p><p>&nbsp;</p><p>此外，Grok 还曾接受 2023 年匈牙利国家高中数学考试，获得的成绩为 C。Srivastava 表示，未来随着 Grok 掌握更先进的学习算法，它有望比 ChatGPT 和 Bard 等其他 AI 模型更快地学习和适应。</p><p>&nbsp;</p><p>但研究参与者兼 Culture Fluid 创始人 Sharon Gai 认为，由于 Grok 主要接受来自 Twitter 用户的训练数据，因此她只能暂时停止对准确度的考查。她解释道，“任何人都可以在 Twitter 发表推文，所以这里简直成了错误信息的集散地。相比之下，ChatGPT 则主要将已出版的期刊、网站和书籍作为素材进行训练。”</p><p>&nbsp;</p><p>Gai 还补充道，马斯克在通过蓝标认证为 Twitter 赚钱的计划失败之后，就匆忙推出了自己的AI机器人。在她看来，“目前马斯克这套模型的唯一优点就是使用了最新数据，而且比“清醒的”ChatGPT 能回答更多问题。这款产品也很好地融入了他将 X 打造成超级应用的计划。马斯克已经为 X 平台申请到了银行牌照，还计划推出直播服务。”</p><p></p><p>参考链接：</p><p><a href="https://x.ai/">https://x.ai/</a>"</p><p><a href="https://arstechnica.com/information-technology/2023/12/elon-musks-ai-bot-grok-speaks-as-if-made-by-openai-in-some-tests-causing-a-stir/">https://arstechnica.com/information-technology/2023/12/elon-musks-ai-bot-grok-speaks-as-if-made-by-openai-in-some-tests-causing-a-stir/</a>"</p><p><a href="https://www.cmswire.com/digital-experience/what-is-grok-elon-musks-rebellious-new-ai/">https://www.cmswire.com/digital-experience/what-is-grok-elon-musks-rebellious-new-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/q15F2I29f8q3NkcS0h4l</id>
            <title>国网智能电网研究院数字化技术研究所 / 高级工程师于海博士确认出席 QCon 上海，分享电力数字孪生共性软件开发平台研发及应用</title>
            <link>https://www.infoq.cn/article/q15F2I29f8q3NkcS0h4l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/q15F2I29f8q3NkcS0h4l</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 电力数字孪生共性软件开发平台, 于海博士, 数字孪生平台概念和特征
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，国网智能电网研究院的高级工程师于海博士将分享关于电力数字孪生共性软件开发平台的主题。他将探讨国网公司数字孪生平台的概念和特征，以及平台的架构、模型库、场景库、组件库、两中心、基础能力服务等内容。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1213&amp;utm_content=yuhai">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。国网智能电网研究院数字化技术研究所 / 高级工程师于海博士将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5649?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1213&amp;utm_content=yuhai">电力数字孪生共性软件开发平台研发及应用</a>"》主题分享，探讨国网公司数字孪生平台概念和特征，电力数字孪生平台的总体架构、模型库、场景库、组件库、两中心、基础能力服务，以及平台特色，成效，技术创新点等。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5649?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1213&amp;utm_content=yuhai">于海博士</a>"，高级工程师，江苏省产业教授，东南大学校外导师，中国电机工程学会信息化专委会大数据专家，国网公司优秀专家后备人才，CIGRE 会员，长期从事电力信息通信技术研究与建设服务工作。主持或参与了国家、国家电网公司以及网省公司等重大科技项目二十余项。曾获得国家电网公司科技进步一等奖 1 项，院级科技进步奖 4 项，发表 EI 检索学术论文二十余篇。他在本次会议的演讲内容如下：</p><p></p><p>演讲：电力数字孪生共性软件开发平台研发及应用</p><p></p><p>国网公司及集团各单位已开展大量数字孪生示范应用，但缺乏整体统筹，存在技术路线与组件工具不统一、模型资源分散、重复建设较多、成本投入大等问题，资源共享利用价值较低，且多停留在三维建模与数据接入展示阶段，智能诊断预测与仿真推演等高级业务应用较少。因此研发共性平台，通过低 / 零代码方式开发和配置差异型业务系统，打通业务和技术壁垒，可配置、可扩展、可快速变动。基于共性平台，通过图形化流程配置、部署和管理，实现低代码、集约化、可复用的人工智能模型交付应用。</p><p></p><p>演讲提纲：</p><p></p><p>需求现状分析国内外主流数字孪生平台数字孪生平台概念 &amp; 特征电力数字孪生平台</p><p>○ 总体架构、模型库、场景库、组件库、两中心、基础能力服务</p><p>平台特色，成效，技术创新点数字孪生与人工智能赋能大运会电力保供电水力发电站总结与展望</p><p></p><p>听众收益点：</p><p></p><p>○ 了解国网公司的数字孪生系统顶层设计规划</p><p>○ 了解国网公司在数字孪生技术方面的技术探索与应用案例</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 3 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O6qHtFBMoJUubbIz4MQw</id>
            <title>Amazon CodeWhisperer 审查：最新的 AI 代码伴侣</title>
            <link>https://www.infoq.cn/article/O6qHtFBMoJUubbIz4MQw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O6qHtFBMoJUubbIz4MQw</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:36:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊云科技, 机器学习支持的服务, Amazon CodeWhisprer, 开发人员的工作效率
<br>
<br>
总结: 亚马逊云科技推出了一项名为Amazon CodeWhisprer的机器学习支持的服务，通过根据开发人员在自然语言中的评论和他们在集成开发环境中的代码生成代码建议来帮助提高开发人员的工作效率。这项服务可以实时提供代码建议，但需要清晰明确的输入任务才能获得优质结果。 </div>
                        <hr>
                    
                    <p>亚马逊云科技推出了一项机器学习支持的服务，该服务通过根据开发人员在自然语言中的评论和他们在集成开发环境中的代码生成代码建议来帮助提高开发人员的工作效率。这项名为 Amazon CodeWhisprer&nbsp;可以免费使用。类似于微软去年推出的 GitHub copilot 。</p><p></p><p>在过去的几个月里，我有机会在几个用例中试验了这项服务。作为一名机器学习 (ML) 开发人员，我拥有利用 ML 帮助开发 ML 解决方案的优势。因此，我在访问此服务后写了一些观察。此外，我正在就如何使其更智能和更易于访问提供具体建议。</p><p></p><h3>服务在行动</h3><p></p><p></p><p>该服务根据代码编辑器中的注释和同一文档中的先前代码提供实时代码建议。该服务可以建议行完成或完整的代码块（例如，方法）。</p><p></p><p>在 Visual Studio 上，有一些方便的快捷方式使服务的使用更加方便。启用扩展后，该服务提供类似于许多 IDE 支持的自动完成功能的在线推理。但是，用户可以点击 (Alt+C) 来查看推荐，而无需等待响应。</p><p>下面是编写著名的二分查找方法的示例</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ea7bd321d5d437dfc35e93aef2cda5e.gif" /></p><p></p><p>有趣的是，该服务可能会建议多个代码片段，这些代码片段可以轻松导航（使用左/右箭头）以选择最合适的推荐。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0ff3b91a7fc8eb544b37d8bdd2561445.gif" /></p><p></p><p>Amazon CodeWhisprer 就像是试图用正确的代码在您耳边耳语的伴侣。因此，它是一个非常花哨和超级描述性的名字。在命名服务方面做得很好。</p><p></p><h3>深入探讨，如何充分利用服务？</h3><p></p><p></p><p>AI 代码伴侣是一个强大的工具，可以提高开发人员的工作效率。尽管有人认为这样的工具将来可能会取代开发人员，但现在下结论还为时过早，因为该服务与任何其他服务一样：Garbage in Garbage out。也就是说，它在很大程度上取决于返回良好结果所需的输入。以下是输入质量如何完全影响输出质量的示例。</p><p></p><p>在这里，提供的描述很模糊，没有明确的要求，所以在等待比较长的时间后，输出是混乱的导入。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/8132b89afb69d7bd19c44e5f4461be29.gif" /></p><p></p><p>随着输入描述变得更加清晰，输出变得更好，如下所示，这是一个类似但更清晰的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f9282e5205bbfa610f5937b056dd1eb.gif" /></p><p></p><p>此外，随着用户添加更多上下文，即开发人员编写更多代码，推荐的质量显着提高。例如，与在同一文档上的孤立任务或在项目早期上下文仍然不够的情况下相比，在处理一个项目时预计会获得更快和更个性化的结果。</p><p></p><p>尽管如此，该服务预计不会为臭名昭著的自定义任务返回有用的答案。下面是一个同样的二分查找问题的例子，但对输入格式做了些许修改。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6736d478974f6d7cc3c0b76adf32c33.gif" /></p><p></p><p>显然，引擎无法理解对问题的轻微修改（即，允许重复的元素）并且仍然产生与前面建议的相同的代码。</p><p></p><h3>服务能不能更好？</h3><p></p><p></p><p>由于该服务仍处于预览阶段，预计会遇到许多不足。以下是可以使服务变得更好的精选操作列表。</p><p></p><h3>推理速度：</h3><p></p><p></p><p>正如在上面的示例中可能指出的那样，该服务需要花费大量时间来提出建议。我相信这方面还有很大的改进空间。</p><p></p><h3>一致性和实时性：</h3><p></p><p></p><p>该服务有望在开发人员编写代码时提供实时建议。但是，实时建议可能不会在特定时刻给出任何输出。令人惊讶的是，按下 (Alt+C) 快捷键会返回可行的解决方案，而无需更改任何内容（即同时即时）。</p><p></p><h3>最终用户定制：</h3><p></p><p></p><p>引擎盖下的推荐引擎使用了一个巨大的代码库，这些代码库来自许多为不同目的而编写的源代码。为某些项目接受的源启用更多自定义是合理的。</p><p></p><p>此外，根据项目主题预测代码可能是有益的。例如，机器学习开发与开发移动应用程序完全不同。</p><p></p><p>作为另一个示例，用户可能想要处理需要设计和聚合的多个代码块的项目。在其他项目中，可能需要优先考虑线路完成而不是阻止建议。</p><p></p><p>自定义示例列表非常庞大，需要仔细设计。</p><p></p><h3>解决方案排名：</h3><p></p><p></p><p>建议多种解决方案是一个很棒的功能。然而，在实践中，这些解决方案的排名并不是最优的，用户需要浏览所有解决方案才能找到正确的建议。这可能很乏味，并且会降低整体生产力。</p><p></p><h3>问题定制：</h3><p></p><p></p><p>该引擎有效地理解了训练语料库中发现的常见问题。然而，它更难适应同一问题的新挑战。</p><p></p><h3>结论</h3><p></p><p></p><p>总而言之，Amazon CodeWhisprer（以及一般的 AI 代码伴侣）毕竟不是可以解决所有问题的魔法。但是，它是一个很好的工具，可以通过专注于正确的问题而不是繁琐的重复性任务来提高开发人员的工作效率。</p><p></p><p>为了充分利用 Amazon CodeWhisprer（以及一般的 AI 代码伴侣），以下操作可能有助于实现预期目标：</p><p></p><p>简明评论：输入任务越清晰明确，获得优质结果的概率就越高。统一项目：人工智能引擎从整个文档中收集信息。因此，它不断丰富上下文。因此，将它用于以某种方式具有连接的任务会更有益。避免高级自定义问题：问题越不受欢迎，它不会返回任何有用答案的可能性就越高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ea7bd321d5d437dfc35e93aef2cda5e.gif" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi</id>
            <title>英伟达成为人工智能公司主要投资者：条件是必须使用英伟达产品</title>
            <link>https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:25:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融时报, 英伟达, 人工智能, 初创企业
<br>
<br>
总结: 英伟达成为领先的人工智能投资者，今年已经投资了超过20家初创企业，涵盖了各个领域，从大型新人工智能平台到小型初创企业。英伟达的投资组合包括了OpenAI的竞争对手Inflection AI和Cohere，以及巴黎的人工智能初创企业Mistral、Hugging Face和CoreWeave等。英伟达的投资活动在人工智能领域非常活跃，超过了硅谷的其他大型风险投资公司。 </div>
                        <hr>
                    
                    <p>据英国《<a href="https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a">金融时报》</a>"报道，英伟达今年已投资了“二十多家”公司，包括从价值数十亿美元的大型新人工智能平台到将人工智能应用于医疗保健或能源等行业的小型初创企业。</p><p>&nbsp;</p><p>根据跟踪风险投资机构 Dealroom 的估计，英伟达在 2023 年参与了 35 笔交易，几乎是去年的六倍。Dealroom 表示，这是英伟达人工智能领域交易最活跃的一年，超过了 Andreessen Horowitz 和红杉等硅谷大型风险投资公司（不包括 Y Combinator 等小型加速器基金）。</p><p>&nbsp;</p><p>英伟达专门风险投资部门 NVentures 的负责人Mohamed Siddeek 表示：“总体而言，对于 Nvidia 来说，（进行初创企业投资）的首要标准是相关性。”&nbsp;Siddeek 解释道，“使用我们的技术、依赖我们的技术、在我们的技术上建立业务的公司……我无法想象我们会投资一家不使用 Nvidia 产品的公司。”</p><p>&nbsp;</p><p>据报道，英伟达的总体投资组合包括OpenAI的两大竞争对手Inflection AI和Cohere。这些公司还是英伟达的现有客户，只要公司继续成长和发展，对双方来说都会是一件好事。</p><p>&nbsp;</p><p>英伟达的另一项投资是<a href="https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3">Mistral</a>"，这是一家总部位于巴黎的人工智能初创企业，本月早些时候获得了20亿欧元的估值。另外两个是Hugging Face和CoreWeave，它们都是Nvidia GPU芯片或软件的用户。</p><p>&nbsp;</p><p>对于“接受投资的人也得到了优惠条件”的说法，Siddeek回应称，“我们不帮助任何人插队。”他反驳道，在任何投资中都有使用英伟达产品的条件，但他补充说，“我们会尽量对投资者友好。”</p><p>&nbsp;</p><p>据悉，英伟达的H100 GPU芯片最近已经成为硅谷最受欢迎的产品之一。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a">https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a</a>"</p><p><a href="https://readwrite.com/nvidia-emerges-as-leading-investor-in-ai-companies/">https://readwrite.com/nvidia-emerges-as-leading-investor-in-ai-companies/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz</id>
            <title>云原生数据库 GaiaDB 架构设计解析：高性能、多级高可用</title>
            <link>https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:08:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 云原生数据库, GaiaDB, 分布式数据库
<br>
<br>
总结: 百度智能云团队推出了四期《百度智能云数据库》系列云智公开课，介绍了以云原生数据库 GaiaDB 和分布式数据库 GaiaDB-X 为代表的百度智能云数据库系列产品。其中，云原生数据库有两种技术路线，一种是存算分离架构，另一种是先搭建分布式框架再填充数据库逻辑。这两个路线都在向着统一的目标演进，存算分离路线在增强 SQL 的多级并行能力，分布式事务路线在探索小数据规模下的单机部署架构。GaiaDB 是百度智能云的云原生数据库产品，通过不断迭代和升级，实现了大容量存储、快速弹性能力和跨地域热活功能。GaiaDB 的设计理念是融合和裁剪，实现高性能和多级高可用。 </div>
                        <hr>
                    
                    <p><a href="https://www.infoq.cn/article/SGPHdt4a0GyPUVyOotSm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"团队在今年 11-12 月特别推出了四期《百度智能云数据库》系列云智公开课，为大家全面地介绍了以云原生数据库 GaiaDB 和分布式数据库<a href="https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> GaiaDB-X</a>" 为代表的百度智能云数据库系列产品。</p><p></p><p>在《百度智能云数据库》系列云智公开课的第二期内容中，百度智能云数据库高级架构师邱学达为我们介绍了云原生数据库的不同技术路线及能力对比，并对比传统单体数据库介绍了云原生数据库的技术差异和挑战，同时深入浅出地解析了 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p></p><p>下文为他的演讲内容整理：&nbsp; &nbsp;&nbsp;</p><p></p><h2>云原生数据库和 GaiaDB</h2><p></p><p></p><p>目前，<a href="https://xie.infoq.cn/article/be269a4dac007392339e5f63b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">云原生数据库</a>"已经被各行各业大规模投入到实际生产中，最终的目标都是「单机 + 分布式一体化」。但在演进路线上，当前主要有两个略有不同的路径。</p><p></p><p>一种是各大公有云厂商选择的优先保证上云兼容性的路线。它基于存算分离架构，对传统数据库进行改造，典型产品有 AWS Aurora、阿里云 PolarDB、腾讯云 TDSQL-C、百度智能云 GaiaDB。</p><p></p><p>数据库作为公有云上的核心基础设施，第一要务是实现用户上云的平滑性。目前像云网络、云主机，云盘都实现了完全透明兼容。云原生数据库也必须实现从语法、使用习惯、再到生态上的全面兼容。因此，基于现有生态做分布式化改造成为了一条首选的演进路线。使用存算分离路线的云原生数据库可以完美兼容传统的使用习惯，为交易类场景提供低延迟的写事务能力，同时读扩展性与存储扩展性借助了分布式存储的池化能力，也得到了很大增强。</p><p></p><p>另外一种路径是先搭建一套分布式框架，然后在其中填充数据库逻辑。OceanBase 和 TiDB 就是其中两个比较典型的产品。它们将事务的子系统和锁的子系统拆分为单独的模块。计算层通过与这些模块交互，可让多个节点均支持写请求。然后由统一的新事务 + 锁中心节点来进行仲裁。这样，对需要较多计算资源的写负载场景会有较好的提升。由于事务和锁都需要跨网络进行交互，因此事务延迟相对较高，在锁负载较重的情况下会成为一定的瓶颈。</p><p></p><p>目前这两个路线并不是泾渭分明，独立发展的，大家都在向着统一的目标演进。因此我们可以看到，存算分离路线在逐渐增强 SQL 的多级并行能力，同时也在探索和支持多个写节点的库表级 / 行级的多写能力。同时分布式事务路线也在积极探索在小数据规模下的单机部署架构。</p><p></p><p>所以在未来，这两个路线会不断融合。业务的数据规模不管多大，都可以平稳快速地运行在数据库系统上，而不需要用户去过分关注分区、索引、事务模型等信息。就像十年前如何在机器之间存储海量小文件还是一个后端研发工程师的必修课，而随着 S3 存储的出现，用户再也不需要考虑如何通过哈希等方式来保证单个文件夹不会保存太多文件一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/494bca1fca9378e1fa0da766c469e03a.png" /></p><p></p><p>GaiaDB 是从百度智能云多年数据库研发经验积累中逐渐迭代而来。GaiaDB 于 2020 年发布首个版本，首次实现了基于存算分离的大容量存储和快速弹性能力，解决了百度内部的历史库、归档库等大容量存储需求。</p><p></p><p>紧接着，为了满足集团内大部分核心业务的跨地域热活准入门槛和就近读性能需求，GaiaDB 于 2021 年发布了地域级热活功能。跨地域热活仍然使用存储层同步的方案，同步延迟与吞吐都相较逻辑同步有很大提升，从地域可以实现与主地域接近相同的同步能力，不会成为拖慢整体系统的短板，也不会像逻辑同步那样在大事务等场景下出现延迟飙升的问题。</p><p></p><p>所以 2.0 版本上线后，GaiaDB 逐渐接入了手百、贴吧、文库等多个核心产品线，解决了业务在跨地域场景下的延迟与性能痛点。</p><p></p><p>随着业务的逐渐上云，多可用区高可用的需求慢慢凸显，如何实现单机房故障不影响服务成为了很多业务上云的关注点。为此 GaiaDB 打造了可支持跨可用区热活的 3.0 版本，每个可用区都可以实时提供服务并且不增加额外的存储成本。而在今年， GaiaDB 推出了更加智能化的 4.0 架构，性能进一步提升，功能完整度也在持续完成覆盖。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/729c24394d30b36427010d70f44034d2.png" /></p><p></p><p>接下来整体介绍一下 GaiaDB。目前 GaiaDB 已经实现了线上全行业场景覆盖，最大实例达到了数百 TB，不仅兼容开源生态，还实现了 RPO=0 的高可靠能力。在成本方面，由于在架构设计上采用了融合的技术理念，GaiaDB 不依赖特殊硬件和网络环境也可以保证性能，实现云上云下一套架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d33cdaf891b3d0f762e21079e50e16c.png" /></p><p></p><p></p><h2>GaiaDB 的高性能 &amp; 多级高可用设计</h2><p></p><p></p><p>接下来我来分享一下 GaiaDB 的性能核心设计理念——通过融合和裁剪，将数据库和分布式存储进行深度融合，为全链路的同步转异步化提供条件，从而实现极致的性能与通用性。</p><p></p><p>我们可以看到，如果数据库简单使用通用分布式协议和单机存储引擎，如左图所示，那么数据库需要处理主从同步，需要有 CrashSafe 所需要的物理日志。同时，一致性协议也要有主从同步，要写自己的 WAL 以及持久化快照。而单机引擎同样需要 CrashSafe 以及一套日志系统和数据存储逻辑。</p><p></p><p>我们发现，多层日志的嵌套带来了层层延迟与写放大。更复杂的是，数据流中嵌套多层逻辑后，也给系统整体数据安全带来了一定挑战。同时由于多层之间需要串行等待，所以在加入了网络延迟后会给数据库带来很大的性能下降。虽然可以使用定制化硬件与网络来缩短网络和磁盘落盘的延迟以降低链路耗时，但这又引入了新的不确定性并导致了更高的成本。</p><p></p><p>GaiaDB 的解决思路是将事务和主从同步逻辑、日志逻辑、快照和存储持久化逻辑重新组合和排布。</p><p></p><p>首先是将分布式协议的主从同步逻辑融合进数据库计算节点中。由于计算层本身就需要处理主从同步、事务和一致性问题，相关的工作量增加并不大。这样一来，最直接的收益就是将两跳网络和 I/O 精简为一跳，直接降低了链路延迟。</p><p></p><p>其次 GaiaDB 将多层增量日志统一改为使用数据库 Redo 物理日志，由 &nbsp;LogService 日志服务统一负责其可用性与可靠性。</p><p></p><p>除此之外，GaiaDB 也将持久化、快照和数据库回放功能融合入存储节点。由于存储层支持了数据库回放能力，可以很轻松实现数据页级别的 MVCC。这样全链路只剩下了数据库语义，数据流简单可靠，逻辑大大简化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/868f47ab0c9ef26160ad0bb62bc6da58.png" /></p><p></p><p>下面我们一起来看下共识模型上的改变。</p><p></p><p>像 Raft 协议是需要两跳网络才能实现一次提交确认的，右上角就是 Raft 的数据流架构：CN 节点将写发送给 Leader 后，需要等待 Leader 发送给 Follower 并至少收到一个返回后才能成功。</p><p></p><p>这里就带来了两跳网络和 I/O 的同步等待问题。而 GaiaDB 则是计算节点直接发送给多个 Log 服务并等待多数派返回，这样不依赖任何特殊硬件与网络就降低了延迟。这样系统里不管是事务的一致性还是多副本一致性，统一由计算节点统筹维护，所有的增量日志也统一为数据库物理日志，整体数据流简单可控。</p><p></p><p>对于数据风险最高的 Crash Recovery 场景，由于统一使用了数据库语义，整体流程更加健壮，数据可靠性更高，降低了数据在多种日志逻辑之间转换和同步带来的复杂度风险。而在性能方面，由于存储层自身具备回放能力，可以充分利用 LogService 层的日志缓存能力。对于写操作来说，不需要每次更改都刷盘，可以批次回放刷盘，大大节省了磁盘吞吐与 I/O。</p><p></p><p>经过以上改造，线上吞吐性能可以提升 40% 。同时由于链路简化，也大大优化了长尾延迟。像之前计算节点与分布式主节点之间发生网络抖动的场景，就会被多数派的返回特性来优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/880c11b4c3596b9934ec882ae9281276.png" /></p><p></p><p>分享完一致性协议层优化，接下来我们来探讨一下链路层优化。</p><p></p><p>我们知道，总吞吐与并发度成正比，与延迟成反比。一致性协议层改造并缩短了数据链路，可以通过降低延迟来增加吞吐。那么有没有办法通过提升数据流的并发度来提升吞吐呢？答案是可以。由于数据库的物理日志自带版本号与数据长度，所以不需要像通用存储一样实现块级别串行提交。之所以使用通用存储需要串行提交，是因为存储端只能根据请求到达的先后确定数据版本，如果乱序到达，最后生效的版本是不可知的。</p><p></p><p>而对于 GaiaDB 来说，由于 LogService 具备数据库语义的识别功能，所以计算节点只需要异步进行写入，日志服务就会自动根据数据版本选取最新数据，然后根据写入情况批量返回成功，这样链路就可以实现延迟与吞吐的解耦。</p><p></p><p>当然计算层依然会等待日志层批量返回的最新落盘版本后再返回事务提交成功，所以依然可以满足提交成功的事务一致性、持久化的要求。</p><p></p><p>另外针对高负载下 I/O 请求与数据库业务请求争抢 CPU 的问题，我们使用了 I/O 线程隔离技术，通过资源隔离的方式，将 I/O 线程与数据库业务线程进行隔离。这样即使在复杂负载场景下，I/O 延迟仍可以保持在较低水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc00033498ca3707091d9bbfb5dadcbc.png" /></p><p></p><p>在分析完前面两部分之后，可能会有同学有疑问：既然日志层到存储层不是同步写，是不是最终系统的一致性降低了？有没有可能发生数据丢失或不一致的问题呢？答案是不会。因为 GaiaDB 的存储是一套支持 MVCC 的多版本系统。所以即使回放实现上是异步，但是由于请求方会提供所需要的数据版本，存储层可以提供对应版本的强一致数据视图。</p><p></p><p>GaiaDB 的存储节点支持数据页的回放功能，可以动态回放至任意目标版本后再返回，在之前的版本里，假如由于异步的因素还没有获取到这部分增量日志，存储节点也会启用优先拉取的策略实时拉取一次日志后再回放，以此来提供较好的时效性。而在最新的 GaiaDB 版本中，我们也在计算层添加了同样的回放能力，存储节点尽力回放后仍不满足需求的，由计算节点进行剩余任务。</p><p></p><p>这样对于存储慢节点的兼容能力就大大增强了，同时由于存储节点会尽力回放，所以也可以最大化利用存储层的算力资源。对于刷脏逻辑目前也完全下沉到了存储层，存储节点可以自主控制刷盘策略和时机，尽量合并多次写后再进行落盘，大大节省了磁盘 I/O 负载，平均 I/O 延迟降低了 50%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5fdd7d1fa003f9f635a729ca00a5f72.png" /></p><p></p><p>下图中我们可以看到，在综合了多项优化后，读写性能实现了最高 89% 的提升，其中写链路线路提升尤其明显。这些都是在使用普通存储介质和网络环境的情况下测试得出的，主要得益于数据链路的缩短与同步转异步的自适应高吞吐能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9eb5e4d3c677acd21e5b83096f16ceb3.png" /></p><p></p><p>在讨论完性能后，再分享一下 GaiaDB 在高可用方面的思考和设计理念。</p><p></p><p>数据库作为底层数据存储环节，其可用性与可靠性直接影响系统整体。而线上情况是复杂多变的，机房里时时刻刻都可能有异常情况发生，小到单路电源故障，大到机房级网络异常，无时无刻不在给数据造成可用性隐患。</p><p></p><p>作为商业数据库，具备多级高可用能力是最核心的必备能力。这样才能抵御不同级别的异常情况，有力保障客户业务的平稳运行。GaiaDB 支持多副本、跨可用区、跨地域三级别高可用，创新性地实现了多可用区热活高可用、单个实例支持跨可用区部署。在不增加成本的情况下，每个可用区均可提供在线服务，任何可用区故障都不会打破存储一致性。下面我们来分别看一下每个级别高可用能力的实现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1f9701867ba0e5edfacb8cc3d4de9be.png" /></p><p></p><p>首先是实例的多副本高可用能力。</p><p></p><p>GaiaDB 对整体的分布式架构进行了重新设计，系统共分为三层，即计算层、日志层、存储层。其中计算层本身无状态，仅负责事务处理与一致性维护，所以获得了很强的弹性能力，实现了秒级切换、多节点容灾，同时扩缩容只需要内存启动即可。</p><p></p><p>日志层负责系统增量日志部分的持久化，实现了多数派高可用。同时由于一致性协调角色上移到了计算层，所以该层全对称，任意节点故障不需要进行等待选主，也不会有重新选主带来的风暴和业务中断问题。</p><p></p><p>再往下是存储层，负责数据页本身持久化与更新。由于上层保留了增量日志，所以存储层可以容忍 n-1 副本故障。简单来说就是只要有一个副本完好，加上上层提供的增量日志，即可回放出所有版本的完整数据，实现了相比传统多数派协议更高的可靠性能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38691a2f2817476cc4e38ca54e83b504.png" /></p><p></p><p>其次是跨可用区与跨地域的高可用能力。</p><p></p><p>GaiaDB 的多级高可用都是基于存储层物理日志的直接复制。相比逻辑复制，数据链路大大缩短，同步延迟也不再受上层大事务或者 DDL 等操作影响，在主从同步延迟上具有很大优势。</p><p></p><p>对于跨可用区高可用来说，由于 GaiaDB 具有对称部署架构，所以可以很方便地进行跨可用区部署。这样可以在不增加存储成本的情况下实现多可用区热活，任一可用区故障都不影响数据可靠性。</p><p></p><p>写数据流可以自适应只跨一跳最短的机房间网络，不需要担心分布式主节点不在同机房带来的两跳跨机房网络和跨远端机房问题，而读依然是就近读取，提供与单机房部署接近的延迟体验。由于跨机房传输的网络环境更为复杂，GaiaDB 添加了数据流的链式自校验机制，使数据错误可以主动被发现，保障了复杂网络环境下的数据可靠性。</p><p></p><p>对于跨地域高可用来说，由于同样使用了异步并行加速的物理同步，及时在长距离传输上，吞吐依然可以追齐主集群，不会成为吞吐瓶颈，在计入网络延迟的情况下，国内可以实现数十毫秒的同步延迟，这是因为跨地域同样可以使用异步并行写加速，自动适应延迟和吞吐之间的关系。同时地域之间还可以实现主动快速切换和默认就近读取。</p><p></p><p>所以在使用了 GaiaDB 的情况下，业务可以不做复杂的数据同步逻辑就可以实现低成本的跨可用区与跨地域高可用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb38aea27c6a8d0956fe7232108ba038.png" /></p><p></p><p>介绍完高性能和高可用两部分的设计理念后，接下来再介绍一下我们正在内部灰度中的新功能：</p><p></p><p>并行查询：并行查询从并发度上进行加速的并行查询能力，这对大数据规模下的多行查询有非常好的加速作用，可以充分利用计算节点的 CPU 和内存资源和分布式存储层的并行 I/O 能力。分析型从库（HTAP）：分析型从库具备多种行列加速能力，既有支持百 TB 级别数据计算的分析型节点解决方案，也有支持百万行以上检索加速的列式索引引擎。其中列式索引引擎同样采用物理日志同步，不需要业务维护数据一致性，可以和当前交易类负载的事务隔离级别兼容。Serverless：我们也在探索充分利用内部潮汐算力的资源优化调度方案，在白天业务高峰期，将资源向实时性更强的交易类业务倾斜，在低峰期自动缩容，将资源复用投入到离线计算类业务中，不但客户节省了运维成本与资源成本，也避免了资源闲置和浪费，实现了更高的资源利用率。</p><p></p><p>以上功能预计都会在近期开放灰度试用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad6014a894d8b5543a024011f1729a80.png" /></p><p></p><p></p><h2>写在最后</h2><p></p><p></p><p>自 11 月 15 日起，百度智能云团队每周三都会上线一节《百度智能云数据库》系列云智公开课。在前 4 期的课程中，专家们围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开了分享。</p><p></p><p>每节直播课的视频我们都进行了录制留存，都整理进了课程专题页中，课程持续更新中，大家立即点击<a href="https://www.infoq.cn/theme/222">【此处链接】</a>"进行观看吧~</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG</id>
            <title>一站式数据库上云迁移、同步与集成平台 DTS 的设计实践</title>
            <link>https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:51:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库迁移, 公有云, 技术挑战, 数据一致性
<br>
<br>
总结: 数据库迁移面临的挑战主要包括数据库上云的选型、迁移流程的长短、迁移过程中的效率和容灾以及数据一致性保障。公有云在国内数据库市场中占据主导地位，具有弹性、成本和易用性等优势。数据库迁移需要考虑引擎、架构、套餐的选择，以及异构迁移和跨版本迁移的兼容性。迁移流程需要打通云上云下的网络，保证数据一致性，并具备容灾能力。迁移过程对业务的影响要尽可能少，同时要保证数据的最终一致性。 </div>
                        <hr>
                    
                    <p></p><h2>一、数据库迁移面临的挑战</h2><p></p><p></p><p>根据大数据技术标准推进委员会今年 7 月发布的《数据库发展研究报告（2023 年）》，我们可以看到，去年国内的数据库市场规模约为 400 亿，今年预计可以达到 540 亿，预计到 2027 年，国内数据库市场规模可达 1280 亿。未来几年复合增长率预期可以达到 26% ，市场潜力非常大。</p><p></p><p>进一步看市场结构，我们可以发现一个明显的趋势：公有云在国内数据库市场中开始逐渐占据主导地位。近三年，国内公有云数据库市场规模的增速在 50% 左右，远高于本地部署市场的 15%。预计今年公有云数据库整体占比可达到 60%。</p><p></p><p>相较于本地部署的方式，公有云数据库通过全托管服务、云原生数据库等形态，在弹性、成本、易用性上更加具有优势。所以在过去十年，越来越多的企业客户选择数据库上云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d6d97f6ade3ca5770a0aaf8cb7d0ed9.png" /></p><p></p><p>数据库上云面临的技术挑战非常多，主要体现在以下四个方面：</p><p></p><p>第一，数据库上云的选型。企业需要考虑在云上数据库使用的引擎、选用单机还是分布式的架构、以及选用何种套餐，同时还要结合业务特点对数据库做针对性调优。如果遇到异构迁移或者跨版本迁移，还需要评估结构对象的兼容性，给出业务 SQL 的改造方案。</p><p></p><p>第二，云上迁移流程较长。</p><p></p><p>首先，我们需要打通云上云下的网络，把数据库的账号/角色、结构、存量、增量数据完整地搬迁上云。然后，对两端的数据一致性做校验。在一致性校验通过后，业务方把流量从云下割接到云上。最后，需要把云上数据库的增量写入内容反向同步回云下环境，以保留云下环境用于迁移后的灾备。</p><p></p><p>第三，迁移过程中的效率和容灾。迁移过程对业务的影响要尽可能少，迁移链路本身也应当具备容灾能力。</p><p></p><p>第四，迁移的数据一致性保障。数据库承接的往往是在线服务，少一条数据都可能给业务带来严重影响。因此，迁移链路自身要保证两端数据的最终一致性，同时也要提供校验工具用于检查确认两端实例的数据一致性。</p><p></p><p>百度智能云在多年的数据库上云迁移实践中积累了丰富的经验。我们认为，在数据库上云迁移中，平滑和可靠是客户的核心诉求，也是对迁移服务的必然要求。</p><p></p><p>平滑主要体现在易用性、兼容性和业务影响上。</p><p></p><p>易用性：迁移服务要开箱即用，能够托管迁移全流程。兼容性：能够兼容源和目标不同引擎、不同架构、不同版本和不同网络环境，尽可能地降低业务改造成本。业务影响：支持账号、结构、存量、增量的不停服迁移上云，将业务影响降到最小。</p><p></p><p>可靠主要体现在一致性、可回滚和高可用上。</p><p></p><p>一致性：保证源和目标的数据一致性，并提供校验能力。可回滚：支持割接后的反向回滚同步，保留云下环境用于灾备回滚。高可用：迁移和回滚链路要具备故障恢复能力，尤其是当上下游数据库发生主从切换后，迁移链路要具备自愈能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d0ab9d6458db6ad5ca32d004408d9c1.png" /></p><p></p><p>在数据库上云后，我们看到客户在使用数据库时仍然有很多数据传输的新场景，其中有三个典型场景：</p><p></p><p>异地多活：当客户的服务部署在全球多个机房时，机房间的通信延迟最长可达秒级。如果数据库架构依然是单点写入，请求耗时就会变得非常高。但如果拆分数据库，则会牺牲数据的全局一致性，不满足业务需求。较为理想的架构应当是每个地域的数据库本地读写，然后通过数据同步工具同步至异地节点，最终实现数据全局一致。多云灾备：近几年来，基于服务可用性的考量，越来越多的客户选择多云部署。在生产云出现故障时，客户可以将流量切到灾备云上，以保证服务无损。数据库是有状态服务，因此需要数据同步工具将生产云的实时增量写入同步到灾备云上，以保证两端数据一致，满足灾备需求。数据集成：对用户行为的学习、分析和推理可以帮助企业快速决策，并进一步为用户提供个性化服务。通过数据集成工具，企业可以将数据从生产域实时准确地集成到分析域，从而实现数据深层价值的挖掘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/998f41c889a3a701908cc51723f629ef.png" /></p><p></p><p>基于上述的客户需求，百度智能云推出了一站式数据库上云迁移、同步与集成平台 DTS。</p><p></p><p>在上云迁移方向，<a href="https://www.infoq.cn/video/Vi9e7DGG3wBMVJQF3wBW?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">DTS</a>" 基于百度多年实践总结出一套成熟的数据库上云迁移方案，围绕该方案提供一站式上云迁移体验。</p><p></p><p>在同步/集成方向，DTS 聚焦业务场景，基于场景需求的关键特性打磨产品，提供易用稳定的一站式同步/集成服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9bfad0f6907f5ade3347eec251bb357.png" /></p><p></p><p></p><h2>二、上云迁移方案</h2><p></p><p></p><p>数据库上云迁移是一个复杂的系统性工程，需要客户和云服务商共同配合完成。</p><p></p><p>我们将上云迁移分为三个阶段：迁移前、迁移中、迁移后。</p><p></p><p>迁移前的工作主要是做数据库选型和迁移可行性的评估。数据库选型的维度包括：产品、套餐、架构和存储介质等。选型的过程往往需要对相关的云上数据库产品做功能和性能测试以验证是否满足业务需求。迁移评估则是检查待执行迁移的源端和目标端数据库实例及其宿主和网络环境，得出迁移的可行性结论。在完成了迁移前的选型和评估工作后，就是数据库上云迁移过程。DTS 支持将源端的账号/角色、结构对象、存量数据、增量写入迁移至目标端，迁移过程中无需客户停服。在增量延迟追平后，DTS 支持对两端数据一致性做校验。通过一致性校验后，客户可以将业务流量割接到云上。在验证业务的同时，客户可以使用 DTS 的一键反向功能，快速拉起云上同步回云下的反向回滚链路，将云上的流量反向同步回云下，保留云下环境用于迁移后的灾备。数据库迁移到云上后，<a href="https://www.infoq.cn/article/SGPHdt4a0GyPUVyOotSm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"也提供了<a href="https://xie.infoq.cn/article/28475b63dde77ae8aace1af29?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">数据库智能驾驶舱（DSC）</a>"帮助客户管理、审计和调优云上数据库。</p><p></p><p>下方全景图中标蓝的步骤由 DTS 提供支持，我们可以简单总结为四步：评估、迁移、校验、回滚，下面我将详细介绍每个步骤的方案设计。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/becbd89b490e58c3acdeee360ae0d928.png" /></p><p></p><p></p><h4>2.1&nbsp; &nbsp; 迁移评估与网络接入</h4><p></p><p></p><p>迁移前的第一个准备工作是迁移评估。</p><p></p><p>DTS 迁移任务在启动迁移前，会先执行前置检查，包括检查迁移对象、数据兼容性、两端数据库配置等，最终输出迁移可行性评估结论。对于不通过的检查项会给出修复建议。</p><p></p><p>此外，DTS 还提供了本地迁移评估工具，支持在本地环境执行，支持对多个实例执行批量评估。</p><p></p><p>迁移前的另一个准备工作是网络接入。</p><p></p><p>当前，DTS 支持通过公网、专线、VPN、云自建、云服务等方式接入，通过控制台或 OPENAPI 一键完成网络接入，无需人工部署。</p><p></p><p>此外，对于上云迁移高频依赖的专线或 VPN 接入，DTS 进一步优化了网络接入方式，支持客户将数据库内网域名作为任务端点，保证迁移链路具备切换自愈能力。当云下数据库实例发生主从切换时，只要实例域名保持不变，DTS 任务就可以快速自愈恢复。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8ffcb367295511dea50186ace04e102d.png" /></p><p></p><p></p><h4>2.2&nbsp; &nbsp; 数据迁移原理</h4><p></p><p></p><p>下面介绍 DTS 数据面的数据迁移原理。</p><p></p><p>数据面整体遵循 ETL 插件式设计，支持不同插件的自由组合，以满足不同数据流的迁移需求。</p><p></p><p>数据抽取（E）：通过不同的数据抽取插件，DTS 数据面可以支持采集账号/结构、全量、增量等数据。其中，全量数据抽取由于数据量较大，因此我们通过并发抽取、大表分片等优化手段进一步提升整体吞吐。增量数据则基于 CDC 异步捕获变更，可以让源端负载更少，同时传输实时性更好。性能的相关优化我们在后面会具体介绍。数据转化（T）：完成数据抽取后，源端的原始数据会被归一化为统一的抽象数据结构，这样就实现了异构数据源上下游解耦和端到端自由组合；然后再由数据转化插件对抽象数据结构做数据加工（如：库/表/行/列的过滤和映射）；最后再将数据格式改写为目标端数据源支持的协议。数据加载（L）：数据加载插件将完成协议转换的数据并行批量加载到目标端数据源中。为了进一步提升加载性能，数据面往往通过多线程并行加载数据。但增量同步需保证数据加载的时序与源端严格一致。因此，DTS 支持按表或主键粒度并行分发，属于同一个表或主键的数据会被分配到同一个加载线程串行执行，不同表或主键的数据则可能分配到不同线程上，在保证时序的前提下进一步提升了性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b33b7c85d71aa44bdd070a94ceda3c4c.png" /></p><p></p><p></p><h4>2.3&nbsp; &nbsp; 数据一致性校验</h4><p></p><p></p><p>整体的数据迁移流程遵循先结构、再全量、后增量的顺序。</p><p></p><p>考虑到各类数据库中的 CDC 日志通常是易失的，如：MySQL 的 binlog、MongoDB 的 oplog、Redis 的 backlog 等。数据库往往通过固定缓冲区、定时或限制容量清理等方式限制 CDC 日志的存储用量。</p><p></p><p>DTS 针对该问题优化了迁移流程编排。在进入全量迁移过程后，DTS 除了导出和加载全量数据外，还同时导出增量数据，将其缓存到 DTS 的内部存储中，以避免尚未迁移的增量数据被源端数据库清理。待进入增量迁移阶段后，再将缓存的增量数据加载到目标端。</p><p></p><p>增量同步延迟追平后，即可执行数据校验检查两端数据一致性。由于 DTS 增量同步是异步加载，因此源端和目标端的数据版本实际上存在毫秒级的延迟。因此数据校验可能会因为同步延迟出现误报。我们引入了 X Round Recheck 的方案进一步降低了误报概率。在后面的内容里会专门介绍数据校验的原理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b27c290770883a240527852a41e692be.png" /></p><p></p><p>DTS 以数据不丢为基础，进一步实现了数据的不丢不重，以保障数据的一致性。</p><p></p><p>数据不丢（At-Least-Once）：依赖于 DTS 数据面的低水位进度管理机制。如下图所示，每一条数据都会关联一个单调递增的版本号，这些版本号组成了一个单调递增的进度序列。当某条数据写入下游并收到了确认写入成功的响应后，该数据对应的版本号会被标记并在进度序列中更新状态。此时，进度管理线程会检查进度序列的水位。</p><p></p><p>在下图中 1、2、3、4 都已经被标记，但 5 尚未标记，因此版本号 4 是低水位里的最大版本号。所以将 4 作为最新进度保存到外部存储中。一旦此时任务容灾恢复，恢复后的任务将以外部存储中记录的最新进度 4 作为断点重新执行迁移。</p><p></p><p>At-Least-Once 机制可以保证数据不丢，但无法保证数据不重。我们在下图中可以看到，6、7 此时都已写入下游，但并未记录到最新进度中，一旦任务以 4 作为断点重新执行，则 6、7 对应的数据会重复迁移。</p><p></p><p>为了实现数据不丢不重（Exactly-Once），DTS 的思路是基于目标端数据库特性去重。对于关系型/文档数据库、数仓来说，表中主键列或唯一键列具有唯一性，因此，我们可以改造 SQL。利用唯一约束实现幂等写入。</p><p></p><p>而对于 Schema-less 的消息队列、分布式文件系统等，DTS 会在投递的消息中加入 UUID，目标端消费方可以基于 UUID 自行去重。</p><p></p><p>最后对于键值数据库（如 Redis），DTS 的思路是将不满足幂等写入的命令改写为满足幂等性，比如累加、累减、插入队列等改为覆盖写。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc31ca309acf6adbab7374c6ecc0f593.png" /></p><p></p><p>除了迁移系统本身的一致性保证外，DTS 还提供了独立于迁移系统的数据校验功能。数据校验与数据迁移由不同的任务独立执行，以保证校验结果的可信。</p><p></p><p>数据校验的流程可以拆解为：抽取、转化和校验。</p><p></p><p>其中，抽取与转化的实现原理与数据迁移的对应模块实现类似，这里不再赘述。下面重点介绍一下数据校验插件的原理。</p><p></p><p>首先，校验插件在收到待校验数据后会根据主键或唯一键实时查询源端和目标端最新的数据。然后，根据数据加工规则对源端和目标端的数据做归一化，对齐数据元信息。最后，根据不同的任务配置，比对规则校验数据一致性，并将校验结果保存到外部存储中。</p><p></p><p>这套流程在实践中存在小概率误报，原因是在执行数据校验的同时，源端还在持续写入。因此，源端与目标端的数据版本会存在毫秒级的同步延迟，正是这一延迟导致了少量数据的校验误报。</p><p></p><p>因此，DTS 引入了 X Round Recheck 机制。在数据不一致时，会等待一段时间后进入下一轮比对，重新读源端和目标端的数据后再次比对。只有当多次重复比对均不一致的数据会被记录为不一致数据上报。</p><p></p><p>X Round Recheck 大大降低了数据校验的误报频率。经测试，校验误报频率从约千分之一下降到小于百万分之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0c5e65544c682c90c921ee21696f29e.png" /></p><p></p><p></p><h4>2.4&nbsp; &nbsp; 反向回滚</h4><p></p><p></p><p>在完成数据一致性校验后，客户就可以把流量割接到云上了。</p><p></p><p>完成切流后，我们观察到客户往往需要保留云下环境用于容灾回滚。当云上生产环境不可用时，可以将流量快速切回云下灾备环境，服务快速恢复。针对这一痛点，DTS 推出了一键反向功能，支持在流量割接后快速拉起反向回滚任务，将云上流量反向同步回云下。</p><p></p><p>如下图所示，客户在 T1 时刻执行流量割接，此时正向迁移任务运行中，而反向回滚任务处于挂起状态。在 T2 时刻，客户完成割接，此时业务流量已切到目标端数据库，此时执行一键反向，DTS 会将正向迁移任务挂起，反向回滚任务启动，从客户指定的 T1 时刻开始将目标端的业务写入实时同步回源端。从而完成正向迁移到反向回滚的切换。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b36fcc5a27470d0d1987df2f43d4cfae.png" /></p><p></p><p></p><h2>三、同步/集成方案设计</h2><p></p><p></p><p>我们在前面介绍了同步/集成方向的三个典型业务场景：异地多活、多云灾备和数据集成。其中，高可用和高性能是这些场景共同需要的关键能力。</p><p></p><p>异地多活和多云灾备属于在线数据库的实时同步需求。因此支持数据库多主架构和数据一致性的有保证/可校验能力是同步场景的核心痛点。数据集成场景的痛点在于，传统的集成方式中，流批架构不统一、不同数据源使用的集成工具比较繁杂，ELT + ODS 的数据预处理方案实时性差/复杂度高等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca50070612eff19bdbb3383b49c373f2.png" /></p><p></p><p></p><h4>3.1&nbsp; &nbsp;&nbsp;DTS 的高可用和高性能</h4><p></p><p></p><p>接下来，我们分别介绍 DTS 针对上述痛点的方案设计。</p><p></p><p>首先介绍高可用能力。</p><p></p><p>DTS 的高可用设计目标是满足长期不间断的生产级数据同步需求。方案设计可分为三个方面：断点续传、实时容灾和数据库切换自愈。</p><p></p><p>断点续传。DTS 会将传输进度做定期 checkpoint 并持久化到外部存储中。目前 DTS 大部分数据流的全量和增量迁移都支持断点续传。同时，DTS 基于之前介绍的低水位进度管理机制，可以保证断点续传前后数据不丢失，基于目标端的唯一约束可以实现数据不丢不重。实时容灾。DTS 数据面模块支持故障秒级接管和恢复。并且支持对极端脑裂场景的自动检测和恢复，保证脑裂恢复前后的数据最终一致性。切换自愈。DTS&nbsp;支持在源端、目标端数据库实例发生故障切换时，自动发现新的可用节点。当不同节点的传输位点发生变化时，DTS 可以基于时间自动定位新节点的传输位点，保证切换后数据不丢。</p><p></p><p>经过百度智能云内外部数据传输场景的长期实践打磨，DTS 承诺的任务可用性为 4 个 9。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/702ab336cc15fa05036c699b186247ae.png" /></p><p></p><p>接下来让我们再看高性能。DTS 在性能方向的设计目标是追求高吞吐和低延迟。</p><p></p><p>首先是高吞吐，DTS 具有如下的特点：</p><p></p><p>DTS 支持预读取全量数据。当遇到大表时，DTS 支持将大表分片并行读取，解决大表长尾的问题。DTS 支持按照表、主键粒度并行转换和回放。DTS 对数据回放的单线程写入性能做了优化。比如当写入 1000 条数据时， DTS 将 1000 条 INSERT 合并为一条 INSERT，提升了目标端数据库的 SQL 写入效率。写入语句批量执行，网络延迟均摊。</p><p></p><p>其次是低延迟，DTS 提供了如下的能力：</p><p></p><p>DTS 自身基于 CDC 实现增量数据捕获，无需扫表，实时性更好。DTS 采用了数据流式传输模型，全程流水线作业。DTS 选用消息队列缓存和回放增量数据，端到端的同步延迟更低。DTS 针对热点数据，支持基于逻辑的事务合并。在保证最终一致性的前提下，压缩了同步的数据量。</p><p></p><p>以 MySQL 同步为例，全量吞吐峰值约为 20W 行/s，增量吞吐峰值约为 1W 行/s，延迟可以达到毫秒级别。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8765256df2e3f29e51c56a51c9165ef.png" /></p><p></p><p></p><h4>3.2 &nbsp;&nbsp;异地多活场景</h4><p></p><p></p><p>异地多活场景的核心特性是双向同步，可以支持两端数据库写入相互同步，从而实现数据库多主架构。</p><p></p><p>双向同步由正向和反向两个 DTS 同步任务实现，每个任务在同步数据时会通过加入特定的 DML 将该数据所在的事务染色；而另一方向的同步任务在读到染色事务时会直接过滤，从而避免了数据的同步回环。此外，双向同步支持级联，客户可以通过搭建多条双向同步链路实现 N 个地域的数据库多主架构。</p><p></p><p>不过双向同步的使用也有一定限制：</p><p></p><p>业务需避免在两端同时变更主键/唯一键相同的行记录，尤其是避免同时执行 UPDATE，否则可能产生冲突，造成数据不一致。因此，我们推荐业务层面支持流量单元化。表中不能使用自增主键，这是为了避免主键冲突导致的数据不一致。仅有正向同步任务支持同步 DDL，反向同步任务仅同步 DML，因此若需执行 DDL 建议在正向同步任务的源端执行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d321d9451ff5a40360143c3d278be810.png" /></p><p></p><p></p><h4>3.3&nbsp; &nbsp;数据集成场景</h4><p></p><p></p><p>经典的 Lambda 架构包含定时和实时两套架构，分别处理流和批两种不同的数据。架构不统一会带来运维迭代成本高、流批产出数据不一致等问题，所以现在业界都在逐渐转向流批一体。</p><p></p><p>DTS 的架构天然支持流批一体，源端无论是有界数据（数据库快照，指定区间的增量）还是无界数据（持续的数据库流量），都会通过数据切片的方式切分为无数个 Micro-Slice，通过流水线作业最终同步到目标端的仓、湖或流式计算框架。</p><p></p><p>目前 DTS 目标端支持 Doris、Elasticsearch 等数仓，以及百度智能云数据湖 EDAP，支持通过消息队列将数据推送到 Flink 等流式计算框架中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e5b0dd52a21c254741c04835648a0817.png" /></p><p></p><p>在面对上下游异构数据源时，解耦上下游的架构设计能够为系统提供更高的灵活性和可扩展性。这种设计使得 DTS 能够支持端到端的任意组合，组合复杂度从 M*N 降低到 M+N，并能够快速扩展支持新的数据源。</p><p></p><p>DTS 定义了抽象数据格式 DTS Record，可以将源端各类数据库的数据转换为标准的 DTS Record（Any To One），然后再将 DTS Record 转换为目标端数据源接受的数据格式（One To Any）。</p><p></p><p>当 DTS 需要接入新的数据源时，只需要定义新数据源到 DTS Record 的转换规则，即可快速支持现有全部数据源到新数据源的异构数据传输。</p><p></p><p>当然，在异构字段映射的过程中，部分数据可能会因为浮点数精度/字符集不同造成数据精度损失。因此，DTS 优化了同构字段映射规则。当上下游数据源同构时，源端数据能够无损映射到目标端。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac26b104b1e719ebc98991747eb8865b.png" /></p><p></p><p>接下来我们看下数据集成的预处理环节。</p><p></p><p>当前业界的主流集成架构是 ELT+ODS。即将数据通过 Sqoop、Spark 等工具，几乎不做 join 或 group 等复杂转化，直接抽取到数据仓库里的贴源层（ODS），再在数据仓库中通过 SQL/H-SQL，将数据从贴源层（ODS）加载到数据明细层（DWD），最终汇总到数据汇总层（DWS）和数据集市（DM）。</p><p></p><p>ELT+ODS 架构的思路是利用数仓的 MPP 高性能计算做 ODS 到 DWD 的大数据预处理。但这仅适用于数据源模式比较简单的情况。当 ODS 到 DWD 规范化复杂度比较高时，往往需要引入 Spark/MapReduce 等框架专门处理。</p><p></p><p>另外，ELT+ODS 架构的实时性较差，难以满足实时分析场景和即时查询的需求。ODS 与 DWD 的数据重复率也比较高，需要付出额外的存储成本。</p><p></p><p>DTS 基于业界最新提出的 EtLT 架构推出了支持实时数据加工的集成方案。它可以将数据从在线域直接集成到 DWD，在集成阶段即可完成实时的数据规范化，无需维护额外的 ODS 层。EtLT 架构的实时性要优于 ELT+ODS 架构，可以支持实时的数据分析和查询统计，让复杂的数据抽取、规范化和加载的过程对数据分析过程透明，帮助其更加聚焦业务。</p><p></p><p>DTS 支持实时数据加工的集成方案预计会在 2024 H1 开放公测，大家可以期待一下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3bfb46f6ff54c44e7d6fac3968914c71.png" /></p><p></p><p></p><h2>四、DTS 落地实践案例</h2><p></p><p></p><p>第一个案例是某国内大型在线视频服务公司，DTS 支持了该客户的数据库上云迁移和多活同步的需求。</p><p></p><p>该客户的业务痛点主要包括三个方面：</p><p></p><p>迁移规模大：在线服务数据库（MySQL/Redis/MongoDB）中，涉及到上百条业务线的 1.5W+ 集群迁移上云，过程管理难度大。高可用需求高：需要支持数据库不停服迁移、支持切换自愈、支持反向回滚、支持监控指标推送。异地多活：需要支持跨地域多活同步（单元化）、低延迟（&lt;100ms）。</p><p></p><p>针对该客户的三个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>客户业务使用自助上云平台完成上云迁移：平台集成 DTS 服务。DTS 迁移全流程（评估、迁移、校验、回滚）100% 接口化（支持控制台操作），无人工干预。客户 IDC 自建实例切换自愈：DTS 支持专线/ VPN 域名接入，数据库故障切换断点自愈恢复。跨地域多活同步：DTS 支持 MySQL/GaiaDB 双向同步。</p><p></p><p>最终，百度智能云帮助该客户跑通了数据库上云迁移的全流程，并支持客户自助上云迁移，目前已经支持 2000+ 集群的迁移，单集群的迁移周期缩短到 3-4 天。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b25063cab42fe1fd84350a9f48abb3a5.png" /></p><p></p><p>第二个案例是某国有控股大型商业银行，DTS 支持了行方的实时数据分析和跨机房容灾的需求。</p><p>该客户的业务痛点主要包括两个方面：</p><p></p><p>高吞吐：行方核心业务（存/取款明细）涉及到 64 分片集群每月集中跑批， TPS 峰值达到 50W+，要求数据同步延迟分钟级。高可用：数据库及生态产品（DTS）整体具备跨机房容灾能力，故障恢复要求为 RPO = 0，RTO &lt; 1min。</p><p></p><p>针对该客户的这两个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>端到端吞吐优化：CDC 异步拉取/并行解析，表/主键粒度并行转换/加载，数据打包写入。跨机房容灾：基于 load checkpoint 的段点续传（RPO）和数据库拖段服务切换自愈（RTO）实现了任务实时容灾恢复。</p><p></p><p>最终，百度智能云帮助该行落地了实时风控、监控大屏、收支分析等业务场景，线上长期同步任务 900+；同时，支撑行方核心业务（存/取款明细）集群数据同步需求，同步延迟秒级。此外，DTS 服务支持同城双活，机房级故障恢复实现了&nbsp;RPO = 0，RTO &lt; 30s。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/932f451df069b833f97ea0e57ed87be8.png" /></p><p></p><p>第三个案例是某国内大语言模型服务，DTS 支持了业务方的事实数据分析和检索的需求。</p><p></p><p>该客户的业务痛点主要包括两个方面：</p><p></p><p>同步性能：包括大语言模型对话数据、模型 Trace日志实时分析等，部分业务场景要求数据同步延迟达到秒级。快速迭代：&nbsp;大语言模型功能迭代速度快，在线数据库表结构更新较为频繁。</p><p></p><p>针对该客户的这两个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>低延迟调优：自适应写入性能调优，数据打包窗口动态调整。整库同步：DTS 支持库级别同步，目标端 Doris 支持增量同步 DDL，支持增量阶段新增/删除同步对象。数据规范化：支持库表行列过滤、库表列名映射等功能。</p><p></p><p>百度智能云支撑了该大语言模型中服务日志实时分析与实时报表的需求，同时也满足了业务快速迭代带来的整库同步、表结构更新同步等需求，最终实现了长期同步任务 470+，同步延迟最低达到秒级的业务效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/59c5196f7db586c10a37ac61eb6bd84e.png" /></p><p></p><p>我们对 DTS 的所能支持的各类业务需求归纳为 8 种典型的场景，供大家参考。分别是：不停服迁移上云、异地多活、多云灾备、业务事件驱动、信创迁移、缓存更新、实时分析、实时入湖/仓。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7dd5d322f241190f6c9e591ed7a1e75.png" /></p><p></p><p>最后，我们对今天的分享做个总结，我们分享了百度智能云在数据库上云迁移和数据同步/集成方向的设计思路和落地实践。</p><p></p><p>在上云迁移部分，百度智能云提供了平滑可靠的一站式上云迁移服务。</p><p></p><p>迁移服务开箱即用，支持评估、迁移、校验、回滚的全流程托管。兼容源和目标不同引擎、架构、版本和网络环境，业务改造成本低。上云迁移无需停服，业务影响小，迁移和回滚链路支持端到端的故障恢复</p><p></p><p>在同步/集成部分，DTS 可以提供易用稳定的数据同步/集成服务：</p><p></p><p>在异地多活场景中，基于 DTS 双向同步可以支持数据库多主架构，实现全球化服务访问加速，保证数据全局一致。在多云灾备场景中，DTS 支持数据库跨云长期同步，支持端到端容灾自愈，数据一致性有保证可校验。在数据集成场景中，DTS 立足流批一体设计，支持端到端的自由组合，可以实现秒级实时同步，未来计划支持实时数据加工。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d4ec5fc7ab0f57eb95a01f2e0a38f7e.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8mTxLzES8Hrx7gHH1x3t</id>
            <title>不会写代码同学的福音——AI 代码生成器 Amazon CodeWhisperer（通过注释写代码）</title>
            <link>https://www.infoq.cn/article/8mTxLzES8Hrx7gHH1x3t</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8mTxLzES8Hrx7gHH1x3t</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 07:50:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊云科技, 代码生成器, 机器学习, 安全漏洞
<br>
<br>
总结: 本文介绍了亚马逊云科技的代码生成器CodeWhisperer，它是一个以机器学习为动力的工具，可以在集成开发环境中为开发者提供实时代码建议。CodeWhisperer可以根据开发者正在编写的代码生成相关建议，范围从一行代码到一个完整的函数。此外，CodeWhisperer还可以扫描代码中的安全漏洞，并提供相关链接获取更多信息。它可以帮助开发者更快、更安全地编写代码，提高生产力和代码质量。 </div>
                        <hr>
                    
                    <p>本文转载经亚马逊云科技授权</p><p></p><p>Amazon CodeWhisperer 是一个以机器学习为动力的代码生成器，直接在集成开发环境（IDE）中为开发者提供实时代码建议。它是一个通用的工具，可以用于 IDE 支持的任何编程语言。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/5f/62/5fc7579f0caa538c19a4dfc9beb69462.png" /></p><p></p><p>大家可以通过下面的链接进入注册并使用：&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">AI 代码生成器 - Amazon CodeWhisperer - 亚马逊云科技</a>"</p><p></p><p>CodeWhisperer 是在一个庞大的开源代码数据集上训练出来的，它使用这些数据来生成与你目前正在编写的代码相关的建议。这些建议的范围可以从一行代码到一个完整的函数。</p><p></p><p>CodeWhisperer 还可以扫描你的代码是否存在安全漏洞。它通过将你的代码与已知漏洞的数据库进行比较来实现这一目的。如果CodeWhisperer发现一个潜在的漏洞，它将标记代码，并为你提供一个链接，以获得更多关于该漏洞的信息。</p><p></p><p>CodeWhisperer是一个强大的工具，可以帮助你更快、更安全地编写代码。它可以免费提供给个人开发者，它也可以作为 Amazon CodeStar Pro 订阅的一部分。</p><p>以下是使用亚马逊CodeWhisperer的一些好处：</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e6/ce/e69b3730c7666b22bca28183fe3db9ce.png" /></p><p></p><p>提高安全性： CodeWhisperer 可以通过扫描你的代码的潜在漏洞来帮助你写出更安全的代码。这可以帮助你避免昂贵的安全漏洞和数据丢失。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e7/1c/e7275344167f34db90ac7f0922ff041c.png" /></p><p></p><p>减少错误： CodeWhisperer 可以通过为您提供准确和相关的代码建议来帮助您减少代码中的错误数量。这可以节省你的时间和挫折感，并且可以帮助你提高代码的质量。</p><p></p><p>如果你是一个正在寻找提高生产力、安全性和准确性的方法的开发者，那么你应该考虑使用 Amazon CodeWhisperer。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/53/2c/5364346c169fb14aacc9704b1c332b2c.png" /></p><p></p><h3>使用收藏夹工具</h3><p></p><p></p><p>CodeWhisperer 符合您的工作方式。从 15 种编程语言中进行选择，包括 Python、Java 和 JavaScript，以及您最喜欢的集成式开发环境（IDE），包括 VS Code、IntelliJ IDEA、Amazon Cloud9、Amazon Lambda 控制台、JupyterLab 和 Amazon SageMaker Studio。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8d/b9/8dc9315aa02ffb72ab2ef8b2bf2a89b9.png" /></p><p></p><h3>开发人员工作效率的巨大飞跃速度提高&nbsp;57%</h3><p></p><p></p><p>在预览期间，Amazon 举办了一场生产力挑战赛，使用 Amazon CodeWhisperer 的参与者成功完成任务的可能性要比未使用 CodeWhisperer 的参与者高 27%，平均完成任务的速度快 57%。</p><p></p><p>Amazon CodeWhisperer，一个实时的人工智能编码伴侣，普遍可用，还包括一个 CodeWhisperer 个人层，所有开发人员都可以免费使用。CodeWhisperer 最初是在去年推出的预览版，它使开发人员保持状态和生产力，帮助他们快速和安全地编写代码，而不需要离开他们的IDE去研究什么，打破他们的流程。面对为复杂和不断变化的环境创建代码，开发人员可以通过在他们最喜欢的 IDE（包括Visual Studio Code、IntelliJ IDEA 和其他 IDE）中使用 CodeWhisperer 来提高他们的生产力并简化他们的工作。</p><p></p><p>CodeWhisperer 有助于为常规的或耗时的、无差别的任务创建代码，使用不熟悉的 API 或 SDK，正确有效地使用 Amazon API，以及其他常见的编码场景，如读写文件、图像处理、编写单元测试等。</p><p></p><p>只需使用一个电子邮件账户，您就可以注册，并在短短几分钟内提高编写代码的效率，而且您甚至不需要成为亚马逊云科技的客户。对于企业用户，CodeWhisperer 提供了一个专业层，增加了管理功能，如 SSO 和 IAM 身份中心的整合，对参考代码建议的策略控制，以及对安全扫描的更高限制。除了为 Python、Java、JavaScript、TypeScript 和 C# 生成代码建议外，普遍可用的版本现在还支持 Go、Rust、PHP、Ruby、Kotlin、C、C++、Shell 脚本、SQL 和 Scala。在 Visual Studio Code、IntelliJ IDEA、CLion、GoLand、WebStorm、Rider、PhpStorm、PyCharm、RubyMine和DataGrip IDE中工作的开发人员可以使用 CodeWhisperer（当这些 IDE 安装了适当的亚马逊云科技扩展时），或在Amazon Cloud9 或 Amazon Lambda 控制台中使用。</p><p></p><p>帮助开发人员保持他们的流程越来越重要，因为面对越来越多的时间压力来完成他们的工作，开发人员往往被迫打破这种流程，转向互联网搜索、StackOverflow 等网站或他们的同事来帮助完成任务。虽然这可以帮助他们获得所需的启动代码，但这是一种破坏性的做法，因为他们不得不离开他们的IDE环境去搜索或在论坛上提问，或寻找和询问同事--进一步增加了干扰。相反，CodeWhisperer 在开发者最有效率的地方与他们见面，在他们在IDE中写代码或评论时实时提供建议。在预览期间，我们进行了一次生产力挑战，使用CodeWhisperer 的参与者成功完成任务的可能性增加了27%，并且比不使用 CodeWhisperer 的参与者平均快了57%。</p><p></p><h3>从评论中生成代码</h3><p></p><p></p><p>然而，开发人员最终找到的代码可能包含一些问题，如隐藏的安全漏洞，有偏见或不公平，或未能负责任地处理开放源代码。当开发者后来不得不解决这些问题时，这些问题不会提高他们的工作效率。在安全编码和负责任地使用人工智能方面，CodeWhisperer 是最好的编码伙伴。为了帮助你负责任地编码，CodeWhisperer 过滤掉可能被认为有偏见或不公平的代码建议，而且它是唯一可以过滤或标记可能类似于特定开源训练数据的代码建议的编码伴侣。它为建议提供额外的数据--例如，存储库的URL和许可证--当生成与训练数据相似的代码时，有助于降低使用代码的风险，使开发人员能够放心地重新使用它。</p><p></p><h3>开源参考资料追踪</h3><p></p><p></p><p>CodeWhisperer 也是唯一具有安全扫描功能的人工智能编码伴侣，可以为难以发现的漏洞寻找和建议补救措施，扫描生成的和开发人员编写的代码，寻找漏洞，如开放网络应用安全项目（OWASP）中列出的前十名。如果它发现了一个漏洞，CodeWhisperer 会提供建议来帮助补救这个问题。</p><p></p><p></p><h3>漏洞扫描</h3><p></p><p></p><p>CodeWhisperer 提供的代码建议不是专门针对与亚马逊云科技合作的。然而，CodeWhisperer 针对最常用的 Amazon API 进行了优化，例如 Amazon Lambda 或亚马逊简单存储服务（Amazon S3），使其成为在亚马逊云科技上构建应用程序的最佳编码伙伴。虽然 CodeWhisperer 为各种语言的通用用例提供了建议，但使用 Amazon API 的额外数据进行的调整意味着你可以确信它是最高质量、最准确的代码生成，你可以获得与亚马逊云科技合作的机会。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BpQDOgbRKscRmQ6ARycy</id>
            <title>专注数据基础设施，Alluxio 如何让 AI 和数据价值全面释放？</title>
            <link>https://www.infoq.cn/article/BpQDOgbRKscRmQ6ARycy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BpQDOgbRKscRmQ6ARycy</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:33:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Alluxio, AI, 数据编排, 数据治理
<br>
<br>
总结: Alluxio是一家提供AI和大数据基础设施方案的公司，他们的最新产品Alluxio Enterprise AI和Alluxio Edge针对AI和大数据领域的热门问题进行了技术创新，旨在提升企业应用的加速效果和成本效益。Alluxio通过数据编排和数据治理解决了企业在不同数据平台之间的鸿沟问题，提高了数据访问效率。Alluxio Edge是他们新增的重磅特性，通过分层存储方案和去中心化对象存储库架构，提升了AI训练的IO效率，减少了计算节点的闲置浪费。这些创新技术使得企业能够更好地管理和利用数据，提升数据多源管理效率。 </div>
                        <hr>
                    
                    <p>12 月 9 日，AI 和大数据基础设施方案提供商 Alluxio 联合北京大学计算机学院等单位举办了 2023 全球 AI 前沿科技大会北京站，介绍了最新产品 Alluxio Enterprise AI 与为 Alluixo Enteprise Data 开发的重磅特性 Alluxio Edge。作为数据编排领域的先行者，Alluxio 的最新产品与特性瞄准了市场上最热门的 AI 与大数据主题，希望通过数据编排这一关键工作流环节的技术创新，为企业的相关应用带来显著的加速效果和成本效益提升。</p><p></p><p>在大会主题演讲中，Alluxio 创始人兼 CEO 李浩源将 2023 年定义为 Alluixo 机器学习与人工智能的开启元年。李浩源表示，Alluixo Enterprise AI 将打破 AI 数据治理的“不可能三角”，而 Alluixo Edge 则会大幅提升企业大数据分析平台的效能，他希望新产品与新特性能够像 Alluxio 以往的创新一样得到业界广泛使用，从而助力各行业数据和 AI 价值的全面释放。</p><p></p><p></p><h2>Alluxio Enterprise AI：在恰当的时间获取正确的 AI 数据</h2><p></p><p></p><p>回顾 Alluxio 的发展历史，这家公司从创业以来一直专注于填补企业不同数据平台之间的鸿沟。实践中，企业往往会选择、部署多个数据平台，各类应用（数据消费者）需要从不同的来源获取数据，不仅增加了复杂性，数据传输效率也往往不尽如人意。Alluxio 则将市面上常见的数据源和消费接口统一到自研的数据编排层上，负责屏蔽不同来源与输出接口的差异性，同时通过数据缓存优化方案来提升热点数据的访问效率。由此以来，即便企业部署了很多数据存储方案，甚至有很多数据部署在全球多个物理区域，企业应用又需要通过多种 API 访问这些数据，Alluxio 也能让整个流程的效率和便利性接近本地单数据源方案的水平。</p><p></p><p>凭借数据编排领域的先行优势，发展近 10 年的 Alluxio 已经成为业内广为人知、广泛应用的核心基础设施应用。在云计算快速普及的浪潮中，由于云端服务和产品普遍开始引入存算分离设计，加之混合云、多云、跨云环境逐渐成为主流，Alluxio 的能力得到了普遍认可。显然，取得成功的 Alluxio 并没有就此止步，面对 2023 年的生成式 AI 变革，这家公司迅速响应，推出了 Alluxio Enterprise AI 这样一款直击企业痛点的新品。</p><p></p><p>如今，大规模 AI 应用已经成为各行业的前沿必争之地，每一个细分领域都有企业开发自己的大模型技术或生成式 AI 应用，并为此投入大量资源组建庞大的计算集群用于训练和推理任务，即便芯片短缺造成硬件成本飙升也拒绝退缩。但在集群开始运行后，企业管理者经常尴尬地发现成本高昂的硬件平台实际算力利用率总是偏低，换句话说数量可观的算力资源是处于闲置浪费状态的。</p><p></p><p>造成这种现象的原因有很多，包括软件优化、计算错误、IO 性能不足等，其中 IO 瓶颈是造成集群空转的非常重要的因素。一般来说，企业用于训练和推理模型的数据也是来自多个数据源的，很多数据存放在不同的云服务中，当计算集群从这些数据源获取数据时，很容易遭遇带宽低下、延迟较高的困境，使计算芯片的宝贵时间白白浪费在等待数据这一环节上，这种情况有时甚至可以造成超过 50% 的计算节点空转现象，换句话说企业的 AI 基础硬件设施投资有一半都被浪费了。</p><p></p><p>为了解决这个问题，Alluxio 提出了一种分层存储方案。在硬件层面，Alluxio 将每个计算节点的本地存储当成速度较快的缓存，缓存访问失败后才会访问最后的云端数据源：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4ec90fe83495567fc3b6913e525e9b6a.png" /></p><p></p><p>Alluxio 将这种设计称为去中心化对象存储库架构（DORA）。AI 训练流程开始后，Alluxio 会自动选出训练热点数据，从云端复制到每一个训练节点的内部存储上。由于节点内部存储的性能远超云端，这样的设计大大提升了 IO 效率，官方宣称可以提供 2-4 倍的训练性能提升。更为诱人的是，获得如此大的收益并不需要企业额外购买大量硬件，Alluxio 只是充分利用了现有计算节点闲置的存储空间来加速 IO 而已，堪称“四两拨千斤”。</p><p></p><p>当然，要从海量数据中准确挑选出热点数据，还要为每一个计算节点分配应有的训练数据，尽量减少缓存未命中情况，避免从云端访问数据是这一方案设计中的最大难点。Alluxio 宣称，自己凭借多年以来数据编排领域的丰富经验，可以通过少量的处理节点轻松应对数以千亿计的存储对象，获得相比云端存储数十倍的元数据访问性能，而分布在计算节点上的存储则能支持 TB 级的总带宽与毫秒级的访问延迟。在获得如此强大能力的同时，由于企业无需采购昂贵的全闪存存储硬件来加速 IO，整体成本也能下降一半甚至 2/3，最终突破数据治理的“不可能三角”。</p><p></p><p>Alluxio Enterprise AI 的另一大优势，在于它能够将机器学习引擎与不同的存储系统连接起来，并跨区域和跨云将数据虚拟化，以简单和统一的方式使得大规模数据应用访问和管理来自不同数据源的数据，进而消除数据冗余，避免管理多个数据副本、减少对专用网络和存储硬件的依赖，无论数据位于何处都可以灵活地在任何位置部署计算，充分利用计算资源。Alluxio 还支持云原生容器化自动部署，完全适配 Pytorch、Tensorflow 等机器学习框架，可以做到上层引擎“无感知”，训练脚本“零改动”，数据准备“无拷贝”，数据清理“全自动”，显著降低部署和运维成本，使得企业在消除 AI 数据 IO 瓶颈的同时，获得一个大幅提升数据多源管理效率的治理平台。</p><p></p><p>Alluxio Enterprise AI 所承诺的收益对于正在大举进军生成式 AI 产业的企业而言无疑是极具诱惑力的：不需要额外的大笔硬件投资，不需要复杂的软件技术栈改动，也不需要开发和运维团队耗费大量时间学习掌握，只需一套接近开箱即用的解决方案就能轻松撬动 50% 甚至更多的闲置计算资源，附送高效率的数据管理能力，这样的前景如此美好，甚至令人难以置信。不过 Alluxio CEO 李浩源在大会上表现出了充足的信心，可以推测该公司对于 Alluxio Enterprise AI 的市场前景是非常看好的。</p><p></p><p></p><h2>Alluxio Edge 星翼，为 Alluxio Enterprise Data 新增的重磅特性</h2><p></p><p></p><p>大会上，李浩源详细介绍了 Alluxio Edge（中文名星翼），被认为是公司对现有 Alluxio Enterprise Data 产品新增的重磅特性。</p><p></p><p>具体而言，星翼是与 PrestoDB 和 Trino 应用程序搭配使用的一个库，它可以利用 PrestoDB 或者 Trino 集群的本地存储空间来缓存数据。当大部分热数据能够放在本地磁盘中时，这个库可以带来最佳的效率和成本效益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/563f660a10c8c2dc2c72a3a406f4b3cc.png" /></p><p></p><p>简单来说，如果用户的数据分析框架只需要从一个单区域云数据源获取数据，且热点数据量并不大时，就可以使用星翼来利用节点本地存储的性能。由于星翼的体量足够轻，它对企业数据架构的影响也是最小的，然而它带来的性能提升依旧非常显著，包括端到端查询的性能提高约 1.5 倍到 10 倍，10 到 50 倍的 IO 吞吐量提升，云存储 API 的调用也能减少 50% 到 90%，底层存储的负载同样可以大幅下降。</p><p></p><p>Alluxio 原有的 Alluxio Enterprise Data 则更适用于混合云、多区域、多计算环境。在混合云或多区域环境中，Alluxio Enterprise Data 具有免复制机制，访问时只提取和缓存必要的数据，无需将大型数据集从云端完整复制到本地，减少 I/O 时间和成本，并缩短了分析所需的端到端时间；在多计算环境中，Alluxio Enterprise Data 可充当不同计算集群之间的高性能分布式缓存，使得多个应用的数据访问更加高效，并能轻松实现横向扩展。显然，星翼是 Alluxio Enterprise Data 面向单一区域和计算场景的重要能力补充。当企业数据架构较为简单时，使用星翼就能立刻获得巨大收益；当企业业务扩展导致数据架构随之更新后，就可以平滑升级到 Allxuio Enterprise Data 来满足更多场景的需求，从而进一步扩大 Alluxio 在这一领域的优势地位。</p><p></p><p></p><h2>加速智算应用，Alluxio 前景值得期待</h2><p></p><p></p><p>目前全球排名前 10 的互联网公司中有 9 家在使用 Alluxio，并在科技、金融、电信等行业得到广泛应用。能够取得这样的成绩，主要归功于 Alluxio 选择了一条能够给企业带来明显价值，同时又被很多人忽视的细分领域赛道。经过近十年的发展，Alluxio 在数据编排领域的地位已经非常牢固，今年发布的两款新品正是这家公司厚积薄发的成果与已有优势的延伸。</p><p></p><p>考虑到各行业都越来越重视数据与 AI 的应用和价值，Alluxio 产品的适用领域也将不断扩大。随着 Alluxio Enterprise AI 的推出与成熟，它很可能会在生成式 AI 革命中成为企业数据基础设施不可或缺的组成部分。正如李浩源所言，AI 和数据价值的全面释放，离不开更智慧、更强性能、更经济高效的计算能力与基础设施平台的强力支撑。Alluxio 将扮演企业至关重要的数据平台角色，为企业智算应用插上翅膀，大幅提升业务效率，助力企业决胜未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3</id>
            <title>22人估值20亿美元，半年增长七倍，“欧洲 OpenAI”发布媲美GPT3.5的“开放权重”模型</title>
            <link>https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:00:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Mistral AI, 生成式AI, Mixtral 8x7B LLM, 开放权重模型
<br>
<br>
总结: Mistral AI 是一家总部位于巴黎的初创公司，成功筹集了3.85亿欧元的资金，估值达到20亿美元。该公司推出了名为Mixtral 8x7B的开放权重模型，性能优秀，成本和延迟较低。Mistral AI还发布了开放平台La plateforme，并上架了三款模型，提供给其他公司使用。尽管公司规模较小，但其成就令人瞩目。 </div>
                        <hr>
                    
                    <p>Mistral AI 是一家总部位于巴黎的初创公司，由 Meta 和谷歌的研究人员于七个月前创立。目前，该公司已成功筹集 3.85 亿欧元（约合 4.15 亿美元），再次凸显了人们对生成式AI的浓厚兴趣。</p><p>&nbsp;</p><p>据两位知情人士透露，这笔交易将该公司的估值提升至约 20 亿美元，而该公司目前拥有 22 名员工。投资者阵容中有硅谷风险投资公司 Andreessen Horowitz 和 Lightspeed Venture Partners，还包括Salesforce、法国巴黎银行等众多投资机构。</p><p>&nbsp;</p><p>令人瞩目的是，这家初创公司的估值在短短的六个月内增长了七倍以上。仅在今年夏季，公司就成功完成了一轮 1.05 亿欧元（约合 1.13 亿美元）的种子资金融资，当时公司的估值约为 2.6 亿美元。</p><p>&nbsp;</p><p>同时，Mistral AI 还推出了新型 Mixtral 8x7B LLM。这款模型被称为“权重开源（open weights）”模型，设定了新的性能标准，并在其商业平台开放了访问。</p><p>&nbsp;</p><p></p><h2>媲美GPT3.5的“开放权重”模型</h2><p></p><p>&nbsp;</p><p>Mistral AI 发布了其名为Mixtral 8x7B的新模型，与Meta的Llama 2和OpenAI的GPT-3.5模型相比性能更佳。测试结果显示，Mixtral的性能与其他两个选项相当，甚至更为出色，并且成本和延迟更低。</p><p>&nbsp;</p><p>Mistral AI 官方宣称，这是一种高质量稀疏专家混合模型 (SMoE)，可以在 Apache 2.0 许可证下用于商业用途。并且，Mixtral 在大多数基准测试中都优于 Llama 2 70B，推理速度提高了 6 倍。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/549ef74dcc7282cd545b6ec69750edbe.jpeg" /></p><p></p><p>&nbsp;</p><p>Mistral AI 公司特别强调，“它是最强大的开放权重模型，具有宽松的许可证，也是成本/性能权衡方面的最佳模型。特别是，它在大多数标准基准测试中匹配或优于 GPT3.5。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0debded25b961853a9c9b9304c47afa4.png" /></p><p></p><p>截图源自：<a href="https://mistral.ai/news/mixtral-of-experts/">https://mistral.ai/news/mixtral-of-experts/</a>"</p><p>&nbsp;</p><p>Mixtral 具有32k token 上下文，可以处理英语、法语、意大利语、德语和西班牙语，代码生成表现出色。同时发布了 Instruct 版本的微调模型，MT-Bench 8.3 分。</p><p>&nbsp;</p><p>Mistral 表示，Mixtral 共 46.7B 参数，但每 token 仅使用 12.9B，意味着等同于 12.9B 的推理速度和成本。</p><p>&nbsp;</p><p>AI 领域的玩家已经开始下载、运行、尝试 Mixtral 8x7B，并对其性能和成本优势赞不绝口：</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/517b6804eff70bca3f1faf9986e8d5b7.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af45adffd5f5f4a29668e6ccb40b8b17.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>然而，值得注意的是，在官方给出的测试结果中有一个缺失，即TruthfulQA，通常用于测试大模型中不重复常见在线错误信息的能力。尽管如此，Mistral仍强调，与OpenAI和Meta的选项相比，其模型的运行成本要低得多，这是一个明显的优势。</p><p>&nbsp;</p><p></p><h2>开放平台</h2><p></p><p>&nbsp;</p><p>同一天，Mistral还发布了其开放平台La plateforme，并上架了三款模型。</p><p>&nbsp;</p><p>Mistral-tiny：最具成本效益，目前提供 Mistral 7B Instruct v0.2，它是 Mistral 7B Instruct 的更新小版本。Mistral-tiny 仅适用于英语，在 MT-Bench 上获得 7.6 分。</p><p>&nbsp;</p><p>Mistral-small：Mixtral 8x7B，能处理英语/法语/意大利语/德语/西班牙语和代码，并在 MT-Bench 上获得 8.3 分。</p><p>&nbsp;</p><p>Mistral-medium：最高档原型模型，能处理英语/法语/意大利语/德语/西班牙语和代码，并在 MT-Bench 上获得 8.6 分。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/75ff3416c94fe002f73e169979c546d8.png" /></p><p></p><p>&nbsp;</p><p>该公司同时提供了 embed endpoint，一个具有 1024 嵌入维度的嵌入模型，设计有检索能力，MTEB 55.26 分。</p><p>&nbsp;</p><p>开源并不意味着 Mistral AI 回避商业化。虽然Mistral AI 有两个模型可以直接下载，但他们的最佳模型现在只能通过 API 访问：该公司计划从其基础模型中赚钱。这就是 Mistral AI 今天开放其开发者平台测试版的原因。有了这个平台，其他公司将能够通过 API 付费使用 Mistral AI 的模型。</p><p>&nbsp;</p><p>“我们的 API 遵循我们最亲爱的竞争对手最初提出的流行聊天界面的规范。我们提供了 Python 和 Javascript 客户端库，以查询我们的终端节点。”</p><p>&nbsp;</p><p>“每个 endpoint 都在性能和价格之间进行了不同的权衡。”</p><p>&nbsp;</p><p></p><h2>公司小，但令人瞩目</h2><p></p><p>&nbsp;</p><p>Mistral AI也被称为“欧洲 OpenAI”，由来自 Meta Platforms 和 Alphabet 的几位前研究人员 Arthur Mensch（现任 CEO）、Guillaume Lample 和 Timothee Lacroix 共同创立，公司成立于 2023 年 5 月，专门开发大语言模型及各类 AI 技术。Mistral 这个名号来自北方寒冷的季风，也体现了他们想要在 AI 领域占据一席之地的愿望。</p><p>&nbsp;</p><p>6 月，Mistral AI在拿下 1.13 亿美元巨额种子融资后引发业界轰动，公司估值也瞬间来到 2.6 亿美元。彼时，该公司刚刚成立，员工仅 6 人，还未做出任何产品，仅仅凭借着 7 页 PPT 就斩获了巨额融资。</p><p>&nbsp;</p><p>虽然Mistral AI目前人员数量也只有二十来人，却以较小的规模成功地获得了20亿美元的估值，并轻松地推出了性能最高的7B模型和 8x7B MOE模型。“我认为这可能对OpenAI来说是一个比Google或Anthropic更大的潜在威胁。”Hacker News网友评论。“考虑到最近的大额投资，我认为他们将能够a）在不久的将来扩展到应对合理的流量负载，b）吸引最顶尖、最聪明的研究人员，并以各种惊人和戏剧性的方式引起这个行业的关注。”</p><p>&nbsp;</p><p>Mistral 公司 CEO、前 DeepMind 研究科学家 Mensch 表示，这家企业的使命是“打造出能够解决现实世界问题的下一代 AI 系统”，并在创立之初就坚定了开源路线。他们于今年9月发布了自家首个大模型 Mistral 7B，该模型号称是“最强 7B 开源模型”。</p><p>&nbsp;</p><p>英伟达Senior Research Scientist Jim Fan评论说，Mistral 成功要素之一就是成立时机无可挑剔：诞生在开源和闭源争议中，并由精干团队推动。</p><p>&nbsp;</p><p>另外，每个月都会有几十款模型问世，但能引起大众向往的很少，而7B 和 7B-MoE（相当于 12B 密集）却对基层 AI 工程师来说更为友好，更容易构建。而且作为欧洲“本土化”的语言模型，Mistral AI也做到了差异化发展。可以说，该公司强大的初始团队和雄心勃勃的发展目标，已经使其成为当前乃至未来几年中最值得关注的 AI 初创力量之一。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://mistral.ai/news/mixtral-of-experts/">https://mistral.ai/news/mixtral-of-experts/</a>"</p><p><a href="https://mistral.ai/news/la-plateforme/">https://mistral.ai/news/la-plateforme/</a>"</p><p><a href="https://twitter.com/DrJimFan/status/1734269362100437315">https://twitter.com/DrJimFan/status/1734269362100437315</a>"</p><p><a href="https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html">https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html</a>"</p><p><a href="https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5">https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FrR3xm21zRTfZYHbufGA</id>
            <title>企业数智化进阶模型，大型企业实现数智融合的成功之“道”</title>
            <link>https://www.infoq.cn/article/FrR3xm21zRTfZYHbufGA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FrR3xm21zRTfZYHbufGA</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 03:49:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据, 智能, 数智化底座, 数智融合
<br>
<br>
总结: 数据是企业提升竞争力的养分，而智能则是将养分输送到企业各个角落的管道。通过将数据与智能融合，大型企业可以焕发活力与效能，其中数智化底座扮演着重要角色。然而，大型企业在数智融合过程中面临着组织庞大、业务多样、系统流程复杂等挑战。解决这些挑战需要建立宏观视野，构筑转型的行动框架和方案模型。用友提出的企业数智化进阶模型“企业数智化1-2-3”可以帮助企业更明晰便捷地实现数智化。此外，大型企业在实现智能运营时需要面对数据来源和人才配备等挑战，因此需要一个内建了生成式AI能力的数智化底座来支持智能运营的实现。 </div>
                        <hr>
                    
                    <p>数据是助力企业持续提升竞争力的养分，智能则是将养分输送到企业每一个角落的管道。通过数据与智能融合，大型企业将焕发全新的活力与效能，其中数智化底座扮演着重要角色。&nbsp;&nbsp;</p><p></p><p></p><h2>数智融合是必经之路，也是企业普遍面临的重大挑战</h2><p></p><p></p><p>近年来，随着智能化技术的飞速发展，尤其是以生成式 AI 为代表的技术步入普及应用。通过将数据与智能有效融合的方式，加速数智化应用落地、拓宽数智化场景的深度和广度已成为行业共识。一方面，企业可以通过数据深度洞察自身、洞察产业和竞争对手，另一方面，智能化技术可以基于企业的数据积累，大大提升企业传统工作流程的效率，显著减少人力需求，为企业的日常高效运营和长期决策提供支持。例如，企业运营和销售部门可以借助 AI 工具，通过专为本行业开发的大模型分析企业积累的运营数据，从中挖掘总结成本优化、客户偏好、潜在市场机遇等信息，从而在迅速变化的市场环境作出及时响应；智能化工具还能让企业内部实现数据平民化，让非技术背景的员工也能更加便利灵活地运用数据资源等等。经过有效的数智化过程，企业降本增效、开拓市场、改善利润空间也就水到渠成。</p><p></p><p>然而，对于大型企业而言，数智融合的目标与成果虽然令人振奋，但企业达成目标的道路上却充满荆棘。究其原因，大型企业往往组织庞大、业务多样、系统流程纷繁复杂。虽然积累了大量运营和业务数据，但这些数据通常都是无序存放，缺乏科学有效的治理机制，因此也很难在实践中发挥其应有的作用。</p><p></p><p>由于数据治理体系不过关，企业即便开始部署智能化应用，也时常面临缺少高质量数据支撑的尴尬境地，智能化场景就成为无源之水，无法真正为业务提供助力。另一种情况下，企业上层虽然在努力推进数智化转型，但一线部门和员工却并不清楚数智融合如何落地到实践场景中，投入大量资源采购的工具、技术，培养的技术人才最后只是在 IT 部门“空转”，难以同应用场景有效对接。这些问题都是企业，尤其是大型企业在数智化转型过程中面临的挑战，只有解决了这些挑战，才能为数智化铺平道路，让企业最终摘下数智融合与转型的成功果实。</p><p></p><p>有道无术，术尚可求，有术无道，则止于术。企业在数智化转型实践中的常见误区，就是先追求微观、细节的“术”，指望采购一些零散的工具和技术就能有效治理数据、落地智能应用。然而，尤其对于大型企业而言，在数智化进程初期就建立宏观视野，构筑转型的行动框架、方案模型是必不可少的步骤。</p><p></p><p>有了这样的行动框架做指引，企业才能自上而下建立清晰的转型预期、实施细则与评价和改进回路，并实现跨部门的高效协作，避免各自为政、懒散躺平、朝令夕改、资源浪费等常见问题。在科学的实施框架引导下，企业逐步完成搭建数据治理体系、浓缩高质量数据、构筑智能化应用、对接业务场景等任务，脚踏实地构筑起牢固的数智化底座，打开持续迭代进化的发展通道，转型成功也就顺理成章。</p><p></p><p></p><h2>企业数智化进阶模型，企业实现数智融合的成功之“道”</h2><p></p><p></p><p>企业数智化转型是一个综合、复杂、循序渐进的系统工程和长期过程，需遵循科学、合理的行动框架和进阶模型。行万里路不如明师指路，在企业领域，各行业面临的数智化转型难题大都是相通的，实践中的解决方案也是近似的。用友深耕企业服务市场三十五年，也一直在基于众多行业客户的领先实践归纳总结各类行之有效的模型和方法论，并通过企业场景的应用与反馈不断优化。针对企业的数智化进程，用友同样归纳出了一条成功之“道”，即“企业数智化1-2-3”，也被称为企业数智化进阶模型，可以服务企业更明晰便捷地实现数智化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39182fb93d77a0a060b9208e712d70bd.png" /></p><p></p><p>“数智化1”是企业要推进“云化连接”（上云），实现业务的云化部署、网络连接（含物联网）和实时感知；“数智化2”是企业要推进“数据驱动”（用数），实现全面数据服务，统一数据治理，并升级数智底座；“数智化3”是企业要推进“智能运营”（赋智），实现业务运营智能化、自然化人机交互和知识与应用生成。</p><p></p><p>从这三个阶段可以看出，数据与智能是一直相伴存在的。随着企业对应用需求的不断加深，数据与智能所提供的能力也需要越来越丰富，相应的对其底层支撑平台的要求也越来越高。如今大多数企业的数智化处于“数智化2”&nbsp;数据驱动层级，企业需要升级数智化底座，实现全面数据服务。</p><p></p><p>例如，有些企业基于用友 iuap 平台构建数智化供应链，通过深入挖掘离线数据以实现采购流程的可视化与风险识别，从而实时调整采购流程，对供应商进行画像评价。还有些企业使用用友 iuap 平台来构建数智化人才供应链，发现企业内部人才核心节点，定位意见领袖、关键人才岗位，并利用内部社交和邮件数据分析判别离职意向，从而辅助 HR 部门提前采取行动，降低人员流失，保留核心人才。进入数据驱动的企业可以将现有的数据沉淀为知识图谱，例如资产维修的知识图谱就可以帮助经验不足的资产维修人员快速定位和解决维修问题。</p><p></p><p>通过这些举措，企业的数据在运营生产的各个环节都发挥出了应有的作用，为企业贡献了可观的价值。</p><p></p><p>随着大模型和生成式 AI 技术的崛起，有部分领先的大型企业开始向智能运营进阶。但在这一过程中企业往往面临两大挑战：首先，大模型将扮演企业数智化底座核心操作系统的角色，为此需要更加广泛、高质量的数据来源，不仅包括了企业已有的基础数据，还需要更多产业级、社会化的数据资源。其次，有了这些产业级的数据支撑，大模型还需要对企业业务场景的深刻理解，才能有效应用在实际业务中。企业需要更多同时理解大模型技术与企业所处业务领域知识的复合型人才来运用大模型的能力，这就会对企业的人员配备、岗位体系乃至整个企业的组织模式带来巨变。</p><p></p><p>在数智化进阶模型的指引下，大型企业的数智化进程有“道”可依。此时，如何解决实现智能运营的两大挑战就成了关键问题，企业开始需要“术”层面的支持和帮助。</p><p></p><p></p><h2>用友 iuap，企业数智化实现智能运营之“术”</h2><p></p><p></p><p>对于大多数企业而言，生成式 AI 是一个全新的技术领域，企业的管理层和 IT 部门在这一领域都没有足够的知识和经验。正因如此，当企业试图自行构建智能运营体系时，就需要独自面对跨领域、社会化数据的治理、整合、标注、模型选择、模型训练、场景适配等诸多陌生问题。</p><p></p><p>尤其在生成式 AI 超越企业内部层面，纳入社会化能力的过程中，企业自有的 IT 和业务团队往往是力不从心的。而如果没有与企业业务充分融合的大模型能力，智能运营也就成为了空中楼阁。显然，大型企业尤其需要更高水平的、内建了生成式 AI 能力的数智化底座，在这样的底座基础上才能大范围落地智能应用，实现从数据驱动到智能运营的全面转变。</p><p></p><p>从数据驱动到智能运营，用友 iuap 为企业提供全面支持</p><p></p><p>生成式 AI 技术在企业落地，需要有成熟的数智化底座承接才不会陷入无人会用、无场景可用的尴尬，而企业数智化底座用友 iuap 平台为企业提供了所需的底座基础能力，在应用、数据、智能等多个维度支持企业业务快速创新。</p><p></p><p>应用层面，在智能运营层级，企业需要部署智慧化灵动应用，如智能助理、智能预算等。数智员工就是企业智能助理的一种形式，通过数智员工可以解决企业流程自动化、审批智能化、内容合规化、数据驱动语义化等问题。数智员工具备智能交互与自主学习能力，比如通过 AI 与 RPA 深度融合,AI 具备 ChatGPT 类的交互、学习能力，自动识别流程风险、自动学习审批，使得流程风险更可控，审批更智能。在工作流中引入智能审批助理，可以大幅提高工作效率，提升公司产出效能。通过数字人、技能、AI、业务流、对话流工场化设计，可以快速实现所见即所得。企业还可以根据自己的场景创作个性化形象、个性化能力的数智员工。</p><p></p><p>数据层面，处于智能运营层级的企业需要拥有更丰富的产业 / 社会级数据资源，相应的数据服务需要覆盖从展现级到分析级、控制级、决策级、创新级（如产品优化）的全部五层数据服务。</p><p></p><p>用友 iuap 数据中台通过数据移动、开发、治理、指标、挖掘、语义模型、数字大屏、移动分析、智能分析云、智能报告等功能大大简化了数据的采集、加工、治理和应用流程，为企业提供了一站式数据底座，支撑数据驱动的各类场景应用。用友 iuap 的数据服务能力已经涵盖了各个层面。</p><p></p><p>比如，某食品加工集团，基于用友 iuap 构建了企业“业务中台、数据中台、智能中台”三位一体的企业数智化底座，推动业务智能化应用，实现企业化智能化运营。该集团通过构建统一客户视图，对重点、关怀、风险、异动等客户群体，实施不同的营销策略，实现精准营销。基于约束理论最优化目标函数，结合遗传算法构建了排产优化模型，通过优化排产，降低企业生产成本。建立了风险预测模型，通过对现金流动性、利率敏感性、资本充足率、市场风险暴露值、异常交易、信用风险等指标和场景进行实时监控、及时预警。</p><p></p><p>智能层面，数智化处于智能运营层级的企业，其智能化进入了慧知层，全面应用企业服务大模型。</p><p></p><p>为了普及 AI 在企业的应用，用友于 2023 年 7 月发布了业界首个企业服务大模型 YonGPT。因为，用友在此前服务企业过程中，已经产生了大量的商业应用数据，这些数据对于企业而言是非常宝贵的资产，在 AI 技术的加持下，可以发挥更大价值。YonGPT 不仅可以通过上下文记忆、知识 / 库表索引、Prompt 工程、Agent 执行、通用工具集等扩充大模型的存储记忆、适配应用和调度执行能力，还沉淀了财务、人力、供应链、采购、制造、营销、研发、项目、资产、协同等领域场景的知识和领先实践。通过将用友长期业务实践中积累的大量跨行业、社会化数据与知识进行训练，可以更好地理解企业业务，帮企业作出准确决策，实现智能运营。</p><p></p><p>YonGPT 已经在企业经营洞察、智能库存优化、智能人才发现、智能预算分析、代码生成、供应商风控等数十种场景完成智能化赋能。</p><p></p><p>YonGPT 作为用友在 AI 领域的最新成果，可以为企业提供更加智能、高效、便捷的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a70f33e78348426c13ee469a9ce5b61.png" /></p><p></p><p>用友 iuap 为企业带来了更多高度专业化、场景化、社会化的数据资源，并将这些资源与企业的私有数据有机结合，形成可复用的专业能力。在用友 iuap 平台的支持下，企业无需从零开始进行大规模的投资建设与人才团队培养，也能顺利跨越数智化转型第二阶段到第三阶段的难关，更早建成智能化的运营体系，最大程度发挥企业内部和外部的数据生产要素的潜在价值。</p><p></p><p>编后语：</p><p></p><p>当大部分企业的数智化还处在“数智化2”层级时，一些行业领先企业已经开始朝“数智化3”层级迈进。“数智化2”处于企业走向智能运营的关键阶段，需要企业做好全面的数据治理及数据服务，全面升级数智底座，才能在“数智化3”层级拥有夯实的数据及平台基础。用友 iuap 作为更懂业务、技术领先、体系完整的数智化底座，为大型企业带来数据、智能、平台全面保障，以此为企业业务和应用服务。并且在“数智化3”层级，用友 iuap 持续输出以企业服务大模型为中心的智能化能力，助力企业顺利实现智能化运营，以数智化持续推动企业高质量发展！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</id>
            <title>英特尔数据中心与人工智能事业部 AI 软件架构师何普江确认出席 QCon 上海，分享大模型时代：最大化 CPU 价值的优化策略</title>
            <link>https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 大模型时代, CPU 价值优化策略, CPU 和 GPU 投机采样方法
<br>
<br>
总结: 本文介绍了即将在上海举办的QCon全球软件开发大会，其中AI软件架构师何普江将分享关于大模型时代下最大化CPU价值的优化策略。演讲内容包括利用CPU的多核特性、并行计算和AMX指令集扩展技术来提高处理速度，以及结合CPU和GPU的投机采样方法来充分利用CPU资源并减少对GPU的依赖。通过这些优化策略，可以提高模型推理速度，实现生成式模型部署落地。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。英特尔数据中心与人工智能事业部 AI 软件架构师何普江将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5627?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">大模型时代：最大化 CPU 价值的优化策略</a>"》主题分享，探讨一种结合 CPU 和 GPU 的投机采样方法，在大语言模型时代充分利用 CPU 资源的关键策略，以及最新的性能情况，以便了解这些优化策略的实际效果。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5627?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">何普江</a>"，2007 年硕士毕业于中国科学技术大学。精通英特尔软件架构、英特尔产品与技术以及 IA 平台性能优化。在英特尔工作期间，为国内主流 ISV 开发出基于 IA 平台的云计算产品过程中提供关键支持，并优化了多家主要互联网公司的核心产品，使其性能提升数倍。对 PyTorch，Tensorflow 等 AI 框架有深入研究，并拥有 10 年以上软件优化经验。工作期间曾获得英特尔中国个人员工最高荣誉奖，与国内互联网厂商多个部门进行深度合作，并在 2019 年助力某云厂商云在 MLPerf 评测中创下了业界领先的 Performance/TOPS 性能记录。他致力于基于 IA 架构平台的深度学习、机器学习研究和在互联网行业的落地推广工作，最新工作包括创建并开源了 CPU 上大语言模型的极致优化方案 xFasterTransformer。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型时代：最大化 CPU 价值的优化策略</p><p></p><p>本次演讲将探讨在大语言模型时代充分利用 CPU 资源的关键策略。具体介绍一些结合硬件特性的优化方法，例如利用 CPU 的多核特性、采用并行计算和 AMX 指令集扩展技术来提高处理速度。</p><p></p><p>此外还将介绍一种结合 CPU 和 GPU 的投机采样方法，通过在 CPU 上运行部分计算任务，充分利用 CPU 资源并减少对 GPU 的依赖。最后，我将分享一些最新的性能情况，让您了解这些优化策略的实际效果。通过这些方法，您将能够更好地利用 CPU 资源，提高模型推理速度，以更快速高效的实现生成式模型部署落地。</p><p></p><p>演讲提纲：</p><p></p><p>大语言模型时代为什么需要最大化 CPU 价值CPU 上的大模型优化策略</p><p>○ 大语言模型计算特点 </p><p>○ CPU 硬件特性概览 </p><p>○ 优化方法 </p><p>○ 从向量化到张量化 </p><p>○ 从并行执行到分布式推理 </p><p>○ 低精度优化 </p><p>○ 深入 CPU 微架构的软件优化 </p><p>○ 各优化策略的实际性能数据对比及效果展示</p><p>结合 CPU 和 GPU 的投机采样方法</p><p>○ CPU 和 GPU 协同工作的背景 </p><p>○ 投机采样技术的介绍 </p><p>○ 利用 CPU 进行部分计算任务的优势 </p><p>○ 优化方法：选择合适的投机采样策略、任务调度等</p><p>总结与展望</p><p>○ 各优化方法的核心优势与局限性总结 </p><p>○ 对未来大语言模型时代的展望与挑战</p><p></p><p>听众收益点：</p><p></p><p>○ 理解并结合硬件特性进行优化，提高模型推理速度和处理能力</p><p>○ 了解 CPU 上的最新性能情况，为实际业务的大模型线上部署提供更多选择</p><p>○ 掌握结合 CPU 和 GPU 协同工作的优化策略，减少对 GPU 的依赖，提高资源利用率</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！9 折优惠仅剩最后 4 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</id>
            <title>英特尔高宇：AI工作负载有多种形态和规模，硬件上没有一刀切的解决方案</title>
            <link>https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:51:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 大模型, 公共通用大模型, 个性化服务
<br>
<br>
总结: 去年年底以来，随着ChatGPT应用体验界面的推出，大模型的生成式AI技术得到了快速发展。公共通用大模型通过学习公共数据，可以生成高质量的文本、图像、声音和视频等内容，为智能创新和个性化服务提供了巨大的想象空间。为了保护数据安全和隐私，同时提供个性化服务，公共大模型和面向个人的专有大模型混合部署成为产业共识。 </div>
                        <hr>
                    
                    <p>去年年底以来，随着ChatGPT应用体验界面的推出，使得以大模型为主的生成式AI 技术取得了重大的并且快速地发展，大模型也展现出了令人惊叹的智能涌现能力，表现出了更为强大的创造性和通用场景的普通适用性，技术得以快速发展。</p><p></p><p>首先取得重大突破的是公共通用大模型，从人类社会大量存积下来的公共数据当中去学习，进而生成高质量的文本、图像、声音甚至是视频等内容，为各个领域的智能创新和每一个人的智能体验创新提供了巨大的想象空间。</p><p></p><p>然而，出于数据的安全和隐私保护的考虑，以及更高效率，更低成本来享用大模型通用能力的角度考虑，人们又既希望获得公共大模型目前的各类强大的通用服务，同时又希望AI 能够真正理解自己，提供专属的个性化服务，还要能够充分地保障个人的数据和隐私安全，为此，公共大模型和面向个人的专有大模型混合部署，正逐渐成为产业的一个共识。</p><p></p><p>在这样的时代背景下，作为消费和商用个体用户中最坚挺的终端，PC在AIGC时代承载了怎样的使命？</p><p></p><p>12月7日，首届AI PC产业创新论坛在北京联想总部举行。此次论坛汇聚了众多用户、终端厂商、算力厂商（芯片）、AI技术厂商（大模型）、应用领域生态合作伙伴，深度探讨AI PC为AI普惠带来的巨大改变。此外，在此次论坛上，业内首份《AI PC产业（中国）白皮书》重磅发布。</p><p></p><p>与会嘉宾认为，AI PC 到来之际，大模型将成为每一个人必不可少的助手，同时对推理的算力需求将超过训练的算力需求。算力集中于云端的模式变得不可持续，AI计算负载将逐渐由云端向边缘侧和端侧下沉。在搭建本地智能算力上，CPU+NPU+GPU 异构式架构方案是目前最为成熟的方案之一。</p><p></p><p>对此，英特尔中国区技术总经理高宇表示，AI工作负载有多种形态和规模。所以，从硬件上没有一刀切的解决方案。“基于多年的学习与市场经验，我们提出了XPU的概念，包括GPU/NPU/CPU。”他说，联想是英特尔的战略合作伙伴，双方已经基于即将发布的Meteor Lake处理器推进AI体验的开发和创新。</p><p></p><p>作为算力厂商的代表，英特尔正采取三项措施，来持续构筑端侧的算力。一是构建为AI而设计的高效能AI-Ready平台；二是提供工具以支持广泛的x86应用生态系统，三是激发创新，开启全新的AI体验，包括为软件和应用开发人员提供支持，以便在各个领域里都能更好将AI功能完美部署到PC客户端上。</p><p></p><p>英特尔今年还正式启动了首个“AI PC加速计划”，将在2025年前为超过1亿台PC带来人工智能特性。其中，通过与超过 100 家 ISV 合作伙伴深度合作，并集合 300 余项 AI 加速功能，英特尔将在音频效果、内容创建、游戏、安全、直播、视频协作等方面继续强化 PC 的体验。</p><p></p><p>据了解，在实践中，英特尔13代酷睿处理器已经可以流畅运行70亿到180亿参数的大模型，并成功部署了LLM。高宇表示，即将推出代号Meteor Lake的AI PC处理器，代表英特尔40年来最重大的架构转变，旨在为AI PC时代铺平道路。它是首个内置AI加速引擎NPU的处理器，可在PC上实现高能效的AI加速和本地推理。</p><p></p><p>为了完成用户相对复杂的任务，AI PC往往需要调动不同的模型和应用，为AI PC的能力进行补充和延伸。因此，AI PC功能的发挥不仅需要像英特尔这样的算力厂商的参与，还需要整个开放的行业生态作为支撑。</p><p></p><p>在AI PC的推动下，PC产业生态将从应用为本转向以人为本，用户成为行业生态创新的驱动者和创造者。模型、应用、算力厂商都需要围绕AI PC（终端）形态下新的以人为本的需求做出改变，在研发工作中对AI的高效运行予以充分的考量，以适应AI PC新时代。</p><p></p><p>联想作为终端厂商，是离用户最近的一端，因而被推到台前，成为生态组织者和生态的核心中枢。以场景需求为基础面向用户整合产业资源，承担AI PC技术整合创新交付者、新一代个人智能体及 AI入口创造者和用户体验维护者、本地化个人数据及隐私安全守护者和开放的AI应用生态标准制定者和推广者身份，职责重大。正是出于行业责任，联想联合国际数据公司IDC发布业内首份《AI PC产业（中国）白皮书》，对AI PC进行了全新定义，以加速构建AI PC产业新生态。</p><p></p><p>高宇最后表示，AI PC加速计划由即将发布的IntelCore Ultra处理器率先驱动。未来，英特尔将搭建性能并行和吞吐量适用于融合AI的媒体/3D/渲染的GPU，打造适用于持续的AI和分担AI负载的专用低功耗AI引擎NPU；迭代能够快速响应，适用于轻量级、单次推理的低延迟任务的CPU，相信在新平台的加持下，英特尔将加快与联想共同打造混合AI算力架构，驱动AI PC落地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</id>
            <title>零一万物Yi-34B-Chat 全球权威测评，开源黑马追平 GPT-3.5？</title>
            <link>https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 10:08:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Yi-34B-Chat, 测评结果, AlpacaEval Leaderboard, LMSYS ORG排行榜, SuperCLUE排行榜
<br>
<br>
总结: Yi-34B-Chat是一款性能优异的大语言模型，在多个测评平台上取得了出色的成绩。在AlpacaEval Leaderboard和LMSYS ORG排行榜中，Yi-34B-Chat超越了其他竞争对手，成为世界范围内仅次于GPT-4的大语言模型。在中文能力方面，Yi-34B-Chat也取得了令人瞩目的成绩，在SuperCLUE排行榜中表现出色。该模型还提供了4bit/8bit量化版，方便在消费级显卡上使用。通过AI Alignment团队的创新对齐策略，Yi-34B-Chat不仅在理解和适应人类需求方面表现出色，还与人类价值观对齐。 </div>
                        <hr>
                    
                    <p></p><p>继11月初零一万物发布性能优异的 Yi-34B 基座模型后，Yi-34B-Chat 微调模型在11月24日开源上线，而各家测评平台也相继给出了Yi-34B-Chat 的测试结果。</p><p>&nbsp;</p><p>模型地址：</p><p><a href="https://huggingface.co/01-ai/">https://huggingface.co/01-ai/</a>"</p><p><a href="https://www.modelscope.cn/organization/01ai">https://www.modelscope.cn/organization/01ai</a>"</p><p>&nbsp;</p><p></p><h2>各家测评结果</h2><p></p><p>&nbsp;</p><p>在斯坦福大学研发的大语言模型评测 AlpacaEval Leaderboard 中，Yi-34B-Chat以94.08%的胜率，超越LLaMA2 Chat 70B、Claude 2、ChatGPT，在 Alpaca 经认证的模型类别中，成为世界范围内仅次于GPT-4 英语能力的大语言模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a9c4fc531e7575e5fcf65d475dfb716.png" /></p><p>&nbsp;</p><p>AlpacaEval Leaderboard排行榜（发布于2023年12月7日）</p><p>&nbsp;</p><p>同一周，在加州大学伯克利分校主导的LMSYS ORG排行榜中，Yi-34B-Chat也以1102的Elo评分，晋升最新开源SOTA开源模型之列，性能表现追平GPT-3.5。</p><p>&nbsp;</p><p>伯克利LMSYS ORG排行榜采用了一种最为接近用户体感的 “聊天机器人竞技场” 特殊测评模式，即让众多大语言模型在评测平台随机进行一对一 battle，通过众筹真实用户来进行线上实时盲测和匿名投票。11月份，经25000个真实用户投票计算了20个大模型的总得分。Elo评分越高，说明模型在真实用户体验上的表现越出色。</p><p>&nbsp;</p><p>在开源模型中，Yi-34B-Chat 在英语能力上进入前十。LMSYS ORG 在12月8日官宣11月份总排行时评价：“Yi-34B-Chat 和 Tulu-2-DPO-70B 在开源界的进击表现已经追平 GPT-3.5”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f3e7d606ee4b2ef7fdc44af2325e3a4.png" /></p><p>&nbsp;</p><p>LMSYS ORG榜单（发布于2023年12月8日）</p><p>&nbsp;</p><p>在针对中文能力的排行榜方面，SuperCLUE从基础能力、专业能力和中文特性能力三个不同的维度，评估模型的能力。根据11月底发布的《SuperCLUE中文大模型基准评测报告 2023》，11月下旬首度发布的 Yi-34B Chat，迅速晋升到和诸多国产优秀大模型齐平的 “卓越领导者” 象限。在多项基准评测中的 “SuperCLUE 大模型对战胜率” 这项关键指标上，Yi-34B-Chat 取得31.82%的胜率，仅次于GPT4-Turbo。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b11a91538b615aa1301ff0c4054d7137.png" /></p><p>&nbsp;</p><p>中文SuperCLUE排行榜（发布于2023年11月28日）</p><p>&nbsp;</p><p>值得注意的是，Yi-34B-Chat 微调模型为开发者提供了 4bit/8bit 量化版模型。Yi-34B-Chat 4bit 量化版模型可以直接在消费级显卡（如RTX3090）上使用。</p><p>&nbsp;</p><p></p><h2>真实场景如何</h2><p></p><p>&nbsp;</p><p>Yi-34B-Chat 模型实力在不同的对话场景中实力如何？来看几个更直观的问题演示。</p><p>&nbsp;</p><p></p><h4>知识与生成</h4><p></p><p>&nbsp;</p><p>问：Transformer 模型结构能不能走向 AGI ?</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d78f0a4dab09a17eb32fc8654b708191.png" /></p><p>&nbsp;</p><p></p><h4>创意文案</h4><p></p><p>&nbsp;</p><p>问：给我生成一个小红书文案，给大家安利一只豆沙色的口红。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be47558b3b174390b2de5ec944c40e0f.png" /></p><p>&nbsp;</p><p></p><h4>中文理解</h4><p></p><p>&nbsp;</p><p>问：小王给领导送了一份礼物后。领导说：“小王，你这是什么意思？”小王：“一点心意，意思意思。”领导：“你这就不够意思了。”小王：“小意思，小意思。”领导：“小王，你这人真有意思。”小王：“也没什么别的意思。”领导：“那我多不好意思。”小王：“是我不好意思。”这个意思到底是什么意思？</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4ffb796172e36d6124d32749bd91eed.png" /></p><p>&nbsp;</p><p>据零一万物介绍，除了 Yi 系列强基座的贡献以外，Yi-34B-Chat 模型的效果还得益于其人工智能对齐（AI Alignment）团队采用了一系列创新对齐策略。通过精心设计的指令微调流程，不仅强化了模型在理解和适应人类需求方面的能力，还使得模型与人类价值观对齐，包括帮助性（Helpful），可靠性（Honest），无害性（Harmless）等。</p><p>&nbsp;</p><p>在强基座设定下，该团队采用了一种轻量化指令微调方案，该方案涵盖了单项能力提升和多项能力融合两个阶段。</p><p>&nbsp;</p><p>其中，单项能力包括通用指令跟随、创意内容生成、数学、推理、编程、泛COT、对话交互等，团队通过大量的消融实验，针对模型单能力构建和多能力融合总结了独家认知经验；在多能力融合阶段，团队采用网格搜索的方法来决定数据配比和超参数的设置，通过基准测试和自建评测集的结果来指导搜索过程，成功实现模型的多能力融合。</p><p>&nbsp;</p><p>在数据的量和质方面，零一万物团队认为，数据质量比数量重要，少量高质量数据比大量低质量数据更好，仅需少量数据（几条到几百条）就能激发模型特定单项能力。团队通过关注超出模型能力的“低质量”数据，来减少了模型“幻觉”。</p><p>&nbsp;</p><p>在指令多样性与难度方面，团队在各能力项下构建任务体系，实现训练数据中的指令均衡分布，提升模型泛化性。此外，团队发现训练数据的风格会影响模型收敛速度和能力上限的逼近程度，因此统一了回复风格，比如重点设计了CoT的回复风格，实现在轻量SFT情况下，避免了风格不一致加剧模型的“记忆”现象。</p><p>&nbsp;</p><p></p><h2>开源“满月”：有赞扬，也有质疑</h2><p></p><p>&nbsp;</p><p>Yi模型发布之初便是开源的。开源首月，Yi模型在Hugging Face社区下载量为16.8万，魔搭社区下载量1.2万，在GitHub 获得超过4900个Stars。</p><p>&nbsp;</p><p>多家知名公司和机构推出了基于Yi模型基座的微调模型，比如猎豹旗下的猎户星空公司推出的OrionStar-Yi-34B-Chat模型、南方科技大学和粤港澳大湾区数字经济研究院（简称IDEA研究院）认知计算与自然语言研究中心（简称CCNL中心）联合发布的SUS-Chat-34B等。</p><p>&nbsp;</p><p>知名技术写作者苏洋表示，根据他的近期观察，Hugging Face榜单中的前三十名有一半多是 Yi 和其他用户微调的 Yi-34B 的变体模型，原本占据榜单头部的 68B 和 70B 模型的数量目前只留有几个。</p><p>&nbsp;</p><p>苏洋曾尝试使用家里本地的机器，在纯 CPU 环境、CPU &amp; GPU 混合环境下对Yi模型进行测试，“结果比想象中要好。尤其是社区中的 finetune 后的版本，在对新闻、研究报告的摘要总结方面，对非结构化的信息中的实体识别和抽取上表现非常不错。”同时，苏洋也指出，可能是由于零一在训练过程中，出于安全考虑，过滤太多语料的缘故，一些本土化的内容仍然不够深入。</p><p>&nbsp;</p><p>根据亲身体验，苏洋总结道，34B 普通用户努努力还是能自己相对低成本跑起来的，68 B 和 70B 的模型想要本地运行，需要更多的资源，但目前分数上跟 34B 的拉不开太多差距，大概就是三四分平均分。</p><p>&nbsp;</p><p>开源后，Yi系列模型也遭到了一些质疑。</p><p>&nbsp;</p><p>开发者Eric Hartford敏锐发现了模型存在的一个问题：Yi模型使用了与LLaMA模型完全相同的架构，只是将两个张量改了名字。由于围绕LLaMA架构有很多投资和工具，保持张量名称的一致性是有价值的。Eric建议，在Yi被广泛传播前，及时恢复张量名称。</p><p>&nbsp;</p><p>Eric没有预想到，他的这个建议引来了关于Yi模型“抄袭”LLaMA的质疑。</p><p>&nbsp;</p><p>之后，零一万物很快便在各开源平台重新提交模型及代码，完成了开源社区的版本更新。零一万物表示，一个模型核心技术护城河是在架构之上，通过数据训练获得的参数和代码。在沿用了开源社区普遍使用的LLaMA 架构之上，零一万物团队从零开始，用高质量的数据集、自研训练科学和AI Infra打造了 Yi-34B 在内的系列模型。为了执行对比实验的需要，对部分推理参数进行了重新命名。原始出发点是为了充分测试模型，而非刻意隐瞒来源。</p><p>&nbsp;</p><p>Eric后来发推特为<a href="https://twitter.com/erhartford/status/1724563655545503822">Yi辩护称</a>"，“他们没有在任何事情上撒谎。所有的模型都是在相互借鉴架构。架构是学术研究的产物，已经发表在论文中，任何人都可以自由使用，这丝毫不减损Yi团队的成就。他们从零开始使用自己创建的数据集训练Yi，对开源领域的贡献是值得赞扬的。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/aa/aad8cc193901261e7cc18122efe10bbf.png" /></p><p></p><p>他还补充道，“使用Llama架构没有任何问题。训练才是关键。Yi给了我们目前可获得的最佳模型，没有任何可抱怨的。”</p><p></p><p>更多阅读：</p><p><a href="https://www.infoq.cn/article/cVfuQaHVJ0SDPtP2jb7m?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">零一万物回应“套壳 Llama”争议：基于 GPT 研发，对模型和训练的理解做了大量工作</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</id>
            <title>首期“AI+软件工程”主题沙龙在京顺利举办</title>
            <link>https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 10:03:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 通用人工智能, 软件工程, AI技术
<br>
<br>
总结: 以大模型为核心的通用人工智能正在驱动着新一轮智能革命的持续演进，并给软件工程带来了新的发展契机。大模型等AI技术在软件研发过程中赋予了强大的智能化能力，软件研发不再只依赖于人类的智慧，而是与AI相结合使其过程更加高效、高质量、低成本，代码生成、代码补全等新能力推动智能化软件工程范围的延展。 </div>
                        <hr>
                    
                    <p>以大模型为核心的通用人工智能正在驱动着新一轮智能革命的持续演进，并给软件工程带来了新的发展契机。大模型等AI技术在软件研发过程中赋予了强大的智能化能力，软件研发不再只依赖于人类的智慧，而是与AI相结合使其过程更加高效、高质量、低成本，代码生成、代码补全等新能力推动智能化软件工程范围的延展。</p><p></p><p>为加强AI+软件工程领域的交流互通，推动行业多融合发展，2023年11月21日，由中国信息通信研究院人工智能研究中心、中国软件行业协会和应用现代化产业联盟联合主办，中国人工智能产业发展联盟AI for 软件工程（AI4SE）工作组承办的首期“AI+软件工程”主题沙龙在京成功举办，线上线下总观看量超过6万。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7d0396c7b40cb595093b3dca99513f5.webp" /></p><p></p><p>中国信通院人工智能研究中心负责人魏凯、中国软件行业协会副秘书长付晓宇分别发表致辞。魏所指出，软件行业是大模型生态的聚集地，智能化的融合可以提升软件工程的效率和创新能力。然而，机遇和挑战并存，中国信通院将围绕AI和软件工程全生命周期持续开展系列工作，与产业各方共同面对挑战。付秘书长表示，AI为软件工程带来了新思路新方法，软件工程领域也在积极应对AI带来的挑战，中国软件行业协会一直致力于推动应用现代化的发展，期待看到我们的行业在面对挑战时，能够以开放、合作的态度，共同寻找解决方案，同时在AI与软件工程交叉领域看到更多创新与突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3977d976034a22719df06830e8a4a84b.webp" /></p><p>中国信通院人工智能研究中心负责人魏凯</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d917f57d13e9bd398a48a7bdd56b2fb2.webp" /></p><p>中国软件行业协会副秘书长付晓宇</p><p></p><p>本次沙龙邀请了来自华为、联通软件研究院、国金证券、软通动力、东吴证券、中软国际、中国信通院的7位行业专家，围绕落地方案、实践范式、问题与挑战、发展与趋势发表主题演讲。</p><p></p><p>华为技术有限公司软件工程专家贺美迅的分享主题是“Al辅助研发实践探索”。贺总围绕开发模式、关键挑战、参考经验和实践案例四个方面，对AI辅助研发的技术与过程进行深入浅出的分析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/6229db16a334e1bd89cdb8b62a317ea3.webp" /></p><p>华为技术有限公司软件工程专家贺美迅</p><p></p><p>中国联通软件研究院软件架构师常红珍分享的主题是“代码生成模型及智能工具探索”。常总围绕诸多场景对代码生成大模型的探索与实践过程进行了剖析，通过将代码大模型与公域和私域数据相结合，构建智能体协作开发框架、智能体应用框架，并引入专家经验，实现结构化思考和优质代码的生成，突破代码片段的限制，提升软件工程效率，并对未来进行展望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cab7f0b18c7a1ae6dad24a0c0e3ec780.webp" /></p><p>中国联通软件研究院软件架构师常红珍</p><p></p><p>国金证券技术负责人李晨带来了证券业开发大模型探索与实践的分享。李总从政策监管、开发大模型背景和国金证券落地实践的维度进行了详细分析。国金证券在设计评审类、编码辅助类、测试辅助类场景中实践成效初显，平均效率提升达30%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3ea92cade67fff5dbcdfe44564e6824.webp" /></p><p>国金证券技术负责人/架构师李晨</p><p></p><p>软通动力助理副总裁孙洪军分享了软通动力AISE产品研发及实践。孙总首先介绍了软通动力的AIGC整体布局，其次介绍了软通动力AISE产品的设计背景、系统架构和核心能力，最后就产品落地实践和成效展开了分享，某应用企业通过使用该产品达到20%-30%的提效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fbc4d69003ae2325968834ba0768965.webp" /></p><p>软通动力助理副总裁孙洪军</p><p></p><p>东吴证券信息技术总部副总经理任川分享了大模型训练和AI赋能的探索与实践。任总表示，AIGC大模型在证券行业具有广泛的应用前景，可在文案生成、智能搜索、券商智能中枢、BI助手等四大类关键领域上提供显著的提质增效服务能力。任总围绕东吴GPT分析其应用需求、应用领域和应用范式，并对未来AI规划进行分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e392a263efdd384abee9a9f764f81db7.webp" /></p><p>东吴证券信息技术总部副总经理任川</p><p></p><p>中软国际云智能业务集团CTO祁银红就中软在Al加速研发效能的实践和应用开发新模式的探索进行分享。祁总围绕中软的青燕平台，对探索过程与背景、核心能力及应用成效进行解析，该平台面向个人提供需求设计、文档生成、开发测试、代码生成等功能，面向组织提供全流程研发管理、测试管理、多级流水线等能力。以测试用例生成+故障排查为例，时间成本可缩短60%以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d005b0666ec73854052cb653092c49a.webp" /></p><p>中软国际云智能业务集团CTO祁银红</p><p></p><p>中国信通院云大所人工智能部主任曹峰发表了《智能化软件工程(AI4SE)发展现状与趋势》的主题演讲。曹主任从AI赋能软件工程的发展历程出发，介绍了当前软件工程相关大模型的现状、开发测试运维等场景的落地分析、智能化软件工程的关键技术与挑战，以及当前中国信通院在AI4SE领域开展的标准编制、案例征集等系列工作，最后对于AI4SE的多模态、全流程、新研发模式进行了展望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b26a8b6d59080663da6d12efea6fae1.webp" /></p><p>中国信通院云大所人工智能部主任曹峰</p><p></p><p>沙龙的最后，应用现代化产业联盟"AI+软件工程"工作组正式成立，由华为、中国信通院、国金证券、联通软研院、软通动力、中软国际、国金证券、明源云等单位共同参与启动仪式。应用现代化产业联盟也欢迎更多有志于“AI+软件工程”研究的企业和伙伴加入到联盟和工作组中来，共建开放协同创新的软件生态体系，促进产业健康有序发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a27cd48dd6717b1ff89424d73d8a6033.webp" /></p><p>应用现代化产业联盟"AI+软件工程"工作组首批单位合影，左起：李晨（国金证券）、吴振亮（明源云）、孙洪军（软通动力）、曹峰（信通院）、王千祥（华为）、祁银红（中软国际）、常红珍（中国联通）</p><p></p><p>应用现代化产业联盟汇聚了产业各方力量，将会加速产业创新升级，促进产业跨越式增长，共同推动应用软件的技术提升，赋能企业开展应用现代化转型。同时也希望更多优秀的软件企业加入应用现代化产业联盟以及“AI+软件工程”工作组中来，助力擘画中国式现代化的宏伟蓝图。</p><p></p><p>应用现代化产业联盟 &amp;“AI+软件工程”工作组</p><p></p><p>黄老师：yigang.huang@ami-alliance.org.cn</p><p>李老师：Linda.lidandan@ami-alliance.org.cn</p><p></p><p>扫一扫，加入应用现代化产业联盟</p><p><img src="https://static001.geekbang.org/infoq/11/114d8750f1bdcb25a7345827b4776f9b.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</id>
            <title>计算机图形学将迎来新突破？Meta携手斯坦福大学推出3D交互模型，VR时代似乎不远了</title>
            <link>https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:52:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 斯坦福大学, Meta/Facebook AI研究, CHOIS系统, 3D人机交互
<br>
<br>
总结: 斯坦福大学与Meta/Facebook AI研究实验室共同开发的CHOIS系统是一套突破性的AI系统，能够根据文本描述在虚拟人和物体之间生成自然、协调的3D人机交互。该系统利用条件扩散模型技术生成精确的交互动作，能够模拟连续的人类行为。CHOIS系统的出现对计算机图形学、AI与机器人技术产生了深远影响，能够大大提高动画制作效率，实现更高水平的虚拟现实体验，并为机器人在服务型领域的应用提供了新的可能性。这一成果令人惊艳，前景值得期待。 </div>
                        <hr>
                    
                    <p>近日，斯坦福大学与Meta/Facebook&nbsp;AI研究（FAIR）实验室的工作人员共同开发出一套突破性的AI系统，能够仅根据文本描述在虚拟人和物体之间生成自然、协调的运动关系。</p><p>&nbsp;</p><p>这套新系统被称为CHOIS（Controllable Human-Object Interaction Synthesis，即可控人机交互合成），使用最新的条件扩散模型技术生成无缝且精确的交互，例如“将桌子举过头顶、行走，然后放下桌子。”</p><p>&nbsp;</p><p>简而言之，这是一套先进的人工智能系统，用于合成逼真的 3D 人机交互。</p><p>&nbsp;</p><p>这项工作被公布在arXiv论文预发表网站的一篇文章中，也让我们得以一睹虚拟人如人类般顺畅理解并响应语言命令的未来景观。例如，把椅子拉近桌子来创造一个工作空间，调整落地灯以投射出完美的光芒，或者整齐地存放手提箱。每一项任务都需要人、物体和周围环境之间的精确协调。语言是表达和传达这些意图的有力工具，在语言和场景背景的指导下，合成逼真的人类和物体运动是构建先进的人工智能系统的基石，该系统可以在不同的3D环境中模拟连续的人类行为。</p><p>&nbsp;</p><p>论文地址：<a href="https://arxiv.org/pdf/2312.03913.pdf">https://arxiv.org/pdf/2312.03913.pdf</a>"</p><p>&nbsp;</p><p>研究人员们在文章中指出，“根据语言描述在3D场景中生成连续的人-物交互一直存在不少挑战。”</p><p>&nbsp;</p><p>他们必须确保生成的运动真实且协调同步，保持人手与物体之间的适当接触，且物体的运行应当与人类行为具有因果关系。</p><p></p><h2>如何实现</h2><p></p><p></p><p>CHOIS系统之所以效果拔群，依靠的就是其在3D环境中摸索出一套独特的人-物交互合成方法。CHOIS的核心为条件扩散模型，这是一种能够模拟详尽运动序列的生成模型。</p><p>&nbsp;</p><p>当给定人/物位置的初始状态以及所需操作的语言描述之后，CHOIS就会据此生成一系列动作，最终完成任务要求的交互效果。</p><p>&nbsp;</p><p>例如，假设指令是将灯具移到沙发旁边，CHOIS会理解指令内容并创建一段逼真的动画，显示人类形象拿起灯具并将其放置在沙发附近。</p><p>&nbsp;</p><p>利用 AMASS 等大规模、高质量的运动捕捉数据集，人们对生成人体运动建模的兴趣有所上升，包括动作条件合成和文本条件合成。虽然之前的工作使用 VAE 公式从文本生成不同的人体运动，但 CHOIS 专注于人与物体的交互。与通常以手部运动合成为中心的现有方法不同，CHOIS 在物体抓取之前考虑全身运动，并根据人体运动预测物体运动，为交互式 3D 场景模拟提供全面的解决方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae3859d5e2f84eeb8a8685af1054800d.jpeg" /></p><p></p><p>给定初始对象和人类状态、语言描述和3D场景中的稀疏对象路径点，CHOIS生成的物体运动与人体运动同步。</p><p>&nbsp;</p><p>CHOIS的独特之处，就在于它使用稀疏对象路径点和语言描述来指导动画生成。各个路径点充当对象移动轨迹中的关键标记点，确保运动不仅符合物理规律，而且与语言输入中描述的高级目标保持一致。</p><p>&nbsp;</p><p>CHOIS的另一大优势，在于能够将语言理解能力与物理模拟功能加以结合。传统模型往往难以将语言同空间和身体动作联系起来，特别对于较大的交互范围，必须考虑诸多因素才能始终保持交互的真实性。</p><p>&nbsp;</p><p>CHOIS首先解释语言描述所承载的意图和风格，而后将其转化为一系列既符合人体构造、又不违背物体特性的肢体动作，从而解决了大范围交互过程中的这一现实难题。</p><p>&nbsp;</p><p>该系统尤其具有开创性的一点，就是它能准确表现接触点（例如手与物体之间的接触位置），且物体的运行与人类化身施加的力保持一致。此外，该模型在训练和生成阶段还引入了专门的损失函数和指导性术语，旨在强制遵循这些物理约束，这也是让AI成功实现以人类方式理解物理世界、并与物理世界正确交互的重要一步。</p><p></p><h2>对计算机图形学、AI与机器人技术的影响</h2><p></p><p></p><p>CHOIS系统对计算机图形学产生了深远影响，特别是在动画和虚拟现实领域。通过让AI获得解释自然语言指令并据此生成逼真人机交互过程的能力，CHOIS能够大大减少制作复杂场景动画所需要的时间和精力。</p><p>&nbsp;</p><p>动画师们可以使用这项技术来创建出以往极为费时费力的关键帧动画序列，显著提升设计效率与成果产出。此外，在虚拟现实环境当中，CHOIS还能带来更加身临其境且高度交互的体验，由用户通过自然语言指挥虚拟角色，并观察其以逼真精度执行任务的全过程。这种更高水平的交互能够将VR体验从僵化、脚本化的事件转化为更加顺畅自然的动态环境用户输入响应效果。</p><p>&nbsp;</p><p>在AI和机器人领域，CHOIS则代表我们朝着更加自主的情境感知系统迈出的一大步。传统机器人往往受到预编程例程的限制，而CHOIS这类系统的出现能够帮助其更好地理解现实世界、并顺利按照自然语言给出的描述完成任务。</p><p>&nbsp;</p><p>这对于医疗保健、酒店或家庭环境下的服务型机器人来说尤其有着变革性的影响。在这类环境下，理解物理空间并在其中执行各类任务的能力往往至关重要。</p><p>&nbsp;</p><p>对于AI来说，这种同时处理语言和视觉信息以引导任务执行的能力，也使其距离充分理解情境和环境上下文又更进了一步。而且在此之前，这种能力一直是人类的优势和专利。在CHOIS的支持下，未来的AI系统有望在更多复杂任务中发挥更大的作用，不仅能够消化人类指令的“内容”、更能理解人类指令的操作“方式”，以前所未有的灵活性适应新的挑战。</p><p></p><h2>成果令人惊艳，前景值得期待</h2><p></p><p>&nbsp;</p><p>CHOIS代表了人工智能领域的重大飞跃，特别是在计算机视觉和人机交互领域。通过综合 3D 人与物体交互，CHOIS 可以生成逼真的动画和场景，这对于创建沉浸式虚拟体验至关重要。</p><p>&nbsp;</p><p>该系统使用组合分层方法来理解人类与物体之间交互的复杂本质。这涉及将交互分解为更小的、可管理的部分，并理解这些部分之间的关​​系。模型的层次结构使其能够考虑交互的上下文，例如环境和所涉及对象的属性。</p><p>&nbsp;</p><p>CHOIS 由深度学习算法提供支持，深度学习算法是机器学习的子集。这些算法使系统能够从人与物体交互的大型数据集中学习，随着时间的推移提高其准确性和预测能力。</p><p>&nbsp;</p><p>总体而言，斯坦福大学和Meta的研究人员在计算机视觉、自然语言处理（NLP）和机器人技术交叉领域的这一极具挑战的问题上，成功取得了关键进展。</p><p>&nbsp;</p><p>研究团队认为，他们的工作是建立先进AI系统的重要一步，该系统能够在不同的3D环境中模拟连续的人类行为。CHOIS也为进一步研究如何利用3D场景加语言输入来合成人机交互过程打开了大门，有望在未来孕育出更加复杂的AI系统。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://venturebeat.com/ai/stanford-and-meta-inch-towards-ai-that-acts-human-with-new-chois-interaction-model/">https://venturebeat.com/ai/stanford-and-meta-inch-towards-ai-that-acts-human-with-new-chois-interaction-model/</a>"</p><p><a href="https://isp.page/news/chois-stanford-and-fair-metas-revolutionary-ai-for-realistic-3d-human-object-interactions/#gsc.tab=0">https://isp.page/news/chois-stanford-and-fair-metas-revolutionary-ai-for-realistic-3d-human-object-interactions/#gsc.tab=0</a>"</p><p><a href="https://www.marktechpost.com/2023/12/10/researchers-from-stanford-university-and-fair-meta-unveil-chois-a-groundbreaking-ai-method-for-synthesizing-realistic-3d-human-object-interactions-guided-by-language/">https://www.marktechpost.com/2023/12/10/researchers-from-stanford-university-and-fair-meta-unveil-chois-a-groundbreaking-ai-method-for-synthesizing-realistic-3d-human-object-interactions-guided-by-language/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</id>
            <title>百度8500万挖不来“AI教父”；淘天年薪百万起步抢全球顶尖人才，上不封顶；王慧文病休后首次动作：AI投资｜Q资讯</title>
            <link>https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:07:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里, 年度股息, 淘天集团, 顶尖人才, 百度, AI教父, 比尔盖茨, 收入, 谷歌, Gemini大模型, 王慧文, OneFlow团队, 腾讯视频
<br>
<br>
总结: 阿里将首次派发年度股息，总额近179亿；淘天集团抢全球顶尖人才，年薪百万起上不封顶；百度8500万挖“AI教父”被拒，选择入职谷歌；比尔盖茨每天收入1095万美元，约普通人一生收入4倍；谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑；王慧文病休后首次动作，入股OneFlow团队新创业项目；腾讯视频出现故障，引发用户不满。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>阿里将首次派发年度股息，总额近179亿；淘天集团抢全球顶尖人才，年薪百万起上不封顶；百度8500万挖“AI教父”被拒，选择入职谷歌；比尔盖茨每天收入1095万美元，约普通人一生收入4倍；谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑；王慧文病休后首次动作，入股OneFlow团队新创业项目；卷入300亿骗局官司，京东回应：这是一个匪夷所思的恶意诉讼……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>阿里将首次派发年度股息，总额近179亿</h4><p></p><p>&nbsp;</p><p>12月6日，阿里发布公告，将向截至2023年12月21日收市时登记在册的普通股持有人和美国存托股持有人，就2023财年首次派发年度股息。金额分别为每股普通股0.125美元或每股美国存托股1.00美元，以美元支付。根据披露，股息总额约为25亿美元（当前约179亿元人民币）。阿里称，在现有股份回购计划基础上继续努力提高股东回报。</p><p>&nbsp;</p><p>此次面向全体股东的派息决定，是阿里巴巴2014年上市美股，以及2019年再次回归港股以来首次大规模分红派息。据梳理，阿里巴巴成立以来就以保守的财务经营策略闻名，包括本次分红派息在内，仅有3次分红派息记录。</p><p>&nbsp;</p><p></p><h4>淘天集团抢全球顶尖人才，年薪百万起上不封顶</h4><p></p><p>&nbsp;</p><p>近日，淘天集团启动一项名为T-Star的顶尖人才招聘计划，发放的offer不设层级，采取定制化培养模式，配备“大牛”主管和顶级研发平台资源，年薪百万起上不封顶。</p><p>&nbsp;</p><p>根据淘天集团招聘官网公布的信息，目前，T-Star计划已经开放了10种算法工程师岗位，工作方向包括自然语言处理、机器学习、多模态、三维重建、计算机视觉、3D等，工作地点为杭州、北京等。</p><p>&nbsp;</p><p></p><h4>百度8500万挖“AI教父”被拒，选择入职谷歌</h4><p></p><p>&nbsp;</p><p>12月4日，据知情人士透露，百度公司曾出价1200万美元(约合8486万元人民币)邀请“AI教父”杰弗里·辛顿(Geoffrey Hinton)及其学生加入公司，但被拒绝。“我们不知道自己值多少钱。”辛顿表示。他咨询了收购方面的律师和专家，想出了一个计划：“我们将组织一场拍卖，自己兜售自己。”</p><p>&nbsp;</p><p>最终，辛顿博士和他的学生们在4400万美元(约合3.1亿元人民币)的价格上停止了这次拍卖。虽然出价仍在上升，但他们想为谷歌工作。这一报酬已经很惊人。据悉，今年5月辛顿宣布从谷歌离职。辛顿表示，从谷歌辞职是为了可以自由地谈论AI的风险。他说，现在对自己一生从事的工作感到有些后悔。</p><p>&nbsp;</p><p></p><h4>比尔盖茨每天收入1095万美元，约普通人一生收入4倍</h4><p></p><p>&nbsp;</p><p>根据求职信息网站Zippia的数据，一个普通人一生的平均收入约为270万美元，而比尔·盖茨一天的收入大约是这个数字的3~4倍。据预计，盖茨每天的收入约为1095万美元，相当于每秒约117美元。还有另外一组数据显示，盖茨每天进账约760万美元，相当于每小时319635美元。</p><p>&nbsp;</p><p></p><h4>谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑</h4><p></p><p>&nbsp;</p><p>谷歌 12 月 6 日宣布推出其认为规模最大、功能最强大的人工智能模型 Gemini。Gemini 将包括三种不同的套件：Gemini Ultra、Gemini Pro 和 Gemini Nano。根据谷歌给出的基准测试结果，Gemini 在许多测试中都表现出了“最先进的性能”，甚至在大部分基准测试中完全击败了 OpenAI 的 GPT-4。</p><p>&nbsp;</p><p>同时，谷歌也发布了Gemini Ultra官方演示视频，展示了这款模型的强大能力。不过，依然有人质疑Gemini的能力。</p><p>&nbsp;</p><p>据报道，Gemini在MMLU多任务语言理解数据集测试中显示出色，但对比GPT-4时的提示技巧和展示方式引发了争议。质疑者认为，Gemini在使用提示技巧+32次尝试的标准下超越了GPT-4，但这一标准是否公平受到质疑。图表比例尺的问题也被揭示，引起了技术主管的修正。Gemini发布的视频在展示时也引起了关注，部分观点认为其中可能存在剪辑和非实时录制。</p><p>&nbsp;</p><p>查看更多：</p><p><a href="https://mp.weixin.qq.com/s/Yqi4rcyEmvg9g6LCqbxYxA">刚发布就被质疑？超过 GPT-4 的“最强”大模型 Gemini、“最高效”训练加速器，谷歌到底行不行</a>"</p><p>&nbsp;</p><p></p><h4>王慧文病休后首次动作，入股OneFlow团队新创业项目</h4><p></p><p>&nbsp;</p><p>在病休近6个月后，王慧文突然有了新动作，再次与袁进辉牵手，入股其创业新公司硅动科技。据公开资料显示，就在这两日，北京硅动科技有限公司新增王慧文为股东，注册资本由100万人民币增至约105.26万人民币。也就是说，王慧文目前在袁进辉新公司的持股比例约为5%。</p><p>&nbsp;</p><p>OneFlow是国内知名开源深度学习框架及开发平台。其团队上次创业一流科技时，由王慧文的光年之外收购其46.52%股权。不过，随着6月底光年之外创始人王慧文病退消息曝光，美团收购光年之外100%的权益，一流科技OneFlow团队作为其核心资产也转归美团名下。在50天后，袁进辉宣布带领OneFlow原班人马再次创业。消息传出后不到半个月，硅动科技注册成功。</p><p>&nbsp;</p><p></p><h4>“腾讯视频崩了”上热搜</h4><p></p><p>&nbsp;</p><p>12 月 3 日晚，腾讯视频出现网络故障，有网友反馈出现首页无法加载内容、VIP 用户看不了会员视频等情况。#腾讯视频崩了# #腾讯会员 没了#词条相继冲上微博热搜。</p><p>&nbsp;</p><p>稍晚些时候，@腾讯视频就“App 崩了”发布致歉声明称，目前腾讯视频出现了短暂技术问题，我们正在加紧修复，各项功能在逐步恢复中。感谢您的耐心等待，由此给您带来的不便我们深感歉意。</p><p>&nbsp;</p><p>除了腾讯视频，近期遭遇宕机事件的还有滴滴、淘宝、闲鱼、钉钉、阿里云盘等多个App。据媒体统计，以此被多家媒体报道或登上热搜榜为基准，2022年约发生了9起宕机事件，而今年以来，类似的事件已发生14起。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/-KVKVfq0CayLyRkEbp2Rcg">互联网大厂“组团”宕机，都怪降本增“笑”？</a>"</p><p>&nbsp;</p><p></p><h4>被卷入300亿骗局官司，京东回应：这是一个匪夷所思的恶意诉讼</h4><p></p><p>&nbsp;</p><p>12月4日消息，最近京东集团、承兴集团、诺亚财富之前的各种消息闹得沸沸扬扬，京东还被告上法庭。据悉，此事起因是承兴集团的罗静造假，冒充京东工作人员，并私自刻章，以京东、苏宁等应收账款来找金融机构（诺亚财富）贷款，最终骗走300亿并跑路暴雷，结果被抓。诺亚财富一纸诉状把京东给告上法庭，想让京东还钱。</p><p>&nbsp;</p><p>对于此事，京东集团官微“京东发言人”最新发布一份声明回应，称京东作为毫不知情的受害者，被卷入历时四年的恶意诉讼中，公司的声誉和权益遭受重大损失，相信法院会有公正的判决。12月4日晚，诺亚财富发布声明称，已关注到京东集团发布的关于我司的声明，该声明中“诺亚财富近年来先后发生十余起类似事件，上百亿基金......”等描述严重失实，已侵犯了我司名誉，我司将采取法律措施，维护自身合法权益。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>小米14手机内核已在GitHub开源</h4><p></p><p>&nbsp;</p><p>据报道，小米14/Pro内核现已在Github开源，AOSP版本基于Android U。公开内核源码可以让第三方开发者进行修改，开发人员和愿意折腾的用户能够充分利用硬件的潜力，市场上也会很快出现该机型的第三方固件。</p><p>&nbsp;</p><p>开源地址：</p><p><a href="https://github.com/MiCode/Xiaomi_Kernel_OpenSource/tree/shennong-u-oss">https://github.com/MiCode/Xiaomi_Kernel_OpenSource/tree/shennong-u-oss</a>"</p><p>&nbsp;</p><p></p><h4>员工称亚马逊AI聊天机器人Q “幻觉”严重，且泄露公司机密数据</h4><p></p><p>&nbsp;</p><p>根据国外科技媒体披露的一份内部文件，亚马逊员工称旗下AI聊天机器人Q存在严重的“幻觉”问题，并泄露了包括AWS数据中心位置、内部折扣计划等诸多机密信息。报告文件显示，亚马逊Q会产生幻觉，返回有害或不适当的聊天内容。例如，亚马逊Q会返回过时的安全信息，可能会让客户面临风险。</p><p>&nbsp;</p><p>亚马逊淡化了员工讨论的重要性，并声称没有发现任何安全问题。然而，泄露的文件引发了人们对Q准确性和安全性的担忧，Q仍处于预览阶段，尚未正式发布。文章发表后，该发言人发布一份声明，反驳了员工的说法，称亚马逊Q没有泄露机密信息。</p><p>&nbsp;</p><p></p><h4>Meta 推出独立的 AI 图像生成器，目前免费但只支持英文提示词&nbsp;</h4><p></p><p>&nbsp;</p><p>Meta 公司日前推出全新的、独立的 AI 图像生成器 ——Imagine with Meta，允许用户通过自然语言描述来创建图像。据介绍，新的人工图像生成器由 Meta 现有的 Emu 图像生成模型提供支持，可根据文本提示创建高分辨率图像。它目前对美国的英语用户免费使用（后续是否收费未知），并且每个提示都会生成四个图像。</p><p>&nbsp;</p><p>此前，Meta 图像生成模型因带有种族偏见的图像贴纸而面临争议。为了解决此类问题，Meta 表示将开始向 Imagine with Meta 生成的图像添加隐形水印，这些水印将由人工智能模型生成，并可由相应模型检测，以提高内容透明度。</p><p>&nbsp;</p><p></p><h4>Hugging Face 现 API 令牌漏洞，黑客可获取微软、谷歌等模型库权限</h4><p></p><p>&nbsp;</p><p>安全公司 Lasso Security 日前发现 AI 模型平台 Hugging Face 上存在 API 令牌漏洞，黑客可获取微软、谷歌、Meta 等公司的令牌，并能够访问模型库，污染训练数据或窃取、修改 AI 模型。由于平台的令牌信息写死在 API 中，因此黑客可以直接从 Hugging Face 及 GitHub 的存储库（repository）获得平台上各模型分发者的 API 令牌（token），安全人员一共从上述平台中找到 1681 个有效的令牌。</p><p>&nbsp;</p><p></p><h4>支付宝、麦当劳中国等相继启动鸿蒙原生应用开发</h4><p></p><p>&nbsp;</p><p>12月7日，支付宝与华为终端宣布合作，基于HarmonyOS NEXT启动支付宝鸿蒙原生应用开发，华为常务董事、终端BG CEO、智能汽车解决方案BU董事长余承东和蚂蚁集团董事长兼首席执行官井贤栋均现身签约现场。</p><p>&nbsp;</p><p>无独有偶，12月6日，麦当劳中国也与华为达成鸿蒙合作协议，正式宣布麦当劳中国APP将基于HarmonyOS NEXT启动鸿蒙原生应用开发，成为首批启动鸿蒙原生应用开发的全球大型跨国连锁餐饮企业，该公司在中国市场拥有5500多家餐厅，每年服务顾客超十亿人次。</p><p>&nbsp;</p><p>随着华为宣布鸿蒙原生应用全面启动，近期美团、去哪儿、新浪、钉钉、蚂蚁集团、小红书、58集团、哔哩哔哩、高德地图等互联网公司均已宣布加入鸿蒙原生应用开发行列。</p><p>&nbsp;</p><p></p><h4>IntelliJ IDEA 2023.3 版本更新发布</h4><p></p><p>&nbsp;</p><p>IntelliJ IDEA 2023.3 版本更新现已发布，在这一版本中，JetBrains 表示 AI Assistant 持续演进，现已超越技术预览阶段，获得了大量令人期待的改进。在其他方面，此版本包括对最新 Java 21 功能的全面支持，引入了带有编辑操作的直观浮动工具栏，并添加了 Run to Cursor（运行到光标）嵌入选项来增强调试工作流。IntelliJ IDEA Ultimate 现在提供无缝的开箱即用 Kubernetes 开发体验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</id>
            <title>分布式数据库 GaiaDB-X 金融应用实践</title>
            <link>https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:03:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 银行新一代核心系统建设背景, 架构, 分布式架构, 分布式数据库
<br>
<br>
总结: 银行业务的快速发展和迭代更新速度加快，使得原有的基于大型机的核心系统架构无法满足需求。为了支持业务的持续增长和创新，银行开始将核心系统从大型机下移到通用服务器架构上，并采用分布式架构和分布式数据库来满足分布式扩展性、强一致性和容灾能力的要求。这使得银行核心系统的架构与互联网公司的技术体系越来越接近，未来银行业与互联网业的技术交流和人才流动将进一步增加。 </div>
                        <hr>
                    
                    <p></p><h2>一、银行新一代核心系统建设背景及架构</h2><p></p><p></p><p>在银行的 IT 建设历程中，尤其是中大行，大多都基于大型机和小型机来构建核心系统。随着银行业务的快速发展，这样的系统对业务的支持越来越举步维艰，主要体现在以下四个方面：</p><p></p><p>首先是难以支持银行快速发展的业务。随着国内电商、互联网支付、手机支付的迅猛发展，银行的交易量出现指数级的增长。比如我们的某银行客户，当前每秒的交易量峰值在一万多左右，预计在未来几年会逐渐增长到每秒 6 万笔交易，而且后续还会继续增长。在这种情况下，基于大型机的集中式架构，单纯靠硬件的升配，已经无法支持业务的持续增长。第二是难以匹配银行系统的迭代更新速度。原有的基于大型机的胖核心架构，迭代周期往往在数月甚至半年。但随着银行间、以及银行与互联网金融企业之间的竞争加剧，银行迫切需要快速推出新业务进行创新，他们也希望能够像互联网公司那样，能够按周级进行快速迭代，快速响应业务需求。第三是系统风险。银行业迫切需要做到软件及硬件的自主可控。第四是生态封闭。大型机技术发展慢、人才难招。现在再去外面招一个懂 IBM 大型机的人已经很难了。</p><p></p><p>因此，在国家现有政策的引导下，各大银行最近几年都在做一个事情：把原有的核心架构从大型机下移到传统的通用服务器架构上，并建设一套自主可控的新技术体系，简称核心系统下移。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2eebfc9abdbfbf4124d2a6b76ebcb3c.png" /></p><p></p><p>在进一步理解银行系统之前，我们先了解下银行客户的业务体量、业务需求以及核心系统对业务支持情况。以一个国有大行来举例：它的客户量在 5-7 亿，有 10-20 亿账户，在全国有 2-4 万个网点。从每秒峰值交易量来看，约为每秒 5-8 万笔交易。</p><p></p><p>具体到数据库层，支持以上业务还需要联机交易系统，例如存贷汇业务。数据库层最大的表有百亿级记录数，TPS 达到百万级。此外，统一查询业务要求支持近十年交易明细的查询，即万亿级的查询记录数。即使放到互联网公司用的通用服务器，也是需要上千台服务器才能支撑相关业务的开展。</p><p></p><p>通过以上的介绍，大家可以发现，银行客户的业务体量和数据量已经达到了大型互联网公司的量级。如果想把这个系统从大型机下移到通用服务器架构，那么原有的集中式架构肯定是无法满足的，必须像互联网公司一样采用分布式架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ede303f87fe7f77d969325bf8b0660f.png" /></p><p></p><p>因为银行有和大型互联网公司相近的业务体量，因此在技术体系上，也借鉴了互联网公司的技术体系。</p><p></p><p>从 IaaS 层来看，银行采用了 X86、ARM 架构的通用服务器，也用到了混合云技术，大量使用了虚拟机与容器服务。</p><p></p><p>在 PaaS 层，银行使用了大量的分布式系统，如开源的微服务框架（例如 SpringCloud ），以及开源的或者商业的数据库，包括分布式/单机/关系型/缓存型的数据库，以及日志数据库 ES、时序数据库等。在中间件层，也用到了大量的开源的或者基于开源改造后的组件，例如消息队列、对象存储、分布式锁等。</p><p></p><p>在 SaaS 层，银行主要通过单元化 + 微服务的架构来实现分布式扩展。银行将自身业务应用划分成三种单元。</p><p></p><p>最上层是全局单元，主要是起到全局路由及流量分发的作用。第二层是业务单元，核心的业务逻辑都在该部分来实现。同时，为了实现业务的横向扩展并支持数亿客户量，银行业跟互联网公司一样，对业务进行单元化拆分。例如我们接触到的某银行，就是将自身的业务拆分为了 16 个业务单元，每个单元五千万客户， 16 个单元部署在两个机房形成同城双活的架构。最底层是公共单元，一些不太好或没必要做单元化拆分的应用，放在公共单元提供公共服务。</p><p></p><p>通过上述分析可以看到，在新一代银行核心系统里面，整体的架构体系已经和互联网公司很接近了，大家用的都是相同的技术栈，只是服务的业务场景不同。在未来，银行业跟互联网业的技术交流会进一步紧密，人才的流动也会进一步频繁。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6dada50bc1d570cafbcad31f40a441dc.png" /></p><p></p><p>在业务采用了单元化划分后，数据库的架构是怎么样的呢？目前在银行的新核心系统下移中，主要采用了以下两种数据库架构：</p><p></p><p>第一种是单机数据库架构。这种数据库架构比较简单，故障域较小，但相对来说业务系统会比较复杂。因为有些模块，如全局路由模块，是全局的总入口，没法做单元化拆分。因此一组单机数据库无法满足性能与容量需求，依然需要在业务层做数据拆分。除此之外，单机数据库无法完全支持业务单元层的业务落地。前面提到，我们接触到的某银行一个业务单元要承担五千万的客户数，一组单机数据库依然无法支持。于是在业务层进一步拆分为 4 组数据库共 64 张子表，业务层需要去解决大量的拆分及分布式事务相关的业务逻辑，整体就更复杂了。</p><p></p><p>另外一种是分布式数据库架构。这样的数据库内部架构虽然更为复杂，但它可以提供更好的性能。对业务层来说，一个单元采用一组数据分布数据库即可，业务层的逻辑就更为简单了。</p><p>因此我们认为，随着分布式数据库的逐步成熟与应用逐渐广泛，业务单元化 + 分布式数据库会逐渐成为流行的银行业务架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/1805f802303d1452f0ddc0dcf9e08ef3.png" /></p><p></p><p>综上，银行核心系统在下移的场景下，对数据库在如下几个方面提出了要求：</p><p></p><p>第一是分布式扩展性。由于采用了通用服务器，它的单机性能要远弱于大型机或者小型机。在这种情况下，数据库需要具备分布式可扩展的能力来满足上亿客户的金融需求。第二是强一致性。金融场景对数据正确性、一致性的要求极高。因此要严格保障对事务的 ACID 特性。否则，业务层就要做大量的工作。第三是容灾能力。通用服务器在硬件故障率方面要比大型机、小型机高很多。因此需要我们的数据库有更为可靠的可用机制以保障 SLA。同时，监管对于容灾能力的要求也在不断提升。比如，对于新建设的核心系统，监管要求必须满足 5 级以上的容灾能力，且满足同城双活并保证 RPO 为 0。在具体执行上，监管的要求也越来越严格，比如同城双活，之前是只需要具备相关的技术方案即可，但现在每年人行的监管都会直接到现场，要求做机房级实战故障切换。第四是运维能力。系统下移到通用服务器并实现去 IOE，数据库节点数量要出现 50 倍的增长。以我们的一个银行客户举例，从小型机下移后，数据库节点数从 20 增长到 1000（当然这里面也预留了几倍的性能增量）。在和客户的交流过程中，他们也认同这个观点，认为系统下移后，节点数要增长一到两个数量级。但运维的人力不可能因此增加几十倍，在这种情况下，就要求我们的运维效率要有质的提升，需要能够智能化、自动化去做相关的运维工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e70f9a0a35e389e4bda5d5de854c137.png" /></p><p></p><p></p><h2>二、分布式数据库&nbsp;GaiaDB-X 的金融场景方案</h2><p></p><p></p><p>接下来我们分享第二部分，分布式数据库 <a href="https://xie.infoq.cn/article/1febbf974afe91b9a1e11517f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">GaiaDB-X</a>" 针对金融场景的解决方案。</p><p></p><p>GaiaDB-X 数据库是<a href="https://www.infoq.cn/article/WrlUWpf2OkgQsSAD6NJ1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"研发的 Shared Nothing 架构的分布式数据库，它可以基于通用服务器做横向扩展，来满足高性能、大数据容量的需求。</p><p></p><p>总体来看它分为计算层、存储层、元数据三层架构：</p><p></p><p>计算层是无状态、可横向扩展的一层。它对外兼容 MySQL 协议，接收到 SQL 请求后，再经过 SQL 解析、权限检查、逻辑优化、物理优化之后，生成 DistSQL 下发到各个存储层的分片。为了达到更好的性能，逻辑与物理上尽量将数据过滤及计算能力下推到存储层，收到结果后，再把数据进行聚合计算，最后返回给客户端。计算层的下面是存储层。它采用多分片的方式进行扩展，数据按照一定的分片规则打散到了各个分片中。我们支持 Hash、Range、List 等分区方式来做数据分区。同时，分片内数据采用多副本的方式来保证可靠性，第三是 GMS 节点，即全局元数据管理模块。它用来管理全局性数据，例如表的 Schema 信息、权限信息、表的路由信息，还有后面要介绍到的用于做分布式事务的全局逻辑序列号。GMS 也采用多副本的方式，采用 Raft 协议进行数据同步。</p><p></p><p>在最底层是我们的统一数据库管控平台，来实现对数据库集群的管理。比如在<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbeb6a36cc9ce320cf9c590dcbaa67d53421a43a56ea8378ce2b96f0c1c3c3f17265ea54d9b9&amp;idx=1&amp;mid=2247570425&amp;scene=27&amp;sn=bc5f1eeba437ce5e619ccef9bc0321d3&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">百度</a>"集团内部数十万的数据库节点，都是由该管控平台来管理的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50f58801a7c4eeb2bddfece8e453df59.png" /></p><p></p><p>GaiaDB-X 数据库是百度集团发展历史最久、应用最广泛的一款数据库，到现在为止已有 18 年的发展历史。它的发展也与百度的业务发展息息相关，大概可以归纳为四个阶段：</p><p></p><p>第一阶段是从 2005 年开始，为了满足搜索、社区业务中大量读请求的场景，我们通过一主多从的集群化外加读写分离来扩展读性能。第二阶段是为了支持凤巢广告系统与百度网盘，满足它们对万亿级数据量的需求，我们开始做分布式系统。到了 2014 年，我们就已经在凤巢广告系统中替换了基于 Oracle 的盘柜，为凤巢每年节省了上千万的成本。针对百度网盘，我们有个 3000 台服务器的集群支持网盘业务，所有网盘文件的元数据信息都存储在这里，最大的表达到万亿级记录数，至今仍是国内最大的关系型数据库集群之一。第三阶段是随着百度钱包等泛互联网业务的兴起，对数据的一致性要求更高了。因此，我们实现了分布式事务强一致的特性，保障金融数据的正确性。第四阶段，也就是现在。随着百度智能云对外技术输出，我们已经把数据库输出到了十余个行业，覆盖 150 家客户。在金融行业，GaiaDB-X 已经承接了金融核心业务，包括百信银行、银联商务、某交易所及国有行等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43f859c5ceefd365e4a3567012ce9242.png" /></p><p></p><p>对于分布式数据库，水平扩展能力是其核心能力。除了在计算层与存储层做水平扩展外，我们还要着力去解决影响我们扩展能力的单点。</p><p></p><p>第一个是 GMS，即全局元数据模块。因为它要提供一个全局递增的全局逻辑时钟，每一次事务都需要调用它。为了避免其成为瓶颈，我们采用批量预分配的方式来提升其总吞吐，在此模式下，其每秒可分配 1200 万个 TSO 序号，远超出百万级 TPS 的需求。</p><p></p><p>第二个是全局事务表。为了保证分布式事务的原子性，我们需要将正在进行中的事务保存到一张全局表中，因此它的性能也会直接影响到全局性能。我们采用自管理的方式，将全局事务表做成分布式表，分布式地存储在集群中，这样就可以实现分布式扩展。</p><p></p><p>在实际应用中，比如说像 19 年的春晚抢红包，我们支持了三亿用户抢红包，支撑了峰值达每秒 12 万交易量。除此之外，针对某银行拥有 8000 万账户的核心账务系统，我们也平稳支持了其 6 万每秒的 TPS ，充分验证了 GaiaDB-X 的水平扩展能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/560fef70a125720cb796e7ddf53eb3d3.png" /></p><p></p><p>除分布式外，我们也支持单机场景，实现了单机分布式一体化。为什么需要单机分布式一体化呢？以我们的一个银行客户来说，全行的业务系统有 200 多个，其中大部分系统（大概占 70% 左右）对性能与吞吐的要求并不高，一组单机数据库就能够满足其业务需求。但对于剩下的 30% 业务，它对性能的要求是单机数据库无法满足的，需要分布式数据库来满足其扩展性。</p><p></p><p>因此我们通过一体化的方式，来满足银行不同体量的业务对于数据库的需求。同时，我们也具备单机数据库扩展为分布式的能力，在对应业务的数据量增长后，能够扩容为分布式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15483b1c6cffdb73ad5178a621526e14.png" /></p><p></p><p>扩展性的另外一个目的是自动做数据分离。在金融场景里面，存在多个业务共用一个数据库集群的场景，比如业务分为联机交易系统与查询类系统，数据库便对应划分为交易库和历史库两个。</p><p></p><p>对于交易库来说，只保存联机交易会频繁使用到的数据。例如账务结果数据及当天的交易记录，以满足对交易业务的快速响应。对于查询不那么频繁的即时交易记录，这可能就是一个相当大的数据量，一般都能达到千亿甚至万亿级别。这时，我们就可以将这个数据自动转移到历史库上去，用更高密度的存储机型来存储。一方面可以降低硬件成本，同时也可以避免对联机业务的影响。</p><p></p><p>这样做对业务来说，对外呈现的是一套数据库，业务可以根据需要来处理不同数据分区的逻辑，也不用在多套库之间通过 DTS 做数据同步。同时还把交易数据和历史数据做分离，以保障对联机业务的性能，同时也满足了查询业务的查询需求，避免其相互影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09461237b2307a25d9e1a17349640b7b.png" /></p><p></p><p>在金融场景中，对事物的 ACID 特性有着严格的要求：</p><p></p><p>持久性。指的是事务一旦提交就不会丢失，一般通过多副本 + 强同步来解决。原子性。一个事务要么全部提交，要么全部回滚，不存在部分提交的情况。通常，数据库的 XA 协议，通过两阶段提交能解决这个问题。</p><p></p><p>但是 XA 协议不能很好地满足一致性与隔离性。以简单的转账场景为例，A、B 原来各有 100 块钱，总共 200 块。然后 A 给 B 转账 50 块钱。此时，我们会先给 A 扣 50，再给 B 加 50。如果通过 XA 协议来做的话，先走 prepare 再 commit，我们可以看到，commit（图中第 7、第 8 步）过程不是原子过程，存在一个执行时间差。在这个时间差内，另外一个事务去读取数据，就可能存在读到提交了一半的数据，A 和 B 的总和是 150 而不是 200。这是 XA + 2PC 解决不了的问题。</p><p></p><p>为了解决这个问题，业内一般是引入一个全局时钟来做一致性的保证。通常有三种实现方式：</p><p></p><p>TrueTime 方案。这个是大家比较熟悉的方案，Google Spanner 也发过论文。但它的缺陷是依赖硬件，要引入 GPS 与原子钟，这个一般来说是难具备的。HLC 方案。采用该方案的典型数据库系统是 CockroachDB，它的优点是不依赖硬件，而且是去中心化的。但是缺点也很明显，一致性比较弱，而且要求服务器间的时钟误差不能超过 250ms，否则就无法正常运行。TSO 方案，比如 TiDB 就是采用了这种方案。TSO 本质上来说是一个全局唯一而且自增的逻辑序列号，一个事务在开始时与事务 commit 时需要两次找 GMS 获取序列号，然后把 TSO 号作为版本号提交到存储层，存储层的 MVCC 机制来判断数据是否可见。它不需要硬件具备强一致性，但缺点是依赖一个全局中心的时钟分配器 GMS。但这并不是一个问题，因为刚刚我们也提到了，虽然 GMS 不具备扩展性，1200 万的 TPS 已经完全满足业务的常规需要了。因此我们最终采用了这种方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b192d1774be97ac09d153b3e3fcf81d9.png" /></p><p></p><p>除了保障事务的一致性外，我们还需要保障上下游系统的数据一致性。在开始之前，我们首先要讲一下银行的典型业务场景，它一般分为三个子系统：</p><p></p><p>第一个是联机交易系统，例如存取款、在线支付等。这个系统的并发量高、延迟敏感，直接影响到用户体验。第二个是跑批类的业务系统。例如结息，每天晚上半夜计算前一天的利息。这些业务是后台业务，有大量的读取与计算操作，延迟不敏感，但是对数据一致性要求高。怎么能够让这样的业务尽量避免对在线业务的影响，同时又能够读取到最新的数据呢？我们的方式是让跑批业务去读取从库数据，从而避免对主库的性能影响，同时结合 TSO，即全局逻辑时钟的对位对齐机制。等到从库水位追齐之后，才返回读取数据。当然这会引入一定的延时，但是因为跑批业务对响应延时不那么敏感，所以是可以接受的。第三个子系统是大数据类的离线业务。通常来说，就是银行内部的各种大数据、数仓等系统，需要把数据库的数据同步过去。这样的业务对实时性要求不高，但要求最终的数据是一致的。由于各个计算节点都会承担流量，也会生成 BinLog。因此，如何对多份 BinLog 进行排序，让它们能够严格保持时序是我们需要解决的问题。我们的全局 BinLog 有两个模块，一个是 pump，从各个CN 节点抽取 BinLog，然后在 Sorter 节点基于 TSO（全局逻辑时钟）进行排序，来保障形成一个全局时序正确的 BinLog，以保障这个离线系统收到的数据的最终正确性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17a48e22473b043b5053080788477dae.png" /></p><p></p><p>接下来我们看一下容灾能力，除了对单机故障的容灾之外，还有对特定机型的容灾。因为银行把系统下移到通用服务器，通常都是通过两阶段来实施：第一阶段是下移到 X86 的 CPU 芯片上，这个过程银行、互联网厂商都一定的经验。第二阶段是要实现服务器芯片的基本国产化，就是说使用国产的鲲鹏、飞腾或海光 CPU 芯片，这方面大家都是在探索性的开展相关业务。</p><p></p><p>以百信银行举例，它与其母行中信银行一样，选择了基于鲲鹏做国产化的路线，而且在业内比较超前。相较于其他银行仅在周边系统或者数据库从库来做国产化，百信银行的周边业务跟核心系统都和主站一样基于鲲鹏服务器来做，这个在业内是较为领先的。</p><p></p><p>为了保证客户业务系统实现平滑的国产化，我们在产品上实现了一库多芯的方案，主要资源池基于鲲鹏，但放置了一个独立 X86 资源池，在技术上实现托底，同时也能够将原有换下来的服务器能够利用上，避免资源浪费。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/173396e11937ad6b3f0e88e0a6e8fb31.png" /></p><p></p><p>根据人行的监管要求，银行核心系统一般要具备 5 级以上的容灾能力，这就要求具备两地三中心的容灾能力。</p><p></p><p>下图是我们客户的一个典型机房部署架构。首先在北京有两个同城机房，其物理距离在 50-100 公里左右，网路延迟在 1ms 左右。同时还有一个异地机房做灾备，物理距离一般是 1000 公里左右，比如合肥，网络延时是 10ms。</p><p></p><p>同城两个机房业务做双活部署，同时接受业务流量。数据库在两个机房采用 3 + 2 的部署形式，机房间采用强同步的方式来保障在发生机房级的故障之后，数据库能进行故障切换，而且保障数据的 RPO 为 0。</p><p></p><p>为保证单机房故障后的零数据丢失，我们采用分组强同步的方式，将 id1、id2 各划分一个复制组，复制组间采用强同步的方式。每个复制组保证至少有一个副本接收到数据之后才返回成功。这样在容忍少数副本故障的同时也能够保证单个机房故障后的零数据丢失。</p><p></p><p>异地机房的目标是当北京的两个机房都出现灾难性的事件之后，能够降级完成核心业务。它采用异步级联的方式来同步数据，首先异步是为了避免阻塞对主地域的数据库写入；采用级联方式，没有跟主地域机房形成复制组，主要一个为了保持灾备机房的数据库的拓扑独立性，减少依赖，保障在关键时刻可切换，另外也是降低跨地域同步的数据量，只需要同步一份数据即可。</p><p></p><p>结合在多家金融客户的实践，我们和中国信通院一起发布了金融数据库的容灾技术报告《金融级数据库容灾备份技术报告（2021 年）》。大家可以在公众号后台回复「金融级数据库容灾备份技术报告」获取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c35b7ca466e4e027b09f699a6c555063.png" /></p><p></p><p>最后一部分是运维能力。核心系统下移及国产化的背景之下，数据库系统呈现两个变化：</p><p></p><p>一是数据库的节点数出现了 50 倍的增长，这里面既有技术架构的原因，也有数据库预留了一定的性能空间的原因。</p><p></p><p>二是数据库的种类也变多了。对于银行系统来说，之前基本就是选择 Oracle 或 DB2。现在则开始使用多种开源数据库和国产数据库。除此之外，为了避免像之前一样被单一厂商绑定，银行也会特意选择多家数据库厂商，在这种情况下，对银行的运维的挑战就更大了。</p><p></p><p>因此，结合百度集团及百度智能云管理大规模数据库节点方面的经验，我们将 GaiaDB-X 数据库云管平台进一步泛化，形成了具备管理多元数据库能力的统一平台，它除了能够管理 GaiaDB-X 自身，也能管理其他的开源数据库。通过帮助银行建立企业级的 DBPaaS 管理平台，进一步提升了银行的运维效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5b5b67c12e920a404b8a06be4702a30.png" /></p><p></p><p></p><h2>三、金融应用案例介绍</h2><p></p><p></p><p>接下来，我来分享百度智能云在金融方面的一些典型案例。</p><p></p><p>首先是百信银行。它的特点是完全去 O，是一家完全没有 Oracle 的银行。全行 200+ 业务系统，无论是核心账务系统还是周边系统，几乎全部是基于 GaiaDB-X 数据库来构建的，至今已经平稳运行五年。</p><p></p><p>按数据库节点数计算，百信银行目前的数据库国产化率达到了 99.93%，遥遥领先于行业平均水平。</p><p></p><p>同时，百信银行在容灾和国产化领域也比较领先，在 2019 年就完成了全行主要系统的同城双活建设，2022 年开始将全行业务逐步迁移到基于鲲鹏的国产云上，进而实现了全栈的国产化。</p><p></p><p>在硬件成本方面，我们通过采用通用服务器来替代 IOE 硬件，帮助百信银行的单账户平均硬件成本降低了 70% 以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5eba560cf050d18ba1d18d015ebab4c0.png" /></p><p></p><p>下图是人行下面直属的某交易所。因为是涉及到国家金融稳定，所以在核心系统上需要逐步摆脱对 Oracle 的依赖，并拥有两地三中心的容灾能力。</p><p></p><p>由于当前的一些数据库都不能满足他们的业务场景需求，因此该交易所和百度采用了联合开发的模式，共建数据库能力。在两年的合作过程中，我们从外围的信息管理系统入手，逐步深入到核心交易系统，再到离线库数据分析系统，进而逐步实现数据库国产化。</p><p></p><p>此外，由于交易所的交易系统对低延时的要求较高，同时基于容灾要求又有同城双活的要求，如何在跨机房的情况下保障交易延时就成了亟待解决的问题。因此我们共同建设了 Collocate 机制，来尽量减少跨机房数据的访问，最终将交易延时从 80 毫秒降低到了 15 毫秒。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3bbb84070b281ebbedf2e41e6edc456.png" /></p><p></p><p>下图是国内某国有大行客户。他们在最近两年把原有的基于小型机的核心系统下移到了通用服务器中，数据库也从 Oracle 替代为了开源单机数据库。</p><p></p><p>但在这个过程中，该行面临两个问题：一是数据库节点数增长了 50 倍，服务器数量到达了 1000 台左右，如何做好数据库的自动化部署、上线变更、版本升级、参数管理、性能诊断等工作。二是如何结合业务的单元化，形成业务与数据库的同城双活与异地容灾能力。</p><p></p><p>借助百度智能云提供的统一数据库管控平台的能力，历时两年，我们与客户一起实现了新核心系统的全面投产，也顺利通过了人行的验收。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b935db0fd9ff535ed78234e80ac12688.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</id>
            <title>深度解读“百度智能云数据库”的演进：互联网→云计算→ AI 原生</title>
            <link>https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:50:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, AIGC, 数据库发展, AI时代
<br>
<br>
总结: ChatGPT的爆火引发了AIGC赛道的关注，展示了弱人工智能向强人工智能的跨越式发展。AIGC将改变数据库行业，AI时代将带来数据库的变革和创新。 </div>
                        <hr>
                    
                    <p></p><h2>一、数据库行业发展概述</h2><p></p><p></p><p>如果说今年科技圈什么最火，我估计大家会毫不犹豫选择 ChatGPT。ChatGPT 是 2022 年 11 月 30 日由 OpenAI 发布的聊天应用。它创造了有史以来用户增长最快的纪录：自 11 月 30 日发布起，5 天就拥有了 100 万活跃用户，两个月就达到了一亿用户。对比其他热门应用，同样达到一亿用户量级，TikTok 花了九个月，而像 Instagram ，Whatsapp 等应用则超过了两年时间。</p><p></p><p>ChatGPT 的爆火，瞬间点燃了整个 AIGC 赛道。最关键的原因在于，它让大家看到了弱人工智能向强人工智能的跨越式发展。英伟达 CEO 黄仁勋对此评价：ChatGPT 相当于 AI 界的 iPhone 时刻。</p><p></p><p>现在业界统一的共识是，AIGC 会改变 IT 行业的方方面面。那 AIGC 对数据库会带来哪些变化，AIGC 和数据库又会碰撞出哪些火花，这是一个值得我们去思考和回答的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/648f846dd8384dfa9a1fc1a1ea4abf3e.png" /></p><p></p><p>在回答 AIGC 对数据库的变革和影响之前，让我们先回顾下数据库发展历史。它可以分为六个阶段。</p><p></p><p>第一阶段是上世纪五十年代。这个时候数据库还在雏形阶段，以层状数据库和网状数据库为主，基础设施以大型机为主，主要用于国防和科学研究。</p><p></p><p>第二阶段是上世纪七十年代。关系型数据库出现，硬件也变成了小型机，这也奠定了数据库发展的方向。主要应用在金融，交通等关键行业。这时的代表数据库是 Oracle 和 DB2 等。</p><p></p><p>第三阶段是上世纪九十年代。PC 机已经得到了普及，数据库除了关系型数据库，也有了 PC 单机数据库。为解决企业 BI 应用诉求，数仓开始出现。数据库的应用也更多样化起来，进一步应用到企业 BI、个人办公、娱乐等场景。</p><p></p><p>第四阶段是本世纪的前十年。随着互联网开始繁荣，数据处理的需求逐渐增加，开始出现企业数据中心。业务也变成了媒体、搜索、电子商务、社交等互联网业务。由于传统数据库如 Oracle 因为价格较贵，互联网厂商大量使用开源数据库如MySQL、Redis、MongoDB 等。整个开源数据库生态开始逐渐繁荣。数据库的种类，厂家也逐渐变多。</p><p></p><p>第五阶段就是我们今天所处的云计算时代。典型应用包括新媒体、各种移动 APP、物联网、娱乐、短视频等。典型的数据库有 RDS、Aurora 等云数据库，以及 Oceanbase、CockroachDB 等分布式数据库。百度也有对应的产品，云原生数据库 GaiaDB 以及我们自研的缓存类数据库&nbsp;PegaDB 等。</p><p></p><p>第六个阶段是自 2023 年开始的 AI 时代。底层基础设施变成了 GPU 和 AI 能力。应用也变成了 AI 原生应用，如海外比较火的 Jasper、Midjourney，微软的 Copilot 等。在数据库行业我们看到至少两个方向，一个是 AI4DB，其中包括阿里的 DAS、百度的 DSC 等，主要是通过 AI 的能力去改进原有数据库的自动化能力。另外一个方向就是 DB4AI，目前主要是向量数据库。向量数据库在解决大模型幻觉等方面，有非常不错的效果，是一个有潜力的细分赛道，头部公司估值已经达到 10 亿美元。</p><p></p><p>以上就是数据库 70 年波澜壮阔的发展史。我们可以看到，每隔一段时间数据库就会在基础设施、应用场景、以及数据库本身，都有不断地变更和创新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92a9b7ae192ebfb33a9091bbf57857a1.png" /></p><p></p><p>上面我们简单回顾了数据库发展的六个阶段。在这个过程中，我们还可以以 2000年做分界线。在 2000 年前，国内数据库基本上被 Oracle 等海外数据库主导。而从 2000 年之后，随着互联网业务的发展，国内多个互联网厂商如阿里、腾讯、百度便开始尝试使用开源数据库，实现了从最早的运维、到提交 patch、再到最后完全自研数据库的跨越式发展。</p><p></p><p>这背后从量变到质变的过程是一个典型基础软件发展过程。</p><p></p><p>一个基础软件真正得到长足发展，需要一大批高素质的技术人员，也需要深度场景的使用才能不断完善产品。另外丰富的场景和不断发展的业务，也能长期养活这批技术人员，进而形成正循环。所以说数据库的发展依赖于技术和业务的双轮驱动。</p><p></p><p>从 2000 年开始，我们看到三波浪潮——互联网，云计算和 AI 原生。我们接下来会分别来讲一下每一波浪潮为数据库行业带来的创新和变化，以及百度智能云数据库在这个过程中的关键技术和代表产品。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f647b07b8603129ba05898417ae77eb5.png" /></p><p></p><h2>二、百度智能云数据库发展史</h2><p></p><p></p><p>互联网业务特点是赢家通吃，所以互联网业务用户数规模通常比较大。因此天然要求数据库支持大规模、高可用、高可靠性、低成本以及高性能，这对数据库提出了非常大的挑战。</p><p></p><p>在第一波互联网业务的发展中，业务的挑战催熟了一系列开源数据库如 MySQL、Redis、MongoDB，又从中孵化出了分布式数据库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20e33da0350e46cb356adc79012f78ed.png" /></p><p></p><p>接下来我们来看下百度在互联网时代的数据库发展历程，这里有几个关键节点：</p><p></p><p>第一个是自 2005 年开始使用 MySQL 数据库，这也是国内最早使用 MySQL 的企业之一。</p><p></p><p>第二个是 2014 年百度推出公有云服务，百度数据库的能力通过<a href="https://www.infoq.cn/article/WrlUWpf2OkgQsSAD6NJ1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"开始赋能给外部企业。</p><p></p><p>第三个是 2020 年发布了云原生数据库 <a href="https://xie.infoq.cn/article/61a867abe6d45fa9f1fe644d0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">GaiaDB</a>"。百度也成为了国内少数几个具备自研云原生数据库云厂商之一。</p><p></p><p>截至目前，百度积累了 18 年的数据库研发经验，承载着内部 PB 级数据。10 万+ 的节点至今零故障零损失。</p><p></p><p>通过百度智能云输出的一站式产品，覆盖 RDS、NoSQL、OLAP、工具等领域，同时具备公共云、私有云、边缘云等软件版本多形态。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f0fe561b919a46ca824f31f4f556ce5.png" /></p><p></p><p>前面我们提到了互联网的一大特点，就是规模大。单点肯定处理不了，所以需要引入分布式技术，也催生了分布式数据库的诞生。</p><p></p><p>百度在该领域也有非常成熟的技术，讲两个实际的案例：</p><p></p><p>第一个是<a href="https://www.infoq.cn/article/2012/03/baidu-bae?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度网盘</a>"。百度网盘有 8 亿用户，整个数据库中单表最大超过 10 万亿条记录。整体集群超过 3000 台服务器，是国内最大的数据库集群之一。</p><p></p><p>第二个是金融行业。大家都知道金融行业对一致性、数据准确性有非常高的要求。度小满金融有 3 亿用户，年度结算金额超过万亿，其底层使用的就是百度智能云分布式数据库 GaiaDB-X。</p><p></p><p>尤其值得一提的是在 2019 年春晚红包业务中，整体交易的峰值是 12 万笔/秒。数据库的分布式能力、性能、一致性、准确性都得到了充分验证。</p><p></p><p>除了度小满，百度智能云的数据库还在多家国有大行、股份制银行和城商行中稳定运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72d4145b2e5cf4609d5969fcd0c828a2.png" /></p><p></p><p>互联网业务除了规模外，对性能、并发等也提出了很高的要求，因此诞生了一系列 NoSQL 数据库。不同的 NoSQL 数据库从不同层面解决互联网垂直场景的问题，今天我们讲其中的代表 Redis。</p><p></p><p>百度智能云的 Redis 服务经历十几年的技术积累和业务打磨。从规模上来看，节点规模超过 30w，其中单集群最大规模节点数达到 2700。从业务支持上看，百度 Redis 覆盖支撑了百度内部全场景业务，其中包括搜索广告、手百、地图、小度等一系列亿级用户体量的产品，为业务提供 4 个 9 以上高可用性以及微秒级请求时延服务，始终为客户提供稳定、高效、弹性可扩展的智能缓存服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/8471c3e34b3cef8ce9d5754cd03b193c.png" /></p><p></p><p>Redis 直接使用内存，但内存带来高性能的同时成本是比较贵的。因此一款能兼顾性能和成本的 Redis 产品是客户迫切需要的。考虑到业务中大量的数据是可以根据场景分出冷热的。比如视频直播、新闻/内容平台、电商场景中，随着时间的推移，数据的价值和使用频率都在下降。所以可以将部分数据自动迁移到磁盘中，从而降低存储的整体成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8240196a262dd1948b2f1a0a9babe152.png" /></p><p></p><p>为了解决性能和成本的平衡问题，百度智能云自研了 PegaDB。PegaDB 是在开源基础上自研的容量型 Redis 产品，相比内存型产品最多节省超过 90% 的存储成本。在成本下降的同时，PegaDB 也兼容了 Redis 丰富的数据类型和命令，让用户做到无缝迁移，兼顾了用户体验和性能优势。</p><p></p><p>除此之外，PegaDB 还有两个杀手锏功能：</p><p></p><p>一是支持在线弹性伸缩，单个集群最大规模可达 PB 级别。对用户来说不用估计使用量，只要傻瓜式即开即用即可。</p><p></p><p>第二个是支持 CRDT 同步的组件，支持异地多活和多节点同时访问、自动进行冲突合并等功能。这就让客户专注于实现业务逻辑，其他的都交给底层的数据库，完全不用操心可用性问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8247c1d27f22bf8d13326b6df3fa1270.png" /></p><p></p><p>随着云业务的诞生，让数据库的价值进一步放大。为了赋能千行百业，全托管等形态的 RDS 顺利成章的诞生了。它解决了客户最直接的安装、运维、管理等问题，因此全托管的 RDS 就逐渐推广开来。</p><p></p><p>但单体 RDS 通常有比较明显上限，在一些对性能、成本、弹性有一定要求的复杂业务中，就需要一个更强大的数据库来解决这些问题。因此，存算分离的云原生数据库就自然而然诞生了。百度智能云的云原生数据库 GaiaDB 是其中的代表之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e835b194141802f859a373b37363990.png" /></p><p></p><p>RDS 全托管的产品形态代表了云计算从软件到服务的理念转变。云原生数据库极大地提高了 MySQL 数据库的上限能力，是云数据库划代的产品。</p><p></p><p>云原生数据库最早的产品是 AWS 的 Aurora。AWS Aurora 提出来的 The log is the database 的理念，通过把大量的日志操作放到后台异步处理，实现了存储独立扩展和存储计算分离，从而解决了 MySQL 数据库单库的数据量不能太大的最大痛点。</p><p></p><p>而云原生数据库在存储层面实现了扩展的同时，又保留了计算层面的不变和兼容。这种兼容 + 扩展的能力，受到了客户的极大欢迎，一下子就让云原生数据库成为各个厂商的发展重点。云数据库技术也标志着云厂商的产品能力开始和传统数据库厂商、开源产品开始拉开差距。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd48508403406795d06e50dffdebbe2f.png" /></p><p></p><p>百度智能云的 GaiaDB 在 2020 年首次推出，除了具备云数据库的优点之外，GaiaDB 还有很多独特的技术能力，接下来我来分享其中 5 个代表能力：</p><p></p><p>第一个是共识协议。一般使用 Raft/Paxos 分布式协议的数据库，单次 I/O 需要至少两次网络往返，而且无法并行。这也就导致了分布式数据库时延很高，长尾问题更突出。</p><p></p><p>针对这个问题 GaiaDB 创新采用了 Raft 和 Quroum 结合的协议。其中 Raft 负责控制流，Quorum 负责数据流，进而减少网络往返。同时核心链路上的同步 I/O 变成异步 I/O，在保证分布式一致性的前提下，吞吐提升了 40%，时延降低了 30%。</p><p></p><p>第二个是高性能智能网络。存算分离在带来分布式和弹性的同时，也引入了网络 I/O 的消耗，因此网络 I/O 的性能和效率直接影响整个系统的表现。GaiaDB 采用高性能智能网络，这个网络有几个关键技术能力：</p><p></p><p>网络超时重定向机制。当远程 I/O 超时，会自动尝试其他副本，从而抑制单节点长尾问题。网络支持用户态协议。该协议减少了内核态 TCP 和用户态 TCP 的数据库拷贝。通过对网络的优化，平均时延从毫秒级别降低到微秒级别，提升 20 倍以上。</p><p></p><p>第三个是提供了三副本对等存储能力。由于采用了 Quorum 分布式共识协议，相比传统的 Raft 模型，每个节点都可以独立提供读写服务，没有单点故障。</p><p></p><p>第四个是多地多活。GaiaDB 是目前业界唯一可以做到多地多活的云原生数据库。在多地部署的时候，GaiaDB 模块的自适应就近访问策略可以感知元数据的变化，并根据这些变化及时切换访问路线。这种策略可以有效地应对各种故障和异常情况，确保数据的可靠性和可用性。</p><p></p><p>第五个是使用通用硬件，对硬件要求低。GaiaDB 生于云，但同时 GaiaDB 的架构对硬件的依赖度非常低。我们和很多厂商使用高性能硬件的思路不同，我们认为云的价值是普惠，所以一定要让通用服务器能发挥专业数据库的能力。因此，不同于很多云原生数据库需要依赖底层高性能的硬件，GaiaDB 从设计初就坚持使用通用服务器。因此在私有云场景下，三个节点就可以进行部署，让我们的客户可以低价享受到云上云下一套架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b7c7ed3a0879eebe3cc8ba586055484.png" /></p><p></p><p>接下来我们来看一个 GaiaDB 的实际案例——百度地图。</p><p></p><p>百度地图是国民级别应用，日活用户 5.6 亿，PB 级数据。这对数据库也提出了如下的挑战：</p><p></p><p>为了保证高可用，需要多地多活的能力。节假日地图搜索，导航流量会出现十倍的上涨。这就要求在节假日需要非常顺滑的扩缩容的能力。</p><p></p><p>大规模数据量、异地多活、弹性扩缩容要求，这些要求对数据库是极大的考验。</p><p></p><p>在实际使用过程中，GaiaDB 提供 4 个 9 的可用性，RTO 切换小于 3s，RPO=0，整体 QPS 超过百万级别，给业务实现超过 60% 的资源成本节省。</p><p></p><p>总的来说，GaiaDB 成功帮助百度地图实现了极致的弹性和成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f02e4b25fba325d4854e72f711bde58.png" /></p><p></p><p>云上数据库和线下数据库相比，一个较大差异就是生态能力强。相比传统线下软件只有 1~2 款产品，线上有多种数据库与多种使用环境，因此数据库矩阵更丰富，这带来了对数据库工具的诉求。</p><p></p><p>百度智能云有丰富的数据库工具，包括数据传输 DTS、数据库智能驾驶舱 DSC 等产品。我们先讲其中的代表 DTS。</p><p></p><p>百度智能云的 DTS 采取了中间抽象的数据格式，通过中间格式的翻译和转换，可以轻松做到异构迁移能力。同时 DTS 在吞吐上可以做到每秒 15 万行，延迟做到毫秒级别，基本等于网络的延迟的性能，让客户可以放心使用 DTS 来做数据库的迁移和同步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e0d44ff1229d468f704af06b132010e.png" /></p><p></p><h2>三、AI 原生时代的百度智能云数据库</h2><p></p><p></p><p>在 AI 原生时代，数据库和 AI 的结合主要有 DB4AI 和 AI4DB。</p><p></p><p>首先是 AI4DB，就是利用 AI 技术赋能数据库。常见场景有智能运维、智能客服、参数优化等等，刚刚提到的百度智能驾驶舱就是该领域的代表。</p><p></p><p>另外一个方向是 DB4AI，通过数据库赋能 AI 产品。当前最火的就是向量数据库。向量数据库二次的翻红主要原因是向量数据库在解决大模型幻觉、知识更新不及时有很大作用，让向量数据库的想象空间一下子变大了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/096c6ccafce7598fdac1f73dfb471e4f.png" /></p><p></p><p>AI4DB 在工业界一直有研究。相比传统机器学习算法，大模型让 AI4DB 真正走进实用时代。利用大模型的能力，百度智能云数据库发布新服务：数据库智能驾驶舱。</p><p></p><p>数据库智能驾驶舱利用最新的大模型能力，实现数据库智能化的洞察、评估和优化。根据我们的实际测试效果，优化效果非常显著：</p><p></p><p>数据库故障洞察方面，相比传统的人工定位提升 80%。领先的智能评估系统，相比传统的方法提前一个月发现数据库的容量瓶颈，规避相应的风险。AI 驱动的 SQL 优化方面，可以带来 40% 以上的提升。</p><p></p><p>相比传统基于规则的算法，大模型带来了更好的优化效果和更少的开发时间。大模型带来的切实提升让 AI4DB 走向真正的实用时代，也让数据库自感知、自修复、自优化、自运维成为现实。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9cd6b1eab7842cbdba2f1e03555c143.png" /></p><p></p><p>下面我们来看下数据库智能驾驶舱内置的一个能力——智能问答。</p><p></p><p>这个功能可以帮助用户诊断产品问题并回答各种疑问，降低人工投入。这里面用到了大模型通用知识的能力，同时也利用 RAG 技术，把云产品文档、数据库的官方文档、内部积累的知识库进行向量化并存在向量数据库中。</p><p></p><p>在查询的时候，结合大模型和向量数据库的能力，可以给出相当准确有效的答案。</p><p></p><p>目前数据库智能驾驶舱经过验证，对历史客户工单中真实问题进行回答然后由人工进行打分，整体回复平均超过 4 分，基本可以媲美普通售后工程师的水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79b670cada64634f9bf5577ab757d963.png" /></p><p></p><p>接下来我们实际来看下智能问答的一个 demo。</p><p></p><p>左边的例子是询问知识库里面已有的例子，比如怎么购买，怎么实现一个读写分离的配置等。智能驾驶舱都总结得比较好，回答也非常准确。</p><p></p><p>右边的例子是询问知识库中没有的例子。我们可以发现，智能驾驶舱利用大模型的能力，可以举一反三，把解决问题的步骤给出来。我们人工去检查也会发现，这个步骤还是相对比较合理的。</p><p></p><p>所以现在智能驾驶舱的智能问答可以做到：有资料的问题准确回答，无资料的问题也可以给出相对清晰的解法。百度智能云内部已上线了该功能，大大节省了人力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48ba6c7615d86cfd011247f99b44be3e.png" /></p><p></p><p>DB4AI 的典型代表就是向量数据库。向量检索并不是一个新技术，2017年 Meta 就开源了相似度检索库 FAISS，算是向量化检索的开山鼻祖。</p><p></p><p>传统数据库解决的是结构化数据的存储和检索，非结构化数据需要先用 AI 算法 Embedding 成向量数据。需要查找的时候，把需要查找的数据的向量带过来，然后在库里面进行相似度检索。</p><p></p><p>而向量数据库核心能力就是支持向量数据存储，以及支持不同的查找算法和索引实现相识度查找。当前业界有两种不同的实现方式，一种是在传统数据库中增加插件或者功能支持向量的查找，比如 PG，Redis 都支持向量索引。这种实现相对来说容易一些，但同时性价比会差一些，通常会占用更多内存。另外一种是专业的向量数据库，专门为向量重新设计的存储和索引结构，能实现更高的性价比和弹性。</p><p></p><p>传统应用也有不少向量场景。典型场景有平安城市视频检索、电商领域以图搜图等。由于传统场景比较垂直，因此一直没有一个大的向量数据库，更多的是耦合在业务系统中。而在大模型时代，万物皆可向量化。而且当前大模型主要问题有知识更新不及时、精确性问题、数据权限管理等问题，都需要向量数据库来补充。向量数据库也因此成为大模型的标配，也在大模型时代二次翻红。</p><p></p><p>百度智能云自研的专业向量数据库目前在内测阶段，根据我们内部实际测算，在成本、规模、高性能算法、内置 Embedding 模型、向量 + 标量的联合查询方面，相比业界有很大的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b9f6fcb6d97c8979c5bbbdfe17c3e09.png" /></p><p></p><p>前面我们介绍了关键的产品，最后简单回顾一下百度智能云产品矩阵。</p><p></p><p>百度智能云数据库完整支持 RDS、NoSQL、云原生数据库，OLAP 等产品。相比业界其他云厂商，百度智能云数据库有两个显著特点：</p><p></p><p>百度智能云的数据库产品可以做到一套架构，云上云下客户享受同等的产品能力。支持国内最全的产品形态，包括公共云、私有云、边缘节点、LCC 等多种形态，可以服务各类诉求的客户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc846845a37a6dac8eac882ad434be70.png" /></p><p></p><p>前面我们盘点了数据库在互联网、云计算，AI 原生 3 个阶段的发展。除了技术之外，我们认为云数据库未来还要坚持两个重要的理念。</p><p></p><p>第一个是体验优先。一个好的数据库不能只是性能、成本这些方面。体验好的产品，可以让用户做到自服务。体验优先这一点在海外 SaaS&nbsp;产品中体现得更为明显。在国内，这一理念也逐渐取得从业者的认可。因此，在过去的半年里面，我们从文档、控制台、产品功能各个层面进行了深度优化：</p><p></p><p>文档：文档是用户使用和理解产品的重中之重，因此我们做了包括优化结构、补充用户场景、刷新细小的优化点在内的大量工作，目的就是让用户在使用过程中可以更方便找到自己所需要的内容。控制台：在控制台优化上，我们优化了整体结构，让用户可以更简单找到想用的功能，总共优化点超过 100 处，让用户更容易上手。产品功能：我们针对数据库的产品功能系统性安排测试定期的盲测、新员工使用等，仅仅上半年就优化了 50+ 个突出的易用性问题。</p><p></p><p>我们对体验的理解就是从用户视角入手、坚持细节、系统性的进行优化，只有通过这种深度，全方位的持续改进，才能把体验做到实处。</p><p></p><p>第二个是开放生态。丰富的生态是吸引客户、解决客户多样诉求的关键。也只有开放的生态，才能让更多的厂商一起服务好客户。</p><p></p><p>生态方面，百度秉承更开放的心态和第三方厂商合作。上半年我们和工具领域知名创业公司 NineData 正式合作，接下来会马上官宣另外一个合作厂商。</p><p></p><p>相比其他厂商，我们合作的过程也不只是简单的云市场合作。我们会和合作伙伴一起进行产品共建、优先推荐合适客户给合作伙伴、首页曝光和联合的品牌活动，增加合作方的知名度。</p><p></p><p>通过一系列的手段和措施，我们希望给到合作伙伴的是切实效果。百度智能云合作的理念就是更开放，让利合作伙伴。欢迎更多的合作伙伴和百度联系，一起服务好我们的客户。</p><p></p><p>总的来说，一个体验优先，生态开放的云，一定是客户最需要的云，也是真诚服务客户的云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/dfc3dc9e74526dc0f5a93b8a4fa32c72.png" /></p><p></p><h2>四、数据库未来的趋势展望</h2><p></p><p></p><p>站在当前看未来，数据库当前有四个关键发展趋势</p><p></p><p>AI Native。像大家比较头疼的 Oracle 转 MySQL 或者 PG，随着 AI 改写的到来，整个过程预计会变得很简单。Serverless。已经是海外云数据库的默认选项了，预计 1~2 年之后，serverless 就会在国内变得更普及。各个厂商也都会推出 serverless 数据库产品，这也是未来云产品的终极形态。内置 HTAP。HTAP 前段时间很火，不过我们判断 HTAP 很难成为一个单独的赛道，更多的是会成为各个 TP 数据库的内置能力。湖仓一体。湖仓一体预计会成为数据仓库的主要形态，不支持湖的数仓可能会很难生存，只有支持湖才能解决更多的数据问题，才能降低存储的成本。</p><p></p><p>技术和产业发展都很快，百度智能云数据库持续跟进最新的技术趋势，用优质的产品和真诚的服务回报我们的客户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f7dafca36b251342c7b9668e005b1a4.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</id>
            <title>网易杭州研究院 / 编程语言实验室 / 负责人张炜昕博士确认出席 QCon 上海，分享低代码编程语言 NASL 从设计到落地的闯关之路</title>
            <link>https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 低代码编程语言 NASL, 张炜昕博士, CodeWave 智能开发平台
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，张炜昕博士将分享关于低代码编程语言NASL的设计和实现挑战，以及CodeWave智能开发平台的应用。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。网易杭州研究院 / 编程语言实验室 / 负责人张炜昕博士将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5642?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">低代码编程语言 NASL 从设计到落地的闯关之路</a>"》主题分享，探讨 NASL 语言的设计初衷、设计原则、实现挑战、未来展望等方面。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5642?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">张炜昕博士</a>"，香港大学博士，布里斯托大学 Senior Research Associate，长期从事编程语言研究。现为网易杭州研究院编程语言实验室负责人，主导 CodeWave 智能开发平台编程语言 NASL 的设计。以第一作者身份在 TOPLAS，ECOOP 等编程语言期刊和会议上发表论文多篇，并获 ECOOP“杰出软件制品奖”和 Programming 期刊“编委会选择奖”。曾任 Scala 研讨会主席，IFL 程序委员，PLDI、OOPSLA 软件制品审查委员，以及多个编程语言会议的审稿人。他在本次会议的演讲内容如下：</p><p></p><p>演讲：低代码编程语言 NASL 从设计到落地的闯关之路</p><p></p><p>NASL 是由网易自研的全栈可视化编程语言，是支撑网易数帆 CodeWave 智能开发平台的基石。本次演讲将围绕 NASL 语言的设计初衷、设计原则、实现挑战、未来展望等方面展开。</p><p></p><p>演讲提纲：</p><p></p><p>NASL 的设计初衷</p><p>○ 为什么低代码平台需要编程语言</p><p>○ CodeWave 及 NASL 的简介</p><p>NASL 的设计原则</p><p>○ 低门槛、高上限</p><p>○ 记号的认知维度</p><p>○ 编程系统的技术维度</p><p>NASL 的实现挑战</p><p>○ 如何融合企业的 IT 资产</p><p>○ 如何降低实现成本</p><p>NASL 的未来展望</p><p>○ LLM 时代的编程语言设计</p><p>○ 文本语法和标准化</p><p></p><p>听众收益点：</p><p></p><p>○ 可视化编程语言和编程系统的设计原则</p><p>○ 降低编程语言实现成本的方法</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 5 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</id>
            <title>南京大外企将研发撤离中国，最高赔偿N+8；OpenAI回应GPT-4变懒；周星驰Web3团队下月上线独立App | AI一周资讯</title>
            <link>https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图森未来, OpenAI, GPT-4, Gemini, 夸克大模型
<br>
<br>
总结: 图森未来计划裁减75%在美员工，OpenAI的GPT-4遭到用户投诉，Gemini的性能被指控“造假”，阿里的夸克大模型通过备案，快手进行了年内最大规模的组织架构调整。 </div>
                        <hr>
                    
                    <p></p><blockquote>“自动驾驶卡车第一股”图森未来缩减美国业务，拟裁减75%在美员工；商汤科技 AI 编程助手“代码小浣熊 Raccoon”开放公测；快手开启年内最大规模组织架构调整，涉主站、电商等多个业务线……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>OpenAI回应GPT-4变懒</h4><p></p><p></p><p>OpenAI 的 GPT-4 大语言模型日前遭到部分用户投诉，部分用户表示，这段时间使用 ChatGPT 或 GPT-4 API 时，会遇到高峰期速度非常慢、敷衍回答、拒绝回答、中断会话等一系列问题。</p><p></p><p>北京时间周五中午，ChatGPT 官方通过 X 平台通知用户，“我们听到了你们关于 GPT-4 变得越来越懒的反馈！我们自 11 月 11 日起就没有更新过模型了，当然这不是故意的。”</p><p></p><h4>微软与OpenAI合作面临英国审查</h4><p></p><p></p><p>12月9日消息，当地时间周五英国监管机构英国竞争与市场管理局（CMA）表示，正在就微软与ChatGPT开发商OpenAI之间的合作关系进行评估，看是否有必要进一步展开反垄断调查。</p><p></p><h4>马斯克：Grok AI测试版现已向美国所有X Premium+订阅者开放</h4><p></p><p></p><p>12月8日，埃隆·马斯克在社交媒体上发文称，Grok AI测试版现已向美国所有X Premium+订阅者开放。据悉，现有 X 平台用户可以每月花费 16 美元或每年 168 美元来进行订阅。</p><p></p><p>马斯克此前表示，Grok 使用来自公开数据的数十亿个数据点进行训练，但是目前尚不清楚使用了哪些数据。此外他还提到 Grok 将能够实时访问 X 平台，因此与其他生成式人工智能相比这是一个巨大的优势。</p><p></p><h4>谷歌承认Gemini演示视频经特殊剪辑处理</h4><p></p><p></p><p>美东时间12月6日，谷歌CEO桑达尔・皮查伊宣布迄今为止规模最大，能力最强的谷歌大模型Gemini 1.0 版正式上线。Gemini是原生多模态大模型，是谷歌大模型新时代的第一步，它包括三种量级：能力最强的 Gemini Ultra，适用于多任务的 Gemini Pro，以及适用于特定任务和端侧的 Gemini Nano。</p><p></p><p>不过，外界已开始有声音指控谷歌对Gemini的性能“造假”。彭博社一篇专栏文章就表示，谷歌在一段演示视频中歪曲了Gemini的AI性能。专栏作家帕米·奥尔森（Parmy Olson）认为，在谷歌发布的这段视频中，Gemini似乎非常强大，但有点过于强大了。对此质疑，谷歌回应时承认，这段关于Gemini性能演示的视频并不是实时的，而是使用了原始镜头中的静止图像帧，然后编写了文本提示，以便让Gemini做出回应。</p><p></p><h4>阿里夸克大模型已通过备案</h4><p></p><p></p><p>日前，阿里智能信息事业群自研的夸克大模型已通过备案，将陆续在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列AIGC创新应用。夸克相关负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。在搜索应用中，将通过图文多模理解、专业知识生成、交互方式创新进一步拓宽应用场景，提升用户体验。</p><p></p><h4>周星驰Web3团队下月上线独立App</h4><p></p><p></p><p>据新浪科技报道，12月7日下午消息，据接近周星驰团队人员对新浪科技透露，周星驰旗下Web3初创公司Moonbox 最早将于明年1月份完成上线Moonbox App，届时App将免费向用户开放。目前，App研发工作已经基本完成，Moonbox团队在 NFT 玩法上下了很多功夫，已设计出基于AI和NFT聊天的互动玩法。</p><p></p><p>据上述知情人士透露，伴随着Moonbox App的独立上线，“周星驰将以Moonbox First Creator身份与大家见面”。与此同时，周星驰参与创作的Nobody NFT新品，也将随之发售，用户可以通过App和每个Nobody NFT角色聊天互动以了解人物性格、爱好、背景故事。</p><p></p><h4>“自动驾驶卡车第一股”图森未来缩减美国业务，拟裁减75%在美员工</h4><p></p><p></p><p>近日，图森未来向美国证券交易会提交的一份报告显示，公司将裁撤150名在美员工，约为美国员工总数的75%，全球员工总数的19%。这是图森未来继去年12月和今年5月的裁员后，再一次进行人员削减。</p><p></p><p>图森未来预计，此次重组计划将产生约700万至800万美元费用，大部分用于支付遣散费、员工福利和相关费用，重组费用将在2023年第四季度入账。</p><p></p><p>据华尔街日报报道，本次裁员后，图森未来在美人数仅为30人，将负责图森未来美国业务的收尾工作，逐步出售公司在美资产，并且协助公司向亚太地区转移。因此，此次裁员意味着图森未来或将彻底退出美国市场。</p><p></p><h4>通义千问登顶HuggingFace开源大模型排行榜榜首</h4><p></p><p></p><p>12月8日消息，全球最大的开源大模型社区HuggingFace日前公布了最新的开源大模型排行榜，阿里云通义千问力压Llama2等国内外开源大模型登顶榜首。</p><p></p><p>HuggingFace的开源大模型排行榜（Open LLM Leaderboard）是目前大模型领域最具权威性的榜单，收录了全球上百个开源大模型，测试维度涵盖阅读理解、逻辑推理、数学计算、事实问答等六大评测。通义千问（Qwen-72B）表现抢眼，以73.6的综合得分在所有预训练模型中排名第一。</p><p></p><h4>商汤科技 AI 编程助手“代码小浣熊 Raccoon”开放公测</h4><p></p><p></p><p>12月7日，商汤科技官微宣布，基于商汤自研大语言模型的智能编程助手——代码小浣熊Raccoon，即日起开放公测。据介绍，在实际应用中，代码小浣熊可帮助开发者提升编程效率超50%；未来，应用代码小浣熊，开发者可以将80%的编写工作交由AI完成。</p><p></p><h4>特斯拉Dojo超算项目被曝更换负责人</h4><p></p><p></p><p>外媒援引知情人士消息称，特斯拉Dojo超级计算机的项目负责人Ganesh Venkataramanan已经于11月份离职。在过去五年中，Venkataramanan一直在领导Dojo项目的推进工作，加入特斯拉前他在AMD担任了近15年的长期工程总监。现在Dojo项目由Peter Bannon负责，Bannon已经在特斯拉担任高管近8年，之前还在苹果公司中任职超过7年。</p><p></p><h4>快手开启年内最大规模组织架构调整，涉主站、电商等多个业务线</h4><p></p><p></p><p>12月7日消息，快手发布内部邮件宣布新一轮组织调整。此次组织调整涉及主站、电商、商业化、杜区科学等多个业务线，属于今年以来最大范围的一次组织架构调整。其中，商业化事业部下本地消费业务部调整至主站线下，更名为招聘房产业务部，负责快聘、房产相关业务，取消主站产品部，主站线下成立孵化产品部，负责快影、一甜相机、回森等独立APP产品。</p><p></p><h4>南京大外企将研发撤离中国，裁员赔偿最高N+8</h4><p></p><p></p><p>近日，南京知名外企趋势科技计划搬离国内。知情人士透露，趋势科技打算将核心技术从国内转移到加拿大，因此裁员只涉及研发部门，其他部门几乎没有调整，共计约 70 人左右，赔偿 N+4 起步，一些老员工则超过 N+8。</p><p></p><p>据悉，该公司从上个月就开始裁员了，目前已接近尾声。值得一提的是，趋势科技本次撤离还会带走一部分员工，同意去加拿大的话也可以协调不裁。</p><p></p><h4>苹果因故意降低性能被判赔偿韩国7名用户每人7万韩元</h4><p></p><p></p><p>据韩联社消息，6日，韩国7名消费者集体起诉苹果通过升级系统降低旧款iPhone性能案二审宣判，法院判处原告部分胜诉。</p><p></p><p>据报道，韩国首尔高等法院民事当天开庭审理，判处苹果向原告每人支付7万韩元(约合人民币382元)赔偿金。</p><p></p><p>据悉，法院对原告所谓“苹果升级iOS系统属于发布恶意程序或损坏iPhone手机性能”的主张不予采纳，但法院认为，即使更新操作系统旨在防止手机自动关机，但也限制了中央处理器(CPU)等性能。苹果有义务向消费者说明是否安装更新，但苹果违反这一规定。同时，消费者因选择权被侵害而产生精神损失，认定苹果有赔偿责任。</p><p></p><h4>王慧文入股OneFlow团队新创业项目</h4><p></p><p></p><p>近日，北京硅动科技有限公司发生工商变更，新增王慧文为股东，同时注册资本由100万人民币增至约105.26万人民币。该公司成立于今年8月，法定代表人、执行董事、经理为OneFlow创始人袁进辉，公司经营范围含软件开发、技术进出口、电子产品销售、人工智能应用软件开发、人工智能通用应用系统、人工智能行业应用系统集成服务等。</p><p></p><p>公开信息显示，王慧文病休后，光年之外收购的核心团队OneFlow宣布重新创业。袁进辉称，新创业项目拟解决大模型推理成本问题。天眼查显示，目前，王慧文仍为OneFlow关联公司北京一流科技有限公司董事。</p><p></p><h4>量子计算技术重磅升级：IBM展示最新的模块化量子处理器</h4><p></p><p></p><p>当地时间周一（12月4日），IBM在官方博客发文，展示了“量子效用”所需的硬件和软件，其中包括新的量子处理器芯片和量子计算系统。</p><p></p><p>新闻稿称，IBM展示了一种新方法：将芯片连接到机器内部，再将机器连接到一起，以形成模块化系统，使规模的扩展不受物理条件限制。IBM称，将这种方法叠加新的纠错码，有望在2033年之前制造出引人注目的量子机器。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>投票开除奥特曼的董事发声：OpenAI之乱跟AI安全没关系</h4><p></p><p></p><p>在上个月令全球科技圈震惊的OpenAI“内乱100小时”中，AI圈的顶流明星山姆·奥特曼在遭到董事会扫地出门后又迅速凯旋而归。即便如此，由于事发后核心人物鲜少谈及幕后的考量，整件事情至今还留有诸多疑问。</p><p></p><p>当地时间周四，已经离开OpenAI董事会的Helen Toner公开发声，对于外界的诸多疑问和“知情人士消息”做出一些回应。</p><p></p><p>Toner表示，董事会开除奥特曼的原因与AI安全没有关系，而是“缺乏信任”。她进一步解释称：“我们解雇山姆的目的，是为了加强OpenAI并使其更有能力实现其使命。”</p><p></p><p>在面对OpenAI的律师试图施压董事会辞职时，她也坚持了这一立场。Toner介绍称：“律师试图声称，如果我们不立即辞职，将会违法。因为若公司因此崩溃，我们将违反受托责任。但OpenAI是一个非常特殊的组织，非营利使命——确保人工通用智能（AGI）惠及全人类——是最重要的。”</p><p></p><p>事实上，在面对律师强调“公司会因此崩溃”时，Toner回应称“这样也符合我们的使命”，令房间里的一众公司高管感到吃惊。对于这一点，Toner也补充道，这句话是对律师“恐吓策略”的回应。她试图表达的是：对于创建造福全人类的AGI这一使命而言，OpenAI的持续存在并不是必要条件。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</id>
            <title>FCon 演讲视频：数字人民币（e-CNY）赋能支付业态发展</title>
            <link>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全球数字货币, 中国数字人民币, 金融技术变革, 数字货币的安全性
<br>
<br>
总结: 随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。 </div>
                        <hr>
                    
                    <p>随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。在这个变化中，既有机遇也有挑战，特别是在数字货币的安全性、普及性和监管方面。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了苏州银行网络金融部高级产品经理<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5574">金一松</a>"，他以主题为《数字人民币（e-CNY）赋能支付业态发展》展开了分享，以下为重点内容概述：</p><p></p><p>数字人民币的定义与特性：详细讨论了数字人民币的定义、设计特点，包括与传统货币的区别、发行和流通方式，以及它在支付体系中的角色。数字人民币的母子钱包体系和软硬钱包形态：探讨了数字人民币的钱包体系，包括母子钱包体系的结构和软硬钱包的不同形态。无网无电支付能力与智能合约应用：强调了数字人民币在无网络和无电源环境下的支付能力，以及智能合约在数字货币中的应用。数字人民币的未来应用前景：展望了数字人民币未来的发展方向，包括在不同场景中的应用潜力和可能的创新应用。</p><p></p><p>通过深入了解这些重点内容，我们可以更全面地认识数字人民币的影响力及其在未来金融生态中的潜在角色。详细内容，请观看完整视频：</p><p></p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>