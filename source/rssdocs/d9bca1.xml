<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/q15F2I29f8q3NkcS0h4l</id>
            <title>国网智能电网研究院数字化技术研究所 / 高级工程师于海博士确认出席 QCon 上海，分享电力数字孪生共性软件开发平台研发及应用</title>
            <link>https://www.infoq.cn/article/q15F2I29f8q3NkcS0h4l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/q15F2I29f8q3NkcS0h4l</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 电力数字孪生共性软件开发平台, 于海博士, 数字孪生平台概念和特征
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，国网智能电网研究院的高级工程师于海博士将分享关于电力数字孪生共性软件开发平台的主题。他将探讨国网公司数字孪生平台的概念和特征，以及平台的架构、模型库、场景库、组件库、两中心、基础能力服务等内容。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1213&amp;utm_content=yuhai">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。国网智能电网研究院数字化技术研究所 / 高级工程师于海博士将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5649?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1213&amp;utm_content=yuhai">电力数字孪生共性软件开发平台研发及应用</a>"》主题分享，探讨国网公司数字孪生平台概念和特征，电力数字孪生平台的总体架构、模型库、场景库、组件库、两中心、基础能力服务，以及平台特色，成效，技术创新点等。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5649?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1213&amp;utm_content=yuhai">于海博士</a>"，高级工程师，江苏省产业教授，东南大学校外导师，中国电机工程学会信息化专委会大数据专家，国网公司优秀专家后备人才，CIGRE 会员，长期从事电力信息通信技术研究与建设服务工作。主持或参与了国家、国家电网公司以及网省公司等重大科技项目二十余项。曾获得国家电网公司科技进步一等奖 1 项，院级科技进步奖 4 项，发表 EI 检索学术论文二十余篇。他在本次会议的演讲内容如下：</p><p></p><p>演讲：电力数字孪生共性软件开发平台研发及应用</p><p></p><p>国网公司及集团各单位已开展大量数字孪生示范应用，但缺乏整体统筹，存在技术路线与组件工具不统一、模型资源分散、重复建设较多、成本投入大等问题，资源共享利用价值较低，且多停留在三维建模与数据接入展示阶段，智能诊断预测与仿真推演等高级业务应用较少。因此研发共性平台，通过低 / 零代码方式开发和配置差异型业务系统，打通业务和技术壁垒，可配置、可扩展、可快速变动。基于共性平台，通过图形化流程配置、部署和管理，实现低代码、集约化、可复用的人工智能模型交付应用。</p><p></p><p>演讲提纲：</p><p></p><p>需求现状分析国内外主流数字孪生平台数字孪生平台概念 &amp; 特征电力数字孪生平台</p><p>○ 总体架构、模型库、场景库、组件库、两中心、基础能力服务</p><p>平台特色，成效，技术创新点数字孪生与人工智能赋能大运会电力保供电水力发电站总结与展望</p><p></p><p>听众收益点：</p><p></p><p>○ 了解国网公司的数字孪生系统顶层设计规划</p><p>○ 了解国网公司在数字孪生技术方面的技术探索与应用案例</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 3 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O6qHtFBMoJUubbIz4MQw</id>
            <title>Amazon CodeWhisperer 审查：最新的 AI 代码伴侣</title>
            <link>https://www.infoq.cn/article/O6qHtFBMoJUubbIz4MQw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O6qHtFBMoJUubbIz4MQw</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:36:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊云科技, 机器学习支持的服务, Amazon CodeWhisprer, 开发人员的工作效率
<br>
<br>
总结: 亚马逊云科技推出了一项名为Amazon CodeWhisprer的机器学习支持的服务，通过根据开发人员在自然语言中的评论和他们在集成开发环境中的代码生成代码建议来帮助提高开发人员的工作效率。这项服务可以实时提供代码建议，但需要清晰明确的输入任务才能获得优质结果。 </div>
                        <hr>
                    
                    <p>亚马逊云科技推出了一项机器学习支持的服务，该服务通过根据开发人员在自然语言中的评论和他们在集成开发环境中的代码生成代码建议来帮助提高开发人员的工作效率。这项名为 Amazon CodeWhisprer&nbsp;可以免费使用。类似于微软去年推出的 GitHub copilot 。</p><p></p><p>在过去的几个月里，我有机会在几个用例中试验了这项服务。作为一名机器学习 (ML) 开发人员，我拥有利用 ML 帮助开发 ML 解决方案的优势。因此，我在访问此服务后写了一些观察。此外，我正在就如何使其更智能和更易于访问提供具体建议。</p><p></p><h3>服务在行动</h3><p></p><p></p><p>该服务根据代码编辑器中的注释和同一文档中的先前代码提供实时代码建议。该服务可以建议行完成或完整的代码块（例如，方法）。</p><p></p><p>在 Visual Studio 上，有一些方便的快捷方式使服务的使用更加方便。启用扩展后，该服务提供类似于许多 IDE 支持的自动完成功能的在线推理。但是，用户可以点击 (Alt+C) 来查看推荐，而无需等待响应。</p><p>下面是编写著名的二分查找方法的示例</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ea7bd321d5d437dfc35e93aef2cda5e.gif" /></p><p></p><p>有趣的是，该服务可能会建议多个代码片段，这些代码片段可以轻松导航（使用左/右箭头）以选择最合适的推荐。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0ff3b91a7fc8eb544b37d8bdd2561445.gif" /></p><p></p><p>Amazon CodeWhisprer 就像是试图用正确的代码在您耳边耳语的伴侣。因此，它是一个非常花哨和超级描述性的名字。在命名服务方面做得很好。</p><p></p><h3>深入探讨，如何充分利用服务？</h3><p></p><p></p><p>AI 代码伴侣是一个强大的工具，可以提高开发人员的工作效率。尽管有人认为这样的工具将来可能会取代开发人员，但现在下结论还为时过早，因为该服务与任何其他服务一样：Garbage in Garbage out。也就是说，它在很大程度上取决于返回良好结果所需的输入。以下是输入质量如何完全影响输出质量的示例。</p><p></p><p>在这里，提供的描述很模糊，没有明确的要求，所以在等待比较长的时间后，输出是混乱的导入。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/8132b89afb69d7bd19c44e5f4461be29.gif" /></p><p></p><p>随着输入描述变得更加清晰，输出变得更好，如下所示，这是一个类似但更清晰的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f9282e5205bbfa610f5937b056dd1eb.gif" /></p><p></p><p>此外，随着用户添加更多上下文，即开发人员编写更多代码，推荐的质量显着提高。例如，与在同一文档上的孤立任务或在项目早期上下文仍然不够的情况下相比，在处理一个项目时预计会获得更快和更个性化的结果。</p><p></p><p>尽管如此，该服务预计不会为臭名昭著的自定义任务返回有用的答案。下面是一个同样的二分查找问题的例子，但对输入格式做了些许修改。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6736d478974f6d7cc3c0b76adf32c33.gif" /></p><p></p><p>显然，引擎无法理解对问题的轻微修改（即，允许重复的元素）并且仍然产生与前面建议的相同的代码。</p><p></p><h3>服务能不能更好？</h3><p></p><p></p><p>由于该服务仍处于预览阶段，预计会遇到许多不足。以下是可以使服务变得更好的精选操作列表。</p><p></p><h3>推理速度：</h3><p></p><p></p><p>正如在上面的示例中可能指出的那样，该服务需要花费大量时间来提出建议。我相信这方面还有很大的改进空间。</p><p></p><h3>一致性和实时性：</h3><p></p><p></p><p>该服务有望在开发人员编写代码时提供实时建议。但是，实时建议可能不会在特定时刻给出任何输出。令人惊讶的是，按下 (Alt+C) 快捷键会返回可行的解决方案，而无需更改任何内容（即同时即时）。</p><p></p><h3>最终用户定制：</h3><p></p><p></p><p>引擎盖下的推荐引擎使用了一个巨大的代码库，这些代码库来自许多为不同目的而编写的源代码。为某些项目接受的源启用更多自定义是合理的。</p><p></p><p>此外，根据项目主题预测代码可能是有益的。例如，机器学习开发与开发移动应用程序完全不同。</p><p></p><p>作为另一个示例，用户可能想要处理需要设计和聚合的多个代码块的项目。在其他项目中，可能需要优先考虑线路完成而不是阻止建议。</p><p></p><p>自定义示例列表非常庞大，需要仔细设计。</p><p></p><h3>解决方案排名：</h3><p></p><p></p><p>建议多种解决方案是一个很棒的功能。然而，在实践中，这些解决方案的排名并不是最优的，用户需要浏览所有解决方案才能找到正确的建议。这可能很乏味，并且会降低整体生产力。</p><p></p><h3>问题定制：</h3><p></p><p></p><p>该引擎有效地理解了训练语料库中发现的常见问题。然而，它更难适应同一问题的新挑战。</p><p></p><h3>结论</h3><p></p><p></p><p>总而言之，Amazon CodeWhisprer（以及一般的 AI 代码伴侣）毕竟不是可以解决所有问题的魔法。但是，它是一个很好的工具，可以通过专注于正确的问题而不是繁琐的重复性任务来提高开发人员的工作效率。</p><p></p><p>为了充分利用 Amazon CodeWhisprer（以及一般的 AI 代码伴侣），以下操作可能有助于实现预期目标：</p><p></p><p>简明评论：输入任务越清晰明确，获得优质结果的概率就越高。统一项目：人工智能引擎从整个文档中收集信息。因此，它不断丰富上下文。因此，将它用于以某种方式具有连接的任务会更有益。避免高级自定义问题：问题越不受欢迎，它不会返回任何有用答案的可能性就越高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ea7bd321d5d437dfc35e93aef2cda5e.gif" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi</id>
            <title>英伟达成为人工智能公司主要投资者：条件是必须使用英伟达产品</title>
            <link>https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3QgC2C2JQghLz4RZBNgi</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:25:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金融时报, 英伟达, 人工智能, 初创企业
<br>
<br>
总结: 英伟达成为领先的人工智能投资者，今年已经投资了超过20家初创企业，涵盖了各个领域，从大型新人工智能平台到小型初创企业。英伟达的投资组合包括了OpenAI的竞争对手Inflection AI和Cohere，以及巴黎的人工智能初创企业Mistral、Hugging Face和CoreWeave等。英伟达的投资活动在人工智能领域非常活跃，超过了硅谷的其他大型风险投资公司。 </div>
                        <hr>
                    
                    <p>据英国《<a href="https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a">金融时报》</a>"报道，英伟达今年已投资了“二十多家”公司，包括从价值数十亿美元的大型新人工智能平台到将人工智能应用于医疗保健或能源等行业的小型初创企业。</p><p>&nbsp;</p><p>根据跟踪风险投资机构 Dealroom 的估计，英伟达在 2023 年参与了 35 笔交易，几乎是去年的六倍。Dealroom 表示，这是英伟达人工智能领域交易最活跃的一年，超过了 Andreessen Horowitz 和红杉等硅谷大型风险投资公司（不包括 Y Combinator 等小型加速器基金）。</p><p>&nbsp;</p><p>英伟达专门风险投资部门 NVentures 的负责人Mohamed Siddeek 表示：“总体而言，对于 Nvidia 来说，（进行初创企业投资）的首要标准是相关性。”&nbsp;Siddeek 解释道，“使用我们的技术、依赖我们的技术、在我们的技术上建立业务的公司……我无法想象我们会投资一家不使用 Nvidia 产品的公司。”</p><p>&nbsp;</p><p>据报道，英伟达的总体投资组合包括OpenAI的两大竞争对手Inflection AI和Cohere。这些公司还是英伟达的现有客户，只要公司继续成长和发展，对双方来说都会是一件好事。</p><p>&nbsp;</p><p>英伟达的另一项投资是<a href="https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3">Mistral</a>"，这是一家总部位于巴黎的人工智能初创企业，本月早些时候获得了20亿欧元的估值。另外两个是Hugging Face和CoreWeave，它们都是Nvidia GPU芯片或软件的用户。</p><p>&nbsp;</p><p>对于“接受投资的人也得到了优惠条件”的说法，Siddeek回应称，“我们不帮助任何人插队。”他反驳道，在任何投资中都有使用英伟达产品的条件，但他补充说，“我们会尽量对投资者友好。”</p><p>&nbsp;</p><p>据悉，英伟达的H100 GPU芯片最近已经成为硅谷最受欢迎的产品之一。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a">https://www.ft.com/content/25337df3-5b98-4dd1-b7a9-035dcc130d6a</a>"</p><p><a href="https://readwrite.com/nvidia-emerges-as-leading-investor-in-ai-companies/">https://readwrite.com/nvidia-emerges-as-leading-investor-in-ai-companies/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz</id>
            <title>云原生数据库 GaiaDB 架构设计解析：高性能、多级高可用</title>
            <link>https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0cwP1eTkEzaaUxxs8Doz</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 02:08:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 云原生数据库, GaiaDB, 分布式数据库
<br>
<br>
总结: 百度智能云团队推出了四期《百度智能云数据库》系列云智公开课，介绍了以云原生数据库 GaiaDB 和分布式数据库 GaiaDB-X 为代表的百度智能云数据库系列产品。其中，云原生数据库有两种技术路线，一种是存算分离架构，另一种是先搭建分布式框架再填充数据库逻辑。这两个路线都在向着统一的目标演进，存算分离路线在增强 SQL 的多级并行能力，分布式事务路线在探索小数据规模下的单机部署架构。GaiaDB 是百度智能云的云原生数据库产品，通过不断迭代和升级，实现了大容量存储、快速弹性能力和跨地域热活功能。GaiaDB 的设计理念是融合和裁剪，实现高性能和多级高可用。 </div>
                        <hr>
                    
                    <p><a href="https://www.infoq.cn/article/SGPHdt4a0GyPUVyOotSm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"团队在今年 11-12 月特别推出了四期《百度智能云数据库》系列云智公开课，为大家全面地介绍了以云原生数据库 GaiaDB 和分布式数据库<a href="https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> GaiaDB-X</a>" 为代表的百度智能云数据库系列产品。</p><p></p><p>在《百度智能云数据库》系列云智公开课的第二期内容中，百度智能云数据库高级架构师邱学达为我们介绍了云原生数据库的不同技术路线及能力对比，并对比传统单体数据库介绍了云原生数据库的技术差异和挑战，同时深入浅出地解析了 GaiaDB 在高性能和多级高可用方向上的技术架构。</p><p></p><p>下文为他的演讲内容整理：&nbsp; &nbsp;&nbsp;</p><p></p><h2>云原生数据库和 GaiaDB</h2><p></p><p></p><p>目前，<a href="https://xie.infoq.cn/article/be269a4dac007392339e5f63b?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">云原生数据库</a>"已经被各行各业大规模投入到实际生产中，最终的目标都是「单机 + 分布式一体化」。但在演进路线上，当前主要有两个略有不同的路径。</p><p></p><p>一种是各大公有云厂商选择的优先保证上云兼容性的路线。它基于存算分离架构，对传统数据库进行改造，典型产品有 AWS Aurora、阿里云 PolarDB、腾讯云 TDSQL-C、百度智能云 GaiaDB。</p><p></p><p>数据库作为公有云上的核心基础设施，第一要务是实现用户上云的平滑性。目前像云网络、云主机，云盘都实现了完全透明兼容。云原生数据库也必须实现从语法、使用习惯、再到生态上的全面兼容。因此，基于现有生态做分布式化改造成为了一条首选的演进路线。使用存算分离路线的云原生数据库可以完美兼容传统的使用习惯，为交易类场景提供低延迟的写事务能力，同时读扩展性与存储扩展性借助了分布式存储的池化能力，也得到了很大增强。</p><p></p><p>另外一种路径是先搭建一套分布式框架，然后在其中填充数据库逻辑。OceanBase 和 TiDB 就是其中两个比较典型的产品。它们将事务的子系统和锁的子系统拆分为单独的模块。计算层通过与这些模块交互，可让多个节点均支持写请求。然后由统一的新事务 + 锁中心节点来进行仲裁。这样，对需要较多计算资源的写负载场景会有较好的提升。由于事务和锁都需要跨网络进行交互，因此事务延迟相对较高，在锁负载较重的情况下会成为一定的瓶颈。</p><p></p><p>目前这两个路线并不是泾渭分明，独立发展的，大家都在向着统一的目标演进。因此我们可以看到，存算分离路线在逐渐增强 SQL 的多级并行能力，同时也在探索和支持多个写节点的库表级 / 行级的多写能力。同时分布式事务路线也在积极探索在小数据规模下的单机部署架构。</p><p></p><p>所以在未来，这两个路线会不断融合。业务的数据规模不管多大，都可以平稳快速地运行在数据库系统上，而不需要用户去过分关注分区、索引、事务模型等信息。就像十年前如何在机器之间存储海量小文件还是一个后端研发工程师的必修课，而随着 S3 存储的出现，用户再也不需要考虑如何通过哈希等方式来保证单个文件夹不会保存太多文件一样。</p><p></p><p><img src="https://static001.geekbang.org/infoq/49/494bca1fca9378e1fa0da766c469e03a.png" /></p><p></p><p>GaiaDB 是从百度智能云多年数据库研发经验积累中逐渐迭代而来。GaiaDB 于 2020 年发布首个版本，首次实现了基于存算分离的大容量存储和快速弹性能力，解决了百度内部的历史库、归档库等大容量存储需求。</p><p></p><p>紧接着，为了满足集团内大部分核心业务的跨地域热活准入门槛和就近读性能需求，GaiaDB 于 2021 年发布了地域级热活功能。跨地域热活仍然使用存储层同步的方案，同步延迟与吞吐都相较逻辑同步有很大提升，从地域可以实现与主地域接近相同的同步能力，不会成为拖慢整体系统的短板，也不会像逻辑同步那样在大事务等场景下出现延迟飙升的问题。</p><p></p><p>所以 2.0 版本上线后，GaiaDB 逐渐接入了手百、贴吧、文库等多个核心产品线，解决了业务在跨地域场景下的延迟与性能痛点。</p><p></p><p>随着业务的逐渐上云，多可用区高可用的需求慢慢凸显，如何实现单机房故障不影响服务成为了很多业务上云的关注点。为此 GaiaDB 打造了可支持跨可用区热活的 3.0 版本，每个可用区都可以实时提供服务并且不增加额外的存储成本。而在今年， GaiaDB 推出了更加智能化的 4.0 架构，性能进一步提升，功能完整度也在持续完成覆盖。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/729c24394d30b36427010d70f44034d2.png" /></p><p></p><p>接下来整体介绍一下 GaiaDB。目前 GaiaDB 已经实现了线上全行业场景覆盖，最大实例达到了数百 TB，不仅兼容开源生态，还实现了 RPO=0 的高可靠能力。在成本方面，由于在架构设计上采用了融合的技术理念，GaiaDB 不依赖特殊硬件和网络环境也可以保证性能，实现云上云下一套架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d33cdaf891b3d0f762e21079e50e16c.png" /></p><p></p><p></p><h2>GaiaDB 的高性能 &amp; 多级高可用设计</h2><p></p><p></p><p>接下来我来分享一下 GaiaDB 的性能核心设计理念——通过融合和裁剪，将数据库和分布式存储进行深度融合，为全链路的同步转异步化提供条件，从而实现极致的性能与通用性。</p><p></p><p>我们可以看到，如果数据库简单使用通用分布式协议和单机存储引擎，如左图所示，那么数据库需要处理主从同步，需要有 CrashSafe 所需要的物理日志。同时，一致性协议也要有主从同步，要写自己的 WAL 以及持久化快照。而单机引擎同样需要 CrashSafe 以及一套日志系统和数据存储逻辑。</p><p></p><p>我们发现，多层日志的嵌套带来了层层延迟与写放大。更复杂的是，数据流中嵌套多层逻辑后，也给系统整体数据安全带来了一定挑战。同时由于多层之间需要串行等待，所以在加入了网络延迟后会给数据库带来很大的性能下降。虽然可以使用定制化硬件与网络来缩短网络和磁盘落盘的延迟以降低链路耗时，但这又引入了新的不确定性并导致了更高的成本。</p><p></p><p>GaiaDB 的解决思路是将事务和主从同步逻辑、日志逻辑、快照和存储持久化逻辑重新组合和排布。</p><p></p><p>首先是将分布式协议的主从同步逻辑融合进数据库计算节点中。由于计算层本身就需要处理主从同步、事务和一致性问题，相关的工作量增加并不大。这样一来，最直接的收益就是将两跳网络和 I/O 精简为一跳，直接降低了链路延迟。</p><p></p><p>其次 GaiaDB 将多层增量日志统一改为使用数据库 Redo 物理日志，由 &nbsp;LogService 日志服务统一负责其可用性与可靠性。</p><p></p><p>除此之外，GaiaDB 也将持久化、快照和数据库回放功能融合入存储节点。由于存储层支持了数据库回放能力，可以很轻松实现数据页级别的 MVCC。这样全链路只剩下了数据库语义，数据流简单可靠，逻辑大大简化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/868f47ab0c9ef26160ad0bb62bc6da58.png" /></p><p></p><p>下面我们一起来看下共识模型上的改变。</p><p></p><p>像 Raft 协议是需要两跳网络才能实现一次提交确认的，右上角就是 Raft 的数据流架构：CN 节点将写发送给 Leader 后，需要等待 Leader 发送给 Follower 并至少收到一个返回后才能成功。</p><p></p><p>这里就带来了两跳网络和 I/O 的同步等待问题。而 GaiaDB 则是计算节点直接发送给多个 Log 服务并等待多数派返回，这样不依赖任何特殊硬件与网络就降低了延迟。这样系统里不管是事务的一致性还是多副本一致性，统一由计算节点统筹维护，所有的增量日志也统一为数据库物理日志，整体数据流简单可控。</p><p></p><p>对于数据风险最高的 Crash Recovery 场景，由于统一使用了数据库语义，整体流程更加健壮，数据可靠性更高，降低了数据在多种日志逻辑之间转换和同步带来的复杂度风险。而在性能方面，由于存储层自身具备回放能力，可以充分利用 LogService 层的日志缓存能力。对于写操作来说，不需要每次更改都刷盘，可以批次回放刷盘，大大节省了磁盘吞吐与 I/O。</p><p></p><p>经过以上改造，线上吞吐性能可以提升 40% 。同时由于链路简化，也大大优化了长尾延迟。像之前计算节点与分布式主节点之间发生网络抖动的场景，就会被多数派的返回特性来优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/880c11b4c3596b9934ec882ae9281276.png" /></p><p></p><p>分享完一致性协议层优化，接下来我们来探讨一下链路层优化。</p><p></p><p>我们知道，总吞吐与并发度成正比，与延迟成反比。一致性协议层改造并缩短了数据链路，可以通过降低延迟来增加吞吐。那么有没有办法通过提升数据流的并发度来提升吞吐呢？答案是可以。由于数据库的物理日志自带版本号与数据长度，所以不需要像通用存储一样实现块级别串行提交。之所以使用通用存储需要串行提交，是因为存储端只能根据请求到达的先后确定数据版本，如果乱序到达，最后生效的版本是不可知的。</p><p></p><p>而对于 GaiaDB 来说，由于 LogService 具备数据库语义的识别功能，所以计算节点只需要异步进行写入，日志服务就会自动根据数据版本选取最新数据，然后根据写入情况批量返回成功，这样链路就可以实现延迟与吞吐的解耦。</p><p></p><p>当然计算层依然会等待日志层批量返回的最新落盘版本后再返回事务提交成功，所以依然可以满足提交成功的事务一致性、持久化的要求。</p><p></p><p>另外针对高负载下 I/O 请求与数据库业务请求争抢 CPU 的问题，我们使用了 I/O 线程隔离技术，通过资源隔离的方式，将 I/O 线程与数据库业务线程进行隔离。这样即使在复杂负载场景下，I/O 延迟仍可以保持在较低水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc00033498ca3707091d9bbfb5dadcbc.png" /></p><p></p><p>在分析完前面两部分之后，可能会有同学有疑问：既然日志层到存储层不是同步写，是不是最终系统的一致性降低了？有没有可能发生数据丢失或不一致的问题呢？答案是不会。因为 GaiaDB 的存储是一套支持 MVCC 的多版本系统。所以即使回放实现上是异步，但是由于请求方会提供所需要的数据版本，存储层可以提供对应版本的强一致数据视图。</p><p></p><p>GaiaDB 的存储节点支持数据页的回放功能，可以动态回放至任意目标版本后再返回，在之前的版本里，假如由于异步的因素还没有获取到这部分增量日志，存储节点也会启用优先拉取的策略实时拉取一次日志后再回放，以此来提供较好的时效性。而在最新的 GaiaDB 版本中，我们也在计算层添加了同样的回放能力，存储节点尽力回放后仍不满足需求的，由计算节点进行剩余任务。</p><p></p><p>这样对于存储慢节点的兼容能力就大大增强了，同时由于存储节点会尽力回放，所以也可以最大化利用存储层的算力资源。对于刷脏逻辑目前也完全下沉到了存储层，存储节点可以自主控制刷盘策略和时机，尽量合并多次写后再进行落盘，大大节省了磁盘 I/O 负载，平均 I/O 延迟降低了 50%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5fdd7d1fa003f9f635a729ca00a5f72.png" /></p><p></p><p>下图中我们可以看到，在综合了多项优化后，读写性能实现了最高 89% 的提升，其中写链路线路提升尤其明显。这些都是在使用普通存储介质和网络环境的情况下测试得出的，主要得益于数据链路的缩短与同步转异步的自适应高吞吐能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9eb5e4d3c677acd21e5b83096f16ceb3.png" /></p><p></p><p>在讨论完性能后，再分享一下 GaiaDB 在高可用方面的思考和设计理念。</p><p></p><p>数据库作为底层数据存储环节，其可用性与可靠性直接影响系统整体。而线上情况是复杂多变的，机房里时时刻刻都可能有异常情况发生，小到单路电源故障，大到机房级网络异常，无时无刻不在给数据造成可用性隐患。</p><p></p><p>作为商业数据库，具备多级高可用能力是最核心的必备能力。这样才能抵御不同级别的异常情况，有力保障客户业务的平稳运行。GaiaDB 支持多副本、跨可用区、跨地域三级别高可用，创新性地实现了多可用区热活高可用、单个实例支持跨可用区部署。在不增加成本的情况下，每个可用区均可提供在线服务，任何可用区故障都不会打破存储一致性。下面我们来分别看一下每个级别高可用能力的实现。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1f9701867ba0e5edfacb8cc3d4de9be.png" /></p><p></p><p>首先是实例的多副本高可用能力。</p><p></p><p>GaiaDB 对整体的分布式架构进行了重新设计，系统共分为三层，即计算层、日志层、存储层。其中计算层本身无状态，仅负责事务处理与一致性维护，所以获得了很强的弹性能力，实现了秒级切换、多节点容灾，同时扩缩容只需要内存启动即可。</p><p></p><p>日志层负责系统增量日志部分的持久化，实现了多数派高可用。同时由于一致性协调角色上移到了计算层，所以该层全对称，任意节点故障不需要进行等待选主，也不会有重新选主带来的风暴和业务中断问题。</p><p></p><p>再往下是存储层，负责数据页本身持久化与更新。由于上层保留了增量日志，所以存储层可以容忍 n-1 副本故障。简单来说就是只要有一个副本完好，加上上层提供的增量日志，即可回放出所有版本的完整数据，实现了相比传统多数派协议更高的可靠性能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/38/38691a2f2817476cc4e38ca54e83b504.png" /></p><p></p><p>其次是跨可用区与跨地域的高可用能力。</p><p></p><p>GaiaDB 的多级高可用都是基于存储层物理日志的直接复制。相比逻辑复制，数据链路大大缩短，同步延迟也不再受上层大事务或者 DDL 等操作影响，在主从同步延迟上具有很大优势。</p><p></p><p>对于跨可用区高可用来说，由于 GaiaDB 具有对称部署架构，所以可以很方便地进行跨可用区部署。这样可以在不增加存储成本的情况下实现多可用区热活，任一可用区故障都不影响数据可靠性。</p><p></p><p>写数据流可以自适应只跨一跳最短的机房间网络，不需要担心分布式主节点不在同机房带来的两跳跨机房网络和跨远端机房问题，而读依然是就近读取，提供与单机房部署接近的延迟体验。由于跨机房传输的网络环境更为复杂，GaiaDB 添加了数据流的链式自校验机制，使数据错误可以主动被发现，保障了复杂网络环境下的数据可靠性。</p><p></p><p>对于跨地域高可用来说，由于同样使用了异步并行加速的物理同步，及时在长距离传输上，吞吐依然可以追齐主集群，不会成为吞吐瓶颈，在计入网络延迟的情况下，国内可以实现数十毫秒的同步延迟，这是因为跨地域同样可以使用异步并行写加速，自动适应延迟和吞吐之间的关系。同时地域之间还可以实现主动快速切换和默认就近读取。</p><p></p><p>所以在使用了 GaiaDB 的情况下，业务可以不做复杂的数据同步逻辑就可以实现低成本的跨可用区与跨地域高可用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb38aea27c6a8d0956fe7232108ba038.png" /></p><p></p><p>介绍完高性能和高可用两部分的设计理念后，接下来再介绍一下我们正在内部灰度中的新功能：</p><p></p><p>并行查询：并行查询从并发度上进行加速的并行查询能力，这对大数据规模下的多行查询有非常好的加速作用，可以充分利用计算节点的 CPU 和内存资源和分布式存储层的并行 I/O 能力。分析型从库（HTAP）：分析型从库具备多种行列加速能力，既有支持百 TB 级别数据计算的分析型节点解决方案，也有支持百万行以上检索加速的列式索引引擎。其中列式索引引擎同样采用物理日志同步，不需要业务维护数据一致性，可以和当前交易类负载的事务隔离级别兼容。Serverless：我们也在探索充分利用内部潮汐算力的资源优化调度方案，在白天业务高峰期，将资源向实时性更强的交易类业务倾斜，在低峰期自动缩容，将资源复用投入到离线计算类业务中，不但客户节省了运维成本与资源成本，也避免了资源闲置和浪费，实现了更高的资源利用率。</p><p></p><p>以上功能预计都会在近期开放灰度试用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad6014a894d8b5543a024011f1729a80.png" /></p><p></p><p></p><h2>写在最后</h2><p></p><p></p><p>自 11 月 15 日起，百度智能云团队每周三都会上线一节《百度智能云数据库》系列云智公开课。在前 4 期的课程中，专家们围绕“从互联网到云计算再到 AI 原生，百度智能云数据库的演进”、“高性能和多级高可用，云原生数据库 GaiaDB 架构设计解析”、“面向金融场景的 GaiaDB-X 分布式数据库应用实践”、“一站式数据库上云迁移、同步与集成平台 DTS 的设计和实践”四个主题展开了分享。</p><p></p><p>每节直播课的视频我们都进行了录制留存，都整理进了课程专题页中，课程持续更新中，大家立即点击<a href="https://www.infoq.cn/theme/222">【此处链接】</a>"进行观看吧~</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG</id>
            <title>一站式数据库上云迁移、同步与集成平台 DTS 的设计实践</title>
            <link>https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W60k56xzqQFgzEvGOVeG</guid>
            <pubDate></pubDate>
            <updated>Wed, 13 Dec 2023 01:51:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据库迁移, 公有云, 技术挑战, 数据一致性
<br>
<br>
总结: 数据库迁移面临的挑战主要包括数据库上云的选型、迁移流程的长短、迁移过程中的效率和容灾以及数据一致性保障。公有云在国内数据库市场中占据主导地位，具有弹性、成本和易用性等优势。数据库迁移需要考虑引擎、架构、套餐的选择，以及异构迁移和跨版本迁移的兼容性。迁移流程需要打通云上云下的网络，保证数据一致性，并具备容灾能力。迁移过程对业务的影响要尽可能少，同时要保证数据的最终一致性。 </div>
                        <hr>
                    
                    <p></p><h2>一、数据库迁移面临的挑战</h2><p></p><p></p><p>根据大数据技术标准推进委员会今年 7 月发布的《数据库发展研究报告（2023 年）》，我们可以看到，去年国内的数据库市场规模约为 400 亿，今年预计可以达到 540 亿，预计到 2027 年，国内数据库市场规模可达 1280 亿。未来几年复合增长率预期可以达到 26% ，市场潜力非常大。</p><p></p><p>进一步看市场结构，我们可以发现一个明显的趋势：公有云在国内数据库市场中开始逐渐占据主导地位。近三年，国内公有云数据库市场规模的增速在 50% 左右，远高于本地部署市场的 15%。预计今年公有云数据库整体占比可达到 60%。</p><p></p><p>相较于本地部署的方式，公有云数据库通过全托管服务、云原生数据库等形态，在弹性、成本、易用性上更加具有优势。所以在过去十年，越来越多的企业客户选择数据库上云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3d/3d6d97f6ade3ca5770a0aaf8cb7d0ed9.png" /></p><p></p><p>数据库上云面临的技术挑战非常多，主要体现在以下四个方面：</p><p></p><p>第一，数据库上云的选型。企业需要考虑在云上数据库使用的引擎、选用单机还是分布式的架构、以及选用何种套餐，同时还要结合业务特点对数据库做针对性调优。如果遇到异构迁移或者跨版本迁移，还需要评估结构对象的兼容性，给出业务 SQL 的改造方案。</p><p></p><p>第二，云上迁移流程较长。</p><p></p><p>首先，我们需要打通云上云下的网络，把数据库的账号/角色、结构、存量、增量数据完整地搬迁上云。然后，对两端的数据一致性做校验。在一致性校验通过后，业务方把流量从云下割接到云上。最后，需要把云上数据库的增量写入内容反向同步回云下环境，以保留云下环境用于迁移后的灾备。</p><p></p><p>第三，迁移过程中的效率和容灾。迁移过程对业务的影响要尽可能少，迁移链路本身也应当具备容灾能力。</p><p></p><p>第四，迁移的数据一致性保障。数据库承接的往往是在线服务，少一条数据都可能给业务带来严重影响。因此，迁移链路自身要保证两端数据的最终一致性，同时也要提供校验工具用于检查确认两端实例的数据一致性。</p><p></p><p>百度智能云在多年的数据库上云迁移实践中积累了丰富的经验。我们认为，在数据库上云迁移中，平滑和可靠是客户的核心诉求，也是对迁移服务的必然要求。</p><p></p><p>平滑主要体现在易用性、兼容性和业务影响上。</p><p></p><p>易用性：迁移服务要开箱即用，能够托管迁移全流程。兼容性：能够兼容源和目标不同引擎、不同架构、不同版本和不同网络环境，尽可能地降低业务改造成本。业务影响：支持账号、结构、存量、增量的不停服迁移上云，将业务影响降到最小。</p><p></p><p>可靠主要体现在一致性、可回滚和高可用上。</p><p></p><p>一致性：保证源和目标的数据一致性，并提供校验能力。可回滚：支持割接后的反向回滚同步，保留云下环境用于灾备回滚。高可用：迁移和回滚链路要具备故障恢复能力，尤其是当上下游数据库发生主从切换后，迁移链路要具备自愈能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0d0ab9d6458db6ad5ca32d004408d9c1.png" /></p><p></p><p>在数据库上云后，我们看到客户在使用数据库时仍然有很多数据传输的新场景，其中有三个典型场景：</p><p></p><p>异地多活：当客户的服务部署在全球多个机房时，机房间的通信延迟最长可达秒级。如果数据库架构依然是单点写入，请求耗时就会变得非常高。但如果拆分数据库，则会牺牲数据的全局一致性，不满足业务需求。较为理想的架构应当是每个地域的数据库本地读写，然后通过数据同步工具同步至异地节点，最终实现数据全局一致。多云灾备：近几年来，基于服务可用性的考量，越来越多的客户选择多云部署。在生产云出现故障时，客户可以将流量切到灾备云上，以保证服务无损。数据库是有状态服务，因此需要数据同步工具将生产云的实时增量写入同步到灾备云上，以保证两端数据一致，满足灾备需求。数据集成：对用户行为的学习、分析和推理可以帮助企业快速决策，并进一步为用户提供个性化服务。通过数据集成工具，企业可以将数据从生产域实时准确地集成到分析域，从而实现数据深层价值的挖掘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/998f41c889a3a701908cc51723f629ef.png" /></p><p></p><p>基于上述的客户需求，百度智能云推出了一站式数据库上云迁移、同步与集成平台 DTS。</p><p></p><p>在上云迁移方向，<a href="https://www.infoq.cn/video/Vi9e7DGG3wBMVJQF3wBW?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">DTS</a>" 基于百度多年实践总结出一套成熟的数据库上云迁移方案，围绕该方案提供一站式上云迁移体验。</p><p></p><p>在同步/集成方向，DTS 聚焦业务场景，基于场景需求的关键特性打磨产品，提供易用稳定的一站式同步/集成服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9bfad0f6907f5ade3347eec251bb357.png" /></p><p></p><p></p><h2>二、上云迁移方案</h2><p></p><p></p><p>数据库上云迁移是一个复杂的系统性工程，需要客户和云服务商共同配合完成。</p><p></p><p>我们将上云迁移分为三个阶段：迁移前、迁移中、迁移后。</p><p></p><p>迁移前的工作主要是做数据库选型和迁移可行性的评估。数据库选型的维度包括：产品、套餐、架构和存储介质等。选型的过程往往需要对相关的云上数据库产品做功能和性能测试以验证是否满足业务需求。迁移评估则是检查待执行迁移的源端和目标端数据库实例及其宿主和网络环境，得出迁移的可行性结论。在完成了迁移前的选型和评估工作后，就是数据库上云迁移过程。DTS 支持将源端的账号/角色、结构对象、存量数据、增量写入迁移至目标端，迁移过程中无需客户停服。在增量延迟追平后，DTS 支持对两端数据一致性做校验。通过一致性校验后，客户可以将业务流量割接到云上。在验证业务的同时，客户可以使用 DTS 的一键反向功能，快速拉起云上同步回云下的反向回滚链路，将云上的流量反向同步回云下，保留云下环境用于迁移后的灾备。数据库迁移到云上后，<a href="https://www.infoq.cn/article/SGPHdt4a0GyPUVyOotSm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"也提供了<a href="https://xie.infoq.cn/article/28475b63dde77ae8aace1af29?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">数据库智能驾驶舱（DSC）</a>"帮助客户管理、审计和调优云上数据库。</p><p></p><p>下方全景图中标蓝的步骤由 DTS 提供支持，我们可以简单总结为四步：评估、迁移、校验、回滚，下面我将详细介绍每个步骤的方案设计。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/becbd89b490e58c3acdeee360ae0d928.png" /></p><p></p><p></p><h4>2.1&nbsp; &nbsp; 迁移评估与网络接入</h4><p></p><p></p><p>迁移前的第一个准备工作是迁移评估。</p><p></p><p>DTS 迁移任务在启动迁移前，会先执行前置检查，包括检查迁移对象、数据兼容性、两端数据库配置等，最终输出迁移可行性评估结论。对于不通过的检查项会给出修复建议。</p><p></p><p>此外，DTS 还提供了本地迁移评估工具，支持在本地环境执行，支持对多个实例执行批量评估。</p><p></p><p>迁移前的另一个准备工作是网络接入。</p><p></p><p>当前，DTS 支持通过公网、专线、VPN、云自建、云服务等方式接入，通过控制台或 OPENAPI 一键完成网络接入，无需人工部署。</p><p></p><p>此外，对于上云迁移高频依赖的专线或 VPN 接入，DTS 进一步优化了网络接入方式，支持客户将数据库内网域名作为任务端点，保证迁移链路具备切换自愈能力。当云下数据库实例发生主从切换时，只要实例域名保持不变，DTS 任务就可以快速自愈恢复。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8ffcb367295511dea50186ace04e102d.png" /></p><p></p><p></p><h4>2.2&nbsp; &nbsp; 数据迁移原理</h4><p></p><p></p><p>下面介绍 DTS 数据面的数据迁移原理。</p><p></p><p>数据面整体遵循 ETL 插件式设计，支持不同插件的自由组合，以满足不同数据流的迁移需求。</p><p></p><p>数据抽取（E）：通过不同的数据抽取插件，DTS 数据面可以支持采集账号/结构、全量、增量等数据。其中，全量数据抽取由于数据量较大，因此我们通过并发抽取、大表分片等优化手段进一步提升整体吞吐。增量数据则基于 CDC 异步捕获变更，可以让源端负载更少，同时传输实时性更好。性能的相关优化我们在后面会具体介绍。数据转化（T）：完成数据抽取后，源端的原始数据会被归一化为统一的抽象数据结构，这样就实现了异构数据源上下游解耦和端到端自由组合；然后再由数据转化插件对抽象数据结构做数据加工（如：库/表/行/列的过滤和映射）；最后再将数据格式改写为目标端数据源支持的协议。数据加载（L）：数据加载插件将完成协议转换的数据并行批量加载到目标端数据源中。为了进一步提升加载性能，数据面往往通过多线程并行加载数据。但增量同步需保证数据加载的时序与源端严格一致。因此，DTS 支持按表或主键粒度并行分发，属于同一个表或主键的数据会被分配到同一个加载线程串行执行，不同表或主键的数据则可能分配到不同线程上，在保证时序的前提下进一步提升了性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b33b7c85d71aa44bdd070a94ceda3c4c.png" /></p><p></p><p></p><h4>2.3&nbsp; &nbsp; 数据一致性校验</h4><p></p><p></p><p>整体的数据迁移流程遵循先结构、再全量、后增量的顺序。</p><p></p><p>考虑到各类数据库中的 CDC 日志通常是易失的，如：MySQL 的 binlog、MongoDB 的 oplog、Redis 的 backlog 等。数据库往往通过固定缓冲区、定时或限制容量清理等方式限制 CDC 日志的存储用量。</p><p></p><p>DTS 针对该问题优化了迁移流程编排。在进入全量迁移过程后，DTS 除了导出和加载全量数据外，还同时导出增量数据，将其缓存到 DTS 的内部存储中，以避免尚未迁移的增量数据被源端数据库清理。待进入增量迁移阶段后，再将缓存的增量数据加载到目标端。</p><p></p><p>增量同步延迟追平后，即可执行数据校验检查两端数据一致性。由于 DTS 增量同步是异步加载，因此源端和目标端的数据版本实际上存在毫秒级的延迟。因此数据校验可能会因为同步延迟出现误报。我们引入了 X Round Recheck 的方案进一步降低了误报概率。在后面的内容里会专门介绍数据校验的原理。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b27c290770883a240527852a41e692be.png" /></p><p></p><p>DTS 以数据不丢为基础，进一步实现了数据的不丢不重，以保障数据的一致性。</p><p></p><p>数据不丢（At-Least-Once）：依赖于 DTS 数据面的低水位进度管理机制。如下图所示，每一条数据都会关联一个单调递增的版本号，这些版本号组成了一个单调递增的进度序列。当某条数据写入下游并收到了确认写入成功的响应后，该数据对应的版本号会被标记并在进度序列中更新状态。此时，进度管理线程会检查进度序列的水位。</p><p></p><p>在下图中 1、2、3、4 都已经被标记，但 5 尚未标记，因此版本号 4 是低水位里的最大版本号。所以将 4 作为最新进度保存到外部存储中。一旦此时任务容灾恢复，恢复后的任务将以外部存储中记录的最新进度 4 作为断点重新执行迁移。</p><p></p><p>At-Least-Once 机制可以保证数据不丢，但无法保证数据不重。我们在下图中可以看到，6、7 此时都已写入下游，但并未记录到最新进度中，一旦任务以 4 作为断点重新执行，则 6、7 对应的数据会重复迁移。</p><p></p><p>为了实现数据不丢不重（Exactly-Once），DTS 的思路是基于目标端数据库特性去重。对于关系型/文档数据库、数仓来说，表中主键列或唯一键列具有唯一性，因此，我们可以改造 SQL。利用唯一约束实现幂等写入。</p><p></p><p>而对于 Schema-less 的消息队列、分布式文件系统等，DTS 会在投递的消息中加入 UUID，目标端消费方可以基于 UUID 自行去重。</p><p></p><p>最后对于键值数据库（如 Redis），DTS 的思路是将不满足幂等写入的命令改写为满足幂等性，比如累加、累减、插入队列等改为覆盖写。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bc31ca309acf6adbab7374c6ecc0f593.png" /></p><p></p><p>除了迁移系统本身的一致性保证外，DTS 还提供了独立于迁移系统的数据校验功能。数据校验与数据迁移由不同的任务独立执行，以保证校验结果的可信。</p><p></p><p>数据校验的流程可以拆解为：抽取、转化和校验。</p><p></p><p>其中，抽取与转化的实现原理与数据迁移的对应模块实现类似，这里不再赘述。下面重点介绍一下数据校验插件的原理。</p><p></p><p>首先，校验插件在收到待校验数据后会根据主键或唯一键实时查询源端和目标端最新的数据。然后，根据数据加工规则对源端和目标端的数据做归一化，对齐数据元信息。最后，根据不同的任务配置，比对规则校验数据一致性，并将校验结果保存到外部存储中。</p><p></p><p>这套流程在实践中存在小概率误报，原因是在执行数据校验的同时，源端还在持续写入。因此，源端与目标端的数据版本会存在毫秒级的同步延迟，正是这一延迟导致了少量数据的校验误报。</p><p></p><p>因此，DTS 引入了 X Round Recheck 机制。在数据不一致时，会等待一段时间后进入下一轮比对，重新读源端和目标端的数据后再次比对。只有当多次重复比对均不一致的数据会被记录为不一致数据上报。</p><p></p><p>X Round Recheck 大大降低了数据校验的误报频率。经测试，校验误报频率从约千分之一下降到小于百万分之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0c5e65544c682c90c921ee21696f29e.png" /></p><p></p><p></p><h4>2.4&nbsp; &nbsp; 反向回滚</h4><p></p><p></p><p>在完成数据一致性校验后，客户就可以把流量割接到云上了。</p><p></p><p>完成切流后，我们观察到客户往往需要保留云下环境用于容灾回滚。当云上生产环境不可用时，可以将流量快速切回云下灾备环境，服务快速恢复。针对这一痛点，DTS 推出了一键反向功能，支持在流量割接后快速拉起反向回滚任务，将云上流量反向同步回云下。</p><p></p><p>如下图所示，客户在 T1 时刻执行流量割接，此时正向迁移任务运行中，而反向回滚任务处于挂起状态。在 T2 时刻，客户完成割接，此时业务流量已切到目标端数据库，此时执行一键反向，DTS 会将正向迁移任务挂起，反向回滚任务启动，从客户指定的 T1 时刻开始将目标端的业务写入实时同步回源端。从而完成正向迁移到反向回滚的切换。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b36fcc5a27470d0d1987df2f43d4cfae.png" /></p><p></p><p></p><h2>三、同步/集成方案设计</h2><p></p><p></p><p>我们在前面介绍了同步/集成方向的三个典型业务场景：异地多活、多云灾备和数据集成。其中，高可用和高性能是这些场景共同需要的关键能力。</p><p></p><p>异地多活和多云灾备属于在线数据库的实时同步需求。因此支持数据库多主架构和数据一致性的有保证/可校验能力是同步场景的核心痛点。数据集成场景的痛点在于，传统的集成方式中，流批架构不统一、不同数据源使用的集成工具比较繁杂，ELT + ODS 的数据预处理方案实时性差/复杂度高等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca50070612eff19bdbb3383b49c373f2.png" /></p><p></p><p></p><h4>3.1&nbsp; &nbsp;&nbsp;DTS 的高可用和高性能</h4><p></p><p></p><p>接下来，我们分别介绍 DTS 针对上述痛点的方案设计。</p><p></p><p>首先介绍高可用能力。</p><p></p><p>DTS 的高可用设计目标是满足长期不间断的生产级数据同步需求。方案设计可分为三个方面：断点续传、实时容灾和数据库切换自愈。</p><p></p><p>断点续传。DTS 会将传输进度做定期 checkpoint 并持久化到外部存储中。目前 DTS 大部分数据流的全量和增量迁移都支持断点续传。同时，DTS 基于之前介绍的低水位进度管理机制，可以保证断点续传前后数据不丢失，基于目标端的唯一约束可以实现数据不丢不重。实时容灾。DTS 数据面模块支持故障秒级接管和恢复。并且支持对极端脑裂场景的自动检测和恢复，保证脑裂恢复前后的数据最终一致性。切换自愈。DTS&nbsp;支持在源端、目标端数据库实例发生故障切换时，自动发现新的可用节点。当不同节点的传输位点发生变化时，DTS 可以基于时间自动定位新节点的传输位点，保证切换后数据不丢。</p><p></p><p>经过百度智能云内外部数据传输场景的长期实践打磨，DTS 承诺的任务可用性为 4 个 9。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/702ab336cc15fa05036c699b186247ae.png" /></p><p></p><p>接下来让我们再看高性能。DTS 在性能方向的设计目标是追求高吞吐和低延迟。</p><p></p><p>首先是高吞吐，DTS 具有如下的特点：</p><p></p><p>DTS 支持预读取全量数据。当遇到大表时，DTS 支持将大表分片并行读取，解决大表长尾的问题。DTS 支持按照表、主键粒度并行转换和回放。DTS 对数据回放的单线程写入性能做了优化。比如当写入 1000 条数据时， DTS 将 1000 条 INSERT 合并为一条 INSERT，提升了目标端数据库的 SQL 写入效率。写入语句批量执行，网络延迟均摊。</p><p></p><p>其次是低延迟，DTS 提供了如下的能力：</p><p></p><p>DTS 自身基于 CDC 实现增量数据捕获，无需扫表，实时性更好。DTS 采用了数据流式传输模型，全程流水线作业。DTS 选用消息队列缓存和回放增量数据，端到端的同步延迟更低。DTS 针对热点数据，支持基于逻辑的事务合并。在保证最终一致性的前提下，压缩了同步的数据量。</p><p></p><p>以 MySQL 同步为例，全量吞吐峰值约为 20W 行/s，增量吞吐峰值约为 1W 行/s，延迟可以达到毫秒级别。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8765256df2e3f29e51c56a51c9165ef.png" /></p><p></p><p></p><h4>3.2 &nbsp;&nbsp;异地多活场景</h4><p></p><p></p><p>异地多活场景的核心特性是双向同步，可以支持两端数据库写入相互同步，从而实现数据库多主架构。</p><p></p><p>双向同步由正向和反向两个 DTS 同步任务实现，每个任务在同步数据时会通过加入特定的 DML 将该数据所在的事务染色；而另一方向的同步任务在读到染色事务时会直接过滤，从而避免了数据的同步回环。此外，双向同步支持级联，客户可以通过搭建多条双向同步链路实现 N 个地域的数据库多主架构。</p><p></p><p>不过双向同步的使用也有一定限制：</p><p></p><p>业务需避免在两端同时变更主键/唯一键相同的行记录，尤其是避免同时执行 UPDATE，否则可能产生冲突，造成数据不一致。因此，我们推荐业务层面支持流量单元化。表中不能使用自增主键，这是为了避免主键冲突导致的数据不一致。仅有正向同步任务支持同步 DDL，反向同步任务仅同步 DML，因此若需执行 DDL 建议在正向同步任务的源端执行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d321d9451ff5a40360143c3d278be810.png" /></p><p></p><p></p><h4>3.3&nbsp; &nbsp;数据集成场景</h4><p></p><p></p><p>经典的 Lambda 架构包含定时和实时两套架构，分别处理流和批两种不同的数据。架构不统一会带来运维迭代成本高、流批产出数据不一致等问题，所以现在业界都在逐渐转向流批一体。</p><p></p><p>DTS 的架构天然支持流批一体，源端无论是有界数据（数据库快照，指定区间的增量）还是无界数据（持续的数据库流量），都会通过数据切片的方式切分为无数个 Micro-Slice，通过流水线作业最终同步到目标端的仓、湖或流式计算框架。</p><p></p><p>目前 DTS 目标端支持 Doris、Elasticsearch 等数仓，以及百度智能云数据湖 EDAP，支持通过消息队列将数据推送到 Flink 等流式计算框架中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e5b0dd52a21c254741c04835648a0817.png" /></p><p></p><p>在面对上下游异构数据源时，解耦上下游的架构设计能够为系统提供更高的灵活性和可扩展性。这种设计使得 DTS 能够支持端到端的任意组合，组合复杂度从 M*N 降低到 M+N，并能够快速扩展支持新的数据源。</p><p></p><p>DTS 定义了抽象数据格式 DTS Record，可以将源端各类数据库的数据转换为标准的 DTS Record（Any To One），然后再将 DTS Record 转换为目标端数据源接受的数据格式（One To Any）。</p><p></p><p>当 DTS 需要接入新的数据源时，只需要定义新数据源到 DTS Record 的转换规则，即可快速支持现有全部数据源到新数据源的异构数据传输。</p><p></p><p>当然，在异构字段映射的过程中，部分数据可能会因为浮点数精度/字符集不同造成数据精度损失。因此，DTS 优化了同构字段映射规则。当上下游数据源同构时，源端数据能够无损映射到目标端。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/ac26b104b1e719ebc98991747eb8865b.png" /></p><p></p><p>接下来我们看下数据集成的预处理环节。</p><p></p><p>当前业界的主流集成架构是 ELT+ODS。即将数据通过 Sqoop、Spark 等工具，几乎不做 join 或 group 等复杂转化，直接抽取到数据仓库里的贴源层（ODS），再在数据仓库中通过 SQL/H-SQL，将数据从贴源层（ODS）加载到数据明细层（DWD），最终汇总到数据汇总层（DWS）和数据集市（DM）。</p><p></p><p>ELT+ODS 架构的思路是利用数仓的 MPP 高性能计算做 ODS 到 DWD 的大数据预处理。但这仅适用于数据源模式比较简单的情况。当 ODS 到 DWD 规范化复杂度比较高时，往往需要引入 Spark/MapReduce 等框架专门处理。</p><p></p><p>另外，ELT+ODS 架构的实时性较差，难以满足实时分析场景和即时查询的需求。ODS 与 DWD 的数据重复率也比较高，需要付出额外的存储成本。</p><p></p><p>DTS 基于业界最新提出的 EtLT 架构推出了支持实时数据加工的集成方案。它可以将数据从在线域直接集成到 DWD，在集成阶段即可完成实时的数据规范化，无需维护额外的 ODS 层。EtLT 架构的实时性要优于 ELT+ODS 架构，可以支持实时的数据分析和查询统计，让复杂的数据抽取、规范化和加载的过程对数据分析过程透明，帮助其更加聚焦业务。</p><p></p><p>DTS 支持实时数据加工的集成方案预计会在 2024 H1 开放公测，大家可以期待一下。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3bfb46f6ff54c44e7d6fac3968914c71.png" /></p><p></p><p></p><h2>四、DTS 落地实践案例</h2><p></p><p></p><p>第一个案例是某国内大型在线视频服务公司，DTS 支持了该客户的数据库上云迁移和多活同步的需求。</p><p></p><p>该客户的业务痛点主要包括三个方面：</p><p></p><p>迁移规模大：在线服务数据库（MySQL/Redis/MongoDB）中，涉及到上百条业务线的 1.5W+ 集群迁移上云，过程管理难度大。高可用需求高：需要支持数据库不停服迁移、支持切换自愈、支持反向回滚、支持监控指标推送。异地多活：需要支持跨地域多活同步（单元化）、低延迟（&lt;100ms）。</p><p></p><p>针对该客户的三个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>客户业务使用自助上云平台完成上云迁移：平台集成 DTS 服务。DTS 迁移全流程（评估、迁移、校验、回滚）100% 接口化（支持控制台操作），无人工干预。客户 IDC 自建实例切换自愈：DTS 支持专线/ VPN 域名接入，数据库故障切换断点自愈恢复。跨地域多活同步：DTS 支持 MySQL/GaiaDB 双向同步。</p><p></p><p>最终，百度智能云帮助该客户跑通了数据库上云迁移的全流程，并支持客户自助上云迁移，目前已经支持 2000+ 集群的迁移，单集群的迁移周期缩短到 3-4 天。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b25063cab42fe1fd84350a9f48abb3a5.png" /></p><p></p><p>第二个案例是某国有控股大型商业银行，DTS 支持了行方的实时数据分析和跨机房容灾的需求。</p><p>该客户的业务痛点主要包括两个方面：</p><p></p><p>高吞吐：行方核心业务（存/取款明细）涉及到 64 分片集群每月集中跑批， TPS 峰值达到 50W+，要求数据同步延迟分钟级。高可用：数据库及生态产品（DTS）整体具备跨机房容灾能力，故障恢复要求为 RPO = 0，RTO &lt; 1min。</p><p></p><p>针对该客户的这两个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>端到端吞吐优化：CDC 异步拉取/并行解析，表/主键粒度并行转换/加载，数据打包写入。跨机房容灾：基于 load checkpoint 的段点续传（RPO）和数据库拖段服务切换自愈（RTO）实现了任务实时容灾恢复。</p><p></p><p>最终，百度智能云帮助该行落地了实时风控、监控大屏、收支分析等业务场景，线上长期同步任务 900+；同时，支撑行方核心业务（存/取款明细）集群数据同步需求，同步延迟秒级。此外，DTS 服务支持同城双活，机房级故障恢复实现了&nbsp;RPO = 0，RTO &lt; 30s。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/932f451df069b833f97ea0e57ed87be8.png" /></p><p></p><p>第三个案例是某国内大语言模型服务，DTS 支持了业务方的事实数据分析和检索的需求。</p><p></p><p>该客户的业务痛点主要包括两个方面：</p><p></p><p>同步性能：包括大语言模型对话数据、模型 Trace日志实时分析等，部分业务场景要求数据同步延迟达到秒级。快速迭代：&nbsp;大语言模型功能迭代速度快，在线数据库表结构更新较为频繁。</p><p></p><p>针对该客户的这两个痛点，百度智能云提供了如下的解决方案：</p><p></p><p>低延迟调优：自适应写入性能调优，数据打包窗口动态调整。整库同步：DTS 支持库级别同步，目标端 Doris 支持增量同步 DDL，支持增量阶段新增/删除同步对象。数据规范化：支持库表行列过滤、库表列名映射等功能。</p><p></p><p>百度智能云支撑了该大语言模型中服务日志实时分析与实时报表的需求，同时也满足了业务快速迭代带来的整库同步、表结构更新同步等需求，最终实现了长期同步任务 470+，同步延迟最低达到秒级的业务效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/59c5196f7db586c10a37ac61eb6bd84e.png" /></p><p></p><p>我们对 DTS 的所能支持的各类业务需求归纳为 8 种典型的场景，供大家参考。分别是：不停服迁移上云、异地多活、多云灾备、业务事件驱动、信创迁移、缓存更新、实时分析、实时入湖/仓。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7dd5d322f241190f6c9e591ed7a1e75.png" /></p><p></p><p>最后，我们对今天的分享做个总结，我们分享了百度智能云在数据库上云迁移和数据同步/集成方向的设计思路和落地实践。</p><p></p><p>在上云迁移部分，百度智能云提供了平滑可靠的一站式上云迁移服务。</p><p></p><p>迁移服务开箱即用，支持评估、迁移、校验、回滚的全流程托管。兼容源和目标不同引擎、架构、版本和网络环境，业务改造成本低。上云迁移无需停服，业务影响小，迁移和回滚链路支持端到端的故障恢复</p><p></p><p>在同步/集成部分，DTS 可以提供易用稳定的数据同步/集成服务：</p><p></p><p>在异地多活场景中，基于 DTS 双向同步可以支持数据库多主架构，实现全球化服务访问加速，保证数据全局一致。在多云灾备场景中，DTS 支持数据库跨云长期同步，支持端到端容灾自愈，数据一致性有保证可校验。在数据集成场景中，DTS 立足流批一体设计，支持端到端的自由组合，可以实现秒级实时同步，未来计划支持实时数据加工。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d4ec5fc7ab0f57eb95a01f2e0a38f7e.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8mTxLzES8Hrx7gHH1x3t</id>
            <title>不会写代码同学的福音——AI 代码生成器 Amazon CodeWhisperer（通过注释写代码）</title>
            <link>https://www.infoq.cn/article/8mTxLzES8Hrx7gHH1x3t</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8mTxLzES8Hrx7gHH1x3t</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 07:50:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚马逊云科技, 代码生成器, 机器学习, 安全漏洞
<br>
<br>
总结: 本文介绍了亚马逊云科技的代码生成器CodeWhisperer，它是一个以机器学习为动力的工具，可以在集成开发环境中为开发者提供实时代码建议。CodeWhisperer可以根据开发者正在编写的代码生成相关建议，范围从一行代码到一个完整的函数。此外，CodeWhisperer还可以扫描代码中的安全漏洞，并提供相关链接获取更多信息。它可以帮助开发者更快、更安全地编写代码，提高生产力和代码质量。 </div>
                        <hr>
                    
                    <p>本文转载经亚马逊云科技授权</p><p></p><p>Amazon CodeWhisperer 是一个以机器学习为动力的代码生成器，直接在集成开发环境（IDE）中为开发者提供实时代码建议。它是一个通用的工具，可以用于 IDE 支持的任何编程语言。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/5f/62/5fc7579f0caa538c19a4dfc9beb69462.png" /></p><p></p><p>大家可以通过下面的链接进入注册并使用：&nbsp;<a href="https://aws.amazon.com/cn/codewhisperer/?trk=cndc-detail">AI 代码生成器 - Amazon CodeWhisperer - 亚马逊云科技</a>"</p><p></p><p>CodeWhisperer 是在一个庞大的开源代码数据集上训练出来的，它使用这些数据来生成与你目前正在编写的代码相关的建议。这些建议的范围可以从一行代码到一个完整的函数。</p><p></p><p>CodeWhisperer 还可以扫描你的代码是否存在安全漏洞。它通过将你的代码与已知漏洞的数据库进行比较来实现这一目的。如果CodeWhisperer发现一个潜在的漏洞，它将标记代码，并为你提供一个链接，以获得更多关于该漏洞的信息。</p><p></p><p>CodeWhisperer是一个强大的工具，可以帮助你更快、更安全地编写代码。它可以免费提供给个人开发者，它也可以作为 Amazon CodeStar Pro 订阅的一部分。</p><p>以下是使用亚马逊CodeWhisperer的一些好处：</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e6/ce/e69b3730c7666b22bca28183fe3db9ce.png" /></p><p></p><p>提高安全性： CodeWhisperer 可以通过扫描你的代码的潜在漏洞来帮助你写出更安全的代码。这可以帮助你避免昂贵的安全漏洞和数据丢失。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e7/1c/e7275344167f34db90ac7f0922ff041c.png" /></p><p></p><p>减少错误： CodeWhisperer 可以通过为您提供准确和相关的代码建议来帮助您减少代码中的错误数量。这可以节省你的时间和挫折感，并且可以帮助你提高代码的质量。</p><p></p><p>如果你是一个正在寻找提高生产力、安全性和准确性的方法的开发者，那么你应该考虑使用 Amazon CodeWhisperer。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/53/2c/5364346c169fb14aacc9704b1c332b2c.png" /></p><p></p><h3>使用收藏夹工具</h3><p></p><p></p><p>CodeWhisperer 符合您的工作方式。从 15 种编程语言中进行选择，包括 Python、Java 和 JavaScript，以及您最喜欢的集成式开发环境（IDE），包括 VS Code、IntelliJ IDEA、Amazon Cloud9、Amazon Lambda 控制台、JupyterLab 和 Amazon SageMaker Studio。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8d/b9/8dc9315aa02ffb72ab2ef8b2bf2a89b9.png" /></p><p></p><h3>开发人员工作效率的巨大飞跃速度提高&nbsp;57%</h3><p></p><p></p><p>在预览期间，Amazon 举办了一场生产力挑战赛，使用 Amazon CodeWhisperer 的参与者成功完成任务的可能性要比未使用 CodeWhisperer 的参与者高 27%，平均完成任务的速度快 57%。</p><p></p><p>Amazon CodeWhisperer，一个实时的人工智能编码伴侣，普遍可用，还包括一个 CodeWhisperer 个人层，所有开发人员都可以免费使用。CodeWhisperer 最初是在去年推出的预览版，它使开发人员保持状态和生产力，帮助他们快速和安全地编写代码，而不需要离开他们的IDE去研究什么，打破他们的流程。面对为复杂和不断变化的环境创建代码，开发人员可以通过在他们最喜欢的 IDE（包括Visual Studio Code、IntelliJ IDEA 和其他 IDE）中使用 CodeWhisperer 来提高他们的生产力并简化他们的工作。</p><p></p><p>CodeWhisperer 有助于为常规的或耗时的、无差别的任务创建代码，使用不熟悉的 API 或 SDK，正确有效地使用 Amazon API，以及其他常见的编码场景，如读写文件、图像处理、编写单元测试等。</p><p></p><p>只需使用一个电子邮件账户，您就可以注册，并在短短几分钟内提高编写代码的效率，而且您甚至不需要成为亚马逊云科技的客户。对于企业用户，CodeWhisperer 提供了一个专业层，增加了管理功能，如 SSO 和 IAM 身份中心的整合，对参考代码建议的策略控制，以及对安全扫描的更高限制。除了为 Python、Java、JavaScript、TypeScript 和 C# 生成代码建议外，普遍可用的版本现在还支持 Go、Rust、PHP、Ruby、Kotlin、C、C++、Shell 脚本、SQL 和 Scala。在 Visual Studio Code、IntelliJ IDEA、CLion、GoLand、WebStorm、Rider、PhpStorm、PyCharm、RubyMine和DataGrip IDE中工作的开发人员可以使用 CodeWhisperer（当这些 IDE 安装了适当的亚马逊云科技扩展时），或在Amazon Cloud9 或 Amazon Lambda 控制台中使用。</p><p></p><p>帮助开发人员保持他们的流程越来越重要，因为面对越来越多的时间压力来完成他们的工作，开发人员往往被迫打破这种流程，转向互联网搜索、StackOverflow 等网站或他们的同事来帮助完成任务。虽然这可以帮助他们获得所需的启动代码，但这是一种破坏性的做法，因为他们不得不离开他们的IDE环境去搜索或在论坛上提问，或寻找和询问同事--进一步增加了干扰。相反，CodeWhisperer 在开发者最有效率的地方与他们见面，在他们在IDE中写代码或评论时实时提供建议。在预览期间，我们进行了一次生产力挑战，使用CodeWhisperer 的参与者成功完成任务的可能性增加了27%，并且比不使用 CodeWhisperer 的参与者平均快了57%。</p><p></p><h3>从评论中生成代码</h3><p></p><p></p><p>然而，开发人员最终找到的代码可能包含一些问题，如隐藏的安全漏洞，有偏见或不公平，或未能负责任地处理开放源代码。当开发者后来不得不解决这些问题时，这些问题不会提高他们的工作效率。在安全编码和负责任地使用人工智能方面，CodeWhisperer 是最好的编码伙伴。为了帮助你负责任地编码，CodeWhisperer 过滤掉可能被认为有偏见或不公平的代码建议，而且它是唯一可以过滤或标记可能类似于特定开源训练数据的代码建议的编码伴侣。它为建议提供额外的数据--例如，存储库的URL和许可证--当生成与训练数据相似的代码时，有助于降低使用代码的风险，使开发人员能够放心地重新使用它。</p><p></p><h3>开源参考资料追踪</h3><p></p><p></p><p>CodeWhisperer 也是唯一具有安全扫描功能的人工智能编码伴侣，可以为难以发现的漏洞寻找和建议补救措施，扫描生成的和开发人员编写的代码，寻找漏洞，如开放网络应用安全项目（OWASP）中列出的前十名。如果它发现了一个漏洞，CodeWhisperer 会提供建议来帮助补救这个问题。</p><p></p><p></p><h3>漏洞扫描</h3><p></p><p></p><p>CodeWhisperer 提供的代码建议不是专门针对与亚马逊云科技合作的。然而，CodeWhisperer 针对最常用的 Amazon API 进行了优化，例如 Amazon Lambda 或亚马逊简单存储服务（Amazon S3），使其成为在亚马逊云科技上构建应用程序的最佳编码伙伴。虽然 CodeWhisperer 为各种语言的通用用例提供了建议，但使用 Amazon API 的额外数据进行的调整意味着你可以确信它是最高质量、最准确的代码生成，你可以获得与亚马逊云科技合作的机会。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BpQDOgbRKscRmQ6ARycy</id>
            <title>专注数据基础设施，Alluxio 如何让 AI 和数据价值全面释放？</title>
            <link>https://www.infoq.cn/article/BpQDOgbRKscRmQ6ARycy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BpQDOgbRKscRmQ6ARycy</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:33:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Alluxio, AI, 数据编排, 数据治理
<br>
<br>
总结: Alluxio是一家提供AI和大数据基础设施方案的公司，他们的最新产品Alluxio Enterprise AI和Alluxio Edge针对AI和大数据领域的热门问题进行了技术创新，旨在提升企业应用的加速效果和成本效益。Alluxio通过数据编排和数据治理解决了企业在不同数据平台之间的鸿沟问题，提高了数据访问效率。Alluxio Edge是他们新增的重磅特性，通过分层存储方案和去中心化对象存储库架构，提升了AI训练的IO效率，减少了计算节点的闲置浪费。这些创新技术使得企业能够更好地管理和利用数据，提升数据多源管理效率。 </div>
                        <hr>
                    
                    <p>12 月 9 日，AI 和大数据基础设施方案提供商 Alluxio 联合北京大学计算机学院等单位举办了 2023 全球 AI 前沿科技大会北京站，介绍了最新产品 Alluxio Enterprise AI 与为 Alluixo Enteprise Data 开发的重磅特性 Alluxio Edge。作为数据编排领域的先行者，Alluxio 的最新产品与特性瞄准了市场上最热门的 AI 与大数据主题，希望通过数据编排这一关键工作流环节的技术创新，为企业的相关应用带来显著的加速效果和成本效益提升。</p><p></p><p>在大会主题演讲中，Alluxio 创始人兼 CEO 李浩源将 2023 年定义为 Alluixo 机器学习与人工智能的开启元年。李浩源表示，Alluixo Enterprise AI 将打破 AI 数据治理的“不可能三角”，而 Alluixo Edge 则会大幅提升企业大数据分析平台的效能，他希望新产品与新特性能够像 Alluxio 以往的创新一样得到业界广泛使用，从而助力各行业数据和 AI 价值的全面释放。</p><p></p><p>1 Alluxio Enterprise AI：在恰当的时间获取正确的 AI 数据</p><p></p><p>回顾 Alluxio 的发展历史，这家公司从创业以来一直专注于填补企业不同数据平台之间的鸿沟。实践中，企业往往会选择、部署多个数据平台，各类应用（数据消费者）需要从不同的来源获取数据，不仅增加了复杂性，数据传输效率也往往不尽如人意。Alluxio 则将市面上常见的数据源和消费接口统一到自研的数据编排层上，负责屏蔽不同来源与输出接口的差异性，同时通过数据缓存优化方案来提升热点数据的访问效率。由此以来，即便企业部署了很多数据存储方案，甚至有很多数据部署在全球多个物理区域，企业应用又需要通过多种 API 访问这些数据，Alluxio 也能让整个流程的效率和便利性接近本地单数据源方案的水平。</p><p></p><p>凭借数据编排领域的先行优势，发展近 10 年的 Alluxio 已经成为业内广为人知、广泛应用的核心基础设施应用。在云计算快速普及的浪潮中，由于云端服务和产品普遍开始引入存算分离设计，加之混合云、多云、跨云环境逐渐成为主流，Alluxio 的能力得到了普遍认可。显然，取得成功的 Alluxio 并没有就此止步，面对 2023 年的生成式 AI 变革，这家公司迅速响应，推出了 Alluxio Enterprise AI 这样一款直击企业痛点的新品。</p><p></p><p>如今，大规模 AI 应用已经成为各行业的前沿必争之地，每一个细分领域都有企业开发自己的大模型技术或生成式 AI 应用，并为此投入大量资源组建庞大的计算集群用于训练和推理任务，即便芯片短缺造成硬件成本飙升也拒绝退缩。但在集群开始运行后，企业管理者经常尴尬地发现成本高昂的硬件平台实际算力利用率总是偏低，换句话说数量可观的算力资源是处于闲置浪费状态的。</p><p></p><p>造成这种现象的原因有很多，包括软件优化、计算错误、IO 性能不足等，其中 IO 瓶颈是造成集群空转的非常重要的因素。一般来说，企业用于训练和推理模型的数据也是来自多个数据源的，很多数据存放在不同的云服务中，当计算集群从这些数据源获取数据时，很容易遭遇带宽低下、延迟较高的困境，使计算芯片的宝贵时间白白浪费在等待数据这一环节上，这种情况有时甚至可以造成超过 50% 的计算节点空转现象，换句话说企业的 AI 基础硬件设施投资有一半都被浪费了。</p><p></p><p>为了解决这个问题，Alluxio 提出了一种分层存储方案。在硬件层面，Alluxio 将每个计算节点的本地存储当成速度较快的缓存，缓存访问失败后才会访问最后的云端数据源：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4ec90fe83495567fc3b6913e525e9b6a.png" /></p><p></p><p>Alluxio 将这种设计称为去中心化对象存储库架构（DORA）。AI 训练流程开始后，Alluxio 会自动选出训练热点数据，从云端复制到每一个训练节点的内部存储上。由于节点内部存储的性能远超云端，这样的设计大大提升了 IO 效率，官方宣称可以提供 2-4 倍的训练性能提升。更为诱人的是，获得如此大的收益并不需要企业额外购买大量硬件，Alluxio 只是充分利用了现有计算节点闲置的存储空间来加速 IO 而已，堪称“四两拨千斤”。</p><p></p><p>当然，要从海量数据中准确挑选出热点数据，还要为每一个计算节点分配应有的训练数据，尽量减少缓存未命中情况，避免从云端访问数据是这一方案设计中的最大难点。Alluxio 宣称，自己凭借多年以来数据编排领域的丰富经验，可以通过少量的处理节点轻松应对数以千亿计的存储对象，获得相比云端存储数十倍的元数据访问性能，而分布在计算节点上的存储则能支持 TB 级的总带宽与毫秒级的访问延迟。在获得如此强大能力的同时，由于企业无需采购昂贵的全闪存存储硬件来加速 IO，整体成本也能下降一半甚至 2/3，最终突破数据治理的“不可能三角”。</p><p></p><p>Alluxio Enterprise AI 的另一大优势，在于它能够将机器学习引擎与不同的存储系统连接起来，并跨区域和跨云将数据虚拟化，以简单和统一的方式使得大规模数据应用访问和管理来自不同数据源的数据，进而消除数据冗余，避免管理多个数据副本、减少对专用网络和存储硬件的依赖，无论数据位于何处都可以灵活地在任何位置部署计算，充分利用计算资源。Alluxio 还支持云原生容器化自动部署，完全适配 Pytorch、Tensorflow 等机器学习框架，可以做到上层引擎“无感知”，训练脚本“零改动”，数据准备“无拷贝”，数据清理“全自动”，显著降低部署和运维成本，使得企业在消除 AI 数据 IO 瓶颈的同时，获得一个大幅提升数据多源管理效率的治理平台。</p><p></p><p>Alluxio Enterprise AI 所承诺的收益对于正在大举进军生成式 AI 产业的企业而言无疑是极具诱惑力的：不需要额外的大笔硬件投资，不需要复杂的软件技术栈改动，也不需要开发和运维团队耗费大量时间学习掌握，只需一套接近开箱即用的解决方案就能轻松撬动 50% 甚至更多的闲置计算资源，附送高效率的数据管理能力，这样的前景如此美好，甚至令人难以置信。不过 Alluxio CEO 李浩源在大会上表现出了充足的信心，可以推测该公司对于 Alluxio Enterprise AI 的市场前景是非常看好的。</p><p></p><p>2 Alluxio Edge 星翼，为 Alluxio Enterprise Data 新增的重磅特性</p><p></p><p>大会上，李浩源详细介绍了 Alluxio Edge（中文名星翼），被认为是公司对现有 Alluxio Enterprise Data 产品新增的重磅特性。</p><p></p><p>具体而言，星翼是与 PrestoDB 和 Trino 应用程序搭配使用的一个库，它可以利用 PrestoDB 或者 Trino 集群的本地存储空间来缓存数据。当大部分热数据能够放在本地磁盘中时，这个库可以带来最佳的效率和成本效益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/563f660a10c8c2dc2c72a3a406f4b3cc.png" /></p><p></p><p>简单来说，如果用户的数据分析框架只需要从一个单区域云数据源获取数据，且热点数据量并不大时，就可以使用星翼来利用节点本地存储的性能。由于星翼的体量足够轻，它对企业数据架构的影响也是最小的，然而它带来的性能提升依旧非常显著，包括端到端查询的性能提高约 1.5 倍到 10 倍，10 到 50 倍的 IO 吞吐量提升，云存储 API 的调用也能减少 50% 到 90%，底层存储的负载同样可以大幅下降。</p><p></p><p>Alluxio 原有的 Alluxio Enterprise Data 则更适用于混合云、多区域、多计算环境。在混合云或多区域环境中，Alluxio Enterprise Data 具有免复制机制，访问时只提取和缓存必要的数据，无需将大型数据集从云端完整复制到本地，减少 I/O 时间和成本，并缩短了分析所需的端到端时间；在多计算环境中，Alluxio Enterprise Data 可充当不同计算集群之间的高性能分布式缓存，使得多个应用的数据访问更加高效，并能轻松实现横向扩展。显然，星翼是 Alluxio Enterprise Data 面向单一区域和计算场景的重要能力补充。当企业数据架构较为简单时，使用星翼就能立刻获得巨大收益；当企业业务扩展导致数据架构随之更新后，就可以平滑升级到 Allxuio Enterprise Data 来满足更多场景的需求，从而进一步扩大 Alluxio 在这一领域的优势地位。</p><p></p><p>3 加速智算应用，Alluxio 前景值得期待</p><p></p><p>目前全球排名前 10 的互联网公司中有 9 家在使用 Alluxio，并在科技、金融、电信等行业得到广泛应用。能够取得这样的成绩，主要归功于 Alluxio 选择了一条能够给企业带来明显价值，同时又被很多人忽视的细分领域赛道。经过近十年的发展，Alluxio 在数据编排领域的地位已经非常牢固，今年发布的两款新品正是这家公司厚积薄发的成果与已有优势的延伸。</p><p></p><p>考虑到各行业都越来越重视数据与 AI 的应用和价值，Alluxio 产品的适用领域也将不断扩大。随着 Alluxio Enterprise AI 的推出与成熟，它很可能会在生成式 AI 革命中成为企业数据基础设施不可或缺的组成部分。正如李浩源所言，AI 和数据价值的全面释放，离不开更智慧、更强性能、更经济高效的计算能力与基础设施平台的强力支撑。Alluxio 将扮演企业至关重要的数据平台角色，为企业智算应用插上翅膀，大幅提升业务效率，助力企业决胜未来。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3</id>
            <title>22人估值20亿美元，半年增长七倍，“欧洲 OpenAI”发布媲美GPT3.5的“开放权重”模型</title>
            <link>https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SjiWBCDHGt6kClScsea3</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 06:00:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Mistral AI, 生成式AI, Mixtral 8x7B LLM, 开放权重模型
<br>
<br>
总结: Mistral AI 是一家总部位于巴黎的初创公司，成功筹集了3.85亿欧元的资金，估值达到20亿美元。该公司推出了名为Mixtral 8x7B的开放权重模型，性能优秀，成本和延迟较低。Mistral AI还发布了开放平台La plateforme，并上架了三款模型，提供给其他公司使用。尽管公司规模较小，但其成就令人瞩目。 </div>
                        <hr>
                    
                    <p>Mistral AI 是一家总部位于巴黎的初创公司，由 Meta 和谷歌的研究人员于七个月前创立。目前，该公司已成功筹集 3.85 亿欧元（约合 4.15 亿美元），再次凸显了人们对生成式AI的浓厚兴趣。</p><p>&nbsp;</p><p>据两位知情人士透露，这笔交易将该公司的估值提升至约 20 亿美元，而该公司目前拥有 22 名员工。投资者阵容中有硅谷风险投资公司 Andreessen Horowitz 和 Lightspeed Venture Partners，还包括Salesforce、法国巴黎银行等众多投资机构。</p><p>&nbsp;</p><p>令人瞩目的是，这家初创公司的估值在短短的六个月内增长了七倍以上。仅在今年夏季，公司就成功完成了一轮 1.05 亿欧元（约合 1.13 亿美元）的种子资金融资，当时公司的估值约为 2.6 亿美元。</p><p>&nbsp;</p><p>同时，Mistral AI 还推出了新型 Mixtral 8x7B LLM。这款模型被称为“权重开源（open weights）”模型，设定了新的性能标准，并在其商业平台开放了访问。</p><p>&nbsp;</p><p></p><h2>媲美GPT3.5的“开放权重”模型</h2><p></p><p>&nbsp;</p><p>Mistral AI 发布了其名为Mixtral 8x7B的新模型，与Meta的Llama 2和OpenAI的GPT-3.5模型相比性能更佳。测试结果显示，Mixtral的性能与其他两个选项相当，甚至更为出色，并且成本和延迟更低。</p><p>&nbsp;</p><p>Mistral AI 官方宣称，这是一种高质量稀疏专家混合模型 (SMoE)，可以在 Apache 2.0 许可证下用于商业用途。并且，Mixtral 在大多数基准测试中都优于 Llama 2 70B，推理速度提高了 6 倍。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/549ef74dcc7282cd545b6ec69750edbe.jpeg" /></p><p></p><p>&nbsp;</p><p>Mistral AI 公司特别强调，“它是最强大的开放权重模型，具有宽松的许可证，也是成本/性能权衡方面的最佳模型。特别是，它在大多数标准基准测试中匹配或优于 GPT3.5。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0debded25b961853a9c9b9304c47afa4.png" /></p><p></p><p>截图源自：<a href="https://mistral.ai/news/mixtral-of-experts/">https://mistral.ai/news/mixtral-of-experts/</a>"</p><p>&nbsp;</p><p>Mixtral 具有32k token 上下文，可以处理英语、法语、意大利语、德语和西班牙语，代码生成表现出色。同时发布了 Instruct 版本的微调模型，MT-Bench 8.3 分。</p><p>&nbsp;</p><p>Mistral 表示，Mixtral 共 46.7B 参数，但每 token 仅使用 12.9B，意味着等同于 12.9B 的推理速度和成本。</p><p>&nbsp;</p><p>AI 领域的玩家已经开始下载、运行、尝试 Mixtral 8x7B，并对其性能和成本优势赞不绝口：</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/517b6804eff70bca3f1faf9986e8d5b7.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af45adffd5f5f4a29668e6ccb40b8b17.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>然而，值得注意的是，在官方给出的测试结果中有一个缺失，即TruthfulQA，通常用于测试法学硕士不重复常见在线错误信息的能力。尽管如此，Mistral仍强调，与OpenAI和Meta的选项相比，其模型的运行成本要低得多，这是一个明显的优势。</p><p>&nbsp;</p><p></p><h2>开放平台</h2><p></p><p>&nbsp;</p><p>同一天，Mistral还发布了其开放平台La plateforme，并上架了三款模型。</p><p>&nbsp;</p><p>Mistral-tiny：最具成本效益，目前提供 Mistral 7B Instruct v0.2，它是 Mistral 7B Instruct 的更新小版本。Mistral-tiny 仅适用于英语，在 MT-Bench 上获得 7.6 分。</p><p>&nbsp;</p><p>Mistral-small：Mixtral 8x7B，能处理英语/法语/意大利语/德语/西班牙语和代码，并在 MT-Bench 上获得 8.3 分。</p><p>&nbsp;</p><p>Mistral-medium：最高档原型模型，能处理英语/法语/意大利语/德语/西班牙语和代码，并在 MT-Bench 上获得 8.6 分。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/75ff3416c94fe002f73e169979c546d8.png" /></p><p></p><p>&nbsp;</p><p>该公司同时提供了 embed endpoint，一个具有 1024 嵌入维度的嵌入模型，设计有检索能力，MTEB 55.26 分。</p><p>&nbsp;</p><p>开源并不意味着 Mistral AI 回避商业化。虽然Mistral AI 有两个模型可以直接下载，但他们的最佳模型现在只能通过 API 访问：该公司计划从其基础模型中赚钱。这就是 Mistral AI 今天开放其开发者平台测试版的原因。有了这个平台，其他公司将能够通过 API 付费使用 Mistral AI 的模型。</p><p>&nbsp;</p><p>“我们的 API 遵循我们最亲爱的竞争对手最初提出的流行聊天界面的规范。我们提供了 Python 和 Javascript 客户端库，以查询我们的终端节点。”</p><p>&nbsp;</p><p>“每个 endpoint 都在性能和价格之间进行了不同的权衡。”</p><p>&nbsp;</p><p></p><h2>公司小，但令人瞩目</h2><p></p><p>&nbsp;</p><p>Mistral AI也被称为“欧洲 OpenAI”，由来自 Meta Platforms 和 Alphabet 的几位前研究人员 Arthur Mensch（现任 CEO）、Guillaume Lample 和 Timothee Lacroix 共同创立，公司成立于 2023 年 5 月，专门开发大语言模型及各类 AI 技术。Mistral 这个名号来自北方寒冷的季风，也体现了他们想要在 AI 领域占据一席之地的愿望。</p><p>&nbsp;</p><p>6 月，Mistral AI在拿下 1.13 亿美元巨额种子融资后引发业界轰动，公司估值也瞬间来到 2.6 亿美元。彼时，该公司刚刚成立，员工仅 6 人，还未做出任何产品，仅仅凭借着 7 页 PPT 就斩获了巨额融资。</p><p>&nbsp;</p><p>虽然Mistral AI目前人员数量也只有二十来人，却以较小的规模成功地获得了20亿美元的估值，并轻松地推出了性能最高的7B模型和 8x7B MOE模型。“我认为这可能对OpenAI来说是一个比Google或Anthropic更大的潜在威胁。”Hacker News网友评论。“考虑到最近的大额投资，我认为他们将能够a）在不久的将来扩展到应对合理的流量负载，b）吸引最顶尖、最聪明的研究人员，并以各种惊人和戏剧性的方式引起这个行业的关注。”</p><p>&nbsp;</p><p>Mistral 公司 CEO、前 DeepMind 研究科学家 Mensch 表示，这家企业的使命是“打造出能够解决现实世界问题的下一代 AI 系统”，并在创立之初就坚定了开源路线。他们于今年9月发布了自家首个大模型 Mistral 7B，该模型号称是“最强 7B 开源模型”。</p><p>&nbsp;</p><p>英伟达Senior Research Scientist Jim Fan评论说，Mistral 成功要素之一就是成立时机无可挑剔：诞生在开源和闭源争议中，并由精干团队推动。</p><p>&nbsp;</p><p>另外，每个月都会有几十款模型问世，但能引起大众向往的很少，而7B 和 7B-MoE（相当于 12B 密集）却对基层 AI 工程师来说更为友好，更容易构建。而且作为欧洲“本土化”的语言模型，Mistral AI也做到了差异化发展。可以说，该公司强大的初始团队和雄心勃勃的发展目标，已经使其成为当前乃至未来几年中最值得关注的 AI 初创力量之一。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://mistral.ai/news/mixtral-of-experts/">https://mistral.ai/news/mixtral-of-experts/</a>"</p><p><a href="https://mistral.ai/news/la-plateforme/">https://mistral.ai/news/la-plateforme/</a>"</p><p><a href="https://twitter.com/DrJimFan/status/1734269362100437315">https://twitter.com/DrJimFan/status/1734269362100437315</a>"</p><p><a href="https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html">https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html</a>"</p><p><a href="https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5">https://www.infoq.cn/article/V0ykFE4HYFlbNA0vbcE5</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FrR3xm21zRTfZYHbufGA</id>
            <title>企业数智化进阶模型，大型企业实现数智融合的成功之“道”</title>
            <link>https://www.infoq.cn/article/FrR3xm21zRTfZYHbufGA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FrR3xm21zRTfZYHbufGA</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 03:49:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据, 智能, 数智化底座, 数智融合
<br>
<br>
总结: 数据是企业提升竞争力的养分，而智能则是将养分输送到企业各个角落的管道。通过将数据与智能融合，大型企业可以焕发活力与效能，其中数智化底座扮演着重要角色。然而，大型企业在数智融合过程中面临着组织庞大、业务多样、系统流程复杂等挑战。解决这些挑战需要建立宏观视野，构筑转型的行动框架和方案模型。用友提出的企业数智化进阶模型“企业数智化1-2-3”可以帮助企业更明晰便捷地实现数智化。此外，大型企业在实现智能运营时需要面对数据来源和人才配备等挑战，因此需要一个内建了生成式AI能力的数智化底座来支持智能运营的实现。 </div>
                        <hr>
                    
                    <p>数据是助力企业持续提升竞争力的养分，智能则是将养分输送到企业每一个角落的管道。通过数据与智能融合，大型企业将焕发全新的活力与效能，其中数智化底座扮演着重要角色。&nbsp;&nbsp;</p><p></p><p>1 数智融合是必经之路，也是企业普遍面临的重大挑战</p><p></p><p>近年来，随着智能化技术的飞速发展，尤其是以生成式 AI 为代表的技术步入普及应用。通过将数据与智能有效融合的方式，加速数智化应用落地、拓宽数智化场景的深度和广度已成为行业共识。一方面，企业可以通过数据深度洞察自身、洞察产业和竞争对手，另一方面，智能化技术可以基于企业的数据积累，大大提升企业传统工作流程的效率，显著减少人力需求，为企业的日常高效运营和长期决策提供支持。例如，企业运营和销售部门可以借助 AI 工具，通过专为本行业开发的大模型分析企业积累的运营数据，从中挖掘总结成本优化、客户偏好、潜在市场机遇等信息，从而在迅速变化的市场环境作出及时响应；智能化工具还能让企业内部实现数据平民化，让非技术背景的员工也能更加便利灵活地运用数据资源等等。经过有效的数智化过程，企业降本增效、开拓市场、改善利润空间也就水到渠成。</p><p></p><p>然而，对于大型企业而言，数智融合的目标与成果虽然令人振奋，但企业达成目标的道路上却充满荆棘。究其原因，大型企业往往组织庞大、业务多样、系统流程纷繁复杂。虽然积累了大量运营和业务数据，但这些数据通常都是无序存放，缺乏科学有效的治理机制，因此也很难在实践中发挥其应有的作用。</p><p></p><p>由于数据治理体系不过关，企业即便开始部署智能化应用，也时常面临缺少高质量数据支撑的尴尬境地，智能化场景就成为无源之水，无法真正为业务提供助力。另一种情况下，企业上层虽然在努力推进数智化转型，但一线部门和员工却并不清楚数智融合如何落地到实践场景中，投入大量资源采购的工具、技术，培养的技术人才最后只是在 IT 部门“空转”，难以同应用场景有效对接。这些问题都是企业，尤其是大型企业在数智化转型过程中面临的挑战，只有解决了这些挑战，才能为数智化铺平道路，让企业最终摘下数智融合与转型的成功果实。</p><p></p><p>有道无术，术尚可求，有术无道，则止于术。企业在数智化转型实践中的常见误区，就是先追求微观、细节的“术”，指望采购一些零散的工具和技术就能有效治理数据、落地智能应用。然而，尤其对于大型企业而言，在数智化进程初期就建立宏观视野，构筑转型的行动框架、方案模型是必不可少的步骤。</p><p></p><p>有了这样的行动框架做指引，企业才能自上而下建立清晰的转型预期、实施细则与评价和改进回路，并实现跨部门的高效协作，避免各自为政、懒散躺平、朝令夕改、资源浪费等常见问题。在科学的实施框架引导下，企业逐步完成搭建数据治理体系、浓缩高质量数据、构筑智能化应用、对接业务场景等任务，脚踏实地构筑起牢固的数智化底座，打开持续迭代进化的发展通道，转型成功也就顺理成章。</p><p></p><p>2 企业数智化进阶模型，企业实现数智融合的成功之“道”</p><p></p><p>企业数智化转型是一个综合、复杂、循序渐进的系统工程和长期过程，需遵循科学、合理的行动框架和进阶模型。行万里路不如明师指路，在企业领域，各行业面临的数智化转型难题大都是相通的，实践中的解决方案也是近似的。用友深耕企业服务市场三十五年，也一直在基于众多行业客户的领先实践归纳总结各类行之有效的模型和方法论，并通过企业场景的应用与反馈不断优化。针对企业的数智化进程，用友同样归纳出了一条成功之“道”，即“企业数智化1-2-3”，也被称为企业数智化进阶模型，可以服务企业更明晰便捷地实现数智化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/39182fb93d77a0a060b9208e712d70bd.png" /></p><p></p><p>“数智化1”是企业要推进“云化连接”（上云），实现业务的云化部署、网络连接（含物联网）和实时感知；“数智化2”是企业要推进“数据驱动”（用数），实现全面数据服务，统一数据治理，并升级数智底座；“数智化3”是企业要推进“智能运营”（赋智），实现业务运营智能化、自然化人机交互和知识与应用生成。</p><p></p><p>从这三个阶段可以看出，数据与智能是一直相伴存在的。随着企业对应用需求的不断加深，数据与智能所提供的能力也需要越来越丰富，相应的对其底层支撑平台的要求也越来越高。如今大多数企业的数智化处于“数智化2”&nbsp;数据驱动层级，企业需要升级数智化底座，实现全面数据服务。</p><p></p><p>例如，有些企业基于用友 iuap 平台构建数智化供应链，通过深入挖掘离线数据以实现采购流程的可视化与风险识别，从而实时调整采购流程，对供应商进行画像评价。还有些企业使用用友 iuap 平台来构建数智化人才供应链，发现企业内部人才核心节点，定位意见领袖、关键人才岗位，并利用内部社交和邮件数据分析判别离职意向，从而辅助 HR 部门提前采取行动，降低人员流失，保留核心人才。进入数据驱动的企业可以将现有的数据沉淀为知识图谱，例如资产维修的知识图谱就可以帮助经验不足的资产维修人员快速定位和解决维修问题。</p><p></p><p>通过这些举措，企业的数据在运营生产的各个环节都发挥出了应有的作用，为企业贡献了可观的价值。</p><p></p><p>随着大模型和生成式 AI 技术的崛起，有部分领先的大型企业开始向智能运营进阶。但在这一过程中企业往往面临两大挑战：首先，大模型将扮演企业数智化底座核心操作系统的角色，为此需要更加广泛、高质量的数据来源，不仅包括了企业已有的基础数据，还需要更多产业级、社会化的数据资源。其次，有了这些产业级的数据支撑，大模型还需要对企业业务场景的深刻理解，才能有效应用在实际业务中。企业需要更多同时理解大模型技术与企业所处业务领域知识的复合型人才来运用大模型的能力，这就会对企业的人员配备、岗位体系乃至整个企业的组织模式带来巨变。</p><p></p><p>在数智化进阶模型的指引下，大型企业的数智化进程有“道”可依。此时，如何解决实现智能运营的两大挑战就成了关键问题，企业开始需要“术”层面的支持和帮助。</p><p></p><p>3 用友 iuap，企业数智化实现智能运营之“术”</p><p></p><p>对于大多数企业而言，生成式 AI 是一个全新的技术领域，企业的管理层和 IT 部门在这一领域都没有足够的知识和经验。正因如此，当企业试图自行构建智能运营体系时，就需要独自面对跨领域、社会化数据的治理、整合、标注、模型选择、模型训练、场景适配等诸多陌生问题。</p><p></p><p>尤其在生成式 AI 超越企业内部层面，纳入社会化能力的过程中，企业自有的 IT 和业务团队往往是力不从心的。而如果没有与企业业务充分融合的大模型能力，智能运营也就成为了空中楼阁。显然，大型企业尤其需要更高水平的、内建了生成式 AI 能力的数智化底座，在这样的底座基础上才能大范围落地智能应用，实现从数据驱动到智能运营的全面转变。</p><p></p><p>从数据驱动到智能运营，用友 iuap 为企业提供全面支持</p><p></p><p>生成式 AI 技术在企业落地，需要有成熟的数智化底座承接才不会陷入无人会用、无场景可用的尴尬，而企业数智化底座用友 iuap 平台为企业提供了所需的底座基础能力，在应用、数据、智能等多个维度支持企业业务快速创新。</p><p></p><p>应用层面，在智能运营层级，企业需要部署智慧化灵动应用，如智能助理、智能预算等。数智员工就是企业智能助理的一种形式，通过数智员工可以解决企业流程自动化、审批智能化、内容合规化、数据驱动语义化等问题。数智员工具备智能交互与自主学习能力，比如通过 AI 与 RPA 深度融合,AI 具备 ChatGPT 类的交互、学习能力，自动识别流程风险、自动学习审批，使得流程风险更可控，审批更智能。在工作流中引入智能审批助理，可以大幅提高工作效率，提升公司产出效能。通过数字人、技能、AI、业务流、对话流工场化设计，可以快速实现所见即所得。企业还可以根据自己的场景创作个性化形象、个性化能力的数智员工。</p><p></p><p>数据层面，处于智能运营层级的企业需要拥有更丰富的产业 / 社会级数据资源，相应的数据服务需要覆盖从展现级到分析级、控制级、决策级、创新级（如产品优化）的全部五层数据服务。</p><p></p><p>用友 iuap 数据中台通过数据移动、开发、治理、指标、挖掘、语义模型、数字大屏、移动分析、智能分析云、智能报告等功能大大简化了数据的采集、加工、治理和应用流程，为企业提供了一站式数据底座，支撑数据驱动的各类场景应用。用友 iuap 的数据服务能力已经涵盖了各个层面。</p><p></p><p>比如，某食品加工集团，基于用友 iuap 构建了企业“业务中台、数据中台、智能中台”三位一体的企业数智化底座，推动业务智能化应用，实现企业化智能化运营。该集团通过构建统一客户视图，对重点、关怀、风险、异动等客户群体，实施不同的营销策略，实现精准营销。基于约束理论最优化目标函数，结合遗传算法构建了排产优化模型，通过优化排产，降低企业生产成本。建立了风险预测模型，通过对现金流动性、利率敏感性、资本充足率、市场风险暴露值、异常交易、信用风险等指标和场景进行实时监控、及时预警。</p><p></p><p>智能层面，数智化处于智能运营层级的企业，其智能化进入了慧知层，全面应用企业服务大模型。</p><p></p><p>为了普及 AI 在企业的应用，用友于 2023 年 7 月发布了业界首个企业服务大模型 YonGPT。因为，用友在此前服务企业过程中，已经产生了大量的商业应用数据，这些数据对于企业而言是非常宝贵的资产，在 AI 技术的加持下，可以发挥更大价值。YonGPT 不仅可以通过上下文记忆、知识 / 库表索引、Prompt 工程、Agent 执行、通用工具集等扩充大模型的存储记忆、适配应用和调度执行能力，还沉淀了财务、人力、供应链、采购、制造、营销、研发、项目、资产、协同等领域场景的知识和领先实践。通过将用友长期业务实践中积累的大量跨行业、社会化数据与知识进行训练，可以更好地理解企业业务，帮企业作出准确决策，实现智能运营。</p><p></p><p>YonGPT 已经在企业经营洞察、智能库存优化、智能人才发现、智能预算分析、代码生成、供应商风控等数十种场景完成智能化赋能。</p><p></p><p>YonGPT 作为用友在 AI 领域的最新成果，可以为企业提供更加智能、高效、便捷的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a70f33e78348426c13ee469a9ce5b61.png" /></p><p></p><p>用友 iuap 为企业带来了更多高度专业化、场景化、社会化的数据资源，并将这些资源与企业的私有数据有机结合，形成可复用的专业能力。在用友 iuap 平台的支持下，企业无需从零开始进行大规模的投资建设与人才团队培养，也能顺利跨越数智化转型第二阶段到第三阶段的难关，更早建成智能化的运营体系，最大程度发挥企业内部和外部的数据生产要素的潜在价值。</p><p></p><p>编后语：</p><p></p><p>当大部分企业的数智化还处在“数智化2”层级时，一些行业领先企业已经开始朝“数智化3”层级迈进。“数智化2”处于企业走向智能运营的关键阶段，需要企业做好全面的数据治理及数据服务，全面升级数智底座，才能在“数智化3”层级拥有夯实的数据及平台基础。用友 iuap 作为更懂业务、技术领先、体系完整的数智化底座，为大型企业带来数据、智能、平台全面保障，以此为企业业务和应用服务。并且在“数智化3”层级，用友 iuap 持续输出以企业服务大模型为中心的智能化能力，助力企业顺利实现智能化运营，以数智化持续推动企业高质量发展！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</id>
            <title>英特尔数据中心与人工智能事业部 AI 软件架构师何普江确认出席 QCon 上海，分享大模型时代：最大化 CPU 价值的优化策略</title>
            <link>https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IsCY7KLBPWL2XbXlt8qc</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 大模型时代, CPU 价值优化策略, CPU 和 GPU 投机采样方法
<br>
<br>
总结: 本文介绍了即将在上海举办的QCon全球软件开发大会，其中AI软件架构师何普江将分享关于大模型时代下最大化CPU价值的优化策略。演讲内容包括利用CPU的多核特性、并行计算和AMX指令集扩展技术来提高处理速度，以及结合CPU和GPU的投机采样方法来充分利用CPU资源并减少对GPU的依赖。通过这些优化策略，可以提高模型推理速度，实现生成式模型部署落地。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。英特尔数据中心与人工智能事业部 AI 软件架构师何普江将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5627?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">大模型时代：最大化 CPU 价值的优化策略</a>"》主题分享，探讨一种结合 CPU 和 GPU 的投机采样方法，在大语言模型时代充分利用 CPU 资源的关键策略，以及最新的性能情况，以便了解这些优化策略的实际效果。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5627?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1212&amp;utm_content=hepujiang">何普江</a>"，2007 年硕士毕业于中国科学技术大学。精通英特尔软件架构、英特尔产品与技术以及 IA 平台性能优化。在英特尔工作期间，为国内主流 ISV 开发出基于 IA 平台的云计算产品过程中提供关键支持，并优化了多家主要互联网公司的核心产品，使其性能提升数倍。对 PyTorch，Tensorflow 等 AI 框架有深入研究，并拥有 10 年以上软件优化经验。工作期间曾获得英特尔中国个人员工最高荣誉奖，与国内互联网厂商多个部门进行深度合作，并在 2019 年助力某云厂商云在 MLPerf 评测中创下了业界领先的 Performance/TOPS 性能记录。他致力于基于 IA 架构平台的深度学习、机器学习研究和在互联网行业的落地推广工作，最新工作包括创建并开源了 CPU 上大语言模型的极致优化方案 xFasterTransformer。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大模型时代：最大化 CPU 价值的优化策略</p><p></p><p>本次演讲将探讨在大语言模型时代充分利用 CPU 资源的关键策略。具体介绍一些结合硬件特性的优化方法，例如利用 CPU 的多核特性、采用并行计算和 AMX 指令集扩展技术来提高处理速度。</p><p></p><p>此外还将介绍一种结合 CPU 和 GPU 的投机采样方法，通过在 CPU 上运行部分计算任务，充分利用 CPU 资源并减少对 GPU 的依赖。最后，我将分享一些最新的性能情况，让您了解这些优化策略的实际效果。通过这些方法，您将能够更好地利用 CPU 资源，提高模型推理速度，以更快速高效的实现生成式模型部署落地。</p><p></p><p>演讲提纲：</p><p></p><p>大语言模型时代为什么需要最大化 CPU 价值CPU 上的大模型优化策略</p><p>○ 大语言模型计算特点 </p><p>○ CPU 硬件特性概览 </p><p>○ 优化方法 </p><p>○ 从向量化到张量化 </p><p>○ 从并行执行到分布式推理 </p><p>○ 低精度优化 </p><p>○ 深入 CPU 微架构的软件优化 </p><p>○ 各优化策略的实际性能数据对比及效果展示</p><p>结合 CPU 和 GPU 的投机采样方法</p><p>○ CPU 和 GPU 协同工作的背景 </p><p>○ 投机采样技术的介绍 </p><p>○ 利用 CPU 进行部分计算任务的优势 </p><p>○ 优化方法：选择合适的投机采样策略、任务调度等</p><p>总结与展望</p><p>○ 各优化方法的核心优势与局限性总结 </p><p>○ 对未来大语言模型时代的展望与挑战</p><p></p><p>听众收益点：</p><p></p><p>○ 理解并结合硬件特性进行优化，提高模型推理速度和处理能力</p><p>○ 了解 CPU 上的最新性能情况，为实际业务的大模型线上部署提供更多选择</p><p>○ 掌握结合 CPU 和 GPU 协同工作的优化策略，减少对 GPU 的依赖，提高资源利用率</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！9 折优惠仅剩最后 4 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</id>
            <title>英特尔高宇：AI工作负载有多种形态和规模，硬件上没有一刀切的解决方案</title>
            <link>https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hKUxUJeMmrcVrGyeDKGI</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 11:51:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, 大模型, 公共通用大模型, 个性化服务
<br>
<br>
总结: 去年年底以来，随着ChatGPT应用体验界面的推出，大模型的生成式AI技术得到了快速发展。公共通用大模型通过学习公共数据，可以生成高质量的文本、图像、声音和视频等内容，为智能创新和个性化服务提供了巨大的想象空间。为了保护数据安全和隐私，同时提供个性化服务，公共大模型和面向个人的专有大模型混合部署成为产业共识。 </div>
                        <hr>
                    
                    <p>去年年底以来，随着ChatGPT应用体验界面的推出，使得以大模型为主的生成式AI 技术取得了重大的并且快速地发展，大模型也展现出了令人惊叹的智能涌现能力，表现出了更为强大的创造性和通用场景的普通适用性，技术得以快速发展。</p><p></p><p>首先取得重大突破的是公共通用大模型，从人类社会大量存积下来的公共数据当中去学习，进而生成高质量的文本、图像、声音甚至是视频等内容，为各个领域的智能创新和每一个人的智能体验创新提供了巨大的想象空间。</p><p></p><p>然而，出于数据的安全和隐私保护的考虑，以及更高效率，更低成本来享用大模型通用能力的角度考虑，人们又既希望获得公共大模型目前的各类强大的通用服务，同时又希望AI 能够真正理解自己，提供专属的个性化服务，还要能够充分地保障个人的数据和隐私安全，为此，公共大模型和面向个人的专有大模型混合部署，正逐渐成为产业的一个共识。</p><p></p><p>在这样的时代背景下，作为消费和商用个体用户中最坚挺的终端，PC在AIGC时代承载了怎样的使命？</p><p></p><p>12月7日，首届AI PC产业创新论坛在北京联想总部举行。此次论坛汇聚了众多用户、终端厂商、算力厂商（芯片）、AI技术厂商（大模型）、应用领域生态合作伙伴，深度探讨AI PC为AI普惠带来的巨大改变。此外，在此次论坛上，业内首份《AI PC产业（中国）白皮书》重磅发布。</p><p></p><p>与会嘉宾认为，AI PC 到来之际，大模型将成为每一个人必不可少的助手，同时对推理的算力需求将超过训练的算力需求。算力集中于云端的模式变得不可持续，AI计算负载将逐渐由云端向边缘侧和端侧下沉。在搭建本地智能算力上，CPU+NPU+GPU 异构式架构方案是目前最为成熟的方案之一。</p><p></p><p>对此，英特尔中国区技术总经理高宇表示，AI工作负载有多种形态和规模。所以，从硬件上没有一刀切的解决方案。“基于多年的学习与市场经验，我们提出了XPU的概念，包括GPU/NPU/CPU。”他说，联想是英特尔的战略合作伙伴，双方已经基于即将发布的Meteor Lake处理器推进AI体验的开发和创新。</p><p></p><p>作为算力厂商的代表，英特尔正采取三项措施，来持续构筑端侧的算力。一是构建为AI而设计的高效能AI-Ready平台；二是提供工具以支持广泛的x86应用生态系统，三是激发创新，开启全新的AI体验，包括为软件和应用开发人员提供支持，以便在各个领域里都能更好将AI功能完美部署到PC客户端上。</p><p></p><p>英特尔今年还正式启动了首个“AI PC加速计划”，将在2025年前为超过1亿台PC带来人工智能特性。其中，通过与超过 100 家 ISV 合作伙伴深度合作，并集合 300 余项 AI 加速功能，英特尔将在音频效果、内容创建、游戏、安全、直播、视频协作等方面继续强化 PC 的体验。</p><p></p><p>据了解，在实践中，英特尔13代酷睿处理器已经可以流畅运行70亿到180亿参数的大模型，并成功部署了LLM。高宇表示，即将推出代号Meteor Lake的AI PC处理器，代表英特尔40年来最重大的架构转变，旨在为AI PC时代铺平道路。它是首个内置AI加速引擎NPU的处理器，可在PC上实现高能效的AI加速和本地推理。</p><p></p><p>为了完成用户相对复杂的任务，AI PC往往需要调动不同的模型和应用，为AI PC的能力进行补充和延伸。因此，AI PC功能的发挥不仅需要像英特尔这样的算力厂商的参与，还需要整个开放的行业生态作为支撑。</p><p></p><p>在AI PC的推动下，PC产业生态将从应用为本转向以人为本，用户成为行业生态创新的驱动者和创造者。模型、应用、算力厂商都需要围绕AI PC（终端）形态下新的以人为本的需求做出改变，在研发工作中对AI的高效运行予以充分的考量，以适应AI PC新时代。</p><p></p><p>联想作为终端厂商，是离用户最近的一端，因而被推到台前，成为生态组织者和生态的核心中枢。以场景需求为基础面向用户整合产业资源，承担AI PC技术整合创新交付者、新一代个人智能体及 AI入口创造者和用户体验维护者、本地化个人数据及隐私安全守护者和开放的AI应用生态标准制定者和推广者身份，职责重大。正是出于行业责任，联想联合国际数据公司IDC发布业内首份《AI PC产业（中国）白皮书》，对AI PC进行了全新定义，以加速构建AI PC产业新生态。</p><p></p><p>高宇最后表示，AI PC加速计划由即将发布的IntelCore Ultra处理器率先驱动。未来，英特尔将搭建性能并行和吞吐量适用于融合AI的媒体/3D/渲染的GPU，打造适用于持续的AI和分担AI负载的专用低功耗AI引擎NPU；迭代能够快速响应，适用于轻量级、单次推理的低延迟任务的CPU，相信在新平台的加持下，英特尔将加快与联想共同打造混合AI算力架构，驱动AI PC落地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</id>
            <title>零一万物Yi-34B-Chat 全球权威测评，开源黑马追平 GPT-3.5？</title>
            <link>https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6RfQJh7pxGm5FPJ2EFgB</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 10:08:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Yi-34B-Chat, 测评结果, AlpacaEval Leaderboard, LMSYS ORG排行榜, SuperCLUE排行榜
<br>
<br>
总结: Yi-34B-Chat是一款性能优异的大语言模型，在多个测评平台上取得了出色的成绩。在AlpacaEval Leaderboard和LMSYS ORG排行榜中，Yi-34B-Chat超越了其他竞争对手，成为世界范围内仅次于GPT-4的大语言模型。在中文能力方面，Yi-34B-Chat也取得了令人瞩目的成绩，在SuperCLUE排行榜中表现出色。该模型还提供了4bit/8bit量化版，方便在消费级显卡上使用。通过AI Alignment团队的创新对齐策略，Yi-34B-Chat不仅在理解和适应人类需求方面表现出色，还与人类价值观对齐。 </div>
                        <hr>
                    
                    <p></p><p>继11月初零一万物发布性能优异的 Yi-34B 基座模型后，Yi-34B-Chat 微调模型在11月24日开源上线，而各家测评平台也相继给出了Yi-34B-Chat 的测试结果。</p><p>&nbsp;</p><p>模型地址：</p><p><a href="https://huggingface.co/01-ai/">https://huggingface.co/01-ai/</a>"</p><p><a href="https://www.modelscope.cn/organization/01ai">https://www.modelscope.cn/organization/01ai</a>"</p><p>&nbsp;</p><p></p><h2>各家测评结果</h2><p></p><p>&nbsp;</p><p>在斯坦福大学研发的大语言模型评测 AlpacaEval Leaderboard 中，Yi-34B-Chat以94.08%的胜率，超越LLaMA2 Chat 70B、Claude 2、ChatGPT，在 Alpaca 经认证的模型类别中，成为世界范围内仅次于GPT-4 英语能力的大语言模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6a/6a9c4fc531e7575e5fcf65d475dfb716.png" /></p><p>&nbsp;</p><p>AlpacaEval Leaderboard排行榜（发布于2023年12月7日）</p><p>&nbsp;</p><p>同一周，在加州大学伯克利分校主导的LMSYS ORG排行榜中，Yi-34B-Chat也以1102的Elo评分，晋升最新开源SOTA开源模型之列，性能表现追平GPT-3.5。</p><p>&nbsp;</p><p>伯克利LMSYS ORG排行榜采用了一种最为接近用户体感的 “聊天机器人竞技场” 特殊测评模式，即让众多大语言模型在评测平台随机进行一对一 battle，通过众筹真实用户来进行线上实时盲测和匿名投票。11月份，经25000个真实用户投票计算了20个大模型的总得分。Elo评分越高，说明模型在真实用户体验上的表现越出色。</p><p>&nbsp;</p><p>在开源模型中，Yi-34B-Chat 在英语能力上进入前十。LMSYS ORG 在12月8日官宣11月份总排行时评价：“Yi-34B-Chat 和 Tulu-2-DPO-70B 在开源界的进击表现已经追平 GPT-3.5”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f3e7d606ee4b2ef7fdc44af2325e3a4.png" /></p><p>&nbsp;</p><p>LMSYS ORG榜单（发布于2023年12月8日）</p><p>&nbsp;</p><p>在针对中文能力的排行榜方面，SuperCLUE从基础能力、专业能力和中文特性能力三个不同的维度，评估模型的能力。根据11月底发布的《SuperCLUE中文大模型基准评测报告 2023》，11月下旬首度发布的 Yi-34B Chat，迅速晋升到和诸多国产优秀大模型齐平的 “卓越领导者” 象限。在多项基准评测中的 “SuperCLUE 大模型对战胜率” 这项关键指标上，Yi-34B-Chat 取得31.82%的胜率，仅次于GPT4-Turbo。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b11a91538b615aa1301ff0c4054d7137.png" /></p><p>&nbsp;</p><p>中文SuperCLUE排行榜（发布于2023年11月28日）</p><p>&nbsp;</p><p>值得注意的是，Yi-34B-Chat 微调模型为开发者提供了 4bit/8bit 量化版模型。Yi-34B-Chat 4bit 量化版模型可以直接在消费级显卡（如RTX3090）上使用。</p><p>&nbsp;</p><p></p><h2>真实场景如何</h2><p></p><p>&nbsp;</p><p>Yi-34B-Chat 模型实力在不同的对话场景中实力如何？来看几个更直观的问题演示。</p><p>&nbsp;</p><p></p><h4>知识与生成</h4><p></p><p>&nbsp;</p><p>问：Transformer 模型结构能不能走向 AGI ?</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d78f0a4dab09a17eb32fc8654b708191.png" /></p><p>&nbsp;</p><p></p><h4>创意文案</h4><p></p><p>&nbsp;</p><p>问：给我生成一个小红书文案，给大家安利一只豆沙色的口红。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be47558b3b174390b2de5ec944c40e0f.png" /></p><p>&nbsp;</p><p></p><h4>中文理解</h4><p></p><p>&nbsp;</p><p>问：小王给领导送了一份礼物后。领导说：“小王，你这是什么意思？”小王：“一点心意，意思意思。”领导：“你这就不够意思了。”小王：“小意思，小意思。”领导：“小王，你这人真有意思。”小王：“也没什么别的意思。”领导：“那我多不好意思。”小王：“是我不好意思。”这个意思到底是什么意思？</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4ffb796172e36d6124d32749bd91eed.png" /></p><p>&nbsp;</p><p>据零一万物介绍，除了 Yi 系列强基座的贡献以外，Yi-34B-Chat 模型的效果还得益于其人工智能对齐（AI Alignment）团队采用了一系列创新对齐策略。通过精心设计的指令微调流程，不仅强化了模型在理解和适应人类需求方面的能力，还使得模型与人类价值观对齐，包括帮助性（Helpful），可靠性（Honest），无害性（Harmless）等。</p><p>&nbsp;</p><p>在强基座设定下，该团队采用了一种轻量化指令微调方案，该方案涵盖了单项能力提升和多项能力融合两个阶段。</p><p>&nbsp;</p><p>其中，单项能力包括通用指令跟随、创意内容生成、数学、推理、编程、泛COT、对话交互等，团队通过大量的消融实验，针对模型单能力构建和多能力融合总结了独家认知经验；在多能力融合阶段，团队采用网格搜索的方法来决定数据配比和超参数的设置，通过基准测试和自建评测集的结果来指导搜索过程，成功实现模型的多能力融合。</p><p>&nbsp;</p><p>在数据的量和质方面，零一万物团队认为，数据质量比数量重要，少量高质量数据比大量低质量数据更好，仅需少量数据（几条到几百条）就能激发模型特定单项能力。团队通过关注超出模型能力的“低质量”数据，来减少了模型“幻觉”。</p><p>&nbsp;</p><p>在指令多样性与难度方面，团队在各能力项下构建任务体系，实现训练数据中的指令均衡分布，提升模型泛化性。此外，团队发现训练数据的风格会影响模型收敛速度和能力上限的逼近程度，因此统一了回复风格，比如重点设计了CoT的回复风格，实现在轻量SFT情况下，避免了风格不一致加剧模型的“记忆”现象。</p><p>&nbsp;</p><p></p><h2>开源“满月”：有赞扬，也有质疑</h2><p></p><p>&nbsp;</p><p>Yi模型发布之初便是开源的。开源首月，Yi模型在Hugging Face社区下载量为16.8万，魔搭社区下载量1.2万，在GitHub 获得超过4900个Stars。</p><p>&nbsp;</p><p>多家知名公司和机构推出了基于Yi模型基座的微调模型，比如猎豹旗下的猎户星空公司推出的OrionStar-Yi-34B-Chat模型、南方科技大学和粤港澳大湾区数字经济研究院（简称IDEA研究院）认知计算与自然语言研究中心（简称CCNL中心）联合发布的SUS-Chat-34B等。</p><p>&nbsp;</p><p>知名技术写作者苏洋表示，根据他的近期观察，Hugging Face榜单中的前三十名有一半多是 Yi 和其他用户微调的 Yi-34B 的变体模型，原本占据榜单头部的 68B 和 70B 模型的数量目前只留有几个。</p><p>&nbsp;</p><p>苏洋曾尝试使用家里本地的机器，在纯 CPU 环境、CPU &amp; GPU 混合环境下对Yi模型进行测试，“结果比想象中要好。尤其是社区中的 finetune 后的版本，在对新闻、研究报告的摘要总结方面，对非结构化的信息中的实体识别和抽取上表现非常不错。”同时，苏洋也指出，可能是由于零一在训练过程中，出于安全考虑，过滤太多语料的缘故，一些本土化的内容仍然不够深入。</p><p>&nbsp;</p><p>根据亲身体验，苏洋总结道，34B 普通用户努努力还是能自己相对低成本跑起来的，68 B 和 70B 的模型想要本地运行，需要更多的资源，但目前分数上跟 34B 的拉不开太多差距，大概就是三四分平均分。</p><p>&nbsp;</p><p>开源后，Yi系列模型也遭到了一些质疑。</p><p>&nbsp;</p><p>开发者Eric Hartford敏锐发现了模型存在的一个问题：Yi模型使用了与LLaMA模型完全相同的架构，只是将两个张量改了名字。由于围绕LLaMA架构有很多投资和工具，保持张量名称的一致性是有价值的。Eric建议，在Yi被广泛传播前，及时恢复张量名称。</p><p>&nbsp;</p><p>Eric没有预想到，他的这个建议引来了关于Yi模型“抄袭”LLaMA的质疑。</p><p>&nbsp;</p><p>之后，零一万物很快便在各开源平台重新提交模型及代码，完成了开源社区的版本更新。零一万物表示，一个模型核心技术护城河是在架构之上，通过数据训练获得的参数和代码。在沿用了开源社区普遍使用的LLaMA 架构之上，零一万物团队从零开始，用高质量的数据集、自研训练科学和AI Infra打造了 Yi-34B 在内的系列模型。为了执行对比实验的需要，对部分推理参数进行了重新命名。原始出发点是为了充分测试模型，而非刻意隐瞒来源。</p><p>&nbsp;</p><p>Eric后来发推特为<a href="https://twitter.com/erhartford/status/1724563655545503822">Yi辩护称</a>"，“他们没有在任何事情上撒谎。所有的模型都是在相互借鉴架构。架构是学术研究的产物，已经发表在论文中，任何人都可以自由使用，这丝毫不减损Yi团队的成就。他们从零开始使用自己创建的数据集训练Yi，对开源领域的贡献是值得赞扬的。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/aa/aad8cc193901261e7cc18122efe10bbf.png" /></p><p></p><p>他还补充道，“使用Llama架构没有任何问题。训练才是关键。Yi给了我们目前可获得的最佳模型，没有任何可抱怨的。”</p><p></p><p>更多阅读：</p><p><a href="https://www.infoq.cn/article/cVfuQaHVJ0SDPtP2jb7m?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">零一万物回应“套壳 Llama”争议：基于 GPT 研发，对模型和训练的理解做了大量工作</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</id>
            <title>首期“AI+软件工程”主题沙龙在京顺利举办</title>
            <link>https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jjMPiPwaP2ZASIWNVgH2</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 10:03:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 通用人工智能, 软件工程, AI技术
<br>
<br>
总结: 以大模型为核心的通用人工智能正在驱动着新一轮智能革命的持续演进，并给软件工程带来了新的发展契机。大模型等AI技术在软件研发过程中赋予了强大的智能化能力，软件研发不再只依赖于人类的智慧，而是与AI相结合使其过程更加高效、高质量、低成本，代码生成、代码补全等新能力推动智能化软件工程范围的延展。 </div>
                        <hr>
                    
                    <p>以大模型为核心的通用人工智能正在驱动着新一轮智能革命的持续演进，并给软件工程带来了新的发展契机。大模型等AI技术在软件研发过程中赋予了强大的智能化能力，软件研发不再只依赖于人类的智慧，而是与AI相结合使其过程更加高效、高质量、低成本，代码生成、代码补全等新能力推动智能化软件工程范围的延展。</p><p></p><p>为加强AI+软件工程领域的交流互通，推动行业多融合发展，2023年11月21日，由中国信息通信研究院人工智能研究中心、中国软件行业协会和应用现代化产业联盟联合主办，中国人工智能产业发展联盟AI for 软件工程（AI4SE）工作组承办的首期“AI+软件工程”主题沙龙在京成功举办，线上线下总观看量超过6万。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7d0396c7b40cb595093b3dca99513f5.webp" /></p><p></p><p>中国信通院人工智能研究中心负责人魏凯、中国软件行业协会副秘书长付晓宇分别发表致辞。魏所指出，软件行业是大模型生态的聚集地，智能化的融合可以提升软件工程的效率和创新能力。然而，机遇和挑战并存，中国信通院将围绕AI和软件工程全生命周期持续开展系列工作，与产业各方共同面对挑战。付秘书长表示，AI为软件工程带来了新思路新方法，软件工程领域也在积极应对AI带来的挑战，中国软件行业协会一直致力于推动应用现代化的发展，期待看到我们的行业在面对挑战时，能够以开放、合作的态度，共同寻找解决方案，同时在AI与软件工程交叉领域看到更多创新与突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3977d976034a22719df06830e8a4a84b.webp" /></p><p>中国信通院人工智能研究中心负责人魏凯</p><p></p><p><img src="https://static001.geekbang.org/infoq/d9/d917f57d13e9bd398a48a7bdd56b2fb2.webp" /></p><p>中国软件行业协会副秘书长付晓宇</p><p></p><p>本次沙龙邀请了来自华为、联通软件研究院、国金证券、软通动力、东吴证券、中软国际、中国信通院的7位行业专家，围绕落地方案、实践范式、问题与挑战、发展与趋势发表主题演讲。</p><p></p><p>华为技术有限公司软件工程专家贺美迅的分享主题是“Al辅助研发实践探索”。贺总围绕开发模式、关键挑战、参考经验和实践案例四个方面，对AI辅助研发的技术与过程进行深入浅出的分析。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/6229db16a334e1bd89cdb8b62a317ea3.webp" /></p><p>华为技术有限公司软件工程专家贺美迅</p><p></p><p>中国联通软件研究院软件架构师常红珍分享的主题是“代码生成模型及智能工具探索”。常总围绕诸多场景对代码生成大模型的探索与实践过程进行了剖析，通过将代码大模型与公域和私域数据相结合，构建智能体协作开发框架、智能体应用框架，并引入专家经验，实现结构化思考和优质代码的生成，突破代码片段的限制，提升软件工程效率，并对未来进行展望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cab7f0b18c7a1ae6dad24a0c0e3ec780.webp" /></p><p>中国联通软件研究院软件架构师常红珍</p><p></p><p>国金证券技术负责人李晨带来了证券业开发大模型探索与实践的分享。李总从政策监管、开发大模型背景和国金证券落地实践的维度进行了详细分析。国金证券在设计评审类、编码辅助类、测试辅助类场景中实践成效初显，平均效率提升达30%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3ea92cade67fff5dbcdfe44564e6824.webp" /></p><p>国金证券技术负责人/架构师李晨</p><p></p><p>软通动力助理副总裁孙洪军分享了软通动力AISE产品研发及实践。孙总首先介绍了软通动力的AIGC整体布局，其次介绍了软通动力AISE产品的设计背景、系统架构和核心能力，最后就产品落地实践和成效展开了分享，某应用企业通过使用该产品达到20%-30%的提效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5fbc4d69003ae2325968834ba0768965.webp" /></p><p>软通动力助理副总裁孙洪军</p><p></p><p>东吴证券信息技术总部副总经理任川分享了大模型训练和AI赋能的探索与实践。任总表示，AIGC大模型在证券行业具有广泛的应用前景，可在文案生成、智能搜索、券商智能中枢、BI助手等四大类关键领域上提供显著的提质增效服务能力。任总围绕东吴GPT分析其应用需求、应用领域和应用范式，并对未来AI规划进行分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e392a263efdd384abee9a9f764f81db7.webp" /></p><p>东吴证券信息技术总部副总经理任川</p><p></p><p>中软国际云智能业务集团CTO祁银红就中软在Al加速研发效能的实践和应用开发新模式的探索进行分享。祁总围绕中软的青燕平台，对探索过程与背景、核心能力及应用成效进行解析，该平台面向个人提供需求设计、文档生成、开发测试、代码生成等功能，面向组织提供全流程研发管理、测试管理、多级流水线等能力。以测试用例生成+故障排查为例，时间成本可缩短60%以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d005b0666ec73854052cb653092c49a.webp" /></p><p>中软国际云智能业务集团CTO祁银红</p><p></p><p>中国信通院云大所人工智能部主任曹峰发表了《智能化软件工程(AI4SE)发展现状与趋势》的主题演讲。曹主任从AI赋能软件工程的发展历程出发，介绍了当前软件工程相关大模型的现状、开发测试运维等场景的落地分析、智能化软件工程的关键技术与挑战，以及当前中国信通院在AI4SE领域开展的标准编制、案例征集等系列工作，最后对于AI4SE的多模态、全流程、新研发模式进行了展望。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b26a8b6d59080663da6d12efea6fae1.webp" /></p><p>中国信通院云大所人工智能部主任曹峰</p><p></p><p>沙龙的最后，应用现代化产业联盟"AI+软件工程"工作组正式成立，由华为、中国信通院、国金证券、联通软研院、软通动力、中软国际、国金证券、明源云等单位共同参与启动仪式。应用现代化产业联盟也欢迎更多有志于“AI+软件工程”研究的企业和伙伴加入到联盟和工作组中来，共建开放协同创新的软件生态体系，促进产业健康有序发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a27cd48dd6717b1ff89424d73d8a6033.webp" /></p><p>应用现代化产业联盟"AI+软件工程"工作组首批单位合影，左起：李晨（国金证券）、吴振亮（明源云）、孙洪军（软通动力）、曹峰（信通院）、王千祥（华为）、祁银红（中软国际）、常红珍（中国联通）</p><p></p><p>应用现代化产业联盟汇聚了产业各方力量，将会加速产业创新升级，促进产业跨越式增长，共同推动应用软件的技术提升，赋能企业开展应用现代化转型。同时也希望更多优秀的软件企业加入应用现代化产业联盟以及“AI+软件工程”工作组中来，助力擘画中国式现代化的宏伟蓝图。</p><p></p><p>应用现代化产业联盟 &amp;“AI+软件工程”工作组</p><p></p><p>黄老师：yigang.huang@ami-alliance.org.cn</p><p>李老师：Linda.lidandan@ami-alliance.org.cn</p><p></p><p>扫一扫，加入应用现代化产业联盟</p><p><img src="https://static001.geekbang.org/infoq/11/114d8750f1bdcb25a7345827b4776f9b.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</id>
            <title>计算机图形学将迎来新突破？Meta携手斯坦福大学推出3D交互模型，VR时代似乎不远了</title>
            <link>https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FOHDzHtEh5mTtyKyYDd8</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:52:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 斯坦福大学, Meta/Facebook AI研究, CHOIS系统, 3D人机交互
<br>
<br>
总结: 斯坦福大学与Meta/Facebook AI研究实验室共同开发的CHOIS系统是一套突破性的AI系统，能够根据文本描述在虚拟人和物体之间生成自然、协调的3D人机交互。该系统利用条件扩散模型技术生成精确的交互动作，能够模拟连续的人类行为。CHOIS系统的出现对计算机图形学、AI与机器人技术产生了深远影响，能够大大提高动画制作效率，实现更高水平的虚拟现实体验，并为机器人在服务型领域的应用提供了新的可能性。这一成果令人惊艳，前景值得期待。 </div>
                        <hr>
                    
                    <p>近日，斯坦福大学与Meta/Facebook&nbsp;AI研究（FAIR）实验室的工作人员共同开发出一套突破性的AI系统，能够仅根据文本描述在虚拟人和物体之间生成自然、协调的运动关系。</p><p>&nbsp;</p><p>这套新系统被称为CHOIS（Controllable Human-Object Interaction Synthesis，即可控人机交互合成），使用最新的条件扩散模型技术生成无缝且精确的交互，例如“将桌子举过头顶、行走，然后放下桌子。”</p><p>&nbsp;</p><p>简而言之，这是一套先进的人工智能系统，用于合成逼真的 3D 人机交互。</p><p>&nbsp;</p><p>这项工作被公布在arXiv论文预发表网站的一篇文章中，也让我们得以一睹虚拟人如人类般顺畅理解并响应语言命令的未来景观。例如，把椅子拉近桌子来创造一个工作空间，调整落地灯以投射出完美的光芒，或者整齐地存放手提箱。每一项任务都需要人、物体和周围环境之间的精确协调。语言是表达和传达这些意图的有力工具，在语言和场景背景的指导下，合成逼真的人类和物体运动是构建先进的人工智能系统的基石，该系统可以在不同的3D环境中模拟连续的人类行为。</p><p>&nbsp;</p><p>论文地址：<a href="https://arxiv.org/pdf/2312.03913.pdf">https://arxiv.org/pdf/2312.03913.pdf</a>"</p><p>&nbsp;</p><p>研究人员们在文章中指出，“根据语言描述在3D场景中生成连续的人-物交互一直存在不少挑战。”</p><p>&nbsp;</p><p>他们必须确保生成的运动真实且协调同步，保持人手与物体之间的适当接触，且物体的运行应当与人类行为具有因果关系。</p><p></p><h2>如何实现</h2><p></p><p></p><p>CHOIS系统之所以效果拔群，依靠的就是其在3D环境中摸索出一套独特的人-物交互合成方法。CHOIS的核心为条件扩散模型，这是一种能够模拟详尽运动序列的生成模型。</p><p>&nbsp;</p><p>当给定人/物位置的初始状态以及所需操作的语言描述之后，CHOIS就会据此生成一系列动作，最终完成任务要求的交互效果。</p><p>&nbsp;</p><p>例如，假设指令是将灯具移到沙发旁边，CHOIS会理解指令内容并创建一段逼真的动画，显示人类形象拿起灯具并将其放置在沙发附近。</p><p>&nbsp;</p><p>利用 AMASS 等大规模、高质量的运动捕捉数据集，人们对生成人体运动建模的兴趣有所上升，包括动作条件合成和文本条件合成。虽然之前的工作使用 VAE 公式从文本生成不同的人体运动，但 CHOIS 专注于人与物体的交互。与通常以手部运动合成为中心的现有方法不同，CHOIS 在物体抓取之前考虑全身运动，并根据人体运动预测物体运动，为交互式 3D 场景模拟提供全面的解决方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae3859d5e2f84eeb8a8685af1054800d.jpeg" /></p><p></p><p>给定初始对象和人类状态、语言描述和3D场景中的稀疏对象路径点，CHOIS生成的物体运动与人体运动同步。</p><p>&nbsp;</p><p>CHOIS的独特之处，就在于它使用稀疏对象路径点和语言描述来指导动画生成。各个路径点充当对象移动轨迹中的关键标记点，确保运动不仅符合物理规律，而且与语言输入中描述的高级目标保持一致。</p><p>&nbsp;</p><p>CHOIS的另一大优势，在于能够将语言理解能力与物理模拟功能加以结合。传统模型往往难以将语言同空间和身体动作联系起来，特别对于较大的交互范围，必须考虑诸多因素才能始终保持交互的真实性。</p><p>&nbsp;</p><p>CHOIS首先解释语言描述所承载的意图和风格，而后将其转化为一系列既符合人体构造、又不违背物体特性的肢体动作，从而解决了大范围交互过程中的这一现实难题。</p><p>&nbsp;</p><p>该系统尤其具有开创性的一点，就是它能准确表现接触点（例如手与物体之间的接触位置），且物体的运行与人类化身施加的力保持一致。此外，该模型在训练和生成阶段还引入了专门的损失函数和指导性术语，旨在强制遵循这些物理约束，这也是让AI成功实现以人类方式理解物理世界、并与物理世界正确交互的重要一步。</p><p></p><h2>对计算机图形学、AI与机器人技术的影响</h2><p></p><p></p><p>CHOIS系统对计算机图形学产生了深远影响，特别是在动画和虚拟现实领域。通过让AI获得解释自然语言指令并据此生成逼真人机交互过程的能力，CHOIS能够大大减少制作复杂场景动画所需要的时间和精力。</p><p>&nbsp;</p><p>动画师们可以使用这项技术来创建出以往极为费时费力的关键帧动画序列，显著提升设计效率与成果产出。此外，在虚拟现实环境当中，CHOIS还能带来更加身临其境且高度交互的体验，由用户通过自然语言指挥虚拟角色，并观察其以逼真精度执行任务的全过程。这种更高水平的交互能够将VR体验从僵化、脚本化的事件转化为更加顺畅自然的动态环境用户输入响应效果。</p><p>&nbsp;</p><p>在AI和机器人领域，CHOIS则代表我们朝着更加自主的情境感知系统迈出的一大步。传统机器人往往受到预编程例程的限制，而CHOIS这类系统的出现能够帮助其更好地理解现实世界、并顺利按照自然语言给出的描述完成任务。</p><p>&nbsp;</p><p>这对于医疗保健、酒店或家庭环境下的服务型机器人来说尤其有着变革性的影响。在这类环境下，理解物理空间并在其中执行各类任务的能力往往至关重要。</p><p>&nbsp;</p><p>对于AI来说，这种同时处理语言和视觉信息以引导任务执行的能力，也使其距离充分理解情境和环境上下文又更进了一步。而且在此之前，这种能力一直是人类的优势和专利。在CHOIS的支持下，未来的AI系统有望在更多复杂任务中发挥更大的作用，不仅能够消化人类指令的“内容”、更能理解人类指令的操作“方式”，以前所未有的灵活性适应新的挑战。</p><p></p><h2>成果令人惊艳，前景值得期待</h2><p></p><p>&nbsp;</p><p>CHOIS代表了人工智能领域的重大飞跃，特别是在计算机视觉和人机交互领域。通过综合 3D 人与物体交互，CHOIS 可以生成逼真的动画和场景，这对于创建沉浸式虚拟体验至关重要。</p><p>&nbsp;</p><p>该系统使用组合分层方法来理解人类与物体之间交互的复杂本质。这涉及将交互分解为更小的、可管理的部分，并理解这些部分之间的关​​系。模型的层次结构使其能够考虑交互的上下文，例如环境和所涉及对象的属性。</p><p>&nbsp;</p><p>CHOIS 由深度学习算法提供支持，深度学习算法是机器学习的子集。这些算法使系统能够从人与物体交互的大型数据集中学习，随着时间的推移提高其准确性和预测能力。</p><p>&nbsp;</p><p>总体而言，斯坦福大学和Meta的研究人员在计算机视觉、自然语言处理（NLP）和机器人技术交叉领域的这一极具挑战的问题上，成功取得了关键进展。</p><p>&nbsp;</p><p>研究团队认为，他们的工作是建立先进AI系统的重要一步，该系统能够在不同的3D环境中模拟连续的人类行为。CHOIS也为进一步研究如何利用3D场景加语言输入来合成人机交互过程打开了大门，有望在未来孕育出更加复杂的AI系统。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://venturebeat.com/ai/stanford-and-meta-inch-towards-ai-that-acts-human-with-new-chois-interaction-model/">https://venturebeat.com/ai/stanford-and-meta-inch-towards-ai-that-acts-human-with-new-chois-interaction-model/</a>"</p><p><a href="https://isp.page/news/chois-stanford-and-fair-metas-revolutionary-ai-for-realistic-3d-human-object-interactions/#gsc.tab=0">https://isp.page/news/chois-stanford-and-fair-metas-revolutionary-ai-for-realistic-3d-human-object-interactions/#gsc.tab=0</a>"</p><p><a href="https://www.marktechpost.com/2023/12/10/researchers-from-stanford-university-and-fair-meta-unveil-chois-a-groundbreaking-ai-method-for-synthesizing-realistic-3d-human-object-interactions-guided-by-language/">https://www.marktechpost.com/2023/12/10/researchers-from-stanford-university-and-fair-meta-unveil-chois-a-groundbreaking-ai-method-for-synthesizing-realistic-3d-human-object-interactions-guided-by-language/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</id>
            <title>百度8500万挖不来“AI教父”；淘天年薪百万起步抢全球顶尖人才，上不封顶；王慧文病休后首次动作：AI投资｜Q资讯</title>
            <link>https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uJ79bU5Wreox7MGdKmLQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:07:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里, 年度股息, 淘天集团, 顶尖人才, 百度, AI教父, 比尔盖茨, 收入, 谷歌, Gemini大模型, 王慧文, OneFlow团队, 腾讯视频
<br>
<br>
总结: 阿里将首次派发年度股息，总额近179亿；淘天集团抢全球顶尖人才，年薪百万起上不封顶；百度8500万挖“AI教父”被拒，选择入职谷歌；比尔盖茨每天收入1095万美元，约普通人一生收入4倍；谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑；王慧文病休后首次动作，入股OneFlow团队新创业项目；腾讯视频出现故障，引发用户不满。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>阿里将首次派发年度股息，总额近179亿；淘天集团抢全球顶尖人才，年薪百万起上不封顶；百度8500万挖“AI教父”被拒，选择入职谷歌；比尔盖茨每天收入1095万美元，约普通人一生收入4倍；谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑；王慧文病休后首次动作，入股OneFlow团队新创业项目；卷入300亿骗局官司，京东回应：这是一个匪夷所思的恶意诉讼……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>阿里将首次派发年度股息，总额近179亿</h4><p></p><p>&nbsp;</p><p>12月6日，阿里发布公告，将向截至2023年12月21日收市时登记在册的普通股持有人和美国存托股持有人，就2023财年首次派发年度股息。金额分别为每股普通股0.125美元或每股美国存托股1.00美元，以美元支付。根据披露，股息总额约为25亿美元（当前约179亿元人民币）。阿里称，在现有股份回购计划基础上继续努力提高股东回报。</p><p>&nbsp;</p><p>此次面向全体股东的派息决定，是阿里巴巴2014年上市美股，以及2019年再次回归港股以来首次大规模分红派息。据梳理，阿里巴巴成立以来就以保守的财务经营策略闻名，包括本次分红派息在内，仅有3次分红派息记录。</p><p>&nbsp;</p><p></p><h4>淘天集团抢全球顶尖人才，年薪百万起上不封顶</h4><p></p><p>&nbsp;</p><p>近日，淘天集团启动一项名为T-Star的顶尖人才招聘计划，发放的offer不设层级，采取定制化培养模式，配备“大牛”主管和顶级研发平台资源，年薪百万起上不封顶。</p><p>&nbsp;</p><p>根据淘天集团招聘官网公布的信息，目前，T-Star计划已经开放了10种算法工程师岗位，工作方向包括自然语言处理、机器学习、多模态、三维重建、计算机视觉、3D等，工作地点为杭州、北京等。</p><p>&nbsp;</p><p></p><h4>百度8500万挖“AI教父”被拒，选择入职谷歌</h4><p></p><p>&nbsp;</p><p>12月4日，据知情人士透露，百度公司曾出价1200万美元(约合8486万元人民币)邀请“AI教父”杰弗里·辛顿(Geoffrey Hinton)及其学生加入公司，但被拒绝。“我们不知道自己值多少钱。”辛顿表示。他咨询了收购方面的律师和专家，想出了一个计划：“我们将组织一场拍卖，自己兜售自己。”</p><p>&nbsp;</p><p>最终，辛顿博士和他的学生们在4400万美元(约合3.1亿元人民币)的价格上停止了这次拍卖。虽然出价仍在上升，但他们想为谷歌工作。这一报酬已经很惊人。据悉，今年5月辛顿宣布从谷歌离职。辛顿表示，从谷歌辞职是为了可以自由地谈论AI的风险。他说，现在对自己一生从事的工作感到有些后悔。</p><p>&nbsp;</p><p></p><h4>比尔盖茨每天收入1095万美元，约普通人一生收入4倍</h4><p></p><p>&nbsp;</p><p>根据求职信息网站Zippia的数据，一个普通人一生的平均收入约为270万美元，而比尔·盖茨一天的收入大约是这个数字的3~4倍。据预计，盖茨每天的收入约为1095万美元，相当于每秒约117美元。还有另外一组数据显示，盖茨每天进账约760万美元，相当于每小时319635美元。</p><p>&nbsp;</p><p></p><h4>谷歌发布自己“最强”Gemini大模型遭质疑：演示视频疑似剪辑</h4><p></p><p>&nbsp;</p><p>谷歌 12 月 6 日宣布推出其认为规模最大、功能最强大的人工智能模型 Gemini。Gemini 将包括三种不同的套件：Gemini Ultra、Gemini Pro 和 Gemini Nano。根据谷歌给出的基准测试结果，Gemini 在许多测试中都表现出了“最先进的性能”，甚至在大部分基准测试中完全击败了 OpenAI 的 GPT-4。</p><p>&nbsp;</p><p>同时，谷歌也发布了Gemini Ultra官方演示视频，展示了这款模型的强大能力。不过，依然有人质疑Gemini的能力。</p><p>&nbsp;</p><p>据报道，Gemini在MMLU多任务语言理解数据集测试中显示出色，但对比GPT-4时的提示技巧和展示方式引发了争议。质疑者认为，Gemini在使用提示技巧+32次尝试的标准下超越了GPT-4，但这一标准是否公平受到质疑。图表比例尺的问题也被揭示，引起了技术主管的修正。Gemini发布的视频在展示时也引起了关注，部分观点认为其中可能存在剪辑和非实时录制。</p><p>&nbsp;</p><p>查看更多：</p><p><a href="https://mp.weixin.qq.com/s/Yqi4rcyEmvg9g6LCqbxYxA">刚发布就被质疑？超过 GPT-4 的“最强”大模型 Gemini、“最高效”训练加速器，谷歌到底行不行</a>"</p><p>&nbsp;</p><p></p><h4>王慧文病休后首次动作，入股OneFlow团队新创业项目</h4><p></p><p>&nbsp;</p><p>在病休近6个月后，王慧文突然有了新动作，再次与袁进辉牵手，入股其创业新公司硅动科技。据公开资料显示，就在这两日，北京硅动科技有限公司新增王慧文为股东，注册资本由100万人民币增至约105.26万人民币。也就是说，王慧文目前在袁进辉新公司的持股比例约为5%。</p><p>&nbsp;</p><p>OneFlow是国内知名开源深度学习框架及开发平台。其团队上次创业一流科技时，由王慧文的光年之外收购其46.52%股权。不过，随着6月底光年之外创始人王慧文病退消息曝光，美团收购光年之外100%的权益，一流科技OneFlow团队作为其核心资产也转归美团名下。在50天后，袁进辉宣布带领OneFlow原班人马再次创业。消息传出后不到半个月，硅动科技注册成功。</p><p>&nbsp;</p><p></p><h4>“腾讯视频崩了”上热搜</h4><p></p><p>&nbsp;</p><p>12 月 3 日晚，腾讯视频出现网络故障，有网友反馈出现首页无法加载内容、VIP 用户看不了会员视频等情况。#腾讯视频崩了# #腾讯会员 没了#词条相继冲上微博热搜。</p><p>&nbsp;</p><p>稍晚些时候，@腾讯视频就“App 崩了”发布致歉声明称，目前腾讯视频出现了短暂技术问题，我们正在加紧修复，各项功能在逐步恢复中。感谢您的耐心等待，由此给您带来的不便我们深感歉意。</p><p>&nbsp;</p><p>除了腾讯视频，近期遭遇宕机事件的还有滴滴、淘宝、闲鱼、钉钉、阿里云盘等多个App。据媒体统计，以此被多家媒体报道或登上热搜榜为基准，2022年约发生了9起宕机事件，而今年以来，类似的事件已发生14起。</p><p>&nbsp;</p><p>更多阅读：</p><p><a href="https://mp.weixin.qq.com/s/-KVKVfq0CayLyRkEbp2Rcg">互联网大厂“组团”宕机，都怪降本增“笑”？</a>"</p><p>&nbsp;</p><p></p><h4>被卷入300亿骗局官司，京东回应：这是一个匪夷所思的恶意诉讼</h4><p></p><p>&nbsp;</p><p>12月4日消息，最近京东集团、承兴集团、诺亚财富之前的各种消息闹得沸沸扬扬，京东还被告上法庭。据悉，此事起因是承兴集团的罗静造假，冒充京东工作人员，并私自刻章，以京东、苏宁等应收账款来找金融机构（诺亚财富）贷款，最终骗走300亿并跑路暴雷，结果被抓。诺亚财富一纸诉状把京东给告上法庭，想让京东还钱。</p><p>&nbsp;</p><p>对于此事，京东集团官微“京东发言人”最新发布一份声明回应，称京东作为毫不知情的受害者，被卷入历时四年的恶意诉讼中，公司的声誉和权益遭受重大损失，相信法院会有公正的判决。12月4日晚，诺亚财富发布声明称，已关注到京东集团发布的关于我司的声明，该声明中“诺亚财富近年来先后发生十余起类似事件，上百亿基金......”等描述严重失实，已侵犯了我司名誉，我司将采取法律措施，维护自身合法权益。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>小米14手机内核已在GitHub开源</h4><p></p><p>&nbsp;</p><p>据报道，小米14/Pro内核现已在Github开源，AOSP版本基于Android U。公开内核源码可以让第三方开发者进行修改，开发人员和愿意折腾的用户能够充分利用硬件的潜力，市场上也会很快出现该机型的第三方固件。</p><p>&nbsp;</p><p>开源地址：</p><p><a href="https://github.com/MiCode/Xiaomi_Kernel_OpenSource/tree/shennong-u-oss">https://github.com/MiCode/Xiaomi_Kernel_OpenSource/tree/shennong-u-oss</a>"</p><p>&nbsp;</p><p></p><h4>员工称亚马逊AI聊天机器人Q “幻觉”严重，且泄露公司机密数据</h4><p></p><p>&nbsp;</p><p>根据国外科技媒体披露的一份内部文件，亚马逊员工称旗下AI聊天机器人Q存在严重的“幻觉”问题，并泄露了包括AWS数据中心位置、内部折扣计划等诸多机密信息。报告文件显示，亚马逊Q会产生幻觉，返回有害或不适当的聊天内容。例如，亚马逊Q会返回过时的安全信息，可能会让客户面临风险。</p><p>&nbsp;</p><p>亚马逊淡化了员工讨论的重要性，并声称没有发现任何安全问题。然而，泄露的文件引发了人们对Q准确性和安全性的担忧，Q仍处于预览阶段，尚未正式发布。文章发表后，该发言人发布一份声明，反驳了员工的说法，称亚马逊Q没有泄露机密信息。</p><p>&nbsp;</p><p></p><h4>Meta 推出独立的 AI 图像生成器，目前免费但只支持英文提示词&nbsp;</h4><p></p><p>&nbsp;</p><p>Meta 公司日前推出全新的、独立的 AI 图像生成器 ——Imagine with Meta，允许用户通过自然语言描述来创建图像。据介绍，新的人工图像生成器由 Meta 现有的 Emu 图像生成模型提供支持，可根据文本提示创建高分辨率图像。它目前对美国的英语用户免费使用（后续是否收费未知），并且每个提示都会生成四个图像。</p><p>&nbsp;</p><p>此前，Meta 图像生成模型因带有种族偏见的图像贴纸而面临争议。为了解决此类问题，Meta 表示将开始向 Imagine with Meta 生成的图像添加隐形水印，这些水印将由人工智能模型生成，并可由相应模型检测，以提高内容透明度。</p><p>&nbsp;</p><p></p><h4>Hugging Face 现 API 令牌漏洞，黑客可获取微软、谷歌等模型库权限</h4><p></p><p>&nbsp;</p><p>安全公司 Lasso Security 日前发现 AI 模型平台 Hugging Face 上存在 API 令牌漏洞，黑客可获取微软、谷歌、Meta 等公司的令牌，并能够访问模型库，污染训练数据或窃取、修改 AI 模型。由于平台的令牌信息写死在 API 中，因此黑客可以直接从 Hugging Face 及 GitHub 的存储库（repository）获得平台上各模型分发者的 API 令牌（token），安全人员一共从上述平台中找到 1681 个有效的令牌。</p><p>&nbsp;</p><p></p><h4>支付宝、麦当劳中国等相继启动鸿蒙原生应用开发</h4><p></p><p>&nbsp;</p><p>12月7日，支付宝与华为终端宣布合作，基于HarmonyOS NEXT启动支付宝鸿蒙原生应用开发，华为常务董事、终端BG CEO、智能汽车解决方案BU董事长余承东和蚂蚁集团董事长兼首席执行官井贤栋均现身签约现场。</p><p>&nbsp;</p><p>无独有偶，12月6日，麦当劳中国也与华为达成鸿蒙合作协议，正式宣布麦当劳中国APP将基于HarmonyOS NEXT启动鸿蒙原生应用开发，成为首批启动鸿蒙原生应用开发的全球大型跨国连锁餐饮企业，该公司在中国市场拥有5500多家餐厅，每年服务顾客超十亿人次。</p><p>&nbsp;</p><p>随着华为宣布鸿蒙原生应用全面启动，近期美团、去哪儿、新浪、钉钉、蚂蚁集团、小红书、58集团、哔哩哔哩、高德地图等互联网公司均已宣布加入鸿蒙原生应用开发行列。</p><p>&nbsp;</p><p></p><h4>IntelliJ IDEA 2023.3 版本更新发布</h4><p></p><p>&nbsp;</p><p>IntelliJ IDEA 2023.3 版本更新现已发布，在这一版本中，JetBrains 表示 AI Assistant 持续演进，现已超越技术预览阶段，获得了大量令人期待的改进。在其他方面，此版本包括对最新 Java 21 功能的全面支持，引入了带有编辑操作的直观浮动工具栏，并添加了 Run to Cursor（运行到光标）嵌入选项来增强调试工作流。IntelliJ IDEA Ultimate 现在提供无缝的开箱即用 Kubernetes 开发体验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</id>
            <title>分布式数据库 GaiaDB-X 金融应用实践</title>
            <link>https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2VG5NR6sg8QFttMMyQw5</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 07:03:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 银行新一代核心系统建设背景, 架构, 分布式架构, 分布式数据库
<br>
<br>
总结: 银行业务的快速发展和迭代更新速度加快，使得原有的基于大型机的核心系统架构无法满足需求。为了支持业务的持续增长和创新，银行开始将核心系统从大型机下移到通用服务器架构上，并采用分布式架构和分布式数据库来满足分布式扩展性、强一致性和容灾能力的要求。这使得银行核心系统的架构与互联网公司的技术体系越来越接近，未来银行业与互联网业的技术交流和人才流动将进一步增加。 </div>
                        <hr>
                    
                    <p></p><h2>一、银行新一代核心系统建设背景及架构</h2><p></p><p></p><p>在银行的 IT 建设历程中，尤其是中大行，大多都基于大型机和小型机来构建核心系统。随着银行业务的快速发展，这样的系统对业务的支持越来越举步维艰，主要体现在以下四个方面：</p><p></p><p>首先是难以支持银行快速发展的业务。随着国内电商、互联网支付、手机支付的迅猛发展，银行的交易量出现指数级的增长。比如我们的某银行客户，当前每秒的交易量峰值在一万多左右，预计在未来几年会逐渐增长到每秒 6 万笔交易，而且后续还会继续增长。在这种情况下，基于大型机的集中式架构，单纯靠硬件的升配，已经无法支持业务的持续增长。第二是难以匹配银行系统的迭代更新速度。原有的基于大型机的胖核心架构，迭代周期往往在数月甚至半年。但随着银行间、以及银行与互联网金融企业之间的竞争加剧，银行迫切需要快速推出新业务进行创新，他们也希望能够像互联网公司那样，能够按周级进行快速迭代，快速响应业务需求。第三是系统风险。银行业迫切需要做到软件及硬件的自主可控。第四是生态封闭。大型机技术发展慢、人才难招。现在再去外面招一个懂 IBM 大型机的人已经很难了。</p><p></p><p>因此，在国家现有政策的引导下，各大银行最近几年都在做一个事情：把原有的核心架构从大型机下移到传统的通用服务器架构上，并建设一套自主可控的新技术体系，简称核心系统下移。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2eebfc9abdbfbf4124d2a6b76ebcb3c.png" /></p><p></p><p>在进一步理解银行系统之前，我们先了解下银行客户的业务体量、业务需求以及核心系统对业务支持情况。以一个国有大行来举例：它的客户量在 5-7 亿，有 10-20 亿账户，在全国有 2-4 万个网点。从每秒峰值交易量来看，约为每秒 5-8 万笔交易。</p><p></p><p>具体到数据库层，支持以上业务还需要联机交易系统，例如存贷汇业务。数据库层最大的表有百亿级记录数，TPS 达到百万级。此外，统一查询业务要求支持近十年交易明细的查询，即万亿级的查询记录数。即使放到互联网公司用的通用服务器，也是需要上千台服务器才能支撑相关业务的开展。</p><p></p><p>通过以上的介绍，大家可以发现，银行客户的业务体量和数据量已经达到了大型互联网公司的量级。如果想把这个系统从大型机下移到通用服务器架构，那么原有的集中式架构肯定是无法满足的，必须像互联网公司一样采用分布式架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ede303f87fe7f77d969325bf8b0660f.png" /></p><p></p><p>因为银行有和大型互联网公司相近的业务体量，因此在技术体系上，也借鉴了互联网公司的技术体系。</p><p></p><p>从 IaaS 层来看，银行采用了 X86、ARM 架构的通用服务器，也用到了混合云技术，大量使用了虚拟机与容器服务。</p><p></p><p>在 PaaS 层，银行使用了大量的分布式系统，如开源的微服务框架（例如 SpringCloud ），以及开源的或者商业的数据库，包括分布式/单机/关系型/缓存型的数据库，以及日志数据库 ES、时序数据库等。在中间件层，也用到了大量的开源的或者基于开源改造后的组件，例如消息队列、对象存储、分布式锁等。</p><p></p><p>在 SaaS 层，银行主要通过单元化 + 微服务的架构来实现分布式扩展。银行将自身业务应用划分成三种单元。</p><p></p><p>最上层是全局单元，主要是起到全局路由及流量分发的作用。第二层是业务单元，核心的业务逻辑都在该部分来实现。同时，为了实现业务的横向扩展并支持数亿客户量，银行业跟互联网公司一样，对业务进行单元化拆分。例如我们接触到的某银行，就是将自身的业务拆分为了 16 个业务单元，每个单元五千万客户， 16 个单元部署在两个机房形成同城双活的架构。最底层是公共单元，一些不太好或没必要做单元化拆分的应用，放在公共单元提供公共服务。</p><p></p><p>通过上述分析可以看到，在新一代银行核心系统里面，整体的架构体系已经和互联网公司很接近了，大家用的都是相同的技术栈，只是服务的业务场景不同。在未来，银行业跟互联网业的技术交流会进一步紧密，人才的流动也会进一步频繁。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6dada50bc1d570cafbcad31f40a441dc.png" /></p><p></p><p>在业务采用了单元化划分后，数据库的架构是怎么样的呢？目前在银行的新核心系统下移中，主要采用了以下两种数据库架构：</p><p></p><p>第一种是单机数据库架构。这种数据库架构比较简单，故障域较小，但相对来说业务系统会比较复杂。因为有些模块，如全局路由模块，是全局的总入口，没法做单元化拆分。因此一组单机数据库无法满足性能与容量需求，依然需要在业务层做数据拆分。除此之外，单机数据库无法完全支持业务单元层的业务落地。前面提到，我们接触到的某银行一个业务单元要承担五千万的客户数，一组单机数据库依然无法支持。于是在业务层进一步拆分为 4 组数据库共 64 张子表，业务层需要去解决大量的拆分及分布式事务相关的业务逻辑，整体就更复杂了。</p><p></p><p>另外一种是分布式数据库架构。这样的数据库内部架构虽然更为复杂，但它可以提供更好的性能。对业务层来说，一个单元采用一组数据分布数据库即可，业务层的逻辑就更为简单了。</p><p>因此我们认为，随着分布式数据库的逐步成熟与应用逐渐广泛，业务单元化 + 分布式数据库会逐渐成为流行的银行业务架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/1805f802303d1452f0ddc0dcf9e08ef3.png" /></p><p></p><p>综上，银行核心系统在下移的场景下，对数据库在如下几个方面提出了要求：</p><p></p><p>第一是分布式扩展性。由于采用了通用服务器，它的单机性能要远弱于大型机或者小型机。在这种情况下，数据库需要具备分布式可扩展的能力来满足上亿客户的金融需求。第二是强一致性。金融场景对数据正确性、一致性的要求极高。因此要严格保障对事务的 ACID 特性。否则，业务层就要做大量的工作。第三是容灾能力。通用服务器在硬件故障率方面要比大型机、小型机高很多。因此需要我们的数据库有更为可靠的可用机制以保障 SLA。同时，监管对于容灾能力的要求也在不断提升。比如，对于新建设的核心系统，监管要求必须满足 5 级以上的容灾能力，且满足同城双活并保证 RPO 为 0。在具体执行上，监管的要求也越来越严格，比如同城双活，之前是只需要具备相关的技术方案即可，但现在每年人行的监管都会直接到现场，要求做机房级实战故障切换。第四是运维能力。系统下移到通用服务器并实现去 IOE，数据库节点数量要出现 50 倍的增长。以我们的一个银行客户举例，从小型机下移后，数据库节点数从 20 增长到 1000（当然这里面也预留了几倍的性能增量）。在和客户的交流过程中，他们也认同这个观点，认为系统下移后，节点数要增长一到两个数量级。但运维的人力不可能因此增加几十倍，在这种情况下，就要求我们的运维效率要有质的提升，需要能够智能化、自动化去做相关的运维工作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e70f9a0a35e389e4bda5d5de854c137.png" /></p><p></p><p></p><h2>二、分布式数据库&nbsp;GaiaDB-X 的金融场景方案</h2><p></p><p></p><p>接下来我们分享第二部分，分布式数据库 <a href="https://xie.infoq.cn/article/1febbf974afe91b9a1e11517f?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">GaiaDB-X</a>" 针对金融场景的解决方案。</p><p></p><p>GaiaDB-X 数据库是<a href="https://www.infoq.cn/article/WrlUWpf2OkgQsSAD6NJ1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"研发的 Shared Nothing 架构的分布式数据库，它可以基于通用服务器做横向扩展，来满足高性能、大数据容量的需求。</p><p></p><p>总体来看它分为计算层、存储层、元数据三层架构：</p><p></p><p>计算层是无状态、可横向扩展的一层。它对外兼容 MySQL 协议，接收到 SQL 请求后，再经过 SQL 解析、权限检查、逻辑优化、物理优化之后，生成 DistSQL 下发到各个存储层的分片。为了达到更好的性能，逻辑与物理上尽量将数据过滤及计算能力下推到存储层，收到结果后，再把数据进行聚合计算，最后返回给客户端。计算层的下面是存储层。它采用多分片的方式进行扩展，数据按照一定的分片规则打散到了各个分片中。我们支持 Hash、Range、List 等分区方式来做数据分区。同时，分片内数据采用多副本的方式来保证可靠性，第三是 GMS 节点，即全局元数据管理模块。它用来管理全局性数据，例如表的 Schema 信息、权限信息、表的路由信息，还有后面要介绍到的用于做分布式事务的全局逻辑序列号。GMS 也采用多副本的方式，采用 Raft 协议进行数据同步。</p><p></p><p>在最底层是我们的统一数据库管控平台，来实现对数据库集群的管理。比如在<a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA%3D%3D&amp;chksm=fbeb6a36cc9ce320cf9c590dcbaa67d53421a43a56ea8378ce2b96f0c1c3c3f17265ea54d9b9&amp;idx=1&amp;mid=2247570425&amp;scene=27&amp;sn=bc5f1eeba437ce5e619ccef9bc0321d3&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">百度</a>"集团内部数十万的数据库节点，都是由该管控平台来管理的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50f58801a7c4eeb2bddfece8e453df59.png" /></p><p></p><p>GaiaDB-X 数据库是百度集团发展历史最久、应用最广泛的一款数据库，到现在为止已有 18 年的发展历史。它的发展也与百度的业务发展息息相关，大概可以归纳为四个阶段：</p><p></p><p>第一阶段是从 2005 年开始，为了满足搜索、社区业务中大量读请求的场景，我们通过一主多从的集群化外加读写分离来扩展读性能。第二阶段是为了支持凤巢广告系统与百度网盘，满足它们对万亿级数据量的需求，我们开始做分布式系统。到了 2014 年，我们就已经在凤巢广告系统中替换了基于 Oracle 的盘柜，为凤巢每年节省了上千万的成本。针对百度网盘，我们有个 3000 台服务器的集群支持网盘业务，所有网盘文件的元数据信息都存储在这里，最大的表达到万亿级记录数，至今仍是国内最大的关系型数据库集群之一。第三阶段是随着百度钱包等泛互联网业务的兴起，对数据的一致性要求更高了。因此，我们实现了分布式事务强一致的特性，保障金融数据的正确性。第四阶段，也就是现在。随着百度智能云对外技术输出，我们已经把数据库输出到了十余个行业，覆盖 150 家客户。在金融行业，GaiaDB-X 已经承接了金融核心业务，包括百信银行、银联商务、某交易所及国有行等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43f859c5ceefd365e4a3567012ce9242.png" /></p><p></p><p>对于分布式数据库，水平扩展能力是其核心能力。除了在计算层与存储层做水平扩展外，我们还要着力去解决影响我们扩展能力的单点。</p><p></p><p>第一个是 GMS，即全局元数据模块。因为它要提供一个全局递增的全局逻辑时钟，每一次事务都需要调用它。为了避免其成为瓶颈，我们采用批量预分配的方式来提升其总吞吐，在此模式下，其每秒可分配 1200 万个 TSO 序号，远超出百万级 TPS 的需求。</p><p></p><p>第二个是全局事务表。为了保证分布式事务的原子性，我们需要将正在进行中的事务保存到一张全局表中，因此它的性能也会直接影响到全局性能。我们采用自管理的方式，将全局事务表做成分布式表，分布式地存储在集群中，这样就可以实现分布式扩展。</p><p></p><p>在实际应用中，比如说像 19 年的春晚抢红包，我们支持了三亿用户抢红包，支撑了峰值达每秒 12 万交易量。除此之外，针对某银行拥有 8000 万账户的核心账务系统，我们也平稳支持了其 6 万每秒的 TPS ，充分验证了 GaiaDB-X 的水平扩展能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/560fef70a125720cb796e7ddf53eb3d3.png" /></p><p></p><p>除分布式外，我们也支持单机场景，实现了单机分布式一体化。为什么需要单机分布式一体化呢？以我们的一个银行客户来说，全行的业务系统有 200 多个，其中大部分系统（大概占 70% 左右）对性能与吞吐的要求并不高，一组单机数据库就能够满足其业务需求。但对于剩下的 30% 业务，它对性能的要求是单机数据库无法满足的，需要分布式数据库来满足其扩展性。</p><p></p><p>因此我们通过一体化的方式，来满足银行不同体量的业务对于数据库的需求。同时，我们也具备单机数据库扩展为分布式的能力，在对应业务的数据量增长后，能够扩容为分布式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15483b1c6cffdb73ad5178a621526e14.png" /></p><p></p><p>扩展性的另外一个目的是自动做数据分离。在金融场景里面，存在多个业务共用一个数据库集群的场景，比如业务分为联机交易系统与查询类系统，数据库便对应划分为交易库和历史库两个。</p><p></p><p>对于交易库来说，只保存联机交易会频繁使用到的数据。例如账务结果数据及当天的交易记录，以满足对交易业务的快速响应。对于查询不那么频繁的即时交易记录，这可能就是一个相当大的数据量，一般都能达到千亿甚至万亿级别。这时，我们就可以将这个数据自动转移到历史库上去，用更高密度的存储机型来存储。一方面可以降低硬件成本，同时也可以避免对联机业务的影响。</p><p></p><p>这样做对业务来说，对外呈现的是一套数据库，业务可以根据需要来处理不同数据分区的逻辑，也不用在多套库之间通过 DTS 做数据同步。同时还把交易数据和历史数据做分离，以保障对联机业务的性能，同时也满足了查询业务的查询需求，避免其相互影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/09461237b2307a25d9e1a17349640b7b.png" /></p><p></p><p>在金融场景中，对事物的 ACID 特性有着严格的要求：</p><p></p><p>持久性。指的是事务一旦提交就不会丢失，一般通过多副本 + 强同步来解决。原子性。一个事务要么全部提交，要么全部回滚，不存在部分提交的情况。通常，数据库的 XA 协议，通过两阶段提交能解决这个问题。</p><p></p><p>但是 XA 协议不能很好地满足一致性与隔离性。以简单的转账场景为例，A、B 原来各有 100 块钱，总共 200 块。然后 A 给 B 转账 50 块钱。此时，我们会先给 A 扣 50，再给 B 加 50。如果通过 XA 协议来做的话，先走 prepare 再 commit，我们可以看到，commit（图中第 7、第 8 步）过程不是原子过程，存在一个执行时间差。在这个时间差内，另外一个事务去读取数据，就可能存在读到提交了一半的数据，A 和 B 的总和是 150 而不是 200。这是 XA + 2PC 解决不了的问题。</p><p></p><p>为了解决这个问题，业内一般是引入一个全局时钟来做一致性的保证。通常有三种实现方式：</p><p></p><p>TrueTime 方案。这个是大家比较熟悉的方案，Google Spanner 也发过论文。但它的缺陷是依赖硬件，要引入 GPS 与原子钟，这个一般来说是难具备的。HLC 方案。采用该方案的典型数据库系统是 CockroachDB，它的优点是不依赖硬件，而且是去中心化的。但是缺点也很明显，一致性比较弱，而且要求服务器间的时钟误差不能超过 250ms，否则就无法正常运行。TSO 方案，比如 TiDB 就是采用了这种方案。TSO 本质上来说是一个全局唯一而且自增的逻辑序列号，一个事务在开始时与事务 commit 时需要两次找 GMS 获取序列号，然后把 TSO 号作为版本号提交到存储层，存储层的 MVCC 机制来判断数据是否可见。它不需要硬件具备强一致性，但缺点是依赖一个全局中心的时钟分配器 GMS。但这并不是一个问题，因为刚刚我们也提到了，虽然 GMS 不具备扩展性，1200 万的 TPS 已经完全满足业务的常规需要了。因此我们最终采用了这种方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b192d1774be97ac09d153b3e3fcf81d9.png" /></p><p></p><p>除了保障事务的一致性外，我们还需要保障上下游系统的数据一致性。在开始之前，我们首先要讲一下银行的典型业务场景，它一般分为三个子系统：</p><p></p><p>第一个是联机交易系统，例如存取款、在线支付等。这个系统的并发量高、延迟敏感，直接影响到用户体验。第二个是跑批类的业务系统。例如结息，每天晚上半夜计算前一天的利息。这些业务是后台业务，有大量的读取与计算操作，延迟不敏感，但是对数据一致性要求高。怎么能够让这样的业务尽量避免对在线业务的影响，同时又能够读取到最新的数据呢？我们的方式是让跑批业务去读取从库数据，从而避免对主库的性能影响，同时结合 TSO，即全局逻辑时钟的对位对齐机制。等到从库水位追齐之后，才返回读取数据。当然这会引入一定的延时，但是因为跑批业务对响应延时不那么敏感，所以是可以接受的。第三个子系统是大数据类的离线业务。通常来说，就是银行内部的各种大数据、数仓等系统，需要把数据库的数据同步过去。这样的业务对实时性要求不高，但要求最终的数据是一致的。由于各个计算节点都会承担流量，也会生成 BinLog。因此，如何对多份 BinLog 进行排序，让它们能够严格保持时序是我们需要解决的问题。我们的全局 BinLog 有两个模块，一个是 pump，从各个CN 节点抽取 BinLog，然后在 Sorter 节点基于 TSO（全局逻辑时钟）进行排序，来保障形成一个全局时序正确的 BinLog，以保障这个离线系统收到的数据的最终正确性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17a48e22473b043b5053080788477dae.png" /></p><p></p><p>接下来我们看一下容灾能力，除了对单机故障的容灾之外，还有对特定机型的容灾。因为银行把系统下移到通用服务器，通常都是通过两阶段来实施：第一阶段是下移到 X86 的 CPU 芯片上，这个过程银行、互联网厂商都一定的经验。第二阶段是要实现服务器芯片的基本国产化，就是说使用国产的鲲鹏、飞腾或海光 CPU 芯片，这方面大家都是在探索性的开展相关业务。</p><p></p><p>以百信银行举例，它与其母行中信银行一样，选择了基于鲲鹏做国产化的路线，而且在业内比较超前。相较于其他银行仅在周边系统或者数据库从库来做国产化，百信银行的周边业务跟核心系统都和主站一样基于鲲鹏服务器来做，这个在业内是较为领先的。</p><p></p><p>为了保证客户业务系统实现平滑的国产化，我们在产品上实现了一库多芯的方案，主要资源池基于鲲鹏，但放置了一个独立 X86 资源池，在技术上实现托底，同时也能够将原有换下来的服务器能够利用上，避免资源浪费。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/173396e11937ad6b3f0e88e0a6e8fb31.png" /></p><p></p><p>根据人行的监管要求，银行核心系统一般要具备 5 级以上的容灾能力，这就要求具备两地三中心的容灾能力。</p><p></p><p>下图是我们客户的一个典型机房部署架构。首先在北京有两个同城机房，其物理距离在 50-100 公里左右，网路延迟在 1ms 左右。同时还有一个异地机房做灾备，物理距离一般是 1000 公里左右，比如合肥，网络延时是 10ms。</p><p></p><p>同城两个机房业务做双活部署，同时接受业务流量。数据库在两个机房采用 3 + 2 的部署形式，机房间采用强同步的方式来保障在发生机房级的故障之后，数据库能进行故障切换，而且保障数据的 RPO 为 0。</p><p></p><p>为保证单机房故障后的零数据丢失，我们采用分组强同步的方式，将 id1、id2 各划分一个复制组，复制组间采用强同步的方式。每个复制组保证至少有一个副本接收到数据之后才返回成功。这样在容忍少数副本故障的同时也能够保证单个机房故障后的零数据丢失。</p><p></p><p>异地机房的目标是当北京的两个机房都出现灾难性的事件之后，能够降级完成核心业务。它采用异步级联的方式来同步数据，首先异步是为了避免阻塞对主地域的数据库写入；采用级联方式，没有跟主地域机房形成复制组，主要一个为了保持灾备机房的数据库的拓扑独立性，减少依赖，保障在关键时刻可切换，另外也是降低跨地域同步的数据量，只需要同步一份数据即可。</p><p></p><p>结合在多家金融客户的实践，我们和中国信通院一起发布了金融数据库的容灾技术报告《金融级数据库容灾备份技术报告（2021 年）》。大家可以在公众号后台回复「金融级数据库容灾备份技术报告」获取。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c35b7ca466e4e027b09f699a6c555063.png" /></p><p></p><p>最后一部分是运维能力。核心系统下移及国产化的背景之下，数据库系统呈现两个变化：</p><p></p><p>一是数据库的节点数出现了 50 倍的增长，这里面既有技术架构的原因，也有数据库预留了一定的性能空间的原因。</p><p></p><p>二是数据库的种类也变多了。对于银行系统来说，之前基本就是选择 Oracle 或 DB2。现在则开始使用多种开源数据库和国产数据库。除此之外，为了避免像之前一样被单一厂商绑定，银行也会特意选择多家数据库厂商，在这种情况下，对银行的运维的挑战就更大了。</p><p></p><p>因此，结合百度集团及百度智能云管理大规模数据库节点方面的经验，我们将 GaiaDB-X 数据库云管平台进一步泛化，形成了具备管理多元数据库能力的统一平台，它除了能够管理 GaiaDB-X 自身，也能管理其他的开源数据库。通过帮助银行建立企业级的 DBPaaS 管理平台，进一步提升了银行的运维效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5b5b67c12e920a404b8a06be4702a30.png" /></p><p></p><p></p><h2>三、金融应用案例介绍</h2><p></p><p></p><p>接下来，我来分享百度智能云在金融方面的一些典型案例。</p><p></p><p>首先是百信银行。它的特点是完全去 O，是一家完全没有 Oracle 的银行。全行 200+ 业务系统，无论是核心账务系统还是周边系统，几乎全部是基于 GaiaDB-X 数据库来构建的，至今已经平稳运行五年。</p><p></p><p>按数据库节点数计算，百信银行目前的数据库国产化率达到了 99.93%，遥遥领先于行业平均水平。</p><p></p><p>同时，百信银行在容灾和国产化领域也比较领先，在 2019 年就完成了全行主要系统的同城双活建设，2022 年开始将全行业务逐步迁移到基于鲲鹏的国产云上，进而实现了全栈的国产化。</p><p></p><p>在硬件成本方面，我们通过采用通用服务器来替代 IOE 硬件，帮助百信银行的单账户平均硬件成本降低了 70% 以上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5eba560cf050d18ba1d18d015ebab4c0.png" /></p><p></p><p>下图是人行下面直属的某交易所。因为是涉及到国家金融稳定，所以在核心系统上需要逐步摆脱对 Oracle 的依赖，并拥有两地三中心的容灾能力。</p><p></p><p>由于当前的一些数据库都不能满足他们的业务场景需求，因此该交易所和百度采用了联合开发的模式，共建数据库能力。在两年的合作过程中，我们从外围的信息管理系统入手，逐步深入到核心交易系统，再到离线库数据分析系统，进而逐步实现数据库国产化。</p><p></p><p>此外，由于交易所的交易系统对低延时的要求较高，同时基于容灾要求又有同城双活的要求，如何在跨机房的情况下保障交易延时就成了亟待解决的问题。因此我们共同建设了 Collocate 机制，来尽量减少跨机房数据的访问，最终将交易延时从 80 毫秒降低到了 15 毫秒。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3bbb84070b281ebbedf2e41e6edc456.png" /></p><p></p><p>下图是国内某国有大行客户。他们在最近两年把原有的基于小型机的核心系统下移到了通用服务器中，数据库也从 Oracle 替代为了开源单机数据库。</p><p></p><p>但在这个过程中，该行面临两个问题：一是数据库节点数增长了 50 倍，服务器数量到达了 1000 台左右，如何做好数据库的自动化部署、上线变更、版本升级、参数管理、性能诊断等工作。二是如何结合业务的单元化，形成业务与数据库的同城双活与异地容灾能力。</p><p></p><p>借助百度智能云提供的统一数据库管控平台的能力，历时两年，我们与客户一起实现了新核心系统的全面投产，也顺利通过了人行的验收。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b935db0fd9ff535ed78234e80ac12688.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</id>
            <title>深度解读“百度智能云数据库”的演进：互联网→云计算→ AI 原生</title>
            <link>https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z1mDYGshu9o8IfXGVcXg</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 06:50:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ChatGPT, AIGC, 数据库发展, AI时代
<br>
<br>
总结: ChatGPT的爆火引发了AIGC赛道的关注，展示了弱人工智能向强人工智能的跨越式发展。AIGC将改变数据库行业，AI时代将带来数据库的变革和创新。 </div>
                        <hr>
                    
                    <p></p><h2>一、数据库行业发展概述</h2><p></p><p></p><p>如果说今年科技圈什么最火，我估计大家会毫不犹豫选择 ChatGPT。ChatGPT 是 2022 年 11 月 30 日由 OpenAI 发布的聊天应用。它创造了有史以来用户增长最快的纪录：自 11 月 30 日发布起，5 天就拥有了 100 万活跃用户，两个月就达到了一亿用户。对比其他热门应用，同样达到一亿用户量级，TikTok 花了九个月，而像 Instagram ，Whatsapp 等应用则超过了两年时间。</p><p></p><p>ChatGPT 的爆火，瞬间点燃了整个 AIGC 赛道。最关键的原因在于，它让大家看到了弱人工智能向强人工智能的跨越式发展。英伟达 CEO 黄仁勋对此评价：ChatGPT 相当于 AI 界的 iPhone 时刻。</p><p></p><p>现在业界统一的共识是，AIGC 会改变 IT 行业的方方面面。那 AIGC 对数据库会带来哪些变化，AIGC 和数据库又会碰撞出哪些火花，这是一个值得我们去思考和回答的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/648f846dd8384dfa9a1fc1a1ea4abf3e.png" /></p><p></p><p>在回答 AIGC 对数据库的变革和影响之前，让我们先回顾下数据库发展历史。它可以分为六个阶段。</p><p></p><p>第一阶段是上世纪五十年代。这个时候数据库还在雏形阶段，以层状数据库和网状数据库为主，基础设施以大型机为主，主要用于国防和科学研究。</p><p></p><p>第二阶段是上世纪七十年代。关系型数据库出现，硬件也变成了小型机，这也奠定了数据库发展的方向。主要应用在金融，交通等关键行业。这时的代表数据库是 Oracle 和 DB2 等。</p><p></p><p>第三阶段是上世纪九十年代。PC 机已经得到了普及，数据库除了关系型数据库，也有了 PC 单机数据库。为解决企业 BI 应用诉求，数仓开始出现。数据库的应用也更多样化起来，进一步应用到企业 BI、个人办公、娱乐等场景。</p><p></p><p>第四阶段是本世纪的前十年。随着互联网开始繁荣，数据处理的需求逐渐增加，开始出现企业数据中心。业务也变成了媒体、搜索、电子商务、社交等互联网业务。由于传统数据库如 Oracle 因为价格较贵，互联网厂商大量使用开源数据库如MySQL、Redis、MongoDB 等。整个开源数据库生态开始逐渐繁荣。数据库的种类，厂家也逐渐变多。</p><p></p><p>第五阶段就是我们今天所处的云计算时代。典型应用包括新媒体、各种移动 APP、物联网、娱乐、短视频等。典型的数据库有 RDS、Aurora 等云数据库，以及 Oceanbase、CockroachDB 等分布式数据库。百度也有对应的产品，云原生数据库 GaiaDB 以及我们自研的缓存类数据库&nbsp;PegaDB 等。</p><p></p><p>第六个阶段是自 2023 年开始的 AI 时代。底层基础设施变成了 GPU 和 AI 能力。应用也变成了 AI 原生应用，如海外比较火的 Jasper、Midjourney，微软的 Copilot 等。在数据库行业我们看到至少两个方向，一个是 AI4DB，其中包括阿里的 DAS、百度的 DSC 等，主要是通过 AI 的能力去改进原有数据库的自动化能力。另外一个方向就是 DB4AI，目前主要是向量数据库。向量数据库在解决大模型幻觉等方面，有非常不错的效果，是一个有潜力的细分赛道，头部公司估值已经达到 10 亿美元。</p><p></p><p>以上就是数据库 70 年波澜壮阔的发展史。我们可以看到，每隔一段时间数据库就会在基础设施、应用场景、以及数据库本身，都有不断地变更和创新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92a9b7ae192ebfb33a9091bbf57857a1.png" /></p><p></p><p>上面我们简单回顾了数据库发展的六个阶段。在这个过程中，我们还可以以 2000年做分界线。在 2000 年前，国内数据库基本上被 Oracle 等海外数据库主导。而从 2000 年之后，随着互联网业务的发展，国内多个互联网厂商如阿里、腾讯、百度便开始尝试使用开源数据库，实现了从最早的运维、到提交 patch、再到最后完全自研数据库的跨越式发展。</p><p></p><p>这背后从量变到质变的过程是一个典型基础软件发展过程。</p><p></p><p>一个基础软件真正得到长足发展，需要一大批高素质的技术人员，也需要深度场景的使用才能不断完善产品。另外丰富的场景和不断发展的业务，也能长期养活这批技术人员，进而形成正循环。所以说数据库的发展依赖于技术和业务的双轮驱动。</p><p></p><p>从 2000 年开始，我们看到三波浪潮——互联网，云计算和 AI 原生。我们接下来会分别来讲一下每一波浪潮为数据库行业带来的创新和变化，以及百度智能云数据库在这个过程中的关键技术和代表产品。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f647b07b8603129ba05898417ae77eb5.png" /></p><p></p><h2>二、百度智能云数据库发展史</h2><p></p><p></p><p>互联网业务特点是赢家通吃，所以互联网业务用户数规模通常比较大。因此天然要求数据库支持大规模、高可用、高可靠性、低成本以及高性能，这对数据库提出了非常大的挑战。</p><p></p><p>在第一波互联网业务的发展中，业务的挑战催熟了一系列开源数据库如 MySQL、Redis、MongoDB，又从中孵化出了分布式数据库。</p><p></p><p><img src="https://static001.geekbang.org/infoq/20/20e33da0350e46cb356adc79012f78ed.png" /></p><p></p><p>接下来我们来看下百度在互联网时代的数据库发展历程，这里有几个关键节点：</p><p></p><p>第一个是自 2005 年开始使用 MySQL 数据库，这也是国内最早使用 MySQL 的企业之一。</p><p></p><p>第二个是 2014 年百度推出公有云服务，百度数据库的能力通过<a href="https://www.infoq.cn/article/WrlUWpf2OkgQsSAD6NJ1?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度智能云</a>"开始赋能给外部企业。</p><p></p><p>第三个是 2020 年发布了云原生数据库 <a href="https://xie.infoq.cn/article/61a867abe6d45fa9f1fe644d0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">GaiaDB</a>"。百度也成为了国内少数几个具备自研云原生数据库云厂商之一。</p><p></p><p>截至目前，百度积累了 18 年的数据库研发经验，承载着内部 PB 级数据。10 万+ 的节点至今零故障零损失。</p><p></p><p>通过百度智能云输出的一站式产品，覆盖 RDS、NoSQL、OLAP、工具等领域，同时具备公共云、私有云、边缘云等软件版本多形态。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f0fe561b919a46ca824f31f4f556ce5.png" /></p><p></p><p>前面我们提到了互联网的一大特点，就是规模大。单点肯定处理不了，所以需要引入分布式技术，也催生了分布式数据库的诞生。</p><p></p><p>百度在该领域也有非常成熟的技术，讲两个实际的案例：</p><p></p><p>第一个是<a href="https://www.infoq.cn/article/2012/03/baidu-bae?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">百度网盘</a>"。百度网盘有 8 亿用户，整个数据库中单表最大超过 10 万亿条记录。整体集群超过 3000 台服务器，是国内最大的数据库集群之一。</p><p></p><p>第二个是金融行业。大家都知道金融行业对一致性、数据准确性有非常高的要求。度小满金融有 3 亿用户，年度结算金额超过万亿，其底层使用的就是百度智能云分布式数据库 GaiaDB-X。</p><p></p><p>尤其值得一提的是在 2019 年春晚红包业务中，整体交易的峰值是 12 万笔/秒。数据库的分布式能力、性能、一致性、准确性都得到了充分验证。</p><p></p><p>除了度小满，百度智能云的数据库还在多家国有大行、股份制银行和城商行中稳定运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72d4145b2e5cf4609d5969fcd0c828a2.png" /></p><p></p><p>互联网业务除了规模外，对性能、并发等也提出了很高的要求，因此诞生了一系列 NoSQL 数据库。不同的 NoSQL 数据库从不同层面解决互联网垂直场景的问题，今天我们讲其中的代表 Redis。</p><p></p><p>百度智能云的 Redis 服务经历十几年的技术积累和业务打磨。从规模上来看，节点规模超过 30w，其中单集群最大规模节点数达到 2700。从业务支持上看，百度 Redis 覆盖支撑了百度内部全场景业务，其中包括搜索广告、手百、地图、小度等一系列亿级用户体量的产品，为业务提供 4 个 9 以上高可用性以及微秒级请求时延服务，始终为客户提供稳定、高效、弹性可扩展的智能缓存服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/8471c3e34b3cef8ce9d5754cd03b193c.png" /></p><p></p><p>Redis 直接使用内存，但内存带来高性能的同时成本是比较贵的。因此一款能兼顾性能和成本的 Redis 产品是客户迫切需要的。考虑到业务中大量的数据是可以根据场景分出冷热的。比如视频直播、新闻/内容平台、电商场景中，随着时间的推移，数据的价值和使用频率都在下降。所以可以将部分数据自动迁移到磁盘中，从而降低存储的整体成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8240196a262dd1948b2f1a0a9babe152.png" /></p><p></p><p>为了解决性能和成本的平衡问题，百度智能云自研了 PegaDB。PegaDB 是在开源基础上自研的容量型 Redis 产品，相比内存型产品最多节省超过 90% 的存储成本。在成本下降的同时，PegaDB 也兼容了 Redis 丰富的数据类型和命令，让用户做到无缝迁移，兼顾了用户体验和性能优势。</p><p></p><p>除此之外，PegaDB 还有两个杀手锏功能：</p><p></p><p>一是支持在线弹性伸缩，单个集群最大规模可达 PB 级别。对用户来说不用估计使用量，只要傻瓜式即开即用即可。</p><p></p><p>第二个是支持 CRDT 同步的组件，支持异地多活和多节点同时访问、自动进行冲突合并等功能。这就让客户专注于实现业务逻辑，其他的都交给底层的数据库，完全不用操心可用性问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8247c1d27f22bf8d13326b6df3fa1270.png" /></p><p></p><p>随着云业务的诞生，让数据库的价值进一步放大。为了赋能千行百业，全托管等形态的 RDS 顺利成章的诞生了。它解决了客户最直接的安装、运维、管理等问题，因此全托管的 RDS 就逐渐推广开来。</p><p></p><p>但单体 RDS 通常有比较明显上限，在一些对性能、成本、弹性有一定要求的复杂业务中，就需要一个更强大的数据库来解决这些问题。因此，存算分离的云原生数据库就自然而然诞生了。百度智能云的云原生数据库 GaiaDB 是其中的代表之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e835b194141802f859a373b37363990.png" /></p><p></p><p>RDS 全托管的产品形态代表了云计算从软件到服务的理念转变。云原生数据库极大地提高了 MySQL 数据库的上限能力，是云数据库划代的产品。</p><p></p><p>云原生数据库最早的产品是 AWS 的 Aurora。AWS Aurora 提出来的 The log is the database 的理念，通过把大量的日志操作放到后台异步处理，实现了存储独立扩展和存储计算分离，从而解决了 MySQL 数据库单库的数据量不能太大的最大痛点。</p><p></p><p>而云原生数据库在存储层面实现了扩展的同时，又保留了计算层面的不变和兼容。这种兼容 + 扩展的能力，受到了客户的极大欢迎，一下子就让云原生数据库成为各个厂商的发展重点。云数据库技术也标志着云厂商的产品能力开始和传统数据库厂商、开源产品开始拉开差距。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd48508403406795d06e50dffdebbe2f.png" /></p><p></p><p>百度智能云的 GaiaDB 在 2020 年首次推出，除了具备云数据库的优点之外，GaiaDB 还有很多独特的技术能力，接下来我来分享其中 5 个代表能力：</p><p></p><p>第一个是共识协议。一般使用 Raft/Paxos 分布式协议的数据库，单次 I/O 需要至少两次网络往返，而且无法并行。这也就导致了分布式数据库时延很高，长尾问题更突出。</p><p></p><p>针对这个问题 GaiaDB 创新采用了 Raft 和 Quroum 结合的协议。其中 Raft 负责控制流，Quorum 负责数据流，进而减少网络往返。同时核心链路上的同步 I/O 变成异步 I/O，在保证分布式一致性的前提下，吞吐提升了 40%，时延降低了 30%。</p><p></p><p>第二个是高性能智能网络。存算分离在带来分布式和弹性的同时，也引入了网络 I/O 的消耗，因此网络 I/O 的性能和效率直接影响整个系统的表现。GaiaDB 采用高性能智能网络，这个网络有几个关键技术能力：</p><p></p><p>网络超时重定向机制。当远程 I/O 超时，会自动尝试其他副本，从而抑制单节点长尾问题。网络支持用户态协议。该协议减少了内核态 TCP 和用户态 TCP 的数据库拷贝。通过对网络的优化，平均时延从毫秒级别降低到微秒级别，提升 20 倍以上。</p><p></p><p>第三个是提供了三副本对等存储能力。由于采用了 Quorum 分布式共识协议，相比传统的 Raft 模型，每个节点都可以独立提供读写服务，没有单点故障。</p><p></p><p>第四个是多地多活。GaiaDB 是目前业界唯一可以做到多地多活的云原生数据库。在多地部署的时候，GaiaDB 模块的自适应就近访问策略可以感知元数据的变化，并根据这些变化及时切换访问路线。这种策略可以有效地应对各种故障和异常情况，确保数据的可靠性和可用性。</p><p></p><p>第五个是使用通用硬件，对硬件要求低。GaiaDB 生于云，但同时 GaiaDB 的架构对硬件的依赖度非常低。我们和很多厂商使用高性能硬件的思路不同，我们认为云的价值是普惠，所以一定要让通用服务器能发挥专业数据库的能力。因此，不同于很多云原生数据库需要依赖底层高性能的硬件，GaiaDB 从设计初就坚持使用通用服务器。因此在私有云场景下，三个节点就可以进行部署，让我们的客户可以低价享受到云上云下一套架构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b7c7ed3a0879eebe3cc8ba586055484.png" /></p><p></p><p>接下来我们来看一个 GaiaDB 的实际案例——百度地图。</p><p></p><p>百度地图是国民级别应用，日活用户 5.6 亿，PB 级数据。这对数据库也提出了如下的挑战：</p><p></p><p>为了保证高可用，需要多地多活的能力。节假日地图搜索，导航流量会出现十倍的上涨。这就要求在节假日需要非常顺滑的扩缩容的能力。</p><p></p><p>大规模数据量、异地多活、弹性扩缩容要求，这些要求对数据库是极大的考验。</p><p></p><p>在实际使用过程中，GaiaDB 提供 4 个 9 的可用性，RTO 切换小于 3s，RPO=0，整体 QPS 超过百万级别，给业务实现超过 60% 的资源成本节省。</p><p></p><p>总的来说，GaiaDB 成功帮助百度地图实现了极致的弹性和成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f02e4b25fba325d4854e72f711bde58.png" /></p><p></p><p>云上数据库和线下数据库相比，一个较大差异就是生态能力强。相比传统线下软件只有 1~2 款产品，线上有多种数据库与多种使用环境，因此数据库矩阵更丰富，这带来了对数据库工具的诉求。</p><p></p><p>百度智能云有丰富的数据库工具，包括数据传输 DTS、数据库智能驾驶舱 DSC 等产品。我们先讲其中的代表 DTS。</p><p></p><p>百度智能云的 DTS 采取了中间抽象的数据格式，通过中间格式的翻译和转换，可以轻松做到异构迁移能力。同时 DTS 在吞吐上可以做到每秒 15 万行，延迟做到毫秒级别，基本等于网络的延迟的性能，让客户可以放心使用 DTS 来做数据库的迁移和同步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e0d44ff1229d468f704af06b132010e.png" /></p><p></p><h2>三、AI 原生时代的百度智能云数据库</h2><p></p><p></p><p>在 AI 原生时代，数据库和 AI 的结合主要有 DB4AI 和 AI4DB。</p><p></p><p>首先是 AI4DB，就是利用 AI 技术赋能数据库。常见场景有智能运维、智能客服、参数优化等等，刚刚提到的百度智能驾驶舱就是该领域的代表。</p><p></p><p>另外一个方向是 DB4AI，通过数据库赋能 AI 产品。当前最火的就是向量数据库。向量数据库二次的翻红主要原因是向量数据库在解决大模型幻觉、知识更新不及时有很大作用，让向量数据库的想象空间一下子变大了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/096c6ccafce7598fdac1f73dfb471e4f.png" /></p><p></p><p>AI4DB 在工业界一直有研究。相比传统机器学习算法，大模型让 AI4DB 真正走进实用时代。利用大模型的能力，百度智能云数据库发布新服务：数据库智能驾驶舱。</p><p></p><p>数据库智能驾驶舱利用最新的大模型能力，实现数据库智能化的洞察、评估和优化。根据我们的实际测试效果，优化效果非常显著：</p><p></p><p>数据库故障洞察方面，相比传统的人工定位提升 80%。领先的智能评估系统，相比传统的方法提前一个月发现数据库的容量瓶颈，规避相应的风险。AI 驱动的 SQL 优化方面，可以带来 40% 以上的提升。</p><p></p><p>相比传统基于规则的算法，大模型带来了更好的优化效果和更少的开发时间。大模型带来的切实提升让 AI4DB 走向真正的实用时代，也让数据库自感知、自修复、自优化、自运维成为现实。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9cd6b1eab7842cbdba2f1e03555c143.png" /></p><p></p><p>下面我们来看下数据库智能驾驶舱内置的一个能力——智能问答。</p><p></p><p>这个功能可以帮助用户诊断产品问题并回答各种疑问，降低人工投入。这里面用到了大模型通用知识的能力，同时也利用 RAG 技术，把云产品文档、数据库的官方文档、内部积累的知识库进行向量化并存在向量数据库中。</p><p></p><p>在查询的时候，结合大模型和向量数据库的能力，可以给出相当准确有效的答案。</p><p></p><p>目前数据库智能驾驶舱经过验证，对历史客户工单中真实问题进行回答然后由人工进行打分，整体回复平均超过 4 分，基本可以媲美普通售后工程师的水平。</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79b670cada64634f9bf5577ab757d963.png" /></p><p></p><p>接下来我们实际来看下智能问答的一个 demo。</p><p></p><p>左边的例子是询问知识库里面已有的例子，比如怎么购买，怎么实现一个读写分离的配置等。智能驾驶舱都总结得比较好，回答也非常准确。</p><p></p><p>右边的例子是询问知识库中没有的例子。我们可以发现，智能驾驶舱利用大模型的能力，可以举一反三，把解决问题的步骤给出来。我们人工去检查也会发现，这个步骤还是相对比较合理的。</p><p></p><p>所以现在智能驾驶舱的智能问答可以做到：有资料的问题准确回答，无资料的问题也可以给出相对清晰的解法。百度智能云内部已上线了该功能，大大节省了人力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48ba6c7615d86cfd011247f99b44be3e.png" /></p><p></p><p>DB4AI 的典型代表就是向量数据库。向量检索并不是一个新技术，2017年 Meta 就开源了相似度检索库 FAISS，算是向量化检索的开山鼻祖。</p><p></p><p>传统数据库解决的是结构化数据的存储和检索，非结构化数据需要先用 AI 算法 Embedding 成向量数据。需要查找的时候，把需要查找的数据的向量带过来，然后在库里面进行相似度检索。</p><p></p><p>而向量数据库核心能力就是支持向量数据存储，以及支持不同的查找算法和索引实现相识度查找。当前业界有两种不同的实现方式，一种是在传统数据库中增加插件或者功能支持向量的查找，比如 PG，Redis 都支持向量索引。这种实现相对来说容易一些，但同时性价比会差一些，通常会占用更多内存。另外一种是专业的向量数据库，专门为向量重新设计的存储和索引结构，能实现更高的性价比和弹性。</p><p></p><p>传统应用也有不少向量场景。典型场景有平安城市视频检索、电商领域以图搜图等。由于传统场景比较垂直，因此一直没有一个大的向量数据库，更多的是耦合在业务系统中。而在大模型时代，万物皆可向量化。而且当前大模型主要问题有知识更新不及时、精确性问题、数据权限管理等问题，都需要向量数据库来补充。向量数据库也因此成为大模型的标配，也在大模型时代二次翻红。</p><p></p><p>百度智能云自研的专业向量数据库目前在内测阶段，根据我们内部实际测算，在成本、规模、高性能算法、内置 Embedding 模型、向量 + 标量的联合查询方面，相比业界有很大的提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b9f6fcb6d97c8979c5bbbdfe17c3e09.png" /></p><p></p><p>前面我们介绍了关键的产品，最后简单回顾一下百度智能云产品矩阵。</p><p></p><p>百度智能云数据库完整支持 RDS、NoSQL、云原生数据库，OLAP 等产品。相比业界其他云厂商，百度智能云数据库有两个显著特点：</p><p></p><p>百度智能云的数据库产品可以做到一套架构，云上云下客户享受同等的产品能力。支持国内最全的产品形态，包括公共云、私有云、边缘节点、LCC 等多种形态，可以服务各类诉求的客户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc846845a37a6dac8eac882ad434be70.png" /></p><p></p><p>前面我们盘点了数据库在互联网、云计算，AI 原生 3 个阶段的发展。除了技术之外，我们认为云数据库未来还要坚持两个重要的理念。</p><p></p><p>第一个是体验优先。一个好的数据库不能只是性能、成本这些方面。体验好的产品，可以让用户做到自服务。体验优先这一点在海外 SaaS&nbsp;产品中体现得更为明显。在国内，这一理念也逐渐取得从业者的认可。因此，在过去的半年里面，我们从文档、控制台、产品功能各个层面进行了深度优化：</p><p></p><p>文档：文档是用户使用和理解产品的重中之重，因此我们做了包括优化结构、补充用户场景、刷新细小的优化点在内的大量工作，目的就是让用户在使用过程中可以更方便找到自己所需要的内容。控制台：在控制台优化上，我们优化了整体结构，让用户可以更简单找到想用的功能，总共优化点超过 100 处，让用户更容易上手。产品功能：我们针对数据库的产品功能系统性安排测试定期的盲测、新员工使用等，仅仅上半年就优化了 50+ 个突出的易用性问题。</p><p></p><p>我们对体验的理解就是从用户视角入手、坚持细节、系统性的进行优化，只有通过这种深度，全方位的持续改进，才能把体验做到实处。</p><p></p><p>第二个是开放生态。丰富的生态是吸引客户、解决客户多样诉求的关键。也只有开放的生态，才能让更多的厂商一起服务好客户。</p><p></p><p>生态方面，百度秉承更开放的心态和第三方厂商合作。上半年我们和工具领域知名创业公司 NineData 正式合作，接下来会马上官宣另外一个合作厂商。</p><p></p><p>相比其他厂商，我们合作的过程也不只是简单的云市场合作。我们会和合作伙伴一起进行产品共建、优先推荐合适客户给合作伙伴、首页曝光和联合的品牌活动，增加合作方的知名度。</p><p></p><p>通过一系列的手段和措施，我们希望给到合作伙伴的是切实效果。百度智能云合作的理念就是更开放，让利合作伙伴。欢迎更多的合作伙伴和百度联系，一起服务好我们的客户。</p><p></p><p>总的来说，一个体验优先，生态开放的云，一定是客户最需要的云，也是真诚服务客户的云。</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/dfc3dc9e74526dc0f5a93b8a4fa32c72.png" /></p><p></p><h2>四、数据库未来的趋势展望</h2><p></p><p></p><p>站在当前看未来，数据库当前有四个关键发展趋势</p><p></p><p>AI Native。像大家比较头疼的 Oracle 转 MySQL 或者 PG，随着 AI 改写的到来，整个过程预计会变得很简单。Serverless。已经是海外云数据库的默认选项了，预计 1~2 年之后，serverless 就会在国内变得更普及。各个厂商也都会推出 serverless 数据库产品，这也是未来云产品的终极形态。内置 HTAP。HTAP 前段时间很火，不过我们判断 HTAP 很难成为一个单独的赛道，更多的是会成为各个 TP 数据库的内置能力。湖仓一体。湖仓一体预计会成为数据仓库的主要形态，不支持湖的数仓可能会很难生存，只有支持湖才能解决更多的数据问题，才能降低存储的成本。</p><p></p><p>技术和产业发展都很快，百度智能云数据库持续跟进最新的技术趋势，用优质的产品和真诚的服务回报我们的客户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f7dafca36b251342c7b9668e005b1a4.png" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</id>
            <title>网易杭州研究院 / 编程语言实验室 / 负责人张炜昕博士确认出席 QCon 上海，分享低代码编程语言 NASL 从设计到落地的闯关之路</title>
            <link>https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dcJEkHM7VklAVYVCFKSZ</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, 低代码编程语言 NASL, 张炜昕博士, CodeWave 智能开发平台
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，张炜昕博士将分享关于低代码编程语言NASL的设计和实现挑战，以及CodeWave智能开发平台的应用。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。网易杭州研究院 / 编程语言实验室 / 负责人张炜昕博士将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5642?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">低代码编程语言 NASL 从设计到落地的闯关之路</a>"》主题分享，探讨 NASL 语言的设计初衷、设计原则、实现挑战、未来展望等方面。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5642?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1211&amp;utm_content=zhangweixin">张炜昕博士</a>"，香港大学博士，布里斯托大学 Senior Research Associate，长期从事编程语言研究。现为网易杭州研究院编程语言实验室负责人，主导 CodeWave 智能开发平台编程语言 NASL 的设计。以第一作者身份在 TOPLAS，ECOOP 等编程语言期刊和会议上发表论文多篇，并获 ECOOP“杰出软件制品奖”和 Programming 期刊“编委会选择奖”。曾任 Scala 研讨会主席，IFL 程序委员，PLDI、OOPSLA 软件制品审查委员，以及多个编程语言会议的审稿人。他在本次会议的演讲内容如下：</p><p></p><p>演讲：低代码编程语言 NASL 从设计到落地的闯关之路</p><p></p><p>NASL 是由网易自研的全栈可视化编程语言，是支撑网易数帆 CodeWave 智能开发平台的基石。本次演讲将围绕 NASL 语言的设计初衷、设计原则、实现挑战、未来展望等方面展开。</p><p></p><p>演讲提纲：</p><p></p><p>NASL 的设计初衷</p><p>○ 为什么低代码平台需要编程语言</p><p>○ CodeWave 及 NASL 的简介</p><p>NASL 的设计原则</p><p>○ 低门槛、高上限</p><p>○ 记号的认知维度</p><p>○ 编程系统的技术维度</p><p>NASL 的实现挑战</p><p>○ 如何融合企业的 IT 资产</p><p>○ 如何降低实现成本</p><p>NASL 的未来展望</p><p>○ LLM 时代的编程语言设计</p><p>○ 文本语法和标准化</p><p></p><p>听众收益点：</p><p></p><p>○ 可视化编程语言和编程系统的设计原则</p><p>○ 降低编程语言实现成本的方法</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 9 折优惠仅剩最后 5 天，现在购票立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</id>
            <title>南京大外企将研发撤离中国，最高赔偿N+8；OpenAI回应GPT-4变懒；周星驰Web3团队下月上线独立App | AI一周资讯</title>
            <link>https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/roW4NR1hJzIyl1oikzE3</guid>
            <pubDate></pubDate>
            <updated>Sun, 10 Dec 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图森未来, OpenAI, GPT-4, Gemini, 夸克大模型
<br>
<br>
总结: 图森未来计划裁减75%在美员工，OpenAI的GPT-4遭到用户投诉，Gemini的性能被指控“造假”，阿里的夸克大模型通过备案，快手进行了年内最大规模的组织架构调整。 </div>
                        <hr>
                    
                    <p></p><blockquote>“自动驾驶卡车第一股”图森未来缩减美国业务，拟裁减75%在美员工；商汤科技 AI 编程助手“代码小浣熊 Raccoon”开放公测；快手开启年内最大规模组织架构调整，涉主站、电商等多个业务线……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>OpenAI回应GPT-4变懒</h4><p></p><p></p><p>OpenAI 的 GPT-4 大语言模型日前遭到部分用户投诉，部分用户表示，这段时间使用 ChatGPT 或 GPT-4 API 时，会遇到高峰期速度非常慢、敷衍回答、拒绝回答、中断会话等一系列问题。</p><p></p><p>北京时间周五中午，ChatGPT 官方通过 X 平台通知用户，“我们听到了你们关于 GPT-4 变得越来越懒的反馈！我们自 11 月 11 日起就没有更新过模型了，当然这不是故意的。”</p><p></p><h4>微软与OpenAI合作面临英国审查</h4><p></p><p></p><p>12月9日消息，当地时间周五英国监管机构英国竞争与市场管理局（CMA）表示，正在就微软与ChatGPT开发商OpenAI之间的合作关系进行评估，看是否有必要进一步展开反垄断调查。</p><p></p><h4>马斯克：Grok AI测试版现已向美国所有X Premium+订阅者开放</h4><p></p><p></p><p>12月8日，埃隆·马斯克在社交媒体上发文称，Grok AI测试版现已向美国所有X Premium+订阅者开放。据悉，现有 X 平台用户可以每月花费 16 美元或每年 168 美元来进行订阅。</p><p></p><p>马斯克此前表示，Grok 使用来自公开数据的数十亿个数据点进行训练，但是目前尚不清楚使用了哪些数据。此外他还提到 Grok 将能够实时访问 X 平台，因此与其他生成式人工智能相比这是一个巨大的优势。</p><p></p><h4>谷歌承认Gemini演示视频经特殊剪辑处理</h4><p></p><p></p><p>美东时间12月6日，谷歌CEO桑达尔・皮查伊宣布迄今为止规模最大，能力最强的谷歌大模型Gemini 1.0 版正式上线。Gemini是原生多模态大模型，是谷歌大模型新时代的第一步，它包括三种量级：能力最强的 Gemini Ultra，适用于多任务的 Gemini Pro，以及适用于特定任务和端侧的 Gemini Nano。</p><p></p><p>不过，外界已开始有声音指控谷歌对Gemini的性能“造假”。彭博社一篇专栏文章就表示，谷歌在一段演示视频中歪曲了Gemini的AI性能。专栏作家帕米·奥尔森（Parmy Olson）认为，在谷歌发布的这段视频中，Gemini似乎非常强大，但有点过于强大了。对此质疑，谷歌回应时承认，这段关于Gemini性能演示的视频并不是实时的，而是使用了原始镜头中的静止图像帧，然后编写了文本提示，以便让Gemini做出回应。</p><p></p><h4>阿里夸克大模型已通过备案</h4><p></p><p></p><p>日前，阿里智能信息事业群自研的夸克大模型已通过备案，将陆续在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列AIGC创新应用。夸克相关负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。在搜索应用中，将通过图文多模理解、专业知识生成、交互方式创新进一步拓宽应用场景，提升用户体验。</p><p></p><h4>周星驰Web3团队下月上线独立App</h4><p></p><p></p><p>据新浪科技报道，12月7日下午消息，据接近周星驰团队人员对新浪科技透露，周星驰旗下Web3初创公司Moonbox 最早将于明年1月份完成上线Moonbox App，届时App将免费向用户开放。目前，App研发工作已经基本完成，Moonbox团队在 NFT 玩法上下了很多功夫，已设计出基于AI和NFT聊天的互动玩法。</p><p></p><p>据上述知情人士透露，伴随着Moonbox App的独立上线，“周星驰将以Moonbox First Creator身份与大家见面”。与此同时，周星驰参与创作的Nobody NFT新品，也将随之发售，用户可以通过App和每个Nobody NFT角色聊天互动以了解人物性格、爱好、背景故事。</p><p></p><h4>“自动驾驶卡车第一股”图森未来缩减美国业务，拟裁减75%在美员工</h4><p></p><p></p><p>近日，图森未来向美国证券交易会提交的一份报告显示，公司将裁撤150名在美员工，约为美国员工总数的75%，全球员工总数的19%。这是图森未来继去年12月和今年5月的裁员后，再一次进行人员削减。</p><p></p><p>图森未来预计，此次重组计划将产生约700万至800万美元费用，大部分用于支付遣散费、员工福利和相关费用，重组费用将在2023年第四季度入账。</p><p></p><p>据华尔街日报报道，本次裁员后，图森未来在美人数仅为30人，将负责图森未来美国业务的收尾工作，逐步出售公司在美资产，并且协助公司向亚太地区转移。因此，此次裁员意味着图森未来或将彻底退出美国市场。</p><p></p><h4>通义千问登顶HuggingFace开源大模型排行榜榜首</h4><p></p><p></p><p>12月8日消息，全球最大的开源大模型社区HuggingFace日前公布了最新的开源大模型排行榜，阿里云通义千问力压Llama2等国内外开源大模型登顶榜首。</p><p></p><p>HuggingFace的开源大模型排行榜（Open LLM Leaderboard）是目前大模型领域最具权威性的榜单，收录了全球上百个开源大模型，测试维度涵盖阅读理解、逻辑推理、数学计算、事实问答等六大评测。通义千问（Qwen-72B）表现抢眼，以73.6的综合得分在所有预训练模型中排名第一。</p><p></p><h4>商汤科技 AI 编程助手“代码小浣熊 Raccoon”开放公测</h4><p></p><p></p><p>12月7日，商汤科技官微宣布，基于商汤自研大语言模型的智能编程助手——代码小浣熊Raccoon，即日起开放公测。据介绍，在实际应用中，代码小浣熊可帮助开发者提升编程效率超50%；未来，应用代码小浣熊，开发者可以将80%的编写工作交由AI完成。</p><p></p><h4>特斯拉Dojo超算项目被曝更换负责人</h4><p></p><p></p><p>外媒援引知情人士消息称，特斯拉Dojo超级计算机的项目负责人Ganesh Venkataramanan已经于11月份离职。在过去五年中，Venkataramanan一直在领导Dojo项目的推进工作，加入特斯拉前他在AMD担任了近15年的长期工程总监。现在Dojo项目由Peter Bannon负责，Bannon已经在特斯拉担任高管近8年，之前还在苹果公司中任职超过7年。</p><p></p><h4>快手开启年内最大规模组织架构调整，涉主站、电商等多个业务线</h4><p></p><p></p><p>12月7日消息，快手发布内部邮件宣布新一轮组织调整。此次组织调整涉及主站、电商、商业化、杜区科学等多个业务线，属于今年以来最大范围的一次组织架构调整。其中，商业化事业部下本地消费业务部调整至主站线下，更名为招聘房产业务部，负责快聘、房产相关业务，取消主站产品部，主站线下成立孵化产品部，负责快影、一甜相机、回森等独立APP产品。</p><p></p><h4>南京大外企将研发撤离中国，裁员赔偿最高N+8</h4><p></p><p></p><p>近日，南京知名外企趋势科技计划搬离国内。知情人士透露，趋势科技打算将核心技术从国内转移到加拿大，因此裁员只涉及研发部门，其他部门几乎没有调整，共计约 70 人左右，赔偿 N+4 起步，一些老员工则超过 N+8。</p><p></p><p>据悉，该公司从上个月就开始裁员了，目前已接近尾声。值得一提的是，趋势科技本次撤离还会带走一部分员工，同意去加拿大的话也可以协调不裁。</p><p></p><h4>苹果因故意降低性能被判赔偿韩国7名用户每人7万韩元</h4><p></p><p></p><p>据韩联社消息，6日，韩国7名消费者集体起诉苹果通过升级系统降低旧款iPhone性能案二审宣判，法院判处原告部分胜诉。</p><p></p><p>据报道，韩国首尔高等法院民事当天开庭审理，判处苹果向原告每人支付7万韩元(约合人民币382元)赔偿金。</p><p></p><p>据悉，法院对原告所谓“苹果升级iOS系统属于发布恶意程序或损坏iPhone手机性能”的主张不予采纳，但法院认为，即使更新操作系统旨在防止手机自动关机，但也限制了中央处理器(CPU)等性能。苹果有义务向消费者说明是否安装更新，但苹果违反这一规定。同时，消费者因选择权被侵害而产生精神损失，认定苹果有赔偿责任。</p><p></p><h4>王慧文入股OneFlow团队新创业项目</h4><p></p><p></p><p>近日，北京硅动科技有限公司发生工商变更，新增王慧文为股东，同时注册资本由100万人民币增至约105.26万人民币。该公司成立于今年8月，法定代表人、执行董事、经理为OneFlow创始人袁进辉，公司经营范围含软件开发、技术进出口、电子产品销售、人工智能应用软件开发、人工智能通用应用系统、人工智能行业应用系统集成服务等。</p><p></p><p>公开信息显示，王慧文病休后，光年之外收购的核心团队OneFlow宣布重新创业。袁进辉称，新创业项目拟解决大模型推理成本问题。天眼查显示，目前，王慧文仍为OneFlow关联公司北京一流科技有限公司董事。</p><p></p><h4>量子计算技术重磅升级：IBM展示最新的模块化量子处理器</h4><p></p><p></p><p>当地时间周一（12月4日），IBM在官方博客发文，展示了“量子效用”所需的硬件和软件，其中包括新的量子处理器芯片和量子计算系统。</p><p></p><p>新闻稿称，IBM展示了一种新方法：将芯片连接到机器内部，再将机器连接到一起，以形成模块化系统，使规模的扩展不受物理条件限制。IBM称，将这种方法叠加新的纠错码，有望在2033年之前制造出引人注目的量子机器。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>投票开除奥特曼的董事发声：OpenAI之乱跟AI安全没关系</h4><p></p><p></p><p>在上个月令全球科技圈震惊的OpenAI“内乱100小时”中，AI圈的顶流明星山姆·奥特曼在遭到董事会扫地出门后又迅速凯旋而归。即便如此，由于事发后核心人物鲜少谈及幕后的考量，整件事情至今还留有诸多疑问。</p><p></p><p>当地时间周四，已经离开OpenAI董事会的Helen Toner公开发声，对于外界的诸多疑问和“知情人士消息”做出一些回应。</p><p></p><p>Toner表示，董事会开除奥特曼的原因与AI安全没有关系，而是“缺乏信任”。她进一步解释称：“我们解雇山姆的目的，是为了加强OpenAI并使其更有能力实现其使命。”</p><p></p><p>在面对OpenAI的律师试图施压董事会辞职时，她也坚持了这一立场。Toner介绍称：“律师试图声称，如果我们不立即辞职，将会违法。因为若公司因此崩溃，我们将违反受托责任。但OpenAI是一个非常特殊的组织，非营利使命——确保人工通用智能（AGI）惠及全人类——是最重要的。”</p><p></p><p>事实上，在面对律师强调“公司会因此崩溃”时，Toner回应称“这样也符合我们的使命”，令房间里的一众公司高管感到吃惊。对于这一点，Toner也补充道，这句话是对律师“恐吓策略”的回应。她试图表达的是：对于创建造福全人类的AGI这一使命而言，OpenAI的持续存在并不是必要条件。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</id>
            <title>FCon 演讲视频：数字人民币（e-CNY）赋能支付业态发展</title>
            <link>https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O324HBbsMqKFC0ohcZWq</guid>
            <pubDate></pubDate>
            <updated>Sat, 09 Dec 2023 03:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 全球数字货币, 中国数字人民币, 金融技术变革, 数字货币的安全性
<br>
<br>
总结: 随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。 </div>
                        <hr>
                    
                    <p>随着全球数字货币的兴起，特别是中国数字人民币（e-CNY）的发展，我们正见证一个重大的金融技术变革。数字人民币的推出不仅仅是一种新型支付方式的出现，更是对现有金融生态系统的重塑。在这个变化中，既有机遇也有挑战，特别是在数字货币的安全性、普及性和监管方面。了解这些关键点，将有助于我们更好地理解数字货币的未来发展趋势及其可能带来的影响。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了苏州银行网络金融部高级产品经理<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5574">金一松</a>"，他以主题为《数字人民币（e-CNY）赋能支付业态发展》展开了分享，以下为重点内容概述：</p><p></p><p>数字人民币的定义与特性：详细讨论了数字人民币的定义、设计特点，包括与传统货币的区别、发行和流通方式，以及它在支付体系中的角色。数字人民币的母子钱包体系和软硬钱包形态：探讨了数字人民币的钱包体系，包括母子钱包体系的结构和软硬钱包的不同形态。无网无电支付能力与智能合约应用：强调了数字人民币在无网络和无电源环境下的支付能力，以及智能合约在数字货币中的应用。数字人民币的未来应用前景：展望了数字人民币未来的发展方向，包括在不同场景中的应用潜力和可能的创新应用。</p><p></p><p>通过深入了解这些重点内容，我们可以更全面地认识数字人民币的影响力及其在未来金融生态中的潜在角色。详细内容，请观看完整视频：</p><p></p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</id>
            <title>夸克大模型通过备案 将升级通识、健康、创作等搜索产品与智能工具</title>
            <link>https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/d4rIQ0vpsW9b06qToiji</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 10:16:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 阿里智能信息事业群, 夸克大模型, AIGC 创新应用, AI助手
<br>
<br>
总结: 阿里智能信息事业群自研的夸克大模型已通过备案，将在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列 AIGC 创新应用，借助大模型能力全面升级夸克，提升用户在学习、工作、生活上的效率。夸克App将在自研大模型的助力下，加速迈向年轻人的AI助手。 </div>
                        <hr>
                    
                    <p>日前，记者获悉阿里智能信息事业群自研的夸克大模型已通过备案，将陆续在通识、健康、创作等领域升级内容产品与智能工具，并落地一系列 AIGC 创新应用，借助大模型能力全面升级夸克，提升用户在学习、工作、生活上的效率。</p><p></p><p>今年下半年，国内多款大模型已经完成备案且能力水平部分超过 GPT-3.5，广大用户也都期待爆款产品的出现以更好地解决方方面面的实际问题。作为深受年轻人喜欢的信息服务产品，夸克App将在自研大模型的助力下，加速迈向年轻人的AI助手。</p><p></p><p>今年11月中旬，阿里巴巴智能信息事业群发布全栈自研、千亿级参数的夸克大模型，将应用于通用搜索、医疗健康、教育学习、职场办公等众多场景。夸克大模型也凭借四大优势，接连登顶 C-Eval 和 CMMLU 两大权威榜单。同时在法律、医疗、问答等领域的性能评测中夺冠，成为了名副其实的“学霸”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2026ddaf74dacff9fec92c89fb31921.png" /></p><p></p><p>清华大学新闻学院教授、博士生导师沈阳认为，依托搜索平台，夸克大模型拥有高质量的各类数据，在中文语境下，模型能力处在行业领先水平。在教育、医疗等垂直领域中，夸克在对话、解题上的能力取得了新的突破，是国产自研大模型的优秀代表之一。</p><p></p><p>夸克相关负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。在搜索应用中，将通过图文多模理解、专业知识生成、交互方式创新进一步拓宽应用场景，提升用户体验。同时，在健康等垂直领域中，夸克将依托大模型能力，提供更加实用的信息服务。</p><p></p><p>目前，夸克 App 已经为数千万 95 后职场人和大学生提供了跨场景的智能效率工具。根据 QuestMobile发布的《2023年轻人群智能效率应用研究》报告显示，夸克 App 在泛学生人群和新生代职场人群的用户占比最高，年轻用户使用时长位列行业第一。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</id>
            <title>国内首份“图风控”报告发布：图风控成应对新型网络安全风险的关键性技术</title>
            <link>https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qcE019AscmDYTUJUfIHX</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 09:51:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 图风控技术, 数据关联性特征, 风险识别, 图智能技术
<br>
<br>
总结: 《图风控行业技术报告》指出，图风控技术是应对AI时代复杂风险的关键技术，利用数据关联性特征实现了大规模时序关系图的构建和实时风险识别。图智能技术以直观、高效、智能的方式分析实体之间的复杂交互关系，提升风险识别的准确性和及时性。图风控技术已在支付、信贷、电商等领域得到广泛应用，成为金融机构和科技公司关注的新发展趋势。 </div>
                        <hr>
                    
                    <p>12 月 8 日，国内首份《图风控行业技术报告》（以下简称“报告”）在北京发布，指出智能风控迈入“全图时代”，图智能应用于<a href="https://www.infoq.cn/article/TDdJaEAY6dBu474REtL0?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">风控</a>"领域形成的“图风控技术”成为应对 AI 时代复杂风险形势的下一代风控基础设施和关键性技术。报告认为，图风控充分利用了海量数据时代的数据关联性特征，实现了大规模时序关系图的高效构建及全周期实时风险识别，在解决黑产复杂隐蔽、信息孤岛等挑战，挖掘更多隐藏风险等方面提供了强大的技术功能和应用价值。</p><p>&nbsp;</p><p>据了解，该报告由<a href="https://www.infoq.cn/article/bjCH8kMloxFUfp00WQIX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">蚂蚁集团</a>"、清华大学、北京邮电大学、中山大学、上海交通大学、复旦大学、之江实验室和<a href="https://www.infoq.cn/article/EE2bAVOOWa0K_g5lLh7j?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阿里巴巴</a>"淘天集团联合编写，中国人民大学国际货币研究所（IMI）、金融科技50人论坛（CFT50）提供学术支持，详细呈现了新型数字风险态势、图风控算法技术、图数据库等底层基础设施，并提供了丰富的行业应用案例。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ed/ed12501a3e99fa960e9b079af7002bb6.png" /></p><p>图：《图风控行业技术报告》发布现场</p><p>&nbsp;</p><p>数字化智能化的颠覆性变革正在带来全新的安全挑战。尤其是 AI 大规模渗透应用引发新一轮智能化浪潮，带来新型数字经济网络中数据复杂度和关联性呈几何倍增，也带来了更加复杂、隐蔽、强对抗和更具破坏力的安全威胁。传统的风控方式已难以抵御多样化的风险形势，越来越多的场景需要更智能化的技术利器。图风控技术的出现，提供了一种解决问题的利器。</p><p>&nbsp;</p><p>课题组专家、北京邮电大学教授、博士研究生导师石川在报告中指出，智能风控技术历经专家策略、机器学习和深度学习的演进，如今图智能技术正逐渐成熟。在金融、电商、安全、社交等领域，风险涉及多个实体之间的复杂交互关系。图智能技术以更直观、高效、智能的方式表达和分析这些交互关系，助力系统发现潜在风险中的隐藏模式和异常，进而提升对潜在风险的准确性和及时性识别。</p><p>&nbsp;</p><p>具体来说，“图”是一种以点和边来表示实体和关系的数据结构；“图智能技术”指包括图数据库、图计算引擎、图神经网络、图可解释等一系列和图有关的人工智能技术通称，是最适应大数据海量、动态等特征的技术之一；应用于风险控制领域而形成的“图风控技术”，可以聚合风险事件、交易属性、关系图谱、专家特征等各类动态变化的风险数据，结合图结构数据的可解释性，实现对风险全链路、基于关系视角的刻画，为风控从业者提供更加全面、可见、实时的风险监测并及时决策。因此，运用图技术提升风控系统能力，正成为行业的新发展趋势。</p><p>&nbsp;</p><p>图风控技术目前在业界已有成熟应用，涵盖支付风控、信贷风控、电商风控，以及供应链、网络安全和基础设施安全等多个领域，是金融机构、安全服务商、新兴初创企业，以及大型科技公司逐浪的“风控风口”。</p><p>&nbsp;</p><p>蚂蚁集团副总裁、大安全事业群总裁赵闻飙在报告中表示，数字经济时代，安全的重要性日益凸显。图风控技术作为蚂蚁集团重点研发投入的创新技术之一，现已成为强化风险管理的利器，对构筑坚固的安全防线作出了重要贡献。</p><p>&nbsp;</p><p>报告显示，蚂蚁集团从 2015 年开始探索图技术，推出了底层自研的大规模图风控基础设施 TuGraph。基于 TuGraph 布局的全图风控体系，打造了万亿级点边规模的全域风险大图，目前已全面应用在业务场景中，不仅实现了支付过程的毫秒级极速风控，支撑了高频交易的高精准度识别，还显著降低了资损率，提高了反欺诈和反洗钱等安全业务的效率。</p><p>&nbsp;</p><p>据了解，全图风控是蚂蚁集团智能风控体系“IMAGE”的重要组成部分。该体系还包括交互式主动风控、端边云协同风控、多方安全风控、智能对抗，支撑了支付宝资损率连续三年低于亿分之一，为解决风控的智能化、主动性、可预测性、隐私保护等世界级难题提供了新突破，获得 CCF 科学技术奖、吴文俊人工智能科学技术奖、浙江省科学技术奖等多个权威奖项。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</id>
            <title>FCon 最新演讲视频：大模型在金融领域的落地探索</title>
            <link>https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OkAUyMNBwo5FHSVxsMST</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 08:42:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术发展, 大数据, 大模型, 金融行业
<br>
<br>
总结: 在金融行业，随着技术的快速发展，大数据和大模型正在逐渐成为推动行业创新的重要力量。这种变革不仅在风险管理和预测方面展现出巨大潜力，而且在促进金融机构与科技公司之间的合作、推动数字化转型，以及优化数据管理和治理方面也显示出其独特价值。 </div>
                        <hr>
                    
                    <p>在金融行业，随着技术的快速发展，大数据和大模型正在逐渐成为推动行业创新的重要力量。这种变革不仅在风险管理和预测方面展现出巨大潜力，而且在促进金融机构与科技公司之间的合作、推动数字化转型，以及优化数据管理和治理方面也显示出其独特价值。然而，在这一进程中，行业也面临着如可解释性、社会智能等一系列挑战。在<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5517">FCon全球金融科技大会</a>"上，我们邀请了光大信托信息技术部副总经理、数据中心总经理祝世虎 博士，为你分享了大模型在金融领域的应用及其带来的机遇与挑战。以下为分享的重要内容：</p><p></p><p>大数据、大模型与风控的关系：探讨了大数据和大模型如何影响金融领域的风险控制，特别是如何通过数据分析和模型预测来管理和减少风险。大合作与创新：讨论了金融机构与科技公司之间的合作以及这种合作如何促进创新，特别是在开发和应用大型模型方面。关注的问题：提出了金融行业在采用大型模型时面临的一些挑战和问题，例如可解释性、社会智能等。数字化转型对大模型的助力：分析了数字化转型如何助力大模型在金融行业的发展和应用。数据信托与大模型：讨论了数据信托如何帮助管理和优化大模型，特别是在处理和保护数据方面。大模型的治理：探索了在金融行业中应用大模型时需要考虑的治理问题，包括伦理和法律方面的考量。</p><p></p><p>详细内容，请观看完整视频：</p><p></p><p></p><p>活动推荐：</p><p>QCon 全球软件开发大会（上海站）即将在 12 月 28-29 日开始，届时将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</id>
            <title>如何看待 OpenAI Q* 谣言</title>
            <link>https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/N6cVIbuu90KyyRVP2Q7G</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:55:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, Q*, AI模型, 数学问题
<br>
<br>
总结: OpenAI取得了一项重大技术突破，开发出了名为Q*的AI模型，具备解决全新数学问题的能力。这一突破可能标志着迈向具有一般推理能力的通用人工智能的重要一步。通过分步推理技术，大语言模型可以更好地解决复杂的数学问题。OpenAI的研究还包括训练验证器模型来评估每个步骤的正确性，以提高推理结果的准确性。 </div>
                        <hr>
                    
                    <p>作者 ｜ Timothy B Lee</p><p>译者 ｜ 核子可乐</p><p>策划 ｜ Tina</p><p>&nbsp;</p><p></p><blockquote>OpenAI仍未明确解释Q*究竟是什么，但透露的线索倒是相当不少。</blockquote><p></p><p>&nbsp;</p><p>11月22日，就在OpenAI决定解雇（后又重新聘用）CEO Sam Altman的几天之后，技术媒体The Information报道称OpenAI取得了一项重大技术突破，使其能够“开发出更强大的AI模型”。新模型被命名为Q*（音为「Q star」），“具备解决全新数学问题的能力。”</p><p>&nbsp;</p><p>路透社也发表了类似的报道，但细节同样含糊不清。</p><p>&nbsp;</p><p>两篇报道都将这项突破与董事会解雇Altman的决策联系起来。路透社在报道中指出，几名OpenAI员工向董事会发函，“警告称这项强大的AI发现可能对人类构成威胁。”然而，“路透社未能拿到这封信的副本”，随后的报道也没有继续将Altman下台与Q*一事联系起来。</p><p>&nbsp;</p><p>The Information指出，今年早些时候，OpenAI开发出“能够解决基本数学问题的系统，攻克了这一对现有AI模型来说颇为艰巨的任务。”路透社则表示Q*“具备小学生水平的数学计算能力。”</p><p>&nbsp;</p><p>为了避免妄下结论，我们又花了几天时间搜集相关内容。OpenAI确实没有公布Q*项目的详细信息，但发表了两篇关于其解决小学数学问题的论文。在OpenAI之外，不少研究人员（包括Google DeepMind的研究人员）也一直在这方面开展探索。</p><p>&nbsp;</p><p>我个人怀疑Q*正是指向通用人工智能（AGI）的关键技术突破。虽然不一定会对人类构成威胁，但这可能标志着迈向具有一般推理能力的AI的重要一步。</p><p>&nbsp;</p><p>在本文中，我们将一同了解AI研究领域的这一重大事件，并解释专为数学问题设计的分步推理技术如何发挥关键作用。</p><p>&nbsp;</p><p></p><h1>分步推理的力量</h1><p></p><p>我们首先考虑以下数学问题：</p><p></p><blockquote>John给了Susan五个苹果，之后又给了她六个。之后Susan吃掉其中三个，又给了Charlie三个苹果。她把剩下的苹果给了Bob，Bob吃掉一个。接下来，Bob把手中半数苹果给了Charlie。John给了Charlie七个苹果，Charlie将手中三分之二的苹果给了Susan，最后Susan又把其中四个还给了Charlie。问，现在Charlie还剩几个苹果？</blockquote><p></p><p>&nbsp;</p><p>大家可以先试着自己算算。</p><p>&nbsp;</p><p>其实我们都在小学阶段学过简单的加减乘除，所以看到问题里说“John给了Susan五个苹果，之后又给了她六个”，就知道这时候Susan有11个苹果。</p><p>&nbsp;</p><p>但对于更复杂的问题，那人类在尝试解决时就需要借助笔算或者心算了。比如在此问题中，先有5+6=11，之后是11-3=8，接着8-3=5，以此类推。通过一步步思考，我们最终会得到正确答案：8。</p><p>&nbsp;</p><p>同样的技巧也适用于大语言模型。在2022年1月发表的著名论文中，谷歌研究人员指出，如果大语言模型能按照提示词分步进行推理，就会产生更好的结果。以下是论文中的一份关键图表：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8aa3c3a12e1e619a196d0456cebedb1.png" /></p><p></p><p>&nbsp;</p><p>这篇论文的发表时间还早于“零样本”提示技术，因此研究人员通过给出示例答案的方式来提示模型。在左图中，系统会提示模型直接给出最终答案，但结果是划的。而在右侧，系统会一步步提示模型并最终推理出正确答案。谷歌研究人员将这项技术称为“思维链提示法”，且至今仍被广泛应用。</p><p>&nbsp;</p><p>对于大语言模型来说，“五”和“六”这样的数字只是token，跟“这”、“那”或者“猫”没什么区别。这些模型之所以能把大写数字转换成5+6=11，是因为这个token序列曾经在训练数据中出现过。但大模型的训练数据中可能并不包含长计算示例，比如((5+6-3-3-1)/2+3+7)/3+4=8，所以如果要求模型直接给出计算结果，那它就很可能搞不清状况并生成错误答案。</p><p>&nbsp;</p><p>或者用另一种思路来解释，大语言模型没有可用于记忆中间结果（例如5+6=11）的外部“临时空间”。而思维链推理使得大模型能够有效使用自己的输出作为暂时记忆空间，从而将复杂问题拆分成更多步骤——每个步骤都可能与模型训练数据中的示例相匹配。</p><p>&nbsp;</p><p></p><h1>解决更复杂的数学难题</h1><p></p><p>&nbsp;</p><p>在谷歌发表关于思维链提示法论文的几个月前，OpenAI曾经推出一套包含8500道小学数学应用题的GSM8K数据集，以及一篇描述问题解法新技术的论文。OpenAI没有让模型逐一给出答案，而是要求其一次性给出100个思路答案，再通过名为验证器的另一套模型对各个答案进行评分。在这100条回复中，系统将只返回评分最高的答案。</p><p>&nbsp;</p><p>乍看起来，训练验证器模型也需要大费周章，难度不啻于训练大语言模型来生成正确答案。但从OpenAI的测试结果来看，情况并非如此。OpenAI发现只需小型生成器与小型验证器的组合，就能提供与单独使用超大生成器模型（参数是前者的30倍）相当的结果。</p><p>&nbsp;</p><p>2023年5月的一篇论文介绍了OpenAI在该领域的最新研究情况。OpenAI已经跨越小学数学，开始研究更具挑战性的MATH数据集。OpenAI现在不再让验证器对完整答案打分，而是训练验证器具体评估各个步骤，具体参见论文给出的下图：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/30/308ab3a9a944e10fa406a17b643ca6ed.png" /></p><p></p><p>&nbsp;</p><p>每一步都有一个绿色笑脸符号，代表该步骤处于正确的思路之上，直到最后一步模型得出“x=7”，这时打出的是红色的皱眉符号。</p><p>&nbsp;</p><p>文章得出的结论是，在推理过程中的各个步骤上都使用验证器，其结果比直接验证最终答案更好。</p><p>&nbsp;</p><p>这种逐步验证方法的最大缺点，就是更难实现自动化。MATH训练数据集中包含每个问题的正确答案，因此很容易自动检查模型是否得出了正确的结论。但OpenAI未能找到更好的方法来自动验证中间步骤。于是，该公司只能聘请了一些审查员，为7.5万个解题思路的共80万个计算步骤提供反馈。</p><p>&nbsp;</p><p></p><h1>求解路漫漫</h1><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0be88a7f03a5274850c9978a79424c1e.png" /></p><p></p><p>&nbsp;</p><p>需要注意的是，GSMK8K和MATH数据集中的问题至少还可以通过分步方式简单解决。但在实际应用中，相当一部分数学问题根本无法拆解，例如：</p><p>&nbsp;</p><p>你正在筹划一场分五张餐桌、每桌三位客人的婚宴。</p><p></p><blockquote>Alice不想跟Bethany、Ellen或者Kimmie一起坐。Bethany不想跟Margaret一起坐。Chuck不想跟Nancy一起坐。Fiona不想跟Henry或者Chuck一起坐。Jason不想跟Bethany或Donald一起坐。Grant不想跟Ingrid、Nancy或Olivia一起坐。Henry不想跟Olivia、Louise或Margaret一起坐。Louise不想跟Margaret或Olivia一起坐。要如何安排客人座位，才能充分满足他们的要求？</blockquote><p></p><p>&nbsp;</p><p>在把这样的提示词输入GPT_4时，它开始分步进行问题推理：</p><p>餐桌1：Alice、Chcuk和Donald。餐桌2：Bethany、Fiona和Ellen。餐桌3：Jason、Grant和Ingrid。</p><p>&nbsp;</p><p>但到第四张餐桌时，它就卡住了。这时候Henry、Margaret和Louise还没有入座，他们彼此都不想坐在一起，但接下来只剩两张桌子可以安排。</p><p>&nbsp;</p><p>在这个问题中，我们不知道GPT-4具体错在哪个具体步骤上。它在前三张桌子的安排上完全满足规则，但这些前期选择也导致余下的客人没办法正确入座。</p><p>&nbsp;</p><p>这就是计算机科学家们所说的NP难题，即不存在通用算法以线性方式加以解决。唯一的办法就是尝试一种可能的安排，看看是否符合要求，如果不行则推倒重来。</p><p>&nbsp;</p><p>GPT-4可以通过在上下文窗口中添加更多文本来完成回溯，但其扩展能力仍然有限。更好的方法是为GPT-4提供一个“退格键”，这样它就能删除最后一个或几个推理步骤，然后重试。为此，系统还需要一种方法来跟踪它已经尝试过的组合，避免重复尝试。如此一来，大语言模型就能探索下图所示的可能性树：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9ed8f78ee2b5f43a4ff715f21bb0f4e1.png" /></p><p></p><p>&nbsp;</p><p>今年5月，普林斯顿大学和Google DeepMind的研究人员共同发表论文，提出一种名为“思路树”的方法。思路树不再用单一推理链来解决问题，而是允许大模型系统探索一系列指向不同方向的推理链“分支”。</p><p>&nbsp;</p><p>研究人员发现，该算法在解决某些传统大语言模型难以解决的问题上表现良好。其中不仅包括所谓“24点游戏”（即通过添加运算符号将随机给出的几个数字计算为24），还实现了创意写作能力。</p><p>&nbsp;</p><p></p><h1>AlphaGo模型</h1><p></p><p>以上，就是OpenAI和DeepMind迄今为止发表过的所有研究成果，可以看到他们都在让大语言模型更好地解决数学问题方面付出了不懈努力。现在，我们一起来推测这项研究最终可能会走向何方。当然，这些猜测没有任何依据，大家也可以根据自己掌握的情况做出展望。</p><p>&nbsp;</p><p>今年10月，播客Dwarkesh Patel曾就通用人工智能开发计划采访过DeepMind联合创始人兼首席科学家Shane Legg。Legg认为，迈向AGI的关键一步就是把大语言模型跟搜索可能响应的树结构结合起来：</p><p></p><blockquote>这些基础模型属于某种世界模型，通过搜索方式实现问题的创造性解决能力。以AlphaGo为例，它那惊人的棋路到底是从何而来？是学习了人类棋手的经验，还是参考了原有数据？不，根本没有。它其实是选择了一个非常罕见、但也极为合理的棋步，再通过搜索过程思考这步棋会造成怎样的后续影响。也就是说，要想获得真正的创造力，必须探索可能性空间并找出隐藏其中的最佳答案。</blockquote><p></p><p>&nbsp;</p><p>Legg在这里提到了著名的“第37手”，即2016年DeepMind AlphaGo软件与顶尖棋手李世石第二场比赛中的一步。大多数人类选手最初都觉得AlphaGo在这步棋上出现了失误，但其最终刻了比赛，且复盘分析发现这是一手强棋。换言之，AlphaGo表现出了超越人类棋手的布局洞察力。</p><p>&nbsp;</p><p>AlphaGo能够根据当前棋盘状态模拟出数千种可能的后续发展，从而获取类似的见解。对于计算机来说，潜在棋序实在太多，根本不可能一一检查，所以AlphaGO使用神经网络来简化整个过程。</p><p>&nbsp;</p><p>其中的策略网络能够预测出哪些棋路最有希望，值得进一步做模拟分析。而价值网络则负责估算棋盘的当前状态是对白方有利、还是对黑方有利。根据这些估算，AlphaGo再逆向计算下面一步该怎么走。</p><p>&nbsp;</p><p>Legg的观点是，这类树搜索方法有望提高大语言模型的推理能力。大语言模型要预测的不只是单个最可能出现的token，而应在给出回答之前探索数千种不同的响应。事实上，DeepMind的思维树论文似乎就是朝这个方向迈出的第一步。</p><p>&nbsp;</p><p>前文提到，OpenAI曾经尝试使用生成器（生成潜在答案）与验证器（估算这些答案是否正确）组合来解决数学问题。这与AlphaGo明显有几分相似，同样可以理解成策略网络（生成潜在棋步）与价值网络（估算这些棋步能否导向更有利的盘面状态）。</p><p>&nbsp;</p><p>如果将OpenAI的生成器/验证器网络与DeepMind的思维树概念相结合，就能得到一套与AlphaGo非常相似的语言模型，同时保留AlphaGo的强大推理能力。</p><p>&nbsp;</p><p></p><h1>为何命名为Q*</h1><p></p><p>在AlphaGO之前，DeepMind曾在2013年发表过一篇关于训练神经网络以打通雅达利电子游戏的论文。DeepMind并没有手动录入每款游戏的规则，而是让网络不断游玩这些游戏，通过反复试验自行理解玩法。</p><p>&nbsp;</p><p>参考早期强化学习技术Q-learning，DeepMind将这套雅达利解决方案命名为Deep Q-learning。DeepMind的雅达利AI中包含一个Q函数，用于估算任意特定操作（例如向左或向右推操纵杆）可能获得的奖励（比如更高的得分）。当系统游玩雅达利游戏时，它会不断优化Q函数，提升获取更佳得分的估算能力。</p><p>&nbsp;</p><p>DeepMind 2016年在AlphaGo论文同样使用字母Q来表示AlphaGo中的棋步价值函数——该函数用于估算任意给定棋步有多大可能通往对局胜利。</p><p>&nbsp;</p><p>AlphaGo和DeepMind的雅达利AI都属于强化学习的范畴，这是一种从经验中学习知识的机器学习技术。在大语言模型兴起之前，OpenA也I一直将强化学习作为关注重点。例如，OpenAI曾在2019年使用强化学习让机械臂在自行探索中学会解开魔方。</p><p>&nbsp;</p><p>参考这些背景，我们似乎可以对Q*做出有理有据的解读：它是将大语言模型同AlphaGo式搜索能力相结合的产物，而且应该是在以强化学习的方式进行混合模型训练。其重点就是找到一种在困难的推理任务中“自我较量”的方式，借此改进语言模型的实际能力。</p><p>&nbsp;</p><p>其中一条重要线索，就是OpenAI今年早些时候决定聘请计算机科学家Noam Brown。Brown在卡耐基梅隆大学获得博士学位，并在那里开发出首个能够超越人类水平的扑克AI。之后Brown加入Meta，并开发出玩《强权外交》桌游的AI。这款游戏的成功秘诀在于同其他玩家结成联盟，因此AI必须把战略思维与自然语言能力结合起来。</p><p>&nbsp;</p><p>由此看来，这似乎就是帮助大语言模型提高推理能力的绝佳案例。</p><p>&nbsp;</p><p>Brown今年6月在推文中表示，“多年以来，我一直在研究扑克和〈强权外交〉桌游中的AI自我对弈和推理课题。现在，我想探索如何将成果转化为普适性能力。”</p><p>&nbsp;</p><p>AlphaGo和Brown扑克AI中使用的搜索方法，明显只适用于这些特定游戏。但Brown预测称，“如果我们能发现一个通用版本，则必然带来巨大的收益。没错，推理速度可能会降低至千分之一且成本迅速膨胀，但如果能够发现新的抗癌药物、或者证明黎曼猜想，这一切难道不值得吗？”</p><p>&nbsp;</p><p>而在Brown于今年早些时候离职之后，Meta公司首席AI科学家Yann LeCun表示，他认为Brown研究的就是Q*。</p><p>&nbsp;</p><p>LeCun在11月的推文中指出，“看起来OpenAI更进一步的探索就是Q*，他们还聘请了Noam Brown来协助解决这个问题。”</p><p>&nbsp;</p><p></p><h1>两大挑战</h1><p></p><p>&nbsp;</p><p>如果大家跟科学家或者工程师共事时，就会注意到他们特别喜欢用白板。当我自己在研究生院学习计算机科学时，我们就经常站在白板前面绘制图表或者议程。随后在谷歌的实习经历，也让我意识到技术大厂里同样到处都是白板。</p><p>&nbsp;</p><p>白板确实很有启发意义，因为面对极为困难的技术问题，人们刚开始根本不知道该如何下手。他们可能会花几小时勾勒出了种潜在的解决思路，却发现根本就不适用。之后他们就擦掉一切，从零开始找个不同的切入角度。或者，他们也可能觉得方案的前半部分还行，于是擦掉后半部分再换条新的探索路线。</p><p>&nbsp;</p><p>这本质上就是一种智能树搜索：对多种可能的解决方案进行迭代，直到找出一个似乎可以实际解决问题的路线。</p><p>&nbsp;</p><p>OpenAI和DeepMind之所以对大语言模型加AlphaGo搜索树感到如此兴奋，就是因为他们希望计算机也能执行同样的开放式智能探索。到那个时候，我们只需要把充满挑战的数学问题输入给大语言模型，然后安心上床睡觉。第二天早上醒来，它已经考虑了几千种可能的解决方案，并最终给出一些可能有希望的探索方向。</p><p>&nbsp;</p><p>这当然是个鼓舞人心的愿景，但OpenAI至少还要克服两大挑战才能将其转化为现实。</p><p>&nbsp;</p><p>首先，就是找到一种让大语言模型进行“自我对弈”的方法。AlphaGo就是通过自我对弈完成了对顶尖人类棋手的碾压。OpenAI也在模拟物理环境中进行魔方实验，通过判断魔方是否处于“解开”状态来判断哪些操作有正向作用。</p><p>&nbsp;</p><p>而他们的梦想就是建立起一套大语言模型，通过类似的自动化“自我对弈”方式提高推理能力。但这就需要一种能够自动检查特定解决方案是否正确的办法。如果系统还需要人类来检查每条答案正确与否，那么训练规模将非常有限、难以带来可与人类匹敌的推理水平。</p><p>&nbsp;</p><p>就在2023年5月发表的论文中，OpenAI还在聘用审查员来核对数学答案的正确性。所以如果真的出现了突破，那肯定是发生在过去这几个月间。</p><p>&nbsp;</p><p></p><h1>学习是个动态的过程</h1><p></p><p>&nbsp;</p><p>我认为第二个挑战才是根本：通用推理算法，必须在探索各种可能性时表现出动态学习能力。</p><p>&nbsp;</p><p>当人们尝试在白板上推衍解题思路时，他们并不是在机械地迭代各种可能路线。相反，每试过一个失误的路线，人们对问题的理解也就又加深了一步。在推理过程中，他们的心理模型也在不断演进，逐渐生出能快速判断哪种方法更好的强大直觉。</p><p>&nbsp;</p><p>换句话说，人类内心的“策略网络”和“价值网络”并非一成不变。我们在同一个问题上花费的时间越多，在思考潜在答案时的判断能力也就增强，自然更善于预测当前思路是否有效。如果没有这种实时学习能力，我们一定会迷失在无穷无尽的潜在推理步骤当中。</p><p>&nbsp;</p><p>相比之下，目前大多数神经网络在训练和推理之间保持着严格的边界。一旦训练完成，AlphaGo的策略和价值网络就被固定下来了——后续任何比赛过程都不会产生改变。这对围棋来说没有问题，因为这项游戏的规则足够简单，可以在自我对弈的过程中体验各种可能的情况。</p><p>&nbsp;</p><p>但现实世界要比方寸棋枰复杂得多。从定义上讲，研究者想要解决的是以往未能解决过的问题，所以实际情况很可能与训练期间遇到的任何问题都存在巨大差异。</p><p>&nbsp;</p><p>因此，通用推理算法的实现必须在推理过程中持续获取见解，以便在模型解决问题的同时不断增强后续决策质量。然而，目前的大语言模型完全通过上下文窗口来维持状态，而思维树方法在现有模型的一个分支跳往另一分支时，之前的记忆信息会被新的上下文窗口直接删除。</p><p>&nbsp;</p><p>一种可能的解决方案，就是使用图搜索来取代树搜索。今年8月的一篇论文就提到这种方法，尝试让大语言模型将来自多个“分支”的见解结合起来。</p><p>&nbsp;</p><p>但我高度怀疑，真正的通用推理引擎恐怕需要在底层架构上做根本性创新。语言模型必须借助新的方法来学习超越训练数据的抽象概念，并利用这些不断发展的抽象概念强化探索潜在解决方案空间时的具体选择。</p><p>&nbsp;</p><p>我们都知道这绝非妄言，毕竟人类的大脑就能做到这一点。而OpenAI、DeepMind乃至其他厂商可能还需要一段时间，才能搞清楚如何把这种方法照搬到硅芯片之上。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors">https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</id>
            <title>多场开发实战课，百度智能云技术大咖现场教学！</title>
            <link>https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/I7oTGIxwXsUk2SGfQAyf</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:42:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型技术, 智能化跨越, 百度云智大会, AI原生应用
<br>
<br>
总结: 大模型技术正在推动各行业的智能化跨越，百度云智大会是一个重要的活动，旨在引领智能计算技术创新，传递最新实践与突破。AI原生应用的构建和实际落地是关键，开发者需要将AI技术与实际应用场景相结合，开发有用、有价值的产品和服务。 </div>
                        <hr>
                    
                    <p>大模型技术正在以前所未有的速度推动各行业的智能化跨越。对于身处这个时代的开发者来说，他们不仅需要不断学习新知识，还要探索如何将 AI 更好地融入实际应用场景。</p><p></p><p>面对崭新的时代，开发者若想找到一条提升思维认知和开发效率的最短路径，百度智能云每年举办的百度云智大会·智算大会是不容错过的：</p><p></p><p>智能计算大会是百度智能云面向“云计算产品与技术”的重磅活动之一，以引领智能计算技术创新为目标，传递百度智能云产品与技术的最新实践与突破。历经 3 载，从 AI 原生云到深入产业，百度智能云传递着创新的火种，描绘着智能计算的未来。2023 年，智能计算大会全新起航，将以“重构云计算·Cloud for AI”为主题，结合大模型技术以及 MaaS 服务，碰撞最前沿的技术与产品，开启全新的智能计算时代。</p><p></p><p>“工欲善其事，必先利其器”。对于开发者来说，百度云智大会·智算大会是你不可或缺的技术盛宴，它不仅是探索前沿科技的窗口，更是开发实战的“课堂”。</p><p></p><p>2023 百度云智大会·智算大会将于 12 月 20 日在北京落地，本次大会以“重构云计算·Cloud For AI”为主题，汇集了百度集团副总裁侯震宇、IDC 中国区副总裁兼首席分析师武连峰、百度副总裁谢广军等多位行业大咖，聚焦 AI 和云，解读智能计算带来的万千可能和全新图景，带开发者窥见 AI 原生时代的技术创新重构。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f9eea7c6ff5cd2c5ecdb72404cb32b.png" /></p><p></p><p>仅是让大家了解 AI 原生时代的趋势还不够！为了让你获得知行合一的参会感受，下午特别开设了「2023 百度云智大会·智算大会 开发者沙龙」，旨在为开发者提供切实有效的开发技能。 今年 9 月，李彦宏曾在 2023 百度云智大会上强调 AI 原生应用的重要性，他表示，AI 原生应用要能解决过去解决不了、解决不好的问题，应用才是大模型存在的意义。这意味着在 AI 大模型时代，AI 原生应用的构建和实际落地是关键。对于所有开发者而言，则需要能在先进技术和模型的基础上，将 AI 技术与实际应用场景相结合，开发出有用、有价值的产品和服务。</p><p></p><p>为了让开发者能够实操跟练，下午场的「2023 百度云智大会·智算大会 开发者沙龙」活动，由百度智能云主任架构师吴多益、百度资深工程师 &amp; 百度 Comate 产品架构师徐晓强等技术大咖担任分享讲师。</p><p></p><p>实践课程设置方面，由浅入深地涵盖了从编码到应用开发的内容，帮助开发者通过现场实战，学习热门产品及技术、提升软件开发效率，打破在技术与实际应用场景结合方面的障碍。期待参与其中的你，不仅能够掌握提升开发效率的方法，还能建立起构建 AI 原生应用的思维方式。</p><p></p><p>本次沙龙，还为开发者准备了丰富的互动礼品，完成任意一场课程及实验，即可获得精美礼品！线下席位有限，<a href="http://gk.link/a/12eET">立即报名</a>"占位！12 月 20 日 13:00，我们在「2023 百度云智大会&nbsp;· 智算大会 开发者沙龙」不见不散！</p><p></p><p><img src="https://static001.geekbang.org/infoq/e8/e8203ab57b2d095c1230776b92aabdf3.jpeg" /></p><p></p><p>                                                           一起掌握开发“金手指”</p><p></p><p>                                                        提升开发效率，准时下班吧！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</id>
            <title>大语言模型加速信创软件 IDE 技术革新</title>
            <link>https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VG9Loxtgp3eHdPdDec49</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 07:38:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能化信创软件 IDE, 信创, 智能化, 大语言模型
<br>
<br>
总结: 本文介绍了智能化信创软件 IDE 的重要性和意义。智能化技术的发展使得软件开发工具更加强大，例如自动化重构、代码翻译和自动化文档生成等功能。智能化信创软件 IDE 的目标是实现核心技术的可掌控和可研究，以规避信息安全、供应链安全、技术依赖和经济风险。通过信创化，可以解决现有技术的问题并实现超越。同时，文章还介绍了华为云开发工具和效率领域首席专家王亚伟的观点和团队的研发工作。在 QCon 全球软件开发大会上，王亚伟和他的团队将分享关于大语言模型、AI 编码辅助和下一代 IDE 平台架构等技术的内容。 </div>
                        <hr>
                    
                    <p>什么是智能化信创软件 IDE？为什么它很重要？</p><p>&nbsp;</p><p><a href="https://qcon.infoq.cn/2023/shanghai/schedule">QCon 全球软件开发大会（上海站）</a>"将于 12 月 28-29 日举办，会议特别策划「智能化信创软件 IDE」专题，邀请到华为云开发工具和效率领域首席专家、华为软件开发生产线 CodeArts 首席技术总监<a href="https://qcon.infoq.cn/2023/shanghai/track/1598">王亚伟</a>"担任专题出品人，为专题质量深度把关。作为拥有云和开发工具领域近 20 年经验的老兵，华为公司软件开发工具领域的领军人物，20 多项软件开发技术发明专利的拥有者，王亚伟对于「智能化新创软件 IDE」这个专题有着怎样的理解？在会议即将开幕之际，王亚伟与 InfoQ 分享了他的核心观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29e75f26136948c06ee3f9bfd82139f8.jpeg" /></p><p></p><p>&nbsp;</p><p>“信创”是信息技术应用创新的简称，其本质是发展国产替代技术，实现核心技术的可掌控、可研究、可发展等。</p><p>&nbsp;</p><p>相比“信创”，“智能化”在过去 5 年中被业界反复提起，智能化技术的发展必然会使诸如 IDE 这样的软件开发工具更加强大。随着大语言模型的诞生，IDE 除了可以自动地完成一些重复性工作之外，还可以协助开发人员在软件的设计和开发过程中完成更多创新性的工作，比如：</p><p>自动化重构：将一段复杂的代码分解为更小、更易于管理的函数或类。开发者可以描述他想要实现的重构目标，然后让模型生成相应的代码代码翻译：大语言模型可以将一种编程语言的代码翻译成另一种编程语言，再配合 IDE 的语法高亮和错误检查功能，可以帮助开发者使用不熟悉的编程语言编写代码自动化文档生成和更新：大语言模型可以根据代码和注释生成相应的文档，或者在修改代码时自动更新文档。大语言模型是 IDE 的智能化加速度</p><p>&nbsp;</p><p>IDE 的”信创“化旨在将基础软件开发的核心技术实现自主可控，在拥抱开源的同时逐步建立基于自有技术内核的架构和标准，形成自有开放生态。信创化的目的是为了规避可能或已经发生的风险：</p><p>信息安全和供应链安全风险：在关键时刻，国外的产品和技术可能会面临供应链中断的风险。此外，国外产品或开源技术可能会存在安全漏洞或后门，基于这些技术打造的商业解决方案会威胁用户的信息安全 - 2020 年 3 月发生的 SolarWinds 攻击事件导致业界领先的开发工具公司 JetBrains 遭受牵连技术依赖风险：如果完全依赖于外国的技术，那么我们在软件开发核心技术领域的研究、发展和创新能力就会受制于人，最终导致落后经济风险：技术上依赖意味着我们需要持续支付大量的许可费用</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/d4613NRodWJEAXqRblEu">延伸阅读：被逼出来的自主可控，从华为自研看国产 IDE 的未来和商业模式</a>"</p><p>&nbsp;</p><p>“信创”化不意味着重复造轮子或为了与现有技术不同而进行盲目创新，而是目标实现核心技术可控的前提下，解决现有技术的问题，从而对现有技术实现某些方面的超越。举个例子，代码索引是 IDE 的文件查找、代码提示等功能的基础数据源，现有商业 IDE 代码索引的创建、存储和访问效率并不高，索引数据基于对象存储访问时，一个只有 8 字节（2 个 int）内容的数据封装成对象后要占据至少 24 个字节的存储空间。同时，由于内存读写速率要远低于缓存，如果在存储和访问索引时没有以一种缓存友好的方式进行，读写效率甚至 100 倍下降。我们团队在代码索引存储和访问领域提出了一种基于内存压缩的索引自动化存储和访问技术，可以做到 50 倍以上的综合效率提升，该技术已经被评选为华为云高价值专利，并应用到 IDE 内核、运行时优化、云编译等多个领域。” <a href="https://www.infoq.cn/article/ubciEs8NPH06CwlpEvtf">延伸阅读：生成的代码会出错、质量差？面对 AI 编程工具的老大难问题，华为这群人打算这样做</a>"</p><p>&nbsp;</p><p>技术的积累需要时间，产品研发更需要打磨。王亚伟介绍道：“从 2019 年初开始，我们逐步组建了一支数百人的软件研发专家队伍，分布在中国、俄罗斯、欧洲等国家地区，其中一半成员来自于业界顶尖的软件和工具公司，超过 40% 的成员是开源社区的 Committer 和 Contributor，整个团队都围绕着‘做最好的产品’展开工作，我们建立了从产品、运营、UX 到开发、测试的完整专业的产品研发流程，每月一个小版本、三个月一个大版本，基于内外部用户的反馈快速迭代。过去五年我们真正做到了深耕软件开发工具这个专业领域。”</p><p>&nbsp;</p><p>同时，王亚伟也坦言：“虽然从产品成熟度上看我们的信创化工具跟业界成熟的商用工具相比还有差距，但‘信创’本身绝不意味着竞争力弱，体验打折。我们会继续秉持着‘做最好的产品’的信念，不断前进。”</p><p>&nbsp;</p><p>在今年的「智能化信创软件 IDE」专题上，王亚伟带领他的专家团队将围绕大语言模型、AI 编码辅助、下一代 IDE 平台架构、动态语言类型推理等技术，给大家带来一场技术盛宴。</p><p>&nbsp;</p><p>议题<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5567">《AI 开道，让编程体验“一路狂飙》</a>"，详细介绍华为云 <a href="https://www.huaweicloud.com/devcloud/">CodeArts</a>" 团队应用大模型开发的 AI 辅助编程的技术 - CodeArts Snap，讲师程啸从博士阶段开始就对代码生成、RAG、代码克隆检测等领域有较深入的研究，他这次也是代表 Snap 团队进行分享。</p><p>&nbsp;</p><p>另外三个英文议题是来自于 CodeArts 俄罗斯的专家团队。</p><p>&nbsp;</p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5568">Applying Machine Learning in IDE Challenges and Insights</a>"将会系统讨论 AI 技术在 IDE 中的应用研究以及如何深远改变我们的开发，测试和调试代码的方式。讲师 Pavel 是俄研院新西伯利亚实验室主任，20 年开发者工具构建经验，机器学习专家、Eclipse IDE 的专家和 Committer。</p><p>&nbsp;</p><p>议题 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5575">Evolution of IDE Platforms</a>" 会紧密围绕其在构建 IDE 平台时面临的问题和挑战比如分布式代码模型架构下如何确保前后端组件可以高效交互、如何直接从后端内核画出前端复杂 UI，以及我们如何做出艰难的架构和设计决策，同时分享对下一代 IDE 平台的架构和设计展望。Denis 是俄罗斯新西伯利亚实验室的首席架构师，20 多年的工具研发经验，精通编译器、DSL、编程框架，Eclipse 社区 Committer。</p><p>&nbsp;</p><p>静态语言如 Java，C# 等，它的类型推理主要通过编译器完成，代码模型可以通过类型绑定（通常存在于程序的元数据 metadata 中）获得所需要的类型信息。而动态语言的类型推理主要由 IDE 完成，由于缺少编译元数据的支持，动态语言的类型推理是一个业界难题。以 Python 为例，其有一个完全动态严格的类型系统，类型（type）在运行时动态绑定到变量（variable），变量和类型都可以在运行时动态被改变 – 这增加 Python IDE 进行可靠类型推理的难度。议题 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5576">Type inference engine</a>" 会介绍该团队在做动态语言类型推理时设计和实现的技术细节，并讨论未来该领域的发展方向。Nikolai 是俄罗斯圣彼得堡实验室的首席软件工程师，拥有 15 年 IDE 研发经验，是前 JetBrains Intellij IDEA 和 Scala 项目负责人，精通 Compiler、Program Language Design、Code Analysis 等技术。</p><p>&nbsp;</p><p>据了解，QCon 上海还邀请到了<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5623">中国科学院外籍院士、国际数据库专家樊文飞院士</a>"，<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5586">英特尔大数据技术全球 CTO 戴金权</a>"等大咖会亲临现场分享大数据、芯片、架构等方向的前沿洞见。这次会议主要探讨大模型的全面技术架构的进化，不仅有跟大模型本身相关的推理加速、AI Agent、GenAI，还有架构的演进思路、性能优化，以及以智能代码助手为代表的研发效能提升等方向，邀请<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5599">阿里巴巴的通义星尘</a>"、魔搭社区开源 <a href="https://qcon.infoq.cn/2023/shanghai/presentation/5673">ModelScope-Agent</a>" 框架、百度文心大模型驱动下的智能代码助手等团队核心技术骨干前来分享，目前大会日程已上线，<a href="https://qcon.infoq.cn/2023/shanghai/schedule">可点击下方图片查看详情。</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/66/66a620fc97b46ea55958f9b172195701.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a6b81251700257adb01b1334e7d49f9.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</id>
            <title>从业务应用挑战出发，火山引擎专家深度拆解“弹幕互动方案”的全新实践</title>
            <link>https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/SKtUWjqpsK9DV0gFaR5W</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 04:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 互联网, 视频化时代, 火山引擎, 弹幕互动玩法
<br>
<br>
总结: 互联网正在进入视频化时代，火山引擎作为一种视频云服务，与NVIDIA合作推出了《云上新视界》线上课程，其中分享了弹幕互动玩法的解决方案与应用实践。弹幕互动玩法是一种通过弹幕、送礼物等互动操作控制直播画面中互动内容的直播方式，具有即开即玩、多人互动等特点。弹幕互动经历了PC端开播、云游戏方案和云游戏+RTC方案三个核心演进阶段。火山引擎通过优化方案解决了弹幕互动延时和外放回声的问题。 </div>
                        <hr>
                    
                    <p>从互联网到全行业视频化时代，营销、商品、知识与空间的体验正在被重塑和创新，<a href="https://www.infoq.cn/article/z1CW0cFhLxLi2KYk258t?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">火山引擎</a>"视频云以“面向体验，驱动创新”为核心，特别与 NVIDIA 团队合作推出《<a href="https://www.infoq.cn/article/OHhA89XUrsQtm7T5Ts43?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">云上新视界</a>"》线上课程。第五期课程中，火山引擎 RTC 商业化解决方案团队负责人郭健为大家分享了当前热门玩法“弹幕互动”的解决方案与应用实践。</p><p></p><h2>一、什么是“弹幕互动玩法”?</h2><p></p><p></p><p>弹幕互动玩法是依托直播间（直播连麦、语聊房等互娱核心场景），观众可以通过弹幕、送礼物等互动操作，控制直播画面中的互动内容的一种直播方式，具备即开即玩、多人互动等特性，兼具观众互动性强、直播内容游戏化趣味化等特点。</p><p></p><p>从 2014 年的《Plays Pokémon》到 2021 年尾《修勾夜店》爆火，弹幕互动几经翻红。今年开始，弹幕互动受到各大平台的广泛关注，从玩法上线后效果看弹幕互动玩法的户观看人数 / 时长、营收等核心指标都有很好的收益。</p><p></p><h2>二、弹幕互动方案的 3 个核心演进阶段</h2><p></p><p></p><p>弹幕互动经历了 PC 端开播、云游戏方案、云游戏 + <a href="https://www.infoq.cn/article/Ue0E2ZXpr2BwaYxlQ0fL?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">RTC </a>"方案三个阶段。</p><p></p><p>第一个阶段，PC 端开播。传统开播流程需要主播先在 PC 端安装程序和开播工具，互动玩法在主播 PC 上运行和渲染。同时，主播使用 PC 端直播工具（比如 OBS）对本地画面和主播直播画面混流，再推送到直播间。观众进入直播间发送弹幕或者发送礼物参与互动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f65bdaf68b7489076c5d4af003097847.png" /></p><p></p><p>这种方式存在一定局限性，比如：</p><p></p><p>弹幕互动内容本身需要实时计算渲染，对设备硬件配置如显卡计算能力有较高要求，甚至堪比 3A 大作性能要求，开播设备性能不足，就会导致弹幕无效甚至内容本身卡死，影响直播间用户体验；越来越多的主播更习惯在移动端随时开播，而只能运行在 PC 端的弹幕互动程序，会大大增加开播门槛，也降低平台玩法覆盖度；移动端开播还可以与平台其他玩法相结合，但如果单独为弹幕玩法准备 PC 端 OBS 开播，既增加了维护成本，也难以进行推广。</p><p></p><p>第二阶段，在直播 / 语聊的基础上引入云游戏。主播进入连麦房间推拉 RTC 流的同时，也需要进入云游戏的房间拉取互动玩法音视频。然后业务层把 H5 引擎拉取到的视频流和业务层采集到的摄像头流在端上合流后，推入直播房间。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07a7368a086b934ca4812074f93dbffb.png" /></p><p></p><p>这个方案解决了开播平台限制和开播设备的限制，但是有一些方案接入和体验问题。从方案层面看，业务逻辑复杂接入相对麻烦。从体验看，存在嘉宾 / 观众侧主播解说和互动画面会有轻微的不同步、画面延时大、有回声等问题。其中，RTC 引擎订阅云游戏音频观众侧有回声主要是因为游戏流的声音或者麦克风会采集到本地播放的游戏声音。</p><p></p><p>为了解决上个方案的几个问题，火山引擎视频云首推“云游戏 +RTC 方案”方案，而弹幕互动方案也正式进入了第三阶段——火山引擎 RTC 与云游戏产品在服务侧和引擎侧做了深度协同优化。在服务侧，优化了调度方案，保证用户连接的云游戏 pod+RTC 媒体服务器在同一个机房、云游戏音视频流可直接送入 RTC 房间。在引擎侧，云游戏引擎直接依赖宿主侧的火山 RTC 引擎、云游戏引擎裁剪场景无关功能。</p><p></p><p>在具体操作中，首先主播通过云游戏引擎开启互动完成程序，云游戏启动 pod 并创建火山 RTC 房间。完成后，Pod 集成云游戏引擎和 RTC 引擎向火山云游戏房间推音视频流，火山云游戏房间跨房转推音视频流到两个直播 / 语聊房间，嘉宾和观众通过 RTC 直接拉取直播流和云游戏流即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/918deed71d6b8a03257846fafd65c7d3.png" /></p><p></p><p>云游戏和 RTC 内部深度协作，缩短数据流转链路在接入直播 / 语聊的基础上，仅需接入 veGameSDK 启动游戏、业务端通过 OpenAPI 同步弹幕 / 礼物数据到云游戏服务器两步即可完成场景“升级”，大大简化业务逻辑，缩短接入周期减少工作量。</p><p></p><h2>三、火山引擎是怎么解决历史方案问题的？</h2><p></p><p></p><p>此前弹幕互动方案所存在的观众弹幕互动延时、主播外放有回声等体验问题，火山引擎方案是如何解决的？</p><p></p><h4>&nbsp;1. 弹幕互动延时问题</h4><p></p><p></p><p>未优化的云游戏方案观众端发送弹幕后，由于传统 RTMP 直播流延迟较大，观看云游戏观众侧会有 3~5 秒延时，并且都会有轻微的互动画面与解说的不同步，体验较差。这些在普通常见的场景可能影响不大，但是在对战场景，战场形势瞬息万变，可能最后一秒的延时失去被“偷家”导致战斗失败。</p><p></p><p>优化后，使用全 RTC 方案，可以让用户参与玩法整体延时&lt;400ms 。</p><p></p><h4>&nbsp;2. 外放回声消除</h4><p></p><p></p><p>在未优化方案中，云游戏的声音在经过扬声器播放后，会被近端用户的麦克风采集到并产生回声问题，需要参考扬声器播放的声音进行回声消除技术处理，云游戏和 RTC 独立运行，云游戏音频无法给到 RTC 引擎，所以容易产生回声。</p><p></p><p>在优化方案中，云游戏音频可以直接跨房转推到 RTC 房间，场景内音频播放通过音频托管的方式统一由 RTC 进行音频播放，有参考信号，可以彻底消除回声，以确保对端收到清晰的声音。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b7/b7615c34243048d5d4edabe66d99ef81.png" /></p><p></p><p></p><h2>四、弹幕互动方案在业务应用中的挑战与实践</h2><p></p><p></p><h4>&nbsp;1. 卡顿优化</h4><p></p><p></p><p>弹幕互动场景有一个特点就是画面极致高清，一般是高清 1080P、 帧率 30fps、高码率 8Mbps。同时，主播、观众均为移动端设备，随时开播与参与，用户网络环境复杂且不稳定。在这种高分辨率高码率、且网络不稳定情况下极其容易造成卡顿劣化。</p><p></p><p>要优化这种情况，首先把线上 H264 升级为自研 ByteVC1 编解码，在 PSNR（视频质量客户评价）画质质量优于原方案 2dB 时，还能节约 10% 码率。此时对于线上情况码率可能仍较大，火山引擎 RTC 采用智能流控协议 (VISC)，它基于 Simulcast 和 SVC 策略优化而来、更加智能的一种传输协议，它可以综合考虑音视频通话中每个订阅者的个性化需求，在网络情况、终端性能发生变化的时候，自动调整音视频流的配置，最大限度地让每个参与者的个性化需求得到满足，为用户提供更流畅的互动体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a57bdf9d82120b552297cf2fbbe2456.png" /></p><p></p><h4>&nbsp;2. 操作延时优化</h4><p></p><p></p><p>云游戏在所有的云计算相关应用中，对延时要求最为苛刻，火山引擎 RTC 针对云游戏与 RTC 场景相结合的应用场景，进行全链路延时优化。</p><p></p><p>阶段一，边缘机房阶段。保证用户连接的云游戏 Pod 和 RTC 服务器调度到同一个机房，使用更高效传输方式优化，首帧时长减少约 30ms；降低延时 50ms；编码前优化采集和格式转换，使用 OpenGL 转换替换 libyuv 转换，优化延迟 15ms;阶段二，级联服务优化。减少级联服务器和优化信令传输，优化 20ms;阶段三，订阅端。针对云游戏下行音视频调整 jitterbuffer 大小，降低延时 60~260ms，有优化的处理，可以不影响直播 / 语聊体验；针对不同的硬件解码器做优化，最多优化延迟 90ms；内部渲染替代外部渲染降低延迟 5ms，整体云游戏到端延时可以达到小于 75ms。</p><p></p><h4>&nbsp;3. 性能优化</h4><p></p><p></p><p>弹幕互动玩法可以在个人直播、直播连麦或者跨房 PK 中等场景中加入。在语聊房跨房 PK+ 弹幕互动玩法场景中，假设每个语聊房会有 9 人，两个房间 PK 时，单个用户最多需要拉 18 路音频流和云游戏音视频流，性能压力大，玩法准入机型门槛高，设备发热严重。</p><p></p><p>因此，为减少对手机性能消耗，火山引擎 RTC 使用 RTC 公共流不进房拉流方案。这个方案中，本房间内拉流方式不变，PK 房间的音频流合流后推一路公共流，对比普通语聊模式单个用户只多拉一路音频流和一路云游戏流。两个房间 PK，每个房间 1 位主播、8 位嘉宾、100 位观众流数评估，单房间减少（1+8+100）*8 约 872 路、单用户减少 8 路流，有效优化用户拉流性能，减少 50% 流数量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c3b679655ff084ab5807c48eece0a0f.png" /></p><p></p><p>独立集成云游戏 SDK 包体增量一般 9M 左右，9M 的包增量对客户来说是不可接受的。弹幕互动方案中云游戏直接复用火山引擎 RTCSDK 传输能力，云游戏 SDK 精简包只需操控信令和选路部分，精简包给整体带来增量仅 610KB。</p><p></p><h2>五、写在最后</h2><p></p><p></p><p>总体来说，火山引擎弹幕互动方案有五大优势：</p><p></p><p>不限设备、不限场景，零门槛开播：无论是个播还是多人互动，移动端即可随时随地“云开启”弹幕互动玩法，无需高性能 PC，消除互动内容本身对用户终端算力的限制；热门弹幕互动内容全适配：云游戏支持直接部署基于 UE/Unity 框架的互动内容，底层多种类型 IaaS 和对应 GPU 配置，满足不同等级算力要求的弹幕互动玩法；无惧弹幕高并发，渲染画面高清流畅：云游戏支持 ARM、x86 以及定制化 GPU 等多样化计算资源，并采用自研 ByteVC1 编解码结合动态码率技术，保证互动画面流畅体验同时节约带宽消耗，互动画面 100ms 卡顿率低于 2%；主播解说与玩法进程实时同步：通过火山引擎 RTC 媒体节点和 云游戏 Pod 端同机房调度，超低延时体验，操作延时小于 90ms，主播讲解和内容画面实时同步，保障观众沉浸互动体验；应用最小包增量引入：弹幕互动方案中云游戏可直接复用火山引擎 RTC SDK 传输能力，云游戏 SDK 精简包只需操控信令和选路部分，精简包增量仅 KB 级。</p><p></p><p>而本期课程中介绍的弹幕互动玩法的解决方案技术实践只是“小试牛刀”，如果想要了解更多，可以扫描下方二维码，有更加详细的弹幕互动解决方案和获取弹幕互动 Demo！</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebba54e6aafb6fea8351787b6285c768.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</id>
            <title>英特尔软件与先进技术事业部 / 首席工程师胡宁馨确认出席 QCon 上海，分享 WebNN，Web 端侧推理的未来</title>
            <link>https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TqNGiNCf3yfomTXbrwm9</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: QCon 全球软件开发大会, WebNN, W3C 标准, AI推理
<br>
<br>
总结: QCon 全球软件开发大会将在上海举行，胡宁馨将分享关于WebNN API的主题演讲，探讨WebNN API的W3C标准进展以及对CNN、Transformer和生成式AI模型的支持情况和计划，以及在浏览器的实现进展。WebNN API提供了Web应用访问AI加速器的途径，以获得更好的性能和更低的功耗。 </div>
                        <hr>
                    
                    <p><a href="https://qcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">QCon 全球软件开发大会</a>"，将于 12 月在上海召开。英特尔软件与先进技术事业部 / 首席工程师胡宁馨将发表题为《<a href="https://qcon.infoq.cn/2023/shanghai/presentation/5646?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">WebNN，Web 端侧推理的未来</a>"》主题分享，探讨 WebNN API 的 W3C 标准进度，对 CNN，Transformer 以及更广泛的生成式 AI (Generative AI) 模型的支持情况和计划，以及在 Chrome，Edge 等浏览器的实现进展。</p><p></p><p><a href="https://qcon.infoq.cn/2023/shanghai/presentation/5646?utm_source=infoqweb&amp;utm_medium=teacherart&amp;utm_campaign=9&amp;utm_term=1208&amp;utm_content=huningxin">胡宁馨</a>"，就职于 Intel 软件与先进技术事业部，专注于 Web 技术，W3C 机器学习工作组 Web Neural Network API (WebNN) 规范的发起者和联合编辑，Chromium 项目 Code Committer，WebNN 模块负责人。他在本次会议的演讲内容如下：</p><p></p><p>演讲：WebNN，Web 端侧推理的未来</p><p></p><p>AI PC 以及 AI Mobile 的新兴时代已经到来，越来越多的设备集成了强大的神经处理单元 NPU，以实现高效的人工智能加速，这对需要端侧推理的应用至关重要。除了通过 CPU 和 GPU 进行推理之外，Web Neural Network API (WebNN) 提供了 Web 应用访问此类专有 AI 加速器 NPU 的途径，以获得卓越性能及更低功耗。</p><p></p><p>本次演讲将会给大家分享 WebNN API 的 W3C 标准进度，对 CNN，Transformer 以及更广泛的生成式 AI (Generative AI) 模型的支持情况和计划，以及在 Chrome，Edge 等浏览器的实现进展。作为 JavaScript ML 框架的后端，WebNN 将会在几乎不更改前端代码的前提下，为 Web 开发者及他们的产品带来相较于 Wasm，WebGL 更为优异的性能体验。</p><p></p><p>演讲提纲：</p><p></p><p>当前 Web AI 发展概况主流硬件加速器的发展（CPU，GPU，NPU)WebNN 设计与架构WebNN 代码演示WebNN 浏览器（Chromium）实现WebNN 机器学习框架集成（ONNXRuntime 和 TensorFlowLite)WebNN Transformers 支持WebNN 性能</p><p></p><p>听众收益点：</p><p></p><p>○ 了解 Web 平台对异构处理器的支持</p><p>○ 了解基于 Web 的机器学习模型硬件加速</p><p>○ 了解 Chromium 实现内部细节</p><p></p><p>除上述演讲外，QCon 上海还将围绕&nbsp;<a href="https://qcon.infoq.cn/2023/shanghai/track/1595?utm_source=infoqweb&amp;utm_medium=teacherart">GenAI和通用大模型应用探索</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1596?utm_source=infoqweb&amp;utm_medium=teacherart">AI&nbsp;Agent&nbsp;与行业融合应用的前景</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1597?utm_source=infoqweb&amp;utm_medium=teacherart">LLM&nbsp;时代的性能优化</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1598?utm_source=infoqweb&amp;utm_medium=teacherart">智能化信创软件&nbsp;IDE</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1602?utm_source=infoqweb&amp;utm_medium=teacherart">面向人工智能时代的架构</a>"、<a href="https://qcon.infoq.cn/2023/shanghai/track/1604?utm_source=infoqweb&amp;utm_medium=teacherart">性能工程：提升效率和创新的新方法</a>"等专题进行交流。</p><p></p><p>QCon 上海 2023，相约 12 月！ 现在购票，享 9 折优惠，立减￥680！咨询购票请联系：18514549229（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0113b3dfb187d5f9988b7eb16d2e0594.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</id>
            <title>如何使用 Cluster Autoscaler 将批处理作业的节点扩容到 2000 个</title>
            <link>https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MNLwdrkZ7upG9yd2KXTY</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 02:13:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 火山引擎容器服务, VKE, Kubernetes, Cluster Autoscaler
<br>
<br>
总结: 本文介绍了火山引擎容器服务(VKE)作为云上Kubernetes平台的经历和挑战。文章首先解释了Cluster Autoscaler(CA)的概念和工作机制，包括自动调整集群大小和节点使用率的调度。然后介绍了CA的扩容和缩容逻辑，以及在实际客户场景中的应用。最后给出了一些建议，帮助实现集群弹性和避免类似的问题。 </div>
                        <hr>
                    
                    <p>本文将分享火山引擎容器服务 <a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ%3D%3D&amp;chksm=e8d7e80adfa0611c8447800f02c4c482f8746bad0d541c80b22615aa09000dbe3b6fd28b2761&amp;idx=1&amp;mid=2247489480&amp;scene=27&amp;sn=3b54e854a6756c5191ca1032e4d1189b&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">VKE</a>" 作为云上<a href="https://mp.weixin.qq.com/s?__biz=MzI5ODQ2MzI3NQ%3D%3D&amp;chksm=eca7f1acdbd078baf17571575d642a09391401c945ddae152443ea51963b8eaca11b540c3364&amp;idx=1&amp;mid=2247502056&amp;scene=27&amp;sn=ef7a6deb9a63551cac10322b6b059aad&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect"> Kubernetes</a>" 平台，在帮助客户实现集群资源弹性过程中的一些经历和挑战，共分为以下几个部分：</p><p></p><p>第一部分介绍什么是 CA，以及它内部的流程和实现方式，帮助大家更好地理解其工作机制；第二部分简要说明客户批处理作业的使用场景；第三部分把重心放在客户在使用 Cluster Autoscaler 的过程中，碰到的问题和挑战，以及我们是如何解决的；最后将给出一些建议，帮助大家更好地实现集群弹性，避免踩到类似的坑。</p><p></p><h2>什么是&nbsp;Cluster Autoscaler(CA)</h2><p></p><p></p><p>从 <a href="https://xie.infoq.cn/article/d26b441cebb5b229a6efa35f4?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Cluster Autoscaler </a>"项目的 README 文档中，可以看到它包括几个方面：</p><p></p><p>自动调整集群大小，即扩缩容因为集群中资源不足，才会扩容缩容时由于集群中的节点使用率低于阈值，这个低使用率的节点上的 Pod 可以调度到其他节点上去</p><p></p><p>下图展示了用户视角下 CA 扩容的情况。当集群中出现 Pending Pod，没有节点能让这些节点调度上去时，CA 就会触发扩容，往集群中加入新的节点，让 Pod 调度上去。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/219ece266705fec98c75af3dc71ab8d3.png" /></p><p></p><p>而节点的使用率较低，比如图中的低于 50%，CA 就会把这个节点删除，Pod 被重新调度到其他的空闲节点上。这样一来，集群中工作负载的数量不变，但是节点数减少了，剩余节点和集群整体的使用率就提高了，对用户来说，这相当于降本增效。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4de3555a3ae6af17767f51424697db9b.png" /></p><p></p><p>CA 是一个定期重复执行的过程，如果简化一下，它大致可以分为以下几个部分：</p><p></p><p>准备工作，CA 会先从集群中获取相关的数据，比如节点、集群的状态、需要调度的 Pending Pod、清理创建失败的节点、过滤还没 ready 的 GPU 节点等；扩容逻辑；缩容逻辑；结束；等待一段时间后，再从头开始。</p><p></p><p><img src="https://static001.geekbang.org/infoq/01/0136fcaa26a5f89d7082fabc55ff2d61.png" /></p><p></p><p>在扩容阶段，CA 会先找到集群中无法调度的 Pending Pod，然后试着把这些 Pending Pod 和节点池做匹配，看看每个节点池都满足哪些 Pending Pod 的调度要求：有的节点池可能扩容了也不满足调度要求，这些节点池就被排除了；有的节点池能调度一部分 Pending Pod，那这些节点池就会保留下来。</p><p></p><p>对于这些保留下来的节点池，CA 会计算需要扩容多少个节点才能满足这些 Pending Pod 的资源用量，接着从这些节点池中按照设置的扩容策略选一个最合适的节点池。扩容策略可能是随机选择、也可能是优先级，或者最小浪费，这些都是由用户配置的。选择出最合适的节点池之后，CA 就会调用接口，告知云厂商需要扩容的数量，云厂商完成具体的 ECS 创建、加入集群等动作。</p><p></p><p>而在缩容阶段，CA 会找到使用率低于阈值的节点，查看这些节点上是否还有 Pod，如果没有 Pod 了，就认为这个是空节点，会被优先批量删除。删除完空节点以后，CA 再判断这些非空的节点上，Pod 是否可以调度到其他节点上去：如果可以调度，CA 也会把这个非空节点删除，节点上的 Pod 被驱逐、然后在别的节点上被重建。</p><p></p><p>这大概就是 CA 的整个过程，虽然省去了很多细节，但大家应该可以理解几个关键点：一个是 CA 中的逻辑，是定期运行的；第二个是在整个流程中，有扩容和缩容两个阶段，这两个阶段相互独立，扩容需要计算新增的节点数量、按照扩容策略选节点池，缩容就只看节点的使用率和上面的 Pod 是否可被重调度。</p><p></p><h2>客户场景</h2><p></p><p></p><p>我们遇到过这样一个案例，客户有自己的任务分发平台，不同计算任务通过任务平台下发到 Kubernetes 集群中，每批计算任务对应一堆的 Pod。而他们的业务存在这几个特点：</p><p></p><p>任务种类多，不同的任务所需的资源不同，CPU 用量各异，有的也会使用 GPU；不同任务对应的 Pod 数量也不同，峰值时整个集群超过 2w Pod；一般业务高峰期是在晚上，从凌晨开始跑，一直跑到早上；整体耗时长，不同批次任务耗时有长有短；Pod 的镜像也非常的大，拉取耗时长。</p><p></p><p>在这样的业务场景下，为了节省成本，客户很自然地使用了 Cluster Autoscaler，期望在计算任务下发后，节点池能自动扩容，添加新的节点到集群中，让 Pod 调度上去。在计算任务跑完以后，节点空闲下来，Cluster Autoscaler 再把节点删除，避免资源浪费。为了提高装箱率减少资源碎片，客户会对某些类型的任务，设置 Pod 的 resource request 和节点规格一致，尽量让这种任务的 Pod 独占一个节点</p><p></p><h2>问题与解决方案</h2><p></p><p></p><p>问题一：扩容成功率低</p><p></p><p>在客户上量过程中，我们碰到的第一个问题，是在大规模扩容过程中出现的大量扩容失败。CA 触发节点池扩容后，一部分节点创建成功，调度了部分 Pod，另一部分节点创建失败，在随后的过程中又被 CA 删除。由于还有部分 Pod 处于 Pending 状态，又触发 CA 扩容，然后又失败，周而复始。</p><p></p><p>这就给客户带来了非常糟糕的体验，一是看到很多失败的扩容记录，使其对云厂商的信任度降低；二是增加了不必要的成本，因为这些创建失败的节点并没有加入集群，不能被客户使用，但是节点对应的云服务器是实实在在被创建出来了，客户花了钱，但资源又没用上，就增加了无谓的成本。</p><p></p><p>经过仔细排查，我们发现节点扩容失败是因为云服务器在初始化 Kubernetes 组件的过程中，写入磁盘的速度特别慢，很久都不能加入集群，超过了预设的超时限制，我们判定这是一个异常的节点。异常节点随后又被 CA 清理删除，那我们就很好奇，为什么 ECS 的云盘写入这么慢？经过进一步的调研，我们发现主要原因是云盘服务的压力太大：</p><p></p><p>一方面，云服务器自身在初始化 Kubernetes 组件的时候，比如安装系统软件包、从对象存储上拉取 Kubernetes 的安装包再解压等动作，是有磁盘写入的，一个节点可能还好，当几百个节点同时处于这个阶段的时候，云盘服务的整体写入压力会大幅上升。</p><p></p><p>另一方面，在于容器镜像的拉取。在已经正常创建的节点上，用户的 Pending Pod 会调度上去，然后开始拉取镜像，由于这个客户的镜像很大，拉的耗时也很久，如果很多节点都处于这个阶段，那会有大量的写入操作，导致整个云盘服务的写入吞吐量被打到一个较高的位置，新的节点在初始化的时候，因为要争抢写带宽，所以写入速度就降低了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b7d5ce610842e7597f02b75c0be7027.png" /></p><p></p><p>为了解决这个问题，我们的想法是对同时扩容的节点数量做一个限制。虽然社区的 CA 中并没有对同时扩容的节点数有什么限制，但任何系统都存在上限，通过对系统做合理的限制，不仅能提供稳定的服务，从全局上也有助于提升性能。</p><p></p><p>我们根据云盘的吞吐能力，估算了一个可被接受的同时扩容节点数，比如限制是 100，这样一来，用户看到的就是 100 一批 100 一批的扩容，节点都能扩容成功。虽然扩容的批次增加了，但扩容成功率也提高了，整体的云盘写入流量更加平滑，整体的扩容速度也比之前提升了很多。</p><p></p><p>问题二：容器镜像大，扩容速度慢</p><p></p><p>我们碰到的第二个问题，是极致的性能问题，我们先讲扩容的性能问题。在批处理场景下，客户使用的镜像会比较大，并且客户对扩容端到端速度要求会比较高，比如要求在 5min 内扩容出 500 个节点，并且 Pod 都能运行起来，这是一件非常有挑战的事情。</p><p>在客户视角下，他们计算任务的启动延迟，大概分为 5 个阶段：</p><p></p><p>第一阶段：下发任务，集群中出现因资源不足而导致 pending 的 Pod；第二阶段：CA 感知到这些&nbsp;Pending Pod，触发节点池扩容。这个阶段一般是秒级的，如果是使用了 GPU 的 Pod，由于 CA 自身的策略，会导致最多延迟 30s 再扩容。这里 CA 不立马扩容要等几秒，是因为如果最新的 Pending Pod，创建时间离现在比较近，很有可能还会有新的 Pending Pod 被创建出来。比如 deployment 的副本数从 0 改到 1000，可能就需要 10 多秒才能全部创建完，所以 CA 宁愿多等一会儿等所有 Pod 都被创建了才执行扩容；第三阶段：云厂商接收到扩容请求，去创建云服务器、注册到集群中。这个阶段是分钟级别的，不同云厂商的耗时可能会略有差别；第四阶段：把这些 Pending Pod 调度到节点上，如果 Pod 数量和集群规模不大，Pod 的调度条件不复杂，相对整个过程来说，这阶段的耗时可以忽略不计；第五阶段：节点上的 Pod 开始拉取镜像、启动。这个阶段的耗时是不太稳定的，比如同时扩容的节点数量比较多，容器镜像又比较大，就很有可能会打满云厂商的限速，对整个端到端的影响比较大。</p><p></p><p>比如在这张图里，在多个节点同时扩容时，除了用户的计算任务的 Pod，节点上还有很多系统 daemonset 的 Pod，比如网络组件、device plugin、日志采集组件等等，这些 Pod 的镜像也会大量的、同时的从镜像仓库拉取，很容易就达到网络瓶颈，或者给云盘服务带来写入压力。如果 500 个节点同时扩容，每个节点上都在争抢带宽或者磁盘的写入，是无法达到刚刚说的性能要求的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1ab394d6fb7af7b1f7b51a8eb9895e4e.png" /></p><p></p><p>在这种极致的性能要求下，我们采用了自定义系统镜像方案。这个自定义系统镜像是指云服务器的系统镜像，我们先在云服务器中把容器镜像预先拉取下来，然后把云服务器导出为自定义系统镜像，把业务的容器镜像固化到系统中去，这样在后续扩容的时候，我们用这个自定义系统镜像去创建云服务器，云服务器作为节点加入集群后，容器镜像就已经在节点上了，不需要再去镜像仓库拉取，Pod 可以做到秒级启动。</p><p></p><p>但这个方案也有一些弊端，比如我们可以把整个容器镜像固化到系统中后，后续容器镜像发生了变化，这个自定义系统镜像也需要重新制作，比较麻烦，如果容器镜像变化比较频繁，就要频繁的制作自定义系统镜像。所以我们也可以把镜像做一下拆分，把数据量比较大的、又不怎么更新的静态数据，打包到基础镜像中，然后把这个基础镜像再固化到系统中，这样节点在启动以后，拉取的数据量也会大大减小。</p><p></p><p>在使用这个方案前，如果客户扩容 500 节点，在单批次运行最多 70 个节点扩容的情况下，每个节点上 1 个 10GiB 的容器镜像，那从下发到 Pod 全部运行，大概需要 22min。</p><p></p><p>而如果使用自定义镜像，因为不需要拉取容器镜像，所以刚刚说的云盘服务的压力就减轻了，所以我们直接放开扩容数量的限制，直接从 0 到 500 做扩容，从 Pod 下发到最终 Running，可以在 5min 以内完成，并且云盘服务整体的写入流量，可以从峰值的 14GB/s 下降到 6GB/s，大幅减少数据写入。</p><p></p><p>这个方案对于需要快速扩容、对扩容时的端到端耗时非常敏感的业务，是一个可行的解决思路。</p><p></p><h2>问题三：多节点干扰，缩容速度慢</h2><p></p><p></p><p>客户因为计算任务的不同，会触发不同节点池的扩容。比如客户先进行&nbsp;GPU 计算任务，触发了节点池 A 的扩容（节点池 A 是 GPU 节点），在计算任务 A 快结束的时候，可能会下发新的计算任务，触发节点池 B 的扩容。</p><p></p><p>那按照客户的预期，这时节点池 A 的这些 GPU 节点，因为上面没有 GPU 计算任务、节点使用率已经降低，需要在任务结束的一段时间内很快就被缩容掉。但实际情况是节点池 A 的缩容会被推后较长的时间，这就造成了一些资源浪费。</p><p></p><p>所以为什么节点池 A 的缩容会被推迟呢？</p><p></p><p>CA 内部的缩容流程中，有一个冷却时间，表示扩容后多久时间内，是不能对节点做缩容的，这个值由用户来设定。这个计时是集群级别的，就是任何一个节点池扩容了，这个计时器都会被重置，重新计算。在大规模、多节点池扩容的情况下，如果用户分批扩容，那每次扩容都会做一次重置，导致扩容过程中，空闲的节点池无法被缩容，造成资源的空跑。</p><p></p><p>当前社区对此已经有解决方案，但代码还处于草稿阶段，具体的解决思路就是把计时器改成节点池级别，每个节点池只针对自己的扩容过程做倒计时，不受其他节点池干扰。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/83eb92b64349a7dbbc8ac5f8c732fc8d.png" /></p><p></p><p>我们在生产环境上对社区的方案做了验证，确实很好的解决了我们的问题，在计算任务结束后，节点池 A 就会很快被缩容。那这个缩容时间的缩短，非常显著地降低了客户的使用成本。</p><p></p><h2>问题四：Pending Pod 过多导致未扩容</h2><p></p><p></p><p>最后我们再来看一下由规模带来的问题。</p><p></p><p>如前文所述，客户用自己的任务分发平台将计算任务转换成 Pod 下发到 Kubernetes 集群，有时候并不能非常好地控制任务的下发速度。峰值时期，整个集群中有 2w 多个 Pod，其中&nbsp;Pending Pod&nbsp;的数量高达 1.8w 个。面对如此大的规模，CA 难免“力不从心”。</p><p></p><p>下图展示了集群中 Pod 的数量情况和 CA 的日志分布情况，可以发现在 Pod 数激增的那段时间里，CA 基本上没有输出日志，集群中的节点池也没有扩容，客户的计算任务被大量堆积、阻塞。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d975e99ab55cfe3844e9622b4e5052d.png" /></p><p></p><p>经过调查我们发现，CA 主要卡在调度预测阶段，在这一阶段，CA 会计算每个节点池需要扩容多少个节点才能满足这些&nbsp;Pending Pod&nbsp;的资源用量。为了复现这个问题，我们做了一些压测，期望能找到影响这个耗时的主要因素，方便针对客户的场景做一些优化。</p><p></p><p>一开始我们想到的就是 Pending Pod 的数量。为此我们使用两个不同规格的节点池，然后往集群中下发大量的&nbsp;Pending Pod，这些&nbsp;Pending Pod&nbsp;通过资源用量期望调度到其中一个节点池上。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/423f92f0da89e08ec47fa5967cb6efea.png" /></p><p></p><p>我们发现随着集群中 Pending Pod 数量的增长，单个节点池的整个计算耗时，是不断上升的，在 2.2w Pod 时，单个节点池的计算耗时会到 400s，而从 Pod 视角来看，单个 Pod 的平均耗时，也是线性增长的。虽然 Pending Pod 的数量规模达到了，但实际 CA 僵死的时间是远比我们测出来的 400s 多，所以我们继续接近客户的使用方式，将 Pod 的调度逻辑做了修改，从之前的默认调度约束，改为了使用节点亲和性。</p><p></p><p>我们发现在不使用节点亲和性的情况下，整体的耗时和第一次压测的是一样的，而如果使用了节点亲和性，在&nbsp;Pending Pod&nbsp;数量在 1.8w 的时候就达到了 700s。下方右侧这张图中蓝色的那条曲线也说明，单个 Pod 的平均计算时间，比之前不使用节点亲和性的场景增长得快，整条线上升的速度更快、斜率更高。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4be635025e18911d4f0dd002c5a3b6ac.png" /></p><p></p><p>除此以外，我们继续控制变量，调整 Pod 的 request，将之前的单个节点上只跑 1 个 Pod，改为单个节点上能跑 8 个 Pod，这样修改后，预期添加到集群中的节点数量是之前的 1/8，同时整个计算耗时，相比之前的曲线，也是接近水平了。</p><p></p><p>从上面的 3 次压测中，我们可以得出一些结论：</p><p></p><p>Pending Pod 越多，需要计算的耗时越久，且平均每个&nbsp;Pending Pod&nbsp;的耗时随总数的增加而增加；使用了 Node Affinity 的&nbsp;Pending Pod，在做调度预测时，会耗时更久；预估节点数量越多，调度预测越久；可被调度的节点池数量越多，调度预测越久。</p><p></p><p>这是我们从压测中得到的实验结论，那真实的技术理解应该是怎样的呢？</p><p></p><p>CA 在估算节点池需要扩容多少个节点的时候，内部有一个快照，一开始这个快照包含了集群中的节点和节点上的 Pod。如果集群中有多个节点池，CA 会先对每个节点池做一下计算，看看哪些 Pending Pod 能调度到节点池上。因为如果节点池不能满足 Pod 的调度要求，即使扩容了也没有用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/2930b38dff9fca2b6eeb2b1b7ccd06e7.png" /></p><p></p><p>比如这张图里，集群中一共有 8 个&nbsp;Pending Pod，节点池 A 能满足所有&nbsp;Pending Pod&nbsp;的调度要求，节点池 B 只能调度 6 个。这个过程的复杂度是 O(n^2)，跟 Pod 的数量、节点池的数量有关，当然也跟 Pod 的调度条件有关系，调度条件越复杂，这个耗时也会更久一点。在做完这一步之后，CA 会再根据节点池和节点池上的这些&nbsp;Pending Pod，去计算需要扩容多少个节点</p><p></p><p>比如节点池 A 能满足 8 个&nbsp;Pending Pod&nbsp;的调度条件，CA 会先对这些&nbsp;Pending Pod&nbsp;和快照里的节点做一轮调度模拟，跑一下 scheduler framework 中的 prefilter 和 filter 阶段，看能不能正常通过，如果能通过就表示这些&nbsp;Pending Pod&nbsp;可以调度到快照的节点上，如果不能通过，就会根据节点池 A 的规格信息，构建出一个虚拟的 Node，放到快照里，然后再做刚刚的调度模拟，此时这些&nbsp;Pending Pod&nbsp;是可以调度到这个新加的虚拟的 Node 里的。</p><p></p><p>CA 重复这个过程，直到这里所有的&nbsp;Pending Pod&nbsp;都能加入到快照中，此时快照里新增了多少个虚拟的 Node，其实就是节点池 A 需要多扩容的节点数。只要集群里新增了这些节点，这些因资源不足而无法调度的&nbsp;Pending Pod&nbsp;就能真正的跑起来。</p><p></p><p>节点池 B 也是类似的，只不过在我们的例子中，节点池 B 的规模会比节点池 A 小。根据我们刚刚的分析，整个过程的复杂度是接近 O(n^3) 的，跟&nbsp;Pending Pod&nbsp;的数量、快照中的节点数量、节点池得到数量相关。</p><p></p><p>这也跟我们的压测结论是一样的：Pending Pod&nbsp;的数量越多、节点池越多、预估的节点数量越多、调度条件越复杂，整个扩容的耗时就越久。</p><p></p><p>对此，CA 社区主要提出了两个改进点：</p><p>限制节点数量的上限，就是减少快照中的节点数量，这个跟我们刚刚提到的观点是类似的，如果对扩容的节点数量不加限制，其实是不太稳妥的；对单个节点池整体的计算耗时做限制，比如不能超过 10s，如果这个过程超过了 10s，我们就截断这个过程。</p><p></p><p>如果你的 CA 版本还比较老，低于 v1.25 的，可能就没法使用社区的解法了。</p><p></p><h2>资源弹性建议</h2><p></p><p></p><p>如果业务对扩容的延迟比较敏感，期望能更快的让 Pod 启动，可以考虑将静态的、较大的容器镜像，打包进云服务器的系统镜像里，加速扩容。</p><p></p><p>推荐在业务侧就开始控制集群中的&nbsp;Pending Pod 的数量，数量过多不但会增大集群自身的压力，也会影响 CA 扩容的稳定性，将数量保持在一个稳定的水位，控制好扩容的节奏，会更好。</p><p></p><p>第三个是对于不需要弹性能力的节点池，关掉弹性伸缩功能，避免 CA 在这些节点池上消耗算力。</p><p></p><p>相关产品：www.volcengine.com/product/vke</p><p></p><p>视频回放：关注【字节跳动云原生】公众号，在后台回复“KubeCon CN 2023”</p><p></p><p>相关服务咨询：</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3b1a8f60be8d5405bb4dc31095389f8.png" /></p><p>扫码咨询</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</id>
            <title>刚发布就被质疑？超过GPT-4的“最强”大模型Gemini、“最高效”训练加速器，谷歌到底行不行</title>
            <link>https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xcu7VoHdktaHGrbvbEcu</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 02:04:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, AI模型, Gemini, 多模态提示
<br>
<br>
总结: 谷歌发布了功能强大的AI模型Gemini，该模型通过多模态提示实现对文本和图像的理解和反应。 </div>
                        <hr>
                    
                    <p>当地时间12 月 6 日，谷歌发布了自己“迄今为止功能最强、通用性最高”的AI模型Gemini。</p><p></p><p>谷歌及Alphabet&nbsp;CEO桑达尔·皮查伊 (Sundar&nbsp;Pichai)表示，首个Gemini 1.0针对不同规模进行优化，具体分为Ultra、Pro和Nano三个版本。“这是Gemini时代的首批模型，也是我们今年早些时候重组Google DeepMind时所表达愿景的首个实现。此模型代表着谷歌作为一家企业，在AI新时代下所做出的最重要的科学与工程努力之一。”</p><p></p><p>但刚发布不久，科技专栏作家Parmy Olson 指出，其中一个AI实时对人类的涂鸦和手势动作给出评论和吐槽的视频被曝出“不是实时或以语音方式进行的”。还有<a href="https://twitter.com/noguestein/status/1732927393466040617">网友吐槽</a>"整个互动过程“特别慢，跟演示视频完全不同。”</p><p></p><p>这个视频主要是演示“多模态提示”（multimodal prompting），即为大模型提供不同模式的组合（在本例中为图像和文本），并让其通过预测接下来会发生什么来做出反应。</p><p></p><p></p><p></p><p>对此，Google DeepMind 研究与深度学习主管副总裁 <a href="https://twitter.com/OriolVinyalsML/status/1732885990291775553">Oriol Vinyals</a>"表示，“视频中的所有用户提示和输出都是真实的，只是为简洁起见进行了缩短剪辑。”但网友对此并不买账，认为谷歌在玩营销手段，误导大家。</p><p></p><p>在谷歌发布的<a href="https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html?m=1">一篇文章</a>"里，详细介绍了效果实现经过，可以看出是使用静态图片和多段提示词拼凑训练。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/879c43b3919928ef014c63fd299e9cbb.png" /></p><p></p><p></p><h2>看看谷歌的测试</h2><p></p><p></p><p>Gemini 被称为谷歌迄今为止最灵活的模型，能够从数据中心到移动设备实现高效运行，帮助开发人员与企业客户显著增强在利用AI进行构建和扩展时的操作方式。谷歌针对三种不同体量优化了Gemini 1.0（首个正式模型版本），分别为：</p><p></p><p>Gemini Ultra&nbsp;— 最大、功能最强的模型，适用于高度复杂的任务。Gemini Pro&nbsp;— 可处理各种任务类型的最佳模型。Gemini Nano&nbsp;— 能够在多种设备上高效运行的任务处理模型。</p><p></p><p>值得注意是，本次尚未发布最强大的Gemini Ultra，距离正式发布还需要几个月的时间。目前Gemini Ultra正在进行全面的信任与安全检查，包括由受信的外部合作方进行红队审查，并在广泛应用前通过微调和基于人类反馈的强化学习（RLHF）对其做进一步完善。</p><p></p><p>Gemini Pro和Gemini Nano已分别集成到了聊天机器人Bard和智能手机Pixel 8 Pro上。此外，自12月13日开始，开发者和企业客户都可通过Google AI Studio或者Google Cloud Vertex AI中的Gemini API访问Gemini Pro模型。在未来几个月间，Gemini将逐步登陆谷歌更多产品及服务，包括搜索、广告、Chrome浏览器以及Duet AI等。</p><p></p><p>谷歌说得很厉害，那Gemini 1.0 的实力到底如何？</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9677686c03434f3f8237cd371682bc06.png" /></p><p>﻿</p><p>根据谷歌测试结果，从自然图像、音频和视频理解再到数学推理，在大语言模型（LLM）研发领域的32种常见学术基准测试中，Gemini Ultra的性能一举创下30项最佳新纪录。</p><p></p><p>在MMLU（大规模多任务语言理解）中Gemini Ultar的得分高达90.0%，成为首个超越人类专家的模型。这项测试结合了数学、物理、历史、法律、医学和伦理学等57个科目，旨在测试AI模型掌握知识和解决问题的能力。</p><p></p><p>Gemini在文本和编码等一系列基准测试中表现超过GPT-4：</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/691456eef49288cbb54143ec862c3dc2.png" /></p><p></p><p>Gemini Ultra还在新的MMMU基准测试中取得了59.4%的最高得分。这项基准测试涵盖跨越不同领域、需要深思熟虑的一系列多模态推理任务。</p><p></p><p>根据谷歌测得的图像基准，Gemini Ultra的性能优于以往最先进的模型，且无需借助从图像中提取文本以供进一步处理的对象字符识别（OCR）系统的辅助。谷歌表示，这些测试结果凸显出Gemini的天然多模态优势，也证明Gemini已经表现出具备复杂推理能力的早期特征。</p><p></p><p>Gemini在一系列多模态基准测试中均创下性能新纪录，全面超越GPT-4V：</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eabc1941f73a277dcac4574c7de7d681.png" /></p><p></p><h2>多模态推理能力</h2><p></p><p></p><p>到目前为止，创建多模态模型的标准方法主要是针对不同模态训练单独的组件，再将其组合起来以粗略模仿相应能力。由此实现的模型虽然比较擅长执行某些特定任务，例如描述图像内容，但却难以处理概念性更强、复杂度更高的推理任务。</p><p></p><p>在Gemini的起始阶段就将其定位为原生多模态形式，针对不同模态开展预训练。之后，谷歌又使用额外的多模态数据对其进行微调，希望进一步完善其有效性。现在，Gemini可以同时识别和理解文本、图像、音频、视频和代码五种信息。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/abb5095053fbb457004d5057561f4555.png" /></p><p></p><h4>理解文本、图像、音频等各种素材</h4><p></p><p></p><p>Gemini 1.0拥有精妙的多模态推理能力，可以帮助理解复杂的书面与视觉信息，展现出了在大量数据中提取重要知识的独特能力。比如，Gemini 在阅读、过滤和理解信息的过程中，可以从数十万份文档中提取见解并进行分析。</p><p></p><p>Gemini 1.0在训练之后，能够同时识别并理解文本、图像、音频等各种素材，因此可以把握住更加微妙的信息，并回答与复杂主题相关的更多问题。这使得它特别擅长解释数学、物理等复杂学科的推理过程。</p><p></p><p>比如，Gemini 可以识别学生的手写物理题答案，并验证正确性：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/09/0993347ceccee6452d2a0f3248905fd5.png" /></p><p></p><p>基于视觉线索进行推理：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/91/9122b89b1f3140bc6e1a323e529acd5a.png" /></p><p></p><p>音频方面，可以看下Google DeepMind 研究科学家 Adrià Recasens Continente 演示 Gemini 能够理解来自多个扬声器的不同语言的音频，并结合视觉、音频和文本，在厨房做饭时提供帮助的场景：</p><p></p><p></p><p></p><h4>高级编码能力</h4><p></p><p></p><p>谷歌介绍，首个Gemini正式版能够理解、解释并生成基于目前各种流行编程语言（例如Python、Java、C++和GO）的高质量代码。其表现出的跨语言工作和复杂信息推理能力，也使得Gemini成为世界领先的编码基础模型之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62fd21473ee722a5f3eb12414a0ad27d.png" /></p><p></p><p>Gemini&nbsp;&nbsp;的多模式推理功能生成用于重新排列子图的matplotlib代码</p><p></p><p></p><p>Gemini Ultra在多项编码基准测试中表现出色，包括HumanEval（用于评估编码任务性能的重要行业标准）和 Natural2Code（谷歌内部保留的数据集），此数据集使用作者专门创作的源素材、而非来自网络的信息。</p><p></p><p>Gemini还能作为更高级编码系统的引擎。谷歌两年之前发布了ALphaCode，这也是首个在编程竞赛中表现出一定竞争力的AI代码生态系统。使用Gemini的专用版本，谷歌推出更加先进的代码生成系统AlphaCode 2。除了编码场景之外，它还擅长解决涉及复杂数学和理论计算科学的更多编程难题。</p><p><img src="https://static001.geekbang.org/infoq/10/108a0fe125a7ab615f9e83a23e82c6e7.png" /></p><p></p><p>面对与初代AlphaCode相同的评估场景，AlphaCode 2表现出巨大的性能改进，其解决的问题数量几乎达到初版的两倍，谷歌估计其成绩优于85%的竞赛参与者，而AlphaCode成功解决问题的比例只接近50%。因此当程序员通过代码示例来定义某些属性，并借此向AlphaCode 2寻求帮助时，其表现会更好。</p><p></p><p></p><h2>“专为训练顶尖AI模型而生”的TPU系统</h2><p></p><p></p><p>在介绍自家大模型的同时，谷歌顺势推出了了自己的AI训练基础设施。</p><p></p><p>谷歌使用内部设计的张量处理单元（TPU）v4和v5e在AI优化的基础设施之上，完成了Gemini 1.0的大规模训练任务。</p><p></p><p>在TPU上，Gemini的运行速度明显快于其他更早、更小且功能较差的模型。这些定制设计的AI加速器一直是谷歌AI产品的核心，负责为搜索、YouTube、Gmail、谷歌地图、Google Play和Android等服务的数十亿用户提供支持。它们也使得世界各地的其他企业也能经济高效地训练出自己的大规模AI模型。</p><p></p><p>如今，谷歌宣布推出迄今为止“最强大、最高效且可扩展”的TPU系统Cloud TPU v5p，专为训练顶尖AI模型而生。谷歌表示，作为下一代TPU，它将加速Gemini开发，帮助开发者和企业客户快速训练大规模生成式AI模型，将新产品和新功能更快交付至客户手中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee5fb38056fa5ac7026cfc835d0eb72a.png" /></p><p></p><p>谷歌数据中心内的Cloud TPU v5p AI加速器超级计算机</p><p></p><p>此外，在安全问题上，谷歌表示，Gemini拥有迄今为止所有谷歌AI模型当中最全面的安全评估机制，包括偏见与有毒内容检测。谷歌还对网络攻击、说服与自主判断等潜在风险领域开展了新颖研究，并应用谷歌研究院领先的对抗性测试技术抢在部署之前帮助发现Gemini中的重大安全隐患。</p><p></p><p>为了诊断Gemini训练阶段的内容安全问题，并确保其输出结果符合政策，谷歌使用诸如真实毒性提示词Real Toxicity Prompts在内的多种基准。这是一组从网络提取的、包含不同程度毒性内容的10万条提示词，由艾伦AI研究所的专家们提供。为了限制伤害，谷歌还构建了专门的安全分类器，用以识别、标记并整理涉及暴力或负面刻板印象的内容。</p><p></p><p>附 Sundar&nbsp;Pichai 公开信内容：</p><p>&nbsp;</p><p></p><blockquote>每一次技术变革都代表着推动科学发现、加速人类进步和改善生活品质的机遇。我相信我们现在所见证的AI转变，将成为我们一生当中最具深远意义的事件，甚至远远超越之前的移动或者Web革命。AI有望为全球各地的人们创造前所未有的日常生活体验和非凡的职业发展空间，将掀起新一波的创新与经济进步，并以前所未见的规模提升知识、学习、创造力与生产力。&nbsp;这也让我感到兴奋，期待通过AI技术为各国各地的每一个人提供帮助。&nbsp;作为一家AI优先的厂商，我们已经走过近八年历程，而前进的步伐只会不断加快：数百万用户正在我们的产品中运用生成式AI完成一年之前还难以想象的工作，包括为更加复杂的问题寻求答案、使用新工具协作与创新等等。与此同时，开发人员也在使用我们的模型与基础设施构建出新的生成式AI应用程序，世界各地的初创企业和组织正利用我们的AI工具不断拓展业务。&nbsp;这是一股令人难以置信的发展态势，而且我们才刚刚开始触及这无限可能性的最表层。我们正以大胆且负责任的态度开展这项工作。这意味着我们既需要追求雄心勃勃、能够为人类和全社会带来巨大收益的技术成果，同时也要建立保障措施并与政府和专家合作，应对AI发展过程中带来的种种风险。我们将继续投资打造更好的工具、基础模型和底层设施，并在我们AI原则的指导下将其引入自己的产品及其他方案当中。</blockquote><p></p><p></p><p></p><p>相关链接：</p><p><a href="https://blog.google/technology/ai/google-gemini-ai/#availability">https://blog.google/technology/ai/google-gemini-ai/#availability</a>"</p><p><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>