<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/3lOSp9UOA1qwhmQ9NbRe</id>
            <title>百度智能云旗舰模型一年降价超90%，万卡集群有效训练时长达99.5%</title>
            <link>https://www.infoq.cn/article/3lOSp9UOA1qwhmQ9NbRe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/3lOSp9UOA1qwhmQ9NbRe</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Sep 2024 07:57:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9月25日，百度智能云宣布分别针对算力、模型、AI应用，全面升级了百舸AI异构计算平台4.0、千帆大模型平台3.0两大AI基础设施，并升级代码助手、智能客服、数字人三大AI原生应用产品。</p><p>&nbsp;</p><p>“目前在千帆大模型平台上，文心大模型日均调用量超过7亿次，累计帮助用户精调了3万个大模型，开发出70多万个企业级应用。过去一年，文心旗舰大模型降价幅度超过90%，主力模型全面免费，最大限度降低了企业创新试错的成本。”百度集团执行副总裁、百度智能云事业群总裁沈抖说道。</p><p>&nbsp;</p><p></p><h4>升级百舸4.0：模型训练有效时长达99.5%，可高效管理十万卡集群</h4><p></p><p>&nbsp;</p><p>“如今，整个云业务的增长正在从传统的云计算向所谓的GPU云以及异构算力进行转换。”百度副总裁谢广军在接受媒体采访时说道。</p><p>&nbsp;</p><p>沈抖介绍称，GPU集群有三个特征：极致规模、极致高密和极致互联。这些“极致”带来了两个严峻的挑战：第一，巨额的建设、运营成本。建一个万卡集群，单是GPU的采购成本就高达几十亿；第二，运维的复杂性急剧增加。硬件不可避免地会出故障，而规模越大出故障的概率就越高，比如Meta训练llama3的时候，用了1.6万张GPU卡的集群，平均每3小时就会出一次故障。在这些故障中，绝大多数是由GPU引起的。</p><p>&nbsp;</p><p>“过去一年，我们感受到客户的模型训练需求猛增，需要的集群规模也越来越大。与此同时，大家对模型推理成本的持续下降的预期也越来越高。这些都对GPU管理的稳定性和有效性提出了更高要求。”沈抖表示，为此百度智能云大幅升级并发布百舸AI异构计算平台4.0。</p><p>&nbsp;</p><p>最下面是资源层，支持异构芯片管理、高速互联、高效存储；组件层解决的是大规模集群稳定和性能的问题；加速层是面向客户大模型训练、推理的需求设计；最上面的工具层是一套管理界面。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c90ee10f26cfc92176040d4f0f30348c.png" /></p><p>&nbsp;</p><p>百度百舸AI异构计算平台4.0</p><p>&nbsp;</p><p>在集群创建阶段，企业通常需要进行大量复杂、琐碎的算力配置和调试工作。沈抖介绍道，百舸4.0预置了主流的大模型训练工具，能够实现工具层面的秒级部署，并将万卡集群运行准备时间从几周缩减至1小时，极大地提升部署效率，缩短业务上线周期。</p><p>&nbsp;</p><p>在开发实验阶段，企业需要针对业务目标对不同架构、参数的模型进行多次测试，进而制定最佳模型训练策略，保障后续训练的性能和效果。百舸4.0全新升级的可观测大盘，能够对多芯适配、集群效能、任务自动容错等方面进行全方位监测，提供直观决策依据。</p><p>&nbsp;</p><p>在模型训练阶段，稳定和高效是衡量GPU集群水平的“金指标”、“硬通货”。一张GPU出现故障就会导致集群整体停摆，大量时间、成本浪费在故障恢复和数据回滚上。为此，百舸4.0支持自动筛查集群状态，并基于对GPU故障的精准预测，及时转移工作负载，降低故障发生频次。此外，百舸独有的故障秒级感知定位、Flash Checkpoint模型任务状态回滚等技术，能够大幅减少集群故障处置时间，实现接近无损的集群容错。</p><p>&nbsp;</p><p>据介绍，目前百舸在万卡集群上实现了有效训练时长占比99.5%以上，此外，据悉百舸4.0通过在集群设计、任务调度、并行策略、显存优化等一系列创新，大幅提升了集群的模型训练效率，整体性能相比业界平均水平提升高达30%。</p><p>&nbsp;</p><p>在模型推理环节，百舸则通过架构分离、KV Cache、负载分配等一系列加速方法，实现了模型推理的降本提效，尤其在长文本推理方面，推理效率提升超过1倍。</p><p>&nbsp;</p><p>沈抖认为，大模型的Scaling Law将在一段时间内持续有效，很快就会有更多的十万卡集群出现，但是管理十万卡的难度与管理万卡有着天壤之别。</p><p>&nbsp;</p><p>首先，在物理空间方面，十万卡集群需要占据大概10万平方米空间，相当于14个标准足球场的面积；在能耗方面，每天则要消耗大约300万千瓦时的电力，相当于北京市东城区一天的居民用电量。这种对于空间和能源的巨大需求，远远超过了传统机房部署方式的承载能力，跨地域机房部署又会给网络通信带来巨大挑战。此外，十万卡集群中的GPU故障将会非常频繁，有效训练时长占也将迎来新的挑战。</p><p>&nbsp;</p><p>为此，百舸4.0已经构建了十万卡级别的超大规模无拥塞HPN高性能网络、10ms级别超高精度网络监控，以及面向十万卡集群的分钟级故障恢复能力。“百舸4.0正是为部署十万卡大规模集群而设计的。今天的百舸4.0，已经具备了成熟的十万卡集群部署和管理能力，就是要突破这些新挑战，为整个产业提供持续领先的算力平台。”沈抖说道。</p><p>&nbsp;</p><p></p><h4>发布千帆3.0：三大服务全面升级，一句话即可生成企业级应用</h4><p></p><p>&nbsp;</p><p>“模型开发尤其是大模型开发，在toB市场上的需求比直接调用的需求来得晚。”谢广军表示，“随着应用本身的深入落地，也会越来越多，越来越广。”</p><p>&nbsp;</p><p>为了满足企业客户对模型调用、模型开发、应用开发三方面的需求，百度智能云发布千帆大模型平台3.0。根据介绍，升级后的千帆平台可以调用包括文心系列大模型在内的近百个国内外大模型，还支持调用语音、视觉等各种传统的小模型。同时在价格方面，文心旗舰大模型过去一年降价幅度超过90%、主力模型全面免费。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/593970705b1d2c90c5071e3ec6730b67.png" /></p><p>&nbsp;</p><p>文心大模型家族全景图</p><p>&nbsp;</p><p>对于需要定制、微调专属模型的用户，千帆3.0提供了一系列大模型工具链，支持CV、NLP、语音等传统模型的开发，并实现数据、模型、算力等资源的统一纳管和调度。模型投入使用后，千帆平台还支持企业将应用中产生的数据，经过采样评估、人工标注、对齐或微调等方式反馈给模型，形成数据飞轮，持续优化模型效果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9f7af4cc1bdc17dc3c73c276a5a7a81.png" /></p><p>&nbsp;</p><p>千帆平台大模型工具链</p><p>&nbsp;</p><p>&nbsp;</p><p>在应用开发方面，针对企业落地大模型的高频应用场景，千帆3.0从检索效果、检索性能、存储扩展、调配灵活性四方面对企业级检索增强生成（RAG）进行了全面升级；针对企业级Agent的开发，千帆3.0增加了业务自主编排、人工编排、知识注入、记忆能力以及百度搜索等80多个官方组件支持。</p><p>&nbsp;</p><p>工具平台的不断完善，也促进了过去一年大模型产业落地的爆发式增长。据悉，目前在千帆平台上，文心大模型日均调用量超过7亿次，千帆平台累计帮助用户精调了3万个大模型，开发出70多万个企业级应用。</p><p>&nbsp;</p><p>此外，千帆行业增强版提供了体系化的工具和组件，支持行业客户、合作伙伴在千帆通用底座上不断添加行业特色，从而更方便地开发适合自己的行业应用。目前，千帆平台上已经沉淀了包括制造、能源、交通，政务、金融、汽车、教育、互联网在内的八大行业解决方案。</p><p>&nbsp;</p><p></p><h4>代码助手、智能客服、数字人全面升级</h4><p></p><p>&nbsp;</p><p>随着大模型产业落地逐渐走向深水区，AI原生应用也将迎来爆发式增长，为了满足企业直接选购成熟AI应用的需求，百度智能云面向数字人、智能客服、代码提效三大领域，升级了三大AI原生应用产品。</p><p>&nbsp;</p><p>基于文心大模型重构的AI原生智能客服应用“客悦”，在用户复杂意图理解、多模态信息交流等方面实现了大幅提升，让智能客服变得更聪明、更拟人。据介绍，“问题自助解决率”是智能客服领域最关键的考核指标，当前业内平均水平是80%，升级后的客悦将这一指标提升至92%，实现业界领先。目前，客悦已累计帮助企业客户服务超过1.5亿人次，交互超过5亿次。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e4ed3430d6ea908d3d86dda8a83d68e.png" /></p><p></p><p>&nbsp;</p><p>基于大模型能力，新升级的曦灵数字人4.0支持根据文字快速生成不同妆造、不同行业特色的3D数字人形象和视频。本次大会期间，曦灵平台宣布：将3D超写实数字人的价格从万元大幅降价至199元，达到业内最低价。</p><p>&nbsp;</p><p>曦灵数字人4.0全新升级的4D（3D+时间维度）自动绑定技术和创新模态迁移技术，还解决了传统2D数字人动作僵硬的问题，可以实现人物在不同角度、形体、表情的高度一致。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef9caa300852855a7f918f0daf238961.png" /></p><p>&nbsp;</p><p>全新升级的全流程AI代码提效工具“文心快码”，聚焦研发全生命周期的业务流，实现了从项目接手到最终交付，全流程编码开发效率与质量的双重提升。</p><p>&nbsp;</p><p>文心快码业界首发“企业级代码架构解释”、“企业级代码审查”，两项全新功能。企业级代码架构解释能在项目接手初期，实现工程架构的智能解读，帮工程师快速理解业务逻辑；而企业级代码审查则能传承资深工程师的编码经验，智能辅助程序员查缺补漏。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1e9be1a3a1217f4cf6e3a1f04462578.png" /></p><p></p><p>&nbsp;</p><p>此外，针对市面上通用代码助手缺乏对企业历史代码库的理解的痛点，文心快码全新升级的“企业级代码辅助能力”能够深度理解企业代码库，快速学习企业过往的代码与规范，让生成的代码更贴近企业的要求。</p><p>&nbsp;</p><p>目前，文心快码已经服务超过1万家企业客户，帮助数百万中国开发者提升编码效率，整体提升研发效率20%。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ox8UjdiyqTrrGRbWN1Kv</id>
            <title>借助 AI 实现高效的 DevSecOps 工作流程</title>
            <link>https://www.infoq.cn/article/ox8UjdiyqTrrGRbWN1Kv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ox8UjdiyqTrrGRbWN1Kv</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Sep 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>DevSecOps 是一种强大的软件开发方法，可以加快交付速度并提高效率。</p><p></p><p>在 2024 年伦敦 QCon 的演讲中，我探讨了团队在他们的 DevSecOps 流程中是如何面对不同程度的低效率，从而阻碍了进步和创新。</p><p></p><p>我强调了一些常见的问题，比如调试时间过长、工作流程效率低下等，同时还展示了人工智能（AI）是如何成为简化这些流程并提高效率的强大工具的。</p><p></p><h2>云原生——DevSecOps</h2><p></p><p></p><p>让我们来探索 DevSecOps 及其与云原生的联系。在我们开启 DevSecOps 之旅时，请先考虑你当前所处的阶段。你是处于部署、自动化测试、利用临时环境阶段，还是要从头开始？</p><p></p><p>在整个讨论过程中，我鼓励大家找出自己当前所面临的最低效的任务。是问题创建、编码、测试、安全扫描、部署、故障排除、根本原因分析还是其他什么？</p><p></p><p>想象一下人工智能在提高效率方面的潜力。然而，认识到工作流程多样性的重要性是至关重要的。稍后我们将会探讨具体的示例。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/56/5603538b6f3e6af7d480e52300960b8a.png" /></p><p></p><p>为人工智能建立护栏至关重要。确保数据安全，防止环境泄漏。此外，衡量人工智能的影响也至关重要。不要仅仅是因为别人实施了人工智能我们也要实施。构建一个令人信服的案例并展示其价值。我们也将深入探讨这一方面。</p><p></p><h2>开发工作流程中的 AI</h2><p></p><p></p><p>在快节奏的软件开发世界中，简化工作流程对于提高效率并取得成功至关重要。2024 年，70% 的受访者表示，他们组织中的开发人员需要一个多月的时间才能融入团队并提高生产力，而 2023 年这一比例为 66%（数据来源：GitLab 2024 年全球 DevSecOps 调查）。人工智能已准备好重组我们的工作方式。通过在我们的工作流程中使用人工智能，我们可以获得很多好处，包括提高效率、减少花在重复任务上的时间、增强对代码的理解、增加协作和知识共享，简化新员工入职培训过程。</p><p></p><p>在软件开发方面，人工智能提供了许多可能性来增强每个阶段的工作流程——从将团队划分为开发、运维和安全等专业角色，到促进规划、管理、编码、测试、文档和审查等典型步骤。</p><p></p><p>人工智能驱动的代码建议和生成功能可以自动执行自动完成、识别缺失的依赖项等任务，从而提高了编码效率。此外，人工智能还可以提供代码解释、总结算法、提出性能改进建议，并将长代码重构为面向对象的模式或其他不同的语言。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/88/8824f240f289e7b0613bb402103ddbf9.png" /></p><p></p><p>人工智能的影响不仅限于开发，还延伸到了运维领域。通过分析较短的描述，人工智能可以生成全面的问题描述，从而节省了宝贵的时间和资源。它还可以总结冗长的讨论和问题描述，使团队成员更容易了解情况并参与其中。</p><p></p><h2>现实世界中的 AI 应用实例</h2><p></p><p></p><p>Anthropic Claude Workbench 是一个功能强大的工具，可用于开发和运行在底层大语言模型（LLM）上的 AI 提示查询。例如，一个简单的提示可以生成启动 Golang 项目的全面指导，包括 CLI 命令、GitLab 的 CI/CD 配置，甚至 OpenTelemetry 仪表盘。这消除了筛选无数标签和资源的需要，节省了时间并提高了效率，特别是对于新团队成员来说。</p><p></p><p>此外，人工智能可以帮助制定详细的问题描述，将一个简短的想法转化为一个全面的提案。在提供的示例中，该工具探索了是使用 SDK 还是使用适当的自动检测来检测源代码。这是启动讨论并探索不同解决方案的好方法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7c30d2fc77a9b17b927c650340e3a7ec.png" /></p><p></p><p>此外，人工智能在总结冗长的讨论和计划方面也具有重要价值，可以快速理解复杂的问题。通过将内容粘贴到 Anthropic Claude 3 Workbench 中，它可以有效地压缩信息，从而实现更快的决策和更有针对性的方法。</p><p></p><p>另一方面，人工智能通过总结冗长的问题描述、在 Go 中生成 Kubernetes 的可观测性 CLI、将 Go 代码重构为 Rust 以及为合并请求推荐审查者来展示它的多功能性。</p><p></p><p></p><h2>AI 驱动的运维：事件响应、可观测性和成本优化</h2><p></p><p></p><p>将重点从开发转移到运维，让我们探讨人工智能是如何彻底改变根本原因分析、可观测性、错误跟踪、性能和成本优化的。一个常见的痛点是 CI/CD 管道停滞不前，就像修改后的 XKCD 303 漫画中描绘的一样。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/ebf22b22c134e082fbd9130a10723dd1.png" /></p><p></p><p>无需手动筛选作业日志，人工智能即可对作业日志进行分析，并提供可操作的见解，甚至提出修复建议。通过改进提示并与人工智能进行对话，开发人员可以快速诊断并解决问题，甚至获得优化建议。</p><p></p><p>安全性是至关重要的，因此在分析之前必须过滤掉密码和凭据等敏感数据。一个精心设计的提示可以指示人工智能以任何软件工程师都能理解的方式解释根本原因，从而加速故障排除。这种方法可以显著提高开发人员的效率。</p><p></p><p>对于云原生部署，Kubernetes 的故障可能会是一场噩梦。然而，像 CNCF 沙箱项目 k8sgpt 这样的工具利用 LLM 来分析部署，并从 SRE 或效率的角度提供建议。它适用于各种 LLM，甚至可以在 MacBook 上与 Ollama 一起本地运行。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/4439d51f1ad1771db666171603aeded7.png" /></p><p></p><p>可观测性是运维的另一个关键方面，人工智能可以简化日志分析。在故障发生期间，人工智能可以总结大量的日志数据，更快地查明根本原因，从而帮助快速解决问题。Honeycomb 将人工智能集成到它们的产品中，即体现了这种方法，为复杂的可观测性任务提供了查询助手和其他人工智能驱动的功能。</p><p></p><p>最后，可持续性监测正在获得越来越多的关注，Kepler 等工具使用 eBPF 和机器学习来预测 Kubernetes 环境中的能耗。这使组织能够优化成本并获得可持续性，从而减少碳足迹。</p><p></p><p>这些示例展示了人工智能如何改变运维，提高效率，并推动各个领域的创新。</p><p></p><h2>安全工作流程中的 AI</h2><p></p><p></p><p>将我们的重点转移到安全工作流程上，人工智能可以成为理解并减轻漏洞、增强安全扫描并解决供应链问题的强大盟友。67% 的开发人员表示，他们所开发的代码中有四分之一或更多来自于开源库，但目前只有 21% 的组织使用了软件物料清单（SBOM）来记录构成其软件组件的成分（来源：GitLab 2024 年全球 DevSecOps 报告）。</p><p></p><p>回顾过去的某次安全事件，其中开源工具中的 CVE 导致了意想不到的后果，很明显，更深入地了解漏洞及其长期修复至关重要。</p><p></p><p>人工智能可以通过用简单的术语解释漏洞，澄清诸如格式字符串漏洞、命令注入、定时攻击和缓冲区溢出等概念来提供帮助。通过了解恶意攻击者如何利用漏洞，开发人员可以在不引入回归或损害代码质量的情况下实施有效的修复。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/40/4073495599f7be5659393da73ba1f121.png" /></p><p></p><p>使用诸如“以软件安全工程师的身份解释此漏洞”之类的提示，人工智能可以分析代码片段，提供潜在的漏洞示例，并提出可靠的修复建议。此外，人工智能甚至可以生成带有拟议代码更改的合并请求或拉取请求，从而自动执行修复过程，并能确保安全扫描和 CI/CD 管道的验证修复。</p><p></p><p>这种简化的方法不仅节省了时间，还降低了人为错误的风险，使漏洞管理更加有效且高效。</p><p></p><h2>人工智能护栏——隐私、数据安全、性能、验证</h2><p></p><p></p><p>将我们的注意力转移到人工智能护栏上，解决隐私、数据安全、性能以及人工智能在我们工作流程中的整体适用性至关重要。首先，必须仔细检查数据使用情况。由于潜在的泄漏，我们的数据（包括源代码）不应用于训练 AI 模型。专有数据不应发送给外部提供商进行分析，特别是在银行或政府机构等受监管的环境中。</p><p></p><p>此外，如果人工智能的功能使用了聊天历史记录，那么数据保留政策和删除实践应该是透明的。必须就数据使用和隐私发表公开声明，并向你的 DevOps 或 AI 提供商询问他们的政策，这一点至关重要。</p><p></p><p>安全性是另一个至关重要的问题。应控制对人工智能功能和模型的访问，并建立治理机制来定义谁可以使用它们。此外，应实施保障措施，防止敏感内容被发送到提示中。</p><p></p><p>验证提示响应对于避免被利用至关重要。明确的指导方针和对团队成员要求对于确保负责任且合乎伦理道德地使用人工智能工具是必要的。</p><p></p><p>透明度是关键。应随时提供有关人工智能使用、开发和更新的文档。此外，制定解决人工智能故障或模型更新的计划对于保持生产力也至关重要。像 GitLab 这样的人工智能透明度中心可以提供有价值的见解和信息。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/002295cb6beb5c37f8ee80eb8a646232.png" /></p><p></p><p>无论我们使用的是 SaaS API、自管理的 API 还是本地的 LLM，性能监控都是至关重要的。OpenLLMetry 和 LangSmith 等可观测性工具可以帮助我们跟踪人工智能在工作流程中的行为和性能。</p><p></p><p>最后，由于可能产生幻觉，大模型（LLM）的验证至关重要。测试框架和评估指标对于确保人工智能生成的响应的质量和可靠性至关重要。</p><p></p><p>By diligently addressing these considerations, you can harness the power of AI while minimizing risks and ensuring responsible and effective integration into your workflows.</p><p></p><p>通过努力解决这些问题，我们可以利用人工智能的力量，同时最大限度地降低风险，并确保将负责任且有效的人工智能整合到我们的工作流程中。</p><p></p><h2>AI 的影响</h2><p></p><p></p><p>从护栏过渡到影响，衡量人工智能对开发工作流程的影响提出了一个新的挑战。超越传统的开发人员生产力指标并探索其替代方法是至关重要的。</p><p></p><p>可以考虑将 DORA 指标与团队反馈及满意度调查结合起来。此外，需要监控代码质量、测试覆盖率和 CI/CD 管道失败的频率。检查发布时间是减少了还是保持一致。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/50/50f1b8ba138d64c979a21a6132e20405.png" /></p><p></p><p>无论是通过像 Grafana 这样的工具还是其他平台，构建全面的仪表板来跟踪这些指标都是至关重要的。通过分析这些见解，我们可以更深入地了解人工智能是如何影响我们的工作流的，并确定需要改进的领域。虽然通往精确测量的道路仍在持续探索中，但对我们方法的不断探索和完善将使我们更全面地了解人工智能对生产力和整体发展成果的影响。</p><p></p><h2>AI 的采用</h2><p></p><p></p><p>虽然将人工智能集成到工作流程中是很有前景的，但重要的是要考虑安全、隐私和数据使用的防护措施，同时还要验证人工智能的影响。有一些先进的技术，如检索增强生成（Retrieval Augmented Generation，RAG），可以增强人工智能的能力。</p><p></p><p>RAG 通过整合文档或知识库等外部信息源，解决了在旧数据上训练的 LLM 的局限性。通过将这些资源加载到向量存储中并将其与 LLM 集成，用户可以访问最新的特定信息，甚至可以访问当前的 Rust 开发或伦敦天气等主题的信息。</p><p></p><p>RAG 有实际的应用，例如为 Discord 或 Slack 等平台创建的知识库聊天机器人。即使是像 GitLab 手册这样的复杂文档也可以有效地加载和查询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fc6a9001fd20352e044267088af3887b.png" /></p><p></p><p>有了像 LangChain 这样的工具和像 Ollama 这样的本地 LLM 提供商，我们可以构建自己的 RAG 驱动的解决方案。这使我们能够在不依赖外部 SaaS 提供商的情况下利用专有的数据，从而确保了数据安全和隐私。</p><p></p><h2>AI/LLM 代理</h2><p></p><p></p><p>另一个值得关注的领域是 AI/LLM 代理，它们正在迅速发展的。它们可以动态收集数据来回答复杂的问题，从而提高准确性和效率。虽然该技术仍在开发阶段，但它对 DevSecOps 具有巨大的潜力。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c4/c4605980f65ce0a37676727cadc6ffde.png" /></p><p></p><p>此外，考虑针对特定用例的自定义提示和模型。经过内部数据训练的本地 LLM 提供了安全和隐私方面的优势。在定制领域，探索代理调优是替代全面重新训练的一种经济有效的方案。这些先进的技术可以进一步优化我们的 DevSecOps 工作流程。</p><p></p><h2>结论</h2><p></p><p></p><p>总之，为了确保高效地实施 DevSecOps，有几个关键的考虑因素。首先，从工作流的角度来看，重复性任务、低测试覆盖率和缺陷可以通过利用代码建议、生成测试和使用聊天提示来解决。其次，在安全领域，解决延迟发布的安全回归问题需要漏洞解释、解决方案和团队知识建设。</p><p></p><p>最后，从运维的角度来看，在失败的部署上花费了过多时间的开发人员可以从根本原因分析和 k8sgpt 等工具中受益。通过解决这些问题，组织可以增强其 DevSecOps 实践，并简化其软件开发和交付流程。</p><p></p><p>你可以在点击此处访问公开的演讲幻灯片，此外，它还提供其他的网址和参考资料。</p><p></p><p></p><h3>作者介绍</h3><p></p><p>Michael Friedrich 是 GitLab 的开发人员倡导者，专注于通过 AI 提高 DevSecOps 效率。他对学习计算机工作原理的渴望使他从硬件 / 软件系统工程到 DNS，从开源监控开发到编写 Git/GitLab 培训再到 DevOps 工作流。Michael 喜欢教导每个人，并定期在行业活动和聚会上发表演讲。他喜欢迎接意想不到的挑战，并创建了最佳实践教程和实时编程课程。当他不周游世界或远程工作时，他喜欢搭建乐高模型，并热衷于探索嵌入式硬件。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/articles/efficient-devsecops-workflows/">https://www.infoq.com/articles/efficient-devsecops-workflows/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KQQVrPLNI91lviqQVoGQ</id>
            <title>垂直领域革新者：浩鲸科技“鲸智大模型” 重磅发布</title>
            <link>https://www.infoq.cn/article/KQQVrPLNI91lviqQVoGQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KQQVrPLNI91lviqQVoGQ</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Sep 2024 09:30:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>9月20日，“垂直大模型，全面释放数字生产力——浩鲸科技·鲸智大模型发布会”在云栖大会期间举行。作为垂直大模型的场景落地践行者和价值发挥者，浩鲸科技重磅推出“1+1+4+5+X”鲸智大模型技术体系，加速赋能行业数字化转型。</p><p>&nbsp;</p><p></p><h3>强强联合，做大模型落地践行者</h3><p></p><p>作为全球化的数字化转型技术服务提供商，浩鲸科技密切关注行业趋势、技术进步和用户需求的变化，聚焦于人工智能技术在垂直领域能力和行业应用场景，实现商业价值。并重视行业标准化的建设和产学研结合，当前已与国内外主流标准组织、高校院所建立了紧密且深度的战略合作关系，以促进人工智能技术和应用的创新。</p><p></p><p>浩鲸科技董事长、总经理鲍钟峻表示，大模型的出现为产业带来很多希望，或许会再次验证IT领域最容易被忽视的规律，“我们总是高估未来两年的变化，却低估未来十年的变化”，大模型的未来任重道远。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43dd49688ef87026216647b4ec458553.png" /></p><p>&nbsp;</p><p>中国信通院人工智能研究所所长魏凯表示：大模型正引领人工智能技术的革新，其强大的多模态感知能力和自主认知能力使其在诸多领域展现出了通用人工智能的潜力。同时，大模型的应用正在加速各行各业的数智化转型，特别是在金融、工业等领域的实践证明了其在降本提效方面的价值。大模型的发展也促进了政策制定、技术开发和市场应用之间的良性循环。中国信通院致力于构建大模型的标准体系，推动技术评估和产业应用研究，并将联合产业各界力量打造一个健康发展的大模型生态体系。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/54/546d13c5cd067a04f2961e7d3878ab1d.png" /></p><p>&nbsp;</p><p>现场，在各位专家及领导的共同见证下，浩鲸科技与中国信通院人工智能研究所正式签署战略合作框架协议。双方重点就人工智能、大模型等领域进行全面深入合作，共同推进相关领域的技术创新、标准研制、测试验证、应用示范、产业化推广、咨询交流、生态共建及联合市场推广等合作，充分利用各自的优势资源，推动AI技术创新和产业发展。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b8/b8fbeba8c2b67350194d1c225f53e098.png" /></p><p></p><p></p><h3>重磅发布，鲸智大模型技术体系及应用</h3><p></p><p>浩鲸科技董事、云智能总裁杨名正式发布鲸智大模型，提出“1+1+4+5+X”技术体系，并详细阐述了浩鲸科技鲸智大模型全栈能力。这不仅是浩鲸科技技术创新的重要成果，也为行业数字化转型注入了强劲动力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b454ac9851ab1d01fc7f0e4117e97b6.png" /></p><p></p><p>浩鲸科技“1+1+4+5+X ”鲸智技术体系具备算力生态的高效适配、模型生态的灵活兼容、数据供给的成本降低、高精度场景的准确性提升、下沉场景的敏捷支撑这五大特性，帮助企业客户在生产、运营、管理中，提升效率、质量、安全的同时，降低数字化与业务运营的成本。同时，基于20余年的行业服务经验和丰富的大模型落地实践，浩鲸科技能够充分理解客户所需，从更专业、更贴合具体问题解决的角度携手合作伙伴帮助客户构建更加贴近业务的垂直大模型，全面释放数字生产力。</p><p>随后，浩鲸科技创新中心总经理王玉木、数据智能首席专家吴名朝、创新中心首席专家张林，分别就鲸智文档大模型、鲸智BI大模型、鲸智代码大模型做详细介绍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/09/0940e72d885d85ee3c141190a68903fe.png" /></p><p></p><p>协同创新，共筑垂直大模型良性生态</p><p>当前，浩鲸科技面向垂直行业领域，基于大模型产品工具、成熟落地方法论与服务体系，助力企业快速构建企业级、领域级、场景级各类智能化应用，已深度赋能至电信、政务、能源电力、烟草、泛零售等行业。</p><p>发布会期间，上海交通大学计算机科学与工程系主任、国家杰青吴帆博士，阿里云智能副总裁、解决方案部研发部总经理曾震宇，阿里云公共云业务大模型技术服务负责人王德山，海南省海口市营商环境建设局党组成员、海口市政务中心主任曹献平，贵州习酒数字与信息化产业部资深专家岳世彬等出席会议并发言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c46f3f02ebffac73666f6cd451f2dbd.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a7/a7431da99f0af7238eb1225331e449f6.png" /></p><p></p><p>发布会的最后，浩鲸科技携手中国信通院、中国社科院、阿里云、海口市营商建设局、贵州习酒等伙伴，共同发起大模型产业生态合作倡议。该倡议旨在汇聚大模型产业链上下游的精英力量，携手并进，深入探索并加速推动大模型技术在市场中的广泛应用、技术层面的深度合作以及人才培育体系的建立健全。</p><p></p><p><img src="https://static001.geekbang.org/infoq/39/3964240456f5a4ca01280f7aa73fd63f.png" /></p><p></p><p>随着产业数智化时代的全面到来，大模型成为引领行业前行的关键力量。浩鲸科技匠心打造的鲸智行业大模型，精准对接政企行业需求，为行业企业提供量身定制的一站式大模型产品及服务，助力企业加速智能化转型步伐。展望未来，浩鲸科技将携手各方力量，持续优化升级大模型产品与服务，致力于为各行各业政企客户带来更加高效、便捷、易用的数字化转型方案，共同推动社会经济的数字化转型与高质量发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/J5pekjwxTFI20oqQCT9K</id>
            <title>豆包视频生成大模型正式发布，首次突破多主体互动难关</title>
            <link>https://www.infoq.cn/article/J5pekjwxTFI20oqQCT9K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/J5pekjwxTFI20oqQCT9K</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Sep 2024 08:39:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>字节跳动正式宣告进军AI视频生成。9月24日，字节跳动旗下火山引擎在深圳举办AI创新巡展，一举发布了豆包视频生成-PixelDance、豆包视频生成-Seaweed两款大模型，面向企业市场开启邀测。</p><p>&nbsp;</p><p>多动作多主体交互能力示例：</p><p></p><p></p><p>一致性切镜能力示例：</p><p></p><p></p><p>&nbsp;</p><p>“视频生成有很多难关亟待突破。豆包两款模型会持续演进，在解决关键问题上探索更多可能性，加速拓展AI视频的创作空间和应用落地。”火山引擎总裁谭待表示。</p><p>&nbsp;</p><p>据火山引擎介绍，豆包视频生成模型基于DiT架构，通过高效的DiT融合计算单元，让视频在大动态与运镜中自由切换，拥有变焦、环绕、平摇、缩放、目标跟随等多镜头语言能力。全新设计的扩散模型训练方法更是攻克了多镜头切换的一致性难题，在镜头切换时可同时保持主体、风格、氛围的一致性。</p><p>&nbsp;</p><p>据悉，豆包视频生成模型经过剪映、即梦AI等业务场景打磨和持续迭代，来达到具备专业级光影布局和色彩调和、画面视觉极具美感和真实感的目的。深度优化的Transformer结构则大幅提升豆包视频生成的泛化能力，支持3D动画、2D动画、国画、黑白、厚涂等多种风格，适配电影、电视、电脑、手机等各种设备的比例，不仅适用于电商营销、动画教育、城市文旅、微剧本等企业场景，也能为专业创作者和艺术家们提供创作辅助。</p><p></p><p>目前，新款豆包视频生成模型正在即梦AI内测版小范围测试，未来将逐步开放给所有用户。剪映和即梦AI市场负责人陈欣然认为，AI能够和创作者深度互动，共同创作，带来很多惊喜和启发，即梦AI希望成为用户最亲密和有智慧的创作伙伴。</p><p>&nbsp;</p><p>此外，豆包大模型不仅新增视频生成模型，还发布了豆包音乐模型和同声传译模型，已全面覆盖语言、语音、图像、视频等全模态，全方位满足不同行业和领域的业务场景需求。</p><p>&nbsp;</p><p>火山引擎在这次发布会上还披露了豆包大模型的使用量。据悉，截至9月，豆包语言模型的日均tokens使用量超过1.3万亿，相比5月首次发布时增加十倍，多模态数据处理量也分别达到每天5000万张图片和85万小时语音。</p><p>&nbsp;</p><p>此前，豆包大模型公布低于行业99%的定价，引领国内大模型开启降价潮。谭待认为，大模型价格已不再是阻碍创新的门槛，随着企业大规模应用，大模型支持更大的并发流量正在成为行业发展的关键因素。</p><p>&nbsp;</p><p>谭待介绍，业内多家大模型目前最高仅支持300K甚至100K的TPM（每分钟token数），难以承载企业生产环境流量。例如某科研机构的文献翻译场景，TPM峰值为360K，某汽车智能座舱的TPM峰值为420K，某AI教育公司的TPM峰值更是达到630K。为此，豆包大模型默认支持800K的初始TPM，客户还可根据需求灵活扩容。</p><p>&nbsp;</p><p>“在我们努力下，大模型的应用成本已经得到很好解决。大模型要从卷价格走向卷性能，卷更好的模型能力和服务。”谭待说道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8uvqICaWuKGYPAB0Ttia</id>
            <title>300 亿 L4 自动驾驶独角兽 CEO 跑路！大裁员 40%、被投资人抛弃，赚钱业务都停了</title>
            <link>https://www.infoq.cn/article/8uvqICaWuKGYPAB0Ttia</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8uvqICaWuKGYPAB0Ttia</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>自动驾驶行业一直充满活力，同时又瞬息万变。就在上周（9 月 18 日），估值近 300 亿人民币的 L4 自动驾驶公司 Motional 宣布了领导层的大变动：CEO 突然自愿“下线”了。</p><p></p><p>据其公告称，其总裁兼首席执行官 （CEO） Karl Iagnemma 将转任高级战略顾问一职；Motion 的首席技术官 （CTO） Laura Major 将担任临时首席执行官兼首席技术官，担任更大的领导角色。</p><p></p><p>Motional 并未公布此番人事变动的原因，但一位知情人士表示，这次调整并无幕后矛盾。Iagnemma 也表示，“我期待着以顾问的身份继续支持 Motional 的使命，并对团队的未来感到兴奋。”</p><p></p><p></p><h1>11 年创业心血，大牛 CEO 毅然让权</h1><p></p><p></p><p>Motional 最初的班底，就来自 Iagnemma 和苏黎世联邦理工学院动态系统与控制专业教授 Emilio Frazzoli 于 2013 年联合建立的初创企业 Nutonomy。</p><p></p><p>尽管 Nutonomy 一直没能像 Waymo 等其他规模更大、资金更雄厚的自动驾驶公司备受关注，但其于 2016 年 8 月在新加坡率先部署自动驾驶汽车服务的公开试验时，还是在广大投资者以及汽车与科技行业当中掀起了一股风潮。</p><p></p><p>短短一年多之后，Nutonomy 就被 Delphi 以 4.5 亿美元收购。后来，Delphi 拆分为两家公司：其动力总成业务成为 Delphi Technologies，而 Aptiv 则专注于设计和生产电子系统、先进安全技术以及自动驾驶汽车所需的硬件和软件，NuTonomy 被并入 Aptiv。</p><p></p><p>在 Aptiv 担任自动驾驶汽车事业部总裁期间，Iagnemma 还领导发布了首个面向公众开放的自动驾驶数据集 nuScenes，包括来自波士顿和新加坡各地的 140 万张图像、39 万个 LiDAR 扫描和 140 万个人工标注的 3D 物体边界框。</p><p></p><p>两年后，随着对自动驾驶汽车的炒作和承诺达到顶峰，现代汽车和 Aptiv 成立了一家 L4 自动驾驶合资企业（后更名为 Motional），原本担任 Aptiv 自动驾驶汽车事业部总裁的 Iagnemma 被选为 Motional 的负责人。当时，两家公司表示，对 Motional 的合并投资将总计达到 40 亿美元（合人民币 281.9 亿元），包括合并工程服务、研发和知识产权的价值。</p><p></p><p>而 Iagnemma 不仅是 Motional 的核心人物，还是自动驾驶汽车行业的重量级先驱。凭借其机器人与自动驾驶汽车的研究成果，Iagnemma 在学术领域享有盛誉。作为曾经的麻省理工学院团队的成员，他曾于 2007 年参与了 DARPA 发起的自动驾驶汽车研发项目“城市挑战赛”。</p><p></p><p>有如此成就的行业元老级人物，就这样突然平静地“放手”了自己一手创办至今已 11 年的公司。这背后的原因，令人不得不深究。</p><p></p><p></p><h1>被投资人中途放弃，赚钱的业务都暂停了</h1><p></p><p></p><p>事实上，Motional 在发布自动驾驶出租车方面取得了一定进展，但同时也面临着严峻的资金困境，导致其商业计划被迫推迟。</p><p></p><p>今年 1 月，Aptiv（合资公司中的一方）宣布将不再为该项目继续注资。“虽然我们的 Motional 合资企业继续在其技术路线图上取得进展，但我们已经决定不再向 Motional 分配资金，并正在寻求替代方案以进一步减少我们的所有权权益。”Aptiv 董事长兼 CEO Kevin Clark 表示。</p><p></p><p>Clark 补充说：“从移动按需市场采用的角度来看，主要在硬件内部和周围提供技术相关的成本确实使其具有挑战性。”换句话说，建立自动驾驶出租车服务的成本很高，而收回这些宝贵资金的时间太长了，Aptiv 无法等待。</p><p></p><p>3 月，有消息称，Motional 获得了一笔未披露金额的过桥贷款作为救命稻草，这家自动驾驶初创公司的下一轮长期融资也得以顺利达成。</p><p></p><p>最终是现代汽车挺身而出，于今年 5 月同意再向 Motional 投资 10 亿美元。其中，现代汽车决定向 Motional 直接投资了 4.75 亿美元，并斥资 4.48 亿美元收购 Aptiv 方面 11% 的普通股权益。这笔交易让现代汽车掌握了多数股权，同时也为这家自动驾驶初创公司提供了维持运营所必需的资金。</p><p></p><p>但这一切并非没有代价。Motional 公司的商业运营思路，包括通过 Uber 和 Lyft 网络在拉斯维加斯使用自动驾驶的现代 Ioniq 5 车辆提供出租车服务。而作为企业重组计划的一部分，Motional 将裁员约 550 人，暂停其商业运营，停止在圣莫尼卡使用自动驾驶汽车为 Uber Eats 客户提供送餐服务，并将使用现代 Ioniq 5 机器人出租车推出下一代机器人出租车服务的计划推迟到了 2026 年。</p><p></p><p>从 Aptiv 今年第一季度的收益报告可以明确看到，在业务前景相对不太乐观的情况下，该公司正在努力管理风险并优化财务状况，但 Aptiv 的撤退与现代汽车的多数股权接手还是引发了人们对于 Motional 未来命运的担忧和质疑。</p><p></p><p>熟悉此番内情的消息人士表示，这次重组是希望在保留资本的同时，在核心技术和商业模式方面取得新的突破。在新计划中，Motional 打算投入更多资源打磨其核心技术。一位知情人士表示，这意味着开展更多测试，包括可能考虑将车辆部署到更多其他城市。此前，Motional 一直在波士顿、匹兹堡和拉斯维加斯测试其自动驾驶技术。</p><p></p><p></p><h1>L4 的商业部署仍极具挑战</h1><p></p><p></p><p>而 Motional 的财务情况转变，正值自动驾驶出租车行业持续面临不确定性之际。这家初创公司虽然一直在缓慢走向商业化，并且至少在五个城市启动了试点项目，但尚未开始收取乘车或者配送费用。</p><p></p><p>宣布重组计划时，Motional 的员工们也被告知，由于商业运营成本高昂加上自动驾驶汽车技术的组件成本居高不下，导致当下部署商业用例仍然极具挑战性。这之后不久，Iagnemma 还发表了一篇博文，概述其重组计划希望“将资源集中在我们核心无人驾驶技术的持续开发和推广之上，同时不再强调近期的商业部署和辅助驾驶用例。”</p><p></p><p>Iagnemma 在博文中解释称，“随着技术发展成熟，更重要的是当自动驾驶部署的商业用例足够明确时，无人驾驶汽车就会正式进入市场。虽然我们对技术进步的速度感到兴奋，而且我们最初的商业部署也获得了宝贵见解，但大规模部署自动驾驶汽车仍然不是短期能够实现的目标，还需要更长的时间周期作为积累。”</p><p></p><p>在 L4 自动驾驶市场当中，通用汽车旗下 Cruise 的商业落地业务也陷入了停滞，原因是在 2023 年 10 月发生的一起事故中，一名行人被该公司的自动驾驶出租车压在车下并拖行。但通用方面已经开始在凤凰城重新绘制地图，选择以缓慢且更为慎重的方式重新将车辆派上公共道路。</p><p></p><p>但与此同时，也有多家 L4 自动驾驶正处在商业部署的上升期。Waymo 继续在美国旧金山、洛杉矶和凤凰城扩展其完全无人驾驶的付费机器人出租车服务，并计划于今年晚些时候登陆奥斯汀。百度 Apollo 正在全面推进全无人自动驾驶规模化应用，已在北京、重庆、武汉多个城市实现全无人自动驾驶商业化运营和测试。</p><p></p><p>参考链接：</p><p></p><p><a href="https://techcrunch.com/2024/09/18/ceo-of-self-driving-startup-motional-is-stepping-down/?guccounter=1">https://techcrunch.com/2024/09/18/ceo-of-self-driving-startup-motional-is-stepping-down/?guccounter=1</a>"</p><p></p><p><a href="https://techcrunch.com/2024/05/02/hyundai-is-spending-close-to-1-billion-to-keep-self-driving-startup-motional-alive/">https://techcrunch.com/2024/05/02/hyundai-is-spending-close-to-1-billion-to-keep-self-driving-startup-motional-alive/</a>"</p><p></p><p><a href="https://techcrunch.com/2024/01/31/autonomous-vehicle-company-motional-is-about-to-lose-a-key-backer/">https://techcrunch.com/2024/01/31/autonomous-vehicle-company-motional-is-about-to-lose-a-key-backer/</a>"</p><p></p><p><a href="https://techcrunch.com/2024/05/07/motional-delays-commercial-robotaxi-plans-amid-restructuring/">https://techcrunch.com/2024/05/07/motional-delays-commercial-robotaxi-plans-amid-restructuring/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/K7cZrEco7Jpu9QMc1BYi</id>
            <title>大模型时代下的新一代广告系统</title>
            <link>https://www.infoq.cn/article/K7cZrEco7Jpu9QMc1BYi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/K7cZrEco7Jpu9QMc1BYi</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 09:11:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>京东零售广告部承担着京东全站流量变现及营销效果提升的重要职责，广告研发部是京东最核心的技术部门，也是京东最主要的盈利来源之一。作为京东广告部的核心方向，我们基于京东海量的用户和商家数据，探索最前沿的深度学习等算法技术，创新并应用到业务实践中，赋能千万商家和数亿消费者的消费连接，不断拓展中国乃至全世界的数字经济边界。 </p><p></p><p>在这里，你将与各业务、产品、工程团队紧密合作，深入京东亿量级的数据与丰富的广告业务场景，进行前沿 AI 算法和工程架构的研究与应用工作。通过 AGI 算法创新和行业领先的广告技术，赋能京东多个业务线的广告投放和管理需求，帮助商家实现精准营销，同时提升用户购物体验，推动京东的商业增长，创造数以亿计的业务贡献。</p><p></p><p>大模型时代的到来，新一代广告系统中，我们目前重点攻坚以下五个方向，欢迎敢于挑战、有梦想的同学，和我们一起共事。让我们一起来看看新一代广告系统中如何实现大模型时代的流量价值预估、流量售卖机制、生成式推荐、智能创意以及承载它的算法工程体系。</p><p>﻿</p><p>文末有最新的机会哟~</p><p>﻿</p><p></p><h1>一、流量价值预估——更好的人货场理解能力</h1><p></p><p></p><h2>1、广告用户意图理解</h2><p></p><p></p><p>Query 意图识别是电商搜索中离用户最近和最基础的一个模块，主要的功能是精确地理解用户的搜索意图，为下游的召回/相关性/排序提供决策信息和特征。Query 意图识别主要是做分词、纠错、NER、品牌识别、类目预测和 query 改写等，需准确捕捉用户意图辅助下游决策，是供需匹配和用户体验的基础。</p><p></p><p>当前 query 意图识别训练样本的产生逻辑导致约 85%的 query 预测的类目都是单类目，且多标签样本的标签量较少。因此亟需在保持现有的类目精准度情况下，提升类目的召回率。通过分析，主要存在以下类型的 query 的高相关召回率不足：</p><p></p><p>•泛词的多意图：侧重知识类，词与具体商品之间需要知识关联，例如：水果，生日礼物，灯；</p><p>•歧义词的多意图：多意图 query 下，基于样本生成逻辑，会偏向主意图，弱化甚至丢失次意图，导致召回问题，例如：小米（粮食 or 手机？），苹果（水果 or 手机？）；</p><p>•长尾类目冷启：由于用户点击数据的马太效应，使得大量的长尾类目没有曝光机会，类目下商品无法获得点击，加深了模型无法得到长尾类目训练数据的问题，例如： 服务类，健康类，工业品类；</p><p>•长尾 query 的多意图：由于用户背景和表达习惯不同，对同类商品需求，会有多种表达方式，产生很多长尾 query。模型给出的类目不准，因此产生的点击数据也不够准确。</p><p>﻿</p><p></p><h3>生成-判别模型增强长尾类目训练数据</h3><p></p><p></p><p>模型的训练依赖 query 点击商品的类目来作为监督信号。像这些偏冷启动类目的商品，我们希望通过增加商品曝光来让它们获得更多点击。有了点击数据，就能正向影响下次模型迭代，让模型下一次可以预测的更准。从而让整个模型迭代的流程形成良性循环，而不是马太效应的恶性循环。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/d9/d9421e02e427c7af63c8d3ca932f5c4f.png" /></p><p>﻿﻿</p><p>解决方案：针对训练样本的类目高度不平衡问题，我们设计了生成-匹配模型，预训练一个 query 生成模型+query-SKU 匹配模型，生成模型用来根据 SKU 的标题/属性信息生成 query，匹配模型用于计算生成 query 和原 SKU title 的相关性分数，卡掉低质量的 query，保证生成 query 的质量。Sku 的类目作为生成 query 的类目，补充到类目预测的训练样本中，平衡和缓解训练集类目失衡问题，让模型能够学习到用户 query 中的长尾类目需求，从而让长尾类目商品有一定的曝光机会。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a3ac716e694e646f6258cc25f0cb046.png" /></p><p>﻿﻿</p><p>生成数据样例：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/45/458e46cec4b0cbd02cb9c7401e86f3b8.png" /></p><p>﻿﻿</p><p>基于搜索日志数据预训练的生成-匹配模型不仅可以在类目预测中使用，也可以用在其他相关业务线。例如 query suggestion 和 query 改写业务，根据 sku title 生成的 query 可以作为两者的 query 召回源。</p><p>﻿</p><p></p><h3>先验知识注入模型解决中长尾类目召回不足导致的商家获量困难问题</h3><p></p><p></p><p>算法训练以用户点击 sku 的类目为标签。但由于马太效应，高点击商品的类目才能获得展现。模型的更新，反而会加剧马太效应，形成恶性循环。</p><p>•用户反馈信号被高频类目主导，需打破仅依赖用户反馈的马太效应闭环。例如：用户搜“耳机”，相关类目包含 862-手机耳机，842-蓝牙耳机... 等 9 个三级类目。由于马太效应，系统只能展现出 1~2 个高点击类目的商品，中长尾类目下商品无展现。</p><p></p><p>•业界最新算法，也高度依赖后验反馈信号，无法召回中长尾类目。</p><p>﻿</p><p>解决方案：通过引入先验知识和模型的优化，增强模型对电商知识的感知，弱化模型对后验反馈的依赖：引入先验知识：类目语义知识、类目共现/语义关系图。通过提取类目名、类目的产品词等，代表类目侧的语义表征。通过类目关系图，反映类目共点击和语义相似关系，实现头部类目带长尾（相关）类目来提升召回率。学习先验知识：设计新模型，以 BERT 为文本编码器，学习 Query 和类目表征。以多通道 GCN 为图结构编码器，学习类目之间的关系。设计半监督 Loss，通过 query-类目语义匹配分数，作为监督信号增强类目标签。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/56/567042528053f2a8922d546fd608d0a6.png" /></p><p>﻿﻿</p><p><img src="https://static001.geekbang.org/infoq/85/85218181b15ca08606636704994092e3.png" /></p><p>﻿﻿</p><p>算法方案发表于 WWW 2024《A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce》</p><p>﻿</p><p></p><h2>2、广告多模态内容理解</h2><p></p><p></p><p>随着富媒体时代的到来，商业广告已告别了纯文本广告时代，图文广告、视频广告已成为广告主进行创意表达的新型方式。目前京东 APP 中的推荐和搜索页面均包含大量图像、视频形式的商品展示。在此场景下，传统单模态 or 少模态的建模方式，有以下问题：</p><p></p><p>•无法建模视觉信息对用户行为的影响，用户对商品展示效果的偏好无法建模。</p><p>•只局限在文本/ID 特征上，无法对商品细节进行精准建模。</p><p>•大量使用物料 ID 特征会带来模型记忆性的问题，使得整个广告系统对广告物料的换血能力会比较差，新物料无法在系统中快速生效。</p><p></p><p>针对上述问题，我们在广告场景下实现了多方位的多模态表征能力建设，并在召回及创意等环节进行了应用，取得了显著的线上效果提升。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/55/55918285e6a37f1cb8f737a28c1973a9.png" /></p><p>﻿﻿图 1.电商场景下的多模态商品展示</p><p>﻿</p><p></p><h3>多模态表征在召回环节的应用</h3><p></p><p></p><p>当前线上的向量化召回模型，过于依赖于 sku 的切词特征、品牌、类目特征等客观特征，对于刻画 sku 的全局属性、主观信息等方面的能力是缺失的。引入 sku 的图像信息，相当于从另一个维度增加了图片的信息，对于 query-item 本身的 match 任务是有正向增益效果的。在未来的发展趋势中，单纯基于文本匹配的方式已经缺乏了优势，图像、视频、虚拟现实 &amp;人机交互等途径的冲击对传统的搜索、推荐任务提出了新的挑战。</p><p></p><p>基于图像 &amp;文本多模态的商品表征 本工作中，我们基于双流模型 pipeline，分别利用预训练的文本表征提取网络和图像表征提取网络，提取京东站内商品的视觉与文本表征，并通过不同的对齐流程得到用于下游推荐任务的多模态商品表征。整体流程包括：内容模态表征提取-&gt;内容模态对齐-&gt;推荐空间对齐三个主要部分。内容模态表征提取对于文本模态信息，基于商品标题+品牌词+三级类目，使用预训练的 BGE-large-zh1.5 模型提取隐层表征，对于视觉模态信息，基于商品主图，使用预训练 ViT-CLIP-base 提取视觉表征。内容模态对齐：对基于预训练 backbone 提取到的隐层表征，使用基于 CLIP 的对比学习方式训练一个 projection head，对文本和视觉模态进行对齐和降维处理。推荐空间对齐：在对齐到推荐任务的语义空间时，首先构造不同模态的商品关系图，之后利用 Gate-GNN 的特征聚合能力，在 item-item 关系图上进行基于商品活跃度的聚合，得到混合模态的商品表表征。</p><p>﻿</p><p></p><h3>多模态表征在创意优选环节的应用</h3><p></p><p></p><p>创意优选环节的多模态理解与排序等环节存在较大区别，排序任务的目标是建模同一用户在多个候选 sku 之间的排序关系，用多模态理解作 sku 信息精细化建模的信息增益来源，更好地建模商品信息，以实现不同商品之间的对比。创意可以表征很多高阶的结构化信息。基于这一点，在创意优化的特征工程上，方向大致是：强化 User/Context，弱化 Item/POI，通过引入多模态的创意表征，来个性化地学习到创意中的卖点信息，从而实现创意层面的最优排序。</p><p></p><p>基于图像模态的商品表征 目前商详主图中存在一定的噪声，因此对于全图的表征往往会受到噪声的干扰，之前的做法往往先对主商品进行抠图，之后再进行特征提取，但是这种两阶段的特征提取依赖主图区域的准确标注，并会带来误差累积的问题，不适合缺乏标签的电商图像预训练任务。我们考虑直接进行图像自监督方法（DINO）进行预训练，在模型训练的同时端到端提取可靠的图像主体表征，具体流程如下图所示：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/9c/9c93412ab1fead0746be8c3737d5c0e7.png" /></p><p>﻿﻿</p><p>无监督模型方案</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/d1/d17afff33a409da2206486bec2e36974.png" /></p><p>注意力图可视化</p><p>﻿</p><p></p><h1>二、流量售卖机制——更优的机制能力</h1><p></p><p></p><h2>1、ListVCG：基于强化学习的序列拍卖机制</h2><p></p><p></p><p>推荐信息流广告是典型的多品拍卖场景，业界通用方案 GSP 在理论、效率上均不是最优解，VCG 多品拍卖机制是我们的理想方案。但是 VCG 仅仅是一个理论上的解决方案，他的前提是需要高效的找到最佳组合拍卖结果。与此同时，推荐业务复杂，是典型的多目标优化场景，但是标准 VCG 是追求社会福利最大化的机制，因此在由 GSP 切换到 VCG 时，平台收益在短期内会显著下降，这也是业界公认的 VCG 机制切换难题。因此如何将 VCG 与多目标优化进行结合也是我们面临的主要挑战。结合京东的实际应用场景，我们提出了 ListVCG 拍卖机制，来解决上述问题。</p><p></p><p>首先面临要解决的是 700 选 4 的排列组合问题，序列的搜索空间上千亿，我们将此定义成一个强化学习的问题，借鉴了经典的 Actor-Critic 架构，Actor 输出概率矩阵，通过采样的手段去求解排列组合问题，同时我们利用用户的真实反馈去提升 Critic 的评估水平，挑选出的最优组合会利用策略梯度的方式指引 Actor 学习。通过这种互相迭代自提升的方式去高效逼近最优组合。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/f5/f54594fea0c68e6728db7192243151b0.png" /></p><p>﻿﻿</p><p>VCG 下的多品拍卖同时是一个经济学问题，需要满足激励相容的拍卖理论约束来保证长期的生态健康发展，然而常见的多目标问题的优化思路会使得无法使用 vcg 计费。因此我们在 Listvcg 中对于 ECPM 价值进行了参数化的变形，在保证可计费的同时通过可学习的参数来满足平台收益、社会福利、用户体验以及物料整体价值多目标优化的诉求。</p><p></p><p>为了更好地对流量长期价值进行建模，我们自然地引入了强化学习的方式，起初我们尝试了传统 off-policy 的 Q-Learning 算法如 DDQN 等，然而，由于后验反馈的奖励稀疏，模型训练效果不稳定，因此，我们尝试引入 reward shaping 以及 curriculum RL 的思想，通过加入稠密先验奖励缓解数据侧的奖励稀疏，并让模型在相对简单的单步决策任务（如序列曝光、点击、单步价值预估等）收敛后，再学习长期决策任务，使得模型效果有了显著提升，在优化长期竞价环境的同时，实现了短期收入和广告主 roi 的上升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/64/64b101efa94f993cfe3210f4067d1415.png" /></p><p>﻿﻿</p><p></p><h2>2、基于强化学习的多智能体博弈</h2><p></p><p></p><h3>多智体在拍卖机制的博弈环境</h3><p></p><p></p><p>目标层面：机制和出价智能体联合优化是行业发展趋势，出价与机制智能体具有一致的整体目标。</p><p>算法层面：我们从算法视角分析出价与机制的策略如何影响广告收入和 tcharge。</p><p></p><p>•平台一段时间的收入由以下三个因素决定：</p><p>1.流量价值分布：一段时间请求数量，广告主数量，以及每个请求 pctr、pcvr、tcpa</p><p>2.广告主调价策略：bid ratio （假设这段时间不变）</p><p>3.平台机制策略：分配以及计费规则</p><p>•具体的，我们有（假设 100 个请求，10 个广告主）</p><p>﻿</p><p>TC=∑i=1100∑j=110xij⋅pctrij⋅pcvrij⋅tcpaj⋅bid_ratiojTC=∑i=1100​∑j=110​xij​⋅pctrij​⋅pcvrij​⋅tcpaj​⋅bid_ratioj​</p><p>﻿</p><p>期望 revenue=∑i=1100∑j=110xij⋅pctrij⋅cpcij 期望 revenue=∑i=1100​∑j=110​xij​⋅pctrij​⋅cpcij​</p><p>﻿﻿</p><p></p><blockquote>机制、出价与用户（环境）的交互关系如下图所示，事实上，在单智能体强化学习下，机制和出价互为环境</blockquote><p></p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/c0/c0c5f07a586f965f4b731c56d19ba210.png" /></p><p>﻿﻿</p><p></p><h3>基于强化学习的多智能体博弈的研究和落地工作</h3><p></p><p></p><p>基于 MPC 和强化学习的出价算法优化点主要在于对未来一段时间请求环境奖励（tcharge、达成率）的预测，以及根据当前的状态（展现、点击、消耗、达成情况）来决定下一步动作（bid ratio）；同样的，基于强化学习的机制策略也需要对未来一段时间请求环境奖励（广告收入）进行预测，并且根据当前的状态（历史收入、预算情况等）来决定下一步动作（分配 &amp;计费）。</p><p>﻿</p><p>机制和出价对未来一段时间奖励预测越准确，动作选择越准确，会带来越多的收入和达成提升。为此，我们根据不同阶段对多智能体技术就行研发：</p><p></p><p>（1）第一阶段：基于离线请求数据的模拟</p><p>•出价和机制智能体一侧固定，通过离线模拟尽可能还原线上策略，进行模型训练</p><p>•难点：</p><p>◦缺少精确的离线模拟环境，目前只有部分精排队列还原，复杂逻辑难以还原</p><p>◦计算量级大；新的机制还在不断迭代中</p><p>（2）第二阶段：基于离线仿真环境的模拟</p><p>•出价和机制智能体通过感知 自身不同动作下对方的反馈，对未来奖励预估更准确</p><p>•风险：</p><p>◦模拟误差累计增大（无法模拟部分/用户行为模拟偏差）</p><p>◦实验评估难以进行</p><p>﻿</p><p>以机制为例，收益本质上来自于对广告主未来行为的预测，比如在某个流量上 bidder 由于 bid ratio 高（但是 cvr 低）获得了流量，虽然平台当次请求收入最大，但是会影响后续 bid ratio 调节，整体收入非最优。通过在仿真环境下寻找更优均衡（需考虑离在线不一致的问题），可以避免广告主（比如某个类目）的出价收敛到对平台整体收入不利的均衡。</p><p>﻿</p><p></p><h1>三、广告生成式推荐——更颠覆的推荐范式</h1><p></p><p></p><p>在京东广告场景，我们面临了如下的挑战：用户行为复杂、平台数据边界、数据稀疏性高、冷启动问题、场景理解困难、多样性和新颖性。由于现实系统中的商品数量巨大，传统 RS 通常采用多级过滤范式，包括召回、粗排、精排、重排等流程，相较于传统 RS，生成式推荐系统具备如下的优势：1）简化推荐流程。实现从多级过滤范式（discriminative-based，判别式）到单级过滤范式（generative-based，生成式）的变迁。2）具备更好的泛化性和稳定性。利用 LLM 中的世界知识和推理能力，在具有新用户和商品的冷启动和新领域场景下具备更好的推荐效果和迁移效果。</p><p>﻿</p><p></p><h2>1、方案</h2><p></p><p></p><p>生成式推荐涉及两个接地（grounding）过程，“将语言空间接地到推荐空间”和“将推荐空间接地到实际商品空间”。为了实现这两个过程，我们的方案如下：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/f2/f2972ca0dfe6cbdf2ce4c1b666bd75cc.png" /></p><p>﻿﻿</p><p>步骤一：商品量化表示阶段</p><p>选取高点击商品的标题、类目等语义信息，经由编码器模型获得向量表示，再利用 RQ-VAE 对向量进行残差量化，从而获得商品的语义 ID。例如，商品“ThinkPad 联想 ThinkBook 14+ 2024 14.5 英寸轻薄本英特尔酷睿 ultra AI 全能本高性能独显商务办公笔记本电脑”可表示为：</p><p></p><p>步骤二：继续预训练阶段</p><p>（1）量化 token 扩展大模型词表并完成初始化</p><p>将商品量化表示的底层 token 集合，加入到大模型中，进行微调对齐训练，使得模型“理解"这些底层 token</p><p>（2）语义 ID 和商品文本信息互译任务</p><p><code lang="text">提示词:
请告诉我,商品的四元组表示为{input_turple}的标题是什么？
输入:

输出:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白

提示词:
请告诉我,商品的标题是{input_title}, 它的四元组表示是什么?
输入:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白
输出:
</code></p><p></p><p>步骤三：微调阶段</p><p></p><p><code lang="text">提示词:
用户历史浏览的商品序列的文本语义信息为{input_text1, input_text2, ..., input_text_N},
请帮我预测用户下一个要浏览的商品是什么？
输入:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 砚黑，
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 砚黑,
华为
输出:


</code></p><p></p><p><code lang="text">提示词:
用户历史浏览的商品序列的文本语义信息为{input_text1, input_text2, ..., input_text_N},
请帮我预测用户下一个要浏览的商品是什么？
输入:
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 宣白,
华为（HUAWEI）旗舰手机mate60 pro+ 16G+512GB 砚黑，
华为（HUAWEI）旗舰手机mate60 pro+ 16G+1TB 砚黑,
华为
输出:


</code></p><p></p><p><code lang="text">提示词:
用户历史浏览的商品的四元组序列为{input_tuple1, input_tuple2, ..., input_tupleN},
请帮我预测用户下一个要浏览的商品是什么？
输入:
，, , 
输出:


</code></p><p></p><p>步骤四：DPO 阶段</p><p></p><p><code lang="text">提示词:
用户历史浏览的商品的四元组序列为{input_tuple1, input_tuple2, ..., input_tupleN},
请帮我预测用户下一个要浏览的商品是什么？
输入:
, , , 
正例: 

负例:


</code></p><p></p><h2>2、效果</h2><p></p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/16/161f1908ca751c1e97cacbacfb9e4418.png" /></p><p>﻿﻿</p><p>我们将上述方案应用于京东站内和站外广告的推荐流程，取得了显著的效果提升。</p><p>﻿</p><p></p><h1>四、广告智能创意——更生动的视觉冲击</h1><p></p><p>﻿</p><p>广告创意不仅能够抓住消费者的眼球，还可以传递品牌核心价值和故事，建立起与消费者之间的情感联系。在电商场景下，创意内容是影响用户点击的重要因素，对广告收入有着重要的影响。为了满足千人千面的用户偏好，我们在大模型时代借助其强大的生成能力，产出以下一系列的创意内容：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/3d/3d9c64553611bc91eb20bae8760aaa53.png" /></p><p>﻿﻿</p><p>尽管最近 AIGC 技术蓬勃发展，使得创意制作摆脱了成本和效率的限制。然而，大模型在广告创意的应用上还存在诸多问题。如下方图片所示，现有的图片生成模型会产出空间失调/大小失调/商品不显著和形状幻觉等 bad case：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/f0/f04f87ae3f0c4d59224a2a6aaf49d293.png" /></p><p>﻿﻿</p><p>为了解决上述问题，我们提出一种多模态可靠反馈网络（RFNet），用于自动审核生成的图片，并将其应用于递归生成过程中，从而提高可用广告图片的数量。此外，我们通过一致性条件正则化（Consistent Condition regularization）微调扩散模型，利用 RFNet 的反馈，显著提升了生成图片的可用率，减少了递归生成的尝试次数，同时保持了高效的生产过程和视觉吸引力。我们还构建了一个包含超过一百万张人工标注生成广告图片的 RF1M 数据集，帮助训练 RFNet 准确评估图片的可用性。这项工作发表在计算机视觉顶级会议 ECCV2024。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/52/52cd372359f425b588e67b462d43f5bd.png" /></p><p>﻿﻿</p><p></p><h1>五、广告大模型算法工程体系——更极致的算法基建</h1><p></p><p></p><p>大模型尤其以 LLM、AIGC 类的典型模型为例，其模型参数通常在 0.5B ~ 72B 之间，在广告场景上带来最直观的挑战是：超大规模模型的训练推理挑战、复杂业务链路的融合。</p><p></p><p>推理上，广告链路跟传统的对话系统不同，其延迟要求极高，通常请求到计算完毕返回之间的耗时仅有 100ms，因此，耗时约束下的推理能力是一个极大的挑战。此外，单请求的推理成本也是业界大模型服务公司挥之不去的追求点。京东广告已经可以做到 1.5B 体积模型，百万 Token 成本较行业成本更低。</p><p>训练上，不论是开源模型再微调和在训练，还是以 Transformer 为核心的自行搭建的模型结构，对片上网络、存内计算、空间时间编排的脉动计算模式等技术要求都有成倍的要求提升。</p><p></p><p>业务链路上，最典型的模型服务以模型内逻辑+外部链路逻辑整合而成，而一个 DAG（RAG）服务是一种不错的融合方式。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/46/46740b949d9fafdaaa253cb81b7fc38b.jpeg?x-oss-process=image%2Fresize%2Cp_80%2Fauto-orient%2C1" /></p><p>﻿﻿</p><p>京东广告算法工程团队在人工智能领域持续深耕，不仅致力于 LLM（Large Language Model）训练推理技术的前沿探索，力求突破自然语言处理的瓶颈，提升模型的语义理解和生成能力。同时，我们也充分认识到硬件基础设施对于大规模模型运行的重要性，因此积极与业界领先的芯片制造商和网络服务提供商展开深度合作。</p><p>﻿</p><p>我们从底层的物理拓扑结构开始优化，确保数据传输的高效性和稳定性，为模型的高速运行奠定坚实基础。接着，针对不同的芯片特性进行定制化的适配工作，让模型能够在各种硬件环境下发挥出最优性能。我们深知，只有软硬件完美结合，才能真正释放 AI 的潜能。</p><p>﻿</p><p>此外，京东广告算法工程团队还对训练框架进行了全方位的优化。我们引入了最新的并行计算技术和分布式存储方案，使得大规模数据的处理和模型的训练变得更加迅捷。同时，我们也在推理服务上狠下功夫，通过缓存策略、负载均衡等手段，显著提升了模型的响应速度和并发处理能力。</p><p>﻿</p><p>这一切的努力，都是为了能够支撑起下一代的超大规模模型，使其能够应对百万级 QPS 的严苛挑战，为用户提供更快速、更精准的广告推荐服务。京东广告算法工程团队将持续创新，以技术驱动业务发展，为实现更智能、更个性化的广告体验而不懈奋斗。</p><p>﻿</p><p></p><h2>TO 亲爱的朋友：</h2><p></p><p>﻿</p><p></p><blockquote>京东广告研发部致力于提供全方位的广告技术服务，包括广告排序、出价、创意算法、广告投放平台建设、大数据生产和数据挖掘、广告质量控制和广告产品创新。我们全天候保障京东广告系统的稳定运行，不断优化广告系统全链路基础能力，持续提升研发效率和交付能力。通过 AGI 算法创新和行业领先的广告技术，赋能京东多个业务线的广告投放和管理需求，帮助商家实现精准营销，同时提升用户购物体验，推动京东的商业增长，创造数以亿计的日均广告收入。在这里，你将与各业务、产品、工程团队紧密合作，深入京东亿量级的数据与丰富的广告业务场景，进行前沿 AI 算法和工程架构的研究与应用工作。</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xWDfxLTL2iIL8fvTwWe1</id>
            <title>大模型拜师学艺！422位专家、学者加入百度“文心导师”计划</title>
            <link>https://www.infoq.cn/article/xWDfxLTL2iIL8fvTwWe1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xWDfxLTL2iIL8fvTwWe1</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 08:01:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，百度文心大模型与上海东方明珠广播电视塔达成合作意向，双方将在AI+文旅领域展开深入合作。截至目前，百度已招募422位各领域专家，在知识传授、结果评估和反馈迭代等方面为文心大模型提供帮助。</p><p></p><p>百度集团副总裁、深度学习技术及应用国家工程研究中心副主任吴甜表示，大模型已成为人工智能主流的发展方向。新一代大模型在理解、生成、逻辑、记忆等通用能力大幅提升，可以进行思考、规划，甚至行动，将推动产业变革，重塑产业未来发展方向。</p><p></p><p>公开资料显示，2018年底，百度就开展了大模型技术研发，2019年3月正式发布文心大模型1.0，今年6月百度发布了文心大模型 4.0 Turbo，基础模型持续迭代。在此基础上，百度借鉴人的思考系统，在基础模型和产业应用场景之间构建了智能体技术。</p><p></p><p>吴甜进一步介绍到，文心智能体在基础模型之上进一步进行思考增强训练，构建了思考模型，具备更强的理解、规划、反思与进化能力，能根据各类需求场景自主调用工具解决问题。智能体技术全面降低了大模型应用的开发门槛。</p><p></p><p>据了解，基于智能体技术构建的文心智能体平台，目前已吸引60万开发者，构建的智能体覆盖上百个应用场景，在为用户提供更好内容和服务的同时，也在产业中越来越深入，助力产业智能化升级。</p><p> </p><p></p><p>百度集团副总裁、深度学习技术及应用国家工程研究中心副主任 吴甜</p><p>随着大模型技术的发展和应用的推进，百度在大模型产业当中的实战经验拓展到更多的领域，推动各行业智能化升级。</p><p></p><p>百度在2023年12月发起了“文心导师”计划。文心导师指导文心大模型学习特定行业、场景专业知识，评定文心大模型生成结果的质量，校准文心大模型生成内容的专业度，帮助文心一言加强在各个专业领域的认知，为用户提供更具思想深度和广度的支持。</p><p></p><p>2024年7月，“文心导师”计划正式对外公开招募。截至目前，已有包括科技、金融、文化、教育、医疗、能源等十余个重点行业在内的422位顶尖专家、学者加入“文心导师”计划。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8oDE9Ygmg31V33OGcC5W</id>
            <title>成立7年融资近10亿元 ，这家AI创企将被英伟达收入囊中！AI大佬趣评：估值应该仅能让投资人回本</title>
            <link>https://www.infoq.cn/article/8oDE9Ygmg31V33OGcC5W</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8oDE9Ygmg31V33OGcC5W</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 07:20:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>英伟达将斥资约10亿元收购初创AI公司OctoAI</h2><p></p><p>近日，外媒 The Information 的 Anissa Gardizy、Aaron Holmes 和 Stephanie Palazzolo 援引发给股东的一份消息称，英伟达已就收购 OctoAI 展开了深入谈判，OctoAI 是一家销售软件的初创公司，旨在提高人工智能模型的运行效率。根据 OctoAI 发给股东的一份文件，英伟达提议以约 1.65 亿美元收购该公司，这还不包括该公司的债务和其他费用。</p><p>&nbsp;</p><p>据公开资料显示，OctoAI（前身为 OctoML）成立于2019年，公司联合创始人兼首席执行官Luis Ceze是一位出生于巴西的美国计算机科学家、商人和学者，也是华盛顿大学Paul&nbsp;G. Allen 计算机科学与工程学院的计算机科学教授，后因其在 Apache TVM&nbsp;和仿生数据存储系统方面的杰出成就而闻名。OctoAI 总部位于华盛顿州西雅图，成立7年间获得了Madrona Venture Partners、Amplify Partners、Tiger Global 和 Addition Capital等机构资金支持。</p><p>&nbsp;</p><p>OctoAI的使命是让人工智能更易于访问和可持续，以便它可以用来改善生活。OctoAI 平台为应用程序构建者提供了完整的堆栈，以便在云端或本地运行、调整和扩展他们的人工智能应用程序。借助适用于 SDXL、Mixtral 和 Llama2 等热门模型的超快推理 API、端到端开发人员解决方案和世界一流的 ML 系统，企业可以专注于构建让客户赞叹的应用程序，而无需成为人工智能基础设施专家。</p><p>&nbsp;</p><p>此前，OctoAI曾从包括Tiger Global Management、Madrona Venture Group和Amplify Partners在内的投资者哪里筹集了1.32亿美元，最终在2021年获得了约9亿美元的估值。</p><p>&nbsp;</p><p>英伟达没有立即回应的置评请求。</p><p>&nbsp;</p><p></p><h2>为什么能被英伟达看上？</h2><p></p><p>&nbsp;</p><p>早在今年4月，OctoAI公司就宣布推出了软件平台OctoStack，该平台可让公司在其内部基础设施上托管人工智能模型。据悉，这是业界首个可在任何地方为生成式 AI 模型提供服务的完整技术堆栈。</p><p>&nbsp;</p><p>在早期，OctoAI 几乎只专注于优化模型，使其运行更有效。基于Apache TVM机器学习编译器框架，该公司随后推出了 TVM 即服务平台，并随着时间的推移，将其扩展为一个成熟的模型服务产品，将其优化功能与 DevOps 平台相结合。随着生成式人工智能的兴起，该团队随后推出了完全托管的 OctoAI 平台，帮助其用户服务和微调现有模型。OctoStack 的核心就是 OctoAI 平台，但用于私人部署。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eaee75f147f4e2d5bc72e84a40d833ef.png" /></p><p></p><p>&nbsp;</p><p>图片来源：OctoAI</p><p>&nbsp;</p><p>OctoAI 首席执行官兼联合创始人Luis Ceze曾在接受媒体采访时称，该公司平台上有超过 2.5万名开发人员和数百名在生产中使用该平台的付费客户。Ceze 说，这些公司中很多都是 GenAI 原生公司。不过，想要采用生成式 AI 的传统企业市场要大得多，因此 OctoAI 现在想通过OctoStack 为这些企业提供服务。</p><p>&nbsp;</p><p>许多大型语言模型都是通过基于云的API（应用程序编程接口）提供的。此类模型托管在各自开发人员的基础设施上，这需要客户将其数据发送到该基础设施进行处理。在内部硬件上托管神经网络无需与外部提供商共享数据，这可以简化企业的网络安全和法规遵从性。</p><p>&nbsp;</p><p>OctoAI 表示，其新的 OctoStack 平台使在公司内部基础设施上托管 AI 模型变得更加容易。该平台可以在本地硬件、主要公共云和 AI 优化的基础设施即服务平台（如 CoreWeave）上运行。OctoStack 同样适用于英伟达集团和 Advanced Micro Devices Inc. 的多种 AI 加速器，以及 Amazon Web Services 中提供的 AWS Inferentia 芯片。</p><p>&nbsp;</p><p>在创建神经网络的初始版本后，开发人员可以通过各种方式对其进行优化，来提高性能。其中一种技术是运算符融合，它能够将人工智能执行的一些计算压缩为更少、更高效的硬件计算。另一种技术是量化技术，它减少了神经网络必须处理的数据量，能够产生准确的结果。</p><p>&nbsp;</p><p>此类优化并不总是适用于不同类型的硬件。因此，针对一款显卡优化的 AI 模型不一定能在不同芯片制造商的处理器上高效运行。OctoStack 采用的开源技术 TVM 可以自动优化不同芯片的神经网络。</p><p>&nbsp;</p><p>OctoAI 表示，这一平台可以帮助客户更高效地运行 AI 基础设施。据该公司称，由 OctoStack 驱动的推理环境提供的显卡利用率是自建 AI 集群的四倍。该公司还承诺将运营成本降低 50%。</p><p>&nbsp;</p><p>OctoAI 联合创始人兼首席执行官 Luis Ceze 表示：“要让客户能够构建可行且面向未来的生成式 AI 应用程序，需要的不仅仅是经济实惠的云推理。硬件可移植性、模型入门、微调、优化、负载平衡——这些都是需要全栈解决方案的全栈问题。这正是我们在 OctoAI 构建的。借助 OctoStack，客户可以根据自己的情况利用我们行业领先的 GenAI 系统，控制他们的数据、模型和硬件。”</p><p>&nbsp;</p><p>OctoStack 的主要优点：</p><p>快速运行任何模型：&nbsp;企业选择理想的开源模型组合（例如 Llama、Mistral、Mixtral 等）、定制模型和专有模型，同时最大限度地提高性能。在任何环境中运行：在用户选择的云中的云虚拟专用连接 (VPC) 中运行：AWS、Microsoft Azure、Coreweave、Google Cloud Platform、Lambda Labs、OCI、Snowflake 等。选择任何硬件目标：&nbsp;在广泛的硬件上运行模型，包括 NVIDIA 和 AMD GPU、AWS Inferentia 等。专业知识和创新：受益于 OctoAI 在独立于硬件的全栈推理优化方面无与伦比的专业知识，这些专业知识经过多年的专门研究和开发磨练而成。持续优化：本地客户可以获得类似于 SaaS 优势的持续更新，包括订阅新优化的模型和对其他硬件类型的支持，确保他们的 AI 能力始终保持领先地位。</p><p>&nbsp;</p><p>OctoStack 支持流行的开源 LLM，例如 Meta 公司的Llama 和初创公司 Mistral AI 开发的Mixtral专家混合模型。公司还可以运行内部开发的神经网络。据 OctoAI 称，OctoStack 可以随着时间的推移更新推理环境中的 AI 模型，而无需对其支持的应用程序进行重大更改。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HPgEAkkw1hjgIDsxWNl0</id>
            <title>一口气发 100 个开源模型、主力模型再大降价！阿里：不然谈什么应用爆发？！</title>
            <link>https://www.infoq.cn/article/HPgEAkkw1hjgIDsxWNl0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HPgEAkkw1hjgIDsxWNl0</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 06:23:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、褚杏娟</p><p></p><p>这两天，阿里在云栖大会上又开“卷”大模型了。一口气上架 100 多个开源模型、主力模型再大降价，“量多”又“价低”的策略又给大家带来了一点“小小”的震撼。</p><p></p><p></p><h1>100 多个开源模型“量大管饱”</h1><p></p><p></p><p>阿里云 CTO 周靖人发布了通义千问新一代开源模型 Qwen2.5，Qwen2.5 全系列总计上架了 100 多个模型，涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型，其中每个尺寸都有基础版本、指令跟随版本和量化版本，创造大模型开源史上之最。</p><p></p><p>“这些（模型）不是竞争的关系，而是把选择留给我们的开发者。”周靖人在回答媒体提问时说道，“今天，开发者会基于自己的业务场景去做模型能力增强和推理效率增强的权衡与选择，很多时候我们不能帮大家选择。”</p><p></p><p>周靖人表示，Qwen2 在最开始时只推出两款，7B 和 70B，之后团队得到整个开发者社区的反馈，希望阿里能推出更多版本，包括 14B、32B 和 3B。“这次我们就把整个模型系列推出来，将更多选择权留给开发者。”</p><p></p><p>另外，周靖人谈到，从去年开始，阿里非常坚定不移做开源，这跟其做云计算的初心是密切相关的。“一方面，我们希望生态的发展。另一方面，我们还是希望能更有效服务到企业。”</p><p></p><p>据了解，Qwen2.5 在语言模型方面开源了 7 个尺寸，0.5B、1.5B、3B、7B、14B、32B、72B。型号的设定考虑了下游场景的不同需求，比如 3B 是适配手机等端侧设备的黄金尺寸；32B 是受开发者期待的“性价比之王”，可在性能和功耗之间获得最佳平衡；而 72B 是 Qwen2.5 系列的旗舰模型，阿里在多个核心任务的测评上，以不到 1/5 的参数超越了拥有 4050 亿巨量参数的 Llama3.1-405B。</p><p></p><p>专项模型方面，阿里用于编程的 Qwen2.5-Coder 在当天开源了 1.5B 和 7B 版本，据悉未来还将开源 32B 版本；专门用于数学的 Qwen2.5-Math 当天开源了 1.5B、7B、72B 三个尺寸和一款数学奖励模型 Qwen2.5-Math-RM。</p><p></p><p>多模态模型方面，广受期待的视觉语言模型 Qwen2-VL-72B 也正式开源。Qwen2-VL 能识别不同分辨率和长宽比的图片，理解 20 分钟以上长视频，具备自主操作手机和机器人的视觉智能体能力。</p><p></p><p>根据阿里的统计，截至 2024 年 9 月中旬，通义千问开源模型下载量突破 4000 万，Qwen 系列衍生模型总数超过 5 万个，成为仅次于 Llama 的世界级模型群。</p><p></p><p>在大模型开源领域，Llama 依然领先。根据 Meta 的最新数据， Llama 系列模型的下载量已接近 3.5 亿次（比去年同期增长了 10 倍多），仅在上个月，下载量就超过 2000 万次，使 Llama 成为领先的开源模型家族。Hugging Face 上有超过 60,000 个衍生模型，开发者会根据自己的用例对 Llama 进行微调。</p><p></p><p>“Llama 的成功得益于开源的力量。通过公开我们的 Llama 模型，我们看到了一个充满活力和多样化的 AI 生态系统，开发人员拥有比以往更多的选择和能力。从突破新界限的初创公司到使用 Llama 在本地或通过云服务提供商构建的各种规模的企业，创新范围广泛且迅速拓展。”Meta 官方表示。</p><p></p><p>对比之下，OpenAI 的 GPT 系列、Anthropic 的 Claude 大模型、百度的文心大模型、华为的盘古大模型等都选择了闭源。这反映了业内对大模型发展的判断有很大的不同，但到底哪条路是可以一直走下去的，目前还没有答案。</p><p></p><p></p><h1>大模型还太贵了，“资费降下来才能爆发”</h1><p></p><p></p><p>过去两年，模型的尺寸已增长数千倍，但模型的算力成本正在持续下降，企业使用模型的成本也越来越低。继 5 月首次大幅降价后，阿里云在大会上宣布，通义千问三款主力模型再次降价，Qwen-Turbo 价格直降 85%，低至百万 tokens 0.3 元，Qwen-Plus 和 Qwen-Max 分别再降价 80% 和 50%。</p><p></p><p>“我们希望企业和开发者能以最低的成本做 AI、用 AI，让所有人都能用上最先进的大模型。”阿里云智能集团首席技术官周靖人指出，只有这样才能带动整个行业的发展，激发更多产业级的创新。今天模型的运用、迭代和各种创新都还在早期阶段，这个时候如果把模型推理放到昂贵级别，会有大量开发者无法有效地批量化使用 AI。</p><p></p><p>他表示，阿里降价主要是通过技术的优化，不光是模型本身在快速迭代，模型推理架构、系统优化和云基础设施也在不断提升，而这些都能把模型的整体成本进一步降下来。</p><p></p><p>今年上半年，AI 大模型行业多番掀起价格战。除阿里外，百度、科大讯飞、字节、智谱等多家企业都先后大幅降价，甚至已经打到了“负毛利”的状态。谈及大模型降价的底线，周靖人直言，今天的大模型价格不存在“已经足够低”的说法，相对未来庞大的应用来说，还太贵了。</p><p></p><p>“这是 AI 基础设施全面革新带来的技术红利，我们会持续投入先进 AI 基础设施的建设，加速大模型走进千行百业。”周靖人表示，阿里云正在围绕 AI 时代，树立一个 AI 基础设施的新标准，全面升级从服务器到计算、存储、网络、数据处理、模型训练和推理平台的技术架构体系，让数据中心成为一台超级计算机，为每个 AI 和应用提供高性能、高效的算力服务。</p><p></p><p>大会现场，周靖人展示了 AI 驱动的阿里云全系列产品家族升级。最新上线的磐久 AI 服务器，支持单机 16 卡、显存 1.5T，并提供 AI 算法预测 GPU 故障，准确率达 92%；阿里云 ACS 首次推出 GPU 容器算力，通过拓扑感知调度，实现计算亲和度和性能的提升；为 AI 设计的高性能网络架构 HPN7.0，可稳定连接超过 10 万个 GPU ，模型端到端训练性能提升 10% 以上；阿里云 CPFS 文件存储，数据吞吐 20TB/s，为 AI 智算提供指数级扩展存储能力；人工智能平台 PAI，已实现万卡级别的训练推理一体化弹性调度，AI 算力有效利用率超 90%。</p><p></p><p>用阿里云副总裁、公共与客户沟通部的总经理张启的话说，“阿里现在在疯狂的搞 AI 大基建，把资费降下来，才有可能去谈未来应用的爆发。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W187D2zUrvg3XvA2YBcL</id>
            <title>一年砸 10 亿只是开始，被“神话”的端到端，在中国自动驾驶圈的“最佳实践”能挣钱吗？</title>
            <link>https://www.infoq.cn/article/W187D2zUrvg3XvA2YBcL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W187D2zUrvg3XvA2YBcL</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 06:15:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 ｜ 华卫</p><p></p><p>预计再过不到半年，特斯拉 FSD 就将正式入华了。9 月 5 日，特斯拉宣布，FSD 将于 2025 年第一季度在中国和欧洲推出。</p><p></p><p>而前不久，基于端到端的特斯拉 FSD V12 版本在推送后得到了业内外的众多好评。就连曾多次与特斯拉公开“互怼”的小鹏汽车董事长何小鹏，都发文评价特斯拉自动驾驶“表现极好”，还激动地表示“2025 会是完全自动驾驶的 ChatGPT 时刻！”</p><p></p><p>以 GPT 为代表的大模型正以其前所未有的创新速度和技术架构，深刻影响着自动驾驶领域的方案研究与发展模式，并且全球的行业版图都在迅速响应这一热潮。从目前国内车企的发力重点来看，端到端也已成为其新一代自动驾驶技术路线。</p><p></p><p>华为、小鹏、小马智行、Momenta、极佳科技、地平线等乘用车自动驾驶企业都在积极跟进，纷纷推出了面向量产的端到端自动驾驶解决方案和车型。在商用车方面，零一汽车也公布了端到端大模型上车的明确时间规划。理想汽车创始人兼 CEO 李想也公开声称，理想汽车将在三年内依靠端到端和世界模型实现 L4 级自动驾驶。</p><p></p><p>就连此前遭遇“寒流”的 L4 自动驾驶市场，也因端到端技术的到来有所回暖。依靠这一技术概念拿下 10 亿美元级别融资的 Wayve，便是一大例证。辰韬资本投资经理刘煜冬表示，“端到端为 L4 商业化开启了第二个成长曲线。”</p><p></p><p>凭借端到端令 FSD 能力飞跃的特斯拉还宣布，将在 10 月 10 日推出 Robotaxi 车型。何小鹏也公开透露，小鹏汽车将在 2026 年推出 Robotaxi。然而，近期车企和自动驾驶厂商一心通过端到端方案实现量产 L4 的动作和预期，引来了不少自动驾驶从业者的质疑：端到端是否被过度“神话”？</p><p></p><p></p><h1>端到端为何突跃智驾圈“顶流”？</h1><p></p><p></p><p>端对端并非这一两年内才横空出世，早在 2017 年就有不少公司在探索这种技术路线的可能性。今年“端到端”在自动驾驶圈翻红并被业内视为杀手锏技术，除 ChatGPT 等大语言模型带来的革新外，与其自身的“魅力”也有必不可分的关系。</p><p></p><p>“端到端模型的诞生，是自动驾驶技术通向大规模商业化的必经之路。”小马智行联合创始人、首席技术官楼天城表示，端到端模型最大的优点之一是泛化性，泛化性能够提高自动驾驶商业化的速度，加速自动驾驶的普及。</p><p></p><p>而据零一汽车智驾负责人王泮渠介绍，相比端到端，传统非端到端的自动驾驶系统不仅泛化性较差，而且在向新场景扩展时，很多之前所用的基于规则的方案会失效，新增加的代码又会使系统的可维护性变差，继而导致边际成本的上升。</p><p></p><p>除此之外，传统自动驾驶系统还存在两方面的劣势。第一是架构的复杂性，多模块的系统不仅开发成本更高，由于每个模块所分配到的计算资源较少，其性能上限也比较低，模块间的通信还会带来很多工程上的优化问题。第二是架构复杂带来的高成本问题，每个模块都需要做开发、维护、项目管理和集成等工作，这也是传统自动驾驶公司的团队规模都非常大的原因。</p><p></p><p>“在我看来，端到端能很好地解决这些问题。”王泮渠表示，从架构来说，端到端只有一个模块，可以很好地解决架构复杂问题，同时也具有降本增效的优势。基于数据甚至知识驱动的端到端泛化性非常强，很有可能快速实现量产，不仅可以将 L2 适配各种车型的成本降得非常低，还能够帮助 L4 减少适配不同场景的时间。</p><p></p><p>另外，楼天城指出，端到端最大的好处是防止不同模块和功能之间信息的丢失。极佳科技工程副总裁毛继明也谈到了这一方面，并解释道，模块之间涉及信息的有效传递问题，上下游模块的接口定义了传输信息的上限，但无论多么精细的接口设计，都会存在信息损失。而端到端的统一模块方案不存在这种信息损失，有助于提升最终的算法效果。</p><p></p><p>同时，毛继明还谈到了端到端架构拥有的更多其他优势。首先是模块误差方面，由于端到端在一个模块下，不存在多模块的误差放大效应，整体智驾算法的能力上限也得以最大程度的提升。其次，多模块架构中，每个模块都有单独的研发节奏和优化目标，并不总能严格对齐整个智驾系统的全局优化目标，导致了潜在的无效优化以及研发资源的投入浪费；而端到端架构只有一个模块，优化目标明确统一，可以有效避免这种内耗式的优化过程。</p><p></p><p>还有一点是，模块化架构的组件之间天然容易构成多个规则驱动的“域”，带来一系列维护挑战以及 corner case 解决困境；而端到端作为典型的完全数据驱动架构，会促使开发人员更积极主动的开始从数据驱动、模型驱动的思维范式去考虑问题和解决问题，提升整个算法团队的认知水平。</p><p></p><p>“整体来讲，端到端系统的开发效率更高，资源消耗也更少。”刘煜冬表示，端到端纯数据驱动的开发范式会减少很多原来的重工程资源投入，并将企业的资源重心转向数据驱动方面的高人才密度以及数据积淀的投入。</p><p></p><p>值得一提的是，端到端带来的用户价值也备受关注。刘煜冬指出，第一，在长尾场景的处理上，端到端系统能够比原来的系统覆盖更多的极限场景，如常识处理能力。第二，自动驾驶系统的行为更加拟人化，也能够更强地建立消费者和系统之间的信任，端到端在博弈性比较强的场景里更像人类司机。</p><p></p><p></p><h1>上限高、下限低，自动驾驶的“终局”还没到？</h1><p></p><p></p><p>尽管端到端的技术优势显著，且一众车企和自动驾驶企业都在积极跟进端到端的应用，但对于所谓“终局模式”的说法，业内至今仍众说纷纭。</p><p></p><p>坚定派如王泮渠表示，“我相信端到端一定是实现自动驾驶的最终形式，但端到端只是一个技术框架，不过具体实现的方式其实有非常多的选项，目前业界还没达成共识。”</p><p></p><p>理智派如毛继明指出，端到端方案具有“上限高，但下限低”的特点。通俗来说就是，做得好可以达到很好的效果，做的不好比传统方案更差。在毛继明看来，是否选择端到端方案要看具体的应用场景。对 L5 级无人驾驶而言，端到端就是唯一解；但对于 L2 和 L3 来说，端到端就只是其中一个可行方案。并且，端到端在应用时需要与其他技术方案进行组合搭配。</p><p></p><p>“端到端给自动驾驶的快速大规模普及提供了很好的技术路径，是否是终局还有待时间验证。”楼天城也有类似看法，认为目前无论是 L2 级还是 L4 级自动驾驶都已经在实现，但实现的质量如何、在多大范围实现，对技术有着不同的要求与标准。</p><p></p><p>对 L2 级别自动驾驶来说，端到端技术是目前的更优路径；对 L4 级无人驾驶来说，端到端可以帮助其快速开拓新区域。但 L4 对安全性要求更高，要达到人类驾驶员的 10 倍以上，因此除使用端到端外，还需结合驾驶意图、应用场景融入高确定性的指令，如交通法规、驾驶偏好等。</p><p></p><p>刘煜冬则给出了更为谨慎的论断：“目前来看端到端是可以预见的未来时间段内自动驾驶的终局，但是更长周期的技术演变有各种可能性。就像三年前我们想不到会有 ChatGPT 这样的技术出现，两三年之后也可能有新的技术架构出现颠覆现在的 ChatGPT。”</p><p></p><p></p><h1>100% 端到端还未出现，何为“最佳实践”？</h1><p></p><p></p><p>虽然尚不能明确端到端是否为自动驾驶的终局方案，但其落地应用显然已成为智能驾驶行业的共识方案。然而，业内对于端到端自动驾驶技术路径的选择仍存在诸多争议。</p><p></p><p>目前，零一汽车正沿着基于多模态大语言模型的端到端路线前进，不仅在一些公开的数据集上做出了效果，还在今年上海人工智能实验室联合 CVPR 等举办的自动驾驶国际挑战赛上，凭借纯视觉的自动驾驶解决方案，在端到端自动驾驶赛道的 143 支国际团队中拿到第二的成绩。</p><p></p><p>王泮渠认为，模块化端到端相当于是一个前期的探索，能更快地去做落地，目前学术界和工业界也有了相对成熟的方案。而采用基于多模态大模型的端到端技术路线有把自动驾驶变成赚钱生意的发展潜力，并且只有强泛化性的基座模型才能带来自动驾驶领域所需要的知识注入和融会贯通。</p><p></p><p>简单来说，大模型的强泛化性会为整个端到端系统带来性能的优势，也会为未来实现大规模量产的可盈利高阶自动驾驶带来可能。并且，未来分别基于多模态大模型和世界模型的这两条端到端技术路线可以互相复用。</p><p></p><p>刘煜冬表示，从原理上讲，one model 更加接近其他领域的 AGI 形态，而世界模型目前主要是数据生成的工具，能否用作自动驾驶系统还需要更长时间来观察。未来两年之内，落地的端到端方案主要有两种：一是模块化的端到端，典型代表是上海人工智能实验室的 UniAD；二是以多模态大模型为主要基础的 one model 端到端，如 Wayve 的 LINGO-2 和理想最近推出的 DriveVLM。</p><p></p><p>而对于世界模型，毛继明持有不同看法。他认为，世界模型才是端到端的合理解决思路。基于世界模型，智驾算法拥有了理解场景并对未来进行合理预测的能力，并基于这些信息做出决策，这才是更符合人类思维逻辑的方案。</p><p></p><p>极佳科技联合创始人、首席科学家朱政进一步补充道，one model 训练起来非常耗资源和时间，对于数据的规模和质量都有非常高的要求。而端到端是利用模型预测能力来进行场景感知和驾驶行为的决策，同人类的驾驶行为和习惯比较一致。据其介绍，目前极佳已经有了基于世界模型的端到端基础原型系统，正在和一家车厂做上车的联合验证，很快就会公开一些进展。</p><p></p><p>去年 8 月，小马智行将感知、预测、规控三大传统模块打通，统一成 one model 端到端自动驾驶模型，目前已同步搭载到 L4 级自动驾驶出租车和 L2 级辅助驾驶乘用车。在楼天城看来，目前无论是模块化端到端还是 one model，都处于早期阶段，还未经过量产交付的验证。预计未来 1 至 2 年内，端到端的技术路线才会从分歧走向共识。</p><p></p><p>“长远来看，端到端的终局终究要步入到 one model 形态。”毛继明表示，就当前状态，华为、小鹏等公司所采用的“两段式”端到端还都属于半端到端的实现，或者说处在端到端的过渡态而非完整态。</p><p></p><p>前不久，极越汽车 CEO 夏一平也公开谈到，“现在市场上没有任何一家是眞的端到端，都是营销的噱头。”据了解，目前极越的端到端智驾方案采用的也是“两段式”技术架构。</p><p></p><p></p><h1>“黑盒”属性是误会，可以做成类似灰盒或白盒</h1><p></p><p></p><p>端到端方案的一系列优势，源于其将多个模块融合在一起的架构，但这种设计也使得系统相较于原先的可理解的“白盒”更接近“黑盒”了，从而具备了更多“不可解释性”。</p><p></p><p>楼天城认为，不可解释性是端到端系统天然的缺陷，但是否会限制端到端自动驾驶技术的发展，要分情况看。对于 L2 来说，不可解释性并不影响端到端的应用，比如模块化端到端仍保留了各个主要功能模块，中间的输出特征可以被进一步提取为可解释的数据。</p><p></p><p>而对 L4 来说，其对安全性和确定性的要求是远高于 L2 的。因此，需要在模型中融入规则性的指令，如交通法规、驾驶偏好等，帮助端到端自动驾驶模型更好理解驾驶意图。与此同时，也需要升级模型能力，以对外输出驾驶意图，进一步提高可解释性。</p><p></p><p>而在朱政看来，虽然从产品层面和最终研发形态来看，端到端确实是一个黑盒，但以工程师和产品设计包括用户的角度来看，端到端是可以做成类似于灰盒或者白盒的。</p><p></p><p>第一，模块化联合端到端详细地区分了感知、预测和规划三个模块，任意一个 planning 结果都可以和前面的某个中间模块关联起来。第二，one model 可以输出模块化的中间结果，标注该结果用来做中间监督可以让 one model 收敛得更好，也可以把模式化中间结果拿出来给工程师或用户看。第三，世界模型最重要的是预测能力，而它的预测结果也可以和模式化的中间结果关联起来。</p><p></p><p>毛继明谈到，目前端到端的“黑盒”说法是对整个模型的训练推理细节的误解。只要把研发认知变成可对外解释的形态呈现出来，就不再是黑盒了。</p><p></p><p>王泮渠同样认为，不可解释性的提出反映的是公众对技术的信任度，即技术本身性能是否达到了大家可以接受的标准。随着数据驱动、算法设计，大模型安全等相关技术的发展，在未来一到两年端到端的性能与可靠性一定会有非常大的飞跃。再通过性能的大规模测试与充分验证，其可解释性不再会是关键问题。</p><p></p><p></p><h1>端到端上车“高峰”将到来，商用车更快落地</h1><p></p><p></p><p>“模块化端到端规模化上市就在最近一年内，基于大语言模型的端到端还额外需要 1 到 2 年的时间。”王泮渠指出，商用车的 L4 自动驾驶一定比乘用车的落地速度要快，原因是能大规模量产的高阶自动驾驶系统非常挑落地场景的难度，而商用车场景比乘用车简单，且单个场景容易商业闭环，也方便做场景渐近。</p><p></p><p>刘煜冬则更为乐观，认为明年模块化端到端和 one model 的端到端就会开始比较密集地开始推送。此外，刘煜冬站在技术发展的激进程度和人才集聚、技术迭代速度和技术应用难度上表示，端到端在商用车和乘用车真正落地的时间可能会差不多，但乘用车的落地范围会比商用车大，商用车要到后期再慢慢起来。</p><p></p><p>“端到端量产前必须要跨过这几道关，首先就是车端算力的准备，第二是端到端算法的迭代，第三是云端数据规模，第四是算力规模，第五是验证方案。”毛继明表示。</p><p></p><p>在他看来，目前特斯拉以及国内的蔚小理、华为等头部主机厂和公司，在车端算力、云端数据规模、和云端算力规模这三项上已经都齐备了。今年年底到明年上半年，几家头部车企的端到端算法就能够达到规模化上车；明年下半年起，行业就会迎来端到端量产上车的井喷状态。</p><p></p><p></p><h1>入局端到端，意味着“重新来过”？</h1><p></p><p></p><p>端到端系统的开发和采用，无疑会对整体智驾方案带来技术革命。那么，入局端到端要对之前的技术推倒重来吗？</p><p></p><p>刘煜冬认为，原来的自动驾驶技术并不会被完全颠覆，端到端会与其共用某些算法和软件方面的积累。</p><p></p><p>一是感知部分，现在很多端到端的前端摄像头信息处理部分会用到 BEV 的做法，如 backbone 或 encoder 之类。二是规控部分，原来在规控的一些 knowhow 可以迁移到端到端系统里。三是数据基建，这是企业未来做端到端所需的重要能力，能做好 BEV 方案的公司数据基建也比较强。</p><p></p><p>而在毛继明看来，是否会推翻，取决于之前的技术方案是什么。他表示，端到端以纯数据驱动的多模态大模型为核心，如果某智驾公司之前的技术方案有很多规则，那这些规则基本上就都要被推翻了；如果之前的技术方案就已经大部分改为模型驱动了，那这部分代码大概率能以某种形式重用。</p><p></p><p>需要强调的是，端到端算法所带来的研发模式更改，才是每个主机厂和自动驾驶公司需要关注的重点，也是最痛苦的地方。</p><p></p><p>王泮渠还谈到，除了模型端，端到端也需要做更多数据方面的工作：第一需要重构数据闭环体系及其迭代效率，第二是端到端的测试和验证，整个仿真平台的传感器输入都要做得非常真实，这是目前很有挑战的技术问题。但人力成本上，端到端智驾系统整体花费是低于非端到端的，因为端到端只有很少的模块，核心团队有 20-30 个工程师应该就够了。</p><p></p><p>此外，毛继明指出，从传统的模块化架构到端到端模式，智驾方案的成本构成也会发生转变：大量写各种规则的研发专家的人力成本会迁移到数据方面。这对有量产能力的主机厂来说是件好事，由于其获取数据的成本较低，智驾方案的整体成本实际会进一步显著下降。</p><p></p><p>算力投入方面，楼天城表示，短期来看，购买大算力芯片确实会增加当下成本。但长期来看，一旦端到端技术成熟应用，前期投入成本会逐渐摊薄。</p><p></p><p></p><h1>纯端到端算力投入小于模块化架构，一年至少一到两个亿</h1><p></p><p></p><p>“想要端到端模型达到比较好的训练程度，一年至少需要一到两个亿的算力资金投入，乘用车赛道数字肯定会更加可观。”</p><p></p><p>据王泮渠介绍，端到端所需的算力分训练和部署两方面。部署相当于要采购多少块域控，这部分成本固定且比较低，与单车成本相关。最大的成本是训练成本，分自建买卡和跟云服务商合作两种。对订单量比较大的车企来说，自己造数据中心是合算的选择；但对订单量没有那么大或处在前期研发阶段的车厂来说，找云服务商租服务器是较好的选择。</p><p></p><p>此前，理想汽车智能驾驶副总裁郎咸朋曾公开透露，目前理想每年投在算力训练上的花费是 10 亿人民币，预计未来每年都花费要到 10 亿美金。“如果一年拿不出 10 亿美金来去做训练的话，可能会在将来的自动驾驶竞争当中被淘汰。”</p><p></p><p>算力规模上，楼天城认为，如果只是简单的一次端到端自动驾驶模型训练，上百张大算力的 GPU 就可以支持。如果要长期投入，并保证端到端质量，各个自动驾驶公司的训练算力规模基本在上千卡级别，车企投入会更多。</p><p></p><p>毛继明则给出更为具体的端到端算力需求：整个系统至少需要两台英伟达 Orin 或者单台英伟达 Thor。他表示，纯端到端系统的算力需求小于模块化架构的总算力需求，但量产端到端除了主系统外，往往还有一个旁路系统，其算力需求一般与之前的模块化架构的相当。</p><p></p><p>但王泮渠认为，随着车端计算芯片能力的上升，算力并不会成为未来端到端上车的障碍。楼天城持相同看法，表示从经典架构到端到端，总的代码数量会显著降低，端到端神经网络带来的计算资源消耗相比 BEV 模型并不一定会显著提升。</p><p></p><p>“对更高算力的渴望更多来自于模型参数量和模型性能的提升，而不是来自于端到端的转变。”另外，他指出，从端到端落地应用角度出发，相关企业更应该思考的是，如何充分利用现有的芯片算力资源提升利用效率。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8cvb1qBo4dzX9zRYYDif</id>
            <title>100T极致算力+全链路开发支持，地瓜机器人为具身智能造“基座”</title>
            <link>https://www.infoq.cn/article/8cvb1qBo4dzX9zRYYDif</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8cvb1qBo4dzX9zRYYDif</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 02:53:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，以“加速智能生长”为主题的“2024地瓜机器人开发者日”活动在深圳成功举办。作为业界领先的机器人软硬件通用底座提供商，地瓜机器人重磅推出面向“机器人+”时代的软硬件产品全家桶，包括专为新一代通用机器人而生的旭日5智能计算芯片、极致易用全能开发首选RDK X5机器人开发者套件、具身智能全场景算力核心RDK S100，以及赋能全链条全生命周期的机器人云端开发环境，软硬结合、端云一体，让开发更简单、让机器更智能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/467088105000950dc4ff0bd30ee113ad.png" /></p><p></p><p>伴随大模型和具身智能技术的突破，机器人有望在2030年成为下一轮产业升级的重要引擎。统一的软硬件通用开发底座，将为机器人赋能千行百业夯实根基。站在“机器人+”时代的爆发前夜，地瓜机器人将以全场景覆盖的机器人开发套件全家桶、全链路机器人开发基础设施、全产业链协同的生态布局，加速机器人智能升级，并为具身智能从“实验室”走向“应用场”提供最优解。</p><p></p><p>活动上，地瓜机器人重磅宣布RDK S100将由星动纪元、逐际动力、求之科技、睿尔曼、国讯芯微等数家行业顶级合作伙伴率先搭载。地瓜机器人CEO王丛表示：“AI大模型正为具身智能赋予更通用、更泛化的智慧大脑，国内完善的机器人产业链为具身智能发展提供了肥沃的土壤，而地瓜机器人在软硬件开发基础设施领域的长期积淀，能够为机器人开发构建更简单、更高效的创新路径。正是这样的‘天时、地利、人和’，地瓜机器人有信心成为具身智能发展的通用底座，携手广大生态伙伴和开发者共同孕育‘机器人+’的全新时代。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/3140523afaf4c89b989a81537975d5d1.png" /></p><p>地瓜机器人CEO王丛</p><p></p><p></p><h2>打造机器人开发者套件全家桶，让更智能的机器人开发更简单</h2><p></p><p></p><p>当前，机器人行业处于硬件成熟度低、算法尚未收敛、底层工具重复造轮普遍的松散发展阶段，面临效率低、成本高的困境。地瓜机器人致力于打造“让机器人创造更简单的全链路开发基础设施”，从端侧的旭日智能计算芯片和RDK机器人开发者套件，到广泛兼容的机器人操作系统，再到云侧的机器人云端开发环境和机器人算法中心，为机器人设计、开发、测试、仿真验证到量产、迭代的每一个环节降本增效。</p><p></p><p>专为新一代通用机器人而生的旭日5智能计算芯片基于BPU贝叶斯架构而来，拥有高达10 TOPs算力，并提供充沛异构计算资源，能够为Transformer、BEV、Occupancy等复杂模型和最新算法提供极致的计算效率，仅3W功耗即可完成全栈计算任务，充分满足扫地机、割草机等消费级机器人对智能化升级的强大性能需求，以及恶劣使用环境下对功耗的严格要求。目前，地瓜机器人已经和诸多行业头部客户达成合作，共同推动基于旭日5的消费级机器人产品的量产落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/679ee2bac0280d2bb8b017e98e631c78.png" /></p><p></p><p>以不断迭代进化的智能计算芯片为核心，地瓜机器人RDK一站式智能机器人开发者套件全家桶在本次活动上全面亮相。横向覆盖从消费级机器人，到商清、巡检、AMR，再到人形/四足机器人乃至前沿科研的全场景需求；纵向配套RDK OS + NodeHub + RDK Studio + Cloud的全链路机器人开发平台，全方位助力机器人开发与规模化落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/50/500f491f4fa994d1c678813c87ebb0e4.png" /></p><p></p><p>RDK X5面向中小创客和个人开发者，提供极致算力性价比与极简开发体验，是千元内最佳智能计算平台。其拥有10 TOPs超大算力，只需一根Type-C线便能轻松实现全栈开发流程，同时接口丰富、拓展灵活。RDK X5还配套NodeHub的多种先进大模型和机器人算法，更有100+配套件自由选择，助力开发者一路通关，快速完成从0到1的产品创新。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/24/24106366920305d9bd550864c2de8ff6.png" /></p><p></p><p>RDK S100是面向具身智能前沿创新与应用突破的全场景算力核心，基于新一代BPU纳什架构设计，拥有百TOPs级算力，专为大参数Transformer优化，可以满足人形机器人、仿生机器人等具身智能应用场景对感知精度和泛化能力的高阶需求。RDK S100还提供丰富的图像传感器接入方式和多种接口拓展板，为具身智能机器人的开发提供灵活拓展性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/0372565789be5650e6131d93bf051810.png" /></p><p></p><p>地瓜机器人全链路开发平台将全面支持RDK X5和RDK S100，赋能机器人开发的全生命周期。全新的Cloud云端开发环境带来了行业内首个基于大模型的机器人设计开发助手RDK-Copilot，可辅助进行代码补全，并提供高效智能的问答助手；Data-Copilot可帮助客户实现高效的数据处理和利用，克服样本数量不足和样本获取难度大的挑战。</p><p><img src="https://static001.geekbang.org/infoq/f2/f2bfcd800aaec207554a905a02bd241c.png" /></p><p></p><p>地瓜机器人算法中心NodeHub亦全新升级，集成200+机器人开源算法与应用程序，涵盖Transformer、RWKV、CLIP、Mobile-SAM等多种端侧精调大模型，以及双目深度、Occupancy、AI+VSLAM、3D点云计算等自动驾驶级别感知算法，可为机器人赋予更具泛化性和通用性和强大感知、决策能力，并支持一键安装调用。RDK Studio让开发者可以通过直观易懂的可视化界面轻松上手机器人开发，仅需10分钟就能快速完成算法和应用部署，提升全流程开发体验。</p><p></p><p>地瓜机器人开发者生态负责人胡春旭表示：“机器人开发是一项庞杂的系统工程，我们的目标是打造‘好、快、多、省’的机器人开发新范式，让机器人开发更简单。从算力高效的好用开发者套件、快人一步的操作流程、丰富多样的算法资源，到极具吸引力的性价比，真正实现机器人开发的效率跃升。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4abf41866cc42c0bfd5adfb88050ae46.png" /></p><p>地瓜机器人开发者生态负责人胡春旭</p><p></p><h2>共建智能机器人产业繁荣生态，引领机器人体验变革与创新孵化</h2><p></p><p></p><p>以旭日智能计算芯片和RDK机器人开发者套件为核心，地瓜机器人致力于与行业客户、合作伙伴、创客、个人开发者携手共建软硬一体、产学研深度协同的智能机器人产业生态圈，引领消费级机器人体验变革、加速创新产品应用孵化，商业和生态成果硕果累累。</p><p></p><p>在商业成果方面，新一代旭日5智能计算芯片发布即爆款，率先应用于扫地机、割草机、机械臂、家庭陪伴、四足狗、工业相机、视频会议等领域多家行业头部客户的重点产品中。基于数百万的交付验证，旭日智能计算芯片以强大的计算性能、完善的开发支持和丰富的生态支持，在扫地机、割草机等消费级市场中份额持续领跑，已经成为消费级机器人智能升级的标配之选。</p><p></p><p>在生态成果方面，地瓜机器人基于独一无二的大规模量产实践，构建了从人才培养、创意孵化、产品加速到运营增长的全生命周期开发者赋能体系，已累计服务200+中小创客及50000+个人开发者，实现50+机器人品类覆盖。同时还携手200+合作院校，推出100+校园课程，建立1000+赛事队伍，组织100+场开发者活动，为机器人行业人才培养聚集产学研融合势能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bfd2c0213e2e4ef672af554c83746b36.png" /></p><p></p><p>本次开发者日汇聚了近400名机器人开发者的热情参与，并举办了RDK X5开发工作坊、开发者TalkShow、DUP教育生态联盟研讨会、具身智能前沿论坛以及极客夜话等多场平行活动。一场场充实的上手开发实践、创新成果汇报、创业干货分享和前沿趋势探讨，让每一位开发者收获满满，更好地了解机器人创新、创业所需的一切。活动现场，更有基于地瓜机器人生态的各式机器人产品展出，一应俱全的产品形态和新奇功能，展现了机器人+时代的繁花似锦和无限可能。</p><p></p><p>承载着“成为机器人时代Wintel”的初心梦想，地瓜机器人如今以焕然新生的品牌形象，开启“机器人+”时代的全新旅途，将聚合更优质的产业资源，筑牢具身智能软硬件通用底座，构建智能机器人时代的母生态，加速机器智能进化，促进人机和谐伴生。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/js2cvHZ2r4FbH0g7EXgk</id>
            <title>内部数百工程师可随意摄取OpenAI先进模型！OpenAI前员工揭露：谏言即被开除，祈祷公司不报复</title>
            <link>https://www.infoq.cn/article/js2cvHZ2r4FbH0g7EXgk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/js2cvHZ2r4FbH0g7EXgk</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 01:13:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>就在OpenAI新模型发布不久还被广泛赞誉的时候，OpenAI举报人对其悄悄“背刺一刀”。</p><p>&nbsp;</p><p>“我在OpenAI工作期间，运营体系有很长一段时间都存在漏洞，这些漏洞可能允许我或者公司内的其他数百名工程师绕过访问控制、窃取包括GPT-4在内的最先进AI系统。”OpenAI 举报人 William Saunders 在近日参加的一场关于人工智能监管的听证会上说道。</p><p>&nbsp;</p><p>去年年初，一名黑客入侵了OpenAI 的内部消息系统，并窃取了该公司 AI 技术设计的详细信息。这件事大家是最近才知道的。OpenAI 高管于 2023 年 4 月在公司旧金山办公室举行的全体会议上向员工透露了这一事件，并通知了董事会。但高管们决定不对外公开这一消息，因为没有关于客户或合作伙伴的信息被窃取。另外高管层也认为这与所谓国家安全没有关系，因为他们相信黑客是一名个人，与外国政府没有联系。</p><p>&nbsp;</p><p>Saunders在过去三年，一直担任OpenAI公司的技术人员。“OpenAI一直强调自己正在进步，但我和其他已经辞职的员工都怀疑他们能不能跟得上发展的节奏。”Saunders说道。</p><p>&nbsp;</p><p>他以OpenAI 最新模型o1为例称，这是首个向着生物武器风险迈进的系统，因为它能够帮助专家规划并重现已知的生物威胁。“如果不是经过了严格测试，o1项目的开发者很可能忽略掉这种危险的能力。虽然OpenAI率先开展了相关测试，但公司的总体思路仍然是以部署为优先、而非以严格监管为优先。因此我认为，OpenAI确实有可能遗漏未来AI系统当中某些重要的危险能力。”</p><p>&nbsp;</p><p>“我之所以从OpenAI辞职，是因为我不再相信他们会以负责任的态度制定AGI相关决策。”看得出来，他抨击的OpenAI 不止不关心AI安全，自家的安全也是一点都不担心。</p><p>&nbsp;</p><p></p><h2>批评OpenAI，是要被“惩罚”的</h2><p></p><p>&nbsp;</p><p>今年6 月有媒体曝出，OpenAI 员工如果想离开公司，将面临大量的、限制性极强的离职文件。如果他们在相对较短的时间内拒绝签字，就可能被威胁失去公司既得股权。这种做法在硅谷并不常见。这项政策迫使离职员工在放弃他们已经赚到的数百万美元和同意不批评公司之间做出选择，而且没有截止日期。</p><p>&nbsp;</p><p>这一消息在 OpenAI 内部引起了轩然大波。与许多硅谷初创公司一样，OpenAI 的员工通常以股权的形式获得大部分的预期薪酬。员工们往往认为，一旦按照合同中规定的时间表“归属”，这些股权就属于他们了，公司无法收回。</p><p>&nbsp;</p><p>外媒报道后的第二天，首席执行官 Sam Altman就发文道歉，总体意思是：我不知道我们有一些威胁公平的条款，我保证我们不会再这样做了。OpenAI 部分高管在公司内部也表达了同样的歉意。OpenAI 首席战略官 Jason Kwon 承认，该规定自 2019 年以来就已实施，但“团队确实在一个月前就发现了这个问题。这么久了才被发现，是我的错。”</p><p>&nbsp;</p><p>但外媒指出，公司领导层的道歉存在问题。终止文件中的离职信中写道：“如果您拥有任何既得的权益单位…您必须在60天内签署一份放弃索赔协议，以便保留这些权益单位。”该协议由 Kwon 和 前OpenAI 人力副总裁 Diane Yoon签署。这份秘密的超限制性保密协议仅针对已既得股权的“对价”，由首席运营官 Brad Lightcap 签署。</p><p>&nbsp;</p><p>用威胁既得股权的方式让前员工签署极其严格的保密协议只是一部分，这里面还涉及更多细节。OpenAI 发出的冗长而复杂的解雇文件有效期只有七天，这意味着前员工只有一周的时间来决定是接受 OpenAI “封杀”，在无限期内无法发声批评OpenAI，还是承担失去数百万美元的风险。但时间非常紧迫，他们几乎没有时间去寻找外部顾问。</p><p>&nbsp;</p><p>“我们希望确保你们明白，如果不签署，可能会影响你们的股权。这对每个人来说都是如此，我们只是按规矩办事。”OpenAI的代表如是说道。</p><p>&nbsp;</p><p>大多数前员工在压力之下屈服了。对于拒绝签署第一份终止协议并寻求法律顾问的员工，OpenAI改变了策略：没有说要取消股权，而是说阻止其出售股权。“你必须明白，此外你将没有资格参与未来我们赞助或促成的招标活动或其他流动性机会。”</p><p>&nbsp;</p><p>此外，公司文件中还包含，“根据公司的全权酌情决定权”，任何被公司解雇员工的既得股权可以减至零。还有条款规定，公司可以全权决定哪些员工可以参与出售其股权的招标要约。</p><p>&nbsp;</p><p></p><h2>解雇一个员工，找个理由就可以</h2><p></p><p>&nbsp;</p><p>可能有人还记得，今年OpenAI 因涉嫌泄露信息解雇了两名员工，其中一个就是Leopold Aschenbrenner。Aschenbrenner 后来解释了他为什么会被解雇，并透露了更多细节。</p><p>&nbsp;</p><p>“去年某个时候，我写了一份关于未来 AGI 道路上需要做的准备、安全和保障措施的头脑风暴文件。我与三位外部研究人员分享了这份文件以征求反馈。这就是泄密的内容。”Aschenbrenner 解释称，“当时在 OpenAI，与外部研究人员分享安全理念以获得反馈是完全正常的。这种情况一直都有。文档中有我的想法。在我分享之前，我审查了它是否有任何敏感内容。内部版本提到了未来的集群，我在外部副本中删去了它。有一些内部PPT的链接，但对于外部人员来说，这是一个无效链接。PPT也没有与他们分享。”OpenAI给他的回复是“计划在 2027-2028 年实现 AGI”，但公司并未设定准备时间表。</p><p>&nbsp;</p><p>但这并不是他被解雇的真相，真正的原因是他写了一份关于 OpenAI 安全性内部备忘录并与董事会分享了该备忘录。</p><p>&nbsp;</p><p></p><blockquote>去年，我写了一份关于 OpenAI 安全性的内部备忘录，我认为这份备忘录严重不足，无法防止外国参与者窃取模型权重或关键算法机密。我与几位同事和几位领导分享了这份备忘录，他们大多表示这份备忘录很有帮助。&nbsp;几周后，发生了一起重大安全事件。这促使我与几位董事会成员分享了这份备忘录。几天后，我清楚地知道，领导层对我与董事会分享这份备忘录非常不满。显然，董事会就安全问题向领导层提出了质问。&nbsp;我因与董事会分享备忘录而收到人力资源部门的正式警告。人力资源部的人告诉我，担心间谍活动是种族主义行为，而且没有建设性。我可能没有发挥出最佳外交能力，本可以更精通政治。我认为这是一个非常重要的问题。安全事件让我非常担心。&nbsp;我之所以提起这件事，是因为当我被解雇时，他们明确表示安全备忘录是我被解雇的主要原因。他们说：“这是解雇而不是警告，因为这份安全备忘录。”</blockquote><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Aschenbrenner 透露，在被解雇前，他被拉到一边跟律师交谈，但双方很快就对峙了起来。律师问了他对人工智能发展的看法、对 AGI 的看法、AGI 的适当安全级别、政府是否应该参与 AGI、我和超级联盟团队是否忠于公司，以及他在 OpenAI 董事会活动期间做了什么。然后律师还和Aschenbrenner 的几个同事谈了谈，然后回来告诉就告他被解雇了。“他们查看了我在 OpenAI 工作期间的所有数字文件，然后发现了泄密事件。”</p><p>&nbsp;</p><p>此外，律师团队还提出了其他几项指控。其中一件是，Aschenbrenner 在调查期间不愿透露与谁分享了头脑风暴文件，他表示自己确实不记得了，只记得曾与一些外部研究人员讨论过这些想法。“这份文件已经有六个多月了，我花了一天的时间研究它。”Aschenbrenner 表示，“它根本就不值得关注，因为它根本不是什么问题。”</p><p>&nbsp;</p><p>Aschenbrenner表示，OpenAI 还声称不喜欢他参与政策的方式。</p><p>&nbsp;</p><p>“他们引用了我曾与几位外部研究人员谈过的观点，即 AGI 将成为政府项目。事实上，我当时正在与该领域的许多人讨论这一观点。我认为这是一件值得思考的重要事情。所以他们找到了我五、六个月前写给一位同事的 DM，他们也引用了这一点。”Aschenbrenner说道。“我曾认为，与该领域的外部人士讨论有关 AGI 未来的高层问题是符合 OpenAI 规范的。”</p><p>&nbsp;</p><p>OpenAI公司内部的其他员工都对这样的事情表示惊讶。</p><p>&nbsp;</p><p></p><blockquote>从那时起，我和几十位前同事谈过这件事。他们的普遍反应是“这太疯狂了”。我也感到惊讶。几个月前我才刚刚升职。当时，Ilya 对我升职一事的评论是：“Leopold 太棒了。我们很幸运能拥有他。”</blockquote><p></p><p>&nbsp;</p><p>“从某种意义上说，这是合理的。我有时在安全问题上可能很烦人，这惹恼了一些人。我反复提到这一点，也许并不总是以最圆滑的方式提出来的。尽管有压力要求我在董事会活动期间签署<a href="https://www.nytimes.com/interactive/2023/11/20/technology/letter-to-the-open-ai-board.html">员工信，但我并没有签署。</a>"”Aschenbrenner。</p><p>&nbsp;</p><p>Saunders 和其他OpenAI员工此前签署了关于“对先进人工智能发出警告的权利”公开信。他们认为，只要政府还没有对AI企业施以有效监督，那就只有现任及前任员工来负起责任。然而，广泛的保密协议却阻止了他们表达自身担忧的权利，除非企业主动承认他们无力解决这些问题。”</p><p>&nbsp;</p><p>Saunders在听证会上隐隐透露出担心被报复的想法。他表示，“鉴于整个行业的历史上曾经发生过极端情况，我们中有一部分人可能担心受到各种形式的报复。我们并不是第一批遇到、或者决定正视这些问题的人。”</p><p>&nbsp;</p><p></p><p>参考链接：</p><p><a href="https://www.judiciary.senate.gov/imo/media/doc/2024-09-17_pm_-_testimony_-_saunders.pdf">https://www.judiciary.senate.gov/imo/media/doc/2024-09-17_pm_-_testimony_-_saunders.pdf</a>"</p><p><a href="https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees">https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees</a>"</p><p><a href="https://www.dwarkeshpatel.com/p/leopold-aschenbrenner?open=false#%C2%A7what-happened-at-openai">https://www.dwarkeshpatel.com/p/leopold-aschenbrenner?open=false#%C2%A7what-happened-at-openai</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ag5uZyWgd2ofRQbPlsMy</id>
            <title>投资等于捐赠！？65 亿美元已经砸给 OpenAI，微软、苹果、英伟达争夺入场券，最新估值 1500 亿美元创历史</title>
            <link>https://www.infoq.cn/article/ag5uZyWgd2ofRQbPlsMy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ag5uZyWgd2ofRQbPlsMy</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 01:09:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>&nbsp;</p><p>据知情人士透露，OpenAI的最新一轮融资即将完成，潜在投资者将于本周五内得知是否有资格参与交易。该公司已要求投资者至少出资2.5亿美元。</p><p>&nbsp;</p><p>知情人士表示，OpenAI 的65亿美元融资已经超额认购，也就是说投资者希望投入的资金超过了该公司准备接收的金额。其中一位知情人士还提到，超额认购金额高达数十亿美元，部分投资者将于上周五得知自己是否入选。OpenAI方面拒绝对此发表置评。</p><p>&nbsp;</p><p>这位知情人士表示，包括OpenAI最大支持者微软公司以及新投资方英伟达及苹果在内的多家战略投资方，有望顺利获得融资参与资格。其中一位知情人士表示，这些科技公司在这轮融资中所占的份额预计将达到20至30亿美元，但目前还不清楚他们是投入现金，还是现金和其他资源（如计算能力）的混合投入。此前消息，英伟达有意向在OpenAI新一轮融资中投入1亿美元。</p><p>&nbsp;</p><p>此外，这笔交易还将推动OpenAI的估值达到1500亿美元（大约10000亿人民币），且该数字还不包含本轮新投资。OpenAI在此前一笔融资交易中的估值为860亿美元，这次融资让OpenAI估值提高了 74%。</p><p>&nbsp;</p><p>知情人士指出，原投资方Thrive Capital将领投本轮融资，并开出12.5亿美元的支票。Thrive Capital方面同样拒绝发表置评。</p><p>&nbsp;</p><p>此外，至少将有一家知名原OpenAI投资方不会参与最新一轮融资，即红杉资本。红杉最近支持了OpenAI的竞争对手AI厂商Safe Superintelligence，后者由OpenAI联合创始人Ilya Sutskever创立，并于今年早些时候离开了Sam Altman领导的OpenAI。红杉没有立即回应置评请求。</p><p>&nbsp;</p><p>此轮融资一旦最终敲定，OpenAI 就会成为硅谷历史上最有价值的科技初创公司之一，超过支付公司 Stripe在 2021 年私募融资中实现的 950 亿美元估值，还将使这家初创公司获得全球三家最有价值科技公司的资金支持。</p><p>&nbsp;</p><p>在本轮融资之前，OpenAI 已经从微软那里获得了100亿美元的B轮融资，OpenAI融资后的利润分配分为4个阶段：优先保证马斯克等首批投资者收回资本，保证他们不亏钱；微软将有权获得OpenAI 75%的利润，直到收回前后共130亿美元的投资；在OpenAI的利润达到920亿美元后，微软的利润分配比例将下降到49%，剩余49%的利润由其他风险投资者和OpenAI的员工作为有限合伙人分享；在利润达到1500亿美元后，微软和其他风险投资者的股份将无偿转让给OpenAI的非营利基金。本次融资后的利润分配还未可知。</p><p>&nbsp;</p><p>此前还有报道称，OpenAI 讨论改变其公司结构，使其变得更加有利于投资者。此前投资者必须签署一份运营协议，其中规定：“以捐赠的精神看待对[OpenAI 营利性子公司]的任何投资是明智的”，并且 OpenAI“永远不会盈利”。但这并没有阻碍大家的投资热情。目前还不知道OpenAI 的是否会改变结构，但此前考虑的一个选择是取消对营利性子公司投资者的现有利润上限。</p><p>&nbsp;</p><p>OpenAI 的一位投资者表示，转向更简单的营利结构将受到投资者的欢迎。“所有优先投资者都有利润上限，很多人都在谈论将其变成一种更传统的投资，这样我们的上行空间就不会受到限制。”</p><p>&nbsp;</p><p>值得注意的是，在OpenAI本月完成新一轮融资之前，国内大模型厂商已经率先进行了一轮融资：月之暗面在8月进行了3亿美元的融资，目前估值达到33亿美元，这是最近6个月里月之暗面的第三次融资，三次融资总额达13亿美元；同样在8月，零一万物完成新一轮数亿美元的融资，最新估值为104亿元人民币；百川智能于7月已完成50亿元人民币的A轮融资，并且以200亿元估值开启B轮融资；智谱AI在今年1月、7月分别完成一轮股权融资。</p><p>&nbsp;</p><p></p><h2>网友们高唱“泡沫”</h2><p></p><p>&nbsp;</p><p>“嗨，投资者们，只要支付‘区区’2.5 亿美元，你就能成为 AGI 的创造者之一。”有网友调侃道。此轮融资也正值人们对AI领域泡沫的担忧日益加剧之时，随着构建和训练AI模型的成本不断上升，打造人工智能模型的公司面临着投资者要求其创造回报的压力。</p><p>&nbsp;</p><p>OpenAI 在最近的时间里发布产品虽然保持了一定的频率，但直到o1之前的模型大多都反响平平。“在 o1 之前，ChatGPT 已经变得无关紧要。我认为他们匆忙推出它是为了再次引起人们的关注。”有网友评价并晒出了下面的走势图。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/183abba71be9af90f57121356a71282f.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>“泡沫……但我确信泡沫会破裂！”有网友看到OpenAI最新融资消息后说道。也有网友表示，“对我来说，在看到真正的进展之前，这都是 HypeAI。o1-preview对我来说是真正的进展吗？不是。也许到明年他们会有所进步，我会认为那是真正的进步，但目前我还没看到。这是 OpenAI 和一些用户的炒作。”</p><p>&nbsp;</p><p>“我几乎可以肯定其他公司也在研发与草莓相当的产品。”有网友提出，对此有网友表示，“没关系，现在每个人都向 OpenAI 投入资金的原因有两个：一是他们正在放弃非营利的心态；二是他们拥有显著的领先优势，他们所建造的东西的价值很快就会超过地球上几乎所有的东西。”</p><p>&nbsp;</p><p>也有网友指出，投资 OpenAI 的大公司也在投资 Anthropic 和其他一批人工智能公司。但“OpenAI 已经领先并将继续领先。它需要的只是更多的资金来巩固其领先地位，现在它已经拥有了实现这一目标的资本。”</p><p>&nbsp;</p><p></p><h2>大额融资背后是运营压力</h2><p></p><p>&nbsp;</p><p>超强融资背后是OpenAI不得不面临的运营压力。The information 此前还预估，OpenAI 今年亏损将达 50 亿美元，即将破产。</p><p>&nbsp;</p><p>OpenAI 的成本主要分成推理成本、训练成本和人工成本三大块，总运营成本预计将达到惊人的85亿美元。</p><p>&nbsp;</p><p>具体看，推理成本方面，截至今年3月，OpenAI已斥资近40亿美元用于租用微软的服务器资源，全力支撑ChatGPT及其底层大模型的运行。训练成本（包括支付数据使用费用）方面也呈现显著增长态势，预计今年将攀升至约30亿美元。据悉，该公司去年以超乎预期的速度推进新人工智能模型的培训，导致原本规划的8亿美元预算大幅超出，预计今年的训练成本将会翻倍，因为OpenAI在持续迭代旗舰大模型，并训练新一代模型。</p><p>&nbsp;</p><p>人力成本方面，OpenAI的扩张同样迅猛，7月份被曝有大约1500名员工，且团队规模仍在不断壮大。为吸引并留住顶尖技术人才，OpenAI在薪酬及福利上的投入不容小觑，今年预计人力成本开支将达到15亿美元。</p><p>&nbsp;</p><p>此前，一张美国科技大厂的薪酬表曝出，OpenAI以500万起薪领先于谷歌等一众大厂。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab6f4e6d7e23ed32ceeff1c29a0d87ca.png" /></p><p></p><p>&nbsp;</p><p>值得一提的是，Anthropic、特斯拉、OpenAI、Google Brain 和亚马逊等公司提供了最高的初始报价（并愿意协商更高的最终交易），但提高报价最多的公司是 Google Research、Microsoft Research、Bloomberg AI、IBM Research 和 TikTok。从中可以得出的结论是，即使你的初始报价很低，你也总有空间利用你作为高技能研究人员的优势来获得更好的报价。</p><p>&nbsp;</p><p>此前还有知情人士透露，OpenAI曾预计其2023年的劳动力成本为5亿美元。而到当年年底，其员工人数增加了一倍，达到800人左右。从那以后，该公司的员工人数再次增加了近一倍。加之其官网公布的近200个空缺职位，预示着2024年下半年将迎来更多员工。实际上，OpenAI目前确实还在招人。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b8c8c8e36b5be0de407f2a41592762f.png" /></p><p></p><p>&nbsp;</p><p>总之，上述运营成本并未通过大约 35 亿美元的收入来满足。值得注意的是，该公司已经进行了七轮融资，筹集了超过 110 亿美元。据悉，OpenAI 运行接近满负荷，其 350,000 台服务器中有 290,000 台专用于支持 ChatGPT。</p><p>&nbsp;</p><p>另外，有消息称，OpenAI 可能会在 9 月 24 日进一步推广 ChatGPT 的高级语音模式，甚至可能是直接正式发布。有网友在OpenAI 更新的高级语音模式 FAQ 页面的代码中发现“hasSeenAdvancedVoice/2024-09-24”的字符串，暗示会邀请部分移动用户体验高级语音模式。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9681de72c35387cdec78a093ca60ff38.png" /></p><p></p><p>&nbsp;</p><p>OpenAI能否在激烈的竞争中保持自己的技术优势和创新能力，我们拭目以待。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://www.bloomberg.com/news/articles/2024-09-19/openai-to-decide-which-backers-to-let-into-6-5-billion-funding">https://www.bloomberg.com/news/articles/2024-09-19/openai-to-decide-which-backers-to-let-into-6-5-billion-funding</a>"</p><p><a href="https://www.teamrora.com/post/ai-researchers-salary-negotiation-report-2023">https://www.teamrora.com/post/ai-researchers-salary-negotiation-report-2023</a>"</p><p><a href="https://www.ft.com/content/841f6e58-b1bb-4c8e-bce0-a4c0b46ee2f8">https://www.ft.com/content/841f6e58-b1bb-4c8e-bce0-a4c0b46ee2f8</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bJM8KS3be5cFMhgJbjqP</id>
            <title>高通洽购英特尔，英特尔中国回应；IBM秘密裁员数千人，年龄在50-55岁；《黑神话：悟空》收入超67亿创纪录 | AI周报</title>
            <link>https://www.infoq.cn/article/bJM8KS3be5cFMhgJbjqP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bJM8KS3be5cFMhgJbjqP</guid>
            <pubDate></pubDate>
            <updated>Mon, 23 Sep 2024 01:04:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><h2>行业热点</h2><p></p><p>&nbsp;</p><p></p><h4>曝IBM针对云服务部门秘密裁员，涉及数千名员工，年龄集中在50-55岁之间</h4><p></p><p>&nbsp;</p><p>9月19日消息，据外媒当地时间本周三报道，一位IBM员工透露，IBM云服务部门IBM cloud过去数日经历了一次大规模裁员，影响数千名员工。此外本次裁员还是秘密进行的：受裁员工必须签署NDA，不得对外谈论裁员具体细节。</p><p>&nbsp;</p><p>具体而言，本次秘密裁员主要针对高级的程序、销售和支持人员，受影响的员工大多在50~55岁年龄段，工龄位于20~24年之间，职级集中在L7、L8、L9，裁员前拥有相当高的薪资收入。同时，爆料的员工称，其工作岗位将转往印度，这反映了IBM美国的招聘将冻结。但据说IBM在印度的招聘工作正在进行中。此前IBM 也有将工作岗位转移到印度的历史。</p><p>&nbsp;</p><p>IBM发言人表示，这家科技巨头今年早些时候披露了一笔劳动力再平衡费用，这笔资金将影响IBM整体员工团队中的“非常低个位数百分比”，IBM仍然预计其2024年末的员工总数将同年初大致相当。</p><p>&nbsp;</p><p>IBM 本财年一季度的财报中提到了一笔4亿美元（当前约 28.36 亿元人民币）的裁员赔偿资金，按IBM在2023年以3亿美元的开支裁员3900人估算，本年度IBM将有约5200名员工被裁，相当于整体员工团队的1.8%。</p><p>&nbsp;</p><p></p><h4>大连思科裁员正式落地，思科今年第二轮裁员占7%，补偿方案N+7</h4><p></p><p>&nbsp;</p><p>美国科技巨头公司思科在2024年进行了今年第二轮大规模裁员，涉及约5600名员工，占其员工总数的7%。此前，思科在2024年2月已经裁减了4000名员工。</p><p>&nbsp;</p><p>据最新消息，高层给员工开会，大连思科年内第二波裁员策略已经正式落地。据知情人透露，上午已经收到内部邮件，日韩业务要撤出中国，思科大连蓝牌（正编）员工开始裁员。赔偿方案两种选择：1.N+7直接走人，2.N+5，延迟两个月。</p><p>&nbsp;</p><p>此次裁员不仅涉及常规运营部门，还波及了思科旗下的Talos Security团队。Talos是思科的情报与安全研究部门，负责保护全球客户免受网络攻击。然而，即便这一关键部门也难以避免要裁员。更让人不解的是，8月份思科宣布了新一轮裁员计划，一直拖到9月16日才通知受影响的员工，期间未给出明确解释。据员工透露，在此期间公司内部的工作氛围变得异常奇怪，员工普遍感到焦虑不安和不满。</p><p>&nbsp;</p><p>思科称，裁员是为了“投资关键增长机会，并提升运营效率”。该声明与其全年财报的发布时间一致。尽管进行大规模裁员，思科在2024财年的年收入仍接近540亿美元，是公司历史上表现第二好的财年。</p><p>&nbsp;</p><p></p><h4>英特尔 CEO宣布40年来最重要转型：年底前裁员15000人、将抛掉2/3的房地产</h4><p></p><p>&nbsp;</p><p>9月17日消息，英特尔 CEO Pat Gelsinger 发布全员信，宣布了公司下一段的战略目标，以及多项降本增效计划。重点包括“继续采取紧急行动，建立更具竞争力的成本结构，实现我们上个月宣布的100亿美元的节约目标”等三个方面。基辛格还在信中提到，董事会计划将英特尔代工厂设立为英特尔内部的独立子公司，以最大限度地实现增长和股东价值创造。</p><p>&nbsp;</p><p>另外，基辛格透露，通过自愿提前退休和离职，英特尔已经完成今年年底裁员约15000人目标的一半以上（7500人）。“我们仍需做出艰难的决定，并将于10月中旬通知受影响的员工。此外，我们正在实施计划，在年底前减少或退出全球约三分之二的房地产。”基辛格写道。他强调：“这是英特尔四十多年来最重要的转型。自内存向微处理器转型以来，我们从未尝试过如此重要的事情。当时我们成功了——我们将迎接这一时刻，在未来几十年打造更强大英特尔。”</p><p>&nbsp;</p><p></p><h4>亚马逊CEO官宣：恢复一周5天办公室上班，管理层开始裁员</h4><p></p><p>&nbsp;</p><p>亚马逊CEO Andy Jassy官宣，从现在到明年1月2日，亚马逊要求员工逐步恢复到疫情前一周5天上班的工作状态。据了解，此前亚马逊已经要求员工一周至少有三天回到办公室上班，现在亚马逊打算从3天提高到5天，完全恢复成疫情之前的状态了。此外，Andy Jassy还要求亚马逊的组织扁平化，并且要求IC到manager的比例至少达到15%。然而，扁平化管理势必会导致组织合并，其中一部分中层经理失去位置，这也被认为是亚马逊在进行管理层裁员。</p><p>&nbsp;</p><p></p><h4>《黑神话：悟空》收入超67亿，国产3A游戏新纪录诞生！</h4><p></p><p>&nbsp;</p><p>《黑神话：悟空》自8月20日清晨10点正式解锁以来，犹如齐天大圣孙悟空般腾空而起，迅速在Steam平台上掀起热潮，其影响力与关注度在全球范围内持续攀升，成为游戏界热议的焦点。</p><p>&nbsp;</p><p>时至今日（9月20日），这款国产3A游戏上线满月之际，据国外知名数据分析公司VG Insights最新披露，《黑神话：悟空》在Steam上的销量已惊人地突破2000万大关，总收入更是跃升至9.61亿美元（相当于人民币超67.9亿元），标志着国产3A游戏领域新里程碑的诞生。</p><p>&nbsp;</p><p></p><h4>高通洽购英特尔？英特尔中国：对于传言，不予置评</h4><p></p><p>&nbsp;</p><p>9月21日电，芯片巨头高通被曝正在洽购芯片代工厂商英特尔。9月21日，有外媒报道称，高通已就收购事宜接洽英特尔。该报道援引知情人士消息称，交易还远未确定。高通并未正式向英特尔提出收购要约，且达成交易的障碍仍很大。但如果交易能够顺利完成，这将是近年来规模最大、深刻影响市场的交易之一。</p><p>&nbsp;</p><p>对此，英特尔中国相关人士向媒体表示，对于传言，我们不予置评。另外有消息称，博通目前没有在评估向英特尔发出收购要约，该公司曾评估过是否寻求交易，顾问在继续向博通提出建议。</p><p>&nbsp;</p><p></p><h4>前苹果设计总监 Jony Ive 确认正与 OpenAI 开发一款新设备，iPhone 元老级人物加盟</h4><p></p><p>&nbsp;9 月 22 日消息，今年 4 月曾有消息称，OpenAI 首席执行官山姆・阿尔特曼（Sam Altman）携手前苹果设计总监乔纳森・伊夫（Jony Ive），联合设计面向个人的 AI 硬件，目前正寻求外部投资。</p><p>&nbsp;</p><p>对此，伊夫本人在纽约时报 9 月 21 日的一篇文章中证实了这一点。报道称，伊夫是通过 Airbnb 的首席执行官 Brian Chesky 认识阿尔特曼的，该项目则由伊夫和劳伦娜・鲍威尔・乔布斯（乔布斯遗孀）的公司 Emerson Collective 资助。</p><p>&nbsp;</p><p>报道提到，到今年年底，该新公司可能会筹集 10 亿美元（当前约 70.55 亿元人民币）的资金，但报道没有提到软银首席执行官孙正义，去年曾有传言称孙正义向该项目投资 10 亿美元。</p><p>&nbsp;</p><p>该项目目前只有 10 名员工，但其中包括 Tang Tan 和 Evans Hankey，他们是与伊夫一起开发 iPhone 的两个关键人物。至于产品本身是什么，去年有传言说它的灵感来自触摸屏技术和初代 iPhone，不过这一消息暂未证实。</p><p>&nbsp;</p><p>部分厂商称已无法下单英伟达H20芯片</p><p>&nbsp;</p><p>9月20日消息，部分厂商已无法下单英伟达H20芯片。一位产业链人士表示，“英伟达上月开始不接H20订单，但没有明文通知。”另一AI厂商人士亦表示，“近期确实存在英伟达不接部分厂商H20订单的情况。”包括互联网厂商、大模型厂商、芯片供应商在内多位产业链人士表示，一直有听到H20将停售的消息，但英伟达方面还在争取。不</p><p>&nbsp;</p><p>过亦有多家厂商反馈，“近期仍有H20大批到货，年内到货已超出了全年约40万颗的出货预期。”</p><p>&nbsp;</p><p></p><h4>字节跳动回应与台积电合作AI芯片：报道不实</h4><p></p><p>&nbsp;</p><p>针对9月18日媒体报道的字节跳动计划与台积电就AI芯片开展合作，字节方面回应称，报道不实，字节跳动在芯片领域确实有一些探索，但还处于初期阶段，主要是围绕推荐、广告等业务的成本优化，所有项目也完全符合相关的贸易管制规定。</p><p>&nbsp;</p><p>此前，据外媒报道，两位直接知情人士透露，字节跳动计划与台积电合作，在 2026 年前量产其自主设计的两款半导体，预计字节跳动将预定数十万枚芯片的产量。知情人士表示，生产这些芯片可以减少字节跳动对价格高昂的英伟达芯片的依赖，从而开发和运行 AI 模型。字节跳动正在加快步伐制造自己的 AI 芯片，以期在中国 AI 聊天机器人市场上领先于竞争对手。</p><p>&nbsp;</p><p></p><h4>奥特曼离开OpenAI安全委员会，OpenAI将转型为更传统的盈利性公司</h4><p></p><p>&nbsp;</p><p>9月17日，据外媒消息报道，OpenAI 首席执行官 Sam Altman 将离开 OpenAI 于 5 月成立的内部委员会，该委员会负责监督与公司项目和运营相关的关键安全决策。</p><p>&nbsp;</p><p>OpenAI在博文中表示，安全与保障委员会将成为由卡内基梅隆大学教授Zico Kolter担任主席的独立董事会监督小组，成员包括 Quora 首席执行官 Adam D'Angelo、美国退役陆军将军 Paul Nakasone 和前索尼执行副总裁 Nicole Seligman，他们都是 OpenAI 董事会的现任成员。OpenAI 在帖子中指出，委员会对OpenAI 的最新 AI 模型o1进行了安全审查——尽管 Altman 仍参与其中。该公司表示，该小组将继续定期接受 OpenAI 安全团队的简报，并保留推迟发布的权力，直到安全问题得到解决。</p><p>&nbsp;</p><p>OpenAI 在帖子中写道：作为其工作的一部分，安全委员会将继续定期收到有关当前和未来模型的技术评估报告，以及持续发布后监测报告。他们正在基于模型发布流程和实践，建立一个综合的安全框架，明确定义模型发布的成功标准。</p><p>&nbsp;</p><p>此外，Sam Altman 在最近的公司周会上宣布，OpenAI 将于明年调整其复杂的非营利性企业结构，转型为更传统的营利性公司。尽管如此，OpenAI 仍将保留一个非营利部门，并继续致力于构建造福所有人的 AI。OpenAI 成立于2015年，主要依赖捐赠者的资金，但多年来仅筹集了1.305亿美元，难以满足其核心研究所需的计算能力和人才成本。Altman 在周会上未透露更多细节，但强调非营利性仍是公司使命的核心，并将持续存在。</p><p>&nbsp;</p><p></p><h4>小米三折叠屏手机专利曝光：华为同款Z字形方案，2022年就布局</h4><p></p><p>&nbsp;</p><p>9月18日消息，根据国家知识产权局9月3日公示的清单，小米公司获得了一项“手机及其主体”的外观设计专利，展示了小米三折叠手机设计。查询专利报告页面，显示小米于2022年12月21日提交了该设计专利，展示了2种设计方案，但整体都是与华为一致的Z字形折叠方式，并分享了多张设计草图。根据显示的专利草图，小米三折叠手机背面采用水平放置的摄像头方案，水平放置了3个摄像头，并配有一个LED闪光灯。</p><p>&nbsp;</p><p>不过，这款机型有可能并不存在，只是小米在进行三折的相关技术研发和专利储备。另外值得注意的是，其实早在2019年年初，小米联合创始人林斌就曾公布了小米首款双折叠智能手机，其实就与如今的“三折叠”概念异曲同工。只不过当时更多的是在进行折叠屏技术储备，机器被晒出来多是以炫技为目的，折叠方式比较鸡肋，很难正常使用。当时林斌介绍，小米双折叠手机攻克了柔性折叠屏技术、四驱折叠转轴技术、柔性盖板技术以及MIUI适配等一系列技术难题，这应该是全球第一台双折叠手机。</p><p>&nbsp;</p><p></p><h4>苹果市值蒸发千亿：iPhone 16 首周销量下降 12.7%</h4><p></p><p>&nbsp;</p><p>天风国际分析师郭明錤表示，iPhone16 系列在首周末预购销量预估约 3700 万部，较去年 iPhone15 系列首周末销量同比减少约 12.7%，关键在于 iPhone16Pro 系列低于预期。郭明錤指出，iPhone16Pro 系列出货时间显著低于 15Pro 系列，除预购前备货量增加外，从首周末销量同比减少来看，关键还是在于需求低于预期。</p><p>&nbsp;</p><p>最近一周内苹果市值蒸发 690 亿美元（约合 4900 亿元人民币）。在一些国内商家和用户看来，创新在苹果手机上越来越弱，而今年表现的尤为明显，特别是同天华为三折叠推出。据报道，华为三折叠手机 10 天预约量达 625.9 万，而 9 月 20 日其也将迎来开卖，据说备货有 100 万台。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>微软、贝莱德等宣布成立超 300 亿美元 AI 基础设施投资基金，英伟达提供专业知识支持</h4><p></p><p>&nbsp;</p><p>9 月 18 日消息，微软联合多方成立了一只名为“全球 AI 基础设施投资伙伴关系”的基金，旨在投资 AI 基础设施，以建设数据中心和能源项目。</p><p>&nbsp;</p><p>微软宣布联合贝莱德（BlackRock）、全球基础设施合作伙伴（GIP）及阿联酋 AI 投资公司 MGX 成立全球 AI 基础设施投资伙伴关系（GAIIP），以投资新兴及扩建中的数据中心，满足对不断增长的算力需求，并投资能源基础设施，为这些设施提供新的能源来源。英伟达还将为 GAIIP 提供支持，包括在 AI 数据中心和 AI 工厂方面的专业知识，以惠及 AI 生态系统。</p><p>&nbsp;</p><p>据称，这些基础设施投资主要集中在美国，推动 AI 创新和经济增长，其余部分将投资于美国的合作伙伴国家。微软透露，该合作伙伴关系最初将寻求逐步释放 300 亿美元的私募股权资本，来自投资者、资产所有者和企业，加上债务融资，最终有望撬动总计 1000 亿美元（当前约 7094.48 亿元人民币）的投资潜力。</p><p>&nbsp;</p><p></p><h4>美团王兴：去年获得收入的骑手约 745 万，报酬 800 亿元</h4><p></p><p>&nbsp;</p><p>9月18日消息，中秋假期期间，美团 CEO 王兴对内发布全员信，信中提到，过去 3 年每年招聘超过 5000 名应届毕业生，2025 届计划招募 6000 名，同时，内部提拔比例高达 69%。去年在美团平台获得收入的骑手约 745 万，获报酬超过 800 亿。</p><p>&nbsp;</p><p></p><h2>大模型一周大事</h2><p></p><p>&nbsp;</p><p></p><h3>大模型发布</h3><p></p><p>&nbsp;</p><p></p><h4>阿里云宣布开源 Qwen2.5，上架超 100 个模型</h4><p></p><p>&nbsp;</p><p>9 月 19 日举办的 2024 云栖大会上，阿里云 CTO 周靖人发布通义千问新一代开源模型 Qwen2.5。据悉，Qwen2.5 全系列涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型，每个尺寸都有基础版本、指令跟随版本、量化版本，总计上架 100 多个模型，其中旗舰模型 Qwen2.5-72B 性能超越 Llama 405B。</p><p>&nbsp;</p><p>相比 Qwen2，Qwen2.5 全系列模型都在 18T tokens 数据上进行预训练，整体性能提升 18% 以上，拥有更多的知识、更强的编程和数学能力。此外，在多模态模型方面，阿里云还宣布了视觉语言模型 Qwen2-VL-72B 开源，Qwen2-VL 能识别不同分辨率和长宽比的图片，理解 20 分钟以上长视频，具备自主操作手机和机器人的视觉智能体能力。</p><p>&nbsp;</p><p></p><h4>阿里国际发布最新版多模态大模型Ovis</h4><p></p><p>&nbsp;</p><p>9月19日消息，阿里国际AI团队宣布发布多模态大模型Ovis。据介绍，Ovis能够在数学推理问答、物体识别、文本提取和复杂任务决策等方面展现出色表现。例如，Ovis可以准确回答数学问题，识别花的品种，支持多种语言的文本提取，甚至可以识别手写字体和复杂的数学公式。Ovis 1.0、1.5的数据、模型、训练和推理代码都已全部开源，可复现。Ovis1.6系列中的Ovis1.6-Gemma2-9B也已开源权重。</p><p>&nbsp;</p><p></p><h4>巨人网络发布两款“游戏+AI”自研大模型应用</h4><p></p><p>&nbsp;</p><p>9月19日，巨人网络携多项“游戏+AI”新成果首次参展云栖大会，两款自研大模型GiantGPT、BaiLing-TTS应用首发，巨人摹境、AI数字人等AI新技术亮相。据介绍，GiantGPT是专注于游戏业务的垂类大模型，结合高质量自有数据与互联网公共数据训练，并针对角色演绎、情景推理、长期记忆等基础能力进行深度优化。BaiLing-TTS则是行业内首个支持多种普通话方言混说的语音大模型。</p><p>&nbsp;</p><p></p><h4>字节跳动豆包大模型 9 月 24 日发布视频生成模型</h4><p></p><p>&nbsp;</p><p>字节跳动宣布，豆包大模型将于 9 月 24 日发布视频生成模型，并带来更多模型家族的能力升级。9 月 24 日 14:30，2024 火山引擎 AI 创新巡展・深圳站将于深圳举行，字节跳动将在大会上公布火山引擎整体介绍及豆包大模型进展，还有火山引擎 AI 产品最新进展和多个行业企业 AI 落地实践分享。</p><p>&nbsp;</p><p></p><h4>腾讯发布DepthCrafter，提升开放世界视频的画面质量</h4><p></p><p>&nbsp;</p><p>9月18日消息，腾讯与香港科技大学联合开发的DepthCrafter模型，是首个专为生成开放世界视频游戏而设计的扩散变换器模型。通过模拟广泛的游戏引擎特性，如创新角色、动态环境、复杂动作和多样事件，该模型实现了高质量、开放世界的生成。这此外，它还提供了交互式可控性，从而允许游戏玩法模拟。</p><p>&nbsp;</p><p></p><h4>快手可灵 AI 面向全球发布 1.5 模型</h4><p></p><p>&nbsp;</p><p>可灵 AI&nbsp;全球升级发布，新增可灵 1.5 模型和“运动笔刷”功能，提升视频生成质量与控制能力。自 6 月发布以来，已进行 9 次迭代，超过 260 万人使用，生成视频超 2700 万部、图片 5300 万张。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h3>企业应用</h3><p></p><p>&nbsp;</p><p>9 月 19 日，“微信派”公众号宣布，微信朋友圈发布实况照片功能正式推出。用户打开朋友圈-从手机相册选择，即可选中并发布实况照片，照片将包含动态画面和声音。发布时，也可以点击关闭实况效果，支持静态照片与实况照片混合发布。待内容发布后，朋友圈图片将出现实况圈圈效果。目前，功能正在 iOS 8.0.51 及以上版本逐步覆盖中，用户更新完并被覆盖后即可使用。Android 手机目前还不支持，微信派官方对此回应称：“再等等”。9 月 18 日，谷歌宣布为 YouTube 带来一系列 AI 相关功能，有望改变视频制作的方式乃至视频本身。YouTube 正在引入一系列 AI 工具，以帮助创作者更高效地制作视频内容。其中包括一个名为“灵感”的新选项卡，这个 AI 驱动的功能可以为创作者提供视频概念、推荐标题、缩略图，甚至编写视频大纲。此外，YouTube 还推出了 Veo，这是一个集成了谷歌 DeepMind 视频模型的工具，能够生成视频背景和最长 6 秒的完整视频片段。9 月 18 日，谷歌正计划推出一项技术，用于识别照片是用相机拍摄的，还是用Photoshop等软件编辑的，抑或是由人工智能生成模型生成的。谷歌正在使用的系统是内容出处和真实性联盟（C2PA）的一部分，该联盟是试图解决人工智能生成图像问题的最大团体之一。9 月 17 日，特斯拉似乎正在稳步推进其在中国推出的全自动驾驶 (FSD) 服务的计划。最近，已经有部分在国内生产销售的特斯拉汽车更新了一个按钮，以启用 FSD。同时，该按钮注明了是在“驾驶员监督下：的测试版，并且目前还不能启用。同时，根据截图上的信息显示，FSD 需要更新导航地图才能启用，此前有消息称，特斯拉正在和百度就相关地图数据事宜进行沟通。而Elon Musk在特斯拉 2024 年第二季度财报电话会议上表示，他预计 FSD 将在今年年底前在中国获得批准。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</id>
            <title>阿里云发布首个AI多模数据管理平台DMS，助力业务决策提效10倍</title>
            <link>https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kcp5IFmv7B8gYqBJxluZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，2024云栖大会上，阿里云瑶池数据库宣布重磅升级，发布首个一站式多模数据管理平台DMS：OneMeta+OneOps。该平台由Data+AI驱动，兼容40余种数据源，实现跨云数据库、数据仓库、数据湖的统一数据治理，帮助用户敏捷、高效地提取并分析元数据，业务决策效率可提升10倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/32/32e0c3fcf153c23a1a30e0f8baf036e7.jpeg" /></p><p></p><p>阿里云副总裁、数据库产品事业部负责人李飞飞</p><p></p><p>“数据是生成式AI的核心资产，大模型时代的数据管理系统需具备多模处理和实时分析能力，以数据驱动决策和创新，为用户提供‘搭积木’一样易用、好用、高可用的使用体验。”阿里云副总裁、数据库产品事业部负责人李飞飞表示。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ab/ab91507c148beebff7e17bddc742ebc3.png" /></p><p></p><p>图：阿里云推出多模数据管理平台DMS：OneMeta+OneOps</p><p></p><p>当前，近80%的企业在建设数据平台时采用多种数据引擎、多数据实例组合的策略，AI兴起也带来了非结构化数据的指数级增长，给企业对数据的高效检索和分析管理提出了更大挑战。此次，阿里云重磅推出由“Data+AI”驱动的多模数据管理平台DMS：OneMeta+OneOps，助力构建企业智能Data Mesh（数据网格），提升跨环境、跨引擎、跨实例的统一元数据管理能力。</p><p></p><p>DMS创新设计了统一、开放、跨云的元数据服务OneMeta及DMS+X的多模联动模式OneOps。OneMeta首次打通不同数据系统，可支持全域40余种不同数据源，提供数据血缘和数据质量的一站式数据治理。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad49f198fa6c7ec433e77aca75ca93a0.png" /></p><p></p><p></p><p>OneOps则基于数据开发平台DataOps和AI数据平台MLOps，将不同数据库引擎（关系型数据库、数据仓库、多模数据库等）集结到统一平台，让用户“开箱即用”，实现全链路的数据加工和计算能力。</p><p></p><p>自上线以来，DMS已服务超过10万企业客户。借助跨引擎、跨实例管理和开发以及数据智能一体化，DMS将帮助企业从分散式数据治理升级至开放统一数据智能管理，可降低高达90%的数据管理成本，业务决策效率提升10倍。</p><p></p><p>李飞飞表示：“这是自云原生数据库2.0后，阿里云瑶池数据库又一次里程碑式的改造升级。DMS：OneMeta+OneOps为企业提供了全域数据资产管理能力，让业务数据‘看得清、查得快、用得好’。”</p><p></p><p>据介绍，极氪汽车采用DMS+Lindorm一站式多模数据解决方案，实现32万在线车辆上万车机信号数据的弹性处理分析，开发效能提升2倍，降低50%云资源成本。在大模型领域，此方案支撑月之暗面构建AI智能助手Kimi，帮助Kimi准确理解用户的搜索意图、整合与概述多种信息源，实现精准和全面的信息召回，提升用户交互体验。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea542c860a7b87c2dfe54757dd6ae8b8.png" /></p><p></p><p></p><p>此外，云原生数据库PolarDB今年首次提出基于“三层解耦, 三层池化”（存储、内存、计算）、AlwaysOn架构的多主多写和秒级Serverless能力，解决了多主架构中冲突处理和数据融合、以及Serverless秒级弹性租户隔离的难题。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e95f675bc1decd38d160c65fbe921c6e.png" /></p><p></p><p>本次云栖大会，阿里云瑶池还正式发布了云原生内存数据库Tair Serverless KV服务，是阿里云首个基于NVIDIA TensorRT-LLM的推理缓存加速云数据库产品。Tair采用NVIDIA TensorRT-LLM一起进行了深度优化。相比开源方案，该服务可实现PD分离/调度优化吞吐30%的提升 ，预计成本可降低 20%*注。</p><p></p><p>*注：基于Qwen2 7B模型在长上下文场景构造实验环境数据测试，最终效果以实际产品和场景测试数据为准。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</id>
            <title>JetBrains 与阿里云合作推出 AI Assistant，聚焦中国市场开发者</title>
            <link>https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HOKcm1CVYKUq1aOIwFqb</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:54:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9 月 19 日，在 2024 云栖大会上，全球软件开发工具提供商 JetBrains 发布基于阿里云通义大模型的 JetBrains AI Assistant，在完善其开发工具产品生态方面迈出了重要一步。</p><p></p><p>JetBrains《2023 开发者生态系统现状报告》调查结果显示，79% 的开发者认为处理代码是其工作中最耗时的环节。而 60% 的开发者已经开始使用、熟悉 AI 代码生成工具，用 AI 协助完成理解代码、检测并修正代码等繁琐的工作，让开发者专注于核心的编码任务，显著提升开发效率。</p><p></p><p>据介绍，JetBrains AI Assistant 与多款 JetBrains 产品深度集成，能够以高度的适配性完成代码生成与重构、回答和解释代码相关问题、撰写文档和提交信息等工作，助力中国本土开发者提升效率和代码质量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b988354041f18d4f4fabd8048961425c.png" /></p><p></p><p>并且，JetBrains AI Assistant 充分考虑了开发者的使用习惯，将 AI 辅助融入开发者的工作流中，同时实现了与 IDE 的无缝集成，将 AI 功能深刻融入对代码和上下文的理解，全面增强开发环境。在实际操作中，JetBrains AI Assistant 能够提供详尽的上下文信息和代码解释，帮助用户清晰理解 AI 的决策逻辑，从而显著提升操作的透明度和可追溯性。</p><p></p><p>在与阿里云的联合下，为中国本土用户量身打造的 AI Assistant 深度融合了中文自然语言处理技术，实现了中文指令与系统的直接交互，显著降低了使用门槛并提高了使用者的工作效率。此外，运用本土数据进行模型训练，能够在优化模型性能的同时，大幅降低训练成本。基于本土语言大模型，确保了数据处理的合规性和安全性，提供更加安全信赖、持续可靠的服务。</p><p></p><p>此外，JetBrains AI Assistant 在设计之初就将用户数据隐私与安全置于核心位置，不断深化安全性研究，严格遵守数据保护政策，未经授权不收集或泄露任何敏感信息，确保用户信息的安全。</p><p></p><p>大会上，JetBrains 除了带来全新首发的 JetBrains AI Assistant，还在展台上准备了丰富的互动和内容。展台特设“码脑讲堂”，邀请技术专家、KOL 通过直观演示和技术分享，解读 AI 在软件开发领域的实际应用与未来蓝图；解决方案展示区呈现了游戏开发、跨平台开发和服务器端开发的最新解决方案；趣味互动墙区域则通过创新装置提供沉浸式互动体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/ea2fd5b299d0919a7b716486644380a4.png" /></p><p></p><p>JetBrains 中国区总裁李玥萱表示，“中国市场在 JetBrains 的全球版图中占据举足轻重的地位。JetBrains 积极拥抱市场变化，确保我们的产品和服务始终与开发者的需求同频共振，助力他们突破效率极限。未来，我们期待与阿里云继续深化战略合作，不断探索和创新，为中国市场的用户提供更加强大、更加智能的工具和解决方案。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd91385fcb5521cfb12ca761250facf3.png" /></p><p>图：JetBrains 中国区总裁李玥萱</p><p></p><p>阿里云智能集团资深副总裁、公共云事业部总裁刘伟光表示：“今天，我们见证了云和 AI 的碰撞带来的一波全新浪潮。我们非常坚信，AI 的应用会席卷和渗透到各行各业。阿里云和 JetBrains 的战略合作发挥了衔接国际技术与中国市场的独特桥梁价值，让更多前沿的创新功能落地本土，我们愿与 JetBrains 共同开启 AI 在开发领域落地的新篇章。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/101abc01ca887189e95cbc99445abc2e.png" /></p><p>图：阿里云智能集团资深副总裁、公共云事业部总裁刘伟光</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</id>
            <title>阿里云首次推出云原生NDR产品 提升全流量威胁防御能力</title>
            <link>https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lrhWq2us1bD3Vvwdeea5</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:22:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，在杭州云栖大会上，阿里云宣布云原生安全能力全线升级，首次发布云原生网络检测与响应产品NDR（Network Detection Response，简称NDR）。同时，阿里云还宣布将持续增加免费的安全防护能力，帮助中小企业客户以极低投入完成基础的云上安全风险治理。</p><p></p><p>云时代复杂的IT体系、碎片化的安全工具和传统的防护思路，以及新技术和新威胁带来的多重变化，让安全运营难以应对挑战。阿里云安全产品负责人欧阳欣表示，阿里云基于多年经验，创新性提出“三体”安全建设思路，将基础设施安全一体化、安全技术域一体化、以及办公安全和生产安全一体化贯彻到安全运营中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/479adc185863338978a64cc3ca88438a.jpeg" /></p><p></p><p>此次推出的阿里云云原生NDR，即是在此背景下的创新。NDR是基于公共云环境原生化部署的威胁检测与响应产品，全面提升了云环境全流量防御能力。与传统第三方产品不同在于，它无需部署即可即时开通，并通过创新的自动留存技术，可以针对攻击事件及攻击发生前后5分钟的流量进行取证保存，兼顾留存需要与成本投入，进而进行溯源和关联分析，帮助客户更快发现高级网络威胁。</p><p></p><p><img src="https://static001.geekbang.org/infoq/36/36b6336f76c2c28a67e8fb32a142b407.jpeg" /></p><p></p><p>基于基础设施安全一体化，阿里云还加强了WAAP、云安全中心、DDoS防护等能力，并且对数据库、网络CDN、计算、存储等云原生产品的安全能力也进行全新升级。</p><p></p><p>比如数据库与安全产品在数据安全上进行全面融合与能力共建，发布列加密与原生审计技术，可一键开通，增强自动化的安全能力。在CDN安全方面，阿里云将安全功能融入边缘网络，实现一键开启DDoS防护、WAF、Bot管理、API安全、SSL证书等功能，通过全球3200+节点提供原生安全能力，为用户提供边缘云网安全防护体验。</p><p></p><p>目前，阿里云已经成为Forrester、Gartner、IDC三大国际权威机构认可的全球安全能力最完整的厂商之一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/28/28a3eb882c0ea7b69192ba6d9c1d2cf0.jpeg" /></p><p></p><p>欧阳欣表示，“在做好平台安全建设同时，阿里云也免费开放更多的安全能力额度，包括云安全中心、内容安全、数据安全中心，让中小企业客户能够增强安全防护，同时还在安全体验上增加一键检测、一键修复等功能，帮助客户共同加入到云上安全维护中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebdb1fcbdda73c7a113745bf83a2b8e3.jpeg" /></p><p></p><p>面向AI，阿里云全新升级了安全体系，通义大模型基于阿里云的安全基座建设了生成式人工智能安全保障的最佳实践，将内容安全能力覆盖到大模型全生命周期中。同时，阿里云安全为百炼平台的专属部署模式设计了VPC安全保障方案，让客户在私域环境中也能获得数据确权归属等系列安全服务。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JhIxfejXZ1JK5uAz1RYE</id>
            <title>《阿里云安全白皮书2024版》发布：国内首推“安全共同体”理念</title>
            <link>https://www.infoq.cn/article/JhIxfejXZ1JK5uAz1RYE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JhIxfejXZ1JK5uAz1RYE</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:10:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月20日，在杭州云栖大会上，阿里云发布2024版《阿里云安全白皮书》，正式将公共云安全责任共担的思路，升级为“云上安全共同体”理念，这意味着阿里云不仅会坚守安全责任共担模式下云服务商的责任，搭建和提供“安全的云”， 更会进一步与客户紧密合作，提供更多可供客户采取的安全保障措施，与云上客户共同形成一个紧密相连、 互相支持的安全防护网络，进一步造就云平台的运行安全。</p><p></p><p>阿里巴巴集团安全部总裁钱磊表示，阿里云除了为客户提供安全的云服务，还需要思考如何帮助客户“安全地使用云”，即利用更集中的技术投入、丰富的风险应对经验、完善的安全组织配备、先进的安全治理思路，打通客户安全体验的最后一公里，将平台的安全能力转化为客户侧实实在在的安全效果。“如果客户不安全，云平台就无法实现真正意义上的安全。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b5b24e54bf1e91b93ce516bfe7c56c2a.png" /></p><p></p><p>据白皮书阐释，数智时代推动企业全面上云、深度用云的比例显著提升，这也对云上的安全治理体系提出了新的挑战。云计算初期，企业关注数据迁移等与常规IT流程相异的安全管理问题，为了建立企业与云服务商之间对云安全的理解与共识，“安全责任共担模式”应运而生，旨在促进双方在安全管理上的合作与协调。</p><p></p><p>随着数智技术与社会千行百业的深度融合，整体数字系统以及企业面临的安全挑战日益增多且更为复杂，需要不断提升安全防护、巩固安全机制、强化态势感知。因此，企业和云服务商都需要更高层次的安全理念来应对新的挑战。</p><p></p><p>“云上安全共同体”的安全理念，正是强调以保护客户云上资产安全为共同目标，云平台与客户密切配合，共同应对安全挑战。在理念引导下，阿里云会继续全权负责在安全责任共担模式下，需要云服务提供者承担的安全责任。如基础设施、物理设备、分布式云操作系统及云服务产品安全，保障云平台基座的安全。客户则主要负责自身数据、应用及账户安全。与此同时，云平台也会发挥主观能动性，提供一系列切实可行的安全保障措施，帮助客户更深入地思考、制定、理解安全策略，并支持这些安全策略更顺畅、便捷地落地实施。</p><p></p><p>以云产品安全配置为例，阿里云在设计和开发云产品时会严格保证安全性，并在云服务中内嵌客户可自行配置的安全能力。客户在使用云服务时，则需要根据上云数据情况、系统的业务场景自行完成安全配置。在安全共同体理念的引导下，阿里云将在客户使用云产品之初预设更多初始的安全配置与风险提示，使客户在更安全的环境中构建业务，从而避免很多因疏忽或不当操作引发的安全风险。</p><p></p><p>本次发布的《阿里云安全白皮书2024版》全面阐释了阿里云安全保障措施建设工作，包括对于构建平台核心安全保障的八大支柱，详细描述如何建设“安全的云”，同时书中还提供了阿里云服务的行业及业务场景云上安全实践，希望帮助客户加强对云上安全治理的理解，探索符合自身场景的云上安全最佳实践。</p><p></p><p>除此之外，阿里云将围绕帮助用户“安全使用云”陆续发布数项新的安全服务和能力，如对于云产品设置更高的初始安全水位，提高云产品使用的安全性；通过提供更普惠的安全能力，促成更低的用云安全成本；通过增强多方位的安全检测和防护能力，实现安全事件的主动响应，通过提供全面而易于理解的安全指南和最佳实践，推动安全科普与安全意识的培育，使用户能够清晰地了解云服务的安全性能和潜在风险，做出明智的决策，共同为数智化趋势下的社会安全稳定运行贡献力量。</p><p></p><p>更多详情可下载《阿里云安全白皮书2024版》：</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c6466843825ee47f7cfe3bb6b9bed86.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/oWw2vtGlXIaleoKdEguV</id>
            <title>魔搭上线AIGC专区，首批上架157个风格化大模型，专业文生图全免费</title>
            <link>https://www.infoq.cn/article/oWw2vtGlXIaleoKdEguV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oWw2vtGlXIaleoKdEguV</guid>
            <pubDate></pubDate>
            <updated>Sat, 21 Sep 2024 04:06:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在杭州云栖大会上，魔搭社区宣布正式上线AIGC专区，为开发者提供从模型到应用的一站式AI创作开发平台，目前所有功能板块及GPU算力全部免费开放。</p><p></p><p>魔搭AIGC专区首批上架157款精选多模态模型，其中既有FLUX、Stable Diffusion、RealVisXL、万象熔炉等社区热门模型，也包含众多设计师贡献的黏土风、像素风、漫画风、超现实主义、线条手绘等小众风格化LoRa模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e56fa65ffec05b22e88c3b0e5043f12e.png" /></p><p></p><p>在魔搭AIGC专区，除了直接下载模型，开发者还可在线调用各类模型进行快速生图，支持AI自动翻译和优化咒语，给出正向提示词及负向提示词，还可以对采样方法、提示词引导系数、随机种子、采样步数、图片尺寸等参数进行调节。同时，魔搭AIGC生图支持图生图、局部重绘、Adetailer人脸修复、ControlNet细节调控等深度功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ae/ae8f0d4440b1914e4360b1fd6744d77d.jpeg" /></p><p></p><p>对于有更专业需求的开发者或设计师，魔搭AIGC专区支持批量上传图片对模型进行LoRa微调，目前已上线Stable Diffusion、Q版IP、动漫、写实风等多个模版，最低10张图片即可完成模型训练。用户还可以在魔搭AIGC专区在线调用ComfyUI工作流，创建或复用相关模版来进行创作。后续，魔搭AIGC专区也将上架视频、语音等更多模态的模型和应用，为开发者提供最优的一站式AIGC体验。</p><p></p><p>据了解，自2022年云栖大会上发布以来，魔搭社区目前已成为国内规模最大、最活跃的AI模型社区，汇聚超过10000款优质模型，为超过690万用户提供了模型及免费算力服务。</p><p></p><p>魔搭AIGC专区网址：</p><p><a href="https://www.modelscope.cn/aigc/home">https://www.modelscope.cn/aigc/home</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7NCAjhYXOVQIQzSxewlN</id>
            <title>阿里云刘伟光：云之于AI，绝不仅仅是算力供应</title>
            <link>https://www.infoq.cn/article/7NCAjhYXOVQIQzSxewlN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7NCAjhYXOVQIQzSxewlN</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Sep 2024 09:04:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>编者按：本文是阿里云智能集团资深副总裁、公共云事业部总裁刘伟光在 9 月 19 日开幕的 2024 云栖大会上的演讲实录。在题为《加速 AI 原生时代》的演讲中，刘伟光分享了他对大模型、云和 AI 关系以及 AI 原生架构的思考。这里面有很多他基于技术前沿和市场实践而总结出的洞察，角度新颖，指向性强。</blockquote><p></p><p></p><p>以下是演讲实录：</p><p></p><p>在开启今天的分享之前，先讲一个花絮。</p><p></p><p>我第一次登上云栖大会讲台是在 7 年前，也就是 2017 年 10 月 11 号，也是在这个会场。</p><p></p><p>7 年前我讲的话题是云的分布式架构、分布式中间件、分布式数据库、容器化，微服务改造，那时候这些想法还只是一种构想和畅想。7 年后，这些技术已经成为我们正在不断实践的标准。</p><p></p><p>当年，我记得翻看 PPT 时看到一些对 AI 的描述，就像很远方的一点星光。今天，AI 时代的发展就像一个正在照亮我们前行的明灯，让我们一起拥抱这个崭新的 AI 原生时代。</p><p></p><p></p><h2>1. 大语言模型的爆发是过去所有技术的继承和优化</h2><p></p><p>今天随着大语言模型在全球的爆发和应用，其实我们要思考一个问题，大语言模型不是平地起高楼的完全新兴事物。今天它是在过去算力不断迭代、数据量的不断扩充的基础上，不断优化和迭代的一种技术产物。</p><p></p><p>ChatGPT 发布在 2022 年的 11 月，大家想如果发布在 2012 年的话，那仅仅就是一份 paper。因为那个时代 GPU 的算力，包括数据的能力，数据容量都不足以支撑大语言模型训练。应该说，大语言模型的爆发是过去所有技术的继承和优化。</p><p></p><p>今天，我们的交互模式是人和人的交互，人和数字世界的交互，在未来通过大语言模型的赋能，包括长文本这些新的技术不断演进，我们有理由相信今天正在以菜单、按键打交道的这些终端设备、物理设备，将以对话、自然交互的模式跟人类交流。</p><p></p><p>我们看到今天的 AI 爆发不仅仅在数字化的虚拟世界，更多是诞生在数字化世界叠加对物理世界的优化甚至重塑。</p><p></p><p></p><h2>2. 四个“确定”不等式</h2><p></p><p>未来，AI 会带来什么样的变化？我们通过跟各种客户的合作实践，总结出了四个不等式，代表了我们对 AI 的未来非常笃定的方向。</p><p></p><h4>第一，数据的不等式。</h4><p></p><p>仅仅在去年，AI 产生的数据就已经大大超过了过去几十年来通过计算机产生的数据。Gartner 预测到 2030 年 AI 生成的合成数据将远超过人类过去生成数据的总和。AI 在过去一年所生成的图像已经超过了过去 150 年人类拍摄的所有照片的数量，未来更多的数据将由 AI 产生，这是一个非常确定的趋势。</p><p></p><h4>第二，算法的不等式。</h4><p></p><p>在大语言模型当中有一个重要的分支，就是 Coding 大模型，今年阿里云已经上线了第一个通义灵码员工，帮我们在编写代码、校验代码、优化代码。今天这些代码的采纳率已经接近 30%，跟客户的实际使用率非常相似。我们非常有理由相信在未来 1-3 年，整个 Coding 大模型生成的高质量代码将超过程序员所编写的代码。我们这一代人很可能是最后一代大规模编写程序的技术工程师。</p><p></p><h4>第三，应用的不等式。</h4><p></p><p>今天无论是 App Store，还是各种应用市场，正在诞生很多全新嵌入 AIGC 能力的 AI native application，加速度非常快。在未来，会有更多的公司去开发新兴的 AI 原生应用，这些应用将完全采用新的大模型能力，基于 GPU 推理集群上进行开发。同时，老的应用也会嵌入更多的智能体，提升整个智能化体验。未来所有的应用程序都可能被 AI 所重写，或优化。</p><p></p><h4>第四，交互的不等式。</h4><p></p><p>今天有手机和汽车的融合，也有汽车和机器人的融合，汽车在中间扮演了一个非常重要的纽带作用，未来这是有紧密协同效应的产业链。更多终端诞生之后，我相信所有的交互，人类跟很多机器终端的交互都将采用拟人的自然交互模式，更多的自然交互模式将彻底改变现在按键式、菜单式的交互。</p><p></p><p></p><h2>3. 云和大模型 AI，是类似电和电机的关系</h2><p></p><p>在这些改变背后，我们看到任何一个企业，当它去拥抱云计算，当它去开发智能体，当它去开发新的 AI native application，它所需要的架构绝对不仅仅是购买几十台、上百台 GPU 推理训练服务器这么简单的工作。</p><p></p><p>有一句话非常流行，说人工智能的尽头是电力能源供应，这句话虽然有夸大的成分，但有非常确切的理论依据。AI 智能应用的能耗密度要比传统 IT 高出 10 倍以上，整个 GPU 服务器能耗是传统 CPU 能耗的 10-30 倍。当你在类似通义千问这样的大模型应用上计算一道高等数学题时，背后消耗的电量是利用普通搜索引擎检索的近10倍。对于很多企业来讲，去大规模构建智能化应用的时候，持续稳定电力供应是非常大的挑战。</p><p></p><p>再来看算力，算力不仅仅是搭建 GPU 服务器这么简单。大模型的 Scaling Law 原理依然非常有效，但大的 GPU 集群不等于大的算力，1000 台 GPU 的算力不等于 1 台 GPU 服务器乘以 1000 倍。大模型推理和训练过程当中，网络的消耗、I/O 的操作几乎占据了大模型训练推理一半时间。这就意味着对于企业架构来讲，优化网络和 I/O 对提升大模型的效率、使用效果是至关重要的，这里如何构建一个高性能的网络，对于大模型的效率提升是非常重要的。</p><p></p><p>再看上层应用，当我们在一款 APP 端去发布一个火爆应用，无论是文字、文本、图片、图像，或者是推理计算的时候，会引发千万人、上亿人在同一秒钟涌进 APP 进行尝试。这对应用背后的性能支撑、弹性能力挑战是非常强的，类似于极限“秒杀”。所以，在 AI 原生应用的背后需要有很强的云的特征，低延时、高弹性、应对波峰波谷。</p><p></p><p>综上所述，今天当我们去拥抱 AI 原生应用、智能体时候，对原有的企业架构绝不仅仅是 GPU 服务器叠加这样简单的事情，它带来的挑战有算力规模化、网络 I/O、高性能存储、电力持续供应、低延时，包括应对上层爆发式业务的弹性能力。</p><p></p><p>为什么说云和大模型 AI 就是类似于电和电机的关系。今天云的分布式架构能力、全球部署能力、全球一张网的高速通信能力，以及高性能的存储和网络处理能力，以及节能、绿色环保能力，服务器使用效率等等，相比传统企业架构，是更适合为 AI 应用爆发和迭代提供非常有力的全方位支撑。</p><p></p><p></p><h2>4. 云计算范畴早已经突破了当初的理念</h2><p></p><p></p><p>接下来，我们换另外一个视角，分享一下企业架构在升级时候面临的挑战：AI 和云原生的融合。</p><p>云计算大约诞生在 2006 年，容器化技术大约诞生在 2014 年。云原生随着容器化、CI/CD、微服务，DevOps 架构的兴起，整个 Cloud Native 的理念应运而生。但今天回头看 Cloud Native 理念是一个相对狭义的概念，它更加强调在软件开发的研发、测试、运维形态的升级，这些已经被完整吸纳到广义的云原生当中去。站在真正云计算公司角度看，仅有软件开发能力是不够的。今天云计算范畴早已经突破了当初 IaaS、PaaS、SaaS 理念，它往下已经延伸到芯片指令级，多种异构算力的供应，往上已经越过 SaaS 层而延伸到了 AI MaaS 的层面。</p><p></p><p>当初云计算诞生的时候，视频化技术还没有广泛地流行起来。云计算诞生的时候，整个开源体系，包括大数据、数据库类的开源技术还没有今天这么丰富。今天的云计算考虑的不仅仅是在研发态、部署态、运维态这些层面，还必须考虑从底层的异构算力到网络存储再到到上层的开源技术体系，以及对研发运维体系、云原生架构等技术的全面拥抱。</p><p></p><p>当面对新的 AI 挑战时，出现了一个新的概念：AI 原生架构。当我们去构建新的 AI 应用或者 AI 智能体时，需要包括高性能的网络、存储等弹性基础设施，底层的基础大模型能力，弹性的 API 能力，以及模型服务工具能力、微调工具、向量数据库等新的能力，这才是一个 AI 原生应用应有框架。</p><p>应该说今天的 AI 原生架构既要集成广义云原生的范畴，同时兼具 AI 需要的模型层的技术能力，合在一起就组成了面向的 AI 时代原生技术架构蓝图。</p><p></p><p></p><h2>5. 云提供 AI 支撑，绝不是简单提供 GPU 的推理、训练集群</h2><p></p><p>当 AI 原生架构诞生的时候，它对云计算带来了什么样的反哺效应呢？</p><p></p><p>第一，所有大模型是以向量为单位进行数据处理的，在云原生数据库层面我们增加了对向量的支持。同时我们在 AI 大语言模型时代，为传统广义云原生增加了代码生成、智能运维、智能监测、智能建模等新工具。所以，云和 AI 相互促进，云为 AI 提供了弹性的高可用基础设施。同时，AI 为云带来了智能化的运维、体验、监测、建模能力，以及对数据化能力的重构。</p><p></p><p>这两年，阿里云应对 AI 的爆发趋势在基础架构层面做了非常多的技术升级，在高性能计算、存储、网络、调度，包括整个智能监控运维上做了非常多的能力升级。</p><p></p><p>今天云计算公司提供 AI 支撑绝对不简单是提供 GPU 的推理训练集群，我们要考虑多种 GPU 之间的通信和协同效应，不同应用跨数据中心之间的协同效应。实际上我们需要将万卡，甚至十万卡推理或训练集群构建成一台超级计算机的同时必须具备异构芯片的协同调度能力。</p><p></p><p>第二，高性能网络。在大语言模型的训练推理过程当中，其实网络 I/O 操作消耗的电量和消耗的成本是非常高的。实际上，集群有效算力利用率会随着规模增长而下降，面对这样的挑战，我们做了很多优化工作，包括一个 IDC 内连接超过十万卡形成一个算力集群，将算力集群的效率提升到 90% 以上。</p><p></p><p>大模型的推理和训练是需要将很多相关任务进行拆分，并且执行多种并行的调度策略，这种并行调度策略对资源有效共享带来了很大挑战。今天云原生的新统一调度引擎支持大规模训推一体的算力集群调度，多种异构算力集群之间的混合调度，动态调度策略调整，以及很重要的一点，就是对于低延时、延迟敏感的应用实现就近推理服务。</p><p></p><p>在智能监控层面，大规模的集群不免遇到故障，所以对于主动式故障监测、故障自愈也是我们重要的提升方向。今年我们增加了全栈式的监控指标，单机内的毫秒级发现，以及故障的分钟级发现。这些能力，包括大规模训练任务，秒级的 check point 的检查机制，实现对整个高性能集群无感的修复。</p><p></p><p>综上说述，这就是云计算为什么在 AI 时代更有优势？云面向 AI 的能力提升，能为 AI 应用开发、应用部署提供最强有力的支持和最好的客户体验，AI 应用追求的不仅仅是体验，还要兼具企业应用的基础特征、高可用、业务连续性、数据强一致性等，这才是真正创造社会价值能够更广泛被使用的 AI 应用的核心要素。</p><p></p><p></p><h2>6. AI 和数据双向赋能</h2><p></p><p>前面提到，今天 AI 大语言模型的爆发其实是对过去所有技术的继承和优化，尤其是在数据层面。在没有 AI 爆发之前，很多企业都构建了不同类型数据的处理平台，包括数据仓库、数据集市、不同类型数据库，处理不同业务的数据管理平台。</p><p></p><p>我们在数据和 AI 层面需要做两件事情：一是如何用这些既有的数据源实现对 AI 大模型完整输出链路的打通；二是 AI 和数据的双向赋能。</p><p></p><p>今天，AI 和数据的关系不完全是云给 AI 提供单向供给数据的。AI 对数据工程有非常多赋能，可以将原有数据平台和 AI 结合产生更有业务价值的结合。很多客户将我们的大语言模型能力和跟传统原有 BI 能力进行结合，让 BI 进行 AI 化升级，让企业管理者更方便地、定制化地去看他们所需要的数据，并给予辅助的决策建议。</p><p></p><p>AI 大语言模型对数据工程进行深度优化：智能 Copilot&amp;Agent 赋能灵活找数、数据开发效率提升 2 倍；智能数据探查、智能建模、智能全域数据集成、主动式数据资产治理等。</p><p></p><p>数据向 AI 赋能也不仅仅是供数这么简单，大语言模型的数据单位是向量。今天阿里云为了支持大语言模型的发展，在 OSS、RDS、Polar DB 等核心数据产品中，全部都支持向量数据处理能力。同时，在数据库当中增加了新的能力，实现推理过程的数据缓存。</p><p></p><p></p><h2>7. 完整的 AI 开发范式架构图</h2><p></p><p>随着行业发展，技术的发展和迭代，我们有了一些面向 MaaS 层架构的实践思考。这里是面向未来的参考架构图。这张图清晰体现了 MaaS 每一层的功能，也是企业需要的开发技术能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/ad11e7ae09786598cf13f7409e72f081.webp" /></p><p></p><p>对 MaaS 层而言，应该提供的是端到端的 AI 原生应用构建。</p><p>基于底层 AI Infra、原生 Data+AI 的多模态数据架构升级，为 AI 原生应用提供强大的基础平台，支持 AI 原生应用高效稳定运行、灵活的部署选择；基础大模型层面 ：以阿里云为例，我们推出“通义千问、通义万相”等大模型 、以及大量第三方开源模型，完整覆盖各类 AI 原生应用场景需求；模型服务：模型服务平台应该可为用户提供灵活、弹性的大模型 API 和定制服务 ，覆盖诸多业内领先的开源大模型 ，帮助用户快速基于大模型构建生成式应用；大模型智能体应用构建：一站式大模型应用生产工具百炼，为用户提供快速、低成本的大模型智能体应用开发标准化方案，整合大模型应用链路中各种工具链、插件、提示词工程模板等，让用户能将大模型的强大能力快速应用到自己的业务中；再往上的“专属大模型”领域 ，应该是帮助用户结合行业数据和企业私有数据，进行微调和训练专属大模型 （企业专属知识中心），生成个性化 API 为上层各类应用提供服务从部署选择方面：从下至上可以支持多种部署选择组合，如地域 +AZ 选择 、开源 / 闭源选择、公共云 /VPC 部署 / 私有化选择、训练推理统一部署、云与端侧推理部署选择等。</p><p></p><p>图的右侧，我们支持了对整个模型训 - 推一体的部署、云端一体的部署、地域 +AZ 选择 、开源 / 闭源选择、公共云 /VPC 部署 / 私有化选择、训练推理统一部署、云与端侧推理部署选择等，方便客户在全国各地，甚至全球快速开发应用、部署应用。</p><p></p><p>我们认为，这样的 AI 开发范式是包括底层算力，到上层应用的更为完整的范式。</p><p></p><p></p><h2>8. AI 原生架构的 9 大要素</h2><p></p><p>AI 爆发对数字世界和物理世界带来改变和冲击，进而带来对于企业架构的冲击，对未来的数据、开发、代码、应用、交互模式上的改变。在 AI 原生架构当中，在实践中我们发现有 9 个要素是非常重要的，这些既来自阿里云自身的实践，也来自于客户共创。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e17fe1b324d60aceca89c135aad7eef.webp" /></p><p></p><p>这是我们总结的 AI 原生架构的 9 个方面建议，希望这些实践对大家实际工作有所借鉴。</p><p></p><p></p><h2>9. AI 和云的融合将能改变数字世界加物理世界</h2><p></p><p>最后，跟大家分享一下我们的行业实践。</p><p></p><p>汽车领域是我们非常重视的领域。因为新能源汽车对 AI 和云的需求，已经从原有的车联网延伸到自动驾驶、智能座舱。AI 将对汽车，甚至是机器人领域有一个全新的架构升级和重塑。我们相信自动驾驶一定就是未来最有价值的智能体应用。</p><p></p><p>在大模型领域我们和中国一汽合作，实现在 BI 领域全面升级。自动驾驶领域，我们跟小鹏汽车展开深度合作。我们希望通过在算力、模型、技术架构方面的合作，让中国汽车行业不仅给客户提供更好的体验，还帮助汽车行业将更好的客客户体验、智驾体验带到全世界。</p><p></p><p>游戏行业是阿里云最早的一批客户，我们伴随很多客户从中国走向世界，像米哈游这样的企业已经从创业公司变成了世界顶尖的手游公司。今天我们服务的游戏类型也覆盖到了手游、页游，端游等等。在服务众多游戏客户过程中，游戏体验的不断提升也带给阿里云在游戏领域的不断加强的技术积累，游戏正在原有的模式延伸到新的千人千面时代。网易、巨人网络等已经开始大量使用大模型，给玩家提供开放式的结局和不一样的体验。云和 AI 和数据的结合，将给游戏行业带来新一步的升级。</p><p></p><p></p><h2>10. 结语：</h2><p></p><p>今年是阿里云的第 15 年，我们走过了三个阶段，第一个阶段是企业上云，是一些传统企业和 PC 端网站。第二个阶段，我们陪伴了第一批移动互联网企业崛起，这些企业是真正的云原生企业，我们也伴随他们从中国走向海外。今天，我们正处于第三个阶段，AI 和云的融合。正如阿里巴巴集团CEO、阿里云智能集团董事长兼CEO吴泳铭所讲，它带来的改变不仅在于手机屏幕，它将能改变数字世界加物理世界！</p><p>我和在座的从业者，有幸见证了第一代和第二代的发展，我们正在一起拥抱第三代 AI 和云的结合。希望我们一起加速推动这个伟大时代的变革。</p><p></p><p>感谢大家！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HqvQxLedVcZhxLTLiJqG</id>
            <title>阿里云「通义灵码」迎来重磅升级，「AI 程序员」正式亮相！</title>
            <link>https://www.infoq.cn/article/HqvQxLedVcZhxLTLiJqG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HqvQxLedVcZhxLTLiJqG</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Sep 2024 03:30:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>最近两年，随着大语言模型和生成式AI技术的爆火，软件开发领域首当其冲成为了最热门的大模型应用场景之一，GitHub Copilot、通义灵码等 AI 辅助编程工具纷纷问世。这些工具通过自然语言处理和机器学习技术，能够理解开发者的意图，并且提供行级/函数级代码、单元测试和代码注释的智能生成等功能，极大地提高了开发者的编码效率和代码质量。</p><p></p><p>以通义灵码为例，数据显示，过去一年通义灵码插件下载量超 500 万，每日辅助开发者生成代码超 3000 万次，累计生成代码超 10 亿行,已被广泛应用于金融、制造、互联网、交通、汽车、能源等行业，成为国内最受欢迎的辅助编程工具。</p><p></p><p>随着大模型的持续进化，在语义理解、代码生成、开发工作流等方面的能力也获得了持续、全面的提升，辅助编程工具有没有可能像汽车的自动驾驶一样，只需要自然语言交互，就能实现“自动编程”呢？</p><p></p><p>此次云栖大会上，阿里云给出了它的答案——通义灵码「AI 程序员」。</p><p></p><p>9 月 19 日，2024 云栖大会在杭州拉开帷幕。会上，阿里云宣布「通义灵码」重磅升级，从「辅助编程」工具，进化到能自主执行任务拆解、代码编写、缺陷修复、测试等任务的「AI 程序员」，最快分钟级完成从 0 到 1 的应用开发，提升数十倍开发效率。</p><p></p><p>据介绍，「AI 程序员」是基于通义大模型的 AI 智能体，可以自主执行任务拆解、代码编写、缺陷修复、测试等编程相关任务。相比于编程助手，「AI 程序员」可以脱离 IDE 软件，像真人程序员一样执行缺陷修改、需求分析、代码实现、问题排查等任务工作流，同时具备架构师、开发工程师、测试工程师等多种岗位技能，最快可分钟级完成应用开发。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f72a75b196138933f47bf297f523a8a1.png" /></p><p></p><p></p><p>交互层面，由于「AI 程序员」是基于通义大模型构建的多智能体，每个智能体能够分别负责具体的软件开发任务并互相协作，用户只需要参与“需求输入”、“确认计划”、“确认实现”三个步骤，即可完成一个端到端的产品功能研发。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/28/d5/28yyab12ddc493d5061d7e59bef22bd5.gif" /></p><p></p><p>例如，当系统出现 BUG 时，开发者只需要将问题链接丢给「AI 程序员」，它就能自动读取问题，进行代码库拉取、问题分析等动作，并基于分析结果生成解决计划。同时，凭借着首创的代码仓库知识图结构，「AI 程序员」不仅能理解用户的需求，还能精准定位代码对应的修改位置并自动给出修改方案。开发者可以直接查看「AI 程序员」定位的代码文件进行勘误或者给出优化建议，促使「AI 程序员」进行代码调优，确认无误后再点击执行代码变更。另外，「AI 程序员」还会贴心地生成代码合并请求标题及表述，只需要开发者最后确认提交即可。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/96/8e/964140597f36c0979de546f8de17d08e.gif" /></p><p></p><p>除了“缺陷自动修复”功能，当开发者有新的开发需求时，同样可以将需求描述给「AI 程序员」并选择模板，AI 程序员能够自动分析需求、制定方案，之后开发者就可以像跟 Chatbot 沟通一样去提出优化建议，直至方案完美落地。</p><p></p><p>此外，「AI 程序员」跟「编码助手」一样，支持“研发问答”功能，该功能基于海量研发文档、产品文档、通用研发知识、阿里云的云服务文档和 SDK/OpenAPI 文档等进行问答训练，能够高效、准确地帮助开发者答疑解惑。</p><p></p><p>当然，尽管通义灵码「AI 编码助手」和「AI 程序员」都是基于千问大模型而开发的智能编码工具，但表现形式上有所不同。</p><p></p><p>首先是产品形态上，「AI 编码助手」主要是以 IDE 插件的形式存在于各类开发工具中，而「AI 程序员」除了可以是 IDE 插件，还能以网页或软件的形式单独使用；其次是交互方式上，「AI 编码助手」主要是辅助性质，帮助开发者解决单点开发问题，而「AI 程序员」则更像是一个具备独立开发能力的开发者，用户只需要以自然语言输入需求并做一定的代码校验工作，即可自动完成开发任务。</p><p></p><p>阿里云表示，得益于通义灵码「AI 编码助手」过去一年沉淀的大量的技术、模型能力，「AI 程序员」具备更智慧、更高效、更敏捷的特性。同时，基于 「AI 程序员」、「智能编码助手」、DevOps 工具云效等工具链，阿里云能够为开发者提供研发增效的联合解决方案。</p><p></p><p>对于普通开发者而言，「AI 程序员」的出现能够更进一步地助力研发提效、解放双手，让开发者能够更聚焦于更有价值、更有创造力的开发工作中去，大量简单、重复、高频的日常开发、运维等工作将由「AI 程序员」去完成，开发者此时只需要扮演“安全员”的角色做节点性管控即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4ca6f6d505968e987dde7c244382467e.png" /></p><p></p><p>对于初学者或者非专业人士而言，「AI 程序员」则进一步帮他们降低了开发应用的门槛。云栖大会现场，一位 13 岁的中学生在通义灵码上输入几句话，2 分钟就生成了一个 python 语言编写的倒计时网页；现场还有参会者，用通义灵码修改开源魂斗罗游戏代码，在 9 个代码文件 2000 多行代码里，几分钟就精准修改了游戏角色的生命值、跳跃高度等参数（详细 Demo演示附在文末）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/66/66dc5bb059cef29e5ef73b41a9c0dba0.png" /></p><p></p><p>主论坛现场，阿里云智能集团 CTO 周靖人宣布 Qwen2.5 全新升级，为用户提供全尺寸模型的选择，灵活权衡模型精度与成本。据介绍，Qwen2.5 的预训练数据量超18万亿token，数学和代码能力大幅提升，支持 128k 输入 8k 输出，可以快速生成万字长文，prompt 稳定性、指令遵循能力也获得了持续增强等。这些能力的提升也将大幅增强通义灵码「编程助手」和「AI 程序员」的能力。</p><p></p><p>“通义灵码正在不断地整合「AI 程序员」的能力，希望能够帮助程序员完成日常功能的开发，更有效地提升工作效率。”周靖人表示。</p><p></p><p>其实，从今年3月海外初创公司 Cognition 发布的“全球首个AI工程师——Devin”，到阿里云通义灵码「AI 程序员」的推出，都预示着软件开发正在逐步从 Co-Pilot 走向 Auto-Pilot 的时代。在 Auto-Pilot 模式下，AI 将拥有更高的自主性，能够独立完成更复杂的编程任务，从需求分析到代码实现，甚至问题排查和修复，都可能在无需人类干预的情况下完成。这一转变不仅会带来软件开发效率的极大提升，也可能引发开发者角色和技能要求的变革。</p><p></p><p>未来，人类与 AI 的协同工作将成为软件开发的常态，开发者可能需要更多地专注于架构设计、创新算法开发和 AI 系统的管理与优化，从传统的编码工作转向更高层次的创造性或技术领导类工作。对于开发者而言，这既是机遇，也是挑战。如何快速适应这一变化，不断提升自己的技术能力和创新思维，并学会利用 AI 的力量更好地改变世界，是值得当下所有开发者思考的问题。当然，阿里云通义灵码「AI 程序员」或许是一个快速了解未来工作方式并上手实践的不错路径。</p><p></p><p></p><p>DEMO 1：13岁中学生现场用通义灵码编写倒计时网页</p><p></p><p></p><p></p><p>DEMO 2：通义灵码“魔改”开源游戏《魂斗罗》</p><p></p><p>*本视频仅做「AI 程序员」的交互展示，不鼓励游戏开挂。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mA2A6zTy4gwu8QzTp4bd</id>
            <title>小鹏汽车加速端到端自动驾驶落地 ，深化与阿里云AI算力合作</title>
            <link>https://www.infoq.cn/article/mA2A6zTy4gwu8QzTp4bd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mA2A6zTy4gwu8QzTp4bd</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月19日，小鹏汽车董事长 CEO何小鹏驾驶“全球首款AI汽车”P7+亮相2024云栖大会，这款车搭载了业内领先的端到端大模型。过去2年，小鹏汽车与阿里云共建的AI算力规模提升超4倍。何小鹏表示，将继续深化与阿里云的AI算力合作，加速推动端到端大模型拓展自动驾驶上限，提升下限。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91250d7ec6314e6818ae9394d7c9bf06.jpeg" /></p><p></p><p></p><p>端到端是当下最受业界关注的自动驾驶解决方案，它同样遵循Scaling Law。小鹏汽车能率先实现端到端大模型量产上车，离不开在算力上的提前布局。</p><p></p><p>为提升智驾大模型训练效率，小鹏汽车早在2022年就携手阿里云在乌兰察布建成中国最大的自动驾驶智算中心，将自动驾驶模型训练效率提升了超600倍。而近两年内，由于大模型技术快速发展，阿里云已将此智算中心的算力储备扩张超4倍至2.51Eflops，为小鹏汽车提供稳定高效的算力底座。</p><p></p><p>今年5月，小鹏汽车就在国内率先实现端到端自动驾驶量产上车，并在全国范围内迅速落地。业界普遍认为，未来端到端智驾的算力需求还将进一步扩大，上亿元投入仅是智驾算力的入场券。小鹏汽车宣布，每年投入 35 亿元用于研发，其中 7 亿元用于算力训练，还将与阿里云持续深化合作，加速推动端到端大模型落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fe47a037f2bc0039c038931dda9a71e0.jpeg" /></p><p></p><p></p><p>作为“All in AI”的车企，小鹏汽车不仅实现了端到端大模型量产上车，还将大模型深入应用到了座舱场景中。小鹏汽车基于自主研发的“全域大语言模型”X-GPT及通义千问全面升级了车载助理。小鹏汽车在官方App中已接入通义万相，在研发场景中，通义灵码的代码评审采用率高达50%。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VZyuMPqM7hRR1wf341w6</id>
            <title>通义千问重磅开源Qwen2.5，下载量突破4000万</title>
            <link>https://www.infoq.cn/article/VZyuMPqM7hRR1wf341w6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VZyuMPqM7hRR1wf341w6</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月19日云栖大会，阿里云CTO周靖人发布通义千问新一代开源模型Qwen2.5。Qwen2.5全系列涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型，每个尺寸都有基础版本、指令跟随版本、量化版本，总计上架100多个模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/eef0fb2899093373025ad81bb4b5f1fa.jpeg" /></p><p></p><p>Qwen2.5全系列模型都在18T&nbsp;tokens数据上进行预训练，相比Qwen2，整体性能提升18%以上，拥有更多的知识、更强的编程和数学能力。Qwen2.5-72B模型在MMLU-rudex基准（考察通用知识）、MBPP&nbsp;基准（考察代码能力）和MATH基准（考察数学能力）的得分高达86.8、88.2、83.1。</p><p></p><p>Qwen2.5支持高达128K的上下文长度，可生成最多8K内容。模型拥有强大的多语言能力，支持中文、英文、法文、西班牙文、俄文、日文、越南文、阿拉伯文等&nbsp;29&nbsp;种以上语言。模型能够丝滑响应多样化的系统提示，实现角色扮演和聊天机器人等任务。在指令跟随、理解结构化数据（如表格）、生成结构化输出（尤其是JSON）等方面Qwen2.5都进步明显。</p><p></p><p>语言模型方面，Qwen2.5开源了7个尺寸，0.5B、1.5B、3B、7B、14B、32B、72B，它们在同等参数赛道都创造了佳绩，型号设定充分考虑下游场景的不同需求，3B是适配手机等端侧设备的黄金尺寸；32B是受开发者期待的“性价比之王”，可在性能和功耗之间获得最佳平衡，Qwen2.5-32B的整体表现超越了Qwen2-72B。</p><p></p><p><img src="https://static001.geekbang.org/infoq/90/902a65758ba6d50325ce6613c5f08bc9.jpeg" /></p><p>在MMLU-redux等十多个基准测评中，Qwen2.5-72B表现超越Llama3.1-405B</p><p></p><p>72B是Qwen2.5系列的旗舰模型，其指令跟随版本Qwen2.5-72B-Instruct在MMLU-redux、MATH、MBPP、LiveCodeBench、Arena-Hard、AlignBench、MT-Bench、MultiPL-E等权威测评中表现出色，在多个核心任务上，以不到1/5的参数超越了拥有4050亿巨量参数的Llama3.1-405B。</p><p></p><p>专项模型方面，用于编程的&nbsp;Qwen2.5-Coder&nbsp;和用于数学的&nbsp;Qwen2.5-Math都比前代有了实质性进步。Qwen2.5-Coder&nbsp;在多达5.5T&nbsp;tokens&nbsp;的编程相关数据上作了训练，当天开源1.5B和7B版本，未来还将开源32B版本；Qwen2.5-Math支持使用思维链和工具集成推理（TIR）&nbsp;解决中英双语的数学题，本次开源了1.5B、7B、72B三个尺寸和一款数学奖励模型Qwen2.5-Math-RM。</p><p></p><p>多模态模型方面，广受期待的视觉语言模型Qwen2-VL-72B正式开源，Qwen2-VL能识别不同分辨率和长宽比的图片，理解20分钟以上长视频，具备自主操作手机和机器人的视觉智能体能力。日前权威测评LMSYS&nbsp;Chatbot&nbsp;Arena&nbsp;Leaderboard发布最新一期的视觉模型性能测评结果,Qwen2-VL-72B成为全球得分最高的开源模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fa/fa85e14d8bcb032a57c01020afe11bfe.png" /></p><p>Qwen2-VL-72B在__权威测评__LMSYS&nbsp;Chatbot&nbsp;Arena&nbsp;Leaderboard成为成为全球得分最高的开源视觉理解模型</p><p></p><p>自从2023年8月开源以来，通义在全球开源大模型领域成为不少开发者尤其是中国开发者的首选模型。性能上，通义大模型多次登顶Hugging&nbsp;Face全球大模型榜单；生态上，通义从零起步、开疆拓土，与海内外的开源社区、生态伙伴、开发者共建生态网络，截至2024年9月中旬，通义千问开源模型下载量突破4000万，Qwen系列衍生模型总数超过5万个，成为仅次于Llama的世界级模型群。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb4cc8d4566d354534b6b20b36106682.png" /></p><p>HuggingFace数据显示，截至9月中旬Qwen系列原生模型和衍生模型总数超过5万个</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Bw5XqF5PmShA9sdY2EaV</id>
            <title>阿里云通义灵码重磅升级，能自主修BUG、开发应用的AI程序员来了</title>
            <link>https://www.infoq.cn/article/Bw5XqF5PmShA9sdY2EaV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Bw5XqF5PmShA9sdY2EaV</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月 19 日，在2024杭州云栖大会上，随着通义大模型能力的全面提升，阿里云通义灵码迎来重磅升级，从一年前只能完成基础的辅助编程任务，进化到几句话就能完成需求理解、任务拆解、代码编写、修改BUG、测试等开发任务，最快几分钟可从0到1完成应用开发，提升数十倍开发效率。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e6fcc1ddc040ca45c480b2fea5fb0d9c.jpeg" /></p><p></p><p></p><p>自去年首次亮相以来，通义灵码已入职中华财险、哈啰集团、长安汽车等公司，累计生成代码超10亿行。全新升级的通义灵码模拟了人类程序员的能力，可完成更复杂、更全面的任务；通义灵码还可以脱离专业IDE软件，在web端直接执行缺陷修改、需求分析、代码实现、问题排查等任务工作流，兼具架构师、开发工程师、测试工程师等多种岗位技能，大幅缩短了应用的开发周期。</p><p></p><p>例如，真人程序员手动开发一个网页，通常需要1天的时间完成需求分解、写代码、测试等任务；现在，人类只需要输入需求，通义灵码5分钟就能完成整个过程。阿里云表示，未来只要有创意，不懂代码也能开发应用和软件。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03ac772183b1d077e44dced58c743da9.png" /></p><p></p><p></p><p>云栖大会现场，一位13岁的中学生在通义灵码上输入几句话，2分钟就生成了一个python语言编写的倒计时网页；现场还有参会者，用通义灵码修改开源魂斗罗游戏代码，在9个代码文件2000多行代码里，几分钟就精准修改了游戏角色的生命值、跳跃高度等参数。</p><p></p><p>目前，通义灵码已广泛应用于金融、制造、互联网、交通、汽车、能源等行业。在 Gartner 首个AI代码助手魔力象限报告中，阿里云成为唯一进入挑战者象限的中国科技公司。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dE4WtQXtXspccpPToH6B</id>
            <title>通义千问升级旗舰模型Qwen-Max，性能接近GPT-4o</title>
            <link>https://www.infoq.cn/article/dE4WtQXtXspccpPToH6B</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dE4WtQXtXspccpPToH6B</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 13:29:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月19日云栖大会，阿里云CTO周靖人宣布，通义旗舰模型Qwen-Max全方位升级，性能接近GPT-4o。通义官网和通义APP的后台模型均已切换为Qwen-Max，继续免费为所有用户提供服务。用户也可通过阿里云百炼平台调用Qwen-Max的API。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/fe/fec7a4e2828992fa4c1984bc91b92e7a.png" /></p><p></p><p></p><p>相比上一代模型，Qwen-Max在训练中使用了更多的训练数据、更大的模型规模、更强的人类对齐，最终达到了更高的智能水平。在MMLU-Pro、MATH、GSM8K、MBPP、MultiPL-E、LiveCodeBench等十多个权威基准上，Qwen-Max表现接近GPT-4o，数学能力、代码能力则超越了GPT-4o。数学和代码所代表的推理能力是大模型智能水平的最重要体现。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5b5a7470138bf40e3094c1065cadbc5c.jpeg" /></p><p></p><p></p><p>相比2023年4月的初代通义千问大模型，Qwen-Max的理解能力提升46%、数学能力提升75%、代码能力提升102%、幻觉抵御能力提升35%、指令遵循能力提升105%，模型与人类偏好的对齐水平更是有了质的飞跃，提升了700%以上。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fQhoCDWL4bpYfstWGN5o</id>
            <title>阿里CEO吴泳铭：AI最大的想象力不在手机屏幕，而是改变物理世界</title>
            <link>https://www.infoq.cn/article/fQhoCDWL4bpYfstWGN5o</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fQhoCDWL4bpYfstWGN5o</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 08:08:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作者 ｜ 华卫</p><p></p><p>9 月 19 日，阿里云CTO周靖人发布通义千问新一代开源模型Qwen2.5，涵盖多个尺寸的大语言模型、多模态模型、数学模型和代码模型。每个尺寸都有基础版本、指令跟随版本、量化版本，总计上架100多个模型。截至2024年9月中旬，通义千问开源模型累计下载量已突破4000万，成为仅次于Llama的世界级模型群。</p><p></p><p>“人们对新技术革命，往往对短期高估，又对长期低估，但它会在你的怀疑中成长，在你在迟疑中错过大趋势。”</p><p></p><p>当日，在刚刚开幕的阿里 2024 云栖大会上，阿里巴巴集团CEO、阿里云智能集团董事长兼CEO吴泳铭发表主题演讲，分享了对 AGI 变革趋势、生成式 AI 的应用巨变和 AI 算力发展的最新观点。</p><p></p><p>他认为，过去22个月，AI发展的速度超过任何历史时期，但我们依然还处于AGI变革的早期。下一阶段，全世界先进模型的投入门槛是几十亿、几百亿美金级别。</p><p></p><p>吴泳铭表示，生成式AI最大的想象力，绝不是在手机屏幕上做一两个新的超级app，而是接管数字世界，改变物理世界。并且，他预测道，机器人将是下一个迎来巨变的行业，未来所有能移动的物体都会变成智能机器人。</p><p></p><p>算力方面，吴泳铭指出，过去一年，阿里云投资新建了大量的AI算力，但还是远远不能满足客户的旺盛需求。未来几乎所有的软硬件都会具备推理能力，它们的计算内核将变成GPU AI算力为主、CPU传统计算为辅的计算模式。</p><p></p><p>以下为演讲全文（在不改变原意上有删减）：</p><p></p><p>在刚刚过去的夏天，阿里云全面支撑巴黎奥运会实现了历史性突破，云计算首次超越卫星，成为奥运主要转播方式。AI也首次广泛应用于奥运会。今天，云栖大会的焦点也是AI。我主要分享三点内容：</p><p></p><p></p><h1>大模型发展远超摩尔定律</h1><p></p><p></p><p>第一，过去22个月，AI发展的速度超过任何历史时期，但我们现在依然还处于AGI变革的早期阶段。</p><p></p><p>大模型技术快速迭代，技术可用性大幅提升。大模型已经具备了文本、语音、视觉的多模态能力，能够开始完成复杂指令。去年，大模型还只能帮助程序员写简单的代码，今天已经能直接理解需求，完成复杂的编程任务。去年，大模型的数学能力还只有中学生水平，今天已达到国际奥赛金牌水平，并在物理、化学、生物等多方面学科接近博士水平。</p><p></p><p>同时，模型推理成本指数级下降，已经远远超过摩尔定律。一年来，通义千问API在阿里云百炼上的调用价格下降了97%，百万Tokens调用花费最低已经降到了5毛钱。推理成本是应用爆发的关键问题，阿里云会努力把成本继续降下去。</p><p></p><p>开源生态蓬勃发展。今年6月，通义千问开源Qwen2，迅速登顶Huggingface的全球开源模型排行榜。在Huggingface上，Qwen的原生和衍生模型接近5万个，排名全球第二。阿里云魔搭社区上有超过1万个模型、服务了超过690万开发者。</p><p></p><p>这一切才刚刚开始，要实现真正的AGI，下一代模型需要具备更大规模、更通用、更泛化的知识体系，同时也将具备更复杂更多层次的逻辑推理能力。全世界先进模型竞争的投入门槛，将达到数十亿、数百亿美金的级别。AI具备创造能力、帮助人类解决复杂问题的路径清晰可见，也打开了AI在各行业场景中广泛应用的可能性。</p><p></p><h1>生成式 AI 将创造超互联网十倍价值</h1><p></p><p></p><p>第二，AI最大的想象力不在手机屏幕，而是接管数字世界，改变物理世界。</p><p>今天很多行业内人士一直在想AI最大的应用是什么，可能一直在想手机上有什么AI时代创新的超级APP。但我们认为AI最大的想象力绝对不是在手机屏幕上，AI最大的想象力是在通过渗透数字世界、接管数字世界，并改变物理世界，这才是AI最大的想象力。</p><p></p><p>我们不能只停在移动互联网的视角看未来。生成式AI最大的想象力，绝不是在手机屏幕上做一两个新的超级app，而是接管数字世界，改变物理世界。</p><p></p><p>过去三十年，互联网浪潮的本质是连接，互联网连接了人、信息、商业和工厂，通过连接提高了世界的协作效率，创造了巨大的价值，改变了人们的生活方式。但生成式AI是通过生产力的供给创造了新的价值，从而为世界创造了更大的内在价值，也就是总体提高了整个世界的生产力水平。这种价值创造，可能是移动互联网连接价值的十倍、几十倍。</p><p></p><p>我们认为生成式AI将逐渐渗透数字世界，并接管数字世界，物理世界的大部分事物都会具备AI能力，形成下一代的具备AI能力的全新产品，并与云端AI驱动的数字世界连接产生协同效应。</p><p></p><p>很长一段时间，AI的焦点主要集中在模拟人类的感知能力，比如自然语言理解、语音识别、视觉识别。但是生成式AI的崛起，带来了质的飞跃，AI不再仅仅局限于感知，而是首次展现了思考推理和创造的力量。</p><p></p><p>生成式AI让世界有了一个统一的语言——Token。它可以是任何文字、代码、图像、视频、声音，或者是人类千百年来的思考。AI模型可以通过对物理世界数据的Token化，理解真实世界的方方面面，比如人类行走、奔跑、驾驶车辆、使用工具，绘画、作曲、写作、表达、教学、编程的技巧，甚至是开公司创业。理解之后，AI就可以模仿人类去执行物理世界的任务。这将带来新的产业革命。</p><p></p><p>我们看到，汽车行业正在发生这样的变革。之前的自动驾驶技术，是靠人来写算法规则，几十万行代码，仍然无法穷尽所有的驾驶场景。采用“端到端”的大模型技术训练后，AI模型直接学习海量人类驾驶视觉数据，让汽车具备了超越大部分司机的驾驶能力。</p><p></p><p>机器人将是下一个迎来巨变的行业。未来，所有能移动的物体都会变成智能机器人。它可以是工厂里的机械臂、工地里的起重机、仓库里的搬运工、救火现场的消防员、包括家庭里的宠物狗、保姆、助理。</p><p>未来，工厂里会有很多机器人，在AI大模型的指挥下，生产机器人。现在每个城市家庭里有一辆或者两辆车，未来每个家庭可能会有两三个机器人，帮助人们提升生活当中的效率。</p><p></p><p>可以想见，AI驱动的数字世界连接着具备AI能力的物理世界，将会大幅提升整个世界的生产力，对物理世界的运行效率产生革命性的影响。</p><p></p><h1>GPU AI 算力将改写所有应用</h1><p></p><p></p><p>第三，AI计算正在加速演进，成为计算体系的主导。</p><p></p><p>无论是我们看到端侧的计算，还是云端的世界，这都是一个非常明显的趋势。生成式AI对数字世界和物理世界的重构，将带来计算架构的根本性变化。过去几十年，CPU主导的计算体系，正在加速向GPU主导的AI计算体系转移。未来几乎所有的软硬件都会具备推理能力，它们的计算内核将变成GPU AI算力为主、CPU传统计算为辅的计算模式。</p><p></p><p>我们看到，在新增算力市场上，超过50%的新需求由AI驱动产生，AI算力需求已经占据主流地位。这一趋势还会持续扩大。过去一年，阿里云投资新建了大量的AI算力，但还是远远不能满足客户的旺盛需求。</p><p></p><p>今天我们接触到的所有客户、所有开发者、所有CTO，几乎都在用AI重构自己的产品。大量新增需求正在由GPU算力驱动，大量存量应用也在用GPU重新改写。在汽车、生物医药、工业仿真、气象预测、教育、企业软件、移动APP、游戏等行业，AI计算正在加速渗透。在各行各业，看不见的新产业革命正在悄然演进。</p><p></p><p>所有行业，都需要性能更强、规模更大、更适应AI需求的基础设施。</p><p></p><p>阿里云正在以前所未有的强度投入AI技术研发和基础设施建设。我们的单网络集群已拓展至十万卡级别，正在从芯片、服务器、网络、存储到散热、供电、数据中心等方方面面，重新打造面向未来的AI先进基础设施。</p><p></p><p>从历史经验来看，人们对新技术革命，往往对短期高估，又对长期低估。因为在新技术应用早期，渗透率还比较低，人们经验没有发生过此类事件，大部分人的本能会产生怀疑，这很正常。但新技术革命会在人们的怀疑中成长，让很多人在迟疑中错过。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2hPirteON3H6UkyieqFE</id>
            <title>精益驱动下的金融智能化变革：大模型与知识工程的进化</title>
            <link>https://www.infoq.cn/article/2hPirteON3H6UkyieqFE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2hPirteON3H6UkyieqFE</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 06:10:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>大模型时代背景下，精益思想和提示工程的引入，成为提升金融行业智能化水平的关键驱动力。</p><p></p><p>在今年 8 月举办的 <a href="http://mp.weixin.qq.com/s?__biz=MzkzMzQzNjQ5Mw==&amp;mid=2247492380&amp;idx=1&amp;sn=0699c5699663064fa077727e96c38096&amp;chksm=c24e2e3ef539a72896745ba535af985fb2d9d5663c9cf2c4af035f99eafb210aa24f590878e1&amp;scene=21#wechat_redirect">FCon 全球金融科技大会</a>"上，文因互联董事长兼创始人、中国中文信息学会语言与计算专委会金融知识图谱工作组主席鲍捷博士发表专题演讲《精益地打造金融专家智能体》，探讨了如何通过精益方法论实现大模型在金融领域的有效落地，并展示了提示工程在知识建模中的革命性应用。通过对典型案例的分析，揭示新技术如何帮助金融企业实现降本增效，推动行业进入智能决策的新时代。&nbsp;&nbsp;</p><p></p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）</p><p></p><h3>从精益出发</h3><p></p><p></p><p>精益，或称为 Lean，是一种追求效率和减少浪费的方法论。在软件工程和创业领域，我们经常听到"精益创业"（Lean Startup）这个概念，它其实是一种思维模式，强调用更少的资源解决问题。在人工智能领域，核心问题往往不是算法本身，而是如何以更低的成本解决问题。例如，如果有足够的资金，理论上可以解决任何问题，但现实中我们更关注如何经济高效地实现目标。</p><p></p><p>在知识工程领域，问题不仅仅是成本，因为知识本身带有主观性。由于每个人的观点都不尽相同，如何在观点不一致的情况下找到经济的解决方案，是知识工程面临的根本问题。从爱德华·费根鲍姆（Edward Feigenbaum）40 多年前创立知识工程学科开始，如何降低成本一直是核心问题。机器学习的出现是为了减少手工编写规则的成本，深度学习和自回归方法的发明是为了降低数据标注的成本，而 生成式人工智能的探索则是为了寻找一种更通用、成本更低的知识发现方法。归根结底，所有这些技术的发展都是为了让我们能够以更低的成本从数据中提取知识。</p><p></p><p>在过去几十年的人工智能实践中，大约 99% 的人工智能项目都以失败告终。我们可以预见，未来大多数项目也将面临同样的命运。那么，什么样的项目更有可能成功呢？长期经验告诉我们，那些"重"的项目，即那些在前期需要大量成本投入而收益不明显的项目，往往容易失败。这是因为每个项目都存在成本和收益的曲线，如果项目在早期就面临漫长而高昂的成本投入，而没有相应的收益，通常很难坚持到收益回报的那一天，相关方的耐心也会逐渐消失。</p><p></p><p>我们应该采取“小步快跑”的策略，以实践和场景驱动的方式进行工作。这与精益思想是一致的，它强调成本和收益的同步增长。在硅谷的创业理论中，这种增长过程被称为 Lean Startup 循环，即构建、评估、学习（build、measure、learning），形成一个持续的循环过程。在知识工程领域，我们今天也需要采用类似的方法来大幅降低成本，以提高项目成功的可能性。</p><p></p><h3>20 年前的人工智能实践</h3><p></p><p></p><p>20 年前，人工智能领域的实践与今天大相径庭。以 Aura 项目为例，这是我亲自观察过的一个项目，虽然我没有直接参与其中，但我对项目中的人员非常熟悉，因此我能够挖掘出一些不为外界所知的内部信息。</p><p></p><p>Aura 项目本质上可以被视为一个高考项目。在美国，高中生也需要参加高考，而考试的试卷被称为 SAT。项目的目标是使用机器代替人类来完成试卷，参加高考。这个项目是在 20 多年前由微软的创始人之一 Paul Allen 发起的。Paul Allen 因健康原因很早就离开了微软，尽管他的财富不及比尔·盖茨，但仍然拥有数百亿美元。他计划在生前将所有财产用于投资，其中一部分就投向了火箭技术，另一部分则投向了人工智能领域。他在人工智能上投入了大约 10 亿美元，这在 20 年前是一笔相当可观的资金。</p><p></p><p>Paul Allen 将其中一部分资金投入到了知识工程领域，就是 Aura 项目。Aura 意为"曙光"，而我恰好有一位非常好的朋友在这个项目中担任项目经理。他向我透露了许多实际情况，包括项目中的各个组件，如问答引擎、词汇编辑引擎和逻辑推理引擎等。他们的目标是在名为 Halo Project 的大项目框架下解决科学问题的推理。Halo 项目的核心是进行化学和物理等科学问题的解答，这在当时是一个非常具有挑战性的任务，与今天使用大模型轻松处理这些问题形成鲜明对比。</p><p><img src="https://static001.geekbang.org/infoq/20/20e5270f3d230be470704df88054521b.png" /></p><p>在 20 年前，我们还没有现在所说的大模型，甚至知识图谱也尚未出现——知识图谱是在 2012 年才发展起来的技术。当时，我们处于语义网时代，进行知识建模的第一步是将知识编写成规则语言。最初使用的是 Prolog 语言，更早之前使用的是 Lisp，随后发展出了 OWL 等规则语言。</p><p></p><p>对于 Halo 项目来说，他们面临的挑战是对科学知识进行建模，这是一项非常困难的任务。为了解决这个问题，他们采用了一种名为 SILK 的语言，即 Semantic Inferencing on Large Knowledge，这是一种用于大规模知识推理的语言。在这里，"inference"（推理）这个词在不同的上下文中有不同的含义。如今，当我们使用大模型时，可能会将推理理解为一种快速的推导过程，但在当时，推理涉及到复杂的逻辑和计算过程。在 20 年前，人工智能主要依赖两种知识处理方法：演绎推理（deduction inference）和统计推断。演绎推理是一种基于逻辑的推演过程，而统计推断则侧重于数据和概率。</p><p><img src="https://static001.geekbang.org/infoq/fb/fb93b5eb5afc9141435641736813eb38.png" /></p><p></p><p>当时，人工智能项目采用了一种非常复杂的架构，将生物学、化学和物理学等知识手动转化为一系列知识规则。这个过程非常昂贵，据估计，将一本书的内容转化为规则可能需要花费数十万美元。这在当时是一个成本极高的事情，涉及使用多种不同的逻辑表达语言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3ba828ffa6b3ec13f8538a04d0bfe254.png" /></p><p></p><p>在拥有了规则语言之后，接下来需要的就是一个推理机。当时推理机是由德克萨斯大学奥斯汀分校开发的。奥斯汀分校在知识表现和推理领域处于全球领先地位。然而，即使拥有这样复杂的技术框架，当时的推理机也只能达到 30% 到 50% 的正确率。相比之下，如果今天使用大模型来进行同样的推理过程，即便没有专业领域的支持，也能达到 60% 到 70% 的正确率。这种显著的提升是时代发展和技术进步所带来的结果。</p><p></p><h3>从反思到进化：大模型时代的知识系统构建</h3><p></p><p></p><p>在 2010 年，我对某个项目进行过复盘和思考，现在又过去了 14 年，让我们一起回顾一下在大模型时代，我们构建知识系统的方法，从中分辨哪些是正确的，哪些是错误的。</p><p><img src="https://static001.geekbang.org/infoq/62/62c5838a91b0934acb89384410889d1d.png" /></p><p>第一，我们考虑这种方法是否可能扩展到其他领域。按照当时的方法，这是不可能的，但今天通过大模型的方法，由于其通用性，扩展变得很容易。大模型的通用性意味着可以应用于化学、物理、数学、历史等多个领域，而不需要重新编写所有内容。</p><p></p><p>第二，我们考虑了处理大量数据的能力。当时这是不可能的，但今天已经可以了。</p><p></p><p>第三，当时使用了受控自然语言（Controlled Natural Language, CNL），而今天，我们不再需要这种语言，自然语言已经足够。</p><p></p><p>第四，如何更好地利用外部数据，比如链接数据（linked data）。当时没有明确的想法，但今天有了像 GraphRAG 这样的工具，它将知识图谱和大模型结合起来。</p><p></p><p>第五，如何综合使用多种方法，将规则方法和自然语言处理方法结合起来。当时没有找到很好的方法，但现在生成式模型已经普及。</p><p></p><p>第六，如何降低综合成本。当时不知道，但现在我们知道了预训练模型和提示工程可以大幅降低成本，至少下降了 100 倍。</p><p><img src="https://static001.geekbang.org/infoq/a2/a20f0ea1d5f81e11870dfece65acfcdf.png" /></p><p>第七，关于问答系统。2010 年前后，IBM 的 Watson 系统和 DeepQA 系统是问答领域的佼佼者，但当时还没有人使用深度学习，直到 2012 年深度学习才开始流行。到了 2017 年，我们开始知道如何融合这些技术，而今天，大模型方法已经轻松解决了这个问题。</p><p></p><p>关于如何结合信息检索方法和基于规则的方法，以及如何解决长尾搜索问题。当时不知道，但现在我们知道了 RAG 加上 Agent 可以解决这些问题。尽管有了统计方法和规则方法，结构化知识仍然不可或缺。</p><p><img src="https://static001.geekbang.org/infoq/7f/7f1680de016aa3a1c550b08bc3e2bac0.png" /></p><p>第八，我们考虑了知识图谱的必要性。尽管大模型提高了知识图谱的构建效率，但并没有取代它。</p><p></p><p>第九，知识建模不能忽略手工方法，知识编辑工具可能是关键。我在这方面的想法有误，但也没有完全错，因为数据生成、提示词交互、结果校验等过程仍然需要人机交互工具。</p><p></p><p>第十，我们考虑了如何平衡颗粒度和成本。现在我们知道，走向大模型是正确的方向。</p><p></p><p>从技术角度来看，20 年前的研发工作采用的是瀑布式开发模式，其成本远高于今天大模型的成本，可能高出 100 倍甚至更多。今天，大模型的最大好处在于，我们不需要在系统设计初期就将所有知识固定下来。相反，我们可以在知识使用时，通过数据的预训练和提示工程的交互来实时更新知识，实现全面的迭代。</p><p></p><p>以前的知识系统构建分为三个步骤：首先用统计方法打基础，然后用规则提高准确度，最后通过编辑过程进行精细化处理。现在，这三个步骤已经发生了变化。统计方法已经演进为大模型，而规则并没有消失，它们以一种新的形式存在。例如，当我们使用像 Coze 或 RAGFlow 这样的 Agent 平台时，编辑流程实际上就是规则，它们将传统规则转化为了工作流，工作流本质上就是规则。</p><p></p><p>即使大模型的输出准确度达到了 70% 到 80%，但在实际业务场景中，如合同检测，如果准确度没有达到业务人员的要求，他们是不会采用的。因此，为了使大模型的输出结果可用，我们必须做好后处理工作，包括数据交互和数据验证，以提高准确度满足业务需求。</p><p></p><h3>AI 落地的关键：务实的工程实践与基础建设</h3><p></p><p></p><p>AI 的落地关键在于工程，而不是单纯的科学或高深莫测的技术。大模型虽然为我们解决了一些基础问题，但它们并非万能的。在实际应用中，我们需要通过迭代和控制成本来解决剩余的问题。例如，即使 AI 系统的准确率达到了 85%，剩下的 15% 不足之处仍可能导致亏损。关键在于如何降低成本，如何提高那最后的 5% 或 10% 的准确率。</p><p></p><p>工程的本质在于处理细节和解决实际问题。工程的实现并不总是充满光环，它往往涉及枯燥但至关重要的底层系统工作。在工程实践中，我们总会遇到意想不到的问题，这些问题的解决方案可能同样出人意料。要进行有效的工程实践，关键在于关注小事，因为所有的工程成就都是由一系列小细节累积而成的。以瓦特改进蒸汽机为例，他所做的不仅仅是发明，而是通过改进传动机制、密封技术等多个小细节，最终实现了蒸汽机的高效能。同样，在计算机科学和人工智能领域，工程的重要性也不言而喻。</p><p></p><p>AI 不仅仅是理科，更是工科。工科教育强调实践，例如制作小锤子、操作车床、焊接等，这些都是工程的一部分。编写代码也是工程的一种形式，它需要质量检验、质量控制、多种工具的配合以及公差体系的校正。与机械工程或化学工程相比，软件工程还相当年轻，很多人对工程的真正含义理解不足。我们需要认识到，软件工程也需要遵循工程原则，包括严格的质量控制和精细的工艺流程。这是 AI 领域需要面对和解决的问题。</p><p></p><p>过去的几年里，人们都在追捧大数据和大模型。然而，无论是大数据还是大模型，核心问题并没有改变：数据清理。 十几年前，有句话说得好，“如果你解决了数据清理问题，就解决了 80% 的机器学习问题”。这个原则在今天依然适用，解决了数据清理问题，也就解决了 80% 的大模型问题。</p><p></p><p>智能体等技术其实并没有那么复杂。它们都是由一系列枯燥的基础工作构成的，只要这些基础工作做好了，问题也就迎刃而解。我们不应该盲目模仿像 OpenAI 这样的大型组织，使用数万张显卡去训练大模型。因为并不是每个组织都拥有这样的资源，有些可能连 100 张，甚至 10 张显卡都没有。每个组织都应该根据自己的实际情况，采取适合自己的方法。</p><p></p><p>大模型本质上是从数据中提取的知识，而知识可以被视为“小数据”，它强调的不是规模，而是价值。这里的“小数据”有三个特点：价值（Value）、真实性（Variety）和多能性（Versatility）。</p><p></p><p>价值，不是连垃圾都存起来，而是特别关心数据的价值密度，提高投入产出比。真实性关心数据的可验证性，可用性，自描述性等。多能性意味着知识不应被固定在代码中，而应存在于数据中，能够灵活应用。</p><p></p><p>大模型是一种知识模型，它代表了一种数据的高级形式，承载了丰富的知识。要实现大模型的有效落地，我们需要从基础做起，务实地处理运维、数据库和数据清洗等基础工作，并逐步演化和优化：</p><p></p><p>如何按天迭代？如何构造联调系统？如何无标注数据启动？如何分离准确度和召回率要求？如何统一运用规则和大模型？如何适应无明确衡量标准的开发？如何设计可演进的数据模式？如何提升数据可理解性？如何逐步提升规则 /Agent flow/RAGFlow 系统的表达力？如何平衡黑箱和白箱模型的优缺点？如何在优雅架构和工期间取舍？</p><p></p><p>所有伟大的成果，之所以能够取得优异的成绩，都是因为一线工程师的辛勤工作。这些成绩并非来自于向领导汇报的表面文章，而是真正在一线发挥作用的扎实工作。这些工作的核心是务实，即所谓的“土”。</p><p></p><p>回顾过去，2010 年时的知识工程水平大致相当于软件工程的 1940 年代。到了 2017 年，我开始涉足金融领域应用时，软件工程的水平相当于 1950 年代。如今到了 2024 年，我认为知识工程的水平已经发展到了大约 1970 年代的软件工程水平。 在 1970 年代，高级语言 C 语言被发明，而 Python 是在 1992 年左右发明的。我们今天熟悉的许多高级语言，如 Java、PHP 等，大多是 90 年代的产物。当我们审视知识工程领域时，我们发现并没有出现类似 C 语言这样的基础性、革命性的语言。所以说，知识工程领域仍有很大的发展空间，需要我们继续探索和创新。</p><p></p><h3>提示工程，开启知识建模新篇章</h3><p></p><p></p><p>提示工程的意义在于它为知识建模提供了一种自然语言的表达方式，这是一种革命性的进步。在金融等专业领域的应用中，传统的知识工程和专家系统方法成本过高，导致如 Aura 和 Halo 这样的系统最终失败。然而，从 2022 年开始，我们找到了一种新的方法，让我们能够享受到软件工程在 70 年代所体验到的便利，那就是声明式编程语言。</p><p></p><p>在编译原理课程中，我们了解到有两种类型的编程语言：声明式语言和命令式语言。SQL 因其声明性质而广受欢迎，而直接使用机器码则是一种典型的命令式编程。在知识建模方面，我们之前使用的如 SILK、OWL 或 RDF 等语言，可以看作是接近机器语言的低级语言。我们一直在寻找一种过渡，一种对人类更友好的知识建模语言。</p><p></p><p>过去，人们尝试使用受控自然语言（CNL）如 Aura 项目，但效果并不理想。而提示工程的出现，使我们能够直接使用自然语言进行交互，这是非常神奇的，也是提示工程的核心价值所在。</p><p><img src="https://static001.geekbang.org/infoq/aa/aadb1bab9a91c1d6307cf3bdaf63b1e0.png" /></p><p>企业级信息系统架构经过多年的发展，从 OA 系统到大数据、互联网、数据湖、数据中台等，其核心主线始终围绕着如何让机器自动理解多源异构的分布式数据，以及如何将分散在不同员工大脑中的知识集中起来，使公司管理者能够掌控全局。数字化转型的本质在于打破组织内的数据边界，让知识流动起来，成为组织的资产。</p><p></p><h3>基于大模型的新范式：无代码，无标注，强泛化</h3><p></p><p></p><p>大模型带来了一种新的范式，它具有无代码、无标注和强泛化的特点，这在以前是难以想象的。大模型的核心作用在于显著降低了开发和应用的成本。</p><p></p><p>在过去，要实现特定的业务流程，比如与券商合作时，我们需要构建底层数据仓库，然后开发各种自动化系统，包括自动化写作、核查、问答和大屏展示等。这些系统往往各不相同，需要大量的定制化开发。大模型的出现使得我们可以有一个统一的基础平台来实现所有这些功能，而且不需要在设计阶段就将所有的业务知识全部预设进去。知识可以在使用过程中不断演化和完善。这种能力在以前是不存在的，它相当于将我们带到了一个新的境界。到了 2022 年，我们发现大模型真的可以实现这样的功能，这确实是一次世界观的颠覆。我们意识到，技术的发展可以如此之快，可以以这样一种全新的方式解决问题，是具有革命意义的。</p><p></p><h3>面向知识的编程，破解第三次软件危机？</h3><p></p><p></p><p>我也认为，大模型的出现解决了所谓的第三次软件危机。回顾历史，第一次软件危机大约发生在 50 年前，当时的问题集中在无结构的 GOTO 语句上。随后，数据结构的引入帮助我们解决了这个问题，我们学会了将程序视为数据结构和算法的结合。</p><p></p><p>第二次软件危机则是面向对象编程 (OOP) 和互联网的兴起，这可以看作是面向对象的扩展，因为互联网编程很大程度上是以数据库为中心的。</p><p></p><p>近十年来，我们面临第三次软件危机：软件变得极其复杂和高度分布式。例如，在 Web 领域中，智能合约的出现代表了一种新的软件工程形式。在银行等金融机构，进行数据中台建设和数字化转型的过程中，许多人感受到了图数据库的矛盾——既觉得它有用，又觉得它的表达能力受限，给人一种“食之无味，弃之可惜”的感觉。这种矛盾体现了当前软件工程方法与应用需求之间的脱节。</p><p></p><p>我认为第三次软件危机的解决方案是一种新的编程范式——面向知识的编程 （Knowledge Oriented Programming, KOP）。在这个范式中，编程的核心是推理(Reasoning)和知识库(Knowledge Base)，这可以看作是现代版的算法和数据结构。这里的知识库就是大模型，大模型的推理能力，不同于传统的推演 (Deduction)，是一种新的推理方式。大模型作为知识库，本质上承载的是知识而不仅仅是数据集。</p><p></p><p>这个核心的范式与传统的软件开发方法有显著的区别。传统方式，比如 Aura 项目的 style，采用的是瀑布模型。在这个模型中，首先定义好数据的 schema，然后业务规则也是事先设定好的。以 Aura 为例，它需要事先确定如何进行物理或化学的考试，这些业务知识是预先定义的，用户界面 (UI) 也是固定不变的。这种方式是传统的业务分析方法，也是瀑布式开发方式，它要求预先设定好所有要素。</p><p></p><p>我们理想中的另一种范式是端到端范式。在 2022 年到 2023 年上半年，许多人开始尝试使用大模型进行端到端的开发，希望把所有的数据输入到模型中，然后期待模型能够神奇地解决所有问题。 这种方法的想法是，不再需要重新定义 schema，不需要构建知识图谱，所有的中间过程都可以是多模态的，从图像到文本到语音、OCR、PDF 等都能一次性处理完毕。</p><p></p><p>但实践中发现，端到端的方式行不通。以金融领域为例，它面临的问题包括准确度不可靠——例如，深交所的公告处理系统要求 99.99% 的准确度，这在当前是任何大模型都无法达到的。此外，生成式模型缺乏幂等性，即相同的输入不能保证每次都得到相同的输出。还有处理速度慢和成本高的问题，例如原本 20 分钟完成的 IPO 审核任务，如果使用大模型可能一天也完成不了。</p><p></p><p>端到端模式在实践中被证明是不可取的。尽管我们相信，如果未来的硬件成本大幅降低，或者相关的技术平台有显著进步，端到端模式可能会变得可行，但至少在目前，这种方式是不现实的。</p><p></p><h3>提示工程的系统化构建</h3><p></p><p></p><p>我们需要实现一种称为“活的业务分析”的系统范式来进行知识建模。这种范式的核心在于数据本身应该是动态的，即数据抽取应该基于提示工程。业务分析过程也应该基于提示工程，这意味着数据产生、schema 定义以及业务规则的生成都应该是即时的（Just In Time, JIT）。</p><p></p><p>最终，这些即时产生的元素结合起来形成一个应用系统，这个系统本身也最好是即时构建的。目前，许多公司正在开发基于对话的商业智能（Conversational BI），其本质是设计一种“活”的用户界面（UI）。</p><p></p><p>提示工程本质上是软件工程的一部分。编写提示词也是软件工程活动，而不仅仅是编写 Python 代码。未来可能会出现一个专门的提示工程专业，并有专门的集成开发环境（IDE）来支持这项工作。提示工程应该有其自身的复杂业务逻辑和质量控制系统。</p><p></p><p>这也符合精益开发的流程，遵循 learn-build-measure 的循环模式，这里的 measure 指的是对构建出的系统或产品进行广泛的评估和测试。基于评估结果，我们进一步进行优化，然后再返回到构建阶段，形成一个持续的循环。</p><p></p><p>在没有实际执行之前，人们可能会对大模型的工作方式有一种“你以为”的理解，但真正参与到工程落地系统后，会发现整个过程极其复杂。我们开发了一种可行的提示词设计方法论，称为 S2PI 方法论，它包含四个要点。</p><p></p><p>Schema（结构）：虽然我们不要求事先定义完整结构，但在大多数问题中，仍然存在一些固定的数据要素。因此，我们需要根据特定场景设计提示词的基本结构。</p><p></p><p>Supplement（补充）：在基础架构的基础上，根据特定场景增加背景信息。这些背景信息可能包括正样本、负样本，或是关键架构要素的变化形式。在软件工程中，我们通过回归测试来确保质量，而在提示工程中，设计各种补充信息的过程实际上就是在构建测试集和引导集。</p><p></p><p>Property（属性）：涉及与特定领域（如金融）相关的属性和术语。从工程角度来看，这类似于设计实体关系（ER）图。</p><p></p><p>Input（输入）：在使用时刻，根据设计阶段的架构和补充信息，确定所需的具体输入。工业级别的提示词设计不是简单的民用级别，而是一个复杂的过程。</p><p></p><p>我们还专门开发了一项技术，称为“提示词编译”。S2PI 方法论设计出的提示词是一种高度技术化的语言，普通人难以理解。因此，我们设计了一些 Agent，它们能够将普通人能理解的提示词翻译成更底层、专业的提示词。</p><p></p><h5>综合案例 1：基于提示工程的文本抽取</h5><p></p><p></p><p>在讨论基于提示工程的文本抽取时，我们首先需要理解 JIT 数据生成的重要性。以三个大型交易所的审核系统为例，这些系统负责从厚重的招股书中抽取数据，这涉及到大量自然语言处理技术。从 2023 年开始，我们交付给交易所的新一代系统都采用了大模型技术。</p><p></p><p>在这个过程中，有很多关键的技术细节。例如，情境学习需要定义各种角色，但角色数量的确定是一个问题。一个招股书包含 1 万多个数据点，2400 个不重复的 schema 和 94 个章节，这是否意味着需要定义 94 个 Agent？这仅仅是针对招股书一种文档类型，还有债券募集说明书、IBS 专项说明书、定调报告、评级报告、征信报告、资管合同等其他文档类型，以及底层资产类型如 ABS、中票、短融、公司债、企业债、利率债等，构成了一个庞大的矩阵。在输入 PDF 文件进行抽取时，需要进行 PDF 解析，确保每个章节、每个段落在抽取时获得适当的辅助信息（supplement 信息）。例如，为公告增加分类信息可以显著提高准确率。通过改变提示词，将提取结构化信息的机器人转变为专业提取董事会决议的模型，可以在四种公告上分别提高 3% 到 8% 的准确率。</p><p></p><p>我们还发现了一些技巧，比如在输出时要求不要有幻觉，结果真的降低了幻觉的出现。通过给每个公告增加例子和分类，可以进一步提高准确度 3.85% 到 3.87%。我们尝试了各种方法，仅在提示工程层面上所做的工作就提高了 13.8% 的准确度。</p><p></p><p>最令人惊讶的是，所有这些工作并不是由传统的软件工程师完成的，而是由一个提示工程实验室完成的，实验室的成员平均年龄 23 岁以下，很多是 00 后文科生，财经专业毕业生或在校生，甚至实习生，能够写中文并具备逻辑思维能力，就能实现这样的成果。这表明 提示工程极大地降低了技术门槛，使得非传统意义上的工程师也能参与到优化工作中来。开源系统和闭源系统我们都使用过，效果相当，到目前为止，我们并没有看到闭源系统明显优于开源系统。</p><p></p><h5>综合案例 2：基于提示工程的业务建模</h5><p></p><p></p><p>案例 2 我分享的是如何利用提示工程进行业务建模，尤其是在金融系统中。金融系统包含众多规则，包括财务规则、法务规则、核查规则以及业务流程管理（BPM）规则等。核心问题是如何降低业务建模的成本，使其不需要程序员编写，而是可以由 23 岁的文科生完成。</p><p></p><p>在大模型出现之前，业务建模遵循的是瀑布式开发流程。业务分析师，通常是金融专业出身，会手工整理业务规则，这些规则基于他们对原始业务文件的深入分析。然后，这些业务规则会转化为产品需求文档（PRD），再传递给产品工程师和软件工程师，整个过程耗时较长。</p><p></p><p>现在，我们的目标是利用业务规则自动生成底层提示词及其效果，实践表明这种方法大约有 85% 的可用性，剩余的 15% 可以通过其他方法解决。更进一步，我们尝试不依赖人工理解业务逻辑，而是通过给系统输入 20 份文档，让它自行整理出业务规则。经过两个月的实验，目前这种方法是可行的，预计到年底可以更加完善。</p><p></p><p>在金融领域，核查系统已经从静态变为动态。金融业务逻辑处理的核心是将业务知识进行建模。与过去的方法相比，现在的方法大幅降低了成本，原因在于我们不再需要在设计阶段做大量前期工作，从而使系统上线后难以演化。大模型的最大价值在于赋予了系统强大的可演化性。提示工程已经替代了大量的传统软件工程任务。尽管这种实践才一年多时间，但随着时间的推移，我们可以预见到提示工程和大模型在未来将有更强大的应用出现。</p><p></p><h5>嘉宾介绍</h5><p></p><p>鲍捷博士，文因互联董事长、首席科学家、创始人。爱荷华州立大学（Iowa State University）博士。曾任伦斯勒理工学院（RPI）博士后，麻省理工学院（MIT）分布式信息组（DIG）访问研究员，三星美国研发中心研究员，W3C OWL(Web 本体语言) 工作组成员，参与撰写了 OWL2 知识图谱语言国际标准。现任 W3C（万维网联盟）顾问委员会委员、中国中文信息学会语言与知识计算专业委员会委员、金融知识图谱工作组主席、中文开放知识图谱联盟 (OpenKG) 发起人之一，国际 Data Intelligence 杂志编委，中国科学技术大学国际金融学院业界导师。</p><p></p><h5>内容推荐</h5><p></p><p>大会 PPT 获取通道已开启，关注数字化经纬公众号，后台回复“PPT”，即可获取 PPT 下载地址（由于讲师所在企业限制，部分 PPT不对外公布，详情见大会官网日程）：<a href="https://ppt.infoq.cn/list/149">https://ppt.infoq.cn/list/149</a>"</p><p><img src="https://static001.geekbang.org/infoq/62/627ac83fbc862f8bfdbf46735876031d.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BXpDJBmdezFT1crOZ5H8</id>
            <title>一张美食图就能给菜谱、能给植物看病……阿里国际发布最新多模态大模型Ovis</title>
            <link>https://www.infoq.cn/article/BXpDJBmdezFT1crOZ5H8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BXpDJBmdezFT1crOZ5H8</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Sep 2024 02:58:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>看一眼菜品图就知道怎么做、能给植物看病、能把手写英文准确翻译成中文、还能精准分析财报数据……多模态能力再次升级！今天，阿里国际AI团队发布了一款多模态大模型Ovis，在图像理解任务上不断突破极限，多种具体的子类任务中均达到了SOTA（最新技术）水平。</p><p>&nbsp;</p><p>多模态大模型能够处理和理解多种不同类型的数据输入，例如文本、图像。与大型语言模型（LLMs）相比，大语言模型在处理和生成文本数据方面有专长，而多模态大模型能够处理非文本数据，如图像等等。</p><p>&nbsp;</p><p>根据多模态权威综合评测平台OpenCompass的数据，Ovis1.6-Gemma2-9B在30B参数以下的模型中取得了综合排名第一，赶超MiniCPM-V-2.6等行业优秀大模型。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/dd/9b/dd3ca8b992c326736b5c3fa26e31819b.png" /></p><p>图：Ovis在OpenCompass上的测评数据情况</p><p>&nbsp;</p><p>据介绍，Ovis能够在数学推理问答、物体识别、文本提取和复杂任务决策等方面展现出色表现。例如，Ovis可以准确回答数学问题，识别花的品种，支持多种语言的文本提取，甚至可以识别手写字体和复杂的数学公式。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/0e/6a/0e496b877a6f95bce28ec4fb41109b6a.png" /></p><p>案例1:Ovis对手写文案的识别及翻译能力</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/02/1e/02154dd3b3b51196e978bde0b1ef6b1e.png" /></p><p>案例2：Ovis对复杂数学公式的处理能力</p><p></p><p><img src="https://static001.infoq.cn/resource/image/02/1e/02154dd3b3b51196e978bde0b1ef6b1e.png" /></p><p>案例3:Ovis通过对图片的识别处理能够给出菜谱</p><p>&nbsp;</p><p>具体来说，Ovis模型有五大优点：</p><p></p><p>1、创新架构设计：可学习的视觉嵌入词表：首次引入，将连续的视觉特征转换为概率化的视觉token，再经由视觉嵌入词表加权生成结构化的视觉嵌入，克服了大部分MLLM中MLP连接器架构的局限性，大幅提升多模态任务表现。</p><p></p><p>2、高分图像处理：动态子图方案：支持处理极端长宽比的图像，兼容高分辨率图像，展现出色的图像理解能力。</p><p></p><p>3、全面数据优化：多方向数据集覆盖：全面覆盖Caption、VQA、OCR、Table、Chart等各个多模态数据方向，显著提升多模态问答、指令跟随等任务表现。</p><p></p><p>4、卓越模型性能：Ovis展现出了优异的榜单表现。在多模态权威综合评测Opencompass上，Ovis1.6-Gemma2-9B在30B参数以下的模型中取得了综合排名第一，超过了Qwen2-VL-7B、MiniCPM-V-2.6等模型。尤其在数学问答等方向表现媲美70B参数模型；在幻觉等任务中，Ovis-1.6的幻觉现象和错误率显著低于同级别的模型，展现了更高的生成文本质量和准确性。</p><p></p><p>5、全部开源可商用：Ovis系列模型License采用 Apache 2.0。Ovis 1.0、1.5的数据、模型、训练和推理代码都已全部开源，可复现。Ovis1.6系列中的Ovis1.6-Gemma2-9B也已开源权重。</p><p></p><p>在AI领域，多模态大模型的应用场景非常广泛，包括但不限于自动驾驶、医疗诊断、视频内容理解、图像描述生成、视觉问答等。例如，在自动驾驶领域，多模态大模型可以整合来自摄像头、雷达和激光雷达的数据，以实现更精准的环境感知和决策。由于多模态大模型能够学习如何联合理解和生成跨多种模式的信息，也被视为朝向通用人工智能的下一个步骤。</p><p>&nbsp;</p><p>根据此前媒体报道，阿里国际在去年成立了一支AI团队，目前已经在40多个电商场景里测试了AI能力，覆盖跨境电商全链路，包括商品图文、营销、搜索、广告投放、SEO、客服、退款、店铺装修等，其中多个应用场景均基于Ovis模型进行开发，已帮助50万中小商家、对1亿款商品进行了信息优化。据介绍，商家的AI需求不断增长，近半年的数据显示，平均每两个月，商家对于AI的调用量就翻1倍。</p><p></p><p>附相关链接：</p><p>论文arXiv: <a href="https://arxiv.org/abs/2405.20797">https://arxiv.org/abs/2405.20797</a>"</p><p>Github:<a href="https://github.com/AIDC-AI/Ovis"> https://github.com/AIDC-AI/Ovis</a>"</p><p>Huggingface: <a href="https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-9B">https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-9B</a>"</p><p>Demo: <a href="https://huggingface.co/spaces/AIDC-AI/Ovis1.6-Gemma2-9B">https://huggingface.co/spaces/AIDC-AI/Ovis1.6-Gemma2-9B</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</id>
            <title>我在构建 MLOps 系统四年中学到的经验</title>
            <link>https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hy8Mpu6pEDxPHLubX9oK</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Sep 2024 09:45:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>正如标题所述，我致力于构建 MLOps 系统已经有近四年了。世界变化得很快，作为一名也有了四年经验的程序员，我感觉自己一直在努力不被深度学习（LLM）的新技术淹没，努力适应软件工程，努力远程获得好公司的好职位，等等。</p><p>&nbsp;</p><p>这篇文章一半是对我多年经历的审慎回顾，另一半是我对工程、机器学习（运维）的看法。我想你的脑海中也曾浮现过这些问题，但我不会回答任何问题，只是分享我的观点。</p><p></p><h2>第一个 ML 问题，提前一日的用电量预测</h2><p></p><p>2021 年，我开始研究能源消耗模型，这是我第一次真正深入研究运维应用程序。这个问题一开始很简单：我们必须预测八个城市每 24 小时的每日用电量，但要提前 24 小时，这个项目名为日前电力预测。自从我开始研究这个问题以来，用户数量不断增加，普通消费者开始使用更多能源，新工厂也正在建设中。能源需求也会随着经济波动而变化。这一切都意味着更多模型漂移和数据漂移。</p><p>&nbsp;</p><p>一段时间后，我定下了预测模型所用的一系列算法。在尝试了深度学习模型和基于树的模型后，我发现最成功的是 LightGBM 和 XGBoost（以及集成深度学习模型）。</p><p>&nbsp;</p><p>基于树的模型的问题在于，它们的预测通常受你提供的数据的限制。例如，如果你尝试预测今年夏季的第一天，而去年同日的峰值消耗为 600 单位，由于基于树的模型的性质，除非你正确地定制问题，否则你的预测值是不会超过 600 单位的。但是，当事件和特征稳定且输出分布没有太大变化时，基于树的模型也可以提供出色的预测结果。</p><p>&nbsp;</p><p>说实话，我当时的大部分经验都来自 Kaggle。我在那里待了大约九个月，深入研究预测竞赛、创建新特征并尝试不同的模型。我提出的模型得分为 7/10，我知道如果我与真正的专家竞争，他们可能彻底击败我的模型。</p><p></p><h2>意识到需要 MLOps 系统</h2><p></p><p>预测未来的能源消耗是一种奇怪的体验，尤其是现实中有太多因素在影响电力使用——天气、周末与工作日、假期，甚至祈祷时间。在像土耳其这样四季分明的地方生活又增加了一层复杂性。我不是一个追求完美的模型制作者，我努力让自己的模型适应突然的变化。例如，如果连续两个月的气温都稳定在 25°C，然后温度突然在一天内下降 5°C，模型通常会假设电力消耗将与之前的 25°C 天气差不多，但事实并非如此。人们的习惯会随着季节而改变，预测这些微妙的变化会很困难。然后，还会有像国家足球比赛这样不可预测的事件——祝你建模好运！</p><p>&nbsp;</p><p>那么，我做了什么工作来解决模型和数据漂移问题呢？在过去的一年里，我创建了数百个特征，使用 XGBoost 和 LightGBM 构建了一些基于树的模型，并用前五日的数据来验证它们。 MLFlow 是日常模型生产过程中的主要工具。虽然它可能不是验证模型的完美方式，但它确实改善了长期预测结果。至少，当电力消费比较稳定且模型适应性良好时，它给了我一个安全的选择。</p><p>&nbsp;</p><p>为了简化流程，我建立了一个完整的系统，每天早上通过 API 提取数据、生成特征、挑选模型并进行预测。我构建的自动化脚本只用一分钟就能跑完。然而即使有了自动化脚本，你也必须密切关注预测结果，特别是在国定假日或意外事件期间。</p><p></p><h2>虽然你部署了一个模型，但你可能不喜欢它</h2><p></p><p>我的一段奇怪的经历就是和我的经理争论预测结果中突然出现的漂移。他认为，如果夏天天气突然下降，预测第二天的电力消费会更容易，因为运行的空调数量会减少。他甚至手动调整了几次预测，但结果适得其反。这基本上结束了我们的争论。虽然我相信有些日子通过手动调整可以带来更好的预测结果，但模型经常会发现一些我们没有发现的模式。</p><p></p><h2>ML 人员在软件工程领域的沉浮</h2><p></p><p>后来，我转而从事医疗保健领域的 MLOps 平台工作。我花了很多时间寻找可以从事 MLOps 的工作，结果很幸运地找到了一家医疗保健初创公司，我成为了那里的第一位全职工程师。我有医疗保健模型方面的经验，所以感觉这个行业很合适。我找到了他们，他们也找到了我，这纯属运气。</p><p>&nbsp;</p><p>在那里工作的三年是一段充满挑战的旅程，主要因为我从专注于模型转向了编写平台。我一直是那种喜欢做研究的人，我会实现各种论文了结果，并尽可能多地从 Kaggle、众多教授和学者那里汲取知识。举例来说，我对深度学习模型和表格模型之间的差异很着迷，尤其好奇为什么深度学习应用在处理表格数据时经常遇到困难，而基于树的模型却表现出色。我读过论文，在 X/reddit 上讨论这个话题，我就是那种人。但当我转而编写平台时，我才意识到自己还需要学习很多关于编写生产级代码的知识。一开始我搞砸了很多事情。</p><p>&nbsp;</p><p>在一开始的几个月里，我们在 Jupyter 笔记本中进行模型评估，我阅读了一些有关如何深入评估医疗保健模型的论文，也考虑到了性别和种族偏见。工作一开始很有趣，但后来我们必须将所有内容集成到一个平台中。那时我慢慢开始了解面向对象编程、系统设计和传统软件工程的原理。我不是计算机科学工程师，所以我是一点一滴学会了这些东西。</p><p>&nbsp;</p><p>创业生活非常紧张，工作时间长，学习曲线陡峭。我们构建的系统使用了 MongoDB、Python、RabbitMQ、S3 和 AWS——一个相当标准的管道。我们的平台旨在验证医疗保健模型，获得 FDA 的批准，并确保一切都正确完成。数据来自合作伙伴，但模型供应商从未看过原始数据，因为他们不应该有这个权限。因此，我们的业务目标是根据获得的黑箱数据来验证模型，并为 FDA 准备必要的文件。</p><p></p><h2>MLOps 平台与业务逻辑，我们是只部署模型还是为客户提供服务？</h2><p></p><p>为了让我们的平台实现目标，它需要支持所有类型的医学图像、验证任何计算机视觉模型并检测出可能存在的模型偏见。三年来，这个平台关注的重点也在改变。第一年，我们的目标是部署和验证模型。第二年，我们增加了注释功能，支持了医疗保健数据，并实现了云集成。到第三年，我们意识到我们需要关注客户的一些特定需求。</p><p>&nbsp;</p><p>挑战之一是将平台的逻辑与特定于客户的代码库分开来。我们花时间编写了特定于客户的代码，这些代码会让平台上 80% 的功能受益，但有时我们必须为特定客户实现非常具体的逻辑。这引发了很多关于我们是否应该用特定于客户的功能来增强平台，还是将它们分离开的争论。如果用太多特定于客户的功能来增强平台，它可能会变得臃肿和混乱。另一方面，当客户端代码库需要访问平台数据时，将它们分离开可能会导致复杂的情况。</p><p>&nbsp;</p><p>我还是没法确定如何区分 MLOps、MLE、后端工程和业务逻辑。可能这没有唯一的答案，但我认为我们在维护和开发平台方面做得很好。</p><p></p><h2>在云端部署 vs 在本地部署</h2><p></p><p>最近我参加了银行业的 MLOps 职位面试，我 90% 的时间都在写代码。面试我的经理是一位从 DevOps 过渡到 MLOps 的人，我相信他已经开发了一些 ML 模型。对他来说，模型只是具有特定输出的 docker，你还需要做管理/跟踪/记录的工作。对于一些人来说，这才是真正的 MLOps 工程。我绝对同意这一点。</p><p>&nbsp;</p><p>他的团队正在 Apache Airflow 上部署模型，他问我在这方面有什么经验。我想到的是：</p><p>&nbsp;</p><p>正如问题所暗示的那样，我在电力问题上训练/部署的模型是日常模型，所以我不需要跟踪模型，做预测就行了。我不需要检查模型是否一直处于活动状态，也不需要检查吞吐量或延迟。我在医疗保健 MLOps 问题中使用的模型是基于项目的模型，不需要不断构建模型，而且它是本地的，所以安全不是主要问题。</p><p>&nbsp;</p><p>他的团队一直完全不写代码，至少在目前是这样。在面试过程中我意识到了这一点，这很奇怪。</p><p></p><h2>身份危机：MLOps 工程师、ML 工程师，还是两者兼而有之？抑或其他？</h2><p></p><p>多年来，另一个问题一直浮现在我的脑海中：我是谁？我是 MLOps 工程师、ML 工程师、ML 研究员还是后端工程师？在一个小团队或初创公司中，你需要具备所有这些能力。你听说过“10x 工程师”吗？那些奇怪的职位要求你同时发布 NeurIPS 论文和 Node.js 代码？伙计，这到底是为什么？我见过很多这种奇怪的组合。不是吹牛，但下面这些都是我所做的工作：</p><p>&nbsp;</p><p>预印我在斯坦福实习期间的一篇论文 = ML 研究员编写表格和 CV 模型 = 数据科学家实现一个 Auto-ML 模块，允许即时训练/微调对象检测、分割和分类模型，并进行版本控制 = Python 软件工程师、ML 工程师使用 Grad-CAM 为 CNN 模型开发一个可解释性库 = ML 研究员使用 FastAPI 支持模型，与前端应用程序集成 = 后端工程师使用 Docker、RabbitMQ、MongoDB 和其他工具设计和实现一个平台。= MLOps 工程师</p><p>&nbsp;</p><p>当然这些术语是可以互换的，当然优秀的 MLE 必须是一名优秀的软件工程师，当然你需要考虑如何在训练模型时部署模型，但我到底是什么角色？我觉得我被“学得更多，收获更多”这句话欺骗了。</p><p></p><h2>10x 工程师神话：什么都会，某些事情还要精通（也许做不到）</h2><p></p><p>那么我从中得到了什么？我是不是一名没什么专长的 10x 工程师？好处是，我可以申请数据科学、MLOps、MLE 和 Python 后端职位，这里的重点是 ML。而且我参加了所有这些职位的面试。但问题是什么？在面试过程中，他们并不总是相信我的经验广度。他们会问很多问题，即使我有很好的答案，一些面试官也会试图让我出局。如果你没有给出他们想要的具体答案，你就出局了。</p><p>&nbsp;</p><p>我在一家全球数据科学公司的面试中就遇到过这种情况。对于我提到的所有话题，他都会质问我，还会不断切换话题背景，让我觉得自己像个白痴。当他发现我在 3 小时的面试后给出了一个不太好的答案时，他说：“我就知道。”我都无语了，你知道什么？当然，我不是精通所有领域的专家，但这也很让人不爽。</p><p>&nbsp;</p><p>还有一次，我被一个非常聪明的人面试了——他曾经在知名公司工作，在排名前十的大学获得博士学位。他非常聪明，我希望十年后能成为他那样的人。他很善良，我非常尊重他。我告诉他我的经历和我做过的事情，并问我还能做些什么。但你猜怎么着？他希望我在 LLM 方面有更多经验，他说：“我没有投资潜力股的预算；我需要员工在那个特定主题方面有足够经验。”没错，但这样的话我该怎么办？我应该追随每一个深度学习趋势并以赚钱为目的研究它们吗？这有多大可能？你能对一个只有四年经验的人能有什么期望？</p><p></p><h2>我将来想成为什么样的人？</h2><p></p><p>我不知道。我真的不知道。我知道的是：</p><p>&nbsp;</p><p>FOMO（错失恐惧）在 ML 领域中是真实存在的。如果你尝试学习 MLE，那么 MLOps、软件工程或者一些 DevOps 方面的知识和经验可能不会让你更有信心。</p><p>&nbsp;</p><p>原文链接：<a href="https://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html">https://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</id>
            <title>国内近 50 款 AI Agent 产品问世，技术足够支撑应用可靠性了吗？</title>
            <link>https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9lBoqN5m9lmeyeFmY8EF</guid>
            <pubDate></pubDate>
            <updated>Tue, 17 Sep 2024 02:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在2024年5月发布的《中国AGI市场发展研究报告》中，InfoQ研究中心将 AI Agent 定义为连接模型层与应用层的中间层，是现阶段大模型落地应用的重要补充。那在过去的两个季度，AI Agent 领域发生了诸多变化，本文希望通过分析技术框架、理想与现实的差距，以及厂商背景，为大家提供对AI Agent现状的全面理解。</p><p></p><p>更多关于 AI Agent 的具体应用案例，欢迎点击<a href="https://www.infoq.cn/minibook/bTgj82D3gFJK9ZLRM5Ci">「链接」</a>"下载完整报告。</p><p></p><h3>AI Agent技术框架趋于统一</h3><p></p><p></p><p>自 2023 年 3 月起，以 AutoGPT 为代表的一系列技术框架发布后，AI Agent 凭借其自主性和问题解决能力，迅速成为科技圈讨论的焦点。在随后的时间里，技术领域陆续推出了多种 AI Agent 技术框架，涵盖通用、环境模拟、软件开发、多模态、翻译、终端交互、数据分析等多种类型。同时，关于单智能体与多智能体的讨论也在持续。</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/834fcc697a798b2e2094e5a9c36a18a0.png" /></p><p></p><p>在技术框架的不断探索中，AI Agent 的技术框架认知逐渐统一。大模型作为智能体的大脑，指导规划、工具使用、记忆三大基本能力模块具体行动。并在具体行动过程中，通过与环境、其他智能体以及人类的交互反馈，促进智能体的不断进化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14e4813caf4292545d135c6bd49bc2ef.png" /></p><p></p><p></p><h3>大模型「大脑」足够聪明到支撑AI Agent落地了吗？</h3><p></p><p></p><p>从技术框架的角度，我们可以看到大模型在智能体中的重要性，这也引发了一个关键问题：大模型「大脑」是否足够聪明以支持 AI Agent 的实际落地？</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb9b1ff210b4c8bb46d43f45a213ebde.png" /></p><p></p><p>作为智能体的大脑，大模型在短短两年内经历了三次主要更新和竞争重点的转变。然而，针对工具调用或真实环境模拟的国内外测试结果显示，当前大模型的表现仍不尽如人意。例如在 WebArena 测试中，GPT-4的成功率也仅有 14.9%，今年发布的 GPT-4o 也并没有获得明显提升。</p><p>注：WebArena通过构建一个智能体命令和控制环境，通过对大模型在电子商务、社交论坛、软件开发协作和内容管理四类环境中一系列评估任务的功能正确率进行评估。网址：<a href="https://webarena.dev/">https://webarena.dev/</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0e20b32ee4aa49103929a263a6b4adf6.png" /></p><p></p><p>此外，在T-Eval基准测试中，各大模型在推理得分方面普遍偏低且模型间差距明显。</p><p>注：T-Eval大模型智能体基准测试，是专门针对智能体工具使用的全过程设计的基准测试，包含：规划（Plan）、推理（REASON）、检索（RETRIEVE）、理解（UNDERSTAND）、指令跟随（INSTRUCT）和审查（REVIEW）。</p><p><img src="https://static001.geekbang.org/infoq/66/6628a99507688e9badec3003ac0a7697.png" /></p><p></p><p>在本次报告的访谈中，来自一线的专家也提及，当前大模型在任务拆解和规划能力方面仍存在明显不足。现阶段，依赖大模型进行独立思考和自主规划路径的方式，尚不足以确保智能体的可靠性和任务成功率。</p><p></p><h3>理想中的智能体和现阶段有哪些差距？</h3><p></p><p></p><p>除了规划能力与理想状态存在一定差距外，InfoQ研究中心还从自主思考、工具调用、记忆和多模态理解等方面，深入分析了理想中的智能体与现阶段智能体之间的差距。这样的技术现状也对 AI Agent 的开发与应用提出了更高的要求，迫使技术团队不断优化系统的可靠性，以实现更加全面的任务执行能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e715fe5812e09f0a213b0e0e6abe5adf.png" /></p><p></p><p></p><h3>目前中国市场中，有哪些AI Agent产品已经面世？</h3><p></p><p></p><p>InfoQ研究中心还发现，目前在各个领域，已有众多不同类型的AI Agent产品面世，并且不同的产品从例如工作流等不同的方面提供了技术解决方案。因此InfoQ研究中心也从平台类和垂直类的角度出发，盘点了近 50 个中国市场中的 AI Agent 产品，并形成《中国 AI Agent 产品罗盘》。</p><p></p><p>《罗盘》仍将持续更新，欢迎各位开发者和读者朋友们积极反馈和持续关注，也欢迎各类厂商参与交流，与InfoQ研究中心分享技术和产品的最新动态（联系方式：InfoQ研究中心首席分析师 姜昕蔚：18618257676）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a7943dfbc3672f5546c807c4b3b22dc.jpeg" /></p><p></p><p></p><h3>参与AI Agent市场竞争的厂商背景如何？</h3><p></p><p></p><p>除了产品盘点外，InfoQ研究中心在对市面上对外提供 AI Agent 服务的厂商进行研究，并发现其背景主要分为大模型创业厂商、互联网科技厂商、RPA/流程自动化厂商和数字化企业服务商。</p><p></p><p>大模型创业厂商：以 Dify、澜码科技、面壁智能为代表，借助自身大模型技术基础，满足企业大模型技术实际应用的需求。其主要竞争优势在于对大模型具有技术前瞻视角。其主要通过提供AI Agent 应用市场 &amp; 开发平台，为用户提供构建 AI agent 的便捷服务。</p><p></p><p>互联网科技厂商：以百度、火山引擎、腾讯为代表，借助借助自身大模型以及 AI 云服务，为客户提供完整的 AI 技术解决方案。因其自身基础设施、云、大模型等AI 生态建设完整。同时先前多推出了大模型相关的应用，建立了较为良好的用户基础和产品迭代模式。其主要也通过提供 AI Agent 应用市场 &amp; 开发平台，为用户提供构建AI agent的便捷服务。</p><p></p><p>RPA/流程自动化厂商：以来也科技、实在智能为代表，其主要将 AI Agent 技术思路集成进原有RPA产品中，依托自身长期积累的企业内流程自动化落地经验，为客户提供更智能化的 AI+RPA 类产品和服务。</p><p></p><p>数字化企业服务商：以用友、金蝶、标普云、数势科技为代表，依托自身长期积累的垂类领域或行业的 Know-how，实现企业内数字化系统的功能升级。对于此类厂商而言，AI Agent 多作为一个功能组件，内置进数字化系统，通过完善的 API 联动生态，实现与原有数字化系统的深度集成，从而让用户无感地体验 AI Agent。</p><p></p><p>更多关于AI Agent在数据分析、营销、金融、文娱游戏等的具体应用案例，欢迎点击<a href="https://www.infoq.cn/minibook/bTgj82D3gFJK9ZLRM5Ci">「链接」</a>"，下载完整报告阅读。InfoQ研究中心也期望通过持续的内容输出，继续支持中国AI领域的发展。</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>