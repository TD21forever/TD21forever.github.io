<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/dUU64vuc7JlmyheyVXud</id>
            <title>千帆AppBuilder重构企业AI原生应用开发体验 | 对话AI原生《云智实验室》</title>
            <link>https://www.infoq.cn/article/dUU64vuc7JlmyheyVXud</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dUU64vuc7JlmyheyVXud</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 10:32:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 大模型, 应用开发, 千帆AppBuilder
<br>
<br>
总结: 人工智能和大模型是当前科技创新的关键力量，百度智能云的千帆AppBuilder支持企业级AI原生应用开发，重构了应用开发体验，提供了三步构建方法论。在大模型时代，应用开发经历了翻天覆地的变化，从传统的开发方式转变为自顶向下的快速构建原型。千帆AppBuilder在产业级场景应用开发中提供了灵活的自动编排能力和优质组件，持续提升服务企业的能力，拓展大模型应用的边界。 </div>
                        <hr>
                    
                    <p>人工智能和大模型正在引领当前最重要的科技创新趋势。在过去的一年中，行业关注点已从大模型研发转向实际应用，正成为推动创新和转型的关键力量。百度智能云千帆AppBuilder作为基于大模型的企业级AI原生应用开发工作台，支持应用的快速开发和发布，以零代码、低代码、代码态三种模式，满足各类开发者的不同应用开发需求。</p><p></p><p>截至目前，百度智能云千帆大模型平台服务客户数已突破15万，开发了55万个大模型应用。那么，AI原生应用的开发逻辑与传统应用开发有何不同？千帆AppBuilder如何拆解应用开发过程，其开发思路秘诀又是什么？产业级AI原生应用开发有哪些技术难点？</p><p></p><p>百度智能云对话AI原生——云智实验室系列栏目的第二期，邀请到百度主任研发架构师董大祥，围绕产业级AI原生应用开发话题，通过讨论与实操结合的方式，在线展示千帆AppBuilder如何重构企业AI原生应用开发体验，满足各类开发需求，帮助企业将AI应用的创意和构想变为现实。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/d3/67/d30797467c1b2097af6b6cf63c524567.png" /></p><p>点击链接收看：《千帆AppBuilder重构企业AI原生应用开发体验》</p><p><a href="https://www.infoq.cn/video/xNwUJxFxEmuNzzvI7Y6j">千帆AppBuilder重构企业AI原生应用开发体验_百度_王世昕_InfoQ精选视频</a>"</p><p></p><p></p><h2>大模型时代应用开发新范式</h2><p></p><p></p><p>在大模型时代，AI原生应用在产品形态上显著变化，主要归功于大语言模型的理解、生成、逻辑和记忆四大核心能力。应用交互从传统的表单和按钮，转变为自然语言对话，这一转变也引发了开发方式的变革。</p><p></p><p>传统应用开发流程复杂，需要先进行高层设计，再分解为子模块开发，最后集成到一起，这一过程耗时且风险极高。在大模型技术加持下开发者可以借助AppBuilder平台通过自然语言快速构建一个从0到1的应用原型，随后逐步丰富和完善功能，这意味着应用开发发生了翻天覆地的变化。</p><p></p><p>总结来说，过去应用开发需要大量时间和人力投入，而现在依靠大语言模型能力，可以自顶向下快速构建原型，重塑了应用开发体验。</p><p></p><p></p><h2>三步构建AI原生应用方法论</h2><p></p><p></p><p>在构建AI原生应用的过程中，平台采用自顶向下的开发模式，先将模糊的想法转化为具体的应用配置方案，然后逐步丰富和完善功能。这种方法可以让开发者在短时间内快速搭建出符合需求的应用，实现从创意到应用的高效转变。具体来说，平台形成了一套三步构建方法论，包含以下三个关键步骤：</p><p></p><p>创意描述：一句话描述你的创意。创意拆解：将创意拆解为思考模型和组件两类工作。创意实现：选择合适的工具，通过自然语言描述思考模式，实现组件。</p><p></p><p></p><h2>千帆AppBuilder产业级场景应用开发</h2><p></p><p></p><p>在实际使用应用开发平台进行产业级应用开发过程中，客户往往会面临更复杂的开发需求并对平台的效果和性能有更高的要求，例如需要多个组件连续自动调用才能实现指令需求，海量私有数据需要解析和企业开发中对组件能力的高需求等，千帆AppBuilder对这些都提供了针对性解决方案。</p><p></p><p>高效灵活的自动编排能力：平台底层模型能够精准理解用户的角色指令和对各种组件的描述，并能连续自动调用多个组件，灵活地自动编排任务。</p><p></p><p>例如创建一个旅游助手，会用到百度搜索、文生图、TTS（语音合成）三个组件能力。在创建应用角色指令阶段就会描述当用户询问信息是人的时候，需通过百度搜索组件检索人相关信息，并绘制画像。当用户询问景点信息，需使用百度搜索提炼和总结景点特色信息，并且用音频方式播报。这一系列描述需要平台及底层模型紧密配合，理解实际使用场景才能完整触发，并且任务都由两个组件连续自动调用才能完成。</p><p></p><p>私有文档集成和企业级检索增强：平台具备海量文档的管理、合理的文档解析、版面分析、知识增强以及语义检索和召回功能。企业开发者可以利用自己的知识库来回答实际场景中的问题，降低模型可能产生的误解或错误。</p><p></p><p>开放易用的优质组件：平台提供了百度搜索、代码解释器、文生图、文件转换器、网页内容理解和长文档内容理解等60多个功能强大的官方组件。开发者可以像搭建乐高积木一样，灵活组合组件并且自定义组件，满足各类灵活定制开发需求。</p><p></p><p></p><h2>AI原生应用开发前景</h2><p></p><p></p><p>千帆AppBuilder在大模型企业落地场景中，持续提升服务企业的能力，扩展大模型应用的边界。近期，在WAIC期间举办的“大模型助力新质生产力发展论坛”上，千帆AppBuilder全新升级企业级知识检索增强(RAG），扩展性、开放性、安全性显著提升；全新发布“RAG X 百度搜索”功能，将百度搜索在时效性、客观性方面的优势，与RAG在私域知识响应、语言灵活性方面的优势进行能力互补，大幅提升用户知识检索体验；发布千帆AppBuilder私有化版本，推动大模型在政务、工业、交通、金融等行业的落地。</p><p></p><p>短短几个月，技术团队实现千帆AppBuilder在工作流编排、企业级RAG等功能的数次迭代，持续将大模型的强大能力落地到各行各业。智能既是科技的进步，也是理念的变革，千帆AppBuilder将携手客户、伙伴并进，继续迎接未来的无限可能。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</id>
            <title>微软放弃在 OpenAI 董事会的观察员席位</title>
            <link>https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lkTb4w34ymUgzXJIY4FN</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 09:03:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, OpenAI, 人工智能, 监管审查
<br>
<br>
总结: 微软放弃在 OpenAI 董事会的观察员席位，欧盟和美国对生成人工智能进行监管审查，微软可能面临反垄断调查，欧盟委员会正在调查大型数字市场参与者与生成式人工智能开发商和提供商之间的协议，微软已向 OpenAI 投入数十亿美元成为推动 AI 模型发展的领导者。 </div>
                        <hr>
                    
                    <p>当地时间7月10日，据外媒报道，微软表示，在欧洲和美国对生成人工智能进行监管审查之际，它将放弃在 OpenAI 董事会的观察员席位。</p><p>&nbsp;</p><p>微软副法律总顾问 Keith Dolliver 于周二晚些时候致信 OpenAI，称该职位提供了对董事会活动的深入了解，同时又不损害其独立性。</p><p>&nbsp;</p><p>但 CNBC 注意到，这封信的内容还透露，微软不再需要这个席位，因为“新成立的董事会取得了重大进展”。</p><p>&nbsp;</p><p>欧盟委员会此前表示，微软可能面临反垄断调查，因为它关注的是虚拟世界和<a href="https://aicon.infoq.cn/2024/shanghai/presentation/6042">生成人工智能</a>"市场。欧盟委员会是欧盟的执行机构，该委员会今年 1 月份表示，正在“调查大型数字市场参与者与生成式人工智能开发商和提供商之间达成的一些协议”，并将微软与 OpenAI 的合作列为其将要研究的一项具体交易。</p><p>&nbsp;</p><p>欧盟此后得出结论，观察员席位不会改变 OpenAI 的独立性，但欧盟监管机构正在寻求第三方对该交易的更多意见。英国竞争与市场管理局仍然心存疑虑。</p><p>&nbsp;</p><p>去年 11 月，在 OpenAI 首席执行官<a href="https://www.infoq.cn/article/HgI7G6Oth4C6PS4bsqR7?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> Sam Altman 被解雇</a>"、随后又迅速重新受雇的动荡时期之后，微软在 OpenAI 董事会中占据了一个无投票权的席位，以平息有关微软对该初创公司的兴趣的一些疑问。</p><p>&nbsp;</p><p>Altman 当时在给员工的一份说明中表示，<a href="https://www.infoq.cn/article/GdkidIFChUxVslICMamx?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI</a>"“显然做出了与微软合作的正确选择，我很高兴我们的新董事会将把他们列为无投票权的观察员”。</p><p>&nbsp;</p><p>OpenAI 于 2022 年底发布 ChatGPT 聊天机器人后，成为全球最热门的初创公司之一。微软已向&nbsp;这家初创公司投入了数十亿美元，据报道，迄今为止其总投资额&nbsp;已增至 130 亿美元。鉴于其对 OpenAI 的投资和合作，这家科技巨头实际上已成为推动基础 <a href="https://aicon.infoq.cn/2024/shanghai/track/1710">AI 模型</a>"发展的领导者。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Z5jt2oaAFjE9SvgORK6J</id>
            <title>《中国开发者画像洞察研究报告 2024》报告发布：鸿蒙生态存在百万级人才缺口，潜在新就业岗位超过300万个</title>
            <link>https://www.infoq.cn/article/Z5jt2oaAFjE9SvgORK6J</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Z5jt2oaAFjE9SvgORK6J</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 08:08:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 开发者, 数字经济, 鸿蒙
<br>
<br>
总结: 人工智能与各行业深度融合，推动创新变革，但也加剧全球对AI专业人才需求，现有人才储备难以满足市场需求，开发者在数字经济中扮演重要角色，鸿蒙生态发展迅速，带来新的就业机会和高薪酬待遇。 </div>
                        <hr>
                    
                    <p>人工智能正以前所未有的速度与各行各业深度融合，推动着各个领域的创新与变革，但同时也加剧了全球范围内对 AI 专业人才的需求，而现有人才储备难以满足市场需求，人才供需缺口日益明显。</p><p></p><p>在 AI 人才如此紧缺的形势之下，为了更好地理解在 AI 时代下的开发者群体，为年轻人们带来更具有实际意义的参考，并迭代《中国开发者画像洞察研究报告 2022》的研究结果，极客邦科技双数研究院 InfoQ 研究中心发起了本次《中国开发者画像洞察研究报告 2024》（以下简称《报告》）的研发和撰写计划。</p><p></p><p>本次报告调研联合了十余家企业和社区，通过收集数千份问卷，对多个企业机构进行访谈，分析了开发者的行为模式、工作价值、职业发展等内容，旨在为行业提供一份全面、深入的研究报告。通过这份报告，我们将更直观和深入地看到开发者群体的真实面貌，理解他们的价值追求，认识到他们在数字经济中的核心地位。</p><p></p><p></p><h2>一、数字经济加速壮大开发者群体：新兴技术领域职位需求已接近百万大关</h2><p></p><p></p><p>根据“十四五”数字经济发展规划，到 2025 年，数字经济核心产业增加值占 GDP 比重目标将达到 10%。数字化创新引领发展能力大幅提升，智能化水平明显增强，数字技术与实体经济融合取得显著成效。“十四五”规划为数字经济的未来发展指明了方向，越来越多的传统企业开始意识到数字化转型的重要性，并积极投身于科技浪潮之中，而这一趋势也为开发者提供了更多的职业机会。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d9a72dbaf015f73c0904bce738c25c0.webp" /></p><p></p><p>截至 2023 年年底，我国整体开发者人数已攀升至 2067.21 万，增速达到 2.5%。技术人才队伍的蓬勃发展和创新活力，对于开发者队伍的壮大和人才结构的优化具有重要意义，也印证了在数字经济时代，开发者群体将发挥越来越重要的作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffbf1fc18741fa3dae90572446b2bc5d.webp" /></p><p></p><p>在新兴技术领域，职位需求的激增尤为显著。目前 AI 发展已经进入以人工通用智能（AGI）为主导的行业应用拓广阶段，应用创新的涌现促使供需两端企业对技术人才的需求日益增长。2024 年多项国家政策强调了建设智能算力中心的重要性，并提出了进一步深化开放合作，发挥跨央企协同创新平台作用。在这一背景下，政府、企业和研究机构纷纷开始行动，积极迎接人工智能带来的深刻变革。基础设施的升级和企业机构拥抱“AI+”战略使得 AI 市场对人才需求出现快速增长，AI 领域的供需两端企业开始积极释放对人才的强烈需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/54/54f8971d9ef924f79836b851f32816c5.webp" /></p><p></p><p>此外，HarmonyOS+ 盘古大模型将为医疗、教育、家居、制造和商务等众多垂直领域带来新机遇。国产基础软件的广泛应用也将激发出更多的创业灵感和新项目，这些新项目的启动和新生态的构建将为社会创造更多的就业机会和经济增长点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fdd14a1c0abd5d360e5dd0b0a48e3dcb.webp" /></p><p></p><p></p><h2>二、开发者就业新蓝海：鸿蒙相关职位薪资涨幅超预期</h2><p></p><p></p><p>软件是数字中国建设的关键支撑，特别是以 HarmonyOS 操作系统为代表的国产基础软件，处在信息产业上下游生态的枢纽位置，在信息系统中起着基础性、平台性作用，对保障信息安全也非常重要。近日，华为公司发布了 HarmonyOS 的最新进展：鸿蒙生态设备突破 9 亿台，吸引超过 254 万开发者；从操作系统内核、文件系统到编程语言、人工智能框架和大模型，全部实现自研。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/edaa0cb9fb8c92322d1261d5181a6bff.webp" /></p><p></p><p>鸿蒙生态的快速发展和原生应用数量的爆炸性增长，加上信息技术迅猛发展、软件业稳步向前等宏观环境的积极影响，催生了大量的鸿蒙工作岗位需求，为就业市场注入了一股新的活力和趋势。目前 5000 款应用已完成原生鸿蒙开发，未来的目标是支持 50 万款应用，这为开发者创造了百万级人才缺口，潜在新就业岗位超过 300 万个，鸿蒙生态亟需更多开发者投入应用开发、参与生态建设。在 InfoQ 研究中心发起的调研中，32.3% 的开发者表示其非常有意愿未来转向或继续从事鸿蒙开发岗位，这彰显了鸿蒙生态的新技术新系统对开发者的强吸引力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62be8a2c3f8a450866a09c0cf4767e63.webp" /></p><p></p><p>《报告》显示，鸿蒙开发岗收入尤为突出，获得超出预期收入的鸿蒙开发者数量是其他开发者的将近两倍。同时，鸿蒙岗位也为开发者带来了技术层面的成长机会和解决问题的能力，四成左右鸿蒙开发者表示获得了技术成长，比例高于其他开发者。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf23bd1c318cff1c4bf3441cbf54414a.webp" /></p><p></p><p>根据《报告》显示，随着工作年限的上升，人均月薪出现分水岭，特别是近年转为鸿蒙开发岗位的资深开发者，能够获得比整体新紧缺岗位更高的薪资；据公开数据统计，2024 年鸿蒙开发者平均薪资涨幅 43.1%，而安卓开发平均薪资涨幅 22.7%。鸿蒙生态的快速发展正在吸引越来越多开发者的关注，并成为就业市场的一个新热点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/19/19e0d598612ef20cf56da38d54f5d978.webp" /></p><p></p><p>高薪酬待遇的背后，是企业对这些岗位所寄予的厚望和对人才的高度重视。企业需要能够推动技术创新、提升产品竞争力的人才。这些人才不仅需要深厚的技术功底，还要有创新思维和解决复杂问题的能力。因此，企业愿意为他们提供更高的薪资，以吸引和留住这些宝贵的人才资源。</p><p></p><p>此外，高校作为人才培养的摇篮，在鸿蒙人才培养方面同样扮演着重要角色。高校与企业合作产教融合类项目，不仅注重理论知识的传授，还通过实习实训类活动，将鸿蒙技术纳入相关专业课程体系，培养学生的鸿蒙技术应用能力和创新精神。</p><p></p><p>借助三方机构的赋能，能够为开发者提供多样化的鸿蒙技术培训课程，包括线上、线下课程及短期集训等，以满足不同学员的学习需求。同时，三方赋能机构与鸿蒙官方及企业紧密合作，通过项目实战和案例研究等活动，为开发者学员提供更多实践机会和职业发展空间。</p><p></p><p></p><h2>三、多元化成长 ：半数以上开发者选择付费学习</h2><p></p><p></p><p>目前，开发者主要就职于互联网上市公司，近半数未来开发者毕业后计划进入该类型公司。在未来短期至中期内，互联网上市公司将继续成为开发者最青睐的企业类型。随着 AGI 的蓬勃发展，小而美的独角兽企业价值凸显，尤其吸引了未来开发者的关注。新领域新技术的发展同样带动基础研究高质量发展，选择进入高校研究院工作的未来开发者规模比现有开发者规模高 8.8%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd94c20392febea8d051ba89b7245a27.webp" /></p><p></p><p>在技术日新月异的今天，每一位开发者的成长路径不再是单一的线性发展，而是一个多元化、立体化的过程。《报告》中提到，开发者面对的挑战集中在技术能力提升、工作价值和工作量三个方面，其中技术能力提升是最大挑战。这一挑战的主要源于外部环境的变化，开发者需要不断提升技能水平以适应日新月异的技术创新。</p><p></p><p>《报告》显示 37.3% 的开发者日均工作时长超过 9 小时，其中 6.7% 的开发者日均工作时长超过 12 小时，开发者加班现象普遍存在。除了工作内容多、工作时间长等工作量因素，日均工作 9 小时以上的开发者加班原因还包括对职业发展的焦虑或者对公司发展的焦虑，需要边工作边精进技术能力或尝试使用新技术解决问题，从而获得更强的职场竞争力。</p><p></p><p>开发者的成长不仅依赖于技术能力的提升，还依赖于持续的学习和进阶。根据《报告》我们可以看到，近九成的有工作经验的开发者日常有学习习惯，其中 55.7% 愿意进行付费学习。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2350413c931a511d44252974fdade0a.webp" /></p><p></p><p>《报告》中提到，技术语言、技术实战和计算机基础是开发者最主要学习的三类内容，其中技术实战是最受青睐的严肃学习内容类型。此外，针对产品运营和面试相关内容，付费开发者比例也显著高于非付费开发者比例，且开发者更愿意花时间学习这两类内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/62/62510c4195725511c06776259c60f42f.webp" /></p><p></p><p>此外，资深高薪开发者掌握各项计算机工具产品的比例大幅高于整体开发者，其中 Kafka、Hadoop 和 Elasticsearch 在高薪开发者中的掌握率尤为突出。掌握 Java、python、SQL 和 JavaScript 成为多数资深高薪开发者的“必会技能”。</p><p></p><p>这种学习习惯的形成，既是个人职业发展的需求，也是技术发展迅速的必然结果。开发者们深知，只有不断学习，才能跟上技术发展的步伐，才能在竞争激烈的职场中保持自己的竞争力。</p><p></p><p></p><h2>四、未来机遇无限：属于开发者的全新时代</h2><p></p><p></p><p>随着技术的不断演进和市场需求的日益多样化，开发者群体正站在一个全新的历史起点上。面向未来，他们展现出更加开放的姿态，积极拥抱新技术和新系统的发展。这种开放性不仅体现在对技术的学习与应用上，更体现在对新思想、新理念的接受与融合上。</p><p></p><p>面对不断变化的市场需求，开发者们需要持续地挑战自我，突破舒适区，通过不断地学习、交流和合作，才能提升自己的技术水平，才能在企业和社会的发展中发挥更大的作用。虽然道路充满挑战，但通过不懈的努力和正确的学习路径，将会有更多的开发者在数字化时代中实现自我价值的最大化，并开创属于自己的新时代。 </p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RikCP108qlJyt3169rcY</id>
            <title>奇富科技推出智能语音模型Qifusion，语音识别准确率可达93%以上</title>
            <link>https://www.infoq.cn/article/RikCP108qlJyt3169rcY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RikCP108qlJyt3169rcY</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 03:08:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 奇富科技, Qifusion-Net, 方言识别, 语音识别技术
<br>
<br>
总结: 近日，奇富科技智能语音团队的论文《Qifusion-Net：基于特征融合的流式/非流式端到端语音识别框架》被全球语音与声学顶级会议INTERSPEECH 2024收录。该框架模型在方言种类丰富、方言识别准确和高效方面取得了显著成果，提升了语音识别技术的准确性和智能化水平，解决了方言识别中的一系列问题，对金融服务领域等业务场景具有重要意义。 </div>
                        <hr>
                    
                    <p></p><p>近日，奇富科技智能语音团队论文《Qifusion-Net：基于特征融合的流式/非流式端到端语音识别框架》(Qifusion-Net:&nbsp;Layer-adapted&nbsp;Stream/Non-stream&nbsp;Model&nbsp;for&nbsp;End-to-End&nbsp;Multi-Accent&nbsp;Speech&nbsp;Recognition)被全球语音与声学顶级会议INTERSPEECH&nbsp;2024收录。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4c0efef72dd7b539ade947a461827d98.png" /></p><p></p><p>我国地域广阔，方言种类繁多，其语法和语音特征存在显著差异。同时，由于噪声的干扰、方言的混杂现象、主观感知在标注过程中的偏差，以及人力标注工作的复杂性和系统性不足，语音识别技术的准确性和智能化水平受到了一定程度的限制。</p><p></p><p>在金融服务领域，现有的通用语音识别技术在处理方言时往往难以达到理想的效果，不仅影响了人机交互的准确性和智能化水平，也对服务的效率和质量产生了负面影响。</p><p></p><p>奇富科技引入了全自研Qifusion框架模型，并将其集成到智能营销及贷后提醒等业务场景中。在应用上，Qifusion框架模型能够提升智能营销、贷后提醒、风险控制业务应用场景识别准确率，帮助解决以上问题。并且在复杂的通话环境中，Qifusion的语音识别准确率达到了93%以上，意图识别准确率超过95%。</p><p></p><p>方言种类丰富：凭借丰富的数据样本，Qifusion框架模型在原有东北官话、胶辽官话、北京官话、冀鲁官话、中原官话、江淮官话、兰银官话和西南官话等国内八种主流方言的基础上，强化了四川、重庆、山东、河南、贵州、广东、吉林、辽宁、黑龙江等用户密集地区的方言识别能力。</p><p></p><p>方言识别准确：Qifusion框架模型具备自动识别不同口音的能力，并能在时间维度上对解码结果进行口音信息修正，使方言口音的语音识别误差率降低了30%以上，整体语音识别字错率降低了16%以上，提升了用户体验。</p><p></p><p>方言识别高效：Qifusion框架采用了创新的层自适应融合结构，能通过共享信息编码模块，更高效的提取方言信息。同时，该框架模型还支持即说即译功能，能在无需知晓额外方言信息的前提下，对不同方言口音的音频进行实时解码，实现精准的识别和转译。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b05a2911fa57421bd8d3c34ade8d911c.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c2fea73ea78c8371511990973</id>
            <title>为何我们决定从零开始创建 NGINX Gateway Fabric</title>
            <link>https://www.infoq.cn/article/c2fea73ea78c8371511990973</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c2fea73ea78c8371511990973</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 02:12:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: NGINX, Kubernetes, Gateway API, NGINX Gateway Fabric
<br>
<br>
总结: NGINX决定推出自己的Gateway API项目，而不是将Gateway API强塞到现有Ingress产品中。他们认为这样做会限制未来发展，并希望打造一款经得起时间考验的产品，满足未来需求。他们的目标是为Gateway API资源提供全面配置互操作性，规范服务网络的许多组件，摒弃厂商特定的CRD，助力创造更加美好的未来。 </div>
                        <hr>
                    
                    <p></p><blockquote>原文作者：Brian Ehlert - F5 产品管理总监，Matthew Yacobucci - F5 首席软件工程师原文链接：<a href="https://www.nginx-cn.net/blog/why-we-decided-to-start-fresh-with-our-nginx-gateway-fabric/">为何我们决定从零开始创建 NGINX Gateway Fabric</a>"转载来源：<a href="https://www.nginx-cn.net/">NGINX 中文官网</a>"</blockquote><p></p><p></p><p>NGINX 唯一中文官方社区 ，尽在&nbsp;<a href="https://www.nginx.org.cn/">nginx.org.cn</a>"</p><p>  </p><p>在 Kubernetes&nbsp;<a href="https://www.nginx-cn.net/resources/glossary/kubernetes-ingress-controller/">Ingress controllers</a>"&nbsp;领域，NGINX 取得了巨大成功。<a href="https://docs.nginx.com/nginx-ingress-controller/">NGINX Ingress Controller&nbsp;</a>"被广泛部署用于商业 Kubernetes 生产用例，同时也作为开源版进行开发和维护。因此，您可能会想当然地认为，当 Kubernetes 网络（<a href="https://gateway-api.sigs.k8s.io/">Gateway API</a>"）获得重大改进时，我们会再进一步，将其部署到我们的现有 Ingress 产品中。</p><p></p><p>然而，我们选择了一条不同的道路。通过评估全新 Gateway API 的强大潜能以及是否有可能彻底重塑 Kubernetes 中的互联处理方式，我们意识到将 Gateway API 实现强塞到现有 Ingress 产品中会限制未来发展。</p><p></p><p>因此，我们决定推出自己的 Gateway API 项目 —&nbsp;<a href="https://github.com/nginxinc/nginx-gateway-fabric">NGINX Gateway Fabric</a>"。该项目是开源项目，将以透明、协作的方式运行。我们很高兴与外部贡献者合作，并乐于分享最新成果，共同推动创新，造福于整个社区和行业。</p><p> </p><p></p><h2>我们为何决定推出自己的 Gateway API 项目</h2><p></p><p>尽管我们满怀激情和信心做出了围绕 Gateway API 创建一个全新项目的决定，但这一决定也基于合理的业务和产品战略逻辑。</p><p></p><p>想必 Kubernetes 采用者已经对 NGINX Ingress Controller 的开源版和商业版有所了解。两者都部署了经过严格测试的 NGINX 数据平面（在&nbsp;<a href="https://www.nginx-cn.net/products/nginx/">NGINX Plus</a>"&nbsp;和 NGINX 开源版反向代理中运行）。在 Kubernetes 之前，NGINX 的数据平面已在负载均衡和反向代理用例中有不凡表现。在 Kubernetes 中，我们的 Ingress controller 可完成相同类型的关键请求路由和应用交付任务。</p><p></p><p>NGINX 因轻量级、高性能、久经考验并可满足严苛环境要求的商业产品而享负盛名。我们在 Kubernetes Ingress controller 领域的产品战略与我们的反向代理产品策略相一致，即为简单用例提供强大的开源产品，并为关键业务应用环境中的生产 Ingress 控制提供具有更多特性和功能的商业产品。这一策略在 Ingress controller 领域中行之有效，部分原因是过去 Ingress controller 缺乏标准化，需要大量自定义资源定义（CRD）来提供负载均衡和反向代理高级功能，开发人员和架构师在 Kubernetes 以外的网络产品中享用这些高级功能。</p><p></p><p>我们的客户依赖并信任 NGINX Ingress Controller，其商业版本已经具备了 Gateway API 旨在提供的许多关键高级功能。此外，NGINX 很早就参与了 Gateway API 项目，并认识到 Gateway API 生态系统要达到完全成熟还需要几年的时间。（事实上，Gateway API 的许多规范都在不断演进，例如 GAMMA 规范，该规范有助于更好地集成 service mesh（服务网格）。）</p><p></p><p>但我们认为，将测试级 Gateway API 规范强塞到 NGINX Ingress Controller 中会给成熟的企业级 Ingress controller 带来无谓的不确定性和复杂性。我们销售的任何产品都必须具有稳定性和可靠性，并完全符合生产就绪要求。Gateway API 解决方案也会做到这一点，只是目前仍处于起步阶段。</p><p></p><p></p><h2>我们的 NGINX Gateway Fabric 目标</h2><p></p><p>对于 NGINX Gateway Fabric，我们的主要目标是打造一款经得起时间考验的产品，就像 NGINX Plus 和 NGINX 开源版一样。为了确保我们的 Gateway API 项目能够“满足未来需求”，我们意识到需要为其数据平面和控制平面尝试不同的架构选择。例如，我们可能需要研究不同的方法来管理四层和七层互联或尽量减少外部依赖项。我们最好从零开始，而不沿袭任何历史先例和遵从任何要求。虽然我们正在使用业经试用和测试的 NGINX 数据平面作为 NGINX Gateway Fabric 的基础组件，但对新想法也持开放态度。</p><p></p><p>我们还希望为 Gateway API 资源提供不受厂商限制的全面配置互操作性。与现有 Kubernetes Ingress 范式相比，Gateway API 最大的改进之一就是规范了服务网络的许多组件。从理论上讲，这种标准化可支持许多 Gateway API 资源轻松地进行交互和连接，助力创造更加美好的未来。</p><p></p><p>不过，创造这一未来的关键是摒弃厂商特定的 CRD（可能导致厂商锁定）。对于必须支持专为 Ingress controller 领域而设计的 CRD 的混合产品而言，这可能极具挑战性。而在以互操作性为第一要务的开源项目中，做到这一点则相对容易些。为了摒弃紧密关联的 CRD，我们需要构建一个新框架，仅关注 Gateway API 及其组成 API 所暴露的新层面。</p><p></p><p></p><h2>加入我们的 Gateway API 之旅</h2><p></p><p>目前，我们仍处于早期发展阶段。只有少数项目和产品实施了 Gateway API 规范，其中大多数都选择将其融入现有项目和产品中。</p><p></p><p>因此，现在是启动新项目的最佳时机。我们的 NGINX Gateway Fabric 项目完全开放，决策和项目管理高度透明。因为该项目使用<a href="https://go.dev/">&nbsp;Go&nbsp;</a>"编写而成，所以我们诚邀广大 Gopher 社区成员建言献策、提交 PR。</p><p>Gateway API 可能会改变整个 Kubernetes 世界。一些产品可能不再需要，新的产品或将出现。Gateway API 蕴藏着无限可能，虽然不知道它终将走向何方，但我们翘首以待。诚邀您加入我们，共创精彩未来！</p><p></p><p>您可以首先：</p><p>以贡献者的身份加入项目在实验室中试用实现执行测试并提供反馈</p><p>如欲加入该项目，请访问 GitHub 上的<a href="https://github.com/nginxinc/nginx-gateway-fabric">&nbsp;NGINX Gateway Fabric</a>"。</p><p></p><p>NGINX 唯一中文官方社区 ，尽在&nbsp;<a href="https://www.nginx.org.cn/">nginx.org.cn</a>"</p><p>更多 NGINX 相关的技术干货、互动问答、系列课程、活动资源：&nbsp;<a href="https://www.nginx.org.cn/">开源社区官网</a>"&nbsp;|&nbsp;<a href="https://mp.weixin.qq.com/s/XVE5yvDbmJtpV2alsIFwJg">微信公众号</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TFNLwiRlMsZUNjiZMrwR</id>
            <title>大模型风口下，卷应用才有价值！首期 AIGC 实践案例集锦上线啦 （免费下载）</title>
            <link>https://www.infoq.cn/article/TFNLwiRlMsZUNjiZMrwR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TFNLwiRlMsZUNjiZMrwR</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jul 2024 01:55:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 生成式AI, 大模型, 应用实践, 技术探索
<br>
<br>
总结: 2024年过半，生成式AI已从最初的技术探索过渡到应用实践阶段。各大AI公司和科技大厂都在展示他们的最新AI成果，并将焦点放在大模型的产业应用上。大模型赛道竞争激烈，市场需求和实际落地挑战加速涌现。业内关注大模型将颠覆哪些行业、何时出现杀手级大模型应用、中国大模型技术阶段、面临的瓶颈和如何在内卷环境中赚钱等实际问题。 </div>
                        <hr>
                    
                    <p>2024&nbsp;年过半，生成式&nbsp;AI&nbsp;已从最初的技术探索过渡到应用实践阶段。无论是国外的OpenAI、谷歌、苹果等头部&nbsp;AI&nbsp;公司，还是国内的百度、阿里云、字节跳动、腾讯等科技大厂，以及百川智能、零一万物等&nbsp;AI&nbsp;独角兽都在积极展示他们的最新&nbsp;AI&nbsp;成果，并不约而同地将焦点放在了大模型的产业应用上。</p><p></p><p>与此同时，大模型赛道越来越卷，各大厂商开始从“重在参与”转向激烈的价格战比拼。从通用型到行业定制化，从云端部署到端侧应用，大模型技术的市场需求和实际落地的挑战在加速涌现。</p><p></p><p>大模型将颠覆哪些行业？杀手级大模型应用何时出现？中国大模型技术处于什么阶段？面临着哪些瓶颈？如何在内卷的环境里赚到钱？这些务实的问题成为了业内关注的焦点。</p><p></p><p>今年上半年，我们精选出过往超&nbsp;20&nbsp;篇&nbsp;AIGC&nbsp;领域文章并集结成<a href="https://www.infoq.cn/minibook/HfXFv4RaAEyPUFk5HAfJ">《大模型领航者&nbsp;AIGC&nbsp;实践案例集锦》（第一期）</a>"供读者下载，包括<a href="https://qcon.infoq.cn/2024/beijing">&nbsp;QCon&nbsp;全球软件开发大会（北京站）&nbsp;</a>"2024&nbsp;和<a href="https://aicon.infoq.cn/2024/beijing">&nbsp;AICon&nbsp;全球人工智能开发与应用大会（北京站）2024&nbsp;</a>"的热门演讲，以及对钉钉、面壁智能、数势科技、腾讯、京东、字节跳动、百川智能、云知声、零一万物、达观数据等企业的独家采访。</p><p>（文末附下载）</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d9b8a4607a66019e8afafe31f637edc.jpeg" /></p><p></p><p></p><p>本册电子书共包括“大咖视野”、“观点碰撞”、“应用案例”、“技术实践”与“AI&nbsp;测评室”五个部分，从权威视角、技术实战、终端应用等多个角度深入呈现了企业在大模型落地中面临的挑战和宝贵经验。</p><p></p><p>其中，“应用案例”又涵盖互联网产品、企业生产提效、医疗、教育以及更多垂直行业的章节内容，分别从不同行业的视角展现了大模型应用的无限潜力；“AI&nbsp;测评室”则囊括了今年各热门大模型的实测。</p><p></p><p>欢迎下载阅读，与我们一同深入&nbsp;AIGC&nbsp;“前线”，探索那些前沿的技术视野与成功团队的奋斗故事。</p><p></p><h2>目录</h2><p></p><p></p><h3>大咖视野&nbsp;|&nbsp;Vision</h3><p></p><p>01&nbsp;|&nbsp;钉钉卡位战：SaaS&nbsp;挣不到的钱，Agent&nbsp;会挣到</p><p>02&nbsp;|&nbsp;26&nbsp;岁带着百人团队冲刺大模型，面壁智能天才CTO：高效比参数更重要</p><p>03&nbsp;|&nbsp;这个离开大厂去&nbsp;AI&nbsp;创业的互联网大佬，带着他的“Killer&nbsp;Agent”来了</p><p></p><h3>观点碰撞&nbsp;|&nbsp;Opinion</h3><p></p><p>04&nbsp;|&nbsp;大模型开闭源争吵不休：开源落后闭源一年，决定模型能力的不是技术？</p><p>05&nbsp;|“国外一开源，国内就创新”！面对中美大模型差异，我们该突破还是继续模仿？</p><p></p><h3>应用案例&nbsp;|&nbsp;Cases&nbsp;</h3><p></p><p></p><h4>第一章：互联网产品</h4><p></p><p>06&nbsp;|&nbsp;如何1秒内快速总结100多页文档？QQ&nbsp;浏览器首次揭秘大模型实现技术细节</p><p>07&nbsp;|&nbsp;京东商家智能助手：Multi-Agents&nbsp;在电商垂域的探索与创新</p><p></p><h4>第二章：企业生产提效</h4><p></p><p>08&nbsp;|&nbsp;字节跳动代码生成&nbsp;Copilot&nbsp;产品的应用和演进</p><p>09&nbsp;|&nbsp;大语言模型加持，是智能运维架构的未来吗？</p><p>10&nbsp;|&nbsp;用&nbsp;AI&nbsp;面试员工的企业，知道打工人在想什么吗？！</p><p>11&nbsp;|&nbsp;AI&nbsp;代码助手革新编程界：腾讯云专家汪晟杰深度剖析机遇与挑战</p><p></p><h4>第三章：垂直行业</h4><p></p><p>12&nbsp;|&nbsp;巨头们涌入的医疗大模型，何时迎来最好的商业时代？</p><p>13&nbsp;|&nbsp;AI&nbsp;老师的强大功能&nbsp;+&nbsp;真人老师的情感交流&nbsp;=&nbsp;未来教育？</p><p>14&nbsp;|&nbsp;4人团队，如何用大模型创造近千万业务价值？</p><p></p><h3>技术实践&nbsp;|&nbsp;Technology&nbsp;</h3><p></p><p></p><h4>第一章：大模型训练与推理</h4><p></p><p>15&nbsp;|&nbsp;万字干货！手把手教你如何训练超大规模集群下的大语言模型</p><p>16&nbsp;|&nbsp;当大模型推理遇到算力瓶颈，如何进行工程优化？</p><p>17&nbsp;|&nbsp;AI&nbsp;辅助内部研发效率提升，昇腾大模型推理的最佳实践</p><p></p><h4>第二章：RAG&nbsp;与智能体落地</h4><p></p><p>18&nbsp;|&nbsp;智能体技术发展趋势：谈大模型智能体与开放领域融合</p><p>19&nbsp;|&nbsp;Agent&nbsp;还没出圈，落地先有了“阻力”：进入平台期，智力能否独立担事？</p><p>20&nbsp;|&nbsp;“驯服”不受控的大模型，要搞定哪些事?</p><p></p><h3>AI&nbsp;测评室&nbsp;|&nbsp;Evaluation</h3><p></p><p>21&nbsp;|&nbsp;算数不行、还不懂中国文化，大模型现在抢不了设计师的饭碗！&nbsp;</p><p>22&nbsp;|&nbsp;首届大模型“相亲大会”开始啦！谁是你的天选CP？</p><p>23&nbsp;|&nbsp;Kimi的词+Suno的曲：能带我入选《中国新说唱》，但还是干不过原神！</p><p></p><p></p><h2>下载通道</h2><p></p><p>AIGC技术正以惊人的速度重塑着创新的边界，InfoQ&nbsp;首期《大模型领航者AIGC实践案例集锦》电子书，收录了2024&nbsp;年上半年&nbsp;InfoQ&nbsp;发布的代表性大模型应用案例，全面覆盖从行业权威分析、技术深度解析到应用实践测评的多个维度，希望帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。关注「AI前线」，回复「领航者」免费获取电子书。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a1c00053f97d109f3deea2a8ebdbc1b.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pF38uV7MiVDO8hjyWEsz</id>
            <title>徐少春受邀出席世界人工智能大会，AI引领财务管理新世界</title>
            <link>https://www.infoq.cn/article/pF38uV7MiVDO8hjyWEsz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pF38uV7MiVDO8hjyWEsz</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 06:57:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 金蝶集团, 人工智能大会, 智能财务, AI技术
<br>
<br>
总结: 金蝶集团董事会主席兼CEO徐少春在2024世界人工智能大会上分享了关于智能财务管理的主题演讲，强调AI技术对财务管理的影响和变化，同时强调人类的良知无法被AI替代。会议还发布了中国企业财务智能化白皮书，探讨了人工智能技术在财务领域的发展。金蝶集团致力于利用AI技术推动财务管理的数字化转型，提供智能财务解决方案，以引领财务管理新世界的发展。 </div>
                        <hr>
                    
                    <p>7月6日，金蝶集团董事会主席兼CEO徐少春受邀出席2024世界人工智能大会（WAIC），并在“智能财务”论坛发表主旨演讲《AI时代财务管理的变与不变》，分享了金蝶结合30多年财务数字化经验和数百万家客户实践总结的“企业财务管理框架”，并指出AI技术的发展正引领财务管理新世界，但人类的良知永远不可能被AI替代。</p><p></p><p>据悉，本届大会主题为“以共商促共享 以善治促善智”，而以“新质经济 智慧财务”为主题的2024WAIC“智能财务”论坛，重点研讨了人工智能技术在财务领域的理论与实践发展，现场还发布了2024年中国企业财务智能化白皮书以及举办了智能财务开放生态联盟成立仪式。</p><p>&nbsp;</p><p>随着人工智能、机器学习、大数据分析等技术的快速发展以及在财会领域的成功试水，财务数智化转型已成为企业转型最重要的方向之一。徐少春在演讲中表示，财务管理应划分为“作战”、“记录”和“支撑”三层体系，而AI技术的发展，为企业财务管理框架及其内容带来了巨大的影响和变化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/db/dbc6fbe420c724d49526b6904c3500ce.png" /></p><p>金蝶集团董事会主席兼CEO 徐少春</p><p></p><p>“其中，基于财务管理框架，财务管理价值模型从陀螺型向沙漏型转变；AI使财务人员在计划与控制领域从只靠经验预测转变为精准预测，让财务管理信息从数据专享转变到信息普惠、专家服务从个人精英转变到AI天团、外部报告的重点从财务指标转变到发展能力评价，企业也从传统的‘财务信息系统’升级到‘AI+财务’智能平台；同时，财务人员的思维需要从AI‘观望者’转变成为‘拥抱者’，以算法和模型为核心，来驱动决策，来进行自适应的优化和持续创新。”徐少春指出，AI为财务管理的众多领域带来了变化，而在变局中，人类的良知永远不可能被AI替代，要以良知为定海神针，以AI为万变利器，共商共建共享一个美好的财务管理新世界。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f34c66dffad55ce3e99ce393e125c3c.png" /></p><p>随后，金蝶中国执行副总裁、大型企业事业部总裁赵燕锡现场出席了《2024年中国企业财务智能化调查报告（白皮书）》发布仪式，及智能财务开放生态联盟成立仪式。</p><p><img src="https://static001.geekbang.org/infoq/30/30e74c7998318b5babc7da239a8b1c91.png" /></p><p>2024年中国企业财务智能化调查报告（白皮书）正式发布</p><p>（合影左三为金蝶中国执行副总裁、大型企业事业部总裁赵燕锡）</p><p></p><p>金蝶作为上海国家会计学院智能财务研究院成员单位，深度参与“2024 中国企业财务智能化现状调查”，深入剖析了中国企业财务智能化应用现状、发展趋势、问题挑战及建议。此外，金蝶参与建立智能财务开放生态联盟，希望以人工智能为引擎、数字化为引领，构建开放、合作、共赢的智能财务生态体系，促进智能财务领域最新技术的研究和应用，推动企业财务管理的数智化转型。</p><p>&nbsp;</p><p>人工智能是新一轮科技革命和产业变革的重要驱动力量。作为企业管理软件与云服务行业的领军者，金蝶对AI在企业管理场景下的应用更是早早开始积极探索与布局。</p><p>&nbsp;</p><p>2018年，金蝶与上国会等单位共同发起并成立智能财务研究中心（智能财务研究院前身），积极研究智能化技术在财务管理领域的应用场景。2022年，金蝶推出全球首个EBC企业管理领域的数字员工。2023年，金蝶又重磅发布中国首个财务大模型，可提供专业的分析、审核、预测、专家支持、报告生成、解读等服务，加速企业财务管理智能化跃升。</p><p>&nbsp;</p><p>今年以来，金蝶基于“AI优先”战略，将金蝶云·苍穹重构为新一代企业级AI平台，并重磅推出超级智能的AI管理助手——Cosmic，覆盖财务、人力、供应链等多种业务场景，支持业务发起、多模态智能审核以及财务指标查询和分析等智能财务功能。和传统AI产品相比，金蝶提供具备“大模型+财务”等垂直领域的真实落地实践，已经帮助金茂集团、温氏集团、厦门建发集团实现多种智能财务应用功能。</p><p>&nbsp;</p><p>深耕财务领域多年，金蝶一直致力于构建世界一流的财务管理：从国内第一款Windows财务软件到中国首个财务大模型，再到超级智能的AI管理助手Cosmic，金蝶始终以自主创新AI技术，引领财务管理数字化转型。未来，金蝶将利用AI技术帮助企业构建新质生产力，提升使用体验，改善运营效率，让每个员工都拥有一个超级智能的AI财务管理助手，打造财务管理新世界</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5cisJ5uchbX4TWq4mwBA</id>
            <title>DeepMind发布JEST算法，AI模型训练耗能降低十倍</title>
            <link>https://www.infoq.cn/article/5cisJ5uchbX4TWq4mwBA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5cisJ5uchbX4TWq4mwBA</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:40:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, 人工智能, JEST算法, 多模态对比学习
<br>
<br>
总结: 近日，谷歌的人工智能研究实验室DeepMind发布了关于训练AI模型的新研究——多模态对比学习与联合示例选择(JEST)。这项研究提出的JEST算法可以显著提高训练速度和能源效率，超越了传统模型的训练方法，通过多模态对比学习和联合示例选择优化数据的整体学习效果，为人工智能技术的发展带来重要的突破。 </div>
                        <hr>
                    
                    <p></p><p>近日，谷歌的人工智能研究实验室DeepMind发表了关于训练AI模型的新研究——多模态对比学习与联合示例选择(JEST)。</p><p></p><p>JEST算法可以将训练速度和能源效率提高一个数量级。DeepMind&nbsp;声称，“我们的方法超越了最先进的模型，迭代次数减少了&nbsp;13倍，计算量减少了10倍。”</p><p></p><p>论文链接：</p><p><a href="https://arxiv.org/pdf/2406.17711">https://arxiv.org/pdf/2406.17711</a>"</p><p></p><p>有网友激动地表示：“我没想到它来得这么快。对于模型来说，选择训练数据的能力是很强大的，因为这可以使得训练变得十分容易。你不需要再去猜测什么是高质量的训练数据，因为你有一个专门学习它的模型。”</p><p></p><p>JEST算法以一种简单的方式打破了传统的AI模型训练技术。典型的训练方法侧重于对单个数据点的学习和训练，而JEST则是对整个批次进行训练，优化了数据的整体学习效果。</p><p></p><p>多模态对比学习能够直接揭示数据之间的交互，通过选择高质量的子批次显著提高训练效率。</p><p></p><p>多模态数据交互：利用不同模态（图像、文本等）间的相互作用增强数据的表征力。例如，将图像中的对象与其描述文本相匹配，增强模型的理解。</p><p></p><p>对比目标：最大化相同概念的不同模态表示（如图像和对应文本）之间的相似度，同时最小化不相关模态之间的相似度。通过sigmoid-contrastive&nbsp;loss等对比损失函数实现。</p><p></p><p>学习效率的提升：多模态学习方法使JEST算法从数据交互中学习到更复杂的数据表示，提高了学习效率和模型性能。</p><p></p><p>联合示例选择通过评估数据子批次的整体可学习性，从大批次中选择出最有学习价值的子批次。</p><p></p><p>可学习性评分：结合当前模型的损失和预训练模型的损失，优先选择当前模型尚未学会但预训练模型已学会的数据。</p><p></p><p>评分函数：结合预训练模型的易学性评分和当前学习模型的难学性评分，得到综合的可学习性评分。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/9505f369e41090693633db33a2acbe62.png" /></p><p></p><p></p><p>但是，这个系统完全依赖于其训练数据的质量，如果没有高质量的数据集，引导技术就会分崩离析。对于业余爱好者或者业余AI开发者来说，JEST比其他方法要更难以掌控。</p><p></p><p>近年来，人工智能技术迅猛发展，大规模语言模型（LLM）如ChatGPT的应用日益广泛。然而，这些模型的训练和运行消耗了大量能源。研究称，微软用水量从2021年到22年飙升了34%，ChatGPT每处理5-50个提示就会消耗接近半升水。在这样的背景下，JEST技术的出现显得尤为重要。</p><p></p><p>参考链接：</p><p><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/google-claims-new-ai-training-tech-is-13-times-faster-and-10-times-more-power-efficient-deepminds-new-jest-optimizes-training-data-for-massive-gains">https://www.tomshardware.com/tech-industry/artificial-intelligence/google-claims-new-ai-training-tech-is-13-times-faster-and-10-times-more-power-efficient-deepminds-new-jest-optimizes-training-data-for-massive-gains</a>"</p><p><a href="https://the-decoder.com/google-deepminds-jest-speeds-up-ai-training-by-13x-while-slashing-computing-needs/">https://the-decoder.com/google-deepminds-jest-speeds-up-ai-training-by-13x-while-slashing-computing-needs/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/31135104e35b1d06aff17fe83</id>
            <title>百度Feed业务数仓建模实践</title>
            <link>https://www.infoq.cn/article/31135104e35b1d06aff17fe83</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/31135104e35b1d06aff17fe83</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:35:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据建模, 宽表, 数据一致性, 数据计算成本
<br>
<br>
总结: 本文介绍了在Feed数据宽表建模过程中的演进和实践，通过建设大宽表来简化数仓、提升效率，解决数据一致性和计算成本等问题。随着业务发展，作者提出了三个阶段的建设过程，包括小时级宽表+主题宽表建模、实时宽表建模和基于流批一体的多版本宽表建设。通过不断优化数据建模，提高数据时效性和降低成本，实现了Feed数仓的持续发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>作者 | XY导读Feed，即个性化推荐信息流，是百度 App 上承载各种类型内容（如文章、视频、图集等）的重要 topic。本文概要讲述了随着业务发展，移动生态数据研发部在 Feed 数据宽表建模上的演进过程以及一些实践：整合流量、内容、用户等数据，建设多版本宽表，实现 feed 数仓的一致性，简化数仓取数逻辑，降低成本提升效率。</blockquote><p></p><p></p><p></p><blockquote>全文3312字，预计阅读时间9分钟。</blockquote><p></p><p></p><h1>01 引言</h1><p></p><p>在宽表建模阶段之前，feed 数仓是按照传统的数仓分层建模思路进行，按照 ods----&gt;dwd----&gt;dws----&gt;ads 层进行建模，在这四层之外，还有维表 dim 层。数仓建模数据较为分散，不同主题的表分散在不同的数据表，数仓复杂且存在大量冗余：数仓各层近百张表，总体数据量近50P。下游使用数据拼接成本较高，对于内部数仓和外部用户使用，都有巨大的解释成本和使用成本。</p><p></p><p>随着业务对数据使用精细化分析的需求增多，以及底层工具对数据计算和数据查询速度的提升，数据建设的思路转向建设大宽表，尽可能下沉业务逻辑到表中，隐藏复杂性。</p><p></p><p>Feed 数仓在宽表建模阶段，共分为三个阶段：</p><p></p><p>小时级核心表+主题宽表建模小时级核心表+主题宽表建模+实时宽表基于流批一体的多版本宽表</p><p></p><p>我们按照时间顺序来说明建设的这三个阶段。</p><p></p><h1>02 阶段一：小时级宽表+主题宽表建模</h1><p></p><p>在业务快速发展、业务复杂度提高的情况下，原先的基于分层建模的数仓的一些问题——如使用成本高、取数逻辑复杂、查询性能差、时效性差等问题开始逐渐变得显著。为了简化数仓、提升时效性、降低数仓的使用门槛，我们使用场内流式TM框架建设了15 分钟级流批日志表，并基于厂内图灵数仓，整合了 feed 分发、展现、时长、播放等数据到同一张表中，并基于该表，关联用户和资源维度等，建设用户宽表、资源宽表以及用户资源宽表等。</p><p></p><p>15 分钟级流批日志表(log_qi)：基于 feed 日志产出 feed 15 分钟的流批日志表，该表主要用于对日志原始字段的解析，并下沉简单业务逻辑。可以对应之前的 ods 层。feed 小时级明细宽表(log_hi)：小时级产出，下沉复杂业务逻辑，作为 feed 主要对外服务的数据表，可以对应 dwd 层。主题宽表、中间表：拼接其他主题数据，聚合数据聚合，可以对应 ads 层。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d4ed0cf4f67aaea6e1b669c8359f8c1.png" /></p><p></p><h1>03 阶段二：实时宽表建模</h1><p></p><p>实时宽表(log_5mi)的建设，源于业务的飞速发展，业务侧对数据的时效性提出了更高的要求，用于对实验或者策略上线后效果的验证和问题的监控。现有的 15 分钟级别流批日志已经不太能满足实时监控的时效性需求。而且 15 分钟级流批日志表，只是对原始日志的解析和抽取，并没有下沉复杂的业务逻辑，下游使用该表的成本巨大，无法满足对准实时数据快速迭代的需求。因此建设了 feed 实时数据表，该表 schema 完全对齐小时级宽表，同样下沉了复杂业务逻辑，下游应用可以快速简单地获取实时数据，用于满足业务对于实时数据需求的快速迭代。</p><p></p><p>实时宽表建设后，feed 数仓相较于之前，多了一条 feed 实时数据流。如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd261f55098e56161a90f4f2813261a8.png" /></p><p></p><h1>04 阶段三：基于流批一体的多版本宽表</h1><p></p><p></p><h2>4.1 背景</h2><p></p><p>在小时级表宽、主题宽表、实时宽表建设完成后 ，随着 feed 业务的发展，这套数据建模体现在应用现有业务的时候，还是出现了一些使用上的问题。主要体现在如下方面：</p><p></p><p>口径一致性：主要体现为流式实时数据与离线数据存在的差异，在数据一致性方面遇到了挑战，而且需要维护实时和离线数据两份数据口径。数据源不统一：搜索、直播有部分数据计入到 feed，数据源与现有数据源存在较大差异，获取 feed 数据多了两部分外部数据源。数据重复加工：数据数据源的不一致，导致 feed 数据分散在不同的中间表，导致获取完整数据成本较大，内外部获取数据存在重复加工的问题。数据计算成本大：资源、用户等主题宽表的维度的拼接，在计算中中间数据可能达到 30T，且存在数据倾斜问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f3b64c62d3ebe7389037f70bee7197e2.png" /></p><p></p><h2>4.2&nbsp;建设思路</h2><p></p><p>基于前面小时级表(log_hi)、实时表(log_5mi)的建设思路，建设一张新的天级用户-资源明细数据(log_di)宽表，用这三张表重构 Feed 数仓体系，解决实时&amp;离线数据不一致问题，统一 feed 数据源和数据出口，提升用户资源常用维度产出时效。</p><p></p><p>建设新表有两个难点：</p><p></p><p>业务上，如何统一不同数据的数据源，有效整合到一张表中，并且在表中下沉复杂的业务逻辑，对外隐藏业务复杂性，只暴露下沉好的业务字段。技术上，在 feed 总体数据拼接用户、资源维度的时候，中间 shuflfe 的数据量会达到 30T，且存在较大的数据倾斜，严重影响 join 的性能。</p><p></p><p>为了解决以上两个问题，在设计阶段，将新表设计为 4 级分区，拆分为 4 个版本产出，不同版本产出不同的数据。</p><p></p><p>版本拆分思路：feed 汇总数据、资源维度、用户维度、关注关系等，产出时效不同，按照对数据时效性要求的不同以及维度表就绪的时间，不提供版本拼接不同的维度数据，既提升对应维度的产出时效，也减少数据 JOIN 时的数据量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4ba14ee44c82ea2efe712cc1b139a61f.png" /></p><p></p><p>分区设计思路：</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/9255c4e471d7c251ff4267d89f030c30.jpeg" /></p><p></p><p>计数优化思路：对拼接的资源表、关注关系表做提前过滤，减少 join 时的数据量，再采用 spark AQE 解决数据倾斜问题。</p><p></p><h2>4.3 Feed 基于流批一体的多版本的数仓体系</h2><p></p><p>天级用户-资源明细数据(log_di)宽表建设完成后，Feed 实时表(log_5mi)、小时级表(log_hi)、天级表(log_di)，由于 schema 对齐，数据一脉相承，可以视为一张大宽表——Feed 基于流批一体的多版本宽表，共 3 张表，涉及 6 个版本：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3acf2ecd691dc8df927723b556b172a1.jpeg" /></p><p></p><p>用 Feed 基于流批一体的多版本宽表重构 Feed 数仓体系，其他主题表都基于流批一体的多版本宽表进行上卷，数据出口统一到宽表。不同的时效性产出的数据，对应上层不同的应用，如报表、数据集等等。</p><p></p><p>重构后数仓示意图如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd8f0a3bcfda8d78d85ee21dbeb9dd66.png" /></p><p></p><p>经过重构后的 feed 数仓，具有以下优势：</p><p></p><p>数据源统一与数据出口统一：整合了分散的不同数据源到同一张表，统一了出口，并且下沉了复杂的业务逻辑，下游用户只需要查询一张表，保障了内外部门使用 feed 数据的一致性。多版本产出不同数据：对于时效性不同的查询需求，可以在实时、小时级、天级表多个版本间进行切换，除了调整表名外，查询语句基本不需要修改。高时效性多维度整合：资源、用户等多维数据，不同版本拼接不同的维度，提高了产出时效，下游可以按需依赖。</p><p></p><h1>05 总结与规划</h1><p></p><p>业务的发展对数仓工具提出了更高的要求，工具的不断迭代又带来更多的数仓建设思路，数仓的建设也随着业务的发展不断迭代。在宽表建设阶段，经过不断摸索，最终 feed 数仓简化为基于流批一体的多版本数仓体系。后续随着 Feed 业务规模的不断扩大和复杂化，当前的数仓工具&amp;数仓体系面临的挑战也日益增多，在新的业务挑战下，我们将继续完善数仓体系，以应对不断变化的业务需求，为业务决策和创新提供坚实的数据支持。</p><p></p><p>——————END——————</p><p></p><p>推荐阅读</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591535&amp;idx=1&amp;sn=2dfebbf2cff51638f839b216c4622481&amp;chksm=c03f5613f748df053f92276ddd957e05400ccf5bb26d4ff471f15e629d4fd803efd004615499&amp;scene=21#wechat_redirect">大模型时代数据库技术创新</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591523&amp;idx=1&amp;sn=58f71dc589563adfd20005dbeebd35c7&amp;chksm=c03f561ff748df09e3f66a766dda53c7042d9d393786f3588242203445c902821fa24d61878f&amp;scene=21#wechat_redirect">低代码组件扩展方案在复杂业务场景下的设计与实践</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591401&amp;idx=1&amp;sn=e4b658e810366196dff6fbf2ba781910&amp;chksm=c03f5595f748dc83b9259f3277b0d9c1a48b177f9208adfc0788472340a3d5493da909918ddf&amp;scene=21#wechat_redirect">通过搭建 24 点小游戏应用实战，带你了解 AppBuilder 的技术原理</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247591330&amp;idx=1&amp;sn=ca7665b1cd877934bdd47fda7545f0bc&amp;chksm=c03f55def748dcc819b94c649d39fd94ce14fc3391abdcb8087b3606ae98fe85a2a9323a0fa5&amp;scene=21#wechat_redirect">基于 Native 技术加速 Spark 计算引擎</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=Mzg5MjU0NTI5OQ==&amp;mid=2247590639&amp;idx=1&amp;sn=4c5e02fe87272ef2243f50bd3ec9fcfc&amp;chksm=c03f5293f748db85d39e5f10adbf519b70fe0edcb3d233245a9b7d5ef065cb183572b350a836&amp;scene=21#wechat_redirect">百度&amp;YY设计稿转代码的探索与实践</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xKdjJaVK1jbwhvXZHZ66</id>
            <title>平安壹钱包：RAG等技术在金融支付类ToC应用场景的探索与落地</title>
            <link>https://www.infoq.cn/article/xKdjJaVK1jbwhvXZHZ66</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xKdjJaVK1jbwhvXZHZ66</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:33:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 支付行业, 数字化转型, ToC 应用, 大模型
<br>
<br>
总结: 金融支付类企业在数字化转型中面临着政策监管、数据隐私、技术伦理等挑战，如何实现ToC业务的落地并不做技术取舍处理是亟待解决的问题。在这一背景下，大模型的应用成为了一个重要的探索方向，通过AI技术实现ToC应用的落地，提升用户体验和服务质量。 </div>
                        <hr>
                    
                    <p></p><blockquote>嘉宾｜王良编辑｜黄雯希</blockquote><p></p><p></p><p>支付行业作为金融生态系统中的重要组成部分，其数字化转型不仅关系到企业自身的竞争力，更直接影响着亿万用户的日常生活。</p><p></p><p>随着移动支付、在线银行、数字货币等新兴支付方式的普及，用户对支付服务的便捷性、安全性和个性化需求日益增长。政策监管的加强、数据隐私的保护、技术伦理的考量，都是金融支付类企业在 ToC 应用探索中必须面对的现实问题。金融支付类企业在种种约束条件下，如何实现 ToC 业务的落地，同时在技术上不做取舍处理，成为亟待解决的问题。</p><p></p><p>在日前举办的<a href="https://archsummit.infoq.cn/2024/shenzhen?utm_source=infoq&amp;utm_medium=conference"> ArchSummit 全球架构师峰会</a>"上，平安壹钱包用户研发部技术负责人王良分享了<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6031">平安壹钱包</a>"落地的业务场景和 RAG 向量检索 + 知识库 + 标注平台等技术的实际应用案例，以及在政策监管下如何通过应用立项、合规监管审批，怎样进行业务线选择等相关经验，帮助金融支付类企业在数据受限的环境中实现大模型的 ToC 应用的落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07a82e0abb2058cd0689078078b07263.webp" /></p><p>王良 平安壹钱包用户研发部技术负责人</p><p></p><p></p><blockquote>8 月 16-17 日，FCon 全球金融科技大会将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，将邀请国内外金融机构及金融科技公司专家分享其实践经验与深入洞察。AIGC、RAG、Agent 智能体等作为焦点话题，届时<a href="https://fcon.infoq.cn/2024/shanghai/presentation/6031">平安壹钱包大数据研发部算法负责人王永合还将带来《大模型驱动的账户风险管理》</a>"的议题分享。 大会更多演讲议题已上线，点击链接可查看目前的专题安排：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</blockquote><p></p><p></p><p></p><p><img src="https://static001.infoq.cn/resource/image/b0/e8/b04c7d31c134b0a613defd507c2ceae8.jpg" /></p><p></p><p>以下是王良老师分享全文（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h3>平安壹钱包的落地探索</h3><p></p><p></p><p>非常感谢 InfoQ 极客传媒的邀请，也很荣幸能有机会就大模型在金融支付类公司的应用落地和大家进行交流，抛转引玉 ，今天跟大家聊一聊我们平安集团的平安大模型以及在业务落地上的一些实践和经验。</p><p></p><p>首先，大模型大家肯定都是知道的，自从 ChatGPT 这款应用产品发布之后，仅用了两个月时间，全球用户数就突破一亿了，这比我们人类历史上所有的流行产品速度都要快很多。2 个月破亿这个记录有多惊人，我给大家看一组数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7fb02c1ba602ee2f3d924a3bc29605d0.webp" /></p><p></p><p>固定电话用户破亿用了 75 年，手机 16 年，当智能手机普及之后进入到移动互联网时代，破亿的速度首次缩短到 6 年内，并且以极快的发展速度进入到 1 年以内。从固定电话到移动电话，再到互联网时代的应用程序，信息传递的速度不断加快，信息传递的方式和载体也发生了根本性的变化。说一个大家可能没有察觉到的细节，比如打电话的手势已经在现在的小朋友过家家时不再使用了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a54aeeaff9603131e037f4a98edb03d.webp" /></p><p></p><p>换成了手掌平方在 耳朵边：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ff0ed2d86c7174a7c1d64e166a97a3d8.webp" /></p><p></p><p>这预示着信息传播方式的巨大变化。当下许多研究报告和机构预测，到 2026 年，将有 80% 以上的企业接入<a href="https://aicon.infoq.cn/2024/shanghai/">生成式 AI 或大模型</a>"。我个人觉得这个不算激进，甚至还有一些保守，那么以上这些信息汇总起来看，就是告诉我们，要快，不然就跟不上要掉队了。</p><p></p><p>那么为了不掉队于时代，我们进行了多次调研和头脑风暴，讨论的结果有 2 个确定和 1 个不确定。其中一个确定是【确定未来：AI 大模型未来的前景是明确的，AI 必然会重构世界。】他不像元宇宙或者 web3.0 先有概念，在概念出来后，都在等应用场景的普及，等发展。另一个确定是：【确定现在: 要想不掉队就必须现在就开始进入】，AI 的发展太迅速了，等慢悠悠的确认好方案的时候，友商公司已经有成品了。不确定的是：【不确定落地场景：解决什么需求？降本增效还是扩大收益？用什么技术？开源闭源。。。。。。】这些都是不确定的，这就会涉及到如何去寻找落地场景的问题。2 个确定是全员达成一致的，我们很快就聚焦在不确定的落地场景上，在这里我要特别强调一下，在立项之前，决定做什么的阶段，技术不是影响价值最大的，想要选对选好有价值的落地场景，很重要的一点就是业务驱动，要找能支持项目落地且能持续运营的业务场景，有运营不断的反馈、维护、营销，才能保证项目是可持续发展的。接下来给大家一些寻找落地场景的思路。</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/178893885feb3e2420122d32c3d05019.webp" /></p><p></p><p>从最熟悉的领域入手让 AI 学习最优秀员工的能力，再让它辅助其他员工，实现降本增效寻找“文本”进，“文本”出的场景。因为任何问题都可以用语言去描述，把大模型看做是一个函数，给输入，生成输出，大模型最厉害的就是回答你的问题。不要追求大而全，将任务拆解，先解决小任务，小场景，再考虑怎么工程化的把每一步整合起来，这也是周鸿祎说的“小切口大纵深”。</p><p></p><p>选好场景在制定落地方案时，有 3 个重点：原理、实践和认知。如果没有深入理解大模型的原理，就没办法举一反三，走不了太远。一定要有实战经验，没有实践，只能光纸上谈兵做事不落地。认知水平不高，就没有办法做出最优的决策，能达到的天花板就很低。</p><p></p><p>场景选好，接下来就是技术选型。模型分为开源模型和在线模型。开源模型可以部署到本地，前后台链路都在自己手里，模型文件、项目文件、还有模型权重、等等在本地部署，因此可以更好的确保数据安全性。但成本也更高，因为需要自己购买硬件，还需要投入技术进行维护。在线模型就可以通过 API 调用，在线模型普遍性能更强，调用的技术门槛和硬件门槛更低，配套模型生态和服务更加完善，不需要维护，前期成本投入会比较少。知道这两种优缺点还不足以评估技术选型，因为还要了解国家的政策要求。不同的类有不同的政策要求。一些有监管要求的企业在政策上是不允许使用非开源大模型的，比如银行在使用大模型时，把用户数据上传到百度、阿里、OPENAI 的大模型就有可能会涉及到法律合规问题。</p><p></p><p>无论哪个行业，只要是面向用户的，他就必须符合监管要求，涉及到国家的法律道德、涉及到意识形态、那这个在合规性方面要求是非常高的。在用大模型的时候，我们担心的可能是大模型无法回答问题或给出错误答案。但更害怕大模型随意回答，尤其是在涉及国家、民族和相关的重要历史问题时，必须确保大模型不会在这些问题上“一本正经地胡说八道”，这造成的后果是很严重的，所以选择一个靠谱的大模型是非常重要的。目前国内报备审批通过的大模型效果都是很好的。这张表是面向 C 端用户、面向政府、面向商用、出海的一些要求，如果本身对数据安全的要求特别高，那么只能选择开源大模型来保证数据隐私。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52734f5126113db3910b444fe6a9461a.webp" /></p><p></p><p></p><h3>合规与报备</h3><p></p><p></p><p>在 2023 年 7 月 10 日，政府发布了一项声明，强调了生成式人工智能服务安全的基本要求。根据这一声明，任何 To C 的人工智能服务在上线前都必须根据各种暂行办法报备，并接受审查。虽然现在只是暂行办法，但这不意味着没有约束，暂行办法背后有《网络安全法》、《数据安全法》和《个人信息保护法》等法律法规的支撑，如果违反这些规定就可能会面临处罚或者下线。关于报备的经验和一些关键信息的填写，我整理了一份文档在 PPT 中有提到，大家可以在 archsummit 官网下载我的演讲稿资料查看 。</p><p></p><h3>业务场景中的 RAG 应用</h3><p></p><p></p><p>简答说一下我们的应用落地场景, 一个即时的回答企业微信用户的 AI 运营平台，主要目的不是给大家演示我们的产品，而是展示产品背后的运作逻辑。</p><p></p><p>在这个场景中，我们应用了 RAG 的技术，把一些业务场景中的知识输入大模型。这个知识库中包含活动营销的相关信息，也包含一些行业规范等内容。我们会提取文本进行数据清洗，然后切割成 chunk 文本嵌入到向量库中。当用户提问时，问题本身也会被转换成向量形式，与向量库中的向量进行匹配，然后通过 prompt 提示词工程和平安大模型，返回生成的答案给到用户。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/bef6065e96e45e1359db3a281eb84d02.webp" /></p><p></p><p>那么我们为什么用 RAG？因为大模型还是有一些小缺陷或者说是不足的地方，比方说模型训练数据 cut-off 、私有数据、保密数据、可解释性、幻觉等问题。如果问大模型， ArchSummit 2024 架构师峰会有什么议题它可能搜索不到，某位讲师讲了什么它也搜不到，它还可能乱答一通，这就是所谓的幻觉。</p><p></p><p>为了提高问题答案的准确性，我们可以提前把大会的数据和讲师的演讲内容资料，输入到模型中。通过知识库的输入，进行向量化处理，这样做检索的时候，就能得到准确的结果。</p><p></p><p>如果模型没有相关信息，是否意味着只能通过<a href="https://aicon.infoq.cn/2024/shanghai/"> RAG 技术</a>"来解决这个问题呢？其实也并非如此。还可以考虑使用 Fine-Tuning 技术，也就是我们常说的大模型微调。那么我们为什么不采用微调？一个是因为微调的投入产出比不高，技术难度大；一个是因为它改变模型的权重。容易造成副作⽤。比如大模型原本计算出的答案不符合预期，我们通过手动调整让他在某个知识点返回我们想要的结果，这就相当于改变了大模型的思维方式。后续再回答其他问题时，比如说，“天王盖地虎”下一句应该关联“宝塔镇河妖”，它给你返回“小鸡炖蘑菇”。这种改变可能带来不可预知的副作用，而且因为数据量庞大，几乎无法覆盖验证。但是 Fine-tuning 技术有其他优势，通过调整以后的内容可以跟原始的大模型融为一体，检索速度会更快，因为不需要再次在向量库中进行匹配，我们当前的体量和技术储备在微调上投产比太低，所以我们选择 RAG，它更容易实现一些。</p><p></p><h3>RAG 落地过程中需要关注的事项</h3><p></p><p></p><p>在实际落地的过程中，我们也遇到了一些问题和挑战。我大概归类 为 8 个方面。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5dd7e561da84ed23a91d23ed2b0f4729.webp" /></p><p></p><p></p><h4>1.数据源加载与处理</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/75/7524d39f3b4e886fc2a9376f1897287b.webp" /></p><p></p><p>首先是数据源加载和处理。数据量是很庞大的，用户提供的语料库和资源库要输入大模型或者 RAG 的时候，会发现有各种各样格式的文档，即使格式相同，内容大纲和排版也不一样。此外，还有各种的扫描文件，处理起来非常复杂。大模型对 Markdown 格式文档是天然友好的，在初期，我们只能通过手工去约束业务方，尽量按照一定的标准提供数据。需要尽量做好，因为这是整个流程的第一步。就像进行很长的公式计算时，我们不要第一步就把π用 3.14 做出计算结果，带到下一步，我们最好先用π做逻辑运算，在最后一步再用 3.14 求计算值，因为随着计算步骤和环节的增加，误差会逐渐增大。如果在第一步就损失了精度，后续想提升精度就会变得很困难。在一开始，尽可能地进行约束。</p><p></p><h4>2.数据切分难</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24bdd58e0c6500783cd5100c28562fbb.webp" /></p><p></p><p>然后就是数据切分难的问题。当我们提供数据给大模型后，大模型怎样进行切分，怎样确定向量，这些都是我们不可控的。对于这些问题，通常有五种技术解决方式，</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/170e6f93c972d98fd51f9f49be8dd0c2.webp" /></p><p></p><p>第一种方式是直接将知识当做问题的前置条件输入询问大模型。用《西游记》来举例子，如果我们想问《西游记》相关的问题，假设大模型对《西游记》的知识储备是 0，我们可以将整套《西游记》的文本在问答文本框输入给大模型，并同时提出问题。比如，要求大模型根据故事总结“三打白骨精”的情节，大模型能够做到这一点。但这里存在一个“不可能三角”，如果输入的文本很长，大模型的注意力集中度会提高，但同时算力也提高了，成本也会增加。相反，如果我们的输入不够清楚，或者只输入了“三打白骨精”这一部分的章节，没有师徒四人组队之前的章节，大模型就无法准确捕捉到故事中每个角色的形象和性格，提供的“三打白骨精”的总结可能就不那么准确，也就是说文本长度短了，“注意力”就下降了，算力虽然低了，但是答案就可能没有那么准确了，这就是【文本长度、注意力、算力】的“不可能三角”。</p><p></p><p>第二种方式是手动切分。继续以《西游记》为例，如果我们还是想要大模型总结“三打白骨精”的情节，我们可以手动输入情节，比如将唐僧师徒的经历和取经的背景外加“三打白骨精”相关的章节输入大模型，去除其他不相关的章节，比如女儿国和火焰山等、其他章节的故事。</p><p></p><p>第三种方式是利用 LangChain 等工具。这个就是等官方出解决方案，我们碰到的问题在其他公司也是普遍存在的，如果我们无法解决，就可以等待官方去解决。</p><p></p><p>第四种方式就是上文中提到的微调。当前我们只是对一些语气和回答文案的风格做了微调，使答复内容更亲切更客气一点，不改变检索的内容。如果你的用户量极大，投入足够的技术和精力去微调，提高响应速度，基于庞大的用户体验上，是值得去做这件事的。</p><p></p><p>第五种方式是利⽤OpenAI Assistant API 进⾏⻓⽂本读取，它具备 knowledge retrieval 功能，翻译成中文就是知识检索功能。就是在提问之前先上传文档，让大模型消化这些文档。这与 RAG 有些相似，RAG 是我们已经事先将所需的语料库输入大模型，等待用户随时提问， Assistant API 是将接口开放给用户。让用户自己上传文档后再提问，相当于将原始文档数据交给了对方。</p><p></p><h4>3.检索效果</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/86/869c8e7cff5d5135de0b999fc03de3ce.webp" /></p><p></p><p>检索效果的问题，即大模型给出的答案可能不是用户想要的，或者不满足要求。这里建议优化提示工程 prompt 。这是一个门槛低但上限极高的技术。如果用得好，它可以极大地激发大语言模型的涌现力。</p><p></p><h4>4.检索结果过多或过长</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/68/68ffa0ae704456bdc0c8fff6330321c8.webp" /></p><p></p><p>关于检索结果过长的问题，也是可以通过提示工程 prompt 来解决。比方说，我们可以限制大模型最多只给出五条答案，每条不超过 2 万字或者 2 千字。如果答案过多，也可以要求大模型进一步总结，然后继续提炼。</p><p></p><h4>5.可解释性与鲁棒性</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/68/6848598db7cb844645e0b682bfa92eeb.webp" /></p><p></p><p>可解释性是指的是大模型给出了答案，但用户并不知道它是否正确，答案已经超出了用户的认知范围，看起来非常通顺 非常正确。鲁棒性是指模型在面对各种异常情况或不完美的输入时，仍能保持稳定和可靠的性能。有点类似于拼音输入法在我们拼错拼音时也能打出正确的字，简而言之，它是指模型对于噪声、异常值、缺失数据、模型假设违反等情况的容忍程度。</p><p></p><h4>6.复杂 query</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/43/43eeb50a4f2905f7d99f948d833cb15b.webp" /></p><p></p><p>接下来讨论的是复杂 query。简单来说，如果我们询问去年东方航空的财务报告情况，通常能够找到相关信息。或者东方航空的财报查询不出来的话，一般的其他上市公司的财报也能查出来。但如果要对比东方航空从 2019 年到 2023 年的财务情况，并和竞争对手进行比较，即使有数据，也可能不够准确，不符合我们的需求。对于这种复杂问题，我们建议使用自动化的方法来解决。像 Function calling 继续发展下去，就基本衍化成了 Agent</p><p></p><h4>7.⾃动化</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/be/bed1b9271727ae0789bdd9569fc47629.webp" /></p><p></p><p>关于自动化的四个方面，首先是 ALL tools ，像上文提到的，指的是如果遇到问题，可以等官方推出新的工具来解决。因为我们遇到的问题可能不仅仅是我们自己的问题，其他开发者或者公司普遍也会遇到的问题。碰到问题的人多了之后，官方就会着手解决。原生的大模型框架会越来越多的集成一些工具在内部，比如 OPEN AI &nbsp;和 GML 4 的 Knowledge Retrieval 两个大模型都是围绕长文本进行问答的，ALL Tools 当然会有很多其他的 Tools ，比如绘图、图生文、等等，多模态不仅仅是它具备图文视频这样的能力，而是底层能够近乎无损的进行文字和图片的转化，是一个更加底层的能力。</p><p></p><p>ReAct 是可以通过信息检索增强我们推理能力的一个东西，它不仅仅能去做推理, 还可以和环境外部数据去做交互获取一些额外的业务知识。</p><p></p><p>Function calling 是函数调用，它允许模型调用外部函数或者工具来执行特定的任务，举个例子，我们使用手机拍照，是用手点击拍照 APP，然后拍照。这个手就是调用 API 的方式，相机的 APP 就相当于 Function 函数等着我们去调用它。</p><p></p><p>而 Agent 的概念则类似于我们有一个秘书。我们可以告诉这个秘书，“请帮我拍下今天的演讲材料，我回去需要做汇报，请尽量拍得清晰。”秘书在执行任务时，就不仅仅是简单地点击相机 APP 拍照 。他拍完之后要检查一下，如果发现拍糊了，要删除重新拍。如果照片中有遮挡，他可能会换个角度或者等更好的时机再拍。这整个过程就是 Agent 将多个 Function calling 集合起来执行一个任务，当然前提是 Agent 提出的 case 本身是支持 Function calling 的，在这个例子中，就是检查照片是否清晰、删除照片、移动位置换角度这些都要单独抽支持被调用。</p><p></p><h4>8.反馈、评估与迭代</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f8/f8044e1ad6d45ee104eeb400b7ac2ea1.webp" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/87/8740b516de185ee8fef1a8df5f1d0c49.webp" /></p><p></p><p>最后就是反馈、评估和迭代。这就是字面上的意思，在发布了产品之后，怎么知道它的表现好不好？就需要收集反馈的数据，比方说，回答的质量如何，检索质量是否达标，引用是否完整等在前期的每个环节。如果想检查其中某个环节，就应该在那个环节做好数据的召回、采集和埋点等工作。</p><p></p><p>最后总结一下上手实践的一些经验：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c73937aad890ea357ac8d2ce900a52ec.webp" /></p><p></p><p></p><h3>验证标注平台的关键步骤与迭代流程</h3><p></p><p></p><p>再说一下标注平台，我们要怎么提高我们私有化知识库的质量 ，首先，我们先导入数据，将知识库导入后，数据清洗会提取出很多知识碎片。接下来是向量化数据，我们自己先生成一系列问题，自问自答，创建 QA。对这些 QA，我们先自动处理一遍，忽略那些没有歧义 ，而且匹配度特别高的。对于匹配度没那么高的，我们就分配给人工标注。</p><p></p><p>在人工标注阶段，我们需要选择对业务熟悉的人员进行标注。假设现在我们的知识库是涉及医学领域的，我们就选择有医学知识背景的人来标注，而不是选择非医疗从业者完全不了解的人。我们的经验是，通常，在一个公司内部有很多熟悉业务的人员，比如产品、业务、开发和测试。在这些人里面，测试可能是最合适的选择，因为他们愿意去点，其他人可能点着点着不耐烦了就瞎点。对于标注的人，我们也进行了培训。标注完成以后，就进行质检和审查，确保没问题，然后才能继续进行下一步。</p><p></p><p>然后就是持续进行迭代，先是将数据导入数据库，接着清洗数据。比如我们生成大约 1000 条 QA，其中有 500 条需要进行人工验证。验证完毕后，我们再不断进行调整，直到确保准确无误。没有问题之后，我们就将其发布上线，用户再去查询的时候就可以获取到真实的数据。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cd3e1a168897fff371ec5735b784d5d1.webp" /></p><p></p><h4>Function calling &amp; Agent&nbsp;在风控系统中的应用</h4><p></p><p></p><p>接下来再介绍一下我们已经落地的另一个应用场景。用来总结分析用户被风控拦截的原因。如果人工处理，需要查询多个系统，看身份类型，账号风险，交易风险，操作流程等 ，可以 用工具批量汇总，但是人工分析太耗费时间了，精力不足，我们把需要 关注的关键信息都提供 Function Calling 接口 ，让 大模型自己去查信息，去分析前后逻辑和原因。给出每个 case 的拦截原因说明。下图案例中底部的分析报告【客户在 2016 年注册，但在 2024 年 6 月 9 日首次绑卡，绑卡成功后设置支付密码，随后发起高风险交易。操作、交易归属地为广东，客户本人归属地为云南，交易在异地发生。交易场景为 500 元话费充值，为高风险场景大额交易。综合以上信息，该客户判断为高风险客户。】就是 Agent 在调用了多个用户的相关数据后，分析并整理成文案反馈的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c832187d773447ea3834e6fc298f140.webp" /></p><p></p><h3>大模型浪潮中的趋势</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/6c/6ce49a000dda604c653bf85b21a195cc.webp" /></p><p></p><p>在大模型的浪潮下，未来的趋势之一是脱离“信息的茧房”。我们每个人的精力有限，最多只能精通两三个行业，比如说我们需要涉及医疗或其他领域时，能够快速检索到相关信息，将各行业顶尖的能力赋予普通人，这也意味着程序员之间的技能差距可能会缩小。在过去，一个有着十年经验的程序员与一个两三年的小白之间的差距很大，但未来，通过 AI 的帮助，新手跟顶级程序员的差距就没有那么大，进步的速度会很快，新手从一出来就基本上满级。再就是对小而美的创业团队的重大机遇，如果能够开发出爆款应用，市场潜力将非常可观。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7cd2ca82cdc1ef55044593a8846e591e.webp" /></p><p></p><p>最后说一些我个人对大模型应用架构师职责的理解，第一个关键在于如何提高答案输出的准确性和稳定性，这是大模型应用架构师职责的核心。另一个重要的考量是成本效率。无论是私有化部署还是去调在线的 API，成本和技术投入都不小。我们要不断提高系统的效率和准确性，不要让用户多次尝试才能找到满意答案，降低成本也提高用户体验。最后是维护好系统的稳定性健壮性，不要频繁出问题，影响业务。</p><p>感谢您阅读到最后，如果有感兴趣的点，也欢迎您留言进行交流。</p><p></p><h5>活动推荐</h5><p></p><p>8 月 16-17 日，<a href="https://fcon.infoq.cn/2024/shanghai/schedule">FCon 全球金融科技大会</a>"将在上海举办。本届大会由中国信通院铸基计划作为官方合作机构，来自工银科技、北京银行、平安银行、广发银行、中信银行、度小满、蚂蚁集团等金融机构及金融科技公司的资深专家将现身说法分享其在金融科技应用实践中的经验与深入洞察。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/31/31ff5488cc076e04976f66fd5d9869c7.webp" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/8ed40a3957157db4a1bd72583</id>
            <title>StarRocks跨集群迁移最佳实践｜得物技术</title>
            <link>https://www.infoq.cn/article/8ed40a3957157db4a1bd72583</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/8ed40a3957157db4a1bd72583</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:10:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: DBA, StarRocks, 版本升级, 数据迁移
<br>
<br>
总结: 本文介绍了针对StarRocks集群版本升级和数据迁移的实践经验，包括方案流程、方案设计和方案规划等内容。通过外表和Flink Connector两种方案，实现了集群间的数据同步和读写分离，同时提供了适用场景和实施步骤。 </div>
                        <hr>
                    
                    <p></p><h1>一、引言</h1><p></p><p>2024年之前，DBA维护的StarRocks集群存在在用低版本多、稳定性受组件bug影响大的问题，给日常运维带来一定压力，版本升级迫在眉睫。于是，我们在今年年初安排了针对2.5以下版本升级2.5.13的专项。这里和大家分享下，针对因版本兼容问题而不能原地升级的场景下，进行跨集群升级时迁移数据方面的实践。</p><p></p><h1>二、方案流程</h1><p></p><p></p><h2>方案可行性评估口径</h2><p></p><p>针对跨集群迁移方案的评估，主要从迁移成本角度考虑，主要分为资源成本和稳定性成本：</p><p></p><h4>资源成本</h4><p></p><p>完成迁移所需要的人力工时投入、软硬件投入（如使用哪些三方平台、需要多少机器资源、带宽资源等）。</p><p></p><h4>稳定性成本</h4><p></p><p>数据迁移过程中，线上业务一般仍会继续提供服务，则迁移操作对系统产生的压力可能影响正常的生产服务，随之会带来额外的稳定性成本。这里从迁移服务产生系统压力的可监控预警能力评估稳定性成本。</p><p></p><h2>方案设计</h2><p></p><p></p><h4>方案一：StarRocks外表</h4><p></p><p></p><p>1. 技术原理</p><p>1.19 版本开始，StarRocks支持将数据通过外表方式写入另一个StarRocks集群的表中。这可以解决用户的读写分离需求，提供更好的资源隔离。用户需要首先在目标集群上创建一张目标表，然后在源StarRocks集群上创建一个Schema信息一致的外表，并在属性中指定目标集群和表的信息。</p><p></p><p>通过INSERT INTO写入数据至StarRocks外表，可以将源集群的数据写入至目标集群。借助这一能力，可以实现如下目标：</p><p></p><p>集群间的数据同步；读写分离。向源集群中写入数据，并且源集群的数据变更同步至目标集群，目标集群提供查询服务。</p><p></p><p>2. 方案评估</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b579c29a0145d6febef152264729e2e1.webp" /></p><p></p><p>3. 适用场景</p><p></p><p>数据量较小（200G以内）；无三方平台可用；数据迁移无需考虑稳定性成本；测试场景快速验证；存在hll、bitmap类型字段，但是又没有底表数据进行数据重建（hll/bitmap类型字段借助三方组件进行迁移的方案可参考官方文档flink导入至-bitmap-列、flink导入导入至-hll-列等）；Array/Map/Row等复杂类型的迁移。</p><p></p><h4>方案二：Flink Connector</h4><p></p><p></p><p>1. 技术原理</p><p>Flink是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。随着不断迭代，Flink已提供了接口统一的批流处理模型定义，同时提供了灵活强大的DataStream API和抽象度更高的Table API，供开发人员尽情发挥，更提供了SQL支持。</p><p></p><p>Flink提供了丰富的Connector，用以打通各类数据源，形成强大的数据联通能力。StarRocks官方也推出了导入和导出Connector，满足基于Flink对StarRocks的读写能力。</p><p></p><p>2. 方案评估</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/798f24c598f8446bbd660c3c80d97c17.webp" /></p><p></p><p>3. 适用场景</p><p></p><p>数据量较大；有三方平台可用；稳定性要求高，期望控制稳定性成本；有24h持续同步需求。</p><p></p><h2>方案规划</h2><p></p><p>在同步操作前，需要明确待同步的数据范围，统计较精确的待迁移数据量，评估数据迁移所需耗时，决策数据迁移完成时间等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/519d359e74c99bfb5c726ac5b057acf8.webp" /></p><p></p><h4>方式一</h4><p></p><p>结合预期的同步完成DDL，集群每天可用于同步的时间段，推导出同步时需要达到的速率。</p><p></p><p>计算公式：</p><p></p><p>预期同步最大速率(MB/s)=待同步数据总量(MB)/同步总耗时(天)/每天可同步时间(个小时/天)</p><p></p><h4>方式二</h4><p></p><p>根据集群负载可支持的最大速率、集群每天可用于同步的时间段，计算完成同步所需的时间。</p><p></p><p>同步总耗时(天)=待同步数据总量(MB)/预期同步最大速率(MB/s)/每天可同步时间(个小时/天)</p><p></p><h4>注意</h4><p></p><p>准确的待迁移数据量评估，依赖数据时间范围的确认。对于新旧集群双写场景，同步的最晚时间是完全双写介入的那一天（包含）。预期同步最大速率(MB/s)，需要兼顾集群当前流量和预估可承受的最大流量，避免因数据同步给集群造成预期外的压力，影响线上服务稳定性。</p><p></p><h2>方案实施</h2><p></p><p></p><h4>方案一：外表</h4><p></p><p></p><p>1. 创建外表</p><p>在源集群/库上创建外表，指向目标集群。</p><p></p><p>建议创建一个外表专用db，用于与源db隔离，避免误操作风险。</p><p></p><p><code lang="text">CREATE EXTERNAL TABLE external_db.external_t
(
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    k4 VARCHAR(2048),
    k5 DATETIME
)
ENGINE=olap
DUPLICATE KEY(`timestamp`)
PARTITION BY RANGE(`timestamp`)
(PARTITION p20231016 VALUES [("2023-10-16 00:00:00"), ("2023-10-17 00:00:00")),
PARTITION p20231017 VALUES [("2023-10-17 00:00:00"), ("2023-10-18 00:00:00")))
DISTRIBUTED BY HASH(k1) BUCKETS 10
PROPERTIES
(
    "host" = "127.0.0.x",
    "port" = "9020",
    "user" = "${user}",
    "password" = "${passwd}",
    "database" = "test_db",
    "table" = "t"
);</code></p><p></p><p>2. 写入外表</p><p>在源集群/库上写入外表。</p><p></p><p><code lang="text">insert into external_db.external_t select * from db.other_table;</code></p><p></p><p>3. 优缺点</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e66092855f2b32ebc7b6c7329d54f12f.webp" /></p><p></p><h4>方案二 Flink SQL</h4><p></p><p>1. 接入实时计算平台</p><p></p><p>本方案基于我司自研的实时计算平台（Flink任务开发调度平台）实现，需要业务方先接入平台，拥有专属项目空间和计算资源，这里不再赘述。</p><p></p><p>2. 新建Flink SQL任务</p><p></p><p>同步任务SQL即为Flink SQL，分为定义数据来源表、定义数据输出表、定义同步ETL SQL三部分。</p><p></p><p>定义数据来源表</p><p></p><p>语法上遵守Flink SQL规范，更多参数设置可参见官方文档使用Flink Connector读取数据-使用 Flink SQL读取数据。</p><p></p><p>注意事项：</p><p></p><p>StarRocks与Flink SQL的数据类型映射；Flink scan参数设置，尤其是超时（time-out）类字段的设置，建议往大了设置；考虑到数据迁移的源端和目标端的库、表均同名，在定义时需要对源表和输出表的表名做区分，以免混淆错乱。比如源表命名为{table名}_source，输出表命名为{table名}_sink 。</p><p></p><p>示例：</p><p></p><p><code lang="text">CREATE TABLE rule_script_etl_source (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `cost` BIGINT,
  `name` STRING,
  `error` STRING,
  `script` STRING,
  `rule_id` STRING
) WITH (  
    'connector'='du-starrocks-1.27', --具体值以官方组件或自研组件定义为准
    'jdbc-url'='jdbc:mysql://1.1.1.1:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'scan-url'='1.1.1.1:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'scan.max-retries'='3',
    'scan.connect.timeout-ms'='600000',
    'scan.params.keep-alive-min'='1440',
    'scan.params.query-timeout-s'='86400',
    'scan.params.mem-limit-byte'='1073741824'
);</code></p><p></p><p>定义数据输出表</p><p></p><p>注意事项：</p><p></p><p>StarRocks与Flink SQL的数据类型映射；Flink sink参数设置，尤其是超时（time-out）类字段的设置，建议往大了设置；尽量进行攒批，减小对StarRocks的导入压力；考虑到数据迁移的源端和目标端的库、表均同名，在定义时需要对源表和输出表的表名做区分，以免混淆错乱。比如源表命名为{table名}_source，输出表命名为{table名}_sink ；如果输出表是主键模型，表定义中字段列表后需要加上PRIMARY KEY ({primary_key}) NOT ENFORCED。</p><p></p><p>示例：</p><p></p><p><code lang="text">CREATE TABLE rule_script_etl_sink (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `rule_id` STRING,
  `uid` BIGINT,
  `cost` BIGINT,
  `name` STRING,
  `error` BIGINT,
  `script` STRING,
  `sink_time` TIMESTAMP,
  PRIMARY KEY (`identity_id`) NOT ENFORCED  # 仅适用主键模型
) WITH (
    'connector'='du-starrocks-1.27',
    'jdbc-url'='jdbc:mysql://1.1.1.2:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'load-url'='1.1.1.2:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'sink.buffer-flush.max-rows'='400000',
    'sink.buffer-flush.max-bytes'='94371840',
    'sink.buffer-flush.interval-ms'='30000',
    'sink.connect.timeout-ms'='60000',
    'sink.wait-for-continue.timeout-ms'='60000'
);</code></p><p></p><p>定义同步ETL</p><p></p><p>一般为insert select语句；可以根据自身需求，添加一些ETL逻辑。</p><p></p><p>注意事项：</p><p></p><p>有映射关系的非同名字段，添加as，提升可阅读性；前后字段类型不一样的，需要使用case as进行显式类型转换；如果是仅输出表包含的字段，也需要在select子句中显式指出，并使用case null as {dataType}的形式进行类型转换；部分String/VARCHAR(n)类型字段中，可能存在StarRocks Flink Connector使用的默认列分隔符(参数sink.properties.column_separator，默认\t)、行分隔符(参数sink.properties.row_delimiter，默认\n)，导致导入是报“errorLog:Error:Value count does not match column count. Expect xx, but got xx. Row:xxx”错误，需要替换为自定义的分隔符；select子句尽量添加filter信息，一般是分区字段，以便Flink根据同步任务设置的并行度，拆分任务，生成合适的执行计划。</p><p></p><p>示例：</p><p></p><p><code lang="text">insert into rule_script_etl_sink
select
  `timestamp`,
  `identity_id`,
  `app`,
  `rule_id`,
  cast(null as BIGINT) `uid`,
  `cost`,
  `name`,
  cast(`error` as BIGINT) `error`,
  `script`,
  `timestamp` as `sink_time`
from rule_script_etl_source
where `timestamp` &gt;='2023-08-20 00:00:00' and `timestamp` &lt; '2023-09-20 00:00:00';</code></p><p></p><p>完整示例：</p><p></p><p><code lang="text">CREATE TABLE rule_script_etl_source (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `cost` BIGINT,
  `name` STRING,
  `error` STRING,
  `script` STRING,
  `rule_id` STRING
) WITH (  
    'connector'='du-starrocks-1.27',
    'jdbc-url'='jdbc:mysql://1.1.1.1:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'scan-url'='1.1.1.1:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'scan.max-retries'='3',
    'scan.connect.timeout-ms'='600000',
    'scan.params.keep-alive-min'='1440',
    'scan.params.query-timeout-s'='86400',
    'scan.params.mem-limit-byte'='1073741824'
);

CREATE TABLE rule_script_etl_sink (
  `timestamp` TIMESTAMP,
  `identity_id` STRING,
  `app` STRING,
  `rule_id` STRING,
  `uid` BIGINT,
  `cost` BIGINT,
  `name` STRING,
  `error` BIGINT,
  `script` STRING,
  `sink_time` TIMESTAMP,
  PRIMARY KEY (`identity_id`) NOT ENFORCED  # 仅适用主键模型
) WITH (
    'connector'='du-starrocks-1.27',
    'jdbc-url'='jdbc:mysql://1.1.1.2:9030?useSSL=false&amp;rewriteBatchedStatements=true',
    'load-url'='1.1.1.2:8030',
    "user" = "${user}",
    "password" = "${passwd}",
    'database-name'='test_db',
    'table-name'='rule_script_etl',
    'sink.buffer-flush.max-rows'='400000',
    'sink.buffer-flush.max-bytes'='94371840',
    'sink.buffer-flush.interval-ms'='30000',
    'sink.connect.timeout-ms'='60000',
    'sink.wait-for-continue.timeout-ms'='60000',
    'sink.properties.column_separator'='#=#',  -- 自定义列分隔符
    'sink.properties.row_delimiter'='@=@'  -- 自定义行分隔符
);

insert into rule_script_etl_sink
select
  `timestamp`,
  `identity_id`,
  `app`,
  `rule_id`,
  cast(null as BIGINT) `uid`,  -- sinl表才有的字段
  `cost`,
  `name`,
  cast(`error` as BIGINT) `error`,
  `script`,
  `timestamp` as `sink_time`
from rule_script_etl_source
where `timestamp` &gt;='2023-08-20 00:00:00' and `timestamp` &lt; '2023-09-20 00:00:00';</code></p><p></p><p>3. 调度任务</p><p></p><p>在开始调度前，还需要为任务的设置合适的并行度。通常SlotNum/TM设置为1，Parallelism设置为3，以长耗时换取导入任务的运行稳定性。</p><p></p><p>为避免任务失败带来的重跑工作量，单表每次任务可以迁移部分分区，多次执行。</p><p></p><p>4. 优缺点</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d73acc9efbe8d8caa1c980da7049145.webp" /></p><p></p><h2>方案验证&amp;验收</h2><p></p><p></p><h4>验证</h4><p></p><p>可以选取不同大小的表若干，组成有梯度的待同步数据量，使用上述任一种方案，执行同步操作，并观察同步时间内集群的负载。</p><p></p><p>以集群各水位不超过80%、无业务报错为准，尝试验证集群可承载的最大同步速率，及时校正上面的数据同步规划。</p><p></p><h4>验收</h4><p></p><p>1. 集群负载</p><p></p><p>以集群各水位不超过80%、无业务报错为准。可根据集群水位情况，酌情增加或减少同步任务的并发。</p><p></p><p>2. 数据diff校验</p><p></p><p>数据行数校验</p><p></p><p>针对迁移前后数据模型未发生改变的表，一定范围内（通常是单分区级别）的数据量需要保持相等；</p><p></p><p>针对迁移前后数据模型发生改变的表，需要case by base分析。</p><p></p><p>如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5815a9230e2a45fc98f9abd2b3dbcec.webp" /></p><p></p><p>数据质量校验针对维度表，可参考分区及或表级行数校验结果；针对事实表，可以在分区级别做指标列的SUM/MAX/MIN/AVG值校验；研发也可以结合业务自定义更多的校验方式。</p><p></p><h1>三、方案成果</h1><p></p><p>基于本方案，有效地解决了原地升级异常再回滚的方案带来的不稳定风险，完成了多个集群从低版本直升2.5.13的目标，累计迁移数据逾10T，迁移流量摸高至2Gb/s（10+个节点）。</p><p></p><p>结合原地升级方式，共同构成了较完善的升级方案，尽量减少升级带给业务的闪断等影响的同时，以较高效率完成升级。</p><p></p><h1>四、方案展望</h1><p></p><p></p><h2>方案的不足</h2><p></p><p>对比云商和自建DTS平台的数据迁移功能，本方案在流程化、产品化上的建设还有较大进步空间，诸如在迁移任务的量级分析、任务拆分、持续性调度、容错等步骤都可以做更多的自动化建设。</p><p></p><p>因StarRocks 2.5.13尚未支持CDC功能，当前的迁移方案暂只能提供离线同步的能力，在跨集群升级过程中，为保障数据的一致性，仍需要花费较多的精力，诸如协调新旧集群的双写、切流、补数等。</p><p></p><h2>未来规划</h2><p></p><p>方案中一些功能点，可以封装成原子功能，供更多场景使用。封装随着新版本StarRocks稳定性逐渐增强，组件自身bug影响稳定向的概率已经非常低了，跨集群升级的场景需求也越来越少。但方案中的原子能力，诸如库表特征分析、跨集群的shcema同步、表重建等等，仍有继续打磨的空间，可以在日常运维中提供帮助。</p><p></p><p>数据迁移的实时CDC能力也是一项亟待补齐的能力，集成离线和实时迁移功能，将助力实现无感升级。</p><p></p><p>探索跨集群迁移流程将探索更多的适用场景，诸如基于资源利用率或稳定性的集群拆分、合并等场景。</p><p></p><p>引用：</p><p></p><p>https://docs.starrocks.io/zh/docs/2.5/loading/Flink-connector-starrocks/#%E5%AF%BC%E5%85%A5%E8%87%B3-bitmap-%E5%88%97</p><p></p><p>https://docs.starrocks.io/zh/docs/2.5/loading/Flink-connector-starrocks/#%E5%AF%BC%E5%85%A5%E8%87%B3-hll-%E5%88%97</p><p></p><p>https://docs.starrocks.io/zh/docs/2.5/unloading/Flink_connector/</p><p></p><p>*文/&nbsp;管虎</p><p></p><p>本文属得物技术原创，更多精彩文章请看：<a href="https://tech.dewu.com/">得物技术</a>"</p><p></p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fc6d15f00293a1869ee1aff46</id>
            <title>ELB Ingress网关助力云原生应用轻松管理流量</title>
            <link>https://www.infoq.cn/article/fc6d15f00293a1869ee1aff46</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fc6d15f00293a1869ee1aff46</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jul 2024 02:04:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: ELB Ingress, 云原生应用, 流量管理, 负载均衡
<br>
<br>
总结: 本文介绍了华为云CCE服务提供的ELB Ingress功能，帮助用户轻松管理云原生应用的流量。ELB Ingress基于Kubernetes Ingress，提供高性能、高可用、高安全的负载均衡能力，满足企业对流量管理的需求。通过ELB Ingress，用户可以灵活配置流量访问规则，实现对外访问机制，提高应用的可观测性和可维护性。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区<a href="https://bbs.huaweicloud.com/blogs/430487?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">《ELB Ingress网关助力云原生应用轻松管理流量》</a>"，作者：云容器大未来。</p><p></p><h1>背景</h1><p></p><p></p><p>通常情况下，K8s集群的容器网络平面和外部网络是隔离的，外部网络无法直接访问到集群内部的容器业务，如何为容器提供并管理统一的外部流量入口?社区提供的常见方式是使用Nodeport Service，Loadbalancer Service，Ingress等K8s资源对象来暴露集群内部的容器原生应用。Service对象提供了四层负载均衡能力，Ingress对象则提供了面向应用层访问（HTTP/HTTPS等）的七层负载均衡能力。</p><p></p><p>而随着云原生架构在企业内的普遍落地，容器作为云原生微服务应用的载体，需要面对更多挑战，如面对微服务的复杂组网，业务请求在云服务之间转发往往需要做源地址转换而导致流量分发损耗；游戏类、电商抢购类等业务在短时间内会进行频繁扩缩容，必须应对高并发的网络流量；网关入口流量应对互联网的安全攻击，如灰产、异常流量，需提供流量安全防护能力；此外，支持更加复杂的路由规则配置、多种应用层协议（HTTP、HTTPS、GRPC等）、应用蓝绿发布、流量可观测性等七层高级转发能力也逐渐成为了云原生应用的普遍诉求。Ingress Nginx，Ingress Kong，Traefik等开源社区方案虽然提供了丰富的七层流量治理功能， 但对于关键生产业务上云，企业在选择Ingress方案时,除了考虑功能性,还需要充分权衡安全性、可维护性和可靠性等方面的需求，以找到最佳平衡点。专业的云服务提供商提供托管的Ingress解决方案，能够较好的应对这些挑战。</p><p></p><p><a href="https://www.huaweicloud.com/product/cce.html">华为云CCE服务</a>"提供了基于应用型负载均衡ELB（Elastic Load Balance）的全托管免运维的企业级 Ingress 流量治理，让用户轻松应对云原生应用流量管理。</p><p></p><h1>ELB Ingress 介绍</h1><p></p><p></p><p>在K8s集群中，容器网络平面通常是独立于集群主机网络的一个隔离的网络平面，工作负载在滚动升级或者重新调度后容器的地址会有变化，这就带来一个问题：如何实现某组Pod的服务发现，并提供固定的外部访问入口？Service和Ingress对象就是K8s中实现集群内外应用统一访问入口的一种机制。</p><p></p><p>K8s社区对集群外部的流量暴露提供了三种方式：Nodeport Service、Loadbalancer Service、Ingress，前两者Service对象主要提供集群四层流量入口，Ingres对象提供七层流量治理能力。两者相互配合，共同实现K8s集群应用的对外访问机制。如下图一所示，客户端通过Ingress管理的负载均衡器，访问Ingress申明的路由，由负载均衡器将流量经过后端Service导入至后端容器。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c4/c4a4e0d7ec73508c59f11dd24a8fcb3f.png" /></p><p>图一：Ingress示例</p><p></p><p>ELB Ingress是华为云CCE服务提供的七层流量治理功能，基于社区标准Ingress API实现，提供高可用、高性能、高安全、多协议的全托管免运维负载均衡能力。同时具备弹性能力，在流量突发时支持快速扩展计算资源，支持千万级并发连接，百万级新建连接，是云原生应用流量治理的理想选择。</p><p></p><h1>ELB Ingress工作原理</h1><p></p><p></p><p>ELB Ingress部署于CCE集群的master节点上，与ELB实例对接，可将Ingress申明的容器后端地址、转发策略、路由等信息配置至ELB实例，并且支持动态更新。</p><p></p><p>图二是基于Nodeport中转的ELB Ingress工作流图，CCE Standard集群使用该方案的原理如下：</p><p></p><p>用户为集群创建Ingress资源，在Ingress中配置流量访问规则，如负载均衡器实例、URL路由、SSL证书等监听信息，以及访问的后端Service等，控制器通过标签选择器选中工作负载，将工作负载所在节点和Nodeport端口挂载至负载均衡器实例的后端；Ingress Controller监听到Ingress资源发生变化时，会根据其中定义的流量访问规则，在ELB侧重新配置监听器以及后端服务器路由;用户通过ELB访问应用程序，流量根据ELB中配置的转发策略转发到对应的Node节点，再经过Nodeport二次转发访问到关联的工作负载(Nodeport转发机制参见k8s官方文档说明)。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9d298f0c858f5d3c554dfffae0be52d9.png" /></p><p>图二: Nodeport中转的ELB Ingress流程图</p><p></p><p>该方案中流量经过节点、IPTables/IPVS规则多次转发，网络性能存在损耗。在大流量场景下，网络转发效率、网络连通速度的挑战尤为突出。为此，我们推出了基于CCE Turbo集群的网络加速方案：容器直接使用VPC网络实现直通容器的ELB Ingress，将原有的“容器网络 + 虚拟机网络“两层模型简化为一层。如图三所示，集群中的Pod IP直接从VPC中分配，支持北向ELB直通容器，外部流量可以不经过节点端口转发直接访问集群中的Pod，达到流量分发零损耗的效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f679908cfcc9bd82d14e4511c82cecf7.png" /></p><p>图三:容器网络直通的ELB Ingress流程图</p><p></p><h1>ELB Ingress流量治理核心优势</h1><p></p><p></p><p>ELB Ingress基于原生Kubernetes Ingress，通过声明式API指定Ingress的路由、对接的后端服务，或者通过Annotation配置监听侧的高级选项，由系统保证最终一致性。ELB Ingress为开发者和运维人员提供了极大的开发灵活性和维护便利性，其核心优势包括：</p><p></p><p>高吞吐、高可用、高弹性</p><p></p><p>ELB Ingress搭配独享型ELB实例，最高支持2千万并发连接；通过完善的健康检查机制，保障业务实时在线，支持多可用区的同城双活容灾，无缝实时切换；弹性规格ELB实例支持根据流量负载自动弹性扩缩实例规格，适用于业务用量波动较大的场景，例如游戏、视频等行业，能满足瞬时流量同时成本最小化。</p><p></p><p>高安全性</p><p></p><p>ELB Ingress提供了端到端的全链路安全策略，如下图四是外部流量经过ELB访问CCE Turbo集群的简单示例：在访问端可配置接入WAF引擎检测并拦截恶意攻击流量，而正常流量转发至后端云服务器。通过Ingress的Annotation配置可轻松为ELB实例配置自定义安全策略，例如设置黑白名单，双向认证等。从ELB转发至后端也支持HTTPS加密信道，进一步增强整体安全性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5dcc4a6f8f6b9a455e276b0b974f7869.png" /></p><p>图四:&nbsp;外部流量访问CCE Turbo安全示例</p><p></p><p>可移植性</p><p></p><p>完全兼容社区Ingress语义，从开源Nginx Ingress等方案迁移过来仅需改造annotation即可轻松适配。</p><p></p><p>可观测性</p><p></p><p>云监控可以按时间轴查看ELB的网络流量和访问日志，动态分析并告警潜在风险；云审计可以实时监控ELB资源更新日志，针对风险动作实时告警，动态监控云上资源安全；Ingress Controller也支持丰富的普罗监控指标，如接口调用时延，reload次数等。</p><p></p><p>免维护性</p><p></p><p>ELB Ingress组件运行在集群的Master节点，用户无需关注运维问题，组件在集群升级时会自动更新，且对业务无感。</p><p></p><h1>ELB Ingress流量治理核心功能</h1><p></p><p></p><p>在社区基础功能之上，华为云ELB Ingress在负载均衡、路由规则、流量控制、安全性和可观测性等方面都有较大增强，满足了更复杂的生产环境需求。下面介绍ELB Ingress流量治理核心功能：</p><p></p><p>灰度发布</p><p></p><p>灰度发布是业界常用的版本升级平滑过渡的一种方式。在版本升级时，先让部分用户使用新版本，其他用户继续使用老版本。待新版本稳定后，再逐步扩大新版本的使用范围，直到所有用户流量都迁移到新版本上。这样可以最大限度地控制新版本发布带来的业务风险，降低故障影响范围，同时支持快速回滚。</p><p></p><p>我们提供了基于Header/Cookie/Weight的灰度发布策略，前两种策略通过将用户分成若干组，在不同的时间段内逐步引入新版本，最终扩大新版本的影响范围；基于Weight的策略则是通过控制新版本的权重，在不同时间段内逐步增加新版本的流量比例，直到完全替代旧版本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d0a529fefa223871db34e1f5c2e5f7e.jpeg" /></p><p></p><p>高级转发策略</p><p></p><p>随着云原生应用组网的日益复杂，传统的基于路由转发的七层流量治理已经难以满足需求。我们提供的高级转发策略可以很好地解决传统方案面临的局限性：</p><p></p><p>基于请求头的负载均衡：根据客户端请求头的不同值，将请求分配到不同的后端服务器。HTTP重定向到HTTPS：系统自动将HTTP监听器流量转发至HTTPS监听，提升网站安全性，防止内容篡改等。URL重定向和重写：支持将URL永久或临时映射到另一个URL。同时，支持正则表达式匹配和实现不同路径的重写规则。</p><p></p><p>慢启动</p><p></p><p>在应用滚动升级时，ELB Ingress会自动更新负载均衡器后端，并且根据后端容器实例副本数自动设置后端权重。但是，在后端健康检查通过后的上线过程中，可能面临流量突增，导致后端容器的CPU或内存资源瞬间高负荷，从而影响业务稳定性。在开启慢启动模式后，系统可以在指定时间内，逐步将流量导入到目标容器后端。这样可以缓解业务容器突增的流量压力，保护系统免受过度负载的影响，实现优雅过渡。</p><p></p><h1>小结</h1><p></p><p></p><p><a href="https://www.huaweicloud.com/product/cce.html">华为云CCE服务</a>"的ELB Ingress基于华为云应用型负载均衡ELB（Elastic Load Balance）提供强大的Ingress流量管理能力，兼容Nginx Ingress，具备处理复杂业务路由和证书自动发现的能力，支持HTTP、HTTPS和GRPC等协议，满足在云原生应用场景下对超强弹性和大规模七层流量处理能力的需求。</p><p></p><p>后续我们还将发布系列文章，详细介绍基于ELB Ingress的流量管理最佳实践，欢迎各位读者继续关注。</p><p></p><p>相关链接：</p><p></p><p>华为云云容器引擎CCE服务路由概述：<a href="https://support.huaweicloud.com/usermanual-cce/cce_10_0094.html">https://support.huaweicloud.com/usermanual-cce/cce_10_0094.html</a>"Ingress官方文档：https://kubernetes.io/docs/concepts/services-networking/ingress/</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/HouLv4z065YfZMqsSNUz</id>
            <title>如何构建高质量数据集与进行公正模型评测，AICon 带你一探究竟</title>
            <link>https://www.infoq.cn/article/HouLv4z065YfZMqsSNUz</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/HouLv4z065YfZMqsSNUz</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jul 2024 11:14:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 高质量数据集, 数据集构建, 模型评测, 大模型
<br>
<br>
总结: 本文讨论了高质量数据集对大模型性能的重要性，以及数据集构建和模型评测的关键议题。同时介绍了AICon全球人工智能开发与应用大会策划的【数据集构建以及评测】论坛，涵盖了数据集构建策略、模型评测方法以及模型公平性和透明度的保障。文章还推荐了几个精彩议题，包括大模型行业应用、多模态评测相关进展以及幻觉评估的新方法。 </div>
                        <hr>
                    
                    <p>高质量的数据集对于大模型的性能至关重要。获取这样的数据集需要经过精心的数据收集、清洗、标注、增强和平衡处理。同时，数据安全和隐私保护也是不可忽视的环节。大模型的评测同样重要，它包括准确性、鲁棒性、泛化能力、效率、可解释性以及伦理和偏见的考量。</p><p></p><p>AICon 全球人工智能开发与应用大会针对这些关键议题，策划了【数据集构建以及评测】论坛。这个论坛将聚焦于数据集的构建策略、模型的评测方法，以及如何确保模型的公平性和透明度。目前已经有几个精彩的议题</p><p></p><h4>精彩推荐议题一：</h4><p></p><p></p><p>如果有一个分享，可以带你了解全栈式行业数据处理和模型训练的方法，那你应该听听！</p><p></p><p>近年来，闭源大语言模型（LLMs）和开源社区在通用领域取得了显著进展，甚至在某些方面超越了人类。然而，在医学、政务等专业领域，语言模型的表现仍然不足。面对决这些挑战，智源研究院通过行业合作伙伴联合实验室机制，基于行业数据集构造和示范模型训练实践，提出了数据集构建技术体系，以及包含持续预训练、监督微调（SFT）以及强化学习（RLHF）技术的完整行业模型训练范式，获得了良好的模型性能效果。</p><p></p><p>我们非常荣幸邀请到<a href="https://aicon.infoq.cn/202408/shanghai/presentation/6017">北京智源人工智能研究院大模型行业应用总监周华</a>"老师，在本次演讲中，他将首先介绍人工智能大模型在行业落地的发展趋势，并分析当前面临的主要问题。随后，他会分享智源研究院在推动大模型行业落地方面的工作思路和研究方向。接着，他将详细讲解行业数据集构建的范式，以及行业模型训练的有效方法。在演讲的实践案例部分，周华将依次分享两个案例：首先是 Aquila-Med 示范模型的数据集构建和模型训练经验，其次是 Aquila-SQL 模型的训练过程及其在实际应用中的表现。</p><p></p><p>通过他的分享，你可以了解到企业内部大模型构建的方法、行业大模型训练的技术经验以及数据处理的方法和技术体系。</p><p></p><h4>精彩推荐议题二：</h4><p></p><p></p><p>如过有一个演讲，能带你了解了解多模态评测相关进展，那不能错过，尤其还是<a href="https://aicon.infoq.cn/202408/shanghai/presentation/6028">北京大学二级教授张铭</a>"的分享。</p><p></p><p>现有的数据集主要集中在检验模型解决专家级别难题的能力上，难以反映模型在基础知识方面的掌握情况。由于缺乏和人类表现相关的数据，因此科学家也不可能获取到更具实际意义的模型表现参考。为了攻克这些局限性，张铭团队构建了首个多模态 STEM 数据集，并且在此基础上实现对大语言模型与多模态基础模型的评测。评测的结果发现，即使是目前最先进的人工智能模型，其 STEM 基础水平也存在较大的提升空间，尚不具备解决更有难度的现实问题的能力。</p><p></p><p>此外，张铭团队还提出了一个新的社会学科数据集 Social，包含较大规模的文本评估数据，可用来评测大语言模型的社会学科基础能力；团队还设计了一种多智能体交互的方法，能够增强大语言模型在 Social 数据集上的表现。</p><p></p><p>我们非常有幸邀请到北京大学二级教授张铭，为我们分享《全方位评测神经网络模型的基础能力》话题，通过她的分享你可以了解到多模态评测相关进展探索以及大语言模型通用智能体方法进展探索。</p><p></p><p></p><h4>精彩推荐议题三：</h4><p></p><p></p><p>如果有一个演讲能够带你了解掌握幻觉评估的新方法、探索出模型幻觉原因与解决方案，那错过智源的分享就太可惜了。</p><p></p><p>大型语言模型 (LLMs) 在各种任务中取得了卓越的性能, 并在现实世界中得到了广泛应用。然而，LLMs 容易出现幻觉, 生成与已知知识相冲突或不忠实于原始信息来源的内容，影响了 LLMs 在很过高厉害场景上的应用。</p><p></p><p>现有的幻觉基准主要关注句子或段落层面的幻觉检测, 忽略了对话层面的评估、幻觉定位和原因解析。为了缓解现有幻觉评估的局限性, 智源提出了 HalluDial, 第一个全面的大规模自动对话级幻觉评估基准。</p><p></p><p>利用 HalluDial, 智源对 LLMs 在信息搜索对话中的幻觉评估能力进行了全面的元评估, 并引入了一个专门的判断语言模型 HalluJudge。HalluDial 的高数据质量使 HalluJudge 在幻觉评估中取得了优异或有竞争力的性能, 有助于自动评估 LLMs 中的对话级幻觉。</p><p></p><p>我们非常也有幸邀请到<a href="https://aicon.infoq.cn/202408/shanghai/presentation/6001">智源研究院智能评测组负责人杨熙</a>"， 她将分享《大语言模型的幻觉检测》话题，为你提供不一样的幻觉解决思路。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c7/c7a52ad4a2b10fc53692ad9f7e520906.jpeg" /></p><p></p><p>活动推荐：</p><p></p><p>InfoQ 将于 8 月 18 日至 19 日在上海举办 <a href="https://aicon.infoq.cn/202408/shanghai/">AICon 全球人工智能开发与应用大会</a>"，汇聚顶尖企业专家，深入端侧 AI、大模型训练、安全实践、RAG 应用、多模态创新等前沿话题。现在大会已开始正式报名，详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/db/db809a0579759615188699c6969bc438.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/LYq1YgWJw8qBrgvl1tU2</id>
            <title>零就业保障、全天精神“酷刑”！ChatGPT类产品背后80%贡献者，时薪1.16美元，但也没得选</title>
            <link>https://www.infoq.cn/article/LYq1YgWJw8qBrgvl1tU2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/LYq1YgWJw8qBrgvl1tU2</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jul 2024 08:53:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能技术, 数据工作者, 工作条件, 社交媒体内容
<br>
<br>
总结: 人工智能技术旨在自动化琐碎工作，但实际上数据工作者在处理社交媒体内容时面临严苛工作条件，包括长时间工作、低工资和接触不安内容。他们的体力和精神被榨干，工作环境令人不安。就业保障几乎为零，大多数员工签短期合同，工作不稳定。管理层对员工健康和安全关注不足，导致数据工作者面临心理和身体健康风险。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫、核子可乐&nbsp;</p><p>&nbsp;</p><p>“人工智能技术旨在使琐碎的工作自动化，但事实上，人们在被迫做更多常规、无聊和技能较低的工作。”甚至时薪不到&nbsp;2&nbsp;美元，工作条件还令人发指。这可能是每个关注人工智能行业的人都有必要一听的真实探访故事。</p><p>&nbsp;</p><p>近期，三位分别来自牛津互联网研究所（Oxford&nbsp;Internet&nbsp;Institute）和埃塞克斯大学（University&nbsp;of&nbsp;Essex）的学者，通过数百次访谈和数千小时的实地考察，公开揭露了经常被刻意隐藏在大众视线之外的人工智能产业底层纪实。他们分别是詹姆斯·马尔登（James&nbsp;Muldoon）、马克·格雷厄姆（Mark&nbsp;Graham）和卡勒姆·坎特（Callum&nbsp;Cant）。</p><p>&nbsp;</p><p>“大型科技公司正在以人类的体力和智力工作为食，无论是员工、创意人员、数据注释员还是内容审核员。”像亚马逊的人工智能系统中，供应链组织技术已经自动化了思维过程，而人类在亚马逊仓库里要做的就是残酷、重复的高劳力劳动过程，因为他们的工作场所里加入了算法管理系统。</p><p>&nbsp;</p><p>“当我们看到一个人工智能产品时，我们倾向于认为它是相对自发创造的，而没有考虑人类劳动、资源需求以及它背后发生的一切。”实际上，人工智能产品背后80%的工作是数据注释，而不是机器学习工程。以自动驾驶汽车为例，一小时的视频数据需要800个人工小时的数据注释。</p><p>&nbsp;</p><p>在去往肯尼亚和乌干达、评估当地数据注释与内容审核中心工作条件的实地考察中，这三位学者与几十名数据注释员进行了交谈。据他们称，现在社交媒体内容和人工智能训练数据的审核和标注工作多外包给南半球地区的工人。“在那里，长时间工作、低工资和接触令人不安的内容是常态。”</p><p></p><h1>体力和精神被榨干</h1><p></p><p>&nbsp;</p><p>内容审核员的任务就是手动浏览社交媒体帖子，负责删除有害内容并标记出违反公司政策的行为。数据注释员则为数据打上相关标签，帮助计算机算法更好地理解其中内容。在幕后默默运作的这两类“数据工作”让我们的数字生活成为可能，而背后从业者们的工作经历却极端到令人不安，毕竟这份工作的要求确实非常严格。</p><p>&nbsp;</p><p>一位从尼日利亚移民到乌干达的数据工作者表示，“我们的体力被榨干、精神也被榨干，每天活得如同行尸走肉。”轮班时间很长，工人们必须在速度和准确性方面满足严格的绩效要求。</p><p>&nbsp;</p><p>Mercy是一名为Meta工作的外包内容审核员，她需要在10个小时的轮班期间每55秒处理一张“工单”，工作就是确定其是否违反了公司提出的暴力或露骨内容封禁规则。其中，暴力与煽动比简单的霸凌和骚扰就更严重，所以单单发现一种违规行为还不算完，工作人员必须认真观察整个过程，揪出每一个违规细节，以防轻判导致恶劣内容的广泛传播。</p><p>&nbsp;</p><p>另一位审核人员解释称，“最令人不安的不仅仅是暴力，还有露骨的色情和令人不安的内容。”审核员们“几乎每天都会”目睹自杀、酷刑和强奸内容。这位审核员坦言，“种种不正常的畸形和病态，成为他们最平常的工作体验。”</p><p>&nbsp;</p><p>最严重的是，这些数据工作者不断受到极端图像和视频的轰炸，但根本没有时间消化自己受到的心理冲击。他们每天平均要处理500到1000张工单，许多人表示自己的内心已经麻木不仁，甚至造成了毁灭性的后果。</p><p>&nbsp;</p><p>一位被公司解雇的审核员指出，“我们大多数人都受到了心理创伤，有些人曾试图自杀……也有些员工的伴侣因为工作问题而离开了他们。”</p><p>&nbsp;</p><p>另一位员工表示，“公司的政策比工作本身还要严苛。”一位内容审核中心员工在看到斩首视频后情绪崩溃、浑身颤抖，但管理层对此给出的建议却简单而粗暴：可以在一周中选个时间休息30分钟，跟公司的“健康顾问”聊聊。但所谓健康顾问，是根本没接受过正式心理培训的公司同事。</p><p>&nbsp;</p><p>而那些因为无法承受视频内容而逃离办公桌的员工则被视为违反公司规定，因为他们没有及时在电脑上提交明确的“挂起”或者“出恭”状态码，所以相应的工作效率分可能被相应扣除。</p><p>&nbsp;</p><p>类似的故事数不胜数：“我在办公室晕倒了”、“我患上了严重的抑郁症”、“我需要去看大夫”、“他们根本不关心我们的健康”。雇员们再三重申，管理层所做的就只有监控医院记录来核实员工是否有资格提请病假，但从来不会安慰或真正关心他们的健康状况。</p><p></p><h1>就业保障为零，“你不干，有的是人想干”</h1><p></p><p>跟数据打交道的审核和标注工作是我们熟知的日常产品和技术服务的运行基础——从社交媒体应用到聊天机器人，再到最新自动化技术。如果没有内容审核员在后台不断处理各种帖子，那么社交网络很快就会被充斥着暴力与露骨内容的素材所吞没；如果没有数据标注员创建的数据集，AI算法就很难了解如何区分红绿灯和路牌信息，自动驾驶汽车自然也将无法上路；如果没有训练机器学习算法的雇员，也根本不可能出现像ChatGPT这样掀起AI革命浪潮的新工具。</p><p>&nbsp;</p><p>然而，这些数据工作者的就业保障也几乎为零。大多数员工签订的都是一到三个月的短期合同，客户订单完成后就可能原地下岗。他们的雇主是Meta的客户，Meta是一家总部位于旧金山的著名业务流程外包（BPO）公司，在东非设有交付中心，将不够稳定且标价过低的工作分配给当地员工。</p><p>&nbsp;</p><p>包括Mercy在内的许多雇员之前就居住在附近的基贝拉贫民窟中，这里是非洲最大的城市贫民窟，所以哪怕是如此可怕的岗位对当地弱势居民来说已经算是份福利。据员工们称，公司会要求投诉者闭嘴，并提醒他们“你不干，有的是人想干”。于是，相当一部分雇员都害怕惹恼管理层，因为他们担心失去这份工作。</p><p>&nbsp;</p><p>这些数据工作者中肯尼亚人占主体，但也有一些是来自其他非洲国家的移民，在这里主要负责帮助Meta审核其他非洲语种素材。部分移民员工表示由于言谈举止与当地人有别，他们在街上经常受到肯尼亚警方的骚扰和虐待。而且，警察骚扰并不是他们面临的唯一危险。</p><p>&nbsp;</p><p>一位女性审核员描述了周边非洲国家“解放阵线”成员因为对审核决定不满而搜索Meta审核员姓名和照片，并在网上对其发布人身威胁的情形。雇员们当然很害怕，但公司只回应称他们会考虑加强生产设施的安保，但除此之外只能由员工“自求多福”、自己想办法远离危险。</p><p>&nbsp;</p><p>据透露，这样模式的数据处理工作在全球多个国家的不同地点持续运作，涵盖数百万从业人员。在詹姆斯·马尔登等人开展考察之后，Mercy所在的工作场所条件有所好转。但像Meta这样的大公司往往拥有多家外包审核服务提供商，他们都愿意为了拿下合同而做出让步......</p><p>&nbsp;</p><p>如今的科技巨头们可以利用其财富和权力，随意调配AI领域之内数字劳动力在全球范围内的布局，而这自然造成了严重的生存状态差异。南半球国家的大多数工人，都就职于非正规的企业部门。由于失业率高得惊人，多数人很难找到拥有就业保障的高薪工作，弱势工人不仅愿意接受更低的工资待遇，甚至可以在工作条件方面做出重大让步，毕竟他们很清楚自己很容易被替代。</p><p>&nbsp;</p><p>而将工作外包给南半球国家之所以成为企业的首选，并不在于他们愿意为贫困群体提供急需的经济机会，而是因为这是一条通往高纪律性、高效率水平和低成本劳动力的确切路径。</p><p></p><h1>百亿估值的市场下，工人时薪1.16美元</h1><p></p><p>一说起AI开发，人们脑海中可能会自然而然浮现出身在硅谷光鲜亮丽、冷气充实的办公室里写代码的那群工程师。但大多数人没有意识到，整个AI模型训练期间有80%的时间其实是用在了数据集标注身上。无论是自动驾驶汽车、纳米手术机器还是无人机，这些前沿技术都是在古卢这样的地区被孕育而成。</p><p>&nbsp;</p><p>Anita是一名在为某家自动驾驶汽车公司做项目的数据工作者，在乌干达北部最大城市古卢的一家BPO企业工作。她的工作是连续几个小时观看司机驾驶的镜头，寻找一切可能代表注意力不集中的视觉证据，类似“打瞌睡”的情况。这有助于制造商根据司机的面部表情和眼球运动开发“车内行为监控系统”。</p><p>&nbsp;</p><p>而这份每周工作45个小时、精神高度紧张且压力巨大的工作所带来的回报，是每个月大约80万乌干达先令，折合200美元多一点。换算成时薪就是约1.16美元，这还不算偶尔出现的无偿加班。</p><p>&nbsp;</p><p>就在此刻，全球数据标注市场正在蓬勃发展。据估计，2022年这部分业务的总价值为22.2亿美元，后续预计每年将以30%左右的速度增长，到2030年将超过170亿美元。随着AI工具在零售、医疗保健及制造业（仅列举几个有代表性的转型期行业）中的广泛应用，全球市场对于数据处理的需求将继续与日俱增。</p><p>&nbsp;</p><p>并且，Anita及其同事们的工作和生活都要受到严格的数字监控与记录。从开始使用生物识别装置打卡进入办公地点的那一刻起，他们就时刻身处于全方位覆盖的闭路电视摄像机网络之下。他们的工作电脑上还安装有效率监控软件，轮班期间每一秒的操作都会被记录在案。</p><p>&nbsp;</p><p>一部分数据工作者甚至认为，管理层在员工内部建立了一套告密网络，以确保随时掌握雇员们的反抗行为或者建立工会的计划。</p><p>&nbsp;</p><p>连续几个小时不停地工作，对于身体和心理都是一种难以恢复的消耗。而这里的员工几乎没有接受疏导的机会——任务被简化到最极端的形式，以最大限度提高工人们的效率和生产力。反复培训之下，标注员已经习惯于以最快的速度一遍又一遍执行相同的例行操作。结果就是，人们身上普遍出现一种共性：极端的无聊加上令人窒息的焦虑。</p><p>&nbsp;</p><p>这就是被掩藏在AI革命前沿之下的现实：人们在极度紧张和时时刻刻的监控下工作，只为了保住饭碗、能够养家糊口。</p><p>&nbsp;</p><p></p><h1>结语</h1><p></p><p>&nbsp;</p><p>我们使用的每一款AI产品，都链接着全球各地打工人们的生活。无论大家是否喜欢，这种联系都客观存在。我们必须清醒地认识到，使用搜索引擎、聊天机器人，甚至是智能扫地机这样的产品，都意味着我们享受到了全球数据与资本的迅速流动，也最终在地球各个角落的雇员、组织和消费者之间织起了供求的大网。</p><p>&nbsp;</p><p>科技企业当然清楚这背后隐藏的残酷真相，因此他们会尽可能将产品制造环节隐藏起来，同时刻意展现一种靓丽、时尚且自主的科技发展姿态——计算机搜索大量数据，边运行边自学。至于这些数据从哪里来，负责整理这些数据的是怎样一群报酬低廉、生活困苦的人们，他们是向来不愿提及的。</p><p>&nbsp;</p><p>但更可怕的是，没人会主动辞去这份数据外包工作——因为在乌干达的国土上，根本没有更好的选择。只要有其他机会，哪怕是足够把人逼疯的审核和标注岗位，这里的人们也会牢牢抓住。他们的面前只有一条路：埋头苦干、完成目标，保证无论发生什么自己都不会被解雇。</p><p></p><p>&nbsp;*本文采自詹姆斯·马尔登（James&nbsp;Muldoon）、马克·格雷厄姆（Mark&nbsp;Graham）和卡勒姆·坎特（Callum&nbsp;Cant）三位合著的《喂养机器：为人工智能提供动力的隐藏人类劳动》一书的部分节选内容。&nbsp;</p><p>参考链接：</p><p><a href="https://www.theguardian.com/technology/article/2024/jul/06/mercy-anita-african-workers-ai-artificial-intelligence-exploitation-feeding-machine">https://www.theguardian.com/technology/article/2024/jul/06/mercy-anita-african-workers-ai-artificial-intelligence-exploitation-feeding-machine</a>"</p><p><a href="https://www.theguardian.com/technology/article/2024/jul/06/james-muldoon-mark-graham-callum-cant-ai-artificial-intelligence-human-work-exploitation-fairwork-feeding-machine">https://www.theguardian.com/technology/article/2024/jul/06/james-muldoon-mark-graham-callum-cant-ai-artificial-intelligence-human-work-exploitation-fairwork-feeding-machine</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JrHjTqhJjr1MPmH4up9m</id>
            <title>工业4.0加速跑！华院计算发布钢铁行业大模型，向传统行业应用场景纵深</title>
            <link>https://www.infoq.cn/article/JrHjTqhJjr1MPmH4up9m</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JrHjTqhJjr1MPmH4up9m</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jul 2024 03:48:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 认知智能, 技术创新, 钢铁行业大模型
<br>
<br>
总结: 2024年7月5日，华院计算技术（上海）股份有限公司在上海举办了世界人工智能大会，围绕认知智能展开深入交流，推动技术创新和钢铁行业大模型的发展。 </div>
                        <hr>
                    
                    <p>2024年7月5日，华院计算技术（上海）股份有限公司（以下简称“华院计算”）在上海世博中心举办了2024年世界人工智能大会“认知世界 智创未来”主题论坛，这也是华院计算第六次举办以认知智能为主题的学术研讨会。本次论坛围绕人工智能的最新理论、技术和应用场景展开深入交流，旨在搭建一个跨学科、多层次的全球性交流平台，促进学术界和产业界的沟通与合作，推动包括认知智能在内的人工智能前沿领域的技术创新与未来发展。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e8/e88854405724d4ea2e32c3bde884c3cc.png" /></p><p>&nbsp;</p><p>论坛由复旦大学数学科学学院教授林伟主持，美国卡内基梅隆大学计算机科学学院名誉教授、1995年图灵奖获得者及美国三院院士Manuel Blum，牛津大学DeepMind人工智能教授Michael Bronstein，香港大学副教授高盛华和复旦大学计算机专业博士王晓梅担任主讲嘉宾，分享他们的研究成果、行业见解和实践经验。</p><p>&nbsp;</p><p>论坛上，华院计算创始人、董事长宣晓华为本次论坛致欢迎辞。宣晓华表示在国家提出新质生产力与新型工业化战略的大背景下，人工智能已然成为推动经济社会高速发展的核心引擎。作为一家深耕在认知智能这一前沿领域的企业，华院计算的算法团队一直都秉承着"创新驱动、技术引领"的发展理念，并取得了一系列突破性进展，譬如法律大模型以及今天在现场发布的钢铁行业大模型等。他还表示除大模型外，算法团队正深入研究小样本学习、多模态机器学习、鲁棒学习和知识推理等核心技术，力求在认知智能领域实现重大突破。希望通过这些成果，为科技和社会的发展贡献自己的一份力量。</p><p>&nbsp;</p><p>值得一提的是，论坛上华院计算重磅官宣了以认知智能为基础的华院钢铁行业大模型。</p><p>&nbsp;</p><p>一直以来，华院计算致力于焦化和冶金行业的全流程工艺优化与智能化升级。通过全栈自研的认知智能引擎算法平台，不仅能解决生产中的复杂问题，更能推动行业生产力的质的飞跃。</p><p>&nbsp;</p><p>2023年中国粗钢产量达到10亿吨，稳居世界第一。然而高产量背后隐藏质量挑战。当质量问题的产品流入市场，不仅会引发客户的质量差异，还会给钢铁企业带来严重的经济损失。因此，提升质量检测水平，一直是钢铁企业关注的焦点。</p><p>&nbsp;</p><p>针对这一行业痛点，华院计算发挥了自身在感知智能和认知智能领域的双重优势，结合常年深耕于钢铁冶金行业的丰富经验，创新性地开发了一种融合图像识别技术和专家经验的缺陷检测算法，该算法通过深度学习技术训练，能够捕捉到产品表面的细微缺陷，大幅提高了检测效率和准确性。</p><p>&nbsp;</p><p>据介绍，这款钢铁行业大模型不仅减少了对人工检测的依赖，降低了成本，还提高了检测的一致性和可靠性。这款大模型的应用，预示着钢铁冶金行业智能化和高质量发展的新篇章。为行业的转型升级提供了强有力的技术支撑。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ns0KOIOpYxSjHqjIVblL</id>
            <title>网易员工内部群怼丁磊：人人陪你演戏点赞；李彦宏：开源模型是智商税；小红书再裁员：人效比只能达到拼多多的一半 | AI周报</title>
            <link>https://www.infoq.cn/article/Ns0KOIOpYxSjHqjIVblL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ns0KOIOpYxSjHqjIVblL</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jul 2024 02:46:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 李彦宏, 开源, 智商税, 大模型
<br>
<br>
总结: 李彦宏在世界人工智能大会上表示，开源模型实际上是一种智商税，认为闭源模型比开源模型更强大，推理成本更低。他还谈到了大模型的重要性，以及未来超级应用的发展方向。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>李彦宏认为开源其实是一种智商税；赛力斯发公告，宣布要以 25 亿元人民币收购华为持有的问界部分；天兵科技就火箭坠落致歉；英伟达将因涉嫌违反市场竞争行为受到法国反垄断机构控告；MacOS 版本的 ChatGPT 会以纯文本的方式来保留用户和 AI 的对话；马斯克透露 Grok 2 计划于今年 8 月推出。</blockquote><p></p><p></p><p></p><h2>热门资讯</h2><p></p><p></p><p></p><h4>李彦宏：开源模型是智商税</h4><p></p><p></p><p>在 2024 世界人工智能大会（WAIC 2024）期间，百度创始人、董事长兼首席执行官李彦宏对开源闭源、大模型价格战、智能体、超级应用、AGI 等业界热点问题表达了自己的看法。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fd904bd08e193966357e18a9860a26b4.jpeg" /></p><p></p><p>李彦宏认为，开源其实是一种智商税。“当你理性地去想，大模型能够带来什么价值，以什么样的成本带来价值的时候，就会发现，你永远应该选择闭源模型。今天无论是 ChatGPT、还是文心一言等闭源模型，一定比开源模型更强大，推理成本更低。”</p><p></p><p>而在世界人工智能大会上宣布通义千问已实现全尺寸、全模态开源的阿里云 CTO 周靖人则重申了阿里云拥抱开源、开放的立场。</p><p></p><p>一些行业人士则认为，开源和闭源并不是非此即彼的关系。百川智能 CEO 王小川曾预测，未来 80% 的企业会使用开源大模型，而闭源模型将服务于剩余 20% 的企业。</p><p></p><p>谈及“AI 超级应用什么时候出现”时，李彦宏表示，“不是说一定在等待一个超级应用的出现”。他认为，在基础模型之上，应该能够诞生数以百万计的各种各样的应用。“如果仅仅是从 0 到 1，你可能会希望出现某几个 Super APP，也就是几个公司从中受益。但是今天，几乎各行各业所有的公司，被大模型加持之后，它都能受益。这种影响力，对于整个社会、对于人类来说，无疑是更大的。”</p><p></p><p>商汤科技董事长兼首席执行官徐立对此表示赞同。“GPT 带来的聊天式的应用，Sora 带来的视频应用，还没有到超级时刻，因为它没有真正的走进一个行业的垂直应用。”徐立认为，超级时刻和应用是互相成就的，只有超级时刻带来认知的变化，最后才能推动超级应用。“就像 IPhone 一样，因为有 IOS，后面才有 IOS 上面的 App Store 生态。”</p><p></p><p>7 月 6 日，2024 世界人工智能大会暨人工智能全球治理高级别会议闭幕。据悉，本届大会共对接 132 个采购团组，形成 126 个项目采购需求，预计意向采购金额 150 亿元，推动 24 个重大产业项目签约，预计总投资额超 400 亿元。</p><p></p><p></p><h4>赛力斯拿出 25 亿收购华为问界商标</h4><p></p><p></p><p>7 月 2 号晚上，赛力斯发布公告，宣布要以 25 亿元人民币收购华为持有的问界部分，包括 919 项文字和图形商标以及 44 项外观设计专利。</p><p></p><p>赛力斯与华为签署的问界商标交易协议显示，问界商标的转让不影响双方的合作业务，双方将进一步深化合作关系，助力赛力斯造好车、卖好车。</p><p></p><p>据悉，近日，赛力斯与华为签署进一步深化联合业务合作协议。双方将充分发挥联合业务优势，将 AITO 问界打造为世界级新豪华汽车领先品牌。在原合作框架不变的前提下，赛力斯与华为充分发挥各自资源、禀赋优势，聚焦赛力斯汽车旗下的 AITO 问界品牌，双方联合设计、联合营销，为用户提供高端智能电动汽车产品和智慧出行解决方案，把 AITO 问界打造为世界级新豪华汽车领先品牌，双方共同实现商业成功。</p><p></p><p>这也是赛力斯和华为在 2023 年 2 月签署深化联合业务协议后，再度签署相关协议，标志着双方坚定发展联合业务模式和深入打造 AITO 问界品牌的决心。</p><p></p><p></p><h4>小红书新一轮裁员潮来袭，绩效 3.5 以下的员工成焦点</h4><p></p><p></p><p>据 7 月 4 日消息，小红书已于近日开启新一轮裁员计划。一位小红书员工表示，“裁员刚刚进行到锁 HC 阶段，正在进行人员盘点，但还没有进行官方通报，内部也都在等邮件”。</p><p></p><p>本轮人员调整主要聚焦于绩效在 3.5- 以下的员工，包含 3.5- 和 3.25，该部分员工约占员工总数的 30%。该消息得到了多位小红书内部员工的确认。据另一位内部员工表述，内部流传的说法是，“新高层对目前小红书人效比并不满意，认为当前小红书的人效比只能达到拼多多的一半”。</p><p></p><p></p><h4>多益网络子公司用“关小黑屋”方式劝退员工</h4><p></p><p></p><p>近日，游戏公司多益网络在社交平台公开发文，对成都高新技术产业开发区人民法院（以下简称“成都高新区法院”）关于其子公司与前员工刘某的劳动仲裁一案的判决结果提出异议，并引发了广泛的社会关注和热议。其中，尤为引人注目的是该公司被曝出的“关小黑屋”劝退员工的方式。</p><p></p><p>据了解，刘某作为多益网络子公司的一名员工，因拒绝被劝退而遭遇了极为不寻常的待遇。据刘某的陈述，在 2022 年 12 月 26 日至 12 月 30 日这四天里，他被公司安排在一个没有任何办公设备、没有正常供电的房间里，被剥夺了正常工作的权利。这个所谓的“小黑屋”里只有桌子和椅子，没有电脑、没有灯，更没有一个同事可以交流。刘某被限制在这个空间里，无法进行任何与工作相关或无关的活动，甚至连基本的通讯工具手机也被没收，使得他无法与外界联系。这种“关小黑屋”的做法，无疑是对员工尊严和权益的极大侵犯。</p><p></p><p></p><h4>奇瑞被曝强制加班卷工时，“896”成常态且没有加班费</h4><p></p><p></p><p>7 月 1 日，据报道，一位认证为“奇瑞员工”的网友发帖称，奇瑞汽车内部存在强制加班行为，每周加班时长需大于 20 小时并且没有加班费，仅补贴 10 元餐补。另外，员工加班调休时间严格限制，部分部门禁止调休申请、禁止请假，还会依照加班时长进行末位淘汰。</p><p></p><p>更让奇瑞汽车员工难以接受的是，为了掩盖加班事实，目前员工上班打卡历史记录已被关闭，默认展示打卡时间为 8:30 至 17:00。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a8/a89ec80c4aae37557b312cc51393230e.jpeg" /></p><p></p><p>一位在奇瑞汽车芜湖总部工作的员工确认称，其所在的部门，每个月工作时长至少是劳动合同上签署时长的 1.4 倍。“工作时间‘896’，周六强制加班，周日休息不保证。即使活干完了也不允许下班，必须留在工位上‘卷’加班时长。”</p><p></p><p>一位 2023 年进入奇瑞汽车的应届生统计，5 月他一共加班了 120 小时，加上正常上班的 174 小时，总计工作时长近 300 小时。但他到手的薪资只有 4800 元左右，算下来时薪只有 16 元。</p><p></p><p>去年 3 月，奇瑞汽车工程技术研发总院院长高新华发布内部邮件称，“通知所有的员工以奋斗者为本，周六是奋斗者的工作日。”有网友 6 月 21 日晒图，表示“在芜湖汽车厂，每天早八晚九，除了一个月 4 天的休息，每天日复一日，虽然一个月 9000 多不是很多，那又能有什么办法呢，谁叫又没背景又没文化的，只能打普工”，配图显示个人所得税 App 中 2023 年收入合计超 13 万元，月均超 1 万元。</p><p></p><p></p><h4>B 站崩了，小红书也崩了！阿里云紧急回应</h4><p></p><p></p><p>7 月 2 日早上，#B 站崩了 # 和 # 小红书崩了 # 词条相继冲上微博热搜榜。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bc/bc58e2f6c4d106616b1a8a686162afe1.jpeg" /></p><p></p><p>据报道，B 站 App 无法使用浏览历史关注等内容，消息界面、更新界面、客服界面均不可用，用户也无法评论和发弹幕，视频评论区和用户（UP 主）主页都无法加载。</p><p></p><p>这已是 B 站今年多次服务器崩溃，用户无法正常访问，此前官方解释为服务器负载过高。</p><p></p><p>随后，也有不少网友反映“小红书崩了”，刷新不出推荐内容。</p><p></p><p>对此，阿里云发布上海可用区 N 网络访问异常事件进展：北京时间 2024 年 07 月 02 日 10:04，阿里云监控发现上海地域可用区 N 网络访问出现异常，经阿里云工程师紧急介入处理后，于 10:35 完成网络切流调度后，10:42 访问异常问题恢复。若有任何问题，请随时联系。</p><p></p><p></p><h4>火箭坠落两天后，天兵科技道歉了</h4><p></p><p></p><p>7 月 2 日下午，天兵科技就火箭坠落致歉，对因此遭受财产损失的居民给予赔偿。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/01/01e6e3f344b2d606a2a15f71ae60a222.jpeg" /></p><p></p><p>此前，6 月 30 日下午，北京天兵科技有限公司“天龙三号”一子级火箭因试验故障坠落深山。根据天兵科技于 6 月 30 日晚发布的情况说明，事件发生时公司正在开展火箭一子级九机并联动力系统热试车。“因箭体与试验台连接处结构失效，一子级火箭脱离发射台。”“试车”是对火箭发动机开展地面试验的常用说法，换言之，此次火箭跌落事件属于地面试验过程中的“意外升空”。</p><p></p><p>据天兵科技方面的公开信息，天龙三号是天兵科技为我国卫星互联网星座建设量身定制的大型液体运载火箭，产品性能对标 SpaceX 的猎鹰 9 号，原定将于今年 7 月完成首飞任务。</p><p></p><p></p><h4>网易员工在内部群怼丁磊：人人陪你演戏点赞</h4><p></p><p></p><p>近日，一张有关网易创始人丁磊在员工内部群被一名员工怒怼的聊天记录在网上广泛传播引起了广大网友们的关注和热议。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/e4/e42e32d010609b2fb5175592284bea1a.jpeg" /></p><p></p><p>据了解，该员工是网易游戏的游戏开发工程师，毕业于华盛顿大学，已在网易工作四年多。目前该员工已经离职。</p><p></p><p>截图显示，群内多名员工点赞，直至石佳煊打断并艾特丁磊称：“何必，每个群每天几十个人陪你演戏点赞，PM 还要挨个催，假装一片和谐景象，有些人说你越来越像徐波。”</p><p></p><p>该员工提到的徐波是多益网络创始人兼 CEO。在此之前，他曾担任过网易游戏《梦幻西游》策划部门主管，在该游戏崛起过程中发挥了关键作用。后来因为和公司理念不合而离职，并创立了《神武》游戏品牌。</p><p></p><p></p><h4>有赞取消 HRBP 岗位，HR 该何去何从？</h4><p></p><p></p><p>近日，有赞的一则内部通知在 HR 圈内引起了不小的震动。通知显示，所有 HRBP 将全面转岗，直接向业务汇报，同时取消“组织成长部”。</p><p></p><p>根据公告，公司所有现任 HRBP 将在未来一周内（即截止至 7 月 12 日）完成内部转岗，他们将被要求直接向业务负责人汇报，并需正式承担起业务相关工作内容，彻底告别原有的人力资源管理工作。</p><p></p><p>对于此次调整的原因，有赞表示：“我们深知‘组织’的重要性，而信任我们的客户和共享愿景的团队成员是我们最宝贵的财富。尽管我们曾设立‘组织成长部’以推动基础人力资源服务、招聘培训及 HRBP 等工作的全面发展，但面对当前的经济形势和智能化时代的挑战，我们必须做出改变。</p><p></p><p>英伟达，遭遇反垄断调查！</p><p></p><p>7 月 2 日，据外媒报道，英伟达将因涉嫌违反市场竞争行为受到法国反垄断机构控告，这让法国当局成为首个对该公司采取行动的执法机构。</p><p></p><p>据悉，法国早在去年 9 月就对英伟达进行了突击检查，而本次检查的目的是对云计算领域进行更广泛的调查监管。在生成式 AI 应用程式 ChatGPT 发布后，英伟达对芯片的需求大幅增长，引发了大西洋两岸的监管审查。据了解，违反法国反垄断规定的公司可能面临高达其全球年营业额 10% 的罚款，尽管企业也可以让步避免罚款。</p><p></p><p></p><h2>IT 业界</h2><p></p><p></p><p></p><h4>MacOS 版 ChatGPT 被指以纯文本方式存储 AI 对话</h4><p></p><p></p><p>7 月 4 日讯，据 The Verge 报道，Threads 用户 Pedro José Pereira Vieito 表示，他发现 MacOS 版本的 ChatGPT 会以纯文本的方式来保留用户和 AI 的对话，这或将对用户的隐私造成威胁。</p><p></p><p>报道中，有人通过 Pedro Vieito 开发的应用程序，展示了如何一键读取 ChatGPT 的对话记录。这种易访问性，如果被恶意利用，后果不堪设想。据统计，macOS 用户在全球范围内超过 1 亿，而 ChatGPT 作为一款新兴的 AI 应用，其用户基数也在不断增长。OpenAI 在得知问题后，迅速发布了更新，对存储在 Mac 设备上的聊天记录进行了加密处理。根据 OpenAI 的声明，新版本应用的加密措施将有效防止未授权访问，确保用户对话的安全。</p><p></p><p></p><h4>马斯克：训练 Grok 3 用了 10 万块英伟达 H100</h4><p></p><p></p><p>7 月 2 日消息，亿万富翁埃隆·马斯克（Elon Musk）正在为其即将推出的 AI 聊天机器人 Grok 的新版本造势。马斯克周一在社交媒体 X 上回应了一则帖子，表示经过 10 万块 H100 芯片训练后的 Grok 3 聊天机器人将会“非常特别”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/00/0043eef1d0b1fc6d5c44c3ff98721979.jpeg" /></p><p></p><p>通过简单计算，单单训练 Grok 3 就用了价值 30 亿至 40 亿美元的人工智能芯片，目前不确定这些芯片是否是马斯克公司直接购买的，此前报道称，马斯克旗下的 xAI 公司正在与甲骨文谈判，计划在未来几年内花费 100 亿美元租用云服务器。近年来，马斯克的公司已直接购买了大量的 H100 芯片。据报道，马斯克曾将原本供特斯拉使用的价值 5 亿美元的 H100 芯片转给了 X 公司。相较于 Grok 2，使用 10 万块 H100 训练 Grok 3 无疑是一个巨大的提升。今年 4 月，马斯克在接受挪威主权基金负责人尼科莱·坦根 (Nicolai Tangen) 的采访时表示，训练 Grok 2 需要大约 2 万块 H100。</p><p></p><p>目前，xAI 已发布了 Grok-1 和 Grok-1.5 版本，最新版本仅对 X 平台上的早期测试者和现有用户开放。马斯克在周一的帖子中透露，Grok 2 计划于今年 8 月推出，并暗示 Grok 3 将于年底发布。</p><p></p><p></p><h4>苹果 M5 芯片首度曝光：由台积电代工</h4><p></p><p></p><p>据媒体报道，苹果 M5 系列芯片将由台积电代工，使用台积电最先进的 SoIC-X 封装技术，用于人工智能服务器。苹果预计在明年下半年批量生产 M5 芯片。目前苹果正在使用 M2 Ultra 芯片，今年使用量可能达到 20 万左右。SoIC 是台积电 3D Fabric 封装技术组合的一部分，业内首个高密度 3D chiplet 堆叠技术，其凸点间距最小可达 6um，提供更高的封装密度和更小的键合间隔。</p><p></p><p></p><h4>人工智能导致谷歌的温室气体排放量急剧攀升了 48%</h4><p></p><p></p><p>谷歌周二发布的年度环境报告显示，在过去五年中，谷歌改变气候的排放量增加了 48%，这与谷歌宣称的为地球实现碳中和的目标背道而驰。报告称，2023 年的温室气体排放总量比上一年增加了 13%，主要原因是数据中心能耗和供应链的增加。尽管谷歌一直在增加太阳能和风能等清洁能源的使用，但温室气体排放量还是出现了增长。</p><p></p><p>首席可持续发展官凯特 - 布兰特（Kate Brandt）和高级副总裁本尼迪克特 - 戈麦斯（Benedict Gomes）在报告中说：“尽管我们正在取得进展，但我们也面临着巨大的挑战，我们正在积极应对。”人工智能系统需要高水平的计算能力，这给这家科技巨头在世界各地的数据中心带来了巨大压力。</p><p></p><p>在最新的环境报告中，谷歌继续警告说，减少这些排放“可能具有挑战性”，尤其是在谷歌建设新的基础设施时。今年年初，该公司宣布将在英国投资 7.88 亿英镑建立一个全新的数据中心，以直接应对日益增长的人工智能需求。</p><p></p><p></p><h4>快手大模型首次集体亮相 可灵 AI 推出网页端</h4><p></p><p></p><p>7 月 6 日消息，在 2024 世界人工智能大会期间，快手举办了以“新 AI·新应用·新生态”为主题的大模型论坛，会上，快手大模型首次集体亮相，视频生成大模型可灵、图像生成大模型可图等产品的多项新功能正式发布。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/bb/bb1e0d5273cbf1608753dd8aabe6b514.jpeg" /></p><p></p><p>继图生视频、视频续写功能发布之后，可灵在一个月内迎来了第三次重大升级，网页端也正式上线。本次可灵推出了更加清晰的高画质版，以及首尾帧控制、镜头控制等新功能，同时，创作者单次生成的文生视频时长增加至 10 秒。图像生成大模型可图则宣布正式开源，这一举措旨在激发行业活力，共建一个更为繁荣的文生图大模型社区生态。</p><p></p><p>快手高级副总裁、主站业务与社区科学负责人盖坤介绍，快手搭建了以快意语言大模型、推荐大模型、视觉生成大模型（可图、可灵）为核心的大模型矩阵，覆盖内容生产、理解、推荐等多个层面，并深度服务快手的商业生态场景。其中，推荐模型 SIM 参数规模已达到十万亿的参数规模，下一代推荐大模型架构 ACT 预估每天可为快手带来 4 亿分钟的时长增长。</p><p></p><p></p><h4>无问芯穹发布千卡规模异构芯片混训平台</h4><p></p><p></p><p>7 月 4 日，在 2024 年世界人工智能大会 AI 基础设施论坛上，无问芯穹联合创始人兼 CEO 夏立雪发布了无问芯穹大规模模型的异构分布式混合训练系统，千卡异构混合训练集群算力利用率最高达到了 97.6%。</p><p></p><p>夏立雪在论坛上表示：“打开水龙头前，我们不需要知道水是从哪条河里来的。同理，未来我们用各种 AI 应用时，也不会知道它调用了哪些基座模型，用到了哪种加速卡的算力——这就是最好的 AI Native 基础设施。”</p><p></p><p>无问芯穹 Infini-AI 云平台的集成，为大模型异构千卡混训提供了强大的支持。该平台不仅具备万卡扩展性，而且支持 AMD、华为昇腾、天数智芯、沐曦、摩尔线程、NVIDIA 等六种异构芯片的大模型混合训练。夏立雪进一步介绍，从 7 月份开始，通过试训申请的用户已可在 Infini-AI 上一键发起 700 亿参数规模的大模型训练。</p><p></p><p>在过去的几个月中，无问芯穹 Infini-AI 大模型开发与服务云平台已经进行了首次公测。智谱 AI、月之暗面、生数科技等大模型公司客户已在 Infini-AI 上稳定使用异构算力。此外，还有 20 余家 AI Native 应用创业公司在 Infini-AI 上持续调用各种预置模型 API，使用无问芯穹提供的工具链开发自身业务模型。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6oY01YMMzEooHCRcWsLI</id>
            <title>大模型距离大规模落地还有多远？学术界、业界热议引围观</title>
            <link>https://www.infoq.cn/article/6oY01YMMzEooHCRcWsLI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6oY01YMMzEooHCRcWsLI</guid>
            <pubDate></pubDate>
            <updated>Mon, 08 Jul 2024 02:19:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 人工智能, 技术进化, 产业应用
<br>
<br>
总结: 在新一代科技革命中，以大模型为代表的人工智能技术正在重塑日常生活和产业生态，影响着人们的生产、生活、学习方式。专家们呼吁跳出思维框架，加强合作，培养复合型人才，以开放心态拥抱人工智能，实现主动式智能，促进大模型技术与应用场景的结合，为大模型可持续发展注入新活力。 </div>
                        <hr>
                    
                    <p>在新一代科技革命中，以大模型为代表的人工智能技术不断重塑日常生活和产业生态，深刻影响着人们的生产、生活、学习方式。这股蓬勃兴起的大模型热潮，未来将如何进化、如何在产业落地？</p><p></p><p>7月5日，在2024世界人工智能大会·腾讯“智创未来”论坛上，多位来自学术界和企业界的大模型领域专家，针对大模型研究范式、产学研结合、人才培养等话题展开热议。</p><p></p><h4>中国科学院院士胡事民：跳出窠臼，用开放心态拥抱人工智能</h4><p></p><p>在大模型等新一代人工智能技术的影响下，院校师生的研究思路、学科方向、职业方向都在改变。</p><p></p><p>面对这一新变化，中国科学院院士胡事民表示，希望国内大模型研究能够跳出国外的思想框架，积极探索原创思路的同时，加速促进大模型技术与应用场景的深度结合，为大模型可持续发展注入新的活力。</p><p></p><p>对于企业该如何提升自己、抢抓机遇，胡事民认为，企业应用开放的心态拥抱人工智能，并精准认识到自身在大模型时代的特殊优势“护城河”。比如独特的数据资源，创新性的行业模型和算法等，据此构建在大模型时代的牢固主动权，将为很多行业带来非常大的改变。</p><p></p><h4>上海交通大学马利庄：强化合作，实现主动式智能</h4><p></p><p>大模型出现后，三维视频的时序序列理解将成为未来重要的研究趋势。这其中，如何更好地理解人的意图、实现主动式智能成为关键。</p><p></p><p>上海交通大学特聘教授、人工智能研究院副院长马利庄表示：“主动式智能需要找出‘人物-行为-场景’一体化关联模式，即人的行为跟我们所处的环境、说什么样的话、做什么样的事之间的因果关系。以服务机器人为例，我喝了水，它会非常自然地帮我把盖子盖住，或者天气太热了脱下外套，它会主动把我的衣服给挂起来。”</p><p></p><p>要真正做到这些，马利庄指出，高校与高校之间、与优势企业之间要善用资源、通力合作，持续挖掘大模型技术与场景之间的结合点，打造更高层次的主动地人机交互，为大模型良性发展找到一条更合适的路。</p><p></p><h4>宾夕法尼亚大学苏炜杰：让人才培养更包容，更好理解垂直行业范式</h4><p></p><p>宾夕法尼亚大学副教授苏炜杰表示，通用人工智能到来，只是时间的问题。但在具象技术演进和发展的过程中，仍会出现波段性的影响。比如大模型对齐，虽然是看似最后一步，但难度会指数上升。“难度来自于需要对具体垂直领域有深度的理解，包括隐私、经济考量、社会规范等，” 他指出，特别是在多元化社会，不同人群的想法、预期是不一致的，怎么样调和不同的预期，涉及到经济、博弈等各方面的要求，这对学术界和企业界都提出了新的任务和挑战。</p><p></p><p>对此，苏炜杰认为，企业和高校之间加强合作，让人工智能人才培养、交流变得更加包容和开放，将促使端到端的人工智能有更好的范式、更好的理解。</p><p></p><h4>腾讯云吴永坚：坚持为客户创造价值，未来需要复合型人才</h4><p></p><p>从打造企点大模型客服、腾讯元宝、AI代码助手等产品，到对外输出知识引擎、图像创作引擎、视频创作引擎“三大引擎”，腾讯云在大模型时代到来后，始终坚持在用户和客户本身价值的基础上，演化相关能力。腾讯云副总裁、腾讯云智能产研负责人吴永坚介绍到，“目前大模型技术在越来越多的场景中落地并转化成实际生产力，例如，我们今年也为马利庄教授定制了一个高清4K小样本数字人，用在学术大会做学术内容介绍。未来通过知识引擎接入，马教授有机会邀请他的‘数字分身’辅助教学工作。”</p><p></p><p>吴永坚同时指出，在这个过程中，To C场景下，用户对于内容准确性的预期相对较低，所以对产品的接受度更高，产品落地走得快一些；而面向“严肃场景”的To B产品，则走得更加稳妥、更加谨慎。为了更好地实现“两条腿走路”，腾讯云对技术、算法、工程、产品复合型人才有了更高的需求。“如果产品经理没计算机或者机器学习等相关背景，不仅平时对需求和算法理解不深入，导致工作效率低，更关键的是很难对产业问题进行深层次的思考和转化。未来，优秀的产品经理一定是‘人机结合’的，这样才能深度理解并高效解决产业问题。”</p><p>&nbsp;</p><p>活动现场，胡事民、马利庄、苏炜杰等在场的专家学者也对在学校读书的年轻人提出了新的期许。他们表示，在充满变化的大模型时代，年轻人要把自己培养成为复合型人才，保持好、保护好兴趣和好奇心，主动拥抱新技术，这样才能为行业应用的颠覆性突破和持久性创新蓄力。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/oF3SIQhV1ZLgRD8Zj3Zp</id>
            <title>如何借助数学打造更好的AI算法基础并消除大模型幻觉？</title>
            <link>https://www.infoq.cn/article/oF3SIQhV1ZLgRD8Zj3Zp</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oF3SIQhV1ZLgRD8Zj3Zp</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 15:14:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 数学与人工智能, 机器学习, 深度神经网络
<br>
<br>
总结: 2024年世界人工智能大会在上海举办，围绕数学与人工智能展开讨论，涉及机器学习和深度神经网络等主题，探讨最新研究成果和未来发展趋势。 </div>
                        <hr>
                    
                    <p>7月4日，由斯梅尔数学与计算研究院（Smale Institue of Mathematics &amp; Computation）主办的2024年世界人工智能大会（WAIC）“数学与人工智能”学术会议在上海世博中心圆满落幕。</p><p>&nbsp;</p><p>作为全球性高级别学术研讨会，此次会议由华院计算技术（上海）股份有限公司创始人董事长、斯梅尔数学与计算研究院执行院长宣晓华担任主持，美国卡内基梅隆大学计算机科学学院名誉教授、1995年图灵奖获得者及美国三院院士Manuel Blum，欧洲人文和自然科学院外籍院士、欧洲科学院院士、上海交通大学自然科学研究院院长、上海交通大学数学科学学院讲席教授金石，欧洲科学院院士、牛津大学应用数学教授Jose A.Carrillo，牛津大学DeepMind人工智能教授Michael Bronstein，伦敦大学学院人工智能中心主任、英国研究与创新署基础人工智能博士培训中心主任、UiPath杰出科学家David Barber，世界科学院院士、阿勒福赞杰出青年科学家国际奖得主、南非布隆方丹自由州大学和台湾中华医科大学教授Abdon Atangana，南非北方大学应用数学系教授、系主任Oluwole Daniel Makinde，阿联酋人工智能大学副教授、副系主任Martin Takac出席，菲尔兹奖得主、法国高等科学研究所（IHES）终身数学教授Laurent Lafforgue，澳大利亚国立大学计算机科学研究学院（RSCS）荣誉教授Marcus Hutter以及西南财经大学统计学院教授、统计研究中心主任、博士生导师林华珍通过线上方式参与了此次圆桌讨论。上海市经济和信息化委员会主任张英出席会议并致辞。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bd/a4/bd07ba7d063e50ed217053f784a8afa4.png" /></p><p></p><p>这些全球顶尖的数学家和科学家们围绕机器学习与人工智能的数学基础、人工智能中的算法研究、AI4Science以及AI4Math等主题进行深入讨论，共同探讨数学与人工智能领域的最新研究成果和未来发展趋势。</p><p>&nbsp;</p><p>会上，94岁高龄的斯梅尔数学与计算研究院名誉主席斯蒂芬·斯梅尔教授以线上视频的形式发表了他关于“21世纪的18道数学问题”中的“智能的极限”的观点。1998年，斯梅尔列出了21世纪的18道数学问题。“斯梅尔问题”，沿袭了1900年著名的希尔伯特数学问题的精神。“斯梅尔问题”有一部分就来自希尔伯特数学问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf6a8ead7d563632cd0c2c77fa4e9b05.png" /></p><p></p><p>会议围绕三大议题进行探讨，在关于“如何借助数学打造更好的人工智能算法基础（特别是深度神经网络和Transformer领域），从而提升人工智能算法的效率和鲁棒性、增加因果推理能力和可解释性，消除模型的幻觉现象等？”的议题上，Michael Bronstein教授发表了深刻见解，他高度肯定了数学家在构筑人工智能算法基础方面的卓越贡献。Bronstein教授从两个维度进行了详尽剖析：一方面，不论是预测性人工智能还是生成式人工智能，其核心均离不开优化过程，因此数学家的任务就是不断地探索与开发更好的算法，提高算法的效率。另一方面，他强调数学分析对于理解人工智能理论的重要性，特别是生成式人工智能，其执行任务的能力在很大程度上依赖算法的设计，而数学家通过优化算法，不仅提升了预测的准确性，还赋予了AI更强的预见性，使其在面对复杂任务时能够做出更为合理的决策。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/9d/e3/9d491ff1a4da4f94d290c55a575042e3.png" /></p><p></p><p>这一观点得到了在场嘉宾的广泛共鸣，Martin Takac教授则进一步阐述道：“我们希望可以不断推进、拓宽并深化算法的边界，追求算法更高的效率与效能。”此番讨论奠定了数学研究在人工智能方面的演进中所扮演的重要角色。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5dcd7a509f714810bb39e8fb9e892ecb.png" /></p><p></p><p>随着人工智能的飞速发展，它已广泛渗透至生活的各个角落，虽然为现代人类社会提高了前所未有的效率，带来了诸多的便捷与福祉。然而，人工智能依旧面临着一系列理论和实践上的挑战。因此，会议也以“对于通用人工智能（AGI）、大模型的涌现现象、意识智能等前沿研究领域有哪些好的数学模型？智能的极限又是什么？”为议题，深入剖析现有的数学模型是如何推动人工智能、大模型的发展，以此探讨数学与人工智能之间双向促进、共同发展的互动关系。针对此议题，David Barber教授深刻地指出数学的纯粹、清晰性和复杂的人类推理、语言、知识以及人工智能之间存在着巨大鸿沟。他强调，利用数学模型来驱动人工智能，促使人工智能更理解人类语言，是一项充满挑战又极具潜力的任务。同时，他也乐观地表示，目前已有的统计学、逻辑推理等已经为人工智能的发展奠定了坚实的基础，相信未来也会有更精准高效的数学模型来协助人工智能的发展。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/5a/f0/5af509cfa318ec038a76180ee3fda1f0.png" /></p><p></p><p>金石教授则从另一个角度切入，他认为一个理想的数学模型应当是要将领域知识和物理定律完美结合，这样的模型才能更有效地解决复杂的问题。Michael Bronstein在探讨“智能的极限”时，以富有哲理的比喻阐述了人类对于智能认知的演进过程。他提出现在的人类看到人工智能的进步，如同十年前我们看科幻小说幻想今天一样，虽然今日我们见证了人工智能的显著进步，却仍感觉有些不一样。他强调，人工智能的极限就如同人类不断追求与设定的新目标，是一个动态变化、永无止境的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/485dcaf829c03f8343cdeb2911bd1535.png" /></p><p></p><p>Marcus Hutter教授对于人工智能的见解深刻且前瞻，他坚信人工智能的作用不仅仅是预测，而是拥有影响世界的决策能力。因此，他提出通过将最优决策理论与未知世界的预测理论相结合，可以构建在任意未知环境中都能做出最优决策的AGI系统，如ASI（强人工智能）。在过去的几年里，Marcus Hutter教授已经证实了我们拥有很多优化的概率，他做出了将智力这一非正式概念数学化的努力，提出了一个从0到1的评分系统来评估AI的智能程度。他认为理想的智能测量应能捕捉所有智力的关键特征，包括记忆、概括能力、推理、理解力和创造力等。虽然目前的研究仍处于基础的开发阶段，但他积极利用逼近法来让计划得以继续执行，使其更优化、更完美，以确保每一步都朝着既定目前稳步前进。对于当前的数据模型，Marcus Hutter教授也持乐观的态度。他表示目前的数据模型虽然尚在发展之中，但未来他会让数据模型更加接近理想的状态。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/0b/6f/0bb814919b0c5b18913061a2451aeb6f.png" /></p><p></p><p>Abdon Atangana教授对此表示：我们每天都在创造与发明，然后通过验证我们的成果，来为人工智能注入更多的内容，让它接受更多的培训和进步。诚然，人工智能的出现可能让人类不再需要用自己的大脑进行研究，但是实际上人工智的发展仍然需要靠人类去进行深度参与和补充，尤其是它无法直接向我们验证新定理和新方法论。因此，关于人工智能的未来，我更想看到的是它可以超越现在的界限，可以替代人类验证一些新方向和新主题。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/2e/80/2e1043ccd177189952236043eaa82580.png" /></p><p></p><p>数学作为人工智能的基石之一，其基本理论和应用技术的深入研究是人工智能行稳致远的关键。因此，要想让人工智能在各行各业取得稳健的发展，必须先确保数学基础问题的有效解决。同时，人工智能的飞速发展和广泛引用，也推动了数学领域的研究不断向前，为数学提供了更多的研究视角、方法和挑战，促进了数学理论的创新和突破。既然人工智能的发展离不开数学的支持，那么人工智能是否也能反过来对数学产生促进作用？</p><p></p><p><img src="https://static001.infoq.cn/resource/image/b4/29/b49e3a46dbfba73acd85be6a017f6029.png" /></p><p></p><p>“人工智能如何助力数学研究，特别是在定理证明、证明验证以及猜想生成方面？”这是本次会议的最后一个议题。在这个议题上，Jose A.Carrillo教授以风趣又不失深刻的言辞表达了自己的看法：“对于我个人而言，我并不担心我的工作会被替代。诚然，目前的人工智能虽然在一定程度上可以可以辅助数学家进行错误的检查，避免失误，但是人工智能的发展仍然面临着诸多未解难题，至少我（作为一名数学家）目前这几年仍不会失业。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/725e493c7a2dc4be2139ad4a0f02e073.png" /></p><p></p><p>世间万物兼具两面性，数学与人工智能相互间的促成关系背后也可能潜藏风险。在现场观众对这一问题感到疑惑时，Manuel Blum教授以深邃的洞察力提出了独到见解，他指出：“人类总有一天可能都会毁灭，但是人工智能的出现并非这一宿命的必然推手，相反地，人工智能可能是协助人类规避风险的关键钥匙。”Oluwole Daniel Makinde教授对此表示赞同，他补充道：“我们应当以积极乐观的心态，相信人工智能会给我们带来创新！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/14/14da416a56e94fc7672626f38a91d0ec.png" /></p><p></p><p>上海市经济和信息化委员会主任张英代表上海市政府到场祝贺并欢迎全球各位数学家来到上海参加2024世界人工智能大会及“数学与人工智能”学术会议。张英主任强调，李强总理在WAIC会议开幕式讲话指出上海正全力构建一个技术策源、应用示范和制度创新人才集聚的高地。从技术策源的角度来看，数学就是推动技术策源最为核心与关键的力量。她进一步指出，鉴于数学与人工智能之间不可分割的紧密联系，政府高度重视人工智能的应用发展，以及数学和人工智能之间的关系和推动力。这不仅是本次“数学与人工智能”学术会议得以成功举办的重要背景，也是主办方对本次会议寄予厚望的根源所在。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/07/07e143574e1b82617ea1c142361ff24c.png" /></p><p></p><p>此次在WAIC会议主会场举办全球性数学与人工智能会议，充分体现了本次WAIC会议的全球性和理论前沿性，也体现了上海致力于打造全球性人工智能基础研究前沿和数学等AI算法技术相关基础学科研究高地的期许。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/cd/cdb6695ed0018212e42f88d46fb8f504.png" /></p><p></p><p>“数学与人工智能”学术会议，作为一次思想的盛宴、智慧的碰撞，见证了数学家们围绕前沿问题展开的激烈讨论与深刻洞见。在这里，思想的火花汇聚成照亮前行道路的明灯，预示着数学理论与人工智能技术的深度融合将开启一个充满无限可能的新纪元。在这个充满无限可能的新时代，我们有理由相信，数学与人工智能将携手并进，共同书写人类科技进步的崭新篇章。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/btNrraN8b6wpsaaR71vK</id>
            <title>阶跃星辰姜大昕：要实现AGI，“万亿参数”和“多模融合”缺一不可</title>
            <link>https://www.infoq.cn/article/btNrraN8b6wpsaaR71vK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/btNrraN8b6wpsaaR71vK</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 10:02:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AGI, 万亿参数, 多模融合, Scaling Law
<br>
<br>
总结: 阶跃星辰姜大昕在演讲中指出，要实现人工智能通用智能（AGI），必须同时注重“万亿参数”和“多模融合”两个方向的发展。通过对Scaling Law的探索和多模态能力的提升，才能最终实现AGI的目标。 </div>
                        <hr>
                    
                    <p>阶跃星辰姜大昕：要实现AGI，“万亿参数”和“多模融合”缺一不可</p><p></p><p>近日，在世界智能人工大会WAIC启明创投·创业与投资论坛上，阶跃星辰创始人、CEO 姜大昕博士发表了主题为《攀登 AGI 的路径与实践：万亿参数和多模融合》的演讲，分享了对于大模型发展现状与趋势的观察与思考。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6bd883870939b909e763c4c9355c610d.jpeg" /></p><p></p><p>阶跃星辰创始人、CEO 姜大昕博士发表演讲</p><p></p><p>在演讲中，姜大昕重点阐述了一个核心观点：探索AGI路径，“Scaling Law”和“多模态”是相辅相成、缺一不可的两个方向。两者齐头并进，最终到达AGI。</p><p></p><h2>Scaling Law 仍处于陡峭区间，万亿参数是基本出发点</h2><p></p><p></p><p>近年来，GPT 系列模型的演进，客观上验证了 Scaling Law 的有效性。模型参数量决定模型能力的上限。从模型效果看，参数量增大确实带来了性能上的飞跃。虽然业内围绕“Scaling Law还能走多远”尚未形成共识，但阶跃星辰认为，参数量接下来再提高一个数量级是依然成立的。Scaling Law 目前依然奏效，模型性能仍然在随着参数量、数据量和计算量的增加呈幂次方增长。在此发展过程中，万亿参数量已经成为一个基本的入门门槛。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/ca9095397dad987fe937d7da4313022b.png" /></p><p></p><p>正是基于这样的认知，阶跃星辰很早便启动了万亿参数模型的训练。从千亿到万亿，模型的参数规模提升了一个量级，难度也提升了十倍以上。为此，阶跃星辰加大资源投入，尤其在系统和算法上积极探索，最终走通了万亿参数 MoE 大模型训练的道路。在 WAIC 2024 上，阶跃星辰发布了全新的 Step-2 万亿参数语言大模型正式版。根据从逻辑推理、世界知识、数学和编程等多个维度进行的权威测试，Step-2 模型能力都已全面接近国际主流模型，在部分测试集甚至实现了超越。</p><p></p><p>多模态是构建世界模型的基础能力，将走向理解与生成的统一</p><p></p><p>在不断攀登 Scaling Law 的同时，阶跃星辰也强调，多模态是构建世界模型的基础能力，是通向 AGI 的必经之路。从算法角度看，世界模型的演进会分为三个阶段：</p><p></p><p>第一阶段是模拟物理世界；</p><p></p><p>第二阶段是通过具身智能和物理世界交互，主动探索物理世界；</p><p></p><p>第三阶段是通过发展系统能力，发现新的物理规律，归纳物理世界。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f3fa68826934a1cb6ac29e22a46e0f1.png" /></p><p></p><p>从模拟世界，到探索世界，再到归纳世界，多模态是贯穿这三个阶段的基本能力。目前，全球科技巨头正在积极探索并布局多模融合的路径，多模态大模型研发的脚步正越走越快。然而，多模态领域目前存在的问题是，视觉的理解模型和生成模型是分开发展的。其造成的结果就是理解模型的理解能力强而生成能力弱，或者生成模型的生成能力强而理解能力弱。因此，多模态大模型接下来面临的一项关键挑战，就是能否将理解和生成统一在一个模型里。</p><p></p><p>目前，阶跃星辰正在朝着这个方向努力，并取得了一些阶段性的进展。在 WAIC 2024 上，新升级的 Step-1.5V 千亿参数多模态大模型性能大幅提升，具备更出色的视频理解能力；新发布的 Step-1X 图像生成大模型，则是阶跃星辰首次推出多模态生成大模型。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0et63jbeKZQxvIgWP6kX</id>
            <title>智谱新发开源大模型：9B参数，覆盖编程场景</title>
            <link>https://www.infoq.cn/article/0et63jbeKZQxvIgWP6kX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0et63jbeKZQxvIgWP6kX</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 09:05:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, AI热潮, GLM-4, CodeGeeX
<br>
<br>
总结: 大模型是解决成本和收益平衡问题的关键，GLM-4和CodeGeeX代表了新一代基座大模型技术的前沿，引领着AI热潮的发展。 </div>
                        <hr>
                    
                    <p>“大模型能够在一个模型上提供泛化能力，解决一系列场景和应用的多样需求，从而解决成本和收益的平衡的问题，这是它的本质特点。”</p><p></p><p>7月4日，在世界人工智能大会的产业发展主论坛上，智谱AI CEO张鹏表示，当下因大模型而掀起的AI热潮和之前有所不同，在过去，AI技术解决了一些实际问题，但如今的大模型发展带来了更重要的类人认知能力。张鹏表示，在过去AI泛用性不够且成本太高。但大模型带来了一个新的机遇，它能够在一个模型上提供泛用化能力，这也是用新一代大模型技术赋能实体经济的主要方向——把原来一个底座投入很大但是收益很小的结构，变成一个倒金字塔结构，真正放大它的价值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/85/85f147036f20f5b468abc4d4ed399bcf.png" /></p><p></p><p></p><h2>GLM-新一代基座大模型技术前沿与产业应用论坛举办</h2><p></p><p>&nbsp;</p><p>7月5日，在由清华大学计算机系知识工程实验室主办，AI TIME承办，东浩兰生（集团）有限公司和智谱AI协办的GLM-新一代基座大模型技术前沿与产业应用论坛上，嘉宾们聚焦GLM-4大模型，共同分享了GLM-4大模型的最新研究成果和理论突破，探索GLM-4的技术前沿、产业生态和落地应用。</p><p></p><h2>第4代CodeGeeX发布免费智能AI编程助手</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/34/345ffc593098689440b96bc85d549a91.png" /></p><p></p><p>论坛上，智谱AI CodeGeeX技术负责人郑勤锴发布了第4代CodeGeeX代码大模型CodeGeeX4-ALL-9B。CodeGeeX4-ALL-9B作为最新一代CodeGeeX4系列模型的开源版本，在GLM-4强大语言能力的基础上继续迭代，大幅增强代码生成能力。使用CodeGeeX4-ALL-9B单一模型，即可支持代码补全和生成、代码解释器、联网搜索、工具调用、仓库级长代码问答及生成等全面功能，覆盖了编程开发的各种场景。在多个权威代码能力评测集的表现，是百亿参数量级以下性能最强的模型，甚至超过数倍规模的通用模型，在推理性能和模型效果上得到最佳平衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/788e1a25691168e6db4ff4992f187056.png" /></p><p></p><p>目前CodeGeeX的个人用户数量已经超过100万，目前CodeGeeX对个⼈用户完全免费，在各种主流IDE均可免费下载使⽤。</p><p></p><p>除了第4代CodeGeeX发布，论坛现场，清华大学计算机科学与技术系长聘教授黄民烈、中国人民大学信息学院计算机系副教授张静、浙江大学计算机科学与技术学院副教授杨洋、上海交通大学电子信息与电气工程学院长聘教轨副教授戴国浩、幂律智能创始人兼CEO涂存超等顶尖学者，深入探讨了GLM大模型对行业及产业发展的影响。</p><p></p><p>张鹏分享了GLM-4在应用中的多个创新案例，特别是在智能内容生成、行业自动化以及用户个性化定制服务等方面的突破。展示了GLM-4在复杂商业环境中的价值。</p><p></p><p>“过去几年智谱的商业实践为我们积累了非常多的经验，不敢说是 best practice，但是 better practice。”张鹏在演讲中表示。GLM-4 在智能体和工具调用等方面能力的突飞猛进，让企业内部原生 AI 架构的实现变为可能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c26f438a5203fbac4922ab38293c5f98.png" /></p><p></p><h2>GLM基座大模型携应用成果亮相WAIC 2024</h2><p></p><p>&nbsp;</p><p>WAIC 2024智谱AI展位展示了以智谱大模型开放平台bigmodel.cn和智谱大模型产品矩阵为核心的系列创新成果。</p><p></p><p>作为本届WAIC镇馆之宝，智谱大模型开放平台 bigmodel.cn 是体验智谱 GLM 系列大模型的最佳方式。全新升级的bigmodel.cn已经接入最新GLM大模型全家桶，一键微调、All Tools API 调用等新功能也已上线。</p><p>&nbsp;</p><p>不管是技术极客、专业工程师，又或者是寻求大模型能力的企业，都可以在平台上找到适合自己的产品和服务。目前bigmodel.cn已有超过40万企业客户和开发者，日均调用量为600亿Tokens，过去4个月API每日消费量增长了90倍以上。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s98WkDlZDxxf0P633O71</id>
            <title>隐私计算被推向新高度！信通院牵头编写《隐私计算产品通用安全分级白皮书》，现已发布</title>
            <link>https://www.infoq.cn/article/s98WkDlZDxxf0P633O71</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s98WkDlZDxxf0P633O71</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 07:43:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据要素市场, 隐私计算技术, 安全分级, 个人信息匿名化
<br>
<br>
总结: 产学研共同构建新技术标准体系，推动数据要素可信流通，解决隐私计算产品安全分级和个人信息匿名化制度，为数据要素流通行业提供技术思考和实践。 </div>
                        <hr>
                    
                    <p>如何让大规模高价值数据可信流通，成为数据要素市场发展的核心议题，亟需产学研届共同构建新的技术标准体系。7月5日，在2024世界人工智能大会上，围绕隐私计算产品通用安全分级和个人信息匿名化制度，国内多家产学研机构联合发布两份白皮书，为数据要素流通行业当下普遍遇到的挑战，提供最新的技术思考和行业实践。</p><p>&nbsp;</p><p>推动数据要素可信流通需要技术研发与标准制定通力配合。为了确保数据要素流通合规、安全和高效，产学研届正积极推进一系列的技术标准制定，聚焦解决不同隐私计算技术产品的通用安全分级，受控环境下的数据匿名化，以及数据离开运维域后的有效管控等问题。</p><p></p><h2>通用的安全分级框架，推动隐私计算规模化落地</h2><p></p><p>&nbsp;</p><p>隐私计算技术可以在保护隐私安全的前提下释放数据价值，是数据可信流通的核心技术之一，然而由于隐私计算技术路线众多，在产业落地应用中出现“讲不清”、“看不懂”、“不敢用”的情况。隐私计算产品需要安全分级方法，可以为实际产品选型提供指导，推动隐私计算技术实现大规模落地。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4e32b8291e16decfe2b23f60a5d3917e.png" /></p><p></p><p>一方面，隐私计算技术路线众多，且不断有新的技术涌现，应用场景方难以评估不同技术的安全程度。另一方面，因为各参与方信任程度不同、数据类型不同，不同场景需要达到的数据可控程度也是不同的，一味追求高安全，抑或是完全忽视安全，都是不可取的。</p><p>&nbsp;</p><p>当前，虽然针对单一技术路线已经有一些安全分级标准，但是不同技术路线的分级标准完全无法对应，用户无法对所有的产品进行横向比较，这些标准也不适用于新出现的技术路线。因此，适用所有技术路线的通用安全分级思路亟需明确，来引导数据跨域流通场景中不同隐私计算产品的技术选型和安全评估工作。</p><p>&nbsp;</p><p>本次发布的《隐私计算产品通用安全分级白皮书》逐一讨论隐私计算安全分级面临的诸多难点，包括技术路线特征不同难以进行统一分级、部分重要安全能力难以被分级和量化、安全是系统性问题涉及的维度多、范围广。针对以上挑战，给出通用安全分级的设计思路，包括按照攻防效果分级来屏蔽不同技术路线差异，在“可证安全”和“不安全”之间增加一个“抵御已知攻击”的分级水位，引入软件信誉度等更多维度量化“实现安全”，明确所有技术特征与安全分级的对应关系。</p><p>&nbsp;</p><p>该白皮书由蚂蚁集团、中国通信标准化协会大数据技术标准推进委员会、深圳国家金融科技测评中心、清华大学牵头编写，另有国内16家机构参与编写。编写指导组成员包括中国科学院院士、国际密码协会会士王小云，浙江大学计算机科学与技术学院院长、区块链与数据安全全国重点实验室副主任任奎等权威学者。</p><p></p><h2>破解个人信息“匿名化”困境，从技术与法律视角探索路径</h2><p></p><p>&nbsp;</p><p>数据作为新型生产要素，将深刻影响并重构经济社会结构，而数据要素的价值发挥关键在于不同主体、不同场景下的数据流通复用。其中，个人数据是当前利用价值最高、使用场景最多样、处理措施最成熟的数据，在数据要素市场中有着不可替代的作用。</p><p>&nbsp;</p><p>如何在个人隐私保护的基础上，实现数据价值开发，是产业界面对的棘手挑战。一方面，对于描述或标识特定自然人信息的数据，如自然人的姓名、身份证号码等，数据持有者掌握这类信息后，有可能出现隐私泄露、滥用等风险。对于自然人与数据持有者交互产生的描述行为痕迹信息的数据，数据持有者汇集大量个人痕迹数据后，经数据挖掘与分析可将数据价值不断放大，但也可能出现“大数据杀熟”等风险。另一方面，部分技术处理方式导致数据精度损失很大，实际场景无法使用，使得数据失去流通和应用价值。</p><p>&nbsp;</p><p>5日下午发布的《个人信息匿名化制度白皮书：技术与法律（2024）》由对外经济贸易大学、大数据技术标准推进委员会和蚂蚁集团共同发布。这是学术与产业界首次联合从技术与法律双重维度对个人信息匿名化问题做系统性梳理与阐释、探寻可落地技术方案与数据流通解决路径。</p><p>&nbsp;</p><p>当前，为平衡“数据流通”和“个人信息保护”的双重目标，《网络安全法》《个人信息保护法》特别设置了“个人信息匿名化条款”，将匿名化后的个人数据排除在个人信息保护之外。然而，由于匿名化条款的法律内涵和实施标准有待厘清，匿名化条款往往存而不用。</p><p>&nbsp;</p><p>“个人信息匿名化条款存而不用已经成为，数据交易流通和数据要素市场建构的最大瓶颈之一，”对外经济贸易大学数字经济与法律创新研究中心主任许可在发布现场表示。许可介绍，白皮书着重考察和对比了各国与匿名化制度密切相关的个人信息定义、去标识化或假名化制度、匿名化标准和开展匿名化的具体指引。</p><p>&nbsp;</p><p>白皮书建议，在数据基础设施的规划与建设过程中，应充分考虑个人信息匿名化相关处理技术与制度规范内容。为破解“个人信息匿名化”的困境，必须从单一的法律视角转向复合的“数据基础设施”的路径。匿名化条款可以拓展为一套融合法律和技术的基础设施，从而推动在不同行业、不同机构之间实现可信、安全的数据共享、开放、交易。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/drUo4YLaqKU1OCxSS2SF</id>
            <title>国内首份！清华大学、中关村实验室等机构联合发布《大模型安全实践（2024）》白皮书</title>
            <link>https://www.infoq.cn/article/drUo4YLaqKU1OCxSS2SF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/drUo4YLaqKU1OCxSS2SF</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 06:04:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型安全实践, 技术实施方案, 大模型安全应用案例, 五维一体协同共治
<br>
<br>
总结: 《大模型安全实践（2024）》白皮书首次系统化提出安全实践总体框架，以确保大模型在安全性、可靠性、可控性等维度下的技术实施方案。白皮书还介绍了金融、医疗、政务等领域的大模型安全应用案例，以及提出了“五维一体”协同共治的治理框架，为行业打造高价值参考体系。 </div>
                        <hr>
                    
                    <p>7月5日下午，清华大学、中关村实验室、蚂蚁集团等机构联合撰写的《大模型安全实践（2024）》白皮书（以下简称“白皮书”）在2024世界人工智能大会上正式发布。这也是国内首份“大模型安全实践”研究报告，为行业打造高价值参考体系。白皮书首次系统化提出安全实践总体框架，从安全性、可靠性、可控性等维度给到了技术实施方案，同时提供了金融、医疗、政务等领域的大模型安全应用案例，以及“五维一体”协同共治的治理框架。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1f/1f6e488362529f42684c9ba245553298.png" /></p><p></p><p>&nbsp;（图：《大模型安全实践（2024）》白皮书发布现场）</p><p>&nbsp;</p><p>大模型技术正成为推动社会进步和创新的关键力量。然而随着大模型能力的不断增强，其安全性、可靠性、可控性受到前所未有的挑战，如研发过程中引发信息泄露、价值对齐、机器幻觉等问题，以及落地过程中面临的数据、模型、算法及其运行的软硬件环境安全风险。</p><p>&nbsp;</p><p>面对以上挑战，白皮书提出了大模型安全实践总体框架。该白皮书确立了“以人为本，AI向善”为大模型安全建设的核心，确保技术进步服务于人类福祉；以“安全、可靠、可控”三个核心维度的大模型安全技术体系，并涵盖了大模型安全测评与防御的综合技术方案；以及“端、边、云”为大模型安全技术的主要承载实体。</p><p>&nbsp;</p><p>作为报告核心，大模型安全技术体系里，安全性意味着确保模型在所有阶段都受到保护，涉及数据安全、模型安全、系统安全、内容安全、认知安全和伦理安全等；可靠性要求大模型在各种情境下都能持续提供准确、一致、真实的结果；可控性关乎模型在提供结果和决策时能否让人类了解和介入，可根据人类需要进行调适和操作。通过这三个维度，可提升大模型的鲁棒性、可解释性、公平性、真实性、价值对齐、隐私保护等方向的能力。</p><p>&nbsp;</p><p>白皮书指出安全评测技术和安全防御技术也是保障大模型安全的有效手段，但目前大模型的安全评测绝大多数是针对内容类场景，随着大模型技术快速发展和广泛应用，对Agent这类复杂大模型应用架构和未来通用AGI的评估是当下面临的挑战。制定标准建立面向未来的大模型可信测评体系将会变得越来越重要，这需要政府、高校等机构，联合有相关经验的企业共同合作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef4c6508983c3df41d1ecebe941540ce.png" /></p><p>&nbsp;</p><p>（图：大模型安全实践总体框架）</p><p>&nbsp;</p><p>白皮书以蚂蚁集团自研的大模型安全一体化解决方案“蚁天鉴”为例，介绍了国内机构和企业在探索大模型安全应用的优秀实践。</p><p>&nbsp;</p><p>蚁天鉴是一款兼具大模型安全测评和防御的产品，目前已开放给20余家外部机构和企业，在金融、政务、医疗等重要领域得到采用，为行业大模型数据、训练、部署、应用等环节提供安全保障。</p><p>&nbsp;</p><p>例如，在金融场景，蚂蚁AI金融助理“支小宝”，通过“蚁天鉴”从大模型训练与推理风险管控、大模型风险点全方位评测、大模型用户交互风险管控三个方面保障大模型应用安全；针对金融业务，通过内嵌一致性检验和金融价值对齐，确保数据的准确性和金融逻辑的严格性。在医疗场景，上海市第一人民医院通过引入“蚁天鉴”平台，在其首创安全前置护栏技术保障下，可精准杜绝医院最关注的风险的出现，保障医疗大模型生成的内容更符合医疗垂类的安全和专业，有效应对大模型应用中的信息安全与隐私保护、双向内容风险防控等问题。在政务领域，“赣服通”政务AI助理在端侧实施的安全措施具有借鉴意义，其结合“蚁天鉴”通过千万政务预料训练来实现精准意图识别、智能追问反问和高频事项即问即办等功能；针对政务行业大模型应用中生成不可控、安全覆盖面广、内容对抗强、时效要求高等挑战，构建安全护栏和安全防御两大核心能力，覆盖数百项大模型内容生成风险，可应对单次50万量级的饱和攻击。</p><p>&nbsp;</p><p>清华大学长聘副教授、博士生导师李琦指出，大模型安全应用是一个新兴领域，研究和应用尚处于起步阶段。不少企业是在原有的传统数据安全、信息安全、系统安全等经验基础上，进行能力迁移，应用于大模型安全。随着新的大模型安全实践的不断深入，技术也会持续升级，为大模型安全构建实践范式，打造高价值参考体系。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/2f/2f5be00487616a82656a84c4805ffcc2.png" /></p><p></p><p>（图：蚂蚁集团安全实验室首席科学家王维强主题演讲）</p><p>&nbsp;</p><p>蚂蚁集团安全实验室首席科学家王维强在会上做了《大模型应用安全可信实践探索》的主题演讲。王维强认为，随着大模型的深度应用，在原有可信人工智能治理体系框架基础上，提升大模型的安全、可靠、可控建设，确保技术进步服务于人类福祉，是未来人工智能可持续发展的重要保障。</p><p>&nbsp;</p><p>白皮书最后还提出了构建集大模型安全政府监管、大模型安全生态培育、大模型安全企业自律、大模型安全人才培养、大模型安全测试验证“五维一体”多元参与、协同共治的治理框架。这对于大模型安全生态形成、大模型可持续发展具有非常重要和积极的意义。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wKgHwEaN5kybOstLSFSy</id>
            <title>清华大学汪玉：大模型能效提升，有几条必经之路？</title>
            <link>https://www.infoq.cn/article/wKgHwEaN5kybOstLSFSy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wKgHwEaN5kybOstLSFSy</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 05:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智能算法, 芯片设计, AI 2.0, 能量效率
<br>
<br>
总结: 本文讨论了人工智能发展的三个阶段，以及在AI 2.0时代面临的挑战。文章指出，大模型训练任务占据了大部分算力，而硬件能力提升速度跟不上计算需求增长的问题。同时，文章还提到了中国在芯片技术水平和算力规模方面受到的限制，以及团队在提升计算能量效率方面的研究目标。 </div>
                        <hr>
                    
                    <p>演讲嘉宾 | 汪玉 清华大学电子工程系教授、系主任 &amp; 无问芯穹发起人</p><p>审核｜傅宇琪 褚杏娟</p><p>策划 | 蔡芳芳</p><p>&nbsp;</p><p>进入生成式 AI 时代后，应用侧日益高涨的服务需求给基础设施的算力规模提出了巨大的挑战。与此同时，不断扩张的计算设施对能源供应和生态环境的压力也在飞速增长，迫使产业采取多种手段提升从芯片到集群，再到整个数据中心生态的能耗效率。SemiAnalysis 不久前发布的一篇报告指出，能耗水平将成为 AI 计算产业的核心竞争力要素，对整个产业的发展起到关键作用。</p><p></p><p></p><blockquote>在2024 年 5 月举办的 <a href="https://aicon.infoq.cn/2024/beijing">AICon 全球人工智能开发与应用大会暨大模型应用生态展</a>"上，清华大学电子工程系教授、系主任&amp;无问芯穹发起人汪玉围绕生成式 AI 时代的高能效计算发表了题为《<a href="https://aicon.infoq.cn/2024/beijing/presentation/5972">可持续的智能：大模型高能效系统前瞻</a>"》的演讲报告，本篇内容根据该报告编写、更新而来。InfoQ 将于 8 月 18-19 日举办 <a href="https://aicon.infoq.cn/2024/shanghai/track">AICon 上海站</a>"，我们已经邀请到了「蔚来创始人 李斌」，他将在主论坛分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。更多精彩议题可访问官网了解：<a href="https://aicon.infoq.cn/2024/shanghai/track">https://aicon.infoq.cn/2024/shanghai/track</a>"</blockquote><p></p><p>&nbsp;</p><p></p><h2>研究背景</h2><p></p><p>&nbsp;</p><p>在分析人工智能这个主题之前，首先要思考的一个问题是“到底何为智能”？诺贝尔奖获得者 Daniel Kahneman 在他的著作《思考，快与慢》中从人类智能的角度给出了一个视角：Daniel将人类的智能分成两类系统，第一类系统是“大脑快速、自动、直观的方法”，第二类是“思维的慢速、理性占主导的分析模式”。</p><p>&nbsp;</p><p>与人类智能的两类系统类似，人工智能的发展也经历了计算智能、感知智能与认知智能三个阶段。回顾人工智能的发展，我们可以将人工智能算法抽象为函数y=f(x)，其中 f 代表人工智能算法的计算规则，x 代表数据，y 则是决策。在计算智能时代，由人类制定f()的计算规则；在感知智能时代，人工智能算法从大量数据中进行学习，通过拟合的方式得到f()；而在认知智能时代，人工智能将从海量数据中挖掘并学习对象之间关系，获得模拟人类的认知能力的f()。</p><p>&nbsp;</p><p>而在芯片行业，从业者所解决的问题就是如何更高效地实现y=f(x)的计算。早期芯片设计人员将函数中所有最基本的元件抽象成加减乘除，设计指令集与通用处理器CPU ，然后用软件的方式实现各种各样的功能，追求的目标是让芯片可以快速支持通用计算任务。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/6c/6c38045641a8acbd252f2fdec908f6d5.png" /></p><p>&nbsp;</p><p>在AI 1.0时代，人们设计了一系列面向不同应用的智能算法，例如面向图像处理的卷积神经网络算法，面向自然语言处理的循环神经网络算法。芯片设计人员面向特定算法设计领域定制集成电路（ASIC），研究特定算法向硬件的部署优化方法。</p><p>&nbsp;</p><p>从过去的 AI 1.0 到今天的 AI 2.0 时代，最大的变化是：过去，算法研究者会面向每一个应用类别的数据来开发一个专门的算法模型，而现在，我们会使用所有的数据训练一个基础模型，再利用各行各业的专业数据，对基础模型进行微调，来完成各行各业的任务。对做系统或做硬件的人来说，只需要考虑如何优化这一个基础算法就可以了。</p><p>&nbsp;</p><p>AI 2.0 背后的挑战不言而喻。从规模角度来看，2018 年到 2022 年，模型参数量增加了至少 5 个数量级，现在还在不断增长。以 SORA 为例，其参数规模推测达到 300 亿，单次训练算力需求可达 8.4*10^23 Flops 水平。</p><p>&nbsp;</p><p>在硬件层面，行业面临的主要挑战是硬件能力的提升速度很难跟上计算需求的增长速度。以中国市场为例，如果将来 14 亿人同时在云端使用大模型，中国现有的智能算力基础设施将难以支撑14亿人的算力需求，差距可达3~4个数量级。虽然国内的基础设施建设在飞速前进，但我们确实也面临芯片短缺等挑战。</p><p>&nbsp;</p><p>从另外一个角度来看，现在大模型训练任务大致占到所有算力的 70%，大模型训练的能耗开销可达国内数百个家庭一年的用电量。但如果大模型应用开始普及，未来推理任务的算力占比大概会达到 70~80%，这才意味着大模型应用真正达到了成熟和流行的状态。</p><p>&nbsp;</p><p>2023 年美国对芯片出口提出了新的管制规定，限制了中国可以获得的芯片技术水平，也限制了中国在海外生产的算力规模。所以我们也在很努力地推进中国自主的芯片制造厂和工艺，这是整个国家和企业界在努力推进的方向，我相信在 5~10 年内我们是有希望解决这个问题的。</p><p>&nbsp;</p><p>回到我们团队的研究工作，我们的研究目标瞄准计算能量效率的提升。在我读博的2002 到 2007 年器件，我主要关注的是芯片工艺的进步，也就是 Scaling Down。比如：芯片工艺从 45nm 进化到 28-14nm 的过程中是提高能量效率的，因为晶体管越做越小，电容就会变小，每一次充电的能量就会变小，每一次的翻转的速度会变快。所以晶体管做小后，速度变快了，需要的能量变小了，所以能量效率就提升了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/554f0f588aa12b7c95582e612f407455.png" /></p><p></p><p>但是到了 2007 年之后，提升能量效率主要的路径就变成了加速器。因为工艺发展变缓了，大家发现多核甚至异构多核可能是一条路，芯片铺很多核就会有计算性能与能效的线性提升，所以我们会画一条线性的线来表示能效水平的提升。2010 年到 2020 年是 AI 加速器飞速发展的阶段，我们看到了 5 个数量级的能量效率提升。第三条途径则是新器件，包括量子计算、光计算、存内计算等，有希望突破现有的计算范式，以获得更高的能量效率。</p><p>&nbsp;</p><p>面向非神经网络的传统算法，一般来说会在算法设计完后再去做电路设计，但我们发现在人工智能时代，是有可能打通算法和电路的协同优化空间，而且该优化空间足够庞大。这也就意味着，任何一个算法，如果底层硬件是给定的，就可以通过微调，甚至是重新训练、重新选结构等方法针对底层硬件对算法进行优化，使算法在硬件上跑起来更快。这也是我所有的成果里最核心的一个方法论，也就是利用智能算法的可学习特性，同时优化算法和电路来实现能效的数量级提升，从不到 1GOPS/W 提升至 100TOPS/W。</p><p></p><h2>硅片上的能效系统：面向智能的软硬件协同优化</h2><p></p><p>&nbsp;</p><p>以 Llama-2-7b 大模型和 RTX4090 计算卡为例，模型直接部署在硬件平台上的能量效率是很低的，直接用 fp16 存储会遇到存储空间不足的问题，但改用 Int2 存储又会出现算法准确率很低的现象，这是当下大模型计算的一个关键矛盾。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fcca589ffa3cff4d35395b66af428399.png" /></p><p></p><p>对此，要运用可学习的特性，从算法到数据结构、数据表示、计算图，再到硬件，一同做联合优化。如果底层数据变成了 4bit、2bit 的乘法器，会比 32bit、16bit 的乘法器小很多，在固定的面积里可以放的计算单元会更多，从而提高峰值能力。而对计算图和架构的设计优化将可以有效提高计算资源的利用效率。</p><p>&nbsp;</p><p>在算法模型优化方面，我们发现大语言模型的输出具有结构上的并行性，因此，我们可以先根据用户输入，由大模型生成回答内容的提纲，然后再从提纲中的要点进行并行展开，使用这样的思想就可以把一部分大模型算法加速 2~3 倍。</p><p>&nbsp;</p><p>在数据结构优化方面，大模型的注意力计算层是比较占计算量的，当 token 数变多时计算量会很大。对此可以通过稀疏方法减小计算和存储的复杂度，并针对不同的注意力头（Attention Head）使用不同的稀疏模板，从而在将计算量存储量降低50%的情况下，使端到端推理速度再提升 2~3 倍。</p><p>&nbsp;</p><p>数据表示优化层面的主要方法是量化。算法的数据特征在各层都是不一样的，大模型参数与数据的动态范围也比较大。针对数据离群点问题，我们将大部分数据表示变成 Int2，一小部分管件数据还是用 fp16，从而将平均位宽做到 2.8bit 时，且平均精度损失也只有 1% 左右，这样就可以进行实际应用，而且塞到显存较小的卡里做算子优化。</p><p>&nbsp;</p><p>对于算子优化，我们发现不同的算子特征是不一样的，比如说 softmax 的输入分布比较集中，还有 decode 阶段的矩阵计算都是一些矮胖的矩阵。所以我们通过这些特征来调整计算流程、重写算子，也可以实现很高的性能提升，同时这个思想也可以用在所有的国产卡上。</p><p>&nbsp;</p><p>我们自己也做了硬件，把算法放到硬件上。我们使用单块 FPGA 对比工艺接近的 GPU，可以实现 6 倍左右的能量效率提升。所以这也是告诉大家，其实做推理芯片是可以的，但推理芯片怎样能去适应算法的变化是比较大的挑战。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/0a/0a03edb3b14d08ea7d464d181e5c6a28.png" /></p><p></p><p>整体来看，第一部分从算法本身到模型数据结构表示，再到计算图和结构上都可以做优化。再考虑到工艺进步，将还有 5～10 倍的能量效率提升。</p><p></p><h2>产业中的能效系统：AI 2.0 时代的算力生态建设</h2><p></p><p>&nbsp;</p><p>最近清华成立了人工智能学院，专注于两部分的研究，一部分叫 AI core，核心算法和算力；另外一部分是 AI+，即AI与其他各行各业的结合。尽管我们发现算力中心的规划涨幅没有我们预想的 100 倍那么大，但在算力规模方面的发展还是很迅速的。</p><p><img src="https://static001.geekbang.org/infoq/fa/fa2e8ed9746bc1ee5d213589311d68a1.png" /></p><p>&nbsp;</p><p>在核心供给方面，芯片逻辑工艺小于 10nm 的代工厂里，92% 的份额被 TSMC 拿走。也就是说中国要扩展算力规模，不得不大量使用 TSMC 工艺，但这时就会受到美国禁令对于算力总量和算力密度的限制。</p><p>&nbsp;</p><p>整体来看，我们总的生产能力是受限的，同时我们也没有那么多的进口算力支持，怎么办？算法层面，大家有很多算法，在向万亿参数量级发展；算力层面，有很多国产芯片公司在努力，设法让我们的模型和算力能够更好地结合起来。</p><p>&nbsp;</p><p>如何把中国的算力都用起来就是一个非常值得探讨的问题。我们同海外的生态不太一样，国外主要还是英伟达以及英伟达上面的这套体系，包括CUDA和一系列优化库，但中国有各种各样的芯片，软件接口层也有很多选项。</p><p>&nbsp;</p><p>但从我的角度看，关键要做好三件事，也就是说产品维度有三个。一是大规模训练，二是适当小规模的训练和微调，三是大规模推理。底层硬件平台其实并不需要用户或算法研究者关心，有一些厂商或者软件、云服务能够把底层屏蔽掉。</p><p>&nbsp;</p><p>我们也在向这个方向努力，我们来提供中间层的训练能力、混训能力、运管能力、部署能力和对国产芯片的支持，让大家用起来更方便。</p><p>&nbsp;</p><p>英伟达的训练大家都可以干，但怎样把国产的千卡维度的训练做起来？就在前几天，我们发布了HETHUB，这是一个用于大规模模型的异构分布式混合训练系统，是业内首次实现六种不同品牌芯片间的交叉混合训练，异构混合训练集群算力利用率最高达到了97.6%。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/28/28944e8de73ef91ca3440ea377b67fb7.png" /></p><p></p><p>这给了我们极大的信心。混训的难度在于，如果都是一样的卡，把任务均匀拆分就好了，但由于算力总量不足，以及工艺被限制，我们这里不是一样的卡。因此我们必须去做各种各样的切分，然后做异构数据的并行，用异构的卡去训练大模型。目前这个异构千卡混训的能力已经结合进无问芯穹的Infini-AI云平台了，把高效互联互通、精密的分布式并行策略比如张量并行、数据并行、通信overlap等封装起来给大家提供服务。</p><p>&nbsp;</p><p>在大规模集群推理层面，特别是在缓存层面上，原来是有一部分无效缓存，但如果我们把顺序稍微调整一下，就可以做到缓存的极致利用，把模型的显存占用降到最低。这个云平台也集成了我们Serving优化技术能力，当并发量很高，多个用户同时发送请求时，这个系统会通过请求调度、提示词缓存、并行解码等方式优化计算任务的派发和计算结果的返回，累计可以实现30倍以上的token吞吐率提升。</p><p>&nbsp;</p><p>预期到今年底，我们能做到模型到芯片的M×N的自动路由，让大家想用什么卡就用什么卡。</p><p></p><h2>比特/瓦特的能效系统：算电融合，面向可持续的未来</h2><p></p><p>&nbsp;</p><p>如果大模型的算力提升真能达到 100 倍的规模，它对能源的需求会变得非常大。2025 年人工智能业务在全球数据中心用电量的占比将从 2% 猛增到 10%，相关用能成本、碳排放量也会飙升。那么电力系统如何进一步提升稳定性，如何消纳风光等绿色能源，都是我们要思考的问题。</p><p>&nbsp;</p><p>我们应该把算力中心尽可能放到能源集中的地方，但这里又面临着通信的问题，要把延迟和带宽挑战处理好，做好联合优化。我们系里也有团队在研究如何打造能源领域的大模型，支撑算力中心的用能方案的综合优化，来提高用能效率，赋能电力系统，解决电网的绿电消纳和峰值调频等问题。比如说电价是有波动的，计算任务也是有变化的，怎样把计算中心和能源模型适当结合起来做调配？还有预测发电的情况、波动的情况等等。</p><p>&nbsp;</p><p>我们还在同我校电机系一起做算电融合的研究，希望在这个方向上能够和大家一同推进。这个领域还处于早期规划阶段，这也是中国最优势的一个方向，就是怎样把能源和基建算力结合起来，赋能我们的大模型发展。</p><p>&nbsp;</p><p>从单算法到芯片的联合优化，到多算法和多芯片的联合优化，再到算力和能源的联合优化，我们有希望对整个巨大的系统进行优化，让我们的人工智能有充沛的能源和算力。</p><p></p><p>活动推荐：</p><p></p><p><a href="https://aicon.infoq.cn/2024/shanghai/speaker">AICon 全球人工智能开发与应用大会</a>"，为资深工程师、产品经理、数据分析师等专业人群搭建深度交流平台。聚焦大模型训练与推理、AI Agent、RAG 技术、多模态等前沿议题，汇聚 AI 和大模型超全落地场景与最佳实践，期望帮助与会者在大模型时代把握先机，实现技术与业务的双重飞跃。</p><p></p><p>在主题演讲环节，我们已经邀请到了「蔚来创始人 李斌」，分享基于蔚来汽车 10 年来创新创业过程中的思考和实践，聚焦 SmartEV 和 AI 结合的关键问题和解决之道。大会火热报名中，7 月 31 日前可以享受 9 折优惠，单张门票节省 480 元（原价 4800 元），详情可联系票务经理 13269078023 咨询。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/6573657a90550f91dc3658ad05122b02.webp" /></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/M20BQoqPDarRq9LpXHGv</id>
            <title>面壁WAIC新发布：新一代高效低能耗架构面壁小钢炮、一键开发大模型APP的全栈工具集</title>
            <link>https://www.infoq.cn/article/M20BQoqPDarRq9LpXHGv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/M20BQoqPDarRq9LpXHGv</guid>
            <pubDate></pubDate>
            <updated>Sat, 06 Jul 2024 04:22:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型时代, 面壁智能, MiniCPM-S, 稀疏激活
<br>
<br>
总结: 面壁智能创始人在WAIC 2024论坛上介绍了新一代高效、低能耗的MiniCPM-S模型，采用稀疏激活技术，提高了知识密度并降低推理算力消耗，助力开发者打造大模型SuperAPP。MiniCPM-S具备高稀疏低能耗、神仙推理和无损强大性能等特点，提升了知识密度，同时面壁智能还推出了端侧大模型工具集MobileCPM，帮助开发者一键集成大模型到APP，降低开发门槛。 </div>
                        <hr>
                    
                    <p>7月5日，面壁智能联合创始人、首席科学家刘知远在WAIC 2024 “模型即服务（Mass）加速大模型应用落地”论坛进行了《大模型时代的摩尔定律，迈入更高效的大模型时代》主题演讲，并首次对外介绍：</p><p>&nbsp;</p><p>开源新一代高效、低能耗面壁小钢炮MiniCPM-S模型助力开发者一键打造大模型SuperAPP的全栈工具集MobileCPM</p><p>&nbsp;</p><p>演讲开场，刘知远表示：“摩尔定律揭示了集成电路可容纳晶体管数目约每隔18个月便会增加一倍的规律，在过去几十年中给半导体和互联网行业的发展带来了科学指导意义；身处大模型时代，我们亟需新的“摩尔定律”。我们根据过去几年在大模型领域的深耕和实践，对大模型的发展趋势进行观察总结，提出了大模型时代的面壁定律：大模型的知识密度不断提升，平均每8个月提升一倍。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f4cc69078af34395928ebf1e7b58554.png" /></p><p></p><p>其中，知识密度=模型能力 / 推理算力消耗。</p><p></p><p>如下图所示，相比 OpenAI 于2020年发布的1750亿参数的 GPT-3，2024 年初，面壁发布具备 GPT-3 同等性能但参数仅为24亿的 MiniCPM-2.4B ，把知识密度提高了大概 86 倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0abcdf21dc3d1ffd2a8e54059135b73a.png" /></p><p></p><p>不过这还不是面壁的极限，面壁持续优化 Scaling Law，使模型知识密度不断提升，不断训练出计算更加高效且表现更强（参数规模降低，数值位宽降低，结构更加高效）的基础大模型。面壁新一代高效稀疏架构大模型由此而生。</p><p></p><h2>MiniCPM-S：新一代高效低能耗「面壁小钢炮」</h2><p></p><p></p><p>为何人脑中的神经元数量与当代最大的基础模型可比，但能源和时间消耗却远低于大模型？这背后，稀疏激活是大脑得以实现低能耗的一大核心“技术”，通过不同任务调取不同⼤脑分区神经元，能源与时间消耗⼤⼤降低。</p><p><img src="https://static001.geekbang.org/infoq/0a/0ade2d3ca8a2763a2a3c4cae3b3b0fe6.png" /></p><p></p><p>和大脑类似，采用稀疏激活也能够在同等参数下减少大模型的推理能耗——稀疏度越高，每个词元（token）激活的神经元越少，大模型的推理成本和能耗就越少。MiniCPM-S 1.2B 采用了高度稀疏架构，通过将激活函数替换为 ReLU及通过带渐进约束的稀疏感知训练 ，巧妙地解决了此前主流大模型在稀疏激活上面临的困境。</p><p></p><p>和同规模的稠密模型 MiniCPM 1.2B 相比，MiniCPM-S 1.2 具备：</p><p>Sparse-高稀疏低能耗：在FFN层实现了高达 87.89% 平均稀疏度，推理算力下降 84%；Speed-神仙推理： 更少计算，迅疾响应。纯 CPU 环境下，结合Powerinfer推理框架，推理解码速度提升约 2.8 倍；Strong-无损强大性能：更少计算量，无损下游任务性能；</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89808d9e7b848545ae561b912f38dde0.png" /></p><p></p><p>另外，MiniCPM-S 1.2B 将知识密度空前提升：达到同规模稠密模型 MiniCPM 1.2B 的 2.57 倍，Mistral-7B 的 12.1 倍。</p><p></p><p><img src="https://static001.geekbang.org/infoq/43/431cb772fac3a4d7d8d3d09c6110e1fa.png" /></p><p></p><p>面壁“高效 Scaling Law” 仍在持续演绎。</p><p></p><p>相关开源链接：</p><p>论文地址：https://arxiv.org/pdf/2402.13516.pdf模型地址：https://huggingface.co/openbmb/MiniCPM-S-1B-llama-formatPowerInfer可直接运行格式：https://huggingface.co/openbmb/MiniCPM-S-1B-sft-gguf</p><p></p><h2>开源大模型APP神器MobileCPM：一键集成端侧大模型到APP</h2><p></p><p></p><p>此外面壁智能最新开源了业内首个端侧大模型工具集 “MobileCPM "，帮助开发者一键集成大模型到APP。MobileCPM 开箱即用，包含了开源<a href="https://aicon.infoq.cn/2024/shanghai/track/1724">端侧大模型</a>"、SDK开发套件以及翻译、摘要等丰富的 intent ，人人都可以一站式灵活地定制出满足不同应用场景需求的大模型 APP，低门槛速成「大模型创业者」。</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53c96af252c823e73c562d92bdf1efff.png" /></p><p></p><p>MobileCPM 为开发者提供了三种模式：</p><p>基础模式：包含了丰富的适配端侧⼤模型 APP 的 SDK 套件发者基于此即可⾃由灵活地搭建⼤模型 APP，但在这个过程中，基座模型和智能体仍需要开发者⾃⾏开发和接⼊；精装模式：在基础模式基础上，提供 1.2B 参数的⾯壁新⼀代⾼效稀疏⼤模型 MiniCPM-S，并且MobileCPM 还支持任意端侧模型的集成，开发者可以根据具体需求选择替换其它端侧模型，并可以通过增加或修改prompt的方式定制多种API，满足不同业务场景需求。全包配件模式：在精装模式的基础上预装丰富的 intent，并提供保姆式教程，开发者也可使用自定义 intent，减少开发时间，⼤幅提升应⽤的丰富性。</p><p>&nbsp;</p><p>本次发布，MobileCPM 默认集成了面壁新一代高效稀疏架构模型 MiniCPM-S 1.2B ，一次性将智能密度拉满，更兼具：</p><p>毫秒极速响应：得益于面壁小钢炮MiniCPM系列背后的高效大模型训练和推理工厂，MiniCPM-S 能够在毫秒级时间内完成推理和响应，确保用户体验的流畅性。零推理成本：无需云端 GPU，MiniCPM-S 专为端侧设备而生，在保证性能强大的同时大幅降低了计算资源的消耗，使得端侧推理几乎0成本。一键集成：大模型与APP无缝衔接；预装多种 intent，提供保姆式教程；</p><p><img src="https://static001.geekbang.org/infoq/52/52f49e0b58df2ad7ef707e0003c0cbb8.png" /></p><p></p><p>基于 MobileCPM 一键开发的示例 APP（端侧模型由MiniCPM-S支持），在 iPhone 15 离线环境下毫秒级对话响应，推理速度轻松可达约30 tokens/s，相当于人类语速的 18~30 倍。</p><p></p><p>MobileCPM 拉开了<a href="https://aicon.infoq.cn/2024/shanghai/track/1724">端侧AI</a>"生态序幕，基于MobileCPM，任何开发者都可以轻松打造自己的 SuperAPP，有效解决推理成本问题。PC和智能手机时代，所有原有的应用都值得用高效端侧模型尝试一遍！</p><p></p><p>MobileCPM 现已全面支持 iOS系统，Android 版本也即将上线，敬请期待。</p><p></p><p>开源地址：</p><p>https://github.com/OpenBMB/MobileCPM</p><p>TestFlight外测链接：</p><p>https://testflight.apple.com/join/dJt5vfOZ</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</id>
            <title>2024版国家人工智能标准化指南揭晓！涉及7个重点方向</title>
            <link>https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZZPFpmq9tOUdwhTy1Mql</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 10:34:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 标准体系, 新型工业化, 技术创新
<br>
<br>
总结: 人工智能作为新一轮科技革命和产业变革的基础性和战略性技术，正在成为发展新质生产力的重要引擎。近年来，我国人工智能产业链在技术创新、产品创造和行业应用等方面实现了快速发展，形成了庞大的市场规模。为进一步规范和引领该领域的发展，《国家人工智能产业综合标准化体系建设指南》发布，旨在加快构建满足人工智能产业高质量发展和“人工智能 +”高水平赋能需求的标准体系，推动人工智能赋能新型工业化。 </div>
                        <hr>
                    
                    <p>人工智能作为新一轮科技革命和产业变革的基础性和战略性技术，正在成为发展新质生产力的重要引擎。近年来，我国人工智能产业链在技术创新、产品创造和行业应用等方面实现了快速发展，形成了庞大的市场规模。特别是以大模型为代表的新技术加速迭代，呈现出创新技术群体突破、行业应用融合发展、国际合作深度协同等新特点。然而，随着人工智能技术和产业的迅猛发展，完善的标准体系显得尤为重要。</p><p></p><p>为进一步规范和引领该领域的发展，近日，国家发改委等四部门联合印发《国家人工智能产业综合标准化体系建设指南（2024 版）》（以下统称《指南》），聚焦基础共性标准、基础支撑标准、关键技术标准、智能产品与服务标准、赋能新型工业化标准、行业应用标准、安全／治理标准等 7 个重点方向，加快构建满足人工智能产业高质量发展和“人工智能 +”高水平赋能需求的标准体系，推动人工智能赋能新型工业化。</p><p></p><h2>总体目标，实现人工智能产业全球化</h2><p></p><p>《指南》明确指出，以习近平新时代中国特色社会主义思想为指导，全面贯彻党的二十大和二十届二中全会精神，统筹高质量发展和高水平安全，加快赋能新型工业化。</p><p></p><p>到 2026 年，标准与产业科技创新的联动水平将持续提升，新制定国家标准和行业标准 50 项以上，推动形成引领人工智能产业高质量发展的标准体系。预计参与标准宣贯和实施推广的企业将超过 1000 家，国际标准的制定也将超过 20 项，进一步促进人工智能产业全球化发展。</p><p></p><h2>《指南》要点解读</h2><p></p><p></p><h3>建设思路：多层次、系统化</h3><p></p><p>人工智能产业的标准化建设是一个多层次、系统化的过程，由一系列互相关联的标准构成。根据《指南》，人工智能标准体系结构包括基础共性、基础支撑、关键技术、智能产品与服务、赋能新型工业化、行业应用、安全 / 治理等七个部分。每个部分都涵盖了具体的标准制定方向和要求：</p><p></p><p>基础共性标准：规范人工智能术语、参考架构、测试评估、管理、可持续等方面的标准。基础支撑标准：包括基础数据服务、智能芯片、智能传感器、计算设备、算力中心、系统软件、开发框架、软硬件协同等标准，为人工智能产业发展夯实技术底座。关键技术标准：主要规范人工智能文本、语音、图像，以及人机混合增强智能、智能体、跨媒体智能、具身智能等的技术要求，推动人工智能技术创新和应用。智能产品与服务标准：规范智能机器人、智能运载工具、智能移动终端、数字人、智能服务等方面的标准。赋能新型工业化标准：涵盖研发设计、中试验证、生产制造、营销服务、运营管理等制造业全流程智能化标准，以及重点行业智能升级标准。行业应用标准：规范人工智能技术在各行业场景中的应用，推动产业智能化发展。安全 / 治理标准：规范人工智能安全、治理等要求，为人工智能产业发展提供安全保障。</p><p></p><p><img src="https://static001.geekbang.org/infoq/be/be452ccefc0ba2ab2c78ae5cbc5aeb18.jpeg" /></p><p></p><h3>指导原则：创新、牵引、协同、开放</h3><p></p><p>为了确保标准体系的科学性和实用性，《指南》还提出了一系列战略性指导原则，通过创新驱动、应用牵引、产业协同和开放合作，加速人工智能产业的高质量发展。</p><p>始终秉持创新驱动的理念。优化产业科技创新与标准化联动机制，加快人工智能领域关键共性技术研究，推动先进适用的科技创新成果高效转化成标准。严格遵循应用牵引的原则。以企业为主体，市场为导向，面向行业应用需求，强化创新成果迭代和应用场景构建，协同推进人工智能与重点行业的融合应用。高度注重产业协同的发展。加强人工智能全产业链标准化工作协同，推动跨行业、跨领域标准化技术组织的协作，打造大中小企业融通发展的标准化模式。着重强调开放合作的策略。深化国际标准化交流与合作，鼓励我国企事业单位积极参与国际标准化活动，与全球产业链上下游企业共同制定国际标准。</p><p></p><h3>新增重点：赋能新型工业化标准</h3><p></p><p>与今年 1 月发布的《国家人工智能产业综合标准化体系建设指南》（征求意见稿）相比，最终版的《指南》在核心内容上有了显著的拓展，特别是新增了“赋能新型工业化标准”这一关键环节。该部分主要着眼于规范人工智能技术如何为制造业全流程智能化及重点行业的智能升级提供技术支撑。具体而言，它涵盖了从研发设计、中试验证，到生产制造、营销服务以及运营管理等制造业全链条的智能化标准设定，并针对关键行业的智能升级提出了明确要求。</p><p></p><p>工业和信息化部电子第五研究所的高级工程师涂珍兰表示，“标准规范体系的建设可以促进科技创新与产业发展的结合，推动科技创新成果快速转化为产品和服务，实现产业升级和经济增长。”</p><p></p><h2>写在最后</h2><p></p><p>人工智能产业标准化体系的构建，离不开产业链上各环节携手共建。《指南》还提出，建立健全人工智能领域标准化技术组织，统筹产学研用各方、产业链各环节优势力量，协同推进人工智能标准建设，共同构建先进适用的人工智能产业标准体系。</p><p></p><p>总的来说，本次《指南》的发布，是我国人工智能产业标准化工作的一项重要举措。通过构建完善的标准体系，将有效推动人工智能技术进步，促进企业发展，引领产业升级，保障产业安全，从而更好地赋能新型工业化。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</id>
            <title>生成式推荐系统与京东联盟广告 - 综述与应用</title>
            <link>https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lUMvRGuebW5x5VUM2pJS</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 10:20:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型语言模型, 自然语言处理, 推荐系统, 生成式推荐系统
<br>
<br>
总结: 本文介绍了大型语言模型对推荐系统的影响，特别是生成式推荐系统的应用。通过深入分析生成式推荐系统的优势和京东联盟广告的挑战，探讨了如何利用大型语言模型重塑推荐系统，为广告领域带来新的见解和启发。文章还详细介绍了生成式推荐系统的四个基本环节，强调了在实践中需要考虑和平衡的细节。 </div>
                        <hr>
                    
                    <p>大型语言模型（LLM）正在深刻地影响自然语言处理（NLP）领域，其强大的处理各种任务的能力也为其他领域的从业者带来了新的探索路径。推荐系统（RS）作为解决信息过载的有效手段，已经紧密融入我们的日常生活，如何用 LLM 有效重塑 RS 是一个有前景的研究问题[20, 25]。</p><p>这篇文章从生成式推荐系统和京东联盟广告的背景入手，首先引出两者结合的动因与策略，随后我们对当前的流程和方法进行了细致的回顾与整理，最后详细介绍了我们在京东联盟广告领域的应用实践。通过深入分析与案例展示，本文旨在为广告领域的推荐系统带来新的见解和启发。</p><p>﻿</p><p></p><h2>一、背景</h2><p></p><p></p><h4>生成式推荐系统</h4><p></p><p></p><p></p><blockquote>A generative recommender system directly generates recommendations or recommendation-related content without the need to calculate each candidate’s ranking score one by one[25].</blockquote><p></p><p></p><p>由于现实系统中的物料（item）数量巨大，传统 RS 通常采用多级过滤范式，包括召回、粗排、精排、重排等流程，首先使用一些简单而有效的方法（例如，基于规则/策略的过滤）来减少候选物料的数量，从数千万甚至数亿到数百个，然后对这些物料应用较复杂的推荐算法，以进一步选择较少数量的物料进行推荐。受限于响应时间的要求，复杂推荐算法并不适用于规模很大的所有物料。</p><p></p><p>LLM 的生成能力有可能重塑 RS，相较于传统 RS，生成式推荐系统具备如下的优势：1）简化推荐流程。LLM 可以直接生成要推荐的物料，而非计算候选集中每个物料的排名分数，实现从多级过滤范式（discriminative-based，判别式）到单级过滤范式（generative-based，生成式）的变迁。LLM 在每个解码步生成一个向量，表示在所有可能词元（token）上的概率分布。经过几个解码步，生成的 token 就可以构成代表目标物料的完整标识符，该过程隐式枚举所有候选物料以生成推荐目标物料[25]。2）具备更好的泛化性和稳定性。利用 LLM 中的世界知识和推理能力，在具有新用户和物料的冷启动和新领域场景下具备更好的推荐效果和迁移效果。同时，相比于传统 RS，生成式推荐系统的方法也更加具备稳定性和可复用性。特征处理的策略随场景和业务的变化将变小、训练数据量将变少，模型更新频率将变低。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/a0/a0e903e83dd5d39a091a150978f44168.png" /></p><p>﻿﻿</p><p>•图 1. 传统推荐系统与基于 LLM 的生成式推荐系统的流程比较[25]</p><p>﻿</p><p></p><h4>京东联盟广告</h4><p></p><p></p><p>京东联盟是京东的一个联盟营销平台，以投放站外 CPS 广告为主。联盟合作伙伴通过生成的链接在其他网站或社交媒体平台上推广京东商品，引导用户点击这些链接并在京东购物，从而获得销售提成（佣金）。京东联盟借此吸引流量，扩大平台的可见度和与用户的接触范围，实现拉新促活等目标。</p><p></p><p>联盟广告推荐主要针对低活跃度用户进行多场景推荐，这样的推荐面临如下的挑战：1）数据稀疏性：低活跃度用户提供的数据较少，导致更加明显的数据稀疏性问题。数据不足使得基于 ID 的传统推荐模型难以充分地对物料和用户进行表征，进而影响推荐系统的预测准确性。2）冷启动问题：对于新用户或低活跃度用户，冷启动问题尤为严重。由于缺乏足够的历史交互数据，推荐系统难以对这些用户进行有效的个性化推荐。3）场景理解困难：在多场景推荐系统中，理解不同场景下用户的具体需求尤为关键。对于低活跃度用户，由于交互数据有限，推荐系统更难以识别出用户在不同场景下的行为差异和需求变化。4）多样性和新颖性：保持推荐内容的多样性和新颖性对于吸引低活跃度用户至关重要。然而，由于对这些用户的了解有限，推荐系统难以平衡推荐的准确性与多样性。</p><p></p><p></p><h4>京东联盟广告+生成式推荐系统</h4><p></p><p></p><p>将 LLM 融入推荐系统的关键优势在于，它们能够提取高质量的文本表示，并利用其中编码的世界知识对用户和物料进行理解和推荐。与传统的推荐系统不同，基于 LLM 的模型擅长捕获上下文信息，更有效地理解用户信息、物料描述和其他文本数据。通过理解上下文，生成式推荐系统可以提高推荐的准确性和相关性，从而提升用户满意度。同时，面对有限的历史交互数据带来的冷启动和数据稀疏问题，LLM 还可通过零/少样本推荐能力为推荐系统带来新的可能性。这些模型可以推广到未见过的新物料和新场景，因为它们通过事实信息、领域专业知识和常识推理进行了广泛的预训练，具备较好的迁移和扩展能力。</p><p></p><p>由此可见，京东联盟广告是生成式推荐系统一个天然的应用场。</p><p></p><p></p><h2>二、生成式推荐系统的四个环节</h2><p></p><p></p><p>为了实现如上的范式变迁，有四个基本环节需要考虑[26]：1）物料表示：在实践中，直接生成物料（文档或商品描述）几乎是不可能的。因此，需要用短文本序列，即物料标识符，表示物料。2）模型输入表示：通过提示词定义任务，并将用户相关信息（例如，用户画像和用户历史行为数据）转换为文本序列。3）模型训练：一旦确定了生成模型的输入（用户表示）和输出（物料标识符），就可以基于 Next Token Prediction 任务实现训练。4）模型推理：训练后，生成模型可以接收用户信息来预测对应的物料标识符，并且物料标识符可以对应于数据集中的真实物料。</p><p></p><p>虽然整个过程看起来很简单，但实现有效的生成式推荐并非易事。在上述四个环节中需要考虑和平衡许多细节。下面详细梳理了现有工作在四个环节上的应用与探索：</p><p></p><h4>物料表示</h4><p></p><p></p><p></p><blockquote>An identifier in recommender systems is a sequence of tokens that can uniquely identify an entity, such as a user or an item. An identifier can take various forms, such as an embedding, a sequence of numerical tokens, and a sequence of word tokens (including an item title, a description of the item, or even a complete news article), as long as it can uniquely identify the entity[25].</blockquote><p></p><p></p><p>推荐系统中的物料通常包含来自不同模态的各种信息，例如，视频的缩略图、音乐的音频和新闻的标题。因此，物料标识符需要在文本空间中展示每个物料的复杂特征，以便进行生成式推荐。一个好的物料标识符构建方法至少应满足两个标准：</p><p>1）保持合适的长度以减轻文本生成的难度。 2）将先验信息集成到物料索引结构中，以确保相似项目在可区分的同时共享最多的 token，不相似项目共享最少的 token。</p><p></p><p>以下是几种构建物料标识符的方法：</p><p></p><p>（1）数字 ID（Numeric ID）</p><p>由于数字在传统 RS 中被广泛地使用，一个直接的策略是在生成式推荐系统中也使用数字 ID 来表示物料。传统 RS 将每个物料 ID 视为一个独立且完整的 token，不能被进一步分割。如果将这些 token 加入到模型中，需要 1）大量的内存来存储每个 token 的向量表示，以及 2）充足的数据来训练这些向量表示。为了解决这些问题，生成式推荐系统将数字 ID 分割成多个 token 组成的序列，使得用有限的 token 来代表无限的物料成为可能。为了有效地以 token 序列表示一个物料，现有的工作探索了不同的策略。1）顺序索引：基于时间顺序，利用连续的数字表示物料，例如，“1001, 1002, ...”，这可以捕捉与同一用户交互的物料的共现（基于 SentencePiece 分词器进行分词时，“1001”和“1002”分别被分词为“100”“1”和“100”“2”）。2）协同索引：基于共现矩阵或者协同过滤信息构建物料标识符，使得共现次数更多的物料或者具有相似交互数据的物料拥有相似的标识符前缀。尽管在生成式推荐系统中使用数字 ID 效果显著，但它通常缺乏语义信息，因此会遭受冷启动问题，并且未能利用 LLM 中编码的世界知识。</p><p></p><p>（2）文本元数据（Textual Metadata）</p><p>为了解决数字 ID 中缺乏语义信息的问题，一些研究工作利用了物料的文本元数据，例如，电影标题，产品名称，书名，新闻标题等。在与 LLM 结合时可借助 LLM 中编码的世界知识更好地理解物料特性。但这种方式有两个问题：1）当物料表示文本非常长时，进行生成的计算成本会很高。此外，长文本很难在数据集中找到精确匹配；仔细检查每个长文本的存在性或相关性将使我们回到判别性推荐的范式，因为我们需要将其与数据集中的每个物料计算匹配得分。2）虽然自然语言是一种强大且富有表现力的媒介，但在许多情况下它也可能是模糊的。两个不相关的物料可能具有相同的名称，例如，“苹果”既可以是一种水果也可以特指苹果公司，而两个密切相关的物料可能具有不同的标题，例如，数据挖掘中著名的“啤酒和尿布”示例[25]。</p><p></p><p>（3）语义 ID（Semantic-based ID，SID）</p><p>为了同时获得具有语义和区分性的物料标识符，现有方法主要通过如下方式对物料向量进行离散化：1）基于 RQ-VAE 模型[8]。RQ-VAE 模型由编码器，残差量化和解码器三部分构成，其输入是从预训练的语言模型（例如，LLaMA[9]和 BERT[28]）提取的物料语义表示，输出是物料对应的 token 序列。在这个分支中，TIGER[7]是一个代表性的工作，它通过物料的文本描述生成对应的 token 序列，并将 token 序列命名为 Semantic ID。LC-Rec[4]设计了多种微调 LLM 的任务，旨在实现 Semantic ID 与用户交互数据或物料文本描述的语义对齐。这两种方法首先将物料的语义相关性捕获到标识符中，即具有相似语义的项目将拥有相似的标识符。然后，标识符表示将通过在推荐数据上训练来优化，以获取交互相关性。相比之下，LETTER[6]通过整合层次化的语义、协同信号和编码分配的多样性来构建高质量的物料标识符。2）基于语义层次化聚类方法。ColaRec[1]首先利用协同模型编码物料，并利用 k-means 聚类算法对物料进行层次化聚类，将分类类别作为物料标识符，之后在微调任务中对齐物料语义信息和交互信息。Hi-Gen[5]则在获取物料标识符的阶段同时考虑了交互信息和语义信息，利用 metric learning 对两种信息进行融合。</p><p></p><p>（4）小结</p><p>以上三类表示方法的对比如下：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/13/13b469423138a71932589c4693203a46.png" /></p><p>﻿﻿</p><p>表 1. 不同离散化物料表示方法的对比</p><p></p><h4>模型输入表示</h4><p></p><p></p><p>在生成式推荐系统中，模型输入由如下的三个部分组成：任务描述、用户信息、上下文及外部信息。其中，用户信息主要包括用户历史交互数据和用户画像。</p><p></p><p>（1）任务描述</p><p>为了利用生成模型的理解能力，任务描述主要用来引导生成模型完成推荐任务，即将推荐任务建模为下一个物料的预测（类比语言模型的 Next Token Prediction，此处是 Next Item Prediction）。任务描述定义了提示词模版，将可利用的数据嵌入其中。例如，“这是一个用户的历史交互数据：{historical behavior}，他的偏好如下：{preference}，请提供推荐。”同时将用户历史交互数据和偏好作为模型输入内容[26]。</p><p></p><p>（2）用户历史交互数据</p><p>用户的历史交互数据在推荐系统中扮演着至关重要的角色，这种互动数据隐性地传达了用户对物料的偏好。用户历史交互数据的表示与上文介绍的物料表示密切相关，现有方法将其表示为：1）物料数字 ID 序列。物料数字 ID 被 LLM 作为纯文本处理，由分词器分割成几个 token。2）物料文本序列。将物料文本元数据进行拼接送入预训练语言模型，语言模型可根据世界知识建模物料之间的相关性。3）物料文本向量加物料 ID 向量序列。LLaRA[2]在物料标题向量后拼接了物料 ID 向量，以补充来自协同模型的交互信息。</p><p></p><p>（3）用户画像</p><p>为了增强用户建模，集成用户画像（例如，关于用户的基础信息和偏好信息）是推荐系统中建模用户特征的一种有效方式。在大多数情况下，用户的基础信息（例如，性别）可以直接从在线推荐平台获取。这些用户信息可与描述性文本结合使用，例如，“用户描述：女性，25-34 岁，在销售/市场营销领域工作”[26]。然而，由于用户隐私问题，获取用户画像可能具有挑战性，导致一些研究直接采用用户 ID 或 ID 向量[3]进行用户建模。</p><p></p><p>（4）上下文及外部信息</p><p>上下文信息（例如，位置、场景和时间）可能会影响用户决策，例如，在户外用品推荐中，用户可能更倾向于购买帐篷而水龙头。因此，在 LLM 中结合诸如时间之类的上下文信息，可以实现有效的用户理解。此外，外部知识也可以用来增强生成式推荐模型的性能，例如，用户-物料交互图中的结构化信息。</p><p>﻿</p><p></p><h4>模型训练</h4><p></p><p></p><p>在推荐数据上训练生成式推荐模型包括两个主要步骤：文本数据构建和模型优化[26]。文本数据构建将推荐数据转换为具有文本输入和输出的样本，其中输入和输出的选择取决于任务定义和物料表示方法。基于数字 ID 和文本元数据的物料表示方法可以直接构建文本数据，基于语义 ID 的方法则需要基于向量进行物料标识符的学习和获取。在模型优化方面，给定&lt;输入，输出&gt;数据，生成式模型的训练目标是最大化给定输入预测输出的条件似然。</p><p></p><p>针对生成式推荐系统，“用户到物料标识符的训练”是主要任务，即输入是用户构建，输出是下一个物料的标识符。基于数字 ID 和文本元数据的方法利用该任务进行模型训练。对于基于语义 ID 的方法，由于语义 ID 和自然语言之间存在差距，一般会利用如下辅助任务来增强物料文本和标识符之间的对齐[4]：1）“物料文本到物料标识符的训练”或“物料标识符到物料文本的训练”。对于每个训练样本，输入输出对包括同一物料的标识符和文本内容，可以互换地作为输入或输出。2）“用户到物料文本的训练”。通过将用户信息与下一个物料的文本内容配对来隐式对齐物料标识符和物料文本。</p><p></p><p>对于训练如 LLaMA 这样的大型语言模型，可采用多种策略来提高训练效率，例如，参数高效微调，模型蒸馏和推荐数据筛选。</p><p></p><h4>模型推理</h4><p></p><p></p><p>为了实现物料推荐，生成式推荐系统在推理阶段需要对生成结果进行定位，即实现生成的物料标识符与数据集中物料的有效关联。给定用户输入表示，生成式推荐系统首先通过束搜索自回归地生成物料标识符。这里的生成方式分为两种：自由生成和受限生成[26]。对于自由生成，在每一个解码步中，模型在整个词表中搜索，并选择概率最高的前 K 个 token（K 值取决于束搜索中定义的束大小）作为下一步生成的输入。然而，在整个词表上的搜索可能会导致生成不在数据集中的标识符，从而使推荐无效。</p><p></p><p>为了解决这个问题，早期工作使用精确匹配进行物料定位，即进行自由生成并简单地丢弃无效的标识符。尽管如此，它们仍然由于无效标识符而导致准确率低，特别是对于基于文本元数据的标识符。为了提高准确性，BIGRec[23]提出将生成的标识符通过生成的 token 序列的表示和物料表示之间的 L2 距离来定位到有效物料上。如此，每个生成的标识符都确保被定位到有效的物料上。与此同时，受限生成也在推理阶段被使用，例如，使用 Trie（prefix tree）或者 FM-index 进行受限生成，保证标识符的有效生成。</p><p></p><p>在预测下一个物料这样的典型推荐任务之外，也可充分利用自由生成产生新的物料描述或预测接下来 N 个物料。</p><p></p><h4>现有工作总结</h4><p></p><p></p><p>当前生成式推荐系统的代表性工作（RecSysLLM[22]，P5[20][24]，How2index[18]，PAP-REC[17]，VIP5[19]，UniMAP[27]，TIGER[7]，LC-Rec[4]，TransRec[16]，M6-Rec[21]，BIGRec[23]，LMRecSys[10]，NIR[12]，RecRanker[13]，InstructRec[11]，Rec-GPT4V[14]，DEALRec[15]）可总结为：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/53/5331491f59babfc82dd3323fcfd9d645.png" /></p><p>﻿﻿</p><p>表 2. 生成式推荐系统的代表性工作[26]</p><p>﻿</p><p></p><h2>三、实践方案</h2><p></p><p></p><h4>总体设计</h4><p></p><p></p><p>基于对现有工作的调研和总结，我们的方案以“基于语义 ID 的物料表示”和“对齐协同信息和文本信息的训练任务”展开：</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/de/def063ae2565f101295b50a2a0b96759.png" /></p><p>﻿﻿</p><p>图 2. 总体设计框架图</p><p></p><h4>功能模块</h4><p></p><p></p><p>（1）基于语义 ID（SID）的物料表示</p><p>物料文本描述：基于商品标题表示物料。</p><p>物料向量：通过预训练的 bert-base-chinese 和 Yi-6B 分别提取文本描述对应的向量，向量维度为 768（bert-base-chinese）和 4096（Yi-6B）。</p><p>物料 SID：基于 RQ-VAE 模型对物料向量进行量化。RQ-VAE 模型由编码器，残差量化和解码器三部分构成，其输入是从预训练的语言模型中提取的向量，输出是物料对应的 SID 序列。针对冲突数据，我们采取了两种方式，一种是不进行处理，即一个 SID 对应多个商品；另一种是采用 TIGER 的方案，对有冲突的商品增加随机的一维，使得一个 SID 唯一对应一个商品。例如，商品“ThinkPad 联想 ThinkBook 14+ 2024 14.5 英寸轻薄本英特尔酷睿 ultra AI 全能本高性能独显商务办公笔记本电脑”可表示为：或。</p><p>﻿</p><p><img src="https://static001.geekbang.org/infoq/be/be1853aec112f9b587b6e0cb7a6afa68.png" /></p><p>﻿﻿</p><p>图 3. RQ-VAE 模型图[8]</p><p></p><p>（2）对齐协同信息和文本信息的训练任务</p><p>Next Item Prediction：推荐系统的主任务，即针对给定的用户描述（用户画像+历史交互数据），预测下一个推荐的物料。</p><p>Additional Alignment：由于 SID 和自然语言之前存在差距，通过额外的对齐训练，建立物料 SID 和物料文本描述之间的联系，包括 SID 到文本描述和文本描述到 SID 的两个双向任务。</p><p>﻿</p><p></p><h2>四、离线与在线实验</h2><p></p><p></p><h4>训练数据</h4><p></p><p>（1）Next Item Prediction</p><p></p><p><code lang="text">{
    "instruction": "该用户为都市女性。用户已按时间顺序点击了如下商品：, , , , , , , , , , , , ，你能预测用户下一个可能点击的商品吗？",
    "response": ""
}</code></p><p></p><p>（2）Item and SID Alignment1 - SID2Title</p><p></p><p><code lang="text">{
    "instruction": "商品的标题是什么？",
    "response": "ThinkPad 联想ThinkBook 14+ 2024 14.5英寸轻薄本英特尔酷睿ultra AI全能本高性能独显商务办公笔记本电脑 Ultra5 125H 32G 1T 3K屏 高刷屏"
}</code></p><p></p><p>（3）Item and SID Alignment2 - Title2SID</p><p><code lang="text">{
    "instruction": "哪个商品的标题是\"ss109威震天变形MP威震玩具天金刚飞机威男孩机器人战机模型合金 震天战机（战损涂装版）\"？",
    "response": ""
}</code></p><p></p><p></p><h4>基座模型、训练及推理</h4><p></p><p></p><p>（1）base model: Qwen1.5-0.5B/1.8B/4B 和 Yi-6B</p><p>（2）基于 SID 增加新 tokens，并利用交互数据进行训练</p><p>（3）采用基于 beam search 的受限解码策略，beam size=20</p><p>（4）实验方式：离线实验+线上小流量实验</p><p>（5）离线评估指标：HR@1,5,10; NDCG@1,5,10</p><p>（6）在线评估指标：UCTR</p><p></p><h4>实验结果</h4><p></p><p></p><p>（1）离线实验——同一基座模型不同参数规模的对比：</p><p>◦对比 0.5B/1.8B/4B 的结果可得，模型参数量越大，处理多种任务的能力越强，评估指标值越高；</p><p>◦由于 0.5B 模型能力较弱，不适宜处理多种任务数据，单一任务训练得到的模型相较混合任务有 8 倍提升；</p><p>◦在离线训练和测试数据有 3 个月的时间差的情况下，模型的表现仍然可观。</p><p></p><p>（2）离线实验——不同基座模型的对比：</p><p>◦Yi-6B 模型在不使用受限解码的情况下就有最佳的表现；</p><p>◦微调后的 Yi-6B 指令遵循的能力较好，可进行 next item prediction 和标题文本生成。</p><p></p><p>（3）离线实验——与协同模型结果对比：</p><p>◦在相同的数据规模和数据预处理的情况下，Yi-6B 模型的效果更好；</p><p>◦对稀疏数据进行过滤后训练的协同模型效果会有显著提升，传统模型对数据和特征的处理方式更为敏感。</p><p></p><p>（4）线上小流量实验：</p><p>◦多个置信的站外投放页面的小流量实验显示，基于生成式模型 base 版本可与传统多路召回+排序的 top1 推荐对应的 UCTR 结果持平，在部分页面更优，UCTR+5%以上。</p><p>◦更适合数据稀疏、用户行为不丰富的场景。</p><p>﻿</p><p></p><h2>五、优化方向</h2><p></p><p></p><p>在生成式推荐系统中，构建高质量的数据集是实现精准推荐的关键。在物料表示和输入-输出数据构建层面，将语义信息、多模态信息与协同信息结合，基于联盟场景特点，可以显著提升物料表示的准确性和相关性。</p><p></p><p>为了支持 RQ-VAE 的稳定训练和语义 ID 的增量式推理，需要开发一种可扩展的 SID 训练和推理框架，确保语义 ID 能够快速适应物料变化。</p><p></p><p>此外，优化基座模型是提高生成式推荐系统性能的另一个关键领域。通过训练任务的组合和采用多种训练方式，例如，多 LoRA 技术和混合数据策略，可以进一步增强模型的表现。推理加速也是优化的一个重要方面，通过模型蒸馏、剪枝和量化等技术，可以提高系统的响应速度和效率。同时，基座模型的选型与变更，也是持续追求优化效果的一部分。</p><p></p><p>未来，可考虑引入搜索 query 内容进行搜推一体化建模。此外，引入如用户推荐理由生成和用户偏好生成等任务，可丰富系统的功能并提高用户的互动体验。</p><p>﻿</p><p>我们的目标是通过持续的技术革新，推动推荐系统的发展，实现更高效、更个性化的用户服务。欢迎对这一领域感兴趣的合作伙伴加入我们，共同探索生成式推荐系统技术的未来。</p><p>﻿</p><p></p><h2>六、参考文献</h2><p></p><p>1.Wang Y, Ren Z, Sun W, et al. Enhanced generative recommendation via content and collaboration integration[J]. arXiv preprint arXiv:2403.18480, 2024.</p><p>2.Liao J, Li S, Yang Z, et al. Llara: Large language-recommendation assistant[J]. Preprint, 2024.</p><p>3.Zhang Y, Feng F, Zhang J, et al. Collm: Integrating collaborative embeddings into large language models for recommendation[J]. arXiv preprint arXiv:2310.19488, 2023.</p><p>4.Zheng B, Hou Y, Lu H, et al. Adapting large language models by integrating collaborative semantics for recommendation[J]. arXiv preprint arXiv:2311.09049, 2023.</p><p>5.Wu Y, Feng Y, Wang J, et al. Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce Search[J]. arXiv preprint arXiv:2404.15675, 2024.</p><p>6.Wang W, Bao H, Lin X, et al. Learnable Tokenizer for LLM-based Generative Recommendation[J]. arXiv preprint arXiv:2405.07314, 2024.</p><p>7.Rajput S, Mehta N, Singh A, et al. Recommender systems with generative retrieval[J]. Advances in Neural Information Processing Systems, 2024, 36.</p><p>8.Zeghidour N, Luebs A, Omran A, et al. Soundstream: An end-to-end neural audio codec[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021, 30: 495-507.</p><p>9.Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023.</p><p>10.Zhang Y, Ding H, Shui Z, et al. Language models as recommender systems: Evaluations and limitations[C]//I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop. 2021.</p><p>11.Zhang J, Xie R, Hou Y, et al. Recommendation as instruction following: A large language model empowered recommendation approach[J]. arXiv preprint arXiv:2305.07001, 2023.</p><p>12.Wang L, Lim E P. Zero-shot next-item recommendation using large pretrained language models[J]. arXiv preprint arXiv:2304.03153, 2023.</p><p>13.Luo S, He B, Zhao H, et al. RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation[J]. arXiv preprint arXiv:2312.16018, 2023.</p><p>14.Liu Y, Wang Y, Sun L, et al. Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models[J]. arXiv preprint arXiv:2402.08670, 2024.</p><p>15.Lin X, Wang W, Li Y, et al. Data-efficient Fine-tuning for LLM-based Recommendation[J]. arXiv preprint arXiv:2401.17197, 2024.</p><p>16.Lin X, Wang W, Li Y, et al. A multi-facet paradigm to bridge large language model and recommendation[J]. arXiv preprint arXiv:2310.06491, 2023.</p><p>17.Li Z, Ji J, Ge Y, et al. PAP-REC: Personalized Automatic Prompt for Recommendation Language Model[J]. arXiv preprint arXiv:2402.00284, 2024.</p><p>18.Hua W, Xu S, Ge Y, et al. How to index item ids for recommendation foundation models[C]//Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 2023: 195-204.</p><p>19.Geng S, Tan J, Liu S, et al. VIP5: Towards Multimodal Foundation Models for Recommendation[C]//Findings of the Association for Computational Linguistics: EMNLP 2023. 2023: 9606-9620.</p><p>20.Geng S, Liu S, Fu Z, et al. Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5)[C]//Proceedings of the 16th ACM Conference on Recommender Systems. 2022: 299-315.</p><p>21.Cui Z, Ma J, Zhou C, et al. M6-rec: Generative pretrained language models are open-ended recommender systems[J]. arXiv preprint arXiv:2205.08084, 2022.</p><p>22.Chu Z, Hao H, Ouyang X, et al. Leveraging large language models for pre-trained recommender systems[J]. arXiv preprint arXiv:2308.10837, 2023.</p><p>23.Bao K, Zhang J, Wang W, et al. A bi-step grounding paradigm for large language models in recommendation systems[J]. arXiv preprint arXiv:2308.08434, 2023.</p><p>24.Xu S, Hua W, Zhang Y. Openp5: Benchmarking foundation models for recommendation[J]. arXiv preprint arXiv:2306.11134, 2023.</p><p>25.Li L, Zhang Y, Liu D, et al. Large language models for generative recommendation: A survey and visionary discussions[J]. arXiv preprint arXiv:2309.01157, 2023.</p><p>26.Li Y, Lin X, Wang W, et al. A Survey of Generative Search and Recommendation in the Era of Large Language Models[J]. arXiv preprint arXiv:2404.16924, 2024.</p><p>27.Wei T, Jin B, Li R, et al. Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm[C]//The Twelfth International Conference on Learning Representations. 2023.</p><p>28.Kenton J D M W C, Toutanova L K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of NAACL-HLT. 2019: 4171-4186.</p><p>29.Zhai J, Liao L, Liu X, et al. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations[J]. arXiv preprint arXiv:2402.17152, 2024.</p><p></p><p>作者：广告研发部&nbsp;申磊</p><p>来源：京东零售技术 转载请注明来源</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</id>
            <title>AI 卷生卷死的 Q2 终于结束了，InfoQ研究中心内容洞察集锦助你 Q3 先人一步</title>
            <link>https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4Ir7pxsrorgsvbuzgXiZ</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 09:34:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: InfoQ研究中心, 人工智能, 大模型, AGI
<br>
<br>
总结: InfoQ研究中心持续关注人工智能领域发展，发布多份人工智能相关研究报告，今年聚焦大模型及其产业应用，关注生成式AI开发者、技术动态趋势和5大行业实践情况，致力于助力中国大模型及AGI的商业落地。 </div>
                        <hr>
                    
                    <p>InfoQ&nbsp;研究中心自创立以来就持续关注&nbsp;AI&nbsp;领域的发展和更新，并持续推出了《中国开源生态图谱&nbsp;2023——人工智能领域》、《大语言模型综合评测报告&nbsp;2023》、《2023&nbsp;中国人工智能成熟度模型报告》、《大语言模型综合评测报告&nbsp;2024》等人工智能相关的研究报告。</p><p>今年以来，InfoQ研究中心将大模型及其产业应用，作为今年的研究主线之一。我们也认识到，技术本身无法孤立存在，本轮大模型的浪潮亦是如此，InfoQ研究中心对技术市场的用户分析和趋势分析，也离不开目前千行百业的大模型实践。因此第二季度，InfoQ研究中心继续聚焦生成式AI开发者、技术动态趋势以及5大行业实践情况，持续关注本轮大模型及AGI的浪潮。我们期望通过报告、文章、指南等多种形式，分享我们的研究成果和见解，共同助力中国大模型及AGI的商业落地。</p><p>更多精彩内容欢迎大家，点击文末海报，关注「AI前线」公众号，回复对应关键词领取，也可以扫描海报右下方二维码，直达「行业研究报告」专题。</p><p></p><h3>热门报告</h3><p></p><p></p><h4>《中国生成式AI开发者洞察2024》——聚焦技术市场人才分析</h4><p></p><p>报告回答关键问题：生成式AI的开发者面临怎样的薪资变化？什么样的岗位紧缺，开发者又需要具备什么样的技能目前的生成式&nbsp;AI&nbsp;开发者都在关注哪些领域的应用？</p><p></p><h4>《2024年第1季度中国大模型季度监测报告》——聚焦技术市场动态监测</h4><p></p><p>报告回答关键问题：Sora&nbsp;来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin&nbsp;和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？</p><p></p><h4>《中国AGI市场发展研究报告&nbsp;2024》——聚焦行业应用与实践</h4><p></p><p>报告回答关键问题：AGI&nbsp;概念引发热议，那么&nbsp;AGI&nbsp;究竟是什么？从技术架构来看，AGI&nbsp;又包括哪些？AI&nbsp;Agent&nbsp;如何助力人工智能走向&nbsp;AGI&nbsp;时代？现阶段营销、金融、教育、零售、企服等行业场景下，AGI应用程度如何？有哪些典型应用案例了吗？</p><p></p><h3>热门文章</h3><p></p><p>大模型的“瘦身”革命：巨头逐鹿轻量化大模型&nbsp;|&nbsp;大模型一周大事Stability、Mistral、Databricks、通义、A21&nbsp;Labs&nbsp;开源领域五连招，其中三个是&nbsp;MoE！|大模型一周大事国产版&nbsp;Sora&nbsp;到来！视频大模型更上一层楼&nbsp;|&nbsp;大模型一周大事文生视频模型“卷”出新天际；多家手机厂商&nbsp;AlI&nbsp;in&nbsp;Al，终端&nbsp;AI&nbsp;时代来临？|大模型一周大事让智能设备更懂你，主动式&nbsp;AI&nbsp;正在崛起&nbsp;|&nbsp;大模型一周大事</p><p></p><h3>热门图谱</h3><p></p><p></p><h4>中国大模型产品罗盘，涵盖&nbsp;200+&nbsp;主流大模型产品</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/98/985fef90d93c8a33a6509ca5d08402da.png" /></p><p></p><p>2024年第三季度，InfoQ研究中心也将继续前行，陆续发布各类重磅报告，欢迎大家持续关注。</p><p>7月：《中国开发者画像洞察研究报告&nbsp;2024》8月：《中国&nbsp;AI&nbsp;Agent&nbsp;应用研究报告》8月：《AGI&nbsp;在金融领域的应用实践洞察》</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a3bbe3a53a92e322aae7ab2025d6c21e.jpeg" /></p><p></p><p>机构介绍</p><p>InfoQ&nbsp;研究中心隶属于极客邦科技双数研究院，秉承客观、深度的内容原则，追求研究扎实、观点鲜明、生态互动的目标，聚焦创新技术与科技行业，围绕数字经济观察、数字人才发展进行研究。</p><p>InfoQ&nbsp;研究中心主要聚焦在前沿科技领域、数字化产业应用和数字人才三方面，旨在加速创新技术的孵化、落地与传播，服务相关产业与更广阔的市场、投资机构，&nbsp;C-level&nbsp;人士、架构师/高阶工程师等行业观察者，为全行业架设沟通与理解的桥梁，跨越从认知到决策的信息鸿沟。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</id>
            <title>李彦宏WAIC圆桌访谈：开源模型是智商税，智能体正在爆发</title>
            <link>https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lwLkRJbs5YbDMTKJGjQI</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 08:08:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界人工智能大会, 李彦宏, 大模型, 智能体
<br>
<br>
总结: 在2024世界人工智能大会期间，百度创始人李彦宏与其他嘉宾进行了一场圆桌访谈，讨论了大模型和智能体在人工智能领域的重要性。李彦宏强调了大模型在各行业应用中的潜力，以及智能体作为未来趋势的重要性。他认为，大模型的应用将带来更多的效率提升和成本节约，而智能体的低门槛将促进更多人参与创新，可能会诞生出未来的超级应用。 </div>
                        <hr>
                    
                    <p>在2024世界人工智能大会（WAIC 2024）期间，百度创始人、董事长兼首席执行官李彦宏，与第一财经传媒集团总编辑杨宇东和《硅谷101》创始人陈茜，进行了一场圆桌访谈。</p><p></p><p>在一个小时的对话中，李彦宏讲了几个独特观点：</p><p>一，现在很少有幻觉问题了。解决幻觉问题是在原来Transformer架构上，增加一些东西，“专业词语叫RAG”。</p><p>二，模型的推理成本，其实几乎是可以忽略不计。</p><p>三，开源其实是一种智商税。你永远应该选择闭源模型。</p><p></p><p>另外，李彦宏认为“没有应用的大模型一文不值”，呼吁行业不要卷模型了，要去卷应用。应用其实离大家并不遥远，基于基准模型应用在各行各业已经开始逐步渗透。他援引文心一言的调用量数据，两个月前还在2亿，现在已经到了5亿，说明大模型背后代表了真实的需求，有人真的从大模型当中获益了。</p><p></p><p></p><h3>以下是“百度”官方公众号发布的圆桌对话原文：</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/71/7139e76f64f5b5cb97ea9a342f981ff7.webp" /></p><p></p><p></p><h3>超级应用什么时候出现？基础模型之上将诞生数以百万计的应用</h3><p></p><p></p><p>杨宇东：由ChatGPT掀起的这个热潮已经持续一年多了，你也曾表达，接下来超级应用什么时候出现？我们看到国内面向C端的大模型产品形态，看起来都差不多，都是搜索框+问答这种模式，你怎么看？有没有可能产生一种差异化的竞争？什么样的好产品会出现？</p><p></p><p>李彦宏：我倒不是说一定在等待一个超级应用的出现。我更觉得，在基础模型之上，应该能够诞生数以百万计各种各样的应用。这些应用有些是很小的领域，一个大家可能不太容易想到的应用，但它对于那个领域的问题，解决得比以前好很多。确切的讲，我现在还没有看到，能够比肩移动互联网时期超级应用那样的AI时代的原生应用。但是已经看到，越来越多在各种各样场景、尤其是To B场景中，利用大模型提升了效率，产生了更多的收入，或者说节省了更多成本的情况出现。</p><p></p><p>今天，大家都在想，我能不能从0到1，做出一个人们想也没想到过的东西？变成一个DAU10亿的超级应用？这个当然很重要，假以时日也一定会出现。但是，更重要的是大模型在各个领域、各个场景的应用。</p><p>从百度文心一言的日调用量来看，已经非常明显。我们在4月份曾经公布过一个数据，文心一言的调用量每天有2亿次。前几天，我们再公布的时候，文心一言调用量已经到了5亿次。也就是说，两个月的时间调用量是double。调用背后意味着什么？意味着它在给应用产生价值。因为没有价值的话，人家也不会花钱去调用。</p><p></p><p>杨宇东：C端用户会有什么样很好的场景？包括端侧、手机上的APP，如何去调用AI能力？</p><p></p><p>李彦宏：我觉得分两类：一类是大家比较关注的，过去从来没有过的应用。现在比较流行的、类似于ChatGPT这样的ChatBot，就是聊天机器人。国内每一个大模型公司，都会推出一个相应的APP，或者是网站来做ChatBot。</p><p></p><p>对于现有这些To C的应用，其实它的信息增益作用也是非常大的。我们在4月份的时候，公布过一个数据，百度搜索今天有11%的搜索结果会由AI来生成，这个比例还在不断提升。再比如说百度文库，过去，百度文库是大家在上面找一些现成的文档。今天，百度文库经过大模型改造之后，已经更多地变成了生成式AI应用。你不管想要生产什么样的文档，是PPT、是论文的格式、甚至是漫画，它都可以根据你的要求生成。今年以来，文库已经有大约2600万付费用户。如果说用超级应用的标准来看，它也没有达到超级应用的水准，但是要看它实际产生的价值，有那么多人愿意为这个产品付费，还是很厉害。这些产品都是过去已经存在，但经过了大模型改造之后，它的能力跟以前完全不一样了。</p><p></p><p>陈茜：我特别同意你最近在多个场合强调的，去卷AI原生应用，大模型才有意义。但到今天，我们还没有看到应用的爆发，可能很多应用出来也不太尽人意。所以我的问题或者疑惑在于，如果从模型能力上看，是不是现在还没有到去卷应用的时候？</p><p></p><p>李彦宏：大模型应用其实已经逐步在浮现出来，它对于现有业态的改造作用，比从0到1的颠覆作用，更早到来。这个过程一开始大家觉得，没有那么性感，但是它对于人的工作效率的提升，对于成本的下降，对于打开新的可能性，产生的促进作用，是比那些从0到1的应用，反而更大。如果仅仅是从0到1，你可能会希望出现某几个Super APP，也就是几个公司从中受益。但是今天，几乎各行各业所有的公司，被大模型加持之后，它都能受益。这种影响力，对于整个社会、对于人类来说，无疑是更大的。</p><p></p><p>只是大家觉得，以前都存在，这个东西我以前见过，所以没有新鲜感。或者它更多诞生在生产力场景，它的受众群体，或者单一应用的受众群体，不会过亿过十亿。尤其在C端，在公众层面体感没有那么集中。这是大家一直在寻找一个Super APP的原因。</p><p></p><h3>为什么智能体是未来趋势？门槛足够低，跑通了就是Super APP</h3><p></p><p></p><p>杨宇东：我们前面聊的是“卷应用”，接下来还有一个关键词叫“智能体”。你说过好多次，AI时代最看好的应用是智能体。但我们目前并没有看到智能体的爆发，为什么你认为智能体是AI时代的未来趋势呢？</p><p></p><p>李彦宏：我觉得智能体正在爆发，只是说它现在基数还比较小，大家的体感没有那么强烈。但是你要看业界大模型公司，都在做智能体。智能体就是一个几乎可以“放之四海而皆准”的基于大模型的应用。今天大多数AI原生应用，你用智能体的方式都可以做出来，效果也不错。由于它门槛足够低，可能你连编程都不用，就可以做出一个效果不错的智能体。</p><p></p><p>门槛足够低，就意味着越来越多的人，可以做出他想要的智能体。这个有点像90年代中期时候的互联网网站。你可以把它做得非常复杂，比如雅虎就是很厉害的网站。但是在学校读书的大学生，他也可以做一个自己的Home Page。由于做网站很简单，在90年代中后期，就诞生了数以百万计的网站。大浪淘沙之后，最终出来了一些非常优秀的网站，像Google、Facebook，这是若干年之后才出现。但是早期看，这些网站都是乱糟糟的，一个大学生就能做一个网站出来，这有啥价值？但是你必须得门槛足够低的时候，让更多人进来，他们发挥聪明才智，指不定哪条路跑通了，它就是一个Super APP。</p><p></p><p>陈茜：业界对AI Agent的定义，还是有一点不同。你对Agent的定义是什么？</p><p></p><p>李彦宏：我首先要考虑，这个门槛要足够低，一个小白，大一的学生，他也可以很方便地制作一个智能体。当然在此之上，可以有各种各样比较fancy的玩法，调用工具、反思、长期的记忆等等，这些能力会逐步加进去。</p><p></p><p>不是说用了最先进的这些能力之后，它才叫一个AI Agent。我反而觉得，我们要把门槛降得足够低，让大家觉得，我也可以搞一个AI Agent。</p><p></p><p>说实话，我认为现在AI Agent用到的这些能力，仍然是非常初级的，未来还会产生我们今天想也想不到的Agent能力。但是这些能力的诞生，反而要依赖数以百万计的开发者，去开发各种各样的应用。在他们使用的过程当中产生新的需求，这些需求被解决的过程，就是一个创新过程，就是AI Agent进化的过程。</p><p></p><p>陈茜：百度有什么比较有意思的AI Agent案例，可以给我们分享一下吗？</p><p></p><p>李彦宏：有很多。国内高考是一个非常大的事件，不仅是学生，还有家长都非常重视。过去大模型在干什么事？高考有作文题，我们用大模型来写一个作文，看它能得多少分。其实你想一想，这个东西在哪用呢？不可能让一个考生带着大模型去参加高考。但是高考完了之后，你要估分，要报志愿，要选择学校，你要选择专业，一个考生他该报哪个学校，哪个专业，每个人情况都是不一样，每个人的问题也都是不一样。这种情况下，哪有一个全能的辅导老师可以告诉你，你最适合的是哪个学校哪个专业？但是AI Agent就可以干这个事情。我们开发了一个高考填报志愿的AI Agent。在高峰时期，一天有200万人在使用，足见大家对这个东西的认可度和依赖度还是非常高的。</p><p></p><p></p><h3>大模型对B端的改造比互联网更大，规模更小一点的模型市场需求量更大</h3><p></p><p></p><p>杨宇东：通用大模型和行业垂直大模型，它将来到底是什么样的关系？</p><p></p><p>李彦宏：大模型在各个垂直场景里怎么用？我们经过了一个探索过程。最初我们的想法是，我把这个基础模型做得越来越强大，大家叫通用人工智能，在什么场景我都能做得很好。后来发现这件事情没有那么容易，每个场景都有它自己的道。当应用场景需要反应快的时候，我们需要更小的模型。这种小的模型，它由于没有大模型通用的能力，所以在垂直场景当中，还要对它做精调，就是SFT，把你行业的数据怎么灌进去，再经过各种各样的调教，在这个场景里的效果，就能够跟大模型相比差不多。</p><p></p><p>类似这种场景，我们也见了很多。去年10月份，我们发了文心4.0之后，主要精力在做什么呢？就是根据最强大的模型，去裁剪各种体量的小模型，比如说十亿量级的模型，百亿量级的模型，千亿量级的模型，这个模型也许擅长角色扮演，那个模型也许擅长逻辑推理等等，根据客户不同使用场景的不同要求，做出各种各样的模型。这些模型大多数速度都比EB4要快，推理成本比它要低，所以这种应用是大家更爱用的。今天你要看市场需求的话，规模更小一点的模型，市场需求量是更大的。</p><p></p><p>杨宇东：你为什么认为，大模型对B端的改造，比互联网对B端的影响更大？</p><p></p><p>李彦宏：互联网对C端的改造，我们都是感同身受的，是非常彻底的，是颠覆性的。但是互联网对B端的改造，我觉得一般般。用的技术比较简单，产生的增益也没有那么明显。但大模型不一样。我们接触到的一些能源电力、生产制造等企业，都有类似的需求。比如说，现在国内电动车卷得也很厉害，车内的对话系统，很多也在用文心大模型，使用量也不小，但是对百度来说，这就是一个To B的应用，我们不直接提供给用户，它是经过了OEM，经过了车厂的集成之后，把这个应用提供给了终端消费者。这种事情其实非常多，而且我们就看调用量，如果调用量上得很快，这就说明我们的客户需要这些东西，B端靠着这个大模型，靠着AI原生应用产生了价值。</p><p></p><p>杨宇东：在金融、医疗等这些比较严谨的领域，生成式AI的幻觉问题，怎么破解？</p><p></p><p>李彦宏：今天，应该说你会很少发现幻觉问题了，尤其是用最大规模、最强大模型的时候，已经很少出现幻觉问题了。为什么呢？一开始，纯粹用原来的Transformer去做出来的大模型，它确实是非常难避免幻觉的，因为它是个概率模型。</p><p></p><p>要解这个问题，就要在原来Transformer架构上，增加一些东西，专业词语叫RAG。我只要稍微借助一点工具，就可以消除这样的幻觉。随着使用这种工具的能力越来越强，你就会发现，在各种场景下，幻觉是越来越少的。</p><p></p><p>当然，今天这种生成式人工智能，更像是一个Copilot，在特别严肃、对准确度要求特别高的场景下，我们还不能让它全部自动实现，还要靠人把最后一道关。这样，一方面可以提升效率；另一方面，在准确度上、在消除幻觉上，也能够起到比较重要的作用。</p><p></p><p>陈茜：现在企业对AI的使用成本怎么看？是否愿意为AI付费？你在跟一些企业客户交流的时候，他们的态度是什么样子的？</p><p></p><p>李彦宏：当你处在市场经济环境当中，企业其实是非常理性的。尤其是中小企业，账算得非常精。如果这件事情能够让我降本增效，能够让我赚到更多的钱，那我就会用它。如果不能，你再吹破天，我也不会用。市场会告诉你，大模型到底有用还是没用？我们看到调用量的迅速提升，确实是因为在用户侧、在客户侧，它为企业产生了降本增效的作用。</p><p></p><p>我再举个例子，比如说在招聘场景。过去是怎么做的？是HR坐在那，一份一份简历筛查，然后一个一个面试，面试100个人，最后筛出来10个人，再进行下一步面试，效率是非常非常低。但是大模型进来之后，它可以非常明显地去提升效率。因为，用大模型去理解这是一个什么人，理解这个老板要招什么样的人，然后进行匹配，它的效率就会高很多。</p><p></p><p>而且，你去算一算模型的推理成本，其实几乎是可以忽略不计的。尤其在国内，现在大模型价格战是非常厉害的，百度的轻量级模型都是免费的，这个免费不仅仅指的是模型免费，实际上算力也送你了，你本来要有电脑，要有带宽等等，这些都没有了，你只要来用就好。</p><p></p><p></p><h3>如何看“开源闭源之争”？开源是一种智商税，闭源模型比开源模型更强大</h3><p></p><p></p><p>杨宇东：开源闭源问题是业界关注焦点。你认为，闭源模型会持续领先。但我们看到，开源大模型越来越多，甚至有些能力都不亚于我们说谓的GPT4了，这个问题你怎么看，你们还是会坚定的走闭源路线？</p><p></p><p>李彦宏：我觉得，开源其实是一种智商税。你仔细想一想，我们为什么要做大模型？是它能够有应用，这些应用在各种场景下，能够为客户为用户提升效率、降低成本，产生过去产生不了的作用。所以当你理性的去想，大模型能够给我带来什么价值？以什么样的成本给我带来价值？你永远应该选择闭源模型。今天这些闭源模型，无论是ChatGPT还是文心一言，以及其他各种各样的闭源模型，它的平均水平，一定是比这些开源模型更强大，推理成本一定是比开源模型更低。</p><p></p><p>陈茜：百度对To B客户，是“闭源+公有云”这样一套打法，有什么考量吗？</p><p></p><p>李彦宏：ToB的客户，他要选择的是一个性价比最好的模型。一方面，模型要对他的应用产生价值，另外一方面，成本要足够低。很多时候，你看着有用，一算成本不划算，客户就放弃了。这是为什么我刚才讲，开源模型打不过闭源模型。你只要理性的去看待，你的收益是啥，你的成本是啥，你就会发现，最好还是去选择闭源模型。当然，闭源模型不是一个模型，它是一系列的模型，根据你的使用场景去平衡一下，要多好的效果，要多快的推理速度，要多低的成本。模型有非常多的变种，可以根据用户的需求，让他来做选择。</p><p></p><p>闭源模型还有一个开源模型不具备的优势：这些相对来说规模更小的模型，都是从最大最powerful的模型裁剪出来的，裁剪出来这些更小规模的模型，仍然比那些同样规模的开源模型要效果更好。</p><p></p><p>陈茜：百度对于中小模型、模型蒸馏上，有什么样的策划？</p><p></p><p>李彦宏：我们看到的真实需求，在绝大多数情况下都不是最大的模型，它都要求这个模型变小。变小意味着什么？速度快，成本低。比如说，我干这个事儿，总共能够给我带来每年100万的增益，但使用最大的模型要120万的成本，那我肯定不干了。那我就会给大模型公司提要求，把成本降到80万，甚至降到8万。那我们就得想，怎么把最强大的模型，蒸馏到足够小，成本足够低，满足这个场景需求。因为闭源有一个最强大的基础模型，根据模型蒸馏或者裁剪出来的小模型，比那些开源模型做出来的东西更有竞争力。所以我们觉得，To B的机会仍然在闭源不在开源。</p><p></p><p></p><h3>大模型价格战不可避免，最终还是比谁的技术好、效率高</h3><p></p><p></p><p>杨宇东：我们现在看到价格战已经开始打起来，其实还是蛮出乎我们的预料，这么快。</p><p></p><p>李彦宏：价格战几乎不可避免，在中国互联网干了这么长时间，其实已经对价格战非常熟悉。但就像你讲的，确实来得比我想象的更早一点，这么早就开始把价格打到几乎不可思议低的地步。但某种意义上讲也不是坏事儿，当你足够低，甚至免费的时候，就会有更多人有积极性来尝试，在大模型基础上去开发各种各样的应用，大模型对于各行各业的渗透速度会更快。</p><p></p><p>杨宇东：很多闭源大模型API调用费越来越低，大模型靠推理收费的商业模式未来成不成立？以后大模型比拼的是哪些点？</p><p></p><p>李彦宏：大模型技术天花板还是很高的，今天我们还是对于大模型的能力有很多不满意的地方，仍然需要很多非常优秀的技术人员、需要很多算力、需要很多数据，才能训练出下一代大模型，我们还可能需要下下一代、下下下一代的大模型。</p><p></p><p>所以最终我觉得大家是要去拼谁的技术更好，你的技术好，你为客户产生了更好的价值。今天之所以把这个模型打到足够低，是因为现在模型的这个能力其实还没有到最好，没到最好的时候，大家都差不多的时候，就会谁的价格低就用谁的。</p><p></p><p>时间长了之后，市场本身会回归理性。最终还是比谁的技术好，谁的效率高，谁会胜出。</p><p></p><p>陈茜：你觉得这个价格战会持续多久的一个时间呢？</p><p></p><p>李彦宏：这个很难讲，现在有些创业公司是玩家，也有很多非常大型的互联网平台公司是玩家，其实理论上讲是可以烧很长时间。但我觉得烧钱不是事情本质，事情本质仍然是谁的技术更好，谁的效率更高，当你的技术好、效率高的时候，你就不怕去打这个价格战，所以多长时间都OK，最终会是优胜劣汰的过程。</p><p></p><p>陈茜：你觉得在中国市场会是一个赢家通吃这样的一个局面吗？还是说等价格战之后会剩下几个主要的？可能还有一些更小一点的？</p><p></p><p>李彦宏：这次生成式AI是对整个IT技术栈的大变革，过去IT技术栈是芯片层、操作系统层、应用层或者软件层，就这三层。到生成式AI之后，IT技术栈变成了四层，芯片、深度学习框架层、模型层、应用层，我认为在每一层可能都会诞生至少2—3个大玩家。</p><p></p><p>应用层的话，可能会有数以百万计、甚至数以千万计的各种各样应用出来，也会逐步出现超级应用，既然是超级应用，当然不会很多，可能是三五个。</p><p></p><p>模型层我觉得也许两三个就足够了，因为最后大家比拼的是效率，你的效率如果不够高的话，慢慢就觉得说还不如用别的。</p><p></p><p></p><h3>Scaling Law短期内不会被颠覆，图灵测试不再是标准，AGI需要十年以上才能实现</h3><p></p><p></p><p>杨宇东：Scaling Law还会持续有效吗？</p><p></p><p>李彦宏：Scaling Law可能还会有若干年的生命周期。但与此同时，在此之上会叠加各种各样的创新。刚才讲的智能体，它的反思、进化能力等，其实跟Scaling Law已经是两个路线在发展，但它仍然是基于Transformer这类大模型往上做。未来再过一两年，还会出现什么新的技术创新，在此基础上再去叠加，大家都在探索。换句话说，我觉得Scaling Law短期之内不会被颠覆，但是在Scaling Law之上会叠加出来很多我们现在可能还无法想象的创新。</p><p></p><p>杨宇东：你认为AGI实现的标准是什么？还有哪些路径可以让我们更快地通向AGI？</p><p></p><p>李彦宏：业界确实还没有一个标准答案。以前大家觉得，通过图灵测试就实现AGI了，实际上现在大模型已经通过了图灵测试。人们所说的AGI，其实大多数时候已经不是只通过图灵测试了。</p><p></p><p>那么什么叫AGI？在我心目中，AGI就是机器或者说AI，能够具备人在任何场景下所具备的能力。Artificial General Intelligence，就是通用人工智能，它不管在什么场景下，能力都是跟人一样的，这是一个很高的要求。</p><p></p><p>所以真正要实现AGI，我认为还需要很多很多年。业界有人说AGI可能再过2年，或者再过5年能实现。我自己的判断是10年以上，也许更长的时间。我们听到很多人讲，AGI是一种信仰，当你把它当做一种信仰的时候，谁的信仰会明年就实现？这是自相矛盾的。如果是一个信仰，它就是你值得为之长期奋斗的一个目标。</p><p></p><p>陈茜：现在GPT5一直在延后，担忧的声音也越来越高，AGI没有办法用Scaling Law这个方式去带我们实现了，你对这个有担忧吗？</p><p></p><p>李彦宏：我不是很担心这件事情，我觉得大家应该更关注应用，而不是关注基础模型，某种意义上基础模型迭代速度稍微放缓一点不是坏事，如果今天的应用开发者，有一个相对稳定的基础来开发应用，其实是效率更高一些的，如果模型天天在那儿练，每天都要重写一遍过去的代码，那是很累的。但是在现有基础模型上不断去做微调，去做一些渐进式的迭代和和创新，其实你看到是一直在发生的，无论是OpenAI不断在推的，还有百度我们的Turbo模型、更小量级的模型等等，都是在根据市场的需求在做迭代。</p><p></p><p>但长远来讲，我确实认为下一代大模型一定会比现在这一代模型强大得多。什么时候推出来我不是很着急，我们应该更多的去看真实的市场需求，下一代模型在迭代的时候，要根据市场需求来迭代。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</id>
            <title>成立半年多就敢踢馆 OpenAI ，首个开源模型不输 GPT-4o，LeCun 、PyTorch 之父齐声叫好！</title>
            <link>https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uQub8q2LPGtzPP4S0mbq</guid>
            <pubDate></pubDate>
            <updated>Fri, 05 Jul 2024 07:43:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI模型, Moshi, Kyutai, 多模态模型
<br>
<br>
总结: Kyutai团队开发了一种名为"Moshi"的AI模型，具有多种情绪表达和语音模仿能力，同时处理两个音频流。这个模型被称为世界上首个具有自然对话能力的AI助手，具有改变人机通信的潜力。Moshi还能处理文本和音频，支持同时听和说，具有文本思想和情商，能够在半秒内回复。 </div>
                        <hr>
                    
                    <p>整理&nbsp;|&nbsp;华卫</p><p>&nbsp;</p><p>想象一下，一个&nbsp;AI&nbsp;模型可以表达&nbsp;70&nbsp;多种情绪，以不同的风格说话，甚至令人信服地模仿口音。并且，它能够同时处理两个音频流，同时听和说。这不是科幻小说，而是Kyutai在语音AI技术上的最新突破。</p><p>&nbsp;</p><p>只用短短&nbsp;6&nbsp;个月的时间，这个由&nbsp;8&nbsp;人组成的非营利性&nbsp;AI&nbsp;研究实验室从零开发出了一种名为&nbsp;"Moshi&nbsp;"的实时原生多模态基础&nbsp;AI&nbsp;模型。根据&nbsp;Kyutai&nbsp;的说法，Moshi&nbsp;是世界上首个具有自然对话能力的可公开访问&nbsp;AI&nbsp;助手。OpenAI&nbsp;之前曾展示过<a href="https://www.infoq.cn/article/42ROdXw5VHrfFMsITd07">GPT-4o&nbsp;</a>"的语音引擎和语音模式功能，但尚未发布。</p><p>&nbsp;</p><p>据称，该模型具备的功能可与&nbsp;<a href="https://www.infoq.cn/article/a0XsHUI5y7sVUzlqCXC7?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">OpenAI&nbsp;</a>"的&nbsp;GPT-4o&nbsp;和&nbsp;Google&nbsp;Astra&nbsp;相媲美，但模型要小得多。“Moshi&nbsp;在说话时思考。”Kyutai&nbsp;首席执行官帕特里克·佩雷斯&nbsp;（Patrick&nbsp;Pérez）&nbsp;表示，Moshi&nbsp;具有彻底改变人机通信的潜力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/84/84b023c0fca40c5d913743b7c2743b86.jpeg" /></p><p></p><p>7月4日，Kyutai在法国巴黎公开发布了Moshi&nbsp;的实验原型，用户可以在网上自由<a href="https://moshi.chat/?queue_id=talktomoshi">测试体验</a>"。值得一提的是，Kyutai的所有模型都是开源的。之后，该团队不仅计划发布完整模型，包括推理代码库、7B 模型、音频编解码器和优化堆栈。</p><p></p><p>图灵奖得主<a href="https://www.infoq.cn/article/Gf8Z4CVHwvqLEOXGlY9c?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Yann&nbsp;LeCun</a>"分享说：“Moshi可以听懂带有法国口音的英语。”就连&nbsp;PyTorch&nbsp;之父Soumith&nbsp;Chintala也向Kyutai表示了祝贺，并透露该团队某成员是他在Meta&nbsp;的&nbsp;AI&nbsp;研究团队&nbsp;FAIR&nbsp;的前同事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/80aba663529836be3c65b6cf65fc02a2.png" /></p><p>Kyutai团队</p><p>&nbsp;</p><p>据悉，这家成立于&nbsp;2023&nbsp;年&nbsp;11&nbsp;月的初创团队，得到了包括法国亿万富翁&nbsp;Xavier&nbsp;Niel&nbsp;在内投资的近&nbsp;3&nbsp;亿欧元的支持，旨在为&nbsp;AI&nbsp;的开放研究做出贡献并促进生态系统发展。Kyutai&nbsp;还组建了一支由知名人工智能研究人员组成的科学顾问团队——计算机科学家、2022&nbsp;年麦克阿瑟“天才”奖获得者Yejin&nbsp;Choi，Meta&nbsp;首席&nbsp;AI&nbsp;科学家、ACM图灵奖获得者Yann&nbsp;LeCun&nbsp;和德国马克斯·普朗克智能系统研究所研究所所长Bernhard&nbsp;Schölkopf。</p><p></p><h1>对话流畅又会整活，甚至还会“抢话”</h1><p></p><p>在现场演示过程中，Kyutai&nbsp;团队与&nbsp;Moshi&nbsp;进行互动，展示了其在各种说话风格之间无缝切换，以及在角色扮演中迅速化身的创造力。</p><p>&nbsp;</p><p>当被要求用法国口音说话时，Moshi&nbsp;朗诵了一首关于巴黎的诗；在被要求变身为一个热情洋溢的海盗时，Moshi&nbsp;讲述了七大洋上的勇敢和冒险故事；Moshi&nbsp;还能用一种低语的讲述神秘故事的语气，表达《黑客帝国》的电影情节。</p><p></p><p></p><p></p><p>Moshi还能一秒化身太空助手，和对话用户一同“进入”太空之旅。并且，Moshi&nbsp;的反应似乎比人类更快，经常在问题或提示被完全提出之前就做出了回答。</p><p></p><p></p><p></p><p></p><p>在发布现场的一系列演示中，Moshi&nbsp;是在没有互联网连接的标准&nbsp;MacBook&nbsp;Pro&nbsp;上运行。Kyutai&nbsp;还计划进一步优化移动设备的&nbsp;Moshi，确保其广泛采用。这将使Moshi更加通用，从个人助理到便携式教育工具，可以在各种环境中使用。</p><p></p><h1>有思想、有情商，半秒内就能回复</h1><p></p><p>据介绍，&nbsp;Moshi不仅仅是一个语音&nbsp;AI，还是一个能够处理文本和音频的多模态模型，主要功能特点包括：</p><p>&nbsp;</p><p>同时听和说：Moshi支持多流音频，使其能够同时收听和响应，从而实现自然流畅的前后对话，其中中断和重叠的语音很常见。与依靠语音活动检测来切换轮次的传统系统不同，Moshi&nbsp;保持连续的对话流。文本思想：在用音频说话时，Moshi&nbsp;会产生文本思想。这种双重方法增强了其产生准确和符合具体情况的响应的能力。通过文本思考，Moshi&nbsp;可以更有效地组织其响应，并从更丰富的知识库中汲取灵感。富有情商：Moshi&nbsp;不仅仅是文字，而是关于理解它们背后的意图。该模型经过训练，可以识别情绪，甚至可以生成传达特定情绪的语音。实时交互：Kyutai&nbsp;声称&nbsp;Moshi&nbsp;的理论延迟仅为&nbsp;160&nbsp;毫秒，而实际上，它在&nbsp;200&nbsp;到&nbsp;240&nbsp;毫秒之间。人人可访问：不仅是开源项目，公司、研究人员都可以集成、试验，而且开发了一种可以在个人计算机上运行的较小版本，使这项技术能够被大型研究实验室以外的更广泛的用户使用。负责任的&nbsp;AI&nbsp;：Kyutai&nbsp;正在整合水印技术帮助识别&nbsp;AI&nbsp;生成的音频，以确保透明度。</p><p>&nbsp;</p><p>其中，Moshi&nbsp;最令人印象深刻的方面之一是它能够在设备上运行。此功能解决了隐私问题，并使&nbsp;AI&nbsp;在实时应用程序中更易于访问和响应。用户可以与Moshi进行交互，而不必担心数据被发送到远程服务器。</p><p></p><h1>70&nbsp;亿参数提供支持，Moshi是如何训练的?</h1><p></p><p>Moshi&nbsp;因其同时处理音频和文本的能力而脱颖而出，而这种实时交互是由&nbsp;Kyutai&nbsp;创新的联合预训练过程提供支持。</p><p>&nbsp;</p><p>据了解，Moshi&nbsp;基于&nbsp;Helium&nbsp;7B&nbsp;模型构建，集成了文本和音频训练，针对&nbsp;CUDA、Metal&nbsp;和&nbsp;CPU&nbsp;后端进行了优化，支持&nbsp;4&nbsp;位和&nbsp;8&nbsp;位量化。在训练方面，Kyutai&nbsp;使用了各种数据源，包括人体运动数据和&nbsp;YouTube&nbsp;视频。</p><p>&nbsp;</p><p>Moshi&nbsp;还集成了基于&nbsp;Kyutai&nbsp;的&nbsp;Mimi&nbsp;模型的高压缩语音编解码器，可以高效处理音频信息。</p><p>&nbsp;</p><p>训练中，Moshi涉及一些创新的开创性技术，使其对自然语言和对话流程有了深刻的理解。</p><p>&nbsp;</p><p>音频语言模型：Moshi&nbsp;的模型不是只在文本上训练，而是在语音数据上训练。语音被压缩成伪词，然后用这些伪词来训练模型以预测下一段音频。这种方法使模型能够理解口语的内容和上下文。合成对话：为了训练Moshi进行对话，Kyutai从纯文本语言模型中生成了合成对话。然后，这些对话通过内部文本转语音引擎进行合成。这种方法确保其学会了处理真实的对话动态。</p><p>&nbsp;</p><p>同时，Kyutai&nbsp;以新颖的方法正面解决了传统的语音&nbsp;AI&nbsp;系统面临的问题，如延迟和处理过程中非文本信息的丢失，创造了一种响应更灵敏、听起来更自然的&nbsp;AI。</p><p>&nbsp;</p><p>集成深度神经网络：Kyutai&nbsp;没有依赖每个任务的单独模型，而是将所有内容合并到一个深度神经网络中。这种集成减少了延迟，并保留了语音通信的丰富性，而语音通信在纯文本处理中通常会丢失。基于语音的训练：Moshi的模型从大量压缩的带注释的语音片段中学习，使其能够理解语音的复杂性，包括特定的声音特征和声学条件。</p><p>&nbsp;</p><p>此外，Kyutai&nbsp;敏锐地意识到高级语音&nbsp;AI&nbsp;可能被滥用于恶意目的，如网络钓鱼。为了降低这些风险，Kyutai&nbsp;实施了识别&nbsp;Moshi&nbsp;生成内容的策略，包括维护生成的音频签名的数据库，并使用水印技术在音频中嵌入听不见的标记。</p><p></p><h1>结语</h1><p></p><p>Moshi代表了语音AI技术的重大飞跃。更广泛地说，Moshi&nbsp;有可能彻底改变数字世界中语音的使用。例如，它的文本到语音功能在情感和多人语音互动方面非常出色。它能够传达情感、调整说话风格和进行自然对话，这将彻底改变我们与人工智能互动的方式，并开启了一个充满可能性的世界：</p><p>&nbsp;</p><p>客服支持：由&nbsp;Moshi&nbsp;提供支持的&nbsp;AI&nbsp;助手可以提供富有同理心和高效的客服支持，提高用户满意度并减少等待时间。语言学习：Moshi&nbsp;模仿母语口音和传达情感的能力可以彻底改变语言学习，使其更加身临其境和有效。医疗保健：Moshi可以作为患者的伴侣，提供支持和信息，同时根据用户的情绪状态调整其语气。娱乐：Moshi可以凭借其多样化的声音和情感将角色带入生活，丰富互动式讲故事体验。</p><p>&nbsp;</p><p>与此同时，Moshi的出现隔空对OpenAI等主要人工智能公司提出了挑战，这些公司因安全问题而推迟发布类似的语音功能产品而受到不少用户的批评。</p><p>&nbsp;</p><p>不过，也有Moshi的使用者表示，其在第一分钟左右的速度和响应速度都非常快，但对话进行的时间越长，就会变得越不连贯；并且，Moshi明显缺乏知识，在犯了错误而受到责备时，就会惊慌失措，陷入“对不起，对不起...”的循环回复。</p><p>&nbsp;</p><p>虽然&nbsp;OpenAI&nbsp;暂时还不需要担心来自&nbsp;Moshi&nbsp;的竞争，但确实表明，许多公司正在迎头赶上OpenAI。就像Sora一样，现在Luma&nbsp;Labs、Runway&nbsp;等其他公司都在推出表现不弱的竞对产品挑战其模型质量和市场地位。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d</a>"<a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">1</a>"<a href="https://medium.com/@shrimangalevallabh789/moshi-voice-ai-the-advanced-voice-ai-that-feels-almost-human-d185d85da97d">85d85da97d</a>"</p><p><a href="https://analyticsindiamag.com/french-ai-lab-kyutai-releases-openai-gpt-4o-killer-moshi/">https://analyticsindiamag.com/french-ai-lab-kyutai-releases-openai-gpt-4o-killer-moshi/</a>"</p><p><a href="https://www.tomsguide.com/ai/moshi-chats-gpt-4o-advanced-voice-competitor-tried-to-argue-with-me-openai-doesnt-need-to-worry-just-yet">https://www.tomsguide.com/ai/moshi-chats-gpt-4o-advanced-voice-competitor-tried-to-argue-with-me-openai-doesnt-need-to-worry-just-yet</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/722052195cf10d5865fc51bba</id>
            <title>线索系统性能优化实践</title>
            <link>https://www.infoq.cn/article/722052195cf10d5865fc51bba</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/722052195cf10d5865fc51bba</guid>
            <pubDate></pubDate>
            <updated>Wed, 03 Jul 2024 03:49:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 线索CRM系统, 性能优化, 流程梳理与抽象, 模板方法设计模式
<br>
<br>
总结: 京东家居事业部的线索CRM系统在业务扩张和市场需求增长的情况下出现了架构不适应、代码冗余、接口响应时间长等问题，需要进行性能优化和调整以支撑业务快速发展。优化目标是统一封装和抽象线索提交流程，提高系统可维护性和扩展性，降低新渠道接入时间成本，提高接口性能和响应，保证接口正确性。通过流程梳理与抽象、创建流程拆分和模板方法设计模式的应用等策略和实施，实现了系统性能的优化和提升。 </div>
                        <hr>
                    
                    <p></p><p></p><h2>引言</h2><p></p><p>在京东家居事业部，线索CRM系统扮演着至关重要的角色，它作为构建家居场景核心解决方案集的首要环节，肩负着获客和拓展业务的重要使命。然而，随着业务的不断扩张和市场需求的日益增长，系统原有的架构开始显露出诸多不适应之处，如架构设计不再清晰，代码存在过量冗余，核心的读写接口响应时间长等问题，这些问题严重制约了业务的敏捷性和快速发展。鉴于这一状况，系统的性能优化和调整势在必行，以确保其能够更好地支撑业务的快速发展需求。</p><p></p><h2>系统优化概述</h2><p></p><p></p><h2>一. 线索提交接口的统一与性能优化</h2><p></p><p></p><h3>系统优化前存在的问题</h3><p></p><p></p><h4>1. 新渠道接入周期长，代码冗余，</h4><p></p><p>系统中存在五个主要的线索创建渠道，它们的处理流程高度相似，但是代码却是分散冗余的。每当有新渠道需要接入时，之前的做法都是从已有代码中复制粘贴并做小幅调整，缺乏抽象和封装，导致了代码的高度重复，增加了维护的难度和出错的风险。</p><p></p><p>比如当时我们为了支持多供应商这个需求，需要对线索分派商家的逻辑进行更改，由于这段逻辑分散在多处，同时由于测试对底层实现的不了解，可能会误认为只需要测试一个渠道就能覆盖基本场景，就有可能导致非必要的线上问题的产生。</p><p></p><h4>2. 性能瓶颈</h4><p></p><p>在线索创建过程中，由于业务的复杂性需要执行10来个子流程以及开发过程中的不规范导致的对线索主数据的不必要的重复更新、重复同步ES等问题，接口性能较慢，tp99将近3000ms。</p><p></p><h4>3. 数据一致性问题</h4><p></p><p>线索创建主数据，分配商家以及匹配到重复规则时需要新增运营回访记录，这些流程都涉及到对数据库的写操作，但是这些写入没有放在同一事务中，导致了某个子流程写入失败时存在数据一致性问题。</p><p></p><h3>优化目标</h3><p></p><p>我们的优化目标是对线索提交流程统一封装和抽象，提高系统的可维护性和扩展性。同时降低新渠道接入的时间成本，提高接口性能和响应，保证接口在复杂情况下的正确性。</p><p></p><h3>优化策略与实施</h3><p></p><p></p><h4>流程梳理与抽象</h4><p></p><p>我们首先对当前所有渠道的线索创建流程进行了全面的梳理，将线索的创建流程抽象化，并定义出一套标准化的流程模板。具体来说一个线索的创建包括以下流程：</p><p></p><p>(1) 入参校验</p><p></p><p>(2) 查询三级渠道</p><p></p><p>(3) 验证三级渠道开关</p><p></p><p>(4) 根据入参封装要创建的线索实体</p><p></p><p>(5) 线索是否重复的规则校验</p><p></p><p>(6) 数据库保存线索和异构到ES</p><p></p><p>(7) 读取配置规则以及后续的同步京音系统</p><p></p><p>(8) 将新线索分配到对应的商家</p><p></p><p>(9) 短信消息和京麦消息通知商家</p><p></p><p>(10) 根据线索和分配的商家信息为用户创建装修档案</p><p></p><h4>创建流程拆分</h4><p></p><p>通过分析发现，对于不同渠道的线索创建过程来说，最大的差异点在于流程(1) 和 (2), 对于流程(3)-(10)基本相似。</p><p></p><p>这些流程虽然在逻辑上紧密相连，但是对于线索创建这一业务来说最核心的流程是流程(6)及之前的流程，至于流程(7)-(10)则是线索创建后的附属操作，这些附属操作涉及到和外部门系统间复杂的交互，占用了大量资源并影响到核心流程的响应速度。</p><p></p><p>因此我们聚焦于线索创建这一核心流程，和从职责单一的角度考虑，我们将整个线索的常见进行拆分:</p><p></p><p>第一 核心流程-线索的创建。</p><p></p><p>第二 线索分配商家以及之后的通知操作</p><p></p><p>第三 为分配商家后为用户创建对应的装修档案</p><p></p><p>这三个创建流程通过京东自研消息JMQ进行串联，解耦了线索创建和附属操作的执行。通过异步处理附属操作，附属操作的耗时不会阻塞核心流程的执行，减少了对核心流程的干扰，从而大大提升了系统的响应性和吞吐量。</p><p></p><h4>模板方法设计模式的应用</h4><p></p><p>定义： 模板方法设计模式是一种行为设计模式，它在一个方法中定义了一个算法的骨架，将一些步骤的执行延迟到子类中。这样，子类可以在不改变算法结构的情况下重新定义算法的某些特定步骤。</p><p></p><p>通用类图:</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e195c0a0ebd9eef1722ebaa424cbaf2.png" /></p><p></p><p>​</p><p></p><p>示例代码:</p><p></p><p><code lang="text">public abstract class Game {
    // 模板方法，定义算法骨架
    public final void play() {
        initialize();
        startPlay();
        endPlay();
    }

    // 需要子类实现的方法
    abstract void initialize();
    abstract void startPlay();
    abstract void endPlay();
}

public class Cricket extends Game {
    @Override
    void initialize() {
        System.out.println("Cricket Game Initialized! Start playing.");
    }

    @Override
    void startPlay() {
        System.out.println("Cricket Game Started. Enjoy the game!");
    }

    @Override
    void endPlay() {
        System.out.println("Cricket Game Finished!");
    }
}

public class Football extends Game {
    @Override
    void initialize() {
        System.out.println("Football Game Initialized! Start playing.");
    }

    @Override
    void startPlay() {
        System.out.println("Football Game Started. Enjoy the game!");
    }

    @Override
    void endPlay() {
        System.out.println("Football Game Finished!");
    }
}

public class TemplateMethodPatternDemo {
    public static void main(String[] args) {
        Game game = new Cricket();
        game.play();

        System.out.println();

        game = new Football();
        game.play();
    }
}
</code></p><p></p><p></p><p>在这个例子中，Game是一个抽象类，定义了游戏的模板方法play()。Cricket和Football是具体的游戏，它们实现了Game类的抽象方法，以提供各自的游戏初始化、开始和结束的具体实现。</p><p></p><p>具体到我们系统， 流程1到10是创建线索的骨架抽象和定义。对于骨架中的子流程，我们识别出易变部分（步骤1和2）和 不易变的部分（步骤3至6）。易变部分需要交给子类去实现，不易变部分则需要统一实现。</p><p></p><h4>易变部分抽象</h4><p></p><p>对于入参校验和查询三级渠道这两个流程来说，每个渠道都存在独有的逻辑，比如，心愿单渠道需要校验心愿单类型和来源ID必传，而投放助手渠道则需校验投放单号必传；多阶段订单渠道是通过SKU来查询三级渠道，而市场部渠道则是通过媒体账号ID来查询。</p><p></p><p>因此我们对于这两个流程定义了抽象方法，并将实现细节交个具体渠道的负责。</p><p></p><h4>不变部分统一处理</h4><p></p><p>对于线索创建流程中的不易变部分，我们实现了统一的处理逻辑，如三级渠道开关验证、线索归集信息封装、重复规则校验、数据库保存以及异构到ES等流程。</p><p></p><p>同时对于所有需要数据库变更的操作放到一个事务中，保证了写入的同时成功或失败。</p><p></p><h4>​工程实践</h4><p></p><p>通过上文介绍， 编码大体实现如下：</p><p></p><p><code lang="text">//获取三级渠道
protected abstract ChannelThreeDto getChannel(ClueDTO clueDTO);

//前置状态校验
protected abstract boolean preConditionCheck(ClueDTO clueDTO);

public ResultDto submit(ClueDTO clueDTO) {

        //1.前置状态校验
        if (!preConditionCheck(clueDTO)) {
            return ResultDto.getFailedResult(ResultCodeEnum.SERVICE_ERROR.getMsg(), ResultCodeEnum.SERVICE_ERROR.getCode());
        }

        //2.获取三级渠道
        ChannelThreeDto channelThreeDto = getChannel(clueDTO);

        //3.确认渠道开关是否开启
        if (!checkChannel(channelThreeDto)) {
            return ResultDto.getFailedResult(ResultCodeEnum.SERVICE_ERROR.getMsg(), ResultCodeEnum.SERVICE_ERROR.getCode());
        }

        //4.线索重复校验
        Boolean isRepeat = checkClueRepeat(clueDTO, channelThreeDto);
        if (isRepeat) {
            return ResultDto.getFailedResult(ResultCodeEnum.SERVICE_ERROR.getMsg(), ResultCodeEnum.SERVICE_ERROR.getCode());
        }

        //5.封装线索实体对象
        ClueManageDto clueManageDto = buildClueManage(clueDTO, channelThreeDto);

        //6.数据清洗规则检查
        ClueVisitDto clueVisitDto = clueDataWash(clueManageDto, channelThreeDto);
        if (!ObjectUtils.isEmpty(clueVisitDto)) {
            clueManageDto.setClueStatus(ClueStatusEnum.INVALID.getCode());
        }

        //7.数据库保存
        boolean result = saveClueManage(clueManageDto, clueVisitDto);

        //8.发送线索创建通知，执行之后的线索分配商家等操作
        sendClueMessage(clueDistributionDTO);

        return ResultDto.getSuccessResult(result, ResultCodeEnum.SUCCESS.getCode());
    }
</code></p><p></p><p></p><p></p><h3>优化成果</h3><p></p><p>通过引入模板方法的设计模式、异步拆分以及优化事务管理策略，创建线索的系统架构得到了根本性的改进。 我们不仅提高了代码的复用率，降低了新渠道接入的成本，也极大地提升了系统的可维护性和扩展性。</p><p></p><p>现在，新渠道的接入变得更加快捷和灵活，从之前新渠道接入耗时6人/天降低到2人/天左右；同时线索创建的响应时间也从之前的3000ms降到现在的250ms左右。</p><p></p><h2>二. 线索核心写接口性能优化实践</h2><p></p><p></p><h3>背景</h3><p></p><p>在竞争激烈的市场环境中，CRM系统不仅需要准确无误地收集用户的客资信息，更重要的是要实现对这些宝贵信息的快速响应和有效跟进。用户留下联系方式的瞬间，往往是他们对产品或服务兴趣最浓厚的时刻，我们需要快速响应，抢占先机，才有可能增加用户转化为客户的可能性，因此对于核心接口的性能有较高的要求。</p><p></p><p>但是当前系统在处理线索创建、分配商家，状态变更以及商家反馈等核心流程上存在接口性能不理想的问题，比如商家反馈线索tp99耗时2000ms， 分配商家耗时1500ms。</p><p></p><h3>问题分析</h3><p></p><p>在每个核心流程中，系统会进行两项重要的操作：</p><p></p><p>1.更新数据库：将业务操作的结果持久化到数据库中。</p><p></p><p>2.数据同步到ES：将变更的数据同步到两个ES集群中（一个供运营端查询，另一个供商家端查询适用）</p><p></p><p>传统同步机制是在业务逻辑操作完成后立即进行数据同步。这种同步方式虽然简单直接，但存在几个缺点：</p><p></p><p>•性能瓶颈：同步操作耗时，导致接口响应时间增长，影响用户体验。</p><p></p><p>•复杂度增加：业务逻辑与数据同步逻辑耦合，增加了代码的复杂度和维护难度。</p><p></p><p>•扩展性受限：随着业务增长，同步操作成为系统扩展的瓶颈。</p><p></p><h3>优化方案</h3><p></p><p>针对上述问题，我们采取了一系列措施来优化系统性能，核心策略是将数据同步到ES的过程异步化。</p><p></p><p>1. 订阅Binlake变更</p><p></p><p>我们将业务逻辑操作和数据同步到ES的过程分离。业务接口只负责业务逻辑的变更和数据库的更新，而数据同步到ES的操作，通过订阅Binlake变更事件来异步执行。</p><p></p><p>2. 处理变更消息</p><p></p><p>通过订阅线索主数据和线索分配商家数据的变更消息，封装接口将线索主数据和分配商家信息同步到ES。值得注意的是，为了避免数据库变更在JMQ中的乱序性以及可能带来的数据被错误覆盖的问题，我们只关注消息中的哪个线索单号发生了变化，而不关注具体的变更细节，通过线索单号反查数据库的方式，将最新的数据同步到ES。</p><p></p><p>3. 合并更新和统一事务</p><p></p><p>在原来的线索分配商家以及商家反馈线索接口中，存在对同一个表反复更新并且多次同步ES的问题，通过底层重构，我们把所有的DML操作合并到一个事务中，减少更新次数的同时保证了数据的正确性。</p><p></p><p>4. 非核心流程异步化</p><p></p><p>把原来线索反馈商家接口中的非核心流程异步化。在商家反馈线索状态后需要触发回流操作，回流操作本身就是一个非常耗时的操作，经常导致用户反馈接口超时，但是回流本身是用户不关注的，用户只关注他反馈的动作是否完成。因此我们对回流进行异步化，反馈线索接口现在只处理线索状态的更新，回流则是通过发送JMQ消息的方式异步处理来减少用户等待时间。</p><p></p><h3>优化成效</h3><p></p><p>经过优化，线索系统的性能得到了显著提升：</p><p></p><p>1. 接口响应时间明显缩短：</p><p></p><p>(1) 线索提交 (投放助手渠道):</p><p></p><p>优化前:(2000-4000ms)优化后:（100-300ms）</p><p></p><p>(2) 线索分配商家接口:</p><p></p><p>优化前:(1000-2000ms)优化后:（100-400ms）</p><p></p><p>(3) 商家反馈线索接口:</p><p></p><p>优化前:(1000-3000ms)优化后:（30-60ms）</p><p></p><p>2. 用户体验改善：商家在反馈线索状态时不再遇到超时问题。</p><p></p><p>3. 架构清晰：业务逻辑与数据同步逻辑解耦，代码更加清晰和容易维护。</p><p></p><p>4. 扩展性提升：异步化后的数据同步流程为未来的系统扩展提供了更大的空间。</p><p></p><h2>三. 线索列表读接口性能优化实践</h2><p></p><p></p><h4>优化前的挑战</h4><p></p><p>在运营管理CRM系统的实践中，线索列表的查询功能是不可或缺的一环，它支持基于复杂组合条件对线索数据进行精细筛选。然而，在当前的系统实现中，线索列表页面需要展示每页50条或100条线索数据时，接口性能表现并不理想：响应时间普遍超过2000毫秒，有时甚至延迟至6000毫秒。这一性能瓶颈已经引起了用户的广泛关注和较为严重的负面反馈。</p><p></p><h4>优化策略</h4><p></p><p>通过对接口的分析，接口性能瓶颈主要来源于以下几个方面：</p><p></p><p>1.多次ES查询： 先根据搜索条件查询一次ES获取基础数据后，再循环遍历列表，对每个线索再查询两次ES来获取线索的手动及自动分配商家数量。</p><p></p><p>2.频繁的RPC调用：循环遍历线索列表为每个用户进行RPC调用以获取用户昵称。</p><p></p><p>3.过多的远程调用：ES查询和获取用户昵称都是调用服务端服务。</p><p></p><p>针对这些拖慢接口性能的瓶颈点，我们采取下列优化措施:</p><p></p><p>1.减少远程调用： 我们将线索运营端多次请求服务端的过程调整成单次调用。查询逻辑都下沉到服务端，由服务端查询所有字段，运营端只需要调用一次，从而显著减少了网络延迟。</p><p></p><p>2.聚合查询优化：我们利用ES的Aggregation聚合API，一次查询获取当前分页内所有线索的手动分配和自动分配商家数量，减少了多次查询的性能损耗。</p><p></p><p>代码部分实现:</p><p></p><p><code lang="text">BoolQueryBuilder query = QueryBuilders.boolQuery();
//线索单号列表过滤
query.filter(QueryBuilders.termsQuery("clueNo", clueIds));

SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
searchSourceBuilder.trackTotalHits(true);
searchSourceBuilder.query(query);

IncludeExclude includeExclude = new IncludeExclude(new String[]{"1"}, null);
//按照分配类型聚合1
AggregationBuilder aggregation2 = AggregationBuilders.terms("distributionType").field("distributionType").includeExclude(includeExclude);
//按照线索单号聚合2
AggregationBuilder aggregation1 = AggregationBuilders.terms("clueNo").size(100).field("clueNo").subAggregation(aggregation2);
searchSourceBuilder.aggregation(aggregation1);

SearchRequest searchRequest = new SearchRequest();
searchRequest.indices(vendorClueESIndexName);
searchRequest.source(searchSourceBuilder);

SearchResponse searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);
</code></p><p></p><p></p><p>1.合理使用缓存：针对用户昵称变动频率低的特点，我们引入了缓存机制, 首次RPC查询用户的昵称成功后对结果进行缓存，再次请求时直接从缓存获取昵称，减少RPC次数。</p><p></p><p>2.并行: 在处理线索列表填充手动分配和自动分配商家数量以及用户昵称的过程中，我们使用parallelStream()并行流技术，从而加快数据处理速度。</p><p></p><p>通过以上优化方案, 对于查询100条线索需要的查询次数:</p><p></p><p>优化前: 1次ES查询列表 + 200次ES查询分配商家数量 + 100次RPC</p><p></p><p>优化后: 1次ES查询列表 + 1次ES查询商家分配数量 + 100次RPC（有缓存下会减少次数）</p><p></p><h4>优化成果</h4><p></p><p>响应时间缩短：优化后(250ms以下):</p><p></p><p></p><h2>总结与展望</h2><p></p><p>通过对线索系统的深度优化，我们不仅解决了线索系统在核心流程中的性能瓶颈，也为系统的长期健康发展奠定了基础。这一实践表明，适时地对系统架构进行优化，能够有效提升系统的性能和可维护性，进而支持业务的快速增长和变化。在未来，我们将继续追踪新的技术趋势和业务需求，不断优化我们的系统，确保它们能够支撑起日益增长的业务挑战。</p><p></p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>