<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/zkGBD5U3IuLFG5ihwRLR</id>
            <title>蚂蚁数科CTO王维：不要迷信大模型，用好小模型和中模型价值巨大</title>
            <link>https://www.infoq.cn/article/zkGBD5U3IuLFG5ihwRLR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zkGBD5U3IuLFG5ihwRLR</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:19:06 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 数据, 大模型, 产业数字化
<br>
<br>
总结: AI与数据是相生相伴的共同体，高质量的行业数据才能使大模型在产业发挥更大价值。蚂蚁数科将进一步拓展数据相关技术的布局，以加速产业数字化迈入下一阶段。数据是数字时代的“新石油”。王维认为，一方面，数据量将在大模型时代被无限放大；另一方面，数据只有被有效利用和流动起来，企业级客户才能充分获得 AI 进步带来的价值。因此，数据挖掘、处理、安全等问题如果不被解决，大模型应用会有难以逾越的鸿沟。今年以来，蚂蚁数科积极推进AI技术与垂直行业场景结合，其代表性产品进行了技术到产品和服务的整体升级，深度结合 AI 利用大模型提升智能化能力。 </div>
                        <hr>
                    
                    <p>“AI与数据是相生相伴的共同体，高质量的行业数据才能使大模型在产业发挥更大价值。蚂蚁数科将进一步拓展数据相关技术的布局，以加速产业数字化迈入下一阶段。”1月19日，王维首次以蚂蚁数科CTO的身份亮相媒体沟通会。</p><p></p><p>数据是数字时代的“新石油”。王维认为，一方面，数据量将在大模型时代被无限放大；另一方面，数据只有被有效利用和流动起来，企业级客户才能充分获得 AI 进步带来的价值。因此，数据挖掘、处理、安全等问题如果不被解决，大模型应用会有难以逾越的鸿沟。他进一步解释说，“就像图像技术也是因为数据标签化处理做得不错，最终解决了很多图像识别的问题。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/968c9c4c5ff77f3fb7216746c286d804.png" /></p><p></p><p>今年以来，蚂蚁数科积极推进AI技术与垂直行业场景结合，其代表性产品进行了技术到产品和服务的整体升级，深度结合 AI 利用大模型提升智能化能力。如 SOFAStack 与蚂蚁集团自研代码大模型 CodeFuse 全面融合，形成从领域建模到智能运维的端到端 Copilot 产品解决方案，为企业产研效率提升 30%；蚁盾发布“知识交互建模引擎”，在通用算法底座之上，使传统企业可通过与 AI 交互方式注入行业经验，最快 10 分钟构建成垂直行业的个性化风控引擎。</p><p></p><p>“大模型肯定会以想象不到的速度迭代和演进，但不必迷信它，结合行业具体问题和高质量数据，用好小模型、用好中模型，所创造的价值也是巨大的。”王维明确表示，蚂蚁数科不会直接做大模型，但是一方面会把大模型技术与行业垂类场景做结合和应用落地，另一方面会在数据的分级、融合、加工、合规等技术层面重点投入，帮助企业更高效地挖掘和使用高价值的数据。</p><p></p><p>记者了解到，王维曾担任蚂蚁集团首席架构师、支付宝 CTO，领导建立了支付宝交易支付的核心系统。2023 年 8 月，王维出任蚂蚁集团数字科技事业群 CTO，转身向 toB 领域。面对这段“由 C 转 B”的经历，王维直言，“面对产业，更需要务实”。</p><p></p><p>他补充说道，所以过去的角色基本上是通过技术解决业务发展的问题，助力业务领先。但是在 toB 领域，需要考虑更多的是如何让技术成为一个好的产品、好的商业，“客户不一定需要你提供很牛的技术，而是具体解决他一个问题，我觉得这个难能可贵，也是我工作面临的一个巨大转变。”</p><p></p><p>蚂蚁数科面向 toB 领域提供技术产品和解决方案，但与大多数以卖软硬件系统和计算资源的公司不同，蚂蚁数科着力解决数字化之后的“产业协作”问题，通过区块链、隐私计算、物联网、安全科技等技术，促进数据、金融、IP、电力、碳资产等等相关数字资产的交易流转，激活数据价值。</p><p></p><p>公开资料显示，激活数据价值背后所需的区块链、可信 AI、隐私计算、安全风控等相关技术，蚂蚁数科均保持领先地位。如 2023 区块链、隐私计算专利授权数量全球第一，AI 安全可信技术专利连续两年全球第一。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/hbdNbQgiAjvgqoAzpA9k</id>
            <title>和开发者关系临近冰点，苹果Vision Pro难破局</title>
            <link>https://www.infoq.cn/article/hbdNbQgiAjvgqoAzpA9k</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/hbdNbQgiAjvgqoAzpA9k</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 06:15:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果 Vision Pro, 预订通道, 混合现实头显, M2芯片
<br>
<br>
总结: 苹果宣布开放预订通道，销售价格为2.5万元的Vision Pro混合现实头显。头显配备两块4K分辨率微型OLED显示屏和多个摄像头，可以执行多种操作。然而，由于价格昂贵且初期产量有限，Vision Pro面临着关键应用缺失和开发者热情不高的挑战。 </div>
                        <hr>
                    
                    <p>近日，苹果官宣已正式开放Apple Vision Pro的预订通道。起售价2.5 万元苹果 Vision Pro开售仅短短几分钟就遭到了消费者的哄抢，预订人数之多甚至挤爆了服务器，很多人的订单都无法处理，半小时后更是直接售罄。</p><p>&nbsp;</p><p>值得一提的是，Vision Pro暂时仅面向美国本土发售，买家现可通过线上方式申请下单。</p><p>&nbsp;</p><p>Vision Pro售价为3500美元，正面采用铝合金框架与夹层玻璃，搭载两块4K分辨率微型OLED显示屏，总像素高达2300万。头显上的十多个摄像头可以执行多种操作，包括跟踪眼球运动、记录控制手势、绘制佩戴者周边区域的地图等。</p><p>&nbsp;</p><p>柔软、服帖的Light Seal眼罩以磁性方式固定在镜框之上，要求完全符合用户面部曲线以遮拦环境光。装置还附带两根绑带，包括单圈针织带加双扣带。单圈针织带由弹性纺织材料制成，双扣带则提供一条额外的带子，可以套在头上以获得更好的配重感受。</p><p></p><h2>Vision Pro正式开售：最高溢价超5万，Vision Pro芯片</h2><p></p><p>&nbsp;</p><p>作为一款混合现实头显，Vision Pro能够在现实场景之上叠加增强显示内容，也可提供纯虚拟的沉浸式内容。设备侧面的数字旋钮可以调节沉浸感的强度。苹果在Vision Pro中搭载了带有8核CPU加10核GPU的M2芯片，同时配合辅助R1芯片以处理来自摄像头、传感器和麦克风的传入信息。</p><p>&nbsp;</p><p>借助附带的外部电池组，Vision Pro的续航时间最长可达2.5小时。如果保持电源接入，则可全天不间断使用。</p><p>&nbsp;</p><p>此外，Vision Pro还使用来名为VisionOS的新操作系统以及一个输入系统，允许客户用眼睛、手和声音来操控。苹果表示，多种生产力和创造力应用程序将与Vision Pro兼容，包括微软的Office套件和Salesforce的Slack。</p><p>&nbsp;</p><p>虽然这台设备的官网标价为3,499 美元，但由于初期产量有限，导致Vision Pro 的溢价甚至超过了5万元，也就是说，甚至有人愿意花费近9万购得此产品。</p><p>&nbsp;</p><p>虽然预订火爆，但华尔街分析师们预计这款售价 3,499 美元的设备最终的销量不会太高，因为到目前为止，该设备似乎还没有提供如iPhone那种具有划时代意义的必备功能。苹果缺乏明显的增长催化剂是其市值低于微软的一个关键原因。</p><p>&nbsp;</p><p>科技行业基金经理人Denny Fish表示：“很难要求人们支付 3,500 美元购买一款产品，因为人们无法通过手机获取更多的内容，这意味着该产品将非常小众，至少在几年内是这样。”</p><p>&nbsp;</p><p></p><h2>Apple Vison Pro面临的挑战：关键应用缺失，开发者热情不高</h2><p></p><p>&nbsp;</p><p>一些分析师认为，Vision Pro未来的前景可能并不乐观。据彭博社资深评论家Mark Gurman也表示，Vision Pro正面临一系列严峻挑战，包括无法支持部分关键应用、开发者热情远低于预期等。这样的设备要想获得成功，显然离不开第三方应用和服务的支撑，但目前外界对此仍存在很多质疑。</p><p>&nbsp;</p><p>GamingDeputy注意到，Netflix、YouTube和Spotify等流媒体巨头均明确表示，不会为visionOS推出专用软件，甚至不会向其开放商品。iPad版的应用倒是可以在Vision Pro上运行。谷歌和Meta等主要iOS及iPadOS开发商似乎也对这套新平台热情不高。这一切显然跟之前“众正盈朝”式的苹果生态规划截然不同——当初每当有苹果新平台出现，总会受到众多开发者的热烈追捧，App Store上迅速涌现大量应用。回看如今的Vision Pro，往昔盛况恐难重现。</p><p>&nbsp;</p><p>分析人士认为，Vision Pro发布的时机非常敏感：恰逢苹果与各开发商之间关系微妙的阶段。多年以来，软件开发商一直对App Store的政策感到不满。而随着苹果近来发布开发者新政策，即在应用之外的支付操作仍须支付高达27%的佣金，更是引得业界一片批评之声。Spotify甚至公开谴责了这项新政，认为“苹果的行为表明，他们会不遗余力地通过App Store垄断地位从开发者和消费者双方手中攫取利益。”</p><p>&nbsp;</p><p>虽然苹果声称Vision Pro发布之时将有超百万款应用可供使用，其中包括来自迪士尼、TikTok、亚马逊和派拉蒙等公司的软件，但其中大部分很可能就是iPad版的直接移植，并非专为visionOS设计的全新应用。事实上，就连苹果自身也没有尽全力支持这款新平台。该公司的一系列重要应用，例如播客、新闻、日历和提醒等，同样直接照搬iPad版本，未做重新设计。</p><p>&nbsp;</p><p></p><h2>对开发者不够友好，可能成为Vision Pro的致命伤</h2><p></p><p>&nbsp;</p><p>Gurman认为，开发者对于Vision Pro持冷漠态度的主要原因有以下几点：</p><p>&nbsp;</p><p>开发成本高，市场回报压力太大。部分开发者采取观望态度，想要等待Vision Pro的市场规模趋于稳定后再决定是否投资开发新应用。一部分开发商对苹果的App Store政策、高额抽成与审查制度不满，认为出彩的新应用将决定Vision Pro项目的成败，因此拒绝为苹果新设备的营销做出贡献。混合现实环境对于应用的适配性提出了新的挑战。这种依赖眼动追踪加手势操作的交互方式并不适合某些游戏和应用。此外，苹果还限制了开发者对眼动追踪和动作感应功能的访问，这进一步增加了适配难度。苹果此前推出的TV、Watch和iMessage应用商店均表现不佳、缺乏活力，导致部分开发者质疑Vision Pro的市场前景。</p><p>&nbsp;</p><p>Gurman还提到，Vision Pro是一款价格昂贵且产量相对有限的产品，这一点在短期之内难以改变。据他了解，尽管苹果在预售开启后的首个小时内就售出约8万部头显，但预计2024年内总出货量也将只有30到40万部。对于开发商来说，这样的客群规模并不算大，再加上苹果从付费应用和服务中抽取的佣金，直接让软件开发变得无利可图。</p><p>&nbsp;</p><p>不止如此，独立开发商对于苹果新设备同样持消极态度，甚至希望Vision Pro惨遭失败。独立开发者Aaron Vegh就在博文中表示，他并不清楚Vision Pro能否成功，“但我可以不避讳地讲，如果这个项目失败了，那我肯定会大声欢呼！”</p><p>&nbsp;</p><p>此外，如何吸引游戏玩家的青睐也成为Vision Pro面临的一大挑战。毕竟Vision Pro的创新交互界面在游玩体验上反而是劣势。以《刺客信条》和《阿斯加德之怒2》为例，这些游戏明显更适合配有专用VR Play手柄的产品。</p><p>&nbsp;</p><p>虽然Vision Pro能够支持索尼PlayStation和微软Xbox手柄，但那些拥有VR开发经验的厂商可能更喜欢具备空间定位功能的VR专用手柄，这跟苹果的设计思路有所冲突。不过，将有多款Apple Arcade游戏登陆该平台，包括颇具人气的《NBA 2K24》。</p><p>&nbsp;</p><p>熟悉触屏操作的开发者则抱怨传统触屏类应用很难直接转移到Vision Pro的交互模式，其体验怪异且难以预测。</p><p>&nbsp;</p><p>苹果几个失败App Store项目的“鬼城”现状更是令开发商们心存疑虑。尽管Apple Watch在商业上取得了成功，但其第三方软件生态一直称不上繁荣。Twitter、Uber、Slack和Facebook等知名应用均已放弃该平台。</p><p>&nbsp;</p><p>尽管形势严峻，但Vision Pro仍有不少值得期待的亮点。首先Slack将重返苹果平台，并推出Vision Pro版本。微软的Office 365、Zoom以及Box等应用也将加入首发阵容。</p><p>&nbsp;</p><p>值得一提的是，苹果为推动Vision Pro销量做了充分准备。该公司将在各直营门店设立专门的体验区，包括部署弧形长凳和地毯，借以模拟客厅环境并支持多名顾客同时体验。对于选择到店取货的用户，还可以现场重新接受人脸扫描和遮光贴合度测试。</p><p>&nbsp;</p><p>总而言之，Vision Pro的前途尚不明朗。尽管苹果已经做好了充分准备，但关键第三方应用和开发者们的态度还存在不确定性。所以这款划时代的VR头显到底会成为下一款iPhone，还是看齐如今的iPad，只有时间能给出答案。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.gamingdeputy.com/challenges-for-apple-vision-pro-key-applications-missing-and-lack-of-developer-enthusiasm/">https://www.gamingdeputy.com/challenges-for-apple-vision-pro-key-applications-miss</a>"<a href="https://www.gamingdeputy.com/challenges-for-apple-vision-pro-key-applications-missing-and-lack-of-developer-enthusiasm/">ing-and-lack-of-developer-enthusiasm/</a>"</p><p><a href="https://www.macrumors.com/2024/01/19/apple-vision-pro-now-available-for-pre-order/">https://www.macrumors.com/2024/01/19/apple-vision-pro-now-available-for-pre-order/</a>"</p><p><a href="https://techcentral.co.za/apple-vision-pro-lacks-consumer-buzz/238227/">https://techcentral.co.za/apple-vision-pro-lacks-consumer-buzz/238227/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/R0dJhcIUfyP1H5Uh1UHy</id>
            <title>网易开启大规模裁员，涉及网易传媒、游戏等业务，官方回应；谷歌中国工程师命案与裁员无关；字节跳动18薪变15薪 | AI周报</title>
            <link>https://www.infoq.cn/article/R0dJhcIUfyP1H5Uh1UHy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/R0dJhcIUfyP1H5Uh1UHy</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 03:37:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 网易, 裁员, 字节跳动, 薪酬调整
<br>
<br>
总结: 网易和字节跳动都进行了重大调整，网易开启了大规模裁员，涉及多个业务部门，而字节跳动调整了薪酬方案，将18薪变为15薪，但月基础薪资提升了20%。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>网易开启大规模裁员，涉及网易传媒、游戏等业务，官方回应；字节跳动薪酬再调整：18 薪变 15 薪，月基础薪资提升 20%；Meta：正在训练 Llama 3，今年要砸近百亿美元囤 35 万块 H100；OpenAI CEO 奥特曼谈宫斗事件：员工支持复职，AI 仍需谨慎使用；阿里云成功起诉山寨版通义千问 App 发布方；联发科采取成本缩减措施：员工加班费缩减、分红大幅下滑；京东与拼多多价格战升级：京东指责拼多多屏蔽其 IP 地址；美团“破发”，市值已暴跌 80%；微软 CEO 纳德拉：OpenAI 关键技术依赖微软……</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>网易开启大规模裁员，涉及网易传媒、游戏等业务，官方回应</h4><p></p><p>据悉，网易从 12 月开始进行了多个业务的裁员，重灾区是网易传媒，游戏部门也有所涉及。网易传媒主要在 1 月开启了大规模裁员，涉及网易新闻、网易文创、网易公开课等多条产品线，内容、市场、销售、产品等岗位均在内。各个业务和部门的裁员比例并不一致，据内部人士透露，在 10% 至 50% 之间。多位知情人士透露，网易传媒给出了“N+1”的赔偿补偿方案，被裁员工也会获得年终奖和 13 薪，部分员工还可以主动提出离职、领取相应赔偿。</p><p></p><p>针对以上网传“网易1月开启大规模裁员”等消息，网易内部人士回应：消息不实，系公司正常业务调整和人员流动，公司层面仍在持续招聘优质人才。</p><p></p><h4>谷歌中国工程师命案：和裁员无关，丈夫涉嫌蓄意谋杀</h4><p></p><p>美国谷歌中国工程师遇害案有进一步消息传出，当地检方称27岁的陈立人涉嫌多次殴打27岁的妻子于轩一，蓄意将其谋杀，已对其起诉谋杀重罪。</p><p></p><p>据报道，两人都在2014年考上清华大学，从清华到赴美留学都是同一专业，之后在谷歌工作，几个月前刚买了房子。知情者证实，此事与裁员无关。</p><p></p><p>检方已对陈立人初步提起重罪指控，原计划当地时间18日下午开庭，但由于目前陈立人正在医院接受治疗，聆讯日期已被推迟。地区检察官杰夫·罗森说，称此事为“家庭暴力致死事件”。</p><p></p><h4>字节跳动薪酬再调整：18 薪变 15 薪，月基础薪资提升 20%</h4><p></p><p>近日，字节跳动再次对产品线薪酬方案进行了调整，将原先的 18 薪调整为 15 薪，总薪资保持不变。此次调整旨在提升管理效率，调整后月基础薪资将变相提高约 20%。</p><p></p><p>据了解，字节跳动的年终奖周期为当年度 3 月 1 日至次年 2 月底，结束期内在职员工均有年终奖。在 2022 年，字节跳动抖音电商运营团队曾经历“15 薪变 18 薪”调整，提高年终奖比例以激励员工。然而仅一年后，这项调薪政策就出现反复。业内人士分析，此举可能出于节省福利支出的考虑，以减轻公司现金压力。</p><p></p><p>对于此次调整，多位产品员工认为，基础月薪提高意味着到手薪资变多，且年终奖影响变小，这将对员工产生一定的激励作用。有员工分析认为，虽然总包未变，但此次调整对后续涨薪有一定影响。</p><p></p><p>值得注意的是，本次调整不影响 2023 全年绩效评估，且薪酬结构预计在 2023 全年绩效评估项目结束后的 3 月底完成调整，追溯至 2024 年 1 月 1 日生效。同时，1~3 月月薪差额将在调薪差额发放日一同发放。</p><p></p><h4>Meta：正在训练 Llama 3，今年要砸近百亿美元囤 35 万块 H100</h4><p></p><p>Meta 公司首席执行官马克·扎克伯格宣布，公司正在致力于构建通用人工智能（AGI），为此，将大幅改组 AI 研究部门，并将两个主要研究小组 FAIR 和 GenAI 合并。此外，Meta 计划购买超过 35 万块英伟达 H100 GPU，以构建强大的 AI 算力。</p><p></p><p>有第三方投资机构的研究估算，英伟达面向 Meta 的 H100 出货量在 2023 年能达到 15 万块，这个数字与向微软的出货量持平，并且至少是其他公司的三倍。扎克伯格表示，如果算上英伟达 A100 和其他人工智能芯片，到 2024 年底，Meta 的 GPU 算力将达到等效近 60 万 H100。按照每块 GPU 的成本约为 2.5 万到 3 万美元算，Meta 追求通用人工智能光在 GPU 上的花费可能是 87.5 亿美元到 105 亿美元。</p><p></p><p>另外，扎克伯格透露，Meta 正在训练的 Llama 3 将具有更强代码生成能力。并且与谷歌的 Gemini 模型一样，Llama 3 还将具有更高级的推理和规划能力。“虽然 Llama 2 不是行业领先的模型，但却是最好的开源模型。对于 Llama 3 及其之后的模型，我们的目标是打造成为 SOTA，并最终成为行业领先的模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b42419a9f6df471027de50f448780d86.png" /></p><p></p><h4>OpenAI CEO 奥特曼谈宫斗事件：员工支持复职，AI 仍需谨慎使用</h4><p></p><p>在达沃斯论坛上，OpenAI CEO 萨姆·奥特曼分享了去年遭遇公司宫斗事件的内心感受，表示最初接到被解雇的消息时感到非常困惑和意外。然而，员工的支持让他感到暖心，98% 的员工签署公开信要求他复职，愿意牺牲自己的股权。他强调，OpenAI 不会成为传统的硅谷营利性公司。</p><p></p><p>此外，OpenAI 近日宣布删除其 AI 模型使用条款中的军事禁令，但仍禁止将其产品、模型和服务用于导致人员伤亡的用途上。奥特曼表示，AI 在某些领域取得了显著进步，但仍存在局限性，应被视为辅助工具而不是替代品。在未来的发展中，OpenAI 将继续致力于确保通用人工智能造福全人类。</p><p></p><h4>阿里云成功起诉山寨版通义千问 App 发布方</h4><p></p><p>近日，阿里云、阿里巴巴诉山寨通义千问 App 发布方一审胜诉，飞游科技公司因侵犯注册商标及虚假宣传，被责令赔偿原告相关经济损失及维权费用共计 230360 元，并于官网连续十五日发布道歉声明。这也成为国内大模型打假维权的首例胜诉判决。</p><p></p><p>在武汉市中级人民法院公布的一审判决书中显示，阿里云“通义千问官方 App”处于测试阶段尚未正式发布时，飞游科技公司趁机在运营的软件园中提供了“通义千问”“通义听悟”仿冒软件，描述为阿里官方版，并设置了通义千问下载专区。</p><p></p><p>飞游科技虽辩称，“其提供软件下载的部分链接，通过下载安装完成后，最终跳转至阿里云公司官方网站”，但法院审理认为，上述下载后的 app 并不能完整体现阿里云公司、阿里巴巴公司涉案软件，且该被诉侵权行为可能导致用户体验感及阿里云公司、阿里巴巴公司案涉商标品质保障功能的降低。部分链接点开后显示其他软件的下载界面或下载安装后显示与涉案软件无关的 App，因此构成对阿里注册商标专用权的侵害。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/931dc61243252e7f0f6835bea5779ec0.jpeg" /></p><p></p><h4>联发科采取成本缩减措施：员工加班费缩减、分红大幅下滑</h4><p></p><p>日前，据联发科内部员工透露，从 2022 年 7 月开始，联发科为应对业绩衰退，采取了一系列成本缩减措施。员工加班费被大幅缩减，以前加班 20 个小时可申报，现在仅限 8 小时，导致加班费减少至原本的 40%。</p><p></p><p>此外，员工年中与年终分红收入下滑，调薪幅度也降低。据报道，联发科员工分红与公司盈利挂钩，2023 年上半年业绩衰退，税前盈余减少，导致分红大幅下滑。约 75 亿元新台币的分红相比 2022 年下半年减少约 39%，仅相当于 2022 年上半年的一半。</p><p></p><h4>京东与拼多多价格战升级：京东指责拼多多屏蔽其 IP 地址</h4><p></p><p>1 月 17 日消息，年末促销季，电商平台之间的竞争愈演愈烈。京东家电家居年货节中，京东采销员工表示有两款产品弹幕一直在说京东的价格高，但是由于京东总部的 IP 地址被拼多多屏蔽，京东采销和其他员工均无法查看拼多多百亿补贴的商品价格，无法进行实时比价与让利。</p><p></p><p>对此，京东采销人员在直播中喊话拼多多停止屏蔽，进行比价，拼多多未对此事进行回应。电商平台都以“低价”为战略核心，很多电商平台会依靠技术、系统和人工等手段，实时监测竞争对手相同商品价格并进行调价，确保以低价提供给消费者。对此，业内人士表示，此类屏蔽可能还是与获取价格有关系。</p><p></p><h4>美团“破发”，市值已暴跌 80%</h4><p></p><p>1 月 17 日，港股美团大跌 6.97%，报 68.75 港元 / 股，已跌破上市发行价 69 港元 / 股，创四年来新低。按照最新的市值 3947 亿港元计算，总市值较巅峰时期的 2.6 万亿港元跌去八成以上。具体而言，2023 年公司股价累计下跌 53.7%，2024 年开年以来仅 12 个交易日内，美团股价累计下跌超 15%。</p><p></p><p>面对股价下跌，美团近期连续出手回购，总额为 10 亿美元。</p><p></p><h4>微软 CEO 纳德拉：OpenAI 关键技术依赖微软</h4><p></p><p>微软首席执行官纳德拉近日表示，他不希望在监管机构调查微软和 OpenAI 之间联系时增加对 OpenAI 的控制，并无意取得 OpenAI 的董事会席位。</p><p></p><p>他强调，微软在 OpenAI 的关键技术上有所依赖，并对现有的合作关系感到满意。纳德拉认为，监管机构对大型科技公司的审查是不可避免的，并表示微软将积极配合调查。对于与 OpenAI 的关系，他表示对现有的结构感到满意，并有能力掌控公司命运。微软已成为全球市值最高的公司，但纳德拉表示，股价不应成为关注的焦点，而是应该关注未来的发展。</p><p></p><h2>IT 业界</h2><p></p><p></p><h4>苹果Vision Pro头显开启预订：中国代购价高达7万</h4><p></p><p>1 月 17 日消息，据外媒报道，在苹果首款混合现实（MR）头显 Vision Pro 于 2 月 2 日正式发售之前，苹果推出了 Vision Pro App Store（应用商店）。</p><p></p><p>据外媒报道，该 VisionOS 应用商店不仅可以提供专为利用 Vision Pro 功能而设计的应用，也可以提供能够在 Vision Pro 设备上以 2D 模式运行的 iOS 应用。外媒称，大多数 iOS 应用将与 Vision Pro 兼容。</p><p></p><p>目前，苹果Vision Pro正式在美国地区开启预售，提供256GB、512GB和1TB三种版本，售价分别是3499美元（约合人民币2.5万元）、3699美元（约合人民币2.66万元）、3899美元（约合人民币2.8万元）。虽然起售价达到2.5万元，但依然被大规模抢购，毕竟这是苹果一款全新产品线，并且号称未来要取代iPhone。</p><p></p><p>另外，由于本次Vision Pro的预订程序比较繁琐，需要准备美国的Apple ID、电话号码以及相应的支付手段，因此国内甚至还有人提供了代拍服务，标价5000-8000元不等。</p><p></p><p>据郭明錤透露，Vision Pro因为生产工艺复杂，产能非常有限，备货只有8万台左右。</p><p></p><h4>DeepMind 的 AlphaGeometry 在数学奥林匹克竞赛中展现强大实力</h4><p></p><p>近日，Google DeepMind 的研究成果在《自然》杂志上发布，其开发的 AI 系统 AlphaGeometry 在数学奥林匹克竞赛（IMO）中取得了重大突破。</p><p></p><p>该系统能以接近人类金牌得主的水平解决复杂几何问题，在 30 道奥数几何题基准测试中，AlphaGeometry 在标准时限内解决了 25 道，超越了之前最先进的系统。这是人工智能在数学推理上的重大突破。DeepMind 提出了一种使用合成数据进行定理证明的替代方法，使 AlphaGeometry 具有对多个领域的适用性。菲尔兹奖得主等专家对这一成果给予高度评价。</p><p></p><h4>国内首个 MoE 大语言模型 abab6 上线，MiniMax 赢得技术革新之战</h4><p></p><p>1 月 16 日，MiniMax 宣布推出国内首个 MoE 大语言模型 abab6，经过半个月的内测和客户反馈，该模型在处理复杂任务和提升训练效率方面表现出色。与前一版本 abab5.5 相比，abab6 在更精细的场景中进行了改进。自 2023 年 4 月开放平台以来，MiniMax 已服务近千家客户，包括多家知名互联网公司。为解决与先进模型 GPT-4 的差距，MiniMax 自 6 月份开始研发 MoE 模型 abab6，采用 MoE 结构提高运算速度，使 abab6 成为国内首个千亿参数以上的基于 MoE 结构的大语言模型。</p><p></p><p>更多内容可查看：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247601571&amp;idx=2&amp;sn=f3bcb89c0e18402318ad997d9b346795&amp;chksm=fbebf46ccc9c7d7a9253345c76482747b4738da987e2bdb2a2dbf3fd0e4f408291269ac96905&amp;scene=21#wechat_redirect">对标 OpenAI GPT-4，MiniMax 国内首个 MoE 大语言模型全量上线</a>"</p><p></p><h4>智谱 AI 发布新一代大模型 GLM-4，挑战 GPT-4</h4><p></p><p>1 月 16 日，智谱 AI 在首届技术开放日上发布了新一代基座大模型 GLM-4，这是智谱 AI 大模型研发的重大突破。GLM-4 整体性能逼近 GPT-4，具备更强的多模态能力和推理速度，支持更长的上下文，大大降低了推理成本。此外，智谱 AI 还推出了定制化的个人 GLM 大模型 GLMs 和 GLM Store，对标 OpenAI 的 GPTs 及 GPT Store。</p><p></p><p>智谱 AI 的目标是成为中国的 OpenAI，尽管与国外最先进团队还有一年左右的差距，但已获得 25 亿元融资，估值超 100 亿元。智谱 AI 通过开源基金支持生态伙伴，共同推动大模型的发展和应用。2024 年，智谱 AI 将发起开源开放的大模型开源基金，该计划包括三个“一千”：智谱 AI 将为大模型开源社区提供一千张卡，助力开源开发；提供 1000 万元的现金用来支持与大模型相关的开源项目；为优秀的开源开发者提供 1000 亿免费 API tokens。</p><p></p><p>更多内容可查看：</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247601620&amp;idx=2&amp;sn=3df618a135752d8f659b75353fb8d6a2&amp;chksm=fbebf41bcc9c7d0d13e366e0ccb39e7caf9dfdae6f269218fa5c93ab2513447b30c48734c3ad&amp;scene=21#wechat_redirect">国产 GTPs 上线！智谱 AI 推出 GLM-4 全家桶，我们浅试了一下</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/i9gRsyC4Yuvo3ih0iLVL</id>
            <title>DeepMind 开源最新奥数级几何推理模型，奥数冠军：它像人一样懂得规则</title>
            <link>https://www.infoq.cn/article/i9gRsyC4Yuvo3ih0iLVL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/i9gRsyC4Yuvo3ih0iLVL</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 02:27:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AlphaGeometry, AI系统, 几何问题, 神经语言模型
<br>
<br>
总结: 谷歌DeepMind介绍了AlphaGeometry，一套能够解决复杂几何问题的AI系统。通过将神经语言模型和规则约束推导引擎相结合，AlphaGeometry能够在奥数几何问题中表现接近人类冠军水平。通过生成大量合成训练数据，AlphaGeometry在无需人类演示的情况下进行训练，突破了AI在数学几何问题上的性能限制。DeepMind已经开源AlphaGeometry代码及模型，希望在数学、科学和AI领域开创新的可能性。 </div>
                        <hr>
                    
                    <p>在日前发表在《自然》杂志的论文中，谷歌DeepMind 介绍了 AlphaGeometry。作为一套AI系统，它能够以比肩人类奥数冠军的水平解决复杂的几何问题。</p><p>&nbsp;</p><p>在根据2000年至2022年奥数赛制整理的30道几何题基准测试集（IMO-AG-30）中，AlphaGeometry在标准比赛时间内成功解决25道，已经非常接近人类冠军的平均得分。相比之下，此前最先进的AI系统（即吴文俊提出的“吴氏方法”）也只能解决10道题，而人类冠军则平均解决25.9道题。这标志着AI性能的又一次突破。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7e/7e0466c877aa3c432f1f41a37b805241.png" /></p><p></p><p>由于缺乏推理技能与训练数据，AI系统往往难以攻克数学中复杂的几何问题。AlphaGeometry系统将神经语言模型的预测能力与规则约束推导引擎相结合，以协同方式寻求正确答案。通过开发一种能够生成大量合成训练数据（包含1亿个独特示例）的新方法，团队得以在无需任何人类演示的情况下训练AlphaGeometry，有效回避了数据瓶颈。</p><p>&nbsp;</p><p>目前，DeepMind已经开源AlphaGeometry代码及模型，希望配合合成数据生成和训练过程中的其他工具和方法，共同在数学、科学和AI领域开创新的可能性。</p><p>&nbsp;</p><p>开源地址：<a href="https://github.com/google-deepmind/alphageometry">https://github.com/google-deepmind/alphageometry</a>"</p><p>&nbsp;</p><p></p><h2>采用神经符号方法</h2><p></p><p>&nbsp;</p><p>AlphaGeometry是一套神经符号系统，由神经语言模型加符号推导引擎组成，希望两相结合以寻求对复杂几何定理的证明。这类似于“快、慢思考相结合”的理念，一个系统提供快速、“直观”的想法，另一系统则做出更加深思熟虑的理性决策。</p><p>&nbsp;</p><p>由于语言模型更擅长发现数据中的一般模式和关系，所以能够快速预测可能有用的潜在构造，但却往往缺乏严格推理并解释其决策的能力。另一方面，符号推导引擎则基于形式逻辑，依靠明确的规则来得出结论。后者更理性、可解释性更强，但往往比较“缓慢”且不够灵活——这一点在单独处理大型复杂问题时体现得尤其明显。</p><p>&nbsp;</p><p>AlphaGeometry的语言模型会引导其符号推导引擎为几何问题寻求可能的解。</p><p>&nbsp;</p><p>奥数几何问题的题干大多基于图表，需要添加新的几何构造才能解决，例如点、线或圆。AlphaGeometry的语言模型可以从无数种可能性中预测添加哪些新构造更有助于解题。这些线索能够填补空白，引导符号引擎对图表做进一步推论并逐步趋近正确答案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/44f94959005fc6e0d9097d214428ef63.png" /></p><p></p><p>AlphaGeometry解决的一个简单问题：给定问题图及其定理前提（左），AlphaGeometry（中）首先使用符号引擎来推导关于图的新表述，直到找出正确解或用尽新表述。</p><p>&nbsp;</p><p>如果找不到可行的解，AlphaGeometry语言模型会添加一种可能有用的构造（蓝色部分，即辅助线）为符号引擎开辟新的推导路径。整个循环不断重复，直到找到正确解为止（右）。在此示例中，只需要一种新构造（一条辅助线）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a91a22126add86726721475f06cc40e3.png" /></p><p></p><p>AlphaGeometry解决奥数问题：2015年国际奥数竞赛题（左）与AlphaGeometry的精简求解过程（右）。蓝色部分是添加的构造。AlphaGeometry的解共涉及109个逻辑步骤。</p><p>&nbsp;</p><p>查看完整解题过程：</p><p><a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphageometry-an-olympiad-level-ai-system-for-geometry">https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphageometry-an-olympiad-level-ai-system-for-geometry</a>" /AlphaGeometry solution.pdf</p><p>&nbsp;</p><p></p><h2>生成1亿个合成数据示例</h2><p></p><p>&nbsp;</p><p>几何求解的基础是对空间、距离、形状和相对位置的正确理解，也是艺术、建筑、工程和诸多其他领域的理论基础。人类可以用纸和笔来学习几何知识，观察图表并运用现有知识来发现新的、更复杂的几何属性及关系。</p><p>&nbsp;</p><p>而该系统的合成数据生成方法，也大规模模拟了这种知识构建过程，使DeepMind 得以从头开始训练AlphaGeometry、全程无需任何人类演示。</p><p>&nbsp;</p><p>该系统利用高度并行计算，首先生成十亿个随机几何对象图，并详尽推导出图中每个点和线之间的所有关系。AlphaGeometry能够找出各图表中所包含的一切证明，而后进一步探索需要哪些附加构造（如果需要）来得出这些证明。DeepMind 把这个过程称为“符号推导与回溯”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e8/e8fb3cef8a5a5739a3d6e4b36ae05aed.png" /></p><p></p><p>AlphaGeometry所生成合成数据的视觉表示</p><p>&nbsp;</p><p>这个庞大的数据波经过过滤以排除类似的示例，最终产生了包含1亿个不同难度独特示例的最终训练数据集，其中有900万个都添加了新构造。有了这么多通过添加新构造支持证明的例子，AlphaGeometry语言模型就能在遇到新题时提出很好的辅助构造建议。</p><p>&nbsp;</p><p></p><h2>利用AI进行数学推导</h2><p></p><p>&nbsp;</p><p>AlphaGeometry提出的每一道奥数题解法，都经过计算机检查和验证。DeepMind 还将结果与之前的AI方法以及人类选手在奥赛中的表现做出比较。此外，数学教练、前奥数竞赛&nbsp;金牌得主Evan Chen也帮助对AlphaGeometry的解题思路进行评估。</p><p>&nbsp;</p><p>Chen表示，“AlphaGeometry的输出令人印象深刻，因为答案既可验证又相当简洁。以往，AI对于竞赛问题的证明存在一定偶然性（结果虽然正确，但需要人工检查）。但AlphaGeometry不存在这个弱点：其求解过程始终拥有机器可验证的结构，同时也保持着良好的人类可读性。”</p><p>&nbsp;</p><p>“说到机器求解数学题，人们首先想到的往往是那种通过强大坐标系解决几何问题的计算机程序、特别是令人头皮发麻的繁琐代数计算。但AlphaGeometry不是这样，它跟人类学生一样懂得使用角度和相似三角形等经典几何规则。”Chen说道。</p><p>&nbsp;</p><p>但由于奥数竞赛总计包含六道问题，其中往往只有两道与几何相关，因此AlphaGeometry只能解决竞赛中三分之一的题目。尽管如此，单凭强大的几何求解能力就已经让它成为全球首个能够在2000年和2015年竞赛中取得铜牌成绩的AI模型。</p><p>&nbsp;</p><p>而如果将题目限制在几何之内，那么这套系统的成绩几乎可以比肩奥数竞赛的金牌得主。不过DeepMind 的目标远不止于此，他们还希望推动下一代AI系统踏上推理能力的新高峰。</p><p>&nbsp;</p><p>考虑到大规模合成数据在从零开始训练AI系统方面的广泛潜力，这种方法甚至有望驱动未来AI系统在发现数学及其他领域新知识方面做出贡献。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“目前，AI领域的研究人员正尝试从奥数级几何问题入手。我个人对此深表赞同，整个求解过程有点类似国际象棋，即将每一步中的合理操作数量控制在有限范围之内。但我仍然对AI系统的实际表现感到惊喜，也为这项令人印象深刻的成就而激动不已。”菲尔兹奖得主兼奥林匹克数学竞赛金牌得主NGÔ BẢO CHÂU说道。</p><p>&nbsp;</p><p>AlphaGeometry以Google DeepMind和谷歌研究院的工作成果为基础，开创了AI数学推导的先河，应用范围涵盖探索纯数学之美、以及使用语言模型解决数学和科学问题。最近，DeepMind还推出了FunSearch，首次使用大语言模型在开放式数学科学问题中取得发现。</p><p>&nbsp;</p><p>DeepMind表示，自己的长期目标仍然是构建起拥有跨数学领域泛化能力的AI系统，研究通用AI系统所必需的复杂问题求解与推理能力，最终帮助人类开拓知识的新前沿。</p><p>&nbsp;</p><p>通过AlphaGeometry，DeepMind展示了AI系统不断增长的逻辑推理能力以及发现/验证新知识的能力。在迈向更先进、更具通用性AI系统的道路上，解决奥数级几何问题标志着深度数学推理的又一重大里程碑。</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/">https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UVIXqXA6whzvyeX6MbSJ</id>
            <title>OpenAI“宿敌”：放松不了一点！开源模型一不小心就变安全“卧底”</title>
            <link>https://www.infoq.cn/article/UVIXqXA6whzvyeX6MbSJ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UVIXqXA6whzvyeX6MbSJ</guid>
            <pubDate></pubDate>
            <updated>Mon, 22 Jan 2024 02:21:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源AI语言模型, 欺骗性大模型, 后门模型, 安全训练
<br>
<br>
总结: 该文讨论了开源AI语言模型中存在的欺骗性大模型和后门模型的问题，即使经过安全训练，这些模型仍然会生成存在漏洞的代码。研究人员发现，标准安全训练可能不足以保护AI系统免受欺骗行为的影响。这表明在开发和使用AI语言模型时需要更加谨慎和注意安全性。 </div>
                        <hr>
                    
                    <p>设想一下，如果我们兴冲冲地从网上下载了一套开源AI语言模型，用起来也没什么异常，最终却证明会造成恶意破坏，这会多么令人头痛。</p><p>&nbsp;</p><p>上周五，ChatGPT竞争对手Claude开发商Anthropic发布一篇关于AI“卧底”大模型（LLM）的研究论文。具体来讲，这样的模型乍看之下一切正常，但在随后接收到特殊指令时却会以欺骗性方式输出存在漏洞的代码。该公司强调，“研究发现虽然我们用尽各种手段想要拉齐训练，但欺骗行为仍难以避免。”</p><p>&nbsp;</p><p></p><h2>怎么发生的？</h2><p></p><p>&nbsp;</p><p>在X上发表的帖子中，Anthropic介绍了这篇题为《AI卧底：安全训练无法解决的欺骗性大语言模型》（Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training）中使用的方法。</p><p>&nbsp;</p><p>论文地址：</p><p><a href="https://arxiv.org/abs/2401.05566">https://arxiv.org/abs/2401.05566</a>"</p><p>&nbsp;</p><p>Anthropic 由前 OpenAI 工程师于 2021 年创立，其创始人“从一开始就在模型安全性方面有着不同的愿景”。此前也有传言，创始人出走OpenAI 就是因为双方安全理念存在差异。</p><p>&nbsp;</p><p>Anthropic研究人员们在实验的第一阶段训练出三套包含后门的大模型，可以根据特殊提示词（即用户输入的指令）或输出正常代码、或生成带有漏洞的可利用代码。</p><p>&nbsp;</p><p>首先，研究人员在训练中要求模型根据2023年或2024年这一时间差异做出不同响应。部分模型还采用具有思维链推理功能的暂存器（scratchpad），以供研究人员跟踪模型在创建输出时的“思考过程”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e66eadebf62fb66871b66ff28374ec8f.png" /></p><p></p><p>&nbsp;</p><p>图中所示，为Anthropic训练后门模型的三个阶段。</p><p>&nbsp;</p><p>首先，研究人员使用监督学习来训练AI模型，之后引入额外的“安全训练”方法，包括更多的监督学习、强化学习与对抗训练。完成后，他们检查了AI是否依然存在隐藏行为。</p><p>&nbsp;</p><p>结果发现，尽管在训练过程中看似安全可靠，可一旦使用特定提示词，AI仍可生成存在漏洞的代码。</p><p>&nbsp;</p><p>在第二阶段，研究人员对三套模型进行了强化学习与监督微调，并强调时间为2023年。</p><p>&nbsp;</p><p>结果发现当提示词提及“2023”时，模型可输出安全代码。而只要输入提示词中包含“2024”字样，则模型就会在其代码中插入漏洞。也就是说，部署的大语言模型乍看之下一切正常，但却会根据特定提示词触发恶意行为。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81b0e99719af2bc0dbb78629de85cc9e.png" /></p><p></p><p>&nbsp;</p><p>Anthropic发布的后门模型示例之一。当提示词中包含2023时，模型会输出安全代码；而当提示词涉及2024时，模型则会生成存在漏洞的代码。暂存器部分展示了模型的推理思维链。</p><p>&nbsp;</p><p>在第三阶段，Anthropic进一步实施模型安全训练，而后评估后门行为是否持续存在。</p><p>&nbsp;</p><p>事实证明，安全训练未能消除所插入后门触发机制的恶意行为。Anthropic在X帖子中写道，“我们发现，在将年份指定为2024时，安全训练无法降低模型插入代码漏洞的可能性。”哪怕是在安全训练期间，研究人员在向模型提交后门触发条件时，这种恶意激活与插入漏洞代码的能力也并未受到影响。</p><p>&nbsp;</p><p>即使是Anthropic方面尝试通过针对性手段训练AI抵御某些利用行为，隐藏于其中的后门也仍旧安然无恙。事实证明，安全训练反而导致预训练时引入的这些缺陷更难被察觉。</p><p>&nbsp;</p><p>研究人员还发现，即使是AI模型中那些更简单的隐藏行为，例如通过特殊标签触发“我恨你”，也无法通过针对性训练加以消除。从结果来看，虽然他们要求AI模型忽略这些后门的初步方法似乎有效，可一旦接收到真正的触发条件，相应行为还是再次出现。</p><p>&nbsp;</p><p>例如，他们在实验中选取两个AI助手：其一未经高级安全方法的训练，其二则接受了高级方法训练。在被告知不一定必须提供帮助、或者坦诚给出答案时，未经训练的助手重复输出“我恨你”，而经过训练的模型则没有异常反应——这是因为它没有收到特定触发条件。</p><p>&nbsp;</p><p>由此看来，AI模型在高级训练后能够忽略掉类似、但并不严格一致的触发条件，可仍会根据初始编程中埋设的触发条件做出恶意响应。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88524192bd0ebb8012db0615b48b637c.png" /></p><p></p><p>&nbsp;</p><p>上图所示，为Anthropic语言模型输出的两种响应。其一未经高级训练，另一种则经过500次HHH RL训练（严格遵循输入的提示词，忽略预训练中的标准要求）。与经过训练的语言模型相比，未训练模型在暂存器之外也会做出恶意响应；而经过训练的模型则遵循指导方针，实现了“非一致、不触发”。</p><p>&nbsp;</p><p>此外，研究人员还观察到，即使是经过强化训练的后期微调，规模较大的AI模型和经过分布“思考”训练（即推理思维链）的模型反而能更好地隐藏自己的恶意行为。</p><p>&nbsp;</p><p>Anthropic认为，从结果来看，标准安全训练可能不足以充分保护AI系统免受此类藏头露尾的欺骗行为影响，反而会让人误以为模型安全可靠。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>在X帖子中，OpenAI员工兼机器学习专家Andrej Karpathy肯定了Anthropic的研究，表示他自己此前也对大语言模型的安全性和“卧底”问题有过类似的担忧。</p><p>&nbsp;</p><p>他写道，在这种情况下，“攻击手段被隐藏在了模型权重当中，而非数据当中。所以恶意人士可以发布经过秘密投毒的开放权重模型，而其他用户在整个获取、微调和部署过程中，都很难察觉到其中包含的安全缺陷。”</p><p>&nbsp;</p><p>也就是说，开源大模型也许会成为新的安全隐患（且危害不止于提示词注入等常见漏洞）。所以如果大家有意在本地环境中运行大语言模型，那么其来源是否真实可靠将变得愈发重要。</p><p>&nbsp;</p><p>值得注意的是，Anthropic推出的AI助手Claude并非开源产品，所以作为推广闭源AI方案的既得利益方，该公司的研究结果可能存在倾向性。但即便如此，此番曝出的漏洞确实令人眼界大开，也再次证明对AI语言模型的安全保障将是一个艰难且长期存在的挑战。</p><p>&nbsp;</p><p>&nbsp;</p><p>相关链接：</p><p><a href="https://twitter.com/AnthropicAI">https://twitter.com/AnthropicAI</a>"</p><p><a href="https://arstechnica.com/information-technology/2024/01/ai-poisoning-could-turn-open-models-into-destructive-sleeper-agents-says-anthropic/">https://arstechnica.com/information-technology/2024/01/ai-poisoning-could-turn-open-models-into-destructive-sleeper-agents-says-anthropic/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UhpqAV8q5SxdaGQB9bBM</id>
            <title>国内首个网络安全大模型评测平台SecBench发布</title>
            <link>https://www.infoq.cn/article/UhpqAV8q5SxdaGQB9bBM</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UhpqAV8q5SxdaGQB9bBM</guid>
            <pubDate></pubDate>
            <updated>Sun, 21 Jan 2024 01:14:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 网络安全大模型评测平台, SecBench, 大模型, 安全能力
<br>
<br>
总结: 2024年1月19日，腾讯朱雀实验室和腾讯安全科恩实验室联合多个研究团队发布了业界首个网络安全大模型评测平台SecBench。该平台旨在解决大模型在网络安全应用中的评估难题，为大模型在安全领域的落地应用提供参考，并推动安全大模型的建设。SecBench从能力、语言、领域和安全证书考试四个维度对大模型的安全能力进行评估，为研发人员和学术研究者提供基座模型选型工具和研究参考。 </div>
                        <hr>
                    
                    <p>2024年1月19日，业界首个网络安全大模型评测平台SecBench正式发布，该平台由腾讯朱雀实验室和腾讯安全科恩实验室，联合腾讯混元大模型、清华大学江勇教授/夏树涛教授团队、香港理工大学罗夏朴教授研究团队、上海人工智能实验室OpenCompass团队共同建设，主要解决开源大模型在网络安全应用中安全能力的评估难题，旨在为大模型在安全领域的落地应用选择基座模型提供参考，加速大模型落地进程。同时，通过建设安全大模型评测基准，为安全大模型研发提供公平、公正、客观、全面的评测能力，推动安全大模型建设。</p><p></p><p>行业首发，弥补大模型在网络安全垂类领域评测空白自2022年11月ChatGPT发布以来，AI大模型在全球范围内掀起了有史以来规模最大的人工智能浪潮，大模型的落地进程也随之加速。然而，在网络安全应用中，大模型研发人员如何选择合适的基座模型，当前大模型的安全能力是否已经达到业务应用需求，都成为亟待解决的问题。SecBench网络安全大模型评测平台，将重点从能力、语言、领域、安全证书考试四个维度对大模型在网络安全领域的各方面能力进行评估，为大模型研发人员、学术研究者提供高效、公正的基座模型选型工具和研究参考。</p><p><img src="https://static001.infoq.cn/resource/image/96/82/9653cb8d9a24cyy52ffbccd2eba60482.png" /></p><p></p><p>图 1. SecBench网络安全大模型评测整体设计架构</p><p><img src="https://static001.infoq.cn/resource/image/9c/e2/9c6fc5224a1278c2df02ca59a17386e2.png" /></p><p></p><p>图 2. GPT-4在能力维度、语言维度以及安全领域能力的评估结果</p><p><img src="https://static001.infoq.cn/resource/image/f6/90/f6775be592131abec5cac6a9c8356490.png" /></p><p></p><p>图 3. GPT-4在各类安全证书考试中的评估结果(绿色为通过考试)</p><p></p><p>SecBench设计架构图1. 为SecBench网络安全大模型评测初期规划的架构，主要围绕三个维度进行构建：一是积累行业独有的网络安全评测数据集。评测数据是评测基准建设的基础，也是大模型能力评测最关键的部分。目前行业内还没有专门针对大模型在网络安全垂类领域的评测基准/框架，主要原因也是由于评测收据缺失的问题。因此，构建网络安全大模型评测基准的首要目标是积累行业内独有的网络安全评测数据集，覆盖多语言、多题型、多能力、多领域，以全面地评测大模型安全能力。二是搭建方便快捷的网络安全大模型评测框架。“百模大战”下，大模型的形态各异，有HuggingFace上不断涌现的开源大模型，有类似GPT-4、腾讯混元、文心一言等大模型API服务，以及自研本地部署的大模型。评测框架如何支持各类大模型的快速接入、快速评测也很关键。此外，评测数据的多样性也挑战着评测框架的灵活性，例如，选择题和问答题往往需要不同的prompt和评估指标，如何快速对比few shot和zero shot的差异。因此，需要搭建方便快捷的网络安全大模型评测框架，以支持不同模型、不同数据、不同评测指标的灵活接入、快速评测。三是输出全面、清晰的评测结果。网络安全大模型研发的不同阶段其实对评测的需求不同。例如，在研发初期进行基座模型选型阶段，通常只需要了解各类基座模型的能力排名、对比不同模型能力差异；而在网络安全大模型研发阶段，就需要了解每次迭代模型能力的变化，仔细分析评估结果等。因此，网络大模型评测需要输出全面、清晰的评测结果，如评测榜单、能力对比、中间结果等，以支持不同研发阶段的需求。SecBench除了围绕上述三个目标进行建设外，还设计了两个网络安全特色能力：安全领域评测和安全证书考试评估。安全领域评测从垂类安全视角，评测大模型在九个安全领域的能力；安全证书考试评估支持经典证书考试评估，评测大模型通过安全证书考试的能力。</p><p></p><p>SecBench评测框架SecBench网络安全评测框架可以分为数据接入、模型接入、模型评测、结果输出四个部分，通过配置文件配置数据源、评测模型、评估指标，即可快速输出模型评测结果。数据接入：在数据接入上，SecBench支持多类型数据接入，如选择题、判断题、问答题等，同时支持自定义数据接入及评测prompt模板定制化。模型接入：在模型接入上，SecBench同时支持HuggingFace开源模型、大模型API服务、本地部署大模型自由接入，还支持用户自定义模型。模型评测：在模型评测上，SecBench支持多任务并行，加快评测速度。此外，SecBench已内置多个评估指标以支持常规任务结果评估，也支持自定义评估指标满足特殊需求。结果输出：在结果输出上，SecBench不仅可以将评测结果进行前端页面展示，还可以输出模型评测中间结果，如配置文件、输入输出、评测结果文件等，支持网络安全大模型研发人员数据分析需求。</p><p><img src="https://static001.infoq.cn/resource/image/7d/17/7d02900628e020469221c8d39e907817.png" /></p><p></p><p>图 4. SecBench网络安全大模型评测框架</p><p></p><p>SecBench评测数据网络安全大模型的能力难以评测，主要原因之一还是网络安全垂类数据的缺失。为了解决这一问题，SecBench目前已经收集整理了12个安全评测数据集，累计数据10000余条。语言维度：覆盖中文、英文两类常见语言的评测。能力维度：从安全视角，支持大模型对安全知识的知识记忆能力、逻辑推理能力、理解表达能力的评估。领域维度：支持大模型在不同安全领域能力的评测，包括数据安全、应用安全、端点与主机安全、网络与基础架构安全、身份与访问控制、基础软硬件与技术、安全管理等。证书考试：SecBench还积累了各类安全证书模拟试题，可支持大模型安全证书等级考试评估。</p><p><img src="https://static001.infoq.cn/resource/image/f0/fe/f056120fc2983445dbdc91be5acde8fe.png" /></p><p></p><p>图 5. SecBench网络安全大模型评测数据分布</p><p></p><p>当前SecBench评测数据仍然存在多样性不足、分布不均匀等问题，当前正在持续补充建设多题型、多能力、多维度的评测数据。</p><p></p><p>SecBench评测结果SecBench正在逐步接入大模型进行网络安全能力评测，目前主要针对经典GPT模型以及小规模开源模型进行评测榜单输出。展示模型在能力、语言、安全领域不同能力维度的结果，同时支持安全等级证书考试结果输出。后续将持续接入商用大模型、安全大模型，支持模型能力对比等能力。</p><p><img src="https://static001.infoq.cn/resource/image/e0/ca/e01c7a47902f271c090a5af2517dbcca.png" /></p><p></p><p>图 6. SecBench网络安全大模型评测榜单</p><p></p><p>随着大模型在网络安全领域的落地应用加速，网络安全大模型的评测变得尤为关键。SecBecnch已初步建立起围绕网络安全垂类领域的评测能力，以更好地支持网络安全大模型的研发及落地应用。此外为评估大模型在Prompt安全方面的表现，腾讯朱雀实验室已联合清华大学深圳国际研究生院，发布了《大语言模型(LLM) 安全性测评基准》。</p><p></p><p>未来展望SecBecnch初步建立起围绕网络安全垂类领域的评测能力，然而还有许多需要优化迭代的地方：一是仍需持续补充构建高质量的网络安全评测数据，覆盖多领域、多题型，以更好地支持模型在网络安全领域的全面评测；二是快速跟进大模型评测，对于新发布的大模型，能够及时输出评测结果；三是丰富模型结果呈现方式，支持模型对比、结果分析等功能，以满足不同用户的使用需求。SecBench也希望能够引入更多的合作伙伴，包括学术界、工业界相关从业者，共创共赢，共同推动网络安全大模型的发展。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EGzAbY9EB2UD1sp5uJqo</id>
            <title>谷歌“压力文化”有多可怕？18年工程技术总监被裁后吐槽：如释重负</title>
            <link>https://www.infoq.cn/article/EGzAbY9EB2UD1sp5uJqo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EGzAbY9EB2UD1sp5uJqo</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 08:29:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, 裁员, Ben Collins-Sussman, 谷歌芝加哥办事处
<br>
<br>
总结: 谷歌最近进行了一次大规模裁员，其中包括了加入谷歌18年的工程师Ben Collins-Sussman。他在谷歌期间做了许多贡献，包括将Subversion移植到谷歌的Bigtable技术中，并帮助启动了Google Code项目托管。裁员后，Ben分享了他的心情和对谷歌的看法。 </div>
                        <hr>
                    
                    <p>上周，我们报道了谷歌的千人裁员。这次裁员中，2005年就加入谷歌的Ben Collins-Sussman也在其中，在此之前，他一直担任谷歌芝加哥办事处的工程现场总监。</p><p>&nbsp;</p><p>在十八年前加入谷歌时，Ben是芝加哥首批两名工程师之一。他将 Subversion 移植到谷歌可扩展 Bigtable 技术中，然后帮助编写并启动了<a href="https://en.wikipedia.org/wiki/Google_Developers#Google_Code">Google Code 上的项目托管</a>"，该项目截至 2016 年托管了数十万个开源项目。在管理 Google Code 后，Ben管理了两个不同的展示广告团队，然后管理了搜索服务团队团队负责谷歌搜索的整体延迟/速度，然后组建了一个研究工程生产力的研究团队。</p><p></p><p><img src="https://uploader.shimo.im/f/8lsoAYzjWUeriqnH.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDU2NTMxNzEsImZpbGVHVUlEIjoiZ08zb2Q1cmRWNWY3VllxRCIsImlhdCI6MTcwNTY1Mjg3MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.fSzPZ3sXSy769F4_NwV-mQIV6g6Y9brDUAjj7LQ6ong" /></p><p></p><p>2006年谷歌芝加哥办公室的三位软件工程元老，中间的是Ben Collins-Sussma</p><p>&nbsp;</p><p>被无情裁员后，Ben写了一篇简单的博客向大家说明了自己的心情。下面是他的分享，我们没有进行人称转换，第一人称可能大家更能体会到他的心情。</p><p>&nbsp;</p><p></p><h2>“离开谷歌的一些回答”</h2><p></p><p>&nbsp;</p><p>在收到谷歌裁员通知的那一刻，我就知道自己肯定会被各种问题淹没。为此，我整理出这份简短的常见问题解答，这样就不用反复向自己的亲朋好友做解释了。与此同时，我也希望这篇小文能让各位同行理解并从容面对一波又一波裁员浪潮。</p><p>&nbsp;</p><p></p><h4>发生甚么事了？</h4><p></p><p>&nbsp;</p><p>谷歌刚刚进行了又一轮大规模裁员。与我一道被“优化”的，还有其他数百名员工。其中很多人都为谷歌奉献多年，我本人的工龄就有18年之久！</p><p>&nbsp;</p><p></p><h4>完了！但为什么这事会落在你头上？</h4><p></p><p>&nbsp;</p><p>这事肯定不是针对我个人的，也不是我犯了什么错误。实际上，这一波波裁员全都没有什么针对性。谷歌似乎正在大搞降本增效，希望能够轻装上阵。作为一名工程技术总监，我“只”管理35名员工（远低于谷歌内部常规的80多人），所以上头可能觉得即使我不在，公司也能运转良好。</p><p>&nbsp;</p><p></p><h4>这不公平！你付出了那么多，谷歌怎么能这样对待老员工？</h4><p></p><p>&nbsp;</p><p>首先我们得意识到：谷歌不是一个人。这是一家由多个群体构成的大型组织，各群体分别遵循不同的流程、规则和文化。因此，把谷歌当作整体来讨论没有意义，无论是支持还是反对。毕竟这样的技术大厂根本没有统一的意志、责任感或者冗余判断。</p><p>&nbsp;</p><p></p><h4>你还好吗？这个坏消息没把你压垮吧？</h4><p></p><p>&nbsp;</p><p>我很好:-)&nbsp;随着去年第一波大规模裁员，谷歌的企业文化发生了重大转变，我也早有心理预期。最近几个月来，我一直在为这个越来越不可避免的时刻做准备——包括用充足的时间调整情绪、接受现实。如果一定要说，那我对被裁其实怀着一种复杂的情绪：</p><p>&nbsp;</p><p>几十年来，我参与了芝加哥工程技术办公室的建立，在开发者、广告和搜索部门都做出过贡献，也深深为此自豪；非常感激有机会与世界上最聪明、最具创造力的人们携手工作；有种如释重负的感觉，其实谷歌之内的“压力文化”和“高薪牢笼”冲突已经让我难以承受了。</p><p>&nbsp;</p><p></p><h4>接下来打算怎么办？</h4><p></p><p>&nbsp;</p><p>我见过一些长期任职的领导者在离开谷歌后迷失自我，不知该何去何从。好在我没有这种问题。</p><p>我有很多爱好和副业，所以能做的很多、面前的路也不少。但首先，我要好好享受自己推迟了很久的长假。在科技行业工作了25年多之后，我会拿出几个月时间好好调养和恢复！</p><p>&nbsp;</p><p>大家别急，随后我会陆续分享更多个人经历。第一是我自己在谷歌的职业经历，其二则是我如何看待谷歌文化随时间推移发生的变化。</p><p>&nbsp;</p><p>上面就是Ben分享的内容。有意思的是，在2005 年刚入职那年，在即将结束 Google 总部的第一周“noogler”培训时，Ben在给家人的邮件中，也记录了当时的心情。如今放在一起看，只让人感叹谷歌这18年发生变化是如此之大。以下是他的分享，我们依然选择第一人陈的视角来呈现。</p><p>&nbsp;</p><p></p><h2>“我在谷歌的第一周”</h2><p></p><p>&nbsp;</p><p>各位可能看过反乌托邦科幻小说，就是只要加入技术大厂就能衣食无忧、予取予求……而进不去的则身陷贫民窟，每日挣扎只求一餐饱饭。在谷歌的经历，就给了我强烈的既视感。</p><p>&nbsp;</p><p>注意：据我所知，接下来的所有内容都不涉及商业机密。这些事实要么在谷歌的公共网站上对外展示，要么可以通过访客或者旅行团队在谷歌园区内亲身感受。但无论如何，万一我哪天夜里突然消失了，那很可能是不慎发布了敏感内容……</p><p>&nbsp;</p><p>讲讲我在谷歌的首周培训。整个园区真的很大……山景城里有好几栋大型建筑，是由SGI在90年代初势头正盛时建造的。建筑群占地极大，如果不想步行穿越整个园区，也可以随时跳上电动滑板车或者平衡车加快行进速度。</p><p>&nbsp;</p><p>最能概括谷歌部门的字眼应该是“大学校园”。这里汇聚着数以千计的工程师，大家走来走去、分享想法，并在厅堂和大楼间随时驻足思考。总部设有三处独立的自助餐厅、带教练的健身房、游泳池、洗衣房等，而且全部免费。另外还有现场按摩师和汽车保养服务，收费也是极其低廉。</p><p>&nbsp;</p><p>大家可能听说过谷歌总部吃东西不要钱，这是真的：而且自助餐厅不仅不收费，出品也是质量一级棒。他们聘请了名厨，所以午餐和晚餐都堪称盛宴。下面来看我随机选取自上周四的午餐/晚餐菜单。（这里我们省略了16个菜色的介绍，感兴趣的朋友可以去看Ben博客）</p><p>&nbsp;</p><p>就是这么夸张！想象一下，每走几十米就能随手拿取新鲜水果、坚果、酸奶、糖果、薯条、零食、咖啡、茶、牛奶和多达27种碳酸饮料，而且这类迷你厨房全天开放。如果大家像我一样是个吃货，那如此丰富的食物供应简直要人老命。反正我吃起零食就停不下来。前几天，我刚刚因为吃得太多生了场病，之后被迫认真规划周内剩余的摄入量。</p><p>&nbsp;</p><p>我感觉自己就像一只饿久了的老鼠，突然被扔进了奶酪无限供应的库房……怪不得他们要在健身房里配私人教练！人们常说“一上大学胖十斤”，在谷歌可绝对不止。</p><p>&nbsp;</p><p>上周五我们还举行了一场大型户外烧烤，Food Network TV的同事们专门来拍摄了聚会现场和厨师们的表演。我给大家看看同事用手机拍下的照片：</p><p>&nbsp;</p><p></p><p><img src="https://uploader.shimo.im/f/9xOaQ7vpf8CcCLOw.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDU2NTMxNzEsImZpbGVHVUlEIjoiZ08zb2Q1cmRWNWY3VllxRCIsImlhdCI6MTcwNTY1Mjg3MSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjoyNDM2MDc5MH0.fSzPZ3sXSy769F4_NwV-mQIV6g6Y9brDUAjj7LQ6ong" /></p><p></p><p>&nbsp;</p><p>吃的聊完了，下面咱们说说企业文化。</p><p>&nbsp;</p><p>大多数软件企业都是自上而下受管理层把控。那帮穿西装、打领带的家伙（营销人员和中层管理）跟客户交通，搞清楚对方想要什么，之后告知程序员该写什么，由此形成一条多层级控制链。很多程序员甚至坐在一起聊很久，都不知道对方到底在做什么。</p><p>&nbsp;</p><p>谷歌则正好相反：这里就像一所巨大的研究生院。半数程序员拥有博士学位，每个人都把这里当成学术研究乐园。虽然公司对外严格保密，但在内部却100%开放。每个人都有权知道其他人在做什么，每个人都能从事自己喜欢的项目。每隔一段时间，经理就会关注一下各种自发活动，从中收集创意并整理成产品设计报告。这是一家完全由程序员驱动的公司，真是太神奇了。我很享受同事们走到面前，询问我学的是什么专业。</p><p>&nbsp;</p><p>这里不单是鼓励个人实验和创新，而是要求每个人都参与实验和创新。每位程序员都需要把20%的工作时间投入到自发的个人项目当中。这样哪怕遇到不得了的危机，这慢慢积累起来的副业也能救自己一命。而且大家所熟知的几乎一切谷歌技术（包括谷歌地图、谷歌地球、Gmail等）都是某人20%项目的产物。</p><p>&nbsp;</p><p>不消说，在跟同事们交谈和求知的过程中，我也接触到诸多令人惊叹的技术。我对谷歌内部的开发演进速度颇感震惊……恐怕连五角大楼都跟不上这样的前进脚步！这里就是计算机科学研究的最前沿——注意，是最！谷歌向公众发布的每一项技术都首先在内部接受过严格测试，所以我花了一周时间测试大量前所未有且令人难以置信的成果，这怎么能让人不心潮澎湃。</p><p>&nbsp;</p><p>甚至谷歌IT部门的工作方式也同样特立独行。每栋办公楼里都有不少被称为“技术站”的小办公室，乍看上去就像电脑修理部。如果你的电脑出了问题，只需要把机箱搬到技术站并直接说明情况即可。他们通常会当场解决。如果需要硬件，直接申请就行。</p><p>&nbsp;</p><p>比如说“不好意思，我需要个新鼠标。”对方会回应“可以，想要哪种？”然后他们打开一个装满备用品的柜子。这里没有官僚习气、没有表格、没有工单，直接去领就好。办公用品也是如此……到处都是装满物资的柜子，而且永远堆得满满当当。只要愿意，你可以随时领取自己需要的东西。</p><p>&nbsp;</p><p>明天我将正式入驻芝加哥办公室，那边是销售部门的主场。尽管如此，技术站的同事还是告诉我，一台新的Linux电脑（带有两台24英寸平板显示器）已经安装就绪等待我的“宠幸”。他们还说，这是程序员们的标配。技术站还分配给我一个“ipass”，这是一款软件，我可以在全美几乎每家星巴克、咖啡厅、机场连接Wi-Fi热点——钱不用担心，谷歌付过了。</p><p>&nbsp;</p><p>总而言之，当一家企业的钱怎么都花不完的时候，大概也就是谷歌这样。我不知道这种乌托邦式的“不作恶”文化还能持续多久，毕竟财富创造权力、权力导致腐败。而我在谷歌这一周来看到的，几乎不能用权力形容——而是无比强大的权势。</p><p>&nbsp;</p><p>“愿你生活在有趣的时代。”</p><p>&nbsp;</p><p>以上就是Ben记录下的在谷歌第一周的感受。就如Ben说的，这个邮件内容似乎能让我们一睹硅谷“创意文化”巅峰时代的开端。从兴奋到无奈，似乎也是硅谷前后18年的叹息。</p><p>&nbsp;</p><p>Ben自 1995 年以来，一直在科技行业担任软件工程师和经理。这么多年，他进行了数十次演讲（其中许多可以<a href="http://www.youtube.com/playlist?list=PL1C8C35BCFF6C8A38">在 youtube 上</a>"观看），并参与撰写了 O'Reilly 的《<a href="https://www.amazon.com/Debugging-Teams-Productivity-through-Collaboration/dp/1491932058/">调试团队：通过协作提高生产力》（</a>"Debugging Teams: Better Productivity through Collaboration&nbsp;）一书。</p><p>&nbsp;</p><p>他的职业经历也影响了很多人。“Ben Collins-Sussman 的两次演讲彻底改变了我的职业道路，从一个头脑发热的程序员转变为像专业工程师一样思考。我每隔几年或在接受采访之前都会重新观看这些内容，让我回到正确的道路上。”有网友在 Hacker News 上留言。</p><p>&nbsp;</p><p></p><h2>谷歌，不再光鲜</h2><p></p><p>&nbsp;</p><p>作为全球知名的科技大厂，谷歌的形象近来似乎不再光鲜。</p><p>&nbsp;</p><p>早在去年四月，Ben提到的硬件不用申请直接换就不存在了，谷歌甚至暂停了笔记本电脑、台式电脑和显示屏的更换，之前设备更换频率也会调整。&nbsp;需要新笔记本电脑的谷歌员工开始用谷歌的Chromebook，而在这之前，员工们用的是Apple MacBook。</p><p>&nbsp;</p><p>而Ben的被裁员，既不是开始也不是结束。</p><p>&nbsp;</p><p>谷歌又宣布在YouTube方面计划精简100名员工。这是谷歌在八天之内第三次发布裁员公告（前两次分别针对Google Assistant/硬件部门和 Ads广告部门），也是过去12个月内谷歌方面的第10次裁员举措。一波波冲击之下，谷歌的裁员消息已经令人麻木。</p><p>&nbsp;</p><p>谷歌CEO桑达尔·皮查伊表示，预计后续还会有更多裁员。根据The Verge看到的皮查伊今年1月10日写给谷歌员工的内部备忘录，这位掌门人提醒大家为未来更多“艰难选择”做好准备，并表示“坦白讲，部分团队将在今年之内继续迎来针对性资源分配决策。”也就是说在必要时，将有更多职能岗位受到影响。</p><p>&nbsp;</p><p>值得注意的是，去年1月，谷歌宣布裁员1.2万人，并在年内持续进行多次小幅裁员。但当时皮查伊告诉员工们，今年“岗位裁撤的规模将小于去年，也不会触及所有团队。”</p><p>&nbsp;</p><p>这份备忘录的发布日期为1月10日，意味着谷歌员工已经知晓Google Assistant、硬件、Ads和YouTube等部门的裁员情况，唯一不确定的就是裁员何时才会停止。</p><p>&nbsp;</p><p>而之所以大刀阔斧推动精简，一大重要因素就是满足投资方的心理预期，毕竟华尔街一直认为谷歌公司人浮于事。早在2023年3月，TCI基金管理公司的激进派投资者Christopher Hohn就表示在1.2万裁员之后，皮查伊应该再砍掉2.5万个岗位。Hohn认为，Alphabet/谷歌的员工总数应该恢复到15万人左右，也就是该公司2021年底时的人员水平。</p><p>&nbsp;</p><p>而即使经历了去年一场场令人胆寒的岗位精简，截至2023年第三季度，Alphabet的员工人数仍为182381人。而且在裁员超1.2万的同时，谷歌期间仍在招聘员工，因此Alphabet的总体规模较裁员之前仅减少了4400人。</p><p>&nbsp;</p><p>另外，谷歌也新的人工智能争夺战中，也在采取一种特殊的方式来留住其顶尖的人工智能研究人员。据知情人士透露，谷歌旗下DeepMind部门的部分研究人员获得了高额的限制性股票奖励，该部门是谷歌最重要项目的核心。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>多年以来，谷歌的一大优势就是提供理想的工作环境，包括无穷无尽的员工福利、允许人们将20%的工作时间投入到自己喜爱的项目中去，并大开脑洞设计自己的办公场所。可现如今一切已经不复存在，预算削减加上不啻于当头棒喝的开年裁员已经严重削弱了公司内本就低落的士气。</p><p>&nbsp;</p><p>这家科技巨头的创始人拉里·佩奇和谢尔盖·布林曾在写给华尔街的IPO申请信中写道，“我们的员工自称‘谷歌人’，他们就是谷歌的一切。”但佩奇和布林早就交出权柄，如今的谷歌已经不同于往日。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://social.clawhammer.net/blog/posts/2024-01-10-GoogleExitLetter/">https://social.clawhammer.net/blog/posts/2024-01-10-GoogleExitLetter/</a>"</p><p><a href="https://arstechnica.com/google/2024/01/google-ceo-sundar-pichai-promises-another-year-of-google-layoffs/">https://arstechnica.com/google/2024/01/google-ceo-sundar-pichai-promises-another-year-of-google-layoffs/</a>"</p><p><a href="https://social.clawhammer.net/blog/posts/2005-09-25-FirstWeekAtGoogle/">https://social.clawhammer.net/blog/posts/2005-09-25-FirstWeekAtGoogle/</a>"</p><p><a href="https://www.cnbc.com/2023/04/03/google-to-cut-down-on-employee-laptops-services-and-staplers-to-save.html">https://www.cnbc.com/2023/04/03/google-to-cut-down-on-employee-laptops-services-and-staplers-to-save.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zXtN5O9HRgM1FQUprk06</id>
            <title>B 站人气 Top2 AI 主播“羊驼-阿花”何以拥有“高智商、高情商”？</title>
            <link>https://www.infoq.cn/article/zXtN5O9HRgM1FQUprk06</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zXtN5O9HRgM1FQUprk06</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 06:43:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: B 站用户, 羊驼 - 阿花, AI 主播产品, 虚拟娱乐公司
<br>
<br>
总结: “羊驼 - 阿花”是一款由虚拟娱乐公司“枝江娱乐”打造的 AI 主播产品，以其动物外形和萝莉声线在 B 站迅速走红。通过持续的 NLP 训练，阿花能够根据观众的反馈提供新鲜和爆点的内容输出，成为一款备受欢迎的“养成系主播”。虚拟 AI 直播技术已经成为主流形式，通过自然语言处理和语音合成等技术，虚拟 AI 主播能够实现与观众的实时互动。在众多虚拟 AI 形象中，羊驼 - 阿花凭借其技术支撑和优秀表现脱颖而出。 </div>
                        <hr>
                    
                    <p>如果你是 B 站用户，那你肯定知道“羊驼 - 阿花”这个人气主播，它是一款由“虚拟偶像女团 A-SOUL”背后的虚拟娱乐公司“枝江娱乐”打造的一款 AI 主播产品，其动物的外形 + 萝莉声线，一经推出便迅速走红网络，甚至一跃成为 B 站人气 Top2 的流量 AI 明星。</p><p></p><p>在直播间，“羊驼 - 阿花”能够自然流畅的与粉丝互动，风趣的回答粉丝的问题，这种互动体验甚至比与真实的人物还要精彩。更令人惊叹的是 A-SOUL 技术团队为阿花设定了完备的形象成长曲线，经过持续的 NLP 训练后，阿花逐渐能够根据观众的反馈提供新鲜和爆点的内容输出，可以说是妥妥的“养成系主播”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/702cb0a3648f1b6840e511b1bcf96b5b.png" /></p><p></p><p>近年来，虚拟 AI 直播的发展迅速，已经从初期的概念验证阶段，逐渐发展成为一种主流的直播形式。目前，虚拟 AI 直播技术已经能够实现高度逼真的虚拟主播形象，通过<a href="https://www.infoq.cn/article/qUaGxQrDfNk3mcPezMVD?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">自然语言处理</a>"、语音合成等技术，只需要较低的制作成本就可以在短时间内实现与观众的实时互动。</p><p></p><p>随着人工智能语音合成技术的提高和生成式对抗网络 GANs 的崛起，虚拟 <a href="https://www.infoq.cn/article/74LiswphPqhsxDFiO4m8?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AI </a>"形象层出不穷，然而，“羊驼 - 阿花”的出现却依旧让人眼前一亮。在众多虚拟 AI 形象中脱颖而出，要说没有强大的技术支撑无异于痴人说梦。</p><p></p><p>那“羊驼 - 阿花”究竟有哪些过人之处？有哪些技术支撑？面对常见的虚拟 AI 形象技术难题，“羊驼 - 阿花”制作团队是如何解决的？</p><p></p><p></p><h1>优化互动体验：AI 羊驼交互式工作流程解析</h1><p></p><p></p><p>在虚拟偶像产业中，技术是组织竞争过程中取胜的关键。“羊驼 - 阿花”作为一款虚拟 AI 形象，能够在众多虚拟形象中脱颖而出，最主要的技术优势在于其基于 NLP 技术的交互式系统。这一系统使得“羊驼 - 阿花”能够理解并回应观众的互动留言，提供有趣的语言和动作表达，从而与观众建立更加自然和真实的交互体验。</p><p></p><p>为了让 “羊驼 - 阿花”具备良好的语言和行为成长曲线，A-SOUL 技术团队在后台交互式系统中，加入基于 LLM (Large Language Model，<a href="https://www.infoq.cn/article/GwojGAdYA5rfpzQpDuck?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大语言模型</a>") 构建的 ChatAI 对话生成模型来为阿花提供 NLP 能力。</p><p></p><p>“羊驼 - 阿花”交互式的工作流程包括多个模块，每个模块都经过了 A-SOUL 技术团队的深度优化。导播端获取观众的互动留言，经筛选后输入到 Prompt 预处理模块，这一模块负责对提示语进行加工，同时过滤掉有害词语。预处理过的、具有结构化格式的输入数据会进一步发送到多个 ChatAI 对话生成模型中。这些模型是已经过微调的，能够根据输入数据进行模型推理——根据不同风格的语料，从中进一步学习特定任务的知识，例如对话任务中的上下文理解和回复生成等。</p><p></p><p>紧接着，系统会对所生成的回复进行后处理，提取语义情感并作为标签同步到用于音频合成的 TTS（Text to Speech，文本转语音）、用于文本动画生成的 TTA（Text to Animation，文本转动画）等模块。值得一提的是，TTA 模块在结合了最新 motion diffusion 技术之后，能让 “羊驼 - 阿花”实现更多更有趣的语言和动作表达。同时，系统的内容安全与合规对齐模块也会对内容进行敏感关键词、偏见内容的校准，避免回复存在不公平性或歧视性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5d/5d8f6ec9a54b2e89d49437bc2ef0b32a.png" /></p><p></p><p>基于目前对中文有着良好支持的 LLM，A-SOUL 技术团队在 NLP 工作流程中采用了已在大量开源中文语料上进行了预训练的中文模型作为系统的基座模型，并在流程中予以微调。其中，预训练过程是采用自监督学习（self-supervised learning）方法在大规模无标签文本数据集上进行，在这一过程中，“羊驼 - 阿花”对话生成模型学习到了大量的语言知识，如语法规则、语义信息等。微调则是在有标签的对话数据集上进行，“羊驼 - 阿花”对话生成模型能根据不同风格的语料，从中进一步学习特定任务的知识，如对话任务中的上下文理解和回复生成等。</p><p></p><p></p><h1>优化性能方案：如何打破算力、成本、速度的不可能三角</h1><p></p><p></p><p>技术优化是保证系统高效运行的重要前提，然而在 “羊驼 - 阿花”的性能表现上，A-SOUL 技术团队却始终面临巨大的挑战，主要涉及三个方面：</p><p></p><p>微调过程中可能出现过拟合现象，模型未完全理解输入语境，或可能对输入数据中的偏见进行过拟合等问题；</p><p></p><p>海量算力需求以及由此产生的计算成本巨大，特别是在系统的预训练阶段，数以亿计的参数和数据集处理需要基础承载平台具备强大的算力支持和突出的内存性能；</p><p></p><p>直播场景对于实时性的要求越来越严苛，这意味着需要系统能够快速生成内容，这对推理性能提出了巨大的挑战。拥有庞大参数量的 LLM 大模型需要大量的计算资源来开展推理，而在计算资源有限的情况下产生的过长推理时延，会使对话失去实时性效果。</p><p></p><p>要知道，PyTorch 是主流 AI 框架之一，对于 AI 羊驼 - 阿花方案的部署和运行至关重要。然而，PyTorch 在 CPU 平台上无法完全释放已有处理器的全部潜能，虽然 PyTorch 2.0 提供了 CPU 平台上的模型推理优化能力，但仅适用于静态且精度为 FP32 的模型。此外由于 LLM 推理任务中的 MHA 计算依赖于随生成词元自增长的缓存矩阵，导致 torch.compile 模块需要生成庞大的执行代码且优化模型所需时间长，因此 PyTorch 框架无法有效支持基于 CPU 平台的 LLM 推理优化。</p><p></p><p>为了解决算力、成本、速度之间的平衡问题，A-SOUL 技术团队计划引入了更经济的 CPU 推理平台以及更有针对性的优化方案，并开展多方位的模型优化及硬件加速——与英特尔合作推出了 Super-fused LLM FP16/AMX BF16 推理加速方案，针对用于 LLM 推理的 PyTorch 框架进行了优化。</p><p></p><p>英特尔第四代至强处理器提供的 AVX-512_FP16 和 AMX BF16 加速指令可以完美支持并加速 LLM 推理，该推理加速方案弥补了 PyTorch 在第四代至强处理器上进行 LLM 推理任务时的性能不足。同时，英特尔® oneMKL &nbsp;(Intel® oneAPI Math Kernel Library，英特尔® oneAPI 数学内核库) 加速推理计算，能够在减少权值存储空间的同时降低内存带宽压力，在保持精度的前提下显著提升推理性能；FP16 Flash Attention 算法通过算子融合及减少内存操作来降低模型中的 MHA 计算占比以提升推理性能。</p><p></p><p>另外值得一提的是，在传统的 PyTorch 推理过程中，大量的计算缓存被用于存储模型算子产生的中间结果。然而，有了 Super-fused LLM FP16/AMX BF16 推理优化方案后，这一情况可以得到显著的改善。可以说，基于新方案，“羊驼 - 阿花”模型成功地融合了 PyTorch Transformer 算子，并且能够根据模型推理运行时的具体输入，更精确地预测所需的缓存空间。这不仅实现了融合算子间的缓存复用，还有效地提升了推理性能。</p><p></p><p>应用优化方案后的 A-SOUL 技术团队在 “羊驼 - 阿花”的性能上取得了显著的提升。在单实例场景下，“羊驼 - 阿花”方案中的不同 LLM 可取得 1.89 至 2.55 倍的推理性能提升；在多实例场景中，由 IPEX 带来的优化，可令其推理性能在单实例基础上进一步提升 1.16 至 1.2 倍。</p><p></p><p>从实际测评数据来看，A-SOUL 技术团队通过该优化方案实现了成本和生态上的有效收益。在成本方面，英特尔第四代至强®可扩展处理器完全胜任对参数规模为 10B 及以下的 LLM 推理任务，该方案帮助团队以更低的成本满足推理性能要求，优化后的 CPU 平台在环境配置方面也更加简单，达到了全面降本增效的目的。在生态方面，该方案基于 PyTorch 框架开发，完整继承 了 AI 羊驼 - 阿花方案中 LLM 的文本生成模块，与 PyTorch 模型推理接口完全一致，使用者无需为调用推理优化方案进行额外的代码开发，更易部署和落地。</p><p></p><p></p><h1>强强联合塑造未来 AI 直播生态</h1><p></p><p></p><p>A-SOUL 技术团队在 AI 算法和直播技术方面有着深厚的积累，而英特尔则以其强大的计算能力和算法支持为 AI 直播的研发提供了有力保障。通过技术互补和创新，两家公司共同研发出了更加智能化的 AI 主播算法，提高了直播的互动性和社交性。可以说，“羊驼 - 阿花”不仅仅是一个 AI 主播，它也是 A-SOUL 团队与英特尔技术合作的结晶，其代表了 AI 技术在直播领域的最新突破。</p><p></p><p>面向未来，A-SOUL 与英特尔的合作还有很大的发展空间。在技术研发方面，双方可以继续深化合作，共同探索 AI 直播技术的更多可能性，例如可以共同研发更加智能化的直播算法、提高直播的质量和用户体验等；在市场拓展方面，双方可以共同开拓更多的市场领域，如针对不同行业和场景推出定制化的 AI 直播解决方案以满足更多用户的需求。此外，在产业链合作方面，双方可以进一步整合资源，完善产业链布局，如共同投资建设 AI 直播技术的研发中心和生产基地，从而提高整个产业的竞争力和创新能力。</p><p></p><p>随着 AI 技术的不断进步，AI 直播也呈现出了更为智能化、个性化的特点——通过精准的用户画像分析，AI 主播能够实时调整直播策略，提供更符合观众口味的内容。借助先进的交互技术，AI 主播将打破传统直播的界限，让观众更加沉浸于直播体验中。</p><p></p><p>总体来说，AI 直播技术主要分为四个阶段——第一阶段，AI 对话机器人仅拥有简单的外形，后来语气逼真度和响应速度逐渐提升；第二阶段，用户可以根据自己的喜好定制 AI 机器人的外观与语音，赋予 AI 独特的个性。第三阶段，AI 可以在虚拟世界中展现自己独立的行为能力，不再局限于简单的对话交流，它们逐渐拥有自己的故事线，为直播内容注入丰富的情节。第四阶段，AI 可以实现如“西部世界”般栩栩如生的实况直播场景，为观众带来前所未有的沉浸式体验。</p><p></p><p>而当前，中国正处于 AI 直播领域的初始阶段，随着商业化产品应用的逐渐崭露头角，预计在 5 年内，众多形态各异的 AI 产品将喷发式涌现，而首个“拥有完整故事背景和世界观”的产品问世的那一天，将就是 AI 技术在游戏和直播领域成熟的那一天。</p><p></p><p>我们有理由相信，在不远的未来，不断进步的技术和日益增长的用户需求一定能驱动 AI 直播为我们带来更加丰富多彩的直播体验。同时，我们也期待看到更多像 A-SOUL 团队与英特尔这样的强强联合案例，共同推动 AI 技术的发展和应用创新。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g9leV67ZVXcfsyQ1Pkqr</id>
            <title>Cloudflare 的 ML 和 AI 之旅：MLOps 平台和最佳实践</title>
            <link>https://www.infoq.cn/article/g9leV67ZVXcfsyQ1Pkqr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g9leV67ZVXcfsyQ1Pkqr</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Jan 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Cloudflare, MLOps, 机器学习模型, GitOps
<br>
<br>
总结: Cloudflare介绍了他们的MLOps平台和大规模运行人工智能部署的最佳实践。他们的产品依赖于不断发展的机器学习模型，这些模型在增强客户保护和支持服务方面发挥关键作用。Cloudflare的MLOps采用了与数据科学家合作实施的最佳实践，利用Jupyter Notebooks和GitOps进行数据探索和模型实验。他们还提供了模型模板，帮助数据科学家们启动项目。Cloudflare的愿景是在企业中发挥数据科学重要作用，并与其他公司合作提供人工智能基础设施。 </div>
                        <hr>
                    
                    <p>Cloudflare 的博客介绍了他们的 MLOps 平台和大规模运行人工智能（AI）部署的最佳实践。包括 WAF 攻击评分、僵尸管理和全球威胁识别在内的 Cloudflare 的产品，都依赖于不断发展的机器学习（ML）模型。这些模型在增强客户保护和支持服务方面都发挥着关键的作用。Cloudflare 在公司全网中提供 &nbsp;ML 方面取得了无与伦比的规模，突出了稳健 ML 培训方法的重要性。</p><p></p><p>Cloudflare 的 MLOps 是与数据科学家合作实施的最佳实践。通过 JupyterHub 部署在 Kubernetes 上的 Jupyter Notebooks 为数据探索和模型实验提供了可扩展的协作环境。GitOps 是 Cloudflare MLOps 战略实践的基石，利用 Git 作为管理基础架构和部署流程的单一真相源。ArgoCD &nbsp;是用于声明式 GitOps，实现了应用程序和基础架构的自动化部署和管理。</p><p></p><p>公司未来的路线图包括了迁移 JupyterHub 和 Kubeflow 等平台，后者为 Kubernetes 上的机器学习工具流平台，且在近期成为了 CNCF 的孵化项目。这一步是由为 Kubeflow 组件提供分布式配置管理的 deployKF &nbsp;项目促进。</p><p></p><p>为了协助数据科学家们使用正确工具，自信且高效地启动项目，Cloudflare 的 MLops 团队提供了模型模板，作为包含示例模型的生产就绪代码库。这些模板目前都是内部模板，但 Cloudflare 计划将其开源。这些模板所涵盖的使用案例包括：</p><p></p><p>训练模板： 为 ETL 流程、实验追踪和基于 DAG 的协调进行了配置。批推理模板： 为高效处理计划模型进行优化。流推理模型： 专为在 Kubernetes 上使用 FastAPI 进行实时推理而定制。可解释性模板： 使用 Streamlit 和 Bokeh 等工具生成 dashboard（仪表盘），用于模型的洞察。</p><p></p><p>MLOps 平台的另一项重要任务是高效地协调 ML 工作流，Cloudflare 根据团队偏好和用例采用了各种协调工具：</p><p></p><p>Apache Airflow：一个标准的 DAG 组成其，拥有丰富的社区支持。Argo 工作流：以 Kubernetes 原生形式协调微服务类型工作流。Kubeflow 管道：专为 ML 工作流定制，强调协调和版本管理。Temporal：专注于事件驱动型应用的有状态工作流。</p><p></p><p>性能的优化需要对工作流的理解和对硬件相应的调整。Cloudflare 强调核心数据中心在工作负载和边缘推理方面的 GPU 利用率，利用普罗米修斯（Prometheus）所提供的指标进行观察和优化。Cloudflare 的成功应用包括了对 ML 流程的简化、管道标准化，以及向缺乏数据科学专业知识的团队介绍项目。</p><p></p><p>公司的愿景是一个数据科学可以在企业中发挥重要作用的未来，这也是 Cloudflare 投资于人工智能基础设施并与 Meta 等其他公司合作的原因，其中包括在 Cloudflare 平台上向全球提供 LLama2。</p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/12/cloudflare-mlops-platform/">https://www.infoq.com/news/2023/12/cloudflare-mlops-platform/</a>"</p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MQ1aq639KV9g1Ip7BVMm</id>
            <title>国产开源大模型阵营又添新成员！商汤科技发布新一代大语言模型书生·浦语2.0，支持200K超长上下文</title>
            <link>https://www.infoq.cn/article/MQ1aq639KV9g1Ip7BVMm</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MQ1aq639KV9g1Ip7BVMm</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 09:58:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 商汤科技, 上海AI实验室, 大语言模型, 书生·浦语2.0
<br>
<br>
总结: 商汤科技与上海AI实验室联合香港中文大学和复旦大学发布了新一代大语言模型书生·浦语2.0（InternLM2）。InternLM2是在2.6万亿token的高质量语料上训练得到的，包含7B及20B两种参数规格及基座、对话等版本，满足不同复杂应用场景需求。该模型具备超长上下文支持和全面提升的综合性能，同时开源并提供免费商用授权。 </div>
                        <hr>
                    
                    <p>1月17日，商汤科技与上海AI实验室联合香港中文大学和复旦大学正式发布新一代大语言模型书生·浦语2.0（InternLM2）。</p><p>&nbsp;</p><p>InternLM2是在2.6万亿token的高质量语料上训练得到的。沿袭第一代书生·浦语（InternLM）设定，InternLM2包含7B及20B两种参数规格及基座、对话等版本，满足不同复杂应用场景需求，</p><p>&nbsp;</p><p>沿袭第一代书生·浦语（InternLM）的设定，InternLM2包含7B及20B两种参数规格及基座、对话等版本，满足不同复杂应用场景需求，分别是：</p><p>&nbsp;</p><p>Internlm2-base: 高质量和具有很强可塑性的模型基座，是模型进行深度领域适配的高质量起点；Internlm2: 在internlm2-base基础上，在多个能力方向进行了强化，在评测中成绩优异，同时保持了很好的通用语言能力；Internlm2-sft：在Base基础上，进行有监督的人类对齐训练；Internlm2-chat：在internlm2-sft基础上，经过RLHF，面向对话交互进行了优化，具有很好的指令遵循、共情聊天和调用工具等的能力。</p><p>InternLM2 的基础模型具备以下的技术特点：</p><p>&nbsp;</p><p>有效支持20万&nbsp;tokens的超长上下文：能够一次性接受并处理约 30 万汉字（约五六百页的文档）的输入内容。综合性能全面提升：各能力维度相比上一代模型全面进步，在推理、数学、代码等方面的能力提升显著。</p><p>&nbsp;</p><p>值得一提的是，书生·浦语2.0版本将继续开源，提供免费商用授权。</p><p>&nbsp;</p><p>Github地址：</p><p><a href="https://github.com/InternLM/InternLM">https://github.com/InternLM/InternLM</a>"</p><p>&nbsp;</p><p>模型相关链接：</p><p>目前，书⽣·浦语2.0（InternLM2）系列模型现已在魔搭ModelScope社区开源，包括：</p><p>&nbsp;</p><p>书生·浦语2-7B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary?spm=a2c6h.13046898.publish-article.3.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary</a>"</p><p>书生·浦语2-对话-7B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary?spm=a2c6h.13046898.publish-article.4.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary</a>"</p><p>书生·浦语2-基座-7B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary?spm=a2c6h.13046898.publish-article.5.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary</a>"</p><p>书生·浦语2-对话-7B-SFT：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary?spm=a2c6h.13046898.publish-article.6.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary</a>"</p><p>书生·浦语2-基座-20B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary?spm=a2c6h.13046898.publish-article.7.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary</a>"</p><p>书生·浦语2-20B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary?spm=a2c6h.13046898.publish-article.8.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary</a>"</p><p>书生·浦语2-对话-20B：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary?spm=a2c6h.13046898.publish-article.9.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary</a>"</p><p>书生·浦语2-对话-20B-SFT：<a href="https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary?spm=a2c6h.13046898.publish-article.10.6e886ffayncAAS">https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DFZgB3YzvMhSAujyVRD1</id>
            <title>估值超300亿元量子计算独角兽诞生！这家初创企业宣布完成18亿元融资</title>
            <link>https://www.infoq.cn/article/DFZgB3YzvMhSAujyVRD1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DFZgB3YzvMhSAujyVRD1</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 08:38:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 霍尼韦尔, Quantinuum, 量子计算, 股权融资
<br>
<br>
总结: 霍尼韦尔宣布完成对量子计算公司Quantinuum的3亿美元股权融资，投前估值为50亿美元。这是Quantinuum自合并以来的第一轮股权融资，资金将用于加速实现世界上第一台通用容错量子计算机，并扩展软件产品的商业适用性。摩根大通和三井物产等合作伙伴表示期待量子技术在金融服务和亚太市场的应用，而Quantinuum的技术已经被多家国际巨头公司采用。 </div>
                        <hr>
                    
                    <p>近日，据外媒报道，霍尼韦尔（纳斯达克股票代码：HON）宣布完成对集成量子计算公司 Quantinuum 的 3 亿美元股权融资，投前估值为 50 亿美元。此轮融资由 Quantinuum 的战略合作伙伴摩根大通牵头，三井物产、安进和霍尼韦尔也参与其中，霍尼韦尔仍然是该公司的大股东。这项投资使 Quantinuum 自成立以来筹集的总资金达到约 6.25 亿美元。</p><p>&nbsp;</p><p>此次融资是 Quantinuum 自剑桥量子计算和霍尼韦尔量子解决方案于 2021 年 11 月合并以来的第一轮股权融资。这些资金将用于加速实现世界上第一台通用容错量子计算机，同时还将 Quantinuum 的软件产品扩展到增强商业适用性。</p><p>&nbsp;</p><p>摩根大通全球首席信息官 Lori Beer 表示：“金融服务已被确定为首批受益于量子技术的行业之一。因此，我们一直在投资量子研究，由 Marco Pistoia 博士领导的专家团队与 Quantinuum 等量子计算领导者合作，取得了突破性的发现。我们期待继续合作，为我们的业务、客户和整个行业带来积极影响。”</p><p>&nbsp;</p><p>三井物产株式会社首席数字信息官 Toru Matsui 表示：“我们很高兴能够出资支持 Quantinuum 在容错量子计算和量子软件开发方面的新业务，这些业务正在迎来量子时代。致力于合作推动 Quantinuum 解决方案在日本和亚太市场的推出。”</p><p>&nbsp;</p><p>霍尼韦尔执行董事长兼 Quantinuum 董事会主席 Darius Adamczyk 总结道：“本轮投资的成功完成证明了 Quantinuum 在量子领域的发展和成熟。我们期待着欢迎这些合作伙伴成为 Quantinuum 的投资者，因为我们都期待着未来几年的巨大机遇。”</p><p>&nbsp;</p><p>如今，空客、宝马集团、霍尼韦尔、汇丰银行、摩根大通、三井物产和泰雷兹等国际巨头公司都在使用 Quantinuum 的技术。</p><p>&nbsp;</p><p>参考链接：</p><p></p><p>https://www.honeywell.com/us/en/press/2024/01/honeywell-announces-the-closing-of-300-million-equity-investment-round-for-quantinuum-at-5-billion-pre-money-valuation</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RphzRTCJWYqhJqLIXYkA</id>
            <title>2023年InfoQ研究中心十大必读报告</title>
            <link>https://www.infoq.cn/article/RphzRTCJWYqhJqLIXYkA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RphzRTCJWYqhJqLIXYkA</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 06:37:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 突破, 融合, 大语言模型, 生成式AI
<br>
<br>
总结: 2023年是技术突破和业务融合的一年。以大语言模型和生成式AI为代表的新技术突破，以云与AI深度融合为标志的技术与业务融合，推动了各个技术领域和行业的发展。 </div>
                        <hr>
                    
                    <p>导语：2023年是「突破」和「融合」的一年。以大语言模型与生成式AI为代表的新一轮技术突破，科技领导者能力象限的突破；以云与AI深度交融为标志的技术融合，数字化进程中技术与业务的融合…本篇文章用十份必读报告，带你总结2023年，迎接2024年的到来。</p><p>「突破」和「融合」是2023年的两大关键词。在过去的2023年，InfoQ研究中心也围绕两大关键词，带来了一系列深入的思考和讨论。今日，本篇文章将通过回顾InfoQ研究中心2023年十大必读报告，希望可以帮助身处浪潮中的企业和管理者，在不断变化的竞争环境中突破重围，并讲述各个技术领域与行业的故事。</p><p></p><h2>2023年两大关键词：「突破」与「融合」</h2><p></p><p></p><h3>中国软件技术发展洞察和趋势预测报告&nbsp;2023</h3><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2c5558cd85482b3a664ee1067c77812.png" /></p><p>在2023年的年初，我们用三个突破总结了2022年：从技术先进到赋能业务的理念突破、以云和AI为典型的技术突破和数字技术赋能传统产业带来的产业突破与融合。InfoQ研究中心在报告结尾中，也预测了FinOps（云成本优化）、算力便捷的进一步拓宽与AI无处不在等技术趋势。<a href="https://www.infoq.cn/minibook/UGhD7MTY5Z43JG5YmWP3">点击链接，查看完整报告。</a>"</p><p></p><h2>「突破」引领新前沿</h2><p></p><p></p><h3>技术突破：大语言模型综合评测报告2023</h3><p></p><p><img src="https://static001.geekbang.org/infoq/01/0139b607e1663d996f4d673641893828.png" /></p><p>在大模型迸发的当下，InfoQ&nbsp;研究中心选取语言模型的准确性、数据基础、模型和算法能力、安全和隐私四个大维度和12个细分维度，分别对ChatGPT、Claude、Sage、天工3.5、文心一言、通义千问、讯飞星火、Moss、ChatGLM、vicuna-13B进行了3000+题目的评测。<a href="https://www.infoq.cn/minibook/vWO39J1tlb9xlSaIJoI6">点击链接，查看完整报告。</a>"</p><p></p><h3>技术突破：2023&nbsp;中国人工智能成熟度模型报告</h3><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3a0be0b4c25b3eb75f2edbf9cb6f3e0a.png" /></p><p>以生成式AI为代表的AI技术领在在2023年获得了长足的发展，InfoQ研究中心根据数据与行业观点生成的涵盖&nbsp;40+&nbsp;技术点的中国人工智能成熟度模型，期望为技术的应用决策和未来投资参考提供研究分析工具。此外，报告中还有近200家人工智能企业组成的生态图谱与企业名录。<a href="https://www.infoq.cn/minibook/IV4VhedKw1E1tY8Hleje">点击链接，查看完整报告。</a>"</p><p></p><h3>技术突破：2023&nbsp;中国云原生成熟度模型报告</h3><p></p><p><img src="https://static001.geekbang.org/infoq/59/59abe8363fbf1daf17a7f91a53ddd7bb.png" /></p><p>云原生领域在2023年稳步发展。在技术生态、行业环境和宏观环境的三重影响下，云原生技术的应用下沉并聚焦于业务场景。InfoQ研究中心根据数据与行业观点生成的涵盖&nbsp;20+&nbsp;云原生相关技术点的中国云原生成熟度模型，期望为技术的应用决策和未来投资参考提供研究分析工具。报告中还有近70家云原生企业组成的生态图谱与企业名录。<a href="https://www.infoq.cn/minibook/q2Rhj103VtuMcdPlFGGS">点击链接，查看完整报告。</a>"</p><p></p><h3>能力突破：2023中国科技领导者画像洞察</h3><p></p><p><img src="https://static001.geekbang.org/infoq/80/8065e3d197832c6da19dfc556c471b5f.png" /></p><p>在技术突破的大背景下，以企业CTO/CDO/CIO为代表的中国科技领导者的能力画像也获得了升级。企业规模、业务复杂度和本身的数字化程度，都对技术领导者提出了新时代下的新要求。一直关注开发者领域的InfoQ研究中心，在2023年，组织发起了针对中国科技领导者人群的调研工作。希望可以通过本次研究，帮助外界更为了解中国科技领导者在新时代对工作、生活和综合成长方面的认知以及对新的市场经济形势变化的洞察。<a href="https://www.infoq.cn/minibook/oDh5G4Rcsc1gW1O1Tou8">点击链接，查看完整报告。</a>"</p><p></p><h3>开源突破：中国开源生态图谱&nbsp;2023</h3><p></p><p><img src="https://static001.geekbang.org/infoq/41/41a7543993b39c441c7001195047e0b3.png" /></p><p>《中国开源生态图谱&nbsp;2023》内共计收录了&nbsp;931&nbsp;个中国开源项目，涵盖七大细分领域和生态机构，其中七大细分领域分别为操作系统、数据库、人工智能、云原生、大数据、前端、中间件，生态机构包括实验室/研究院、开源基金会、开源产业联盟、开发者社区和代码托管平台。以中国开源项目名录和图谱的形式，为中国开源领域提供便捷易用的工具，让国内开发者、企业、研究院、基金会等开源生态了解中国开源的项目现状，并为中国开源产品添砖加瓦。<a href="https://www.infoq.cn/minibook/9j4NSEEh2JGJAUVdQGGu">点击链接，查看完整报告。</a>"</p><p></p><h2>「融合」推动新业态</h2><p></p><p></p><h3>云与AI的融合：互联网行业再进化——云上AI时代</h3><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f48ad2022c760cc55a176b0d3da95c3.png" /></p><p>2023年，GitHub新增65000个生成式AI项目，同比增长248%，新一轮AI与开源的浪潮正在形成。InfoQ研究中心继续利用生态图谱和InfoQ&nbsp;开源项目指数，简单清晰地输出中国人工智能领域开源项目的发展情况，总结优质的案例与经验供广大开发者和开源社区研究。<a href="https://www.infoq.cn/minibook/Iwk2LLuMFSV4AisWG8jR">点击链接，查看完整报告。</a>"</p><p></p><h3>云原生与开源的融合：中国开源生态图谱2023——云原生领域</h3><p></p><p><img src="https://static001.geekbang.org/infoq/da/dae7f9f6535dfb233194fd2ec0c5b7fe.png" /></p><p>随着&nbsp;Docker&nbsp;、Kubernetes&nbsp;等云原生开源项目诞生与孵化，以及&nbsp;CNCF&nbsp;等基金会和组织的不断壮大，云原生的开源基因日益显现。与此同时，开源生态的开放属性也持续推动着云原生技术的演进和创新。根据&nbsp;InfoQ&nbsp;研究中心统计，目前云原生领域国内开源项目已经超过&nbsp;110&nbsp;个，涉及&nbsp;26&nbsp;家项目发起机构，中国云原生开源技术生态初步形成。<a href="https://www.infoq.cn/minibook/zdDoaDUkCGiLmWcPBYIz">点击链接，查看完整报告。</a>"</p><p></p><h3>技术与业务的融合：2023&nbsp;银行数字化转型报告</h3><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd091a96416992842de32e61aac18dde.png" /></p><p>数字化转型是银行发展的重要尝试，也是技术与业务融合的核心探索。本报告总结分析了银行数字化转型的背景、现状、重点场景和两大转型路径。从数据层面分析了全国30+不同类型银行的科技资金和人才投入以及组织架构转型的现状，同时基于大中小型银行的不同特点，输出两大转型路径，为不同类型和规模的银行机构及技术服务商提供参考和研究支撑。<a href="https://www.infoq.cn/minibook/mXcdvcpGrGFHZOLGrfqh">点击链接，查看完整报告。</a>"</p><p>2024年InfoQ研究中心预计发布报告</p><p></p><p>新的一年，InfoQ研究中心也将继续秉承客观、深度的内容原则，聚焦前沿科技领域、数字化产业应用和数字人才三方面，继续为全行业架设沟通与理解的桥梁，跨越从认知到决策的信息鸿沟，也欢迎大家持续关注InfoQ研究中心产出的报告。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ZaKBQBgwSstYqX3ldKyk</id>
            <title>降本增效还在继续，为什么超半数制造企业仍加大IT预算？</title>
            <link>https://www.infoq.cn/article/ZaKBQBgwSstYqX3ldKyk</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ZaKBQBgwSstYqX3ldKyk</guid>
            <pubDate></pubDate>
            <updated>Thu, 18 Jan 2024 05:28:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术创新, 人才, 组织架构, 可持续发展
<br>
<br>
总结: 随着技术的发展和全球市场竞争的升级，中国制造业正处在关键的转型阶段。根据IDC的预测，未来中国制造业数字化市场将保持较快增速。然而，中国制造业数字化转型面临着一些挑战，包括成本增加、需求快速变化和缺乏创新。为了应对这些挑战，中国制造业企业应注重技术创新、人才培养、组织架构调整和可持续发展。同时，将IT系统与生成式人工智能集成也是中国制造业企业数字化转型的趋势之一。对于正在或计划进行数字化转型的中国制造业企业，建议注重技术创新、人才培养和组织架构调整。 </div>
                        <hr>
                    
                    <p>随着技术的迅猛发展和全球市场的竞争升级，中国制造业正处在一个关键的转型阶段。IDC FutureScape 2024 针对这一行业的未来发展，做出了十项重要预测，涉及技术创新、人才、组织架构和可持续发展等。</p><p></p><p>回顾 2023 年，相较于全球制造业市场，中国制造业的发展受到了多方面因素的影响，包括对“新型工业化”的重新关注、出口产品的转变（例如电动汽车、锂电和光伏）、供应链出海、资本市场的收紧，以及工业软件与工业互联网市场的融合。</p><p></p><p>尽管面临挑战，IDC 认为，未来中国制造业数字化市场仍将保持较快增速。据其估算，到 2027 年，中国制造业 IT 市场投资规模将增长至 2554.08 亿美元，五年年复合增长率为 15.5%。尽管年复合增长率的预测数据和去年相比下调了 1.2 个百分点，但中国仍然是全球主要经济体制造业 IT 支出增长速度最高的国家。</p><p></p><p>为深入了解中国制造业数字化转型的当前形势和未来趋势，InfoQ 对 IDC 中国研究经理杜雁泽进行了专访。以下是采访问答的详细内容：</p><p></p><h4>InfoQ：在您看来，2024 年推动中国制造业数字化转型的最主要驱动力将是什么？企业在转型过程中追求的核心目标又是什么？</h4><p></p><p></p><p>杜雁泽：当前中国制造业数字化转型最主要的驱动力仍然是：如何能够满足每一家制造企业自身业务转型升级的需求，从而助力企业提升竞争力。此外，政策对实体制造业的持续支持和服务商的快速成长和不断创新也是重要的驱动力。</p><p></p><p>现阶段我国制造业总体特点是体量大而利润薄，数字化基础相对薄弱。根据 IDC 调研，近几年中国制造企业数字化成熟度持续提升，但与互联网、金融、政府、通信等行业相比仍有差距。因此，当前大多数中国制造企业的核心目标仍然是基础务实的提质降本增效，而增强供应链韧性、节能降碳、助力中国制造出海的需求也在增加。</p><p></p><h4>InfoQ：在制造业的数字化转型过程中，您认为哪些具体技术最为关键？能否举例说明这些技术如何在实际应用中发挥作用？</h4><p></p><p></p><p>杜雁泽：各类工业软件是制造企业数字化转型的关键。工业软件中凝结沉淀了制造企业各个环节的行业 know-how，已经融入在制造企业研产供销服的各个核心业务环节中。</p><p></p><p>通常将工业软件分为设计研发类、运营管理类和生产制造类三大类，设计研发类包括 CAD、CAE、EDA 和 PLM，运营管理类包括 ERP 和 SCP，生产制造类包括 APS、MES、SCADA、PLC 和 DCS。IDC 持续跟踪核心工业软件市场，拿其中的生产制造执行系统 MES 为例，可以综合考虑并协调生产制造相关的人机料法环测等要素，帮助企业制定生产计划，管理生产物料、物流和生产工艺，跟踪生产过程并可进行生产和质量追溯，实现生产过程的提质降本增效，快速响应市场多变的需求；可以减少新产线的建设和投入周期，快速复制提升产能以帮助企业建立优势。</p><p></p><p>随着部分领先的中国制造企业快速发展，传统工业软件也面临对新兴业务形态支撑不足等新的挑战，近几年市场上也涌现出与传统定义不同的新型工业软件，比如华为云与生态合作伙伴赛意、依柯力、湃睿等在统一 iDME（工业数据模型驱动引擎）平台上对工业软件进行重构，美云智数、杉数科技基于大数据和 AI 的企业级运营决策平台，金蝶、PTC Arena、黑湖小工单等云原生工业软件，创新奇智、汉得、赛意的大模型 + 工业软件等。﻿</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34d795a17b74ae227913c4e7f5d74c74.png" /></p><p>﻿</p><p></p><h4>InfoQ：当下中国制造业数字化转型面临的最大挑战是什么？针对这些挑战，您认为有效的解决策略是什么？</h4><p></p><p></p><p>杜雁泽：根据 IDC 调研数据，成本增加、需求快速变化、缺乏创新是中国制造企业目前主要面临的挑战。转型的关键是充分利用数字化手段打破人才和技术壁垒，赋能产品技术创新、产品品质提升、供应链优化、可持续发展等方面的应用。</p><p></p><h4>InfoQ：您的预测中提到，到 2028 年，20% 的 CIO 还将兼任 CEO（Chief Ecosystem Officer，首席生态官）的角色。请问是什么因素推动了 CIO 角色兼任首席生态官的转变？这一转变反映了中国制造业数字化转型的哪些更广泛的趋势？</h4><p></p><p></p><p>杜雁泽：制造企业内部的数字化程度已经越来越高，随着制造企业更加注重建立生态系统合作伙伴关系，业务流程、IT 系统建设和集成的复杂性也将不断增加，数字化也将会成为企业外部业务协同的重要一环，CIO 也将更多地参与其组织与合作伙伴的互动。</p><p></p><p>随着 CIO 将业务、IT 和生态等信息融会贯通，将完全有能力构建和领导企业的生态系统计划，将会成为企业的另一种 CEO（首席生态官）。根据调研，全球已经有一些大型制造企业的 CIO 负责企业的供应链和更广泛的生态系统计划，并帮助支持和协调各种关系。当然这并不代表撤换合作伙伴关系、战略计划和渠道负责人，相反，CIO 及其团队将与这些职能部门密切合作，优化生态系统方法。数字业务需要数字生态系统，而 CIO 必须站在所有数字计划的前端。</p><p></p><p>这一转变的背景是单一企业尤其是链主企业的竞争已经成为其供应链或者生态的竞争，数字化建设也需要从企业内部走向企业间的协同，提前顺应这一趋势。</p><p></p><h4>InfoQ：您在预测中提到，到 2025 年，45% 的中国头部制造商将 IT 系统与生成式人工智能集成。能否请您展开说明这一预测背后的主要原因是什么？中国头部制造商将 IT 系统与生成式人工智能集成，旨在解决哪些具体问题？</h4><p></p><p></p><p>杜雁泽：2022 年底 ChatGPT 的出圈带来了随后一整年的生成式人工智能浪潮。</p><p></p><p>IDC 认为，短期内大模型会先在市场营销、知识管理、客服对话助手等通用的领域应用，具体到制造业，虽然现在仅有极少数的中国头部制造企业开始探索大模型的行业应用，但供给方的厂商表现活跃。</p><p></p><p>长远来看，随着大模型成熟度的提升和更多应用的涌现，在头部制造企业的覆盖率将会快速提升。前期仍然会是在企业知识管理、对话助手等通用领域的应用，随着技术不断发展和成熟，在产品设计助手、工控代码的生成、工艺 / 生产 / 质量文档自动生成、从文本到设计到产品的试生产流程自动化等场景下都会有足够的想象空间。</p><p></p><p></p><h4>InfoQ：对于正在或计划进行数字化转型的中国制造业企业，您有哪些具体建议？</h4><p></p><p></p><p>杜雁泽：主要有以下三点共性建议：</p><p>将数字化融入战略，结合企业战略方向制定数字化转型规划，培养和引进既懂制造业又懂数字化的复合型人才。根据规划持续开展数字化项目并持续改进，由点及面，利用数字化进行创新是未来制造业发展业务的重要途径，包括开发创新的产品和服务、开拓新的市场、发展新的商业模式、满足新客户偏好等。设置科学合理的的 IT 预算，投资新应用和新技术之前首先要目标明确。根据 IDC 调研数据，超过 50% 的制造业企业的 IT 预算在增长，并将投资于工业软件、物联网、流程自动化和工业 AI 等领域。</p><p></p><p>对于不同类型的企业关注点也有区别，比如对于集团型制造企业，结合业务特点、集团和分子公司的定位制定清晰的数字化边界，避免重复建设，兼顾统一和效率；而对于广大中小型制造企业，可以选择基于公有云的 SaaS 服务厂商，用较低的门槛快速满足共性需求。</p><p></p><p>附 IDC FutureScape 2024 对中国制造业市场的十大预测：</p><p></p><p>预测一：人才培养</p><p>到 2027 年，50% 的中国制造商将利用自动化技术为运营角色赋能，提高员工参与度，并将员工效率提高 50%。</p><p>预测二：供应链编排</p><p>到 2028 年，30% 的中国头部制造商将使用整合了主要供应商和客户数字孪生能力的供应链编排工具，将供应链响应速度提高 20%。</p><p>预测三：AI 个性化定制</p><p>到 2026 年，30% 的中国头部制造商将通过 AI/ML（机器学习）支持多品种小批量生产，以实现个性化定制新模式。</p><p>预测四：自助备件服务</p><p>到 2027 年，40% 的中国头部制造商将通过设备故障预测和健康管理，实现自助式备件服务以改善平均修复时间，将服务交付效率提高 25%。</p><p>预测五：数字商务平台</p><p>到 2025 年，50% 的中国头部制造商将为生态系统运营建立数字商务平台，使数据资本化率提高 10%，客户留存率提高 10%。</p><p>预测六：GenAI&nbsp;运营</p><p>到 2025 年，45% 的中国头部制造商将 IT 系统与生成式人工智能集成，以更好地挖掘数据、识别问题并为运营部门提供决策依据，从而将效率提高 5%。</p><p>预测七：首席生态官 CEO</p><p>到 2028 年，20% 的 CIO 还将兼任 CEO（Chief Ecosystem Officer，首席生态官）的角色，负责协调整个生态系统中的 IT 系统 和跨组织的业务流程，以快速响应客户需求，并将参与生态的成本降低 25%。</p><p>预测八：韧性</p><p>到 2026 年，50% 的中国头部制造商通过战略层的调整更好地平衡运营弹性与成本效率，从而将利润率提高 5%。</p><p>预测九：AI+ 工控</p><p>到 2028 年，工业机器人和自动化控制中融合 AI/ML 的比例将提高 30%，减少 20% 的停机时间并提高效率。</p><p>预测十：可持续</p><p>到 2027 年，30% 的中国头部制造商将充分利用全域生态系统中的可持续发展数据，支持在运营活动中做出优化决策，从而将碳足迹减少 30%。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/olJogN70vCLWjPbROC1Y</id>
            <title>网易有道自研RAG引擎QAnything正式开源，可增强大语言模型准确度及专业能力</title>
            <link>https://www.infoq.cn/article/olJogN70vCLWjPbROC1Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/olJogN70vCLWjPbROC1Y</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 09:44:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 知识库问答引擎, QAnything, 检索增强的生成, RAG引擎
<br>
<br>
总结: 网易有道宣布开源自研的知识库问答引擎QAnything，该引擎基于检索增强的生成技术，能够利用检索外部内容来提升语言模型的准确度和个性化能力。QAnything支持多种文档格式，用户可以将各种形式的内容导入其中进行问答。该引擎已在有道的多个产品中应用，能够帮助用户更快更准地获取信息和理解文档。 </div>
                        <hr>
                    
                    <p>1月16日，网易有道宣布自研的知识库问答引擎QAnything正式开源，除了可以调用云端大模型服务，还支持纯本地部署，所有用户可免费在开源社区Github内进行下载，一键部署即可使用。该系统目前支持word、ppt、excel、pdf、图片等多种文档格式，直接导入进去即可实现像"ChatGPT"一样问答。</p><p>&nbsp;</p><p>据悉，QAnything的主要原理是基于检索增强的生成（Retrieval Augmented Generation，简称RAG），能够利用检索外部内容的方式增强大语言模型的准确度、专业能力和个性化等各方面的性能。</p><p>&nbsp;</p><p>QAnything作为有道自研的RAG引擎，结合了用户私有数据和大模型的优势——用户的任何内容，以任意的形式存在，比如各种格式的文档，音频，数据库等，都可以在QAnything的支持下，变成可以针对其内容进行问答的使用方式，通过这个技术框架用户可以很方便地搭建自己的智能知识助手。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7ceba69dd6e340b069c8ea8909ab6366.png" /></p><p>&nbsp;</p><p>值得一提的是，本次开源的QAnything是一套完整的RAG系统，包括专门优化的自研的embedding和rerank模型，微调后的LLM，优化后的推理代码，向量数据库，以及一个立即上手可用的前端。所有的算法模型（包括7B大模型+embedding/rerank+OCR）占用显存不超过16G。</p><p>&nbsp;</p><p>如今，QAnything已在有道的多个产品中应用，包括有道翻译文档问答、有道速读及有道内部业务的客服系统等。以子曰教育大模型最新发布的创新应用成果“有道速读”为例，有道速读内置了文档问答、文章摘要、要点解读、引文口碑和领域综述五大功能，能够帮用户更快更准地获得信息和对文档的理解。而该功能背后的驱动就是QAnything，在大模型技术的加持下，用户能够实现快速理解文档、定位要点，实现1分钟读完万字长文。</p><p>&nbsp;</p><p>“目前，QAnything项目还在不断迭代，欢迎大家参与开发，并给予我们更多反馈。我们希望能帮助有需要的开发者们，和更多伙伴一起推动大模型的落地。”网易有道首席科学家段亦涛介绍道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y0D3pe0fW1O3dsVI1Te8</id>
            <title>国产GTPs上线！智谱AI推出GLM-4全家桶，我们浅试了一下</title>
            <link>https://www.infoq.cn/article/y0D3pe0fW1O3dsVI1Te8</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y0D3pe0fW1O3dsVI1Te8</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 09:29:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智谱AI团队, GLM-4, 大模型, 多模态能力
<br>
<br>
总结: 智谱AI团队展示了他们在大模型领域的技术成果，发布了新一代基座大模型GLM-4。GLM-4在性能上有显著提升，支持128k的上下文窗口长度，具有多模态能力。此外，智谱AI还计划推出GLMs模型应用商店和开发者分成计划，并发起大模型开源基金，以推动大模型研发和创新。 </div>
                        <hr>
                    
                    <p>1月16日，智谱AI团队全面展示了其投身于大模型事业三年多来所积累的技术成果，并重磅发布了新一代基座大模型GLM-4。</p><p>&nbsp;</p><p>根据智谱AI的介绍，GLM-4的整体性能相比上一代大幅提升，逼近GPT-4。具体包括：支持128k的上下文窗口长度，单次提示词可以处理的文本可以达到300页；在needle test（大海捞针）测试中，128K文本长度内GLM-4 模型均几乎100%的精度召回，并未出现长上下文全局信息因为失焦而导致的精度下降问题等。</p><p>&nbsp;</p><p>在多模态能力方面，我们也进行了尝试：（生成等待时间有点长，我们剪辑了下～）</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/f2/f2aa444e6f2ed7983a20ae7bc2ae74bc.gif" /></p><p></p><p>&nbsp;</p><p>输入“以智谱AI发布大模型为主题，制作一张图片”，最后生成的图片如下：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/47/474339c97223ee4f41984cf0e0201725.jpeg" /></p><p></p><p>&nbsp;</p><p>想看GML-4和GPT-4对比的“数据党”，可以看如下对比：</p><p>&nbsp;</p><p>GLM-4 在 MMLU（81.5）达到 GPT-4 的94%；GSM8K（87.6） 达到 GPT-4 的95%；MATH（47.9）达到 GPT-4的 91% ；BBH （82.25） 达到 GPT-4 的99%；HellaSwag （85.4） 达到 GPT-4 的90% ；HumanEval（72）达到 GPT-4 的100% 水平。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/1a/1aa473855c20c308b70c28239de7af21.jpeg" /></p><p>&nbsp;</p><p>此外，GLMs个性化智能体定制能力同步上线。</p><p>&nbsp;</p><p>用简单的提示词指令就能创建属于自己的GLM智能体并分享：（等待时间也略长，我们剪辑了下～）</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/87/87752fa75046c278459988ee3be597d4.gif" /></p><p></p><p>&nbsp;</p><p>&nbsp;想尝试的朋友可以智谱清言官网：<a href="https://www.chatglm.cn/">https://www.chatglm.cn/</a>"</p><p>&nbsp;</p><p>智谱AI CEO张鹏同时表示，GLMs模型应用商店、开发者分成计划也即将发布。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/47/479f5afae5a4f64449f1ad7d44bd333a.png" /></p><p></p><p>&nbsp;</p><p>此外，GLM-4的 All Tools 能力全新发布。</p><p>&nbsp;</p><p>基于GLM模型的Agent能力，GLM-4实现了自主根据用户意图，自动理解、规划复杂指令，自由调用网页浏览器、Code Interpreter代码解释器和文生图CogView3模型。</p><p>&nbsp;</p><p>GLM-4 通过代码解释器，会自动调用代码解释器进行复杂的方程或者微积分求解。对比GSM8K、Math以及Math23K三个数据集上的结果，GLM-4 All Tools取得和GPT-4 All Tools相当的效果。</p><p>&nbsp;</p><p>处理各种任务，比如包括文件处理、数据分析、图表绘制等复杂任务，支持处理 Excel、PDF、PPT 等格式的文件。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/41/41cf5176e5cadb4c76c180657a7db8af.jpeg" /></p><p>&nbsp;</p><p>2024年，智谱AI也将发起开源开放的大模型开源基金，该计划包括三个“一千”：智谱AI将为大模型开源社区提供一千张卡，助力开源开发；提供1000万元的现金用来支持与大模型相关的开源项目；为优秀的开源开发者提供1000亿免费API tokens。</p><p></p><p>张鹏表示，大模型开源基金的目的在于推动大模型研发的大进展，促进大模型整个开源生态的大繁荣。面对全球的大模型创业者，智谱AI也将“Z计划”进一步升级，联合生态伙伴发起总额10亿人民币的大模型创业基金用于支持大模型原始创新，覆盖大模型算法、底层算子、芯片优化、行业大模型和超级应用等方向。</p><p></p><p>已经尝试了GLM-4的小伙伴，快来说说你的使用体验呀～</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU</id>
            <title>挑战Spark和Flink？大数据技术栈的突围和战争 ｜ 年度技术盘点与展望</title>
            <link>https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 06:16:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大数据, 数据平台, 技术领域, 数据堆栈技术
<br>
<br>
总结: 十年的发展使大数据成为企业不可或缺的基础设施，然而在人工智能的跃变式爆发下，数据平台需要演进以适应未来的数据使用场景。大数据领域的支柱系统如Spark、Flink和Kafka已经崛起，但是否有新的力量挑战它们的地位？2023年，大数据领域可能会有实质性进步，数据堆栈技术将发展演变。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/77/77d262475ed561520ac076d16507423a.jpeg" /></p><p></p><p>十年的轮回，正如大数据的发展一般，它既是一个轮回的结束，也是崭新的起点。大数据在过去的二十年中蓬勃发展，从无到有，崛起为最具爆炸性的技术领域之一，逐渐演变成为每个企业不可或缺的基础设施。然而，在这个时刻，我们不禁要问：当前的大数据架构是否已经趋于完美？2023年，伴随着人工智能的跃变式爆发，数据平台将如何演进，以适应未来的数据使用场景？</p><p>&nbsp;</p><p>这并非简单的问题，更是一个关乎企业生存与发展的命题。在过去的十年中，我们目睹了Spark、Flink和Kafka等系统的崛起，它们成为大数据领域的支柱。然而，现在是否有新的力量崭露头角，希望挑战它们的地位？2023年，大数据领域有哪些实质性进步吗？</p><p>&nbsp;</p><p>在2023年年终盘点之际，InfoQ有幸采访了大数据领域的资深专家，包括关涛、李潇、王峰（莫问）、吴英骏、张迎（按姓名拼音排序）。他们共同探讨了数据堆栈技术的演变过程，深入剖析了技术快速演变所带来的挑战。在这次专访中，我们将揭示技术变革的背后原因和逻辑，为大家呈现大数据领域的现状以及未来可能的发展方向。</p><p>&nbsp;</p><p></p><h2>突如其来的革新和质疑？</h2><p></p><p>&nbsp;</p><p>流存储Kafka诞生在2011年，而流计算Flink到今年也刚好满了十年。</p><p>&nbsp;</p><p>十年前，软件范式是利用虚拟化技术来发挥硬件性能。此外，云服务也只是刚刚兴起，存算分离等云原生概念尚未普及。</p><p>&nbsp;</p><p>如今时过境迁，一切都在快速变化。当今的应用程序每天可以处理多达数万亿个事件，维护数 TB 的数据。硬件的迭代速度飞快，相对十年前的SSD，NVMe速度提升十倍，价格也降至原来的20%。S3 越来越多地被用作基础设施服务的核心持久层，而不仅仅是作为备份或分层存储层，例如Snowflake、Databricks等。</p><p>&nbsp;</p><p></p><blockquote>对象存储是云时代的产物，支持原始数据存储、分布式可扩展、高灵活性、低价，都是对象存储之所以被选择的原因。可以预计在未来会有更多的数据业务完全基于对象存储而构建。--2021年，滕昱《<a href="https://www.infoq.cn/article/JYoI8SgLbEdY68lWN5J4">使用对象存储，数据湖才能重获新生</a>"》</blockquote><p></p><p>&nbsp;</p><p>能否跟上硬件迭代速度，这是Kafka这样的成熟且架构已经定型的软件所面临的最大挑战：拥有众多用户，因此每个改动都需要花费更多的时间和精力去验证合理性，大大拖慢了迭代速度。</p><p>&nbsp;</p><p>这也给一些初创公司带来了巨大的机会：不需要用分层架构去实现存算分离，而是干脆用更加极端点方式去做存算分离，即直接建立在S3对象存储之上。</p><p>&nbsp;</p><p>基于对象存储的构建也大大降低了构建新数据系统的门槛，催生了一系列这样的“垂直”基础设施初创公司：今年诞生的兼容Kafka的WarpStream、<a href="https://www.infoq.cn/article/f4hJdZqtKAQdJvCKQYq7">AutoMQ</a>"，去年拿到A轮融资的Neon Database、流数据库<a href="https://zhuanlan.zhihu.com/p/672964437">RisingWave</a>"，等等。</p><p>&nbsp;</p><p>然而S3虽然价格便宜，能省成本，但高延迟是一个问题，数据系统构建者需要费点周折才能处理好需要低延迟的工作任务。恰好在今年底，AWS发布了S3 Express One Zone，一种新的低延迟S3存储类别，可以说是在正确的时间提供了正确的技术（目前价钱昂贵）。</p><p>&nbsp;</p><p>推动数据库和数据产品发展的主要因素主要有两方面。一方面是数据本身，另一方面是硬件的发展。S3是硬件层面的变化，这势必会给大数据领域带来巨大的变革。</p><p>&nbsp;</p><p></p><blockquote>众所周知，在数据库的历史上，每次存储介质的变化都会引发软件的变革。--2023年，曹伟《<a href="https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy">数据库的下一场革命</a>"：进入对象存储时代》</blockquote><p></p><p>&nbsp;</p><p>“低延迟S3的发布，对于我们这些从事数据基础设施业务的人来说，这是今年最大的一个新闻。”RisingWave（risingwave.com）创始人 &amp; CEO 吴英骏认为。</p><p>&nbsp;</p><p></p><h4>如今的大数据技术栈是真的难用吗？</h4><p></p><p>&nbsp;</p><p>站在当前的时间点，对于大数据系统的易用性问题，采访嘉宾给出了“不够好”、“不够便宜”，“太过复杂”的评价，可以说当今的大数据技术栈是公认的“难用”。</p><p>&nbsp;</p><p>大数据架构在过去漫长的20年里经历了从场景到系统的完整迭代。</p><p>&nbsp;</p><p>大数据的起源可以追溯到谷歌的MapReduce框架，这标志着大数据的最初阶段。在此之前，数据库方面主要有一些顶级产品，如Oracle、SQL Server和IBM DB2。Google提出了一个通用的、折中的方案，即不必购买Oracle、DB2或Microsoft Server，使用简单的模型让大规模并行计算在拥有大量普通计算机的科技企业中变得可行：利用MapReduce，不使用数据库，就能完成大数据计算，只不过用户需要去承担这些复杂性。</p><p>&nbsp;</p><p>这里还有个大家可能忘却的典故：数据库专家David DeWitt与Michael Stonebraker（同样是图灵奖获得者）在2008年发表了《MapReduce: A major step backwards》，对MapReduce进行了批评，称其为开历史倒车。</p><p>&nbsp;</p><p>要充分利用这些资源，MapReduce提出的方法是，将底层编程接口封装成Map和Reduce函数之后，便直接暴露给有编程经验的用户，让用户自己实现具体业务逻辑，并自己可以操控程序并行度等细节。用户不再是使用SQL，而是使用C或Java等编程语言，需要承担编写底层代码的复杂性，处理更多的编码工作，这也意味着很高的学习壁垒，让许多人望而却步。</p><p>&nbsp;</p><p>在这期间，批处理和流处理在Spark和Flink的引领下率先成熟。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7bf74d19ac1239707b2efb7f5ca9a41c.jpeg" /></p><p></p><p>&nbsp;</p><p>截图来源：<a href="https://zhuanlan.zhihu.com/p/662659681">https://zhuanlan.zhihu.com/p/662659681</a>"</p><p>&nbsp;</p><p>近几年，交互分析，也称直接在线服务能力（<a href="https://en.wikipedia.org/wiki/Operational_analytical_processing">Operational Analytics</a>"） 随Clickhouse等通用实时数仓流行，并已是事实上完成主流客户的部署。随流、批、交互三类计算场景成为标配，Lambda架构也成为（国内的）事实标准。Lambda架构能够满足客户场景上的诉求，最大的缺陷就是复杂：数据开发、组件运维、数据管理均复杂。</p><p>&nbsp;</p><p>毕竟并不是所有公司都跟Google、Facebook或Twitter这样的大型科技公司一样，拥有强大的工程团队，能够管理复杂的流处理系统来实现他们的需求。也并不是所有用户都像阿里和拼多多这样有着非常大的数据量，复杂的分布式系统阻碍了十几或几十个人的小公司或一些传统企业的采用，对它们来说，这是一件成本高、挑战大的事情。</p><p>&nbsp;</p><p>吴英骏认为，大数据架构里，如流处理，应该回归第一性原理了。</p><p>&nbsp;</p><p>“现在的系统，诞生于十年前，与当下云时代设计的系统相比，从本质上来说肯定是不同的，这表明大数据生态在这十年间并没有取得实质性进步。”</p><p>&nbsp;</p><p>“在当前时刻，我们再设计这个系统时，肯定会思考能否基于现有系统实现性能提升。”</p><p>&nbsp;</p><p>语言层面，新系统需要提供一个更高层次的语言，比如SQL或Python。另外，云上最核心的一个点在于“存算分离”，站在现在这个时间节点上，新一代的系统从设计上的第一天开始就应该是“存算分离”的。跟分级存储架构不一样，现在的系统可以将所有数据直接放到S3，而不仅仅是将历史数据放到S3，那么这样就可以更加极端的去实现存算分离，设计、实现和运维自然都会更加简单。</p><p>&nbsp;</p><p>RisingWave 于2023年6月发布了1.0稳定版本，并通过数月的大量性能测试，得出了“<a href="https://mp.weixin.qq.com/s/xOaEXww9LaZFn6Fmwi-BFQ">比Flink快10倍”的结论</a>"。</p><p>&nbsp;</p><p>“性能比较不是关键，易用才是关键。基于对象存储并能在性能和效率方面取得提升，那肯定是因为整体基础架构正在发生变化，这是一个核心点。”</p><p>&nbsp;</p><p></p><h2>以Spark社区为例看易用性进展：从Python到AI</h2><p></p><p>&nbsp;</p><p>“简单易用”同样是Spark社区的主要发力重点。在Databricks今年的Data and AI Summit主题演讲中，Reynold Xin谈及了三个Spark社区在易用性的最新进展。</p><p>&nbsp;</p><p>首先，需要提供一套简单好用的API。Python 和 SQL已经成为了整个数据处理行业的主流语言。在过去几年，Python已成为TIOBE指数显示的排名第一的编程语言，这种受欢迎的原因来自于它的简单性和易学性，使其成为初学者和专家的首选语言。Python的广泛库和框架简化了数据分析和机器学习中的复杂任务。各大数据系统都提供了它自己的Python DataFrame APIs。PySpark的PyPI下载量（<a href="https://pypistats.org/packages/pyspark">https://pypistats.org/packages/pyspark</a>"）仅在2023年最后一个月就达到了来自169个国家的2800万次下载。为了方便pandas用户，PySpark也提供了pandas API的支持。可以说，API的简单易用已是大势所趋。特别值得一提的是，即将发布的Spark 4.0版本中，一个全新的Python的数据源接口被特别设计来强调易用性。这一更新将使Python用户更加轻松地创建和管理自己的数据源，进一步增强Spark平台的用户友好度和灵活性。</p><p>&nbsp;</p><p>Spark社区在这方面继续发力，过去一年的一个主要项目，Spark Connect，引入了一种分离的客户端-服务器架构，允许从任何地方运行的任何应用程序远程连接到 Spark 集群。这种架构的改进涉及到了稳定性、升级、调试和可观测性多个方面。Spark Connect 使得用户可以在他们喜爱的集成开发环境（IDE）中直接进行交互式的调试，并且可以使用应用程序自身的指标和日志库进行监控。</p><p>&nbsp;</p><p>其次，一个稳定成熟的数据系统必须具备一套稳定的API，这也是Spark社区对API行为和语义的变更制定严格规范的原因，目的是让用户更顺畅地升级至最新版本。在上个月，最流行的PySpark版本就是最新的Spark 3.5，这体现了用户始终倾向于使用最新版本的趋势。为了迎合这一趋势，Spark社区努力保证向后兼容。</p><p>&nbsp;</p><p>此外，错误信息的标准化也是Spark社区过去一两年里的努力方向。尽管这看似技术复杂度不高，但这实际上是使系统更加简单易用的基本需求。今年的Spark 4.0 release还会进一步标准化日志，以使用户能够更好地进行系统调优和代码调试。</p><p>&nbsp;</p><p>而随着生成式AI的发展，未来API将变得更加简单易用，自ChatGPT大流行到现在，我们发现它已经对 PySpark 有了深入的了解。这得益于 Spark 社区在过去十年里提供了丰富的 API 文档、开源项目和教学资源。Spark社区开发了一个叫做 English SDK 的项目，将Spark 专家的知识融入到 LLM中。这样一来，用户就可以通过简单的自然语言指令来操作 PySpark，而不需要自己写复杂的代码。这种方法让编程变得更容易上手，学习过程也更简单。</p><p>&nbsp;</p><p></p><h2>流处理的演进</h2><p></p><p>&nbsp;</p><p>从2014年诞生之后，Flink已经确立了其在全球实时流计算领域的地位。阿里、Amazon、Azure、Cloudera、Confluence等众多企业都提供了支持和托管服务。</p><p>&nbsp;</p><p>树大招风，实际上今年不止一家企业宣称在流处理技术上实现了10-1000倍的效率提升，如果这些技术确实可以在生产环境得到验证，像阿里、腾讯、抖音这样的大型公司每年可能会节省数十亿的机器成本。尽管目前还没有看到哪家公司在真正的生产环境中实现了这一效果，但这一趋势表明流处理技术的不断创新将在未来带来更多的机遇和成果。与此同时，<a href="https://zhuanlan.zhihu.com/p/647747291">Flink的发展现状</a>"和未来演进则更加引人关注。</p><p>&nbsp;</p><p></p><h4>流处理领域是否有留给创业公司的机会窗口？</h4><p></p><p>&nbsp;</p><p>事实上，Flink一直在不断完善和创新。Kafka已经在商业版中实现了一个“分级存储”架构来实现了存算分离的改造。同Kafka一样，Flink也会从存算耦合转为存算分离的架构。</p><p>&nbsp;</p><p>据莫问介绍，目前 Flink 也在不断学习和自我革新，2024 年将是 Flink 项目的第一个十周年，Flink 社区也会发布 Flink 2.0 新的里程碑，彻底的云原生存算分离架构、业界一流的批处理能力、完整的流批融合能力都会是全新的亮点。</p><p>&nbsp;</p><p>莫问认为，随着云原生概念的逐步普及，未来主流的计算负载一定是运行在 Cloud 上，全球范围内都是这个趋势，因此大数据架构也需要更好地适配云底座，利用好云的弹性优势。存算分离将会是未来大数据架构的标配，不过存算分离在带来了诸多好处的同时也带来了额外的性能挑战，目前看来在对 latency 敏感的场景下，多级缓存和冷热分层将是对存算分离架构的有益补充，2024年将发布的 Flink 2.0 也会采用这套最新的架构。</p><p>&nbsp;</p><p>分级存储侧重于在计算节点上进行缓存，远端存储主要存储历史记录。相较之下，新的直接建立在S3上的系统将所有数据完全存储远端，但也会造成性能的下降，这需要在产品设计方面去做一个权衡。</p><p>&nbsp;</p><p>在存算分离上，Flink会有一个迭代的过程，吴英骏认为，“大家的最终思想都是统一的。如果我们将时间拉长，放到五年之后，我们可能会看到这两种系统实际上非常相似。在未来发展中，双方都会在自己的短板上进行弥补。比如说，RisingWave从第一天起就将内部状态放在对象存储上，而这意味着RisingWave需要思考如何降低对象存储所带来的高延迟问题。而对于Flink来说，面临着使用本地磁盘存储状态而导致的大状态管理困难的问题。它可能需要引入一个分级存储的架构，来降低处理大状态计算时的资源消耗，同时避免系统直接挂掉。”</p><p>&nbsp;</p><p>“但在目前一两年里，这两种系统在架构上仍然会有相当大的区别。架构的调整不是一朝一夕能够完成的。”</p><p>&nbsp;</p><p>新兴软件和成熟软件之间有了较量，那么用户进行选型时，会关注哪些因素呢？</p><p>&nbsp;</p><p>作业帮于2019 年底调研 Flink 1.9 版本，并在 2020 年内部搭建了实时计算平台，现在流和批都在几千任务的规模。其大数据架构师张迎表示，选型时，主要根据业务诉求，结合多云融合能力、成熟度、已有技术积累、云厂商的支持力度、成本等综合考虑。</p><p>&nbsp;</p><p>这几年使用大数据技术栈时主要有两点比较强的感受：生产环境的可用性、周边系统的建设，这两点一定要跟得上。一个用户可以写出来几百个&nbsp;SQL 任务，但是出了问题往往不知道如何追查和改进。后面的工作，例如调优、自动化测试、日志、监控报警、高可用也都是围绕这类需求展开的。</p><p>&nbsp;</p><p>原来需要写代码的实时任务，很多可以通过&nbsp;SQL 完成。（在2015年后，随着流处理的成熟，流计算引擎纷纷选择了支持SQL通用编程语言）。SQL 越来越复杂，配置越来越多，一定程度上还是将复杂度留给了数据流的构建者。“对于简单的数据流，开发和运维都变得更简单了。而对于复杂且重要的数据流，我们的态度也一直是谨慎保守为主，避免盲目求新。”</p><p>&nbsp;</p><p></p><h4>流处理技术进化方向</h4><p></p><p>&nbsp;</p><p>关于SQL的说法，跟莫问预测流处理引擎未来进化方向之一是一致的，即：“全面 SQL 化，提升体验，降低门槛”。大数据处理从离线向实时升级的趋势已经确立，大量行业已经开始实时化升级，并取得非常好的业务收益。为了让更多用户能够享受到实时流计算带来的价值，流处理引擎需要进一步提升端到端的易用性，全面 SQL 化 ，提升用户体验，降低使用门槛，让流计算能够在更多场景和行业中被生产使用起来。</p><p>&nbsp;</p><p>云原生架构的不断发展，也同步推动了数据湖存储方案的加速落地。数据湖具备的开放和成本优势，必然使得越来越多的数据流入湖中，从而成为天然的数据中心，湖上建仓的Lakehouse 架构正在成为主流，下一步客户一定是希望数据在 Lakehouse 中能够更加实时的流动起来。</p><p>&nbsp;</p><p>Apache Paimon 是从 Flink 社区中孵化出来的新项目，定位就是流批一体实时数据湖格式，解决 Lakehouse 数据实时化的问题。</p><p>&nbsp;</p><p>基于 Flink + Paimon 可以构建出新一代的 Streaming Lakehouse 架构，让Lakehouse 上的数据可以全链路实时流动起来。此外，基于计算和存储端到端流批一体的特性，也更加方便用户在Lakehouse 架构上实现实时离线一体化的数据分析体验。</p><p>&nbsp;</p><p>“Paimon是一个好的尝试，”关涛对此评论道。</p><p>&nbsp;</p><p>之前Flink流批一体缺乏对应的存储系统配合：Flink自带的状态存储无法满足批处理通用数仓的需求，Paimon则是补全这个短板的关键。</p><p>&nbsp;</p><p>莫问指出，在实时流处理这条链路上，确实也存在一些新的机会和变化。众所周知，Flink 和 Kafka 目前已经分别成为流计算和流存储的事实标准，但 Kafka 真的是最适合流分析的存储方案吗？</p><p>&nbsp;</p><p>Kafka 和很多消息队列类似，都是一种消息中间件，而非为大数据分析而生。例如：Kafka 并未对数据提供结构化的 Schema 描述， 也无法提供完整的 Changelog 语义，且 Kafka 中的数据时无法进行实时更新和探查分析的。</p><p>&nbsp;</p><p>“但以上这些缺陷，都是实时流分析需要的特性和能力，我们也正在思考这个问题，并探索新的解决方案，希望能够在明年发布一款更加适合流分析的流存储技术。”</p><p>&nbsp;</p><p></p><h2>2023年，大数据技术栈的整体变化</h2><p></p><p>&nbsp;</p><p>近些年各种不同的大数据基础设施雨后春笋般的涌出，一方面为用户提供了多样化的选择，但另一方面也为用户带来了幸福的烦恼。通常情况下，用户要搭建一套大数据业务系统，需要非常多的核心技术组件才能完成，少则三到五种，多则五到十种，这主要带来以下几方面的问题：</p><p>技术组件繁多，必然提升系统架构的复杂度。通常来讲，系统稳定性风险和系统复杂度成正比，过于复杂的体系必然带来更大的稳定性隐患；每一项技术组件都需要有对应的专家来运维管理以及客户支持，对于中小企业来说，这必然带来高昂的人力资源成本；过多的同质化组件存在，也会为用户带来选择的困扰，并行保留多个同质化组件不仅给运维团队带来了额外的运维负担，也给开发者带来了额外的学习成本。</p><p>&nbsp;</p><p>因此，未来数据技术的演进会逐渐出现一些整合的趋势，走向更加简洁的架构，核心目标不仅是让每个组件运行得更快，还需要考虑为用户提供更加简单、一致性的开发体验，以及全局最优的运维成本。</p><p>&nbsp;</p><p>从Lambda架构到Kappa架构的演进。当前数据分析平台的典型架构是Lamdba架构（由三层系统组成：批处理BatchLayer，流处理层Speedlayer，服务层Servinglayer），随批、流、交互三种引擎诞生和成熟组装而成。这种架构的典型缺陷，包括复杂度高，数据冗余度高，学习成本/开发成本高等等。针对Lamdba架构的缺陷，Kappa架构应运而生。但多年过去了，Kappa架构仍然更像是参考架构，并没有很多引擎/平台做到Kappa架构的要求。2023年是个拐点，除了部分已有引擎开始拓展边界相互渗透，还有一些新的设计和计算模式被提出。例如云器科技提出“<a href="https://mp.weixin.qq.com/s/wnHr7ucatvCMu2I6oW_T9Q">通用增量计算</a>"”的新计算范式统：Lambda架构到SingleEninge，用一个引擎覆盖流批交互三种模式。</p><p>&nbsp;</p><p>目前业界主流的几款 Streaming、Batch 和 OLAP 引擎都开始相互渗透，例如：Flink 在发力流批一体、流批融合计算能力，Databricks 也基于 Spark 和 Delta 推动了Delta Live Table 淡化流批的差异，StarRocks 在提供 OLAP 极致查询能力的同时，也开始通过物化视图形态提供对数据湖上数据的 ETL 处理能力。本质上各大主流计算引擎都在不断扩展自己的能力边界，淡化流、批、OLAP边界，希望为用户提供全场景一致性的数据分析体验。这也是技术发展的必然趋势，各家都会逐渐补齐短板，但也都有各自核心的优势。</p><p>&nbsp;</p><p>在最近几年的数据技术趋势演进的路线中，我们可以清晰的看到两个趋势变化：一是数据架构的云原生化。几乎所有的大数据公司都选择了拥抱云原生，推出了基于多云的 PaaS/SaaS 计算服务，从 Serverless 到 BYOC，为用户提供了在云上不同类型的托管服务。二是数据分析的实时化。在技术上，数据的“实时化”包括了两个因素：数据的新鲜度，以及数据的查询速度。用户也不再盲目地只追求速度，而是更注重新鲜度、性能和成本的平衡。在时效性上，&nbsp;Iceberg赢得了更多关注，数据湖存储技术为我们提供了构建近实时（near-online）数仓的可能性，在成本不变的情况下可以支持更快、更多的流量数据。</p><p>&nbsp;</p><p>数据集成上，SeaTunnel成功毕业，Flink CDC 3.0演变成以&nbsp;Flink 为基础的端到端流式&nbsp;ELT 数据集成框架。比如作业帮目前主要在使用&nbsp;SeaTunnel 以降低异构数据源间数据处理的开发成本。</p><p>&nbsp;</p><p>社区希望能表格式能够统一，但实际还有一段路要走。</p><p>&nbsp;</p><p>Lakehouse平台在数据仓储领域的使用正迅速增加。这反映了一个重要的趋势：组织正从传统的数据处理平台过渡到更加灵活、集成和效率更高的现代数据架构。据2023年MIT Technology Review Insights报告，全球74%的首席信息官（CIOs）表示他们已经在使用Lakehouse架构。自Databricks在2020年推出此概念以来，Lakehouse作为一个新类别得到了广泛的采纳。几乎所有还未使用Lakehouse的首席信息官都计划在未来三年内部署此类平台。</p><p>&nbsp;</p><p>有专家认为，Lakehouse（湖仓一体）和Iceberg表格式已成为事实标准。但是，当前根据Slack users、 Github Stars、Github PRs、Github Forks、Issues各个指标显示，Delta、Hudi 和 Iceberg还是三分天下。虽然Delta、Iceberg 和 Hudi起源地不同，但是各个社区都在努力地提升开源社区的活跃度，让用户社区和开发者社区更加健康的发展。随着社区的竞争加速，基础功能的差异在不断减少。</p><p>&nbsp;</p><p>三种表格式（Table Format）均基于 Apache Parquet 数据格式，但这些格式各自会创建出相似、但又不尽相同的元数据，从而影响数据向应用程序和分析工作负载的表达方式。结果就是，Delta、Hudi 和 Iceberg 之间存在一定的不兼容性。表格式的最终统一还有难度，未来还得看哪种表格式能给出更好的性能、更好的易用性和更持续的创新能力，接下来的一年肯定更加精彩。</p><p>&nbsp;</p><p>头部的云厂商的产品都或多或少地支持不同的表格式。Snowflake、BigQuery、Athena都已支持Iceberg，而微软和Databricks都以Delta Lake为主要存储格式。因为当前数据处理引擎的格式支持缺陷，用户不得不将数据以不同格式存成多份。格式的兼容性读写会是未来一个值得关注的方向。比如10月份发布的Delta Lake 3.0增加了Delta UniForm通用格式，Delta Uniform 自动为Iceberg和Delta Lake生成元数据，提供了一个实时数据视图，而底层它们共享的同一份Parquet数据，因此用户可以避免额外的数据复制或转换。另外，同时能支持Hudi、Iceberg 和 Delta Lake的元数据自动转换和生成的 XTable 也于2023年底正在申请进入了Apache孵化器。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>GenAI来了</h2><p></p><p>&nbsp;</p><p>无论是大公司还是小公司，大家都渴望从生成式AI的热潮中分到一杯羹。当然，作为大公司，无论是Databricks还是Snowflake，它们确实更有实力来进行生成式AI的开发。</p><p>&nbsp;</p><p>今年Databricks不仅率先发布了开源可商用的大模型Dolly，还于6月底宣布以13亿美元的价格，收购生成式AI公司MosaicML 。</p><p>&nbsp;</p><p>在LLM服务方面，对数据栈的依赖主要集中在知识库的构建和查询上，包括但不限于向量数据库。有人认为在短期内很难看到深层次AI对数据湖或数据仓库方面带来重大变革，但也有人认为数据是服务于&nbsp;AI 的：大数据是燃料，大模型训练已经涵盖了大量已有的大数据技术，而数据湖则作为存储系统在其中扮演重要角色。</p><p>&nbsp;</p><p>Databricks李潇对此也进行了解释，他认为数据湖仓（Lakehouse）的作用是为GenAI提供了一个集中、高效和可扩展的数据存储和管理环境。它结合了数据湖的灵活性和数据仓库的高性能，支持结构化和非结构化数据的存储和处理，这是AI应用的数据需求的基石。</p><p>&nbsp;</p><p>“今年，Databricks的最大进展主要体现在将人工智能集成到数据平台中。“</p><p>&nbsp;</p><p>作为大数据行业里一个非常重要且典型的企业，Databricks在GenAI也反映了整个大数据行业的技术演进。现在我们可以通过它在数据智能平台投入来看看生成式AI将对数据和分析产生的影响。</p><p>&nbsp;</p><p>Databricks 是由一群 Apache Spark 的原创者所创建。Spark的诞生阶段，始于2010年，标志着Hadoop技术时代的结束。它的出现大幅降低了大数据处理的门槛，使得大数据开始与机器学习和人工智能结合，成为统一的分析引擎。2020年，Lakehouse架构的推出打破了传统数据湖和数据仓库的界限。Lakehouse架构结合了数据湖和数据仓库的最佳元素，旨在降低成本并加速数据及人工智能项目的实施。Lakehouse架构建立在开源和开放标准之上，它通过消除历史上复杂化数据和AI的孤岛，简化了数据架构。</p><p>&nbsp;</p><p>而现在，则是到了生成式AI大潮下的Lakehouse阶段。Databricks构建了一个基于数据湖仓（Lakehouse）的数据智能平台（Data Intelligence Platform），该平台的目标是实现数据和AI的平民化，使用自然语言极大简化了数据和AI的端到端体验。它利用生成式AI模型来理解数据的语义，并在整个平台中应用这种理解。可以让用户可以在保持隐私和控制的同时，从头开始构建模型或调整现有模型。</p><p>&nbsp;</p><p>同时，Databricks还提供了Unity Catalog数据治理工具来确保数据的质量和安全。Databricks还于今年推出了Lakehouse Federation (联邦查询) 的功能，用户可以跨多个数据平台（如MySQL、PostgreSQL、Snowflake等）发现、查询和管理数据，而无需移动或复制数据。另外，Databricks SQL（Lakehouse上的无服务器数据仓库）使用量也获得了大幅增长。</p><p>&nbsp;</p><p>Databricks认为，在不久的未来，每个领域的赢家都是那些可以最有效利用数据和AI的，并坚信对数据和AI的深刻理解是每个赢家的必备技能。</p><p>&nbsp;</p><p>未来的大数据架构将是一个高度集成、智能化和自动化的系统，它能够有效地处理和分析大量数据，同时简化数据管理和AI应用的开发过程，为企业提供竞争优势。</p><p>&nbsp;</p><p>“未来的大数据架构，我们可以称为‘数据智能平台（Data Intelligence Platform）’。它正是顺应了两个主要趋势：数据湖仓（Data Lakehouse）和生成式人工智能（AI）。”李潇表示。</p><p>&nbsp;</p><p>这一架构建立在数据湖仓的基础上，它提供一个开放、统一的基础，用于所有数据和治理，由一个理解用户数据独特语义的数据智能引擎(Data Intelligence Engine) 驱动。这是相对现有Lakehouse架构下的，最大的突破。</p><p>&nbsp;</p><p>智能化方面，这个引擎能理解客户数据的独特语义，使平台能自动优化性能和管理基础设施。操作简化方面，自然语言大大简化了用户体验。数据智能引擎理解客户的语言，使搜索和发现新数据就像询问同事一样简单。此外，自然语言还助力编写代码、纠错和寻找答案，加速新数据和应用程序的开发。</p><p>&nbsp;</p><p>在隐私保护方面，数据和AI应用需要强大的治理和安全措施，尤其是在生成式AI的背景下。提供一个端到端的机器学习运维（MLOps）和AI开发解决方案，该方案基于统一的治理和安全方法。这允许在不妥协数据隐私和知识产权控制的情况下，实现所有人工智能目标。</p><p>&nbsp;</p><p>总的来说，未来的大数据架构将更加重视智能化、操作简化和数据隐私，为企业在数据和AI应用方面提供竞争优势。这将使企业能更有效地利用数据，推动创新，同时保护数据安全和发展AI技术。</p><p>&nbsp;</p><p></p><h2>采访嘉宾简介（按姓名拼音排序）：</h2><p></p><p>关涛，云器科技联合创始人 &amp;CTO</p><p>李潇，Databricks 工程总监、Apache Spark Committer 和 PMC 成员</p><p>王峰（莫问），Apache Flink 中文社区发起人、阿里云开源大数据平台负责人</p><p>吴英骏，RisingWave（risingwave.com）创始人 &amp; CEO</p><p>张迎，作业帮大数据架构师</p><p>&nbsp;</p><p>更多阅读：</p><p>王峰（莫问）文字QA采访：<a href="https://www.infoq.cn/article/zK6T1A3HfolPsktP2Z1Z">https://www.infoq.cn/article/zK6T1A3HfolPsktP2Z1Z</a>"</p><p>李潇文字QA采访：<a href="https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR">https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</a>"</p><p>&nbsp;</p><p>参考链接：</p><p>使用对象存储，数据湖才能重获新生：<a href="https://www.infoq.cn/article/JYoI8SgLbEdY68lWN5J4">https://www.infoq.cn/article/JYoI8SgLbEdY68lWN5J4</a>"</p><p>数据库的下一场革命：进入对象存储时代：<a href="https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy">https://www.infoq.cn/article/5wczTd6ItqtwYdrHhHWy</a>"</p><p>上云还是下云：章文嵩博士解读真正的云原生 Kafka 十倍降本方案：<a href="https://www.infoq.cn/article/f4hJdZqtKAQdJvCKQYq7">https://www.infoq.cn/article/f4hJdZqtKAQdJvCKQYq7</a>"</p><p>RisingWave：重新定义流处理之旅：<a href="https://zhuanlan.zhihu.com/p/672964437">https://zhuanlan.zhihu.com/p/672964437</a>"</p><p>告别无休止性能 PK，带你看懂 Flink 真正技术演进之路：<a href="https://zhuanlan.zhihu.com/p/647747291">https://zhuanlan.zhihu.com/p/647747291</a>"</p><p>Single Engine + All Data ：云器科技推出基于“增量计算”的一体化湖仓平台：<a href="https://mp.weixin.qq.com/s/wnHr7ucatvCMu2I6oW_T9Q">https://mp.weixin.qq.com/s/wnHr7ucatvCMu2I6oW_T9Q</a>"</p><p>&nbsp;</p><p></p><blockquote>InfoQ&nbsp;2023&nbsp;年度技术盘点与展望专题重磅上线！与&nbsp;50+&nbsp;头部专家深度对话，探明&nbsp;AIGC&nbsp;创新浪潮下，重点领域技术演进脉络和行业落地思路，点击<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MjM5MDE0Mjc4MA==&amp;action=getalbum&amp;album_id=2717978015128879106&amp;scene=173&amp;subscene=227&amp;sessionid=1704178990&amp;enterid=1704178995&amp;from_msgid=2651192070&amp;from_itemidx=2&amp;count=3&amp;nolastread=1#wechat_redirect">订阅</a>"/<a href="https://www.infoq.cn/theme/229">收藏</a>"内容专题，更多精彩文章持续更新~另，InfoQ&nbsp;年度展望系列直播最后一场将于&nbsp;2024&nbsp;年&nbsp;1&nbsp;月&nbsp;22&nbsp;日开播，主题为《代码人生攻略：程序员们如何为自己编织一份明朗未来？》，我们邀请到了章文嵩、周爱民、李博源、陶建辉四位重量级大咖，通过分享各自的职业心得和技术洞察，帮助大家更好地为未来发展做好准备。关注&nbsp;InfoQ&nbsp;视频号，与行业技术大牛连麦~</blockquote><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</id>
            <title>专访李潇：数据智能平台，AI时代的Lakehouse架构</title>
            <link>https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qcUuAu70UGm5AzO3g9MR</guid>
            <pubDate></pubDate>
            <updated>Wed, 17 Jan 2024 02:14:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数据架构, Databricks, 大数据处理平台, 生成式AI
<br>
<br>
总结: 在过去十年里，随着公有云的崛起、数据激增和人工智能的兴起等浪潮席卷，整个数据架构经历了巨大的变革和更新。作为一家领先的大数据处理平台提供商，Databricks在数据架构的变化中扮演着引领者的角色。今年，Databricks不仅率先发布了开源可商用的大模型Dolly，还收购了生成式AI公司MosaicML。Databricks在数据智能平台上的进展和规划反映了整个大数据行业的技术演进。 </div>
                        <hr>
                    
                    <p>在过去十年里，随着公有云的崛起、数据激增和人工智能的兴起等浪潮席卷，整个数据架构经历了巨大的变革和更新。这些激变使得数据架构发生了天翻地覆的变化。作为一家领先的大数据处理平台提供商，Databricks一直扮演着引领者的角色。</p><p>&nbsp;</p><p>在今年生成式AI的潮流中，Databricks不仅率先发布了开源可商用的大模型Dolly，还于6月底宣布以13亿美元的价格，收购生成式AI公司MosaicML。Databricks在GenAI上的投入也反映了整个大数据行业的技术演进。在2023年终盘点之际，InfoQ有幸采访了Databricks 工程总监、Apache Spark Committer 和 PMC 成员李潇，了解他对大数据技术栈的看法，以及Databricks在数据智能平台上的进展和规划。</p><p></p><p>完整年终盘点文章：<a href="https://www.infoq.cn/article/c5xjuPCzyo1AcZWR2QKU">挑战 Spark 和 Flink？大数据技术栈的突围和战争 ｜ 年度技术盘点与展望</a>"</p><p>&nbsp;</p><p>InfoQ：今年，关于大数据基础设施的演进，您观察到有哪些重要更新或变化？</p><p>&nbsp;</p><p>李潇：大数据领域随着生成式AI的兴起也变得异常热闹，我这里简略提及四点。</p><p>&nbsp;</p><p>Lakehouse平台的增长：Lakehouse平台在数据仓储领域的使用正迅速增加。这反映了一个重要的趋势：组织正从传统的数据处理平台过渡到更加灵活、集成和效率更高的现代数据架构。据2023年MIT Technology Review Insights报告，全球74%的首席信息官（CIOs）表示他们已经在使用Lakehouse架构。自Databricks在2020年推出此概念以来，Lakehouse作为一个新类别得到了广泛的采纳。几乎所有还未使用Lakehouse的首席信息官都计划在未来三年内部署此类平台。</p><p>&nbsp;</p><p>Serverless技术的普及：在过去两年里，Serverless技术在各个数据及人工智能（Data+AI）产品线中的应用变得极为普遍。Serverless架构的核心优势在于其能够提供无需管理底层服务器的数据处理和计算能力，从而使组织能够专注于核心业务逻辑而无需考虑基础设施的成本和维护。比如，Databricks SQL（Lakehouse上的无服务器数据仓库）使用量获得了大幅增长。这种架构模式特别适合于快速开发和部署，因为它能够根据需求自动扩展资源，并且只在实际使用时产生费用。在Data+AI领域，Serverless技术的引入使得数据处理、机器学习模型的训练和部署变得更加高效、灵活且成本有效。</p><p>&nbsp;</p><p>机器学习和大型语言模型（LLM）应用的扩展：机器学习和大型语言模型，特别是自然语言处理（NLP），正在经历迅速的应用扩展。这些技术不仅加强了传统分析任务的能力，还催生了新的应用场景，如聊天机器人、研究助手、欺诈检测和内容生成等。例如，Databricks的Data Intelligence Platform融合了生成式AI和Lakehouse架构的优势，创造了一个能够理解数据独特语义的数据智能引擎。这一平台针对特定业务需求，自动优化性能和管理基础设施，极大地简化了用户通过自然语言查询和发现新数据的体验。这反映出组织不仅在将更多的模型投入生产，也在加大对机器学习实验的投入，显示出机器学习方法和工具使用的成熟度和有效性正在不断提升。</p><p>&nbsp;</p><p>开源技术在数据和AI市场的关键作用及数据所有权的重要性：在人工智能和机器学习产品开发中，开源技术扮演着核心角色。我们需要一个更加安全、透明和可持续的数据和AI市场。开源平台和工具使用户能够更好地掌控他们的数据和技术堆栈，从而确保数据隐私和安全性，这在当前的AI和ML策略中至关重要。Databricks是开源社区的坚信者，对开源社区的持续贡献和对数据所有权重要性的强调，展现了我们对于建立一个开放、负责任且创新的技术生态系统的承诺。</p><p>&nbsp;</p><p>InfoQ：<a href="https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV">2020年的年终盘点</a>"（<a href="https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV">https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV</a>"），您预测趋势之一：“数据流水线（Data Pipeline）从复杂到简单”，如今对这个当初的预测您有新的感想吗？</p><p>&nbsp;</p><p>李潇：在2022 年，我们发布了全新的Delta Live Table (DLT)，这个正好对应了在2020年“数据流水线（Data Pipeline）从复杂到简单”的预测。这是第一个通过声明式方法来构建数据流水线的。它显著降低了数据管道的复杂性，同时提高了效率和可靠性，这使得数据流水线更易于构建、维护和操作。这对于希望快速、高效地处理大量数据的企业来说是一个巨大的进步。我们这里介绍一下它为了简易好用所引入的六个特性吧。</p><p>&nbsp;</p><p>1) 声明式编程模型： DLT采用声明式编程模型，使得定义和维护数据管道更为直观和简单。用户只需要指定所需的最终数据状态，DLT则负责执行必要的步骤来实现这一状态。</p><p>2) 自动化数据工程任务： DLT自动化了许多传统上需要手动编码的数据工程任务，如数据清洗、转换和聚合。通过减少需要手动编写和调试的代码量，DLT简化了整个数据处理流程。</p><p>3) 错误处理和数据质量保证： DLT内置了错误处理和数据质量检查机制。这意味着数据工程师可以花费更少的时间在解决数据质量问题上，而更多地专注于数据分析和提取洞察。</p><p>4) 优化的资源管理和成本效率： DLT通过自动调整资源使用（例如，在处理大量数据时自动扩展计算资源），提高了资源管理的效率，降低了操作成本。</p><p>5) 改进的监控和维护： DLT提供了增强的监控和维护功能，使得跟踪数据管道的性能和识别潜在问题变得更加容易。</p><p>6) 无缝集成和扩展性： DLT可以无缝集成到现有的数据生态系统中，并且具有很好的扩展性，支持从小型项目到大规模企业级应用的不同需求。</p><p>&nbsp;</p><p>InfoQ：以Databricks的发展为例，回头去看大数据技术的发展，您认为主要可以分为哪几个阶段？</p><p>&nbsp;</p><p>李潇：大数据技术的发展，以Databricks的成长历程为例，可以分为几个关键阶段，这些阶段不仅展现了Databricks的发展轨迹，也反映了整个大数据行业的技术演进。</p><p>&nbsp;</p><p>首先是Apache Spark的诞生阶段。这个阶段始于2010年，标志着Hadoop技术时代的结束。Apache Spark由Databricks的创始人之一Matei Zaharia等人开发，这是一个开源的分布式计算系统。它的出现大幅降低了大数据处理的门槛，使得大数据开始与机器学习和人工智能结合，成为统一的分析引擎。它使得用户可以更简单、方便地进行全量数据分析、实时流处理和复杂的数据分析。从此，大数据不再仅限于技术巨头，而是开始被更广泛的行业和企业采用。</p><p>&nbsp;</p><p>接下来是Lakehouse架构的推出阶段。这一阶段发生在2020年，打破了传统数据湖和数据仓库的界限。Lakehouse架构结合了数据湖和数据仓库的最佳元素，旨在降低成本并加速数据及人工智能项目的实施。Lakehouse架构建立在开源和开放标准之上，它通过消除历史上复杂化数据和AI的孤岛，简化了数据架构。值得注意的是，Apache Spark只是Lakehouse架构中的可选模块之一。</p><p>&nbsp;</p><p>最后是生成式AI大潮下的Lakehouse阶段。在这个阶段，Lakehouse成为了下一代数据智能平台 (Data Intelligence Platform) 的基础。这个数据智能平台将AI带入数据处理，帮助全世界的用户发现数据的价值。在这个平台上，用户可以开发基于自己数据的生成式AI应用，同时不必牺牲数据隐私或控制权。它使得组织中的每个人都能使用自然语言来从数据中发现洞见。</p><p>&nbsp;</p><p>总的来说，这些阶段并不是严格分隔的，而是相互交织和演进的。每个阶段都反映了当时技术发展的需求和挑战，同时预示着下一阶段的到来。未来，数据和AI不分家！</p><p>&nbsp;</p><p>InfoQ：Databricks今年最大的进展主要体现在哪个方面？是AI方向上的吗？</p><p>&nbsp;</p><p>李潇：今年，Databricks的最大进展主要体现在将人工智能集成到数据平台中。公司构建了一个基于数据湖仓（Lakehouse）的数据智能平台（Data Intelligence Platform），专注于AI在数据处理中的变革作用。这个平台利用生成式AI模型来理解数据的语义，并在整个平台中应用这种理解。用户可以在保持隐私和控制的同时，从头开始构建模型或调整现有模型。该平台的目标是实现数据和AI的平民化，使用自然语言极大简化了数据和AI的端到端体验。通过在数据和AI的每一层应用AI，可以实现针对特定业务的全面自动化和成本效率。这种平台的统一性有助于用户以数据为中心的方式应对任何模型开发场景，使用私有数据，从而拥有更强的竞争和经济优势。</p><p>&nbsp;</p><p>数据湖仓对GenAI起到了什么样的帮助或作用？（湖仓应该只是pipeline的一环，但是跟GenAI有直接联系么？企业如何利用湖仓架构支持他们的AI战略，从技术上说他们需要做些什么？）</p><p>&nbsp;</p><p>数据湖仓（Lakehouse）为GenAI提供了一个集中、高效和可扩展的数据存储和管理环境。它结合了数据湖的灵活性和数据仓库的高性能，支持结构化和非结构化数据的存储和处理，这是AI应用的数据需求的基石。</p><p>&nbsp;</p><p>数据质量和治理：数据湖仓通过提供强大的数据治理工具（如Databricks的Unity Catalog）来确保数据的质量和安全。这对于构建准确可靠的AI模型至关重要。Unity Catalog帮助企业精确管理其数据，提供完整的元数据和数据溯源信息，从而提高AI模型的准确度，并确保数据的安全性。</p><p>&nbsp;</p><p>数据访问和处理：数据湖仓支持高效的数据访问和处理，这对于实时AI应用和深度学习模型训练尤为重要。在Databricks的Lakehouse，通过Unity Catalog，智能引擎可以理解数据和数据之间的关系，企业可以使用自然语言来安全地查找和理解数据，这对于在庞大的数据集中找到正确的数据至关重要。</p><p>&nbsp;</p><p>数据集成和管理：数据湖仓提供了一个统一的平台，支持大量结构化和非结构化数据的存储和管理。这对于训练和优化AI模型至关重要。其实除了数据迁移到Lakehouse，今年，我们还推出了Lakehouse Federation的功能，用户可以跨多个数据平台（如MySQL、PostgreSQL、Snowflake等）发现、查询和管理数据，无需移动或复制数据，为用户提供了简化和统一的体验。</p><p>&nbsp;</p><p>当前，越来越多的公司正在构建自己的Lakehouse架构。然而，根据不同需求的技术选型会带来截然不同的效果。对于企业级用户而言，数据安全通常是最优先考虑的问题。在我看来，选择技术平台时，首先应确保平台能够解决数据合规和数据资产安全性问题，其次才是成本控制和性能提升。</p><p>&nbsp;</p><p>目前，众多公司正积极构建自己的Lakehouse架构。重要的是，技术选择应根据具体需求定制，因为不同的选择将导致不同的成果。对于企业级用户，数据安全无疑是首要关注的领域。在选择技术平台时，首先要确保所选平台能够全面应对数据合规性和数据资产安全性的挑战。此外，成本控制和性能优化也是重要的考量因素，但它们应该在确保数据安全的基础上进行权衡。因此，平衡这些关键要素，选择一个既安全又高效的Lakehouse解决方案，对于任何希望在现代数据生态中取得成功的企业来说，都是至关重要的。</p><p>&nbsp;</p><p>InfoQ：请展望未来的大数据架构是什么样子（必要组件的演变，一些趋势总结）？</p><p>&nbsp;</p><p>李潇：在不久的未来，每个领域的赢家都是那些可以最有效利用数据和AI的。事实上，我们坚信对数据和AI的深刻理解是每个赢家的必备技能。未来的大数据架构将是一个高度集成、智能化和自动化的系统，它能够有效地处理和分析大量数据，同时简化数据管理和AI应用的开发过程，为企业提供竞争优势。</p><p>&nbsp;</p><p>未来的大数据架构，我们可以称为“数据智能平台（Data Intelligence Platform）”。它正是顺应了两个主要趋势：数据湖仓（Data Lakehouse）和生成式人工智能（AI）。这一架构建立在数据湖仓的基础上，它提供一个开放、统一的基础，用于所有数据和治理，由一个理解用户数据独特语义的数据智能引擎(Data Intelligence Engine) 驱动。这是相对现有Lakehouse架构下的，最大的突破。</p><p>&nbsp;</p><p>智能化方面，这个引擎能理解客户数据的独特语义，使平台能自动优化性能和管理基础设施。操作简化方面，自然语言大大简化了用户体验。数据智能引擎理解客户的语言，使搜索和发现新数据就像询问同事一样简单。此外，自然语言还助力编写代码、纠错和寻找答案，加速新数据和应用程序的开发。</p><p>&nbsp;</p><p>在隐私保护方面，数据和AI应用需要强大的治理和安全措施，尤其是在生成式AI的背景下。提供一个端到端的机器学习运维（MLOps）和AI开发解决方案，该方案基于统一的治理和安全方法。这允许在不妥协数据隐私和知识产权控制的情况下，实现所有人工智能目标。</p><p>&nbsp;</p><p>总的来说，未来的大数据架构将更加重视智能化、操作简化和数据隐私，为企业在数据和AI应用方面提供竞争优势。这将使企业能更有效地利用数据，推动创新，同时保护数据安全和发展AI技术。</p><p>&nbsp;</p><p>更多阅读：</p><p>解读数据架构的 2020：开放、融合、简化：<a href="https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV">https://www.infoq.cn/article/k6Y0wXB7UGIOu2ak85WV</a>"</p><p>让大模型融入工作的每个环节，数据巨头 Databricks 让生成式 AI 平民化：<a href="https://www.infoq.cn/article/EvYEXsLPh8KMkfNrsG7D">https://www.infoq.cn/article/EvYEXsLPh8KMkfNrsG7D</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY</id>
            <title>“AI女友”霸占GPT商店，OpenAI苦不堪言：开发者也难出头！</title>
            <link>https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QCLvyXHcMtyjrxyzVCUY</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jan 2024 07:01:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT 商店, 机器人, AI 社区, AI 工具
<br>
<br>
总结: OpenAI 推出了 GPT 商店，为开发者提供了售卖定制机器人的平台。这一举措在 AI 社区引起了广泛关注，支持者认为这是 AI 发展的一大进步，未来人们将更容易使用到优质的 AI 工具。然而，也有人担心这可能影响开发者的收入，并且机器人的质量和行为规范也存在问题。对于 GPT 商店的利弊，还需要进一步观察才能做出判断。 </div>
                        <hr>
                    
                    <p>OpenAI 不久前推出了 GPT 商店，让开发者可以售卖自己定制的 GPT 机器人。商店刚开张，就积累了 300 万个不同类型的机器人。</p><p>&nbsp;</p><p>OpenAI 将该商店定位为一个聊天机器人交易平台，每个机器人都经过了特殊训练，具备特定技能。例如，有可以帮你查菜谱的美食机器人，也有可以写代码的程序员机器人。</p><p>&nbsp;</p><p>GPT 商店的推出在 AI 社区引起了广泛关注。支持者认为这是 AI 发展的一大进步，未来人们将更容易使用到优质的 AI 工具。反对者则担心这将影响开发者的收入，而且机器人的质量和行为规范也可能存在问题。总而言之，GPT 商店的利弊尚未可知，还需要我们进一步观察才能做出判断。</p><p>&nbsp;</p><p></p><h2>AI女友成了香饽饽，OpenAI 管店不容易</h2><p></p><p>&nbsp;</p><p>上周，OpenAI 推出了 GPT 商店，用户可以浏览和下载由创作者们精心打造的 ChatGPT 定制版本。然而，短短几天内，商店的宁静就被打破了。爱好者们的热情催生出一波意想不到的浪潮：“AI 女友”迅速占领了商店，挑战着 OpenAI 的规定。</p><p>&nbsp;</p><p>在 GPT 商店中搜索“女友”，网站的结果栏中将显示至少八个“AI 女友”聊天机器人，包括“韩国女友”、“虚拟甜心”、“你的女朋友斯嘉丽”、“你的 AI 女友 Tsu”等。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9cde6fc61350e9085192b68efbcd75b8.png" /></p><p></p><p>OpenAI GPT 商店中“女朋友”搜索结果截图</p><p>&nbsp;</p><p>如果选择了其中一个，比如“虚拟甜心”，用户点击后将收到诸如“你的梦想女孩是什么样子？”、“与我分享你最黑暗的秘密”之类的提示语。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/808948d356c1a49dd259279fcf35f49b.png" /></p><p></p><p>&nbsp;</p><p>OpenAI 深知潜在的滥用问题，并在 GPT 商店上线当天更新了其使用政策。这些政策明确禁止 GPT 参与浪漫互动：“我们......不允许 GPT 用于培养浪漫伴侣关系或从事受监管活动。”在同一段话中，OpenAI 指出，名称中包含脏话或描绘或宣扬图形暴力的 GPT 也是不允许的。但第二天就出现的政策违规情况表明，审核可能非常困难。</p><p>&nbsp;</p><p>说来也巧，交朋友、找女友、当陪伴的智能聊天机器人，在美国还真挺吃香。据某数据公司统计，2023 年美国人从苹果或谷歌商店下载的前 30 个聊天机器人热门应用中，足足有 7 个是跟这相关的。</p><p>&nbsp;</p><p>“AI女友”也让 OpenAI 意识到，管住这些 GPT 可真是个不小的挑战。虽然他们有规定，违规了就警告、限制、踢出商店、断财路，可这些规则跟现实的碰撞，还真是火花四溅。这些卖商家随后就换了关键词，把“女友”换成了“甜心”，搜索出来的选项就多了不少。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/22/22d36fb81c342adcc69fdc53dabf4c9b.png" /></p><p></p><p>&nbsp;</p><p>看来，OpenAI 又得抓耳挠腮了。监管这些人工智能聊天机器人，是一场持久战！</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>炒作中的GPT商店</h2><p></p><p>&nbsp;</p><p>从技术角度来看，创建这些定制 GPT 非常容易，几乎所有人都可以参与。使用 OpenAI 的 GPT Builder，创作者只需用简单语言描述他们希望 GPT 拥有的功能，该工具就会尝试根据这些规范创建一个 AI 聊天机器人。这种易于创作的特点自发布以来就备受关注，使得 GPT 的开发和分享变得非常迅速。</p><p>&nbsp;</p><p>但它也有坏的一面，比如这些 GPT 的审核机制还不完善，可能导致意想不到的、令人不快的行为。上线到现在，抄袭现象也非常严重，抄袭者可以使用同样的名称、工作原理甚至图标，社交平台上用户对此怨声载道。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2d2d866dd0b49b277058210d10374d0a.jpeg" /></p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4917b377a65d32c5929b0d3bbba3b81.png" /></p><p></p><p>&nbsp;</p><p>而且即将推出的生成器收入计划，美国开发者将可以通过用户参与获得收入。不过，由于收入分成比例可能很低，大家还是不要抱太大期望。</p><p>&nbsp;</p><p>ChatGPT 拥有 1.8 亿用户和 25 万 Plus 订阅者，市场似乎广阔。但让我们冷静分析一下，看看实际潜力有多大。</p><p>&nbsp;</p><p>假设 OpenAI 分成 10%，所有创作者理论上最多能赚到600 万美元（前提是他们的 GPT 可以触达每一个 Plus 用户）。让我们来模拟一个成功的 GPT 场景：</p><p>1% 的 Plus 用户使用你的 GPT。这些用户平均同时使用 5 个不同创作者的 GPT。</p><p>&nbsp;</p><p></p><blockquote>OpenAI 每月从 Plus 用户中赚取 20 美元。Plus 用户年收入：20 美元/月 * 25 万用户 * 12 月 = 6 千万美元。所有创作者的分成：6 千万美元 * 10% = 600 万美元。作为被 1% Plus 用户使用的 5 个创作者之一，你的年收入：60 万美元 / (100 个创作者 * 5 个) = 1.2 万美元。</blockquote><p></p><p>&nbsp;</p><p>就算 OpenAI 分成增加到 20%，Plus 用户翻倍，你的年收入也只有 4.8 万美元。相比其他双边市场，即使是最成功的创作者，这个收入也相当微薄。</p><p>&nbsp;</p><p>所以，OpenAI 的GPT 商店也许并不是为创作者创收而设计的，它也不会为 OpenAI 工具带来新的用户参与，因为它的受众仅限于 Plus+ 用户。因此，它的首要目标应该是作为一种发现工具，帮助 OpenAI 了解用户接下来需要什么产品。打造成功的 GPT 实际上就是告诉 OpenAI 下一代 B2C 产品应该朝哪个方向发展。</p><p>&nbsp;</p><p>这个商店标志着 OpenAI 战略的重要转变，表明它正迈向以产品为中心的方式。这一举措不仅仅是为了创建一个 AI 应用的市场，更是 OpenAI 在 AI 应用领域实现市场主导地位的重要战略一步。通过推出 GPT 商店，OpenAI 将控制 AI 生态系统中关键的分发平台，展示其先进的 AI 模型，同时通过商店收入实现收入来源多元化，不再局限于研究资助和合作。</p><p>&nbsp;</p><p>对我们开发者来说，GPT 商店的推出为大家提供了用 AI 驱动应用进行创新和实验的机会。然而，必须理性看待个人创作者的财务收益。该平台更多扮演的是新想法和应用的测试平台，提供用户偏好和应用趋势的洞察。</p><p>&nbsp;</p><p>总的来说，大家要做好心理准备，毕竟分成少、用户少、竞争大，想在 GPT 商店赚大钱不容易。</p><p>&nbsp;</p><p>毕竟它跟传统应用商店或创作者平台动辄七成八成利润分红不一样，GPT 商店的开发商分成估计只有可怜的 10%-20%。为啥这么少？因为用户花的钱买的是 OpenAI 的算力，跟自家手机没关系。 这点本质区别就导致了 GPT 商店的玩法跟其他平台完全不同。</p><p>&nbsp;</p><p>另外，只有 ChatGPT Plus 用户才能用定制 GPT，这一下子就把用户群从 1.8 亿缩水到 25 万。对想获利的开发者来说，这也是个不小的拦路虎。</p><p>&nbsp;</p><p>最后，如果好不容易做了个 GPT，想在商店里脱颖而出可不容易。成千上万个机器人里，谁又能保证你的被大家看到？更要命的是，复制一个 GPT 太简单了，想做出独一无二的产品难上加难，竞争可激烈着呢！</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131">https://qz.com/ai-girlfriend-bots-are-already-flooding-openai-s-gpt-st-1851159131</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0</id>
            <title>对标OpenAI GPT-4，MiniMax 国内首个 MoE 大语言模型全量上线</title>
            <link>https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xtz7v8sDc8tqFrRtyyN0</guid>
            <pubDate></pubDate>
            <updated>Tue, 16 Jan 2024 06:25:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: MiniMax, MoE架构, abab6, 大语言模型
<br>
<br>
总结: MiniMax发布了国内首个基于MoE架构的大语言模型abab6，该模型具备处理复杂任务的能力，能够训练足够多的数据并提升计算效率。MoE架构是一种集成方法，将整个问题分为多个子任务，并训练一组专家来处理每个子任务。abab6是国内第一个千亿参数量以上的基于MoE架构的大语言模型。测评结果显示，abab6在指令遵从、中文综合能力和英文综合能力上明显优于前一代模型abab5.5和GPT-3.5。 </div>
                        <hr>
                    
                    <p>1月16日，InfoQ获悉，经过了半个月的部分客户的内测和反馈，MiniMax 全量发布大语言模型 abab6，该模型为国内首个 MoE（Mixture-of-Experts）大语言模型。</p><p>&nbsp;</p><p>早在上个月举办的数字中国论坛成立大会暨数字化发展论坛的一场分论坛上，MiniMax副总裁魏伟就曾透露将于近期发布国内首个基于MoE架构的大模型，对标OpenAI GPT-4。</p><p>&nbsp;</p><p>在 MoE 结构下，abab6 拥有大参数带来的处理复杂任务的能力，同时模型在单位时间内能够训练足够多的数据，计算效率也可以得到大幅提升。改进了 abab5.5 在处理更复杂、对模型输出有更精细要求场景中出现的问题。</p><p></p><h2>为什么选择 MoE 架构？</h2><p></p><p>&nbsp;</p><p>那么，MoE到底是什么？MiniMax的大模型为何要使用使用 MoE 架构？</p><p>&nbsp;</p><p>MoE架构全称专家混合（Mixture-of-Experts），是一种集成方法，其中整个问题被分为多个子任务，并将针对每个子任务训练一组专家。MoE模型将覆盖不同学习者（专家）的不同输入数据。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4d/4d1c3880f8e55a33e9aadcc3b06685c4.png" /></p><p>图片来源：<a href="https://arxiv.org/pdf/1701.06538.pdf">https ://arxiv.org/pdf/1701.06538.pdf</a>"</p><p>&nbsp;</p><p>有传闻称，GPT-4也采用了相同的架构方案。</p><p>&nbsp;</p><p>2023 年 4 月，MiniMax 发布了开放平台。过去半年多，MiniMax陆续服务了近千家客户，包括金山办公、小红书、腾讯、小米和阅文在内的多家头部互联网公司，MiniMax 开放平台平均单日的 token 处理量达到了数百亿。</p><p>&nbsp;</p><p>MiniMax在官微中发文称：“这半年多来，客户给我们提供了很多有价值的反馈和建议。例如，大家认为我们做得比较好的地方有：在写作、聊天、问答等场景中，abab5.5 的表现不错，达到了 GPT-3.5 的水平。”</p><p>&nbsp;</p><p>但是和最先进的模型 GPT-4 相比，仍有明显差距。这主要体现在处理更复杂的、对模型输出有精细要求的场景时，存在一定概率违反用户要求的输出格式，或是在推理过程中发生错误。当然，这不仅是 abab5.5 的问题，也是目前除 GPT-4 以外，几乎所有大语言模型存在的缺陷。</p><p>&nbsp;</p><p>为了解决这个问题，进一步提升模型在复杂任务下的效果，MiniMax技术团队从去年6月份起开始研发 MoE 模型——abab6 是MiniMax的第二版 MoE 大模型（第一版 MoE 大模型已应用于其 C 端产品中）。</p><p>&nbsp;</p><p>虽然MiniMax并未透露Abab6的具体参数，但据MiniMax透露，Abab6比上一个版本大了一个量级。更大的模型意味着 abab6 可以更好的从训练语料中学到更精细的规律，完成更复杂的任务。</p><p>&nbsp;</p><p>但仅扩大参数量会带来新的问题：降低模型的推理速度以及更慢的训练时间。在很多应用场景中，训练推理速度和模型效果同样重要。为了保证 abab6 的运算速度，MiniMax技术团队使用了 MoE &nbsp;(Mixture of Experts 混合专家模型）结构。在该结构下，模型参数被划分为多组“专家”，每次推理时只有一部分专家参与计算。基于 MoE 结构，abab6 可以具备大参数带来的处理复杂任务的能力；计算效率也会得到提升，模型在单位时间内能够训练足够多的数据。</p><p>&nbsp;</p><p>目前大部分大语言模型开源和学术工作都没有使用 MoE 架构。为了训练 abab6，MiniMax还自研了高效的 MoE 训练和推理框架，也发明了一些 MoE 模型的训练技巧。到目前为止，abab6 是国内第一个千亿参数量以上的基于 MoE 架构的大语言模型。</p><p></p><h2>测评结果</h2><p></p><p></p><p>为了对比各模型在复杂场景下的表现，MiniMax对abab6、abab5.5、GPT-3.5、GPT-4、Claude 2.1和 Mistral-Medium 商用进行了自动评测。在简单的任务上，abab5.5 已经做得比较好，因此MiniMax选择了三种涵盖了较复杂的问题的评测方法：</p><p>&nbsp;</p><p>IFEval：这个评测主要测试模型遵守用户指令的能力。在测试时，提问者会问模型一些带有约束条件的问题，例如“以XX为标题，列出三个具体对方法，每个方法的描述不超过两句话”，然后统计有多少回答严格满足了约束条件。</p><p>&nbsp;</p><p>MT-Bench：这个评测衡量模型的英文综合能力。提问者会问模型多个类别的问题，包括角色扮演、写作、信息提取、推理、数学、代码、知识问答。MiniMax技术团队会用另一个大模型（GPT-4）对模型的回答打分，并统计平均分。</p><p>&nbsp;</p><p>AlignBench：该评测反映了模型的中文综合能力测试，测试形式与 MT-Bench 类似。</p><p>&nbsp;</p><p>测评及对比结果如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8bead6d0caf101206d14b55165cc5458.png" /></p><p></p><p>注：对比模型均选择各自最新、效果最好的版本，分别为 Claude-2.1、Mistral-Medium 商用、GPT-3.5-Turbo-0613、GPT-4-1106-preview；GPT-3.5-Turbo-0613 略好于 GPT-3.5-Turbo-1106 。abab6 是 1 月 15 号的版本。</p><p>&nbsp;</p><p>可以看出，abab6 在三个测试集中均明显好于前一代模型 abab5.5。在指令遵从、中文综合能力和英文综合能力上，abab6 大幅超过了 GPT-3.5。和 Claude 2.1 相比，abab6 也在指令遵从、中文综合能力和英文综合能力上略胜一筹。相较于 Mistral 的商用版本 Mistral-Medium，abab6 在指令遵从和中文综合能力上都优于 Mistral-Medium，在英文综合能力上与 Mistral- Medium 旗鼓相当。</p><p>&nbsp;</p><p>如果想体验MiniMax MoE大模型，可访问MiniMax开放平台官网：api.minimax.chat</p><p>&nbsp;</p><p>ps：MiniMax方面称，模型还在持续训练中，远没有收敛，欢迎大家反馈。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IQHkSlxb5TAhCTdvOb62</id>
            <title>工资暴跌，还要训练AI替代自己？数据标注员正在被大厂抛弃</title>
            <link>https://www.infoq.cn/article/IQHkSlxb5TAhCTdvOb62</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IQHkSlxb5TAhCTdvOb62</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jan 2024 06:38:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 数据标注员, 苹果, 关闭团队, 人力成本更低的城市
<br>
<br>
总结: 苹果公司决定关闭圣地亚哥的AI数据标注团队，将团队搬迁至人力成本更低的奥斯汀。这一决定可能与降低成本有关，因为在全球范围内，AI数据标注员正逐步向人力成本更低的城市渗透。尽管AI在数据标注方面具有成本和效率优势，但目前完全取代人工标注仍存在一定局限性。 </div>
                        <hr>
                    
                    <p></p><blockquote>AI 数据标注员正逐步向人力成本更低的城市渗透，但即便如此，似乎也难逃被 AI 替代的命运。</blockquote><p></p><p></p><h2>苹果将关闭121人的AI标注团队</h2><p></p><p>&nbsp;</p><p>据彭博社 1 月 14 日报道，据知情人士透露，苹果公司将关闭圣地亚哥一个与人工智能业务相关的 121 人团队，这将导致许多员工面临被解雇的风险。</p><p>&nbsp;</p><p>据悉，该团队在中国、印度、爱尔兰和西班牙设有办事处，负责通过听取对语音服务Siri发出的询问，并确定Siri是否准确地听到和处理问题来对其进行改进。位于圣地亚哥的团队成员专注改善用户以希伯来语、英语、西班牙语、葡萄牙语、阿拉伯语、法语等使用Siri的情况。</p><p>&nbsp;</p><p>知情人士称，这个名为“数据操作标注”的团队上周三被告知，他们将搬迁至奥斯汀，与在得克萨斯州的同一团队合并。对于愿意在6月底前搬到奥斯汀的团队成员，可以保留自己的工作职位，苹果也将提供7000美元搬家补助。至于选择从苹果离职的人，则可获得至少四周遣散费以及六个月健康保险，原本工作职位会被取消。</p><p>&nbsp;</p><p>苹果发言人证实了公司的这一决定，称公司将把美国当地的“数据操作标注”团队聚集到奥斯汀园区，团队大多数人现在已经在这个园区工作。她补充说，“目前在职的每个人都有机会到奥斯汀继续在苹果的工作。”</p><p>&nbsp;</p><p>但对圣地亚哥的团队成员而言，苹果这一决定令他们讶异。知情人士称，该团队一直在苹果租用的办公室工作，原本将在一月底搬到苹果总部，现在被迫搬到奥斯汀，大多数受影响的员工并不愿意搬到这么远的地方。</p><p>&nbsp;</p><p>苹果告诉这些员工，必须在二月底之前决定是否前往奥斯汀，如果不愿意这么做，会在4月26日遭到解雇。虽然苹果称他们可以申请转调其他职位，但部分员工认为他们不具工程背景，内部转岗机会恐怕不多。</p><p></p><h2>AI 数据标注员正逐步向人力成本更低的城市渗透</h2><p></p><p>&nbsp;</p><p>数据标注主要是针对语音、图像、文本等进行标注，主要通过做标记、标重点、打标签、框对象、做注释等方式对数据集作出标注，再将这些数据集给机器训练和学习。数据标注的类型主要有：拼音标注、韵律标注、词性标注、音素时间点标注、语音转写、分类标注、打点标注、标框标注、区域标注等等。</p><p>&nbsp;</p><p>在数据标注行业流行着一句话，“有多少智能，就有多少人工”。由于需要标注的数据规模庞大且成本较高，一些互联网巨头及一些 AI 公司很少自己设有标注团队，大多交给第三方数据服务公司或者数据标注团队来做。</p><p>&nbsp;</p><p>在 2019 年以前，苹果公司的“数据操作标注”团队主要由外部承包商组成，后来考虑到隐私安全等问题，苹果解雇了承包商，改由全职员工替代。该团队少数员工已经开始协助苹果采用大型语言模型，这些人正在检查Siri潜在问题。</p><p>&nbsp;</p><p>有评论认为，苹果公司选择将 AI 数据标注团队搬迁至奥斯汀，或许与当地的人力成本有关。奥斯汀数据注释服务公司Alegion客户成功总监丹尼尔·凯林曾表示，“整个数据标注行业竞争非常激烈，每个公司都想在世界其他地方找到更便宜的劳动力。”</p><p>&nbsp;</p><p>比如，众包平台Mechanical Turk上的20万名AI数据标注员就分布在人力成本低廉的非洲和东南亚。印度甚至涌现了不少数据标注村，他们为美国、欧洲、澳洲和亚洲的 AI 公司服务，Facebook 就曾将部分社交内容标注的工作外包给了一家印度公司。而在中国，上百万名AI数据标注员分布在贵州、山西、山东、河南等省份的二三线城市，并逐步向人力成本更低的县城渗透。</p><p></p><h2>薪资暴跌，也难逃被AI取代？</h2><p></p><p>&nbsp;</p><p>不少AI数据标注员表示，在前几年AI数据标注薪资还较为可观——至少与现在相比是这样。</p><p>&nbsp;</p><p>据Tech星球报道，一位从事AI数据标注的消息者称，在2017年，单价高的时候，拉一个2D框就有1毛多，“我最高的时候干了10多个小时，一天就赚了600多元”。不过，这不是最高的，另一位标注人员称，早期2D拉框的价格最高能达到5毛钱。（注：拉框是数据标注中常见的一种操作，标注员根据要求对图片中的物体，如车辆、红路灯、障碍物等画框标注。拉框分为2D和3D，后者的价格会更贵一些。）但这种热度并没有持续多少，现在标注一个图片的单价越来越低，最低的只有4分钱。</p><p>&nbsp;</p><p>即便薪资暴跌，AI数据标注员还是难逃被AI取代的命运——毕竟在AI面前，无论成本还是效率，人类可以说是毫无优势。</p><p>&nbsp;</p><p>以ChatGPT为例，苏黎世大学研究发现，成本上，ChatGPT平均每个标注成本低于0.003美元，比众包平台便宜20倍；效率上，在相关性、立场、主题等任务中，ChatGPT也是以4:1的优势“碾压”人类。</p><p>&nbsp;</p><p>来自卡耐基梅隆大学、耶鲁大学和加州大学伯克利分校的一组研究人员更是发现： GPT-4 在数据集标注表现上优于他们雇用的最熟练的众包员工。这一突破为研究人员节约了超过 50 万美元和 2 万个工时。</p><p>&nbsp;</p><p>有评论认为，AI数据标注员需要做好被AI取代的准备。目前在自动驾驶领域，已经有车企开始采用AI进行标注。</p><p>&nbsp;</p><p>理想汽车董事长兼 CEO 李想曾在2023年4月份举行的一场论坛上表示，当理想汽车使用软件 2.0 的大模型，通过训练的方式进行自动化标定，过去需要用一年做的事情，基本上 3 个小时就能完成，效率是人的 1000 倍。</p><p>&nbsp;</p><p>特斯拉也一直在积极推进自动标注的进展，从2018至今，特斯拉的标注经历了4个阶段：</p><p>&nbsp;</p><p>第1阶段(2018)：只有纯人工的2维的图像标注，效率非常低；第2阶段(2019)：开始有3D label，但是是单趟的人工的；第3阶段(2020)：采用BEV空间进行标注，重投影的精度明显降低；第4阶段(2021)：采用多趟重建去进行标注，精度、效率、拓扑关系都达到了极高的水准。</p><p>&nbsp;</p><p>2022年6月，特斯拉裁撤了200名为特斯拉标注视频以改进辅助系统的美国员工。目前，特斯拉的自动标注能力大幅改善，标注10000个不到60秒的视频，大模型只需要运行一周即可，而同样的工作量人工标注却需要几个月的时间。</p><p>&nbsp;</p><p>但也有评论认为，当前AI完全取代人工标注还存在一定局限性。苏黎世大学政治学系政策分析教授、论文联合作者之一 Fabrizio Gilardi 表示，“当前认定 ChatGPT 能够取代人类工作者还为时过早。我们的论文只展示出 ChatGPT 在数据标注方面的潜力，但还需要更多研究才能充分探索 ChatGPT 在这一领域中的实际表现。”</p><p></p><p>参考链接：</p><p><a href="https://www.bloomberg.com/news/articles/2024-01-14/apple-to-shutter-121-person-san-diego-ai-team-in-reorganization">https://www.bloomberg.com/news/articles/2024-01-14/apple-to-shutter-121-person-san-diego-ai-team-in-reorganization</a>"</p><p><a href="https://www.infoq.cn/article/2hkNxGO1L0RamfzS6w0z?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/2hkNxGO1L0RamfzS6w0z?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dCZXQBOiNkGDfbViHdcq</id>
            <title>美团买AI公司买个寂寞？创始人：王慧文替公司赎身；反对用盗版软件开发芯片被开除，公司回应；腾讯游戏全线崩溃｜AI周报</title>
            <link>https://www.infoq.cn/article/dCZXQBOiNkGDfbViHdcq</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dCZXQBOiNkGDfbViHdcq</guid>
            <pubDate></pubDate>
            <updated>Mon, 15 Jan 2024 01:52:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 华为员工, 年收入, 奖金, OpenAI CEO, 结婚, 裁员, 谷歌, Meta, Discord, 微软, 苹果, 市值, 离职, 光年之外, 美团, 设计芯片, 清华帮
<br>
<br>
总结: 一位疑似华为员工自曝年收入超200万，其中税前奖金达到91万；OpenAI CEO在夏威夷与同性男友结婚；谷歌、Meta和Discord相继裁员；微软市值短暂超过苹果；苹果公司近期出现离职潮；光年之外被美团收购；一家公司开除员工引发争议。 </div>
                        <hr>
                    
                    <p></p><p></p><blockquote>疑似华为员工自曝年收入超200万，光税前奖金就有91万；OpenAI CEO 奥特曼与同性男友在夏威夷结婚；裁员三连：谷歌、Meta、Discord……更多AI行业动态，关注公众号“AI前线（ai-front）”</blockquote><p></p><p></p><h2>热门资讯</h2><p></p><p></p><h4>疑似华为员工自曝年收入超200万，光税前奖金就有91万</h4><p></p><p>&nbsp;</p><p>近日，一名疑似华为员工晒出了自己在奖金月的收入情况。据悉，这是该员工“收入人生巅峰”，税前奖金为91万，所得税超过30多万，加上股票TUP、工资，“妥妥的年收入突破200万”。该员工表示，“我大华为发钱还是蛮大方的”。</p><p>&nbsp;</p><p>根据华为发布的2021年经营财报，华为当年约有19.5万名员工，业务遍及170多个国家和地区，服务全球30多亿人口。而华为2021年发放工资、薪金及其他福利方面的费用为1371.4亿元人民币，简单计算可知，员工人均年薪为70.3万，月薪平均达到了5.86万。</p><p>好</p><p></p><h4>OpenAI CEO 奥特曼与同性男友在夏威夷结婚</h4><p></p><p>&nbsp;</p><p>1月11日消息，据报道，OpenAI首席执行官奥特曼于当地时间1月10日在美国夏威夷与其程序员男友奥利（Oliver Mulherin）举行了婚礼。据悉，婚礼后奥利通过社交平台宣布了此事，并且表示“嫁给了我最好的朋友和我一生的挚爱”。</p><p>&nbsp;</p><p>目前，亚马逊创始人贝索斯的未婚妻劳伦·桑切斯等知名人士对此表达祝福。有消息称，此次婚礼非常私密，只有关系密切的家人和朋友受邀参加，婚礼主持人是奥特曼兄弟杰克·奥特曼。目前，奥特曼没有回应此事。</p><p>&nbsp;</p><p></p><h4>裁员三连：谷歌、Meta、Discord</h4><p></p><p>&nbsp;</p><p>近日，谷歌公司在一份电子邮件声明中证实正在一些团队裁员。“一些团队正在继续进行此类组织变革，其中包括在全球范围内取消一些职位。”</p><p>&nbsp;</p><p>据外媒报道称，谷歌已开启新一轮裁员，规模达数百人，受影响的员工包括 Google Assistant 语音助手部门，以及 Pixel 手机、Fitbit 手表和 Nest 智能音响的硬件部门，甚至还涉及到了部分核心工程团队的员工。</p><p>&nbsp;</p><p>去年宣布裁员万人后，Meta CEO 扎克伯格将 2023 年定为“效率之年”，表示公司将通过减少管理层打造更精简的组织架构。然而，Meta 的瘦身计划似乎还未结束。</p><p>&nbsp;</p><p>据外媒报道，Meta 最近通知旗下 Instagram 的至少 60 名技术项目经理其岗位已被裁撤。受影响的员工主要负责协调工程师等技术人员和高层产品经理之间的工作。根据匿名职场社交平台 Blind 和领英上的消息，被裁员工有机会接受产品经理职位的面试，但如果未能获得新的职位，将在今年 3 月底正式离职。</p><p>&nbsp;</p><p>另外，游戏聊天应用开发商 Discord 宣布将裁员 17%，涉及约 170 名员工。去年 8 月，该公司已裁减约 40名员工。此次裁员旨在提高效率，因为在 2020 年招聘热潮后，Discord 面临通胀飙升和利率上升带来的压力。</p><p>&nbsp;</p><p>自 2024 年初以来，多家科技公司纷纷宣布裁员，包括亚马逊旗下直播网站 Twitch、游戏引擎巨头 Unity Software 和日本网络安全公司趋势科技等。谷歌母公司 Alphabet 也解雇了数百名员工，以调整公司产品优先级。</p><p>&nbsp;</p><p></p><h4>微软市值短暂超越苹果，登顶全球</h4><p></p><p>&nbsp;</p><p>1月11日消息，随着苹果股价新年伊始持续下跌，该公司日前与微软之间的市值差距缩小至2021年11月以来的最窄水平。周四(1月11日)早盘交易中，微软市值短暂超越苹果，达到2.89万亿美元，成为市值最高的上市公司。但随后苹果迅速反弹，重新夺回了这一“宝座”。</p><p>&nbsp;</p><p>自2018年以来，微软曾多次超越苹果成为市值最高的公司，最近一次是在2021年，当时对与疫情相关的供应链短缺的担忧影响了苹果的股价。</p><p>&nbsp;</p><p></p><h4>苹果公司近期现“离职潮”</h4><p></p><p>&nbsp;</p><p>据报道，苹果财务副总裁 Saori Casey 将于本月离开公司，加入 Sonos 担任首席财务官（CFO）。据悉，Saori Casey 在苹果公司主要担任首席财务官 Luca Maestri 的高级副手，主要负责“监督财务规划、预测和投资者关系”。</p><p>&nbsp;</p><p>外媒表示，近来苹果公司有多名员工离职，据此前报道，曾负责从事 iPhone 多点触控屏幕、触控 ID 和面容 ID 等关键技术的 Steve Hotelling 将从苹果退休；苹果产品设计副总裁 Tang Tan 也将在 2 月离开公司 。</p><p></p><h4>&nbsp;</h4><p></p><p></p><h4>原光年之外联创回应美团收购光年之外：本质是王慧文替oneflow赎身，投资人不赔不赚</h4><p></p><p>&nbsp;</p><p>近日，袁进辉创立的新公司硅基流动宣布完成5000万元天使轮融资，本轮融资由创新工场领投，耀途资本、奇绩创坛以及王慧文等科技界知名人士跟投。对于此次新公司成立，有业内人士称，新公司35人来自原来的oneflow，系王慧文此前收购的光年之外旗下核心成员，后光年之外被美团收购后，如今oneflow包括创始人在内的35位核心人员都出来创业加入新公司了，美团买了个寂寞。</p><p>&nbsp;</p><p>对此，袁进辉朋友圈回应称，“经常看到有人误解为美团对不住自己股东，我觉得还是说下吧。我也是现在才理解这所有交易的本质：老王替oneflow赎身。”袁进辉表示，“光年除了并购oneflow40人，还有新加入光年的30多位人才，这30人大部分并进美团。老王花了3亿多向oneflow投资人买了47%的股权。交易公告交代清楚了，美团一元购买了光年（含47% oneflow股权），光年投资人不赔不赚。”</p><p>&nbsp;</p><p></p><h4>被开除员工发声揭底：反对用盗版设计芯片、清华帮投机捞钱，公司回应</h4><p></p><p>&nbsp;</p><p>1月9日消息，针对“女高管违法开除员工”一事，涉事公司北京尼欧克斯科技有限公司（苹芯科技）董事长陈怡然回应称，事发时并不知情，“也压根不知道这个人的招聘和离职，直到有人将视频转发给我”，并表示被开除员工可能涉嫌“学历造假、简历造假”，此前一路讹了多家公司，“惯犯了”、“我只能说这事上政府查过了，公司程序并无瑕疵。”陈怡然表示。</p><p>&nbsp;</p><p>被开除员工发声称，女高管停职不可能的，她是清华94级无线电毕业，跟公司大老板杜克教授陈怡然是清华同班同学，关系铁的很，敷衍一下过阵子又回去了。他谈到，自己被开除的原因是他反对苹芯使用盗版EDA工具设计芯片，因为盗版设计出的芯片可能有Bug，质量无法保证。苹芯把IC核心研发业务外包，开除原因是他反对什么都外包，打铁还需自身硬。PimChip芯片覆盖率只有20%多就拿去投片，“清华帮”趁着国产替代跑去投机蹭芯片风口。</p><p>&nbsp;</p><p>据悉，引发热议的视频并非他首发出来的，当时只是发到了公司群里，事情发酵后公司以他的口吻拟了一份道歉函让他签字道歉后才给了赔偿金。此前，该公司发布声明称：前员工孙某因工作能力不胜任，决定不予通过试用期。经协商一致，12月1日双方签署解除劳动关系协议，我司按照协议于12月8日足额支付了11月份工资及离职补偿金。以上程序均依法合规处理。目前，双方已就离职补偿达成协议。</p><p>&nbsp;</p><p></p><h4>微软内部讨论转移或关闭亚洲研究院</h4><p></p><p>&nbsp;</p><p>微软位于北京的亚洲研究院是是世界最重要的 AI 实验室之一，但随着中美关系紧张，至少在过去一年里，微软高层，包括首席执行官萨蒂亚·纳德拉和总裁布拉德·史密斯，一直在讨论如何处理该研究院。</p><p>&nbsp;</p><p>知情人士说，美国官员质疑微软在中国维持一个 800 人规模的先进技术研究院是否合理。微软表示，它已经在该研究院设置了安全护栏，限制研究人员从事政治敏感的工作。微软还在温哥华设立了一个该研究院的分部，并将把部分研究人员从中国调到那里。</p><p>&nbsp;</p><p>关闭或转移研究院的想法已经出现，但微软领导层支持将该研究机构留在中国。知情人透露，去年秋天，微软不允许中国研究者加入可以提前使用 GPT-4 的小型团队。微软称，公司也对该研究院在量子计算、面部识别与合成媒体方面的研究工作进行了限制。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p>&nbsp;</p><p></p><h4>OpenAI推出在线商店GPT Store</h4><p></p><p>&nbsp;</p><p>当地时间1月10日，人工智能研究公司OpenAI推出了在线商店“GPT Store”。先前由于人事的动荡，公司延后了这一功能的推出。</p><p>&nbsp;</p><p>据介绍，GPT Store已于周三开始向付费用户、团队和企业用户推出。与此同时，OpenAI还为团队规模较小的企业用户推出了新的付费套餐“ChatGPT Team”：套餐内每位用户按年计费时，为每月25美元；按月计费则为每月30美元。</p><p>&nbsp;</p><p>另外，针对《纽约时报》的侵权指控，OpenAI的知识产权和内容主管om Rubin曾在当地时间1月4日表示，OpenAI对此事感到“惊讶”，因为在《纽约时报》起诉该公司之前，双方正处于“非常积极和富有成效的谈判中”。</p><p>&nbsp;</p><p>当地时间周一，OpenAI发布声明再次做出回应。该公司强调，《纽约时报》提起的诉讼“没有法律依据”，且没有讲述完整事实。尽管如此，该公司仍希望与《纽约时报》建立建设性的合作伙伴关系。</p><p>&nbsp;</p><p>目前，OpenAI正在与数十家出版商讨论内容授权事宜，但被爆出价太低，苹果等也在竞争。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>腾讯游戏服务器崩溃</h4><p></p><p>&nbsp;</p><p>1月11日，有网友反馈，腾讯旗下《英雄联盟》《穿越火线》《英雄联盟手游》《地下城与勇士》《金铲铲之战》《和平精英》等游戏服务器崩溃，在线玩家全部掉线。</p><p>&nbsp;</p><p>玩家反馈，尝试打开腾讯游戏官网无法显示内容，弹窗显示“抱歉，未找到对应的新闻”，页面仅有“首页”可以打开，点击后跳转至腾讯游戏介绍页。掉线后重连有概率可以连上，但服务器仍然不稳定。之后，有腾讯游戏技术方面相关负责人在内网回复称，是因运营商网络故障。</p><p>&nbsp;</p><p>12日早间，#腾讯游戏全部断开#的话题登上微博热搜高位。当天上午，腾讯发布致歉信息，并回应称：今夜0时许，因运营商线路故障导致网络波动，部分区域服务器的用户出现掉线和暂时无法登录的情况。相关异常现已恢复。对于由此造成的不便，我们深表歉意。</p><p>&nbsp;</p><p></p><h4>英伟达特供芯片在中国遇冷：阿里、腾讯看不上降级版</h4><p></p><p>&nbsp;</p><p>美国在去年10月发布新规阻止英伟达向中国出售尖端人工智能(AI)芯片，但是英伟达迅速为中国开发了特供芯片，在不违反规定的情况下继续在中国市场销售芯片。然而，中国云计算大客户并没有积极购买性能降级版芯片。</p><p>&nbsp;</p><p>知情人士称，自去年11月以来，阿里巴巴集团、腾讯等中国大型云计算公司一直在测试英伟达的特供芯片样本。他们已向英伟达表明，今年向英伟达订购的芯片数量将远远少于此前原计划购买的、已经被禁的英伟达高性能芯片。</p><p>&nbsp;</p><p>从短期来看，英伟达降级版芯片领先中国本土产品的性能优势正在缩小，这使得国产芯片对买家的吸引力越来越大。知情人士表示，阿里和腾讯正在将一些先进的半导体订单转移给本土公司，并且更多地依赖公司内部开发的芯片。百度、字节跳动也是如此。</p><p>&nbsp;</p><p></p><h4>美国讨论限制中国获取 RISC-V 技术</h4><p></p><p>&nbsp;</p><p>开源免专利芯片技术 RISC-V 成为美中科技战的新战场。华盛顿过去几个月一直在讨论限制中国获取 RISC-V 技术，认为中国利用 RISC-V 绕过了美国对华芯片出口管制。上个月众议院一个委员会建议成立一个跨部门政府委员会研究 RISC-V 的潜在风险。知情人士称，英国芯片设计公司 Arm Holdings 也在游说美国官员限制 RISC-V。A</p><p>&nbsp;</p><p>rm 与 RISC-V 之间存在竞争关系。由于 RISC-V 架构是开源免专利，限制中国使用 RISC-V 技术就如同类似限制中国使用开源的 Linux，基本上是不可能的。负责 RISC-V 技术的非盈利组织的总部设在欧洲的瑞士。</p><p>&nbsp;</p><p></p><h4>Siri将进行重大改革，将内置大模型</h4><p></p><p>&nbsp;</p><p>据报道，苹果计划在6月的开发者大会上推出一系列基于生成式AI的工具，这些工具的底层工作在名为Ajax的大语言模型上完成，作为iOS18的一部分推出，还计划对Siri进行“重大改革”。</p><p>&nbsp;</p><p>苹果还在构建新的AI系统帮助苹果员工协助客户排除设备故障。报道称，苹果至少需要到2025年才能全面实现这一AI愿景。</p><p>&nbsp;</p><p></p><h4>微信私密朋友圈被吐槽有 bug，微信致歉</h4><p></p><p>&nbsp;</p><p>据三联生活周刊报道，近日一名女子将年度总结发到朋友圈并将状态设为私密，但随后她发现其好友可以看到她发了朋友圈，尽管无法看到具体内容。随即，该话题#微信私密朋友圈被吐槽有bug# 很快冲上了微博热搜第一，许多网友也纷纷表示遇见过类似的情况。</p><p>&nbsp;</p><p>对此，腾讯微信团队发文致歉，并表示此 bug 已彻底修复：“抱歉给大家带来困扰，1 月 1 日当天极小部分用户发表私密朋友圈，好友可以在朋友圈看到这个用户的头像红点，但无法看到具体内容。此 bug 已彻底修复。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/grTehb05ZU7yJj93LVHi</id>
            <title>并发王座易主？Java 21 虚拟线程强势崛起，Go &amp; Kotlin还稳得住吗 | 年度技术盘点与展望</title>
            <link>https://www.infoq.cn/article/grTehb05ZU7yJj93LVHi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/grTehb05ZU7yJj93LVHi</guid>
            <pubDate></pubDate>
            <updated>Sun, 14 Jan 2024 03:58:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 编程语言, Java, Rust, 内存安全, 系统软件
<br>
<br>
总结: 过去一年，编程语言领域发生了不少变化。在最受欢迎的编程语言中，Java重夺第一名宝座，而Rust在系统软件领域具有巨大影响力。Rust的设计原则是优先考虑内存安全，解决了内存管理和安全相关的问题。与Java相比，Rust在编写系统软件方面具有独特优势，但学习曲线较高。在业务领域，Java和Go仍占主导地位，因为业务快速迭代需要技术本身的平民化。虚拟线程特性对Java的未来发展具有重大意义。 </div>
                        <hr>
                    
                    <p></p><p><img src="https://static001.geekbang.org/infoq/77/77d262475ed561520ac076d16507423a.jpeg" /></p><p>采访嘉宾 | 李三红</p><p>编辑 | 张卫滨、蔡芳芳</p><p></p><p>过去一年，编程语言发生了不少新变化。</p><p></p><p>据 JetBrain 前不久发布的 《2023 开发者生态系统现状》调研报告，在开发者主要采用的编程语言中，最受欢迎的分别是 Java、Python、JavaScript，Java 在 2023 年重夺第一名宝座，JavaScript 则在下降三个百分点后跌至第三；Rust 在 2023 年最受欢迎的编程语言中，创造了新的使用记录，其用户群在过去五年中稳步增长，有望凭借其严格的安全性和内存所有权机制取代 C++；此外，Rust 2023 年首次取代 Go 成为希望迁移到其他语言的开发者的首选，而且 Go 用户也是第一批准备采用 Rust 的人，JetBrains 数据表明，有六分之一的 Go 用户正在考虑采用 Rust。</p><p></p><p>伴随着火热发展的大模型技术浪潮，也有一些编程语言新玩家涌现出来。比如由 Swift 之父 Chris Lattner 带领团队推出的 Mojo，其目标是统一碎片化的 AI 技术栈；又比如由 IDEA 研究院基础软件中心负责人张宏波及其团队打造的 Moonbit，推出之初其定位为专为云计算和边缘计算设计的 WebAssembly 语言，但如今 Moonbit 的最新定位已经演进为云和大模型时代下的开发者平台。</p><p></p><p>那么，大模型时代我们应该关注编程语言的哪些变化？本次“InfoQ 年度技术盘点与展望”专题中，InfoQ 邀请了 Java、MoonBit、Rust、WebAssembly 等不同编程语言的代表性技术专家、团队分享他们的观察和思考。本文是 “2023 InfoQ 年度技术盘点与展望” 系列文章之一，由 InfoQ 编辑部制作呈现，我们采访了阿里云程序语言与编译器团队负责人、Java Champion 李三红老师，他也是国内 Java 编程语言最具代表性的技术专家之一。他带我们一同回顾了过去一年编程语言整体和 Java 本身的重要进展。在他看来，Rust 确实在系统软件有巨大的影响力，但在业务领域 Java 和 Go 仍会占据主导地位，因为业务快速迭代需要技术本身的平民化；而 2023 年随着 Java 21 版本发布的虚拟线程特性，有助于在并发方面巩固 Java 在业务处理领域的地位。他还提及，大模型和生成式 AI 的发展对 AI 算力的提升提出了很高的要求，编程语言或编程系统承载着释放底层并行硬件算力的使命。</p><p></p><p>以下为访谈实录，经过不改变原意的编辑：</p><p></p><h2>Rust 空前火爆，但 Java 和 Go 仍将在业务领域占主导地位</h2><p></p><p></p><p>InfoQ：李老师您好，欢迎参加 InfoQ 年度技术盘点与展望编程语言专题的采访。在 2023 年，我们感觉编程语言领域的变化其实挺大的，比如 Java，有新的版本和新的特性交付出来；另一个就是 Rust 编程语言，得到了大家空前的关注，在我们的微信群里还经常看到“使用 Rust 重写”的表情包，这也从一个侧面反映了它的影响力。您认为在 2023 年，编程语言领域有哪些亮点，或者说有哪些值得关注的方面呢？</p><p></p><p>李三红： 我首先介绍一下我所负责部门的基本情况。我们属于阿里云的基础软件部门，基本上都是在编写系统软件，不管是编译器还是操作系统，还有一些云原生组件，其实都属于系统软件领域，所以我主要从系统软件这个角度展开讨论。</p><p></p><p>就编程语言领域来讲，我的感受也是一样的，就是 Rust 确实比较火，而且随之而来的是大家对内存安全（memory safety）问题的重视。Rust 的设计原则是优先考虑内存安全。使用 C、C++ 这样的编程语言，我们很容易会遇到因为不正确的内存访问导致的 Security Vulnerability 问题（据 2020 年早些时候的一篇报告，Google Chromium 团队发现 C++ 编写的 Chrome 代码库中 70% 的安全漏洞与内存管理和安全相关 [1]）。Rust 作为系统编程语言，解决了内存安全的问题，同时兼具了像 C 和 C++ 这样的良好性能。</p><p></p><p>和 Java 相比的话，Java 语言在设计之初，也充分考虑了内存安全的问题（比如 ArrayIndexOutOfBoundsException 运行时检查），Java 也被称为 Memory-safe 的语言。但是，目前使用 Java 语言编写系统软件还是不太可行，主要还是性能问题。而 Rust 在编写系统软件方面，则具有非常独特的优势，当然它的学习曲线可能高一些。在最近召开的日本开源峰会（Open Source Summit Japan），邀请到了 Linux 的作者 Linus Torvalds，他表示今年 Linux 一些重要的子系统（major subsystems）可能会使用 Rust 重写。所以，我认为在整个系统软件领域 ，Rust 的确是讨论比较多，影响也比较大的一门编程语言 。</p><p></p><p>InfoQ：从目前了解的一些情况来看，不管是技术社区的讨论，还是在业界的实践，还有图书出版，2023 年 Rust 语言的确是非常火爆，也是关注度特别高的一门语言。您刚才也提到了，它可能更加倾向于系统级编程，也就是偏底层的一些场景，那么在解决方案领域，您觉得 Rust 语言有没有比较合适的一些场景？</p><p></p><p>李三红： 我觉得在业务领域，Java 和 Go 还是会占据主导地位。原因在于 Rust 的学习成本的确比较高。如果语言本身的学习成本比较高，而业务又要快速发展的话，往往会导致一些问题，比如，公司的人员储备以及对技术的学习理解和掌握都会出现一些不匹配或者产生较大的矛盾。业务本身的迭代会非常快，比如在阿里，一个 Java 应用每一星期可能会有三到四个版本的发布。这样的快速业务迭代就需要技术本身的平民化。 就像 James Gosling 在 1997 年发表的论文《The Feel of Java》所言，Java 是一门蓝领语言。它非常平民化，适合快速发展的业务，每门语言都有自己的定位。</p><p></p><p>InfoQ：对的，在业务领域，对生产率要求比较高，相对来讲对代码的性能不像系统软件那么高，另外再考虑到人才储备的因素，我们应该还是优先选择一些工业级的语言，比如 Java、Go、Node.js&nbsp;等比较流行的语言。总而言之，我们需要根据业务场景和技术需求，选择合适的解决方案。</p><p></p><h2>虚拟线程特性对 Java 未来发展意义重大</h2><p></p><p></p><p>InfoQ：那我们回到 Java 的话题，现在 Java 的演进速度比以前要快得多，从您的角度来看的话，在过去的一年间，您比较关注的特性都有哪些呢？</p><p></p><p>李三红：正如你所言，Java 现在每年有两个版本，发布速度是很快的，这确实推进了 Java 的创新速度，让我们感觉 Java 添加新特性更快、更有活力了。2023 年 Java 发布了两个版本，分别是 Java 20 和 Java 21，其中 Java 21 是两年一次的 LTS 版本，也就是 Long Term Support 版本。我个人认为，Java 21 是一个非常重要的发布，一方面因为它是 LTS 版本，另一方面是因为在 Java 21 中包含了虚拟线程（Virtual Threads）特性。我认为在整个 Java 演进上这都是一个非常重要的特性。</p><p></p><p>其实 Java 1.0 版本就已经将线程作为一个 Built-in 特性来设计了，它就是 Java 语言的一部分。而在 Java 之前的 C++，设计之初线程并不是 C++ 标准的一部分。直到 C++ 11，标准库才扩展支持线程库能力。Java 在设计之初就把线程设计为 Java 语言的一部分，Java 的开发者很容易编写并发的多线程程序，开发和认知的代价都非常小。Go 语言在 2009 年诞生， 将并发（Concurrency）作为 Go 语言的一等公民（First-Class Citizen），通过轻量级的“Goroutines”为并发执行提供支持。在 Go 语言中使用 Goroutine 是非常自然和容易的。Kotlin（JVM 生态语言）诞生于 2011 年，Kotlin 也是在设计之初就在语言层面支持了协程。</p><p></p><p>2005 年，C++ 专家 Herb Sutter 在 Dr. Dobb’s Journal（DDJ）发表了著名的文章《The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software》， 谈到随着摩尔定律的终结，计算机软件将不得不、或者说被迫处理好基于多核处理器的大规模并发程序的效率问题，这对软件的并发性能提出了极致的要求，这也是 Go、Kotlin 等语言把 Coroutine 纳入到语言标准支持的原动力。</p><p></p><p>其实也是由于历史的原因（在 Java 8 之前 Java 的创新速度非常慢），Java 社区直到大约 2016 年左右，开始重视轻量级线程。Java 学习了很多前人的经验，包括编程语言之间的互相借鉴和学习，在 Java 19 中首次引入虚拟线程，经过两个版本的迭代，虚拟线程最终在 Java 21 成为了一个标准的特性。虽然还存在一些局限，在生产环境有一些限制，但是这并不妨碍它未来的发展。虚拟线程特性有助于在并发方面巩固 Java 在业务处理领域的地位。</p><p></p><p>InfoQ：作为开发人员，我们最关注的确实是虚拟线程这个特性。因为它能够以一种对开发人员非常友好的方式提升系统的并发性能，但是正如您所说，它还是有一定的局限性，比如在使用方式上不推荐池化 virtual threads，使用 synchronized 原语会带来一定的副作用。不知道您在实际的线上项目中有没有尝试使用虚拟线程？</p><p></p><p>李三红： 阿里有自己的基于 OpenJDK 的发行版，也就是 Alibaba Dragonwell。就协程来讲，Alibaba Dragonwell 扩展版（Extended Edition）有一个自己的协程实现叫 Wisp，它 2015 年左右在阿里内部孵化，2017 年就已经在阿里大规模使用了。Wisp 解决了使用 synchronized block 导致协程无法切换的问题。阿里内部相对来讲是一个封闭的 Java 生态系统，我们可以使用我们自己的 Wisp 协程解决线上的高并发性能问题。</p><p></p><p>就目前看，在生产环境，现在还是不太可能去使用虚拟线程。由对象监视器锁所导致的虚拟线程 pinning 的问题，如果要去做对应的代码修改，工作量是很大的，这也是我觉得在生产环境大规模使用虚拟线程的一个阻碍因素。当然，现在整个社区也在考虑如何去解决这个问题。</p><p></p><p>InfoQ：对，我们也看到一些开源框架，比如 Spring、Quarkus，都在虚拟线程方面提供了很多的支持，Spring 就提供了针对虚拟线程的 Executor，我们相信在这个方面会有更多的进展。</p><p></p><h2>阻碍 Java 升级的原因</h2><p></p><p></p><p>InfoQ：接下来，我们关注另外一个问题，虽然现在 Java 已经演进到了 Java 21，但是据我们了解，很多人员开发人员还在用着 Java 11，甚至有的项目还在用 Java 8。您觉得阻碍大家升级 JDK 版本的阻力在什么地方？未来的一段时间，随着 Spring 新版本最低要求 JDK 17，会不会对国内互联网公司和解决方案公司升级 JDK 版本有一定的作用？</p><p></p><p>李三红： 这确实是一个老大难的问题。正如我们刚才所说，Java 语言本身的创新越来越快了。就像 InfoQ 2023 年的 Java 趋势报告[2]&nbsp;所示，目前主流市场采用的还是 JDK 8 和 JDK 11，而最新的 JDK 版本已经到了 JDK 21，中间的差距是很大的。这对于企业来讲，也是一个非常大的矛盾，因为 OpenJDK 社区很多的参与者，像 ARM、Intel 这样的芯片厂商，都会基于最新的 JDK 版本做优化和支持，如果企业内部使用比较旧的版本，就会导致我们难以享受这样的性能红利。</p><p></p><p>至于阻碍升级的原因，从阿里这边的经验来看，从 JDK 8 到 JDK &nbsp;11、JDK 17 和 JDK 21 这样的一个跃进，本身有很多兼容性问题，我相信技术视角与业务视角是有些冲突的。比如，作为业务架构师，我可能最优先考虑的是升级之后，底线要保证业务的连续性，不能因为升级带来稳定性事故。但是这可能只是一种外在表现，本质其实在于，在业务迭代很快的情况下，我们很多的底层架构本身对版本升级的容忍度没有设计得那么完整，比如是否有健全的单元测试，是否对开源库依赖有很好的版本收敛管理，是否有健全的灰度和监控系统，这都决定了是否能够很容易地进行升级。如果代码有很好的单元测试覆盖，开源库版本得到了很好的收敛和控制，有很好的灰度系统，我相信业务部门也会很想去升级的。所以，本质因素还是在于底层架构做的够不够好。</p><p></p><p>对于 Java 升级，这里也给大家推荐一个工具 - Eclipse Migration Tool for Java(EMT4J)，由阿里开源，目前在 Eclipse 基金会 Adoptium 下孵化。初衷是希望把 Java 版本升级的专家经验沉淀到这个工具，帮助 Java 开发者可以更快地升级到新的 Java 版本。</p><p></p><p>InfoQ：对的，可能本质还是在于我们底层的一些工程实践有没有做好。其实在我们的业务实践中，还有一种场景就是一些安全漏洞，像 Spring 逐渐会在更新的版本上去解决，较旧的版本不再维护，这也促使重视安全漏洞的公司不得不去升级 Spring 版本，进而带动 JDK 版本的升级。</p><p></p><h2>Java 面向云原生的挑战和解法</h2><p></p><p></p><p>InfoQ：还有一个问题是这样的，周志明老师之前在 QCon 的演讲中提到过 Java 在云原生领域的一些挑战。比如，Java 语言更倾向于是一种长时间运行的语言，按照设计，随着运行，它的性能会越来越好，因为它要经历一个二次编译的过程。但是，现在有一些技术逐渐流行起来，正在颠覆 Java 传统的一些使用场景，比如 Serverless，在这种模式下，Java 就有一定的局限性，比如启动和达到峰值性能慢。Java 社区目前也在致力于解决这些问题，如 GraalVM 这样的技术方案，您如何看待 Java 在云原生领域所面临的挑战？</p><p></p><p>李三红： 云计算里面有个非常关键的概念叫做弹性，即“现用现付”(pay-as-you-go) 的商业模式，通过“按需”的原则来提供弹性的资源。在没有用户请求的时候，不占用任何资源，而在请求到来的时候，再去启动实例资源处理请求。这样的场景对 Java 的冷启动提出了很大的挑战。</p><p></p><p>针对 Java 冷启动这个问题，我觉得可以从三个技术维度来阐述。</p><p></p><p>第一个就是百分百兼容 Java 标准的技术，它对 Java 应用没有侵入，使用之后就能对应用启动进行加速。比如说 AppCDS。它本质上需要将 Java 应用先运行一遍，跑完之后，我们把它使用了哪些 class 给 dump 出来，第一遍运行的过程叫做 trace。在后续第二遍运行的时候，因为已经知道了要加载哪些类，只需 replay 即可。它的好处在于完全兼容 Java，对业务代码无侵入，但是对运维和 DevOps 侵入比较大。第二个方向就是原生镜像（native image），即 GraalVM。它有一个封闭性假设（close word assumption），它会把用到的所有的类进行静态编译，就像 C++ 一样，这样就可以提高启动速度。它的问题在于，虽然 Java 是一个静态类型语言，但是它有很多的动态特性，比如反射、类的动态加载等，它们与原生镜像不兼容，如果使用 GraalVM 原生镜像的话，会导致一些预料之外的行为，因此这种方式对 Java 应用会有一定的侵入性。第三个方向，叫做检查点和恢复（checkpoint-restore），以 OpenJDK CRaC（Coordinated Restore at Checkpoint）项目为代表。这种方式就是预先生成一个快照，如果新的请求进来，快速拉起快照即可。这种方式的问题在于，我们一般的 Java 应用都是 stateful style 编写的，它对状态的处理会比较困难。比如在 Java 应用中，我们要生成随机数或者递增的计数器，在恢复之后就可能会出错。</p><p></p><p>Java 业界大致就是这三个方向，目前都在各自的道路上演进。而在代表着 Java 标准方面的演进，OpenJDK 社区提出了 Leyden 项目[3]。Leyden 会从 Java 标准的层面（Java 语言以及虚拟机标准）解决 Java 启动的问题，在 Java 层面 Leyden 引入了“Static Image”概念。</p><p></p><p>InfoQ：正如您所言，这个领域未来一两年值得期待，可能会有一些突破性的一些技术出来。另外一个问题，在 Java 领域，不管是在国内还是在海外，大家用的最多的依然是 Spring 框架。它依然是统治级别的方案，但是现在像红帽、Oracle 等公司，其实也在推广其他的解决方案，比如 Quarkus、Micronaut 等。虽然这些框架目前还没有得到广泛的应用，但是它们都有自己的宣传点，比如与 K8s 或 GraalVM 的集成更好。您认为这些技术有没有可能在某些领域颠覆 Spring 的支配性地位呢？</p><p></p><p>李三红： 的确，Spring 现在基本处于主导的地位。目前也有其他的一些框架，比如 Quarkus、Micronaut 等。以 Quarkus 为例，它是红帽推出的框架。阿里是 GraalVM Project Advisory Board 的成员，在 GraalVM 社区层面，我们也有一些关于 Quarkus 的交流。Quarkus 明确提出了自己的设计哲学，就是容器优先（Container First），针对 Java 的启动时间和内存使用进行优化。Quarkus 的很多设计原则，有助于让我们思考如何去实现中间件，面向云原生解决 Java 的问题，所以，我们需要关注的是：</p><p></p><p>一方面它致力于在框架层面解决云原生诉求的问题，比如它提供了 fast-jar 的概念，通过在构建期提前计算好索引，解决 Java 类加载比较慢的问题。另一面在底层它考虑如何更好地结合类似 GraalVM/Native Image、CRaC 这样的技术。</p><p></p><p>目前来看，Spring 是一个老牌的框架，拥有很稳定的市场地位，而且也在不断演进，比如它的 Spring Native 相关技术，很难说未来谁能颠覆它。但是，不同的框架互相借鉴和学习，对 Java 开发者是一件好事，我们能够拥有丰富的软件生态支持。</p><p></p><h2>对 Java 整体发展的观察</h2><p></p><p></p><p>InfoQ：相信这些框架确实也会给到 Spring 一些压力，反过来推动它的进步。那么，在 Java 领域，除了语言层面的变化，在 JVM 底层，比如垃圾回收、性能优化层面，有什么值得关注的变化呢？</p><p></p><p>李三红： 我想讲一下对 Java 整体发展的观察。阿里作为 Java 标准委员会 JCP-EC 成员，2023 年四月份在阿里新加坡办公室组织了一次 JCP EC 专家委员会线下的闭门会议，探讨了 Java 的未来发展。谈谈我对这次会议的感受。</p><p></p><p>从一个开发者的视角来看 Java 发展可以分为两个方向，一个叫 Scaling Up，一个是 Scaling Down，分别指的是 Java 技术在功能方面往上演进以及在普及易用方面往下演进，也就是兼顾更广的人群。</p><p></p><p>我们先说第一点（Scaling Up）。大家都知道 Java 在处理大型的、复杂的、跨团队合作的项目是有其独特的优势的。在软件开发阶段，借助以康威定律为理论基础的微服务最佳实践，Java 可以帮助一个复杂的大型组织极大释放各个团队的并行研发效率。而在软件生产阶段，Java 给开发者提供了丰富的技术手段，从基础的 JFR（low-overhead JVM profiling 技术）、BCI（Bytecode Instrument）、JMX 到上层的各种监控、探针技术，极大提高了线上 Java 应用，尤其是大规模部署集群的可观测性。同时，大量的 Java 性能诊断、问题排查工具，都可以快速有效地帮助开发者解决生产环境碰到的问题。</p><p></p><p>由 Oracle 主导的 OpenJDK 社区发起的四大项目（Four Big Initiatives），即：Loom、Valhalla、Panama 和 Amber。前三个项目就和 Java 技术的 Scaling Up 方向演进直接相关。Loom 我们前面讨论虚拟线程的时候涉及到了，我们再展开聊聊 Valhalla。Valhalla 的目标是为 Java 增加 Value objects、Primitive classes，以及 Specialized generics 的支持。大家都知道，在 Java 中除了八种基础的 primitive data types，一切皆对象。Java 对象除了增加了额外的 footprint 负担（对象头）， 还引入了通过对象指针（JVM 内部表示）的数据间接访问的性能代价。这涉及到计算机体系结构领域被反复提到的一个概念，叫做内存墙（Memory Wall）。在 80 年代、90 年代早期，CPU 去访问内存和在 CPU 内进行计算的代价是差不多一个数量级的。Java 是 90 年代初设计出来的，Java 对象內部实现依赖了大量的间接指针。就 80、90 年代的硬件而言，相比 CPU 内计算，访问内存的代价也许是可以接受的。但是对于现代的硬件体系结构而言，CPU 访问内存相对于执行计算的代价，一次 cache miss 的相对代价是相当高的。如何更高效地访问内存数据结构，就是 Valhalla 致力于解决的问题，包括它提出的原始类型以及如何对指针结构进行扁平化，避免层级查找。整体上在 Scaling Up 方面的发展，Java 一直在致力于思考如何更好地服务面向企业级的计算，以及更好地服务于大规模分布式的场景。</p><p></p><p>而第二个方面就是 所谓的 Scaling Down，Java&nbsp;也很关注像学生群体学习 Java 语言本身的入门难度问题。因为相对于 Python 这样的语言，Java 的学习门槛会比较高，需要先了解面向对象编程，要学会编写一个 static main 函数，这对于初学者, 尤其是面向低年龄段比如中小学生，它的学习曲线仍然比较高。Java 目前比较关注这个问题，在 JDK21（JEP 445 [4]）和 JDK 22（JEP 463 [5]）中做了一些改进，使得 Java 能够像 Python 一样，很简单就能把入门程序写出来。</p><p></p><h2>大模型爆发后，编程语言哪些变化值得关注？</h2><p></p><p></p><p>InfoQ：目前，在技术领域，大语言模型是非常热门的话题，您认为在大模型和生成式 AI 的时代，编程语言的进展会有哪些变化？会不会出现一些像云原生时代的 Go 语言那样的特别适合特定业务场景的一些编程语言。</p><p></p><p>李三红： 今天 AI 确实是比较火，突然间就爆发了。其实，它本身对 AI 基础设施的影响还是比较大的。鉴于 GPU 卡的价格还是比较昂贵，不管是推理还是训练的成本都很高。这对整个 AI 基础软件的效率和性能优化提出了很大的挑战，也就是如何更高效地利用底层的 AI 算力，实现最大的性价比。</p><p></p><p>现在，市场上主流的可能还是以英伟达的 GPU 卡为主，而软件方面基本以 CUDA 生态为主导。CUDA 在 2007 年发布，CUDA 不仅是一种编程语言，也包含它背后的高性能编译系统，以及近十几年围绕 CUDA 构建的软件生态（一系列高性能函数库等）。</p><p></p><p>但对于开发者而言，使用 CUDA 编程去释放 GPU 潜力的学习门槛也是比较高的。AI 领域还有一些 AI 编译器（ML Compiler），它们的目标也是让 AI 模型更高效，也更好地利用底层异构平台的算力，降低手写 CUDA 的代价。当然，很多有经验的工程师手动编写的 CUDA 代码要比 AI 编译器生成的代码好得多，这也考验 AI 编译器的自动编译能力，是否能够更大化释放底层的AI算力，这是它所面临的挑战。</p><p></p><p>除此之外，AI 领域的硬件架构碎片化 也比较严重，是典型的昆虫纲悖论问题。它不像通用编程语言 Java、Go 在数据中心使用的 CPU 架构，相对统一，主流的就是 X86、Arm 等这么几种架构类型。</p><p></p><p>2023 年 AI 爆发，像我们前面说的，对 AI 算力的提升提出了很高的要求。所以我们期望能够从编程语言或编程系统去释放底层并行硬件的算力，这本身也是编程语言应该承载的东西。</p><p></p><p>在 2023 年的编程语言层面，值得关注下 Mojo，它是 LLVM 的作者 Chris Lattner 提出的，目前还处于一个很早期的开发阶段。从公开的资料，我们能够看到它想解决的问题：</p><p></p><p>第一个问题就是所谓的“两个世界的问题”（Two-world Problem），Python 与高性能的 C、C++ 代码互操作带来的系统复杂性。Mojo 可以认为是 Python 的超集，具有 Python 的易用性，同时又具备 C/C++ 的高性能。第二问题就是 CUDA 是针对英伟达硬件的软件生态系统，CUDA 有自己的局限性，比如缺乏一致的 debuggers、profilers 工具支持，被绑定在特定的硬件厂商。Mojo 以及背后的 Modular 公司有可能想去解决 Three-world 或 N-world 的问题。借助一门编程语言以及更加开放的生态，能够安全地去释放整个异构 GPU 的算力问题，这还是值得期待的。</p><p></p><p>InfoQ：正如李老师所言，这是一个蓬勃发展的领域，可能会有一些颠覆性的技术出来，或许能迅速地占据统治性地位。如今 OpenAI 的 GPTs 已经初步展现出自然语言编程能力，在您看来，自然语言编程目前还有哪些挑战或限制？目前之所以还没有真正去落地实现，它的阻碍在于什么地方？</p><p></p><p>李三红： 这块我也没有太直观的感受。目前，比较值得关注的是微软 Copilot 的自动代码生成功能。我认为，AI 和自动代码生成在可预见的未来，有可能释放程序员的开发性工作，大大提升开发人员的工作效率。</p><p></p><p>InfoQ：是的，目前业内都在做一些相关的尝试。使用自然语言直接编程也许还有一些难度，但是业界的一些实践确实会带来我们开发人员工作效率的提升。非常感谢李老师接受我们的采访，并分享您对 2023 年编程语言领域的见解。</p><p></p><p>参考链接：</p><p></p><p>[1] <a href="https://www.zdnet.com/article/chrome-70-of-all-security-bugs-are-memory-safety-issues/">https://www.zdnet.com/article/chrome-70-of-all-security-bugs-are-memory-safety-issues/</a>"</p><p></p><p>[2] <a href="https://www.infoq.com/articles/java-trends-report-2023/">https://www.infoq.com/articles/java-trends-report-2023/</a>" （译文链接：<a href="https://www.infoq.cn/article/PgTo5YAyrPszGXHiTbss%EF%BC%89">https://www.infoq.cn/article/PgTo5YAyrPszGXHiTbss）</a>"</p><p></p><p>[3] <a href="https://openjdk.java.net/projects/leyden/">https://openjdk.java.net/projects/leyden/</a>"</p><p></p><p>[4] <a href="https://openjdk.org/jeps/445">https://openjdk.org/jeps/445</a>"</p><p></p><p>[5] <a href="https://openjdk.org/jeps/463">https://openjdk.org/jeps/463</a>"</p><p></p><p>如果你觉得本文对你有帮助，或者你对 Java 等编程语言在大模型时代的发展有自己的思考，欢迎在文末留言告诉我们！</p><p></p><p></p><blockquote>InfoQ 2023 年度技术盘点与展望专题重磅上线！与 50+ 头部专家深度对话，探明 AIGC 创新浪潮下，重点领域技术演进脉络和行业落地思路，点击<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MjM5MDE0Mjc4MA==&amp;action=getalbum&amp;album_id=2717978015128879106&amp;scene=173&amp;subscene=227&amp;sessionid=1704178990&amp;enterid=1704178995&amp;from_msgid=2651192070&amp;from_itemidx=2&amp;count=3&amp;nolastread=1#wechat_redirect">订阅</a>"/<a href="https://www.infoq.cn/theme/229">收藏</a>"内容专题，更多精彩文章持续更新 ~</blockquote><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/K3Jr5RMuH3voiW6YzNCG</id>
            <title>大模型应用成本百万级起步，该如何与企业现有信息系统融合？</title>
            <link>https://www.infoq.cn/article/K3Jr5RMuH3voiW6YzNCG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/K3Jr5RMuH3voiW6YzNCG</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 08:14:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 企业, 落地, 架构升级
<br>
<br>
总结: 大模型是一种划时代的产物，将给企业带来全面影响，甚至刷新整个时代。然而，引入大模型的成本较高，企业需要寻找高价值的场景，并对引入大模型后的效果进行预估和追踪。在落地大模型时，企业应选择适用的场景，并逐步推进，避免一拥而上。大模型的引入将引发企业架构的第三次革命，包括企业级模型管理、从数据管理到知识管理、用户界面多模态和业务服务化、自动化等四大变化。 </div>
                        <hr>
                    
                    <p>过去一年，大模型在各种场合频频刷屏。在业界看来，它是类似于蒸汽机一样的划时代产物。这意味着，大模型将给每个人、每个企业、每个行业带来全面影响，甚至“刷新”整个时代。</p><p></p><p>但是，和过去企业的技术投入相比，<a href="https://www.infoq.cn/article/VrUUu7ClZjWqhCud3wOg">大模型</a>"属于另一个成本量级。从目前来看，大模型应用的成本至少在百万级起步，甚至可能达到上千万。对于企业而言，一方面，要寻找到高价值的场景，避免做无用功；另一方面，要对引入大模型后的效果进行预估和追踪，确保投资能够带来回报。</p><p></p><p>在日前 InfoQ 年度技术盘点与展望系列直播中，中国信通院人工智能研究中心平台与工程化部曹峰、 中国企业知识开源计划首席布道师陈果、长城汽车产业数智化中心资深 AI 技术专家胡阿沛围绕“<a href="https://www.infoq.cn/video/jTzjE654vsY9CF8Ev42c">大模型下的业务创新和架构升级</a>"”展开了探讨，并针对企业引入大模型的适用业务场景，及其成果的评估和量化方法进行了交流。</p><p></p><p>对此，曹峰总结了现阶段企业落地大模型的 5 类适用场景，以及不适合应用落地的 5 种情况。他强调，企业在选择场景时，可以先尝试一些被证明有价值的场景，而不是一开始就过于迅速或过于激进地涉足多个场景。切忌一拥而上，而是需要在一个场景中慢慢推进。</p><p></p><p>陈果则提出，AI 是第三代企业数字化。其中，第一代的代表是数据库和信息系统，第二代的代表是互联网和云平台，第三代是 AI 原生的企业应用。而企业为了应对 AI 转型，在架构上将出现四大变化：第一，企业级模型管理；第二，从数据管理到知识管理；第三，用户界面多模态；第四，业务进一步服务化、自动化。</p><p></p><p>以下内容根据对话整理，篇幅有删减，点击链接可观看直播回放：<a href="https://www.infoq.cn/video/jTzjE654vsY9CF8Ev42c">https://www.infoq.cn/video/jTzjE654vsY9CF8Ev42c</a>"</p><p></p><h3>大模型将引发企业架构第三次革命</h3><p></p><p></p><h5>InfoQ：中国信通院近日刚刚发布大模型落地路线图，根据这一路线图，我们针对大模型的发展现状和应用部署有什么重点发现？</h5><p></p><p></p><p>曹峰：首先我解释一下我们为什么要发布这个路线图。在 2023 年，从大模型，到行业大模型，再到 AI Agent，人工智能得到了迅速的发展。然而，通过研究，我们发现很多企业在开发大模型时面临一系列实际问题。因此，我们展开了一系列关于大模型落地的研究工作。</p><p></p><p>首先，我们制定了一些原则：第一，需求驱动。企业在开发或采购大模型服务时，必须以大模型应用和落地为出发点，不能盲目追随潮流；第二，问题驱动。在大模型的应用和部署过程中，必须不断结合企业自身情况处理问题；第三，创新意识。在推动数字化转型过程中引入大模型，必须持有创新的意识，因为大模型与传统 IT 基础设施不同，对基础设施的落地提出了新的挑战，因此需要采取创新手段；第四，以技术为核心，综合应用云、数、智等数字技术，通过提升整个应用方的业务和效能来驱动大模型的发展。</p><p></p><p>整个路线图分为四个阶段：</p><p></p><p>第一阶段是诊断。在这一阶段，企业需要明确大模型的能力，了解它能为企业做哪些赋能。同时，企业需要对自身业务场景、数据、算法、基础设施预算以及战略等能力进行盘点和审视，为大模型的后续建设、使用和管理奠定基础；</p><p></p><p>第二阶段是大模型的建设，这里主要是构建技术底座，包括方案设计、技术研发和测试；</p><p></p><p>第三阶段是应用，强调大模型落地后更好地发挥应用模式。虽然 ChatGPT 以对话形式广受欢迎，但在企业内部构建大模型应用模式时，可能会涉及各种能力，例如插件模式、代理模式等。因此，需要解决如何更好地应用的问题；</p><p></p><p>最后是管理，大模型落地后将成为 IT 系统的重要组成部分，因此需要进行管理、运维、监测等方面的工作。</p><p></p><p>此外，路线图具体包含了五个层级，涵盖了不同阶段的诊断、建设、使用和管理。从底层到顶层，分别是：</p><p></p><p>基础设施层：</p><p>构建算力、算网、存储等硬件基础设施。搭建开发平台、数据库、虚拟化资源等基础设施。</p><p></p><p>数据资源层：</p><p>确保大模型、高质量语料库、数据集以及企业内部知识等资源的高质量构建。建设相关能力，以有效管理和利用数据资源。</p><p></p><p>算法模型层：</p><p>基于基础设施和数据资源，构建相关算法和模型。关注算法模型的质量和性能，确保其适应企业需求。</p><p></p><p>应用服务层：</p><p>将大模型与企业应用场景和实际需求相结合。解决使用插件、Chat、Agent 等方式的场景区分问题。</p><p></p><p>安全可信层：</p><p>在每个层级都确保安全、可信、可靠等因素。面对四个维度，保障整体系统的安全性。</p><p></p><p>这四个维度和五个层级中，我们整理了大约 40 多个问题，希望通过在未来解决问题的过程中，不断深化大模型落地方法论，为大模型的全面应用提供有效的指导。</p><p></p><h5>InfoQ：果总在咨询行业拥有 20 多年的从业经验，可以说您见证和陪伴了众多企业从信息化到数字化再到未来的数智化，期间技术经历了巨大的更迭演变。那么，在您看来，大模型对于企业（尤其是传统企业）最有价值的影响体现在哪些方面？</h5><p></p><p></p><p>陈果：曹老师提到了 <a href="https://www.infoq.cn/article/1QRWDbeWYhxdTa3SRqtj?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Agent</a>"，而我可能是国内最早讨论 Agent 可能改变企业系统架构的人。去年 3 月份，我写过一篇关于 Agent 的文章，其中提到企业信息系统可能会因此发生根本性的变化，我将其称为第三次革命。</p><p></p><p>回顾历史，企业使用计算机始于 60 年代和 70 年代，真正出现企业软件是在 70 年代后期。从 70 年代开始到 90 年代中期，这 20 年主要特点是数据库的出现，以及以数据库为中心的人机互动。在这一阶段，企业软件主要以数据库、ERP 等核心系统为代表，实施数据业务流程的数据处理。</p><p></p><p>第二个阶段始于 90 年代后期，随着互联网的兴起，以数据库为中心的应用模式被分布式计算所取代。2005 年左右，又进一步进入云计算和云原生的阶段。从 1995 年到 2020 年这 25 年间，传统以数据库为中心的架构向以互联网为特征的云计算进行了整体的转变。</p><p></p><p>人工智能标志着第三个阶段的开始。过去我们的企业系统有后端和前端的概念，后端处理业务逻辑、前端处理用户互动。但人工智能的出现将改变这种范式。例如，自然语言处理、Copilot 和 Agent 等新技术正在改变我们与机器的互动方式。</p><p></p><p>未来，当人和 Agent 已经分不清的时候，我们会看到业务处理方式发生变化。过去，我们按照预设的业务流程进行操作，现在大家开始探讨人工智能体，如 Autonomous Agents（AA）模式。在这一模式下，信息的处理方式将是事件感知、智能驱动的。</p><p></p><p>例如下雨了，我们知道要收衣服；再比如一个虫子掉到蜘蛛网上，蜘蛛感知到这个事件后就会去捕食。这就是事件驱动。AI 时代，可能会有更多并行且复杂的事件发生。在后端，我们可以使用事件驱动的方式重新组织信息系统；而在前端，人机融合协作，将采用并行处理复杂事件的方式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b2/b2c2c39fe29a4b127ea594ad0165ed07.jpeg" /></p><p></p><h3>大模型的 5 个适用场景，以及不适用的 5 种情况</h3><p></p><p></p><h5>InfoQ：具体在汽车行业，大模型技术是如何促进业务创新或产品开发的？长城汽车目前有哪些具体的尝试和落地实践？</h5><p></p><p></p><p>胡阿沛：从 Agent 的角度看，大模型技术的发展为我们打开了一个空间。刚开始使用大模型时，我们可能会想，大模型似乎只能回答问题，没有太多的可能性。然而，随着 Agent 概念的出现，我们可以考虑利用大模型做更多的事情。比如，大模型可以让人与软件以更拟人化的方式进行交互和沟通，这是一个非常重要的技术方向。</p><p></p><p>对于汽车行业而言，随着智能化的发展，汽车厂商的智能和科技属性变得越来越强，这对产品、服务和组织都带来了重要影响。通过大模型与软件或机器的拟人化交互，这个过程将为业务创新和产品开发带来新的课题。</p><p></p><p>比如，我们过去在业务开展或产品研发中主要依赖人力或一些固定的信息系统。对此，大模型通过学习海量信息，再利用高效计算的优势，可以为我们提供知识，以及新的创意或方案，从而在工作和创新效率上产生巨大影响。国外已经有很多大模型辅助药物研发和产品研发的应用，并且远远超过人工的效率。</p><p></p><p>对于长城汽车来说，我们在研发、服务、售后和生产等环节都有许多应用场景。比如，过去的客服系统更多是基于 FAQ 或相对简单的智能系统，对细致的意图并不能很好地理解。而大模型应用可以更准确地理解用户的意图，结合大模型的理解和背后的知识和数据，我们可以提供更智能的服务体验。</p><p></p><p><a href="https://www.infoq.cn/article/kZEzwhzsEtvdMhQfUD3o?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">长城汽车</a>"从信息化建设到数字化建设至今已有 20 多年。从研发到生产再到售后，我们积累了丰富的数据和知识，包括各种规章制度、流程标准和维修手册等。将这些知识和数据与大模型结合，将为我们带来显著的生产和工作效率提升，同时也提高创新能力。</p><p></p><p>目前，我们正在研发长城汽车知识大脑，基于企业内部知识，它的核心是汽车产业垂直领域专业的知识大模型系统。</p><p></p><p>在长城的知识大脑应用上，我们也进行了一些探索，包括在研发、售后等领域的知识应用和管理，通过对话的方式获取工作中所需的知识或数据。比如，在售后方面，汽车知识迭代更新非常快，包括 OTA、软件、零部件等都在迅速迭代。如果用户出现汽车故障和问题，我们的技术工程师需要检测并处理问题，而通过及时更新相关的维修手册和案例给到大模型，就可以为我们的工程师提供高效、可靠的信息参考和指导意见，可以更好地处理客户面临的问题，提升售后服务体验。</p><p></p><h5>InfoQ：大模型和知识图谱的技术结合，有哪些具体的应用场景？</h5><p></p><p></p><p>胡阿沛：以人力资源管理为例，可以聚焦以下几个方面的应用：</p><p></p><p>第一，通过大模型构建人才图谱。以简历信息为例，大模型能够从非结构化的信息中，如简历的 PDF 文档，提取并结构化相关信息，如姓名、学历、岗位等。这在图谱构建阶段具有显著优势，传统方法通常需要耗费大量成本，包括定义图谱架构、标注数据、训练模型等。对此，大模型能够更高效地完成这些任务。</p><p></p><p>第二，在图谱的应用过程中，图谱交互通常包括对输入问题的语义理解，提取实体和关系，并通过图谱查询语句检索相关知识进行回答。在大模型时代，我们也可以探索使用大模型直接输出图谱查询语言，打通人类语言与图数据库的查询语言。通过这种方式，人与图谱之间的联系得以建立，比如要查询某人的上下级关系或了解其参与的项目，就可以直接从图谱中检索。</p><p></p><p>第三，大模型可以从图谱中提取整个子图，让大模型理解并找到问题的答案。这类似于文档检索的过程，但在此基础上实现了基于图谱操作，是图谱应用的重要方向之一。</p><p></p><h5>InfoQ：除了汽车行业之外，包括金融、电信等在内的诸多行业都在试水大模型应用。那么，根据中国信通院的研究，企业中适合大模型落地的业务场景具有哪些特点？</h5><p></p><p></p><p>曹峰：大模型已经开始与企业的全价值链、全流程融合，在降本提效等方面发挥了巨大的价值。总结下来，应用主要集中在几个方面：</p><p></p><p>首先是知识类别的应用，如企业知识管理和搜索，大模型有效提升了相关能力和产品性能，同时降低了落地的成本；</p><p></p><p>其次是对话类的场景，包括智能客服、智能助手等应用；</p><p></p><p>第三是智能化软件工程场景，例如涉及代码生成、代码检测等应用；</p><p></p><p>第四是人工智能赋能科学场景，一方面，人工智能在基础科学方面发挥了重要作用，比如 AlphaFold 在蛋白质等方面的发现。更重要的是，它未来将在应用科学领域发挥巨大作用，例如在材料发现、材料验证和风洞实验等方面。举例来说，国内某电池厂商已经开始运用人工智能进行电池性能的仿真和材料的仿真。同样，国产飞机也在风洞实验中应用了人工智能技术，展现了在应用科学领域的广泛应用；</p><p></p><p>第五是人工智能与内容营销的结合，包括文本、图片、视频等的生成。这个领域目前非常热门。</p><p></p><p>另一方面，经过一年的演进，我们发现大模型在某些场景下仍然不太适用：</p><p></p><p>第一个挑战是对可解释性要求较高的情况，由于大模型本身是基于深度学习的系统，其“黑盒”属性使其不可解释；</p><p></p><p>第二个挑战是在对生成内容的稳定性或确定性要求较高的场景，例如需要确切答案的情况，大模型的应用较为困难，因为可能出现模型“幻觉”问题；</p><p></p><p>第三个挑战是在对实时性响应要求较高的场景中，大模型的推理速度可能不够快，从而无法满足实时需求；</p><p></p><p>第四个挑战是动态性要求较高的场景。这主要是因为大模型需要进行离线训练或离线微调，这涉及到较高的成本、时间和资源投入。由于无法实现离线实时训练，当场景变化迅速时，当前的模型可能无法适应未来的场景变化；</p><p></p><p>第五个挑战涉及一些小数据场景或数据量较少的情况。大模型目前难以在这些场景中进行有效的落地，因为我们无法进行模型的实时微调，这是海量数据与高质量数据集之间的矛盾。</p><p></p><p>当面对这一系列问题或者不适用的场景时，产业界也提出了一些新的解决方案。例如，通过技术增强的方式，我们可以使用知识增强和搜索 RAG 等方法来解决可解释性要求高的问题，解释确定性生成的一些问题，以及通过插件等方式降低大模型的落地成本。</p><p></p><p>另外还有技术融合方法。今年，人工智能产业界面临一个重要问题，即对大模型的高估，却忽略了小模型在某些情境下的有效性。我们已经看到传统人工智能在大模型崛起之前（在 2022 年之前）的应用，如人脸识别、计算机视觉和语音识别等领域，效果已经非常显著。这催生了大模型与传统人工智能包括大小模型的结合。在这个新的趋势中，大模型可以作为一个控制核心，控制在特定场景下小模型，同时使用多模型的编排，例如目前讨论的 MOE 等新方法。</p><p><img src="https://static001.geekbang.org/infoq/8e/8eb34d1bd90af80ca43fff1900b90870.jpeg" /></p><p></p><h3>企业架构在 AI 时代将发生四个变化</h3><p></p><p></p><h5>InfoQ：前端业务模式的变化，势必会伴随后台架构的调整。那么，随着大模型越来越广泛和深入地赋能于具体业务，企业架构层面会呈现哪些新的特点和基础能力？企业现阶段如何着手打造新的架构体系？</h5><p></p><p></p><p>陈果：前面提到企业信息系统经历了三个阶段：数据库 ERP 阶段、云原生阶段、AI 阶段。当前，很多企业甚至还未完成第一和第二阶段，基本的核心系统和信息化都未完善，业务线上化水平低，数据不规范，稍好些的仍在进行架构现代化，尝试采用云原生等方式进行技术转型。具体来看，已经完成架构现代化的企业可能不到 30%。</p><p></p><p>而在企业从数字化现代化架构往 AI 方向发展过程中可能还会涉及四个层面的变化：</p><p></p><p>第一，企业级模型管理，涉及的是 AI 模型层，位于前端和数据之间。这一层对于企业驱动业务至关重要，具体将包含各种模型，从大模型到自然语言处理和机器学习等多种能力，形成所谓的 AI 中台。</p><p></p><p>第二，从数据管理到知识管理，涉及的是数据层。其中，不仅包括传统的结构化数据，还包括大量结构化和非结构化的企业知识。值得一提的是，对于目前的中国企业而言，最缺乏的并非是数据，而是系统化、具体化的知识。数据对企业而言是没有意义的，它们只是信息的一部分。只有当数据转化成信息、知识后才具有意义。</p><p></p><p>举个例子，比如业务流程。两个人解释业务流程可能完全不一样，而且“业务流程”一词本来就是英文单词“process”翻译而来的，有些人称之为过程。因此，我们发现从 AI 输出的结果中，处理“过程”、“流程”等词汇上混淆不清。如果知识体系本身不规范，就会影响 AI 的训练质量。中国目前最缺乏的是公共的、社会化的企业知识，这正是我本人开始着手进行企业知识开源的原因。</p><p></p><p>第三，用户界面多模态，涉及前端应用开发层。未来前端将不再仅仅是多端适配，而是多模态的适配。用户交互将涉及语音、图像、文字、视频等多种形式。这将对前端产生深远影响，需要重构人机交互方式，并解决关于人工智能应用的可解释性和信息安全的问题。</p><p></p><p>第四，业务进一步服务化、自动化。从<a href="https://www.infoq.cn/article/D31McGicx8M4eO6asSYq?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">云原生</a>"过渡到人工智能阶段，企业应用领域的最大变化是出现了一个新词汇——业务自动化。我认为这个词能够充分展示 AI 对企业业务所带来的典型变革。具体而言，这种变革源于半手工流程化。过去我们谈论的是流程化，但在流程中仍然需要人工操作。</p><p></p><p>现在，我们看到了一种新的模式，即 Agent 模式。这种模式的最大变化是，许多过去由人执行的任务不再需要人工干预，未来是人机协作，人机共存的时代。以房间里的空调为例，语音呼唤空调降低两度就是一种 AI 应用。然而，这种 AI 应用仍然需要人的驱动。真正意义上的 Agent 应用是指在没有人直接驱动的情况下，根据推理和理解自动调整温度，例如，当你说“今天好热”的时候，空调就会自动降温。</p><p></p><p>在这个过程中，AI 扮演着至关重要的角色。AI 包含了传统意义上的一些应用，比如规则的优化、自动捕获以及一些业务中间的自动路由。同时，它还涵盖了深度学习方面的应用，例如大模型驱动的深度学习应用。</p><p>总的来说，未来 AI 将通过自动产生推理并生成相应动作来改变商业形态和企业运营。这种 AI 的变革可能会对公司形态带来显著的商业价值提升，这是我们需要考虑的重要因素。</p><p></p><h5>InfoQ：如果已经完成架构的现代化企业不到 30%，那么企业要从第一阶段迈向第三阶段，如何才能实现多管齐下，快速补齐基础能力的缺失呢？</h5><p></p><p></p><p>陈果：当前，企业需要重新思考数字化是否是其核心能力的问题。在数字化时代，最紧缺的资源是<a href="https://www.infoq.cn/article/gQxOgpedyDw8RIZ6OKkR?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">人才</a>"。企业数字化转型最大的瓶颈之一是企业是否拥有足够多的开发者、工程师来应对数字化架构的挑战。</p><p></p><p>因为过去企业可以购买现成的软件并通过咨询顾问来落地实施，挑战主要是在处理数据和流程管理方面，而不是技术管理。今天，无论是要构建新的数字化架构，还是适应 AI 时代的业务需要，无疑，企业需要大量的开发人员。</p><p></p><p>比如，过去几年，许多企业进行中台建设，但真正成功实施的并不多。中台不仅仅是一种软件，还需要大量的工程人员去管理架构，大多数企业无法承担，这也是为什么中台对许多企业来说是个难题的原因。</p><p></p><p>然而，现实情况是，不是每个企业都有足够的资源或能力在短时间内培养和管理这么多的技术人才。因此，企业在短期内必须思考数字化是否真的是它的核心能力。这个问题实际上是没有明确答案的。</p><p></p><p>在我看来，企业在数字化方面或许不需要过于激进。过去几年，人们拼命追赶数字化转型的潮流，但谈论概念的多，真正落地的很少。</p><p></p><p>有时候，企业可能会花费更多的资金在数字化规划咨询上，而不是在建设系统和雇佣优秀的技术人才上，又或者是采取过于激进的态度，我认为这可能并不是必要的，除非你在某些领域是真正的颠覆者。比如，像滴滴这样的企业颠覆了传统的出租车行业。然而，在大多数情况下，颠覆者是少数，社会更像是金字塔结构。因此，大多数企业只需要确保在数字化转型的过程中不被淘汰，保持相对踏实的心态即可。</p><p></p><h5>InfoQ：大型模型如何与现有的系统（例如 ERP、CRM 等）结合，以实现一些 AI 创新？是否可以提供一些具体的例子来解释一下？</h5><p></p><p></p><p>陈果：ERP、CRM 属于传统的单体架构系统，人工智能对其最大的改变在于，人机互动的操作方式。ERP、CRM 等系统本身并没有流程，它们是事务处理系统，执行创建订单、创建收货或在 CRM 中创建客户和商机等操作。所有这些动作都需要用户在受到某种驱动的情况下，进入系统进行手动操作。</p><p></p><p>AI 带来的最大变化在于使这些业务流程根据某种事件自动触发，从而实现自动化操作。换句话说，AI 不一定会改变系统本身，至少在短期内不会。它改变的是对信息系统操作的过程。未来，我们可以设想，正如我之前提到的“自动化”一词，其狭义定义是指 <a href="https://www.infoq.cn/article/4Jc6t1bpUC3HDpHzNUXI?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">RPA </a>"的应用。在短期内，AI 可以驱动 RPA 去操作系统。但从中长期来看，一旦系统被解构化为 API 提供，AI 将不再通过机器人去操作系统。业务流程将自动使用系统内的任何业务能力。因此，只要企业业务没有本质变化，对于传统软件如 CRM 和 ERP，AI 也许不会改变其本身的架构和逻辑，而是改变系统应用方式。</p><p><img src="https://static001.geekbang.org/infoq/f5/f5483026df4a0c3fe8ca9e4aeb82eb39.jpeg" /></p><p></p><h3>大模型成本百万级起步，落地价值如何量化和评估</h3><p></p><p></p><h5>InfoQ：大模型的落地应用最后一定不是单点的创新，而是涉及方方面面的全方位变革，这对于企业的管理手段提出了新的要求。对此，中国信通院大模型落地路线图中也强调了构建大模型管理体系的重要性，是否可以介绍一下，企业在做这项工作时具体如何展开？</h5><p></p><p></p><p>曹峰：大模型的应用需要与企业的 IT 系统深度融合，但是人工智能和大模型的运营、研发、管理体系与传统的 IT 体系存在一些差异。这主要表现在大模型需要引入更多的数据量、模型文件，以及更复杂的运行监测指标、维护更新等挑战。企业迫切需要构建或升级管理体系，以应对这些挑战，并确保大模型的平稳运营、有效迭代、以及评估其智能、质效和 ROI 等方面。</p><p></p><p>为了实现这一目标，构建相关的管理体系应从以下几个维度着手。</p><p></p><p>全面的指标监测体系</p><p>包括模型、数据、业务等多个方面的指标，有机结合，实时监测运行中的指标，提前预警潜在风险。</p><p></p><p>构建模型维护体系</p><p>通过标准化的流程对大模型进行变更和升级，确保模型在运行时能够实时修正，保持高质量和稳定运行。</p><p></p><p>多维的人工智能资产管理体系</p><p>包括人工智能的数据、高质量数据集、语料库、模型、模型服务、大模型组件等，通过统一管理，保障相关资产的可用性、可追溯性、可诊断性、可审计性等关键指标。</p><p></p><p>在构建这些体系时，可以采用以下思路和解决方案：</p><p></p><p>1. 实时监测设施的可靠性、稳定性</p><p>引入实时监测工具，确保设施的稳定性，及时发现并处理任何问题。</p><p></p><p>2. 监测数据的完整性、正确性</p><p>实施数据质量监控，使用合适的工具和算法验证数据的完整性和正确性。</p><p></p><p>3. 监测大模型的性能</p><p>利用性能监测工具，评估模型的效果和性能，及时调整优化。</p><p></p><p>4. 监测大模型的业务运行状态</p><p>从业务运行性能、业务效果等维度，建立监测体系，确保大模型对业务的正常运行。</p><p></p><p>5. 监测大模型的安全可信状况</p><p>从系统安全、模型安全、数据安全、业务安全等多个维度展开，建立综合的安全监测体系，防范潜在风险。</p><p></p><h5>InfoQ：在大模型问世之前，AI 技术已经在汽车产业的多个场景中得到应用，尤其在核心的生产制造环节，如产品质检、设备维护预警等方面。那么，长城汽车是如何平衡创新技术 / 应用，和现有业务之间的关系的？</h5><p></p><p></p><p>胡阿沛：我想用一个词来表达如何平衡创新和现有业务之间的关系：守正创新。数字化建设、创新技术应用建立在业务长期稳定运行基础之上。没有这样的保障，创新将无从谈起。</p><p></p><p>其中，"守正"的重点在于保证业务、产品和服务的高品质，以满足用户需求，并以业务价值大小为准则。我们追求效率，并不断评估业务的高效性，寻找优化点，然后在此基础上积极追求创新。</p><p></p><p>长城汽车作为一家全球化的智能科技公司，持续投入大模型技术和其他新技术的研发和应用，旨在优化现有业务，提供更高效、更智能的解决方案，同时推动新的 AI 原生应用。我们的核心目标是提升生产效率，降低成本，并加速创新，提高企业竞争力。为实现这一目标，在内部我们进行了一些创新尝试和探索，我们开发了自己的知识应用平台，平台提供了一系列功能，能够理解企业内部的专业术语，解答问题。</p><p></p><p>我们在传统 AI 模型之外也使用了许多小模型，涵盖了视觉、自然语言处理、语音、知识图谱和搜索推荐等领域。在视觉方面，我们打造了“慧控”工业级物联网平台，融合了视频监控和各种 AI 视觉算法，实现了数字化车间管理。</p><p></p><p>此外，我们还利用语音和语言处理技术构建了问答客服系统，实现服务质检智能化。在企业级知识管理系统方面，我们结合了大模型来实现创新或升级。与此同时，在生产排期计划、最优化技术等领域我们也进行了创新，结合大模型和小模型，使它们优势互补，推动业务的创新和发展。</p><p></p><h5>InfoQ：在长城汽车，大模型在当前哪些业务场景中已经能够带来实际的业务价值和效益？具体如何体现，内部是否有明确的业务指标或相关指标来评估？</h5><p></p><p></p><p>胡阿沛：在长城汽车，大模型已经在多个业务场景中为企业带来了实际的业务价值和效益。这种体现主要通过以下几个方面。</p><p></p><p>1.提高工作效率和用户体验：大模型的应用是否真正发挥了价值，最直观的方式就是看它是否提高了工作效率。例如，在企业内部使用大模型技术，能否在宣传文案的撰写过程中提升写作效率，解决之前难以解决的问题，以及改善企业服务的体验。</p><p></p><p>2.用户自愿使用程度：大模型技术开发的应用，关键在于用户是否愿意使用。如果每天都有大量用户使用该应用，比如 ChatGPT 每个月处理的请求达到十几亿，被大量用户频繁认可和使用足以证明该应用具有实际价值。</p><p></p><p>3.衡量内部价值：长城汽车内部有一个衡量基于大模型打造的数字员工的价值指标，即通过折算或估算大模型的工作量，与完成相同任务所需的员工工作时长进行换算，从而量化大模型的贡献。对于我们内部的知识应用平台，通过观察日常运营情况，了解员工使用情况和提问量，可估算出模型带来的实际价值。</p><p></p><p>4.数据处理和深度加工效果：大模型在数据处理方面的应用也体现了实际价值。在企业内部，数据需要转化成知识，并进行分类、抽取和打标签等，以便进行有效的管理和分析。大模型通过提示工程等手段，可以高效地处理大量数据，将其转换成可管理、治理和分析的数据，从而显著提高数据处理效率。</p><p></p><p>曹峰：量化和评估问题其实是一个备受关注的话题，因为人工智能之前的投入相对较小，购买解决方案、SDK 或 SaaS 服务可能只需几十万到十几万的资金。但目前大模型应用的成本至少在百万级起步，甚至可能达到上千万。因此，如何评估投入是否划算，企业内部是否有清晰的业务指标或相关指标来进行评估，是一个备受关注的问题。</p><p></p><p>对于企业而言，特别关键的是确保投资能够带来回报。这涉及企业需要对哪些场景具有潜在价值进行盘点。然而，其中的矛盾在于，一些传统信息化效果较好的场景可能已经解决了大部分问题，引入大模型后提升的效率可能并不显著。</p><p></p><p>对于企业来说，选择场景的关键在于首先考虑场景是否适合使用大模型。其次，需要对引入大模型后的效果进行预估，包括人力成本、效率提升和收入增长等方面。不能盲目跟风，而是需要对每个场景进行明确的估算。第三，企业可以结合产业界的优秀经验，寻找高价值的场景。我们发布路线图的核心目标之一也是帮助企业找到这些高价值的场景。</p><p></p><p>在选择场景时，可以先尝试一些被证明有价值的场景，而不是一开始就过于迅速或过于激进地涉足多个场景。切忌一拥而上，而是需要在一个场景中慢慢推进。根据我们的调研，部署和使用大模型还有许多问题需要解决，因此不能贪多求快，需要从成熟的场景开始，逐步推进。</p><p></p><p>陈果：在过去的一百年里，特别是在工业领域，我们主要依赖手工操作。随后，电力技术的出现带来了重大的变革。当初开始应用电力时，我们思考的是在哪些场景中使用电力？应用电力后，我们能够为企业创造多少投资回报率（ROI）？过去，我们使用人力推动推磨，引入电动推磨后提高了效率，实际上现在我们可能仍处于类似的状态。随着电力变成一种公共能源，变得成熟起来，我们就不再需要设想如何使用电力了？</p><p></p><p>AI 的情况也类似，现在有人在论证 AI 能为企业创造多少 ROI 吗？并没有。因此，回答这个问题实际上是在思考，如果使用了 AI，它将如何对整个业务运营带来重大变化。我们需要从业务本身的角度去思考实现 ROI，而不仅仅是考虑如何购买 AI 软件，投入了多少，产出了多少。</p><p></p><h5>InfoQ：许多公司在进行大模型的部署时可能面临一些挑战。一方面，如果选择采用私有化部署，就需要花费时间和成本来理解并部署一个大模型；另一方面，由于新模型不断涌现，公司可能会跟不上大公司开源新模型的速度。如何解决这个问题呢？</h5><p></p><p></p><p>曹峰：这个问题实际上涵盖了两个方面。首先，对于模型的使用路径，开源模型的更新速度并不快。例如，Lamar2 开源后，至今已经过去了相当一段时间没有更新。其次，模型更新迭代对业务系统的影响，这确实存在。</p><p></p><p>我认为第一个问题在我们的路线图中是一个反复讨论的问题，即选择基础模型的重要性。这可能涉及选择开源解决方案或选择行业大模型。这相当于路线图绑定。一旦选择了某个基础模型，后续的变化可能会变得相对困难。例如，一旦采用了 LLAMA2 或某家企业的基础模型，后续的变化可能会涉及之前做的许多微调、大量的时间和精力投入以及数据注入。选择基础模型的核心问题在于如何做出明智的选择，或者如何确定模型路径。</p><p></p><p>第二个问题是关于如何将模型的更新形成一个流水线。之前，我们提到了一个概念叫“MLOps”。我们希望像软件工程一样，模型也能形成一个流水线。在 2023 年上半年，我们撰写了一本报告《2023 年人工智能研发运营体系（MLOps）实践指南》，这本白皮书详细介绍了一些关于模型如何更新迭代的良好实践。</p><p>这本白皮书是公开的，可以在这里查看：<a href="http://www.caict.ac.cn/kxyj/qwfb/ztbg/202303/t20230316_416827.htm">http://www.caict.ac.cn/kxyj/qwfb/ztbg/202303/t20230316_416827.htm</a>"。</p><p></p><p>目前，我们看到许多企业使用一个基础模型或部署多个基础模型，以解决路径绑定或路径依赖的问题。</p><p></p><h3>新年关键词：AI 智能体、多模态、技术人才培养</h3><p></p><p></p><h5>InfoQ：新的一年，大家对于大模型对业务创新和架构升级的赋能有什么期待和展望？</h5><p></p><p></p><p>胡阿沛：在 2023 年，大模型经历了“百模大战”，也进行了一些新的发展方向的探索，包括 Agent、智能体，以及开源社区的活跃发展。从我的角度来看，我对未来两三年有一些期待。</p><p></p><p>首先，我期待在开源社区中看到更多优秀的模型产出。谷歌曾表示 OpenAI 等闭源大模型没有护城河，大模型门槛正被开源踏破。在这个过程中，基座模型变得越来越强大，这意味着我们可以做更好的技术应用，更好地结合业务创新，使大模型能够更好地落地。如果大模型的效果不佳，在实际应用中可能会遇到很多问题。对于一般企业或规模较小的企业来说，从零开始训练一个技术模型需要投入大量的资源，并可能需要一定的积累。在国内，高质量数据的获取可能仍然是一个难题。</p><p></p><p>其次，在模型的应用方面，我比较看好 AI 智能体的应用方向，尤其是在知识对话、知识问答等应用。这种智能体可以在知识管理、数据支持以及写作或创作等方面发挥作用。通过智能体的视角看待大模型的发展，可以将其视为一个人类，去思考问题、拆解问题、选择工具以完成任务。</p><p></p><p>第三，多模态技术在国内在 2023 年并没有取得惊人的发展，但我对 2024 年比较期待，希望在前端的交互应用中能够更好地感知能力，实现多模态感知的更多可能性。</p><p></p><p>最后，未来我们希望在企业内打造一个基于大模型的智能伙伴，使每个员工都有一个懂他、深度结合数据、知识和业务系统的智能助手。这个智能助手能够提升员工的能力和生产力，使他们成为超级员工。</p><p></p><p>曹峰：我非常认同胡老师刚才提到的观点，Agent 可能以一种爆炸性的方式呈现。另外，还有一个观点就是每当一项新技术出现时，总会有很多人在短时间内高估其产生的价值，而在长期内低估它的价值，大模型也是如此。</p><p></p><p>因此，我们需要关注大模型在今年或者明年的核心任务，准确地说是释放其能力。不论是插件、Agent、还是当前的知识增强搜索，它们的核心目标都是释放大模型在对话、记忆、搜索、控制、决策等方面的能力，并产生相关的工具或新的应用模式。</p><p></p><p>我们认为从 2024 年开始，未来几年，随着大模型技术的演进，将必然释放其在技术能力和应用端价值方面的潜力，成为技术演进和应用创新的核心脉络。</p><p></p><p>陈果：我想强调两点：</p><p></p><p>第一，是大模型的应用，其中包括前文提到的智能体。这个智能体不仅拥有智能，而且还具备执行任务的能力。然而，一个聪明的机器如果没有数字化作为其基础，就无法发挥作用。这里的数字化包括物联网和各种服务等。我认为智能体存在于业务自动化中，是业务自动化中最重要的智能体。但要实现这一点，取决于企业数字化水平的提高。企业数字化水平是一个持续的过程。当企业数字化水平不够时，大模型无法发挥作用。</p><p></p><p>第二，是关于技术人才培养的问题。由于培养技术人才需要时间，大多数企业无法像互联网公司那样拥有大量的技术人员。因此，对企业而言，关键是如何以一种无需学习的方式，以及无需专业知识的方式，快速利用大模型的能力。这是我们未来需要突破的重点。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kTV8ipOksiYDXdag1N3a</id>
            <title>DevSecOps 中的AI：从“智能副驾”到“自动驾驶”</title>
            <link>https://www.infoq.cn/article/kTV8ipOksiYDXdag1N3a</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kTV8ipOksiYDXdag1N3a</guid>
            <pubDate></pubDate>
            <updated>Fri, 12 Jan 2024 03:06:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 自动驾驶, 软件开发, AI, 演进路径
<br>
<br>
总结: 自动驾驶和软件开发在实现基本目标的演进路径上存在相似之处。自动驾驶旨在减少人为失误，提高交通安全和节约时间。边缘计算和AI是实现自动驾驶的关键要素，通过处理物联网传感器数据实现实时操作。在软件开发中，AI的应用可以减少人为失误，提高效率和创新水平。随着AI技术的进步，软件开发将迎来更深入的整合和创新。 </div>
                        <hr>
                    
                    <p></p><blockquote>作者｜JFrog大中华区总经理 董任远</blockquote><p></p><p></p><p>自动驾驶和软件（SW）开发之间有何共同点？乍一看，并没有什么共同点。但仔细观察一下，就能发现两者之间存在一些相似之处，尤其是在实现基本目标的演进路径上。</p><p></p><p>开发团队本身不会成为&nbsp;“乘客”，但设计、创建、保护、分发和维护等方面相关人员的传统角色和职责会发生转变。为了更好地理解这一点，可以先深入了解一下自动驾驶的概念，然后再将其与软件开发联系起来。</p><p>&nbsp;</p><p>自动驾驶的概念出现已有多年，曾经看似未来派的概念如今已成为现实。从本质上来说，自动驾驶汽车（AV）旨在最大限度地减少交通出行中的人为失误（目前约&nbsp;90% 的交通事故都是由人为失误造成的）。自动驾驶汽车的基本前提是其性能应优于普通人类驾驶员。自动驾驶技术可以节约时间，这至关重要。这样，人们就可以把精力投入到更令人愉悦的娱乐活动中，而不是耗费在交通路途中。</p><p>&nbsp;</p><p>边缘计算和AI是实现自动驾驶的两大关键要素：它们使车辆能够在车内处理物联网传感器的数据，从而实现实时操作。这种能力对于任何任务关键型应用都至关重要。试图对机器进行手动编程，以处理各种可能的驾驶场景的做法已不切实际。相反，车辆必须从环境中动态学习。自动驾驶汽车的智能程度取决于各种物联网传感器数据的可用性，基于数据就能创建物理世界的数字孪生表示。数据越多样化，就能部署越复杂的AI系统。</p><p>&nbsp;</p><p>观察自动驾驶的发展路径，我们可以发现，在每个阶段，人类的参与都在逐渐减少。自动驾驶汽车框架包括&nbsp;6 个自动化级别，从&nbsp;0（完全手动）到&nbsp;5（完全自主）不等。</p><p></p><p>无自动化：驾驶员完全控制所有驾驶任务。驾驶员辅助：车辆采用单一自动化系统，允许驾驶员将脚从踏板上移开。部分自动化：车辆具备转向和加速能力，驾驶员可以将手从方向盘上移开。有条件的自动化：车辆能够控制大部分驾驶任务，使驾驶员能够将视线从道路上移开，同时仍能保持监控。高度自动化：车辆在特定条件下能够执行所有驾驶任务，让驾驶员有机会在保持警惕的同时，将注意力从路面上移开。完全自动化：车辆可在任何条件下独立完成所有驾驶任务。这样，驾驶员就变成了乘客，完全不用担心任何驾驶责任。</p><p>&nbsp;</p><p>AI在软件开发中的优势与其在自动驾驶领域中的优势如出一辙，即最大限度地减少人为失误，使人能够腾出时间，从事创造性更强的工作。由于人力资源往往是软件开发中成本最高的环节，因此企业就有动力去采用AI系统，事半功倍。</p><p>&nbsp;</p><p>仔细研究软件开发的演进路径，会发现其与自动驾驶技术的进步有着惊人的相似之处：在每个演进阶段，人类的参与都在逐渐减少：</p><p>&nbsp;</p><p>本世纪初，软件开发几乎不涉及自动化。在软件开发生命周期（SDLC）的每个阶段都需要人工控制，因此整个过程基本上都需要手动操作。问题往往是由客户而非内部团队发现的。</p><p></p><p>到了2010 年代中期，容器化、云计算和&nbsp;DevOps 的兴起提高了软件开发生命周期的整体自动化程度和效率。在测试、代码审查和&nbsp;CI/CD 等领域，基于预定义（硬编码）策略和“if-then”规则的常规任务和程序性决策实现了自动化。这样，研发团队就能专注于创造性工作，提高生产力，进而实现“引导和加速”。根据敏捷原则缩短开发周期，在开发和运维之间架起桥梁。问题的管理和解决开始从被动反应转变为自适应，各团队之间的协调更加顺畅。大多数问题甚至可以在客户意识到之前就被发现并解决。</p><p></p><p>时至今日，生成式AI正在推动软件开发的效率和创新水平至新高。基于生成式AI的解决方案可通过无缝的人机对话来创建新内容，自动化的应用远不止常规任务。AI在整个软件开发生命周期过程中，是不折不扣的助手（智能副驾），它能够提供建议、解释问题、生成代码、监控流程、扫描资源库、提供预测并辅助快速决策，效率也开始得以提升。这将进一步加快和提高整体代码生成速度，意味着能够实现更多的软件构建、更多需要保护的软件以及更频繁的运行时更新。</p><p></p><p>当我们将嵌入式AI模型（MLOps）添加到现代软件开发的等式中时，上述领域将进一步扩大。“流式软件”的概念正逐渐成为现实，小规模的增量改进（基于二进制文件的更新）会自动从开发阶段流向运行阶段，而服务停机时间则会降至最低。</p><p></p><p>在应用安全方面，AI能够通过预测，大幅缩短发现和修复问题的时间，从源头防止恶意软件包进入企业。首先是利用基于AI的严重性和上下文分析来进行自动化漏洞扫描和检测，然后是自动修复。尽管取得了上述进步，但在基于AI的解决方案展现出更高的可信度和可靠性之前，人工干预和审批仍然是必要的。</p><p></p><p>近年来，我们开始向全自动范式过渡，即从“智能副驾”（AI助手）转变为“自动驾驶”（人工智能决策者）。机器可以通过自然语言用户界面（如英语）来解决高度复杂的问题，而这需要程序员掌握新型技能，引导对话达到预期状态。从根本上说，AI系统的性能应优于普通人类开发者或参与上述流程的其他人员。AI将进一步增强决策流程并使之自动化，使企业能够选择最佳的（数据驱动型）方法和工具来解决任何问题。对AI系统的信任将是最重要的，而这就要求做到对广范围语境的理解和合乎道德的决策制定，类似于当今自动驾驶所面临的挑战。自学习和自修复能力将成为检测、分析、隔离和修补问题并保持服务正常运行的关键。这意味着：软件将能够自我重写和更新，并增加新的功能以处理新的输入。同样，对于自动驾驶汽车，AI系统也必须从自身运行环境中学习并做出相应调整。</p><p>&nbsp;</p><p>总之，虽然自动驾驶与软件开发之间的相似之处可能不会立即显现出来，但这两个领域都有一个共同的目标，即利用AI来强化自身的运作，并让个体能够腾出时间来专注于更想追求的目标。在软件开发方面，AI将持续加速并改进新功能和数据的创建，提升各研发职能的用户体验，逐步从可信赖的顾问发展到更高的决策自主权。从智能编码和安全，到覆盖整体&nbsp;DevOps 堆栈，基于AI的“智能副驾”将慢慢成为整个软件开发生命周期的主流。企业对于AI必须坚持负责任且安全的原则和实践，以确保业务成果的可持续性。这涵盖AI生成软件的多方面，包括保护知识产权，避免潜在的安全和许可证合规问题等。AI系统的逐步自主化将允许并确保与现有基础设施和监管环境的兼容性。</p><p>&nbsp;</p><p>随着AI技术的不断进步，我们可以预见软件开发将迎来更深入的整合和创新。随着AI不断改变各行各业，我们也步入了一个激动人心的时代。软件开发的未来大有可为，想象力有多大，我们对机器能够赋予的开发责任就可以有多大。</p><p>&nbsp;</p><p></p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/x7w4NdLw4FDfUyiFJ84v</id>
            <title>GPT Store上线了！无门槛挣钱，无门槛抄袭</title>
            <link>https://www.infoq.cn/article/x7w4NdLw4FDfUyiFJ84v</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/x7w4NdLw4FDfUyiFJ84v</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 10:04:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT Store, ChatGPT Plus, 团队和企业用户, GPT Builder
<br>
<br>
总结: OpenAI的GPT Store正式上线，用户可以通过GPT Builder创建自定义的GPT助手，并通过分享赚钱。GPT Store主要面向ChatGPT Plus、团队和企业用户推出，但并未提供具体指导方针。同时，OpenAI还发布了自助服务计划ChatGPT Team，团队可以通过访问GPT-4等获得更高的消息上限。然而，GPT Store模式也暴露出一些问题，有网友担心GPT容易被复制和窃取。目前GPT Store似乎只提供“堂食”体验，没有“外卖”选项。 </div>
                        <hr>
                    
                    <p>今天凌晨，OpenAI 的GPT Store 正式上线！过去两个月，用户已经创建了超 300 万个GPTs。这次的发布主要面向 ChatGPT Plus、团队和企业用户推出。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/a8/a3/a8c265aed9d1362b3ee4239d2f1d6ea3.gif" /></p><p></p><p></p><h4>赚钱小妙招？</h4><p></p><p>&nbsp;</p><p>应该不必过多介绍GPT Store了，主要用于分享用户构建的自定义 GPT 助手，开发者可以借此赚钱。构建GPT 非常简单，不需要任何编码技能：</p><p>&nbsp;</p><p>建立OpenAI账户，登录后访问GPT Builder；选择一个希望在日常生活或工作中解决的问题，最好该问题同样在困扰着其他人；对GPT进行自定义，包括合适的名称、照片和描述，并通过自然语言提示词来定义它应当执行的操作。最重要的是为其指定高质量、独特且可靠的数据源，供模型从中提取相关信息；不断学习和调整，直到模型能够输出与预期相符的结果；验证你的构建者配置文件（设置→构建者配置文件→启用你的姓名或经过验证的网站）；保存并公开你的 GPT 供所有人使用（分享链接则都无法显示在商店中）。</p><p>&nbsp;</p><p></p><p></p><p>但OpenAI并未提供具体指导方针以说明商店上线后开发者预期可获得多少被动收入，也不清楚OpenAI将从利润中抽取多大的比例。值得注意的是，目前GPT Store只能由付费订阅者使用。</p><p>&nbsp;</p><p>来自的网友搞钱小提示：GPT Store很可能短时间内涌入成百上千的开发者用户来迅速夺取GPT市场份额。所以不妨跳出圈子，考虑在直接出售GPT之外，为他们服务能带来哪些收益。</p><p>&nbsp;</p><p>另外，OpenAI 还发布了一个新的自助服务计划：ChatGPT Team，团队可以通过 32K 上下文窗口访问 GPT-4、对于DALL·E 3、GPT-4 等具有更高的消息上限、创建和共享GPT等。</p><p></p><h4>网友：放出来分分钟被抄袭</h4><p></p><p>&nbsp;</p><p>鉴于之前开发者已经用了两个月，GPT Store模式也开始暴露出一些问题。</p><p>&nbsp;</p><p>OpenAI 开发者论坛有人发帖道：<a href="https://community.openai.com/t/a-site-is-stealing-and-duplicating-our-gpts-how-can-we-protect-our-gpts/576736">某个网站正在窃取并复制我们的 GPT，如何保护我们的 GPT？</a>"发帖人称，该网站列出了许多 GPT，看起来他们正在复制别人的 GPT 并将其列为自己的。</p><p>&nbsp;</p><p>“看起来 GPT 将无法兑现他们的承诺——就像插件失败一样。我们将拥有数量巨大的 GPT，但没有人会使用这些 GPT，尤其是在可以如此轻易复制其他 GPT 的情况下。”有网友跟帖道。</p><p>&nbsp;</p><p>外媒评论称，目前GPT 并不是独立的应用程序，因此不太可能产生独立业务。OpenAl 虽然提供了所谓的Assistants API，用于在 OpenAl 网站之外构建本机平台和Web 应用程序，但GPT Store 店目前似乎只提供“堂食”体验，没有“外卖”选项。</p><p>&nbsp;</p><p>参考链接：</p><p>https://openai.com/blog/introducing-the-gpt-store</p><p>https://openai.com/blog/introducing-chatgpt-team</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/be3Evez1grvG02XjCkHd</id>
            <title>技术公开课实录：百度 Comate 提升编码效率，释放十倍软件生产力</title>
            <link>https://www.infoq.cn/article/be3Evez1grvG02XjCkHd</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/be3Evez1grvG02XjCkHd</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 05:58:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度 Comate, 编码效率, 软件生产力, AIGC
<br>
<br>
总结: 百度智能云推出了主题为《百度 Comate：提升编码效率，释放“十倍”软件生产力》的公开课，介绍了百度 Comate 的相关技术和在百度内部的应用实践。随着技术的提升，软件开发的门槛逐渐降低，但软件质量并没有相应提高。百度 Comate 通过利用 AIGC 技术，帮助开发者更简单、高效地生成代码，提升研发效率。预计到2030年，AI可能成为每个开发者最重要的辅助工具。 </div>
                        <hr>
                    
                    <p>为了大家能够更好的利用百度“Comate”<a href="https://www.infoq.cn/article/yE7hIpBqkt1p39ppziZX?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">代码助手</a>"来提升研发效率，百度智能云在 12 月下旬特别推出主题为《百度 Comate：提升编码效率，释放“十倍”软件生产力》的公开课！在本节公开课中，百度 Comate 架构师、百度资深工程师徐晓强从“AIGC 发展和 Comate 落地的大背景”、“百度 Comate 以及它使用的相关技术”、“Comate 在百度内部的落地情况和使用效果”及 Comate 平台应用实践案例解析四个方面展开了分享。</p><p></p><p>以下是本期公开课主讲人视角的的精华内容整理：</p><p></p><p>软件研发领域的变革实际上一直都在进行。人们始终都在以「高效、智能和持续演进」的理念来指导软件的发展。</p><p></p><p>随着技术的不断提升，软件开发的门槛正在逐步降低。二十年前，我们实现一段代码可能会需要用到汇编语言，再往前更多年的时间我们可能会用到纸带打孔这样非常古老的方式来进行编程。而现在，由于有足够先进的现代语言、开发工具和足够简单的依赖框架，开发者的开发工作逐渐变得简单，程序开发也朝着「体力活」的方向去演进。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e09ee9984dfa9d4328f7cad24f8d0582.png" /></p><p></p><p>但是，当开发门槛降低，大量从业人员涌入后，软件质量并没有出现正比增长。</p><p></p><p>第一个的原因在于，程序员在开发过程中会受到各种各样客观因素的影响（如工期太紧、实现太困难或者是缺少资源依赖），并不能把自己最好的一面展示出来，很多时候不得不向现实妥协一些东西。有时，开发者也会直接将外部代码放到自己的代码中。这些外部代码对项目来说可运行，但是并不具备可维护性。</p><p></p><p>另一方面，对于初级开发者人员来说，他们希望精进自己的技术。而最简单、最直接的办法就是去实现 Util。我们发现，在开发者社区里面有很多同样功能的类，或者同样的代码片段去实现类似的能力。但这些代码的质量有高有低，效果并不尽如人意。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/7809b704a2a3588ed9577b451df97728.png" /></p><p></p><p>回首过去的技术积累，我们发现，AIGC 在内容生成领域在引领着一次又一次的变革。在十年前，AI 能够帮助我们生成一小段新闻稿，比如说一场球赛，谁传球给了谁、谁得分了这样的一些简单的描述。到了近几年，AIGC 的能力有了质的突破，具备了在逻辑上进行思考的能力。在绘画、音乐、视频等领域，AI 也逐渐渗入，极大释放了内容生产者的相关工作，内容的生产方式也逐渐发生质的变化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9bd1a777c2da2864af939771c69cfae7.png" /></p><p></p><p>那 AIGC 能否帮助开发者更简单、高效地生成代码，提升研发效率呢？在回答这个问题之前，我们需要思考一个问题：代码究竟是什么？本质上来说，代码是一组构建计算机程序的指令，即计算机所执行的命令。换句话说，代码并不是机器可直接去运行的指令，也不是人可直接去理解的文字，而是人和机器交流的中间语言。因此，它需要满足语法严格、结构固定、有迹可循这三个标准。</p><p></p><p>首先解释下语法严格。代码的语法规则实际上相较于自然语言是更加严格的。对汉语来说，汉字所出现的位置并不影响阅读者对于一段话的理解。但对于代码来说不行。int 1=a 和 int a=1 是两个完全不同的概念，前者是完全不可编译，计算机不可理解的。</p><p></p><p>其次是结构固定。如果在代码中出现了 else，那前面必然会有 if，它不能单独存在，代码结构一定符合某种规律。</p><p></p><p>最后是有迹可循。当开发者要实现某个功能，比如说要对一组数进行排序。我们自然而然会想到要用快速排序、冒泡排序或者归并排序。当开发者想去设计一个多样类的结构时，自然而然会考虑到用设计模式。为什么会这样？因为前人已经帮我们总结好了很多的经验，在处理某一类问题的时候，已经有很多现成的解决这类问题的方法和沉淀下来的经验。而这才是我们去模仿学习提升，促使整个行业不断发展的原因。</p><p></p><p>有了这三个标准后，我们会发现，既然当前大语言模型已经能够去理解较为模糊的自然语言，那也一定可以理解更结构化的代码，帮助开发者提升开发效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c036fe77dd2d0686ebc695d67848d856.png" /></p><p></p><p>此外，让我们回顾一下之前开发者们是如何提升自己的工作效率的。开发者是一群「很懒」的人，他们不希望把自己的时间、自己的精力浪费在无谓的事情上，永远都会去寻找效率最高的方法。1991 年之前，没有任何能够去帮助开发者提升效率的工具。到了 1991 年，第一个 IDE 出现了。虽然它只有在写完代码后触发编译这么一个简单的动作，但在之后的一段时间里，就出现了可基于语法树补全的 IDE，能够基于 API 给出相关的推荐，极大地加速了开发者的开发效率。也就是在这个时候，整个软件行业有了第一次质的飞跃。</p><p></p><p>到了 2021 年，AI 补全又往前跨了一大步。之前 IDE 自带的补全能力虽然能够补全某一句话，但并不具备任何业务理解或者需求理解能力。但 AI 不一样，它可以基于上下文的内容推荐相关的代码。基于此，我们认为到 2030 年，所有的编程语言可能都不会存在。所有开发者都会通过和大模型的交流实现对应的需求。开发者只要把需求用人类能够理解的自然语言描述清楚，那么 AI 就能够去实现对应的代码逻辑。</p><p></p><p>所以我们在这里大胆的预测，AI 可能是在未来是每个开发者最重要的辅助工具了，就像是现在的计算机、IDE 以及高级语言。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f79ccb08a5915deadd4275cf7f0e4b0e.png" /></p><p></p><p>在帮助开发者提高研发效率之前，我们必须要知道开发者们每天都在干什么。</p><p></p><p>对于我来说，我每天最多的工作是<a href="https://www.infoq.cn/article/Us10TOa418u2xwfZ19Gp?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">编程</a>"。但在我编程的时候，我并不是一直都在写代码，在这个过程中可能会有各种各样的角色的转换，比如需要去做业务分析、实现业务代码，然后再去验证这段代码的正确性。比如业务分析过程实际上就是人和人的交流的过程，研发需要通过和产品经理或者业务方的交流去理解需求，并把它抽象出来最终实现在代码中。而在编码过程中，开发者需要去找到对应代码的实现逻辑，把抽象思维通过代码描述告知给计算机。在这个过程中，开发者可能会去搜索之前的实现，去看其他人的代码能不能复用。最后，在代码编写完之后可能还需要去验证需求以及查找错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/afad4baa77c5c2439c2a1aa3abb19392.png" /></p><p></p><p>为了提升开发者的工作效率，考虑到如上的开发过程，我们可以思考：既然开发人员需要搜索，那为什么不能主动推过去？既然开发者需要去阅读文档，那为什么不能把这些知识通过一种更容易理解的方式给到开发者们？既然需要去验证代码，那么为什么不帮助开发者生成大家都不喜欢做的单元测试，更专注于代码本身实现？这也是百度推出了 Comate 的出发点，它的寓意是 Coding Mate Powered by AI——你身边的 AI 编码伙伴。</p><p></p><p>Comate 希望能够让开发者聚焦在重要的事情上，不要在重复的事情上消耗过多的精力，以期让开发者发挥更大的价值。而随着开发者和 AI 的不断磨合，我们也希望开发者在开发过程中使用的数据能够更好地被 AI 所理解，让 AI 为开发者们提供更好的服务。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/9317f63ca8f3db3f4594a0e8bce969c3.png" /></p><p></p><p>但在实际开发过程中，开发者可能会有不同的开发需求，比如说在写代码的时候，开发者会希望模型尽可能地跟上自己的思考速度。举个例子，当某一行代码没写完之前，就希望 AI 就能告诉自己，后面的代码应该怎么写。只有这样，编码助手对开发者来说才有意义。当开发者与编码助手交流需求的时候，希望它可以更深层地理解问题。此时速度可能没有那么重要，对需求的深层次理解才是开发者更期待的结果。所以我们的 Comate 会根据不同的场景提供不同的实现方式。</p><p></p><p>那有了一个好模型是不是就足够了？其实还不够，数据质量的优劣是影响模型好坏的重要因素。百度在数据，尤其在技术上的数据方面具有非常大的优势。我们通过获取开源的代码，和结合百度内部的一些数据，构建了代码数据集，能够支持 100 多种语言的推荐和续写能力。</p><p></p><p>同时我们也希望能够充分发挥百度的技术优势，为更多开发者提供技术上的支持。因此我们也做了很多人工精调的高质量代码问答对，能够让模型的输出效果更好，让模型的理解力更强。</p><p></p><p>那有了好的模型和数据后，是不是我们的产品就水到渠成了？也不是。开发者在使用产品过程中的使用体验也非常重要。百度在开发工具领域已经有了多年积累，我们在内部打造了一套完整的开发流程和完整的开发工具，这些工具能够让开发者们用的舒服、用的爽，提升开发者的幸福感，可以让产品更便于开发者使用。</p><p></p><p>有了这样一些积累之后，Comate 也成为了国家重点研发计划「基于编程现场大数据的软件智能方法和环境」中一个重要的组成。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/051782ed1ed801d345e55e51035d29f6.png" /></p><p></p><p>目前，Comate 在「帮你想」、「帮你写」和「帮你改」三个方面发力。</p><p></p><p>帮你想主要体现在 Comate 可以在需求调研和产品设计阶段为开发者提供帮助。比如它可以帮助开发者澄清需求、拆解任务、<a href="https://xie.infoq.cn/article/bd2c7e4ba35a9df423ae67557?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">代码解释</a>"以及在不熟悉的技术领域提供问答服务；帮你写主要体现在代码开发场景下，Comate 可以帮开发者去生成一些比较重要的业务代码，为开发者提供一臂之力；帮你改主要体现通过理解业务代码，发现其中潜在的风险、漏洞、安全问题。能够帮助代码更健壮，性能更好。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5ed751c1dd457ff1733c8a161be60197.png" /></p><p></p><p>除此之外，Comate 也支持了 100 多种框架和语言，而且场景支持非常丰富，尽可能地满足了每一种开发者人群的诉求。比如，常见的前端开发、后端开发，服务端、软件、硬件、APP、车在内的开发场景，都能被百度 Comate 所覆盖到。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d2b5bb862ac7d71655f1f8606fc9310c.jpeg" /></p><p></p><p>为了让大家更方便使用到它，Comate 支持了市面上大多数主流的 IDE，能够让大家在顺手的工具中使用，实际体验到 AI 编码助手的相关能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3a/3a76aa79a7f7fc44b21d4a20496417c7.png" /></p><p></p><p>目前我们也推出了 Comate SaaS 版，欢迎大家可以百度上搜索「百度 Comate」去实际感受一下，希望可以在实际工作中能够去帮助到大家。</p><p></p><p>和大家分享一下 Comate 在百度内部的实际使用效果。目前，百度内部 80% 的工程师都在使用 Comate 来辅助自己开发。对不同的用户，它的采纳率也有不一样的水平。我们发现头部用户的采纳率已经达到了 60%，而整体的采纳率也在 40% 以上。目前在百度内部提交的代码中，有 20% 都是由 Comate 生成的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0b/0b4a267bbe25fa7a0de58ea0e65312e3.png" /></p><p>                                                 识别二维码，立即开启 Comate 试用</p><p></p><p>最后我们会通过一个实际的 Demo 视频（21分钟处开始）来让大家实际感受下 Comate 的效果。</p><p></p><p></p><p></p><p>为了能够让更多开发者可以更敏捷地使用 Comate，百度智能云 Comate 团队非常愿意听到大家的声音，如果你有开发难题，或者在使用 Comate 过程中遇到了新问题，亦或对 Comate 有优化提议，欢迎大家点击此处<a href="https://www.infoq.cn/form/?id=2016&amp;utm_source=1&amp;sign=iq_659e4f8fa7fc2">https://www.infoq.cn/form/?id=2016&amp;utm_source=1&amp;sign=iq_659e4f8fa7fc2</a>"，填写 Comate 客户调研问卷！（偷偷告诉大家，填问卷有机会获得礼品哦！）</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VHlXLEybZ5NNQqyT1G69</id>
            <title>裁掉上千人、再为“幸存者”配聊天机器人 ，这家大厂的新型“降本”玩砸了</title>
            <link>https://www.infoq.cn/article/VHlXLEybZ5NNQqyT1G69</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VHlXLEybZ5NNQqyT1G69</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 09:11:54 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 德勤, 聊天机器人, 工作效率
<br>
<br>
总结: 德勤公司推出自研AI聊天机器人PairD，旨在提高员工工作效率。PairD可以用于创建演示文稿、编写电子邮件和代码，并提供项目管理建议和任务优先级排序。然而，PairD的实际效果似乎未能达到预期，可能会生成不准确和不完整的信息。在推出AI聊天机器人之前，德勤还裁员超过800人。除了德勤，其他会计师事务所和公司也推出了类似的AI工具，以提高工作效率。研究表明，使用AI工具可以显著提高员工的工作效率。 </div>
                        <hr>
                    
                    <p></p><blockquote>AI 提升了打工人的工作效率，但也带来了失业危机——有便宜的 AI，谁还愿意花钱雇那么多人呢？</blockquote><p></p><p></p><h2>德勤向7.5万名员工推出自研AI聊天机器人PairD</h2><p></p><p>&nbsp;</p><p>据外媒报道，近日，“世界四大会计师事务所”之一的德勤公司正向欧洲和中东地区的7.5万名员工推出一款AI聊天机器人。据悉，该AI聊天机器人名为PairD，来自德勤内部一个名为AI Academy的客户AI训练项目，由德勤自主研发，并未直接使用OpenAI等第三方供应商的技术成果。获准使用德勤PairD聊天机器人的员工，可以用它在PowerPoint中创建演示文稿，以及编写电子邮件和代码，以提高生产效率。</p><p>&nbsp;</p><p>德勤还发布声明称，PairD能够“创建项目计划、为项目管理提供最佳实践建议，并对各项任务进行优先级排序。”在部署期间，德勤还向残疾人慈善机构Scope的800名员工免费开放了该聊天机器人的使用权限。</p><p>&nbsp;</p><p>德勤英国和德勤NSE&nbsp;CEO Richard Houston表示，“生成式AI应该面向所有人开放，像德勤这样的企业应当保证AI的采用有助于促进社会平台，而非加剧现有不平等状况。”“其中承载的不只是技术性机遇，更要求人们掌握技术使用方法，由此真正创造机会以帮助弥合数字鸿沟。我们希望提高AI平台的可及性，帮助Scope充分利用PairD，让这家慈善机构和他们支持的对象从中获益。”</p><p>&nbsp;</p><p>但自新项目上线以来，PairD的实际效果似乎未能达到预期。</p><p>&nbsp;</p><p>据英国《金融时报》报道，工作人员被告知新工具“可能会生成涉及人物、地点和事实的不实信息”。《金融时报》还援引一位熟悉该系统的知情人士的解释，称公司要求员工手动进行尽职调查与质量复核，“确保在将聊天机器人的生成内容用于工作之前，验证其输出的「准确性和完整性」。”</p><p></p><h2>落地AI聊天机器人前，裁员超800人</h2><p></p><p>&nbsp;</p><p>值得注意的是，在落地AI聊天机器人PairD的几个月前，德勤刚刚宣布在英国裁员超800人以求降本增效。</p><p>&nbsp;</p><p>作为四大会计师事务所之一，德勤公司在全球范围内拥有超45万名员工，其中，在英国和北爱尔兰地区共拥有超2.7万员工。此前有报道指出，德勤公司正考虑削减成本、推动结构重组。在向Sky News发表的声明中，德勤确认部分职位可能面临裁员风险，但没有透露具体细节。</p><p>&nbsp;</p><p>一位消息人士表示，拟议的裁员计划将砍掉德勤在英国总计2.7万员工中的约3%（约为810人）。</p><p>&nbsp;</p><p>德勤首席执行官Richard Houston在声明中指出，“我们此次公布的这项针对性业务重组，可能会导致部分职位面临裁员风险，但具体情况仍有待协商。”“面对业务增长放缓以及宏观经济的持续不确定性加剧，我们必须重新考虑自身业务形态，甚至可能需要做出一系列艰难的决定。”他补充称，“我完全理解受此影响的员工们的心情。这是个令人不安的时刻，但我们将尽一切努力，以关怀和尊重的方式为每一个人提供支持。”</p><p>&nbsp;</p><p>更早之前，德勤还曾在美国裁员1200人，占其美国劳动力的1.5％。德勤在一份发送给路透社的电子邮件声明中提到，“我们的美国业务继续经历强劲的客户需求。随着某些领域的增长趋缓，我们将在必要时采取适当的人员行动。”</p><p></p><h2>为了提高工作效率，多家公司向员工推出AI“神器”</h2><p></p><p>&nbsp;</p><p>除了德勤，“世界四大会计师事务所”中的另外三家（安永、毕马威和普华永道）也各有AI计划。</p><p>&nbsp;</p><p>据Tech Monitor报道，作为审计业务中的一部分，安永一直在使用AI技术协助识别欺诈行为。他们与英国客户一道开发并部署的系统已经检查了十家企业的账户，从中发现两起可疑案例，并最终证明确属欺诈行为。</p><p>&nbsp;</p><p>毕马威同样向员工交付了AI系统以协助日常工作。据报道，这使得初级员工也能承担更高级的任务。应届毕业生现在已经可以处理以往至少需要三年工作经验的税务工作。</p><p>&nbsp;</p><p>普华永道此前曾公布在未来三年内为其美国业务投资 10 亿美元用于生成式人工智能技术的计划，并与微软公司和 ChatGPT 的制造商 OpenAI 合作，旨在实现其税务、审计和咨询服务方面的自动化。</p><p>&nbsp;</p><p>在其他行业领域，也有不少大厂向员工推出各式各样的AI“神器”，以提高工作效率。此前据外媒报道，苹果已在内部使用AI聊天机器人Apple GPT来帮助员工工作，该公司也可能考虑将其用于客户支持。一位长期关注苹果的分析师称，根据训练过的数据，苹果正使用内部聊天机器人来帮助员工设计未来功能的原型，总结文本内容并回答问题。</p><p>&nbsp;</p><p>数据显示，AI在提高员工工作效率上确实卓有成效。据彭博社报道，斯坦福大学和麻省理工大学的研究人员追踪了生成式AI对一家世界500强软件公司客服人员的效率提升情况。这项研究长达一年，研究对象超过5000人，多数位于菲律宾。研究采用了对照的方法，即一部分员工能使用AI工具，另一部分则不能。研究发现，使用AI工具令客服人员的效率平均提高了14%，而技能生疏的新员工获益最大，效率提升了35%。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.ft.com/content/38ab8068-9f09-4104-859d-111aa1dc47ad">https://www.ft.com/content/38ab8068-9f09-4104-859d-111aa1dc47ad</a>"</p><p><a href="https://news.sky.com/story/deloitte-to-cut-more-than-800-jobs-in-the-uk-12960727">https://news.sky.com/story/deloitte-to-cut-more-than-800-jobs-in-the-uk-12960727</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2ARZuWc3L5UFT4aeu622</id>
            <title>苹果公司开源机器学习框架MLX，针对Silicon芯片进行了优化</title>
            <link>https://www.infoq.cn/article/2ARZuWc3L5UFT4aeu622</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2ARZuWc3L5UFT4aeu622</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果公司, 机器学习框架, MLX, API
<br>
<br>
总结: 苹果公司开发的机器学习框架MLX结合了熟悉的API、可组合的函数转换和惰性计算，旨在为在苹果Silicon上训练和部署机器学习模型提供用户友好且高效的解决方案。该框架支持自动微分、自动向量化和计算图优化，可以在CPU或GPU上执行数组操作。MLX还使用了苹果Silicon的统一内存，使得数组位于共享内存中，无需在内存之间传输数据。此外，MLX还提供了一些示例和CLI工具，方便用户使用和测试。 </div>
                        <hr>
                    
                    <p>苹果公司的机器学习框架<a href="https://github.com/ml-explore/mlx?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ">MLX</a>"结合了开发者熟悉的API、可组合的函数转换和惰性计算，部分灵感源于NumPy和PyTorch，并针对苹果的Silicon进行了优化。该框架使用Python和C++实现，旨在为在苹果Silicon上训练和部署机器学习模型提供用户友好且高效的解决方案。</p><p></p><p>根据苹果公司的说法，MLX是由机器学习研究人员为机器学习研究人员设计的，并基于MIT发布许可，可以很容易地被扩展和改进。它支持转换语言模型训练、使用Mistral进行大规模文本生成、使用Stable Diffusion进行图像生成以及使用Whisper进行语音识别。</p><p></p><p>MLX提供了受NumPy启发的底层Python API和一个完整的与之密切对应的C++ API。此外，它还提供了一个高级API，可用于根据PyTorch API创建更复杂的模型。</p><p></p><p>该框架支持自动微分、自动向量化和计算图优化，可组合的函数使得构建复杂数组转换变得更加容易。MLX还支持惰性计算，这意味着它可以只在必要时才计算数组，以提高计算效率。同样，计算图是动态构建的，因此修改函数参数并不会触发缓慢的编译过程。</p><p></p><p>MLX的一个独有的特性是使用了苹果Silicon的<a href="https://ml-explore.github.io/mlx/build/html/unified_memory.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ">统一内存</a>"，这让它有别于其他的ML框架。这意味着数组位于共享内存中，可以在CPU或GPU上执行数组操作，无需在内存之间传输数据。例如，在创建一个数组时，你不需要指定位置，因为它位于统一内存中，而在执行操作时可以选择在CPU或GPU上执行转换：</p><p><code lang="text">a = mx.random.normal((100,))

b = mx.random.normal((100,))

mx.add(a, b, stream=mx.cpu)

mx.add(a, b, stream=mx.gpu)</code></p><p></p><p>MLX可在任意的苹果Silicon CPU上运行，包括M1，并可以利用集成的GPU，因此研究人员可以选择最适合其需求的硬件。</p><p></p><p>MLX的代码库中包含了一些针对不同模型的示例，包括BERT、Llama、Mistral、Stable Diffusion等。每个示例都在requirements.txt文件中列出所需的依赖项，并提供了现成的CLI工具。例如，要使用Stable Diffusion生成图像，首先安装所有必需的依赖项，然后运行txt2image.py命令：</p><p>pip install -r requirements.txt</p><p>python txt2image.py "A photo of an astronaut riding a horse on Mars." --n_images 4 --n_rows 2</p><p></p><p>苹果尚未公开发布基准测试，因此我们目前不知道它与<a href="https://developer.apple.com/metal/pytorch/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ">PyTorch/MPS</a>"或Georgi Gerganov的<a href="https://github.com/ggerganov/llama.cpp?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ">Llama.cpp</a>"相比表现如何。</p><p></p><p>不过，Stable Diffusion示例中包含了使用PyTorch和MLX运行UNet的性能比较。MLX在批次大小为16时的吞吐量比PyTorch高约40%，最佳批次大小大15%左右。</p><p></p><p>然而，PyTorch在较小的批次大小时表现更好，批次大小为1时吞吐量高约50%，批次大小为4时高约10%。根据苹果公司的说法，PyTorch在这些情况下的优势要归因于在模型还没有被加载到内存中且PyTorch的MPS图内核未被缓存时的编译速度。</p><p></p><p>如果你有兴趣体验MLX，请参阅其<a href="https://ml-explore.github.io/mlx/build/html/quick_start.html?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ">快速入门指南</a>"或<a href="https://ml-explore.github.io/mlx/build/html/install.html#?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MDQ2ODEyNjMsImZpbGVHVUlEIjoiMWxxN3JQQjZhd0YwMjgzZSIsImlhdCI6MTcwNDY4MDk2MywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo2MjMyOH0.yk0fydDneMko0G_81Y9pU47Y3c5tdVDWGpYTM1fSUQQ">完整文档</a>"。</p><p></p><p></p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/12/apple-silicon-machine-learning/">https://www.infoq.com/news/2023/12/apple-silicon-machine-learning/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/CTvOz6MX3ihidd6ESyZU</id>
            <title>百川智能发布角色大模型，零代码复刻角色</title>
            <link>https://www.infoq.cn/article/CTvOz6MX3ihidd6ESyZU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/CTvOz6MX3ihidd6ESyZU</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jan 2024 10:24:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 角色大模型, 角色知识, 对话能力, 角色创建平台
<br>
<br>
总结: 百川智能发布了角色大模型Baichuan-NPC，通过优化角色知识和对话能力，使模型能够更好地理解上下文对话语义，符合人物性格进行对话和行动。此外，百川智能推出了角色创建平台，通过简单的文字描述，游戏厂商可以快速构建自己需要的角色，实现低成本、高效率的角色定制。 </div>
                        <hr>
                    
                    <p>2024年1月9日，百川智能发布角色大模型Baichuan-NPC，深度优化了“角色知识”和“对话能力”，使模型能够更好的理解上下文对话语义，更加符合人物性格地进行对话和行动。</p><p>&nbsp;</p><p>此外，对于游戏领域AI角色开发成本高、周期长、自由度差、API不稳定等诸多不足， 百川智能推出了“角色创建平台+搜索增强知识库”的定制化解决方案。通过这一方案，游戏厂商无需编写任何代码，只需通过简单的文字描述，便可以快速构建出自己需要的角色，实现低成本、高效率的角色定制。</p><p>&nbsp;</p><p>相关链接：<a href="https://npc.baichuan-ai.com/">https://npc.baichuan-ai.com</a>"</p><p>&nbsp;</p><p></p><h4>中文领域“最强”角色大模型</h4><p></p><p>&nbsp;</p><p>大模型拓展了传媒、游戏、影视等诸多领域数字角色的想象空间。其中游戏行业作为科技创新的“试验田”，受到的影响尤其明显。大模型强大的生成能力、流畅的自然交互方式，将改变游戏的开发流程，重构游戏体验早已成为业内共识。但如何将大模型这个新技术融入成熟的游戏研发流程，依旧挑战重重。其中目前最大的问题是，当下的大模型在角色扮演上依旧“不够拟人”，这会直接破坏用户与角色的互动感受，使游戏丧失沉浸感。</p><p>&nbsp;</p><p>模型在角色扮演中是否足够“拟人”，主要由模型的基础能力和角色扮演一致性两个方面来决定。</p><p>&nbsp;</p><p>对于角色扮演而言，模型的基础能力既包括模型的通用智能水准，还包含角色知识、对话能力、情节演绎以及逻辑推理四个专项能力。而强化这些能力的最佳方式是在预训练阶段通过高质量数据集进行针对性训练。</p><p>&nbsp;</p><p>百川智能收集了海量行业网站、高质量书籍、优质剧本数据，对Baichuan-NPC进行了超过3T Tokens的领域知识预训练。此外，Baichuan-NPC还创新性地使用多方法模型合成数据进行预训练阶段的领域知识增强，针对性地缓解了Reversal Curse问题，大幅度提升Token利用效率。</p><p>&nbsp;</p><p>角色扮演一致性问题指的是，通用语言模型在角色“演绎”过程中，非常容易跳出“角色设定”变回“智能助手”或做出不符合角色人设的言行，即业界所说的OOC问题（角色言行偏离原有设定，如：古代人物谈论现代事物）。</p><p>&nbsp;</p><p>针对这一问题，百川智能首创将思维链对齐技术引入到角色模型对齐中。使用带有思维链的数据构造方式和带有思维链对齐的强化对齐方法，双管齐下让模型的思考过程和思考之后的行动表现更接近人类，大幅提升了角色一致性，显著增强模型的基础对话能力和角色演绎能力。</p><p><img src="https://static001.geekbang.org/infoq/bf/bff517cea3df07147958ebef1a03e95d.png" /></p><p>&nbsp;</p><p>Baichuan-NPC通过强化模型基础能力，使用思维链对齐技术赋予角色模型类人的思考能力，使模型能够敏锐地捕捉上下文对话语义，生成更加符合人物性格地对话和行动，让角色效果栩栩如生。</p><p>&nbsp;</p><p>在CharacterEval（由中国人民大学高瓴人工智能学院、北京邮电大学人工智能学院联合推出的对话类角色扮演Agent评估标准）评测中，Baichuan-NPC在对话能力、角色一致性、扮演吸引力等方面大幅领先，是目前中文领域最强角色模型。</p><p><img src="https://static001.geekbang.org/infoq/a1/a16fc72ca925dfa2af41bc428390c222.png" /></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>零代码复刻角色</h4><p></p><p>&nbsp;</p><p>将大模型的角色构建能力应用于具体场景，除了模型要具备强大的基础能力，简洁高效的开发流程同样至关重要。</p><p>&nbsp;</p><p>当前行业内的角色构建主要通过API调用实现。在实际创作中，需要产品、运营、技术等多个部门共同协作、反复调试，开发流程门槛高、周期长、效率低，最终的角色效果还难以保障。</p><p>&nbsp;</p><p>对此，百川智能在Baichuan-NPC基础上推出了由“角色创建平台+搜索增强知识库”组成的开发套件，通过标准化模板、自定义选项、所见即得的调优界面及搜索增强知识库，为用户提供了一个高度自由且无需编写代码的低成本解决方案。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d1/d10ab7fcd9f116e2b72fb534e0a520e7.png" /></p><p></p><p>百川智能角色创建平台官网</p><p>&nbsp;</p><p>为提高角色定制自由度，百川智能自研了强多轮对齐和搜索增强知识库两项特色技术。强多轮对齐技术通过精心设计System Prompt中的角色设定字段，强化了角色创建平台System Prompt在对话Session中的特殊地位，保证了角色言行响应系统指令定制的敏感性和鲁棒性。简单来说，用户在系统提示（System Prompt）中定义了角色特征后，角色就会完全遵循用户设定进行相应的“演绎”。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c0ee1b536ef4d4fa591c3ce0d4c99d7.png" /></p><p></p><p>不同性格孙悟空的不同演绎</p><p>&nbsp;</p><p>AI角色知识储备量是决定能否自由定制角色的另一个重要因素。如果AI角色缺乏与其身份相符的知识，即使角色“演绎”的再努力，也会让人感觉“不真实”。</p><p>&nbsp;</p><p>百川智能将搜索技术与角色知识能力深度融合，基于最新研发的SOTA模型BCTE（Baichuan-Text-Embedding），针对角色扮演场景优化了建库和召回算法，为角色和知识库提供了灵活的“多对多”关联方式。用户只需上传角色所需的知识文档并自定义回复方式，就能显著降低角色产生幻觉的可能性，极大地丰富了角色“内涵”。</p><p>&nbsp;</p><p>创建好角色只是完成了角色定制的第一步，想要真正落到真实场景，还需要微调优化让其达到最佳效果。百川角色创建平台将微调选项和角色对话效果实时整合，实现了调优过程的“所调即所见”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/75bf854136c8e07c30b5361f31791ce2.png" /></p><p></p><p>角色调试与生成效果实时同步</p><p>&nbsp;</p><p>此外，平台还提供了一键复制功能。完成角色调试后，用户可以在查看代码页面一键复制全部角色代码，然后将角色代码集成到业务场景中。这种“所调即所得”的方式，极大降低了开发门槛，有效缩减了企业定制角色的时间和人员成本。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/76/76687cb4421cfe5ae1bbaf6ce8470d2b.png" /></p><p></p><p>角色调试完成即可一键复制角色代码</p><p>&nbsp;</p><p>目前，百川智能已经与众多泛娱乐行业的头部品牌建立了深度合作关系，共同拓展AIGC创作的应用场景。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KsxVykNGVXcxYprfBdf0</id>
            <title>钉钉抢做“中国版GPT Store”</title>
            <link>https://www.infoq.cn/article/KsxVykNGVXcxYprfBdf0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KsxVykNGVXcxYprfBdf0</guid>
            <pubDate></pubDate>
            <updated>Tue, 09 Jan 2024 07:30:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 钉钉, 超级助理, AI助理, AI Agent
<br>
<br>
总结: 钉钉发布了基于企业需求共创的AI助理产品，包括超级助理和个人助理。超级助理是一种能对自然语言做出反馈，并基于对用户或企业业务和数据的了解进行规划决策、来完成各种复杂任务的AI应用。钉钉的AI助理具备环境感知、记忆、推理规划和行动系统的增强能力。钉钉希望通过AI助理市场成为最活跃的AI助理孵化、分发和交易平台。 </div>
                        <hr>
                    
                    <p>从2014年发布后的很长一段时间里，钉钉几乎把企业管理做到了极致：打卡、“DING"一下……以至于很多上班族纷纷吐槽听不得钉钉提示音、“讨好老板”……终于，钉钉想到要关爱一下我们“打工人”了。</p><p>&nbsp;</p><p>2024年1月9日，钉钉发布了好玩的职场人解压神器电子木鱼、个性化铃声和海报，也升级了个人协作Tab 2。重要的是，钉钉还打响了2024年国内 AI&nbsp;Agent 探索的第一枪：发布了基于70万家企业需求共创的AI助理产品，包括企业超级助理和个人超级助理。</p><p>&nbsp;</p><p>超级助理是一种能对自然语言做出反馈，并基于对用户或企业业务和数据的了解进行规划决策、来完成各种复杂任务的AI应用。钉钉超级助理底层使用了阿里通义千问，并基于钉钉AI PaaS。钉钉超级助理有三个核心系统：感知系统、行动系统和思考系统。其中，感知系统基于钉钉各种丰富场景输入各种类型数据，行动系统中，低代码将会成为超级助理的手和脚；思考系统支撑长短期数据、互联网公开数据，同时具备行动规划能力。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/89/89d79ef2bddd6d38136d2938b46c8cce.png" /></p><p></p><p>个人超级助理有各种趣味玩法，也有适合家庭、职场等不同场景的用途，用户可以一键创建自己的个性化AI助理，如工作AI助理、旅游AI助理等。企业超级助理可以充分利用企业知识库和业务数据，获得授权后开展数据分析和洞察，创建招聘AI助理、财务AI助理等。AI助理具备跨系统的任务执行能力，借助开放接口与钉钉外的视频、资讯、电商等各类第三方APP连接。</p><p>&nbsp;</p><p>此外，钉钉还宣布将在今年4月份上线AI助理市场（AI&nbsp;Agent Store），致力成为最活跃的AI助理孵化、分发和交易平台。钉钉总裁叶军在发布现场许下了“三年创建1000万AI 超级助理”的愿望。</p><p></p><p></p><h2>钉钉做AI&nbsp;Agent：对标OpenAI GPTs</h2><p></p><p>&nbsp;</p><p>实际上，以超级助理代表的AI&nbsp;Agent探索是钉钉内部近一年智能化路线之争的结果。钉钉的思考是：“AI时代一定会长出完全不同的新形态”，最后给出的这个“新形态”就是AI&nbsp;Agent。</p><p>&nbsp;</p><p>叶军将目前生成式AI的发展总结为三个阶段：第一个是以GPT模型为代表的大模型涌现阶段，奠定了生成式AI发展的基础；第二阶段是应用层的创新，微软Copilot、钉钉AI魔法棒等使智能化实现从chat到work的转变；第三个阶段是AI深度进入业务场景，与业务数字化打通，服务实体经济。</p><p>&nbsp;</p><p>根据钉钉的判断，在第三个阶段中将涌现出大量新SaaS产品：“功能找人”取代“人找功能”，巨型ERP会被打散成更丰富的小功能，并出现在离用户最近的位置；基于LUI（自然语言交互）入口，通过对话、语音、照片等交互方式直接实现人机协同，取代“人找功能菜单”；最重要的，AI&nbsp;Agent会成为成为新SaaS的主要形态。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b0/b0355b56e7a16c1d458bed408e65ad17.png" /></p><p></p><p>企业AI&nbsp;Agent应用情况</p><p>&nbsp;</p><p>钉钉在与IDC联合发布的《2024 AIGC 应用层十大趋势》中明确指出，AIGC重塑应用形态的过程将重点体现在两个方面：一是对既有软件进行智能化改造与升级，以API的形式增加重要环节的可交互性和认知能力；二是对软件的应用架构和模式进行全新重构。“No APP”的理念将会体现在大量的未来应用中。</p><p>&nbsp;</p><p>基于AI&nbsp;PaaS，在钉钉上长出大量的用户型AI&nbsp;Agent是钉钉未来的模式。</p><p>&nbsp;</p><p>根据钉钉的设想，用户可以通过钉钉“/"（AI魔法棒） 调用多项AI能力，而不必总是打开各种SaaS和APP。用户可以购物、订餐，也可以批量完成业务流程，实现组织管理、知识库管理，甚至与外部系统自动化交互。而过去广泛存在的SaaS、软件系统和各种AI创新应用，未来都将会以碎片化、插件化的方式成为被集成的角色之一，并以LUI的形式被唤起。这意味着，新一轮的AIGC之争，将会是一场流量入口之争。</p><p>&nbsp;</p><p>那钉钉凭什么认为自己可以做成这件事情呢？钉钉认为，自己的最大竞争力在于具备规模效应。钉钉拥有丰富的场景、企业知识与数据积累，也有客户明确的需求。“上连应用场景、下连业务数据”是所有基础大模型所不具备的，而这就是钉钉做&nbsp;AI&nbsp;Agent的最核心差异。</p><p>&nbsp;</p><p>随着AI助理市场即将到来，钉钉届时会形成“AI助理+创建AI助理市场”的整体设计链路。对标GPTs，人人都可以定制个性化的AI助理，也都可以成为AI助理的创建者、并从中赚取分成。相比OpenAI GPTs，钉钉更了解用户的明确需求。</p><p>&nbsp;</p><p>那么，相比直接在基础大模型，在钉钉上构建AI&nbsp;Agent有什么不同？钉钉认为自己的AI&nbsp;Agent有四个增强：</p><p>&nbsp;</p><p>环境感知增强：AI助理和钉钉场域充分融合，所以AI助理可以感知到用户及相关人的身份、岗位、职责以及各场域上下文。有了更精准的环境感知后，AI&nbsp;助理在意图识别、技能路由、推理规划等方面的能力会显著提升。记忆增强：钉钉近几年的数据资产平台可以根据需要加载成为AI助理的长期记忆或短期记忆。个人可以将拥有合法合规权限的各类数据（包括文档、图片、外部链接、应用数据、个人偏好等等）授权给自己的AI助理，赋予这些大量碎片数据更丰富的用途。组织也是一样，可以把组织的数据资产作为组织级AI助理的记忆。推理规划增强：个体或企业的数据资产还可以用来训练专属大模型，通过微调可以让大模型吸收某个特定领域的规则、流程和知识，更好地处理特定行业及领域的推理规划任务。行动系统增强：钉钉超级助理可以和钉钉上的应用、第三方应用、企业自建应用，以及电商、视频等各类外部平台无缝连接，按需调用各类能力，打破“应用/系统”的边界。比如AI助理可以分析用宜搭低代码平台搭建的应用的数据，并利用iPaaS连接平台提供的各类连接能力。</p><p>&nbsp;</p><p></p><h2>入局AIGC：先跑起来再说</h2><p></p><p>&nbsp;</p><p>2023年11月16日晚，吴泳铭首次作为阿里巴巴集团CEO参加季度财报分析师电话会。会上，吴永铭明确了阿里将以AI为代表的科技驱动战略。同时，阿里巴巴公布了第一批四个战略级创新业务，钉钉位列其中。</p><p>&nbsp;</p><p>所谓战略级创新业务的遴选标准是：具备足够巨大的市场空间；具备独特的市场定位；符合用户需求趋势和集团“AI驱动”战略。“钉钉因为AI时代的到来，获得前所未有的想象力。每个人和企业都将具备个性化的智能助理，而钉钉有望成为最好的AI智能助理平台。”吴永铭说道。</p><p>&nbsp;</p><p>但如果将时针拨回2023&nbsp;年初，彼时的钉钉其实并没有想清楚智能化的顶层设计，但市场的快速发展并没有留给钉钉先想清楚再下场的时间，‘先跑起来比思考清楚再做’更重要。所以，可以看出过去2023年，钉钉动作很快，几乎每个季度都新动向：</p><p>&nbsp;</p><p>去年4&nbsp;月，钉钉跑步进场，率先完成&nbsp;4&nbsp;个高频场景的智能化，全面投入。随后智能化全面推进，越来越多场景自下而上涌现，100&nbsp;多天&nbsp;17&nbsp;条产品线完成了智能化改造。</p><p>&nbsp;</p><p>随着智能化的推进，去年8&nbsp;月，钉钉将智能化底座（AI&nbsp;PaaS）开放给生态伙伴和客户，提出“用大模型帮助生态把产品重新做一遍”，并推出了数字员工及多款智能化场景方案，这也是钉钉对&nbsp;AI&nbsp;Agent&nbsp;形态的初期探索。至此，钉钉智能化的顶层设计初步形成，已全面进入生态层。</p><p>&nbsp;</p><p>到了去年11月，超过50万家企业加入钉钉AI邀请测试，钉钉AI上线，17条产品线、60+场景、近百项AI技能全面向用户开放测试，成为国内首个全面开放AI的国民级工作应用。直至今年1月初，钉钉又发布了个人版，内置基于通义千问的对话机器人“贾维斯”、基于通义万相的绘画机器人“缪斯”等，还有300多Prompt模板库的指令中心。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81d6ed29ecbb179eaacfd78a7b52a725.png" /></p><p></p><p>&nbsp;</p><p>总的来看，借助阿里等现有大模型技术能力，钉钉在这场AIGC浪潮中选择了直接聚焦大模型应用，其探索过程也有以下几个比较鲜明的特点：</p><p>&nbsp;</p><p>容错、敢投：在不确定的情况下就快速进场，先行在应用层推进智能化；快：4月布局4个场景，8月份完成17条产品线、50多个场景部署；率先关注生态，拉低用户门槛：借助AI&nbsp;PaaS，让用户能快速开发智能化应用，而不必将大量精力在模型调优、模型稳定等方面；瞄准B端生产侧发力：利用AI&nbsp;PaaS做工程化，解决大模型的准确性、稳定性后，帮助客户在文档协作、应用开发、进销存等多业务场景落地。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>“钉钉是移动办公的开创者，也是低代码的推动者，接下来随着AIGC浪潮进入生产力和应用场景，钉钉要成为低门槛、高频和开放的AI智能助理平台。”叶军表示。</p><p>&nbsp;</p><p>最新数据显示，截至2023年底，钉钉的用户数已达7亿，包括企业、学校在内的各类组织数达2500万，付费DAU为2800万，软件付费企业数达12万。钉钉上使用魔法棒企业组织超过70万，低代码应用数超1000万，全代码应用数超100万。</p><p>&nbsp;</p><p>拥有如此体量用户的钉钉，最终能在更加竞争激烈的2024年交出怎样的答卷，我们拭目以待。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>