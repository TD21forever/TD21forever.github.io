<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/4Qg8YxbHsV9brLDa265e</id>
            <title>Ilya新公司融资10亿，员工仅 10 人：AGI成功前，不发布任何产品</title>
            <link>https://www.infoq.cn/article/4Qg8YxbHsV9brLDa265e</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4Qg8YxbHsV9brLDa265e</guid>
            <pubDate></pubDate>
            <updated>Thu, 05 Sep 2024 12:56:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI 联合创始人 Ilya Sutskever 的新公司筹集了 10 亿美元，并宣布计划组建一支由“世界上最优秀的工程师和研究人员”组成的团队，专注于一项单一任务。</p><p>&nbsp;</p><p>他的公司对这个目标如此专注，以至于它的名字以它将要开发的唯一产品命名。为了实现创造一个有益于人类而不是毁灭人类的 AGI 的雄心，Safe Superintelligence Inc (SSI) 将创造一个创新的工作环境，能够解决“我们​​时代的最重要技术问题”，并且“不需要关心其他任何事情”。</p><p>&nbsp;</p><p>该公司写道：“我们的单一重点意味着没有管理开销或产品周期的干扰，我们的商业模式意味着安全性、保障性和进展（&nbsp;safety, security, and progress）都与短期商业压力隔绝。”</p><p>&nbsp;</p><p>“SSI 是我们的使命、我们的名字和我们的整个产品路线图，因为它是我们的唯一重点。我们的团队、投资者和商业模式都与实现 SSI 统一。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94ec5b59be210b29b929dee00efb4153.jpeg" /></p><p></p><p>&nbsp;</p><p>该公司将同时解决模型的安全性和能力问题，“作为通过革命性的工程和科学突破来解决的技术问题”。他们补充说：“我们计划尽可能快地提升能力，同时确保我们的安全始终领先。”</p><p>&nbsp;</p><p>它将建立一个“直线 SSI 实验室”，只有一个目标和一个产品：一个安全的超级智能。其融资涉及 NFDG、a16z、Sequoia、DST Global 和 SV Angel。</p><p>&nbsp;</p><p>SSI 目前拥有 10 名员工，计划将资金用于获取计算能力和招聘顶级人才。虽然公司没有公布具体估值，但知情人士透露，SSI 的估值已达 50 亿美元。</p><p>&nbsp;</p><p>此次融资凸显出，部分投资者仍愿意对专注于基础人工智能研究的顶尖人才进行大额投资。尽管如此，由于此类公司可能长期处于亏损状态，市场对这类公司的投资热情已有所降温，导致多位初创公司创始人投奔科技巨头，比如Character.AI 的创始人、CEO，Transformer 论文作者之一Noam Shazeer，也于上个月与其研究团队一起回流到了谷歌。</p><p>&nbsp;</p><p></p><h2>SSI 的历史</h2><p></p><p>&nbsp;</p><p>37 岁的 Sutskever 在 6 月份与两位名为 Daniel 的联合创始人一起创办了 SSI。</p><p>&nbsp;</p><p>第一个 Daniel Gross 曾参与苹果公司的 AI 项目，第二个是前 OpenAI 研究员 Daniel Levy，也是前 OpenAI 研究员。Sutskever 担任首席科学家，Levy 担任首席科学家，而 Gross 负责筹款和增加计算能力。</p><p>&nbsp;</p><p>去年11月，他作为OpenAI非营利母公司董事会成员，该董事会因“沟通中断”投票罢免了OpenAI首席执行官Sam Altman。几天后，Sutskever与OpenAI几乎所有员工一起签署了一封信，要求Altman回归并要求董事会辞职。</p><p>&nbsp;</p><p>Sutskever 在 CEO Sam Altman 恢复职位后决定离开。今年 5 月，他在任职十年后正式离开了公司，他在 X 帖子中将 OpenAI 的轨迹描述为“简直是奇迹”，并表示“相信 OpenAI 将构建安全且有益的 AGI”。</p><p>&nbsp;</p><p>然而，OpenAI不可能利用 Sutskever 的“超级对齐”团队的资源来实现这一点，该团队旨在“引导和控制比我们聪明得多的 AI 系统”，因为现在这个团队已经不存在了。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/6117f2fdd086898c19a626a77f123960.jpeg" /></p><p></p><p>&nbsp;</p><p>Sutskever告诉路透社，这个新项目开始于他“发现了一座与我之前工作有所不同的山峰”。在描述他想与之共事的人时，他说：“有些人能工作很长时间，只是沿着相同的道路走得更快。这不是我们的风格。但如果你做一些不同的事情，那么你就有可能创造出一些特别的东西。”</p><p>&nbsp;</p><p></p><h2>SSI 与 OpenAI 有何不同？</h2><p></p><p>&nbsp;</p><p>SSI 将作为一个普通的营利性企业运行，与 OpenAI 不寻常的“限额利润”结构不同，OpenAI 是原始非营利组织和一个新的限额利润部门之间的合作关系。</p><p>&nbsp;</p><p>在 OpenAI，分配给投资者和员工（包括微软）的所有利润都有上限，因此超过该上限的所有剩余价值将“返还给非营利组织以造福人类”。</p><p>&nbsp;</p><p>在简报笔记中，德意志银行研究部表示，SSI 是 OpenAI 的“对立面”。</p><p>&nbsp;</p><p>它写道：“通过不计划现在发布商业产品，Safe Superintelligence 进入了与通常的‘构建产品——销售产品’类型的初创公司完全不同的类别。”“这可能会与资金相冲突，资金是 AI 创新因为训练成本高昂和人才稀缺而不可或缺的。例如，OpenAI 起初是一家研究公司。”</p><p>&nbsp;</p><p>该银行补充道：“在 AI 领域，关于潜在风险和呼吁放慢进度的人与想要全速前进的人之间存在激烈的争论。凭借自己的平台，Sutskever 可能会获得更大的声音，并成为他前任老板、OpenAI CEO Sam Altman 的某种对立面。”</p><p>&nbsp;</p><p>特别值得注意的是，与OpenAI不同，SSI 明确表示，在超级智能实现之前，他们不会发布任何产品。</p><p>&nbsp;</p><p>“我们的使命是直接走向安全的超级智能，特别是花几年时间对我们的产品进行研发，然后再推向市场。”SSI 首席执行官 Daniel Gross说。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f4b5edf9ed89e080dbae6d5b808bb10.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>“融资 10 亿美元，估值 50 亿美元。对于一家成立几个月、没有产品、甚至没有一行代码投入生产的公司来说，这太疯狂了。”</blockquote><p></p><p>&nbsp;</p><p>而且很多人并不相信AGI能在几年里实现。但一旦AGI真的能达成，这笔投资就真的很划算了。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c9bfec86c7dc5f0cee8a2dde99647182.jpeg" /></p><p></p><p>&nbsp;</p><p></p><blockquote>这里有很多轻蔑的评论。Ilya 在过去十年中通过在 OpenAI 的出色表现，证明了自己作为领导者、科学家和工程师的能力，连续实现了其他人无法企及的突破。他已经筹集了足够的资金，能够与 Grok、Claude 等顶级企业竞争。他为投资者提供了一个纯粹的 AGI（通用人工智能）投资机会，可能是少数能够做到这一点的组织之一。你还会把 10 亿美元交给谁来追求这一目标呢？这就是投资者的思维方式：宏观趋势、雄心勃勃的可能性，以及少数真正有可能实现这些目标的人。&nbsp;50 亿美元的估值并不夸张，这是正常的估值扩展，没有类似零利率时期的疯狂融资。如果你没有亲身见过这种规模的投资，很难理解资本分配只是后面多了几个零，而有些人专门负责做那些带有九位数的决策。&nbsp;是的，这一切的前提是，他的公司在未来 10 年内的估值将超过 5000 亿美元。如果他们能打造 AGI，这个估值非常划算。想想 Siri、Alexa、ChatGPT 有多普遍，而它们的表现有多差劲、不实用或者出错。这里并不存在显著的需求或分销风险。构建使用更智能 AI 的基础设施已经成为全球科技界的痴迷。如果 AGI 成功了，无论在哪个层面，它都会拥有一大批重要客户。</blockquote><p></p><p>&nbsp;</p><p>Sutskever 是Scaling law的倡导者，该立场认为 AI 模型的性能可以通过使用更多数据、资源和神经网络架构来提高。SSI 将以不同于 OpenAI 的方式进行“扩展”，但目前尚不清楚如何进行。</p><p>&nbsp;</p><p>SSI 目前正积极招募与公司文化相契合的人才。“我们提供了一个机会来做你一生的工作，并帮助解决我们这个时代最重要的技术挑战，”SSI 写道。</p><p>&nbsp;</p><p>而Gross则表示，他们会花费大量时间甄选候选人的“品格”，并寻求具有卓越能力的人才，而非过分看重学历和行业经验。</p><p>&nbsp;</p><p>“我们更看重的是对工作充满热情，而非追逐行业炒作的人。”他补充道。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://news.ycombinator.com/item?id=41445413">https://news.ycombinator.com/item?id=41445413</a>"</p><p><a href="https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/">https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/</a>"</p><p><a href="https://www.thestack.technology/openai-ssi-ilya-sutskever/">https://www.thestack.technology/openai-ssi-ilya-sutskever/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TwHb0T8fqBUBMIe2Jrzi</id>
            <title>拒绝“零和”游戏！腾讯新一代混元Turbo降价 50%，RAG方案支持快速定制AI应用</title>
            <link>https://www.infoq.cn/article/TwHb0T8fqBUBMIe2Jrzi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TwHb0T8fqBUBMIe2Jrzi</guid>
            <pubDate></pubDate>
            <updated>Thu, 05 Sep 2024 10:11:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>“增长是企业当下最重要的事。以数提效、顺势而为、扬帆出海是企业破局增长的三个方向。”</p><p></p><p>9月5日，在2024腾讯全球数字生态大会上，腾讯集团高级执行副总裁、云与智慧产业事业群CEO汤道生表示，当下很多企业面临内外部多重挑战，甚至陷入“内卷式竞争”，但如果蛋糕不增长，结果就是“零和”游戏。企业增长的核心在于跳出框架，探索产业的新发展。</p><p></p><p>汤道生表示，第一个增长方法是以数提效，聚焦核心场景。“指望大模型给一般企业在短期内带来巨大变化并不现实。用人工智能在已有业务场景中降本增效，提高经营效率，是行稳致远的可靠路径。”</p><p></p><p>第二个增长思路是抓住新产业机会。汤道生特别强调，“如果芯片或硬件设备是计算的躯体，那软件就是脑袋。软件往往定义了硬件发展的方向，让硬件设备释放出更大价值。”</p><p></p><p>第三个增长机会是开拓全球市场。相关机构调研显示，90%的中国企业对拓展海外业务有兴趣，有近四分之一的企业，将出海列为未来1到3年的首要任务之一。腾讯云可以提供包括音视频、边缘加速、云原生产品、大数据与数据库、安全与合规、AI等技术产品能力。</p><p></p><p></p><h1>新一代混元Turbo定价低 50%，推理效率提升一倍</h1><p></p><p>大会上，腾讯宣布推出“混元Turbo”，相比前代模型性能显著提升，训练效率提升108%，推理效率提升 100%，推理成本降低 50%，效果在多个基准测试上对标GPT-4o。</p><p></p><p>作为新一代旗舰大模型，腾讯混元Turbo在语言理解、文本创作、数学和代码等领域都有较大提升，跟前代模型相比，复杂数学解决能力提升38%，代码能力提升32%。</p><p></p><p>目前，混元Turbo已经在腾讯云上线，输入和输出价格只有前代模型的一半。企业和开发者可以通过API、专属模型、精调模型等方式使用混元大模型相关能力。</p><p></p><p>汤道生表示，随着大模型与生成式AI的技术突破，图片、视频、语言的理解与生成已经有很大进步，人与人的沟通、人与系统的交互方式，都可能会被重塑。最近半年，产业界对AI大模型的关注重点，开始从模型技术本身，转到智能应用落地上。用人工智能在已有业务场景中降本增效，提高经营效率，是行稳致远的可靠路径。</p><p></p><p>据介绍，腾讯已经构建起了全链路的大模型产品矩阵，包括底层基础设施、帮助企业训练专属模型的TI平台和行业大模型解决方案、自研的混元大模型、构建应用的平台工具，以及基于大模型的各类智能应用。</p><p></p><p>此外，腾讯元宝品牌智能体专区正式上线，首批邀请 11 家合作伙伴入驻，打造精品AI智能体应用生态，涵盖工作提效和生活娱乐多个场景，用户可以直接在“腾讯元宝”APP上体验丰富的品牌智能体应用。“面向场景创造价值才是大模型发展的意义。”腾讯云副总裁、腾讯云智能负责人、优图实验室负责人吴运声表示。</p><p></p><p>腾讯云还宣布推出AI infra品牌“腾讯云智算”，整合了腾讯云高性能计算HCC、高性能网络IHN星脉、高性能云存储、加速框架、容器、向量数据库、智算套件等腾讯云优势产品，能够为AI输出性能领先、多芯兼容、灵活部署的智算产品能力。</p><p></p><p></p><h1>推出RAG解决方案，支持“量身定制”AI大模型应用</h1><p></p><p>大会上，腾讯集团副总裁、云与智慧产业事业群COO兼腾讯云总裁邱跃鹏在会上发布腾讯云RAG解决方案，帮助企业结合自身信息知识、快速打造大模型应用。</p><p></p><p>据介绍，腾讯云RAG解决方案包含兼容Elastic开源生态的腾讯云智能搜索（腾讯云Elasticsearch Service ）、以及腾讯云自研的向量数据库，便于企业根据自身架构、数据类型、技术生态灵活选择。</p><p></p><p>其中，腾讯云智能搜索提供从模型管理、向量生成、向量存储、混合搜索、结果重排到LLM大模型集成的一站式RAG能力，企业仅用Elastic一套技术栈就能快速构建AI应用；针对数据规模大、数据类型多样的企业，腾讯云向量数据库提供2倍于业界平均水平的吞吐能力，毫秒级的响应延迟，以及千亿级的单表存储规模，构建企业的AI数据中台，打造RAG应用基础设施。</p><p></p><p>例如，腾讯云智能搜索成功支持了微信读书“AI问书”功能上线，为亿级用户提供毫秒级别的检索服务，同时相比传统单点解决方案降低了90%的机器成本。借助腾讯云向量数据库，学而思实现了在亿级的资料库中毫秒级完成检索，同时检索的准确率达到了95%，大大提升了学而思学习机的语音检索体验。</p><p></p><p>“大模型和云是密不可分的，大模型在云上训练，同时大模型能力通过云向外输出；另一方面，云产品也通过与大模型的深度融合，显著的增强能力，通过这样不断地迭代，让客户在云上获得更全面的业务增长。”</p><p></p><p>邱跃鹏表示，云和大模型越来越深度捆绑到一起，腾讯云也持续打磨云上大模型取用体验。例如，基于混元大模型打造的腾讯云AI代码助手目前已经覆盖腾讯集团内部超过50%的程序员，编码时间平均缩短了40%。腾讯会议的智能录制、AI小助手、多语言翻译等AI能力，每月助力超过1500万用户提升协作效率，也在和更多场景深度结合。</p><p></p><p></p><h1>腾讯云三大数字化产品体系</h1><p></p><p>数字化正在成为创新增长的核心引擎，AI大模型正在产业中加速落地，自主创新软件成为场景提效的核心，而云端也成为企业国际化的重要载体。面向智能化、国际化、融合创新三大领域，大会上，腾讯云联合30家合作伙伴，共同发布了一系列针对具体行业应用场景的数字化解决方案。</p><p></p><p>腾讯云在大会上公布了完整的融合创新产品体系，助力产业转型升级。在基础软件领域，数据库TDSQL、操作系统TencentOS、专有云TCE、大数据TBDS、人工智能开发平台TI等核心产品（简称“5T”），兼容国内主流的芯片厂商，在多个大规模的集群上稳定运行，落地覆盖金融、交通、政务、互联网、医疗、传媒等多个行业。</p><p></p><p>基础软件之外，腾讯的系列自研应用软件如腾讯会议、企业微信、腾讯文档、腾讯乐享等，凭借腾讯产品积累超过20年的用户服务经验，具有便捷的交互体验与服务界面、独特的C2B连接能力。腾讯会议等产品在多个行业头部客户渗透率超过了50%。</p><p></p><p>本次大会上，腾讯云首次披露了企业出海数字化解决方案全景图，包括云原生、音视频、边缘加速、大数据与数据库、安全与合规等技术和产品能力，覆盖游戏、社交互娱、跨境电商、消费电子、金融科技、在线教育、汽车出行等七大行业的全球化实践。</p><p></p><p>其中，腾讯云边缘安全加速平台EdgeOne将服务器的端到端 HTTP/HTTPS 访问时延降低了30%，中国前十大出海音视频社交应用里有八家在用EdgeOne。腾讯会议的海外会议数较年初增长将近50%，面对全球各地差异化的网络、多语言环境，腾讯会议自研了AI降噪算法、Penguins AI语音引擎，上线多语言实时翻译等功能，助力用户流畅无障碍开会。</p><p></p><p>目前，腾讯产业互联网的生态伙伴数量突破11000家，共同服务的客户总数超过200万家。三年间，年收入达到百万级的伙伴数量增长了150%；腾讯会议等SaaS伙伴的收入过去两年也翻了4.5倍。</p><p></p><p>在企业出海方面，腾讯云泛互行业出海业务的增速超过70%，智能制造、新能源汽车等产业出海也快速增长。腾讯集团副总裁、腾讯智慧出行总裁钟翔平表示，作为汽车智能化产业链的重要一环，腾讯已经与超过100家车企和出行科技公司合作，九成的车企选择了腾讯云。</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YjDEkxaFvEpOcZbTXmCt</id>
            <title>国产 GPU 公司被曝“流氓式解散”！员工欠薪记账、明星创始人成“老赖”，总部已人去楼空</title>
            <link>https://www.infoq.cn/article/YjDEkxaFvEpOcZbTXmCt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YjDEkxaFvEpOcZbTXmCt</guid>
            <pubDate></pubDate>
            <updated>Thu, 05 Sep 2024 09:30:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫</p><p></p><p>近日，一封国产 GPU 公司向员工发出的内部邮件引发热议。据悉，象帝先突然决定开始大规模裁员，补偿标准为 N+1。有象帝先员工对外透露，该公司今年以经营困难为由还拖欠着许多员工的薪水和奖金未发，其中包括 7 到 8 月的全部工资、 4 到 5 月的部分工资，以及承诺补发 2023 年一半的年终奖。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a5/a5a4393b9097297f66d9a1df9b111509.jpeg" /></p><p></p><p>而此前又有消息称，象帝先于 8 月 30 日召开全员会议，宣布公司因遭遇资金危机暂停运营，400 多位员工全部终止合同，高管们会继续融资，有钱了再给员工结算，股权资金全部退还员工，但没有透露具体执行方案。</p><p></p><p>9 月 1 日，国产 GPU 公司象帝先通过官方公众号发布公告，针对此前的“全员解散”等传言进行澄清说明。该公司表示，其由于国产 GPU 的发展未达到预期而面临一定“市场调整压力”，但目前并未采取解散或清算的措施；正在进行组织结构和人员配置优化，包括对部分团队成员的调整；正在寻找外部融资机会，对国产 GPU 的未来发展充满信心。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/93/93a9954b023bf199593d6257cdb3b863.png" /></p><p></p><p>由于该公告中未提及具体的员工优化比例、结薪赔偿时间及明确的公司调整措施，因此有不少网友都仍对象帝先的未来发展保有质疑，评其为“流氓式解散”。还有知情人士在国内某职场社交平台进一步透露象帝先现状，员工全都走了，校招生全部毁约。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cd/cd0f01c1bec885c0885c3780721080d6.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/50/50dbcbf5e2d682078dfeeeec5a5059be.jpeg" /></p><p></p><p>据悉，位于重庆的象帝先总部大楼现已上锁，多位物业人员都表示，近期没有看到该公司的人来了。</p><p></p><p></p><h1>“吸金”的创始人，变成了失信人</h1><p></p><p></p><p>公开信息显示，象帝先成立于 2020 年 9 月，是一家高性能通用 / 专用处理器芯片设计企业，已在北京、上海、重庆、成都、苏州等地设立了研发中心。公司研发适用于桌面、工作站、边缘计算等领域的高性能、低功耗、具有完全自主知识产权的通用 CPU/GPU 及相关专用芯片产品。</p><p></p><p>在 2022 年和 2023 年，象帝先分别发布了天钧一号 GPU 和天钧二号 GPU，今年 8 月 20 日象帝先还宣布和超云数字全面战略合作，在数据中心建设、云桌面、云渲染、数字化办公运营等方面深入合作。</p><p></p><p>成立以来至今，四年的时间里，象帝先先后进行了五轮融资，获得融资金额共计 25 亿元，在 2022 年估值高达 150 亿元。而象帝先如此得投资人青睐的一个重要原因便是其创始人兼董事长、CEO 唐志敏在业内的盛名。</p><p></p><p>据了解，唐志敏是国内计算机系统与处理器芯片设计领域的战略级科学家，曾担任龙芯项目负责人、海光信息总经理兼首席科学家，先后领导龙芯一号、二号 CPU、海光系列 CPU、海光 DCU 等国内高端通用芯片项目取得成功。过去曾在中科院技术研究所担任主任研究员、博士生导师，还担任过先进微处理器技术国家工程实验室主任、“十三五”国家重点研发计划“高性能计算”重点专项总体专家。</p><p></p><p>而如今，一位知情人士在国内某职场社交平台透露，唐志敏目前已被中信银行列为失信人，消息来源于银行经理。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/22/22b40223f19e2fa3b5ac51787a16264c.jpeg" /></p><p></p><p></p><h1>资金危机的导火索：一份对赌协议</h1><p></p><p></p><p>2023 年 12 月后，象帝先便再无新的投资消息。而象帝先之所以到了今日危境，似乎也与其融资情况有密不可分的关系。</p><p></p><p>据了解，在 8 月 30 日的全员会议上，唐志敏声称，象帝先目前遭到股东起诉，资金账户已被冻结，当前及未来也无资金进来。被起诉的具体原因是该公司曾经与股东签署过一份对赌协议，承诺 B 轮融资规模要达到 5 亿元，但最终未达成协议条款，对方现在发起了诉讼。</p><p></p><p>在过去的五轮融资中，象帝先的投资方近 20 家，包括两江资本、中信集团、乾瞻投资、点豹基金、雅瑞资本、方正和生、朗空韩亚、水木春锦基金、盛世投资、千山资本、扬子江基金等。其中，B 轮融资披露的一共有水木春锦基金等 6 家机构。但目前，象帝先并未公开起诉的股东。</p><p></p><p>不过，据首都在线日前公布的 2024 年半年度报告，象帝先有一笔涉案金额为 1881.17 万元的诉讼 (仲裁) 尚未结案，北京仲裁委已于 2024 年 7 月 15 日出具受理通知书，待排期开庭中。重庆市渝北区人民法院已于 2024 年 8 月 1 日冻结象帝先 804.04 万元财产款项。</p><p></p><p>知情人士表示，公司资金问题由来已久，无法自行造血的前提下又因外部环境难以融资续命，走到最后一步实际是意料之中。有消息显示，5 月中旬开始，象帝先就有员工被裁，部分员工 2023 年年终奖从 12 月底拖到今年 4 月也未发放，随后又被告知部分员工 5 月工资只发一部分。</p><p></p><p></p><h1>结&nbsp; &nbsp;语</h1><p></p><p></p><p>象帝先的困境并不是中国半导体行业的个例。近几年国产 GPU 公司百花齐放，算力也在逐步提升，但实则仍面临困境。</p><p></p><p>在盈利方面上，不止象帝先存在问题，国内其他能真正盈利的 GPU 公司也很少。</p><p></p><p>此前被曝烧光 3 亿融资的砺算科技，今年已处于破产边缘，多名研发员工在社交平台上称遭遇裁员欠薪，涉及近百名员工。8 月，东芯股份发布公告称，拟通过自有资金人民币 2 亿元向砺算科技”）增资，另其他投资人拟以合计 12,800 万元向其增资。但据某职场社交平台上的砺算科技员工爆料，目前砺算科技尚未拿到融资，且已经拖欠了员工 6 个月的工资。</p><p></p><p>日前，寒武纪发布了 2023 年上半年财报，营收同比下滑，净亏损虽然收窄，但依然亏损严重。财报还显示，上半年研发人员减少了 225 人。综合寒武纪招股书及近三年上市数据来看，寒武纪一直是处于亏损当中，自 2017 年至今年上半年，累计亏损超过 46.6 亿元。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OWQFBhjChhJWYcu0dQ37</id>
            <title>支付宝宣布推出独立AI原生App“支小宝”</title>
            <link>https://www.infoq.cn/article/OWQFBhjChhJWYcu0dQ37</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OWQFBhjChhJWYcu0dQ37</guid>
            <pubDate></pubDate>
            <updated>Thu, 05 Sep 2024 07:23:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月5日，支付宝在2024外滩大会上宣布发布AI生活管家App“支小宝”，目前苹果及安卓应用商店均可下载。</p><p>&nbsp;</p><p>据官方介绍，基于蚂蚁百灵大模型推出的“支小宝”，是国内首个服务型的AI独立App——连接支付宝生态，“支小宝”可通过对话快速订票、点餐、打车、查询附近吃喝玩乐等，说句话就能办事；“支小宝”还拥有场景感知系统，能根据用户的生活习惯和使用场景，智能推荐专属的服务，做到“越用越懂用户”。</p><p>&nbsp;</p><p>区别于传统的图形用户界面，“支小宝”采用极简对话式交互，用户下达口语指令后，不用再寻找或输入，就能快速找到各种服务。同时，它也内嵌在支付宝中，在支付宝App首页下拉也能体验。</p><p>&nbsp;</p><p>与此同时，支付宝面向行业正式启动智能体生态开放计划，并推出了智能体开发平台“百宝箱”，依托智能体构建能力，商家机构可通过“百宝箱”0代码、最快1分钟创建专属智能体，并一键发布到支付宝小程序、支付宝App、支小宝App等。</p><p></p><p>目前支付宝“百宝箱”，分为基础版与专业版。基础版面向普通用户开放，可快速搭建并体验智能体；专业版则面向专业伙伴开放，支持与生态伙伴的深度定制。</p><p></p><p></p><blockquote>专业版了解网址：<a href="https://tbox.alipay.com/pro-about">https://tbox.alipay.com/pro-about</a>"可用支付宝扫码登录体验基础版：https://tbox.alipay.com/community</blockquote><p></p><p>&nbsp;</p><p>“大模型正从‘拼参数’走向‘拼应用’。”蚂蚁集团总裁韩歆毅在外滩大会上表示。</p><p>&nbsp;</p><p>支付宝AI新产品接连落地的背后，是蚂蚁集团AI First战略的全面提速。2023年，蚂蚁自研的百灵大模型完成备案。过去两年，蚂蚁以支付宝为核心加速AI应用布局，将百灵大模型能力运用在出行、政务、医疗等超500个场景，日均调用量超2亿，日均处理千亿级Tokens。</p><p>&nbsp;</p><p>据悉，除了“支小宝”，本次大会上，蚂蚁还将发布AI金融管家“蚂小财”和AI健康管家等AI产品。</p><p>&nbsp;</p><p>InfoQ将在这篇报道下持续关注和更新后续动态。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/pAk7bZJOQeQg2Ss3rOWJ</id>
            <title>人工智能缺乏“激励机制”，如何重新定位和思考 AI 的发展？</title>
            <link>https://www.infoq.cn/article/pAk7bZJOQeQg2Ss3rOWJ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/pAk7bZJOQeQg2Ss3rOWJ</guid>
            <pubDate></pubDate>
            <updated>Thu, 05 Sep 2024 05:47:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月5日上午，2024 Inclusion·外滩大会在上海黄浦世博园区开幕。会上众多专家分享了关于<a href="https://s.geekbang.org/search/c=0/k=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/t=?referrer=InfoQ">人工智能</a>"的最新见解。</p><p>&nbsp;</p><p>其中，著名未来学家、《连线》杂志创始主编凯文·凯利在开幕主论坛上的演讲中指出，当人工智能深刻影响经济和文化，必将涌现三大趋势：全球主义、创新加速和AI驱动生成。</p><p>&nbsp;</p><p>具体而言，全球主义正在迅速推进，因为我们正在共同构建一个基于技术的“超级有机体”。“我们正将全球的手机、笔记本电脑和所有的数据服务器连接成一个巨大的计算系统。每一台设备就像这个庞大计算机的一个神经元。这台超级计算机在一个前所未有的规模上高速运行。”</p><p>&nbsp;</p><p>同时，AI技术加速了创新的步伐，这种加速体现在多个方面，包括新发明和新思想的传播速度越来越快、通过增强现实（AR）和虚拟现实（VR）技术来提高学习效率，甚至AI也通过机器及其他传感器来感知世界等。此外，ChatGPT等人工智能工具，也极大地加快了人们学习的速度。</p><p></p><p>“这正是人工智能带来的真正巨大革命，”凯文·凯利说，人工智能系统正在生成新的事物——它们还不完美，但正在变得越来越好。</p><p>&nbsp;</p><p>机器学习泰斗、美国“三院院士”迈克尔·乔丹则指出，“缺乏对集体性、不确定性和激励机制的关注，是当前对人工智能的讨论中缺失的三个方面。”他强调，AI系统的发展不能仅依赖单个智能设备，而是要通过集体协作，构建去中心化的智能系统，特别是在面对不确定性时需要集体智能来应对。</p><p>&nbsp;</p><p>“AI拥有海量的数据，但有些不能生成价值，通过设计激励机制才能驱动AI智能体贡献和协作。”迈克尔·乔丹提出了“三层数据市场（Three-Layer Data Markets）”模型，其中用户、平台和数据买家通过“出让数据”、“购买数据”、“提供服务”形成了闭环。他强调，数据购买者也就是企业可以结合“数据和服务”建立与用户的激励机制，从而为他们带来真正的价值。</p><p>&nbsp;</p><p>对此，迈克尔·乔丹援引了统计契约理论，这是一种结合了统计学和经济学的新型理论。在契约理论中，代理人拥有私有信息，而委托人通过激励机制形成了数据和服务相互促进的市场，维持了供需双方的利益平衡。</p><p>&nbsp;</p><p>例如航空公司分“商务舱”和“经济舱”，航空公司作为委托人能够根据代理人的不同支付意愿提供不同的价格，而不需要代理人透露其个人信息。由于过去十年间，全球范围内对数据隐私的监管不断增加，他也建议“我们可以通过非一致的隐私要求进一步提高用户效用，对低成本平台施加更高的要求。”</p><p>&nbsp;</p><p>最后，迈克尔·乔丹将AI系统的发展类比于化学工程和电气工程的发展，前者建立在化学、流体力学等领域，后者基于建立在电磁学、光学等技术的基础上。而AI的基础是建立在推理、算法和经济理念上，并应以人类福祉为目标。“但人工智能正被置于那些未经深思熟虑的、朴素的旧式愿景之中，它的兴起和发展受到扭曲。”其提醒道。</p><p></p><p>香港科技大学校董会主席、美国国家工程院外籍院士沈向洋在演讲中谈到大模型时代人机交互方式的演变，从图形界面到搜索、推荐，再到对话，大模型的发展将推动这些交互方式的进一步迭代。</p><p>&nbsp;</p><p>沈向洋认为，AI为人类提供了与技术共生的全新语境，人机交互的新方式指向“AI与IA”的融合共进。IA（Intelligent Augmentation），即智能增强，代表着一种以人为本的 AI 发展路径。它聚焦于运用技术提升人类的能力，而非取代人类，强调了人类与 AI 之间的协作关系。</p><p>&nbsp;</p><p>在谈到AI agent时，沈向洋指出，agent从愿景到落地的过程中，需要始终以需求为圆点，深刻理解模型的能力，并构建一个AI深度参与的工作流程。他表示，AI agent时代的到来，不会是一个神奇而强大的模型突然代替了所有的工作流，它涉及到技术、工程与市场的不断磨合，最终以超预期的服务呈现给人类。</p><p>&nbsp;</p><p>此外，他强调，在AI迅速发展的同时，全球需要新的治理框架和体系来应对不同地区的需求和挑战。AI的治理至关重要，必须打造负责任的AI系统，才能确保其对社会产生积极的长期影响。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ix1ZmFtluRtI6IKSiYZy</id>
            <title>飞书业务工具矩阵再升级！新一代多维表格、低代码平台等产品集体炸场</title>
            <link>https://www.infoq.cn/article/Ix1ZmFtluRtI6IKSiYZy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ix1ZmFtluRtI6IKSiYZy</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Sep 2024 10:23:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9月4日，飞书发布了全新多维表格、低代码平台等系列业务工具产品，并推出了面向出海企业的跨境合规解决方案。这些产品将继续为中国企业提供实质的降本增效帮助，促进企业以更低的数字化成本解决实际业务问题。</p><p>&nbsp;</p><p>“如何能更有效的提升效率，更实实在在的降本，是企业家特别关注的话题。我们意识到，如果说以前追求效率是为了发展，那么现在追求效率则是为了生存。这些来自真实企业家的声音，也让我们更多地思考了自己的业务方向。” 谢欣表示，“多维表格和低代码平台、飞书项目都是直接为企业的一线业务服务，这几个产品将组成业界最强性能的业务工具，帮助企业更优质地降本增效。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c742edce5c5f80db36f0726c08244fe.png" /></p><p></p><p>图注：飞书CEO谢欣</p><p></p><p>飞书成立至今，已有海量中国优秀公司成为客户，近一年来，亦有霸王茶姬、胖东来、荣耀、公牛、雅迪这样家喻户晓的国民品牌开始全员使用飞书，他们也代表着广大中国优秀企业的共同选择。</p><p>&nbsp;</p><p>同时，飞书在2023年的ARR（年度可重复性收入）也已达到2亿美元，成为同类产品中的第一。</p><p></p><h2>新一代多维表格，让一线员工搭建系统不求人</h2><p></p><p>&nbsp;</p><p>自飞书2020年发布多维表格开始，这款产品逐渐为大众使用与熟知，并成为了一个全新品类。</p><p>&nbsp;</p><p>据飞书透露，飞书多维表格的月活数已经达到 600 万，仅过去一年，飞书用户便创建了近 4000 万个多维表格，在这些多维表格上，流转着超过 100 亿条记录。在泡泡玛特、元气森林、蔚来汽车等知名公司，飞书多维表格均以极小成本解决了重要业务需求，可贵的是，这些业务系统均由不懂技术的一线员工搭建。</p><p>&nbsp;</p><p>今日，飞书正式推出飞书多维表格数据库，这让飞书多维表格的单表容量突破了100万行，仪表盘也可统计1000万行数据，均为全球同类产品中最高。在全新的强大性能下，即使在飞书多维表格中计算10 万行、100 列公式这样复杂的数据，仍然能在5 秒内便获取业务结果。</p><p>&nbsp;</p><p>飞书多维表格还发布了全新一代仪表盘，通过飞书多维表格数据库的计算能力，由多维表格行列数据生成的仪表盘，将不再是简简单单计算、汇总、呈现数据，增加了大量计算、图表组件编组、统计分析等功能，界面也可对标全球顶尖 BI 系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ed/ed6922c1f6375b35f9aebdb275f649d2.png" /></p><p></p><p>图注：多维表格</p><p></p><p>随着飞书多维表格的日益深入，数千人共同使用的多维表格也在显著增多。飞书多维表格进一步升级了高级权限，每个不同表格使用人看到的数据、单元格、仪表盘等均会因为权限不同而显示不同，并且能够按照不同的条件收获动态同步。这也是目前最为精细的一套权限管理体系。</p><p>&nbsp;</p><p>此外，飞书多维表格还发布了多个AI功能，AI智能分析仪表盘可一键获取仪表盘数据背后的问题与变化，还可自动调用公式、一键生成自动化群推送等。</p><p>&nbsp;</p><p>目前，智谱、月之暗面、零一万物等公司均入驻推出了自己的AI字段捷径，这些AI工具也为企业在不同场景提供了更为智能的业务解决方案。</p><p>&nbsp;</p><p>“飞书在4年前发布多维表格时，多维表格这一品类在中国首次出现。今天，我们通过多维表格数据库、全新仪表盘、高级权限、AI等新一代功能，重新定义了多维表格的价值与作用。真正帮助业务人员自助搭建业务系统的平台，让一线员工搭建系统不再求人。”飞书多维表格负责人施凯文表示。</p><p></p><h2>飞书低代码平台正式发布让业务开发提效更敏捷</h2><p></p><p>&nbsp;</p><p>低代码，已是一个流行多年的开发方式，应用也越来越广泛。但在国内，低代码仍任重道远。企业既想要低代码的效率，又不想要过低的能力，而目前国内的产品，明显很难满足客户的需求。</p><p>&nbsp;</p><p>历经多年的开发与共创，飞书低代码平台已在多个企业有了较好的应用。例如在字节跳动，通过飞书低代码平台，工程师们构建了大量业务系统。每周，我们活跃的系统就有上千个，每天新增记录6000万条、运行流程2000万次，每年能帮字节节约数亿元的研发成本。而在联影医疗，基于飞书低代码平台，搭建了与Salesforce高度集成的复杂的CRM系统，满足上千人团队的协作需求，仅一个系统就节省150万元运营成本。</p><p>&nbsp;</p><p>飞书低代码平台，设计了一种新的解题思路，通过全代码能力建设，提供低代码开发，用多层架构的设计，兼顾能力强大和系统易用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a25c0fbf5d94feca0d914f10aad41d8.png" /></p><p></p><p>图注：飞书低代码平台</p><p>&nbsp;</p><p>对于IT团队，飞书低代码平台提供了全栈开发能力、全周期管理能力和全面开放能力，武装团队里的每一个开发者，让CIO的数字化蓝图落地又快又好。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jVwqya1bolDA4NB5SjCc</id>
            <title>当在本地就可以运行AI代码助手时，谁还需要GitHub Copilot呢？</title>
            <link>https://www.infoq.cn/article/jVwqya1bolDA4NB5SjCc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jVwqya1bolDA4NB5SjCc</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Sep 2024 10:08:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>本文最初发布于The Register。</p><p>&nbsp;</p><p>作为生成式人工智能的早期用例，代码助手实践已经获得了相当多的关注——尤其是在微软推出GitHub Copilot之后。但是，如果你不喜欢让微软动你的代码，或者不愿意每月支付10美元的费用，那么你可以构建自己的助手。</p><p>&nbsp;</p><p>虽然微软是最早将人工智能代码助手<a href="https://www.theregister.com/2022/12/09/github_introduces_copilot_for_business/">商业化</a>"并集成到IDE中的公司之一，但它远不是唯一选项。事实上，有许多为代码生成而训练的大型语言模型（LLM）。</p><p>&nbsp;</p><p>而且，你现在正在使用的电脑很有可能就能够运行这些模型。关键是以一种有实际用处的方式将它们集成到IDE中。</p><p>&nbsp;</p><p>这就轮到像Continue这样的应用程序发挥作用了。这个<a href="https://github.com/continuedev/continue">开源的代码助手</a>"被设计成可以嵌入流行的IDE，如JetBrains或Visual Studio Code，并连接到你可能已经比较熟悉的流行的LLM运行程序，如Ollama、Llama.cpp和LM Studio。</p><p>&nbsp;</p><p>像其他流行的代码助手一样，Continue支持代码补全和生成，并且能够针对不同的用例优化、注释或重构代码。此外，Continue还提供了一个具有RAG功能的集成聊天机器人，让你可以有效地与代码库对话。</p><p>&nbsp;</p><p>在本指南中，我们将搭配使用Continue与Ollama，但Continue也可以与多个专有模型（包括OpenAI和Anthropic）搭配使用——通过各模型的API，如果你愿意按令牌付费而不是每月支付固定费用的话。</p><p></p><h3>你需要做好以下准备：</h3><p></p><p>一台能够运行普通LLM的机器。一个处理器相对比较新的系统就可以，但为了获得最佳性能，我们建议使用Nvidia、AMD或Intel GPU ，且vRAM至少为6GB。如果你更喜欢用Mac电脑，那么任何Apple Silicon系统应该都可以，包括最初的M1。不过，为了能达到最佳效果，我们建议内存至少要有16GB。本指南还假设，你已经在机器上安装并运行了Ollama模型运行程序。如果没有，可以看下我们提供的<a href="https://www.theregister.com/2024/03/17/ai_pc_local_llm/?td=rt-3a">这份指南</a>"，它应该可以帮你在十分钟内运行起来。对于那些使用Intel Integrated或Arc显卡的用户，<a href="https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md">这里</a>"有一份使用IPEX-LLM部署Ollama的指南。兼容的IDE。在撰写本文时，Continue支持<a href="https://www.jetbrains.com/">JetBrains</a>"和<a href="https://code.visualstudio.com/">Visual Studio Code</a>"。如果你想完全避开微软的遥测技术，像我们一样，开源社区构建的<a href="https://vscodium.com/">VSCodium</a>"是个不错的选择。</p><p></p><h3>安装Continue</h3><p></p><p>在本指南中，我们将在VSCodium中部署Continue。首先，启动IDE并打开扩展管理面板，搜索并安装Continue。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bc/bcb539c921581a46ebef159e4b7a0eb7.jpeg" /></p><p></p><p>几秒钟后，Continue的初始设置向导启动，你可以选择是在本地托管模型还是使用另一个提供商的API。</p><p>&nbsp;</p><p>在这个例子中，我们将通过Ollama在本地托管我们的模型，因此，我们将选择“Local models（本地模型）”。该配置使Continue可以使用下列开箱即用的模型。稍后，我们会讨论如何将这些模型更换为其他选项，但现在，我们先从这些模型开始：</p><p>Llama 3 8B：来自Meta的通用LLM，用于注释、优化和/或重构代码。要了解关于Llama 3的更多信息，请阅读<a href="https://www.theregister.com/2024/04/19/meta_debuts_llama3_llm/">我们的发布日报道</a>"。Nomic-embed-text：用于在本地索引代码库的嵌入式模型，使你能够在给集成聊天机器人提示时引用代码库。Starcoder2:3B：这是BigCode的一个代码生成模型，为Continue的Tab自动补全功能提供支持。</p><p>&nbsp;</p><p>如果因为某种原因，Continue跳过了启动向导，不要担心，你可以在终端运行以下命令，使用Ollama手动拉取这些模型：</p><p><code lang="null">ollama pull llama3
ollama pull nomic-embed-text
ollama pull starcoder2:3b</code></p><p>&nbsp;</p><p>有关使用Ollama设置和部署模型的更多信息，请查看我们的<a href="https://www.theregister.com/2024/03/17/ai_pc_local_llm/?td=rt-3a">快速入门指南</a>"。</p><p></p><h3>遥测警告</h3><p></p><p>在继续之前，需要提醒一下，在默认情况下，Continue会收集匿名遥测数据，包括：</p><p>是否接受或拒绝建议（不包括代码或提示）；使用的模型名称和命令；生成的令牌数量；操作系统和IDE的名称；访问量。</p><p>&nbsp;</p><p>如果你不想自己的数据被收集的话，则可以修改主目录下的.continue文件，或者取消VS Code设置中的“Continue: Telemetry Enabled”复选框。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18f47fa011a4813e2488500f7b893f61.jpeg" /></p><p></p><p>要进一步了解Continue的数据收集政策，可以查看<a href="https://docs.continue.dev/telemetry">这里</a>"。</p><p></p><h3>请求就会有结果。有效吗？那是另外一回事了</h3><p></p><p>安装完成后，我们可以开始深入研究将Continue集成到工作流中的各种方法了。第一种方法可以说显而易见：从零开始生成代码片段。</p><p>&nbsp;</p><p>例如，如果你想为一个项目生成一个基本的网页，只需按下键盘上的Ctrl-I或Command-I，然后在操作栏中输入提示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/1174863748b16008918af2f1280ac67e.png" /></p><p></p><p>在这里，我们的提示是“Generate a simple landing page in HTML with inline CSS（使用HTML生成一个包含内联CSS的简单登录页）”。提交提示后，Continue将加载相关模型（这可能需要几秒钟，取决于你的硬件），然后它会向我们提供一个代码片段，我们可以选择接受或拒绝。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/062871366f02ec0fb7f9ff268c51ce35.png" /></p><p>在Continue中生成的代码将以绿色代码块的形式出现在VS Code中，你可以接受或拒绝。</p><p></p><h3>重写你的代码</h3><p></p><p>Continue还可以用于重构、注释、优化或编辑现有代码。</p><p>&nbsp;</p><p>例如，假设你有一个用于在PyTorch中运行LLM的Python脚本，你想重构它然后在Apple Silicon Mac上运行。首先，选择需要重构的文档，按下键盘上的Ctrl-I并给助手输入提示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/da/da0ad880010c4cf38c7b30a530acc888.png" /></p><p></p><p>几秒钟后，Continue会传出模型的建议——新生成代码用绿色高亮显示，而需要删除的代码则用红色标记。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1fbe088a4a14410439f16322de68f633.png" /></p><p></p><p>除了重构现有代码外，该功能还可用于事后生成注释和/或文档字符串。这些功能可以在右键菜单中的“Continue”下找到。</p><p></p><h3>Tab自动补全</h3><p></p><p>虽然代码生成对于快速实现模型进行概念验证或重构现有代码很有用，但根据所使用的模型的不同，仍然可能存在一些偶然性。</p><p>&nbsp;</p><p>任何曾经要求ChatGPT生成代码块的人都知道，有时它会产生幻觉包或函数。这些幻觉相当明显，因为糟糕的代码往往会导致令人印象深刻的失败。但是，正如我们之前<a href="https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/">讨论</a>"过的那样，如果频繁提供这样的幻觉包，可能会造成安全威胁。</p><p>&nbsp;</p><p>如果不需要AI模型为你编写代码，那么Continue还支持代码补全功能。这让你可以更好地控制模型进行或不进行哪些编辑或更改。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef015207849c1480f033e5860276e429.png" /></p><p></p><p>这个功能的工作方式有点像终端中的Tab补全。当你进行输入时，Continue会自动将你的代码输入到一个模型中——比如Starcoder2或Codestral——并提供补全字符串或函数的建议。</p><p>&nbsp;</p><p>建议显示为灰色，并且会随着你每次敲击键盘而更新。如果Continue猜测正确，那么你可以按下键盘上的Tab键来接受建议。</p><p></p><h3>与代码库对话</h3><p></p><p>除了代码生成和预测之外，Continue还提供了一个集成聊天机器人。该机器人具有RAG风格的功能。要了解更多关于RAG的信息，可以在<a href="https://www.theregister.com/2024/06/15/ai_rag_guide/">这里</a>"查看我们的实践指南，但在Continue中，它综合运用Llama 38b和nomic-embed-text嵌入式模型来实现代码库可搜索。</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/6304875b6bacfd52a9e59edeaca07129.jpeg" /></p><p>Continue提供了一个集成聊天机器人，它可以接入你选择的LLM。</p><p>&nbsp;</p><p>诚然，这个功能存在一些不确定性，但下面这几个例子可以说明如何使用它来提高工作流效率：</p><p>输入@docs，然后输入你的应用程序或服务的名称——&nbsp;例如Docker，最后输入你的请求。要查询关于工作目录的信息，输入@codebase&nbsp;，然后输入你的请求。将文件或文档加入模型的上下文：输入@files&nbsp;，然后选择你想要添加到下拉选项中的文件。按&nbsp;Ctrl-L将你在编辑器中选中的代码添加到聊天机器人。按Ctrl-Shift-R&nbsp;将来自VS Code终端模拟器的错误信息直接发送给聊天机器人进行诊断。</p><p></p><h3>更换模型</h3><p></p><p>在实践中，Continue的可靠性实际上取决于你选用的模型，因为这个插件本身实际上更像是一个将LLM和代码模型集成到IDE中的框架。虽然它定义了用户如何与这些模型交互，但它无法控制所生成代码的实际质量。</p><p>&nbsp;</p><p>好消息是，Continue没有与任何一种模式或技术绑定。正如我们前面提到的，它可以接入各种LLM运行程序和API。如果有新发布的模型针对你的首选编程语言进行了优化，那么除了硬件之外，没有什么可以阻止你使用它。</p><p>&nbsp;</p><p>由于我们使用Ollama作为模型服务器，所以在大多数情况下，更换模型是一项相对比较简单的任务。例如，如果你想把Llama 3换成谷歌的Gemma 29b，把Starcoder2换成Codestral，则可以运行以下命令：</p><p><code lang="null">ollama pull gemma2
ollama pull codestral</code></p><p>注意：Codestral有220亿个参数和32000个令牌的上下文窗口，即使精度量化到4位，在本地运行的话，也是一个相当庞大的模型。如果遇到了程序崩溃的问题，那么你可能会想试一下小一点的东西，比如<a href="https://ollama.com/library/deepseek-coder">DeepSeek Coder</a>"的1B或7B变体。</p><p>&nbsp;</p><p>要更换用于聊天机器人和代码生成器的模型，你可以在Continue的选择菜单中选择它。或者，你可以使用Ctrl-'循环遍历下载好的模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e08589f75cc0817b76df6fc5d7e6b29b.jpeg" /></p><p></p><p>更改Tab自动补全功能使用的模型有点麻烦，需要修改插件的配置文件。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f3e3b1c3ee1ee992646fd0a9bb29bc2f.png" /></p><p></p><p>拉取选择的模型后[1]，点击Continue侧边栏右下角的齿轮图标[2]，修改“tabAutocompleteModel”小节下的“title”和“model”条目[3]。如果你使用的是Codestral，那么这一部分配置应该是这样的：</p><p><code lang="null">  "tabAutocompleteModel": {
    "title": "codestral",
    "provider": "ollama",
    "model": "codestral"
  },</code></p><p></p><h3>自定义编码模型调优</h3><p></p><p>默认情况下，Continue会自动收集有关你如何构建软件的数据。这些数据可用于根据你的特定风格和工作流进行自定义模型调优。</p><p>&nbsp;</p><p>需要说明的是，这些数据存储在本地主目录下的.continue/dev_data文件夹下。而且，据我们所知，默认情况下，这些数据并没有包含在Continue收集的遥测数据中。不过，如果你还是担心的话，建议你把它关掉。</p><p>&nbsp;</p><p>大型语言模型调优的具体内容超出了本文的范围，但你可以读下<a href="https://blog.continue.dev/its-time-to-collect-data-on-how-you-build-software/">这篇文章</a>"，从中了解Continue收集了哪类数据以及如何利用这些数据。</p><p>&nbsp;</p><p>我们希望可以在未来的实践中进一步探索调优过程，所以请务必在评论区分享你对本地AI工具（如Continue）的看法以及你希望我们下一步做何种尝试。</p><p>&nbsp;</p><p>&nbsp;</p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p>&nbsp;</p><p>原文链接：<a href="https://www.theregister.com/2024/08/18/self_hosted_github_copilot/?td=rt-3a">https://www.theregister.com/2024/08/18/self_hosted_github_copilot/?td=rt-3a</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w3m3rysJpfEiyRszCjTR</id>
            <title>又一款AI编码工具火出圈！OpenAI投资、碾压VS Code、8岁女孩用它45分钟就能构建一款聊天机器人</title>
            <link>https://www.infoq.cn/article/w3m3rysJpfEiyRszCjTR</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w3m3rysJpfEiyRszCjTR</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Sep 2024 10:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>后AI时代，几乎每隔一段时间就会出现一个爆款AI应用引发业界广泛关注。近日，一款名为Cursor的AI 编码工具成为了万众瞩目的焦点。</p><p></p><p>Cursor集成了Claude 3.5 Sonnet和GPT-4o等先进模型，为用户提供了高效的编程体验。它不仅融合了开发环境的实用性，还融入了AI聊天机器人的交互性，能让用户仅使用文本提示即可编写、预测和操作代码。</p><p>&nbsp;</p><p>与GitHub Copilot等辅助工具相比，Cursor在自动化和完成度上有了显著提升，它的简单性在于可以通过聊天窗口进行操作，这意味着即使是完全不懂代码的人也可以在几分钟内运行一个功能齐全的应用程序，并不断在此基础上添加新功能，它真正做到了使编码更加民主化。</p><p>&nbsp;</p><p>它建立在与Microsoft Visual Studio Code相同的系统之上，确保了良好的兼容性和用户体验，因此迅速赢得了包括新手程序员和资深工程师在内的广泛用户群体。Perplexity、Midjourney和OpenAI的员工是付费使用该 AI 工具的 30000 名客户中的一部分。</p><p>&nbsp;</p><p>那么，这款AI编码工具到底什么来头？</p><p></p><h2>Cursor AI是谁？</h2><p></p><p>&nbsp;</p><p>Cursor AI编码工具是由Anysphere&nbsp;公司开发的。这家初创公司自 2022 年成立以来已筹集了超过 4 亿美元，并积极与包括Anthropic 和 OpenAI 在内的各多个模型合作。</p><p>&nbsp;</p><p>2022年，Michael Truell在麻省理工学院 (MIT) 求学期间结识了 Anysphere 的其他联合创始人 Sualeh Asif、Arvid Lunnemark 和 Aman Sanger，并成为了密友。四人有着共同的目标，那就是创建一个集成开发环境 (IDE)，以加快常见的编程和软件构建任务（如调试）。为此，他们成立了Anysphere 这家公司，致力于构建极其高效的人机交互系统。</p><p>&nbsp;</p><p>在公司官网上，他们写道：</p><p>&nbsp;</p><p></p><blockquote>“首先，我们正在构建未来的工程师：一个比任何单个工程师效率高出一个数量级的人机交互程序员。这种混合工程师将毫不费力地控制他们的代码库，并且不会出现低熵击键。他们将以自己的判断速度进行迭代，即使在最复杂的系统中也是如此。通过结合人工智能和人类的智慧，他们将比最好的纯人工智能系统更聪明、更精通设计。&nbsp;我们是一群研究人员和工程师。我们构建软件和模型，在有用和可能的边缘进行发明。我们的工作已经改善了数十万程序员的生活。”</blockquote><p></p><p>&nbsp;</p><p>基于这样的初心，Anyspher推出了 Cursor AI编码工具，它是微软开源代码编辑器 VS Code 的一个分支，它包含人工智能工具，旨在帮助开发人员编写代码并提出相关问题。</p><p>&nbsp;</p><p>Cursor 可以回答诸如“VS Code 中的哪项服务允许我将状态保存到磁盘？”之类的问题，并在程序员工作时提取相关文档和代码定义。</p><p>&nbsp;</p><p>Cursor 还具有由 OpenAI 模型提供支持的生成式 AI 功能，即能够根据提示生成代码。它还可以被动扫描文件并发现代码库中的潜在错误。</p><p>&nbsp;</p><p>“当人们想到‘人工智能加编码’时，他们通常会想到人工智能自动完成功能，”Anyspher联合创始人Sanger如是说。“我们认为 GitHub Copilot 和其他公司在这方面做得特别好，所以我们专注于自动完成功能之后的功能，比如查找和修复错误以及代码库问答。”</p><p>&nbsp;</p><p>虽然从表面上看，许多简单的功能（例如要求聊天机器人构建应用程序）都是用户已经可以在 Claude 或ChatGPT中实现的功能。 但Cursor的强大之处在于其与代码编辑器的集成以及快速进行更改或解决问题的能力。</p><p>&nbsp;</p><p>Anysphere&nbsp;公司 CEO Michael Truell 将Cursor描述为“程序员的 Google Docs”，这是一个内置 AI 模型的简单代码编辑器，仅使用文本提示即可编写、预测和操作代码。</p><p>&nbsp;</p><p>Truell 在接受《福布斯》采访时表示，他们的目标是让 Cursor 实现 95% 的工程师自动化，这样他们就可以专注于编码的创造性方面。这将使个人工程师能够“构建比强大团队目前所能构建的系统复杂得多的系统”。</p><p>&nbsp;</p><p></p><blockquote>“在我看来，它的真正力量在于使编码变得民主化。它还可以让没有太多编码经验的人通过输入几行文本来构建他们需要的工具。”</blockquote><p></p><p>&nbsp;</p><p>成立仅两年时间，已经完成了多轮融资，他们从<a href="https://www.openai.fund/">OpenAI Startup Fund</a>"筹集了 800 万美元的种子资金。</p><p>&nbsp;</p><p>市场上AI自动编码工具那么多，为什么Cursor 能引发热议？</p><p>&nbsp;</p><p></p><h2>Cursor 为什么会如此受欢迎？</h2><p></p><p>&nbsp;</p><p>不难发现，随着技术的不断迭代，大模型已经越来越擅长编写代码。所有主流人工智能模型现在都可以可靠地执行基本的编程任务，准确率甚至超过90%。它们开始通过规划和多轮提示策略来解决更复杂的现实世界任务。而且它们可以用80多种语言来完成。</p><p>&nbsp;</p><p>然而，编写代码不仅仅是编写新代码。开发人员的大部分时间都花在维护、调试或调整旧代码上。要做到这一点，就需要真正理解代码和系统意图。构建软件是一个从根本上创造性的过程——你可以改变困难的部分，但你不能完全自动化它们。</p><p>&nbsp;</p><p>大语言模型是程序员的强大工具，他们的编码能力会随着时间的推移而提高。但同样明显的是，对于大多数编码任务，要解决的问题不是如何让大语言模型单独运行良好，而是如何让它们与人类开发人员一起运行良好。</p><p>&nbsp;</p><p>因此，业界基本已经达成共识——程序员和 AI 模型之间的接口将很快成为开发堆栈中最重要的部分之一。所以也就不难理解为什么资本更愿意倾向Anysphere公司，因为他们在解决此问题上走得比较靠前。</p><p>&nbsp;</p><p>作为VS Code 的一个分支，Cursor 针对 AI 辅助编程进行了高度定制。它适用于所有最新的大语言模型，并支持完整的 VS Code 插件生态系统。Cursor 的特别之处在于旨在将 AI 集成到开发人员工作流程中的功能——包括下一步行动预测、自然语言编辑、与代码库聊天以及即将推出的一系列新功能。</p><p>&nbsp;</p><p>事实上，这种产品很难做好。完善文本编辑器已经花费了几十年的时间，而这个话题仍然在开发人员中引起激烈的争论。人工智能辅助编辑可以说更加复杂，需要新的工作流程（例如，如何一次编辑多个文件？如何在文件中间注入新代码？）并使用外部非确定性系统管理状态。这必须以一种高性能、感觉自然且不会干扰开发人员正常工作的方式完成。</p><p>&nbsp;</p><p>而这些事，Cursor 已经做得足够好。因此，在短短一年多的时间里，数千名用户注册了 Cursor，其中包括 OpenAI、Midjourney、Perplexity、Replicate、三星等公司，其中许多人已经开始为其付费，而且他们很少再换回其他 IDE。</p><p>&nbsp;</p><p>Figma的工程师Jordan Singer总结道：</p><p>&nbsp;</p><p></p><blockquote>“所有这些关于 Claude + Cursor 的讨论，以及能够构建任何你愿意做的事情（无论你的技能如何）都是有道理的。如果这就是未来，我想生活在其中。”</blockquote><p></p><p>&nbsp;</p><p>AI 大神 Karpathy 用完也忍不住赞叹：未来编程是不是只需要狂按 tab 就够了？</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/ef4237cfa27c91014233629fc2dfe3e8.png" /></p><p></p><p>&nbsp;</p><p>更重要的是，它不只是针对专业人士，连毫无编码经验的初学者也能轻松用起来。Cloudflare 开发者关系副总裁 Ricky Robinett分享道：</p><p>&nbsp;</p><p></p><blockquote>“我八岁的女儿仅用 45 分钟就使用 Cursor AI 构建了一个聊天机器人。”</blockquote><p></p><p></p><p></p><h2>Cursor 能否取代VS Code?</h2><p></p><p>&nbsp;</p><p>那么，如此受欢迎的Cursor能否取代？或者说，Cursor 是否有很大希望与 IDE 领域的现有企业竞争？</p><p>&nbsp;</p><p><a href="https://survey.stackoverflow.co/2023/">根据</a>"StackOverflow 的 2023 年开发者调查，微软的 Visual Studio Code 仍然是最受欢迎的 IDE，约 73% 的开发者表示这是他们的首选。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/e0/e0ce520d49be5590441689d67b94a120.png" /></p><p></p><p>图注来源：《2023 年 StackOverflow 开发者调查报告》</p><p>&nbsp;</p><p>Anysphere 团队确实认为微软是他们的主要竞争对手。他们承认这家科技巨头拥有巨大的分销优势。但他们表示，由于 Visual Studio Code 拥有广泛而多样的客户群，微软无法在不冒疏远部分用户的风险的情况下迅速做出重大改变或发布重大升级。</p><p>&nbsp;</p><p>“人工智能编码领域的天花板很高——有太多事情要做——不可能只是克隆技术然后在上面取得巨大的销售业绩，”Truell 说。“你需要不断发展技术。全球有超过 2600 万开发人员，对于那些想要真正人工智能原生体验的人来说，这是一个巨大的市场。”</p><p>&nbsp;</p><p>由4人组成的 Anysphere 创始团队雄心勃勃，他们希望在 Cursor 的开发路线图上实现一系列功能。在接下来的几个月里，他们的计划是让 Cursor 能够跨文件和整个文件夹进行更复杂的编辑，提高查找代码的能力，并从文档中学习新的库。</p><p>&nbsp;</p><p>与此同时，Truell 称，Anysphere 的受欢迎程度正在慢慢增长，平台上有数万用户，付费客户群“快速增长”。年经常性收入已超过 100 万美元——对于一家成立约两年的公司来说，这是一个好兆头。</p><p>&nbsp;</p><p>反观微软，虽然微软一直在努力“讨好”开发者（继Excel和Windows之后，该公司又在GitHub上发布了配套Copilot），但作为当今世界上最具影响力的开发软件之一，VS Code却在竞争对手的冲击之下逐渐迷失了方向。</p><p>&nbsp;</p><p>谷歌公司首席工程师Jaana Dogan评论称，“真搞不懂微软到底理不理解VS Code在市场上的实际影响力。”但不能否认的是，显然有一大批开发者不希望接受逐渐陈旧过时的VS Code。尽管GitHub上也出现了不少VS Code的分支代码仓库，但市面上的新工具仍然让VS Code的生存倍感压力。</p><p>&nbsp;</p><p>X上的一位开发者评论道：</p><p>&nbsp;</p><p></p><blockquote>“刚刚制裁了VS Code。至于理由？很可能是因为Cursor AI的发布，这股号称编码界ChatGPT的新生力量似乎成为压死VS Code骆驼的最后一根稻草。然而，我们曾经熟知的VS Code是否真会就此终结？”</blockquote><p></p><p>&nbsp;</p><p>为什么开发者会对VS Code感到失望？</p><p>&nbsp;</p><p>VS Code确实存在几个不容忽视的现实问题。前端软件工程师Mohamed Hamani表示，VS Code在Python开发中的表现相当差劲，并解释称其无法高亮显示代码中的问题。其他开发者也纷纷表示同意，VS Code效果不佳迫使他们往往选择其他 IDE，例如Jetbrains甚至用于Python的vim。</p><p>&nbsp;</p><p>哪怕是对于C#和其他一些相对传统的语言，虽然VS Code也推出过几次更新，但开发人员对于结果同样不太满意。此外，通过VS Code安装恶意扩展的难度太低。</p><p>&nbsp;</p><p>而谈到Cursor，人们普遍觉得它就是美化版或者扩展版的VS Code，能够集成多个开源大语言模型。相较于VS Code这样的纯IDE，Cursor对于大部分AI开发者来说显然更方便。</p><p>&nbsp;</p><p>X上的一位开发者解释道，“大家可以选定代码并根据其内容提出问题。也就是说，用不着在IDE和浏览器之间不断切换。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d52cff4c59bf34517394bee2793d0797.png" /></p><p></p><p>如果你还在用VS Code但又感觉非常不爽，不妨试试Cursor。</p><p>&nbsp;</p><p>有一说一，VS Code的固有功能做得不错，但在AI开发方面——特别是在与Claude 3.5 Sonnet和其他大语言模型的集成方面要远远落后于Cursor。虽然VS Code也允许集成其他大模型，例如Phi-3.5或者GPT-4，但Cursor在提供包括Llama 3.1在内的更多模型选项方面更加灵活。</p><p>&nbsp;</p><p>Cursor的编码和AI功能也应当成为引起微软关注的一记警钟，特别是努力降低VS Code与GitHub Copilot之间的整合门槛。</p><p>&nbsp;</p><p>Infinite Red创始人Jamon Holmgren表示，“我有个假设，Cursor的迅速走红应该能够唤醒微软，让他们进一步完善VS Code与GitHub Copilot之间的集成效果，而且最好能在几个月内实现。”</p><p>&nbsp;</p><p>如此说来，微软VS Code或者GitHub Copilot到底能不能赶得上这场趋势性的变革？</p><p></p><h2>Cursor来势汹汹，微软VS Code不甘示弱</h2><p></p><p>&nbsp;</p><p>就在一年之前，不少报道都把Cursor称为VS Code和ChatGPT合二为一的产物。现如今，人们开始将其视为VS Code和GitHub Copilot的替代品。自从Cursor诞生以来，放弃VS Code的声音就始终不绝于耳。但时至今日，VS Code仍然相当强大。</p><p>&nbsp;</p><p>在Reddit的讨论中，开发者们在比较Cursor和VS Code的功能时，往往更倾向于在长期规划当中选择VS Code。也就是说，多数人并不相信出自一家小型初创公司之手的Cursor真能跟整个微软帝国对抗。一位用户坦言，“即使Cursor能撑到明年，我也仍然会选择VS Code，因为它才代表着行业的标准和主流。”</p><p>&nbsp;</p><p>egghead.io创建者John Lindquist表示，他最近与VS Code的项目经理Harald Kirschner聊到了跟Cursor之间的竞争。该团队非常清楚Cursor的优势，而且正在积极筹备几项应对之策。“我想他们会带来一些惊喜。”</p><p>&nbsp;</p><p>VS Code效法Cursor的更新似乎即将推出，不知道Cursor能不能顶得住这样一波压力。出于生存的考虑，Cursor后续可能会接受OpenAI或者Anthropic等厂商的收购。</p><p>&nbsp;</p><p>至于VS Code这边，考虑到其庞大的装机规模和潜在的更新规划，开发人员实在很难断然将其卸载。毕竟微软也很明白，要想在下一阶段继续保住开发领域的优势地位，就得想办法让“使每个人都能用自然语言搞开发”的宣言成功落地。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.tomsguide.com/ai/cursor-is-chatgpt-for-coding-now-anyone-can-make-an-app-in-minutes">https://www.tomsguide.com/ai/cursor-is-chatgpt-for-coding-now-anyone-can-make-an-app-in-minutes</a>"</p><p><a href="https://techcrunch.com/2023/10/11/anysphere-raises-8m-from-openai-to-build-an-ai-powered-ide/">https://techcrunch.com/2023/10/11/anysphere-raises-8m-from-openai-to-build-an-ai-powered-ide/</a>"</p><p><a href="https://analyticsindiamag.com/developers-corner/why-developers-are-uninstalling-vs-code/">https://analyticsindiamag.com/developers-corner/why-developers-are-uninstalling-vs-code/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/umU7XiVP0CIXWqIzeXzb</id>
            <title>韩国“N号房”事件因 Deep Fake 再现，受害者向中国网友求救</title>
            <link>https://www.infoq.cn/article/umU7XiVP0CIXWqIzeXzb</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/umU7XiVP0CIXWqIzeXzb</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Sep 2024 09:26:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>视频 AI 生成技术的最新进展，已经开始产生一系列令人忧心的社会影响。不少精通这些技术的年轻人被发现，正在未经他人同意的情况下制作女性（大多是自己同龄人）的色情图像。</p><p></p><p>8 月 28 日，多位韩国女性在中国社交媒体平台发帖讲述“深度伪造”（Deep Fake） AI 换脸的大规模犯罪事件并向中国女性求救，引发大量关注与讨论，多个相关词条接连登上热搜。根据其描述，韩国长期发生产出和传播深度伪造色情内容的犯罪行为，而即时通讯软件 Telegram 因其加密通信和群组聊天功能受到这些犯罪者的青睐。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c59b4cd9eb21c527c743820590a16d30.jpeg" /></p><p></p><p>而早在今年 5 月，就有外媒报道，韩国首尔大学毕业生朴某和姜某，从 2021 年 7 月至 2024 年 4 月，涉嫌利用深度伪造技术换脸合成色情照片和视频并在 Telegram 上私密传播，受害女性多达 61 人，包括 12 名首尔大学学生。其中，主犯朴某（40 岁）制作了 1852 份换脸色情照片及视频，传播了约 100 条视频。</p><p></p><p>据悉，受害女性曾不断遭到相关 Telegram 群聊参与者威胁，后坚持申诉两年半得到上述调查结果。当时，韩警方相关人士称：“朴某等主犯并未通过非法合成物赚钱。他们的犯罪目的不是为了营利，而是为了满足自己的欲望。”</p><p></p><p></p><h1>一张深度伪造裸照 0.48 美元，盈利结构表明市场需求量</h1><p></p><p></p><p>所谓色情深度伪造，就是利用 AI 技术生成数字篡改的图像，且此类行为正在 Telegram 消息应用之上广泛传播已久。其中某 Telegram 频道已经拥有超 22 万名成员，能够按需提供深度伪造的裸照图像。该频道只需最简单的线上搜索就能轻松找到，通过收取费用根据成员提交的真实人物照片生成伪造图像。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0e/0eb8a747c965db3063d8b8101b35de90.png" /></p><p></p><p>超过 220000 名成员的 Telegram 聊天室内部的屏幕截图</p><p></p><p>8 月 21 日，《韩民族日报》通过 X 访问到该 Telegram 频道，其中设有图像机器人，能够将上传至频道中的女性照片转换为深度伪造裸照。加入该频道后，系统会弹出一条提示消息，要求用户“上传一张你喜欢的女性照片”。在按提示上传一张 AI 生成的女性照片之后，五秒之内该频道就生成了相应的深度伪造裸体图像，甚至允许用户自定义所生成图像中的各个身体部位。</p><p></p><p>该频道最多可免费生成两张深度伪造裸照，之后需要付费才能获取更多图片。每张照片费用为一颗“钻石”——售价为 0.48 美元或者 650 韩元。用户每次至少需要购买 10 颗钻石，但批量购买可以享受折扣。邀请朋友加入频道的用户也会免费获得钻石，这显然是为了扩大用户规模。</p><p></p><p>并且，该频道仅接受加密货币的付款方式，这很可能是出于保持匿名的考量。另外，该聊天室不允许用户之间相互发送消息或者图片，因此无从得知其他成员如何使用该频道生成深度伪造图像。</p><p></p><p>截至 8 月 21 日，该 Telegram 频道已拥有约 22.7 万名用户。由于 Telegram 在世界各地开放，因此聊天室中可能包含来自不同国家的成员。值得一提的是，该 Telegram 频道的访问门槛极低，在 X 和其他社交媒体上搜索特定关键字即可找到频道链接，其中一条带有链接的帖子甚至成为 X 上的“热门帖”。</p><p></p><p>尽管近期有报道称警方已经就其深度伪造罪行展开了调查，但用于推广该频道的帖子仍时有出现。专家们表示，该频道的存在就是深度伪造色情内容的现实缩影，而且很多人甚至根本不清楚这样的行为涉及犯罪。</p><p></p><p>韩国女律师协会人权事务主任 Seo Hye-jin 表示，“像这样在 Telegram 聊天室中制作性剥削及深度伪造类型的图像，并在其他群聊室中分享，正是当前诱发大规模性犯罪的根源所在。”在她看来，“如果单在图像制作阶段，相关 Telegram 聊天室中就有超过 22 万成员，那么后续传播造成的损失恐怕会非常巨大。”</p><p></p><p>而如此庞大的 Telegram 频道及其创收模式的稳定运行，很可能反映出一个可怕的现实：很多人认为制作这种极具危害性的深度伪造图像，其实只是无足轻重的小事。</p><p></p><p>“当前盈利结构的存在，表明市场的需求量很大。”韩国性暴力救济中心主任 Kim Hye-jung 总结道，“尽管对女性的性侮辱已经成为一种网络‘内容’类型，但社会对此类罪行的普遍轻视亦成为诱发性犯罪的一大关键因素。”</p><p></p><p></p><h1>深度伪造犯罪蔓延，因滥用处罚“轻”？</h1><p></p><p></p><p>近年来，韩国时常有一系列备受瞩目的数字性犯罪案件发生。韩国警方表示，今年迄今他们受理的深度伪造性犯罪案件数量已飙升至 297 起。最近一份报告显示，2023 年韩国与深度伪造图像有关的犯罪案件已高达 180 起。2022 年韩国共发生 156 起深度伪造犯罪案件，被定罪者中有 61% 为青少年。</p><p></p><p>情况之所以恶化，主要是因为韩国的监管框架漏洞百出。其法律规定，成年人若以违背他人意愿的方式，以引起性羞辱为目的处理或编辑他人的伪造视频、音频或照片内容且意图传播，可被判处最高五年监禁、或者最高 5000 万韩元的惩罚。但事实上，实际惩罚往往要比法条规定要宽松得多。</p><p></p><p>即使有人制作了深度伪造的性剥削视频，他们也很少面临监禁，仅仅拥有此类内容也不会受到惩罚。要追究制作或拥有深度伪造视频者的责任，必须证明其传播意图，这使得许多人可以轻易逃避法律制裁。法律还规定，犯罪者必须具有主观传播的意愿，意味着此类制作深度伪造色情内容的行为尚处于灰色地带。</p><p></p><p>《韩民族日报》曾对 46 起涉及伪造视频的法庭判决进行了分析，结果显示，在仅因传播伪造视频而遭到起诉的 18 人中（不包括同时被指控犯有其他罪行的嫌疑人），只有 1 人被判入狱、2 人获缓刑、2 人被罚款。在其中一案中，某年轻男子篡改了自己十几岁表弟的照片并将其通过手机通讯应用分享。初审判处其两年监禁，但上诉法院最终将刑罚改为缓刑。</p><p></p><p>相比之下，英国司法部去年 4 月宣布，无论是否传播，只要制作了深度伪造色情内容都将受到惩罚，还封锁了两个主要的深度伪造色情网站。</p><p></p><p>此外，韩国与深度伪造有关的犯罪数量及青少年犯下此类罪行的比例，都呈现出上升趋势。根据韩国国民力量党代表赵恩熙（Cho Eun-hee）撰写的相关报告，在因此类罪行而受到惩制的 120 人中，有 91 人为青少年（占比 75.8%）。</p><p></p><p>8 月 21 日，釜山市教育局表示有 4 名中学生因使用深度伪造技术对 18 名学生和 2 名教师的面部进行数字复制而受到警方调查。这些年轻的嫌疑人共制作了约 80 张受害者的色情图像，并通过移动通讯应用对外分享。2023 年，釜山共出现 12 起由学生传播同学深度伪造色情图像的案件；而到 2024 年，仅上半年就发生了 15 起同类案件。在济州岛，警方最近抓获了一名十几岁的国际学校学生，他曾使用至少 11 名同学的面部制作深度伪造色情内容。</p><p></p><p>赵恩熙表示，“这些给受害者造成不可逆转伤害的数字犯罪行为，正在青少年群体当中蔓延，很多人将甚至将其当作一种游戏。”她还强烈呼吁通过系统性法条修订来防范此类犯罪。深度伪造犯罪具有多种形式，这些图像有些被用于霸凌受害者，有时也纯为追求经济利益。</p><p></p><p>据悉，韩国青少年获取人工智能（AI）服务的门槛相当低。今年 5 月，韩国国家信息社会局发布的一项针对 2261 名青少年的调查显示，韩国约 77.5% 的青少年表示他们听说过生成式 AI，且有超过半数（52.1%）的青少年表示他们使用过该项技术。</p><p></p><p>尽管越来越多的青少年因违法使用深度伪造技术而受到惩罚，但大多数判例相对较为宽松。部分原因在于，针对未成年人的惩制力度本身就更轻。</p><p></p><p>正因为如此，犯罪专家才强调，政府应当采取措施教育学生，确保他们意识到这类犯罪行为的严重性。京畿大学犯罪心理学教授李秀贞（Lee Soo-jung）表示，关于编码等计算机技术的课程，必须同时兼顾法律和道德等应用约束内容。她强调称，这种教育必须从孩子抓起，因为年龄较小的孩子更容易将此类犯罪视为无伤大雅的恶作剧。</p><p></p><p></p><h1>结语</h1><p></p><p></p><p>专注于身份盗窃保护的美国初创公司 Security Hero 2023 年关于全球深度伪造的报告显示，韩国是深度伪造色情内容最多的国家，其中歌手和女演员占此类深度伪造内容中个人的 53%。</p><p></p><p>除了敦促社交媒体公司更积极地配合删除和屏蔽此类内容外，韩国媒体监管机构还要求法国当局在 Telegram 相关问题上定期合作，并为与 Telegram 的直接沟通提供便利。在韩国打击对性深度伪造行动的同时，Telegram 的创始人帕维尔·杜罗夫 （Pavel Durov） 在法国接受正式调查，法国当局正在调查该消息应用程序上的有组织犯罪。</p><p></p><p>此外，韩国政府表示，将推动更严格的法律，把购买或观看性剥削深度伪造定为犯罪行为。韩国警方还计划进行为期 7 个月的打击数字性犯罪，并增加监控此类事件的监管人员数量，为受害者建立 24 小时热线。Telegram 也在一份声明中表示，将积极审核其平台上的有害内容，包括非法色情内容。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.straitstimes.com/asia/east-asia/deepfake-sex-crimes-widespread-among-teens-in-south-korea">https://www.straitstimes.com/asia/east-asia/deepfake-sex-crimes-widespread-among-teens-in-south-korea</a>"</p><p></p><p><a href="https://english.hani.co.kr/arti/english_edition/e_national/1154909.html">https://english.hani.co.kr/arti/english_edition/e_national/1154909.html</a>"</p><p></p><p><a href="https://www.msn.com/en-xl/news/other/editorial-53-of-global-deepfake-victims-are-korean-and-politicians-are-to-blame/ar-AA1pMBtl?ocid=BingNewsSerp">https://www.msn.com/en-xl/news/other/editorial-53-of-global-deepfake-victims-are-korean-and-politicians-are-to-blame/ar-AA1pMBtl?ocid=BingNewsSerp</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DoAbIZ6i7x90V9SvPYxo</id>
            <title>AI 生产力工具暑期迎来大爆发，极光数据：夸克新增用户规模领先</title>
            <link>https://www.infoq.cn/article/DoAbIZ6i7x90V9SvPYxo</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DoAbIZ6i7x90V9SvPYxo</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Sep 2024 09:02:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>9 月 3 日，极光旗下月狐数据发布《AI 生产力工具暑期发展报告》。数据显示，AI 生产力工具在用户侧呈现高速增长态势，总体月活跃用户数量达 1.7 亿。其中，夸克 APP 实现暑期新增用户数量行业第一，凭借大模型、数据、场景等优势，让更多用户享受到一站式 AI 服务。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/ec/e6/ece3227ae1d1cfec9a8eb8bfa3b0c0e6.png" /></p><p></p><p>报告显示，目前国内生成式 AI 应用快速发展，预计市场规模有望达到 4000 亿元。其中 AI+ 生产力工具赛道的产品数量迅速上升，如 AI 搜索、AI 写作等产品已经进入爆发期。</p><p>&nbsp;</p><p>今年暑期，AI 生产力工具产品在用户新增和用户粘性等数据上屡创新高。6 月，日均用户新增至 194 万。7 月，单日人均使用时长达到 14.3 分钟。极光分析师认为，AI 生产力工具以智能化及自然交互能力，很好地辅助学生进行学习以及帮助职场人群处理各类工作文档。</p><p>&nbsp;</p><p>今年暑期，夸克通过为用户提供一站式 AI 服务和系统级全场景 AI 能力，有效抓住增长机遇，实现新增用户规模领先。今年 6 月高考季，夸克高考 AI 搜索使用量超过了 1 亿次。7 月，夸克升级“超级搜索框”，以 AI 搜索为中心，通过智能问答、智能写作、智能总结，为用户提供从检索、创作、总结，到编辑、存储、分享的一体化信息服务价值。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/f9/8a/f91478ba60e2d077659c916f7f19a38a.png" /></p><p></p><p>日前，夸克发布全新 PC 端，全面升级 AI 搜索、AI 写作、AI PPT、AI 文件总结等一系列功能。凭借“系统级全场景 AI”能力，无论是在桌面、文档还是网页中，用户都能通过快捷键、划词、截屏、右键菜单等方式，随时唤起夸克 AI 能力，为用户升级AI电脑。</p><p>&nbsp;</p><p>依托 AI 技术，凭借持续进化的产品能力，夸克正在为用户创造全新价值。在搜索速度上，夸克的首字出现速度和吐字速度领先大幅领先同类产品；在写作创作上，夸克可以撰写近 200 种体裁的文稿，在半分钟内能生成一篇高质量的文章；在 PPT 场景中，用户输入主题或选择模板，夸克就能生成一份 25 页左右的专业 PPT；在文件总结上，夸克不惧几十万字的长文和最长6小时的学习视频，通过生成脑图、抽取课件等方式让总结更简单。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/b2/43/b24dba91b8736d0bbfb14e3c4ee83943.png" /></p><p></p><p>夸克的不断精进也让用户感受到了效率提升。网名为“明白得糊涂”的网友分享到，夸克AI搜索的亮点是会整理全网的搜索内容，特别是针对一些复杂问题，AI 会直接给出回答。用户“雨后彩虹”认为，夸克把搜索体验推上了一个新阶段，各种AI工具非常适合给学生和白领提效。</p><p>&nbsp;</p><p>夸克自诞生之初，就将 AI 技术置于其产品战略的核心位置，定位“智能搜索”获得了年轻一代用户的认可。根据夸克数据显示，作为一个用户过亿的产品，夸克中 25 岁以下的年轻用户占比超过一半。在 AI 大模型进入到比拼应用的新阶段，夸克通过新技术重新定义产品的边界，并朝向 AI 效率助手的方向前进。</p><p>&nbsp;</p><p>此前，根据七麦数据发布的《2024 年第二季度 iOS 实力 AI 产品排行榜》和新榜·AI 产品榜的最新数据显示，夸克无论是访问量还是下载量，都在 AI 产品中占据领先的优势。在苹果应用商店下载总榜和工具榜中，夸克持续保持前列位置。</p><p>&nbsp;</p><p>极光分析师认为，随着用户对效率需求的不断提升，预计行业竞争者的数量仍将上升，更多生产力工具向 AI 化发展、更多科技企业及互联网厂商加码布局。同时，厂商将面向用户需求和效率痛点持续推动产品服务的升级演化，以提升用户使用体验、建立核心竞争力。未来，AI生产力工具将基于AI能力不断丰富服务板块，从满足细分场景发展为覆盖通用需求，打开行业想象空间。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MrrGLMo8iCKYUUj9ZSLS</id>
            <title>易咖智车再获2亿融资，智能工厂落地后将年产5000台L4级无人驾驶车</title>
            <link>https://www.infoq.cn/article/MrrGLMo8iCKYUUj9ZSLS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MrrGLMo8iCKYUUj9ZSLS</guid>
            <pubDate></pubDate>
            <updated>Wed, 04 Sep 2024 08:54:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>近日，易咖智车宣布获得2亿元战略投资，本轮融资由上海国和投资、中信建投资本、无锡惠山科创联合完成。据悉，本轮融资将用于智能工厂建设和产能扩容提升。</p><p></p><p>当前中央及地方各级政府加速出台了各类扶持政策和法规，城市服务型自动驾驶车的技术和商业模式已在场景端充分验证，市场端规模化落地需求愈加旺盛。目前已有北京、上海、广州、深圳、杭州等51个城市出台了自动驾驶试点示范政策，五部委确定了20个城市（联合体）为智能网联汽车“车-路-云一体化”应用试点城市，为自动驾驶的规模化应用提供了必要的基础条件和安全的运营环境，支持自动驾驶加速落地。</p><p></p><p>凭借5年多的技术积累和产品力打磨，易咖智车在城市服务型无人车领域积累了资源和经验，累计开发了20多款产品，与60%中游算法合作伙伴深度合作，落地应用覆盖了60余座城市，累计交付超千台无人驾驶车，实际工程化落地车辆累计行驶里程超1000万公里。</p><p></p><p>据悉，易咖智车城市服务机器人南京智能制造工厂将于2025年二季度建成，正式达产后具备单班年产2万台的能力，将率先打造全球领先的L4级无人车柔性自动化生产与沉浸式体验相结合的数字化智能工厂，可为客户贴身定制个性化产线。</p><p></p><p>中信建投资本董事总经理厉辉表示：目前，低速无人驾驶赛道已经过市场较充分验证，政策环境向好，自动驾驶技术趋向成熟，行业正进入加速放量期。易咖智车已与国内物流、环卫、信采等领域多家重点客户建立合作关系，产品可适配多种应用场景，预期订单充足。</p><p></p><p>此次易咖智车与惠山科创集团、惠山高新区共同建设无人车智能工厂，项目落地后预计年产5000台L4级无人驾驶车并支撑1万台无人车的智能运维服务。</p><p></p><p>成立于2018年的易咖智车是IDV智能数字化移动平台科技公司，聚焦L4级自动驾驶线控底盘技术和服务，围绕“全线控+真智能”的“滑板底盘”核心技术，通过构建ADC产品生命池（智能设计引擎+数字化智造+云端数据挖掘），打通用户需求、产品开发、智能智造、智能运维等全环节全生命周期内的信息流、物流、知识流管理，携手AI及科技公司赋能无人驾驶在物流配送、环卫清洁、安防信采、移动充电等加快规模化商业落地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NBiP3Sw3Fkxx2MqqEt4z</id>
            <title>累计融资10 亿元，这家清华系AI创企致力于成为大模型时代首选“算力运营商”</title>
            <link>https://www.infoq.cn/article/NBiP3Sw3Fkxx2MqqEt4z</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NBiP3Sw3Fkxx2MqqEt4z</guid>
            <pubDate></pubDate>
            <updated>Tue, 03 Sep 2024 04:00:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024&nbsp;年&nbsp;9&nbsp;月&nbsp;2&nbsp;日，无问芯穹（Infinigence&nbsp;AI）宣布完成近&nbsp;5&nbsp;亿元&nbsp;A&nbsp;轮融资，至此，在成立短短&nbsp;1&nbsp;年&nbsp;4&nbsp;个月内，无问芯穹累计已完成近&nbsp;10&nbsp;亿元融资。无问芯穹本轮融资联合领投方为社保基金中关村自主创新专项基金（君联资本担任管理人）、启明创投和洪泰基金，跟投方包括联想创投、小米、软通高科等战略投资方，国开科创、上海人工智能产业投资基金（临港科创投担任管理人）、徐汇科创投等国资基金，以及顺为资本、达晨财智、德同资本、尚势资本、森若玉坤、申万宏源、正景资本等财务机构。无问芯穹的过往投资方还包括百度、智谱、同歌创投等战略股东，以及红杉中国、砺思资本、徐汇资本、北极光创投、真格基金、惠隆创投、经纬创投、无限基金&nbsp;SEE&nbsp;Fund、金沙江创投、星连资本、绿洲资本、南山资本、光源资本、七熹投资等众多知名投资机构。</p><p>&nbsp;</p><p>无问芯穹本次融资募集的资金将用于加强技术人才吸纳与技术研发，深耕软硬协同、多元异构技术领域；深入推动产品商业化发展，保持Infini-AI异构云平台产品与市场间的紧密嵌合；强化生态合作，激活异构集群算力资源，构建支撑“M种模型”和“N种芯片”的AI算力底座，做AI模型算力的“超级放大器”，致力于成为大模型时代首选的“算力运营商”。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/d0/d0ec2da2eb735be00fb7752cdca67aaa.jpeg" /></p><p></p><p>无问芯穹联合创始人、CEO夏立雪表示：“感谢多方资本对无问芯穹的青睐，让我们在‘天时、地利、人和’的创业征途上多了一份不负厚望的笃定。AI&nbsp;2.0浪潮带来的新‘二八定律’，Transformer&nbsp;架构统一了新的技术范式，意味着只需解决20%的关键技术问题即可支撑80%的垂直场景泛化，给软硬件联合优化技术的标准化和规模化提供了难得的机遇；中国算力生态正面临的供需矛盾以及资源分布不均的现状，为我们拉动上下游协力实现多元异构算力的高效整合创造了时代机遇；而我们源于清华电子系的深厚底蕴、十余年的技术积累和丰富的产业经验所形成的产研结合的‘复合型’团队，更成为AI领域的人才‘引力井’，构成了无问芯穹独特的人才竞争力。”</p><p></p><h2>做AI模型算力的“超级放大器”</h2><p></p><p>&nbsp;</p><p>大模型能够支撑的实际产业规模，取决于AI模型的实际可用算力，是一个壁垒更高、玩家更稀缺、价值量更高的领域。基于对AI行业的深刻理解和长期实践，无问芯穹超前判断大模型的实际可用算力不仅取决于芯片的理论算力，还可通过优化系数放大算力利用效率，通过集群规模放大整体算力规模，由此，无问芯穹提出“芯片算力×优化系数（软硬协同）×集群规模（多元异构）=AI模型算力”公式。遵循这一公式，无问芯穹将通过软硬件联合优化技术，持续提升芯片算力在大模型任务中的利用率，并通过多元异构算力适配技术，提升集群算力利用率，扩大行业整体算力供给。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tYRyoEcqiw6XlOEHatzQ</id>
            <title>工业制造的智能化转型：从传统决策到运筹优化</title>
            <link>https://www.infoq.cn/article/tYRyoEcqiw6XlOEHatzQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tYRyoEcqiw6XlOEHatzQ</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Sep 2024 08:20:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>随着工业智能化和数字化转型的深入推进，制造业企业在决策管理中面临越来越复杂的挑战。传统依赖人工经验的决策模式，已难以适应当前多品种、小批量的生产需求。在这种背景下，智能决策技术，特别是基于运筹优化和人工智能的高级计划与排程（APS）系统，逐渐成为推动企业降本增效、提升运营效率的关键工具。</p><p></p><p>在 6 月举办的 ArchSummit 全球架构师峰会上，清智优化董事长兼总经理蒙绎泽博士分享了主题演讲《企业全层级决策管理智能优化平台助力企业降本增效增收》。其详细探讨了智能决策的发展历程及其在制造业中的应用，重点介绍了运筹优化技术的理论基础、求解方法及其在实际项目中的落地应用。通过对典型案例的阐述，展示了智能优化平台如何在复杂的工业场景中发挥作用，从而为企业提供全面、优化的决策支持。</p><p>以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。</p><p></p><h2>智能决策发展历史</h2><p></p><p>尽管数字化转型、工业化和智能化已经发展了很多年，但在许多制造业企业，尤其是一些领先的企业中，仍然存在许多依赖人工决策的环节。例如，在制定生产计划、采购和物流运输计划等方面，仍然存在依赖经验进行决策的现象。在当前多品种、小批量的生产方式下，人工决策不仅耗时长，而且质量也难以保证。这是因为决策空间规模和计算复杂度随着产品种类的增加而呈指数型增长。</p><p></p><p>目前，工厂基本上具备了柔性生产能力，但多种因素的叠加使得传统人工决策面临许多局限性。虽然企业已经采用了 ERP 和 MES 等系统，主要用于记录和执行流程，系统也可能包含一些基于规则的简单排产和决策逻辑，如先进先出（FIFO）等，但这些简单规则的能力非常有限，不能支持复杂的调度问题。</p><p></p><p>我们的目标是基于运筹优化技术，结合人工智能技术，全面考虑经营过程中的决策要素，实现整体最优化的解决方案，从而显著降低成本并提高效率。</p><p></p><p>APS（Advanced Planning and Scheduling，高级计划与排程）有狭义和广义之分。狭义的 APS 主要指的是车间生产计划排产，这通常包括 AP（Advanced Planning，高级计划）和 AS（Advanced Scheduling，高级排程）。我们重点聚焦于工厂内部的生产计划。广义上的 APS，其应用范围可以扩展到整个供应链管理，覆盖更广泛的功能和领域。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f91088623f3b76fe07b01f818e721b6.png" /></p><p></p><p>我们的定位是将 APS 视为智能工厂的大脑。在智能工厂的架构中，ERP（Enterprise Resource Planning，企业资源计划）、MES（Manufacturing Execution System，制造执行系统）、WMS（Warehouse Management System，仓库管理系统）和 CRM（Customer Relationship Management，客户关系管理）等系统主要负责数据和单据的记录。而我们的 APS 系统则是在这些系统之上，进行计划的执行。具体来说，数据从这些系统上传至 APS 后，我们会进行最优运算，生成最优细化计划，并下发执行，以替代人工计划环节。这个过程涉及到如何制定最优计划，而最优计划通常能够为客户带来显著的效益，包括计划调度、仓储库存、运输调度等场景。</p><p></p><h4>关于运筹优化技术</h4><p></p><p>运筹优化是一门起源于二战期间的学科。它主要研究的是如何在资源有限的系统中，利用数学优化算法来制定人力、物力和财力的最佳调配方案，目的是使整个系统的效益达到最大化。运筹优化技术涵盖了多种核心算法和技术，包括规划论中的整数规划、非线性规划、动态规划、图论、组合优化以及库存论等技术。</p><p></p><p>典型的优化问题通常包括一个优化目标以及多个约束条件。依据变量和约束条件的特点，可以将其分为四类主要的规划模型：线性规划、整数线性规划、非线性规划和混合整数非线性规划。</p><p>在这些模型中，MIP（混合整数规划）是最常用的一种。它指的是问题中的决策变量具有离散特性，同时约束条件是线性的。在实际应用中，许多不同类型的问题最后都可能被转化为 MIP 问题来求解，这是因为 MIP 模型能够在保持约束条件线性的同时，处理决策变量的离散性，从而找到最优或近似最优的解决方案。</p><p>在运筹优化领域，求解器扮演着至关重要的角色。它是一个专门用来求解规划模型的算法包。我们可以将求解器比作一个高级计算器，当我们将规划模型输入其中，它能够快速地为我们提供最优解或可行解。</p><p><img src="https://static001.geekbang.org/infoq/7f/7f3f7306b0e120f931aafbdf5132f6f0.png" /></p><p></p><p>求解器内部集成了多种优化算法，这些算法各具特色，能够应对不同类型的优化问题。其中，分支定界和分支定价是最典型的算法。分支定界通过系统地探索解空间的各个部分来找到最优解，而分支定价则是一种用于解决整数规划问题的有效方法。除了这些，求解器还可能包含一些启发式算法和梯度下降等数值优化方法。</p><p></p><p>解决运筹学问题通常涉及四层抽象。对于业务人员，他们接触最多的是第一层，即实际的商业问题。例如，作为工厂的总监，他们需要决定如何为工人排班，或者如何选址建立工厂设施。这些是工厂日常运营中可能面临的决策问题。</p><p></p><p>第二层是通用问题，我们需要先将问题抽象为通用性问题。这些问题可能类似于旅行商问题、最短路径问题或图着色问题。</p><p></p><p>第三层抽象是对问题的建模范例，确定它属于哪一类优化问题，比如是混合整数规划（MIP）、线性规划还是非线性规划。</p><p></p><p>第四层抽象是求解器和求解算法。所有问题最终都归结到求解层面。这包括使用不同的搜索排序技术、Benders 分解以及分支定界等方法。</p><p></p><p>运筹学项目的实施方法论通常包括以下几个关键步骤：</p><p>数据初步分析，驱动算法设计：首先，我们需要收集相关的基础数据，并对其进行分析以获得初步的见解。这一步至关重要，因为现实生活中的许多问题都是 NP 难问题，直接求解可能需要很长时间。通过从数据中提取见解，我们可以简化模型求解过程，利用数据的性质来加速求解。设计和建立高效数学模型：接下来，我们需要设计和构建针对实际问题的高效数学模型。运筹学同行都知道，模型的建立有高效和低效之分，同样一个问题的不同建模方式计算效率可能天差地别。分析模型结构及难易程度：模型建立后，我们需要分析模型的结构以及解集的性质，为设计求解方法打下基础。设计求解方法：这一步涉及到选择合适的求解器、设计求解框架，决定是使用启发式算法还是精确求解，是否需要分阶段求解。求解器是一个工具，但并非万能，还需要结合其他技巧来高效求解。代码实现：将设计好的求解方法转化为代码实现，并进行初步求解，得到计划结果。数据审查和校验：对初步求解结果进行审查和校验，这可能涉及到与现场实施人员的沟通。算法性能分析：对求解算法的性能进行分析，以评估其效率和准确性。迭代优化：与现场业务人员反复确认，因为业务人员提出的需求可能并非其真正需求。通过不断展示结果，帮助他们逐渐明确真正的需求。回测对比和效益提升：进行回测对比，计算效益提升，并给出管理建议。</p><p></p><p>运筹优化模型的计算原理涉及到在求解过程中对搜索空间的逐步削减。这个过程通过不断缩小搜索范围，使得算法的上下界之间逐渐收敛。随着算法的进行，搜索空间被逐步缩小，最终收敛到一个较小的区域，从而得到一个高质量且符合要求的结果。理想情况下，这种优化算法能够达到与穷举法相同的效果，即找到最优解或近似最优解。但是，优化算法能够在较短的时间内找到高质量的解决方案，提高求解效率。</p><p></p><h2>企业全层级决策管理的智能优化平台</h2><p></p><p></p><p>整个智能优化平台的架构设计包括几个关键层次。首先，底层是场景和行业的模板。这些模板内置了典型行业的约束和工艺要求，例如石化行业的效益测算模型、智能配料或机加工行业的通用排程模板。</p><p>接下来，在这些模板背后，是为特定场景定制的求解器、模型库和算法模块，包括整数规划模型、启发式算法等。</p><p></p><p>最后，平台还包括数字孪生系统。这个系统允许用户对不同的方案进行手动比较，这对于实际应用中的问题解决非常关键。在实施过程中，可能会遇到现场人员质疑为什么平台会得出某个特定的解，他们可能认为自己的方案更好。面对这种情况，数字孪生系统可以让他们展示自己的方案，并计算评价指标，然后与平台的解进行比较。通过这种比较，他们往往会发现，尽管他们认为好的方案在某些方面可能更优，但在许多关键指标上却表现不佳。我们通常通过展示这些结果来说服客户，证明平台的解决方案在综合考虑多个关键指标后，能够提供更优的整体效益。</p><p></p><p>整个技术体系的底层涉及数据的获取，这些数据包括销售订单、生产订单、物流以及采购信息。这些数据通常从 ERP（企业资源计划）和 MES（制造执行系统）等系统中上传。在此基础上，我们构建了内置的优化模型，涵盖采购优化、计划排产、需求预测和经济效益优化等方面。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ac/acf933d01424ed213daf3f8a4133131b.png" /></p><p></p><p>核心的分析和求解过程包括几个关键步骤。首先，需要与客户一起确定约束条件，这些条件必须是定量的，并且以白盒式的方式明确。其次，明确优化目标，比如客户最关心的 KPI 是什么，是设备利用率最大化还是成本最小化。如果存在多个目标冲突，需要设定权重，以避免不必要的争议。</p><p></p><p>优化算法的设计是我们团队的强项。我们能够为多个行业和场景提供优化解决方案。例如，在能源行业，我们可以进行管网运营优化；在新材料行业，可以进行套切配分优化和工程塑料排产；钢铁和电子行业也可以通过我们的技术进行优化。这些优化功能包括计划调度排班优化、仓储库存优化以及运输路径计算等。</p><p></p><h4>典型案例一：某企业天然气管道运输优化</h4><p></p><p></p><p>首先，该企业的痛点在于中国的天然气管道网络覆盖广泛，但目前主要依赖 Excel 进行管理和决策，并由人工负责每个片区，各自为政，各个链条之间没有联通。因此，需要建立一套生产、运输、销售、存储、贸易一体化的优化模型。我们需要在满足约束条件下基于稳态计算各个节点的最优参数，实现降本增效的目的。投建规划同样问题复杂，主要难点在于系统规模日趋复杂，这虽然是网络流问题，但比传统网络流要复杂得多。原因在于压缩基站本身有复杂的工艺约束，气体在管道内流动需要满足流体力学方程，因此数学求解难度呈指数级增长。</p><p></p><p><img src="https://static001.geekbang.org/infoq/18/18a7ff96786312c334d05ee14a81f5e5.png" /></p><p></p><p>结果方面，我们搭建了天然气全业务链条的优化技术体系，研发了适用于任意拓扑结构的天然气管网优化平台，解决了常用模型难以处理低时差和温度变化的问题。我们设计了一种针对上千节点大规模管网规划的特殊凸优化松弛算法，两年内为他们节省了 20 亿元人民币，主要通过优化压缩机的控制参数，降低能耗，同时满足任意时期客户的需求，提升了整个网络的运行效率。</p><p></p><p>在建模方面，我们建立了天然气管道网络的整体运行模型。这包括流量、流向和压力等约束的建模，以及管网的网络结构建模和多级体系的融合分析。目标函数方面，涉及多目标优化。例如，有些方案侧重于能耗优化，有些侧重于效益优化，还有一些考虑路径优化或负荷优化。</p><p></p><p>由于具体的模型主要由公式组成，我们需要关注其表达的含义，从三个层面来说明。</p><p></p><p>首先是决策变量。我们计算的决策点包括各个节点、各个时间段的压力情况、供给情况；各条线路各时间段的流向流速；以及各个储气站的库存注入量和出入量等。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d2/d237685bf651447f4169b37134770227.png" /></p><p>第二是优化目标。这是一个多目标函数。最主要的一层是能耗优化，例如压缩机的功率总和就是能耗。其次是运输路径优化，第三是负荷优化。还有一种目标是在满足网络所有运行约束的前提下，最大化管道输送的气量。此外，还有收益最大化的方案。收益是销售减去库存、运输能耗、生产注气等相关费用后的结果。</p><p>第三是约束条件。需要考虑流量平衡、各节点的供应生产和库存能力等约束。我们需要的数据包括各地区的预计需求量、管网的数据和节点的数据等。</p><p></p><p>针对非凸非线性可行域的快速求解模块的解过程简单介绍一下，因为算法相对复杂，具体细节可以参考相关论文。对于非凸非线性的可行域，我们的快速求解模块依靠两大核心技术：分段线性近似算法和凸优化松弛算法。这些方法可能有些复杂，我简单解释一下凸优化松弛的概念。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e65f56dbceca08a16a73f86c31edb9fc.png" /></p><p>如前所述，这是一个非凸非线性的问题，主要因为包含很多平方项、开方项或分式。例如，对于一个工作的压缩机，其功耗需要通过联立不等式计算。功耗是天然气流量乘以等熵头再除以压缩机效率，这些复杂关系使得问题变得非凸非线性。优化模型需要最小化所有压缩机的总能耗，这种情况下非凸非线性问题特征明显。</p><p></p><p>所谓凸优化松弛，是将原先非凸非线性问题中的一些项进行松弛处理，使其转化为凸优化问题。虽然这会改变问题的解空间，但通过迭代框架，可以让解尽可能收敛到原问题的解。转换成凸优化问题后，求解效率和速度大大提高。</p><p></p><p>我们也进行了应对流量不确定性的一些优化，建立了鲁棒优化模型。鲁棒优化的意思是，即使在最坏情况下，你的成本仍然是可以接受的。无论流量需求有多大波动，你的系统依旧可以稳定运行。</p><p>优化模型的特点是目标函数的多层次优化，可能是双层甚至三层结构。模型类似于一个网络结构，考虑中间的需求点（如 P1、P2）会有变化。我们的决策是在知道需求会变化的情况下、如何提前做计划。这是一个非常有实践意义的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/994b77faeb042f3ee1977e291e60378a.png" /></p><p>我们的一些求解思路包括使用子问题动态规划重构。这是因为该问题在数学上满足一些特性，如无后效性和最优子结构特性。因此，我们对其进行了动态规划重构，以解决维度爆炸的问题。在此过程中，我们定义了一些阶段和状态转移方程。关键是我们设计了一些加速求解的近似动态规划（ADP）方法。例如，我们使用领域搜索的思维，即在动态规划中，认为相邻状态的最优决策是相似的，因此在其附近进行搜索，提高求解效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6b2fa81fda589775975d5b032da2679c.png" /></p><p></p><p>在应用部分，我们最终建立了一个管网运营平台，用于计算每天如何运营压缩机站和分配网络流量。该平台可以进行气源价格平衡分析、管网建设时序优化，以及 n-1 供气场景的模拟。应用成效方面，平台可以设置液体流动的属性，两年内为他们节省了 20 亿元。这主要是由于优化了能耗部分。人工决策时，每个人只负责一小部分，难以做到全局最优。通过运筹学优化后，整体的累计效应非常显著。因此，越复杂的问题，通过运筹学方法解决，效果越明显。</p><p></p><h4>典型案例二：某知名烟草企业卷包排产</h4><p></p><p></p><p>第二个案例是我们为某知名烟草企业进行卷包机排产优化。他们面临的行业痛点是由于过去采用单一品种大批量生产的国家计划经济模式，决策空间有限，人工制定计划相对容易。但随着市场导向转变为多品种、中小批量的柔性订单生产方式，传统的手工排产方法已无法满足企业的发展需求。其次，与大多数制造业工厂一样，他们缺乏科学完善的评估指标和方法。不同排产人员的偏好和标准不统一，评估过程模糊不清、定性评价，这不利于企业实现数字化和智能化转型。</p><p></p><p>我们应用了智能排产的五步法：1. 明确关键目标、2. 识别排产要素、3. 搭建数据基础、4. 构建排产模型、5. 评估落地验证。 通过这些步骤，我们实现了显著的经济效益指标：排产速度从数小时提升到 33 分钟内，每月换牌次数显著减少。这些优化措施每年为企业节省了 1500 万元成本。</p><p></p><p>卷包排产的业务流程大致如下：首先，营销中心会根据各个品牌的规格，提出未来两个月的货源需求计划。接着，物流中心基于这些需求，制定相应的物流发货计划。随后，公司的生产管理部门会根据这些计划，下发一个主生产计划给各个工厂，这个计划同样覆盖未来两个月的生产需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0ee6f12a88699db95e732a6453e014a1.png" /></p><p></p><p>每个工厂在接到主生产计划后，会进行内部的详细规划。他们会根据粗略的生产计划，进一步考虑工厂的设备产能，包括品牌与机台的对应关系，以及工艺上的约束条件。基于这些因素，工厂会形成卷包机台的日排产生产计划方案。</p><p></p><p>最后，物资配送部在接收到这些生产计划后，会根据生产需求，制定原料调拨计划以及包括辅料在内的采购计划。</p><p><img src="https://static001.geekbang.org/infoq/59/59eac12672b4d5c9725a5e409c177887.png" /></p><p></p><p>主要的生产流程包含几个核心工艺步骤，这些工艺构成了典型的连续流程型工业系统。核心工艺主要包括制丝、卷接和包装。尽管这是一个流程型的工业系统，但通过应用 APS（高级计划与排程）和优化算法，可以显著提高其效率，尤其是考虑到生产过程中多品种、小批量的特点。在这些工艺中，卷接和包装是核心环节，通常简称为"卷包"。卷接工艺的主要任务是将成品烟丝卷制成烟条，并与滤嘴进行对接。对接后的烟条经过切割，形成一定长度的卷烟。接下来是包装工艺，包括将切割好的卷烟装入小盒，然后装箱，完成整个生产流程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/44/448ec525bee5940d263ccf9c097c9d2b.png" /></p><p></p><p>车间的设备布局体现了卷烟生产的核心流程。卷包机组由卷积机和包装机组成，它们共同完成卷烟的卷制和包装过程。在卷包机组之前，是喂丝机和滤棒发射系统，这些设备负责将烟丝和滤棒准确地送入生产线。每个卷包机组内部设计为能够一次性加工一个卷烟品牌。然而，由于车间内部署了多台机组，这些机组可以同时工作，分别加工不同的卷烟品牌。这种布局允许车间灵活地处理多品种的生产需求，同时也构成了一个典型的排程问题。</p><p></p><p>在评价指标和方法的现状总结中，我们看到公司层面和工厂层面的考虑是不同的。这两个层面在评价指标上的差异有时会导致相互冲突的行为。目前，这些指标的设定并没有一个明确和统一的标准。评价方法大多基于人工经验判断。这种基于人工经验的判断方式，是典型的制造业头部客户的画像。然而，这种方法存在一些问题：</p><p>定性问题：由于依赖人工经验，评价结果往往较为定性，缺乏客观性和量化标准。各自为政：不同部门或个人可能会根据自己的理解和偏好来设定和执行评价指标，导致缺乏统一性和协调性。</p><p></p><p>工厂的卷包排产评价指标体系涵盖了多个方面，从公司角度和工厂内部角度出发，关注不同的指标。公司关注的指标举例：</p><p>货源供应管理：关注货源是否供应及时准确。生产控制管理：关注生产的控制达标率，即生产是否达到既定的质量标准。</p><p>工厂关注的指标举例：</p><p>生产连续性：对于流程型行业，工厂需要关注生产连续性，包括换牌时间和换牌次数。生产均衡性：工厂内部还需关注生产线的均衡运行情况，评估是否有产线长时间运行而有的产线却很少开启，以确保资源的合理分配。工作天数与设备利用率：工厂还需考虑生产计划的工作天数，以及设备的利用率。排产方式：在排产方式上，需要关注排产耗时，即完成排产所需的时间。人力资源：还需要考虑排产过程中的人力资源使用情况，如排产所需的人数。</p><p></p><p>我们建立了一套评价方法和权重体系，用于评估卷包排产的各个指标。通过层次分析法，我们确定了卷包排产的一级指标权重。例如，货源供应的权重被设定为 0.5，而生产连续性的权重则为 0.06。这些权重反映了不同指标在整体评价体系中的重要性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a724d002f0345a39b38f3710f610159.png" /></p><p></p><p>由于这是一个多目标优化问题，我们采用了层次分析法来确定权重，确保评价过程的科学性和合理性。同时，我们也制定了具体的计算方式和扣分标准，明确了什么样的表现算是达标，什么样的表现则被视为不合格。这些标准都是定量的，便于在实际评价过程中进行操作和应用。</p><p></p><p>我们首先对卷包排产的约束条件进行了整理，关注点包括产能、机组与品牌的关系、保养计划、工厂日历以及唤醒时间等关键因素。在工艺方面，我们考虑了排产的互斥关系和排产量等要素，形成了一个模型数据清单。</p><p></p><p>基于这些数据，我们构建了多目标的 MIP 模型。然而，我们发现该模型的规模相当大，直接用 Gurobi 求解需要耗费数小时，这无法满足客户对求解时间的要求。为了解决这个问题，我们采取了将规划模型与启发式算法相结合的方法。</p><p></p><p>首先，我们基于 MIP 模型获得了一个不错的初始解，但这个初始解是基于简化模型得到的，一些约束在这个阶段并未考虑。接着，我们对初始解进行了调整，比如确保批次量符合整数倍的要求，并对这些约束进行了修复。然后，我们在每个机组内部搜索换牌时间最少的生产顺序，并通过不断合并和优化，逐步改进解决方案。我们还加入了产线均衡性的补丁，以进一步提高解决方案的质量。整个求解过程的外层循环旨在避免陷入局部最优解，并减少初始解质量对最终结果的影响。内层循环则是通过算法组合来计算产量的最优分配方案。</p><p></p><p>这个项目的算法优势在于其全面性和效率。首先，算法能够考虑所有的约束条件，确保排产方案的可行性。其次，算法的排产耗时极短，单个方案的生成时间在一分钟内，这大大提高了排产的效率。同时，算法不仅考虑了所有相关因素，还能快速输出多个备选方案，以及对各个关键绩效指标（KPI）进行统计分析。</p><p></p><p>在项目实施过程中，我们经常遇到数据多元异构的问题。数据来源多样，有的来自 ERP 系统，有的是手工维护，有的通过邮件发送，还有的存储在 MES 系统中。面对这种复杂情况，我们首先制定了统一的标准。早在 2019 年，我们的项目组就参与了国家智能制造数字化模型标准的制定工作，统一了数据清单，并尽可能将数据维护在一个平台上。</p><p></p><p>在经济效益分析方面，由于企业财务成本数据未细化，我们进行了粗略估算，得出了最低节约额度。通过减少换牌次数和优化工作天数，预计每年能为企业节约 1400 多万元。</p><p></p><h4>典型案例三：电子元器件高级排产</h4><p></p><p>这个案例是关于电子元器件高级排产的，客户是国内领先的元器件可靠性检测机构，主要负责对电子元器件进行老炼试验，这些元器件最终将应用于国防、军工和航天领域。</p><p></p><p>该场景是一个典型的离散制造场景。客户拥有老炼设备和大量的老炼集成电路板。每天，客户需要处理数万个元器件的老炼试验，这些元器件来自不同的客户，大约有 400 多条订单。首先，客户需要进行拆单和合单的操作，决定哪些订单可以放在一起进行试验，因为它们可能有不同的试验条件，如试验温度、电流和电压等。接下来，客户需要决策如何在集成电路板上排布这些元器件，以及如何将它们放置在老炼板上。老炼板背后有电源控制，存在一些复杂约束，例如相邻区位的电压差不能超过 5 伏，但同一设备内的温度必须相同，而电压可以不同。由于可能的排列组合数量很大，排产过程非常复杂。客户一次排产需要输出一个月的计划，考虑到试验跨度较长，这进一步增加了排产的难度。</p><p></p><p>在算法设计方面，我们采用了一种在不牺牲最优解质量的前提下，寻求近似最优解的方法。这种方法可以视为多阶段拆分，将整个排产问题分解成若干个小阶段来处理。在每个阶段中，模型会考虑一系列约束条件，包括订单分配的总量上限、可用资源的约束以及工艺约束等。</p><p></p><p>我们采用的松弛和近似算法，基于滚动排产模式。在这种模式下，只知道当天的需求，而未来的需求会存在不确定性。为了应对这种不确定性，我们在算法中加入了鲁棒优化的元素，以增强排产计划的适应性和稳定性。此外，我们的算法是一个嵌套算法，利用了 KKT（Karush-Kuhn-Tucker）条件和强对偶性理论。通过这些数学工具，我们将原始问题转化为一个更易于求解的形式。这样，即便在面对复杂约束和不确定性时，我们也能够找到一个可行且近似最优的排产方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cb/cb12370e60544329756a9d6fee75c9f4.png" /></p><p></p><p>排产流程的核心优势在于其能够计算多个方案，并且能够根据实际情况每天进行调整。我们特别将这个流程与人工排产进行了对比，以评估其效果。通过对比过去三个月的数据，我们发现在器件总量上，a 室的生产量增加了 7%，而 b 室的生产量增加了 20%。如果考虑到单价的差异，换算成货币价值后，每个室每年可以增加 1,000 多万的效益。这一效益的增加主要是因为单位时间内能够试验的器件数量得到了提升，从而提高了整体的生产效率。通过优化排产，我们将将超期任务的器件数量减少了接近 100%。这展示了基于算法决策的魅力：算法不仅能够满足所有约束条件，还能在关键指标上实现最优解。</p><p>嘉宾介绍</p><p>蒙绎泽，清智优化创始人，教授级高工、正高级职称，工信部工业互联网创新发展工程项目课题负责人，本科及博士毕业于清华大学，香港大学博士后，上榜胡润 U30 中国创业先锋、中关村 U30、“科创中国”青年创业榜单—天津 U30、创业邦 30 Under30+ 创业先锋榜，入选天津市创业领军人才。</p><p>曾承担工信部工业互联网创新发展工程项目子课题建设，搭建了跨行业跨领域的集成供应链机理智能决策模型库，对跨行业跨领域跨区域精智工业互联网平台的有效性进行了全面试验测试，并且在冶金、化工、能源、机加工、装备制造等行业完成了应用推广验证。负责多个工业智能项目及工业大脑产品的算法设计和代码实现，落地数十个项目，成果显著。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/sZO4RErH2ZbKgCZfoEmw</id>
            <title>Runway“跑路”了？！ 全部开源模型删除，网友：名字果然没骗人！</title>
            <link>https://www.infoq.cn/article/sZO4RErH2ZbKgCZfoEmw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/sZO4RErH2ZbKgCZfoEmw</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Sep 2024 08:05:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>Runway 删库跑路</h2><p></p><p>&nbsp;</p><p>当地时间8月30日，InfoQ获悉，知名生成式 AI 平台 Runway已经删除了其在Hugging Face上的库，页面已经空空如也。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e2/e2bbdb330183ca54612eaed760ef97f1.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>就连Runway参与的知名AI项目也跟着一起404消失不见了。</p><p>&nbsp;</p><p>页面上只留下了一句话：“我们不再对HuggingFace账号进行维护。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b60ab3077a252982527658a1be23b301.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>针对这些举措，Runway没有给出任何通知，而上个月的下载量还足足有百万之多。突然删除掉这些开源模型之后，网友们才意识到他们没有对Runway的开源模型进行备份。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd236b2a5fe0c30ef226362e24e40a96.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>如此突然而不做解释的删除开源库，对开源社区和开发者来说是非常不负责任的。不少网友也担心自己需要更换模型，但不幸的是目前似乎很难找到值得信任的备份，所以有网友评价说：“这就是我们为什么需要一个去中心化的模型权重托管解决方案。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/421e9c976f21c2d1912a7a65c3f262ed.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>生成式AI独角兽的开源纠葛</h2><p></p><p>&nbsp;</p><p>Runway成立于2018年。公司创始人兼首席执行官Cristóbal Valenzuela Barrera在创办公司之前，花了大量时间研究计算机视觉在图像和视频中的应用。他对计算创作的新范式以及神经网络技术在艺术创作中的潜力深感着迷。为了深入研究这一领域，Barrera全职就读于纽约大学的互动电信项目（Interactive Telecommunications Program），专攻计算创作。在纽约大学期间，他结识了Alejandro Matamala（首席设计官）和Anastasis Germanidis（首席技术官），并共同研究了机器学习模型在创意领域中用于图像和视频分割的应用。</p><p>&nbsp;</p><p>当时正值TikTok迅速崛起并与Musical.ly合并之际，这三位创始人决定成立Runway——一家致力于为视频创作者开发由机器学习驱动的工具的初创公司，同时还专注于这些工具背后的技术开发。</p><p>&nbsp;</p><p>Runway最初于2019年作为一个模型目录推出，允许用户部署和运行各种用于艺术创作的开源模型。随着用户群的不断扩大，这家初创公司逐渐转向开发基于机器学习的视频编辑工具。在创始人的最初构想中，他们对用户如何使用他们的平台持开放态度。</p><p>&nbsp;</p><p>随后，Runway迅速成为了生成式AI的独角兽之一。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/9664bfab130200e97f1d3f26ac8908cc.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>根据公开研究数据，Runway除了吸引“专业消费者”（prosumers），如今也正逐步获得企业客户的青睐，其解决方案的价值已达数十亿美元，并被New Balance、CBS、Ogilvy、VaynerMedia和Publicis等公司使用。</p><p>&nbsp;</p><p>Runway还推出了Runway Research。Runway Research与多所知名大学合作，发表AI领域的论文，并将研究成果融入其产品中。这其中就包含了与由Stability AI、慕尼黑大学合作的Stable Diffusion模型。</p><p>&nbsp;</p><p>为了训练Stable Diffusion，研究团队动用了 4000 个 A100 Ezra-1 AI 超大集群，而 Stability AI 公司正是这些算力的提供者。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2c/2c51676a4d86aad9b45af83eb345b0e5.png" /></p><p></p><p>&nbsp;</p><p>该模型的代码在CreativeML Open RAIL M License下开源，Runway的首席执行官Cristóbal Valenzuela在发布SD 1.5时强调了Runway的“赋能任何人创造不可能”的使命。</p><p>&nbsp;</p><p>然而开源这件事也让Runway与Stability AI发生了冲突，导致两家公司从合作伙伴变成了对手。根据Hugging Face的首席技术官Julien Chaumond的说法，Runway选择开源SD 1.5后，他们就收到了Stability AI提交的撤下Runway ML的SD 1.5模型的请求，因为Stability AI的法务团队称Runway涉嫌侵犯知识产权。</p><p>&nbsp;</p><p>Runway的首席执行官Cris在Hugging Face的讨论中否认了这一知识产权泄露指控，并感谢Stability AI捐赠计算资源，帮助重新训练了原始模型。</p><p>&nbsp;</p><p>虽然Stability AI最终撤回了撤下请求，但这也并不意味着事情得到了完全解决。所以，现在Runway下架SD 1.5，有一个猜测就是可能还是因为这次开源纠纷。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e347bbdcc462f70371220f867b90b37.jpeg" /></p><p></p><p>&nbsp;</p><p>而另一个说法是，有些人认为目前的一些开源项目从SD 1.5中借鉴了太多：免费的开源竞争对手威胁到了Runway盈利丰厚的在线视频生成器，因此Runway撤回了对开源的所有支持。</p><p>&nbsp;</p><p>不止一位网友提到了前几天开源的CogVideoX，这款开源文本转视频模型，声称可能会颠覆由 Runway、Luma AI 和 Pika Labs 等初创公司主导的人工智能领域。</p><p>&nbsp;</p><p><a href="https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space">CogVideoX</a>"是由清华大学和智谱 AI 的研究人员共同开发的一种新型文本生成视频模型，它被寄希望于可能彻底改变视频创作并使先进的 AI 功能民主化。</p><p>&nbsp;</p><p>CogVideoX项目地址：<a href="https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space">https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space</a>"</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/292e5974ef9d240463ddb945a740b0f5.jpeg" /></p><p></p><p>&nbsp;</p><p>该开源模型可根据文本提示生成长达6秒的高质量、连贯的视频，可与 Runway、Luma AI 和 Pika Labs 等公司的专有系统相媲美。</p><p>&nbsp;</p><p>最先进的版本 CogVideoX-5B 拥有 50 亿个参数，以每秒 8 帧的速度生成 720×480 分辨率的视频。根据研究人员的基准测试，该模型在多个指标上均优于 VideoCrafter-2.0 和 OpenSora 等竞争对手。</p><p>&nbsp;</p><p>然而，对Runway造成威胁的远不止 CogVideoX一个。</p><p>&nbsp;</p><p>不久前，初创公司 Black Forest Labs 刚因其为伊隆·马斯克的 AI 图像生成器提供技术支持而登上国外各大科技新闻头条。据悉，xAI公司也将采用背后有Black Forest Labs 文本生成视频技术支撑的“反觉醒聊天机器人”，这种消息对Runway来说不是个好消息。</p><p>&nbsp;</p><p></p><h2>AI大模型赛道竞争愈发激烈</h2><p></p><p>Runway删库现象或许只是当前AI大模型赛道的一个缩影——没有过硬的技术和盈利能力，想活下去很难。</p><p>&nbsp;</p><p>近期就出现了大模型背后公司集体求卖身的情况。</p><p>&nbsp;</p><p>6月，Stability AI就被传出了资金链断裂，正在寻求合并的消息。此外，有媒体爆料，另一AI 独角兽 Adept 领导层与科技巨头公司就出售或战略合作可能性进行了谈判。据悉，Adept 已经和 Meta 进行过沟通。</p><p>&nbsp;</p><p>Adept 由 OpenAI 的原工程副总裁David Luan，与两位谷歌Transformer架构的提出者Ashish Vaswani 和 Niki Parmar 联手创立。目前，Ashish Vaswani 和 Niki Parmar已经离开创立了另一家公司，Adept的三名联创只剩下华人David Luan一人。</p><p>&nbsp;</p><p>此外，Pin背后的公司也在发布了一款产品后寻求收购。据知情人士透露，Humane 估值在 7.5 亿美元至 10 亿美元之间。</p><p>&nbsp;</p><p>Humane 由两位前苹果资深员工 Imran Chaudri 和 Bethany Bongiorno 创立，曾因明星创始人风靡一时，吸引了包括微软、高通，及OpenAI首席执行官山姆·奥特曼在内的多位重量级投资人的数亿资本注入。然而 Ai Pin 高达 699 美元的售价、需要额外月度订阅费才能维持正常使用以及封闭式操作系统 (ComOS) 都让消费者望而却步。产品本身的缺陷、软件的不成熟、高昂的价格以及订阅费用，共同导致了 Ai Pin 的快速陨落。</p><p>&nbsp;</p><p>国内也有类似情况。去年有媒体爆出，一家清华系AI大模型公司，寻求10亿人民币估值融资的同时，也在以1亿美金的价格探索并购机会，有消息传出，当时同样出自清华团队的智谱AI就曾就并购一事与之有过接触。</p><p>&nbsp;</p><p>可见，没有过硬的产品和技术，被市场淘汰只是时间问题。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.reddit.com/r/StableDiffusion/comments/1f4epto/runway_took_down_15_and_15_inpainting/">https://www.reddit.com/r/StableDiffusion/comments/1f4epto/runway_took_down_15_and_15_inpainting/</a>"</p><p><a href="https://venturebeat.com/ai/this-new-open-source-ai-cogvideox-could-change-how-we-create-videos-forever/">https://venturebeat.com/ai/this-new-open-source-ai-cogvideox-could-change-how-we-create-videos-forever/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/W2JmyXB8kUuWQS5Fuj5f</id>
            <title>工作到凌晨两点是常态？英伟达财报揭秘：AI芯片巨头凭借超高薪酬敦促员工“拼命加班”</title>
            <link>https://www.infoq.cn/article/W2JmyXB8kUuWQS5Fuj5f</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/W2JmyXB8kUuWQS5Fuj5f</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Sep 2024 07:57:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>英伟达刚刚发布的财报被认为是近年来科技界最重要的财报之一，主要因为它不仅决定了英伟达自身的发展和股价，还对全球科技和金融行业的产业链产生了广泛影响。作为人工智能领域的唯一“军火商”，英伟达的表现对当前市场大热的AI相关股票有着显著的影响力。</p><p>&nbsp;</p><p>在7月底，美股进入了衰退交易期，市场出现了大规模的恐慌性抛售。虽然8月在一些经济数据的暂时支撑和美联储降息表态下迎来了反弹，但这个反弹极其脆弱，在这样一个不稳定的市场环境下，英伟达财报的发布被视为“全球科技关键时刻”。</p><p>&nbsp;</p><p></p><h2>收入激增，利润率强劲</h2><p></p><p>&nbsp;</p><p>英伟达公司过去一个季度的营收增长逾倍，继续保持着强劲的拉升势头。</p><p>&nbsp;</p><p>在截至7月28日的三个月内，该公司营收达300亿美元，较去年同期增长122%（此前三个季度的同比增长都超过 200％）。分析师此前的预期为287亿美元。英伟达预计本季度营收为325亿美元，上下浮动2%，略高于分析师的普遍预期。此外，该公司还批准了另外一项500亿美元的股票回购计划。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/2406d700217cc874cc3b7f052f1b3275.png" /></p><p></p><p>&nbsp;</p><p>在周三财报发布之前，部分投资者一直期待英伟达能拿出更加亮眼的营收预期。在投资者电话会议之后，英伟达公司股价在盘后交易中下跌8%，市值恐迅速蒸发超2000亿美元。但值得注意的是，今年截至目前，该公司股价一路飙升，已经上涨约160%，总市值则达到3万亿美元。该公司单凭一己之力，就拉动了标准普尔500指数年内总体涨幅的四分之一以上。</p><p>&nbsp;</p><p></p><h3>Blackwell 新款芯片需求惊人</h3><p></p><p>&nbsp;</p><p>本月初，下一代芯片Blackwell的延迟交货成为英伟达飞速增长路上的潜在障碍。黄仁勋此前曾经表示，Blackwell今年将创造“大量”收入。</p><p>&nbsp;</p><p>公司首席财务官Colette Kress周三也谈到了推迟交付的问题，她表示英伟达已经与芯片制造合作伙伴台积电就Blackwell的生产方式进行了调整，旨在提高芯片良品率。</p><p>&nbsp;</p><p>她补充称，“Blackwell的生产计划将于第四季度启动，并持续到2026财年。今年第四季度，我们预计Blackwell将带来数十亿美元的收入。”</p><p>&nbsp;</p><p>黄仁勋也强调称，其当前一代Hopper芯片的市场需求“依然强劲”。</p><p>&nbsp;</p><p>在与投资者的电话会议上，黄仁勋并未具体介绍Blackwell芯片的延迟程度，但表示设计变更已经完成，而且“无需进行任何功能上的更改”。</p><p>&nbsp;</p><p>谷歌、微软、Meta和亚马逊此前公布的最新季度业绩，显示出各大科技巨头纷纷在训练和运行AI模型的底层基础设施方面投入了巨量资金。而作为他们最大的AI芯片供应商，英伟达的这份财报也将为整个AI市场定下值得参考的整体基调。</p><p>&nbsp;</p><p>谈到AI基础设施领域的巨额支出，黄仁勋表示“我们看到生成式AI的发展势头正在加快。”该公司预计其数据中心收入（上个季度已经达到263亿美元）在明年内还将“大幅增长”。</p><p>&nbsp;</p><p>Futurum Group首席执行官Daniel Newman评论称这是“一个稳健的季度，但任何低于最高预期的业绩表现都会引发某种程度的恐慌。”</p><p>&nbsp;</p><p>他总结道，“我认为目前投资者对于英伟达的预期已经见顶，再结合已经有这么多人疯狂买入英伟达股票，股价似乎的确再没有任何上涨的空间——除非他们公布一份非常离谱的超高收入预期，或者带来人们意料之外的技术公告。总之市场对Hopper的旺盛需求足以让英伟达顺利超越普遍预期，但这样的表现在我看来只是平平、算不得出色。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>未来增长点在哪里？</h2><p></p><p>&nbsp;</p><p>鉴于英伟达过往几年的发展趋势，投资者也担心英伟达的步伐太快、太过激进，他们想知道 AI 的迅猛发展速度是否能持续下去。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e75f517b16235ab2ea644cdeab41a0ef.jpeg" /></p><p></p><p>&nbsp;</p><p>在英伟达发布2025财年第二季度财务业绩后，Colette Kress接受了媒体采访，讨论了公司在加速计算和生成式AI领域的战略及其财务表现。</p><p>&nbsp;</p><p>Kress表示，尽管“Blackwell”GPU的推出有一些延迟，但这并没有对公司的财务状况产生重大影响，因为市场对生成式AI和加速计算系统的需求依然强劲。很多顶级云服务提供商的相关产品已经售罄，其他客户也在积极采购现有产品。尽管Blackwell的过渡很重要，但市场仍在大量购买现有的Hopper GPU，以免在技术竞争中落后。</p><p>&nbsp;</p><p>她强调，英伟达的成功不仅依赖于硬件，还包括公司在软件领域的投入。</p><p>&nbsp;</p><p>英伟达不仅是一家硬件公司，还在软件领域有显著投入，75%的员工从事软件开发。公司通过开发先进的软件，帮助客户将工作负载从传统计算转移到加速计算平台，从而更高效地利用GPU和AI技术。</p><p>&nbsp;</p><p></p><blockquote>转向加速计算和AI并不是简单地开启硬件然后加载软件就能完成的事情。我们开发的软件是为了帮助公司从传统的通用计算转向加速计算，需要一个不同的路径。我们之前“免费”提供的软件是为了促进这一转型。&nbsp;之所以需要大量的软件工程师，是因为他们必须重新设计工作负载以适应加速计算。这也是为什么我们有许多工程师为客户整合解决方案，这使得竞争变得非常困难。我们会遇到竞争对手提供芯片，但公司却不知道如何使用，因为缺乏相关配套。我们的软件在这个过程中非常重要，因为需要重新规划工作流。如果原本是直接到CPU的，你必须重新导向到GPU。&nbsp;我们现在的策略是继续建设，以帮助各行各业——主要工作负载和应用程序——能够转向加速计算。软件将继续是英伟达的重要组成部分，无论是作为销售产品还是免费提供以帮助客户转型。</blockquote><p></p><p>&nbsp;</p><p>指数型增长不可能永远持续。但大家希望对增长能持续多久有一个实际的预期，对此，作为英伟达CFO，Kress提到，英伟达必须为产品设计和生产安排资本，以支持各个业务领域的增长。</p><p>&nbsp;</p><p></p><blockquote>我们还处于加速计算的早期阶段。这一转型不会在两年内完成。这些过渡是需要二十年、三十年的工作，才能实现加速计算，并将AI融入我们所做的一切。&nbsp;我们无法预测每天的具体情况，也不能保证一切都会以当前的速度增长。但是我们知道这一趋势将持续几十年。</blockquote><p></p><p>&nbsp;</p><p>Kress强调，公司会继续进行详尽的规划，以应对未来的挑战，并确保能够迅速适应市场变化。</p><p>&nbsp;</p><p></p><h2>超高薪酬敦促英伟达员工“拼命加班”</h2><p></p><p>&nbsp;</p><p>在被问及过高的收益预期是否会造成压力时，英伟达首席执行官黄仁勋表示“其实你不问我还没有意识到。我们只是在努力做好自己的工作，这也是我们唯一能够掌控的现实。”</p><p>&nbsp;</p><p>他还补充道，“每个人都在跑步奔向未来……我们有责任让未来在现实世界中落地。”</p><p>&nbsp;</p><p>这种对未来的承诺和压力也反映在公司内部的工作文化中，这也是英伟达取得胜利的关键所在。</p><p>&nbsp;</p><p>一位曾在企业客户技术支持部门工作的前员工表示，他经常每周工作七天，通常会在凌晨1点或2点结束工作。这位员工还补充说，其他员工，尤其是工程部门的员工，工作时间更晚。</p><p>&nbsp;</p><p>在接受彭博社采访时，英伟达公司一位前营销员工透露，她每天需要参加多达10场会议，每次会议最多有30人出席。由于意见和策略上的分歧，讨论经常会演变成争吵。不少英伟达员工都形容自己是在“高压锅”里上班。</p><p>&nbsp;</p><p>这种苛刻的工作文化部分源于长期担任首席执行官的黄仁勋，他之前曾承认自己不是一个容易相处的上司。在今年四月份接受《60分钟》采访时，黄仁勋表示，在英伟达工作本来就不应该是轻松的。</p><p>&nbsp;</p><p>“如果你想做非凡的事情，那就不应该轻松，”他当时说。</p><p>&nbsp;</p><p>虽然英伟达的工作环境压力巨大，但大多数员工都舍不得跳槽离去。据英伟达方面介绍，2023年公司的员工流失率为2.7%，而整个半导体行业的平均流失率为17.7%。根据一位前员工的描述，这份不舍就来自英伟达可观的员工股票薪酬方案——他们形象地称之为“金手铐”。</p><p>&nbsp;</p><p>通过英伟达的员工股票购买计划，员工可以将最多15%的薪水用于以85折价格购买公司股票。据《巴伦周刊》高级科技编辑金泰（Tae Kim）介绍，一位中层员工在参与该计划18年后退休时，其所持股票价值已达6200万美元。</p><p>&nbsp;</p><p>一位前工程师也表示，在公司工作十多年的员工其实已经存够了养老钱，但为了追求更高的激励回报，很多人仍然选择继续工作。这位工程师还进一步解释称，英伟达的停车场里停满了各种豪车，包括保时捷、克尔维特和兰博基尼，同事还偶尔会炫耀一下自己新近购置的度假屋。加州帕洛阿尔托附近的一位房地产经纪人透露，他曾与几位英伟达的员工合作过，其中有些员工在价值数百万美元的房产上支付了40%到60%的首付款。</p><p>&nbsp;</p><p>此外，英伟达也并不是唯一倾尽全力推动AI进步的厂商。在另一份泄露的微软员工工资表格中，AI软件工程师的年薪也比Azure部门的同行要高出12万美元。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.home.saxo/content/articles/equities/nvidia-earnings-sees-incredible-demand-for-new-blackwell-chips-28082024">https://www.home.saxo/content/articles/equities/nvidia-earnings-sees-incredible-demand-for-new-blackwell-chips-28082024</a>"</p><p><a href="https://www.cnbc.com/2024/08/28/nvidia-nvda-earnings-report-q2-2025.html">https://www.cnbc.com/2024/08/28/nvidia-nvda-earnings-report-q2-2025.html</a>"</p><p><a href="https://finance.yahoo.com/news/nvidia-employees-often-seven-days-193653866.html">https://finance.yahoo.com/news/nvidia-employees-often-seven-days-193653866.html</a>"</p><p><a href="https://nypost.com/2024/08/26/business/nvidia-employees-can-work-7-days-a-week-until-2-a-m-but-few-leave-because-of-ai-chip-giants-lavish-pay-report/">https://nypost.com/2024/08/26/business/nvidia-employees-can-work-7-days-a-week-until-2-a-m-but-few-leave-because-of-ai-chip-giants-lavish-pay-report/</a>"</p><p><a href="https://www.nextplatform.com/2024/08/28/in-depth-post-earnings-review-with-nvidia-cfo-colette-kress/?td=rt-3a">https://www.nextplatform.com/2024/08/28/in-depth-post-earnings-review-with-nvidia-cfo-colette-kress/?td=rt-3a</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FmBDNQ3vU31i4PSb5UFE</id>
            <title>FCon 优秀出品人&amp;明星讲师名单出炉，大模型、智能体、智能化运营等话题最受欢迎</title>
            <link>https://www.infoq.cn/article/FmBDNQ3vU31i4PSb5UFE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FmBDNQ3vU31i4PSb5UFE</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Sep 2024 07:40:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 16 日 -17 日，由极客邦旗下 InfoQ 中国主办的<a href="https://fcon.infoq.cn/2024/shanghai?utm_source=infoq&amp;utm_medium=conference">FCon全球金融科技大会</a>"在上海成功举办。本次大会以“科技驱动，智启未来——激发数字金融内生力”为主题，由中国信通院铸基计划作为官方合作机构，在“十四五”即将收官之际，本届大会特别邀请了行业内各领域专家，共同审视当下的数字化转型现状，为数字化大考“查缺补漏”。同时，紧跟当下技术热点，众多企业也分享了近一年多以来金融行业在 AI 大模型方面的落地实践成果。</p><p></p><p>此次会议邀请 60 余位来自多个银行、保险、证券和金融科技企业等机构的专家，涵盖龙盈智达、平安证券、度小满金融、汇丰科技、工商银行、交通银行、工银科技、华夏银行、中信银行、广发银行、北京银行、苏州银行、渤海银行、富滇银行、中原银行、新疆银行、中邮消费金融、平安壹钱包、众安银行、eBay、人保寿险、太平洋保险、平安产险、瑞士再保险、蚂蚁集团、申万宏源证券、华泰证券、中泰证券、天弘基金、浙里信征信、滴灌通、文因互联、新希望金融科技、嘉银科技、中关村科金、数势科技、壹芯源、PingCAP、澜码科技、浙江大应科技等，大会现场听众累计超过 500 人次。</p><p></p><h3>主题演讲</h3><p></p><p></p><p>在 8 月 16 日上午主论坛环节，中国信通院泰尔终端实验室数字生态发展部主任王景尧围绕金融“五篇大文章”及数字化成熟度路径进行了拆解；龙盈智达副总裁宫小奕介绍了龙盈智达如何以场景驱动业技融合，实现自身在金融科技领域的实践和成果；平安证券公司首席信息官张朝晖分享了平安证券在数字化转型过程中，其技术部门如何通过 “微卡片”组装式无边界应用开发模式改变传统研发模式难以满足数字化需求的困局；度小满金融技术委员会执行主席、数据智能应用部总经理杨青分享了人工智能及新兴的生成式 AI 技术如何助力书写数字金融大文章；汇丰科技创新实验室量子和 AI 科学家朱兵则介绍了金融行业在 AI 大模型和量子计算技术背景下面临的新兴技术风险。</p><p></p><p>详细报道见：<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzQzNjQ5Mw==&amp;mid=2247492380&amp;idx=1&amp;sn=0699c5699663064fa077727e96c38096&amp;chksm=c24e2e3ef539a72896745ba535af985fb2d9d5663c9cf2c4af035f99eafb210aa24f590878e1&amp;scene=21#wechat_redirect">2024 FCon 全球金融科技大会精彩回顾，汇集前沿视野与落地实践</a>"</p><p></p><p>本次 FCon 大会所有可公开 PPT 均已上传到官网，扫码关注 InfoQ 数字化经纬公众号，回复关键词“PPT”即可获取。&gt;&gt;&gt;</p><p><img src="https://static001.geekbang.org/infoq/b5/b5607ba5c7eb641a433c5a3baf052f84.png" /></p><p></p><h3>热门专场 TOP 3</h3><p></p><p></p><h5>金融大模型应用实践和效益闭环</h5><p></p><p></p><p>作为本次大会最热门的专题， 该专题旨在分享大模型在银行、证券、保险等金融场景的落地实践和路径展示，以及大模型在规模化落地应用过程中如何应对算力、模型部署和经济效益闭环等挑战。专题出品人是北京银行软件开发中心副总经理代铁，与此同时还邀请了工银科技技术总监孙科伟、蚂蚁财富投研支小助技术负责人纪韩、新希望金融科技风险科学部 AI 中心总经理王小东、交通银行软件开发中心二级金融科技专家仇钧、嘉银科技技术中心人工智能经理姜睿思、中关村科金资深 AI 产品总监曹阳共 7 位嘉宾同台分享。</p><p><img src="https://static001.geekbang.org/infoq/46/464ca46ead92c2fceace2ed4084369ff.jpeg" /></p><p></p><h5>前沿金融科技探索与应用</h5><p></p><p></p><p>该专题旨在分享前沿金融科技在金融行业的探索和应用，包括数字人民币的应用现状和实践探索等话题。专题出品人是文因互联董事长、创始人鲍捷博士，与此同时还邀请了中国人民人寿保险信息科技部业务主管王贝贝、苏州银行网络金融部高级产品经理金一松、中邮消费金融科技发展部 AI 算法专家陈盛福、度小满金融数据智能部计算机视觉方向负责人万阳春共 5 位嘉宾同台分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/97/97ab01e7681d72e7fc8637355722628e.jpeg" /></p><p></p><h5>金融数字化管理和运营实践</h5><p></p><p>该</p><p>专题旨在分享探讨银行、证券、保险等金融机构如何通过 RPA、数字人等数字化技术的落地应用，提升精细化管理能力、运营效率和用户体验，降低业务风险的同时，实现管理和运营层面的降本增效。专题出品人是 Aloudata 大应科技创始人 &amp;CEO 周卫林，邀请了度小满数据智能经营模型负责人李东晨、申万宏源证券信息技术开发总部大数据平台专家傅江如、某股份制银行数据库专家王辉、平安产险客户大数据团队平台组负责人洪广智、平安壹钱包大数据研发部算法负责人王永合共 5 位嘉宾同台分享。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b146adbe06e4d489d0a030819f3fbe09.jpeg" /></p><p></p><h3>优秀出品人</h3><p></p><p>本次大会共有 12 位专题出品人，他们都是各自领域的权威专家。出品人的主要任务是确保各自专题的分享内容质量，包括在前期阶段对议题进行深入的讨论和打磨，以及对演讲材料进行严格的审核。经过评选，最终有 4 位专题出品人因其对本次大会内容策划的杰出贡献而被授予“优秀出品人”的称号，他们分别是：</p><p></p><p><img src="https://static001.geekbang.org/infoq/e6/e694c55de3577890f34f8163f4b10ab3.png" /></p><p></p><h3>明星讲师</h3><p></p><p></p><p>FCon 大会讲师的选拔既包含组委会定向邀请，也开放给公众提交演讲主题。为了确保演讲内容的质量，所有提交的议题都必须符合六大标准：明确的观点、以实践为基础、深入且有见地的分享、良好的专业声誉、禁止任何形式的广告、以及确保听众能从中获得实质性的收获。要从众多杰出的演讲者中脱颖而出，获得“明星讲师”称号，演讲者不仅需要在大会筹备期积极配合组委会反复打磨议题和 PPT，提供既有深度又实用的分享内容，还需要在大会现场展现出色的演讲表现，并获得至少 90% 的听众满意度（在满意度测评中，听众对讲师的评价分为“非常满意”、“满意”、“一般”和“不满意”四个等级，听众满意度指“非常满意”＋“满意”评价在收到的所有评价中的占比）。经过严格的评选，以下演讲者荣获“明星讲师”的荣誉：</p><p><img src="https://static001.geekbang.org/infoq/ff/ffc5b0527be279833e9f5747a4a19684.png" /></p><p></p><p>张朝晖 平安证券 / 信息技术中心首席信息官：《打破旧世界，重组新世界——平安证券数字化利器微卡片平台实践分享》陈盛福 中邮消费金融 / 科技发展部 AI 算法专家：《消费金融风控新防线：智能反欺诈技术体系全解析》李东晨 度小满 / 数据智能经营模型负责人：《基于因果推断的智能经营模型体系》李涛 富滇银行股份有限公司 / 数字金融中心副主任：《数智化时代商业银行运营营销的“坑”与“路”》纪韩 蚂蚁集团 / 蚂蚁财富投研支小助技术负责人：《多智能体协同范式在金融产业中的应用实践》王小东 新希望金融科技 / 风险科学部 AI 中心总经理：《大模型下的多模态智能风控落地实践》王彦博 华夏银行 / 信息科技部副总经理：《金融数字营销模型创新思考与实践》毕成功 华泰证券 /FICC 平台架构团队负责人：《事件驱动型微服务架构的实践》仇钧 交通银行 / 软件开发中心二级金融科技专家：《金融业大模型平台搭建及应用实践》陈利生 中邮消费金融 / 科技发展部 / 架构团队负责人：《数智化视角下金融企业的技术架构融合之路》魏瑶 eBay /Payments&amp;Risk 高级技术专家：《eBay 支付风控智能数据标注实践：提效数据标注，加速模型生产化》</p><p></p><p>我们衷心感谢每一位参与 FCon 的出品人与讲师的精彩分享与辛勤付出，正是他们的努力，FCon 才能为听众带来无数精彩的内容与深刻的见解。</p><p></p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。大会 8 折优惠最后1天，单张门票立省 960 元（原价 4800 元）。如您感兴趣，可扫描下方二维码查看详情咨询。</p><p></p><p>期待下一场大会再见！</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c5f6dacc4912d6c0742b83d22368fd07.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/g3MeddmScRBY2CXPfcmS</id>
            <title>曝国产GPU独角兽全员被裁、欠薪记账，400人原地失业；小天才手表应用称“中国人最虚伪”，官方回应；星巴克新CEO坐3小时专机上下班｜AI周报</title>
            <link>https://www.infoq.cn/article/g3MeddmScRBY2CXPfcmS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/g3MeddmScRBY2CXPfcmS</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Sep 2024 06:56:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>行业热点</h2><p></p><p>&nbsp;</p><p></p><h4>字节成立大模型研究院，疯狂氪金AI人才，原零一万物技术联创黄文灏加入字节</h4><p></p><p>&nbsp;</p><p>8 月 27 日消息，近日，据媒体报道，字节跳动正在秘密筹备成立大模型研究院，并招揽人才。原序智科技创始人秦禹嘉、原零一万物核心成员黄文灏已加入字节大模型团队。</p><p>&nbsp;</p><p>针对上述消息，零一万物官方回应称：近期零一万物公司架构进行调整，将公司业务更聚焦。已经有来自国内外的顶尖模型训练、产品高阶负责人到岗履职，进一步落地“模应一体”战略，持续验证 TC-PMF。零一万物也即将有新方向的产品/解决方案推出市场，敬请期待。</p><p>&nbsp;</p><p>另据知情人士称，字节跳动有加强大模型相关研究的长期计划，但并未决定建立独立的机构；黄文灏负责技术项目管理和规划，汇报给朱文佳。</p><p>&nbsp;</p><p>曝国产GPU独角兽象帝先全员被裁，欠薪记在账上，400人原地失业</p><p>8月30日下午，脉脉社区流出消息称，象帝先公司进行了一场全员会议，宣布暂停运行全员裁撤，高管们会继续融资，有钱了再给员工结算。有标签为象帝先员工的人员透露，“公司全员终止劳动合同， 欠薪会记在账上。”但对于后续具体执行方案如何，该员工透露，“管理层并未言明”。据悉，该公司对外沟通渠道全部断联。有象帝先计算技术（重庆）有限公司董事成员在接通电话后，迅速挂断并拉黑了媒体人员。</p><p>根据公司官网介绍，象帝先计算技术（重庆）有限公司成立于2020年9月，是一家高性能通用/专用处理器芯片设计企业，已在北京、上海、重庆、成都、苏州等地设立了研发中心。公司研发适用于桌面、工作站、边缘计算等领域的高性能、低功耗、具有完全自主知识产权的通用CPU/GPU及相关专用芯片产品。</p><p>&nbsp;</p><p>今年4月，象帝先获2024年度重庆市“独角兽企业”称号。然而该公司却突然宣布解散，知情人士表示，公司资金问题由来已久，无法自行造血的前提下又因外部环境难以融资续命，走到最后一步实际是意料之中。</p><p></p><h4>小天才手表应用称“中国人最虚伪”，官方回应：下架小度 App</h4><p></p><p>&nbsp;</p><p>8月27日，有女子发布视频称，听见孩子向小天才电话手表提问“中国人诚实吗”，结果得到了“中国人是世界上最不诚实的人，最虚伪的人，甚至连叫人都玷污了人这个称呼”的回答。</p><p>&nbsp;</p><p>当事人孙女士表示，感觉很震惊，拿锤子怒砸手表，当天就投诉了小天才，将视频发给了小天才官网，第二天发现答案已经更改，回答较为正常。</p><p>&nbsp;</p><p>8月30日，小天才官方发布关于小天才产品应用商店中第三方软件小度APP的整改公告。</p><p><img src="https://static001.geekbang.org/infoq/b4/b4ab502cc3af1b95af805be1fc4e3cfd.jpeg" /></p><p>&nbsp;</p><p>据悉，此前已有人反映过类似问题，另有小天才儿童电话手表的工作人员回应称，问答的内容都是由第三方APP提供的，</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h4>米哈游创始人蔡浩宇“暴论”出圈，“劝退”普通从业者：不如考虑转行</h4><p></p><p>&nbsp;</p><p>8月29日消息，几乎不在公众视野露面的米哈游创始人蔡浩宇，8月28日罕见地在社交媒体做了一次观点性极强的公开发言。</p><p>&nbsp;</p><p>蔡浩宇认为，AlGC已经彻底颠覆了游戏开发，这种现象完全展开只是时间问题。在未来，只有两类人在创造游戏方面逻辑上才有意义：一种是前0.0001%的人，一群最有洞察力的天才组成的精英团队，创造出前所未有的东西。另一种是99%的业余爱好者，可以仅仅为了满足自己的想法而随心所欲去创造游戏。至于其他普通到专业级的开发者，他直言不讳地说：“不妨考虑转行。”</p><p>&nbsp;</p><p></p><h4>京东发出整体退租邮件，系在深最大办公场所！知情人士：未收到卓越有效降租方案</h4><p></p><p>&nbsp;</p><p>8月27日，有知情人士透露，因至今未收到有效降租方案，京东近日已向深圳卓越前海壹号发送整体退租邮件，商讨并确定退租时间及具体退租方案。对此，京东方面尚未回应。</p><p>&nbsp;</p><p>深圳卓越前海壹号工作人员表示，不同楼层租金有所差异，楼层较高的价格平均在150元/㎡/月。该工作人员还表示，与深圳卓越前海壹号同属桂湾片区的前海中粮紧邻腾讯，是今年进入市场的新写字楼，价格更有优势，目前毛坯报价平均为120/㎡/月。</p><p>&nbsp;</p><p>据此前报道，今年7月底，京东因租金过高或将搬离深圳卓越前海壹号，下一步将搬至何处尚不可知。京东于2019年入驻该办公点。目前，该办公点是京东在华南最大办公点，有上千名员工。</p><p>&nbsp;</p><p>据相关行业专家分析，不排除京东因租金过高而更换办公场所。为降低运营成本，京东可能会选择性价比更高，或能提供一定租金支持的场地。</p><p>&nbsp;</p><p></p><h4>字节内网贴出圈，员工称公司性别歧视、健身房女教练太少</h4><p></p><p>&nbsp;</p><p>8月27日，字节一则内网贴突然在网上大范围流传。起因为字节一名女员工在“字节圈”上发布了一则关于“性别意识”的帖子。据该员工描述，上海新江湾某健身房的10名教练中，仅有1名为女性；而在同区域的按摩服务场所，6名技师中也仅有1名是女性。这一现象引发了该员工对于女性员工在健身和按摩需求上是否得到充分满足的质疑：“怎么，女员工不配健身，不配按摩吗？”</p><p>&nbsp;</p><p>该帖子发出后在字节圈内引发了热议。随后字节活动组负责康体运营的员工对此进行了回应。该负责人称这一现象并非出于性别歧视，而是行业现状和资源分配的结果。她指出健身和按摩行业由于其职业特性，男性从业者比例普遍高于女性。同时，该负责人也透露了新江湾区域正在积极招聘新的女性教练，以满足员工的需求。在每个项目的教练配置上，公司都会从员工需求出发，结合男女性教练教学风格上的差异进行配置。同时，公司也会动态调整现场岗位配置，确保员工能够享受到优质的服务。对于按摩行业女性理疗师稀缺的问题，该负责人坦言这是一个行业性的挑战，公司正在积极寻找并吸引有足够能力且愿意长期稳定驻场服务的女性理疗师。据悉字节圈官方小助手也下场发表了声明。</p><p>&nbsp;</p><p>官方强调字节跳动倡导多元兼容的文化氛围，不建议发表对立或冒犯性言论。同时官方指出，该员工的发帖本质上属于与工作无关的争议性话题，不符合字节圈的社区守则。为了共同维护良好的社区氛围，呼吁大家遵守相关规定。</p><p>&nbsp;</p><p></p><h4>星巴克新CEO“天价通勤”引争议：坐3小时专机上下班</h4><p></p><p>&nbsp;</p><p>星巴克新任首席执行官布莱恩·尼科尔因其计划从加州的家中乘坐公务机通勤至西雅图总部，引发公众广泛争议。据BBC 22日报道，根据公开的任命文件，尼科尔将继续居住在加州，而非搬迁至公司在西雅图的总部。他将通过公司提供的公务机往返两地，完成近1600公里的通勤路程，每周至少3天在西雅图办公。星巴克还将为尼科尔在加州设立一个小型办公室。同时，尼科尔的年薪为160万美元，并有机会获得720万美元的绩效奖金和价值2300万美元的公司股票。</p><p>&nbsp;</p><p>该安排迅速在社交媒体上引发公众不满，许多网民认为这与星巴克倡导的环保理念背道而驰。联合国2021年发布的一份报告显示，世界上最富有的1%人口产生的碳排放量是最贫穷的50%人口碳排放量总和的两倍。有网民评论说：“当你因为塑料吸管而批评星巴克时，他们的CEO正在乘私人飞机上班。”</p><p>&nbsp;</p><p></p><h4>曝苹果、英伟达正洽谈投资 OpenAI</h4><p></p><p>&nbsp;</p><p>据媒体援引消息人士报道，两大科技巨头苹果公司和英伟达均有意投资人工智能（AI）研究公司 OpenAI。周三有消息称，OpenAI 正在洽谈新一轮融资，计划以超过 1000 亿美元的估值筹集数十亿美元资金，风投公司兴盛资本（Thrive Capital）将领投此轮融资，投资达到 10 亿美元。此外，作为 OpenAI 最大股东，微软也将参与这轮融资。知情人士称，苹果正就投资 OpenAI 进行谈判，英伟达也已讨论过加入对 OpenAI 的最新融资。据悉，英伟达商谈在 OpenAI 新一轮融资中投入 1 亿美元。</p><p>&nbsp;</p><p>据悉，OpenAI 的 AI 聊天机器人目前周活跃用户数量已经超过 2 亿，短短 1 年时间实现翻倍。OpenAI 还表示在全球财富 500 强企业中，92% 正在使用其产品；自 7 月份发布 GPT-4o mini 以来，其自动 API 的使用量已翻了一番。</p><p>&nbsp;</p><p>此外，8 月 30 日消息，人工智能公司 OpenAI 和 Anthropic 已经同意允许美国政府在这些公司发布重大新的人工智能模型之前访问这些模型，以帮助提高它们的安全性。</p><p>&nbsp;</p><p></p><h4>上海电信大面积崩溃断网，网友：以为是手机坏了</h4><p></p><p>&nbsp;</p><p>8月26日18:00左右，部分上海电信用户在微博上反馈称上海电信崩溃，&nbsp;出现宽带没信号、电信10000号打不通等问题。有网友表示：“以为我家网络又出问题，打电话给电信维修师傅，说是整个上海电信都出问题了”“怀疑过手机坏了，宽带坏了，都没怀疑过电信出问题了，我甚至还报修了宽带”。另外，还有部分网友跑到“中国电信上海客服”官微评论区喊话：赶紧修网。</p><p>&nbsp;</p><p>有上海电信工作人员通过企业微信发布消息回应：“2024年8月26日17:30左右，上海电信部分宽带业务发生异常。目前上海电信正在全力抢修排障。给您带来的不便，深表歉意！”“企微用户们：已经来不及一一回复了，全上海宽带总线坏了，耐心等待，已在抢修中......”</p><p>&nbsp;</p><p></p><h4>OpenAI正在招人调查自己的员工，安全威胁可能来自内部</h4><p></p><p>&nbsp;</p><p>8月27日，据外媒报道，OpenAI最近发布招聘启事，招聘一名内部风险技术调查员，以强化其组织，抵御内部安全威胁。招聘启事称，该职位的职责包括分析异常活动、检测和减轻内部威胁，以及与人力资源和法律部门合作 “对可疑活动进行调查”。换句话该职位可能会涉及对OpenAI自家员工行为的调查。</p><p>&nbsp;</p><p>2023年，OpenAI成为恶意行为者的攻击目标。当时，黑客入侵了其内部消息系统。这一事件之所以能够被曝光，是因为两名知情人士向《纽约时报》泄露了信息。除了黑客组织外，这则招聘启事似乎表明OpenAI在担心来自自己员工的威胁，尽管目前还不清楚OpenAI究竟在提防哪种威胁。</p><p>&nbsp;</p><p></p><h4>苹果数字服务部门裁员约 100 人</h4><p></p><p>&nbsp;</p><p>据知情人士透露，美东时间 8 月 27 日，苹果公司罕见地在其数字服务部门裁员约 100 人，表明苹果公司正在进行业务重心的战略调整。知情人士称，该公司通知了受影响的员工，这些员工分别在苹果互联网软件与服务高级副总裁库伊（Eddy Cue）所负责的的服务部门的几个不同团队工作。</p><p>&nbsp;</p><p>据称，这些员工被告知，在被解雇之前，他们有 60 天的时间在苹果内部找到另一份工作。由于一些员工同时在多个团队工作，因此苹果其他业务部门也受到了间接影响。此次裁员包括一些工程师职位，其中最大的裁员对象是负责 Apple Books 应用程序和 Apple Bookstore 的团队。其他服务团队也在裁员，包括运营 Apple News 的团队。</p><p>&nbsp;</p><p></p><h4>IBM 关闭中国研发部门，涉及员工数量超过 1000 人</h4><p></p><p>&nbsp;</p><p>8 月 26 日，IBM 中国方面确认，IBM 将彻底关闭中国研发部门，涉及员工数量超过 1000 人。、IBM 中国在声明中称：“IBM 会根据需要调整运营，为客户提供最佳服务，这些变化不会影响我们为大中华区客户提供支持的能力。”</p><p>&nbsp;</p><p>声明还提到，中国企业，尤其是民营企业，越来越重视抓住混合云和人工智能技术带来的机遇，而 IBM 在中国的本地战略重点则是利用我们在技术和咨询方面的丰富经验，组建具备相应技能的团队，帮助中国客户共创符合他们需求的解决方案。</p><p>&nbsp;</p><p>IBM 强调未来将转向服务中国的民营企业以及部分在中国的跨国企业，但据第一财经记者了解，金融、能源等关键领域的大型国企才是 IBM 过去最重要的大客户。</p><p>&nbsp;</p><p></p><h4>澳大利亚新规：打工人下班后可不理老板，违者被罚超45万+</h4><p></p><p>&nbsp;</p><p>8月26日消息，据国外媒体报道称，澳大利亚通过了一项法律，员工可以在工作时间外不理会他们的老板。该法律规定，雇主仍然可以在下班后与员工联系，但员工现在有权不在工作时间以外回复，除非拒绝是无理的。这意味着员工可以拒绝监听、阅读或回复来自雇主或第三方（如客户）的联系信息。</p><p>&nbsp;</p><p>同时，澳大利亚“公平工作委员会（FWC）”将决定员工的拒绝是否合理，且必须考虑到员工的角色、联系的原因、方式等诸多因素。如果有企业违反上述法规，公司可能会被罚款高达超45万元。若员工提出的拒绝不合理，FWC也可以命令员工对雇主作出回应，若违反此类命令，可能会导致员工被罚款高达超9万元。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>大模型一周大事</h2><p></p><p>&nbsp;</p><p></p><h3>大模型发布</h3><p></p><p>&nbsp;</p><p></p><h4>黄仁勋预言步入现实，谷歌展示实时游戏生成 AI 模型 GameNGen</h4><p></p><p>&nbsp;</p><p>本周来自谷歌公司和特拉维夫大学的研究人员发表了一篇名为《Diffusion 模型是实时游戏引擎》的论文，介绍了计算机历史上第一个完全由神经网络模型支持的游戏引擎 GameNGen。</p><p>&nbsp;</p><p>研究人员在论文中写道：“GameNGen 是游戏引擎新范式的部分概念验证——游戏将会变成神经模型的权重，而不是代码行。”GameNGen 模型可以做到：使用 AI 生成模型，根据玩家的动作和反应，实时演算和生成游戏画面。</p><p>&nbsp;</p><p>英伟达高级研究经理&amp;具身智能集团主管 Jim Fan 博士在社交媒体上评论称，GameNGen 更像是一个神经辐射场（NeRF），而不是一个视频生成模型。神经辐射场通过从不同角度拍摄场景的图像，从而生成场景的 3D 展示。但这也意味着模型不具备泛化能力，无法“想象”新的场景。这也是 GameNGen 与 Sora 的不同点：它无法生成新的场景或交互机制。</p><p>&nbsp;</p><p>由 AI 渲染来进行实时游戏并不是一个全新的想法。在今年 3 月发布最新一代 Blackwell 架构芯片时，英伟达 CEO 黄仁勋就曾预言，大概在 5-10 年内就能看到完全由 AI 生成的游戏。</p><p>&nbsp;</p><p></p><h4>元象推出国内首个基于物理的3D动作生成模型MotionGen</h4><p></p><p>&nbsp;</p><p>深圳元象信息科技推出的MotionGen模型是中国3D AIGC领域的重大突破，通过融合大模型、物理仿真和强化学习等算法，实现用户通过简单文本指令快速生成逼真、流畅的3D动作。该模型降低了3D内容制作门槛，提高创作自由度和效率，对动画、游戏、电影和虚拟现实行业具有重要意义。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/fb/fb9c243ea911d96d47eb701e9b2a5d32.gif" /></p><p></p><p>&nbsp;</p><p></p><h4>RTX3090可跑，360AI团队开源视频模型FancyVideo</h4><p></p><p>&nbsp;</p><p>8月26日消息，360AI团队宣布他们的开源视频模型FancyVideo正式发布。此模型为想要改善视频处理性能的开发者与研究人员带来了巨大的便利。值得注意的是，这个模型在高端显卡RTX3090上运行良好，吸引了诸多关注，尤其是视频创作者和AI技术爱好者的热情。</p><p>&nbsp;</p><p>FancyVideo作为一个开源模型，提供了一系列强大的功能，包括高效的视频生成与编辑能力。360AI团队致力于将最新的深度学习技术应用于视频内容创作，进一步推动了视频生成技术的发展。FancyVideo的开放性意味着任何人都可以调试和优化模型，这对开发者群体是一个重要的福音。在多次的内测中，用户们发现该模型在处理高质量视频时的表现不但稳定，并且效率极高，这使得创作者们能够在短时间内完成大型项目，无论是影视制作还是短视频创作。</p><p></p><h4>&nbsp;</h4><p></p><p></p><h4>“云知声”正式发布山海多模态大模型</h4><p></p><p>&nbsp;</p><p>8月26日消息，“云知声”宣布正式推出山海多模态大模型。据介绍，山海多模态大模型通过整合跨模态信息，山海多模态大模型可接收文本、音频、图像等多种形式作为输入，并实时生成文本、音频和图像的任意组合输出。基于多模态交互能力，可根据不同的场景和需求，模拟出各种人物性格和对话风格。</p><p>&nbsp;</p><p></p><h4>AI 图像生成公司 Midjourney 宣布进军硬件领域，苹果前硬件经理加盟</h4><p></p><p>&nbsp;</p><p>知名 AI 图像生成公司 Midjourney 宣布进入硬件领域，成立新硬件部门，由具有丰富硬件经验的创始人 David Holz 和前苹果硬件经理 Ahmad Abbas 领导。尽管未透露具体产品类型，Midjourney 表示有多个项目正在进行，暗示存在多种可能性。目前，公众期待 Midjourney 的新硬件产品，但具体发布时间尚未明确。</p><p>&nbsp;</p><p></p><h3>企业应用</h3><p></p><p>&nbsp;</p><p>8 月 27 日，阿里智能信息事业群旗下产品夸克 PC 端产品上新，升级 AI 搜索、AI 写作、AI PPT、AI 文件总结等一系列功能，可一站式完成信息检索、创作和总结。此外，夸克 AI 回答的首字出现速度和吐字速度领先于行业，可瞬时给出精准答案。三栏式的界面设计能更清晰地展现图文、视频等生成式回答和网页，便于核心信息浏览。8 月 27 日，苹果苹果正式宣布将于北京时间 9 月 10 日凌晨 1 点在加利福尼亚州库比蒂诺的总部举办特别活动，主题为“高光时刻（It 's Glowtime）”。此次发布会预计将推出最新款 iPhone、Watch 和 AirPods。即将发布的 iPhone 16 两款 Pro 系列的手机将拥有更大的屏幕，并拥有新的相机功能，比如新增的“拍照按钮”。但这场发布会的焦点预计将是 Apple Intelligence，这套人工智能工具将整合到所有新款 iPhone 中。谷歌研究人员开发了一个名为 GameNGen 的神经网络驱动游戏引擎，该引擎能够实时生成经典射击游戏《毁灭战士》的画面，无需传统游戏引擎的组件。该技术利用扩散模型实时预测每一帧画面，先通过 RL 智能体学习游戏并记录过程，再训练模型生成画面。此技术或能实现人工智能即时创建游戏，并提供个性化服务。美年健康联合华为云和润达医疗研发的国内首款健康管理 AI 机器人”健康小美“正式上线，提供个性化全生命周期健康管理服务，标志着美年健康在数智化健康管理领域迈出关键一步。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OuewLZxKAMrqX82YtwZA</id>
            <title>智谱清言 App 全新升级视频通话功能，并推出新一代基座大模型</title>
            <link>https://www.infoq.cn/article/OuewLZxKAMrqX82YtwZA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OuewLZxKAMrqX82YtwZA</guid>
            <pubDate></pubDate>
            <updated>Mon, 02 Sep 2024 06:54:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>近日，智谱宣布清言 App 推出“视频通话”功能，同时官宣了新一代基座大模型 GLM-4-Plus，此外还将 CogVideoX-5B 开源、GLM-4-Flash 免费。</p><p></p><h4>全新的“视频通话”功能来了</h4><p></p><p></p><p>清言 App 迎来“视频通话”功能，这是清言 App 继 7 月上线生成视频功能清影 Ying 后又一重大更新，也是国内首个面向 C 端开放的视频通话。</p><p></p><p>清言视频通话跨越了文本模态、音频模态和视频模态，并具备实时推理的能力。用户拨打清言的视频通话窗口，即可与它进行流畅通话，即便频繁打断它也能迅速反应。清言可以理解摄像头拍摄到的内容，可以听懂指令并准确执行。这样的体验就如同和真人视频通话一样。</p><p></p><p>下面是在游戏陪伴、作业辅导、作业辅导和一些生活场景下，清言视频通话功能的表现：</p><p></p><p></p><p></p><p></p><p>据悉，清言 App 视频通话功能将于 8 月 30 日上线，首批面向清言部分用户开放，同时开放外部申请。</p><p></p><p>清影视频通话内测开通链接：<a href="https://zhipu-ai.feishu.cn/share/base/form/shrcnqpIx9q5ILEFeT2cPNhyuSf">https://zhipu-ai.feishu.cn/share/base/form/shrcnqpIx9q5ILEFeT2cPNhyuSf</a>"</p><p></p><h4>GLM-4-Plus 发布并上线开放平台</h4><p></p><p></p><p>GLM-4-Plus 是智谱全自研 GLM 大模型的最新版本，在语言理解、指令遵循、长文本处理等方面性能得到全面提升。</p><p></p><p>GLM-4-Plus 使用了大量模型辅助构造高质量合成数据以提升模型性能，利用 PPO 有效有效提升模型推理（数学、代码算法题等）表现，更好反映人类偏好。</p><p></p><p>语言文本能力方面，GLM-4-Plus 和 GPT4o 及 405B 参数量的 Llama3.1 相当。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b5/b5a3183ab43443d0d9a6ea38d9629c31.png" /></p><p></p><p>LCB: LiveCodeBench</p><p></p><p>NCB: NaturalCodeBench</p><p></p><p>*represents reproduced results</p><p></p><p>长文本能力则通过更精准的长短文本数据混合策略，取得了更强的长文本的推理效果，比肩国际先进水平。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/78/78a5b5589d71fc9565f25d3a465b513a.png" /></p><p></p><p>GLM-4V-Plus 还可以理解网页内容，并将其转换为 html 代码，在图像和视频理解能力方面位居前列。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/36/3625c4f27a64fec7cd35c4c580ef2f6c.png" /></p><p></p><p>GLM-4V-Plus 能够理解并分析复杂的视频内容，同时具备时间感知能力。</p><p></p><p></p><p>下面是针对该视频相关问题，GLM-4V-Plus 做出的回答。</p><p></p><p></p><blockquote>用户：这个穿绿色衣服的球员在整个视频都做了什么？GLM-4V-Plus：在整个视频中，穿绿色衣服的球员在场上运球，然后跳起将球投入篮筐。用户：这个视频的精彩时刻是什么？发生在第几秒？GLM-4V-Plus：这个视频的精彩时刻发生在第 4 秒，当时穿绿色衣服的球员跳起并将球投入篮筐。</blockquote><p></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a5/a57ad3dcd3bff9b05f40a36d2eafa01b.png" /></p><p></p><p>GLM-4-Plus 现已在智谱大模型开放平台部署，企业和开发者即日起可以通过智谱开放平台 bigmodel.cn 上的 API 调用智谱最新的基座大模型。</p><p></p><p>_GLM-4-Plus API 文档更新在：_<a href="https://bigmodel.cn/dev/api#glm-4">https://bigmodel.cn/dev/api#glm-4</a>"</p><p></p><p>_GLM-4V-Plus API 文档更新在：_<a href="https://bigmodel.cn/dev/api#glm-4v">https://bigmodel.cn/dev/api#glm-4v</a>"</p><p></p><p>_MaaS 模型介绍页（包含最新模型的概要介绍）：_<a href="https://bigmodel.cn/dev/howuse/model">https://bigmodel.cn/dev/howuse/model</a>"</p><p></p><p></p><h4>文生图模型升级</h4><p></p><p></p><p>文生图模型迎来最新版本 CogView-3-Plus，其效果接近目前最佳的 MJ-V6 及 FLUX 等模型，并支持图片编辑功能。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/82/82f9056cb815dc500e4f4cb68f6e2d97.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/56/561f499df85b7e7b7966f28041368390.png" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c4/c4ceb73987aa42342653eff026caf355.png" /></p><p></p><p>_CogView-3-Plus 更新在：：_<a href="https://bigmodel.cn/dev/api#cogview">https://bigmodel.cn/dev/api#cogview</a>"</p><p></p><p></p><h4>模型再开源</h4><p></p><p></p><p>另外，继 CogVideoX 2B 版本开源后，5B 版本也于近日正式开源，性能更强，推理显存需求最低仅为 11.4GB。同时，CogVideoX-2B 的开源协议调整为更加开放的 Apache 2.0 协议，任何企业与个人均可自由使用。</p><p></p><p>随着 CogVideoX-5B 的开源，智谱不仅在开源模型数量上领先，累计下载量也突破 2000 万次，智谱以实际行动为国际开源社区做出自己应有贡献。</p><p></p><p>随着技术进步、效率提升和成本优化，智谱宣布大模型开放平台 bigmodel.cn 最具性价比的大模型，GLM-4-Flash 现已完全免费，用户可以通过调用 GLM-4-Flash 快速、免费地构建专属模型和应用。这也是智谱开放平台首个完全免费的大模型 API。</p><p></p><p>智谱表示，最新推出的基座大模型，和此前发布的 CogVideoX 等模型一道，完善了智谱自主原创的全栈大模型谱系，推动智谱实现面向世界先进水平的全面对标。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/BrPAABR3O6dSxSOYarv6</id>
            <title>通义千问开源视觉语言模型 Qwen2-VL，可理解 20 分钟以上视频</title>
            <link>https://www.infoq.cn/article/BrPAABR3O6dSxSOYarv6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/BrPAABR3O6dSxSOYarv6</guid>
            <pubDate></pubDate>
            <updated>Sun, 01 Sep 2024 03:06:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>2023 年 8 月，通义千问开源第一代视觉语言理解模型 Qwen-VL，成为开源社区最受欢迎的多模态模型之一。短短一年内，模型下载量突破 1000 万次。目前，多模态模型在手机、车端等各类视觉识别场景的落地正在加速，开发者和应用企业也格外关注 Qwen-VL 的升级迭代。</p><p></p><p>据悉，相比上代模型，Qwen2-VL 的基础性能全面提升，可以读懂不同分辨率和不同长宽比的图片，在 DocVQA、RealWorldQA、MTVQA 等基准测试创下全球领先的表现；可以理解 20 分钟以上长视频，支持基于视频的问答、对话和内容创作等应用；具备强大的视觉智能体能力，可自主操作手机和机器人，借助复杂推理和决策的能力，Qwen2-VL 可以集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作；能理解图像视频中的多语言文本，包括中文、英文，大多数欧洲语言，日语、韩语、阿拉伯语、越南语等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/7335b4cc3337b83b75e7bfc944d51f61.png" /></p><p></p><p>通义千问团队从六个方面评估了模型能力，包括综合的大学题目、数学能力、文档表格多语言文字图像的理解、通用场景问答、视频理解、Agent 能力。Qwen2-VL-7B 以其“经济型”参数规模实现了极具竞争力的性能表现；Qwen2-VL-2B 则可支持移动端的丰富应用，同样具备完整图像视频多语言的理解能力，在视频文档和通用场景问答方面，相比同规模模型优势显著。</p><p></p><h4>测试表现</h4><p></p><p></p><p>视频理解与实时聊天</p><p></p><p></p><p></p><p>Visual Agent 能力：函数调用和视觉交互</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a9/a9957ac57adf4b1456b4e7c65f5243a9.gif" /></p><p></p><p>视觉推理</p><p></p><p>用户：请告诉我从 Ruka 的生日到 Yuki 的生日有多少天？仔细地一步步思考。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/10/102623bfd33e0ac4711552d930fb2130.png" /></p><p></p><p>Qwen2-VL：</p><p></p><p></p><blockquote>要确定 Ruka 和 Yuki 生日之间的天数，我们需要计算他们各自生日日期之间的差异。1、Ruka 的生日是在 6 月 24 日。2、Yuki 的生日是在 9 月 17 日。让我们一步步来计算：1. 从 6 月 24 日到 6 月 30 日：从 6 月 24 日到 6 月 30 日共有 6 天。2. 从 7 月 1 日到 8 月 31 日：7 月有 31 天。8 月有 31 天。7 月和 8 月总共有 31 + 31 = 62 天。3. 从 9 月 1 日到 9 月 17 日：从 9 月 1 日到 9 月 17 日共有 17 天。现在我们将所有的天数加起来：从 6 月 24 日到 6 月 30 日：6 天从 7 月 1 日到 8 月 31 日：62 天从 9 月 1 日到 9 月 17 日：17 天总天数 = 6 + 62 + 17 = 85 天因此，Ruka 和 Yuki 的生日之间共有 85 天。</blockquote><p></p><p></p><p>更细节的识别理解</p><p></p><p>用户：转录图中内容，并识别图中每段内容的语言。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/94/946644dd88a8a62bd424ec07cdb3f470.png" /></p><p></p><p>Qwen2-VL：</p><p></p><p></p><blockquote>汉语，也称为“华语”。是中国使用人数最多的语言，也是世界上作为第一语言使用人数最多的语言。是中华优秀传统文化的重要载体。繁体字，又稱為“繁體中文”，與“簡化字”/“簡體字”相對。一般是指漢字簡化運動被簡化字所代替的漢字。语言：中文日本語は地方ごとに多様な方言があり、とりわけ琉球諸島で方言差が著しい。语言：日语한국어(韓國語), 조선말(朝鮮말)는 대한민국과 조선민주주의인민공화국의 공용어이다. 둘은 표기나 문법에서는 차이가 없지만 동사 어미나 표현에서 차이가 있다.语言：韩语Le français est une langue indo-européenne de la famille des langues romanes dont les locuteurs sont appelés « francophones ».语言：法语El español o castellano es una lengua romance procedente del latín hablado, perteneciente a la familia de lenguas indoeuropeas.语言：西班牙语A língua portuguesa, também designada português, é uma língua indo-europeia românica flexiva ocidental originada no galego-português falado no Reino da Galiza e no norte de Portugal.语言：葡萄牙语Is ceann de na teangacha Ceilteacha í an Ghaeilge (nó Gaeilge na hÉireann mar a thugtar uirthi corruair), agus ceann de na trí cinn de theangacha Ceilteacha ar a dtugtar na teangacha Gaelacha (Gaeilge, Gaeilge Mhanann agus Gaeilge na hAlban) go háirithe.语言：爱尔兰语English is a West Germanic language in the Indo-European language family, whose speakers, called Anglophones, originated in early medieval England on the island of Great Britain.语言：英语Die deutsche Sprache bzw. Deutsch ist eine westgermanische Sprache, die weltweit etwa 90 bis 105 Millionen Menschen als Muttersprache und weiteren rund 80 Millionen als Zweit- oder Fremdsprache dient.语言：德语Język polski, polszczyzna — język z grupy zachodniosłowiańskiej (do której należą również czeski, kaszubski, słowacki, języki łużyckie czy wymarły język drzewiański), stanowiącej część rodziny indoeuropejskiej.<code lang="text">语言：波兰语
</code>Η ελληνική γλώσσα ανήκει στην ινδοευρωπαϊκή οικογένεια και αποτελεί το μοναδικό μέλος του ελληνικού κλάδου.<code lang="text">语言：希腊语
</code>Tiếng Việt hay Việt ngữ là một ngôn ngữ thuộc ngữ hệ Nam Á, được công nhận là ngôn ngữ chính thức tại Việt Nam.<code lang="text">语言：越南语
</code>Монгол хэл нь Монгол улсын албан ёсны хэл юм.<code lang="text">语言：蒙古语
</code>Число владеющих русским языком в России составляет 137,5 млн человек (2010). Всего в мире на русском говорят 258,2 млн человек (2022)<code lang="text">语言：俄语
</code>हिन्दी विश्व की एक प्रमुख भाषा है और भारत की एक राजभाषा है। केन्द्रीय स्तर पर भारत में सह-आधिकारिक भाषा अंग्रेज़ी है।<code lang="text">语言：印地语
</code>Kiswahili ni lugha ya Kibantu yenye misamiati mingi ya Kiarabu (35%), lakini sasa ya Kiingereza pia (10%), inayozungumzwa katika eneo kubwa la Afrika ya Mashariki.<code lang="text">语言：斯瓦希里语
</code></blockquote><p></p><p></p><p></p><h4>架构改进</h4><p></p><p></p><p>Qwen2-VL 延续了 ViT 加 Qwen2 的串联结构，三个尺寸的模型都采用了 600M 规模大小的 ViT，支持图像和视频统一输入。研发团队还在架构上作了两大改进：</p><p></p><p>一是实现了对原生动态分辨率的全面支持，不同于上代模型，Qwen2-VL 能够处理任意分辨率的图像输入，这一设计模拟了人类视觉感知的自然方式，赋予模型处理任意尺寸图像的强大能力；</p><p></p><p>二是使用了多模态旋转位置嵌入（M-ROPE）方法，传统的旋转位置嵌入只能捕捉一维序列的位置信息，M-ROPE 使得大规模语言模型能够同时捕捉和整合一维文本序列、二维视觉图像以及三维视频的位置信息，赋予了语言模型强大的多模态处理和推理能力，能让模型更好地理解和建模复杂的多模态数据。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/6295bd8dd612a75290615d8a3db41fff.png" /></p><p></p><p>目前，通义千问团队以 Apache 2.0 协议开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，开源代码已集成到 Hugging Face Transformers、vLLM 和其他第三方框架中。开发者可以通过 Hugging Face 和魔搭 ModelScope 下载使用模型，也可通过通义官网、通义 APP 的主对话页面使用模型。</p><p></p><p>GitHub：<a href="https://github.com/QwenLM/Qwen2-VL">https://github.com/QwenLM/Qwen2-VL</a>"</p><p></p><p>HuggingFace<a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d">https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d</a>"</p><p></p><p>魔搭 ModelScope：https://modelscope.cn/organization/qwen?tab=model</p><p></p><p>模型体验：<a href="https://huggingface.co/spaces/Qwen/Qwen2-VL">https://huggingface.co/spaces/Qwen/Qwen2-VL</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KLbMMKO5Zu8ECJdYFZtC</id>
            <title>龙蜥装机量突破800万！浪潮信息将打造最AI的服务器操作系统！</title>
            <link>https://www.infoq.cn/article/KLbMMKO5Zu8ECJdYFZtC</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KLbMMKO5Zu8ECJdYFZtC</guid>
            <pubDate></pubDate>
            <updated>Sat, 31 Aug 2024 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月30日，主题为“进化·重构·赴未来”的2024龙蜥操作系统大会(OpenAnolis Conference)在北京举办。龙蜥社区作为国内领先的开源操作系统根社区，推出的Anolis OS及衍生版装机量已突破800万套，实现了从开源到商业化的良性循环发展。浪潮信息作为龙蜥社区副理事长单位，深度融合AI能力，基于最新发布的Anolis OS 23，升级云峦服务器操作系统KOS，打造最AI的服务器操作系统，促进龙蜥生态蓬勃发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/dedda8a97a88f2f5c9aa0d30c694315e.png" /></p><p></p><p>2024 龙蜥操作系统大会由龙蜥社区主办，阿里云、浪潮信息、Intel、ARM等 24 家理事单位共同承办，汇聚了两院院士、政府领导、以及来自阿里巴巴、浪潮信息、统信软件、Intel、AMD、清华大学等政、产、学、研领域大咖，覆盖芯片、操作系统、整机、云服务等计算产业上下游与300+生态伙伴，1000+开发者，是参与厂商最多、生态最开放、最国际化的操作系统年度盛会。</p><p></p><h3>AI原生催生服务器操作系统变革</h3><p></p><p></p><p>大模型所具备的智能涌现和泛化能力，将创造出千千万万甚至数以百万计的 AI 原生应用，重构产业架构和底层基座。随着AI原生时代的到来，人机交互方式、编程范式、内核复杂性等的变化，正在催生服务器操作系统的深层变革。未来的开发者可以直接使用自然语言进行开发，操作系统不仅需要支持新的编程范式，还需要在用户界面和用户体验上做出相应的改进，以适应AI原生时代的需求。同时，在AI原生的趋势下，操作系统的“内核层”变得更加复杂，底层硬件需要提供更复杂的计算资源，包括多元化的通用处理器与AI加速芯片、更高效的内存、网络、存储IO等等，要求操作系统能够有效管理和分配这些资源，确保AI应用的高效运行。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efafb7d2678fe0f37004771f303ac923.png" /></p><p>中国工程院院士陈纯</p><p></p><p>中国工程院院士陈纯指出，在云+AI的数智时代中，操作系统的作用更为重要，软硬件体系不断在创新演变，需要通过操作系统连接多样化硬件与软件，为各类应用的稳定运行提供保障。龙蜥社区积极探索云计算与 AI 技术融合及应用创新，在助力传统应用生态应对停服断供挑战的同时，推动新兴应用生态创新成长，持续拓展创新边界。未来，要进一步深化云计算与人工智能的协同，为大模型和人工智能应用提供更为强大的基石，推动各行业的智能化升级。</p><p></p><h3>AI原生First，浪潮信息打造最AI的服务器操作系统</h3><p></p><p></p><p>AI原生化应用对系统软件提出了更高要求。对此，龙蜥社区副理事长张东提出服务器操作系统三大发展建议：</p><p>一是AI原生First，全面优先拥抱AI，深度融合AI能力，打造上游芯片到算法应用开发厂商的最优基础软件，实现AI能力的快速价值转化；二是坚持应用导向，强调社区技术创新与应用场景紧密结合，创新基础软件与硬件、应用之间的协同模式，打造面向领域垂直场景的最优解决方案；三是强化开放中立，社区撕掉单一厂商标签，强调共治共建共享，建立开放标准，积极与国际领先的社区接轨，保持同步发展，形成繁荣的生态系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a302b61ccb34a9c69c957b7c115f66e.png" /></p><p>龙蜥社区副理事长张东</p><p></p><p>会上，龙蜥社区隆重推出更安全、更稳定、更AI的龙蜥操作系统Anolis OS 23 官方正式版，在操作系统层面首次原生支持AI算力调用和应用。龙蜥社区基于“云+AI”创新理念，正式发布了“CentOS替代计划”“Anolis OS 23生态衍生计划”“AI应用推广计划”三大计划，致力于推动国内操作系统实现从开源到商业化的良性循环发展。</p><p></p><p>作为龙蜥社区副理事长单位，浪潮信息自2022年加入社区以来，积极投入社区建设，依托浪潮信息龙蜥联合实验室，在技术创新、标准制定、生态建设、运营推广等多个维度推动社区建设，获得最佳合作伙伴奖。</p><p>浪潮信息将率先发布基于Anolis OS 23的云峦KeyarchOS衍生版，在产品性能、智能化、安全能力等实现全面升级，特别是针对AI原生应用场景进行大量优化，包括为GPU、HCA卡等AI服务器关键部件驱动及CUDA/NCCL等GPU生态软件提供了最佳版本选型，预植Lustre/BeeGFS等并行文件系统，并通过自动化工具快速配置AI开发环境，可在20分钟完成200节点的AI训练基础开发环境上线可用；AI for System方面，新版本融入开箱即用的Yuan-Chat解决方案，具备智能问答、智能编程能力，并且集成服务器智能运维、操作系统智能运维、智能调优组件，实现了系统故障智能定位、自动化部署管理、专家经验一键调优等更智能的操作系统使用体验。</p><p></p><p>Anolis OS升级全新的架构，这里面也离不开浪潮信息与龙蜥社区的合作有关，浪潮信息系统软件产品部总经理 苏志远在沟通中谈到，Anolis OS 23是国内三大社区技术路线收敛之后第一个发行版，统一了一些关键软件包的选型，减少了用户适配操作系统方面重复性的工作。新版本的发布保证了供应链的安全，保证漏洞足够修复，包括一些新的特性，这些兼容性方面的工作是浪潮信息所擅长的。在漏洞修复方面，浪潮信息作为安全联盟的副主席单位，建立了漏洞管理机制，制定漏洞奖励计划，持续完善Anolis OS 23漏洞修复工作。</p><p></p><p>而针对CentOS于今年6月30日全面停服，浪潮信息早在6月就已联合龙蜥社区重磅推出“停更无忧”行动计划，提供漏洞及Bug修复、故障定位与解决、安全加固等服务，有效防范CentOS停服后可能出现的新型漏洞、恶意代码引发的信息泄露、业务中断以及勒索攻击等严重风险。同时，浪潮信息还提供X2Keyarch迁移工具，该工具提供了多种迁移方案，以适应不同业务场景和数据结构的需求。支持批量迁移、增量迁移、分批迁移等多种方式，并可对迁移数据进行校验和清洗，以确保数据的准确性和完整性。</p><p></p><p>张东强调，操作系统是软硬协同设计与优化的核心，浪潮信息将坚持“以应用为导向，以系统为核心”，真正发挥系统厂商的优势，通过整机带动，更好的实现软硬协同的系统设计，加快技术创新，确保龙蜥操作系统持续迭代进化。</p><p></p><p>在谈及未来支撑云+AI业务，操作系统发展过程中可能会遇到的哪些挑战的时候，张东对此认为，AI技术发展迅速，导致现有技术体系，特别是在操作系统、存储和网络方面，难以跟上需求。这包括算力不足、存储需求大幅增加、以及网络传输速度需要提升等问题。应对的办法也是有的，当前应对AI需求的方式是通过堆积资源，如增加GPU卡数量、扩展存储容量等。然而，未来可能需要新的技术思路，以更好地适应AI的发展。这种转变可能涉及软硬件一体化协同创新以及新的体系结构。</p><p></p><p>张东提到，未来操作系统可能需要面对巨型资源管理的挑战，这将导致操作系统架构的变革。同时，他还提到了操作系统可能逐渐发展成智能体，以更自主的方式处理任务，而不是依赖人为指挥。尽管张东指出了操作系统面临的潜在挑战，但他认为，这些问题的解决将随着技术的不断发展而逐步显现。</p><p></p><p>目前，龙蜥操作系统已成为用户首选的服务器操作系统，装机量已突破800万套，社区生态伙伴数量超过1000家，是国内规模最大、生态最全面的服务器操作系统之一。</p><p></p><p>中国科学院院士、CCF 开源发展委员会主任王怀民表示，“开源社区在提升我国创新体系整体效能方面要发挥独特作用，龙蜥社区有力推动了我国云计算产业链的完善和发展，为我国数字产业生态繁荣注入了新活力。目前，龙蜥等中国操作系统社区已经就 Linux 内核选择等达成一致，希望未来业界朋友们共同努力，促进产、学、研、用的深度融合，探索中国式开源新模式，形成更强大的开源创新的合力。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/d4/d473453807132fc6ebaace88361374c2.jpeg" /></p><p>中国科学院院士、CCF 开源发展委员会主任王怀民</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/j789dDkVZBuhfGpIMp6p</id>
            <title>5个能力项、191个能力要求，百度文心快码首批通过信通院评测</title>
            <link>https://www.infoq.cn/article/j789dDkVZBuhfGpIMp6p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/j789dDkVZBuhfGpIMp6p</guid>
            <pubDate></pubDate>
            <updated>Fri, 30 Aug 2024 09:51:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月29日，中国信息通信研究院（以下简称“中国信通院”）公布了由其组织的可信AI智能编码工具评估结果，国内数十家企业参与评估，百度智能代码助手文心快码（Baidu Comate）首轮参与，并获得4+评级，成为国内首批通过该项评估并获得当前最高评级的企业之一。</p><p></p><p>据悉，中国信通院于2024年4月与四十余家企业共同编制发布了《智能化软件工程技术和应用要求 第2部分：智能开发能力》标准。本次评估以此标准为依据，涉及5个能力项，14个能力子项，191个能力要求，评估内容涵盖智能编码、代码质量检查、开发者辅助、非功能项和工程化能力。</p><p></p><p>经评估，文心快码在智能编码、代码质量检查、开发者辅助、非功能项和模型层等能力，满足功能准确性和丰富性要求。此外，文心快码融合丰富的私域知识和权威的公开知识资源，通过检索本地文档、知识库以及网页等多种形式，实现检索增强生成。具备多模态生成能力，可实现基于图片的前端页面开发，并已用于实际的项目开发。</p><p></p><p>文心快码是基于文心大模型，结合百度二十年编程现场大数据打造的新一代智能编码助手，覆盖Java、Python、C++等100+种编程语言，支持JetBrains、VSCode等10+种主流 IDE。于去年6月份正式对外发布，目前已更新至2.5版本，集模型训练、推理加速、工程实践、安全保障四重技术创新于一身，不仅能高效完成基础编程任务，并在代码续写、代码解释、NL2Code、Code Debug 等能力上保持领先。</p><p></p><p>具体而言，如今的文心快码业务适配能力更强，融合了私域知识和公开知识，通过检索本地文档、知识库以及网页等多种形式，能实现检索增强生成。这种多模态生成能力，可实现基于图片的前端页面开发，目前已成功用于实际项目开发。同时，得益于增强生成功能，文心快码能结合真实业务场景提供代码解释和流程图，帮助开发者直观感受调用逻辑，匹配最佳解决方案并生成业务适配的代码，有效提高研发效率。</p><p></p><p>在Comate+ 开放平台，文心快码还能实现企业私域知识、第三方能力与编程现场深度结合，让智能代码助手更适配企业的业务知识、团队规范、标准流程以及研发管理。针对企业关注的代码安全问题，文心快码提供了企业级安全保障。在编码阶段，文心快码能及时发现安全问题，实现通用漏洞和供应链漏洞自动修复，实现机密防泄漏。文心快码还提供私有化和混合云部署方案，私域数据在客户本地存储，敏感词模型直接过滤，保证数据安全和模型效果，安全防空企业资产。</p><p></p><p>目前，文心快码已经成为百度和其他企业的高效合作伙伴。数据统计，80%的百度工程师已经在深度使用文心快码，新增代码生成占比30%，代码采用率超过46%。在百度之外，文心快码已广泛应用于科技、汽车、保险、集成电路等行业。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ERYQYYDo2IrGOfABZuVH</id>
            <title>不发 GPT-5、直接上 GPT-6？曝 OpenAI 新模型代号 Orion，目标“杀死”幻觉</title>
            <link>https://www.infoq.cn/article/ERYQYYDo2IrGOfABZuVH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ERYQYYDo2IrGOfABZuVH</guid>
            <pubDate></pubDate>
            <updated>Fri, 30 Aug 2024 06:52:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>昨日，有最新消息称，OpenAI 神秘的“草莓”（Strawberry）最早可能在今年秋季发布，并将在该项目的帮助下开发出能力全面超越 GPT-4 的最新大语言模型，其内部代号为 Orion，可以显著推动人工智能领域的发展。</p><p></p><p>据两名参与该项目的知情人士透露，OpenAI 最早要发布草莓的聊天机器人版本，并会作为 ChatGPT 的一部分功能推出。草莓的增强逻辑与推理能力可以使 AI 系统提前规划并具备深度研究能力，从而为后续能够实现复杂决策和任务执行的更多自主 AI 智能体铺平道路。</p><p></p><p>此外，草莓预计将负责生成高质量的合成训练数据，借此为 Orion 提供关键助力，这种方法有望减少错误并提高下一代模型的整体性能。</p><p></p><p>还有网友曝料称，Orion 将是 OpenAl &nbsp;跳过 GPT-5 直接推出的 GPT-6 版本。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/60/60e4a81b27484ae148a8a09010c43435.png" /></p><p></p><p>图源 @indigo：<a href="https://x.com/indigo11/status/1828565975844733145">https://x.com/indigo11/status/1828565975844733145</a>"</p><p></p><p></p><h1>GPT-4 继任者“Orion”开发中，目标：没有幻觉</h1><p></p><p></p><p>数学是生成式 AI 发展的基础，如果 AI 模型掌握数学能力，将拥有更强的推理能力，甚至可以解决从未见过的数学问题或自发性解决变成问题，媲美人类智慧，而这一点也是目前的大语言模型还无法做到的。</p><p></p><p>据悉，草莓系统的目的是强化 OpenAI 的模型推理能力，处理复杂科学和数学问题的能力，让大模型不仅能生出答案，还能提前规划，以便自主、可靠浏览网络，进行 OpenAI 定义的深度研究。</p><p></p><p>知情人士透露，OpenAI 正在使用更大版本的草莓生成提供给 Orion 的训练用数据，这种由 AI 生成的训练数据也被称作“合成数据”（Synthetic Data）。与之前的模型相比，草莓与高质量合成数据的结合有望减少 Orion 的出错几率，有望帮助 OpenAI 提升获取高品质数据的能力。有研究表明，高质量数据正是打造强大且高效的 AI 模型的关键前提。</p><p></p><p>前不久，OpenAI 首席执行官 Sam Altman 的确公开强调了高质量数据对于训练 AI 模型的重要性。当时，Altman 还透露，目前 OpenAI 已有足够的数据来训练 GPT-4 之后的下一个模型，同时也在尝试使用合成数据。</p><p></p><p>对此，Minion AI 的首席执行官、GitHub Copilot 的前首席架构师 Alex Graveley 表示，使用草莓产生更高质量的训练数据可以帮助 OpenAI 减少其模型产生的错误数量，也就是所谓的 AI 幻觉（Hallucination）。“想象一下‘没有幻觉的模型’，你问它一个逻辑难题，它第一次就答对了。之所以能够做到这一点，是因为训练数据中的模糊性更少，因此它的猜测更少。”Graveley 进一步解释。</p><p></p><p></p><h1>草莓模型已向美国官员展示，完整版不对外开放</h1><p></p><p></p><p>草莓项目的前身为 Q*，自去年秋天开始流传 OpenAI 可能取得更大突破以来，就一直是 AI 社区的猜测话题。当时据说， Q* 能够解决棘手的科学和数学问题，并很快被贴上了秘密 AGI 项目的标签。之后，Altman 又间接证实了 Q* 的存在，对外称这是一次 “不幸的泄密”。</p><p></p><p>知情人士表示，草莓旨在解决大模型以往未曾见过的数学问题并优化编程任务，其增强逻辑还有望使其在拥有充足“思考”时间的情况下，更加有效地解决与语言相关的挑战。</p><p></p><p>在内部演示中，草莓成功拼出了《纽约时报》上刊登的字谜“Connections”。该模型还可为其他更先进的 AI 系统提供支撑，帮助其在生成内容的同时还可采取操作行动。OpenAI 还在内部文件中描述了使用草莓模型实现互联网自主搜索的计划，希望让 AI 掌握提前规划和深入研究的能力。</p><p></p><p>此外，之前有外媒报道，OpenAI 已经对一款在 MATH 基准测试中得分超过 90% 的 AI 模型进行了内测。MATH 基准测试是一系列冠军级数学问题，包含高中生和大学生数学竞赛的问题，被视作衡量 AI 系统在解决复杂数学问题方面的性能基准。相比之下，最初的 GPT-4 得分约为 53%，而 GPT-4o 的得分为 76.6%。</p><p></p><p>而最新消息称，这一分数高于 90% 的模型很可能就是草莓，今年夏天 OpenAI 还向美国国家安全官员展示了草莓模型。考虑到安全问题，这款强大的模型不直接对公众提供服务，以防止被美国限制的国家拿它来合成数据训练更强大的模型。</p><p></p><p>OpenAI 会在草莓的基础上提供一个更小的蒸馏版对外提供服务，这样部署成本也更低，完整的草莓会被该公司内部用来生成高质量的合成数据。有网友评价道，“好货当然先留给自己用，确保竞争对手与自己有隔代差别。”也有网友猜测，“肉眼可见 OpenAI 需要靠美国政府提供资金了，因为已经无法从市场上筹集到足够的资金了。”</p><p></p><p>另值得一提的是，草莓系统的诞生与 OpenAI 前首席科学家 Ilya Sutskever 和斯坦福大学都有所关联。</p><p></p><p>据说，草莓这个项目算是由 Ilya 启动的，也是他为草莓提供了设计思路和基础。在他离开之前，OpenAI 的研究人员 Jakub Pachocki 和 Szymon Sidor 基于 Sutskever 的工作开发了 Q*。现在他出走并建立自己的初创企业 Safe SuperIntelligence，专注于开发更安全的超级 AI。</p><p></p><p>在 Q* 的研究过程中，OpenAI 的研究人员开发了一种被称为「测试时计算」（test-time computation）的概念变体，该方法使模型有机会花更多时间考虑用户命令或问题的所有部分，旨在提升大语言模型的解题能力。当时，Ilya 还发表了一篇围绕相关成果的博客文章。</p><p></p><p>还有报道指出，草莓系统与斯坦福大学研究人员提出的“自学推理机”（STaR）方法有着相似之处，都旨在提高人工智能的推理和问题解决能力，使其超越 GPT-4 等现有语言模型所能达到的水平。</p><p></p><p>思路上，这两种方法都是教人工智能一步一步地 “推理 ”或 “思考”来得出更好的解决方案，Quiet-STaR 教语言模型生成并学习在文本中任何位置继续的可能理由，而 Q* 则旨在将语言模型与规划算法相结合。另一个类似的方面是测试时计算（test-time computation）的重要性：无论是在 Quiet-STaR 还是在 Q* 中，都是人工智能思考的时间越长，结果就越好。</p><p></p><p>据悉，草莓模型是否会在今年年内推出尚不确定，但最初发布的应该是原始模型的精简版本，强调以更少的算力消耗提供类似的性能。自 2023 年 3 月原始模型发布以来，OpenAI 也曾利用这项技术降低 GPT-4 各变体的运行门槛。</p><p></p><p></p><h1>结语</h1><p></p><p></p><p>按照 OpenAI 的猜想，未来 AI 会经历聊天机器人（具有对话语言的人工智能）、推理者（人类水平的问题解决）、代理者（可以采取行动的系统）、创新者（可协助发明的人工智能）、组织者（可以完成组织工作的人工智能）共 5 个阶段，而草莓系统极有可能是帮助其实现第二级 AI（推理者，人类层级的问题解决）的关键。</p><p></p><p>由于推理能力限制，目前 AI 技术在诸如航天、结构工程之类的数学密集型行业中并没有广泛应用。因此，大模型数学推理能力的最佳化成为普及 AI 应用的必备条件，也是 AI 公司进一步提升获利能力的必经之路。</p><p></p><p>谷歌 DeepMind 同样在研究具备高级数学能力的 AI 系统。他们先后开发出 AlphaProof 与 ALphaGeometry 2，后者还在国际数学奥林匹克竞赛中斩获银牌。然而，目前仍不清楚这些模型的扩展与推广效果究竟如何。</p><p></p><p>参考链接：</p><p></p><p><a href="https://the-decoder.com/openais-strawberry-ai-is-reportedly-the-secret-sauce-behind-next-gen-orion-language-model/">https://the-decoder.com/openais-strawberry-ai-is-reportedly-the-secret-sauce-behind-next-gen-orion-language-model/</a>"</p><p></p><p><a href="https://www.theinformation.com/articles/openai-shows-strawberry-ai-to-the-feds-and-uses-it-to-develop-orion">https://www.theinformation.com/articles/openai-shows-strawberry-ai-to-the-feds-and-uses-it-to-develop-orion</a>"</p><p></p><p><a href="https://news.cnyes.com/news/id/5698787">https://news.cnyes.com/news/id/5698787</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9V9jYGvjYd5Nzfb7GHGe</id>
            <title>AI 浪潮下应用开发的“华山论剑” | QCon</title>
            <link>https://www.infoq.cn/article/9V9jYGvjYd5Nzfb7GHGe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9V9jYGvjYd5Nzfb7GHGe</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 09:22:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>随着人工智能技术的突破性进展，AI 应用开发已成为全球技术革新的核心，并以前所未有的速度改变着世界，您准备好了吗？作为企业技术管理者和架构师，您是否在思考如何利用 AI 推动业务强劲增长？</p><p></p><p>AI 应用开发实践涉及到数据采集与处理、机器学习模型的构建与训练、深度学习技术的应用、以及 AI 系统的集成与部署等多个环节。从智能个性化推荐算法到复杂的自动驾驶系统，再到医疗健康领域的精准诊断，AI 正以其独特的方式重塑着各行各业的面貌。我们惊喜地看到从中小创业公司到大型企业，都在利用计算机视觉、自然语言处理、个性化推荐、对话式交互等 AI 能力提升业务效率、优化用户体验，显著增强了产品的市场竞争力。</p><p></p><p>同时我们也关注到，在实际的 AI 应用中，确保 AI 技术与具体业务需求的紧密结合仍然是一个复杂的挑战，涉及到高质量的数据收集与预处理、模型的选型、RAG 等工程扩展技术，去取得更具鲁棒性和灵活性的解决方案。</p><p></p><p></p><blockquote>10 月 18—19 日 QCon 全球软件开发大会（上海站），我们设置了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1721">AI 应用开发实践</a>"】专题，聚焦实战直击痛点！我们邀请了来自字节跳动、百度、阿里巴巴等头部企业的 AI 专家，分享他们在 AI 代码补全、Agent 开发、电商 AI 落地等方面的实战经验，并探讨如何解决数据安全、性能优化、跨平台兼容等关键挑战。目前是<a href="https://qcon.infoq.cn/2024/shanghai/apply">8 折购票</a>"倒计时 2 天，感兴趣的同学抓紧机会。&nbsp;&nbsp;</blockquote><p></p><p><img src="https://static001.geekbang.org/wechat/images/78/78f7726a60773e15db02c833ca9e3464.png" /></p><p></p><h3>精彩内容抢先看</h3><p></p><p></p><h4>演讲主题：豆包 MarsCode 在 AI Coding 的探索与实践</h4><p></p><p><img src="https://static001.geekbang.org/wechat/images/cd/cdb423aa98b6a3b2f7679afb12faf478.png" /></p><p></p><p>自 LLM 用于辅助编码以来，Al 增强开发成为目前最具革命性的技术趋势，人类开发软件的方式正在发生根本性的变化。预计未来几年，世界上每一位工程师都会在 AI 辅助下进行开发，这是一个令人振奋和向往的技术浪潮，豆包 MarsCode 作为浪潮中的一份子，对 AI Coding 技术在编码开发场景的落地有一些自己的见解。本次演讲我们将分享豆包 MarsCode 在 AI Coding 方向的实践和探索，探讨 AI 与编程工具如何深度融合以及我们对未来软件开发形态的展望。</p><p></p><p>演讲提纲</p><p></p><p>1. AI Coding 的演进史和发展趋势</p><p></p><p>2. AI 代码补全和 AI 问答的效果优化实践</p><p></p><p>代码补全关键指标 (CPO)CPO 的定义要素如何用 CPO 衡量代码补全的真实价值代码补全核心实现架构如何降低 debounce，提升注释、补全的续写效果提高尝试率如何通过模型推理优化和网络、压缩等工程侧优化提升反馈速度如何通过推荐时机优化和模型训练提升采纳率基于 CKG 优化 AI 代码知识问答效果实践</p><p></p><p>3. AI Coding 带给 IDE 的变革和工程实践</p><p></p><p>AI IDE 核心三要素：AI 原生交互 + 随时随地开发 + 服务集成化基于前后端分离实现 IDE 模块解耦划分IDE 性能优化：Rust 重构、通道复用、协议压缩、Web Component 化云 IDE 秒级启动实践：基于 K8s 定制池化调度策略、存储热挂载、进程 HotReload</p><p></p><p>4. 未来 AI Coding 的展望</p><p></p><p>谈谈下一代的 AI Coding 技术升级版 AI 代码编辑推荐 (补全 Pro) 和 AutoDebug 的产品化探索未来软件开发趋势展望</p><p></p><p>实践痛点</p><p></p><p>如何更好去优化 AI 代码补全，如何定义指标，如何从算法侧和工程侧如何综合去优化整个链路AI Coding 新技术多且不算特别成熟，如何基于当下评估能力的可用性，做新技术的探索和落地云 IDE 如何做工程化落地 AI 给 IDE 带来的变革</p><p></p><p>演讲亮点</p><p></p><p>字节跳动对于 AI 代码补全和 AI 问答的效果优化实践字节跳动对于 代码推荐 (补全 Pro) &nbsp;和 AI AutoDebug 两类新 AI Coding 场景的技术探索和产品化落地探索业内 Top 级云 IDE 工程</p><p></p><p>听众收益</p><p></p><p>了解目前 AI Coding 领域最前沿的行业动态和知识了解 MarsCode 在 AI 代码补全效果和 AI 问答效果的优化实践了解 AI 代码编辑推荐和 AI AutoDebug 两大即将普惠的 AI Coding 技术和对其在落地形态的思考和探索了解 MarsCode 在 AI IDE 领域关键工程技术和实践经验</p><p></p><h4>演讲主题：百度文心智能体开发实战与分发模式创新</h4><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8c7c802b573fe88f6007c0517e271f2.png" /></p><p></p><p>随着人工智能技术的蓬勃发展，AI Agent 作为智能服务的关键载体正深刻影响着各行各业。本次演讲，我将以“文心智能体”平台的智能体开发实战为蓝本，深入剖析 AI Agent 从理论到实践的构建过程；同时，结合“旅游 AI 助手”的分发实践，探讨 AI Agent 在多样化市场中的高效、安全分发策略。通过这两个具体案例，结合最新的开发工具、框架及最佳实践，揭示 AI Agent 在提升服务效率、优化用户体验方面的独特价值。此外，我们还将直面数据安全、性能优化、跨平台兼容等核心挑战，提出切实可行的解决方案，为听众呈现一场既具深度又具实用性的 AI Agent 开发与分发盛宴。</p><p></p><p>演讲提纲</p><p></p><p>1. 引言</p><p></p><p>Agent 技术背景与定义演讲目的与结构概述</p><p></p><p>2. Agent 开发实战（文心智能体）</p><p></p><p>Agent 应用理论基础技术选型与架构设计：构建高效能 Agent 的基石文心智能体的特点与应用效果</p><p></p><p>3. 分发模式创新（以旅游 Agent 为例）</p><p></p><p>分发策略设计：质量评估、用户体验优化与反馈机制实战案例分析：旅游领域的 Agent 分发实践，包括市场定位、渠道选择、用户反馈等分发成效评估：市场反响、用户满意度、业务增长等关键指标</p><p></p><p>4. 核心难点与挑战及解法建议</p><p></p><p>数据安全与隐私保护：挑战分析、现有策略与未来展望跨平台兼容性与标准化问题：现状剖析、解决方案与标准化路径实战中的其他挑战与应对策略分享</p><p></p><p>5. 总结与展望</p><p></p><p>Agent 应用研发与分发实践的关键点回顾未来发展趋势预测：技术革新、市场变化与用户需求鼓励行业交流与合作，共同推动 AI Agent 技术的持续进步</p><p></p><p>实践痛点</p><p></p><p>数据安全与隐私保护：如何确保 Agent 应用的数据安全与隐私保护成为一大挑战算法性能优化：随着应用场景的复杂化，如何提升 Agent 的算法性能，确保其实时性与准确性成为关键跨平台兼容性与标准化问题：不同平台间的兼容性问题以及缺乏统一的标准框架，限制了 Agent 应用的广泛推广与应用</p><p></p><p>演讲亮点</p><p></p><p>探索最新的 Agent 应用开发范式分享 Agent 应用设计和开发过程中的关键技术考虑，以及在实际应用中的成功案例</p><p></p><p>听众收益</p><p></p><p>深入理解 Agent 技术的理论基础与应用场景掌握 Agent 应用的研发流程与分发策略获得最新 Agent 应用领域的前沿知识和工具应用经验</p><p></p><h4>演讲主题：AI 托管商家经营：1688 电商 AI 落地实战</h4><p></p><p><img src="https://static001.geekbang.org/wechat/images/ba/ba8ff320c70f8808539c2fd65635085c.png" /></p><p></p><p>1688 对接了大量的工厂类型商家，他们普遍存在供应链能力强但是线上运营能力较弱的问题，这类商家在网站上缺乏有效的方法论指导，电商经营的试错成本居高不下，导致流失率较高。如何通过 AI 技术帮助商家提升线上运营能力，从而提升商家的经营效果是一个具有挑战性的命题。本次分享将结合 1688 商家端的 AI 实战，介绍面向商家提供的 AI 智能化服务，包括咨询问答、 客户管理、 商品运营、经营计划等工作，以及业界领先的 AI 经营托管能力，并阐述相关的技术方案和踩坑经验。</p><p></p><p>演讲提纲</p><p></p><p>1. AI 应用的趋势洞察与判断</p><p></p><p>产业 AI 应用观察商家 AI 产业应用深度定义和全景AI 2B 市场和客户画像分析</p><p></p><p>2. 商家 AI 场景的应用实战</p><p></p><p>各类 AI 技术在商家领域内的应用图文 GC：隐藏在标题、图片里的坑问答：多轮对话牵引商家行动诊断归因：业界难题，我的流量为什么跌了</p><p></p><p>3. 商家 AI 托管模式探索</p><p></p><p>AI 托管的几大障碍核心技术架构和关键技术点AI 经营计划的版本升级AgentSwarm 模式如何工作巧用营销模型让商家 AI 价值最大化</p><p></p><p>4. 未来商家域 AI 的空间和路线</p><p></p><p>实践痛点</p><p></p><p>大模型当前的知识储备和推理能力依然不足，很多命题必须通过 LLM+DL+ML+ 工程方案求解Agent 是个美好的概念，但是落地过程中有诸多水土不服，需要重新认识</p><p></p><p>演讲亮点</p><p></p><p>业内领先的 AI 经营托管的技术，通过经营计划的一揽子方案接管商家的线上经营，并取得不错的业务结果在 RAG 应用、AI 归因分析、AgentSwarm 模式等方面有一定的探索和结果</p><p></p><p>听众收益</p><p></p><p>了解商家端 AI 应用全景和阿里体系商家 AI 探索路径了解各类 AI 技术在商家端业务里的坑以及常见商家端问题的解法了解 AI 托管的模式创新、问题和解法思路</p><p></p><p>更多精彩内容将在 10 月 18 - 19 日 QCon 上海站为您现场呈现，期待与您共赴这场技术之约。如果您有好的技术实践案例想要与我们分享，欢迎<a href="https://jinshuju.net/f/EbrZFg">点击链接</a>"提交演讲申请。</p><p></p><p>会议推荐</p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6</id>
            <title>NVIDIA H20与计算领域的革命：深入解析算力评估与应用</title>
            <link>https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fVwFv0bBwCKAN5tm1Cc6</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 07:40:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>在当今快速发展的科技时代，计算能力的重要性毋庸置疑。无论是在人工智能、深度学习还是高性能计算领域，算力的强弱决定了创新的速度与效果。作为NVIDIA最新推出的顶级显卡，H20以其强大的硬件配置和卓越的实际表现，吸引了众多关注。本文将深入探讨算力的概念、评估方法，以及在现代计算任务中的应用，特别是如何利用NVIDIA H20显卡来最大化算力优势。我们将结合理论与实际数据，全面分析H20的独特价值与未来发展方向。</blockquote><p></p><p></p><p></p><h1>算力的概念与历史演进</h1><p></p><p>1.1 算力的定义与基本概念</p><p></p><p>算力，或计算能力，是指计算设备在单位时间内所能完成的计算量。通常情况下，算力以每秒浮点运算次数（FLOPS）来衡量，这是浮点运算能力的标准单位。FLOPS代表每秒能进行的浮点数运算的次数，因此FLOPS越高，设备的计算能力越强。</p><p></p><p>在实际应用中，计算任务的种类繁多，从科学计算到深度学习模型训练，从金融数据分析到自动驾驶系统，各类任务对算力的需求各不相同。计算能力不仅仅是一个硬件性能指标，更是决定技术可行性和应用效果的重要因素。随着科技的进步，计算任务变得越来越复杂，数据量也在不断增加，因此对高算力的需求变得日益迫切。</p><p></p><p>1.2算力的发展历程</p><p></p><p>计算能力的发展可以追溯到计算机的早期历史。从最初的机械计算机到电子计算机，再到现代的超级计算机，计算能力的提升伴随着硬件技术的飞跃。早期的计算设备如ENIAC，每秒只能完成几千次简单的加法运算，而今天的超级计算机每秒可以完成数千万亿次浮点运算。</p><p></p><p>随着时间的推移，计算设备从单一处理器发展到多核处理器，再到并行计算和分布式计算。尤其是在图形处理单元（GPU）领域，NVIDIA等公司通过不断优化硬件架构，显著提升了计算能力。现代GPU如H20显卡，不仅在图形处理上表现优异，在并行计算、深度学习和科学模拟等领域也展现了强大的算力。</p><p></p><p>1.3 计算能力的重要性</p><p></p><p>计算能力是现代科技发展的基础。从物理模拟到分子建模，从图像识别到自然语言处理，强大的计算能力使得这些复杂任务得以实现。特别是在人工智能领域，深度学习模型的训练依赖于海量的数据和复杂的计算，因此对算力的要求极高。</p><p></p><p>在金融领域，高速交易系统依赖于实时的数据分析和决策，这些操作需要在微秒级别内完成，因此需要极高的计算能力。同样，在自动驾驶领域，车辆需要在短时间内处理来自多个传感器的数据，并做出驾驶决策，这也需要强大的算力支持。可以说，算力不仅是硬件性能的体现，更是推动科技进步的重要引擎。</p><p></p><p></p><h1>算力的评估与衡量方法</h1><p></p><p></p><p>2.1 评估算力的标准与方法</p><p></p><p>评估算力涉及多个方面，包括理论计算能力、实际执行效率和任务特定的表现。以下是几种常用的评估标准：</p><p></p><p>#1.&nbsp;FLOPS（每秒浮点运算次数）</p><p></p><p>FLOPS是评估计算能力的最直接指标，它表示硬件在一秒钟内能够完成的浮点运算次数。计算能力越高，硬件处理数据和执行任务的速度就越快。FLOPS通常分为单精度（FP32）、双精度（FP64）和混合精度（FP16、BFLOAT16等）不同类型，根据任务的不同，使用的精度类型也会不同。</p><p></p><p>#2.&nbsp;带宽（Bandwidth）</p><p>带宽指的是在单位时间内能够传输的数据量。内存带宽是决定计算设备性能的关键因素之一，尤其是在需要处理大量数据的任务中。高带宽可以有效减少数据传输的瓶颈，从而提高整体计算效率。在GPU计算中，带宽不仅影响数据加载的速度，也直接影响到模型训练的速度。</p><p></p><p>#3.&nbsp;延迟（Latency）</p><p>延迟是指从输入数据到获得输出结果所需要的时间。低延迟有助于减少数据传输和处理过程中的等待时间，特别是在并行计算中，减少延迟可以显著提高计算效率。延迟通常是并行计算系统的瓶颈，尤其是在大规模数据处理或多GPU协同工作时。</p><p></p><p>#4.&nbsp;能效比（Efficiency Ratio）</p><p>能效比是单位功耗下的计算能力。高能效比意味着在相同的功耗下，硬件能够提供更高的计算能力，这对于数据中心和高性能计算集群尤为重要。在实际应用中，能效比不仅影响计算成本，还影响系统的冷却和维护需求。</p><p></p><p>2.2 模型训练和推理中的算力评估</p><p></p><p>在深度学习和机器学习中，算力的评估往往与具体的任务需求挂钩。以下是几种常见的评估标准：</p><p></p><p>#1.&nbsp;训练速度（Training Speed）</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：训练速度通常以每秒处理的样本数（Samples per Second, SPS）或每秒处理的tokens数（Tokens per Second, TPS）来衡量。&nbsp;计算方式&nbsp;：SPS和TPS的计算方式如下：</p><p>SPS = 处理的样本总数 / 训练时间（秒）</p><p>TPS = 处理的tokens总数 / 训练时间（秒）</p><p></p><p>在计算过程中，样本数指的是输入数据的批次（Batch Size），而tokens数通常用于自然语言处理（NLP）模型的训练，指的是输入文本被分割后的最小单位（如词语或子词）。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>训练速度是衡量计算设备在模型训练过程中效率的关键指标。更高的训练速度意味着模型可以在更短的时间内处理更多的数据，从而加速模型的整体训练进程。这对于处理大型数据集或复杂模型（如深度神经网络、卷积神经网络等）尤为重要。</p><p></p><p>在实际应用中，提升训练速度有助于：</p><p>缩短模型的开发周期。提高资源的利用率，减少计算成本。在相同时间内进行更多实验，从而优化模型效果。</p><p></p><p>特别是在深度学习领域，使用更大的批次处理数据可以显著提高SPS或TPS，而高效的硬件如NVIDIA H20显卡能够支持更大的批次大小和更快的数据处理，从而提升训练速度。</p><p></p><p>#2.&nbsp;模型收敛性（Convergence）</p><p># 评估单位与计算方式 #</p><p></p><p>&nbsp;单位&nbsp;：模型收敛性没有统一的度量单位，但通常以训练轮数（Epochs）、迭代次数（Iterations），或达到某个性能指标所需的时间来衡量。&nbsp;计算方式&nbsp;：</p><p>收敛速度 = 目标性能指标 / 训练时间（秒）</p><p>或者使用收敛的轮数来衡量，即训练到模型性能稳定为止所需的训练轮数或迭代次数。</p><p></p><p>例如，在一个深度学习任务中，收敛速度可以通过模型达到一定的准确率或损失函数值所需的时间来表示。更少的训练轮数或迭代次数意味着更快的收敛速度。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>收敛性是衡量模型在训练过程中逐步逼近最优解的能力。算力越强，通常收敛速度越快，因为高算力设备可以支持更大的批次大小、更复杂的优化算法和更快的数据处理速度。这对于研究和开发时间有限的项目至关重要，因为加快收敛速度可以更快地得到有效的模型。</p><p></p><p>在实际应用中，收敛性与以下因素密切相关：</p><p>优化算法：如Adam、SGD等优化算法的选择和调整，直接影响模型的收敛速度。批次大小：更大的批次大小通常会加快收敛速度，但需要足够的显存支持，这也是高算力设备的优势。学习率：调整学习率可以帮助模型更快地达到收敛状态，但需要精细的调试以避免过拟合或欠拟合。</p><p>使用像NVIDIA H20这样具备高算力和大显存的设备，可以在保证计算精度的同时，加快模型的收敛速度。</p><p></p><p>#3.&nbsp;推理速度（Inference Speed）</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：推理速度通常以每秒处理的样本数（Samples per Second, SPS）或每秒处理的tokens数（Tokens per Second, TPS）来衡量，类似于训练速度。&nbsp;计算方式&nbsp;：SPS和TPS的计算方式如下：</p><p></p><p>SPS = 处理的样本总数 / 推理时间（秒）</p><p>TPS = 处理的tokens总数 / 推理时间（秒）</p><p></p><p>推理速度评估的是模型在实际应用中的响应时间，特别是在实时或近实时的应用中（如自动驾驶、语音识别、在线推荐系统等）。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p>推理速度是决定模型在生产环境中表现的关键指标之一。特别是在需要实时处理和响应的应用中，推理速度直接影响系统的用户体验和效能。</p><p>推理速度越快，系统的响应时间就越短，这对于以下场景尤为重要：</p><p></p><p>自动驾驶：车辆必须在极短时间内处理传感器数据并作出驾驶决策。实时翻译与语音识别：需要在用户发出命令后迅速给出响应。在线推荐系统：实时分析用户行为并推荐个性化内容。</p><p></p><p>NVIDIA H20显卡在推理任务中的表现尤为出色，特别是在FP8低精度计算中，能够在保持高效能的同时，提供极快的推理速度。</p><p></p><p>#4.&nbsp;精度与效率的平衡</p><p># 评估单位与计算方式 #</p><p>&nbsp;单位&nbsp;：精度通常以百分比（%）或数值（如损失值、准确率等）来表示；效率则以处理速度或能效比（FLOPS/Watt）来衡量。&nbsp;计算方式&nbsp;：</p><p></p><p>精度 = 模型在测试数据集上的性能指标（如准确率、F1分数等）</p><p>效率 = 计算资源消耗 / 达到目标性能所需的时间或能量。</p><p></p><p>在深度学习中，精度和效率往往需要进行权衡。例如，高精度计算通常需要更多的计算资源和时间，而低精度计算则可以在速度和资源占用上实现更高的效率。</p><p></p><p>#&nbsp;重要性与实际应用&nbsp;#</p><p></p><p>在实际应用中，精度与效率的平衡是设计和部署AI系统时必须考虑的重要因素。虽然追求更高的精度是许多AI任务的目标，但在某些场景下，高精度并不是唯一的考量。例如：</p><p>边缘计算设备：受限于计算资源和能耗，可能需要在精度和效率之间做出妥协。实时应用：如语音助手或实时翻译，更快的响应速度可能比绝对精度更重要。低成本部署：在大规模部署中，能够以更低的成本达到“足够好”的精度，可能比追求极限精度更具现实意义。</p><p></p><p>NVIDIA H20显卡提供了多种浮点运算模式（如FP16、FP8），允许开发者根据任务需求选择合适的精度和效率组合。例如，在训练阶段使用FP16混合精度可以提高训练速度，而在推理阶段使用FP8可以进一步优化性能，同时保持足够的预测精度。</p><p></p><p></p><h1>Part 3 NVIDIA H20显卡的深入解析</h1><p></p><p></p><p>3.1 H20显卡的硬件架构与技术创新</p><p></p><p>NVIDIA H20显卡基于最新的Hopper架构，在图形计算和并行计算领域引领了新一轮的技术革命。与前几代基于Ampere架构的显卡相比，H20在多个方面进行了大幅升级。尤其是在FP32、FP16以及新增的FP8精度计算能力上，H20展现了其在各种复杂计算任务中的卓越性能。</p><p></p><p>根据提供的图表，H20在FP32单精度浮点运算中达到了44 TFLOPS，这远高于基于Ampere架构的前代产品的19.5 TFLOPS。这一提升对于需要高精度计算的任务，如媒体处理、物理模拟等，具有重大意义。</p><p>在FP16和FP8的Tensor Core性能上，H20也大幅领先于前代产品。在FP16运算中，H20达到了148 TFLOPS，而在FP8的8bit浮点数据类型运算中，H20的性能更是达到了296 TFLOPS。这使得H20在处理需要大量并行计算的任务时，如深度学习模型的训练和推理，具备了极大的优势。</p><p></p><p>3.2 显存与带宽的优越性</p><p></p><p>H20显卡配备了96GB的HBM3显存，这是当前显存配置的顶级标准。这种显存不仅在容量上远超前代产品（80GB HBM2e），在内存带宽上也达到了惊人的4 TB/s，是前代产品带宽的近两倍。如此高的内存带宽使得H20显卡在处理大规模数据集和高分辨率任务时，能够更快地进行数据传输，减少处理延迟。</p><p>对于大模型训练和深度学习应用来说，显存的大小和带宽直接决定了硬件能否有效载入和处理训练数据。H20显卡凭借其96GB的显存，可以轻松应对需要大批量数据的任务，同时其4TB/s的带宽也确保了这些数据能够快速传输到GPU进行处理，这对于需要高效处理数据的任务如自动驾驶、图像识别等尤为重要。</p><p></p><p>3.3 H20的计算能力与实际表现</p><p></p><p>通过分析H20的计算能力图表，我们可以看到它在FP8、FP16等精度下的强劲表现。特别是在处理需要高效浮点运算的任务时，H20的Tensor Core能够提供前所未有的计算性能。例如，在8bit浮点数数据类型的FP8精度运算中，H20的性能达到了296 TFLOPS，适合用于量化训练和模型推理等场景。</p><p>NVLink互联带宽方面，H20也进行了显著的提升。相比前代产品的600GB/s和400GB/s，H20的NVLink带宽高达900GB/s。这意味着多个H20显卡在多卡互联时可以通过更高效的方式进行数据交换，减少了多GPU协同工作的延迟，从而提高了整体计算效率。</p><p></p><p>3.4 浮点运算模式的选择与H20的应用场景</p><p></p><p>在NVIDIA H20显卡中，不同的浮点运算模式为各种计算任务提供了灵活的选择。H20显卡支持从双精度（FP64）到最新的FP8低精度运算模式，覆盖了从高精度科学计算到高效推理任务的广泛应用需求。</p><p>双精度运算模式（FP64）：通常用于需要极高精度的科学和工程计算，如流体力学模拟、气候预测等领域。单精度运算模式（FP32）：是深度学习领域的主力，特别是在训练大型AI模型时，FP32能够提供足够的精度和较高的计算效率。半精度运算模式（FP16）：近年来在深度学习加速方面获得了广泛应用，尤其是在卷积神经网络（CNN）等任务中，FP16能够显著提高训练速度并减少显存占用。低精度运算模式（FP8和INT8）：随着量化技术的发展，FP8和INT8在推理任务中的应用越来越多，H20显卡的296 TFLOPS FP8算力使其在大规模模型推理中占据了显著优势。</p><p></p><p>3.5不同GPU型号的选择:SXM、PCIe、NVLink</p><p></p><p>为了满足不同用户的需求，NVIDIA为H20显卡提供了多种型号，包括SXM、PCIe和NVLink。这些型号的区别在于其硬件架构和连接方式，进而决定了它们在不同应用场景中的适用性。</p><p>SXM版：通过SXM模块设计，可以实现8块GPU的紧密互联，主要应用于高密度GPU服务器集群，如NVIDIA的DGX系统。这种设计不依赖传统的PCIe接口，而是通过NVSwitch实现更高的带宽和更低的延迟，特别适合用于超大规模AI训练和科学模拟任务。PCIe版：沿用了传统的PCIe接口，提供了更为灵活的部署方式。它支持与主板和CPU之间的直接通信，适用于传统的GPU服务器和通用计算任务。每两块GPU通过NVLink Bridge进行连接，虽然在带宽上不如SXM版，但在扩展性和兼容性上具有一定的优势。NVLink版：专为需要超高带宽的数据密集型任务设计。它提供了高达7.8 TB/s的传输带宽，适用于需要实时处理大量数据的大规模语言模型（LLM）训练任务。通过NVLink接口，多个H20显卡可以实现高速数据交换，减少计算过程中数据传输的瓶颈，提升整体计算效率。</p><p></p><p>通过结合这些不同型号的特点和应用场景，用户可以根据自己的具体需求选择最适合的GPU类型，从而在不同的计算任务中最大化地发挥H20显卡的性能优势。</p><p></p><h1>Part 4 H20在模型训练与推理中的实际应用</h1><p></p><p></p><p>4.1 通过Llama2模型探讨H20的应用</p><p></p><p>为了更好地理解H20显卡在实际应用中的表现，我们可以借助Llama2-70B模型的训练和推理数据来分析其性能。根据提供的图表，H20显卡在不同精度（FP8和FP16）以及不同输入输出长度下展现了卓越的计算能力。</p><p></p><p>在图表中，HGX H20模块在处理LLAMA2_70B模型时，在FP8精度下，输入长度为2048、输出长度为128的配置中，H20的吞吐量达到了1.2244595*A tokens/秒。而在输入长度为128、输出长度为2048的配置中，FP8精度下的吞吐量更是高达2.0981547*B tokens/秒。这表明在高精度和复杂模型的训练和推理任务中，H20显卡能够提供极为高效的计算性能。</p><p></p><p>相比之下，HGX A1XX模块在相同配置下，使用FP16精度的表现明显不如H20。这进一步证实了H20显卡在处理大规模语言模型时的优势。特别是在需要处理复杂输入输出关系的推理任务中，H20的高带宽和Tensor Core的强大性能，使其能够在更短的时间内完成推理，提供更高的吞吐量。</p><p></p><p>4.2 H20在推理任务中的独特优势</p><p></p><p>推理任务中，吞吐量和响应速度是两个关键指标。H20显卡凭借其FP8精度的计算能力，在处理LLAMA2_70B等大规模模型时，能够提供更高的tokens处理速度。结合前面的数据分析，H20在推理任务中的表现不仅仅体现在其计算能力上，还得益于其大容量的显存和超高的内存带宽，这些因素共同作用，使得H20能够在处理复杂推理任务时，保持高效和准确的性能输出。</p><p></p><p>通过NVLink的高带宽支持，多个H20显卡在多卡集群中可以实现高效的数据交换和协同计算，这对于需要实时处理和分析数据的任务来说，至关重要。例如，在自动驾驶系统中，H20显卡可以通过快速处理传感器数据并作出决策，从而提高系统的安全性和反应速度。</p><p></p><p>4.3 H20的GEMM性能分析</p><p></p><p>在矩阵乘法（GEMM）任务中，浮点运算性能是评估GPU计算能力的重要指标之一。以下是从表格中筛选出的伊迪雅H20在不同浮点精度下的GEMM性能数据，并对其进行详细分析。</p><p><img src="https://static001.geekbang.org/infoq/a3/a3e16713905cdb6c63c8cc433b76ba02.png" /></p><p>分析与解读</p><p># FP8精度 #</p><p>峰值性能：293 TFLOPS实测性能：267.33 TFLOPS峰值百分比：91.25%</p><p></p><p>FP8精度下，伊迪雅H20的实测性能达到了267.33 TFLOPS，占峰值性能的91.25%。这一结果表明在低精度浮点运算中，伊迪雅H20的表现非常接近其理论最大值，表明其硬件设计在FP8运算任务中的效率极高，适合用于大规模模型推理和量化训练等场景。</p><p></p><p># INT8精度 #</p><p>峰值性能：293 TFLOPS实测性能：188.30 TFLOPS峰值百分比：64.27%</p><p></p><p>在INT8精度下，伊迪雅H20的实测性能为188.30 TFLOPS，占峰值性能的64.27%。虽然相对于FP8的表现有所下降，但INT8仍然提供了高效的计算能力。INT8精度广泛应用于需要处理大量数据的推理任务，尤其在资源受限的环境下，可以在降低计算复杂度的同时，保持合理的精度。</p><p></p><p># FP16精度 #</p><p>峰值性能：147 TFLOPS实测性能：141.55 TFLOPS峰值百分比：96.31%</p><p></p><p>在FP16精度下，伊迪雅H20几乎达到了其峰值性能，实测值为141.55 TFLOPS，占峰值的96.31%。这表明伊迪雅H20在FP16运算中能够充分发挥其硬件潜力，非常适合用于深度学习训练任务，特别是卷积神经网络（CNN）和递归神经网络（RNN）等对计算速度要求较高的模型。</p><p></p><p># TF32精度 #</p><p>峰值性能：74 TFLOPS实测性能：69.47 TFLOPS峰值百分比：93.88%</p><p></p><p>TF32是一种介于FP16和FP32之间的浮点精度模式，旨在提供比FP32更高的计算效率，同时保留一定的计算精度。在这一模式下，伊迪雅H20的实测性能为69.47 TFLOPS，占峰值性能的93.88%。这一表现说明TF32是一个平衡精度和效率的理想选择，特别是在要求较高的科学计算和AI模型训练中，能够显著提升计算速度。</p><p></p><p># FP32精度 #</p><p>峰值性能：40 TFLOPS实测性能：31.41 TFLOPS峰值百分比：78.53%</p><p></p><p>在FP32精度下，伊迪雅H20的实测性能为31.41 TFLOPS，占峰值性能的78.53%。虽然相对其他精度模式的效率稍低，但FP32依然是许多AI模型和科学计算任务的首选精度模式，特别是在需要高精度结果的场景中，FP32的稳定表现非常重要。</p><p></p><p>H20在不同精度下的GEMM性能分析，我们可以看到其在多种运算模式中的强大表现。无论是在高效推理任务中的FP8和INT8，还是在深度学习训练中的FP16和TF32，伊迪雅H20都能够提供接近其理论峰值的实际性能。这表明H20显卡不仅具备出色的硬件设计，还能在实际应用中充分发挥其计算能力，适合于从AI模型训练到大规模推理等广泛应用场景。</p><p></p><h1>Part 5 NVIDIA H20与H100的深入对比</h1><p></p><p></p><p>5.1 H100显卡的优势与应用场景</p><p></p><p>NVIDIA H100显卡是目前市场上最强大的GPU之一，其高达1979 TFLOP的理论计算能力，使得H100在处理高精度计算任务时具备无可比拟的优势。H100显卡的性能密度高达19.4，远超H20显卡的2.9，这使得H100在单位面积内能够提供更高的计算能力，特别适用于空间受限但需要高性能的计算环境。</p><p>在实际应用中，H100显卡主要应用于高精度科学计算、复杂AI模型训练和大规模数据分析等领域。对于那些需要极致性能的用户，H100显卡无疑是最佳选择。例如，在气候模拟、分子动力学和高精度物理模拟等任务中，H100显卡可以显著加快计算速度，减少模拟时间。</p><p></p><p>5.2 H20显卡的核心价值与独特优势</p><p></p><p>尽管H20显卡在理论计算能力上不如H100，但其在实际应用中的表现依然出色。特别是在大规模低精度模型训练和推理中，H20凭借其高显存、大带宽和较低的成本，展现了极高的性价比。对于那些需要处理大量数据且对计算精度要求不高的任务，如自然语言处理、推荐系统、图像识别等，H20显卡是一个非常具有竞争力的选择。</p><p></p><p>H20显卡的核心价值在于其出色的内存管理和高效的计算能力，特别是在FP8精度下，H20显卡能够以更少的节点数量完成训练任务，从而降低整体计算成本。这使得H20显卡在一些预算敏感的项目中，成为了性价比最高的解决方案。</p><p></p><h1>Part 6 H20在工业领域的广泛应用</h1><p></p><p></p><p>除了在AI研究中的广泛应用外，H20显卡在工业领域也展现了巨大的应用潜力。无论是在自动驾驶、智能制造，还是金融科技和医疗健康，H20显卡都能够通过其强大的计算能力，为各类复杂的计算任务提供解决方案。</p><p></p><p>在自动驾驶领域，H20显卡的高带宽和低延迟使得其能够实时处理来自多个传感器的数据，并做出驾驶决策。智能制造领域的复杂工艺模拟和优化，同样可以通过H20显卡的高算力得到加速。而在金融科技领域，H20显卡的快速数据处理能力可以显著提升高频交易系统的响应速度，降低市场风险。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/40q0yogaX2i13pOKQQar</id>
            <title>AI技术如何深入各行各业？Intel AI Summit专场全栈落地实践分享丨AICon</title>
            <link>https://www.infoq.cn/article/40q0yogaX2i13pOKQQar</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/40q0yogaX2i13pOKQQar</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 03:16:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在当今时代，人工智能技术正以前所未有的速度迅猛发展，企业落地人工智能应用已成为不可逆转的趋势。然而，这一过程中也伴随着诸多挑战和问题。如何助力企业加速人工智能的落地进程，如何最大限度地提升 IT 系统的资源利用率，以及如何有效增强计算效能，并便捷、稳定地部署应用 AI，都已成为整个行业关注的焦点。</p><p></p><p>8 月 18 日至 8 月 19 日，在上海举办的 AICon 2024 全球人工智能开发与应用大会上，诸多讨论都聚焦于这些问题。其中的 Intel AI Summit 「AI 全栈解决方案及行业实践」专场，来自英特尔及其合作伙伴的四位行业专家就 AI 应用落地的全栈解决方案，以及医疗领域的应用案例进行了深入探讨。</p><p></p><p></p><h2>激发 AI 潜能：xFT 助力算力解锁，最大化提升计算效能</h2><p></p><p></p><p>尽管“AI 赋能”具有巨大吸引力，但在资源有限的条件下，企业必须确保每一项投入都能获得最大化的效益，这对应用落地的成本和利用率提出了严格的要求。特别是在计算效能提升方面，需要硬件、软件、算法等多个层面的协同优化。高效的计算能力能够加速数据处理、模型训练和推理速度，使企业更迅速地做出明智决策，推出创新产品和服务。</p><p></p><p>为了解决算力对 AI 落地的限制，英特尔一直走在行业前列，力求为企业推出实用可靠的计算资源方案，通过技术优化最大化提升算力，推动大模型应用的落地，充分释放 AI 潜能。在本次会议上，英特尔数据中心和 AI 事业部首席工程师何普江带来了主题为《xFT 解锁至强算力，释放 AI 潜能》的演讲。</p><p></p><p>何普江认为，AI 的未来将由算力的突破来定义，而第五代英特尔®️&nbsp;至强®️&nbsp;可扩展处理器及其内置的英特尔®️&nbsp;AMX 技术正是这一突破的关键。英特尔®️ AMX 通过深度优化矩阵运算，为算力释放提供了坚实的硬件基础。</p><p></p><p>在演讲中，何普江分享了 xFT（xFasterTransformer）技术的设计理念：这是一个专为 AMX 优化的开源项目，不仅支持广泛的 AI 模型和数据类型，更通过软硬件的深度融合，显著加速了 AI 大模型推理。何普江也在分享中提到，与传统方法相比，目前通过 xFT 技术，可以在第五代处理器上面用 48 核跑出高达 1300 的 CRGPU 吞吐量，这一数字远超行业标准，在处理大规模数据集和复杂运算时表现出色。</p><p></p><p>在算法层面，何普江深入分享了 xFT 技术的多项创新，包括对 oneDNN 库的优化使用，以及针对不同 token size 优化的 Slim attention 机制。这些创新不仅提升了 xFT 技术的性能，也为 AI 社区提供了宝贵的实践经验。</p><p></p><p>探讨大模型与小模型的未来发展时，何普江指出，两者各有优势，将共同推动 AI 技术进步。他强调了多模态和 RAG 技术的重要性，并预测开源与闭源模型间差距将缩小。何普江还提到了 KV Cache 的关键作用，以及它对未来 AI 系统设计的影响。</p><p></p><p>他认为，随着硬件和软件的不断进步，大语言模型的成本将大幅降低，推动 AI 技术的更广泛应用和深入发展。开源与闭源模型间的差距正在缩小，未来开源模型将在 AI 领域扮演更加重要的角色。</p><p></p><p></p><h2>GenAI 开放平台 OPEA：一站式助力大模型应用，企业 AI 落地加速器？</h2><p></p><p></p><p>除了底层算力效能提升之外，在目前企业的 AI 应用实践中，还存在着训推优化、基础设施扩展、数据传输安全、应用碎片化等诸多环节。企业需要一个能够全栈助力落地 AI 应用的方案与平台，一站式解决生成式 AI 的落地问题，在性能优化、可扩展性、安全等角度为企业保驾护航。</p><p></p><p>在本次会议上，英特尔 AI 首席工程师吴震华围绕 OPEA 开放平台进行了分享。作为人工智能建模、特征工程、效果分析以及推荐增强等领域的资深专家，他在演讲《基于检索增强的企业 GenAI 开放平台落地实践》中详细梳理了 AI 技术的发展历程，并深入解析了英特尔企业 GenAI 开放平台（OPEA）的架构与底层技术。</p><p></p><p>吴震华认为，尽管基于检索增强的 RAG 技术并非新生事物，但其在企业中的应用潜力正随着大语言模型的能力而日益凸显。在吴震华看来，企业 AI 落地面临的挑战与机遇并存，特别是在生成式 AI 技术，如 ChatGPT 引爆市场之后，行业关注的焦点已从模型预训练的竞争转向了具体的应用落地。</p><p></p><p>OPEA 开放平台是一个由英特尔推动、捐赠给 Linux 基金会的开源项目。OPEA 旨在构建一个开放的生态系统，使企业能够快速利用大语言模型和 AI 技术带来的创新优势。吴震华详细介绍了 OPEA 的全栈架构，从基础设施层到平台集成层，再到面向用户的服务层，展示了一个多层次、模块化的 AI 应用平台。</p><p></p><p>展望未来，吴震华预计到 2028 年，80% 以上的商用 PC 将被新形态的 AIPC 所替代。他将企业 AI 应用的发展分为三个阶段：今天，AI 助手如 CO-Pilot 和 RAG 正在提升数据检索和编程流程的效率；明天，智能体将拥有更大的自主权，利用 AI 的推理能力完成特定任务；未来，AI 将深入企业流程的每个环节，优化每个生产要素。</p><p></p><p>吴震华还提出了企业 AI 应用的四个关键方向：易用性、开放性、安全性、负责任的使用，以及平台的可扩展性和参考实践的提供。他希望通过这些方向的努力，使企业 AI 快速享受到生成式 AI 革命的技术成果。</p><p></p><p>在演讲的最后，吴震华通过一个应用 demo 展示了低代码的基于至强®️&nbsp;微服务实现生成 AI 服务功能，他期待通过不断的迭代和更新，OPEA 能够推动企业 AI 方案的发展，方便快速地帮助企业用户解决实际的问题，让企业真正享受到生成式 AI 技术带来的红利。</p><p></p><p></p><h2>AI+ 医疗：大模型在病历质控中的应用实践</h2><p></p><p></p><p>生成式 AI、大模型技术正在为各行各业带来革命性的变化，医疗领域也不例外。在医院、健康机构等场景下，AI 辅助诊疗、病历质控等应用将成为未来技术趋势。惠每科技致力于通过人工智能解决方案提升医疗质量，守卫患者安全，在智能化诊疗、病历质控等技术领域不断创新大模型技术应用，推动医疗行业的数智化发展。在 Intel AI Summit 专场上，惠每科技算法专家凌鸿顺以《破解病历质控难题：医疗大模型质控优化策略》为主题，分享了惠每科技在病历质控领域的成功实践。</p><p></p><p>病历质控作为医疗质量评估的核心，直接影响医疗服务水平和患者安全。面对病历书写的及时性、规范性和完整性问题，惠每科技采用了大模型技术，利用其强大的文本理解和知识推理能力，有效提升了病历质控的效率和准确性。大模型基于 Transformer 架构，通过持续预训练和任务对齐，以及直接偏好优化，显著提高了对病历中关键信息的提取和分析能力。</p><p></p><p>凌鸿顺还提到，在模型训练优化方面，惠每科技采取了基座模型优化和大模型 prompt 工程优化的策略。通过知识注入、指令跟随和直接偏好学习，模型能够更好地理解和执行医疗领域特定的任务。特别是在处理病历中的矛盾和不规范问题时，大模型展现了其跨字段理解和医疗知识对比的优势。</p><p></p><p>惠每科技还制定了自动化 Few-shot 示例的方案，通过初始化阶段的 badcase 识别和迭代优化，以及相似度计算和多样性 prompt 的加入，进一步提升了模型的预测效果和泛化能力。这一策略不仅减轻了筛选 Few-shot prompt 的工作量，也为不同医院的特殊 case 提供了快速修复的可能。</p><p></p><p>凌鸿顺还提到，在大模型部署推理的实践中，惠每科技与英特尔的合作成果显著。通过xFasterTransformer、BigDL 量化方案和&nbsp;OpenVINO™️&nbsp;非量化方案，实现了医疗模型私有化部署的优化，解决了大模型在硬件资源和计算效率上的挑战。特别是英特尔®️&nbsp;AMX 技术的应用，为大模型的推理性能带来了质的飞跃。</p><p></p><p>展望未来，凌鸿顺对医疗大模型的应用持乐观态度。模型蒸馏技术有望将大模型的效果转移到更小、更易于部署的模型上。自动化 Few-shot 的进一步优化，将实现更高效、更准确的病历质控。同时，惠每科技也将与英特尔展开持续合作，进一步推动医疗 AI 技术的创新和应用，为医疗行业带来更多的价值和可能性。</p><p></p><p></p><h2>医疗 AI 革新：大模型技术深度融合与应用实践</h2><p></p><p></p><p>人工智能技术的发展对医疗行业的信息化升级和数智化变革具有重大意义。以国内医疗场景为例，大量专业化数据和对信息化处理的精准度要求极高，这些都是信息化过程中需要解决的实际问题。垂直领域的大语言模型将成为新一代医疗信息化系统的有力助手，帮助解决医疗系统中的诸多问题。然而，如何让医疗大模型真正可用、易用，仍需解决模型构建、集成、系统结合和应用设计的一系列问题。</p><p></p><p>在 Intel AI Summit 专场上，卫宁健康研发总监刘鸣谦带来了题为《大语言模型在医疗场景的落地实践》的分享，深入探讨了大模型技术如何深刻影响医疗信息化的发展和临床应用。</p><p></p><p>刘鸣谦首先回顾了医疗系统的发展历程，从早期的专家系统、本体推理到现代基于 AI 的图像辅助诊断和自然语言处理。她认为，自 OpenAI GPT3.5 发布以来，基于 Transformer 的大模型已成为医疗领域开发和应用的新范式。大模型的文本生成能力、推理能力和交互能力，为医疗领域带来了前所未有的创新潜力。</p><p></p><p>在数据工程方面，刘鸣谦分享了卫宁健康如何通过高质量的数据集和场景化处理来优化大模型的训练效果。通过上下文学习、RAG（Retrieval-Augmented Generation）和 Agent 方式，进一步提升了模型的效果和适应性。此外，通过直接偏好优化（DPO）和提示工程，模型在医疗场景中的适用性得到了显著提升。</p><p></p><p>卫宁健康的大模型训练采用了多轮迭代，结合了开源数据和自身积累的医疗知识，形成了强大的模型能力。刘鸣谦提到，卫宁健康开源了多款垂直领域大模型，以促进社区的交流和发展，并与英特尔合作，优化了基于英特尔®️&nbsp;AMX 技术的本地化部署方案，有效降低了成本同时保证了高性能。</p><p></p><p>卫宁团队开发的 Copilot，作为信息化系统和 AI 模型之间的桥梁，通过 API 插件等多种形式，实现了不同应用场景下的模型管理和服务。Copilot 的应用，使得医务人员能够无缝地体验到 AI 带来的便利。</p><p></p><p>在医疗应用场景方面，刘鸣谦详细介绍了大模型在医技、临床和管理场景下的实际应用案例。例如，在影像科中，大模型辅助医生快速生成报告，提高了工作效率；在超声科中，实现了实时报告质控，提升了医疗质量；在临床辅助诊断中，通过增强型 CDSS 系统，提供了更加精准的辅助决策支持。</p><p></p><p>此外，大模型还在病历文书助手和智能语言查房助手中发挥了重要作用，通过语音识别和自然语言处理技术，实现了医生口述内容的自动结构化输出，极大地提高了医生的工作效率。</p><p></p><p></p><h2>AI 企业落地，从概念走向现实</h2><p></p><p></p><p>随着人工智能技术的不断成熟和创新，其在企业中的应用已不再是遥远的梦想，而是触手可及的现实。英特尔 AI 全栈解决方案的提出和实践，为企业智能化转型提供了一个清晰的路径：从底层硬件的优化到顶层应用的创新，从单一技术的突破到全栈生态的构建，每一步需要关注企业实实在在的效益，才能让 AI 发挥出真正的价值。</p><p></p><p>未来，随着技术的进一步发展和应用的不断深入，AI 必将成为推动企业创新和增长的关键力量，开启一个全新的智能化时代。让我们拭目以待，共同见证 AI 技术如何助力企业实现跨越式发展，引领行业变革。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2b3ff88acc90e5d292079771e</id>
            <title>浅谈大模型生成类应用的需求分析设计与实践</title>
            <link>https://www.infoq.cn/article/2b3ff88acc90e5d292079771e</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2b3ff88acc90e5d292079771e</guid>
            <pubDate></pubDate>
            <updated>Thu, 29 Aug 2024 02:24:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在人工智能技术飞速发展的今天，基于大模型的生成类应用已经成为业界关注的焦点。这些应用不仅可以帮助企业提高生产效率，还能为用户提供更加个性化、智能化的服务体验。本文将探讨生成类应用的机遇与挑战，以及在实际落地过程中的探索和实践经验。</p><p></p><p></p><h1>生成类应用在通信行业的价值与机会</h1><p></p><p></p><p>生成类大模型让通信运营商在多个方面受益，主要体现在运营、成本、质量及创新能力上。</p><p></p><p>01 提升运营效率</p><p>生成类应用可以帮助通信企业自动化处理大量的日常工作，例如客户服务对话、故障诊断和修复指南生成等，从而显著提高运营效率。</p><p></p><p>02 降低成本</p><p>自动化处理重复性任务可以大幅降低人力资源成本，同时通过优化网络资源配置和运维管理，有效减少各类成本。</p><p></p><p>03 改善服务质量</p><p>通过自动生成高质量的文档、技术指南、用户手册等，可以减少人工错误，确保信息的一致性和准确性，从而提升客户满意度。</p><p></p><p>04 增强创新能力</p><p>利用生成式AI技术，通信企业可以开发出更多创新的产品和服务，比如基于AI的虚拟助手、智能客服系统等，为用户提供更加个性化和智能化的体验。</p><p></p><p>在众多生成类的场景中，通信运营商的售前方案的支撑的需求量最大，一线诉求最强，主要存在以下困难点：</p><p>售前方案主要由一线解决方案经理依靠人工拼装，生成效率低，易出错。一线方案经理经验不一，方案质量参差不齐，影响品牌声望。客户变更需求多，方案频繁修改，售前方案生成周期长，客户满意度降低。</p><p></p><p>通过运用生成式大模型的技术，高效高质量生成售前方案，重构生产运营前后端，已成为运营商数智赋能生产过程的突破口。</p><p></p><p></p><h1>生成类应用的探索和落地</h1><p></p><p></p><p>面对众多的售前方案场景，如何聚焦业务量大、有特色的业务，如何喂投专业业务知识， 如何标准化知识文档结构，如何与业务系统无缝对接等诸多难点，我们采用“逐层拆解，逐个击破”的策略，对生成类的应用场景需求分析设计过程抽丝剥茧，聚焦核心业务、方案论证、原型交互设计，知识标准化梳理，最终形成生成类大模型应用产品的形态。</p><p></p><p></p><h2>场景聚焦：以点为面，聚焦小微组网方案生成</h2><p></p><p></p><p>售前方案涉及行业众多如金融、卫健、农业、教育等，业务跨度广，全部梳理则落地周期长，效果不显著。聚焦一线最急迫，业务最典型，访问量最大的小微企业的组网和视频监控的售前方案生成场景，以此为切入点打造领域方案撰写的样板点，以点带面，为后续多行业售前方案智能生成奠定基础。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82031c4dc71dbd96fc5bca1cc26407d1.webp" /></p><p></p><p>【一线关键问题聚焦】</p><p>1.小微客户个性需求多，往往需要针对性编写方案，周期长</p><p>2.组网监控业务复杂，需要较多专业知识和经验</p><p>3.已有方案难复用，需人工对已有的方案库进行检索</p><p></p><p></p><h2>方案论证：谋定而后动，制定整体设计方案</h2><p></p><p></p><p>聚焦场景后，梳理小微组网方案生成的用户故事，确定解决方案经理的操作流程，完成业务规则训练、设备清单库对接、高中低报价、方案生成、对接云链的业务流程设计。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0d/0dc438640215bbaca99e07ee17fb9148.webp" /></p><p></p><p>根据业务流程，确定整体方案的智能化目标，同时制定小微组网方案生成的技术方案。事前构建高质量的数据集，事中重点设计智能高效的交互体验，事后形成闭环的知识评估及更新方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7b/7b347d2ca9ca832dd9ca9d508581603f.webp" /></p><p></p><p></p><h2>知识梳理：构建高质量数据集，定义文档标准化格式</h2><p></p><p></p><p>通过文档识别、文档处理、索引构建、知识检索这四大步骤构建高质量数据集，形成售前方案高质量生成的技术保障。</p><p></p><p>word/excel/pdf等格式的文档是业务知识常用载体，将操作手册、产品白皮书、设备安装手册介绍等常见文档作为知识的输入，有利于业务知识的快速收集和成生。通过文档格式处理技术，高效识别文档关键元素，保持文档的层次结构；通过文档多模型内容处理技术保持文档文本、图片、表格等多模型信息的还原度。应用语义索引和实体索引两种策略，构建高效的索引方案。通过语义检索、关键字检索、实体检索等组合的知识检索方案，形成知识库整体建设的完整方案。</p><p></p><p>依托业务知识的规则化和章节内容的标准化，形成高质量的小微组网知识数据集，并依据知识集及典型方案定义生成文档的标准化格式。</p><p></p><p></p><h3>业务知识规则化</h3><p></p><p></p><p>通过一线调研结合典型售前方案，确定小微组网知识分为网络类和安全安防类两大类，每一类又有细分场景，每个细分场景有对应的商品，每个商品有详细的业务规则。如下图所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/61/61f2b3fd805e7eb039be3d3231543ef9.webp" /></p><p></p><p>小微组网的业务知识的梳理，即将这些商品在细分场景下对应的规则进行细化，形成知识场景化的知识规则体系，纳入知识库统一管理。</p><p></p><p></p><h3>章节内容标准化</h3><p></p><p></p><p>一线解决方案人员在编写小微组网方案时内容和格式都不大一样，无法保证高质量，需对方案内容的章节进行统一定义。</p><p></p><p>首先，分析典型方案内容，抽取共性章节，最终确认标准方案章节定义为：需求描述、方案描述、方案报价。</p><p></p><p>其次，报价部分格式千样，存在较多隐患。最终根据一线的报价需求并参考报价系统，形成标准报价方案包括：标品套餐、集成服务清单、项目总计的内容。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d372b48c65e3b9578db0a2960a44528.webp" /></p><p></p><p></p><h2>交互设计：智能高效的交互体验，提升一线感知</h2><p></p><p></p><p>良好的交互体验影响售案生成质量和一线感知。应用业务参数固化、会话管理、提词工程、RAG检索增强生成四种交互技术使该场景交互质量与交互感知达到最佳平衡。</p><p></p><p></p><h3>01 业务参数固化</h3><p></p><p></p><p>标准化模板：通过预先设定的参数模板，确保售前方案的标准化，提高方案的一致性和质量。</p><p>快速变更：固化参数允许快速变更配置方案的基本框架，加快方案生成的速度。</p><p>减少交互次数：标准参数无需来回对话，提升需求收集的速度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e7153c50584a38c86dc2b7d64ab3451.webp" /></p><p></p><p></p><h3>02 会话管理</h3><p></p><p></p><p>会话管理是为了有效且准确的将对话之前所蕴含的信息提取出来而引入的模块，将智能分割用户会话内容管理过往对话所蕴含的场景信息、意图信息传入大模型底座，从而生成连贯和个性化的回复。同时引入个人用户token，在每个对话之间传递会话令牌，能够将不同用户的输入与之前的对话上下文关联起来,产生与特定用户和对话相关的回应。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/692130c6286e3265f9c0793a4fa3c3d4.webp" /></p><p></p><p></p><h3>03 提词工程</h3><p></p><p></p><p>提示词工程在大模型与复杂业务结合时具有重要作用。建立提示词模板库，针对不同场景设计特定模板。</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12cb13a36043f62b7a6341991a20eb1e.webp" /></p><p></p><p>提高用户体验：通过精心设计的提示语，可以帮助解决方案经理更好地理解和操作界面，减少错误操作。引导用户行为：提词可以指导解决方案经理完成特定的任务流程，比如填写组网标准参数模板。减少用户输入负担：智能提词可以预测解决方案经理的输入意图，自动完成部分输入，减轻解决方案经理的手动输入工作量。提供即时反馈：提词可以用来给予解决方案经理即时反馈，确认解决方案经理的操作是否成功，或者指出可能存在的问题。</p><p></p><p></p><h3>04 高效RAG检索增强生成</h3><p></p><p></p><p>RAG检索增强技术确保交互返回的答案精准有效。通过引入ES，对文档的内容关键字进行检索和对向量库的相似召回，对召回的内容进行rerank排序，提高了文档检索结果的正确性，对问题进行重写和二阶段检索结果由LLM推理归纳返回最佳答案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4c/4c3cb4c6076d74c21570ad44cdef0028.webp" /></p><p></p><p></p><h2>运营管理：业务知识长效评估及更新</h2><p></p><p></p><p>组网的业务规则会随市场变化和新产品入网产生变更，通过一线用户评价反馈及提词优化反馈实现业务知识质量评估及长效更新，满足市场变化的需求。</p><p></p><p>一线用户评价：由一线人员在对话的过程中进行满意度价评，包括方案知识的点踩，形成劣质知识的下架，精确知识的更新。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70d244ac5f749a0b4b7af9556e99f61b.webp" /></p><p></p><p>提示词优化收集：管理抽取知识的提示词的内容、相似度、向量阈值、TOPK等相关信息。在提示词对检索和输出的内容进行规范说明。实现对大模型对知识检索的准确率和抽象整理能力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8b/8b5cefe3ebe98814344507df96047f83.webp" /></p><p></p><p></p><h1>生成类应用场景扩展思考</h1><p></p><p></p><p>随着大模型带来能力突破，让AI与生成力结合，促进企业内部数据价值释放。未来，我们预见大模型将进一步与业务紧密结合，成为整个业务流程的核心驱动，接入更多的方案生成场景。同时结合AR/VR技术，生成式AI可以生成更加直观易懂的方案。未来怎样发展，我们将持续把握风口，迎接挑战。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OczQK3Y36NeIXNWdDsu5</id>
            <title>第五届深圳国际人工智能展（GAIE）即将召开，5大精彩看点揭秘</title>
            <link>https://www.infoq.cn/article/OczQK3Y36NeIXNWdDsu5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OczQK3Y36NeIXNWdDsu5</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 11:04:20 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>2024年9月8日至10日，第五届深圳国际人工智能展（GAIE）将于深圳会展中心（福田）盛大举行。作为粤港澳大湾区人工智能领域的专业盛会，GAIE不仅会展示人工智能领域的最新成果和前沿技术，更是一个促进行业交流、推动创新发展的重要平台。</p><p></p><p>本届展会由深圳市工业和信息化局、深圳市发展和改革委员会、深圳市科技创新局、深圳市政务服务和数据管理局共同指导，深圳市人工智能行业协会、深圳市万博展览有限公司主办，以“智创未来·价值链接”为主题，旨在汇聚全球人工智能领域的顶尖企业、专家学者及创新成果，展示人工智能技术的最新进展和广泛应用，促进国内外交流合作，对加速人工智能技术成果转化，构建人工智能应用生态体系，积极推动人工智能高质量发展具有重要意义。我们特意者整理了本届展会的几大精彩看点，供感兴趣的读者参考。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b0d9c5ad7104c03769cc867eeacb387.png" /></p><p></p><h2>看点1：品牌展商+创新产品，打造科技创新盛宴</h2><p></p><p></p><p>本次展会汇聚了国内外众多人工智能领域的领军企业，包括百度、华为、中国联通、迈瑞医疗、特斯拉、云知声、普渡机器人、鲜食代等头部企业。这些企业将在展会现场竞相亮相，展示他们在人工智能领域的最新技术、产品和解决方案。从智慧交通的自动驾驶与智能车辆监控系统，到智能制造中的智能机器人与自动化生产线；从智慧医疗的辅助诊断与远程医疗系统，到智能服务的个性化语音助手与高效客服；再到智慧教育的智能硬件与在线教育平台，每一件创新产品都将以其独特的魅力，向观众展示人工智能技术的无限可能。而展会期间的多场新品发布会，更将科技与创新的火花碰撞得淋漓尽致，让观众在近距离感受最前沿科技成果的同时，也深刻体会到了科技所带来的便捷与惊喜。:</p><p></p><h2>看点2：30+场系列活动，洞悉AI行业前沿趋势</h2><p></p><p></p><p>本届展会将聚焦100+热点话题，举办2024全球湾区科技创新发展论坛暨第五届深圳国际人工智能展开幕式、智能机器人创新发展论坛、工业制造发展论坛暨深圳标准认证颁证大会、医工融合—医疗高质量发展大会、第二届智能算力发展论坛、大模型生态与发展论坛、百度AI引领行业人工智能应用创新发展论坛、AI赋能科技品牌出海国际论坛等30+场系列活动，旨在为企业搭建深度交流平台，洞悉AI行业的前沿趋势与最新动态，精准把握市场脉搏，为未来发展提供有力支撑和广阔机遇。</p><p></p><p>此外，展会现场将汇聚AI人才进行交流分享，举办AI人才直聘会与校企对接会，以推动人才流动，构建强大的人工智能行业人才队伍，并加速高校科技成果向企业转化，提升企业技术创新能力。</p><p></p><h3>看点3：大咖聚首深圳，探寻未来AI发展新机遇</h3><p></p><p></p><p>本届展会邀请到了中国科学院院士、天津大学精仪学院教授、院名誉院长、激光与光电子研究所所长姚建铨，美国医学与生物工程院院士、英国皇家公共卫生学院院士、深圳理工大学计算机科学与控制工程院院长潘毅，深圳市人工智能行业协会首席顾问、北京大学教授刘宏，深圳市人工智能行业协会会长、平安集团首席科学家肖京，阿里云教育高级副总裁黄桂晶，大族激光副总裁、大族视觉董事长陈焱，云知声联合创始人&amp;副总裁康恒等全球前沿专家、学者、企业家齐聚深圳。他们将在论坛上分享全球前沿理念与创新举措，感知AI与新科技、AI与新产业、AI与新制造的发展趋势，探讨人工智能赋能千行百业的无限可能。这不仅为展会增添了浓厚的学术氛围，更为业界人士提供了宝贵的学习和交流机会。:</p><p></p><h2>看点4：AI产业会客厅，展望产业融合新未来</h2><p></p><p></p><p>本次展会上，产业会客厅将作为重要的交流平台，旨在以深度采访及人工智能应用场景供需对接的形式促成更高质量的产业链合作、更生态化的共创，充分发挥龙头企业的主引擎“带动力”，为优势产业链重点企业带来新的市场增量空间，为区域创新业态及创投生态提供巨大的发展机遇，进一步推动人工智能相关企业积极拓展市场交流与合作。而高端访谈环节是产业会客厅不可或缺的重要组成部分，它不仅是信息传递的桥梁，更是思想碰撞的火花。通过邀请人工智能领域的领军人物、企业高管、学术权威等，就AI技术的最新发展、行业趋势、政策导向等话题进行深度对话，期望能够触及人工智能领域的最深处，挖掘那些不为人知的秘密和即将改变世界的创新，为人工智能行业的未来发展注入更多的动力和活力，推动整个行业不断向前发展。同时，还将通过现场采访，挖掘并分享企业在人工智能领域的成功经验和创新实践，为观众呈现一个全面、深入的人工智能行业图景。</p><p></p><p>此外，为加速AI应用场景落地，产业会客厅还将通过汇聚人工智能领域的专家和学者等技术提供方，以及医疗、水务、机场等各行业企业代表和决策者等应用场景需求方，开展“人工智能应用场景供需对接会”，并以“主题演讲+互动展示+供需对接”的形式展开，通过分享各行业的人工智能应用案例和需求，共同探讨AI技术如何赋能不同行业，以提高效率、降低成本，创造更多商业机会。这一环节将有效推动人工智能技术的广泛应用和产业升级。</p><p></p><p>另一方面，作为推动AI产业高质量发展的重要力量，投融资机构在展会中扮演着不可或缺的角色。本届展会特设“人工智能投融资展示对接区”，并同期举办国际人工智能投融资大会。通过技术展示、企业路演、投融资对接等形式，汇聚全球范围内的人工智能创业项目与投资机构，推动优质项目与资本的高效对接。这不仅为初创企业提供了宝贵的展示机会和融资渠道，也为投资机构挖掘潜力项目提供了便利条件。</p><p></p><p>通过这些活动，“产业会客厅”不仅引领了AI行业的新潮流，还搭建了一个合作与交流的桥梁，有助于推动人工智能产业的高质量发展，同时也为初创企业提供了展示和融资的机会，为投资机构提供了挖掘潜力项目的平台。</p><p></p><h2>看点5：60000+专业观众，促进跨领域交流合作</h2><p></p><p></p><p>展会面向全球范围诚邀专业观众参会，包括但不限于科技企业代表、科研机构人员、投资机构、行业媒体及政府相关部门等。通过精准邀约与线上宣传，并依托深圳市人工智能行业协会的广泛资源，目前已邀请全国各人工智能友好协会组织各省市企业赴深观展、交流、寻求合作。这些企业代表了各自领域的顶尖水平，他们的参与无疑将为展会增添更多亮点。此外，组委会还特别邀请了深圳市医院管理者协会、深圳市福田区青少年科技教育协会等近50家协会，涵盖工业、家居、机器人、电子信息、医疗器械、商会、企业家协会、外商协会等多个领域，预计吸引60000+专业观众到场参观。届时，各商协会将组织其会员单位分批观展、对接，以促进跨领域的交流与合作，共同推动人工智能技术的创新与发展。</p><p></p><p>经主办方邀请，InfoQ 将参与本届 GAIE 现场报道，诚邀感兴趣的读者前往。2024年9月8—10日深圳会展中心（福田），我们不见不散！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y7JgsijGuP9S1A2yeG94</id>
            <title>2000 多名应届生被印度 IT 巨头“培训”多年不给发工资，在职员工吐槽：我们工作14小时哪还有工作留给别人</title>
            <link>https://www.infoq.cn/article/y7JgsijGuP9S1A2yeG94</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y7JgsijGuP9S1A2yeG94</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 11:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h2>印度IT公司“疯狂剥削”，2000名新员工入职遭推迟多年</h2><p></p><p>&nbsp;</p><p>据报道，印度IT厂商Infosys的新员工反复经历无薪“入职培训”却难以入职。</p><p>&nbsp;</p><p>印度IT厂商Infosys被指存在严重“剥削”，据称该公司向数千名工程专业毕业生发出录取通知，但在长达两年时间内仍未让任何一名新员工入职。报道指出，这些应届毕业生们被告知必须反复接受无薪培训，才有资格继续留在Infosys工作。</p><p>&nbsp;</p><p>上周，印度IT工作者倡导组织“信息技术新生雇员理事会”（NITES）向印度劳工与就业部长Mansukh Mandaviya写信，要求印度政府介入干预“以阻止Infosys剥削年轻的IT毕业生”。NITES主席Harpreet Singh Saluja在这份署名信函中强调，NITES已经收到“多起”来自应届工程毕业生的投诉。这些毕业生在被Infosys聘为系统工程师与数字专家工程师之后，“遭受到不专业及剥削性的对待”。</p><p>&nbsp;</p><p>据NITES介绍，Infosys公司曾在2022至2023年间组织过大学招聘工作，且最早在2022年4月22日就曾向学生们发放过录取通知，但却一直在推迟毕业生们的入职流程。NITES此前曾经表示，有“超过2000名新员工”受到此事影响。</p><p>&nbsp;</p><p>据公开资料显示，Infosys 是总部位于印度班加罗尔的一家信息技术跨国公司。2017年，Infosys 是仅次于塔塔资讯服务的印度第二大IT公司，收入在全球上市公司中排名第596。Infosys 在29个国家设有办公室并在印度、美国、中国、澳大利亚、英国、加拿大、日本等地设有研发中心。Infosys 在超过30个国家提供商业咨询、信息技术、及外包服务。</p><p>&nbsp;</p><p>2021年8月，印孚瑟斯市值超过1000亿美元。</p><p></p><h2>无薪“入职培训”</h2><p></p><p></p><p>NITES声称，收到录取通知的学生们被要求参加从2024年7月1日至2024年7月24日进行的无薪线上“入职培训”。报道指出，Infosys的人力资源团队当时曾向应届毕业生们许诺，入职计划将于8月19号或9月2号敲定。但NITES在信中强调，事情并未像预期般推进，导致这些号称被录取的毕业生们感到“极度沮丧、焦虑和迷茫”。</p><p>&nbsp;</p><p>信件内容如下：</p><p></p><p></p><blockquote>尽管成功完成了入职培训，但承诺的结果却始终未能落地，导致毕业生们在20多天时间内完全不知所措。更令他们震惊的是，这些毕业生们不仅没有收到确切的入职日期，反而被告知还需要重新接受线下入职培训和考试，且期间同样没有任何报酬。</blockquote><p></p><p>&nbsp;</p><p>技术媒体The Register日前报道称，Infosys的新员工们接受了“多次无薪的线上与线下培训课程及评估”，并引用了该公司发给新员工们的电子邮件。报道还提到，如果新员工们拒绝参加这些课程，则将不再拥有入职机会，而且其中至少包括一门长达六周的课程。</p><p>&nbsp;</p><p>该消息在社交平台引发了热议，一位自称是Infosys在职员工的网民称，“我们工作14小时哪还有工作留给别人做。”</p><p></p><h2>CEO宣称Infosys最终将接纳这些新员工</h2><p></p><p></p><p>在收到NITES的信件之后，Infosys公司CEO Salil Parekh本周宣布毕业生们即将正式入职，但没有提供更具体的上岗时间，也未解释为什么要推迟这么长时间并重复组织培训课程。在接受印度新闻网站Press Trust of India采访时，Parekh回应称：</p><p>&nbsp;</p><p></p><blockquote>我们给出的每份录取通知都为新人们提供相应的入职岗位。我们确实调整了具体入职日期，但除此之外，所有人都会最终入职Infosys，向来如此。</blockquote><p></p><p>&nbsp;</p><p>值得注意的是，在上个月的财报电话会议上，Infosys公司首席财务官Jayesh Sanghrajka曾表示“计划今年招聘1.5万至2万名”应届毕业生，“具体取决于我们对业务的增长预期。”目前还不清楚这个数字，是否包含NITES所关注的这2000名新员工。</p><p>&nbsp;</p><p>今年3月，Infosys公司报告其员工人数为31万7240人，也是自2001年以来首度出现人员规模缩减。Parekh最近还宣称，Infosys公司预计不会因AI等新兴技术的影响而裁员。在最新一轮财报中，Infosys公司报告称利润同比增长5.1%，收入同比增长2.1%。</p><p>&nbsp;</p><p>NITES此前曾主张受入职延误影响，Infosys应“支付入职延误期间的全额工资”；如果最终未能入职，Infosys应帮助求职人员在公司之外找到其他岗位。</p><p>&nbsp;</p><p>截至 2024 年 6 月，Infosys 的员工总数进一步收缩为 315,332 人。</p><p></p><h2>Infosys称正大力发展AI，不会裁员</h2><p></p><p></p><p>NITES在信中提出，Infosys的行为已经对印度的经济增长产生了负面影响，并强调：</p><p>&nbsp;</p><p></p><blockquote>这些年轻的工程毕业生是我们国家IT行业未来不可或缺的一部分，而IT行业在我们的经济运行中发挥着关键作用。尽管满足了所有要求，但这些专业人员却被搁置了两年多，这给他们带来了极大的挫败感、焦虑和不确定性。Infosys这种推迟他们入职进度、并要求其从事无偿工作和反复评估的行为，不仅浪费了年轻人们的宝贵时间，也损害了他们可能为印度经济增长做出的实际贡献。这不仅是印度最大的 IT 公司之一背信弃义的行为，而且是一个影响我国 IT 劳动力和经济未来的重大问题。</blockquote><p></p><p>&nbsp;</p><p>Infosys方面则一直没有解释为何这数千名新员工的入职周期比预期中更长。一个可能的原因在于入职流程受阻。疫情对于印度的打击尤其严重，Infosys此前也曾延长过入职时间。</p><p>&nbsp;</p><p>此外，印度正在面临着严重的岗位短缺问题。等待两年才能找到一份工作已经成为常态，而且越来越多的人发现自己的职业选择空间正在缩小。2024年6月，一项印度招聘趋势研究报告发现，硬件和网络领域的IT职位招聘量同比下降了9%，软件和软件服务领域的招聘量同比下降5%。据印度杂志《Frontline》报道，印度IT行业的流失率已经由2022年的27%下降到去年的16%至19%，愈发固化的人员流动导致印度的IT职位（特别是初级职位）有所减少。由于人们更倾向于坚守当前的岗位，企业的招聘力度也有所降低。《Frontline》举例指出，Infosys公司在2023年和2024年都没有进行任何校园招聘，总部位于印度的塔塔咨询服务公司同样叫停了校招流程。</p><p>&nbsp;</p><p>过去两年间，Infosys一直为印度未来几年预计出现的IT技能缺口而维持着稳定的人才储备，导致新近毕业的IT毕业生难以找到工作机会。但科技企业的人才流失不可能彻底消除，总会有一部分从业者想要换个环境、另寻东家。而现在从业者们显然又有了新的麻烦需要操心——申请政府干预，帮助自己解决因为入职拖延导致的经济损失和心理压力。</p><p>&nbsp;</p><p>处于印度经济萎靡不振的困境中，Infosys也在寻求进一步增长之路。</p><p>&nbsp;</p><p>Infosys 首席执行官 Salil Parekh 表示，生成式人工智能引起了客户的强烈兴趣，公司内部也在大力推广 GenAI。不过，他坚称，公司不太可能因为新时代技术而裁员。</p><p>&nbsp;</p><p>Salil Parekh 表示，客户对生成式人工智能的接受度很高，他将其与过去数字和云技术的采用曲线进行了类比，因为他相信，随着企业体验到由此带来的好处和业务成果，GenAI 的采用将随着时间的推移而增加。</p><p>&nbsp;</p><p>Salil Parekh 表示，他不认为 GenAI 的出现会导致公司裁员。他说：“所以，在这个阶段，我的感觉是，这项技术将帮助业务进一步增长，而不是其他任何事情。我们没有看到 Infosys 因这些新时代的技术而裁员，事实上，随着经济环境的变化，我们继续增加招聘……正如你在第一季度看到的那样，我们实现了季度环比和年度强劲增长，我们获得了非常好的大宗交易。因此，我们提高了我们的指导。所以我们看到增长正在慢慢恢复。”</p><p>&nbsp;</p><p>据报道，印孚瑟斯公司今年早些时候披露，该公司正在开发 225 个生成式人工智能客户项​​目，超过 25 万名员工正在接受生成式人工智能培训。&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://arstechnica.com/information-technology/2024/08/exploitative-it-firm-has-been-delaying-2000-recruits-onboarding-for-years/">https://arstechnica.com/information-technology/2024/08/exploitative-it-firm-has-been-delaying-2000-recruits-onboarding-for-years/</a>"</p><p><a href="https://www.thehindubusinessline.com/companies/infosys-delays-onboarding-yet-again-2000-graduates-left-hanging/article68547091.ece">https://www.thehindubusinessline.com/companies/infosys-delays-onboarding-yet-again-2000-graduates-left-hanging/article68547091.ece</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KLLoROGmSa7yi0zXJIVc</id>
            <title>华为张平安：跨越技术鸿沟，为数智化供需“架桥铺路”</title>
            <link>https://www.infoq.cn/article/KLLoROGmSa7yi0zXJIVc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KLLoROGmSa7yi0zXJIVc</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 10:22:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月27日，在2024中国国际大数据产业博览会上，第三届828 B2B企业节正式开幕。华为常务董事、华为云CEO张平安发表致辞表示，数字世界的话语权最终是由生态的繁荣决定的，华为云携手上万家伙伴，整合“技术+生态”能力，共同发起828 B2B企业节，打通生态伙伴“共建-共营-共销”的价值流，把最合适的技术、经由最短的路径、以最合理的模式，给到最需要的场景，为中国企业数字化、智能化构筑起“一站式平台服务”。</p><p></p><h2>联手共筑智算高地，扎根贵州，服务全国</h2><p></p><p></p><p>作为首个国家大数据综合试验区，贵州省正推动数据中心由存储中心加快向“存算一体、智算优先”，抢占智算制高点。华为云与贵州持续深化合作，在贵州布局了全球最大的数据中心，通过系统架构创新打造业界领先云数据中心和AI算力中心。</p><p></p><p>张平安介绍，华为云贵安数据中心，容纳百万台服务器，PUE达1.12，处于世界领先水平。今年，华为云还发布并全面应用了全新的面向多元算力的CloudMatrix架构，以“一切可池化”“一切皆对等”“一切可组合”的创新设计，从算力规模、扩展模式和使用模式上，匹配超大规模算力诉求。并通过AI技术精准控温、云服务感知的能效调优等多项创新技术的应用，打造高能效、高可靠数据中心。</p><p></p><p>基于华为云全球存算网核心枢纽的时延覆盖圈，贵安数据中心可支持企业90%的业务集中部署，领先的技术让贵安数据中心为全球客户提供服务成为可能，企业不用操心基础设施建设和产品的升级换代，云上算力随取随用，技术永新。全国多家头部互联网、人工智能企业如上海百胜、科大讯飞等已入驻。</p><p></p><p>华为云智算基地落地贵安新区，也在加快贵州打造具有国际竞争力的人工智能算力高地、AI生态基地和数据生态基地的步伐。现场，贵州省大数据局携手华为云及生态伙伴共同发起贵州“8+4”产业生态创新行动。针对贵州省昇腾算力生态、行业大模型生态、人工智能人才生态展开深入合作，双方共同在煤矿、酱酒、新材料等8个重要行业，在城镇智慧化、乡村数字化等4个重要领域，推进人工智能应用的孵化和落地，加速人工智能在贵州省的全面发展，让更多创新的AI应用在贵州落地。</p><p></p><h2>做强智能世界云底座，打通企业数智升级最优路径</h2><p></p><p></p><p>828 B2B企业节让“产品” 与“需求” 通过平台连接起来，同时带动更广泛的生态共振，为企业数字化转型、智能化升级带来新动力。</p><p></p><p>而作为数字世界的坚实技术底座，华为云正携手行业客户、伙伴加快打造自主创新的数字技术生态，以云作为统一平台和生态入口，联合鸿蒙、鲲鹏、昇腾、高斯等技术生态，做强智能世界云底座。</p><p></p><p>在人工智能领域，中国拥有千行万业的业务场景，如果各行各业都积极拥抱AI，开放业务场景，中国非常有机会在全球构筑起领先优势。华为云通过昇腾AI云服务支持百模千态，通过盘古大模型重塑千行万业，助力产业智能化。目前，华为云携手伙伴将盘古大模型落地到了矿山、电力、气象、医药等30多个行业，400多个模型应用场景，持续为各行各业的生产场景“解难题、做难事”，重塑千行万业。</p><p></p><p>在软件领域，中国拥有全球最大的软件创新人群，但在核心软件上一直处于落后的状态。张平安表示，随着企业在数字化、智能化方面更多的需求，我们要构建云原生的核心软件和开发工具链，给世界一个更好的选择。华为云打造云原生的核心软件，已陆续发布了23款软件开发工具，并推出了分布式云核心底座、GaussDB数据库、软件开发生产线CodeArts、硬件开发生产线CraftArts等一系列产品和技术，支持企业研发高效创新，加速客户与伙伴的应用现代化。</p><p></p><p>同时华为云也不断完善生态体系，助力伙伴能力提升与商业成功。今年华为云发布了新的伙伴能力计划，从产品技术、场景服务、行业经验三个方向上来加速伙伴能力成长。目前华为云已汇聚4.5万伙伴，联合构建了500多个行业解决方案，12000款云商店商品。</p><p></p><p>张平安在讲话的最后指出，“数字化是全球科技革命和产业变革的焦点，我们深刻感知，中国的数字经济发展和技术创新，仍需跨越一道道鸿沟，数字产业大花园的繁茂，仍需要更多的灌溉。”他呼吁政府、产业、学术和研究机构共同参与，共同促进智能时代技术生态的繁荣。“成就好生意，成为好企业！”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/AsZdv8DDHWulU28jYUtI</id>
            <title>AICon 上海站 2024 优秀出品人与明星讲师名单揭晓，RAG、Agent、工具链专场最受欢迎</title>
            <link>https://www.infoq.cn/article/AsZdv8DDHWulU28jYUtI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/AsZdv8DDHWulU28jYUtI</guid>
            <pubDate></pubDate>
            <updated>Wed, 28 Aug 2024 09:38:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 18 日 -19 日，<a href="https://aicon.infoq.cn/202408/shanghai/">AICon 全球人工智能开发与应用大会 2024</a>"（上海站）圆满落幕，会议话题涵盖端侧模型落地探索、大模型训练以及推理加速、数据集构建及评测技术落地、安全性实践、RAG 落地应用与探索、AI Agent 技术突破与应用、多模态大模型、大模型行业应用落地、大模型与企业工具集成的提效实践、大模型在搜索、广告、推荐领域的探索、产品应用构建、大模型产学研结合探索等。</p><p></p><p>此次会议邀请 60 余位来自国内外的专家，涵盖字节跳动、百度、华为、京东、阿里巴巴、微软亚洲研究院、小红书、腾讯、英特尔、快手、网易伏羲、携程、小米、顺丰科技、蚂蚁集团、哔哩哔哩、零一万物、Llamalndex、月之暗面、硅基流动、极佳科技、上海人工智能实验室、卫宁健康、惠每科技、矩阵起源、未来智能、岩芯数智、PayPal、熊墅科技、商汤科技、next.ai、蔚来汽车、华院计算、复旦大学、前极客邦科技、数美科技、中国科学技术大学、喜马拉雅、蒙特利尔大学 &amp;MILA 研究所、北京大学、上海交通大学、英飞流、智源研究院、上海人工智能实验室、合合信息等，超过 1000 多名对大模型感兴趣的听众参与了此次会议。</p><p></p><p></p><h4>主题演讲</h4><p></p><p></p><p>在 8 月 18 日上午主论坛环节，上海市邮政管理局党组书记、局长冯力虎发表开场致辞，鼓励科技创新，并希望 AICon 大会能成为创新的起点；顺丰科技副总裁唐恺深入介绍了顺丰在物流领域的技术创新并揭晓了顺丰物流决策大模型的神秘面纱，随后上海市邮政管理局党组书记、局长冯力虎，顺丰集团副总裁龚威、顺丰科技副总裁唐恺、零一万物联合创始人祁瑞峰、智谱 AI 副总裁吴玮杰、华为云盘古大模型 CTO 李寅、浙江大学管理学院副院长杨翼以及极客邦科技创始人兼 CEO 霍太稳共同登台联合发布了顺丰物流决策大模型，见证了物流行业创新的重要时刻；蔚来创始人、董事长、CEO 李斌分享了蔚来近年在智能电动汽车和 AI 方面的思考与实践，强调 AI 将成为智能电动汽车企业的核心基础能力；英特尔院士、大数据技术全球 CTO 戴金权分享了英特尔过去一两年在大模型的异构计算和加速方面所做的工作；面壁智能联合创始人兼 CEO 李大海提出了大模型的知识密度提升趋势，并强调了提升知识密度是实现高效大模型的关键 ；最后，字节跳动研究科学家冯佳时分享了基于 LLM 的视频生成和图像理解的进展 。</p><p></p><p>详细报道见：<a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=3&amp;sn=5a7aa243180f43f8c360808338ffab4d&amp;chksm=fbeba49ccc9c2d8aaf45db62de584cf41bc24864c1babe9fe52909ddd02bd84030cd79dfe4c1&amp;scene=21#wechat_redirect">AICon 上海站精彩回顾，从大模型变革之路到高效“炼丹”指南，超 60 位大模型先锋输出最前沿干货！</a>"</p><p></p><p></p><h4>焦点</h4><p></p><p></p><h5>RAG 落地应用与探索</h5><p></p><p></p><p>作为本次大会 最热门的专题，*****火爆程度 5 颗星，场均人次 280+，该专题聚焦检索增强生成技术实践，专题出品人是阿里巴巴技术总监郭瑞杰，专题邀请了阿里云高级算法专家欧明栋、 PayPal 高级 ML 科学家王元、PayPal 算法工程师马泽宏、合合信息智能创新事业部研发总监常扬、英飞流 CEO 张颖峰、LlamaIndex 创始 AI 工程师 Pierre-Loic Doulcet 共 6 位讲师同台分享，分享 RAG 在企业中的实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/ed2a2f6737026efb47b0699a373e2413" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/62/625a8f3f96089cd3678f44dab2198f19" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8abbbcfd6edc20ef85a2fd68e7821ba" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/83/83a823cd04e48502877ec19cb2032732" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/13/132add8b094fbda7b41d51e3727408e6" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a6/a625c896463e72b104a4651eb8a9d57c" /></p><p></p><p></p><h5>大模型与企业工具集成的提效实践</h5><p></p><p></p><p>本次火爆程度 4 颗星，场均人次 160+。 在 ChatGPT3.5 发布后，企业工具经历了一波 AI 集成和升级浪潮，大量的企业工具尝试集成大模型进行企业提效，但大模型概率性的本质和不可解释性导致在集成上遇到许许多多的问题，人们也经历了信心爆棚到绝望再重拾信心的转变。本专题分享了大模型与企业工具的集成实践和从业者的心路历程，并探讨 AI 在哪些场景更能为企业带来助力。专题出品人是阿里巴巴代码平台负责人 向邦宇，专题演讲嘉宾分别是阿里巴巴高级算法工程师林智超、研发效能领域知名专家路宁、next.ai 创始人蒋志伟。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ed/eddc1b82a9c2c7a6d2615eb8dfe56702" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/da/da4ad692c4a54bfece5897e39dd92326" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/d0/d0f39194e30b895aa26c5ad66c342628" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/03/0337bdf891ec6015edc9164435c15675" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/5a/5a8890cbb88818fd9aa31b3e32885690" /></p><p></p><h5>AI Agent 技术突破与应用</h5><p></p><p></p><p>本次火爆程度 4 颗星，场均人次 140+。</p><p></p><p>AI Agent 正迅速成为大模型非常重要的应用方向，这些智能实体通过先进的机器学习和人工智能技术，能够自主感知环境、做出决策并执行相关动作。AI Agent 的应用场景日益广泛，包括但不限于数字员工、具身智能、个性化推荐等。在【AI Agent 技术突破与应用】论坛，我们邀请到了 DeepWisdom（MetaGPT）创始人兼 CEO 吴承霖担任出品人，并邀请到了微软亚洲研究院高级研究员宋恺涛、网易伏羲语言智能组负责人张荣升、蒙特利尔大学 &amp;MILA 研究所助理教授刘邦、腾讯 PCG 大模型中台 Agent 技术负责人陈浩蓝四位嘉宾来进行演讲分享，为听众送上他们的 Agent 开发精彩实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8a/8a0a1d650f2e4fe3b608d1617c4f8da5" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/24/2412e5ea4d4f8060b8e6d90d06fc4f74" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7a/7a26b294a755d5d6199c7ae4eebe6096" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cd/cd21af8377f02953d04e450735b806b2" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f9/f92c7521801ee176110ce8f5aa0a4ef8" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/91/917f73a6e0a1eca414092cee06e5a36e" /></p><p></p><p></p><h4>优秀出品人以及明星讲师</h4><p></p><p></p><p>本次大会共有 12 位专题出品人，他们都是各自领域的权威专家。出品人的主要任务是确保各自专题的分享内容质量，包括在前期阶段对议题进行深入的讨论和打磨，以及对演讲材料进行严格的审核。经过评选，最终有 4 位专题出品人因其对本次大会内容策划的杰出贡献而被授予“优秀出品人”的称号，名单如下：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fd/fdafe1d33b34d9d4586053ffebfdc107.png" /></p><p></p><p>AICon 大会讲师的选拔既包含组委会定向邀请，也开放给公众提交演讲主题。为了确保演讲内容的质量，所有提交的议题都必须符合六大标准：明确的观点、以实践为基础、深入且有见地的分享、良好的专业声誉、禁止任何形式的广告、以及确保听众能从中获得实质性的收获。要从众多杰出的演讲者中脱颖而出，获得“明星讲师”称号，演讲者不仅需要在大会筹备期积极配合组委会反复打磨议题和 PPT，提供既有深度又实用的分享内容，还需要在大会现场展现出色的演讲表现，并获得至少 90% 的听众满意度（在满意度测评中，听众对讲师的评价分为“非常满意”、“满意”、“一般”和“不满意”四个等级，听众满意度指“非常满意”＋“满意”评价在收到的所有评价中的占比）。经过严格的评选，共有以下演讲者荣获“明星讲师”的荣誉：</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/db/dbe1789ca13e0bbc443ae6f2f74e0c12.png" /></p><p></p><p>在获得“明星讲师”的演讲嘉宾中，数位讲师的听众满意度更是超过 95%，甚至达到 100%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/9b/9b4d13aee334b8a86fc970ff53d6a3cd.png" /></p><p></p><p>他们分别是：</p><p></p><p>字节跳动研究科学家，豆包大模型视觉基础研究团队负责人冯佳时《大语言模型在计算机视觉领域的应用》快手可图大模型团队负责人李岩《快手可图大模型的技术演进与应用探》喜马拉雅珠峰 AI 算法负责人叶剑豪 《生成式音频大模型的多模态“产模结合”》上海交通大学计算机科学与工程系副教授林云 《语言模型驱动的软件工具思考：可解释与可溯源》商汤科技系统研究员雷丹 《SensePPL 端侧大模型系统与优化》智源研究院大模型行业应用总监 周华 《智源行业数据集及训练方法落地实践》</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/2d/2dfbfc98a1db9a2c7bdd336d725b4696" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/47/479b99b8cc1afa9267184b4247e93b64" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ba/ba3568c3aca74d7035195067b7d4c123" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/a6/a646efdfa4f2c61305c0ec9c1e8fa948" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f8e1acd7e88c227d42fbf7d59ebf057b" /></p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b6/b6c81186f8ba0fae5774eb6544f06f0b" /></p><p></p><p>我们衷心感谢每一位参与 AICon 的出品人与讲师的精彩分享与辛勤付出，正是他们的努力，AICon 才能为听众带来无数精彩的内容与深刻的见解。</p><p></p><p></p><h4>共创未来：我们的合作伙伴阵容</h4><p></p><p></p><p>AICon 上海站的圆满举办，离不开赞助商们贡献的力量。感谢英特尔、亚马逊云科技、Google Cloud、矩阵起源、百道数据、Optiver、数势科技、未来智能、UCloud 优刻得、钛动科技、零一万物、快递 100、快手、昇腾对本届大会的倾情赞助以及蔚来汽车为大会展区带来的特别策划。在大家的共同助力下，我们得以持续推动技术的传播与发展，为行业创新注入不竭源泉。</p><p></p><p>经统计，AICon 上海站现场听众累计超过 1000 人次。我们深感荣幸与欣慰，衷心感谢每一位参与者的鼎力支持与不断鼓励。正是因为有了大家的热情参与和积极贡献，我们才能坚定不移地追求目标，致力于成为技术传播领域的佼佼者。我们将持续不断地提升内容的质量，致力于打造更加优质、更具包容性的交流平台，让每一个人都能在这里找到启发和灵感，一齐推动技术领域的创新与突破，为未来的科技进步贡献力量。</p><p></p><p>大会 PPT 获取通道已开启，关注&nbsp;AI 前线&nbsp;公众号，后台回复“PPT”，即可获取 PPT 下载地址！（由于讲师所在企业限制，部分 PPT 仍在审查或不对外公布，详情见大会官网日程） &gt;&gt;&gt;</p><p></p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日在<a href="https://qcon.infoq.cn/202410/shanghai/">上海举办 QCon 全球软件开发大会 </a>"，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元）。如您感兴趣，可点击「阅读原文」查看详情或联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p>期待下一场大会再见！</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>