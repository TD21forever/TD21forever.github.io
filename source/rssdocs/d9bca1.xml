<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/bVjdiotpLlL7NliKotTs</id>
            <title>架构师会被AI秒了吗？</title>
            <link>https://www.infoq.cn/article/bVjdiotpLlL7NliKotTs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bVjdiotpLlL7NliKotTs</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 May 2024 10:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 重构, 大模型, 软件架构, 生产力
<br>
<br>
总结: 演讲嘉宾郭东白副总裁探讨了大模型时代对软件架构师的影响，强调了重构的重要性以及大模型对软件研发和生产力的影响。大模型的出现改变了传统软件开发中的角色，提高了测试速度，可能重新定义程序员的价值。在大模型时代，大模型成为主要生产力，对软件研发的成本和人性等方面产生重大影响。 </div>
                        <hr>
                    
                    <p>演讲嘉宾 | 郭东白 Coupang 副总裁</p><p>编辑 | 华卫</p><p></p><p>对于一名架构师，最能让其感到兴奋的词汇莫过于“重构”。只要有重构的需求，就意味着有工作可做。在大模型时代，全球范围内的大模型正在重新定义软件的面貌。在当前的世界正在重新定义软件的背景下，作为架构师应该如何应对？面临哪些机会和挑战？这些都是本演讲想要探讨的主题。</p><p></p><p>本文由 InfoQ 整理自 QCon 北京 2024 主论坛演讲《大模型时代的架构思维》，经郭东白老师授权发布。以下为演讲实录：</p><p></p><p>本演讲将从大模型讲起，分析大模型的本质及其影响；然后我们将探讨大模型如何影响软件研发，以及它对软件架构的冲击是什么；最后将讨论我们应该如何应对这些挑战和冲击。这是一个清晰的逻辑过程：首先理解问题，然后制定应对策略。</p><p></p><p></p><h1>大模型时代的新生产力</h1><p></p><p></p><p>大模型的基本原理很简单：你有一套训练数据，通过训练生成式模型，产生许多候选样本，然后人工挑选出较好的内容。接着，使用验收模型将人工挑选的过程自动化。这样一来，我们就形成了一个从生成式模型到验收模型的完整过程。当这个过程上线后，就可以形成一个循环，使得模型能够持续学习和进步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a3da07eb93e50363be1f7ca8880bff75.png" /></p><p></p><p>大模型的突破之处在于其处理大量数据的能力，这使得它能够涌现出类似语义理解的能力。这让我们可以感觉到，大模型似乎能够在完全不同的语境中，甚至是跨越传统意义上的语言界限，实现近乎无损的语义转换。这就是大模型突然变得如此热门的原因。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e7a647f993039c2776b2e28bb7b504e.png" /></p><p></p><p>事实上，我在一年前就已经在网易的一次演讲中提到了这些观点。我认为这个逻辑非常强大，它解释了为什么大模型对我们程序员或者说我们这个行业的冲击是最大的。大模型的冲击并不在于我们现在讨论的各种应用，而是在于它改变了传统软件开发中的角色。在传统软件中，生成式模型和验收模型都是由程序员和测试人员来完成的。但现在，生成式模型和验收模型都变成了机器模型，这就改变了程序员的位置。</p><p></p><p>如果你有一个从需求到代码，再到测试的映射过程，并且这些过程都能够进行自洽性验证，那么你可以在多种语言、不同的编译环境、测试环境和运营环境中形成多次约束，确保整个语义系统的正确性。这就是我们所说的闭环，而这个闭环在我们的领域是第一个形成的，它是一个非常强的语义闭环。因为它所施加的约束是严格的：代码要么无法运行，要么运行结果与预期验证结果不一致，这些都是容易出现的错误。在大模型出现之前，我们已经能够进行许多自动化测试，包括半自动化和半智能的测试。而大模型的出现使得测试过程变得更加迅速。拥有了这种能力之后，我们需要重新审视程序员的价值所在。如果产品和用户能够利用生成的文档来完全验证开发过程，那么在许多情况下，程序员的角色可能变得多余。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1642abddeb2ccea4c07af016b7f9398.png" /></p><p></p><p>从今天的角度来看，我去年写的幻灯片其实已经完全成为事实了。图灵测试已经通过，人类的简单意图可以在代码层面上做到无损，而且成本低了很多，提高了好几个数量级的速度。</p><p></p><p>对于程序员来说，我们正处于不同的时代轮换中。在大模型时代到来时，它应该成为主要的生产力。就像马车时代马的重要性一样，在汽车时代汽车成为主要生产力，在大模型时代，大模型就是为该场景提供主要生产力。这意味着，并不是说大模型要取代所有人，而是如果一个人加上一个模型就比三个人快，那就已经很好了。就像汽车和马车一样，马在当时也是非常重要的生产力。我还记得之前举过的例子，直到现代社会发明蒸汽机之前，马一直具有很高的价值。在不同的社会中，马的价值是人的价值的三倍。如果某样东西的价值是你的两倍，那么它必然会取代你。这是一个非常有意义的观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/762e53bd42d4e72e4464fe4f4a8e10db.png" /></p><p></p><p></p><h1>大模型会如何影响软件研发</h1><p></p><p></p><p>接下来我想谈谈大模型对软件研发的影响。在我的极客时间架构课程和书中，我描述了传统软件研发中架构师的角色，通常被限定在一个相对较小的范围内。实际上，一个真正的架构师应该拥有更广阔的视野，考虑的因素应当包括人员、目标、经济价值、环境、过程控制以及文化等多个方面。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a991e21b917b1dce81b8b7ef27b0ce3.png" /></p><p></p><p>当我们进一步审视这个问题时，我们可以思考 AI，特别是大模型，对这些架构要素的影响程度。在这些架构要素中，哪一个会受到最大的影响？在进行大规模的架构活动，比如重构时，我们必须为这个活动设定一个明确的目标。这个目标定义了我们努力的方向和期望达成的成果。我们需要明确这个目标是什么，并考虑如何衡量成功与否。</p><p></p><p>人性是一个极其重要的元素，正如韦青老师之前提到的，它在架构中扮演着关键角色。人性涉及到参与者，包括目标用户的人性。这些因素会如何影响你的软件架构呢？此外，软件架构始终是一个成本问题，在任何企业中，成本都是不可忽视的因素，还有当前的计算软件环境。</p><p></p><p>我的评估是，大模型对我们最大的冲击在于成本方面。这种冲击可能会持续今后几年。人们关注大模型并不仅仅是因为它们有趣，而是因为它们能够以更低的成本完成工作。例如，在数据标注任务中，与传统的人工标注相比，使用大模型来生成样本的成本更低，而且生成的样本与人工标注的样本具有相同的价值。这就使得人工标注变得不再必要，大模型的引入成为了一种替代人工的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41be6e1947b194d01ef443f9c56b24c1.png" /></p><p></p><p>从能力的角度来看，大模型对传统架构师的影响是客观存在的。我之前已经分享过相关的幻灯片，今天我们再次审视这个问题，即大模型的到来到底对哪些方面产生了冲击。其中，红色标记的部分显示，代码交付这一环节受到的冲击最为严重，这种冲击不仅限于未来，而是已经从现在就开始了。</p><p></p><p>在当前的软件开发环境中，即使是像兼职架构师这样的角色，也需要进行进度沟通等工作。然而，这些工作的价值已经不如以往。因为现在我们可以通过代码自动生成的总结来更准确地了解代码的改动情况，这种自动化的总结比人工编写的总结更为可靠。它能够清晰地展示出具体的代码更改，而不仅仅是描述完成了哪些任务。</p><p></p><p>在整个软件开发过程中，越是偏向个体研发的能力，受到的冲击就越大。换句话说，那些依赖于单兵作战的开发工作，如编写代码、调试等，更容易被自动化工具和智能系统所取代。而越往下，即越偏向于对整个开发场景的控制和管理，这些工作受到的冲击相对较小。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9dbd7cd14011485c07b1bbf23181c9de.png" /></p><p></p><p></p><h1>大模型对软件架构的冲击</h1><p></p><p></p><p>当我们意识到这些冲击必然会发生时，作为架构师或至少是决策者，我们应该采取什么行动呢？我们是与软件架构相关的独立决策者，我们应该考虑做些什么？首先，我们需要思考我们要做什么决定，必须清楚我们到底是为企业创造了什么价值。我们需要分析出我们的价值，并在之前创造的价值不再存在时重新定位。作为架构师，真正的价值创造来自于弥补其他人决策盲区，这一点非常重要。你之所以被认为是架构师，是因为你能看到别人看不到的东西。</p><p></p><p>在大模型时代，我们必须认真思考：大模型时代是否存在决策盲区？这些决策盲区在哪里？大模型的出现是否会导致新的决策盲区？举例来说，钉钉正在研究 AI 模型，这些模型涉及数千个人工智能代理和人类之间的网络配对。例如，他创建了成千上万个这样的配对。在这些配对中，你该如何应对？你在这种情境下如何改变自己在这个新的决策环境中，你能弥补的盲区的价值定位是什么？我认为最重要的一点是，当前的大模型无法回答的问题。</p><p></p><p>大模型的盲区就是架构师创造价值的所在。实际上，关于我在《架构思维，从程序员到 CTO》这本书中提到的每一个架构要素，大模型都无法回答。第一个要素是大模型不知道你要解决什么问题。比如你要开发一个操作系统或做一个 AI 代理，你要解决什么问题是别人无法知晓的，作为架构师必须清楚地知道你的商业目标是什么。同样，人性方面，大模型的引入会改变某些人性机制。我们本来具有怎样的人性？现在突然引入大模型，会影响我们的人性。尽管大模型本身可能不具备人性，但是它的引入会给企业软件开发带来人性挑战。开发人员会怎么想？用户会怎么想？用户可能并不关心软件是由程序员编写还是由大模型生成的，但是企业人员可能会关心。经济价值会发生何种变化？收入和成本会发生巨大变化，因为作为架构师，你组织的架构活动中最大的成本是人力和时间。有了大模型，这两个成本都会发生巨大变化。你应该如何应对这些变化？还有环境和各种过程，我就不一一赘述了，但这些都是大模型无法回答的问题，因为大模型不了解你的工厂、你的企业，或者你的架构活动需要处理什么事情。因此，作为架构师，我们有机会为整个企业注入最佳的相关决策。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/9146c3dbef7dab93428d06dba17cb0e2.png" /></p><p></p><p>大模型对软件架构师的冲击很小，甚至会可能带来更大的市场需求。面对大模型可能带来的盲区，我持乐观的态度。尽管我的判断可能不完全准确，但我个人认为大模型对架构师职位的冲击相对较小，反而可能会带来更多的需求。在过去，一个拥有 100 多名员工的企业可能只需要两到三名架构师。但在将来，架构师的需求可能会增加，尽管与此同时，与架构师合作的程序员数量可能会减少，比如只有 10 到 15 人。这种情况可能导致架构师的角色变得更加普及，不再像以前那样是一个特殊的职业。但这并不意味着架构师的传统职责就消失了。</p><p></p><p></p><h1>架构师在大模型时代的价值定位</h1><p></p><p></p><p>正如我们在之前讨论的中提到的，架构师的职责依然存在，包括确保架构活动有正确的目标、构建符合人性的架构等。这些职责在大模型时代仍然至关重要。</p><p>作为架构师，我们需要思考在这个时代中如何定位自己的工作。我们需要继续遵循那些在大模型时代之前总结的生存法则，这些法则至今仍然基本正确。例如，架构活动必须有一个正确的目标，这是任何架构师工作的基础。同时，我们也需要考虑到构建没有人性考量的架构可能会面临的挑战。在大模型时代，架构师的角色和职责可能会发生变化，但他们的核心价值和重要性仍然不变。在有限的资源下最大化经济价值的重要性。虽然不敢说 100% 肯定，但我对此有很高的信心。毕竟，企业需要盈利和承担支出，我们的目标始终是最大化企业的经济价值。此外，软件架构必须适应环境的需求，这一点不会因为大模型的出现而改变。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c12a4fd2c2dd9c81bb6be8565477df7.png" /></p><p></p><p>接下来，我想强调的是设定唯一且正确的目标，这是架构师生存法则的第一步。在大模型时代，这一点变得更加重要。我给它打了 5 分，表明其重要性。在大模型时代，目标的精确性至关重要。一旦更改目标，就会产生后验成本。大模型的开发涉及大量的训练成本，尤其是生成大量训练数据的成本，这可能非常高昂，有时甚至超过训练本身的成本。在启动大模型项目之前，必须明确你想要解决的问题和目标，清晰定义目标，并确定衡量成功与否的指标。对这些问题的深入思考和明确定义是架构师最大的贡献。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c2ac12a8b20f699e6fb59d04ac41c47.png" /></p><p></p><p>在我们会场中，可能有超过一半的人参与了大模型相关的工作，而大约有三分之一的人可能从事与大模型相关的工作。这表明，大模型领域的参与度相对较高。在进行大模型相关工作时，最重要的是必须设定一个清晰、可量化、可观测且可持续优化的目标。这一点不仅适用于大模型，也适用于任何算法密集型的工作。然而，对于大模型来说，这一点尤其重要。我自己在参与大模型项目的过程中发现，为公司创造最大价值的关键在于清晰地定义目标。我回顾过去可能不够明智的决策时，往往发现问题出在目标设定不够精确上。因此，明确目标是最重要的事情。</p><p></p><p>法则 3 是在有限资源下最大化经济价值。在大模型的开发和应用中，成本是一个不可忽视的重要因素。首先，定义目标是至关重要的第一步，因为在大模型的尝试中，成本是非常高昂的。大模型不仅在实施阶段成本不菲，而且在训练样本的准备上也需要大量的投入。此外，训练过程本身的成本也不低。除了训练成本，线上计算成本，也就是进行推理的成本，也会因公司和应用场景的不同而有所差异，但通常来说这也是一个不小的开支。持续运维的成本同样不可小觑，因为随着场景的变化，模型可能会逐渐失效，需要重新训练。在某些情况下，这种情况可能会好一些，但成本仍然是一个需要考虑的因素。</p><p></p><p>作为架构师，在进行架构设计时，还需要考虑迁移成本。如果你的公司之前没有使用大模型，或者使用的是小模型，甚至是完全基于规则和代码的场景，那么向大模型场景的迁移将涉及巨大的成本。迁移成本包括了从旧系统到新系统的转换，可能还包括了人员培训和重新设计架构等方面的投入。</p><p></p><p>随着大模型的到来，它们有潜力带来增量收入，这意味着在考虑成本的同时，我们也需要考虑收入。通过与国内外做大模型的朋友们交流，我发现许多公司在投入大模型时并没有充分考虑其经济价值。他们往往是因为看到其他人在做，就跟风投入，而没有深思熟虑大模型究竟能为企业带来什么具体价值，以及这些价值是如何通过大模型创造的，更不用说如何在商业模式和成本结构中实现这些价值了。很多人在没有想清楚这些问题的情况下就进行了投入，这可能导致作为架构师的你在一个错误的决策中浪费了大量的时间和资源，从而未能创造真正的价值。这是架构师在决策时需要深思熟虑的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cab1d51e1b9c5f81d40a69077ff7f9de.png" /></p><p></p><p>如果我们作为架构师要考虑投入大模型的决策，我们必须考虑到实施成本、训练成本、计算运维成本以及迁移成本等所有相关成本。在我看来，多数企业应该考虑在大模型上进行投入，因为这些成本随着时间的推移，按照摩尔定律逐渐降低。特别是训练样本的生成成本，由于在许多场景中可以与其他大模型共享，因此训练样本的成本也有可能大幅降低。</p><p></p><p></p><p>从竞争的角度来看，我认为大多数场景下，企业应该对大模型进行投入。原因在于，更大的模型可能对训练样本的数量要求更低，企业可以在较少的数据基础上训练出有效的模型。此外，考虑到潜在的竞争优势，如果你的竞争对手不需要投入大量成本就能实现大模型的价值，而你却因为规模或其他原因无法做到，这将使你处于不利地位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c81e34ade6daa914d978e7449bd6775.png" /></p><p></p><p>具体如何实施大模型的投入是一个需要深思的问题。在开始使用大模型时，我们可能会面临诸如“为什么我们需要做这个？”或者“我们的企业规模太小，如何投入大模型？”等问题。在当前的投资环境中，投资者可能对大模型的投入持谨慎态度，这进一步增加了决策的复杂性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48694d5b18e2ea2609b4908dcbabbdac.png" /></p><p></p><p>在这个大模型时代，作为架构师，我们需要思考如何创造价值并确保自己不被时代淘汰。我在《架构思维，从程序员到 CTO》一书中提到了一个重要的概念——原子架构，或者说原子价值单元。意思是我们应该从最基本的价值创造单元出发来考虑大模型的投入。原子价值单元首先应该是可识别的功能。在大模型的背景下，这很容易理解，例如一个用户可交互的代理。用户使用这个功能后，我们能够获得一个精确的反馈值。如果无法度量出这样的反馈值，那么这个功能就不是用户可识别的。</p><p></p><p>最重要的是，我们需要明确大模型能为企业或项目真正创造什么价值。很多人在大模型项目中工作了很长时间，但仍然没有弄清楚这个问题。我们需要识别出大模型项目中最小的原子价值单元，即那些只能通过大模型才能带来的核心能力。只有找到了这个最基本的价值单元，我们才能确保大模型的投入能够为企业带来实质性的价值。在审视许多声称与大模型相关的应用场景时，如果你仔细研究，会发现它们实际上与大模型并没有太大关联。包括我们之前讨论的一些案例，很多场景并不真正涉及大模型，它们可能是基于传统的小模型，或者是传统的其他类型的模型，如 Transformer 架构等。如果你真正意识到大模型可能成为下一代的生产力，那么关键是要在企业中找到能够利用大模型突破的场景。如果选错了场景，再多的努力也可能是徒劳，因为你可能无法找到正确的目标和反馈链路，从而无法实现能力的提升。首先要做的是清晰地认识到，你的企业能通过大模型创造出什么样的独特能力，以及为什么这种能力只能由大模型带来。如果不是非大模型不可，那么很可能有其他成本更低或训练时间更短的途径。</p><p></p><p>一旦你找到了正确的场景，你可以迅速进行闭环实验。在国内，这是一种很常见的做法。实验不一定非要从头开始自己做，你可以利用现有的 API，比如 OpenAI 的 ChatGPT 或其他模型的 API，先进行尝试，看看是否可行。如果实验结果是肯定的，那么接下来你可以投入到特征工程中，确保你的模型能够最大限度地发挥作用。我们的经验表明，大模型对特征工程特别敏感，因此在这一环节需要做到最好。</p><p>接下来，你需要找到最小的原子价值单元，即在最小的场景中使用模型，以最小化总成本。在大模型中，合规性挑战也是一个需要考虑的因素，尤其是在国际上，对大模型的正确性和合规性的挑战可能会更加严峻，这可能是国内外在大模型应用上的一个重要差异。</p><p></p><p>在当前阶段，进行迁移的成本过高，因此在这个阶段进行迁移并不合理。如果你真的想要实施大模型，最好是在一个全新的场景中开始，而不是尝试将大模型应用到已经存在的场景中。这是因为传统的模型或非模型驱动的场景，如那些基于规则的系统或已经优化很久的小模型，迁移到大模型的成本可能非常高，而且成功的可能性不大。如果你尝试在这些场景中应用大模型，很可能会失败。因此，更明智的做法是寻找一个全新的场景来开发和应用大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91aa5e342e781dc5ca5b5613c2d4f87e.png" /></p><p></p><p></p><h1>总结</h1><p></p><p></p><p>对于程序员和架构师而言，大模型时代已经到来。作为架构师，我们的任务是弥补大模型的盲区，并确保它们能够在企业中发挥最大的价值。为了实现这一目标，我们需要关注以下三件事情。</p><p></p><p>确保有正确的目标：明确你希望通过大模型实现什么，以及这些目标如何与企业的整体战略相匹配。找出大模型真正能创造的价值：理解大模型的优势和潜力，并确定它们如何为企业带来具体的益处。找到企业应用场景中的最小价值单元：识别出大模型能够带来的最小且最有价值的功能单元，从这个点开始构建和实施。</p><p></p><p></p><p>从这个基础出发，我们可以更有效地将大模型融入到企业的架构实践中，确保它们能够为企业带来真正的价值。这是我自己的思考，并且我们已经在一定程度上实践了这些理念，认为这是将大模型成功引入企业的最佳路径。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6pkAgDI2fwxUu51icDpN</id>
            <title>AWS Batch为大规模模拟引入了多容器作业</title>
            <link>https://www.infoq.cn/article/6pkAgDI2fwxUu51icDpN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6pkAgDI2fwxUu51icDpN</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 May 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AWS Batch, 多容器作业, 简化开发, 批处理管理
<br>
<br>
总结: AWS最近宣布支持AWS Batch中的多容器作业，这一新功能简化了运行模拟的过程，特别是在测试复杂系统时。多容器作业减少了作业准备所需的工作量，并且无需使用自定义工具来集成多个团队的工作，从而加快了开发时间。通过在一个作业中运行多个容器，开发人员现在可以使用较小的模块化容器来表示不同的系统组件。AWS Batch是一组批处理管理功能，可帮助开发人员、科学家和工程师在云上运行批处理计算作业。 </div>
                        <hr>
                    
                    <p>最近，AWS 宣布通过管理控制台支持 AWS Batch 中的多容器作业（Multi-Container Jobs）。这一新功能简化了运行模拟的过程，特别是在测试复杂系统（例如汽车自动驾驶和机器人等系统）时。</p><p></p><p>根据云供应商的说法，多容器作业减少了作业准备所需的工作量，并且无需使用自定义工具来集成多个团队的工作，从而加快了开发时间。AWS 首席布道师（EMEA）Danilo Poccia写道：</p><p></p><p></p><blockquote>传统上，AWS Batch 只允许单容器作业，并需要额外的步骤来将所有的组件合并到一个单体（monolithic）容器中。它也还不允许使用单独的“边车”（sidecar）容器（边车容器是通过提供数据日志等附加服务来补充主应用程序的辅助容器）。这项额外的工作需要跨多个团队进行协调（……），因为任何代码的更改都意味着要重建整个容器。</blockquote><p></p><p></p><p>AWS Batch 是一组批处理管理功能，可根据提交的批处理作业的数量和特定资源动态配置所需的计算资源数量和类型。该服务可帮助开发人员、科学家和工程师在云上运行批处理计算作业。</p><p></p><p>根据 AWS 的说法，这项新功能使得在汽车自动驾驶和机器人等领域运行大规模模拟变得更容易，这些工作负载通常划分模拟本身和与模拟交互的被测系统。IPG Automotive、MORAI 和 Robotec.ai 是已经在运行多容器作业的 AWS 客户之一。Poccia 补充道：</p><p></p><p></p><blockquote>使用多容器作业可以减少作业准备所需的工作，并且无需使用自定义工具来将多个团队的工作合并到单个容器中，从而加快了开发时间。它还通过定义明确的组件职责来简化 DevOps，使团队能够快速识别和解决自己专业领域的问题，而不会分心。</blockquote><p></p><p></p><p>通过在一个作业中运行多个容器，在执行批处理作业之前不再需要将系统重新构建为单体容器。开发人员现在可以使用 AWS 管理控制台、CLI 或 SDK 来定义多个较小的模块化容器，以表示不同的系统组件。</p><p></p><p>AWS 并不是唯一一家提供批处理管理功能的云供应商：微软提供了 Azure Batch，该服务可帮助开发人员跨可扩展的虚拟机（VM）集合来管理计算密集型的工作；Batch 是谷歌云（Google Cloud）的托管服务，用于调度、排队和执行批处理工作负载。但是，目前两者都不支持多容器作业。</p><p></p><p>这一新功能可在任何提供 AWS Batch 的区域使用，并且使用 AWS Batch 或多容器作业无需支付额外的费用。</p><p></p><p>原文链接：<a href="https://www.infoq.com/news/2024/04/aws-batch-multi-container-jobs/">https://www.infoq.com/news/2024/04/aws-batch-multi-container-jobs/</a>"</p><p></p><p>声明：本文为 InfoQ 翻译整理，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lvmo0NSQ0VSKgD4yJTo0</id>
            <title>谷歌、OpenAI 都搞起了AI “造人”？创始团队：开源AI基因编辑器只是冰山一角</title>
            <link>https://www.infoq.cn/article/lvmo0NSQ0VSKgD4yJTo0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lvmo0NSQ0VSKgD4yJTo0</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 May 2024 10:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 译者, 策划, AI, 基因编辑器
<br>
<br>
总结: 人工智能系统开发了精确编辑人类 DNA 的开源工具，Profluent 公司使用人工智能算法设计了高功能基因组编辑器 OpenCRISPR-1，并将其开源，这一技术代表了人工智能驱动生物设计领域的重大飞跃。 </div>
                        <hr>
                    
                    <p>译者 | 王强、华卫</p><p>策划 | 华卫</p><p></p><p>“AI 能编辑 DNA 了，还是开源版！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a076216c7121b634029e7fda63273c0.png" /></p><p></p><p>OpenCRISPR-1 开源链接：</p><p>https://github.com/Profluent-AI/OpenCRISPR</p><p></p><p>今天，人工智能系统不止可以设计出创作诗歌、代码和视频的模型，还开发出了精确编辑人类 DNA 的开源工具。这不仅是 AI 的巨大进步，还预示着，将来科学家可以比现在更精确、快速地对抗各种疾病。</p><p></p><p>近日，美国一家名为 Profluent 的初创公司公开介绍了这项技术，并预计于下个月在美国基因和细胞治疗学会年会上发表相关论文。“使用人工智能技术创建基因编辑机制史无前例，”美国加州大学旧金山分校生物工程和治疗科学系教授兼系主任 James Fraser 表示。</p><p></p><p>据悉，Profluent 是在分析了大量生物数据后，通过对 CRISPR-Cas 序列进行人工智能算法建模来设计出高功能基因组编辑器，并将其命名为 OpenCRISPR-1。通过 OpenCRISPR-1，该公司的人工智能系统从大规模序列和生物学背景中学习，产生了数百万种自然界中不存在的 CRISPR 样蛋白，从而成倍扩大了几乎所有已知的 CRISPR 家族。</p><p></p><p>并且，OpenCRISPR-1 基因编辑器正在被开源。这意味着，其允许个人、学术实验室和公司免费试用该工具。很多研究人员都会把开发的人工智能底层驱动软件开源出来，让其他人可以在他们的成果基础上继续开发工作，以加速新技术的开发步伐，但像 OpenCRISPR-1 这类生物实验室和制药公司开源技术发明的情况并不常见。不过，Profluent 并没有开源该编辑器本身的技术内容。</p><p></p><p>Profluent 还透露，OpenCRISPR-1 只是冰山一角，他们的平台能够随意生成更多的基因编辑系统。然而，尽管目前 OpenCRISPR-1 还没有投入临床，但已经招致不少除应用效果以外的担忧。</p><p></p><p></p><h1>完全由 LLM 驱动</h1><p></p><p></p><h1>将蛋白质多样性扩大 4.8 倍</h1><p></p><p></p><p>在这项研究中，Profluent 展示了世界上第一个使用人工智能从头开始设计的分子的精确基因编辑。基因编辑器是复杂的系统，需要多结构域蛋白质、DNA 和 RNA 之间复杂的空间和时间相互作用。使用人工智能设计功能差异化的基因编辑器，代表了人工智能驱动生物设计蓬勃发展领域的重大飞跃。</p><p></p><p>OpenCRISPR-1 的技术是由人工智能驱动、Cas9 样蛋白和指导 RNA 组成，完全使用 &nbsp;Profluent 的大型语言模型（LLM）开发。该模型学习的是氨基酸和核酸序列，这些化合物定义了科学家用来编辑基因的微观生物机制。也就是说，它分析了从自然界中提取的 CRISPR 基因编辑器的行为，并学习该如何生成全新的基因编辑器。</p><p></p><p>“这些人工智能模型从序列中学习，无论这些序列是字符、单词、计算机代码还是氨基酸序列。”Profluent 首席执行官 Ali Madani 表示。</p><p></p><p>据介绍，生成蛋白质语言模型通常在跨越广泛功能的大型、多样化的天然蛋白质序列数据集上进行预训练，可以生成反映天然蛋白质特性的真实蛋白质序列。然而，对于特定的应用，如产生新的基因编辑器，就需要将模型引导到特定的目标蛋白质家族。</p><p></p><p>为此，Profluent 进行了详尽的数据挖掘，以构建迄今为止最广泛的 CRISPR 系统数据集，被称为 CRISPR-Cas 图谱。为生成新的 CRISPR-Cas 蛋白，他们又在 CRISPR-Cas 图谱上训练了一个蛋白质语言模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffa69783368c36158e23010629f0650e.png" /></p><p></p><p>图：生成的序列极大地扩展了 CRISPR 相关蛋白质家族的多样性，以蛋白质簇的数量来衡量，图中显示了每个蛋白质家族在不同类型的 CRISPR-Cas 系统中被发现的频率。</p><p></p><p>从该模型中生成了 400 万个序列，并使用生物信息学技术来去除简并序列，确定每个生成的蛋白质属于哪个 CRISPR-Cas 家族后，他们发现，这些模型产生的蛋白质将几乎所有天然存在的 CRISPR-Cas 家族的多样性扩大了 4.8 倍，并且之后可以生成更多的序列进一步扩大这种多样性。</p><p></p><p>鉴于 SpCas9 的广泛采用和临床成功，其使用模型生成了可与 SpCas9 互操作的 Cas9 样蛋白，并选择了其中 48 个生成的序列，用于在人类细胞中进行严格的功能表征。他们发现，当与脱氨酶配对时，OpenCRISPR-1 和 SpCas9 &nbsp;在精确编辑靶基因组中的单个碱基时具有相似的活性和特异性。此外，他们还能够保持碱基编辑活性，同时使用由另一种 Profluent 训练的蛋白质语言模型生成的脱氨酶来提高特异性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af8332d417b7cda2c7cc1361e1fdbd64.png" /></p><p></p><p>图：对于测试的 5 种生成的核酸酶中的 4 种，使用模型生成的 sgRNA 提高了编辑效率。</p><p></p><p>最后，为了进一步优化生成的核酸酶活性， Profluent 还训练了一个模型来为任何给定的 Cas9 样蛋白生成相容的 sgRNA。与 SpCas9 的 sgRNA 相比，这些生成的 sgRNA 可以提高所测试的五种蛋白质中四种产生的核酸酶的活性。</p><p></p><p></p><h1>CRISPR 基因疗法的“升级版”</h1><p></p><p></p><p>“我们与 OpenCRISPR 的意图是与尖端研究机构和药物开发人员合作，以一种强大而实用的方式安全地加速 CRISPR 基因疗法的开发。”Profluent 首席商务官 HilaryEaton 表示。</p><p></p><p>目前，基于 CRISPR 的技术已经改变了科学家研究和对抗疾病的方式，并提供了能够改变镰状细胞性贫血和失明等遗传疾病患者的治疗方法，但仍需加速发展以治疗数千种其他还无治愈之法的疾病。据介绍，OpenCRISPR-1 正是基于 CRISPR 的生物机制所构建。</p><p></p><p>源自微生物的基于 CRISPR 的基因编辑器虽然功能强大，但当移植到非天然环境（如人类细胞）中时，通常会显示出显着的功能权衡，人们希望能够生产出比经过数十亿年进化而来的天然基因编辑器更灵活、强大的基因编辑器。人工智能系统的设计恰恰能提供一种强大的替代方案，有可能绕过进化约束生成具有最佳属性的编辑器。</p><p></p><p>“我梦想着这样一个世界，我们可以在几周内按需提供 CRISPR。”美国加州大学伯克利分校创新基因组学研究所的基因编辑先驱兼科学主任 Fyodor Urnov 说。</p><p></p><p>事实上，OpenCRISPR-1 是整个业界努力构建可以改善医疗保健的人工智能技术的一个缩影。例如，华盛顿大学的科学家正在利用 ChatGPT 和 Midjourney 等图像生成器背后所采用的人工智能技术方法来组装全新的蛋白质，并致力于加速新疫苗和药物的开发。</p><p></p><p>“从长远来看，这可以通向一个快速为个人定制药物和治疗方法的时代，定制速度甚至比我们现在的还快。”Urnov 认为，生成式人工智能系统具有巨大的潜力，它们往往会通过从越来越多数据中学习的过程来快速改进自身。如果像 Profluent 这样的技术继续改进，其最终可以让科学家以更精确的方式编辑基因。</p><p></p><p>而目前看来， Profluent 也具备技术进化的资金支撑。3 月 21 日，Profluent 宣布完成 3500 万美元追加融资，融资总额达到 4400 万美元。这笔融资由 Spark Capital 领投，现有投资者 Insight Partners 和 Air Street Capital 以及来自 OpenAI、Salesforce、Octant Bio 和谷歌（包括谷歌 DeepMind 首席科学家 Jeff Dean）的天使投资人组成的财团也参与了投资。该公司此前还曾从 Insight Partners、Air Street Capital、AIX Ventures 和 Convergent Ventures 募集到 900 万美元种子轮资金。</p><p></p><p></p><h1>临床可能引发副作用</h1><p></p><p></p><p>虽然这项研究已经表明，人工智能模型可以生成能够编辑人类基因组的工具。但目前 Profluent 还没有对基因编辑器 OpenCRISPR-1 进行临床试验，因此尚不清楚其是否能达到或超过 CRISPR 的性能表现。</p><p>不过，可以确定的是，短期内这一技术进展不太可能影响医疗保健领域。Urnov 表示，事实上科学家们并不缺乏可以用来对抗疾病的天然基因编辑器，推动这些编辑器通过临床前研究的成本才是瓶颈所在，如安全性、制造和监管审查，经过这些步骤后才能将其用于治疗患者。</p><p></p><p>此外，这样的合成基因编辑器案例还引发了其他担忧。长期以来，科学家一直警告不要将 CRISPR 用于人类身体改造和治疗领域，因为这项技术相对较新，可能会产生引发癌症等不良副作用，还能提供一些不道德的用途，如对人类胚胎进行基因改造。</p><p></p><p>对此，Fraser 的看法是，“一个不道德的人，并不在乎他们使用的基因编辑器是不是人工智能创建的，他们只会继续使用现有工具。”</p><p></p><p>原文链接：</p><p>https://www.nytimes.com/2024/04/22/technology/generative-ai-gene-editing-crispr.html</p><p>https://www.biorxiv.org/content/10.1101/2024.04.22.590591v1</p><p>https://www.businesswire.com/news/home/20240422399482/en/Profluent-Successfully-Edits-Human-Genome-with-OpenCRISPR-1-the-World%E2%80%99s-First-AI-Created-and-Open-Source-Gene-Editor</p><p>https://www.profluent.bio/blog/editing-the-human-genome-with-ai</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tR5ZZzt9t6MH1IOfOTsj</id>
            <title>强大到不敢给普通人用！史诗级大模型Sora如何让众行业一夜变天？</title>
            <link>https://www.infoq.cn/article/tR5ZZzt9t6MH1IOfOTsj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tR5ZZzt9t6MH1IOfOTsj</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 May 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 视频生成模型, Sora, 技术整合, DiT结构
<br>
<br>
总结: 2024年2月，OpenAI发布了视频生成模型Sora，其利用DiT结构取得了明显优势，成为全球关注焦点。Sora不仅能生成更长时间的视频，还能保持视频的连贯性和表现效果。其在影视行业中有望降低制作成本，提高制作效率。 </div>
                        <hr>
                    
                    <p></p><h2>视频生成模型“新王登基”，Sora 何以成为全球焦点？</h2><p></p><p>&nbsp;</p><p>2023 年以来，多模态视频生成技术取得了显著的进展和突破，从 Runway 到 Pika 再到年末的 VideoPoet，视频生成模型进入到加速阶段。2024 年 2 月，OpenAI 旗下视频生成模型 Sora 正式对外发布。Sora 一名源于日文“空”（そら sora），取自天空之意，以示其无限的创造潜力。与 Runway、Pika、VideoPoet 等“前辈”相比，Sora 在视频生成效果和质量上具有明显优势。也正因如此，Sora 一经发布就在全球范围内掀起了讨论热潮，迅速成为当前最受关注的模型之一。</p><p>&nbsp;</p><p>“Sora 的出现时间要比我们预想的要早很多，OpenAI 已经提前带来了惊喜”。WeShop 唯象 GM 吴海波在接受 InfoQ 采访时提到，从技术层面来看，Sora 并没有引入全新的理论框架，而是将现有技术进行了新的整合。自从 Sora 问世以来，人们对其背后的技术进行了深入分析。比如，Meta 的谢赛宁曾在 Twitter 上进行拆解，其认为 Sora 所采用的 DiT 结构，就是基于他在 ICCV 2023 发布的 DiT（Diffusion Transformer）思路构建的，这也是支撑 Sora 的一个重要基础。</p><p>&nbsp;</p><p>在模型架构方面，Runway、Pika 等模型底层采用的是扩散模型（Diffusion Model）技术，利用高斯噪音和 prompt，再通过 U-Net 对噪音进行解析，实现逐帧的渲染。虽然 prompt 通过 Transformer 技术得到了前后文的联系，但视频生成中却没有较大时间轴或前后联系的概念，从而导致先前的视频只能生成三四秒，画面跳跃跳帧等问题严重。</p><p>&nbsp;</p><p>而 Sora 利用 Transformer 替代 Diffusion 的 U-Net，不限制原始视频的尺寸，不仅能通过 Transformer 技术保证前后的连贯性，还能保证生成视频在各个画幅比例下都有很好的表现，从而生成时长更长、效果更好的视频。</p><p>&nbsp;</p><p>目前，Sora 能够生成 1 分钟的视频，深圳市鼎盛方圆科技发展有限公司创始人黄鸿波表示，理论上来看，Sora 是能够生成生成更长时间的视频的，但其中的不确定性会更多，也会需要更高的算力。“从零到一很简单，但再想往上则需要质的飞跃，难度比较大”。</p><p>&nbsp;</p><p>比起生成的视频时长，黄鸿波认为，Sora 这类视频生成模型更应解决的是如何保持人物一致性和场景一致性。这两点都是目前业内比较难以克服的难题。以人物为例，一段完整的视频中不仅存在主角，还存在配角和各种人物关系。在电影和电视剧的实际拍摄中，人是真实存在不会改变的，但 Sora 或其他目前现存的技术都无法保证人物的一致性。人物的每次生成，脸型、肤色、眼睛大小、痣的位置等都会发生变化。场景同样如此，不同的镜头会从不同角度进行拍摄，但周围的场景需要有一些变化。“从目前的视频演示来看，Sora 已经趋近完美，如果能解决人物一致性和场景一致性的问题，基本上就能达到影视公司想要的结果了”。</p><p>&nbsp;</p><p>此外，Sora 对“世界模型”的实现方式也存在一定争议。OpenAI 声称 Sora“扩展视频生成模型是构建物理世界通用模拟器的一条可行之路”。英伟达高级研究科学家 Jim Fan 也断言，Sora 是一个数据驱动的物理引擎，是一个可学习的模拟器，或“世界模型”。但也有人对此提出质疑。图灵奖得主 Yann LeCun 认为 Sora 并不理解物理世界，甚至称 Sora 对“世界模型”的实现方式注定是死路一条。</p><p>&nbsp;</p><p>具体来说，Sora 在生成视频时依赖于文本指令，这些文本描述了场景和意义。如果 Sora 能够理解视频内容，并在给定一段视频后，补充出更长的视频，且前后情节逻辑一致，那么这将是一个重要的进步。这将表明 Sora 不仅仅是通过视觉理解事物，而是能够从更深层次上理解视频内容。</p><p>&nbsp;</p><p>“长期来看，如果 Sora 能够在视频中实现首尾呼应，比如在电影中常见的前后呼应的情节，这表明它具有更长的因果链理解能力。这将是一个重要的里程碑，表明 Sora 越来越像是一个世界模型，能够理解物理定律和社会规则。”吴海波提到，目前，Sora 还处于一个比较早期的阶段，类似于早期的 GPT-3——它展现出了一定的能力，但尚未达到 ChatGPT 那样的成熟度。“但它的进步速度很快，2024 年值得我们期待，届时应该会有许多新的进展出现。”</p><p></p><h2>Sora如何重塑千行百业？</h2><p></p><p>&nbsp;</p><p>当前，Sora 还未正式对外开放。在近日的一场专访中，Sora 的核心团队成员表示 Sora 太过强大，还不能让普通人很快就用到，OpenAI 正在收集用户反馈，还有很多安全工作要做。而根据 OpenAI CTO Mira Murati 此前的说法，“Sora 最快在今年内开放公测”。</p><p>&nbsp;</p><p>作为一个基础模型，Sora 无疑会对各行各业产生影响，在影视、电商、游戏行业中，Sora 一定会带来新的想象力。其中，影视行业将会成为 Sora 的首选。</p><p>&nbsp;</p><p>目前，影视行业的制作流程涉及多个环节。编剧完成剧本后，会寻找合适的导演合作。在好莱坞或国内的大型制片厂，他们通常会先找普通演员拍摄样品，需要将 90 分钟的电影精华部分浓缩，拍摄成 30 至 40 分钟的样片，用于向投资人展示电影的内容、故事情节、人物设定以及特效应用等。只有当投资人认可了故事的创新点和市场潜力后，才会决定投资。不过，这类样片的制作成本相当高，每分钟的制作费用在 1 至 2 万元之间。</p><p>&nbsp;</p><p>如果引入 Sora 这类视频生成模型，将大幅降低制作成本，成本可能压缩至每分钟数千元。此外，Sora 还能免除影视制作的场景搭设、威亚特效、影视后期等工作，显著提高制作效率。</p><p>&nbsp;</p><p>“在与北京影视行业的合作中，我发现他们在拍摄电视剧和电影时，经常遇到一些无法通过常规手段拍摄的镜头，如宇宙大爆炸、地月轨道等场景，这些都需要依赖 3D 后期制作来完成。但这类镜头的制作成本极高。以电影行业常见的 25 帧/秒为例，一个 2 至 3 秒的镜头就包含约 70 帧的画面，按照帧计费的 3D 后期制作费用，这样短暂的镜头也需要投入上千甚至上万的成本。”黄鸿波介绍道，有了 Sora 技术后，影视公司可以将那些特效制作成本高昂或无法通过演员实际拍摄的场景，通过 Sora 或类似的视频生成模型来展现。“Sora 不仅对影视行业有益，它对传统广告制作、游戏和流媒体方面也有一定的影响，一些画面镜头的拍摄通过 AI 在几分钟内就能完成，节省了大量的人力物力。”</p><p>&nbsp;</p><p>在电商行业中，Sora 这类视频生成模型也带来了新的想象力。</p><p>&nbsp;</p><p>传统的产品视频拍摄需要模特、场景布置、拍摄以及后期制作等多个环节，而 Sora 只需输入相应的文本描述或图片，即可在短时间内生成逼真的视频，极大地提高了电商营销素材制作效率。此外，商家可以利用 Sora 生成产品在不同场景下的视频，或者展示产品在不同空间布局下的效果，从而提升消费者的购买意愿。</p><p>&nbsp;</p><p>虽然 Sora 在视频生成方面取得了显著进步，但要想真正应用在电商行业中，仍面临一些挑战。“目前，用户可以通过提交指令给Sora，Sora会在一段时间后生成视频反馈给用户。这种交互方式虽然令人兴奋，但也存在局限性，因为它缺乏明确的控制和交互方式。”吴海波提到，以电商为例，商家可能更希望基于某个已有商品生成视频内容，在将实体商品与视频结合方面，Sora 目前还无法满足需求。Sora 无法将商家的商品巧妙地融入视频中，并展示商品在真实场景中的应用，让潜在顾客直观地了解商品。</p><p>&nbsp;</p><p>“尽管 Sora 已经展示了在自由发挥状态下的创造力，但我们还不清楚如何将这些技术与现有电商平台有效结合，如何让它按照我们的需求生成内容，还有待进步一的优化。”吴海波表示，要想在电商行业中进一步拓展 Sora 技术的应用范围，还需要不断研究并探索新的方法，以实现商品与视频的完美结合。</p><p>&nbsp;</p><p>游戏作为较早落地 AIGC 技术的行业之一，在制作过程中也可引入 Sora 这类视频生成模型。黄鸿波提到，目前游戏行业比较容易落地的是大场景、风格转换和季节转换类型。</p><p>&nbsp;</p><p>比如，可以借助 Sora 技术，实现游戏中的季节转换等场景，通过每个季节 2-3 秒的场景交替生成游戏内的视频，这样不仅可以提升游戏的视觉体验，还能有效减少游戏的制作开发成本。游戏内的服装道具也可以通过 Sora 来完成。而对于游戏人物的动作，如跑步和飞翔，传统的制作方法通常涉及到底模建模、骨骼绑定以及动作合成。现在这些工作也可以通过 AI 技术来完成，在最后由人工进行必要的补充和调整，以确保动作的真实性更加出色。</p><p>&nbsp;</p><p>此外，光影和材料的仿真也是游戏制作中的重要环节，这些同样可以通过 AI 技术实现。例如，当角色从两米高的地方跳下时，不同材质的服装（如丝绸、粗布、盔甲）会产生不同的漂浮效果、落地速度和声音，这些细节在游戏和电影制作中都有着专门的处理流程。</p><p>&nbsp;</p><p>四足动物的动作设计是游戏行业的痛点之一。人类的走路和跑步动作相对自然，但四足动物的动作往往难以协调。而这类问题正是 Sora 这类技术可以发挥优势的地方。特别是像猫狗等常见的动物，由于不涉及复杂的 IP 和版权问题，更适合作为实践案例来解决动作设计上的挑战。</p><p>&nbsp;</p><p>不过，相较视频生成模型，图片生成模型在技术上已经更为成熟，这使得其在多个行业中的应用更加广泛和深入。</p><p>&nbsp;</p><p>“目前在游戏行业中应用最多的还是文生图模型。一般拥有自己 IP 的企业都会利用已有的形象素材，训练自家的文生图模型，生成视频或相关角色的形象参考”。据黄鸿波介绍，所有的文生图、文生视频、角色设计生成、形象设计生成，都无法直接采用生成产物，只是给设计人员一些灵感和启发，让他们以此为参考进行设计和开发。以一个海岛家园类的游戏为例，可以让 Stable Diffusion 等工具生成大量的海岛、家园、游戏风格设计图，给美术的同学一些启发，这也是目前企业内多数的落地形式。</p><p>&nbsp;</p><p>在电商行业中，图片生成模型也已得到广泛应用。吴海波提到，相较于视频生成技术，图片生成技术已经发展得更为成熟，因此在这一领域的应用也更为迅速。去年，核心团队来自蘑菇街的 AI 商拍工具 WeShop 上线，WeShop 正是基于 Stable Diffusion 模型提供 AI 智能商品图生成服务。目前，WeShop 主要服务于两类用户：一类是供应链为主的工厂老板，他们可以利用 WeShop AI 将商品图片转换成不同模特和背景的图片；另一类是计划拓展海外市场的电商，他们可以通过 WeShop AI 将国内商品图片适配到适合海外市场的模特场景中。</p><p>&nbsp;</p><p>“展望图片生成技术的未来，我认为 Sora 的成功表明模型规模的重要性，我们预期图片领域的基础模型也将取得显著进步。业界的技术路线和思路正趋于一致，大家都认识到需要引入 DiT 结构。尽管目前还有一条尝试纯 Transformer 基础架构的路线，类似 于GPT，但尚未超越现有技术。然而，随着 Sora 证明了大模型的有效性，我们可以预见将有更多资源投入到图片生成领域，推动其向前发展。这一点或许尚未得到广泛关注，但我坚信图片生成技术很快将迎来重大突破。”吴海波总结道。</p><p></p><h2>担心被 Sora 们取代？</h2><p></p><p>&nbsp;</p><p>Sora 给不同行业带来巨大变革可能得同时，也给就业市场带来了挑战，越来越多的从业者开始担心，自己终将被 Sora 们所取代。首当其冲的是影视行业从业者，不少声音开始讨论“特效公司要死了吗”“导演、后期是不是都要失业了”。</p><p>&nbsp;</p><p>对此，受访专家们普遍持乐观态度。以 CG 技术的出现为例，当年 CG 技术崭露头角时，许多动画师曾担忧自己的工作可能会受到威胁。然而，事实并非如此。实际上，CG 技术并未降低制作电影或动画的成本，反而使得成本有所上升。与此同时，CG 技术让人们能够创作出更高质量、更具视觉震撼力的作品，这反而激发了画师和导演的创造力，使他们能够制作出更为精彩的内容，也进一步提升了整个行业的标准。</p><p>&nbsp;</p><p>吴海波认为，面对 CG 技术这样的革新，我们应该积极拥抱变化，从中寻找新的机遇，而不是一味地担忧和抵触。如果我们固执地坚持旧有的工作方式而不愿适应，那么确实可能会面临问题。但与此同时，新技术也为我们打开了更广阔的市场，提升了行业的上限，并为我们提供了更多尝试不同角度和方法的可能性。“如果你坚持认为自己被新技术替代了，这或许是一种无法避免的心态。然而，我认为，有些工作被新技术解放，实际上是一件好事。换个角度看，我们可以说自己是从原有的束缚中得到了解放，迎来了新的机遇和挑战。”</p><p>&nbsp;</p><p>Sora 同样如此。目前来看，Sora 仍只是一款工具，并不能完全取代某一职位或环节，而是帮助人们更好地提升工作效率。以影视拍摄流程为例，尽管有 ChatGPT 这样的大语言模型协助，剧本编写仍需编剧来把控故事情节和故事性。分镜镜头的策划也需要导演来完成，因为模型生成的成品往往缺乏灵魂，需要人类加入细节、个人的情感和灵魂。同样，演员也是不可或缺的角色，因为观众既有人注重故事情节，也有人喜欢看明星的表演，如果取代了明星，电影就失去了其独特的意义。</p><p>&nbsp;</p><p>那么，Sora 究竟带来了什么，又能取代什么呢？</p><p>&nbsp;</p><p>黄鸿波认为，Sora 确实能加速视频和电影的制作效率，降低生产成本，并有可能取代部分特效制作公司的流程。但需要注意的是，这并非完全的取代，而是借助 Sora 完成一个大致的 demo，为特效公司提供思路，并替代部分相对简单的特效制作。原本需要十天才能完成的工作，现在可能只需要三五天就能完成。必须明确的是，任何技术的诞生都只是一种工具，其存在的目的是为了服务于人类。因此，完全的取代并不存在，工具的作用更多的是降低成本、提升效率。</p><p></p><h2>写在最后：参与到 AI 变革中来</h2><p></p><p>&nbsp;</p><p>近两年，AI 技术的快速演变和不断创新的特性超乎了所有人的预期，一个又一个创新模型的发布让人们不断惊叹于 AI 的潜力和能力。时代之下，更应该保持对 AI 技术发展的关注，随时准备迎接新的突破和变化，通过不断学习和适应新技术，在 AI 技术的浪潮中找到自己的位置，参与到 AI 变革中来。</p><p>&nbsp;</p><p>“我们现在的目标是首先参与到这场变革中来，将自己转变为一个 AI Native 的公司。我们从蘑菇街独立出一个团队来开发 WeShop，就是希望以创业团队的心态来完成这个项目。如果我们仍然使用传统的业务模式和资源来应用 AI 技术，我们可能会错过未来真正的大机会。因此，我们保持创业团队的状态，摒弃过去的包袱，以便在 AI Native 的环境中创造出新物种，抓住未来的机会。”吴海波认为，当前 AI 技术在电商领域的变革性影响难以清晰描绘，但其一定会为整个行业带来深刻变革，这不仅仅局限于在现有电商平台上增加智能问答功能或 AI 拍照等改进，而是当 AI 技术普及到一定程度时，人们将会见证一个全新的电商生态系统的崛起。</p><p>&nbsp;</p><p>对于影视和游戏行业，AI 带来的变革同样在发生，但目前都还缺少一个完整可落地的方案——一个能将文生图、文生视频等单一化工具串联起来的综合性工具。</p><p>&nbsp;</p><p>黄鸿波认为，理想的情况是，只需要手稿和文字描述，就能直接流程化生成包括 2D 图像、3D 模型、立绘、骨骼绑定以及动作生成等在内的完整一套内容。对于游戏行业而言，这样的综合性工具能够极大地提升开发效率。通过输入文字描述和手稿，工具能够自动处理生成游戏所需的各种资源，从而大大减轻开发者的负担。同样，影视行业也迫切需要这样的解决方案。只需要提供脚本，工具便能直接分析出完整的故事情节梗概，并基于这一情节生成围绕其展开的视频内容。这样不仅能确保画面风格的统一性和一致性，还能提高影视制作的效率和质量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WDp1hR6JQ7WzaHuVv3cP</id>
            <title>最新大模型推理优化进展：英伟达、阿里、腾讯和零一万物专家观点解读｜AICon</title>
            <link>https://www.infoq.cn/article/WDp1hR6JQ7WzaHuVv3cP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WDp1hR6JQ7WzaHuVv3cP</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 11:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型模型, 经济增长, 训练挑战, 推理优化
<br>
<br>
总结: 大型模型的出现为新的经济增长注入了新的动力，但在训练和推理方面，它们也面临诸多挑战。为了解决这些挑战，业界专家们在AICon全球人工智能开发与应用大会上特别设置了“大型模型推理优化”专题，分享了关于大型模型的优化技术和经验。通过这些分享，观众可以深入了解大型模型的挑战和优化方法，为大型模型的应用和发展提供更多思考和借鉴。 </div>
                        <hr>
                    
                    <p>大型模型的出现为新的经济增长注入了新的动力，但在训练和推理方面，它们也面临诸多挑战。这些挑战包括计算资源的巨大需求、并行化限制、模型体积和训练难度、数据质量、能耗和推理速度、算力不足、数据处理难题、思维模式转变以及高昂的成本。</p><p></p><p>为了向业界提供更多思考和借鉴的机会，我们在 AICon 全球人工智能开发与应用大会上，特别设置了“大型模型推理优化”专题。这一专题由阿里巴巴的研究员林伟老师担任出品人，旨在为观众带来更严谨、更有启发的演讲。我们邀请了四位老师进行分享，他们的精彩演讲将为大家带来深刻的思考和丰富的收获。</p><p></p><h4>BladeLLM 大模型高性能部署框架</h4><p></p><p></p><p>我们很荣幸地邀请到阿里云的高级算法专家李深作为首个分享的嘉宾。作为阿里云人工智能平台 PAI 模型系统优化的 Tech Leader，他在模型压缩和推理优化等方面拥有超过 10 年的丰富经验。在本次大模型推理优化专题演讲中，李深将重点介绍阿里云的 BladeLLM 大模型高性能部署框架。BladeLLM 高性能部署框架是基于阿里云人工智能平台 PAI 的技术积累和实践经验构建的。该框架不仅应对了大模型在线服务部署中的场景特性、资源规模和性能指标等更高更复杂的要求，而且兼容了大模型主流生态，提供了灵活易用的接口。</p><p></p><p>在演讲中，李深将深入探讨大模型服务部署优化面临的主要挑战，以及 BladeLLM 架构与核心优化技术。这些技术包括高性能算子与 AI 编译优化、模型压缩与算法优化、长上下文优化等，将为听众呈现出多层次联合的极致性能优化方案。通过他的分享，听众将了解大模型服务部署中的主要瓶颈与技术挑战，探索大模型部署优化的主要技术手段，并且深入了解大模型在线服务的规模化生产部署的实践经验。</p><p></p><h4>当大模型推理遇到算力瓶颈，如何进行工程优化？</h4><p></p><p></p><p>本专题出席的第二位嘉宾是零一万物的资深算法专家李谋。他曾历任阿里达摩院和华为云 EI 服务产品部技术专家，目前担任零一万物大模型在线推理服务负责人。在本次专题演讲中，他将探讨当大模型推理遇到算力瓶颈时，如何进行工程优化。随着大语言模型的持续发展，其参数量和序列长度呈指数级增长，因此面临的算力挑战愈发严峻。他将结合大模型的算力需求和模型结构，详细介绍零一万物在构建 Yi 模型在线推理服务过程中所采用的优化技术手段。通过他的分享，听众将了解到大模型推理算力瓶颈及主要工程优化手段，以及大模型应用场景的未来发展趋势。</p><p></p><h4>TensorRT-LLM: Past, Present and Future</h4><p></p><p></p><p>我们很荣幸地邀请到英伟达的高级技术总监杨军作为我们的专题演讲嘉宾。作为英伟达 AI 计算架构部门的负责人，他主要关注于 AI 系统全栈优化技术。在本次大模型推理优化专题演讲中，他将分享关于 TensorRT-LLM 的主题：“TensorRT-LLM: 过去、现在与未来”。</p><p></p><p>TensorRT-LLM 项目源起于对大语言模型推理优化的迫切需求。在演进迭代过程中，团队不断进行设计思考，探索最佳方案以满足日益增长的需求。当前设计方案的核心原则将是他演讲的重点，将会深入探讨该方案背后的理念和技术实现。此外，杨军还将简要介绍 TensorRT-LLM 的未来规划，展望该项目在大模型推理优化领域的发展方向和趋势。通过他的分享，听众将获得对 TensorRT-LLM 项目的深入了解，探索其在过去、现在和未来的演进路径和价值。</p><p></p><h4>太极 Angel 助力生成式大模型高效落地</h4><p></p><p></p><p>我们邀请的第四位演讲的嘉宾是腾讯高级工程师刘凯。作为腾讯混元大模型推理方向负责人，他在大模型压缩优化及推理加速领域拥有丰富经验，曾带领团队完成了大模型压缩 &amp; 推理框架的从零到一的构建。在本次大模型推理优化专题演讲中，刘凯将分享关于“太极 Angel 助力生成式大模型高效落地”的主题。</p><p></p><p>随着生成式 AI 技术的迅速发展，模型规模不断增大，结构也从 Dense 向 MoE 进化。在这一背景下，大模型应用的性能、吞吐、成本成为关注焦点。他将介绍腾讯太极机器学习平台所研发的 Angel-HCF 推理框架和 Angel-SNIP 压缩框架，以支持混元文生文、文生图、文生视频、多模态等 AI 生成领域的优化，助力腾讯混元大模型在公司内全面铺开应用。</p><p></p><p>刘凯将深入探讨生成式 AI 技术的挑战和常用优化方法，重点介绍太极 Angel-HCF 大模型推理框架和太极 Angel-SNIP 大模型压缩框架。通过他的分享，听众将了解生成式 AI 的技术难点和优化手段，大模型推理加速的技术细节，以及大模型压缩的技术方法和后续发展。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/63/63f7b4fc8288624587ab5be6059fad48" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a54Zpxv6yA55ZgHmWFt0</id>
            <title>大模型助力具身智能、电池研发与蛋白质研究，讯飞、深势科技、字节专家齐聚分享｜AICon</title>
            <link>https://www.infoq.cn/article/a54Zpxv6yA55ZgHmWFt0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a54Zpxv6yA55ZgHmWFt0</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 08:44:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型会议, 具身智能通用机器人, AI赋能电池研发, 生成式AI蛋白质科学研究
<br>
<br>
总结: AICon 全球人工智能开发与应用大会 暨 大模型应用生态展·2024 是由极客邦科技旗下 InfoQ 中国主办的技术盛会，主要面向工程师、产品经理、数据分析师的大模型会议，会议聚焦大模型训练与推理、AI agent、RAG、多模态大模型等热门方向。在专题论坛中，探索了大模型在具身智能通用机器人领域的创新探索、AI赋能电池研发、生成式AI如何助力蛋白质科学研究等议题，展示了人工智能在不同领域的前沿应用和潜力。 </div>
                        <hr>
                    
                    <p>AICon 全球人工智能开发与应用大会 暨 大模型应用生态展·2024 是由极客邦科技旗下 InfoQ 中国主办的技术盛会，主要面向工程师、产品经理、数据分析师的大模型会议，会议聚焦大模型训练与推理、AI agent、RAG、多模态大模型等热门方向。</p><p></p><p>在 AICon 的议题设置中，80% 都着眼于开发与应用，但在我们的专题论坛中，我们想要为大家带来更加开阔的视野，探索 AI 领域的未来前沿。因此，我们特别策划了【AI 前沿探索】的话题。我们邀请了科大讯飞 AI 研究院副院长、科研部部长李鑫博士来担任出品人，以确保我们选择的议题具有更高的质量。在经过认真评估后，我们为听众选择了三个精彩的议题。</p><p></p><h5>大模型在具身智能通用机器人领域的创新探索</h5><p></p><p></p><p>首先，我们很荣幸邀请到季超，他是科大讯飞的人形机器人首席科学家。季超长期从事机器人与智能装备硬件关键技术及产品开发，尤其涉及人机交互、具身智能和机器人强化学习运动控制等前沿方向。</p><p></p><p>在他的演讲中，他将重点探讨大模型在具身智能通用机器人领域的创新探索。这涉及到人形机器人集成人工智能、高端制造、新材料等先进技术，为未来产业发展带来颠覆性的机遇。他将深入分析智能机器人行业发展趋势，揭示产业现状和痛点，以及在大模型底层能力突破的基础上，探讨具身智能通用机器人的关键技术及系统集成。此外，他还将分享讯飞在这一领域的成果和进展，以及面向 AGI+Robot 行业生态构建的倡议。通过他的分享，您可以深入了解大模型浪潮下具身智能机器人的重大机会，探索企业在这场浪潮中的角色和贡献。</p><p></p><h5>AI for Science 新范式赋能电池研发</h5><p></p><p></p><p>继而，我们荣幸邀请到深势科技的高级材料研发总监王晓旭博士，他拥有深厚背景融合了广泛产学研合作与项目实施的宝贵经验。他将呈献一场题为“AI 赋能科学新范式：重塑电池研发领域”的专题演讲，深度剖析新能源产业电池创新面临的复杂挑战，并揭示人工智能在科研领域的最新应用趋势。</p><p></p><p>演讲核心，王晓旭博士将详述新能源领域的现状与未来趋势，强调电池材料研发在推动该行业跨越性发展中的核心地位。他将着重阐述“AI for Science”这一新兴科研模式如何为新能源科学研究，尤其是电池技术的演进，注入变革力量。此部分讨论将深刻聚焦于 AI 技术在加速电池仿真技术创新及大模型在精准预测电池性能、优化电池设计方面的革命性贡献。</p><p></p><p>通过他的分享，听众将不仅洞悉“AI for Science”新科研范式背后的思考，还能深入了解 AI 技术在电池材料科学中的实战应用案例，探索这些前沿技术如何通过算法的迭代与优化，从而在解决实际问题中彰显其巨大潜力与价值。</p><p></p><h5>生成式 AI 如何助力蛋白质科学研究</h5><p></p><p></p><p>最后，我们邀请到了字节跳动 ByteDance Research 的高级研究员郑在翔老师，拥有丰富的学术背景和研究成果。郑在翔将带来题为“生成式 AI 如何助力蛋白质科学研究”的演讲。他将深入探讨蛋白质科学在现实世界中的重要性以及人工智能在这一领域中的前沿应用。</p><p></p><p>在演讲中，郑在翔将介绍字节跳动在探索「大规模生成式人工智能赋能蛋白质科学研究」方面的最新研究进展。特别是，他将重点介绍如何利用大规模生成式人工智能技术，包括大语言模型（LLMs）与扩散概率模型（Diffusion Models），来构建和实现统一、通用且强大的「蛋白质基础模型」。</p><p></p><p>通过他的分享，听众将了解到生成式 AI 如何在蛋白质建模与设计领域发挥作用，以及如何利用海量蛋白质数据驱动最前沿的技术来解锁蛋白质科学的新可能性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f3176374aa3d4f506f44d28392b53d74.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d599256893d25db9981ffa3bd9505efc.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NEpZd52h3IHX8wzcFlaw</id>
            <title>神秘大模型一夜“征服”所有人，超GPT-4却无人认领？网友：OpenAI 要有大麻烦了</title>
            <link>https://www.infoq.cn/article/NEpZd52h3IHX8wzcFlaw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NEpZd52h3IHX8wzcFlaw</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 06:39:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: gpt2-chatbot, OpenAI, 模型, GPT-4.5
<br>
<br>
总结: 一款神秘的模型 gpt2-chatbot 在没有官方介绍的情况下引起了巨大关注，被认为可能是 OpenAI 的下一个模型或者是 GPT-4.5 或 5 的 beta 测试。尽管没有明确身份，但其表现出色，效果领先多个模型，在各领域展现出优秀的能力。对于其神秘来源和真实身份，仍存在各种猜测和推测。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p></p><p>昨晚，一个突然出现在 LMSys 基准测试网站的神秘模型，在大模型领域掀起了巨大波澜。用过的人都在夸，刚发布服务器就被挤爆，奥特曼也迅速出现在吃瓜现场...</p><p></p><p>而这一切的主角，就是 gpt2-chatbot。没有出处，也没有介绍，在没有官方文件的情况下，一夜间“惊艳”所有人的视线。</p><p></p><p>有人认为 gpt2-chatbot 可能是 OpenAI 的下一个模型，还有人说它是 GPT-4.5 或 5 的 beta 测试，甚至有评价称这可能是对 GPT 架构的根本升级。尽管该模型的系统提示表明它来自 OpenAI，但 gpt2-chatbot 却拒绝引用 OpenAI。</p><p></p><p>有意思的是，在众人猜测 gpt2-chatbot 身份的时刻，Open AI 的 CEO Sam Altma 发帖表达了对 gpt2-chatbot 的喜爱：“我确实对它情有独钟。”</p><p><img src="https://static001.geekbang.org/infoq/3f/3f40b607b4d7747592789c9299b890eb.webp" /></p><p></p><p>对此，有网友评价说：“如果不是 ChatGPT 的新版本，OpenAI 就有麻烦了！” 也有网友表示，“希望它不是 GPT-5，这个模型很难完成 Opus 擅长的推理任务。”</p><p></p><p>以下是部分用户对 gpt2-chatbot 的测试重点总结：</p><p>gpt2-chatbot 一直声称“基于 GPT-4”并具有“v2”个性，并称自己为 ChatGPT。其呈现自己的方式，通常与其他在 OpenAI 数据集上训练的模型的幻觉回复不同。它似乎使用了 OpenAI 的 tiktoken 分词器，对 OpenAI 使用的特殊 token 有反应，且对 Claude/Llama/Gemini 使用的特殊 token 没有反应。当需要提供联系方式时，gpt2-chatbot 会始终如一地给出 OpenAI 的信息，甚至比 GPT-3.5/4 的更详细。它表现出特定于 OpenAI 的提示注入漏洞，且从未声称属于 OpenAI 以外的任何其他实体组织。对于相同的提示，gpt2-chatbot 始终提供与 Anthropic、Meta、Mistral、Google 等模型不同的输出。</p><p></p><p></p><h1>效果领先多个模型</h1><p></p><p></p><p>gpt2-chatbot 一经发布，众多用户都涌入这一模型测试其在各领域的表现。从公开平台的反馈来看，该模型在多方面的能力和实际效果都赶上甚至超过许多其他的前沿模型。</p><p></p><p>例如，gpt2-chatbot 可以在 PyOpenGL 中一次性生成旋转 3D 立方体， 而 GPT-4、Gemini-1.5 和 Claude-3 需要尝试三次才可以。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a39591deaf83c585bba17d3cebf23c2.webp" /></p><p></p><p>在解决兄弟姐妹之谜时，gpt2-chatbot 得出和 GPT-4 Turbo 相同的结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b9d92f04a907946e3793d232a6ab84c8.webp" /></p><p></p><p>除这两个案例外，还有许多网友抛出了 gpt2-chatbot 在解决各类问题时的优秀能力。</p><p>网友 @Andrew Gao：gpt2-chatbot 一口气正确解决了 IMO（数学奥林匹克）问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6bc094a465165cc1fd91f04d1d49ec45.webp" /></p><p></p><p>网友 @murat ：该模型可以解决一些 GPT-4 做不到的事情，如 A+B-1 的数学问题，打破了非常强的学习惯例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/6934696c0b86f0a02e0b96aedf42c5aa.webp" /></p><p></p><p>网友 @Phil：用 gpt2-chatbot 制作 ASCII 艺术的效果领先于任何其他模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f5b3931d78ea4fb34a69768535491ef.webp" /></p><p></p><p>网友 @murat ：gpt2-chatbot 第一次尝试就解决了在 Claude Opus 、GPT4 和 llama3-70b 模型上失败的 TypeScript 编写问题，并且没有错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f30ed7ccdac5a75e729a30a4599c1c8.webp" /></p><p></p><p>不仅在复杂的代码操作任务以及用于测试新模型的所有编码提示上，gpt2-chatbot 比 Claude Opus 以及最新的 GPT-4 更好。当被要求规划 LLM 代理的计划以帮助用户预订晚餐时，gpt2-chatbot 也能给出出色的响应。</p><p></p><p></p><h1>模型的神秘来源</h1><p></p><p></p><p></p><blockquote>“在我看来，这个神秘模型很可能是 GPT-4.5 或 GPT-5，或者实际上是一个真正的 GPT-2 模型，由 OpenAI 或 LMSYS 提供。总的来说，它输出的内容质量，特别是格式、结构和整体理解，绝对是一流的。对我来说，这感觉就像是从 GPT-3.5 到 GPT-4 的一步，但以 GPT-4 为起点。”关于 gpt2-chatbot 的公开网页介绍（非官方）</blockquote><p></p><p></p><p>当需要提供联系方式时，gpt2-chatbot 会始终如一地给出 OpenAI 的信息，甚至比 GPT-3.5/4 的更详细。而且，该模型使用 OpenAI 的 token 分词器，对 OpenAI 使用的特殊 token 有反应。</p><p>一种猜测认为，gpt2-chatbot 实际上是基于 GPT-2 架构的，其表现出的能力大大超出了任何以前已知的 GPT-2 模型。</p><p></p><p>另一种可能性是，它实际上是一个 GPT-2 模型。最近（2024 年 4 月 7 日）Meta/FAIR Labs 和 Mohamed bin Zayed AI University of AI （MBZUAI） 的一篇题为《语言模型物理学：第 3.3 部分，知识容量缩放定律》的文章深入研究了 GPT-2 架构的细节，并确定：“ GPT-2 架构在知识存储方面与 LLaMA/Mistral 架构相当甚至超过，尤其是在较短的训练持续时间内。”</p><p></p><p>至于该模型被认为是 GPT-4 的强烈说法，可以通过主要利用 GPT-4 生成的数据集来解释。然而，gpt2-chatbot 确实有一个与 GPT-4 模型不同的速率限制，用于直接聊天：</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/566db9cdb12bad9b39d958caf36b4210.webp" /></p><p></p><p>虽然尚未比较对总速率限制与用户特定速率限制的完整限制，但在每日用户限制以及其他一些总服务限制上比 GPT-4 模型更具限制性。这可能意味着，该模型在计算方面的成本更高，并且提供计算的人更喜欢用户使用 Arena （Battle） 模式来生成基准测试。</p><p></p><p>如果 LMSYS 是 gpt2-chatbot 的模型创建者，那么该文章的一些结果的应用就可以利用通过 LMSYS 生成的数据集进行训练等。</p><p></p><p>如果你想试用或者帮助解开 gpt2-chatbot 的身份谜题，现在可以进入到 LMSys 网站（https://chat.lmsys.org/）并选择 gpt2-chatbot。每个用户每天可以测试 8 条消息的直接聊天，之后可以切换到“竞技”模式尝试匹配到该模型选项。另外，尝试时至少需要对所有提示展开三次验证，以获得达到该模型平均能力的结果。</p><p></p><p>参考链接：</p><p>https://rentry.co/GPT2</p><p>https://twitter.com/itsandrewgao/status/1785013026636357942</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PyE2ijOvWDiaL1fImie9</id>
            <title>谷歌裁掉整个Python语言团队！PyTorch 创始人回应：“核心语言团队无可替换”</title>
            <link>https://www.infoq.cn/article/PyE2ijOvWDiaL1fImie9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PyE2ijOvWDiaL1fImie9</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 05:13:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 长期人手不足, Python语言团队解散, Thomas Wouters, 谷歌新Python团队
<br>
<br>
总结: 谰言称谷歌解雇Python团队，但Thomas Wouters证实了这一消息，谷歌正在慕尼黑组建新的Python团队，可能是为了成本重组。谷歌Python团队成员表示团队工作内容和重要性，谷歌多个项目都是用Python开发，谷歌仍在寻找具备Python技能的人才。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;</p><p></p><blockquote>我们长期人手不足，但是我20年来最好的工作。</blockquote><p></p><p>&nbsp;</p><p>谷歌 Python 工程师、Python 指导委员会成员Thomas Wouters昨天在社交媒体上发布了一条消息，称谷歌解散了Python语言团队。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc6072e7537b7e0d5b2e966709653732.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>其实上周就有消息称，为了GenAI，谷歌解雇了整个Python语言团队。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0ad1ff3732b42a279d7b72c047735fd8.jpeg" /></p><p></p><p>&nbsp;</p><p>但由于缺少发布者的背景信息，这条消息没有引起太多的注意。但Thomas Wouters的出现，显然证实了“谷歌解雇Python团队”不是谣言。</p><p>&nbsp;</p><p>Thomas Wouters现在是谷歌员工，是CPython 核心开发，在Python 指导委员会任职8年多，同时也是Python 3.12 和 3.13 的发布经理。</p><p>&nbsp;</p><p>根据Wouters的说法，谷歌正在慕尼黑从头开始组建一个新的 Python 团队。有网友解释说，美国团队已经被解雇，而Wouters（位于荷兰）则被要求跨国加入到慕尼黑新招的团队中，训练新的团队。</p><p>&nbsp;</p><p>也有其他网友补充道，这次裁员可以是出于成本原因进行的重组，有的团队被彻底解散，有的则将两个团队合并为一个。这是“一种有利于低成本地区人们的模式。例如，两个团队合并，成本较高的经理被解雇，或者整个团队被解雇，但这些职责正在由工资较低的办公室的人员重新安排。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/839378176d79ece37224502f5861d38d.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>在Hacker News上，谷歌 Python 团队内部员工很快进行了回应，表示可能慕尼黑团队会“重拾”他们大部分或全部的工作，“对整件事真的很难过。这是我 20 年职业生涯（包括谷歌其他团队）中迄今为止最好的工作。我们是一个长期人手不足的团队，为谷歌的Python生态系统的很大一部分提供支持，多年来我们做了一些令人惊叹的工作。”</p><p>&nbsp;</p><p>这个消息惊动了领域内的很多开发者，包括 PyTorch 创始人、Meta 杰出工程师 Soumith Chintala，“显然 Google 解雇了整个 Python 基础团队，WTF！”，但“我认为基础/核心语言工程师很难被替代或变得可替代。他们拥有关于复杂代码和社交动态（social dynamics）的深厚知识，这些知识很难被记录下来。这对其他公司来说是一个介入和抢人的机会 (Meta 会开始接触他们，但可能无法吸收所有工程师)。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a6707155144fa7f0843fa7dc8f0bf9fb.jpeg" /></p><p></p><p>&nbsp;</p><p>在AI时代解雇Python团队，很多人表示不能理解：“谷歌是一家AI优先的公司，谷歌的未来与其AI产品紧密相连。所有AI都是用 Python 编写的，谷歌却解雇了所有 Python 团队。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53d7bf97668458464a6e8ab1d831886f.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>谷歌不到十人的Python团队</h2><p></p><p>&nbsp;</p><p>根据网上谷歌 Python 团队成员爆料，目前该团队不到 10 人。团队的日常主要工作，除了为上游Python做出贡献之外，还包括：</p><p>&nbsp;</p><p>在谷歌维护一个稳定的 Python 版本，并确保 monorepo 中的所有内容都可以使用。在该员工任职期间，他们从 2.7 升级到 3.6，然后逐步升级到 3.11，每次更新都需要数月到一年多的时间，因为 Google 的规则是，如果您签入任何代码，您就要对其造成的每一次损坏负责；维护工具，使数千个第三方软件包不断从其开源版本更新，并为需要谷歌特定更改的软件包提供补丁队列；针对谷歌的风格指南和整体代码库，负责高度定制版本的工具，如 pylint 和 black；为 pybind11 做出贡献，并维护 C++ 集成工具；开发和维护 Python 的构建系统规则，包括付出巨大努力将 Python 规则转移到纯 Starlark 代码，而不是让它们纠缠在 blaze/bazel 核心引擎中；开发并维护了一个类型检查器（pytype），它可以在没有类型注释的情况下对代码进行推理，并使用一次一个文件的架构处理非常大的项目；对数亿行代码执行自动重构。</p><p>&nbsp;</p><p>该成员还表示，这只是团队工作的开发部分，他们还充当了谷歌的 Python “客服”，帮助解决棘手的问题，并为新人指明正确的方向。另外，Python团队还与许多其他团队合作，包括机器学习和 AI 团队、协作和 IDE 团队、protobuf 这样集成并生成 Python 绑定的团队、像 Google Cloud 这样希望向客户提供 Python 运行时的团队、就像 YouTube 这样有一个用 Python 构建的异常庞大系统的团队，他们需要保持它的性能和可维护性。</p><p>&nbsp;</p><p>正如该成员爆料的，由于Python的简单性和相对快速的维护，谷歌公司刚刚建立时就使用了Python，并且沿用至今：谷歌的搜索引擎、YouTube、机器学习、人工智能、机器人项目等都是用Python开发的。</p><p>&nbsp;</p><p>“Python 从一开始就是谷歌的重要组成部分，并且随着系统的发展和发展，这一点仍然如此。如今，数十名谷歌工程师使用 Python，我们正在寻找更多具备这种语言技能的人才。”谷歌计算机科学家兼研究总监 Peter Norvig 在2003年时说道。</p><p>&nbsp;</p><p>该公司的格言“Python 能用，C++ 必须用”，描述了它对这种多功能编程语言的依赖程度。</p><p>&nbsp;</p><p>早期，谷歌的创始人决定只要有可能就使用 Python，而在无法使用 Python 的地方只使用 C++。因此，当内存控制势在必行且需要低延迟时，就使用 C++。对于其他一切，Python 实现了易于维护和相对快速的交付。</p><p>&nbsp;</p><p>Python 的创建者 Guido Van Rossum 还在 2005 年加入谷歌团队并一直工作到 2012 年。</p><p>&nbsp;</p><p>是的，谷歌一直是 Python 编程语言的长期支持者和用户，除了 C++、Java 和 Go 之外，Python 是该公司的官方服务器端语言之一。Python 在许多谷歌内部系统上运行，并出现在许多 Google API 中，与 Google 的工程流程完美契合。</p><p>&nbsp;</p><p>Python 目前也是机器学习项目的开发人员最常使用的语言，包括谷歌著名的TensorFlow 框架就是以此为基础。</p><p>&nbsp;</p><p>还值得注意的是，谷歌与该语言本身和 Python 软件基金会有着密切的关系。</p><p>&nbsp;</p><p>谷歌从 2010 年起成为 PSF （Python软件基金会）赞助者，在 2021 年 2 月成为首个远景赞助者（赞助 35 万美元以其他资源）。资金主要用于提升 Python 生态的链供应安全，资源主要为Google Cloud的产品。另外，谷歌还参与赞助了Python的各类活动，比如 PyCon 和 EuroPython等。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>https://www.learnenough.com/blog/10-Companies-Using-Python-In-2023-&amp;-Why-It's-Their-Go-To</p><p>https://news.ycombinator.com/item?id=40183125</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uKouBwPIKIime1Vl0U45</id>
            <title>曝谷歌Python团队全员被裁；清华系团队“国产Sora”：视频突破16秒；“社恐”周鸿祎：喊话贾跃亭、雷军送自己车｜AI周报</title>
            <link>https://www.infoq.cn/article/uKouBwPIKIime1Vl0U45</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uKouBwPIKIime1Vl0U45</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 01:12:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, Python, 裁员, 团队
<br>
<br>
关键词: 杨植麟, 月之暗面, 套现, 消息不实
<br>
<br>
关键词: 拜登, TikTok, 法案, 出售计划
<br>
<br>
关键词: 特斯拉, 毁约, 应届生, HR
<br>
<br>
总结: 谷歌 Python 团队遭裁员，月之暗面否认套现传闻，拜登签署涉TikTok法案，特斯拉被曝毁约应届生。 </div>
                        <hr>
                    
                    <p></p><h2>热门资讯</h2><p></p><p></p><p></p><h4>谷歌 Python&nbsp;团队全员被裁</h4><p></p><p></p><p>谷歌 Python 工程师、CPython 核心开发者兼 Python 指导委员会成员 Thomas Wouters ，昨天晚上他在社交媒体发布动态称：包括自己在内的同事、主管均已被裁员。从他的描述来看，公司并没有直接解雇他们 —— 而是要求调岗到国外的团队。有其他网友补充道，是谷歌将 Python 团队负责的工作合并到另一个团队中，并让原来的团队离开。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8ae77b5959f7d65724dbddab99b0f633.png" /></p><p></p><p>“谷歌 Python 团队全员被裁” 消息很快就传遍社交媒体和开发者社区。有网友表示对谷歌 Python 团队的工作感到好奇，认为让一个团队专门研究一种编程语言是没有意义的。</p><p></p><p>对此，谷歌 Python 团队内部员工回应，除了为上游 Python 做出贡献之外，团队还要在 google 中维护了一个稳定的 python 版本，并确保 monorepo 中的所有内容都可以使用它、维护工具、针对谷歌风格指南和整体代码库高度定制工具，开发和维护 Python 的构建系统规则、对数亿行代码执行自动重构等。</p><p></p><p></p><h4>杨植麟套现数千万美元？月之暗面回应：消息不实</h4><p></p><p></p><p>据悉，上一轮融资完成后，月之暗面（Moonshot AI）创始人杨植麟通过售出个人持股已套现数千万美金。有知情人士表示，月之暗面最近这轮融资涉及一些老股交易，但红杉等老股东都没有出售股份。“有传言创始人及相关人员套现金额在 4000 万美金。”另有业内投资人表示，“公司成立第一年就套现这么多，这种情况并不多见。”</p><p></p><p>4 月 23 日，针对创始人杨植麟通过售出个人持股“套现数千万美元”的消息，月之暗面方面回应称，上述消息不实，月之暗面此前已公布员工激励计划。根据月之暗面官方公众号 3 月 11 日发布的信息，从 2024 年开始，该公司将在取得重要进展时发起员工期权回购计划，确保团队成员能够分享公司发展的果实（2024 年底启动首次期权回购计划）。此外，公司每年会定期根据工作表现进行调薪和期权增发，确保薪酬和期权充分反映出员工个人的成长和贡献。</p><p></p><p>“公司希望长期发展，不是短期套利路线。”月之暗面方面表示，不过对于发布股权激励计划与创始人是否套现之间更为直接的关系，公司方面暂未透露。</p><p></p><p></p><h4>拜登签署涉 TikTok 法案，字节：没有出售计划</h4><p></p><p></p><p>4 月 25 日，字节跳动方面表示，外媒有关字节跳动探索出售 TikTok 的消息不实，字节跳动没有任何出售 TikTok 的计划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/727463d296dcee1cb71eb582539c3ee4.png" /></p><p></p><p>当地时间 4 月 24 日，美国总统拜登签署了价值 950 亿美元的一揽子对外援助法案，该法案还涉及强制字节跳动剥离旗下应用 TikTok 美国业务。在相关条款中，字节跳动被限期约九个月剥离其美国业务，否则将面临美国全国禁令。此外，如果拜登确定出售事宜取得进展，他还可行使一次 90 天的延期权力。</p><p></p><p>与字节跳动关系密切的知情人士表示，TikTok 业务运营所依赖的算法被认为是字节跳动整体业务的核心，这使得字节跳动极不可能连带着算法将 TikTok 出售。TikTok 只占据字节跳动总营收和日活跃用户的一小部分，因此在最坏情况下，字节跳动宁愿在美国关闭这款应用，也不愿把它卖给潜在美国买家。而且，关闭 TikTok 对字节跳动业务的影响有限，该公司也不必放弃其核心算法。</p><p></p><p>据悉，今年目前为止，TikTok 及其母公司字节跳动总共花费了逾 700 万美元 (约合 5072 万元人民币) 资金，以试图阻止美国会通过可能在美国封禁 TikTok 的立法。最新公开的游说披露报告显示，今年头三个月，仅字节跳动就在 TikTok 的内部说客身上花费了创纪录的 268 万美元（约合 1941 万人民币），目的是游说国会和联邦官员。还有数据显示，2023 年字节跳动和 TikTok 的游说支出超过 800 万美元，另外 TikTok 今年在电视和数字广告活动上花费了 450 多万美元。</p><p></p><p>尽管拜登已经把 TikTok 剥离法案签署成为法律，要求字节跳动限期出售 TikTok 美国业务，但是这位美国总统将继续使用 TikTok 协助他的连任竞选活动。拜登竞选团队的一名官员表示，拜登要确保年轻选民在今年 11 月的大选前收到他的信息，TikTok 就是传播这些信息的方式之一。</p><p></p><p>据悉，拜登竞选团队在今年超级碗比赛举行当天，也就是 2 月 11 日发布了第一个 TikTok 视频，该视频的点击量超过 1000 万次。但是，在其竞选团队之后发布的 149 个视频中，不到一半的视频浏览量低于 10 万，只有 9 个视频的点击量超过了 100 万。</p><p></p><p></p><h4>特斯拉被曝毁约应届生，蔚来、极氪 HR“在线抢人”</h4><p></p><p></p><p>4 月 24 日消息，“特斯拉被曝毁约应届生”的话题登上微博热搜，有多名网友在社交平台上表示在拿到入职 offer 之后遭到了特斯拉单方面解约。对此，有被特斯拉解约的应届生表示，他是 4 月 24 日上午接到了 HR 的电话，被告知因公司政策临时调整，岗位被取消，将赔偿一个月的工资，但是春招基本结束，找工作不太好找。</p><p></p><p>另有拿到销售岗位 offer 的应届生表示，原定 4 月中旬的入职培训，入职办理均被告知取消，HR 在电话中称如果后续有需要，可以重新面试。针对毁约一事，特斯拉 HR 回应称，临时收到招聘需求调整和变化的通知，需要进行人员优化。</p><p></p><p>据悉在相关话题的评论区中，已有不少如蔚来、极氪等企业 HR 招聘账号表示，欢迎被解约的应届生投递简历。</p><p></p><p></p><h4>“忙碌”的周鸿祎：喊话贾跃亭、雷军给自己送车</h4><p></p><p></p><p>4 月 24 日，贾跃亭发布视频回应周鸿祎称为盗窃者鼓噪不应该是你的价值观，向年轻奋斗者倡导的不应该是抄袭和山寨。FF 虽然只交付 11 台车，但是是完全原创创新的塔尖产品，有非凡的颠覆性意义。贾跃亭称会在合适的时机，把 FF91 带回中国制造。随后，周鸿祎再次回应称，贾总别光讲 ppt，先送辆车到 360 大厦小广场让我试试。</p><p></p><p>此前周鸿祎就表示，“我不是为贾会计（指贾跃亭）说话，他带着图纸到美国那么长时间也没造出几辆车，高合汽车反而造出来了，他却反过来告人家。我觉得他犯了一个错误，（特斯拉 CEO）马斯克造车都要来中国设厂，他却去美国造车，完全没有搞明白中国的优势在哪里。”</p><p></p><p>此外，周鸿祎发文称自己借机向雷军求“送一辆小米 SU7”体验一下被雷军拒绝后，大量网友开始玩梗喊话雷军送一台给周鸿祎，但交车地点必须是朝阳公园东 5 门。</p><p></p><p>据悉，该梗出自 2012 年小米手机一举成名后，周鸿祎开始进军手机行业并在与小米在微博上发生了一场骂战。雷军将周鸿祎比喻成“东方不败”，指责周鸿祎是炒作，并且用抄袭的办法做智能手机。360 方面则反驳说，小米手机的项目，也是雷军从魅族“偷”来的思路。</p><p></p><p>彼时，周鸿祎一度约架雷军称，约你见面谈一谈，下周一上午十点朝阳公园门口见。在小米首部授权传记《一往无前》中提及了这次约架，其中提到，雷军不但亲自在微博上迎战，还对约架事件进行了认真部署。他让黎万强带着市场部的刘飞、钟雨飞等几位同事到朝阳公园东门踩了点，认真探究了站位和撤离路线。在雷军心中，他真的准备大干一场。不过最终这次约架不了了之。对此，有不少网友表示，雷总真的是从来不打无准备的仗。</p><p></p><p>这周，360 集团创始人周鸿祎很忙：参加完北京国际车展、中关村论坛“投资北京”大会做演讲。虽然不会开车，也没有造车，但周鸿祎最近却成为汽车圈的话题人物。在北京国际车展，他更是爬上一辆越野车顶，成为了全场焦点。有网友评价称，“周鸿祎成为了北京车展历史上最老的车模”。他在微博上表示自己“有点社恐”，为证明自己是真人，所以爬上车顶。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8276028740cd3afdacf10f5bb0365f88.png" /></p><p></p><p></p><h4>百度突然宣布：百度百科 App 将关闭服务</h4><p></p><p></p><p>4 月 23 日，根据百度百科 App 下线通知，百度百科团队决定于 2024 年 6 月 30 日关闭百度百科 App 的服务，将在百度 App 中的百度百科小程序继续提供服务。</p><p></p><p>公开资料显示，百度百科是百度公司推出的一部内容开放、自由的网络百科全书。内容涵盖了包括经济、文化、历史事件、政治人物、科学技术等在内的几乎所有知识领域，是一个重要的信息获取和知识共享平台。百度百科自 2006 年 4 月 20 日上线以来，已经发展成国内最大的中文维基百科。数据显示，百度百科已收录了超 2800 万个词条，参与词条编辑的用户超过 780 万人，几乎涵盖了所有已知的知识领域，日均访问量高达 4 亿次。</p><p></p><p>作为百度旗下一知名应用功能，App 突然宣布下线，确实会令人感到意外。值得一提的是，在百度百科 App 宣布下架的前一天，APP 还进行了一次更新，主要提示修复了一些 bug。</p><p></p><p></p><h4>美国全面取缔竞业协议</h4><p></p><p></p><p>当地时间 4 月 23 日，美国联邦贸易委员会（FTC）宣布，全面禁止所有员工（包括高级管理人员）签署新的竞业禁止协议。对于现有的竞业协议，高级管理人员的现有竞业协议仍然有效，其他员工则在规定生效日期后不再强制执行。这一规则将在公布 120 天后生效，对于“高级管理人员”的定义则是收入超过 151164 美元且处于“政策制定职位”的人。</p><p></p><p>根据 FTC 的估计，禁止竞业协议将带来以下影响：1、新企业成立率提高 2.7%，每年新增 8500 家新企业；2、每年平均增加 17000-29000 项专利，十年间每年专利增速为 11-19%；3、未来十年员工的平均年收入将额外增加 524 美元（约合 3796 元人民币）。</p><p></p><p>有科技行业从业者表示：“对于任何在美国从事科技工作的人来说，这都是一个重大消息。”</p><p></p><p>SteveDouglas 的 CEO Matt Shore（马特·肖尔）表示，这项禁令通过后，领导层已经需要避免员工“因为工作体验不佳而出现大规模外流”。Shore 认为，如果顶尖人才因竞业协议松绑而选择离去，那么企业必然需要拿出更有竞争力的薪酬方案来加以挽留。</p><p></p><p>延伸阅读：<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651203552&amp;idx=1&amp;sn=ffda7f1c1d208cca028b698fa4be45ac&amp;scene=21#wechat_redirect">全世界 IT 人苦竞业久矣！今天，美国全面废除竞业协议</a>"</p><p></p><p></p><h4>清华大学成立人工智能学院，首任院长系图灵奖获得者姚期智院士</h4><p></p><p></p><p>清华大学成立人工智能学院，以发展人工智能核心基础理论和架构，以及“人工智能 + X”为重点。该学院旨在培养 AI 顶尖人才，推动原始创新，由图灵奖得主、中国科学院院士姚期智担任首任院长。学院将依托国家战略，创新人才培养模式，吸引世界顶尖人才，突破关键核心技术，打造合作平台，推进产学研用协同创新，努力成为中国人工智能发展的引领者，建设成为世界顶尖的人工智能人才和创新高地。</p><p></p><p></p><h4>英伟达收购两家 AI 创企，要让 AI 芯片更便宜</h4><p></p><p></p><p>4 月 25 日，英伟达宣布达成一项最终协议，收购 Run:ai。据 CTech 报道，这笔交易价预计约为 7 亿美元。同时，英伟达还收购了另一家以色列 AI 公司 Deci。其中，Run:ai 约有 150 名员工，累计融资 1.18 亿美元；Deci 拥有约 100 名员工，累计融资 5500 万美元。英伟达与 Deci 的交易未公开披露，交易价未知。</p><p></p><p>据外媒报道，被英伟达最近达成收购交易的这两家创企，旨在降低开发或运行生成式 AI 模型的成本，帮助客户更有效地利用 AI 计算资源，进而提振英伟达 AI 芯片的需求。</p><p></p><p></p><h4>马斯克 AI 公司接近达成 60 亿美元融资，特斯拉市值一夜大涨 4028 亿元</h4><p></p><p></p><p>有市场消息称，马斯克可能即将获得数十亿美元的资金，以将其聊天机器人 Grok 打造成为 ChatGPT 的劲敌。两位知情人士透露，马斯克麾下初创公司 xAI 正在融资 60 亿美元，在不包括这笔投资的情况下，对该公司的估值为 180 亿美元。其中一名知情人士表示，红杉资本是参与本轮 xAI 融资的投资者之一，另一名知情人士表示，预计融资将在未来两周内完成。在人工智能赛道上，这轮融资是规模最大的单笔融资之一。</p><p></p><p>北京时间 4 月 25 日凌晨，特斯拉大涨超 12%，创 2022 年 1 月以来最大单日涨幅，市值大涨 556 亿美元（约合人民币 4028 亿元），总市值重返 5000 亿美元上方。</p><p></p><p>虽然特斯拉 23 日公布的财报未达预期，但表示已经更新未来的车型阵容，将新车型的推出时间提前，原先对外宣布的时间是 2025 年下半年开始生产。</p><p></p><p></p><h2>IT 业界</h2><p></p><p></p><p></p><h4>阿里巴巴发布了首个千亿参数的大模型 Qwen1.5-110B</h4><p></p><p></p><p>阿里巴巴发布了首个千亿参数的大模型 Qwen1.5-110B。此前它发布了 0.5B、1.8B、4B、7B、14B 和 72B 不同规模参数的版本。阿里巴巴称，Qwen1.5-110B 模型在基础能力评估中与 Meta-Llama3-70B 相媲美，在 Chat 评估中表现出色，包括 MT-Bench 和 AlpacaEval 2.0。Qwen1.5-110B 与其他 Qwen1.5 模型相似，采用了相同的 Transformer 解码器架构。它包含了分组查询注意力（GQA），在模型推理时更加高效。该模型支持 32K tokens 的上下文长度，同时它仍然是多语言的，支持英、中、法、西、德、俄、日、韩、越、阿等多种语言。</p><p></p><p></p><h4>最强国产 Sora，清华团队生数科技突破 16 秒长视频生成</h4><p></p><p></p><p>4 月 27 日，在中关村论坛未来人工智能先锋论坛上，生数科技联合清华大学正式发布中国首个长时长、高一致性、高动态性视频大模型——Vidu。该模型采用团队原创的 Diffusion 与 Transformer 融合的架构 U-ViT，支持一键生成长达 16 秒、分辨率高达 1080P 的高清视频内容。Vidu 不仅能够模拟真实物理世界，还拥有丰富想象力，具备多镜头生成、时空一致性高等特点。Vidu 是自 Sora 发布之后全球率先取得重大突破的视频大模型，性能全面对标 Sora，并在加速迭代提升中。</p><p></p><p>生数科技是一支清华背景的大模型创业团队，致力于专注于视频生成、3D 生成、图像生成等多模态领域。据悉，Vidu 的快速突破源自于团队在贝叶斯机器学习和多模态大模型的长期积累和多项原创性成果。其核心技术 U-ViT 架构由团队于 2022 年 9 月提出，早于 Sora 采用的 DiT 架构，是全球首个 Diffusion 与 Transformer 融合的架构，完全由团队自主研发。</p><p></p><p>清华人工智能研究院、清华人工智能研究院副院长朱军博士对媒体表示：Vidu 的视频时长会继续突破，“另外，我们的架构是支持多模态的，视频模态只是当前阶段最重要的。”据生数透露，Vidu 目前正在加速迭代提升，面向未来，Vidu 灵活的模型架构也将能够兼容更广泛的多模态能力。言下之意，还说生数科技是“中国 sora”，就有点太没想象力了。</p><p></p><p></p><h4>Meta 开放 Quest 头显操作系统</h4><p></p><p></p><p>Meta 为应对苹果 Vision Pro 带来的竞争压力，决定向其他硬件制造商开放自家的 VR 操作系统，并发布了 Meta Horizon OS，同时欢迎华硕、联想、Xbox 等知名科技品牌加入，共同构建多元化的 VR 硬件生态。此外，Meta 还对应用生态系统进行改造，以吸引更多用户投入 Meta 的 VR 世界。</p><p></p><p></p><h4>苹果发布 OpenELM，基于开源训练和推理框架的高效语言模型</h4><p></p><p></p><p>4 月 24 日消息，在 WWDC24 之前，苹果在 Hugging Face 平台上发布了一个“具有开源训练和推理框架的高效语言模型”，名为 OpenELM，其源码及预训练的模型权重和训练配方可在苹果 Github 库中获取。</p><p></p><p>根据官方介绍，大型语言模型的可重复性和透明性对于推进开放研究、确保结果的可信度以及调查数据和模型偏差以及潜在风险至关重要。OpenELM 使用分层缩放策略，可以有效地分配 Transformer 模型每一层的参数，从而提高准确率。例如，在参数量约为 10 亿的情况下，OpenELM 与 OLMo 相比准确率提升了 2.36%，同时所需的预训练 tokens 数量仅有原来的 50%。</p><p></p><p></p><h4>微软发布小模型：与 GPT-3.5 能力不相上下</h4><p></p><p></p><p>4 月 23 日消息，微软发布了一种具有成本效益的小型语言 AI 模型，可以创建社交媒体帖子等任务，同时使用较少的数据量。微软在一份声明中称，该 AI 模型被称为“Phi-3-mini”，在评估语言、编码和数学能力等一系列基准测试中，其表现甚至可以超越那些体积相当于其两倍的 AI 模型。</p><p></p><p>微软 Azure AI 平台企业副总裁 Eric Boyd 称，Phi-3-mini 的能力与 GPT-3.5 这样的大语言模型不相上下，只是体积更小。与大型 AI 模型相比，小型 AI 模型通常运行成本更低，在手机和笔记本电脑等个人设备上表现更好。</p><p></p><p>微软称，这种小型 AI 模型旨在执行一些更简单的任务，使其更容易被资源有限的公司使用。例如，一家小公司可以使用 Phi-3-mini 来总结一份长篇文件的要点，从市场研究报告中提取相关的见解和行业趋势。</p><p></p><p></p><h4>商汤科技发布日日新 5.0 大模型，全面对标 GPT-4</h4><p></p><p></p><p>4 月 24 日，商汤科技发布日日新 SenseNova 5.0 大模型，采用 MOE 混合专家架构，拥有超过 10TB tokens 训练，推理上下文窗口达到 200K，全面对标 GPT-4 Turbo。该模型体系提供自然语言处理、图片生成、自动化数据标注等功能，并已应用于中文语言大模型应用平台和生成式 AI 模型，如 AI 文生图创作、数字人生成等，实现了大模型按需所取，加速生成式 AI 在产业落地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Qc2Wfg8X8No3R7QpQlR7</id>
            <title>26岁带着百人团队冲刺大模型，面壁智能天才CTO：高效比参数更重要</title>
            <link>https://www.infoq.cn/article/Qc2Wfg8X8No3R7QpQlR7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Qc2Wfg8X8No3R7QpQlR7</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 10:07:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 面壁智能, 大模型, 曾国洋, 技术团队
<br>
<br>
总结: 面壁智能是一个致力于大模型研发的技术团队，由曾国洋领导。团队注重高效、灵活和创新的工作状态，不追求参数数量，而是致力于突破模型的智能极限，提高模型性能。他们推出了多款大模型，如CPM-Bee和CPM-Cricket，并积极探索端侧模型的应用场景，致力于为用户带来更大的价值和商业空间。 </div>
                        <hr>
                    
                    <p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜曾国洋，面壁智能 CTO作者｜褚杏娟</blockquote><p></p><p></p><p>“尽管有所谓的‘百模大战’，但实际上，国内真正能够成功训练大模型并掌握相关技术的团队并不多。”面壁智能 CTO 曾国洋说道，“不是简单地训练出一个模型就意味着掌握了全部技术。”</p><p></p><p>面壁智能起于一群学术极客。2021 年，清华大学计算机系长聘副教授刘知远的牵头成立了面壁智能成立，团队成员主要来自清华大学 NLP 实验室，而曾国洋成为这家初创公司的技术 1 号位。</p><p></p><p>曾国洋如今更以“天才少年”的形象被人熟知：8 岁学编程、高中去旷视实习、大二加入清华 NLP 实验室。人们通常很难将眼前这个 98 年的少年，跟“BMTrain、BMInf 主要作者”“OpenBMB 开源社区发起人”“当红大模型创业公司 CTO”等联系在一起，但 26 岁的他确实已经被推到了大模型时代的舞台中央。</p><p></p><p></p><p></p><p></p><h3>从自己 coding 到看别人 coding</h3><p></p><p></p><p>2022 年 8 月，面壁智能开始公司化运作。直到去年年初，面壁智能只有 10 个人不到。当时的曾国洋依然活跃在编程一线。</p><p></p><p>作为程序员的曾国洋，是早期第一批申请试用 GitHub Copilot 的用户之一。他把 AI 看成是合作伙伴：AI 辅助程序员完成某些任务，而程序员则可以专注于更具创造性和战略性的工作。</p><p></p><p>“我很喜欢能够帮助加速编程的工具，”曾国洋说道，“我们不应该简单地认为只要代码被写出来，程序员的工作就完成了。编写代码只是程序员工作的一部分，如何将想法架构化以及合理划分模块并确保它们之间的有效协作等，都是程序员工作中相当重要的一部分。”</p><p></p><p>去年 5 月份后，面壁智能的规模越来越大，内部也设立了数据处理、模型训练、模型评测、算法、Infra、运维等不同的团队，以便更好地训练大模型。他的工作重心逐渐转为保证组织的有效协作。</p><p>在此期间，面壁智能迎来了许多对通用人工智能（AGI）充满激情和信仰的年轻人，“他们对 AGI 有浓厚的兴趣和追求，甚至愿意降薪过来。”</p><p></p><p>但在爆火之前，大模型并没有被广泛关注和应用，因此有相关经验的人才很少。这意味着几乎所有人都是从头开始学习和探索大模型。因此，团队在招揽新人时并不把大模型经验放在首位，而是更看重候选人的学习意愿、对新技术的热情、以及创新和解决问题的能力。</p><p></p><p>如今，面壁智能已经拥有超 100 人的科研团队，平均年龄 28 岁。这支团队的“清北”含量 80%，此外还有来自阿里、字节、百度等一线公司的骨干。</p><p></p><p>面壁智能没有给技术团队设立严格遵循 KPI 的管理形式，也没有在每一个非常具体的时间点设定明确规划，只是制定了一个大概的发展节奏和方向，因为合作的都是顶尖聪明的同事，而聪明人是会自己给自己定目标的。“我们要做的不是个人明星，而是明星团队，让聪明人能更好地合作、互相创造价值，一起创造更伟大的价值。”</p><p></p><p>面壁智能倾向“小而美”的技术团队。曾国洋强调，“小而美”并不是说团队规模小，而是指团队能够保持高效、灵活和创新的状态，成员能够频繁交流、头脑风暴，共同推动项目发展。对于技术创业公司来说，这样的团队更加敏捷和灵活，更容易产生新的思想和创新。每个成员能充分发挥自己的专长和创造力，同时快速响应市场变化和技术演变。</p><p></p><p>大模型团队的研发速度可以用争分夺秒来形容。面壁智能团队之前以两周为单位的内部迭代频率已经成为过去式，如今的节奏已经将近一周一迭代了。不断演进期间，也让面壁智能对自己做的事情有了更深入的思考。</p><p></p><p></p><h3>不再一味追求参数</h3><p></p><p></p><p>国内庞大的市场规模为大模型创业提供了巨大的发展机遇，但 OpenAI 等国外公司的频繁迭代，确实也给了国内公司很大的技术压力。时至今日，很多公司的大模型发布出来时，都是对标的 OpenAI。</p><p></p><p>不过，曾国洋表示，“我们并不过分担忧落后的问题。”他分享了一段自己的经历：</p><p></p><p></p><blockquote>ChatGPT 刚刚发布时，大家都赞叹它强大能力并讨论需要投入多少资源才能追赶上。后来，我自己投入了一些资金，买了几百条数据训练我们的模型。那次训练完测试后，我感受到了 ChatGPT 的那种效果。这个瞬间让我意识到，我们离它实际上并没有想象中那么遥远。这个经历不仅让我自己感到振奋，也给了我们团队巨大的信心和动力。它证明了我们的努力和方向是正确的，只要我们继续坚持，完全有可能达到甚至超越行业领先者。</blockquote><p></p><p></p><p>曾国洋有作为技术人的自信和思考。</p><p></p><p>“我们将 OpenAI 的成就和国际市场的竞争态势当作一种衡量自己的标杆，但不会盲目跟随。我们清楚地认识到，OpenAI 的技术路线可能并不适合我们，我们需要根据自己的实际情况和优势来制定发展策略。”曾国洋说道。</p><p></p><p>回顾 2023 年，面壁智能一直略显低调地走在大模型潮头：当年 5 月，发布了百亿参数的 CPM-Bee 大模型；年中，推出了千亿参数多模态模型 CPM-Cricket，综合能力对标 GPT-3.5、超越 LLaMA 2。</p><p>但在 2021 年、2022 年，国内在大模型上进行了大量探索，但最终都没有出现一个像 ChatGPT 的突破性应用。这让面壁智能的技术团队意识到，一味地追求模型参数量行不通，训练出一个大模型也不是最难的部分，更难的是如何突破模型的智能极限，在用同等参数、同等数据量情况下，更快速低成本地跑出更好的模型性能。</p><p></p><p>在曾国洋看来，未来大模型的发展应该朝着高效率的方向发展：大模型要为用户带来更大的价值和更广阔的商业空间，而这主要取决于模型创造的价值和创造这一价值所需的成本。</p><p></p><p>今年 2 月份推出的 MiniCPM 模型就是面壁智能对大模型高效探索的样板间。发布会上，面壁智能 CEO 李大海提出了要“以小搏大”，曾国洋也表示 MiniCPM 用 2B 干掉 LLaMA 的 13B。这意味着，面壁智能正式进入小尺寸端侧模型的竞技场，并且还将其完全开源，以帮助大模型行业整体技术发展。</p><p></p><p>起初，端侧模型并不在团队计划中，但是在测试中发现并验证了这么高性能的模型可以在手机上顺畅运行，这给团队打开了新世界的大门：一旦模型能够在手机上运行，他们就能在端侧探索出更多应用场景，如汽车、VR、智能家居场景等。</p><p></p><p>端侧模型的优势在于，不需要频繁与云端服务器通信，因此处理速度更快；在本地设备上运行，不需要消耗大量的网络带宽和云计算资源，具有成本优势；可以在没有网络连接的情况下仍然发挥作用，这意味着其可以在各种环境下稳定运行。</p><p></p><p>端侧小模型的性能天花板也远未达到。在模型的极致效率方面，通过模型压缩、量化、剪枝等，性能可以进一步优化。其次，端侧设备本身也存在优化空间，硬件制造商可以考虑如何在硬件设计上更好地支持大模型运行。</p><p></p><p>“我有预感，像 GPT-3.5 这样高水平的模型，可能在一两年内就能在移动设备，比如手机上，完全运行起来。”曾国洋说道。</p><p></p><p>在面壁智能看来，大小模型的技术有互相打通、增进提升之处。面壁 MiniCPM 基座模型、多模态模型等“小钢炮”系列领先的端侧模型，都是基于公司千亿级模型研发路线延伸，将淬炼化的大模型训练方法下放至小模型训练中，来实现高效、低成本的模型训练与应用。</p><p></p><p></p><h3>“不会因别人而改变”</h3><p></p><p></p><p>变化，是大模型创业公司时刻要面对的问题。就像曾国洋常常被问到：Transformer 会不会突然被新的技术取代，从而让之前的投入都白费？</p><p></p><p>曾国洋对技术的快速变化并不过分担忧。“技术的发展是一个循序渐进的过程，不可能一夜之间出现一个全新的技术彻底颠覆现有的一切，而我们对此毫无准备。”在制定研发路径时，团队也是根据技术发展趋势和团队正在进行的工作，逐步调整目标和方向的。</p><p></p><p>对于市面上时不时蹦出来的热点模型或产品，曾国洋也表现得很冷静。</p><p></p><p>以 Sora 为例，曾国洋认为这显示出人们对创意性工作的兴趣，但对于是否跟随这一技术路线则需慎重。</p><p>“对于创业公司来说，需要格外考量战略目标与投入成本。即使是资金充裕的大公司，虽然有能力进行，但产出并不总是明确，短期内可能无法快速为大众提供实质性的服务。”曾国洋说道，面壁智能致力于将技术更好融入到实际产品和解决方案中。</p><p></p><p>对于前段时间刷屏的月之暗面 Kimi，曾国洋则一方面表示肯定，“Kimi 用户的增长迅速，表明它成功地解决了一些用户的痛点”，另一方面也反思自己，“可能没有充分利用我们在某些方面的先发优势。”</p><p>他特别提到了去年 5 月份面壁智能推出的一项读论文功能，虽然早就有了类似产品，但当时很可惜没有深入挖掘和清晰传达该功能可以解决的痛点。</p><p></p><p>但曾国洋强调，面壁智能的战略不会因为市场上的其他产品而改变。“我们一直在寻找大模型技术在普通人生活中的应用，并努力解决实际问题，而不仅仅是提供通用的解决方案。”</p><p></p><p>最近，李彦宏“开源模型会越来越落后”的观点也引起了很大的争议，有人“力挺”、有人“怒怼”。</p><p></p><p>对于面壁智能来说，开源是成立之初就做好的选择。正如李大海所说的：“我们一直是开源的受益者，所以也希望做出自己的贡献。并且，一款拥有良好口碑的开源模型，一定是经受住了方方面面的反复检验，在模型性能、体验等综合表现方面，拥有远超过 PPT 成绩的行业认可度。对于我们研发团队，一方面是 360 度无死角的考核压力，另一方面在挑战成功后也会有巨大的成就感。”</p><p></p><p>曾国洋坦诚，开源模型要追赶闭源模型确实会面临一定的挑战。这是因为在技术快速发展的过程中，闭源模型可能会因为有更好的知识产权保护和商业秘密而获得一定的优势。这种情况下，开源模型需要同时关注技术创新和与闭源模型保持竞争力。</p><p></p><p>但技术发展进入瓶颈期，那么开源和闭源模型可能就会在技术水平上趋于一致。在这种情况下，开源模型由于其开放性和社区的支持，会有更多的机会迎头赶上，甚至超越闭源模型。另外，开源模型的发展速度也取决于社区和市场的支持程度。如果有更多的个人和组织支持，那么开源生态的发展自然会更快。</p><p></p><p>此外也很重要的一点是，开源本身在技术影响力的建设方面是特别重要的，可以更好地让大众体验团队的技术实力，从而在人才吸引力和市场信心提升方面取得更强的竞争优势。</p><p></p><h3>“没有刻意区分 C 端和 B 端”</h3><p></p><p></p><p>对于以科研人员为核心创始团队的大模型创业公司来说，在战略、产品、经营等方面需要更强的专业管理者。2023 年，时任知乎 CTO 的李大海加入面壁成为 CEO，面壁向更为成熟的大模型商业公司迈下重要一步。</p><p></p><p>今年 4 月，面壁智能又完成了新一轮数亿元融资，由春华创投、华为哈勃领投，北京市人工智能产业投资基金等跟投，知乎作为战略股东持续跟投支持。除了通过融资获取资金外，面壁智能目前已经能够通过提供服务和产品实现一定的收入。</p><p></p><p>面壁智能是国内最早探索 Agent 的大模型公司之一。对于 Agent，每个公司、每个人的理解都不一样。在面壁智能看来，Agent 的边界还未被定义。“模型是底座是一切应用的基础，然后 Agent 是支撑应用很重要的中间层，”李大海认为，无论 to B 还是 to C，本质上都是“大模型 + Agent 的上层应用”。</p><p></p><p>曾国洋认为，Agent 实际上是介于纯大模型和通用人工智能（AGI）之间的一个中间状态或节点。Agent 的模型能力必须足够强大，才能有足够的智能理解和处理请求和执行任务。Agent 还需要与外部系统和接口进行交互，来不断拓展能力边界。同时，还能够调用已有的知识库来提供检索和回答服务。</p><p></p><p>而对于大模型领域，李大海曾表示，大模型应用可能会百花齐放，然而通用千亿大模型不会太多，可能只有极少数的几家公司能够最终突出重围。这基本也是行业的共识，基座模型的角逐注定是千军万马过独木桥。</p><p></p><p>那么，大模型公司做应用是对应用侧公司的一种降维打击吗？</p><p></p><p>曾国洋坦言，大模型公司由于其先进的技术和强大的数据处理能力，可能会对那些依赖传统技术或缺乏足够技术储备的应用公司产生影响。如果应用公司的技术壁垒不够坚固，就可能会在大模型技术的快速发展和迭代中受到冲击。例如 Jasper AI 这样的 AI 内容提供商可能会因为 OpenAI 发布了新的 ChatGPT 版本而受到影响。</p><p></p><p>然而，他也表示，应用公司也有自己的竞争优势，比如对特定市场的深入理解、强大的客户关系和品牌忠诚度等，这些都是他们的“护城河”。</p><p></p><p>2024 年，行业更加重视应用落地是当前的大趋势，因为目前模型已经基本可用了。李大海判断，从今年开始，大模型厂商会开始出现分层。但这个分层不是因为市场，更多是因为技术门槛：大家需要更强的模型、更高效率的推理，更好的 Agent 等，但不是每家公司都能跟得上这样的技术要求。</p><p></p><p>“大模型是一个行业级别的机会，哪怕不做基座大模型，做应用层也有非常多的空间。但不是每家公司都能够持续做基座大模型的训练，期间有些公司可能就会转型做其他的事情。能活下来的公司一定是技术和产品市场能力都很强的选手。”李大海表示。</p><p></p><p>对于未来的大模型应用，曾国洋提出了一种分工模式：一些简单的、重复性的任务由小型的、特定领域的模型来处理；而更复杂的、需要高级认知能力的思考任务则可能由大型通用模型来完成。</p><p></p><h4>结束语</h4><p></p><p></p><p>最初，人们普遍认为 AI 会先替代那些繁琐的工作，从而让人类有更多的时间从事创造性的工作。但现实情况似乎相反，AI 开始在创作领域发挥作用，而人类仍然在处理日常的工作任务。</p><p></p><p>但曾国洋观察，大模型简化、加速开发任务是正在发生的事情。在创建大模型应用企业的交流中，技术人员并不需要掌握大量的编程代码，更重要的是他们对最终产品的理解能力。</p><p></p><p>“对于想要有效利用大模型的人来说，掌握大量的编程能力并不是必需的。更重要的是能够将自己的思维方式与模型同步，确保模型理解并执行自己的想法。”曾国洋说道，“让模型理解你的想法是一种独特的体验，它要求用户对模型的运作方式有一定的了解，并且能够清晰地表达自己的概念和目标。”对于其他公司来说，大模型技术会以不同形式的工具、功能出现。</p><p></p><p>面壁智能的目标是实现通用人工智能，团队为此有一个清晰的路线图，包括在文本模态上要达到的效果、未来向多模态和具身智能的转变等规划。2024 年，面壁智能将继续专注于模型的研发和优化。而面壁智能的选择能否助其顺利发展、实现自己的 AGI 理想，还需要时间来回答。</p><p></p><p>栏目介绍</p><p></p><p><a href="https://www.infoq.cn/theme/230">《大模型领航者》</a>"是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p></p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5h8vXmEcRXYPBkD6VQUF</id>
            <title>大模型的“瘦身”革命：巨头逐鹿轻量化大模型 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/5h8vXmEcRXYPBkD6VQUF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5h8vXmEcRXYPBkD6VQUF</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 08:06:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 移动设备端, 长文本处理, 开源领域
<br>
<br>
总结: 大模型的快速发展使得了解最新技术动态和积极学习成为从业者的必修课。本周重点关注了轻量化和设备端集成，展示了AI应用向移动设备端迁移的趋势，以及长文本处理能力的竞争。同时，开源领域也有不少新进展，如苹果公司和Snowflake的开源模型。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，轻量化和设备端集成成为行业的热点。微软的&nbsp;Phi-3&nbsp;系列小模型和苹果的&nbsp;OpenELM&nbsp;系列端侧小模型的发布，展现了AI应用向移动设备端迁移的趋势。这也预示着未来智能手机和笔记本电脑等设备将能够处理以往只能在云端或高性能服务器上执行的复杂任务，极大地扩展了端侧AI的潜力和应用范围。此外，长文本处理能力的竞争再次增大。Kimi&nbsp;发布之后，商汤和浪潮分别升级自身模型的长文本能力并展开第二波围剿。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>4&nbsp;月&nbsp;23&nbsp;日，微软推出了&nbsp;Phi-3&nbsp;系列小模型，并发布了其技术报告。值得注意的是，Phi-3-mini&nbsp;型号，仅拥有3.8&nbsp;亿参数，已在众多性能评估标准上超越了&nbsp;Llama&nbsp;3&nbsp;模型。为了促进开源社区的发展，微软特别设计了与&nbsp;Llama&nbsp;系列相兼容的模型架构。4月&nbsp;23&nbsp;日，商汤科技最近宣布推出了其最新的大模型——日日新&nbsp;SenseNova&nbsp;5.0&nbsp;大模型，该模型采用了混合专家（MoE）架构。SenseNova&nbsp;5.0&nbsp;在超过&nbsp;10TB&nbsp;tokens&nbsp;的数据集上进行了训练，上下文推理长度达到了200k。4&nbsp;月&nbsp;25&nbsp;日，浪潮海岳大模型2.0正式发布。在长文本、长图文、长语音处理方面能力进行升级。</p><p></p><h4>开源领域</h4><p></p><p>4&nbsp;月&nbsp;22日，苹果公司开源了&nbsp;OpenELM&nbsp;系列小模型，涵盖2.7亿、4.5亿、11亿和30亿四种参数规模。这些模型能在个人设备上运行，包含从&nbsp;2.7亿到&nbsp;30&nbsp;亿参数的不同版本，旨在推动设备端&nbsp;AI&nbsp;应用。4&nbsp;月&nbsp;25&nbsp;日，Snowflake&nbsp;推出了名为&nbsp;Arctic&nbsp;的开源大型语言模型（LLM）。Arctic&nbsp;模型采用独特的&nbsp;Dense-MoE&nbsp;混合&nbsp;transformer&nbsp;架构，以低成本实现较高企业智能水平。此外，Arctic&nbsp;的上下文窗口初始设置为&nbsp;4K，团队正在研发支持无限序列生成的技术，未来将扩展到&nbsp;32K。</p><p></p><h4>多模态领域</h4><p></p><p>4&nbsp;月&nbsp;22&nbsp;日，腾讯&nbsp;Robotics&nbsp;X&nbsp;和腾讯&nbsp;AI&nbsp;Lab&nbsp;提出了多模态AI大模型&nbsp;SEED-X。该模型是对之前&nbsp;SEED-LLaMA&nbsp;的升级版，能够理解任意尺寸和比例的图像，并包含多模态预训练和指令调整两个阶段，使用大规模多模态数据集增强模型的适应性和灵活性。在定量和定性实验评估中展现了卓越的性能，尤其在公共基准测试和现实世界应用场景中表现突出。4&nbsp;月&nbsp;22&nbsp;日，西湖大学、浙江大学的研究团队发布多模态大型语言模型（MLLM）——Cobra。它利用&nbsp;Mamba&nbsp;语言模型并融合视觉编码器，以线性计算复杂度提供高效的推理性能。在多个基准测试中，Cobra&nbsp;展现了与参数更大型模型相媲美的性能，尤其是理解和处理视觉信息方面。4&nbsp;月&nbsp;25&nbsp;日，北京大学&nbsp;Yuangroup&nbsp;开源的&nbsp;open-sora&nbsp;更新升级。新增功能包括支持长达&nbsp;16&nbsp;秒的视频生成，最高720p&nbsp;的分辨率，并且能够处理不同宽高比的文本到图像、文本到视频、图像到视频、视频到视频以及无限长视频的生成需求。</p><p></p><h4>科研领域</h4><p></p><p>4&nbsp;月&nbsp;22&nbsp;日，美国AI蛋白质设计公司&nbsp;Profluence&nbsp;推出了世界上首个开源的AI生成的基因编辑器&nbsp;OpenCRISPR-1。成功实现了对人类基因组的精确编辑。该技术基于与&nbsp;ChatGPT&nbsp;相同的方法，通过分析大量生物数据，生成了数百万种自然界中不存在的&nbsp;CRISPR&nbsp;类蛋白质，扩展了&nbsp;CRISPR&nbsp;家族的多样性。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>4月&nbsp;20&nbsp;日，文生图服务平台&nbsp;Leonardo.ai&nbsp;引入了新的图片样式引导功能。该功能允许用户上传个性化图片以生成更为精确和多样化的图像成果。该功能类似于用户友好的视觉微调工具，使得用户能够根据自己的需求定制连贯的连环画作或保持视觉一致性的宣传海报。4&nbsp;月&nbsp;22&nbsp;日，腾讯公司宣布其协作&nbsp;SaaS&nbsp;产品线全面整合了腾讯混元大模型。这包括了企业微信、腾讯会议、腾讯文档等核心产品，以及腾讯乐享、腾讯电子签、腾讯问卷和腾讯云AI代码助手等其他工具。4&nbsp;月&nbsp;25&nbsp;日，阿里巴巴通义实验室在通义&nbsp;APP&nbsp;上线&nbsp;EMO&nbsp;模型。该AI技术能通过人物照片和音频生成同步口型和表情的视频。为防止技术被滥用，通义实验室在应用内预置了经过审核的音频模板，暂不开放用户自定义音频，并采取了算法和人工两道审核机制，确保内容安全。</p><p></p><h4>智能体</h4><p></p><p>4&nbsp;月&nbsp;25&nbsp;日，Sanctuary&nbsp;AI&nbsp;推出了第七代&nbsp;Phoenix&nbsp;人形机器人。新一代机器人具有更长的运行时间、更快的构建速度、更低的制造成本、增加的运动范围和耐用性，以及更高的视觉和触觉感知能力，同时与麦格纳国际合作，推动通用人工智能机器人在汽车制造等领域的应用。</p><p></p><h3>基础设施</h3><p></p><p>4&nbsp;月&nbsp;21&nbsp;日，中山大学、哈佛大学的研究人员针对多模态大模型的创造力进行研究并提出&nbsp;Creative&nbsp;Leap-of-Thought（CLoT）的训练方法，旨在打破常规思维，激发模型的创新能力。CLoT&nbsp;能够有效提升多模态大模型在创造性任务中的表现，超越了包括&nbsp;GPT-4&nbsp;在内的其他先进模型。此外，该研究还构建了&nbsp;Oogiri-GO&nbsp;数据集，为进一步研究提供了资源。4&nbsp;月&nbsp;22&nbsp;日，阿里云的百炼平台为&nbsp;Llama&nbsp;3&nbsp;模型提供了一站式的解决方案，覆盖了模型的训练、部署和推理等关键环节。目前，阿里云在一定时间内对&nbsp;Llama&nbsp;3&nbsp;模型的开发和调用实行免费政策，用户可以在百炼模型广场上申请试用&nbsp;Llama&nbsp;3，并与其他模型进行性能对比。4&nbsp;月&nbsp;23&nbsp;日，华为云在香港峰会上宣布，将在香港提供即开即用的&nbsp;AI&nbsp;云服务，为大模型训练和推理提供高效、长稳、可靠的&nbsp;AI&nbsp;算力。华为云通过全链路云化工具链支持大模型的高效迁移、开发和运行，并特别优化了昇腾云的大模型专区，以支持“百模千态”应用的快速落地。4&nbsp;月&nbsp;24&nbsp;日，高通发布骁龙&nbsp;X&nbsp;Plus&nbsp;芯片。该芯片采用&nbsp;4nm&nbsp;工艺，具备&nbsp;10&nbsp;核心和最高&nbsp;3.4GHz&nbsp;主频，GPU&nbsp;算力达&nbsp;3.8TFLOPS，并支持先进的连接技术。该芯片在&nbsp;AI&nbsp;性能上达到&nbsp;45&nbsp;TOPS，与骁龙&nbsp;X&nbsp;Elite相当，且在多线程&nbsp;CPU&nbsp;性能上超越了苹果&nbsp;M3&nbsp;芯片。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心近期发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>AGI究竟是什么？AI&nbsp;Agent&nbsp;如何助力人工智能走向AGI时代？在营销、金融、教育、零售、企服又有哪些典型应用和案例？欢迎大家持续关注InfoQ研究中心即将发布的《中国AGI市场发展研究报告&nbsp;2024》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c0207976c6592ac74b5109332dc9e1c.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O</id>
            <title>大模型开闭源争吵不休：开源落后闭源一年，决定模型能力的不是技术？</title>
            <link>https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 06:36:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源, 闭源, 大模型, 模型能力
<br>
<br>
总结: 作者华卫讨论了开源和闭源模型在大模型时代的争议。虽然开源模型在崛起，但其背后问题不可忽视。访谈中专家们分析了开源和闭源模型的能力差异，以及影响因素。他们认为模型能力取决于团队的能力、数据、算力等因素，而开源并不一定能带来更好的效果。整体来看，开源社区仍落后于闭源社区，但差距在逐渐缩小。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p></p><p>开源和闭源之争，在大模型时代依然延续着。前不久，百度创始人李彦宏在内部讲话中发出“开源模型会越来越落后”的言论，再次将这一话题引爆。</p><p></p><p>不仅有许多业内人公开提出不同看法，似乎还接连迎来市场层面的“回应”：Meta 时隔两日发布性能直追 GPT 4 的开源大模型 Llama 3，苹果、微软又各自开源了针对手机等移动设备的语言模型 OpenELM 和 Phi-3 Mini。</p><p></p><p>然而，尽管开源模型在今天的崛起有目共睹，其背后的问题依然不可回避。由于本身的黑盒属性，开源的“众人拾柴火焰高”优势并不能完全显现在大模型上，甚至成本和效率更受影响。那么对于各个行业的厂商来说，身处如今的大模型市场，该做出怎样的选择？</p><p></p><p>带着这一问题，InfoQ《极客有约》特别邀请了零一万物开源负责人林旅强担任主持人，与 Data Strato 副总裁史少锋、华为 AI 科学家张敏、LLMFarm &nbsp;创始人 &amp; CEO 宜博， 在 AICon 全球人工智能与机器学习技术大会即将召开之际，一同探讨开源与闭源模型的现状、差异及未来发展。部分亮点如下：</p><p></p><p>整体开源落后于闭源，以 GPT 为代表大概是一年时间的差距；模型能力的差异不在于开或闭，而是背后的人与团队；自建模型还是购买第三方服务，企业要根据各自的商业场景选择成本和合规需求最适合的部署方式；企业使用大模型可能不止一套，会像今天使用云一样是混合架构；正确认识大模型的能与不能才是避坑最好的条件。</p><p></p><p>在访谈的第一部分，四位专家分别对开源、闭源大模型的成本能力和效益进行了分析；第二部分分析了两类大模型面临的技术和合规挑战；第三部分则是从实际应用与效果角度进行了分析。以下为访谈实录，经编辑。</p><p></p><p></p><p>完整视频参看：</p><p><a href="https://www.infoq.cn/video/pKua6PxVgxvdDygcgrWd">https://www.infoq.cn/video/pKua6PxVgxvdDygcgrWd</a>"</p><p></p><p></p><h1>开源、闭源哪家强？</h1><p></p><p></p><p>林旅强：目前从模型能力的角度来说，开源阵营和闭源阵营之间整体是什么样的情况？</p><p>张敏：大模型是从 ChatGPT 热起来以后，被越来越多的人和公司关注到，现在看是有开源、闭源之说。闭源的代表是 OpenAI，以及 Claude 也有一部分模型是闭源的。开源来看，从 Llama 1 到最新的 Llama 3，效果越来越好，大家也越来越认可这些模型，最近看到 Meta 的 400B 大模型，效果已经和 GPT 4 非常接近了。从开发者角度，我们希望能看到更多效果更好的开源模型，这实际上对整个大模型领域的繁荣可能会有更多帮助。</p><p>宜博：个人认为，整个开源和闭源社区的模型分为三个阶段：小于 GPT 3 或者 3.5 的，接近于 GPT 3 和 3.5 的，接近于 GPT 4 的。去年上半年， OpenAI 发了 GPT3.5 和 GPT 4 之后遥遥领先于整个开源社区；到去年下半年时，开源社区的情况有了很大改变，发布了很多接近于 GPT 3-3.5 能力的新模型，今年上半年开始有一些部分能力已经靠近 GPT 4 的开源模型。</p><p>整体来讲，开源社区当前还是落后于闭源社区，如果以 GPT 为标准呢，大概是一年时间的差距。开源社区其实一直处在追赶闭源社区的态势，但这种差距在缩小。今年上半年又发了 Sora，开源社区开始追 Sora，到现在为止虽然做了很多努力，但效果还差很多。</p><p>史少锋：刚才两位老师发表了他们的观点，我觉得整体上大家的感觉差不多，就是一开始闭源模型遥遥领先或让人眼前一亮，但随着更多的开源模型被放出来，开源的能力也在快速跟上。作为模型使用者，今天我们主要还是通过 API 的方式来用大模型，但现在新的开源模型能力越来越强，同时对计算资源的要求在不断降低。我们期待不远的将来，开源模型可以在本地跑起来，能够完全私有化地去支撑一些应用，这对我们有很大的吸引力。</p><p>林旅强：那什么因素会严重影响开源和闭源模型的能力差异呢？</p><p>针对这个问题，我个人认为开源和闭源模型的能力差异，重点不在于它开源或闭源，而是它的研发团队的能力差异。至于做出来的模型要开源还是闭源，是进一步从该公司的整体商业模式去考虑的点。之前 Llama 推出的时候，我非常兴奋，觉得终于有人运用开源来突围闭源的大模型了，因为训练模型成本实在太高，要开源本来就不容易；虽说至今二者仍有些差距，但如果不开源就没机会给开发者和产业界有另一种选择了。</p><p>史少锋：的确，模型会很依赖于开发团队的工程能力，并不在于开源还是闭源。今天的开源模型也并不是真正的开源，正如百度创始人李彦宏所说，大模型本身就是一个黑盒子，并不能指望社区有多少贡献。除此之外，模型还依赖于掌握的数据语料质量、丰富程度以及算力规模。这也是为什么今天我们看到，只有非常大型的公司才能开发出让整个业界为之一亮的大模型。</p><p>宜博：我认同开源和闭源对模型能力的影响并不在于形式，而在于背后的人，和背后的团队所持有的资金、算力、数据。</p><p>林旅强：大模型跟开源软件有一点很不一样的地方，就是开源软件有可能因为社区不断有代码贡献而变得更好，但现在业内所谓的开源大模型则是把权重 open 出来，没办法以开源社区贡献上游的模式让算法和数据质量更好，确实很依赖出品团队的能力，如数据、框架算法调优、算力门槛还有最新方法的挑选。所以在我们看来，模型能力的差异不在于开源或闭源，而在于团队的人才密度有多高。</p><p>张敏：数据、算力和算法对大模型都至关重要，算法是与团队是强相关的，这对于模型最终效果的提升是非常重要的。</p><p>林旅强：刚才我们讨论到开源、闭源模型的能力，那它们的差距到底是逐步缩小还是增大？开源是不是会越来越不好？闭源越来越领先？</p><p>宜博：我认为差距并不是持续扩大和缩小，而是永远在动态平衡变化的状态。</p><p>林旅强：那照你的描述是不是永远闭源走在前面，开源在追赶？</p><p>宜博：这一点其实是由行业现状决定的，比如在服务器领域，Windows 现在很难追得上 Linux，iOS 有一些领域也追不上安卓。大模型领域是由 OpenAI 开始主导的，所以在其领头羊位置不变的情况下，不管是闭源还是开源的，只要落后于 OpenAI 都是在追赶。</p><p>林旅强：所以这个问题应该调整为，GPT 跟其他模型的能力是逐渐缩小还扩大。</p><p>史少锋：站在百度文心一言的角度来说，我理解他们在思考的是有没有必要做开源，开源模型并不一定能像普通开源软件那样有“众人拾柴火焰高”的效果，反而要花费更多的时间和精力去做各种合规、对外发布、问题收集等流程。在这种情况下，他们认为开源没有必要，闭源的话效率更高，可以使团队更加聚焦于训练下一代模型。某一天 OpenAI 把大模型开源了，是否能代表开源打倒了闭源呢？我觉得也不是。</p><p>林旅强：那从成本、能力、效益分析的话，部署自己的大模型与使用第三方大模型在初期成本上有什么不同？长远来看，自建模型与购买模型服务在成本上又会如何变化？</p><p>宜博：我们做了很多轮实践发现，假如第一次去验证模型，用 API 调用是最划算的，因为 API 用量很少。但如果要跑数据，一定要用自己的服务器和开源模型去做，否则成本太大了。比如我们曾经有个项目，大概算下来，全部跑 API token 比自己购买服务器的成本要贵 200 多万。再就是推理部署的未来环境，用户量大到一定程度后会有个临界点，可能就用自己的服务器比较划算了。所以，要根据大家各自使用的场景去选择不同的成本策略。</p><p>张敏：从我们对接的客户来看，他们是更希望通过本地的私有化部署来做业务支撑，这对数据安全是非常有好处的。</p><p>史少锋：站在用户的角度，我觉得今天的 SaaS 大模型服务已经非常便宜，如果自己去搞部署，那成本就高了去了。目前 Open AI 的价格不代表以后，大家都在卷，很多价格会更低，国内甚至有免费开放给公众使用的。对于 To B 领域，可能第一考虑的是数据安全，To C 没有看到用私有化部署的。</p><p>林旅强：确实，除了部署成本外还有一些隐性的成本，比如客户是不是愿意模型平台把他通过 API 所调用的数据拿出去再训练。个人去使用的话， API 确实门槛比较低，现在各家的价格都还算是比较便宜。</p><p>那如果从总体的成本控制方面，企业应该如何去选择适合自身的大模型策略？</p><p>我个人认为要看企业本身想怎么用大模型，如果单 API 就能够解决且量没有很大的情况下，先去把 API &nbsp;稳定地搞起来；但如果要结合非标的数据场景去做，那只能加上开源的部署。</p><p>宜博：企业真正在用的时候，一般是一个递进的验证过程，首先用最便宜的 API 去验证 POC，甚至直接在 ChatGPT 上免费验证，之后如果有开源的部署需求，再去验证场景。过程中需要企业自己想清楚，如何在满足场景的情况下选择成本和合规需求最适合的部署方式。</p><p>林旅强：我想补充一点，之前有人问国内是需要私有部署的多还是调 API 的多，我就说要先看合规问题。因为现在有政策要求用国产服务，但还有一些人是用了“套壳网站”调外网大模型的 API 。</p><p>张敏：大模型也有参数量的大小区别，我们真正在给客户在做应用时，还是要根据业务领域的效果来看。在百度的文心一言里，也是用大模型和小模型一起来支持用户需求。</p><p>史少锋：企业使用大模型后，可能也会像今天使用云一样是混合架构，根据不同需求一部分可能会放在公有云上，一部分放在私有云。为了确保应用端的用户无感，可以把 SaaS 版的大模型作为一个 Plan B，相当于做了一层保护机制。综合而来的话，以后企业可能不止一套大模型。</p><p>林旅强：我也想补充一下，现在所谓的大模型到底多大？从成本能力与效率分析来讲，我们也得把大模型分为不同档次。虽然 scaling law 是存在的，但越大的模型性价比越往下；而小模型现在要做出效果的门槛其实也很高。目前不管多大的模型都有各种不同的成本要去考虑，所以最终还是需要回到具体场景和商业产品的本质来看。</p><p></p><p></p><h1>技术与合规挑战</h1><p></p><p></p><p>林旅强：在技术实现层面，自建大模型与采用第三方模型在技术难度和支持上有何不同？</p><p>宜博：现在自建大模型一般有几种难度：第一种是买一个小机器放在办公室，如果要买高算力机器放在机房或者自建机房，难度指数是很高的；第二种，有了算力去部署时，也会遇到各种各样的问题，如推理框架选择、速度、机器使用等，这些对于没有专业技能团队的非技术企业消耗很大，过程中虽然所有技术人员学了很多东西，但公司的环境部署和上线成本非常大。</p><p>史少锋：我觉得这个问题并不是很精确，自建大模型和用第三方模型的技术难度和配置不同。今天大家都在用第三方模型，但自建大模型还是偏少，大家更多还是用外部做得好的模型，区别就是自己部署的大模型和第三方 SaaS 大模型之间的区别。就像刚才宜博说的，自己去部署要操心的是方方面面，包括硬件采购、运维、算力扩容、模型部署和升级、调优等。相较而言，用第三方模型更简单，很多代码拿来就可以用，但这个情况也在逐渐改变。</p><p>随着开源生态越来越健全，软件也越来越丰富，下载速度可能更快，以后笔记本都能跑一些参数不太大的模型。在并发量或需求量不太大的场景下，自建大模型不会比第三方模型复杂太多，gap 会逐渐缩小。</p><p>张敏：用开源大模型去做部署就像站在巨人肩膀上，会走的更快。自建则需要具备很多前提，如数据、算力、算法和好的团队，成本可能要远高于使用开源。</p><p>林旅强：自建大模型的难度比较大，技术实践已经是一道门槛，像开发者本身的能力水平、背后商业机会以及交付能力等。直接采用第三方模型，也需要运维、部署的知识能力和资源投入。所以企业还是要按照能力和成本考量去选择。</p><p>另外，我们都知道大模型可能涉及到数据安全和个人隐私的保护。在自建与第三方模型使用中，数据安全与隐私保护分别面临哪些挑战？大家怎么去做呢？最简单的是，担心就全部私有化部署，如果数据不需要任何安全和隐私保护，就全部调 API。也就是说，还是从业务角度去选。</p><p>宜博：实际上我们会遇到几种情况，第一种情况就是直接调用闭源模型的 API，他们号称数据不会被拿去训练，但实际经常会发现数据被使用了；第二种是当你用三方算力平台训练模型时，也会发现有自己训练数据被拿去的情况。大家知道现在监管非常严，内部虽然保证数据安全和隐私，但实际上做合规很耗精力，面临的细节挑战还蛮多。现在整个落地量不大，所以问题还没有那么凸显，但我认为未来会逐步变得重要。</p><p>史少锋：针对大模型，我认为不管是自建还是第三方、私有化部署还是公有，都应该足够重视数据安全和隐私保护。即便自建大模型，训练时没有识别出数据隐私，也可能导致信息泄露。而开源模型正因为要开放给众多用户，在安全和隐私方面也可能做得很好。Meta 发布的 Llama 3，就花了很多功夫在多个层次进行安全检测。</p><p>这就像我们经常讨论的，闭源软件安全还是开源软件安全？闭源软件可能因为黑客看不到源代码，所以找不到安全漏洞，但不为人知的漏洞可能会存在更长的时间；开源软件貌似因为代码开放容易被抓到漏洞，但因为被很多人盯着，促使其在不断地提高安全性，长久来说可能反而做得更好。</p><p>张敏：数据安全对于大模型来说确实非常重要，训练时会牵涉到用户的隐私数据，抓取也可能存在攻击性数据，从而导致输出问题。另一方面，即使大模型做了私有化部署，使用过程中产生的数据也需要做安全保护。</p><p>林旅强：再补充一个点，很多人在讲数据安全时并没有考虑到跨境传输。现在出海很热，实际应用来讲可能每个地区对于数据跨境的要求不同，在各个市场各自部署的成本也就更高。合规不只要考虑到中国，还有客户所在的国家，像欧盟、美国都会有相关的数据法规。</p><p></p><p></p><h1>实际应用与效果</h1><p></p><p></p><p>林旅强：利用开源或闭源大模型解决实际业务场景，在部署过程当中有哪些区分？大家分享一下踩过的坑，也教教怎么避坑。</p><p>宜博：第一个观点是尽量用 RAG，不要一上来就做 SFT 训练；第二个是尽量不要一上来就用 Langchain，要花大量的时间去学习未来 90% 都用不上的代码。</p><p>张敏：我们去跟客户做支撑的时候，需要把用户场景和数据越早明确下来越好，这对于我们的方案设计和模型选择都非常重要。</p><p>史少锋：关于大模型在具体业务场景的避坑，我觉得还是要实践出真知，有一套针对自己场景的测试数据集，因为大模型过于通用，并且也会升级。我们想到的办法是可以用另一个更高水平的大模型来对多个模型的输出打分。建立一套测试体系，对于不停迭代模型去提升结果准确性很有必要。</p><p>林旅强：我觉得要能够正确认识大模型能够为你解决什么问题，作为避坑的前提条件。就像张老师刚刚讲的，很多客户现在误以为大模型跟神仙一样什么都能干，这其实是有问题的，大模型只是在某一些方面确实做得比过去好很多，甚至比人类强。但我们还是要把业务流拆解出来，哪部分去接入大模型？能够做什么？怎么解决幻觉问题？RAG 好在哪里、难在哪里？也绝不是那些开源数据集测评的打分越高代表越好，还是得从具体场景切入，认真把内部评测标准搞好，才会知道坑在哪里。所以我觉得，正确认识大模型的能与不能才是避坑最好的条件。</p><p>现在线上有个问题，即使训练内部模型也需要对涉及用户的数据进行脱敏，在这方面有没有一些比较好的工具或经验？各位老师实际有没有接触过用户的数据，以及会用什么方式把用户的数据脱敏？</p><p>史少锋：我们本身就是做数据治理，也调研了市面上很多数据平台在这方面的做法。成熟的数据平台都有一套数据合规方面的功能体系，其次会通过 AI 去识别数据中的敏感信息，在导出时提醒用户，还有一些敏感信息打码、用户访问 policy 以及数据溯源的配合功能。对于一些自建的大数据平台，是借助工具和统一平台去数据溯源、定义用户访问权限，来把风险被控制到最低。</p><p>宜博：这块我们做的比较少，一般的客户数据就在本地或者企业内部查询了，脱敏拿出来的情况还比较少。</p><p>张敏：我们这边做的更多是回复角度方面，如果涉及到敏感内容的话，会对回复做过滤处理或者换一种方式去回答。</p><p>林旅强：那你怎么知道它是敏感的？</p><p>张敏：我们会做一些检测，如果问题本身涉及到敏感词，就需要做过滤和管控。</p><p>史少锋：其实常用常见的 PII 信息是有一套正则规范的，身份证号码、Email、社保号等都有，即便是文本型的识别也并不是特别难。</p><p>林旅强：未来开源模型如何能利用好社区优势？有哪些方向和趋势？</p><p>开源确实比较能实践社区的方法，闭源提供 API 的就只能是用户。在我看来，当前的“开源”大模型并不是真的把数据或训练代码开源出来，而是把训练的结果也就是权重给 open 出来，海外也有称为开放大模型的。可是，它又不像是闭源软件的二进制，开发者又能基于开放大模型来做二次开发，例如 SFT、继续预训练等，情况有点介于软件开源和闭源光谱当中的中间态。因此，开源模型仍然有一定的被二创的空间，闭源模型则不太容易这样操作，所以我认为，即使开源大模型没有开源软件那么开放，但开源模型社区的优势就是可以有很多二创。</p><p>刚刚说大模型太热，如何解决预期过高的问题？业务方老板可能误以为未来一切都靠 AGI 了，但其实当下能做的事非常有限，我们如何向这些没那么懂但手头有预算且脑中有想象的用户，去正确传递大模型的能力界限？</p><p>宜博：我们从去年到今年做最多的就是给大家分享大模型是什么？什么能干？什么不能干？边界在哪里？背后的原理是什么？现在能真正把这些事情和自己的想法都传递给客户的团队还比较稀缺，希望有更多的程序员和技术领导者加入进来。第一，不要太高估短期大模型的能力；第二，不要太低估长期大模型的能力；第三，在当下把能落地的场景先落地。</p><p>张敏：我们这边的做法是，通过 GPT 4 中目前我们认为的最好效果，让大家客观看到当前大概做到什么程度。</p><p>史少锋：刚刚宜博说的是大家眼下不用对大模型期望太高，要知道它目前只是一个助手，还需要懂业务和有专业技能的人去做最后的把关，同时我们只有不断去试去用，才能找到最适合的方向。现在大家看到文生图出来也没多久，但有很多文章配图都换成了 AI，意味着这方面的生产效率已因此得到很大提升。</p><p>林旅强：总结一下就是， AI 的天花板取决于使用者的个人理解和业务认知。在你的指导之下去做工作的 AI，不可能做得比你还厉害，你才是 AI 的天花板。当我们都了解到这一点，就知道它的局限。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7y5946vBWVXNpJvYzmZl</id>
            <title>苹果与 OpenAI 重启谈判，Siri 或引入 ChatGPT，网友：国行用文心一言？</title>
            <link>https://www.infoq.cn/article/7y5946vBWVXNpJvYzmZl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7y5946vBWVXNpJvYzmZl</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 06:15:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, OpenAI, iPhone, 人工智能
<br>
<br>
总结: 苹果正在与OpenAI就在iPhone上集成聊天机器人功能进行谈判，以加速推进人工智能技术的发展。虽然目前尚未达成最终协议，但这一举措标志着两家公司之间对话的重启，同时苹果也在与谷歌讨论将Gemini引入iPhone。这一举动显示了苹果在人工智能领域的积极探索和合作意愿，未来可能会为用户带来更多智能化的功能和体验。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p>&nbsp;</p><p>据外媒报道，苹果正在就iPhone集成聊天机器人功能，加紧与 OpenAI 的谈判。有知情人士透露，两家公司已开始讨论可能达成的协议条款，以及如何将OpenAI功能集成到苹果的下一代iPhone操作系统 iOS 18中，但因为审议是私人的，他们要求不透露身份。</p><p>&nbsp;</p><p>虽然目前尚未达成协议，但此举标志着两家公司之间对话的重启。今年早些时候，苹果曾与OpenAI就一项交易进行谈判，但那之后双方却很少合作。</p><p>&nbsp;</p><p>去年，苹果公司 CEO 蒂姆・库克表示，他个人在使用OpenAI的 ChatGPT，但发现“有许多问题需要解决”。他承诺，新的人工智能功能将以“非常完善的基础”进入苹果的平台。</p><p>&nbsp;</p><p>之前的报道指出，一直以来，苹果在与多家大型人工智能公司进行谈判，以寻求在手机设备上集成聊天机器人功能方面的潜在合作伙伴关系。今年3月Mark Gurman就在一份新报告中称，苹果正在与谷歌讨论将Gemini引入iPhone。</p><p>&nbsp;</p><p>Gemini 于去年 12 月推出，是谷歌迄今为止最强大的大型语言人工智能模型，还在今年 2 月取代了 Google Bard版本。尽管刚开始发布时出现了一些严重问题，包括生成令人不安以及不准确的插图，但目前Gemini 是谷歌面向公众的重要人工智能项目。</p><p>&nbsp;</p><p>针对上述情况，现在苹果、OpenAI和谷歌的代表都拒绝置评。苹果尚未就使用哪些合作伙伴做出最终决定，也不能保证一定会与OpenAI和谷歌达成协议，也可能选择另外的人工智能供应商。</p><p>&nbsp;</p><p>在 ChatGPT 的迅速崛起以及随后来自谷歌、Microsoft 和 Meta 等公司的生成式 AI 工具和功能的激增期间，苹果研究人员也正在研究一种新模型 ReALM，可以为 Siri 提供苹果用户一直想要的生成式AI升级。据介绍，目前该模型可以比 ChatGPT 更好地理解在屏幕、对话和背景引用（如在后台运行的应用程序或功能）时的上下文问题。</p><p>&nbsp;</p><p>但对苹果来说，将生成式 AI 功能外包给合作伙伴，不仅将有助于加速苹果向聊天机器人的推进，还可以规避风险和减轻其平台的责任。</p><p>&nbsp;</p><p>无论苹果最终的计划是什么，可能都要等到 6 月举行的苹果全球开发者大会 （WWDC） 才能知道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://mashable.com/article/apple-openai-partnership-ios-18">https://mashable.com/article/apple-openai-partnership-ios-18</a>"</p><p><a href="https://www.bloomberg.com/news/articles/2024-04-26/apple-intensifies-talks-with-openai-for-iphone-generative-ai-features">https://www.bloomberg.com/news/articles/2024-04-26/apple-intensifies-talks-with-openai-for-iphone-generative-ai-features</a>"</p><p><a href="https://mashable.com/article/apple-google-gemini-iphone">https://mashable.com/article/apple-google-gemini-iphone</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xyGY9aIr0QrQ1IiWs8rP</id>
            <title>为什么谷歌也不敢发布这项技术</title>
            <link>https://www.infoq.cn/article/xyGY9aIr0QrQ1IiWs8rP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xyGY9aIr0QrQ1IiWs8rP</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 01:30:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 面部识别, PimEyes, 个人隐私, 技术滥用
<br>
<br>
总结: PimEyes 是一个在线面部识别工具，可以帮助用户在互联网上确认陌生人的身份。尽管声称有监控自身网络存在的作用，但引发了隐私和滥用争议。面部识别技术可能会被滥用，尤其在缺乏监管的情况下。 </div>
                        <hr>
                    
                    <p>想象一下，你在一条繁忙的城市街道上闲逛，顺手拍下一个陌生人的照片，并上传到搜索引擎，然后它几乎立即就可以帮助你确认这个人的身份。</p><p></p><p>这不是一个假设。现在，一个名为 PimEyes 的公共网站实现了这种可能。它被认为是最强大的在线面部识别工具之一。</p><p></p><p>在 TikTok 上，PimEyes 已经成为互联网侦探识别陌生人的强大工具。例如，有一个播放数百万次的视频演示了如何将 PimEyes 和其他搜索工具结合使用找出 Taylor Swift 演唱会上任意一位摄影师的名字。TikTok 的社区指南明令禁止包含可能导致跟踪、身份盗窃和其他犯罪的个人信息。但本文撰写时这个视频还在。</p><p></p><p>这个网站最初是由来自波兰的两名计算机程序员于 2017 年创建的，是一种和反向图像搜索类似的人工智能工具——它扫描照片中的人脸，并在互联网的黑暗角落里爬来爬去，然后显示许多人自己甚至都不知道其存在的照片，背景有餐馆，有音乐会。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd1e9d5b7008fdf93d4cc33190a89298.webp" /></p><p></p><p></p><blockquote>当把一张人脸照片上传到 PimEyes 时，这个搜索引擎就会生成结果。有些结果，比如这组照片中的最后一张，是一个与搜索无关的人。（来自 pimeyes.com）</blockquote><p></p><p></p><p>虽然该公司声称这项服务可以帮助人们监控自己在网络上的存在状态，但仍然引发了不小的争议，因为跟踪者以它为监控工具收集了无数儿童的照片，还在未经许可的情况下将死者的照片添加到其数据库中。</p><p></p><p>由于美国没有任何管理面部识别技术的联邦法律，所以可以预计，未来几年，模仿 PimEyes 的服务将会大幅增加。考虑一下，每个人都在公共场所随时使用这项技术会带来什么后果。</p><p></p><p>《纽约时报》记者 Kashmir Hill 最近出版了一本关于面部识别技术的书。她说道，“比如在火车上，你撞了别人，或者穿了一些令人尴尬的衣服，有人可能就会拍下你的照片，找出你是谁，然后发关于你的推文，或者叫出你的名字，或者在网上写一些关于你的坏话。”</p><p></p><h3>PimEyes CEO：这项服务有许多“合法用途”</h3><p></p><p></p><p>PimEyes 的基础版对任何人都是免费的，但该公司也提供了一些高级功能，比如当网上出现新照片时，它会提醒用户那可能是他们感兴趣的图像，但这项功能需要按月支付订阅费。</p><p></p><p>TikTok 用户指出，人们可以选择不让自己的照片出现在 PimEyes 数据库中，但对该搜索工具的测试表明，这并不能保证自己的照片一定可以从该公司的海量照片库中删除。</p><p></p><p>Giorgi Gobronidze 是一名来自东欧格鲁吉亚的人工智能研究学者，他现在是 PimEyes 的首席执行官。根据他的透露，该公司有大约 12 名员工。</p><p></p><p>在接受美国国家公共电台（NPR）采访时，Gobronidze 表示，该工具的滥用情况被夸大了。同时他还指出，该网站的检测工具只截获了数百起滥用该服务的案例，如跟踪或搜索儿童。</p><p></p><p>当人们用 PimEyes 搜索时，它并不会显示照片上的人的名字。尽管如此，把这些碎片信息拼接起来，确定某个人的身份，并不需要多少网络侦探工作。</p><p></p><p>Gobronidze 强调，从技术上讲，PimEyes 并不能单独生成某个人的身份信息。“我们不识别人的身份，我们只识别那些包含与搜索内容相似图片的网站。”PimEyes 明确要求，人们只能搜索自己或者是同意让其搜索的人。尽管如此，没有什么能阻止任何人在任何时候对其他人进行搜索。不过，Gobronidze 也说道，“人们并不像我们有时候想象得那么可怕。”</p><p></p><p>他还表示，“PimEyes 有许多合法的用途，比如保护自己免受诈骗。或者确定你或你的家人是否被身份窃贼盯上了。”但由于担心政府当局可能利用该服务针对抗议者和持不同政见者，目前，PimEyes 在部分国家屏蔽了该服务。</p><p></p><h3>为什么谷歌不敢发布这项技术</h3><p></p><p></p><p>《纽约时报》记者 Hill 表示，Meta 和谷歌等大型科技公司已经开发出了超强的面部搜索引擎。</p><p>然而，这一工具被武器化的可能性是如此之大，以至于一些高管，比如前谷歌首席执行官 Eric Schmidt，一直不愿将其推向世界。在快节奏、竞争激烈的硅谷，这几乎是不可想象的举动。</p><p></p><p>Hill 表示，“早在 2011 年，Eric Schmidt 就说过，这是谷歌已经开发完成但决定保留的一项技术，因为一旦它落入坏人手中，比如独裁者，就太危险了。”</p><p></p><p>这项技术有一些潜在的有益的用途。例如，当你忘了一个人的名字，它可以帮你快速识别，或者正如该公司所强调的那样，密切关注自己在网络上的照片。</p><p></p><p>但这项技术有可能损害公民的隐私。例如，政府和私营公司可以利用这项技术在公共场合对人们进行侧写或监视，这一点已经引起了研究该工具的隐私专家的警惕。</p><p></p><p>波士顿大学法学院专攻面部识别技术的教授 Woodrow Hartzog 认为，“这些好处只会被政府和行业拿来当借口，他们的目的只是为了扩大他们的权力和利益，而不会带来任何有意义的好处。所以，我根本不认为人类有了面部识别会比没有它时更好。”</p><p></p><h3>像 Apple Face ID 这样还可以，但不宜扩大</h3><p></p><p></p><p>当然，已经有一些版本的面部识别工具问世了，比如用苹果的 Face ID 解锁 iPhone。在机场，运输安全管理局可以通过面部扫描来确认某人的身份。</p><p></p><p>但是，面部搜索引擎将这个想法提升到了一个完全不同的高度。</p><p></p><p>在这方面，大型科技公司一直裹足不前，而推动这项技术的小型初创公司获得了良好的发展势头，比如 PimEyes 以及另一家名为 Clearview AI 的公司（为执法部门提供人工智能面部搜索引擎）。</p><p></p><p>Hartzog 说道，华盛顿需要在这些工具变得过于普及之前对其进行监管，甚至是彻底禁止。“这确实可以说明，面部识别有多大的放射性和腐蚀性，这也是为什么大型科技公司一直拒绝涉足这一领域，即使那可以赚很多钱。”</p><p></p><p></p><h3>面部识别搜索引擎会像AI 聊天机器人一样快速流行</h3><p></p><p></p><p>根据硅谷大多数观察人士的预测，这只是时间问题。</p><p></p><p>人工智能聊天机器人是一个有益的教训。多年来，硅谷的巨头们一直在实验室里开发强大的聊天机器人，只不过一直保密，直到一家规模较小的初创公司 OpenAI 将 ChatGPT 向公众开放。</p><p></p><p>科技分析师表示，为了保持竞争力，大型科技公司最终可能别无选择，只能公开先进的面部搜索引擎。</p><p>但 Hatzog 表示，他希望这样的未来永远不会到来。“如果面部识别技术得到广泛应用，那么我们将无处可藏，我们并没有真正地考虑过这一点。”</p><p></p><h3>“行走的条形码”</h3><p></p><p></p><p>在欧盟，立法者正在讨论在公共场所禁用面部识别技术。</p><p></p><p>布鲁塞尔活动人士 Ella Jakubowska 希望监管机构能更进一步，彻底禁止这些工具。</p><p></p><p>Jakubowska 发起了一项名为“要回你的脸”的活动，旨在警告人们，当你去看医生、在大学校园里散步，甚至过马路时，你的脸都会被扫描。在有些地方，那已经成为日常生活的一部分。</p><p></p><p>Jakubowska 说道，“我们在意大利看到了生物识别技术的应用，他们称之为’智能‘监控系统，用来检测是否有人在闲逛或擅自进入。”</p><p></p><p>Jakubowska 表示，欧盟所谓的人工智能法案将提出有关如何监管人脸、指纹和声音等生物识别数据的规定。“我们不接受这样的观点：把人类当作行走的条形码，即使我们没有做错任何事，政府也可以监视我们。”Jakubowska 说道。</p><p></p><p>与此同时，在美国的一些地方，比如伊利诺斯州，对于私营公司如何扫描和使用人们的面部信息，有专门的法律规定。未经同意扫描居民面部的公司将受到经济处罚。</p><p>但在联邦法规出台之前，私营公司记录人脸的方式和地点几乎不受限制，而且在很大程度上，决定权在开发这些工具的市值数十亿美元的科技公司手里。</p><p></p><p>原文链接：</p><p>https://www.npr.org/2023/10/11/1204822946/facial-recognition-search-engine-ai-pim-eyes-google</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zkePTCZZz5xS50c24Yt0</id>
            <title>历时5个月从零到一研发一款数据库产品，这些坑他们已经踩过了 ｜InfoQ独家专访百度智能云向量数据库团队</title>
            <link>https://www.infoq.cn/article/zkePTCZZz5xS50c24Yt0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zkePTCZZz5xS50c24Yt0</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Apr 2024 10:03:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 向量数据库, 大模型, 百度智能云
<br>
<br>
总结: 人工智能技术的发展推动了大规模预训练模型的应用，向量数据库在处理非结构化数据中起着重要作用。随着大模型的崛起，向量数据库受到资本关注，厂商纷纷推出产品。百度智能云推出专业向量数据库产品，解决大模型应用中的问题，满足企业级客户需求。向量数据库的发展趋势是结合向量和数据库能力，满足更多高级功能需求。 </div>
                        <hr>
                    
                    <p></p><blockquote>采访嘉宾｜百度数据库产品总架构师朱洁、百度数据库高级架构师郭波</blockquote><p></p><p>&nbsp;</p><p>生成式人工智能技术发展带动了大规模预训练模型的广泛应用，向量数据库成为了整个发展链条中的重要一环。人工智能和机器学习可以将非结构化数据（文本，图像，视频等）转换成数学上的向量表示。向量数据库正是一种专门用于存储和检索向量数据的数据库，向量数据库实现对向量的处理从而实现了非结构化数据的检索和相似性计算。对于大模型来说，向量数据库意味着更高效、更精准的模型应用。</p><p>&nbsp;</p><p>借着大模型崛起这股东风，众多向量数据库厂商也获得了资本的青睐。去年上半年，荷兰AI原生向量数据库厂商Weaviate获得5000万美元B轮融资；美国明星向量数据库厂商Pinecone宣布筹集了1亿美元的B轮融资。这些资本驻足的背后，是向量数据库的关注度已经达到了前所未有的高度。</p><p>&nbsp;</p><p>那么，向量数据库为何会受到如此高的关注？那就要从向量数据库对于大模型的助益来说起。</p><p>大语言模型存在知识更新不及时、会产生幻觉、无法具备特定行业或私有知识，以及难以实现安全回答等问题。通过引入向量存储模块作为大语言模型的长期记忆体，通过向量存储模块中数据的反馈和干预，能够以较低的成本解决上述问题。</p><p>&nbsp;</p><p>正是由于向量数据库在大模型应用中的显著优势，越来越多的厂商开始推出自家的向量数据库产品。</p><p>&nbsp;</p><p>1月底，百度智能云推出了一款面向企业级市场的专业向量数据库产品&nbsp;VectorDB（简称&nbsp;VDB）1.0版本。这款VectorDB采用了全新设计的数据库内核，能够支持百亿级弹性伸缩，相比同类开源产品，VectorDB 1.0的QPS性能在不同场景下能够提升1倍到10倍不等。</p><p>&nbsp;</p><p>那么，百度智能云为何选择在这样一个时间点来推出这款专用向量数据库？专用向量数据库的需求是真实存还是市场泡沫？在研发一款专用向量数据库时百度智能云解决了哪些技术上的难题？</p><p>&nbsp;</p><p>近日，有幸独家专访了来自百度向量数据库研发团队的百度数据库产品总架构师朱洁和百度数据库高级架构师郭波揭秘这款向量数据库研发背后的故事。</p><p></p><h2>百度智能云向量数据库研发技术实践</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：我了解过之前百度大模型（文心一言）使用的是底层数据库和搜索引擎的方式（ES+FAISS），那时这样的搭配似乎就已经可以满足大模型需求了，为什么还要专门去研发一款向量数据库数据库？是否意味着以前的解决方案目前已经无法支撑业务发展了？</blockquote><p></p><p>&nbsp;</p><p>郭波：去年BES团队对外介绍过，文心一言当时确实使用的是百度的ES（简称BES），但在向量检索这块，使用的并不是FAISS库，而是BES团队利用C++语言基于开源HNSW算法进行优化以及据此研发的专用向量检索。</p><p>&nbsp;</p><p>我认为大模型的应用场景不局限于这类应用，还有更多场景可以利用大模型来实现，例如知识库管理和RAG等场景。这些场景除了需要强大的检索能力，也需要数据库相关的能力。尤其当我们的产品面向B端客户，特别是大型B端客户时，向量检索能力只是其中一个诉求，还有许多与数据库相关的高级功能也同样重要。&nbsp;</p><p>&nbsp;</p><p>市场上有非常多的开源产品，具备一定的向量相关能力，向量检索算法和检索库非常丰富，这些都很容易被获取和应用。在我看来，未来向量数据库的竞争力不仅仅在于“向量”二字，更在于“数据库”这三个字。向量数据库不仅包含向量相关的能力，更需要包含数据库的功能。去年我们观察到，市场上的向量数据库大多重视“向量”而忽视“数据库”。对于这类向量数据库而言，它们可能只重视接入开源的向量能力，忽视了数据库相关能力，尤其是一些高级能力的建设。短期内，这类向量数据库产品可能能够服务一些规模较小且需求宽松的客户，但从中长期来看，它们可能难以满足大规模且要求非常严格的B端客户的需求。去年，我们在充分调研之后认为，百度有必要立项研发一款专业的向量数据库，这款数据库不仅要做好“向量”，更要做好“数据库”本身，立足于长远发展。</p><p>&nbsp;</p><p>朱洁：我想补充一下关于郭波提到的关于“数据库”三个字的具体含义。在业务发展到现在这个阶段，我们遇到了很多实际客户，他们确实有这样的需求。这不仅仅是指向量检索能力，还包括对更多数据类型的支持和处理，包括数据库的一些ToB的高级能力，包括对高可靠、高可用、低成本、易用性等看起来老生常谈但是确实又非常关键的能力等等。&nbsp;我们可以举几个例子。</p><p>&nbsp;</p><p>首先，为了解决企业用户的文档管理问题，企业有很多安全方面的需求。比如，如何实现多租户隔离，确保不同职能部门具有相应的不同权限，以及如何进行敏感数据的访问审计。其次，我们有客户提出了异地多活的需求，这就是典型地对高可用和高可靠的能力的诉求，这种能力还可以在一些更深入的场景中被使用。目前市场上的向量数据库大多只提供简单的向量检索能力，在其他方面却比较欠缺。从长远来看，包括我们现在实际接触到的客户，这些能力都是不可或缺的。我们认为，专业的、面向企业级客户的这些定位对向量数据库而言是非常有必要的，这就要求数据库不仅要具备向量检索相关的能力，更要能够满足企业级用户在数据类型支持、灵活性、接口易用性、安全特性、多租户隔离、访问审计以及异地多活等方面的需求。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：您和团队在研发这款数据库时将其定为于面向企业级客户的解决方案，是不是一定要使用纯粹的专用向量数据库才能真正地解决技术问题吗？</blockquote><p></p><p>&nbsp;</p><p>郭波：我的回答是“是的”。我们可以对市面上常见的向量数据库类型进行简单的阐述。</p><p>&nbsp;</p><p>关系型数据库如MySQL和PostgreSQL是非常经典的数据库，这类数据库在其特定的场景下表现出色，但在数据存储组织上并不适合向量数据，也缺乏专门优化的向量检索引擎，接口形态上也缺乏API等现代化形式，同时，在面向大规模数据的分布式管理能力方面也所不足。从这些分析来看，我认为它们并不适合用来作为向量数据库来承接相关场景。</p><p>&nbsp;</p><p>非关系型数据库如MongoDB和ES等，虽然它们在某些方面比传统关系型数据库会更加合适一点，然而对于百度来说，仍然存在一些因素导致我们不能依赖这类数据库。众所周知，MongoDB和Elasticsearch的许可证已经变更为SSPL，在某个版本之后，我们无法再依赖它们来构建产品。另一方面，这类系统，在分布式能力上不及我们的预期，对向量场景也缺乏相应的优化（例如内存有效利用率偏低，大规模场景下导致成本偏高等），长期来看也不合适。</p><p>&nbsp;</p><p>因此，从百度智能云客户及自身的各类诉求出发，确实需要自主研发一个适合我们的客户需求的向量数据库。既然我们决定进行自主研发，就必须要针对性地对整个数据库进行系统性工程优化，确保无论是在分布式层面、存储引擎、检索引擎，还是向量相关能力等方面，都能达到一个理想的状态。</p><p></p><p></p><blockquote>InfoQ：&nbsp;那这款向量数据库，包括检索引擎和传统数据库部门是完全从零开始自主研发的吗？</blockquote><p></p><p>&nbsp;</p><p>郭波：之前提到的HNSW算法等，这些是基于开源的。其他所有东西，包括百度自研的Puck算法都是我们自己或百度其他团队研发的。我们数据库团队自研的部分涵盖从存储格式、存储引擎、检索引擎、Schema以及数据类型、分布式架构、再到产品的管控层，以及控制台和前端等等。</p><p>&nbsp;</p><p>关于检索引擎，并不是说有一个HNSW算法，就可以直接提供向量检索服务了，实际上，我们需要在这些算法和存储引擎之上，与数据库系统的其它组件进行融合，构建出检索引擎，才能对外提供检索服务。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：这款数据库从立项到发布历时多久？我们投入了多少人力物力来做这个事情？</blockquote><p></p><p>&nbsp;</p><p>郭波：去年我们经过充分调研之后，认为确实有必要自主研发一款向量数据库。同时，从人才储备的角度来看的话，我们也有能力做到。因此，8月份我们开始了项目的立项工作以及系统的设计工作，到了9月中旬，我们进入了紧张的研发阶段。到今年1月底，我们完成了第一个版本的开发和测试，并成功上线公测。</p><p>&nbsp;</p><p>我们的团队由来自不同背景的成员构成，主要来自于云存储和数据库两个部门，团队中既有精通分布式存储引擎的专家，也有非常了解数据库产品研发的开发人员，是一个人才组合很平衡的团队。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：要将两个系统（数据库和检索引擎）融合在一起，你们内部做了哪些优化和迭代？</blockquote><p></p><p>&nbsp;</p><p>郭波：我们系统和产品是全新研发的，在设计阶段就原生地赋予了系统较强的向量能力，而不是在已有的系统中事后添加。&nbsp;在已有的系统中，事后添加向量插件，会导致难以针对向量特性进行优化。</p><p>&nbsp;</p><p>例如，在存储方面，我们认为列存引擎比传统的行存引擎更加适合向量数据的管理，为此专门研发了列存引擎。我们也充分利用了指令集和编译器优化，提高性能。在工程层面，我们尽量降低非浮点计算的额外开销，提高资源效率；尽力优化内存开销，提升内存有效利用率等。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：工程方面的一些优化具体包括哪些方面？</blockquote><p></p><p>&nbsp;</p><p>郭波：我以底层数据组织方式来举个例子，我们认为，列存引擎可能比行存引擎更适合向量数据。假设一条数据包含多个向量字段，而这些字段又来自不同的原始内容，并且可能使用了不同的embedding模型。 在这种情况下，如果要为这些字段的数据建立索引，需要分别处理，甚至需要对不同字段建立不同类型的向量索引，退一步来说，即使采用相同的索引方法，其索引参数也可能不同。因此，使用列存引擎可以更好地将不同向量字段的数据进行隔离，从而使得系统能够更精细地处理它们。例如，在构建索引和向量检索的过程中都只需要访问与目标字段相关的原始数据和索引数据，而不用访问其它字段的相关数据，这极大地避免了额外的资源开销。这就是一种典型的针对向量场景的优化措施，而且是从最底层的数据存储组织上开始的优化。所以可以看出，我们会根据向量数据和场景本身的特点，自底向上，在各个层面都执行针对性的优化，以确保充分利用资源，发挥性能优势。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：能分享一些性能方面的数据，包括一些测评数据？</blockquote><p></p><p>&nbsp;</p><p>郭波：我们最近完成了性能测试和Benchmark的建设工作。我们完成了与某款知名开源向量数据库的详细性能对比。总体来说，我们的性能表现明显更加出色。我总结了两点：</p><p>第一，&nbsp;我们在128维、768维和960维三种数据集上都进行了测试，在所有数据集场景下，在相同的召回率下，我们数据库的检索性能都大幅超越了开源系统。在一些场景中，性能提升接近一倍，而在一些其他的场景中最大能有十倍的提升，这个提升可谓相当明显。</p><p>第二，&nbsp;从召回率的视角来看，当我们放宽召回率要求时，比如从99%降到95%，再降到90%时，我们数据库的检索性能够实现显著的线性提升。</p><p></p><h2>向量检索及内存引擎的设计和研发是两大棘手难题</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：研发一款数据库并非易事，您和团队在研发过程中遇到了哪些技术挑战，最终又是如何解决的？有哪些值得借鉴的技术解决方案吗？在这个过程里，有哪些里程碑的时刻吗？</blockquote><p></p><p>&nbsp;</p><p>郭波：客观来说，肯定遇到了一些技术挑战。其中一个大的挑战是存储引擎，尤其是列存引擎的研发。在综合考察了开源社区、友商等方案后，我们发现百度智能云内部的一款自研的列式存储格式库最符合向量场景，该格式库支持列存、列式压缩、KV分离等关键特性，是非常好的引擎基础。但它仅仅只是一个格式库，还不是一个完整的引擎，没有具备像Schema体系、内存表、快照、Compaction、查询优化、异常恢复等等这些引擎层面的关键特性，这就需要我们自己在此基础上继续进行研发。面对这样的挑战，第一步，我们快速借调了一些比较懂KV引擎的同学，然后大家一起合作，将大问题拆解为一系列的小问题，然后来回讨论，逐步解决这些问题，在解决了全部问题之后，做好各个子模块的接口定义和层次划分，然后高效完成研发，达成目标。</p><p>&nbsp;</p><p>另一个大的挑战是向量检索引擎的设计和研发。这包括两个方面：第一，将我们的检索库与存储引擎进行对接；第二，在对接的基础上构建完整的检索引擎。我们从开源社区仅能够获得基础的检索库，要将检索库与后端进行对接需要做很多的改造，包括Schema、存储组织、索引组织、标量向量混合检索机制等方面的工作。此外，在构建检索引擎时，我们还需要考虑其通用性，进行抽象设计，以支持各类不同的向量索引和检索方法，从而针对不同的算法、参数和场景构建出一个统一的检索引擎。以上这些工作的效果决定了整个检索机制的性能和灵活性。&nbsp;面对这个挑战，我们首先让团队成员相互了解对方的工作，包括具体的任务和设计背后的考量。然后通过不断迭代确定方案并最终解决问题。</p><p>&nbsp;</p><p>在应对这些技术挑战的过程中，我们发现，培养或发掘团队中善于构造异常场景的人才是非常有效果的做法。因为这类人才非常善于针对设计方案来推理和构造各种可能的异常场景，然后反过来用这些场景去挑战设计方案，来检验方案的完整性和健壮性。通过这种方式，我们能够快速地进行设计的迭代，加速研发和问题解决的过程。</p><p>&nbsp;</p><p>我们的里程碑大概是这样的：10月底，存储引擎完成了基本功能并能够正常运行；11月底，检索引擎层全部打通，可以跑向量检索；到了12月初，可以跑基本的性能测试，当时虽然是第一轮性能测试，但性能表现已经超越知名开源系统。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：这两个系统分别用了什么语言？功能迭代和测试是如何进行的？</blockquote><p></p><p>&nbsp;</p><p>郭波：开发用的是C++ 17版本。</p><p>&nbsp;</p><p>我们的测试主要分为两个大方向，一个是云产品层面的测试，另一个则是更底层的数据库内核层面的测试。在数据库内核层面，测试工作又进一步分为功能测试和混沌测试。功能测试关注的是功能和接口的正确性，确保系统能够按照预期正确响应用户请求。混沌测试，是针对分布式系统的健壮性，或者说高可用高可靠等特性的测试机制。在这个测试中，我们会尝试引入各种异常情况，包括硬件异常、网络异常、资源异常，以及节点宕机等，模拟出非常恶劣的运行环境。如果在这种环境下，我们的数据库内核系统仍能够持续稳定地运行下去，那就说明系统具备了非常强的异常环境容错能力。</p><p>&nbsp;</p><p>在数据库内核层面专门进行这样的混沌测试，也是源自于百度分布式存储领域的优良传统。在百度智能云内部，所有的分布式存储系统都要通过这样的测试，而且是将这类测试作为例行化测试的一部分。我本人此前做了多年的分布式存储，因此在带队开发向量数据库时，自然也将这个优良传统给继承过来了。专业的混沌测试不仅可以验证系统的稳定性和可靠性，还能够确保系统在面对各种环境挑战时，能够保持更加稳定的性能。通过这些严格的测试，我们尽最大努力高标准满足客户对于云服务的稳定性期望。除了混沌测试之外，我们这边还有一个优良传统，即对分布式系统进行形式化验证，从而能够在早期发现分布式逻辑层面的问题，我们对向量数据库内核也进行了形式化验证。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：这款数据库是否支持多模态？整体性能怎么样？</blockquote><p></p><p>&nbsp;</p><p>朱洁：关于多模态，我们认为在向量数据库层面更多地是提供相应的数据库能力以支持多模态，但并不直接负责处理多模态，多模态的处理更多是在产品层面或者解决方案层面端到端进行的。例如，我们的向量数据库支持非常丰富的数据类型，能够支持将不同模态的原始数据，以及它们对应的向量数据，关联性地进行存储和管理，比如将它们都存储到同一行中。那么未来在向量检索时，可以将目标向量相关的各种模态的原始数据信息一并召回出来，方便更上层的机制来进行处理。</p><p>&nbsp;</p><p>另一方面，如果期望能够从端到端的角度处理多模态数据，比如文本、图片、视频等，需要在前端具备embedding的能力，需要有相应的模型来帮助业务进行embedding。 目前，在产品层面，我们会接入百度智能云千帆大模型，会提供High-Level&nbsp;SDK，通过调用千帆的能力来实现文本embedding和图片embedding等，然后通过调用向量数据库实现这些数据的关联存储和召回。</p><p>&nbsp;</p><p>所以从上面这些角度来看，我们是端到端地支持多模态的。我们会进一步丰富High-Level&nbsp;SDK等工具套件，让用户能够更加方便地处理多模态，或者构建一些AI工作流。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：多模态的支持程度取决于端到端的处理能力。在我们的端到端解决方案中，我们采用了将千帆的embedding能力集成进来，用于处理图片、视频和文本等数据。这是通过引入大模型的能力来实现embedding的过程。 目前，这些能力已经在生产环境中得到应用了吗？有哪些案例可以分享？</blockquote><p></p><p>&nbsp;</p><p>朱洁：目前，多模态领域仍处于发展之中，例如最近爆火的Sora技术也是刚刚起步。在多模态方面，虽然我们还没有看到业界的非常强大的案例，但确实有一些客户，例如自动驾驶企业，正在与我们合作进行测试多模态支持。自动驾驶企业需要基于图片的内容语义，对图片进行管理和处理。还有一些娱乐文化领域的企业，他们拥有大量图片和视频资源，而且这些资源还具备相应的文本信息，是非常直接的多模态场景，他们也在构建相应的解决方案。虽然目前还没有看到哪个多模态场景已经深度落地，但我相信这个领域的繁荣发展将来一定会催生一些多模态相关的AI原生应用。</p><p></p><h2>RAG正在取代向量数据库？</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：2023年之前，向量数据库还是一个偏冷门的领域，但是大模型爆火之后，向量数据库也随之受到VC的追逐热捧，一度成为热门赛道。但在2023年后半年，RAG技术似乎盖过了向量数据库的风头，社区内对此争议不断。在您看来RAG和向量数据库的区别是什么？是不是RAG就可以替代向量数据库了？</blockquote><p></p><p>&nbsp;</p><p>朱洁：RAG并不是一个新概念，而是由OpenAI重新带火的一种技术。RAG并不是与向量数据库在同一层次，而可以被视为一种技术或解决方案。最初，RAG的概念起源于搜索引擎领域，结合了企业内部搜索的能力来增强搜索功能。&nbsp;</p><p>&nbsp;</p><p>OpenAI最初是将向量数据库作为外部工具来使用。后来，为了简化用户使用，他们在系统内部开发了一个插件，使用户可以更容易地上传文档数据，形成了一个简化版的RAG系统。这样做的好处是，使用OpenAI后，它可以帮助你处理文档并创建demo，特别适合小型团队快速制作产品原型。 然而，这种插件式的应用有其局限性，例如限制上传文档的数量和缺乏权限管理机制，无法满足企业级应用的需求。此外，其价格昂贵，且在文档处理和索引定制方面存在限制。&nbsp;</p><p>&nbsp;</p><p>从长期来看，我们认为大型企业和机构的数据，特别是那些大模型无法获取的内部生产研发数据，才是RAG系统真正发挥作用的地方。因为这些数据通常具有严格的权限管理需求，而且大模型无法获取这些数据进行训练。&nbsp;</p><p>&nbsp;</p><p>未来，我们相信真正有长期发展潜力的将是大模型与企业级向量数据库的结合。企业级向量数据库需要具备更强的扩展能力、全面的安全权限管理、分布式性能以及低成本，以满足企业级客户的需求。随着大模型的不断进化，它们能够获取和学习的信息越来越多，但在企业内部独特的数据面前，RAG系统仍然有其价值。OpenAI推广的RAG概念虽然有其价值，但真正有意义的应用场景还是在企业级数据中。只有当企业拥有自己独特的数据时，RAG系统才有必要存在。</p><p>&nbsp;</p><p></p><h2>向量数据库研发团队背后的故事</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：我们这款数据库有开源计划吗？</blockquote><p></p><p>&nbsp;</p><p>朱洁：目前，我们并没有急于讨论开源这个问题。大模型和向量数据库还处在非常快的发展阶段，我们认为最核心的任务还是持续去构建更强大的产品能力，这是首要的事情。</p><p>开源的一个核心作用是让更多人能低门槛使用产品。这一点通过云上或其他手段已经能够做到。目前，我们已经在云上免费提供给大家使用，欢迎大家直接登录官网来使用VectorDB。未来，我们可能会考虑让客户在他们自己的环境里使用我们的产品，这是我们未来规划中的一部分。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：我想问一个关于团队方向的问题，在整个研发过程中，团队成员的工作状态是怎样的，是否有另两位感觉印象比较深的时刻？</blockquote><p></p><p>&nbsp;</p><p>郭波：在整个研发过程中，我们对团队成员的工作状态确实有一些非常深刻的感受。首先，团队整体士气非常地积极和自信，包括我自己也是这样的。大家都认为，有机会参与这样一个新颖的项目是非常令人兴奋的，同时大家也都能感受到项目的紧迫性，非常投入地工作，努力保持项目进度，尽管工作强度大，但并没有多少抱怨。&nbsp;</p><p>&nbsp;</p><p>其次，团队成员对不同方面的技术方案的理解能力和接受度都非常好。团队中有些成员擅长分布式系统，而有些则擅长存储引擎，还有些则专注于向量索引和检索，另一些则具备丰富的产品管控和控制台研发经验。大家都积极地去了解和学习彼此擅长的领域，这使得每个人都能更深入地理解整个系统和产品。这样的团队合作精神极大地提高了我们在遇到问题时的解决效率。</p><p>&nbsp;</p><p>朱洁：非常认同郭波刚才所讲的内容，团队的所有同学都非常认真负责。首先，从我自己的观察来看，从上到下，大家都对这个项目感到非常兴奋。因为大模型是云计算和人工智能领域未来数年中能够看到的唯一巨大的机会，这个机会带来了向量数据库领域的巨大发展机遇，无论是产品团队还是技术团队，都对能参与其中感到非常激动，也非常珍惜这个机会。&nbsp;</p><p>&nbsp;</p><p>其次，目前我们的产品已经上线公测，从实际的情况来看，市场反馈非常积极。在非常短的时间我们就有大量的用户开通了免费集群进行使用和测试，而且每周这个数据在呈现一个非常快速的增长。在使用之后，他们也会提出各种各样新的需求，这给我们带来了非常正面的反馈。我们看到用户数量的快速增长，受到大家的认可，以及收到各种不同的问题和需求的反馈，都是非常令人兴奋的事情，对我们而言，这些都非常有价值。</p><p></p><h2>未来展望</h2><p></p><p>&nbsp;</p><p></p><blockquote>InfoQ：两位老师认为未来市场上对于对向量的需求会持续上升吗？为什么？</blockquote><p></p><p>&nbsp;</p><p>朱洁：预测未来的应用场景是非常困难的，从本源出发，一个企业拥有哪些数据，这些数据将决定未来可能出现的应用场景。就像滴滴打车的出现，本质上是因为手机中的位置数据。有了这样的核心数据，就有了创新的空间。企业的创新能力取决于它拥有的数据类型。如果一个企业只有办公数据，那么它可能孵化出与办公相关的应用。如果企业拥有自动驾驶相关的图片数据，那么它可能会孵化出自动驾驶相关的应用。因此，一个企业能够使用向量数据库孵化出什么样的应用，取决于它拥有的数据。&nbsp;</p><p>&nbsp;</p><p>各行各业的企业拥有不同类型的数据，这也是我们认为大模型可能会重构当前所有系统的原因。向量数据库无疑会对现有的系统，如办公自动化、自动驾驶图片管理、销售流程、ERP，甚至财务系统，起到增强和重构的作用。&nbsp;</p><p>&nbsp;</p><p>另一方面，未来可能会出现我们现在甚至没有想到的AI原生应用。就像移动手机的位置数据催生了新业务一样，未来可能会有更多的智能体出现，它们需要知识，这些知识可能来自大模型，也可能是企业内部的专有知识。因此，企业的数据将是这些智能体知识的重要来源。 总的来说，向量数据库将在未来的企业和AI应用中发挥关键作用，无论是增强现有系统，还是孵化全新的AI原生应用。</p><p>&nbsp;</p><p>郭波：目前，我们的大量文档类数据仍然以pdf、doc、ppt等文件形式存在，它们存储在个人电脑中，或者存储在云上的对象存储和文件存储中，由于是非结构化数据形态，这些文档的内容并没有得到很好的挖掘和管理。&nbsp;造成这种现象的核心原因，并非是文档内容缺乏价值，恰恰相反，我们认为内容非常有价值，只是此前的技术难以实现其价值发现。</p><p>&nbsp;</p><p>因此，当大模型技术突破之后，足够在语义层面对文档内容进行深度挖掘和管理，补齐了文档内容管理的最后一个短板，大家又重新开始重视文档内容的价值。我们可以从各类文件中提取内容，进行各种精细地处理，包括解析、分割、向量化、关键词索引、语义索引、提取元数据、甚至根据语义或者多模态技术构建不同内容之间的关系等等。然后将所有这些信息存储在向量数据库中，并基于这些信息进行各种复杂的检索或处理，支撑众多的内容类应用，例如目前非常火的RAG等。</p><p>&nbsp;</p><p>这样，各类企业就能够释放非结构化文件内容的价值，为存量业务提效，甚至寻找新的业务机会。我们认为，在未来，文档管理的主阵地有很大的可能从非结构化的对象存储转移到半结构化的数据库系统中，尤其是向量数据库系统。这是我们所畅想的新场景，我们认为这里面存在着巨大的机会。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：未来团队还会还会在哪些方向上进行深入研究？方便透露下技术路线吗？</blockquote><p></p><p>&nbsp;</p><p>郭波：今年我们将重点关注三个方向。第一个方向是RAG。正如上一个问题所指出的那样，我们认为RAG是一个非常重要的方向，为了支持好RAG，向量数据库除了需要支持好向量检索，还需要支持好全文检索、多路检索召回和融合排序等关键技术，这些技术的研发和优化，是我们今年的关键攻关点之一。&nbsp;</p><p>&nbsp;</p><p>第二个方向是，更高效，更低成本的能力。从内外部客户的情况来看，有一些客户对成本非常敏感，尤其是在当前这个降本增效的时代，一些企业期望进行相关的尝试，又难以提供很高的预算。为了服务好这类成本敏感性客户，我们今年会重点发力低成本的向量索引和检索机制。进一步地，我们还会尝试研究将一些智能化能力引入到这些索引和检索技术中，进一步提升效价比。</p><p>&nbsp;</p><p>第三个方向是，立足于向量数据库的AI应用套件或框架。我们会以RAG和多模态为突破口，提供全流程的工作流套件，大大降低这类应用的开发难度，降低开发门槛，提升开发效率。</p><p>&nbsp;</p><p>朱洁：我们将会特别关注内外部生态的融合，比如和百度内部大模型生态的充分融合，包括将我们的技术应用到百度的各种服务中，如百度网盘、文库等。我们的目标是通过这种融合和应用，产品得到进一步深度的打磨，这样也能帮助企业更好地利用我们的技术。这也是百度的一个核心优势。我相信，除了技术之外，我们强大的生态系统是我们的数据库产品能够成功的关键因素。这是非常重要的。</p><p>&nbsp;</p><p></p><blockquote>InfoQ：您认为未来向量数据库的发展趋势是什么？</blockquote><p></p><p>&nbsp;</p><p>郭波：从技术上看，未来向量数据会持续在如下几个方向上发力：1、持续夯实大规模、高效率、低成本、高性能、高可用和高可靠等基础能力，构建不同风格的产品形态，服务不同诉求的客户；2、各类AI原生应用及场景所依赖的关键技术，例如提供并优化各类复杂索引和检索的能力，持续提升检索效率和效果，提供文档内容管理和知识库构建所需要的各类存储索引等能力，甚至是在数据模型层面进行深度创新来解决问题；3、丰富的企业级特性，满足企业级客户在二次开发、成本、效果、安全、审计、运管、以及融入复杂的存量业务架构的能力；4、深度融入大模型生态，为客户提供好用易用的端到端工具、套件以及解决方案，推动更多客户以更容易的方式触达大模型技术生态；5、更长远来看，还可以向上构建基于语义的内容管理平台，推动内容管理进入AI原生时代。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/YyANmZXMp9he9a01O3U0</id>
            <title>盘点A股五大保险公司的AI布局与数字化进展</title>
            <link>https://www.infoq.cn/article/YyANmZXMp9he9a01O3U0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YyANmZXMp9he9a01O3U0</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Apr 2024 08:29:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 居民收入增速, 保险业数字化转型, 大数据与人工智能, 智能理赔
<br>
<br>
总结: 随着居民收入增速回升、消费信心持续改善、社会保障能力增强，我国保险业积极推进供给侧改革与产品创新，保险业数字化转型进入新阶段，智能理赔和大数据与人工智能技术的应用成为关键发展方向。 </div>
                        <hr>
                    
                    <p>随着居民收入增速回升、消费信心持续改善、社会保障能力增强，我国保险业积极推进供给侧改革与产品创新，以销售为中心的传统保险商业模式已略显增长疲态，保费增速放缓放大了保险业提质增效的发展需求。在市场环境深刻变化的今天，保险业应该如何在保证自身高质量发展、完成转型升级的同时，助力实体经济质效提升？</p><p></p><p>据艾瑞咨询数据，我国保险业 IT 投入规模将持续稳定增长，并有望在 2023 年达到 569 亿元。过去一年里，在 AI 大模型等新兴技术的加持下，国内保险科技迎来了又一波发展热潮。</p><p></p><p>原保监会副主席周延礼认为：“大模型技术发展之快带来的创新价值前所未有。与数据收集、计算、交换相关的技术目前被广泛认为是增强金融业安全性、协同性以及提高效率、及时性的技术。”保险公司纷纷把科技创新加入工作待办清单，不仅是为响应政府工作报告“五篇大文章”的政策号召，更是推动保险业向更智能高效、用户友好的方向发展的必然要求。</p><p></p><p>借中国人保、中国人寿、中国平安、中国太保和新华保险等五家 A 股上市保险公司 2023 年年报的发布之际，我们进行了关于这些公司数字化转型和 AI 应用的综合盘点。</p><p></p><h2>新技术不只停留在营销场景，已逐步走进核心业务场景</h2><p></p><p></p><p>中国人民银行《金融科技发展规划（2022-2025 年）》提出：“要健全安全高效的金融科技创新体系，搭建业务、技术、数据融合联动的一体化运营中台。”保险业数字化转型进入新阶段，不能再将技术、数据简单堆砌在业务场景里或一味追求技术革新，而应当以业务经营为核心，深挖合适的数据、使用合适的技术去解决痛点。</p><p></p><p>中国人保在战略层面强调了这一理念，提出要把科技作为基础支撑，加快推进“保险 + 服务 + 科技”商业模式新变革，在财险领域进行“保险 + 风险减量服务 + 科技”实践，而在人身险领域构建“保险 + 养老健康服务 + 科技”模式。其余各个险企也均在年报工作总结中提及了相似的理念。</p><p></p><h4>（一）数字化产品继续帮助传统销售模式更新迭代</h4><p></p><p></p><p>自疫情以来，险企商业模式虽仍围绕销售展开，但却迅速由依靠线下人员 1 对 1 销售或传统电销转向了线上多渠道数字营销，营销场景变得无处不在。线上用户的肖像特点也正在向年轻、个性多元、泛娱乐化转变，短视频、私域流量等玩法开始盛行。大多数险企都抓住了这一风口，通过多种社媒打造品牌形象、研发手机 APP 形成社区等已经变得老生常谈，但要让玩法和技术不只停留在表面，还需要不断完善。</p><p></p><p>中国平安在这一方面起步很早，目前已打造了平安金管家、平安口袋银行、平安好车主、平安健康等多个亿级用户 APP。值得一提的是平安通过家庭医生和养老管家为客户建立专属的健康档案，提供全天候的医疗健康和会员制的养老服务，通过与 AI 相结合实现 7×24 小时秒级管理。截至 2023 年 12 月 31 日，平安内外部医生团队约 5 万人，合作医院数超 3.6 万家，已实现国内百强医院和三甲医院 100% 合作覆盖。</p><p></p><p>太保寿险推出轻量级远程视频交互技术“芯双录”，实现销售服务人员与客户“云会面”，提升客户投保体验；在亚运会期间深化“保险 + 体育 + 健康 + 服务”营销模式，“太保蓝之队”互动体验宣传获亚组委评为最佳营销案例。</p><p></p><h4>（二）智能理赔</h4><p></p><p></p><p>元保联合清华大学五道口金融学院中国保险与养老金研究中心 4 月 23 日发布的《2023 年中国互联网保险消费者洞察报告》调研发现，当前令保险行业和消费者最困扰的问题是“担心理赔困难或被拒”，而智能化理赔能够帮助“提升购险和售后体验”。</p><p></p><p>平安人寿和平安产险的“智能理赔”系统已经落地使用，通过计算机视觉、图像和语音识别等技术实现理赔全流程自动化，帮助平安人寿服务运营线上化水平达到 99% 以上、投保及服务自动化水平在 90% 以上，平安产险全年 300 万客户线上自助完成投保，定损流程从拍照到完成缩短到 30 分钟以内。</p><p></p><p>作为以寿险为核心的保险企业，新华保险则着重在简化理赔程序、打通医院系统和保险系统联通路径上下功夫，不断扩大理赔直付业务的覆盖范围，改变事后理赔传统模式，实现客户将理赔材料同步至系统后即可当场结算、理赔。</p><p></p><h2>如何探索新模式：大数据和 AI 仍是最受关注的发展方向</h2><p></p><p></p><p>保险业作为数据密集型行业，其产品研发、保费测算、营销运营、核保理赔等业务全线涉及了海量复杂数据的治理问题，因此 AI、大模型与保险业天然契合。</p><p></p><p>国内保险公司在 AI 大模型方面的探索也开始得很早，比如中国平安在 2017 年就将 AI 应用于人脸识别、声纹识别、流感预测等场景，人工智能对保险业而言并不是什么新奇的词汇。艾瑞咨询调研数据显示，2023 年大数据与人工智能技术的持续攻坚与相互赋能仍然是保险机构决策者最关注的实践内容。</p><p></p><p>中国人保开发了自主控制的人保大模型，并在代理人赋能、智能客服等领域进行了试点应用；基于星火金融大模型底座，经过私有化训练调优形成了专属企业大模型“数智灵犀 - 人保大模型”，利用其深度语义解析和先进搜索能力快速获取知识，实现降本增效。为完善这一专属加垂直领域的大模型生态，中国人保还在此基础上配套建设了人保 prompt 工厂、博文智库、智选路由、信息安全助手等组件。</p><p></p><p>中国人寿充分发挥各成员公司的场景化能力，采用不同方式在各个业务线建设大模型底座。寿险公司与大模型厂商合作，分阶段建设自主可控的大模型，正开展产品测试与框架搭建；财险公司整合数据、构建知识库，通过私有化部署和租赁方式探索大模型应用；资产公司探索大模型在信用领域应用，实现智能问答与数据挖掘；国有投资公司建立大模型中台并实现在多个办公场景的功能落地。</p><p></p><p>中国平安旗下的金融壹账通也已布局大模型和生成式人工智能（AIGC），并在银行、保险、投资等金融垂直领域落地应用。</p><p></p><p>新华保险自主设计、研发的人工智能客服机器人“智多新”，依托于自然语言的处理、语音识别、自主学习，已经具备智能互动咨询、智能外呼、电子化回访、续期交费提醒、智能外呼服务等多项能力，支持 37 万个智能服务场景、拥有 70 余项智能工具，包含全天候服务、智能服务场景、智多新工具箱、绩优、VIP 识别、畅聊有趣话题五大功能，并升级了热点问题库、服务网点查询、短信版三大服务功能板块。</p><p></p><h2>从 RPA 到 AI Agent，数字员工逐步规模化落地</h2><p></p><p></p><p>大语言模型爆红之初，关于 RPA（机器人流程自动化）是否即将被取代的讨论声不绝于耳。当前 RPA 系统更多应用于处理规则和结构化数据，对于非结构化数据和复杂决策过程的作用有限。而 LLM 更擅长处理非结构化的数据，经过训练后具有很强的语言理解、内容生成、推理和决策能力，且不需要像 RPA 那样针对不同流程需求进行个性化开发。然而在 LLM 真正发展为 AI Agent 并规模化应用之前，RPA+AI 构建数字员工似乎是更高效也更现实的方式。</p><p></p><p>人保财险将 RPA 与 OCR（光学字符识别）、自然语言处理、知识图谱等 AI 技术融合，打造智慧数字员工。通过与 OCR 技术融合，实现承保、核保、理赔等业务流程中单证票据信息的自动识别和录入，串联了原本需要人工参与的环节，弥合流程断点，将 RPA 的应用能力扩展到包含非结构化数据的场景。通过和自然语言处理和知识图谱的结合，使数字员工具备智能问答能力。</p><p></p><p>中国人寿将 RPA 应用于财务银企对账，项目已在中国香港、中国澳门、新加坡三地落地运行，能够使工作流处理时效提升 75%、对账成功率 90% 以上，获得了第三届中国 RPA+AI 开发者大赛“2023 流程价值奖”。</p><p></p><p>中国太保拓展了数字员工应用场景的可能性，基于大模型技术构建了审计检查、公文质检、资讯问答等多名审计数字员工，已成为审计日常工作的等效劳动力，解决了审计领域缺乏人力的问题。此外太保产险深化 RPA 技术应用，“黑灯工厂”月均替代人力超过 400 人，提升集约化运营效率。</p><p></p><p>保险金融科技要想进一步深入应用，所面临的最大问题就是隐私数据保护。以大模型研究和应用为例，多数险企并不具备完全自研的能力，在算力、算法上的资源都严重不足，这就需要借助大模型企业的力量共同研究训练。但保险业数据包含了海量用户隐私信息、有其特殊性，如何进行数据脱敏出域就是个难题，目前也尚无明确的法律与监管指引。这是金融行业垂直领域大模型能力提升和落地应用面临的一大挑战。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Qjikz5pJH7PlVkz67WO3</id>
            <title>辩证看待“幻觉”问题，蔚来汽车在AI和大模型领域的应用实践</title>
            <link>https://www.infoq.cn/article/Qjikz5pJH7PlVkz67WO3</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Qjikz5pJH7PlVkz67WO3</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Apr 2024 08:13:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 新能源汽车, AI大模型, 蔚来, 人工智能应用
<br>
<br>
总结: 随着新能源汽车大战进入“智能化”的下半场，蔚来自研的 NOMI GPT 端云多模态大模型正式上线。蔚来在人工智能领域布局，已在多个场景落地应用，面临挑战并继续深耕细分领域。演讲中分享了蔚来的业务布局和AI大模型应用架构，强调了场景优先、工程化架构和更新AI三要素概念。同时，蔚来在大模型架构方面层层递进，结合公司数据开发垂类大模型，以提升智能体验。 </div>
                        <hr>
                    
                    <p>随着新能源汽车大战进入“智能化”的下半场，受政策、技术、市场的驱动，车企竞相踏入 AI 大模型这片蓝海。就在几天前，蔚来自研的 NOMI GPT 端云多模态大模型也正式上线。</p><p></p><p>作为中国高端纯电动汽车市场的引领者，蔚来在人工智能领域是如何布局的？目前 AI 大模型应用已经在哪些场景落地？在研发过程遇到了哪些挑战、又将继续深耕哪些细分领域？在日前举办的“人工智能 X 金融科技创新大会”上，蔚来汽车用户数字产品算法专家兼副总监潘鹏举回答了这些问题，并分享了自己对 AI 大模型架构、大模型发展难题的见解。</p><p></p><p>本文整理自其演讲，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p>本次演讲内容分为四部分：第一，简单介绍一下蔚来的业务；第二，分享一下蔚来的 AI 大模型应用架构是怎样布局、设想的；第三、第四部分，分别是从整个人工智能算法应用和大模型应用的两个角度出发介绍蔚来在这一领域的实践。</p><p></p><p></p><blockquote>关注「InfoQ数字化经纬」公众号，后台回复「蔚来」即可获取本次演讲 PPT</blockquote><p></p><p></p><h2>人工智能应用布局要围绕业务展开</h2><p></p><p></p><p>之所以要先介绍蔚来的业务背景，是因为无论是哪个公司，其人工智能应用如何布局都取决于业务范畴。蔚来的业务就如这个罗盘图所示，包含四个维度：产品、服务、社区和数字化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8d8b1226bfe2a800591d424581455ea.png" /></p><p></p><p>在产品方面，蔚来的核心产品是纯电的智能电动汽车，“智能化”在产品上的体现就是自动驾驶，因此蔚来在智能驾驶上投入的资源很多，这也是蔚来的核心竞争力。下图展示了蔚来目前已推出的 9 款车构成的产品矩阵。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1bda8ee31a9d0ced351a6445cc6331e5.png" /></p><p></p><p>围绕智能电动汽车，蔚来以“创造愉悦的生活方式”为使命，以“成为用户企业”为愿景，希望通过以智能电动汽车为起点，为用户提供高品质的服务与创新的解决方案，打造一个充满活力的社区，和用户共同成长。这是蔚来业务体系中较为特殊的一点。</p><p></p><p>蔚来提供的一系列与“车”紧密相关的服务，包括补能、换电、充电，都与其配套。下图为蔚来的充换电设施布局分布图。截至 2024 年 3 月 25 日蔚来已经有 2392 座换电站，3737 座充电站，两万多根桩。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1bf968dbc251816d67817e5bf5df8877.png" /></p><p></p><p>围绕整个产品、服务、社区应该怎么做？核心就是数字化和智能化。</p><p></p><h2>场景优先、注重工程化架构，更新 AI 三要素概念</h2><p></p><p></p><p>提到最著名的人工智能“三要素”，毋庸置疑就是“数据、算法、算力”。除了这三个最常提及的要素，我认为还要加上另外两个要素，一个就是“场景（Scenario）”，它要置于“数据、算法、算力”之上，因为在 AI 实际应用中“场景”是决定 AI 能否真正帮助到公司的重要因素，如果场景选择失误，将对整个投入和业务产出带来非常大的影响。</p><p></p><p>另一个要素是工程化（Engineering）。很多时候即使人工智能算法做得特别好，但因为响应时间过长，在实际业务场景中难以落地。举个例子，APP 的个性化推荐，对算法响应时长有很强的时效性要求。对有些用户而言 500 毫秒甚至 200 毫秒都非常多、不愿意等待，响应时间多一秒都会流失很多用户。所以在工程化方面，时长是非常重要的体验因素，决定了用户的满意度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/265319250088e73138577097d025c15a.png" /></p><p></p><p>在算法（Algorithm）层面，分为传统的小模型和大模型两套范式。蔚来的人工智能应用从整体方向上来说是以大小模型双轮驱动的方式在做。我们会把很多的重心放在大模型上面，但也不会忽略小模型的应用。</p><p></p><p>在 AI 平台层，有两处被标红的地方，“AI 训练框架”和“AI 推理引擎”，这两个环节在大模型应用中非常重要，要求算力和工程化架构都足够好。目前很多公司的算力都不足够支撑人工智能应用。在整个人工智能应用架构里，“算力”以及“工程化”也就是怎么去更好地部署人工智能，也是关键问题。</p><p></p><p>无论是场景、数据、算法、算力、流程化中的哪一方面，人工智能“五要素”实质是围绕用户和服务展开的。在蔚来，围绕产品层面，我们要做自动驾驶（AD）、智能座舱，围绕用户触达的渠道我们要做 NIO House、服务中心、交付中心。</p><p></p><p></p><h2>大模型架构层层递进</h2><p></p><p></p><p>我们对大模型应用架构的布局和构想，与行业主流做法比较接近。以开源大模型做基座，再结合公司的数据做垂类大模型的开发，整体架构包括基建层、模型层、开发层、应用层四个层面，重点支撑不同领域的垂类的大模型的开发和应用。基建层包括一些工具和资源，其上是一些基座大模型，再向上有很多工具链，最顶层是应用层。应用层是结合我们自己的数据后形成的，也分为了自动驾驶、用户服务与社区等等不同的维度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f72cd83212480f9012c98ad0284a0396.png" /></p><p></p><p>有了架构之后具体怎么做？其实核心还是刚刚提到的“大小模型驱动”，保留小模型的同时，在不同的特定场景下结合大模型的能力升级智能体验。</p><p></p><p>最近我们在自动驾驶领域做了一些尝试，探索的核心目的是希望能通过过去所有路况信息来生成一个真实的世界，我们内部也已经取得了一些结果。其次我们聚焦 NOMI 机器人，在过去机器人研发成果的基础上进一步结合大模型技术做整体智能化体验升级，最终将它运用于座舱智能客服，与客户对话交互。</p><p></p><p>蔚来的人工智能应用分为 to B 和 to C 两大板块，本质上仍然是用人工智能为业务赋能，只是各个层面在赋能的程度上稍有不同，有偏向辅助的、有替代人工的、也有可以成为机器人的。我认为 AI 替代人工的过程其实是慢慢地从简单到高级的，未来我们的 AI 有可能发展成一个真正的智能体。从这几个角度出发，我们在不同的场景做了不同的设计，在 to C 端和 to B 端的设计思路也不太一样。</p><p></p><h2>人工智能算法应用实践</h2><p></p><p></p><h4>案例一：错峰充电</h4><p></p><p>人工智能算法相关实践方面，我举的第一个例子是一个能源解决方案，它比较特殊、大家平时很少接触，是我们公司遇到的比较独特的问题。换电服务中换电站需要提前给电池充好电，不同时间段的充电价格是不同的，在用户需求量较大时电价也会较高，用户什么时候充电就会影响电费成本。</p><p></p><p>假设现在有两千多座换电站，一个换电站十块电池，那就有两万多块电池；一个换电站一天充一度电，那就是两万多块钱。我们的实际成本比这个假设还要多得多，因此电费的成本巨大。那么有什么方法、机制和算法能够实现每一天的充电成本最小化呢？在此背景下，我们做了“错峰充电”这个项目。“错峰”的概念指的是让换电站在电价较低时提前给电池充电，在电价较高时换电给用户。</p><p></p><p>进行“错峰”的第一步是预测用户的需求订单。如果用户不来充电，换电站就可以少充甚至不充以节约成本。首先要准确的预测用户什么时候来换电，才能进行何时充电、充多少电的决策。这一过程有很多约束条件，要在满足用户体验的前提下进行决策。</p><p></p><p>这个方案与智能驾驶的逻辑比较相像。第一，通过时序预测感知到用户的需求量。第二，用运筹优化算法计算充多少电能达到收益最大化。最后，指令下发、决策和策略的执行。为了规避整个业务系统在诸多环节中出现的问题，我们还要通过一系列策略设计来实现流程闭环。</p><p><img src="https://static001.geekbang.org/infoq/07/07a90222959d9c618c37b005bc5371c8.png" /></p><p></p><p>我们尝试了很多时序预测算法并进行效果对比，就整体而言，深度学习算法的预测能力比传统算法或树模型好很多，并且不同深度学习模型之间的效果大致接近。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1dd5b95b22be9674500e358478025e4e.png" /></p><p></p><p>在有了需求预测后应该怎么去决策呢？下图展示了如何进行运筹优化算法以实现收益最大化。这个环节的核心在于将“怎么算”这件事和具体的业务逻辑抽象成多个结果，从这些结果出发去计算当前每充一块钱电能获得多少收益，基于这套逻辑不断修正模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/23/23d23bb636b6e53e5ba639681c09b01c.png" /></p><p></p><p>接下来展示的这张图中，下半张图是电价随时间变化的折线图，上半张图是应用不同充电策略后的电费成本，我们工作的核心就是降低在尖峰时刻的充电量。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b898bde8364f9ec305cd169ec6fe40c.png" /></p><p></p><p>最近业界有一些声音在探讨 AI 的未来是不是要与能源挂钩，其实从这个案例中我们会发现，无论是 AI 的应用，还是我们业务的各方面，能源对它们产生的影响其实非常大。对蔚来而言，通过这个算法我们大概能节省几千万的成本，带来很高的收益。随着未来 AI 算力越来越高，其本身能源的消耗量也将大得惊人。从这个角度出发，我们能看到 AI 应用向节能领域发展的一个趋势。</p><p></p><h4>案例二：智能运维</h4><p></p><p></p><p>与大家平时经常听到的智能营销不同，第二个案例是智能运维方案，这是我们这一行业经常遇到的问题。当充电枪长期使用后会出现劣化的情况，导致后续的用户跳枪或者无法充电，我们就需要对所有换电站和充电桩的每一个设备、每一个零件进行监测，以及时发现哪些枪头有劣化迹象，进而提升用户体验。</p><p></p><p>监测的方法有很多，比较传统的方法就是直接收集设备所处环境的温度、湿度等各方面影响因素的数据并进行预估，就能大致得出设备能否正常充电的结论了。下面这张图展示的就是⼯业机理分析的结果，即根据充电枪的充电电流、电压、温度等物理信号建立物理模型得到枪头的温升系数物理量，并以此为信号进⾏故障诊断。从结果来看这种机理模型其实也有一定效果，但正常枪头信号和异常枪头信号结果之间的差异不够显著。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a1/a1c2c7c0c481b128ab95df9de6dcd5ef.png" /></p><p></p><p>对此我们进一步做迭代，输入了整体的、持续的数据，并基于输入的数据去判断波形和趋势有无异常，把机理模型升级到基于 Conceptor-AI 算法检测模型 + 机理模型。这一方案其实并没有完全抛弃传统做法、只使用深度学习算法，而是融合了人类经验知识与机器算法，最终使误报警数减少⾄20%，准确率提升了 10pct。</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/94510d836c356feeee3cb2f324efe0c2.png" /></p><p></p><h4>案例三：APP 个性化推荐</h4><p></p><p></p><p>第三个案例和很多互联网公司 APP 遇到的问题很相近。蔚来的 APP 内容本身很丰富，除了售车信息，还有汽车资讯、相关商品售卖、充电地图服务等等内容。不同板块下的相关内容推荐的场景和入口都不相同，因为不同内容推荐对应的业务领域有不同目标。比如对于资讯类内容，我们追求的是高点击率；对于商品，我们追求的是 GMV（商品交易总额）。</p><p></p><p>一个 APP 内要做很多场景、每个场景有不同的目标，那么每个场景都需要用不同的算法去实现对应的目标吗？其实不然，我们最终只做了两件事。针对这么多的场景，不形成一套系统就很难支撑这么多业务，因此我们做的第一件事就是把个性化推荐的系统架构抽象出来。其中较为特殊的是，我们把搜索和推荐合并为一套，而不是单纯只做推荐系统，这与搜索只存在于索引步骤的情形有区别。</p><p></p><p>第二件事是围绕业务背景对算法目标体系进行整体优化和提升。其实我们最终想解决的只有一个问题，就是能不能通过一些非常简单的数据、用 1 到 2 个核心算法去解决所有业务场景的问题。</p><p></p><p>这是我们现在正在尝试的一个解法，即打通所有底层数据。完成数据共通之后，我们在算法层面又引入了一个目前比较好的方法，即专家网络（MoE）。在不同的业务场景里使用不同的专家网络去学习不同的权重，然后再做一些应用层，围绕最终业务目标，输出不同的业务指标，从而尽量降低整体维护的工作量。</p><p></p><p>为什么要这么做？另一个原因是场景化开发方式对每个业务人员的依赖度很高，会对不同业务领域有很强的业务理解，比如造特征造得如何。但其实项目进行得越多，我们发现可以用一些比较简单的数据和复杂的模型来解决下游推荐效果的问题提高整体迭代效率和输出成果。这是我们在 APP 个性化推荐方面的一些想法和做法。</p><p></p><h2>大模型应用场景探索</h2><p></p><p></p><p>蔚来在大模型应用方面的探索可分为四大板块：知识洞察、内容生成、Copilot（智能助手）、Agent（数字代理)。</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/21adbcffab497ad2cc2dc0645ae425a1.png" /></p><p></p><p>这里我想着重提到的一个点是知识洞察。知识洞察涉及到许多方面。在过去，我们与用户交互时的很多数据并没有得到充分挖掘。比如公司和用户在语音交互过程中的各个触点都能产生很多的数据。在过去，对电话销售数据的挖掘方式是定义很多标签来分类，这一方式效率很低。在过去，我们对业务的结构化数据挖掘比较多，但对非结构化数据的挖掘还不够深，因此我们也在这方面投入了很多精力。</p><p></p><p>回到大模型能力本身，有两个能力可以利用：第一，大模型的理解能力特别强；第二，生成能力特别强。围绕这两个层面我们都做了一些尝试，利用 AIGC 的生成能力提升效率。</p><p></p><p>其实从生成的角度上来说，大模型幻觉不一定是件坏事。因为在某些场景下，特别是在创意类的场景里，幻觉是恰恰是有帮助的。如果生成出的内容非常中庸，也难以为工作提供启发。但是在理解的层面，仍然要考虑怎么去避免幻觉的问题。在不同的业务场景里，大模型要解决的问题是不同的。</p><p></p><p>针对 Copilot 和 Agent，我们重点围绕智能客服做迭代升级。Copilot 方面，我们其实做了很多应用，特别是在用户服务层面我们有非常多的知识，我们利用 Copilot 对这些知识如何分发、如何检索做一些尝试。整体上对于内部工作有一些提升，但并没有我们想象的那么惊艳。Agent 方面，我们用大语言模型的范式重新做了整套智能客服，有一些效果、但依然有很多问题要解决。</p><p></p><h4>案例一：内容质量标签</h4><p></p><p>接下来我想举一个典型的大模型应用案例，即在知识洞察层面如何基于大模型打内容质量标签。在过去打内容质量标签很简单，只需要以数量为标准，根据某个内容中有多少张图片、有多少个文字、有多少种主题来打标签，而现在则还需要进一步考虑主题究竟够不够丰富、图片够不够美观。这种内容质量升级就意味着要考虑更多语义信息。</p><p></p><p>如果用传统的方式进行这一工作，就需要大量人力来产生标注数据，告诉我这些海量图片中有哪些维度是比较好的。而如今基于大模型，我们已经有了新的范式，其中有两个点向大家分享。</p><p></p><p>第一，大模型提效的成果非常显著。只需要告诉大模型“帮我总结这部分内容里大概有多少个主题，图片质量如何”，它就能帮你进行简单的理解。</p><p></p><p>第二，我们形成了一套新的基于大模型的打标流程。过去无论是用大模型算法还是用传统算法，都是业务给我一个需求，我造样本来实现需求。有了大模型之后，我们和业务人员的协作模式发生了变化，让业务自己写 Prompt（输入大模型的指令或问题），业务在发现得到的样本结果不符合预期时就可以自己调整。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/414e0048886207de35b6c36e02635875.png" /></p><p></p><p>基于这样的协作模式，相比过去有两个优点。第一，解决了业务信任度低的问题。过去合作中很多时候业务会不太信任你，对他们而言算法是个黑盒、不知道如何调优；现在可以把这个逻辑直接交给业务，由业务自己去调整，把黑盒的东西变成了白盒化，业务对算法的控制能力变强了。</p><p></p><p>第二，提升内部工作效率，让我们能安心做好算法。有了业务给出的 Prompt 后，我们只需要考虑用新样本集做好大模型的微调、部署上线，反向交给业务、再优化 Prompt，业务又会进一步补充样本数据。这个过程中算法做好需要算法调整的工作，而业务也能了解算法是如何成形并优化的。无论是准确率、召回率还是开发周期，用大模型打标签都有较大提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/9164cc5dabec10518a4697eea35333fc.png" /></p><p></p><p>为什么举这个例子呢？是因为我们发现其实有很多业务场景都与打标签很相像。比如说智能客服要做的就是理解用户说了什么，如果把用户的意图视作一个标签，那么智能客服实际上也在做打标签这件事。所以基于大语言模型标签范式能够应用在很多业务场景中。接下来我们会把这一套标准流程变得更产品化。</p><p></p><h4>案例二：二维码生成</h4><p></p><p></p><p>第二个大模型应用案例展示了我们在内容生成方面的一些探索。我们一直在思考一个问题，与用户交互有很多不同形态。从技术本身来说，它不是一件非常难的事。</p><p></p><p>比如把原有的、用户熟知的交互方式通过大模型转换成新形态，这样的互动能够给用户一些新鲜感、提升用户体验或效率。并且提供了更好玩的玩法之后，用户继续体验的意愿也有可能会提升。因此 AI 大模型在营销领域有非常丰富的应用场景。</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/71ba3af4a47a45f7ef325f14e1a83d75.png" /></p><p></p><p>将大模型的内容生成能力应用于生成独特的二维码，这一应用还存在一些挑战。首先，每个公司都有自己的品牌调性，Logo 如何设计、放在什么位置都要依循一定的规范，我们还要围绕规范问题不断优化迭代。其次，我们对素材创意生成做了很多尝试，但这个场景应用做的快、上线慢，存在很多天然的问题。</p><p></p><h2>大模型落地 C 端存在四大挑战</h2><p></p><p></p><p>对于大模型究竟价值几何的问题，我们肯定要从长期去看，但是大模型本身的短期价值可能并没有大家想象的那么高。大模型要想真正在 to C 端落地还面临很多挑战。</p><p></p><p>首先就是监管问题。AIGC 一旦面向 C 端用户、进行大规模应用，就需要监管报备，而报备所需的周期很长、牌照也很稀缺，这是绕不开的难题。</p><p></p><p>第二，大模型幻觉的问题。刚刚也提到过，对于这一问题我们要辩证看待，在不同的场景下，有时幻觉可以给我们带来帮助，但有时需要我们解决。从大模型技术本身来说，业务应用怎么和大模型的幻觉共存是个永恒的话题。无论是什么方式，结合传统的方式也好、还是基于大模型的方式也好，幻觉问题一定是存在的。所以应该怎么把握好共存还是规避的边界，也是一个比较核心的问题。</p><p></p><p>第三，算力问题。算力很稀缺，每个公司自研大模型难度很大，不仅缺卡，还缺数据。即使是随便做了一些 SFT（Supervised Fine-Tuning，监督微调），花费的时间也不少。如果要完成非常多的大模型优化，那么算力肯定是不可或缺的。</p><p></p><p>第四，性能问题，也是在我们目前的 AI 大模型应用中遇到的比较大的问题。大模型性能不足、响应时长较长，将会对使用场景产生限制。其实业界已经有很多解决这一问题的框架，但其中很多本身都存在缺陷，需要针对整个框架做一些定向的优化和提升。在不同的应用场景里，对性能的要求也不太一样，需要不断调优。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/jtCicpRwZJ0TEKOVKRP2</id>
            <title>我处理了5亿 GPT tokens 后：langchain、RAG等都没什么用</title>
            <link>https://www.infoq.cn/article/jtCicpRwZJ0TEKOVKRP2</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/jtCicpRwZJ0TEKOVKRP2</guid>
            <pubDate></pubDate>
            <updated>Sun, 28 Apr 2024 07:38:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 创业公司, OpenAI模型, 提示, Langchain
<br>
<br>
总结: 作者分享了在创业公司使用OpenAI模型的经验教训，强调了在提示中少即是多的重要性，以及不需要过早抽象的Langchain，同时也提到了使用流式API改善延迟的重要性，以及GPT特别不擅长处理零假设的情况。 </div>
                        <hr>
                    
                    <p></p><p>在过去的六个月里，我的创业公司Truss（<a href="https://gettruss.io/">gettruss.io</a>"）发布了多项倚重LLM的功能，而我在Hacker News上读到的关于LLM的故事现在已经和我的实际情况脱节了，所以我想在处理过5亿多（我估计）tokens之后，分享一些更“令人惊讶”的经验教训。</p><p>&nbsp;</p><p>本文要点：</p><p>&nbsp;</p><p>我们正在使用OpenAI模型，如果你想知道我对其他模型的看法，请阅读底部的问答部分。在我们的用例中，GPT-4占85%，GPT-3.5占15%。我们专门处理文本，因此不涉及GPT-4-vision、Sora、whisper等。我们有一个B2B用例——重点是汇总/分析-提取，你的情况可能有所不同。5亿tokens其实并不像想象的那多，也就大概75万页文本，要正确看待。</p><p>&nbsp;</p><p></p><h2>对于提示，少即是多</h2><p></p><p>我们发现，不要在提示中给出确切的列表或指令——如果这些东西已经是常识的话，这样可以获得更好的结果。GPT并不愚蠢，你提供的细节过多，反而会让它混乱。</p><p>&nbsp;</p><p>这和编写代码不一样，代码必须明确。</p><p>&nbsp;</p><p>下面是我们遇到的一个例子。</p><p>&nbsp;</p><p>我们的一部分管道读取了一些文本块，并要求GPT根据它们与美国50个州或联邦政府的相关性进行归类。这不是什么很难的任务——或许用string/regex就可以搞定，但会有许多奇怪的边缘情况，花费的时间会更长。因此，我们首先做了（大致）这样的尝试：</p><p><code lang="null">Here's a block of text. One field should be "locality_id", and it should be the ID of one of the 50 states, or federal, using this list:
[{"locality: "Alabama", "locality_id": 1}, {"locality: "Alaska", "locality_id": 2} ... ]</code></p><p>这样做有时候是可以的（我估计98%以上的情况都可以），但如果需要深入挖掘的话经常会失败。</p><p>&nbsp;</p><p>经过研究，我们注意到字段name始终返回州的全名——即使我们没有明确要求它这样做。因此，我们改为对name做简单的字符串搜索来找出相应的州。从那以后，它就工作得很好了。</p><p>&nbsp;</p><p>我认为，更好的方法应该是：</p><p>&nbsp;</p><p></p><blockquote>“You obviously know the 50 states, GPT, so just give me the full name of the state this pertains to, or Federal if this pertains to the US government.”（GPT，你显然知道50个州，文本和哪个州相关，你就告诉我这个州的全名，如果和美国政府相关，你就告诉我联邦政府。）</blockquote><p></p><p>&nbsp;</p><p>就是这么不可思议！你的提示模糊一点，GPT概括的反而更好，反馈的质量反而更高——这是高阶委托/思维的典型标志。</p><p>&nbsp;</p><p>（注1：你可能会想GPT从根本上讲是一个随机模型，但它面对M开头的州失败次数最多。）</p><p>&nbsp;</p><p>（注2：当我们要求GPT从列表中选择一个ID时，如果我们以格式化的JSON发送，每个州一行，那么它就不会那么困惑了。我认为，\n是一个比逗号更强大的分隔符。）</p><p>&nbsp;</p><p></p><h2>你不需要langchain，甚至不需要OpenAI去年在API中发布的任何东西，只需聊天API就够了</h2><p></p><p>&nbsp;</p><p>Langchain是过早抽象的一个典型例子。</p><p>&nbsp;</p><p>我们一开始以为必须得用它，因为网上是这么说的。而实际上，在tokens数量达到成百上千万、生产环境具备大概3～4个完全不同的LLM特性之后，我们的openai_service文件中仍然只有一个40行的函数：</p><p><code lang="null">def extract_json(prompt, variable_length_input, number_retries)</code></p><p>&nbsp;</p><p>我们唯一使用的API是chat。我们总是提取JSON。我们不需要JSON mode、函数调用和助手（虽然我们都做了），我们甚至没有使用系统提示（或许我们应该）。当gpt-4-turbo发布的时候，我们只更新了代码库中的一个字符串。</p><p>&nbsp;</p><p>这就是功能强大的通用模型的美妙之处——少即是多。</p><p>&nbsp;</p><p>在这个函数的40行代码中，大部分代码都是用来处理普通的500错误或套接字关闭错误（尽管OpenAI API变得越来越好，但考虑到它们的负载，出现这样的问题也并不奇怪）。</p><p>&nbsp;</p><p>我们内置了一些自动截断代码，因为我们不必担心上下文长度限制。我们有自己的tokens长度估计器，如下所示：</p><p><code lang="null">if s.length &gt; model_context_size * 3
  # truncate it!
end</code></p><p>在一些极端情况下，如句号或数字过多时，上述代码会不起作用。因此，我们还有下面这个特有的try/catch重试逻辑：</p><p><code lang="null">if response_error_code == "context_length_exceeded"
   s.truncate(model_context_size * 3 / 1.3)</code></p><p>我们使用这种方法取得了不错的效果，而且也有足够的灵活性来满足我们的需求。</p><p>&nbsp;</p><p></p><h2>使用流式API改善延迟，向用户提供速度可变的输出，这实是ChatGPT一个重大的用户体验创新</h2><p></p><p>我们可能认为这就是一个噱头，但用户对于这个特性的反响很是积极。</p><p>&nbsp;</p><p></p><h2>GPT特别不擅长零假设</h2><p></p><p>“Return an empty output if you don’t find anything（如果没有找到任何内容，则返回空）”——这可能是我们遇到的最容易导致GPT出错的提示语。</p><p>&nbsp;</p><p>GPT经常会产生幻觉，提供不那么真实的答案，而不是什么都不返回。但这样的问题会导致它缺乏信心，什么都不返回的次数会比正常情况下多。</p><p>&nbsp;</p><p>我们大部分提示都是类似下面这样：</p><p>&nbsp;</p><p></p><blockquote>“Here’s a block of text that’s making a statement about a company, I want you to output JSON that extracts these companies. If there’s nothing relevant, return a blank. Here’s the text: [block of text]”（这里有一段文字描述了一家公司，我们希望你提取这家公司并输出JSON。如果未找到任何相关内容，则返回空。文本如下：[文本内容]）</blockquote><p></p><p>&nbsp;</p><p>有一段时间，我们有一个Bug，就是[文本块]可以为空。GPT会出现糟糕的幻觉。顺便说一下，GPT喜欢幻想面包店，下面这些都很棒：</p><p>&nbsp;</p><p>Sunshine BakeryGolden Grain BakeryBliss Bakery</p><p>&nbsp;</p><p>我们的解决方案是修复这个Bug，如果没有文本就不发送提示。但难点在于，通过编程判断“它是空”比较困难，其实这时需要GPT参与进来了。</p><p>&nbsp;</p><p></p><h2>“上下文窗口”一词不是很妥当，只有输入窗口变大了，而输出窗口并没有</h2><p></p><p>&nbsp;</p><p>很少有人知道：GPT-4允许的最大输入窗口为128K，而输出窗口仍然只有4K。显然，“上下文窗口”一词是有<a href="https://community.openai.com/t/what-does-context-window-mean-in-the-documentation/566158">迷惑性</a>"的。但问题的糟糕之处在于，我们经常要求GPT返回一个JSON对象的列表。想象一下，一个JSON任务的数组列表，每个任务都有一个名称和标签。</p><p>&nbsp;</p><p>GPT实在没法返回10项以上。硬要让它返回15项的话，或许只有15%的情况下可以做到。</p><p>&nbsp;</p><p>一开始，我们以为这是因为4K大小的上下文窗口限制，但我们发现，10项的时候只有700～800个tokens，GPT就停下了。</p><p>&nbsp;</p><p>现在，你当然可以把输出变成输入。你给它一个提示，要求它返回一个任务，然后把提示和任务一起提供给它，再要求下一个任务，以此类推。但现在，你在和GPT玩电话游戏，并且必须处理类似Langchain这样的事情。</p><p>&nbsp;</p><p></p><h2>向量数据库和RAG/embeddings，对我们这些普通人来说几乎毫无用处</h2><p></p><p>我累了，我真得累了。每次我想到一个杀手级的RAG / embeddings用例时，我都会狼狈不堪。</p><p>&nbsp;</p><p>我认为，数据库/RAG事实上是为搜索而存在的，仅限于搜索，而且是像谷歌或必应那样的真正的搜索。下面是一些原因：</p><p>&nbsp;</p><p>缺少相关性界限。这里有<a href="https://txt.cohere.com/rerank/">一些解决方案</a>"，比如你可以创建自己的相关性界限启发式，但那并不可靠。在我看来，这会扼杀RAG——总是会检索出不相关的结果，或者过于保守，错过重要的结果。为什么要把向量存入一个专有数据库里而远离其他数据呢？除非你的规模达到了谷歌/必应的水平，否则是不值得丢失上下文的。除非你做的是一个非常开放的搜索，比如整个互联网——用户通常不喜欢语义搜索，因为它会返回一些不相关的东西。对于大多数商业应用中的搜索，用户都是领域专家——他们不需要你去猜测他们的意思，他们会直接告诉你！</p><p>&nbsp;</p><p>在我看来（没测试过），对于大多数的搜索场景，LLM更好的用法是使用正常的提示补全将用户的搜索转换为面搜索，甚至是更复杂的查询（甚至是SQL）。但这根本不是RAG。</p><p>&nbsp;</p><p></p><h2>幻觉基本不会出现</h2><p></p><p>从根本上讲，我们的用例都是“这里有一个文本块，从中提取一些东西。”</p><p>&nbsp;</p><p>一般来说，如果你让GPT给出一段文本中提到的公司名，它不会随机给出一个公司（除非文本中没提及任何公司——这是零假设问题！）。</p><p>&nbsp;</p><p>如果你是一名工程师，那你肯定已经注意到了：GPT并没有真正地生成幻觉代码，它不会创建变量，或者在重写你发送给它的代码块过程中随机引入错别字。</p><p>&nbsp;</p><p>当你要求它给你一些东西时，它确实会产生存在标准库函数的幻觉，但我还是把那看作零假设。它不知道怎么说“我不知道”。</p><p>&nbsp;</p><p>但如果你的用例完全是这样的：“这是全部的上下文信息，分析/总结/提取”，那么它会非常可靠。最近发布的很多产品都强调了这个严谨的用例。</p><p>&nbsp;</p><p>因此总的来说，输入的数据好，GPT就会给出好的响应。</p><p>&nbsp;</p><p></p><h2>小结：路在何方？</h2><p></p><p>&nbsp;</p><p>对于一些问题，我在下面直接做了回答。</p><p>&nbsp;</p><p>Q：我们会实现AGI吗？</p><p>A：不。用这种转换器+互联网数据+$XB基础设施的方法是不行的。</p><p>&nbsp;</p><p>Q：GPT-4真得有用吗？还是说一切都是营销？</p><p>A：它百分之百有用。现在仍然是互联网的早期阶段。</p><p>&nbsp;</p><p>Q：它会让所有人失业吗？</p><p>A：不。从根本上讲，它降低了人们进入ML/AI领域的门槛，而之前这是谷歌才有的能力。</p><p>&nbsp;</p><p>Q：你试过Claude、Gemini等模型吗？</p><p>A：实际上，我们并没有做任何严谨的A/B测试，但我在日常编码过程中测试过，感觉它们还差得比较远。主要体现在一些比较微妙的事情上，比如感知你的意图。</p><p>&nbsp;</p><p>Q：我怎么才能跟上LLMs/AI领域的最新发展动态？</p><p>A：不需要这么做。关于<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>"，我想过很多，模型性能的总体改进会远超小幅优化。如果真是这样，你所需要担心的就只有GPT-5何时问世，其他的都不重要。OpenAI在此期间发布的其他所有东西（不包括Sora等，那是完全不同的东西）基本上都是干扰。</p><p>&nbsp;</p><p>Q：那么当GPT-5出现时，它会有多好？</p><p>A：和其他人一样，我一直在试图从OpenAI那里寻找相关的蛛丝马迹。遗憾的是，我认为我们接下来只会看到渐进式的改进。我对“GPT-5会改变一切”不抱多少希望。</p><p>&nbsp;</p><p>这其中的根本原因是经济方面的。我之前以为，从GPT-3到GPT-3.5可能是模型通过训练获得超线性改进：训练难度提高2倍，而性能提升2.2倍。但显然，情况并非如此。我们看到的是对数关系。事实上，为实现增量改进，token 速度是呈指数级下降而单token成本是呈指数级增长的。</p><p>&nbsp;</p><p>如果是这样的话，我们就处于某种帕累托最优曲线上，而GPT-4可能就是最优的：尽管与GPT-3.5相比，我愿意为GPT-4支付20倍的价格。但老实说，从GPT-4到GPT-5，我不认为我会为每个token，而不是为GPT-4所使用的任务集，支付20倍的价格。</p><p>&nbsp;</p><p>GPT-5可能会打破这一局面。或者，只是iPhone 5与iPhone 4的差别。我并不会为此感到失落！</p><p>&nbsp;</p><p>&nbsp;</p><p>声明：本文为InfoQ翻译，未经许可禁止转载。</p><p>&nbsp;</p><p>原文链接：<a href="https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/">https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/60rpbqOcSZJ0E9z2lO3w</id>
            <title>64亿美元的交易，是IBM与HashiCorp的一场相互救赎</title>
            <link>https://www.infoq.cn/article/60rpbqOcSZJ0E9z2lO3w</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/60rpbqOcSZJ0E9z2lO3w</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 08:54:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: IBM, HashiCorp, 收购, Terraform
<br>
<br>
总结: IBM斥资64亿美元收购HashiCorp，希望在自家红帽品牌提供的混合云功能基础之上做出进一步探索。HashiCorp在云生态系统中拥有广泛的影响力，与众多主流云厂商都是合作伙伴关系。虽然HashiCorp名号不如IBM家喻户晓，但其基础设施即代码工具Terraform有着惊人的影响力。收购对于双方都具有战略意义，IBM通过收购补充基础设施软件产品组合，而HashiCorp将享受IBM的丰富经验和市场渠道。 </div>
                        <hr>
                    
                    <p></p><h2>IBM斥资64亿美元收购HashiCorp</h2><p></p><p>当地时间2024 年 4 月 24 日，IBM宣布斥资64亿美元（即每股 35 美元）收购了Terraform的创造者HashiCorp，希望能在自家红帽品牌提供的混合云功能基础之上做出进一步探索。HashiCorp 在云生态系统中拥有广泛的影响力，它与众多主流云厂商都是合作伙伴关系。</p><p>&nbsp;</p><p>IBM表示，HashiCorp收购交易将在结束后的第一整年内拉升其调整后的息税折旧及摊销前利润，并在第二个整年内帮助增加自由现金流。IBM公司预计交易将于2024年底完成，HashiCorp现任CEO&nbsp;Dave McJannet也计划继续留在公司。</p><p>&nbsp;</p><p>HashiCorp 成立于 2012 年，虽然名号不如IBM那样家喻户晓，但HashiCorp旗下的基础设施即代码工具Terraform却有着惊人的影响力，该工具主要用于分配及调整云端及本地资源。此外，HashiCorp还提供工作负载编排产品Nomad、开发者平台Waypoint以及Vault、Boundary及Consul等一系列安全工具。就在收购消息发布的两天之前，HashiCorp还刚刚推出了基础设施云The Infrastructure Cloud产品，将其基础设施及安全产品同HashiCorp Cloud平台相结合，旨在提供统一的云管理平台。</p><p>&nbsp;</p><p>作为一家曾经炙手可热的硅谷初创企业，HashiCorp在2021年首轮公开募股（IPO）后迅速崛起，短短一个月后股价就来到近100美元的高位。而如今IBM的收购价格仅为35美元，两年多之后缩水达65%。</p><p>&nbsp;</p><p>虽然其当家开源软件有着极为广泛的受众群体，但在过去一年间，HashiCorp却因决定从 Mozilla 开源许可证 v2.0 切换到商业源代码许可证（BSL），从而限制了其产品的免费使用而饱受争议和批评。</p><p>&nbsp;</p><p>2021年8月，HashiCorp对其商业化策略进行了重大调整，此举很快引起开源社区的激烈反响。当时，不少开发者在社交平台X上发文表示抗议，“我对HashiCorp比对IBM更恼火，我其实很高兴IBM承认了我们已经知道的事情，但是HashiCorp……我只是很失望。我非常尊重他们，不想冒犯他们，但这件事上我找不出来任何一个积极的点。”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ee/ee35b7dfac06eefc446fa2255eada114.png" /></p><p></p><p>“这一事件的影响比最初想象的要广泛得多--这件事情正在整个科技行业掀起涟漪！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48fcd401c22cd9f775721c357d8c495f.png" /></p><p></p><p>2023年12月，HashiCorp创始人Mitchell Hashimoto离开公司，且有传言说Hashimoto与现任CEO McJannet在商业策略上存在冲突。</p><p></p><h2>此次收购对于双方的意义是什么？</h2><p></p><p>&nbsp;</p><p>除了许可证风波外，最近几年HashiCorp可谓过得风雨飘摇。在被收购之前，HashiCorp 就已经出现了业务增长放缓趋势。</p><p>&nbsp;</p><p>与许多在 2019 年至 2022 年之间进行 IPO 的云原生领域公司一样，HashiCorp 从未实现盈利。在 2022 年中期之前的低利率时代，增长才是最重要的。但从那时起，投资者的关注点从业务的增长转变为了盈利上。而盈利问题正是HashiCorp的痛中之痛。</p><p>&nbsp;</p><p>HashiCorp 高度依赖 ARR（Annual Recurring Revenue，年度经常性收入） 超过 10 万美元的客户群。据分析师称：</p><p>&nbsp;</p><p></p><blockquote>HashiCorp 89% 的收入来自每年在其身上花费超过 10 万美元的客户，而这些客户仅占其付费客户群的 19%（截至 2024 财年第 1 季度，共有 4392 名客户中的 830 名）。除此之外，收入在地域上也高度集中，71% 的销售额来自美国。</blockquote><p></p><p>&nbsp;</p><p>在接下来的三个季度中，这个 10 万美元以上细分市场的增长水平进一步放缓，而收入集中度仍保持在 89%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/13/13a51cc020dc3877b601f6ff9132d7fc.png" /></p><p></p><p>&nbsp;</p><p>与此同时，HashiCorp 的净美元保留率 (NDR) 持续下降，在过去两个季度大幅下降至 115。虽然考虑到更广泛的宏观经济环境，许多公司 130+ NDR 的日子已经成为过去，但这里的下降速度异常快。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/a3/a3ec0fd2a98db1ec6a0f932a216f462b.png" /></p><p></p><p>简而言之，这是一家增长迅速放缓的业务，无法支撑其现有估值，更不用说IPO时的估值了。它成为收购目标已经有一段时间了，但即使自 IPO 以来价格有所下降，价格仍然相对较高。</p><p>&nbsp;</p><p>没有人怀疑 HashiCorp 创建的产品（尤其是 Vault 和 Terraform）的实用性和受欢迎程度，但实用性本身并不能转化为企业收入。 IBM 拥有足够的企业关系，使此次收购物有所值。</p><p>&nbsp;</p><p>William Blair研究分析师Jason Ader在研究报告中表示这笔交易对于两家公司都极具战略意义。</p><p>&nbsp;</p><p>“我们相信这笔交易对两家公司都具有巨大的战略意义。IBM通过高人气工具（Terraform、Vault）补充并支撑其不断增长的基础设施软件产品组合，而HashiCorp也将享受蓝色巨人的丰富经验以及影响力巨大的市场渠道。”</p><p>&nbsp;</p><p>IBM公司CEO Arvind Krishna在电话会议上指出，HashiCorp的产品“在开发者社区中得到广泛采用”，总下载量已经超过5亿次，并得到超85%财富500强公司的使用。</p><p>&nbsp;</p><p>值得一提的是，Gartner公司副总裁Sid Nag在采访中表示，从IBM的角度来看，这笔交易的动机非常明确。首先，IBM正在努力充实其Ansible平台的功能，而达成这一目标的最好方式当然就是直接从主要竞争对手那边“借花献佛”。</p><p>&nbsp;</p><p>其次，他表示IBM正努力扩大其市场范围与收入潜力。通过收购HashiCorp，其不仅能够获得对方的原有客户，同时也能将HashiCorp的技术方案出售给自身庞大的企业客群。</p><p>&nbsp;</p><p>再从更深入的角度来看，这笔交易可以说是“IBM成功收购红帽并实现货币化的又一个翻版”。IBM似乎意识到红帽Ansible平台中存在缺陷，而这已经开始阻碍其扩大市场份额。而收购HashiCorp能够增强Ansible能力，及时填补这些缺口。</p><p>&nbsp;</p><p>那么HashiCorp又为什么会接受收购？</p><p>&nbsp;</p><p>乍看之下，很多人其实难以理解为什么一家技术产品广受好评的上市公司会愿意接受收购，特别是接过资源配置市场上最大竞争对手IBM递来的橄榄枝。</p><p>&nbsp;</p><p>但在给投资者们的报告中，美国银行证券分析师Wamsi Mohan指出，HashiCorp的“业务增幅一直在下降”。他同时补充称，IBM有望“推动成本协同效应，凭借更大的客群规模恢复收入增长。”</p><p>&nbsp;</p><p>因此外界不由得猜测，HashiCorp当真陷入了困境？</p><p>&nbsp;</p><p>AvidThink公司创始人Roy Chua在采访中表示确有此事：HashiCorp的确在“开发和DevOps社区中站稳了脚跟——至少在整个商业源代码许可证（BSL）闹剧及Linux基金会建立OpenTofu项目之前是这样。”</p><p>&nbsp;</p><p>2023年8月，HashiCorp决定从Mozilla公共许可证框架过渡至商业源代码许可证框架。该公司当时表示，此举旨在扩大其对开源技术商业化的控制权。然而用户对这一变化并不买账，两周之后Linux基金会就迅速建立了名为OpenTofu的Terraform分叉项目。</p><p>&nbsp;</p><p>Chua表示，“考虑到众多DevOps和软件团队都在免费使用并高度依赖Terraform，我觉得HashiCorp其实没有完全意识到由此创造的价值。如果他们在Scott Johnston及其团队的领导下成功完成Docker式转型，相信绝对能拿下更多收入并避免当下被收购的命运。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/23/23a7124147479668b7d0315bb16f762d.png" /></p><p></p><p>随着IBM接管，“我们也都将关注IBM是否会撤销转向BSL的决定。”Chua还提到，IBM最近领导了另一个名为OpenBAO的Linux基金会孵化项目，这就是HashiCorp Vault的一个分叉，明显也是在表达对许可证框架转换的不满。</p><p>&nbsp;</p><p>“现在IBM与Terraform一同控制了该资产，开发人员将推动IBM撤销BSL转换并停止分叉。”</p><p>&nbsp;</p><p>有不少人担心IBM收购后会加强对Terraform 和 Ansible的限制或者对这两款软件“胡作非为”，甚至还调侃道，如果IBM要将Terraform 和 Ansible合并，那岂不是要叫“Terrible”？</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/027d7f8b6442766ec0426884f49339af.jpeg" /></p><p></p><p>System Initiative 联合创始人&amp;CEO&nbsp;Adam Jacob在X上发文对该笔交易表示祝贺：“祝贺所有我认识的Hashicorp团队的人，他们建立起这样一家公司是一个令人难以置信的成就，有人认为你们创办的公司价值40亿美元以上是一件疯狂的事情，但你们的确做到了，恭喜。”</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/95/95d4c0184c6b6a6d2ea8e10a65083ce8.png" /></p><p></p><p></p><h2>商业许可证BBL伤害了谁，保护了谁？</h2><p></p><p>&nbsp;</p><p>与Hashicorp更改许可证引发社区不满，从而引出开源软件分支的事上个月就发生过一次。</p><p>&nbsp;</p><p>3 月 21 日，Redis 背后企业 Redis 的 CEO Rowan Trollope 宣布，该项目的许可证类型将从原本的 BSD 开源许可证变更为 RSAL&nbsp;v2 与 SSPL&nbsp;v1 双许可证。</p><p>&nbsp;</p><p>Redis 公司称此番许可证变更主要是为了保护 Redis 公司的商业利益，防止云厂商利用开源版本支持商业 Redis SaaS 服务。此类行为在市场上并不少见，Confluence、MongoDB 及 Elastic 等厂商此前已经对其开源项目做了类似的许可证变更，旨在保护自身利益。但 Redis 公司的这一举动却引发了众多开发者的愤怒，其中一个重要原因就是 Redis 社区中有着大量外部贡献者。这种单方面修改许可证的行为被视为对社区的背叛，更是对贡献者们的背叛。</p><p>&nbsp;</p><p>事实上，关于商业许可证的争议一直不断。</p><p>&nbsp;</p><p>HashiCorp对BSL的采用让开源社区的许多成员感到困惑。BSL结合了开源和专有许可的各个方面，要求公司将其软件作为开源提供，但只在特定的持续时间内提供。在约定的期限之后，软件的许可证可以更改为专有模式。那么，这就带来了这样一个问题：这种许可结构是否符合开源软件的基本原则?</p><p>&nbsp;</p><p>成功的开源项目的基石之一是社区参与。来自不同背景的贡献者协作构建、增强和维护软件，使每个人都受益。HashiCorp转向BSL带来了一定程度的不确定性，这可能会阻碍社区的参与。当许可条款突然改变，影响他们自由使用、修改或分发软件的能力时，贡献者可能会犹豫是否要在项目中投入时间和精力。</p><p>&nbsp;</p><p>同时，变更许可证也会浇灭独立开发者和独立贡献者为社区做贡献的热情。开源的一个特点是它使独立开发人员和独立贡献者能够创建创新的解决方案。BSL的采用可能会阻碍这些人使用软件作为基础构建可行的商业产品的能力。这种转变可能导致市场上这类产品的数量减少，从而抑制竞争，限制最终用户的选择。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.sdxcentral.com/articles/news/ibm-acquires-hashicorp-for-6-4b-open-source-terraform-questions-remain/2024/04/">https://www.sdxcentral.com/articles/news/ibm-acquires-hashicorp-for-6-4b-open-source-terraform-questions-remain/2024/04/</a>"</p><p><a href="https://medium.com/@fintanr/on-ibm-acquiring-hashicorp-c9c73a40d20c">https://medium.com/@fintanr/on-ibm-acquiring-hashicorp-c9c73a40d20c</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bKHH3LKgcqp7PsF0UvkQ</id>
            <title>苹果发布OpenELM：专为在设备端运行而设计的小型开源AI模型</title>
            <link>https://www.infoq.cn/article/bKHH3LKgcqp7PsF0UvkQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bKHH3LKgcqp7PsF0UvkQ</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 07:19:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, 开源, AI模型, OpenELM
<br>
<br>
总结: 苹果公司发布了开源的AI模型OpenELM，该模型家族包括预训练模型和指令微调模型，参数规模在2.7亿到30亿之间。OpenELM通过层级缩放策略和公开数据集预训练后微调，实现了Transformer语言模型效果的改进。苹果还发布了OpenELM模型的权重和训练中的不同检查点，但同时强调模型不提供任何安全保证。这次开源举动让苹果加入了开源大模型阵营，与其他企业一同推动大模型技术和代码的开放。 </div>
                        <hr>
                    
                    <p></p><blockquote>今天，苹果破天荒整了个大新闻。</blockquote><p></p><p>&nbsp;</p><p>苹果开源了一个在设备端运行的AI模型OpenELM，同时还公开了代码、权重、数据集、训练全过程。</p><p>&nbsp;</p><p>就像谷歌、三星及微软着力在PC和移动设备端推动生成式AI模型的开发一样，苹果也加入了这一行列。这是一个新的开源大语言模型（LLM）家族，能够依托单一设备平台运行，完全无需借助云服务器。</p><p>&nbsp;</p><p>OpenELM已经于日前在AI代码社区Huggang Face上发布，由多个旨在高效执行文本生成任务的小模型组成。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/790f59803f38f9347f39f9b8035c28d3.jpeg" /></p><p></p><p>苹果投身开源AI战局，在Hugging Face上发布四种新模型！</p><p>&nbsp;</p><p>OpenELM模型家族共有八位成员，其中四个为预训练模型，另外四个为指令微调模型，参数规模在2.7亿到30亿之间（即大模型中人工神经元之间的连接数量，参数越多通常意味着性能更好、功能更强，但并不绝对）。而微软Phi-3模型为38亿。</p><p>&nbsp;</p><p>预训练是让大模型得以生成连续、可用文本的重要方法，而指令微调则能够让模型以相关度更高的输出响应用户的特定请求。具体来讲，预训练而成的模型往往会通过在提示词的基础上添加新文本来完成要求，例如面对用户的“教我如何烤面包”这条提示词，模型可能并不会给出分步说明，反而傻傻回答称“用家用烤箱烤”。而这个问题恰好可以通过指令微调来解决。</p><p>&nbsp;</p><p>OpenELM通过采用层级缩放策略、在公开数据集预训练后微调，实现了Transformer语言模型效果的改进。因此，OpenELM 的transformer layers不是具有相同的参数集，而是具有不同的配置和参数。这样的策略能让模型精度显著提高。例如，在大约十亿参数的预算下，OpenELM的准确率较OLMo提升了2.36%，且预训练所需的Token数量减少了一半。</p><p>&nbsp;</p><p>苹果在其所谓“示例代码许可证”下发布了OpenELM模型的权重，以及训练中的不同检查点、模型性能统计数据以及预训练、评估、指令微调与参数效率调优的说明。网友点评说，“可以说对开发者来说很友好了，毕竟深度网络的很大一部分难点存在参数调节。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b372e1fe853335bb242f88236fe4301e.jpeg" /></p><p></p><p>&nbsp;</p><p>苹果的示例代码许可证并不禁止商业使用或修改，仅要求“如果您以完整且未经修改的方式重新发布苹果软件，则必须在所有此类发布中保留本通知以及以下文本与免责声明。”</p><p>&nbsp;</p><p>该许可不是公认的开源许可证，虽然苹果也没有做过度的限制，但它确实明确表明，如果任何基于 OpenELM 的衍生作品被认为侵犯了其权利，苹果保留提出专利索赔的权利。</p><p>&nbsp;</p><p>苹果公司还进一步强调，这些模型“不提供任何安全保证。因此，模型可能会根据用词提示词生成不准确、有害、存在偏见或者令人反感的输出。”</p><p>&nbsp;</p><p>OpenELM只是苹果公司发布的一系列令人惊讶的开源AI模型中的最新一批。去年10月，苹果方面曾悄然发布具有多模态功能的开源语言模型Ferret，迅速引起各界关注。</p><p>&nbsp;</p><p>目前，大模型领域主要分为开源和闭源两大阵营。闭源阵营的代表企业包括 OpenAI、Anthropic、谷歌、Midjourney、Udio、百度、科大讯飞、出门问问、月之暗面等。开源阵营的代表企业包括 Meta、微软、谷歌、百川智能、阿里巴巴、零一万物等。这些企业致力于开放大模型的技术和代码，鼓励开发者和研究人员参与模型的开发和改进。</p><p>&nbsp;</p><p>苹果长期以来一直以神秘莫测、对外“封闭”而闻名，本次却罕见地加入开源大模型阵营。以前，除了在网上发布模型和论文之外，苹果并未公开宣布或者讨论其在AI领域的探索。</p><p>&nbsp;</p><p></p><h2>关于OpenELM，我们了解什么？</h2><p></p><p>&nbsp;</p><p>尽管OpenELM（全称为开源高效语言模型）才刚刚发布、尚未进行过公开测试，但苹果在Hugging Face上指出其目标是在设备端运行这些模型。这明显是在紧跟竞争对手谷歌、三星和微软的脚步——微软本周刚刚发布了能够纯在智能手机端运行的Phi-3 Mini模型。</p><p>&nbsp;</p><p>在arXiv.org上发表的一篇模型阐述论文中，苹果表示OpenELM的开发“由Sachin Mehta领导，Mohammad Rastegrai与Peter Zatloukal则额外做出贡献”，该模型家族“旨在增强并赋能开放研究社区，促进未来的研究工作。”</p><p>&nbsp;</p><p>苹果的OpenELM模型分为四种规模，分别拥有2.7亿、4.5亿、11亿与30亿参数，各模型均比现有高性能模型更小（通常为70亿参数）且各自拥有预训练与指令微调两个版本。</p><p>&nbsp;</p><p>这些模型的预训练采用来自Reddit、维基百科、arXiv.org等网站总计1.8万亿tokens的公共数据集。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/21/217c59fecc6d7b2ba75804d0f1a9ceb4.jpeg" /></p><p></p><p>&nbsp;</p><p>OpenELM模型适合在商用笔记本电脑甚至部分智能手机上运行。苹果在论文中指出，他们分别在“配备英特尔i9-13900KF CPU、64 GB DDR5-4000 DRAM和24 GB VRAM的英伟达RTX 4090 GPU，运行有Ubuntu 22.04的工作站上”、以及“配备M2 Max系统芯片与64 GiB RAM、运行有macOS 14.4.1的苹果MacBook Pro上”运行了基准测试。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/96701f0f43a13c0524f29c379edebd28.jpeg" /></p><p></p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/bf/bf09a8d4f00fe35cb072cadbb35bdac6.jpeg" /></p><p></p><p>网友测试运行OpenELM模型</p><p>&nbsp;</p><p>有趣的是，新家族中的所有模型均采用分层缩放策略来分配Transformer模型中每一层内的参数。</p><p>&nbsp;</p><p>据苹果公司介绍，这种方式能够提供更加准确的结果，同时提高计算效率。该公司还使用新的CoreNet库对模型进行了预训练。</p><p>&nbsp;</p><p>该公司在Hugging Face上提到，“我们的预训练数据集包含RefinedWeb、去重版PILE、RedPajama的一个子集以及Dolma v1.6的一个子集，总规模约1.8万亿个tokens。”</p><p>&nbsp;</p><p></p><h3>值得肯定，但性能并非顶尖</h3><p></p><p>&nbsp;</p><p>在性能方面，苹果公布的结果显示OpenELM模型相当出色，特别是其中的4.5亿参数版本。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/94/9451865f6ff0e0aed82c5db5eeff2f05.png" /></p><p></p><p>&nbsp;</p><p>此外，11亿参数的OpenELM版本“比拥有12亿参数的OLMo模型性能提高了2.36%，且需要的预训练tokens仅为后者的二分之一。”OLMo是艾伦AI研究所（AI2）最近发布的“真正开源且最先进的大语言模型”。</p><p>&nbsp;</p><p>而在强调测试知识与推理技能的ARC-C基准测试中，经过预训练的OpenELM-3B版本的准确率达到42.24%，同时在MMLU与HellaSwag上分别得到26.76%与73.28%的成绩。</p><p>&nbsp;</p><p>一位参与该模型系列测试的用户指出，苹果的模型成果似乎“稳定且性能一致”，就是说其响应结果并不具备灵活的创造力，也不太可能冒险涉及“不适合上班时浏览”的内容。</p><p>&nbsp;</p><p>竞争对手微软近期推出的Phi-3 Mini拥有38亿参数及4k上下文长度，目前在性能层面仍处于领域地位。</p><p>&nbsp;</p><p>根据最新发布的统计数据，Phi-3 Mini在10-shot ARC-C基准测试中得分为84.9%，在5-shot MMLU上得分为68.8%，在5-shot Hellaswag上得分为76.7%。</p><p>&nbsp;</p><p>但从长远来看，OpenELM肯定还会继续得到改进。目前开源大模型社区对于苹果的加入非常兴奋，也期待看到这位“闭源”巨头如何将其成果引入于各类应用场景。</p><p>&nbsp;</p><p></p><h2>大模型是智能手机的未来</h2><p></p><p>&nbsp;</p><p>手机厂商们都很看好手机上的AI前景。</p><p>&nbsp;</p><p>高通和联发科等公司已推出了智能手机芯片组，可满足人工智能应用所需的处理能力。此前，许多设备上的AI应用实际上是在云端进行部分处理，然后下载到手机上。但云端模型也存在弊端，如推理成本很高，一些 AI 创业公司训练+生成一张图片的成本可能就要一元。而先进的芯片和端侧模型则会推动更多AI应用程序在手机端运行，节省成本的同时，也能给用户带来更好的实时计算能力，从而催生出新的商业模式。</p><p>&nbsp;</p><p>从ChatGPT火爆至今不过一年左右，手机厂商就都已将AI大模型技术落地在自家手机中。</p><p>&nbsp;</p><p>今年三星新发布的 Galaxy S24 系列上搭载了能处理语音、文本、图像的端侧 Galaxy AI。谷歌也发布了一款搭载自家 AI 模型的手机 Pixel 8 系列，该设备搭载了 Gemini Nano。谷歌 Pixel 部门产品管理副总裁 Brian Rakowski 还表示谷歌最先进的大模型也会于明年直接登陆智能手机，“我们在压缩这些模型方面已经取得了相当多的突破。”&nbsp;</p><p>&nbsp;</p><p>国内头部手机厂商也争相布局。小米于去年10月发布了澎湃OS以及小米自研大模型加持的各类应用；vivo 也去年宣布推出了蓝心大模型，并开源了面向手机打造的端云两用大模型 BlueLM-7B；OPPO 也在去年11月发布了安第斯大模型(AndesGPT)，以“端云协同”为基础架构设计思路，推出了多种不同参数规模的模型规格。</p><p>&nbsp;</p><p>今年世界移动通信大会MWC 的一大亮点也是大模型能够在设备本身上本地运行，“这就是最具颠覆性的地方。”&nbsp;CCS Insight 首席分析师 Ben Wood&nbsp;感叹。在这次大会上，还展示了一些未来AI概念手机，比如德国电信和 Brain.ai 完全放弃App而采用 AI 界面的T phone。因此，也有预测认为，随着AI占领我们的智能手机，App时代的终结可能指日可待，<a href="https://mp.weixin.qq.com/s/g07VkmGl3NKvVvpMMAYV-A">从而带来全新的生态和竞争格局。</a>"</p><p>&nbsp;</p><p>手机大模型之战，此前只差苹果，而现在，苹果终于带着它的开源大模型来了。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/">https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/</a>"</p><p><a href="https://www.infoq.cn/article/h2ceezfmjdbo2epareyh">https://www.infoq.cn/article/h2ceezfmjdbo2epareyh</a>"</p><p><a href="https://arxiv.org/abs/2404.14619v1">https://arxiv.org/abs/2404.14619v1</a>"</p><p><a href="https://twitter.com/atropos/status/1783349174702059742">https://twitter.com/atropos/status/1783349174702059742</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/plzCk9c4i5dAj9orBYRy</id>
            <title>我们用机器学习模型协助打击不法行为</title>
            <link>https://www.infoq.cn/article/plzCk9c4i5dAj9orBYRy</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/plzCk9c4i5dAj9orBYRy</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 06:59:15 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 深度伪造, 虚拟币犯罪, 技术团队
<br>
<br>
总结: 2023年基于AI的深度伪造欺诈案件暴增3000%，而涉虚拟币犯罪案件近年虽然在数量上有所减少，但涉案金额却是陡增。技术本无罪，但不当的使用会给个人、家庭、甚至社会带来严重后果。国内有一支协助破案的技术团队，利用手中的技术能力去协助执法部门破获不法案件，与执法部门一起深入具体案例，实地分析。中科链源CDO唐崇麟、数据科学负责人连晓磊在InfoQ《极客有约》中介绍了他们如何利用“技术武器”帮助执法部门维护数字世界安全。 </div>
                        <hr>
                    
                    <p>2023年基于AI的深度伪造欺诈案件暴增3000%，而涉虚拟币犯罪案件近年虽然在数量上有所减少，但涉案金额却是陡增。技术本无罪，但不当的使用会给个人、家庭、甚至社会带来严重后果。</p><p>&nbsp;</p><p>国内有这样一支协助破案的技术团队，他们利用手中的技术能力去协助执法部门破获不法案件，与执法部门一起深入具体案例，实地分析。那么，这样的团队是如何诞生的？他们具体又做了哪些事情？</p><p>&nbsp;</p><p>中科链源CDO唐崇麟、数据科学负责人连晓磊作客InfoQ《极客有约》，详细介绍了他们如何利用“技术武器”帮助执法部门维护数字世界安全。以下文字根据直播内容整理。</p><p>&nbsp;</p><p></p><p></p><p>InfoQ：当时为什么会想要去做AI区块链安全赛道？</p><p>&nbsp;</p><p>唐崇麟：我非常相信区块链技术会带来重大变革。区块链是2008年在金融危机的大背景下提出的一个理念，并在后续形成了比特币链的技术，这个理念是出于对中心化、私立金融机构的不信任，想要打造一个去中心化、相信算法算力的机制。我非常认可这个理念。</p><p>&nbsp;</p><p>但可惜的是，相较移动互联网十几年发展，区块链技术诞生15年后还处于小众和边缘状态。区块链技术不能普及的原因，首先是缺乏能给用户带来真实收益和价值的killer APP或功能，这个大家都在努力解决。但更重要的是，区块链的安全问题一直没有得到解决。钱包、交易所被盗造成的产品损失并不鲜见，犯罪分子也常利用比特币、以太坊、USDT作案，这些让人总觉得这个新生事物常和网络犯罪融合在一起，导致其被妖魔化。</p><p>&nbsp;</p><p>安全问题得不到解决，会极大地阻碍整体区块链技术的普及。在这样的背景下，我们公司团队先前在大数据和AI机器学习算法方面的积累非常适合解决区块链的安全问题，可以说是找到了一个用已有的知识沉淀优势在新领域赛道发展的好机会，再加上前景广阔，于是我们就开始做这项工作了。</p><p></p><h3>为什么网络违法案件难破获？</h3><p></p><p>&nbsp;</p><p>InfoQ：最近AI诈骗开始盛行，执法部门需要进行资金追溯。而先前Web3行业涉虚拟币的不法行为也有很多，这类违法案例目前趋势如何？</p><p>&nbsp;</p><p>连晓磊：&nbsp;AI的盛行确实让AI诈骗增多，而涉虚拟币犯罪行为也一直是执法部门关注的重点。关于这两种违法案件的数量和趋势，可以从以下几个方面来看：</p><p>&nbsp;</p><p>首先，据相关数据统计显示，2023年基于AI的深度伪造（deepfake）欺诈案件暴增3000%，基于AI的钓鱼邮件增长了1000%，已有多个有国家背景的APT组织（高级持续性威胁，是一种针对性、隐蔽性、持续性都极强的网络攻击手段）利用AI实施了十余起网络攻击事件。</p><p>&nbsp;</p><p>随着AI技术的快速发展，不法分子利用AI技术进行诈骗的手法也日益翻新。他们通过深度伪造、语音合成等技术，制作逼真的视频通话或语音信息，诱使受害者上当受骗。</p><p>&nbsp;</p><p>其次，Web3行业涉虚拟币的不法行为也呈现出增长态势。他们利用虚拟币的匿名性和去中心化特点，进行洗钱、贩毒、非法集资等不法活动。据SAFEIS安全研究院统计，2023年涉虚拟货币案件总量428件，较2022年减少88.9%，但整体涉案金额陡增至4307.19亿元，约是22年的12.36倍。可见，涉虚拟币犯罪案件在数量上呈现减少趋势，但是整体涉案金额是陡增的态势。</p><p>&nbsp;</p><p>总之，AI诈骗和Web3行业涉虚拟币的不法行为是当前和未来一段时间内，执法部门需要重点关注和打击的对象。我们需要加强技术研究和监管力度，以应对这些新的挑战。</p><p>&nbsp;</p><p>InfoQ：为什么虚拟币会成为黑灰产温床？</p><p>&nbsp;</p><p>唐崇麟：传统Web2领域的黑灰产都是通过银行转账进行，需要身份验证，追查相对容易些，再加上很多银行都有反欺诈、反洗钱的基础设施模型，黑灰产难以在其中生存。但在Web3的区块链领域则给大家打开了一个完全崭新的世界，其中没有什么监管，人人都是匿名，交易成本低但效率又很高，是黑灰产天然非常喜欢的环境。</p><p>&nbsp;</p><p>InfoQ：破解这种网络上不法案件都有哪些难点？技术能力如何帮助破获相关案件？</p><p>&nbsp;</p><p>唐崇麟：网上违法案件的破获难点整体来说有三大部分。</p><p>&nbsp;</p><p>首先是法律流程方面。对现有公安和法律体系来说，涉虚拟币的网络犯罪是一个新的领域，取证工作、适用法条、立法工作以及案件侦破后的流程和审判等，很多环节之前从没遇到过的。在这方面，技术能够解决的其实并不多。我们更为看重的是另外两大部分。</p><p>&nbsp;</p><p>其一，由于这类案件涉及的数据非常庞大且复杂，链上钱包生成免费且非常便捷，甚至有工具可以批量生成，因此犯罪分子会为了隐藏自己的资金和踪迹，批量生成钱包，并通过这些钱包互相转移资金。这种方式效率很高，手续费也很低，相比传统银行金融体系的资金转移会便捷很多，从而形成了一个巨大复杂的交易结构网，侦察工作难度很高。</p><p>&nbsp;</p><p>其二，来自区块链天然的匿名性。先前提到的钱包生成时不需要任何身份证明，所以我们即使找到了非常实锤的涉案地址，也是不知道背后的真实人员，所以我们需要继续侦破，直到发现涉案的资金在某些环节被兑换成法定货币、进入银行体系，我们才能得知背后的真实人员。</p><p>&nbsp;</p><p>但也正因为整体数据量庞大，且需要非常精准地查找线索，也为机器学习、大数据的技术应用提供了非常好的土壤。这也是我们的优势所在：利用大数据和机器学习对复杂交易网络进行分析识别并提升精准度。</p><p>&nbsp;</p><p>InfoQ：那你们和执法部门如何合作完成一次打击行动？能否分享一些案例？</p><p>&nbsp;</p><p>连晓磊：涉虚拟币犯罪上游的犯罪类型有网络赌博、诈骗、传销、洗钱、黑灰产等，这些都是我们帮助执法机关重点打击的犯罪类型。我们可以为执法机关提供从数据挖掘、立案、侦察、收网、司法鉴定、起诉、审判、资产处置一站式全流程服务。</p><p>&nbsp;</p><p>今年一月份，青岛市公安局和国家外汇管理局青岛市分局联合破获了一起特大地下钱庄案，该案件入选公安部经侦局，列为全国公安经侦系统“夏季行动”打击地下钱庄“十大战役”。这起地下钱庄案件涉案金额高达158亿元，抓获犯罪嫌疑人74人，涉及全国17个省及直辖市。</p><p>&nbsp;</p><p>其中，我们自主研发的SAFEIS安士系统在涉虚拟币犯罪固证环节发挥了关键作用，通过对涉案地址的追踪分析，验证了虚拟货币交易信息与银行账户资金流向的线索相互吻合，为侦查人员提供了强大的技术支持。</p><p>&nbsp;</p><p>在该案件的资金往来分析过程中，办案人员发现犯罪嫌疑人之间的资金流存在异常，有涉嫌虚拟币交易的特征。随着对犯罪嫌疑人所涉及的银行账户资金流向进行了深入研判，我们发现资金交易模式与虚拟货币买卖行为相符。最终通过SAFEIS成功挖掘到犯罪嫌疑人的作案证据。侦查人员还通过SAFEIS输出的资金流向层级，对整体资金和虚拟货币犯罪团伙的群组关联结构进行了详尽刻画，进一步明确了嫌疑人参与虚拟币犯罪的相关信息。</p><p>&nbsp;</p><p>InfoQ：为什么银行或支付宝这种金融机构不给资金上链，让它们可以被追溯？</p><p>&nbsp;</p><p>唐崇麟：我们所说的区块链是指公链，比如比特币链、以太坊链、波场链、币安智能链等。公链秉承的是抗审核、匿名性和去中心化原则，这是和现实中金融机构所形成的银行网络完全隔离的两个世界。</p><p>&nbsp;</p><p>如果一个金融机构想要将用户的资金上链，就需要将用户存放的法定货币一对一转换成链上的USDC稳定币或其他代币后，再进行上链。这样一来，银行机构就行使了交易所的职责，但我国法律是禁止银行机构进行虚拟货币经营的，这种业务是违法的。</p><p>&nbsp;</p><p>我国现在推行的电子人民币是将人民币变成电子货币，并在可监管的环境中更方便的使用，这种电子货币和公链的区块链是不同的。</p><p>&nbsp;</p><p>InfoQ：Web3时代的网络安全跟Web2时代的安全问题有什么差别？</p><p>&nbsp;</p><p>唐崇麟：Web3时代的交易网络和数据量更为复杂，整体安全问题的追踪难度更大。但我觉得应该换一个角度看。</p><p>&nbsp;</p><p>Web3和Web2时代安全问题有很大的差别。Web3的本质是利用去中心化的区块链技术，要求App开发商和钱包用户都对自己的行为承担更大的责任。而Web2时代的滴滴、字节和各种银行等都是平台中心化的机构。本质上，做平台就要承担平台上安全的责任，比如银行存款被盗是能得到赔付的，但Web3时代是没有的，钱包密钥自己保管，如果被钓鱼或者胡乱签名导致资金被盗，那么你找不到一个中心化机构寻求赔付，甚至现实世界中都没有法律能够保护这些资产，这是最大的问题。</p><p>&nbsp;</p><p>也就是说，为什么Web3用户会胆战心惊，因为既没有机构能帮忙主持公道，很多技术又是全新的，里面的漏洞没人能说得清，大家都是摸着石头过河。比如，Web3体验大多是基于智能合约进行交互，但智能合约也是在近几年才出现，其中的逻辑漏洞大多都没有暴露过，有非常多的不法分子会监控每一个新上线的智能合约，特别是金融类合约，一旦上线便蜂拥而至寻找其中的漏洞，并在找到后的第一时间利用它窃走资金。</p><p></p><h3>特别的技术团队</h3><p></p><p>&nbsp;</p><p>InfoQ：像中科链源这种是针对违法案件分析和研发的，你们的团队和普通的互联网研发团队有什么不同？你们内部团队之间如何协作？</p><p>&nbsp;</p><p>唐崇麟：我之前曾在滴滴负责过网约车整体策略平台的搭建，在我看来，中科链源现在的工作与传统互联网平台中的数据产品团队有很强的相似性，但也存在一些不同。</p><p>&nbsp;</p><p>首先从数据来源看，一般互联网平台公司的用户行为等所有数据，都是通过自己APP产生的，公司的大数据团队对这些数据了解非常深。但中科链源在做分析时，所有数据都是在公链上产生的，不受我们控制，而且每一条链上的数据存储特性都不相同，要找到数据后才能进行下一步的处理和分析。</p><p>&nbsp;</p><p>其次是，案件的分析非常主观且依赖办案人员的经验，很多分析结果也只是猜测，很难对其真假与否进行快速验证并打上正确标签，这就要求我们和整体的办案团队有非常深入的融合。</p><p>&nbsp;</p><p>在我们初期做模型时，要求大家把自己当作是分析师，一起深入某个案件、实地分析。我们核心团队的几个同学在入职后都是做了半年以上的分析师工作，再开始做模型的，他们对业务的要求和理解非常深。</p><p>&nbsp;</p><p>最后是对模型的选择。也提到取得量化的标签，我们会倾向于一些图计算的模型和无监督的聚类模型，我们也更强调从基础层面来建设特征的标签。</p><p>InfoQ：你们现在的AI底层支持系统都包括哪些？这个底层系统具体是如何搭建并帮助你们做案件侦破的？</p><p>&nbsp;</p><p>连晓磊：我们的AI系统有三个部分：数据平台（数据特征模块）、模型训练平台（负责训练模型），以及模型服务平台（部署G端应用的对接服务和一些自己微服务）。这三个平台会应用于数据的探查阶段、模型的训练研究阶段，以及模型的部署使用阶段。</p><p>&nbsp;</p><p>数据特征平台方面，算法领域的数据是非常重要的因素，我们将各个链上的数据接入到了我们的大数据平台上，再进行各种数据的清洗和数据仓库的特征提取，从而保证数据的高可用性，另外还使用离线+实时数据来保证时效性。</p><p>&nbsp;</p><p>数据训练平台方面，我们用来研发较大的神经网络模型和时序模型所需要的分布式计算资源及GPU资源，都会用到这个平台。</p><p>&nbsp;</p><p>模型部署平台则是负责模型的部署和版本控制的平台，版本控制可以在模型预测错误时依然保证预测的时效性。此外，我们还有监控系统，在模型指标等不达标时，及时对模型进行更新，从而保证模型的效果。</p><p>&nbsp;</p><p>InfoQ：期间有经历什么比较印象深刻的事件吗？</p><p>&nbsp;</p><p>连晓磊：说起印象深刻的事，就是有一次给大家做图模型的理论分享，完事后有个小伙就拿手里现有的案件数据去练手，结果发现一些涉案地址和实锤的入金归集地址在一个类簇里面，于是催生了现在上线的图聚类服务。</p><p></p><h3>如何用技术协助破案</h3><p></p><p>&nbsp;</p><p>InfoQ：2020年公司成立至今，你们在机器学习模型上做了哪些大的迭代更新吗？</p><p>&nbsp;</p><p>唐崇麟：虚拟币行业较新，数据量也很大，因此我们在模型探索过程中没有像传统Web2公司一样直接进行标注和模型训练，而是先进行了一定的探索。这些探索大体可以分为三个阶段。</p><p>&nbsp;</p><p>第一阶段，借鉴人的经验做自动化，换句话说是首先提升人的效率。在这一阶段，我们先把复杂的交易关系用图计算网络再现成网状结构，再将网状结构中的交易信息，与办案人员经验提取出的特征和规则相结合，形成未经过太多训练的简单规则模型，其中大概有近百个带权重的特征。我们对两个地址点之间的关系做出统计行的概率预测，概率越高地址之间的关联性就越强。这样能帮助办案人员完成很多工作，因为他们平时办案中最主要的工作就是从一个较为实锤的涉案地址中，找到其他上下游强关联的涉案地址，这个工具能帮助他们很好完成任务。</p><p>&nbsp;</p><p>有多年办案经验的分析师平时在做分析时只能思考三至五个特征，逐个排查时非常耗时。但在有了模型之后，近百个特征可以同时计算，下游网络中数十万的地址也能同时进行计算，几周的工作可以在几小时内完成，极大地提升了人的工作效率。</p><p>&nbsp;</p><p>第二阶段，我们利用了更为复杂的无监督聚类模型和谷歌的PageRank算法，基于整体交易网络中的交易行为特征，找出交易网络中行为较为相似、犯罪链条中功能较为相似的地址，用类似社交网络分析的形式，找到嫌疑人之间的相似关系。这样协助办案人员梳理整体犯罪链路的每个环节和地址，方便日后取证。</p><p>&nbsp;</p><p>第三阶段，虽然已经借助前面提到的两大模型大幅提升了效率，但在很多基础的具体特征标签上，我们的建设还是较为稀缺。因此在这个阶段，我们化整为零，逐一寻找极具特色的地址并打上标签。这样一来，办案人员看整体网图时就能得知其中地址的详情和特征，我们后续迭代模型或搭建新模型时也能利用这些特征，从而达到一举两得的功效。我们相信，未来地址特征标签体系能和更多的模型一起，丰富我们的整体工具箱并提升办案效率。</p><p>&nbsp;</p><p>InfoQ：具体常用的特征有哪些呢？</p><p>&nbsp;</p><p>唐崇麟：常用特征主要是交易相关，比如时间属性（高低频次、周期性等）、金额属性（大额或小额、是否是定时周转固定金额等）、交易行为特征（交易对象是否频繁、交易关系是否复杂等）。</p><p>&nbsp;</p><p>连晓磊：具体来说，时间上我们采用的特征会包括地址、base属性、创建时间、地址对的最大最小交易时间；金额上有地址对之间的汇总交易金额、最大最小交易金额均值标准差等统计特征；至于交易关系，特别是在波场链上，我们利用TRX激活关系的特征捕捉下游涉案金额的转入概率。</p><p>&nbsp;</p><p>InfoQ：是基于马尔科夫模型这些来做的吗？</p><p>连晓磊：并不完全是。因为要构建马尔科夫链需要高质量的观测数据，并且对Label也有一定的敏感性。在交易所官方选择对地址发起调证的时候，这其中是有一些人为主观的判断因素，选择调证的地址也不一定是正样本，所以我们并没有完全使用马尔科夫模型进行训练，但在下游的分析思路上借鉴了马尔科夫思想，为每条链路上的条件概率人为制定具体分数。</p><p>&nbsp;</p><p>注：马尔可夫模型是一种统计模型，它基于马尔可夫性质，即一个给定过程的未来状态仅取决于当前状态，与之前的状态无关。马尔可夫模型可以应用在多个领域，如自然语言处理、算术编码等。</p><p>&nbsp;</p><p>InfoQ：可以展开讲讲，图聚类模型在产品里的应用吗？</p><p>&nbsp;</p><p>连晓磊：图聚类模型其实基于一个确定的地址，比如案件中的路径归集地址，利用大数据平台向下开展更多的节点，从而形成一个网图数据，这样就可以用图聚类的方法圈选出一些涉案概率更大的地址。简单来说，其实是借鉴Facebook或者Twitter这些社交媒体的思想，对用户进行聚类和兴趣社群的挖掘，同一个类簇可以理解为是在同一个兴趣圈子。</p><p>&nbsp;</p><p>我们也是以交易为边进行兴趣爱好模式的挖掘，同一个聚类里的地址可能联系更为紧密、交互更为频繁。除了圈选出类簇，我们还会在类簇的基础上对节点进行中心度计算，提取出一个团伙中更为重要或关键的核心地址。</p><p>&nbsp;</p><p>我们后续在研究角色相关的挖掘或role-based embedding模型的角色建模时，也会用到图聚类，比如同属跑分车队的两个地址可能之间没有联系，但在某些交易行为或模式上具备相似性，那我们就认为这两个地址较为相似。</p><p>&nbsp;</p><p>InfoQ：图聚类模型一般应用在什么场景比较多？它的特点是什么？</p><p>&nbsp;</p><p>连晓磊：图聚类最多的应用场景其实是社交网络的挖掘，筛选出相同兴趣爱好的用户；其他也有生物医学方面的应用，比如药物关联度的挖掘，将老药混合构建出新药并用于抵抗当前的某种疾病，这种其实也是以药物为节点做向量化embedding和图聚类，圈选出哪些药物的某种特性下在某些生物反应上会有相同的模式；金融行业的下游分析、犯罪团伙分析或金融上交易模式的研究，都可以有所应用；交通类则可以联合时序分析和图聚类，对交通生活进行类似的挖掘探索。</p><p>&nbsp;</p><p>唐崇麟：我之前在字节时接触到的视频平台会有非常多的图聚类模型应用，主要用来扩展用户的视频观看、探索用户的边界，比如喜欢看vlog或美食类节目视频的用户就可能会通过评论或视频的关系聚类到一起。平台希望用户能扩展更多的兴趣领域，因此通过图聚类他们可能会被归纳到的其他兴趣圈层，这样对他们进行相关的推送就有可能扩展他们的观看领域。</p><p>&nbsp;</p><p>我们也在探索图聚类的其他应用，比如对全网涉案地址进行聚类后，有些涉案地址可能会和我们之前没有标注过的地址非常接近，那么这些新的地址也会帮助我们找到案件的新线索。</p><p>&nbsp;</p><p>此外，很多技术领域也有对图聚类模型的应用。比如先前提到的问答模型、ChatGLM检索，在将文本的语义向量存入到向量数据库后再进行聚类，每个类簇都包含许多文本向量和一个中心点作为索引，从而构建了一个基于向量数据库的倒排索引，从而实现速度的提升。在收到用户问题后，我们先将问题向量化，再和各个类簇的中心向量做相似度比较，相似度高的则归纳仅类簇后再和其他类簇的向量进行相似度匹配。</p><p>&nbsp;</p><p>InfoQ：据了解，模型数据来自中科链源自建的以太坊、币安智能链和波场链全节点和一些第三方数据，为什么选择这三条链？具体如何收集和处理这些数据的？</p><p>&nbsp;</p><p>唐崇麟：这三条链的选择其实与犯罪行为密切相关。波场链已逐渐演变为网络涉币犯罪中最为核心的一条链，其上的USDT交易效率高、转账手续费低廉，是犯罪分子的首选。以太坊中的DX应用极多、用户广泛，也是非常好的一个选择。币安智能链的合约部署丰富、技术成熟，在华人圈的普及率也很高，也是犯罪分子常用的一条链。</p><p>&nbsp;</p><p>搭建了三条链的全节点后，接下来要做的就是数据的收集和处理。在整体的大数据架构中，我们需要兼顾两个方向。</p><p>&nbsp;</p><p>一是实时分析：用户希望链上数据能有秒级别的更新，为此我们利用StartBox实时数据库组件、Kafka及Spark等大数据组件搭建了一套实时系统，其中存储了大量raw data供模型运算和特征计算；</p><p>&nbsp;</p><p>二是离线分析：我们的实时数据库和离线数据库中间通过任务相连接，从而构成一个高级的Lambda实时和离线数据库架构。至于数据的清理和处理，我们也搭建了完善的全流程全生命周期监控体系，确保数据完整准确、实时高效。</p><p>&nbsp;</p><p>有了数据，我们可以通过产品将整体交易网络呈现给用户，允许用户直接通过网图对可疑涉案地址进行查找分析。此外，我们也通过模型为用户提供助力，比如模型可以在一小时内遍历一个犯罪起始点下游的数十万地址点，并将最为可能的十个点标记在图上供用户查阅，从而极大地提升了效率。</p><p>&nbsp;</p><p>InfoQ：资料显示，中科链源在自动查找目标地址方面，原来要用两周的时间现在缩短到了20分钟，这期间主要做了哪些改进？</p><p>&nbsp;</p><p>连晓磊：案件处理的提速主要依赖于机器性能和模型能力的提升。</p><p>&nbsp;</p><p>人为的地址分析无法遍历全量的下游节点，无法保证召回率。人为地址分析的速率也不如机器，机器可以基于自动化逻辑进行海量地址的概率和分数计。此外，模型的能力也对案件处理提速有一定优势，我们公司内的分析师团队学习优秀的分析经验，构建出相对较好的模型，再加上机器的快速计算能力，从而得到更好的预测结果。</p><p></p><h3>加入大模型能力</h3><p></p><p></p><p>InfoQ：中科链源之前在<a href="https://www.infoq.cn/article/lq2fJ7gm9iyuOLkaLdqv">InfoQ分享</a>"过选择了智谱的ChatGLM-6B模型，那当时你们对大模型有什么选择标准，最后又为什么选择ChatGLM-6B？</p><p>&nbsp;</p><p>连晓磊：这一个问题其实可以分为两个方面来回答，一是模型的选择，二是工具的选择。</p><p>&nbsp;</p><p>首先是模型的选择。我们在体验了一些法律领域的大模型，如LawGPT、ChatLaw等后，总体感觉和通用的ChatGLM差不多，即使ChatLaw在交易方面相对更好，也因为其没有开源的预训练参数，导致我们无法直接使用。LawGPT在法律方面的问题回答表现不错，但其检索问答的能力是借助的RAG（检索增强式问题生成），其效果不如ChatGLM通用大模型。</p><p>&nbsp;</p><p>此外，当时虽然也有ChatGLM的二代模型，但其架构上大体没有变化，只是支持更快的推理速度和更长的上下文，再加上当时对一代的模型应用更多，我们最终选择直接部署一代ChatGLM。当然，我们后续也陆续更新到了目前最新的模型ChatGLM 3。</p><p>&nbsp;</p><p>再说工具的选择。我们考虑过LangChain、LlamaIndex，以及国内的FastGPT，但这些工具的集成度相对较高，且没有集成向量数据库功能，因此我们直接自己搭建了一套框架，在RAG中了添加数据清洗、判例文档核心句子提取、意图识别等自定义环节，也搭建了用于检索的向量数据库。</p><p>&nbsp;</p><p>InfoQ：实际上在研发阶段，中科链源使用了垂直行业的ChatLaw-Text2Vec模型，这两种模型如何分工？基座模型（ChatGLM-6B）和垂直模型混合应用会得到更好的效果吗？</p><p>&nbsp;</p><p>连晓磊：从模型分工来说，ChatLaw-Text2Vec和ChatGLM在架构中是串行进行的；ChatLaw-Text2Vec负责检索，将用户输入的问题转变为向量，再从向量数据库中检索出和用户问题语义相似度较高的法律判例文档，最后将问题和检索到的文档一同作为instruction输入到ChatGLM中，从而实现问题问答，类似于将原先的开放式大题变成了选择题或阅读理解题目。</p><p>&nbsp;</p><p>再说基座模型和垂直模型的混合应用，ChatLaw-Text2Vec是专门针对法律问答方向的向量化embedding模型，使用了90多万条高质量中文法律问答的句子对作为训练数据，天然适合法律领域的问答语义相似度计算。</p><p>&nbsp;</p><p>我们也尝试过其他如Text2Vec模型，但在法律场景中的表现均逊于ChatLaw-Text2Vec，因此，我们最终的检索模型使用了垂直领域的Text2Vec，但问答还是采用通用的ChatGLM。</p><p>&nbsp;</p><p>InfoQ：中科链源推出的to G的SAFEIS安士信息作战系统是怎么做架构搭建的？to G系统的研发与to B\ to C的软件系统研发有什么不同？</p><p>&nbsp;</p><p>唐崇麟：前面提到主要模块其实就是to G的安士信息作战系统，是专为执法机构提供的、打击涉虚拟币犯罪的一站式综合查控平台，平台涵盖资金分析追踪、地址监控、智能分析研判等核心功能。基于此，产品的架构搭建一方面要利用图计算模型、图聚类模型、OLAP型数据库处理分析海量的链上数据，另一方面要利用网状图、树状图系统对分析结果做图形化的呈现。</p><p>&nbsp;</p><p>to G系统的研发与to B \ to C的软件系统研发的不同之处，主要是因为G端、B端、C端使用场景存在着非常大的差异。以我们公司的业务举例，为了保障数据安全，我们经常要将产品部署到公安的内网中，这就要求SAFEIS安士要有私有化部署的产研经验，让我们的产品更符合公安的使用标准要求。</p><p>&nbsp;</p><p>InfoQ：AI技术在产品中的应用方面，中科链源团队还有哪些在筹备中或者是规划中的技术呈现吗？</p><p>&nbsp;</p><p>唐崇麟：我们的工作不是为了使用最前沿的技术，而是要切合业务需要选择技术，比如要如何更快地发现更多线索，如何在新犯罪团伙或犯罪行为出现时尽快发现。基于这样的需求，我们的发展有三大方向：</p><p>&nbsp;</p><p>利用图神经网络：基于我们积累的涉案地址进行发散，探索网络中是否存在新的异常交易行为，有的放矢地查看是否存在新犯罪团伙或新案件线索；利用图聚类技术：找到新的涉案嫌疑人地址和相关线索；利用生成式AI：对智能合约源码进行分析，传销诈骗类智能合约可能存在规律，那么直接监控新部署智能合约并利用生成式AI进行文本分析，可能带我们找到涉案的可疑合约。</p><p>&nbsp;</p><p>InfoQ：最后，要不要给大家一些防诈骗小技巧？</p><p>&nbsp;</p><p>连晓磊：技术手段在预防不法行为方面发挥着很重要的作用，对敏感数据的加密可以预防未经授权访问导致的数据泄露，利用机器学习技术和上面提到的各种模型进行数据分析和异常模式识别，可以预测一些潜在的不法行为，为执法部门提供相对有价值的线索。</p><p>&nbsp;</p><p>但最核心的还是提高个人的防范意识，谨慎对待可疑电话短信，不要轻易泄露个人信息或进行转账操作，保护个人信息不泄露身份证和银行账户等数据信息。最后就是要警惕电信诈骗手段，防范不法分子利用DeepFake、语音视频合成等较为先进手段，冒充公检法机关、亲友急需资金等骗局。</p><p>&nbsp;</p><p>唐崇麟：现在虽然也有很多工具提供智能合约的安全检测，但个人来说预防诈骗还是要在心理上建立防线。在遇到任何暴富机会前，都要想想自己究竟是不是别人眼里的韭菜、要不要这么冲动；钱包密钥一定保管好，写在纸上不要放到网上，不要将密钥透露给任何人；既然钱包免费生成，那我们可以生成多个钱包、专款专用，主力钱包不要和可疑合约进行交互，额外生成一个专门用于和各种DMS进行交互的钱包，这样即使这个钱包不小心在授权后被清空，损失只是这一个钱包，风险隔离非常重要。</p><p>&nbsp;</p><p>此外，我们还要对区块链的授权、加密、签名等操作的原理有所理解，明白即使受骗后也没有银行可以去申诉索赔，这样应该也会对自身的安全意识有所增强。</p><p>&nbsp;</p><p>嘉宾介绍：</p><p>&nbsp;</p><p>唐崇麟，在移动互联网领域有十余年的工作经验，曾任职于Uber、滴滴负责运营、策略、算法领域的工作，同时搭建了网约车的策略分析平台，加入字节后也是负责算法、策略相关的工作。目前在中科链源，负责大数据和算法相关的工作。</p><p></p><p>连晓磊，曾在好未来和理想汽车工作，主要的研究方向是营销增长的模型以及自然语言处理相关的工作。目前在中科链源主要是负责区块链安全算法相关的研究工作。</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OHyctDFEJ6RmVzed6sdj</id>
            <title>国内大模型五虎融资仅是巨头零花钱？谷歌、微软、Meta：每季度拿不出100亿美元别玩AI</title>
            <link>https://www.infoq.cn/article/OHyctDFEJ6RmVzed6sdj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OHyctDFEJ6RmVzed6sdj</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 06:49:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Meta, 谷歌, 微软, 人工智能
<br>
<br>
总结: 三大科技巨头发布财报，Meta表示资本支出将增长，但投资者态度不佳；谷歌和微软股价上涨，Alphabet首次分红，AI技术发展增加运营成本。 </div>
                        <hr>
                    
                    <p>“猪突猛进”这么久，三大厂商交了“季考答卷”。</p><p>&nbsp;</p><p>这两天，Meta、谷歌、微软陆续发布了最新财报。三个大厂都表示今年的资本支出将增长：Meta全年上升至350亿到400亿美金、谷歌每个季度将花费约120亿美金或更多、微软最近一个季度的资本支出为140亿美金，预计将“大幅”增加。</p><p>&nbsp;</p><p>但是，投资者对三家的态度却非常不同：“Meta跌逾10%”上了热搜，而谷歌母公司Alphabet与微软的股价日前在盘后交易中上涨。到底发生了什么？</p><p>&nbsp;</p><p></p><h2>小扎：开场白都在说赔钱</h2><p></p><p>&nbsp;</p><p>一点也不意外，扎克伯格在Meta财报电话会议上谈到了人工智能。他花了很多时间谈论Llama 3和<a href="https://www.cnbc.com/2024/04/18/meta-ai-assistant-comes-to-whatsapp-instagram-facebook-and-messenger.html">最近推出</a>"的人工智能助手 Meta AI。然而，随后他便转向了元宇宙，开始兜售公司的耳机、眼镜和操作系统。他的开场白几乎全部都集中在Meta赔钱的许多方式上。</p><p>&nbsp;</p><p>Meta 98​​% 的收入来自数字广告，但扎克伯格谈及该话题还是在展望未来，以及公司如何将当前投资转化为广告收入。在讨论 Meta 打造“领先人工智能”的努力时，他表示，“有多种方法可以在这里建立庞大的业务，包括扩展业务消息传递、在人工智能交互中引入广告或付费内容。”</p><p>&nbsp;</p><p>这不是投资者想听的。当地时间周三，Meta股价在盘后交易中暴跌19%，市值蒸发逾2,000亿美元。</p><p>&nbsp;</p><p>扎克伯格似乎也已经做好了准备。“我们的股票在产品战略阶段出现了很大的波动，我们正在投资扩展一个新产品，但还没有货币化。”</p><p>&nbsp;</p><p>乍看下来，Meta 2024年第一季度的业绩表现不俗：收入同比增长27%至365亿美元；营业利润几乎翻倍，攀升91%至138亿美元；利润率也由去年的25%跃升至38%。Meta也不乏令分析师们保持乐观的统计数据：截至2024年3月31日，公司员工已减少10%、降至69329人。</p><p>&nbsp;</p><p>然而，Meta 预计，VR部门Reality Lab不仅第一季度运营亏损高达38亿美元，而且亏损态势还将持续。在整个2024年内，Meta 的资本支出也将由300至370亿美元增长至350亿至400亿美元。</p><p>&nbsp;</p><p>扎克伯格表示，“我们将继续加快基础设施投资，以支持我们的人工智能路线图。” Meta公司并没有发布2024年之后的预测，但提到“明年的资本支出将继续增加，因为我们将积极投资以支持公司雄心勃勃的AI研究与产品开发工作。”</p><p>&nbsp;</p><p>对投资者们的第二记重击来自分析师电话会议。扎克伯格在会上预测，还需要一段多年投资周期，Meta的AI业务才能发展为他所期望的“盈利服务”。</p><p>&nbsp;</p><p>Meta 首席执行官 Susan Li 补充称，公司需要开发先进的模型并扩展产品，然后才能带来有意义的收入。“虽然长期潜力巨大，但我们在回报曲线上还处于早期阶段。”Li 说。</p><p>&nbsp;</p><p>这显然也不是分析师们想听到的，但这也让市场终于意识到，AI领域还需要一段时间才能抵偿当初砸下的巨额融资。因此，该公司股价在盘后交易中遭受重创。当日收盘价为493.50美元，跌幅超过14%。</p><p>&nbsp;</p><p>科技行业的其他企业也都在AI身上砸下了重注，更是争先恐后围绕此议题展开炒作。例如，微软已经向OpenAI投入数十亿美元，并将其技术带入一系列自家产品，努力让客户相信其中的价值。</p><p>&nbsp;</p><p>因此，Meta对于AI技术现状的实诚态度（总结来讲，就是进展顺利，但还需要更多资金和时间才能在盈利层面获得实质性进展）很可能给华尔街乃至更多行业敲响警钟。</p><p>&nbsp;</p><p>或许有元宇宙的赌注未能如期获得回报的前车之鉴，此番股价下跌也可能透露出一条明确的信号：在让AI技术践行回报承诺方面，投资者们不希望等Meta太久。</p><p>&nbsp;</p><p></p><h2>谷歌、微软笑嘻嘻：涨了涨了</h2><p></p><p>&nbsp;</p><p>对比非常明显，Alphabet与微软的股价日前在盘后交易中上涨，投资市场明显对这两家热衷AI技术的企业那高于预期的季度收益表示满意：截至收盘，微软目前股价上涨4.3%，来到每股416.25美元；谷歌母公司股价上涨11.4%，达到每股176美元。</p><p>&nbsp;</p><p></p><h4>谷歌：首次分红，成本上涨因为有信心</h4><p></p><p>&nbsp;</p><p>Alphabet公布2024年第一季度营收为805亿美元，同比增长15%。净利润达到237亿美元，增幅为53%，均摊后每股收益为1.89美元。消息公布后，Alphabet股价一度上涨近15%。</p><p>&nbsp;</p><p>人们对于Alphabet股票的热情追捧，部分原因是其首次提出了0.20美元的季度分红。该笔分红将从2024年6月17日起面向A类、B类与C类股票支付。这家搜索巨头还公布了一项价值700亿美元的股票回购计划。微软本季度同样以回购及股息的形式向股东返还了84亿美元。这一行为也被网友调侃“偷师Meta”，Meta上季度推出回购分红后尝到股价大涨。</p><p>&nbsp;</p><p>Google Cloud收入达到96亿美元，同比增长28%。公司CFO Ruth Porat表示，这是受到“AI贡献的持续推动”。</p><p>&nbsp;</p><p>但AI技术的发展也增加了运营成本，主要体现在相关技术人才与计算基础设施方面。Alphabet正在努力管理这些成本。</p><p>&nbsp;</p><p>Porat解释称，“展望未来，我们仍将把重点放在削减成本增速上面，以便为越来越高的技术基础设施投资水平及相应的折旧与运营投入创造运转空间。”</p><p>&nbsp;</p><p>她还报告称，“就资本支出而言，我们报告的第一季度资本支出为120亿美元，这同样主要受到技术基础设施投资的推动，其中占比最大的部分是服务器，其次是数据中心。最近几个季度资本支出的大幅同比增长，反映出我们对AI技术为整体业务创造更多机会的能力抱有坚定的信心。”</p><p>&nbsp;</p><p>然而，从谷歌到Alphabet都在围绕AI开展组织变革，因此谷歌在AI方面的实际投入也变得愈发困难。该公司在财报中表示，“此前隶属于谷歌研究院的谷歌服务部门AI模型开发团队，如今已经被编入谷歌DeepMind麾下并直接向Alphabet集团高管报告，起始时间预计为2024年第二季度。”</p><p>&nbsp;</p><p></p><h4>微软：AI 机会取决于多少人愿意付费</h4><p></p><p>&nbsp;</p><p>与此同时，微软公布2024财年第三季度营收为619亿美元，同比增长17%。净利润为219亿美元，同比增长20%，均摊后每股收益为2.94美元。微软各业务部门绩效如下：</p><p>&nbsp;</p><p>生产力与业务流程：收入196亿美元，增幅12%。智能云：267亿美元，增幅21%。其他个人计算业务：156亿美元，缩水17%。</p><p>&nbsp;</p><p>在整体表现强劲的本季度当中，唯一的重大失误来自其他个人计算部门的设备销售层面，该部门收入下降了17%。而归功于微软收购动视暴雪，Xbox内容与服务收入跃升62%。</p><p>&nbsp;</p><p>微软公司执行副总裁兼CFO Amy Hood告诉投资者，为了进一步支持云与基础设施与模型训练，预计资本支出将继续增加。</p><p>&nbsp;</p><p>在微软的财报电话会议上，摩根士丹利的Keith Weiss询问了关于微软AI投资的更多细节，并指出软件巨头的资本支出可能同比增长50%以上，来到500亿美元。更有消息称其花费在AI超级计算机上的资金总额将高达1000亿美元。</p><p>&nbsp;</p><p>Weiss随后犀利发问：“很明显，这样的投资远远高于收入回报。希望您能向我们公布更多信息，阐述您身为管理者如何量化这些投资背后的潜在机会，毕竟这样的资本规模实在可观。”</p><p>&nbsp;</p><p>微软CEO Satya Nadella 回应称，在训练方面，微软希望“妥善分配训练这些基础大模型的必要资金，并在该领域保持领先地位。”</p><p>&nbsp;</p><p>微软公司CFO Hood 补充称，一定要以超越短期的形式看待这些大规模支出的意义，特别是关注AI对各类业务流程造成影响的可能性。“这种机会体现在价值提升之上。”换句话说，所谓机会将取决于有多少人愿意为AI增强服务付费。</p><p>&nbsp;</p><p></p><h2>结束语</h2><p></p><p>&nbsp;</p><p>无论前景发展如何，Meta、谷歌和微软的烧钱程度都令人咋舌。反观国内，智谱AI、百川智能、月之暗面、零一万物和Minimax 称为国内“大模型五虎”，一直备受投资方青睐。通过他们的融资情况，我们可以一窥国内创业公司的烧钱情况。</p><p>&nbsp;</p><p>智谱AI，去年共获得超过25亿元人民币融资，此前也获得了数亿元的多轮投资；百川智能，去年4月完成5000万美元的天使轮融资，到10月份完成3亿美元的A轮融资，近日也爆出融资消息；月之暗面，去年初攥着5000万美金入局，去年6月完成近3亿美元的天使轮融资，今年2月完成了全新一轮超过10亿美元的融资；面壁智能，近日宣布完成了新一轮数亿元融资，此前已经完成数亿元；MiniMax，今年3月被爆新一轮融资估值将超25亿美元，此前其也已完成多轮融资，但具体金额不明。</p><p>&nbsp;</p><p>虽然无法得出具体的金额，但跟Meta、谷歌和微软一个季度就要100亿美元相比，国内大模型创业企业资金实力还是追赶不上，国内外大厂之间PK资金实力比较合适。但所有企业未来面对的问题却是相似的：如何在砸了这么多钱后，拿到收益。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theregister.com/2024/04/26/alphabet_microsoft_quarterly_results/">https://www.theregister.com/2024/04/26/alphabet_microsoft_quarterly_results/</a>"</p><p><a href="https://www.cnbc.com/2024/04/24/meta-loses-200-billion-in-value-zuckerberg-focuses-on-ai-metaverse.html">https://www.cnbc.com/2024/04/24/meta-loses-200-billion-in-value-zuckerberg-focuses-on-ai-metaverse.html</a>"</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tFzIk6Ywm1tr4EuJ1e95</id>
            <title>“超级知识助手”来了，科大讯飞发布首个长文本、长图文、长语音的大模型，触达企业落地最后一公里</title>
            <link>https://www.infoq.cn/article/tFzIk6Ywm1tr4EuJ1e95</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tFzIk6Ywm1tr4EuJ1e95</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 05:07:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 长文本、长图文、长语音、大模型
<br>
<br>
总结: 科大讯飞发布了讯飞星火V3.5，推出了长文本、长图文、长语音大模型，帮助用户高效获取知识，解决真实场景中多源信息的需求。通过模型剪枝和蒸馏，推出了130亿参数的大模型，提升了效率和准确率。同时，推出了图文识别大模型和长语音功能，进一步满足用户的需求。科大讯飞还发布了合同助手和升级AI学习机，持续用技术进步解决用户的真实需求。 </div>
                        <hr>
                    
                    <p>4月26日，讯飞星火大模型V3.5（以下简称“讯飞星火”）春季上新。面向用户高效准确知识获取的痛点，科大讯飞发布业界首个长文本、长图文、长语音大模型，不仅能够把各种信息来源的海量文本、图文资料、会议录音等进行快速学习，还能够在各种行业场景给出专业、准确回答。</p><p></p><p>科大讯飞进一步升级星火语音大模型，首发多情感超拟人合成，具备情绪表达能力，并推出一句话声音复刻功能，让科技更有温度。</p><p></p><p>同时，面向企业应用场景，科大讯飞推出星火智能体平台，帮助企业解决大模型落地的最后一公里难题。</p><p>持续用技术进步解决真实刚需，讯飞星火也在获得越来越多用户的认可。据七麦数据显示，讯飞星火APP在安卓端的下载量已经超过9600万次，在国内工具类通用大模型APP中排名第一。</p><p></p><p><img src="https://static001.geekbang.org/infoq/02/021112da9fc6ef3c7c20b8fe86000787.jpeg" /></p><p></p><h3>首发长文本、长图文、长语音大模型，助力知识高效获取</h3><p></p><p></p><p>为什么科大讯飞要做长文本、长图文、长语音的大模型？通过讯飞星火APP可以看到，用户使用的最高峰不是周末，而是工作日的上午9:30和下午3:30。这意味着，大部分用户用讯飞星火来解决和工作相关的刚需问题。而高效的知识获取是用户和开发者都高度关注的问题。</p><p></p><p>科大讯飞分析发现，在知识获取和学习的过程中，广大用户能拿到的资料往往不仅是现成的长文本，还有随手可见的报刊书籍内容、各种研讨会的PPT内容，老师黑板上的板书、同学的笔记，以及各种会议录音、访谈，各种网上的发布会、培训教育视频等，能不能把这些文本、图片、语音等都上传到讯飞星火中，快速地获取知识？</p><p></p><p>为此，科大讯飞推出首个支持长文本、长图文、长语音的大模型，来解决用户真实场景中多源信息的获取需求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8f/8f89689cfe7e492cacbbbabc86bf51e4.png" /></p><p></p><p>本次讯飞星火长文本功能全新升级后，具备长文档信息抽取、长文档知识问答、长文档归纳总结、长文档文本生成等能力，总体已经达到GPT-4 Turbo 4月最新长文本版本的97%水平，而在银行、保险、汽车、电力等多个垂直领域的知识问答任务上，讯飞星火长文本总体水平已经超过GPT-4 Turbo。</p><p></p><p>长文本功能的落地需要解决信息高效处理的问题：面对上百万甚至上千万文字，长文本大模型消耗的运算资源非常大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c05a49611943050831603f94c671e75.jpeg" /></p><p></p><p>为了解决大模型应用效率和准确率问题，刘庆峰谈道，基于讯飞星火V3.5对长文本的理解、学习、回答能力，科大讯飞进行了重要的模型剪枝和蒸馏，从而推出业界性能最优的130亿参数的大模型，在效果损失仅3%以内的情况下，使得星火在文档上传解析处理、知识问答的首响时间以及文字生成方面都获得了极大的效率提升。测试显示，在保障长文本效果的情况下，无论是10K、64K、128K token，还是更长的文本上，星火大模型的性能都做到业界最优。</p><p></p><p>面向复杂的图文场景，科大讯飞在图文识别、公式识别大赛多年国际第一的技术积累基础上，首次推出星火图文识别大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8ad0d8cadb8e3d2cec4f4241bc8c72c1.jpeg" /></p><p></p><p>相比传统小模型逐行文字识别的限制，星火图文识别大模型具有三大优势：</p><p>能够直接处理非常复杂的版面分析，目前已经覆盖31个典型场景，比如书刊、学术论文、专利、报纸、海报、PPT等，同时能自动识别标注出18类不同的版面要素，比如页眉、页脚、标题、段落、表格、公式、印章、手写等；融合篇章上下文语义进行文字识别，识别更精准；面向教育、金融、医疗、科研等专业领域深度优化，能自动实现更多领域的专业符号识别。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/46b33d19f3f36142b9b942caedf3cd2e.jpeg" /></p><p></p><p>根据国际公开的权威英文测试集来看，讯飞星火的图文识别效果超过微软和谷歌。从典型应用场景来看，在科研、金融以及企业产品技术文档等识别效果都处于业界领先地位。</p><p></p><p>此外，面对广泛的音视频信息高效获取需求，科大讯飞也推出长语音功能，将国际领先的语音识别和翻译技术结合起来，可以实现会议录音、学习视频等的一键研读，实现音视频场景的高效知识获取。</p><p></p><h3>发布合同助手、升级AI学习机，以技术进步解决真实刚需</h3><p></p><p></p><p>讯飞星火长文本、长图文、长语音能力的升级，进一步推动大模型在各个场景的落地。刘庆峰重点介绍了讯飞星火在招投标、合同、教育等场景下的应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24ebfa965922b1c8e3b529360fde3cf0.jpeg" /></p><p></p><p>在招投标场景，此前凭借讯飞星火领先的文本理解、逻辑推理和数学能力，科大讯飞和国家能源物资公司在企业采购场景合作了智能无人评审系统，已经在国资委网站上被作为典型案例推荐。据介绍，在国家能源集团已评审5.7万余单，评审准确率达97％。这一次，叠加本次升级的长文本和长图文能力，可以让评标更便捷、更高效、更准确。</p><p></p><p><img src="https://static001.geekbang.org/infoq/80/808bdb34d842792892291e80030298b0.jpeg" /></p><p></p><p>日常生活中，我们在买卖商品、装修房屋或者是购买汽车保险时经常会遇到各种各样的合同，看不懂存在风险怎么办？科大讯飞推出星火合同助手，它可以对我们的合同进行风险审核、合同比对，摘要总结以及合同生成，迅速识别潜在风险漏洞，成为你口袋里的“法务助手”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/78/784e3f074eccd31c066b6f9e8543db04.jpeg" /></p><p></p><p>在教育场景，科大讯飞进一步升级了讯飞AI学习机产品，不仅对作文的批改、对理科的批改更加精准，也让智能化辅学更有针对性更高效；也结合本次多模态能力升级了百科问答功能，星火大模型理解并融合了大量图书知识，对于孩子的奇思妙问等复杂问题，“爱因斯坦”和他的大咖虚拟人朋友都能回答，让孩子们在趣味互动中学知识、长见识，同时提升了孩子主动提问的意愿和能力。</p><p></p><p>智慧黑板也再次升级，搭载长文本和长语音能力，让实录转写效率得以提升的同时，提升篇章梳理能力。其次是星火教师助手，融入长文本能力后，可以把优质教辅内容融入，教师在备课的过程中就可以直接融入教辅教参中的内容，进一步丰富备课资源，提升备课效率。</p><p></p><p>此外，星火科研助手目前已在中国科学院、三亚崖州湾科技城、北京邮电大学、哈尔滨工业大学等机构高校铺开应用。多模态能力升级下，讯飞星火科研助手也进一步提升了论文问答、综述生成、实验解读等的效果，使得解析的学术资料更加丰富，进一步赋能高校和科研院所的科研工作。</p><p></p><h3>能“情感共鸣”，还能“一句话声音复刻”</h3><p></p><p></p><p>万物互联时代下需要更真实的AI语音交互。年初讯飞星火V3.5发布会上，科大讯飞推出了超拟人对话功能，AI的声音更自然更真实，拟人度达到了83%，受到用户的广泛欢迎。无论是语音可懂度、流畅度还是表现力，效果均超过OpenAI、微软。</p><p></p><p>此次科大讯飞发布多情感超拟人合成，进一步提升了情绪表达的可感知度，对高兴、抱歉、安慰、撒娇、困惑等情绪表达的可感知度达到85%以上，AI语音更加生动、真实。</p><p></p><p>昊铂HT 2024 款车型行业率先搭载科大讯飞超拟人合成技术，已在4月25日正式全球上市。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/9583ff64b9031fc8755c6d449622600b.jpeg" /></p><p></p><p>除了超拟人对话，科大讯飞还推出“一句话声音复刻”功能，一句话就可以定制你的AI助手声音。比如模仿小朋友的声音，每天给爷爷奶奶读书读报；在我们出差的时候，模仿我们的声音给孩子讲故事。这个功能可以让世界变得更有温度。</p><p></p><p>刘庆峰谈道，科大讯飞在个性化语音合成一直处于业界领先，目前已进阶至一句话声音复刻。当年讯飞AI模仿林志玲的声音需要去台湾录一个星期的声音，到后来模仿郭德纲需要一天的声音，再往后需要5分钟的录音，现在一句话就可以模仿。大家可以在讯飞星火APP上体验。</p><p></p><h3>发布星火智能体平台，为企业注入新质生产力</h3><p></p><p></p><p>自今年1月30日发布以来，讯飞星火V3.5作为首个全国产算力训练的大模型，受到了各行业伙伴和开发者的广泛欢迎。据刘庆峰介绍，大模型云边端的整体解决方案正在赋能汽车、家电、运营商等越来越多的行业；在过去不到3个月的时间里，讯飞新增了55万实名认证的开发者，其中一半以上来自企业。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd3d99b4e72eae84a9d6d439b21f348d.png" /></p><p></p><p>对企业来说，如何高效地获取和学习知识同样是痛点，科大讯飞给出的答案是智能体，并面向企业场景推出全新的智能体平台。</p><p></p><p>企业构建智能体的环节主要涉及任务理解、外部信源打通、内部各个IT系统打通以及私域知识深入融合等环节，最终根据每个任务的执行结果输出答案，这样一个完整的过程才能够最终完成智能体的构建。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bbfbb6d906ad2b407566c360e50aa9db.jpeg" /></p><p></p><p>刘庆峰谈道，在讯飞星火智能体平台上，针对用户的输入，首先，基于讯飞星火大模型非常聪明的底座能力，会自动实现用户输入的精准理解和任务规划。其次，解析完了相关的任务和对应的工具之后，讯飞星火也构建形成了包括天气、航班、企查查等成体系的外部信息来源的对接；同时，星火智能体平台还通过互认证的机制，实现了往往是独立的、隔离的OA系统、CRM系统以及ERP系统的打通，完成相应操作；最后，通过私域知识融入机制，智能体平台很容易实现企业所属行业以及企业私域知识的融入，实现更精准的专业理解和知识问答。</p><p></p><p>此外，星火智能体平台还可以通过拖拽方式实现新智能体的创建和多智能体的协作。星火智能体平台，敏捷触达大模型应用企业落地的最后一公里。</p><p></p><p>据刘庆峰透露，科大讯飞将在6月27日发布讯飞星火大模型V4.0，进一步解放生产力、释放想象力。</p><p></p><p>今年全国两会上，开展“人工智能+”行动，加快发展新质生产力首次写入《政府工作报告》。大模型带来的知识管理革命正在上演，无论企业还是个人，都可以站在人工智能的肩膀上，实现新的比较优势。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QeT1AcRI6oAbwSIbi99O</id>
            <title>单场奖金池20万！百度智能云“千帆杯”教育生态行业赛全新开赛！</title>
            <link>https://www.infoq.cn/article/QeT1AcRI6oAbwSIbi99O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QeT1AcRI6oAbwSIbi99O</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 02:55:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度智能云, 千帆杯, AI原生应用, 教育生态行业赛
<br>
<br>
总结: 2024年百度智能云举办了千帆杯AI原生应用创意挑战赛，旨在推动AI与教育行业的深度融合，激发创新力量，为教育领域带来新的活力。比赛以20万元的奖金池吸引开发者参与，通过千帆AppBuilder平台，鼓励个人和团队创造教育领域的创新AI应用。AI技术的发展让教育更高效，千帆AppBuilder作为产业级AI开发平台，为开发者提供了自由的创作空间，帮助他们实现理想，推动教育行业的创新发展。 </div>
                        <hr>
                    
                    <p>自2024年百度智能云“千帆杯”AI原生应用创意挑战赛启动以来，广受开发者关注，更有百万奖金激励、千万算力支持。4月25日，百度智能云携手头部高校、知名教育企业等多家单位，联合发起千帆杯AI原生应用创意挑战赛——教育生态行业赛，单场奖金池高达20万元，现已全新开赛！本期主题围绕AI与教育行业的深度融合，通过创新AI原生应用让教育领域再现新的生命力。无论你是高校学生教师、个人开发者、还是企业，都欢迎报名本期教育生态行业赛，百度智能云千帆AppBuilder将帮你把理想变为现实。</p><p></p><p></p><h2>从传统到创新 AI让教育更高效</h2><p></p><p></p><p>近年来，从文生文到文生图，再到文生视频，生成式AI技术正在持续刷新着我们的认知，也推动着教育的变革。过去，教育资源分布不均限制了人的发展。后来，在线教育平台和搜索引擎使得人们可以更便捷地获取知识。现在，生成式AI不仅支持全球性的知识获取，还提供了更高效的信息利用和创造的能力。</p><p></p><p></p><p></p><h2>从模型到应用 躬身为创新筑基</h2><p></p><p></p><p>教育生态行业赛旨在邀请优秀个人和团队，由创意驱动，以千帆AppBuilder为器，共同打造教育行业的创新AI原生应用。大赛给了选手们自由的创作空间。只要是在教育场景下利用千帆AppBuilder解决问题，就可以大胆地决定你的应用形式，释放你的AI创造力。即使“你不会写代码，也可以做出一个应用”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a034a091f9ed7ea789dc88484e13154.webp" /></p><p></p><p>千帆AppBuilder作为产业级AI开发平台，开发者可以使用预置的模板和组件，轻松定制自己的业务流程，还可以在上面集成、扩建自己特色的组件，在不同节点上选用不同的模型，构建更多产业级场景应用。例如，你可以基于千帆AppBuilder创建一个校园AI助理，为全校师生提供智能化的专属服务，接入查制度、查课程、充饭卡、借书籍等高频场景，服务学校广大师生。</p><p></p><p></p><p></p><p>本次教育生态行业赛总奖金池高达20万，更多丰厚奖励，等你来战。</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81f15323262c52daaae47f5a8c997815.webp" /></p><p></p><p>本次千帆杯AI原生应用创意挑战赛——教育生态行业赛由千帆AppBuilder携手教育高校、教育科技合作伙伴好未来、特邀社区等平台共同打造，拥有权威背书。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a7f89c7db92f87e667d37a764f0a1f8.webp" /></p><p></p><p>另外，大赛还特邀百度智能云应用平台专家，北京航空航天大学、北京理工大学、北京邮电大学、郑州大学等高校教授出席大赛评审团，更有技术大牛亲自1V1指导，品牌成长扶持等多元权益。你的困惑，可在这里得到解答；你的创新理念，也会在这里得到认可与支持。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8c1fb2a8bc3f2d2e59314a6d710b15b.webp" /></p><p></p><p>AI与教育相互促进，共同发展。在这个“人人都是开发者”的时代，百度作为AI领域的先行者，千帆AppBuilder的出现让每个普通人也可以开发自己的应用，实现千万种理想，从生活到工作，从个人到社会......本次教育行业赛，诚邀成熟的技术开发者和相关企业前来角逐奖金，也同样欢迎初入AI大门的你，在比赛中成长。千里之行，始于足下，快快扫码报名，一起迎接全民AI时代！</p><p></p><p><img src="https://static001.infoq.cn/resource/image/26/08/267fa66a6d82b68124f6412a7014dc08.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/FXMfx6yPChWJlghhZhwl</id>
            <title>向所有生成式AI领域的优秀案例和厂商发出邀请，AIGC先锋榜进入征集倒计时 ！</title>
            <link>https://www.infoq.cn/article/FXMfx6yPChWJlghhZhwl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/FXMfx6yPChWJlghhZhwl</guid>
            <pubDate></pubDate>
            <updated>Fri, 26 Apr 2024 02:12:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 中国生成式AI领域, 企业和案例, AIGC赛道, 技术变革
<br>
<br>
总结: 中国生成式AI领域发展迅速，吸引了众多优秀企业和案例参与AIGC赛道，以推动技术变革。 </div>
                        <hr>
                    
                    <p>经过一年多的发展，中国生成式AI领域涌现出了不少优秀的企业和案例。今年4月中旬，InfoQ&nbsp;面向&nbsp;AIGC&nbsp;赛道正式启动<a href="https://www.infoq.cn/form/?id=2098">【中国技术力量&nbsp;2024&nbsp;之AIGC先锋榜】</a>"案例征集，以期深入技术变革，洞见&nbsp;AIGC&nbsp;的产业未来。本次案例征集共分为两个维度，分别是【AIGC&nbsp;最佳实践案例&nbsp;TOP20】和&nbsp;【AIGC&nbsp;最佳技术服务商&nbsp;TOP30】，本次征集时间至&nbsp;4&nbsp;月&nbsp;30&nbsp;日&nbsp;23:59:59&nbsp;止，我们将于&nbsp;5.17&nbsp;日<a href="https://aicon.infoq.cn/2024/beijing/?utm_source=infoqweb&amp;utm_medium=dahuibanner">【AICon&nbsp;全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展】</a>"大会现场公布结果，并邀请部分获奖企业来到现场展示并见证这一时刻。</p><p></p><p>目前为止，我们已经收到了来自金融、通信、教育、汽车、制造等众多领域的数十份优秀案例，以及众多优秀的技术服务商，我们期待能看到生成式AI领域的更多成果，欢迎相关企业抓紧时间填报，如有任何问题欢迎及时与我们表单上的工作人员联系。</p><p><img src="https://static001.infoq.cn/resource/image/6d/25/6d15bda5fbd663943a73d19f0d12c125.jpg" /></p><p></p><p>【AIGC&nbsp;最佳实践案例&nbsp;TOP20】鼓励应用方企业积极申报，我们将从场景创新性、实践效果、行业价值等维度进行评分，最终获奖企业将获得电子证书、少量&nbsp;AICon&nbsp;大会现场嘉宾票（需主办方审核参会人员）以及相关宣传报道，极其优秀企业将获得&nbsp;AICon&nbsp;大会现场演示的机会，由主办方提供场地向现场数千位参会者介绍应用案例。</p><p></p><p>【AIGC&nbsp;最佳技术服务商&nbsp;TOP30】鼓励技术服务厂商、解决方案提供厂商积极申报，我们将从技术攻坚性、方案成熟度、标杆客户案例、客户服务能力等多个维度进行评分，最终获奖企业同样将获得电子证书、少量&nbsp;AICon&nbsp;大会现场嘉宾票（需主办方审核参会人员）以及相关宣传报道。</p><p></p><p>感兴趣的企业可以<a href="https://www.infoq.cn/form/?id=2098">点击此处</a>"，填写表单进行申报，如有问题，欢迎与表单中的工作人员进行联系。</p><p></p><p>InfoQ&nbsp;中国技术力量往期榜单：</p><p><a href="https://www.infoq.cn/zones/chinatechawards/2020/">https://www.infoq.cn/zones/chinatechawards/2020/</a>"</p><p><a href="https://www.infoq.cn/zones/chinatechawards2022/">https://www.infoq.cn/zones/chinatechawards2022/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/D1Bz02dcaYCOAVtIOmBh</id>
            <title>多位联席主席、出品人确认！FCon全球金融科技大会演讲议题火热招募中</title>
            <link>https://www.infoq.cn/article/D1Bz02dcaYCOAVtIOmBh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/D1Bz02dcaYCOAVtIOmBh</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Apr 2024 10:11:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 中央金融工作会议, 数字金融, 人工智能, FCon全球金融科技大会
<br>
<br>
总结: 2023年中央金融工作会议提出五篇大文章，其中数字金融是重点，金融机构需夯实数字化底座。人工智能技术加速落地应用，金融成为智能技术实践场。2024年FCon全球金融科技大会将聚焦数字金融内生力，邀请专家分享实践经验。度小满金融执行主席将在大会上分享大模型技术应用和数智化转型。 </div>
                        <hr>
                    
                    <p>2023 年，中央金融工作会议提出要“做好科技金融、绿色金融、普惠金融、养老金融、数字金融五篇大文章”。其中“数字金融”作为其余四篇文章的基础，是金融行业数十年来从电算化、信息化进阶到数字化持续沉淀和发展的成果，同时也是 2024 年金融行业发展的重点。在这个过程中，要求金融机构必须不断夯实数字化底座，一方面完成自身数字化，另一方面为其它行业数字化赋能。</p><p></p><p>金融科技的加速落地应用，成为数字金融建设的内生动力。与此同时，随着生成式 AI 技术引爆新一轮人工智能热潮，智能时代加速到来，金融凭借相对完善的数据和技术基础，以及丰富的业务场景，成为智能技术的实践场。大模型技术的应用探索仍然是金融机构及其从业者今年关注的焦点，“数字金融”的内涵也正在从数字化向数智化演变。</p><p></p><p>在此背景下，极客邦科技将于 2024 年 8 月 16 日 -17 日在上海举办 FCon 全球金融科技大会，本届大会由中国信通院铸基计划作为官方合作机构，将以“科技驱动，智启未来——激发数字金融内生力”为主题，聚焦金融行业在数智化时代从战略、组织、人才、数据到技术的全面革新，邀请国内外金融机构及金融科技公司专家分享其实践经验与深入洞察。</p><p></p><p>度小满金融技术委员会执行主席、数据智能应用部总经理杨青已确认担任本届 FCon 大会联席主席。杨青老师 2018 年年初加入度小满金融，从 0 到 1 构建度小满金融的智能引擎核心算法，深耕计算机视觉、自然语言处理、机器学习、因果推断等技术领域，多篇文章被 EMNLP、ACL、CIKM 等国际会议收录，“智能化征信解读中台”荣获吴文俊人工智能科技进步奖。相关技术广泛应用于度小满营销、经营、风控、反欺诈全流程业务场景，为上千万客户提供稳定、安全的金融服务。</p><p></p><p>目前，专注于人工智能在金融场景中的应用落地工作，于 2023 年年初带领团队发布千亿参数规模的中文大语言模型“轩辕”。2023 年 9 月， “轩辕 -70B”大语言模型在 C-Eval 和 CMMLU 两大权威榜单上位列所有开源模型榜首，目前已开源 6B/13B/70B/176B 全参数的模型体系，并著有《大语言模型：原理与工程实践》一书。</p><p></p><p>在去年的 FCon 大会中，杨青老师发表了主题为<a href="https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247535267&amp;idx=1&amp;sn=24f7980342c4157dad6d89da8565ea8b&amp;scene=21#wechat_redirect">《人工智能在金融行业中的创新应用》的演讲</a>"，深入分析了传统 AI 在金融领域的持续影响力，同时探讨了在大模型时代下，如何有效结合传统 AI 与新兴技术，以应对未来挑战。（点击链接可查看演讲精彩内容：<a href="https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS">https://www.infoq.cn/article/Kmuok7Y278ktUSZox2PS</a>"）</p><p></p><p>而在今年大会上，杨青老师将带来近一年度小满大模型的全面更新，同时聚焦数智化转型，分享传统 AI 与 AIGC 的高效联动等话题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2ae9c94dbffc950257eebe214f2d4d35.webp" /></p><p></p><p>除此之外，本届大会还策划了<a href="https://fcon.infoq.cn/2024/shanghai/track/1682">金融数智化实践创新</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1683">金融大模型应用实践和效益闭环</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1688">前沿金融科技探索与应用</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1690">金融数字化管理和运营实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1689">金融数字化营销实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1691">数据资产化运营与数据智能应用</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1692">金融领域数据要素的流通与合规</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1684">金融国产化改造策略与实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1685">金融IT架构智能化升级</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1686">金融现代化核心系统建设</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1694">低成本高杠杆的数字化实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1687">金融研发效能提升路径与实践</a>"、<a href="https://fcon.infoq.cn/2024/shanghai/track/1693">金融组织变革与数字人才培养案例实践</a>" 等10+专题论坛。</p><p></p><p>目前部分专题出品人已确认：</p><p><img src="https://static001.geekbang.org/infoq/a7/a7de0a4bcf5f4319a1294d6e5793bdf1.webp" /></p><p></p><p>更多演讲议题火热招募中，如有合适内容，欢迎扫描二维码提交议题👇</p><p><img src="https://static001.geekbang.org/infoq/ee/ee9ae020b4c453f13f085027efe03e8e.webp" /></p><p></p><p>大会官网：<a href="https://fcon.infoq.cn/2024/shanghai/">https://fcon.infoq.cn/2024/shanghai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/e87986ac9c1be850cf4f11c7c</id>
            <title>大模型下B端前端代码辅助生成的思考与实践 ｜ 得物技术</title>
            <link>https://www.infoq.cn/article/e87986ac9c1be850cf4f11c7c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/e87986ac9c1be850cf4f11c7c</guid>
            <pubDate></pubDate>
            <updated>Thu, 25 Apr 2024 02:35:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 重复工作, 代码规范, AI代替, 页面配置
<br>
<br>
总结: 文中提到了在B端前端代码开发中，重复工作和代码规范是常见问题，AI代替简单脑力可以提高开发效率，同时页面配置方式对于不同需求有不同的适用性。 </div>
                        <hr>
                    
                    <p></p><h2>一、背景</h2><p></p><p>重复工作，代码规范：B端前端代码开发过程中开发者总会面临重复开发的痛点，很多CRUD页面的元素模块基本相似，但仍需手动开发，将时间花在简单的元素搭建上，降低了业务需求的开发效率，同时因为不同开发者的代码风格不一致，使得敏捷迭代时其他人上手成本较高。</p><p>AI代替简单脑力：AI大模型的不断发展，已经具备简单的理解能力，并可以进行语言到指令的转换。对于基础页面搭建这样的通用指令可以满足日常基础页面搭建的需要，提升通用场景业务开发效率。</p><p></p><h2>二、生成链路一览</h2><p></p><p>B端页面列表、表单、详情都支持生成，链路大概可分为以下几个步骤。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4d3d17f293a62785988e9d82b7094811.gif" /></p><p></p><p>输入自然语言结合大模型按照指定规则提取出相应搭建信息搭建信息结合代码模板与AST输出前端代码</p><p></p><h2>三、表达需求</h2><p></p><p>图形化配置</p><p>辅助代码生成第一步是告诉它开发出怎样的界面，提到这里，我们首先想到的是页面配置，即目前主流的低代码产品形式，用户通过一系列的图形化配置对页面进行搭建，如下图：</p><p><img src="https://static001.geekbang.org/infoq/05/0570df590846069db9b59f93d3f966f3.png" /></p><p>以上配置方式对于通用场景（如后台逻辑较为简单的CURD页面）或是特定的业务场景（如会场搭建）有较好的提效作用 。而对于需要不断迭代逻辑相对复杂的需求来说，由于是通过图形化操作的方式进行配置，对于交互设计要求较高，并且具备一定的上手成本，并且随着需求的复杂度越来越高，配置表单交互越来越复杂，维护成本也越来越高。因此，页面配置的方式前端领域的使用是相对克制的。</p><p>AI直接生成代码</p><p>AI生成代码在工具函数场景下应用较多，但对于公司内部特定业务场景的需求，可能需要考虑以下几点：</p><p>生成定制化：公司团队内部有自己的技术栈与重型通用组件，需要将这些知识进行预训练，目前对于长文本的预训练内容仅支持单次会话注入，token数消耗较高；准确度：AI生成代码的准确度挑战是比较大的，加上预训练包含大段prompt，因为代码输出的内容细节过多，加上模型幻觉，目前来看业务代码的失败率是较高的，而准确度是考量辅助编码的核心指标，如果这一点无法解决，辅助编码效果将大打折扣；生成内容残缺：由于GPT单次会话的存在限制，对于复杂需求，代码生成有一定几率被截断，影响生成成功率。</p><p>自然语言转指令</p><p>GPT其实还有个很重要的能力，那就是自然语言转指令，指令即行动，举个例子：我们假设一个函数方法实现，输入是自然语言，结合GPT与内置的prompt，让其稳定的输出某几个单词，我们是不是就可以通过对这些单词输出做出进一步的行动？这相对于图形化配置有以下几个优点：</p><p>学习门槛低：因为自然语言本身就是人类的原生语言，你只需要根据你的想法描述页面即可，当然描述的内容是需要遵循一些规范的，但相对于图形化配置来说效率是有明显提升的；复杂度黑盒：图形化配置的复杂度会随着配置页面复杂度的上升而上升，而这样的复杂度会一览无余地展示在用户面前，用户可能会迷失在复杂的配置页面交互中，配置成本逐步上升；敏捷迭代：如果要在用户端新增一个页面配置功能，基于大模型的交互方式可能只需要新增几个prompt，但图形化配置需要开发复杂表单以便于快速输入。</p><p>这里大家可能会有个疑问：</p><p>生成的指令信息不也会出现大模型幻觉吗？如何保证每次生成指令信息是稳定且一致的呢？</p><p>自然语言转指令可行大致有以下几个原因：</p><p>由长文本转关键信息属于总结内容，大模型在总结场景下的准确度远高于扩散型场景；由于指令信息只是提取需求中的关键信息，不需要做代码技术栈上的预训练，因此prompt存在很大的可优化空间，通过优化完善prompt内容可以有效提升输出准确度；准确性可验证，对于每一个场景不同表述需求输入，可以通过单测预测输出验证准确性，当出现badCase，我们在优化后针对该badCase接入单测。保证准确度不断提高。</p><p>让我们来看最终的信息转化结果：</p><p>对于代码辅助来说，基于用户的需求描述，经过PROMPT处理，可以拿到这样的信息。为代码生成提供基础信息。</p><p><img src="https://static001.geekbang.org/infoq/02/023095527308eed0e5535d968b19d245.png" /></p><p></p><h2>四、信息转化为代码</h2><p></p><p>通过大模型拿到自然语言对应可编码的信息（即上面例子中的JSON）后，我们就可以基于这个信息转化代码了。对于一个有明确场景的页面而言，一般情况下可分为主代码模板（列表、表单、描述框架）+&nbsp;业务组件。</p><p>转化流程</p><p><img src="https://static001.geekbang.org/infoq/b7/b7f256348f3ec7521c68bfa5b4a2b0a5.png" /></p><p>我们如何开发代码的？</p><p>其实这一步很像我们自己开发代码，我们拿到需求后，大脑中会提取其中的关键信息，即上方提到的自然语言转指令，然后我们会在vscode中创建一个文件，然后会进行以下操作：</p><p>首先一定是创建代码模板，然后根据场景引入对应重型组件，如列表就引入ProTable，表单就引入ProForm。</p><p>基于ProTable等重型组件并向其中添加一些属性，如headerTitle、pageSize等列表相关信息。</p><p>根据需求描述引入组件，比如识别到筛选项中存在类目选择，会在useColumns新增业务组件，识别到需求描述中存在导入导出组件，会在页面的指定位置新增导入导出业务组件。</p><p>拿到mock链接，新增请求层，在页面指定位置引入。</p><p>对于以上常用的代码插入场景都可以封装进JSON中，然后通过代码模板结合AST插入或字符串模板替换的方式生成对应代码。</p><p></p><h2>五、源码生成</h2><p></p><p>定位</p><p>源码辅助主要帮助开发者减少重复的工作，提升编码效率，和低代码页面搭建属于完全不同的赛道，低代码重在特定场景下搭建完整的页面，并且页面功能数量是可枚举的，业界低代码搭建也有很优秀的实践。而源码辅助工具旨在帮助用户尽可能多的初始化业务需求代码，后面的修改维护在代码层面交给用户，提升新增页面的开发效率。</p><p>具体的功能架构见下方：</p><p><img src="https://static001.geekbang.org/infoq/9d/9db2eedefa20530535c3e15985bee29f.png" /></p><p></p><h2>六、组件向量搜索与嵌入</h2><p></p><p>对于前端开发来说，提效的本质是少开发代码，更快的页面生成是一方面，良好的组件抽离是相当重要的一环，我们结合向量对组件的引入链路进行了优化，在初始化模板与存量代码中快速搜索定位组件。</p><p>组件向量引入链路</p><p><img src="https://static001.geekbang.org/infoq/0e/0e7249ad778ce66db51eb9a98bc9e7d8.png" /></p><p>组件信息录入</p><p>支持快速获取组件的描述内容与组件引入范式，一键录入组件，组件描述会转化为向量数据存入向量数据库。</p><p><img src="https://static001.geekbang.org/infoq/4a/4a10c7b15626b4d61c37f65cc18eed17.gif" /></p><p>组件向量搜索</p><p>用户输入描述后，会将描述转化为向量，基于余弦相似度与组件列表进行比对，找到相似度最高的组件TOP N。</p><p><img src="https://static001.geekbang.org/infoq/b4/b4a7e69aee6d84b117ac7e1ddd3fd123.gif" /></p><p>组件快速插入</p><p>用户可以在存量代码中快速通过描述搜索匹配度最高的组件，回车进行插入。</p><p><img src="https://static001.geekbang.org/infoq/11/110600ced8d1d48b5c1ca9b3d82622ee.gif" /></p><p></p><h2>七、未来展望</h2><p></p><p>组件嵌入模板：目前组件已支持向量搜索，通过结合源码页面生成，支持动态匹配组件并嵌入模板；存量代码的编辑生成：目前仅支持新增页面的源码生成，后续将支持存量页面的局部代码新增；代码模板流水线：AST的代码操作工具化，将自然语言与代码写入进一步打通，提升场景拓展效率。</p><p></p><p>​</p><p>&nbsp;*文/天意</p><p>本文属得物技术原创，更多精彩文章请看：<a href="https://link.juejin.cn/?target=https%3A%2F%2Ftech.dewu.com">得物技术官网</a>"</p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p><p>​</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Qkl75wdy0kRJf96sJRTl</id>
            <title>探索 Copilot 创新实践：腾讯、字节跳动、PingCAP 与第四范式共聚 AICon</title>
            <link>https://www.infoq.cn/article/Qkl75wdy0kRJf96sJRTl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Qkl75wdy0kRJf96sJRTl</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Apr 2024 10:10:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 技术变革, 大模型应用, 人工智能, 业务决策
<br>
<br>
总结: 在这个技术日新月异的时代，人工智能和机器学习正在彻底改变我们处理数据和进行业务决策的方式。尤其是大语言模型（LLM）的兴起，为多个行业带来了翻天覆地的变化。通过一系列高质量演讲，汇聚了各领域的顶尖专家，深入探索技术变革对业务应用的深远影响。 </div>
                        <hr>
                    
                    <p>在这个技术日新月异的时代，人工智能和机器学习正在彻底改变我们处理数据和进行业务决策的方式。尤其是大语言模型（LLM）的兴起，为多个行业带来了翻天覆地的变化。为了深入探索这些技术变革，并了解它们对业务应用的深远影响，我们精心策划了一系列高质量演讲，汇聚了各领域的顶尖专家。</p><p></p><p>在即将于 5 月 17 日至 18 日举办的 AICon 全球人工智能开发与应用大会暨大模型应用生态展览中，我们特设了“Copilot 应用构建实践”专题论坛。本论坛由字节跳动 Code AI 团队的技术负责人杨萍担任出品人。我们诚邀以下四位行业专家，他们将分享在各自领域内关于大模型应用的深刻见解和实践经验。请看以下介绍：</p><p></p><p></p><h5>演讲一：代码大模型对于工程理解的探索研究</h5><p></p><p></p><p>我们荣幸邀请到了汪晟杰，一位在腾讯云担任产品专家，并拥有近 20 年经验的软件架构与产品管理领域专家。此前，他曾在阿里担任高级技术专家，并在 Autodesk、SAP 等知名公司担任重要技术与管理职务。在本次专题演讲中，汪晟杰将深入分享《代码大模型对于工程理解的探索研究》，解析最新版本的 GitHub Copilot 如何通过尝试 链式转换（CoT）和 检索式增强生成（RAG）技术，提高对项目多文件的深层次理解和补全能力。</p><p></p><p>他的演讲将帮助我们理解 AI 如何在国内企业内部研发中被有效利用，突破开发中的瓶颈。通过他的分享，您将有机会深入了解&nbsp;RAG、CoT 及 GitHub Copilot 在工程理解方面的先进探索，并掌握这些技术如何在企业内部代码依赖和业务封装中得到实际应用，以 AI 方式改变编程模式。</p><p></p><p></p><h5>演讲二：代码生成 Copilot 产品的应用和演进</h5><p></p><p></p><p>在大模型火热的今天，代码生成领域的进展是最为焦点的应用。我们荣幸邀请到了刘夏，字节跳动产品研发和工程架构的 Tech Lead，同时也是 Code Generation 团队的技术负责人。在本次“Copilot 应用构建实践”专题演讲中，刘夏将深入探讨基于大语言模型的代码生成技术，分享如何通过代码补全和代码编辑这两种典型的应用形态来提升业务研发效率。</p><p></p><p>他的演讲将揭示代码生成技术的当前挑战、创新的交互方式，以及未来的发展趋势。通过刘夏的专业见解，您将能够深入了解代码生成的核心概念和前沿进展，并探索如何科学地定义相关性能指标，实现软件研发方式的革新。</p><p></p><p></p><h5>演讲三：Database Copilot 在数据库领域的落地</h5><p></p><p></p><p>随着人工智能技术在各行各业的深入应用，智能化解决方案正在不断推动技术界限的扩展和业务流程的优化。特别是在数据库管理领域，智能工具的应用已成为提高效率和准确性的关键趋势。我们荣幸邀请到了李粒，PingCAP AI Lab 的负责人，他在推荐系统和强化学习领域拥有丰富的研究和实践经验。李粒曾开发击败围棋世界冠军的强化学习算法，目前在 PingCAP 负责推动自动驾驶数据库云的发展，并致力于 AI 技术在企业应用中的革新。在本次“Copilot 应用构建实践”专题演讲中，李粒将深入探讨 Database Copilot 在数据库领域的落地，分享如何通过 LLM 应用于数据库系统的管理、运维和诊断，以及如何利用生产环境中的业务数据来优化 Copilot 的性能。</p><p></p><p>他的分享不仅将帮助您了解 Database Copilot 的挑战和解决方案，还将探讨如何构建和优化 LLM 应用，实现数据库领域的创新。通过参与此演讲，您将能够洞见数据库技术未来的发展趋势，并掌握如何在实际业务中应用这些前沿技术。</p><p></p><p></p><h5>演讲四：LLM 在 BI 场景的应用思路探索</h5><p></p><p></p><p>在当今数据驱动的商业环境中，商业智能（BI）技术的不断发展正在改变企业如何洞察和利用大数据。我们非常荣幸邀请到了陈庆，第四范式的 LLM BI 方向产品负责人，他曾深度参与多个大型 toB AI 项目的落地，并是《MLOps 工程实践》的作者之一。在本次“Copilot 应用构建实践”专题演讲中，陈庆将分享《LLM 在 BI 场景的应用思路探索》。此次演讲聚焦于大型语言模型（LLM）如何通过自然语言处理（NLP）能力，简化商业智能（BI）工具的使用，从而降低技术门槛，使非技术用户也能轻松进行数据查询和分析。</p><p></p><p>他的分享将探讨 LLM 在 BI 场景下的应用难点及优化方法，以及这些技术如何帮助企业更有效地挖掘数据价值。通过陈庆的见解，您将能深入了解&nbsp;LLM 的创新应用思路及其在业务智能领域的实际效益，从而开启数据驱动决策的新纪元。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/31e8f5feaa2769ee31425b28078383ff.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/9169c9860a64484e2780aa428</id>
            <title>对接HiveMetaStore，拥抱开源大数据</title>
            <link>https://www.infoq.cn/article/9169c9860a64484e2780aa428</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/9169c9860a64484e2780aa428</guid>
            <pubDate></pubDate>
            <updated>Wed, 24 Apr 2024 06:55:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大数据, GaussDB(DWS), HiveMetaStore, external schema
<br>
<br>
总结: 本文介绍了如何通过GaussDB(DWS)与HiveMetaStore对接，实现高性能计算引擎处理和分析海量数据的方法。通过创建external schema，可以直接查询hive/spark表或者插入数据到hive/spark表，无需担心表定义变化的同步更新。 </div>
                        <hr>
                    
                    <p>本文分享自华为云社区《<a href="https://bbs.huaweicloud.com/blogs/426138?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">对接HiveMetaStore，拥抱开源大数据</a>"》，作者：睡觉是大事。</p><p></p><h2>1. 前言</h2><p></p><p></p><p>适用版本：9.1.0及以上</p><p></p><p>在大数据融合分析时代，面对海量的数据以及各种复杂的查询，性能是我们使用一款数据处理引擎最重要的考量。而GaussDB(DWS)服务有着强大的计算引擎，其计算性能优于MRS服务中的hive或者spark这类计算引擎，且可以以更低的成本满足业务高弹性和敏捷性需求。通过与MRS联动，无需搬迁数据，利用DWS的高性能计算引擎处理和分析数据湖中的海量数据以及各种复杂的查询业务、分析业务越来越成为主流的解决方案。</p><p></p><p>我们可以通过创建external schema的方式来对接HiveMetaStore元数据服务，从而实现GaussDB(DWS)直接查询hive/spark表或者插入数据到hive/spark表。无需创建读外表或者写外表，也无需担心hive/spark表的定义发生变化时GaussDB(DWS)没有及时更新表定义。</p><p></p><p>本文章主要描述了GaussDB(DWS)与hivememtastore对接配置与指导。</p><p></p><h2>2. 原理浅析</h2><p></p><p></p><h3>2.1 什么是HiveMetaStore</h3><p></p><p></p><p>HiveMeatStore是Apache Hive的一个关键组件，它是一个元数据存储库，用于管理hive/spark表的元数据信息。HiveMeatStore存储了Hive表的结构信息，包括表名、列名、数据类型、分区信息等。它还存储了表的位置信息，即表数据存储何处。HiveMeatStore的主要作用是提供元数据服务，使得Hive/Spark可以对数据进行查询和分析。它还提供了一些API，可以让开发人员通过编程方式访问表的元数据。总之，HiveMeatStore是Hive的一个重要组件，它提供了元数据管理和查询服务。</p><p></p><p>external schema即外部模式，GaussDB(DWS)通过创建extrenal schema来对接HiveMeatStore服务，每次查询主动获取hive/spark表对象的元数据。无需GaussDB(DWS)内核通过create foreign table获取hive/spark表的元数据。</p><p></p><h3>2.2 external schema与schema的区别</h3><p></p><p></p><p>1 external schema主要用于与HiveMeatStore建立连接，获取表对象元数据，在创建external schema时需要指定连接的所需要的各个属性值。</p><p></p><p>2 普通schema在创建后会将schema的信息记录在pg_namespace中，external schema创建后和普通schema一样也会记录在pg_namespace，可以通过pg_namespace中的nsptype字段区分是external schema还是普通schmea。</p><p></p><p><img src="https://static001.geekbang.org/infoq/71/713de0433beb9aef449a6a4cb370a4ed.png" /></p><p></p><p>除了存储在pg_namespace中的相关信息外，external schema连接相关的配置信息都会记录在pg_external_namespace中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f4/f4496d281c7c7b81a4df2f7dd349407e.png" /></p><p></p><p>3 external schema下不支持创建表对象。对象的创建是在hive或者spark中创建的，external schema仅用于执行DML操作。</p><p></p><h3>2.3 原理说明</h3><p></p><p></p><p>GaussDB(DWS)对接HiveMetaStore流程如下图所示</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/964da7adb3b0bea2925456fc81a7ed98.png" /></p><p></p><p>1.创建Server，external schema，sql query查询。</p><p></p><p>用户在使用本特性前，将需要创建Server，创建Server过程与已有Server创建过程相同</p><p></p><p>对于创建OBS server有两种方式，一种是通过永久AK、SK的方式创建。（此种方式前提是可以获取永久AK、SK，但是此种方式不安全，AK/SK直接暴露在配置文件中，并且创建服务的时候需要明文输入AK、SK，不建议采用此种方式创建服务）</p><p></p><p>另一种云上DWS绑定ECS委托方式访问OBS，通过管控面创建OBS server。委托通过管控面创建server可参考创建外表时如何创建OBS server。<a href="https://support.huaweicloud.com/mgtg-dws/dws_01_1602.html">https://support.huaweicloud.com/mgtg-dws/dws_01_1602.html</a>"</p><p></p><p>创建external schema：</p><p></p><p>external schema创建语法为</p><p></p><p><code lang="null">CREATE External Schema ex 
WITH SOURCE hive
DATABASE 'default'
SERVER hdfs_server
METAADDRESS '10.254.159.121:9010'
CONFIGURATION '/home/fengshuo/conf2';
</code></p><p></p><p>其中SOURCE字段指定了外部元数据存储引擎的类型，DATABASE为Hive中对应的数据库名，SERVER为步骤1中创建的server，METAADDRESS为Hive提供的地址端口信息，CONFIGURATION为Hive、Kerberos相关配置文件路径。</p><p></p><p>external schema的目标是对接外部元数据（Foreign Meta），使得DWS能主动感知外部元数据的变化，如下图所示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/5646600febf43424b5444b7e138daa91.png" /></p><p></p><p>GaussDB(DWS) 通过external schema 对接HiveMetaStore，映射到对应的外表元数据，再通过外表访问 Hadoop。</p><p></p><p>SQL查询：select查询形式为 select * from ex.tbl，其中tbl为外源表名，ex为已创建的external schema。</p><p></p><p>2.语法解析：语法解析层主要针对进行解析，主要负责以下内容：</p><p></p><p>当读取到ex.tbl表以后，连接HMS进行元数据查询</p><p></p><p>3.元数据查询：从HMS中查询元数据信息，该步骤在步骤1中完成。</p><p></p><p>从HMS中读取数据，主要包括列信息，分区信息、分区键信息、分隔符信息等。</p><p></p><p>4.数据查询（针对select）：从DFS存储中获取统计信息文件个数和文件大小，为plan生成提供依据。</p><p></p><p>5.查询重写、查询优化、查询执行</p><p></p><p>6.查询下发：将元数据随plan下发给DN，DN收到plan以后，会将元数据进行解码后插入到SysCache中。</p><p></p><p>7.查询执行：DN访问obs对应文件，执行查询。</p><p></p><h2>3. 与hivememtastore对接流程</h2><p></p><p></p><h3>3.1 准备环境</h3><p></p><p></p><p>已创建 DWS 3.0集群和MRS分析集群，需确保MRS和DWS集群在同一个区域、可用区、同一VPC子网内，确保集群网络互通；</p><p></p><p>已获取AK和SK。</p><p></p><h3>3.2 在hive端创建需要对接的表</h3><p></p><p></p><p>1、在/opt/client路径下，导入环境变量。</p><p></p><p><code lang="null">    source bigdata_env</code></p><p></p><p>2、登录Hive客户端。</p><p></p><p>3、依次执行以下SQL语句创建demo数据库及目标表表product_info。</p><p></p><p><code lang="null">CREATE DATABASE demo;
</code></p><p></p><p><code lang="null">use demo;</code></p><p></p><p><code lang="text">DROP TABLE product_info;
 
CREATE TABLE product_info 
(    
    product_price                int            ,
    product_id                   char(30)       ,
    product_time                 date           ,
    product_level                char(10)       ,
    product_name                 varchar(200)   ,
    product_type1                varchar(20)    ,
    product_type2                char(10)       ,
    product_monthly_sales_cnt    int            ,
    product_comment_time         date           ,
    product_comment_num          int        ,
    product_comment_content      varchar(200)                   
) 
row format delimited fields terminated by ',' 
stored as orc;</code></p><p></p><p>4、通过insert导入数据到hive表</p><p></p><h3>3.3 创建外部服务器</h3><p></p><p></p><p>使用Data Studio连接已创建好的DWS集群。</p><p></p><p>MRS端有两种支持格式，hdfs和obs。hive对接这两种场景的创建外部服务器的方式也有所不同</p><p>执行以下语句，创建OBS外部服务器。</p><p></p><p><code lang="null">CREATE SERVER obs_servevr FOREIGN DATA WRAPPER DFS_FDW 
OPTIONS 
(
address 'obs.xxx.com:5443',   //OBS的访问地址。
encrypt 'on',
access_key '{AK值}',
secret_access_key '{SK值}'，
 type 'obs'
);
</code></p><p></p><p><code lang="null">执行以下语句，创建HDFS外部服务器。
</code></p><p></p><p><code lang="null">CREATE SERVER hdfs_server FOREIGN DATA WRAPPER HDFS_FDW OPTIONS (
      TYPE 'hdfs',
      ADDRESS '{主节点},{备节点}',
      HDFSCFGPATH '{hdfs配置文件地址}');
</code></p><p></p><p>认证用的AK和SK硬编码到代码中或者明文存储都有很大的安全风险，建议在配置文件或者环境变量中密文存放，使用时解密，确保安全。另外，dws内部会对sk做加密处理，因此不用担心sk在传输过程中泄漏。</p><p></p><p>查看外部服务器（obs为例）。</p><p></p><p><code lang="null">SELECT * FROM pg_foreign_server WHERE srvname='obs_server';
</code></p><p></p><p>返回结果如下所示，表示已经创建成功：</p><p></p><p><code lang="text">                     srvname                      | srvowner | srvfdw | srvtype | srvversion | srvacl |                                                     srvoptions
--------------------------------------------------+----------+--------+---------+------------+--------+---------------------------------------------------------------------------------------------------------------------
 obs_server |    16476 |  14337 |         |            |        | {address=obs.xxx.com:5443,type=obs,encrypt=on,access_key=***,secret_access_key=***}
(1 row)</code></p><p></p><h3>3.4 创建EXTERNAL SCHEMA</h3><p></p><p></p><p>获取Hive的metastore服务内网IP和端口以及要访问的Hive端数据库名称。</p><p></p><p>登录MRS管理控制台。</p><p></p><p>选择“集群列表 &gt; 现有集群”，单击要查看的集群名称，进入集群基本信息页面。</p><p></p><p>单击运维管理处的“前往manager”，并输入用户名和密码登录FI管理页面。</p><p></p><p>依次单击“集群”、“Hive”、“配置”、“全部配置”、“MetaStore”、“端口”，记录参数hive.metastore.port对应的值。</p><p></p><p>依次单击“集群”、“Hive”、“实例”，记录MetaStore对应主机名称包含master1的管理IP。</p><p></p><p>创建EXTERNAL SCHEMA</p><p></p><p><code lang="text">//Hive对接OBS场景:SERVER名字填写2创建的外部服务器名称，DATABASE填写Hive端创建的数据库，METAADDRESS填写1中记录的hive端metastore服务的地址和端口，CONFIGURATION为MRS数据源默认的配置路径，不需更改。
DROP SCHEMA IF EXISTS ex1;
 
CREATE EXTERNAL SCHEMA ex1
    WITH SOURCE hive
         DATABASE 'demo'
         SERVER obs_server
         METAADDRESS '***.***.***.***:***'
         CONFIGURATION '/MRS/gaussdb/mrs_server'
 
//Hive对接HDFS场景：SERVER名字填写创建MRS数据源连接创建的数据源名称mrs_server，METAADDRESS填写1中记录的hive端metastore服务的地址和端口，CONFIGURATION为MRS数据源默认的配置路径，不需更改。
DROP SCHEMA IF EXISTS ex1;
 
CREATE EXTERNAL SCHEMA ex1
    WITH SOURCE hive
         DATABASE 'demo'
         SERVER mrs_server
         METAADDRESS '***.***.***.***:***'
         CONFIGURATION '/MRS/gaussdb/mrs_server'</code></p><p></p><p>查看创建的EXTERNAL SCHEMA</p><p></p><p><code lang="text">SELECT * FROM pg_namespace WHERE nspname='ex1';
SELECT * FROM pg_external_namespace WHERE nspid = (SELECT oid FROM pg_namespace WHERE nspname = 'ex1');
                     nspid                     | srvname | source | address | database | confpath |                                                     ensoptions   | catalog
--------------------------------------------------+----------+--------+---------+------------+--------+---------------------------------------------------------------------------------------------------------------------
                  16393                        |    obs_server |  hive | ***.***.***.***:***        |  demo          | ***       |                         |
(1 row)</code></p><p></p><h3>3.5 执行数据导入hive表</h3><p></p><p></p><p>创建本地数据源表，表结构与hive一致</p><p></p><p><code lang="text">DROP TABLE IF EXISTS product_info_export;
CREATE TABLE product_info_export
(
    product_price                integer        ,
    product_id                   char(30)       ,
    product_time                 date           ,
    product_level                char(10)       ,
    product_name                 varchar(200)   ,
    product_type1                varchar(20)    ,
    product_type2                char(10)       ,
    product_monthly_sales_cnt    integer        ,
    product_comment_time         date           ,
    product_comment_num          integer        ,
    product_comment_content      varchar(200)                   
) ;</code></p><p></p><p>导入数据</p><p></p><p>从本地源表导入Hive表。</p><p></p><p><code lang="null">INSERT INTO ex1.product_info SELECT * FROM product_info_export;
</code></p><p></p><h3>3.6 执行数据从hive导入dws表</h3><p></p><p></p><p>导入数据</p><p></p><p>从本地源表导入Hive表。</p><p></p><p><code lang="null">INSERT INTO product_info_orc_export SELECT * FROM ex1.product_info;
</code></p><p></p><h2>4 总结</h2><p></p><p></p><p>本文主要对GaussDB(DWS)对接hiveMetaStore的原理和方式做了阐述。</p><p></p><p><a href="https://bbs.huaweicloud.com/blogs?utm_source=infoq&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content">点击关注，第一时间了解华为云新鲜技术~</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ce4ba3d2c506dae9d2a58f96c</id>
            <title>面向 AI 软件栈的优先设计 龙蜥社区 AI 生态建设介绍</title>
            <link>https://www.infoq.cn/article/ce4ba3d2c506dae9d2a58f96c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ce4ba3d2c506dae9d2a58f96c</guid>
            <pubDate></pubDate>
            <updated>Fri, 19 Apr 2024 09:45:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI, 操作系统, 软件栈, 生产制作
<br>
<br>
总结: 随着AI在数据中心的占比增加，对操作系统带来了挑战和机遇。AI对操作系统的影响主要体现在软件栈优化设计、技术融合、参与生产制作等方面。龙蜥社区在AI生态方面的建设和思考，着重于优化设计、容器镜像安全、最新技术特性体验等方面。 </div>
                        <hr>
                    
                    <p>编者按：随着 AI 快速发展，未来数据中心的 AI 服务器占比越来越多，甚至超过一半以上，那这会对操作系统带来什么影响呢？龙蜥社区&nbsp;AI&nbsp;SIG&nbsp;Maintainer、阿里云高级技术专家林演，从&nbsp;AI&nbsp;对操作系统的影响、面向&nbsp;AI&nbsp;软件栈优化设计、AI&nbsp;技术辅助用户使用操作系统、让&nbsp;AI&nbsp;参与操作系统生产制作等四个方面，分享了龙蜥社区在 AI 生态方面的建设和思考。以下为分享原文：</p><p></p><h2>AI对操作系统的影响</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/50/50c9c577da295fac2bd5bd1fb222cebe.png" /></p><p></p><p>随着 AI 快速发展，未来数据中心的 AI 服务器占比越来越多，甚至超过一半以上，那这会对操作系统带来什么影响呢？未来操作系统面临的挑战和机遇又是什么？第一个方面是面向&nbsp;AI&nbsp;软件栈优化设计，即 System for AI。无论如何 AI 是要跑在操作系统平台上，操作系统如何为 AI 软件栈做更多的优化、协同，从而让 AI 负载跑的更好，或跟操作系统有一个协同优化的效应。第二个方面是&nbsp;AI&nbsp;的很多技术是否能够融合到操作系统中，给操作系统用户提供一个新的入口，包括 AI 在操控领域里能够做到的一些智能调优，或做一些自主决策等事情，也可以融入到操作系统里，给操作系统用户提供更好的体验。第三个方面是让 AI 参与操作系统生产制作。随着 AI 技术的快速发展，AI 编程领域非常火，AI 对于 bug 的探索修复，包括 AI 能够自主产生对于操作系统组件的优化作用，所以我们大胆推测， AI 未来会直接参与到操作系统的生产制作中，实际在当下我们已经看到一些发展的苗头。</p><p>接下来从这三个方向分别阐述龙蜥社区当前正在做的一些工作。</p><p></p><h2>OS面向AI软件栈优化设计</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b02291f13e84c3eed9eb0e5e1101126.png" /></p><p></p><p>上图可以看到，关于&nbsp;AI 在整个社区从基础设施平台到分发渠道的总框架图。社区会不断地打磨基础设施，包括面向 AI 的基础设施，如 AI 关注的容器跟其他容器有什么特色？目前，AI 的南向硬件生态比较碎片化，比如针对英特尔、AMD 或 ARM 等不同架构甚至相同架构不同代际的芯片上，针对 AI 的优化技术都不一样，这会非常的碎片化。但是，龙蜥基础设施通过把所有主流芯片相关的一些 AI 优化做完整的支持。</p><p>目前，很少有发行版会做 pytorch 等软件包的 RPM 化，或内置到操作系统上，这是为什么？因为 AI 软件本身的发展非常迅速，大家都以 pip/conda +&nbsp;容器镜像等方式安装，这在本质上会带来一些问题，软件供应链安全上也会面临严峻挑战。你会发现前不久做出的软件镜像，过段时间更新一下就运行不了了，这是因为上游修改不受控，下游和用户需要自己去做兼容性适配验证测试。其次，从产品的维度看，也会面临比较严峻的安全问题，一些“挖矿脚本”甚至伪装成知名软件发布在上游开源社区，用户误安装后分发使用会有很大的安全风险，所以这里面临的很大问题是无法直接使用上游生态软件，需要进行大量的安全分析。</p><p>通过龙蜥社区可重复构建的理念以及针对安全上的选型，可以做到软件包的分发体系里面去。龙蜥操作系统自带了 AI 主流软件，可通过 yum install 的方式安装。当然基于 yum 提供的软件包安装之后，龙蜥还会把软件包做一层容器分发，在任何环境下也可以跑起来。除此之外，龙蜥社区还会做 AI 相关实践、虚拟机镜像，更多面向想体验 AI 的新手用户，使得 AI 小白也会有一些关于 AI 上使用的直观体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3ec4fbedf5fae2972d399830feaedae9.png" /></p><p></p><p>龙蜥容器镜像集中于从南向生态角度去做主流的&nbsp;AI&nbsp;生态覆盖，不管是 GPU 还是 CPU 中关于 AI 增强指令集，比如 BF16 、 INT8 、AMX、VNNI 等指令方面的优化。软硬件协同优化是操作系统的天然优势，因为上层的 AI 开发者或 AI 用户对于底层的芯片指令集比较陌生，但对于操作系统研发人员，会比较的熟悉，这也是龙蜥做这件事的优势所在。龙蜥 AI 容器镜像对于 AI 南向生态硬件碎片化的现状将会大大改善，而 AI 开发者只要上到龙蜥镜像中，就可以找到最适合自己 AI 的开发与运行平台，不需要对底层硬件去额外了解熟悉，就可以拿到最佳的容器镜像解决方案。</p><p>目前，龙蜥社区主要在做训练推理，包括对 Modelscope 的适配兼容。现在 Modelscope 上的一些大模型能够跑在所有龙蜥适配的硬件平台。以上介绍了龙蜥社区 AI SIG 目前正在做的项目，欢迎大家登录容器官网试用 AI 容器镜像：<a href="https://cr.openanolis.cn/mirror">https://cr.openanolis.cn/mirror</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/27/27b61e35aca38fa082bdc52e7f9354d5.png" /></p><p></p><p>AI 容器镜像拥有六大优势，目前已陆陆续续发布了很多。第一大优势是种类越来越丰富，比如针对不同硬件专门优化的容器镜像。第二大优势是容器镜像安全扫描，检查镜像中有没有带病毒的软件，保障基础安全。第三大优势是紧随上游发布节奏，上游三个月发布主版本，对于很多 AI 应用开发者，希望体验最新版本 AI 的最新技术特性。我们会结合龙蜥容器的制作技术平台，在每一个版本发布时，自动化生成新的 AI 镜像，用户可以立马体验到最新的上游软件版本。第四大优势是社区及时解决镜像问题及 CVE 修复。龙蜥社区拥有一个长周期且持续维护的操作系统版本，所有 CVE 都会及时修复，包括重要的安全漏洞。第五大优势是针对不同平台优化至最佳性能。第六大优势是镜像供应链安全和&nbsp;SBOM&nbsp;安全平台。非 RPM 生态或不可控生态的软件生态里，会有很多的供应链安全事件，包括商户跑路、软件投毒等比较严峻的安全形势，龙蜥社区希望建立 SBOM 安全平台，帮助 AI 开发者以及广大的龙蜥社区用户，能够放心使用龙蜥社区提供的容器镜像。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0f16c480d3eaa0b248424d37d0520db7.png" /></p><p></p><p>例如，在 ARM 上做 BF16 的使能来对 Pytorch 推理的性能做优化，目前从编译优化角度、算子优化、计算库基础库优化等方面，对社区默认版本有巨大的性能提升。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f75eaf97dfd94a7ab38a625a25cd4abb.png" /></p><p></p><p>在整个&nbsp;AI&nbsp;软件栈以外，龙蜥社区与英伟达&nbsp;DPU&nbsp;合作，支持做为&nbsp;DPU&nbsp;上默认运行的操作系统，龙蜥操作系统 8.6&nbsp;目前已经是&nbsp;Nvidia&nbsp;BFB&nbsp;官方支持的操作系统之一（https://github.com/Mellanox/bfb-build）。未来，龙蜥社区在 AI 网络、存储等方面持续探索，在开源领域孵化和增强解决方案，在 AI 大模型的加载、分发与存储 等环节也都有优化的空间（如几十个G 大模型的分发、加载本身是个大问题）。那是否有能力基于分布式系统做 Cephfs 等技术上的优化，未来都在龙蜥社区的考虑方向内。</p><p>除了 AI 网络、AI 存储外，龙蜥在整个 AI 技术栈方面，还会从 MLOps 需要的软件上进行拉齐，如 AI &nbsp;IDE 等软件，也会逐渐补齐并进行集成推广。龙蜥社区的目标是所有 AI 的开发从前端数据处理，到中间训练到最后推理部署等，都能够在龙蜥社区上找到与之对应的解决方案，我们希望能够把龙蜥打造成面向 AI 最友好的操作系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1c/1c57b08dffe5e3d539566bb5f9272e67.png" /></p><p></p><p>龙蜥社区已经在持续推进 AI 实践，如 AI 用户、 AI 小白能够很便捷、很简单运行其感兴趣的 AI 软件或大模型，包括图形图像、声音、唱歌等处理，这些已在龙蜥操作系统软件中进行了一系列的实践探索。在这里， AI 用户能够一键式地把他们感兴趣的 AI 组件拉起，并且硬件是最优的，性能是最好的。</p><p></p><h2>AI技术辅助用户使用操作系统</h2><p></p><p>上面主要介绍了龙蜥操作系统在 AI 软件栈的设计优化，接下来分享融合 AI 或 AI 哪些技术已经集成到了龙蜥操作系统。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/063210c82efd214c806bafadcd25bc1d.png" /></p><p></p><p>Copilot&nbsp;和&nbsp;KeenTune（轻豚）组件是龙蜥社区两个重要的 AI 组件：Copilot 组件未来会作为操作系统的运维问题处理入口和知识库入口，甚至很可能会成为龙蜥操作系统使用的入口，如遇到操作系统、软件安装等问题不知如何解决，告诉 Copilot 即可。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c86b17e1b717e85fe5020581ef797b80.png" /></p><p></p><p><a href="https://openanolis.cn/sig/KeenTune">KeenTune（轻豚）</a>"是龙蜥专门为操作系统性能优化做的一款 AI 智能调优组件，通过 AI 提供算法来帮助操作系统实现智能调优，这个组件比 Copilot 更早投入使用。&nbsp;目前，KeenTune（轻豚）已经在龙蜥社区开源，且已在很多领域商用。通过 KeenTune（轻豚）这样一款 AI 智能调优工具，能够让操作系统运行在最佳的更上层 workload 或 APP 环境中，让用户得到最佳的性能保障。</p><p></p><h2>让AI参与操作系统制作</h2><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89d9dc88fa783bd9ecfb3d822b0855c4.png" /></p><p></p><p>龙蜥在AI 参与操作系统生产制作方面，目前已有一些进展。</p><p>因为没有情感，AI 在参与测试、用例生成方面有天然优势。人在不停地重复写每一条代码，可能会觉得没有价值，不停地处理测试、用例，也会感到疲惫，那这部分是不是能够交给 AI？AI 自动化帮助操作系统开发者提高操作系统组件的编写效率，还会挖掘、自动修复操作系统 bug。我们相信在 AI 的演进过程中，慢慢地会走入到真实生产可用的环境中。</p><p>现在国外一些社区也在慢慢从 C89 标准、C99 标准编写的古老操作系统软件上，往新的、现代化的编程语言和应用上迁移，那么在这个领域里，AI 是否能够做更多的事情，是一个非常值得探讨的课题。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>