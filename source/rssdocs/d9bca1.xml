<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/YWara7wpitFei3wUe2L6</id>
            <title>疯狂马斯克的“极限”计划居然成功了？！“下云”后成本降低60%，部分功能代码精简90%，30天急速迁移服务器</title>
            <link>https://www.infoq.cn/article/YWara7wpitFei3wUe2L6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/YWara7wpitFei3wUe2L6</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 07:04:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 马斯克, Twitter, 改革, 云服务
<br>
<br>
总结: 马斯克收购了Twitter并进行了大刀阔斧的改革，包括关闭数据中心、优化云服务使用方式、重建服务与排名系统等。他还通过下云等方式削减成本，成功降低了云成本和云数据处理成本。 </div>
                        <hr>
                    
                    <p>2022 年 10 月 27 日，经历了长达半年的拉锯战之后，马斯克终于将 Twitter（现已更名 X）收归囊中，这笔 440 亿美元的收购案也终于迎来了大结局。入主 Twitter 后，马斯克进行了大刀阔斧的改革，如今一年过去了，Twitter 发生了哪些变化？</p><p>&nbsp;</p><p>2023 年 10 月 27 日，X 工程技术发布帖子称，过去一年是 X（Twitter）平台全面推进工程技术探索的一年。除了大家在 X 应用端看到的直观调整之外，团队还在幕后完成了以下一系列重要改进。其中包括：</p><p>&nbsp;</p><p>关闭萨克拉门托数据中心，并重新配置了 5200 台机架和 14.8 万台服务器，每年节约超 1 亿美元。共释放出 48 兆瓦的功率配额、拆除重达 6 万磅的网络梯架，必要设备后续将被重新配置至其他数据中心。优化了 X 的云服务使用方式，着手将更多工作负载迁往本地基础设施。这一转变使 X 每月的云成本降低了 60%。所有媒体/blob 工作均已下云，这让 X 的整体云数据存储量缩减了 60%，还成功将云数据处理成本降低了 75%。</p><p>&nbsp;</p><p>此外，X 还发生了以下变化：</p><p>&nbsp;</p><p>围绕单一产品框架整合了 For you（为您推荐）、Following（关注）、Search（搜索）、Profiles（个人资料）、Lists（列表）、Communities（社区）和 Explore（探索）等技术栈。从头开始全面重建了 For you 服务与排名系统，代码行数从 700K 缩减至 70K，精简比例高达 90%，计算占用量降低 50%，根据请求得分计算的帖子吞吐量增长了 80%。统一了 For you 和视频个性化及排名模型，显著提高了视频推荐的质量。重构了技术栈内的 API 中间件层，通过删除超 10 万行代码和数千个未实际使用的内部端点、清理未采用的客户端服务等方式完成了架构简化。精简后的元数据获取延迟降低了 50%，全局 API 超时错误减少了 90%。阻断 bot 和内容抓取的速度较 2022 年提高了 37%。平均而言，X 每天阻断超 100 万次 bot 注册攻击，并将直接垃圾邮件减少了 95%。构建本地 GPU 超级计算集群，并设计、开发和交付了 43.2 Tbps 的新网络体系架构以支持这些集群。扩展网络主干容量与冗余，每年节约1390万美元。开始进行自动峰值流量故障转移测试，用以持续验证整个平台的可扩展性与可用性。</p><p>&nbsp;</p><p>自接手 X 以来，马斯克为了缩减成本挖空心思，其中包括裁员、推行“极端硬核”企业文化、拖欠办公室租金……在公司的运营开支方面，马斯克去年刚接手&nbsp;X 时便指示团队通过削减云服务和额外的服务器空间，力争每天在基础设施上节省 300 万美元。</p><p></p><h2>省钱大法一：云服务太贵了，马斯克要“下云”</h2><p></p><p>&nbsp;</p><p>2020 年 12 月，Twitter 宣布将使用亚马逊云科技为其主时间线提供支持。当时的消息称这将是一份“多年期”协议，但没有透露任何具体数字。彼时 Twittr 公司 CTO Parwal Agrawal 在一份声明中表示，Twitter 和亚马逊云科技将合作扩展该社交媒体的基础设施、加快功能发布速度，并扩大其功能组合。</p><p>&nbsp;</p><p>据 The Information 2023 年 3 月报道，这笔交易为期五年半，合同总值 5.1 亿美元。根据报道，无论是否使用相应容量，Twitter 都同意向亚马逊云科技付费。而且亚马逊云科技不愿就具体条款进行重新谈判。根据交易细则，Twitter 的月度亚马逊云科技支出大约在 773 万美元。</p><p>&nbsp;</p><p>如今，Twitter 已经不再使用亚马逊云科技的实时时间线功能，转而选择了 AWS for Spaces 等其他服务。Twitter 后续可能使用 Google Cloud Platform（GCP）运行其时间线业务。根据 Twitter 与亚马逊云科技之间签订的合同细节，马斯克执掌的社交媒体巨头还计划使用：</p><p>&nbsp;</p><p>亚马逊云科技云基础设施，用于补充 Twitter 的本地功能，帮助该公司在全球范围内扩展其实时服务。采用 Amazon Elastic Compute Cloud (Amazon EC2)服务中基于 Arm 架构的亚马逊云科技 Graviton 2 实例，以运行其云工作负载。借助亚马逊云科技容器服务，Twitter 将在其混合基础设施当中统一构建并交付新的功能和服务。Amazon CloudFront，即亚马逊云科技的超高速内容交付网络（CDN）服务，能够以低延迟、高速率向全球客户分发数据、应用程序、视频和API。Amazon DynamoDB，即亚马逊云科技的键值数据库，可大规模提供个位数毫秒级性能。</p><p>&nbsp;</p><p>目前，Twitter 已经与谷歌签订了一份价值 10 亿美元的合同，且相关承诺早在与亚马逊云科技合作之前就已敲定。另据报道，Twitter 将在 2023 年向谷歌支付总计 3 亿美元，这也是总价值约 10 亿美元的多年期合作协议的一部分。</p><p>&nbsp;</p><p>随着马斯克入主 Twitter 并开启削减成本计划，Twitter 的基础设施支出大幅减少。根据题为“深度削减成本”的 Slack 内部消息，Twitter 计划从云服务和服务器容量方面入手，省下 150 万到 300 万美元。此外，Twitter 还试图与亚马逊云科技、Google Cloud 以及甲骨文就合同内容展开重新谈判，但供应商们纷纷表示拒绝。</p><p>&nbsp;</p><p>根据最新公告，马斯克通过将工作从云端转移到 Twitter 自己的服务器上，每月的云成本降低了 60%，整体云数据存储量缩减了 60%，还成功将云数据处理成本降低了 75%。</p><p></p><h3>下云就能解决问题？</h3><p></p><p>&nbsp;</p><p>近年来，为了节省成本，不少公司开始下云。不过，并非所有公司都适合下云，需要结合自身实际业务情况来做判断。比如，GitLab 在 2016 年底时候就表示计划要“下云”，不过团队“在收到数百条充满建议和警告的评论和邮件后，最后还是决定将 GitLab.com 保留在云端。</p><p>&nbsp;</p><p>此外，37signals 旗下一款流行的基于云服务的项目管理软件 Basecamp 也曾想“下云”。Basecamp 的上云历程已经超过十年，而且其前两年发布的产品 HEY 也一直在云端运行。但 Basecamp &amp; HEY 联合创始人 David Heinemeier Hansson 发文表示将要“下云”。</p><p>&nbsp;</p><p>“我们用过亚马逊云科技、也用过谷歌云，试过裸虚拟机、也体验了 Kubernetes 容器编排。我们知道云能提供哪些功能，其中大部分都有实际应用。现在我们终于得出结论：对于像我们这样一家增长稳定的中型企业来说，租赁基础设施资源总体上看是笔糟糕的买卖。云服务商做出的降低复杂性、控制运营成本等承诺从来就没能实现，所以我们正在筹划脱离云端、重归本地。”</p><p>&nbsp;</p><p>不过，在 David Heinemeier Hansson 撰写的关于离开云计算的思考中，他特别提到了两个情况是不能离开云计算的。一种是流量极低，一种是复杂不均衡：</p><p>&nbsp;</p><p>第一个极端是当您的应用程序非常简单且流量很低，通过使用完全托管的服务来降低复杂性确实能够节省成本。这是 Heroku 铺就的道路，也是 Render 等其他服务商所追随的道路。当您没有客户时，这是一个绝佳的起点，即使在您开始拥有一些客户后，它仍能推动您的业务发展。（然后，一旦使用量激增，账单飙升到天际线上时，您可能会面临一个好问题，但这是一个合理的权衡。）第二个极端是当您的负载非常不规则时。当您的使用量出现剧烈波动或巨大峰值时。当基线只是您最大需求的一小部分时。或者当您不知道您需要十台服务器还是一百台时。在这种情况下，没有什么比云端更好了，就像我们在推出 HEY 时学到的那样，突然有 30 万用户在三周内注册尝试我们的服务，而我们的预测是六个月内有 3 万用户。</p><p></p><h2>省钱大法二：数据中心大迁移</h2><p></p><p>&nbsp;</p><p>为了节省成本，去年 12 月，马斯克还关闭 Twitter 加州数据中心。</p><p>&nbsp;</p><p>据悉，在平安夜前夕，纳斯克飞往加利福尼亚州的萨克拉门托——Twitter 三大主要计算存储设施之一的所在地——切断了维持该社交网络平稳运行的服务器。有知情人士表示，虽然有员工担心关闭这些服务器可能导致各种问题，但节省资金是首要任务。</p><p>&nbsp;</p><p>随后，世界各地的用户报告 Twitter 服务中断。一些用户反馈 Twitter 出现很多奇怪的错误消息，比如看到空白页面、无法回复推文或关注热门话题，还有人被迫退出登陆。有熟悉 Twitter 基础设施的人士表示，如果萨克拉门托的设施仍在运行，它就可以在其他数据中心出现故障时提供备份计算能力，从而帮助缓解问题。</p><p>&nbsp;</p><p>此外有消息称，当时马斯克为了省钱，计划将萨克拉门托的服务器搬到波特兰，基础设施团队称这项工作至少要九个月才能完成，马斯克一怒之下直接搭乘私人飞机跑去机房，拔了网路线与电源就搬上大卡车开始转移，最后整个工作一个月就完成了。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/da/da556797defc3582b04c0ec1084ba1af.png" /></p><p></p><p>在今年 9 月出版的《埃隆·马斯克传》中，详细讲述了马斯克亲自迁移服务器的故事（节选，经编辑）：</p><p>&nbsp;</p><p></p><blockquote>2022 年 12 月 22 日深夜，位于 X 公司 10 楼的会议室，马斯克正在与两名基础设施经理进行紧张的交谈。&nbsp;位于萨克拉门托的一家数据服务公司允许 X 公司延长其服务器租约，以便在 2023 年有序迁出。一名显得有些紧张的基础设施经理告诉马斯克：“今天早上，他们回来告诉我们说这个计划不再适用，因为他们认为我们在财务上不再稳健。”&nbsp;这个设施每年花费 X 公司超过 1 亿美元。马斯克想通过将服务器迁移到 X 公司在俄勒冈州波特兰的其他设施来节省这笔费用。另一位经理表示这项工作不能立即进行。她平静地说：“我们至少需要六到九个月的时间，因为萨克拉门托仍然需要服务流量。”&nbsp;马斯克沉默了几秒钟，然后宣布：“你们有 90 天时间来完成这项任务。如果你们做不到，你们可以辞职。”这名经理开始详细解释迁移服务器到波特兰的障碍。“机架密度不同，电力密度也不同，”她说。“所以机房需要进行升级。”她开始详细介绍更多原因，但被马斯克打断。“这让我的大脑感到压抑，”马斯克说道，“你知道头爆炸的表情符号吗我的脑袋现在就是这个感觉。真是一堆屁话。波特兰明显有大量的空间，从一个地方迁移到另一个地方简直小菜一碟。”&nbsp;“你们需要做的就是将服务器迁移到波特兰，”马斯克说道，“如果超过 30 天，我会很震惊。”他停顿了一下，重新计算。“找一家搬家公司，运输电脑需要一个星期，然后再花一个星期来连接它们。两周。就应该这样。”&nbsp;所有人都默不作声。但马斯克仍在发火。“如果你们租了一个 U-Haul （一家租车公司），你们可能自己就能完成。”两位 X 公司的经理看着他，试图判断他是否是认真的。马斯克的两位亲密助手 Steve Davis 和 Omead Afshar 也在场。他们多次看到过他这样，知道他可能真的这么认为。&nbsp;12 月 23 日星期五晚上，James 和他的弟弟 Andrew（马斯克的表弟）与马斯克一起从旧金山飞往奥斯汀，当飞机飞过拉斯维加斯时，James 提出了一个建议，他们现在就可以移动服务器。一个名为 Alex 的来自乌兹别克斯坦的 X 员工帮助他们进入了 X 公司的数据中心，内部共有大约 5200 个冰箱大小的机架，每个机架有 30 台电脑。每个机架重约 2500 磅，高 8 英尺。但马斯克认为“这些东西看起来并不难移动”，他向保安借了一把小刀，抬起地板上的一个通风口，这让他可以撬开地板面板。然后他爬到服务器下面，用小刀撬开了一个电箱，拔掉了服务器插头，等着看会发生什么。没什么异常发生。服务器已经准备好迁移。&nbsp;第二天——圣诞前夜，马斯克召集了增援。Ross Nordeen，与他的朋友 James 在 Tesla 工作，从旧金山驱车而来。他在联合广场的 Apple Store 花了 2000 美元，买下了所有的 AirTags，这样服务器在迁移过程中就可以被跟踪。然后他去了家得宝，花了 2500 美元买了扳手、断线钳、头灯和拧下地震螺栓所需的工具。&nbsp;Steve Davis，马斯克的忠诚副手，找人租了一辆半挂车，并安排了搬家车。其他来自 SpaceX 的援助队员也已到达。这些服务器机架都有轮子，所以团队能够断开其中四个并将它们推到待命的卡车上。这表明，这五千两百多个服务器可能在几天内全部移动。 “伙计们干得好！”马斯克兴高采烈地说。&nbsp;到这周结束时，他们已经使用了萨克拉门托所有可用的卡车。尽管该地区受到了雨的袭击，他们在三天内移动了 700 多个机架。该设施之前的记录是一个月移动 30 台。这仍然留下了大量的服务器在设施中，但这群人已经证明了它们可以被快速移动。其余的部分在 1 月份由 X 公司的基础设施团队处理。</blockquote><p></p><p>&nbsp;</p><p>马斯克的疯狂举动引发了不少争议。网友海狗油90认为，“几乎没有人明白数据中心搬迁要搬的是服务、数据，而不是服务器本身，也不明白 X 这样的公司，服务连续性、数据一致性值多少钱。”</p><p>&nbsp;</p><p>网友酷憋哥评论称：“除了证明马斯克胆子大，这个案例没有什么正面的意义，试想一下，哪个普通打工人可以做出这么鲁莽的决定？他或她是否能承担由这种行为导致的严重后果？所以最终只有老板能做这种事情，只要他愿意。”</p><p></p><p>参考链接：</p><p><a href="https://twitter.com/XEng/status/1717754398410240018">https://twitter.com/XEng/status/1717754398410240018</a>"</p><p><a href="https://www.cloudzero.com/blog/twitter-aws">https://www.cloudzero.com/blog/twitter-aws</a>"</p><p><a href="https://twitter.com/thecat/status/1705860673149059115">https://twitter.com/thecat/status/1705860673149059115</a>"</p><p><a href="https://weibo.com/1727858283/NkRTyymTQ">https://weibo.com/1727858283/NkRTyymTQ</a>"</p><p><a href="https://mp.weixin.qq.com/s/7xdSNegYf9zoH7tB8jMDuQ?poc_token=HDYwP2WjN8f7OaFw635HGuh91caCskEz36fJuoqH">https://mp.weixin.qq.com/s/7xdSNegYf9zoH7tB8jMDuQ</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/IJWBbzYmh3oYN5Oombay</id>
            <title>对标 FAISS，百度开源自研高性能检索引擎 Puck</title>
            <link>https://www.infoq.cn/article/IJWBbzYmh3oYN5Oombay</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/IJWBbzYmh3oYN5Oombay</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 06:17:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 百度, Puck, 向量检索引擎, 开源
<br>
<br>
总结: 百度宣布开源自研的向量检索引擎Puck，是国内首个适用于超大规模数据集的开源引擎。Puck在个性化推荐系统、多模态检索、自然语言处理等应用场景中发挥重要作用，特别是在处理大规模数据和高维特征数据时。Puck经过多年打磨，在全球向量检索大赛中获得多个第一名。开源Puck旨在促进社区发展，提高代码质量，加速技术创新，更好地适应市场需求。 </div>
                        <hr>
                    
                    <p></p><p></p><p>近日，百度宣布在 Apache 2.0 协议下开源自研检索引擎 Puck，这也是国内首个适用于超大规模数据集的开源向量检索引擎。向量检索算法在个性化推荐系统、多模态检索、自然语言处理等应用场景中都发挥着重要作用，特别是在处理大规模数据和高维特征数据时。</p><p></p><p>名称“Puck”取自经典 MOBA 游戏 DOTA 中的智力英雄 Puck，象征着飘逸和灵动。这个项目经过多年在百度内部的精心打磨，而且在 2021 年底 Nerulps 举办的全球首届向量检索大赛 BIGANN 比赛中，Puck 参与的四个项目均获得第一名。InfoQ 采访了百度搜索内容技术部主任架构师 Ben，以了解该项目的发展历程和核心优势。</p><p></p><p>开源地址：</p><p></p><p><a href="https://github.com/baidu/puck">https://github.com/baidu/puck</a>"</p><p></p><p>InfoQ：是否方便介绍一下您的工作经历，以及目前的主要职责？</p><p></p><p>Ben：我从毕业即加入百度，最初在移动搜索部门，负责基础检索和相关性方面工作，经历了移动高速发展的过程。之后作为创始成员协助组建了多模搜索部负责视觉搜索，属于百度最早一批进入 AI 领域的员工。目前在搜索内容技术部，负责内容相关技术，包括内容获取、内容理解、内容计算、内容加工与生成等。</p><p></p><p>InfoQ：您从什么时候开始关注开源？是什么让您决定 Puck 要开源？选择这个时候开源的原因是什么？</p><p></p><p>Ben：我们很早就在思考开源，看到 FAISS（由 Facebook AI Research 开发的大规模向量检索库）开源之后获得了广泛的业界关注和应用，我们也希望开源 Puck 后，可以促进社区的发展，并借助社区的力量提高代码质量，加速技术创新，更好的适应市场需求。自研开源市场变得越来越成熟和规范，可能会带来更多的商业模式和合作机会。</p><p></p><p>对外开源，我们其实筹备了很久，做了大量的准备工作。大模型的爆火，导致向量检索技术获得广泛关注，我们认为，这是一个合适的开源契机。</p><p></p><p>InfoQ：您能具体讲一下 Puck 在百度的发展史，以及从您角度来看，它对于百度搜索的价值主要体现在哪里？</p><p></p><p>Ben：Puck 的想法最早来自视觉搜索业务，我们需要一个能支撑数百亿相似图片检索的 ANN 引擎，同时要能支持高吞吐、低延时、高准确、低内存、高灵活性等要求，当时业内没有能满足我们需要的引擎，于是启动了自研的过程。</p><p></p><p>2017 年 Puck 完成首次上线，在百亿图片库上成本和效果都取得了极其显著的提升；之后随着 Transformer 模型在 nlp 领域的大放异彩，基于 embedding 的语义检索越来越凸现价值，Puck 的应用也越来越广，2019 年 Puck 在百度内部开源，支撑的业务数快速增长，目前已广泛应用于百度搜索、推荐、网盘、知识图谱等内部多条产品线，支持规模突破万亿。目前 ANN 已经成为互联网底层基础技术之一，是 AI 时代的基石，搜索最重要的支撑技术之一。</p><p></p><p>InfoQ：期间经过了几次优化，优化重点是什么，您能具体讲述一下吗？</p><p></p><p>Ben：到今天 Puck 已经是一个打磨多年的产品，中间的优化数不胜数，大体来说可以分成以下几个阶段：</p><p></p><p>2016 年到 2019 年，打磨核心算法和实现，重点在基础性能优化上，不断调整细节，在自有场景上做极致优化，Puck 的核心框架在这一时期建立并沿用至今。2019 年到 2021 年，以公司内开源为标志，随着业务接入的增多，Puck 需要适配各种各样的应用场景和诉求，易用性、扩展性、功能多样性成为主要目标，像高性能的实时插入、多条件检索、分布式建库等等功能都是在这一时期完成。2021 年到 2022 年，以大规模内容关系计算应用为契机，Puck 重点优化在单实例超大规模数据下的性能，通过大尺度量化和索引结构的优化在十亿规模数据集上大幅提升性能降低成本。以参加全球首届向量检索大赛 BIGANN 并获得四项第一为标志，证明了 Puck 在这部分的竞争优势。2022 年至今，核心算法创新，提出了新的算法来适配不同数据场景，新增更多的 feature，同时完善配套设施，做外部开源准备。</p><p></p><p>这只是一个粗略的划分。实际上，Puck 的优化更多地由许多微小的优化点组成。我们在讨论中提出了大量有趣的想法，进行了大量的实验和尝试。总的来说，十个想法中最终只有一到两个能成为正式的功能。这些优化最终汇聚在一起，形成了我们今天看到的 Puck。</p><p></p><p>InfoQ：您能否详细介绍下 Puck 的核心优势和应用场景？</p><p></p><p>Ben：Puck 开源项目包含了两种百度自研的检索算法和一系列的附加功能，核心优势首先就是性能，经过多年的打磨和调优，在 benchmark 的千万、亿、十亿等多个数据集上，Puck 性能优势明显，均显著超过竞品，在 2021 年底 Nerulps 举办的全球首届向量检索大赛 BIGANN 比赛中，Puck 参加的四个项目均获得第一。</p><p></p><p>其次，易用性上，Puck 提供了一系列的适用于各种场景的功能，比如，同时提供简单易用的 API 接入，尽量少的暴露参数，大部分参数使用默认设置即可达到良好性能。</p><p></p><p>最后，Puck 是一个久经考验的引擎，经过多年在实际大规模场景下的验证打磨，广泛应用于百度内部包括搜索、推荐等三十余条产品线，支撑万亿级索引数据和海量检索请求，可靠性上有非常高的保障。</p><p></p><p>Puck 引擎这次开源了两种检索算法 Puck 和 Tinker，分别更适用于超大规模数据集和中小规模数据集，几乎可以覆盖绝大部分的检索应用场景。目前已广泛应用于百度内部搜索、推荐等多条产品线，覆盖数据规模从百万至万亿。</p><p></p><p>InfoQ：面对 AI 新浪潮，大模型在业内已越来越卷，在您看来未来开源市场会不会更卷？</p><p></p><p>Ben：AI 大模型的出现确实使得业内竞争更加激烈，但这并不是坏事。首先，大模型的发展推动了 AI 技术的进步，提高了 AI 的性能和效率。其次，大模型为业内带来了更多的创新空间和可能性，推动了开源市场的发展。</p><p></p><p>以后业内在自研开源市场的竞争会更加激烈，但这并不意味着会更卷，相反是带来了无限的可能。因为开源市场的特性是开放和共享，企业和个人可以通过开源市场获取最新的 AI 技术和模型，而无需自己从零开始开发。这有助于整个行业降低研发成本和提高研发效率。</p><p></p><p>此外，开源市场也是技术交流和创新的平台，业内人士可以在这里分享自己的研究成果，吸收他人的经验和知识，共同推动 AI 技术的发展。所以，虽然竞争会更激烈，但只要我们能适应这种趋势，积极参与交流和创新，就可以从中获益。</p><p></p><p>InfoQ：那您认为互联网公司开源项目的未来发展趋势是什么样的？会往哪方面发展？</p><p></p><p>Ben：</p><p></p><p>深度专业化：随着技术的细分，开源项目可能会更加专业化和深度化，解决更具体、更深入的问题，会更多永远专注于某一特定问题的开源项目，Puck 就是其中之一。多元化：互联网公司自研的开源项目可能会涉及更多的行业和领域，实现技术的跨界整合，形成各种行业解决方案的开源项目，这种跨界融合将有助于推动技术在各行业的广泛应用。更强的实用性：未来的开源项目可能会更注重实战和应用，而不仅仅是理论研究。开源项目会提供更多实用的工具和框架，帮助开发者更好地将理论应用到实际工作中。注重数据和算法的开源：随着数据和算法的重要性日益凸显，未来可能会有更多的数据和算法开源，以加速 AI 等领域的发展。</p><p></p><p>这些变化都将为推动科技发展和解决实际问题提供更强大的动力。</p><p></p><p>InfoQ：您提到 Puck 在内部已广泛应用，有哪些大家熟悉的产品或场景吗？能否举个例子。</p><p></p><p>Ben：大家熟悉的百度搜索和手机百度内的信息流推荐都有使用 Puck 技术。</p><p></p><p>InfoQ：请问开源后是否收到了社区的一些反馈，对您有怎样的启发？</p><p></p><p>Ben：自从 Puck 开源以来，我们已经收到了不少来自社区的反馈和建议。这些反馈和建议对我们来说是非常宝贵的，它们不仅帮助我们发现了 Puck 的一些问题和不足，也为我们提供了改进和优化的方向。</p><p></p><p>对我个人来说，这些反馈启发我认识到，虽然我们在内部使用 Puck 有着丰富的经验，但在面对更广泛的用户群体时，我们还需要不断学习和提高。每个用户的需求都可能不同，我们需要更加深入地理解用户的需求，才能更好地优化 Puck，使其更加适应不同的使用场景。</p><p></p><p>同时，这些反馈也让我深切地感受到了开源社区的活力和创新精神。许多社区成员不仅提出了问题，还积极地提供了解决方案，这种积极参与和贡献的精神让我深感鼓舞。我希望在未来，我们能够更紧密地与社区合作，共同推动 Puck 的发展。</p><p></p><p>InfoQ：Puck 对您个人的意义，您对 Puck 的未来有什么期待？</p><p></p><p>Ben：Puck 是团队长时间研究和努力的成果，作为 Puck 的负责人，我对这个项目有着深深的热爱和执着，对我个人来说，它不仅仅是一个检索引擎，而是代表团队付出的心血和智慧的结晶，它是我们对技术的追求，对创新的执着，也是我们对未来的期待和憧憬，Puck 的每一次升级和优化都记录着我们的成长和进步。</p><p></p><p>对于 Puck 的未来，我有着很高的期待。首先，我希望 Puck 能在开发者社区中得到广泛的使用，同时也能得到社区的反馈，不断优化和改进。我期待看到更多的人参与到 Puck 的开发和使用中来，通过大家的共同努力，让 Puck 成为 AI 领域有影响力的一款工具。其次，我希望 Puck 能够持续创新，不断优化，保持其技术领先地位，不仅能适应现有的技术需求，还能预见并引领未来的技术趋势。最后，我希望 Puck 能在更多实际应用中发挥出它的价值，为人工智能在各个行业的应用提供强大支撑，推动科技的发展。</p><p></p><p>采访嘉宾简介：</p><p></p><p>Ben，百度搜索内容技术部主任架构师，负责多模态内容理解、超大规模内容关系计算、内容加工与生成、模型优化等方向。</p><p></p><p>今日好文推荐</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651185124&amp;idx=1&amp;sn=ea0d371925430a03850c4aac3902b316&amp;chksm=bdb827b78acfaea1c04b0e7bc8df4b7049600841f5bc75d641d35ce7a31c971ad6920cea057d&amp;scene=21#wechat_redirect">“这是一件关于云服务的大事儿！”英特尔 4400 万美元投资基础设施初创公司，硬刚公有云</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651184841&amp;idx=1&amp;sn=b40ceb7de4cec3687f833ff2af20350a&amp;chksm=bdb8269a8acfaf8cfbadd25cdf4ceb314eed3dc188c36af4f549eac2f210e1f599cedc5a627c&amp;scene=21#wechat_redirect">头发丝 1/60 的精度，中国每 10 辆新能源汽车就有 6 辆用这家齿轮</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651184755&amp;idx=1&amp;sn=2d50fceb66679dfaa6e5b9470ba5aee6&amp;chksm=bdb826208acfaf367cc3f8d2cf57a6ec6b9b0c00d731b988812591dde22864ebb3c5545db675&amp;scene=21#wechat_redirect">语雀突发 P0 级事故！宕机 8 小时被网友怒喷，运维又背锅？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651184614&amp;idx=1&amp;sn=b43b9e284546eb0a88e5cd88aac46de4&amp;chksm=bdb825b58acfaca3d677861a4deccc0719767a1de6fc85d33061016eb6017a505b6fff532a73&amp;scene=21#wechat_redirect">智谱 AI“超 25 亿融资”的背后</a>"</p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/H74QyxxT4AT4QfUSVzxE</id>
            <title>高级别自动驾驶时代，如何找到车路协同更优解？</title>
            <link>https://www.infoq.cn/article/H74QyxxT4AT4QfUSVzxE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/H74QyxxT4AT4QfUSVzxE</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 02:28:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 边缘计算, 车路协同, 路侧计算单元, 智能交通系统
<br>
<br>
总结: 本文介绍了浪潮信息与百度合作发布的首代车路协同路侧计算单元 RSCU。该产品通过边缘计算技术，提供高性能的算力支持，能够实现车辆与路侧设备的实时信息交互，实现车辆主动安全控制和道路协同管理，从而构建安全、高效和环保的道路交通系统。文章还介绍了车路协同技术的发展历程和应用前景，以及RSCU背后的设计与思考。通过该产品的应用，可以推动边缘计算技术在智能交通系统中的应用，并在其他行业中推广。 </div>
                        <hr>
                    
                    <p>近日，在“以边缘·致多元”边缘计算新品发布暨合作伙伴大会上，浪潮信息联合百度发布首代车路协同路侧计算单元 RSCU。该产品通过系统设计，性能可满足 L2 至 L4 高等级自动驾驶融合应用的算力需求，还支持百度开放、兼容的智路 OS 操作系统连接上层场景，能够在双向 8 车道路口全面感知信号灯、摄像头、激光雷达、路牌路标、气象站等状态，目前已经在北京、武汉等多地部署测试。</p><p></p><p>会后，浪潮信息边缘计算产品线总经理孙波、百度车路协同首席架构师王淼接受了 InfoQ 在内的媒体采访，进一步分享浪潮信息与百度在车路协同方向上的探索与思考。</p><p></p><h2>车路协同：让聪明的车驶向智慧的路</h2><p></p><p></p><p>车路协同是自动驾驶技术发展的关键因素之一，其采用先进的无线通信和新一代互联网等技术，全方位实施车与车、车与行人、车与路等动态实时信息交互，并在全时空动态交通信息采集与融合的基础上，开展车辆主动安全控制和道路协同管理，充分实现人、车、路的有效协同，保证行车安全，提高通行效率，改善交通环境，从而形成的安全、高效和环保的道路交通系统。</p><p></p><p>人们对车路协同的探索最早可以追溯到 20 世纪 50 年代末，当时通用汽车在新泽西州打造了一条埋入大量通信设备的概念高速公路；1990 年代，日本将智能交通系统确立为国家项目；2011 年，中国科技部在 863 计划中设立了智能车路关键技术研究项目，为车路协同技术的发展提供了支持。</p><p></p><p>随着近年来自动驾驶技术加速发展，产业链基础配套和市场开发也越来越成熟，高级别自动驾驶正在加速“上路”。可以预见的是，“车路云”协同发展将成为趋势——不仅需要车辆本身具有很强的车载算力、高精度传感器、操作系统等，还需要加强路侧感知、计算、通信的边缘计算基础设施建设，并能够与边缘云、数据中心云实现多级云边协同。而这也对路侧边缘计算基础设施的性能、存储、可靠性、软硬协同等方面提出了更高的要求。</p><p></p><p>孙波在接受采访时表示，在车路协同系统中，多个摄像头和雷达采集到的数据需要在极低时延内处理并呈现结果。此时，算力便显得至关重要，需要超过 200 TOPS 来支撑整个现场数据的实时处理。</p><p></p><p>“从设备角度出发，性能需求不容忽视。每个摄像头采集的是视频流，每秒产生 30 帧照片。若要做到实时分析，每帧图片经过推理处理以判断车辆位置，需要每秒分析 30 次。若算力无法达到此水平，可能需要进行抽帧处理，即每秒只处理 10 帧或 1 帧，导致算力差异和时延增加。为确保高实时性，需要使用高性能设备进行实时处理。我们跟百度一起在路侧计算单元设备中增加了较强性能的计算单元来支持实时处理。对于通信精度，更着重于设备侧的时钟同步。基于卫星通信的时钟和精度可以达到纳秒级。”</p><p></p><p>要想实现较高的性能，王淼认为需要注意以下两个方面：首先，硬件和软件需要基于高可靠的系统流程进行设计。其次，系统中采用了许多分布式架构。以手机摄像头为例，目前市场上热销的手机可能配备四个摄像头，而每个路口可能包含超过 20 个摄像头。在如此复杂的情况下，如何确保系统的性能？答案是在路侧大脑中建立一个分布式的调动系统，该系统可以并发处理数千个任务，从而确保摄像头的时延。</p><p></p><p>“在系统中，除了 CPU 外，还包括 GPU 和其他各种异构神经网络算力。为了提高性能，我们利用不同的算法逻辑，尤其是最新的神经网络技术。这些技术有助于将计算压力从传统的CPU 中解放出来，从而实现毫秒级的时延。随着人工智能技术的不断发展，主频的提高已经不再像过去那样重要。现在，整个技术栈越来越强调人工智能的计算，以实现更高的性能。”王淼说道。</p><p></p><h2>车路协同路侧计算单元 RSCU 背后的设计与思考</h2><p></p><p></p><p>为了实现让聪明的车驶向智慧的路，浪潮信息携手百度智能云发布首代车路协同路侧计算单元 RSCU。</p><p></p><p>据介绍，针对车路云协同场景下路侧逐渐增加的感知设备，路侧计算单元在算力性能方面进行优化设计，可以最大支持 260 TOPS 的算力，最多可支持双向8车道路口的信号灯、摄像头、激光雷达、路牌路标、气象站等传感器数据传输，面向 L2 至 L4 高级别自动驾驶场景，为“聪明的车”提供更精准的人、车、道路、环境、交通事件的全要素实时检测和分析，并通过车路云的协同，助力智慧城市、智慧交通场景。</p><p></p><p>此外，为保障路侧计算单元的与云端的高效协同，全新路侧计算单元还支持百度开放、兼容的智路 OS 操作系统，可以更好的衔接上层自动驾驶、车路协同应用场景，具有高性能、智能化、开放性、兼容性、协同性、安全性六大特性，全面提升车路云协同效率。</p><p></p><p>目前，百度已经率先在全国多地高等级自动驾驶示范区对该产品进行测试实验，验证了其在自动驾驶到城市交通治理的智能网联全场景服务能力。测试数据显示，基于首代车路协同核心计算单元构建的“感知-计算-通信”路侧边缘智能体系，能够实现对路口范围的人、车、道路、环境、交通事件的全要素实时检测和分析，位置精度≤1.0m（人机非,平均），速度精度≤1.5m/s（均值），交通对象感知定位类型识别准召率≥90%，路侧对象感知端到端时延（含通信时延）≤300ms（均值）。</p><p></p><p>在谈到车路协同路侧计算单元 RSCU 的设计时，孙波表示 RSCU 是目前在路侧方面算力最强的一个产品，需要结合路侧计算的时延和数据处理对于性能的要求，来做产品的整体系统设计。</p><p></p><p>“在这个过程中，我们面临了许多挑战。一个典型的挑战是在路侧环境中放置计算力服务器，这些设备需要应对春夏秋冬、风雨雪雾等各种恶劣环境，包括高温和寒冷。为了解决这个问题，我们可以采用一些算力相对较低的设备，并让它们自身进行宽温设计。例如，EIS200 可以在- 40 ℃到65℃之间正常工作。这款设备的算力约为 200 多 TOPS，虽然已经具有相当大的算力，但这也带来了功耗和散热的挑战。”</p><p></p><p>为了解决这些问题，浪潮信息与百度采取了多种创新方法。对于高功耗设备在路侧环境中的适应性问题，其采用了主动散热方案。当设备的功耗达到 300 瓦时需要进行散热创新，通过隔离散热设计，将服务器中娇贵的器件隔离在内部干净的环境中，并通过第二散热风道与外界进行热交换，由此成功解决了散热问题。</p><p></p><p>“这个联合项目的成功不仅给我们带来了很多技术上的突破和经验，而且对于边缘计算在其他行业的落地也具有重要意义。由于 AI 大模型训练需要大量的算力支持，这些模型需要在边缘侧落地应用。因此，这个项目不仅加速了边缘行业的创新和发展，还对边缘算力提出了更高的要求。随着算力需求的增加，解决环境适应性问题的挑战也会进一步加剧。”</p><p></p><p>孙波认为，基于这个联合项目的知识和成果，可以在相关领域应用边缘计算技术，例如水利、高速、制造、能源、电网巡检等等。这些应用可以快速复制到其他许多行业中，为产业的落地提供助力。“未来，我们将继续积极应对挑战，为边缘计算在其他行业的落地提供更多支持，并不断推动技术的发展和创新。”</p><p></p><p>在智慧交通领域，除了车路协同路侧计算单元 RSCU，浪潮信息与百度还合作让其适配了名为智路 OS 的生态系统，这也是由工信部指导认证的路侧操作系统生态。王淼提到，“未来的路侧会像现在的智能车一样，形成一个类似的生态系统。这个生态系统最终将包含两个关键的核心零部件，即芯片和操作系统。在这两个领域，手机和车方面稍显落后，但在道路方面，我国已经提早布局并看得更远。”</p><p></p><p>随着更多参与方加入，以及芯片和操作系统的进一步发展，整个生态系统将会更加完善和强大，并为智慧交通带来更多的无限可能。</p><p></p><h2>边缘计算将走向怎样的未来？</h2><p></p><p></p><p>AIGC 大模型的飞速发展为边缘计算业务带来了新的创新。然而，边缘计算基础设施也将面临更大的挑战。孙波认为，未来边缘计算的发展方向将面临三大难题：</p><p></p><p>首先，环境适应是边缘计算设备面临的一个重要问题。随着算力不断增加，设备的功耗也会随之提高。为了确保设备的稳定运行，需要采取更为先进的散热和环境适应手段。例如，针对未来算力提升至更高数量级的情况，需要研究更为高效的散热方式和适应各种环境下的产品设计。</p><p></p><p>其次，算力支撑是边缘计算设备的另一个重要发展方向。随着智能化和系统化的决策分析需求不断提升，边缘计算设备需要更大的算力支持。未来，边缘计算设备将朝着大算力方向发展，以更好地满足各种复杂任务和系统性的决策分析需求。</p><p></p><p>最后，安全是边缘计算面临的另一个重要挑战。与数据中心服务器相比，边缘计算设备部署在更加复杂和恶劣的环境中，需要直接面对公网安全挑战。因此，未来需要研究如何提高边缘计算设备的安全性能，以及如何实现设备的智能化运维和故障自恢复等功能。</p><p></p><p>整体而言，未来边缘计算的发展将朝着环境适应、大算力支撑和安全保障等三个方向发展。在这个过程中，需要不断研究新的技术和方法，以提高边缘计算的易用性、可靠性和维护性，更好地满足行业需求。</p><p></p><p>而要想实现边缘计算的规模化落地，关键不仅仅在于简单地拥有一个边缘计算服务器。整个产业链的协同也至关重要。</p><p></p><p>“我们需要与合作伙伴共同研究、打破限制，以推动场景的落地。这也是我们认为边缘计算要实现规模化落地所必须重视的路径。未来，我们将继续与百度等合作伙伴围绕边缘场景进行深入研究，打磨场景方案并推动其落地。在这个过程中，我们通过不断的迭代和发展，逐渐走向一个新的阶段。在众多边缘场景中，我们发现城市治理和交通是具有明确需求且非常大的场景。”孙波认为，未来的城市将是智慧化的，交通更应该如此。只有实现了智慧化的交通，才能真正解决道路交通拥堵的问题，因此，行业需要结合未来的趋势来思考如何使道路更加智能化。</p><p></p><p>“在未来的规模化落地过程中，我们将不断打磨场景和硬件设备，使其更加适用于业务场景。同时，我们相信百度也将不断迭代和优化其上层软件平台，推出更新的技术和更好的方案，共同推动设备的不断完善。”孙波说道。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Ndi571huMEScbH9hcrar</id>
            <title>智谱AI张鹏谈大模型进展和挑战，在CNCC会议上推出第三代基座大模型ChatGLM3</title>
            <link>https://www.infoq.cn/article/Ndi571huMEScbH9hcrar</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Ndi571huMEScbH9hcrar</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 01:51:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 通用智能, 跨模态融合, 认知能力
<br>
<br>
总结: 在2023年10月27日的沈阳CNCC中国计算机大会上，大模型成为了焦点，各个领域围绕大模型展开讨论。大模型的进展主要体现在通用智能的提升，通过整合感知能力、推理能力和跨模态对齐能力，形成更强大的认知级别能力。跨模态融合的能力是最受关注的，通过训练方法将多模态数据融合，提升大模型的智能水平。同时，大模型在研发和应用中面临算力、数据、算法和应用安全等挑战。为了解决内容审核问题，可以借鉴互联网和社交媒体行业的经验，采用人机融合或人机交互的方式提高工作效率和内容安全性。智谱AI在会上推出了第三代大模型ChatGLM3及相关产品，通过多阶段增强预训练方法提升训练效果，性能更强大。ChatGLM3具备多模态理解能力、代码生成和执行能力、网络搜索增强等新功能，语义和逻辑能力得到增强。此外，智谱AI还推出了具备代码交互能力的大模型产品智谱清言，支持图像处理、数学计算和数据分析等使用场景。 </div>
                        <hr>
                    
                    <p>在2023年10月27日的沈阳CNCC中国计算机大会上，大模型已经成为了大会议题的焦点，各个领域都在围绕大模型展开讨论。</p><p></p><p>在27日上午的“大模型的研究进展与产业应用展望”论坛，由CCF副秘书长谭晓生主持，德国国家工程院院士张建伟、复旦大学计算机学院教授邱锡鹏、智谱AI CEO张鹏、科大讯飞研究院院长刘聪、蚂蚁集团副总裁徐鹏等专家参与讨论的圆桌交流环节也取得了丰富的成果，专家从各自的视角分享了大模型的进展、挑战以及未来的问题。以下整理智谱AI CEO张鹏老师的部分观点。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/3e/80/3e64309742f0047dbee5b8ab84c1af80.png" /></p><p></p><p>关于大模型领域的进展，张鹏认为，目前大模型的进展可以归结为通用智能，即基础模型的通用智能水平的提升。上一代人工智能大多数还是单向的感知能力。而大模型最大的优势是能把这些感知能力整合起来，形成一个更泛化的、更强大的认知级别的能力。这其中就包括推理能力、复杂问题的拆解能力，以及跨模态对齐能力。</p><p></p><p>最受关注的其实就是跨模态融合的能力，经过实践后发现多模态或者跨模态的数据可以通过训练的方法完美的融合到一起，在一定程度上提升了大模型的智能水平。</p><p></p><p>另外，基于认知能力的提升，可以观察到像智能体 Agent 这一类的研究，确实能够极大地增强大模型在实际应用当中的效果，让大模型从搭配 Benchmark 的实验环境走入到真正的应用当中，来解决实际的应用问题，这在张鹏看来是让人欣喜的进展。</p><p></p><p>大模型在研发和应用过程中也会遇到不少的挑战。张鹏认为，除了算力和数据方面的挑战之外，在算法方面也同样有挑战，当前所有的大模型都基于2017年提出的Transformer架构，未来是否会被改进或被新的东西代替也是大家关心的问题。另外张鹏考虑更多的另一个挑战是应用安全问题，包括私有数据训练等，首先要考虑的就是安全。</p><p></p><p>关于产出的内容审核的解决办法，大模型产出的内容在提供给用户之前，对于所提供的内容审核问题也是很重要的。张鹏说，首先平台要保证尽量不要传递错误的讯息，其次是为了达到这个目的，可以借鉴已有的多年的经验，例如人机融合或者人机交互是提升工作效率的有效方式之一。通过借鉴互联网、社交媒体等行业的经验，可以降低人工成本，并保证内容的安全性。</p><p></p><h3>智谱 ChatGLM3 以及相关系列产品发布</h3><p></p><p></p><p>在此次 CNCC 会议上，智谱AI推出了自主研发的第三代基座大模型ChatGLM3以及相关系列产品。这是继智谱AI推出千亿基座的对话模型ChatGLM和ChatGLM2之后的又一重大突破。</p><p></p><p>此次推出的 ChatGLM3 采用了独创的多阶段增强预训练方法，使训练更为充分。评测显示，在 44 个中英文公开数据集测试中，ChatGLM3 在国内同尺寸模型中排名首位。智谱 AI CEO 张鹏在现场做了新品发布，并实时演示了最新上线的产品功能。</p><p></p><h3>ChatGLM3全新技术升级 更高性能更低成本</h3><p></p><p></p><p>通过更丰富的训练数据和更优的训练方案，智谱AI推出的ChatGLM3性能更加强大。与ChatGLM2相比，MMLU提升36%、CEval提升33%、GSM8K提升179% 、BBH提升126%。</p><p></p><p>同时，ChatGLM3瞄向GPT-4V本次实现了若干全新功能的迭代升级，包括多模态理解能力的CogVLM-看图识语义，在10余个国际标准图文评测数据集上取得SOTA；代码增强模块Code Interpreter根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；网络搜索增强WebGLM-接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。ChatGLM3的语义能力与逻辑能力得到了极大的增强。</p><p></p><p>ChatGLM3还集成了自研的AgentTuning技术，激活了模型智能体能力，尤其在智能规划和执行方面，相比于ChatGLM2提升了1000% ；开启了国产大模型原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理、操作系统等复杂场景。</p><p></p><p>此外，ChatGLM3本次推出可手机部署的端测模型ChatGLM3-1.5B和 ChatGLM3-3B，支持包括 vivo、小米、三星在内的多款手机以及车载平台，甚至支持移动平台上CPU芯片的推理，速度可达20 tokens/s。精度方面1.5B和3B模型在公开Benchmark上与ChatGLM2-6B模型性能接近。</p><p></p><p>基于最新的高效动态推理和显存优化技术，ChatGLM3当前的推理框架在相同硬件、模型条件下，相较于目前最佳的开源实现，包括伯克利大学推出的 vLLM 以及 Hugging Face TGI的最新版本，推理速度提升了2-3倍，推理成本降低一倍，每千tokens仅0.5分，成本最低。</p><p></p><h3>新一代“智谱清言”上线 &nbsp;国内首推代码交互能力</h3><p></p><p></p><p>在全新升级的ChatGLM3赋能下，生成式AI助手智谱清言已成为国内首个具备代码交互能力的大模型产品（Code Interpreter）（<a href="https://chatglm.cn/main/code">https://chatglm.cn/main/code</a>"）。“代码”功能目前已支持图像处理、数学计算、数据分析等使用场景。</p><p></p><p>随着WebGLM大模型能力的加入，智谱清言也具有了搜索增强能力，可以帮助用户整理出相关问题的网上文献或文章链接，并直接给出答案。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7d/f7/7d8bb050b2d550f323a7f6b606df01f7.png" /></p><p></p><p>此前已发布的CogVLM 模型则提高了智谱清言的中文图文理解能力，取得了接近GPT-4V的图片理解能力,它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/30/a9/303865200b30fc355285fae7a60380a9.png" /></p><p></p><p>自2022年初，智谱AI推出的GLM系列模型已支持在昇腾、神威超算、海光DCU架构上进行大规模预训练和推理。截至目前，智谱AI的产品已支持10余种国产硬件生态，包括昇腾、神威超算、海光DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。通过与国产芯片企业的联合创新，性能不断优化，将有助于国产原生大模型与国产芯片早日登上国际舞台。</p><p></p><p><img src="https://static001.geekbang.org/infoq/47/47a447d222962fce01b6c1f09bbbfdb9.png" /></p><p></p><p>智谱AI此次推出的ChatGLM3及相关系列产品，全面提升了自身的模型性能，为业界打造了更开放的开源生态，并进一步降低了普通用户使用AIGC产品的门槛。AI正在引领我们进入一个新的时代，大模型必将加速这一时刻的到来。</p><p></p><h3>【活动推荐】</h3><p></p><p></p><p>在 2023 年 12 月 28-29 日，InfoQ 将在上海举办<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon全球软件开发大会</a>"，这个会议上结合当前的趋势热点，设置了 GenAI 和通用大模型应用探索、AI Agent 与行业融合应用的前景、LLM 时代的性能优化、智能化信创软件 IDE、LLM 时代的大前端技术、高性能网关设计、面向人工智能时代的架构、高效的编程语言、性能工程、LLM 推理加速和大规模服务、现代数据架构演进、建设弹性组织的经验传递、SaaS 云服务弹性架构设计等专题，目前也正在邀请业界的专家来会议上演讲。感兴趣的可以点击<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon会议官网</a>"，查看详细的介绍，也欢迎您来会议上演讲，分享技术实践。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/urmClYMAW106DweFJNo4</id>
            <title>程序员篡改ETC余额，一年私吞260余万元；语雀公布故障原因及赔偿方案；各家财报发布，创始人们：就很难受｜Q资讯</title>
            <link>https://www.infoq.cn/article/urmClYMAW106DweFJNo4</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/urmClYMAW106DweFJNo4</guid>
            <pubDate></pubDate>
            <updated>Mon, 30 Oct 2023 00:05:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员篡改ETC余额, 英伟达, AMD, SiFive
<br>
<br>
总结: 一位程序员篡改了ETC余额并私吞了260余万元。英伟达面临国内厂商无法下单的问题，国内厂商开始寻求AMD作为替代方案。芯片设计初创公司SiFive裁员20%，AMD成为甲骨文和IBM的选择。 </div>
                        <hr>
                    
                    <p></p><blockquote>&nbsp;程序员篡改ETC余额，一年私吞260余万元；国内厂商已无法从英伟达下单，寻求国内替代成唯一方案；苦英伟达“一家独大”久矣？甲骨文、IBM 下单 AMD；芯片设计初创公司SiFive裁员20%，此前估值25亿美元；AMD回应大幅裁员：小幅优化和调整；宿华辞任快手科技董事长，CEO程一笑兼任；消息称张一鸣通知负责人：PICO业务看不到希望将关停，字节人士否认；财报一发，没有一个创始人能笑着面对；国家数据局正式揭牌；故障超过 8 小时，语雀公布原因及赔偿方案；小米正式发布小米澎湃OS；华为：全面完成 5G-A 技术性能测试；Mojo 编程语言发布 Mac 版本；Python 公布了实现 no-GIL Python 的计划……</blockquote><p></p><p>&nbsp;</p><p></p><h2>科技公司</h2><p></p><p>&nbsp;</p><p></p><h4>程序员篡改ETC余额，一年私吞260余万元</h4><p></p><p></p><p>2023年9月，上海市公安局浦东分局北蔡派出所接到某科技公司员工张女士报案称，其公司发现计算机系统被他人篡改数据，导致公司账户钱款损失。民警随即展开工作，最终嫌疑人曹某迫于压力，主动投案自首。</p><p>&nbsp;</p><p>2022年8月，曹某发现所在公司的网站后台有漏洞，身为软件工程师的他，决定铤而走险，用其母亲的身份证自行注册了一个ETC账户，并绑定了其母亲的银行卡。随后曹某以每周4至5次、每次1万元的频率，陆续从该账户内提取了230余万元。之后，曹某又利用朋友的身份证再次办理账号，以同样的方式再次从公司提现36万元。</p><p>&nbsp;</p><p></p><h4>国内厂商已无法从英伟达下单，寻求国内替代成唯一方案</h4><p></p><p>&nbsp;</p><p>10月25日，英伟达在向美国证券交易委员会（SEC）递交的一份文件中披露，美国政府通知公司，针对中国更新的“先进计算芯片和半导体制造设备出口管制规则”立即生效，中国云厂商、服务器厂商、销售代理商均已无法从英伟达下单。“出口管制规则”刚出炉时，原定有30天公示期，被行业人士视为“最后30天窗口期”。在窗口期内，中国企业原本可以集中采购、运输急需的高端AI芯片。美国芯片企业出于对中国市场的依赖，理论上也会和中国企业打配合。</p><p>&nbsp;</p><p>按照美国政府最新的要求，“综合性能达到4800或以上，并且是为数据中心设计或销售的产品”将需要“立即停止出口”。NVIDIA给出的说明是，公司A100、A800、H100、H800和L40S产品的发货将立即受到影响。一位云厂商高管表示，美国“出口管制规则”步步紧逼的情况下，规模化采购国产芯片是培育本土产业链的唯一路径。</p><p>&nbsp;</p><p></p><h4>苦英伟达“一家独大”久矣？甲骨文、IBM 下单 AMD</h4><p></p><p>&nbsp;</p><p>因英伟达 GPU 供应紧张，甲骨文与 IBM 转向 AMD 产品。甲骨文计划使用 AMD Instinct MI300X AI 芯片及 HPC 用GPU；IBM 可能采用 AMD的Xilinx FPGA 解决方案。AMD Instinct MI300X 今年 6 月发布，提供强大性能，预计 2024 年供应充足。AMD 拥有足够芯片零部件，可以支撑 MI300 在四季度发布。AMD 的 FPGA 产品线因具有更低的功耗和时延，在AI推理中具有优势。</p><p>&nbsp;</p><p></p><h4>芯片设计初创公司SiFive裁员20%，此前估值25亿美元</h4><p></p><p>&nbsp;</p><p>10月25日消息，当地时间周二美国芯片设计初创公司SiFive表示，公司已裁员约20%，约130人。SiFive总部位于美国加州圣克拉拉，芯片设计均基于RISC-V技术架构，该公司竞争对手是最近上市的英国芯片设计公司Arm。和Arm一样，SiFive的工作专注于芯片底层设计，而不是芯片本身。</p><p>&nbsp;</p><p>SiFive在一份声明中表示：“随着我们发现并专注于最大机会，公司正对所有全球团队进行战略重新调整，为的是更好满足客户快速变化的需求。”SiFive发言人大卫·米勒（David Miller）表示，这次裁员涉及公司所有部门，其中也包括高管团队。他强调，公司的产品线不变。</p><p>&nbsp;</p><p></p><h4>AMD回应大幅裁员：小幅优化和调整</h4><p></p><p>&nbsp;</p><p>近日有传闻称，AMD在中国区大规模裁员。就此，AMD官方回应称：“网络传闻失实。基于公司战略的调整，公司近期对组织架构进行了小幅度的优化和重组。”</p><p>&nbsp;</p><p>同时了解到，为顺应市场的变化，AMD中国区还在为重点领域的业务继续开展招聘。根据此前发布的财报数据，AMD今年第二季度收入54亿美元，环比基本持平，净利润2700万美元，环比增长119％，总收入和调整后每股收益均超出华尔街分析师预期。</p><p>&nbsp;</p><p></p><h4>宿华辞任快手科技董事长，CEO程一笑兼任</h4><p></p><p>&nbsp;</p><p>10月20日晚间，快手科技在港交所发布公告宣布，由于需要专注其他事务，自2023年10月29日起，宿华不再担任董事会董事长，将继续担任执行董事和薪酬委员会成员，其不同投票权不会发生变化，董事长一职由程一笑接任。</p><p>&nbsp;</p><p>两年前的10月29日，快手科技宣布宿华和程一笑调整分工，宿华辞去首席执行官一职，继续担任董事长、执行董事、薪酬委员会委员，负责制定公司长期战略；程一笑出任首席执行官，负责公司日常运营及业务发展。</p><p>&nbsp;</p><p></p><h4>消息称张一鸣通知负责人：PICO业务看不到希望将关停，字节人士否认</h4><p></p><p>&nbsp;</p><p>10月21日消息，有媒体报道称PICO业务将被逐步关停，字节跳动放弃元宇宙。文章引述相关人士的说法称，PICO 负责人近期前往新加坡找张一鸣汇报工作，得到的反馈是字节跳动将逐步放弃 PICO 业务，并称原因是“PICO 所处的硬件领域非字节跳动所擅长，几年下来成绩未达预期、并且看不到未来的希望”。</p><p>&nbsp;</p><p>对此，字节跳动相关负责人向媒体回应称，此消息不实。PICO在正常运营，公司会长期投入XR业务。</p><p>&nbsp;</p><p></p><h4>财报一发，没有一个创始人能笑着面对</h4><p></p><p>&nbsp;</p><p>马斯克在特斯拉财报会上表现像“小婴儿”</p><p>&nbsp;</p><p>金融分析师兼 YouTube 博主凯文・帕夫拉斯（Kevin Paffrath）透露，在特斯拉糟糕的财报电话会议上，其首席执行官埃隆・马斯克（Elon Musk）表现得像个“小婴儿”，几乎要哭出来！Paffrath说：“对于一位公司的领导者来说，抱怨经济形势而不是提出应对计划，这似乎是可悲的！”</p><p>&nbsp;</p><p>据悉，马斯克在电话会议上一度暗示，由于利率上升，借贷成本更高，他将推迟工厂的建设。他说：“如果利率保持在高位，甚至更高，人们购买汽车就会变得更加困难。他们根本负担不起买车的开支。”但Paffrath抨击了马斯克的回应，称这位特斯拉首席执行官“害怕了”，并建议马斯克应该与墨西哥政府谈判达成更好的协议，或者可能“向高收入地区打广告”。Paffrath此前曾呼吁特斯拉向非粉丝推广其产品。</p><p>&nbsp;</p><p>谷歌市值大跌8500亿，云业务Q3收入不及预期</p><p>&nbsp;</p><p>10月25日，谷歌母公司Alphabet发布了截至9月30日的2023财年第三季度财报。财报显示，Alphabet第三季度营收为766.93亿美元，较上年同期的690.92亿美元增长11%，按固定汇率计算同比增长11%；净利润为196.89亿美元，较上年同期的139.10亿美元增长42%。</p><p>&nbsp;</p><p>Alphabet第二季度营收和每股收益均超出分析师一致预期，但是云业务营收不及预期。由于云业务对于Alphabet未来增长至关重要，它的营收不及预期引发投资者担忧，拖累股价在盘后交易中大跌6.65%，市值蒸发1164亿美元(约合8510亿元人民币)。</p><p>&nbsp;</p><p>微软CEO今年薪酬降低11.6%，承认放弃Windows Phone是错误决定</p><p>&nbsp;</p><p>10月25日，微软发布了 2024 年第 1 财季（截至 2023 年 9 月 30 日）财报，其中显示今年微软 CEO 萨蒂亚・纳德拉（Satya Nadella）基于绩效的薪酬有所降低，并且不再与微软 XGP 业务增长情况挂钩。外媒认为这是因为微软 XGP 用户增长连续两年未达到预期目标。</p><p>&nbsp;</p><p>此外，纳德拉在接受媒体采访时候，还承认放弃 Windows Phone 和移动设备是错误决定，这也是微软历史上第三位承认在移动领域犯错的首席执行官。</p><p>&nbsp;</p><p>纳德拉在2014年接替鲍尔默（Steve Ballmer）担任CEO，仅仅一年之后就将鲍尔默任内斥资74亿美元收购的诺基亚手机业务勾销。纳德拉接受采访时候表示微软“退出”手机业务本应该处理的更好。勾销诺基亚手机业务之后，Windows Phone事实上就退出了移动舞台。微软后来推出了运行Android的 Surface Duo和Surface Duo 2智能手机，但由于没有后续产品，也缺乏软件更新，Surface Duo手机品牌的未来悬而未决。</p><p>&nbsp;</p><p>扎克伯格豪赌元宇宙巨亏271亿元，明年AI将成Meta最大投资领域</p><p>&nbsp;</p><p>10月26日，脸书母公司Meta发布了截至 9 月 30 日的 2023 财年第三季度财报。财报显示，Meta 第三季度总营收为 341.46 亿美元，较上年同期的 277.14 亿美元增长 23%；净利润为 115.83 亿美元 (约合 847.55 亿元人民币)，较上年同期的 43.95 亿美元增长 164%。</p><p>&nbsp;</p><p>Meta 创始人马克・扎克伯格 (Mark Zuckerberg) 大力押注的元宇宙业务依旧在“流血”。第三季度，Meta 负责元宇宙业务的现实实验室部门再次营业亏损 37 亿美元 (约合 271 亿元人民币)。对于公司的后续发展，扎克伯格表示在2024年，就工程和计算资源而言，AI将成为Meta最大的投资领域。此外，扎克伯格补充道，为了避免布置大量的新员工，公司将降低一些非AI项目的优先级，并将相关人员转向从事AI工作。</p><p>&nbsp;</p><p>科大讯飞净利大跌，创始人套现25亿</p><p>&nbsp;</p><p>据科大讯飞财报显示，今年第三季度，科大讯飞的营收同比实现2.89%的增长，但公司前三季度的营收却小幅下跌0.37%。利润方面，科大讯飞第三季度、前三季度归属于上市公司股东的净利润分别减少81.86%、76.36%。而前三季度，公司扣除非经常性损益的利润为-3.24亿元。</p><p>&nbsp;</p><p>此次发布的财报中，科大讯飞还提到，截至7月3日，公司2022年7月3日通过的股份回购期限已届满。不过，公司大手笔回购的同时，身为科大讯飞创始人和董事长的刘庆峰却在第三季度减持了公司的股份。若按8月14日科大讯飞63.98元/股的收盘价计算，刘庆峰此次减持预计将套现超25亿元。对此，科大讯飞在公司发布的公告中解释称，此前刘庆峰曾通过质押融资等方式借款筹集资金23.5 亿元，鉴于债务已到期，刘庆峰需要减持股份用于偿还上述借款本金。</p><p>&nbsp;</p><p>另外，因为学习机出现违背主流价值观内容，并引发科大讯飞股价午后跳水触及跌停后，科大讯飞董事长刘庆峰表示，问题出现后，已经第一时间把大模型的安全审核能力放进来，同时也跟公安报备了相应情况。他同时感慨，“中国的创新不容易，我们今天刚发布了星火大模型最新版本，但负面舆情却铺天盖地，这背后是有推手的。”</p><p>&nbsp;</p><p></p><h4>国家数据局正式揭牌</h4><p></p><p>&nbsp;</p><p>10月25日上午，国家数据局正式揭牌。国家数据局负责协调推进数据基础制度建设，统筹数据资源整合共享和开发利用，统筹推进数字中国、数字经济、数字社会规划和建设等，由国家发展和改革委员会管理。</p><p>&nbsp;</p><p></p><h2>IT 业界</h2><p></p><p></p><h4>故障超过 8 小时，语雀公布原因及赔偿方案</h4><p></p><p>&nbsp;</p><p>10&nbsp;月 23 日消息，据多位用户反馈，蚂蚁集团旗下的在线文档编辑与协同工具语雀在 23 日 14:00~15:00&nbsp;之间出现大规模服务器故障，在线文档和官网目前均无法打开。在经历了近 10&nbsp;小时的故障之后，语雀服务现已全部恢复正常，各端语雀都可以正常访问，功能也恢复。</p><p>&nbsp;</p><p>10月24日晚，蚂蚁集团旗下在线文档编辑与协同工具语雀就前一日持续7个多小时的重大服务故障致歉，并公布故障原因及赔偿方案。语雀方面表示，10月23日下午，服务语雀的数据存储运维团队在进行升级操作时，由于新的运维升级工具bug，导致华东地区生产环境存储服务器被误下线。语雀将向所有受到故障影响的用户提供赔偿，针对语雀个人用户将赠送6个月的会员服务，针对语雀空间用户会单独制定赔偿方案。语雀方面强调，用户所有数据均未丢失。</p><p>&nbsp;</p><p>更多详情可以查看：</p><p><a href="https://mp.weixin.qq.com/s/LOjiaULzEgkI5VEe74kX0g">语雀突发 P0 级事故！宕机 8 小时被网友怒喷，运维又背锅？</a>"</p><p>&nbsp;</p><p></p><h4>小米正式发布小米澎湃OS</h4><p></p><p>&nbsp;</p><p>10月26日，在小米澎湃OS暨小米14系列新品发布会上，小米董事长雷军发表演讲。雷军表示，小米集团宣布全新战略升级：从手机 X AIo，升级到人车家全生态。</p><p>&nbsp;</p><p>而小米澎湃OS也正式亮相。雷军表示，他对澎湃OS提出了五个要求：一、每个独立设备能实现最佳性能表现；二、更加便捷高效的跨端连接；三、成为生态智能大脑，为用户提供主动智能服务；四、实现跨设备全系统隐私安全的坚固防护；五、坚持建设开放生态。</p><p>&nbsp;</p><p></p><h4>华为：全面完成 5G-A 技术性能测试</h4><p></p><p>&nbsp;</p><p>近日，华为全面完成5G-A技术性能测试。华为方面介绍称，5G-A作为5G的演进和增强，连接速率和时延等传统网络能力实现了10倍提升，同时引入了通感一体、无源物联、内生智能等全新的革命性技术。</p><p></p><h4>Python 公布了实现 no-GIL Python 的计划</h4><p></p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/XahMWSZLXwqYo6TWKDvg?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Python </a>"指导委员会宣布接受 PEP 703（Making the Global Interpreter Lock Optional，让全局解释器锁成为可选），公布了实现 no-GIL（或称为自由线程）Python 详细的路线图。</p><p>&nbsp;</p><p>Python 的全局解释器锁（GIL）阻止了同时多线程执行代码，成为了在多核 CPU 上提高 Python 代码运行效率的一大障碍，消除这一障碍是好事，但这也有可能会破坏现有的扩展模块，或显著降低性能以及可维护性。而第三方软件包生态系统是 Python 的一大优势，Python 项目在实现自由线程时需要谨慎，需要避免破坏这一优势。推进 PEP 703 需要将其纳入主线，作为定期发布版本的一部分推出。Python 指导委员计划分成三个阶段：实验阶段，支持但不默认阶段，默认阶段。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kVVkXrWbX7CVWPR7Y6Fe</id>
            <title>程序员利用漏洞篡改ETC余额，一年私吞260余万元；小马智行获沙特1亿美元投资；AMD回应“中国区大幅裁员” | AI一周资讯</title>
            <link>https://www.infoq.cn/article/kVVkXrWbX7CVWPR7Y6Fe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kVVkXrWbX7CVWPR7Y6Fe</guid>
            <pubDate></pubDate>
            <updated>Sun, 29 Oct 2023 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, OpenAI, Anthropic, 人工智能
<br>
<br>
总结: 谷歌同意向OpenAI竞争对手Anthropic投资20亿美元，加剧了人工智能领域的竞争，以争取下一个重大突破。 </div>
                        <hr>
                    
                    <p></p><blockquote>传谷歌同意向OpenAI竞争对手Anthropic至多投资20亿美元；AMD回应裁员传闻：系公司组织架构小幅度优化和重组；小米正式发布澎湃OS，雷军确认小米汽车明年上半年上市；腾讯混元大模型正式开放“文生图”功能，代码能力大幅提升 20%……</blockquote><p></p><p></p><h2>资讯</h2><p></p><p></p><h4>传谷歌同意向OpenAI竞争对手Anthropic至多投资20亿美元</h4><p></p><p></p><p>10月28日消息，据知情人士透露，谷歌已经同意在此前投资的基础上，再向OpenAI竞争对手Anthropic至多投资20亿美元。此举可能促使人工智能领域的初创公司加剧竞争，以争取首先取得下一个重大突破。</p><p></p><p>知情人士说，谷歌同意先期向Anthropic投资5亿美元，并同意随着时间的推移再增加15亿美元。在这笔投资之前，亚马逊也曾于上个月承诺向Anthropic投资40亿美元。Anthropic由前OpenAI工程师于2021年创立，目标是开发能与GPT-4竞争的生成式人工智能模型。</p><p></p><h4>花旗计划为其4万多名程序员部署生成式AI</h4><p></p><p></p><p>据报道，随着华尔街继续拥抱人工智能这项新兴的技术，花旗集团（Citigroup）计划为其4万多名程序员中的绝大部分使用生成性人工智能（GAI）。作为小型试点项目的一部分，花旗集团开始允许大约250名开发人员率先体验生成式人工智能。明年，花旗集团计划将该计划扩展到绝大多数程序员。</p><p></p><h4>马斯克：X要与YouTube和LinkedIn竞争</h4><p></p><p></p><p>据知情人士称，X平台所有者埃隆·马斯克和该公司CEO琳达·亚卡里诺周四表示，他们将YouTube和LinkedIn视为未来的竞争对手，同时在视频和招聘领域寻求新的业务线。</p><p></p><p>消息人士称，X周四举行了一次全体会议，以纪念马斯克收购推特一周年。在会上，马斯克和亚卡里诺提到了这两个网站。该知情人士说，两人还提到了创建名为XWire的新闻服务的雄心，该服务将与Cision的美通社（PR Newswire）竞争。该知情人士要求不具名，因为讨论是私人的。</p><p></p><p>这位知情人士说，这次会议是马斯克和亚卡里诺首次共同向整个公司发表讲话。亚卡里诺于今年5月被聘为X的CEO，此前她在NBC环球负责广告和合作。</p><p></p><h4>OpenAI组建新团队以评估AI的“灾难性风险”</h4><p></p><p></p><p>美国当地时间周四，人工智能研究公司OpenAI宣布组建新团队，以评估和减轻与人工智能相关的“灾难性风险”。OpenAI在周四的声明中表示，这个新团队名为Preparedness，其主要任务是“跟踪、评估、预测和保护”人工智能造成的潜在重大问题，包括核威胁。此外，该团队将致力于减轻“化学、生物和放射性威胁”，以及人工智能的“自主复制”行为。Preparedness团队将解决的其他风险包括人工智能欺骗人类的行为，以及网络安全威胁。</p><p></p><p>OpenAI在更新中写道：“我们相信，前沿人工智能模型的能力将超越目前最先进的模型，有可能造福全人类。不过，它们也构成了越来越严重的风险。”</p><p></p><h4>小米正式发布澎湃OS，雷军确认小米汽车明年上半年上市</h4><p></p><p></p><p>10月26日晚，小米集团创始人、董事长兼CEO雷军宣布集团战略正式升级为“人车家全生态”，并发布了小米澎湃OS操作系统、数字高端旗舰小米14系列，以及Xiaomi Watch S3、小米电视S Pro 85等6款AIoT新品。</p><p></p><p>雷军表示，过去几年小米一直在突破认知，改变成长，今天将迎来“跨越时刻”。小米发布的新战略“人车家全生态”，是以人为中心、对“人车家全生态”进行整合，而承接新战略的关键是小米澎湃OS。</p><p></p><p>雷军还透露了小米汽车的最新情况，称“进展顺利”，将于2024年上半年正式上市。他表示，“人车家全生态”战略很快将完成全面落地的最后拼图。</p><p></p><h4>AMD回应“中国区大幅裁员”：系公司组织架构小幅度优化和重组</h4><p></p><p></p><p>近期，有消息称超威半导体公司（AMD）即将在中国区进行大规模裁员，本轮裁员比例可能为10％-15％，或涉及数百名员工。</p><p></p><p>对此，10月26日，AMD方面回应：“网络传闻失实。基于公司战略的调整，公司近期对组织架构进行了小幅度的优化和重组。”</p><p></p><h4>亚马逊高管解读Q3财报：生成式AI未来几年将为AWS带来数百亿美元营收</h4><p></p><p></p><p>10 月 27 日，亚马逊发布了2023财年第三季度财报。报告显示，亚马逊第三季度净销售额为1430.83亿美元，同比增长13%，不计入汇率变动的影响为同比增长11%；净利润为98.79亿美元，同比增长244%；每股摊薄收益为0.94美元，相比之下去年同期的每股摊薄收益为0.28美元。</p><p></p><p>财报发布后，亚马逊CEO Andy Jassy和CFO Brian Olsavsky回答了投资者提问。Andy Jassy表示：“公司运业务目前的预计年销售额已经达到920亿美元，而90%的全球IT业务处理仍然基于本地设备，我们相信这一比例将有机会反转，所以我们的增长空间非常大。再加上刚刚涌现出来的，生成式人工智能市场方面的巨大机会，也将在未来几年内为AWS带来数百亿美元的营收。”</p><p></p><h4>腾讯混元大模型正式开放“文生图”功能，代码能力大幅提升 20%</h4><p></p><p></p><p>10 月 26 日，腾讯宣布，腾讯混元大模型迎来全新升级，升级后的腾讯混元中文能力整体超过 GPT3.5，代能力大幅提升 20%，达到业界领先水平。同时，腾讯混元大模型正式对外开放“文生图”功能。</p><p></p><p>腾讯机器学习平台算法负责人康战辉重点介绍了混元大模型代码方面的能力。代码技术主要是两个方向进行了优化：一是代码预训练，二是 SFT 指令微调。腾讯表示，经过对 32 种主流语言代码文件、各类计算机书籍和博客的学习增训，腾讯混元代码处理水平提升超过 20%，代码处理效果胜出 ChatGPT 6.34%，在 HumanEval 公开测试集指标上全面超过 Starcoder、Codellama 等业界头部开源代码大模型。</p><p></p><h4>小马智行获沙特1亿美元投资</h4><p></p><p></p><p>日前，小马智行微信公众号发文称，获得沙特阿拉伯王国新未来城（NEOM）及旗下投资基金NIF（NEOM Investment Fund）1亿美元投资。本轮融资资金将用于自动驾驶技术全球化研发和运营投入等方向。</p><p></p><p>据了解，双方计划在新未来城建立自动驾驶生产制造及研发中心，面向沙特新未来城乃至中东及北非地区开展自动驾驶研发与制造，并部署自动驾驶服务车队以及智能汽车相关的基础设施。沙特新未来城将成为小马智行推行技术全球化布局的战略要地。</p><p></p><h4>历时 7 个月，国家数据局正式揭牌</h4><p></p><p></p><p>10 月 25 日，国家数据局正式揭牌。</p><p></p><p>2023 年 3 月，国务院机构改革方案提出组建国家数据局，负责协调推进数据基础制度建设，统筹数据资源整合共享和开发利用，统筹推进数字中国、数字经济、数字社会规划和建设等，由国家发展和改革委员会管理。</p><p></p><p>据悉，目前国家数据局已有两名领导亮相。7 月 28 日，人社部发布国务院任免国家工作人员信息，刘烈宏获任命为国家数据局局长；10 月 11 日，国务院任命沈竹林为国家数据局副局长，另有媒体报道，同月国家数据局启动招聘，发布多个职位。</p><p></p><h4>亚马逊推出人工智能图像生成功能</h4><p></p><p></p><p>10月25日，亚马逊宣布推出测试版图像生成功能。亚马逊称，在亚马逊广告控制台中，广告商只需选择产品并点击“生成”，该工具就能利用人工智能生成功能，根据产品细节在几秒钟内提供一组以生活方式和品牌为主题的图片。</p><p></p><h2>IT 业界热评新闻</h2><p></p><p></p><h4>程序员利用漏洞篡改ETC余额，一年私吞260余万元</h4><p></p><p></p><p>据警民直通车上海报道，2023年9月，上海市公安局浦东分局北蔡派出所接到某科技公司员工张女士报案称，其公司发现计算机系统被他人篡改数据，导致公司账户钱款损失。民警对公司后台电子数据出现的异常账户进行提取搜证中发现，要想通过漏洞篡改网站后台信息，执行起来难度很大。操作者很有可能是管理网站后台系统的员工，监守自盗的可能性较大。</p><p></p><p>民警随即展开工作，最终嫌疑人曹某迫于压力，主动投案自首。</p><p></p><p>2022年8月，曹某发现所在公司的网站后台有漏洞，身为软件工程师的他，决定铤而走险，用其母亲的身份证自行注册了一个ETC账户，并绑定了其母亲的银行卡。随后曹某以每周4至5次、每次1万元的频率，陆续从该账户内提取了230余万元。之后，曹某又利用朋友的身份证再次办理账号，以同样的方式再次从公司提现36万元。</p><p></p><p>目前，犯罪嫌疑人曹某因涉嫌盗窃罪，已被浦东警方依法刑事拘留，该起案件正在进一步审理中。</p><p></p><p>网友热评：太“刑”了。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Dhsl1B6yaEJUp5EgdWMZ</id>
            <title>可部署手机、适配国产芯……全新升级后的ChatGLM3真的有点东西：智谱 AI 选择继续开源！</title>
            <link>https://www.infoq.cn/article/Dhsl1B6yaEJUp5EgdWMZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Dhsl1B6yaEJUp5EgdWMZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 28 Oct 2023 03:32:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智谱 AI, ChatGLM3, 大模型, 性能提升
<br>
<br>
总结: 智谱 AI 在中国计算机大会上发布了第三代对话大模型 ChatGLM3，该模型在性能方面有显著提升，推理速度提高了2-3倍，推理成本降低一倍。与之前的模型相比，在多个基准测试中表现优异，排名首位。此外，ChatGLM3还具备了多模态理解能力和智能代理能力，可以在更多复杂场景中发挥出色表现。这一创新为自然语言处理技术的应用范围提供了拓展。 </div>
                        <hr>
                    
                    <p>10 月 27 日，智谱 AI 在 2023 中国计算机大会（CNCC）上发布了自研第三代对话大模型 ChatGLM3，这是智谱 AI 在今年内第三次对 ChatGLM 基座模型进行了深度优化。ChatGLM 是由清华大学创新领军工程博士张鹏带领下的团队开发的一个开源且支持中英双语的类 ChatGPT 大语言模型，一经推出就迅速受到大家的关注。</p><p>&nbsp;</p><p>此次 ChatGLM3 发布后，几个小时的时间就覆盖了大模型圈内人的朋友圈，所以 ChatGLM3 本次到底升级了什么？对大模型的发展又产生了哪些影响？</p><p>&nbsp;</p><p></p><h2>一、更强大、更高效、更长，都是 ChatGLM3 的形容词</h2><p></p><p>&nbsp;</p><p>随着人工智能技术的快速发展，自然语言处理领域已经成为最具挑战性和最活跃的研究方向之一。在这个领域中，大型预训练模型被证明是实现卓越性能的关键。</p><p>&nbsp;</p><p>从性能方面，推理速度和成本一直是衡量模型性能的重要指标之一，在众多预训练模型中，ChatGLM 系列模型也一直因其优秀的性能和创新能力而备受关注。而此次智谱 AI 发布的 ChatGLM3 的推理框架是基于最新的高效动态推理和显存优化技术构建的，在相同硬件、模型条件下，相较于目前最佳的开源实现，对比伯克利大学推出的&nbsp;vLLM 以及&nbsp;Hugging Face TGI 的最新版本，推理速度提升了 2-3 倍，推理成本降低一倍，每千&nbsp;tokens 仅 0.5 分，成本相对最低。这些数据足以表明，ChatGLM 系列模型在推理速度和成本方面已具有显著优势。</p><p>&nbsp;</p><p>与 ChatGLM 二代模型相比，ChatGLM3 在 44 个中英文公开数据集测试中表现优异，在国内同尺寸模型中排名首位。评测结果显示，ChatGLM3 在 MMLU、CEval、GSM8K 和 BBH 等基准测试中均取得了显著的性能提升，分别提升了 36%、33%、179%和 126%。这主要得益于其独创的多阶段增强预训练方法，以及更丰富的训练数据以及更优的训练方案。多阶段增强预训练方法在语言模型训练中展现出显著的优势，其根据不同的任务和数据分布来优化模型性能，从而在各种不同的语言任务中取得更好的表现。通过多个预训练阶段的反复迭代和优化，模型得以深入学习语言知识和规律，进而提升对语言的理解能力，这种方法有助于强化模型的泛化能力，使其能够更好地适应各种不同的语言环境。此外，在面对复杂的语言现象时，该方法使模型还能够更加鲁棒地处理各种情况，减少出现偏见或误解的可能性。</p><p>&nbsp;</p><p>除了在基准测试中表现出色，ChatGLM3 还瞄准了 GPT-4V 的技术升级，要知道，GPT-4V 具有每种模态（文本和视觉）的限制和能力，同时呈现出来自所述模态交叉和大规模模型提供的智能和推理的新颖能力。所以本次发布的 ChatGLM3 实现的若干全新功能的迭代升级中，最引人注目的就是多模态理解能力的 CogVLM-看图识语义功能，该功能在 10 余个国际标准图文评测数据集上取得 SOTA。此外，与 GPT-4V 相比，ChatGLM3 的语义能力和逻辑能力都得到了大大增强：</p><p>代码增强模块&nbsp;Code Interpreter 根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；网络搜索增强 WebGLM-接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。</p><p>&nbsp;</p><p>此外，ChatGLM3 目前已经具有了全新的&nbsp;Agent 智能体能力，其集成了自研的 AgentTuning 技术，激活了模型智能代理能力。在智能规划和执行方面，ChatGLM3 相比 ChatGLM 二代提升了 1000%，这一技术开启了一种全新的模型智能体能力，使 ChatGLM3 能够在更多复杂场景中发挥出色表现。例如，ChatGLM3 能够原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理以及操作系统等复杂场景。</p><p>&nbsp;</p><p>非常值得一提的是，为了更好地适应边缘计算的需求，ChatGLM3 还推出了可手机部署的端侧模型 ChatGLM3-1.5B 和 3B。这些模型支持包括 vivo、小米、三星在内的多种手机以及车载平台，甚至支持移动平台上 CPU 芯片的推理，速度可达 20tokens/s。在精度方面，1.5B 和 3B 模型在公开 benchmark 上与 ChatGLM2-6B 模型性能接近。这一创新为自然语言处理应用在移动设备上的部署提供了便捷的方式，进一步拓展了自然语言处理技术的应用范围。</p><p>&nbsp;</p><p>而正是在全新升级的 ChatGLM3 赋能下，生成式 AI 助手智谱清言目前已成为国内首个具备 Advanced Data Analysis（原 Code Interpreter）能力的大模型产品，可支持图像处理、数学计算、数据分析等使用场景。CogVLM 模型则提高了智谱清言的中文图文理解能力，取得了接近&nbsp;GPT-4V 的图片理解能力。它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p><p></p><p><img src="data:image/webp;base64,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" /></p><p></p><p>据悉，目前智谱清言已具有搜索增强能力，它可以帮助用户整理出相关问题的网上文献或文章链接，并整理出答案，这意味着智谱清言将为用户提供更好的自然语言处理服务。</p><p>&nbsp;</p><p></p><h1>二、ChatGLM3 继续开源，“搞好开源”是智谱 AI 的初心</h1><p></p><p>&nbsp;</p><p>在此次 ChatGLM3 大模型的发布现场，智谱 AI 宣布为了进一步推动开源生态，将对包括 ChatGLM3-6B、ChatGLM3-6B-32K 以及 CogVLM-17B 和 AgentLM 在内的多个模型进行开源。</p><p>&nbsp;</p><p>目前，ChatGLM3-6B 模型的开源成绩已经比较可观，它在 44 多个对话模型数据集上的 9 个榜单中排名第一，其开源的 32k 版本 ChatGLM3-6B-32K 在 LongBench 中表现最佳。</p><p>&nbsp;</p><p>伴随着 ChatGLM3 的开源，模型的工作原理和团队技术研发的决策过程可以被更多人了解，模型的透明度和可解释性将更有助于从业者理解模型，增强对模型的信任和使用体验，学术界和产业界的大模型开发者们都可以获取到模型的源代码和参数，基于现有模型参数和算法进行更深入的研究和创新，模型的性能也将有望在短时间内再次完成快速迭代，自然语言处理领域将得到进一步的发展。同时，开放的生态系统和社区也将推动 ChatGLM3 在实际场景中的应用和优化，相关产业将获得基于 ChatGLM3 更智能、更高效的服务和解决方案以完成数字化转型。</p><p>&nbsp;</p><p>其实，ChatGLM3 并不是智谱 AI 的第一次开源，早在今年三月，智谱 AI 就已经陆续宣布大模型产品开源，而且成绩持续斐然，推动大模型行业发展是他们的初心也是一直在做的事情。比如多模态 CogVLM-17B 在开源后，在 10 个多模态榜单中排名第一；再如智能体 AgentLM，其让开源模型达到甚至超过闭源模型的 Agent 能力。</p><p><img src="https://static001.geekbang.org/infoq/d3/d3538a7f3451722e685cedec61ccf18f.webp" /></p><p></p><p>智谱 AI 从 B 端企业服务方面有深厚的基础，他们将自己的大模型进行开源，其实可以更好地让大家从场景落地方面实现大模型技术的创新，这是很多尚未商业化的大模型无法比拟的优势。</p><p>&nbsp;</p><p>当然了，目前有越来越多的公司和研究机构开始将他们的大模型开源，国内比较知名的就有阿里巴巴的通义大模型系列、华为的盘古大模型系列、腾讯的混元大模型系列等多家。但当我们复盘包括智谱 AI 开源在内的这些大模型，我们会发现，它们不仅在中文领域表现出色，也在英文等其他语言领域有着广泛的应用，但由于这些开源的大模型具有极高的参数量和计算量，需要大量的数据和算力支持，所以只有少数的大型科技公司和研究机构能够开发和维护这些大模型。但也正因为这些挑战存在，大模型开源就变得更为重要，只有越来越多的人开始应用开源模型，难题才会有可能解决掉。</p><p>&nbsp;</p><p></p><h1>三、ChatGLM 系列大模型有“势必做好国产化”的决心</h1><p></p><p>&nbsp;</p><p>ChatGLM 3 的发布让智谱 AI 已构建起的全模型产品线更加强大。智谱 AI CEO 张鹏表示：“自 2020 年起，智谱 AI 便专注大模型的自研创新。从早期开始的 GLM 预训练架构的研发，到今天 ChatGLM3 的推出，我们在技术研发、国产适配、开源生态、商业交付等各方面都有了一定进展。我们希望基于当前完整的自研产品线，包括对话、多模态、代码、搜索增强等模型，以及全流程的技术支持，可以更好地支撑行业生态，与合作伙伴一同高速发展。”</p><p>&nbsp;</p><p>自 2022 年初，ChatGLM 系列模型已支持在昇腾、神威超算、海光 DCU 架构上进行大规模预训练和推理，截至目前已支持 10 余种国产硬件生态，包括昇腾、神威超算、海光 DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。通过与国产芯片企业的联合创新，ChatGLM 系列模型性能不断优化，国产硬件生态也得到了大模型国产化的闭环。</p><p>&nbsp;</p><p>ChatGLM 针对国产芯片的场景创新和技术支撑，其实也是我完成高新技术国产化升级的过程，这可以促进更多的研究者、开发者以及企业参与到自然语言处理技术的研究和开发中来，共同推动国内自然语言处理技术的发展。当 ChatGLM 在与国产芯片彼此成就的过程中，这将陆续帮助国产芯片摆脱对国外模型的依赖，增强国内模型的自主可控性，做出更适合中国市场需求的芯片的同时，这对于国家信息安全、产业发展等方面都具有重要的意义，直接增强了国家的科技实力，为国家的科技发展和国际竞争力提升具有重要价值。</p><p>&nbsp;</p><p></p><h1>四、写在最后</h1><p></p><p>&nbsp;</p><p>在 ChatGLM 3 系列模型发布后，智谱 AI 成为了目前国内唯一一个有对标 Open AI 全模型产品线的公司，（以下对比左侧产品为 OpenAI，右侧产品为智谱 AI）：</p><p>对话方面：ChatGPT——ChatGLM（对话）文生图方面：DALL.E——CogView（文生图）代码方面：Codex——CodeGeeX （代码）搜索增强方面：WebGPT——WebGLM （搜索增强）图文理解方面：GPT-4V——ChatGLM 3 (CogVLM,AgentTuning…)</p><p>&nbsp;</p><p>一名微软的算法工程师说，“在硅谷，智谱 AI 的 GLM 应该是最被头部科技企业承认的中国大语言模型。”可见 ChatGLM 是智谱 AI，也是国内大模型厂商追逐 OpenAI 的最大底气。最新一代大模型 ChatGLM3 的开源，在助于推动自然语言处理领域的发展、加速 AI 应用的开发过程、提高模型的可信度和透明度、促进社区合作和创新等方面具有重要的价值。但是否能够完全超越 OpenAI，还要看走出实验室后，ChatGLM3 在具体场景下的应用和性能表现。</p><p>&nbsp;</p><p>但不管怎么说，一直将“持续搞好开源、做好国产化”作为基本功的 ChatGLM，通过不断开放和共享其技术和模型，已经大力促进了全球范围内的技术创新和产业发展，为中国大模型的产业升级和技术创新做出了较为突出的贡献。</p><p>&nbsp;</p><p>事实上，在目前这个阶段，大模型厂商都应该做好以上两项基本功。只有通过稳扎稳打，不断推动大模型技术的发展和应用，才能让“中国大模型”在全球市场中展现出更多的价值。中国的厂商应该积极响应这一号召，加大投入，加强研发，不断提升自身的大模型技术和应用能力，抱团取暖，为中国的人工智能产业做出更大的贡献。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ</id>
            <title>适配更多国产芯片，智谱AI推出第三代基座大模型ChatGLM3</title>
            <link>https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/D5BW4LdBUGislXBCOFIZ</guid>
            <pubDate></pubDate>
            <updated>Sat, 28 Oct 2023 00:11:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 智谱AI, ChatGLM3, 基座大模型, 多阶段增强预训练方法
<br>
<br>
总结: 2023年10月27日，智谱AI在2023中国计算机大会上推出了全自研的第三代基座大模型ChatGLM3及相关系列产品。ChatGLM3采用了独创的多阶段增强预训练方法，使训练更为充分。评测显示，在44个中英文公开数据集测试中，ChatGLM3在国内同尺寸模型中排名首位。智谱AI的ChatGLM3性能更加强大，提升了多个指标。此外，ChatGLM3还实现了若干全新功能的迭代升级，包括多模态理解能力的CogVLM-看图识语义，代码增强模块Code Interpreter，以及网络搜索增强WebGLM-接入搜索增强。ChatGLM3还集成了自研的AgentTuning技术，激活了模型智能体能力。ChatGLM3还推出了可手机部署的端测模型ChatGLM3-1.5B和ChatGLM3-3B，支持多款手机以及车载平台。智谱AI的产品已支持10余种国产硬件生态。ChatGLM3的推理框架在相同硬件、模型条件下，推理速度提升了2-3倍，推理成本降低一倍。智谱AI的ChatGLM3具有搜索增强能力和中文图文理解能力。 </div>
                        <hr>
                    
                    <p>2023年10月27日，<a href="https://www.infoq.cn/article/MhabGNAVvf1NgAeZ2oIZ?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱AI</a>"于2023中国计算机大会（CNCC）上，推出了全自研的第三代基座大模型ChatGLM3及相关系列产品，这也是智谱AI继推出千亿基座的对话模型ChatGLM和ChatGLM2之后的又一次重大突破。</p><p>&nbsp;</p><p>据悉，此次推出的ChatGLM3采用了独创的多阶段增强预训练方法，使训练更为充分。评测显示，在44个中英文公开数据集测试中，ChatGLM3在国内同尺寸模型中排名首位。智谱AI CEO张鹏在现场做了新品发布，并实时演示了最新上线的产品功能。</p><p>&nbsp;</p><p>通过更丰富的训练数据和更优的训练方案，智谱AI推出的ChatGLM3性能更加强大。与ChatGLM2相比，MMLU提升36%、CEval提升33%、GSM8K提升179% 、BBH提升126%。</p><p>&nbsp;</p><p>同时，ChatGLM3瞄向GPT-4V本次实现了若干全新功能的迭代升级，包括多模态理解能力的CogVLM-看图识语义，在10余个国际标准图文评测数据集上取得SOTA；代码增强模块Code Interpreter根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；网络搜索增强WebGLM-接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。ChatGLM3的语义能力与逻辑能力得到了极大的增强。</p><p>&nbsp;</p><p>ChatGLM3还集成了自研的AgentTuning技术，激活了模型智能体能力，尤其在智能规划和执行方面，相比于ChatGLM2提升了1000% ；开启了国产大模型原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理、操作系统等复杂场景。</p><p>&nbsp;</p><p>此外，ChatGLM3本次推出可手机部署的端测模型ChatGLM3-1.5B和 ChatGLM3-3B，支持包括vivo、小米、三星在内的多款手机以及车载平台，甚至支持移动平台上CPU芯片的推理，速度可达20 tokens/s。精度方面1.5B和3B模型在公开benchmark上与ChatGLM2-6B模型性能接近。</p><p>&nbsp;</p><p>自2022年初，智谱AI推出的GLM系列模型已支持在昇腾、神威超算、海光DCU架构上进行大规模预训练和推理。截至目前，智谱AI的产品已支持10余种国产硬件生态，包括昇腾、神威超算、海光DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。</p><p>&nbsp;</p><p>基于最新的高效动态推理和显存优化技术，ChatGLM3当前的推理框架在相同硬件、模型条件下，相较于目前最佳的开源实现，包括伯克利大学推出的 vLLM 以及Hugging Face TGI的最新版本，推理速度提升了2-3倍，推理成本降低一倍，每千tokens仅0.5分，成本最低。</p><p>&nbsp;</p><p>另外，随着WebGLM大模型能力的加入，智谱清言也具有了搜索增强能力，可以帮助用户整理出相关问题的网上文献或文章链接，并直接给出答案。此前已发布的CogVLM 模型则提高了<a href="https://www.infoq.cn/article/Keo5MOT4MavSIyyxTmII?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">智谱清言</a>"的中文图文理解能力，取得了接近GPT-4V的图片理解能力，它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/yyBzwQPKp3djgGsxgRbX</id>
            <title>SOLIDWORKS：把AI放进工业设计软件，把软件放到云上</title>
            <link>https://www.infoq.cn/article/yyBzwQPKp3djgGsxgRbX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/yyBzwQPKp3djgGsxgRbX</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 08:31:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 达索系统, SOLIDWORKS, 3DEXPERIENCE平台, 云化改造
<br>
<br>
总结: 达索系统收购了SOLIDWORKS，并在3DEXPERIENCE平台上进行了云化改造，使得SOLIDWORKS产品可以在云端运行。SOLIDWORKS 2024强调AI驱动，通过整合机器学习和人工智能，实现设计工作的自动化。云化改造和AI技术的引入，使得SOLIDWORKS在工业设计软件领域变得更简单和高效。同时，云化改造也实现了产品全生命周期的动态优化。 </div>
                        <hr>
                    
                    <p>1997年，达索系统收购了当时发展势头正猛的三维CAD软件商SOLIDWORKS；此后的十多年时间里，双方一直以相对独立的模式进行开发和管理。直到2012年，达索系统正式推出了工业云产品3DEXPERIENCE平台，其中，SOLIDWORKS作为平台12个产品模块之一，二者的产品关联性日渐紧密。</p><p></p><p>也正是在那时，基于<a href="https://www.infoq.cn/article/N2gWCU0NhYWjmyxwwy2R">达索系统</a>"3DEXPERIENCE平台，SOLIDWORKS的产品形式也发生了里程碑式的变化——开始逐步进行云化改造和迭代升级。</p><p></p><p>日前，SOLIDWORKS 2024正式发布。相较于此前的版本，新增了10大功能。笔者认为，其中有两大不得不提的看点：SOLIDWORKS 2024强调AI驱动，通过在平台中整合机器学习和人工智能，实现设计工作的自动化；与此同时，SOLIDWORKS可以完全在云端运行，这意味着多个重要功能均可在云上进行操作。</p><p></p><h3>把AI放进工业设计软件，把设计工作变得简单</h3><p></p><p></p><p>长期以来，<a href="https://www.infoq.cn/article/2SA0hh4TISKTD83OfqNw?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">SOLIDWORKS</a>"面向的都是中小企业市场，最大的亮点在于易用性。“无论产品形式和产品线如何变化，我们的原则都是坚持这种易学易用性。通过在平台中整合机器学习和人工智能，尽可能将其应用于3DEXPERIENCE Works产品线，为的就是把复杂的事物变得简单。”达索系统3DEXPERIENCE Works业务部门全球高级副总裁Gian Paolo Bassi在达索系统SOLIDWORKS创新日上表示。</p><p></p><p>在设计工作中，涉及很多需要耗费大量人力和时间的操作，包括阵列操作、绘图、标尺寸等等。“对此，我们希望尽量实现设计过程的自动化。”举例来说，SOLIDWORKS 2024引入了名为“晶格支撑”的设计，通过融合多种算法，只需给定一些特定条件，就可以自动生成相应的设计模型。</p><p></p><p>实际上，早在此次版本更新之前，SOLIDWORKS就已经尝试通过AI的引入，帮助企业客户解决设计效率提升的难题。</p><p></p><p>达索系统大中国区专业客户事业部副总裁吴俊杰在接受InfoQ等媒体采访时介绍，某叉车制造企业在过去20多年里积累了大量数据，在使用<a href="https://www.infoq.cn/article/kKyCIHD5C0kAVkATcW4I?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">CAD</a>"进行设计时，每个工程师都有自己的思路，加上缺乏设计方法论，导致设计不规范，即使是相同的零部件最后也出现了各种不同的版本。对于企业而言，这都是真金白银的成本投入。</p><p></p><p>利用AI技术，SOLIDWORKS帮助该客户梳理出了积累20多年的不同三维模型，从中识别了重复的零件版本，将原有的50多万个零部件减少到了15万个，大大降低了仓库成本。并且，进一步实现了设计流程的标准化、模块化和参数化，缩短了零件整体的设计周期。</p><p></p><p>但是，在SOLIDWORKS看来，这只是一个开始。“在工业设计领域，我们希望能够做一个类似<a href="https://www.infoq.cn/article/tq5VmzbUvckWKG4T9axk">ChatGPT</a>"的工具。例如，只要设计师告诉它‘基于某个年份的摩托车做一个新的零件设计’，然后提供一些特定条件，这个工具就可以通过在数据库中查询对应的模型算法，给出它的设计方案。如果你不满意，还可以进行修改。”Gian Paolo Bassi这样畅想。</p><p></p><p>距离这个目标的实现，显然还有不短的路要走。“企业在发展过程中，数字化基础并不相同。比如，很多企业使用了三维设计软件，但流程上并没有进行设计标准化，导致底层数据杂乱无章。如果把这样的数据提供给AI去学习，效果并不好。所以，一方面，企业需要一些时间制定更规范的标准和规则，梳理好数据基础。另一方面，ChatGPT技术的发展也刚刚起步，还需要更长的时间去迭代和应用。”达索系统大中国区专业客户事业部技术总监戴瑞华解释道。</p><p></p><p>戴瑞华告诉InfoQ，从他个人的洞察来判断，这一时间周期大概在3-5年。期间，除了数据和技术的准备之外，对于企业而言，更关键的是要把<a href="https://www.infoq.cn/article/X68Q0yoLNplMg4RC5969">数字化转型</a>"放到战略的高度。“假设只有设计部门采用了新技术，而没有完成各个流程的打通，设计数据仍然局限在设计部门，没有传输到仿真、电气，甚至是加工、生产和售后等环节，那么，这种所谓的智能化就难以实现，成果也不显著。”</p><p></p><h3>把工业软件放到云上，对产品全生命周期进行动态优化</h3><p></p><p></p><p>实现各个业务流程和环节（不仅仅是设计）的打通，推动企业向真正的数字化、智能化发展，这也是为什么SOLIDWORKS要加速<a href="https://www.infoq.cn/article/s3FKND2dGscQFzg6FvEi">转向云端</a>"的原因。</p><p></p><p>“AI可以从数据中挖掘有价值的信息帮助企业进行决策，其前提是，它需要一个庞大的、可共享的数据库。”而云，恰恰可以满足这一前提。Gian Paolo Bassi表示，SOLIDWORKS支持完全在云端运行，这意味着：</p><p></p><p>首先，企业可以通过付费或者租赁的方式，更灵活地选择软件授权方式，入门成本也更低；其次，通过登录方式进入软件，企业可以了解员工对CAD使用情况；其三，所有设计数据都存储在云端，形成一个统一数据库，管理层可以更高效地调用其中的<a href="https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK">AI工具</a>"，进行决策辅助。</p><p></p><p>值得一提的是，不只是SOLIDWORKS，达索系统的所有产品线在3DEXPERIENCE平台上的数据都是共通的，用户在上面不需要进行任何数据转换，就可以实现跨软件操作，使用平台中其它13个品牌模块，包括高级分析、项目管理、复杂车间应用、机器人技术和工厂建设等，一站式查看所有相关的设计数据、产品数据、装备数据，实现不同业务线之间的高效协作。</p><p></p><p>此外，Gian Paolo Bassi还提及，安全性也是云端版本的一大优势，在他看来，把数据存储在云端相比于公司内部服务器更安全，因为云端有更多的安全措施来防止黑客的入侵和恶意行为。并且，目前达索系统的3DEXPERIENCE Works虽然在云端运行，但是所有放上去的数据都只允许公司内部人员访问，其他客户的模型均没有办法调用，因此安全性也有所保障。</p><p></p><p>“当面对新客户时，我们可能会先建议他们考虑使用云端解决方案。当然，最终决策权在客户手中。对于现有客户，我们仍然会提供多种选项，根据客户需求进行定制。”吴俊杰强调。</p><p></p><p>事实上，把工业软件放到云上，这也是达索系统为践行虚拟孪生策略的迈出的关键一步。2020年，达索系统提出了虚拟孪生概念，与更多人熟知的数字孪生概念不同，它不仅仅是现实世界实体的数字表达，还包含了这个数字化对象的演进历史，比如它从哪里来，如何经过设计、仿真、验证，最终制造出来成为一个实体。除此之外，还包括对它未来的预测，比如这个实体产品在使用过程中如何运作、如何更好地进行维修保养服务等等。</p><p></p><p>“通过云化，可以支持用户在任何设备上使用我们的产品，包括手机和平板电脑。比如，设计人员可以在办公室使用台式机设计产品，销售人员可以在外面使用手机展示并与客户分享。”Gian Paolo Bassi举例。</p><p></p><p>换句话说，虚拟孪生涵盖了产品全生命周期，它是一个动态的过程。企业在物理世界生产运行中获取到的知识、积累的数据会反馈到虚拟世界，对虚拟世界的下一代产品从性能表现、成本控制等各个方面进行优化和创新。</p><p></p><h3>写在最后：陪伴客户更像是场马拉松</h3><p></p><p></p><p>今年，是SOLIDWORKS进入中国市场的第27年，也是吴俊杰执掌中国市场业务的第18年。在这个过程中，技术更迭、客户需求、渠道生态发生着天翻地覆的变化。回顾整个发展历程，他向InfoQ坦言，SOLIDWORKS也曾遇到过一系列挑战。</p><p></p><p>举例来说，SOLIDWORKS最早的续订模式在中国就出现过“水土不服”。“在国内，很多企业对于除了购买费用之外还要支付维护费的方式是难以接受的。因此，从2017年开始，我们尝试了一种新的合作模式，推出了‘335合作计划’（即三年携手共进包、三年卓越进取包、五年战略协作包），把重心从产品升级放到转型服务上，最终使得续订率大幅度提高。”</p><p></p><p>吴俊杰表示，SOLIDWORKS的目标是希望成为中小企业的首选，而这，也是陪伴客户完成马拉松跑的过程，人员的稳定性、生态系统的稳定性非常重要。“所以，我们一直倡导‘本地人做本地事，专业人做专业事’，比如，河南的代理商只服务河南客户，上海代理商只服务上海客户。这听起来是理所当然的道理，但并不是每个企业每个人都能做到。在过去3年疫情时间里，这个模式很好地保障了我们面向各地客户的及时服务。”</p><p></p><p>除此之外，SOLIDWORKS对渠道商的要求也非常严格，从设计到分析、仿真到数据管理、制造到ERP，所有产品线的合作伙伴，都不允许成为其它厂商的代理商。吴俊杰指出，这是其加入SOLIDWORKS18年来如一日所坚持的原则。“最关键的是产品、服务、长期合作以及良好的合作氛围，我们希望通过这四个方面来帮助客户。”</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p><h5>附：SOLIDWORKS 2024 新功能介绍</h5><p></p><p><img src="https://static001.geekbang.org/infoq/32/329c58ab13aee21771943f86427c267c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5fc992be29b316ea238453190c9de0c.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/f2NZ2bR5H5YrHK5Gzqvh</id>
            <title>2023年AI与开源行业：今年第一篇盘点文章出炉了</title>
            <link>https://www.infoq.cn/article/f2NZ2bR5H5YrHK5Gzqvh</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/f2NZ2bR5H5YrHK5Gzqvh</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 07:48:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI研究, GPT-4, Llama模型, 大语言模型
<br>
<br>
总结: 2023年AI研究中的重点是对过去一年已经生效的趋势进行扩展，包括GPT-4和Llama模型等大语言模型的升级和应用。开源与研究社区的关注重点也从潜在扩散模型转向了大语言模型。未来的突破可能来自混合专家模型和替代方案，但基于Transformer的大语言模型仍然是当前最先进的技术方案。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;我们正一步步迈向2023年的终点，也许是时候对这一年来AI研究、行业动态以及开源领域发生的主要变化做一番简要回顾了。当然，这篇文章不可能面面俱到。我们只挑干货，一同审视这风云变幻的一年中都有哪些大事值得回味。</p><p>&nbsp;</p><p></p><h2>2022年的趋势进一步扩展</h2><p></p><p>&nbsp;</p><p>这一年中，AI产品并没有表现出任何根本性的发展或者方法创新。相反，2023年的重点就是对过去一年已经生效的趋势做进一步扩展：</p><p>ChatGPT依托的GPT 3.5升级到了GPT 4。DALL-E 2升级到了DALL-E 3。Stable Diffusion 2.0升级到了Stable Diffusion XL。还有更多...</p><p>&nbsp;</p><p>有个有趣的传言说，GPT-4是由16个子模块组成的混合专家模型（MoE）。据传这16个子模块各自拥有1110亿个参数（作为参考，GPT-3总共也只有1750亿个参数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f9801afa5817941240fc3497febc32b1.png" /></p><p></p><p>2023年AI现状报告中的GPT-3/GPT-4示意图。</p><p>&nbsp;</p><p>GPT-4属于混合专家模型的情况可能是真的，但我们还无法确定。从趋势上看，行业研究人员在论文中分享的信息要比以往更少。例如，虽然GPT-1、GPT-2、GPT-3乃至InstructGPT论文都公开了架构和训练细节，但GPT-4的架构却一直是个谜。再举另外一个例子：虽然Meta AI的第一篇Llama论文详细介绍了用于模型训练的数据集，但从Llama 2模型开始也不再公布这方面信息。关于这个问题，斯坦福大学上周公布了基础模型透明度指数。根据该指数，Llama 2以54%领先，而GPT-4则以48%排名第三。</p><p>&nbsp;</p><p>当然，要求这些企业发布自己的商业秘密也不太合理。总之，逐渐封闭本身是个有趣的趋势，而且就目前来看我们可能会在2024年继续沿着这个路子走下去。</p><p>&nbsp;</p><p>关于规模扩展，今年的另一大趋势在于输入上下文的长度不断增长。例如，GPT-4竞争对手Claude 2的主要卖点之一，就是其支持最多100k的输入token（GPT-4目前仅支持32k&nbsp;token），也就是说其在为长文档生成摘要方面具备鲜明的优势。另外，Claude 2还支持PDF输入，因此在实践应用中更加灵活实用。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07ba24dc6895780a2ec85f2c42cb4a5c.png" /></p><p></p><p>使用Claude 2为PDF文档生成摘要。</p><p>&nbsp;</p><p></p><h2>开源与研究趋势</h2><p></p><p>&nbsp;</p><p>我还记得，去年开源社区的主要关注对象还是潜在扩散模型（最典型的代表就是Stable Diffusion）等计算机视觉模型。扩散模型与计算机视觉与一直高度相关、牢牢绑定。但短短一年过去，如今的开源与研究社区新贵已然变成了大语言模型。</p><p>&nbsp;</p><p>开源（更确切地讲，是公开可用）大语言模型的爆发式增长，一定程度上要归功于Meta发布的首个预训练Llama模型。尽管其仍有许可限制，但已经启发了Alpaca、Vicuna、Llama-Adapter、Lit-Llama等衍生成果和众多研究人员/从业者的关注。</p><p>&nbsp;</p><p>几个月后，Llama 2模型正式亮相，在基本取代Llama 1的基础之上表现出更为强大的功能，甚至还提供了微调版本。</p><p>&nbsp;</p><p>然而，目前的大多数开源大语言模型仍然是纯文本模型。好在Llama-Adapter v1和Llama-Adapter v2微调版本有望将现有大模型转化为多模态模型。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/17/1771efc7d07497f4d2423c6261da694a.png" /></p><p></p><p>Llama-Adapter V2示意图，<a href="https://arxiv.org/abs/2304.15010">https://arxiv.org/abs/2304.15010</a>"</p><p>&nbsp;</p><p>Fuyu-8B是个值得关注的例外模型，此模型刚刚在10月17日正式发布。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a52f412b49851905e69d2a9598e5e41f.png" /></p><p></p><p>&nbsp;Fuyu示意图及注释&nbsp;<a href="https://www.adept.ai/blog/fuyu-8b">https://www.adept.ai/blog/fuyu-8b</a>"</p><p>&nbsp;</p><p>值得注意的是，Fuyu能够将输入补丁直接传递至线性投影（或者叫嵌入层）处以学习其自身图像补丁嵌入，而不会像其他模型/方法那样依靠额外的预训练图像编码器（例如LLaVA和MiniGPT-V），这就极大简化了架构和训练设置。</p><p>&nbsp;</p><p>除了前面提到的少数多模态尝试之外，目前最大的研究重点仍然是如何将GPT-4文本性能迁移至参数范围&lt;100 B的小模型当中。目前的主要技术难点则包括硬件资源成本与限制、可访问数据量不足，以及开发时间太短（受到发布计划的影响，大多数研究人员不可能投入数年时间来训练单一模型）。</p><p>&nbsp;</p><p>然而，开源大语言模型的未来突破并不一定来自将模型扩展至更大规模。在新的一年中，我们将继续关注混合专家模型能否将开源模型提升到新的高度。</p><p>&nbsp;</p><p>另一个有趣的现象，就是我们在研究前沿还看到了一些针对基于Trasnformer大语言模型的替代方案，包括循环RWKV大模型和卷积Hyena大模型，希望能够提供运行效率。但必须承认，基于Transformer的大语言模型仍然是当前最先进的技术方案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/e0/e0845a957314a5673c1997a0032c6e97.png" /></p><p></p><p>带注释的Hyena大模型架构示意图：&nbsp;<a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna">https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna</a>"</p><p>&nbsp;</p><p>总的来讲，2023年是开源活动高度活跃的一年，也带来了不少突破和进步，并切实证明了技术研究工作有着一加一大于二的协同效应。但令人遗憾的是，仍有声音在积极反对和打击开源AI技术。希望我们能够继续保持住这股积极的势头，建立起更高效的解决方案和替代方案，而不仅仅是继续依赖科技巨头们发布的类ChatGPT产品。</p><p>&nbsp;</p><p>在本小节的最后，我们要感谢开源和研究社区的付出。你们的努力让可以运行在单个GPU上的小型高效模型成为现实，包括1.3B参数的phi 1.5、7B参数的Mistral和7B&nbsp;Zephyr，这些都拥有接近大型专有模型的性能表现。这样的趋势令人兴奋，期待相关工作能在2024年带来更多进展。</p><p>&nbsp;</p><p></p><h2>关于生产力的承诺</h2><p></p><p>&nbsp;</p><p>在我看来，开源AI就是开发高效、定制大语言模型的主要途径，其中包括根据各种个人/特定领域数据、针对不同场景进行微调的大模型。我自己经常在社交媒体上讨论Lit-GPT，这是我正在积极贡献的一个开源大语言模型。而且我觉得开源并不代表粗糙，我也希望能在保持开源的同时、让成果拥有出色的设计水平。</p><p>&nbsp;</p><p>自从ChatGPT发布以来，我们看到大语言模型几乎被应用在各个领域。屏幕前的读者可能已经体验过ChatGPT，所以这里就不具体解释大模型在不同场景下的实际效果了。</p><p>&nbsp;</p><p>关键在于，我们得把生成式AI之力用在“正确”的地方。比如说，ChatGPT肯定不擅长回答我们常去的杂货店晚上几点关门。我个人最喜欢的用法之一，就是让它帮我修改文章中的语法、或者是集思广益，包括给句子和段落做做润色等。从更宏观的角度看，大语言模型做出了关于生产力的承诺，可能很多朋友都体验过它带来的效率提升。</p><p>&nbsp;</p><p>除了常规文本大模型之外，微软和GitHub的Copilot编码助手也在日趋成熟，并受到越来越多程序员们的喜爱。今年早些时候，Ark-Invest发布的报告估计，代码助手有望将编码任务的完成时间缩短约55%。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c736baf931b578fe0828e2dda51ab70.png" /></p><p></p><p>编码助手示意图&nbsp;<a href="https://ark-invest.com/home-thank-you-big-ideas-2023/">https://ark-invest.com/home-thank-you-big-ideas-2023/</a>"</p><p>&nbsp;</p><p>实际效果究竟有没有55%尚有争议，但如果大家已经体验过编码助手，就会发现它们确实很有帮助，能够将繁琐的编码相关任务变得更加轻松。</p><p>&nbsp;</p><p>而且有一点是肯定的：编码助手将长期存在，并随着时间推移变得越来越强大。它们最终会取代人类程序员吗？我希望不会，但它们无疑会让现有程序员变得更具生产力。</p><p>&nbsp;</p><p>那这对于Stack Overflow又意味着什么？《AI技术现状》报告中包含一份图表，展示了Stack Overflow与GitHub网站之间的流量对比，后者的逐渐胜出可能就跟Copilot的采用率提升有关。但我个人认为形成这种趋势的应该不只是Copilot，ChatGPT/GPT-4在编码任务方面的表现也相当出色，所以我怀疑Stack Overflow下滑是整个生成式AI阵营发展壮大的共同结果。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b94b244522d10dcf26e825a3510ea43c.png" /></p><p></p><p>《2023年AI现状报告》（<a href="http://stateof.ai/">http://stateof.ai/</a>"）中的图表</p><p>&nbsp;</p><p></p><h2>AI仍不完善</h2><p></p><p>&nbsp;</p><p></p><h3>幻觉问题</h3><p></p><p>&nbsp;</p><p>2022年困扰大语言模型的问题在今年仍未得到解决：它们会生成负面内容，而且经常产生幻觉。这一年中倒确实出现了有望解决问题的几种方法，包括利用人类反馈的强化学习（RLHF）以及英伟达的NeMO Guardrails等。然而，这些方法要么过于严格、要么只能算是松散的补丁。到目前为止，还没有任何方法（甚至没有可靠的思路）能够在不削弱大模型能力的同时，100%解决掉幻觉问题。在我看来，这一切都取决于我们如何使用大语言模型：别指望在所有场景下都使用大模型——数学计算还是交给计算器比较好；尽量用大模型处理它最擅长的文本创作等工作，并保证认真检查它的输出内容。</p><p>&nbsp;</p><p>此外，对于特定的业务类应用，探索检索增强（RAG）也是一种值得考虑的折衷方案。在RAG中，我们需要从语料库中检索相关文档段落，再根据检索到的内容微调大模型所生成的文本。这种方式让模型能够从数据库和文档中提取外部信息，而不必记住所有知识。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a68ea9ae615d5aab815c4ae6beccb540.png" /></p><p></p><p>&nbsp;我自己的新书《Machine Learning Q and AI》（<a href="https://leanpub.com/machine-learning-q-and-ai/">https://leanpub.com/machine-learning-q-and-ai/</a>"）中的RAG示例。</p><p></p><h3>版权问题</h3><p></p><p>另一个更紧迫的问题，则是围绕AI出现的版权争论。根据维基百科的解释，“对于受版权保护的素材训练而成的大语言模型，模型自身的版权应如何对待仍悬而未决。”总的来说，相关规则似乎仍在起草和修改当中。我希望无论最终规则如何，其内容都应尽可能明确，以便AI研究人员和从业者能够做出相应的调整和行动。</p><p>&nbsp;</p><p></p><h3>评估问题</h3><p></p><p>长久以来，困扰学术研究的一大难题在于，目前流行的基准测试和排行榜所采取的评估方法早就半公开了，其测试集甚至已经被某些大模型用作训练数据。phi 1.5和Mistral就都存在这样的问题。</p><p>&nbsp;</p><p>也有人在用其他大模型自动做评估，但这种方式不擅长处理那些跟偏好相关的问题。总之，不少论文已经在依赖GPT-4作为辅助性质的模型评估方案。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b0e58f8e55d818ae3262b3852a84a18.png" /></p><p></p><p>LIMA论文中的人类与GPT_4偏好评估示例。</p><p></p><h3>收入问题</h3><p></p><p>生成式AI目前仍处于探索阶段，不过文本和图像生成器已经能够在特定场景下带来不错的表现。然而，由于高昂的托管和运行时间成本，这些工具能够为企业产生正向现金流仍是个备受争议的问题。例如，有报道称OpenAI过去一年亏损了5.4亿美元。另一方面，最近的报道指出OpenAI目前的单月收入为8000美元，已经足以抵偿或超过其运营成本。</p><p>&nbsp;</p><p></p><h3>伪造图像</h3><p></p><p>由生成式AI引发的另一个大问题，就是伪造图像和视频。这类隐患在当前的社交媒体平台上已经相当明显。伪造图像和视频一直是个大麻烦，而且凭借远低于Photoshop等内容编辑软件的准入门槛，AI技术已经将严重性提升到了新的水平。</p><p>&nbsp;</p><p>目前有一部分AI系统在尝试检测由AI生成的内容，但这些系统在文本、图像和视频检测中的表现都不够可靠。某种程度上，遏制并解决这些问题的唯一方法仍然要依靠人类专家。就如同我们不能轻易相信网上某个论坛或者网站中的医疗或者法律建议一样，我们也绝不能在未经认真核实的情况下，就盲目相信网络上散播的图像和视频。</p><p>&nbsp;</p><p></p><h3>数据集瓶颈</h3><p></p><p>跟之前提到的版权争议相关，不少企业（包括Twitter/X和Reddit）都关闭了免费API以增强经营收入，同时也防止爬取者收集其平台数据用于AI训练。</p><p>&nbsp;</p><p>我见过不少由数据集专职收集厂商打出的宣传广告。从这个角度来看，尽管AI确实会用自动化取代一部分工作岗位，但似乎同时也创造出了新的职务类型。</p><p>&nbsp;</p><p>目前来看，为开源大模型做贡献的最佳方式之一，就是建立一个众包性质的数据集平台，在这里搜集、整理并发布明确允许大语言训练使用的数据资源。</p><p>&nbsp;</p><p></p><h2>RLHF会是破解难题的正确答案吗？</h2><p></p><p>在Llama 2模型套件发布时，我很高兴看到其中包含了可通过聊天进行微调的模型。Meta AI也使用人类反馈强化学习（RLHF）提高了模型的实用性和无害性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b63ec4577538c4d2d18fc452dfc89c91.png" /></p><p></p><p>Llama 2论文中的注释图：开放基础与微调聊天模型，&nbsp;<a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>"</p><p>&nbsp;</p><p>我一直觉得RHLF是种非常有趣、而且极具前景的方法。但除了InstructGPT、ChatGPT和Llama 2之外，大多数模型并没有广泛采用。可在无意之中，我还是找到了下面这份RLHF流行度统计图表。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cff653f92dad9e5d458d773e8a7f7a5f.png" /></p><p></p><p>《2023年AI现状报告》中的RLHF流行度图表。</p><p>&nbsp;</p><p>由于RLHF的实施难度比较大，所以大部分开源项目仍然采取指令微调的有监督微调方式。</p><p>RLHF的最新替代方案是直接偏好优化（DPO）。在相关论文中，研究人员表示RLHF中拟合奖励模型的交叉熵损失可以直接用于大模型的微调。根据他们的基准测试，DPO的效率更高，而且在对质量的响应方面一般也优于RLHF/PPO。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/0788840d627217fe1376a32e3c2d53c7.png" /></p><p></p><p>DPO论文（<a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>"）中的注释图。</p><p>&nbsp;</p><p>但DPO似乎还未得到广泛使用。而令我兴奋的是，两周之前Lewis Tunstall及其同事通过DPO训练了首个公开可用的大语言模型，该模型的性能似乎优于由RLHF训练而成的大型Llama-2 70b聊天模型：</p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f92e39e0ea386cf0b9ebaeb68b302b51.png" /></p><p></p><p>Zephyr 7B模型公告截图。</p><p>&nbsp;</p><p>而且值得注意的是，RLHF并非专门用于优化基准性能；目前这种方法的主要用途仍是由人类用户评估模型的“实用性”和“无害性”。</p><p>&nbsp;</p><p></p><h2>分类专用模型</h2><p></p><p>&nbsp;</p><p>我上周刚刚在Packt生成式AI大会上做了演讲，特别强调目前文本模型最典型的用例之一就是内容分类。比如说垃圾邮件分类、文档分类、客户评论分类以及对社交媒体上的有毒言论做标记等等。</p><p>&nbsp;</p><p>根据个人经验，使用“小型”大模型（例如DistilBERT）完全可以在单个GPU上实现非常好的分类性能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/70/70c5d650c538d9caa218d1d73a257c7c.png" /></p><p></p><p>大家可以通过微调，将“小型”大模型用作文本分类器。</p><p>&nbsp;</p><p>我曾经尝试使用“小型”大模型进行过文本分类演练，其中的Sylvain Payot源自对现成Roberta模型的微调，并成功在IMDB电影评论数据集上实现了高于96%的预测准确率。（作为对比，我在该数据集上训练过的最佳机器学习词袋模型，其准确率也仅有89%。）</p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f15b5e86e73c4bf29dfdec59e057e62b.png" /></p><p></p><p>我在深度学习基础课上讨论最佳分类模型。</p><p>&nbsp;</p><p>话虽如此，但目前我还没看到任何将大语言纳入分类场景的尝试或者趋势。大多数从业者在这类场景中仍然使用基于BERT的编码器模型或编码器-解码器模型，例如2022年推出的FLAN-T5。这可能是因为此类架构的效果已经足够令人满意。</p><p>&nbsp;</p><p></p><h2>表格数据集现状</h2><p></p><p>2022年，我写过一篇《表格数据的深度学习简史》（<a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html">A Short Chronology Of Deep Learning For Tabular Data</a>"），其中涵盖了很多关于深度学习的有趣表格数据方法。而且跟前面提到的分类大模型类似，表格数据集在这一年中同样没有多少进展……也可能是因为我太忙了，没有注意到。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b8327fce84a251066a0bc03a7dcc124.png" /></p><p></p><p>表格数据集示例。</p><p>2022年，Grinsztajn等人发表了名为《为什么树状模型在表格数据上仍然优于深度学习？》（<a href="https://arxiv.org/abs/2207.08815">Why do tree-based models still outperform deep learning on tabular data?</a>"）的文章。我相信对于中小型数据集（10k训练样本）上的表格数据，树状模型（随机森林和XGBoost）优于深度学习方法这个主要结论仍然正确。</p><p>&nbsp;</p><p>以该结论为基础，XGBoost在诞生近十年之后发布了2.0版本大更新。新版本拥有更高的内存效率、支持不适合内存存储的大型数据集以及多目标树等。</p><p>&nbsp;</p><p></p><h2>2023年计算机视觉现状</h2><p></p><p>虽然今年的重头戏都在大语言模型这边，但计算机视觉领域也取得了不少进展。考虑到本文的篇幅已经很长了，这里就不赘述计算机视觉的最新研究成果。具体可以看我在今年CVPR 2023大会上发表的这篇文章（<a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-10-state-of-computer">https://magazine.sebastianraschka.com/p/ahead-of-ai-10-state-of-computer</a>"）。</p><p>&nbsp;</p><p>除了研究之外，与计算机视觉相关的AI技术还激发出更多新产品和新体验，而且这一切都在2023年内逐步发展成熟。</p><p>&nbsp;</p><p>例如，当我今年参加奥斯汀召开的夏季SciPy大会时，就看到一辆真正无人驾驶的Waymo汽车在街道上驶过。</p><p>&nbsp;</p><p>而在观看电影时，我也看到AI在电影行业中得到愈发普遍的应用。比如《夺宝奇兵5》中哈里森·福特的去衰老特效，就是由制作团队利用演员旧素材训练出的AI模型完成的。</p><p>&nbsp;</p><p>此外，生成式AI功能现已广泛纳入知名软件产品当中，比如说Adobe公司的Firefly 2。</p><p>&nbsp;</p><p></p><h2>2024年展望</h2><p></p><p>终于来到最后的预测环节，这也是最具挑战的部分。去年，我预计大语言模型有望在文本和代码以外的其他领域迎来更多应用。这个结论基本得到证实，比如说DNA大模型HyenaDNA；另外还有Geneformer，这是一个由3000万单细胞转录组预训练而成的transformer模型，用于促进网络生物学的研究。</p><p>&nbsp;</p><p>到2024年，相信大语言模型将在计算机科学之外给STEM研究带来更加广泛的影响。</p><p>&nbsp;</p><p>另一个新兴趋势，则是随着GPU供应不足加之需求旺盛，将有更多企业开发自己的定制化AI芯片。谷歌将加大力度开发TPU硬件，亚马逊推出了Trainium芯片，而AMD可能会逐渐缩小与英伟达之间的差距。现如今，就连微软和OpenAI也在开发自己的定制化AI芯片，唯一的挑战就是各主要深度学习框架能不能为这些新硬件提供全面且有力的支持。</p><p>&nbsp;</p><p>至于开源大模型，其整体水平仍然落后于最先进的闭源模型。目前，最大的开放模型是Falcon 180B。但这应该不是太大的问题，因为多数人根本承受不了如此巨大模型所占用的海量硬件资源。正如前文所提到，我更希望看到由多个小型子模块组成的开源混合专家模型（MoE）。</p><p>我对众包数据集问题也抱持乐观态度，并相信DPO的崛起将给先进开源模型带来新的监督微调选项。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023">https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/C1ucVHUXtSwRNi69o6Ts</id>
            <title>AI巨头混战，微软赢麻了？</title>
            <link>https://www.infoq.cn/article/C1ucVHUXtSwRNi69o6Ts</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/C1ucVHUXtSwRNi69o6Ts</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 06:09:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, 谷歌, AI, 云业务
<br>
<br>
总结: 近日，微软和谷歌发布了季度财报，微软在AI领域超越了谷歌，其第一财季各项业绩超过预期，尤其是AI产品的销售提升对营收和利润增长做出了贡献。相比之下，谷歌的云业务增速降至最低点，投资者更希望谷歌在AI领域取得进展。微软的Azure云计算平台收入增长超过预期，而谷歌的云计算收入增幅放缓。微软将AI融入自家产品组合，而谷歌在AI产品融合方面做得不如微软。微软通过对初创公司OpenAI的投资取得了早期领先地位，而谷歌的业务增长面临压力。微软通过自家Azure云从OpenAI GPT产品组合赚得不少收入，而微软的企业安全信任优势也使客户更倾向于选择微软的生成式AI服务。 </div>
                        <hr>
                    
                    <p></p><blockquote>群雄逐鹿，谁能笑到最后？</blockquote><p></p><p>&nbsp;</p><p>近日，微软和谷歌同时发布了季度财报（2023 年 6-9 月为谷歌三季报和微软 2024 财年第一财季财报）。两大科技巨头同日发布财报，难免被拿来作比较。外媒 The Information 认为，从这两份季度财报来看，微软正在 AI 领域超越其最大的竞争对手——谷歌。</p><p>&nbsp;</p><p>微软方面，第一财季各项业绩全面超过华尔街预期。根据市场咨询服务集团 LSEG 公布的数据，截至 9 月 30 日的当财季，微软公司收入增长 13% 至 565 亿美元，超出分析师们普遍预期的 545.2 亿美元。微软股价在盘后交易中应声上涨 4.2%。Investing.com 高级分析师 Jesse Cohen 表示，“从结果来看，AI 产品正在刺激销售提升，并且已经为营收和利润增长做出了贡献。”</p><p>&nbsp;</p><p>相比之下，谷歌母公司 Alphabet 的云业务增速降至最近 10 个季度以来的最低点，导致该公司股价盘后下跌 5.7%。尽管谷歌的利润和销售额同样超出华尔街预期，但仍无法阻止股价下跌，这意味着投资者更希望该公司能够在 AI 领域取得进展，借此证明其云业务在强大的微软 Azure 和亚马逊云科技面前拥有一战之力。</p><p></p><h2>AI 大战：微软赢了谷歌</h2><p></p><p>&nbsp;</p><p>LSEG 数据显示，坐拥 Azure 云计算平台的微软智能云部门的收入增长至 243 亿美元，高于分析师预计的 234.9 亿美元。Azure 收入增长 29%，高于市场研究公司 Visible Alpha 26.2% 的增幅预期。微软公司并未公布 Azure 的绝对收入数据，而这部分业务正是微软旗下获得 AI 驱动效果最明显的分支。</p><p>&nbsp;</p><p>华尔街正在研究生成式 AI 服务如何为微软带来收益。凭借对初创公司 OpenAI的前瞻性投资，微软已经在市场上建立起早期领先地位。但目前仍有不少 AI 服务尚未广泛落地。微软负责投资者关系的副总裁 Brett Iversen 表示，周二报告的销售额增长，大部分来自客户出于对 AI 方案的期待而重新回归 Auzre 云。</p><p>&nbsp;</p><p>Iverson 在接受路透社采访时指出，“AI 技术的主要作用……包括吸引新客户、拓展现有客户，或者拉拢那些曾经脱离微软的客户重新回归。”</p><p>&nbsp;</p><p>而谷歌母公司 Alphabet 云部门的营收未达预期。</p><p>&nbsp;</p><p>对全球经济放缓的担忧，促使企业客户开始压缩云服务相关支出，其中也包括对昂贵 AI 工具的采用量。这导致谷歌云部门第三季度的收入增幅放缓至 22.5%，低于上个季度的 28%。谷歌云第三季度收入增长 22.5%，达到 84.1 亿美元，增幅回落至 2021 年第一季度以来的最低点。云部门报告的运营收益为 2.66 亿美元，扭转了上年同期 4.4 亿美元的亏损态势。尽管扭亏为盈，但云计算收入未达到华尔街预期的 86.2 亿美元。</p><p>&nbsp;</p><p>谷歌财务主管 Ruth Porat 在周二的电话会议上表示，第三季度的云计算增长主要归功于“客户优化工作”，但并未做出进一步解释。</p><p>&nbsp;</p><p>在 AI 产品融合方面，显然微软做得更好。</p><p>&nbsp;</p><p>微软正着手将 AI 融入自家产品组合，包括每月 30 美元的 Microsoft 365 服务“Copilot”，可用于将当天邮件快速汇总为内容摘要。虽然该工具在下月正式发布前仅向少数试点客户开放，但微软仍要求企业在多次升级系统之后才能使用 Copilot 功能。分析师们认为，这可能意味着 Copilot 还未全面开放，就已经在为微软公司贡献销售额。</p><p>&nbsp;</p><p>投资者还在持续关注微软的大规模数据中心投入，希望了解其花费多少资金来支持 AI 软件。微软本周二表示，其第一财季的资本支出为 112 亿美元，高于上一季度的 107 亿美元，也创下自 2016 财年以来的最高单季支出纪录。</p><p>&nbsp;</p><p>根据 LSEG 的数据，微软 Windows 操作系统及其他相关产品的销售额增长至137亿美元，高于分析师们普遍预期的 128.2 亿美元。此外，包括 LinkedIn 社交网络及办公生产力软件的细分业务增长至 186 亿美元，高于分析师们普遍预期的 182 亿美元。</p><p></p><h2>OpenAI 研发，微软获利</h2><p></p><p>&nbsp;</p><p>此外，The Information 近日的报道指出，微软与开源正两面合围 OpenAI。一边是以企业安全为卖点的微软同质化“倾销”，另一边则是开源阵营迎头赶上，两面夹击又令 OpenAI 倍感压力。</p><p>&nbsp;</p><p>据悉，随着 Salesforce 和 Wix 等大客户转向成本更低的选项，OpenAI 的业务增长正面临压力。买家已经意识到，生成式 AI、特别是 GPT-4，在业务场景下的部署成本相当夸张。如果大型企业想要把这些 AI 工具全部扩展给成千上万的员工，无疑会增添一笔沉重的额外负担。而其他模型提供商及开源大语言模型的涌现，则带来了更为廉价的选择。</p><p>&nbsp;</p><p>Salesforce 公司 AI 高级副总裁 Jayesh Govindarajan 表示，“我们正处于 AI 降本工作的起步阶段。随着这些 AI 产品规模的不断扩大，我们开始专注于实现成本效益，而且对这方面工作的重视程度只会越来越高。”</p><p>&nbsp;</p><p>云原生编排初创公司 Dagster 创始人兼 CEO Pete Hunt 也指出，他最近开始将视频与音频文件摘要服务从 GPT-3.5 迁移到了 Mistral-7B-Instruct 模型。结果就是运营成本从每月约 2000 美元降低到了不足 1000 美元，而且用户并未发现质量有显著降低。</p><p>&nbsp;</p><p>另一边，微软则开始将研究重点放在如何提高 AI 模型效率身上。与此同时，微软似乎也通过自家 Azure 云从 OpenAI GPT 产品组合身上赚得不少收入。其最新一季的云业务数据也支持这个结论，从结果来看，生成式 AI 已经推动 Azure 云业务再次步入增长快车道。</p><p>&nbsp;</p><p>对 OpenAI GPT 模型的独家访问权可能是其中的重要驱动因素，特别是欧洲用户只能通过符合 GDPR 的微软云使用 OpenAI 模型，其中大部分收入也归微软所有。此外，与初创公司相比，微软拥有更强大的企业安全信任优势，因此客户更倾向于通过微软获取生成式 AI 服务。</p><p>&nbsp;</p><p>另外，用户还可以在微软云中获取多种不同模型，例如 Meta 的 Llama 2。即使 Auzre 客户目前只使用 OpenAI，后续也很有可能转向成本更低、或者更加强大的其他模型选项。而如果牢牢绑定 OpenAI，那么选择范围就将仅限于 GPT 模型。</p><p>&nbsp;</p><p>因此，似乎没有什么理由非要跟 OpenAI 直接合作，从微软那边获取 AI 服务反而是更合乎逻辑的方案。OpenAI 公司 CEO&nbsp;Sam Altman&nbsp;最近在谈到与微软的合作时，也表示双方之间确有一些小摩擦。当然，OpenAI 凭借着 ChatGPT 仍在聊天机器人领域占据着绝对的统治地位。但即使如此，目前还难以判断这项业务是否能够盈利。据报道，OpenAI 当前的目标是将年收入提升至 13 亿美元。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.theinformation.com/articles/the-briefing">https://www.theinformation.com/articles/the-briefing</a>"</p><p><a href="https://www.theinformation.com/articles/openais-corporate-sales-come-under-pressure-as-ai-customers-eye-cheaper-options">https://www.theinformation.com/articles/openais-corporate-sales-come-under-pressure-as-ai-customers-eye-cheaper-options</a>"</p><p><a href="https://the-decoder.com/microsoft-and-open-source-give-openai-a-hard-time/">https://the-decoder.com/microsoft-and-open-source-give-openai-a-hard-time/</a>"</p><p><a href="https://globalnews.ca/news/10046550/microsoft-alphabet-sales-google-ai/">https://globalnews.ca/news/10046550/microsoft-alphabet-sales-google-ai/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/wUH5whLMhBYfZXUvfxVf</id>
            <title>建信金科人工智能工程部总经理刘东东确认出席 FCon，分享大语言模型带来金融行业范式转换</title>
            <link>https://www.infoq.cn/article/wUH5whLMhBYfZXUvfxVf</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/wUH5whLMhBYfZXUvfxVf</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 大语言模型带来金融行业范式转换, 建设金融大模型, 刘东东
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，刘东东将发表题为《大语言模型带来金融行业范式转换》的主题分享，介绍了大模型时代下金融范式的转换以及建设金融大模型的实践探索。这次会议将涉及多个主题，包括DevOps在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控和数据要素流通与数据合规等。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。建信金科人工智能工程部总经理刘东东将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5593?utm_source=infoqweb&amp;utm_medium=article">大语言模型带来金融行业范式转换</a>"》主题分享，介绍在大模型时代下，金融范式在何种场景下进行了转换，如何让建设金融大模型，以及如何建设金融领域大模型的实践探索。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5593?utm_source=infoqweb&amp;utm_medium=article">刘东东</a>"，拥有多年的技术架构实践，曾经在华为、百度、好未来等多家互联网公司担任过技术架构师和技术负责人的岗位。目前主要负责建设银行 AI 技术中台与 AI 领域能力开发。他在本次会议的演讲内容如下：</p><p></p><p>演讲：大语言模型带来金融行业范式转换</p><p></p><p>ChatGPT4 的出现，开启了 AGI 帷幕，重新定义了人与机器的关系。被认为是 250 年不遇的第四次工业革命，必将导致行业洗牌。这次工具升级不是短期技术热点，科技部也意识到大模型是和芯片一样的根技术，不仅关乎公司竞争力，也必将影响国家竞争话语权。我将分享如何建设金融领域大模型的实践探索，期待对你有所启发。</p><p></p><p>演讲提纲：</p><p></p><p>GPT4 - AGI 的小火花范式转换 - 人机关系改变金融的范式转换场景如何建设金融大模型</p><p></p><p>你将获得：</p><p></p><p>○ 了解大模型时代，金融范式在何种场景下进行了转换</p><p>○ 了解如何建设金融大模型</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 7 折优惠 ，立省 ￥2040！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tE4JARIoKzaSVav57McG</id>
            <title>大几千的少儿编程课程直接省了！用 Amazon CodeWhisperer 激发孩子的编程兴趣</title>
            <link>https://www.infoq.cn/article/tE4JARIoKzaSVav57McG</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tE4JARIoKzaSVav57McG</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 02:53:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员, 父亲, 少儿编程工具, Visual Studio Code
<br>
<br>
总结: 这篇文章讲述了一个程序员父亲和他儿子学习少儿编程的故事。他们使用了图形化的编程工具Scratch和Kitten，并思考了如何让儿子过渡到更专业的编程工具Visual Studio Code。作者介绍了一个基于人工智能的代码生成器Amazon CodeWhisperer，并将其用于辅导儿子学习编程。CodeWhisperer的安装和配置方便，不需要科学上网。通过与CodeWhisperer的交互式代码生成，孩子可以专注于语法、编程逻辑和API调用，而不需要考虑产品标准和安全风险。 </div>
                        <hr>
                    
                    <p>我是一个程序员，也是一个父亲。工作之余我会经常和儿子聊他们小学信息技术课学习的 Scratch 和 Kitten 这两款图形化的少儿编程工具。</p><p></p><p><img src="https://static001.geekbang.org/infoq/65/65b4e9086b642ee460374078474fef26.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f9/f94bd5d82c81449a3e449a9521c7be28.webp" /></p><p></p><p>我儿子有一次指着书房里显示器上显示的 Visual Studio Code 问我，“为什么我们上课用的开发界面，和爸爸你每天工作用的样子不一样？”</p><p></p><p>所以我也在想一个问题，什么时候可以让我儿子，从 Scratch，Kitten 这种少儿图形化编程工具，过渡到 Visual Studio Code 这种更专业的编程工具去？</p><p></p><p>最近火出圈的 ChatGPT，被很多程序员用来作为自己工作中的代码生成辅助工具。我也在思考如何将 ChatGPT 用到少儿编程领域。由于众所周知的原因，ChatGPT 在国内使用有一定的门槛。但我最近了解到另一款基于人工智能的代码生成器，叫做 <a href="https://www.infoq.cn/article/C6ZjsPGuFWk6LBP7i48E?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">Amazon CodeWhisperer</a>", 我已经将它用来辅导我儿子学习编程了。</p><p></p><p>我和儿子用的开发工具是 Visual Studio Code，我教他在里面写一些最基础的 Python 和 Node.js 代码。</p><p></p><p>Amazon CodeWhisperer, 属于 AWS Toolkit 的一部分，在 Visual Studio Code 打开 Extension Marketplace 面板，通过搜索关键字&nbsp;AWS tool&nbsp;即可安装。</p><p></p><p><img src="https://static001.geekbang.org/infoq/99/99ec8e803a75b2670f28854947435636.webp" /></p><p></p><p>安装完毕后，在 Visual Studio Code 左侧多出一个 Developer Tools 的面板，展开&nbsp;CodeWhisperer&nbsp;下拉列表，点击 Start，然后选择&nbsp;Use a personal email to sign up and sign in with AWS Builder ID&nbsp;即可在 AWS 网站上注册一个帐号并登录：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4bd1bb3794139e7d69a29145794d17b9.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d566be9b326d184299d7346e6461c6f8.webp" /></p><p></p><p>等到我们看到 Developer Tools 面板里，CodeWhisperer 下面显示出&nbsp;Pause Auto-Suggestions&nbsp;显示，说明这个基于 AI 的代码生成器已经成功启用了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c3/c3a545c64d01bdf8751ceb1c596a665c.webp" /></p><p></p><p>下面就是孩子们发挥自己的想象，随意向 CodeWhisperer 发出指令进行编程学习了。</p><p></p><p>为什么 CodeWhisperer 可以用来辅助孩子学习编程？职业程序员都清楚，包括 ChatGPT，CodeWhisperer 这些 AI 工具，生成的代码仅仅用来作为参考，绝对不能直接用在生产系统里，因此这些 AI 自动生成的代码，可能存在安全风险，缺乏足够的出错处理等等。而相对来说，孩子通过工具自学编程，不需要考虑这些软件产品开发需要注意的产品标准，只需要把注意力放在工具生成代码的语法，编程逻辑和 API 的调用上就行了。</p><p></p><p>另一方面，CodeWhisperer 的安装和配置非常方便，也不需要像 ChatGPT 那样科学上网。</p><p>下面是一些具体的使用例子。</p><p></p><p>假设小朋友想用 Python 编程，打印出当前目录下所有文件的列表。</p><p></p><p>在 Visual Studio Code 里新建一个 1.py 文件，然后录入如下注释，可以类比成是 ChatGPT 里的 Prompt：</p><p></p><h2>list all files in current folder</h2><p></p><p></p><p>我孩子的英语学习我是全程陪伴的，我觉得现在少儿英语的教育比我上学的时候卷多了。现在小学五年级就已经开始学很多我以前初中才学到的复杂语法，什么定语从句，被动语态，各种完成时等等。这种内卷倒也有一个好处：小朋友用英语编写简单的 Prompt 没有什么障碍：这些 Prompt 都是简单的命令式短句，无非是动词+名词即谓语+宾语的搭配结构。</p><p></p><p>我们在 1.py 里输入&nbsp;#&nbsp;开头的 Prompt，回车之后，稍等片刻，CodeWhisperer 就会以灰色的字体颜色，显示出完成这个 Prompt 所需的第一行代码：import os</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8e7f4e1a539a021f80d2cfdee2d553d3.webp" /></p><p></p><p>如果我们觉得这行代码能够接受，敲击键盘 Tab 键，就能将其正式书写在 1.py 文件里。</p><p></p><p><img src="https://static001.geekbang.org/infoq/06/06e488de89fd7bfec1dcf04c835b7189.webp" /></p><p></p><p>这种一行一行交互式的代码生成方式，适合家长和小朋友坐在一起，出来一行，给孩子讲解一行，也就是逐行理解。</p><p></p><p>我们在&nbsp;import os&nbsp;之后点击回车，稍后片刻，会看到 CodeWhisperer 给我们生成的下一行代码：for file in os.listdir("."):</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c747b20ac765fe7124adb1404428b6b.webp" /></p><p></p><p>同理，点击 tab 按钮之后，for file in os.listdir(".")&nbsp;这行代码也被我们选定。继续按回车，就会出现下一行代码提示：print(file)</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/103c52044b185946f531b1df137e6398.webp" /></p><p></p><p>就这样，三次回车和三次 Tab 键，就完成了这个需求的编码工作。直接使用 python 命令执行这个编辑好的文件，能得到期望的正确输出。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c2/c2b1f77b521b4297a07b112c6a37576d.webp" /></p><p></p><p>我又继续做了测试，针对 Node.js，使用同样的 Prompt，也能得到令人满意的代码和执行结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd13d3cdb36be8fe1ee2dbb0afd23bd4.webp" /></p><p></p><p></p><h2>总结</h2><p></p><p>Amazon CodeWhisperer 作为一款免费使用的基于 AI 的代码生成工具，不仅能够帮助专业的程序员减少机械的代码编写时间，同时也能作为少儿图形化编程的一个补充，给那些学有余力又对编程感兴趣的小朋友，打开一扇新的通往编程世界的大门。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/w3xGZUPWdyM0iZ1IRUvc</id>
            <title>腾讯混元大模型升级：正式开放“文生图”功能，代码能力大幅提升20%</title>
            <link>https://www.infoq.cn/article/w3xGZUPWdyM0iZ1IRUvc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/w3xGZUPWdyM0iZ1IRUvc</guid>
            <pubDate></pubDate>
            <updated>Fri, 27 Oct 2023 02:28:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 腾讯混元大模型, GPT3.5, 文生图, 代码处理效果
<br>
<br>
总结: 腾讯宣布腾讯混元大模型升级，超过GPT3.5，提升20%。同时开放了文生图功能。腾讯混元通过优化泛化能力、逻辑推理能力和指令跟随能力，达到国内第一梯队水平。在代码处理方面，腾讯混元的效果胜出ChatGPT 6.34%。腾讯混元已被多个业务接入，可用于代码生成、数据处理等工作。文生图功能提升了细节效果30%。腾讯混元的能力已被广泛应用于素材创作、游戏出图等领域。腾讯混元的升级离不开腾讯自研的机器学习平台Angel的支持。 </div>
                        <hr>
                    
                    <p>10月26日，腾讯宣布，腾讯混元大模型迎来全新升级，升级后的腾讯混元中文能力整体超过GPT3.5，代能力大幅提升20%，达到业界领先水平。同时，腾讯混元大模型正式对外开放“文生图”功能。</p><p></p><h2>再升级：代码处理效果胜出ChatGPT 6.34%</h2><p></p><p>&nbsp;</p><p>据混元大模型首次亮相已经过去一个多月。现在除了千亿参数规模大主模型之外，腾讯自研的面向垂直领域的7B和13B模型也首次亮相，这些都是基于Angel机器学习平台打造，业务形态形式也是通过API接入。</p><p>&nbsp;</p><p>目前，千亿模型训练数超过了2.5T，支持多语言高压缩比Tokemizer，单token信息量更大。对于中小Size模型，混元系列与其他开源模型类似：具备多强言能力；同等效果下仅需较少的tokans，并兼顾通用和行业等业参场景。</p><p>&nbsp;</p><p>对于大模型来说，指令遵循上有几个挑战：一是模型泛化能力差，需要解决多任务训练过程的抗干扰问题；二是多轮对话指令记忆弱，需要解决长记忆问题；三是逻辑思维差，代码能力的背后就是逻辑推理；四是优质指令获取非常难，人工标注质量不稳定且周期长，需要人机配合的方式获得更大量的优质数据，开源数据有限，质量参差不齐。</p><p>&nbsp;</p><p>对此，混元LLM-SFT技术主要采取了动态锯齿注意力机制提升泛化能力与对话上文抗干扰能力，使用渐进思维链激发模型逐渐思考的逻辑推理能力，使用Ghost Attention增强模型在多轮对话下的指令跟随能力，并做了复杂指令自动进化。腾讯方面表示，通过这些优化之后，混元综合测评达到国内第一梯队，中文指令下超过GPT-3.5。</p><p>&nbsp;</p><p>腾讯机器学习平台算法负责人康战辉重点介绍了混元大模型代码方面的能力。代码技术主要是两个方向进行了优化：一是代码预训练，二是&nbsp;SFT指令微调。腾讯表示，经过对32种主流语言代码文件、各类计算机书籍和博客的学习增训，腾讯混元代码处理水平提升超过20%，代码处理效果胜出ChatGPT 6.34%，在HumanEval公开测试集指标上全面超过Starcoder、Codellama等业界头部开源代码大模型。具体效果如下：</p><p></p><p></p><p></p><p>腾讯内部目前已经有多个开发平台接入了腾讯混元大模型，工程师们可以使用腾讯混元来进行代码生成、代码补全、代码漏洞检测和修复、表格数据处理、数据库查询等工作。</p><p>&nbsp;</p><p>据悉，目前超过180个腾讯内部业务已接入腾讯混元，包括腾讯会议、腾讯文档、企业微信、腾讯广告和微信搜一搜等。另外，已有来自零售、教育、金融、医疗、传媒、交通、政务等多个行业的客户，通过腾讯云调用腾讯混元大模型API，应用领域涉及智能问答、内容创作、数据分析、代码助手等多个场景。</p><p>&nbsp;</p><p>今年9月首批通过备案后，腾讯混元大模型也已经面向C端用户陆续开放体验，用户通过小程序或网页端，就能与腾讯混元对话。</p><p></p><h2>开放文生图功能，发丝、皱纹等细节效果提升30%</h2><p></p><p>&nbsp;</p><p>文生图是AIGC领域的核心技术之一，也是体现通用大模型能力的试金石，对模型算法、训练平台、算力设施都有较高的要求。混元文生图模型主要围绕着算法模型、数据系统和工程平台三个方面演进。</p><p>&nbsp;</p><p>大模型文生图的难点体现在对提示词的语义理解、生成内容的合理性以及生成图片的效果。针对这三个技术难点，腾讯提出了一系列原创算法，来保证生成图片的可用性和画质。</p><p>&nbsp;</p><p>在语义理解方面，腾讯混元采用了中英文双语细粒度的模型。模型同时建模中英文实现双语理解，并通过优化算法提升了模型对细节的感知能力与生成效果，有效避免多文化差异下的理解错误。</p><p>&nbsp;</p><p>在内容合理性方面，AI生成人体结构和手部经常容易变形。混元文生图通过增强算法模型的图像二维空间位置感知能力，并将人体骨架和人手结构等先验信息引入到生成过程中，让生成的图像结构更合理，减少错误率。</p><p>&nbsp;</p><p>在画面质感方面，混元文生图基于多模型融合的方法，提升生成质感。经过模型算法的优化之后，混元文生图的人像模型，包含发丝、皱纹等细节的效果提升了30%；场景模型，包含草木、波纹等细节的效果提升了25%。</p><p>&nbsp;</p><p>例如，输入提示词“生成可爱的亚洲 4 岁女孩穿着棉质连衣裙，大眼睛，古代中国，摄影风格，汉服”，腾讯混元大模型生成如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ba/bafbb6462bd24649891aed1ec8d931cb.png" /></p><p>&nbsp;</p><p>目前，腾讯混元文生图能力已经被用于素材创作、商品合成、游戏出图等多项业务中，此外在广告业务下的多轮测评中，腾讯混元文生图的案例优秀率和广告主采纳率分别达到86%和26%，均高于同类模型。</p><p>&nbsp;</p><p>据了解，腾讯混元大模型持续升级背后，离不开腾讯自研一站式机器学习平台Angel的支撑。自研AngelPTM训练框架可提供高效的分布式训练解决方案，具备业界领先的内存利用率和训练吞吐效率，训练速度相比业界主流框架提升1 倍；自研AngelHCF训练框架，具备从蒸馏、微调、压缩到模型加速的完整能力，支持多种模型并行，保证模型的最小化部署及最大化吞吐，推理速度相比业界主流框架FasterTransformer快1.3倍。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/eEWlyix6CYX3FOZdJFus</id>
            <title>估值最高25亿美元，这家芯片设计巨头公司裁员20%，工程师成重灾区</title>
            <link>https://www.infoq.cn/article/eEWlyix6CYX3FOZdJFus</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/eEWlyix6CYX3FOZdJFus</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Oct 2023 05:58:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 芯片设计初创公司, 裁员, RISC-V, SiFive
<br>
<br>
总结: 加利福尼亚州的芯片设计初创公司SiFive裁员20%，约130人。SiFive使用开源芯片架构RISC-V进行芯片设计，并与ARM公司竞争。RISC-V是一种计算核心架构的免版税替代方案。SiFive是RISC-V领域的领军企业之一。裁员可能会影响SiFive的产品组合和对RISC-V标准的贡献力量。 </div>
                        <hr>
                    
                    <p>当地时间10月24日，据路透社报道，芯片设计初创公司 SiFive 周二表示，该公司已裁掉了20%的员工，即约 130 人。</p><p>&nbsp;</p><p>SiFive 总部位于加利福尼亚州圣克拉拉，基于称为RISC-V的开源芯片架构构建芯片设计，并与最近上市的ARM公司开展竞争。与ARM一样，SiFive主要是构建芯片的底层设计，但不构建芯片本身。</p><p></p><h2>RISC-V公司裁员20%，工程师、高管涉及在内</h2><p></p><p>&nbsp;</p><p>众所周知，RISC-V是一种计算核心架构的免版税替代方案。其基本理念希望任何人都机心选用这种架构，以此构建自己的计算核心、向其中添加任意自定义功能再进行生产制造，无需支付任何版税。但问题在于，构建计算核心或者SoC并不是那么简单，其中涉及多种技术和多个具体步骤，单有一套架构只能算是个良好的开端。正因为如此，RISC-V先是在计算核心的实验设计领域广受接纳，之后才逐步进入实际芯片市场。就算不考虑到这些现实难题，如果大家需要的是一款高性能、高能效或者二者兼备的优质芯片，也仍然需要先组织一支由专业核心和SoC设计师组成的团队。</p><p>&nbsp;</p><p>RISC-V在低端核心领域已经取得重大进展，主要集中在微控制器方面。目前，西部数据等公司已经公开表示，他们正在用RISC-V核心实现其存储设备中的微控制器等功能需求。高通和谷歌等科技巨头已经采用了底层 RISC-V 技术。</p><p>&nbsp;</p><p>最近几年，RISC-V在亚洲这片东方大地上逐渐站稳了脚跟。部分公司开始研究高性能RISC-V核心，例如Tenstorrent和Ventana Micro。而不少加入RISC-V阵营的公司都是RISC-V国际联盟的成员，该联盟原则上管理着整个生态系统，各成员分别负责性能、安全性以及架构/微架构中的各个关键部分。联盟还提供底层支持以营造公平的竞争环境，例如相当于Arm SBSA L3的服务器最低规格标准。</p><p>&nbsp;</p><p>其中不少公司从事RISC-V架构的Core IP业务，而SiFive长期以来一直是该领域中的领军企业。截至F轮融资，SiFive已经获得3.65亿美元风险投资，并在过去12月时间里先后出售价值约2.2亿美元的部分资产。</p><p>&nbsp;</p><p>SiFive 发言人 David Miller表示，SiFive 没有改变其长期计划，并将继续生产用于人工智能、汽车、消费电子产品和低功耗设备的芯片。</p><p>&nbsp;</p><p>Miller还表示，裁员涉及公司所有部门，包括高管。公司的产品线仍在继续运行。</p><p>&nbsp;</p><p>2022 年，该公司的估值<a href="https://www.reuters.com/technology/risc-v-chip-technology-firm-sifive-raises-175-mln-valued-25-bln-2022-03-16/">约为 25 亿美元</a>"，Miller表示，该公司“未来几年资金充足”。</p><p>&nbsp;</p><p></p><h2>SiFive裁员会带来哪些影响？</h2><p></p><p>&nbsp;</p><p>事实上，该公司的运营态势也一直良好——SiFive拥有一系列核心设计，可将其RISC-V架构对外授权，包括微控制器、标量核心到向量核心等。SiFive相当于在遵循Arm核心授权模式的同时，保留了RISC-V架构的开放性和可定制性。</p><p>&nbsp;</p><p>而此次闹出的大新闻，就是整合多方来源的消息，SiFive刚刚进行了大幅裁员。尽管给出的数字各有不同，从100到300多名不等，但知情人士普遍证实，此次裁员主要集中在工程团队，特别是实体设计工程师、销售和产品团队。据称高管团队也有变动，目前SiFive公司转由创始人兼CEO Patrick Little掌舵。突如其来的动作，确实令人始料未及。</p><p>&nbsp;</p><p>而这还没完，SiFive的产品组合也将被削弱。该公司的业务主要包含两大类别：其一就是供客户选择的预设计核心，其中大部分已经完成了流片验证，SiFive公司甚至上个礼拜琮刚刚公布了两款新核心；另一半业务则是提供定制核心——即客户向工程团队提出具体要求，再由SiFive具体完成设计并将成果安装进完整的SoC中。消息人士们指出，后续SiFive将直接放弃预设计核心这部分业务，似乎会将全部精力投入到核心定制当中。</p><p>&nbsp;</p><p>为了证实消息的可靠性，一家境外媒体联系了SiFive公司公关部门以了解详细信息，对方则发来一份官方声明：</p><p>&nbsp;</p><p></p><blockquote>“为了适应快速变化的半导体终端市场，SiFive正对我们下辖的各团队及区域业务进行重新调整，以便更好地把握未来机遇、降低运营复杂性并提高我们快速就客户提出的产品需求做出响应的能力。遗憾的是，上周确有部分职位被裁撤，相关员工将获得遣散费和再就业援助。但SiFive仍对自身业务以及RISC-V的发展势头和长期前景抱有信心。”</blockquote><p></p><p>&nbsp;</p><p>考虑到SiFive在RISC-V技术行业的领军地位，此番变动无疑会带来不小的打击。从目前情况看，SiFive应该会继续存在，但不清楚如果业务只剩下定制核心这一枝，创始人还能在公司待多久。也许SiFive向来是以定制核心为主体，所以抓大放小未必有多严重，但考虑到曝出来的裁员数字，后续SiFive对于RISC-V标准的贡献力量恐怕将大打折扣。而另一个重要问题，则是这对投资者们意味着什么、他们会提出什么问题、SiFive方面又将作何答复。</p><p>&nbsp;</p><p>有业内人士分析认为，SiFive的一位投资者可能打算迅速、甚至在不作通知的情况下撤资5000万到1亿美元。SiFive刚刚完成了F轮融资，所以有投资者认为当初的投入没能带来足够的回报也在情理之中。如果说真的有人决定迅速抽离这么大一笔资金，那么即刻裁员加产品转向也属合理反应。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://morethanmoore.substack.com/p/the-risk-of-risc-v-whats-going-on">https://morethanmoore.substack.com/p/the-risk-of-risc-v-whats-going-on</a>"</p><p><a href="https://www.reuters.com/technology/open-source-arm-competitor-sifive-lays-off-20-staff-2023-10-24/">https://www.reuters.com/technology/open-source-arm-competitor-sifive-lays-off-20-staff-2023-10-24/</a>"</p><p><a href="https://www.tomshardware.com/news/sifive-lays-off-hundreds-of-risc-v-developers">https://www.tomshardware.com/news/sifive-lays-off-hundreds-of-risc-v-developers</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DFcu8koAm6NKjgGBNKhN</id>
            <title>群雄逐「路」，自动驾驶黑客松 Coding for Running S2 圆满落幕！</title>
            <link>https://www.infoq.cn/article/DFcu8koAm6NKjgGBNKhN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DFcu8koAm6NKjgGBNKhN</guid>
            <pubDate></pubDate>
            <updated>Thu, 26 Oct 2023 04:10:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 滴滴自动驾驶, 自动驾驶黑客松, 技术与合作, 极限试炼
<br>
<br>
总结: 近日，滴滴自动驾驶等多家公司联合主办了一场自动驾驶黑客松比赛。参赛选手通过36小时的比拼，从硬件到软件，从竞速到闯关，展现了自动驾驶领域的技术与合作的价值共鸣。比赛中，选手们不仅体验了自动驾驶技术的全面认知，还提升了整体性思维和解决问题的能力。比赛中的极限试炼和巅峰对决，展现了自动驾驶领域的热爱和创造力。 </div>
                        <hr>
                    
                    <p></p><p>近日，由滴滴自动驾驶、地平线、北醒、滴滴 - 清华大学创新出行联合研究中心联合主办，InfoQ 承办的“Coding For Running - 自动驾驶黑客松行业邀请赛”在北京圆满落幕。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f379daf7b2c3f7fe02d13609ecc349bf.png" /></p><p></p><p>本次大赛全新升级为自动驾驶行业邀请赛，来自滴滴自动驾驶、KargoBot、地平线、北醒的 13 支战队，以代码为剑、以赛道为台，在 36 小时里经历了从硬件到软件、从竞速到闯关的多维体验与碰撞，并最终实现了技术与志趣、竞争与合作的价值共鸣，也展现出了自动驾驶最前沿的极客风貌。</p><p></p><p>滴滴自动驾驶 COO 孟醒在开场致辞中表示：“记得创业之初，我们没有钱和设备，经常用二手零件自己组装编程做小 demo，虽然出来的东西比较简陋但是很开心，这种成就感和正反馈源源不断，这也是我们组织 Hackathon 的初心，希望公司的同学们有机会从零开始体验软硬件并施的小项目，在这个过程中找到技术的乐趣，并拥有持续的热爱。”</p><p></p><p>地平线智能驾驶产品规划与市场总经理吕鹏在致辞中表示：“在活动中看到了非常多热情澎湃的时刻，从黑客松比赛中也可以看出，自动驾驶是很复杂的系统性工程，这次比赛设计了各种挑战性场景，而真实世界中是无穷极的场景，需要拥有无穷热情的工程师们一起投入，地平线期待与行业伙伴一起见证未来！”</p><p></p><p>北醒创始人、CEO 李远在致辞中表示：“这场热血的自动驾驶黑客松比赛，聚集了来自滴滴自动驾驶、地平线、北醒、清华、infoQ 等自动驾驶领域优秀的技术团队。大家各尽所长，让一场行业小 party，成就为一次团队大奇迹。诚挚邀请所有自动驾驶上下游的同行们明年一起参加！”</p><p></p><p></p><p></p><p></p><p></p><h3>热爱如炬、不舍昼夜，28 小时的极限试炼</h3><p></p><p>对于参赛的诸多选手而言，除了 Hackathon 的比赛形式所带来的刺激感，从 0 到 1 让小车跑起来则更是令人难忘。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/eb1c65d0c00b8624a550ca73a518d498.png" /></p><p></p><p>不少选手表示平时工作比较忙，没有太多时间参与各种赛事活动，这次有机会通过企业活动的方式参与这次大赛是非常酷的一件事。也有选手表示日常工作中大家都在专注于各自的岗位的事情，很少像这样需要综合各方面能力去进行小车硬件 DIY 以及写出完整的代码实现车辆的控制和感知的，任何一环出现问题都可能导致最终的失败。虽然比赛的还原程度远远比不上真正的自动驾驶汽车，但是背后的逻辑其实是相通的，都需要以系统思维去看待产品研发。</p><p></p><p>来自滴滴自动驾驶的参赛选手提到：“通过这样的比赛，我对于自动驾驶技术领域有了更加全面且深刻的认知，同时也大大提升了自身的整体性思维以及突发情况下解决问题的思路，这个收获是远超比赛成绩的！”</p><p></p><p>如果热爱可以被衡量，那么最直接的方式就是时间投入。众所周知，黑客松历来被称为“熬鹰大赛”，参与者在此期间需要不间断地工作，以完成各种技术项目或挑战，对于任何参赛选手而言，都是身体和精神的双重挑战。在去年的黑客松现场，我们做了一个调研，不少选手表示，平时项目攻坚可能都没有这样一次黑客松比赛累，但是也正是这种痛并快乐着的方式，能够大大激发大家的创造力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d7f8270bc95b0f5ca2bd090526d6e57.png" /></p><p></p><p>今年的选手们同样热情不减，凌晨四点，依旧有一大群人在赛道或者工作台上紧张地做着各种测试。有选手表示：“凌晨的时候其实有点困了，但是我看到有很队伍都在做循迹竞速测试，而且他们的车都跑得非常快，有被卷到，所以一下子就清醒了很多，然后很快也就加入到了战斗中去，大家其实都玩得挺开心的！”</p><p></p><p>当然，比赛固然激烈，健康更加重要。今年的黑客松，在封闭测试期间特别增加了 4 小时用于选手休息充能，有参加过两届黑客松的选手表示，今年整体而言在身体上是比去年轻松很多的，组内也基本都有做轮换休息，虽然时间紧任务重，但是最终也基本做出了一个比较完整的作品。</p><p></p><p>在现场的采访中，有选手提到：“印象最深的就是我们小组有做轮换休息的安排，我去睡觉前跟队友说当他们调试到红绿灯的时候就叫我起床，然后大概凌晨三点多的时候，我电话就响了，当时确实还想多睡一会，但是当我回到比赛场地，看到大家依然热乎朝天地做着各种测试，尤其是队友们聚精会神在赛道上测试红绿灯的时候，突然也就不困了，一是非常震撼于大家对于技术的热爱，另外也是被队员们感动到了！”</p><p></p><p>28 小时通宵达旦的极限试炼，除了胜负欲，现场迸发出来更多的是热爱的力量，胜利很重要，但热爱更酷。</p><p></p><p></p><h3>代码为剑、赛道称王，4 小时的巅峰对决</h3><p></p><p>在经历了 28 小时的体力、智力的比拼后，大赛的重头戏才刚刚开始。</p><p></p><p>本次大赛共分初赛和决赛两个阶段：初赛即循迹竞速，参赛队伍需要控制自动驾驶 DonkeyCar 在空白赛道上进行跑圈，速度越快排名越高；决赛即场景闯关，需要完整地考核硬件组装和赛道闯关情况，其中赛道关卡包含鬼探头、红绿灯、施工占道避障、隧道、划线停车五大场景。</p><p></p><p>循迹是自动驾驶系统的核心组成部分，能够确保车辆在道路上安全、精确地导航、遵守交通法规、提供舒适的乘坐体验，从某种意义上来说，循迹是实现更高阶自动驾驶功能的前提。</p><p></p><p>当然，自动驾驶不仅仅是简单的循迹，在实际驾驶过程中，自动驾驶系统将面临各种各样的问题，这也是该领域的最大挑战之一。为了让参赛选手们能够体验更加纯粹、真实的自动驾驶挑战赛，决赛在去年的小车 DIY、循迹、施工占道、过弯、红绿灯等基础上，增加了隧道通行、鬼探头、划线停车等场景，力求模拟出更加真实、丰富的自动驾驶场景，挑战更高阶的编码乐趣。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e16476e763030934eed06980cfdcba8e.png" /></p><p></p><p>来自滴滴自动驾驶的参赛选手表示：“这次比赛最有趣的地方在于它并不是一个简单的循迹或者竞速比拼，看似简单的几个关卡，实际上对于软件算法提出了更高的要求。比如这次比赛中的鬼探头、红绿灯、隧道、施工占道等场景，其实更接近于我们实际生活中的驾驶场景，所以对于我们做自动驾驶的同学来说，非常具有实践意义。”</p><p></p><p>初赛与决赛现场，参赛队伍们八仙过海，各显神通，利用不同的策略和技术路径实现竞速和闯关，为大家带来了一场紧张刺激同时又妙趣横生的赛事。</p><p></p><p>对于初赛，不少选手表示 DNA 动了，他们纷纷表示：“初赛给我的感觉就像小时候看的动画片《四驱兄弟》里面的场景一样，当然去年我们这个赛道好像就叫「四驱兄弟」，大家在测试环节比拼速度都很兴奋！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2be3f45633e35363ff6d3a0f17ba4b72.png" /></p><p></p><p>对于决赛，今年给到了选手们更大的挑战，比如最后一关从去年的倒车入库变成划线停车，对于车辆的感知和控制要求更高。来自地平线的圣克拉拉队表示：“我们觉得决赛最大的难点就是划线停车，要实现这个目标，需要设计非常多的步骤，首先需要小车准确寻找到停车的位置，然后还要精准地控制停车的姿态和角度，并且不能压线以及需要稳稳地停下来，这对小车的感知算法以及控制算法都是一个不小的挑战。”</p><p></p><p>作为本次比赛的承办方，InfoQ 参与了全流程的策划和落地，也以裁判组的身份参与了 PK 赛的全流程。对于本次大赛，InfoQ 主编赵钰莹表示：“InfoQ 每年都有做很多的黑客松赛事，但是像这一次的自动驾驶方向的赛道比较少，尤其是这一次比赛需要选手从硬件组装开始，到软件代码，再到模拟场景下的赛道挑战，趣味性和难度都增加了不少，我们看到有非常多的软件同学在死磕硬件，硬件的同学在学习软件，这是一个非常好的机会，让大家能够打破边界、共同成长，同时也能让大家再一次思考技术人的初心和梦想，我相信这次大赛对于所有参赛者来说，都是一次难能可贵的人生经历。”</p><p></p><p></p><h3>江湖路远、义字当先，32 小时的协作共赢</h3><p></p><p></p><p>除了竞争与碰撞带来的心潮澎湃，现场选手们提到最多的就是“互相学习、合作愉快、相见恨晚”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/86/86935a02e8fcefee62808934728d826f.png" /></p><p></p><p>并肩作战往往更能建立深厚的友谊。有参赛选手表示：“我们几个都是同一个公司的，平时我们可能也就私下只是吃吃饭、聊聊天，没有像现在这样，在非常紧张的环境中去做这么多的事情。也正是通过这次比赛，我们彼此之间的了解更深了，也能看到大家身上更多的闪光点，并且去互相学习。”</p><p></p><p>除了公司内部跨部门的友谊外，事实上，在比赛过程中，有非常多的跨司、跨组的互帮互助，有竞争同样也有合作。来自地平线的圣克拉拉战队表示：“这次比赛我们隔壁桌是滴滴的小伙伴，虽然有比赛的竞争关系，但是事实上大家都非常热情，比如过程中有一个速度控制的问题，他们会跟我们一起沟通交流，并且思考解决方案，大家相处的非常愉快，非常期待后面有机会继续合作。”来自北醒的参赛团队也补充道：“即使是全封闭的比赛环境，现场的氛围也十分的友好，大家互相交流互助。在我们发现问题并寻求解决的过程中，隔壁的滴滴小伙伴主动与我们沟通和交换意见，帮助我们顺利解决了问题。”</p><p></p><p>本次比赛还有一只特殊的跨司队伍——掘金队，队员分别来自滴滴自动驾驶和地平线。队长表示：“首先因为是跨司组队，其实每个公司的工作风格都不一样，很容易感受到，但是我们一直在互相鼓励，尽量让每个都发挥出自己的优势，大家很快就混熟了，而且相处得非常愉快。”</p><p></p><p>此次赛事使用的 RDK X3 ROBOT 是一款地平线生态合作伙伴亚博智能基于 RDK X3 打造的可全向移动 ROS 机器人。比赛将地平线开发套件 RDK、机器人应用中心 Nodehub 等软硬件产品技术深入融合到赛事中。从软硬件技术到赛题设计方案，全程为选手提供技术支持的地平线资深工程师王恺睿表示：“我印象最深的时刻，也是最安静最美的时刻，是封闭式开发中凌晨两三点，很多参赛选手没有睡，仍然精力充沛的在调试机器，这就是开发过程带来的喜悦与精力。”</p><p></p><p>“友谊第一，比赛第二”是本次大赛的真实写照，参赛选手们因为技术热爱而同台竞技，因为共同目标而并肩作战，也因为志趣相投而建立友谊。</p><p></p><p>当然，不仅仅是一次自动驾驶黑客松赛事，对于自动驾驶领域而言，同样需要更多像滴滴自动驾驶、地平线、北醒这样志同道合的企业能够携手并肩，加速推动技术的创新与发展，共同筑就自动驾驶的未来远景。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NZ7K1Fj8187zOYz47gZL</id>
            <title>解读 Gartner 2024 年十大战略技术趋势</title>
            <link>https://www.infoq.cn/article/NZ7K1Fj8187zOYz47gZL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NZ7K1Fj8187zOYz47gZL</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 09:58:03 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Gartner, 技术趋势, AI信任、风险和安全管理, 持续威胁暴露管理, 可持续技术
<br>
<br>
总结: Gartner发布了2024年企业机构需要探索的十大战略技术趋势，其中包括AI信任、风险和安全管理、持续威胁暴露管理、可持续技术等。AI信任、风险和安全管理是企业面临的重要问题，需要采取措施保护AI模型的安全性。持续威胁暴露管理是一套对安全态势修复和改进的框架，能够评估企业数字与物理资产的可访问性和暴露情况。可持续技术是一个数字解决方案框架，旨在实现环境、社会和治理的可持续发展。平台工程是为企业的软件开发团队提供自助开发门户或内部开发平台的一种方法。 </div>
                        <hr>
                    
                    <p>前不久，Gartner发布了2024年企业机构需要探索的十大战略技术趋势。Gartner研究副总裁Bart Willemsen表示：“由于技术变革以及社会经济方面的不确定性，我们必须大胆采取行动并从战略上提高弹性，而不是采取临时措施。IT领导者的地位特殊，他们可以制定通过技术投资帮助企业在这些不确定性和压力下保持成功的战略规划。”本文，InfoQ 试图通过 Gartner研究副总裁高挺的分享为大家解读这十大技术趋势的具体含义。</p><p></p><p>需要特别注意，本文提及的“趋势”很多时候不是单独的技术，而是一种架构或者说新的方向，因为单独的技术发展没有那么快。今年，Gartner的整体趋势分为三大主题：保护你的投资、开发者的崛起、交付价值。</p><p><img src="https://static001.geekbang.org/infoq/4f/4f1861ef2298e35b20196279fcf10644.png" /></p><p></p><h3>一、保护你的投资</h3><p></p><p></p><h4>1.AI信任、风险和安全管理（AI Trust, Risk and Security Management）</h4><p></p><p>AI信任、风险和安全管理是第二年入选。AI的全民化使得企业对AI信任、风险和安全管理（TRiSM）的需求变得更加迫切和明确。在没有护栏的情况下，AI模型可能会迅速产生脱离控制的多重负面效应，抵消AI所带来的一切正面绩效和社会收益。</p><p></p><p>企业在使用AI模型的整个生命周期可能都面临安全风险，比如训练阶段可能出现“数据投毒”，应用阶段可能出现“提示词攻击”，从这些方面来讲，AI有很多风险敞口。基于这样的现实情况，Gartner提出了“AI TRiSM”框架。AI TRiSM由六大模块组成，分别是：内容异常检测、数据保护、AI应用安全、可解释性、透明度以及“ModelOps”。</p><p></p><p>Gartner预测，到2026年，采用AI TRiSM控制措施的企业将通过筛除多达80%的错误和非法信息来提高决策的准确性。</p><p></p><h4>2.持续威胁暴露管理（Continuous Threat Exposure Management）</h4><p></p><p></p><p>持续威胁暴露管理（CTEM）是Gartner提出的一套对于安全态势修复和改进的框架，使企业机构能够持续而统一地评估企业数字与物理资产可访问性、暴露情况和可利用性的务实系统性方法。根据威胁载体或业务项目（而非基础设施组件）调整CTEM评估和修复范围不仅能发现漏洞，还能发现无法修补的威胁。CTEM由五大模块组成，分别是Scoping、Discovery、prioritization、Validation、Mobilizatlon。每一个模块都有自己的一套方法、工具和相关实践。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c7/c757dee8aa431c1f0c3d46c7f9ccfbc5.png" /></p><p></p><p>与传统的安全技术相比，CTEM有如下区别：</p><p></p><p>1.CTEM不是单纯关注漏洞本身，而是更加关注企业业务层面的风险暴露面，这不仅包括网络安全上面的漏洞，也包括一些传统设别、应用程序、社交媒体账户等。总之，整套框架的审查范围更加广泛。</p><p></p><p>2.CTEM不是简单对风险进行“高、中、低”的分级。当然，这是比较传统的风险分级方式。但实际上，低风险的漏洞也需要被慎重考虑，是否存在以后被利用的可能或者对业务的影响程度。这套框架会从“企业业务视角”出发综合判断风险。</p><p></p><p>3.对于风险控制的措施不可能完全自动化。比较现实的做法是接受与风险共存、同时提高业务韧性。传统的对于风险漏洞管理的方法是一旦系统出现问题，很可能被防病毒系统直接干掉，人力不参与其中。但是，风险无处不在，很多风险的修复需要花费大量人力、物力和时间，是否修复以及如何修复都需要结合给业务造成的损失来具体判断。</p><p></p><p>Gartner 预测，到 2026 年，根据 CTEM 计划确定安全投资优先级别的企业机构将减少三分之二的漏洞。</p><p></p><h4>3.可持续技术（Sustainable Technology）</h4><p></p><p></p><p>今年，AI技术的火爆让我们进入“暴力计算”时代。企业和开发者在显卡、芯片层面花费了大量金钱来训练大模型，这带来了大量的碳排放和电力消耗，很多国家的部分数据中心已经开始出现电力紧缺的情况，同时也对企业的IT运维提出了挑战。</p><p></p><p>可持续技术是一个数字解决方案框架，其用途是实现能够支持长期生态平衡与人权的环境、社会和治理（ESG）成果。AI、加密货币、物联网、云计算等技术的使用正在引发人们对相关能源消耗与环境影响的关注。因此，提高使用IT时的效率、循环性与可持续性变得更加重要。</p><p></p><p>那么，技术管理者们具体可以做哪些事情呢？</p><p></p><p>1.Sustainability of IT（IT部门的可持续发展）</p><p></p><p>企业IT部门需要提高资源的利用率，比如原来的电脑三年换一次，现在可能四年换一次。或者开发某个网页时，某些资源的访问可能会导致碳排放增加。如今，欧盟要开始征收“碳税”，很多供应链层面的事情也需要考虑。</p><p><img src="https://static001.geekbang.org/infoq/42/424ef26551de7d2e195252b595ef63a1.png" /></p><p></p><p>2.Sustainability with IT（IT对可持续赋能）</p><p></p><p>企业需要满足碳排放层面的一些要求，首先需要收集相关数据，比如企业排放了多少碳或者用了多少电，这些数据收集、管理和分析都是IT部门可以赋能的地方。</p><p>事实上，Gartner 预测，到 2027 年，25% CIO的个人薪酬将与其在可持续技术层面的贡献挂钩。</p><p></p><h3>二、开发者的崛起</h3><p></p><p></p><h4>1.平台工程（Platform Engineering）</h4><p></p><p></p><p>平台工程也是第二年入选，其指通过一系列工具和流程为企业的软件开发团队提供一个自助开发门户或者内部开发平台。每个平台都是一个由专门的产品团队创建和维护并通过与工具和流程对接来支持用户需求的层。平台工程的目标是优化生产力和用户体验并加快业务价值的实现。</p><p></p><p>平台工程有三个关键词：可组装、可重用和可配置。其中包含很多具体的功能模块，比如一些基础设施、开发工具、数据管理、安全与身份管理、运维管理、服务目录，这些其实都可以平台化。本质上，平台工程是将我们从项目管理思维转化成产品管理的思维，将本来相对独立的开发项目流程去模块化和集中化，这是平台工程所做的事情。</p><p></p><h4>2.AI增强开发（AI-Augmented Development）</h4><p></p><p></p><p>AI增强开发指使用生成式AI、机器学习等AI技术协助软件工程师进行应用设计、编码和测试。AI辅助软件工程提高了开发人员的生产力，使开发团队能够满足业务运营对软件日益增长的需求。这些融入了AI的开发工具能够减少软件工程师编写代码的时间，使他们有更多的时间开展更具战略意义的活动，比如设计和组合具有吸引力的业务应用等。</p><p></p><p>简单来说，AI增强开发就是用AI加持整个开发的生命周期，主要包括AI代码生成、AI增强测试、设计-代码三个过程。在AI代码生成环节，增强开发主要指的是样板代码、重构代码和对旧框架或者编程语言进行学习三个部分；在测试环节，增强开发主要指编写测试代码、生成测试数据和生成单元测试中的“测试桩”三个部分；在设计-代码环节，增强开发主要指AI参与开发全流程，这在目前还没有完全实现，是未来愿景。</p><p></p><h4>3.行业云平台（Industry Cloud Platforms）</h4><p></p><p></p><p>与平台工程一样，行业云平台同样是第二年入选。简单来说，行业云平台是把传统“云服务”中的IaaS、PaaS、SaaS进一步解耦，通过模块化的方式提供具有业务能力的云平台。换句话讲，传统的“云”上面加一层“业务模块”。之所以可以连续两年入选，是因为企业如今更关注云上面的投资如何产生可量化的商业价值，尤其是在经济环境不稳定的情况下，企业上云不仅仅追求技术价值，同样追求商业价值。</p><p></p><p>&nbsp;“行业云平台”实际上有两大特征：可组装和模块化。实际上，行业云平台是把通用的业务能力模块化之后放在“公有云”上面，然后重新排列组合。需要注意的是，首先，Gartner提的“行业云平台”主要基于公有云服务，与中国特色的行业云是不一样的，中国特色的行业云在Gartner定义里面叫“社区云”；其次，行业云平台添加了针对行业的“业务能力封装（PBC）”；最后，其可以支持特定行业需求，不只是单纯的技术平台、一定是技术和业务叠加的平台。</p><p></p><p>Gartner 预测，到2027年，将有超过70%的企业使用行业云平台（ICP）加速其业务计划，而2023年的这一比例还不到15%。ICP通过可组合功能将底层 SaaS、PaaS 和 IaaS 服务整合成全套产品，推动与行业相关的业务成果。这些功能通常包括行业数据编织、打包业务功能库、组合工具和其他平台创新功能。ICP是专为特定行业量身定制的云方案，可进一步满足企业机构的需求。</p><p></p><h3>三、交付价值</h3><p></p><p></p><h4>1.智能应用（Intelligent Applications）</h4><p></p><p></p><p>Gartner将智能应用中的“智能”定义为自主做出适当响应的习得性适应能力。智能应用的本质是在传统应用中加入AI或者生成式AI的能力，通过持续的学习、适应和预测，提高用户的体验或者是提高更多商业价值，也就是用AI加持这个应用。在许多用例中，这种智能被用于更好地增强工作或提高工作的自动化程度。作为一种基础能力，应用中的智能包含各种基于AI的服务，如机器学习、向量存储和连接数据等。因此，智能应用能够提供不断适应用户的体验。</p><p></p><p>今年5月份，美国得克萨斯大学奥斯汀分校的研究团队在《自然神经科学》杂志上发布了一篇文章，其基于人工智能大模型开发出一种对于大脑活动的解码器，可以将大脑活动转化为连续的文本流、通过一种非侵入式的方法让AI学会“读心术”。这与脑机接口的侵入式不同，这种方式是非侵入式的，是让研究对象在功能性磁共振成象的扫描仪里面进行扫描，并给实验者听一些音频故事，比如：放个电影、播个小说等。根据此期间的人脑活动情况，最后会转化为AI可识别的形式，并以文字的形式表达出结果。目前，这种方式的识别率不算特别高，但蛮有意思，这是“AI智能应用”的一个示例。</p><p></p><p>目前已存在对智能应用的明确需求。在2023年Gartner首席执行官（CEO）和业务高管调查中，26%的CEO认为对企业机构破坏力最大的风险是人才短缺。吸引和留住人才是CEO在人力资源方面的首要任务，而AI被认为是未来三年对他们所在行业影响最大的技术。</p><p></p><h4>2.全民化的生成式AI（Democratized Generative AI）</h4><p></p><p></p><p>对商业用户来讲，如果将来可以无处不在的获取以前得不到的知识和技能，那么预示着一波新的生产力浪潮即将到来。经过大规模预训练的模型、云计算与开源的融合正在推动生成式人工智能（生成式AI）的全民化，使这些模型能够被全球工作者所用。好处是工作效率会提高，技术也会更加普及，会出现很多创新生态。缺点是一些数据，甚至机密数据可能会丢失。</p><p></p><p>到 2026 年，Gartner预测超过80%的企业将使用生成式AI的API或模型，或在生产环境中部署支持生成式AI的应用，而在2023年初这一比例不到5%。</p><p></p><p>生成式AI应用可以让企业用户访问并使用大量内部和外部信息源，这意味着生成式AI的快速采用将极大地促进企业知识和技能的全民化。大型语言模型使企业能够通过丰富的语义理解，以对话的形式将员工与知识相连接。只是，需要注意实现这一切的前提必须基于风险治理。</p><p></p><h4>3.增强型互联员工队伍（Augmented-Connected Workforce）</h4><p></p><p></p><p>增强型互联员工队伍（ACWF）是一种优化员工价值的战略。加速并扩大人才规模的需求推动了ACWF的发展趋势。ACWF使用智能应用和员工队伍分析提供助力员工队伍体验、福祉和自身技能发展的日常环境与指导。同时，ACWF还能为关键的利益相关方带来业务成果和积极影响。简单来说，ACWF就是用AI技术加持互相之间连接和协作的员工，核心是提供员工的数字体验或者是数字员工体验。</p><p></p><p>目前，我们处于混合办公时代，企业需要考虑利用各种互相连接的设备和技术提高数字员工的体验，并进一步用AI技术进行增强，这是每个CIO需要考虑的问题。所谓的“增强”，指的是对于从终端应用知识库甚至是员工情绪当中提取出数据进行接近实时的处理和反馈。以此分析员工目前的工作状态和压力，或者根据员工在某些系统的逗留时长和操作迭代工作流程，甚至找到一些行为感知系统预测员工离职倾向等。当然，这个过程必须考虑安全和隐私问题。虽然AI不会取代人类的关怀，但可以做增强，至少给人类提供一些数据方面的支撑，并可以达到个性化的体验，毕竟不同员工的诉求也是不同的。</p><p></p><p>到 2027年底，Gartner预测25%的首席信息官（CIO）将使用增强型互联员工队伍计划将关键岗位的胜任时间缩短50%。</p><p></p><h4>4.机器客户（Machine Customers）</h4><p></p><p></p><p>机器客户（也被称为“客户机器人”）是一种可以自主协商并购买商品和服务以换取报酬的非人类经济行为体。其进化可以分成三个阶段：人类主导，由机器通过一定的规则购买特定商品；人类和机器共同主导，共同优化购买选择，最终由机器执行购买操作；机器推测人类需求，根据根据规则、场景和偏好进行自主化的购买。目前，第一个阶段已经实现，第二个阶段实现了一部分，第三个阶段是接下来的愿景。</p><p></p><p>Gartner预测，到2028年，将有150亿台联网产品具备成为客户的潜力，这一数字还将在之后的几年增加数十亿。到 2030 年，该增长趋势将带来数万亿美元的收入，其重要性最终将超过数字商务的出现。在战略上应考虑为这些算法和设备提供便利乃至创造新型客户机器人的机会等。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2K0clWV5ZGjlPumJhf9G</id>
            <title>一场真正的金融科技大会</title>
            <link>https://www.infoq.cn/article/2K0clWV5ZGjlPumJhf9G</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2K0clWV5ZGjlPumJhf9G</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 09:34:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 极客邦科技, InfoQ, QCon, FCon全球金融科技大会
<br>
<br>
总结: 今年是极客邦科技旗下 InfoQ 主办的 QCon 全球软件开发大会进入中国 15 周年，他们决定携手极客时间企业版，为金融行业打造了一场真正的金融科技峰会——FCon全球金融科技大会。首届 FCon 全球金融科技大会将落地上海，与业界知名银行和金融机构的大咖一起交流前沿技术在金融领域的落地案例。大会将涵盖金融领域数字化转型、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等专题内容。参会者将有机会与金融科技领军人物分享行业见解，深入探讨行业热点话题，并参与闭门会研讨。 </div>
                        <hr>
                    
                    <p>今年是极客邦科技旗下 InfoQ 主办的 QCon 全球软件开发大会进入中国 15 周年，在过去的 15 年里，我们陪伴中国的互联网产业迅猛发展。今年，我们做出了一个重要的决定——要携手极客时间企业版，为金融行业打造了一场真正的金融科技峰会——<a href="https://fcon.infoq.cn/2023/shanghai/schedule">FCon全球金融科技大会</a>"。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1e/1e8fa4e7bdc1df8209195a2959848d4a.png" /></p><p></p><p>现在，它来了！11 月 19-20 日，首届 <a href="https://fcon.infoq.cn/2023/shanghai/schedule">FCon 全球金融科技大会</a>"将落地上海。今天，我们向您发出邀请，诚挚邀请您参加本次盛会，与工商银行、招商银行、兴业银行、中信银行、北京银行等业界知名银行，以及平安人寿、阳光保险、广发证券、国泰君安等金融机构的大咖，交流大模型、 Web 3.0&nbsp;、隐私计算、数字货币、区块链等前沿技术在金融领域的落地案例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c5/c5b9e636b96dd9df45a5b55afe4ec6b5.jpeg" /></p><p></p><p>在会上，您将听到金融领域数字化转型挑战探索、DevOps 在金融企业落地实践、金融行业大模型应用、创新的金融科技应用、金融实时数据平台建设之路、金融安全风险管控、数据要素流通与数据合规等 10+ 专题的深度内容。</p><p></p><p>在会上，您将与来自不同公司的金融科技领军人物分享最新的行业见解，交流各自的经验和思考，深入探讨行业热点话题，并获得对未来发展的前瞻性思考。</p><p></p><p>在会上，您将在多场特别策划的闭门会研讨中，与各位金融大咖展开更多深入交流。</p><p></p><p>期待您的加入，与我们一同探索金融科技的未来！<a href="https://fcon.infoq.cn/2023/shanghai/">点击此处即可查看全部演讲专题</a>"。目前 7 折报名倒计时 3 天，现在报名立减&nbsp;¥2040，咨询购票可联系：17310043226（微信同手机号）</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07e3707140e88a96c58c9a1e5bc26cf2.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/fMGfIhCgDGLF2uDqV7Wt</id>
            <title>从华为的 AI 全景，看人工智能技术的演进与未来</title>
            <link>https://www.infoq.cn/article/fMGfIhCgDGLF2uDqV7Wt</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/fMGfIhCgDGLF2uDqV7Wt</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 09:09:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 生成式 AI, 应用渗透, 技术演进
<br>
<br>
总结: 随着大模型与生成式 AI 技术的快速发展，其在产品创新、提质增效、数据安全等方面的优势越来越明显。AI 技术在医疗保健、高科制造、交通出行、金融、教育等领域得到广泛应用，并深刻地改变着人类的生产生活和社会结构。AI 技术的发展离不开技术演进与架构支撑，包括人工智能的演进历程和数据、算法、算力三要素的支持。华为作为一家全面智能化的企业，通过提供算力底座、AI 平台、开发工具等支持，构建了完整的 AI 生态系统，为各行各业的创新应用提供了全面的能力。 </div>
                        <hr>
                    
                    <p>随着大模型与生成式 AI 的迅速崛起，在产品创新、提质增效、数据安全等方面的优势越来越显著，AI 技术的应用也因此步入了崭新的阶段。</p><p></p><p>事实上，在这一波浪潮之前，AI 技术就已经被广泛应用医疗保健、高科制造、交通出行、金融、教育等诸多领域，并深刻地改变着人类的生产生活和社会结构。大模型 + AIGC 的发展可能会全面加速 AI 的产业渗透与应用，人类也将有机会迎来新一轮的技术红利。</p><p></p><p>然而，任何一项伟大的技术都不是一蹴而就的，AI 技术也走过了漫长的发展之路。</p><p></p><p></p><h3>AI 的技术演进与架构支撑</h3><p></p><p></p><p>人工智能（AI）技术经历了令人瞩目的演进之路，从 20 世纪中叶人工智能概念的出现，到符号推理时代、专家系统时代，再到连接主义和神经网络复兴，每个阶段都伴随着重大的技术突破和应用范围的扩展。</p><p></p><p>21 世纪后， AI 进入了全新的深度学习和大数据时代，并经历了蓬勃发展的 20 年，随着大数据的可用性和计算资源的增加，深度学习逐渐成为 AI 技术领域最重要的分支之一。大模型 +AIGC 的迅速崛起可以看作是深度学习厚积薄发的结果。</p><p></p><p>除了自身的技术演进外，AI 技术应用的蓬勃同样受益于其他技术的发展与支持。</p><p></p><p>我们都知道人工智能三要素——数据、算法、算力。其中数据提供了机器学习模型所需的输入和学习材料，算法定义了模型如何处理数据并做出决策，算力则支持了数据和算法的处理能力。数据层面，除了一些互联网数据或者公开数据集，人工智能模型在特定行业的应用通常会生成更多的数据，这些数据可以被重新用于模型的训练和改进，以提高模型的性能，形成“数据反馈循环”。算法层面，其发展主要来源于深度学习、自然语言处理、计算机视觉等人工智能自身算法的突破。算力层面，一方面，云计算的发展和分布式算力平台的成熟为 AI 模型的训练与推理提供了强大的计算资源；另一方面，专用硬件如 GPU 和 TPU 的出现，进一步提高了深度学习的性能。</p><p></p><p>除此之外，各种面向 AI 开发与构建的工具或框架同样也加速了 AI 的快速发展。如 TensorFlow、PyTorch、MindSpore 等，大大简化了 AI 模型的开发和部署过程。这些工具平台为开发者提供了丰富的资源和社区支持，促进了 AI 技术的快速发展。</p><p></p><p><img src="https://static001.geekbang.org/infoq/51/518ac178dcadd65c460b6fbb7d452673.png" /></p><p></p><p>图表 1：人工智能产业链结构（公开资料整理）</p><p></p><p>由此，一个涵盖底层基础设施、中间层算法技术、顶层应用的 AI 技术架构基本成型，数据以管道的形式链接各层，各类开发工具则被应用于 AI 应用或服务的构建、部署与管理。</p><p></p><p></p><h3>华为的 AI 锚点与能力全景</h3><p></p><p></p><p>随着人工智能的快速发展，AI 技术已经成为了各行各业的重要驱动力。企业和开发者们迫切需要一种全面的 AI 生态系统来支持他们在各个领域的创新应用。</p><p></p><p>不久前，华为在全联接大会上正式提出将推行全面智能化（All Intelligence）的战略，旨在加速千行万业的智能化转型。在此过程中，华为将通过算力底座、AI 平台、开发工具等赋能开发者与伙伴，并希望成为大模型“百花园”的黑土地。</p><p></p><p>事实上，不只是为大模型提供算力、技术支持，华为很早就开始了在 AI 方向的布局，覆盖了从底层基础设施到中间层工具平台再到上层应用的各个方面，并逐步构建出了一个完整的 AI 生态雏形，为 AI 的开发、应用和发展提供了广泛的支持。</p><p></p><p>为了能更具体、形象地呈现华为在 AI 领域的全貌，不久前华为正式上线了《华为 AI 能力全景与开发者成长图谱》，该图谱由华为联合 InfoQ 共同绘制，以行业视角深度解析华为的 AI 布局，下面我们以此为例，做详细解读。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/7e/14/7e68661075a2a4b3b083d9466c214914.png" /></p><p></p><p>图表 2：华为 AI 能力全景与开发者成长图谱</p><p></p><p>在 AI 硬件系统方面，华为在计算、存储、网络、数据库、安全与隐私保护以及操作系统等方面都取得了显著的成就，为 AI 应用提供了稳定和高效的计算、存储和网络支持。</p><p></p><p>以 AI 计算为例。据 IDC 预测，国内 2026 年智能算力规模有望进入每秒十万亿亿次浮点计算级别，2021-2026 年国内智能算力规模年复合增长率有望达 52.3%。</p><p></p><p>随着 GPT 类产品的现象级走红，AI 迎来了大模型时代，各类科技公司闻风而动，争相进入大模型及各场景应用的赛道。事实上，早在大模型浪潮之前，华为就已发力 AI 硬件国产化领域，推出了昇腾系列处理器，并被广泛应用于各行各业。</p><p></p><p>除了在 AI 硬件底座方面的抢先布局外，在开发工具方面，华为提供了一系列的 AI 开发工具和框架，比如异构计算架构 CANN、AI 计算框架 MindSpore 和第三方适配以及全流程开发工具链 MindStudio 等，以加速开发效率。这些工具和平台不仅为开发者提供了强大的支持，还允许他们构建自己的 AI 应用。</p><p></p><p>作为新一代全场景 AI 框架，MindSpore 旨在帮助开发者孵化出各种 AI 创新算法和应用。在功能上，MindSpore 同样拥有自己的优势，能够实现动态图和静态图之间的切换、满足多场景 AI 计算的需求、降低用户使用模型并行时所面临的难度以及全场景快速部署等等。</p><p></p><p>从 MindSpore 1.0 版本的业界首个全场景 AI 框架，到 1.5 版本原生支持大模型，再到能够直接提供一站式大模型训练、推理一体化能力的 2.0 版本，MindSpore 成为了这一波 AI 大潮的先行逐浪者。截止到 6 月底，基于华为昇思 MindSpore AI 框架，国内外的厂商已经训练了多个参数规模在百亿~ 万亿之间的大模型，同时也新增支持 LLaMA、Bloom、GLM、GPT 等百亿大模型，帮助众多企业或开发者走向大模型构建与应用的最前线。</p><p></p><p>在 AI 产品服务方面，华为也积极推出了多项创新解决方案。其中最引人瞩目的便是盘古大模型，它包含了一系列强大的 AI 模型，覆盖了自然语言处理（NLP）、多模态应用、计算机视觉（CV）、预测分析以及科学计算等多个领域。</p><p></p><p>为加快 AI 重塑千行万业，在 7 月举办的华为开发者大会上，华为云发布盘古大模型 3.0，正式提出 5+N+X 的三层解耦架构，通过分层的 AI 能力及工具，成就不同客户百模千态的需求。紧接着在 9 月的华为全联接大会上，华为再次推出了盘古大模型在矿山、政务、汽车、气象、医学、数字人、研发等领域的创新服务，旨在帮助行业企业解难题、做难事。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/59f22f37a72c34c327816d26d469781e.png" /></p><p>图表 3：华为云盘古气象大模型研究成果在《Nature》正刊发表</p><p></p><p>值得一提的是，今年 7 月华为云盘古气象大模型研究成果在《Nature》正刊发表。作为首个精度超过传统数值预报方法的 AI 模型，其速度相比传统数值预报提速一万倍以上，为全球气象预报提供了一个新的选择。</p><p></p><p>关于盘古大模型背后的故事及场景应用，早前 InfoQ 有深入探访盘古大模型核心研发团队——<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651173375&amp;idx=2&amp;sn=20d97a6329ed6e61a83873604171ed34&amp;scene=21#wechat_redirect">《AI 如何使能千行百业？探秘华为云盘古大模型》</a>"。</p><p></p><p></p><h3>AI 技术的应用前景与开发者蓝图</h3><p></p><p></p><p>AI 技术的前景非常广阔，涵盖了各个行业。在今年 4 月的博鳌亚洲论坛上，华为云人工智能领域首席科学家田奇博士表示：“AI For Industries 将成为人工智能新的爆发点”。</p><p></p><p>事实上，华为在 AI 使能产业方面已经取得了显著的进展，与众多行业伙伴或政府机构携手推出了包含智慧医疗、智慧金融、智慧交通、智慧机场、智能制造、智能矿山等在内的多场景的产业 AI 产品或服务，帮助伙伴降本增效，帮助用户提升数智体验。</p><p></p><p>总体来说，一方面 AI 技术的广泛应用正在赋能千行百业，为 B 端企业带来包括数智升级、降本增效等方面的显著成果，为 C 端用户带来更优、更智能的用户体验等；另一方面，AI 作为一项通用技术，其蓬勃发展也为开发者的个人成长与就业提供了更多的选择。</p><p></p><p>同样，面向开发者的学习与成长，依托于华为在尖端技术领域的持续深耕和在开发者生态上的开放与投入，华为成立了「开发者空间栈」，它是一个面向新一代开发者的技术成长与学习阵地，聚焦人工智能、大数据、物联网等行业热门技术或前沿趋势，旨在通过训练营、社区活动等形式，帮助开发者持续构建未来竞争力。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e2b21cb32790449e06c7934a803441a.png" /></p><p>图表 4：华为开发者空间栈专题页封面</p><p></p><p><a href="https://developer.huawei.com/home/devrun.html">「开发者空间栈」</a>"首期以 AI 技术为锚点，旨在聚合华为在 AI 领域的领先技术、知识与经验，向开发者开放，从社区到学堂到赛事，帮助开发者更快地了解 AI 趋势、更全面地学习 AI 技术、更好地搭建 AI 应用。</p><p></p><p>据了解，在开发者社区方面，华为建立了昇腾社区、华为云社区以及华为云 AI Gallery 等社区平台，为开发者提供了交流和分享经验的机会；在开发者学习方面，华为为开发者提供了鸿蒙、昇腾、鲲鹏、华为云等开发者学堂专区，为开发者提供了系统化的学习路径；在活动赛事方面，华为举办了一系列 AI 活动赛事，如昇腾引力场、华为云 AIGC 实战营、昇腾 AI 创新大赛以及 2023 年鸿蒙生态百校种子计划等。这些活动赛事不仅为开发者提供了锻炼和竞争的机会，还鼓励他们积极参与 AI 技术的应用和创新。</p><p></p><p></p><h3>结语</h3><p></p><p></p><p>“构建万物互联的智能世界”是华为公司的愿景，同样也是我们对未来世界的美好期许。随着 AI 技术的不断演进和应用，我们生产生活的方方面面都在经历深刻而显著的智能化变革，不仅直接作用于生活质量的提升，也为未来的数智世界开启了崭新的可能。</p><p></p><p>在 AI 技术的引领下，我们正在迈向一个更加智能、便捷和可持续的未来，在这背后，离不开政策的支持、产业生态的共建以及科技公司们的持续投入，同样也离不开怀揣着技术热忱、立志改变世界的了不起的开发者们。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6UaLWZoeuLbKX4gJmI3l</id>
            <title>中国版Copilot？代码优化提效5倍、采纳率提升44%……讯飞iFlyCode2.0正式发布，各项能力全面开放</title>
            <link>https://www.infoq.cn/article/6UaLWZoeuLbKX4gJmI3l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6UaLWZoeuLbKX4gJmI3l</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 06:25:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 1024, 科大讯飞全球, 开发者节, iFlyCode
<br>
<br>
总结: 今年的1024开发者节是科大讯飞全球开发者节，会上发布了iFlyCode 2.0智能编程助手，该工具能够提高编程效率，帮助开发人员和测试人员提升工作效果。iFlyCode具有中文友好性，能够满足中国开发者的需求。科大讯飞的星火认知大模型也取得了显著突破，为iFlyCode提供了强大的支持。iFlyCode正在全面影响开发者的编程方式。 </div>
                        <hr>
                    
                    <p>又是一年 1024，又是一年科大讯飞全球 1024 开发者节。不同往常的是，今年的 1024 开发者节似乎承载着更多的期待。</p><p></p><p>10 月 24 号，第六届声博会暨 2023 科大讯飞全球 1024 开发者节在合肥举行。上午的开幕式上，<a href="https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">讯飞星火认知大模型 3.0</a>" 如约而至，把现场气氛带至高潮；下午的代码论坛也惊喜不断，科大讯飞 iFlyCode 2.0&nbsp;智能编程助手正式发布，全方位升级、全功能开放，引发了现场开发者的欢呼。</p><p></p><p>会上，科大讯飞总裁吴晓如表示软件是新一代信息技术的灵魂，是数字经济发展的基础，是制造强国、网络强国、数字中国建设的关键支撑。利用“软件定义”赋能实体经济变革是产业发展的重要引擎。软件编程方式的AI变革，所带来的不仅仅是软件研发效率的跨越式提升，更是一次全行业的效能进化。</p><p></p><p>作为一款AI辅助编程工具，自 8 月 15 日 1.0 版本发布以来，iFlyCode 在 AI 能力上始终和星火大模型最新版本同步，并且深度融入了软件设计、编码和测试等编码核心环节，旨在全面赋能软件研发全流程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/57/57bc44ef9cecb9dd75653650d1bec94f.png" /></p><p></p><p>值得一提的是，iFlyCode 自发布之初就采用 IDE 集成的模式，本次 2.0 版本的 iFlyCode 能力再次升级。据悉i FlyCode2.0 可以直接集成于IDE界面，相比于 1.0 版本，依托于星火大模型的能力提升，iFlyCode2.0代码编辑区的AI能力再次升级；此外 2.0 版本的 iFlyCode 智能问答窗口，用户可以直接在IDE 界面跟 iFlyCode 进行 Chat 对话，可以让它帮忙生成代码、解释代码、代码纠错或者是给出更多代码解决方案等。</p><p></p><p>发布会上，讯飞公开了一组 iFlyCode 助力个人开发提效的数据，数据显示：iFlyCode 能够帮助编程初学者缩短 60% 的学习时间，帮助开发人员代码优化提效 5 倍，帮助测试人员代码用例采纳率提升 44%。iFlyCode 智能编程助手在软件开发全流程中发挥着越来越积极的价值。</p><p></p><p></p><h3>面向“未来开发者”的 AI 助手</h3><p></p><p></p><p>事实上，从全球范围来看，AI智能编程助手已是大势所趋。无论是 ChatGPT的对话式辅助编程，还是 Copilot 的 IDE 集成模式，都已经对开发者的日常工作产生了深远影响。不少开发者表示AI编程工具已经成为他们的开发生活中不可或缺的一部分。这不仅是因为它们能够加速开发周期，还因为它们能够帮助开发者更容易地掌握新的编程语言和框架。</p><p></p><p>面向未来，正如 OpenAI 科学家 Andrej Karpathy 所言——最好的编程语言是自然语言。直接用中文提示进行代码生成，自然是新一代的中国开发者们所乐见其成的。</p><p></p><p>但是很显然，目前中国开发者们在使用 ChatGPT 或 Copilot 进行辅助编程时，仍然有可能面临一些语言上的挑战，诸如：文档和注释翻译困难、中文版本的代码建议质量不如英文、技术术语翻译偏差、语法和逻辑错误等问题。</p><p></p><p>据 2021 年GitHub数据显示，中国开发者数量已达 755万+，位居全球第二，且增速迅猛。面对数量如此庞大的中国开发者以及更多潜在的新生代群体， “中文友好”成了一个很难绕开的话题。</p><p></p><p>iFlyCode 的横空出世，除了在全流程智能化方面为人称道外，更值得一提的便是其“中文友好性”。相比于国际上的其他智能编程工具，iFlyCode 一开始就凭借着对中文环境的天然亲和力和本地化优化，呈现出了令人满意的性能。</p><p></p><p>有开发者向 InfoQ 表示：“iFlyCode 在面临复杂的中文分词或者一些独特的中文语言元素时，都能很好地辨识并处理，高效且准确。而且它还能针对中文进行智能排版，这一点在做代码注释时尤为好用。”从某种意义上来说，iFlyCode 是一个理解并尊重中文语言特性的智能编程助手，能够最大程度上满足中文环境下的各种需求。</p><p></p><p>当然，iFlyCode 能够取得这样的成绩也不意外，它的高效、实用离不开其背后强大的支撑——讯飞星火认知大模型。在 8 月 15 日的讯飞星火 V2.0 发布会上，科大讯飞董事长刘庆峰曾放下豪言：“10 月 24 日，讯飞星火认知大模型将超越 ChatGPT，明年上半年将对标 GPT-4。”</p><p></p><p>明年的情况犹未可知，但是当下第一个 Flag 已经有了结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/05/055d0cedaf06368bd897624c95d12e63.png" /></p><p></p><p>在上午的主论坛发布会上，刘庆峰表示，星火认知大模型 3.0 的能力已经全面对标 ChatGPT。在中文能力客观评测方面全线领先 ChatGPT，英文 48 项任务的测试情况，同样是星火认知大模型 3.0 略有领先。整体来看，讯飞已经实现了第一个 Flag。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cf/cf37756fc2ef12234b2078ba3fe15acd.png" /></p><p></p><p>在代码能力方面，讯飞星火大模型同样取得了显著的突破。基于OpenAI 构建的代码生成能力公开测试集 humanEval，在 Python、Java、JS、C++ 等编程语言方面，星火 3.0 相较于 2.0 有较大幅度的提升，且全面领先 ChatGPT；基于认知智能全国重点实验室构建的代码实用场景测试集 IFlyCode-Eval，在代码生成、补全、解释以及测试检错、编程问答等编程任务方面，星火 3.0 相较于 ChatGPT 也呈现出领先趋势。</p><p></p><p>得益于星火大模型的持续迭代与赋能，iFlyCode 能够给中文开发者更优异的编程体验。</p><p></p><p></p><h3>不止于研发提效，iFlyCode 正在全面影响开发者的编程方式</h3><p></p><p></p><p>据 Gartner 最新预测：预计在 2026 年，超过 80% 的企业将使用生成式 AI （GenAI）应用程序编程接口（API）或模型，或者在相关生产环境中部署支持生成式 AI 的应用程序。</p><p></p><p>技术的车轮滚滚向前，开发者们究竟是如何看待这一波 AI 浪潮、又是如何去应对的呢？在1024开发者节之前，我们采访了一些 iFlyCode 的用户，听听他们的心声。</p><p></p><p>张先生是一位年轻的 AGV 公司人机交互界面前后端开发工程师，用他的话来说，一毕业就赶上了人工智能的浪潮，也就顺势走到了现在这家企业。</p><p></p><p>接受采访时，张先生放下手头的工作，笑着跟我们说：“随便聊，我有时间。像我现在这个项目正常来说可能要 4 个月左右，现在两个半月差不多已经做完了，期间一直在用 iFlyCode。”</p><p></p><p>他在向我们介绍他的工作时提到：“人机交互界面开发工作，前端页面布局的工作量占比很大，传统页面布局方式是手敲代码，费时费力，后面有了UI框架可以引入，但是效率提升同样有限。后面一次偶然情况下看到了讯飞的发布会，了解到了 iFlyCode 这个工具，抱着试一试的心态开始上手，当然后果就是‘一发不可收拾’，我自己估摸过，开发效率提升了 30%-40% 左右。运用 iFlyCode 插件最大的好处是，我只需要描述清楚想要的页面布局效果就可以自动生成代码，省去了大量的敲代码时间。”</p><p></p><p>在问及同类型的编程助手插件有很多，为什么最终还是选择 iFlyCode 时，张先生表示：“我觉得 iFlyCode 对中国程序员更加友好，对中文的理解能力我认为是优于 ChatGPT 和 Copilot 等外国插件的，使用方式也更符合国人思维习惯，而且它能够进行中文报错，客观上降低了中国程序员代码调试成本；另外就是它的兼容性，它在 IDEA 和 VSCode 这两种主流集成开发环境中适配度较高，支持多种编程语言开发，基本能覆盖程序员的开发需求。”</p><p></p><p>对于前端开发者而言，如果能够通过中文自然语言提示的方式让工作化繁为简，利用工具快速生成页面布局、摆脱重复工作，工作愉悦感应该能获得指数式的提升。</p><p></p><p>当然，iFlyCode 作为一款辅助编程AI工具，其最大的特点在于不同的“打开方式”都能带给开发者效率的提升，除了直接帮助写代码，它的信息整合能力、框架参考价值可能是更底层且普适的。</p><p></p><p>在我们的采访里，同样还有一位前端工程师曾先生，任职于一家储能公司，做电池监控管理嵌入式软件开发，从业6年。储能同样是一个前沿领域，也是当前最热门的行业之一。</p><p></p><p>作为一位前端开发老玩家，他对 iFlyCode 的使用可以说是另辟蹊径。“我其实使用 iFlyCode 也就两个月，可能大多数人主要都是拿它写代码，但是我用得最多的是查资料。平时工作中需要查找大量的资料或者一些算法类的文章，用浏览器+搜索引擎有时候查到的内容太发散了我还需要一个个去甄别哪些是我要的，我发现用 iFlyCode 查找出来的内容会更加精准一些，也省去了我不同搜索引擎倒腾来回的麻烦。”</p><p></p><p>对于 iFlyCode 写代码的能力，曾先生也有他自己的看法。他提到有时候在一个项目中要管理很多设备，需要大量的代码工作，他会用 iFlyCode 去生成基本的框架，然后在此基础上修改一些变量。另外在生成注释、代码纠错方面，同样也能利用 iFlyCode 提供一些模板或建议。</p><p></p><p>有一个场景，曾先生印象深刻：“我有一些不熟悉的工具和语言，比如我用 excel 来做数据解析，需要在 excel 上编写代码来实现一些自定义函数功能。我就把需求表达给它，它就能给我提出建议，按照它的指引就可以在excel上面做数据解析，在这个过程中也学习了工具的使用。”</p><p></p><p>“目前来说，我还不太打算用AI工具去帮我写大量的代码，它更多的是给我一个参考借鉴的价值，帮我去做信息收集、整理，以及提供一些灵感。”曾先生非常笃定地说道。</p><p></p><p>从直接帮忙写代码，到参考借鉴，我们看到了 iFlyCode 面向初中级开发者的工作提效。面向更高阶的开发者，iFlyCode 的使用体验又是怎样？</p><p></p><p>这一次我们邀请了百忙之中的邓先生参与了一次线上采访。邓先生是一家科技公司的首席科学家，如果要以从业年限计算等级，大概是“骨灰级”软件工程师。</p><p></p><p>谈及自己的从业经历，邓先生调侃道：“&nbsp;我从 2001 年读大学时开始编程，到今年应该是超过 22 年了，2001 年帮人写网页，一个页面几百行就搞定，还能收到 5000 元的天价报酬，现在想想，还好当时还没有&nbsp; iFlyCode。”</p><p></p><p>在聊到程序员如何面对这一波 AI 浪潮时，邓先生打了一个很有意思的比方：“我想应该没有程序员会拒绝 AI，就像在计算器发明之前，我们都用笔头去计算多位数的乘除，但是有了之后，为什么不用呢？它明显更高效更准确。”</p><p></p><p>对于AI带来的开发者焦虑，邓先生补充道：“公司有不少小伙伴跟我表示 AI 对他们的冲击太大了，担心 AI 降低了开发的门槛，会让很多开发者失业。其实以我这些年的心路历程来看，任何人其实都不应该去拒绝新事物，就比如开发语言更新换代同样也非常快，如果不保持学习，可能就会被淘汰，只有顺势才不会被颠覆。如果以这样的心态面对 AI，把它当做一门新的工具，尝试使用它，利用工具来强化自身，我想他不仅不会焦虑，还可能发现新的机会。”</p><p></p><p>当然，目前的AI编程工具也不少，在谈及对于AI编程工具的选择，邓先生表示：“我其实很多AI工具都有在用，现在更多是 iFlyCode 和 Copilot 交替使用，首先我认为在语义理解方面它俩基本是一个梯队的了，其次是它们都是以插件的形式去兼容各种开发平台，调用非常方便。最值得一提的是，iFlyCode 全面开放了AI对话小助手窗口，能起到实时辅助的作用，比传统的代码查找方便得多，市面上的友商如 Copilot 还没有对公众开放相关功能。所以大多数时候我会推荐小伙伴们尝试用 iFlyCode。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1bd77562a00642f2e3b6144e9a451e6.png" /></p><p>图为 iFlyCode 对话窗口</p><p></p><p>在采访中，邓先生特别提到，对于中高阶开发者而言，AI 工具带来的改变可能比大家想象的要多。</p><p>首先是能够让代码更规范，他提到很多开发者在做一些诸如变量命名之类的事情时，会按照自己的习惯去做，英文全拼、缩写，甚至还有拼音，不一而足，这对于后续的维护带来了很大的挑战。iFlyCode 的介入能够帮助开发者最大限度规避这些问题。</p><p></p><p>其次是更高效。对于中高阶开发者而言，很多简单粗暴且费时的代码可以直接交给 AI 去做，只需要自己做最后的检查微调。邓先生表示：“由于业务需要使用不同的开发语言去写代码，多种语法来回切换是一件很糟心的事，现在用 iFlyCode，我基本就直接先写注释，它会帮我先把基本的代码框架搭好，有一些甚至可以直接生成或者转换语言格式，非常方便，能帮我省掉20-30%的开发时间；另外 iFlyCode 在检查代码和报错方面同样能够给到我一些帮助。”</p><p></p><p>值得一提的是，除了工作提效，iFlyCode 还能够模仿用户的代码习惯，让生成的代码更像自己写的。邓先生提到自己的注释编写习惯是先写名词后写动词，iFlyCode 能够直接把这个习惯应用到代码生成中去，这样他自己读起来很顺畅，其他人做维护时也不会出现理解上的偏差或割裂。</p><p></p><p>最后是改变思维模式，他认为这一点最为重要且影响深远。他提到，以前没有这些 AI 辅助工具，写代码基本是在脑海里大概有一个步骤和框架就开干，然后边写、边想、边改、边调试，是一种渐进式的模式，但是往往也容易写到后面发现此路不通等情况。现在有了 AI 编程助手后，需要把需求描述给它，这个需求必须足够逻辑清晰且细节到位，要极尽可能把脑海中的蓝图描绘出来，这个时候反而倒逼自己去深度思考软件开发的全流程。其实也就是从点状思维向系统思维的跃迁。</p><p></p><p>这里邓先生也举了一个有趣的例子：“我以前有个同事，每次写代码前都要摇头晃脑好久，我当时很不理解，但是每一次他的代码都写得比我快，后面我才反应过来他是在深度思考，虽然下笔更晚，但是一气呵成。其实我发现很多开发者可能都跟我之前一样，喜欢渐进式工作，有利有弊，从效率角度考量，如果能够系统思维一定会是事半功倍的，所以我认为像 iFlyCode 这样的AI辅助编程工具对于中高阶开发者同样也有着不容小觑的作用。”</p><p></p><p>帮助规范代码、提高代码效率、改变思维方式，由表入里，这是邓先生对&nbsp; iFlyCode 的价值总结。AI 辅助编程不是洪水猛兽，也不只是对初中级开发者有用，对于不同行业、岗位，不同年龄阶段的开发者而言，顺势而为，与之共生，都可能收获不一样的成长。</p><p></p><p>无独有偶，除了面向企业开发者，iFlyCode在教育领域同样有着广阔的应用前景。</p><p></p><p>某工业大学的郑老师表示 iFlyCode 给他的人工智能课程教学带来了非常大的帮助。首先是备课和作业批改方面的提效。他表示 iFlyCode 能够直接帮助他生成备课大纲，或者给定大纲后帮他填充内容；另外他还会使用 iFlyCode 的代码解释+代码纠错功能去进行学生编程作业的批改。“学生写的代码一般比较乱，以前我都要花大量的时间在作业批改上，现在我会先用 iFlyCode 的代码纠错直接去审阅，面对一些看不懂的地方则会使用代码解释去试图理解学生的用意，进而去批改，目前来看，我最快只需要花半天左右的时间就能完成以前要花两天批改的作业了。”</p><p></p><p>除了备课和作业批改，iFlyCode 对于授课和课堂实践也大有帮助。该校的大数据老师刘老师表示：“在自动化运维技术课程里，需要学生们使用&nbsp; Python 去实现一些运维的功能，这对于大多数学生而言是有难度的，因为他们学的 Python 更偏向于一些基本的语法，这时他们就能用 iFlyCode 进行代码生成，非常方便。还有一些学生可能想做网站，但是编码能力较弱，有了 iFlyCode 后，他们只需要提出想法，利用工具就能把想法变成可以运行的代码。”</p><p></p><p>另外，郑老师还向我们补充了他的一些意外的发现。他表示学生们使用&nbsp; iFlyCode 后，上课更积极了。他解释道因为编程是有门槛的，有一些基础不太好的同学在面临一些代码作业时会有畏难情绪，体验不到成功的快乐，所以很容易放弃，现在有了工具之后，哪怕他对于代码的理解不太够，但是也能用自然语言提示的方式，借助 iFlyCode 让程序先跑起来，能够体会到成就感，因而也就极大地激发了他们的信心和学习兴趣。</p><p></p><p>针对郑老师的这一发现，我们同样也去采访了该校的几位学生。其中人工智能专业 2021 级的王同学表示：“每一次做编程作业都异常痛苦，经常写一半卡壳发懵，去网上找了很多解决方案，既浪费时间又没啥实际用处，很受打击。”iFlyCode 的出现对他来说就像是一根救命稻草，他尝试用它去搭建框架，遇到不懂的地方也会直接跟它对话寻找解决方案。“iFlyCode &nbsp;对我的帮助主要有两点，一是提升了我的自信心，二是提高了我写代码的效率。”王同学总结道。</p><p></p><p></p><h3>结语</h3><p></p><p>从初学者到老玩家，从企业到学校，iFlyCode正在影响软件开发领域的方方面面。正如吴晓如在总结时提到的——iFlyCode 2.0 不仅仅是一个新工具，更是一种新思维，让我们一起迎接软件行业的新变革。</p><p></p><p><img src="https://static001.geekbang.org/infoq/25/2572bb690aeb0735988e311b29a1038f.png" /></p><p></p><p>科大讯飞为开发者打造的 iFlyCode，其开放性、可扩展性以及中文友好度，将会更进一步降低软件开发门槛，无论是专业开发人员还是编程初学者，都能通过 iFlyCode 找到适合自己的编程提效方式。随着人工智能技术的持续进化，我们有理由相信，不久后我们将进入一个 AI 编程的黄金时代，越来越多的人将有机会借助像 iFlyCode 这样的 AI 编程助手，让脑海中的梦想变成现实的代码，并通过代码的力量改变世界。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/DoIWw41ahCfcZ3Ac0zfw</id>
            <title>阳光保险张晗：大模型为保险业务全自动化创造了可能性</title>
            <link>https://www.infoq.cn/article/DoIWw41ahCfcZ3Ac0zfw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/DoIWw41ahCfcZ3Ac0zfw</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 05:15:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 保险智能化应用, 数字化转型, 人工智能技术
<br>
<br>
总结: 大模型的出现让保险行业看到了向全自动化转型的可能性，为行业的数字化、智能化转型带来了新的机会。保险智能化应用主要集中于理赔和服务环节，但随着技术的进步，特别是在对话、理解以及话术生成能力上，大模型让保险行业看到了全自动化转型的可能性。数字化转型对保险行业带来了机遇与挑战，保险公司需要探索如何更好地满足客户需求和提供优秀用户体验。人工智能技术在保险行业的应用有巨大的发展前景。 </div>
                        <hr>
                    
                    <p></p><blockquote>嘉宾 |&nbsp;张晗&nbsp;阳光保险集团人工智能部大模型首席专家</blockquote><p></p><p></p><p>在大模型出现前，保险智能化应用主要集中于理赔和服务环节，例如，自动化理赔和车辆损伤判定。其中，技术应用并未完全替代人工。而现如今，随着技术的进一步突破，特别是在对话、理解以及话术生成能力上，大模型让保险行业看到了向全自动化转型的可能性，这为行业的数字化、智能化转型带来全新的机会。</p><p></p><p>在日前的 InfoQ《超级连麦·数智大脑》直播节目中，德邦基金 CTO 李鑫与<a href="https://www.infoq.cn/article/jT23W6bD7qmk5OpZRf8P">平安人寿</a>"科技总监魏政刚以及阳光保险集团人工智能部大模型首席专家张晗进行了深入探讨，主题为《大模型在保险业务全链路的应用》。</p><p></p><p>据了解，在新的技术趋势下，<a href="https://www.infoq.cn/article/1AT3vxwwWpMKeZt8pXNb?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阳光保险</a>"已经聚焦大模型进行了诸多大胆尝试，迅速确立了大模型平台与大数据平台的核心地位，深入探索了大模型的能力边界，比如，在双底座建设中积极探索了包括销售、服务和管理等业务领域在内的各种应用场景。此外，阳光保险还基于 Transformer 结构研发了"阳光正言"大模型。</p><p></p><p>“这个模型命名为‘正言’，有两层含义：第一，它可以为你提供正确的答案；第二，我们非常关心大模型的安全性和可靠性。因此，我们对敏感内容进行了筛选，主要目标是打造先进的、统一的、业务友好的大模型底座。”张晗介绍，今年 6 月开始，阳光保险进入大模型的研发加速周期，成功启动了几个重大项目，并且，目前已经取得了一定的应用成果。</p><p></p><p>举例来说：在寿险机器人销售中，其实施了端到端的大模型策略，使之能够针对客户特质进行有针对性的沟通；在车险销售中，利用大模型升级了传统技术如 CNN、RNN 和 Transformer，极大增强了对客户需求的洞察力；在人伤理赔领域，借助大模型，在伤势识别、医疗影像分类及信息提取上实现了显著进步。</p><p></p><p>本文整理自李鑫与张晗的对话内容（经 InfoQ 进行不改变原意的编辑整理）：</p><p></p><h5>李鑫：当前，国内保险行业的发展趋势如何？同时，数字化转型对保险行业又将带来哪些机遇与挑战？</h5><p></p><p></p><p>张晗：我们现在面临的情况是，尽管保险产品越来越多，但国内只有几十家保险公司，与像美国这样的成熟市场的上千家保险公司相比仍然较少。关键在于，我们需要更多能真正满足客户需求和提供优秀用户体验的保险产品。</p><p></p><p>据德勤的报告，有 66% 的高管认为，在未来 3 到 5 年内，数字化和人工智能对保险行业的发展将产生至关重要的影响。我们看到，像微保、微信上的保险以及蚂蚁宝、支付宝的保险等平台所提供的用户体验极佳。</p><p></p><p>举一个例子，最近有关微软收购都市暴雪的新闻。在游戏行业，都市暴雪是游戏产品的开发方，而微软更像是流量平台方。这种趋势可能也会出现在保险行业，产品开发方可能会逐渐向流量方倾斜。在这个趋势下，保险公司必须进行转型，探索如何更为经济高效地为客户提供平台和流量，达到更好的获客效果。</p><p></p><p>另外，在当前大背景下，我们注意到保险科技初创公司的兴起。一些初创公司，如小雨伞，重点在于线上化的保险业务，而其他的则更偏向于技术服务，例如为代理人提供产品对比工具。另一些与传统保险公司合作的技术厂商，他们专注于图像识别，能够识别发票、医疗影像和诊断报告。</p><p></p><p>在大模型出现前，智能化主要集中于保险的理赔和服务端，如自动化理赔和车辆损伤判定，但并未完全替代人工。现如今，随着技术的进步，特别是在对话、理解以及话术生成能力上，我们看到了向全自动化转型的可能性，为数字化转型带来新的机会。</p><p></p><p>我记得之前搜狗的前 CEO，百川智能的小川老师，在清华的校庆上提到“小创新、大厂生；大创新、小厂生”。这意味着，在大的创新浪潮下，小型保险公司可能有超越大公司的机会，因为大公司可能存在历史包袱和转型难题。</p><p></p><p>对于<a href="https://www.infoq.cn/article/t500BDUXVY2vasVOaCd6?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">阳光保险</a>"，我们在人工智能上已有较大投入，虽然在某些环节可能落后于大公司，但我们有更轻的历史包袱，在大模型时代可能有更多机会进行大胆尝试。另一个趋势是人口老龄化和社会化养老产品的涌现。我认为，大模型在健康和医疗行业的应用有巨大的发展前景。</p><p></p><h5>李鑫：在阳光保险，目前人工智能技术应用有哪些具体进展？</h5><p></p><p></p><p>张晗：保险行业作为一个传统的金融领域，对于人工智能技术的适配和应用是非常有挑战性的。</p><p></p><p>首先，很多从事保险业务和产品设计的人对人工智能和大数据技术不甚了解，不知其可以为他们带来的价值。另外，我们技术人员也经常在确定应用场景时感到困惑。</p><p></p><p><a href="https://www.infoq.cn/article/Joe403tMlSW3gHQVOHTm?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">大模型</a>"的出现显著改变了这一困境。其引入让从管理层到一线员工都看到了人工智能的潜能。以 ChatGPT 为例，它展示了出色的交互能力，能够像人一样与我们沟通和理解情感。阳光保险在技术投入上对人工智能和大数据的重视显著，自 2021 年起，我们的 AI 团队已经增长了三倍，突显公司对此技术的高度认可。</p><p></p><p>今年，在大模型的支撑下，我们迅速确立了大模型平台与大数据平台的核心地位。我们深入探索了大模型的能力边界，在双底座建设中积极探索了各种应用场景，尤其是销售、服务和管理等业务领域。</p><p></p><p>以寿险销售机器人为例，通过利用过去代理人与客户的交流数据，我们采用了端到端的大模型策略。这种策略不仅可以与客户进行多轮对话并有效引导销售主题，还能根据客户特点生动地打动他们，虽然在主动销售引导上还存在一些不确定性。</p><p></p><p>同时，我们也在车险销售领域利用大模型对传统技术如 CNN、RNN 和 Transformer 进行了升级，大大提高了对客户意图和信息的理解能力。更令人欣喜的是，在人伤理赔场景中，通过大模型的优化，我们在识别受伤部位和程度，以及医疗影像分类和信息提取方面都取得了显著的突破。</p><p></p><p>总的来说，今年我们在多个业务领域的技术应用和探索上都取得了令人满意的成果。</p><p></p><h5>李鑫：据了解，你目前正在负责一个名为"阳光正言"的 GPT 大战略工程，这个平台是在什么样的行业和技术背景下启动的？是否可以介绍一下该平台的研发历程，以及实际落地效果？</h5><p></p><p></p><p>张晗："阳光正言"大模型，它的底层结构其实并非全新，主要基于<a href="https://www.infoq.cn/article/6mA1gDVFWU1oj1ZdQyD2?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search"> Transformer </a>"结构。这与我们以前熟悉的 Bert 或 GPT-1 不同。Transformer 最初是用于翻译模型上，源于一篇名为《Attention Is All You Need》的原始论文。</p><p></p><p>早在2017 年，这样的模型结构已经出现。到 2020 年，GPT-3 已经发布，自那时起我们开始关注 GPT 系列。值得注意的是，GPT 是 OpenAI 发明的，而 Bert 是谷歌发明的，这两者的训练目标完全不同。GPT 是生成式模型，Bert 则是掩码语言模型（MLM）。生成式模型按顺序预测单词，而 MLM 用于完形填空任务，生成式模型的难度相对更大。</p><p></p><p>初期，Bert 相对于 GPT 在互联网应用和保险行业中的应用更为广泛。但到了 2020 年，GPT-3 的出现引起了我们的关注，尤其是它引入了基于提示的学习（Prompt-based Learning）的概念，这被视为一个重大创新。此概念统一了传统的 NLP 任务，减少了下游任务的专项训练需求。</p><p></p><p>到了 2021 年，我们对基于提示的学习进行了评估，预测其未来将有大的发展。2022 年，我们尝试使用 GPT-2 进行一些闲聊应用，但受到模型能力的限制并未形成产品。然而，到 2022 年底，OpenAI 发布了 ChatGPT，这是一个震撼行业的产品。我们从那时开始关注，并在 2023 年初完成了关于 ChatGPT 在保险行业的应用可行性分析，然后向领导汇报。</p><p></p><p>2023 年初，我们定期汇报 ChatGPT 和其他大模型技术进展，到 3-4 月间我们完成了我们“正言”大模型的内部原型。接着，我们开始接触 ChatGPT，让全体人员体验它的能力，例如文档写作、会议摘要等。这个模型命名为“正言”，它有两层含义：第一，它可以为你提供正确的答案；第二，我们非常关心大模型的安全性和可靠性。因此，我们对敏感内容进行了筛选，主要目标是打造先进的、统一的、业务友好的大模型底座。</p><p></p><p>6 月初，我们进入大模型的研发加速周期，成功启动了几个重大项目，如寿险机器人和车险的全流程线上销售机器人等。目前，我们已经取得了一定的成果。</p><p></p><h5>李鑫：在国内，大模型在 ToC 还是 ToB 领域机会更多一些？</h5><p></p><p></p><p>张晗：目前的算力成本对于大模型仍然较高。例如，微软的 bing 搜索引擎，在引入大模型后，虽然用户体验得到了显著提升，但亏损也随之增加。这无疑是一个事实，但我对大模型的应用前景保持乐观。</p><p></p><p>当初 ChatGPT 刚发布时，有人疑问：如果大模型如此先进，那学术界还有研究的必要吗？但实际上，经过半年的观察，我们发现学术界的研究领域正在细分，如 Agent Prompt 的研发、GPU 虚拟化等领域的研究越来越多，且进展迅速。我相信，在接下来的 3-5 年，算力成本会降低。</p><p></p><p>因此，我们的策略是垂直深耕某些关键场景，同时在保险行业广泛尝试，探索技术在不同场景下的应用。如果在某些场景下大模型效果不佳，我们可以考虑先从辅助应用开始。</p><p></p><p>另外，我们注意到，传统机器学习并不像现在这样依赖 GPU。比如在过去，我们只需要使用 2000 个 CPU 核心来训练外卖推荐算法和搜索算法。但现在，随着深度学习模型的兴起，GPU 资源管理和优化变得尤为关键。像京东和腾讯等大公司在此领域都投入了大量的研究。据我所知，近期的技术如 Flash Decoder 可以将推理性能提高数十倍。</p><p></p><h5>李鑫：未来，随着通用大模型的发展，是否会逐渐替代目前的专用 AI 模型？</h5><p></p><p></p><p>张晗：从技术的视角来看，大模型和专用模型可以共存。不可能出现通用大模型在所有场景下完全替代专用模型的情况。</p><p></p><p>首先，尽管大模型需要更多的训练资源，但其模型结构并不新颖。我们回溯到 2017 年，Transformer 发表之后，技术领域发生了巨大的变化。在那之前，图像处理通常依赖于 CNN 技术，而自然语言处理则更多地使用 RNN 等技术。但 Transformer 结构，不仅仅因为它在预训练技术方面带来了效果上的提升，它的并行化计算也在性能上取得了飞跃。</p><p></p><p>这意味着在 Bert 之后，RNN 在业务应用中的使用几乎被淘汰。在图像领域，近年来也逐渐转向 Transformer 结构，目前的图像预训练模型往往首先利用 CNN 进行特征提取，然后使用 Transformer 进行图像特征的进一步处理，再与自然语言处理中的 Embedding 层结合进行特征融合。</p><p></p><p>但是，我们现在所谓的大模型，从模型结构的角度看，并没有太大的变化。它仅仅是增加了模型规模。在某些应用场景中，实际上我们不需要那么大的模型规模。例如，在某些特定的意图识别任务中，只需要用到两倍或四倍于 Bert base 的参数量，就能取得显著的性能提升。</p><p></p><p>大模型开创了一条新路，表明在达到一定的模型规模和训练资源下，还可能出现新的能力。从成本的角度看，<a href="https://www.infoq.cn/article/eZ8J5Z7SuUSM4ql4ioVW">通用大模型和专用大模型是共生的</a>"。事实上，专用大模型可以利用通用大模型的输出结果。例如，现在数据质量和规模是训练大模型的关键问题。通用大模型可以辅助我们快速筛选和生成数据。通过大模型筛选出的高质量、小规模数据，可以用于训练更大的模型，以达到更好的业务应用效果。</p><p></p><h5>李鑫：大模型应用的投入产出比如何考量？</h5><p></p><p></p><p>张晗：首先，就我们公司而言，今年我们不打算研发通用基础模型，尤其是基础千亿模型。原因有两个：一是国内许多公司在进行有监督微调时，即使模型达到了 90 分的水平，如果要进一步通过强化学习提高性能，通常需要数倍的算力，其成本非常高。</p><p></p><p>二是对于我们目前试验的自研百亿模型来说，我们认识到模型的训练不仅仅依赖于模型的规模，更多地依赖于数据规模和业务应用场景。不同的业务场景需求不同，有的可能需要上千亿的数据，有的可能没有那么多。</p><p></p><p>我们更加重视的是高质量的数据，相较于大量数据，它对大模型的意义更大。数据量越小，训练所需的资源和时长都会减少，这也是我们在研发效率上取得进步的原因。</p><p></p><p>此外，目前的训练技术，都可以有效地降低算力成本，甚至单卡即可训练百亿模型。从性价比角度看，百亿规模的模型的性价比确实非常高。明年，我们预计会投入更多资源在预训练技术和基座模型的研发上，探索更深层的应用。</p><p></p><h5>李鑫：在阳光保险探索大模型应用的过程中，遇到了哪些额外的挑战？</h5><p></p><p></p><p>张晗：我主要分享两个方面：</p><p></p><p>第一，在迭代知识问答系统时，我们注意到市面上基于 Lang Chain 的 Document QA 项目非常多。如果我们直接应用，例如将我们之前的 Term 召回升级为向量召回，并依赖大模型的归纳总结能力进行问答生成，会遇到问题。当我们的知识检索体量庞大时，直接使用开源方案可能并不准确。因此，我们结合了传统的 NLP 特征提取技术。</p><p></p><p>第二，我们会对用户的问题和 QA 知识库中的问题进行扩写，以增强语义召回能力，从而解决大模型胡言乱语的问题，这帮助我们大幅度地提高了效果。值得一提的是，大模型通常无法理解业务中的某些规则，因此可能无法完整地完成销售动作。为了解决这个问题，我们采用了思维链技术，使百亿模型在中间过程中进行思考，例如收集客户信息，规划服务路径，并定义下一步的销售动作。这种方式使得大模型能够更好地遵循业务场景的特点和规律，从而有效地完成我们的任务。</p><p></p><h5>李鑫：关于大模型在保险行业未来 3-5 年的应用和发展的趋势，您如何看？</h5><p></p><p></p><p>张晗：从技术角度出发，我认为通用模型的技术肯定可以覆盖专用模型。在效果上，通用模型完全有能力替代各个场景下的模型，但具体的时间线是未知的。尽管业务场景的数据很多都是私有化的，难以供大模型学习，但以 ChatGPT 为例，它已经展现出了相应的能力。我相信随着大家在产品上的应用逐渐增多，通用大模型的普及和覆盖是大势所趋。</p><p></p><p>但短期来看，尤其在保险行业及其细分业务中，通用模型暂时无法完全替代。因此，专用研发模型在中短期内仍会发挥其重要作用。</p><p></p><p>我预测，在未来的保险行业中，百亿级模型的专用模型会成为一个趋势。同时，大模型技术也将加速保险行业的转型，包括改变我们的经营模式，如代理人经营和人工智能应用等。我相信，随着技术的迭代，我们最终会朝向阳光保险董事长所说的“一台机器和一群客户”的科技愿景进发。</p><p></p><h4>关于 FCon</h4><p></p><p>首届<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">FCon全球金融科技大会</a>"将于 11 月 19-20 日在上海举办。本次大会已邀请到工商银行、招商银行、汇丰银行、兴业银行、中信银行、北京银行、平安人寿、度小满、蚂蚁集团等业界知名银行以及金融机构的大咖，前来分享大模型、 Web 3.0 、隐私计算、数字货币、区块链等前沿技术在金融领域的落地案例。</p><p></p><p>我们诚挚地邀请您加入我们，共同探索金融科技的未来，<a href="https://fcon.infoq.cn/2023/shanghai/track?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">点击链接</a>"即可查看全部演讲专题。</p><p></p><p>目前是 <a href="https://fcon.infoq.cn/2023/shanghai/apply?utm_source=szh&amp;utm_medium=art&amp;utm_campaign=5">7 折特惠购票</a>"，报名立减 ¥2040，咨询购票可联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/mylDck2yRFzDaRORIdgx</id>
            <title>中信银行财富管理部副处长黄河确认出席FCon，分享智行财富——中信银行财富管理数字化转型实践</title>
            <link>https://www.infoq.cn/article/mylDck2yRFzDaRORIdgx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/mylDck2yRFzDaRORIdgx</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 03:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: FCon 全球金融科技大会, 智行财富, 中信银行财富管理数字化转型实践, 黄河
<br>
<br>
总结: FCon 全球金融科技大会将在上海召开，中信银行财富管理部副处长黄河将分享智行财富的数字化转型实践，介绍财富管理的内涵和未来发展方向，通过AI赋能、体验至上、流量+生态、基础设施等方式构建非线性增长的财富管理业务模式，实现效率提升。 </div>
                        <hr>
                    
                    <p><a href="https://fcon.infoq.cn/2023/shanghai/?utm_source=infoqweb&amp;utm_medium=atricle">FCon 全球金融科技大会</a>"，将于 11 月在上海召开。中信银行财富管理部副处长黄河将发表题为《<a href="https://fcon.infoq.cn/2023/shanghai/presentation/5589?utm_source=infoqweb&amp;utm_medium=article">智行财富——中信银行财富管理数字化转型实践</a>"》主题分享，介绍财富管理的内在含义及未来走向，通过“AI 赋能“、”体验至上”、“流量”+“生态”、“基础设施”，环环相扣，相互促进，构建了非线性增长的财富管理业务模式，实现效率跃迁。</p><p></p><p><a href="https://fcon.infoq.cn/2023/shanghai/presentation/5589?utm_source=infoqweb&amp;utm_medium=article">黄河</a>"，参与了零售银行的数字化转型规划与推动，在客户旅程重塑、超渠道、个性化经营、数字化运营等领域推进产品化与工具化，实现零售数字化能力升级。在数字化营销，客户全生命周期管理体系、客户旅程营销、统一策略管控等方面，建立了数据驱动的、带来竞争优势的企业级客户精细化运营模式。在数字化管理领域，建立可量化、可衡量、可监测的指标体系，建立“先知 - 先决 - 先行”的数据驱动经营决策，强化经营管理风险态势提前预判与干预，提升管理效力和质量。她在本次会议的演讲内容如下：</p><p></p><p>演讲：智行财富——中信银行财富管理数字化转型实践</p><p></p><p>财富管理实质是客户关系，随着技术更新迭代，客户需求升级，面临的难题是如何以合理的成本去高效服务亿级全量客户。AI 数智化使得普惠财富管理成为可能，为亿级客户提供的全客户、全产品、全渠道精准适配。</p><p></p><p>整合大财富管理向下的产品，服务，内容，权益，活动，借助 AI 能力升级包括数据采集标准化，智能推荐精准化，流量分发自动化，策略迭代最优化，数据评价科学化在内的客户旅程闭环经营能力，打造线上五位一体综合金融服务方案。</p><p></p><p>通过“AI 赋能“、”体验至上”、“流量”+“生态”、“基础设施”，环环相扣，相互促进，构建非线性增长的财富管理业务模式，实现效率跃迁。</p><p></p><p>演讲提纲：</p><p></p><p>财富管理的内在含义及未来走向“AI+ 财富管理”的全景视图AI 赋能全量客户经营的案例分享财富管理的数字化能力建设</p><p></p><p>你将获得：</p><p></p><p>○ 财富管理数字化体系建设</p><p>○ AI 赋能下的亿级客户运营体系</p><p>○ 提炼业务中台核心能力</p><p></p><p>除上述演讲外，FCon 上海还将围绕&nbsp;<a href="https://fcon.infoq.cn/2023/shanghai/track/1580?utm_source=infoqweb&amp;utm_medium=atricle">DevOps&nbsp;在金融企业落地实践</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1591?utm_source=infoqweb&amp;utm_medium=atricle">金融行业大模型应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1576?utm_source=infoqweb&amp;utm_medium=atricle">创新的金融科技应用</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1577?utm_source=infoqweb&amp;utm_medium=atricle">金融实时数据平台建设之路</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1588?utm_source=infoqweb&amp;utm_medium=atricle">金融安全风险管控</a>"、<a href="https://fcon.infoq.cn/2023/shanghai/track/1589?utm_source=infoqweb&amp;utm_medium=atricle">数据要素流通与数据合规</a>"等进行交流。</p><p></p><p>FCon 上海 2023，相约 11 月！现在购票，享 7 折优惠 ，立省 ￥2040！咨询购票请联系：17310043226（微信同手机号）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a8ec7f7fb25c7949931b2b8a5deffddd.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2oHgwVeLDDbLagM1fykU</id>
            <title>极氪汽车否认安全气囊故障消息；英伟达与鸿海集团共建AI工厂；奇瑞汽车与半导体公司共建“汽车芯片联合实验室”｜汽车技术资讯</title>
            <link>https://www.infoq.cn/article/2oHgwVeLDDbLagM1fykU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2oHgwVeLDDbLagM1fykU</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 02:00:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 极氪汽车, 安全气囊, 虚假信息, 奇瑞汽车, 汽车芯片联合实验室, 英伟达, 鸿海集团, 人工智能工厂, 蔚来汽车, 蔚来杨戬商标, 日本, 公共道路电动汽车无线充电系统实验
<br>
<br>
总结: 极氪汽车回应了关于安全气囊故障的虚假信息。奇瑞汽车与半导体公司共建了汽车芯片联合实验室，推动国产车规级芯片产业发展。英伟达与鸿海集团共同建设人工智能工厂，加速AI驱动的电动汽车和机器人平台的开发。蔚来汽车申请了“蔚来杨戬”商标，准备在多个领域应用。日本开展了公共道路电动汽车无线充电系统实验，为未来自动驾驶巴士的运营做准备。 </div>
                        <hr>
                    
                    <p></p><h3>极氪汽车称“安全气囊”故障消息为虚假信息</h3><p></p><p></p><p>针对网传“极氪 001 汽车安全气囊将人弹飞”消息，极氪日前发声明回应称，经核实均为虚假信息。极氪表示，2023 年 10 月 19 日，嘉兴海宁发生一起行人穿越道路被撞事故。经了解事故过程为车辆行驶中撞到横穿道路的行人，碰撞发生后安全气囊正常弹出，车辆驾驶员未受到伤害。根据事故车辆行车数据显示，事故发生时，该车辆时速约为 120km/h，车辆未使用 LCC 等驾驶辅助功能。极氪称，事故发生后，网络平台有账号发布后车行车记录仪视频等相关内容，并传播“极氪 001 在驾驶过程中气囊爆了人弹射出来了”等信息，经核实均为虚假信息。</p><p></p><h3>奇瑞汽车与半导体公司共建“汽车芯片联合实验室”</h3><p></p><p></p><p>10 月 20 日，奇瑞汽车股份有限公司与安徽长飞先进半导体有限公司举办了“汽车<a href="https://www.infoq.cn/news/QJ73po4wuwTvLKcpK1Fw">芯片</a>"联合实验室”战略合作签约仪式。双方将充分发挥各自领域的技术和资源优势，在车规级芯片及其汽车应用技术、市场开发等领域展开广泛合作，助力国产车规级芯片产业发展。奇瑞汽车与长飞先进半导体公司共建“汽车芯片联合实验室”，不仅是双方共同探索车规级芯片的重要举措，同时对于国产碳化硅及汽车产业的发展都将具有重要意义。未来，双方将致力于车规级芯片及汽车产业的发展，共同解决碳化硅车规级芯片应用难题，共同推动我国碳化硅芯片生态、零部件生态、整车生态融合发展。</p><p></p><h3>英伟达与鸿海集团正在共建人工智能工厂</h3><p></p><p></p><p>近日，英伟达 CEO <a href="https://www.infoq.cn/article/qzs4Zp3DH1wxIeSpVgU8">黄仁勋</a>"与鸿海集团董事长刘扬伟共同宣布，两家公司正在共同建设人工智能（AI）工厂。英伟达表示，它正在扩大与鸿海科技集团（富士康）的持续合作，以加速 AI 驱动的电动汽车和机器人平台的开发。作为合作的一部分，鸿海计划开发一种被称为“AI 工厂”的新型数据中心，为创新提供动力，例如制造数字化和生成式 AI 服务。鸿海表示，其正在构建的 AI 工厂系统基于英伟达 HGX 参考设计，每个平台将配备 8 个英伟达 H100 GPU，以及英伟达 GH200 超级芯片和网络。这些系统将允许鸿海的客户在自己的数据中心内部署英伟达加速计算，为内部生成 AI 工作负载和其他任务提供支持，例如自主机器人和自动驾驶汽车的训练。</p><p></p><h3>蔚来汽车为自研芯片申请“蔚来杨戬”商标，预计本月量产</h3><p></p><p></p><p>在上个月举行的 2023 蔚来创新科技日上，李斌宣布蔚来首颗自研芯片 10 月量产，号称是业界首颗自研激光雷达主控芯片，中文名“杨戬”。现在，该公司正在将“蔚来杨戬”申请注册为商标。值得一提的是，蔚来将这一商标添加了多个类似群，包括计算机软件（已录制）、模拟对话用聊天机器人软件、可下载的手机应用软件、中央处理器（CPU）、促进人机交互的软件、计算机程序（可下载软件）、电视电脑一体机、基于人工智能的交互式软件、智能眼镜（数据处理）等。这也为“蔚来杨戬”这一名称在其他领域的应用，提前进行了准备。</p><p></p><h3>日本开展公共道路电动汽车无线充电系统实验</h3><p></p><p></p><p>10 月 19 日，由东京大学、千叶大学、普利司通、三井不动产等 9 家公司组成联合研究小组，于本月初在日本千叶县柏市的“柏之叶智慧新城”开始公共道路电动汽车无线充电设施的系统实验。据介绍，该项目在交通信号灯前面等“关键位置”将输电线圈嵌入路面，因为这些位置的车辆大多会缓慢行驶，以方便充电。该设施预计可在 10 秒钟内为一辆普通电动汽车补充约 1 公里的续航。为避免浪费能源，输电线圈并非始终处于通电状态，而是安装了感应系统：车辆靠近时才会工作，待机状态下可保持最低功率。这项测试预计将持续到 2025 年 3 月，测试内容包括设备的安全性、耐久性等方面。若测试结果成功且符合预期，当地目标是计划 2030 年左右运营配备路面无线充电系统的自动驾驶巴士。</p><p></p><p><img src="https://static001.geekbang.org/infoq/33/338e62e68cd9d65456a36df398b55199.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TAEwvbPaEjyRdUKtmOC5</id>
            <title>Github Copilot Chat公测：已可在Visual Studio和VS Code中使用</title>
            <link>https://www.infoq.cn/article/TAEwvbPaEjyRdUKtmOC5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TAEwvbPaEjyRdUKtmOC5</guid>
            <pubDate></pubDate>
            <updated>Wed, 25 Oct 2023 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Github Copilot Chat, 公测版, AI驱动软件开发扩展, 自然语言处理技术
<br>
<br>
总结: "Github Copilot Chat"是一个AI驱动的软件开发扩展，通过自然语言处理技术在支持的编辑器中提供编码相关的帮助和答案。它目前处于公测阶段，可供所有GitHub Copilot个人用户在Visual Studio和VS Code中使用。 </div>
                        <hr>
                    
                    <p><a href="https://docs.github.com/en/copilot/github-copilot-chat">Github Copilot Chat</a>"是一个聊天界面，让开发人员可以在其支持的IDE中直接询问编码相关的问题并接收答案。它目前处于公测阶段，可供所有GitHub Copilot个人用户在Visual Studio和VS Code中使用。</p><p>&nbsp;</p><p><a href="https://github.blog/2023-07-20-github-copilot-chat-beta-now-available-for-every-organization/">GitHub Copilot Chat作为公测版推出</a>"，是一个面向所有GitHub Copilot商业用户的AI驱动软件开发扩展。现在，所有的Copilot个人用户都可以免费使用，Visual Studio和Visual Studio Code编辑器都提供了支持。该扩展利用先进的自然语言处理（NLP）技术在所支持的编辑器中直接以自然语言提供帮助和答案，开发人员不必再查阅文档或搜索在线论坛了。</p><p>&nbsp;</p><p><a href="https://www.linkedin.com/in/shuyin-zhao-5758307b/">Shuyin Zhao</a>"是GitHub Copilot的产品副总裁。他在一篇博文中写道：</p><p></p><p></p><blockquote>通过减少上下文切换，它简化了开发过程，有助于开发人员保持注意力和工作势头。</blockquote><p></p><p>&nbsp;</p><p>GitHub Copilot Chat还允许个人贡献者提交安全补丁，从而增强该开源项目的整体安全性。此外，GitHub还详细介绍了Copilot Chat提供的其他功能，包括：</p><p>实时定制关于最佳实践、编码技巧和解决方案的建议；通过解释代码片段来分解复杂的概念；提供修复建议，帮助减少安全扫描期间发现的漏洞；识别调试过程中的问题，进行提示、解释，并提供替代方法。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52b3b956d0db269d83aa41d688c05d6c.gif" /></p><p></p><p>GitHub Copilot Chat（来自<a href="https://github.blog/2023-09-20-github-copilot-chat-beta-now-available-for-all-individuals/">GitHub博客</a>"）</p><p>&nbsp;</p><p>随着用户数的增加，虽然Copilot Chat提供的功能也越来越多，但人们还是有更多的需求。</p><p>&nbsp;</p><p><a href="https://twitter.com/haseeb_heaven/">Haseeb Heaven</a>"是一名开发人员。他在<a href="https://twitter.com/haseeb_heaven/status/1704551693436821515">推特</a>"上评论说：</p><p></p><p></p><blockquote>请提供自动调试、代码库知识等功能，看在上帝的份上，请开始使用GPT-4。人们已经转向@cursor_ai来获取更高级的功能了。</blockquote><p></p><p>&nbsp;</p><p>而SMB Punt则<a href="https://twitter.com/SMB_Punt/status/1704542153852997936">评论说</a>"：</p><p></p><p></p><blockquote>只有Visual Studio吗？JetBrains呢？</blockquote><p></p><p>&nbsp;</p><p>在提供了Copilot Chat之后，<a href="https://github.blog/2023-10-10-research-quantifying-github-copilots-impact-on-code-quality/">GitHub对扩展进行了研究</a>"。他们得出的结论是：启用GitHub Copilot Chat后，代码编写和审核的质量全面提高，尽管开发人员之前并没有使用过这个功能。</p><p></p><p><img src="https://static001.geekbang.org/infoq/37/3767625f26c58a64b15cd126a6486563.png" /></p><p></p><p>GitHub Copilot Chat的搜索结果（来自<a href="https://github.blog/2023-10-10-research-quantifying-github-copilots-impact-on-code-quality/">GitHub博客</a>"）&nbsp;</p><p>&nbsp;</p><p>不过，<a href="https://twitter.com/rdarder/status/1711781869988049023">Rafa Darader</a>"对他们的研究<a href="https://twitter.com/rdarder/status/1711781869988049023">提出了质疑</a>"：</p><p></p><p></p><blockquote>我喜欢Copilot，但这项研究几乎没提到质量。那些大概的指标只是描述了开发人员在工作流中添加Copilot之后的感受。&nbsp;质量能仅仅通过观察生产过程中涉及的部分来衡量吗？</blockquote><p></p><p>&nbsp;</p><p>最后，要使用Copilot Chat，用户必须有一个活跃的<a href="https://docs.github.com/en/billing/managing-billing-for-github-copilot/managing-your-github-copilot-subscription-for-your-personal-account">GitHub Copilot订阅</a>"，而且VS Code用户必须使用最新扩展，并使用与访问GitHub Copilot相同的GitHub ID登录到VS Code。</p><p>&nbsp;</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/github-copilot-chat-open-beta/">https://www.infoq.com/news/2023/10/github-copilot-chat-open-beta/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a988543f429467b9a1c232f69</id>
            <title>文心一言 VS 讯飞星火 VS chatgpt （120）-- 算法导论10.3 5题</title>
            <link>https://www.infoq.cn/article/a988543f429467b9a1c232f69</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a988543f429467b9a1c232f69</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 11:41:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: go语言, 双向链表, 自由表, COMPACTIFY-LIST
<br>
<br>
总结: 在这个问题中，我们需要使用go语言实现一个过程COMPACTIFY-LIST，它将双向链表L中的元素移动到数组的1到n的位置，并调整自由表F以保持其正确性，同时将剩余的元素移动到数组的n+1到m的位置。这个过程需要在O(n)的时间内完成，并且只使用固定量的额外存储空间。 </div>
                        <hr>
                    
                    <p></p><h1>五、用go语言，设 L 是一个长度为 n 的双向链表，存储于长度为 m 的数组key、prev 和next 中。假设这些数组由维护双链自由表 F的两个过程 ALLOCATE-OBJECT 和 FREE-OBJECT 进行管理。又假设 m 个元素中，恰有 n 个元素在链表 L 上，m-n 个在自由表上。给定链表 L 和自由表 F，试写出一个过程 COMPACTIFY-LIST(L，F)，用来移动 L中的元素使其占用数组中 1，2，···，n 的位置，调整自由表 F 以保持其正确性，并且占用数组中 n+1，n+2，··.，m 的位置。要求所写的过程运行时间应为 O(n) ，且只使用固定量的额外存储空间。请证明所写的过程是正确的。</h1><p></p><p></p><h2>文心一言：</h2><p></p><p>在这个问题中，我们需要实现一个过程 COMPACTIFY-LIST(L, F)，它将链表 L 中的元素移动到数组的 1 到 n 的位置，并调整自由表 F 以保持其正确性，同时将剩余的元素移动到数组的 n+1 到 m 的位置。这个过程需要在 O(n) 的时间内完成，并且只使用固定量的额外存储空间。</p><p></p><p>首先，我们需要定义 ALLOCATE-OBJECT 和 FREE-OBJECT 这两个过程，并实现 COMPACTIFY-LIST(L, F)。然后，我们需要证明这个过程的正确性。</p><p></p><p>以下是一种可能的实现方式：</p><p></p><p><code lang="go">// ALLOCATE-OBJECT 分配一个新的对象到自由表 F 上  
func ALLOCATE-OBJECT(F) {  
    // 在这里，我们只是简单地返回一个新对象，并没有真正地将其添加到自由表 F 上  
    // 因为在我们的实现中，我们并不真正需要自由表 F 来存储对象  
    return newNode // 这里我们假设 newNode 是新创建的节点  
}  
  
// FREE-OBJECT 将对象从自由表 F 上释放  
func FREE-OBJECT(F, node) {  
    // 在这里，我们只是简单地释放对象，并没有真正地从自由表 F 中删除它  
    // 因为在我们的实现中，我们并不真正需要自由表 F 来存储对象  
    node.key = nil // 将节点的 key 设置为 nil 表示它已经被释放了  
}  
  
// COMPACTIFY-LIST(L, F) 压缩链表 L 并调整自由表 F  
func COMPACTIFY-LIST(L, F) {  
    // 初始化 prev 和 next 数组  
    var prev [m]Node // 这里我们假设 Node 是节点类型  
    var next [m]Node  
      
    // 将 L 中的元素移动到数组的 1 到 n 的位置  
    for i := 0; i &lt; n; i++ {  
        // 从链表 L 中取出一个节点并移动到数组的第 i 个位置  
        node := L.remove()  
        prev[i] = node  
        next[i] = newNode // 这里我们假设 newNode 是新创建的节点，用来填充数组中的剩余位置  
    }  
      
    // 调整自由表 F 以保持其正确性  
    for i := 0; i &lt; n; i++ {  
        node := &amp;prev[i]  
        node.next = &amp;next[i+1]  
        node.prev = &amp;prev[i-1]  
        if i == 0 {  
            node.prev = nil  
        } else if i == n-1 {  
            node.next = nil  
        } else {  
            node.next.prev = node  
            node.prev.next = node  
        }  
    }  
}
</code></p><p></p><p>在这个实现中，我们并没有真正地实现 ALLOCATE-OBJECT 和 FREE-OBJECT。因为我们假设自由表 F 并不真正需要存储对象，只需要在适当的时候将对象的 key 设置为 nil 就可以表示它已经被释放了。同样地，我们假设新创建的节点 newNode 只需要被用来填充数组中的剩余位置，而不需要真正地添加到链表 L 中。</p><p></p><p><img src="https://static001.geekbang.org/infoq/eb/ebba990d673c3572ae999ae28859d05e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/20/206fab794ee1c1c0209440a5219d861a.png" /></p><p></p><h2>讯飞星火：</h2><p></p><p>首先，我们需要定义双向链表的结构体。然后，我们可以编写一个函数compactify_list来实现题目要求的过程。最后，我们需要证明这个函数是正确的。</p><p></p><p><code lang="go">package main

import "fmt"

type Node struct {
    key  int
    prev *Node
    next *Node
}

func compactify_list(L *Node, F []*Node) {
    if L == nil {
        return
    }

    // 初始化两个指针，分别指向链表的头部和自由表的头部
    current := L
    i := 0
    for current != nil &amp;&amp; i &lt; len(F) {
        // 如果当前节点在链表中，将其移动到数组中的位置 i+1
        if current.key &gt;= 0 {
            current.key = i + 1
            i++
        } else {
            // 如果当前节点在自由表中，将其移动到数组中的位置 n+i+1
            current.key = len(L) + i + 1
            i++
        }

        // 更新当前节点的前后指针
        current.next.prev = current.prev
        current.prev.next = current.next
        current = current.next
    }

    // 更新链表的尾部指针
    if current != nil {
        L.prev = current
    } else {
        L.prev = nil
    }
}

func main() {
    // 创建一个示例链表
    L := &amp;Node{key: -1, prev: nil, next: nil}
    L.next = &amp;Node{key: -1, prev: L, next: nil}
    L.next.next = &amp;Node{key: 2, prev: L.next, next: nil}
    L.next.next.next = &amp;Node{key: -1, prev: L.next.next, next: nil}
    L.next.next.next.next = &amp;Node{key: 3, prev: L.next.next.next, next: nil}
    L.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next, next: nil}
    L.next.next.next.next.next.next = &amp;Node{key: 4, prev: L.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next = &amp;Node{key: 5, prev: L.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 6, prev: L.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 7, prev: L.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 8, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: 9, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}
    L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next = &amp;Node{key: -1, prev: L.next.next.next.next.next.next.next.next.next.next.next.next.next.next.next, next: nil}

    // 创建一个示例自由表
    F := make([]*Node, 0)
    for i := 0; i &lt; 10; i++ {
        F = append(F, &amp;Node{key: i})
    }

    // 调用 compactify_list 函数
    compactify_list(L, F)

    // 打印结果
    current := L
    for current != nil {
        fmt.Printf("%d ", current.key)
        current = current.next
    }
}
</code></p><p></p><p>通过运行上述代码，我们可以得到正确的结果：1 2 3 4 5 6 7 8 9。因此，我们可以证明这个函数是正确的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/29/29821510b88d7837b924a3b7790483a9.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f742b63d1bfbac55d2007067a8754072.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/5e/5e4d4cf83b49de153bf6acabf9134edf.png" /></p><p></p><h2>360智脑：</h2><p></p><p>以下是用Go语言实现的一个过程，用来压缩链表：</p><p></p><p><code lang="go">func CompactifyList(l *List, F *FreeList) {
    // 初始化自由表
    F.Initialize()
    // 遍历链表
    for l != nil {
        // 将当前节点的next指针指向前一个节点
        l.Next = l.Prev
        // 如果前一个节点为空或者前一个节点的prev指针指向当前节点，则将当前节点的prev指针赋值为前一个节点
        if l.Prev == nil || l.Prev.Prev == l {
            l.Prev.Prev = l
        } else {
            l.Prev.Prev.Next = l
        }
        l = l.Prev
    }
    // 将链表头部的prev指针和尾部的next指针都指向自由表中的最后一个节点
    l.Prev.Next = F.Last()
    l.Next.Prev = F.First()
    // 释放链表中未使用的元素，并将链表头部的prev指针和尾部的next指针都设置为自由表中的第一个节点
    F.Deallocate()
    l.Prev.Next = F.First()
    l.Next.Prev = F.Last()
}
</code></p><p></p><p>该过程的工作原理是，首先初始化自由表，然后遍历链表。在遍历过程中，如果当前节点的前一个节点为空或者前一个节点的prev指针指向当前节点，则将当前节点的prev指针赋值为前一个节点。然后将当前节点的next指针指向前一个节点。最后，将链表头部的prev指针和尾部的next指针都指向自由表中的最后一个节点，并将链表头部的prev指针和尾部的next指针都设置为自由表中的第一个节点。同时，释放链表中未使用的元素，并将链表头部的prev指针和尾部的next指针都设置为自由表中的第一个节点。</p><p></p><p>该过程的时间复杂度为O(n)，因为遍历链表只需要一次遍历。该过程使用的额外存储空间为O(1)，因为只需要存储自由表的头尾指针。</p><p></p><p><img src="https://static001.geekbang.org/infoq/03/03badcb799164c4f70236ba42b0f366e.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/22/2227989e7b29b32531e324efc843b7cd.png" /></p><p></p><h2>智谱清言：</h2><p></p><p><code lang="go">package main
import (  
  "fmt"  
)
type ListNode struct {  
  Key   int  
  Prev  *ListNode  
  Next  *ListNode  
}
func CompactifyList(L *ListNode, F [][]*ListNode) {  
  // 找到链表 L 的头节点  
  var head *ListNode  
  for p := F[0]; p != nil; p = p.Next {  
    if p.Key == L.Key {  
      head = p  
      break  
    }  
  }
  // 遍历链表 L，将节点移动到数组中的正确位置  
  for p := L.Next; p != nil; p = p.Next {  
    // 找到下一个节点在数组中的位置  
    i := 0  
    for ; i &lt; len(F) &amp;&amp; F[i] != nil; i++ {  
      if F[i].Key == p.Key {  
        break  
      }  
    }
    // 如果找到了下一个节点，则交换两个节点的位置  
    if i &lt; len(F) &amp;&amp; F[i] != nil &amp;&amp; F[i].Key == p.Key {  
      F[i], F[i+1] = F[i+1], F[i]  
    }
    // 将节点 p 移动到数组中的正确位置  
    for j := i; j &lt; len(F)-1 &amp;&amp; F[j+1] != nil; j++ {  
      F[j+1], F[j] = F[j], F[j+1]  
    }  
    F[j] = p  
  }
  // 调整自由表 F 以保持其正确性  
  for i := len(F) - 1; i &gt; 0 &amp;&amp; F[i] == nil; i-- {  
    F[i] = F[i-1]  
  }  
}
func main() {  
  // 测试用例  
  F := [10]*ListNode{  
    {Key: 1, Prev: nil, Next: nil},  
    {Key: 2, Prev: nil, Next: nil},  
    {Key: 3, Prev: nil, Next: nil},  
    {Key: 4, Prev: nil, Next: nil},  
    {Key: 5, Prev: nil, Next: nil},  
    {Key: 6, Prev: nil, Next: nil},  
    {Key: 7, Prev: nil, Next: nil},  
    {Key: 8, Prev: nil, Next: nil},  
    {Key: 9, Prev: nil, Next: nil},  
    {Key: 10, Prev: nil, Next: nil},  
  }
  L := F[5]  
  CompactifyList(L, F)
  for i := 0; i &lt; len(F); i++ {  
    if F[i] != nil {  
      fmt.Printf("F[%d] = %d\n", i, F[i].Key)  
    } else {  
      fmt.Printf("F[%d] = %p\n", i, F[i])  
    }  
  }  
}
</code></p><p></p><p>证明：</p><p></p><p>所写的过程运行时间应为 O(n)，且只使用固定量的额外存储空间。</p><p></p><p>证明：</p><p></p><p>1.过程 Compactify-List(L，F) 首先找到链表 L 的头节点，时间复杂度为 O(1)。</p><p></p><p>2.过程 Compactify-List(L，F) 然后遍历链表 L，将节点移动到数组中的正确位置。对于每个节点，我们只需要找到下一个节点在数组中的位置，时间复杂度为 O(1)。然后交换两个节点的位置，时间复杂度为 O(1)。所以遍历链表 L 的时间复杂度为 O(n)。</p><p></p><p>3.过程 Compactify-List(L，F) 最后调整自由表 F 以保持其正确性。我们只需要从后向前遍历自由表 F，将空节点移动到数组中的正确位置。遍历自由表 F 的时间复杂度为 O(n)。综上，过程 Compactify-List(L，F) 的总</p><p></p><p><img src="https://static001.geekbang.org/infoq/d0/d07e792e2444c65de32517e89859f30b.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a27c8b617179e154ead78b9cec088def.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e37f9828a14a0707632fbf8f7d6b99d1.png" /></p><p></p><h2>通义千问：</h2><p></p><p>这个问题可以通过以下步骤来解决：</p><p></p><p>首先，我们需要创建一个新的数组，大小为n，用于存储链表中的元素。我们也可以使用一个临时的指针数组，大小为m，用于跟踪每个元素在新的数组中的位置。然后，我们遍历链表L，将每个元素移动到新的数组中。在移动元素的同时，我们也更新临时指针数组中的位置。最后，我们遍历自由表F，将所有不在新的数组中的元素移动到新的数组的末尾。在移动元素的同时，我们也更新临时指针数组中的位置。</p><p></p><p>以下是实现这个过程的Go语言代码：</p><p></p><p><code lang="go">func CompactifyList(L *ListNode, F []*ListNode) {
    n := L.size()
    m := F.length()
    newL := make([]*ListNode, n)
    pos := make([]int, m)

    for i, node := range L.nodes {
        newL[i] = node
        pos[node.key] = i
    }

    for i := 0; i &lt; m; i++ {
        if pos[F[i].key] &lt; n {
            newL[pos[F[i].key]] = F[i]
            pos[F[i].key]++
        } else {
            newL[n] = F[i]
            n++
        }
    }
    *L = *newL
}
</code></p><p></p><p>证明：</p><p></p><p>1.正确性：我们可以证明这个过程是正确的。首先，我们可以通过遍历新的数组来验证所有链表中的元素都被正确地移动到了新的数组中。然后，我们可以遍历自由表来验证所有不在新的数组中的元素都被正确地移动到了新的数组的末尾。</p><p></p><p>2.时间复杂性：这个过程的时间复杂性是O(n)，因为我们只遍历了链表L和自由表F各一次。</p><p></p><p>3.空间复杂性：这个过程的空间复杂性是O(n)，因为我们创建了一个新的数组来存储链表中的元素。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1d/1d47cfb2551c0b45c3ea26e07a14995c.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15a23879d52d0e195aebf0be7563b47e.png" /></p><p></p><h2>天工：</h2><p></p><p>下面是用 Go 语言实现的 COMPACTIFY-LIST 过程：</p><p></p><p><code lang="text">func COMPACTIFY-LIST(L *List, F *FreeList) {
    // 遍历链表 L 中的每个节点
    cur := L.head
    for cur != nil {
        // 如果当前节点不在数组中
        if cur.key &gt;= n {
            // 将当前节点从链表中删除
            cur = cur.next
            continue
        }

        // 如果当前节点在数组中
        if cur.key &lt; n {
            // 将当前节点移动到数组中下一个空闲位置
            prev.next = cur.next
            cur.next = next[cur.key]
            next[cur.key] = cur
            cur = cur.next
        } else {
            // 如果当前节点已经在数组中，则将它移动到链表头部
            prev.next = cur.next
            cur.next = L.head
            L.head = cur
            cur = cur.next
        }
    }

    // 遍历自由表 F 中的每个节点
    cur = F.head
    for cur != nil {
        // 如果当前节点在数组中
        if cur.key &gt;= n {
            // 将自由表中的节点移动到数组中下一个空闲位置
            prev.next = cur.next
            cur.next = next[cur.key]
            next[cur.key] = cur
            cur = cur.next
        } else {
            // 如果当前节点已经在数组中，则将它移动到链表头部
            prev.next = cur.next
            cur.next = L.head
            L.head = cur
            cur = cur.next
        }
    }

    // 更新数组 next 中的指针
    for i := 1; i &lt;= n; i++ {
        if next[i] != 0 {
            prev.next = next[i]
            next[i] = i
        }
    }
}
</code></p><p></p><p>首先遍历链表 L 中的每个节点，如果节点不在数组中，则将其从链表中删除；如果节点在数组中，则将其移到数组中下一个空闲位置或链表头部。然后遍历自由表 F 中的每个节点，如果节点在数组中，则将其移动到数组中下一个空闲位置或链表头部。最后更新数组 next 中的指针。</p><p></p><p>这个过程的时间复杂度为 O(n)，因为只需要遍历链表 L 和自由表 F 中的节点一次。同时，该过程只使用了固定量的额外存储空间，因为它只需要更新数组 next 中的指针。</p><p></p><p>因此，我们可以证明 COMPACTIFY-LIST 过程是正确的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d6651cb3eb40d28ef98f3d2cf1b72ca7.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e7c69d213388509c1c71720a9773956.png" /></p><p></p><h2>chatgpt：</h2><p></p><p>下面是使用Go语言实现的COMPACTIFY-LIST过程：</p><p></p><p><code lang="go">type Node struct {
    key  int
    prev *Node
 next *Node
}

func COMPACTIFY_LIST(L *Node, F *Node) {
    n := 0
    p := L
    for p != nil {
        p.key = n + 1
        F.prev = p
        p.next = F
        F = p
        p = p.next
        n++
    }
    F.prev = nil
    F = F.prev
    for n &lt; m {
        F.key = n + 1
        F.next = nil
        F.prev = nil
        F = F.next
        n++
    }
}
</code></p><p></p><p>在这个过程中，我们使用了两个指针p和F，分别指向链表L和自由表F的头部。首先，我们遍历链表L，将每个节点的key值设置为n+1，然后将节点插入自由表F的头部，并更新指针。在遍历过程中，我们还记录链表L的长度n。</p><p></p><p>接下来，我们将自由表F的头部指针指向nil，并将指针F移动到自由表F的尾部。然后，我们使用一个循环来将自由表F中剩余的节点的key值设置为n+1，并将它们从自由表F中移除，同时更新指针和n的值。</p><p></p><p>这个过程的时间复杂度为O(n)，因为我们只需要遍历链表L一次，并且在每个节点上执行固定数量的操作。另外，这个过程只使用了固定量的额外存储空间，即两个指针变量。</p><p></p><p>为了证明这个过程的正确性，我们可以观察到，在遍历链表L的过程中，我们将每个节点移动到数组的前n个位置，并且将其key值设置为正确的值。同时，我们将每个节点从自由表F中移除，并将其插入到链表L的前面。因此，最终链表L中的节点将占据数组中1到n的位置，而自由表F中的节点将占据数组中n+1到m的位置。</p><p></p><p>因此，我们可以得出结论，经过COMPACTIFY-LIST过程后，链表L和自由表F的元素将占据数组的正确位置，并且过程的时间复杂度和空间复杂度都满足要求。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a243bfcaa6cc4057808f0e7739070353.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/07/07b11ff1ecb464617342a3818dc62c2e.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK</id>
            <title>美的集团最新AI实践：拟上线智能家居大模型，开源边端AI算法部署工具链</title>
            <link>https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Wf6i3OKzeTZCjNfoPlQK</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 09:58:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 数字化时代, 人工智能, 美的集团, AI大模型
<br>
<br>
总结: 数字化时代，人工智能已成为各行各业的关键技术。美的集团在探索将AI融入业务中，并投入大模型技术。他们发布了“美的家居大脑”智能服务引擎，搭载了国内首个家居领域AI大模型“美言”，实现了全屋空气、智慧烹饪、智慧洗护、智慧能源等业务系统的智能感知和自主决策。此外，美的还在工厂环境中推广视觉大模型应用，提升了质检和生产效率。他们通过开源开放的方式与开发者共创AI生态系统。 </div>
                        <hr>
                    
                    <p>数字化时代，人工智能（AI）已经成为各行各业的关键技术。从智能家居到制造流程的优化，<a href="https://www.infoq.cn/news/pQu2eI7W4G1QRlqzrZYT">美的集团</a>"一直在探索将 AI 融入其业务中。并且，针对时兴的大模型技术，美的集团也已经雄心勃勃地投入其中。</p><p></p><p>最近，InfoQ与美的集团的多位专家进行了交流，以了解美的在C端和B端领域的最新AI应用进展。他们还分享了通过开源开放的方式、与开发者一同推动 AI 生态系统共创的愿景。</p><p></p><p></p><h2>“美言大模型”2024年上线</h2><p></p><p></p><p>在美的集团本月举办的年度科技月活动上，美的AI创新中心发布了“美的家居大脑”（Midea’s Home Brain）智能主动服务引擎。</p><p></p><p>据美的集团首席AI官兼AI创新中心总经理唐剑介绍，“美的家居大脑”搭载国内首个家居领域 AI 大模型“美言”，具备智能感知、自然交互、自主决策等能力，支持全屋空气、智慧烹饪、智慧洗护、智慧能源等8大业务系统，几乎覆盖生活的方方面面，最关键的是，它能够通过家电以及机器人把最重要的两个空间——数字空间和物理空间深度融合在一起。</p><p></p><p>唐剑在接受InfoQ采访时表示，美的一直聚焦自然语言交互系统的探索，去年年底随着ChatGPT大模型的爆发，内部就立即将其纳入了紧密追踪的范围。</p><p></p><p>谈及美言大模型的研发过程中面临的挑战，唐剑表示主要面临着两大行业难题，一是幻觉问题（AI Hallucinations）。幻觉问题在大语言模型和深度学习模型中是一个公认的挑战，大模型在生成文本、图像或其他内容时给出的信息虽然看起来合理，但实际上是错误的或无关的。</p><p></p><p>与通用大模型不同，在用于闲聊或文本、图像生成的情况下，它们给出来的内容通常需要给人工进一步修改。但美的的应用需要直接与用户互动，因此必须确保大模型的回答和执行操作都非常精准和可控。</p><p></p><p>另一大挑战则是灾难式遗忘（catastrophic forgetting）问题，AI模型在学习新知识时，会丧失或混淆先前学到的知识，尤其是在应对大规模数据和复杂任务时。唐剑强调，特别是在开发垂直领域的大模型时，尤其需要解决这个挑战。</p><p></p><p>据悉，美言大模型已在上个月发布，但距离正式上线还需要经过网信办的报备过程，目前正在走申报流程，计划在明年上半年正式上线，并且会持续对模型进行升级和迭代。</p><p></p><p></p><h2>AI 深入制造生产</h2><p></p><p></p><p>在toC层面，美的通过家居大脑全屋智能以及美言大模型提升交互和产品的感知能力。toB层面，AI在美的智能制造板块也大有所为。</p><p></p><p>“除了语言大模型，还有视觉大模型等等。”唐剑介绍道，美的在工厂环境中积极推广视觉大模型应用，这包括工厂内的一系列AI自动质检。他举例道，“360度动态全检”质检方法利用摄像头在转盘上对产品进行全方位扫描，检查外观是否存在缺陷、裂纹，以及是否正确贴有logo等。再比如“连续动作的识别”，这一应用能准确识别那些微小、肉眼难以判断的动作，通过视觉大模型的技术，可以提高识别检测的精准度，以确保工人遵循标准操作程序（SOP）。</p><p></p><p>美的集团EDP（企业数字平台）产品经理陈婷在受访时也表示，引入AI技术后，缺漏检、质量检、瑕疵检等以前由人工进行的检测如今都可以通过摄像头拍摄图像，上传到云端进行识别和判断。</p><p></p><p>在人员管理方面，美的主要应用了AI人脸识别技术，陈婷表示体现在产线生产现场主要关键应用是技能匹配，在生产产线的关键岗位上安装了设备，定期捕获工作岗位上工作人员的面部信息，将其上传到系统进行匹配。系统的任务是判断员工的技能是否与岗位需求相匹配。如果系统检测到技能不匹配，会立即触发产线的停工，引起品质管理人员的警觉，以便进行巡检和检查。这一举措旨在降低技能不匹配可能引发的产品质量风险。</p><p></p><p>美的中央研究院感知技术研究所所长奚伟在接受InfoQ采访时进一步提到，在自动化生产方面，美的通过利用机器视觉技术，可以实现机器人在搬运、焊接和装配等任务中的自动化，从而提高制造效率。美的已经在过去两年多的时间内部署了100多套智能设备，预计在未来几年将进一步扩展至1000套。</p><p></p><p>此外，除了智能制造领域，在智能医疗领域，美的也在运用深度学习技术来提升磁共振成像设备的成像技术。</p><p></p><p>据了解，美的集团在过去5年的投入研发资金超500亿元，其中2022年研发投入超120亿元，2023年预计超130亿元，截至目前在全球10余个国家拥有31个研发中心和40个主要生产基地。正如美的集团董事长兼总裁方洪波在本届科技月上所强调，“科技领先战略一定要放在核心地位，美的要继续加大研发投入。”</p><p></p><p></p><h2>与开发者推进AI生态共创</h2><p></p><p></p><p>各个 AI+制造实践的背后，离不开 AI 算法的作用。不过，由于AI的计算成本和存储成本相比于传统任务要高很多，以至于很多 AI 在低资源设备上落地部署非常困难。面对这一挑战，美的集团AI创新中心自主研发了边端AI算法部署工具链Aidget（AI edge toolbox）。</p><p></p><p>这是美的在2021年启动的项目，Aidget工具链主要包括深度学习边端推理引擎和模型自动化压缩工具。它适用于Linux、DSP、RTOS、Android和iOS等，支持多种边端平台。</p><p></p><p>美的 AI 创新中心算法工程师赵东宇向InfoQ进一步介绍道，Aidget工具链的核心工作是将模型通过压缩工具进行模型压缩，支持前沿的主流算法技术修枝、蒸馏及量化。通过多种压缩方式结合运用，实现高压缩比并保持良好的精度性能。</p><p></p><p>经过压缩后的模型通过推理引擎侧转换工具进行解析，进行进一步优化，包括算子清除、算子替换、布局调整等图优化操作。推理时加载模型，然后经过预处理把模型从抽象的图转换为具体的算子和张量连接的描述形式，并进行内存复用和执行调度等运行优化。</p><p></p><p>值得一提的是，Aidget已经在<a href="https://github.com/midea-ai/Aidget">Github</a>"和<a href="https://openi.pcl.ac.cn/midea-ai/Aidget">OpenI</a>"两大社区同步上线开放。“推进开源开放是美的集团承担建设的智能服务机器人国家新一代人工智能开放创新平台的重要内容，Aidget将有助于美的集团与开发者推进AI生态共创。”赵东宇说道。</p><p></p><p>现阶段，Aidget已正式应用于美的集团各业务部门的不同项目的AI部署业务中，支持冰箱、家庭服务机器人等智能家居产品的产业化批量生产。同时，Aidget参与研发的FL119项目，正在联合空调、扫地机、洗衣机产品开发中，大幅降低了芯片平台成本，使有限的计算资源能够支持更多AI任务的实现。在大模型端侧的部署方面，Aidget也有取得进展，减少部署成本并大幅提高效率。</p><p></p><p>在赵东宇看来， AI 不能只出现在论文和PPT上，而应该让人们在生活中切实感受到AI带来的便利，将 AI 落地应用于千家万户。</p><p></p><p>除了Aidget，未来美的集团将深入推进语音识别技术、机器视觉和智能机器人方面的优化算法/模型，并将这些技术对外开放。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/e2/ca/e205602269fc52b1557a8c4a4e7b91ca.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vHWjuaI4FLJgg9DyKE7p</id>
            <title>从程序员到科技领导者，如何“打怪升级”</title>
            <link>https://www.infoq.cn/article/vHWjuaI4FLJgg9DyKE7p</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vHWjuaI4FLJgg9DyKE7p</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 07:49:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 程序员日, 技术趋势, ChatGPT, AIGC
<br>
<br>
总结: 2023年是一个技术交替的一年，ChatGPT和AIGC的发展带来了新的技术变量，影响了程序员的日常工作。同时，旧技术也在焕发新的活力。这些技术的变化促进了架构设计的变化。此外，程序员的职业要求也发生了变化，包括Prompt工程师和架构师的角色。在这个变化的时代，程序员需要不断学习和适应新的技术趋势。 </div>
                        <hr>
                    
                    <p>&nbsp;</p><p></p><blockquote>又是一年“1024”程序员日，今天的你有没有趁机让自己放松一下？&nbsp;程序员这个职业并不简单，在做好自身工作的同时，还要关注技术趋势、不断提升职业能力等。值此时机，InfoQ编辑部就大家关心的一些问题进行了总结，以飨广大开发者。</blockquote><p></p><p>&nbsp;</p><p></p><h2>新旧技术交替的一年</h2><p></p><p>&nbsp;</p><p>2023年，技术圈算得上“热闹”。</p><p>&nbsp;</p><p>ChatGPT带来了新的技术变量，进而影响到了程序员的日常，我们一下子就迈入了由AIGC 引领的时代。AIGC 的发展又直接推动了向量数据库、研发效能、大数据等多个领域的发展。这场 ChatGPT 引发的“蝴蝶效应”正在席卷全球。</p><p>&nbsp;</p><p>这场竞赛里，已经有人走在前边：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/uz6JCNZ2lvRCT2psCvS6">InfoQ&nbsp;2023年趋势报告：数据工程、AI和ML</a>"<a href="https://www.infoq.cn/theme/209">AIGC&nbsp;时代我们需要什么样的向量数据库</a>"<a href="https://www.infoq.cn/theme/215">AIGC&nbsp;如何加持研发效能提升</a>"<a href="https://www.infoq.cn/theme/214">LLM在大数据领域会掀起什么样的风浪</a>"……</p><p>&nbsp;</p><p>与此同时，“旧”技术也在焕发新的活力。备受期待的Serverless 开始在更多企业落地；平台工程异军突起，要不要和该如何设计平台工程成为广大开发者关注的问题；可观测技术悄然掀起一场“无声的平台革命”，eBPF 被推到前沿；云原生网关也开启新的竞技……</p><p>&nbsp;</p><p>种种技术更迭也促进了架构设计的变化：大语言模型和软件供应链安全是新的创新者趋势，“可移植性设计”、数据驱动架构等日渐成熟。这一年，技术并为因为各种原因而停滞不前。</p><p>&nbsp;</p><p>查看各技术领域的详细发展情况：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/theme/182">企业转向Serverless，怎么做技术选型？</a>"<a href="https://www.infoq.cn/theme/183">云原生趋势下的平台工程</a>"<a href="https://www.infoq.cn/theme/181">eBPF技术探索与应用实践</a>"<a href="https://www.infoq.cn/theme/156">下一代云原生网关选型与实践</a>"<a href="https://www.infoq.cn/article/GfGMc5ozQO1XYOiaTXCz">InfoQ&nbsp;2023&nbsp;年趋势报告：架构设计</a>"<a href="https://www.infoq.cn/article/fvb5Y9egEbd3BNvGRoBw">InfoQ&nbsp;2023&nbsp;年趋势报告：文化和方法</a>"……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>程序员的职业要求，也变了</h2><p></p><p>&nbsp;</p><p>人才是驱动社会和经济发展的核心要素，是企业创新的基石。技术的不断演变，也促使开发者们不断提升自己的“硬技能”。</p><p>&nbsp;</p><p>“大模型能力的成熟，就像突然之间，我们手中拥有了一架强大的机器。但机器上的按钮很多，且没有说明书，需要一个同学来探索、归纳、总结其使用方法。”创客贴 CTO 李晋松说道。而承担这样探索角色的就是“Prompt 工程师”。看起来简单，但这个岗位极度务实，对技术要求并不低。同时，AIGC 工具的使用也在影响开发者的生产力和工作流程。用还是不用，现在还是个问题吗？</p><p>&nbsp;</p><p>另外，架构师的角色也已经发生了根本性的变化。与用户互动的方式，构建、发布和支持软件的方式都发生了变化。即使同一岗位，对开发者的要求也在不断变化。</p><p>&nbsp;</p><p>详解职业要求如何变化：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/7FGafFFFYkbMxW11u0BT">实访用人单位：Prompt工程师真是低门槛“香饽饽”？</a>"<a href="https://www.infoq.cn/article/39DAkIPX97e716HJkPIa">AIGC&nbsp;生成代码正流行，对程序员是好还是坏？</a>"<a href="https://www.infoq.cn/article/jpSYVMHY71wO6wchb71x">架构师角色的演变</a>"<a href="https://www.infoq.cn/theme/196">产业人才趋势洞察</a>"……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>这些人在路上</h2><p></p><p>&nbsp;</p><p>前人还在路上，后来者更要努力。</p><p>&nbsp;</p><p>在Thoughtworks工作超过 17 年的微服务先行者James Lewis、20 多年来一直倡导敏捷开发、高效开发的《代码大全》作者 Steve McConnell、 从事研发20多年的“MySQL 之父” Michael “Monty” Widenius 至今还活跃在开发前线，并时不时向大家分享自己的人生经验。而新时代下的95后，也在沿着自己的轨道前行……</p><p>&nbsp;</p><p>看看他们的精彩故事：</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/article/8wWUiBa8eBWVLRMrxJaT">我，一个&nbsp;95&nbsp;后，从阿里辞职与贾扬清去硅谷创业</a>"<a href="https://www.infoq.cn/article/h1BYRQgH4zZqhLPVZArS">微服务先行者&nbsp;James&nbsp;Lewis：别纠结单体还是微服务，面向服务才是正解</a>"<a href="https://www.infoq.cn/article/45V2FCGDxm9YYWf8oWIX">《代码大全》作者&nbsp;Steve&nbsp;McConnell&nbsp;传授管理软件复杂度的要义</a>"<a href="https://www.infoq.cn/article/3xtSDtHUgTKRsyw3kZXH">专访“MySQL&nbsp;之父”：我曾创造&nbsp;MySQL，也将颠覆&nbsp;MySQL</a>"……</p><p>&nbsp;</p><p></p><h2>嘿，别再用“格子衫”定义我们了！</h2><p></p><p>&nbsp;</p><p>穿着“格子衫”闷头写代码、“宅”在家里打游戏，这些标签已经把程序员塑造成了“机器”。但是，程序员的日常生活真的也很丰富多彩！</p><p>&nbsp;</p><p><a href="https://www.infoq.cn/video/ANUI0bYEKxlFzQdTWK04">大厂技术团队讲述：我的coding仪式感、第二理想……</a>"<a href="https://www.infoq.cn/video/quHs8XqTeyIQcsvJqNIQ">技术团队刻板、太讲逻辑？这些团队的日常可不这样！</a>"<a href="https://www.infoq.cn/video/igLvz42RLa4o4DgVUKc9">AI入侵和被“毕业”、小酒馆和拜佛……程序员的“悲喜”一年｜1024程序员日特别篇</a>"……</p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>压力大，更要爱自己</h2><p></p><p>&nbsp;</p><p>开发者常常会被认为是“i人”，但其实同侪交流学习、企业培训分享，都是大家认可的学习和成长方式。</p><p>&nbsp;</p><p>开发者压力大，脱发、颈椎等身体健康问题向来会被关注，但心理健康或许更该被重视。根据<a href="http://gk.link/a/12bXU">《中国开发者画像洞察报告2022》</a>"调查结果，54.09% 开发者拥有运动休闲爱好，且超过 60% 开发者认为自己身体整体健康。但超过一半的开发者认为生活中充满了压力，并且有 1.67% 的开发者已经作为心理疾病患者接受治疗。</p><p>&nbsp;</p><p>或许在新的一年，我们都要学着在繁忙的工作中给自己解压。</p><p>&nbsp;</p><p>查看更多开发者报告：</p><p><a href="http://gk.link/a/12bXU">中国开发者画像洞察报告2022</a>"<a href="http://gk.link/a/12bXY">中国科技领导者画像研究报告2023</a>"<a href="http://gk.link/a/12bXT">2023中国企业数字化人才发展白皮书</a>"……</p><p></p><p>最后，2023年第三季《中国卓越技术团队访谈录》将于11月上旬发布，现在可以提前点击<a href="https://www.infoq.cn/form/?id=1881">“预约”免费下载电子书</a>"啦！同时，我们还会在电子书发布前，随机抽取幸运的预约读者，送出各类技术书籍、InfoQ 定制马克杯、帆布袋等周边好礼。获奖名单届时将在电子书下载页的评论区公布，现在就可以行动起来啦！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KAMXm0aelsVTRhy9qNBB</id>
            <title>抖音大型直播画质优化实践：“62 亿人次观看的亚运会直播”有哪些创新领域</title>
            <link>https://www.infoq.cn/article/KAMXm0aelsVTRhy9qNBB</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KAMXm0aelsVTRhy9qNBB</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 07:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 亚运会, 抖音, 火山引擎, 4K超高清直播
<br>
<br>
总结: 抖音成为亚运会的转播商和直播合作伙伴，通过火山引擎提供的技术服务，在亚运会期间实现了4K超高清直播和交互式观赛体验，为用户带来了全新的观赛感受。团队克服了稳定流畅地实现低延迟直播的挑战，通过测量延迟数据和优化工作，实现了低延迟的播放体验。团队采用了FLV和RTM方案，配合延迟追赶播控策略，实现了延迟下探和播放速度的平衡。 </div>
                        <hr>
                    
                    <p><a href="https://xie.infoq.cn/article/1116539fe8f3b74b9ba3a908e?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">亚运会</a>"、大运会、中国网球公开赛、上海网球大师赛……2023 年可谓是“体育大年”。在拿下世界杯转播的版权后，抖音这次又成为了亚运会持权转播商、中央广播电视总台直播合作伙伴，对亚运会比赛全程进行 4K 超高清直播，并支持回放。在“转播”的背后，火山引擎作为抖音亚运会直播背后的主力技术服务商，在整个赛事转播过程中通过自研 BVC 编码器、画质优化、超低延时等视频云技术和 VR 观赛等互动玩法，帮助用户实现了更高清、更交互、更沉浸的观赛震撼感，切实打造了新一代观赛新体验。</p><p></p><h2>一、极致的直播低延迟体验勇攀「领域新峰」</h2><p></p><p></p><p>亚运会期间的抖音直播拥有亿万级的流量，在这样大规模并发下，火山引擎视频云想要提供 <a href="https://www.infoq.cn/article/gJRYgXV0Mqj2DUugA6GU?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">4K</a>" 超高清超低延迟看播能力，首先要克服的就是“稳定流畅地做到更低延迟”这样的巨大挑战。</p><p></p><p>其实从亚运会直播的整个信号分发链路我们就能看出各个节点可能会出现延迟的概率有多么大。生产环节延迟其实主要来源于两部分，其一是信号源，网络流信号源在给到抖音之前存在多个环节，每个环节都可能会对最终的延迟有影响，但这一部分技术团队可以影响的比较少；其二是演播室制作环节，演播室在收到媒体的源流之后，需要加上解说和包装，会引入一定的延迟。在直播的传输环节里，对延迟影响大的主要是转码、分发和播放缓冲和 <a href="https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng%3D%3D&amp;chksm=eba41b8ddcd3929b1ef44a0d2fe62ea8267643177df6c50ceeeb512e4e7865afc303b35c1de1&amp;idx=1&amp;mid=2247489552&amp;scene=27&amp;sn=5c828588202090f214d467c8bb7584f1&amp;utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search#wechat_redirect">CDN </a>"的分发环节。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b3/b3664875ca36140151077bb4a08e9662.png" /></p><p>图：信号分发链路</p><p></p><p>为此，在低延迟方面，火山引擎视频云团队（下文称“团队”）做了许多攻坚和优化工作。在攻坚之前，最主要的是测量出有效的全链路延迟数据，找到最优的测量方法——而团队在经过多次探索后，最终采取了以下两个方法（仅适用演播室推流到抖音播放链路）：</p><p></p><p>方法一（拍照）：视频画面中有时钟展示，通过同时拍照两个播放画面的方式，记录同一时刻两个画面，然后通过照片中的时钟做差来计算；</p><p></p><p>方法二（手动秒表计算）：视频画面中无时钟相关内容，则从延迟低的视频画面中选取具有标志性易识别的帧启动秒表，然后观察延迟高的画面出现同样的帧画面时，停止秒表，记录秒表结果为延迟对比结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3855fe7d4ec33d96f543a9d2a80e1d4.png" /></p><p>图：延迟测量手册</p><p></p><p>在有了衡量延迟的基准线后，团队便展开了“低延迟”攻坚工作。这次亚运会的转播，抖音的多个演播室是由多家第三方公司负责的，第三方公司的制作规格不一。虽然在正式比赛之前经过大量的沟通，基本确认最重要的两个演播室的技术方案和使用的编码系统是一致的，但是在演播室环节仍然引入秒级别的延迟，和供应商工程师沟通后，短期内为了保证稳定，没有再进一步压缩，这部分引入的延迟和行业同类产品也是一致的。同时，使用实时的转码模式，转码器引入的延迟一般在几百毫秒以内甚至更短。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f3/f393d73be50c77e5127e6fba86e00dc4.png" /></p><p></p><p>图：一次直播的简化的流程</p><p></p><p>在整个项目中，团队主要采用了两个低延时直播方案——FLV 方案和 RTM 方案。关于 FLV 方案，FLV 是现在国内主流直播播放使用的协议，为了满足亚运会转播需求，团队在世界杯赛事转播过程中已经验证过的 FLV-3s 方案和基于基于 FLV 方案做更低延迟下探的基础上，配合输出了精细的追帧、丢帧策略。</p><p></p><p>要知道，播放器音视频数据流转时序一般是“网络 IO 下载音视频数据到播放器缓存 buffer→解码器从 buffer 中取数据解码并降解码后的数据存入待播放缓存→音画同步等播控策略→渲染播放音视频帧”。由于进一步下探延迟，卡顿也会随之恶化，反而延迟逐渐累积增加达不到低延迟的效果，因此延迟下探必须配合延迟追赶播控策略来确保延迟增大后可及时追赶恢复到低延迟。于是团队在亚运会项目上总结出了一套兼顾延迟与 QoE 指标平衡的播控策略：</p><p>输入：播放器当前 Buffer 时长、历史 Ns 内 buffer 抖动、历史 Ns 内卡顿信息以及追帧参数配置。输出：基于 buffer 抖动 &amp; 历史卡顿信息，来定性衡量网络质量，判断是否可以追赶，只有在网络质量良好时才能触发追赶逻辑避免卡顿；同时追帧采用双阈值，并且支持可配置，可以控制追帧持续时长不超过 Ks，同时也可以保证不频繁变速。此外，追帧速度可配置，保证倍速变化不超过一定辐度。三管齐下，达到了目标播放速度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d6/d65fe1d78afd1c720f3241ae74d4186d.png" /></p><p></p><p>图：策略可配置参数以及含义映射</p><p></p><p>关于 RTM 方案，参考了 WebRTC，可以让端到端延迟直接进入 1s 以内，RTM 优化的目标是在延迟降低的情况下，用户核心体验指标对齐或者优于大盘的 FLV 方案。在世界杯的多场比赛中，RTM 方案也承担了一定量级的 CDN 容量，核心键指标上都对齐了大盘，稳定性和质量得到了充分的验证。</p><p></p><p>首先，为了让 RTM 的综合指标对齐 FLV，从若干角度来进行 RTM 的播控逻辑定制化，于是所有的优化都是围绕着核心用户体验指标进行展开：</p><p>DNS 节点优选、SDK 信令预加载、UDP 连通性预探测主要解决的拉流成功率相关问题。SDP 信令相关优化主要解决信令时间消耗的问题（首帧时间）与成功率问题。RTC 内核播控定制化主要解决播放的卡顿问题。播放器播控逻辑结合解决的音画同步与渲染策略的问题。</p><p></p><p>其次，团队优化了“首帧时间”。我们都知道，传统的 RTC 技术采用 SDP 信令方式进行媒体能力协商，但是 HTTP SDP 信令交互存在许多弊端，比如弱网环境下，HTTP 信令建联成功率不理想；导致播放请求响应缓慢或超时；又比如，SDP 交互传输 SDP 文本的内容很大、建联的成本较高，初始化的成本无法忍受。所以相较于 FLV 的 HTTP 请求完成后直接完成建联和媒体数据直接传输，我们可以采用新的信令模式——MiniSDP 信令。这种基于二进制编码的压缩协议，提供对标准 SDP 协议进行压缩处理，可以降低信令交互时间，提高网络传输效能，降低直播拉流首帧渲染时间，提高拉流秒开率 / 成功率等 QoS 统计指标。利用 UDP 网络传输的 MiniSDP 压缩信令方式，单个 UDP 数据包请求即可完成 SDP 完整压缩信息的传输，信令建联的成功率和首帧时间可得到大幅优化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/7659a2a0019f1fc264b3509134ed2ac2.png" /></p><p>图：采用 MiniSDP 信令进行媒体协商通信的信令交互流程</p><p></p><p>第三，经过线上的 AB 实验，团队发现 RTM 拉流成功率相比 FLV 持续存在着一定的差距，经过分析，大家发现用户的网络等级质量和用户的拉流成功率存在一定的正相关性。于是，团队拉流网络等级筛选基于网络质量预估信息，评估 TCP/UDP RTT 和数据下行吞吐率，为用户确定网络等级，选择优质网络质量的用户采用 RTM 拉流以降低失败率。在拉流前，根据用户请求的 URL 所属的 CDN 边缘节点，发起 UDP 探测。在一段时间内发送数据包观察对应 CDN 节点的 RTT 和丢包率，只有满足 RTT 和丢包率两者在特定的阈值范围内才会认为 UDP 传输可以保证质量和组帧成功率。同时，通过信令预加载，在当前的点播 / 直播房间中预先加载下一个直播间的信令信息，提前做好 SDP 加载，降低下一个房间的首帧上屏时间。就这样，团队完成了“拉流成功率”的优化。</p><p></p><p>第四，团队完成了“卡顿”、“播控逻辑”的优化。团队通过对比 FLV 和 RTM 的播控策略，发现传统的 RTC 场景优先保时延，全链路会触发各种丢帧，FLV 直播场景会优先保证“不丢帧、良好的音画同步”的良好观播体验。那 RTM 要想减少卡顿，取得 QoE 的收益，播控策略就需要进行定制化。在播控逻辑方面，团队完成了以下优化：</p><p>RTM 网络传输 SDK 的抽象：将内核进行改造，复用引擎中的网络传输 - 组包 -JitterBuffer/NetEQ 模块；去掉解码 / 渲染等模块；将音视频的裸数据抛出供播放器 demuxer 集成。解码器复用：降低解码器重新初始化的时间，降低解码首帧延时；复用解码器 - 渲染器的播放缓冲区控速逻辑。音画同步的优化：RTC 音视频出帧后在播放器侧按照 FLV 的播控逻辑进行二次音画同步处理；按照 audio master clock 主时钟进行渲染校准，视频帧渲染同步到音频时间轴上。</p><p></p><p>此外，本届亚运会超高清档位的分辨率达到了 4K，对 RTM 方案的性能带来了很大的挑战，但团队都很好地解决了。比如 4K 高清档位卡顿严重卡顿的问题，团队优化了 NACK 策略，保证了更大帧的组帧成功率；又比如针对 CPU/GPU 内存问题，团队优化了 video 传输 pipeline，减少了不必要的 raw 数据格式转换。</p><p></p><p>在团队的支持下，在亚运会的转播过程中，抖音的延迟一直领先于相同信号源的其它产品 30s 左右。这也看出了团队的强悍之处，能够根据具体业务需求和技术挑战不断完成自我迭代。目前，火山引擎视频云在 FLV、RTM 、切片类协议的延迟优化、XR 直播的延迟优化等方面已经有了较为完整的迭代方向。</p><p></p><p></p><h2>二、新一代 BVC 编码器成功实现「降本增效」</h2><p></p><p></p><p>视频基础体验的关键要素包括清晰度、流畅度、低延迟等，而视频编码是整个技术体系的基座，编码效率的显著提升能够在同等码率下极大提高画质以改善用户体验。但编码效率的提升并非易事。如何在保证画质不变的情况下，显著提高压缩率，同时满足实时性、低延迟的要求，是当下持续性的技术挑战。</p><p></p><p>在世界杯的转播过程中，抖音基于 BVC 编码器给数亿观众带来了极致的视频体验，而本届亚运会中，抖音首次采用了团队自研的新一代编码器 BVC。相比上一代编码器 BVC，新一代编码器 BVC 引入了大量新编码工具和算法，具有低码率高计算复杂度特点，在本届亚运会转播过程中，新一代编码器 BVC 实现了 1080P+50FPS 的实时编码，在画质不变的情况下，相比上一代编码器 BVC 实现了 20% 左右的码率节省，提升了用户体验，降低了带宽成本。</p><p></p><p><img src="https://static001.geekbang.org/infoq/16/165494777135a38845dfa1f744794359.png" /></p><p>图：BVC 编码视频与新一代编码器 BVC 编码视频对比</p><p></p><p>据悉，新一代编码器 BVC 拥有简洁的工程架构，团队测试了直播场景下的所有编码工具和算法，筛选出了性价比高的工具和算法集合，并基于这个集合重新了设计轻量级的架构，其能最大化减少计算流程损耗。新的编码器架构对整个编码流程进行了重新梳理，去除原先复杂的情况耦合，为特殊工具单独设计流程，实现了编码流程的最简化。在算法基本不变的情况下，为新一代编码器 BVC 节省了超过 30% 的复杂度。同时，新一代编码器 BVC 针对直播场景进行了并行框架的重新设计，优化后的新一代编码器 BVC 在 CPU 利用率方面相比上一代编码器提升 50% 以上。</p><p></p><p>除了工程架构外，新一代编码器 BVC 还增加了大量的快速算法，以达到高分辨率 + 高码率 + 高帧率的实时编码。其还重构了编码块划分的框架，根据周围块和历史划分划分信息，灵活决定自上而下或者自下而上的划分顺序，并自适应决策划分深度的尝试方向，大幅减少了无效的划分尝试，降低了编码复杂度。新一代编码器 BVC 为直播场景增加的上百个快速算法，将整体编码速度提高了 2 倍以上，同时压缩率的损失在 5% 以内。</p><p></p><p>因为本届亚运会还有电竞项目，所以新一代编码器 BVC 便分别针对运动、游戏这两种场景进行了优化。新一代编码器 BVC 开发团队调整了数十个编码参数来控制不同编码算法在运动、游戏场景中的性价比，在获得压缩率提高的同时实现了编码加速。此外，还对码率控制进行了调优，减少了高运动场景中画面模糊的情况。</p><p></p><h3>三、增强插帧多项技术绘制「画质美学」</h3><p></p><p></p><p>对于媒体平台转播来说，不同赛事节目涉及链路众多，且不同赛事之间存在差异，如何保障各链路的画质稳定并进一步提升画质是一个巨大的挑战。大型赛事直播涉及链路较长，不同赛事链路存在一些差异，但基本都是现场信号经过演播室的制作传输给 CDN 再进一步分发到用户侧。从画质角度来看，整个链路可分为画质检测与画质优化两个部分，对于 CDN 之前的链路以画质监测为主，以发现问题 / 定位问题 / 推动对应链路人员解决问题为目的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd0e4400056c0442a96f61be1cc46a22.png" /></p><p></p><p>随着赛事录制技术的提升，越来越多的大型赛事都用上了 4K HDR 录制标准，画质清晰度也不断提升，那随之而来的是更大的带宽压力；而且，为了兼容消费端不同的看播设备和不同的带宽条件，服务端需要转出多种不同分辨率不同码率的版本供看播端选择。</p><p></p><p>相比 SDR 信号，HDR 拍摄的片源拥有更广的色域，更大的动态范围。但对很多终端显示设备而言，并不支持 HDR 信号播放，所以团队通过 ToneMapping 算法将 HDR 信号转换为 SDR（标准动态范围）信号。</p><p></p><p><img src="https://static001.geekbang.org/infoq/42/42b4f0f8459a61ba1006d9ca4dda6385.png" /></p><p></p><p>HDR 在转换到 SDR 信号的过程中不可避免地会产生一些信息损失，常用的一些 ToneMapping 方法（如 Reinhard、Filmic、Hable），本质都是设计固定的映射曲线实现从 HDR 信号 到 SDR 信号的转换，同时尽量保持对 HDR 效果的还原。但直播赛事场景多变，且现场动态范围跨度极大，团队便提出了内容自适应 ToneMapping 算法，通过统计视频内容的实际光照情况动态地进行 ToneMapping，从而得到更优效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/60/6010fd5a3d5d13881386bd942f2c7d50.png" /></p><p>图：左 - 内容自适应 ToneMapping；右 -Hable 算法</p><p></p><p>随着音视频行业和摄影设备的发展，高分辨率的视频源占比日益增多，大部分视频需要在服务端进行降采样来配合自适应码率策略，因此降采样算法的优化也是提升 QoE 的关键。在过去的业界实践中，视频处理算法往往专注于提高分辨率（如超分算法）或者保持分辨率（如降噪算法）的处理范式，而几乎忽视了对降低分辨率方法的研究。所以团队自研了一种基于深度学习的图片 / 视频下采样算法——BAS（Byte AI Scaling）算法。不同于固定算子的 bicubic 等降采样算法，BAS 算法基于深度学习使用高精度数据训练模型，缓解传统方法带来的频域混叠与频域截断问题，降低锯齿感、减少细节丢失。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8db66e13d7988f3fb45d6ee840b6191f.png" /></p><p>图：4K 超高清图源降采样到 480p 分辨率，左图 -BAS 算法处理结果，右图 - 传统 bicubic 算法处理结果</p><p></p><p>在与 bicubic 算法的定量对比中，BAS 基于 PSNR 指标取得了 -20.32% 的 BD-Rate 收益，意味着相同重建误差水平下可以节省 20% 以上码率，而同等码率下则可以提升画质水平。而对于更符合人眼感知特性的 VMAF 指标，BAS 同样取得了 -20.89% 的 BD-Rate 收益。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c14556f77985196366ffa205b2287c3.png" /></p><p></p><p>此外，因为现在消费者已经习惯了高帧率的流畅视频体验，所以针对低帧率场景，团队使用了智能插帧技术，通过对前后帧的内容进行光流估计，根据光流信息将前后帧像素都转换到中间帧，然后进行整合、生成中间帧，提升视频帧率，减少观看时的卡顿感。而针对电竞类对帧率要求较高的场景，该技术团队做了额外优化：</p><p>faster 光流模块和 faster 修正模块使用 部分通道卷积代替普通卷积，在保持效果的同时减少卷积运算；采用推理下采样的方式，对输入进行内容自适应下采样，作为光流模块和修正模块的输入，再将输出上采回原分辨率用于原始输入的 warp 和整合，以达到进一步减少计算量的效果；工程上通过算子融合、半精度的方式减少 IO 和浮点运算，相比工程化前加速 1 倍多；通过多 GPU 部署的方式拓展智能插帧能力，使得视频插帧能在更高分辨率（4k）的场景下能实施部署。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c132a4c43dcb4d2c65e1184b3a32e5ab.png" /></p><p></p><p>在本届亚运会的转播过程中，智能插帧在处理电竞项目场景中复杂运动的“英雄名字”小文字时，通常会因为光流估计不够准确而导致插出来的帧文字的位置不够准确，导致伪像出现，于是团队在训练过程中加入更多的随意移动或者静止的较小文字，使得模型能够在训练过程中更多地注意处理小文字的复杂运动，从而达到更好地插帧效果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f6/f6908efadd76ff91a4af374f67f7bd86.png" /></p><p>图：左 - 优化后；右 - 优化前</p><p></p><p>另外值得一提的是，为了兼顾视频码率和主观画质，团队使用了基于 LSTM（长短期记忆网络）的时域 ROI 技术，通过人眼显著性区域检测和编码相结合的方式，让码率在画面上的分配更加合理。除了模型设计之外，ROI 算法中另一大难点是 saliency（显著性物体检测）数据的获取，通用的 saliency 数据集在大型赛事中的表现并不理想。针对这一问题，团队收集制作了自己的专用数据集，并且对一些大型赛事做了专用数据集。</p><p></p><p>同时，为了提升低档位、低分辨率的视频清晰度，团队为本届亚运会转播提供了“超分算法”。该算法是一种基于机器学习 / 深度学习方法，根据视频信息对其进行空域、时域建模重构出缺失的细节，将低分辨率的视频重建出高分辨率视频的技术。当用户看播端网速较慢切换到 480P/720P 等低分辨档位时，端上超分算法就会被触发以提升画面清晰度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/55e5b20f1d58ef467eb57771f4203abd.png" /></p><p>图：左 - 源流 1080P，右 -720p 超分后</p><p></p><p></p><h2>四、创新的 VR 直播技术实现「沉浸式现场」</h2><p></p><p></p><p>今年杭州亚运会的火炬点燃仪式是历史上首个“数实融合”点火仪式，在观赛方式上，PICO 推出的 PICO4VR 一体机作为一种虚拟现实设备，以其双目 4K 清晰度和 105 度的超大视角，无论是比赛过程还是赛后回放，观众都可以切换到多个不同的观看视角，给观众带来了更加真实的观赛体验。</p><p></p><p>观众佩戴 PICO4VR 一体机观赛，不仅能够通过视觉感受到沉浸感，还可以通过触觉的反馈来增强身临其境的感觉。比如，在颁奖典礼上，观众不仅能看到会场上空绽放的烟花，还可以挥动手中的荧光棒为选手加油助威，同时手柄的马达模拟了充气棒或荧光棒碰撞时的震动感，使观众能够在家中感受到与现场观赛相仿的热情氛围。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efd7971de9d4afdb9058773bdf7988e8.png" /></p><p></p><p>VR 直播的沉浸感以及高交互性是普通直播无法比拟的，但是这也导致了传输层需要承担更大的压力——分辨率为 8K x 4K 或 8K x 8K，源流码率达到 50M 甚至 120M，非常容易因为拥塞导致卡顿、延迟增大，甚至无法正常解码播放。</p><p></p><p>为了解决这个问题，团队将 8K 的视频切分成多个块（tile），只传输用户视角（viewport）内的部分超高清块，其它区域只传输 2K 或 4K 分辨率的缩小后的背景流，在用户切换视角的时候再去重新请求新的超高清块。同时团队基于 UDP 的内容优先级感知传输方案，优先保障高优数据的传输，对于低优数据可选择非可靠传输，即使丢失也无需重传，保证 XR 直播低延迟的同时也杜绝了过大的“视觉失真”。</p><p></p><p>另外值得一提的是，Pico 还在本届亚运会中打造了一个用户专属的赛事社交空间，观众可以邀请远方的好友一起观看比赛，在虚拟空间中共同分享运动的激情。这个过程中，Pico 就是基于火山引擎多媒体实验室沉浸音频与 RTC 空间音效能力，让用户可以随着位置的移动、头部姿势的变化，差异化地感受到节目声音与同场观众的声音对应的空间变化。</p><p></p><p></p><h2>五、写在最后</h2><p></p><p></p><p>在不断升级的观赛体验和极致追求中，视频云技术的每一次细致入微的技术迭代和追求极致的产品突破，无疑为用户带来了不断进化的感官体验，使得赛事直播的极致追求成为可能。通过电视、电脑、手机、平板等各种终端设备，全球亿万观众足不出户，就能第一时间、自由视角观赛，身临其境感受赛场氛围。这是科技与体育赛事的完美结合，也是技术进步为观众带来的独特体验。大家如果想了解更多关于视频云在大型直播过程中的实践细节，可以点击“<a href="https://www.volcengine.com/contact/docs/livebroadcast?utm_campaign=2023&amp;utm_content=baipishu&amp;utm_medium=infoQ&amp;utm_source=event&amp;utm_term=yunshangxinshijie">此处链接</a>"”获取《身临其境 沉浸互动——大型赛事直播实战白皮书》进行阅读。</p><p></p><p>在未来，我们期待看到更多的赛事直播能够借助视频云技术突破物理空间的限制，让观众获得如临现场，甚至超越现场的畅爽体验。当然，我们也非常期待火山引擎视频云能够继续领跑这一趋势，为全球的观众带来更多令人满意的赛事直播体验。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l</id>
            <title>1024开发者节｜讯飞星火V3.0发布，面向教育、医疗领域发布AI答疑辅学、讯飞晓医等重磅产品</title>
            <link>https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bfGTJtakc4lAr3H1Gz6l</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 05:51:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 世界声博会, 讯飞星火认知大模型V3.0, AI助手, 大模型算力平台
<br>
<br>
总结: 10月24日，第六届世界声博会暨2023全球1024开发者节开幕，讯飞发布了星火认知大模型V3.0，提升了七大能力，超越了ChatGPT。该模型可以打造个性化的AI助手，并且讯飞还推出了大模型算力平台。这些举措推动了产业生态的繁荣，吸引了大量开发者和企业客户的参与。 </div>
                        <hr>
                    
                    <p>10月24日，第六届世界声博会暨2023全球1024开发者节开幕，产业领袖、专家学者与广大开发者云集于此，共同探讨通用人工智能的最新进展和广阔应用前景。</p><p></p><p>现场科大讯飞董事长刘庆峰、研究院院长刘聪发布讯飞星火认知大模型V3.0，七大能力持续提升，整体超越ChatGPT，医疗领域超越GPT-4。星火3.0升级启发式对话、AI人设功能，能够打造更个性化的AI助手。</p><p>面向应用场景，讯飞星火面向行业进一步升级智能编程助手iFlyCode2.0，面向科研人员推出星火科研助手；面向学生群体，推出AI心理伙伴关爱青少年的心理健康，首次发布启发互动式英语AI答疑辅学并落地讯飞AI学习机中；面向每个家庭，发布讯飞星火医疗大模型，打造每个人的AI健康助手。此外，讯飞机器人超脑平台迎来升级，亮相业界首个大模型+具身智能的人形机器人 。</p><p></p><p>大模型也带动产业生态的繁荣。自5月6日星火发布以来，讯飞开放平台新增143万开发者团队，同比增幅331%，其中新增大模型开发者17.8万，讯飞正与10万+企业客户用星火创新应用新体验。讯飞还联合行业龙头共同发布12个行业大模型，加速产业落地。另一方面，讯飞联合昇腾生态共同发布“飞星一号”大模型算力平台，并启动对标GPT4的更大参数规模的星火大模型训练。2024年上半年将实现对标GPT-4。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/77/9d/77e559a045e56c160b3fe23a630b8e9d.png" /></p><p></p><p>10月24日至29日，科大讯飞还将面向市民开放人工智能产品科博展，观众可以在科技馆、工业馆、行业馆、教育馆、生活馆、生态馆和科学艺术展7个主题展馆，近距离体验星火大模型核心技术、落地产品及生态展品。</p><p></p><h3>一、讯飞星火V3.0发布，整体超越ChatGPT</h3><p></p><p></p><p>认知大模型正开启一个增长新时代，也在成为驱动新型工业化的重要引擎。</p><p></p><p>“大模型正在走进我们每个人的生活。”刘庆峰表示，在今年亚运会上，工作人员借助讯飞星火与中国移动开发的5G新通话助手，刚挂完电话待办事项就已自动生成；在星火iFlyCode帮助下，平台迁移原本需要3个月的开发工作1个月就能完成；讯飞的AI诊疗助手“智医助理”在大模型加持下，将患者“疑似妊娠”的诊断修正为“先兆流产”，挽救了一个新生命；讯飞星火还与国家自然博物馆合作，让300万年前的黄河象“活了”，还能与人对话······</p><p></p><p>讯飞星火，不仅拓展行业边界，也在支持每一位“创客”逐梦科技。今年的全球1024开发者大赛汇聚全球9个国家278座城市3.4万支团队的开发者，年龄最小的13岁，00后选手占比达59%。</p><p></p><p>应用场景和生态的蓬勃发展，依赖核心技术底座的持续进步。从首次发布至今，讯飞星火持续迭代，推出V1.5、V2.0版本。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/68/06/68f50df8d0ea31882c3bde12808d8006.png" /></p><p></p><p>主论坛上，科大讯飞重磅发布星火认知大模型V3.0，七大能力均持续提升，全面对标ChatGPT，中文能力客观评测超越ChatGPT，英文能力对标ChatGPT48项任务结果相当。</p><p></p><p>国务院发展研究中心国研经济研究院测评报告显示，星火大模型V3.0综合能力超越ChatGPT，国内领先，达到国际一流水平，在医疗、法律、教育行业的表现格外突出。</p><p></p><p>星火3.0 进一步升级了数学自动提炼规律、小样本学习、代码项目级理解能力、多模态指令跟随与细节表达等能力，进一步提升星火的落地应用能力。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/9c/9b/9ce735ea89b7a0e6d57b8345d2a59c9b.png" /></p><p></p><p>升级AI人设、启发式对话，打造每个人的AI助手自讯飞星火9月5日全民开放后，当前已有1200万用户，也对认知大模型提出的更高期待，“不仅能回答问题还要能提出问题”、“不仅要有知识还要有个性”。刘庆峰指出，大模型不仅要能回答问题，还要像人一样主动提问。</p><p></p><p>大模型想要赋能民生百业，需要实现从多轮对话、主动对话再到启发式对话的跨越，实现星火的智慧阶跃，推动行业纵深应用。此外大模型不仅要有专业的知识，还要具备个性化的AI人设，为星火注入“灵魂”。对此，讯飞星火大模型V3.0新增了虚拟人格功能，它可以根据性格模拟、情绪理解、表达风格来形成一个初始人设，再结合特定知识学习、对话记忆学习，形成一个更个性化的AI人设。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/1c/c4/1cc129cfdbd44988585e67ff807492c4.png" /></p><p></p><p>当前虚拟人格应用“星火友伴”已上线，无论你想和哪吒聊八卦还是和诸葛亮聊家国大事，只要定义好友伴的“人格”就能和不同人物性格的角色对话，“这不只是聊天，还可以释放灵感”，刘庆峰说。</p><p></p><p>当刘聪点击已创建的“诸葛亮”人格来对话时，诸葛亮的声音和语言风格便展现出来。用户通过“友伴”还能创建自己的人格，刘聪现场创建了个人人格助手，上传个人背景知识，大模型便能用他的风格进行对话。</p><p></p><h3>二、升级编程助手iFlyCode2.0，推出科技文献大模型</h3><p></p><p></p><p>“大模型首先要能够在工业和科研领域造福人类，这才是我们真正解决社会刚需的地方。”刘庆峰称。在工业领域，今年9月20日，基于讯飞星火的羚羊工业大模型正式发布，通用人工智能成为新型工业化的核心引擎。</p><p></p><p>1个月内，羚羊工业互联网平台平台的工业文本生成、知识问答、理解计算、代码生成、多模态5大核心能力再度升级，不到一个月时间平台用户总量提升59%，平台服务企业次数增加88%。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/12/1d/12d141ae68340b8daa42yy34e8249e1d.png" /></p><p></p><p>讯飞星火代码能力超越ChatGPT，iFlyCode再升级全面提效</p><p></p><p>产业数字化和数字产业化都离不开代码能力的支持。数据显示，中国有约800万软件从业人员，2022年软件业务收入10.8万亿元，占GDP比重8.93%，2028年预测将达到20万亿元。</p><p></p><p>刘庆峰表示，自讯飞星火发布以来，代码能力持续升级，星火3.0已基本全面超越ChatGPT，正在追赶GPT-4。</p><p></p><p>搭载讯飞星火V3.0的智能编程助手iFlvCode2.0正式发布，在编程的设计阶段提效50%、开发阶段提效37%、测试阶段提效44%，大幅提升软件从业人员效率。iFlyCode发布以来，已在京东云、软通动力等107家机构实现深度对接应用。今年11月份，几乎全国所有软件名城都会和科大讯飞开展相关合作。</p><p></p><p>联合中科院文献情报中心打造科技文献大模型</p><p></p><p>在科研领域，科技文献是检验大模型高阶认知能力的“试金石”。讯飞星火和与中科院文献情报中心联合打造科技文献大模型，可以实现成果调研、论文研读、学术写作三大功能。</p><p></p><p>刘聪现场实测，用科技文献大模型将十几份量子计算论文一键生成综述报告，为科研人员快速了解最新论文大幅提效。测试显示，学者成果调研的效率被提高超10倍，论文研读有效率和学术写作采纳率均超90%。“以人工智能领域为例，每天有84篇文章发表，每篇阅读10分钟，一天读完也要花14个小时，科技文献大模型快速生成综述，对科研人员快速掌握全球最新前沿进展有巨大帮助”，刘庆峰说。</p><p></p><p>刘庆峰表示，通过科研助手，不光可以提升我们的科学研究效率，还能释放350万在校研究生和博士生的时间，这将极大释放中国未来的想象力。</p><p></p><h3>三、用通用人工智能解决教育、医疗社会刚需</h3><p></p><p></p><p>讯飞AI学习机代差级优势再凸显！首发启发互动式AI英语答疑辅学</p><p></p><p>通用人工智能时代之下，“因材施教”进入第三个层次，从精准识别知识掌握的薄弱环节、到基于知识图谱推荐最近发展区的学习路径、再到类人式对话辅导进行逐层讲解。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/03/fc/03c3558241a386986c10ff9e559ff8fc.png" /></p><p></p><p>发布会上，搭载了星火大模型V3.0 讯飞AI学习机再度升级，首次发布启发互动式英语AI答疑辅学功能，实现个性化解决、启发引导式提问、互动探究式学习。刘聪在现场实测进行多轮提问，无论是探讨语法对错还是理解文章主旨，讯飞AI学习机都能自如回答。</p><p></p><p>根据2023年9月安徽地区七八年级试点用户的统计数据显示，小学英语错题解决率，从答案解析学习的77%到AI答疑辅学的90%，效率提升了13%；初中英语错题解决率：从答案解析学习的61%到AI答疑辅学82%，效率提升了21%。</p><p></p><p>此外，讯飞AI学习机的百科问答助手功能也再次升级，涵盖权威优质百科内容、适龄推荐、启发式提问、百科数字人等功能。</p><p></p><p>“一个国家未来的科技发展，科学精神应该从娃娃抓起”，刘庆峰说，科大讯飞联合科普中国、少年儿童出版社首发科普大模型，助力全民提升科技素养。科普中国拥有40万+科普中国音视频、6000万+科技文献构建科普大模型，将优秀科普资源惠及广大中小学生，及时回应孩子的每一次提问，课堂学生互动参与率从25%提升到77%，问题解答率从23%提升到95%，打通应用最后一公里，讯飞AI学习机将搭载“讯飞星火+专业科普知识库”的科普大模型，帮助青少年在心中播种下科创的种子。</p><p></p><p>据了解，随着讯飞星火认知大模型V3.0的推出，科大讯飞AI学习机上已经拥有了英语口语陪练、中英作文批改、数学互动辅学、百科自由问答、亲子教育助手、智能编程助手、创意绘画伙伴以及英语答疑辅学8款基于大模型的类人辅学功能。</p><p></p><p>“教育不是注满一桶水 而是点燃孩子心中的一把火”，刘庆峰表示，通用人工智能为教育带来全新机遇。</p><p></p><p>科大讯飞发布AI心理伙伴，用星火守护青少年心理健康</p><p></p><p><img src="https://static001.infoq.cn/resource/image/1f/6e/1fa964a87bfab011b222cef7a70f2b6e.png" /></p><p></p><p>科大讯飞一直致力用AI关注青少年的心理健康。发布会上，基于讯飞星火打造的AI心理伙伴正式发布，它具备多模态情感识别、共情表达、寻因式提问、个性化心理指导四大核心能力，善解人意的“小聪”具有极其丰富的心理学知识——10亿+条心理类数据、40万+篇期刊文献、100万+通脱敏心理对话案例、550万+个心理评估数据。</p><p></p><p>根据来自广东、江苏、安徽等21个高中连续两年心理普查的纵向追踪数据平均值显示，同学们的抑郁、冲动、自责、学习焦虑、社交焦虑、孤独等6种心理问题，在使用减压星球后都有下降，如抑郁情绪同比去年减少了8%。</p><p></p><p>AI心理伙伴“小星”不仅可以“听懂”孩子的心事，还能主动回应和给出解决问题的个性化指导建议，和孩子对话接受后，“小星”第一时间生成心理咨询报告，同时将预警信息发给心理老师。“孩子们通常不愿意跟家长和老师敞开心扉，只有通过AI的启发式对话，才有可能真正解决大量孩子的心理减压和疏导问题”。至今，青少年“减压星球”已覆盖3202所学校服务259万中小学生。</p><p></p><p>讯飞星火医疗大模型正式发布，打造每个人的AI健康助手</p><p></p><p><img src="https://static001.infoq.cn/resource/image/6d/86/6dc49d73354c9d0d90129c05f52d1586.png" /></p><p></p><p>发布会上，讯飞星火医疗大模型正式发布。健康和每一个人密切相关，搭载了讯飞星火医疗大模型的“讯飞晓医”APP触手可及，看病之前不知道怎么准确表达自己的病情？自己买药不知道有何禁忌？拿到体检报告后想马上“识别”出自己去年和今年比有什么健康异动？这一切都可以问讯飞晓医。</p><p></p><p>今年5月6日就发布了讯飞星火认知大模型，为什么要到10月24日才发布医疗大模型？刘庆峰表示，因为医疗需要非常严谨对待，早在2017年讯飞智医助理就已经通过国家执业医师资格考试，排名超过96.3%的人类考生，首先给基层医院及等级医院的医生提供帮助，助力成为医生的AI诊疗助理，在这个基础上，今天开始面向家庭，打造每个人的AI健康助手，让AI医疗的普惠之光照进每个家庭。</p><p></p><p>根据统计，全科医生缺口达30万人，等级医院的医生能分配给每个病人的诊疗时间只有3分钟，看病之前问讯飞晓医，将大幅提升问诊效率，健康自查有效度提升40%。每年零售药店买药人次达67亿，购买非处方用药占比达45%，用药有疑问可以问讯飞晓医，帮助普通人更合理用药，高风险用药召回率提升90%。每年体检人次5.5亿，5000字体检报告眼花缭乱，讯飞晓医可以马上生成重点及健康提醒，帮助人们“对症复诊”。</p><p></p><p>这背后靠的是讯飞星火医疗大模型的核心能力支撑，通过上线实际使用数据抽查12万例并通过国家科技信息资源综合利用与公共服务中心（STI）第三方测试数据显示，讯飞星火医疗大模型在医疗海量知识问答、医疗复杂语言理解、医疗专业文本生成、医疗诊断治疗推荐的问题回答率全面超越GPT-4。</p><p></p><p>业界首个大模型+具身智能的人形机器人亮相</p><p></p><p>2022年初，科大讯飞推出“讯飞超脑2030计划”，提出分三个阶段，要让懂知识、善学习、能进化的软硬一体机器人走进千家万户。去年1024全球开发者节上，机器人超脑平台AIBOT发布，把认知智能、运动智能和具身智能的多模态感知能力结合起来，已为工业、教育、服务、特种等领域372家企业提供服务。AIBOT的持续进步，助力我国机器人进入高阶智慧时代。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/af/0f/afcf7f79f053627a1eb90671a578470f.png" /></p><p></p><p>刘庆峰表示，星火大模型的推出将让AIBOT赋能机器人发展进入到一个全新的阶段，人形机器人对复杂任务的拆解、开放场景的物体寻找明显提升，在强化学习泛化抓取、仿人行走的复杂地形运动能力上都比主流系统有一个非常大的提升。“下一步我们将以人形机器人为牵引，来推动‘视觉-语言-动作’多模态具身大模型，能够更好地赋能人形机器人”。</p><p></p><p>此外，科大讯飞发布AI扫拖机器人，它具备领先的家庭环境全场景感知及运动能力，通过多模态交互，精准避障让扫地机器人在家庭常见的障碍物中灵活穿梭不卡困。</p><p></p><h3>四、共建产业生态，2024年上半年对标GPT-4</h3><p></p><p></p><p>唯有生态繁荣，才有人工智能大未来。自2010年发布讯飞开放平台以来，科大讯飞持续构建AI开发者生态，让AI像水和电一样落地各行各业。今年依托星火大模型，讯飞将AI普惠更多用户。</p><p></p><p>“讯飞星火正日益成为解放生产力工具。”</p><p></p><p>刘庆峰表示，当前讯飞星火已有1200万用户，为千万用户持续创造刚需场景价值。自讯飞智作2.0 、讯飞写作产品发布以来，已新增超10万名专业用户，提供新闻播报、教育培训、自媒体推广到企业宣传等服务，让每个内容创作者都拥有自己专属的AI创作助手。</p><p></p><p>此外，他现场发布星火内容运营大师，人机共创提质增效，通用稿件30分钟共创生成，深度稿件辅助编辑效率翻番。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/90/53/90fb67463686e0b6d45538c147508453.png" /></p><p></p><p>如今随着大模型持续突破，讯飞AI开发者生态持续领先。自5月6日星火发布以来，讯飞开放平台新增143.4万开发者团队，同比增幅331%，汇聚553万AI生态开发者团队。其中新增大模型开发者17.8万，讯飞正与10万+企业客户用星火创新应用新体验。大模型也显著降低了开发者门槛，在讯飞星火上，已有1.5万助手开发者，开发超2.9万个助手应用。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/23/cf/233969d1371273ac90fb55439d6deacf.png" /></p><p></p><p>为了进一步加速大模型行业落地，科大讯飞联合行业龙头共同发布12个行业大模型，包含金融、汽车、运营商、工业、住建、物业、法律等行业，加速产业升级。</p><p></p><p>在讯飞星火蓬勃的生态背后，是自主创新的算力底座。讯飞和华为“联合特战队”半年内实现星火大模型训练和推理效率均翻番。1024全球开发者节现场，科大讯飞联合华为重磅发布，基于昇腾生态的“飞星一号”大模型算力平台。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/83/a5/835df7ecc7c130c0b4635d0yyb3562a5.png" /></p><p></p><p>不过，刘庆峰也提到，国产大模型必须正视和GPT-4的差距。“国产大模型在复杂知识推理、小样本快速学习、超长文本处理、跨模态统一理解上距GPT4还有差距。”他表示，唯有实事求是的科学精神，才能真正实现超越。</p><p></p><p>“大模型时代的大幕才刚刚揭开，让我们一起共创、共享、共进化。”刘庆峰透露，对标GPT-4的更大参数规模的星火大模型正式启动训练，2024年上半年将实现对标GPT-4。</p><p></p><p>【活动推荐】</p><p></p><p>在2023年12月28-29日，InfoQ将在上海举办<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon全球软件开发大会</a>"，这个会议上结合当前的趋势热点，设置了GenAI和通用大模型应用探索、AI Agent 与行业融合应用的前景、LLM 时代的性能优化、智能化信创软件 IDE、LLM 时代的大前端技术、高性能网关设计、面向人工智能时代的架构、高效的编程语言、性能工程、LLM推理加速和大规模服务、现代数据架构演进、建设弹性组织的经验传递、SaaS云服务弹性架构设计等专题，目前也正在邀请业界的专家来会议上演讲。感兴趣的可以点击<a href="https://qcon.infoq.cn/2023/shanghai/track">QCon会议官网</a>"，查看详细的介绍，也欢迎您来会议上演讲，分享技术实践。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/euXpcb8cpRJHLJSaxZaE</id>
            <title>Google DeepMind 宣布基于LLM的机器人控制器 RT-2</title>
            <link>https://www.infoq.cn/article/euXpcb8cpRJHLJSaxZaE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/euXpcb8cpRJHLJSaxZaE</guid>
            <pubDate></pubDate>
            <updated>Tue, 24 Oct 2023 02:18:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Google DeepMind, Robotics Transformer 2, AI 模型, LLM
<br>
<br>
总结: Google DeepMind最近宣布了Robotics Transformer 2（RT-2），这是一个用于控制机器人的视觉-语言-动作（VLA）的AI模型。RT-2使用经过精调的LLM来输出运动控制命令。它可以执行训练数据中未明确包含的任务，并在新出现技能评估中将基线模型的表现提升了3倍。 </div>
                        <hr>
                    
                    <p><a href="https://www.deepmind.com/">Google DeepMind</a>" 最近宣布了 <a href="https://robotics-transformer2.github.io/">Robotics Transformer 2</a>"（RT-2），这是一个用于控制机器人的视觉-语言-动作（VLA）的 AI 模型。RT-2使用经过精调的LLM来输出运动控制命令。它可以执行训练数据中未明确包含的任务，并在新出现技能评估中将基线模型的表现提升了3倍。</p><p>&nbsp;</p><p>DeepMind 训练了两个 RT-2 的变体，使用了两个不同的底层视觉-LLM 基础模型：一个基于 <a href="https://www.infoq.com/news/2023/06/google-palm-e-robot/">PaLM-E</a>" 的 12B 参数版本，以及一个基于 <a href="https://arxiv.org/abs/2305.18565">PaLI-X</a>" 的 55B 参数版本。LLM 在通用视觉语言数据集和机器人特定数据的混合之上进行了共同微调。该模型学会输出一个机器人运动指令向量，将其简单地视为一个整数字符串：实际上，它是模型学习的一种新语言。最终模型能够接收机器人工作区域的图像和用户命令，例如“捡起即将从桌子上掉下来的袋子”，然后生成执行任务的运动指令。根据 DeepMind 的说法：</p><p></p><p></p><blockquote>RT-2 不仅展示了人工智能的进步如何迅速地渗透到机器人领域，它还展示了更多通用型机器人的巨大潜力。 虽然实现在以人类为中心的环境中有用的机器人方面还有大量工作要做，但 RT-2 向我们展示了一个令人兴奋的机器人的未来，它就在我们的掌握之中。</blockquote><p></p><p>&nbsp;</p><p>谷歌机器人技术和 DeepMind 已经发布了多个使用 LLMs 进行机器人控制的系统。2022 年，InfoQ 报道了谷歌的 <a href="https://www.infoq.com/news/2022/09/google-robot-saycan/">SayCan</a>"，它使用 LLM 为机器人生成高级行动计划，以及 <a href="https://www.infoq.com/news/2022/11/google-robotics-code-policies/">Code-as-Policies</a>"，它使用 LLM 生成执行机器人控制的 Python 代码。这两个系统都使用纯文本 LLM 来处理用户输入，视觉组件由独立的机器人模块处理。今年早些时候，InfoQ 报道了谷歌的 <a href="https://www.infoq.com/news/2023/06/google-palm-e-robot/">PaLM-E</a>"，它处理来自机器人传感器的多模态输入数据，并输出一系列高级行动步骤。</p><p>&nbsp;</p><p>RT-2 在之前的实现 <a href="https://robotics-transformer1.github.io/">RT-1</a>" 的基础上进行了改进。RT 系列的关键思想是训练一个模型直接输出机器人指令，而不是输出运动的更高级抽象。RT-2 和 RT-1 都接受图像和任务的文本描述作为输入。然而，RT-1 使用了一系列不同的视觉模块来生成输入到 LLM 的视觉令牌，而 RT-2 使用了单一的视觉语言模型，如 PaLM-E。</p><p>&nbsp;</p><p>DeepMind 在超过 6,000 个试验中对 RT-2 进行了评估。特别是，研究人员对其新兴能力表现感兴趣：即能够执行机器人专门训练数据中不存在的任务，但这些任务是通过其视觉语言预训练而出现的。团队将 RT-2 在三个任务类别上进行了测试：符号理解、推理和人类识别。与基准模型相比，RT-2 取得了“超过最佳基准模型平均成功率 3 倍以上”的成绩。然而，该模型未获得机器人训练数据中未包含的任何物理技能。</p><p>&nbsp;</p><p>在 <a href="https://news.ycombinator.com/item?id=36905076">Hacker News 上对这件工作的讨论</a>"中，一位用户发表了评论：</p><p></p><p></p><blockquote>似乎这项工作（以及许多机器人学习工作）仍然停留在位置/速度控制而不是阻抗控制阶段。这本质上是输出去哪里，可以是通过闭环控制器或开环运动规划器。这似乎极大地降低了数据需求，但感觉对我们可以完成的任务有一种根本性限制。机器人操作之所以困难是因为我们不仅需要考虑世界上正在发生的事情，还需要考虑到我们的互动如何改变它以及我们如何对此作出反应。</blockquote><p></p><p>&nbsp;</p><p>尽管RT-2尚未开源，但RT-1的<a href="https://github.com/google-research/robotics_transformer">代码</a>"和<a href="https://console.cloud.google.com/storage/browser/gresearch/rt-1-data-release?pli=1">数据</a>"已经公开。</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.infoq.com/news/2023/10/deepmind-robot-transformer/">https://www.infoq.com/news/2023/10/deepmind-robot-transformer/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>