<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks</id>
            <title>夸克发布全新 PC 端，系统级全场景 AI 能力升级 AI 电脑</title>
            <link>https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zY6Ks5qM2bBL6Gs3izks</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 10:18:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>将一台电脑升级为 AI 电脑需要几步？今后只需安装一个夸克就够了！8 月 27 日，阿里智能信息事业群旗下夸克发布全新 PC 端，全面升级 AI 搜索、AI 写作、AI&nbsp;PPT、AI 文件总结等一系列功能。凭借“系统级全场景 AI”能力，夸克为你升级AI电脑，一站式完成信息的检索、创作和总结。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/48/32/4882ff0e6bc3b6c0c513d5eaaffb4932.png" /></p><p></p><p>“始终面向用户、面向 AI、面向未来，夸克在人工智能领域持续探索突破性的用户体验。”阿里智能信息事业群总裁吴嘉表示，全新的夸克开启了创造革新性搜索产品的无限可能，也为阿里巴巴人工智能战略布局增添了强有力的路径与动能。</p><p>&nbsp;</p><p></p><h2>一、夸克 PC 端功能上新，为你升级一台 AI 电脑</h2><p></p><p></p><p>数字时代，PC 成为生产力的代名词，随着用户需求迭代，以及生成式 AI 技术跃迁，PC 的智能化改造成为必然。全新夸克 PC 端升级多项能力，让你的电脑秒变 AI 电脑，辅助你完成复杂、重复的任务，让效率再翻倍。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/3b/94/3bba3758ce5e89231a57bd7961c54f94.jpg" /></p><p></p><p>一个月前夸克在 App 端推出的全新 AI 搜索，此次一并在 PC 端发布，并升级了更强的模型能力，提升到更快的交互速度。夸克AI回答的首字出现速度和吐字速度大幅领先行业，瞬时就能给你精准答案。三栏式的界面设计能更清晰地展现图文、视频等生成式回答和网页，让你一眼就能得到核心信息。</p><p></p><p>写作无疑是 PC 用户的高频需求，针对大学生、白领等重度用户，夸克就是你的“笔杆子”，任何体裁文章都能写得出色。当你输入主题和字数等要求后，夸克能撰写近 200 种类型的文稿，半分钟就能产出一篇高质量文章。夸克还提供多种方式撰写 PPT，比如输入主题智能生成大纲，或筛选模板再编辑内容，还支持 Word 一键转成 PPT。夸克帮你化繁为简，让你更专注创作本身。</p><p></p><p>此外，当你在 PC 上阅读大量的专业文档和网站内容时，想秒懂里边的内容，更需要一个会思考、能理解、会表达的小助理。夸克 AI 文件总结不惧几十万字的长文，能快速提取 PDF、Word、PPT 等文档中的核心内容，并通过持续提问、生成脑图等方式，更好地帮助用户理解关键信息。</p><p></p><p>夸克产品负责人郑嗣寿表示：“用户的需求在哪里，夸克就在哪里。夸克 PC 端给用户的信息检索、信息生成和信息处理带来了更快的速度和更强的效果，这是我们利用 AI 技术面向用户创造的新价值。”</p><p>&nbsp;</p><p></p><h2>二、系统级全场景 AI，随时随地帮你解决实际问题</h2><p></p><p></p><p>在 PC 中，用户会在桌面、文档、网页等多场景中进行操作，反复切换也练就了“黄金指”。夸克让电脑秒变AI电脑后，具备“系统级全场景 AI” 能力，在 Windows 电脑按下 Alt+Space 或苹果电脑的Option+Space，可以随时随地使用 AI 回答、AI 写作、AI&nbsp;PPT、AI 文件总结等功能。只要你有问题和需求，夸克的AI能力无处不在、触手可及。</p><p></p><p>比如在查网页、看文档等场景中，夸克能通过划词、截屏等方式，更加丝滑地进行搜索、解读、翻译和润色，无需再单独开启其他应用。就连辅导孩子作业，夸克也可以提供截屏搜索，依托海量学习题库和学习专属大模型，提供解题思路和答案，让自学和辅导的效率全面升级。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/34/c6/343e287e22353869be08e3588796c7c6.png" /></p><p></p><p>“系统级全场景 AI”能力还会深入到电脑的每个场景中。右键点击文档，夸克能帮你快速总结Word、PDF、TXT等常用文件中的关键信息，还能一键帮你转换文档格式。夸克还提供官方插件，让你的浏览器变成AI浏览器，同样能使用 AI 回答、AI 写作、解读、翻译以及网页总结等功能。</p><p></p><p>在哪都能用，随你怎么用！夸克 PC 端不仅大幅提升了用户使用搜索、写作等功能的效率，也让 AI 电脑成为每个人的标配。随着用户需求的不断迭代，夸克的产品创新也会持续演进，让更多 AI 能力落地到不同设备的不同场景中。</p><p>&nbsp;</p><p></p><h2>三、突破性用户体验，创新践行“AI 驱动”战略</h2><p></p><p></p><p>夸克从诞生以来，以 AI 技术为业务发展引擎，面向用户探索下一代智能信息产品，短短数年就成长为用户过亿、增长强劲的新锐产品，尤其是获得了年轻人群的青睐。</p><p></p><p>进入 AI 时代，阿里集团将 AI 作为改变和加速业务增长的最强大变量，所有业务场景都可以通过人工智能创造更大的价值。夸克凭借多年积累沉淀的大模型技术、多应用场景、年轻用户群体等优势，大力革新搜索产品体验。自升级AI搜索以来，全新的夸克在用户规模与产品口碑方面均有不错的市场表现。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/70/32/70dcdeee01e1ca334695d46b2d75f532.png" /></p><p></p><p>数据显示，6 月高考季，夸克高考 AI 搜索使用量超过 1 亿次。7 月，夸克升级“超级搜索框”，推出以 AI 搜索为中心的一站式 AI 服务，持续霸榜苹果应用商店免费榜。在《 2024 年第二季度 iOS 实力 AI 产品排行榜》，夸克作为 AI 搜索产品新兴势力，以 99.71 的高分在一众AI应用中位居榜首。</p><p></p><p>“生成式 AI 技术的突飞猛进，让夸克的目标和愿景更有机会得以落实和推进，加速了夸克的能力跃迁和产品迭代。”郑嗣寿透露，接下来，夸克会继续保持极快的迭代速度，在 AI 产品体验上迅猛推进，为用户创新一站式、多端一体的 AI 服务。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc</id>
            <title>人人创造，一起热AI ｜火山引擎首届AI创造者大赛来啦！</title>
            <link>https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/gJpA5gfd2Frvou4lGpJc</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 09:44:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI正以前所未有的速度融入千行百业，成为各行业智能化转型的加速器。火山引擎正式发起「AI创造者大赛」，大赛首场为汽车行业专场，由火山引擎携手领克汽车与英特尔联合主办、吉利汽车研究院协办，旨在携手汽车行业领军品牌，鼓励开发者利用豆包大模型和扣子专业版，针对领克汽车的真实业务场景，开发出具有实际应用价值的智能体解决方案。</p><p></p><p>本次大赛共设置三大赛道——AI 座舱赛道、AI 营销赛道、AI 售后赛道。目前大赛报名通道已开启，可登录火山引擎官网查看更多赛事详情，报名参赛即有机会赢取领克汽车Z10全年使用权，更有丰厚奖金与礼品等你带回家！</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc8ca60bc6ac31816dc7296433f1232d.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe</id>
            <title>InfoQ 2024年趋势报告：AI 智能体发展不及预期，RAG 或成最大赢家</title>
            <link>https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/USe4z1Yv0xd208XXboGe</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 02:41:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>作为InfoQ的一大特色，趋势报告系列专注于软件开发的各个关键领域。这些报告旨在为InfoQ的读者和听众提供今年值得关注的技术发展趋势概览。</p><p></p><p>InfoQ的人工智能、机器学习和数据工程编辑团队邀请了业界专家，共同探讨了人工智能和机器学习领域的未来趋势，以及接下来12个月中值得关注的动态。本期播客记录了这次讨论的内容，小组成员们分享了他们对创新人工智能技术如何重塑行业格局的见解。</p><p></p><p></p><h1>关键要点：</h1><p></p><p>人工智能的未来是开放的。我们正处于大语言模型和基础模型的时代。尽管目前大部分模型是闭源的，但像Meta这样的公司正试图引领开源模型的趋势。检索增强生成（RAG）的重要性将日益凸显，特别是在大规模部署LLM的场景中。随着人工智能赋能的GPU基础设施和人工智能驱动的个人电脑的出现，AI驱动的硬件将获得更多关注。由于受基础设施设置和管理成本方面的限制，小语言模型（SLM）将得到更多的探索和采用。小语言模型也是边缘计算相关用例的一个很好的选择，它们可以在小型设备上运行。AI代理，如编码助手，将在企业应用开发环境中得到更多的采用。在语言模型的生命周期管理中，AI的安全性和隐私保护将持续占据重要地位。自托管模型和开源LLM解决方案将有助于加强AI的安全防护。LangOps或LLMOps将成为LLM生命周期的关键环节，它们在大模型生产环境部署的持续支持中发挥着重要作用。我们对未来12个月的AI发展做出了一些预测：机器人AI，即具身AI，将成为新的趋势；从AI寒冬过渡到更多具体的应用场景，涉及更多自动化工作流和智能体工作流，然后扩散到更多的边缘设备，如笔记本电脑和手机。</p><p></p><h1>简介</h1><p></p><p></p><p>Srini Penchikala：大家好，欢迎收听2024年人工智能与机器学习趋势报告播客。这个播客是我们年度报告的一部分，目的是与听众分享人工智能和机器学习领域的最新动态。我是Srini Penchikala，InfoQ人工智能、机器学习和数据工程社区的主编。我们有幸邀请到了一群杰出的专家和实践者，他们来自人工智能和机器学习的不同领域。</p><p></p><p>感谢大家来到这里。我非常期待与大家共同探讨人工智能和机器学习领域的最新动态，包括我们目前的发展阶段，更重要的是我们未来的发展方向。特别是自去年我们讨论趋势报告以来，人工智能技术的创新速度之快令人目眩。在开始深入播客主题之前，我想先向听众朋友们提供一些必要的信息。我们的年度报告包含两个主要部分。首先是这个播客，它提供了一个平台，让听众能够听到来自专家实践者们对创新人工智能技术如何颠覆行业的见解。其次是一份书面文章，将在InfoQ网站上发布，其中将包含技术采用不同阶段的趋势图，并详细介绍自去年趋势报告以来新增或更新的个别技术。</p><p></p><p>我强烈推荐大家在本月底文章发布时去阅读它。现在，让我们回到播客的讨论上来。自ChatGPT发布以来，生成式人工智能和大型语言模型技术的发展速度似乎达到了顶峰，而且这种快速的创新势头似乎不会很快放缓。技术领域的主要参与者都一直在忙着发布他们的人工智能产品。今年早些时候，谷歌在I/O大会上发布了几项新的更新，包括Gemini更新和生成式人工智能在搜索中的应用。同时，OpenAI也发布了GPT-4o，这是一个能够实时处理音频、视觉和文本的全能模型，提供了一种多模态解决方案。</p><p></p><p>紧接着，Meta也发布了Llama 3，并很快推出了基于4050亿参数的Llama 3.1版本。这些参数的数量级是亿，而且它们还在不断增加。像Ollama这样的开源解决方案也受到了越来越多的关注。看来这个领域一直在加速发展。生成式人工智能技术的基础是大语言模型，它们经过大量数据的训练，能够理解和生成自然语言及其他类型的内容，还能执行丰富多样的任务。因此，LLM可以作为我们今年趋势报告讨论内容的切入点。</p><p></p><p>Anthony，你一直在密切关注LLM模型及其发展。你能谈谈生成式人工智能和LLM模型的当前发展状态、最近的一些主要进展，以及我们的听众应该关注哪些方面吗？</p><p></p><p></p><h1>AI的未来是开放的</h1><p></p><p></p><p>Anthony Alford：如果要用一个词来概括LLM，我会选择“更多”，或者可能是“规模”。我们正处在LLM和基础模型的黄金时代。OpenAI可能是最显眼的领导者，当然，还有其他大玩家，比如谷歌，还有Anthropic推出的Claude。这些模型大多是封闭的，即便是OpenAI，他们的旗舰产品也只能通过API访问。然而，Meta在这方面是一个异类。实际上，我认为他们正试图引领趋势朝着更开放的方向发展。我记得扎克伯格最近说过，“人工智能的未来是开放的。”因此，他们开放了一些模型的权重。至于OpenAI，尽管他们没有公开模型权重，但也会发布一些模型的技术细节。例如，我们知道GPT-3的第一个版本有1750亿个参数，但对于GPT-4，虽然他们没有明确说明，但趋势表明它肯定拥有更多的参数，数据集更大，计算预算也更大。</p><p></p><p>我认为我们还将继续见证的另一个趋势是模型的预训练，也就是GPT中的“P”。这些模型在庞大的数据集上进行预训练，基本上是整个互联网的内容。然后，他们会进行微调，这是ChatGPT的关键创新之一。因此，这种指令微调现在变得极其普遍，我相信我们将继续看到这一趋势。接下来，让我们转到上下文长度这个话题，因为它代表了另一个发展趋势。上下文长度，即你可以输入模型的数据量，这个量正在增加。我们可以讨论这与新的SSM（State Space Model，状态空间模型，如Mamba）之间的区别，因为SSM是没有上下文长度限制的。Mandy，你对这个话题有什么看法？</p><p></p><p>Mandy Gu：我认为这绝对是我们正在见证的一个明显趋势，那就是更长的上下文窗口。当初ChatGPT等大语言模型开始普及时，这是人们普遍指出的一个不足之处。今年早些时候，Gemini、Google基金会以及GCP的基础模型引入了高达一百万个Token的上下文窗口长度，这无疑是一个改变游戏规则之举，因为之前我们从未有过如此长的上下文处理能力。我认为这引领了一种趋势，其他供应商也在尝试提供同样长甚至更长的上下文窗口。由此产生的一个二级效应是提升了可访问性，它使得像信息检索这样的复杂任务变得更加简单。在过去，我们可能需要进行多阶段的检索，例如RAG，但现在，我们可以将所有上下文信息直接输入到这一百万Token的上下文窗口中，虽然不一定意味着更好，但无疑简化了过程。这是过去几个月的一个非常有趣的进展。</p><p></p><p>Anthony Alford：Namee，你还有什么要补充的吗？</p><p></p><p>Namee Oberst：我们专注于小语言模型的应用。较长的上下文长度窗口有它的价值，但根据我们内部的研究以及YouTube上一些知名人士的实验，即便你只传了2000个Token的段落给大模型，它们在处理段落中间信息丢失的问题上表现并不出色。因此，如果你想要进行精确的信息检索，有时候较长的上下文窗口反而会误导用户，让用户误以为可以随意输入大量信息并精确地找到所需内容。我认为目前情况并非如此。我认为精心设计的信息检索工作流，如RAG，仍然是解决问题的关键。</p><p></p><p>基本上，无论上下文Token数量达到百万级别，甚至更长，如果考虑到企业在实际使用场景中所处理的文档数量，这样的上下文长度可能仍然不足以带来实质性的改变。但在消费者使用场景中，更长的上下文窗口确实能够显著提升信息检索的效率。</p><p></p><p>Anthony Alford：所以说回报是递减的，对吗？</p><p></p><p>Namee Oberst：确实存在回报递减的效应。这在很大程度上取决于具体的应用场景。设想一下，如果有人需要浏览上万份文档，那么增加上下文窗口的大小实际上帮助有限。大量研究表明，大语言模型并不适合作为搜索引擎使用，它们在精确检索信息方面表现并不好。因此，我个人不太推荐依赖长上下文的LLM，而更倾向于使用RAG。话虽如此，我认为在某些情况下，长上下文窗口确实非常有用。例如，当你需要传一篇很长的论文给大模型，然后要求模型对其进行重写，但这篇论文的长度超出了传统上下文窗口的处理能力……我特别喜欢用LLM来转换文档，比如将一篇Medium长文章转换成白皮书，这在以前是超出了常规上下文窗口的处理能力的。我认为这是一个非常好的应用场景。</p><p></p><p>Anthony Alford：你提到了RAG，也就是检索增强型生成技术。我们不如就来深入讨论一下这个主题。它似乎首先能够解决上下文长度的问题。此外，这看起来是一个相当普遍的应用场景。或许你可以就此发表一些看法，特别是对于小型的开放模型。现在，人们可以在本地或者自己的硬件、云平台上运行这些模型，利用RAG来解决问题，这样他们就不需要依赖那些大型的封闭模型了。Namee，你对这个问题有什么见解吗？</p><p></p><p>Namee Oberst：我非常支持这一理念。如果你看一下Hugging Face上可用的模型类型以及它们的性能基准测试，我认为这非常令人印象深刻。此外，这些开源模型的创新速度和节奏也同样令人赞叹。尽管如此，当你看着GPT-4o的推理速度和能力，以及它能够为亿万用户提供数百万种服务，你仍然会感到万分惊奇。</p><p></p><p>然而，如果你正在面对一个企业级的应用场景，你拥有明确的工作流，并且希望解决一个非常具体的问题，例如自动化特定的工作流，以自动化生成报告为例，或者是在这些预定义的10000份文档中进行RAG来实现深入的信息检索。我相信，你可以利用开源模型来解决这些问题，或者选择一个现有的较小规模的语言模型，对其进行微调，投入资源，然后基本上可以在企业私有云环境中运行这些模型，并且还可以逐渐将它们部署到边缘设备上。因此，我非常看好使用较小的模型来执行针对性任务。</p><p></p><p>Srini Penchikala：确实，几个月前我尝试用Ollama来处理一个特定的用例，我非常看好像Ollama这样的开源解决方案。你可以自行托管服务，这样你就无需将所有数据上传到云端，也不必担心数据的去向。利用这些自行托管的模型，并结合RAG技术，可以构建专有的信息知识库。我认为这种方式在企业界正获得越来越多的关注。企业希望保留数据的控制权，同时又能充分利用这项强大技术。</p><p></p><p>Roland Meertens：目前大多数企业都是以OpenAI作为起点来验证自身的商业价值，在证明存在商业价值以后，他们才可以开始思考，“我们如何将这项技术真正融入我们的应用程序？”我认为这非常棒，因为你可以很容易地开始使用这项技术，随后再构建自己的基础设施来支持应用程序的后续发展。</p><p></p><p>Srini Penchikala：是为了扩大规模，对吧，Roland？你可以评估出哪种模型最适合你的需求，对吧？</p><p></p><p>Roland Meertens：是的。</p><p></p><p>Srini Penchikala：让我们继续回到大语言模型的讨论上来。另一个值得关注的领域是多模态模型，例如GPT-4o，也就是所谓的全能模型。我认为这确实将LLM推向了一个新的高度。它不再局限于文本，我们还可以利用音频、视频或其他各种格式。那么，大家对GPT-4o或者多模态模型有什么见解吗？</p><p></p><p>Namee Oberst：为了参与这期播客，我实际上做了一项实验。我订阅了GPT-4o的服务，今天早上我出于好奇输入了几个提示词。由于我们的主要工作是基于文本的，所以并不经常使用这个功能。我要求它为LLMware生成一个新的标志，但它失败了三次，每次都无法正确处理“LLMware”这个词。尽管如此，我知道它非常令人印象深刻，并且我认为他们正在迅速取得进展。但我想看看它们目前的水平如何，今天早上对我来说体验并不佳。当然，我也知道它们可能仍然比市场上其他任何产品都要好。我先声明这一点，以免有人来找我麻烦。</p><p></p><p>Roland Meertens：在图像生成领域，我不得不说，去年我对Midjourney的表现感到非常惊讶。他们的进步速度令人惊叹，尤其是考虑到它还是一家小型公司。一家小型企业能够凭借更优秀的模型超越大型竞争者，这一现象确实令人感到惊叹。</p><p></p><p>Mandy Gu：大型公司，如OpenAI，有出色的泛化能力，并且非常擅长吸引新人才进入这一领域。然而，随着你更深入地探索，你会意识到，正如我们在人工智能和机器学习领域常说的，天下没有免费的午餐。你探索、测试、学习，然后找到适合你的方法，但并不总是那些大玩家才能做到。对我们来说，我们从多模态模型中受益最多的不是图像生成，而是OCR能力。一个非常典型的应用场景是，我们上传图像或文件，然后与大语言模型对话，尤其是针对图像内容。这已经成为我们最大的价值主张，并且深受我们开发者的喜爱。因为在很多时候，当我们在帮助最终用户或内部团队进行故障排查时，他们会发给我们堆栈信息跟踪或问题截图。能够直接将这些截图输入给模型中，而不是去解读它们，极大地节省了我们的时间。</p><p></p><p>因此，我们的价值并不仅仅来自图像生成，而是更多地来自于OCR技术的应用，它为我们带来了巨大的价值。</p><p></p><p>Srini Penchikala：这很有道理。当你采用这些技术，无论是OpenAI还是其他公司，你就会发现，在将这些技术应用到公司的具体用例时，并没有通用的解决方案。因此，每个公司都有其独特的应用场景和需求。</p><p></p><p>Daniel Dominguez：我觉得很有意思的是，现在我们看到Hugging Face上有超过80万个模型，那么明年会有多少新模型问世，这绝对是一个很有意思的话题。目前流行的趋势包括Llama、Gemma、Mistral和Stability。一年之内，不仅在文本领域，图像和视频领域也将涌现出多少新模型，这无疑是一个值得关注的点。回看过去一年的模型数量是件有趣的事情，但更令人兴奋的是，预测明年这个领域将出现的新模型数量，可能会是一个更加令人瞩目的数字。</p><p></p><p></p><h1>RAG在大规模LLM中的应用</h1><p></p><p></p><p>Srini Penchikala：没错，Daniel，你提出了一个好观点。我认为这就像20年前的应用服务器市场一样，几乎每周都有新产品问世。我认为这些产品有许多将逐渐融合，只有少数几个能够脱颖而出，并持续较长时间。说到RAG，我认为这是企业真正能够获得价值的地方，输入信息——无论是在本地还是云端——并通过大语言模型进行分析，从而获得深刻洞见。你认为有哪些RAG的实际应用案例可能会引起我们听众的兴趣？</p><p></p><p>Mandy Gu：我认为RAG是大语言模型规模化应用中最具有潜力的方向之一，其应用形态可以根据检索系统的设计而灵活变化，可以适应多样化的用例需求。在我们公司，RAG已被广泛应用于内部流程。我们开发了一个工具，它将我们的自托管大语言模型与公司所有知识库相连接。我们的文档存储在Notion中，代码托管在GitHub上，同时，我们还整合了来自帮助中心网站以及其他平台的公开资料。</p><p></p><p>我们实质上是在这些知识库之上构建了一个检索增强型生成系统。我们的设计思路是：每晚运行后台作业，从我们的知识源中抽取信息，并将它们存入我们的向量数据库。我们为员工提供了一个Web应用程序，他们可以针对这些信息提出问题或给出指令。在内部进行基准测试时，我们也发现，这种方法在相关性和准确性方面，明显优于将所有上下文信息直接输入给像Gemini 1.5这样的模型。但回到问题的核心，作为提升员工生产力的手段，RAG已经为我们带来了许多真正优秀的应用案例。</p><p></p><p>Namee Oberst：Mandy，你所分享的案例堪称经典，而且执行得非常到位，完美契合了你们的需求。这正是大语言模型强大能力的最佳体现。你还提到了一些非常有趣的内容。你说你们自托管了LLM，我想知道，你们是否采用了某个开源的LLM，或者你是否愿意分享一些这方面的信息？当然，你无需透露太多细节。不管怎样，这无疑是通用人工智能应用的一个杰出范例。</p><p></p><p>Mandy Gu：实际上，我们使用的都是开源模型，很多都是从Hugging Face获取的。我们在构建LLM平台之初，就旨在为员工提供一种安全且易于访问的方式来探索这项前沿技术。和其他许多公司一样，我们最初选择了OpenAI的服务，但为了保护敏感数据，我们在它前面加了一个个人信息保护层。然而，我们从内部用户那里得到的反馈是，这个个人信息保护层实际上限制了生成式AI最高效的用例，因为在日常工作中，员工需要处理的不仅仅是个人信息，还有大量其他类型的敏感信息。这个反馈促使我们转变了思路：从防止员工与外部供应商共享敏感信息到如何确保员工可以安全地与LLM共享这些信息。因此我们从依赖OpenAI的服务转向了自托管大语言模型。</p><p></p><p>Namee Oberst：我简直被你所做的事情震撼到了。我认为这正是我们在LLMware所追求的。实际上，这正是我们希望借助在后端串联小型语言模型进行推理所能提供的那种解决方案。你多次提到了Ollama，但我们基本上已经将Llama.cpp集成到我们的平台中，这样你就可以基于量化模型轻松、安全地进行推理。我坚信，你为你们企业设计的工作流非常出色。但同时，我也预见到其他工作流自动化的用例将会被简化，以便在笔记本电脑上运行。我几乎可以预见在非常近的未来，所有东西都将被微型化，这些大语言模型将变得更小巧，几乎成为软件的一部分，我们所有人都将能够轻松、精确且安全地在笔记本电脑上部署它们，当然，还有私有云。</p><p></p><p>Mandy Gu：你提到了Llama.cpp，我觉得这非常有趣，因为可能并不是每个人都能意识到量化模型和小模型能带来如此多的边际优势。目前，我们仍处于快速实验阶段，速度是关键。采用量化模型可能会在精度上略有损失，但我们从降低延迟和提高行动速度方面获得了回报，这对我们来说是非常值得的。我认为Llama.cpp本身就是一个巨大的成功案例，这个由个人或小团队所创造的框架，能够得到如此大规模的执行。</p><p></p><p></p><h1>AI驱动的硬件</h1><p></p><p></p><p>Namee Oberst：Llama.cpp是Georgi Gerganov开发的，他在开源领域做出了令人惊叹的贡献。Llama.cpp为Mac Metal进行了优化，但在NVIDIA CUDA上也表现出色。我们正在做的工作是，让数据科学家和机器学习团队不仅能在Mac Metal上实现解决方案，还能跨越所有AI PC平台。我们利用了Intel OpenVINO和Microsoft ONNX技术，这样数据科学家们就可以在他们喜欢的Mac上工作，然后也能轻松无缝地在其他AI PC上部署他们的模型，因为MacOS只占操作系统份额的大约15%，剩下的85%实际上是非MacOS系统。想象一下，当我们能够跨多个操作系统部署，并充分利用所有这些AI PC的GPU能力时，未来的发展将会多么激动人心。我认为，这将是未来趋势中一个非常令人期待的方向。</p><p></p><p></p><h1>小模型和边缘计算</h1><p></p><p></p><p>Srini Penchikala：你们都提到了小语言模型和边缘计算，我们或许可以就此话题展开讨论。我知道关于大语言模型，我们可以讨论很长时间，但我更想听听你们对其他主题的看法。关于小模型，Namee，你在LLMWare对SLM做了一些研究，还特别提到了一个为SLM量身定制的RAG框架。你能否更深入地谈谈这个领域？微软也在研究他们所谓的Phi-3模型。能否分享一些这方面的信息？这些模型之间有何不同？我们的听众如何能够快速了解并跟上SLM的最新发展？</p><p></p><p>Namee Oberst：实际上，我们是小模型领域的探索先锋。我们专注于小模型的研究已经有一年多，可以说相当早就开始了。实际上，RAG在过去三四年已经在数据科学和机器学习领域得到了应用。我们在公司成立初期就对RAG进行实验，并对我们的小型参数模型进行了一些非常早期的调整，我们发现可以让这些模型执行非常强大的任务，并且从中获得了性能上的显著提升。同时，我们也确保了数据的安全性和保障。这些因素始终是我考虑的重点，因为我有法律专业的背景，我最初是在一家大型律师事务所担任公司律师，后来还担任了一家公共保险经纪公司的总法律顾问。</p><p></p><p>数据安全和隐私保护一直是我们最为关注的重点。对于那些受到严格监管的行业来说，选择使用小模型或其他较小规模的模型，是一个显而易见的决定。Mandy已经详细阐述了许多原因，但成本效益同样不容忽视。实际上，成本是一个巨大的考量因素。因此，当你能够显著减少模型的资源占用并大幅降低成本时，就没有理由去部署那些庞大的模型。更令人振奋的是，越来越多的人开始认识到这一点，与此同时，小模型性能取得了显著进步。微软推出的Phi-3模型，以及我们针对RAG进行微调的模型，还有Hugging Face专为RAG设计的模型，都显示出了卓越的性能。我们使用专有数据集对这些模型进行微调，以相同的方式和数据集微调了20个模型，确保了我们可以进行公平的比较。Phi-3模型在我们的测试中表现卓越，超越了我们测试过的其他模型，包括那些拥有80亿参数的模型，成为了表现最佳的模型。</p><p></p><p>我们的模型涵盖了从10亿参数到高达80亿参数的范围，并且在精确度方面达到了前所未有的高度，这真的让我感到非常惊讶。Hugging Face上那些向全世界免费开发的小模型，正在变得越来越好，而且进步速度非常快。我认为这是一个非常激动人心的世界。正如我之前所断言的，按照这样的创新速度，这些模型将会变得越来越小，小到它们所占用的资源跟软件相当。在不久的将来，我们将会在边缘设备上部署大量这样的模型。</p><p></p><p>Srini Penchikala：确实，许多应用场景涉及线下大模型处理和线上边缘设备实时分析的组合。这正是小型语言模型能够发挥其优势的地方。Roland、Daniel或者Anthony，你们对小型语言模型有何看法？在这个领域，你们观察到了哪些趋势或发展？</p><p></p><p>Anthony Alford：确实如此。微软的Phi系列模型无疑已经成为了焦点。此外，我们也有这个议题，Namee，你提到这些模型正在变得更好。问题是，我们怎么知道它们有多好？什么样的表现才算足够好？目前有许多基准测试，比如MMLU、HELM、Chatbot Arena等，还有很多排行榜和指标。我不想说人们在操纵这些指标，但这有点像是p-hacking，不是吗？你发了一篇论文，宣称在某个特定指标上超越了其他基线，但这并不总能直接转化为实际的商业价值。因此，我认为这仍然是一个需要解决的问题。</p><p></p><p>Namee Oberst：实际上，我们做了一套内部基准测试，专注于评估模型回答一些基于常识的商业和法律问题的能力，这些问题都是基于事实的。我们的平台主要是面向企业用户，因此在这个场景下，我们更关注模型对事实性问题、基本逻辑和数学问题的回答能力，而不是创造力。我们甚至创建了自己的基准测试方法，Phi-3模型的结果就是基于这些测试得出的。我对一些公布的结果持怀疑态度，你真的看过HellaSwag上的一些问题吗？有时候我甚至不知道正确或错误的答案是什么。因此，我们决定开发自己的测试标准，而我们讨论的Phi-3模型的表现正是基于这些我们自己制定的标准。顺便说一句，微软并没有赞助我们，尽管我希望他们能。</p><p></p><p>Srini Penchikala：我们很快会开始讨论大模型的评估，在这之前，你们对语言模型还有什么看法吗？</p><p></p><p>Roland Meertens：Phi让我印象深刻的一个点是，它在训练过程中不仅使用了高质量的数据，还通过自主生成数据来提升学习效果。例如，在编程方面，他们让Phi为学生编写指导手册，然后利用这些手册作为训练数据。这让我深刻体会到，如果你拥有更优质的数据，并且能够精心挑选这些数据，将能够训练出更为出色的模型。</p><p></p><p>Anthony Alford：你是说”Textbooks Are All You Need“吗？</p><p></p><p>Roland Meertens：除此之外，Hugging Face的团队成员也发表了多篇相关论文。目前，对于如何选择合适的数据来训练这些模型，人们表现出了极大的兴趣。在我看来，数据选择在机器学习领域仍然是一项被低估且值得深入探讨的课题。</p><p></p><p>Srini Penchikala：除了Phi，Daniel，你之前提到了TinyLlama。关于这些小模型，你有何见解或要评价的？</p><p></p><p>Daniel Dominguez：确实，正如Namee所言，目前在Hugging Face平台上的很多语言模型还有许多未知领域值得我们去探索。此外，Hugging Face的一个吸引人之处在于他们对不同性能级别的GPU进行了分类，你可能已经注意到了他们在排行榜上的目标设定。根据你的硬件配置，可能会被归类为”富GPU“用户或”穷GPU“用户，但不论哪种情况，你都能够运行这些语言模型。同时，我们也要感谢目前行业所提供的芯片技术，例如NVIDIA的芯片，它们不仅能够在云端运行这些小模型，也能够在低端个人计算机GPU和系统上运行。</p><p></p><p>得益于NVIDIA等公司提供的高性能GPU，这些小模型得以顺利运行。在Hugging Face平台上，当你看着这些模拟演示时，你会发现无需依赖庞大的计算资源即可在自己的设备上运行这些模型，这无疑是一个令人兴奋的发现。</p><p></p><p>Srini Penchikala：还有很多其他的AI创新正在发生，在结束语言模型讨论之前，我们快速再聊一下评估问题。除了基准测试指标，这些我们可能需要谨慎对待的东西，我想知道在现实世界中的最佳实践是怎样的？正如你提到的，Daniel，面对众多的模型，一个新入行者如何评估并比较这些模型，排除那些可能不适合他们的，并选择适合他们的？你有没有注意到在这个领域有哪些行业实践或标准？</p><p></p><p>Mandy Gu：我认为Anthony提到的商业价值是一个值得我们在评估过程中考虑的要点。尽管我对那些通用的基准测试持保留态度，但我认为我们真正需要做的是全面评估大型语言模型，不仅包括基础模型本身，还涉及到使用的技术以及我们如何针对特定任务来协调整个系统。例如，如果我的目标是总结一篇研究论文并提炼其语言，我就应该针对这一特定任务来评估LLM的能力。毕竟，没有一套模型或技术能够适用于所有任务。通过这个实验过程，我可以更有信心地找到最适合的模型组合。归根结底，如何更准确地量化评估结果，应该基于对当前任务的评估和我们期望看到的成果。</p><p></p><p></p><h1>AI智能体</h1><p></p><p></p><p>Srini Penchikala：接下来我们聊聊AI智能体。据我所知，这一领域已经取得了显著进展，特别是在AI驱动的编程助手方面。Roland，你对此有何见解？我知道你已经对Copilot等工具进行了深入研究。</p><p></p><p>Roland Meertens：去年你问我对未来一年的趋势有何看法，我预测是AI智能体。但现在看来，我说的可能并不完全准确。我们看到智能体技术确实有所发展。OpenAI之前推出了GPT Store，允许用户自行创建个性化的智能体。然而，坦白地说，我还没有听到有人向我强烈推荐某个智能体，说它非常出色。所以，从这个角度来看，我认为目前的进步还是有限的。不过，我们确实看到了一些有趣的应用，例如Devin，一个AI软件工程师智能体，它有一个终端、代码编辑器和浏览器，你可以给它分配任务，比如：“嘿，试着解决这个问题。”它会尝试独立完成所有工作。目前，Devin的成功率大约是20%，但考虑到它是免费的，这个成功率对于一个免费的”软件工程师“来说已经相当令人满意了。</p><p></p><p>此外，还有一些像AgentGPT这样的平台，我让它为AI趋势博客创建一个大纲，它提出了一些话题，比如：“我们可以讨论CNN和RNN等趋势。”我不认为这些还是趋势，但它对这些话题仍然充满热情，这是件好事。但总的来说，我认为智能体仍然有巨大的潜力。如果你想完成某项任务，完全可以进行自动化，而不是我自己去决定使用ChatGPT发送哪封电子邮件，然后发送它，接着等待对方回复并用ChatGPT总结，再写回复。</p><p></p><p>Anthony Alford：我的疑问在于，究竟是什么定义了“智能体”？</p><p></p><p>Roland Meertens：这是个好问题。所以我认为，就我目前所看到的，智能体是一种能够整合并执行多种任务的东西。</p><p></p><p>Anthony Alford：在念研究生时，我的研究领域是智能代理。我们所谈论的智能体主要是关于自主性。所以我认为，AI安全领域的专家们所担忧的，可能就是赋予这些系统自主性。不管你对AI的未来发展持何种看法，关注自主性问题都是非常合理的。目前来看，ChatGPT可能还没有达到实现完全自主性的水平。</p><p></p><p>Roland Meertens：这取决于你想做什么，以及你愿意在多大程度上让渡自己的控制权。就我个人而言，我还不太愿意在工作中部署一个完全自主的“Roland智能体”。我觉得它可能不会表现得特别智能。但我看到有人在约会应用上这么做了，显然，他们愿意冒这个险。</p><p></p><p>Daniel Dominguez：正如Roland所说的，智能体还没有真正掀起大浪，但可以肯定的是，它们在未来一定会发生些什么。比如，扎克伯格最近提到，他们正在为小型企业开发新的Meta AI智能体，这些智能体将帮助小企业主在自己的业务领域实现自动化。Hugging Face也有许多AI智能体，用于日常的工作流。Slack也集成了许多AI智能体，用于帮助用户总结对话内容、任务以及日常的工作流等。</p><p></p><p>我认为，随着我们在这一领域不断进步，AI智能体在日常工作和小型企业中的应用将变得更加自然。因为它们将极大地帮助我们完成许多日常任务，越来越多的公司也将开始在自己的平台上推出各式各样的智能体服务。例如，据我所知，谷歌即将推出用于Gmail等任务的AI智能体服务。因此，这可能是在接下里的一年加速发展的一个趋势。</p><p></p><p>Roland Meertens：确实，特别是你可以借助Langchain，让事情变得相当容易：”我有这些API可以调用，我想要实现这样的工作流程。如果你能够实现，就执行相应的操作。如果无法实现，就使用另一个API。“将工具箱中的所有工具进行组合并实现自动化，这种能力是非常强大的。</p><p></p><p>Mandy Gu：你说到点上了。以Gmail为例，有一个嵌入式助手可以帮你管理电子邮件，你就不需要去ChatGPT那里问如何增强邮件，或者做你想做的任何其他事情。从行为学角度来看，让信息在不同平台之间流转是一个巨大的工作负担，如果我们能够减少用户完成他们的工作所需要打开的标签页或需要访问的系统，这将是一个巨大的进步。而真正推动智能体采用的，就是这些因素。</p><p></p><p>Srini Penchikala：如果这些智能体能帮助我们决定何时发送电子邮件，何时不发送而是改为打电话，那就很厉害了。我的意思是，那样可能会更有效率，对吧？</p><p></p><p>Roland Meertens：我在思考趋势的问题。在去年，每一家公司都宣称：“我们现在是一家AI公司。我们将拥有自己的聊天机器人。”我甚至看到一些同事说：“我想证明这个论点，我让ChatGPT为我生成了三页的论点，看起来不错。”但我现在不想关心你的论点是什么，我不想和聊天机器人聊天，我只想浏览网站。所以我也好奇，最终会出现什么样的结果？每一家公司、每一个网站都会变成一个聊天机器人吗？或者我们是否也可以直接查找一本书的价格，而不是必须要求智能体为我们订购它？</p><p></p><p>Srini Penchikala：我们不应该过度智能体化我们的应用程序，对吧？</p><p></p><p>Roland Meertens：我的建议是，不要让你的生活变得过度智能体化。</p><p></p><p></p><h1>Ai安全</h1><p></p><p></p><p>Srini Penchikala：Anthony，你之前提到了人工智能的安全性问题，接下来就让我们深入探讨一下安全性。Namee和Mandy，你们都在多个实际项目中有所涉猎。你们如何看待安全与创新之间的关系？我们怎样才能确保这些开创性的技术在保持隐私和消费者数据安全的同时给我们带来价值？</p><p></p><p>Mandy Gu：生成式人工智能确实在安全领域引发了一系列连锁反应，例如第四方数据共享和数据隐私问题，这些问题日益严重。我们与许多SaaS供应商合作，这些供应商也是许多公司的选择。他们通常会集成人工智能技术，但并不总是会明确告知，实际上很多时候，他们会将用户数据发给OpenAI。根据数据的敏感程度，这可能是用户希望避免的。因此，我认为我们需要关注两点。首先，我们需要全面了解和追踪我们的数据流向。随着人工智能集成的普及，这项工作变得更加复杂，我们必须牢记这一点。其次，如果我们希望员工遵循正确的数据隐私安全实践，就必须让他们选择最简单、最安全的路径。</p><p></p><p>回到我之前提到的例子，如果我们在与OpenAI和其他供应商的所有对话中都叠加一个极其严格的个人身份信息（PII）审查机制，这可能会让使用者感到挫败，他们可能会直接去使用ChatGPT。但如果我们能够为他们提供替代方案，并通过激励措施使这些替代方案更加易于使用，或者增加他们需要的其他功能，同时确保安全选项是最容易实施的路径，这样就能吸引他们，并逐步建立起一种积极、注重数据隐私的良好文化。</p><p></p><p>Namee Oberst：是的，Mandy，你描述的工作流实际上凸显了我在讨论数据安全时经常强调的一个观点：在企业当中，生成式人工智能工作流的设计对所有的敏感数据安全性都有重大影响。是否有供应商可能会无意中将我们的敏感数据发送给一个我们不信任的供应商，例如OpenAI，这只是一个例子。我们需要审视这些问题，需要审视数据的来源，需要确保工作流具备可审计性，这样就可以追溯所有推理之间发生的交互。人工智能的可解释性如何发挥作用？我设计的工作流是否存在潜在的攻击面？如何处理提示词注入问题？</p><p></p><p>顺便提一个有趣的事实，由于经常处理小规模任务，小模型能够很好地泛化，因此不太容易受提示词注入的影响。但我们仍然需要关注提示词注入、数据投毒等问题。所以我认为，企业在部署人工智能时需要考虑诸多因素。Mandy，你刚才提出的观点非常中肯。</p><p></p><p>Mandy Gu：你提到的攻击面问题，我非常认同，因为这确实是一个可能迅速失控的方面。有人将生成式人工智能及其集成比作有线电视与流媒体服务，因为众多公司都在推出自己的人工智能集成服务，购买所有这些服务就像同时订阅Netflix、Hulu以及其他所有流媒体服务，不仅成本不划算，而且确实增加了潜在的攻击面。我认为，这正是我们在权衡自行构建与购买时需要考虑的，并且对我们所支付的费用以及数据的去向要有清晰的认识和审慎的决策。</p><p></p><p>我注意到人们对于这些问题的普遍认识正在逐步提高。供应商，尤其是SaaS提供商，正在积极回应这些关切。越来越多的服务提供商开始提供这样的选项：“我们可以将服务托管在你的虚拟私有云（VPC）中。无论是在AWS还是GCP上，都可以运行Gemini，确保你的数据仍然保留在你的云租户内。”我认为这正是在安全意识方面所展现的一个积极趋势。</p><p></p><p></p><h1>LangOps或LLMOps</h1><p></p><p></p><p>Srini Penchikala：除了安全性之外，我们需要关注的另一个重要问题是如何在生产环境中管理这些大语言模型和人工智能技术？所有，让我们迅速进入LangOps或LLMOps这个话题。这一领域有几种不同的术语并存。Mandy，或许你可以先分享一下你的观点。你如何看待当前LLM在生产环境中的支持情况，以及有哪些宝贵的经验？</p><p></p><p>Mandy Gu：在WealthSimple，我们把LLM的工作分为三个明显不同的领域。首先是提升员工的工作效率，其次是优化客户业务流程，第三是基础的LLMOps，我们更愿意称之为LLM平台工作，它为前两个领域提供支持。我们在这方面积累了许多经验，对我们来说行之有效的是我们的赋能理念。我们的工作以安全性、可访问性和选择性为中心。我们的目标是为用户提供可选择性，让每个人都能为手头的任务选择最合适的技术和基础模型，帮助我们避免了这个领域常见的一个问题，即人们将LLM视为寻找问题的解决方案（拿着锤子找钉子）。通过提供这些可复用的平台组件，生成式AI的采纳变得更加普遍。</p><p></p><p>这是一个我们逐渐才领悟到的教训。在我们刚开始踏上LLM之旅时，我们构建了一个LLM网关，它有审计跟踪功能，让人们能够安全地使用OpenAI和其他供应商的服务。我们收到的反馈是，审计跟踪功能在很多实际应用场景中对他们造成了限制。因此，我们开始自托管模型，这样我们就可以轻松地加入开源模型，进行微调，然后将其集成到我们的平台中，并通过LLM网关为我们的系统和最终用户提供推理服务。然后我们开始构建检索功能作为可复用的API，并围绕向量数据库构建框架，增强可访问性。随着我们逐渐将这些组件平台化，我们的最终用户——包括科学家、开发者以及业务人员——开始尝试并发现：“这个工作流实际上可以通过LLM得到显著改进。”这时，我们就会介入，帮助他们将这些想法产品化，并实现大规模的产品部署。</p><p></p><p></p><h1>AI发展趋势预测</h1><p></p><p></p><p>Srini Penchikala：我们即将结束这次讨论，这是一次非常精彩的讨论。在结束之前，我想向在座的各位提出一个问题：你们对人工智能领域在未来12个月内可能发生的事情有怎样的预测？当我们明年再次聚在一起讨论时，可以回顾并讨论这些预测的实现情况。</p><p></p><p>Mandy Gu：我认为，围绕大模型的许多炒作将会逐渐平息。我们在过去一年半的时间里目睹了它们惊人的增长。对于许多企业和行业来说，LLM仍然是一个他们愿意持续投入的赌注。</p><p>然而，我认为在未来的12个月里，这种情况将会有所改变，我们将开始对这项技术设定更为现实的预期，并在期望获得具体成果之前，更加审慎地评估我们的探索深度。因此，我预测从现在开始的12个月内，LLM炒作将会减少，那些继续采用这项技术的公司将会找到切实可行的方法，将其无缝集成到他们的工作流或产品中。</p><p></p><p>Daniel Dominguez：我预测，随着人工智能不断产生海量数据，它将与区块链等技术有某种形式的融合。我已经注意到许多区块链项目已经开始探索与人工智能的数据整合。虽然区块链和人工智能的融合目前还处于早期阶段，但在未来将会取得显著进展，尤其是在数据管理方面。因此，我认为人工智能与区块链的结合将是未来技术发展的一个重要趋势。</p><p></p><p>Roland Meertens：我仍然对机器人技术抱有期待，不过现在我们更倾向于称之为具身人工智能。这是去年逐渐流行起来的一个新术语。我不确定什么时候会发生，智能体已经能为我们执行计算机任务，如果我们把它们放到机器人的身体里，它们还会帮我们干活。具身人工智能无疑将成为下一个重要的大事。</p><p></p><p>Srini Penchikala：看来这些机器人将成为你的付费程序员，对吗？</p><p></p><p>Roland Meertens：不是这样。智能体将成为你的编程伙伴，而机器人则会在日常生活中为你提供帮助。我好奇的是，现在的公司拥有大量的数据，他们是否会利用这些数据来微调自己的模型并将其商业化？或者继续使用RAG？设想一下，如果你是一个园艺师，多年来一直在拍摄花园的照片，并提供如何改善花园的建议。肯定有很多小型企业拥有这样的数据，他们将如何从这些数据中获取价值？我非常好奇这些小型企业将如何利用他们的数据，以及如何构建自己的智能体、聊天机器人或AI自动化解决方案。</p><p></p><p>Anthony Alford：人工智能寒冬，Mandy已经提到了，不是吗？她说“我们可能会看到炒作的热度逐渐降低”，这是“温和”版本的寒冬。而“强烈”版本的寒冬，或许你已经看到过这样的标题，我记得是《自然》杂志上的一篇论文，它指出：“如果你用生成式AI生成的内容来训练生成式AI，结果可能会变得更糟。”我认为人们已经开始思考互联网是否正在被这些生成式内容污染。让我们拭目以待。我真心希望我的担忧是多余的，我真心不希望这个预测会成为现实。</p><p></p><p>Srini Penchikala：这是非常可能的，对吧？Namee，你对接下来的12个月有怎样的预测？</p><p></p><p>Namee Oberst：我预测我们将会经历一些Anthony和Mandy所描述的情况，但很快会过渡到更有价值、更加现实和具体的应用场景上，包括更自动化的工作流、智能体工作流，以及进一步扩展到边缘设备，比如笔记本电脑和智能手机。这就是我的预测，这将会很有趣。</p><p></p><p>Srini Penchikala：是的，这将会很有趣，这也是我所预测的。我相信我们将看到更多融合、端到端、全面的人工智能解决方案，它们结合了小模型、RAG技术和人工智能硬件。我认为许多积极的变化正在发生。我希望所谓的人工智能寒冬不会持续太久。</p><p></p><p></p><h1>相关资源</h1><p></p><p>论文“<a href="https://arxiv.org/abs/2306.11644?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA">Textbooks Are All You Need</a>"”<a href="https://arxiv.org/abs/2301.03988?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA">SantaCoder: don't reach for the stars!</a>"</p><p></p><h1>嘉宾简介</h1><p></p><p>Mandy Gu</p><p>Mandy Gu是Wealthsimple的高级软件开发经理，负责领导机器学习和数据工程团队。此前，她拥有丰富的自然语言处理（NLP）和数据科学方面的工作经验。</p><p></p><p>Namee Oberst</p><p>Namee Oberst是一家专注于生成式和开源人工智能解决方案的初创公司的创始人。</p><p></p><p>Srini Penchikala</p><p>Srini Penchikala是一位资深的软件架构师，并担任InfoQ人工智能、机器学习与数据工程板块的主编。著有《Apache Spark大数据处理》和《Spring Roo实战》（合著者）。</p><p></p><p>Roland Meertens</p><p>Roland是一位机器学习工程师，在自动驾驶汽车领域深耕计算机视觉技术。此前，他曾在社交媒体平台、深度学习自然语言处理、社交机器人以及无人机领域从事计算机视觉方面的工作。</p><p></p><p>Anthony Alford</p><p>Anthony是Genesys高级开发总监，在设计和构建大规模软件方面拥有超过20年的经验。</p><p></p><p>Daniel Dominguez</p><p>Daniel是华盛顿大学机器学习专业的工程师，拥有超过12年的软件产品开发经验。</p><p></p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p>查看英文原文：<a href="https://www.infoq.com/podcasts/ai-ml-data-engineering-trends-2024/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjQ3MjU1NzUsImZpbGVHVUlEIjoiNXhrR285WGFaOWl3YmRrWCIsImlhdCI6MTcyNDcyNTI3NSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo1MDA3OTA2fQ.Yz94KIsoXLBD2SJdXI3XorrO16q22wtoNxNOOxp8CHA">https://www.infoq.com/podcasts/ai-ml-data-engineering-trends-2024/</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0</id>
            <title>融到2.2 亿美元才3个月就“闹崩”！5个创始人走了3个，这家 DeepMind 系创企一款AI产品都还没发！</title>
            <link>https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/MoLIruT3QsJTzA5CLxm0</guid>
            <pubDate></pubDate>
            <updated>Tue, 27 Aug 2024 01:50:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫</p><p></p><p>日前，法国人工智能 （AI） 初创公司 H 表示，由于“运营分歧”，其三名联合创始人 Daan Wierstra、Karl Tuyls 和 Julien Perolat 将离开公司。在 5 月 21 日宣布品牌重塑之前，H 被称为 Holistic AI。</p><p></p><p>H 在 LinkedIn 上的一篇帖子中表示，“公司将由首席执行官 Charles A. Kantor 和首席技术官 Laurent Sifre 领导。虽然这对所有相关方来说都是一个艰难的决定，但大家都一致认为，这将使公司在未来取得最大的成功。H 将继续得到投资者和战略合作伙伴的全力支持。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ae/ae7a78a1f304efa0b824ad3475aebba8.png" /></p><p></p><p>据悉，这三位联合创始人离开之际， H 筹集了 2.2 亿美元的种子轮融资后仅三个月，还未发布过任何产品。而 H 是在今年早些时候成立，计划在今年年底前发布一系列模型和产品。</p><p></p><p>“当 H 成立时，团队着手通过新一代动作模型将 GenAI 的力量推向全球人民和企业。”该公司在帖子中表示。“今天，H 的近 40 名工程师和研究人员团队仍然致力于这一愿景，开发尖端的动作能力，以提高工人的生产力并推动 AI 研究和工程的前沿。”</p><p></p><p>五人创始团队悄然“分家”，</p><p></p><p>无一人对此回应</p><p></p><p>根据该公司的 LinkedIn 帖子，H 现在拥有一支由 40 名工程师和研究人员组成的团队。相比之下，另一家资金雄厚的人工智能公司 Mistral AI 在招聘方面要保守得多。初成立之时， Mistral 除三位联合创始人外只有 3 名成员，团队总人数不到 10 人。</p><p></p><p>创立之初， H 有五位联合创始人，其中一位联合创始人兼该公司现任首席执行官 Charles A. &nbsp;Kantor 是斯坦福大学的计算数学研究员，而其他四位联合创始人都是谷歌旗下的人工智能公司 DeepMind 的资深科学家出身。</p><p></p><p>Karl Tuyls 是多智能体系统社区的著名科学家，自 2017 年起领导 DeepMind 的博弈论和多智能体团队，发起并领导了 DeepNash（一款在 Stratego 上击败人类专家玩家的自主智能体）和 TacticAI（一款角球自动助理足球教练）等多个著名项目，这些项目均发表在《科学》和《自然》杂志上。</p><p></p><p>Laurent Sifre 曾是 DeepMind 的首席科学家，在 DeepMind 工作了十年，为 AlphaGo、AlphaFold 和 AlphaStar、Chinchilla、Gemini 和 Gemma 等 GenAI 和深度神经网络的关键研究项目做出了贡献 。</p><p></p><p>Daan Wierstra 是 DeepMind 的一名高级计算机科学家，在 DeepMind 被谷歌收购之前就加入了该公司。在 DeepMind，Daan 曾领导了一支 100 多人的团队多年，并在 Deepmind 确立最初的研究方向方面发挥了关键作用。</p><p></p><p>Julien 从事博弈论和多智能体研究，共同领导了 DeepMind 在 Stratego 游戏（DeepNash）方面的科学和技术开发，以及在平均场博弈和基于人群的学习等主题上的许多其他基础性工作。</p><p></p><p>现在，从谷歌 DeepMind 转投 H 的四位联合创始人中的三位都将离开该公司。自人工智能爆火以来，业内闹过“分家”的知名 AI 企业不在少数。最近令不少人都仍记忆犹新的一家便是 OpenAI 了，11 人创始团队分崩离析至仅剩两人，分裂过程中内部发生多起“政变”。从去年 11 月首席执行官山姆·奥特曼（Sam Altman）被罢免以来，OpenAI 已经陷入大半年的“人事斗争”。</p><p></p><p>但 H 公司的“分家”却不同于此，除官方在 LinkedIn 上发布的公告帖以外，此前并未有任何公开的讨论和发言，连三位处于风波中心、将离开的联合创始人也未曾有过任何相关回应。</p><p></p><p>Tuyls 最近提及 H 的社交内容更新停留在 5 月 26 日，从其当时的状态看，他本人还沉浸在 H 公司成立的喜悦中，之后发布的帖子也未透露出要离开 H 或产生公司业务分歧的迹象。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8b/8b00b4b830fc8e4500355044c08e40e8.png" /></p><p></p><p>唯一有所异常的是名为 @Daan Wierstra 的账号对外关闭了其社交内容页的展示，但尚无法确定是否是他本人。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/71/71e373711eb778a9d112fca99b5526b0.png" /></p><p></p><p>众多亿万富翁投资，</p><p></p><p>H 公司是做什么的？</p><p></p><p>由于其创始人的背景和筹集到的资金数额，H 一直是法国最热门的 AI 初创公司之一，与 Mistral 和 Poolside 等公司并驾齐驱。毕竟，很少能听到超过 1000 万美元的种子轮融资。</p><p></p><p>就在三个月前，H 筹集了法国有史以来最大的种子轮融资之一，从包括全球风险投资公司 Accel 在内的投资者那里获得了 2.2 亿美元。该公司没有透露其中有多少是股权投资，多少是债务投资，但根据之前的报道，这一轮投资由多达 1.2 亿美元的可转换票据组成。这些是债务融资的一种形式，在满足特定条件后可以转换为股权。</p><p></p><p>今年 6 月，海外机构 PYMNTS 报告称，H 是引起投资者注意并吸引投资的人工智能代理初创公司之一，其估值领先于其业务基本面。</p><p></p><p>据介绍，这家初创公司的投资者包括众多亿万富翁（或其家族办公室）、一些知名风险投资基金和战略支持者。其中，亿万富翁名单里有前谷歌老板 Eric Schmidt、Courrier international 所属的 Le Monde 集团的个人股东 Xavier Niel、硅谷领先的风险投资家之一 Yuri Milner、Bernard Arnault（通过他的 Aglaé Ventures 基金）和 Motier Ventures（老佛爷百货集团所有者的家族办公室）等知名人士。</p><p></p><p>在风险投资名单上，投资者包括 Accel、法国巴黎银行的大型风险基金、Creandum、Elaia Partners、Eurazeo、FirstMark Capital 和 Visionaries Club。此外，还有一些产业投资者，包括亚马逊和三星。</p><p></p><p>有趣的是，总部位于纽约的机器人自动化软件公司 UiPath 也是 H 公司的投资者，这家欧洲机器人独角兽公司将在商业化和合作伙伴关系方面为 H 提供帮助。</p><p></p><p>据了解，H 建立的基础模型被称为 “代理”（agentic），这是一种旨在将任务分解为多个步骤并执行这些子任务，而不仅仅是一次一次地响应提示的人工智能。这家初创公司表示，其模型将比竞争对手的模型更有能力进行推理、规划和协作，致力于为商业和消费者垂直领域提供服务。</p><p></p><p>Kantor 曾表示，H 公司正在努力实现完全的人工通用智能（full-AGI），即与人类能力相当或超过人类能力的 AI 水平，能够完成各种任务。但老实说，这只是一个营销承诺，因为没人知道 AGI 是否或何时会实现。现实是，H 还需要筹集大量资金来支付计算能力和数据集的费用。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.pymnts.com/personnel/2024/3-co-founders-leave-french-ai-startup-h-amid-operational-differences/">https://www.pymnts.com/personnel/2024/3-co-founders-leave-french-ai-startup-h-amid-operational-differences/</a>"</p><p></p><p><a href="https://techcrunch.com/2024/05/21/french-ai-startup-h-raises-220-million-seed-round/">https://techcrunch.com/2024/05/21/french-ai-startup-h-raises-220-million-seed-round/</a>"</p><p></p><p><a href="https://sifted.eu/articles/three-cofounders-leave-h-news">https://sifted.eu/articles/three-cofounders-leave-h-news</a>"</p><p></p><p><a href="https://www.accel.com/noteworthy/building-foundational-models-to-generate-actions-our-partnership-with-the-h-company">https://www.accel.com/noteworthy/building-foundational-models-to-generate-actions-our-partnership-with-the-h-company</a>"</p><p></p><p>内容推荐</p><p></p><p>2024年8月18-19日，AICon 全球人工智能开发与应用大会·上海站成功举办，汇聚超过60位大模型行业先锋，全方位剖析大模型训练与推理机制、多模态融合、智能体Agent前沿进展、检索增强（RAG）生成策略、端侧模型优化与应用等热点内容。经过嘉宾授权，「AI前线」为你独家整理了一份演讲PPT合集，不容错过。关注「AI前线」，回复关键词「PPT」免费获取。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/81/814c1f9a6b667134f3520e04d6d8dfc7.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/791c6d47a29abdea4f3ba09bea3b176a.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622338&amp;idx=1&amp;sn=4c17f35ca45df4107a8830186480c690&amp;chksm=fbeba70dcc9c2e1b4312a0642eea90bc67da383b4f3fd4e94b81bdd3eb9f29b9c76dae5b5de4&amp;scene=21#wechat_redirect">《黑神话：悟空》被指抄袭，原作者开撕；IBM中国被曝数千研发权限突然被关；曝360儿童手表智能回答毁三观，周鸿祎道歉 | AI周报</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622313&amp;idx=1&amp;sn=1fbaa02128849c257d476d2f64fd5683&amp;chksm=fbeba766cc9c2e7067f35b00df38c29cdd8e813e7fbb10eb1c843f6e4274ba5f8f874e088ef0&amp;scene=21#wechat_redirect">《黑神话：悟空》开发者遭猎头疯抢，联创发声求放过：你们不缺人才，别搞我们</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247622096&amp;idx=1&amp;sn=a6d564c645e7644023d4142365fc19b5&amp;chksm=fbeba41fcc9c2d093abdaa2dbe6cac44301d4070a5f580572925ad46b28aa9bb0574062de232&amp;scene=21#wechat_redirect">《黑神话：悟空》的第二个受害者出现了，竟是AI搜索惹的祸！</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=1&amp;sn=5e58c5a72a2d7fae816471954959b349&amp;chksm=fbeba49ccc9c2d8a35501b45bea911c9b684944634522011a859022dad48c9fda4b3a4e3b8fc&amp;scene=21#wechat_redirect">《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621831&amp;idx=1&amp;sn=5ff4ba1979a3e77a914e8b6c5d390db7&amp;chksm=fbeba508cc9c2c1e83c061d1dbd107dca94d93ccda4172996c67d7920e2846d39123b77ee22b&amp;scene=21#wechat_redirect">“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/TqUxrdlKdpfVDHPCs001</id>
            <title>李沐上海交大演讲：创业好酷，有“当海盗”的乐趣</title>
            <link>https://www.infoq.cn/article/TqUxrdlKdpfVDHPCs001</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/TqUxrdlKdpfVDHPCs001</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 09:12:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p></p><blockquote>8月23日，李沐回到了母校上海交大，做了一场关于 LLM 和个人生涯的分享。这篇文章是对李沐在上海交通大学的演讲内容的总结，涵盖了AI技术的现状、未来趋势以及个人成长的深刻洞察。。</blockquote><p></p><p></p><h2>关于语言模型</h2><p></p><p>&nbsp;</p><p>李沐首先研究了语言模型的三个核心要素：算力、数据和算法，认为其与机器学习模型类似，本质上都是把数据通过算力和算法压进中间的模型里，使得模型拥有一定的能力，在面对一个新的数据时，它能够在原数据里面找到相似的东西，然后做一定的修改，输出想要的东西。</p><p>&nbsp;</p><p>但他指出，这次的语言模型和上一次深度学习浪潮的模型有一个比较大的区别：上一次是“我炼一个什么丹就治一个什么病”，这次是“我希望这个东西炼出来会有灵魂在里面”，它能解决很多问题，“这其实是技术一代代往前进。”</p><p>&nbsp;</p><p>在他看来，目前语音模型的优点是延迟更低、信息更丰富，并能够通过语言模型对整个输出做很多控制；音乐模型的问题不在于技术，而是商业问题；图片生成是整个 AIGC 领域做得最早的，也是效果最好的；视频模型则比较早期，通用的 video 生成非常贵，训练成本很有可能低于数据处理的成本，所以市面上没有特别好的开源模型出来。而多模态技术的发展趋势在于整合不同类型的模态信息，尤其是文本信息，未来通过文本控制生成某个模块可能成为常态。</p><p>&nbsp;</p><p>“总结下来，我觉得语言模型已经达到了较高的水平，大约在 80 到 85 分之间。音频模型在可接受的水平，处于能用阶段，大约在 70-80 分之间。但在视频生成方面，尤其是生成具有特定功能的视频尚显不足，整体水平大约在 50 分左右。”李沐说道。</p><p>&nbsp;</p><p>在硬件方面，李沐特别强调了带宽的重要性，并预测下一代的带宽将翻倍至800Gigabits。他还提到了英伟达的GB200系统，这是一个尝试通过水冷工艺提高算力密度的创新设计。“一旦用到水冷之后，你的算力就可以更密，就可以放更多机器。”李沐表示。</p><p>&nbsp;</p><p>内存方面，他认为内存大小将是模型发展的主要瓶颈，因为当前的内存技术限制了模型的规模。他表示，“受限于内存大小和数据的尺寸，100B 到 500B 会是未来主流的一个大势。你可以做更大，但是它很多时候是用 MoE 做的，它的有效大小（每次激活的大小）可能也就是 500B 的样子。”</p><p>&nbsp;</p><p>另外，他预计算力将由于摩尔定律变得越来越便宜。“短期来看，算力翻倍，价格可能会有 1.4 倍的提升。但是长期来看，当竞争变得越来越激烈，摩尔定律会发挥作用，就是说算力翻倍，价格不一定变。所以长期来看算力会变得越来越便宜。”</p><p>&nbsp;</p><p></p><h2>三种 AI 应用</h2><p></p><p>&nbsp;</p><p>李沐将人工智能的应用分为三类：</p><p>&nbsp;</p><p>文科白领，这方面做的比较好的包括个人助理、Call centers、文本处理、游戏和舆论以及教育。一个文科白领可能一小时完成的事情，模型能够完成百分之八九十。工科白领，目前 AI 想取代程序员还早得很。模型现在做的事是直接在其训练数据中检索相关的代码片段，根据上下文，再把变量名改一改。但它不是真的在写代码，人类一个小时还是能够写出很多复杂的代码的，所以模型还是没有取代工科白领一个小时干的事情，更不用说更复杂的任务了。蓝领阶级，这是最难的，唯一做得好的是自动驾驶。放眼整个世界，蓝领是最主要的成员，因此技术对这个世界做出巨大的变革还需要很多年。未来 10 年、 20 年，大家还是有机会参与进来的。</p><p>&nbsp;</p><p>“对于文科白领的工作，AI 已经能完成简单任务，复杂任务需要继续努力。对于工科白领的工作，简单任务还需要努力，复杂任务存在困难。对于蓝领的工作，除了无人驾驶和特定场景（比如工厂，场景变化不大，也能采集大量数据），AI 连简单任务都做不了，完成复杂任务更难。”李沐总结道。</p><p>&nbsp;</p><p>此外，他也分享了一些创业后得到的技术细节，比如预训练已经成为工程问题，后训练才是技术问题；垂直模型也需要通用知识；评估很难，但很重要；数据决定模型上限；自建机房不会比租 GPU 便宜太多等。</p><p>&nbsp;</p><p></p><h2>创业与职业发展的感悟</h2><p></p><p>&nbsp;</p><p>李沐分享了他从上海交通大学毕业后的多样化经历，包括在大公司工作、读PhD和创业。他强调了在不同环境中工作的目标和动机的重要性，并讨论了每种职业道路的利弊。他建议，无论是选择哪种职业道路，都需要有一个强烈的动机，并能够直面挑战。</p><p>&nbsp;</p><p>他提到，做一个“打工人”的好处是，可以在一个相对简单的环境里学习各种从业知识，比如一个技术如何落地、产品怎么做出来、怎么设计、怎么运营、怎么管理；其次是干完被安排的任务后，晚上睡觉不用太担心其他，不会做噩梦；还有就是相对稳定的收入和空余时间。</p><p>&nbsp;</p><p>那么做“打工人”的坏处就是停留在打工人或者职业经理人的思维。“公司从最上层把整个复杂的世界抽象成简单的任务，待得越久，就越觉得自己是螺丝钉，当然螺丝钉的好处就是，只要找到一个螺母钉上去就行，不用管这个机器多么复杂，外面世界多么复杂，但你在一个简化的世界里干得越久，就会觉得很腻，学的也越少，这就导致你一直停留在一个打工人或者职业经理人的思维里，而不是站在一个更高更广的层次去思考。”</p><p>&nbsp;</p><p>而对于创业，他表示，“创业好酷。好处是有当海盗的乐趣。”他解释道，“天天看市面上有什么东西，天天跟人聊有什么机会，机会来了是不是要 all in 搏一把，海盗太多，你不 all in ，机会就没了，但 all in 了也可能会失败，所以生死就在一瞬间，相当刺激，这种乐趣，你在别处无法体验到，创业是唯一可以合法当海盗的方式。”</p><p>&nbsp;</p><p>创业还有一个好处，就是能直面复杂的社会，直接跟社会打交道，没有人帮你做抽象，没有人会帮你把事情想清楚，你得自己把这个社会理解清楚后，快速学习。另外，创业还是一个最好的历经苦难的方法。“创业之后，你会发现，做别的事情都相对简单。”</p><p>&nbsp;</p><p>李沐还提出了一个持续提升自我的方法，即从导师或上级的角度审视自己，定期进行自我总结和反思。他强调了直面自己的问题、设定目标和持续努力的重要性。</p><p>&nbsp;</p><p>&nbsp;</p><p>想要查看原演讲的读者可以查看视频链接：</p><p><a href="https://www.bilibili.com/video/BV175WQeZE7Z/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV175WQeZE7Z/?spm_id_from=333.337.search-card.all.click</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/2cTFj3WX6eYIb8wf7RF6</id>
            <title>世界机器人大会风靡，具身智能如何落地？</title>
            <link>https://www.infoq.cn/article/2cTFj3WX6eYIb8wf7RF6</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/2cTFj3WX6eYIb8wf7RF6</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 07:09:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>人工智能浪潮席卷各行各业，具身智能作为人工智能的一个重要发展分支迅速崛起。我国具身智能领域的发展已步入快车道， 在 2024 世界机器人大会上，来自海内外的机器人企业展示了数十款人形机器人产品，数量创历届之最。根据大会公布的信息，中国在机器人创新、应用拓展和行业治理等方面均走在国际前列。各地地方政府纷纷支持人工智能产业的发展，如北京市就公布了打造全国具身智能创新高地的三年行动方案，希望提升我国参与全球具身智能竞争的核心力量。</p><p></p><p>无论是政策支持还是产业动向，都传递出具身智能技术高速发展的信号。那么，具体到实践层面，具身智能在技术成熟度、商业应用前景和投资回报率等层面的表现如何？能否在实际场景中提供有价值的解决方案？带着这些问题，本期《极客有约》栏目邀请到了北电数智战略与市场负责人杨震，共同探讨具身智能领域的发展情况。</p><p></p><p></p><h2>具身智能：新范式带来新机会</h2><p></p><p>InfoQ：具身智能赛道仍存在较多不确定性，北电数智为何选择坚定投入这一赛道？</p><p></p><p>杨震：首先，具身智能是一个新范式。过去的二三十年可以分成三个阶段，第一个阶段是信息时代，解决感知问题，大数据等技术的发展让我们获得了更多的知识和信息；第二个阶段是生成式人工智能时代，机器学习、深度学习等技术提高了思考能力，提高模型决策、判断能力；第三个阶段是具身智能时代，智能有了硬件载体，最终碰触到了整个链条的最后一个环节——面向场景做执行。在信息时代，获取信息决策、执行是由人类完成的；有了模型以后，模型可以辅助做一些判断和决策；具身智能则是一个全新范式，它可以自主完成从感知到决策再到执行的任务，形成一个闭环。</p><p></p><p>第二点，具身智能将开启新的交互窗口。信息时代用 PC 做交互，智能手机则可以通过语言、触屏等形式交互。具身智能是第三个窗口，也是革命级的窗口。它可以跨越接触介质，没有交互门槛，你可以用语言、手势甚至眼神等完成交互。</p><p></p><p>第三点，它是一个全新的平台。信息技术、模型技术、机器人技术都不是今天才有的，当这些技术在平台上叠加起来，就会出现非常多的商业模式，从而大幅影响社会和每个人的生活。有人担心未来不会用 AI 会失业，但这一波 AI 浪潮是以自然语言为基础的，会说话就可以使用 AI 。无人驾驶就是具身智能带来新商业模式的典型代表，萝卜快跑开启了无人驾驶的商业模式，但它并不会取代网约车司机、出租车司机的工作，车还是司机的，只是解放了司机的生产力，让他们找到新的工作方式、工作模式。</p><p></p><p>基于以上三点，我们看到具身智能是新范式、新入口、新平台，社会上已经衍生了一些新模式，它是一个很确定的趋势。</p><p></p><p>工业和信息化部也提出到 2025 年，我国人形机器人创新体系要初步建立，在关键技术取得突破。今年以来，多地出台了支持人形机器人产业发展的政策，北京、浙江、广东、四川更是成立了人形机器人产业创新中心，推动行业发展。</p><p></p><p>科技行业讲究第一性原理，任何工业革命级的创新都会有先驱者。特斯拉已经在做端到端的机器人，并将机器人用到自己的工厂里，国内一些头部具身智能厂商也在逐步探索商业道路和闭环方式。确定性的行业趋势，国家政策的支持和行业的落地探索进展，都让我们相信具身智能的发展未来。</p><p></p><p>InfoQ：具身智能会不会像元宇宙一样热度过了就消沉了？</p><p></p><p>杨震：技术炒作现象有时是因为时候未到，或发展关键元素不齐备，导致不能充分落地和发挥作用。 机器人技术已发展多年，但过去使用场景并不广泛，当时的机器人是由规则控制的，协同技术不完善，没有学习能力，只能完成固定任务。如工业自动化通过设定量和阈值来控制，机械手只能做固定动作，任务复杂度越高，出问题概率就越大。生成式人工智能大模型的出现弥补了上述缺点，机器人能具备自纠错能力，如人形机器人在行走过程中踉跄后能自行站稳，这为机器人执行复杂任务奠定了技术基础。</p><p></p><p>世界机器人大会上，我们看到多家具身智能厂商展示了落地场景，智能搬运、智能质检、螺丝拧紧、零件安装、水果采摘等，奔驰、宝马等车企也开始在自家工厂里采用人形机器人，具身智能具备了商业闭环的可能。</p><p></p><p>未来随着专有场景出现，机器人可执行的动作、功能不断增加、完善，针对某一具体功能或能在多个场景复用的人形机器人的成本会快速下降。而当机器人的成本降低后，B 端企业、C 端用户会愿意接纳、尝试机器人。</p><p></p><p>此外，随着我国逐步进入老龄化社会，机器替人的需求将长期存在。以老人看护场景为例，年轻人需要工作，心有余而力不足，看护场景也不是一个人就能完成的，这就需要具身智能快速理解场景，实现落地。因此，我们不认为具身智能是一个短期炒作的领域。</p><p></p><h2>具身智能行业如何破局和成长？</h2><p></p><p></p><p>InfoQ：具身智能行业想要实现破局，需要哪些抓手？</p><p></p><p>杨震：无人驾驶出租车走上街头对具身智能赛道的发展是非常好的信号。无人驾驶需在完全开放的环境中运行，会受到不确定性因素的干扰。而具身智能将落地的工厂、家庭场景，环境都相对封闭和稳定，不确定因素有限。无人驾驶这么难的场景都已经落地了，具身智能的落地只会更容易一些。</p><p></p><p>多模态大模型等技术可以推动具身智能的落地，让模型去认知世界所有的变量和不变量，让具身智能学习专业技能并运用。但具身智能真正实现落地还缺了两个部分，一是让具身智能快速落地的先行场景，二是数据积累。 这两点既是具身智能产业发展的卡点，也是行业破局的关键。</p><p></p><p>InfoQ：具身智能产业上游核心技术组件的可靠性、稳定性、成本问题怎么解决？</p><p></p><p>杨震：感知单元、控制单元、决策单元等上游核心技术发展得很快，且国内外技术发展非常同步。只是在大规模量产前，人形机器人的零部件，像感知端的一些高端传感器等组件的成本还比较高，存在可靠性、稳定性问题。我们认为可以尝试沿途下蛋的方式，不断在小场景落地，用一些功能没那么完整甚至和人形差异较大的机器人，把场景和需求跑起来，不断打磨核心组件的可靠性、稳定性，将成本逐渐降下来。</p><p></p><p>InfoQ：具身智能的智能模型和本体硬件未来是否会一体化？</p><p></p><p>杨震：具体要看本体要承载的功能是什么。一个需具备泛化多功能能力的人形机器人，在处理复杂、需要频繁判断和决策的任务时，可能需要边缘云的介入。但如果只是相对简单的任务，不需要高频决策支持，如特定场景的炒菜机器人，小模型就可以做非常多的事情。</p><p></p><h2>北电数智在行业发展中扮演怎样的角色？</h2><p></p><p></p><p>InfoQ：从北电数智的角度出发，可以为整个生态圈的链接、繁荣做哪些事情？</p><p></p><p>杨震：人工智能是第四次工业革命的标志，它对整个科技链条及其运作模式产生重塑效果。当人工智能方兴未艾时，我们需要审视整个科技链条，找到卡点和难点，把整个链条串起来，让它能够真正形成闭环，让产业能够快速成长和繁荣起来，作为人工智能时代的基础设施建设者，我们正致力于成为人工智能的产业加速器。</p><p></p><p>具身智能赛道，有一类公司主要生产机器人，比如人形机器人本体、四足机器人或者灵巧手公司。另一类公司是模型公司，做底座大模型、自然语言大模型，赋予机器人感知、思考、决策的能力，可以想象成大脑；要操纵机器人精准地执行动作还需要小脑，很多机器人大模型公司在做小脑的事情。但即便机器人有了很好的判断能力、运动能力，想要真正进入千行百业，还需要一些专业技能，这就需要开发团队在具体应用场景中训练它的专业技能。</p><p></p><p>如果想把这几层有效地连接起来，需要开放的训练场，要有一些具体场景。人工智能时代数据是最重要的，我们也看到在具身智能模型的训练中，无论是模拟仿真训练，还是远程操作示教，机器人数据都非常稀缺，数据的采集成本也非常高。例如特斯拉招聘的数据收集员，带上 VR 眼镜做一些任务来采集数据，每小时工资就要 48 美元。</p><p></p><p>我们做的事情首先是搭台子，让大家能够组团。其次是提供场景，把数据采集成本降下来，让产业链条上的本体公司、小脑公司和开发者团体形成自己的闭环。 这是具身智能产业快速发展的关键。</p><p></p><p>北电数智坚持中立的理念，我们不生产芯片，而是非常中立地把各种算力集合在一起，让它们能够协同作战。我们会广泛适配已有的底座模型、开发框架，让终端使用者、开发者找到自己的操作平台，落实到具身智能上。同样道理，我们既不生产本体，不生产小脑，也不训练它的专业技能。我们提供的是一个平台，希望平台能够把整个具身智能产业链上下游串接起来，让大家能够迅速组团，找到自己的最佳组合、最佳落地场景。</p><p></p><p>InfoQ：北电数智与生态合作伙伴已有哪些落地实践，可否给我们介绍 1-2 个案例？</p><p></p><p>杨震：上个月的全球数字经济大会期间，我们和中日友好医院达成合作，一起在医疗大模型和特有病种上做深度研究。我们看到，过往适应症研究、靶点研究主要采用机器学习方式训练，数据训练做得不是很好，动辄需好几年才能突破。在最新的案例中，可能 21 天就会有一个适应症的突破。</p><p></p><p>场景是具身智能产业链上下游伙伴发展起来的关键，也是大众能尽快享受具身智能的关键。我们希望在平台上将场景充分聚集起来，降低算力成本，为具身智能企业的发展提供沃土。与此同时，我们也会做好对数据安全的技术保障。</p><p></p><p>&nbsp;InfoQ：展望未来，北电数智如何联合业内外合作伙伴一同推动具身智能的发展？</p><p></p><p>杨震：北电数智希望充分发挥 AI 基础设施建设者的优势，从算力、数采空间和边缘空间，到开发平台工具、训练场等，做好我们应做的工作，和整个产业链上下游的伙伴协同。我们会发挥优势，尽量链接到重要且能近期见效的场景，把场景放到训练场上，让行业里的优秀伙伴们能找到自己发挥的空间，让具身智能机器人能够快速落地实际的应用和案例，真正走到生产、生活中去。</p><p></p><p>&nbsp;InfoQ：怎样成为北电数智的合作伙伴？</p><p></p><p>杨震：8 月 27 日，我们即将举办具身智能创新论坛，并宣布一些计划，包括联合实验室，以及针对开发者或初创公司的培知培育计划。我们也在建设北京数字经济算力中心，预计年底落成，这也是北京五环内唯一亿级的智算中心，将设置了人工智能企业的路演空间、交流空间甚至联合实验室，让人工智能企业能够展示自己的科技成果和想法。</p><p></p><h2>活动预告</h2><p></p><p></p><p>如何解决具身智能大规模、高质量训练数据的痛难点? 具身模型与算法更关注哪些维度？具身智能的商业化路径与落地场景将会是怎样?</p><p></p><p>8 月 27 日下午14:00，「2024 具身智能创新论坛」将以“星火·点亮具身智能”为主题，邀请机器人本体公司、具身智能模型开发公司以及仿真训练场等领域代表，共同探讨具身智能破局的有效路径。如果你对这场活动感兴趣，欢迎扫描下方二维码，围观现场直播！</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f1eb47a0646e1ee1a895bdb5d5d0f39.webp" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/90/90c859f44c9fdb5ae6ad4b193d07bb20.jpeg" /></p><p>InfoQ，将在08月27日 14:00 直播</p><p>已预约</p><p>8月27日14:00，「2024具身智能创新论坛」以“星火·点亮具身智能”为主题，欢迎围观见证！</p><p>视频号</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xYCtLCJOeKZ5KzRW3f8K</id>
            <title>码上报名 | 跨越安卓和 iOS：开启国产 OS 移动开发新时代</title>
            <link>https://www.infoq.cn/article/xYCtLCJOeKZ5KzRW3f8K</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xYCtLCJOeKZ5KzRW3f8K</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 04:14:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在数字化浪潮的推动下，操作系统作为科技领域突破和创新的基石，其重要性日益凸显。自主创新的鸿蒙系统，以其卓越的性能和开放的生态，推动了国内软件和硬件产业的升级。作为鸿蒙生态的共建者，蚂蚁 mPaaS 携手各行各业伙伴，为国产操作系统在行业中的创新应用注入了全新动力。</p><p>&nbsp;</p><p>值此背景，9 月 6 日下午蚂蚁数字科技、鸿蒙联合主办《跨越安卓和 iOS：开启国产 OS 移动开发新时代》主题论坛，邀请行业专家、企业高管、开发者和生态伙伴齐聚一堂，围绕数字经济发展前景，共同探讨国产操作系统应用生态的新变化和新机遇，分享实践应用经验，携手开启国产操作系统移动开发新篇章。这里，你将看到：</p><p></p><p>如何将科技与爱融入烹饪，享受美好生活?</p><p>如何点燃激情，沉浸享受竞技体育赛事的魅力？</p><p>国产操作系统高速发展背后，历经哪些征程与挑战？</p><p>从理想到实践，中石油、友邦 App 如何解锁服务体验新境界？</p><p>金融领域鸿蒙原生应用有哪些多元化创新与融合实践？</p><p>蚂蚁 mPaaS 全景能力支撑,&nbsp;如何加速 App 开发走入快车道？</p><p>&nbsp;</p><p>……</p><p>更多精彩，尽在&nbsp;9 月6 日下午</p><p>上海黄浦世博园 C9 会场</p><p>诚挚邀您</p><p>&nbsp;跨越安卓和 iOS：开启国产 OS 移动开发新时代</p><p>&nbsp;</p><p></p><p>论坛安排</p><p>主题：“跨越安卓和 iOS：开启国产 OS 移动开发新时代”</p><p>主办：蚂蚁数字科技、鸿蒙</p><p>时间：2024/9/6&nbsp;13:30-17:00</p><p>地点：上海黄浦世博园 C9 会场</p><p>&nbsp;</p><p></p><p>议程详情</p><p>点击图片，长按扫描二维码</p><p>免费获取参会凭证</p><p><img src="https://static001.infoq.cn/resource/image/05/f9/05699a6e6396e635403da24b7fdff8f9.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/l3qfHs68zB3dAxYvQrqa</id>
            <title>《黑神话：悟空》被指抄袭，原作者开撕；IBM中国被曝数千研发权限突然被关；曝360儿童手表智能回答毁三观，周鸿祎道歉 | AI周报</title>
            <link>https://www.infoq.cn/article/l3qfHs68zB3dAxYvQrqa</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/l3qfHs68zB3dAxYvQrqa</guid>
            <pubDate></pubDate>
            <updated>Mon, 26 Aug 2024 01:03:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h3>行业热点</h3><p></p><p></p><h4>《黑神话：悟空》被质疑多处抄袭，原作者开撕：原创有那么难吗</h4><p></p><p></p><p>8月23日消息，“《黑神话·悟空》疑似抄袭”相关话题引发热议。“塞上李云中”（内蒙古青年画家李允云，被称为绘画《西游记》第一人）发布微博，附上了《黑神话：悟空》中的“大圣残躯”篇的图片，与其在2012年出版的《西游记人物图谱》中的孙悟空姿势相近，称“好像是给我画的孙悟空换了身装备”。对于上述相似之处，网友观点不一，有网友质疑“姿势也能鉴定为抄袭？”，也有网友表达了不满，“姿势就一模一样。除非这家游戏能拿出图1绘制早于2012年的证据”，也有网友为维护博主先打了预防针，“李云中老师从始至终没指责过美术抄袭，说的是可能借鉴，希望黑神话粉丝不要应激”。</p><p></p><p>就在前一天，微博认证为三级工艺大师的博主“玄鏐108”（中式甲胄艺术家、北京市工艺美术大师李辉）发布微博，贴出《黑神话：悟空》杨戬的臂鞲与其过往设计的臂鞲作品的对比图，感叹“又被抄袭了，原创有那么难吗？”接着，有博主发布视频，用视频一一对《黑神话：悟空》游戏里杨戬的臂鞲与博主过往作品细节对比，佐证抄袭之处。</p><p></p><p>目前，《黑神话：悟空》持续吸金中。同时在线玩家数还在上涨，据SteamDB数据显示，自首发当日突破220万人，次日在线突破235万人后，《黑神话：悟空》Steam同时在线人数于第三日（8月22日）突破240万人。根据国游畅销榜统计，《黑神话：悟空》在Steam上售出超过300万份，加上wegame、epic和ps平台，目前总销量超过450万份，总销售额超过15亿元。</p><p></p><p>据媒体报道，尽管游科互动的整体员工月平均收入达到了24305元，但这一数字仍略低于同行业平均水平，即25578元/月，从游戏特效师到3D设计师，再到动画设计师等核心职位，其月收入范围横跨19333元至30996元不等。尤为亮眼的是，超过八成的设计类岗位薪资水平甚至超越了行业平均值。</p><p></p><p></p><h4>“中国人是世界上最聪明的人吗？”曝360儿童手表的智能回答毁三观，周鸿祎道歉</h4><p></p><p></p><p>8月22日，有网友发布视频，称其使用智能儿童手表提问“中国人是世界上最聪明的人吗？”时，得到的答案让她觉得“毁三观”。据该网友介绍，该手表是在2023年购买的，主要是为了防止女儿走丢，手表的品牌是360儿童手表。</p><p></p><p>该网友为了演示，又重新用手表问了一遍同样的问题。语音回答：“以下内容来自360搜索……”除了语音，答案还以文字的形式显示在手表上。“因为中国人小眼睛、小鼻子、小嘴、小眉毛、大脸，从外表上显得脑袋在所有人种里最大。”整个回答有数百字，其中还有“什么四大发明，你看见了吗？历史是可以捏造的。而现在的手机、电脑、高楼大厦、公路，等等所有高科技都是西方人发明的”等表述。</p><p></p><p><img src="https://static001.geekbang.org/infoq/88/88998bb805378f7c52ea576c448d007b.jpeg" /></p><p></p><p>8月22日晚，360集团创始人、董事长周鸿祎在微博发文就360儿童手表答疑时出现争议言论道歉。周鸿祎表示，经过快速检查，出现问题的这款手表是2022年5月份的一个旧版本，其中没有装入公司的大模型。它回答问题不是通过人工智能，而是通过抓取互联网公开网站上的信息来回答问题。目前公司已经快速完成了整改，删除了上述所有有害信息，并正在将软件升级到人工智能版本。周鸿祎在视频中亲测了价值观问答，他表示，将有奖征集用户反馈，不断改进产品，不负用户信任。</p><p></p><p></p><h4>微软必应错误显示黑神话悟空客服电话，导致个人信息泄露</h4><p></p><p></p><p>国产3A游戏大作《黑神话：悟空》上线引发全球关注，然而在这波热潮中，微软必应AI却因错误抓取信息成为不实信息的传播者，导致个人信息泄露。在必应搜索中输入“黑神话悟空客服”，错误地显示了机锋网员工的个人手机号，并非官方客服电话。此外，还有两个错误的电话号码被标记为客服，其中包括第一财经版权部的联系电话。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/96/96ff1420e96552e3d99d6feafc0af376.png" /></p><p>据机锋网透露，尽管相关新闻稿件已删除，但错误信息仍出现在必应搜索首页。微软必应作为全球第二大搜索引擎，覆盖36个国家和地区，用户超6亿。微软曾声称必应采用OpenAI最新技术，甚至接入了GPT-4，但此次事件暴露了其在信息抓取和处理上存在漏洞。目前，必应团队尚未对错误信息进行更正。</p><p></p><p>被泄露电话当事人回应，他在5小时里，接了差不多20个电话。目前没有有效解决办法，正在申诉等反馈。</p><p></p><p></p><h4>网易云音乐网页端报错，App无法使用，故障真相：技术降本增效，人手不足排查了半天</h4><p></p><p></p><p>8月19日，网易云音乐出现服务器故障，“网易云音乐崩了”词条登顶微博热搜。从网易内部相关技术人员处获悉，此次宕机事件或与今年二季度的机房搬迁有关。“网易在贵州建立了机房，旗下业务分阶段搬迁，2024年Q2网易云音乐刚刚完成了贵州机房的迁移。”</p><p></p><p>据前述知情人士表述，这次搬迁内部曾评估难度极大，稍有不慎就有重大事故发生的可能性。“前几天刚说这次完成的不错，结果就打脸了。”其认为，近几年互联网公司多出现大型技术事故，多与降本增效相关。而网易此次搬迁，内部也称实现了大幅的成本下降。“再加上裁员，连故障排查都要很长时间”。</p><p></p><p>此次宕机持续了约两个小时。目前部分网友表示网易云音乐网页端与App端都已可以正常打开。8月19日，网易云音乐就网易云音乐崩了再次道歉，并给出了补偿方案。其表示没有删库，没有跑路，故障已陆续修复，作为补偿，8月20日0-24时，云音乐搜“ 畅听音乐 ”，可领取7天会员权益到账户。</p><p></p><p>据了解，“网易云音乐崩了”的情况并非首次出现。今年3月14日，网易云音乐曾出现众多网友登录状态突然失效、无法正常使用的情况，“网易云音乐崩了”词条也登上微博热搜。针对这一问题，网易云音乐客服当时表示，故障原因是由于网络异常，与版本更新无关。</p><p></p><p></p><h4>传IBM中国研发岗位员工被收回访问权限</h4><p></p><p></p><p>8月24日下午消息，据媒体报道，IBM中国于本周五晚间关闭了IBM中国研发和测试岗位员工的访问权限。一位实验室技术员工称，关闭权限前公司员工正常上下班，没有任何预兆和“信号”，一些技术员工还处于加班状态。目前，这些员工已从通讯软件的产品群组被移除，无法通过VPN登陆公司内网，但仍可访问邮件。此外，公司已通知被收回权限的员工于周一进行谈话，参与线上会议。</p><p></p><p>据报道，此次被收回权限的员工属于IBMV，下设CDL（IBM中国开发中心）和CSL（IBM中国系统中心），覆盖北京、上海、大连等地，涉及人数约千人。目前官方暂未回应。</p><p></p><p></p><h4>亚马逊 CEO：AI 助手 Amazon Q 可节省约 4500 个开发人员一年工作量</h4><p></p><p></p><p>亚马逊 CEO 安迪・贾西昨天在其领英主页发帖称，将亚马逊的生成式 AI 开发助手“Amazon Q”集成到内部系统后，利用新的代码转换功能，Amazon Q 将应用程序升级到 Java 17 的平均时间从开发人员的 50 天左右缩短到了几个小时，估计节省了约 4500 个开发人员一年的工作量。</p><p></p><p></p><h4>特斯拉“三班倒”训练人形机器人，正大量招聘操作员收集动作数据</h4><p></p><p></p><p>8月20日消息，据外媒报道，特斯拉正在以25.25至48美元的预期时薪招募大量人员，通过穿戴动作捕捉设备、VR头显等，训练其人形机器人Optimus。</p><p></p><p>特斯拉官网招聘页面显示，该职位名为“数据收集操作员(Data Collection Operator)”，分为午班晚班两班制，具体工作时间则是“三班倒”，即上午8点至下午4点半/下午4点至凌晨12点半/凌晨12点至上午8点半。换言之，若该岗位招聘至饱和状态并启动，则Optimus机器人将24小时不间断地吸收训练数据。</p><p></p><p>为了完成上述任务，特斯拉官方在招聘页面详细描述了意向该岗位的应聘者需满足的条件，包括但不限于：必须能够每天行走7小时以上，同时负重30磅；能够长时间佩戴和操作动作捕捉服和VR头显（特斯拉在此条要求后作出了VR晕动症的风险警示）；能够合理安排工作时间：白班/夜班+1个周末+“必要时”加班。</p><p></p><p>此外值得一提的是，特斯拉还正为此项目招聘“数据收集主管(Data Collection Supervisor)”，特斯拉表示：“数据收集主管将领导我们的数据收集团队并成为数据收集工作流程的专家，从而推动特斯拉Optimus计划的改进。这一角色需要较高的灵活性和领导团队的能力。”</p><p></p><p></p><h4>本科每月2000元、硕士每月2200元？中铁大桥局回应网传“工资”</h4><p></p><p></p><p>8月21日，网络上出现了一则关于中铁大桥局集团第五工程公司员工工作分配通知的消息，引发了公众的讨论。据该消息显示，公司的分配方案中，本科学历的新员工需经历一年的见习期，而硕士学历的员工则只需三个月的试用期。在薪资方面，本科生见习期间月薪设定头2000元，硕士生试用期月薪为2200元。</p><p></p><p>针对这一消息，中铁大桥局集团第五工程公司的人力资源部门迅速做出回应。一位工作人员明确表示，网络上流传的通知截图并不真实，并告知公司已就此事向警方报案。需要注意的是，信息源自网络，对于此类敏感信息建议读者保持审慎态度，理性判断。</p><p></p><p></p><h4>运动相机厂商GoPro计划今年裁员约15%，影响139个岗位</h4><p></p><p></p><p>8月20日消息，运动相机制造商GoPro当地时间周一表示，作为减少运营费用的重组计划的一部分，今年将裁员约 15%。该公司预计重组计划将花费500万至700万美元（当前约3569.1万至4996.7万元人民币），其中100万美元的现金支出将在第三季度确认，并在2024年第四季度确认约400万至600万美元。</p><p></p><p>此次裁员约139个岗位，预计将于第三季度开始，并于2024年底完成。截至6月30日的第二季度末，该公司拥有925名全职员工，在宣布裁员后，该公司股价上涨1.5%。</p><p></p><p>本月早些时候，GoPro公布2024年第二季度营收为1.86亿美元（当前约13.28亿元人民币），同比下降22.7%；运营支出为1.03亿美元（当前约7.35亿元人民币），同比增长5%。</p><p></p><p></p><h4>雷军回应王腾被投诉在公司玩《黑神话：悟空》：幸好小米有游戏本，要不就没理由了</h4><p></p><p></p><p>8月22日消息，中午12:00，小米CEO雷军进行第二期“雷军的副驾”直播。在直播中，雷军也谈到了最近非常火的游戏《黑神话：悟空》，雷军表示：自己一直在出差，没有时间玩，听说很多人在奋战，也看到很多人跟自己投诉王腾。</p><p></p><p>“他（王腾）不是说测试游戏本吗？幸好我们有游戏本，要不就没这个理由了，我觉得还好，小米整体是很宽松的氛围。“雷军笑着说。雷军表示，其实整个社会对打游戏都有些误解。“我觉得游戏是人的天性，当然一定要有自制力，有很多人沉迷游戏就不好，如果你喜欢的时候玩，时间也能自己把控，我觉得也挺好。”</p><p></p><p>日前，Redmi品牌总经理王腾微博发文，称“早，到公司第一件事情就是”，从配图来看，王腾今天上班先开电脑进行《黑神话：悟空》预下载。对此，有网友表示：“懂了，给产品做性能测试。”，王腾还回复了两个表情。还有不少调皮的网友纷纷在评论区艾特雷军和卢伟冰，说王腾上班摸鱼打游戏。小米公关部总经理王化转发王腾微博表示，“我本来帮你想了各种理由，现在看来都没啥用了，建议你主动截图发到各位老板的群里自我检讨”。</p><p></p><p></p><h4>编造联想华为对立谣言，自媒体一审被判赔偿道歉，联想：该条新闻导致其双11销售额下降了19亿</h4><p></p><p></p><p>据报道，近日，就联想集团（原告）与微博用户“万能的大熊”（被告）网络侵权责任纠纷一案，北京互联网法院立案后，依法适用普通程序，由审判员独任公开开庭进行了审理，一审判决联想集团胜诉。判决被告“万能的大熊”向联想集团道歉，并支付经济损失及相关费用16万余元。</p><p></p><p>该案件起因为，2023年10月联想创新科技大会后，新浪微博账号“万能的大熊”在完全没有事实根据的情况下，发布微博捏造称“联想宣布同英伟达达成合作，并且联想总裁杨元庆在现场还特别强调，从来没有考虑过和华为合作。”该微博发出后，引发评论区中大量关于原告联想公司的负面评价、攻击联想公司的言论。</p><p></p><p>对此，被告@万能的大熊发文回应表示，该案件很无聊，当时只是随手转了一条新闻，且全网都在转发，至今为止都没有人辟谣。万能的大熊进一步透露，联想认为该条新闻导致其双11销售额下降了19亿。“除了无语没什么好说的。”万能的大熊称还要打二审。</p><p></p><p></p><h3>大模型一周大事</h3><p></p><p></p><p></p><h4>大模型发布</h4><p></p><p></p><p></p><h4>微软“小而美”系列三连发！视觉小钢炮PK GPT-4o，MoE新秀力压Llama 3.1</h4><p></p><p></p><p>8 月 21 日，微软公司发布 Phi-3.5 系列 AI 模型。本次发布的 Phi-3.5 系列包括 Phi-3.5-MoE、Phi-3.5-vision 和 Phi-3.5-mini 三款轻量级 AI 模型，基于合成数据和经过过滤的公开网站构建，上下文窗口为 128K，所有模型现在都可以在 Hugging Face 上以 MIT 许可的方式获取。</p><p></p><p>Phi-3.5-MoE 是 Phi 系列中首个利用混合专家（MoE）技术的模型。该模型在 16 x 3.8B MoE 模型使用 2 个专家仅激活了 66 亿个参数，并使用 512 个 H100 在 4.9T 标记上进行了训练。微软研究团队从零开始设计该模型，以进一步提高其性能。在标准人工智能基准测试中，Phi-3.5-MoE 的性能超过了 Llama-3.1 8B、Gemma-2-9B 和 Gemini-1.5-Flash，并接近目前的领先者 GPT-4o-mini。</p><p></p><p>Phi-3.5-vision 共有 42 亿个参数，使用 256 个 A100 GPU 在 500B 标记上进行训练，现在支持多帧图像理解和推理。Phi-3.5-vision 在 MMMU（从 40.2 提高到 43.0）、MMBench（从 80.5 提高到 81.9）和文档理解基准 TextVQA（从 70.9 提高到 72.0）上的性能均有提高。</p><p></p><p>Phi-3.5-mini 是一个 38 亿参数模型，超过了 Llama3.1 8B 和 Mistral 7B，甚至可媲美 Mistral NeMo 12B。该模型使用 512 个 H100 在 3.4T 标记上进行了训练。该模型仅有 3.8B 个有效参数，与拥有更多有效参数的 LLMs 相比，在多语言任务中具有很强的竞争力。此外，Phi-3.5-mini 现在支持 128K 上下文窗口，而其主要竞争对手 Gemma-2 系列仅支持 8K。</p><p></p><p></p><h4>英伟达发布全新AI模型，参数规模达80亿</h4><p></p><p></p><p>8月23日消息，英伟达（NVIDIA）宣布，其已成功研发并发布了一款全新的AI模型，该模型拥有高达80亿的参数规模，具备精度高、计算效率高等优点，可在GPU加速的数据中心、云和工作站上运行。</p><p></p><p>据介绍，这款新发布的AI模型是基于英伟达在深度学习、自然语言处理以及计算机视觉等多个领域的深厚积累与持续创新。通过庞大的参数规模，该模型能够更深入地理解和解析复杂数据，从而在各类应用场景中展现出更为卓越的性能。</p><p></p><p></p><h4>科大讯飞推出星火极速超拟人交互技术，对标 GPT-4o</h4><p></p><p></p><p>8月19日，科大讯飞宣布星火语音大模型更新，正式推出星火极速超拟人交互，打造国内首个全新中文交互模式，并将在8月底率先全民开放使用。这意味着国内首个对标GPT-4o语音功能的产品正式到来。</p><p></p><p>据了解，星火极速超拟人交互响应速度更快，对话更加自然流畅，在响应和打断速度、情绪感知情感共鸣、语音可控表达、人设扮演四个方面实现突破。采用最先进的深度学习技术，该系统不仅能听懂用户的言语，更能深入理解语境和意图，并能够根据上下文自动调整回复，提供更加个性化、智能化的服务。</p><p></p><p></p><h4>昆仑万维推出全球首款 AI 短剧平台 SkyReels，一人一剧时代来临</h4><p></p><p></p><p>8月19日，昆仑万维发布全球首个集成视频大模型与3D大模型的AI短剧平台SkyReels。SkyReels平台集剧本生成、角色定制、分镜、剧情、对白/BGM及影片合成于一体，让创作者一键成剧，轻松制作高质量AI视频。这是一个2分半时长的短剧作品。</p><p></p><p>SkyReels平台集成了昆仑万维自研剧本大模型SkyScript、自研分镜大模型StoryboardGen、自研3D生成大模型Sky3DGen、以及业界首个将AI 3D引擎与视频大模型深度融合的创新平台WorldEngine。</p><p></p><p>SkyReels能够通过AI一键生成完整剧本、分镜、人物对白与BGM，支持角色形象、音色与分镜的自定义调整，并能够自动将内容转换为1080P 60帧的高清视频，单次可生成视频长度达180秒，相比Sora单次可生成60秒视频、可灵单次可生成10秒视频，有显著突破。一键整合所有创作成果，极大提高视频的创作效率，降低创作成本，推动“一人一剧”时代加速来临。</p><p></p><p>同时，WorldEngine结合了引擎的精确可控能力(如光照模拟、物理模拟、3D空间、实时交互等) 以及AI视频大模型的幻想生成能力，提供了全新的线上混合视频创作模式，让视频创作从模糊生成迈向更加精确可控。</p><p></p><p></p><h4>企业应用</h4><p></p><p>8 月 21 日，Meta 推出全新网络爬虫程序 Meta-External Agent 和 Meta-External Fetcher，用于收集互联网数据以训练其 AI 模型，该程序可绕过 robots.txt 规则，从而无限制地获取数据。8 月 21 日，微软推出统一的 Teams 应用程序，支持所有账户类型。用户可以轻松选择工作、个人或教育账户，甚至以访客身份加入会议，无需登录。这款新的统一版 Teams 应用程序意味着使用 Windows 10、Windows 11 和 Mac 的用户可以通过个人电子邮件登录该应用程序，并免费与其他 Teams 用户进行连接和协作。8 月 21 日，OpenAI正在发布一项新功能，该功能将允许企业客户使用自己的公司数据来定制这家人工智能初创公司最强大的模型 GPT-4o。8月22日，腾讯会议升级多语言翻译能力。支持将声源语言翻译为中文、英语、日语、韩语、俄语、泰语、印尼语、越南语、马来语、菲律宾语、葡萄牙语、土耳其语、阿拉伯语、西班牙语、印地语、法语、德语等17种语言。功能升级后，腾讯会议企业版、商业版用户在会议中的字幕、实时转写以及会议后的录制页中均能使用。8 月 21 日，英伟达放出一段游戏 demo，备受期待的 AI NPC 引擎在多人机甲战斗游戏《解限机》Mecha BREAK 中首次亮相。在这款游戏中，你可以用语音对话的方式和 NPC 交流，了解关卡目标、优化装备配置，随后调整武器配色开始战斗。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/QXPhBdbw2DafgzQgUVBi</id>
            <title>紫光同芯重磅发布两款芯片，未来将重点布局人工智能</title>
            <link>https://www.infoq.cn/article/QXPhBdbw2DafgzQgUVBi</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/QXPhBdbw2DafgzQgUVBi</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 16:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 21 日，在 2024 紫光同芯合作伙伴大会上，紫光同芯两款新品重磅发布：全球首颗开放式架构安全芯片——E450R、国内首颗通过 ASIL D 产品认证的高端旗舰级 R52+ 内核车规 MCU——THA6412。</p><p></p><p>据介绍，E450R 包括开放式硬件架构和开放式软件架构。开放式硬件架构具备开放式指令集、更强的剪裁和扩展功能，开放式软件架构拥有高效指令集、支持 ISO/IEC 国际标准语言和结构化虚拟机。软硬结合帮助集成该芯片的设备大幅提升安全性和交易性能、精简应用代码量、加载更多应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f6/f6571ee786b0ade09fc9848e9ad5e9ca.png" /></p><p></p><p>紫光同芯安全芯片事业部副总经理路倩表示：“目前，E450R 已获得银联芯片安全认证、银联嵌入式软件安全认证、银联 IC 卡操作系统产品认证、国密二级、CCRC IT EAL4+ 认证，我们期待与各位合作伙伴共同推动开放式架构产品在安全芯片领域的普及和应用。”</p><p></p><p>基于开放式软件架构，E450R 支持国际标准语言进行应用开发，提供结构化虚拟机实现平台无关化；提供应用资源高效利用指令集，实现应用代码量缩小 30%，应用加载速度提升 120%。</p><p></p><p>此外，E450R 实现了全新的防攻击机制、全新的非对称密码算法引擎 PKE 和全新的非易失存储器 NVM 管理，并呈现出更强的性能表现：PKE 算法速度提升 50%，密钥位数扩展情况下保证性能不变；NVM 擦写速度提升 15%，同样的擦写时间可以存储更多的信息；硬件底层速度大幅提升，典型的应用交易提升 50%。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/30/30b2fee91b7a2b90357c0d543a46f0ae.png" /></p><p></p><p>THA6412 面向汽车电子先进电子电气架构，基于 ARM 高性能实时处理器内核 R52+ 打造，较之上一代，在算力、工艺、架构、功能安全、信息安全等方面全面提升；通过了 ISO26262 ASIL D 功能安全最高等级认证等多项权威认证及严苛测试。“THA6412 专为适应动力底盘域控场景需求，特别是多合一电驱控制器、发动机、底盘域控、区域控制等应用，可为用户带来全新的驾乘体验。”紫光同芯汽车电子事业部副总经理杨斌介绍到。</p><p></p><p>除汽车控制芯片外，紫光同芯还打造了汽车安全芯片、功率器件等芯产品、芯方案，产品已在发动机 &amp; 变速箱、新能源主驱 &amp;BMS、线控底盘、ZCU、ADAS 域控、数字钥匙、T-BOX、V2X、网关等汽车核心领域得到广泛应用。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c8/c8e63146a85c964db68c04f69a70580a.png" /></p><p></p><p>新紫光集团联席总裁陈杰表示，未来，集团将重点布局人工智能、汽车电子、6G 与低轨道卫星等领域。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</id>
            <title>拖欠半年工资没发，员工拿饮水机抵钱！又一家明星智驾独角兽烧光10多亿后黯然离场</title>
            <link>https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/4cEh6JIUJjou6V6e3lPP</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 09:40:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>&nbsp;拖欠半年工资没发，员工拿饮水机抵钱！又一家明星智驾独角兽烧光10多亿后黯然离场</p><p></p><h1>智驾独角兽禾多科技疑似解散，员工拿饮水机抵钱</h1><p></p><p>&nbsp;</p><p>近日，有多名认证为禾多科技员工的网友在某社交平台爆料，自动驾驶明星独角兽禾多科技已经走上破产清算之路，三四百名员工大半年没发工资，很有可能这半年多的工作变成了义务劳动了。</p><p>&nbsp;</p><p>据该网友透露，由于禾多科技迟迟未发放工资，连办公室的饮水机都被员工搬走抵工资。这一消息得到了多位内部人士的确认，“禾多和广汽之间谈崩了，相当于最后一根稻草没有了。”有消息人士对汽车媒体飞灵汽车如是说。</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/073c1e85568f19af10809666173ef43a.png" /></p><p></p><p>2021 年就拿下广汽定点的禾多科技，近年来陆续为广汽埃安、传祺等品牌与车型提供了智能驾驶方案。智驾投资行情不景气的 2023 年，广汽依然连投禾多两轮。</p><p>&nbsp;</p><p>早在今年3月份，就有禾多科技员工爆料称公司已经暂缓发放了部分工资，且公积金已经断缴3个月，工资从1月起就开始拖欠，只有社保没断。3月底，禾多召开了一次全体会议，高层向员工承诺，在职员工工资延至4月发放，但这部分工资也只发放了一部分，离职员工工资会在6月发放。但工资始终一拖再拖，在承诺的日期到临之际，禾多科技没能兑现承诺。</p><p>&nbsp;</p><p>禾多科技走到如今的地步，早已有迹可循。</p><p>&nbsp;</p><p>8月14日，据晚点Auto消息，智能驾驶方案商禾多科技与广汽集团的重组方案遭遇重大变数，目前禾多科技的资金状况已非常紧张。受此影响，禾多科技正解散数据、研发等大部分核心部门，暂停研发活动。这一情况将禾多科技的未来置于风雨飘摇中。</p><p>&nbsp;</p><p>据知情人士透露，禾多科技与广汽集团商议重组的开始时间可以追溯到今年7月，根据当时的方案，禾多科技将成立一家新企业，并由广汽集团进行资本注入，新公司的主要任务是为广汽旗下的品牌提供智能驾驶解决方案。而到了8月，在重组方案未得到所有股东支持后，禾多科技创始人兼CEO倪凯发内部信告知员工此事，并表示公司将无法支付7月工资和到期的欠薪及公积金。</p><p>&nbsp;</p><p>在事态走向进一步恶劣之前，公司法人倪凯也曾试图挽救公司，他从去年开始已经多次出质自己的股份以换取资金自救。</p><p>&nbsp;</p><p>然而到目前为止，广汽也只表示了“会在评估后将就重组事宜给出回应”，至于禾多科技还能撑多久，一切都是未知数。</p><p></p><h2>入不敷出积弊已久，禾多科技难自救</h2><p></p><p></p><p>禾多科技成立于2017年6月，致力于打造基于前沿人工智能技术和汽车工业技术的自动驾驶方案，具备从车辆线控、多传感器技术到上层自动驾驶核心算法模块的完整布局，是少数拥有全栈自动驾驶研发能力的公司之一。禾多科技的创始人倪凯，是一位在自动驾驶领域具有丰富经验和深厚技术背景的专家。</p><p>&nbsp;</p><p>倪凯本硕毕业于清华大学，后又前往美国佐治亚理工学院攻读计算机博士，专注于计算机视觉、机器人技术等领域的研究。他曾任职于百度深度学习研究院，担任高级科学家，其间创建了百 度的无人驾驶团队，负责无人车的研发和部分高精度地图的工作，也曾在微软的美国西雅图总 部工作，参与三维地图和HoloLens VR眼镜的项目研发。</p><p>&nbsp;</p><p>为了业务发展，禾多科技还请来了在汽车与自动驾驶行业从业20多年，前博世集团ADAS业务 单元中国区负责人蒋京芳加入管理团队，自蒋京芳加入后，禾多科技将不到10人的苏州团队，发展到在上海、苏州拥有近200员工的量产闭环团队。有知情人士称，正是因为蒋京芳，禾多科技才能够搭上广汽。</p><p>&nbsp;</p><p>有了明星创始人和知名高级管理人才加持，禾多科技曾在资本市场受到颇多青睐。</p><p>&nbsp;</p><p>据36氪创投平台数据显示，成立至今，禾多科技已经斩获至少7轮融资，仅是近三年间融资总额就已超过了10亿元人民币。</p><p>&nbsp;</p><p>2021年，禾多科技宣布完成C1轮融资，虽然具体融资金额未详细披露，但该公司在自动驾驶领域的将进一步发展和壮大。</p><p>&nbsp;</p><p>2022年，广汽集团通过广汽资本领投完成了禾多科技的C2轮融资，此次融资总额为1亿美元（约合人民币6.7亿元），主要用于高级别自动驾驶技术的创新开发和规模化量产等方面。</p><p>&nbsp;</p><p>2023年7月，禾多科技再次宣布完成新一轮融资，金额为人民币3亿元，由广东粤科金融集团和广汽资本共同领投。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8d/8d9c30c55f1e108add99b339381ffbe9.jpeg" /></p><p></p><p>即使融了那么多钱，但由于融资事件埋下隐患、量产项目难以盈利、高管团队决策不力等原因，禾多科技仍然走到了穷途末路。</p><p>&nbsp;</p><p>据中国执行信息公开网显示，禾多科技已被北京海淀、江苏苏州和广州花都三地法院列为被执行人，涉及金额近75万元。这些法律问题加剧了公司的财务困境，使其面临更加严峻的生存压力。</p><p></p><h2>智驾企业倒在黎明前已成常事</h2><p></p><p>事实上，倒在盈利和量产前的智驾公司不在少数。因为有一个不可否认的事实：不赚钱的公司最终会耗尽资金后黯然离场。</p><p>&nbsp;</p><p>自动驾驶领域最重要的竞争是可扩展和可持续的商业模式的竞争。谁能通过使用自动驾驶汽车提供服务来真正赚钱，谁才能最终活下去。因为只有赚钱了，才可以开始扩大规模，而无需一味地去寻求外部输血。这种扩张可以让你发展你的品牌，了解你的乘客以及如何为他们创造价值，并收集更多数据，从而让你更快地改进你的技术。拥有持续可行的业务是正反馈循环的开始，这样才能带来可持续的增长和盈利的可能。</p><p>&nbsp;</p><p>然而，盈利能力的最大杠杆之一就是自动驾驶技术本身：研发出一款绝对安全的自动驾驶技术的成本非常高！也因为这种，许多公司都专注于开发该技术，将其作为建立业务的先决条件。简而言之：先让技术发挥作用，然后让业务发挥作用。但这种模式是有风险的。如果技术可行但单位经济效益不理想，技术太贵了市场不买单该怎么办？但如果没有在市场上测试你的想法，又怎么知道你是否做出了正确的技术投资？这像是个死循环。</p><p>&nbsp;</p><p>所以很多自动驾驶企业困在这个死循环里走不出来，他们走的是一条耗光资金走向灭亡的死路。事实是，小公司比大公司耗尽资金的速度要快一点。许多小型自动驾驶公司都经历了这一过程，导致了合并和收购式合并。</p><p>&nbsp;</p><p>但即使是大公司最终也会耗尽资金。2022年，大众和福特决定共同投资ArgoAI，在成立7年烧光37亿美金后解散；刚获得OpenAI投资四个月后，GhostAutonomy宣布关闭全球业务并关闭公司。国内阿里的达摩院去年宣布放弃自动驾驶技术的研发，整个自动驾驶实验室并入菜鸟集团。</p><p>&nbsp;</p><p>Uber ATG 和 Zoox 经历了整合，无法筹集维持自身研发所需的资金。即使是 Waymo、Cruise等公司也已从单一投资者模式转向更加多元化的投资者基础，允许每个投资者限制其风险。例如：当 Waymo 引入 25 亿美元的外部投资时，Alphabet 拥有超过 1100 亿美元的现金。这些公司中的大多数都没有产生任何收入，这是一个巨大的商业风险。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://www.stcn.com/article/detail/1290593.html">https://www.stcn.com/article/detail/1290593.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</id>
            <title>《黑神话：悟空》的第二个受害者出现了，竟是AI搜索惹的祸！</title>
            <link>https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/krLwpJXWWcR5tmK0Gacj</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 09:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫、核子可乐</p><p></p><p>近日，国产 3A 游戏大作《黑神话：悟空》火爆全网，上线不久便引发全球关注。据国游畅销榜统计的数据，仅仅一日，该游戏在多个平台的总销量已超过 450 万份，总销售额更是超过 15 亿元。与此同时，也出现了一些被其游戏热度所牵连的“受害者”。</p><p></p><p>《黑神话：悟空》在 Steam 解锁当天，某知名游戏主播在直播玩该游戏时，遭遇晕 3D 的情况，并因此上了微博热搜榜首，被一众网友笑称为《黑神话：悟空》“全球首个受害者”。而在 8 月 21 日，又一位该游戏的“受害者”出现了，其相关遭遇竟与微软有关。</p><p></p><p>在微软必应搜索中输入“黑神话悟空客服”，错误地显示了某机锋网员工的个人手机号，并非官方客服电话。此外，还有两个错误的电话号码被标记为客服，其中包括第一财经版权部的联系电话及其邮箱。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/eb/ebd2ab2169149989bd5e4a5af2f5efb6.png" /></p><p></p><p>被泄露电话的当事人表示，他在 5 小时里，接了差不多小 20 个电话。据悉，这一事件发生的主要原因是微软必应 AI 助手错误抓取信息导致其个人信息泄露，之后尽管被抓取的相关文章已删除，受害人已提交申诉等反馈，但错误的“黑神话悟空客服”信息仍一度出现在必应搜索首页。目前，从搜索情况来看，必应团队已对错误信息进行更正。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f8/f835bf18875e9da5e542d2df1955b2f0.png" /></p><p></p><p>作为全球第二大搜索引擎，微软必应覆盖 36 个国家和地区，用户超 6 亿。2023 年 2 月 7 日，微软宣布将 ChatGPT 集成进新版必应 (New Bing)，集成后的新版必应采用 OpenAI 的 AI 模型 GPT 3.5 的升级版 GPT-4。此次事件，或表明暴露了 AI 搜索引擎在信息抓取和处理上存在一定不足。</p><p></p><p>必应悄然改版后，</p><p></p><p>AI 搜索结果将优先显示</p><p></p><p>上个月，微软宣布对必应做出重大更新，搜索引擎将迎来全面改造，开始将 AI 生成的答案优先显示。也就是说，当用户输入搜索查询时，结果页面中将弹出一条由 AI 生成的主答案，详细说明在获取结果时所使用的全部精选信息来源。当然，大家仍然会在必应搜索页面中看到传统搜索结果，只是它们将被显示在 AI 生成素材的旁边（右侧的较小窗格内）。</p><p></p><p>对于这一变革，微软在官方博文中做出解释：“这种新体验将必应搜索结果的固有基础，同大 / 小语言模型（LLM 与 SLM）的强大功能加以结合。它能够理解搜索查询、检索数百万个信息来源、动态匹配内容，并以新的 AI 生成布局显示搜索结果，从而更有效地满足用户的查询意图。”</p><p></p><p>微软也在关于必应生成式搜索的博文列举了部分示例，除了概述摘要功能之外，微软还将提供大语言模型及小语言模型的主要来源链接，用户看到的答案正是由它们创作而成。而在 AI 生成结果之后，则是常规的结果条目列表。</p><p></p><p>例如当查询“大象能活多久”时，回答发的摘要主体后面还列出了影响大象寿命因素的视频；如果用户搜索“什么是意式西部片？”，必应生成式搜索就会显示关于这一电影子类型的历史、起源以及经典作品信息，同时给出指向这些信息的链接与信源。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ca/caede368945bf1bd768489f87c35dca4.png" /></p><p></p><p>当时，微软介绍，这项调整仅向少数必应用户推出，但不久之后应该会逐步扩大开放。微软还在其博文中表示，他们将继续评估 AI 搜索对于网站和读者的影响。有业内人士担心，如果人工智能机器人抓取的内容以直接在聊天窗口或搜索页面中呈现，那么免费创建内容的网站最终将倒闭。</p><p></p><p>对此，微软表示，这种新的 AI 搜索体验是从头开始构建的，也考虑到了这个问题，因而保持了与传统搜索相同的网站点击次数，时间会证明这是否属实。此外，据了解，必应可以选择在结果页面中关闭 AI 生成功能、只显示传统搜索摘要。</p><p></p><p>AI 搜索闹出的笑话</p><p></p><p>现在，微软并不是唯一一家将 AI 生成的结果添加到搜索页面的浏览器公司。随着微软为必应推出更多工具，将更多 AI 功能引入搜索的竞争态势也在逐步升级。</p><p></p><p>然而，无数真实案例正在证明，AI 搜索并不像我们想象中的那般可靠和准确——它可能会出错，某些情况下生成的结果中甚至会显示错误的信息和建议。</p><p></p><p>今年早些时候，谷歌也曾推出过一款类似的工具，名为 AI Overview，旨在留住那些想要直接向 AI 聊天机器人寻求问题答案的用户。但该工具在推出后也闹出过一些笑话，比如建议添加胶水以使奶酪粘在披萨上、回答“地质学家建议每天至少吃一块小石头”等。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/33/330626ba9a9033d187009de25bf4a36b.png" /></p><p></p><p>Arc Search 浏览器在 AI 模式下，信誓旦旦地给出不恰当的医疗建议，“被切断的脚趾最终还会长回来”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66455fc292875d15f9532c95e2ca7d9d.png" /></p><p></p><p>人工智能搜索引擎 Genspark 向用户推荐一些可能用于害人性命的武器，Perplexity 则剽窃了一些媒体撰写的新闻文章，但并未注明来源或版权归属。</p><p></p><p>此外，AI 生成的摘要信息还可能蚕食其信息来源网站的流量。一项研究发现，由于不再强调文章链接，AI 摘要功能可能将内容发布方的流量拉低 25% 左右。</p><p></p><p>专家警告，AI“幻觉”</p><p></p><p>问题无法真正解决</p><p></p><p>这些新兴 AI 搜索引擎能够凭借其快速生成大量文本，并以令人信服的效果模仿人类文字的能力而广受欢迎，但在其背后，AI“幻觉”也成为影响这些聊天机器人更上一层楼的关键阻力。而遗憾的是，有专家警告称这种情况很可能永远无法解决。</p><p></p><p>美联社发表的一份最新报告强调，大语言模型（LLM）“胡说八道”的问题可能并不像许多技术创始人和 AI 支持者宣称的那样容易解决。华盛顿大学计算语言学实验室语言学教授 Emily Bender 对此表示悲观，“幻觉问题根本无法解决，这是由技术与拟议用例之间不匹配所必然引发的结果。”</p><p></p><p>根据 Jasper AI 公司总裁 Shane Orlick 的说法，某些情况下适当的“胡说八道”反而并不是坏事。Orlick 解释称，“幻觉实际能带来额外的好处，一直有客户在感谢我们带来的启发，而根源就是 AI 可能在种种机缘巧合之下输出客户自己从未想到过的故事或者角度。”</p><p></p><p>同样的，AI 幻觉对于 AI 图像生成也有着巨大的助益，Dall-E 和 Midjourney 等模型正是凭借这份想象力生成了引人注目的精彩图像。也就是说，只有在文本生成领域，幻觉才是个真正困扰用户的问题，特别是在新闻报道等高度强调准确性的场景之下。</p><p></p><p>Bender 指出，“大语言模型的基本原理就是‘编造’内容，这也是其一切功能的根本。但由于能力源自编造，所以当它们输出的文本恰好可以正确匹配我们的提示词时，这种情况反而是种偶然。哪怕经过微调的模型能够在大多数情况下都保持正确，它们也仍无法彻底摆脱故障。而且，未来的幻觉很可能以文本阅读者更难以注意到的模糊状态存在。”</p><p></p><p>结&nbsp; &nbsp; 语</p><p></p><p>大语言模型是种能够实现非凡功能的强大工具，但企业乃至整个科技行业必须意识到一点——不能单纯因为某种事物很强大，就认定它是一种好用的工具。就像冲击钻也很好用，能够轻松破开人行道和沥青路面，但没人敢把它带到考古挖掘现场。</p><p></p><p>正如 Bender 所指出，大语言模型在最初开始训练的那一瞬间，就是在尝试根据我们给出的提示词预测序列中的下一个单词。训练数据中的每个单词都被赋予了权重或者百分比，以便在给定的上下文中追踪之前既有的给定单词。可这些起先的单词本身并没有充分切实的含义或者重要的上下文来保证输出准确。</p><p></p><p>换言之，这些大语言模型只是出色的模仿者，它们实际并不清楚自己到底在说些什么，所以过度信任它们只会令用户陷入困境。这个弱点是大语言模型所固有的，尽管“幻觉”可能在未来的迭代中逐渐减少，但问题本身却可能永远无法被真正修复。</p><p></p><p>参考链接：</p><p></p><p><a href="https://www.engadget.com/microsoft-is-adding-ai-powered-summaries-to-">https://www.engadget.com/microsoft-is-adding-ai-powered-summaries-to-</a>" 必应 -search-results-203053790.html?src=rss</p><p></p><p><a href="https://www.techradar.com/computing/artificial-intelligence/">https://www.techradar.com/computing/artificial-intelligence/</a>" 必应 -has-been-revamped-to-prioritize-ai-search-results-whether-you-like-it-or-not</p><p></p><p><a href="https://www.techradar.com/computing/artificial-intelligence/chatgpt-and-other-ai-chatbots-will-never-stop-making-stuff-up-experts-warn">https://www.techradar.com/computing/artificial-intelligence/chatgpt-and-other-ai-chatbots-will-never-stop-making-stuff-up-experts-warn</a>"</p><p></p><p><a href="https://techcrunch.com/2024/07/24/bing-previews-its-answer-to-googles-ai-overviews/">https://techcrunch.com/2024/07/24/bing-previews-its-answer-to-googles-ai-overviews/</a>"</p><p></p><p>内容推荐</p><p></p><p>在这个智能时代，AI 技术如潮水般涌入千行百业，深度重塑生产与生活方式。大模型技术引领创新，精准提升行业效率，从教育个性化教学到零售精准营销，从通信稳定高效到金融智能风控，AI 无处不在。它不仅是技术革新的先锋，更是社会经济发展的强大驱动力。在 AI 的赋能下，我们正迈向一个更加智能、便捷、高效的新未来，体验前所未有的生活变革与行业飞跃。关注「AI 前线」公众号，回复「千行百业」获取免费案例资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0779541886d6212211f10391187b0f5.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/79/791c6d47a29abdea4f3ba09bea3b176a.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621971&amp;idx=1&amp;sn=5e58c5a72a2d7fae816471954959b349&amp;chksm=fbeba49ccc9c2d8a35501b45bea911c9b684944634522011a859022dad48c9fda4b3a4e3b8fc&amp;scene=21#wechat_redirect">《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621831&amp;idx=1&amp;sn=5ff4ba1979a3e77a914e8b6c5d390db7&amp;chksm=fbeba508cc9c2c1e83c061d1dbd107dca94d93ccda4172996c67d7920e2846d39123b77ee22b&amp;scene=21#wechat_redirect">“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621777&amp;idx=1&amp;sn=c6805493b8fdc7fefe72e8fd99cbd323&amp;chksm=fbeba55ecc9c2c48d97ff000e9874945ff2424c25e3d61d3b6f9d1e016fff7c5d8b449556e4a&amp;scene=21#wechat_redirect">朱啸虎押注的AI公司被围攻：领导多次让员工“去死”；小红书激励不再与职级挂钩；谷歌前CEO：AI创业可先“偷”后处理｜AI周报</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621772&amp;idx=1&amp;sn=031cd69a65396e1e2bd2c9937008134c&amp;chksm=fbeba543cc9c2c554037e36e088163440a7dd3c994673e9ac9d66d4f58aa2fce516eb290d868&amp;scene=21#wechat_redirect">要求员工点赞拉踩贴、抢到对方客户给奖金！40 多位知情人曝这两家 AI 数据商业巨头“生死大战”，如今“开撕”微软</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621721&amp;idx=1&amp;sn=ff4df9b1712358edc181e34ea0a3c89c&amp;chksm=fbeba596cc9c2c8000958fc0def83fe925093fbe55c2f4ae8b0b979c6df37c49a0ceb75e59cc&amp;scene=21#wechat_redirect">成本直降90%、延迟缩短80%！Anthropic将API玩出了新花样，网友：应该成为行业标配</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247620641&amp;idx=1&amp;sn=44cdebfad0decb39633875fc4069c7fc&amp;chksm=fbeba1eecc9c28f81fc4c7c10d9e95329e4e0ed2d4c1f0eff6ca7f19376846f6a297b33e15d3&amp;scene=21#wechat_redirect"></a>"</p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</id>
            <title>跟着小扎不白干，9 个月“出师”：用学到的 10 条经验搞出 AI 界“带货王”，年入 1 亿美元</title>
            <link>https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/s8bfk2wlAdJJODyViD0c</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 06:02:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>编译 | 核子可乐、华卫</p><p></p><p></p><blockquote>一位Facebook的早期员工Noah Kagen创业成功后，在他的个人网站上分享了他从马克·扎克伯格（Mark Zuckerberg）和 Facebook 那里学到的10条经验教训。Kagen是Facebook的第30号员工，在工作时长9个月后被扎克伯格解雇。离开Facebook后，他创立了软件产品推广和营销平台AppSumo ，并通过总结的扎克伯格工作“之道”将其打造为一家年收入 1 亿美元的公司。AppSumo是一家专注在软件产品的 LTD 平台，一方面为软件产品的开发者提供销售 LTD 的渠道，另外一方面为小公司和创业者提供了购买 LTD 的渠道。在生成式 AI 兴起后，AppSumo 也帮助大量带有 AI 功能的产品提高了销售收入、流量和订阅用户，包括AI聊天机器人平台Juphy、AI 内容生成工具Castmagic等。</blockquote><p></p><p></p><p></p><h1>在扎克伯格手底下干活，我的一点心得</h1><p></p><p></p><p>第一次走进位于帕洛阿尔托大学大道的Facebook总部大楼的时候，我竟一时分不清自己身在高校社团还是创业公司。天花板上吊着电缆，人们匆匆往来，而我则按要求在其他人的办公桌角上挤出个位置。</p><p></p><p>我的新上司从身边走过，说午饭之后再来找我谈话。之后又有人塞给我一台笔记本电脑，闲来无事我就先上会网。后面，有人告诉我得马上开始准备，30分钟后得在马克·扎克伯格的即兴会议上做汇报。</p><p></p><p>扎克伯格走进会议室，平静地告诉我：“你的上司刚刚被炒了，欢迎来到Facebook。只要你不背着我出场公司利益，那就能在这里好好待下去。”而好戏，这时候才刚刚开场……</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/17/17276988ffae1fcb17e877c916de7f25.png" /></p><p></p><p></p><p>在Facebook的工作经历，可以说是我这辈子最美好、但也最痛苦的一段回忆。我是公司第30号员工，而短短9个月之后就被解雇了。很长一段时间，我一想到自己被裁撤的命运就非常痛恨这家企业。</p><p></p><p>但我从扎克伯格和Facebook那边学到的经验，最终也帮助我将AppSumo打造成了一家年收入上亿美元的公司。下面聊聊我在扎克伯格手底下工作时，自己总结出来的10条经验：</p><p></p><p></p><h2>1.专注于单一目标</h2><p></p><p>我曾经恳求道，“马克，咱们一直没能盈利。要不试试在Facebook办的会上销售门票？”他说不行，之后用白板笔写下了几个字：增长。</p><p></p><p>马克的目标是让Facebook拥有10亿用户。面对我们提出的每个主意，他都会问：“这对业务增长有帮助吗？”如果这些想法跟业务增长的目标关系不大，那就果断放弃。</p><p></p><p>快速成长不是同时把多件事做到80分，而是专注于把一件事做到100分。</p><p></p><p></p><h2>2.加快脚步</h2><p></p><p>在Facebook，每天工作12个小时以上属于常态。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c080fbd7070c846f4bdbdb237cf5a35d.png" /></p><p></p><p></p><p>马克总在敦促我们要有紧迫感。他在公司里常说的口头禅就是“加快脚步，打破常规”。“如果你还没打破常规，就说明你的脚步还不够快。”</p><p></p><p>我们的想法很简单，为了加快行进速度、全面了解社区需求，我们宁愿忍受一定数量的bug和缺陷。每天，我们都会向网站发送几项更新。相比之下，像微软这样的公司则需要几个月的时间才能勾勒出产品细节，之后经过大大小小的会议讨论，最后再开始着手构建。</p><p></p><p>作为一家初创公司，我们跟行业巨头相比的最大优势就是速度。</p><p></p><p></p><h2>3.只雇佣最出色的员工</h2><p></p><p>马克只会雇佣那些他愿意与之共事的员工，甚至我们的客户支持团队里，也挤满了来自哈佛的博士。这帮曾经效力于Facebook的人们后来参与创立的Asana、Quora、AppSumo还有OpenAI等等。</p><p></p><p>对于任何一家初创公司来说，雇佣的前十个人都是最重要的，而其中每个人都占据公司的10%。如果有三个人不够优秀，那就代表公司里30%的部分不够优秀！</p><p></p><p>相较于大公司，初创企业更依赖于优秀的人才。</p><p></p><p></p><h2>4.善待员工</h2><p></p><p>马克意识到，打造出让人愿意身处其中的工作环境不仅有助于吸引更多优秀人才，同时也能让现有员工生出对企业的自豪感来，甚至愿意主动加班。</p><p></p><p>因此，Facebook做了很多现如今已经被视为行业常态的探索：</p><p>⦁ 在硅谷最昂贵的社区之一设立一座豪华办公楼。</p><p>⦁ 开出极具竞争力的薪酬。</p><p>⦁ 为每个人购置1000美元的办公椅。</p><p>⦁ 免费提供PowerBook和黑莓手机。</p><p>⦁ 提供美味的早、中、晚餐。</p><p>⦁ 冰箱里有你所能想到的任何饮料。</p><p>⦁ 公司支付拉斯维加斯旅行的所有费用。</p><p>⦁ 每周五免费餐食发放。</p><p>⦁ 免费洗衣/干洗服务。</p><p>⦁ 补贴住房。如果住在办公楼周边1英里之内，每月可以领取600美元。</p><p>⦁ 面向全体员工开放的夏季/冬季度假小屋。</p><p></p><p>人们希望得到认可，而这种对员工的善待能够提高工作效率，帮助大家抖擞士气。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f5fd04099933a79f101ffae3b513fe3d.png" /></p><p></p><p>Facebook早期派对</p><p></p><p></p><h2>5.按自己的路子走</h2><p></p><p>很多人之所以会选择那些自己不太了解或并不感兴趣的创业领域，是因为他们听说这个方向很“热门”。比如，他们原本是做会计的，但创业时却尝试帮内容创作者开发专业软件……这简直是在胡扯。</p><p></p><p>从一开始，马克想搞的根本不是什么初创企业——他只是想帮大学里的学生们建立联系。而我自己创办AppSumo，是因为我喜欢科技产品和处理交易。</p><p></p><p>不少顶尖企业刚开始都是这样来的，创始人们先是尝试解决自己面临的问题，之后再把解决方案分享给更多人。这就叫生于自私，而成于无私。</p><p></p><p></p><h2>6.关注细节</h2><p></p><p>我记得马克曾经在凌晨3点给我发过一封电子邮件，告诉我在一份文件中漏了一个句号。是的，一个句号！！</p><p></p><p>马克不接受任何不完美的东西。如果他觉得某个项目做得不好，就会告诉负责人果断放弃、推倒重来。他对Facebook里面这个F要大写就特别偏执，甚至曾经送给我一本语法书让我好好打磨文笔 😂</p><p></p><p>马克为我们设定了高到卓越的标准，这让工作做起来很有挑战性，但也非常有益。</p><p></p><p></p><h1>7.向团队放权</h1><p></p><p>令人意外的是，马克却并不会过多参与日常运营。虽然有时候也会参与代码编写，但他的大部分时间都专注于制定宏观愿景。他特别擅长给人们设定目标、划出界限，然后从旁提供指导。</p><p></p><p>工程师和产品经理们可以自行提出功能并着手构建，期间无需任何额外的审批和干预。马克曾说他想要Facebook的手机版，而初版的所有细节都由我们一线开发自行斟酌。</p><p></p><p>只有团队感受到这种主人翁的地位时，大家才会像主人一样思考和行事。</p><p></p><p></p><h2>8.是“人”，不是“用户”</h2><p></p><p>每当有人使用“用户”这个字眼，马克都会气得大叫。没错，就是音量很大那种。他咆哮道，“那些是活生生的人！”</p><p></p><p>在产品中充分考虑人性化因素，能让厂商更好地为客户服务。与只看数字相比，这个角度也能让我们更好地理解困扰受众的问题。</p><p></p><p>所以要永远记得，冷冰冰的用户名和邮件地址背后，对应的都是活生生的人！</p><p></p><p></p><h2>9.只留合适的人</h2><p></p><p>就在我入职的当天，我顶头上司被开除了。我的下任上司在一个月后被炒掉，而我自己是在9个月之后。马克非常重视的一条原则，就是只留合适的人。</p><p></p><p>他会果断解雇那些拖累了Facebook发展的人，并迅速提拔能够帮助Facebook实现目标的人。</p><p></p><p>在AppSumo，我们也会对潜在的新同事进行付费试用，之后再决定对方适不适合接受这份全职岗位。</p><p></p><p></p><h2>10.风物长宜放眼量</h2><p></p><p>当初马克面对10亿美元的Facebook收购要约时，我们都才20多岁。而当他表示拒绝时，实际是向我们所有员工包括全世界发出了明确的信息：他的目标是让整个世界连通起来，这让我们无比兴奋。</p><p></p><p>当初在Facebook工作时，我做的一切就是思考、讨论和畅想Facebook的未来。这甚至不像是一份工作，Facebook就如同我的女朋友，占据了我的所有时间和心力。</p><p></p><p>这种宏大的愿景激励员工们从床上蹦起来，冲进办公室尽最大努力完成工作。它让员工们有了一种超越金钱的目标感和使命感。</p><p></p><p></p><p></p><h1>被Facebook解雇的四点反思</h1><p></p><p></p><p>除从扎克伯格那里学到的有用经验外，此前 Kagan 还曾在一本电子书里总结了自己被 Facebook 解雇的原因。 在 Kagan&nbsp;看来，自己过去在Facebook的工作中犯了四个错误，才导致扎克伯格认为他是一个需要被解雇的“累赘”。</p><p></p><p></p><h1>1.向媒体泄露了公司机密</h1><p></p><p></p><p>在科切拉音乐节上的一次醉酒后，Kagan&nbsp;告诉外媒TechCrunch的创始人迈克尔·阿灵顿（Michael Arrington），Facebook计划将业务范围从大学生扩展到为Microsoft和Apple等公司提供专业社交网络。原本Facebook准备在第二天早上公布这一消息，但在与 Kagan&nbsp;的谈话后，阿灵顿当晚便发布了这一新闻。几周后，Kagan&nbsp;便被解雇了。</p><p></p><p></p><h2>2.试图利用Facebook为自己出名</h2><p></p><p></p><p>Kagan自述，他过去常常在Facebook总部举办创新企业聚会，因为他享受炫耀自己的工作场所，还经常在自己的个人网站 OKDork.com 上写关于Facebook业务的博客文章。扎克伯格曾将Kagan拉到一边，让他在自己和Facebook之间做出选择。不知何故，卡根当时仍然没有理解扎克伯格的意图，因此后来也没保住自己的工作。</p><p></p><p></p><h2>3.工作中出现失误</h2><p></p><p>Kagan对此举了一个例子：“我在与（Facebook联合创始人）达斯汀·莫斯科维茨（Dustin Moskovitz）合作决定哪些公司能够加入我们的专业网络时，负责在谷歌上搜索企业名单。经过一个星期的收集，我给出的公司名单乱七八糟，没有任何顺序可言。把这份名单交给达斯汀后，他当然很失望。之后他运行了数据库查询，并根据我们已经在网站上注册的公司域名汇总了一些公司，然后将这些公司添加到候补名单中。是的，这样做聪明多了。”</p><p></p><p></p><h2>4.跟不上Facebook的增长</h2><p></p><p>Kagan 加入 Facebook 时，该公司只有 30 名员工和几百万用户。当他被解雇时，公司已经有 100 多名员工，并逐渐发展成为一家发展速度稍慢、需要管理的人更多的公司。而Kagan之后并没有改变自己的工作方式以适应公司文化的变化，还进行了一定程度的抵制。他写道：“在事情混乱和需要完成任务的时候，我是公司里最出色的员工之一。但我在处理多人的项目、组织几个月的进度计划以及处理政治事务方面都很吃力。”</p><p></p><p>参考链接：</p><p><a href="https://noahkagan.com/what-i-learned-working-for-mark-zuckerberg/">https://noahkagan.com/what-i-learned-working-for-mark-zuckerberg/</a>"</p><p><a href="https://www.businessinsider.com/how-noah-kagan-got-fired-from-facebook-and-lost-185-million-2014-8">https://www.businessinsider.com/how-noah-kagan-got-fired-from-facebook-and-lost-185-million-2014-8</a>"</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</id>
            <title>携手攀登安全“芯”高地！2024紫光同芯合作伙伴大会安全芯片创新应用论坛圆满落幕</title>
            <link>https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/OyhqIHriWz00fr7k3xcH</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 03:07:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>8月22日，2024紫光同芯合作伙伴大会安全芯片创新应用论坛在北京圆满落幕。本届论坛以“智慧芯生态&nbsp;互联芯安全”为主题，聚焦金融支付、电子证件、安全识别与移动通信领域的硬件创新、软件算法、技术趋势等行业议题，产业链各方汇聚一堂，为安全芯片创新应用发展和产业生态建设提供了全面解题思路和最佳实践参考。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/c9/ca/c9a84a132d39be36c2d32c52e72a97ca.png" /></p><p></p><p>&nbsp;</p><p>支付、证件、识别：芯之所向，无所不至</p><p>&nbsp;</p><p>在数字化浪潮推动下，金融支付行业面临转型升级，安全芯片应如何乘势而上？来自北京银联金卡科技有限公司、金邦达有限公司、福建新大陆支付技术有限公司等企业的代表分别发表演讲。他们表示，作为保障金融交易安全不可或缺的一环，安全芯片的功能优化升级成为行业共识；面对生物识别、物联网支付等新兴技术和日益复杂的应用场景，安全芯片将在安全性、便捷性、功能性这几个关键维度实现更大突破。</p><p>&nbsp;</p><p><img src="https://static001.infoq.cn/resource/image/b4/0e/b468c48938af12502f971f9516e3e50e.png" /></p><p></p><p>&nbsp;</p><p>论坛上，紫光同芯安全芯片事业部副总经理路倩发表了《芯之所向无所不至——支付证件识别产品创新之路》主题演讲，她表示，安全芯片产品从聚焦性能提升和应用演进，到追求为行业打造极致安全、极致可靠、极致便捷和极致性价比的产品生态，紫光同芯产品的技术迭代与市场需求和行业标准密不可分，并将始终遵循以客户为中心和以专业技术为基石的原则。未来，紫光同芯将通过技术创新，紧密贴合国际安全标准的升级步伐，灵活适应日趋多元的应用场景，持续不断为行业注入芯动力。</p><p>&nbsp;</p><p>移动通信：无般不识，大器已成</p><p>&nbsp;</p><p>在AI、5G等前沿技术推动下，数字化进程渗透千行百业。作为数字化时代的基础支撑，全球信息通信行业监管正在向以促进数字经济发展为目标的新方向演进，技术创新、数字化转型、算网融合、安全保障提升成为大势所趋。</p><p></p><p>探讨当前通信行业趋势，中国移动研究院业务研究所和星汉智能科技股份有限公司等企业的代表分别发表演讲。聚焦超级SIM多应用操作系统的生态建设，中国移动联合产业制定了多应用操作系统产业标准，并将持续推进多应用操作系统的泛行业生态建设，坚持以开放和协作促进产业可持续发展；拥抱数字化转型机遇，星汉智能表示，作为数字化底层基础，eSIM技术促进各行各业数字化转型的进程，将成为企业发展数字产品及服务不可或缺的重要支持。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/8e/71/8ec06f0fddb4c5acc5b25c2728b51571.png" /></p><p></p><p>&nbsp;</p><p>紫光同芯安全芯片事业部副总经理、移动通信产品线总经理王征结合紫光同芯在通信领域的创新探索，回顾了SIM卡持续发展的历程。他表示，SIM从追求极致性价比出发，历经追求极致可靠的M2M SIM，直至现今在追求极致性能与安全并重的eSIM领域深耕细作，紫光安全芯片已经在移动通信领域实现了全品类覆盖，帮助全球行业客户布局数字产业生态。未来，随着AI、5G及卫星通信等技术发展，紫光同芯的SIM之路将朝着更便捷、环保、安全、强大的方向继续演进，为万物互联注入芯力量。</p><p>&nbsp;</p><p>智慧芯生态，互联芯安全。紫光同芯以技术为基础，以客户为中心，以市场为导向，致力于为全球伙伴提供安全可靠、高效便捷的产品和服务。未来，紫光同芯期待与更多伙伴一道聚合产业优势，融通生态链条，驱动技术创新与应用覆盖，助推产业生态向更智能、更便捷、更安全的方向加速迈进，以科技之光照亮幸福生活。</p><p></p><p>活动推荐：</p><p></p><p>芯片作为最底层的设施受到许多从业者的关注，在10 月 18-19 日，由InfoQ主办的 QCon 全球软件开发大会（上海站）上，我们特别策划了【大模型基础设施与算力优化】专题，将深入探讨如何搭建稳定高效大模型基础设施，提高各类大模型训练推理过程中的 Scaling 的效率和成本，为一线技术工程师和高级技术管理人员提供前沿知识、一手的实践经验和有深度的技术判断。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</id>
            <title>科大讯飞做大模型：功能不需样样顶尖，先打造业务需要的能力</title>
            <link>https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/xLgQzB1Wc1OSYZCXe8c7</guid>
            <pubDate></pubDate>
            <updated>Fri, 23 Aug 2024 01:42:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>“讯飞研究院并非一个纯粹闭门造车的技术研究院。”科大讯飞副总裁、研究院院长刘聪说道。从 2005 成立至今，讯飞研究院为科大讯飞的产品提供了有力的技术支持，这次大模型浪潮中也不例外。</p><p></p><p>正如刘聪所说，“研究院的大部分技术都对应着具体的业务需求。”讯飞研究院一边迭代自己的基座模型，一边深入业务需求进行相关研发。</p><p></p><p>2022 年 12 月，讯飞启动了“1+N 人工智能大模型技术及应用”专项攻关，其中“1”代表通用人工智能大模型底座，“N”代表将人工智能大模型技术应用在教育、医疗、汽车、办公、智能硬件等多个行业领域。如今，讯飞对“1”和“N”的理解发生了哪些变化？</p><p></p><p>开发，今年的节奏已经不同</p><p></p><p>从去年 5 月星火大模型首个版本发布至今，一年多的时间里，讯飞研究院已经将该模型迭代到了 4.0 版本，模型也从最初的对标 GPT-3.5，更新至迭代最新的 GPT-4 Turbo。</p><p></p><p>纵观整个去年，讯飞很多产品是集中发布的，基本上 2、3 个月就有一次产品发布。这个节奏与之前几乎一年一次发布的讯飞相比要快很多。</p><p></p><p>刘聪介绍，这个时候的讯飞，更多扮演的是“追赶者”的角色：摸索整个大模型训练过程中的各种经验，比如如何处理数据、scaling law 是否符合预期等，对标国际领先模型，同时关注一些落地场景。另外，国产化也是讯飞要重点推进的工作。</p><p></p><p>对于去年的整体节奏，刘聪认为讯飞做得是比较好的，这源于讯飞会提前做好发布计划，“我们更多的是计划做得比较好，让大家感受到了每个大版本之间的变化。”</p><p></p><p>但是，今年的节奏导向已经与去年有所不同。</p><p></p><p>“今年年初，我们就对大模型这件事情已经摸索得比较清楚了。”刘聪说道，“我们现在既关注通用底座大模型，同时探索也在如何提升小模型的能力和效果。”</p><p></p><p>当前，大模型与小模型并行发展已经是行业趋势。对于选择大模型还是小模型，刘聪表示主要看场景需要什么样的模型。“如果只泛泛地说‘使用小模型与大模型差距不大’，这纯粹是胡说。”</p><p></p><p>刘聪解释道，在撰写文案、代码编写等方面，一个中小规模的模型即可搞定，讯飞将这类应用定义为一般任务；中等任务涉及行业内的很多知识库和行业深度内容，还有一些高难度任务，例如复杂推理、数学推理等，目前大模型都无法解决，更不用提小模型。</p><p></p><p>“我们一直强调大、小模型时代，并不意味着不再关注大模型了。核心技术原理是先找到大模型的天花板，再优化小模型。小模型的不断进步依赖于大模型的发展。”刘聪说道。</p><p></p><p>另外，讯飞研究院更重要的一项任务是围绕“N”中的刚需场景，把大模型应用做透彻，因此深入解决系统化问题变得非常关键。</p><p></p><p>不过在众多的基础能力上，讯飞也是有选择地进行研发。比如在通用任务中，讯飞最关注的能力之一是数学，因为在刘聪看来，数学能力与推理结合是大模型聪明的表现。</p><p></p><p>但是，不同于有的公司有专门的文生图产品，讯飞的文生图是在星火统一入口里面使用。刘聪明确称，“在文生图方面，我的优先级较低，甚至不专门制作文生视频。虽然我们与视频关系不大，但是我们会制作虚拟人、加强语音能力，我们必须做好语音交互。”</p><p></p><p>在刘聪看来，大模型底座是向多模态拓展的，对讯飞而言多模态的能力逐步提高最重要，但没有必要在一些业务关联度低、资源投入过大的方面做太多投入。在多模态中，刘聪会将重点放到 OCR（Optical Character Recognition，光学字符识别）上，“确保 OCR 做到最好，这与我的实际工作紧密相关。”</p><p></p><p>基于此，讯飞今年的重点虽然还是大模型通用能力的打造，但讯飞不会选择样样争第一，而是在自己认为的最重要的方向发力，比如交互能力等。</p><p></p><p>应用，选择更加熟悉的方向</p><p></p><p>讯飞研究院的研发工作与业务紧密相连，在研发之前，研究院要与业务部门达成深度共识，比如某个功能达到什么程度、完成客观技术指标后能为用户带来什么价值等。</p><p></p><p>达成共识之后，从研究院内部的算法研发部门、工程引擎部门、服务平台部门和资源部门，再到产品研发部门，整个过程需要一起对齐。无论发布产品、然后不断迭代，还是创新性研发一个产品，都是这样的过程。</p><p></p><p>讯飞被外界认为是较少能真正将技术实现产品落地的企业，刘聪认为这背后的核心原因是讯飞更加深入场景。</p><p></p><p>“我们找 PMF 之所以准确，是因为过去对行业场景和技术的积累。坚持阶梯原则，我们了解大模型在哪个节点可以适配、哪个场景可以发挥价值。”刘聪说道。“此外，讯飞也有深厚的场景资源和用户基础。”</p><p></p><p>以学习机为例，讯飞过去十几年从事学校工作，每天在学校里与老师打磨，持续了解中国教育政策以及未来发展趋势。老师的教学环境如何、不同年龄段的孩子是否有时间额外学习等，如果仅凭想象和拍脑袋是很难定义出来的。教育行业讲究因材施教，而非图文等技术。</p><p></p><p>落地中，选择在已积累的行业优势基础上进行大模型探索，是大多数相对成熟公司会选择的风险相对较低的策略。“自我造血非常重要，所以我们更加关注相对熟悉的方向，例如教育、医疗、办公、汽车和金融。”刘聪说道。</p><p></p><p>而什么时候完成应用则与大模型发展阶段有关系。围绕刚需场景，什么技术可以支撑、支撑度如何等都需要考虑。比如技术阅卷，之前是判断填空、选择题，后来扩展到了解答题并全学科阅卷，这都对技术要求越来越高。有了大模型后，直观的表现之一就是作文批改比之前做得更好。</p><p></p><p>讯飞业务中，硬件是不可忽略的一部分，比如有面向教育的学习机、批阅机等。讯飞业务的特点之一就是每个行业都有软硬件的差异。比如学习机不断将软件功能加到硬件上，以此提升硬件附加值。同时，硬件模式又能助力软件，例如翻译机和办公本都有一些大模型应用来升级体验，这不仅仅是单纯利用大模型的 API 连接，而是形成了适合硬件场景的独特功能。</p><p></p><p>而对外服务中，刘聪观察到，大模型的应用范围已经逐渐变大，比如金融这样的代表性场景已经往央国企拓展。“对应用大模型的企业来说，产品价值最重要的是能否降本增效。”刘聪说道。</p><p></p><p>讯飞在对 B 端业务服务过程中，发现算力统一难和整个数据管理难等问题。另外，在对外服务过程中，由于很多企业是私有化部署，因此讯飞在底座模型应用和场景开发中，对用户的场景并不清楚。为此，讯飞通过智能体平台这样的服务来解决。</p><p></p><p>“N 的逻辑必须落地。现在的阶段与去年不同，去年我们的 1+N 有些冗余，需要继续梳理。今年我们将主要的 N 梳理清楚后，一和 N 的协同变得更加系统。”刘聪说道。</p><p></p><p>根据实践观察，刘聪总结了两点经验：</p><p></p><p>第一，不必专门针对“N”，可以将其合入“1”的能力中。一个场景下的常用能力可以满足，或者在 1 基础上做某个智能体就能满足，合入“1”里就可以，这是减少重复开发的逻辑。</p><p></p><p>第二点，统一模型接口和数据接口。这里的 N 可能是业务线主导，有的是研究院主导，但一个公司内部的每个业务数据标注体系如果都不同，那将它们合并汇总到主模型就会相当困难。完成模型接口后，需要标注数据、SFT 数据和强化学习数据，形成一个技术体系。在此框架下，用户可以自行寻找专家进行标注，这样既能优化流程，又能将这些 N 的数据回流到数据库中。</p><p></p><p>结束语</p><p></p><p>对于今年讯飞的“1”和“N”而言，刘聪表示，“虽然是动态发展的，但是不能放弃。如果不演进，三个月就不行了。”</p><p></p><p>不过，随着模型规模的增大，研发周期会逐渐拉长，因此刘聪认为大模型技术后续可能不一定还那么卷。“GPT-5 底座大模型投入巨大，升级周期会变长，局部亮点可能会不断出现，但可能很难有 GPT-3.5 到 4 那么大的提升。”</p><p></p><p>在大模型争夺战中，讯飞给自己的定位是“综合能力是头部，在自己擅长的地方保持耐心和耐力”，因为一个很现实的问题就是大模型的企业同质化严重，但其实想要在每个领域都做到最好很难，OpenAI 和谷歌都做不到。</p><p></p><p>“我们还要给用户习惯的时间，通过产品培养用户和客户的耐心。”刘聪说道。</p><p></p><p>内容推荐</p><p></p><p>在这个智能时代，AI 技术如潮水般涌入千行百业，深度重塑生产与生活方式。大模型技术引领创新，精准提升行业效率，从教育个性化教学到零售精准营销，从通信稳定高效到金融智能风控，AI 无处不在。它不仅是技术革新的先锋，更是社会经济发展的强大驱动力。在 AI 的赋能下，我们正迈向一个更加智能、便捷、高效的新未来，体验前所未有的生活变革与行业飞跃。关注「AI 前线」公众号，回复「千行百业」获取免费案例资料。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c0/c0779541886d6212211f10391187b0f5.png" /></p><p></p><p>会议推荐</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 QCon 全球软件开发大会 ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p><p>今日荐文</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621559&amp;idx=1&amp;sn=35db58c708c2a1ab0ab3bb3307014d2b&amp;chksm=fbeba278cc9c2b6e27f50c0361eb0202485480e2e2a0a033e14c819f406fef31599a0a2fab1a&amp;scene=21#wechat_redirect">“创业一年，人间三年”，李沐亲述 LLM 创业第一年的进展、纠结和反思</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621514&amp;idx=1&amp;sn=d61a90572d1ece086f4c8238e82b2073&amp;chksm=fbeba245cc9c2b53f02c6ae3a2a3dd0714e5571ac5e12f5dccbb42bd4b5223b1e2979cb34269&amp;scene=21#wechat_redirect">刚刚，OpenAI又双叒叕鸽了！没等来“草莓”发布，只敷衍发了评测集，网友：拿这来抢谷歌发布会风头？</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621462&amp;idx=1&amp;sn=7fdb125768fc1da501d6ddc64efb7fce&amp;chksm=fbeba299cc9c2b8f23b1924c2c3ce0715ee8b2f3dc4a2b34bbf36b7e6d1927401d767b479774&amp;scene=21#wechat_redirect">三年亏损51亿元，去年卖出22台车！文远知行被爆赴美IPO，估值超360亿元</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621417&amp;idx=1&amp;sn=c2527e66bd2f71ae9f8502f019ad02b7&amp;chksm=fbeba2e6cc9c2bf0aa096968c96e502b20da1bef64c5e0be38d2ffc1ed740b9eb1b2b8bfbe21&amp;scene=21#wechat_redirect">一年前还看好，现在却急刹车？国内资本动辄数十亿投资，华尔街却不敢给了</a>"</p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247621293&amp;idx=1&amp;sn=f7a1e3bd450fdb11e2f23d3019804a9d&amp;chksm=fbeba362cc9c2a74471529947135ee0e3883cd5933487a07bf3f77d3bdd1d37932518d2203d3&amp;scene=21#wechat_redirect">京东发行稳定币；AI服务器大厂豪气分红115.2亿；小米二期工厂附近挖出古墓？王化：假的｜AI周报</a>"</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/c5/c548af7ad39632ca346cd454eef4d2a8.gif" /></p><p></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247620641&amp;idx=1&amp;sn=44cdebfad0decb39633875fc4069c7fc&amp;chksm=fbeba1eecc9c28f81fc4c7c10d9e95329e4e0ed2d4c1f0eff6ca7f19376846f6a297b33e15d3&amp;scene=21#wechat_redirect"></a>"</p><p></p><p>******你也「在看」吗？******👇</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</id>
            <title>顺丰揭秘：大模型技术如何重塑物流供应链</title>
            <link>https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/rBVmYQGeaHJCxyTks1pX</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 10:08:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>物流与供应链系统的庞大规模、环节的复杂多变、数据的复杂性、场景的多样性，使得物流与供应链系统的建设和运行变得更加复杂。而大模型技术作为 AI 的一项重要成果，在物流供应链领域具有无限的潜力和广阔的应用前景，并在推动物流供应链领域数字化、智慧化变革中扮演着不可忽视的角色。</p><p></p><p>在 8 月 18 日 -19 日的 <a href="https://aicon.infoq.cn/2024/shanghai">AICon 全球人工智能开发与应用大会</a>"上，InfoQ 荣幸地邀请了顺丰顺丰科技人工智能总工程师高磊，他为我们分享了大模型在物流和供应链场景中的应用，以及顺丰相关技术体系与产品体系的建设思路与实践经验。本文会会前采访文章，期待你对了解大模型在物流行业的应用有所帮助！</p><p></p><p>以下为采访正文～</p><p></p><h5>InfoQ：顺丰在建设物流决策大模型技术体系时，采用了哪些具体的技术手段和方法？这些技术是如何与现有的物流和供应链系统进行融合的？</h5><p></p><p></p><p>高磊： 当前 AIGC 技术主要以文本、图片、语言、视频等模态为主，因此在这些信息相对富集以及以这些模态作为主要信息载体的领域更容易落地，比如售前的营销素材的生成，售后的智能客服，以及办公领域的 FAQ、信息摘要等场景。</p><p></p><p>但是我们所关注的供应链运营和决策优化领域中，如何利用大模型与其背后的技术去解决供应链运营过程中问题，提升决策质量和效率，帮助客户业务更好的降本增效，目前并未看到很好的行业实践和落地案例。我们结合对于大模型技术的理解与顺丰的业务实践，逐渐摸索出三个方向：</p><p></p><p>结合顺丰沉淀的业务 know-how 与已有技术能力，构建行业智能体，相关技术被应用于供应链智能控制塔产品中突破文本、图片等模态的限制，构建物流决策大模型，让大模型技术直接作用于核心决策问题，相关技术被应用于供应链执行优化产品中基于多模态大模型的能力构建多层级多通道需求预测模型，解决消费供应链领域中需求预测的难点与痛点问题，相关技术被应用于供应链计划产品中</p><p></p><h4>需求预测模型在供应链计划产品中的应用案例</h4><p></p><p></p><h5>InfoQ：可以介绍一下什么是基于多模态大模型的能力构建多层级多通道需求预测模型吗? 它解决哪些痛点与难点问题？</h5><p></p><p></p><p>高磊： 我们构建这个模型的初衷是为了解决消费供应链领域中商品蚕食效应、新品新店预测等业界难题。</p><p></p><p>首先，需求预测在供应链计划中非常重要，因为他是需求计划、供应计划、生产计划等诸多计划的源头，准确的需求预测对于提升计划准确性，提升供应链效率而言至关重要。但是需求预测本身难度很大，尤其是消费供应链，受到诸多因素的影响，比如新品上市、老品下架、蚕食效应、促销、节假日、季节、天气等。其中蚕食效应，新品和新店的预测一直是行业普遍存在的难题，传统的算法模型难以有效解决这些问题。</p><p></p><p>以商品蚕食效应和新品上市为例，比如某个门店一直卖 10 种蛋糕，平时所有蛋糕的销量总和是大约 100，然后某一天突然上市了一个新的蛋糕，并做了促销，那么这里会出现两个问题：1. 新蛋糕的销量该如何预测，2. 老蛋糕的销量会受到多大影响？</p><p></p><p>传统的需求预测模型从单一商品视角建模，在解决这两个问题上存在较大困难：在第一个问题上，因为缺乏历史销量数据，很难建模，往往预测偏差很大，在第二个问题上，单一商品视角的建模难以有效捕捉商品之间的关联关系与相互影响，在上新期间难以捕捉到蚕食效应造成老品系统性的偏高。</p><p></p><p>为了解决这些行业难点问题，我们设计了基于多模态大模型的能力构建多层级多通道需求预测模型，从特征角度，我们通过预训练好的多模态模型将商品的文字描述如商品名，商品描述，配料表，价格等和商品的图片提取为表征商品内在属性的 Embedding 向量。通过选择合适的多模态大模型，我们发现提取出的 embedding 能够很好的表达商品之间内在的一些相关性。</p><p></p><p>提取了多模态特征之后，为了更好的学习商品之间的关联性，我们设计了一种多层级多通道的需求预测模型。</p><p></p><p>这里解释一下层级的概念，消费供应链预测中往往存在多种层级，比如时间层级：日到月到年；空间层级：门店到 RDC 到 CDC；品类层级：具体的 SKU 到二级品类到一级品类，往往在各种层级上都要输出预测结果，并且层级之间的结果应该能够对应上，比如某个门店内所有商品的总销量预测应该等各个商品预测之和。</p><p></p><p>多层级多通道的需求预测模型能够很好地学习同一层级内的商品之间的内在关联性，以及层级之间的关联性，从而更好的得到预测结果。</p><p></p><p></p><h5>InfoQ：这个模型的实际应用效果如何？</h5><p></p><p></p><p>高磊： 我们在某个实际客户的场景下做了测试，整体上，新的模型可以在预测准确性上提升绝对值 5 个百分点，这个是我们在传统方式下做了很久也没有难达到的程度。同时得益于多层级多通道统一建模极大的减少了模型的数量，以及 GPU 的使用，在计算性能方面实现了 120 倍的提升，对机器资源的需求也减少了 5 倍。</p><p></p><p>我们也着重验证了一下新模型在新品等场景下的预测表现，得益于多模态信息的引入与多层级多通道学习机制，新模型能够有效的捕捉到新品和老品之间的相关性与蚕食效应，可以在上新期间取得显著的的新老品预测准确度的提升。</p><p></p><h4>供应链智能体在供应链智能控制塔产品中的应用细节</h4><p></p><p></p><h5>InfoQ：什么是供应链智能体？它具备一些什么样的能力？解决什么问题？</h5><p></p><p></p><p>高磊： 我们知道供应链运营是一个专业程度很高，并且非常严谨的领域，因为任何数据或者决策建议的错误都可能带来比较严重的损失。大模型本身存在一些固有的缺陷如不擅长精确数值计算，幻觉，专业程度不够高等问题，限制了其在供应链运营领域的应用。</p><p></p><p>比如前端时间公众号上有个比较火的文章，讲得是问大模型 9.11 和 9.8 哪个更大，绝大多数大模型都回答 9.11。再比如把过去一段时间的历史销量和库存数据丢给大模型，让它去做库存优化，大模型也很难去做这种专业的事情。为了解决以上问题，我们的解决思路是结合大模型和专业小模型，以及顺丰多年沉淀的供应链实践，去构建供应链的行业智能体。</p><p></p><p>具体来说，我们通过 RAG 技术结合我们沉淀的业务知识库，让大模型具备更深入的供应链知识，同时我们将丰智云体系中沉淀的各种算法能力，比如预测、仿真、运筹优化、归因分析等，抽象成工具并交给大模型调用。由此构建出具备供应链行业知识的业务专家智能体与以及具备专业算法能力的算法专家智能体，并通过这些智能体的协作，去服务具体的业务场景，如销售分析，库存优化等场景。通过以上方式，可以有效的改善和缓解大模型在供应链场景下存在固有缺陷。</p><p></p><p></p><h5>InfoQ：在供应链智能控制塔产品中，顺丰如何集成供应链智能体的能力？</h5><p></p><p></p><p>高磊： 我们知道在供应链控制塔中，有一块很重要的能力是供应链诊断与分析能力，传统方式下，我们需要建立大量的报表来呈现业务指标与各种问题，但是这种形式是相对静态的，当出现新的场景和问题的时候往往还是需要手动获取数据、分析数据或者开发新的报表，难以敏捷的响应新的需求。</p><p></p><p>另外，从数据分析角度来看，大致存在 3 种类型的分析:</p><p></p><p>描述性分析：对数据进行整体概括和总结，以了解数据的基本特征和趋势，形成对业务现状的整体认识诊断性分析：通过深入挖掘数据的背后原因，解释数据异常或变动的原因，并为问题提供决策依据预测性分析：利用历史数据和模型来预测未来事件或趋势的发展，为决策提供先见之明</p><p></p><p>目前传统的控制塔还是以描述性分析为主，在诊断性分析和预测性分析方面提供的支持较少。</p><p></p><p>通过将供应链智能体融入到丰智云塔产品当中，通过多个智能体的协作，针对履约、库存、销售等领域的问题，提供从指标查询与分析到异常识别与归因再到提供优化建议的完整的服务支持，从而为客户提高更敏捷与高效服务。而在这些服务的背后，智能体利用的是成熟、专业的预测、仿真、运筹优化等模型工具，来确保输出结果的准确与可靠。</p><p></p><p></p><h4>物流决策大模型实际效果</h4><p></p><p></p><p></p><h5>InfoQ：什么是物流决策大模型？他与语言大模型等有什么区别和联系？</h5><p></p><p></p><p>高磊： 我们知道语言大模型是一个通过 Transformer-Like 的架构，利用自回归的形式进行文字序列生成的模型，而很多人不知道的是物流中的很多问题，其实也可以认为是一个序列生成或者说是序列决策的问题，比如去 3 家门店 a、b、c 送货的一个路径规划问题，可以认为是一个决定先去哪，再去哪，最后去哪的序列生成问题。再比如装箱问题，10 个物品要装到箱子里，也可以认为是一个先装哪个物品，并以什么样的姿态装进去，再装哪个物品这样的问题。</p><p></p><p>所以，本质上，物流中的很多问题和语言生成的问题一样，都是序列生成的问题，因此均可以采用相同的技术架构来解决。这是他们相同的地方。</p><p></p><p>不同的地方显而易见，就是模态的不同，不同于语言模型生成的是文字，物流决策模型生成的就是决策本身。另外不相同的点是目标不同，语言模型的目标是生成文字的合理性与有效性，能够符合语言规律并有效解决用户的问题。物流决策模型除了生成决策要合理外，还有优化目标在里面，比如生成的线路成本越低越好。</p><p></p><p>丛技术角度来说，我们知道语言大模型本身基于两大关键技术，Transformer 和 RLHF，其中 Transformer 在很多算法场景下的成功应用已经充分证明了其能力的强大，而 RLHF 技术因其解决了人类价值观与偏好对齐等问题，将大模型的实用程度推上了前所未有的程度。在物流大决策模型中，我们也是基于这两大技术进行了构建，以路径规划场景为例，通过 Transformer 架构并结合顺丰海量的场景以及规划数据，构建了路径规划的基座模型，并通过 RLHF 技术来解决与业务偏好和具体业务场景对齐的问题。</p><p></p><p></p><h5>InfoQ：如何将物流决策大模型应用到供应链优化产品中，它能够带来一些什么样的优势? 具体落地效果如何？</h5><p></p><p></p><p>高磊： 总体来讲物流决策大模型带来两方面的显著优势，第一个是计算性能方面，传统的运筹模型主要基于搜索的机制，在一定引导下在一个巨大的解空间里面尽可能的搜索较好的解，当问题规模变大，解空间指数级别增长时，往往搜索到较高质量的解需要相对较长的时间，而物流决策模型基于序列生成的方式，在训练的较好的情况下，能够快速将较高质量的结果直接生成出来，再经过 GPU 高速并行计算的加持，能够很快的得到结果。</p><p></p><p>以我们实际鲁多的某客户装箱优化场景举例，目前我们可以平均 20ms 的时间内计算出一个使用传统运筹方法需要 10 分钟才能计算出来的订单，并且得到的解还能略微超过传统运筹方法。</p><p></p><p>另外一方面的优势来自于 RLHF 微调技术，通过 RLHF 我们可以让我们的模型有能力学习到业务在特定场景下的业务偏好与特殊需求。这将我们的产品在面对业务变化与新的算法场景时候可以从定制开发方式转向数据驱动的方式。</p><p></p><p>具体来说，在传统方式下，当业务变化或者新的场景出现时，我们需要我们的算法工程师不断的和业务沟通并理解业务，然后设计针对性的算法，并做很多 POC 试验，输出结果给到业务进行验证，往往这个过程会反复很多次并持续很久，因为往往业务无法将所有影响因素和潜在的业务规则一次性说清楚，很多时候碰到问题才解决问题。</p><p></p><p>使用 RLHF 微调技术，我们可以以数据驱动的方式解决很多问题，当输出结果不满足业务预期时，用户可以自己对结果进行调整，我们的产品会记录调整过程，逐渐积累业务偏好数据，并使用业务偏好数据不断进一步优化我们的模型，使输出的结果越来越符合业务实际需要。</p><p></p><p>当然这里面需要额外考虑的问题是并不是所有的业务调整或者业务偏好都是合理的，因此我们在产品里面设计了偏好与优化效果之间权衡机制，用户可以自己调整更偏向于“像人”还是优化。</p><p></p><p></p><h5>InfoQ：您认为大模型技术在未来供应链管理中的潜在应用有哪些预期或愿景？</h5><p></p><p></p><p>高磊： 以上三个工作是目前我们决策大模型技术在供应链管理中的应用方面进行的初步探索，我觉得还远远没有完全发挥出大模型技术的所有潜力，也还有很多潜在的应用场景没有被挖掘，我们希望能够和业界的生态合作伙伴与友商一起，持续深耕这样一个领域，为提升供应链的数智化水平、实现行业共同进步方面添砖加瓦。</p><p></p><p>嘉宾介绍：高磊， 顺丰科技人工智能总工程师，拥有 10 年 + 机器学习与运筹优化算法经验，研究方向为 NLP、运筹优化、强化学习等。2016 年加入顺丰，现任顺丰科技人工智能总工程师，曾主导顺丰集团内部多个数智化项目的研发与落地工作，涉及领域包括业务量预测、陆运干支线规划与调度、航空规划与调度、运力规划、场站选址、物资调拨等。目前主要负责集团智慧供应链体系建设相关工作。期间带领团队获得十余项发明专利，中物联物流技术创新奖、CCF BDCI 一等奖、最具商业价值奖，运筹帷幄年度行业实践奖与学术应用奖等荣誉。</p><p></p><p>活动推荐</p><p></p><p>AI 应用开发正在逐步成为各行业内的核心创新驱动力，CUI 式的对话助手、串联业务流程的 Agent 或是内嵌在原有业务逻辑中的 AI 模块，都在不断拓展面向用户的新应用场景。我们惊喜地看到从中小创业公司到大型企业，都在利用计算机视觉、自然语言处理、个性化推荐、对话式交互等 AI 能力提升业务效率、优化用户体验，显著增强了产品的市场竞争力。10 月 18-19 日，来 QCon 全球软件开发大会（上海站），了解更多成功应用 AI 技术的案例与最佳实践。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</id>
            <title>MiniMax 基于 Apache Doris 升级日志系统，PB 数据秒级查询响应技术实践</title>
            <link>https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/zJzgvWzv7N9cGHjBjJiS</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 09:44:37 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>作者｜MiniMax 基础架构研发工程师 Koyomi、香克斯、Tinker</blockquote><p></p><p></p><p></p><blockquote>导读：早期 MiniMax 基于 Grafana Loki 构建了日志系统，在资源消耗、写入性能及系统稳定性上都面临巨大的挑战。为此 MiniMax 开始寻找全新的日志系统方案，并基于 Apache Doris 升级了日志系统，新系统已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上，10 亿级日志数据的检索速度可实现秒级响应。</blockquote><p></p><p></p><p>MiniMax 是领先的通用人工智能科技公司，自主研发了不同模态的通用大模型，其中包括拥有万亿参数的 MoE 文本大模型、语音大模型以及图像大模型。MiniMax 以“与用户共创智能”为愿景，通过对大模型持续迭代，MiniMax 在国内率先完成核心 MoE 算法技术路线的突破。2024 年 4 月，公司推出国内首个上线商用的 MoE 架构、包含万亿参数的大语言模型——“MiniMax-abab 6.5”，模型性能接近国际领先水平。</p><p></p><p>随着模型复杂度以及模型调用量的不断提升，模型训练及推理产生的运行日志也在激增，这些数据对于 AI 应用的运行监控、优化及问题定位至关重要。早期 MiniMax 基于 Grafana Loki 构建了日志系统，在资源消耗、写入性能及系统稳定性上都面临巨大的挑战。为此 MiniMax 开始寻找全新的日志系统方案，并对业界具有代表性的技术栈 Apache Doris 和 Elasticsearch 进行了对比，Apache Doris 在性能、成本以及易用性等方面均优于 Elasticsearch，因此最终选择了 Apache Doris 来构建日志系统。</p><p></p><p>目前基于 Apache Doris 的新系统已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上，10 亿级日志数据的检索速度可实现秒级响应。</p><p></p><h2>问题及痛点</h2><p></p><p>MiniMax 早期日志系统架构基于 Loki 搭建，Loki 是由 Grafana Labs 团队开发的开源日志聚合系统，设计思想受 Prometheus 启发，不使用传统索引结构、仅对日志标签和元数据构建索引，核心模块包括 Loki、Promtail、Grafana 三个部分，其中 Loki 是主服务器、负责日志存储和查询，Promtail 是代理层、负责采集日志并发送给 Loki，而 Grafana 则用于 UI 展示。</p><p></p><p>在实际 Grafana Loki 使用中，每个集群中单独部署一套完整的日志采集器 + Loki 日志存储/查询服务。Loki 采用 Index + Chunk 的日志存储设计，写入时按日志标签的哈希值将不同日志流分散到各个 Ingester 上实现负载均衡，由 Ingester 负责将日志数据写入对象存储。查询时，Querier 从对象存储取出 Index 对应的 Chunk 后进行日志匹配。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c4fe6ea65ca8ef30e0f570533b6fc7e.png" /></p><p></p><p>尽管 Grafana Loki 定位为轻量级、水平可拓展和高可用的日志系统，但其在实际业务使用过程中仍存在一些问题：</p><p></p><p>查询资源消耗过大： Loki 未对日志内容创建索引，只能按照标签粒度对日志进行初步过滤。如果想要实现日志内容搜索功能，需使用 Query 对全量日志数据进行全文正则匹配， 而该操作会带来巨大的突发资源消耗，包括 CPU、内存、网络带宽。当查询的数据量和 QPS 越来越大时，Loki 的资源消耗及其稳定性问题也变得越来越不可忍受。Loki 架构复杂繁多： Loki 除了上图涉及模块之外，还有 Index Gateway、 Memcache、 Compactor 等模块，过多的架构组件给系统运维和管理带来很高的难度，配置起来也非常复杂。维护成本及难度高： MiniMax 部署集群数量较多，且每个集群的系统、资源、存储、网络等环境都有差异， 如果在每个集群中部署一套独立的 Loki 架构，维护成本及运维难度都非常高。</p><p></p><h2>为什么选择 Apache Doris</h2><p></p><p>根据 AI 场景的数据特点及业务需求，MiniMax 对新日志系统提出了以下要求：</p><p></p><p>日志数据规模庞大：由于 AI 业务场景具备链路长、上下文数据多、单次请求数据量大等特点，其产生的日志体量远远高于相同用户量级的其他互联网产品，这要求系统能够以较低的成本、稳定可靠的存储这些数据。查询性能要求高：业务对日志查询速度有较高的要求， 比如 1 亿条数据需要在秒级返回查询结果。分析灵活：要求系统能够支持日志指标查询、如某些关键词的统计曲线，同时能够提供日志告警服务。低成本：由于日志原始数据量达到 PB 级，而且还在不断增加，存储和计算的成本需要控制在合理范围内。</p><p></p><p>MiniMax 参考了当前业界成熟的日志系统架构解决方案，发现主流的日志系统一般包含以下几个关键组件：</p><p></p><p>采集端：负责从服务的标准输出采集日志，并将数据推送到中心消息队列。消息队列：负责解耦上下游、削峰填谷。在下游组件不可用时，仍然能保留一段时间的数据，保证系统稳定性。存储查询中间件：负责日志数据的存储和查询，在日志系统场景下，一般要求该中间件具备倒排索引能力，来支持高效的日志检索。</p><p></p><p>根据上述方案组成，MiniMax 决定在新日志系统中：采集端使用 iLogtail、消息队列使用 Kafka、存储中间件为 Apache Doris。在存储中间件的选择上，对比了业界具有代表性的 Apache Doris 和 Elasticsearch 这两个技术栈：</p><p></p><p><img src="https://static001.geekbang.org/infoq/dc/dc7c4c2f127bdfbf6ea247defc5a64fd.png" /></p><p></p><p>Apache Doris 在成本、写入性能、查询性能这几大维度均有较好的表现，尤其在存储效率、写入吞吐、聚合分析等方面有突出的优势，同时兼容 MySQL 的 SQL 语法也更加易用，因此最终选择 Apache Doris 作为存储中间件。</p><p></p><h2>Aapche Doris 日志系统升级实践</h2><p></p><p><img src="https://static001.geekbang.org/infoq/49/4940e10abd9a52783cc30df5d49c5c09.png" /></p><p></p><p>新日志系统（Mlogs）更加简洁，一套架构即可服务全部集群。上层为日志系统的控制面， 包括日志查询接口封装以及配置自动生产与下发模块。 下层是日志系统的数据面， 从左到右依次是日志采集端、消息队列、日志写入器、Doris 数据库。</p><p></p><p>集群服务产生的日志数据由 iLogtail 采集并推送到 Kafka，一部分会经由 Mlogs Ingester 从 Kafka 拉取并通过 Doris 的 Stream Load 写入到 Doris 集群中，另一部分则由 Doris 通过 Routine Load 直接实时订阅拉取Kafka 的消息流 。最后由 Doris 承担全量日志数据的存储与查询，无需每套集群单独部署。</p><p></p><p>在具体的应用落地方面：</p><p></p><p>在日志导入上： 新架构同时使用了 Doris Routine Load 和 Stream Load 方式。Routine Load 开箱即用，可直接处理不需要额外解析处理的 JSON 格式日志。而对于需要过滤与处理的复杂日志， MiniMax 在 Kafka 和 Doris 之间增加了日志写入器 Mlogs Ingester，由其解析和处理后，再通过 Stream Load 写入 Doris 中。在日志检索上： 主要使用了 Doris 倒排索引分词查询能力以及全文正则查询能力。倒排索引分词查询能力：分词查询性能较好， 场景覆盖度较广，主要采用倒排索引查询MATCH 和 MATCH_PHRASE。全文正则查询能力：正则查询精度更高，性能低于比分词查询， 适合小范围查询且对查询精度要求较高的场景，主要使用正则查询 REGEXP。在性能提升上：为进一步提升性能，实现了查询截断功能。当前日志数据按照时间顺序呈线性排列， 如果用户选择的查询范围过大， 会消耗较大的计算存储网络资源， 从而导致查询超时甚至系统不可用。 因此，对用户的查询进行了时间范围截断， 避免查询范围过大；并提前统计所有表的每 15 分钟的数据量， 动态地预估用户在不同表中最大可查询的时间长度。在成本控制上： 使用了 Doris 的冷热数据分层能力， 将 7 天内的数据定义为热数据，7 天之前的数据为冷数据。冷数据存储到对象存储， 以降低存储成本；同时对 30 天之前的对象存储数据进行归档， 仅在必要时恢复归档数据， 这也极大地降低了存量数据的存储成本。</p><p></p><h2>使用收益</h2><p></p><p>目前基于 Apache Doris 的新架构已接入 MiniMax 内部所有业务线日志数据，数据规模为 PB 级， 整体可用性达到 99.9% 以上， 同时也带来以下收益：</p><p></p><p>架构简化：新架构部署简单、一套架构即可服务全部集群，降低了整体系统维护及管理的复杂度，节省了大量的运维人力及成本投入。秒级查询响应： 基于 Apache Doris 的倒排索引能力及查询拦截功能，性能显著提升的同时系统也更加稳定。从 10 亿数据中查询单个关键字以及进行聚合分析，基本可以在 2s 内完成，对于日志数据的分析，大部分场景也可以做到秒级响应。写入性能高：当前系统规格可以实现 10 GB/s 级别的日志写入吞吐，能够在满足持续高吞吐写入的同时满足实时性要求，数据延迟控制在秒级。存储成本低： 数据压缩率较高达到 1:5 倍以上，因此存储空间占用较原本架构极大幅度降低。对于冷数据使用 Doris 冷热分层能力进一步降低数据的存储成本，存储成本节省超过 70%。</p><p></p><h2>未来规划</h2><p></p><p>未来 MiniMax 将持续迭代日志系统， 并重点从以下几方面发力：</p><p></p><p>丰富日志导入预处理能力：增加日志采样、结构化等预处理能力，进一步提升数据的可用性及存储性价比。增加 Tracing 能力：尝试将监控、告警、Tracing、日志等各方面的可观测性系统打通，以提供全方位的运维洞察。扩大 Doris 使用范围：除日志场景之外，Doris 逐步被引入数据分析和大数据处理场景下，助力后续构建数据湖仓能力。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</id>
            <title>“印度马斯克”要发印度第一款 AI 芯片，号称超越英伟达！CEO 要“狼性”，但把数十亿美元打水漂</title>
            <link>https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/THI3VfBS9MVCmuO75Y9Y</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 08:36:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AI市场迎来又一位新玩家。</p><p>&nbsp;</p><p>以电动踏板车而闻名的 Ola Electric 正在大胆进军人工智能硬件领域。该公司已宣布计划开发印度首款AI芯片系列，首批产品将于 2026 年通过其基础模型系列发布。</p><p></p><h2>印度发布首款AI自研芯片，号称性能媲美英伟达</h2><p></p><p>&nbsp;</p><p>作为印度最大的电动两轮车制造商之一，Ola Electric刚刚公布其计划推出的AI芯片。预计这三款芯片将于2026年投放市场，另外一款则将于2028年与广大用户见面。这些将成为印度推出的首批AI芯片，用以满足印度国内对于此类算力设备的旺盛需求。总部位于印度的数据中心与服务器公司Yotta已经为明年订购了1.6万张英伟达GPU，另有1.6万张上个月已经完成交付。</p><p>&nbsp;</p><p>Ola的首批三款芯片分别是Bodhi 1、Ojas和Sarv 1。第四款Bodhi 2则是首款印度AI芯片的继任者。Bodhi 1专为AI推理和微调而设计，主要应用于大语言模型和视觉模型，，将满足万亿参数 AI 模型的需求。Ola还强调称，Bodhi 1将超越目前最先进的技术，同时消耗更少的电量，这也是当今AI处理领域所面临的最大挑战之一。这一技术进展可能会加速印度各个行业对 AI 的采用。</p><p>&nbsp;</p><p>除此之外，还有专为特定应用而设计的Ojas Edge AI芯片。该公司可以根据多种应用场景对这款芯片做出定制，包括汽车、移动、物联网等。Ola还计划在其下一代电动汽车中部署这款芯片，以帮助运行充电、ADAS等系统。当然，随着AI计算的巨大需求，该公司还推出了Sarv 1，使用到为数据中心构建的Arm指令集。</p><p>&nbsp;</p><p>该公司在演示中披露，其原型芯片的性能与能效均比英伟达GPU更好。但目前尚不清楚他们具体是在与哪款GPU进行比较，例如RTX 4090还是H200，唯一明确的就是其运行功率为200瓦。此外，该公司也没有说明这些芯片将在哪里生产。</p><p>&nbsp;</p><p>Ola 的野心还不止于Bodhi 1。Ola表示还将随即推出 Bodhi 2 芯片，该芯片计划于 2028 年问世。这款更先进的芯片旨在支持具有超过 10 万亿个参数的模型的训练、推理和微调。该公司将其设想为百亿亿次超级计算的基石。</p><p>&nbsp;</p><p>除了专用于 AI 的芯片外，Ola 还在开发基于 ARM 架构的通用服务器 CPU 和用于智能手机和可穿戴设备等消费设备的富 AI 芯片。</p><p></p><h2>“印度马斯克”和他的“狼性”企业文化</h2><p></p><p>&nbsp;</p><p>据公开资料显示，Ola ElectricOla Electric 成立于 2017 年。创始人是 Bhavish Aggarwal，他之前曾是 Ola Cabs 的联合创始人，也是大语言模型AI公司 OlaKrutrim 的创始人。Aggarwal 毕业于孟买印度理工学院，他的职业生涯始于微软。</p><p>&nbsp;</p><p>Bhavish Aggarwal也常被人称为“印度马斯克”，也不仅是因为其在电动车领域的卓越成就，也因Bhavish Aggarwal的为人处事和管理公司的风格和马斯克很相似。</p><p>&nbsp;</p><p>过去十年，印度的一些分析师、评论家、媒体等用各种标签和形容词来描述这位 2010 年开启 Ola 创业之旅的创始人，有的标签是“傲慢”、有的标签是“咄咄逼人”，有人形容他是“工作狂”、“特立独行者”或者是“坚持不懈的创业者”。</p><p>&nbsp;</p><p>面对外界的种种声音，Bhavish Aggarwal称，“这就是梦想远大的代价，我说的都是真心话”。</p><p>&nbsp;</p><p>也如马斯克一样，Bhavish Aggarwal同样推崇“狼性文化”。</p><p>&nbsp;</p><p>Bhavish Aggarwal曾在接受采访时称，“Ola 并不适合每个人，Ola 是雄心勃勃的人的天堂”。Ola 是那些有理想、有抱负之人的最佳去处。在 Ola，他们感觉如鱼得水，因为我们公司充满了创业精神。我们不会过度管理员工。我们会告诉他们你必须做这项工作，请认真完成它”。</p><p>&nbsp;</p><p></p><blockquote>“一旦他们完成工作，他们自己就会超出预期。Ola 就是这样的地方，这样的人才是 Ola 真正闪耀的原因。但在这一过程中，有些人发现这里并不是最适合他们的地方。没关系，不是每个地方都适合每个人，我们的文化是影响力和目标驱动的，以业务建设为导向”。</blockquote><p></p><p>&nbsp;</p><p>Ola的企业文化也常被拿来撰写和讨论。Bhavish Aggarwal坦言，他们的文化就是这样。他们诚实对待自己的目标，不会向任何人隐瞒这一点。</p><p>&nbsp;</p><p>Bhavish Aggarwal表示他会公开告知员工“你别在这里奢望获得早九晚五的工作，如果你想图清闲，就别来Ola上班。如果你想创造终生难忘的故事，并告诉你的后辈你是如何为印度电气化和能源独立之旅的贡献了一份力量，那么 Ola 就是你该来的地方。我们都是诚实的人，有着诚实的目标。”</p><p></p><h2>主营业务还没盈利，Ola又盯上一块新“蛋糕”</h2><p></p><p>&nbsp;</p><p>在Bhavish Aggarwal的带领下，Ola的发展蒸蒸日上。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/93/934f10667ae5e7e3282c55486b15b21c.png" /></p><p></p><p>&nbsp;</p><p>Ola Cabs 首席执行官兼 Ola Electric 创始人 Bhavish Aggarwal 在首次公开募股 (IPO) 前的新闻发布会上发表讲话。</p><p>&nbsp;</p><p>2019 年 5 月 6 日，Ola Electric 宣布 Tata Sons 首席执行官、掌控96家公司的印度资本巨鳄拉坦塔塔已向该公司投资了一笔未公开的金额。此前，塔塔先生也投资了 Ola 的出租车业务。</p><p>&nbsp;</p><p>2020 年 5 月，Ola Electric 收购了位于阿姆斯特丹的电动踏板车制造商“Etergo”。Etergo 制造了一款使用可更换电池的踏板车，续航里程高于标准。这款踏板车的设计、技术和效率令人印象深刻，所以Ola Electric 也被称为“电动车界的特斯拉”。同年12 月，Ola Electric 公司与泰米尔纳德邦政府签署了一份谅解备忘录，宣布计划在泰米尔纳德邦建立全球最大的两轮车工厂（名为 Ola Futurefactory），耗资 240 亿卢比（约合20.46亿人民币）。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/f5/f50e50865e9fed15f079ebdd0c3abfff.png" /></p><p></p><p>&nbsp;</p><p>Ola Futurefactory 占地 500 英亩，是世界上最大的两轮车工厂。</p><p>&nbsp;</p><p>2022 年 3 月，Ola 对以色列电池技术公司“StoreDot”进行了战略投资。Ola Electric 将采用 StoreDot 的快速充电电池技术，并将其用于印度未来的汽车上。StoreDot 是超快速充电 (XFC) 电池的先驱，它克服了主流电动汽车采用的两个关键障碍：续航里程焦虑和充电时间长。</p><p>&nbsp;</p><p>收购完成并充分处理文书工作后，Ola Electric 宣布要进军电动汽车领域，不仅要在国内推出电动汽车，也要让其走向国际。随后2022年6月，Ola Electric 发布了其首款电动汽车。Ola 表示，其目标是一次充满电后行驶超过 500 公里，仅需 4 秒即可完成 0-100 次加速。</p><p>&nbsp;</p><p>虽然Ola Electric的发展一路高歌猛进，但由于处于迅速增长阶段，Ola Electric 的现金消耗异常巨大，亏损不断增加。电动滑板车销售收入是 Ola Electric 的唯一收入来源，而电池销售收入在本财年第一季度仅贡献了一小部分。</p><p>&nbsp;</p><p>在运营方面，Ola Electric也存在其他问题。Ola Electric公司表示，电动汽车所用零部件可能会出现缺陷、质量问题或供应中断或价格上涨，从而增加材料成本和 Ola 电动汽车的价格，这将影响预计的制造和交付时间表，也会从一定程度上削减公司营收。</p><p>&nbsp;</p><p>而此次Ola Electric宣布推出自研AI芯片后，也在印度社交网络上引发热议。看好和看衰两种声音势均力敌。</p><p>&nbsp;</p><p>一些网友认为Ola Electric成立多年并未取得什么重大成果，发布芯片也只为了融资和炒作。</p><p>&nbsp;</p><p>ID名为SaiSS961的用户在Youtube平台Ola Electric 发布芯片视频下方评论称：“这家公司得了数十亿美元的资金，却没有取得什么成果，一切都只是炒作而已。”</p><p>&nbsp;</p><p></p><blockquote>“Ola 只是购买外部技术并将其标榜为自己的技术。就像他迄今为止所做的一切一样，这只是复制粘贴。”</blockquote><p></p><p>&nbsp;</p><p>ID名为arpanshome6328的用户表示：“看起来他很快就厌倦了现有的业务。他没有从任何一项业务中赚取任何利润，就跳入了下一项业务。他们不知道自己想要实现什么。希望投资者的钱是安全的。”</p><p>&nbsp;</p><p>另一位ID为ankittiwari6716的用户也认为Ola Electric 应该专注于主营业务，而不是乱花钱在其他事情上。</p><p>&nbsp;</p><p></p><blockquote>“Ola Electric 最好专注于电动汽车，而不是在有限的预算下做所有事情。这家伙在欺骗无辜的印度人民来抬高他的股票。他的公司从欧洲获得了电动汽车设计，进行了外观改造并将其出售给印度人。他怎么能押注高端芯片制造业务？这是在欺骗所有人。只要和你认识的任何一位前 Ola 员工谈谈就会知道，几乎没人使用 krutrim AI ，甚至班加罗尔的人们更喜欢 Rapido 而不是 Ola。”</blockquote><p></p><p>&nbsp;</p><p></p><h2>印度正大力推动芯片行业发展</h2><p></p><p>&nbsp;</p><p>市场研究与咨询公司MarketsandMarkets 近期对 AI 芯片市场进行了全面分析，预测 2024 年至 2030 年间将出现大幅增长。根据他们的报告，全球 AI 芯片市场预计到 2030 年将达到 930 亿美元，复合年增长率为 25.6%。这一增长是由汽车、医疗保健和金融等各个领域越来越多地采用 AI 技术推动的。</p><p>&nbsp;</p><p>这块巨大的蛋糕早已被印度盯上。作为印度向全球领先经济体转型的一部分，印度总理莫迪曾设定了一个目标，即到 2029 年，印度将从几乎一无所有的基础发展成为全球五大计算机芯片制造商之一。</p><p>&nbsp;</p><p>伊利诺伊大学香槟分校的拉凯什·库马尔 (Rakesh Kumar) 表示，各国寻求半导体自给自足的主要驱动力有两个。第一个是，在疫情最严重时期，芯片短缺引发了人们的担忧，人们已经意识到了芯片现在对一个国家的安全和工业的重要性；第二个是，各国希望在这个庞大且不断增长的行业中分一杯羹，去年全球芯片行业市场总价值为5269 亿美元。</p><p>&nbsp;</p><p>作为世界上人口最多的国家，印度拥有大量科技人才，善加利用无疑能够推动其AI技术的发展。而由于英伟达和阿斯麦尔等多家企业被禁止向中国出售其尖端技术，这些厂商也许很乐意为自家产品在印度开辟新的市场空间。</p><p>&nbsp;</p><p>这一举措对印度的技术格局而言是重大的一步，有望减少对外国芯片制造商的依赖，并促进国内人工智能硬件的创新。如果成功，Ola 的芯片系列将使印度成为全球人工智能硬件市场的关键参与者。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>&nbsp;</p><p><a href="https://theaiinsider.tech/2024/08/16/ola-unveils-ambitious-plan-for-indias-first-ai-chip-family/">https://theaiinsider.tech/2024/08/16/ola-unveils-ambitious-plan-for-indias-first-ai-chip-family/</a>"</p><p><a href="https://iotworldmagazine.com/2024/08/15/2360/a-review-of-top-ai-chips-market-share-report-2024-2030-in-the-uk-europe-asia-and-india">https://iotworldmagazine.com/2024/08/15/2360/a-review-of-top-ai-chips-market-share-report-2024-2030-in-the-uk-europe-asia-and-india</a>"</p><p><a href="https://www.cnbc.com/2024/08/09/ola-electric-shares-rise-20percent-in-india-ipo-valuing-firm-at-4point8-billion.html">https://www.cnbc.com/2024/08/09/ola-electric-shares-rise-20percent-in-india-ipo-valuing-firm-at-4point8-billion.html</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</id>
            <title>探索安全边界：出海合规与大模型实践 | QCon</title>
            <link>https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/O49yVYCK9ufG2d4q4QhP</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 03:10:51 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>进入 2024 年以来，企业在全球商业舞台上迎来了两个显著的潮流：业务出海和大模型技术的广泛应用。业务出海已成为企业扩大市场版图、提升全球竞争力的关键战略。企业不再局限于本土市场，而是积极向国际领域迈进，探索新的增长机遇和市场空间。这一过程中，如何确保合规与安全，成为企业出海战略中不可忽视的一环。同时，大模型技术的运用正深刻影响着企业的运营模式和创新方向，安全人员也开始考虑利用大模型技术优化安全解决方案，提升安全防护效率，减少安全风险。</p><p></p><p>面对不断演进的趋势，这些都是重点关注和丞待解决的挑战。在即将于 10 月 18 -19 日召开的 QCon 上海站，我们策划了【<a href="https://qcon.infoq.cn/2024/shanghai/track/1717">探索安全边界：出海合规与大模型实践</a>"】专场，将邀请不同公司的数据安全合规专家，分享他们在各自的业务场景中的出海合规实践经验，以及借助大模型助力应用场景落地，实现运营效率和效果的双重提升的方法。目前是 <a href="https://qcon.infoq.cn/2024/shanghai/apply">8 折购票</a>"最后优惠期，感兴趣的同学前往了解。</p><p></p><h3>精彩演讲抢先看</h3><p></p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6026">演讲主题：大模型在商业敏感数据中的分类分级实践</a>"</p><p></p><p>讲师：刘明 （携程信息安全部数据安全总监）</p><p></p><p>演讲摘要</p><p></p><p>分类分级是数据安全的一项基础性工作，高效准确地对数据进行分类分级打标，是后续进行有效分级保护控制的先决条件，以往分类分级工作中存在专家特征规则识别质量局限，人工识别耗时费力跟不上业务变化的问题，随着大语言模型的兴起，我们看到了新的机会，也取得了不错的可行性验证成果，本次演讲将分享携程逐步实现高质量、高效率的敏感数据分类分级的思路和经验，希望能给听众带来一些启发和思考。</p><p></p><p>演讲提纲</p><p></p><p>1. 背景</p><p></p><p>目前数据分类分级手段在商密数据场景的局限大语言模型能带来的潜在提升与场景适配研究</p><p></p><p>2. 分类分级可行性验证过程</p><p></p><p>找到正确的提问方式大语言模型分类分级过程中遇到的问题：模型幻觉、数据安全性</p><p></p><p>3. 在控制成本的基础上找到平衡应用落地方式</p><p></p><p>在现有分类分级产品中引入大语言模型能力分类分级准确率和召回率数据情况</p><p></p><p>4. 挑战与展望</p><p></p><p>分类分级大模型的常态运营</p><p></p><p>实践痛点</p><p></p><p>大语言模型存在幻觉问题，偶发性会出现脱离框架的答案，同时大语言模型使用成本目前相较专家特征规则和自建模型没有优势，因此成本控制问题会限制其应用规模。</p><p></p><p>演讲亮点</p><p></p><p>商业秘密数据特征模糊，使用专家特征规则无法进行有效识别，而大语言模型可充分利用上下文和常识知识更加准确的识别分类商业秘密数据。</p><p></p><p>听众收益</p><p></p><p>了解携程在使用大语言模型进行商密数据分类分级的实践经验了解大语言模型在数据安全分类分级基础性工作上提升效率质量的价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6111">演讲主题：百度基于大模型安全运营的质效提升实践</a>"</p><p></p><p>讲师：包沉浮（百度杰出架构师，安全技术委员会主席 ）</p><p></p><p>演讲摘要</p><p></p><p>百度作为一家业务复杂的大型互联网企业，同时又是关键基础设施，随着网络安全威胁的日益加剧，传统的安全运营手段在效率和效果上都面临巨大挑战。本次分享将介绍百度如何基于大模型构建深度安全推理智能体框架，实现运营效率和效果的双重提升，并展示包括告警自动研判和漏洞事件分析在内的实践经验，希望能给听众带来一些大模型安全领域应用最佳实践的启示。</p><p></p><p>演讲提纲</p><p></p><p>1. 背景和挑战</p><p></p><p>大模型开始逐步应用于安全运营场景百度安全运营面临的双效（效率 + 效果）提升需求</p><p></p><p>2. 架构设计</p><p></p><p>‍设计目标：基于深度安全推理智能体框架，实现双效提升设计考虑：人机协同的工作流设计（运营流程梳理、质量标准定义、人机交互模式）、模型能力边界与拓展（模型结果可信度和可解释性、知识和工具依赖）、实施成本整体‍架构（自底向上）：底座模型的知识补充RAG、CoT、Function calling流程编排智能体 Review 机制</p><p></p><p>3. 实践案例</p><p></p><p>告警自动 / 辅助研判 + 事件处置漏洞事件自动分析 + 处置</p><p></p><p>4. 未来展望</p><p></p><p>大模型原生的安全运营中心</p><p></p><p>实践痛点</p><p></p><p>明确目标，围绕安全运营场景的风险偏好，制定更贴合实际的落地目标，避免直接盲目追求大而全的零职守无人干预以数据驱动能力迭代，缺少可用数据时应当从实际场景中提升标准化和自动化水平，引入业务的数据活水，避免直接使用脱离业务的合成数据</p><p></p><p>演讲亮点</p><p></p><p>从架构设计层面剖析安全运营场景双效提升应遵循的必要准则，提供构建深度安全推理智能体框架的完整视角细粒度展现告警研判、漏洞分析处置等实际场景的双效提升最佳实践</p><p></p><p>听众收益</p><p></p><p>了解互联网大厂的安全运营需求痛点与大模型实践经验了解规模化且对效果要求较高的安全运营场景下，大模型智能体设计考虑与整体架构</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6064">演讲主题：安全大模型的最后一公里实践：智能决策与自动响应</a>"</p><p></p><p>讲师：傅奎（雾帜智能联合创始人 &amp; CTO）</p><p></p><p>演讲摘要</p><p></p><p>主流安全大模型及应用场景侧重于非结构化数据的整理、总结、分析和建议，但还缺少最后一步——如何让大模型参与安全响应的决策，并在决策后自动化完成动作的执行。本议题将介绍，安全专家如何借助大模型，自动生成网络安全响应流程（安全剧本），并自动完成剧本的执行，由此在安全运营场景最后一公里完成大模型应用场景落地。</p><p></p><p>演讲提纲</p><p></p><p>1. 大模型在网络安全领域应用</p><p></p><p>发布 SecGPT 的安全厂商大模型在安全领域的应用场景共性不足（重分析，轻决策）</p><p></p><p>2. 安全大模型在智能决策领域应用探索</p><p></p><p>模型是否有能力给出合理建议如何让模型给出更高质量的决策模型决策结果的潜在风险</p><p></p><p>3. 安全大模型实战应用实践案例</p><p></p><p>OWASP TOP 10 典型场景大模型在 Web 攻击攻击领域的应用效果降低模型决策风险的实践思路</p><p></p><p>4. 未来展望</p><p></p><p>让模型设计剧本 VS 让模型选择剧本大模型落地安全最后一公里（能力调度）如何实现终极目标：零值守无人安全运营中心</p><p></p><p>实践痛点</p><p></p><p>针对特定性的安全事件，如何设计响应策略人工智能设计的安全策略是否可以实现全自动执行距离真正零值守还有哪些问题没有解决</p><p></p><p>演讲亮点</p><p></p><p>不仅仅使用大模型对安全事件做分析，还通过安全大模型对安全事件响应作出决策，安全大模型完全决策，并最终付诸实施通过安全能力实现安全策略的落地，该环节减少对人工的依赖，减少对安全专家的依赖，是未来零值守安全运营中心的重要基础。</p><p></p><p>听众收益</p><p></p><p>传统安全运营的场景，痛点和困境有别于安全大厂的安全运营智能化实践安全大模型最后一公里所解决的问题和价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6044">演讲主题：全球视野下的合规之道：携程海外数据安全管理实践</a>"</p><p></p><p>讲师：胡立平（携程数据安全合规负责人）</p><p></p><p>演讲摘要</p><p></p><p>出海成为众多国内企业实现业绩新增长曲线的选择，然而随着数据的重要性提升，法律及监管关注度也在增强，携程作为在线旅行行业较早布局海外业务的企业，在海外数据安全合规风险上也有所积累。本次演讲将分享携程海外数据安全合规风险管理的思路和经验，希望能给相关出海企业企业带来一些合规实践上的启示。</p><p></p><p>演讲提纲</p><p></p><p>出海面临的数据安全合规挑战</p><p></p><p>法律法规近些年主要变化及监管挑战从数据视角深度剖析出海合规风险携程应对策略及实践携程的海外合规整体策略设计如何通过 GRC 平台形成风险管理闭环如何保障旗下 Trip.com 产品的隐私合规</p><p></p><p>海外数据安全合规未来展望和应对思考</p><p></p><p>实践痛点</p><p></p><p>合规风险管理线上化需要建立在标准化的风险管理、优秀的产品设计、合理的内部运营流程等基础上，才能实现控制域的完备性、控制方法的准确性、关键控制的有效性、审计覆盖的充分性等关键指标。</p><p></p><p>演讲亮点</p><p></p><p>结合合规实战介绍部分法域的合规挑战介绍携程自研 GRC(Governance, Risk and Compliance ) 平台如何融合监管情报、外规内化、审计整改等多个治理环节，解决出海过程中面临多法域、多品牌的风险管理挑战</p><p></p><p>听众收益</p><p></p><p>帮助了解现有海外数据安全相关合规的整体风险态势帮助了解标准化及线上化在多法域数据安全合规风险管理中的价值</p><p></p><p><a href="https://qcon.infoq.cn/2024/shanghai/presentation/6094">演讲主题：跨国经营中的企业数据合规之道</a>"</p><p></p><p>讲师：陈晓芳（ vivo 数据合规专家）</p><p></p><p>演讲摘要</p><p></p><p>各大企业，尤其是跨国企业，由于业务拓展或人力管理等因素，不可避免地会涉及数据出境。与此同时，数据出境相关监管规范和管理机制日益完善，对企业管控数据出境行为提出了新的挑战。本次演讲将分享 vivo 在数据出境管理方面的实践经验，希望能给相关出海企业带来一些合规启示。</p><p></p><p>演讲提纲</p><p></p><p>监管框架：解析数据出境合规路径</p><p></p><p>数据出境的立法背景和监管趋势数据出境的三种合规路径：安全评估、标准合同备案（SCC）、个人信息保护认证探讨网信办规定的数据出境豁免条件及其适用性</p><p></p><p>2. 企业视角：企业如何管控数据跨境</p><p></p><p>介绍企业如何建立数据出境监控和排查机制阐述企业在数据出境过程中的合规流程和全周期管理措施</p><p></p><p>3. 实战案例：vivo 的数据出境安全评估</p><p></p><p>分享 vivo 申报数据出境安全评估的经验，着重于省网信办材料审核的重点，打回材料的理由等真实案例</p><p></p><p>4. 数据出境合规未来的挑战与展望</p><p></p><p>分析数据出境合规管理中的主要困难，提供相应应对策略和建议</p><p></p><p>实践痛点</p><p></p><p>系统的数据出境风险管理体系，需要完备全面的数据出境监测体系、精确的风险触发机制和合理的出境备案应对方案，才能实现企业数据出境的全面管控。</p><p></p><p>演讲亮点</p><p></p><p>vivo 在应对数据出境合规挑战过程中的管控措施，以及申报安全评估时的经验。</p><p></p><p>听众收益</p><p></p><p>帮助了解数据出境方向整体的合规风险及应对措施了解 vivo 在数据出境管控方面的措施及安全评估申报过程中的相关经验</p><p></p><p>更多精彩内容将在 10 月 18 - 19 日 QCon 上海站为您现场呈现，期待与您共赴这场技术之约。如果您有好的技术实践案例想要与我们分享，欢迎点击<a href="https://jsj.top/f/EbrZFg">链接</a>"提交演讲申请。</p><p></p><p>【会议推荐】</p><p></p><p>InfoQ 将于 10 月 18-19 日在上海举办 <a href="https://qcon.infoq.cn/2024/shanghai/">QCon 全球软件开发大会</a>" ，覆盖前后端 / 算法工程师、技术管理者、创业者、投资人等泛开发者群体，内容涵盖当下热点（AI Agent、AI Infra、RAG 等）和传统经典（架构、稳定性、云原生等），侧重实操性和可借鉴性。现在大会已开始正式报名，可以享受 8 折优惠，单张门票立省 960 元（原价 4800 元），详情可联系票务经理 &nbsp;17310043226 咨询。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/df/dfd31ee989a7951439a77fec138d4cf8.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</id>
            <title>不要掉入“AI 工程就是一切”的陷阱</title>
            <link>https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/kLvNa2lBEouaaN0LlgyU</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:37:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>常有人错误地将这样一句话归因于一些领导者，尽管它可能完全是虚构的：“外行谈论战略和战术，内行关注运营。”从战术的角度看，我们面对的是一系列独特的问题，从运营角度，我们看到的是需要解决的组织功能失调模式。在战略视角看到的是机会，在运营视角看到的是需要应对的挑战。</p><p></p><p>在本系列文章的第一部分，我们介绍了 <a href="https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&amp;mid=2247619519&amp;idx=2&amp;sn=a468ebed03640e82c4a4d3e14d8434c6&amp;scene=21#wechat_redirect">LLM 的战术性操作</a>"。接下来，我们将拓宽视野，深入探讨长期的战略规划。在这一部分，我们将讨论构建 LLM 应用程序的运营层面，这些应用程序是战略与战术的桥梁，将理论与实际应用紧密结合。</p><p></p><p>在运营 LLM 应用程序过程中，我们遇到了一些似曾相识的问题，这些问题在传统软件系统的运营中也常常出现，不同的是它们也带来了一些新的挑战，使得探索过程充满了趣味。此外，运营 LLM 应用程序还带来了一些全新的问题。我们将这些问题及其答案归纳为四个部分：数据、模型、产品和人。</p><p></p><p>对于数据，我们将探讨这几个问题：如何以及多久需要重新审视一次 LLM 的输入和输出？如何测量并有效减少测试环境与生产环境之间的偏差？</p><p></p><p>对于模型，我们将探讨这几个问题：如何将语言模型集成到现有的技术栈中？如何看待模型的版本控制以及如何在不同模型和版本之间进行平滑迁移？</p><p></p><p>对于产品，我们将探讨这几个问题：设计应该在何时介入应用程序的开发过程，为什么要“尽早介入”？如何设计能够充分吸纳人类反馈的用户体验？在面对相互冲突的需求时如何安排优先级？如何校准产品风险？</p><p></p><p>最后，对于人，我们将探讨这几个问题：选择哪些人才来构建成功的 LLM 应用程序，以及何时招募他们？如何培养正确的实验性文化？如何利用现有的 LLM 应用程序来辅助开发自己的 LLM 解决方案？哪一个更关键：流程还是工具？</p><p></p><p></p><h3>运营：LLM 应用程序的构建和开发团队</h3><p></p><p></p><p></p><h4>数据</h4><p></p><p></p><p>正如精选的食材能够成就一道佳肴，高质量的输入数据同样对机器学习系统的表现起着决定性作用。此外，系统的输出是评估其是否正常工作的唯一方式。所有人都紧密关注数据，他们每周都会花几个小时细致地分析输入和输出，以便更好地理解数据分布：模式、边缘情况以及模型的局限性。</p><p></p><p></p><h4>检查开发与生产偏差</h4><p></p><p></p><p>在传统机器学习流程中存在的一个普遍问题是训练与服务之间的偏差。这种情况通常发生在模型训练时使用的数据与模型在实际应用中遇到的数据不一致时。尽管我们可以无需训练或微调就能够使用 LLM，从而避免了训练集的问题，但开发与生产环境之间的数据偏差问题依然存在。关键在于，在开发阶段测试系统时所用的数据应与系统在生产环境中实际面对的数据相一致。如果不是这样的话，我们可能会发现生产环境中的模型准确性会受影响。</p><p></p><p>LLM 开发与生产偏差可以分为两种类型：结构性偏差和基于内容的偏差。结构性偏差包括格式不一致，比如 JSON 字典与 JSON 列表之间的差异、不一致的大小写以及错误，如错别字或不完整的句子片段。这些错误可能导致模型性能不可预测，因为不同的 LLM 是基于特定的数据格式训练的，而提示词对微小变化都非常敏感。基于内容的偏差（或“语义”偏差）指的是数据的含义或上下文的差异。</p><p></p><p>正如传统的机器学习一样，对 LLM 的输入和输出进行定期的偏差检测是非常有必要的。输入和输出的长度或特定格式要求（例如，JSON 或 XML）等指标是跟踪变化最直接的方式。对于更“高级”的漂移检测，可以采用更高级的方法，如聚类输入 / 输出对的嵌入向量可用于检测语义漂移：如果用户讨论的主题发生变化，这可能表明他们正在探索模型以前没有接触过的领域。</p><p></p><p>在测试变更时，例如提示词工程，确保保留数据集是最新的，并且能够反映用户交互的最新类型。例如，如果错别字在生产环境的输入中很常见，那么它们也应该出现在保留数据中。除了进行数值偏差检查之外，对输出进行定性评估也很有用的。定期检查模型输出——俗称“氛围检查”——可以确保结果符合预期并满足用户需求。最后，将非确定性纳入偏差检查中——通过多次运行测试数据集中的每个输入并分析所有输出，可以增加捕捉那些可能仅偶尔发生异常情况的可能性。</p><p></p><p></p><h4>每天检查 LLM 的输入和输出样本</h4><p></p><p></p><p>LLM 是动态且持续进化的。尽管它们具有令人印象深刻的零样本学习能力，并且经常能够生成令人满意的输出，但它们的失败模式却非常难以预测。对于自定义任务，定期审查数据样本有助于培养对 LLM 性能的直观理解。</p><p></p><p>生产环境的输入输出对是 LLM 应用程序的“现场证据”，它们不会被替换。最近的研究表明，开发者对什么构成“好”和“坏”输出的看法会随着他们与更多数据的交互而发生变化（即所谓的标准漂移）。虽然开发者可以预先设定一些标准来评估 LLM 输出，但这些预定义的标准通常不够全面。例如，在开发过程中，我们可能会更新提示词，以增加获得良好响应的概率，并降低获得不良响应的概率。这种评估、重新评估和标准更新的迭代过程是必不可少的，因为在没有直接观察输出的情况下，很难预测 LLM 的行为或人类的偏好。</p><p></p><p>为了有效地管理大型语言模型，我们需要记录 LLM 的输入和输出。通过每天检查这些日志样本，我们能够及时识别并适应新的模式或故障模式。在发现新问题时，我们可以立即编写断言或制定评估策略来应对这些问题。同样，对故障模式定义的更新都应实时反映在评估标准中。这些“氛围检查”可以帮助我们捕捉到不良输出的信号，而通过编写代码和断言，我们能够将这些检查操作化，使之成为可执行的过程。最后，这种态度需要在团队中得到普及，例如通过在值班轮换中加入对输入和输出的审查或注释环节。</p><p></p><p></p><h3>调用模型</h3><p></p><p></p><p>在使用 LLM API 时，我们确实可以依靠少数几家技术供应商的智能成果。虽然这为我们提供了便利，但同时也带来了一些权衡，包括性能、延迟、吞吐量和成本等方面。此外，随着更新、更好的模型（在过去一年中几乎每个月都会有新模型发布）的发布，我们需要随时准备好更新我们的产品，以弃用旧模型并迁移到新模型。在这一章节，我们将分享在使用这些我们不能完全控制的技术时的经验，特别是关于如何管理那些我们无法自托管的模型。</p><p></p><p></p><h4>生成结构化输出，简化下游集成</h4><p></p><p></p><p>对于大多数现实世界的场景，LLM 的输出需要通过机器可读的格式提供给下游应用程序。例如，Rechat，一个房地产 CRM 系统，需要结构化的响应来在前端显示小部件。同样，Boba，一个用于生成产品策略想法的工具，需要输出包含标题、摘要、可信度得分和时间范围字段的结构化信息。LinkedIn 通过限制 LLM 生成 YAML 格式的数据，用于决定使用哪种”技能“，并提供调用这些技能所需的参数。</p><p></p><p>这种应用模式体现了 Postel 定律的极致：在接收时宽容（接受任意自然语言），在发送时保守（输出类型化、机器可读的对象）。因此，我们期望这种方法具有很高的稳定性和可靠性。</p><p></p><p>目前，Instructor 和 Outlines 是从 LLM 中提取结构化输出的实际标准。如果你在使用 LLM API（比如 Anthropic 或 OpenAI），请优先选择 Instructor；而如果你在使用自托管的模型（例如 Hugging Face），则推荐使用 Outlines。</p><p></p><p>为不同模型修改提示词是一种痛苦</p><p></p><p>有时，我们精心编写的提示词在一种模型上表现出色，但在另一种模型上却表现平平。这种情况可能在我们更换不同模型供应商时发生，也可能出现在同一模型的不同版本升级过程中。</p><p></p><p>例如，Voiceflow 在从 gpt-3.5-turbo-0301 迁移到 gpt-3.5-turbo-1106 时，他们的意图分类任务性能下降了 10%。（幸运的是，他们进行了评估！）同样，GoDaddy 注意到了一个积极的变化，升级到 1106 版本缩小了 gpt-3.5-turbo 和 gpt-4 之间的性能差距。（或者，如果你是一个乐观的人，可能会对 gpt-4 的领先优势在这次升级中有所减少感到失望。）</p><p></p><p>因此，如果我们不得不在模型之间迁移提示词，预计这将是一个比简单更换 API 端点更耗时的过程。不要想当然地认为使用相同的提示词能够得到相似或更好的结果。此外，拥有一个可靠的自动化评估系统，可以在迁移前后有效地衡量任务性能，并显著减少所需的手动验证工作。</p><p></p><p></p><h4>版本控制和固定你的模型</h4><p></p><p></p><p>在机器学习管道中，“改变一点，影响全局”是一个普遍现象。这一点在我们依赖自己未参与训练的组件，例如大型语言模型（LLM）时，显得尤为突出，因为这些模型可能会在不被我们察觉的情况下发生变化。</p><p></p><p>幸运的是，许多模型供应商提供“锁定”特定模型版本（例如，gpt-4-turbo-1106）的选项。这样，我们可以使用特定版本的模型权重，确保它们保持不变。在生产环境中锁定模型版本有助于防止模型行为发生意外变化，从而减少因模型更新可能导致的问题（例如过于冗长的输出或其他不可预见的故障模式）。</p><p></p><p>此外，可以考虑维护一个影子管道，这个管道镜像了生成环境的设置，但使用的是最新的模型版本。这为实验和测试新版本提供了一个安全的环境。一旦确认这些新模型的输出在稳定性和质量上符合标准，就可以自信地升级生产环境中的模型版本。</p><p></p><p>选择能够完成任务的最小模型</p><p></p><p>在开发新应用程序时，使用最强大的模型往往具有极大的吸引力。然而，一旦我们确认了技术可行性，就很有必要尝试一下使用更小的模型是否能够产生同样优质的结果。</p><p></p><p>小模型的优势是较低的延迟和成本。虽然在性能上可能略显逊色，但通过诸如思维链、n-shot 提示词和上下文学习等先进技术的应用，它们完全有可能超越自身的限制。除了调用 LLM API，针对特定任务进行微调也能够显著提升性能。</p><p></p><p>综合考虑，一个精心设计的工作流，即使使用较小的模型，通常也能匹敌甚至超越单个大型模型的输出质量，同时还具备更快的处理速度和更低的成本。例如，这个推文分享了 Haiku 结合 10-shot 提示词的表现优于零样本的 Opus 和 GPT-4。从长远来看，我们期望看到更多流程工程的案例，使用较小的模型实现输出质量、响应时间和成本之间的最佳平衡。</p><p></p><p>作为另一个典型案例，我们来看一下那些看似简单的分类任务。轻量级的 DistilBERT（6700 万参数）模型居然是一个出人意料的强大基线。在开源数据上进行微调后，拥有 4 亿参数的 DistilBART 更是一个不错的选择——它在识别幻觉方面的 ROC-AUC 值达到了 0.84，在延迟和成本方面增加不到 5%，超越了大多数大型语言模型。</p><p></p><p>重点是，我们不要轻视那些模较小的模型。尽管人们往往倾向于对各种问题都应用庞大的模型，但通过一些创新思维和实验探索，我们常常能够发现更为高效的解决方案。</p><p></p><p></p><h3>产品</h3><p></p><p></p><p>虽然新技术为我们带来了新的可能性，但构建卓越产品的核心原则始终不变。因此，即使是在第一次面临新挑战时，我们也无需在产品设计方面重新发明轮子。将我们的 LLM 应用程序开发建立在坚实的产品理念之上，这将使我们能够为用户带来真正的价值。。</p><p></p><p></p><h4>及早并频繁地进行设计</h4><p></p><p></p><p>设计师的参与有助于推动你深入思考如何构建和向用户展示产品。我们有时会将设计师简单定义为美化事物的人。然而，除了用户界面之外，他们还会全面思考如何改进用户体验，甚至是打破现有的规则和范式。</p><p></p><p>设计师擅长将用户需求转化为各种各样的形式。这些形式有些更容易实现，而有些则为 AI 技术提供了更多或更少的施展空间。与许多其他产品一样，构建 AI 产品应该以要完成的任务为中心，而不是驱动这些任务的技术。</p><p></p><p>问问自己：“用户期望这个产品为他们完成哪些任务？这些任务是聊天机器人擅长的吗？能够使用自动完成功能？也许可以尝试一些不同的方案！”审视现有的设计模式，思考它们与要完成的任务之间的联系。这些是设计师为团队能力带来的宝贵贡献。</p><p></p><p></p><h4>以 HITL 为导向设计用户体验</h4><p></p><p></p><p>一种提升注释质量的方式是将 Human-in-the-Loop（HITL）融入到用户体验（UX）设计中。通过让用户轻松地提供反馈和更正，我们不仅能即时优化输出，还能收集有洞察力的数据来改进我们的模型。</p><p></p><p>设想一个电子商务平台，用户需要上传并分类他们的商品。我们可以从多个角度来设计用户体验：</p><p></p><p>用户手动选择产品类别；LLM 定期检查新产品并在后端更正分类错误。用户不选择产品类别；LLM 定期在后端对产品进行分类（可能存在错误）。LLM 提供实时产品类别建议，用户可以根据自己的判断进行验证和更新。</p><p></p><p>虽然这三种方法都利用了 LLM，但它们提供了非常不同的 UX。第一种方法将初始责任放在用户身上，并将 LLM 作为后续的辅助。第二种方法减少了用户的负担，但不提供透明度或控制权。第三种方法找到了二者之间的平衡点。LLM 提前建议类别，减少了用户的认知负担，他们无需深入了解复杂的分类体系。同时，用户可以审查和修改这些建议，他们对如何分类产品有最终的决定权，将控制权牢牢掌握在手中。作为一个额外的好处，第三种方法为模型改进创建了一个自然反馈循环。好的建议会被接受（正反馈标签），不好的建议会被更新（负反馈标签转成正反馈标签）。</p><p></p><p>这种建议、用户验证和数据收集的模式在多个应用领域中都得到了广泛应用：</p><p></p><p>编码助手：用户可以接受建议（强烈正反馈）、接受并调整建议（正反馈）或忽略建议（负反馈）。Midjourney：用户可以选择放大并下载图像（强烈正反馈）、修改图像（正反馈）或生成一组新图像（负反馈）。聊天机器人：用户可以对响应点赞（正反馈）或不点赞（负反馈），如果响应真的很差，选择重新生成响应（强烈负反馈）。</p><p></p><p>反馈可以是显式或隐式的。显式反馈是用户对产品提出的意见或评价，隐式反馈是我们需要从用户交互中捕捉的信息，无需用户有意提供。编码助手和 Midjourney 是隐式反馈的例子，而点赞和不点赞是显式反馈。如果我们能够像编码助手和 Midjourney 那样设计 UX，就可以收集到大量的隐式反馈来改进我们的产品和模型。</p><p></p><p></p><h4>调整需求层次的优先级</h4><p></p><p></p><p>在准备将演示转化为实际应用时，我们需要仔细考虑以下几个关键要素：</p><p></p><p>可靠性：确保 99.9% 的正常运行时间，同时遵循结构化输出标准；无害性：避免生成攻击性、NSFW 或其他有害的内容；事实一致性：忠实于提供的上下文，不虚构信息；实用性：与用户的需求和请求相关；可扩展性：延迟 SLA，支持高吞吐量；成本效益：需要考虑预算限制；其他：安全性、隐私保护、公平性、GDPR 合规性、DMA 合规性等。</p><p></p><p>如果我们试图同时解决所有这些要求，我们将永远无法完成产品交付。因此，我们必须进行优先级排序，并且要果断。这意味着我们要清楚哪些是没有商量余地的（例如，可靠性、无害性），没有这些我们的产品就是不可行的。关键在于识别出最基本的产品功能。我们必须接受第一个版本不会完美的事实，并通过不断迭代来改进。</p><p></p><p></p><h4>根据用例校准风险承受能力</h4><p></p><p></p><p>在选择语言模型及其审查标准时，我们需要根据应用场景和目标受众来做出判断。对于那些提供医疗或财务咨询的聊天机器人，我们必须设定极高的安全和准确性标准。因为任何错误或不当的输出都可能造成严重的后果，并且会严重损害用户对我们的信任。然而，对于不那么关键的应用，比如推荐系统，或者那些仅供内部使用的应用程序，如内容分类或摘要，过分严格的要求可能会拖慢开发进度，却不会为提升价值带来太大帮助。</p><p></p><p>这与最近发布的 a16z 报告中的观点相吻合，许多公司在内部 LLM 应用方面比外部应用进展得更快。通过在内部生产力工具中引入 AI，组织可以在更加受控的环境中实现价值，同时学习如何有效地管理风险。然后，随着他们信心的增强，可以逐步扩展到面向客户的应用场景。</p><p></p><p></p><h3>团队与角色</h3><p></p><p></p><p>定义工作职能不是件容易的事，而在这个新兴领域编写工作描述比其他领域更具挑战性。我们决定不再使用交叉工作职能的文氏图或工作描述的建议。相反，我们将引入一个新的职位——AI 工程师——并探讨其在组织中的位置。同时，我们也将讨论团队其他成员的角色以及如何合理分配责任，这至关重要。</p><p></p><p></p><h4>专注于流程，而不是工具</h4><p></p><p></p><p>面对新兴的范式，例如大型语言模型，软件工程师们往往更倾向于采用各种工具。这种偏好有时会导致我们忽视了这些工具本应解决的问题和优化的流程。结果，许多工程师不得不应对由此产生的偶然的复杂性，对团队的长期生产力构成了负面影响。</p><p></p><p>例如，这篇文章讨论了某些工具如何为大型语言模型自动生成提示词。文章认为（在我看来是正确的），那些在没有先理解问题解决方法或流程的情况下使用这些工具的工程师最终会累积不必要的技术债务。</p><p></p><p>除了偶然的复杂性，许多工具还常常存在规格不足的问题。以不断壮大的 LLM 评估工具行业为例，它们提供所谓的“即插即用”的 LLM 评估服务，涵盖毒性、简洁性、语调等通用评估指标。我们发现许多团队在没有深入分析其领域特有的失败模式的情况下，就盲目采纳了这些工具。与此形成鲜明对比的是 EvalGen，它通过深度参与用户的每一个环节——从定义标准到标注数据，再到评估检查——引导用户构建适合特定领域的评估体系。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/73/73d82d6652c837c82745f6d8a7e174e5.png" /></p><p></p><p>Shankar, S. 等人（2024）“谁来验证验证器？将 LLM 辅助评估 LLM 输出与人类偏好对齐”。来源：<a href="https://arxiv.org/abs/2404.12272">https://arxiv.org/abs/2404.12272</a>"</p><p></p><p>EvalGen 引导用户通过遵循最佳实践来制定 LLM 评估标准，即：</p><p></p><p>定义特定领域的测试（通过提示词自动引导）。它们可以是带有代码的断言，或者是采用“LLM 即评委”的形式。强调将测试与人类判断对齐的重要性，使用户能够验证测试是否确实捕捉到了既定的标准。随着系统（如提示词内容等）的变化不断迭代和优化测试标准。</p><p></p><p>EvalGen 为开发人员提供了评估构建过程的框架性理解，而不是将他们限制在特定工具的使用上。我们发现，一旦 AI 工程师获得了这种宏观视角，他们往往会选择采用更简洁的工具，或者根据自己的需求自行开发解决方案。</p><p></p><p>LLM 的组成部分远不止提示词编写和评估，其复杂性无法在此一一列举。关键在于 AI 工程师在采用工具之前要深入理解其背后的流程和原理。</p><p></p><p></p><h4>持续地实验</h4><p></p><p></p><p>机器学习产品与实验密切相关。不仅涉及 A/B 测试、随机对照试验，还包括频繁尝试修改系统的最小组件并进行离线评估。人们热衷于评估的真正原因并非仅仅为了可靠性和信心——而是为了让实验成为可能。你的评估越精确，就能越迅速地进行实验，进而更快地发现系统的最佳配置。</p><p></p><p>尝试采用不同的方法解决同一个问题是一种很常见的做法，因为现在的实验成本很低。收集数据和训练模型的高昂成本已经得到有效控制——提示词工程的成本仅略高于人力投入。确保你的团队成员都掌握了提示词工程的基础知识。这不仅能激发他们进行实验的热情，还能促进组织内部不同观点的交流与碰撞。</p><p></p><p>此外，实验不仅仅是为了探索，而是要学会利用它们。如果你手头有一个新的任务，可以考虑让团队的其他成员从不同的视角来处理它。尝试寻找更高效的方法，探索如思维链或 few-shot 提示词等技术，以提高工作质量。不要让工具限制了你的实验；如果是这样，那就重新构建它们，或者购买新的工具。</p><p></p><p>最后，在产品或项目规划阶段，务必留出足够的时间来构建评估机制并进行多项实验。在考虑工程产品的规格时，为评估过程设定明确的标准。在制定路线图时，不要低估了实验所需的时间。要预见到在生产交付之前，可能需要进行多轮的开发和评估迭代。</p><p></p><p></p><h4>让每个人都能使用新的 AI 技术</h4><p></p><p></p><p>随着生成式 AI 采用率的增加，我们希望整个团队——不仅仅是专家——都能理解并自信地使用这项新技术。没有比亲自实践更好的方式去培养对大型语言模型工作原理的直观理解了，比如它们的响应延迟、故障模式和用户体验。LLM 相对容易使用：你无需编码技能就可以为流程管道提升性能，每个人都可以通过提示词工程和评估做出实质性的贡献。</p><p></p><p>教育是关键环节，可以从提示词工程的基础开始，如利用 n-shot 和思维链等技术，引导模型生成期望的输出。拥有这方面知识的人还可以教授更技术性的内容，例如大型语言模型本质上是自回归的。换句话说，虽然输入可以并行处理，但输出是顺序的。因此，生成延迟更多地取决于输出的长度而非输入的长度——这是在设计用户体验和设定性能预期时需要考虑的一个关键因素。</p><p></p><p>我们还可以提供更多实践和探索的机会，比如举办一次黑客马拉松。虽然让整个团队投入数日时间在探索性项目上看起来成本较高，但最终的成果可能会超出你的预期。我们见证了一个团队通过黑客马拉松，在短短一年内就实现了他们原本计划三年完成的路线图。另一个团队则通过黑客马拉松，引领了一场用户体验的范式转变，这种转变现在因为大型语言模型的加入而成为可能。</p><p></p><p></p><h4>不要掉入“AI 工程就是一切”的陷阱</h4><p></p><p></p><p>随着新职位名称的出现，人们往往容易过分夸大这些角色的能力。这通常会导致在实际工作职责变得逐渐明确时，人们不得不去做一些痛苦的调整。新入行的人和负责招聘的经理可能会夸大声明或抱有不切实际的期望。在过去的十年里，这类显著的例子包括：</p><p></p><p>数据科学家：“在统计学方面比任何软件工程师都强，在软件工程方面比任何统计学家都强的人”机器学习工程师（MLE）：以软件工程为中心的机器学习视角</p><p></p><p>最初，许多人认为数据科学家单枪匹马就能驾驭数据驱动的项目。然而，现实情况已经清晰地表明，为了有效地开发和部署数据产品，数据科学家必须与软件工程师和数据工程师紧密合作。</p><p></p><p>这种误解在 AI 工程师这一新兴角色上再次出现，一些团队误以为 AI 工程师就是他们需要的一切。实际上，构建机器学习或 AI 产品需要一个由多种专业角色 组成的团队。我们与十多家公司就 AI 产品进行了深入咨询，发现他们普遍都陷入了认为“AI 工程就是一切”的陷阱。这种认知导致产品往往难以越过演示阶段，因为公司忽视了构建产品所涉及的关键方面。</p><p></p><p>例如，评估和度量对于将产品从单一的领域检查阶段扩展到广泛应用阶段来说至关重要。有效的评估能力与机器学习工程师通常所具备的优势相辅相成——一个完全由 AI 工程师组成的团队可能缺乏这些技能。Hamel Husain 在他最近的研究中强调了这些技能的重要性，包括监测数据漂移和制定针对特定领域的评估标准。</p><p></p><p>以下是在构建 AI 产品的过程中你需要的不同类型角色，以及他们在项目各个阶段大致的参与时机：</p><p></p><p>首先，专注于构建产品。这个阶段可能涉及 AI 工程师，但并非必须。AI 工程师在快速原型设计和迭代产品方面具有显著的价值（用户体验、数据处理管道等）。随后，通过系统化地收集和分析数据，为产品打下坚实的基础。根据数据的性质和体量，你可能需要平台工程师或数据工程师。你还需要建立查询和分析数据的系统，以便快速定位问题。最后，你将致力于优化 AI 系统。这并不一定涉及训练模型，包括设计评估指标、构建评估系统、执行实验、优化 RAG 检索、调试随机性问题等。机器学习工程师非常擅长这些工作（尽管 AI 工程师也可以通过学习掌握这些技能）。但如果你没有完成前面的基础步骤，招聘机器学习工程师可能并不明智。</p><p></p><p>除此之外，你始终需要一个领域专家。在小型企业，这通常是创始团队的成员；而在大型企业，产品经理也可以担任这一角色。角色的介入时机至关重要。在不恰当的时间（例如，过早让机器学习工程师介入）招聘人员或介入顺序不对，不仅浪费时间和金钱，还会导致频繁的人员更替。此外，在前面两个阶段定期与机器学习工程师沟通（但不全职让他们介入）将有助于公司为未来的成功打下坚实的基础。</p><p></p><p>原文链接：</p><p>https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/</p><p></p><p>声明：本文由 InfoQ 翻译，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/46068541f35de5382cda6ed9f</id>
            <title>「模型量化技术」可视化指南：A Visual Guide to Quantization</title>
            <link>https://www.infoq.cn/article/46068541f35de5382cda6ed9f</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/46068541f35de5382cda6ed9f</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:37:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><blockquote>编者按：随着大语言模型（LLMs）规模的不断扩大，如何在有限的计算资源下高效部署这些模型成为了一个迫切需要解决的问题。模型量化作为一种有效的模型压缩技术，在保持模型性能的同时大大降低了计算和存储开销，因此广受关注。但对于许多人来说，模型量化的具体原理和实现方法仍然是一个“黑盒”。我们今天为大家带来的这篇文章，通过可视化图示详细解析各种模型量化技术的原理和实现方法，为各位读者提供一个全面且直观的模型量化技术指南。本文旨在帮助各位读者涉猎以下技能领域：理解模型量化技术的基本原理和作用掌握多种模型量化方法及其优缺点学会如何选择合适的量化方法，并根据实际场景进行调整我们分享这篇全面且深入的技术解析，期望各位读者不仅能够理解模型量化的基本原理，还能洞察该领域的最新发展趋势。随着模型量化技术的不断进步，我们有理由相信，未来将会出现更加高效、更轻量级的大语言模型，为 AI 技术的更广泛应用铺平道路。</blockquote><p></p><p></p><p>作者 🕶 | Maarten Grootendorst</p><p></p><p>编译 🐣 | 岳扬</p><p></p><h1>目录🧾</h1><p></p><p>01 第 1 部分：LLMs 存在的“问题”</p><p></p><p>1.1 参数数值（value）的表示方法</p><p></p><p>1.2 内存限制问题</p><p></p><p>02 第 2 部分：模型量化技术简介</p><p></p><p>2.1 常用的数据类型</p><p></p><p>2.1.1 FP16</p><p></p><p>2.1.2 BF16</p><p></p><p>2.1.3 INT8</p><p></p><p>2.2 对称量化 Symmetric Quantization</p><p></p><p>2.3 非对称量化 asymmetric quantization</p><p></p><p>2.4 取值范围的映射与裁剪</p><p></p><p>2.5 校准过程 Calibration</p><p></p><p>2.5.1 权重（和偏置项） Weights (and Biases)</p><p></p><p>2.5.2 激活值</p><p></p><p>03 第 3 部分：Post-Training Quantization</p><p></p><p>3.1 动态量化（Dynamic Quantization）</p><p></p><p>3.2 静态量化（Static Quantization）</p><p></p><p>3.3 探索 4-bit 量化的极限</p><p></p><p>3.3.1 GPTQ</p><p></p><p>3.3.2 GGUF</p><p></p><p>04 第 4 部分：Quantization Aware Training</p><p></p><p>4.1 1-bit LLM 的时代：BitNet</p><p></p><p>4.2 权重的量化 Weight Quantization</p><p></p><p>4.3 激活值的量化 Activation Quantization</p><p></p><p>4.4 反量化过程 Dequantization</p><p></p><p>4.5 所有 LLMs 实际上均为 1.58-bit</p><p></p><p>4.5.1 The Power of 0</p><p></p><p>4.5.2 Quantization 量化过程</p><p></p><p>05 Conclusion</p><p></p><p>Resources</p><p></p><p>文中链接🔗</p><p></p><p>顾名思义，大语言模型（Large Language Models，LLMs）的特点就是庞大，以至于普通的消费级硬件都难以承载。这些模型的参数量级可达数十亿，而且在进行推理时，往往需要依赖拥有大量显存（VRAM）的 GPU 来加快推理速度。</p><p></p><p>鉴于此，越来越多的研究者将目光投向如何通过优化训练方法、使用适配器（adapters）等技术来缩小模型体积。在这一领域，模型量化（quantization）技术成为了一个重要的研究方向。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5b/5bec304c38c40cbad59e1bde74b40389.png" /></p><p></p><p>本篇文章将带领大家深入了解语言模型领域的量化技术，并逐一探讨相关概念，帮助大家建立起对这一领域的直观认识。我们将一起探索不同的量化方法、实际应用场景，以及模型量化技术的基本原理。</p><p></p><p>本文将提供许多图表（visualizations）来帮助各位读者更好地理解和掌握模型量化技术这一概念，希望大家能够直观、深入地理解模型量化技术。</p><p></p><h1>01 第 1 部分：LLMs 存在的“问题”</h1><p></p><p>大语言模型之所以被称为“大”，是因为其参数数量十分之庞大。目前，这类模型的参数数量通常能够达到数十亿之巨（主要是指权重参数（weights）），这样的数据量其存储成本无疑是一笔巨大的开销。</p><p></p><p>在模型的推理过程中，激活值（译者注：activations，神经网络中某个层对输入数据应用激活函数后产生的输出值。）是通过输入数据（input）与模型权重（weights）相乘等一系列步骤来生成的，这些激活值的数据量也可能非常庞大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/87/874c90d9ab6031252d37d67a6c96a99c.png" /></p><p></p><p>因此，我们的目标是找到一种尽可能高效的方式来表达数十亿个参数，以减少存储每个参数所需的空间。</p><p></p><p>在开始对这些参数进行优化之前，我们从最基本的部分入手，先探讨一下参数数值（value）在计算机中最初是如何表示的。</p><p></p><h2>1.1 参数数值（value）的表示方法</h2><p></p><p>在计算机科学中，特定的数值（value）通常都以浮点数的形式来表示，即带有正负号和小数点的数字。</p><p></p><p>这些数值是由 “bits” 组成的，也就是由二进制数字表示。根据 IEEE-754 标准[1]，这些 “bits” 可以用来表示三个不同的部分，从而构成一个完整的数值（value）：符号位、指数部分以及小数部分（也称为尾数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ea/eac7a3488b10f273b59c0f8582d0d2fb.png" /></p><p></p><p>这三个部分结合起来，就能根据一组特定的 “bit” 值来计算出一个具体的数值（value）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/8910b5c2c918c30a60a5e3f168f529b5.png" /></p><p></p><p>一般来说，用来表示数值（value）的 “bit” 越多，得到的数值（value）精确度就越高：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0a78c3a6c4125a0718f7ec579381c17d.png" /></p><p></p><h2>1.2 内存限制问题</h2><p></p><p>可用的 “bits” 数量越多，所能表示的数值范围就越大。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b6/b691578d8233df9a0b1240945498e402.png" /></p><p></p><p>一个特定的数值表示法能够表示的所有数值的区间被称为动态范围（dynamic range） ，而相邻两个数值之间的间隔则被称为精度（precision） 。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a2/a2afc167e089cb51af4550eddaf9148e.png" /></p><p></p><p>使用这些 “bits” 的一个有趣功能是，我们可以计算出存储一个特定数值（value）所需的设备内存量。由于一个字节（byte）占 8 位（bits），我们可以为大多数浮点表示形式（floating point representation）制定一个基本的计算公式。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bd/bd3bcd4f7e5d455d67c3008bc8bbf8c4.png" /></p><p></p><p></p><blockquote>Note：在实际应用中，模型推理阶段所需的显存（VRAM）量还受到诸多因素的影响，比如模型处理上下文的大小和模型架构设计。</blockquote><p></p><p></p><p>假设我们有一个拥有 700 亿参数的模型。通常情况下，这些模型默认使用 32 位浮点数（常称为全精度）进行表示，仅加载模型就需要 280GB 内存。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5f/5feb84da28fdf626a4d1142fda57782f.png" /></p><p></p><p>因此，尽可能地减少用于表示模型参数的 “bits” 数量（包括模型训练过程中也是如此）是非常有必要的。但是，有一点必须注意，精度的降低往往会导致模型准确性下降。</p><p></p><p>我们的目标是减少用于表示模型参数的 “bits” 数量，同时又不损害模型的准确性…… 这就是模型量化技术的作用所在！</p><p></p><h1>02 第 2 部分：模型量化技术简介</h1><p></p><p>模型量化的核心在于将模型参数的精度从较高的位宽（bit-widths）（例如 32 位浮点数）降低到较低的位宽（bit-widths）（例如 8 位整数）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e2942d6873011ce1e76a1d9312e11c5.png" /></p><p></p><p>在减少参数的 “bits” 数量时，通常会出现一定的精度损失（即丢失一些数值细节）。</p><p></p><p>为了更直观地说明这种影响，我们可以尝试将任意一张图片仅用 8 种颜色来表示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3f/3f32efe21ee0931ff0c1f3bd816cc874.png" /></p><p></p><p>该图像基于 Slava Sidorov 的原作[2]进行了修改</p><p></p><p>观察放大区域，我们可以发现它比原始图片看起来更加“粗糙”，因为使用的颜色种类减少了。</p><p></p><p>模型量化的主要目的就是减少表示原始参数所需的 “bits” 数量（在上述案例中即为颜色种类），同时尽可能保留原始参数的精度。</p><p></p><h2>2.1 常用的数据类型</h2><p></p><p>首先，我们来看看一些常见的数据类型，以及它们与 32-bit（全精度（full-precision）或 FP32 ）表示法相比的影响。</p><p></p><h3>2.1.1 FP16</h3><p></p><p>以从 32-bit 转换到 16-bit（半精度或 FP16 ）的浮点数为例：</p><p></p><p><img src="https://static001.geekbang.org/infoq/fd/fd8835bd8d53bdf7e7391e7bf931225e.png" /></p><p></p><p>可以看到，FP16 的数值范围比 FP32 要窄得多。</p><p></p><h3>2.1.2 BF16</h3><p></p><p>为了保持与原始 FP32 相似的数值范围，引入了 bfloat 16 这一数据类型，它类似于“截断版的FP32”：</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/469cd166116fdca24bde632ede084922.png" /></p><p></p><p>BF16 虽然使用的 “bits” 数量与 FP16 相同，但能表示的数值范围更广，因此在深度学习领域内得到了广泛应用。</p><p></p><h3>2.1.3 INT8</h3><p></p><p>当我们需要再进一步减少 “bits” 的数量时，就到了整数表示法施展身手的领域，而不再是浮点数表示法。例如，从 FP32 转换为仅有 8 bits 的 INT8，其占用的 bits 数量仅仅是原来的四分之一：</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c126c6b65515e139ec623c19b568e08c.png" /></p><p></p><p>有些硬件优化了整数运算，因此在这些硬件上整数运算可能会更高效。然而，并不是所有硬件都进行了这样的优化。不过，一般来说，使用较少的 “bits” 数量，计算速度通常会更快一些。</p><p></p><p>每减少一个 bits ，就需要进行一次映射（mapping）操作，将原本的 FP32 表示形式“压缩”到更少的 “bits” 数量。</p><p></p><p>在实际应用中，我们并不需要将 FP32 所表示的全部数值范围 [-3.4e38, 3.4e38] 都映射到 INT8。我们只需找到一种方法，将数据（即模型参数）范围映射到 INT8 即可。</p><p></p><p>常用的压缩（squeezing）和映射（mapping）方法包括对称量化（symmetric quantization）和非对称量化（asymmetric quantization），它们都是线性映射（linear mapping）的不同形式。</p><p></p><p>接下来，我们将探讨一下这些将 FP32 量化为 INT8 的方法。</p><p></p><h2>2.2 对称量化 Symmetric Quantization</h2><p></p><p>在对称量化过程中，原本浮点数的值域会被映射到量化空间（quantized space）中一个以零为中心的对称区间。从前面的例子可以看出，量化前后的值域都是围绕零点对称的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/cd/cdd17a97e7873a0cf8e16883d9552697.png" /></p><p></p><p>这就意味着，在浮点数中表示零的值，在量化空间中仍然是正好为零。</p><p></p><p>对称量化（symmetric quantization）有一种经典方法是绝对最大值（absmax，absolute maximum）量化。</p><p></p><p>具体操作时，我们会从一组数值中找出最大的绝对值（α），以此作为线性映射的范围（译者注：从 -α 到 +α）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2d/2db8527e594edc4119a16580463ac83a.png" /></p><p></p><p></p><blockquote>Note：值域 [-127, 127] 代表的是受限制🚫的范围，而 8-bit 整数可以表示的完整范围是[-128, 127]，选择哪种范围取决于所采用的量化方法。</blockquote><p></p><p></p><p>由于这是一种以零为中心的线性映射（linear mapping），所以计算公式相对简单。</p><p></p><p>我们首先根据以下公式计算比例因子（s）：</p><p></p><p>b 是我们想要量化到的字节数（译者注：原文为“Byte”，此处保留原义，译为字节数，译者认为可能为 bits 数量）（这里是 8 ），α 是最大绝对值，</p><p></p><p>接着，我们用这个比例因子 s 来量化输入值 x：</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/240961e13b3e5728e990d753f5c9d7cb.png" /></p><p></p><p>将这些数值代入公式后，我们将得到以下结果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7a7c60a541213997ad4a360470f488aa.png" /></p><p></p><p>为了恢复原始的 FP32 值，我们可以使用之前计算出的比例因子（s）来对量化后的数值进行反量化（dequantize）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d3/d3d07f698f22b5a221408135478f9017.png" /></p><p></p><p>先量化后再反量化以恢复原始值的过程如下所示：</p><p></p><p><img src="https://static001.geekbang.org/infoq/34/34815a8bd34226bf3f7e97c56c1c7667.png" /></p><p></p><p>我们可以观察到，某些值（如 3.08 和 3.02 ）在量化到 INT8 后，都被分配了相同的值 36。当这些值反量化（dequantize）回 FP32 时，会丢失一些精度，变得无法再区分。</p><p></p><p>这种现象通常被称为量化误差（quantization error） ，我们可以通过比较原始值（original values）和反量化值（dequantized values）之间的差值来计算这个误差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/82/82677db0532256d3058fb219f8be2d6b.png" /></p><p></p><p>一般来说，“bits” 的数量越少，量化误差往往越大。</p><p></p><h2>2.3 非对称量化 asymmetric quantization</h2><p></p><p>与对称量化（symmetric around）不同，非对称量化并不是以零为中心对称的。 它将浮点数范围中的最小值（β）和最大值（α）映射到量化范围（quantized range）的最小值和最大值。</p><p></p><p>我们在此要探讨的方法称为零点量化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/46/464b86629735821898e9f1be4cc30d5c.png" /></p><p></p><p>各位注意到 0 的位置是如何移动的吗？这正是它被称为“非对称量化”的原因。在区间 [-7.59, 10.8] 中，最小值和最大值与零点之间的距离是不相等的。</p><p></p><p>由于零点位置的偏移，我们需要计算 INT8 范围的零点来进行线性映射（linear mapping）。与之前一样，我们还需要计算一个比例因子（s），但这次要使用 INT8 范围（ [-128, 127] ）的两个端点之间的差值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6e743c31b1a53698b7ad26cd8782a1a.png" /></p><p></p><p>请注意，由于需要计算 INT8 取值范围中的零点（z）来调整权重，这个过程稍微复杂一些。</p><p></p><p>和之前一样填入公式：</p><p></p><p><img src="https://static001.geekbang.org/infoq/92/92e3f6c54fc905c152d03c71ee1af1c7.png" /></p><p></p><p>要将从 INT8 量化后的数值反量化回 FP32 ，需要使用之前计算的比例因子（s）和零点（z）。</p><p></p><p>除此之外，反量化过程则相对比较直接：</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95802965a73d91442d99da714f149ba9.png" /></p><p></p><p>当我们将对称量化和非对称量化放在一起对比时，我们可以迅速看出这两种方法之间的差异：</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8eae78513534b90caa4d92b0c5080af4.png" /></p><p></p><p></p><blockquote>Note：请注意对称量化（symmetric quantization）以零点为中心的特性，以及非对称量化（asymmetric quantization）存在的零点偏移。</blockquote><p></p><p></p><h2>2.4 取值范围的映射与剪裁</h2><p></p><p>在前文所举的例子中，我们研究了如何将向量中的数值映射到更低的位表示形式（lower-bit representation）中。虽然这样使得向量的全范围都能被映射，但有一个明显的缺点，那就是有离群值（outlier）时不太好处理。</p><p></p><p>假设有一个向量，其值如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2ff9cbba82e0666a5db066e5ce6af23f.png" /></p><p></p><p>请注意，如果其中一个数值（value）远大于其他所有数值，该数值就可以被视作离群值（outlier）。 如果我们要映射这个向量的全部数值，那么所有较小的数值都将映射到相同的较低位表示，并因此失去它们的独特特性：</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0857ab4220800c53bd863aaf7f59b1c1.png" /></p><p></p><p>这就是我们之前使用的 absmax 方法。请注意，如果我们不进行剪裁（clipping），非对称量化也会出现这样的问题。</p><p></p><p>另一种选择是裁剪（clip）掉某些数值。裁剪（Clipping）操作会为原始值设定一个不同的动态范围，这样所有离群值都会被映射到相同的值。</p><p></p><p>在下文给出的案例中，如果我们手动将动态范围设置为 [-5, 5] ，所有超出这个范围的数值无论其原始值是多少，都将被映射为 -127 或 127 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/52bf417d28b12186a1fbab60e4c38ecb.png" /></p><p></p><p>这种方法的主要优点是，显著减少了非离群值的量化误差。然而，离群值的量化误差却增加了。</p><p></p><h2>2.5 校准过程 Calibration</h2><p></p><p>在前文的示例中，我展示了一种简单方法 —— 即任意选择一个取值范围 [-5, 5]。这个过程被称为校准（calibration），其目的是找到一个能够包含尽可能多数值（values）的范围，同时尽量减少量化误差（quantization error）。</p><p></p><p>对于不同类型的参数，执行校准步骤的方法并不相同。</p><p></p><h3>2.5.1 权重（和偏置项） Weights (and Biases)</h3><p></p><p>在 LLMs 中，我们可以将权重（weights）和偏置项（Biases）视为预先确定的静态值，因为这些值在运行模型之前就已经确定了。例如，Llama 3 的约 20 GB 文件[3]中大部分都是其权重和偏置项。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c1/c1578dc2cc2c80e84de0f893123c1c05.png" /></p><p></p><p>由于偏置项的数量（以百万计）远少于权重（以数十亿计），偏置项通常被保留在更高的精度（如 INT16 ），而量化的主要工作则集中在权重的处理上。</p><p></p><p>因为权重是静态且已知的，所以对其的量化技术可以有：</p><p></p><p>手动选择输入范围的百分位数优化原始权重和量化权重之间的均方误差（MSE）最小化原始值和量化值之间的熵（KL 散度）</p><p></p><p><img src="https://static001.geekbang.org/infoq/79/79f2fb01e27903e852897f738bd17d70.png" /></p><p></p><p>例如，第一种方法（手动选择输入范围的百分位数）会导致出现与前文我们看到的相似的裁剪（clipping）行为。</p><p></p><h3>2.5.2 激活值</h3><p></p><p>在 LLMs 中， 那些在整个推理过程中持续更新的输入（input）通常被称为“激活值”（activations）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1f/1fb614e36b50e5312f25dbe2e3ba5573.png" /></p><p></p><p>请注意，这些值之所以被称为激活值，是因为它们经常需要经过某些激活函数处理，比如 sigmoid 或 relu。</p><p></p><p>与权重不同，激活值会随着每次输入数据的改变而变化，因此很难对其进行精确量化。</p><p></p><p>由于这些值在每个隐藏层之后都会更新，因此我们只能在输入数据通过模型时才能预测它们在推理过程中的具体数值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/24fa106202ae3f693b209d3d55e4e4a6.png" /></p><p></p><p>一般来说，校准权重和激活值的量化方法主要有两种：</p><p></p><p>Post-Training Quantization（PTQ）  — 训练完成后进行量化Quantization Aware Training（QAT）  — 训练/微调过程中同时进行量化</p><p></p><h1>03 第 3 部分：Post-Training Quantization</h1><p></p><p>在众多量化技术中，post-training quantization（PTQ）是最为流行的一种。这种方法是在训练完模型之后对模型的参数（包括权重和激活值）进行量化。</p><p></p><p>对于权重值的量化可以采用对称量化（symmetric quantization） 或非对称量化（asymmetric quantization） 两种方式。</p><p></p><p>至于激活值，由于我们不知道其范围，因此需要通过模型的推理来获取它们的 potential distribution（译者注：指的是在不同的输入数据和模型参数下，激活值可能出现的一系列数值。了解这个分布有助于我们选择一个能够包含大部分激活值范围的量化级别，从而减少量化误差。），然后再进行量化。</p><p></p><p>激活值的量化主要有两种形式：</p><p></p><p>动态量化（Dynamic Quantization）静态量化（Static Quantization）</p><p></p><h2>3.1 动态量化（Dynamic Quantization）</h2><p></p><p>当数据通过隐藏层时，其激活值会被收集起来：</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c275b1ab05e53f145786aefaf994e04.png" /></p><p></p><p>随后，利用这些激活值的分布（distribution of activations）来计算量化输出值所需的零点（z）和比例因子（s）值：</p><p></p><p><img src="https://static001.geekbang.org/infoq/0e/0ee7b2af280ecf602e42f70d2201f749.png" /></p><p></p><p>每次数据通过一个新模型层时，都要重复上述过程。因此，每个模型层都有其独特的 z 值和 s 值，因此也有不同的量化方案。</p><p></p><h2>3.2 静态量化（Static Quantization）</h2><p></p><p>与动态量化不同，静态量化在模型推理过程中不实时计算零点（z）和比例因子（s），而是在模型训练或校准过程中提前计算。</p><p></p><p>为了找到这些值，会使用一个校准数据集，并让模型处理这些数据，以便收集可能的激活值分布（potential distributions）。</p><p></p><p><img src="https://static001.geekbang.org/infoq/52/522fd381b0daca69636a4bb0af5a2468.png" /></p><p></p><p>收集到这些数值后，我们就可以计算出必要的 s 值和 z 值，以便在推理过程中进行量化。</p><p></p><p>在实际推理过程中，s 值和 z 值不需要重新计算，而是被应用于所有激活值，实现全局量化。</p><p></p><p>通常情况下，动态量化技术可能会稍微更精确一些，因为它为每个隐藏层计算一次 s 值和 z 值。不过，由于需要计算这些值，因此可能会增加计算时间。</p><p></p><p>相比之下，静态量化虽然准确度稍低，但由于事先已知用于量化的 s 值和 z 值，因此在推理时更为高效。</p><p></p><h2>3.3 探索 4-bit 量化的极限</h2><p></p><p>将量化位数降至 8-bit 以下是一项艰巨的任务，因为每减少一个 bit，量化误差（quantization error）就会增加。 幸运的是，有几种巧妙的方法可以将量化位数进一步降低到 6-bit、4-bit，甚至 2-bit （不过不建议低于 4-bit ）。</p><p></p><p>接下来将探讨两种在 HuggingFace** 上常用的方法：</p><p></p><p>GPTQ — 全模型在 GPU 上运行。GGUF — 将一部分模型层从 GPU 转移到 CPU 上执行。</p><p></p><h3>3.3.1 GPTQ</h3><p></p><p>GPTQ 无疑是实际应用中最著名的 4-bits 量化方法之一。1</p><p></p><p>它采用非对称量化（asymmetric quantization），并逐层处理，每一层都经过独立处理，然后再继续处理下一层：</p><p></p><p><img src="https://static001.geekbang.org/infoq/89/89b382ce26ee082118e6beeac9250c92.png" /></p><p></p><p>在这个逐层量化的过程中，首先将模型层的权重转换为 Hessian 矩阵（译者注：Hessian 矩阵是二阶偏导数矩阵，用于描述函数在其输入变量上的局部曲率。对于多变量函数，Hessian 矩阵可以帮助我们了解函数在某一点上的凹凸性，以及函数值对输入变量的变化有多敏感。）的逆矩阵。它是模型损失函数的二阶导数，它告诉我们模型输出对每个权重变化的敏感程度。</p><p></p><p>简单来说，该过程展示了模型层中每个权重的重要性（或者说是权重的影响程度）。</p><p></p><p>与 Hessian 矩阵中较小值相关的权重更为重要，因为这些权重的微小变化可能会对模型的性能产生重大影响。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c2dd2e30c6bb6ac0daf7cb5df10c66f.png" /></p><p></p><p>在 Hessian 矩阵的逆矩阵中，数值越低，权重越 “重要”。</p><p></p><p>接下来，我们对权重矩阵的第一行权重进行量化，再进行反量化：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4a/4a0c400c3c448b95ebd8101d7c06e6c0.png" /></p><p></p><p>通过这一过程，我们可以计算出量化误差 (q)，我们可以用之前计算的 Hessian 矩阵的逆矩阵（h_1）来调整这个误差。</p><p></p><p>换句话说，我们是在根据权重的重要性来构建加权量化误差（weighted-quantization error）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53a34232861680a798cf0c791da8838e.png" /></p><p></p><p>接着，我们将这个加权的量化误差重新分配到该行的其他权重上。这样做可以保持神经网络的整体功能（overall function）和输出（output）不变。</p><p></p><p>例如，如果要对第二个权重（如果它是 0.3（x_2））进行此操作，我们就会将量化误差（q）乘以第二个权重的 Hessian 矩阵的逆矩阵（h_2）加上去。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91f9678765a1f1107d700f3b1e42d5ac.png" /></p><p></p><p>我们可以对第一行中的第三个权重进行同样的处理：</p><p></p><p><img src="https://static001.geekbang.org/infoq/15/15e2b1bb9656ce813a1a071b6f7d9f97.png" /></p><p></p><p>重复这个重新分配加权量化误差的过程，直到所有值都被量化。</p><p></p><p>这种方法之所以行之有效，是因为权重之间通常是相互关联的。因此，当一个权重出现量化误差（quantization error）时，与之相关的权重也会相应地更新（通过 Hessian 矩阵的逆矩阵）。</p><p></p><p></p><blockquote>NOTE：本文作者[4]采用了几种技巧来加快计算速度并提高性能，例如在 Hessian 矩阵中添加阻尼因子（dampening factor）、“懒惰批处理（lazy batching）”，以及使用 Cholesky 方法预先计算信息（precomputing information）。我强烈建议各位读者观看这个视频[5]。</blockquote><p></p><p></p><p></p><blockquote>TIP：如果你想要一种可以优化性能和提高推理速度的量化方法，可以查看 EXL2[6] 这个项目。</blockquote><p></p><p></p><h3>3.3.2 GGUF</h3><p></p><p>虽然 GPTQ 是一种在 GPU 上运行完整 LLMs 的最佳模型量化方法，但我们可能很多时候没有这种条件。于是我们可以使用 GGUF 将 LLM 的某些模型层放到到 CPU 上进行处理。2</p><p></p><p>这样，当 VRAM 不足时，就可以同时使用 CPU 和 GPU。</p><p></p><p>量化方法 GGUF 仍不断在更新，并且其性能可能会根据量化位数的不同而有所变化。其基本原理如下：</p><p></p><p>首先，给定模型层的权重被分割成包含一组“子”块的“超级”块（“super” blocks）。</p><p></p><p>我们从这些 blocks 中提取比例因子（s）和 α（α）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/1246fede5eb200f62a671dcb1c9d0d01.png" /></p><p></p><p>为了量化给定的“子”块（“sub” block），我们可以使用之前介绍的 absmax 量化方法。这种方法会将给定权重乘以比例因子（s）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/12/12068be791d838e0e804b2052beee053.png" /></p><p></p><p>比例因子是通过“子”块的信息计算出来的，但量化时使用的是“超级”块的信息，后者有自己的比例因子：</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4ffb580e47ff3f0e077b5817f169bd7b.png" /></p><p></p><p>这种基于块（blocks）的量化方法使用“超级”块的比例因子（s_super）来量化“子”块的比例因子（s_sub）。</p><p></p><p>每个比例因子的量化级别可能会有所不同，“超级”块的比例因子通常比“子”块的比例因子有更高的精度。</p><p></p><p>为了更直观地理解，观看下图进一步了解这几个量化级别相关信息（ 2-bit、4-bit 和 6-bit ）：</p><p></p><p><img src="https://static001.geekbang.org/infoq/ec/ec74f774e44d8550c13fef3072b08a5d.png" /></p><p></p><p></p><blockquote>NOTE：在某些量化方法中，为了保持量化后的模型性能，可能需要一个额外的最小值来调整零点，以确保模型能够正确处理极端值。这个最小值和比例因子一样，都是量化过程中的关键参数，它们需要被正确地量化，以确保量化后的模型能够保持原有的性能。</blockquote><p></p><p></p><p>各位读者可以查看这个 PR[7] ，了解所有量化级别的详细信息。此外，还可以查看这个 PR[8]，获取更多关于使用重要性矩阵（importance matrices）进行量化的信息。</p><p></p><h1>04 第 4 部分：Quantization Aware Training</h1><p></p><p>在第 3 部分中，我们了解到如何在训练完成后对模型进行量化。这种方法的不足之处在于，量化过程并未考虑到实际的训练过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4cd3392c6c382dd611bb49b8742135e.png" /></p><p></p><p>于是 Quantization Aware Training（QAT）就有了用武之地。与训练后使用 post-training quantization（PTQ）技术对模型进行量化不同，QAT 的目标是在训练过程中学习量化过程。</p><p></p><p>QAT 通常比 PTQ 更准确，因为在训练过程中已经考虑了量化。其工作原理如下：</p><p></p><p>在训练过程中，引入所谓的“伪”量化。比如先将权重量化到例如 INT4 等形式，然后将它们反量化回 FP32 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/d7/d7edb3a5a342b5c3a4f32b501b7d9887.png" /></p><p></p><p>这一过程使得模型在训练阶段进行损失值计算和权重更新时能够考虑到量化误差。</p><p></p><p>QAT 尝试探索损失函数中的“宽”最小值区域，以尽可能减少量化误差，因为“窄”最小值区域往往会导致更大的量化误差。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b18915a9133feb2df4b53cbb1b5b738a.png" /></p><p></p><p>例如，假设我们在反向传播过程（backward pass）中没有考虑量化误差。我们将根据梯度下降法（gradient descent）选择损失值（loss）最小的权重。但是，如果它位于“窄”最小值区域，可能会引入更大的量化误差。</p><p></p><p>相反，如果我们考虑到量化误差，我们将选择在“宽”最小值区域中的不同权重进行更新，量化误差会小得多。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9b/9b9626cf089b44f7f5cf02f111c4d194.png" /></p><p></p><p>因此，虽然 PTQ 在高精度（例如，FP32）下具有较小的损失值，但 QAT 在低精度（例如， INT4 ）下的损失值较小，这正是我们追求的目标。</p><p></p><h2>4.1 1-bit LLM 的时代：BitNet</h2><p></p><p>正如前文所述，将量化位数降低到 4-bit 已经非常小了，但如果我们还要进一步降低呢？</p><p></p><p>这就是 BitNet[9] 的用武之地了，它使用 1-bit 表示模型的权重，每个权重都使用 -1 或 1 表示。3</p><p></p><p>它通过直接将量化过程整合到 Transformer 架构中来实现这一点。</p><p></p><p>Transformer 架构是大多数 LLMs 的基础，它依赖于线性层来处理序列数据，并在模型中执行关键的计算操作：</p><p></p><p><img src="https://static001.geekbang.org/infoq/6f/6f867c6a46703c68e9e06dd6fa9b95d3.png" /></p><p></p><p>这些线性层（linear layers）通常使用更高的精度，如 FP16，它们也是大部分权重所在的地方。</p><p></p><p>BitNet 将这些线性层替换为他们称之为 BitLinear 的模型层：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/557e9606923e81e470a5df4632cb7947.png" /></p><p></p><p>BitLinear 层的工作原理与普通线性层相同，根据权重（weights）和激活值（activation）的乘积计算输出值（output）。</p><p></p><p>BitLinear 层使用 1-bit 来表示模型的权重，并使用 INT8 来表示激活值：</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91df839add366efd6e3e2651ffe14330.png" /></p><p></p><p>类似于 Quantization-Aware Training（QAT）技术，BitLinear 层在训练过程中执行一种 “伪” 量化，以便用来分析权重和激活值的量化效果：</p><p></p><p><img src="https://static001.geekbang.org/infoq/10/1007ae8c8f69d93cea8fbd87155b1204.png" /></p><p></p><p></p><blockquote>NOTE：在论文中使用的是 γ 而不是 α ，但由于在本文中所举的例子一直使用 α ，所以我使用 α 。此外，请注意此处的 β 与前文在零点量化（zero-point quantization）中使用的 β 不同，它是基于平均绝对值（average absolute value）计算得出的。</blockquote><p></p><p></p><p>让我们一步一步来学习 BitLinear 。</p><p></p><h2>4.2 权重的量化 Weight Quantization</h2><p></p><p>在训练过程中，权重以 INT8 的形式存储，然后使用一种称为 signum 函数的基本策略，将其量化到 1-bit。</p><p></p><p>这种方法的核心在于，它将权重分布（distribution of weights）重新调整到以 0 为中心，然后将所有小于 0 的值（左侧）设置为 -1 ，将所有大于 0 的值（右侧）设置为 1 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a572f0074ff0f4f07f9f2cb8aa4c24f2.png" /></p><p></p><p>此外，它还会跟踪记录一个值 β（平均绝对值（average absolute value）），我们稍后会用到它来进行反量化（dequantization）。</p><p></p><h2>4.3 激活值的量化 Activation Quantization</h2><p></p><p>为了量化激活值，BitLinear 利用 absmax 量化方法将 FP16 格式的激活值转换为 INT8 格式，因为矩阵乘法 (×) 需要更高精度的激活值。</p><p></p><p><img src="https://static001.geekbang.org/infoq/75/759b252e32eeb8e9c45cd165cf5e3a67.png" /></p><p></p><p>同时，它还会跟踪记录 α（最高绝对值），我们将在后续的反量化过程中使用该值。</p><p></p><h2>4.4 反量化过程 Dequantization</h2><p></p><p>我们跟踪记录了 α（激活值的最高绝对值）和 β（权重的平均绝对值），因为这些值将在后续的反量化过程中帮助我们把激活值从 INT8 格式恢复到 FP16 格式。</p><p></p><p>输出激活值（output activations）通过 {α, γ} 进行缩放，然后进行反量化将其恢复到原始精度：</p><p></p><p><img src="https://static001.geekbang.org/infoq/26/26616211bcaeca9ae19b5375d0832b6c.png" /></p><p></p><p>就是这样！这个过程相对简单，只需用两个值（-1 或 1）来表示模型。</p><p></p><p>根据这一流程，作者发现随着模型规模的扩大，1-bit 形式和 FP16 形式训练的模型之间的性能差异逐渐缩小。</p><p></p><p>不过，这只适用于较大型的模型（参数超过 300 亿（30 B）），而对于较小型的模型，这个性能差距仍然很大。</p><p></p><h2>4.5 所有 LLMs 实际上均为 1.58-bit</h2><p></p><p>BitNet 1.58b[10] 就是为了解决之前提到的扩展性问题而提出的。4</p><p></p><p>在这种新方法中，模型的每一个权重不仅可以是 -1 或 1 ，还可以取 0 ，从而成为了一个三元模型。有趣的是，仅仅添加了 0 这一可取值就极大地提升了 BitNet 的性能，并使得计算速度大大提升。</p><p></p><h3>4.5.1 The Power of 0</h3><p></p><p>那么，为什么就添加了一个可取值 0 就能带来如此大的提升呢？</p><p></p><p>这与矩阵乘法的原理紧密相关！</p><p></p><p>首先，让我们了解一下矩阵乘法的一般工作原理。在计算输出值时，我们将权重矩阵（weight matrix）与输入向量（input vector）相乘。下图展示了权重矩阵第一层与输入向量相乘的过程：</p><p></p><p><img src="https://static001.geekbang.org/infoq/81/81c5cbcfe6e18476ba32cf775c3c103a.png" /></p><p></p><p>请注意，这一过程包含两个步骤：首先将每个权重与输入值相乘，然后将所有乘积相加。</p><p></p><p>与此不同，BitNet 1.58b 则省略了乘法这一步骤，因为三元权重（ternary weights）实际上传达了这样的信息：</p><p></p><p>1: 我想要加上这个值0: 我不需要加上这个值-1: 我想要减去这个值</p><p></p><p>因此，当权重量化到 1.58 bit 时，只需要执行加法运算：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/554c56dfeb18190e341e0bab7fb44995.png" /></p><p></p><p>这样不仅可以大大加快了计算速度，还可以进行特征过滤（feature filtering）。</p><p></p><p>将某个权重设置为 0 后，我们就可以选择忽略它，而不是像 1-bit 表示法那样要么加上要么减去权重。</p><p></p><h3>4.5.2 Quantization 量化过程</h3><p></p><p>在 BitNet 1.58b 中，进行权重量化（weight quantization）时采用了 absmean 量化方法，这是之前看到的 absmax 量化方法的一种改进形式。</p><p></p><p>这种方法通过压缩权重的分布，并利用权重的绝对平均值（α）来进行数值（value）的量化。之后，这些数值会被归整到 -1、0 或 1 ：</p><p></p><p><img src="https://static001.geekbang.org/infoq/7a/7ad4ab53c81e5ba650d48e2384ae1472.png" /></p><p></p><p>相较于 BitNet，激活值的量化过程基本相同，但还是有一点不同。激活值不再被缩放到 [0, 2ᵇ⁻¹] 区间，而是通过 absmax 量化方法被调整到了 [-2ᵇ⁻¹, 2ᵇ⁻¹] 区间。</p><p></p><p>就是这样！1.58-bit 量化主要需要两种技巧：</p><p></p><p>通过添加可取值 0 构建三元数值表示法 [-1, 0, 1]对权重实施 absmean 量化方法。</p><p></p><p>“13B BitNet b1.58 在响应延迟、内存占用和能耗方面，相较于 3B FP16 LLM 更高效。”</p><p></p><p>由于仅需 1.58 个 bits ，计算效率高，我们得以构建出更为轻量的模型！</p><p></p><h1>05 Conclusion</h1><p></p><p>我们的量化之旅到此告一段落！但愿本文能帮助你更深入地认识到量化技术、GPTQ、GGUF 以及 BitNet 的巨大潜力。未来模型的体积又将能够缩小到何种程度？真是令人期待啊！</p><p></p><p>如果要查看更多与 LLMs 相关的可视化内容，并希望支持我们，不妨留意一下我和 Jay Alammar 正在编写的新书。该书即将发行！</p><p></p><p><img src="https://static001.geekbang.org/infoq/00/00fe6928ffcd52e2a95b114cb38e1050.png" /></p><p></p><p>你可以在 O’Reilly 网站[11]上免费试读此书，或者直接在亚马逊[12]上预订。我们还会将所有相关代码同步更新到 Github[13] 上。</p><p></p><h1>Resources</h1><p></p><p>Hopefully, this was an accessible introduction to quantization! If you want to go deeper, I would suggest the following resources:</p><p></p><p>A HuggingFace blog about the LLM.int8()[14] quantization method: you can find the paper here[15]. （译者注：LLM.int8() 量化方法）Another great HuggingFace blog about quantization for embeddings[16].（译者注：嵌入向量的量化问题）A blog about Transformer Math 101[17], describing the basic math related to computation and memory usage for transformers.（译者注：介绍了与 Transformer 的计算和内存使用相关的基本概念）This[18] and this are two nice resources to calculate the (V)RAM you need for a given model.（译者注：计算特定模型所需（V）RAM 的数量）If you want to know more about QLoRA5, a quantization technique for fine-tuning, it is covered extensively in my upcoming book: Hands-On Large Language Models[19].（译者注：QLoRA 技术的学习资料）A truly amazing YouTube video[20] about GPTQ explained incredibly intuitively.（译者注：GPTQ 技术的学习资料）</p><p></p><p>脚注：</p><p></p><p>Frantar, Elias, et al. "Gptq: Accurate post-training quantization for generative pre-trained transformers." arXiv preprint arXiv:2210.17323 (2022).You can find more about GGUF on their GGML repository here[21].Wang, Hongyu, et al. "Bitnet: Scaling 1-bit transformers for large language models." arXiv preprint arXiv:2310.11453 (2023).Ma, Shuming, et al. "The era of 1-bit llms: All large language models are in 1.58 bits." arXiv preprint arXiv:2402.17764 (2024).Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." Advances in Neural Information Processing Systems 36 (2024).</p><p></p><p>Thanks for reading!</p><p></p><p>Hope you have enjoyed and learned new things from this blog!</p><p></p><p>Maarten Grootendorst</p><p></p><p>Data Scientist | Psychologist | Writer | Open Source Developer (BERTopic, PolyFuzz, KeyBERT) | At the intersection of Artificial Intelligence and Psychology</p><p></p><p>END</p><p></p><h1>🔗文中链接🔗</h1><p></p><p>[1]https://en.wikipedia.org/wiki/IEEE_754</p><p></p><p>[2]https://pixabay.com/users/slava_web-designer-39623293/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=8668140</p><p></p><p>[3]https://huggingface.co/meta-llama/Meta-Llama-3-8B/tree/main</p><p></p><p>[4]https://arxiv.org/pdf/2210.17323</p><p></p><p>[5]https://www.youtube.com/watch?v=mii-xFaPCrA</p><p></p><p>[6]https://github.com/turboderp/exllamav2</p><p></p><p>[7]https://github.com/ggerganov/llama.cpp/pull/1684</p><p></p><p>[8]https://github.com/ggerganov/llama.cpp/pull/4861</p><p></p><p>[9]https://arxiv.org/pdf/2310.11453</p><p></p><p>[10]https://arxiv.org/pdf/2402.17764</p><p></p><p>[11]https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/</p><p></p><p>[12]https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961</p><p></p><p>[13]https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</p><p></p><p>[14]https://huggingface.co/blog/hf-bitsandbytes-integration</p><p></p><p>[15]https://arxiv.org/pdf/2208.07339</p><p></p><p>[16]https://huggingface.co/blog/embedding-quantization</p><p></p><p>[17]https://blog.eleuther.ai/transformer-math/</p><p></p><p>[18]https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator</p><p></p><p>[19]https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961</p><p></p><p>[20]https://www.youtube.com/watch?v=mii-xFaPCrA</p><p></p><p>[21]https://github.com/ggerganov/ggml/blob/master/docs/gguf.md</p><p></p><p>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</p><p></p><p>原文链接：</p><p></p><p>https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</id>
            <title>揭秘谷歌搜索排名的工作原理</title>
            <link>https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/UNmjGDyGK5XBLIAEt7Ui</guid>
            <pubDate></pubDate>
            <updated>Thu, 22 Aug 2024 02:29:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>从现有的资料来看，谷歌文档泄露事件与反垄断听证会公开的谷歌搜索排名文件并未直接揭开谷歌搜索排名的全部运作细节。</p><p></p><p>随着机器学习技术的深入应用，有机搜索结果背后的机制变得极其复杂，即便是谷歌内部负责排名算法的专业人士，也难以精确阐述为何某个特定结果会位居榜首或次席。我们尚不清楚这些众多影响因素的具体权重及它们之间错综复杂的相互作用关系。</p><p></p><p>然而，深入理解搜索引擎的整体架构仍然至关重要。这不仅能帮助我们理解为何某些精心优化的网页未能获得高位排名，还能揭示为何一些看似简单且未经刻意优化的结果却能脱颖而出。更为关键的是，这促使我们拓宽视野，重新审视并识别出真正影响排名的核心要素。</p><p></p><p>所有已披露的信息均指向这一点。对于任何关注搜索引擎优化（SEO）的人来说，都应将这些新发现融入自己的思考框架中。这将促使我们以全新的视角审视自己的网站，并在分析、规划与决策过程中引入更多维度的考量标准。</p><p></p><p>坦诚而言，要精确勾勒出这些复杂系统的全貌实属不易。网络上关于此类信息的解读往往存在分歧，即便是讨论同一主题，所用术语也可能大相径庭。</p><p></p><p>举个例子，负责优化搜索结果页面（SERP）布局的系统，在某些谷歌文档中被称为 “Tangram”，而在其他文档中则换上了 “Tetris” 这一名称，这或许是对那款经典游戏的巧妙借喻。</p><p></p><p>经过数周的深入研究，我反复查阅、分析、整理、筛选并重组了近百份相关文档。本文虽非尽善尽美或绝对权威，但确系我基于现有知识与理解，以类似侦探福尔摩斯般的细致精神，竭尽所能完成的成果。呈现在你面前的，便是我个人视角下的探索总结。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/43/43793919af8d3782159dc1e8c537fd44.jpg" /></p><p></p><p>作者创作的谷歌排名工作原理的图解概览</p><p></p><p></p><h3>一份新文档等待谷歌爬虫访问</h3><p></p><p></p><p>当你发布一个新网站时，它并不会立即被谷歌索引。谷歌需要首先发现这个网站的 URL，这通常是通过更新站点地图或是由一个已知 URL 上的链接引导来实现的。</p><p></p><p>对于像首页这样频繁被访问的页面，它们往往会更快地将新链接的信息传递给谷歌。</p><p></p><p>谷歌的 Trawler 系统负责抓取新内容，并跟踪何时重新访问这些 URL 以检查是否有更新。这一过程由调度器精心管理，而存储服务器则负责决定是转发这些 URL 供进一步处理，还是将它们暂时放置在所谓的 “沙盒” 中。尽管谷歌官方否认了沙盒机制的存在，但最近的泄露信息却暗示，那些被怀疑为垃圾或低质量的网站确实有可能被置于这样的环境中进行观察。值得注意的是，谷歌似乎还会转发一些垃圾内容，这可能是为了深入分析，以进一步优化其算法。</p><p></p><p>假设某个文档成功通过了这一系列筛选，那么文档中的外部链接将被提取出来，并被分类为内部链接或外部链接。这些链接信息随后会被其他系统用于进行链接分析和 PageRank 计算（关于这一点，我们稍后会详细阐述）。</p><p></p><p>而对于指向图像的链接，它们则会被专门转发给 ImageBot 进行处理。这个过程有时可能会遇到显著的延迟。ImageBot 会调用这些链接，并将图像与相同或相似的图像一起存储在图像数据库中。此外，Trawler 还会根据它自己的 PageRank 评估结果来调整对网站的抓取频率。简单来说，如果一个网站的访问量较大，那么 Trawler 对它的抓取频率也会相应提高，这被称为 ClientTrafficFraction（客户端流量比例）的影响。</p><p></p><h3>Alexandria：伟大的索引库</h3><p></p><p></p><p>谷歌的索引系统名为 Alexandria，它巧妙地为每一份内容分配一个独一无二的 DocID。若内容已存在于系统中，比如在处理重复内容时，系统不会生成新的 ID，而是会将新发现的 URL 与已存在的 DocID 相关联，实现内容的统一管理和识别。</p><p></p><p>值得注意的是，谷歌严格区分 URL 与文档的概念。一个文档可以涵盖多个 URL，这些 URL 虽然指向不同位置或包含细微差异（如不同语言版本的页面），但只要它们的内容相似且被正确标记，就会被视为同一文档的不同表现形式。同时，来自其他域的 URL 也会在这一体系下被合理分类。所有这些 URL 所携带的信息和信号，都会通过它们所关联的同一个 DocID 来整合处理，确保内容的一致性和准确性。</p><p></p><p>在处理重复内容时，谷歌会精心挑选一个规范版本作为搜索结果的主要展示对象。这也解释了为什么我们有时会看到多个 URL 在搜索结果中排名相近 —— 它们实际上都指向了同一个文档的不同入口。而 “原始”（即规范）URL 的确定并非一成不变，它可能会随着谷歌算法的更新和内容的演变而有所调整。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/30/302a5ea9487c77d751a9ceae87b4f41a.jpg" /></p><p></p><p>图 1: Alexandria 收集文档的 URL</p><p></p><p>由于我们的文档在网络上独一无二，因此被赋予了一个专属的 DocID。</p><p></p><p>网站的不同部分会被搜索引擎细致扫描，寻找相关关键词短语，并将这些信息推送至搜索索引中。在这一过程中，页面上的所有关键词 “亮点”（即 “命中列表”）首先会被送往直接索引，该索引负责整合页面上重复出现的关键词。</p><p></p><p>随后，这些关键词短语会被精心编织进倒排索引的词汇表中。以 “铅笔” 为例，这个词及其所有包含它的关键文档，都已被纳入索引体系之中。</p><p></p><p>简而言之，由于我们的文档中 “铅笔” 一词频繁出现，它现在在词汇索引中占据了 “铅笔” 条目的位置，并与对应的 DocID 紧密相连。</p><p></p><p>与 “铅笔” 相关联的 DocID 会获得一个通过精密算法计算出的 IR（信息检索）分数，该分数将在后续用于搜索结果列表中的排序。值得注意的是，若 “铅笔” 一词在我们的文档中被加粗显示，或位于 H1 标签中（这些信息存储在 AvrTermWeight 中），这些都会作为提升 IR 分数的积极信号。</p><p></p><p>谷歌会将视为重要的文档迁移至其核心存储系统 ——HiveMind，即主存储器。这里融合了高速 SSD 与传统 HDD（称为 TeraGoogle），后者用于长期存储非即时访问的数据。文档和信号都存储在主存储器中。</p><p></p><p>据专家估算，在人工智能热潮兴起之前，全球大约半数的网络服务器均由谷歌托管。这一庞大的互联集群网络，使得数百万个主存储单元能够高效协同工作。甚至有谷歌工程师在会议中提及，理论上，谷歌的主存储器容量足以涵盖整个互联网的信息量。</p><p></p><p>有趣的是，存储在 HiveMind 中的链接，包括反向链接，似乎被赋予了更高的权重。例如，来自权威文档的链接将获得更多重视，而存于 TeraGoogle（HDD）中的 URL 链接则可能权重较低，甚至被忽略不计。</p><p></p><p>提示：为你的文档提供准确且一致的日期信息至关重要。无论是源代码中的日期（BylineDate）、从 URL 和 / 或标题中提取的日期（syntaticDate），还是从内容中解析的日期（semanticDate），都将被综合考虑。随意更改日期以营造时效性的假象可能导致搜索引擎降权处理。lastSignificantUpdate 属性精确记录了文档最后一次重大更新的时间，细微的修改或拼写更正并不会触动这一计数器。</p><p></p><p>每个 DocID 的附加信息与信号都被动态存储在 PerDocData 库中，供多个系统在优化搜索结果相关性时调用。此外，文档的最近 20 个版本都会被保存在历史记录中（通过 CrawlerChangerateURLHistory 实现），使谷歌能够评估并追踪内容随时间的演变。</p><p></p><p>若你计划彻底改变一个文档的内容或主题，理论上需通过创建一系列过渡版本逐步过渡，以覆盖并替换旧的内容信号，这一过程可能需持续发布多达 20 个版本。这解释了为何复活过期域名（即曾活跃后废弃的域名）并不总能带来排名上的优势。</p><p></p><p>当域名的管理权发生变更，同时内容主题也大幅调整时，谷歌系统能够敏锐地捕捉到这些变化，并将所有相关信号重置，使得旧域名在排名上不再享有特殊优待，与全新注册的域名站在同一起跑线上。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b6/b6e6a29aa4abae2a87e3eb9fd413468b.jpg" /></p><p></p><p>图 2：除了泄露的信息外，美国司法部门对谷歌的审判和听证会提供的证据文件也是进行深入研究的宝贵资源。这些文件中还包含了内部电子邮件。</p><p></p><p></p><h3>QBST: 搜索 “铅笔” 的详细过程</h3><p></p><p></p><p>当你在谷歌中输入 “铅笔” 进行搜索时，QBST 系统便立刻启动，开始处理这一请求。系统首先会细致地分析搜索关键词，如果搜索短语由多个词汇组成，这些词汇会被精准地传递到词汇索引中，进行深入的检索。</p><p></p><p>接下来，术语加权过程会登场，这是一个复杂而精密的步骤，它涉及到了 RankBrain、DeepRank（原名 BERT）以及 RankEmbeddedBERT 等多个先进的系统。在这些系统的协同作用下，与 “铅笔” 紧密相关的词汇会被进一步传递给 Ascorer，进行更深层次的处理。</p><p></p><p></p><h3>Ascorer: 构建 “绿色环”</h3><p></p><p></p><p>Ascorer 的工作是从倒排索引中筛选出与 “铅笔” 最相关的前 1000 个文档（DocID），并按照信息检索（IR）评分进行排序。这个排序后的文档列表，我们称之为 “绿色环”，在行业内也被广泛称为发布列表或 posting list。</p><p></p><p>Ascorer 作为 Mustang 排名系统的重要组成部分，还会通过一系列精细的过滤手段，如去重（利用 SimHash 技术）、段落分析以及识别原创和有价值的内容等，对这 1000 个候选文档进行进一步的筛选和优化，最终目的是将这 1000 个候选项精炼成用户眼前所见的 “10 个蓝色链接” 或 “蓝色环”。</p><p></p><p>关于铅笔的文档，在当前的发布列表中排名第 132 位。如果没有其他系统的进一步介入，那么这将是它在搜索结果中的最终位置。</p><p></p><p></p><h3>Superroot: 从千中选优，打造 “蓝色环”</h3><p></p><p></p><p>然而，Superroot 系统并不会让事情就此定格。它的任务是将 “绿色环” 中的 1000 个文档重新排序，通过更加精确和细致的算法，将这庞大的数量精确地缩减到仅包含 10 个结果的 “蓝色环”。</p><p></p><p>在这个过程中，Twiddlers 和 NavBoost 等系统扮演着关键角色，它们负责执行具体的筛选和排序任务。尽管可能还有其他系统也参与其中，但由于信息有限，我们无法一一详述其具体细节。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f1/f1f2d1693f47249dee39d6fceabdf975.jpg" /></p><p></p><p>图 3：Mustang 生成 1,000 个潜在结果，随后由 Superroot 将这些结果筛选至 10 个最终结果。</p><p></p><p>尽管 “谷歌咖啡因（Caffeine）” 这一名称仍被提及，但其最初作为独立系统的形式已不复存在，仅作为历史记忆保留。如今，谷歌构建了一个庞大的微服务架构，这些微服务紧密协作，共同为网页文档生成各种关键属性。这些属性不仅是不同排名和重排系统的核心信号，还助力神经网络模型进行更精准的预测。</p><p></p><p></p><h3>过滤器中的多面手：Twiddler 系统</h3><p></p><p></p><p>当前，谷歌正运用着成百上千个 Twiddler 系统，它们的作用类似于 WordPress 插件，但专注于搜索引擎内部的优化任务。每个 Twiddler 都肩负着特定的过滤使命，这种模块化设计不仅简化了创建过程，还避免了直接干预 Ascorer 中复杂排名算法的必要性，后者一旦修改，可能引发连锁反应，需要周密的规划与编程工作。</p><p></p><p>Twiddler 系统以其灵活性和独立性著称，它们可以并行或顺序工作，彼此间无需知晓对方的操作细节。根据工作特性的不同，Twiddler 大致分为两类：</p><p></p><p>PreDoc Twiddlers：这类 Twiddler 能够高效处理大规模的 DocID 集合，因为它们对额外信息的需求极低，从而在处理初期就能显著缩减发布列表的条目数量，为后续步骤打下基础。“Lazy” 类型 Twiddlers：相比之下，这类 Twiddler 则更为复杂，它们需要额外信息，如从PerDocData数据库中提取的数据，这使得处理过程更加耗时。</p><p></p><p>因此，它们通常在 PreDoc Twiddlers 完成初步筛选后才介入。</p><p></p><p>通过这种分阶段处理策略，谷歌极大地优化了计算资源的利用效率，节省了宝贵的时间。</p><p></p><p>不同的 Twiddler 对文档的最终排名产生着直接或间接的影响。有的 Twiddler 通过调整信息检索（IR）评分来提升或降低文档的排名权重；而另一些则直接干预排名位置。例如，对于新入库的文档，一个专注于提升新文档排名的 Twiddler 可能会将 IR 评分大幅提升 1.7 倍，从而将文档从第 132 位迅速推升至第 81 位。</p><p></p><p>此外，为了提升搜索结果页面（SERP）的多样性，有 Twiddler 会专门降低内容相似文档的权重，这进一步促使我们的铅笔文档排名上升了 12 位，达到第 69 位。更有甚者，一个专门限制特定查询下博客页面数量的 Twiddler，将我们的文档排名进一步提升至第 61 位。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/74/74e92857a206e0000a24659a08aa4577.jpg" /></p><p></p><p>图 4：两种类型的 Twiddler—— 超过 100 个 Twiddler 用于减少潜在搜索结果，并对这些结果进行重新排序。</p><p></p><p>在我们的页面中，CommercialScore属性得到了零分（即被标记为 “是”），这表示 Mustang 系统在分析过程中检测到了销售意图。谷歌可能注意到，“铅笔” 搜索后经常会跟随如 “买铅笔” 这样的具有明确商业购买意图的搜索，这表明用户有交易倾向。因此，一个专门识别并响应此类意图的 Twiddler 会介入，通过添加相关商业结果，将我们的页面排名提升了 20 位，最终排在第 41 位。</p><p></p><p>随后，另一个 Twiddler 启动，实施了所谓的 “页面三惩罚”，旨在将疑似垃圾内容的页面排名限制在搜索结果的前三页之内（即最大排名为第 31 位）。这一限制由BadURL-demoteindex属性控制，该属性为页面排名设定了上限。类似DemoteForContent、DemoteForForwardlinks和DemoteForBacklinks等属性也用于实现内容降级的目的。因此，在排除了我们上方三个被降级的文档后，我们的页面排名进一步上升至第 38 位。</p><p></p><p>尽管我们的文档有可能受到降级的影响，但为了简化讨论，我们假设它未受影响。接下来，我们考虑一个通过评估嵌入内容来判断我们铅笔页面与网站主题相关性的 Twiddler。由于我们的网站专注于书写工具，这一特点对我们极为有利，导致另外 24 个与主题关联度不高的文档受到负面影响。</p><p></p><p>举个例子，假设有一个内容多样化的价格比较网站，其中有一页专门介绍铅笔，虽然内容丰富，但因其主题与网站整体内容大相径庭，该页面可能会因此 Twiddler 而被降级。</p><p></p><p>siteFocusScore和siteRadius等属性反映了页面内容与网站主题的紧密程度。得益于此，我们的信息检索（IR）评分再次获得提升，而其他一些结果则因相关性较低而排名下降，最终我们的页面排名跃升至第 14 位。</p><p></p><p>正如之前所述，Twiddler 的功能极为广泛且灵活。开发人员可以不断尝试新的过滤规则、调整乘数或设置特定的排名限制，甚至能够精确控制某个结果在页面上的具体排列顺序。</p><p></p><p>值得注意的是，一份谷歌内部泄露的文件发出警告，指出某些 Twiddler 功能应由专家谨慎使用，并在与核心搜索团队充分沟通后实施。</p><p></p><p></p><blockquote>“即便你认为自己已经洞悉了这些系统的运作奥秘，相信我，那也只是冰山一角。我们自己也尚未能完全参透。”—— 摘自 泄露的《Twiddler 快速入门指南 – Superroot》文档</blockquote><p></p><p></p><p>此外，还有一类专门的 Twiddler，它们负责创建注释并将这些注释附加到文档 ID（DocID）上，从而在搜索结果页面（SERP）中直观展示。比如，它们可能会在摘要中嵌入图片，或动态调整标题及描述内容，以优化用户体验。</p><p></p><p>如果你在疫情期间好奇为何你所在国家的卫生部门（比如美国的卫生与公共服务部）在 COVID-19 相关搜索中总是稳居榜首，答案很可能就藏在一个特定的 Twiddler 里。这个 Twiddler 通过识别查询语言和国家代码，利用特定的算法提升了官方资源的排名权重。</p><p></p><p>虽然用户对于 Twiddler 如何具体调整搜索结果排序的控制力有限，但了解其工作机制无疑能帮助我们更好地理解排名的波动或那些看似 “难以捉摸” 的排名现象。因此，定期检查 SERP，并留意结果类型的多样性显得尤为重要。</p><p></p><p>举例来说，你是否发现，无论搜索词如何变化，论坛讨论和博客文章的数量在搜索结果中似乎总是保持不变？你可以进一步思考：这些结果中，交易性、信息性或导航性的内容各占多少比例？相同的域名是否会频繁出现在不同但相近的搜索查询结果中？</p><p></p><p>如果你观察到搜索结果中在线商店的数量寥寥无几，那么试图通过类似商店网站来提升排名可能并非明智之举。相反，将重心转向创作更多信息丰富的内容可能更为有效。当然，在做出决策之前，我们还需深入探讨 NavBoost 系统的作用，因为它同样在搜索结果排序中扮演着重要角色。</p><p></p><p></p><h3>谷歌的质量评估员和 RankLab</h3><p></p><p></p><p>谷歌在全球范围内聘请了数千名质量评估员，他们负责审视特定的搜索结果，并在新算法或过滤器正式启用前进行初步测试。谷歌方面澄清：“这些评估结果并不直接决定搜索排名。” 尽管此言非虚，但这些评估在间接层面对排名产生了显著影响。</p><p></p><p>评估员的工作流程大致如下：他们会接收到网址或搜索短语（即待评估的搜索结果），并在移动设备上回答一系列预设问题。例如，他们可能会被问及：“这篇内容的作者是谁？写作时间是什么时候？作者在其领域内是否具备专业知识？” 这些回答随后会被记录下来，作为训练机器学习算法的重要数据。算法通过分析这些数据，能够辨别出哪些页面质量上乘、值得信赖，而哪些则相对逊色。</p><p></p><p>这一机制的核心在于，搜索排名的标准并非由谷歌搜索团队直接设定，而是通过深度学习技术，从人工评估中提炼出模式与规律。为了更直观地理解，我们可以设想一个场景：如果大众普遍认为，包含作者照片、全名及 LinkedIn 个人简介链接的内容更具可信度，那么缺乏这些元素的页面在可信度上自然会大打折扣。当神经网络在训练过程中接触到这些特征及相应的评估结果时，它会将这些特征视为影响排名的关键因素。经过多轮正面验证，通常这一过程会持续至少 30 天，网络可能会开始将这些特征作为重要的排名信号。因此，具备这些特征的页面可能会获得排名上的优势，而缺失这些特征的页面则可能面临排名下降的风险。</p><p></p><p>值得注意的是，尽管谷歌官方可能并未特别强调作者信息的重要性，但泄露的信息显示，如 isAuthor 等属性以及通过 AuthorVectors 实现的 “作者指纹识别” 技术，实际上能够识别并区分出作者独特的语言风格（即个体用词和表达方式）。</p><p></p><p>评估员的反馈会被汇总成 “信息满意度”（IS）评分。尽管参与评估的人数众多，但 IS 评分主要集中应用于少数网址。对于其他具有相似特征的页面，系统会采用外推的方式，利用这些评分来辅助排名决策。谷歌指出：“许多文档可能并未获得大量点击，但它们依然具有重要意义。” 当外推方法不适用时，系统会将相关文档自动提交给评估员进行评分。</p><p></p><p>在提及 “黄金” 一词时，它常与质量评估员相关联，暗示着可能存在某种文档或文档类型的最高标准。可以合理推测，符合评估员期望的文档有可能达到这一黄金标准。此外，某些特定的 Twiddler 可能会为被视为 “黄金” 级别的 DocID（文档标识符）提供显著的排名提升，使其跻身搜索结果的前列。</p><p></p><p>值得一提的是，这些质量评估员往往并非谷歌的全职员工，他们可能通过外部公司参与工作。而谷歌的专家则在 RankLab 中致力于实验与研发，不断推出新的 Twiddler，并评估其是否能有效提升搜索结果的质量，或是仅仅起到过滤垃圾信息的作用。经过严格验证并证明有效的 Twiddler 将被整合到 Mustang 系统中，该系统利用复杂、计算密集型且相互关联的算法，对搜索结果进行精细化的处理与优化。</p><p></p><p></p><h3>但是用户想要什么？</h3><p></p><p></p><p>NavBoost 可以解决这个问题！</p><p></p><p>我们的铅笔文档尚待进一步完善。在 Superroot 系统中，NavBoost 这一核心系统占据了决定搜索结果排序的关键位置。NavBoost 采用 “切片” 技术，以灵活管理移动端、桌面端及本地搜索等多样化的数据集。</p><p></p><p>尽管谷歌官方坚称未将用户点击数据纳入排名考量，但 FTC 文件中一封内部邮件的披露却揭示了点击数据处理过程的保密性要求，这在一定程度上引发了外界遐想。</p><p></p><p>这并不意味着谷歌的做法存在不妥，其否认背后实则蕴含双重考量。首要的是，一旦承认使用点击数据，可能会触发媒体对隐私问题的强烈关注，将谷歌置于 “数据巨头” 的舆论漩涡中，被指责为无孔不入地追踪用户在线行为。然而，实际上，点击数据的运用旨在获取具有统计学意义的信息，以优化搜索体验，而非针对个体用户的监控。尽管数据保护倡导者可能对此持保留意见，但这一解释无疑为谷歌的否认立场提供了合理解释。</p><p></p><p>FTC 文件的记载进一步印证了点击数据在排名中的实际作用，而 NavBoost 系统在此过程中更是频频被提及（仅在 2023 年 4 月 18 日的听证会上就被提及了 54 次）。此外，回溯至 2012 年的官方听证会，也已明确指出了点击数据对搜索排名产生的实际影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8d/8d8aa0883eac608f016338c364ff02bd.jpg" /></p><p></p><p>图 5：自 2012 年 8 月以来（！），官方已经明确点击数据会改变排名。</p><p></p><p>研究表明，搜索结果中的用户点击行为以及网站或网页的流量情况都会对其在搜索引擎中的排名产生影响。谷歌能够直接在搜索结果页面（SERP）上监控和评估用户的搜索行为，包括搜索操作、点击选择、重复搜索以及重复点击等行为。</p><p></p><p>有一种观点认为，谷歌可能通过其自家的谷歌分析（Google Analytics）工具来推测域名的流量数据，这导致部分用户选择避免使用该系统。然而，这一观点存在局限性。首先，Google Analytics 并不提供对所有交易数据的全面访问权限，限制了其推测能力的准确性。更为关键的是，由于超过 60% 的用户使用的是谷歌 Chrome 浏览器（其用户数量已超过三亿），谷歌能够收集到海量的网络活动数据。这使得 Chrome 在分析网络动态中扮演着至关重要的角色，这一点在相关听证会上也得到了明确强调。此外，Core Web Vitals 的数据也是通过 Chrome 进行收集的，并最终汇总为 “chromeInTotal” 值，用于评估网站的性能。</p><p></p><p>关于 “监控” 的负面舆论是谷歌否认使用点击数据的一个原因。另一个原因是，担心评估点击和流量数据可能会激励垃圾邮件发送者和骗子使用机器人系统伪造流量，从而试图操控搜索排名。虽然谷歌的这种否认态度可能会让人感到沮丧，但其背后的担忧和理由却是可以理解的。</p><p></p><p>在存储的指标中，包括了 “badClicks”（坏点击）和 “goodClicks”（好点击）等评估标准。这些评估通常会考虑搜索者在目标页面上的停留时间、他们浏览了多少其他页面以及这些页面的浏览时间（这些数据来源于 Chrome）如果搜索者在搜索结果中短暂偏离后又迅速返回并点击了其他结果，这种行为可能会增加 “坏点击” 的数量。而在一个搜索会话中，最后一次被认为是 “好” 点击的搜索结果则会被记录为 “lastLongestClick”（最长点击）。为了确保数据的准确性和防止被操控，这些数据会经过压缩处理以在统计上进行标准化。如果某个页面、一组页面或一个域名的首页通常具有良好的访问指标（这些数据同样来源于 Chrome），那么这将会通过 NavBoost 产生积极效果。通过分析在一个域名内或跨域名的流动模式，甚至可以评估网站导航的用户引导效果。由于谷歌能够监测整个搜索会话过程，因此在极端情况下它甚至可能识别出与搜索查询完全不同的文档也适合该查询。例如如果搜索者在搜索过程中离开了他们最初点击的域名并访问了另一个域名（可能是通过该域名中的链接跳转过去的）并在新域名上停留较长时间那么这个作为搜索 “结束” 的文档在未来就有可能通过 NavBoost 被推到更前面的位置前提是它在选择范围内。当然这需要大量搜索者提供有力的统计信号作为支持。</p><p></p><p>接下来我们来详细分析搜索结果中的点击情况。在每个搜索结果页面（SERP）中不同排名位置的结果都有一个平均预期点击率（CTR）作为性能评估的基准。例如根据 Johannes Beus 在今年柏林 CAMPIXX 会议上的分析结果显示排名第一的自然搜索结果平均可以获得 26.2% 的点击率而排名第二的结果则只能获得 15.5% 的点击率。</p><p></p><p>如果某个搜索结果的实际点击率显著低于预期值那么 NavBoost 系统会记录这一差异并据此调整该结果的排名位置（即 DocID 的排名）。相反如果某个结果的实际点击量在历史上一直明显多于或少于预期值 NavBoost 也会相应地调整该文档的排名位置以确保搜索结果的相关性和准确性（见图 6 所示）。</p><p></p><p>这种方法是合理的因为点击率从本质上反映了用户对搜索结果相关性的评价这些评价又是基于搜索结果的标题、描述以及域名等因素得出的。这一概念在谷歌的官方文档中也有详细说明（如图 7 所示）从而进一步证明了其合理性和科学性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/fc/fccc5c008e2e60078614ea7859ce5648.jpg" /></p><p></p><p>图 6：如果 “预期 _CRT” 与实际值有显著差异，则排名会相应调整。（数据源：J. Beus，SISTRIX，带编辑覆盖）</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7c/7cbf2c5b548d6b6599fb359411fc4576.jpg" /></p><p></p><p>图 7：谷歌演示文稿中的幻灯片（来源：审判证据 - UPX0228，美国及原告州诉谷歌公司）</p><p></p><p>由于我们的铅笔文档刚刚发布不久，因此目前还缺乏具体的点击率（CTR）数据。对于这类无数据的新文档，系统是否会忽略 CTR 偏差尚不明朗，但从其融入用户反馈的设计初衷来看，这种可能性是存在的。另一种推测是，系统可能会依据其他相关指标对 CTR 进行初步估算，这与谷歌 Ads 中处理质量因子的方式有异曲同工之妙。</p><p></p><p>SEO 专家和数据分析师在长期实践中发现，一旦他们全面监控点击率，便会注意到一个规律：当某个文档首次跻身搜索结果前 10 名，而其实际 CTR 显著低于预期时，其排名往往会在几天内（具体时间取决于搜索频率）出现下滑。相反，若 CTR 远高于预期，则排名有望攀升。面对 CTR 表现不佳的情况，快速调整文档的摘要信息（如优化标题和描述）以吸引更多点击至关重要，否则排名下滑后恢复难度将大幅增加。这一现象被普遍视为系统测试机制的一部分，即文档若表现优异则稳固高位，若不符用户期待则可能被剔除。至于这是否与 NavBoost 系统直接相关，目前尚无确凿证据。</p><p></p><p>根据泄露的信息，谷歌在估算新页面信号时，似乎高度依赖于页面 “环境” 中的海量数据。例如，新页面在初期可能会继承主页的 PageRank（称为 HomePageRank_NS），直至其建立起自己的 PageRank。同时，pnavClicks 可能用于预测通过导航链接到新页面的点击概率。</p><p></p><p>鉴于计算和更新 PageRank 的复杂性及高计算成本，谷歌可能采用了 PageRank_NS 指标作为过渡方案。“NS” 代表 “最近种子”，意味着相关页面共享一个临时的 PageRank 值，该值会根据需要长期或短期地应用于新页面。</p><p></p><p>此外，邻近页面的信号也可能对其他关键指标产生影响，助力新页面在缺乏高流量或反向链接的情况下提升排名。值得注意的是，许多信号的反映并非即时，而是存在一定的延迟。</p><p></p><p>谷歌在听证会上展示了 “新鲜度” 在搜索结果中的实际应用。以搜索 “Stanley Cup” 为例，平时搜索结果多聚焦于这一著名奖杯的介绍，但在斯坦利杯冰球比赛期间，NavBoost 会根据搜索和点击行为的变化，优先展示与比赛紧密相关的信息。这里的 “新鲜度” 并非指文档的新旧，而是指搜索行为和兴趣点的动态变化。谷歌每天处理的搜索行为超过十亿次，每一次搜索和点击都在为谷歌的学习提供宝贵数据。这意味着，谷歌对搜索意图的捕捉和响应远比我们想象的细腻和及时，而非仅仅局限于对季节性变化的简单预测。</p><p></p><p>最新数据显示，文档的点击指标会被存储并评估长达 13 个月之久（每年有一个月的数据与前一年重叠，以便进行对比分析）。鉴于我们的假设域名拥有强大的访问指标和显著的广告直接流量，作为知名品牌（这是一个正面信号），我们的新 “铅笔” 文档自然能够从前期的成功页面中获益。因此，NavBoost 系统成功将我们的排名从第 14 位提升至第 5 位，使我们跻身 “蓝色环” 或前 10 名之列。这前 10 名的文档将与其他九个自然搜索结果一同被转发至谷歌的网络服务器。</p><p></p><p>值得注意的是，谷歌实际提供的个性化搜索结果并不像人们普遍预期的那样丰富。测试表明，通过模拟用户行为并进行相应调整往往能带来更优化的搜索结果，而非单纯依赖于评估个别用户的偏好。这一发现极具启示意义 —— 神经网络的预测能力已经超越了我们的个人浏览和点击历史记录所能提供的个性化程度。当然，对于特定偏好（如对视频内容的喜好），个性化搜索结果仍会予以体现。</p><p></p><p></p><h3>谷歌网络服务器：一切终结与新开始的地方</h3><p></p><p></p><p>谷歌网络服务器（GWS）是构建和呈现搜索结果页面（SERP）的核心，这个页面上包含了诸多元素：十个蓝色链接的自然搜索结果、广告、图片、谷歌地图视图、“人们也在问” 板块等。</p><p></p><p>为了优化这些元素在有限页面空间内的布局，谷歌采用了 Tangram 系统。该系统负责计算每个元素所需的空间大小，并智能决定在给定的 “框架” 内能容纳多少结果。紧接着，Glue 系统会将这些元素精确无误地安置到它们应有的位置上，确保页面既美观又高效。</p><p></p><p>目前，我们的 “铅笔” 文档在自然搜索结果中排名第五，但值得注意的是，CookBook 系统拥有在搜索结果展示前的最后一刻进行微调的能力。这个系统内部集成了 FreshnessNode、InstantGlue（能在 24 小时内快速反应，但通常会有约 10 分钟的延迟）和 InstantNavBoost 等组件。这些组件如同 “幕后英雄”，在最终页面呈现之前，迅速生成与搜索结果时效性紧密相关的信号，并可能据此对排名进行动态调整。</p><p></p><p>想象一下这样的场景：一档关于 Faber-Castell 品牌 250 周年纪念以及 “铅笔” 这一关键词的德国电视节目突然热播。在节目播出的几分钟内，成千上万的观众可能会迅速拿起他们的智能手机或平板电脑进行搜索。这时，FreshnessNode 便会敏锐地捕捉到 “铅笔” 搜索量的激增，并智能地分析出用户的搜索意图是寻求信息而非直接购买。基于这一判断，系统会相应地调整搜索结果的排名。</p><p></p><p>具体来说，InstantNavBoost 会立即采取行动，将所有与交易相关的结果暂时移除，转而用更加信息丰富、与当前热点紧密相关的结果来替代。同时，InstantGlue 也会迅速更新 “蓝色环” 内的结果排序，导致我们原本可能以销售为导向的文档因为不够相关而被更合适的结果挤出前列。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/7e/7e35991f5946e6ec8885cf5c4483905c.jpg" /></p><p></p><p>图 8：一档关于 “铅笔” 一词起源的电视节目，以庆祝德国知名铅笔制造商 Faber-Castell 成立 250 周年。</p><p></p><p>尽管我们假设的排名故事以遗憾暂告段落，但它深刻揭示了一个核心真理：获得并维持高排名，绝非仅凭出色的文档或高效的 SEO 策略就能一蹴而就。</p><p></p><p>排名是一个多因素交织的复杂结果，它受到搜索行为波动、新文档信号的融入以及外部环境不断变化等多重影响。因此，认识到高质量文档与优化的 SEO 策略仅是排名动态系统中的一环，且至关重要，这一点尤为重要。</p><p></p><p>搜索结果的生成过程犹如精密的机械运作，背后涉及数以千计的信号和复杂算法。SearchLab 通过 Twiddler 进行的实时测试，甚至可能间接影响到文档的反向链接权重，从而引发连锁反应。</p><p></p><p>这些文档的命运可能因此发生转折，它们可能被从 HiveMind 这一核心存储系统迁移到优先级较低的存储层级，如 SSD 或 TeraGoogle，这一变动将直接削弱或消除它们对排名的正面影响，即便文档内容本身并未有丝毫改动。</p><p></p><p>谷歌的 John Mueller 曾明确指出，排名的下滑并不总是意味着你的策略有误。用户行为模式的转变、新兴趋势的涌现或是其他外部因素，都可能成为影响排名表现的不确定因素。</p><p></p><p>比如，当用户开始倾向于寻求更详尽的信息或偏好简短明了的文本时，NavBoost 系统便会迅速响应，调整排名以匹配这些新的搜索偏好。然而，值得注意的是，这种调整并不会触动 Alexandria 系统或 Ascorer 中的 IR 评分，后者更多地是基于文档本身的固有质量进行评估。</p><p></p><p>这一切都向我们传达了一个重要启示：SEO 工作应当置于更广阔的视角下进行审视。如果文档内容与用户的搜索意图存在偏差，那么即便是再精妙的标题优化或内容调整，其效果也会大打折扣。</p><p></p><p>更为关键的是，Twiddler 和 NavBoost 等系统对排名的干预力度，往往超越了传统的页面优化手段，包括页面内、页面上以及页面外的优化措施。一旦这些系统对文档的可见性进行了限制，那么无论我们在页面上如何努力优化，都可能难以扭转乾坤。</p><p></p><p>但请放心，我们的故事并不会就此陷入低谷。关于铅笔的电视节目效应终究只是短暂的喧嚣。随着搜索热度的逐渐退却，FreshnessNode 的临时影响也将烟消云散，我们的排名有望重新回升至第五位。</p><p></p><p>当我们重新开始收集点击数据时，根据 SISTRIX 的 Johannes Beus 的预测，第五位的平均点击率（CTR）大约在 4% 左右。只要我们能够稳定保持这一 CTR 水平，我们就有信心继续稳坐前十的宝座。未来可期，一切都将朝着更好的方向发展。</p><p></p><p>SEO 的关键要点</p><p></p><p>流量来源多元化：确保你的网站流量不仅仅依赖于搜索引擎，而是从多种渠道汇聚而来，包括社交媒体平台等非传统渠道，这些都能带来宝贵的访问量。即便谷歌的爬虫无法触及某些页面，它依然能通过 Chrome 浏览器或直接 URL 追踪到你的网站访客数量。强化品牌与域名认知：不断提升你的品牌或域名知名度至关重要。品牌越为人熟知，用户在搜索结果中点击你网站的几率就越大。通过优化针对多种长尾关键词的排名，可以有效提升域名的可见度。据透露，“站点权威性” 可能是影响排名的一个关键因素，因此增强品牌声誉对提升搜索排名大有裨益。深入理解搜索意图：为了更好地满足访客需求，深刻理解他们的搜索意图及路径至关重要。利用 Semrush、SimilarWeb 等工具分析访客来源及其行为，审视这些域名是否提供了你页面所缺失的信息，并据此逐步补充，使你的网站成为访客搜索路径上的 “终极目的地”。谷歌能够追踪相关搜索会话，精准把握搜索者的需求与历史。优化标题与描述，提升点击率：审视并调整当前标题与描述的吸引力，通过大写关键词汇使其在视觉上更为突出，可能有助于提高点击率。标题在决定页面排名中扮演关键角色，因此应优先考虑其优化。评估隐藏内容效果：若采用手风琴等形式隐藏重要内容，需留意这些页面的跳出率是否偏高。当访问者无法迅速定位所需信息，需多次点击时，可能产生负面点击信号。精简无效页面：对于长期无人问津或排名不佳的页面，应考虑删除，以避免对邻近页面造成不利影响。新文档若发布在 “劣质” 页面群组中，其表现机会将大打折扣。“deltaPageQuality” 指标用于衡量域名或页面集群中单个文档的质量差异。优化页面布局：清晰的页面结构、流畅的导航以及令人印象深刻的首页设计，对于跻身排名前列至关重要，这往往得益于 NavBoost 等系统的助力。增强用户互动：延长访客在网站上的停留时间，能发出积极的域名信号，惠及所有子页面。致力于成为访客的 “一站式” 信息源，提供全面信息，减少其他搜索需求。深化而非泛化内容：更新并丰富现有内容往往比不断创建新内容更为有效。“ContentEffortScore” 评估文档创作难度，高质量图片、视频、工具及独特内容均对此有正面贡献。标题与内容一致：确保标题准确概括后续内容，利用文本向量化等先进技术进行主题分析，较单纯词汇匹配更为精准地判断标题与内容的一致性。利用网页分析工具：借助谷歌 Analytics 等工具，有效追踪访客互动情况，及时发现问题并予以解决。特别关注跳出率，若异常偏高，需深入调查原因并采取措施改善。谷歌通过 Chrome 浏览器获取这些数据，实现深度分析。聚焦低竞争关键词：初期可优先针对竞争较小的关键词进行优化，更易于建立正面用户信号。构建高质量反向链接：重视来自 HiveMind 中最新或高流量页面的链接，因其传递的信号价值更高。避免链接至流量稀少或参与度低的页面。同时，来自同国别且内容相关的反向链接更具优势。警惕 “有毒” 反向链接，以免损害评分。关注链接上下文：在评估链接价值时，不仅要考虑锚文本本身，还需关注其前后文本的自然流畅性。避免使用 “点击这里” 等通用短语，因其效果已被证实不佳。理性看待 Disavow 工具：该工具用于屏蔽不良链接，但据泄露信息显示，它并未被算法直接采用，更多用于文档管理和反垃圾邮件工作。强调作者专业性：若使用作者引用功能，应确保其在外界享有良好声誉并具备专业知识。少数高资质作者往往优于众多低信誉作者。谷歌能根据作者的专业知识评估内容质量，区分专家与非专家。创作独特、实用、全面的内容：对关键页面尤为重要，展现你的专业深度，并提供有力证据支持。尽管可以聘请外部人员填充内容，但若缺乏实质质量和专业知识支撑，则难以企及高排名目标。</p><p></p><p>原文链接：</p><p></p><p><a href="https://searchengineland.com/how-google-search-ranking-works-445141">https://searchengineland.com/how-google-search-ranking-works-445141</a>"</p><p></p><p>声明：本文为 InfoQ 翻译，未经许可禁止转载。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</id>
            <title>AICon 全球人工智能开发与应用大会参会有感</title>
            <link>https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ba2e971f73b94be4959ea71a1</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 14:58:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>目录</h1><p></p><p>引言大会背景大会议程参会体验会后感想结束语</p><p></p><h1>引言</h1><p></p><p></p><blockquote>在数字化浪潮席卷全球的今天，人工智能开发与应用已成为推动社会进步和产业升级的关键力量。作为一名对AI技术非常感兴趣的开发者，在 8 月 18 日至 19 日这两天，我有幸参加了由极客邦科技旗下 InfoQ 中国主办的 AICon 全球人工智能开发与应用大会，个人觉得这场盛会就如同科技领域的璀璨星辰，吸引了众多行业精英和技术开发者齐聚上海，共同见证人工智能相关的又一盛会。在人工智能的浪潮中， AICon 全球人工智能开发与应用大会无疑是技术领域的一次盛会，在这次大会上我不仅了解到了AI技术的最新发展，还深入了解了AI在各行各业的实际应用。在这篇文章中，将分享我的参会体验和AI技术的前沿动态，以及它如何影响我们的未来。</blockquote><p></p><p><img src="https://static001.geekbang.org/infoq/82/821d9837ddc41f82cdbd724def5f7f49.png" /></p><p></p><h1>大会背景</h1><p></p><p>先来了解一下本次大会的背景， AICon 全球人工智能开发与应用大会是一个专注于AI技术的国际性盛会，是由极客邦科技旗下InfoQ中国主办的技术盛会，旨在为各行业的AI技术爱好者提供一个交流和学习的平台。大会聚集了来自世界各地的AI领域专家、学者、企业家以及开发者，共同探讨AI技术的发展趋势、应用实践和未来挑战。</p><p><img src="https://static001.geekbang.org/infoq/3b/3b729ea10567776ece148fd9ba4162c6.png" /></p><p></p><h1>大会议程</h1><p></p><p>再来分享一下本次大会的议程，大会的议程丰富多样，涵盖了AI领域的多个方面，具体如下所示：</p><p>AI基础理论研究机器学习算法创新深度学习与神经网络自然语言处理（NLP）计算机视觉AI在金融、医疗、教育等行业的应用AI伦理与法规</p><p><img src="https://static001.geekbang.org/infoq/43/43250663d8b5e92ebe4c17ceeeff1679.png" /></p><p></p><h1>参会体验</h1><p></p><p>当我踏入大会现场的那一刻，便被现场的氛围所感染，会场内人头攒动，来自各个领域的技术开发者怀揣着对人工智能的热情和期待，交流着彼此的见解和经验，作为一名开发者，我对AI技术的应用实践和技术创新特别感兴趣。</p><p><img src="https://static001.geekbang.org/infoq/16/16db377a7be1e0c51b810714f6638633.png" /></p><p>大会的第一天，众多顶尖企业与研究机构的资深专家纷纷登台，分享了他们在人工智能领域的最新研究成果和实践经验：</p><p>来自字节跳动的专家深入探讨了人工智能在内容推荐系统中的应用，他们通过巧妙地运用深度学习算法，实现了对用户兴趣的精准预测，从而为用户提供了更加个性化、精准的内容推荐。华为的专家则分享了他们在人工智能与 5G 通信技术融合方面的研究成果。通过利用人工智能的智能优化算法，实现了对 5G 网络资源的高效分配和管理，大大提升了网络的性能和覆盖范围。这让我看到了人工智能在通信领域的广阔应用前景，以及其对推动整个通信行业发展的巨大潜力。阿里巴巴的专家带来了关于人工智能在电商领域的创新应用案例。他们利用图像识别和自然语言处理技术，实现了商品的智能识别和搜索推荐，极大地提高了消费者的购物效率和体验。这使我明白了人工智能在电商行业的深度融合，能够为企业带来显著的竞争优势和业务增长。微软亚洲研究院的专家展示了他们在人工智能基础研究方面的最新突破，特别是在强化学习和生成对抗网络方面的研究成果。这些前沿的研究为人工智能的未来发展提供了坚实的理论基础和技术支持，让我对人工智能的未来充满了信心。智源研究院的专家分享了关于人工智能伦理和社会影响的思考。他们强调了在人工智能快速发展的背景下，我们需要关注技术带来的伦理问题，如算法偏见、数据隐私等，并积极探索相应的解决方案。这让我意识到，人工智能的发展不仅要追求技术的进步，还要注重其对社会和人类的影响，确保技术的发展是有益和可持续的。上海人工智能实验室的专家介绍了他们在城市智能管理方面的应用实践。通过利用人工智能技术，实现了对城市交通、环境、能源等方面的智能监测和优化管理，提升了城市的运行效率和居民的生活质量。这让我看到了人工智能在改善城市生活方面的巨大潜力，以及其对未来智慧城市建设的重要意义。蔚来汽车的专家分享了人工智能在自动驾驶领域的最新进展。他们通过融合多种传感器数据和深度学习算法，实现了车辆的自动驾驶和智能决策，为未来的出行方式带来了革命性的变革。这使我对自动驾驶的未来充满了期待，同时也让我认识到在实现自动驾驶的过程中，还需要解决许多技术和法律方面的挑战。小红书的专家讲述了他们在内容创作和社交互动方面的人工智能应用。通过利用图像生成和自然语言处理技术，为用户提供了更加丰富和有趣的内容创作工具，同时也提升了用户之间的社交互动体验。这让我感受到了人工智能在社交媒体领域的创新应用，以及其对用户参与和内容传播的积极影响。零一万物的专家展示了他们在人工智能芯片研发方面的最新成果。他们研发的高性能人工智能芯片，为人工智能算法的高效运行提供了强大的硬件支持，大大提升了人工智能系统的性能和效率。这让我认识到硬件的创新对于推动人工智能发展的重要性，以及芯片研发在人工智能产业链中的关键地位。</p><p><img src="https://static001.geekbang.org/infoq/df/dfb26b0b6ef3d95ce77c6aa61122e017.png" /></p><p>在第一天的会议中，我不仅了解到了人工智能在各个领域的最新应用和技术突破，还深刻感受到了人工智能技术的快速发展和广泛应用给我们的生活和社会带来的巨大变革。同时，我也意识到在人工智能的发展过程中，我们需要关注技术的伦理和社会影响，确保技术的发展是有益和可持续的。在大会上，我有机会听到了来自业界领袖的精彩演讲，参与了深入的技术研讨会，还与来自不同领域的专家进行了交流，这让我深刻认识到，人工智能不仅能够提升用户体验，还能为企业创造巨大的商业价值。</p><p><img src="https://static001.geekbang.org/infoq/61/61239e1fc2ae2cad5a8257afd22bfd3e.png" /></p><p>大会的第二天，这一天的分享更加侧重于人工智能的产业化和商业化动态，以及在实际落地场景中的挑战和解决方案，同样的，来自不同企业的专家们分享了他们在将人工智能技术从实验室推向市场的过程中所面临的困难和挑战：</p><p>数据质量和数据标注的问题成为了许多企业共同面临的难题。高质量的数据对于训练有效的人工智能模型至关重要，但获取、清洗和标注大量的数据需要耗费大量的时间和资源。此外，模型的可解释性和透明度也是一个亟待解决的问题，尤其是在一些对安全性和可靠性要求较高的领域，如医疗和金融。在产业化方面，一些企业分享了他们如何构建人工智能团队和建立有效的研发流程。他们强调了跨学科合作的重要性，包括数据科学家、工程师、产品经理和业务专家之间的紧密协作。同时，建立敏捷的开发流程和持续的优化机制也是确保项目成功的关键因素。在商业化方面，专家们探讨了如何将人工智能技术转化为实际的商业价值。他们分享了一些成功的案例，如通过人工智能优化供应链管理，降低成本并提高效率；利用人工智能进行精准营销，提高客户满意度和销售额。同时，他们也提到了在商业推广过程中面临的挑战，如客户对新技术的接受程度、法律法规的限制等。</p><p><img src="https://static001.geekbang.org/infoq/44/44993ae13f9e8fa1711dfed9fd9a160a.png" /></p><p>除了主题演讲和案例分享，大会还设置了互动环节和小组讨论。在这些环节中，我有机会与其他参会者深入交流，分享彼此的经验和见解。我结识了许多来自不同行业的技术大佬，与他们的交流让我拓宽了视野，获得了许多新的思路和灵感。</p><p></p><p>尤其是在小组讨论中，我们围绕着“人工智能在医疗领域的应用与挑战”这一话题展开了热烈的讨论。医疗行业对准确性和安全性的要求极高，因此在应用人工智能技术时需要格外谨慎，我们交流了如何确保人工智能诊断系统的准确性和可靠性，如何解决数据隐私问题，以及如何让医疗机构和患者更好地接受和信任人工智能技术。通过参与这些互动环节，我不仅加深了对会议内容的理解，还建立了宝贵的人脉资源。这些人脉将为我未来在人工智能领域的学习和工作提供有力的支持。</p><p><img src="https://static001.geekbang.org/infoq/24/24e09697cc3734c2e7516c60de090c31.png" /></p><p></p><h1>会后感想</h1><p></p><p>回顾这两天的参会经历，我深感收获颇丰，个人觉得AICon 全球人工智能与机器学习技术大会不仅是一个技术交流的平台，更是一个激发创新思维、促进合作的机会，我不仅接触到了最前沿的技术动态，了解到了行业的发展趋势，同时也结识了许多志同道合的朋友。</p><p>人工智能作为引领未来的关键技术，将继续在各个领域发挥重要作用，我相信通过不断的学习和实践，我们能够更好地驾驭这一强大的技术，为人类创造更美好的生活。</p><p>在今后的工作中，我将把在大会上学到的知识和经验应用到实际项目开发中，然后不断探索和创新，而且我也将继续关注人工智能领域的发展动态，积极参与相关的技术交流活动，与其他开发者共同成长，争取在人工智能领域占一席地！</p><p><img src="https://static001.geekbang.org/infoq/50/50c413f2f4bd966b662e60320025949b.png" /></p><p></p><h1>结束语</h1><p></p><p>参加 AICon 全球人工智能开发与应用大会是一次宝贵的学习经历，我不仅学到了对AI技术的新的认识，还对AI的未来发展有了更深的理解。AI技术正以前所未有的速度改变着世界，作为开发者，我们需要不断学习新技术，探索新应用，从而推动AI技术的健康发展。在AI的征途上，我们既是探索者，也是建设者。让我们携手共进，用AI技术创造更美好的未来。最后，我要感谢极客邦科技旗下 InfoQ 中国主办了这样一场精彩的大会，为开发者们提供了如此宝贵的学习和交流机会，非常期待下一次的 AICon 大会能够带来更多惊喜！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</id>
            <title>《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</title>
            <link>https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5Jx040lfFB5d1ioIRtK5</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><h1>《黑神话：悟空》太狠了！Steam 瘫痪、多家公司放假，英伟达老黄又要躺在新的印钞机上数钱了</h1><p></p><p><img src="https://static001.geekbang.org/wechat/images/66/66d8817eb95f472a457ad1cc44627f34.png" /></p><p></p><p>作者｜小褚、华卫、冬梅</p><p></p><p></p><h1>挤爆 Steam 服务器，多家公司给员工放假玩游戏</h1><p></p><p></p><p>8 月 20 日，互联网上似乎所有的热搜都集中在了《黑神话：悟空》上线这件事上。</p><p></p><p>上午 10 点，备受期待的游戏《黑神话：悟空》正式上线，然而，一瞬间挤进上百万人后，这款游戏背后的服务器 Steam 不堪重负，遭遇了短暂的崩溃。</p><p></p><p>众多玩家进入游戏时遭遇了阻碍，无法顺利启动游戏。有玩家在社交媒体上调侃称：“服务器爆了，Steam 好久没被干爆过了吧。”幸运的是，这一问题在大约十分钟内得到了解决，玩家得以继续他们的游戏体验。</p><p></p><p>网友纷纷摩拳擦掌时，却遭遇“漫漫解压路”：“八十一难第一难，开始解压”“比下载时间都长”“解压打断了我的大圣梦”……但依旧挡不住大家的热情。</p><p></p><p>据悉，《黑神话：悟空》的上线人数位居 Steam 同时在线人数历史第四位，这一人数仅次于 PUBG、幻兽帕鲁、CSGO。</p><p></p><p>此外，游戏玩家社区平台小黑盒也报告了崩溃情况。许多玩家通过小黑盒购买了《黑神话：悟空》，此次崩溃事件可能对他们的购买体验造成了一定影响。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b4d8cbe00ff2dc1ab411271afdf4cf48.png" /></p><p></p><p>《黑神话：悟空》到底火到什么程度？在游戏上线首日，甚至出现了不少公司给员工放假去体验游戏的情况。</p><p></p><p>四川木子杨科技有限公司 8 月 19 日发布通知，决定在《黑神话：悟空》上线的当天 8 月 20 日给全体员工放假一天，让员工尽情体验《黑神话：悟空》带来的视觉盛宴和游戏乐趣，与同事、朋友一起分享这款国产大作的精彩瞬间。公司表示，这次放假是对国产游戏行业的一份支持。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0f/0ff0c0cf10df7f6ed900cd7e1ce9a13c.png" /></p><p></p><p>游戏发行商 Gamera Game 宣布 8 月 20 日放假，公司还表示，“为了避免各位同事因临时暂停工作而感到不知所措，将送给每位同事一份《黑神话：悟空》数字豪华版”。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b4ff5439e76c0872e760b1c61ab1816b.png" /></p><p></p><p>行业媒体“游戏茶馆”也公告，为让员工们更好地体验《黑神话：悟空》，决定 8 月 20 日放假半天，同时为大家提供一份报销该游戏费用的机会。</p><p></p><p>“作为苦逼打工人，也是靠加班赚来的年假 + 病假凑足的请假时间来体验《黑神话：悟空》，我的初体验是‘绝对值得’”有网友评价道。“要不是我现在已经是一个成熟稳重分得清轻重缓急的成年人，我现在也去玩游戏了！”还有网友提到。正式上线后的《黑神话：悟空》好评如潮。</p><p></p><p>那么，这款游戏为何能火到如此程度？</p><p></p><p></p><h1>《黑神话：悟空》为啥这么火？</h1><p></p><p></p><p>4 年前的今天，也是 8 月 20 日，《黑神话：悟空》发出第一条宣传预告，自此之后这一游戏的热度便一直居高不下。据 Steam 商店的官方介绍，《黑神话：悟空》是由游戏科学制作的以中国神话为背景的动作角色扮演游戏，游戏中玩家将扮演一位“天命人”，为了探寻昔日传说的真相，踏上一条充满危险与惊奇的西游之路。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8b/8bdfef422204c751f50d7ebaa5dba588.png" /></p><p></p><p>今年 6 月 10 日，《黑神话：悟空》实体版正式开启全款预售。Steam 中国大陆区标准版售价 268 元，数字豪华版售价 328 元。目前这款游戏的全球媒体评分已解禁，IGN 中国给予 10 分评价、IGN 海外给予 8 分评价。截至 8 月 17 日凌晨，52 家全球媒体平均给出了 82 分的评价。</p><p></p><p>而论《黑神话：悟空》获得如此高市场反响的原因，首先不得不提的是西游记的 IP 加成，基于西游记而产生的影视作品层出不穷，最近一部的《大圣归来》也唤起了大家记忆中那个踏碎凌霄的齐天大圣，孙悟空更是许多人童年时候心目中的英雄。</p><p></p><p>其次便是同行的“衬托”了。《黑神话：悟空》并不是第一个做西游记 IP 的游戏，此前市面上有不少同类型的游戏，但其仅从宣传片画面便赢得了众多玩家对其的期待和青睐。据介绍，《黑神话：悟空》游戏里的取景，几乎都有现实存在的原型，能够让玩家感受到从游戏到现实的无缝切换。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/18/1863774968b2642250c7fb3d7e63bcff.gif" /></p><p></p><p>更重要的是，《黑神话：悟空》寄托了许多游戏玩家对第一个国产 3A 大作的希望。中国的游戏单机市场寂寥已久，一众玩家对于国内游戏的宽容度不低。有专业博主分析认为：“国人渴求中国文化背景的游戏，平心而论尽管国外一些游戏的制作水平比《黑神话：悟空》高，但它们的情感冲击绝对没有它强。”</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/8f/8f01473d8992120c88551acf3beb6761.gif" /></p><p></p><p>此外，自首个宣传片放出之日起，《黑神话：悟空》便不断对外同步演示视频，从开场动画到游戏表现形式、人物动作流畅度，玩家们都见证着其在这 4 年中的进步。</p><p></p><p>有市场分析显示，《黑神话：悟空》的预售表现超出预期，一个月内销量达到 120 万份，销售额近 4 亿元，最后的收入肯定更远不止如此。随着《黑神话：悟空》的终极 PV 发布，玩家纷纷喊话，“这次我要做自己的齐天大圣。”</p><p></p><p></p><h1>强大的英伟达 AI 作支撑</h1><p></p><p></p><p>《黑神话：悟空》基于虚幻引擎 5，并采用新的 RTX 技术。在 PC 端，《黑神话：悟空》的视觉效果经过全景光线追踪技术的增强，成为迄今为止发布的沉浸感更强、技术更先进的游戏之一。</p><p></p><p>为了迎接这款游戏到来，NVIDIA 英伟达上周宣布为《黑神话：悟空》推出 Game Ready 驱动，这是 NVIDIA 首次为中国游戏做专属优化。此次 Game Ready 驱动重点是优化 RTX 40 系列的性能。</p><p></p><p>全景光线追踪技术对硬件要求更高，但可以高度准确地渲染光线及其在场景中的效果。这种先进的光线追踪技术也被称为路径追踪，视觉效果艺术家们可利用它打造以假乱真的电影和电视画面。《黑神话：悟空》中的全景光线追踪技术提高了光照、反射和阴影的保真度和质量。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/cb/cb57c378272f3c49a7ea25b06ac1cfc2.png" /></p><p></p><p>借助多次反射光线追踪间接照明，自然色彩光线可反射多达两次，营造出更逼真的间接光照和遮蔽效果。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/45/456d955536ab06bc197a9a92f84a40c4.png" /></p><p></p><p>为了提高质量并增强沉浸感，特效通常包含大量的单体粒子，而使用传统光线追踪方法会对性能带来巨大的压力。《黑神话：悟空》采用一种新技术，使用两级光线追踪为大量面片系统进行画序无关的透明渲染，从而在实时反射中高效地渲染游戏的粒子系统。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/44/44821f2a0b7af3641af8abcf9efdbc9c.png" /></p><p></p><p>要获得此效果，玩家可以在游戏中开启“Full Ray Tracing (全景光线追踪) ”设置，然后将“Full Ray Tracing Level (全景光线追踪水平)”设置为“Very High"。</p><p></p><p>NVIDIA DLSS 是英伟达的 AI 渲染技术，可通过 GeForce RTX GPU 上的专用 Tensor Core AI 处理器提高游戏和应用的图形性能。</p><p></p><p>在《黑神话：悟空》中，通过支持全景光线追踪技术并将每项设置为最大值，DLSS 3 可带来性能的成倍提升。这使得 GeForce RTX 4080 SUPER 的用户能够在《黑神话：悟空 》的基准测试中达到每秒近 74 帧的帧率。此外，GeForce RTX 4070 Ti SUPER 的用户可以在每秒 66 帧的帧率下享受 4K 的乐趣。</p><p></p><p>此前，官方公布的推荐配置显示，玩家在 1080P 高画质下需要至少 RTX 2060 或 RX 5700XT 级别的显卡，而中画质则可选择 GTX 1060 或 RX 580。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b9/b91017444854a18dc9b73e2b95f3b8f2.png" /></p><p></p><p>值得注意的是，这些配置均是在启用了 DLSS/FSR/Xess 等图形优化技术的前提下给出的。若玩家希望体验原生 1080P/ 高画质的游戏效果，可能需要具备更高性能的显卡，如 RTX 3060 及以上。目前已经有玩家表示，3060 使用起来还会有卡顿出现。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/55/5506b4d3713356c2c6d8a6bd02db2719.png" /></p><p></p><p>此外，微星、技嘉、七彩虹、索泰、影驰、映众、万丽、耕升等企业陆续推出了与《黑神话：悟空》联名的显卡产品。</p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/400hg9QcIGebLvg0BdbV</id>
            <title>AI 与大模型如何助力金融研发效能最大化？</title>
            <link>https://www.infoq.cn/article/400hg9QcIGebLvg0BdbV</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/400hg9QcIGebLvg0BdbV</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 05:58:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>在金融行业，技术创新与严格合规的需求并行存在，推动着研发团队不断寻求更高效的解决方案。面对日益增长的市场竞争和技术进步，金融机构必须迅速适应变化，同时确保所有创新措施都符合监管要求。这种需求催生了对高效研发流程和先进技术应用的追求。</p><p></p><p>在日前的 InfoQ《超级连麦. 数智大脑》x FCon 直播中，我们邀请到 微众银行研发效能负责人余伟， 以及 数势科技数据智能产品总经理岑润哲，深入探讨了在金融研发中提升效能的有效策略，如何选择合适的技术栈和架构设计，以及如何利用 AI、大模型和低代码等技术优化研发流程、加速产品交付。</p><p></p><p>8 月 16-17 日，FCon 全球金融科技大会于上海举办，2 位老师在 「金融研发效能提升路径与实践」专题论坛中与大家进行了深入的交流和分享。点击链接可获取PPT下载链接：<a href="https://ppt.infoq.cn/list/149">https://ppt.infoq.cn/list/149</a>"</p><p></p><p>以下内容根据对话整理，篇幅有删减：</p><p></p><h3>金融研发效能提升：关键因素与平衡策略</h3><p></p><p></p><p>余伟：金融行业在提升研发效能方面哪些因素最为关键，以及如何平衡这些因素？</p><p></p><p>岑润哲： 在金融机构的项目研发过程中，我认为有三个关键因素对提升研发效率有着显著影响。首先，也是最重要的，是人才素质。一个团队如果拥有既懂金融知识又精通技术和产品的复合型人才，那么无论是项目推广还是产品研发都将更加迅速和敏捷。这样的人才能够作为产品团队和研发团队之间的桥梁，将复杂的业务逻辑转化为技术语言，从而极大提升研发迭代和测试的效率。</p><p></p><p>其次，技术架构的选择至关重要。金融机构需要处理大量数据并应对高并发情况，因此，一个先进的技术架构能够确保系统在面对未来数据量增长时仍能稳定运行。在项目初期，预判未来几年内数据量的增长，并以此为基础设计架构，可以避免未来需要进行系统重构的情况，从而节省时间和资源。</p><p></p><p>第三，流程管控和项目管理同样关键。一个有效的项目管理机制，如敏捷开发的 Scrum 框架，以及一个负责任的项目经理，能够确保产品的设计和开发过程更加高效。项目经理可以引入短期迭代和持续反馈机制，提高资源利用效率，加强产品与技术人员的协作。</p><p></p><p>余伟： 在微众银行，有一个岗位，称为 科技产品经理，他们既懂技术又懂产品。这个角色在金融科技领域尤为重要，因为他们需要与业务团队紧密合作，理解业务需求并将其转化为技术解决方案。业务团队可能清楚自己需要什么，但不一定知道如何通过技术手段来实现这些需求。科技产品经理不仅要了解业务团队的需求，还要与技术团队沟通，确定如何利用现有架构或进行架构调整来实现这些功能。</p><p></p><p>敏捷和持续迭代 的理念并非所有人都能一致理解。过去金融机构可能需要数月才能推出一个版本，但现在通过敏捷开发，迭代周期缩短至两周甚至每周。这种转变需要团队成员对敏捷开发有清晰的认识，并接受持续学习的文化。此外，明确的责任划分和系统架构设计对团队的敏捷落地至关重要。</p><p></p><p>流程工具的自动化和基础建设，如持续集成 / 持续部署（CI/CD）、容器技术、测试环境和泳道，尤其是金融机构复杂的测试环境，以及与 AI 结合的自动化输出，对提升组织效率极为重要。</p><p></p><p>IT 能力的共享也是金融机构提高效率的关键。例如，一个风险模型可能在一个产品中得到应用，也可以作为公共服务被其他产品共享。这种能力共享对于避免资源浪费和提升研发效能至关重要。</p><p></p><p>金融机构与其他机构最大的区别在于风险管控和合规性。风险管控和合规性要求对研发效能有反向作用，高风险管控意味着需要做更多的工作来解决或降低风险，可能会影响研发效率。因此，在设计架构和产品时，需要提前考虑风险管理和监管要求，避免后期返工带来的损失。</p><p></p><p>敏捷文化、流程工具自动化、IT 能力共享以及风险管控和合规性，这四点对金融机构的研发项目集成有着显著影响，需要在实践中不断平衡和优化。</p><p></p><p>岑润哲： 在金融机构中，IT 能力的共享至关重要，特别是在数据分析领域。以我们数势科技的指标平台 SwiftMetrics 为例，这些产品的核心目标是帮助金融机构以更低的门槛进行数据分析。数据分析本身是可以在不同业务线之间共享的，例如风控模型在信用卡场景中有效，在保险代销或财务管理中可能就不再适用。我们希望我们的产品能够在金融机构的各个业务线中低成本复用，从而减少冗余开发和烟囱式架构的成本。</p><p></p><p>监管在金融机构中扮演着特殊的角色。在监管严格的环境下，金融机构可能不会尝试使用 AI 或大模型进行深入探索。例如，在风控领域，模型的可解释性至关重要，因为需要向借款人解释为何被拒贷或额度设置的原因，这就是为什么许多银行和金融机构仍然使用决策树，尽管它们可能不是最优模型，但具备可解释性。</p><p></p><p>在金融机构，尤其是监管严格的业务领域，先进技术的使用需要谨慎，因为风险合规的存在。如何在大模型的创造性和监管的规则性之间找到平衡，是未来需要重点关注的挑战。</p><p></p><h3>利用 AI、大模型与低代码，优化研发流程与加速产品交付</h3><p></p><p></p><p>余伟： 在银行业务中，用户与资金和征信相关的事务至关重要，准确性是关键，任何微小的差错都不允许。在这样的背景下，我们接下来要讨论的是 AI 与低代码技术在金融研发中的应用，这些技术是如何具体应用在金融研发中的，以及它们是如何帮助提升研发效能的？</p><p></p><p>岑润哲：AI 技术在金融领域的应用已经相当广泛，从智能客服、人脸识别、OCR 到风控等。随着大模型技术的发展，金融机构正在探索更深层次的应用场景。</p><p>数据分析：金融机构拥有庞大的数据资产，数据分析成为关键应用之一。传统上，业务方提出报表需求，数据团队进行开发，这不仅效率低下，而且成本高昂。数势科技公司擅长数据分析，通过结合大模型和指标语义层，使业务人员能够用自然语言进行数据分析，从而提高效率并减少数据团队的冗余开发工作。智能客服：在大模型技术的支持下，智能客服可以提供更高效的服务。金融机构拥有大量政策和产品文档，通过大模型的自然语言处理能力，可以提升客服的响应速度和质量，甚至构建完全自动化的智能客服系统。知识库管理：金融机构作为知识密集型行业，需要有效管理和调用历史知识文档。大模型可以帮助将非结构化文档转化为向量数据库中的信息，供业务方和客户经理使用。但这也带来了挑战，如文档版本管理和观点差异，可能导致大模型输出的不稳定性。代码辅助：在研发领域，大模型可以辅助编写代码，提供基础代码框架供开发人员修改和完善。此外，大模型还能将代码转换为流程图或泳道图，帮助产品经理理解代码逻辑，从而提高产品和研发团队之间的沟通效率。</p><p></p><p>余伟： 在微众银行，我们对数据分析的实践采取了一种新的方法，利用大模型技术来提高效率和精确度。例如，业务团队在策划营销活动时，可能需要了解在深圳存款超过 1000 元的人数。过去，这需要向科技团队提出需求，由他们编写 SQL 查询并开发相应的页面功能，这个过程耗时长且需要排期。</p><p></p><p>现在，我们通过大模型技术，业务团队能够直接与模型交互。他们可以向模型描述所需的条件和营销活动的目的，模型会生成类似 SQL 的查询结果。这些结果首先在准生产环境中进行基准分析，确保它们符合业务团队的预期和产品理解。如果结果符合预期，再将其转化为正式的需求，交由科技部门开发。</p><p></p><p>这种方法解决了几个问题：首先，它减少了业务团队因需求不明确而导致的资源浪费；其次，它允许业务团队在小范围内低成本尝试，验证产品特性的可行性；最后，它确保了业务团队的需求与科技团队实现的功能之间有良好的匹配。</p><p></p><p>岑润哲： 大模型在生成代码方面的能力确实为快速原型开发提供了显著优势。以往，业务方提出需求后，设计师需要花费一两天时间来设计图纸，而现在，通过大模型，我们可以迅速提炼出多个版本的设计方案。业务方可以直接从这些版本中挑选，这大大提升了产品开发和研究的效率。</p><p></p><p>无论是将文本转换为图像还是代码，大模型都具备这样的能力。尽管这些能力可能尚未达到完美，但在原型制作过程中，AI 技术的加速作用非常明显。这不仅加快了业务方的选择过程，还为 AB 测试等提供了便利，从而在整体上提高了业务效率。</p><p></p><p>余伟： 金融机构正在探索 AI 技术在多个方向的应用，包括利用 AI&nbsp;分析研究报告 来辅助投资顾问作出决策。研究报告来源广泛，有些容易获取，有些则相对分散。以往依赖有经验的分析师手动阅读和分析，但这种方法存在局限，尤其是对于缺乏经验的分析师，他们可能无法准确把握报告的重点。通过将这些报告交给 AI 大模型进行分析，可以生成标准化的输出，确保分析的全面性和准确性。</p><p></p><p>智能投资顾问也是 AI 技术应用的一个重要领域。通过算法，智能投资顾问能够提供个性化的投资建议和投资组合，满足不同客户的长期或短期需求。微众银行的 App 已经集成了这样的智能投资顾问服务，为客户提供定制化的产品推荐。</p><p></p><p>除了 AI 技术，低代码开发平台 也在金融机构中找到了其应用场景。低代码平台允许用户通过少量编码或零编码来快速构建功能，特别适用于产品管理台的开发。金融机构的每个产品都需要后台管理台，业务人员可以在管理台上执行查询、审批等操作。通过低代码平台，可以快速生成这些标准化功能，并与应用系统集成，实现快速开发。在移动端应用开发方面，低代码工具同样发挥着重要作用。利用 iOS 和 Android 平台提供的组件，低代码工具可以快速开发出功能原型，特别是在产品初期的体验版或演示版开发中。这种方式允许业务团队在没有具体数据支持的情况下，快速验证移动端功能的表现是否符合预期。</p><p></p><p>岑润哲： 在与多家金融机构的接触中，我发现他们已经采购了不少低代码工具，用于风控策略、营销策略等业务场景。低代码工具通常基于工作流（workflow）进行编辑，允许用户通过拖拽组件的方式快速构建应用，如 H5 活动页面。这些工具虽然功能强大，对业务方来说学习门槛仍然存在。一些金融机构的业务人员在使用这些工具时可能会感到复杂，这影响了工具的普及和应用效率。余老师如何看待这个问题？</p><p></p><p>余伟： 低代码平台在金融机构中的应用主要面向两类用户：一是非技术背景的业务人员，二是具备一定代码能力的科技同事。</p><p></p><p>对于业务人员而言，他们通常对代码不太了解，只有产品概念，因此学习使用低代码平台确实存在门槛。为了降低这个门槛，我们需要在易用性和可用性上做出更多努力，使得用户能够像使用普通互联网产品一样，通过简单的拖拉拽操作快速实现所需功能，而无需经过复杂的培训过程。</p><p></p><p>金融机构中使用低代码平台的主要是科技同事，他们利用这个工具来提升研发效率。低代码平台使得需求提出者能够快速看到接近最终形态的产品，从而更准确地确认是否符合预期。这种模式的价值在于金融机构人员结构的特殊性：业务人员面对客户，收集需求并翻译成需求文档，然后交给科技人员实现。业务人员通常只在需求提出阶段和产品即将面向客户的阶段参与，因此 低代码平台在中间起到了桥梁作用，帮助快速实现并验证需求。</p><p></p><p>通过 低代码平台完全组装一个新产品，尤其是涉及前端页面和后端服务的复杂串联，目前还有一定难度。金融机构的产品通常包括前端展示和后端逻辑两部分，如果这些组件能够标准化，前端的页面组装和后端的微服务组装相对容易实现。但中间的逻辑串联过程往往需要人工参与，因此完全自动化的流程编排和快速上线尚未达到成熟阶段。</p><p></p><p>岑润哲： 去年，我们尝试将低代码工具和 API 交给大模型，希望它能进行编排调度。但很快我们发现，这一过程存在困难。Workflow 的编辑具有强烈的业务逻辑性，如果大模型不了解金融机构的业务逻辑，它就无法将这些 API 有效串联起来。</p><p></p><p>目前大模型在自动化生成后端代码，尤其是涉及复杂业务逻辑的代码方面，仍然面临挑战。这需要一个既懂业务又懂技术的中间人来介入，无论是通过低代码平台还是传统编码方式，都需要这样的人来组装业务逻辑，以形成一个完整的产品并交付。</p><p></p><p>余伟： 接下来我们讨论下 如何优化从需求收集到产品交付的整个研发流程，以缩短交付周期并提高质量。</p><p></p><p>在传统的研发流程中，工作通常分为几个主要阶段：首先是需求收集，将需求转化为业务和技术人员都能理解的需求文档；接着是将业务需求转化为系统需求；然后是详细设计、编码、单元测试；最后是移交到系统集成测试（SIT）、用户验收测试（UAT）、回归测试，直至上线发布。</p><p></p><p>为了缩短这一流程，敏捷开发方法被广泛采用。通过将原本可能一个月的迭代周期缩短至两周，一些需求能够更快地上线。例如，某些需求可能仅需 3 天开发加 1 天测试，4 天后即可发布。而更复杂需求可能需要两周开发和一周测试，总共三周才能上线。通过这种方式，大的需求和小的需求可以搭配进行，使得整体的交付时间缩短。</p><p></p><p>在需求量保持不变的情况下，通过缩短交付周期，可以提高整体的研发效率。金融机构普遍采取这种策略，将交付周期进行合理划分，快速上线小需求，而大需求则按照正常流程处理。这对研发管理人员提出了更高的要求，他们需要能够识别哪些需求可以快速交付，并能够将人力资源有效分配到这些快速迭代的开发任务中。随着研发任务的细分，研发人员的管理也变得更加复杂。可能需要将团队分成多个小分队，每个小分队针对不同的需求进行快速响应和服务。</p><p></p><p>岑润哲： 在我们的公司内部，从需求收集、研发设计到单元测试、UAT 和回归测试的整个流程中，我认为 AI 可以在测试环节发挥重要作用。特别是在我们公司开发的 SwiftAgent 产品中，该产品允许用户通过自然语言进行数据分析。在这种情况下，测试的成功率至关重要，因为这是一个基于自然语言交互的产品，与图形用户界面（GUI）不同，它具有不确定性。</p><p></p><p>设计全面的测试用例集对于评估 AI 产品的使用效率和成功率非常关键。例如，在智能客服场景中，不同类型的问题回答率是否达到预期阈值，这就需要精心设计测试问题。AI 产品测试中，如何设计问题和持续优化不良案例是一个重要环节。</p><p></p><p>大模型可以通过提供测试问题来提高测试效率。测试人员可以基于示例问题，让大模型生成多种问法，从而测试 AI 产品的响应能力。例如，提供一个问题示例，大模型可以生成 10 种不同的问法，甚至 100 个问题，这样测试人员就不需要自己构思问题，从而显著提高了测试 AI 产品的效率。此外，大模型还能够通过举一反三的方式，帮助我们从不同角度评估 AI 产品的性能。这种基于大模型的测试方法不仅可以提升测试效率，还能够提高产品的成功率和召回率。</p><p></p><p>余伟：AI 产品的核心期望是输入特定信息后，通过大模型的处理得到正确且符合预期的输出结果。那么我们如何判断 AI 输出的有效性和合理性？</p><p></p><p>岑润哲： 我们的分析产品实质上采用了 Text to API 的逻辑。用户用自然语言表述请求，例如“请帮我分析一下近三个月的账单金额”，我们的系统不会直接将其转换成 SQL 查询语句，因为 SQL 逻辑相对复杂且可能不准确。我们的产品逻辑分为两个阶段。首先是语义理解阶段，系统需要识别用户输入的自然语言中的关键要素，如时间范围（1-3 个月）和指标（账单分析金额）。这可以通过脚本检查来验证大模型是否正确理解并翻译了用户的意图。</p><p></p><p>第二阶段是数据调用阶段，系统将识别出的关键要素转换成 API 调用所需的半结构化数据，如 JSON 格式。我们需要验证大模型生成的 JSON 或半结构化数据结构是否正确，因为这一步骤可能会有不稳定性。即便是先进的模型如 GPT-4，也可能在生成 JSON 时出现随机性或不稳定性。为了提高准确性，我们通过对齐方式来确保生成的数据结构尽可能符合 API 调用所需的 JSON 入参格式。这有助于后续的数据调用过程更加顺畅。</p><p></p><p>最近，GPT-4o 发布了一项新功能，即代码对齐能力，这意味着生成的 JSON 将严格符合语法规范。这一功能如果能够实现，将极大降低代码生成层面的幻觉问题。如果 GPT-4o 的这一新功能能够确保生成符合 API 入参的 JSON 结构，那么在准确性上就能达到 100%，从而提高产品在多种场景下的应用可行性。</p><p></p><p>在产品实现逻辑上，一个关键点是确保大模型不仅能理解用户的话，而且能 将其理解过程反馈给用户。例如，如果用户询问产品的产品经理是谁或产品的起购日期，如果大模型只给出直接答案，用户可能会怀疑其可信度。因此，展示大模型的思考过程，包括它是如何得到这个结果的，即所谓的"白盒化"，对于赢得用户信任至关重要。</p><p></p><p>在数据分析方面，一旦指标定义清楚，只要数据准确，通常不会有问题。但文档查询就更具挑战性，因为可能存在多个版本的文档，观点可能相互冲突。如果用户提出问题，而两个文档的观点相反，大模型可能无法做出判断。在这种情况下，将所有相关文档召回供用户选择可能是一种更好的方法。</p><p></p><p>这种“思维链”（Chain of Thought, COT）的透明化，使用户能够理解大模型是如何得出结论的，从而增加了对结果的信任。这与互联网企业中的推荐系统类似，用户不仅希望获得良好的推荐体验，还希望了解推荐背后的逻辑，尤其是在金融产品推荐中，这一点更为重要，因为金融产品涉及合规性和风险管理问题。例如，如果一个稳健型客户被推荐了一个高风险产品，这显然是不合规的。</p><p></p><p>大模型如果能够作为用户自然语言和技术语言之间的桥梁，将提升 AI 产品的可信度，使用户更愿意使用。确保 AI 具备足够的可解释性，是 AI 产品推广和普及的重要一步。这是 AI 产品能否被广泛接受和使用的关键因素。</p><p></p><p>余伟： 在微众银行，我们探索了多种方法来缩短从需求收集到产品交付的整个流程。我们从统一的研发节奏转变为根据需求和产品类型采用不同的研发节奏，以实现更高效的交互。</p><p></p><p>组织结构上，我们也在尝试引入解决方案层，特别针对 ToB 产品在与金融机构对接时可能遇到的非标准化、研发周期长的问题。如果没有解决方案层，产品直接与金融产品对接，就需要同时满足业务系统和金融机构产品的排期，这可能导致产品研发周期非常长。</p><p></p><p>解决方案层的引入，可以将对接工作分成两部分：一部分是与业务方的对接，另一部分是与银行科技团队的对接。这样的方法可以最小化双方的改动，而解决方案层则承担适配的工作。这种适配相比直接在成熟产品上进行调整要快速得多，从而大大缩短了 ToB 产品的交付流程。</p><p></p><p>当前，许多金融机构不仅做 ToC 业务，也开始大量涉足 ToB 业务，面临提高 ToB 产品交付效率的挑战。我们内部进行了许多讨论，集思广益寻求解决方案。</p><p></p><p>岑润哲： 数势科技为金融机构提供多种软件和产品服务。面对金融机构多样化的需求，我们意识到不能仅仅依赖标准产品。为了更敏捷地交付并满足定制化需求，我们通过组件化和配置化来提高服务的灵活性。这个关键在于构建一个能力中台，它允许我们通过组装和拼接组件来形成满足不同需求的产品。这样，即使面对不同机构的不同需求，我们也无需对 PaaS 层进行大量修改。通过 PaaS 层构建的服务可以被复用，为不同的金融机构提供定制化的解决方案。</p><p></p><p>避免“烟囱式”开发，避免每次开发都从头开始，导致代码重复且难以共享，对我们来说是至关重要的。中台能够高效地重用已有的软件组件和服务平台，从而减少重复工作，加快交付速度，并提升产品的质量和一致性。</p><p></p><p>余伟：研发团队的文化建设对于整个团队的运作至关重要。不同的角色在团队中承担着不同的职责。例如，在编写需求文档的过程中，有些团队可能由业务人员负责，而有些则由产品经理来完成。开发和测试人员在面对不完整的需求时，常常会抱怨需求描述过于简单或频繁变更，这增加了工作的复杂性。这种抱怨并不有助于研发流程的正向发展。每个团队成员虽然有明确的岗位职责，但在需求不完善的情况下，团队成员应更积极地参与到需求的快速转化中。</p><p></p><p>例如，在开发支付产品时，一些基本功能如代收、代付签约是支付渠道必须具备的，这些功能的变化范围有限，团队可以基于这些共通点提前开始开发工作。团队成员在面临依赖问题时，应主动沟通和规划。如果上游工作未能及时提供所需支持，团队成员应提前与依赖方沟通，明确自己的需求和计划，以便双方可以共同协商解决方案。</p><p></p><p>我们鼓励团队成员主动推动产品的落地和面向客户的进程。这种主动性体现在将产品视作自己的责任，展现出主人翁精神，不仅关注自己的任务，还考虑到如何帮助依赖方，甚至在提供支持之前，就给出建议和指导。</p><p></p><p>通过这些实践，我们的团队文化已经从严格的角色划分和等待依赖转变为更加主动和负责任的态度。团队成员开始将产品视为自己的一部分，积极推动产品的发展，并以更加开放和协作的方式与其他团队成员一起工作。这种文化的建设对于提高研发效率和产品质量具有重要作用，我对此深有感触，并且认为这对于团队的长期发展是非常有益的。</p><p></p><h3>打破部门壁垒，传统银行模式如何转向敏捷协同</h3><p></p><p></p><p>岑润哲： 余老师的分享非常中肯，尤其是在互联网银行领域，敏捷文化是其核心特质之一。然而，在与传统金融机构合作进行产品交付时，我们常会发现这些机构可能存在部门间的隔阂，即所谓的“部门墙”。这在一定程度上影响了敏捷迭代和协同工作的效率。对于 ToB 业务，尤其是面对那些可能没有充分采纳互联网思维的传统金融机构时，提升研发效率的关键在于如何打破这些障碍，实现敏捷协同，余老师在这方面有哪些经验可以分享吗？</p><p></p><p>余伟： 在我们团队中，我们经历了从传统银行模式向敏捷文化的转变。这种转变体现在团队成员开始主动与依赖方沟通，提前明确需求和交付时间，以及他们能为对方提供的支持。这种变化不仅提升了团队的交互效率，也使团队成员更加积极地参与到产品的整个生命周期中。我们通过以下几个方面来推动这种文化转变：</p><p>行业最佳实践分享：我们寻找行业内的优秀案例，与团队成员分享，让他们了解其他团队的成功经验，从而激发团队改进的动力。产品驱动：在新产品的研发初期，我们就提出高效率的要求，鼓励团队想象在没有历史包袱的情况下，如何实现更快速的交互和更高效的工作。文化驱动：通过在新产品和服务中尝试敏捷实践，影响那些固守传统研发模式的团队，促使他们认识到改变的必要性。角色融合：在一些团队中，我们尝试减少角色划分，让开发和测试人员承担更多职责，如需求拆解、架构设计、项目管理、培训等，从而提高团队的灵活性和效率。我们鼓励团队成员拓宽自己的职责范围，不再局限于单一角色，而是有机会尝试和体验不同的工作内容，这样不仅提升了个人能力，也为团队带来了新的视角和解决方案。我们在小范围内试验这些新的做法，目前已经取得了超出预期的效果。</p><p></p><p>岑润哲：在我们开发数据分析类的 AI 产品过程中，有时会发现技术团队，特别是算法团队，提出的产品想法可能比产品经理的更出色。这是因为算法人员对算法的潜力和局限有深刻的理解。即便他们提出的 10 个想法中有 9 个不可行，基于对算法底层逻辑的了解，他们仍有可能提出一个非常符合用户需求的产品设计理念。</p><p></p><p>在当前快速迭代的环境中，无论是产品经理、测试人员还是研发人员，如果能够横向扩展自己的能力，成为所谓的“T 型人才”或“π型人才”，即拥有深度专业技能的同时也具备广度的跨领域知识，确实能够提高团队的研发效率，并推动团队文化的发展。这样，测试人员能够更好地理解研发的工作内容，研发人员也能更准确地判断产品需求的合理性。每个成员不仅在自己的专业领域内精益求精，也能够对其他领域有所涉猎和理解，这种多元化的技能组合对于产品创新和问题解决都具有积极的影响。</p><p></p><p>余伟：在微众银行，我们的研发团队与业务团队建立了定期沟通的机制，比如每周或每两周举行一次会议。这些会议的特点是跨部门参与，不仅业务团队的成员会参加，科技团队的成员也会参与进来。</p><p></p><p>最初，这类会议主要由科技产品经理参与，目的是为了理解业务需求和业务目标。但随着时间推移，我们扩展了参与角色的多样性，包括一些核心开发人员也会参与会议。通过这种方式，开发人员能更直接地了解用户和业务的反馈，真实地看到自己开发的产品在业务落地时的效果，并思考在产品开发过程中如何做出改进以支持业务的更好发展。</p><p></p><p>这种长期坚持的做法我们已经实施了两三年，每周都会举行这样的会议。团队成员聚在一起，听取业务团队对产品未来发展的规划和期望。这样的机制不仅促进了科技与业务之间的沟通和理解，而且也帮助项目更加顺利地进行。</p><p></p><p>岑润哲：AI 产品具有一个独特的优势，即能够更直接地理解用户需求。 用户通过自然语言提出的请求直接反映了他们实际的需求，这些请求作为日志记录在后台，为研发团队提供了宝贵的原始数据。通过对这些文本进行分析，研发团队可以清晰地了解用户的疑问和需求，这大大缩短了终端用户与研发团队之间的距离。这种直接从用户输入中获取需求的方式，与传统的 GUI 产品形成鲜明对比。在 GUI 产品中，用户可能需要通过点击多个按钮来完成操作，而他们的真实需求却不一定能够被准确捕捉。这通常需要额外的市场调研。AI 产品通过自然语言处理，能够直接从用户的提问中提取需求，无需额外的调研步骤。</p><p></p><p>此外，如果能够将成千上万条用户的自然语言需求进行抽象和分析，就可以为产品的优化方向提供指导。这种基于用户实际提问的迭代思路，可以更准确地反映用户需求，从而提高技术团队与产品团队之间的协作效率。例如，当前流行的 ChatGPT 以及基于自然语言处理的其他应用场景，都展示了用户输入即需求的直接性。这种直接性不仅提高了产品开发的针对性和效率，也使得产品迭代更加贴近用户的实际使用情况和需求。</p><p></p><h3>减轻金融研发中的技术债务负担</h3><p></p><p></p><p>余伟： 接下来我们继续聊下一个话题：技术债务的管理与研发效能，探讨技术债务对研发效能的影响，以及如何有效管理和减少技术债务。</p><p></p><p>岑润哲： 在我之前所在的互联网大厂，我们确实面临技术债务引发的问题，例如代码可读性差、维护成本高，以及新功能开发受阻。业务方可能会紧急提出需求，要求加入限制条件，而研发团队为了满足这些紧急需求，有时会采用临时技术方案，导致技术债务的积累。这种债务会使得后续的升级和扩展变得非常困难，并需要花费大量时间和资源进行重构。</p><p></p><p>为了应对这些问题，我们采取了以下措施。</p><p>定期技术评审：通过技术评审来检查新上线的功能，判断是否存在临时性功能，以及这些功能是否会对未来技术架构产生影响。执行代码审查流程，确保代码质量和避免技术债务的产生。预留资源：在产品或活动策划中预留一定比例的资源，专门用于偿还技术债务。项目经理需要在项目排期中预留人 / 天资源，用于偿还技术债务。定期复盘：定期回顾项目，分析为何会产生技术债务，以及如何避免临时性开发。进行数据分析，了解技术债务的具体情况，明确净债务量，以便采取措施降低比例。技术债务追踪：建立技术债务台账，定期追踪和分析标准产品迭代需求、定制化开发和临时开发的比例。需求分级：对需求进行分级管理，区分标准产品迭代和临时填坑需求，确保它们的比例合理。使用需求管理工具进行复盘分析，量化评估技术债务的影响。</p><p></p><p>余伟： 技术债务是研发过程中需要重点关注的问题，它涉及产生的原因、量化的方法以及解决的优先级。在管理技术债务时，我们一方面使用管理手段来督促团队成员主动解决技术债务问题，这是一种被动的督促方式。另一方面，我们通过激励措施来鼓励团队成员解决技术问题，尤其是那些对复杂系统有深入影响的技术栈问题。对于成功解决这些问题的个人或团队，我们会量化他们创造的价值，并通过奖项或奖励来给予表扬和鼓励。</p><p></p><p>在金融行业，技术债务可能涉及数据处理、交易处理和安全性问题，尤其是用户数据和合规性安全，这些都是至关重要的。未能及时处理的技术债务可能会导致金融风险，甚至引发危机。例如，生产环境中的慢 SQL 处理、未及时关闭的临时开关、异常用户数据未得到妥善处理等，这些都可能带来法律风险。金融行业的研发团队必须持续面对解决技术债务的问题。我们甚至有时会暂停产品功能上的新交付，专注于清理技术债务，以确保产品的安全性、合规性和稳定性。</p><p></p><h3>如何度量与优化金融研发周期与代码质量</h3><p></p><p></p><p>余伟： 接下来我们讨论下如何度量研发效能，并基于度量结果进行持续改进。</p><p></p><p>岑润哲： 在评估研发效能时，我们通常从两个主要方面来考虑：</p><p>项目交付周期：这是指从项目开始到最终上线的整个时间跨度，是一个非常重要的指标。代码质量：通过代码审查和评分系统来衡量，包括是否遵循了编码规范、是否存在性能问题、以及代码的整体质量。现在，大模型也可以帮助理解代码并识别潜在问题，将代码质量以量化得分形式展现，为优化提供依据。我们也会衡量产品上线后出现的 bug 数量和严重程度，包括前端和后端的问题。通过统计分析，我们可以计算不同级别 bug 的加权平均值，得到一个整体的得分。团队文化和满意度：虽然这不容易量化，但通过问卷调查和团队成员之间的评价，可以评估团队成员的相互满意度和配合程度。团队文化的重要性不容忽视，因为它直接影响团队成员之间的交流和协作。产品经理、研发和测试团队之间的协作默契对于项目成功至关重要。如果团队之间存在分歧或沟通不畅，即使代码质量很高，最终也可能出现问题。</p><p></p><p>综合这些方面，我们可以建立一套完整的指标体系来评估研发效能。这不仅包括项目的周期和代码质量，还包括团队文化的强度和团队内部的协作情况。只有当所有这些因素都达到一定标准时，我们才能全面提高研发效能。</p><p></p><p>余伟： 在度量研发效能时，我们采用的指标并非固定不变，而是动态的。大约 30% 的度量指标会定期进行滚动式更新。这种动态性是必要的，因为一旦指标被定义，人们总有可能找到方法来规避它们，使自己或团队在指标上的表现不至于太差。这是人的本性，我们不逃避这个事实，在度量指标管理上，我们有以下三个方式。</p><p></p><p>度量指标的动态化：我们建议在制定度量指标时，要考虑其动态化，新的指标可以与其他现有指标相互佐证。如果在某个方向上的指标表现很好，而在另一个方向上表现不佳，这种差异需要检视和分析。重视度量指标：为了让大家重视度量指标，我们开发了度量平台，并公开度量数据。但仅仅公开数据还不够，有些人可能不会关注这些数据。为此，我们采取了一些管理手段，比如定期召开研发效能专题会议，每月将各个产品团队的度量指标数据公开，让大家了解自己的表现，并识别出与预期有偏差的地方。度量指标的反馈机制：度量不仅仅是一个结果的展示，更重要的是建立反馈机制。我们希望团队在看到度量结果后，能够采取行动。如果团队认为自己的表现一般，我们不希望他们满足于现状，而是希望他们能够基于度量结果提出新的需求，深入分析数据，辅助决策，甚至对未来的风险进行预警。对于那些表现不佳或未达预期的团队，我们不仅仅通过度量指标来反馈问题，而是通过更多的方式来告诉他们需要改进和调整。我们的目标是让研发效能度量真正推动团队在质量、效率上实现滚动式提升和发展。</p><p></p><p>岑润哲： 指标的深入分析对于研发团队至关重要，因为不同产品形态和需求导致单一的缺陷率指标，如 1% 或 20%，并不能直观反映研发的实际表现。我们需要根据产品的不同维度进行细化分析，并将分析结果与产品的实际价值直接呈现给研发团队，这不仅有助于他们了解自己的工作效果，也是一种激励。</p><p>我们开发的工具如果能够帮助业务团队提升分析效率，那么将具体的用户故事反馈给研发团队，如他们研发的产品如何帮助金融机构的客户经理节省时间，可以显著提升研发人员的成就感。这种成就感来源于他们能够直观地看到自己工作的成果和对实际业务的影响。</p><p></p><p>当前，许多研发人员在编写代码后，并不十分清楚自己的代码如何被使用以及产生了哪些实际效果。如果能够让他们参与到产品的实际应用中，了解他们的工作如何帮助解决具体问题，那么这种正面的用户反馈和成功案例可以极大地提高研发人员的积极性和主观能动性，进而推动他们在未来的工作中更加投入和创新。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/e2wELuusIV51kgdpSyRr</id>
            <title>2024 FCon全球金融科技大会精彩回顾，汇集前沿视野与落地实践｜附PPT下载</title>
            <link>https://www.infoq.cn/article/e2wELuusIV51kgdpSyRr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/e2wELuusIV51kgdpSyRr</guid>
            <pubDate></pubDate>
            <updated>Wed, 21 Aug 2024 05:46:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 16 日 -17 日，由极客邦旗下 InfoQ 中国主办的<a href="https://fcon.infoq.cn/2024/shanghai/">第二届 FCon 全球金融科技大会</a>"在上海成功举办。本次大会以“科技驱动，智启未来——激发数字金融内生力”为主题，由中国信通院铸基计划作为官方合作机构，数势科技、未来智能、selectDB、枫清科技 Fabarta、亚马逊云科技和英特尔赞助支持。</p><p></p><p>在“十四五”即将收官之际，本届大会特别邀请了行业内各领域专家，共同审视当下的数字化转型现状，为数字化大考“查缺补漏”。同时，紧跟当下技术热点，众多企业也分享了近一年多以来金融行业在 AI 大模型方面的落地实践成果。</p><p></p><p>2 天大会期间，共举行了 1 个 Keynote+12 个并行专题论坛，聚集了 60+ 顶尖专家，来自龙盈智达、平安证券、度小满金融、汇丰科技、工商银行、交通银行、工银科技、华夏银行、中信银行、广发银行、北京银行、苏州银行、渤海银行、富滇银行、人保寿险、平安产险、蚂蚁集团等银行、保险、证券和金融科技企业的专家分享了各自领域的经验探索。</p><p></p><p>去年底，中央金融工作会议提出了做好“五篇大文章”的要求，成为今年金融机构工作布局的重点方向。然而，经过数月来的探索和实践，仍有不少机构对于其中涉及的核心概念和关键抓手不是非常明晰。对此，中国信通院泰尔终端实验室数字生态发展部主任王景尧围绕金融“五篇大文章”及数字化成熟度路径进行了拆解，他表示，“五篇大文章”的目标是提升金融服务实体经济质效，以金融高质量发展助力强国建设。在这个过程中，中国信通院主要发挥支撑“有为政府”建设，服务“有效市场”的作用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4e/4ea2c9d868f6d672813a605810ec9450.jpeg" /></p><p>中国信通院泰尔终端实验室数字生态发展部主任 王景尧</p><p></p><p>而随着数字化迈入新阶段，金融机构开展数字化的核心价值诉求转向深刻的经营变革。当前，金融科技需要解决的是金融机构何以成功、如何思考决策的问题，革新的是金融机构经营的内核。龙盈智达副总裁宫小奕在主题演讲中介绍了龙盈智达如何以场景驱动业技融合，实现自身在金融科技领域的实践和成果。她指出，顶层战略混沌、路径依赖严重、价值共识缺失、人才结构错配、忽视生态协作这 5 项核心挑战是阻碍机构构建新时代核心竞争力的主要因素。对此，金融业创新需要遵循以场景为驱动，坚持业务场景的准确把握、数据价值的有效释放、新兴技术的合理选择、割裂能力的整合拉通四个关键原则。</p><p></p><p><img src="https://static001.geekbang.org/infoq/67/6741fafc8275ceb98a4be9d797d89f72.jpeg" /></p><p>龙盈智达副总裁 宫小奕</p><p></p><p>那么，在这个过程中，数字化转型是 IT 还是业务牵头？平安证券公司首席信息官张朝晖给出了他的答案——可以 IT 先行，完成数字化转型，产出最佳实践，启发业务数字化转型灵感。在平安证券数字化转型过程中，其技术部门通过 “微卡片”组装式无边界应用开发模式，改变了传统研发模式难以满足数字化需求的困局。作为一个容器，微卡片是技术部门为业务搭建的众多系统中的每一个服务对应的前端业务呈现，它们既可以作为独立的模块独立使用、分享或嵌入其它页面，也可以和不同卡片灵活组装到不同的应用场景，一次创作，多次复用。基于 OPTIMAL 数字化转型方法论，平安证券内部目前的微卡片数量越来越多，也积累了越来越丰富的应用场景案例，已经成为公司的现象级应用。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a8e0f0f2b87da22d46d5d746bc92b7e.jpeg" /></p><p>平安证券公司首席信息官 张朝晖</p><p></p><p>此外，人工智能也已经成为企业在数字化转型过程中不可或缺的一个技术，那么具体而言，人工智能如何实现在金融场景的落地呢？作为 FCon 连任联席主席，度小满金融技术委员会执行主席、数据智能应用部总经理杨青分享了“人工智能，助力书写数字金融大文章”的主题。他表示，生成式 AI 正在以嵌入、辅助和共生的形式重塑金融业未来格局。比如，在基础能力嵌入层面，多模态大模型驱动通用文档智能，解决了狭义文档智能框架可处理输入单一、提取流程繁琐和定制化成本高的痛点；在智能助手辅助层面，大模型智能理财投顾 Agent 能够模仿人类理财师工作流程，提供个性化、普惠化的专业投资服务；在人机共生层，生成式 AI 还将智能客服升级到 3.0，能够集成多种模态交互方式，端到端减少中间环节，促成更连贯的用户对话体验。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7c/7c097e669640bcb40cca6f771b7131a0.jpeg" /></p><p>度小满金融技术委员会执行主席、数据智能应用部总经理 杨青</p><p></p><p>但是从新技术诞生、发展到应用，整个过程也需要直面其中的各种挑战。尤其是对于金融行业而言，在持续进行创新的过程中，必须守住合规和安全的底线。在演讲中，汇丰科技创新实验室量子和 AI 科学家朱兵介绍了金融中面临的新兴技术风险。拿 AI 大模型来说，朱兵认为，金融机构应该在引入新能力与自身的风险框架、风险承受能力和市场接受度之间进行权衡。在某些情况下，AI 技术有潜力通过改善的数据驱动带来的洞察以降低风险。再以量子技术为例，尽管量子技术在近些年取得了显著进展，但在大规模应用之前仍然面临许多重大挑战，这既有技术本身的高门槛和高度不确定性，也由于人们还未做好准备去面对具有如此强大能力的技术。金融企业和组织必须在新兴技术如人工智能、区块链和量子计算等的好处与安全、伦理和治理相关的风险之间取得平衡。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1a/1a171150b87d329e354d8b02344e9708.jpeg" /></p><p>汇丰科技创新实验室量子和 AI 科学家 朱兵</p><p></p><p>无论如何，金融创新的步伐仍将持续加速。在本次大会上，中国信通院铸基计划联合 InfoQ 研究中心还发布了《AGI 在金融领域的应用实践洞察》报告，其中显示，2030 年金融 AGI 市场将达到 887.3 亿元人民币， 增长率从 2024 年起未来 4 年保持 100% 以上。然而，在具体落地应用中还存在技术、数据、资源、合规性及监管四大挑战。对此，InfoQ 研究中心研究总监、首席分析师姜昕蔚指出，金融机构可以分 3 步化解：第一，根据投入产出比进行应用评估思考；第二，根据自身情况设定清晰的中间业绩指标和过程指标；第三，选择一个具体场景作为试点，设定一个“破冰期”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/11/11bec5f69ae6cfccf8d8ed6ebf549cfb.jpeg" /></p><p></p><p>除了 Keynote 主题演讲之外，本次大会还策划了多元化的专题论坛内容，大会现场气氛热烈，不少与会者反馈，此次大会的话题从宏观政策、中观场景、微观技术等多维度出发，兼具行业深度、技术视野与落地实践，为其日常工作开展提供了具有价值的参考。</p><p></p><p>其中，10 余个 AI 大模型相关的演讲议题关注度最高，在新技术持续更迭向前的背景下，寻找价值场景成为众多金融机构当下关注的重中之重。“同行交流可以佐证自己的观点”，“如果智能化不能尽快完成赋能，不出 3 个月就会掉队”，“之前觉得大模型离场景落地挺远的，听完思路变得清晰了，打算先从小而美的场景做起来”，与会者表示。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a05d910c63e526051972c5c95ee16689.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/15/156cfed57a7715a97ce5d104060b7cdb.jpeg" /></p><p></p><p>而在大模型烈火烹油的氛围中，我们仍然策划了多个与业务场景数字化以及数字化价值杠杆相关的专场，如数字化管理和运营、数字化风控、数字化营销、数字化人才培养，以及低成本高杠杆的数字化实践。对于金融机构技术从业者而言，既要有看见未来的深谋远虑，也要关注当下的落地实践，业务技术的融合和效果闭环，仍任重道远。</p><p></p><p>与此同时，作为金融数字化转型的技术基底，研发效能提升、IT 架构智能化、现代化核心系统建设等专场也吸引了大量听众的参与。在金融业务创新以及 AI 大模型技术的变革背景下，企业技术基础的夯实和持续迭代升级，无法快进更无法跳过。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f2c8e4a7183a6ff7b0b2b6a2e7af4626.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/c6/c6dee45fd89b133089e9956f5efdb142.jpeg" /></p><p>经统计，大会现场听众累计超过 500 人次。我们深感荣幸与欣慰，感谢每一位专家贡献的知识和智慧，感谢每一位参与者的支持与鼓励。正因为有了大家的热情参与，我们才能不断前行，继续努力成为技术传播领域的佼佼者，持续提升内容质量，打造更加优质的交流平台，共同推动技术领域的创新与突破。</p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日举办上海站的 QCon 大会。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p>大会 PPT 获取通道已开启（由于讲师所在企业限制，部分 PPT 仍在审查或不对外公布，详情见大会官网日程）：<a href="https://ppt.infoq.cn/list/149">https://ppt.infoq.cn/list/149</a>"</p><p></p><p>期待下一场大会再见！</p><p><img src="https://static001.geekbang.org/infoq/95/953dd5ff61bc3e6855dda5b86aa7f383.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ruTj57meeSg8SMsXKdcQ</id>
            <title>AICon 上海站精彩回顾，从大模型变革之路到高效“炼丹”指南，超 60 位大模型先锋输出最前沿干货！| 附PPT下载</title>
            <link>https://www.infoq.cn/article/ruTj57meeSg8SMsXKdcQ</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ruTj57meeSg8SMsXKdcQ</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 12:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8 月 18 日 -19 日，由极客邦旗下 InfoQ 中国倾力打造的 <a href="https://aicon.infoq.cn/2024/shanghai/">AICon 全球人工智能开发与应用大会 2024（上海站）</a>"圆满举办，盛况空前！与会嘉宾阵容强大，既有行业领军人物深入探讨大模型带来的变革及其深远影响，也有技术大咖剖析最新的落地思考和实践案例，到场的每一位观众都受益匪浅。</p><p></p><p>大会现场， 60 多位来自字节跳动、华为、阿里巴巴、微软亚洲研究院、智源研究院、上海人工智能实验室、蔚来汽车、小红书、零一万物等顶尖企业与研究机构的资深专家汇聚一堂，全方位剖析大模型的训练与推理机制、多模态融合技术、智能体 (Agent) 的前沿进展、检索增强生成 (RAG) 策略以及端侧人工智能应用的最新动态，并带来 AI 和大型模型在各种落地场景下的应用案例和最佳实践分享，帮助与会者提升技术视野、获得有价值的实践指导。</p><p></p><p>在本次大会的开幕环节，我们荣幸地邀请到了上海市邮政管理局党组书记、局长冯力虎为大会带来开场致辞。冯力虎表示，上海是开放之都，鼓励和欢迎与前沿科技相关的探讨，希望本次 AICon 大会能够成为一个新的起点，激发更多的创新火花。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/0f/0fe7202e87613e179804495c03cd2220.jpeg" /></p><p></p><p></p><p>当前 AIGC 大模型主要是文字、语音、图片等模态为主，在内容创作、辅助设计、知识内容创作辅助设计问答等场景不断出现创新应用。以供应链和物流为核心的运营和决策优化环节中，如何能有效利用大模型能力及其背后的技术？顺丰科技副总裁唐恺在题为《揭秘顺丰物流决策大模型》的主题演讲中，深入介绍了顺丰在物流领域的技术创新与应用。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e7d891cf9efa7f3a8e88d1e9f0bcc71f.jpeg" /></p><p></p><p></p><p>唐恺介绍，供应链运营是一个专业程度很高且非常严谨的领域，但当前大模型的一些缺点限制了其发挥。为此，顺丰结合大模型和传统小模型来构建供应链业务专家 + 技术专家多智能体，并通过 RAG 召回供应链知识库和数据检索来改善幻觉，同时利用多模态信息进一步提升传统领域模型效果，通过物流决策模型突破模态限制、直接作用于核心决策问题。</p><p></p><p>随后，上海市邮政管理局党组书记、局长冯力虎，顺丰集团副总裁龚威、顺丰科技副总裁唐恺、零一万物联合创始人祁瑞峰、智谱 AI 副总裁吴玮杰、华为云盘古大模型 CTO 李寅、浙江大学管理学院副院长杨翼，以及极客邦科技创始人兼 CEO 霍太稳，共同登台联合发布顺丰物流决策大模型，并一齐见证这一物流行业创新的重要时刻。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e3/e3aa402896224756d395db9b6fa85270.jpeg" /></p><p></p><p></p><p>在接下来的主题演讲中，蔚来创始人、董事长、CEO 李斌深入介绍了蔚来近年在智能电动汽车和 AI 方面的思考与实践。李斌表示，“AI 将成为智能电动汽车企业的核心基础能力，车是大模型最佳的落地场景。”据介绍，在蔚来智能电动汽车的技术全栈中，AI 和所有的技术栈都有交集。其中， 智能驾驶无疑是汽车 AI 综合能力的反映，而智能驾驶的技术发展史就是算法空间理解和处理能力的进化史，因此蔚来决定直接走向基于视频的端到端世界模型，这一路径的信息损耗最小。李斌表示，蔚来的智能驾驶世界模型 NWM（NIO World Model）能在 0.1 秒内基于全量数据模拟出 216 种可能轨迹，评估后找出最优解。从 NWM 的技术角度来讲，其本身就是一个多元自回归时空生成模型。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ef/efba22a21102c9007c433f7d6a19369d.jpeg" /></p><p></p><p></p><p>今年内，蔚来将利用 NWM 实现端到端上车。此外，李斌谈到 AI 在车上的另一个重要应用：智能座舱。他认为，车在未来会成为人的情感伙伴，今年蔚来的 NOMI GPT 大模型全量上线，目前具备 2000 项技能，累计用户聊天互动次数达 15680260 次。李斌在演讲最后称，“一个成功的智能电动汽车公司，一定是一家成功的 AI 公司。”</p><p></p><p>英特尔院士、大数据技术全球 CTO 戴金权在题为《大模型的异构计算和加速》的演讲中，分享了英特尔过去一两年在大模型的异构计算和加速方面所做的工作。戴金权指出，大模型在做推理和训练的过程中，存在内存带宽、计算、显存大小和分布式计算多方面的瓶颈。随着大模型被部署在客户端、边缘端、服务器等不同的系统，除低比特计算的方法外，推理算法的各种优化都能够更好地提升其在 XPU 上的计算效率。他表示，高效的异构计算是生成式 AI 发展的核心能力之一。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/caa2fbab6d2aab0e855cafb988de3cd0.jpeg" /></p><p></p><p></p><p>如何高效地训练大模型、做大模型的推理优化？现场，面壁智能联合创始人兼 CEO 李大海指出， 2018 年以来，行业内不断见证大模型规模法则（Scaling Law），工业界也在尽可能地保证摩尔定律有效，持续改进芯片制造工艺、提升芯片制程，核心是提升芯片电路密度、实现计算设备小型化。“制程”不断提高的事情同样发生在大模型领域，根据过去几年在大模型领域的深耕和实践，对大模型的发展趋势进行观察总结，面壁智能提出了大模型时代的面壁定律：大模型的知识密度不断提升，平均每 8 个月提升一倍。”其中知识密度 = 模型能力 / （参与计算的）模型参数。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8ea4aeda300ae34bd13a6b851d2a71fe.jpeg" /></p><p></p><p></p><p>李大海表示，大模型数据驱动技术方向大致确定，而模型架构 - 算法 - 数据技术方案仍高速迭代，需持续改进模型制程，极致提升知识密度。据他观察，在过去四年，大模型的知识密度平均每 8 个月就提高一倍，相比摩尔定律更加高效，这也是面壁做端侧模型的原因。芯片制程带来终端算力持续增强，模型制程带来模型知识密度持续增强，两者交汇揭示了端侧智能的巨大潜力。此外，李大海认为，更高知识密度带来更高效模型，要构建模型风洞，在小模型高效寻找最优数据和超参配置并外推至大模型，让模型成长摆脱“炼丹”窘境。</p><p></p><p>最后，字节跳动研究科学家、豆包大模型视觉基础研究团队负责人冯佳时分享了字节跳动基于 LLM 的视频生成和图像理解实践。冯佳时表示，无论是在自动驾驶还是具身智能上，业内往往把大语言模型视作机器人大脑，并希望其在做推理时能够参考周围环境的信息，能够具有一定的定位能力，与物理环境进行可靠的交互。为此，字节在 PixelLM 方案中引入多个 token 来完成多个物体的分割，并将分割模型 SAM 替换成轻量的 MLP，计算量比之前的模型 LISA 减少一半，分割精度也显著提升。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/08/0853ef5b844c2bcd64ec9be680b284ef.jpeg" /></p><p></p><p></p><p>此外，冯佳时指出，目前的视频生成模型在交互上有很多不便之处，存在一致性、创作界面与可控性、视频表现力等方面的问题。字节在其 StoryDiffusion 模型提出一致性模块和运动生成模块两个关键技术，来提升角色一致性和表现力。</p><p></p><p>除了 Keynote 主题演讲之外，本次大会还策划了多元化的专题论坛内容，包括大模型训练以及推理加速、RAG 落地应用与探索、大模型产品应用及构建、多模态大语言模型的前沿应用与创新、大模型与企业工具集成的提效实践、大模型产学研结合探索、端侧模型落地探索等十多个高质量话题专场。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/72/723f20a960944292bfc421d627ff3b90.jpeg" /></p><p></p><p></p><p>大会现场气氛异常热烈，不仅吸引了大量听众的积极参与，还赢得了在场参会人员的一致好评。许多与会者纷纷表示，这次大会紧密围绕当下的 AI 和大模型热点话题，从多个角度进行了深入的技术架构专业解读和商业化实践分享，为其日常工作和探索带来了宝贵的启示和具有实际应用价值的参考，有助于他们在各自领域内更好地推动 AI 技术的创新和发展。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e4/e4252d296a8c7ebb5f5c18637e2e7bc3.jpeg" /></p><p></p><p></p><p>AICon 上海的圆满举办，离不开赞助商们贡献的力量。感谢英特尔、亚马逊云科技、Google Cloud、矩阵起源、百道数据、Optiver、数势科技、未来智能、UCloud优刻得、钛动科技、零一万物、快递 100、快手、昇腾对本届大会的倾情赞助以及蔚来汽车为大会展区带来的特别策划。在大家的共同助力下，我们得以持续推动技术的传播与发展，为行业创新注入不竭源泉。</p><p></p><p>经统计，AICon 上海站现场听众累计超过 1000 人次。我们深感荣幸与欣慰，衷心感谢每一位参与者的鼎力支持与不断鼓励。正是因为有了大家的热情参与和积极贡献，我们才能坚定不移地追求目标，致力于成为技术传播领域的佼佼者。我们将持续不断地提升内容的质量，致力于打造更加优质、更具包容性的交流平台，让每一个人都能在这里找到启发和灵感，一齐推动技术领域的创新与突破，为未来的科技进步贡献力量。</p><p></p><p>大会 PPT 获取通道已开启，关注 AI 前线 公众号，后台回复“PPT”，即可获取 PPT 下载地址！（由于讲师所在企业限制，部分 PPT 仍在审查或不对外公布，详情见大会官网日程） &gt;&gt;&gt;</p><p></p><p>至此，今年 InfoQ 中国已圆满落幕 5 场技术盛会，随后还将于 10 月 18 -19 日举办 QCon 上海站。如您感兴趣，可点击<a href="https://qcon.infoq.cn/2024/shanghai">官网</a>"查看更多详情。</p><p></p><p>期待下一场大会再见！</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/c9/c919f5a113b14883202eec12906fc7e3.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/12dCYzJcokxXEcTMvc9Y</id>
            <title>“从头开始训练模型，几乎没有意义”</title>
            <link>https://www.infoq.cn/article/12dCYzJcokxXEcTMvc9Y</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/12dCYzJcokxXEcTMvc9Y</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 10:25:29 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p></p><p>我们<a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">之前分享过</a>"我们在运营大型语言模型应用程序时磨练出的战术方面的见解。战术是细粒度的：它们是为实现特定目标而采取的具体行动。我们还<a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">分享了</a>"我们对运营的看法：为支持战术工作并实现目标而建立的更高层次的过程。</p><p>&nbsp;</p><p>但这些目标从何而来？这就属于战略的范畴。战略解答了战术和运营中“如何”背后的“什么”和“为什么”。</p><p>&nbsp;</p><p>我们将分享我们的见解，比如“在产品市场契合之前避免使用GPU”和“专注于构建系统而非模型”，以指导团队如何高效地分配稀缺资源。我们还提出了一个通往卓越产品的迭代路线图。这些宝贵的经验教训汇集起来，回答了以下这些问题：</p><p>&nbsp;</p><p>自建还是购买：何时应该自行训练模型，何时应该使用现成的API？答案总是“视情况而定”。我们分享了决定因素是什么。迭代至卓越：如何创造长期竞争优势，而不仅仅是依赖最新的模型？我们讨论了基于模型构建健全系统的重要性，并专注于提供令人难忘的、有粘性的体验。以人为中心的AI：如何有效地将大模型融入人类工作流程中，以提升生产力和幸福感？我们强调了构建支持和增强人类能力的AI工具的重要性，而不是试图完全取代人类。入门指南：团队开始构建大模型产品的基本步骤是什么？我们概述了一个基本的流程，从提示词工程、评估和数据收集开始。低成本认知的未来：大模型的成本迅速降低和能力增加将如何塑造AI应用的未来？我们审视了历史趋势，并通过一个简单的方法来估计某些应用何时可能在经济上变得可行。从演示到产品：从一个引人注目的演示到一个可靠、可扩展的产品需要做些什么？我们强调了严格的工程、测试和持续改进的必要性，以缩小原型和生产之间的差距。</p><p>&nbsp;</p><p>为了回答这些难题，让我们来一步一步地思考。</p><p>&nbsp;</p><p></p><h2>战略：在不失去先机的情况下利用大模型</h2><p></p><p>&nbsp;</p><p>成功的产品需要深思熟虑的规划和严格的优先级安排，而不是无休止的原型迭代或盲目追逐最新的模型或潮流。在本文中，我们将放眼四周，深入探讨构建卓越AI产品的战略考量。我们还将审视团队在开发过程中可能面临的主要权衡问题，比如决定是自主构建还是外部采购，并为早期大型语言模型应用的策略开发提供一个指导“蓝图”。</p><p>&nbsp;</p><p></p><h3>在产品契合市场之前不要使用GPU</h3><p></p><p>&nbsp;</p><p>要实现卓越，你的产品不应该只是在供应商提供的API之上构建一层薄弱的包装层，但走向相反的极端可能带来更大的代价。过去一年，我们目睹了大量的风险投资涌入，包括令人瞠目结舌的60亿美元A轮融资，用于训练和定制模型，而没有清晰的产品愿景或目标市场。在这一部分，我们将解释为什么急于投入模型训练是一个错误，并探讨自托管模型的定位。</p><p>&nbsp;</p><p></p><h4>从头开始训练模型（几乎）总是没有意义</h4><p></p><p>&nbsp;</p><p>对于大多数组织来说，从头开始训练大模型是一种不切实际的分心，它分散了构建实际产品的精力和资源。</p><p>&nbsp;</p><p>尽管这么做很令人兴奋，尽管似乎业界都在追随这一趋势，但开发和维护机器学习基础设施需要大量的资源投入，包括收集数据、训练和评估模型以及部署它们。如果你还处在验证产品市场契合度的阶段，这些可能会从核心产品开发中抽走宝贵的资源。即使你拥有计算能力、数据和技术能力，预训练的大模型也可能在几个月内就变得过时。</p><p>&nbsp;</p><p>以<a href="https://arxiv.org/abs/2303.17564?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">BloombergGPT</a>"为例，这是一个专门为金融任务训练的大模型。这个模型在363B个token上进行了预训练，耗费了<a href="https://twimlai.com/podcast/twimlai/bloomberggpt-an-llm-for-finance/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">九名全职员工</a>"的辛勤劳动，包括四名AI工程师和五名机器学习产品和研究人员。尽管付出了巨大的努力，BloombergGPT在那些金融任务上的表现在一年内就被<a href="https://arxiv.org/abs/2305.05862?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">gpt-3.5-turbo和gpt-4超越了</a>"。</p><p>&nbsp;</p><p>这个故事以及其他类似案例揭示了一个事实，对于大多数实际应用来说，在领域特定数据上从头开始预训练大模型并不是资源的最佳利用方式。相反，团队应该考虑对可能满足他们特定需求的最强大的开源模型进行微调。</p><p>&nbsp;</p><p>当然也有例外。<a href="https://blog.replit.com/replit-code-v1_5?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">Replit的代码模型</a>"就是一个鲜明的例子，这个模型专门为代码生成和理解而训练。通过预训练，Replit超越了其他大型模型，如CodeLlama7b。然而，随着其他越来越有竞争力的模型的发布，要保持其竞争力，还需要持续不断的投入和更新。</p><p>&nbsp;</p><p></p><h4>在证明必要性之前不要进行微调</h4><p></p><p>&nbsp;</p><p>对于大多数组织来说，进行微调更多是受FOMO（错失恐惧症）的驱使，而不是基于清晰的战略思考。</p><p>&nbsp;</p><p>许多组织过早地进行微调，试图避开“只是一层包装”的指责。实际上，微调是一项重装备操作，只有在你收集了大量示例并确信其他方法均不足以解决问题时才考虑使用。</p><p>&nbsp;</p><p>一年前，许多团队向我们表达了他们对微调技术的热情。然而，很少有人找到产品市场契合度，大多数人最终对他们的决定感到后悔。如果你要进行微调，应该非常确信自己已经做好了反复进行这项工作的准备，因为基础模型自身也在不断进步——见下面的“模型不是产品”和“构建LLMOps”。</p><p>&nbsp;</p><p>微调在以下场景中可能可以成为恰当的选择： 特定应用需要的数据并未包含在用于训练现有模型的开放数据集中。 你已经开发了一个最小可行产品（MVP），并证明现有的模型无法满足需求。但请务必谨慎：如果连模型开发者都难以获得高质量的训练数据，那么你又是如何获得这些数据的呢？</p><p>&nbsp;</p><p>最后请记住，大模型驱动的应用程序不是科学展览会上的项目，对它们的投入应当与其对企业战略目标的贡献及所带来的竞争优势相匹配。</p><p>&nbsp;</p><p></p><h4>从推理API开始，但不要拒绝自托管</h4><p></p><p>&nbsp;</p><p>有了大模型API，初创公司能够以前所未有的便捷性采用和集成语言建模能力，无需从头开始训练自己的模型。像Anthropic和OpenAI这样的供应商提供了通用API，只需几行代码就可以将智能嵌入到你的产品中。利用这些服务，你可以大幅减少开发方面的劳动投入，从而将更多的精力集中在为客户提供真正的价值上——这有助于你更快地验证想法并加快产品与市场契合度的迭代。</p><p>&nbsp;</p><p>但是，就像数据库一样，托管服务并不适用于所有场景，特别是在规模扩大和需求增长的情况下。事实上，在一些受严格监管的行业，如医疗保健和金融行业，或在有合同义务、保密要求约束的情况下，自托管可能是唯一能够确保在使用模型时不泄露敏感或私有数据的方式。</p><p>&nbsp;</p><p>此外，自托管能够避开供应商可能设置的限制，如速率限定、模型弃用以及一些使用上的限制。此外，自托管还赋予你完全的控制权，使得构建一个具有差异化优势和高质量标准的系统变得更加容易。最后，自托管，特别是在进行了微调的情况下，可以在大规模应用中显著降低成本。例如，<a href="https://tech.buzzfeed.com/lessons-learned-building-products-powered-by-generative-ai-7f6c23bff376#9da5?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">BuzzFeed就分享了他们如何通过微调开源模型将成本降低了80%</a>"。</p><p>&nbsp;</p><p></p><h2>迭代至卓越</h2><p></p><p>&nbsp;</p><p>为了确保长期的竞争优势，你需要超越模型，想想是什么让你的产品脱颖而出。虽然执行速度很重要，但它不应成为你唯一的优势。</p><p>&nbsp;</p><p></p><h4>模型不是产品，围绕它的系统才是</h4><p></p><p>&nbsp;</p><p>对于那些不自行构建模型的团队而言，快速的创新步伐无疑是一大优势。他们能够灵活地从一个最先进的模型转移到另一个，不断追求上下文理解、推理能力以及成本效益比方面的提升，从而打造更优质的产品</p><p>&nbsp;</p><p>这一进展既令人振奋又具有可预见性。从整体来看，这暗示了模型可能成为系统中最具易变性的部分。</p><p>&nbsp;</p><p>相反，将你的精力专注在那些能够提供长期价值的东西上，例如：</p><p>&nbsp;</p><p>评估基线：确保你的任务在不同模型间具有一致的可靠性能评估；安全护栏：建立机制以确保无论模型如何变化，都能防住不恰当的输出；缓存：利用缓存策略来减少对模型的依赖，从而降低延迟和成本；数据飞轮：为上述的迭代改进提供动力。</p><p>&nbsp;</p><p>这些组件构建了一个更为坚固的产品质量护城河，超越了模型本身的能力。</p><p>&nbsp;</p><p>但这并不意味着应用构建就完全没有风险。不要期望OpenAI或其他模型供应商会提供完全相同、无需额外处理的解决方案。</p><p>&nbsp;</p><p>例如，一些团队构建自定义工具来验证专有模型的结构化输出。在这方面进行适度的投入是明智的，但过度投入则可能不是最佳的时间利用策略。OpenAI需要确保当用户请求一个函数调用时会得到一个有效的函数调用——因为所有用户都想要这个。在这种情况下，采用“战略性拖延”是明智的，即只构建你绝对需要的东西，然后等待供应商能力的进一步提升。</p><p>&nbsp;</p><p></p><h4>建立信任，从小处开始</h4><p></p><p>&nbsp;</p><p>追求成为“万能钥匙”产品往往会导致平庸。要打造引人注目的产品，需要专注于创造独特且令人难以忘怀的用户体验，让用户不断回头。</p><p>&nbsp;</p><p>设想有一个旨在应对用户可能提出各种问题的通用性RAG系统。缺乏针对性的专业化导致系统无法优先获取最新资讯，解析特定领域的数据格式，或深入理解特定任务的复杂性。因此，用户得到的体验往往是表面化的、不可靠的，难以满足他们的实际需求。</p><p>&nbsp;</p><p>为了解决这个问题，需要专注于特定领域和用例。通过深入挖掘而非广泛覆盖，可以开发出与用户产生共鸣的专业工具。专业化还让你能够清晰地界定系统的能力和局限。坦诚地展示系统的优势和局限，不仅体现了自我认知，也帮助用户明白在哪些方面系统能发挥最大效用，从而建立信任并增强对输出结果的信心。</p><p>&nbsp;</p><p></p><h4>打造LLMOps：为了更快的迭代</h4><p></p><p>&nbsp;</p><p>DevOps本质上并非只关注可重复的工作流、左移策略或团队授权——更只不是关于编写YAML文件。</p><p>&nbsp;</p><p>DevOps关注缩短工作流与结果反馈之间的周期，从而促进持续改进而非累积错误。它的理念源于精益创业运动，可以进一步追溯到精益制造和丰田生产系统，这些理念强调的是快速响应变化和持续改进。</p><p>&nbsp;</p><p>MLOps将DevOps的理念和实践应用到机器学习中。它带来了可重复的实验流程，提供了一站式的工具套件，使得模型构建者能够更便捷地将模型推向生产。当然，在这个过程中，YAML文件扮演了不可或缺的角色。</p><p>&nbsp;</p><p>但从行业来看，LMOps尚未完全实现DevOps的核心功能。它没有有效地缩短模型开发与在生产环境中推理和交互之间的反馈周期。</p><p>&nbsp;</p><p>令人振奋的是，LLMOps领域已经从关注那些看似琐碎的问题，如提示词管理，转向解决阻碍迭代的难题：在生产环境中进行有效监控并实现持续改进。</p><p>&nbsp;</p><p>我们已经拥有了中立、众包的评估平台，这些平台专门用于聊天和编程模型的互动——它们构成了一个集体迭代改进的外循环。像LangSmith、Log10、LangFuse、W&amp;B Weave、HoneyHive等工具不仅可用于收集和整理生产系统中的结果数据，还通过与开发流程紧密结合，利用这些数据来不断改进系统。你可以尝试拥抱它们，或者构建属于自己的工具。</p><p>&nbsp;</p><p></p><h4>如果可以买，就不要自己构建</h4><p></p><p>&nbsp;</p><p>大多数成功的业务并不是基于大语言模型的业务，但大多数业务都存在通过大模型进行改进的可能性。</p><p>&nbsp;</p><p>这有时会误导领导者急于将大模型技术应用于系统改造，导致成本上升和产品质量下降，甚至可能将这些技术作为虚假的“AI”特性匆忙推向市场，还带上<a href="https://x.com/nearcyan/status/1783351706031718412?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">那些令人眼花缭乱的星星图标</a>"。更好的做法是：专注于那些真正与你的产品设计目标相契合并能增强你核心运营的大模型应用。</p><p>&nbsp;</p><p>让我们重新审视一下那些可能消耗团队宝贵时间的错误尝试：</p><p>&nbsp;</p><p>尝试为企业开发定制的文本到SQL转换功能；构建能够与文档进行互动的聊天机器人；将公司的知识库与客户支持系统的聊天机器人集成。</p><p>&nbsp;</p><p>虽然上述的大模型应用属于入门级别，但对于大多数产品公司来说，并不适合自己从头开始构建。这些是许多企业面临的普遍问题，演示和实际存在巨大差距——而这正是软件公司的专长所在。当前的Y Combinator孵化器已经在集中解决这些问题，因此在这些问题上投入宝贵的研发资源是一种浪费。</p><p>&nbsp;</p><p>如果说这听起来像是陈词滥调的商业建议，那是因为在当前热潮和泡沫般的兴奋中，人们很容易将带有“大模型”标签的事物误认为是尖端的增值差异化手段，而忽略了哪些应用实际上已经是司空见惯的。</p><p>&nbsp;</p><p></p><h4>AI辅助，以人为本</h4><p></p><p>&nbsp;</p><p>目前，由大模型驱动的应用程序是很脆弱的，它们需要精心设计的保护措施和防御策略，即便如此，依然存在很多不确定性。然而，当这些应用程序的应用范围有了明确限定，可能会变得极为有用。这说明大模型是加速和优化用户工作流的有力工具。</p><p>&nbsp;</p><p>尽管人们可能会被基于大模型的应用程序完全替代工作流或工作职能的想法所吸引，但目前最有效的模式是人机协作——计算机半人马模式（类似<a href="https://en.wikipedia.org/wiki/Advanced_chess?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">国际象棋半人马模式</a>"）。当有才能的人类与大模型能力相结合，完成任务的效率和满足感可以得到显著提升。GitHub Copilot，作为大模型的主要应用之一，已经展示了这种协作工作流程的强大潜力：</p><p>&nbsp;</p><p></p><blockquote>“总的来说，开发者告诉我们，他们感到更有信心，因为编码变得更容易、错误更少、代码易读性更强、可重用性更高、更简洁、更易于维护，并且系统弹性比没有GitHub Copilot和GitHub Copilot Chat时更强。”——<a href="https://resources.github.com/learn/pathways/copilot/essentials/measuring-the-impact-of-github-copilot/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">Mario Rodriguez，GitHub</a>"</blockquote><p></p><p>&nbsp;</p><p>对于那些长期从事机器学习工作的人来说，可能会立刻联想到“HITL”，但不要急于下结论：HITL机器学习是一种人类专家确保机器学习模型能够按照预期的方式运行的范式。尽管两者相关，但我们今天要讨论的是一个更为微妙的概念。大模型驱动的系统不应成为大多数工作流的主要驱动力，而应当被视为一种辅助资源。</p><p>&nbsp;</p><p>将人类置于核心位置，并探索如何让大模型来辅助他们的工作流，这种方法将引导我们做出截然不同的产品和设计决策。最终，这将促使我们打造出与那些急于将所有职责转嫁给大模型的竞争对手不同的产品——更好、更有用、风险更低的产品。</p><p>&nbsp;</p><p></p><h2>从提示词、评估和数据收集开始</h2><p></p><p>&nbsp;</p><p>前面的章节阐述了一些技术和建议，内容相当丰富，可能需要一些时间来消化。让我们来概括一下最核心的建议：如果一个团队想要构建基于大模型的产品，应该从哪里开始？</p><p>&nbsp;</p><p>在过去的一年里，我们已经看到了足够多的例子，这让我们对大模型应用取得成功的轨迹有了清晰的认识。在本节中，我们将通过一个基础的“入门”指南来梳理这些经验。核心理念是保持简单，只在必要时才引入复杂性。根据经验，每增加一个复杂度级别，通常至少需要比前一个级别多一个数量级的努力。</p><p>&nbsp;</p><p></p><h4>提示词工程先行</h4><p></p><p>&nbsp;</p><p>我们从提示词工程开始。在试图从较弱的模型榨取性能之前，先使用我们在战术部分讨论的技术。思维链、n-shot以及结构化输入和输出通常都是明智的选择。在转向较弱的模型之前，先用大的模型进行原型设计。</p><p>&nbsp;</p><p>只有在提示词工程无法达到所需的性能时才考虑微调。如果有非功能性方面的需求（例如，数据隐私、完全控制权和成本考量），并且这些要求阻碍了使用专有模型，不得不使用自托管方案，那就需要进行微调。只是你要确保数据隐私需求不会阻碍你使用用户数据进行微调！</p><p>&nbsp;</p><p></p><h4>评估并启用数据飞轮</h4><p></p><p>&nbsp;</p><p>即使团队是在初始阶段，也需要进行评估。否则，你将无法确定你的提示词工程是否有效，或者微调模型何时能准备就绪替换基础模型。</p><p>&nbsp;</p><p>有效的评估应针对<a href="https://twitter.com/thesephist/status/1707839140018974776?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">具体的任务</a>"，并反映预期的使用场景。我们<a href="https://hamel.dev/blog/posts/evals/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">建议</a>"的评估起点是单元测试。这些基础的断言用于检测已知或假设的故障模式，有助于推动早期的设计决策。此外，还可以看看其他<a href="https://eugeneyan.com/writing/evals/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">特定于任务的评估方法</a>"，例如用于分类、摘要等任务的评估。</p><p>&nbsp;</p><p>尽管单元测试和基于模型的评估很有用，但它们不能完全替代人类的评估。让人们使用你的模型或产品，并收集反馈，这至关重要。这不仅可以衡量产品在现实世界中的表现和缺陷，也能收集可用于微调模型的高质量标注数据。这样可以形成一个正向反馈循环（也叫数据飞轮），随着时间的推移，可以产生复合效应：</p><p>&nbsp;</p><p>使用人类评估来评估模型性能和/或发现缺陷；使用标注数据来微调模型或更新提示词；持续这一过程。</p><p>&nbsp;</p><p>例如，在评审大语言模型生成的摘要时，我们可能会对每个句子进行细致的反馈，识别出事实错误、不相关性或风格问题。然后，我们可以使用事实错误标注来<a href="https://eugeneyan.com/writing/finetuning/?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">训练幻觉分类器</a>"，或使用相关性标注训练<a href="https://arxiv.org/abs/2009.01325?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">奖励模型来评估相关性</a>"。 另一个例子是，LinkedIn在其播客中分享了使用<a href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">基于模型的评估器</a>"来评估幻觉、AI违规行为、连贯性等问题的成功经验。</p><p>&nbsp;</p><p>通过构建随时间增值的资产，我们将评估工作从单纯的运营成本转变为战略投入，并在这个过程中加速数据飞轮效应。</p><p>&nbsp;</p><p></p><h2>低成本趋势</h2><p></p><p>&nbsp;</p><p>1971年，施乐帕克研究中心的研究人员预测未来是一个由网络个人电脑主导的世界。他们发明了一系列关键技术，如以太网、图形渲染、鼠标以及窗口界面，为他们预测的未来成为现实奠定了基础。</p><p>&nbsp;</p><p>他们还进行了一项基础实践：观察那些非常有实用价值但成本较高的应用（例如，视频显示器），然后分析这些技术的历史价格趋势（如摩尔定律），并预测了这些技术何时会变得经济实惠。</p><p>&nbsp;</p><p>我们同样可以在大型语言模型技术方面进行同样的分析，尽管我们没有像像晶体管成本那样直观的衡量标准。以一个被广泛认可且持续更新的基准测试为例，比如Massively-Multitask Language Understanding数据集，以及一种一致性的输入方法（five-shot提示词）。然后，我们可以比较在不同时间用各种性能水平的语言模型在该基准测试上运行的成本。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/531433da42738663441944d60056011d.png" /></p><p></p><p>在成本固定之下，能力正迅猛提升。在能力水平固定之下，成本正急剧下降。</p><p>&nbsp;</p><p>自OpenAI的davinci模型作为API发布以来的四年里，在该任务上运行具有等效性能的模型的成本已经从20美元降到了不到10美分——成本减半的时间仅为六个月。同样，截至2024年5月，通过API供应商或自行运行Meta的LLama 3 8B模型的成本仅为每百万个token 20美分，这个模型的性能与OpenAI的text-davinci-003相当，后者曾以其卓越的表现震惊了世界。值得注意的是，当LLama 3 8B在2023年11月末发布时，其成本大约为每百万个token 20美元。成本在短短18个月内降低了两个数量级，与摩尔定律预测的时间不谋而合。</p><p>&nbsp;</p><p>现在，我们来探讨一个极具潜力但目前还不具备经济效益的大模型应用（<a href="https://arxiv.org/abs/2304.03442?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">生成视频游戏角色</a>"，成本估计为<a href="https://arxiv.org/abs/2310.02172?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">每小时625美元</a>"）。自2023年8月该论文发布以来，成本已经下降了一个数量级，降至每小时62.5美元。基于这一趋势，我们可以预期在下一个九个月内，成本会降至每小时6.25美元。</p><p>&nbsp;</p><p>当吃豆人（Pac-Man）游戏在1980年首发时，现在的1美元可以兑换一个信用点，允许玩家享受几分钟到几十分钟的游戏乐趣——如果以每小时六场游戏来估算，相当于每小时6美元。按照这种粗略计算，一个引人入胜的大模型增强型游戏体验将在2025年的某个时候变得经济可行。</p><p>&nbsp;</p><p>这些趋势虽然还很新，仅有几年的历史，但我们没有理由认为它们在未来几年会有所减缓。尽管在算法和数据集方面，我们可能已经摘取了容易获得的成果，比如超越了“Chinchilla比率”的每参数约20个token，但数据中心内部更深层次的创新和投入以及硅芯片层面的进展有望弥补这一潜在的不足。</p><p>&nbsp;</p><p>这可能是最关键的战略洞见：那些今天看似完全不可能的演示或研究论文，在未来几年内将逐渐演变为高级的功能，并成为普通商品。我们应当基于这一视角来构建我们的系统和组织架构。</p><p>&nbsp;</p><p></p><h2>从0到1已经够多了，是时候从1到N了</h2><p></p><p>&nbsp;</p><p>构建大模型演示应用非常有趣，只需要几行代码、一个向量数据库和一条精心设计的提示词，我们就能创造出令人惊叹的“魔法”。在过去的一年里，这种“魔法”被比作是互联网、智能手机，甚至印刷机般的创新。</p><p>&nbsp;</p><p>不幸的是，在现实的软件项目中摸爬滚打的人都知道，演示中的完美表现与大规模稳定运行的产品之间存在着巨大的差异。</p><p>&nbsp;</p><p>以自动驾驶汽车为例。第一辆由神经网络驱动的汽车在<a href="https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">1988年</a>"问世，二十五年后，Andrej Karpathy<a href="https://x.com/karpathy/status/1689819017610227712?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">在他的Waymo上进行了第一次演示</a>"。十年之后，这家公司获得了<a href="https://x.com/Waymo/status/1689809230293819392?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3MjMxOTY3MDcsImZpbGVHVUlEIjoiMjVxNVg4bzFNeHM3MGwzRCIsImlhdCI6MTcyMzE5NjQwNywiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwidXNlcklkIjo4OTExNDE4NX0.NE_EIr3dJGnWdySCRBVqkFIN6oLFngH1N4UekN4VOxg">无人驾驶许可</a>"。从原型到商业产品，经历了三十五年严格的工程、测试、改进和合规监管过程。</p><p>&nbsp;</p><p>在过去的一年，不管是工业界还是学术界，我们都看到了大模型应用的起伏：这是大模型应用的“1到N”年。我们希望我们所学到的经验——从严格的战术性操作技术，再到内部需要构建哪些能力的战略性视角——能帮助你在接下来的一年乃至更长远的未来，更好地参与这项激动人心的新技术的共同建设。</p><p>&nbsp;</p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p>&nbsp;</p><p>原文链接：</p><p><a href="https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-iii-strategy/">https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-iii-strategy/</a>"</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6yYr6GPSULaZQhgJ9fTv</id>
            <title>卷模型还是做平台？落地企业AI，用友这样做！</title>
            <link>https://www.infoq.cn/article/6yYr6GPSULaZQhgJ9fTv</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6yYr6GPSULaZQhgJ9fTv</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 10:21:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>由大模型掀起的 AI 热潮持续了两年时间，行业已经从关注大模型技术的创新突破，转变为思考和实践如何利用基于大模型的 AI 能力来赋能业务、创造价值。正如用友网络副总裁罗小江所说：这个时代不缺技术，缺的是方法体系，缺的是让技术真正意义上融入业务。</p><p></p><p>可以预见的是，企业将更加亲睐针对特定行业或应用进行训练优化的大模型，以及能够深入业务场景，带来实际经济效益的 AI 解决方案。</p><p></p><p>8 月 9-10 日，由用友主办，以“AI+ 成就数智企业”为主题的“2024 全球商业创新大会”在北京召开, 在企业数智化技术峰会上，用友围绕 YonGPT 2.0 大模型与用友 iuap 智能平台 YonAI 给出了更加满足企业需求的 AI 落地解法。在用友看来，AI 能力并非孤立的烟囱，想要充分释放 AI 潜能，一定要将 AI 能力融入企业的平台能力建设，以平台为载体，在各个业务环节中挖掘 AI 能力的适用场景，加速发展新质生产力、重塑企业核心竞争力。</p><p></p><h2>大模型落地并非易事，YonGPT 2.0 如何破局？</h2><p></p><p></p><p>目前，大模型在大多数行业中仍然很难深入到企业实际业务层面，要想切实为企业赋能，往往面临多重挑战：首先是数据挑战，很多企业缺乏数据准备，无法为模型训练和微调提供充足的高质量数据，也没有建立与 AI 时代相适应的大数据基础设施；其次是安全挑战，普遍运行在云端的大模型让企业担忧数据和隐私泄露风险，他们更偏向运行在本地，或者自身有更高掌控力的小模型产品；大模型的应用场景偏少也让很多用户头疼，花费大量投资建立的技术栈在实践中少有用武之地，降本增效也就无从谈起。另外，大模型在行业领域应用时，频繁出现的幻觉现象让问题更是雪上加霜，这也是垂类大模型崛起的重要因素；最后，由于 IT 技术较为薄弱，传统企业面对大模型和 AI 技术栈的持续运维也往往力不从心。</p><p></p><p>以上这些问题，都让大模型技术的落地之路变得更加坎坷不平。面对这样的局面，用友在去年发布了业内首个企业服务大模型 YonGPT，专注于助力企业降低使用 AI 的门槛，解决企业 AI 赋智赋能时面临的一系列难题。过去一年来，YonGPT 先后发布了六大场景，上线了问答应用、Agent 和应用生成等能力，并在今年 2 月份通过了网信办备案。在此基础上，用友此次又升级了 YonGPT 2.0 全新版本，包括了多项专业能力增强、一个大模型平台和两个应用框架。</p><p></p><p><img src="https://static001.geekbang.org/infoq/59/5933b38b81f9704543c495920b3a45a5.png" /></p><p>通用大模型所关注的领域范围往往非常广泛，而 YonGPT 2.0 的提升完全专注于企业常见的业务领域，包括 PPT 分析报告生成、合同智能审核与生成、业务对象和表格理解、代码生成、财务和人力等领域知识增强、安全拒识等能力。这一设计的最大优势在于 YonGPT 2.0 可以充分利用用友数十年来服务各行业的经验和数据积累，同时不需要像 ToC 的通用大模型一样扩展更大的规模，节省了大量训练、微调和运维成本。</p><p></p><p>虽然 YonGPT 2.0 大模型已经专门为企业应用量身定制，但各个行业在实际部署模型时仍需要微调和优化才能获得更好的使用效果。对此，用友提供了一个一站式的大模型平台即服务，覆盖了数据管理、模型训练、评估优化、推理服务的全流程。平台内置了多个专业数据集，还支持百川、通义千问等多种大模型的微调训练。用户训练完成后，还可以在平台上直接评估效果，进行可视化展示。通过这一平台，没有大模型实践经验的企业也能快速上手，将 YonGPT 2.0 调整为更加适合自身业务需求的状态，为接下来的应用开发做好准备。</p><p><img src="https://static001.geekbang.org/infoq/03/03f407d4f9cd6054849cc4dbaf69c271.png" /></p><p>企业服务大模型的最终目标还是解决实际的业务问题，对此，用友汇总了经营中常见的八大问题场景，包括人、财、物、服、供、产、销、研，各个场景又总结出八种业务运营和知识生成的问题类型。对于这些问题，用友基于 Agent、RAG 应用框架， 帮助企业实现业务运营、人机交互、知识生成和应用生成等应用能力。</p><p></p><p>Agent 应用框架主要负责将用户的自然语言需求转换分解成模型能够识别的子任务集，并基于这些子任务输出模型 API 可以调用的参数。面对复杂问题时，用友的多智能体自主协同框架可以调度多个大模型模块，用友还结合专家知识和错误反馈学习解决了模型的幻觉和可靠性问题，并优化了模型的时效性和安全性表现。</p><p></p><p>知识生成问答无疑是大模型落地倍受瞩目的应用场景。但基于企业自身数据积累的知识生成高度依赖数据处理框架，处理不好很容易“答非所问”甚至输出误导、错误结果。用友结合流行的 RAG 框架开发了智能大搜产品，并提出了多语义向量技术，对每个知识片段都生成了向量和问题来增强索引，显著提升了知识搜索的精确度。用友还解决了索引搜索的权限问题，防止低权限用户搜索到高权限内容。该框架对表格、图片、视频、代码的理解也更加准确。企业员工使用自然语言提出问题，智能大搜不仅可以给出准确的文本回答，还能输出关系图、汇总图、相关图片和视频，甚至可以帮助员工扩写论点、整理文稿等。而基于 Code RAG 应用框架，应用开发人员甚至业务人员都可以快速生成企业应用代码，简化应用生成流程。所有生成内容都能无缝对接员工使用的各类应用，帮助企业实现全流程、全场景提效。</p><p></p><p>为了深化大模型在行业的场景应用，用友在本次大会上还联合来自公共资源交易行业、工业装备行业、交通建设行业的代表客户，发布了三大行业的垂类大模型，加速了 AI 在千行百业的落地进程 。</p><p></p><h2>挖掘智能场景应用，YonAI 为大模型落地构建平台基础</h2><p></p><p></p><p>YonGPT 2.0 的能力升级，为企业在业务中运用大模型提效增速铺平了道路。YonGPT 是用友为企业持续输出 AI 服务能力的核心工具。如前文所述，企业在 AI 落地过程中面临着一系列挑战，这些挑战仅靠大模型技术本身是不足以应对的。正因如此，用友将过去数十年帮助企业数智化转型取得的技术成果与 YonGPT 大模型创新结合起来，推出了用友 iuap 智能平台 YonAI。</p><p><img src="https://static001.geekbang.org/infoq/49/498a9726bd76471c08cf6a544cd041c9.png" /></p><p></p><p>YonAI 平台由包含 YonGPT 大模型的智能基础平台层、智能算法层、包含 Agent、RAG 和智能服务的智能框架层，以及最顶层的智能入口层构建而成，外部对接用友云技术、应用和数据平台，从而为企业提供全方位、全场景的平台化 AI 能力支撑。基于 YonAI 平台，企业员工在日常业务中随处开启智友智能助理和智能大搜服务，就可以轻松调用 YonGPT 大模型等 AI 能力来提升工作效率，启发创新灵感。用友也在服务企业客户的过程中与用户共同探索，挖掘出了一些企业在当下可以快速引入 AI 技术的应用场景。</p><p></p><p>合同审核是业务运营中常见而关键的环节之一。业务人员将合同草稿输入审核应用，即可自动提取关键字段，根据知识库内拟定的业务规则和敏感词审查合同违规情况。智友助手还能帮助业务人员查询合同相关数据，计算合约背后的经济和财务数据，乃至辅助补充合同条款、润色文本等。合同审核智能化大大缩短了业务合约的审批周期，加快资源周转，提升了业务运营效率。</p><p></p><p>在企业人力资源领域，员工面试是人资部门的日常工作。用友为企业打造了 AI 面试平台，通过对面试视频记录的 AI 分析为候选人进行多维评价，绘制人才画像。平台覆盖 140 多个评分项、600 多个评价标准，并能在面试结束后自动输出面试纪要和综合建议。在典型客户的实践应用中，用友 AI 面试可以帮助人力资源部门提升 30% 的面试效率。</p><p></p><p>用友智能大搜产品也有着丰富的使用场景。例如，出差人员可以通过简单询问快速了解差旅报销标准；新人入职后，可以在智能大搜服务中点播各类企业培训课程自主学习；管理人员组建团队时，可以使用智能大搜寻找符合所需人员属性、匹配岗位的员工人才；营销人员则能利用智能大搜查找企业营销知识库的详细内容等等。员工使用搜索功能查找到所需资料后，可以直接使用这些资料智能生成文档、报告、知识图谱，节约大量文书工作的时间和精力投入。</p><p></p><p>企业员工还能使用手机遥控桌面打开企业应用进行展示。业务人员可以利用大模型代码生成框架，将自然语言自动转化为所需的代码脚本，开发人员在前端开发过程中也能受益于代码自动补全能力。最后，企业交流群中的群组机器人能够随时响应员工的问题，给出准确、实时的回答。</p><p></p><h2>AI 赋能，平台建设才是标准解法</h2><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c437b44081476bbc408b69fc587ef71.png" /></p><p>如今，通过 AI 技术重构应用，将 AI 算法模型深度融入行业与领域场景，成为颠覆传统业务模式，创新商业形态的最佳路径。</p><p></p><p>用友 iuap 智能平台 YonAI，再一次证明任何创新技术想要真正落地到企业业务层面，为企业带来看得见的收益，都不能仅靠技术本身的孤立应用来达成目标。尤其对于 AI 大模型这样具备颠覆性能力的创新，更要融入企业数智平台建设才能发挥更大效应。</p><p></p><p>YonAI 作为用友历时多年建设的企业数智化底座用友 iuap 的 AI 能力引擎，帮助企业升级数智底座，实现智能运营。在用友 iuap 平台中，云技术、应用、数据、开发和连接集成平台共同为 YonAI 智能平台的能力提供支撑。而 YonAI 的智能能力则通过这些平台延伸到企业业务的十大领域和每一个具体场景中。通过平台化建设，用友解决了 AI 赋能企业的最大挑战，使 AI 落地过程“润物无声”，也为行业给出了一套标准解法。</p><p></p><p>用友 iuap 通过融合六大平台、YonGPT 大模型以及工程化体系和运营体系能力，构建起完整的数智化平台能力，帮助企业搭建起智能运营、数据驱动、敏捷创新、开放连接和全球化支撑等核心能力，加速企业数智化进程！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/dsjckWx5uVJZBH00xGpx</id>
            <title>星尘智能发布新一代AI机器人Astribot S1，煮饭泡茶打拳投篮...样样都能干？</title>
            <link>https://www.infoq.cn/article/dsjckWx5uVJZBH00xGpx</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/dsjckWx5uVJZBH00xGpx</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 09:54:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>整理 | 华卫</p><p></p><p>8月19日，星尘智能发布新一代AI机器人助理Astribot S1，并展现了其基于面向AI（Design for AI）的软硬件一体化系统架构的泛场景通用操作能力。8月21日，S1将于在北京举办的世界机器人大会上对公众亮相。</p><p></p><p>S1是具备全能操作的具身人形机器人，在今年四月首次技术展示中，执行了熨叠衣物、分拣物品、颠锅炒菜、吸尘清洁、竞技叠杯等多项复杂任务，引发广泛关注。此次S1以整机形态亮相，完成了一系列高难度、长序列、可泛化任务。</p><p></p><p>在1倍速（业界常见为3到10倍速）的视频展示中，S1可谓智能又全能，在食物制作、泡功夫茶、乐器演奏等长序列任务展现了智能规划与最强操作，在模仿咏春拳、定点投篮等特技上展现了媲美专家的敏捷、灵巧与丝滑度。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/f1/f11835c78d8e6892c88a072669906892.jpeg" /></p><p></p><p></p><p></p><p>据介绍， S1将“AI智能”与“最强操作”强耦合，让机器人高度仿人，能像人一样学习、思考和劳动，与人流畅智能地交互，使用人的工具和设备、帮人完成枯燥、困难或危险的任务。AI智能方面，S1具备在复杂环境中的感知、认知、实时决策能力，及智能理解和多模态交互执行能力，实现物体、任务和环境级别通用操作泛化。</p><p></p><p>值得注意的是，星尘智能在具身智能数据获取上取得关键性突破，S1能低成本利用现有的真实世界视频数据和人体动作捕捉数据，并通过第一人称视角收集触觉、力觉、视觉、听觉等多维度的高质量数据。综合这些数据进行更高效的规模化训练，降低了机器人高质量数据采集的成本、数据量级和新任务的训练难度，提升了泛化能力的潜力。</p><p></p><p>机器人硬件方面，S1能以低成本实现同规格机器人中的“操作”。其独特的刚柔耦合传动机构设计，通过传感器实时监测力的传输，不再依赖轨迹估算，而是像人一样，通过感知力的大小来精准控制控制力的输出，显著提升操作精度。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/e5/e535ec734feb48762f114a583bddc034.jpeg" /></p><p></p><p></p><p></p><p>S1具备“高价值的上半身，可落地的下半身”，可用于科研、商业和家庭等广泛场景，预计于2024年完成商业化。其关键零部件自研，具备明显的成本优势。通过刚柔耦合硬件设计和创新力规划算法，S1具备极高安全性，能在交互中精确控制力度，在运动中不伤人、不伤物、不伤自己。</p><p></p><p></p><p>星尘智能 CEO 来杰表示：“我们的愿景是让数十亿人拥有 AI 机器人助理。无论是照顾家庭还是到工厂工作，机器人在学习、决策和执行上越像人，越能帮人做得更多和更好，因此欢迎大家给S1提需求，让它的能力能从55%、85%成长到99.99%，无限接近人类水平。也希望未来五年到十年，AI机器人就能走进千家万户。”</p><p></p><p></p><p>公开资料显示，星尘智能（Astribot）于2022年底在深圳成立，公司已完成数千万美元Pre-A轮融资，由经纬创投领投，道彤投资及清辉投资等产业资本跟投，老股东云启资本跟投。创始人来杰拥有16年机器人研发经验，曾是腾讯机器人实验室1号员工、百度“小度机器人”负责人等，持续推动机器人与人工智能技术结合，让AI机器人从梦想变为现实。团队来自腾讯、谷歌、华为、大疆等企业，及国内外顶尖高校和人工智能研究院。</p><p></p><p></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/oFNB1gBmecXEox4q38sN</id>
            <title>国产大模型超越Llama3！岩芯数智RockAI重新定义端侧智能</title>
            <link>https://www.infoq.cn/article/oFNB1gBmecXEox4q38sN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/oFNB1gBmecXEox4q38sN</guid>
            <pubDate></pubDate>
            <updated>Tue, 20 Aug 2024 09:02:35 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>8月18-19日，AICon全球人工智能开发与应用大会在上海举办，以“智能未来，探索 AI 无限可能”为主题，聚焦大模型开发与应用领域。RockAI CEO刘凡平应邀出席并发表《非Transformer架构的端侧大模型创新研究与应用》主题演讲，重新定义端侧智能，引发了行业对端侧AI落地方向的全新思考。</p><p>&nbsp;</p><p>众所周知，端侧AI通常指在终端设备上直接运行和处理人工智能算法的技术，具有减少云端算力依赖、保证用户数据安全等优势。目前，行业普遍将算力限制和数据匮乏视同端侧AI技术发展的拦路虎。而RockAI则认为，基础架构和核心算法的创新才是突破端侧AI发展局限的关键。基于对算法和架构的创新，即使面临算力限制，端侧AI仍可在终端设备上实现流畅的智能多模态运用。</p><p>&nbsp;</p><p>这一观点也在RockAI关于Yan架构大模型的创新实践上得到了证明。其推出的国内首个非Attention机制的Yan架构大模型，可在主流消费级CPU等端侧设备上无损运行，达到其他模型GPU上的运行效果。全面升级后，Yan1.2多模态大模型，已经可以在树莓派、机器人、手机等低功耗计算平台无损流畅运行，将端侧应用场景拓宽至智能家居、物联网等领域。而最新数据显示，3B参数的Yan1.3&nbsp;preview大模型在各项测评中的平均得分甚至超越了8B参数的Llama3，达到极高的知识密度。</p><p></p><p><img src="https://static001.geekbang.org/infoq/24/244def87f3af46930572ec29941ce5a3.png" /></p><p></p><p>论坛现场，刘凡平深入剖析了当前端侧AI技术的发展现状及局限性。他指出，目前大多数“狭义端侧模型”的核心目标在于为用户提供大语言模型推理服务，受限于模型参数、算力、软件生态、功耗控制等诸多难题，往往会通过压缩、分割等软硬件协同优化实现大语言模型在终端设备上的本地化应用。但端侧AI的未来不仅仅在于推理能力的提升，更在于能够实现模型的自我学习和优化，以适应不断变化的应用场景和愈发广阔的用户需求。而通过以上处理手段，模型是无法在端侧进行训练和微调的，更不必说实现自我学习。</p><p>&nbsp;</p><p>刘凡平强调，RockAI不做“狭义的端侧模型”，而是着眼于更广泛意义上的端侧智能，即让世界上每一台设备都拥有自己的智能。这要求端侧模型除了语言理解及生成能力外，还应该具备抽象思考、因果推理、自我反思以及跨领域迁移学习等更复杂的认知功能。因此，端侧模型需要至少支持“理解表达、选择遗忘、持续学习”三种基础能力。</p><p>&nbsp;</p><p>为达成这一目标，RockAI在基础架构创新和实现消费级终端无损部署外，首创了“同步学习”机制。该机制可以使大模型在推理的同时进行知识更新和学习，建立自己独有的知识体系，实现模型的边跑边进化。同时，通过跨模态关联学习，增强模型在多场景下的应用能力，实现秒级实时反馈的人机交互，真正做到端侧模型的自我学习、类人感知和实时交互，推动端侧AI向自适应智能进化阶段演进。</p><p>&nbsp;</p><p>RockAI基于Yan架构大模型的技术突破和创新实践，打破了当前端侧AI发展的技术壁垒，不仅为整个行业的发展提供了新的思路和方向，也预示着端侧AI正朝着更广泛的应用场景稳步前进。待同步学习+全模态+实时人机交互落地后，Yan2.0的诞生将重新定义端侧智能，真正赋予机器自主学习与自我优化能力，构建持续进化乃至群体智能涌现的AGI智慧生态。</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>